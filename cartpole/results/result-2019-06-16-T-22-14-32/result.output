Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0073],
        [0.5134],
        [0.0114]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0292, 2.0536, 0.0454], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0292, 2.0536, 0.0454], device='cuda:0')
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.077
Iter 2/2000 - Loss: 0.525
Iter 3/2000 - Loss: 0.083
Iter 4/2000 - Loss: 0.526
Iter 5/2000 - Loss: 0.803
Iter 6/2000 - Loss: 0.708
Iter 7/2000 - Loss: 0.475
Iter 8/2000 - Loss: 0.293
Iter 9/2000 - Loss: 0.204
Iter 10/2000 - Loss: 0.202
Iter 11/2000 - Loss: 0.251
Iter 12/2000 - Loss: 0.293
Iter 13/2000 - Loss: 0.283
Iter 14/2000 - Loss: 0.222
Iter 15/2000 - Loss: 0.144
Iter 16/2000 - Loss: 0.086
Iter 17/2000 - Loss: 0.061
Iter 18/2000 - Loss: 0.062
Iter 19/2000 - Loss: 0.075
Iter 20/2000 - Loss: 0.085
Iter 1981/2000 - Loss: -0.229
Iter 1982/2000 - Loss: -0.229
Iter 1983/2000 - Loss: -0.229
Iter 1984/2000 - Loss: -0.229
Iter 1985/2000 - Loss: -0.229
Iter 1986/2000 - Loss: -0.229
Iter 1987/2000 - Loss: -0.229
Iter 1988/2000 - Loss: -0.229
Iter 1989/2000 - Loss: -0.229
Iter 1990/2000 - Loss: -0.229
Iter 1991/2000 - Loss: -0.229
Iter 1992/2000 - Loss: -0.229
Iter 1993/2000 - Loss: -0.229
Iter 1994/2000 - Loss: -0.229
Iter 1995/2000 - Loss: -0.229
Iter 1996/2000 - Loss: -0.229
Iter 1997/2000 - Loss: -0.229
Iter 1998/2000 - Loss: -0.229
Iter 1999/2000 - Loss: -0.229
Iter 2000/2000 - Loss: -0.229
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0052],
        [0.2723],
        [0.0081]], device='cuda:0')
Lengthscale: tensor([[[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]],

        [[0.0210, 0.0560, 0.4990, 0.0118, 0.0010, 0.0866]]], device='cuda:0')
Signal Variance: tensor([0.0030, 0.0210, 1.5760, 0.0328], device='cuda:0')
Estimated target variance: tensor([0.0042, 0.0292, 2.0536, 0.0454], device='cuda:0')
N: 10
Signal to noise ratio: tensor([1.9642, 2.0039, 2.4058, 2.0095], device='cuda:0')
Bound on condition number: tensor([39.5822, 41.1568, 58.8806, 41.3813], device='cuda:0')
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.484897820969728, policy loss: 0.7036372554107848
Experience 1, Iter 1, disc loss: 1.4728897814865363, policy loss: 0.7073759013866446
Experience 1, Iter 2, disc loss: 1.4733246980253347, policy loss: 0.6979423802890364
Experience 1, Iter 3, disc loss: 1.4484336722091604, policy loss: 0.7185191327217688
Experience 1, Iter 4, disc loss: 1.4443343480594912, policy loss: 0.714372374304716
Experience 1, Iter 5, disc loss: 1.436755715165429, policy loss: 0.7145827227493329
Experience 1, Iter 6, disc loss: 1.4322668724702698, policy loss: 0.7112836248432504
Experience 1, Iter 7, disc loss: 1.4155530490940655, policy loss: 0.7238286500318859
Experience 1, Iter 8, disc loss: 1.4094600543493365, policy loss: 0.723696373348331
Experience 1, Iter 9, disc loss: 1.4004936931308678, policy loss: 0.7286981397471283
Experience 1, Iter 10, disc loss: 1.3968788095664522, policy loss: 0.7250594726358843
Experience 1, Iter 11, disc loss: 1.396398981616561, policy loss: 0.7197734482737984
Experience 1, Iter 12, disc loss: 1.3975282390623038, policy loss: 0.7140873449248508
Experience 1, Iter 13, disc loss: 1.3734527894963229, policy loss: 0.7363931732163769
Experience 1, Iter 14, disc loss: 1.3746703311656219, policy loss: 0.7294750111555705
Experience 1, Iter 15, disc loss: 1.3754804947069483, policy loss: 0.7243285853219708
Experience 1, Iter 16, disc loss: 1.3744891997918347, policy loss: 0.7198375255535441
Experience 1, Iter 17, disc loss: 1.354453414957113, policy loss: 0.7411175789758276
Experience 1, Iter 18, disc loss: 1.3604074712057759, policy loss: 0.7284272767025508
Experience 1, Iter 19, disc loss: 1.348719964705837, policy loss: 0.736740485751348
Experience 1, Iter 20, disc loss: 1.3466132943842861, policy loss: 0.7357137446761013
Experience 1, Iter 21, disc loss: 1.3374194255362362, policy loss: 0.742405766997809
Experience 1, Iter 22, disc loss: 1.3303305274549593, policy loss: 0.747954466928851
Experience 1, Iter 23, disc loss: 1.3251181972536874, policy loss: 0.7547299018206709
Experience 1, Iter 24, disc loss: 1.3351522638803477, policy loss: 0.7348145837496779
Experience 1, Iter 25, disc loss: 1.3231233680372965, policy loss: 0.7455914900001268
Experience 1, Iter 26, disc loss: 1.3243195317964631, policy loss: 0.7404414749624502
Experience 1, Iter 27, disc loss: 1.3024233879494618, policy loss: 0.7656994270511275
Experience 1, Iter 28, disc loss: 1.3012279528835875, policy loss: 0.7632739286026845
Experience 1, Iter 29, disc loss: 1.2728997274814655, policy loss: 0.7975116687507635
Experience 1, Iter 30, disc loss: 1.2917561062275928, policy loss: 0.7648133465352298
Experience 1, Iter 31, disc loss: 1.3001808628540408, policy loss: 0.750087800586141
Experience 1, Iter 32, disc loss: 1.2702856103239664, policy loss: 0.7853341463139298
Experience 1, Iter 33, disc loss: 1.266613391882821, policy loss: 0.7867595458276484
Experience 1, Iter 34, disc loss: 1.2491957404963507, policy loss: 0.8077401808532638
Experience 1, Iter 35, disc loss: 1.2546820792505158, policy loss: 0.7959673537575848
Experience 1, Iter 36, disc loss: 1.2572620900204419, policy loss: 0.7921767906760011
Experience 1, Iter 37, disc loss: 1.2568682319373918, policy loss: 0.7850952043066841
Experience 1, Iter 38, disc loss: 1.2397438882433898, policy loss: 0.8056684823983229
Experience 1, Iter 39, disc loss: 1.2531825534907641, policy loss: 0.7868503717157087
Experience 1, Iter 40, disc loss: 1.2250157171258236, policy loss: 0.8222059894769489
Experience 1, Iter 41, disc loss: 1.190957165015016, policy loss: 0.8673595590971954
Experience 1, Iter 42, disc loss: 1.2067818473889886, policy loss: 0.8469395952345964
Experience 1, Iter 43, disc loss: 1.1861710195262725, policy loss: 0.8753501694821706
Experience 1, Iter 44, disc loss: 1.2079939568021785, policy loss: 0.8455532591496882
Experience 1, Iter 45, disc loss: 1.1767110210541514, policy loss: 0.8873340190925565
Experience 1, Iter 46, disc loss: 1.1973023694512548, policy loss: 0.8454443323807046
Experience 1, Iter 47, disc loss: 1.1741205964566432, policy loss: 0.8871116435574741
Experience 1, Iter 48, disc loss: 1.162922800938075, policy loss: 0.8974666867464268
Experience 1, Iter 49, disc loss: 1.171246782747601, policy loss: 0.8867327315169256
Experience 1, Iter 50, disc loss: 1.1398468724559305, policy loss: 0.9434473360928236
Experience 1, Iter 51, disc loss: 1.140410705200459, policy loss: 0.913645694204571
Experience 1, Iter 52, disc loss: 1.1361598754041455, policy loss: 0.9469969068226695
Experience 1, Iter 53, disc loss: 1.1075004186858353, policy loss: 0.9898078828521706
Experience 1, Iter 54, disc loss: 1.116731275629164, policy loss: 0.9843438371887105
Experience 1, Iter 55, disc loss: 1.1228703237700624, policy loss: 0.9466399144929158
Experience 1, Iter 56, disc loss: 1.0895414309157623, policy loss: 1.0066789444240742
Experience 1, Iter 57, disc loss: 1.085511300710308, policy loss: 0.9986032612109139
Experience 1, Iter 58, disc loss: 1.0505578114662533, policy loss: 1.0785922851365024
Experience 1, Iter 59, disc loss: 1.0296443913896864, policy loss: 1.1866281364678968
Experience 1, Iter 60, disc loss: 1.0124573382751454, policy loss: 1.169913065964716
Experience 1, Iter 61, disc loss: 1.0103477139677, policy loss: 1.1862578815568354
Experience 1, Iter 62, disc loss: 1.0517151591843068, policy loss: 1.0589660288099865
Experience 1, Iter 63, disc loss: 1.003390492866011, policy loss: 1.2071851010015162
Experience 1, Iter 64, disc loss: 1.0060348141043485, policy loss: 1.225959768832277
Experience 1, Iter 65, disc loss: 0.9627367042906722, policy loss: 1.266506727491222
Experience 1, Iter 66, disc loss: 0.9925050827536679, policy loss: 1.1631943529943105
Experience 1, Iter 67, disc loss: 0.9878110214446847, policy loss: 1.139223946593475
Experience 1, Iter 68, disc loss: 0.9370637332101106, policy loss: 1.3215672495255548
Experience 1, Iter 69, disc loss: 0.9725054653311487, policy loss: 1.2508452314531664
Experience 1, Iter 70, disc loss: 0.9558679350606971, policy loss: 1.251541524000308
Experience 1, Iter 71, disc loss: 0.8785512327741012, policy loss: 1.5219070275247488
Experience 1, Iter 72, disc loss: 0.9171195635957847, policy loss: 1.3347666884471598
Experience 1, Iter 73, disc loss: 0.869588304718879, policy loss: 1.4865682288978337
Experience 1, Iter 74, disc loss: 0.882370634217326, policy loss: 1.393847791515043
Experience 1, Iter 75, disc loss: 0.8216763266432834, policy loss: 1.6255095624168798
Experience 1, Iter 76, disc loss: 0.8578313900327807, policy loss: 1.5481942678426244
Experience 1, Iter 77, disc loss: 0.8399920973596046, policy loss: 1.504075145697812
Experience 1, Iter 78, disc loss: 0.8251619692390079, policy loss: 1.5853617626892673
Experience 1, Iter 79, disc loss: 0.8261256022460163, policy loss: 1.5877509653666266
Experience 1, Iter 80, disc loss: 0.7943926273273872, policy loss: 1.6324433204770963
Experience 1, Iter 81, disc loss: 0.77384429143363, policy loss: 1.705419163783758
Experience 1, Iter 82, disc loss: 0.763711084088916, policy loss: 1.7476994310263678
Experience 1, Iter 83, disc loss: 0.7687179814119243, policy loss: 1.7308699621300574
Experience 1, Iter 84, disc loss: 0.7308797374959438, policy loss: 1.859077693231958
Experience 1, Iter 85, disc loss: 0.7127585521808354, policy loss: 1.9600036112549937
Experience 1, Iter 86, disc loss: 0.700758258860316, policy loss: 1.9995443317459074
Experience 1, Iter 87, disc loss: 0.7461889635653998, policy loss: 1.7799202530749278
Experience 1, Iter 88, disc loss: 0.6773553381541434, policy loss: 2.1842053704206217
Experience 1, Iter 89, disc loss: 0.6885107214082837, policy loss: 1.9309266290670437
Experience 1, Iter 90, disc loss: 0.6764494521758013, policy loss: 2.0399651312883
Experience 1, Iter 91, disc loss: 0.6570375462816564, policy loss: 2.1023190774292253
Experience 1, Iter 92, disc loss: 0.6510326111073762, policy loss: 2.123312358052668
Experience 1, Iter 93, disc loss: 0.6465384214028744, policy loss: 2.1399668057111962
Experience 1, Iter 94, disc loss: 0.6332002611301347, policy loss: 2.271100540306636
Experience 1, Iter 95, disc loss: 0.6211897822297103, policy loss: 2.3885125432164003
Experience 1, Iter 96, disc loss: 0.5798312165245053, policy loss: 2.5403935923480225
Experience 1, Iter 97, disc loss: 0.6044585892877965, policy loss: 2.3298293019342653
Experience 1, Iter 98, disc loss: 0.6088324850582532, policy loss: 2.1226376766625816
Experience 1, Iter 99, disc loss: 0.5511462087741947, policy loss: 2.63098255899299
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0298],
        [0.4058],
        [0.0056]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4542e-02, 1.0529e-01, 2.5923e-01, 9.7750e-03, 6.5859e-04,
          8.0815e-01]],

        [[1.4542e-02, 1.0529e-01, 2.5923e-01, 9.7750e-03, 6.5859e-04,
          8.0815e-01]],

        [[1.4542e-02, 1.0529e-01, 2.5923e-01, 9.7750e-03, 6.5859e-04,
          8.0815e-01]],

        [[1.4542e-02, 1.0529e-01, 2.5923e-01, 9.7750e-03, 6.5859e-04,
          8.0815e-01]]], device='cuda:0', grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.1191, 1.6231, 0.0225], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.1191, 1.6231, 0.0225], device='cuda:0')
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.850
Iter 2/2000 - Loss: 0.811
Iter 3/2000 - Loss: 0.760
Iter 4/2000 - Loss: 0.661
Iter 5/2000 - Loss: 0.701
Iter 6/2000 - Loss: 0.713
Iter 7/2000 - Loss: 0.667
Iter 8/2000 - Loss: 0.651
Iter 9/2000 - Loss: 0.672
Iter 10/2000 - Loss: 0.676
Iter 11/2000 - Loss: 0.649
Iter 12/2000 - Loss: 0.636
Iter 13/2000 - Loss: 0.650
Iter 14/2000 - Loss: 0.660
Iter 15/2000 - Loss: 0.644
Iter 16/2000 - Loss: 0.626
Iter 17/2000 - Loss: 0.631
Iter 18/2000 - Loss: 0.645
Iter 19/2000 - Loss: 0.642
Iter 20/2000 - Loss: 0.626
Iter 1981/2000 - Loss: -0.568
Iter 1982/2000 - Loss: -0.567
Iter 1983/2000 - Loss: -0.567
Iter 1984/2000 - Loss: -0.568
Iter 1985/2000 - Loss: -0.567
Iter 1986/2000 - Loss: -0.567
Iter 1987/2000 - Loss: -0.568
Iter 1988/2000 - Loss: -0.567
Iter 1989/2000 - Loss: -0.568
Iter 1990/2000 - Loss: -0.568
Iter 1991/2000 - Loss: -0.568
Iter 1992/2000 - Loss: -0.568
Iter 1993/2000 - Loss: -0.568
Iter 1994/2000 - Loss: -0.568
Iter 1995/2000 - Loss: -0.568
Iter 1996/2000 - Loss: -0.568
Iter 1997/2000 - Loss: -0.568
Iter 1998/2000 - Loss: -0.568
Iter 1999/2000 - Loss: -0.568
Iter 2000/2000 - Loss: -0.568
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0222],
        [0.2471],
        [0.0043]], device='cuda:0')
Lengthscale: tensor([[[1.4942e+01, 3.2225e+00, 6.7134e+01, 1.5751e+01, 9.5485e+00,
          1.1801e+01]],

        [[1.4492e-02, 1.0080e-01, 2.4005e-01, 9.7739e-03, 6.5856e-04,
          6.9394e-01]],

        [[1.4493e-02, 1.0090e-01, 2.4048e-01, 9.7739e-03, 6.5856e-04,
          6.9661e-01]],

        [[1.4509e-02, 1.0235e-01, 2.4677e-01, 9.7743e-03, 6.5857e-04,
          7.3560e-01]]], device='cuda:0')
Signal Variance: tensor([0.0387, 0.0909, 1.2948, 0.0171], device='cuda:0')
Estimated target variance: tensor([0.0114, 0.1191, 1.6231, 0.0225], device='cuda:0')
N: 20
Signal to noise ratio: tensor([10.5962,  2.0244,  2.2890,  2.0058], device='cuda:0')
Bound on condition number: tensor([2246.6040,   82.9607,  105.7864,   81.4649], device='cuda:0')
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.7590253642180735, policy loss: 1.4808360287687443
Experience 2, Iter 1, disc loss: 0.6568468297883014, policy loss: 1.8603316438382382
Experience 2, Iter 2, disc loss: 0.7387711618236457, policy loss: 1.6878029188484587
Experience 2, Iter 3, disc loss: 0.7049590052548615, policy loss: 1.7736196577440269
Experience 2, Iter 4, disc loss: 0.7099809504704302, policy loss: 1.7071699048545175
Experience 2, Iter 5, disc loss: 0.8050008158920628, policy loss: 1.3586458408972204
Experience 2, Iter 6, disc loss: 0.6453587023416146, policy loss: 2.0361334728787193
Experience 2, Iter 7, disc loss: 0.6654055920836337, policy loss: 1.8442771062755547
Experience 2, Iter 8, disc loss: 0.6915834893298995, policy loss: 1.9218209241990516
Experience 2, Iter 9, disc loss: 0.6699575200705592, policy loss: 1.7330735030555595
Experience 2, Iter 10, disc loss: 0.6464243561605676, policy loss: 1.8750081929131464
Experience 2, Iter 11, disc loss: 0.6206370851383215, policy loss: 2.0003732373840273
Experience 2, Iter 12, disc loss: 0.6430864684115597, policy loss: 1.8055591583819712
Experience 2, Iter 13, disc loss: 0.5914949008074043, policy loss: 2.132433576999979
Experience 2, Iter 14, disc loss: 0.6352540189034399, policy loss: 1.8474696416750445
Experience 2, Iter 15, disc loss: 0.583012251953195, policy loss: 2.209218318339728
Experience 2, Iter 16, disc loss: 0.585564742312552, policy loss: 1.9983298573242958
Experience 2, Iter 17, disc loss: 0.5476273780109191, policy loss: 2.1831960714749536
Experience 2, Iter 18, disc loss: 0.5735853018263606, policy loss: 2.079044054733899
Experience 2, Iter 19, disc loss: 0.5627075587435642, policy loss: 2.1642018485880685
Experience 2, Iter 20, disc loss: 0.5246299447146632, policy loss: 2.236983909165816
Experience 2, Iter 21, disc loss: 0.5457098654288232, policy loss: 2.2236665858179703
Experience 2, Iter 22, disc loss: 0.5314350694429104, policy loss: 2.0772060613615597
Experience 2, Iter 23, disc loss: 0.4989130433478115, policy loss: 2.3386922037292655
Experience 2, Iter 24, disc loss: 0.4960292687077194, policy loss: 2.3294987068964894
Experience 2, Iter 25, disc loss: 0.46504889202196187, policy loss: 2.5091133785990243
Experience 2, Iter 26, disc loss: 0.49519633162810683, policy loss: 2.26841290574438
Experience 2, Iter 27, disc loss: 0.4800380375169023, policy loss: 2.6501659413293286
Experience 2, Iter 28, disc loss: 0.46239195536076316, policy loss: 2.2909017496282527
Experience 2, Iter 29, disc loss: 0.45879884129361215, policy loss: 2.3136081353341456
Experience 2, Iter 30, disc loss: 0.4411377740416752, policy loss: 2.5229598749986026
Experience 2, Iter 31, disc loss: 0.4053812678909988, policy loss: 2.831782890531449
Experience 2, Iter 32, disc loss: 0.4240907686565855, policy loss: 2.5653490603405156
Experience 2, Iter 33, disc loss: 0.422003886834209, policy loss: 2.617893014535519
Experience 2, Iter 34, disc loss: 0.4027060106597416, policy loss: 2.6673846315072454
Experience 2, Iter 35, disc loss: 0.3986638616709725, policy loss: 2.925201591240536
Experience 2, Iter 36, disc loss: 0.4247331970195729, policy loss: 2.4326994739960366
Experience 2, Iter 37, disc loss: 0.41806061962872965, policy loss: 2.8846987851835966
Experience 2, Iter 38, disc loss: 0.40933384399216166, policy loss: 2.567148070760944
Experience 2, Iter 39, disc loss: 0.3815071570829729, policy loss: 2.609641873499276
Experience 2, Iter 40, disc loss: 0.38318895678789494, policy loss: 2.8637059550167954
Experience 2, Iter 41, disc loss: 0.3456258973791549, policy loss: 3.0087057287220818
Experience 2, Iter 42, disc loss: 0.34836518550748075, policy loss: 2.9676198922517845
Experience 2, Iter 43, disc loss: 0.32582967073388125, policy loss: 3.066098047131633
Experience 2, Iter 44, disc loss: 0.3303657574854489, policy loss: 3.3594601240603383
Experience 2, Iter 45, disc loss: 0.31521219708955767, policy loss: 3.050584963560085
Experience 2, Iter 46, disc loss: 0.296910733244434, policy loss: 3.3456932506978925
Experience 2, Iter 47, disc loss: 0.2897334869155457, policy loss: 3.3647012952012343
Experience 2, Iter 48, disc loss: 0.29590737956817553, policy loss: 3.182712760344983
Experience 2, Iter 49, disc loss: 0.30523842995386263, policy loss: 3.2715096017478262
Experience 2, Iter 50, disc loss: 0.3021397351208598, policy loss: 2.9766179507964745
Experience 2, Iter 51, disc loss: 0.26724767452607556, policy loss: 3.5355149985145937
Experience 2, Iter 52, disc loss: 0.27510739579384236, policy loss: 3.0398654542607426
Experience 2, Iter 53, disc loss: 0.2743502157969948, policy loss: 3.091874000465424
Experience 2, Iter 54, disc loss: 0.27540365205354067, policy loss: 3.405072795806092
Experience 2, Iter 55, disc loss: 0.2708170433278554, policy loss: 3.328670347208128
Experience 2, Iter 56, disc loss: 0.25686212732591207, policy loss: 3.3891746242216048
Experience 2, Iter 57, disc loss: 0.2603885948931227, policy loss: 3.1313088957456956
Experience 2, Iter 58, disc loss: 0.26381201145514527, policy loss: 3.3489687778579165
Experience 2, Iter 59, disc loss: 0.23335316794383984, policy loss: 3.7509835682786026
Experience 2, Iter 60, disc loss: 0.24393719125584562, policy loss: 3.7592100411192444
Experience 2, Iter 61, disc loss: 0.23849030684716965, policy loss: 3.393703133235694
Experience 2, Iter 62, disc loss: 0.22886549980498805, policy loss: 3.6920415494503107
Experience 2, Iter 63, disc loss: 0.21106106254339763, policy loss: 3.648164438803504
Experience 2, Iter 64, disc loss: 0.21097010595995155, policy loss: 3.461736303862597
Experience 2, Iter 65, disc loss: 0.2183963302714571, policy loss: 3.47645443537695
Experience 2, Iter 66, disc loss: 0.2021545284155543, policy loss: 3.5917417560869227
Experience 2, Iter 67, disc loss: 0.20795098394502126, policy loss: 3.4112418659369963
Experience 2, Iter 68, disc loss: 0.20426468676728482, policy loss: 3.486398926228979
Experience 2, Iter 69, disc loss: 0.1862414532101403, policy loss: 3.6137121557536807
Experience 2, Iter 70, disc loss: 0.20418957848509936, policy loss: 3.946617940433781
Experience 2, Iter 71, disc loss: 0.2029009807926247, policy loss: 3.487879152866217
Experience 2, Iter 72, disc loss: 0.1759673793664289, policy loss: 3.8784387598539007
Experience 2, Iter 73, disc loss: 0.1720555332645875, policy loss: 3.5942115617166346
Experience 2, Iter 74, disc loss: 0.15403063012310847, policy loss: 4.3598046300564
Experience 2, Iter 75, disc loss: 0.17714710317169047, policy loss: 3.8357307239642178
Experience 2, Iter 76, disc loss: 0.16921001968209357, policy loss: 3.6829265663483985
Experience 2, Iter 77, disc loss: 0.15385279424723464, policy loss: 3.8601811705602254
Experience 2, Iter 78, disc loss: 0.1453034256602006, policy loss: 4.386175312556591
Experience 2, Iter 79, disc loss: 0.15268760291618705, policy loss: 4.312599993902486
Experience 2, Iter 80, disc loss: 0.14498872887917158, policy loss: 3.961814979673102
Experience 2, Iter 81, disc loss: 0.14367791210519026, policy loss: 4.120498467068731
Experience 2, Iter 82, disc loss: 0.1492737246413177, policy loss: 3.989561149991698
Experience 2, Iter 83, disc loss: 0.13290789362132333, policy loss: 4.069161406815969
Experience 2, Iter 84, disc loss: 0.14312278061802752, policy loss: 4.0148956724062295
Experience 2, Iter 85, disc loss: 0.14402716471327245, policy loss: 4.399706749045956
Experience 2, Iter 86, disc loss: 0.13788467199557758, policy loss: 4.078630557027839
Experience 2, Iter 87, disc loss: 0.14475629940371626, policy loss: 4.155213353488328
Experience 2, Iter 88, disc loss: 0.12175986853674109, policy loss: 4.539645644167658
Experience 2, Iter 89, disc loss: 0.11665731084222374, policy loss: 4.078900565250132
Experience 2, Iter 90, disc loss: 0.11283405503217168, policy loss: 4.517365424207628
Experience 2, Iter 91, disc loss: 0.12778533150562132, policy loss: 3.848377924193776
Experience 2, Iter 92, disc loss: 0.11521194043496596, policy loss: 4.334808282895329
Experience 2, Iter 93, disc loss: 0.1232463317616743, policy loss: 4.322765954744666
Experience 2, Iter 94, disc loss: 0.10290281784432705, policy loss: 4.810059733305428
Experience 2, Iter 95, disc loss: 0.11237816052787562, policy loss: 4.190734967094018
Experience 2, Iter 96, disc loss: 0.10953633067284282, policy loss: 4.382118700629408
Experience 2, Iter 97, disc loss: 0.11023074518183586, policy loss: 4.5289860778653175
Experience 2, Iter 98, disc loss: 0.09489427230916653, policy loss: 4.930346316470827
Experience 2, Iter 99, disc loss: 0.09456022633936986, policy loss: 4.770605895143034
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0058],
        [0.0335],
        [0.3856],
        [0.0061]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.5742e-02, 2.1269e-01, 2.8134e-01, 1.3097e-02, 8.0530e-04,
          1.0910e+00]],

        [[2.5742e-02, 2.1269e-01, 2.8134e-01, 1.3097e-02, 8.0530e-04,
          1.0910e+00]],

        [[2.5742e-02, 2.1269e-01, 2.8134e-01, 1.3097e-02, 8.0530e-04,
          1.0910e+00]],

        [[2.5742e-02, 2.1269e-01, 2.8134e-01, 1.3097e-02, 8.0530e-04,
          1.0910e+00]]], device='cuda:0', grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0232, 0.1342, 1.5425, 0.0245], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0232, 0.1342, 1.5425, 0.0245], device='cuda:0')
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.174
Iter 2/2000 - Loss: 1.342
Iter 3/2000 - Loss: 1.128
Iter 4/2000 - Loss: 1.144
Iter 5/2000 - Loss: 1.216
Iter 6/2000 - Loss: 1.155
Iter 7/2000 - Loss: 1.094
Iter 8/2000 - Loss: 1.119
Iter 9/2000 - Loss: 1.160
Iter 10/2000 - Loss: 1.144
Iter 11/2000 - Loss: 1.101
Iter 12/2000 - Loss: 1.090
Iter 13/2000 - Loss: 1.112
Iter 14/2000 - Loss: 1.128
Iter 15/2000 - Loss: 1.116
Iter 16/2000 - Loss: 1.093
Iter 17/2000 - Loss: 1.088
Iter 18/2000 - Loss: 1.101
Iter 19/2000 - Loss: 1.110
Iter 20/2000 - Loss: 1.102
Iter 1981/2000 - Loss: -6.034
Iter 1982/2000 - Loss: -6.034
Iter 1983/2000 - Loss: -6.034
Iter 1984/2000 - Loss: -6.034
Iter 1985/2000 - Loss: -6.034
Iter 1986/2000 - Loss: -6.034
Iter 1987/2000 - Loss: -6.034
Iter 1988/2000 - Loss: -6.034
Iter 1989/2000 - Loss: -6.035
Iter 1990/2000 - Loss: -6.035
Iter 1991/2000 - Loss: -6.035
Iter 1992/2000 - Loss: -6.035
Iter 1993/2000 - Loss: -6.035
Iter 1994/2000 - Loss: -6.035
Iter 1995/2000 - Loss: -6.035
Iter 1996/2000 - Loss: -6.035
Iter 1997/2000 - Loss: -6.035
Iter 1998/2000 - Loss: -6.035
Iter 1999/2000 - Loss: -6.035
Iter 2000/2000 - Loss: -6.035
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0011],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[22.0807,  7.1556, 66.3556, 11.5803, 12.3493, 63.3293]],

        [[40.1710, 54.1207, 41.9661,  3.2479, 12.3558, 15.9751]],

        [[46.3533, 59.2962, 22.2330,  0.9620,  7.3801, 23.6287]],

        [[52.8425, 65.3447, 14.3331,  4.3644, 14.6426, 37.4307]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1403,  1.3519, 16.6591,  0.5004], device='cuda:0')
Estimated target variance: tensor([0.0232, 0.1342, 1.5425, 0.0245], device='cuda:0')
N: 30
Signal to noise ratio: tensor([21.1602, 35.3180, 82.5932, 40.0011], device='cuda:0')
Bound on condition number: tensor([ 13433.6850,  37421.8910, 204650.1910,  48003.7219], device='cuda:0')
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.4114974606476671, policy loss: 1.3705134670243329
Experience 3, Iter 1, disc loss: 0.4305541655710678, policy loss: 1.3060172132197416
Experience 3, Iter 2, disc loss: 0.4773472394346001, policy loss: 1.2189419894633067
Experience 3, Iter 3, disc loss: 0.5057186917761212, policy loss: 1.176252551295943
Experience 3, Iter 4, disc loss: 0.6177864395929977, policy loss: 1.0069033199613107
Experience 3, Iter 5, disc loss: 0.6597894437038354, policy loss: 0.9951184998712518
Experience 3, Iter 6, disc loss: 0.704550542984987, policy loss: 0.9936266402708054
Experience 3, Iter 7, disc loss: 0.6397178017733738, policy loss: 1.1466081109568202
Experience 3, Iter 8, disc loss: 0.5412560675645975, policy loss: 1.3677454913726992
Experience 3, Iter 9, disc loss: 0.4828499267607645, policy loss: 1.5950072963534503
Experience 3, Iter 10, disc loss: 0.4270350094544562, policy loss: 1.8151155799191756
Experience 3, Iter 11, disc loss: 0.4126273287367434, policy loss: 1.9328434018660483
Experience 3, Iter 12, disc loss: 0.4188667548895255, policy loss: 1.8914705387408404
Experience 3, Iter 13, disc loss: 0.41036504133981677, policy loss: 1.994208910291428
Experience 3, Iter 14, disc loss: 0.40915120981157693, policy loss: 2.025087438255737
Experience 3, Iter 15, disc loss: 0.4037550279862654, policy loss: 2.0923345725533453
Experience 3, Iter 16, disc loss: 0.3898239398802307, policy loss: 2.0935603798558615
Experience 3, Iter 17, disc loss: 0.3683805780535996, policy loss: 2.3212689580672228
Experience 3, Iter 18, disc loss: 0.3631759614169998, policy loss: 2.2183226986570044
Experience 3, Iter 19, disc loss: 0.3420992853226584, policy loss: 2.2634466453156588
Experience 3, Iter 20, disc loss: 0.3087610122741818, policy loss: 2.51008515181018
Experience 3, Iter 21, disc loss: 0.3157364809442041, policy loss: 2.2560861352455754
Experience 3, Iter 22, disc loss: 0.28763812808865913, policy loss: 2.366172024652478
Experience 3, Iter 23, disc loss: 0.26992904995702927, policy loss: 2.3913012758942145
Experience 3, Iter 24, disc loss: 0.2705322967474283, policy loss: 2.2380218818896305
Experience 3, Iter 25, disc loss: 0.26624830046715986, policy loss: 2.3095588725838265
Experience 3, Iter 26, disc loss: 0.22763309064021367, policy loss: 2.4310570628903965
Experience 3, Iter 27, disc loss: 0.20574208930414672, policy loss: 2.6362763289173183
Experience 3, Iter 28, disc loss: 0.2034868908494072, policy loss: 2.548873820312253
Experience 3, Iter 29, disc loss: 0.20267281783898775, policy loss: 2.5506916909587836
Experience 3, Iter 30, disc loss: 0.19979701584502796, policy loss: 2.5050270842099067
Experience 3, Iter 31, disc loss: 0.20249116610252843, policy loss: 2.420464001611399
Experience 3, Iter 32, disc loss: 0.2056548886783569, policy loss: 2.3383217360407875
Experience 3, Iter 33, disc loss: 0.19457424706366638, policy loss: 2.53774878638003
Experience 3, Iter 34, disc loss: 0.18513669211235584, policy loss: 2.56604359833836
Experience 3, Iter 35, disc loss: 0.19216009756578178, policy loss: 2.457021100299624
Experience 3, Iter 36, disc loss: 0.20237341639974085, policy loss: 2.3694835710303193
Experience 3, Iter 37, disc loss: 0.19700574879900468, policy loss: 2.488646313320449
Experience 3, Iter 38, disc loss: 0.1907183992681866, policy loss: 2.4371275715458744
Experience 3, Iter 39, disc loss: 0.17627554853323577, policy loss: 2.5956622640813576
Experience 3, Iter 40, disc loss: 0.17390846530567983, policy loss: 2.579768597583665
Experience 3, Iter 41, disc loss: 0.17982234968997213, policy loss: 2.556544394026808
Experience 3, Iter 42, disc loss: 0.16846068639247885, policy loss: 2.714364251949844
Experience 3, Iter 43, disc loss: 0.19651670043470298, policy loss: 2.428351873690006
Experience 3, Iter 44, disc loss: 0.18208607395349297, policy loss: 2.7558864633753517
Experience 3, Iter 45, disc loss: 0.17902504786592172, policy loss: 2.7299590376893668
Experience 3, Iter 46, disc loss: 0.1451243670783383, policy loss: 3.2371789842139638
Experience 3, Iter 47, disc loss: 0.1699124412670934, policy loss: 2.786339693194869
Experience 3, Iter 48, disc loss: 0.14942056449983343, policy loss: 3.246915729569551
Experience 3, Iter 49, disc loss: 0.16952769691150965, policy loss: 2.9136376900549905
Experience 3, Iter 50, disc loss: 0.15866168877188552, policy loss: 2.8074587105670377
Experience 3, Iter 51, disc loss: 0.13695739415201036, policy loss: 3.2534778059526657
Experience 3, Iter 52, disc loss: 0.14090926831719033, policy loss: 3.094894379042777
Experience 3, Iter 53, disc loss: 0.13567960761499578, policy loss: 3.1100467411866797
Experience 3, Iter 54, disc loss: 0.1312249656412614, policy loss: 3.2674802385982247
Experience 3, Iter 55, disc loss: 0.12975426099966064, policy loss: 3.2179108517789494
Experience 3, Iter 56, disc loss: 0.13398684090832075, policy loss: 3.0599267643087744
Experience 3, Iter 57, disc loss: 0.1295437574035818, policy loss: 3.2524335200444177
Experience 3, Iter 58, disc loss: 0.14264677461092204, policy loss: 3.053468011400268
Experience 3, Iter 59, disc loss: 0.14876110422135896, policy loss: 2.8519240207367966
Experience 3, Iter 60, disc loss: 0.14369107722232535, policy loss: 3.007563956074259
Experience 3, Iter 61, disc loss: 0.1441510048710028, policy loss: 3.1261698815932553
Experience 3, Iter 62, disc loss: 0.11400749351152811, policy loss: 3.3804371027729445
Experience 3, Iter 63, disc loss: 0.10971223957057882, policy loss: 3.755536582273308
Experience 3, Iter 64, disc loss: 0.13151572830239855, policy loss: 3.0704089164741992
Experience 3, Iter 65, disc loss: 0.11918103239334017, policy loss: 3.3167105160187997
Experience 3, Iter 66, disc loss: 0.11315764399408976, policy loss: 3.546434163116514
Experience 3, Iter 67, disc loss: 0.1232533122172621, policy loss: 3.2405848937336126
Experience 3, Iter 68, disc loss: 0.09759818771259897, policy loss: 3.7421175893666065
Experience 3, Iter 69, disc loss: 0.09709065051785948, policy loss: 3.8350385438141807
Experience 3, Iter 70, disc loss: 0.10383687414789486, policy loss: 3.586733670604757
Experience 3, Iter 71, disc loss: 0.11154583795434851, policy loss: 3.229789982642443
Experience 3, Iter 72, disc loss: 0.10831113245149315, policy loss: 3.459690037655358
Experience 3, Iter 73, disc loss: 0.0973953649111426, policy loss: 3.8090782433530803
Experience 3, Iter 74, disc loss: 0.10688922774060898, policy loss: 3.293878361449222
Experience 3, Iter 75, disc loss: 0.09892105876645213, policy loss: 3.537329936241183
Experience 3, Iter 76, disc loss: 0.105043988857938, policy loss: 3.6771964703538096
Experience 3, Iter 77, disc loss: 0.11313861886700208, policy loss: 3.342824134474604
Experience 3, Iter 78, disc loss: 0.10649742238625125, policy loss: 3.508889989376292
Experience 3, Iter 79, disc loss: 0.0854634489844728, policy loss: 4.472793624330816
Experience 3, Iter 80, disc loss: 0.0906148975395985, policy loss: 3.8195867594061887
Experience 3, Iter 81, disc loss: 0.09203423232534999, policy loss: 4.2409545480277195
Experience 3, Iter 82, disc loss: 0.07652160084864434, policy loss: 4.50145165267819
Experience 3, Iter 83, disc loss: 0.0860301174663827, policy loss: 3.975572626786029
Experience 3, Iter 84, disc loss: 0.0970723789264692, policy loss: 3.5905821499047534
Experience 3, Iter 85, disc loss: 0.08546239770672479, policy loss: 3.9963324604977015
Experience 3, Iter 86, disc loss: 0.08637649368711009, policy loss: 3.9006302574397704
Experience 3, Iter 87, disc loss: 0.09282318819914313, policy loss: 3.5721958983228923
Experience 3, Iter 88, disc loss: 0.10291105831594821, policy loss: 3.3099287140617824
Experience 3, Iter 89, disc loss: 0.10799366363171281, policy loss: 3.2985481271360535
Experience 3, Iter 90, disc loss: 0.10477700458998396, policy loss: 3.4056022333316163
Experience 3, Iter 91, disc loss: 0.09357446405522793, policy loss: 3.6331833832248086
Experience 3, Iter 92, disc loss: 0.08968034678590095, policy loss: 3.7111124443578705
Experience 3, Iter 93, disc loss: 0.10758957318259763, policy loss: 3.2903249015595986
Experience 3, Iter 94, disc loss: 0.11012843298340314, policy loss: 3.434039135779453
Experience 3, Iter 95, disc loss: 0.08343055193152753, policy loss: 4.082178364914482
Experience 3, Iter 96, disc loss: 0.07861575895582629, policy loss: 4.239904015500432
Experience 3, Iter 97, disc loss: 0.08625303690593117, policy loss: 4.021388292430637
Experience 3, Iter 98, disc loss: 0.10109812792642153, policy loss: 3.6412963532163323
Experience 3, Iter 99, disc loss: 0.0946475138110478, policy loss: 4.289048053528599
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.0992],
        [1.1655],
        [0.0189]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0208, 0.2101, 0.9109, 0.0212, 0.0075, 2.2142]],

        [[0.0208, 0.2101, 0.9109, 0.0212, 0.0075, 2.2142]],

        [[0.0208, 0.2101, 0.9109, 0.0212, 0.0075, 2.2142]],

        [[0.0208, 0.2101, 0.9109, 0.0212, 0.0075, 2.2142]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0222, 0.3969, 4.6619, 0.0755], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0222, 0.3969, 4.6619, 0.0755], device='cuda:0')
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.800
Iter 2/2000 - Loss: 2.969
Iter 3/2000 - Loss: 2.768
Iter 4/2000 - Loss: 2.795
Iter 5/2000 - Loss: 2.862
Iter 6/2000 - Loss: 2.811
Iter 7/2000 - Loss: 2.748
Iter 8/2000 - Loss: 2.744
Iter 9/2000 - Loss: 2.771
Iter 10/2000 - Loss: 2.771
Iter 11/2000 - Loss: 2.737
Iter 12/2000 - Loss: 2.700
Iter 13/2000 - Loss: 2.680
Iter 14/2000 - Loss: 2.674
Iter 15/2000 - Loss: 2.659
Iter 16/2000 - Loss: 2.623
Iter 17/2000 - Loss: 2.571
Iter 18/2000 - Loss: 2.515
Iter 19/2000 - Loss: 2.459
Iter 20/2000 - Loss: 2.397
Iter 1981/2000 - Loss: -4.722
Iter 1982/2000 - Loss: -4.722
Iter 1983/2000 - Loss: -4.722
Iter 1984/2000 - Loss: -4.722
Iter 1985/2000 - Loss: -4.722
Iter 1986/2000 - Loss: -4.722
Iter 1987/2000 - Loss: -4.722
Iter 1988/2000 - Loss: -4.722
Iter 1989/2000 - Loss: -4.722
Iter 1990/2000 - Loss: -4.722
Iter 1991/2000 - Loss: -4.722
Iter 1992/2000 - Loss: -4.722
Iter 1993/2000 - Loss: -4.723
Iter 1994/2000 - Loss: -4.723
Iter 1995/2000 - Loss: -4.723
Iter 1996/2000 - Loss: -4.723
Iter 1997/2000 - Loss: -4.723
Iter 1998/2000 - Loss: -4.723
Iter 1999/2000 - Loss: -4.723
Iter 2000/2000 - Loss: -4.723
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0008],
        [0.0017],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.5235,  7.6861, 63.3305, 10.9780, 10.6114, 58.5207]],

        [[28.2443, 45.0133,  9.8247,  1.4154,  4.8223, 23.3029]],

        [[27.2887, 43.1859, 15.7837,  0.8844,  1.6722,  7.8597]],

        [[25.3075, 50.5156, 11.7248,  1.2460, 16.9584, 27.3414]]],
       device='cuda:0')
Signal Variance: tensor([0.1322, 2.2171, 9.6103, 0.3304], device='cuda:0')
Estimated target variance: tensor([0.0222, 0.3969, 4.6619, 0.0755], device='cuda:0')
N: 40
Signal to noise ratio: tensor([18.2964, 52.7507, 74.4581, 34.1835], device='cuda:0')
Bound on condition number: tensor([ 13391.3752, 111306.5605, 221761.4435,  46741.5259], device='cuda:0')
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.16422798525098642, policy loss: 2.871973723929957
Experience 4, Iter 1, disc loss: 0.21347971199130017, policy loss: 2.325000028144279
Experience 4, Iter 2, disc loss: 0.256892733999752, policy loss: 2.439133303769024
Experience 4, Iter 3, disc loss: 0.289701871000866, policy loss: 2.3747954310912105
Experience 4, Iter 4, disc loss: 0.3130496788402847, policy loss: 2.3984471203009896
Experience 4, Iter 5, disc loss: 0.26909925607946916, policy loss: 2.556418933099801
Experience 4, Iter 6, disc loss: 0.2733346375032341, policy loss: 2.605786596505423
Experience 4, Iter 7, disc loss: 0.24765768444766573, policy loss: 2.739828871741989
Experience 4, Iter 8, disc loss: 0.21956568543483113, policy loss: 3.3484709527804606
Experience 4, Iter 9, disc loss: 0.24130972208972978, policy loss: 3.4588380561172154
Experience 4, Iter 10, disc loss: 0.25685520944485174, policy loss: 3.37471874775203
Experience 4, Iter 11, disc loss: 0.22573199613315764, policy loss: 4.261460393048463
Experience 4, Iter 12, disc loss: 0.20391639436344705, policy loss: 5.138703534950867
Experience 4, Iter 13, disc loss: 0.19448442994162823, policy loss: 5.042804260194505
Experience 4, Iter 14, disc loss: 0.17826959078090854, policy loss: 5.441205449549495
Experience 4, Iter 15, disc loss: 0.16528316420140265, policy loss: 5.101784068854984
Experience 4, Iter 16, disc loss: 0.14868674115830324, policy loss: 4.7763970267611
Experience 4, Iter 17, disc loss: 0.13987519340885654, policy loss: 4.07087396054367
Experience 4, Iter 18, disc loss: 0.17239328031893875, policy loss: 3.228855070349134
Experience 4, Iter 19, disc loss: 0.26671705430101855, policy loss: 2.5681186413324073
Experience 4, Iter 20, disc loss: 0.14403787556477043, policy loss: 5.459690124844014
Experience 4, Iter 21, disc loss: 0.3160901955993063, policy loss: 2.3519413600974444
Experience 4, Iter 22, disc loss: 0.12472390734538771, policy loss: 3.941291023503653
Experience 4, Iter 23, disc loss: 0.11135175643095657, policy loss: 3.995931627777751
Experience 4, Iter 24, disc loss: 0.13064879860832745, policy loss: 3.771952083058575
Experience 4, Iter 25, disc loss: 0.10610834670990152, policy loss: 3.86552804498179
Experience 4, Iter 26, disc loss: 0.10681626321759502, policy loss: 4.252969910434678
Experience 4, Iter 27, disc loss: 0.12206216831696035, policy loss: 3.773363222010475
Experience 4, Iter 28, disc loss: 0.17555511928065692, policy loss: 2.846578594576285
Experience 4, Iter 29, disc loss: 0.31968723646962066, policy loss: 2.3262855008691337
Experience 4, Iter 30, disc loss: 0.24086917672831495, policy loss: 2.950287495180106
Experience 4, Iter 31, disc loss: 0.313918168569229, policy loss: 2.316240171282296
Experience 4, Iter 32, disc loss: 0.23890154242246364, policy loss: 2.6417104388243526
Experience 4, Iter 33, disc loss: 0.18602703063872175, policy loss: 3.226378739422708
Experience 4, Iter 34, disc loss: 0.17251403088099682, policy loss: 3.637025877015884
Experience 4, Iter 35, disc loss: 0.20140174000591635, policy loss: 3.0258693474821707
Experience 4, Iter 36, disc loss: 0.1699471285095462, policy loss: 4.65392989527369
Experience 4, Iter 37, disc loss: 0.25292979602941434, policy loss: 3.031956699687328
Experience 4, Iter 38, disc loss: 0.2198203380574315, policy loss: 3.452865723395429
Experience 4, Iter 39, disc loss: 0.21903867825224407, policy loss: 3.7967712028490275
Experience 4, Iter 40, disc loss: 0.1996251001144571, policy loss: 3.7221636794252304
Experience 4, Iter 41, disc loss: 0.24518209601049373, policy loss: 3.343144466291001
Experience 4, Iter 42, disc loss: 0.23724309813817157, policy loss: 3.7221846136321175
Experience 4, Iter 43, disc loss: 0.22060659766898333, policy loss: 3.8227891776053813
Experience 4, Iter 44, disc loss: 0.22679296029625146, policy loss: 3.584297882975882
Experience 4, Iter 45, disc loss: 0.2447034251596432, policy loss: 3.331552759073901
Experience 4, Iter 46, disc loss: 0.24054767143270292, policy loss: 3.4166214957623624
Experience 4, Iter 47, disc loss: 0.2051740669814637, policy loss: 3.4073961184068664
Experience 4, Iter 48, disc loss: 0.19701456156901462, policy loss: 3.55461898052982
Experience 4, Iter 49, disc loss: 0.20053562359906396, policy loss: 3.485432532816554
Experience 4, Iter 50, disc loss: 0.2307221719852229, policy loss: 3.252666602316179
Experience 4, Iter 51, disc loss: 0.24302789880214268, policy loss: 3.2239569610075387
Experience 4, Iter 52, disc loss: 0.1533105508313217, policy loss: 3.7375375475985866
Experience 4, Iter 53, disc loss: 0.16178650110375467, policy loss: 3.535706975439865
Experience 4, Iter 54, disc loss: 0.20900280806253135, policy loss: 3.2556582703800783
Experience 4, Iter 55, disc loss: 0.21530578531040223, policy loss: 3.1531071842778156
Experience 4, Iter 56, disc loss: 0.19790029711541396, policy loss: 3.0331429736842757
Experience 4, Iter 57, disc loss: 0.18978575125755778, policy loss: 3.2518934264953536
Experience 4, Iter 58, disc loss: 0.20744440697964028, policy loss: 3.261981296772288
Experience 4, Iter 59, disc loss: 0.1670539674054453, policy loss: 3.818381328135822
Experience 4, Iter 60, disc loss: 0.17818136895194447, policy loss: 3.2164918358875845
Experience 4, Iter 61, disc loss: 0.19812030110727358, policy loss: 3.4528437468520243
Experience 4, Iter 62, disc loss: 0.15685911599434887, policy loss: 3.788447167955907
Experience 4, Iter 63, disc loss: 0.1947707016362603, policy loss: 3.33093015714739
Experience 4, Iter 64, disc loss: 0.23087060591569095, policy loss: 3.3035696819195013
Experience 4, Iter 65, disc loss: 0.19034822325330342, policy loss: 3.506635281489113
Experience 4, Iter 66, disc loss: 0.18179487047677595, policy loss: 3.604862227606398
Experience 4, Iter 67, disc loss: 0.1629373351991588, policy loss: 3.8732008261368316
Experience 4, Iter 68, disc loss: 0.17249340442989927, policy loss: 4.089691069251331
Experience 4, Iter 69, disc loss: 0.19221134787572666, policy loss: 3.863367021325743
Experience 4, Iter 70, disc loss: 0.18564654096757746, policy loss: 3.744109717006377
Experience 4, Iter 71, disc loss: 0.1413280802365849, policy loss: 4.246819338411576
Experience 4, Iter 72, disc loss: 0.16247067577291574, policy loss: 3.7647999610562857
Experience 4, Iter 73, disc loss: 0.17378186846634008, policy loss: 3.667377557076316
Experience 4, Iter 74, disc loss: 0.14603197536041956, policy loss: 3.897768083335233
Experience 4, Iter 75, disc loss: 0.12288471425262618, policy loss: 4.164834810963355
Experience 4, Iter 76, disc loss: 0.14026528504629993, policy loss: 4.056877580471328
Experience 4, Iter 77, disc loss: 0.13530692874116304, policy loss: 4.381628549708678
Experience 4, Iter 78, disc loss: 0.12845615272649488, policy loss: 3.7327541201992895
Experience 4, Iter 79, disc loss: 0.1278084594611169, policy loss: 3.773222213241407
Experience 4, Iter 80, disc loss: 0.14712769646734375, policy loss: 3.9838227600295606
Experience 4, Iter 81, disc loss: 0.1476627921297939, policy loss: 3.4336843430403174
Experience 4, Iter 82, disc loss: 0.10925279191933773, policy loss: 3.9830264836671856
Experience 4, Iter 83, disc loss: 0.11087189299023832, policy loss: 4.398904255146507
Experience 4, Iter 84, disc loss: 0.10256932895317167, policy loss: 4.25896196471518
Experience 4, Iter 85, disc loss: 0.13082002499379838, policy loss: 3.609296524785634
Experience 4, Iter 86, disc loss: 0.1186616973481712, policy loss: 4.134901167290534
Experience 4, Iter 87, disc loss: 0.129010472563585, policy loss: 3.658315651838016
Experience 4, Iter 88, disc loss: 0.1434686906337784, policy loss: 3.4982578666741935
Experience 4, Iter 89, disc loss: 0.12112119419118605, policy loss: 3.81585582156908
Experience 4, Iter 90, disc loss: 0.13982840446927738, policy loss: 3.6265611662711805
Experience 4, Iter 91, disc loss: 0.13631862999098115, policy loss: 3.584326303363672
Experience 4, Iter 92, disc loss: 0.13010785535065655, policy loss: 3.771711414262613
Experience 4, Iter 93, disc loss: 0.11781158945351469, policy loss: 4.212970635125282
Experience 4, Iter 94, disc loss: 0.11382698044767009, policy loss: 4.340285751727179
Experience 4, Iter 95, disc loss: 0.11557132586894786, policy loss: 3.904917969484589
Experience 4, Iter 96, disc loss: 0.08242217786592679, policy loss: 4.900682482017447
Experience 4, Iter 97, disc loss: 0.10056713135567177, policy loss: 4.241334739676263
Experience 4, Iter 98, disc loss: 0.11549667950871262, policy loss: 3.8720935094713966
Experience 4, Iter 99, disc loss: 0.11280527527928016, policy loss: 4.365069502612237
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1420],
        [1.4455],
        [0.0253]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0170, 0.1996, 1.2223, 0.0258, 0.0139, 3.2570]],

        [[0.0170, 0.1996, 1.2223, 0.0258, 0.0139, 3.2570]],

        [[0.0170, 0.1996, 1.2223, 0.0258, 0.0139, 3.2570]],

        [[0.0170, 0.1996, 1.2223, 0.0258, 0.0139, 3.2570]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.5681, 5.7821, 0.1014], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.5681, 5.7821, 0.1014], device='cuda:0')
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.218
Iter 2/2000 - Loss: 3.335
Iter 3/2000 - Loss: 3.180
Iter 4/2000 - Loss: 3.185
Iter 5/2000 - Loss: 3.232
Iter 6/2000 - Loss: 3.200
Iter 7/2000 - Loss: 3.141
Iter 8/2000 - Loss: 3.119
Iter 9/2000 - Loss: 3.133
Iter 10/2000 - Loss: 3.132
Iter 11/2000 - Loss: 3.095
Iter 12/2000 - Loss: 3.046
Iter 13/2000 - Loss: 3.010
Iter 14/2000 - Loss: 2.989
Iter 15/2000 - Loss: 2.959
Iter 16/2000 - Loss: 2.905
Iter 17/2000 - Loss: 2.825
Iter 18/2000 - Loss: 2.729
Iter 19/2000 - Loss: 2.624
Iter 20/2000 - Loss: 2.504
Iter 1981/2000 - Loss: -4.948
Iter 1982/2000 - Loss: -4.948
Iter 1983/2000 - Loss: -4.948
Iter 1984/2000 - Loss: -4.948
Iter 1985/2000 - Loss: -4.948
Iter 1986/2000 - Loss: -4.948
Iter 1987/2000 - Loss: -4.948
Iter 1988/2000 - Loss: -4.948
Iter 1989/2000 - Loss: -4.948
Iter 1990/2000 - Loss: -4.948
Iter 1991/2000 - Loss: -4.948
Iter 1992/2000 - Loss: -4.948
Iter 1993/2000 - Loss: -4.948
Iter 1994/2000 - Loss: -4.948
Iter 1995/2000 - Loss: -4.948
Iter 1996/2000 - Loss: -4.948
Iter 1997/2000 - Loss: -4.948
Iter 1998/2000 - Loss: -4.949
Iter 1999/2000 - Loss: -4.949
Iter 2000/2000 - Loss: -4.949
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0008],
        [0.0019],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.1245,  7.2761, 58.9123,  9.7554,  9.9038, 51.9510]],

        [[22.5958, 47.6206,  8.5140,  1.4446,  4.3219, 26.4974]],

        [[23.5071, 48.9783,  9.1049,  1.4150,  1.0306, 24.6243]],

        [[20.0992, 42.7225, 12.4696,  2.2214,  1.5858, 27.1571]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1213,  2.2190, 20.1427,  0.3242], device='cuda:0')
Estimated target variance: tensor([0.0205, 0.5681, 5.7821, 0.1014], device='cuda:0')
N: 50
Signal to noise ratio: tensor([ 17.2718,  52.3099, 101.6623,  32.3212], device='cuda:0')
Bound on condition number: tensor([ 14916.8124, 136817.1768, 516762.2043,  52233.8711], device='cuda:0')
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.19806434913703597, policy loss: 2.400768934776593
Experience 5, Iter 1, disc loss: 0.22387022476662902, policy loss: 2.319652112162597
Experience 5, Iter 2, disc loss: 0.1753485782525877, policy loss: 3.1858084109419496
Experience 5, Iter 3, disc loss: 0.19980224580944178, policy loss: 2.6063974682795643
Experience 5, Iter 4, disc loss: 0.1781638595964961, policy loss: 3.1350018019487234
Experience 5, Iter 5, disc loss: 0.18870709249497256, policy loss: 3.0437456657264343
Experience 5, Iter 6, disc loss: 0.13597597791199767, policy loss: 3.6397270261181207
Experience 5, Iter 7, disc loss: 0.14306041950739795, policy loss: 3.6972328629465485
Experience 5, Iter 8, disc loss: 0.1930618184972233, policy loss: 2.8491884486829404
Experience 5, Iter 9, disc loss: 0.2282635444552863, policy loss: 2.958484890574283
Experience 5, Iter 10, disc loss: 0.22420468338268373, policy loss: 2.756770083007135
Experience 5, Iter 11, disc loss: 0.2099117630392058, policy loss: 2.9904691494891003
Experience 5, Iter 12, disc loss: 0.25226701832894166, policy loss: 2.6524148834634595
Experience 5, Iter 13, disc loss: 0.23218516881815623, policy loss: 2.5836969972309047
Experience 5, Iter 14, disc loss: 0.28614527671311485, policy loss: 2.375078373591556
Experience 5, Iter 15, disc loss: 0.24325492743078772, policy loss: 2.6683405240474585
Experience 5, Iter 16, disc loss: 0.2655476305389115, policy loss: 2.5212949210437943
Experience 5, Iter 17, disc loss: 0.29006562260786484, policy loss: 2.594635150526112
Experience 5, Iter 18, disc loss: 0.17207482540784205, policy loss: 3.9318750543550816
Experience 5, Iter 19, disc loss: 0.15601487105666212, policy loss: 4.626262055103042
Experience 5, Iter 20, disc loss: 0.1711146959838204, policy loss: 3.8233186795476044
Experience 5, Iter 21, disc loss: 0.23409299989764604, policy loss: 2.5333314667449898
Experience 5, Iter 22, disc loss: 0.10399124853751457, policy loss: 12.508616572678973
Experience 5, Iter 23, disc loss: 0.09104796561966148, policy loss: 8.078928845234232
Experience 5, Iter 24, disc loss: 0.07839484291791647, policy loss: 9.393294679475588
Experience 5, Iter 25, disc loss: 0.06641846706772628, policy loss: 10.784324662727276
Experience 5, Iter 26, disc loss: 0.05555890617096602, policy loss: 11.364787026223166
Experience 5, Iter 27, disc loss: 0.04628630446551677, policy loss: 12.754847915331588
Experience 5, Iter 28, disc loss: 0.03891527579512111, policy loss: 13.788077499302513
Experience 5, Iter 29, disc loss: 0.03318877634528551, policy loss: 12.527338056716673
Experience 5, Iter 30, disc loss: 0.02882225615677669, policy loss: 12.455434068552213
Experience 5, Iter 31, disc loss: 0.025641205991261302, policy loss: 12.795899118625504
Experience 5, Iter 32, disc loss: 0.023407795640897185, policy loss: 12.37390901155128
Experience 5, Iter 33, disc loss: 0.021801595803804183, policy loss: 12.214894408985952
Experience 5, Iter 34, disc loss: 0.020659166342885265, policy loss: 12.468407866187663
Experience 5, Iter 35, disc loss: 0.019824752509268365, policy loss: 12.155565743296478
Experience 5, Iter 36, disc loss: 0.019221027285013504, policy loss: 12.253348528850697
Experience 5, Iter 37, disc loss: 0.018748474162121564, policy loss: 12.062042551160918
Experience 5, Iter 38, disc loss: 0.018370465364972794, policy loss: 12.197958173175795
Experience 5, Iter 39, disc loss: 0.01806234963368804, policy loss: 11.954391535536805
Experience 5, Iter 40, disc loss: 0.017774020065587332, policy loss: 11.87753965177657
Experience 5, Iter 41, disc loss: 0.017532315331176503, policy loss: 11.940619901076108
Experience 5, Iter 42, disc loss: 0.017299740840564303, policy loss: 11.80705424029545
Experience 5, Iter 43, disc loss: 0.017081070904854095, policy loss: 11.989283994688456
Experience 5, Iter 44, disc loss: 0.016884244258424806, policy loss: 11.74261969788982
Experience 5, Iter 45, disc loss: 0.01668883850365865, policy loss: 11.777684167814423
Experience 5, Iter 46, disc loss: 0.016493275680491245, policy loss: 11.687010043165417
Experience 5, Iter 47, disc loss: 0.016313349905135978, policy loss: 11.669998967031205
Experience 5, Iter 48, disc loss: 0.01613677331652604, policy loss: 11.396630891461053
Experience 5, Iter 49, disc loss: 0.015968085809463052, policy loss: 11.64331467902471
Experience 5, Iter 50, disc loss: 0.015815727028256475, policy loss: 11.164559190906619
Experience 5, Iter 51, disc loss: 0.01566709165805647, policy loss: 11.124216006724039
Experience 5, Iter 52, disc loss: 0.015518642374523052, policy loss: 10.907597533651252
Experience 5, Iter 53, disc loss: 0.015385871592380388, policy loss: 10.83461994887897
Experience 5, Iter 54, disc loss: 0.015213522330556097, policy loss: 11.341091119720893
Experience 5, Iter 55, disc loss: 0.015042345467917374, policy loss: 11.683788144820298
Experience 5, Iter 56, disc loss: 0.015053905223565254, policy loss: 10.745206040896694
Experience 5, Iter 57, disc loss: 0.014782514497996089, policy loss: 10.9663665844543
Experience 5, Iter 58, disc loss: 0.014656088175419753, policy loss: 11.08734326512528
Experience 5, Iter 59, disc loss: 0.014503386586056488, policy loss: 11.026725133019873
Experience 5, Iter 60, disc loss: 0.014363875405407272, policy loss: 11.257649432681486
Experience 5, Iter 61, disc loss: 0.014266943288174565, policy loss: 10.832763483091718
Experience 5, Iter 62, disc loss: 0.014137220433326768, policy loss: 10.805631955066815
Experience 5, Iter 63, disc loss: 0.014009361868571059, policy loss: 10.689892509043588
Experience 5, Iter 64, disc loss: 0.013874115684577089, policy loss: 11.192209215996627
Experience 5, Iter 65, disc loss: 0.013799379140736192, policy loss: 10.839645738540112
Experience 5, Iter 66, disc loss: 0.013698304008317019, policy loss: 10.987227239466407
Experience 5, Iter 67, disc loss: 0.013558685253268912, policy loss: 10.797280542459987
Experience 5, Iter 68, disc loss: 0.01342525492931322, policy loss: 10.994352214717726
Experience 5, Iter 69, disc loss: 0.013436380411146502, policy loss: 10.709498049427516
Experience 5, Iter 70, disc loss: 0.013288134454190553, policy loss: 10.451693495077272
Experience 5, Iter 71, disc loss: 0.013136055028377669, policy loss: 10.337935915992915
Experience 5, Iter 72, disc loss: 0.013033185336131117, policy loss: 10.487385099651025
Experience 5, Iter 73, disc loss: 0.0130028807215652, policy loss: 10.596636618289843
Experience 5, Iter 74, disc loss: 0.012828880824261379, policy loss: 10.422186322022329
Experience 5, Iter 75, disc loss: 0.012762713074164258, policy loss: 10.414216519982133
Experience 5, Iter 76, disc loss: 0.012663828290218134, policy loss: 10.80764547686047
Experience 5, Iter 77, disc loss: 0.01253758999248169, policy loss: 10.7127401486626
Experience 5, Iter 78, disc loss: 0.012549282042067208, policy loss: 10.218092999537369
Experience 5, Iter 79, disc loss: 0.012311220790277968, policy loss: 10.356261375658512
Experience 5, Iter 80, disc loss: 0.012216299235794634, policy loss: 10.829949151324119
Experience 5, Iter 81, disc loss: 0.012138748093639391, policy loss: 11.86613129396533
Experience 5, Iter 82, disc loss: 0.012042803283851107, policy loss: 10.302552804091533
Experience 5, Iter 83, disc loss: 0.011984013420307636, policy loss: 10.186404774037314
Experience 5, Iter 84, disc loss: 0.01194724793763245, policy loss: 10.428034536418629
Experience 5, Iter 85, disc loss: 0.011768692819284141, policy loss: 10.262486283679532
Experience 5, Iter 86, disc loss: 0.011802335938637727, policy loss: 9.977880532442954
Experience 5, Iter 87, disc loss: 0.011644562811859214, policy loss: 10.145182690894384
Experience 5, Iter 88, disc loss: 0.011546846336648757, policy loss: 10.302157413165073
Experience 5, Iter 89, disc loss: 0.011465188712215043, policy loss: 10.210957709499947
Experience 5, Iter 90, disc loss: 0.011340412523740153, policy loss: 10.433641535855898
Experience 5, Iter 91, disc loss: 0.01129321044346212, policy loss: 10.295282660921671
Experience 5, Iter 92, disc loss: 0.011246725889444921, policy loss: 10.076711125159598
Experience 5, Iter 93, disc loss: 0.011199993410134827, policy loss: 9.99363284308258
Experience 5, Iter 94, disc loss: 0.011116025461453714, policy loss: 9.993045208767974
Experience 5, Iter 95, disc loss: 0.010965402706309371, policy loss: 10.27729279396262
Experience 5, Iter 96, disc loss: 0.010935377776677819, policy loss: 10.150147646682424
Experience 5, Iter 97, disc loss: 0.011000551420657293, policy loss: 9.866729138975224
Experience 5, Iter 98, disc loss: 0.010862546127428745, policy loss: 9.774240567175653
Experience 5, Iter 99, disc loss: 0.010729429484949992, policy loss: 9.68149766686123
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.1967],
        [1.7581],
        [0.0450]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0157, 0.2184, 1.9678, 0.0297, 0.0217, 4.3064]],

        [[0.0157, 0.2184, 1.9678, 0.0297, 0.0217, 4.3064]],

        [[0.0157, 0.2184, 1.9678, 0.0297, 0.0217, 4.3064]],

        [[0.0157, 0.2184, 1.9678, 0.0297, 0.0217, 4.3064]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0215, 0.7870, 7.0324, 0.1799], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0215, 0.7870, 7.0324, 0.1799], device='cuda:0')
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.816
Iter 2/2000 - Loss: 3.890
Iter 3/2000 - Loss: 3.744
Iter 4/2000 - Loss: 3.767
Iter 5/2000 - Loss: 3.801
Iter 6/2000 - Loss: 3.749
Iter 7/2000 - Loss: 3.692
Iter 8/2000 - Loss: 3.682
Iter 9/2000 - Loss: 3.695
Iter 10/2000 - Loss: 3.680
Iter 11/2000 - Loss: 3.632
Iter 12/2000 - Loss: 3.577
Iter 13/2000 - Loss: 3.534
Iter 14/2000 - Loss: 3.498
Iter 15/2000 - Loss: 3.447
Iter 16/2000 - Loss: 3.369
Iter 17/2000 - Loss: 3.265
Iter 18/2000 - Loss: 3.144
Iter 19/2000 - Loss: 3.008
Iter 20/2000 - Loss: 2.854
Iter 1981/2000 - Loss: -4.466
Iter 1982/2000 - Loss: -4.466
Iter 1983/2000 - Loss: -4.466
Iter 1984/2000 - Loss: -4.466
Iter 1985/2000 - Loss: -4.467
Iter 1986/2000 - Loss: -4.467
Iter 1987/2000 - Loss: -4.467
Iter 1988/2000 - Loss: -4.467
Iter 1989/2000 - Loss: -4.467
Iter 1990/2000 - Loss: -4.467
Iter 1991/2000 - Loss: -4.467
Iter 1992/2000 - Loss: -4.467
Iter 1993/2000 - Loss: -4.467
Iter 1994/2000 - Loss: -4.467
Iter 1995/2000 - Loss: -4.467
Iter 1996/2000 - Loss: -4.467
Iter 1997/2000 - Loss: -4.467
Iter 1998/2000 - Loss: -4.467
Iter 1999/2000 - Loss: -4.467
Iter 2000/2000 - Loss: -4.467
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0007],
        [0.0028],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[16.1195,  6.9358, 53.6130, 11.3582, 13.9483, 57.5009]],

        [[20.2006, 44.8344,  8.1663,  1.2447,  1.2206, 27.1361]],

        [[19.1139, 46.4609, 10.2565,  1.2391,  0.7222, 28.7541]],

        [[19.4875, 42.0601, 14.0737,  2.9118,  2.8107, 21.2586]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1010,  2.3975, 17.1053,  0.4821], device='cuda:0')
Estimated target variance: tensor([0.0215, 0.7870, 7.0324, 0.1799], device='cuda:0')
N: 60
Signal to noise ratio: tensor([14.9164, 59.2573, 77.7767, 37.7010], device='cuda:0')
Bound on condition number: tensor([ 13350.9326, 210686.7610, 362953.9359,  85282.7390], device='cuda:0')
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.011345834370910234, policy loss: 7.925410270406807
Experience 6, Iter 1, disc loss: 0.011369224136396641, policy loss: 8.167084831800121
Experience 6, Iter 2, disc loss: 0.01164945797802406, policy loss: 7.474717012330689
Experience 6, Iter 3, disc loss: 0.011618099681415004, policy loss: 7.04760602957257
Experience 6, Iter 4, disc loss: 0.01203895221608384, policy loss: 6.726370294076844
Experience 6, Iter 5, disc loss: 0.012032386620295437, policy loss: 7.044397442584691
Experience 6, Iter 6, disc loss: 0.012393061878338125, policy loss: 6.611536109222083
Experience 6, Iter 7, disc loss: 0.013245549692099071, policy loss: 6.092411943108608
Experience 6, Iter 8, disc loss: 0.014559837305023877, policy loss: 6.079119144967259
Experience 6, Iter 9, disc loss: 0.01667525484131733, policy loss: 5.736628486287196
Experience 6, Iter 10, disc loss: 0.0182033742103841, policy loss: 5.5784475146016055
Experience 6, Iter 11, disc loss: 0.023258950338237834, policy loss: 5.072191187386428
Experience 6, Iter 12, disc loss: 0.025448302253193833, policy loss: 6.0900643324724575
Experience 6, Iter 13, disc loss: 0.013583171792262198, policy loss: 6.922018046697063
Experience 6, Iter 14, disc loss: 0.01078791981846019, policy loss: 7.69925168280266
Experience 6, Iter 15, disc loss: 0.01202520628323487, policy loss: 6.731200503769764
Experience 6, Iter 16, disc loss: 0.01571713607556465, policy loss: 5.6388478601090934
Experience 6, Iter 17, disc loss: 0.018480251897739178, policy loss: 6.447844976103985
Experience 6, Iter 18, disc loss: 0.026431673569545566, policy loss: 5.071610575083965
Experience 6, Iter 19, disc loss: 0.028802498933404468, policy loss: 4.500368150657717
Experience 6, Iter 20, disc loss: 0.024012769067426182, policy loss: 5.0930338529930905
Experience 6, Iter 21, disc loss: 0.07708686484077554, policy loss: 4.863438719301353
Experience 6, Iter 22, disc loss: 0.05124305564027997, policy loss: 7.547244364901575
Experience 6, Iter 23, disc loss: 0.05121914376030548, policy loss: 4.075916613861487
Experience 6, Iter 24, disc loss: 0.02070332218298806, policy loss: 5.395592816945629
Experience 6, Iter 25, disc loss: 0.02117663873598577, policy loss: 5.824766438499361
Experience 6, Iter 26, disc loss: 0.04605551643363023, policy loss: 4.1949051049894726
Experience 6, Iter 27, disc loss: 0.23192783578749646, policy loss: 2.827455714320144
Experience 6, Iter 28, disc loss: 0.16719641642087127, policy loss: 3.2545126478934385
Experience 6, Iter 29, disc loss: 0.134106680732064, policy loss: 3.6534669266495707
Experience 6, Iter 30, disc loss: 0.0933187887283326, policy loss: 4.185219674895069
Experience 6, Iter 31, disc loss: 0.1264746288891881, policy loss: 3.8376068439497732
Experience 6, Iter 32, disc loss: 0.09258251949321414, policy loss: 4.463958392003085
Experience 6, Iter 33, disc loss: 0.09070441929788098, policy loss: 5.177480577029284
Experience 6, Iter 34, disc loss: 0.04817026052277081, policy loss: 6.5579424694788315
Experience 6, Iter 35, disc loss: 0.0563010894404196, policy loss: 8.337694814284484
Experience 6, Iter 36, disc loss: 0.06954110347176341, policy loss: 7.954125200340509
Experience 6, Iter 37, disc loss: 0.08286259994937417, policy loss: 6.831644248356422
Experience 6, Iter 38, disc loss: 0.09084897028718095, policy loss: 7.921998411790127
Experience 6, Iter 39, disc loss: 0.09447867780839182, policy loss: 8.354385693645046
Experience 6, Iter 40, disc loss: 0.09301004574462622, policy loss: 8.046033154490301
Experience 6, Iter 41, disc loss: 0.0866867804937529, policy loss: 13.292231427673002
Experience 6, Iter 42, disc loss: 0.07923694289067332, policy loss: 9.299139368914183
Experience 6, Iter 43, disc loss: 0.07108372550528334, policy loss: 7.268781100019401
Experience 6, Iter 44, disc loss: 0.06416781998919803, policy loss: 7.313789656781321
Experience 6, Iter 45, disc loss: 0.05185971395666805, policy loss: 10.986750933479213
Experience 6, Iter 46, disc loss: 0.052649531211256935, policy loss: 5.54905186287842
Experience 6, Iter 47, disc loss: 0.03869236861363284, policy loss: 7.1362193262412035
Experience 6, Iter 48, disc loss: 0.03138992717994916, policy loss: 8.120397697515704
Experience 6, Iter 49, disc loss: 0.029292404807193296, policy loss: 7.362969658722208
Experience 6, Iter 50, disc loss: 0.08280923916085065, policy loss: 4.274986933275855
Experience 6, Iter 51, disc loss: 0.06656803315299901, policy loss: 11.735092745259436
Experience 6, Iter 52, disc loss: 0.02152509358993316, policy loss: 9.811323608557608
Experience 6, Iter 53, disc loss: 0.02152268640278592, policy loss: 9.282842459839465
Experience 6, Iter 54, disc loss: 0.025373383354258296, policy loss: 8.381263781540781
Experience 6, Iter 55, disc loss: 0.015912029631203188, policy loss: 15.419376773604203
Experience 6, Iter 56, disc loss: 0.01373268508089031, policy loss: 14.677324391602621
Experience 6, Iter 57, disc loss: 0.022996282630427682, policy loss: 17.99833974946679
Experience 6, Iter 58, disc loss: 0.017103237623050903, policy loss: 14.095409867183438
Experience 6, Iter 59, disc loss: 0.011857167647607669, policy loss: 27.774458410261314
Experience 6, Iter 60, disc loss: 0.01127651923963056, policy loss: 31.856238048203632
Experience 6, Iter 61, disc loss: 0.012402699878671448, policy loss: 28.887831808890734
Experience 6, Iter 62, disc loss: 0.01131410033069683, policy loss: 19.81194690753468
Experience 6, Iter 63, disc loss: 0.03367898090494572, policy loss: 13.879118368600622
Experience 6, Iter 64, disc loss: 0.010281244585530766, policy loss: 14.229455212558058
Experience 6, Iter 65, disc loss: 0.017587501742668828, policy loss: 12.336140746418085
Experience 6, Iter 66, disc loss: 0.02501250456235352, policy loss: 12.508868222081626
Experience 6, Iter 67, disc loss: 0.013633636223462225, policy loss: 12.361705675196461
Experience 6, Iter 68, disc loss: 0.029699130246556717, policy loss: 12.026892116426911
Experience 6, Iter 69, disc loss: 0.010330376466653846, policy loss: 11.211475389644459
Experience 6, Iter 70, disc loss: 0.02013390610923669, policy loss: 12.750767855717541
Experience 6, Iter 71, disc loss: 0.010662621594903143, policy loss: 12.293152107203476
Experience 6, Iter 72, disc loss: 0.013478418150403477, policy loss: 12.326965724802172
Experience 6, Iter 73, disc loss: 0.010310245925971424, policy loss: 11.068346974788463
Experience 6, Iter 74, disc loss: 0.01052035859509151, policy loss: 10.091415161586422
Experience 6, Iter 75, disc loss: 0.011309775329170092, policy loss: 9.984725083143399
Experience 6, Iter 76, disc loss: 0.0092046827233648, policy loss: 10.177842450528285
Experience 6, Iter 77, disc loss: 0.011388815004045984, policy loss: 10.024107675894026
Experience 6, Iter 78, disc loss: 0.0106895776104768, policy loss: 9.261782067901294
Experience 6, Iter 79, disc loss: 0.009548916902063304, policy loss: 10.410373111909022
Experience 6, Iter 80, disc loss: 0.013368218451716521, policy loss: 10.883149664416045
Experience 6, Iter 81, disc loss: 0.009589598639236703, policy loss: 11.550976772947573
Experience 6, Iter 82, disc loss: 0.012935432396186053, policy loss: 10.55423628262257
Experience 6, Iter 83, disc loss: 0.028073628900194155, policy loss: 10.663137895014598
Experience 6, Iter 84, disc loss: 0.009093916610261448, policy loss: 10.741205199817319
Experience 6, Iter 85, disc loss: 0.00981433926850717, policy loss: 11.179708794364153
Experience 6, Iter 86, disc loss: 0.013917120618990522, policy loss: 10.53934801094595
Experience 6, Iter 87, disc loss: 0.018777406596763455, policy loss: 11.124134930444644
Experience 6, Iter 88, disc loss: 0.008801735747515808, policy loss: 11.44819071798587
Experience 6, Iter 89, disc loss: 0.008565867948634693, policy loss: 12.334682295724942
Experience 6, Iter 90, disc loss: 0.008971848721204656, policy loss: 12.964975202444403
Experience 6, Iter 91, disc loss: 0.007620562289473222, policy loss: 26.230840076408228
Experience 6, Iter 92, disc loss: 0.007539868866272314, policy loss: 26.665448172283355
Experience 6, Iter 93, disc loss: 0.007531703610577689, policy loss: 21.926331926139433
Experience 6, Iter 94, disc loss: 0.016714910757845362, policy loss: 15.926348519895631
Experience 6, Iter 95, disc loss: 0.011258668782683544, policy loss: 11.48516004979958
Experience 6, Iter 96, disc loss: 0.009935374989263694, policy loss: 20.45965303264869
Experience 6, Iter 97, disc loss: 0.05142460189365064, policy loss: 21.12688665797359
Experience 6, Iter 98, disc loss: 0.009732498208410743, policy loss: 14.860024207442203
Experience 6, Iter 99, disc loss: 0.010650899322828082, policy loss: 10.771766754953532
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0101],
        [0.1895],
        [1.5633],
        [0.0404]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0476, 0.3984, 1.7688, 0.0315, 0.0192, 4.7101]],

        [[0.0476, 0.3984, 1.7688, 0.0315, 0.0192, 4.7101]],

        [[0.0476, 0.3984, 1.7688, 0.0315, 0.0192, 4.7101]],

        [[0.0476, 0.3984, 1.7688, 0.0315, 0.0192, 4.7101]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0405, 0.7581, 6.2532, 0.1617], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0405, 0.7581, 6.2532, 0.1617], device='cuda:0')
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.991
Iter 2/2000 - Loss: 3.938
Iter 3/2000 - Loss: 3.892
Iter 4/2000 - Loss: 3.868
Iter 5/2000 - Loss: 3.856
Iter 6/2000 - Loss: 3.810
Iter 7/2000 - Loss: 3.761
Iter 8/2000 - Loss: 3.722
Iter 9/2000 - Loss: 3.672
Iter 10/2000 - Loss: 3.597
Iter 11/2000 - Loss: 3.509
Iter 12/2000 - Loss: 3.419
Iter 13/2000 - Loss: 3.323
Iter 14/2000 - Loss: 3.211
Iter 15/2000 - Loss: 3.082
Iter 16/2000 - Loss: 2.936
Iter 17/2000 - Loss: 2.776
Iter 18/2000 - Loss: 2.603
Iter 19/2000 - Loss: 2.415
Iter 20/2000 - Loss: 2.213
Iter 1981/2000 - Loss: -4.739
Iter 1982/2000 - Loss: -4.739
Iter 1983/2000 - Loss: -4.739
Iter 1984/2000 - Loss: -4.739
Iter 1985/2000 - Loss: -4.739
Iter 1986/2000 - Loss: -4.740
Iter 1987/2000 - Loss: -4.740
Iter 1988/2000 - Loss: -4.740
Iter 1989/2000 - Loss: -4.740
Iter 1990/2000 - Loss: -4.740
Iter 1991/2000 - Loss: -4.740
Iter 1992/2000 - Loss: -4.740
Iter 1993/2000 - Loss: -4.740
Iter 1994/2000 - Loss: -4.740
Iter 1995/2000 - Loss: -4.740
Iter 1996/2000 - Loss: -4.740
Iter 1997/2000 - Loss: -4.740
Iter 1998/2000 - Loss: -4.740
Iter 1999/2000 - Loss: -4.740
Iter 2000/2000 - Loss: -4.740
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0027],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[20.9195, 12.8566, 44.5352, 14.3516, 19.7628, 67.8518]],

        [[26.9568, 47.2123,  8.0200,  1.3158,  1.5555, 34.4105]],

        [[26.4196, 52.6531,  9.0821,  1.1453,  0.9113, 24.2334]],

        [[25.9300, 47.3905, 17.4367,  1.5595,  2.3929, 42.1350]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2598,  3.3831, 16.1330,  0.6070], device='cuda:0')
Estimated target variance: tensor([0.0405, 0.7581, 6.2532, 0.1617], device='cuda:0')
N: 70
Signal to noise ratio: tensor([24.1842, 71.0789, 77.2575, 41.5763], device='cuda:0')
Bound on condition number: tensor([ 40942.1813, 353655.4192, 417811.4903, 121002.1441], device='cuda:0')
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.007681150141083156, policy loss: 11.498931005723975
Experience 7, Iter 1, disc loss: 0.012818282642838684, policy loss: 17.237403796211687
Experience 7, Iter 2, disc loss: 0.007187715127566352, policy loss: 26.16478894241944
Experience 7, Iter 3, disc loss: 0.007159072225310592, policy loss: 23.323843262903225
Experience 7, Iter 4, disc loss: 0.008807771642378474, policy loss: 18.911043506351042
Experience 7, Iter 5, disc loss: 0.02369385896911157, policy loss: 13.090901263436283
Experience 7, Iter 6, disc loss: 0.008977850991373004, policy loss: 11.064526462730202
Experience 7, Iter 7, disc loss: 0.007624781733525456, policy loss: 9.6994986520606
Experience 7, Iter 8, disc loss: 0.007156330259665989, policy loss: 10.04738876619406
Experience 7, Iter 9, disc loss: 0.007097807052511823, policy loss: 10.404653590002697
Experience 7, Iter 10, disc loss: 0.007109041189411614, policy loss: 10.206966738597167
Experience 7, Iter 11, disc loss: 0.007840540087252464, policy loss: 10.0067680802584
Experience 7, Iter 12, disc loss: 0.0071090923058618975, policy loss: 9.780599938967178
Experience 7, Iter 13, disc loss: 0.0071543473101056175, policy loss: 9.748353800090658
Experience 7, Iter 14, disc loss: 0.007063638216093691, policy loss: 10.11734999606774
Experience 7, Iter 15, disc loss: 0.007199922850214675, policy loss: 11.305841992744224
Experience 7, Iter 16, disc loss: 0.007245071567199787, policy loss: 11.261215249321888
Experience 7, Iter 17, disc loss: 0.007100465213695856, policy loss: 13.747648829493833
Experience 7, Iter 18, disc loss: 0.010923543948805861, policy loss: 13.831178419220546
Experience 7, Iter 19, disc loss: 0.006999305585975409, policy loss: 15.35449511317871
Experience 7, Iter 20, disc loss: 0.010797960421335108, policy loss: 15.907149602045301
Experience 7, Iter 21, disc loss: 0.006919890542015223, policy loss: 17.291350766140088
Experience 7, Iter 22, disc loss: 0.007228711564673274, policy loss: 18.746405853306854
Experience 7, Iter 23, disc loss: 0.00877362518766393, policy loss: 20.363648230955214
Experience 7, Iter 24, disc loss: 0.007070242693235859, policy loss: 21.921171742868324
Experience 7, Iter 25, disc loss: 0.006999318055052858, policy loss: 20.028783278409364
Experience 7, Iter 26, disc loss: 0.006497038116597506, policy loss: 21.294709665139948
Experience 7, Iter 27, disc loss: 0.006467878624461182, policy loss: 23.00957828259518
Experience 7, Iter 28, disc loss: 0.006412738125272308, policy loss: 24.70879945330521
Experience 7, Iter 29, disc loss: 0.00819189512674627, policy loss: 24.5911562893718
Experience 7, Iter 30, disc loss: 0.006340372908949902, policy loss: 24.065274770164905
Experience 7, Iter 31, disc loss: 0.0064824862615607964, policy loss: 24.0304905980473
Experience 7, Iter 32, disc loss: 0.006277845634082825, policy loss: 25.339746776978632
Experience 7, Iter 33, disc loss: 0.006246660512566667, policy loss: 25.62668716428253
Experience 7, Iter 34, disc loss: 0.006224787599559807, policy loss: 25.701590867701775
Experience 7, Iter 35, disc loss: 0.006182490759749854, policy loss: 24.621684293697726
Experience 7, Iter 36, disc loss: 0.006163164856943559, policy loss: 26.363267976345252
Experience 7, Iter 37, disc loss: 0.0061161120243343, policy loss: 25.379207069887133
Experience 7, Iter 38, disc loss: 0.006083987547568594, policy loss: 26.02857462030707
Experience 7, Iter 39, disc loss: 0.006051134388091397, policy loss: 25.47897145461289
Experience 7, Iter 40, disc loss: 0.006019064769945929, policy loss: 25.55061814615984
Experience 7, Iter 41, disc loss: 0.005988545003337814, policy loss: 25.21357887570038
Experience 7, Iter 42, disc loss: 0.005954274316615536, policy loss: 27.87369059542923
Experience 7, Iter 43, disc loss: 0.005921840672601096, policy loss: 27.303119283996594
Experience 7, Iter 44, disc loss: 0.005889529940406382, policy loss: 25.82455889640702
Experience 7, Iter 45, disc loss: 0.005857594959672768, policy loss: 26.656678373427155
Experience 7, Iter 46, disc loss: 0.006020892559841285, policy loss: 26.151588199196468
Experience 7, Iter 47, disc loss: 0.005794388719764033, policy loss: 27.922248423778584
Experience 7, Iter 48, disc loss: 0.005763194408365053, policy loss: 26.979134264170497
Experience 7, Iter 49, disc loss: 0.005732144413060737, policy loss: 27.238032606257306
Experience 7, Iter 50, disc loss: 0.005707211329188882, policy loss: 26.55561136078547
Experience 7, Iter 51, disc loss: 0.005671504197027839, policy loss: 25.02363056472781
Experience 7, Iter 52, disc loss: 0.0056406170115611675, policy loss: 27.282127609283407
Experience 7, Iter 53, disc loss: 0.005610470182573307, policy loss: 27.042315770152676
Experience 7, Iter 54, disc loss: 0.005580688943691273, policy loss: 27.565019786215505
Experience 7, Iter 55, disc loss: 0.00555915536418186, policy loss: 25.979587087527612
Experience 7, Iter 56, disc loss: 0.005558949851965627, policy loss: 27.34798077076628
Experience 7, Iter 57, disc loss: 0.005493088369095002, policy loss: 27.295109660971285
Experience 7, Iter 58, disc loss: 0.0054642794022168395, policy loss: 26.151130912677743
Experience 7, Iter 59, disc loss: 0.005435818847838553, policy loss: 28.152582793776972
Experience 7, Iter 60, disc loss: 0.005407598615160066, policy loss: 25.78526417714325
Experience 7, Iter 61, disc loss: 0.005379734948167788, policy loss: 26.162310630771834
Experience 7, Iter 62, disc loss: 0.005353219694313731, policy loss: 27.748527874239937
Experience 7, Iter 63, disc loss: 0.00532464791623774, policy loss: 27.132173676443514
Experience 7, Iter 64, disc loss: 0.005297498746852179, policy loss: 28.466327574724865
Experience 7, Iter 65, disc loss: 0.005270545060603651, policy loss: 26.700364973965463
Experience 7, Iter 66, disc loss: 0.0052439082760938165, policy loss: 26.62514822938842
Experience 7, Iter 67, disc loss: 0.005235599692356672, policy loss: 27.794841895575907
Experience 7, Iter 68, disc loss: 0.005191111119761573, policy loss: 29.93249531479492
Experience 7, Iter 69, disc loss: 0.005165118378180777, policy loss: 26.04350566978343
Experience 7, Iter 70, disc loss: 0.0051393953161761, policy loss: 27.24412768765237
Experience 7, Iter 71, disc loss: 0.005131234431926469, policy loss: 26.124082803858215
Experience 7, Iter 72, disc loss: 0.005089243386063137, policy loss: 28.241300968511048
Experience 7, Iter 73, disc loss: 0.005063660256750837, policy loss: 27.139514215992456
Experience 7, Iter 74, disc loss: 0.005038876032446859, policy loss: 28.015158691688317
Experience 7, Iter 75, disc loss: 0.005014382061477334, policy loss: 29.483382189426344
Experience 7, Iter 76, disc loss: 0.0049941633652590585, policy loss: 26.417825700015285
Experience 7, Iter 77, disc loss: 0.004965955721115818, policy loss: 27.617280058186704
Experience 7, Iter 78, disc loss: 0.004942046152893848, policy loss: 27.97529680911284
Experience 7, Iter 79, disc loss: 0.004918414825501128, policy loss: 27.18307819658434
Experience 7, Iter 80, disc loss: 0.004894983958638132, policy loss: 26.739066370203734
Experience 7, Iter 81, disc loss: 0.004871680211511615, policy loss: 28.848367129335138
Experience 7, Iter 82, disc loss: 0.004855843917678935, policy loss: 27.96067753256812
Experience 7, Iter 83, disc loss: 0.004825774687889574, policy loss: 27.011223890890896
Experience 7, Iter 84, disc loss: 0.0048031425110657755, policy loss: 26.341849288280834
Experience 7, Iter 85, disc loss: 0.004780686490945429, policy loss: 27.314480336243797
Experience 7, Iter 86, disc loss: 0.004758504409029957, policy loss: 26.2446310986021
Experience 7, Iter 87, disc loss: 0.0047364195737996904, policy loss: 26.77646964863598
Experience 7, Iter 88, disc loss: 0.0047145298227744305, policy loss: 27.168079042761534
Experience 7, Iter 89, disc loss: 0.004693217286452875, policy loss: 26.128371148784574
Experience 7, Iter 90, disc loss: 0.004671345479590243, policy loss: 26.80627139067942
Experience 7, Iter 91, disc loss: 0.004650029290882994, policy loss: 27.234982626560395
Experience 7, Iter 92, disc loss: 0.004628930764220028, policy loss: 26.599129804293717
Experience 7, Iter 93, disc loss: 0.004607915793542495, policy loss: 27.985496109303234
Experience 7, Iter 94, disc loss: 0.00458708793195986, policy loss: 28.405257059247514
Experience 7, Iter 95, disc loss: 0.0045664537102509785, policy loss: 27.101976493743127
Experience 7, Iter 96, disc loss: 0.0045459890165785175, policy loss: 26.309789018436398
Experience 7, Iter 97, disc loss: 0.004525683380246152, policy loss: 28.003735911567155
Experience 7, Iter 98, disc loss: 0.004505534557996323, policy loss: 25.83864666576047
Experience 7, Iter 99, disc loss: 0.004485582228110026, policy loss: 27.350001542402424
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0104],
        [0.2091],
        [1.7737],
        [0.0386]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0506, 0.4141, 1.7217, 0.0322, 0.0173, 5.1576]],

        [[0.0506, 0.4141, 1.7217, 0.0322, 0.0173, 5.1576]],

        [[0.0506, 0.4141, 1.7217, 0.0322, 0.0173, 5.1576]],

        [[0.0506, 0.4141, 1.7217, 0.0322, 0.0173, 5.1576]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0417, 0.8364, 7.0950, 0.1543], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0417, 0.8364, 7.0950, 0.1543], device='cuda:0')
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.060
Iter 2/2000 - Loss: 4.061
Iter 3/2000 - Loss: 3.960
Iter 4/2000 - Loss: 3.951
Iter 5/2000 - Loss: 3.935
Iter 6/2000 - Loss: 3.866
Iter 7/2000 - Loss: 3.798
Iter 8/2000 - Loss: 3.749
Iter 9/2000 - Loss: 3.698
Iter 10/2000 - Loss: 3.621
Iter 11/2000 - Loss: 3.523
Iter 12/2000 - Loss: 3.418
Iter 13/2000 - Loss: 3.312
Iter 14/2000 - Loss: 3.200
Iter 15/2000 - Loss: 3.075
Iter 16/2000 - Loss: 2.931
Iter 17/2000 - Loss: 2.770
Iter 18/2000 - Loss: 2.595
Iter 19/2000 - Loss: 2.411
Iter 20/2000 - Loss: 2.217
Iter 1981/2000 - Loss: -4.865
Iter 1982/2000 - Loss: -4.865
Iter 1983/2000 - Loss: -4.865
Iter 1984/2000 - Loss: -4.865
Iter 1985/2000 - Loss: -4.865
Iter 1986/2000 - Loss: -4.866
Iter 1987/2000 - Loss: -4.866
Iter 1988/2000 - Loss: -4.866
Iter 1989/2000 - Loss: -4.866
Iter 1990/2000 - Loss: -4.866
Iter 1991/2000 - Loss: -4.866
Iter 1992/2000 - Loss: -4.866
Iter 1993/2000 - Loss: -4.866
Iter 1994/2000 - Loss: -4.866
Iter 1995/2000 - Loss: -4.866
Iter 1996/2000 - Loss: -4.866
Iter 1997/2000 - Loss: -4.866
Iter 1998/2000 - Loss: -4.866
Iter 1999/2000 - Loss: -4.866
Iter 2000/2000 - Loss: -4.866
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0027],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[19.7478, 12.2158, 41.5527, 14.5314, 19.5738, 63.7218]],

        [[28.1703, 47.5325,  7.8266,  1.3858,  1.6361, 36.1639]],

        [[26.8278, 48.8082,  8.1272,  1.0912,  0.9302, 28.4340]],

        [[24.0329, 49.4129, 18.2677,  1.6056,  2.4191, 40.4066]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2161,  3.6190, 17.3332,  0.6042], device='cuda:0')
Estimated target variance: tensor([0.0417, 0.8364, 7.0950, 0.1543], device='cuda:0')
N: 80
Signal to noise ratio: tensor([21.4602, 75.4648, 80.5145, 39.7373], device='cuda:0')
Bound on condition number: tensor([ 36844.1632, 455596.1025, 518607.8086, 126325.2234], device='cuda:0')
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.00446570736823669, policy loss: 26.14530954734399
Experience 8, Iter 1, disc loss: 0.004446042901196613, policy loss: 27.02808089402592
Experience 8, Iter 2, disc loss: 0.004426494996544718, policy loss: 26.839553350449336
Experience 8, Iter 3, disc loss: 0.0044072014471034275, policy loss: 26.35559515284094
Experience 8, Iter 4, disc loss: 0.004387824212323899, policy loss: 26.682442526064364
Experience 8, Iter 5, disc loss: 0.004368740583658167, policy loss: 25.92268523031544
Experience 8, Iter 6, disc loss: 0.004349720145702438, policy loss: 26.833737973455392
Experience 8, Iter 7, disc loss: 0.0043308724874622125, policy loss: 27.977575225643953
Experience 8, Iter 8, disc loss: 0.004328646895991716, policy loss: 27.29519556717196
Experience 8, Iter 9, disc loss: 0.00430071510677831, policy loss: 27.796185609161398
Experience 8, Iter 10, disc loss: 0.004275131669115631, policy loss: 26.213352711000137
Experience 8, Iter 11, disc loss: 0.004256809838334592, policy loss: 27.89317332290736
Experience 8, Iter 12, disc loss: 0.004238641900459649, policy loss: 26.420376319681274
Experience 8, Iter 13, disc loss: 0.004220599141971387, policy loss: 27.781008764085627
Experience 8, Iter 14, disc loss: 0.004202686309601184, policy loss: 26.56639158931288
Experience 8, Iter 15, disc loss: 0.004184902671549777, policy loss: 27.518483675167715
Experience 8, Iter 16, disc loss: 0.004167226413246221, policy loss: 26.67045461217306
Experience 8, Iter 17, disc loss: 0.004149736599313274, policy loss: 27.72208804095642
Experience 8, Iter 18, disc loss: 0.004132252178320518, policy loss: 27.900348482038517
Experience 8, Iter 19, disc loss: 0.004118250815178734, policy loss: 27.268684994394906
Experience 8, Iter 20, disc loss: 0.004097735435132813, policy loss: 28.86650494162867
Experience 8, Iter 21, disc loss: 0.004080673669354107, policy loss: 26.59850531709869
Experience 8, Iter 22, disc loss: 0.00406369487454672, policy loss: 28.139557563895682
Experience 8, Iter 23, disc loss: 0.004046857690323675, policy loss: 26.655565198980455
Experience 8, Iter 24, disc loss: 0.0040301466158742475, policy loss: 27.26386667943322
Experience 8, Iter 25, disc loss: 0.004013472447509794, policy loss: 28.179559239116834
Experience 8, Iter 26, disc loss: 0.003997002704351868, policy loss: 27.759092561446337
Experience 8, Iter 27, disc loss: 0.003980576993890092, policy loss: 27.90733653435168
Experience 8, Iter 28, disc loss: 0.003964308250924111, policy loss: 27.44657685018783
Experience 8, Iter 29, disc loss: 0.003948142746336829, policy loss: 26.855068750969473
Experience 8, Iter 30, disc loss: 0.003932122082337671, policy loss: 27.96690485951757
Experience 8, Iter 31, disc loss: 0.003916279935648289, policy loss: 27.493279220088922
Experience 8, Iter 32, disc loss: 0.003900284275751066, policy loss: 28.601970567881963
Experience 8, Iter 33, disc loss: 0.003884537782266512, policy loss: 28.48420914641971
Experience 8, Iter 34, disc loss: 0.003868904232681897, policy loss: 26.890867973129165
Experience 8, Iter 35, disc loss: 0.0038533610273644645, policy loss: 27.3885592783718
Experience 8, Iter 36, disc loss: 0.003837928758896605, policy loss: 27.150349083281647
Experience 8, Iter 37, disc loss: 0.0038225962401862807, policy loss: 27.40662261472722
Experience 8, Iter 38, disc loss: 0.0038074073603937136, policy loss: 27.626272277283967
Experience 8, Iter 39, disc loss: 0.003792223976037417, policy loss: 27.442421540342632
Experience 8, Iter 40, disc loss: 0.003777196274020195, policy loss: 27.552848200986045
Experience 8, Iter 41, disc loss: 0.00376225115556714, policy loss: 26.364075433993044
Experience 8, Iter 42, disc loss: 0.0037473844597565808, policy loss: 26.710056886680363
Experience 8, Iter 43, disc loss: 0.0037326330989796487, policy loss: 26.644226855205844
Experience 8, Iter 44, disc loss: 0.0037179557670639477, policy loss: 27.18278311084114
Experience 8, Iter 45, disc loss: 0.003703474457764993, policy loss: 26.745380977285823
Experience 8, Iter 46, disc loss: 0.0036888949372451034, policy loss: 27.130672555037954
Experience 8, Iter 47, disc loss: 0.0036744962771981526, policy loss: 27.904675711601932
Experience 8, Iter 48, disc loss: 0.003660195480547314, policy loss: 28.176831548958745
Experience 8, Iter 49, disc loss: 0.0036459739757284636, policy loss: 28.333483580954127
Experience 8, Iter 50, disc loss: 0.0036321871012800623, policy loss: 26.45712414213216
Experience 8, Iter 51, disc loss: 0.0036177898388592413, policy loss: 28.06468370357075
Experience 8, Iter 52, disc loss: 0.003603832390054323, policy loss: 28.021263034832465
Experience 8, Iter 53, disc loss: 0.0035899804231790955, policy loss: 25.916555591989617
Experience 8, Iter 54, disc loss: 0.003576175151492919, policy loss: 27.25092257730254
Experience 8, Iter 55, disc loss: 0.0035624638025464803, policy loss: 28.118053378823802
Experience 8, Iter 56, disc loss: 0.0035488435174847394, policy loss: 26.157086985615884
Experience 8, Iter 57, disc loss: 0.0035353084124130013, policy loss: 26.774755391607282
Experience 8, Iter 58, disc loss: 0.0035218477085880787, policy loss: 26.79126778492372
Experience 8, Iter 59, disc loss: 0.003508975156877987, policy loss: 26.03915063153007
Experience 8, Iter 60, disc loss: 0.003495319776813019, policy loss: 27.703911024843567
Experience 8, Iter 61, disc loss: 0.003481990132991668, policy loss: 26.122895190936866
Experience 8, Iter 62, disc loss: 0.0034689103393122263, policy loss: 26.70109456556706
Experience 8, Iter 63, disc loss: 0.003455764123863178, policy loss: 26.982449368745826
Experience 8, Iter 64, disc loss: 0.003442811299457964, policy loss: 25.931287209952416
Experience 8, Iter 65, disc loss: 0.0034298623896733657, policy loss: 27.10048975199236
Experience 8, Iter 66, disc loss: 0.003417064574729764, policy loss: 26.828248774417464
Experience 8, Iter 67, disc loss: 0.003404283630226011, policy loss: 27.2669527916415
Experience 8, Iter 68, disc loss: 0.0034083690516822297, policy loss: 26.877683237940424
Experience 8, Iter 69, disc loss: 0.0033790245247270734, policy loss: 27.69152948968503
Experience 8, Iter 70, disc loss: 0.0033664552262464196, policy loss: 27.508386782025937
Experience 8, Iter 71, disc loss: 0.0033539971470862625, policy loss: 27.560568403431255
Experience 8, Iter 72, disc loss: 0.0033416132122516314, policy loss: 27.619530822632147
Experience 8, Iter 73, disc loss: 0.0033292829027478546, policy loss: 27.420616347625383
Experience 8, Iter 74, disc loss: 0.0033170238026792452, policy loss: 27.81137108268136
Experience 8, Iter 75, disc loss: 0.0033048217403363464, policy loss: 26.93083536223526
Experience 8, Iter 76, disc loss: 0.0032926720434661894, policy loss: 26.40055823990776
Experience 8, Iter 77, disc loss: 0.0032805513057144217, policy loss: 27.539704740045302
Experience 8, Iter 78, disc loss: 0.0032685226488239666, policy loss: 27.301723213333425
Experience 8, Iter 79, disc loss: 0.003257023484129348, policy loss: 26.944328762386988
Experience 8, Iter 80, disc loss: 0.003244584435824687, policy loss: 27.459904622294648
Experience 8, Iter 81, disc loss: 0.0032327166535280815, policy loss: 27.491005436004837
Experience 8, Iter 82, disc loss: 0.0032208986841202767, policy loss: 26.23263171051216
Experience 8, Iter 83, disc loss: 0.003209557780171496, policy loss: 26.55206626315732
Experience 8, Iter 84, disc loss: 0.003197507006115368, policy loss: 27.203735725349137
Experience 8, Iter 85, disc loss: 0.0031861351466178876, policy loss: 26.409277121027873
Experience 8, Iter 86, disc loss: 0.003174308514878415, policy loss: 27.29906756442913
Experience 8, Iter 87, disc loss: 0.003162799708193511, policy loss: 27.75800889645496
Experience 8, Iter 88, disc loss: 0.0033558023734063715, policy loss: 26.63847759023885
Experience 8, Iter 89, disc loss: 0.003139993310621147, policy loss: 27.270697747979884
Experience 8, Iter 90, disc loss: 0.0031287339265788677, policy loss: 27.909714933745157
Experience 8, Iter 91, disc loss: 0.003153136629100866, policy loss: 27.181411458091382
Experience 8, Iter 92, disc loss: 0.003106387882878908, policy loss: 27.761801604417375
Experience 8, Iter 93, disc loss: 0.003095305109885305, policy loss: 28.240758740838757
Experience 8, Iter 94, disc loss: 0.0030842812720901455, policy loss: 27.856072552396718
Experience 8, Iter 95, disc loss: 0.0030737203603148884, policy loss: 24.894263624128378
Experience 8, Iter 96, disc loss: 0.003062369919997645, policy loss: 27.308011096801835
Experience 8, Iter 97, disc loss: 0.00305151173913735, policy loss: 27.764321795941413
Experience 8, Iter 98, disc loss: 0.0030756386099724305, policy loss: 26.70562825813944
Experience 8, Iter 99, disc loss: 0.0030300382111045953, policy loss: 27.349997931779157
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0104],
        [0.2262],
        [1.9452],
        [0.0372]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0512, 0.4238, 1.6819, 0.0317, 0.0157, 5.5074]],

        [[0.0512, 0.4238, 1.6819, 0.0317, 0.0157, 5.5074]],

        [[0.0512, 0.4238, 1.6819, 0.0317, 0.0157, 5.5074]],

        [[0.0512, 0.4238, 1.6819, 0.0317, 0.0157, 5.5074]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0417, 0.9049, 7.7810, 0.1488], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0417, 0.9049, 7.7810, 0.1488], device='cuda:0')
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.123
Iter 2/2000 - Loss: 4.139
Iter 3/2000 - Loss: 4.034
Iter 4/2000 - Loss: 4.009
Iter 5/2000 - Loss: 3.992
Iter 6/2000 - Loss: 3.927
Iter 7/2000 - Loss: 3.855
Iter 8/2000 - Loss: 3.794
Iter 9/2000 - Loss: 3.732
Iter 10/2000 - Loss: 3.652
Iter 11/2000 - Loss: 3.552
Iter 12/2000 - Loss: 3.443
Iter 13/2000 - Loss: 3.329
Iter 14/2000 - Loss: 3.210
Iter 15/2000 - Loss: 3.081
Iter 16/2000 - Loss: 2.937
Iter 17/2000 - Loss: 2.776
Iter 18/2000 - Loss: 2.601
Iter 19/2000 - Loss: 2.414
Iter 20/2000 - Loss: 2.215
Iter 1981/2000 - Loss: -5.207
Iter 1982/2000 - Loss: -5.207
Iter 1983/2000 - Loss: -5.207
Iter 1984/2000 - Loss: -5.207
Iter 1985/2000 - Loss: -5.207
Iter 1986/2000 - Loss: -5.207
Iter 1987/2000 - Loss: -5.207
Iter 1988/2000 - Loss: -5.207
Iter 1989/2000 - Loss: -5.207
Iter 1990/2000 - Loss: -5.207
Iter 1991/2000 - Loss: -5.207
Iter 1992/2000 - Loss: -5.207
Iter 1993/2000 - Loss: -5.208
Iter 1994/2000 - Loss: -5.208
Iter 1995/2000 - Loss: -5.208
Iter 1996/2000 - Loss: -5.208
Iter 1997/2000 - Loss: -5.208
Iter 1998/2000 - Loss: -5.208
Iter 1999/2000 - Loss: -5.208
Iter 2000/2000 - Loss: -5.208
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0023],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[19.2145, 12.2552, 41.4241, 14.6736, 18.5615, 63.9389]],

        [[28.0084, 50.7526,  8.3256,  1.4537,  1.2491, 34.3938]],

        [[29.3669, 54.9098,  7.9277,  1.0938,  0.9230, 27.8551]],

        [[23.5975, 47.7313, 17.8259,  1.4911,  2.4414, 40.9990]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2068,  3.1817, 16.8615,  0.5585], device='cuda:0')
Estimated target variance: tensor([0.0417, 0.9049, 7.7810, 0.1488], device='cuda:0')
N: 90
Signal to noise ratio: tensor([21.5874, 67.1757, 85.5032, 38.3090], device='cuda:0')
Bound on condition number: tensor([ 41942.5689, 406132.6291, 657973.1540, 132083.1060], device='cuda:0')
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.003019275241743646, policy loss: 28.20286830140433
Experience 9, Iter 1, disc loss: 0.003008651012735973, policy loss: 27.897045418312473
Experience 9, Iter 2, disc loss: 0.0029980842744510173, policy loss: 28.86434617422634
Experience 9, Iter 3, disc loss: 0.0029875752909178624, policy loss: 28.981326962996043
Experience 9, Iter 4, disc loss: 0.0029772240023217184, policy loss: 29.10908410453757
Experience 9, Iter 5, disc loss: 0.002966716274305848, policy loss: 29.10602108625296
Experience 9, Iter 6, disc loss: 0.0029563668584314936, policy loss: 28.07645962990408
Experience 9, Iter 7, disc loss: 0.0029599735470089137, policy loss: 26.53851859649773
Experience 9, Iter 8, disc loss: 0.0029358139188851596, policy loss: 26.78712020529968
Experience 9, Iter 9, disc loss: 0.002925463653941806, policy loss: 27.75944722080043
Experience 9, Iter 10, disc loss: 0.002915157629022189, policy loss: 27.438497926610623
Experience 9, Iter 11, disc loss: 0.0029048845915110354, policy loss: 26.328310766072136
Experience 9, Iter 12, disc loss: 0.0028945585239610635, policy loss: 27.452169908951817
Experience 9, Iter 13, disc loss: 0.002884389113284152, policy loss: 27.09627524630362
Experience 9, Iter 14, disc loss: 0.002873889021770643, policy loss: 26.29233480964372
Experience 9, Iter 15, disc loss: 0.002863546876941617, policy loss: 27.216530361262755
Experience 9, Iter 16, disc loss: 0.002853207827872164, policy loss: 26.43098070462613
Experience 9, Iter 17, disc loss: 0.0028429068405082204, policy loss: 27.45705769239339
Experience 9, Iter 18, disc loss: 0.0028327542892537705, policy loss: 27.69879786969031
Experience 9, Iter 19, disc loss: 0.0028223396225523236, policy loss: 28.274708908474345
Experience 9, Iter 20, disc loss: 0.00281201662962559, policy loss: 27.756607082968223
Experience 9, Iter 21, disc loss: 0.0028015935896352566, policy loss: 26.49447641439539
Experience 9, Iter 22, disc loss: 0.002792373252678549, policy loss: 26.466711020322663
Experience 9, Iter 23, disc loss: 0.002780395937933512, policy loss: 27.038836482144625
Experience 9, Iter 24, disc loss: 0.0027697930157767362, policy loss: 26.902657854303456
Experience 9, Iter 25, disc loss: 0.0027581170457005974, policy loss: 26.07453155719078
Experience 9, Iter 26, disc loss: 0.0027471984517767408, policy loss: 26.062019766958656
Experience 9, Iter 27, disc loss: 0.0027349933977294486, policy loss: 25.85622511187927
Experience 9, Iter 28, disc loss: 0.002722663681767884, policy loss: 26.53922877993353
Experience 9, Iter 29, disc loss: 0.002709777095702953, policy loss: 26.27984282525529
Experience 9, Iter 30, disc loss: 0.0026960336828136485, policy loss: 26.614624619975068
Experience 9, Iter 31, disc loss: 0.0026812041149687324, policy loss: 26.1237777799615
Experience 9, Iter 32, disc loss: 0.0026652519705858023, policy loss: 25.75856241038918
Experience 9, Iter 33, disc loss: 0.0026479864700541212, policy loss: 26.664082176084904
Experience 9, Iter 34, disc loss: 0.002629589896987269, policy loss: 25.90595133793358
Experience 9, Iter 35, disc loss: 0.002610280808489525, policy loss: 26.725410155022907
Experience 9, Iter 36, disc loss: 0.0025897613149694857, policy loss: 26.138671721265684
Experience 9, Iter 37, disc loss: 0.002566732550820913, policy loss: 27.714973484584647
Experience 9, Iter 38, disc loss: 0.002541987136709504, policy loss: 26.42009978179374
Experience 9, Iter 39, disc loss: 0.002514948714721554, policy loss: 25.760050339230517
Experience 9, Iter 40, disc loss: 0.0024855209620428084, policy loss: 26.512436964633615
Experience 9, Iter 41, disc loss: 0.0024540828139014864, policy loss: 25.934316892218092
Experience 9, Iter 42, disc loss: 0.002421471584037411, policy loss: 26.033030982078408
Experience 9, Iter 43, disc loss: 0.002388282244685816, policy loss: 27.875534975948796
Experience 9, Iter 44, disc loss: 0.002354708229322438, policy loss: 26.365569374207986
Experience 9, Iter 45, disc loss: 0.002320923228665649, policy loss: 25.820428158079483
Experience 9, Iter 46, disc loss: 0.0022870916044013803, policy loss: 25.74943180402734
Experience 9, Iter 47, disc loss: 0.0022533223002931925, policy loss: 25.539258786338674
Experience 9, Iter 48, disc loss: 0.002219712873230622, policy loss: 26.51973922961153
Experience 9, Iter 49, disc loss: 0.002186383366298452, policy loss: 26.407732611613817
Experience 9, Iter 50, disc loss: 0.0021533930244174615, policy loss: 26.480097477261495
Experience 9, Iter 51, disc loss: 0.0021208051201891593, policy loss: 26.61223641852713
Experience 9, Iter 52, disc loss: 0.0020886656915636895, policy loss: 27.717078535209858
Experience 9, Iter 53, disc loss: 0.0020570840914375913, policy loss: 25.774263141383646
Experience 9, Iter 54, disc loss: 0.0020258852064119763, policy loss: 26.83907204728272
Experience 9, Iter 55, disc loss: 0.0019952949778738213, policy loss: 26.640109629414777
Experience 9, Iter 56, disc loss: 0.0019652623732318735, policy loss: 26.653421685923657
Experience 9, Iter 57, disc loss: 0.0019358012409604606, policy loss: 26.1199368628739
Experience 9, Iter 58, disc loss: 0.0019069175637915826, policy loss: 27.041294402662373
Experience 9, Iter 59, disc loss: 0.0018799762404330552, policy loss: 25.65056760590265
Experience 9, Iter 60, disc loss: 0.0018508860361058236, policy loss: 26.63025790646451
Experience 9, Iter 61, disc loss: 0.0018237363084956197, policy loss: 27.091036396559062
Experience 9, Iter 62, disc loss: 0.0017971478208980872, policy loss: 26.918368892694883
Experience 9, Iter 63, disc loss: 0.0017711244567656882, policy loss: 25.286743907377755
Experience 9, Iter 64, disc loss: 0.0017456534326514986, policy loss: 26.88494244330512
Experience 9, Iter 65, disc loss: 0.002137899204417553, policy loss: 25.696405971911812
Experience 9, Iter 66, disc loss: 0.0016967122924384974, policy loss: 26.79196510910996
Experience 9, Iter 67, disc loss: 0.0016732060529115929, policy loss: 27.058182680770187
Experience 9, Iter 68, disc loss: 0.0016500989312605483, policy loss: 26.159740692171404
Experience 9, Iter 69, disc loss: 0.001627472467393246, policy loss: 25.869024022527604
Experience 9, Iter 70, disc loss: 0.0016054029003838759, policy loss: 26.34125872246623
Experience 9, Iter 71, disc loss: 0.0015835963098156567, policy loss: 27.106439728777193
Experience 9, Iter 72, disc loss: 0.0015623529034749306, policy loss: 26.481349807582788
Experience 9, Iter 73, disc loss: 0.0015414157911800908, policy loss: 26.201163929547995
Experience 9, Iter 74, disc loss: 0.001520959455613461, policy loss: 27.129352047028505
Experience 9, Iter 75, disc loss: 0.0015009093846738203, policy loss: 26.193750891608914
Experience 9, Iter 76, disc loss: 0.001481255183484237, policy loss: 26.305499035160544
Experience 9, Iter 77, disc loss: 0.0014620539239321643, policy loss: 25.49126750963407
Experience 9, Iter 78, disc loss: 0.0016072558714112675, policy loss: 25.140617788314984
Experience 9, Iter 79, disc loss: 0.0014247461441976798, policy loss: 27.058978678197654
Experience 9, Iter 80, disc loss: 0.0014066871977847582, policy loss: 27.116868736823413
Experience 9, Iter 81, disc loss: 0.0013889876976765979, policy loss: 26.947900494715135
Experience 9, Iter 82, disc loss: 0.0013716058689955185, policy loss: 25.800391857855974
Experience 9, Iter 83, disc loss: 0.001354544324772294, policy loss: 25.921489961775297
Experience 9, Iter 84, disc loss: 0.001337791309849169, policy loss: 26.74646373621499
Experience 9, Iter 85, disc loss: 0.0013213413284196992, policy loss: 26.88906368585713
Experience 9, Iter 86, disc loss: 0.0013057753017742303, policy loss: 25.928737298463027
Experience 9, Iter 87, disc loss: 0.0012893230519033893, policy loss: 26.068202214542257
Experience 9, Iter 88, disc loss: 0.0012737423551883074, policy loss: 26.07638458436357
Experience 9, Iter 89, disc loss: 0.0012584595055067217, policy loss: 25.490549280655003
Experience 9, Iter 90, disc loss: 0.0012433936308291888, policy loss: 26.38141161716962
Experience 9, Iter 91, disc loss: 0.0012286178412052498, policy loss: 26.08243261067311
Experience 9, Iter 92, disc loss: 0.0012141002628963961, policy loss: 26.22065409796395
Experience 9, Iter 93, disc loss: 0.0011998277099553177, policy loss: 26.91637458741488
Experience 9, Iter 94, disc loss: 0.0011858047541961636, policy loss: 26.84825354779651
Experience 9, Iter 95, disc loss: 0.0011720187320497432, policy loss: 26.309350091474286
Experience 9, Iter 96, disc loss: 0.0011584639401360257, policy loss: 25.438851184312547
Experience 9, Iter 97, disc loss: 0.0011451390113034624, policy loss: 27.954249068696917
Experience 9, Iter 98, disc loss: 0.0011320364378795886, policy loss: 26.010406251994937
Experience 9, Iter 99, disc loss: 0.0011191518644828435, policy loss: 25.867055714503245
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0114],
        [0.2311],
        [2.0087],
        [0.0361]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0503, 0.4550, 1.6401, 0.0332, 0.0150, 5.7852]],

        [[0.0503, 0.4550, 1.6401, 0.0332, 0.0150, 5.7852]],

        [[0.0503, 0.4550, 1.6401, 0.0332, 0.0150, 5.7852]],

        [[0.0503, 0.4550, 1.6401, 0.0332, 0.0150, 5.7852]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0457, 0.9242, 8.0346, 0.1443], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0457, 0.9242, 8.0346, 0.1443], device='cuda:0')
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.182
Iter 2/2000 - Loss: 4.179
Iter 3/2000 - Loss: 4.094
Iter 4/2000 - Loss: 4.070
Iter 5/2000 - Loss: 4.046
Iter 6/2000 - Loss: 3.984
Iter 7/2000 - Loss: 3.920
Iter 8/2000 - Loss: 3.862
Iter 9/2000 - Loss: 3.799
Iter 10/2000 - Loss: 3.717
Iter 11/2000 - Loss: 3.622
Iter 12/2000 - Loss: 3.521
Iter 13/2000 - Loss: 3.417
Iter 14/2000 - Loss: 3.303
Iter 15/2000 - Loss: 3.175
Iter 16/2000 - Loss: 3.030
Iter 17/2000 - Loss: 2.870
Iter 18/2000 - Loss: 2.697
Iter 19/2000 - Loss: 2.510
Iter 20/2000 - Loss: 2.310
Iter 1981/2000 - Loss: -5.338
Iter 1982/2000 - Loss: -5.338
Iter 1983/2000 - Loss: -5.338
Iter 1984/2000 - Loss: -5.338
Iter 1985/2000 - Loss: -5.338
Iter 1986/2000 - Loss: -5.338
Iter 1987/2000 - Loss: -5.338
Iter 1988/2000 - Loss: -5.338
Iter 1989/2000 - Loss: -5.338
Iter 1990/2000 - Loss: -5.338
Iter 1991/2000 - Loss: -5.338
Iter 1992/2000 - Loss: -5.338
Iter 1993/2000 - Loss: -5.338
Iter 1994/2000 - Loss: -5.338
Iter 1995/2000 - Loss: -5.338
Iter 1996/2000 - Loss: -5.338
Iter 1997/2000 - Loss: -5.338
Iter 1998/2000 - Loss: -5.339
Iter 1999/2000 - Loss: -5.339
Iter 2000/2000 - Loss: -5.339
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0020],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[19.7907, 12.2441, 43.3954, 14.4230, 17.4643, 62.6840]],

        [[27.6081, 53.5722,  8.4870,  1.3158,  1.3236, 35.9434]],

        [[28.9632, 52.0334,  7.7942,  0.9629,  0.8394, 19.0529]],

        [[21.2793, 43.4946, 18.8213,  2.7896,  2.4326, 28.1481]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1992,  3.1957, 11.9814,  0.6671], device='cuda:0')
Estimated target variance: tensor([0.0457, 0.9242, 8.0346, 0.1443], device='cuda:0')
N: 100
Signal to noise ratio: tensor([21.8548, 67.4037, 77.8279, 39.8598], device='cuda:0')
Bound on condition number: tensor([ 47764.0795, 454326.5033, 605719.6105, 158881.5567], device='cuda:0')
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.0011064795910419314, policy loss: 25.52890307084205
Experience 10, Iter 1, disc loss: 0.0010940141313476176, policy loss: 25.850008685507994
Experience 10, Iter 2, disc loss: 0.0011191471527259593, policy loss: 24.787762306807643
Experience 10, Iter 3, disc loss: 0.0010696901811581404, policy loss: 25.425641454395752
Experience 10, Iter 4, disc loss: 0.001057829265994965, policy loss: 25.41838598725674
Experience 10, Iter 5, disc loss: 0.0010461743800521887, policy loss: 25.339084856994248
Experience 10, Iter 6, disc loss: 0.0010348087707938228, policy loss: 25.439207301694765
Experience 10, Iter 7, disc loss: 0.0010233397604752124, policy loss: 25.736561979832153
Experience 10, Iter 8, disc loss: 0.0010122029437354828, policy loss: 24.810151642399944
Experience 10, Iter 9, disc loss: 0.001001246029958264, policy loss: 25.129740789923034
Experience 10, Iter 10, disc loss: 0.0009904516382535974, policy loss: 26.21317906945405
Experience 10, Iter 11, disc loss: 0.0009798816434854374, policy loss: 25.428313382869703
Experience 10, Iter 12, disc loss: 0.0009693704521605375, policy loss: 25.935693866062035
Experience 10, Iter 13, disc loss: 0.0009591224587946807, policy loss: 25.644467070911624
Experience 10, Iter 14, disc loss: 0.0009489251834060105, policy loss: 25.196411431812656
Experience 10, Iter 15, disc loss: 0.0012067347963717868, policy loss: 25.165951486546412
Experience 10, Iter 16, disc loss: 0.0009293372667234277, policy loss: 25.021060083114385
Experience 10, Iter 17, disc loss: 0.0009198622947319613, policy loss: 26.227955713954515
Experience 10, Iter 18, disc loss: 0.0009105023859556048, policy loss: 25.99581974345684
Experience 10, Iter 19, disc loss: 0.0009014374125478273, policy loss: 24.57050770931225
Experience 10, Iter 20, disc loss: 0.0008921462972074366, policy loss: 24.678687917040655
Experience 10, Iter 21, disc loss: 0.0008831371029677108, policy loss: 26.05727272203047
Experience 10, Iter 22, disc loss: 0.0008743275645276546, policy loss: 25.479883076503828
Experience 10, Iter 23, disc loss: 0.0008654813881669588, policy loss: 24.98240260975255
Experience 10, Iter 24, disc loss: 0.0008568267121072886, policy loss: 25.604941937930562
Experience 10, Iter 25, disc loss: 0.0008482789818560448, policy loss: 25.79784002463888
Experience 10, Iter 26, disc loss: 0.0008401094013504136, policy loss: 25.819916108427993
Experience 10, Iter 27, disc loss: 0.0008315288044054946, policy loss: 26.19293157076136
Experience 10, Iter 28, disc loss: 0.0008233226281583112, policy loss: 25.59118732583753
Experience 10, Iter 29, disc loss: 0.000815238323515846, policy loss: 25.79873596118069
Experience 10, Iter 30, disc loss: 0.0008072335663073983, policy loss: 25.47754918701296
Experience 10, Iter 31, disc loss: 0.0007993449210848657, policy loss: 26.20302201258929
Experience 10, Iter 32, disc loss: 0.0008018989391604016, policy loss: 25.553395356266904
Experience 10, Iter 33, disc loss: 0.00078390064307066, policy loss: 25.40869316418319
Experience 10, Iter 34, disc loss: 0.0007763310893695505, policy loss: 26.753981853783138
Experience 10, Iter 35, disc loss: 0.000768910200494557, policy loss: 25.014852580172985
Experience 10, Iter 36, disc loss: 0.0007614962605552332, policy loss: 27.199953721484277
Experience 10, Iter 37, disc loss: 0.0007542798994860279, policy loss: 25.25473856087407
Experience 10, Iter 38, disc loss: 0.0007470606231806542, policy loss: 24.271022694052036
Experience 10, Iter 39, disc loss: 0.0007400010106410755, policy loss: 25.890602727310764
Experience 10, Iter 40, disc loss: 0.0007330076620780668, policy loss: 24.837473452865236
Experience 10, Iter 41, disc loss: 0.0007260978035441483, policy loss: 25.510790284412224
Experience 10, Iter 42, disc loss: 0.000719282561850235, policy loss: 24.65178734742993
Experience 10, Iter 43, disc loss: 0.000712574574869035, policy loss: 25.400979156206112
Experience 10, Iter 44, disc loss: 0.0007059382465029107, policy loss: 26.10144258786695
Experience 10, Iter 45, disc loss: 0.0006994210256579691, policy loss: 24.798447048883283
Experience 10, Iter 46, disc loss: 0.000692942269819499, policy loss: 26.440575805785066
Experience 10, Iter 47, disc loss: 0.0006866355115778551, policy loss: 25.896770422745035
Experience 10, Iter 48, disc loss: 0.0006805435285447588, policy loss: 24.616769998762496
Experience 10, Iter 49, disc loss: 0.0006740660673552097, policy loss: 25.31948956829516
Experience 10, Iter 50, disc loss: 0.000667939641047844, policy loss: 25.14313523468457
Experience 10, Iter 51, disc loss: 0.0006618860362079698, policy loss: 25.482170280004485
Experience 10, Iter 52, disc loss: 0.000655912529366246, policy loss: 25.6879949897859
Experience 10, Iter 53, disc loss: 0.0006500175536551612, policy loss: 24.95861441141605
Experience 10, Iter 54, disc loss: 0.0006442029224913452, policy loss: 25.37021551880568
Experience 10, Iter 55, disc loss: 0.0006384481619358765, policy loss: 25.52033230499687
Experience 10, Iter 56, disc loss: 0.000632782830312209, policy loss: 24.438235244229688
Experience 10, Iter 57, disc loss: 0.0006271732665217563, policy loss: 25.24227468395987
Experience 10, Iter 58, disc loss: 0.0006216439525453783, policy loss: 25.418757653908465
Experience 10, Iter 59, disc loss: 0.0006161836182823124, policy loss: 24.764122208955158
Experience 10, Iter 60, disc loss: 0.0006107852498838701, policy loss: 25.769302724457543
Experience 10, Iter 61, disc loss: 0.000605465929647529, policy loss: 24.254130510750755
Experience 10, Iter 62, disc loss: 0.0006002183764378928, policy loss: 25.780016237672427
Experience 10, Iter 63, disc loss: 0.0005950286993658289, policy loss: 24.654212664621458
Experience 10, Iter 64, disc loss: 0.0005898802069457825, policy loss: 25.4050448941174
Experience 10, Iter 65, disc loss: 0.0005848712634772996, policy loss: 25.652079172612176
Experience 10, Iter 66, disc loss: 0.0005798084695509633, policy loss: 24.92765943977051
Experience 10, Iter 67, disc loss: 0.0005748838615542548, policy loss: 23.87621122622707
Experience 10, Iter 68, disc loss: 0.0005699847784857555, policy loss: 25.338267104887947
Experience 10, Iter 69, disc loss: 0.0005651733064667373, policy loss: 24.84951323220932
Experience 10, Iter 70, disc loss: 0.0005604035121475422, policy loss: 23.640246161938258
Experience 10, Iter 71, disc loss: 0.0005556995440842943, policy loss: 25.752701331053107
Experience 10, Iter 72, disc loss: 0.0005510524833125161, policy loss: 25.415114015237986
Experience 10, Iter 73, disc loss: 0.0005466975423945732, policy loss: 25.203540935672578
Experience 10, Iter 74, disc loss: 0.000541921719464852, policy loss: 25.253369948227927
Experience 10, Iter 75, disc loss: 0.0005374437498897348, policy loss: 25.22558313697904
Experience 10, Iter 76, disc loss: 0.0005330089445989273, policy loss: 25.253047258047964
Experience 10, Iter 77, disc loss: 0.0005286540136641724, policy loss: 25.036645322302142
Experience 10, Iter 78, disc loss: 0.0005243159156344769, policy loss: 24.908244824443287
Experience 10, Iter 79, disc loss: 0.0005200439110311287, policy loss: 25.676735217567895
Experience 10, Iter 80, disc loss: 0.0005158133723570437, policy loss: 25.00943420764588
Experience 10, Iter 81, disc loss: 0.000511672110669737, policy loss: 24.876663700949976
Experience 10, Iter 82, disc loss: 0.0005075260981582873, policy loss: 24.01256702366436
Experience 10, Iter 83, disc loss: 0.0005034479948459232, policy loss: 24.57191680836536
Experience 10, Iter 84, disc loss: 0.0005023712958757889, policy loss: 23.96615139031136
Experience 10, Iter 85, disc loss: 0.0007101040687109574, policy loss: 23.958052301883797
Experience 10, Iter 86, disc loss: 0.000491615512838566, policy loss: 24.840607075278584
Experience 10, Iter 87, disc loss: 0.000548273349090908, policy loss: 23.96033520816738
Experience 10, Iter 88, disc loss: 0.000484125302363605, policy loss: 24.824381040807218
Experience 10, Iter 89, disc loss: 0.00048043638019738724, policy loss: 24.951494347443084
Experience 10, Iter 90, disc loss: 0.0004767742541666536, policy loss: 26.4571677264358
Experience 10, Iter 91, disc loss: 0.0004731519659340439, policy loss: 24.774122346047186
Experience 10, Iter 92, disc loss: 0.0004695568983895779, policy loss: 25.305553407331644
Experience 10, Iter 93, disc loss: 0.0004659957584723495, policy loss: 24.49873919798522
Experience 10, Iter 94, disc loss: 0.00046254182359362295, policy loss: 25.37402589213493
Experience 10, Iter 95, disc loss: 0.00045897119919214885, policy loss: 24.182998580639797
Experience 10, Iter 96, disc loss: 0.0004555086550814862, policy loss: 25.351353272542696
Experience 10, Iter 97, disc loss: 0.00045762695553856644, policy loss: 25.262201109726035
Experience 10, Iter 98, disc loss: 0.0004486848753570187, policy loss: 26.33655974094375
Experience 10, Iter 99, disc loss: 0.00044533032205579953, policy loss: 24.480928258863493
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0111],
        [0.2231],
        [1.9803],
        [0.0350]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0470, 0.4371, 1.5848, 0.0337, 0.0151, 5.7233]],

        [[0.0470, 0.4371, 1.5848, 0.0337, 0.0151, 5.7233]],

        [[0.0470, 0.4371, 1.5848, 0.0337, 0.0151, 5.7233]],

        [[0.0470, 0.4371, 1.5848, 0.0337, 0.0151, 5.7233]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0444, 0.8926, 7.9210, 0.1399], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0444, 0.8926, 7.9210, 0.1399], device='cuda:0')
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.137
Iter 2/2000 - Loss: 4.115
Iter 3/2000 - Loss: 4.044
Iter 4/2000 - Loss: 4.019
Iter 5/2000 - Loss: 3.989
Iter 6/2000 - Loss: 3.929
Iter 7/2000 - Loss: 3.872
Iter 8/2000 - Loss: 3.819
Iter 9/2000 - Loss: 3.752
Iter 10/2000 - Loss: 3.666
Iter 11/2000 - Loss: 3.572
Iter 12/2000 - Loss: 3.476
Iter 13/2000 - Loss: 3.374
Iter 14/2000 - Loss: 3.259
Iter 15/2000 - Loss: 3.126
Iter 16/2000 - Loss: 2.977
Iter 17/2000 - Loss: 2.813
Iter 18/2000 - Loss: 2.638
Iter 19/2000 - Loss: 2.449
Iter 20/2000 - Loss: 2.242
Iter 1981/2000 - Loss: -5.546
Iter 1982/2000 - Loss: -5.546
Iter 1983/2000 - Loss: -5.546
Iter 1984/2000 - Loss: -5.546
Iter 1985/2000 - Loss: -5.546
Iter 1986/2000 - Loss: -5.546
Iter 1987/2000 - Loss: -5.546
Iter 1988/2000 - Loss: -5.546
Iter 1989/2000 - Loss: -5.546
Iter 1990/2000 - Loss: -5.546
Iter 1991/2000 - Loss: -5.546
Iter 1992/2000 - Loss: -5.547
Iter 1993/2000 - Loss: -5.547
Iter 1994/2000 - Loss: -5.547
Iter 1995/2000 - Loss: -5.547
Iter 1996/2000 - Loss: -5.547
Iter 1997/2000 - Loss: -5.547
Iter 1998/2000 - Loss: -5.547
Iter 1999/2000 - Loss: -5.547
Iter 2000/2000 - Loss: -5.547
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0022],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[19.2226, 12.2594, 44.3335, 14.8700, 17.0319, 62.3860]],

        [[26.9273, 51.1735,  8.3125,  1.3403,  1.4528, 35.0321]],

        [[24.3048, 47.2644,  8.0070,  0.9142,  0.8441, 18.0291]],

        [[20.0915, 44.9394, 19.9647,  2.8025,  2.3160, 31.0511]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1931,  3.1435, 10.8079,  0.7197], device='cuda:0')
Estimated target variance: tensor([0.0444, 0.8926, 7.9210, 0.1399], device='cuda:0')
N: 110
Signal to noise ratio: tensor([21.4116, 69.3438, 70.4288, 41.5664], device='cuda:0')
Bound on condition number: tensor([ 50431.3759, 528943.5228, 545625.4810, 190054.9367], device='cuda:0')
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.00044198798442578717, policy loss: 25.145026719808577
Experience 11, Iter 1, disc loss: 0.0004386905381322688, policy loss: 24.923721971120727
Experience 11, Iter 2, disc loss: 0.0004354331181254338, policy loss: 25.67731532978756
Experience 11, Iter 3, disc loss: 0.00043226948557648515, policy loss: 25.200170663471827
Experience 11, Iter 4, disc loss: 0.0004289971879318504, policy loss: 24.591064486473755
Experience 11, Iter 5, disc loss: 0.00042582957884320597, policy loss: 25.645692113614906
Experience 11, Iter 6, disc loss: 0.0004275748085420935, policy loss: 25.25553903302313
Experience 11, Iter 7, disc loss: 0.0004196055288169282, policy loss: 26.483825154537225
Experience 11, Iter 8, disc loss: 0.00041652784373521405, policy loss: 26.10988938902483
Experience 11, Iter 9, disc loss: 0.00041348878814010987, policy loss: 25.89380386734905
Experience 11, Iter 10, disc loss: 0.00041047907904198205, policy loss: 26.036877427008257
Experience 11, Iter 11, disc loss: 0.00040750600919976885, policy loss: 25.95744408479759
Experience 11, Iter 12, disc loss: 0.00040455877186311125, policy loss: 27.043133199117904
Experience 11, Iter 13, disc loss: 0.0004016484866651265, policy loss: 26.03712181205208
Experience 11, Iter 14, disc loss: 0.0003987596030946933, policy loss: 26.133591044588968
Experience 11, Iter 15, disc loss: 0.0003959082949337412, policy loss: 26.46665287107766
Experience 11, Iter 16, disc loss: 0.0003930789468400506, policy loss: 25.919958245600405
Experience 11, Iter 17, disc loss: 0.0003902828123609622, policy loss: 27.206192850785122
Experience 11, Iter 18, disc loss: 0.0003875150033930283, policy loss: 27.155047710145368
Experience 11, Iter 19, disc loss: 0.0003847770739137974, policy loss: 26.355178761422245
Experience 11, Iter 20, disc loss: 0.00038211403978587, policy loss: 26.0983988717617
Experience 11, Iter 21, disc loss: 0.0003793833185337216, policy loss: 27.444181041629122
Experience 11, Iter 22, disc loss: 0.00037672818807008045, policy loss: 26.931812750384992
Experience 11, Iter 23, disc loss: 0.00037410046575134774, policy loss: 27.132682176352397
Experience 11, Iter 24, disc loss: 0.0003715022865206443, policy loss: 27.280687103497037
Experience 11, Iter 25, disc loss: 0.000369030920153954, policy loss: 25.699526531321137
Experience 11, Iter 26, disc loss: 0.00036637211068015244, policy loss: 27.729020868984616
Experience 11, Iter 27, disc loss: 0.0003638513478838331, policy loss: 27.771906856640435
Experience 11, Iter 28, disc loss: 0.0003613559321204213, policy loss: 26.915190514038066
Experience 11, Iter 29, disc loss: 0.00035887952290534776, policy loss: 27.970683653617808
Experience 11, Iter 30, disc loss: 0.00035643317563082056, policy loss: 27.577529863973645
Experience 11, Iter 31, disc loss: 0.00035403165618838286, policy loss: 27.652886456826458
Experience 11, Iter 32, disc loss: 0.00035161168601034645, policy loss: 26.858303278367277
Experience 11, Iter 33, disc loss: 0.00034923891157352075, policy loss: 26.5981509321273
Experience 11, Iter 34, disc loss: 0.00034688674437181817, policy loss: 27.111943922300323
Experience 11, Iter 35, disc loss: 0.0003445593590409154, policy loss: 28.10467248344938
Experience 11, Iter 36, disc loss: 0.00034227489376984546, policy loss: 26.939063878553085
Experience 11, Iter 37, disc loss: 0.00034004308910581224, policy loss: 26.63479650397059
Experience 11, Iter 38, disc loss: 0.00033772781893674157, policy loss: 27.867861455811862
Experience 11, Iter 39, disc loss: 0.0003354796824272267, policy loss: 27.595979975547387
Experience 11, Iter 40, disc loss: 0.0003332645984692226, policy loss: 27.153447952899853
Experience 11, Iter 41, disc loss: 0.0003312398635176926, policy loss: 27.559744262510502
Experience 11, Iter 42, disc loss: 0.0003289053696667062, policy loss: 26.314099856095034
Experience 11, Iter 43, disc loss: 0.00032675089646014754, policy loss: 28.574759701955948
Experience 11, Iter 44, disc loss: 0.00032462224381595044, policy loss: 28.050205824362937
Experience 11, Iter 45, disc loss: 0.0003225987183407629, policy loss: 27.921060624508392
Experience 11, Iter 46, disc loss: 0.00032042570545819153, policy loss: 27.939971117447012
Experience 11, Iter 47, disc loss: 0.00031835877706362187, policy loss: 28.59022439437988
Experience 11, Iter 48, disc loss: 0.00031631142989615925, policy loss: 27.604089297985464
Experience 11, Iter 49, disc loss: 0.00031428191980526843, policy loss: 27.418810414962486
Experience 11, Iter 50, disc loss: 0.00031227497682028804, policy loss: 27.590725973619968
Experience 11, Iter 51, disc loss: 0.0003102867005985109, policy loss: 27.87598628685954
Experience 11, Iter 52, disc loss: 0.0003083129299756447, policy loss: 27.779565521538352
Experience 11, Iter 53, disc loss: 0.0003063609352881037, policy loss: 27.254017883827665
Experience 11, Iter 54, disc loss: 0.00030442749298151836, policy loss: 27.916504251266073
Experience 11, Iter 55, disc loss: 0.0003025125259497416, policy loss: 26.951307357901623
Experience 11, Iter 56, disc loss: 0.00030061551203649383, policy loss: 26.72127242785016
Experience 11, Iter 57, disc loss: 0.00029873639355854493, policy loss: 27.197427377702887
Experience 11, Iter 58, disc loss: 0.00029687429176137903, policy loss: 27.494273952236078
Experience 11, Iter 59, disc loss: 0.00029503231497091657, policy loss: 26.85475895463626
Experience 11, Iter 60, disc loss: 0.0002932025552510169, policy loss: 27.602911353938843
Experience 11, Iter 61, disc loss: 0.0002913949287923235, policy loss: 27.498111686304142
Experience 11, Iter 62, disc loss: 0.0002895993218740321, policy loss: 27.29441877618627
Experience 11, Iter 63, disc loss: 0.0002878218687272108, policy loss: 28.67217516557192
Experience 11, Iter 64, disc loss: 0.0002860613141979572, policy loss: 27.477879398348357
Experience 11, Iter 65, disc loss: 0.00028431739037598644, policy loss: 27.494037293209022
Experience 11, Iter 66, disc loss: 0.0002825907299820397, policy loss: 27.30841244980661
Experience 11, Iter 67, disc loss: 0.00028087624788583436, policy loss: 27.544529867176326
Experience 11, Iter 68, disc loss: 0.0002791819426659065, policy loss: 27.67251876752916
Experience 11, Iter 69, disc loss: 0.00027749733179439483, policy loss: 27.068624786930812
Experience 11, Iter 70, disc loss: 0.00027583176671870565, policy loss: 26.367238829278033
Experience 11, Iter 71, disc loss: 0.0002741794507635307, policy loss: 27.8177032897027
Experience 11, Iter 72, disc loss: 0.0002725431277441162, policy loss: 27.79152717355757
Experience 11, Iter 73, disc loss: 0.0002709214394250036, policy loss: 27.985499035203866
Experience 11, Iter 74, disc loss: 0.00026931395461806334, policy loss: 28.051266521239015
Experience 11, Iter 75, disc loss: 0.00026773598800527775, policy loss: 27.198274013927495
Experience 11, Iter 76, disc loss: 0.00026614481000781365, policy loss: 27.722308839285382
Experience 11, Iter 77, disc loss: 0.0002645780742731383, policy loss: 26.456327319236422
Experience 11, Iter 78, disc loss: 0.0002630267016680097, policy loss: 28.02302594641504
Experience 11, Iter 79, disc loss: 0.0002614905240548105, policy loss: 27.44319355869498
Experience 11, Iter 80, disc loss: 0.000259974163284152, policy loss: 27.721828524976786
Experience 11, Iter 81, disc loss: 0.000258457834483951, policy loss: 27.19962867458131
Experience 11, Iter 82, disc loss: 0.00025695876912703024, policy loss: 28.36586959125746
Experience 11, Iter 83, disc loss: 0.0002554750120358288, policy loss: 27.5647778486075
Experience 11, Iter 84, disc loss: 0.0002542114693939796, policy loss: 27.96733490861741
Experience 11, Iter 85, disc loss: 0.000252545517357308, policy loss: 27.19899459277017
Experience 11, Iter 86, disc loss: 0.00025114017315834, policy loss: 26.98438152576272
Experience 11, Iter 87, disc loss: 0.00024966805320392996, policy loss: 28.021672517122383
Experience 11, Iter 88, disc loss: 0.0002482462984366202, policy loss: 26.866070187248326
Experience 11, Iter 89, disc loss: 0.0002468375371006659, policy loss: 27.017894297330933
Experience 11, Iter 90, disc loss: 0.0002454417178369964, policy loss: 27.796349862628258
Experience 11, Iter 91, disc loss: 0.00024405655362635966, policy loss: 27.062094483093276
Experience 11, Iter 92, disc loss: 0.00024268463527009172, policy loss: 25.807854648205407
Experience 11, Iter 93, disc loss: 0.00024132854687561144, policy loss: 27.393808456076172
Experience 11, Iter 94, disc loss: 0.0002399783975671066, policy loss: 26.801447345182623
Experience 11, Iter 95, disc loss: 0.00023872923138205842, policy loss: 26.88443857789197
Experience 11, Iter 96, disc loss: 0.00023730837997610306, policy loss: 27.626433474721225
Experience 11, Iter 97, disc loss: 0.00023599260004276106, policy loss: 28.030589765786647
Experience 11, Iter 98, disc loss: 0.00023468808425102514, policy loss: 28.280832277191326
Experience 11, Iter 99, disc loss: 0.00023339403892920455, policy loss: 28.196059977669748
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0114],
        [0.2269],
        [1.9731],
        [0.0365]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0513, 0.4481, 1.6624, 0.0356, 0.0149, 5.9413]],

        [[0.0513, 0.4481, 1.6624, 0.0356, 0.0149, 5.9413]],

        [[0.0513, 0.4481, 1.6624, 0.0356, 0.0149, 5.9413]],

        [[0.0513, 0.4481, 1.6624, 0.0356, 0.0149, 5.9413]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0454, 0.9075, 7.8924, 0.1460], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0454, 0.9075, 7.8924, 0.1460], device='cuda:0')
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.168
Iter 2/2000 - Loss: 4.171
Iter 3/2000 - Loss: 4.064
Iter 4/2000 - Loss: 4.048
Iter 5/2000 - Loss: 4.028
Iter 6/2000 - Loss: 3.956
Iter 7/2000 - Loss: 3.879
Iter 8/2000 - Loss: 3.816
Iter 9/2000 - Loss: 3.756
Iter 10/2000 - Loss: 3.678
Iter 11/2000 - Loss: 3.579
Iter 12/2000 - Loss: 3.470
Iter 13/2000 - Loss: 3.356
Iter 14/2000 - Loss: 3.237
Iter 15/2000 - Loss: 3.107
Iter 16/2000 - Loss: 2.962
Iter 17/2000 - Loss: 2.799
Iter 18/2000 - Loss: 2.619
Iter 19/2000 - Loss: 2.423
Iter 20/2000 - Loss: 2.214
Iter 1981/2000 - Loss: -5.663
Iter 1982/2000 - Loss: -5.664
Iter 1983/2000 - Loss: -5.664
Iter 1984/2000 - Loss: -5.664
Iter 1985/2000 - Loss: -5.664
Iter 1986/2000 - Loss: -5.664
Iter 1987/2000 - Loss: -5.664
Iter 1988/2000 - Loss: -5.664
Iter 1989/2000 - Loss: -5.664
Iter 1990/2000 - Loss: -5.664
Iter 1991/2000 - Loss: -5.664
Iter 1992/2000 - Loss: -5.664
Iter 1993/2000 - Loss: -5.664
Iter 1994/2000 - Loss: -5.664
Iter 1995/2000 - Loss: -5.664
Iter 1996/2000 - Loss: -5.664
Iter 1997/2000 - Loss: -5.664
Iter 1998/2000 - Loss: -5.664
Iter 1999/2000 - Loss: -5.665
Iter 2000/2000 - Loss: -5.665
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0024],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[19.9652, 12.6022, 45.6531, 14.5286, 15.5832, 66.1753]],

        [[26.9457, 50.1224,  8.3195,  1.3752,  1.4329, 35.2044]],

        [[23.3072, 48.0332,  8.1089,  0.9504,  0.7965, 18.1075]],

        [[21.7492, 44.1335, 18.6775,  2.7426,  2.8001, 27.4002]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1938,  3.1258, 10.3217,  0.6758], device='cuda:0')
Estimated target variance: tensor([0.0454, 0.9075, 7.8924, 0.1460], device='cuda:0')
N: 120
Signal to noise ratio: tensor([21.4263, 70.4731, 65.4700, 41.4566], device='cuda:0')
Bound on condition number: tensor([ 55091.5753, 595975.1452, 514359.6585, 206239.2290], device='cuda:0')
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.00023211130676347507, policy loss: 27.138439103693425
Experience 12, Iter 1, disc loss: 0.00023085345582440344, policy loss: 28.020608772524568
Experience 12, Iter 2, disc loss: 0.00022957721060529112, policy loss: 28.10065876646094
Experience 12, Iter 3, disc loss: 0.00022832607947104145, policy loss: 28.728008852247427
Experience 12, Iter 4, disc loss: 0.00022708480314823676, policy loss: 27.582470849871584
Experience 12, Iter 5, disc loss: 0.00022585449813655717, policy loss: 28.104663799937043
Experience 12, Iter 6, disc loss: 0.00022473688628060835, policy loss: 26.948918797173356
Experience 12, Iter 7, disc loss: 0.00022342390591729096, policy loss: 28.196143874083802
Experience 12, Iter 8, disc loss: 0.00022222183800665277, policy loss: 28.1497653080543
Experience 12, Iter 9, disc loss: 0.00022103227249850458, policy loss: 27.227045323852444
Experience 12, Iter 10, disc loss: 0.0002198496706398719, policy loss: 28.263087689386833
Experience 12, Iter 11, disc loss: 0.00021867780284754536, policy loss: 27.14598510723441
Experience 12, Iter 12, disc loss: 0.00021751810974540237, policy loss: 27.803175003487993
Experience 12, Iter 13, disc loss: 0.00021636260398763492, policy loss: 27.351787442270496
Experience 12, Iter 14, disc loss: 0.0002152189259122339, policy loss: 27.920359219805
Experience 12, Iter 15, disc loss: 0.00021408469113208564, policy loss: 28.891675979008088
Experience 12, Iter 16, disc loss: 0.0002129603886449064, policy loss: 27.2867771290324
Experience 12, Iter 17, disc loss: 0.00021184263162316746, policy loss: 27.988533938909278
Experience 12, Iter 18, disc loss: 0.00021073516033419428, policy loss: 27.896761655613908
Experience 12, Iter 19, disc loss: 0.00020964007141557834, policy loss: 27.685520440009345
Experience 12, Iter 20, disc loss: 0.0002085465874245171, policy loss: 27.319463940108008
Experience 12, Iter 21, disc loss: 0.0002074650520929811, policy loss: 26.726284753848105
Experience 12, Iter 22, disc loss: 0.00020639187708360715, policy loss: 27.614205292771103
Experience 12, Iter 23, disc loss: 0.00020532840159217307, policy loss: 26.93487380561904
Experience 12, Iter 24, disc loss: 0.0002042712430953673, policy loss: 27.35033251560122
Experience 12, Iter 25, disc loss: 0.00020322327355550587, policy loss: 27.46909226461831
Experience 12, Iter 26, disc loss: 0.0002021833398411836, policy loss: 27.560251125620603
Experience 12, Iter 27, disc loss: 0.0002011515242818011, policy loss: 27.707818113002123
Experience 12, Iter 28, disc loss: 0.00020022289540813962, policy loss: 26.934554714080704
Experience 12, Iter 29, disc loss: 0.00019911207599226035, policy loss: 27.739492081566837
Experience 12, Iter 30, disc loss: 0.00019810429621858124, policy loss: 27.249742854764378
Experience 12, Iter 31, disc loss: 0.000197104559063666, policy loss: 27.198587058410688
Experience 12, Iter 32, disc loss: 0.00019612063354905917, policy loss: 28.207873408754008
Experience 12, Iter 33, disc loss: 0.0001951272873182542, policy loss: 26.849531261220413
Experience 12, Iter 34, disc loss: 0.0001941524404282465, policy loss: 26.207043890709446
Experience 12, Iter 35, disc loss: 0.00019327238346535181, policy loss: 27.237049847629688
Experience 12, Iter 36, disc loss: 0.0001922164692214262, policy loss: 27.602478546130502
Experience 12, Iter 37, disc loss: 0.0001912631879972828, policy loss: 26.311011324134974
Experience 12, Iter 38, disc loss: 0.0001903131248217027, policy loss: 28.161056407015618
Experience 12, Iter 39, disc loss: 0.00018937238217129885, policy loss: 27.63554581820605
Experience 12, Iter 40, disc loss: 0.0001884383269634202, policy loss: 27.511537940099316
Experience 12, Iter 41, disc loss: 0.0001875114904466527, policy loss: 28.821974192046113
Experience 12, Iter 42, disc loss: 0.0001865916046515953, policy loss: 27.172663801099517
Experience 12, Iter 43, disc loss: 0.0001856799388278088, policy loss: 27.250455655637737
Experience 12, Iter 44, disc loss: 0.00018477349671241366, policy loss: 26.685503995559593
Experience 12, Iter 45, disc loss: 0.00018387501251871557, policy loss: 27.110663409231968
Experience 12, Iter 46, disc loss: 0.0001829801799437366, policy loss: 27.18362818167041
Experience 12, Iter 47, disc loss: 0.00018209391154052292, policy loss: 27.42682577847849
Experience 12, Iter 48, disc loss: 0.00018121399680577627, policy loss: 26.53279740694117
Experience 12, Iter 49, disc loss: 0.00018034218770939835, policy loss: 27.734384627658443
Experience 12, Iter 50, disc loss: 0.0001794749767837472, policy loss: 26.232413711296005
Experience 12, Iter 51, disc loss: 0.00017861351446983597, policy loss: 28.096318574462803
Experience 12, Iter 52, disc loss: 0.00017775911818548404, policy loss: 28.195991845878673
Experience 12, Iter 53, disc loss: 0.00017691168476465714, policy loss: 27.7877393094776
Experience 12, Iter 54, disc loss: 0.00017607079011177745, policy loss: 27.34674603484064
Experience 12, Iter 55, disc loss: 0.00017523393200774383, policy loss: 27.333456631577345
Experience 12, Iter 56, disc loss: 0.0001744048892172465, policy loss: 26.5593546435603
Experience 12, Iter 57, disc loss: 0.00017358127322097662, policy loss: 27.899502486040237
Experience 12, Iter 58, disc loss: 0.0001729531016447963, policy loss: 27.628588603579608
Experience 12, Iter 59, disc loss: 0.0001719545170213898, policy loss: 27.47203785356468
Experience 12, Iter 60, disc loss: 0.00017114449606275899, policy loss: 26.834611432119914
Experience 12, Iter 61, disc loss: 0.00017034434815396386, policy loss: 27.596355140304507
Experience 12, Iter 62, disc loss: 0.00016954963384696538, policy loss: 27.157347551161294
Experience 12, Iter 63, disc loss: 0.0001687606782237622, policy loss: 27.820888013020006
Experience 12, Iter 64, disc loss: 0.00016797732492995166, policy loss: 27.291774258742592
Experience 12, Iter 65, disc loss: 0.0001671994633063414, policy loss: 26.34105376355571
Experience 12, Iter 66, disc loss: 0.00016644431611837464, policy loss: 27.38715941444486
Experience 12, Iter 67, disc loss: 0.0001656604167860327, policy loss: 27.843078635994672
Experience 12, Iter 68, disc loss: 0.00016489890216654936, policy loss: 27.90591587709685
Experience 12, Iter 69, disc loss: 0.00016418900783516093, policy loss: 26.784762967865518
Experience 12, Iter 70, disc loss: 0.0001633923331630467, policy loss: 26.50146307644708
Experience 12, Iter 71, disc loss: 0.0001626463843741027, policy loss: 27.624797755585046
Experience 12, Iter 72, disc loss: 0.0001619091007641271, policy loss: 27.16069043450033
Experience 12, Iter 73, disc loss: 0.00016117102149963694, policy loss: 27.182960854109723
Experience 12, Iter 74, disc loss: 0.00016044052760795963, policy loss: 28.38397751523356
Experience 12, Iter 75, disc loss: 0.0001597154868773905, policy loss: 27.18155260872212
Experience 12, Iter 76, disc loss: 0.0001589955060564527, policy loss: 27.773678502179884
Experience 12, Iter 77, disc loss: 0.0001582802626545783, policy loss: 26.75456744985248
Experience 12, Iter 78, disc loss: 0.00015757437640105142, policy loss: 27.639049225477624
Experience 12, Iter 79, disc loss: 0.00015686449859660806, policy loss: 27.756506018607194
Experience 12, Iter 80, disc loss: 0.00015616432380499581, policy loss: 26.578126452714514
Experience 12, Iter 81, disc loss: 0.0001554682505062063, policy loss: 26.982788470984477
Experience 12, Iter 82, disc loss: 0.00015477794927571398, policy loss: 25.979941718766167
Experience 12, Iter 83, disc loss: 0.00015409658122667396, policy loss: 26.847071632418135
Experience 12, Iter 84, disc loss: 0.00015340957927353427, policy loss: 26.82550253831928
Experience 12, Iter 85, disc loss: 0.00015273198187874468, policy loss: 27.33479838008342
Experience 12, Iter 86, disc loss: 0.00015206001706121383, policy loss: 26.688938272518886
Experience 12, Iter 87, disc loss: 0.00015139124994558207, policy loss: 27.754665997245787
Experience 12, Iter 88, disc loss: 0.0001507276564152192, policy loss: 27.53484702757596
Experience 12, Iter 89, disc loss: 0.00015006851401528235, policy loss: 27.70069960801802
Experience 12, Iter 90, disc loss: 0.00014941383077904366, policy loss: 26.988395122238472
Experience 12, Iter 91, disc loss: 0.0001487638056643447, policy loss: 28.24515657889295
Experience 12, Iter 92, disc loss: 0.00014811753828284396, policy loss: 27.80017956018787
Experience 12, Iter 93, disc loss: 0.0001474756196272344, policy loss: 27.746968932670868
Experience 12, Iter 94, disc loss: 0.00014683865707213804, policy loss: 26.731552525174553
Experience 12, Iter 95, disc loss: 0.0001462048337347946, policy loss: 27.35840007866033
Experience 12, Iter 96, disc loss: 0.00014558057698869833, policy loss: 27.198738299629188
Experience 12, Iter 97, disc loss: 0.00014498831561934346, policy loss: 26.676339848030825
Experience 12, Iter 98, disc loss: 0.00014432990890968258, policy loss: 27.19809859998312
Experience 12, Iter 99, disc loss: 0.00014371394653773584, policy loss: 27.31889200274872
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0116],
        [0.2312],
        [2.0429],
        [0.0359]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0553, 0.4539, 1.6345, 0.0356, 0.0141, 6.0144]],

        [[0.0553, 0.4539, 1.6345, 0.0356, 0.0141, 6.0144]],

        [[0.0553, 0.4539, 1.6345, 0.0356, 0.0141, 6.0144]],

        [[0.0553, 0.4539, 1.6345, 0.0356, 0.0141, 6.0144]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0462, 0.9248, 8.1715, 0.1437], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0462, 0.9248, 8.1715, 0.1437], device='cuda:0')
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.190
Iter 2/2000 - Loss: 4.180
Iter 3/2000 - Loss: 4.089
Iter 4/2000 - Loss: 4.064
Iter 5/2000 - Loss: 4.037
Iter 6/2000 - Loss: 3.968
Iter 7/2000 - Loss: 3.900
Iter 8/2000 - Loss: 3.845
Iter 9/2000 - Loss: 3.781
Iter 10/2000 - Loss: 3.694
Iter 11/2000 - Loss: 3.592
Iter 12/2000 - Loss: 3.484
Iter 13/2000 - Loss: 3.373
Iter 14/2000 - Loss: 3.254
Iter 15/2000 - Loss: 3.119
Iter 16/2000 - Loss: 2.965
Iter 17/2000 - Loss: 2.793
Iter 18/2000 - Loss: 2.605
Iter 19/2000 - Loss: 2.404
Iter 20/2000 - Loss: 2.187
Iter 1981/2000 - Loss: -5.872
Iter 1982/2000 - Loss: -5.872
Iter 1983/2000 - Loss: -5.872
Iter 1984/2000 - Loss: -5.872
Iter 1985/2000 - Loss: -5.872
Iter 1986/2000 - Loss: -5.872
Iter 1987/2000 - Loss: -5.872
Iter 1988/2000 - Loss: -5.872
Iter 1989/2000 - Loss: -5.872
Iter 1990/2000 - Loss: -5.872
Iter 1991/2000 - Loss: -5.872
Iter 1992/2000 - Loss: -5.873
Iter 1993/2000 - Loss: -5.873
Iter 1994/2000 - Loss: -5.873
Iter 1995/2000 - Loss: -5.873
Iter 1996/2000 - Loss: -5.873
Iter 1997/2000 - Loss: -5.873
Iter 1998/2000 - Loss: -5.873
Iter 1999/2000 - Loss: -5.873
Iter 2000/2000 - Loss: -5.873
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[19.6206, 12.8208, 44.0636, 14.7175, 14.1240, 66.0751]],

        [[27.1663, 49.5573,  8.7086,  1.3587,  1.2933, 34.2096]],

        [[25.7191, 52.0311,  7.9916,  0.9703,  0.8081, 20.0390]],

        [[23.9207, 47.4768, 18.7221,  1.1918,  2.3454, 45.3904]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1924,  2.9689, 11.1634,  0.5285], device='cuda:0')
Estimated target variance: tensor([0.0462, 0.9248, 8.1715, 0.1437], device='cuda:0')
N: 130
Signal to noise ratio: tensor([21.0331, 70.9789, 68.3736, 39.5146], device='cuda:0')
Bound on condition number: tensor([ 57511.9179, 654940.9740, 607743.6473, 202983.9631], device='cuda:0')
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.00014310035092994605, policy loss: 28.056234939962643
Experience 13, Iter 1, disc loss: 0.00014249163182959829, policy loss: 27.436176603186954
Experience 13, Iter 2, disc loss: 0.0001418872302286399, policy loss: 26.459524217501148
Experience 13, Iter 3, disc loss: 0.00014128929769350392, policy loss: 27.14593344994551
Experience 13, Iter 4, disc loss: 0.00014068920355908985, policy loss: 27.150186810054777
Experience 13, Iter 5, disc loss: 0.0001400951828058203, policy loss: 27.200554631073267
Experience 13, Iter 6, disc loss: 0.00014037998304987842, policy loss: 26.18136856646542
Experience 13, Iter 7, disc loss: 0.00013892094128008453, policy loss: 28.01996060531953
Experience 13, Iter 8, disc loss: 0.00013834059852376636, policy loss: 26.264817051466572
Experience 13, Iter 9, disc loss: 0.00013784552999169186, policy loss: 27.406113902319184
Experience 13, Iter 10, disc loss: 0.00013719647136598774, policy loss: 27.01626573230339
Experience 13, Iter 11, disc loss: 0.0001366186241383201, policy loss: 26.60565471489614
Experience 13, Iter 12, disc loss: 0.0001360487775739534, policy loss: 26.90344053349964
Experience 13, Iter 13, disc loss: 0.00013548812404961088, policy loss: 27.31631843275541
Experience 13, Iter 14, disc loss: 0.0001349534618667313, policy loss: 26.38836879550023
Experience 13, Iter 15, disc loss: 0.0001343683418519896, policy loss: 27.671238044814327
Experience 13, Iter 16, disc loss: 0.00013389326452784272, policy loss: 27.14814879839646
Experience 13, Iter 17, disc loss: 0.00013326626779621754, policy loss: 27.09804191462831
Experience 13, Iter 18, disc loss: 0.00013272004259365575, policy loss: 26.953989076931947
Experience 13, Iter 19, disc loss: 0.0001321778223333834, policy loss: 27.20832206547417
Experience 13, Iter 20, disc loss: 0.00013163821902359248, policy loss: 28.155884877585954
Experience 13, Iter 21, disc loss: 0.0001311097074316602, policy loss: 27.727830137443085
Experience 13, Iter 22, disc loss: 0.0001305700976378741, policy loss: 26.9287493306936
Experience 13, Iter 23, disc loss: 0.0001300489884287015, policy loss: 26.99013165929167
Experience 13, Iter 24, disc loss: 0.00012951466953457552, policy loss: 27.810372210642583
Experience 13, Iter 25, disc loss: 0.00012899234020889358, policy loss: 26.69719677269635
Experience 13, Iter 26, disc loss: 0.0001285290889326819, policy loss: 26.519743004541596
Experience 13, Iter 27, disc loss: 0.00012797547403211012, policy loss: 26.597313440555272
Experience 13, Iter 28, disc loss: 0.00012744325298494477, policy loss: 26.54052356658288
Experience 13, Iter 29, disc loss: 0.00012693357184001322, policy loss: 27.431699597484872
Experience 13, Iter 30, disc loss: 0.00012642643700626105, policy loss: 26.30738398660091
Experience 13, Iter 31, disc loss: 0.00012592389334669198, policy loss: 25.7635689026116
Experience 13, Iter 32, disc loss: 0.00012542184886294753, policy loss: 26.672226594350015
Experience 13, Iter 33, disc loss: 0.0001250006941226471, policy loss: 26.33852965804499
Experience 13, Iter 34, disc loss: 0.00012442972070225778, policy loss: 26.997439200327314
Experience 13, Iter 35, disc loss: 0.00012393791173716327, policy loss: 26.694501937350957
Experience 13, Iter 36, disc loss: 0.00012345014779175864, policy loss: 27.276504088594802
Experience 13, Iter 37, disc loss: 0.0001229636733585163, policy loss: 27.26640658671272
Experience 13, Iter 38, disc loss: 0.00012248135985935683, policy loss: 26.851724816805152
Experience 13, Iter 39, disc loss: 0.00012200133445087793, policy loss: 26.382492018797233
Experience 13, Iter 40, disc loss: 0.00012152362055526238, policy loss: 27.487552852887323
Experience 13, Iter 41, disc loss: 0.00012104953809108628, policy loss: 26.50589927941875
Experience 13, Iter 42, disc loss: 0.00012060350946489609, policy loss: 26.980289803158907
Experience 13, Iter 43, disc loss: 0.000120110609518968, policy loss: 26.401286601105575
Experience 13, Iter 44, disc loss: 0.00011964366828715126, policy loss: 27.398718733381443
Experience 13, Iter 45, disc loss: 0.00011918457332371477, policy loss: 26.490733842786028
Experience 13, Iter 46, disc loss: 0.00011872803535132092, policy loss: 26.055596684773583
Experience 13, Iter 47, disc loss: 0.00011833281005975581, policy loss: 27.17205606099856
Experience 13, Iter 48, disc loss: 0.00011780897817941042, policy loss: 26.475082981646175
Experience 13, Iter 49, disc loss: 0.00011968384703731335, policy loss: 26.038037286411644
Experience 13, Iter 50, disc loss: 0.00011690640786888683, policy loss: 27.057921761810682
Experience 13, Iter 51, disc loss: 0.00011646886809386824, policy loss: 27.198272410931693
Experience 13, Iter 52, disc loss: 0.00011601623553159184, policy loss: 26.40391361244606
Experience 13, Iter 53, disc loss: 0.00011557344659204027, policy loss: 28.153012797675103
Experience 13, Iter 54, disc loss: 0.00011513413647530194, policy loss: 27.12927209713384
Experience 13, Iter 55, disc loss: 0.00011469849592601029, policy loss: 27.297028000608602
Experience 13, Iter 56, disc loss: 0.00011432212906802089, policy loss: 26.50179766187339
Experience 13, Iter 57, disc loss: 0.00011383190667382764, policy loss: 27.519722413472685
Experience 13, Iter 58, disc loss: 0.0001134027623174111, policy loss: 27.758146254325744
Experience 13, Iter 59, disc loss: 0.00011298887212786649, policy loss: 27.179834285600354
Experience 13, Iter 60, disc loss: 0.00011255232897917928, policy loss: 27.697558360042155
Experience 13, Iter 61, disc loss: 0.0001121387283646241, policy loss: 26.674847327601476
Experience 13, Iter 62, disc loss: 0.00011171107387008399, policy loss: 27.135591482843914
Experience 13, Iter 63, disc loss: 0.00011129414257759372, policy loss: 27.776557487064295
Experience 13, Iter 64, disc loss: 0.00011087984906826687, policy loss: 26.761741624176615
Experience 13, Iter 65, disc loss: 0.00011049236698038134, policy loss: 26.137097302127543
Experience 13, Iter 66, disc loss: 0.00011006418752775348, policy loss: 27.10669081974291
Experience 13, Iter 67, disc loss: 0.00010965835871527705, policy loss: 27.098079144348677
Experience 13, Iter 68, disc loss: 0.00010924506982629866, policy loss: 26.54420407949938
Experience 13, Iter 69, disc loss: 0.00010885373073737279, policy loss: 26.899519990305414
Experience 13, Iter 70, disc loss: 0.00010844779372335614, policy loss: 27.31438632925062
Experience 13, Iter 71, disc loss: 0.00010804259577138429, policy loss: 26.70480413293852
Experience 13, Iter 72, disc loss: 0.0001076467372862908, policy loss: 27.03130216395775
Experience 13, Iter 73, disc loss: 0.00010725577071964464, policy loss: 27.34435551281527
Experience 13, Iter 74, disc loss: 0.000106860937861979, policy loss: 25.68053318256507
Experience 13, Iter 75, disc loss: 0.00010647846553531573, policy loss: 25.68898382834954
Experience 13, Iter 76, disc loss: 0.00010608983549337463, policy loss: 26.877999066040278
Experience 13, Iter 77, disc loss: 0.00010569847153201804, policy loss: 27.29214338113788
Experience 13, Iter 78, disc loss: 0.00010531583858916999, policy loss: 26.947661754147187
Experience 13, Iter 79, disc loss: 0.00010493536242530778, policy loss: 27.514045983248756
Experience 13, Iter 80, disc loss: 0.00010455570666803319, policy loss: 25.832357108458865
Experience 13, Iter 81, disc loss: 0.00010418159270969693, policy loss: 27.498560869113994
Experience 13, Iter 82, disc loss: 0.00010382456952497647, policy loss: 27.194091625498757
Experience 13, Iter 83, disc loss: 0.00010343139225394111, policy loss: 27.459730374529393
Experience 13, Iter 84, disc loss: 0.0001030610022391376, policy loss: 27.463165797591074
Experience 13, Iter 85, disc loss: 0.00010269501360131835, policy loss: 26.891530519057227
Experience 13, Iter 86, disc loss: 0.00010232550590523945, policy loss: 27.395900069618612
Experience 13, Iter 87, disc loss: 0.00010197425135098877, policy loss: 27.422005737625085
Experience 13, Iter 88, disc loss: 0.00010160798492562504, policy loss: 26.108868446999796
Experience 13, Iter 89, disc loss: 0.00010123778873221437, policy loss: 26.701960493905382
Experience 13, Iter 90, disc loss: 0.00010088055979345165, policy loss: 26.027788393182327
Experience 13, Iter 91, disc loss: 0.00010052235953696538, policy loss: 26.150809410259996
Experience 13, Iter 92, disc loss: 0.00010017071067117594, policy loss: 27.141440416649722
Experience 13, Iter 93, disc loss: 9.981505645719807e-05, policy loss: 25.938631467074288
Experience 13, Iter 94, disc loss: 9.946385525441328e-05, policy loss: 27.05800816901167
Experience 13, Iter 95, disc loss: 9.911479381740977e-05, policy loss: 26.8080766398556
Experience 13, Iter 96, disc loss: 9.876736840381356e-05, policy loss: 26.615195512933617
Experience 13, Iter 97, disc loss: 9.842784598012873e-05, policy loss: 26.743930008263376
Experience 13, Iter 98, disc loss: 9.846422687448148e-05, policy loss: 26.77838308152944
Experience 13, Iter 99, disc loss: 9.773826629208019e-05, policy loss: 26.658289801449335
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0115],
        [0.2404],
        [2.1584],
        [0.0354]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0559, 0.4500, 1.6103, 0.0353, 0.0133, 6.1456]],

        [[0.0559, 0.4500, 1.6103, 0.0353, 0.0133, 6.1456]],

        [[0.0559, 0.4500, 1.6103, 0.0353, 0.0133, 6.1456]],

        [[0.0559, 0.4500, 1.6103, 0.0353, 0.0133, 6.1456]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0461, 0.9617, 8.6337, 0.1418], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0461, 0.9617, 8.6337, 0.1418], device='cuda:0')
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.230
Iter 2/2000 - Loss: 4.197
Iter 3/2000 - Loss: 4.124
Iter 4/2000 - Loss: 4.086
Iter 5/2000 - Loss: 4.050
Iter 6/2000 - Loss: 3.984
Iter 7/2000 - Loss: 3.919
Iter 8/2000 - Loss: 3.859
Iter 9/2000 - Loss: 3.782
Iter 10/2000 - Loss: 3.684
Iter 11/2000 - Loss: 3.575
Iter 12/2000 - Loss: 3.463
Iter 13/2000 - Loss: 3.344
Iter 14/2000 - Loss: 3.212
Iter 15/2000 - Loss: 3.061
Iter 16/2000 - Loss: 2.892
Iter 17/2000 - Loss: 2.709
Iter 18/2000 - Loss: 2.513
Iter 19/2000 - Loss: 2.304
Iter 20/2000 - Loss: 2.078
Iter 1981/2000 - Loss: -6.005
Iter 1982/2000 - Loss: -6.005
Iter 1983/2000 - Loss: -6.005
Iter 1984/2000 - Loss: -6.005
Iter 1985/2000 - Loss: -6.005
Iter 1986/2000 - Loss: -6.005
Iter 1987/2000 - Loss: -6.006
Iter 1988/2000 - Loss: -6.006
Iter 1989/2000 - Loss: -6.006
Iter 1990/2000 - Loss: -6.006
Iter 1991/2000 - Loss: -6.006
Iter 1992/2000 - Loss: -6.006
Iter 1993/2000 - Loss: -6.006
Iter 1994/2000 - Loss: -6.006
Iter 1995/2000 - Loss: -6.006
Iter 1996/2000 - Loss: -6.006
Iter 1997/2000 - Loss: -6.006
Iter 1998/2000 - Loss: -6.006
Iter 1999/2000 - Loss: -6.006
Iter 2000/2000 - Loss: -6.006
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0028],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[19.8033, 12.9981, 45.1575, 15.2851, 13.7076, 66.3043]],

        [[27.1878, 48.0442,  8.5881,  1.4050,  1.2260, 34.1381]],

        [[27.4156, 52.9936,  8.2454,  1.0128,  0.8334, 20.9853]],

        [[22.7501, 47.0736, 17.4964,  1.2397,  2.2781, 43.9634]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1989,  3.0943, 12.7114,  0.4952], device='cuda:0')
Estimated target variance: tensor([0.0461, 0.9617, 8.6337, 0.1418], device='cuda:0')
N: 140
Signal to noise ratio: tensor([21.1218, 75.8717, 67.1415, 38.6670], device='cuda:0')
Bound on condition number: tensor([ 62459.2394, 805912.5076, 631117.8317, 209320.3419], device='cuda:0')
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 9.739834046869128e-05, policy loss: 27.551816880108312
Experience 14, Iter 1, disc loss: 9.706168502136331e-05, policy loss: 27.588045645676555
Experience 14, Iter 2, disc loss: 9.6722967409194e-05, policy loss: 26.25062544151338
Experience 14, Iter 3, disc loss: 9.639931447500174e-05, policy loss: 26.371351697744736
Experience 14, Iter 4, disc loss: 9.606146962503542e-05, policy loss: 26.415076884770038
Experience 14, Iter 5, disc loss: 9.572831341864629e-05, policy loss: 26.321680412805204
Experience 14, Iter 6, disc loss: 9.539577590065453e-05, policy loss: 27.75810008319234
Experience 14, Iter 7, disc loss: 9.506821011909711e-05, policy loss: 26.77799441062146
Experience 14, Iter 8, disc loss: 9.474276925667736e-05, policy loss: 27.232209222411406
Experience 14, Iter 9, disc loss: 9.452938288778662e-05, policy loss: 26.32161855535416
Experience 14, Iter 10, disc loss: 9.409595612200573e-05, policy loss: 27.820887008758795
Experience 14, Iter 11, disc loss: 9.377562122863604e-05, policy loss: 26.204783471142413
Experience 14, Iter 12, disc loss: 9.345633984660886e-05, policy loss: 27.553336367048274
Experience 14, Iter 13, disc loss: 9.313915508772674e-05, policy loss: 26.810600013531683
Experience 14, Iter 14, disc loss: 9.282347796008109e-05, policy loss: 26.618287700928953
Experience 14, Iter 15, disc loss: 9.250903487908427e-05, policy loss: 27.192691154454295
Experience 14, Iter 16, disc loss: 9.219835396008146e-05, policy loss: 25.704680960600193
Experience 14, Iter 17, disc loss: 9.188567902265476e-05, policy loss: 27.15340026387466
Experience 14, Iter 18, disc loss: 9.15763256440747e-05, policy loss: 27.43811787239899
Experience 14, Iter 19, disc loss: 9.126860576311416e-05, policy loss: 27.040975008835844
Experience 14, Iter 20, disc loss: 9.634199696662304e-05, policy loss: 26.213739329960802
Experience 14, Iter 21, disc loss: 9.067416199277823e-05, policy loss: 27.1479427472259
Experience 14, Iter 22, disc loss: 9.035802548839204e-05, policy loss: 26.778178952717273
Experience 14, Iter 23, disc loss: 9.006607799290173e-05, policy loss: 27.2578438002433
Experience 14, Iter 24, disc loss: 8.976791207279063e-05, policy loss: 26.913903584802807
Experience 14, Iter 25, disc loss: 8.947612967666835e-05, policy loss: 26.685451070643985
Experience 14, Iter 26, disc loss: 8.916581828322261e-05, policy loss: 27.47551508478715
Experience 14, Iter 27, disc loss: 8.887129537102772e-05, policy loss: 26.639047727342547
Experience 14, Iter 28, disc loss: 8.857937137621965e-05, policy loss: 27.98637286375914
Experience 14, Iter 29, disc loss: 8.828627980740466e-05, policy loss: 26.5154146153314
Experience 14, Iter 30, disc loss: 8.79980719151887e-05, policy loss: 26.530190715783256
Experience 14, Iter 31, disc loss: 8.770715556287724e-05, policy loss: 27.06917880149185
Experience 14, Iter 32, disc loss: 8.74192369398924e-05, policy loss: 26.955376878196514
Experience 14, Iter 33, disc loss: 8.713428755825871e-05, policy loss: 25.95517215358301
Experience 14, Iter 34, disc loss: 8.68482589741564e-05, policy loss: 26.84047486122907
Experience 14, Iter 35, disc loss: 8.656467757230498e-05, policy loss: 27.202788408072493
Experience 14, Iter 36, disc loss: 8.628335457639465e-05, policy loss: 26.76054705156595
Experience 14, Iter 37, disc loss: 8.600202348265028e-05, policy loss: 26.850552225165764
Experience 14, Iter 38, disc loss: 8.572448933390066e-05, policy loss: 27.20138891338107
Experience 14, Iter 39, disc loss: 8.553906696490673e-05, policy loss: 25.72041140587629
Experience 14, Iter 40, disc loss: 8.522198095671235e-05, policy loss: 26.598756696582367
Experience 14, Iter 41, disc loss: 8.490650116778207e-05, policy loss: 27.462443903933654
Experience 14, Iter 42, disc loss: 8.461865153710943e-05, policy loss: 27.805518819123638
Experience 14, Iter 43, disc loss: 8.434602932214226e-05, policy loss: 27.41588325414505
Experience 14, Iter 44, disc loss: 8.40753415322633e-05, policy loss: 27.537843573688292
Experience 14, Iter 45, disc loss: 8.380519255362712e-05, policy loss: 26.235434978878487
Experience 14, Iter 46, disc loss: 8.35362116597556e-05, policy loss: 26.996688899283605
Experience 14, Iter 47, disc loss: 8.32982735923673e-05, policy loss: 26.851287103534432
Experience 14, Iter 48, disc loss: 8.30024826446044e-05, policy loss: 27.21355034105209
Experience 14, Iter 49, disc loss: 0.0006850140225123447, policy loss: 25.94922296671286
Experience 14, Iter 50, disc loss: 8.268767234505527e-05, policy loss: 27.19341656843483
Experience 14, Iter 51, disc loss: 8.261717656368888e-05, policy loss: 26.986801702160342
Experience 14, Iter 52, disc loss: 8.252888485181528e-05, policy loss: 26.498695304962645
Experience 14, Iter 53, disc loss: 8.242440470505706e-05, policy loss: 26.34096612547851
Experience 14, Iter 54, disc loss: 8.230063501911201e-05, policy loss: 28.103218962860957
Experience 14, Iter 55, disc loss: 8.217130739973068e-05, policy loss: 26.68912738140155
Experience 14, Iter 56, disc loss: 8.2020374488707e-05, policy loss: 27.431122367637897
Experience 14, Iter 57, disc loss: 8.186613598863545e-05, policy loss: 27.821505818499972
Experience 14, Iter 58, disc loss: 8.169733971806427e-05, policy loss: 26.811372438982445
Experience 14, Iter 59, disc loss: 8.152165693414161e-05, policy loss: 26.677634236118
Experience 14, Iter 60, disc loss: 8.133922610640479e-05, policy loss: 26.60063776378408
Experience 14, Iter 61, disc loss: 8.115027674085298e-05, policy loss: 27.39988359709882
Experience 14, Iter 62, disc loss: 8.09552919427944e-05, policy loss: 26.30893155520793
Experience 14, Iter 63, disc loss: 8.075664070210927e-05, policy loss: 27.36177766062908
Experience 14, Iter 64, disc loss: 8.060610404479305e-05, policy loss: 27.311758137331786
Experience 14, Iter 65, disc loss: 8.034630108628139e-05, policy loss: 26.34763777637841
Experience 14, Iter 66, disc loss: 8.012903850843023e-05, policy loss: 28.0311735793781
Experience 14, Iter 67, disc loss: 7.991784527811374e-05, policy loss: 27.015502727154093
Experience 14, Iter 68, disc loss: 7.969802894078288e-05, policy loss: 26.129271764037505
Experience 14, Iter 69, disc loss: 7.947638610908958e-05, policy loss: 26.189597713552075
Experience 14, Iter 70, disc loss: 7.925352660881461e-05, policy loss: 27.43463668514634
Experience 14, Iter 71, disc loss: 7.902993125055832e-05, policy loss: 26.70285995007103
Experience 14, Iter 72, disc loss: 7.880491000716579e-05, policy loss: 27.226153211142805
Experience 14, Iter 73, disc loss: 7.861905939856219e-05, policy loss: 27.009881221530605
Experience 14, Iter 74, disc loss: 7.835182588558047e-05, policy loss: 27.205383829064225
Experience 14, Iter 75, disc loss: 7.812364832166137e-05, policy loss: 27.35927620962756
Experience 14, Iter 76, disc loss: 7.789587832311621e-05, policy loss: 27.096250522670488
Experience 14, Iter 77, disc loss: 7.766670714477354e-05, policy loss: 27.22253960422179
Experience 14, Iter 78, disc loss: 7.745433235998279e-05, policy loss: 26.933295195093066
Experience 14, Iter 79, disc loss: 7.720902035885081e-05, policy loss: 26.092969907607817
Experience 14, Iter 80, disc loss: 7.69798941888644e-05, policy loss: 27.16575431249042
Experience 14, Iter 81, disc loss: 7.675123002989098e-05, policy loss: 26.220535536420144
Experience 14, Iter 82, disc loss: 7.652260689365332e-05, policy loss: 27.8260256045032
Experience 14, Iter 83, disc loss: 7.629515204666074e-05, policy loss: 26.519876151256554
Experience 14, Iter 84, disc loss: 7.611350562353525e-05, policy loss: 27.723878951222844
Experience 14, Iter 85, disc loss: 7.583945782239602e-05, policy loss: 26.79093737026414
Experience 14, Iter 86, disc loss: 7.561208032110209e-05, policy loss: 27.509295500439617
Experience 14, Iter 87, disc loss: 7.539204485656196e-05, policy loss: 27.172588146888256
Experience 14, Iter 88, disc loss: 7.515984427657224e-05, policy loss: 27.723852333674294
Experience 14, Iter 89, disc loss: 7.49352594688544e-05, policy loss: 27.40979720879183
Experience 14, Iter 90, disc loss: 7.471881704091642e-05, policy loss: 27.271329539469534
Experience 14, Iter 91, disc loss: 7.44864832709372e-05, policy loss: 27.33265163881248
Experience 14, Iter 92, disc loss: 7.426543970637265e-05, policy loss: 26.66882898752035
Experience 14, Iter 93, disc loss: 7.406563806456318e-05, policy loss: 25.761763579052648
Experience 14, Iter 94, disc loss: 7.396346509836672e-05, policy loss: 27.492684105440546
Experience 14, Iter 95, disc loss: 7.359889278304802e-05, policy loss: 28.231657844022763
Experience 14, Iter 96, disc loss: 7.337912532651444e-05, policy loss: 26.535329760556493
Experience 14, Iter 97, disc loss: 7.316019100424554e-05, policy loss: 26.749063356720225
Experience 14, Iter 98, disc loss: 7.29425621273438e-05, policy loss: 26.78005610377786
Experience 14, Iter 99, disc loss: 7.27244138979883e-05, policy loss: 26.278378397101896
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0115],
        [0.2461],
        [2.1925],
        [0.0346]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0556, 0.4539, 1.5860, 0.0346, 0.0126, 6.3000]],

        [[0.0556, 0.4539, 1.5860, 0.0346, 0.0126, 6.3000]],

        [[0.0556, 0.4539, 1.5860, 0.0346, 0.0126, 6.3000]],

        [[0.0556, 0.4539, 1.5860, 0.0346, 0.0126, 6.3000]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0459, 0.9843, 8.7701, 0.1384], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0459, 0.9843, 8.7701, 0.1384], device='cuda:0')
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.206
Iter 2/2000 - Loss: 4.152
Iter 3/2000 - Loss: 4.093
Iter 4/2000 - Loss: 4.040
Iter 5/2000 - Loss: 4.000
Iter 6/2000 - Loss: 3.941
Iter 7/2000 - Loss: 3.876
Iter 8/2000 - Loss: 3.810
Iter 9/2000 - Loss: 3.728
Iter 10/2000 - Loss: 3.628
Iter 11/2000 - Loss: 3.519
Iter 12/2000 - Loss: 3.406
Iter 13/2000 - Loss: 3.283
Iter 14/2000 - Loss: 3.144
Iter 15/2000 - Loss: 2.986
Iter 16/2000 - Loss: 2.812
Iter 17/2000 - Loss: 2.624
Iter 18/2000 - Loss: 2.422
Iter 19/2000 - Loss: 2.204
Iter 20/2000 - Loss: 1.970
Iter 1981/2000 - Loss: -6.181
Iter 1982/2000 - Loss: -6.181
Iter 1983/2000 - Loss: -6.181
Iter 1984/2000 - Loss: -6.181
Iter 1985/2000 - Loss: -6.181
Iter 1986/2000 - Loss: -6.181
Iter 1987/2000 - Loss: -6.181
Iter 1988/2000 - Loss: -6.181
Iter 1989/2000 - Loss: -6.181
Iter 1990/2000 - Loss: -6.181
Iter 1991/2000 - Loss: -6.181
Iter 1992/2000 - Loss: -6.181
Iter 1993/2000 - Loss: -6.181
Iter 1994/2000 - Loss: -6.182
Iter 1995/2000 - Loss: -6.182
Iter 1996/2000 - Loss: -6.182
Iter 1997/2000 - Loss: -6.182
Iter 1998/2000 - Loss: -6.182
Iter 1999/2000 - Loss: -6.182
Iter 2000/2000 - Loss: -6.182
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0029],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[19.7853, 13.2569, 44.7417, 15.5072, 13.0494, 68.2786]],

        [[26.2626, 47.0831,  8.3875,  1.4181,  1.2218, 34.0367]],

        [[27.0728, 52.7353,  8.4234,  1.0078,  0.8247, 22.0394]],

        [[22.3534, 47.1791, 17.6290,  1.2352,  2.3240, 44.5386]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1978,  2.9927, 13.1452,  0.4974], device='cuda:0')
Estimated target variance: tensor([0.0459, 0.9843, 8.7701, 0.1384], device='cuda:0')
N: 150
Signal to noise ratio: tensor([21.1962, 75.8377, 67.8246, 38.7557], device='cuda:0')
Bound on condition number: tensor([ 67393.0844, 862703.8014, 690026.6477, 225301.4558], device='cuda:0')
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 7.250798556057823e-05, policy loss: 26.533249615335386
Experience 15, Iter 1, disc loss: 7.22951005544167e-05, policy loss: 27.14765609404994
Experience 15, Iter 2, disc loss: 7.207754252578997e-05, policy loss: 26.14001830270843
Experience 15, Iter 3, disc loss: 7.186365476055734e-05, policy loss: 26.982182503221537
Experience 15, Iter 4, disc loss: 7.165064900562089e-05, policy loss: 27.499438291249234
Experience 15, Iter 5, disc loss: 7.143970199731395e-05, policy loss: 26.72757058646183
Experience 15, Iter 6, disc loss: 7.122992518071191e-05, policy loss: 28.134050894739794
Experience 15, Iter 7, disc loss: 7.101723218231416e-05, policy loss: 27.016280963370722
Experience 15, Iter 8, disc loss: 7.080759824941588e-05, policy loss: 27.824607168803198
Experience 15, Iter 9, disc loss: 7.059941935200476e-05, policy loss: 26.279114535235877
Experience 15, Iter 10, disc loss: 7.039147355415918e-05, policy loss: 27.46158574411867
Experience 15, Iter 11, disc loss: 7.018478803726246e-05, policy loss: 26.22419087599183
Experience 15, Iter 12, disc loss: 6.999118041487922e-05, policy loss: 27.80873780842596
Experience 15, Iter 13, disc loss: 6.977428496661991e-05, policy loss: 27.003415525489807
Experience 15, Iter 14, disc loss: 6.957012693139585e-05, policy loss: 26.582614546860473
Experience 15, Iter 15, disc loss: 6.936761903556375e-05, policy loss: 27.18318678152643
Experience 15, Iter 16, disc loss: 6.91643511884898e-05, policy loss: 26.59447884756318
Experience 15, Iter 17, disc loss: 6.896281332041679e-05, policy loss: 26.875942655436663
Experience 15, Iter 18, disc loss: 6.876250562579803e-05, policy loss: 26.37742006928063
Experience 15, Iter 19, disc loss: 6.856308987169314e-05, policy loss: 27.233676833994455
Experience 15, Iter 20, disc loss: 6.836544648573298e-05, policy loss: 26.537247984054098
Experience 15, Iter 21, disc loss: 6.816561224621364e-05, policy loss: 27.128341291851278
Experience 15, Iter 22, disc loss: 6.796845987220585e-05, policy loss: 26.53171204061431
Experience 15, Iter 23, disc loss: 6.777643360891847e-05, policy loss: 27.177846537589645
Experience 15, Iter 24, disc loss: 6.757771594807503e-05, policy loss: 26.627402172829093
Experience 15, Iter 25, disc loss: 6.738498804243155e-05, policy loss: 26.92681912256009
Experience 15, Iter 26, disc loss: 6.718858538669614e-05, policy loss: 27.42482545992897
Experience 15, Iter 27, disc loss: 6.699795995634192e-05, policy loss: 26.62429330499647
Experience 15, Iter 28, disc loss: 6.68166809538517e-05, policy loss: 26.819703304662248
Experience 15, Iter 29, disc loss: 6.661737006591524e-05, policy loss: 26.702345827969594
Experience 15, Iter 30, disc loss: 8.18423037425196e-05, policy loss: 26.883481140812997
Experience 15, Iter 31, disc loss: 6.623735204718628e-05, policy loss: 26.833038356670738
Experience 15, Iter 32, disc loss: 6.66448460728914e-05, policy loss: 25.078520254529487
Experience 15, Iter 33, disc loss: 6.591726048247576e-05, policy loss: 25.81846210514427
Experience 15, Iter 34, disc loss: 6.56895333257746e-05, policy loss: 26.61937180316695
Experience 15, Iter 35, disc loss: 6.550374251717364e-05, policy loss: 26.300046101702883
Experience 15, Iter 36, disc loss: 6.533499625378905e-05, policy loss: 25.648077417325613
Experience 15, Iter 37, disc loss: 6.521968520586923e-05, policy loss: 25.925464259439437
Experience 15, Iter 38, disc loss: 6.495360563729731e-05, policy loss: 26.803228478814887
Experience 15, Iter 39, disc loss: 6.489629678393216e-05, policy loss: 25.77834751369117
Experience 15, Iter 40, disc loss: 6.469599262310763e-05, policy loss: 27.286634863243656
Experience 15, Iter 41, disc loss: 6.441547869550119e-05, policy loss: 26.851467415380604
Experience 15, Iter 42, disc loss: 6.423221321985807e-05, policy loss: 26.560058546611252
Experience 15, Iter 43, disc loss: 6.408324392576963e-05, policy loss: 25.778656474673507
Experience 15, Iter 44, disc loss: 6.387524257574697e-05, policy loss: 25.87035594439498
Experience 15, Iter 45, disc loss: 6.369816937157463e-05, policy loss: 26.56011596017019
Experience 15, Iter 46, disc loss: 6.353442619960616e-05, policy loss: 26.09663341398884
Experience 15, Iter 47, disc loss: 6.334522556936897e-05, policy loss: 27.111484839086053
Experience 15, Iter 48, disc loss: 6.317831503705903e-05, policy loss: 25.144810371896256
Experience 15, Iter 49, disc loss: 6.30578800013373e-05, policy loss: 26.80861390732688
Experience 15, Iter 50, disc loss: 6.326795703841258e-05, policy loss: 25.68398783582923
Experience 15, Iter 51, disc loss: 6.387168655045014e-05, policy loss: 26.113445423674115
Experience 15, Iter 52, disc loss: 6.249641313073155e-05, policy loss: 26.494122487065617
Experience 15, Iter 53, disc loss: 6.242337147338348e-05, policy loss: 25.321213543363886
Experience 15, Iter 54, disc loss: 6.214051795771742e-05, policy loss: 26.894480916973166
Experience 15, Iter 55, disc loss: 6.199297455088942e-05, policy loss: 26.4715325675552
Experience 15, Iter 56, disc loss: 6.179015434106233e-05, policy loss: 25.789081140628205
Experience 15, Iter 57, disc loss: 6.161916644813182e-05, policy loss: 26.81375352497025
Experience 15, Iter 58, disc loss: 6.145101178369273e-05, policy loss: 26.366185861245622
Experience 15, Iter 59, disc loss: 6.135441221342994e-05, policy loss: 26.221034401758104
Experience 15, Iter 60, disc loss: 6.111796607566512e-05, policy loss: 26.1742360282428
Experience 15, Iter 61, disc loss: 6.094570446437087e-05, policy loss: 25.989744773068786
Experience 15, Iter 62, disc loss: 6.0785506721365844e-05, policy loss: 26.205955304102915
Experience 15, Iter 63, disc loss: 6.069326455968241e-05, policy loss: 26.438616115065926
Experience 15, Iter 64, disc loss: 6.0453952241555205e-05, policy loss: 26.036910389750794
Experience 15, Iter 65, disc loss: 6.028534812835834e-05, policy loss: 26.628579779540626
Experience 15, Iter 66, disc loss: 6.01210181609481e-05, policy loss: 27.610608148955116
Experience 15, Iter 67, disc loss: 5.9957877773130205e-05, policy loss: 26.669633981139818
Experience 15, Iter 68, disc loss: 5.9803090107498547e-05, policy loss: 26.528570141844632
Experience 15, Iter 69, disc loss: 5.963353602307816e-05, policy loss: 27.509497457395014
Experience 15, Iter 70, disc loss: 5.947287019383203e-05, policy loss: 27.320395707072816
Experience 15, Iter 71, disc loss: 5.931270778475537e-05, policy loss: 26.78230741426095
Experience 15, Iter 72, disc loss: 5.915470201218259e-05, policy loss: 26.594461729219482
Experience 15, Iter 73, disc loss: 5.899547673331828e-05, policy loss: 26.054042713577722
Experience 15, Iter 74, disc loss: 5.883556149812192e-05, policy loss: 27.03643036740644
Experience 15, Iter 75, disc loss: 5.86772829762537e-05, policy loss: 27.34957757039485
Experience 15, Iter 76, disc loss: 5.852144146602758e-05, policy loss: 27.494944544974892
Experience 15, Iter 77, disc loss: 5.836814101837553e-05, policy loss: 27.11995671654629
Experience 15, Iter 78, disc loss: 5.8207934509320046e-05, policy loss: 26.70680274843096
Experience 15, Iter 79, disc loss: 5.80526275313011e-05, policy loss: 27.885108121457463
Experience 15, Iter 80, disc loss: 5.789873759410274e-05, policy loss: 27.72822912143164
Experience 15, Iter 81, disc loss: 5.774390161182273e-05, policy loss: 27.521484938480533
Experience 15, Iter 82, disc loss: 5.7600184191491944e-05, policy loss: 27.289815449398418
Experience 15, Iter 83, disc loss: 5.743901325757672e-05, policy loss: 26.890817265282173
Experience 15, Iter 84, disc loss: 5.730399069449166e-05, policy loss: 26.228620359040395
Experience 15, Iter 85, disc loss: 5.7142815388836274e-05, policy loss: 27.127563262546452
Experience 15, Iter 86, disc loss: 5.7029851773644095e-05, policy loss: 26.74958244019595
Experience 15, Iter 87, disc loss: 5.683292596126907e-05, policy loss: 27.167178598598454
Experience 15, Iter 88, disc loss: 5.6683124459058185e-05, policy loss: 26.550484708758518
Experience 15, Iter 89, disc loss: 5.65339957258851e-05, policy loss: 27.660133583662237
Experience 15, Iter 90, disc loss: 0.004532670723820939, policy loss: 26.255667834835144
Experience 15, Iter 91, disc loss: 5.712633174474426e-05, policy loss: 27.114978004263417
Experience 15, Iter 92, disc loss: 5.77868295364582e-05, policy loss: 27.50906357521112
Experience 15, Iter 93, disc loss: 5.8372710542543676e-05, policy loss: 26.78422423886927
Experience 15, Iter 94, disc loss: 5.889188341861649e-05, policy loss: 27.097356395310808
Experience 15, Iter 95, disc loss: 5.9343384336032696e-05, policy loss: 27.910878812452175
Experience 15, Iter 96, disc loss: 5.974214977971809e-05, policy loss: 27.388770148893073
Experience 15, Iter 97, disc loss: 6.008179754976287e-05, policy loss: 26.482608673097026
Experience 15, Iter 98, disc loss: 6.038118005369912e-05, policy loss: 25.559919001820727
Experience 15, Iter 99, disc loss: 6.063015990551477e-05, policy loss: 26.915539487925
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0114],
        [0.2484],
        [2.2256],
        [0.0338]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0566, 0.4526, 1.5608, 0.0339, 0.0121, 6.3090]],

        [[0.0566, 0.4526, 1.5608, 0.0339, 0.0121, 6.3090]],

        [[0.0566, 0.4526, 1.5608, 0.0339, 0.0121, 6.3090]],

        [[0.0566, 0.4526, 1.5608, 0.0339, 0.0121, 6.3090]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0454, 0.9934, 8.9022, 0.1351], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0454, 0.9934, 8.9022, 0.1351], device='cuda:0')
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.194
Iter 2/2000 - Loss: 4.121
Iter 3/2000 - Loss: 4.077
Iter 4/2000 - Loss: 4.010
Iter 5/2000 - Loss: 3.963
Iter 6/2000 - Loss: 3.910
Iter 7/2000 - Loss: 3.840
Iter 8/2000 - Loss: 3.762
Iter 9/2000 - Loss: 3.676
Iter 10/2000 - Loss: 3.575
Iter 11/2000 - Loss: 3.463
Iter 12/2000 - Loss: 3.343
Iter 13/2000 - Loss: 3.213
Iter 14/2000 - Loss: 3.069
Iter 15/2000 - Loss: 2.909
Iter 16/2000 - Loss: 2.732
Iter 17/2000 - Loss: 2.540
Iter 18/2000 - Loss: 2.334
Iter 19/2000 - Loss: 2.113
Iter 20/2000 - Loss: 1.876
Iter 1981/2000 - Loss: -6.331
Iter 1982/2000 - Loss: -6.331
Iter 1983/2000 - Loss: -6.331
Iter 1984/2000 - Loss: -6.331
Iter 1985/2000 - Loss: -6.331
Iter 1986/2000 - Loss: -6.332
Iter 1987/2000 - Loss: -6.332
Iter 1988/2000 - Loss: -6.332
Iter 1989/2000 - Loss: -6.332
Iter 1990/2000 - Loss: -6.332
Iter 1991/2000 - Loss: -6.332
Iter 1992/2000 - Loss: -6.332
Iter 1993/2000 - Loss: -6.332
Iter 1994/2000 - Loss: -6.332
Iter 1995/2000 - Loss: -6.332
Iter 1996/2000 - Loss: -6.332
Iter 1997/2000 - Loss: -6.332
Iter 1998/2000 - Loss: -6.332
Iter 1999/2000 - Loss: -6.332
Iter 2000/2000 - Loss: -6.332
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0026],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[19.6002, 13.4486, 43.5179, 14.3942, 11.3066, 68.0305]],

        [[25.9134, 45.7461,  8.2225,  1.4353,  1.2528, 33.4739]],

        [[27.0313, 50.9009,  8.1275,  0.9999,  0.8259, 20.1594]],

        [[22.8971, 46.8924, 17.0686,  1.1634,  2.3681, 48.1416]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2038,  2.8650, 11.7150,  0.4890], device='cuda:0')
Estimated target variance: tensor([0.0454, 0.9934, 8.9022, 0.1351], device='cuda:0')
N: 160
Signal to noise ratio: tensor([21.4856, 74.7163, 67.2094, 38.6255], device='cuda:0')
Bound on condition number: tensor([ 73862.2809, 893204.7681, 722738.2476, 238709.3812], device='cuda:0')
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 6.0837255744504025e-05, policy loss: 27.07724012699697
Experience 16, Iter 1, disc loss: 6.101213900729917e-05, policy loss: 26.33784145908517
Experience 16, Iter 2, disc loss: 6.11687119331941e-05, policy loss: 26.60568429952262
Experience 16, Iter 3, disc loss: 6.126204707504494e-05, policy loss: 27.170634685344922
Experience 16, Iter 4, disc loss: 6.135011230759736e-05, policy loss: 27.024355066172664
Experience 16, Iter 5, disc loss: 6.140509653176134e-05, policy loss: 27.384175942510172
Experience 16, Iter 6, disc loss: 6.144081168846799e-05, policy loss: 27.427074019425085
Experience 16, Iter 7, disc loss: 6.145807105971423e-05, policy loss: 26.980879868078553
Experience 16, Iter 8, disc loss: 6.145567711081048e-05, policy loss: 27.156302459035945
Experience 16, Iter 9, disc loss: 6.156669927056098e-05, policy loss: 27.58846914398782
Experience 16, Iter 10, disc loss: 6.140530330699902e-05, policy loss: 27.0792844221709
Experience 16, Iter 11, disc loss: 6.148248247369338e-05, policy loss: 26.94157473471192
Experience 16, Iter 12, disc loss: 6.131361279885081e-05, policy loss: 27.261691530257348
Experience 16, Iter 13, disc loss: 6.124086982833996e-05, policy loss: 26.48961472935189
Experience 16, Iter 14, disc loss: 6.117182176406345e-05, policy loss: 26.27891536005265
Experience 16, Iter 15, disc loss: 6.10709213985304e-05, policy loss: 26.509085573499938
Experience 16, Iter 16, disc loss: 6.097520992886572e-05, policy loss: 27.24618346107318
Experience 16, Iter 17, disc loss: 6.0874960834904655e-05, policy loss: 26.9139112615814
Experience 16, Iter 18, disc loss: 6.0766867770106986e-05, policy loss: 27.06494635246982
Experience 16, Iter 19, disc loss: 6.0654377353088e-05, policy loss: 26.77066921650804
Experience 16, Iter 20, disc loss: 6.05377965784946e-05, policy loss: 26.62082609146869
Experience 16, Iter 21, disc loss: 6.041647154906699e-05, policy loss: 26.804447000518792
Experience 16, Iter 22, disc loss: 6.028987771369775e-05, policy loss: 26.593045996848172
Experience 16, Iter 23, disc loss: 6.016591780876929e-05, policy loss: 25.788680120171843
Experience 16, Iter 24, disc loss: 6.0034713379571366e-05, policy loss: 26.338080970741622
Experience 16, Iter 25, disc loss: 5.99283202037795e-05, policy loss: 26.439552012128935
Experience 16, Iter 26, disc loss: 5.976449277979348e-05, policy loss: 25.747654157736285
Experience 16, Iter 27, disc loss: 5.961906704873742e-05, policy loss: 26.727041735152838
Experience 16, Iter 28, disc loss: 5.947867599668769e-05, policy loss: 26.646740297861808
Experience 16, Iter 29, disc loss: 5.9336489275285965e-05, policy loss: 27.526966305910076
Experience 16, Iter 30, disc loss: 5.9196121029886e-05, policy loss: 26.80726415189216
Experience 16, Iter 31, disc loss: 5.9048825080578405e-05, policy loss: 27.36478217669851
Experience 16, Iter 32, disc loss: 5.8946606370772334e-05, policy loss: 26.752098685993555
Experience 16, Iter 33, disc loss: 5.87573008569254e-05, policy loss: 26.570354343517103
Experience 16, Iter 34, disc loss: 5.861316168405003e-05, policy loss: 26.498324374100292
Experience 16, Iter 35, disc loss: 5.84775878857027e-05, policy loss: 26.367981816841034
Experience 16, Iter 36, disc loss: 5.832843346471253e-05, policy loss: 25.556583181892368
Experience 16, Iter 37, disc loss: 5.818300076301679e-05, policy loss: 27.080514278492306
Experience 16, Iter 38, disc loss: 5.802101555560841e-05, policy loss: 26.749560089524383
Experience 16, Iter 39, disc loss: 5.787119494461145e-05, policy loss: 25.411171811837256
Experience 16, Iter 40, disc loss: 5.772335072935749e-05, policy loss: 26.56392777609981
Experience 16, Iter 41, disc loss: 5.759738771414276e-05, policy loss: 26.88343335406609
Experience 16, Iter 42, disc loss: 5.8827482038271194e-05, policy loss: 25.880405611998256
Experience 16, Iter 43, disc loss: 5.7277565142508705e-05, policy loss: 27.241498679885304
Experience 16, Iter 44, disc loss: 5.713015131479784e-05, policy loss: 25.715329416136292
Experience 16, Iter 45, disc loss: 5.6983239865342134e-05, policy loss: 27.299669764383395
Experience 16, Iter 46, disc loss: 5.6835220266569484e-05, policy loss: 27.21093195230084
Experience 16, Iter 47, disc loss: 5.669002223135104e-05, policy loss: 27.682678713307972
Experience 16, Iter 48, disc loss: 5.6540953032153124e-05, policy loss: 26.532203663922573
Experience 16, Iter 49, disc loss: 5.6396700818834206e-05, policy loss: 26.00695940740964
Experience 16, Iter 50, disc loss: 5.624828922374268e-05, policy loss: 27.63521855779809
Experience 16, Iter 51, disc loss: 5.610392408675217e-05, policy loss: 25.858295616403634
Experience 16, Iter 52, disc loss: 5.595716054414209e-05, policy loss: 26.568129950266794
Experience 16, Iter 53, disc loss: 5.5812792945470644e-05, policy loss: 26.692556621687768
Experience 16, Iter 54, disc loss: 5.566928621854963e-05, policy loss: 26.177213402294228
Experience 16, Iter 55, disc loss: 5.552326864192587e-05, policy loss: 26.73112698846912
Experience 16, Iter 56, disc loss: 5.537944045250753e-05, policy loss: 26.179536281374208
Experience 16, Iter 57, disc loss: 5.5235826775394595e-05, policy loss: 27.473572597996508
Experience 16, Iter 58, disc loss: 5.5172525270882935e-05, policy loss: 26.199942367592694
Experience 16, Iter 59, disc loss: 5.495103634644875e-05, policy loss: 27.288410070277628
Experience 16, Iter 60, disc loss: 5.4834017255182096e-05, policy loss: 26.125904862237288
Experience 16, Iter 61, disc loss: 5.466722457607757e-05, policy loss: 27.202014842582138
Experience 16, Iter 62, disc loss: 5.4537949066486664e-05, policy loss: 26.02264246643223
Experience 16, Iter 63, disc loss: 5.438742679923107e-05, policy loss: 26.19397574128439
Experience 16, Iter 64, disc loss: 5.424623747078962e-05, policy loss: 26.769372604983396
Experience 16, Iter 65, disc loss: 5.410612079056276e-05, policy loss: 26.579917681967896
Experience 16, Iter 66, disc loss: 5.401867188572559e-05, policy loss: 27.43144344733639
Experience 16, Iter 67, disc loss: 5.382940748931798e-05, policy loss: 26.153646801213533
Experience 16, Iter 68, disc loss: 5.3691013383750866e-05, policy loss: 26.823754773838807
Experience 16, Iter 69, disc loss: 5.355699544576425e-05, policy loss: 25.777168904222215
Experience 16, Iter 70, disc loss: 5.3415847301716015e-05, policy loss: 27.610697157909694
Experience 16, Iter 71, disc loss: 5.328308502357676e-05, policy loss: 26.19335424223727
Experience 16, Iter 72, disc loss: 5.318101554764976e-05, policy loss: 26.640987071692987
Experience 16, Iter 73, disc loss: 5.300858325800398e-05, policy loss: 26.78745006556229
Experience 16, Iter 74, disc loss: 5.2883104891979476e-05, policy loss: 24.87761171293284
Experience 16, Iter 75, disc loss: 5.2738667700992893e-05, policy loss: 27.27835377454144
Experience 16, Iter 76, disc loss: 5.26181402070809e-05, policy loss: 25.80985556374776
Experience 16, Iter 77, disc loss: 5.247058751355484e-05, policy loss: 26.890679038575446
Experience 16, Iter 78, disc loss: 5.233740290934568e-05, policy loss: 26.146222258751656
Experience 16, Iter 79, disc loss: 5.220475984340782e-05, policy loss: 26.70961986971109
Experience 16, Iter 80, disc loss: 5.207303867762823e-05, policy loss: 27.30782646614069
Experience 16, Iter 81, disc loss: 0.0011868815384339603, policy loss: 27.01397221876374
Experience 16, Iter 82, disc loss: 5.2069444336397794e-05, policy loss: 27.386709877387567
Experience 16, Iter 83, disc loss: 5.21662392830531e-05, policy loss: 27.246652245283624
Experience 16, Iter 84, disc loss: 5.2244629642077344e-05, policy loss: 26.37807273284384
Experience 16, Iter 85, disc loss: 5.2299906121556086e-05, policy loss: 27.337731051234655
Experience 16, Iter 86, disc loss: 5.233768473648832e-05, policy loss: 26.274512564933985
Experience 16, Iter 87, disc loss: 5.235854831000355e-05, policy loss: 25.869886798969915
Experience 16, Iter 88, disc loss: 5.236421887199822e-05, policy loss: 26.91469642262483
Experience 16, Iter 89, disc loss: 5.235643020911363e-05, policy loss: 26.706845471481778
Experience 16, Iter 90, disc loss: 5.233655307017982e-05, policy loss: 26.701653547083488
Experience 16, Iter 91, disc loss: 5.2306096501816456e-05, policy loss: 26.52567539071407
Experience 16, Iter 92, disc loss: 5.226436746718621e-05, policy loss: 26.613656694481463
Experience 16, Iter 93, disc loss: 5.221557456561144e-05, policy loss: 25.721334741021792
Experience 16, Iter 94, disc loss: 5.216327092262411e-05, policy loss: 26.506738956545583
Experience 16, Iter 95, disc loss: 5.2098119866343395e-05, policy loss: 26.495189200839175
Experience 16, Iter 96, disc loss: 5.20238252945069e-05, policy loss: 26.06821656751658
Experience 16, Iter 97, disc loss: 5.1948698478549763e-05, policy loss: 26.788833137207316
Experience 16, Iter 98, disc loss: 5.1860530998105385e-05, policy loss: 26.280329532914116
Experience 16, Iter 99, disc loss: 5.17925042518958e-05, policy loss: 27.215461957090007
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0119],
        [0.2432],
        [2.2137],
        [0.0344]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0567, 0.4690, 1.5687, 0.0355, 0.0131, 6.3462]],

        [[0.0567, 0.4690, 1.5687, 0.0355, 0.0131, 6.3462]],

        [[0.0567, 0.4690, 1.5687, 0.0355, 0.0131, 6.3462]],

        [[0.0567, 0.4690, 1.5687, 0.0355, 0.0131, 6.3462]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0475, 0.9727, 8.8550, 0.1377], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0475, 0.9727, 8.8550, 0.1377], device='cuda:0')
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.191
Iter 2/2000 - Loss: 4.140
Iter 3/2000 - Loss: 4.069
Iter 4/2000 - Loss: 4.014
Iter 5/2000 - Loss: 3.967
Iter 6/2000 - Loss: 3.896
Iter 7/2000 - Loss: 3.822
Iter 8/2000 - Loss: 3.746
Iter 9/2000 - Loss: 3.654
Iter 10/2000 - Loss: 3.543
Iter 11/2000 - Loss: 3.423
Iter 12/2000 - Loss: 3.297
Iter 13/2000 - Loss: 3.162
Iter 14/2000 - Loss: 3.011
Iter 15/2000 - Loss: 2.842
Iter 16/2000 - Loss: 2.657
Iter 17/2000 - Loss: 2.457
Iter 18/2000 - Loss: 2.243
Iter 19/2000 - Loss: 2.015
Iter 20/2000 - Loss: 1.771
Iter 1981/2000 - Loss: -6.407
Iter 1982/2000 - Loss: -6.407
Iter 1983/2000 - Loss: -6.407
Iter 1984/2000 - Loss: -6.407
Iter 1985/2000 - Loss: -6.407
Iter 1986/2000 - Loss: -6.407
Iter 1987/2000 - Loss: -6.407
Iter 1988/2000 - Loss: -6.407
Iter 1989/2000 - Loss: -6.407
Iter 1990/2000 - Loss: -6.407
Iter 1991/2000 - Loss: -6.407
Iter 1992/2000 - Loss: -6.407
Iter 1993/2000 - Loss: -6.407
Iter 1994/2000 - Loss: -6.407
Iter 1995/2000 - Loss: -6.407
Iter 1996/2000 - Loss: -6.407
Iter 1997/2000 - Loss: -6.408
Iter 1998/2000 - Loss: -6.408
Iter 1999/2000 - Loss: -6.408
Iter 2000/2000 - Loss: -6.408
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0025],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[18.9895, 13.9926, 39.3192, 14.4504, 10.2367, 67.7800]],

        [[25.6247, 47.0532,  8.1830,  1.3859,  1.4192, 36.1454]],

        [[26.7388, 52.2434,  8.0583,  0.9995,  0.8322, 19.8516]],

        [[22.9765, 47.1965, 18.2610,  1.1889,  2.6089, 48.9050]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2104,  3.1064, 11.4661,  0.5679], device='cuda:0')
Estimated target variance: tensor([0.0475, 0.9727, 8.8550, 0.1377], device='cuda:0')
N: 170
Signal to noise ratio: tensor([22.1306, 73.5466, 67.9517, 42.3243], device='cuda:0')
Bound on condition number: tensor([ 83260.6410, 919549.4703, 784965.5651, 304529.4294], device='cuda:0')
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 5.16833769991795e-05, policy loss: 25.968368463966712
Experience 17, Iter 1, disc loss: 5.1586000882274645e-05, policy loss: 26.964609383710098
Experience 17, Iter 2, disc loss: 5.148793794594392e-05, policy loss: 26.696149245938997
Experience 17, Iter 3, disc loss: 5.139827350341965e-05, policy loss: 27.013451840631248
Experience 17, Iter 4, disc loss: 5.1283986299700864e-05, policy loss: 26.75610825451517
Experience 17, Iter 5, disc loss: 5.1177906982686046e-05, policy loss: 26.69522645852114
Experience 17, Iter 6, disc loss: 5.1069538420043523e-05, policy loss: 27.275912559279078
Experience 17, Iter 7, disc loss: 5.096002994255982e-05, policy loss: 26.89332552285716
Experience 17, Iter 8, disc loss: 5.0851914836014314e-05, policy loss: 26.195310159583695
Experience 17, Iter 9, disc loss: 5.0736727762931716e-05, policy loss: 26.102436303740397
Experience 17, Iter 10, disc loss: 5.062354769557392e-05, policy loss: 26.571820584354988
Experience 17, Iter 11, disc loss: 5.051468736919906e-05, policy loss: 27.143718082171553
Experience 17, Iter 12, disc loss: 5.0393184328243075e-05, policy loss: 26.3680066822988
Experience 17, Iter 13, disc loss: 5.0277237452976125e-05, policy loss: 27.551921072862314
Experience 17, Iter 14, disc loss: 5.016087579949336e-05, policy loss: 26.59292269546637
Experience 17, Iter 15, disc loss: 9.481568280342801e-05, policy loss: 25.646342439128
Experience 17, Iter 16, disc loss: 4.9939953109393246e-05, policy loss: 25.97141744390015
Experience 17, Iter 17, disc loss: 4.9831902398415233e-05, policy loss: 27.188323970609655
Experience 17, Iter 18, disc loss: 4.9758200260942906e-05, policy loss: 25.511656934075873
Experience 17, Iter 19, disc loss: 4.9614803205067916e-05, policy loss: 26.408958830686217
Experience 17, Iter 20, disc loss: 4.95147899741934e-05, policy loss: 26.2664897860421
Experience 17, Iter 21, disc loss: 4.9393466130530265e-05, policy loss: 27.352755069353844
Experience 17, Iter 22, disc loss: 4.928393159068448e-05, policy loss: 25.860726105343208
Experience 17, Iter 23, disc loss: 4.9170143581132164e-05, policy loss: 26.744367272445928
Experience 17, Iter 24, disc loss: 4.9055841580094435e-05, policy loss: 26.934262638692367
Experience 17, Iter 25, disc loss: 4.894600939951158e-05, policy loss: 26.700724320607335
Experience 17, Iter 26, disc loss: 4.882889381148486e-05, policy loss: 26.658324601613668
Experience 17, Iter 27, disc loss: 4.873680410450953e-05, policy loss: 26.162931706565665
Experience 17, Iter 28, disc loss: 4.859991098281615e-05, policy loss: 27.67792414631816
Experience 17, Iter 29, disc loss: 4.848512174234371e-05, policy loss: 27.083088270529572
Experience 17, Iter 30, disc loss: 4.8370791483336905e-05, policy loss: 26.57053889301907
Experience 17, Iter 31, disc loss: 4.8256071107159915e-05, policy loss: 26.316733545801824
Experience 17, Iter 32, disc loss: 4.814173400528435e-05, policy loss: 25.803507130261792
Experience 17, Iter 33, disc loss: 4.802730650247191e-05, policy loss: 25.972260529853674
Experience 17, Iter 34, disc loss: 4.79124410720252e-05, policy loss: 26.81841821143231
Experience 17, Iter 35, disc loss: 4.780841774560047e-05, policy loss: 25.515093542080614
Experience 17, Iter 36, disc loss: 4.768503715145217e-05, policy loss: 25.779193581882595
Experience 17, Iter 37, disc loss: 4.756978584515965e-05, policy loss: 25.90009517962642
Experience 17, Iter 38, disc loss: 4.745496981905307e-05, policy loss: 26.00993205913994
Experience 17, Iter 39, disc loss: 4.734163606563049e-05, policy loss: 25.956820659191067
Experience 17, Iter 40, disc loss: 4.722742056280201e-05, policy loss: 26.653538087590924
Experience 17, Iter 41, disc loss: 4.7116615809398314e-05, policy loss: 25.734927881269822
Experience 17, Iter 42, disc loss: 4.707184270288098e-05, policy loss: 26.236621792738262
Experience 17, Iter 43, disc loss: 4.6888500121231946e-05, policy loss: 27.263443684498096
Experience 17, Iter 44, disc loss: 4.6776284373473046e-05, policy loss: 25.876091729979997
Experience 17, Iter 45, disc loss: 4.666397949222555e-05, policy loss: 26.22227043787884
Experience 17, Iter 46, disc loss: 4.655704160871274e-05, policy loss: 25.86343006269285
Experience 17, Iter 47, disc loss: 4.644273216965573e-05, policy loss: 26.634671476364502
Experience 17, Iter 48, disc loss: 4.641109347757798e-05, policy loss: 26.082097456069384
Experience 17, Iter 49, disc loss: 4.621861912090662e-05, policy loss: 26.461351752719953
Experience 17, Iter 50, disc loss: 4.6110476003615556e-05, policy loss: 26.233648397962472
Experience 17, Iter 51, disc loss: 4.599807781092297e-05, policy loss: 27.041345893469156
Experience 17, Iter 52, disc loss: 4.588834914933002e-05, policy loss: 26.59402425085046
Experience 17, Iter 53, disc loss: 4.579671652985439e-05, policy loss: 26.397568398838093
Experience 17, Iter 54, disc loss: 4.5670066034145574e-05, policy loss: 26.39071850445692
Experience 17, Iter 55, disc loss: 4.5571885413074464e-05, policy loss: 26.459886033405375
Experience 17, Iter 56, disc loss: 4.5457903733470975e-05, policy loss: 26.286711354836896
Experience 17, Iter 57, disc loss: 4.534492277128294e-05, policy loss: 26.038867848623347
Experience 17, Iter 58, disc loss: 4.5239376098937955e-05, policy loss: 25.761904476324794
Experience 17, Iter 59, disc loss: 4.513005039459169e-05, policy loss: 26.01395795733179
Experience 17, Iter 60, disc loss: 4.5051314520083066e-05, policy loss: 25.420000250840765
Experience 17, Iter 61, disc loss: 4.49162853272932e-05, policy loss: 26.1781217799008
Experience 17, Iter 62, disc loss: 4.4810064228083274e-05, policy loss: 27.041261288078466
Experience 17, Iter 63, disc loss: 4.4704653380486675e-05, policy loss: 26.31904336472862
Experience 17, Iter 64, disc loss: 4.4624879949426027e-05, policy loss: 26.75070525355685
Experience 17, Iter 65, disc loss: 4.449613325101802e-05, policy loss: 25.832907910467434
Experience 17, Iter 66, disc loss: 4.438876996405188e-05, policy loss: 26.873170861223073
Experience 17, Iter 67, disc loss: 4.428876928046458e-05, policy loss: 26.01194691161442
Experience 17, Iter 68, disc loss: 4.41804259515687e-05, policy loss: 26.805103040496363
Experience 17, Iter 69, disc loss: 4.407924770268964e-05, policy loss: 26.568114963061475
Experience 17, Iter 70, disc loss: 4.4002474315574586e-05, policy loss: 26.281821323340182
Experience 17, Iter 71, disc loss: 4.390601965983849e-05, policy loss: 26.11486195131687
Experience 17, Iter 72, disc loss: 4.3768241371759756e-05, policy loss: 27.25564413715964
Experience 17, Iter 73, disc loss: 4.366567039066612e-05, policy loss: 26.73393373618248
Experience 17, Iter 74, disc loss: 4.3576371393561986e-05, policy loss: 25.959235303495753
Experience 17, Iter 75, disc loss: 4.353404886601637e-05, policy loss: 26.469181981230747
Experience 17, Iter 76, disc loss: 4.336106065639765e-05, policy loss: 26.15507136299611
Experience 17, Iter 77, disc loss: 4.3818305568229214e-05, policy loss: 26.373492805913422
Experience 17, Iter 78, disc loss: 4.316894578375052e-05, policy loss: 25.992929559776947
Experience 17, Iter 79, disc loss: 4.3063356697295443e-05, policy loss: 25.72539633545547
Experience 17, Iter 80, disc loss: 4.296281164680253e-05, policy loss: 26.279313715133732
Experience 17, Iter 81, disc loss: 4.28616590357296e-05, policy loss: 27.098518729147287
Experience 17, Iter 82, disc loss: 4.2761580462321526e-05, policy loss: 27.185245280563556
Experience 17, Iter 83, disc loss: 4.269194988042883e-05, policy loss: 25.98722816505132
Experience 17, Iter 84, disc loss: 4.25744849838776e-05, policy loss: 25.690028837843986
Experience 17, Iter 85, disc loss: 4.246641469026865e-05, policy loss: 25.774810512032015
Experience 17, Iter 86, disc loss: 4.255646141156644e-05, policy loss: 26.15610968223387
Experience 17, Iter 87, disc loss: 4.227109782098629e-05, policy loss: 26.60810273904744
Experience 17, Iter 88, disc loss: 4.222745662722668e-05, policy loss: 26.338783031058256
Experience 17, Iter 89, disc loss: 4.207746858762284e-05, policy loss: 26.638938412150942
Experience 17, Iter 90, disc loss: 4.198887256967436e-05, policy loss: 25.86489359327656
Experience 17, Iter 91, disc loss: 4.1886159598386914e-05, policy loss: 26.612240104436914
Experience 17, Iter 92, disc loss: 4.179299427160287e-05, policy loss: 26.369880563165285
Experience 17, Iter 93, disc loss: 4.169407278393976e-05, policy loss: 26.055929367123554
Experience 17, Iter 94, disc loss: 4.1917512508687824e-05, policy loss: 26.80690584559442
Experience 17, Iter 95, disc loss: 4.386036428740099e-05, policy loss: 25.88011604890923
Experience 17, Iter 96, disc loss: 4.1424797753397286e-05, policy loss: 27.094280234969325
Experience 17, Iter 97, disc loss: 4.1318648342312745e-05, policy loss: 26.072211602481826
Experience 17, Iter 98, disc loss: 0.04047508115716939, policy loss: 25.929177180057643
Experience 17, Iter 99, disc loss: 4.3459429072091664e-05, policy loss: 26.50508653354065
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0113],
        [0.2470],
        [2.2235],
        [0.0341]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0541, 0.4481, 1.5591, 0.0354, 0.0126, 6.5003]],

        [[0.0541, 0.4481, 1.5591, 0.0354, 0.0126, 6.5003]],

        [[0.0541, 0.4481, 1.5591, 0.0354, 0.0126, 6.5003]],

        [[0.0541, 0.4481, 1.5591, 0.0354, 0.0126, 6.5003]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0452, 0.9880, 8.8940, 0.1365], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0452, 0.9880, 8.8940, 0.1365], device='cuda:0')
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.152
Iter 2/2000 - Loss: 4.092
Iter 3/2000 - Loss: 4.015
Iter 4/2000 - Loss: 3.956
Iter 5/2000 - Loss: 3.904
Iter 6/2000 - Loss: 3.828
Iter 7/2000 - Loss: 3.750
Iter 8/2000 - Loss: 3.675
Iter 9/2000 - Loss: 3.583
Iter 10/2000 - Loss: 3.469
Iter 11/2000 - Loss: 3.346
Iter 12/2000 - Loss: 3.220
Iter 13/2000 - Loss: 3.085
Iter 14/2000 - Loss: 2.935
Iter 15/2000 - Loss: 2.767
Iter 16/2000 - Loss: 2.582
Iter 17/2000 - Loss: 2.384
Iter 18/2000 - Loss: 2.173
Iter 19/2000 - Loss: 1.948
Iter 20/2000 - Loss: 1.706
Iter 1981/2000 - Loss: -6.517
Iter 1982/2000 - Loss: -6.517
Iter 1983/2000 - Loss: -6.517
Iter 1984/2000 - Loss: -6.517
Iter 1985/2000 - Loss: -6.517
Iter 1986/2000 - Loss: -6.517
Iter 1987/2000 - Loss: -6.517
Iter 1988/2000 - Loss: -6.517
Iter 1989/2000 - Loss: -6.517
Iter 1990/2000 - Loss: -6.517
Iter 1991/2000 - Loss: -6.517
Iter 1992/2000 - Loss: -6.517
Iter 1993/2000 - Loss: -6.517
Iter 1994/2000 - Loss: -6.518
Iter 1995/2000 - Loss: -6.518
Iter 1996/2000 - Loss: -6.518
Iter 1997/2000 - Loss: -6.518
Iter 1998/2000 - Loss: -6.518
Iter 1999/2000 - Loss: -6.518
Iter 2000/2000 - Loss: -6.518
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[18.6693, 13.8834, 39.6691, 12.5168,  8.2969, 66.9978]],

        [[25.7796, 48.0474,  7.8587,  1.3444,  1.4988, 36.6376]],

        [[27.5529, 51.2951,  8.0458,  0.9466,  0.8449, 20.3522]],

        [[22.3383, 46.1066, 18.3687,  1.2177,  2.4792, 48.7228]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2086,  3.0209, 11.3508,  0.5619], device='cuda:0')
Estimated target variance: tensor([0.0452, 0.9880, 8.8940, 0.1365], device='cuda:0')
N: 180
Signal to noise ratio: tensor([21.9059, 72.9182, 68.2340, 42.6819], device='cuda:0')
Bound on condition number: tensor([ 86377.4401, 957071.7182, 838059.4897, 327914.5678], device='cuda:0')
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 4.556274385933794e-05, policy loss: 25.771491658684447
Experience 18, Iter 1, disc loss: 4.753811381608152e-05, policy loss: 26.11606319170957
Experience 18, Iter 2, disc loss: 4.935508406788012e-05, policy loss: 28.248628078841143
Experience 18, Iter 3, disc loss: 5.104409713650835e-05, policy loss: 26.43670393444273
Experience 18, Iter 4, disc loss: 5.261413181721375e-05, policy loss: 27.119340338556793
Experience 18, Iter 5, disc loss: 5.40308077376392e-05, policy loss: 25.890415787263983
Experience 18, Iter 6, disc loss: 5.5337882628379715e-05, policy loss: 26.12221683208968
Experience 18, Iter 7, disc loss: 5.653073482484579e-05, policy loss: 27.33724914446299
Experience 18, Iter 8, disc loss: 5.762467075323288e-05, policy loss: 27.344228015441907
Experience 18, Iter 9, disc loss: 5.858991564677281e-05, policy loss: 26.980622823254052
Experience 18, Iter 10, disc loss: 5.948417443347958e-05, policy loss: 26.37938901283226
Experience 18, Iter 11, disc loss: 6.031839438264352e-05, policy loss: 26.522230101135886
Experience 18, Iter 12, disc loss: 6.097806679871738e-05, policy loss: 27.96258706465941
Experience 18, Iter 13, disc loss: 6.161294923842695e-05, policy loss: 26.670815396170674
Experience 18, Iter 14, disc loss: 6.217717652900414e-05, policy loss: 27.43688782332984
Experience 18, Iter 15, disc loss: 6.267766924375031e-05, policy loss: 26.50166096052599
Experience 18, Iter 16, disc loss: 6.31372295616372e-05, policy loss: 27.02218287375435
Experience 18, Iter 17, disc loss: 6.350824987520576e-05, policy loss: 27.381804596999196
Experience 18, Iter 18, disc loss: 6.384138284638131e-05, policy loss: 26.936247279134783
Experience 18, Iter 19, disc loss: 6.415670887911719e-05, policy loss: 26.157107224141576
Experience 18, Iter 20, disc loss: 6.438401961124692e-05, policy loss: 27.67292763300714
Experience 18, Iter 21, disc loss: 6.459760949022811e-05, policy loss: 27.49854883207771
Experience 18, Iter 22, disc loss: 6.59874829550641e-05, policy loss: 27.1040403883094
Experience 18, Iter 23, disc loss: 6.492664866359809e-05, policy loss: 26.518583217121336
Experience 18, Iter 24, disc loss: 6.504831232438435e-05, policy loss: 26.859299546048415
Experience 18, Iter 25, disc loss: 6.514362991833814e-05, policy loss: 26.79520826454707
Experience 18, Iter 26, disc loss: 6.521608355170846e-05, policy loss: 26.68468613335272
Experience 18, Iter 27, disc loss: 6.527063050243873e-05, policy loss: 26.56926487838697
Experience 18, Iter 28, disc loss: 6.530146535271694e-05, policy loss: 26.897119350491177
Experience 18, Iter 29, disc loss: 6.531772367531794e-05, policy loss: 27.314314281031518
Experience 18, Iter 30, disc loss: 6.554067411387582e-05, policy loss: 27.400156862456907
Experience 18, Iter 31, disc loss: 6.530660394374227e-05, policy loss: 27.577993069489608
Experience 18, Iter 32, disc loss: 6.528179200821654e-05, policy loss: 27.465085729204972
Experience 18, Iter 33, disc loss: 6.524906360610655e-05, policy loss: 26.918246921021645
Experience 18, Iter 34, disc loss: 6.520031896700129e-05, policy loss: 27.612868363623896
Experience 18, Iter 35, disc loss: 6.514518269756922e-05, policy loss: 27.24626246575179
Experience 18, Iter 36, disc loss: 6.50864583580349e-05, policy loss: 26.411849406680705
Experience 18, Iter 37, disc loss: 6.501183098590332e-05, policy loss: 27.547514506900086
Experience 18, Iter 38, disc loss: 6.493479912916877e-05, policy loss: 26.91866886966001
Experience 18, Iter 39, disc loss: 6.485219279510634e-05, policy loss: 26.79465377745946
Experience 18, Iter 40, disc loss: 6.482707297864821e-05, policy loss: 27.222181645869593
Experience 18, Iter 41, disc loss: 6.474638996680953e-05, policy loss: 26.392518206415957
Experience 18, Iter 42, disc loss: 6.465311436899414e-05, policy loss: 27.309281410202995
Experience 18, Iter 43, disc loss: 6.447471152057424e-05, policy loss: 26.846154314168146
Experience 18, Iter 44, disc loss: 6.46143076068535e-05, policy loss: 26.21445875949084
Experience 18, Iter 45, disc loss: 6.42658528984168e-05, policy loss: 25.639252899606923
Experience 18, Iter 46, disc loss: 6.415509540315862e-05, policy loss: 25.884451283173572
Experience 18, Iter 47, disc loss: 6.404343232252551e-05, policy loss: 27.30410718432804
Experience 18, Iter 48, disc loss: 6.393033413536934e-05, policy loss: 26.900736966456748
Experience 18, Iter 49, disc loss: 6.406226620184446e-05, policy loss: 26.431129079545407
Experience 18, Iter 50, disc loss: 6.369914673538132e-05, policy loss: 27.191341959808227
Experience 18, Iter 51, disc loss: 6.357964366627837e-05, policy loss: 26.23354292868857
Experience 18, Iter 52, disc loss: 6.34593334914617e-05, policy loss: 27.451780625555323
Experience 18, Iter 53, disc loss: 6.333898509994801e-05, policy loss: 27.347440008895088
Experience 18, Iter 54, disc loss: 6.444163268961928e-05, policy loss: 26.833466861866274
Experience 18, Iter 55, disc loss: 6.309498099002629e-05, policy loss: 26.938088631653923
Experience 18, Iter 56, disc loss: 6.298318719644403e-05, policy loss: 26.472683226636516
Experience 18, Iter 57, disc loss: 6.285769317828254e-05, policy loss: 27.640438838417097
Experience 18, Iter 58, disc loss: 6.273158036734998e-05, policy loss: 27.214667151846943
Experience 18, Iter 59, disc loss: 6.260084405163176e-05, policy loss: 26.937676670652188
Experience 18, Iter 60, disc loss: 6.256005360718532e-05, policy loss: 26.444463062071794
Experience 18, Iter 61, disc loss: 6.235193716718961e-05, policy loss: 26.952266040468857
Experience 18, Iter 62, disc loss: 6.222583374730826e-05, policy loss: 26.287045691477708
Experience 18, Iter 63, disc loss: 6.210253149292135e-05, policy loss: 27.536186948937036
Experience 18, Iter 64, disc loss: 6.197525666296345e-05, policy loss: 26.575870354275303
Experience 18, Iter 65, disc loss: 6.185544553847572e-05, policy loss: 26.735766912493176
Experience 18, Iter 66, disc loss: 6.172500387618535e-05, policy loss: 27.19151372065976
Experience 18, Iter 67, disc loss: 6.18932232295818e-05, policy loss: 26.294082136839194
Experience 18, Iter 68, disc loss: 6.147427196950368e-05, policy loss: 27.002834051550813
Experience 18, Iter 69, disc loss: 6.134932629369268e-05, policy loss: 26.684623610755768
Experience 18, Iter 70, disc loss: 6.124182938586318e-05, policy loss: 27.01778868435956
Experience 18, Iter 71, disc loss: 6.11000171582396e-05, policy loss: 27.50407613486312
Experience 18, Iter 72, disc loss: 6.097942497920144e-05, policy loss: 26.101166980003036
Experience 18, Iter 73, disc loss: 6.085195820374954e-05, policy loss: 26.359745878266622
Experience 18, Iter 74, disc loss: 6.0729895171848933e-05, policy loss: 26.820620993984736
Experience 18, Iter 75, disc loss: 6.060362171171069e-05, policy loss: 26.63887882977641
Experience 18, Iter 76, disc loss: 6.0479894955096686e-05, policy loss: 26.95433134261097
Experience 18, Iter 77, disc loss: 6.0360937035847974e-05, policy loss: 26.10754516427731
Experience 18, Iter 78, disc loss: 6.023396651126598e-05, policy loss: 27.04563173928055
Experience 18, Iter 79, disc loss: 6.011058519202367e-05, policy loss: 27.145177297319343
Experience 18, Iter 80, disc loss: 5.998891633350575e-05, policy loss: 26.85233387720063
Experience 18, Iter 81, disc loss: 5.9907633599165505e-05, policy loss: 26.291277516876043
Experience 18, Iter 82, disc loss: 5.974469584613528e-05, policy loss: 26.71697488500491
Experience 18, Iter 83, disc loss: 5.962598047931357e-05, policy loss: 26.689712886833995
Experience 18, Iter 84, disc loss: 5.9502606471243716e-05, policy loss: 27.081201480656016
Experience 18, Iter 85, disc loss: 5.9396578520020475e-05, policy loss: 27.56787508346803
Experience 18, Iter 86, disc loss: 5.92598971986399e-05, policy loss: 26.45304047301325
Experience 18, Iter 87, disc loss: 5.913945472020193e-05, policy loss: 26.78413220213175
Experience 18, Iter 88, disc loss: 5.9019832734203376e-05, policy loss: 27.346513434220178
Experience 18, Iter 89, disc loss: 5.8900077281553974e-05, policy loss: 27.16493683641226
Experience 18, Iter 90, disc loss: 5.8787065662412686e-05, policy loss: 26.62682785326718
Experience 18, Iter 91, disc loss: 5.866200448800201e-05, policy loss: 25.59972079563164
Experience 18, Iter 92, disc loss: 5.854307534003181e-05, policy loss: 27.036677510173284
Experience 18, Iter 93, disc loss: 5.842454225435881e-05, policy loss: 26.563040203909797
Experience 18, Iter 94, disc loss: 5.8351803150717825e-05, policy loss: 25.69641462967006
Experience 18, Iter 95, disc loss: 5.819091991480559e-05, policy loss: 27.117393684441616
Experience 18, Iter 96, disc loss: 5.807153989457182e-05, policy loss: 27.05951995812442
Experience 18, Iter 97, disc loss: 5.795596753543257e-05, policy loss: 27.486484342785385
Experience 18, Iter 98, disc loss: 5.783805151846981e-05, policy loss: 26.630710994362744
Experience 18, Iter 99, disc loss: 5.777709262708942e-05, policy loss: 26.730302177377464
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0114],
        [0.2475],
        [2.2238],
        [0.0336]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0540, 0.4534, 1.5406, 0.0348, 0.0121, 6.5102]],

        [[0.0540, 0.4534, 1.5406, 0.0348, 0.0121, 6.5102]],

        [[0.0540, 0.4534, 1.5406, 0.0348, 0.0121, 6.5102]],

        [[0.0540, 0.4534, 1.5406, 0.0348, 0.0121, 6.5102]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0456, 0.9898, 8.8953, 0.1344], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0456, 0.9898, 8.8953, 0.1344], device='cuda:0')
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.118
Iter 2/2000 - Loss: 4.040
Iter 3/2000 - Loss: 3.971
Iter 4/2000 - Loss: 3.896
Iter 5/2000 - Loss: 3.837
Iter 6/2000 - Loss: 3.763
Iter 7/2000 - Loss: 3.680
Iter 8/2000 - Loss: 3.594
Iter 9/2000 - Loss: 3.495
Iter 10/2000 - Loss: 3.379
Iter 11/2000 - Loss: 3.251
Iter 12/2000 - Loss: 3.117
Iter 13/2000 - Loss: 2.974
Iter 14/2000 - Loss: 2.818
Iter 15/2000 - Loss: 2.645
Iter 16/2000 - Loss: 2.458
Iter 17/2000 - Loss: 2.257
Iter 18/2000 - Loss: 2.044
Iter 19/2000 - Loss: 1.817
Iter 20/2000 - Loss: 1.575
Iter 1981/2000 - Loss: -6.607
Iter 1982/2000 - Loss: -6.607
Iter 1983/2000 - Loss: -6.607
Iter 1984/2000 - Loss: -6.607
Iter 1985/2000 - Loss: -6.607
Iter 1986/2000 - Loss: -6.607
Iter 1987/2000 - Loss: -6.607
Iter 1988/2000 - Loss: -6.607
Iter 1989/2000 - Loss: -6.607
Iter 1990/2000 - Loss: -6.607
Iter 1991/2000 - Loss: -6.607
Iter 1992/2000 - Loss: -6.607
Iter 1993/2000 - Loss: -6.607
Iter 1994/2000 - Loss: -6.607
Iter 1995/2000 - Loss: -6.607
Iter 1996/2000 - Loss: -6.607
Iter 1997/2000 - Loss: -6.608
Iter 1998/2000 - Loss: -6.608
Iter 1999/2000 - Loss: -6.608
Iter 2000/2000 - Loss: -6.608
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0025],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[18.7568, 13.8517, 39.2071, 11.0685,  5.6918, 65.2543]],

        [[25.3160, 47.4621,  7.7871,  1.3496,  1.5121, 36.9650]],

        [[27.5855, 51.2249,  7.8960,  0.9516,  0.8488, 20.3704]],

        [[22.3012, 46.0335, 19.1309,  1.2923,  2.5516, 43.7654]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2113,  3.0510, 11.2367,  0.5798], device='cuda:0')
Estimated target variance: tensor([0.0456, 0.9898, 8.8953, 0.1344], device='cuda:0')
N: 190
Signal to noise ratio: tensor([22.2419, 74.4989, 67.2819, 42.3693], device='cuda:0')
Bound on condition number: tensor([  93994.4993, 1054518.3156,  860102.0830,  341081.5400],
       device='cuda:0')
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 5.762380338822628e-05, policy loss: 26.120546878168824
Experience 19, Iter 1, disc loss: 0.0001353105024239625, policy loss: 26.77239234829231
Experience 19, Iter 2, disc loss: 5.739150627283925e-05, policy loss: 26.307078035555676
Experience 19, Iter 3, disc loss: 5.7292332455930366e-05, policy loss: 26.09878129145645
Experience 19, Iter 4, disc loss: 5.720031622797259e-05, policy loss: 26.82535553878679
Experience 19, Iter 5, disc loss: 5.708876127067775e-05, policy loss: 26.78927640666607
Experience 19, Iter 6, disc loss: 5.702217236802577e-05, policy loss: 26.47413907520773
Experience 19, Iter 7, disc loss: 5.688339628704572e-05, policy loss: 27.160596010349938
Experience 19, Iter 8, disc loss: 5.677683918626851e-05, policy loss: 26.006432861192682
Experience 19, Iter 9, disc loss: 5.66722371206786e-05, policy loss: 26.40688042662989
Experience 19, Iter 10, disc loss: 5.6566150017337454e-05, policy loss: 26.95549823594909
Experience 19, Iter 11, disc loss: 5.645972570514018e-05, policy loss: 26.600475207332394
Experience 19, Iter 12, disc loss: 5.6358508761086406e-05, policy loss: 26.598743988682244
Experience 19, Iter 13, disc loss: 5.6246737365418e-05, policy loss: 27.080192824046627
Experience 19, Iter 14, disc loss: 5.6142585670820305e-05, policy loss: 26.704914196431314
Experience 19, Iter 15, disc loss: 5.603235621847283e-05, policy loss: 26.735053055320442
Experience 19, Iter 16, disc loss: 5.594943822892923e-05, policy loss: 26.64842810043281
Experience 19, Iter 17, disc loss: 5.581772471979671e-05, policy loss: 26.719008131282564
Experience 19, Iter 18, disc loss: 5.57104393200032e-05, policy loss: 26.71247824689064
Experience 19, Iter 19, disc loss: 5.560355812211759e-05, policy loss: 26.492331180255732
Experience 19, Iter 20, disc loss: 5.54982505881841e-05, policy loss: 26.687141548424016
Experience 19, Iter 21, disc loss: 5.538910891668133e-05, policy loss: 25.3791825146858
Experience 19, Iter 22, disc loss: 5.528250463313063e-05, policy loss: 26.869296362249695
Experience 19, Iter 23, disc loss: 5.518161884738565e-05, policy loss: 27.37949826373967
Experience 19, Iter 24, disc loss: 5.506978059258569e-05, policy loss: 26.383919315022204
Experience 19, Iter 25, disc loss: 5.496071122762139e-05, policy loss: 26.483028635662162
Experience 19, Iter 26, disc loss: 5.576247746231873e-05, policy loss: 26.129240521261707
Experience 19, Iter 27, disc loss: 5.474944391691391e-05, policy loss: 26.65393999696606
Experience 19, Iter 28, disc loss: 5.4641375218948964e-05, policy loss: 26.736389245671255
Experience 19, Iter 29, disc loss: 5.45360934167837e-05, policy loss: 25.79992647780838
Experience 19, Iter 30, disc loss: 5.460332408142227e-05, policy loss: 25.404976114270127
Experience 19, Iter 31, disc loss: 5.432351242570928e-05, policy loss: 26.153785688838912
Experience 19, Iter 32, disc loss: 5.421853755378723e-05, policy loss: 26.13281028319298
Experience 19, Iter 33, disc loss: 5.411468776040988e-05, policy loss: 26.561905244607555
Experience 19, Iter 34, disc loss: 5.400798636014897e-05, policy loss: 26.543085690608642
Experience 19, Iter 35, disc loss: 5.497784053559408e-05, policy loss: 26.717868621724726
Experience 19, Iter 36, disc loss: 5.379886650318456e-05, policy loss: 27.324500834306747
Experience 19, Iter 37, disc loss: 5.369465378096338e-05, policy loss: 27.286288199550558
Experience 19, Iter 38, disc loss: 5.3629870023436606e-05, policy loss: 26.650977953885942
Experience 19, Iter 39, disc loss: 5.3486887717032284e-05, policy loss: 26.40828915020623
Experience 19, Iter 40, disc loss: 5.3383480431921256e-05, policy loss: 25.50184532518469
Experience 19, Iter 41, disc loss: 5.3280212264933655e-05, policy loss: 26.05865013607607
Experience 19, Iter 42, disc loss: 5.3177241962359466e-05, policy loss: 26.081022419280764
Experience 19, Iter 43, disc loss: 5.307512624083249e-05, policy loss: 26.348758851721605
Experience 19, Iter 44, disc loss: 5.297227817144809e-05, policy loss: 26.02746116813178
Experience 19, Iter 45, disc loss: 5.287142990484295e-05, policy loss: 26.866311357914558
Experience 19, Iter 46, disc loss: 5.278018917788352e-05, policy loss: 26.12907112426743
Experience 19, Iter 47, disc loss: 5.2666289632655465e-05, policy loss: 27.23185391185742
Experience 19, Iter 48, disc loss: 5.256500685994411e-05, policy loss: 26.602708693061032
Experience 19, Iter 49, disc loss: 5.246510236310448e-05, policy loss: 26.832506131427586
Experience 19, Iter 50, disc loss: 5.236326270917893e-05, policy loss: 26.491415778334378
Experience 19, Iter 51, disc loss: 5.226273271680826e-05, policy loss: 26.779098334537064
Experience 19, Iter 52, disc loss: 5.2162682640150786e-05, policy loss: 26.378446986014044
Experience 19, Iter 53, disc loss: 5.212292954347114e-05, policy loss: 26.412637790045515
Experience 19, Iter 54, disc loss: 5.196240322248005e-05, policy loss: 27.644076581057625
Experience 19, Iter 55, disc loss: 5.186460028973167e-05, policy loss: 26.23919498831421
Experience 19, Iter 56, disc loss: 5.1769950146190355e-05, policy loss: 27.66745124625976
Experience 19, Iter 57, disc loss: 5.169512922373803e-05, policy loss: 26.721975386448513
Experience 19, Iter 58, disc loss: 5.157716415256633e-05, policy loss: 26.170205711623222
Experience 19, Iter 59, disc loss: 5.146799265541935e-05, policy loss: 27.82294162249208
Experience 19, Iter 60, disc loss: 5.1378906357348706e-05, policy loss: 26.514856410011312
Experience 19, Iter 61, disc loss: 5.127186560721265e-05, policy loss: 26.888088750809892
Experience 19, Iter 62, disc loss: 5.117543802928389e-05, policy loss: 26.08604012504676
Experience 19, Iter 63, disc loss: 5.11078722040782e-05, policy loss: 26.411549601400225
Experience 19, Iter 64, disc loss: 5.098005999365186e-05, policy loss: 27.015505620103923
Experience 19, Iter 65, disc loss: 5.088316136125929e-05, policy loss: 26.50207838018477
Experience 19, Iter 66, disc loss: 5.0786984214494e-05, policy loss: 26.569339557901312
Experience 19, Iter 67, disc loss: 5.070505091631576e-05, policy loss: 26.145635670168787
Experience 19, Iter 68, disc loss: 5.059454434253787e-05, policy loss: 26.41636142264463
Experience 19, Iter 69, disc loss: 5.049880389754821e-05, policy loss: 26.479921123495764
Experience 19, Iter 70, disc loss: 5.040338284603415e-05, policy loss: 27.203652376392192
Experience 19, Iter 71, disc loss: 5.0309423219408855e-05, policy loss: 26.34315941210298
Experience 19, Iter 72, disc loss: 5.021328751356483e-05, policy loss: 26.82464272754359
Experience 19, Iter 73, disc loss: 5.012016653221005e-05, policy loss: 25.736775607148456
Experience 19, Iter 74, disc loss: 5.0024089866672285e-05, policy loss: 26.819921687625936
Experience 19, Iter 75, disc loss: 4.9971042279163846e-05, policy loss: 26.555386247784924
Experience 19, Iter 76, disc loss: 4.993061073983971e-05, policy loss: 26.74509798250012
Experience 19, Iter 77, disc loss: 4.9749551471476646e-05, policy loss: 26.435792026902686
Experience 19, Iter 78, disc loss: 4.964967255302886e-05, policy loss: 27.46042154180107
Experience 19, Iter 79, disc loss: 4.956068631388123e-05, policy loss: 26.52672738829417
Experience 19, Iter 80, disc loss: 4.9463106536033587e-05, policy loss: 26.959085932238594
Experience 19, Iter 81, disc loss: 4.937064054578766e-05, policy loss: 27.336291696431328
Experience 19, Iter 82, disc loss: 4.9283794805429204e-05, policy loss: 26.10261413301988
Experience 19, Iter 83, disc loss: 4.918697382710201e-05, policy loss: 26.503609937289536
Experience 19, Iter 84, disc loss: 4.90969330617429e-05, policy loss: 25.196000105111167
Experience 19, Iter 85, disc loss: 4.900265011437173e-05, policy loss: 26.474030457531125
Experience 19, Iter 86, disc loss: 4.891117986504134e-05, policy loss: 26.95158367637024
Experience 19, Iter 87, disc loss: 4.8821007118943726e-05, policy loss: 25.786845212171627
Experience 19, Iter 88, disc loss: 4.872934763929546e-05, policy loss: 26.614312620558344
Experience 19, Iter 89, disc loss: 4.872286893224887e-05, policy loss: 25.83988559058374
Experience 19, Iter 90, disc loss: 4.854818894392722e-05, policy loss: 26.678500103382053
Experience 19, Iter 91, disc loss: 4.848474189661576e-05, policy loss: 27.328042717052995
Experience 19, Iter 92, disc loss: 4.8368985361869696e-05, policy loss: 26.39878054532198
Experience 19, Iter 93, disc loss: 4.827904060948567e-05, policy loss: 26.623421546028133
Experience 19, Iter 94, disc loss: 4.8362735871347346e-05, policy loss: 26.844455770700772
Experience 19, Iter 95, disc loss: 4.8470988407942246e-05, policy loss: 27.068098175898818
Experience 19, Iter 96, disc loss: 4.8011348128482515e-05, policy loss: 26.95622242956214
Experience 19, Iter 97, disc loss: 4.801277050043255e-05, policy loss: 26.09016747329787
Experience 19, Iter 98, disc loss: 4.783418255040194e-05, policy loss: 26.172081149370136
Experience 19, Iter 99, disc loss: 4.7746214800437094e-05, policy loss: 25.67820459202388
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0111],
        [0.2463],
        [2.2446],
        [0.0348]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0560, 0.4448, 1.6019, 0.0356, 0.0120, 6.4348]],

        [[0.0560, 0.4448, 1.6019, 0.0356, 0.0120, 6.4348]],

        [[0.0560, 0.4448, 1.6019, 0.0356, 0.0120, 6.4348]],

        [[0.0560, 0.4448, 1.6019, 0.0356, 0.0120, 6.4348]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0445, 0.9851, 8.9783, 0.1394], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0445, 0.9851, 8.9783, 0.1394], device='cuda:0')
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.135
Iter 2/2000 - Loss: 4.041
Iter 3/2000 - Loss: 3.982
Iter 4/2000 - Loss: 3.902
Iter 5/2000 - Loss: 3.837
Iter 6/2000 - Loss: 3.766
Iter 7/2000 - Loss: 3.682
Iter 8/2000 - Loss: 3.591
Iter 9/2000 - Loss: 3.493
Iter 10/2000 - Loss: 3.379
Iter 11/2000 - Loss: 3.251
Iter 12/2000 - Loss: 3.115
Iter 13/2000 - Loss: 2.970
Iter 14/2000 - Loss: 2.813
Iter 15/2000 - Loss: 2.641
Iter 16/2000 - Loss: 2.452
Iter 17/2000 - Loss: 2.249
Iter 18/2000 - Loss: 2.032
Iter 19/2000 - Loss: 1.801
Iter 20/2000 - Loss: 1.555
Iter 1981/2000 - Loss: -6.649
Iter 1982/2000 - Loss: -6.649
Iter 1983/2000 - Loss: -6.649
Iter 1984/2000 - Loss: -6.649
Iter 1985/2000 - Loss: -6.649
Iter 1986/2000 - Loss: -6.649
Iter 1987/2000 - Loss: -6.649
Iter 1988/2000 - Loss: -6.649
Iter 1989/2000 - Loss: -6.649
Iter 1990/2000 - Loss: -6.649
Iter 1991/2000 - Loss: -6.649
Iter 1992/2000 - Loss: -6.649
Iter 1993/2000 - Loss: -6.650
Iter 1994/2000 - Loss: -6.650
Iter 1995/2000 - Loss: -6.650
Iter 1996/2000 - Loss: -6.650
Iter 1997/2000 - Loss: -6.650
Iter 1998/2000 - Loss: -6.650
Iter 1999/2000 - Loss: -6.650
Iter 2000/2000 - Loss: -6.650
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[18.6143, 13.9208, 38.4699,  8.2086,  3.8438, 65.8893]],

        [[25.7414, 47.3439,  8.0357,  1.3119,  1.3298, 35.2516]],

        [[27.5980, 50.3982,  7.8908,  0.9436,  0.8376, 20.8290]],

        [[21.6086, 44.2838, 18.6376,  1.4367,  2.5302, 39.3118]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2060,  2.8516, 11.2541,  0.5544], device='cuda:0')
Estimated target variance: tensor([0.0445, 0.9851, 8.9783, 0.1394], device='cuda:0')
N: 200
Signal to noise ratio: tensor([22.6259, 72.2759, 67.8874, 40.0255], device='cuda:0')
Bound on condition number: tensor([ 102387.2142, 1044762.0949,  921740.2428,  320408.9299],
       device='cuda:0')
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 4.765781773606423e-05, policy loss: 26.866640261048765
Experience 20, Iter 1, disc loss: 4.758940176395398e-05, policy loss: 26.395397399760665
Experience 20, Iter 2, disc loss: 4.7642874300868915e-05, policy loss: 27.10917946618048
Experience 20, Iter 3, disc loss: 4.7396227220940534e-05, policy loss: 26.335912366555156
Experience 20, Iter 4, disc loss: 4.730882325930337e-05, policy loss: 26.888757677778933
Experience 20, Iter 5, disc loss: 4.722176385324612e-05, policy loss: 26.566723917209575
Experience 20, Iter 6, disc loss: 4.713694772779185e-05, policy loss: 26.83994639407247
Experience 20, Iter 7, disc loss: 4.7049781342097145e-05, policy loss: 26.757247024254756
Experience 20, Iter 8, disc loss: 4.7020282311898977e-05, policy loss: 25.748625292591285
Experience 20, Iter 9, disc loss: 4.687685301044258e-05, policy loss: 25.871914905989616
Experience 20, Iter 10, disc loss: 4.679247397863949e-05, policy loss: 26.982146974229167
Experience 20, Iter 11, disc loss: 4.675134765226457e-05, policy loss: 26.87033922750865
Experience 20, Iter 12, disc loss: 4.662055719211e-05, policy loss: 26.18160045751194
Experience 20, Iter 13, disc loss: 4.653569903390432e-05, policy loss: 26.936424197511776
Experience 20, Iter 14, disc loss: 4.64962954296363e-05, policy loss: 26.897778117912814
Experience 20, Iter 15, disc loss: 4.636688694392766e-05, policy loss: 26.83331603540776
Experience 20, Iter 16, disc loss: 4.6282347526768696e-05, policy loss: 25.41195551988035
Experience 20, Iter 17, disc loss: 4.6198293482525676e-05, policy loss: 25.508888971021943
Experience 20, Iter 18, disc loss: 4.6115206362249305e-05, policy loss: 26.188944688346183
Experience 20, Iter 19, disc loss: 4.604347234045793e-05, policy loss: 25.496457384101955
Experience 20, Iter 20, disc loss: 4.594737036184178e-05, policy loss: 26.88548346059251
Experience 20, Iter 21, disc loss: 4.586401762928639e-05, policy loss: 27.214936013391664
Experience 20, Iter 22, disc loss: 4.578096302398115e-05, policy loss: 27.086649283457593
Experience 20, Iter 23, disc loss: 4.5723806398058005e-05, policy loss: 26.59232872186627
Experience 20, Iter 24, disc loss: 4.561564299017045e-05, policy loss: 26.54413142693828
Experience 20, Iter 25, disc loss: 4.554539343692273e-05, policy loss: 26.66167592626048
Experience 20, Iter 26, disc loss: 4.5492630873820236e-05, policy loss: 26.410929328881586
Experience 20, Iter 27, disc loss: 4.5370594925717184e-05, policy loss: 26.724489202105225
Experience 20, Iter 28, disc loss: 4.528826854299576e-05, policy loss: 26.427323052763157
Experience 20, Iter 29, disc loss: 4.529286134683044e-05, policy loss: 26.426258202337046
Experience 20, Iter 30, disc loss: 4.512694626469648e-05, policy loss: 26.60471761354441
Experience 20, Iter 31, disc loss: 4.504616086228273e-05, policy loss: 25.518731747706468
Experience 20, Iter 32, disc loss: 4.4963137032518135e-05, policy loss: 26.840432325880727
Experience 20, Iter 33, disc loss: 4.488250163862403e-05, policy loss: 26.920805190894868
Experience 20, Iter 34, disc loss: 4.48020995032869e-05, policy loss: 27.091024543073758
Experience 20, Iter 35, disc loss: 4.529907246469528e-05, policy loss: 26.28324950912351
Experience 20, Iter 36, disc loss: 4.464195634614104e-05, policy loss: 27.439744345132482
Experience 20, Iter 37, disc loss: 4.4564183777630384e-05, policy loss: 25.480138076374622
Experience 20, Iter 38, disc loss: 4.4482721170947245e-05, policy loss: 27.644139872274636
Experience 20, Iter 39, disc loss: 4.441570231062879e-05, policy loss: 26.851976897503057
Experience 20, Iter 40, disc loss: 4.433939971688247e-05, policy loss: 26.479823164175002
Experience 20, Iter 41, disc loss: 4.4260393117764923e-05, policy loss: 25.829983602457425
Experience 20, Iter 42, disc loss: 4.4168433482389166e-05, policy loss: 25.447681343363246
Experience 20, Iter 43, disc loss: 4.410119589020536e-05, policy loss: 26.237357016915333
Experience 20, Iter 44, disc loss: 4.413062829515032e-05, policy loss: 26.363980924210715
Experience 20, Iter 45, disc loss: 4.393691682675384e-05, policy loss: 26.749312897246632
Experience 20, Iter 46, disc loss: 4.3877799400147444e-05, policy loss: 26.92951497282488
Experience 20, Iter 47, disc loss: 4.3776399975321014e-05, policy loss: 26.514978766006525
Experience 20, Iter 48, disc loss: 4.369848875286602e-05, policy loss: 26.09529302516711
Experience 20, Iter 49, disc loss: 4.362134537833963e-05, policy loss: 27.158259101694888
Experience 20, Iter 50, disc loss: 4.3544075085996647e-05, policy loss: 26.406584989267778
Experience 20, Iter 51, disc loss: 4.34695737403332e-05, policy loss: 27.233751491826503
Experience 20, Iter 52, disc loss: 4.339036857665669e-05, policy loss: 27.00906766021142
Experience 20, Iter 53, disc loss: 4.3499357806582294e-05, policy loss: 26.012604591239935
Experience 20, Iter 54, disc loss: 4.32375973517418e-05, policy loss: 26.72169990281995
Experience 20, Iter 55, disc loss: 4.317241006546534e-05, policy loss: 25.966482405657352
Experience 20, Iter 56, disc loss: 4.309227898680134e-05, policy loss: 26.146862043397135
Experience 20, Iter 57, disc loss: 4.3011393572522485e-05, policy loss: 26.25763105531778
Experience 20, Iter 58, disc loss: 4.294014614268011e-05, policy loss: 26.200917205065082
Experience 20, Iter 59, disc loss: 4.286626774148054e-05, policy loss: 26.48283539776984
Experience 20, Iter 60, disc loss: 4.278376071141269e-05, policy loss: 26.728487037578823
Experience 20, Iter 61, disc loss: 4.270985371626861e-05, policy loss: 26.055365386746935
Experience 20, Iter 62, disc loss: 4.265511392131148e-05, policy loss: 26.29055282440418
Experience 20, Iter 63, disc loss: 4.256081846978728e-05, policy loss: 25.788885638061863
Experience 20, Iter 64, disc loss: 4.2491358058194006e-05, policy loss: 26.20248276116084
Experience 20, Iter 65, disc loss: 4.241057829045939e-05, policy loss: 26.79086370094322
Experience 20, Iter 66, disc loss: 4.239088359511578e-05, policy loss: 25.142380757245114
Experience 20, Iter 67, disc loss: 4.22626449959126e-05, policy loss: 26.724936117207854
Experience 20, Iter 68, disc loss: 4.218903144074425e-05, policy loss: 26.696426906407982
Experience 20, Iter 69, disc loss: 4.211601205484216e-05, policy loss: 25.66212578710175
Experience 20, Iter 70, disc loss: 4.205075893071661e-05, policy loss: 26.208991080652595
Experience 20, Iter 71, disc loss: 4.198267440695406e-05, policy loss: 26.30234347657261
Experience 20, Iter 72, disc loss: 4.189718299593983e-05, policy loss: 25.910758601179516
Experience 20, Iter 73, disc loss: 4.1824134523917394e-05, policy loss: 26.570657119880714
Experience 20, Iter 74, disc loss: 4.1750886027239145e-05, policy loss: 26.513810943309895
Experience 20, Iter 75, disc loss: 4.1680357016779564e-05, policy loss: 25.62856191203789
Experience 20, Iter 76, disc loss: 4.160625175037367e-05, policy loss: 27.5246315523389
Experience 20, Iter 77, disc loss: 4.153451923791421e-05, policy loss: 26.676772671322503
Experience 20, Iter 78, disc loss: 4.1468902979358546e-05, policy loss: 26.13444830138429
Experience 20, Iter 79, disc loss: 4.13917791056575e-05, policy loss: 26.05665704874061
Experience 20, Iter 80, disc loss: 4.1523802387779885e-05, policy loss: 26.11635796929368
Experience 20, Iter 81, disc loss: 4.128854839036371e-05, policy loss: 26.51815425593892
Experience 20, Iter 82, disc loss: 4.117711797350657e-05, policy loss: 26.216209300156066
Experience 20, Iter 83, disc loss: 4.1849244754613615e-05, policy loss: 26.49893232728523
Experience 20, Iter 84, disc loss: 4.1041093816232935e-05, policy loss: 26.821836814644044
Experience 20, Iter 85, disc loss: 4.0966428287176286e-05, policy loss: 24.912318398085517
Experience 20, Iter 86, disc loss: 4.0896444025694654e-05, policy loss: 26.193551243916282
Experience 20, Iter 87, disc loss: 4.082454913308107e-05, policy loss: 27.009735162052653
Experience 20, Iter 88, disc loss: 4.075661282411012e-05, policy loss: 27.088998129422112
Experience 20, Iter 89, disc loss: 4.068761524017106e-05, policy loss: 26.094103669160397
Experience 20, Iter 90, disc loss: 4.061540872359494e-05, policy loss: 26.28838459169763
Experience 20, Iter 91, disc loss: 4.054575252053389e-05, policy loss: 26.044430192136545
Experience 20, Iter 92, disc loss: 4.047675232856378e-05, policy loss: 26.494242567222834
Experience 20, Iter 93, disc loss: 4.040772432461009e-05, policy loss: 26.077478419634012
Experience 20, Iter 94, disc loss: 4.0338317948866826e-05, policy loss: 26.84141977903486
Experience 20, Iter 95, disc loss: 4.027007595069613e-05, policy loss: 25.850306957927394
Experience 20, Iter 96, disc loss: 4.0202422941368904e-05, policy loss: 25.910102225169616
Experience 20, Iter 97, disc loss: 4.013253853279246e-05, policy loss: 26.20897695744073
Experience 20, Iter 98, disc loss: 4.006669944072918e-05, policy loss: 26.305521036956467
Experience 20, Iter 99, disc loss: 4.000810769983044e-05, policy loss: 25.951499960707608
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0107],
        [0.2493],
        [2.2458],
        [0.0345]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0539, 0.4273, 1.5922, 0.0355, 0.0116, 6.5745]],

        [[0.0539, 0.4273, 1.5922, 0.0355, 0.0116, 6.5745]],

        [[0.0539, 0.4273, 1.5922, 0.0355, 0.0116, 6.5745]],

        [[0.0539, 0.4273, 1.5922, 0.0355, 0.0116, 6.5745]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0427, 0.9972, 8.9834, 0.1382], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0427, 0.9972, 8.9834, 0.1382], device='cuda:0')
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.099
Iter 2/2000 - Loss: 3.996
Iter 3/2000 - Loss: 3.930
Iter 4/2000 - Loss: 3.844
Iter 5/2000 - Loss: 3.770
Iter 6/2000 - Loss: 3.691
Iter 7/2000 - Loss: 3.598
Iter 8/2000 - Loss: 3.500
Iter 9/2000 - Loss: 3.395
Iter 10/2000 - Loss: 3.274
Iter 11/2000 - Loss: 3.139
Iter 12/2000 - Loss: 2.996
Iter 13/2000 - Loss: 2.845
Iter 14/2000 - Loss: 2.683
Iter 15/2000 - Loss: 2.507
Iter 16/2000 - Loss: 2.316
Iter 17/2000 - Loss: 2.110
Iter 18/2000 - Loss: 1.892
Iter 19/2000 - Loss: 1.661
Iter 20/2000 - Loss: 1.416
Iter 1981/2000 - Loss: -6.749
Iter 1982/2000 - Loss: -6.749
Iter 1983/2000 - Loss: -6.749
Iter 1984/2000 - Loss: -6.749
Iter 1985/2000 - Loss: -6.749
Iter 1986/2000 - Loss: -6.749
Iter 1987/2000 - Loss: -6.749
Iter 1988/2000 - Loss: -6.749
Iter 1989/2000 - Loss: -6.749
Iter 1990/2000 - Loss: -6.749
Iter 1991/2000 - Loss: -6.749
Iter 1992/2000 - Loss: -6.749
Iter 1993/2000 - Loss: -6.750
Iter 1994/2000 - Loss: -6.750
Iter 1995/2000 - Loss: -6.750
Iter 1996/2000 - Loss: -6.750
Iter 1997/2000 - Loss: -6.750
Iter 1998/2000 - Loss: -6.750
Iter 1999/2000 - Loss: -6.750
Iter 2000/2000 - Loss: -6.750
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0025],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[18.4402, 13.9312, 37.3828,  7.8089,  3.5098, 68.0207]],

        [[25.2201, 46.4362,  7.8613,  1.3304,  1.3407, 35.1293]],

        [[26.9465, 50.3234,  7.9391,  0.9562,  0.8346, 21.5043]],

        [[21.5991, 44.2862, 18.7477,  1.3962,  2.4770, 39.3763]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1982,  2.8340, 11.6767,  0.5326], device='cuda:0')
Estimated target variance: tensor([0.0427, 0.9972, 8.9834, 0.1382], device='cuda:0')
N: 210
Signal to noise ratio: tensor([22.3634, 73.5716, 67.8359, 39.2453], device='cuda:0')
Bound on condition number: tensor([ 105027.0058, 1136683.3450,  966360.1967,  323441.9273],
       device='cuda:0')
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 3.9931858062150576e-05, policy loss: 26.612294828487663
Experience 21, Iter 1, disc loss: 3.986132491151375e-05, policy loss: 26.44576985350363
Experience 21, Iter 2, disc loss: 3.979253145695601e-05, policy loss: 26.505532188300208
Experience 21, Iter 3, disc loss: 3.9825719098012174e-05, policy loss: 26.58657698062023
Experience 21, Iter 4, disc loss: 3.965791356499564e-05, policy loss: 25.75881823145349
Experience 21, Iter 5, disc loss: 3.959236883656065e-05, policy loss: 26.436563240482357
Experience 21, Iter 6, disc loss: 3.952690891975144e-05, policy loss: 26.853274853036858
Experience 21, Iter 7, disc loss: 3.945675828989997e-05, policy loss: 26.716002199176216
Experience 21, Iter 8, disc loss: 3.9419725497770885e-05, policy loss: 26.201598179538337
Experience 21, Iter 9, disc loss: 3.932357955904629e-05, policy loss: 26.409770336184295
Experience 21, Iter 10, disc loss: 3.9259269958216254e-05, policy loss: 26.342892801900078
Experience 21, Iter 11, disc loss: 3.9191935494624784e-05, policy loss: 26.175092752201127
Experience 21, Iter 12, disc loss: 3.915784279086443e-05, policy loss: 25.807648623594076
Experience 21, Iter 13, disc loss: 3.910318660802416e-05, policy loss: 25.902964483704082
Experience 21, Iter 14, disc loss: 3.9000015242643484e-05, policy loss: 26.61420487087575
Experience 21, Iter 15, disc loss: 3.892782582972256e-05, policy loss: 26.679120823943776
Experience 21, Iter 16, disc loss: 3.886572410844636e-05, policy loss: 26.58516939894682
Experience 21, Iter 17, disc loss: 3.879724358048355e-05, policy loss: 25.94200827560571
Experience 21, Iter 18, disc loss: 3.873616489514018e-05, policy loss: 25.405290272152822
Experience 21, Iter 19, disc loss: 3.866761343574202e-05, policy loss: 25.655761306612902
Experience 21, Iter 20, disc loss: 3.8602564347005265e-05, policy loss: 26.304654625671166
Experience 21, Iter 21, disc loss: 3.8538855233587755e-05, policy loss: 26.290787645362187
Experience 21, Iter 22, disc loss: 3.8473797623016574e-05, policy loss: 27.332517590259954
Experience 21, Iter 23, disc loss: 3.840899653004383e-05, policy loss: 26.49938402573472
Experience 21, Iter 24, disc loss: 3.837981182446404e-05, policy loss: 25.683051374287412
Experience 21, Iter 25, disc loss: 3.828087965652059e-05, policy loss: 26.44401038254199
Experience 21, Iter 26, disc loss: 3.8217561361310826e-05, policy loss: 26.126537851278062
Experience 21, Iter 27, disc loss: 3.815460313790216e-05, policy loss: 25.354402449352143
Experience 21, Iter 28, disc loss: 3.8090448171407046e-05, policy loss: 24.966599283304852
Experience 21, Iter 29, disc loss: 3.802667943389052e-05, policy loss: 26.266937827835743
Experience 21, Iter 30, disc loss: 3.8030188646071915e-05, policy loss: 25.56366416282163
Experience 21, Iter 31, disc loss: 3.790012549897126e-05, policy loss: 26.86186277094061
Experience 21, Iter 32, disc loss: 3.783722281480562e-05, policy loss: 26.57494631068296
Experience 21, Iter 33, disc loss: 3.777450159798972e-05, policy loss: 26.93793409797322
Experience 21, Iter 34, disc loss: 3.771286798599818e-05, policy loss: 26.092620605572087
Experience 21, Iter 35, disc loss: 3.7668533843671565e-05, policy loss: 26.077581078294617
Experience 21, Iter 36, disc loss: 3.758701954867377e-05, policy loss: 26.694419675804
Experience 21, Iter 37, disc loss: 3.753078096041091e-05, policy loss: 26.650385382764703
Experience 21, Iter 38, disc loss: 3.746301239362689e-05, policy loss: 26.58051062159468
Experience 21, Iter 39, disc loss: 3.7401071709272516e-05, policy loss: 25.432265735399397
Experience 21, Iter 40, disc loss: 3.734000037244552e-05, policy loss: 26.262416842626003
Experience 21, Iter 41, disc loss: 3.727796599430186e-05, policy loss: 25.909156079719413
Experience 21, Iter 42, disc loss: 3.72173560355913e-05, policy loss: 27.241991949622978
Experience 21, Iter 43, disc loss: 3.715966096424535e-05, policy loss: 26.146073281659163
Experience 21, Iter 44, disc loss: 7.443624175342047e-05, policy loss: 26.28831915278723
Experience 21, Iter 45, disc loss: 3.706900764326272e-05, policy loss: 26.379478628600502
Experience 21, Iter 46, disc loss: 3.698441910123529e-05, policy loss: 27.031920550155718
Experience 21, Iter 47, disc loss: 3.6932136424096295e-05, policy loss: 26.17885429818527
Experience 21, Iter 48, disc loss: 3.690877514724712e-05, policy loss: 26.44612214387937
Experience 21, Iter 49, disc loss: 3.6817886786581954e-05, policy loss: 25.830200498688342
Experience 21, Iter 50, disc loss: 3.676081902502251e-05, policy loss: 26.410867885352292
Experience 21, Iter 51, disc loss: 3.67575173736891e-05, policy loss: 25.497243087058404
Experience 21, Iter 52, disc loss: 3.66471618525221e-05, policy loss: 26.073704775519722
Experience 21, Iter 53, disc loss: 3.659114105697702e-05, policy loss: 26.525379477354555
Experience 21, Iter 54, disc loss: 4.282235250539627e-05, policy loss: 25.856727165430517
Experience 21, Iter 55, disc loss: 3.6527720532876885e-05, policy loss: 25.936100474509104
Experience 21, Iter 56, disc loss: 3.642406019567973e-05, policy loss: 25.908586376258967
Experience 21, Iter 57, disc loss: 3.6361316573231564e-05, policy loss: 27.064355930074036
Experience 21, Iter 58, disc loss: 3.636042330945525e-05, policy loss: 26.131735493551236
Experience 21, Iter 59, disc loss: 3.6246585232036665e-05, policy loss: 25.581681394653394
Experience 21, Iter 60, disc loss: 3.627206348637881e-05, policy loss: 26.679504456704187
Experience 21, Iter 61, disc loss: 3.613192940418157e-05, policy loss: 25.988974591565565
Experience 21, Iter 62, disc loss: 3.6076249754380675e-05, policy loss: 26.354763067392867
Experience 21, Iter 63, disc loss: 3.60199202052765e-05, policy loss: 25.822006628919148
Experience 21, Iter 64, disc loss: 3.5974198105736064e-05, policy loss: 26.00769858890951
Experience 21, Iter 65, disc loss: 3.590178498474613e-05, policy loss: 26.78811067362873
Experience 21, Iter 66, disc loss: 3.5863512135634274e-05, policy loss: 25.39344975575139
Experience 21, Iter 67, disc loss: 3.5787662925711065e-05, policy loss: 25.66669180767079
Experience 21, Iter 68, disc loss: 3.574120837690409e-05, policy loss: 26.424160145785944
Experience 21, Iter 69, disc loss: 3.567410500151911e-05, policy loss: 26.07050166427244
Experience 21, Iter 70, disc loss: 3.561543677426064e-05, policy loss: 26.083540778140826
Experience 21, Iter 71, disc loss: 3.563192181772317e-05, policy loss: 25.005692323862423
Experience 21, Iter 72, disc loss: 3.5505725976461163e-05, policy loss: 25.75621367181202
Experience 21, Iter 73, disc loss: 3.545804527444558e-05, policy loss: 25.83641340210771
Experience 21, Iter 74, disc loss: 3.54296517533952e-05, policy loss: 25.855722599386944
Experience 21, Iter 75, disc loss: 3.533575632046687e-05, policy loss: 25.371265690894234
Experience 21, Iter 76, disc loss: 3.527408210906772e-05, policy loss: 26.621360699023878
Experience 21, Iter 77, disc loss: 3.522033957183937e-05, policy loss: 26.715687428041576
Experience 21, Iter 78, disc loss: 3.527484876230332e-05, policy loss: 26.30196309039504
Experience 21, Iter 79, disc loss: 3.510567831414418e-05, policy loss: 26.502450715839785
Experience 21, Iter 80, disc loss: 3.5056500178110496e-05, policy loss: 26.145816461662164
Experience 21, Iter 81, disc loss: 3.499443365811015e-05, policy loss: 25.941025130874237
Experience 21, Iter 82, disc loss: 3.508961435392927e-05, policy loss: 25.346144003572277
Experience 21, Iter 83, disc loss: 3.491404453982787e-05, policy loss: 26.148218093757706
Experience 21, Iter 84, disc loss: 3.4824870010920854e-05, policy loss: 26.134281666260424
Experience 21, Iter 85, disc loss: 3.4770748511988964e-05, policy loss: 25.75872771037904
Experience 21, Iter 86, disc loss: 3.4713732510843165e-05, policy loss: 26.47504890160205
Experience 21, Iter 87, disc loss: 3.4658702957029973e-05, policy loss: 26.022452960070773
Experience 21, Iter 88, disc loss: 3.460339511744241e-05, policy loss: 25.511541370238078
Experience 21, Iter 89, disc loss: 3.4548010188896556e-05, policy loss: 26.4929648392472
Experience 21, Iter 90, disc loss: 3.4495254181654106e-05, policy loss: 26.355597429986865
Experience 21, Iter 91, disc loss: 4.532708543481471e-05, policy loss: 25.223304413775942
Experience 21, Iter 92, disc loss: 3.5218126047226625e-05, policy loss: 26.552881821825665
Experience 21, Iter 93, disc loss: 3.433223910089248e-05, policy loss: 27.095774774284035
Experience 21, Iter 94, disc loss: 3.429070022364237e-05, policy loss: 25.092508432144466
Experience 21, Iter 95, disc loss: 3.4226535882169e-05, policy loss: 26.22645992164344
Experience 21, Iter 96, disc loss: 3.4173459495803155e-05, policy loss: 26.45166008303442
Experience 21, Iter 97, disc loss: 3.4124687713871744e-05, policy loss: 24.984398107667833
Experience 21, Iter 98, disc loss: 3.40678705056232e-05, policy loss: 26.721620773439984
Experience 21, Iter 99, disc loss: 3.402373717826538e-05, policy loss: 25.68952221271304
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0113],
        [0.2485],
        [2.2788],
        [0.0352]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0545, 0.4496, 1.6130, 0.0367, 0.0126, 6.6263]],

        [[0.0545, 0.4496, 1.6130, 0.0367, 0.0126, 6.6263]],

        [[0.0545, 0.4496, 1.6130, 0.0367, 0.0126, 6.6263]],

        [[0.0545, 0.4496, 1.6130, 0.0367, 0.0126, 6.6263]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0451, 0.9940, 9.1151, 0.1408], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0451, 0.9940, 9.1151, 0.1408], device='cuda:0')
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.108
Iter 2/2000 - Loss: 4.019
Iter 3/2000 - Loss: 3.931
Iter 4/2000 - Loss: 3.847
Iter 5/2000 - Loss: 3.772
Iter 6/2000 - Loss: 3.676
Iter 7/2000 - Loss: 3.574
Iter 8/2000 - Loss: 3.474
Iter 9/2000 - Loss: 3.359
Iter 10/2000 - Loss: 3.223
Iter 11/2000 - Loss: 3.076
Iter 12/2000 - Loss: 2.924
Iter 13/2000 - Loss: 2.764
Iter 14/2000 - Loss: 2.591
Iter 15/2000 - Loss: 2.402
Iter 16/2000 - Loss: 2.197
Iter 17/2000 - Loss: 1.981
Iter 18/2000 - Loss: 1.754
Iter 19/2000 - Loss: 1.515
Iter 20/2000 - Loss: 1.263
Iter 1981/2000 - Loss: -6.826
Iter 1982/2000 - Loss: -6.826
Iter 1983/2000 - Loss: -6.826
Iter 1984/2000 - Loss: -6.826
Iter 1985/2000 - Loss: -6.826
Iter 1986/2000 - Loss: -6.826
Iter 1987/2000 - Loss: -6.826
Iter 1988/2000 - Loss: -6.826
Iter 1989/2000 - Loss: -6.826
Iter 1990/2000 - Loss: -6.826
Iter 1991/2000 - Loss: -6.826
Iter 1992/2000 - Loss: -6.827
Iter 1993/2000 - Loss: -6.827
Iter 1994/2000 - Loss: -6.827
Iter 1995/2000 - Loss: -6.827
Iter 1996/2000 - Loss: -6.827
Iter 1997/2000 - Loss: -6.827
Iter 1998/2000 - Loss: -6.827
Iter 1999/2000 - Loss: -6.827
Iter 2000/2000 - Loss: -6.827
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0004]], device='cuda:0')
Lengthscale: tensor([[[17.3788, 14.0259, 32.9564,  7.8185,  4.0913, 67.0616]],

        [[24.6385, 46.9886,  7.8414,  1.2803,  1.3468, 34.0351]],

        [[26.0777, 50.2641,  7.8311,  0.9662,  0.8421, 21.5345]],

        [[20.7215, 43.2625, 18.1313,  1.8234,  2.2835, 32.4380]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2087,  2.6558, 11.7598,  0.5019], device='cuda:0')
Estimated target variance: tensor([0.0451, 0.9940, 9.1151, 0.1408], device='cuda:0')
N: 220
Signal to noise ratio: tensor([23.0335, 72.0208, 69.5258, 37.7338], device='cuda:0')
Bound on condition number: tensor([ 116720.3548, 1141139.2063, 1063445.3967,  313246.2161],
       device='cuda:0')
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 3.396199035102313e-05, policy loss: 25.674724962147877
Experience 22, Iter 1, disc loss: 3.3909081656053714e-05, policy loss: 25.002387234240338
Experience 22, Iter 2, disc loss: 3.3859004745718584e-05, policy loss: 25.359003103979074
Experience 22, Iter 3, disc loss: 3.384306917310388e-05, policy loss: 25.389486492219604
Experience 22, Iter 4, disc loss: 3.375161756001794e-05, policy loss: 26.25791801326895
Experience 22, Iter 5, disc loss: 3.3712018867052626e-05, policy loss: 26.749659688327768
Experience 22, Iter 6, disc loss: 3.364432099043348e-05, policy loss: 26.640751535964664
Experience 22, Iter 7, disc loss: 3.359695738879416e-05, policy loss: 24.90569112069694
Experience 22, Iter 8, disc loss: 3.3716956526154365e-05, policy loss: 26.451291460107647
Experience 22, Iter 9, disc loss: 3.349071643002377e-05, policy loss: 26.304741720367897
Experience 22, Iter 10, disc loss: 3.343565574513242e-05, policy loss: 26.973034328074988
Experience 22, Iter 11, disc loss: 3.3384396561639007e-05, policy loss: 26.5182929324893
Experience 22, Iter 12, disc loss: 0.0006458346659760419, policy loss: 26.64482193259346
Experience 22, Iter 13, disc loss: 3.3369830578393835e-05, policy loss: 25.4074672550251
Experience 22, Iter 14, disc loss: 3.341304358562192e-05, policy loss: 26.461492699482115
Experience 22, Iter 15, disc loss: 3.3429034567714706e-05, policy loss: 25.756114363175463
Experience 22, Iter 16, disc loss: 0.00044765583609323575, policy loss: 26.471603878589168
Experience 22, Iter 17, disc loss: 3.352627935958865e-05, policy loss: 26.210036322087813
Experience 22, Iter 18, disc loss: 3.3617135722915505e-05, policy loss: 26.08932580604143
Experience 22, Iter 19, disc loss: 3.364486543179634e-05, policy loss: 26.96232661008512
Experience 22, Iter 20, disc loss: 3.369274975494851e-05, policy loss: 26.43407159021903
Experience 22, Iter 21, disc loss: 3.37269670076639e-05, policy loss: 26.740432739722557
Experience 22, Iter 22, disc loss: 3.375857255291763e-05, policy loss: 25.45074567629577
Experience 22, Iter 23, disc loss: 3.377368436360102e-05, policy loss: 26.416082633769985
Experience 22, Iter 24, disc loss: 3.4017481343325214e-05, policy loss: 25.96993428391563
Experience 22, Iter 25, disc loss: 3.918966639377892e-05, policy loss: 25.470355055249556
Experience 22, Iter 26, disc loss: 3.398653971931202e-05, policy loss: 26.131425153037693
Experience 22, Iter 27, disc loss: 3.380397556192996e-05, policy loss: 25.374763566977837
Experience 22, Iter 28, disc loss: 3.378102172071863e-05, policy loss: 25.701279947618897
Experience 22, Iter 29, disc loss: 3.3768290056045276e-05, policy loss: 26.426520103653793
Experience 22, Iter 30, disc loss: 3.3766198057776974e-05, policy loss: 25.802974352815767
Experience 22, Iter 31, disc loss: 3.372164274729947e-05, policy loss: 25.863448498335856
Experience 22, Iter 32, disc loss: 3.388046586789791e-05, policy loss: 24.97140711306549
Experience 22, Iter 33, disc loss: 3.366933266164201e-05, policy loss: 26.414505807937534
Experience 22, Iter 34, disc loss: 3.36381999836171e-05, policy loss: 26.38833994990979
Experience 22, Iter 35, disc loss: 3.361933829234851e-05, policy loss: 25.665239481633918
Experience 22, Iter 36, disc loss: 3.357473209000269e-05, policy loss: 25.350241380318657
Experience 22, Iter 37, disc loss: 3.3627800198381266e-05, policy loss: 25.63970564770875
Experience 22, Iter 38, disc loss: 3.355866152613323e-05, policy loss: 26.614544844817633
Experience 22, Iter 39, disc loss: 3.346046465469146e-05, policy loss: 26.226855784666558
Experience 22, Iter 40, disc loss: 3.3415384979777174e-05, policy loss: 25.926833242574858
Experience 22, Iter 41, disc loss: 3.337307575104601e-05, policy loss: 26.241204507597782
Experience 22, Iter 42, disc loss: 3.3342877696975e-05, policy loss: 25.55295713777935
Experience 22, Iter 43, disc loss: 3.328610159347232e-05, policy loss: 26.317302956898807
Experience 22, Iter 44, disc loss: 3.324046511979476e-05, policy loss: 26.259486530740645
Experience 22, Iter 45, disc loss: 3.319575327703255e-05, policy loss: 26.28109037134238
Experience 22, Iter 46, disc loss: 3.3148262848900274e-05, policy loss: 26.960940662597856
Experience 22, Iter 47, disc loss: 3.310165293314578e-05, policy loss: 26.219394366761335
Experience 22, Iter 48, disc loss: 3.306211974550902e-05, policy loss: 25.70089031070256
Experience 22, Iter 49, disc loss: 3.300793877711415e-05, policy loss: 26.167235367965215
Experience 22, Iter 50, disc loss: 3.2958975934592226e-05, policy loss: 25.32999354648409
Experience 22, Iter 51, disc loss: 3.291112487573518e-05, policy loss: 25.649820485684252
Experience 22, Iter 52, disc loss: 3.2879399488368826e-05, policy loss: 27.152019006094292
Experience 22, Iter 53, disc loss: 3.2812146989289105e-05, policy loss: 25.888520467202582
Experience 22, Iter 54, disc loss: 3.276416257193087e-05, policy loss: 25.93053231786947
Experience 22, Iter 55, disc loss: 3.272845966243566e-05, policy loss: 25.355353467854968
Experience 22, Iter 56, disc loss: 3.266540743691951e-05, policy loss: 25.591204877577916
Experience 22, Iter 57, disc loss: 3.261417436850817e-05, policy loss: 26.61107051836194
Experience 22, Iter 58, disc loss: 3.256438336438583e-05, policy loss: 26.078534842710543
Experience 22, Iter 59, disc loss: 3.2514840191081016e-05, policy loss: 26.927178410243005
Experience 22, Iter 60, disc loss: 3.2479618808479933e-05, policy loss: 24.91266448350425
Experience 22, Iter 61, disc loss: 3.248302506236955e-05, policy loss: 25.35163142543246
Experience 22, Iter 62, disc loss: 3.251665001812904e-05, policy loss: 25.073025382903282
Experience 22, Iter 63, disc loss: 3.231657048597274e-05, policy loss: 26.280765959639687
Experience 22, Iter 64, disc loss: 3.22647276551046e-05, policy loss: 25.563953121026433
Experience 22, Iter 65, disc loss: 3.2217514993782326e-05, policy loss: 26.56190891586511
Experience 22, Iter 66, disc loss: 0.00042487986535643036, policy loss: 25.965092385561515
Experience 22, Iter 67, disc loss: 3.218018637202771e-05, policy loss: 25.056067333235614
Experience 22, Iter 68, disc loss: 3.218027486705349e-05, policy loss: 26.173537282810457
Experience 22, Iter 69, disc loss: 3.218015118802911e-05, policy loss: 25.758377443609206
Experience 22, Iter 70, disc loss: 3.2283431449031195e-05, policy loss: 26.140993116079343
Experience 22, Iter 71, disc loss: 3.223780107716303e-05, policy loss: 26.349678588798206
Experience 22, Iter 72, disc loss: 3.217709168309984e-05, policy loss: 25.66665976468532
Experience 22, Iter 73, disc loss: 3.213364996070122e-05, policy loss: 25.50070195579176
Experience 22, Iter 74, disc loss: 3.211283813102789e-05, policy loss: 25.7652236363653
Experience 22, Iter 75, disc loss: 3.223677823938098e-05, policy loss: 26.20616404246893
Experience 22, Iter 76, disc loss: 3.208438271741593e-05, policy loss: 25.044901145266273
Experience 22, Iter 77, disc loss: 3.2097777810093014e-05, policy loss: 26.307109594392916
Experience 22, Iter 78, disc loss: 3.200579255842075e-05, policy loss: 25.788815074089207
Experience 22, Iter 79, disc loss: 3.1969803080262784e-05, policy loss: 26.8679631942977
Experience 22, Iter 80, disc loss: 3.253629289107077e-05, policy loss: 26.487390347355788
Experience 22, Iter 81, disc loss: 3.1903285826507796e-05, policy loss: 25.36035301002301
Experience 22, Iter 82, disc loss: 3.185996292462219e-05, policy loss: 25.875738547204374
Experience 22, Iter 83, disc loss: 3.1835978749694324e-05, policy loss: 25.517685599638178
Experience 22, Iter 84, disc loss: 3.180192396453302e-05, policy loss: 25.808452432904893
Experience 22, Iter 85, disc loss: 0.000578382723493652, policy loss: 25.898802572998292
Experience 22, Iter 86, disc loss: 3.1775011598496625e-05, policy loss: 25.621539581767543
Experience 22, Iter 87, disc loss: 3.179796281622734e-05, policy loss: 26.022959987723574
Experience 22, Iter 88, disc loss: 3.1827932520236985e-05, policy loss: 26.75813040360184
Experience 22, Iter 89, disc loss: 3.182427467798673e-05, policy loss: 26.995318480848177
Experience 22, Iter 90, disc loss: 3.182981016399529e-05, policy loss: 25.759733003954395
Experience 22, Iter 91, disc loss: 3.1829007789155e-05, policy loss: 26.25142690999376
Experience 22, Iter 92, disc loss: 3.1823569022612175e-05, policy loss: 25.273392443034204
Experience 22, Iter 93, disc loss: 3.182469010623627e-05, policy loss: 26.397845525646844
Experience 22, Iter 94, disc loss: 3.184790878809121e-05, policy loss: 25.1815862080472
Experience 22, Iter 95, disc loss: 3.1783523836889166e-05, policy loss: 25.770484716017236
Experience 22, Iter 96, disc loss: 3.196505669309282e-05, policy loss: 27.288456781474558
Experience 22, Iter 97, disc loss: 3.173476618504204e-05, policy loss: 26.283416293470644
Experience 22, Iter 98, disc loss: 3.173664876453835e-05, policy loss: 25.377003035045046
Experience 22, Iter 99, disc loss: 3.1680804656506026e-05, policy loss: 25.07144667724873
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0109],
        [0.2490],
        [2.2635],
        [0.0344]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0531, 0.4347, 1.5796, 0.0365, 0.0122, 6.6583]],

        [[0.0531, 0.4347, 1.5796, 0.0365, 0.0122, 6.6583]],

        [[0.0531, 0.4347, 1.5796, 0.0365, 0.0122, 6.6583]],

        [[0.0531, 0.4347, 1.5796, 0.0365, 0.0122, 6.6583]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0434, 0.9960, 9.0541, 0.1374], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0434, 0.9960, 9.0541, 0.1374], device='cuda:0')
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.095
Iter 2/2000 - Loss: 4.001
Iter 3/2000 - Loss: 3.919
Iter 4/2000 - Loss: 3.833
Iter 5/2000 - Loss: 3.757
Iter 6/2000 - Loss: 3.664
Iter 7/2000 - Loss: 3.562
Iter 8/2000 - Loss: 3.460
Iter 9/2000 - Loss: 3.345
Iter 10/2000 - Loss: 3.210
Iter 11/2000 - Loss: 3.062
Iter 12/2000 - Loss: 2.909
Iter 13/2000 - Loss: 2.749
Iter 14/2000 - Loss: 2.575
Iter 15/2000 - Loss: 2.384
Iter 16/2000 - Loss: 2.179
Iter 17/2000 - Loss: 1.960
Iter 18/2000 - Loss: 1.731
Iter 19/2000 - Loss: 1.490
Iter 20/2000 - Loss: 1.236
Iter 1981/2000 - Loss: -6.946
Iter 1982/2000 - Loss: -6.946
Iter 1983/2000 - Loss: -6.946
Iter 1984/2000 - Loss: -6.946
Iter 1985/2000 - Loss: -6.946
Iter 1986/2000 - Loss: -6.946
Iter 1987/2000 - Loss: -6.946
Iter 1988/2000 - Loss: -6.946
Iter 1989/2000 - Loss: -6.946
Iter 1990/2000 - Loss: -6.946
Iter 1991/2000 - Loss: -6.946
Iter 1992/2000 - Loss: -6.946
Iter 1993/2000 - Loss: -6.946
Iter 1994/2000 - Loss: -6.946
Iter 1995/2000 - Loss: -6.946
Iter 1996/2000 - Loss: -6.947
Iter 1997/2000 - Loss: -6.947
Iter 1998/2000 - Loss: -6.947
Iter 1999/2000 - Loss: -6.947
Iter 2000/2000 - Loss: -6.947
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.3284, 13.9356, 32.2099,  8.5314,  3.9725, 66.8634]],

        [[24.3321, 46.1677,  7.8059,  1.2629,  1.3666, 34.1173]],

        [[25.5529, 48.7617,  7.8784,  0.9577,  0.8435, 21.5636]],

        [[20.8061, 42.7544, 18.1828,  1.5099,  2.4040, 35.7843]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2071,  2.6077, 11.6087,  0.4887], device='cuda:0')
Estimated target variance: tensor([0.0434, 0.9960, 9.0541, 0.1374], device='cuda:0')
N: 230
Signal to noise ratio: tensor([23.1976, 72.1607, 70.1345, 38.0796], device='cuda:0')
Bound on condition number: tensor([ 123770.4542, 1197648.9826, 1131335.1035,  333513.9110],
       device='cuda:0')
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 3.166073921836868e-05, policy loss: 25.259769336976248
Experience 23, Iter 1, disc loss: 3.162199760910671e-05, policy loss: 25.703890463855018
Experience 23, Iter 2, disc loss: 3.158757808010889e-05, policy loss: 26.266239678428658
Experience 23, Iter 3, disc loss: 3.1553699057245915e-05, policy loss: 26.30310842158056
Experience 23, Iter 4, disc loss: 3.15089523039996e-05, policy loss: 25.95128133060731
Experience 23, Iter 5, disc loss: 0.0007749625982415839, policy loss: 26.322960546645536
Experience 23, Iter 6, disc loss: 3.1577527839430834e-05, policy loss: 26.22686101265485
Experience 23, Iter 7, disc loss: 3.162867050733333e-05, policy loss: 25.731489151088677
Experience 23, Iter 8, disc loss: 3.236190663896479e-05, policy loss: 25.577739063867277
Experience 23, Iter 9, disc loss: 3.175095176646637e-05, policy loss: 26.205220216543594
Experience 23, Iter 10, disc loss: 3.179408597556452e-05, policy loss: 25.243156390337834
Experience 23, Iter 11, disc loss: 3.1822838448348254e-05, policy loss: 25.403644938110517
Experience 23, Iter 12, disc loss: 3.183827590086682e-05, policy loss: 25.324597417185952
Experience 23, Iter 13, disc loss: 3.220930376511091e-05, policy loss: 25.96107393459629
Experience 23, Iter 14, disc loss: 0.00021571134058144287, policy loss: 26.675756177858
Experience 23, Iter 15, disc loss: 3.189438429779925e-05, policy loss: 26.96315240436956
Experience 23, Iter 16, disc loss: 3.2335487111299106e-05, policy loss: 25.01911406030471
Experience 23, Iter 17, disc loss: 3.194220359246442e-05, policy loss: 26.32579996988487
Experience 23, Iter 18, disc loss: 3.195590260483246e-05, policy loss: 26.00698128760116
Experience 23, Iter 19, disc loss: 3.196322329634289e-05, policy loss: 25.68125691645707
Experience 23, Iter 20, disc loss: 3.196855490261791e-05, policy loss: 26.10369925489971
Experience 23, Iter 21, disc loss: 3.1962525312910314e-05, policy loss: 25.846182154620998
Experience 23, Iter 22, disc loss: 3.195116336602758e-05, policy loss: 27.090063803848132
Experience 23, Iter 23, disc loss: 3.194806365726531e-05, policy loss: 25.790831043333057
Experience 23, Iter 24, disc loss: 3.193539504005948e-05, policy loss: 25.34783902672383
Experience 23, Iter 25, disc loss: 3.1903423417847485e-05, policy loss: 25.809325420840967
Experience 23, Iter 26, disc loss: 3.188084819921954e-05, policy loss: 27.00358264328449
Experience 23, Iter 27, disc loss: 3.201234179656273e-05, policy loss: 25.13467431029668
Experience 23, Iter 28, disc loss: 3.1824047226041476e-05, policy loss: 26.4871762926465
Experience 23, Iter 29, disc loss: 3.182330005470347e-05, policy loss: 25.72401414260895
Experience 23, Iter 30, disc loss: 3.177397947367476e-05, policy loss: 26.309980512051478
Experience 23, Iter 31, disc loss: 3.1726932179457405e-05, policy loss: 25.268505875826254
Experience 23, Iter 32, disc loss: 3.189362163935685e-05, policy loss: 25.007208691001175
Experience 23, Iter 33, disc loss: 0.0001965147094370355, policy loss: 25.805963250015658
Experience 23, Iter 34, disc loss: 3.164828776021596e-05, policy loss: 25.557995173119153
Experience 23, Iter 35, disc loss: 3.1630569806428014e-05, policy loss: 26.84879740453831
Experience 23, Iter 36, disc loss: 3.167828628287048e-05, policy loss: 25.210482985664818
Experience 23, Iter 37, disc loss: 3.1594453459276086e-05, policy loss: 25.357660347891482
Experience 23, Iter 38, disc loss: 3.158074230541799e-05, policy loss: 25.335175874184372
Experience 23, Iter 39, disc loss: 3.1548241789089595e-05, policy loss: 25.832190378381256
Experience 23, Iter 40, disc loss: 3.164969493098329e-05, policy loss: 25.25989136601912
Experience 23, Iter 41, disc loss: 3.148611615970544e-05, policy loss: 25.265096402091807
Experience 23, Iter 42, disc loss: 3.147652473781971e-05, policy loss: 25.90630154958637
Experience 23, Iter 43, disc loss: 3.143846784209098e-05, policy loss: 25.85986810794494
Experience 23, Iter 44, disc loss: 3.144593779040685e-05, policy loss: 25.47086765461762
Experience 23, Iter 45, disc loss: 3.137705455474054e-05, policy loss: 24.62467078775208
Experience 23, Iter 46, disc loss: 3.1315606812147365e-05, policy loss: 25.67370090809624
Experience 23, Iter 47, disc loss: 3.1307805194955074e-05, policy loss: 25.225597979679286
Experience 23, Iter 48, disc loss: 3.122867880599518e-05, policy loss: 25.948757939004558
Experience 23, Iter 49, disc loss: 4.4325445889035255e-05, policy loss: 24.874490637862117
Experience 23, Iter 50, disc loss: 3.948314066948954e-05, policy loss: 24.996835309173562
Experience 23, Iter 51, disc loss: 3.11058035814291e-05, policy loss: 25.991286727111454
Experience 23, Iter 52, disc loss: 3.117832928517208e-05, policy loss: 25.76253382966314
Experience 23, Iter 53, disc loss: 3.10332501789527e-05, policy loss: 25.725511473249444
Experience 23, Iter 54, disc loss: 3.09763776722103e-05, policy loss: 24.657381769205706
Experience 23, Iter 55, disc loss: 3.0933746343156836e-05, policy loss: 26.412025642527833
Experience 23, Iter 56, disc loss: 3.0909676909148056e-05, policy loss: 24.888270859659563
Experience 23, Iter 57, disc loss: 3.084115014778735e-05, policy loss: 25.50204554247567
Experience 23, Iter 58, disc loss: 3.0836942349999216e-05, policy loss: 24.87404670771663
Experience 23, Iter 59, disc loss: 3.091785738178981e-05, policy loss: 25.65167781841965
Experience 23, Iter 60, disc loss: 3.070829568642356e-05, policy loss: 25.649928194697708
Experience 23, Iter 61, disc loss: 3.067715280353332e-05, policy loss: 25.808053229911067
Experience 23, Iter 62, disc loss: 3.438780622942542e-05, policy loss: 25.647406067194325
Experience 23, Iter 63, disc loss: 3.0787449581881416e-05, policy loss: 25.888966796788925
Experience 23, Iter 64, disc loss: 0.0005229681953360235, policy loss: 26.586698116150227
Experience 23, Iter 65, disc loss: 3.0561460873233905e-05, policy loss: 26.817299483431086
Experience 23, Iter 66, disc loss: 3.059584141842881e-05, policy loss: 26.531190756037603
Experience 23, Iter 67, disc loss: 3.0611703652822216e-05, policy loss: 27.08946654767985
Experience 23, Iter 68, disc loss: 3.064740387392952e-05, policy loss: 27.210788486295115
Experience 23, Iter 69, disc loss: 3.0750309683218375e-05, policy loss: 25.23047248540056
Experience 23, Iter 70, disc loss: 3.0666602774515944e-05, policy loss: 24.576478957650835
Experience 23, Iter 71, disc loss: 3.0646361727759716e-05, policy loss: 25.521923363756258
Experience 23, Iter 72, disc loss: 3.072338216878393e-05, policy loss: 24.56340283476585
Experience 23, Iter 73, disc loss: 3.0647015976053325e-05, policy loss: 22.94117107454006
Experience 23, Iter 74, disc loss: 3.060062817085986e-05, policy loss: 23.897366973666887
Experience 23, Iter 75, disc loss: 3.058838642215864e-05, policy loss: 22.929272046503847
Experience 23, Iter 76, disc loss: 3.056923775955702e-05, policy loss: 22.375430079465126
Experience 23, Iter 77, disc loss: 3.0543099121868735e-05, policy loss: 23.27053647173239
Experience 23, Iter 78, disc loss: 3.0513159099489324e-05, policy loss: 23.059524019463858
Experience 23, Iter 79, disc loss: 3.0493308157470157e-05, policy loss: 22.551988083326503
Experience 23, Iter 80, disc loss: 3.0455498176028176e-05, policy loss: 23.25411857515801
Experience 23, Iter 81, disc loss: 3.0421217830590926e-05, policy loss: 23.456184719044135
Experience 23, Iter 82, disc loss: 3.0385065328014497e-05, policy loss: 24.652350634617754
Experience 23, Iter 83, disc loss: 3.0349715962716024e-05, policy loss: 24.53295146734294
Experience 23, Iter 84, disc loss: 3.0313014187568695e-05, policy loss: 24.68738337676441
Experience 23, Iter 85, disc loss: 3.0275159302317425e-05, policy loss: 25.32619027605372
Experience 23, Iter 86, disc loss: 3.023644640948542e-05, policy loss: 25.808254725534635
Experience 23, Iter 87, disc loss: 3.0196863438754177e-05, policy loss: 25.405109119510627
Experience 23, Iter 88, disc loss: 3.0156466604616233e-05, policy loss: 25.944216449079505
Experience 23, Iter 89, disc loss: 3.011895081342544e-05, policy loss: 25.549984662804416
Experience 23, Iter 90, disc loss: 3.007665563135784e-05, policy loss: 25.50273347255984
Experience 23, Iter 91, disc loss: 3.0493021513551504e-05, policy loss: 25.639118169924863
Experience 23, Iter 92, disc loss: 3.0155962529743712e-05, policy loss: 25.653510350944735
Experience 23, Iter 93, disc loss: 2.9945489644100468e-05, policy loss: 26.20797657092455
Experience 23, Iter 94, disc loss: 2.9915686396045332e-05, policy loss: 25.35202722158338
Experience 23, Iter 95, disc loss: 2.9858143141227574e-05, policy loss: 26.43347847158314
Experience 23, Iter 96, disc loss: 2.9815434874653068e-05, policy loss: 26.03741694799909
Experience 23, Iter 97, disc loss: 2.9793165258270848e-05, policy loss: 27.701061529814226
Experience 23, Iter 98, disc loss: 2.9746488282337404e-05, policy loss: 27.423906883186817
Experience 23, Iter 99, disc loss: 3.0174249520935902e-05, policy loss: 26.666283393995545
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0105],
        [0.2447],
        [2.2282],
        [0.0337]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0511, 0.4213, 1.5486, 0.0364, 0.0120, 6.5836]],

        [[0.0511, 0.4213, 1.5486, 0.0364, 0.0120, 6.5836]],

        [[0.0511, 0.4213, 1.5486, 0.0364, 0.0120, 6.5836]],

        [[0.0511, 0.4213, 1.5486, 0.0364, 0.0120, 6.5836]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0421, 0.9789, 8.9128, 0.1346], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0421, 0.9789, 8.9128, 0.1346], device='cuda:0')
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.067
Iter 2/2000 - Loss: 3.975
Iter 3/2000 - Loss: 3.893
Iter 4/2000 - Loss: 3.810
Iter 5/2000 - Loss: 3.736
Iter 6/2000 - Loss: 3.643
Iter 7/2000 - Loss: 3.542
Iter 8/2000 - Loss: 3.443
Iter 9/2000 - Loss: 3.330
Iter 10/2000 - Loss: 3.195
Iter 11/2000 - Loss: 3.047
Iter 12/2000 - Loss: 2.895
Iter 13/2000 - Loss: 2.735
Iter 14/2000 - Loss: 2.562
Iter 15/2000 - Loss: 2.371
Iter 16/2000 - Loss: 2.165
Iter 17/2000 - Loss: 1.946
Iter 18/2000 - Loss: 1.716
Iter 19/2000 - Loss: 1.474
Iter 20/2000 - Loss: 1.219
Iter 1981/2000 - Loss: -6.999
Iter 1982/2000 - Loss: -6.999
Iter 1983/2000 - Loss: -6.999
Iter 1984/2000 - Loss: -6.999
Iter 1985/2000 - Loss: -6.999
Iter 1986/2000 - Loss: -6.999
Iter 1987/2000 - Loss: -6.999
Iter 1988/2000 - Loss: -6.999
Iter 1989/2000 - Loss: -6.999
Iter 1990/2000 - Loss: -6.999
Iter 1991/2000 - Loss: -6.999
Iter 1992/2000 - Loss: -6.999
Iter 1993/2000 - Loss: -6.999
Iter 1994/2000 - Loss: -7.000
Iter 1995/2000 - Loss: -7.000
Iter 1996/2000 - Loss: -7.000
Iter 1997/2000 - Loss: -7.000
Iter 1998/2000 - Loss: -7.000
Iter 1999/2000 - Loss: -7.000
Iter 2000/2000 - Loss: -7.000
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0025],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[16.8317, 13.6333, 31.6863,  7.9423,  4.1320, 65.5221]],

        [[23.8195, 44.3747,  7.7045,  1.2666,  1.3094, 25.7479]],

        [[25.4651, 47.2364,  7.8511,  0.9471,  0.8409, 21.3699]],

        [[20.6231, 42.5042, 18.1706,  1.3351,  2.3090, 41.9185]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1988,  2.0549, 11.1559,  0.4911], device='cuda:0')
Estimated target variance: tensor([0.0421, 0.9789, 8.9128, 0.1346], device='cuda:0')
N: 240
Signal to noise ratio: tensor([22.7235, 64.9069, 66.9458, 38.5781], device='cuda:0')
Bound on condition number: tensor([ 123927.0061, 1011097.8398, 1075617.8892,  357186.1491],
       device='cuda:0')
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 2.9640873306127823e-05, policy loss: 30.356382269462557
Experience 24, Iter 1, disc loss: 2.98007720649239e-05, policy loss: 29.054442292360147
Experience 24, Iter 2, disc loss: 2.9587511904991107e-05, policy loss: 30.390781124880412
Experience 24, Iter 3, disc loss: 2.9757876271683886e-05, policy loss: 30.685759647728517
Experience 24, Iter 4, disc loss: 2.9768393320781472e-05, policy loss: 30.896082021504675
Experience 24, Iter 5, disc loss: 2.9414092857285686e-05, policy loss: 31.311250087836186
Experience 24, Iter 6, disc loss: 2.9373015978600023e-05, policy loss: 31.01538614257827
Experience 24, Iter 7, disc loss: 2.9320586053494667e-05, policy loss: 30.237776071347135
Experience 24, Iter 8, disc loss: 2.9275821726029933e-05, policy loss: 29.042982866111778
Experience 24, Iter 9, disc loss: 2.9733304203002833e-05, policy loss: 29.263215036830616
Experience 24, Iter 10, disc loss: 2.921061265115202e-05, policy loss: 28.15311168413598
Experience 24, Iter 11, disc loss: 2.9190642186589516e-05, policy loss: 28.6451958334997
Experience 24, Iter 12, disc loss: 2.9532543646003274e-05, policy loss: 26.923886829349414
Experience 24, Iter 13, disc loss: 2.9073260553551707e-05, policy loss: 27.29443955632646
Experience 24, Iter 14, disc loss: 2.9264341130856686e-05, policy loss: 26.378515243761967
Experience 24, Iter 15, disc loss: 2.9050340803861302e-05, policy loss: 26.649985694860906
Experience 24, Iter 16, disc loss: 2.9065745245855343e-05, policy loss: 25.46319498447739
Experience 24, Iter 17, disc loss: 2.8935371648869106e-05, policy loss: 26.17824977439099
Experience 24, Iter 18, disc loss: 3.1199755505810565e-05, policy loss: 24.610992418203704
Experience 24, Iter 19, disc loss: 2.952005518534671e-05, policy loss: 23.95331805169545
Experience 24, Iter 20, disc loss: 2.8949318111896424e-05, policy loss: 24.413548737599275
Experience 24, Iter 21, disc loss: 2.9874600028818545e-05, policy loss: 23.84347509747796
Experience 24, Iter 22, disc loss: 2.8903551356296398e-05, policy loss: 24.15821160290508
Experience 24, Iter 23, disc loss: 2.859958073721065e-05, policy loss: 24.43863092001339
Experience 24, Iter 24, disc loss: 2.9395428362904798e-05, policy loss: 23.781327350305613
Experience 24, Iter 25, disc loss: 2.8513336811367858e-05, policy loss: 24.315761698193807
Experience 24, Iter 26, disc loss: 2.9063040628969745e-05, policy loss: 24.02580031151529
Experience 24, Iter 27, disc loss: 2.876702476014097e-05, policy loss: 23.761221972649686
Experience 24, Iter 28, disc loss: 2.8550500107139145e-05, policy loss: 23.37423364639639
Experience 24, Iter 29, disc loss: 2.8680493595070964e-05, policy loss: 23.86093015964488
Experience 24, Iter 30, disc loss: 3.0145615293385442e-05, policy loss: 22.790672725406854
Experience 24, Iter 31, disc loss: 2.8770591910931294e-05, policy loss: 23.923852741133576
Experience 24, Iter 32, disc loss: 2.82051806779877e-05, policy loss: 23.681107008035106
Experience 24, Iter 33, disc loss: 2.858634627345747e-05, policy loss: 23.542966029155473
Experience 24, Iter 34, disc loss: 2.822685162097836e-05, policy loss: 22.90387072641896
Experience 24, Iter 35, disc loss: 2.8335003443849578e-05, policy loss: 22.512154546330038
Experience 24, Iter 36, disc loss: 2.811263842676405e-05, policy loss: 23.04989453186932
Experience 24, Iter 37, disc loss: 2.8614379309255847e-05, policy loss: 22.35561401871739
Experience 24, Iter 38, disc loss: 2.817443728446505e-05, policy loss: 22.63764728463634
Experience 24, Iter 39, disc loss: 2.790043918094046e-05, policy loss: 22.415138591013676
Experience 24, Iter 40, disc loss: 2.8900672466775327e-05, policy loss: 21.40523024914029
Experience 24, Iter 41, disc loss: 2.9266198841036227e-05, policy loss: 20.946434269818074
Experience 24, Iter 42, disc loss: 3.079030926455109e-05, policy loss: 20.54585485155071
Experience 24, Iter 43, disc loss: 2.8060486404979868e-05, policy loss: 20.46176069066444
Experience 24, Iter 44, disc loss: 2.7754020719025652e-05, policy loss: 20.818484185312066
Experience 24, Iter 45, disc loss: 5.9481312297062026e-05, policy loss: 19.678910310896264
Experience 24, Iter 46, disc loss: 4.988736971474457e-05, policy loss: 19.571861592999202
Experience 24, Iter 47, disc loss: 2.9410012529422423e-05, policy loss: 18.835936695545044
Experience 24, Iter 48, disc loss: 3.107877865029146e-05, policy loss: 19.069683301594722
Experience 24, Iter 49, disc loss: 2.912440238726508e-05, policy loss: 18.93785905501851
Experience 24, Iter 50, disc loss: 7.814119124796005e-05, policy loss: 17.256779858460703
Experience 24, Iter 51, disc loss: 6.277026275815573e-05, policy loss: 17.941168797916763
Experience 24, Iter 52, disc loss: 3.43306561776088e-05, policy loss: 16.65710998788593
Experience 24, Iter 53, disc loss: 6.242089522645388e-05, policy loss: 16.187382975066757
Experience 24, Iter 54, disc loss: 4.175881484950867e-05, policy loss: 17.39470565510055
Experience 24, Iter 55, disc loss: 8.488806739641128e-05, policy loss: 17.12533144524848
Experience 24, Iter 56, disc loss: 0.00019364634180530076, policy loss: 17.584794915178765
Experience 24, Iter 57, disc loss: 3.605514171079629e-05, policy loss: 18.178372292846163
Experience 24, Iter 58, disc loss: 0.00017967964141158156, policy loss: 17.47653570007948
Experience 24, Iter 59, disc loss: 7.49428810507047e-05, policy loss: 17.18381016441741
Experience 24, Iter 60, disc loss: 3.632013908223265e-05, policy loss: 17.05717801537904
Experience 24, Iter 61, disc loss: 4.987987694241481e-05, policy loss: 17.067930894106908
Experience 24, Iter 62, disc loss: 4.424032430452504e-05, policy loss: 17.734173910021568
Experience 24, Iter 63, disc loss: 6.185586142253435e-05, policy loss: 16.918987633973636
Experience 24, Iter 64, disc loss: 4.581221893910568e-05, policy loss: 16.374110798484978
Experience 24, Iter 65, disc loss: 8.127091807977658e-05, policy loss: 17.227974484170616
Experience 24, Iter 66, disc loss: 3.7392641421936375e-05, policy loss: 17.4701991019215
Experience 24, Iter 67, disc loss: 3.84719623384576e-05, policy loss: 17.333293225061965
Experience 24, Iter 68, disc loss: 4.148594920721678e-05, policy loss: 17.052237881916994
Experience 24, Iter 69, disc loss: 3.2940957311908076e-05, policy loss: 17.889707588829047
Experience 24, Iter 70, disc loss: 2.9143617499625314e-05, policy loss: 17.789356649419886
Experience 24, Iter 71, disc loss: 4.107375159970488e-05, policy loss: 18.07949784587206
Experience 24, Iter 72, disc loss: 2.7931996662167134e-05, policy loss: 19.067608431920156
Experience 24, Iter 73, disc loss: 2.9688672459202042e-05, policy loss: 18.01553514218724
Experience 24, Iter 74, disc loss: 2.803372120473514e-05, policy loss: 18.649358227760874
Experience 24, Iter 75, disc loss: 2.982586259399052e-05, policy loss: 18.05061805871729
Experience 24, Iter 76, disc loss: 3.810519462773132e-05, policy loss: 17.38909033097504
Experience 24, Iter 77, disc loss: 5.347520739477465e-05, policy loss: 16.365040541287648
Experience 24, Iter 78, disc loss: 4.348340315334605e-05, policy loss: 18.447841700521558
Experience 24, Iter 79, disc loss: 3.762086433746853e-05, policy loss: 16.776154619580545
Experience 24, Iter 80, disc loss: 5.3380674316647334e-05, policy loss: 17.245103373078187
Experience 24, Iter 81, disc loss: 3.289794035628201e-05, policy loss: 16.871645263743517
Experience 24, Iter 82, disc loss: 4.971589528429534e-05, policy loss: 16.36197539446259
Experience 24, Iter 83, disc loss: 3.413046960996577e-05, policy loss: 17.067700774110577
Experience 24, Iter 84, disc loss: 3.0485921238854858e-05, policy loss: 17.38597666705099
Experience 24, Iter 85, disc loss: 5.570351771981819e-05, policy loss: 16.03343838049924
Experience 24, Iter 86, disc loss: 0.0001250166304633233, policy loss: 16.324632128972187
Experience 24, Iter 87, disc loss: 0.00013375187529118444, policy loss: 16.893966285798406
Experience 24, Iter 88, disc loss: 3.380444733993628e-05, policy loss: 16.571889004414807
Experience 24, Iter 89, disc loss: 7.918576535071243e-05, policy loss: 17.19600510667991
Experience 24, Iter 90, disc loss: 0.0001696421553836807, policy loss: 16.363846604116695
Experience 24, Iter 91, disc loss: 6.25547850363798e-05, policy loss: 16.216646072820712
Experience 24, Iter 92, disc loss: 8.144867088941813e-05, policy loss: 16.076756447220824
Experience 24, Iter 93, disc loss: 7.402716168858163e-05, policy loss: 16.39208583031974
Experience 24, Iter 94, disc loss: 5.8233339792480356e-05, policy loss: 16.354818146116834
Experience 24, Iter 95, disc loss: 4.912825237008316e-05, policy loss: 16.099114778326804
Experience 24, Iter 96, disc loss: 5.1225157312141095e-05, policy loss: 16.872618079592456
Experience 24, Iter 97, disc loss: 5.908873743257092e-05, policy loss: 16.052151803545474
Experience 24, Iter 98, disc loss: 6.91214811542072e-05, policy loss: 16.327548174823974
Experience 24, Iter 99, disc loss: 5.544326214481406e-05, policy loss: 15.960547805847288
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0102],
        [0.2456],
        [2.2345],
        [0.0330]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0501, 0.4097, 1.5243, 0.0357, 0.0117, 6.5708]],

        [[0.0501, 0.4097, 1.5243, 0.0357, 0.0117, 6.5708]],

        [[0.0501, 0.4097, 1.5243, 0.0357, 0.0117, 6.5708]],

        [[0.0501, 0.4097, 1.5243, 0.0357, 0.0117, 6.5708]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0408, 0.9825, 8.9378, 0.1319], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0408, 0.9825, 8.9378, 0.1319], device='cuda:0')
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.059
Iter 2/2000 - Loss: 3.963
Iter 3/2000 - Loss: 3.888
Iter 4/2000 - Loss: 3.802
Iter 5/2000 - Loss: 3.726
Iter 6/2000 - Loss: 3.636
Iter 7/2000 - Loss: 3.535
Iter 8/2000 - Loss: 3.432
Iter 9/2000 - Loss: 3.316
Iter 10/2000 - Loss: 3.180
Iter 11/2000 - Loss: 3.030
Iter 12/2000 - Loss: 2.873
Iter 13/2000 - Loss: 2.708
Iter 14/2000 - Loss: 2.530
Iter 15/2000 - Loss: 2.336
Iter 16/2000 - Loss: 2.126
Iter 17/2000 - Loss: 1.903
Iter 18/2000 - Loss: 1.669
Iter 19/2000 - Loss: 1.424
Iter 20/2000 - Loss: 1.166
Iter 1981/2000 - Loss: -7.115
Iter 1982/2000 - Loss: -7.115
Iter 1983/2000 - Loss: -7.115
Iter 1984/2000 - Loss: -7.115
Iter 1985/2000 - Loss: -7.116
Iter 1986/2000 - Loss: -7.116
Iter 1987/2000 - Loss: -7.116
Iter 1988/2000 - Loss: -7.116
Iter 1989/2000 - Loss: -7.116
Iter 1990/2000 - Loss: -7.116
Iter 1991/2000 - Loss: -7.116
Iter 1992/2000 - Loss: -7.116
Iter 1993/2000 - Loss: -7.116
Iter 1994/2000 - Loss: -7.116
Iter 1995/2000 - Loss: -7.116
Iter 1996/2000 - Loss: -7.116
Iter 1997/2000 - Loss: -7.116
Iter 1998/2000 - Loss: -7.116
Iter 1999/2000 - Loss: -7.116
Iter 2000/2000 - Loss: -7.116
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[16.5837, 13.5670, 30.8898,  7.5543,  3.7891, 64.8159]],

        [[23.7681, 44.6100,  7.8369,  1.2687,  1.2714, 26.7143]],

        [[25.0364, 46.9021,  7.7242,  0.9487,  0.8545, 21.2851]],

        [[20.3780, 41.9068, 18.0898,  1.3611,  2.2756, 41.7119]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1967,  2.1094, 11.0392,  0.4816], device='cuda:0')
Estimated target variance: tensor([0.0408, 0.9825, 8.9378, 0.1319], device='cuda:0')
N: 250
Signal to noise ratio: tensor([22.9188, 66.4786, 68.1111, 38.6648], device='cuda:0')
Bound on condition number: tensor([ 131318.3548, 1104851.1398, 1159782.4476,  373743.4641],
       device='cuda:0')
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 4.437304491678182e-05, policy loss: 16.45351856605256
Experience 25, Iter 1, disc loss: 5.1951330269244134e-05, policy loss: 15.50918315813303
Experience 25, Iter 2, disc loss: 5.072172302170742e-05, policy loss: 16.35164716133155
Experience 25, Iter 3, disc loss: 9.97822054124372e-05, policy loss: 16.228844442530256
Experience 25, Iter 4, disc loss: 8.630856894113616e-05, policy loss: 15.722167908792557
Experience 25, Iter 5, disc loss: 0.00015664079731842322, policy loss: 14.698127631639885
Experience 25, Iter 6, disc loss: 6.575436257411836e-05, policy loss: 15.944345796389474
Experience 25, Iter 7, disc loss: 6.505325384572363e-05, policy loss: 15.66849920476314
Experience 25, Iter 8, disc loss: 5.24010235520622e-05, policy loss: 15.694141548552729
Experience 25, Iter 9, disc loss: 6.661240163430327e-05, policy loss: 15.572191593681847
Experience 25, Iter 10, disc loss: 0.00011152909185905755, policy loss: 15.390579712531643
Experience 25, Iter 11, disc loss: 5.81161006000012e-05, policy loss: 15.465001194430549
Experience 25, Iter 12, disc loss: 9.571683714758716e-05, policy loss: 15.292140411375927
Experience 25, Iter 13, disc loss: 0.00023815986845465551, policy loss: 14.439525111161405
Experience 25, Iter 14, disc loss: 6.0393828734624426e-05, policy loss: 15.464511553994885
Experience 25, Iter 15, disc loss: 0.00019440156484622326, policy loss: 14.713047627427464
Experience 25, Iter 16, disc loss: 0.00014169516386250492, policy loss: 15.007775306299909
Experience 25, Iter 17, disc loss: 0.0003966244999590832, policy loss: 14.047592739555796
Experience 25, Iter 18, disc loss: 5.407843621883115e-05, policy loss: 14.76849105798896
Experience 25, Iter 19, disc loss: 0.00014725349890056105, policy loss: 14.134699785752595
Experience 25, Iter 20, disc loss: 0.00012125092284951015, policy loss: 14.187938721913953
Experience 25, Iter 21, disc loss: 0.00012598025801610475, policy loss: 13.6461374763911
Experience 25, Iter 22, disc loss: 0.00016050082770375625, policy loss: 13.782855168670084
Experience 25, Iter 23, disc loss: 4.501914299277477e-05, policy loss: 14.663722076630147
Experience 25, Iter 24, disc loss: 6.872845635035306e-05, policy loss: 14.660228535141727
Experience 25, Iter 25, disc loss: 5.7752982752920856e-05, policy loss: 13.729234730015715
Experience 25, Iter 26, disc loss: 5.748215061748859e-05, policy loss: 13.988255695701
Experience 25, Iter 27, disc loss: 9.07710459388427e-05, policy loss: 13.094407243089353
Experience 25, Iter 28, disc loss: 9.07561303582196e-05, policy loss: 13.570667673848067
Experience 25, Iter 29, disc loss: 0.0001038407160734071, policy loss: 13.336997457950915
Experience 25, Iter 30, disc loss: 0.00015128043312076874, policy loss: 13.555352500316587
Experience 25, Iter 31, disc loss: 9.006800008069225e-05, policy loss: 13.831500735128285
Experience 25, Iter 32, disc loss: 7.586858135127372e-05, policy loss: 12.682100933879921
Experience 25, Iter 33, disc loss: 6.773571372054407e-05, policy loss: 13.697536584707574
Experience 25, Iter 34, disc loss: 0.00011567548709547576, policy loss: 13.87143124907153
Experience 25, Iter 35, disc loss: 0.00010852413357339455, policy loss: 13.539502017120665
Experience 25, Iter 36, disc loss: 0.00013119289119744058, policy loss: 12.5853508562544
Experience 25, Iter 37, disc loss: 0.00012102297437108716, policy loss: 13.432154871067102
Experience 25, Iter 38, disc loss: 0.00013478305561575563, policy loss: 14.48658232806424
Experience 25, Iter 39, disc loss: 0.00020770634579892757, policy loss: 12.644357793762723
Experience 25, Iter 40, disc loss: 0.0001589417290091001, policy loss: 12.804829682811736
Experience 25, Iter 41, disc loss: 0.00010953696299282633, policy loss: 13.23370143752749
Experience 25, Iter 42, disc loss: 0.00010647605897890341, policy loss: 13.184821389742694
Experience 25, Iter 43, disc loss: 0.00013119980405942362, policy loss: 13.040404399547974
Experience 25, Iter 44, disc loss: 0.00031322362532466174, policy loss: 13.059367736524525
Experience 25, Iter 45, disc loss: 0.00015972378476400689, policy loss: 13.16985073674347
Experience 25, Iter 46, disc loss: 0.00014820771146547518, policy loss: 13.544146353790238
Experience 25, Iter 47, disc loss: 0.00019719737009586382, policy loss: 12.458936462908524
Experience 25, Iter 48, disc loss: 0.00018205455773590237, policy loss: 12.681123575162776
Experience 25, Iter 49, disc loss: 0.00030916741982940296, policy loss: 12.177583708154366
Experience 25, Iter 50, disc loss: 0.00016800915563524915, policy loss: 12.452827417486905
Experience 25, Iter 51, disc loss: 0.00021025351574589075, policy loss: 12.409944113986526
Experience 25, Iter 52, disc loss: 0.00024925926330108584, policy loss: 13.297750719519705
Experience 25, Iter 53, disc loss: 0.000594335306413785, policy loss: 11.674920073005094
Experience 25, Iter 54, disc loss: 0.00041832643877032704, policy loss: 12.294442742969995
Experience 25, Iter 55, disc loss: 0.00012659245197158697, policy loss: 12.965574627827774
Experience 25, Iter 56, disc loss: 0.00040645728165819117, policy loss: 12.512142334585173
Experience 25, Iter 57, disc loss: 0.00027696312819881346, policy loss: 11.437928572087019
Experience 25, Iter 58, disc loss: 0.00016888041744477763, policy loss: 11.839727745901287
Experience 25, Iter 59, disc loss: 0.00018015577126179416, policy loss: 11.808984693700257
Experience 25, Iter 60, disc loss: 0.00043135959351294786, policy loss: 11.16487517566078
Experience 25, Iter 61, disc loss: 0.00027387653658664696, policy loss: 11.868714461890836
Experience 25, Iter 62, disc loss: 0.0004904697041611163, policy loss: 11.835266599978711
Experience 25, Iter 63, disc loss: 0.0001944839136805716, policy loss: 12.292822105860761
Experience 25, Iter 64, disc loss: 0.00028732985085241127, policy loss: 12.929393539130272
Experience 25, Iter 65, disc loss: 0.0013884329622918317, policy loss: 11.642329481864241
Experience 25, Iter 66, disc loss: 0.0005028122442271193, policy loss: 12.543997754104748
Experience 25, Iter 67, disc loss: 0.0018599739845636119, policy loss: 12.434230814103948
Experience 25, Iter 68, disc loss: 0.0007254777148238265, policy loss: 12.32562321176603
Experience 25, Iter 69, disc loss: 0.002034043077751635, policy loss: 10.989871820322602
Experience 25, Iter 70, disc loss: 0.0015159862110364884, policy loss: 11.640464315121113
Experience 25, Iter 71, disc loss: 0.0016324955759302166, policy loss: 11.959103565961023
Experience 25, Iter 72, disc loss: 0.0013313375802381656, policy loss: 11.38949963956242
Experience 25, Iter 73, disc loss: 0.002177398058218973, policy loss: 11.595063720481114
Experience 25, Iter 74, disc loss: 0.0009103461024918703, policy loss: 11.284446214490298
Experience 25, Iter 75, disc loss: 0.0007595957626578655, policy loss: 11.551581483539648
Experience 25, Iter 76, disc loss: 0.0007679119499202458, policy loss: 11.769700976084394
Experience 25, Iter 77, disc loss: 0.0005919692986727651, policy loss: 12.23588491870104
Experience 25, Iter 78, disc loss: 0.001758055681680865, policy loss: 12.452912360328861
Experience 25, Iter 79, disc loss: 0.00037836229558978174, policy loss: 13.208687638196814
Experience 25, Iter 80, disc loss: 0.00014972485389627124, policy loss: 12.310573521488003
Experience 25, Iter 81, disc loss: 0.00023818539288765215, policy loss: 13.45084446098619
Experience 25, Iter 82, disc loss: 9.299590585624524e-05, policy loss: 13.532792449558686
Experience 25, Iter 83, disc loss: 9.246333281392273e-05, policy loss: 13.68325537202115
Experience 25, Iter 84, disc loss: 8.196219210164304e-05, policy loss: 13.82417330025488
Experience 25, Iter 85, disc loss: 5.336084568134056e-05, policy loss: 14.449500541894107
Experience 25, Iter 86, disc loss: 6.35456215121932e-05, policy loss: 14.668002793581536
Experience 25, Iter 87, disc loss: 8.568163977596495e-05, policy loss: 15.256722401107595
Experience 25, Iter 88, disc loss: 5.7362300101666964e-05, policy loss: 15.974763457146246
Experience 25, Iter 89, disc loss: 7.753747093414148e-05, policy loss: 16.471392481696505
Experience 25, Iter 90, disc loss: 5.408801652898268e-05, policy loss: 16.49127741437794
Experience 25, Iter 91, disc loss: 5.404826934573534e-05, policy loss: 16.763234956457502
Experience 25, Iter 92, disc loss: 5.9062475854958476e-05, policy loss: 16.942388322422158
Experience 25, Iter 93, disc loss: 5.695845583699128e-05, policy loss: 17.251770198889467
Experience 25, Iter 94, disc loss: 5.987119569138974e-05, policy loss: 17.281968900331908
Experience 25, Iter 95, disc loss: 5.586566685738753e-05, policy loss: 15.759526840171443
Experience 25, Iter 96, disc loss: 5.3962290555999565e-05, policy loss: 17.531739920572697
Experience 25, Iter 97, disc loss: 5.646178917337016e-05, policy loss: 16.521623552614415
Experience 25, Iter 98, disc loss: 5.8524231089822866e-05, policy loss: 17.09764752588988
Experience 25, Iter 99, disc loss: 8.079356730298195e-05, policy loss: 15.885828690917851
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0100],
        [0.2478],
        [2.2535],
        [0.0331]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0504, 0.4026, 1.5341, 0.0358, 0.0116, 6.6535]],

        [[0.0504, 0.4026, 1.5341, 0.0358, 0.0116, 6.6535]],

        [[0.0504, 0.4026, 1.5341, 0.0358, 0.0116, 6.6535]],

        [[0.0504, 0.4026, 1.5341, 0.0358, 0.0116, 6.6535]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0401, 0.9913, 9.0139, 0.1323], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0401, 0.9913, 9.0139, 0.1323], device='cuda:0')
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.054
Iter 2/2000 - Loss: 3.947
Iter 3/2000 - Loss: 3.882
Iter 4/2000 - Loss: 3.792
Iter 5/2000 - Loss: 3.714
Iter 6/2000 - Loss: 3.629
Iter 7/2000 - Loss: 3.529
Iter 8/2000 - Loss: 3.424
Iter 9/2000 - Loss: 3.309
Iter 10/2000 - Loss: 3.177
Iter 11/2000 - Loss: 3.030
Iter 12/2000 - Loss: 2.874
Iter 13/2000 - Loss: 2.708
Iter 14/2000 - Loss: 2.530
Iter 15/2000 - Loss: 2.336
Iter 16/2000 - Loss: 2.126
Iter 17/2000 - Loss: 1.903
Iter 18/2000 - Loss: 1.666
Iter 19/2000 - Loss: 1.419
Iter 20/2000 - Loss: 1.159
Iter 1981/2000 - Loss: -7.162
Iter 1982/2000 - Loss: -7.162
Iter 1983/2000 - Loss: -7.163
Iter 1984/2000 - Loss: -7.163
Iter 1985/2000 - Loss: -7.163
Iter 1986/2000 - Loss: -7.163
Iter 1987/2000 - Loss: -7.163
Iter 1988/2000 - Loss: -7.163
Iter 1989/2000 - Loss: -7.163
Iter 1990/2000 - Loss: -7.163
Iter 1991/2000 - Loss: -7.163
Iter 1992/2000 - Loss: -7.163
Iter 1993/2000 - Loss: -7.163
Iter 1994/2000 - Loss: -7.163
Iter 1995/2000 - Loss: -7.163
Iter 1996/2000 - Loss: -7.163
Iter 1997/2000 - Loss: -7.163
Iter 1998/2000 - Loss: -7.163
Iter 1999/2000 - Loss: -7.163
Iter 2000/2000 - Loss: -7.163
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0023],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[16.9979, 13.3858, 28.0784,  3.9800,  5.2771, 65.4149]],

        [[23.6997, 43.8607,  7.8363,  1.2781,  1.2980, 25.5004]],

        [[24.4378, 46.6817,  7.4151,  0.9367,  0.8146, 21.8671]],

        [[20.3402, 41.6050, 18.3640,  1.3268,  2.4019, 43.1627]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1790,  2.0549, 10.5556,  0.4932], device='cuda:0')
Estimated target variance: tensor([0.0401, 0.9913, 9.0139, 0.1323], device='cuda:0')
N: 260
Signal to noise ratio: tensor([21.8732, 66.1228, 67.9395, 39.5377], device='cuda:0')
Bound on condition number: tensor([ 124394.0276, 1136778.1428, 1200103.5470,  406441.3129],
       device='cuda:0')
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 7.526417329076801e-05, policy loss: 16.635654616414055
Experience 26, Iter 1, disc loss: 6.677442134899516e-05, policy loss: 17.563707409184346
Experience 26, Iter 2, disc loss: 5.7230963661822614e-05, policy loss: 17.03347361289535
Experience 26, Iter 3, disc loss: 5.474612027478012e-05, policy loss: 16.709545171604795
Experience 26, Iter 4, disc loss: 5.752383720242675e-05, policy loss: 17.251791796702662
Experience 26, Iter 5, disc loss: 5.3191099887761304e-05, policy loss: 18.346356992772158
Experience 26, Iter 6, disc loss: 6.0873690393063375e-05, policy loss: 16.85190356830566
Experience 26, Iter 7, disc loss: 5.8389551509220094e-05, policy loss: 17.723921470455654
Experience 26, Iter 8, disc loss: 5.545629784623459e-05, policy loss: 17.777752882776696
Experience 26, Iter 9, disc loss: 5.3104168501450747e-05, policy loss: 17.56509078994015
Experience 26, Iter 10, disc loss: 7.19152413043113e-05, policy loss: 16.995829656167615
Experience 26, Iter 11, disc loss: 6.312357105097502e-05, policy loss: 16.748261017274864
Experience 26, Iter 12, disc loss: 6.285534857546944e-05, policy loss: 17.503904944762255
Experience 26, Iter 13, disc loss: 5.5656764285345966e-05, policy loss: 16.91206543960796
Experience 26, Iter 14, disc loss: 5.911405977758549e-05, policy loss: 17.003228446560094
Experience 26, Iter 15, disc loss: 5.386565369612333e-05, policy loss: 17.45147975644171
Experience 26, Iter 16, disc loss: 5.279510238943504e-05, policy loss: 16.537637461038337
Experience 26, Iter 17, disc loss: 5.542248439517686e-05, policy loss: 18.015478204860578
Experience 26, Iter 18, disc loss: 5.864766046724042e-05, policy loss: 17.03925917752526
Experience 26, Iter 19, disc loss: 6.325437688101647e-05, policy loss: 16.869062857322454
Experience 26, Iter 20, disc loss: 9.01277214545023e-05, policy loss: 17.157813043539413
Experience 26, Iter 21, disc loss: 5.535402975456278e-05, policy loss: 17.401227886845405
Experience 26, Iter 22, disc loss: 5.852681856119191e-05, policy loss: 17.196109061915834
Experience 26, Iter 23, disc loss: 5.6660327639037764e-05, policy loss: 16.992248196255915
Experience 26, Iter 24, disc loss: 5.455371394200253e-05, policy loss: 16.67928096369546
Experience 26, Iter 25, disc loss: 6.777515287894778e-05, policy loss: 16.003370920491353
Experience 26, Iter 26, disc loss: 6.751955219460791e-05, policy loss: 16.05335955213824
Experience 26, Iter 27, disc loss: 5.441876380929628e-05, policy loss: 16.791574823822202
Experience 26, Iter 28, disc loss: 6.330192016670746e-05, policy loss: 16.247283011190948
Experience 26, Iter 29, disc loss: 5.553029931343141e-05, policy loss: 15.60653748533
Experience 26, Iter 30, disc loss: 5.28415993999544e-05, policy loss: 16.76069802256717
Experience 26, Iter 31, disc loss: 5.578238044552197e-05, policy loss: 16.924571594914102
Experience 26, Iter 32, disc loss: 6.34246004271021e-05, policy loss: 17.166629651129266
Experience 26, Iter 33, disc loss: 9.859982118490886e-05, policy loss: 16.631808107211825
Experience 26, Iter 34, disc loss: 5.550581367187805e-05, policy loss: 16.36970293762062
Experience 26, Iter 35, disc loss: 5.987084246323986e-05, policy loss: 16.06007735239932
Experience 26, Iter 36, disc loss: 5.804064448144634e-05, policy loss: 15.727421920600978
Experience 26, Iter 37, disc loss: 5.888741659713734e-05, policy loss: 16.44626864934888
Experience 26, Iter 38, disc loss: 5.299435937065778e-05, policy loss: 16.063573558138682
Experience 26, Iter 39, disc loss: 5.2996570436908564e-05, policy loss: 16.75563893642838
Experience 26, Iter 40, disc loss: 9.15613025403756e-05, policy loss: 15.677305324746872
Experience 26, Iter 41, disc loss: 6.075994450397941e-05, policy loss: 16.16100927548467
Experience 26, Iter 42, disc loss: 8.638930985666317e-05, policy loss: 16.188142142569962
Experience 26, Iter 43, disc loss: 7.981344362307589e-05, policy loss: 16.022060622253434
Experience 26, Iter 44, disc loss: 7.44903246546198e-05, policy loss: 15.817600707872273
Experience 26, Iter 45, disc loss: 9.41809705524013e-05, policy loss: 15.03424955540877
Experience 26, Iter 46, disc loss: 9.091068028945442e-05, policy loss: 15.199259081148016
Experience 26, Iter 47, disc loss: 5.817445740672892e-05, policy loss: 15.501814278792436
Experience 26, Iter 48, disc loss: 8.205221212476316e-05, policy loss: 16.538299618446974
Experience 26, Iter 49, disc loss: 7.58506434751148e-05, policy loss: 15.907527989740187
Experience 26, Iter 50, disc loss: 9.95483680695507e-05, policy loss: 15.865490032945548
Experience 26, Iter 51, disc loss: 6.345945818906853e-05, policy loss: 16.242944666979774
Experience 26, Iter 52, disc loss: 7.106340210528191e-05, policy loss: 15.961891613399171
Experience 26, Iter 53, disc loss: 0.0001361938066788526, policy loss: 15.192559330908132
Experience 26, Iter 54, disc loss: 5.794619406811028e-05, policy loss: 15.519121412190003
Experience 26, Iter 55, disc loss: 9.5837874205029e-05, policy loss: 15.899032922027521
Experience 26, Iter 56, disc loss: 7.145136126521259e-05, policy loss: 15.695577689175089
Experience 26, Iter 57, disc loss: 8.288876958962427e-05, policy loss: 15.609768778864826
Experience 26, Iter 58, disc loss: 0.0001193584612267133, policy loss: 15.282115079353648
Experience 26, Iter 59, disc loss: 0.00013359025789151934, policy loss: 14.876609992836427
Experience 26, Iter 60, disc loss: 6.985042982957022e-05, policy loss: 15.180063714523314
Experience 26, Iter 61, disc loss: 6.886896846732786e-05, policy loss: 17.251306120168017
Experience 26, Iter 62, disc loss: 7.845078382413916e-05, policy loss: 17.3363248996523
Experience 26, Iter 63, disc loss: 0.00016644890455914217, policy loss: 15.787634888899575
Experience 26, Iter 64, disc loss: 0.00022550550433948145, policy loss: 15.53135091594083
Experience 26, Iter 65, disc loss: 0.00010192386533249805, policy loss: 16.568725810241858
Experience 26, Iter 66, disc loss: 0.0003958882451663183, policy loss: 15.785317408852231
Experience 26, Iter 67, disc loss: 0.0001227462550373422, policy loss: 16.075829421390438
Experience 26, Iter 68, disc loss: 0.00030500481634865057, policy loss: 15.849510056751722
Experience 26, Iter 69, disc loss: 0.00011556961892276446, policy loss: 15.166302551299278
Experience 26, Iter 70, disc loss: 0.00018869345304803626, policy loss: 16.36414076194052
Experience 26, Iter 71, disc loss: 0.00012567647060621884, policy loss: 16.38889050799266
Experience 26, Iter 72, disc loss: 9.974906781176148e-05, policy loss: 16.006871563544717
Experience 26, Iter 73, disc loss: 0.000212941374984888, policy loss: 15.883426283897812
Experience 26, Iter 74, disc loss: 8.274762070868268e-05, policy loss: 16.422062488335044
Experience 26, Iter 75, disc loss: 0.00012090425764024552, policy loss: 15.811000567354832
Experience 26, Iter 76, disc loss: 8.955441009277252e-05, policy loss: 14.435790216084229
Experience 26, Iter 77, disc loss: 0.0009010541118798926, policy loss: 14.605028459745803
Experience 26, Iter 78, disc loss: 0.00017746283365743278, policy loss: 14.64962136140747
Experience 26, Iter 79, disc loss: 0.00034519719441841024, policy loss: 14.741118658922513
Experience 26, Iter 80, disc loss: 0.00023183996839060315, policy loss: 14.94397041055986
Experience 26, Iter 81, disc loss: 9.169248498831079e-05, policy loss: 14.610526268776596
Experience 26, Iter 82, disc loss: 0.000112605557871548, policy loss: 15.91883798333578
Experience 26, Iter 83, disc loss: 0.00018431520582685418, policy loss: 15.033149368491092
Experience 26, Iter 84, disc loss: 7.700651461466795e-05, policy loss: 14.876065916999611
Experience 26, Iter 85, disc loss: 0.00014069086067832484, policy loss: 14.584097825237079
Experience 26, Iter 86, disc loss: 0.00013586042153522493, policy loss: 15.42889977112988
Experience 26, Iter 87, disc loss: 0.00017941905479840794, policy loss: 14.72687218225207
Experience 26, Iter 88, disc loss: 0.00017549886874624422, policy loss: 14.634984975938199
Experience 26, Iter 89, disc loss: 0.0001929958048512968, policy loss: 14.539154667781235
Experience 26, Iter 90, disc loss: 0.0001057455742095481, policy loss: 14.477494742187968
Experience 26, Iter 91, disc loss: 7.835700913984475e-05, policy loss: 14.448879176955183
Experience 26, Iter 92, disc loss: 0.00011017349403021834, policy loss: 14.933749392727954
Experience 26, Iter 93, disc loss: 7.839656606701508e-05, policy loss: 14.133724729172114
Experience 26, Iter 94, disc loss: 9.102914380705474e-05, policy loss: 13.595442818786886
Experience 26, Iter 95, disc loss: 0.00010757509931669612, policy loss: 14.4636389982537
Experience 26, Iter 96, disc loss: 0.00010914495046388343, policy loss: 14.345011189495041
Experience 26, Iter 97, disc loss: 0.00012315181313609893, policy loss: 14.536662089400386
Experience 26, Iter 98, disc loss: 8.994488042843306e-05, policy loss: 14.104707500368916
Experience 26, Iter 99, disc loss: 6.806540794527881e-05, policy loss: 14.64734258955463
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0099],
        [0.2504],
        [2.2779],
        [0.0331]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0504, 0.3974, 1.5390, 0.0358, 0.0115, 6.7262]],

        [[0.0504, 0.3974, 1.5390, 0.0358, 0.0115, 6.7262]],

        [[0.0504, 0.3974, 1.5390, 0.0358, 0.0115, 6.7262]],

        [[0.0504, 0.3974, 1.5390, 0.0358, 0.0115, 6.7262]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0395, 1.0017, 9.1116, 0.1323], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0395, 1.0017, 9.1116, 0.1323], device='cuda:0')
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.047
Iter 2/2000 - Loss: 3.929
Iter 3/2000 - Loss: 3.866
Iter 4/2000 - Loss: 3.769
Iter 5/2000 - Loss: 3.680
Iter 6/2000 - Loss: 3.593
Iter 7/2000 - Loss: 3.489
Iter 8/2000 - Loss: 3.374
Iter 9/2000 - Loss: 3.252
Iter 10/2000 - Loss: 3.116
Iter 11/2000 - Loss: 2.966
Iter 12/2000 - Loss: 2.803
Iter 13/2000 - Loss: 2.630
Iter 14/2000 - Loss: 2.445
Iter 15/2000 - Loss: 2.247
Iter 16/2000 - Loss: 2.034
Iter 17/2000 - Loss: 1.808
Iter 18/2000 - Loss: 1.569
Iter 19/2000 - Loss: 1.318
Iter 20/2000 - Loss: 1.057
Iter 1981/2000 - Loss: -7.235
Iter 1982/2000 - Loss: -7.235
Iter 1983/2000 - Loss: -7.235
Iter 1984/2000 - Loss: -7.235
Iter 1985/2000 - Loss: -7.235
Iter 1986/2000 - Loss: -7.235
Iter 1987/2000 - Loss: -7.235
Iter 1988/2000 - Loss: -7.235
Iter 1989/2000 - Loss: -7.235
Iter 1990/2000 - Loss: -7.235
Iter 1991/2000 - Loss: -7.235
Iter 1992/2000 - Loss: -7.235
Iter 1993/2000 - Loss: -7.235
Iter 1994/2000 - Loss: -7.235
Iter 1995/2000 - Loss: -7.235
Iter 1996/2000 - Loss: -7.235
Iter 1997/2000 - Loss: -7.236
Iter 1998/2000 - Loss: -7.236
Iter 1999/2000 - Loss: -7.236
Iter 2000/2000 - Loss: -7.236
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.2532, 13.7260, 24.1914,  7.2112,  3.6225, 63.5970]],

        [[23.5252, 43.8064,  7.7951,  1.2949,  1.2876, 24.6699]],

        [[24.4877, 46.7879,  7.4552,  0.9310,  0.8116, 21.8960]],

        [[20.2286, 40.9201, 17.9842,  1.2602,  2.3933, 44.6967]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2048,  1.9851, 10.4597,  0.4804], device='cuda:0')
Estimated target variance: tensor([0.0395, 1.0017, 9.1116, 0.1323], device='cuda:0')
N: 270
Signal to noise ratio: tensor([23.4516, 66.4159, 68.3110, 39.0527], device='cuda:0')
Bound on condition number: tensor([ 148494.8836, 1190991.0382, 1259925.9149,  411781.7527],
       device='cuda:0')
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 8.391101564461365e-05, policy loss: 14.091400329680317
Experience 27, Iter 1, disc loss: 0.00013256202737497747, policy loss: 13.326074034150164
Experience 27, Iter 2, disc loss: 9.541752094439271e-05, policy loss: 14.108210023983686
Experience 27, Iter 3, disc loss: 6.397213316143621e-05, policy loss: 14.884677815171798
Experience 27, Iter 4, disc loss: 0.00010010484976789863, policy loss: 13.636768244898185
Experience 27, Iter 5, disc loss: 7.108182207076231e-05, policy loss: 14.05083419951855
Experience 27, Iter 6, disc loss: 7.813879255300934e-05, policy loss: 14.210590892255377
Experience 27, Iter 7, disc loss: 9.693321969882489e-05, policy loss: 14.509324946308238
Experience 27, Iter 8, disc loss: 6.029650930978229e-05, policy loss: 14.665234063781275
Experience 27, Iter 9, disc loss: 0.000135370241340824, policy loss: 14.234397867676206
Experience 27, Iter 10, disc loss: 7.61741759700268e-05, policy loss: 14.715202190560824
Experience 27, Iter 11, disc loss: 6.955521154359954e-05, policy loss: 14.43786735806108
Experience 27, Iter 12, disc loss: 7.968900605080451e-05, policy loss: 14.086705293299772
Experience 27, Iter 13, disc loss: 6.261473703727352e-05, policy loss: 14.992633870125923
Experience 27, Iter 14, disc loss: 7.586672241585708e-05, policy loss: 14.182227001778354
Experience 27, Iter 15, disc loss: 7.063320148498271e-05, policy loss: 14.412152265790935
Experience 27, Iter 16, disc loss: 5.939730706501201e-05, policy loss: 15.268189475498374
Experience 27, Iter 17, disc loss: 0.00010129808842159526, policy loss: 14.137898684153054
Experience 27, Iter 18, disc loss: 0.00013610802697390905, policy loss: 13.633412786095704
Experience 27, Iter 19, disc loss: 7.955041373658234e-05, policy loss: 14.38651010390741
Experience 27, Iter 20, disc loss: 8.576316298399522e-05, policy loss: 13.513033129910802
Experience 27, Iter 21, disc loss: 7.326344059960383e-05, policy loss: 14.40959864226237
Experience 27, Iter 22, disc loss: 6.0377061050973794e-05, policy loss: 14.705895027302079
Experience 27, Iter 23, disc loss: 8.656825912114539e-05, policy loss: 14.80187775397103
Experience 27, Iter 24, disc loss: 6.267821600266339e-05, policy loss: 14.52745614853872
Experience 27, Iter 25, disc loss: 0.00011333877372482655, policy loss: 14.29730494335114
Experience 27, Iter 26, disc loss: 7.968732866754836e-05, policy loss: 14.044219011159566
Experience 27, Iter 27, disc loss: 8.109136971380689e-05, policy loss: 13.840240482659745
Experience 27, Iter 28, disc loss: 9.41611467363779e-05, policy loss: 14.302909928092667
Experience 27, Iter 29, disc loss: 6.592991535124887e-05, policy loss: 14.213094739798796
Experience 27, Iter 30, disc loss: 0.0001296277674371429, policy loss: 13.131478848053002
Experience 27, Iter 31, disc loss: 7.437743127541594e-05, policy loss: 14.682426532249885
Experience 27, Iter 32, disc loss: 7.775692813116304e-05, policy loss: 15.234128647795302
Experience 27, Iter 33, disc loss: 8.899782531245697e-05, policy loss: 14.694728725776411
Experience 27, Iter 34, disc loss: 0.0001048074042939717, policy loss: 13.855101436790314
Experience 27, Iter 35, disc loss: 0.00010888567511408063, policy loss: 14.59642502802707
Experience 27, Iter 36, disc loss: 7.563779485413028e-05, policy loss: 13.961370161584837
Experience 27, Iter 37, disc loss: 8.108096004164283e-05, policy loss: 13.923232699148514
Experience 27, Iter 38, disc loss: 0.00011261092134830374, policy loss: 13.872015385452864
Experience 27, Iter 39, disc loss: 8.465821123261623e-05, policy loss: 13.967011297358379
Experience 27, Iter 40, disc loss: 0.00012961360349097664, policy loss: 13.50702341228017
Experience 27, Iter 41, disc loss: 0.0001600362858557524, policy loss: 13.251811063599716
Experience 27, Iter 42, disc loss: 7.873811202455149e-05, policy loss: 13.588107380709417
Experience 27, Iter 43, disc loss: 8.616998364014921e-05, policy loss: 14.009803789935162
Experience 27, Iter 44, disc loss: 7.766614584949299e-05, policy loss: 13.769569287766444
Experience 27, Iter 45, disc loss: 0.00016490327997771578, policy loss: 12.858327941032742
Experience 27, Iter 46, disc loss: 0.00013792227291923724, policy loss: 12.977598464238813
Experience 27, Iter 47, disc loss: 9.887211045126917e-05, policy loss: 13.479862704712243
Experience 27, Iter 48, disc loss: 0.0001129965822849676, policy loss: 14.569462835939191
Experience 27, Iter 49, disc loss: 0.00014634086021714747, policy loss: 12.434239305871035
Experience 27, Iter 50, disc loss: 9.497541969905905e-05, policy loss: 13.894068571560078
Experience 27, Iter 51, disc loss: 0.00013207352679861102, policy loss: 12.639877339035277
Experience 27, Iter 52, disc loss: 9.943706087298966e-05, policy loss: 12.390008026649907
Experience 27, Iter 53, disc loss: 0.00017104876686467954, policy loss: 12.65750288580567
Experience 27, Iter 54, disc loss: 8.757853157009115e-05, policy loss: 13.846478870887225
Experience 27, Iter 55, disc loss: 6.907545529476536e-05, policy loss: 14.256419221135108
Experience 27, Iter 56, disc loss: 0.00010798818933061081, policy loss: 13.473712516432904
Experience 27, Iter 57, disc loss: 0.00011216054782226177, policy loss: 13.844243640845768
Experience 27, Iter 58, disc loss: 0.00014486869407045027, policy loss: 14.18589158836502
Experience 27, Iter 59, disc loss: 0.000131349542652348, policy loss: 12.868413881000347
Experience 27, Iter 60, disc loss: 7.300338289563927e-05, policy loss: 14.478790772325766
Experience 27, Iter 61, disc loss: 6.969610349599089e-05, policy loss: 14.412094040440673
Experience 27, Iter 62, disc loss: 7.126165289437201e-05, policy loss: 14.160750470414367
Experience 27, Iter 63, disc loss: 0.00010803506106314036, policy loss: 13.401553620963444
Experience 27, Iter 64, disc loss: 7.085975491999874e-05, policy loss: 13.722415515540932
Experience 27, Iter 65, disc loss: 7.746949957124563e-05, policy loss: 13.75914967382904
Experience 27, Iter 66, disc loss: 8.500059002587002e-05, policy loss: 13.922443828507461
Experience 27, Iter 67, disc loss: 0.00013058765292165542, policy loss: 13.53274798901974
Experience 27, Iter 68, disc loss: 0.00010081984413745404, policy loss: 13.263339069465104
Experience 27, Iter 69, disc loss: 7.040217437469907e-05, policy loss: 13.349267100535194
Experience 27, Iter 70, disc loss: 7.943548195124686e-05, policy loss: 14.088460117061782
Experience 27, Iter 71, disc loss: 9.236405866728734e-05, policy loss: 13.25684607032358
Experience 27, Iter 72, disc loss: 9.574871861023739e-05, policy loss: 13.386330021687815
Experience 27, Iter 73, disc loss: 9.382921478875766e-05, policy loss: 12.435894214666336
Experience 27, Iter 74, disc loss: 8.346565950898365e-05, policy loss: 13.324452451070176
Experience 27, Iter 75, disc loss: 0.00011758288144819953, policy loss: 13.586309101437589
Experience 27, Iter 76, disc loss: 0.00013700976894554472, policy loss: 13.483217843776433
Experience 27, Iter 77, disc loss: 0.0001396474576184377, policy loss: 12.227406325266081
Experience 27, Iter 78, disc loss: 7.575480652204805e-05, policy loss: 14.245952214751236
Experience 27, Iter 79, disc loss: 9.516393104271512e-05, policy loss: 13.4901952962304
Experience 27, Iter 80, disc loss: 0.00010501988254644564, policy loss: 12.684849005558252
Experience 27, Iter 81, disc loss: 0.00010445997828660127, policy loss: 13.534550415087356
Experience 27, Iter 82, disc loss: 9.798208826004537e-05, policy loss: 13.310373340168484
Experience 27, Iter 83, disc loss: 9.306902391377383e-05, policy loss: 13.373230253604302
Experience 27, Iter 84, disc loss: 0.00011914204255461422, policy loss: 13.62454978770668
Experience 27, Iter 85, disc loss: 0.0001544007750665955, policy loss: 13.483958935689689
Experience 27, Iter 86, disc loss: 7.131043408874725e-05, policy loss: 13.733425205713083
Experience 27, Iter 87, disc loss: 0.00012653691132870787, policy loss: 13.744264449685648
Experience 27, Iter 88, disc loss: 0.0001021870833055489, policy loss: 13.971503100281371
Experience 27, Iter 89, disc loss: 0.0001220714828304669, policy loss: 12.692277878128312
Experience 27, Iter 90, disc loss: 0.00010841591315183819, policy loss: 13.948363895569992
Experience 27, Iter 91, disc loss: 8.528080710747299e-05, policy loss: 13.97454973971081
Experience 27, Iter 92, disc loss: 7.168788119894315e-05, policy loss: 13.508501906737553
Experience 27, Iter 93, disc loss: 0.00012237517679476042, policy loss: 13.752624860629346
Experience 27, Iter 94, disc loss: 0.000124901418236851, policy loss: 13.084362947869066
Experience 27, Iter 95, disc loss: 7.324693741152884e-05, policy loss: 13.549527178801185
Experience 27, Iter 96, disc loss: 0.0001020211656859058, policy loss: 13.849830464657494
Experience 27, Iter 97, disc loss: 7.836404116651876e-05, policy loss: 13.78076706956427
Experience 27, Iter 98, disc loss: 0.00011313256735775664, policy loss: 13.968875548901213
Experience 27, Iter 99, disc loss: 0.0001546449035135202, policy loss: 13.258159201400066
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0096],
        [0.2510],
        [2.2817],
        [0.0325]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0495, 0.3894, 1.5153, 0.0352, 0.0113, 6.7100]],

        [[0.0495, 0.3894, 1.5153, 0.0352, 0.0113, 6.7100]],

        [[0.0495, 0.3894, 1.5153, 0.0352, 0.0113, 6.7100]],

        [[0.0495, 0.3894, 1.5153, 0.0352, 0.0113, 6.7100]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0386, 1.0038, 9.1269, 0.1298], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0386, 1.0038, 9.1269, 0.1298], device='cuda:0')
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.027
Iter 2/2000 - Loss: 3.900
Iter 3/2000 - Loss: 3.841
Iter 4/2000 - Loss: 3.741
Iter 5/2000 - Loss: 3.644
Iter 6/2000 - Loss: 3.557
Iter 7/2000 - Loss: 3.452
Iter 8/2000 - Loss: 3.332
Iter 9/2000 - Loss: 3.203
Iter 10/2000 - Loss: 3.065
Iter 11/2000 - Loss: 2.913
Iter 12/2000 - Loss: 2.748
Iter 13/2000 - Loss: 2.570
Iter 14/2000 - Loss: 2.380
Iter 15/2000 - Loss: 2.178
Iter 16/2000 - Loss: 1.963
Iter 17/2000 - Loss: 1.736
Iter 18/2000 - Loss: 1.495
Iter 19/2000 - Loss: 1.243
Iter 20/2000 - Loss: 0.980
Iter 1981/2000 - Loss: -7.313
Iter 1982/2000 - Loss: -7.314
Iter 1983/2000 - Loss: -7.314
Iter 1984/2000 - Loss: -7.314
Iter 1985/2000 - Loss: -7.314
Iter 1986/2000 - Loss: -7.314
Iter 1987/2000 - Loss: -7.314
Iter 1988/2000 - Loss: -7.314
Iter 1989/2000 - Loss: -7.314
Iter 1990/2000 - Loss: -7.314
Iter 1991/2000 - Loss: -7.314
Iter 1992/2000 - Loss: -7.314
Iter 1993/2000 - Loss: -7.314
Iter 1994/2000 - Loss: -7.314
Iter 1995/2000 - Loss: -7.314
Iter 1996/2000 - Loss: -7.314
Iter 1997/2000 - Loss: -7.314
Iter 1998/2000 - Loss: -7.314
Iter 1999/2000 - Loss: -7.314
Iter 2000/2000 - Loss: -7.314
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.1723, 13.8007, 23.9315,  7.0681,  3.7584, 63.3028]],

        [[23.5492, 43.4792,  7.8333,  1.2756,  1.2863, 24.8054]],

        [[24.2697, 46.1197,  7.4569,  0.9339,  0.8185, 21.9644]],

        [[20.1062, 40.5373, 18.4409,  1.2480,  2.3592, 45.1278]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2025,  1.9670, 10.5786,  0.4902], device='cuda:0')
Estimated target variance: tensor([0.0386, 1.0038, 9.1269, 0.1298], device='cuda:0')
N: 280
Signal to noise ratio: tensor([23.5603, 65.9154, 69.1262, 39.6928], device='cuda:0')
Bound on condition number: tensor([ 155425.6262, 1216556.8018, 1337960.0418,  441146.4941],
       device='cuda:0')
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 7.085254557549536e-05, policy loss: 13.674302136326595
Experience 28, Iter 1, disc loss: 8.305851505957041e-05, policy loss: 13.870633980301053
Experience 28, Iter 2, disc loss: 7.68700689228716e-05, policy loss: 13.541287330894594
Experience 28, Iter 3, disc loss: 8.925138762175627e-05, policy loss: 13.200406561346295
Experience 28, Iter 4, disc loss: 9.998062928235057e-05, policy loss: 13.844596844580796
Experience 28, Iter 5, disc loss: 8.815859172435373e-05, policy loss: 13.585471378331833
Experience 28, Iter 6, disc loss: 7.285529010107107e-05, policy loss: 13.698536061896494
Experience 28, Iter 7, disc loss: 9.763397470073174e-05, policy loss: 14.0431735622665
Experience 28, Iter 8, disc loss: 0.0001075526162116754, policy loss: 13.44019801441538
Experience 28, Iter 9, disc loss: 9.447160495143939e-05, policy loss: 14.346205995618947
Experience 28, Iter 10, disc loss: 8.053467781578296e-05, policy loss: 14.319432575300382
Experience 28, Iter 11, disc loss: 0.00011920301029742545, policy loss: 13.247152471065762
Experience 28, Iter 12, disc loss: 8.820240035580448e-05, policy loss: 13.438195275988893
Experience 28, Iter 13, disc loss: 8.702519766381334e-05, policy loss: 14.321042084913548
Experience 28, Iter 14, disc loss: 0.00010584804883167603, policy loss: 13.67047504765323
Experience 28, Iter 15, disc loss: 9.10500120665894e-05, policy loss: 14.27266959919212
Experience 28, Iter 16, disc loss: 5.7521881557710416e-05, policy loss: 13.991456644252768
Experience 28, Iter 17, disc loss: 0.00010449583693413313, policy loss: 14.333422273990905
Experience 28, Iter 18, disc loss: 8.259819983585405e-05, policy loss: 14.438920135678693
Experience 28, Iter 19, disc loss: 7.039308088694599e-05, policy loss: 14.420260511017423
Experience 28, Iter 20, disc loss: 7.326010831989491e-05, policy loss: 14.781421048665159
Experience 28, Iter 21, disc loss: 0.00014776575143627757, policy loss: 13.837787335264228
Experience 28, Iter 22, disc loss: 9.0149525868523e-05, policy loss: 14.330089765637698
Experience 28, Iter 23, disc loss: 6.397937028451963e-05, policy loss: 14.103677573300601
Experience 28, Iter 24, disc loss: 7.53830744798753e-05, policy loss: 14.41705872180529
Experience 28, Iter 25, disc loss: 0.00014635027027773087, policy loss: 13.546325721361173
Experience 28, Iter 26, disc loss: 7.37949429834017e-05, policy loss: 14.504399813973473
Experience 28, Iter 27, disc loss: 0.0001259594967250192, policy loss: 14.023614035686846
Experience 28, Iter 28, disc loss: 5.9083420159486294e-05, policy loss: 14.318091790749271
Experience 28, Iter 29, disc loss: 7.457166136803747e-05, policy loss: 13.904134505836602
Experience 28, Iter 30, disc loss: 8.910208085854103e-05, policy loss: 13.706251581362586
Experience 28, Iter 31, disc loss: 0.00013863967758557903, policy loss: 13.544877798637376
Experience 28, Iter 32, disc loss: 0.00017039658890802533, policy loss: 13.318249774324434
Experience 28, Iter 33, disc loss: 9.372567820031064e-05, policy loss: 14.16682040868816
Experience 28, Iter 34, disc loss: 0.00010495136838687345, policy loss: 14.451194992007053
Experience 28, Iter 35, disc loss: 8.993741417864398e-05, policy loss: 14.891644412531832
Experience 28, Iter 36, disc loss: 8.531871445658515e-05, policy loss: 13.629633986476547
Experience 28, Iter 37, disc loss: 0.00010622863700675232, policy loss: 13.302770611863638
Experience 28, Iter 38, disc loss: 7.849776676814138e-05, policy loss: 13.293752779637234
Experience 28, Iter 39, disc loss: 8.705014400924591e-05, policy loss: 13.626644487211948
Experience 28, Iter 40, disc loss: 0.00018170057344808212, policy loss: 13.979953980071032
Experience 28, Iter 41, disc loss: 0.00018770025357050163, policy loss: 15.363062899317605
Experience 28, Iter 42, disc loss: 9.93162002444244e-05, policy loss: 14.616108422870191
Experience 28, Iter 43, disc loss: 0.00023496789111694292, policy loss: 14.685657386220985
Experience 28, Iter 44, disc loss: 0.0005408476794254259, policy loss: 13.67416832432534
Experience 28, Iter 45, disc loss: 0.0002568153383680963, policy loss: 14.238333829496083
Experience 28, Iter 46, disc loss: 0.000228152061876063, policy loss: 15.08092202643374
Experience 28, Iter 47, disc loss: 0.0004315738162240434, policy loss: 14.443540405037547
Experience 28, Iter 48, disc loss: 0.00017357580847103917, policy loss: 13.830269817506668
Experience 28, Iter 49, disc loss: 9.321559651337141e-05, policy loss: 14.365538679641784
Experience 28, Iter 50, disc loss: 0.0003715556308280805, policy loss: 14.290865097329117
Experience 28, Iter 51, disc loss: 0.00029387552427510003, policy loss: 15.046585777720654
Experience 28, Iter 52, disc loss: 0.00026094367943865163, policy loss: 13.79481415273769
Experience 28, Iter 53, disc loss: 0.00012693728751081158, policy loss: 14.217167155874147
Experience 28, Iter 54, disc loss: 0.00036557120943498127, policy loss: 13.33999413465146
Experience 28, Iter 55, disc loss: 0.0003960831266128098, policy loss: 13.074322655941184
Experience 28, Iter 56, disc loss: 0.00039824330783500646, policy loss: 12.84514838577463
Experience 28, Iter 57, disc loss: 0.0006621956114659814, policy loss: 13.914289407267152
Experience 28, Iter 58, disc loss: 0.0001718066321277329, policy loss: 13.82500150644744
Experience 28, Iter 59, disc loss: 0.0006173407594935992, policy loss: 12.985820698650937
Experience 28, Iter 60, disc loss: 0.0005783910987628009, policy loss: 12.61422921583304
Experience 28, Iter 61, disc loss: 0.000556753865588059, policy loss: 12.934257341247044
Experience 28, Iter 62, disc loss: 0.0004660932626958279, policy loss: 11.883409295524075
Experience 28, Iter 63, disc loss: 0.0008673961666002866, policy loss: 12.66390716169324
Experience 28, Iter 64, disc loss: 0.0007695882487266329, policy loss: 12.455053083060953
Experience 28, Iter 65, disc loss: 0.000224433949359162, policy loss: 13.94449099885805
Experience 28, Iter 66, disc loss: 0.00042572819751108507, policy loss: 13.236497352230453
Experience 28, Iter 67, disc loss: 0.00013314876035705093, policy loss: 12.990171804712714
Experience 28, Iter 68, disc loss: 0.00021339197982663118, policy loss: 12.879952211356633
Experience 28, Iter 69, disc loss: 0.00012251282576748816, policy loss: 12.79705224578095
Experience 28, Iter 70, disc loss: 0.00013774266301611748, policy loss: 12.732808580564685
Experience 28, Iter 71, disc loss: 0.00013757294935875792, policy loss: 12.207969336472363
Experience 28, Iter 72, disc loss: 9.873006254197439e-05, policy loss: 12.725768102959783
Experience 28, Iter 73, disc loss: 9.914475575295994e-05, policy loss: 12.659611152209749
Experience 28, Iter 74, disc loss: 8.252455764859928e-05, policy loss: 13.700618944159123
Experience 28, Iter 75, disc loss: 9.856083070508627e-05, policy loss: 12.648552899058314
Experience 28, Iter 76, disc loss: 0.00013892822624585752, policy loss: 12.858500249471515
Experience 28, Iter 77, disc loss: 9.38965761115158e-05, policy loss: 12.80343205884613
Experience 28, Iter 78, disc loss: 9.073019681440427e-05, policy loss: 13.156368962026283
Experience 28, Iter 79, disc loss: 8.269420133484893e-05, policy loss: 12.406166758261438
Experience 28, Iter 80, disc loss: 9.048752365121605e-05, policy loss: 12.99678521646101
Experience 28, Iter 81, disc loss: 0.0001400764358680101, policy loss: 13.65931056536272
Experience 28, Iter 82, disc loss: 0.00011068354674349677, policy loss: 13.568097387816904
Experience 28, Iter 83, disc loss: 9.523418322652452e-05, policy loss: 13.052693001101794
Experience 28, Iter 84, disc loss: 9.117448171288137e-05, policy loss: 12.278502368232271
Experience 28, Iter 85, disc loss: 0.00010149406571296612, policy loss: 12.338359318254032
Experience 28, Iter 86, disc loss: 0.00015496464501215683, policy loss: 12.644578187909676
Experience 28, Iter 87, disc loss: 0.00014135376145814484, policy loss: 12.879590885015103
Experience 28, Iter 88, disc loss: 0.00014456181128783298, policy loss: 11.807568608039787
Experience 28, Iter 89, disc loss: 0.00014625386769180572, policy loss: 12.919742223867143
Experience 28, Iter 90, disc loss: 0.00011000595971417952, policy loss: 11.814884402181136
Experience 28, Iter 91, disc loss: 0.00010787926470752794, policy loss: 12.523302277249286
Experience 28, Iter 92, disc loss: 0.0001427219847182268, policy loss: 12.929810686998309
Experience 28, Iter 93, disc loss: 0.00013179421187246027, policy loss: 12.244339256086999
Experience 28, Iter 94, disc loss: 0.0001586605429778764, policy loss: 12.028723480018222
Experience 28, Iter 95, disc loss: 0.00012545623711768, policy loss: 12.222613028621963
Experience 28, Iter 96, disc loss: 0.00013833502671501348, policy loss: 12.502443487766596
Experience 28, Iter 97, disc loss: 0.0002579032117013604, policy loss: 11.82529762352312
Experience 28, Iter 98, disc loss: 0.00019831456927950534, policy loss: 11.690635193595991
Experience 28, Iter 99, disc loss: 0.00011463264404918908, policy loss: 12.979178968891995
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0112],
        [0.2484],
        [2.2491],
        [0.0323]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0588, 0.4459, 1.5068, 0.0360, 0.0113, 6.7312]],

        [[0.0588, 0.4459, 1.5068, 0.0360, 0.0113, 6.7312]],

        [[0.0588, 0.4459, 1.5068, 0.0360, 0.0113, 6.7312]],

        [[0.0588, 0.4459, 1.5068, 0.0360, 0.0113, 6.7312]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0447, 0.9934, 8.9966, 0.1292], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0447, 0.9934, 8.9966, 0.1292], device='cuda:0')
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.027
Iter 2/2000 - Loss: 3.924
Iter 3/2000 - Loss: 3.831
Iter 4/2000 - Loss: 3.737
Iter 5/2000 - Loss: 3.649
Iter 6/2000 - Loss: 3.541
Iter 7/2000 - Loss: 3.426
Iter 8/2000 - Loss: 3.313
Iter 9/2000 - Loss: 3.185
Iter 10/2000 - Loss: 3.036
Iter 11/2000 - Loss: 2.874
Iter 12/2000 - Loss: 2.706
Iter 13/2000 - Loss: 2.528
Iter 14/2000 - Loss: 2.336
Iter 15/2000 - Loss: 2.128
Iter 16/2000 - Loss: 1.904
Iter 17/2000 - Loss: 1.666
Iter 18/2000 - Loss: 1.417
Iter 19/2000 - Loss: 1.158
Iter 20/2000 - Loss: 0.888
Iter 1981/2000 - Loss: -7.378
Iter 1982/2000 - Loss: -7.378
Iter 1983/2000 - Loss: -7.378
Iter 1984/2000 - Loss: -7.378
Iter 1985/2000 - Loss: -7.378
Iter 1986/2000 - Loss: -7.378
Iter 1987/2000 - Loss: -7.378
Iter 1988/2000 - Loss: -7.378
Iter 1989/2000 - Loss: -7.378
Iter 1990/2000 - Loss: -7.378
Iter 1991/2000 - Loss: -7.378
Iter 1992/2000 - Loss: -7.378
Iter 1993/2000 - Loss: -7.378
Iter 1994/2000 - Loss: -7.378
Iter 1995/2000 - Loss: -7.378
Iter 1996/2000 - Loss: -7.378
Iter 1997/2000 - Loss: -7.378
Iter 1998/2000 - Loss: -7.378
Iter 1999/2000 - Loss: -7.379
Iter 2000/2000 - Loss: -7.379
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.1492, 13.8435, 25.0350,  2.8567,  6.2790, 62.4949]],

        [[24.6024, 45.1368,  7.6874,  1.3193,  1.2229, 23.2844]],

        [[24.8775, 47.0088,  7.6384,  0.9072,  0.8466, 21.7095]],

        [[20.4837, 41.3476, 18.4109,  1.2471,  2.4097, 44.1184]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1885,  1.8159, 10.6449,  0.5000], device='cuda:0')
Estimated target variance: tensor([0.0447, 0.9934, 8.9966, 0.1292], device='cuda:0')
N: 290
Signal to noise ratio: tensor([23.1380, 64.2795, 69.7233, 40.2953], device='cuda:0')
Bound on condition number: tensor([ 155257.2494, 1198238.6710, 1409788.9610,  470877.8191],
       device='cuda:0')
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.00026821047197796264, policy loss: 12.43585624028977
Experience 29, Iter 1, disc loss: 0.000407891143115118, policy loss: 12.186547032135634
Experience 29, Iter 2, disc loss: 0.00011744612535107097, policy loss: 11.640790759943336
Experience 29, Iter 3, disc loss: 0.00020051287284052966, policy loss: 12.606591322718444
Experience 29, Iter 4, disc loss: 0.00024212071993672294, policy loss: 11.484062099980974
Experience 29, Iter 5, disc loss: 0.00020512261102937528, policy loss: 12.214490867859116
Experience 29, Iter 6, disc loss: 0.00018601112210602498, policy loss: 12.907304438944239
Experience 29, Iter 7, disc loss: 0.0001302764431928653, policy loss: 12.514663923081716
Experience 29, Iter 8, disc loss: 0.000109319750072406, policy loss: 13.065511049836086
Experience 29, Iter 9, disc loss: 0.0001095936544742053, policy loss: 12.98823127077025
Experience 29, Iter 10, disc loss: 0.00044813409435971784, policy loss: 12.080046445890364
Experience 29, Iter 11, disc loss: 0.0003098149077043529, policy loss: 12.943093006941542
Experience 29, Iter 12, disc loss: 0.0002656758173042888, policy loss: 11.990027209003042
Experience 29, Iter 13, disc loss: 0.00033337981599954617, policy loss: 12.40768994537806
Experience 29, Iter 14, disc loss: 0.00044146875933126934, policy loss: 12.387772546598544
Experience 29, Iter 15, disc loss: 0.0003744266490383476, policy loss: 12.093688878941334
Experience 29, Iter 16, disc loss: 0.0005444762280238448, policy loss: 12.018638740625477
Experience 29, Iter 17, disc loss: 0.0004377242984352152, policy loss: 11.190511990561252
Experience 29, Iter 18, disc loss: 0.0013048142464314875, policy loss: 10.924668867728693
Experience 29, Iter 19, disc loss: 0.0003722825675905404, policy loss: 11.961850175132883
Experience 29, Iter 20, disc loss: 0.001410457987538604, policy loss: 10.749520948735743
Experience 29, Iter 21, disc loss: 0.0006534279701722016, policy loss: 11.139913579032555
Experience 29, Iter 22, disc loss: 0.000961944406526778, policy loss: 10.668518917961212
Experience 29, Iter 23, disc loss: 0.0007709164503678374, policy loss: 10.637981322935492
Experience 29, Iter 24, disc loss: 0.0011567468559902229, policy loss: 10.03055624796439
Experience 29, Iter 25, disc loss: 0.001310146937396069, policy loss: 10.305954145852002
Experience 29, Iter 26, disc loss: 0.000683914155928916, policy loss: 10.287592399475239
Experience 29, Iter 27, disc loss: 0.0004679438611632503, policy loss: 10.046789031489707
Experience 29, Iter 28, disc loss: 0.0005623350971243665, policy loss: 10.165266795938571
Experience 29, Iter 29, disc loss: 0.0005421735701429845, policy loss: 9.954142806038906
Experience 29, Iter 30, disc loss: 0.0002535026154173979, policy loss: 10.220299000096182
Experience 29, Iter 31, disc loss: 0.00043810451655294476, policy loss: 9.885378156182249
Experience 29, Iter 32, disc loss: 0.0004787309770432542, policy loss: 9.90121016114487
Experience 29, Iter 33, disc loss: 0.0003659910424685677, policy loss: 10.029521243653361
Experience 29, Iter 34, disc loss: 0.0006440182892534295, policy loss: 10.38512657310023
Experience 29, Iter 35, disc loss: 0.00021622640798925173, policy loss: 10.581846460542721
Experience 29, Iter 36, disc loss: 0.0005200973403119767, policy loss: 10.011366378558945
Experience 29, Iter 37, disc loss: 0.00042688485934009966, policy loss: 10.170523037183832
Experience 29, Iter 38, disc loss: 0.00022574109402089839, policy loss: 10.314097813004913
Experience 29, Iter 39, disc loss: 0.0003791804460902647, policy loss: 10.01856831373767
Experience 29, Iter 40, disc loss: 0.0002871133243062264, policy loss: 9.993567619599748
Experience 29, Iter 41, disc loss: 0.0003093812344477736, policy loss: 9.778969683341941
Experience 29, Iter 42, disc loss: 0.0003889001725427178, policy loss: 9.928305870782088
Experience 29, Iter 43, disc loss: 0.000509301909148819, policy loss: 9.694670226534399
Experience 29, Iter 44, disc loss: 0.0011399443028516647, policy loss: 10.044040982719526
Experience 29, Iter 45, disc loss: 0.0006342955480145942, policy loss: 10.231060307397268
Experience 29, Iter 46, disc loss: 0.0004374913046942077, policy loss: 10.070797807892536
Experience 29, Iter 47, disc loss: 0.00023714853385855598, policy loss: 10.123954064340419
Experience 29, Iter 48, disc loss: 0.0005567283491720058, policy loss: 9.946971024080174
Experience 29, Iter 49, disc loss: 0.0003804284264060078, policy loss: 9.7391367414629
Experience 29, Iter 50, disc loss: 0.0002338776274066601, policy loss: 10.076328086486756
Experience 29, Iter 51, disc loss: 0.00024490536076060993, policy loss: 10.098825521567816
Experience 29, Iter 52, disc loss: 0.0003966891552154669, policy loss: 9.631600479688043
Experience 29, Iter 53, disc loss: 0.00020376279691450944, policy loss: 10.260900838733376
Experience 29, Iter 54, disc loss: 0.0002632460921388822, policy loss: 9.745461849544913
Experience 29, Iter 55, disc loss: 0.0003094903507141854, policy loss: 9.696436410642395
Experience 29, Iter 56, disc loss: 0.00040404468270755346, policy loss: 9.441612867131877
Experience 29, Iter 57, disc loss: 0.00021509941409763518, policy loss: 10.581912279795464
Experience 29, Iter 58, disc loss: 0.0002565679525038407, policy loss: 10.024288336952697
Experience 29, Iter 59, disc loss: 0.00038275289281908124, policy loss: 10.073769656437577
Experience 29, Iter 60, disc loss: 0.000278216706386741, policy loss: 10.22265156035758
Experience 29, Iter 61, disc loss: 0.0003504154774812128, policy loss: 9.800080850849458
Experience 29, Iter 62, disc loss: 0.000291793265990645, policy loss: 10.063783878047591
Experience 29, Iter 63, disc loss: 0.0006299604698179433, policy loss: 9.906267366685515
Experience 29, Iter 64, disc loss: 0.00029564826683790727, policy loss: 10.1004456600822
Experience 29, Iter 65, disc loss: 0.0002808722902427345, policy loss: 9.985418734030793
Experience 29, Iter 66, disc loss: 0.00022402206550791104, policy loss: 10.344704311497715
Experience 29, Iter 67, disc loss: 0.0004176290800239443, policy loss: 9.992213710251129
Experience 29, Iter 68, disc loss: 0.0002804412578428448, policy loss: 9.786125398492143
Experience 29, Iter 69, disc loss: 0.000216156318158981, policy loss: 10.234399387037822
Experience 29, Iter 70, disc loss: 0.0001829733708779821, policy loss: 10.064027945616989
Experience 29, Iter 71, disc loss: 0.0004704856703341084, policy loss: 9.860800120132723
Experience 29, Iter 72, disc loss: 0.00031613971240120005, policy loss: 9.970003494870674
Experience 29, Iter 73, disc loss: 0.00031930895457042273, policy loss: 10.271724477395338
Experience 29, Iter 74, disc loss: 0.00018834860846503532, policy loss: 10.174724523370237
Experience 29, Iter 75, disc loss: 0.0002434177066997358, policy loss: 9.954871073638284
Experience 29, Iter 76, disc loss: 0.00016193890140806628, policy loss: 10.228142803778798
Experience 29, Iter 77, disc loss: 0.00019076237814769678, policy loss: 10.396461591347377
Experience 29, Iter 78, disc loss: 0.0002239891653712066, policy loss: 9.81915700523583
Experience 29, Iter 79, disc loss: 0.00017458031606714636, policy loss: 9.951922831696116
Experience 29, Iter 80, disc loss: 0.0002195061052211863, policy loss: 10.064463996915798
Experience 29, Iter 81, disc loss: 0.0002128630903849531, policy loss: 10.406235186068272
Experience 29, Iter 82, disc loss: 0.00029754650745762604, policy loss: 9.985141882108843
Experience 29, Iter 83, disc loss: 0.00019083602064742013, policy loss: 10.20030333437612
Experience 29, Iter 84, disc loss: 0.00026498933350896413, policy loss: 10.204013713064858
Experience 29, Iter 85, disc loss: 0.00020576026212675128, policy loss: 10.175068447172778
Experience 29, Iter 86, disc loss: 0.00019238037787979725, policy loss: 10.267486685986755
Experience 29, Iter 87, disc loss: 0.00025637896651898017, policy loss: 9.96886426583616
Experience 29, Iter 88, disc loss: 0.0001942510029794902, policy loss: 9.975346742016118
Experience 29, Iter 89, disc loss: 0.00019785449374579132, policy loss: 10.108694868934169
Experience 29, Iter 90, disc loss: 0.0001626309766105807, policy loss: 10.524561814948497
Experience 29, Iter 91, disc loss: 0.00021703594632069818, policy loss: 9.927787618658833
Experience 29, Iter 92, disc loss: 0.00018813482031885826, policy loss: 10.219303534106754
Experience 29, Iter 93, disc loss: 0.00026225880563852597, policy loss: 10.18138729068883
Experience 29, Iter 94, disc loss: 0.00017285264629105705, policy loss: 10.248420440282484
Experience 29, Iter 95, disc loss: 0.00017848051447246212, policy loss: 10.36365301562175
Experience 29, Iter 96, disc loss: 0.00016167614932804627, policy loss: 10.160338880799152
Experience 29, Iter 97, disc loss: 0.00025991399016135614, policy loss: 10.155626835217436
Experience 29, Iter 98, disc loss: 0.00021209149702669337, policy loss: 10.216708846603217
Experience 29, Iter 99, disc loss: 0.00032382758747117127, policy loss: 10.05235388012635
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0120],
        [0.2438],
        [2.1934],
        [0.0317]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0647, 0.4774, 1.4772, 0.0364, 0.0110, 6.6725]],

        [[0.0647, 0.4774, 1.4772, 0.0364, 0.0110, 6.6725]],

        [[0.0647, 0.4774, 1.4772, 0.0364, 0.0110, 6.6725]],

        [[0.0647, 0.4774, 1.4772, 0.0364, 0.0110, 6.6725]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0481, 0.9751, 8.7736, 0.1268], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0481, 0.9751, 8.7736, 0.1268], device='cuda:0')
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.003
Iter 2/2000 - Loss: 3.914
Iter 3/2000 - Loss: 3.802
Iter 4/2000 - Loss: 3.713
Iter 5/2000 - Loss: 3.622
Iter 6/2000 - Loss: 3.504
Iter 7/2000 - Loss: 3.385
Iter 8/2000 - Loss: 3.271
Iter 9/2000 - Loss: 3.140
Iter 10/2000 - Loss: 2.985
Iter 11/2000 - Loss: 2.817
Iter 12/2000 - Loss: 2.642
Iter 13/2000 - Loss: 2.458
Iter 14/2000 - Loss: 2.260
Iter 15/2000 - Loss: 2.046
Iter 16/2000 - Loss: 1.816
Iter 17/2000 - Loss: 1.573
Iter 18/2000 - Loss: 1.320
Iter 19/2000 - Loss: 1.057
Iter 20/2000 - Loss: 0.784
Iter 1981/2000 - Loss: -7.464
Iter 1982/2000 - Loss: -7.464
Iter 1983/2000 - Loss: -7.464
Iter 1984/2000 - Loss: -7.464
Iter 1985/2000 - Loss: -7.464
Iter 1986/2000 - Loss: -7.464
Iter 1987/2000 - Loss: -7.464
Iter 1988/2000 - Loss: -7.464
Iter 1989/2000 - Loss: -7.464
Iter 1990/2000 - Loss: -7.464
Iter 1991/2000 - Loss: -7.464
Iter 1992/2000 - Loss: -7.465
Iter 1993/2000 - Loss: -7.465
Iter 1994/2000 - Loss: -7.465
Iter 1995/2000 - Loss: -7.465
Iter 1996/2000 - Loss: -7.465
Iter 1997/2000 - Loss: -7.465
Iter 1998/2000 - Loss: -7.465
Iter 1999/2000 - Loss: -7.465
Iter 2000/2000 - Loss: -7.465
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.3109, 14.0215, 25.1495,  2.6023,  7.1566, 62.2102]],

        [[25.0718, 45.5901,  7.7160,  1.3379,  1.2199, 23.4210]],

        [[25.5057, 47.2075,  7.6352,  0.8846,  0.8679, 21.9975]],

        [[20.7380, 41.5912, 18.5671,  1.2643,  2.4671, 43.4119]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1923,  1.8539, 10.7858,  0.5007], device='cuda:0')
Estimated target variance: tensor([0.0481, 0.9751, 8.7736, 0.1268], device='cuda:0')
N: 300
Signal to noise ratio: tensor([23.8075, 65.8686, 70.3561, 40.5432], device='cuda:0')
Bound on condition number: tensor([ 170040.0313, 1301602.0430, 1484995.5497,  493125.8136],
       device='cuda:0')
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.0002428327068845199, policy loss: 10.06994323398762
Experience 30, Iter 1, disc loss: 0.00017057687125281225, policy loss: 10.263279334786516
Experience 30, Iter 2, disc loss: 0.0001800660268573312, policy loss: 10.103885996707323
Experience 30, Iter 3, disc loss: 0.00017556194100644523, policy loss: 10.15821783162897
Experience 30, Iter 4, disc loss: 0.00016969227229586106, policy loss: 10.30913833336714
Experience 30, Iter 5, disc loss: 0.00021920760307410917, policy loss: 10.251716556634507
Experience 30, Iter 6, disc loss: 0.00020173634432001848, policy loss: 10.114866234958967
Experience 30, Iter 7, disc loss: 0.00015531648318041946, policy loss: 10.243009093985744
Experience 30, Iter 8, disc loss: 0.0002650615844301711, policy loss: 9.96922053435831
Experience 30, Iter 9, disc loss: 0.0006189274573835464, policy loss: 9.906730778601151
Experience 30, Iter 10, disc loss: 0.00023671159836559044, policy loss: 10.0003896615343
Experience 30, Iter 11, disc loss: 0.00022068357012485287, policy loss: 10.167703348564936
Experience 30, Iter 12, disc loss: 0.0002525050185276965, policy loss: 9.74114618808563
Experience 30, Iter 13, disc loss: 0.000615283835035563, policy loss: 10.29597116565849
Experience 30, Iter 14, disc loss: 0.0002130990125407415, policy loss: 10.071180845474277
Experience 30, Iter 15, disc loss: 0.00019938688810714787, policy loss: 9.989300699430634
Experience 30, Iter 16, disc loss: 0.00014878297695284783, policy loss: 10.390754274110723
Experience 30, Iter 17, disc loss: 0.00019509091466609387, policy loss: 10.137694564799883
Experience 30, Iter 18, disc loss: 0.00022905414468511666, policy loss: 10.039497872620187
Experience 30, Iter 19, disc loss: 0.0002994018026946704, policy loss: 10.03711112240114
Experience 30, Iter 20, disc loss: 0.00019210752413434222, policy loss: 10.024239474162329
Experience 30, Iter 21, disc loss: 0.0001713625195498473, policy loss: 10.104174862617867
Experience 30, Iter 22, disc loss: 0.0001695618770754415, policy loss: 10.638378454420861
Experience 30, Iter 23, disc loss: 0.00016530883301245445, policy loss: 10.287833326908789
Experience 30, Iter 24, disc loss: 0.00017816524844531867, policy loss: 9.895697052252522
Experience 30, Iter 25, disc loss: 0.00019697116725055758, policy loss: 10.075492045930766
Experience 30, Iter 26, disc loss: 0.00016009506574372857, policy loss: 10.255116700993907
Experience 30, Iter 27, disc loss: 0.0002823416632217555, policy loss: 10.094876969678989
Experience 30, Iter 28, disc loss: 0.00019859289366430156, policy loss: 10.277760031083158
Experience 30, Iter 29, disc loss: 0.00015382229942510707, policy loss: 10.17675308562559
Experience 30, Iter 30, disc loss: 0.00038557741685873125, policy loss: 10.053098783424478
Experience 30, Iter 31, disc loss: 0.0001533712768361306, policy loss: 10.463816281334587
Experience 30, Iter 32, disc loss: 0.0001895115130283961, policy loss: 10.261204357307346
Experience 30, Iter 33, disc loss: 0.0002000315651427132, policy loss: 9.973128215494533
Experience 30, Iter 34, disc loss: 0.00022505003693577323, policy loss: 10.084403705766025
Experience 30, Iter 35, disc loss: 0.00014423781856737759, policy loss: 10.553238581049875
Experience 30, Iter 36, disc loss: 0.0001659960671616814, policy loss: 10.21734207721313
Experience 30, Iter 37, disc loss: 0.00017511668084706374, policy loss: 10.291606683469709
Experience 30, Iter 38, disc loss: 0.0001586052541835001, policy loss: 10.202413854266396
Experience 30, Iter 39, disc loss: 0.00019681971855337965, policy loss: 10.204930807611056
Experience 30, Iter 40, disc loss: 0.00015658365920188764, policy loss: 10.492572210897748
Experience 30, Iter 41, disc loss: 0.00017332489425065875, policy loss: 10.12487096561359
Experience 30, Iter 42, disc loss: 0.00016147972002256307, policy loss: 10.443509370197631
Experience 30, Iter 43, disc loss: 0.00016655438170543515, policy loss: 10.264431937376244
Experience 30, Iter 44, disc loss: 0.00018802243740021306, policy loss: 10.170945858928313
Experience 30, Iter 45, disc loss: 0.00016473500258624952, policy loss: 10.741248614564457
Experience 30, Iter 46, disc loss: 0.0001450222372428251, policy loss: 10.75325645937736
Experience 30, Iter 47, disc loss: 0.00020171565719495052, policy loss: 10.356915680759862
Experience 30, Iter 48, disc loss: 0.00025598874572011567, policy loss: 10.646136852562552
Experience 30, Iter 49, disc loss: 0.00013530593895004736, policy loss: 10.607093951117607
Experience 30, Iter 50, disc loss: 0.00019982862637125715, policy loss: 10.219062476514768
Experience 30, Iter 51, disc loss: 0.00014239985319024743, policy loss: 10.463776160073081
Experience 30, Iter 52, disc loss: 0.0001765805282750886, policy loss: 10.173679106966402
Experience 30, Iter 53, disc loss: 0.0001466081873546807, policy loss: 10.477274235690274
Experience 30, Iter 54, disc loss: 0.0001401161450543277, policy loss: 10.286886552118956
Experience 30, Iter 55, disc loss: 0.00014481349511063016, policy loss: 10.60410337095458
Experience 30, Iter 56, disc loss: 0.00016442093501948954, policy loss: 10.294910054041104
Experience 30, Iter 57, disc loss: 0.00013998582230863082, policy loss: 10.366636459760468
Experience 30, Iter 58, disc loss: 0.00016373324897988792, policy loss: 10.573565776797182
Experience 30, Iter 59, disc loss: 0.00015100341644890946, policy loss: 10.426673247086086
Experience 30, Iter 60, disc loss: 0.00016397218567332315, policy loss: 10.417167657017298
Experience 30, Iter 61, disc loss: 0.00017178559674299688, policy loss: 10.351824584539651
Experience 30, Iter 62, disc loss: 0.0001472312376494586, policy loss: 10.396852754491013
Experience 30, Iter 63, disc loss: 0.00016253659855482928, policy loss: 10.381135373581195
Experience 30, Iter 64, disc loss: 0.0001244782505865795, policy loss: 10.65455766220961
Experience 30, Iter 65, disc loss: 0.00014677801391133592, policy loss: 10.504461617580528
Experience 30, Iter 66, disc loss: 0.00013887793697587233, policy loss: 10.740460522493738
Experience 30, Iter 67, disc loss: 0.00012567184663741252, policy loss: 10.722475159480505
Experience 30, Iter 68, disc loss: 0.00011398157518127947, policy loss: 10.777016046386759
Experience 30, Iter 69, disc loss: 0.00015892915246187546, policy loss: 10.524894880253846
Experience 30, Iter 70, disc loss: 0.00012230354187326066, policy loss: 10.791617980328235
Experience 30, Iter 71, disc loss: 0.00016227648908898604, policy loss: 10.302258703816875
Experience 30, Iter 72, disc loss: 0.00016160471577482118, policy loss: 10.46407584014616
Experience 30, Iter 73, disc loss: 0.000152509632111743, policy loss: 10.485444562929601
Experience 30, Iter 74, disc loss: 0.0001269009597184113, policy loss: 10.618655328922202
Experience 30, Iter 75, disc loss: 0.00013934713218530085, policy loss: 10.447352245044623
Experience 30, Iter 76, disc loss: 0.0001663916770143553, policy loss: 10.658518857306312
Experience 30, Iter 77, disc loss: 0.00015446364911091034, policy loss: 10.338805957751351
Experience 30, Iter 78, disc loss: 0.00013335755334343722, policy loss: 10.515832894218747
Experience 30, Iter 79, disc loss: 0.00014521871092296594, policy loss: 10.302371270264572
Experience 30, Iter 80, disc loss: 0.00016593354807008473, policy loss: 10.805331035876435
Experience 30, Iter 81, disc loss: 0.00015507979942770296, policy loss: 10.279991727192254
Experience 30, Iter 82, disc loss: 0.00015753109790261124, policy loss: 10.489452726571868
Experience 30, Iter 83, disc loss: 0.00018458333606312345, policy loss: 10.393609414472989
Experience 30, Iter 84, disc loss: 0.00012477187587891413, policy loss: 10.554402089757058
Experience 30, Iter 85, disc loss: 0.00016852471237020127, policy loss: 9.967770067034113
Experience 30, Iter 86, disc loss: 0.00011043822231184637, policy loss: 10.974257120945193
Experience 30, Iter 87, disc loss: 0.0001337993373324774, policy loss: 10.58170867642754
Experience 30, Iter 88, disc loss: 0.00014612283085364995, policy loss: 10.47154100109498
Experience 30, Iter 89, disc loss: 0.00010077335565816188, policy loss: 10.963434044793239
Experience 30, Iter 90, disc loss: 0.00011148382481608485, policy loss: 10.891218597311628
Experience 30, Iter 91, disc loss: 0.00015575202178757893, policy loss: 10.417841094803695
Experience 30, Iter 92, disc loss: 0.00021525159178553422, policy loss: 10.318624973156744
Experience 30, Iter 93, disc loss: 0.0001477849659255569, policy loss: 10.512258673660387
Experience 30, Iter 94, disc loss: 0.00015913010687202672, policy loss: 10.53520601584433
Experience 30, Iter 95, disc loss: 0.00016070559098507682, policy loss: 10.737707650849686
Experience 30, Iter 96, disc loss: 0.00011696283550564519, policy loss: 10.450784920311966
Experience 30, Iter 97, disc loss: 0.0001147396140811734, policy loss: 10.880200966767827
Experience 30, Iter 98, disc loss: 0.00013087944825796035, policy loss: 10.632014040854532
Experience 30, Iter 99, disc loss: 0.00011179886467584013, policy loss: 10.747181685058266
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0128],
        [0.2395],
        [2.1361],
        [0.0310]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0693, 0.5065, 1.4449, 0.0367, 0.0108, 6.6219]],

        [[0.0693, 0.5065, 1.4449, 0.0367, 0.0108, 6.6219]],

        [[0.0693, 0.5065, 1.4449, 0.0367, 0.0108, 6.6219]],

        [[0.0693, 0.5065, 1.4449, 0.0367, 0.0108, 6.6219]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0513, 0.9580, 8.5442, 0.1240], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0513, 0.9580, 8.5442, 0.1240], device='cuda:0')
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.972
Iter 2/2000 - Loss: 3.889
Iter 3/2000 - Loss: 3.764
Iter 4/2000 - Loss: 3.673
Iter 5/2000 - Loss: 3.575
Iter 6/2000 - Loss: 3.447
Iter 7/2000 - Loss: 3.321
Iter 8/2000 - Loss: 3.199
Iter 9/2000 - Loss: 3.062
Iter 10/2000 - Loss: 2.902
Iter 11/2000 - Loss: 2.727
Iter 12/2000 - Loss: 2.545
Iter 13/2000 - Loss: 2.354
Iter 14/2000 - Loss: 2.151
Iter 15/2000 - Loss: 1.935
Iter 16/2000 - Loss: 1.703
Iter 17/2000 - Loss: 1.458
Iter 18/2000 - Loss: 1.202
Iter 19/2000 - Loss: 0.937
Iter 20/2000 - Loss: 0.663
Iter 1981/2000 - Loss: -7.525
Iter 1982/2000 - Loss: -7.525
Iter 1983/2000 - Loss: -7.525
Iter 1984/2000 - Loss: -7.525
Iter 1985/2000 - Loss: -7.525
Iter 1986/2000 - Loss: -7.525
Iter 1987/2000 - Loss: -7.525
Iter 1988/2000 - Loss: -7.525
Iter 1989/2000 - Loss: -7.525
Iter 1990/2000 - Loss: -7.525
Iter 1991/2000 - Loss: -7.525
Iter 1992/2000 - Loss: -7.525
Iter 1993/2000 - Loss: -7.525
Iter 1994/2000 - Loss: -7.525
Iter 1995/2000 - Loss: -7.525
Iter 1996/2000 - Loss: -7.525
Iter 1997/2000 - Loss: -7.525
Iter 1998/2000 - Loss: -7.526
Iter 1999/2000 - Loss: -7.526
Iter 2000/2000 - Loss: -7.526
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.5444, 14.1430, 24.9520,  2.5536,  7.5917, 62.3971]],

        [[25.3041, 46.1476,  7.5995,  1.3759,  1.2212, 23.1922]],

        [[26.0277, 48.1701,  7.6594,  0.8870,  0.8663, 21.8669]],

        [[20.7883, 42.0482, 18.6304,  1.2516,  2.4055, 44.4921]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1950,  1.8504, 10.6682,  0.5049], device='cuda:0')
Estimated target variance: tensor([0.0513, 0.9580, 8.5442, 0.1240], device='cuda:0')
N: 310
Signal to noise ratio: tensor([23.8001, 66.0440, 70.1483, 41.1712], device='cuda:0')
Bound on condition number: tensor([ 175599.5741, 1352161.4260, 1525443.5700,  525473.2654],
       device='cuda:0')
Policy Optimizer learning rate:
0.009688861611972634
Experience 31, Iter 0, disc loss: 0.00013046100637833922, policy loss: 10.680909678899056
Experience 31, Iter 1, disc loss: 0.00018645184634099034, policy loss: 10.027811730438206
Experience 31, Iter 2, disc loss: 0.00013258010989071847, policy loss: 10.831732369224342
Experience 31, Iter 3, disc loss: 0.00017967512894870042, policy loss: 10.463843538066412
Experience 31, Iter 4, disc loss: 0.00018012945878277025, policy loss: 10.031605006292452
Experience 31, Iter 5, disc loss: 0.00011183924798968369, policy loss: 10.689161423731425
Experience 31, Iter 6, disc loss: 0.00013406899189031662, policy loss: 10.397863412752892
Experience 31, Iter 7, disc loss: 0.00013456189547770536, policy loss: 10.555433879460761
Experience 31, Iter 8, disc loss: 0.0001405182205470251, policy loss: 10.65002168694184
Experience 31, Iter 9, disc loss: 0.00013185187953735153, policy loss: 10.461047073572145
Experience 31, Iter 10, disc loss: 0.00015118755502888494, policy loss: 10.392768216537856
Experience 31, Iter 11, disc loss: 0.00017793578692244688, policy loss: 10.51030592501041
Experience 31, Iter 12, disc loss: 0.00015456482614060682, policy loss: 10.493611228089627
Experience 31, Iter 13, disc loss: 0.00012722521716628172, policy loss: 10.906871272973106
Experience 31, Iter 14, disc loss: 0.00020447230318447837, policy loss: 10.435881695481449
Experience 31, Iter 15, disc loss: 0.00013513230487560704, policy loss: 10.473208642184051
Experience 31, Iter 16, disc loss: 0.00012804680890363728, policy loss: 10.730188337075766
Experience 31, Iter 17, disc loss: 0.00011399370996042453, policy loss: 10.815354411911876
Experience 31, Iter 18, disc loss: 0.0001331130693811581, policy loss: 10.679589655814945
Experience 31, Iter 19, disc loss: 0.0001715351647432113, policy loss: 10.423597576421287
Experience 31, Iter 20, disc loss: 0.00017635237645873648, policy loss: 10.469699902763486
Experience 31, Iter 21, disc loss: 0.00012135899562070528, policy loss: 10.741463252120381
Experience 31, Iter 22, disc loss: 0.00010239316226335666, policy loss: 10.938651393614604
Experience 31, Iter 23, disc loss: 0.00010629183950523714, policy loss: 11.006450294346504
Experience 31, Iter 24, disc loss: 0.00015707133099907396, policy loss: 10.71279942854599
Experience 31, Iter 25, disc loss: 0.0002519104250370918, policy loss: 10.402146122089546
Experience 31, Iter 26, disc loss: 0.0001551439933921756, policy loss: 10.95224518489838
Experience 31, Iter 27, disc loss: 0.00014303713753150083, policy loss: 10.659730876322149
Experience 31, Iter 28, disc loss: 0.00011568358878991661, policy loss: 10.679699967919454
Experience 31, Iter 29, disc loss: 0.00015539711527996205, policy loss: 10.58378614910253
Experience 31, Iter 30, disc loss: 0.00013155451752165114, policy loss: 10.936217327925942
Experience 31, Iter 31, disc loss: 0.00016800863059183665, policy loss: 10.427849991044884
Experience 31, Iter 32, disc loss: 0.00015568132849006695, policy loss: 10.571063124797288
Experience 31, Iter 33, disc loss: 0.0002067167551253696, policy loss: 10.208881595477033
Experience 31, Iter 34, disc loss: 0.0001411356012293373, policy loss: 10.515149516833455
Experience 31, Iter 35, disc loss: 0.00018313745470146514, policy loss: 10.247429315765313
Experience 31, Iter 36, disc loss: 0.00013821001726808284, policy loss: 10.545629291782932
Experience 31, Iter 37, disc loss: 0.00017212571951089192, policy loss: 10.697971312121688
Experience 31, Iter 38, disc loss: 0.0001658029419082922, policy loss: 10.592023646468089
Experience 31, Iter 39, disc loss: 0.00011643904788773954, policy loss: 10.646884286102747
Experience 31, Iter 40, disc loss: 0.00014077198457133766, policy loss: 10.540610743942892
Experience 31, Iter 41, disc loss: 0.00012902756235242345, policy loss: 10.644169859641767
Experience 31, Iter 42, disc loss: 0.00011551181540009385, policy loss: 10.703672597853675
Experience 31, Iter 43, disc loss: 0.0001330521087534982, policy loss: 10.441872486959925
Experience 31, Iter 44, disc loss: 0.00011255777958255227, policy loss: 10.522114634711368
Experience 31, Iter 45, disc loss: 0.00016792114854121196, policy loss: 10.204561761836505
Experience 31, Iter 46, disc loss: 0.00014392535027582548, policy loss: 10.573085429890567
Experience 31, Iter 47, disc loss: 0.0001532893184805697, policy loss: 10.436803410791278
Experience 31, Iter 48, disc loss: 0.00013122039947381073, policy loss: 10.876039462684552
Experience 31, Iter 49, disc loss: 0.0002065773509199081, policy loss: 10.512591558191115
Experience 31, Iter 50, disc loss: 0.00014134401277024732, policy loss: 10.567937079958948
Experience 31, Iter 51, disc loss: 0.00013697050196400127, policy loss: 10.781083968213636
Experience 31, Iter 52, disc loss: 0.00025136914650886167, policy loss: 10.326487357917989
Experience 31, Iter 53, disc loss: 0.00013394453482392304, policy loss: 10.248045640771899
Experience 31, Iter 54, disc loss: 0.00020016849618629816, policy loss: 10.323033838940667
Experience 31, Iter 55, disc loss: 0.0001356139804814915, policy loss: 10.49461708932628
Experience 31, Iter 56, disc loss: 0.00014103143386458408, policy loss: 10.756268769243194
Experience 31, Iter 57, disc loss: 0.00016648522233511358, policy loss: 10.86435408714069
Experience 31, Iter 58, disc loss: 0.00014634769977946963, policy loss: 10.399422987691365
Experience 31, Iter 59, disc loss: 0.00012412151868261946, policy loss: 10.567993630271907
Experience 31, Iter 60, disc loss: 0.00014838750581017585, policy loss: 10.535325804005259
Experience 31, Iter 61, disc loss: 0.00015478540150748083, policy loss: 10.438429180714554
Experience 31, Iter 62, disc loss: 0.00013651964667082988, policy loss: 10.615501423814433
Experience 31, Iter 63, disc loss: 0.00015647185545388984, policy loss: 10.361649535564423
Experience 31, Iter 64, disc loss: 0.0001398668777511102, policy loss: 10.53065755834579
Experience 31, Iter 65, disc loss: 0.00010396324500069304, policy loss: 10.994017754542783
Experience 31, Iter 66, disc loss: 0.000195626492393299, policy loss: 10.389233723262253
Experience 31, Iter 67, disc loss: 0.0001324119147132208, policy loss: 10.528838854025931
Experience 31, Iter 68, disc loss: 0.00013040956039614646, policy loss: 10.441871047109625
Experience 31, Iter 69, disc loss: 0.00011863844571698929, policy loss: 10.538711422634883
Experience 31, Iter 70, disc loss: 0.00014315086037684587, policy loss: 10.541543276930739
Experience 31, Iter 71, disc loss: 0.00015759859105174927, policy loss: 10.849674877149976
Experience 31, Iter 72, disc loss: 0.00012307936594343062, policy loss: 10.579213606304235
Experience 31, Iter 73, disc loss: 0.00013892297721203203, policy loss: 10.273335040651569
Experience 31, Iter 74, disc loss: 0.00010483389349282182, policy loss: 11.086526104397986
Experience 31, Iter 75, disc loss: 0.00017175109355457624, policy loss: 10.316670080373932
Experience 31, Iter 76, disc loss: 0.0001465951595157651, policy loss: 10.674055070212715
Experience 31, Iter 77, disc loss: 0.00012830353697953562, policy loss: 10.69021880106763
Experience 31, Iter 78, disc loss: 0.0001127051530474483, policy loss: 10.643967075313832
Experience 31, Iter 79, disc loss: 0.00013689406734406707, policy loss: 10.553424244817876
Experience 31, Iter 80, disc loss: 0.00013467842325368498, policy loss: 10.569154082523937
Experience 31, Iter 81, disc loss: 0.00012558863250452178, policy loss: 10.367925265430351
Experience 31, Iter 82, disc loss: 0.00012677701573735097, policy loss: 10.3061352383342
Experience 31, Iter 83, disc loss: 0.00016765854720812243, policy loss: 10.1638129923091
Experience 31, Iter 84, disc loss: 0.00010085462569747434, policy loss: 10.825280549424743
Experience 31, Iter 85, disc loss: 0.0001217132703712697, policy loss: 10.868004947751906
Experience 31, Iter 86, disc loss: 0.0001459788199637358, policy loss: 10.57592289848606
Experience 31, Iter 87, disc loss: 0.00011554733521928166, policy loss: 10.855875406171744
Experience 31, Iter 88, disc loss: 0.00012225903057989953, policy loss: 10.86850849709316
Experience 31, Iter 89, disc loss: 0.00017496925932368356, policy loss: 10.101347625712028
Experience 31, Iter 90, disc loss: 0.00012635835516380323, policy loss: 10.599340062402186
Experience 31, Iter 91, disc loss: 0.0001320171934660281, policy loss: 10.558032061128829
Experience 31, Iter 92, disc loss: 0.00025276728150404494, policy loss: 10.457268704696983
Experience 31, Iter 93, disc loss: 0.00022999447424658321, policy loss: 10.384236017930924
Experience 31, Iter 94, disc loss: 0.00011336889291327858, policy loss: 10.776511759819423
Experience 31, Iter 95, disc loss: 0.00010882108149248623, policy loss: 10.776719369385749
Experience 31, Iter 96, disc loss: 0.00011337442581425667, policy loss: 10.711277617230238
Experience 31, Iter 97, disc loss: 0.0001741466602254331, policy loss: 10.350158954431201
Experience 31, Iter 98, disc loss: 0.00012551192687912572, policy loss: 10.95632758536016
Experience 31, Iter 99, disc loss: 0.00014080836137316164, policy loss: 10.392319293387295
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0139],
        [0.2366],
        [2.0998],
        [0.0307]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0766, 0.5452, 1.4309, 0.0371, 0.0107, 6.6200]],

        [[0.0766, 0.5452, 1.4309, 0.0371, 0.0107, 6.6200]],

        [[0.0766, 0.5452, 1.4309, 0.0371, 0.0107, 6.6200]],

        [[0.0766, 0.5452, 1.4309, 0.0371, 0.0107, 6.6200]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0556, 0.9465, 8.3993, 0.1229], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0556, 0.9465, 8.3993, 0.1229], device='cuda:0')
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.969
Iter 2/2000 - Loss: 3.898
Iter 3/2000 - Loss: 3.757
Iter 4/2000 - Loss: 3.666
Iter 5/2000 - Loss: 3.570
Iter 6/2000 - Loss: 3.438
Iter 7/2000 - Loss: 3.301
Iter 8/2000 - Loss: 3.168
Iter 9/2000 - Loss: 3.026
Iter 10/2000 - Loss: 2.864
Iter 11/2000 - Loss: 2.683
Iter 12/2000 - Loss: 2.489
Iter 13/2000 - Loss: 2.285
Iter 14/2000 - Loss: 2.071
Iter 15/2000 - Loss: 1.846
Iter 16/2000 - Loss: 1.610
Iter 17/2000 - Loss: 1.361
Iter 18/2000 - Loss: 1.100
Iter 19/2000 - Loss: 0.829
Iter 20/2000 - Loss: 0.549
Iter 1981/2000 - Loss: -7.580
Iter 1982/2000 - Loss: -7.580
Iter 1983/2000 - Loss: -7.580
Iter 1984/2000 - Loss: -7.580
Iter 1985/2000 - Loss: -7.580
Iter 1986/2000 - Loss: -7.580
Iter 1987/2000 - Loss: -7.581
Iter 1988/2000 - Loss: -7.581
Iter 1989/2000 - Loss: -7.581
Iter 1990/2000 - Loss: -7.581
Iter 1991/2000 - Loss: -7.581
Iter 1992/2000 - Loss: -7.581
Iter 1993/2000 - Loss: -7.581
Iter 1994/2000 - Loss: -7.581
Iter 1995/2000 - Loss: -7.581
Iter 1996/2000 - Loss: -7.581
Iter 1997/2000 - Loss: -7.581
Iter 1998/2000 - Loss: -7.581
Iter 1999/2000 - Loss: -7.581
Iter 2000/2000 - Loss: -7.581
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.9375, 14.8720, 24.5400,  3.2784,  5.8428, 61.9193]],

        [[25.4235, 46.4282,  7.5887,  1.3535,  1.2274, 23.1994]],

        [[26.1709, 48.6798,  7.6126,  0.8928,  0.8576, 21.7312]],

        [[20.8389, 42.1950, 18.5362,  1.2479,  2.3178, 45.0575]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2107,  1.8422, 10.5336,  0.4982], device='cuda:0')
Estimated target variance: tensor([0.0556, 0.9465, 8.3993, 0.1229], device='cuda:0')
N: 320
Signal to noise ratio: tensor([24.4613, 66.8773, 70.6671, 40.8398], device='cuda:0')
Bound on condition number: tensor([ 191473.9997, 1431225.8679, 1598030.9828,  533725.9050],
       device='cuda:0')
Policy Optimizer learning rate:
0.009678658753253001
Experience 32, Iter 0, disc loss: 0.00010501582005709232, policy loss: 10.785056055061514
Experience 32, Iter 1, disc loss: 0.00012121806323746562, policy loss: 10.51152591854694
Experience 32, Iter 2, disc loss: 0.00013511682592383604, policy loss: 10.651952407292724
Experience 32, Iter 3, disc loss: 0.00011297361259125781, policy loss: 10.64613370535788
Experience 32, Iter 4, disc loss: 0.00013340579769567014, policy loss: 10.59384337484208
Experience 32, Iter 5, disc loss: 0.00011565162189076178, policy loss: 10.731950914831865
Experience 32, Iter 6, disc loss: 0.00010383864653987057, policy loss: 10.722452329977594
Experience 32, Iter 7, disc loss: 0.00012210055110601395, policy loss: 10.647982050421945
Experience 32, Iter 8, disc loss: 9.633188122170431e-05, policy loss: 10.86772277310126
Experience 32, Iter 9, disc loss: 0.00010578960174604587, policy loss: 10.726999260777394
Experience 32, Iter 10, disc loss: 0.00011882936348540062, policy loss: 10.508522042743213
Experience 32, Iter 11, disc loss: 0.00014652676899935433, policy loss: 10.209557161601474
Experience 32, Iter 12, disc loss: 0.00012278990487936593, policy loss: 10.579034625127388
Experience 32, Iter 13, disc loss: 0.00014364957808631453, policy loss: 10.575231186564473
Experience 32, Iter 14, disc loss: 0.00012174428352438284, policy loss: 10.833534105357188
Experience 32, Iter 15, disc loss: 0.0001454556598403708, policy loss: 10.63352199852218
Experience 32, Iter 16, disc loss: 0.00013520430597058252, policy loss: 10.742243187000852
Experience 32, Iter 17, disc loss: 8.602218526456825e-05, policy loss: 10.977596834972852
Experience 32, Iter 18, disc loss: 8.641384152143647e-05, policy loss: 10.799305987934385
Experience 32, Iter 19, disc loss: 0.00010980093080753622, policy loss: 11.113068830672992
Experience 32, Iter 20, disc loss: 9.753594646945186e-05, policy loss: 10.866778217324294
Experience 32, Iter 21, disc loss: 0.00014684262597267113, policy loss: 10.334580166537261
Experience 32, Iter 22, disc loss: 0.00012486450617453371, policy loss: 10.748245858883056
Experience 32, Iter 23, disc loss: 0.00012736852052158573, policy loss: 10.481259665680545
Experience 32, Iter 24, disc loss: 0.00018472592653725632, policy loss: 9.961074846063353
Experience 32, Iter 25, disc loss: 0.0001136019833011009, policy loss: 10.969283764482137
Experience 32, Iter 26, disc loss: 0.00011579600197656666, policy loss: 11.108447925105562
Experience 32, Iter 27, disc loss: 0.00012396186897152355, policy loss: 10.406452175097968
Experience 32, Iter 28, disc loss: 0.00018235338513910128, policy loss: 10.199214242247756
Experience 32, Iter 29, disc loss: 0.00013440023911584807, policy loss: 10.588438926211381
Experience 32, Iter 30, disc loss: 0.00010920730803581845, policy loss: 10.777382411651207
Experience 32, Iter 31, disc loss: 0.00012048191207957234, policy loss: 10.479024926810434
Experience 32, Iter 32, disc loss: 0.0001358996743014777, policy loss: 10.37881487215607
Experience 32, Iter 33, disc loss: 9.511803917496436e-05, policy loss: 11.127395543392494
Experience 32, Iter 34, disc loss: 0.00014884073862840577, policy loss: 10.791529073421952
Experience 32, Iter 35, disc loss: 0.00013499263127766636, policy loss: 10.382318775696064
Experience 32, Iter 36, disc loss: 0.0001118083148022914, policy loss: 10.682362782461905
Experience 32, Iter 37, disc loss: 0.0001151629079289337, policy loss: 10.534212348103626
Experience 32, Iter 38, disc loss: 9.55273593718666e-05, policy loss: 10.892531780793002
Experience 32, Iter 39, disc loss: 0.00013223907023184934, policy loss: 10.543518366862266
Experience 32, Iter 40, disc loss: 0.00012925772698279712, policy loss: 10.612970322773005
Experience 32, Iter 41, disc loss: 0.0001381055868612875, policy loss: 10.79003702020384
Experience 32, Iter 42, disc loss: 0.00010618560056579279, policy loss: 10.898808589923673
Experience 32, Iter 43, disc loss: 0.0001686464862141101, policy loss: 10.840237908753704
Experience 32, Iter 44, disc loss: 0.0001500202770089271, policy loss: 10.619735094220275
Experience 32, Iter 45, disc loss: 0.00010916585906868047, policy loss: 11.124253563748862
Experience 32, Iter 46, disc loss: 0.00013921677386092264, policy loss: 10.657333584657787
Experience 32, Iter 47, disc loss: 0.00010926103726315614, policy loss: 10.618291254530163
Experience 32, Iter 48, disc loss: 0.0001847802173056515, policy loss: 10.619020310301465
Experience 32, Iter 49, disc loss: 9.797790037750556e-05, policy loss: 10.785382248741232
Experience 32, Iter 50, disc loss: 8.92108053048052e-05, policy loss: 10.89553172524871
Experience 32, Iter 51, disc loss: 8.63765727727893e-05, policy loss: 10.935941865727994
Experience 32, Iter 52, disc loss: 0.00013759313869877608, policy loss: 10.63733428791836
Experience 32, Iter 53, disc loss: 9.581088796201841e-05, policy loss: 10.923825554876027
Experience 32, Iter 54, disc loss: 0.0001457992189763956, policy loss: 10.705369980434185
Experience 32, Iter 55, disc loss: 0.00011239099236902325, policy loss: 10.351094962208105
Experience 32, Iter 56, disc loss: 0.00016232474152478465, policy loss: 10.63873854307219
Experience 32, Iter 57, disc loss: 0.00015467323201200707, policy loss: 10.92452408842476
Experience 32, Iter 58, disc loss: 0.00011551380801057203, policy loss: 11.088364853417382
Experience 32, Iter 59, disc loss: 0.00011734596071598472, policy loss: 10.552189598840279
Experience 32, Iter 60, disc loss: 0.00021850743666501188, policy loss: 10.397501601918417
Experience 32, Iter 61, disc loss: 0.00010344543509406815, policy loss: 11.051300119605656
Experience 32, Iter 62, disc loss: 0.00014929978262192862, policy loss: 10.598570709755101
Experience 32, Iter 63, disc loss: 0.0002204835701218473, policy loss: 10.226072532383808
Experience 32, Iter 64, disc loss: 0.00010789221959953124, policy loss: 10.572234442219216
Experience 32, Iter 65, disc loss: 0.00011581011923969073, policy loss: 10.498666534773212
Experience 32, Iter 66, disc loss: 0.00011067989816763871, policy loss: 10.690844363545153
Experience 32, Iter 67, disc loss: 0.0001463250466390001, policy loss: 10.584443335284973
Experience 32, Iter 68, disc loss: 0.00011501523471755316, policy loss: 10.84193085714833
Experience 32, Iter 69, disc loss: 0.00013465200367527663, policy loss: 10.632266341356857
Experience 32, Iter 70, disc loss: 0.00017286009010673817, policy loss: 10.74568767328952
Experience 32, Iter 71, disc loss: 0.0002786681384531245, policy loss: 10.181815286787426
Experience 32, Iter 72, disc loss: 0.00012065314879691369, policy loss: 10.84024193119695
Experience 32, Iter 73, disc loss: 0.00015417949347441612, policy loss: 11.3366952454308
Experience 32, Iter 74, disc loss: 8.645806252591113e-05, policy loss: 10.796353350186102
Experience 32, Iter 75, disc loss: 0.00020022991052119022, policy loss: 10.699679113310047
Experience 32, Iter 76, disc loss: 0.0001739740256893051, policy loss: 10.427642526978733
Experience 32, Iter 77, disc loss: 0.0001285896066909307, policy loss: 10.928783072910264
Experience 32, Iter 78, disc loss: 9.240256476988041e-05, policy loss: 10.811856591696603
Experience 32, Iter 79, disc loss: 0.00012907466746368476, policy loss: 10.945231009720601
Experience 32, Iter 80, disc loss: 9.216677786977048e-05, policy loss: 10.818023843654695
Experience 32, Iter 81, disc loss: 0.00010532996478346162, policy loss: 10.716647286572417
Experience 32, Iter 82, disc loss: 0.00015378278435508204, policy loss: 10.60147269917472
Experience 32, Iter 83, disc loss: 0.00011579034950530923, policy loss: 10.769513055584618
Experience 32, Iter 84, disc loss: 0.00013114570153143577, policy loss: 10.708926649944702
Experience 32, Iter 85, disc loss: 0.00014555405519773885, policy loss: 10.581052737788017
Experience 32, Iter 86, disc loss: 8.88578011641167e-05, policy loss: 10.834528343640777
Experience 32, Iter 87, disc loss: 0.00020542445358593973, policy loss: 10.764580233805205
Experience 32, Iter 88, disc loss: 8.787236681915082e-05, policy loss: 11.135392134964729
Experience 32, Iter 89, disc loss: 0.00022864219563914156, policy loss: 10.89365104769549
Experience 32, Iter 90, disc loss: 0.00013583477193833196, policy loss: 10.85296663819375
Experience 32, Iter 91, disc loss: 0.00010473265108909804, policy loss: 11.061518239869233
Experience 32, Iter 92, disc loss: 0.0001496622689162101, policy loss: 11.18543928236928
Experience 32, Iter 93, disc loss: 0.00011111168973506462, policy loss: 10.922673131645173
Experience 32, Iter 94, disc loss: 0.00010380022387078054, policy loss: 10.927194419449076
Experience 32, Iter 95, disc loss: 0.0001224255526537098, policy loss: 11.047830866054618
Experience 32, Iter 96, disc loss: 9.437791541003252e-05, policy loss: 11.101112664143201
Experience 32, Iter 97, disc loss: 6.817462728644699e-05, policy loss: 11.505558222331707
Experience 32, Iter 98, disc loss: 0.00011356407125951946, policy loss: 10.816656752537224
Experience 32, Iter 99, disc loss: 9.898952957162899e-05, policy loss: 11.127871256392318
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0148],
        [0.2336],
        [2.0608],
        [0.0303]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0815, 0.5783, 1.4121, 0.0375, 0.0106, 6.6066]],

        [[0.0815, 0.5783, 1.4121, 0.0375, 0.0106, 6.6066]],

        [[0.0815, 0.5783, 1.4121, 0.0375, 0.0106, 6.6066]],

        [[0.0815, 0.5783, 1.4121, 0.0375, 0.0106, 6.6066]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0591, 0.9345, 8.2430, 0.1213], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0591, 0.9345, 8.2430, 0.1213], device='cuda:0')
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.953
Iter 2/2000 - Loss: 3.867
Iter 3/2000 - Loss: 3.729
Iter 4/2000 - Loss: 3.636
Iter 5/2000 - Loss: 3.529
Iter 6/2000 - Loss: 3.388
Iter 7/2000 - Loss: 3.245
Iter 8/2000 - Loss: 3.106
Iter 9/2000 - Loss: 2.956
Iter 10/2000 - Loss: 2.785
Iter 11/2000 - Loss: 2.596
Iter 12/2000 - Loss: 2.396
Iter 13/2000 - Loss: 2.187
Iter 14/2000 - Loss: 1.970
Iter 15/2000 - Loss: 1.742
Iter 16/2000 - Loss: 1.502
Iter 17/2000 - Loss: 1.250
Iter 18/2000 - Loss: 0.987
Iter 19/2000 - Loss: 0.716
Iter 20/2000 - Loss: 0.438
Iter 1981/2000 - Loss: -7.627
Iter 1982/2000 - Loss: -7.627
Iter 1983/2000 - Loss: -7.627
Iter 1984/2000 - Loss: -7.627
Iter 1985/2000 - Loss: -7.627
Iter 1986/2000 - Loss: -7.627
Iter 1987/2000 - Loss: -7.627
Iter 1988/2000 - Loss: -7.627
Iter 1989/2000 - Loss: -7.627
Iter 1990/2000 - Loss: -7.627
Iter 1991/2000 - Loss: -7.627
Iter 1992/2000 - Loss: -7.627
Iter 1993/2000 - Loss: -7.627
Iter 1994/2000 - Loss: -7.627
Iter 1995/2000 - Loss: -7.627
Iter 1996/2000 - Loss: -7.627
Iter 1997/2000 - Loss: -7.627
Iter 1998/2000 - Loss: -7.627
Iter 1999/2000 - Loss: -7.628
Iter 2000/2000 - Loss: -7.628
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.5727, 14.5620, 24.7288,  2.9372,  6.2959, 61.3440]],

        [[25.3836, 46.7677,  7.5976,  1.3561,  1.2267, 22.6566]],

        [[26.3457, 49.7350,  7.6749,  0.8847,  0.8567, 21.6413]],

        [[20.7983, 42.4715, 18.6679,  1.2524,  2.2953, 44.8558]]],
       device='cuda:0')
Signal Variance: tensor([ 0.1994,  1.8112, 10.3639,  0.5015], device='cuda:0')
Estimated target variance: tensor([0.0591, 0.9345, 8.2430, 0.1213], device='cuda:0')
N: 330
Signal to noise ratio: tensor([23.7665, 66.9298, 69.3423, 41.0004], device='cuda:0')
Bound on condition number: tensor([ 186400.5620, 1478269.2647, 1586757.7764,  554740.5865],
       device='cuda:0')
Policy Optimizer learning rate:
0.009668466638656902
Experience 33, Iter 0, disc loss: 8.565957315029192e-05, policy loss: 11.156880599456716
Experience 33, Iter 1, disc loss: 0.0001153296870924097, policy loss: 10.866403888011213
Experience 33, Iter 2, disc loss: 0.00012445230243160406, policy loss: 10.847694366059251
Experience 33, Iter 3, disc loss: 0.00017770134063379878, policy loss: 10.480975459923487
Experience 33, Iter 4, disc loss: 0.00010561627060646164, policy loss: 10.93764773345185
Experience 33, Iter 5, disc loss: 0.00012899626459301366, policy loss: 10.806550046326576
Experience 33, Iter 6, disc loss: 7.450873061947124e-05, policy loss: 11.067468496937018
Experience 33, Iter 7, disc loss: 0.00012964157523012525, policy loss: 10.743794505455218
Experience 33, Iter 8, disc loss: 0.00010699322675743086, policy loss: 10.79877651668302
Experience 33, Iter 9, disc loss: 0.00021685186074770394, policy loss: 10.955433871642697
Experience 33, Iter 10, disc loss: 0.00010887306908216462, policy loss: 10.976276366266301
Experience 33, Iter 11, disc loss: 0.00014393853758978072, policy loss: 11.041011424457995
Experience 33, Iter 12, disc loss: 0.00024144008592998634, policy loss: 10.898792935466348
Experience 33, Iter 13, disc loss: 9.175208944882561e-05, policy loss: 11.03555386156396
Experience 33, Iter 14, disc loss: 0.000182232580969681, policy loss: 10.674932613630927
Experience 33, Iter 15, disc loss: 0.00011839163887778876, policy loss: 11.105549875953123
Experience 33, Iter 16, disc loss: 0.00010420812580472725, policy loss: 10.944064386218376
Experience 33, Iter 17, disc loss: 7.58993881251397e-05, policy loss: 11.04525715920905
Experience 33, Iter 18, disc loss: 0.0001550643804008404, policy loss: 11.121724913200662
Experience 33, Iter 19, disc loss: 0.00011144283929186575, policy loss: 11.15746375358216
Experience 33, Iter 20, disc loss: 0.00010964437383835976, policy loss: 10.886078087601948
Experience 33, Iter 21, disc loss: 0.000110911594617254, policy loss: 10.801889880531636
Experience 33, Iter 22, disc loss: 0.00015599879412937437, policy loss: 11.07410107193266
Experience 33, Iter 23, disc loss: 0.0001607591544803749, policy loss: 11.426660633725266
Experience 33, Iter 24, disc loss: 0.00013189848016625195, policy loss: 10.733451908429373
Experience 33, Iter 25, disc loss: 0.00016171148225949735, policy loss: 11.531727924061482
Experience 33, Iter 26, disc loss: 0.00014388376818957852, policy loss: 10.77633706559699
Experience 33, Iter 27, disc loss: 0.00015189270398364876, policy loss: 11.502320158651973
Experience 33, Iter 28, disc loss: 6.878847787886641e-05, policy loss: 11.653243882230289
Experience 33, Iter 29, disc loss: 0.00014408287573190948, policy loss: 10.723340309027641
Experience 33, Iter 30, disc loss: 9.998272519381944e-05, policy loss: 11.039195458464258
Experience 33, Iter 31, disc loss: 0.00014090780275609528, policy loss: 10.764071268749811
Experience 33, Iter 32, disc loss: 0.00015163080894169752, policy loss: 10.909538549723617
Experience 33, Iter 33, disc loss: 0.0001384468854763845, policy loss: 10.77098930863381
Experience 33, Iter 34, disc loss: 0.00013068188558408046, policy loss: 10.669060912876217
Experience 33, Iter 35, disc loss: 0.00014547197899209097, policy loss: 11.007759763248503
Experience 33, Iter 36, disc loss: 0.00019316080758334492, policy loss: 10.991031812139681
Experience 33, Iter 37, disc loss: 0.00013650030417314317, policy loss: 10.873996660997822
Experience 33, Iter 38, disc loss: 8.059583761093833e-05, policy loss: 11.102878657595367
Experience 33, Iter 39, disc loss: 0.000149522350928985, policy loss: 10.925648498400037
Experience 33, Iter 40, disc loss: 0.0001404860083575602, policy loss: 11.059388587358427
Experience 33, Iter 41, disc loss: 0.000320679222421592, policy loss: 10.800189260013969
Experience 33, Iter 42, disc loss: 0.0001293554484288957, policy loss: 10.83951210951564
Experience 33, Iter 43, disc loss: 6.592800078560561e-05, policy loss: 12.112949695901282
Experience 33, Iter 44, disc loss: 0.00024034597274511356, policy loss: 10.861664618494526
Experience 33, Iter 45, disc loss: 0.00010656713224890281, policy loss: 11.32559609655269
Experience 33, Iter 46, disc loss: 0.00011011976800220181, policy loss: 11.131085466715355
Experience 33, Iter 47, disc loss: 7.834576262356836e-05, policy loss: 11.262172169626634
Experience 33, Iter 48, disc loss: 9.965064590854836e-05, policy loss: 11.026044925594231
Experience 33, Iter 49, disc loss: 0.0002759158824163744, policy loss: 10.593438512769954
Experience 33, Iter 50, disc loss: 0.0002559024888656885, policy loss: 11.415563820895262
Experience 33, Iter 51, disc loss: 0.00011808387425943378, policy loss: 11.189240529440621
Experience 33, Iter 52, disc loss: 0.00018752007000116813, policy loss: 11.704325721738568
Experience 33, Iter 53, disc loss: 0.00013671315999695324, policy loss: 11.370825079444558
Experience 33, Iter 54, disc loss: 0.00023827516371164914, policy loss: 10.383187021562124
Experience 33, Iter 55, disc loss: 0.00018876002402853435, policy loss: 10.986181848484279
Experience 33, Iter 56, disc loss: 0.00022156301752932462, policy loss: 11.329990934550793
Experience 33, Iter 57, disc loss: 0.00024053599855460115, policy loss: 11.247074421219105
Experience 33, Iter 58, disc loss: 0.00018769414500302851, policy loss: 10.815625743872403
Experience 33, Iter 59, disc loss: 0.00019628682486819762, policy loss: 11.164435153393088
Experience 33, Iter 60, disc loss: 9.142193787026309e-05, policy loss: 11.0871483413306
Experience 33, Iter 61, disc loss: 0.00015269197216070845, policy loss: 11.090541011409188
Experience 33, Iter 62, disc loss: 0.00029682388256806327, policy loss: 10.833301401736804
Experience 33, Iter 63, disc loss: 0.00012487557757751485, policy loss: 10.980949053228723
Experience 33, Iter 64, disc loss: 0.00010160213615890051, policy loss: 10.7990762691337
Experience 33, Iter 65, disc loss: 0.00012769125767044055, policy loss: 11.10403021498302
Experience 33, Iter 66, disc loss: 7.869884990383624e-05, policy loss: 11.007103882738084
Experience 33, Iter 67, disc loss: 0.0001249112866834077, policy loss: 11.13428654869616
Experience 33, Iter 68, disc loss: 0.0002493615556494919, policy loss: 10.9283744376663
Experience 33, Iter 69, disc loss: 0.0001351697823895218, policy loss: 11.267618490700947
Experience 33, Iter 70, disc loss: 0.0001054483154107163, policy loss: 11.213643758032443
Experience 33, Iter 71, disc loss: 0.00015852764348940845, policy loss: 10.876751092230009
Experience 33, Iter 72, disc loss: 0.0001283098964950127, policy loss: 11.069349866698255
Experience 33, Iter 73, disc loss: 0.0001361681815416925, policy loss: 11.31096193744419
Experience 33, Iter 74, disc loss: 0.00010745916769844819, policy loss: 10.981485087539973
Experience 33, Iter 75, disc loss: 0.0001712604489219901, policy loss: 11.11185851213308
Experience 33, Iter 76, disc loss: 0.00016401385883329432, policy loss: 10.70525142406755
Experience 33, Iter 77, disc loss: 8.130148056565045e-05, policy loss: 11.454657147502285
Experience 33, Iter 78, disc loss: 8.30609140073746e-05, policy loss: 11.51889358602011
Experience 33, Iter 79, disc loss: 7.554102515533162e-05, policy loss: 11.495779401319805
Experience 33, Iter 80, disc loss: 0.00012570872381331181, policy loss: 11.223307957759538
Experience 33, Iter 81, disc loss: 6.962202238904719e-05, policy loss: 11.440896044948255
Experience 33, Iter 82, disc loss: 8.868537440827048e-05, policy loss: 11.023589702251389
Experience 33, Iter 83, disc loss: 9.211850008934767e-05, policy loss: 11.264461601622553
Experience 33, Iter 84, disc loss: 8.947036604747356e-05, policy loss: 10.977720172774454
Experience 33, Iter 85, disc loss: 7.218094897203008e-05, policy loss: 11.440911270304834
Experience 33, Iter 86, disc loss: 0.00010894994412174869, policy loss: 11.221464966188353
Experience 33, Iter 87, disc loss: 7.170169060192616e-05, policy loss: 11.662524065683396
Experience 33, Iter 88, disc loss: 8.704612883760198e-05, policy loss: 11.27889356706744
Experience 33, Iter 89, disc loss: 9.00812735538799e-05, policy loss: 11.316750837965905
Experience 33, Iter 90, disc loss: 9.626577142586023e-05, policy loss: 11.290730365649154
Experience 33, Iter 91, disc loss: 0.00011879255992076746, policy loss: 10.756438172406101
Experience 33, Iter 92, disc loss: 9.756680994721565e-05, policy loss: 11.093104574572232
Experience 33, Iter 93, disc loss: 9.770023202664634e-05, policy loss: 11.304542220614069
Experience 33, Iter 94, disc loss: 7.223018047431768e-05, policy loss: 11.382861209032168
Experience 33, Iter 95, disc loss: 0.00016565823010197405, policy loss: 11.126493045050083
Experience 33, Iter 96, disc loss: 0.0001232635025482804, policy loss: 11.007985864822487
Experience 33, Iter 97, disc loss: 7.866779315191503e-05, policy loss: 11.521969958260566
Experience 33, Iter 98, disc loss: 6.981027575561941e-05, policy loss: 11.332980275584568
Experience 33, Iter 99, disc loss: 9.49599682595761e-05, policy loss: 11.045000487834933
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0156],
        [0.2304],
        [2.0249],
        [0.0300]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0873, 0.6096, 1.3962, 0.0377, 0.0105, 6.5774]],

        [[0.0873, 0.6096, 1.3962, 0.0377, 0.0105, 6.5774]],

        [[0.0873, 0.6096, 1.3962, 0.0377, 0.0105, 6.5774]],

        [[0.0873, 0.6096, 1.3962, 0.0377, 0.0105, 6.5774]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0624, 0.9217, 8.0997, 0.1201], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0624, 0.9217, 8.0997, 0.1201], device='cuda:0')
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.911
Iter 2/2000 - Loss: 3.807
Iter 3/2000 - Loss: 3.670
Iter 4/2000 - Loss: 3.566
Iter 5/2000 - Loss: 3.447
Iter 6/2000 - Loss: 3.302
Iter 7/2000 - Loss: 3.157
Iter 8/2000 - Loss: 3.015
Iter 9/2000 - Loss: 2.858
Iter 10/2000 - Loss: 2.678
Iter 11/2000 - Loss: 2.483
Iter 12/2000 - Loss: 2.281
Iter 13/2000 - Loss: 2.072
Iter 14/2000 - Loss: 1.854
Iter 15/2000 - Loss: 1.623
Iter 16/2000 - Loss: 1.379
Iter 17/2000 - Loss: 1.125
Iter 18/2000 - Loss: 0.861
Iter 19/2000 - Loss: 0.591
Iter 20/2000 - Loss: 0.315
Iter 1981/2000 - Loss: -7.665
Iter 1982/2000 - Loss: -7.665
Iter 1983/2000 - Loss: -7.665
Iter 1984/2000 - Loss: -7.665
Iter 1985/2000 - Loss: -7.665
Iter 1986/2000 - Loss: -7.665
Iter 1987/2000 - Loss: -7.666
Iter 1988/2000 - Loss: -7.666
Iter 1989/2000 - Loss: -7.666
Iter 1990/2000 - Loss: -7.666
Iter 1991/2000 - Loss: -7.666
Iter 1992/2000 - Loss: -7.666
Iter 1993/2000 - Loss: -7.666
Iter 1994/2000 - Loss: -7.666
Iter 1995/2000 - Loss: -7.666
Iter 1996/2000 - Loss: -7.666
Iter 1997/2000 - Loss: -7.666
Iter 1998/2000 - Loss: -7.666
Iter 1999/2000 - Loss: -7.666
Iter 2000/2000 - Loss: -7.666
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.7676, 15.0524, 24.1607,  4.2733,  5.0887, 61.2219]],

        [[25.3940, 46.9279,  7.6065,  1.3383,  1.2309, 23.1915]],

        [[26.4718, 50.2685,  7.6559,  0.8763,  0.8515, 21.6795]],

        [[20.7834, 42.9607, 18.6387,  1.2491,  2.2224, 46.4656]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2212,  1.8563, 10.2541,  0.5015], device='cuda:0')
Estimated target variance: tensor([0.0624, 0.9217, 8.0997, 0.1201], device='cuda:0')
N: 340
Signal to noise ratio: tensor([25.0767, 68.0996, 68.6767, 40.4921], device='cuda:0')
Bound on condition number: tensor([ 213806.2058, 1576768.4237, 1603609.0339,  557468.1383],
       device='cuda:0')
Policy Optimizer learning rate:
0.009658285256870233
Experience 34, Iter 0, disc loss: 6.897852585123948e-05, policy loss: 11.463397352689558
Experience 34, Iter 1, disc loss: 7.543208021200581e-05, policy loss: 11.47364648526878
Experience 34, Iter 2, disc loss: 9.128480272481773e-05, policy loss: 11.177849412127824
Experience 34, Iter 3, disc loss: 6.14492296801638e-05, policy loss: 11.104101518250143
Experience 34, Iter 4, disc loss: 0.0001026566466495557, policy loss: 11.41395671696545
Experience 34, Iter 5, disc loss: 8.817306472968256e-05, policy loss: 11.218896634288871
Experience 34, Iter 6, disc loss: 6.683669228836105e-05, policy loss: 11.658306119066173
Experience 34, Iter 7, disc loss: 9.119368761020068e-05, policy loss: 10.768788919738576
Experience 34, Iter 8, disc loss: 8.07839649137047e-05, policy loss: 11.026853705591254
Experience 34, Iter 9, disc loss: 7.726040901133592e-05, policy loss: 11.383114471923351
Experience 34, Iter 10, disc loss: 7.921895064641878e-05, policy loss: 11.306422405070805
Experience 34, Iter 11, disc loss: 6.997959419735559e-05, policy loss: 11.211449453563299
Experience 34, Iter 12, disc loss: 7.223842530148328e-05, policy loss: 11.339339793553773
Experience 34, Iter 13, disc loss: 7.85208732424457e-05, policy loss: 10.888795016902833
Experience 34, Iter 14, disc loss: 7.473981829849398e-05, policy loss: 11.300735441383296
Experience 34, Iter 15, disc loss: 7.264882176084007e-05, policy loss: 11.189354457426628
Experience 34, Iter 16, disc loss: 6.554004428595849e-05, policy loss: 11.237324099901148
Experience 34, Iter 17, disc loss: 8.258084241582805e-05, policy loss: 10.871501360420481
Experience 34, Iter 18, disc loss: 9.69491735086487e-05, policy loss: 10.535927992266117
Experience 34, Iter 19, disc loss: 7.893812718060724e-05, policy loss: 11.23890552717149
Experience 34, Iter 20, disc loss: 6.818091091743579e-05, policy loss: 11.49547016725468
Experience 34, Iter 21, disc loss: 7.069637644059465e-05, policy loss: 11.385273285741542
Experience 34, Iter 22, disc loss: 7.803330909331236e-05, policy loss: 11.283723279648207
Experience 34, Iter 23, disc loss: 6.725949578588148e-05, policy loss: 11.120499822089684
Experience 34, Iter 24, disc loss: 0.00011940929454955155, policy loss: 10.914210903151393
Experience 34, Iter 25, disc loss: 8.276373427966622e-05, policy loss: 10.854129923217478
Experience 34, Iter 26, disc loss: 7.152410968992908e-05, policy loss: 11.274921243193285
Experience 34, Iter 27, disc loss: 7.037612315823951e-05, policy loss: 11.262224785194515
Experience 34, Iter 28, disc loss: 9.430967957054846e-05, policy loss: 10.677553113584251
Experience 34, Iter 29, disc loss: 6.839070223412904e-05, policy loss: 11.256640694122373
Experience 34, Iter 30, disc loss: 7.58108037235558e-05, policy loss: 11.289310485848276
Experience 34, Iter 31, disc loss: 7.728317495780753e-05, policy loss: 10.871331198966747
Experience 34, Iter 32, disc loss: 6.159780989691696e-05, policy loss: 11.471079927984444
Experience 34, Iter 33, disc loss: 6.348053024309776e-05, policy loss: 11.245991068762859
Experience 34, Iter 34, disc loss: 8.083251838141004e-05, policy loss: 10.868932255714764
Experience 34, Iter 35, disc loss: 7.915313197648659e-05, policy loss: 10.990073013498279
Experience 34, Iter 36, disc loss: 7.601281781924038e-05, policy loss: 11.111456550378053
Experience 34, Iter 37, disc loss: 6.739456688935669e-05, policy loss: 11.326247364201299
Experience 34, Iter 38, disc loss: 7.36389122329146e-05, policy loss: 11.204507741555798
Experience 34, Iter 39, disc loss: 8.471513063007432e-05, policy loss: 11.010271421851757
Experience 34, Iter 40, disc loss: 6.651921585083324e-05, policy loss: 11.21410760640602
Experience 34, Iter 41, disc loss: 6.402880639314195e-05, policy loss: 11.204875677448111
Experience 34, Iter 42, disc loss: 8.033037379765221e-05, policy loss: 10.917531800670108
Experience 34, Iter 43, disc loss: 6.836573888573608e-05, policy loss: 11.174140688697381
Experience 34, Iter 44, disc loss: 7.381571440400817e-05, policy loss: 11.229696326544332
Experience 34, Iter 45, disc loss: 6.30531948988282e-05, policy loss: 11.641860894963077
Experience 34, Iter 46, disc loss: 8.834843874897441e-05, policy loss: 10.83054631663366
Experience 34, Iter 47, disc loss: 6.1736958955661e-05, policy loss: 11.314257481454046
Experience 34, Iter 48, disc loss: 6.471506564379668e-05, policy loss: 11.392875832561726
Experience 34, Iter 49, disc loss: 6.170924116913062e-05, policy loss: 11.339423165684043
Experience 34, Iter 50, disc loss: 8.288752858990329e-05, policy loss: 11.086409925556438
Experience 34, Iter 51, disc loss: 7.02393271470314e-05, policy loss: 11.307714418900375
Experience 34, Iter 52, disc loss: 7.443620456187992e-05, policy loss: 11.160567264359527
Experience 34, Iter 53, disc loss: 6.139613161051533e-05, policy loss: 11.50315108069936
Experience 34, Iter 54, disc loss: 7.61217874448104e-05, policy loss: 10.915984053490904
Experience 34, Iter 55, disc loss: 7.321072274439199e-05, policy loss: 11.019237346687586
Experience 34, Iter 56, disc loss: 5.1759175502823676e-05, policy loss: 11.674153207043759
Experience 34, Iter 57, disc loss: 7.383955298445659e-05, policy loss: 11.213268501804437
Experience 34, Iter 58, disc loss: 7.849830255871153e-05, policy loss: 11.233883372586462
Experience 34, Iter 59, disc loss: 8.042730436030684e-05, policy loss: 11.377544111909039
Experience 34, Iter 60, disc loss: 6.323937363129169e-05, policy loss: 11.231325565869781
Experience 34, Iter 61, disc loss: 7.019655935137555e-05, policy loss: 11.235786198061923
Experience 34, Iter 62, disc loss: 8.475434322447657e-05, policy loss: 10.82584057671735
Experience 34, Iter 63, disc loss: 7.354419402562289e-05, policy loss: 11.063411265252418
Experience 34, Iter 64, disc loss: 6.510100277341588e-05, policy loss: 11.502443258339898
Experience 34, Iter 65, disc loss: 0.00011593739630013275, policy loss: 10.640970713976259
Experience 34, Iter 66, disc loss: 6.27680703538726e-05, policy loss: 11.239631712921774
Experience 34, Iter 67, disc loss: 6.78203539251782e-05, policy loss: 11.165389513953764
Experience 34, Iter 68, disc loss: 9.105427962415548e-05, policy loss: 11.075270903064462
Experience 34, Iter 69, disc loss: 6.331119884769839e-05, policy loss: 11.131587632767694
Experience 34, Iter 70, disc loss: 7.064827495486493e-05, policy loss: 11.387388920405254
Experience 34, Iter 71, disc loss: 8.86223362506227e-05, policy loss: 11.120375059545553
Experience 34, Iter 72, disc loss: 8.452797580793127e-05, policy loss: 10.860281680256907
Experience 34, Iter 73, disc loss: 5.808237937706177e-05, policy loss: 11.617046154587927
Experience 34, Iter 74, disc loss: 0.00010270618703295469, policy loss: 11.356134959357473
Experience 34, Iter 75, disc loss: 6.66303424697239e-05, policy loss: 11.336022084866823
Experience 34, Iter 76, disc loss: 6.921204951496117e-05, policy loss: 11.020265938281423
Experience 34, Iter 77, disc loss: 6.910879343084203e-05, policy loss: 11.362798911172325
Experience 34, Iter 78, disc loss: 7.884395962103302e-05, policy loss: 11.438220282457722
Experience 34, Iter 79, disc loss: 6.51997930521969e-05, policy loss: 11.259575749353063
Experience 34, Iter 80, disc loss: 6.423593132923175e-05, policy loss: 11.14912271811766
Experience 34, Iter 81, disc loss: 5.709947653244075e-05, policy loss: 11.538406454524837
Experience 34, Iter 82, disc loss: 6.593709161562821e-05, policy loss: 11.164286504986833
Experience 34, Iter 83, disc loss: 7.379473793174546e-05, policy loss: 11.29214494235595
Experience 34, Iter 84, disc loss: 8.799495932370995e-05, policy loss: 11.428535676381347
Experience 34, Iter 85, disc loss: 6.659676311508996e-05, policy loss: 11.093363886505315
Experience 34, Iter 86, disc loss: 6.514018236132166e-05, policy loss: 11.258065760490734
Experience 34, Iter 87, disc loss: 7.308038075349769e-05, policy loss: 11.330731114242766
Experience 34, Iter 88, disc loss: 6.485676343906367e-05, policy loss: 11.073431654339306
Experience 34, Iter 89, disc loss: 6.802815320547498e-05, policy loss: 10.983468775415936
Experience 34, Iter 90, disc loss: 8.598622941055785e-05, policy loss: 10.561342879339632
Experience 34, Iter 91, disc loss: 6.261533138419428e-05, policy loss: 11.4512615680918
Experience 34, Iter 92, disc loss: 6.261687409213037e-05, policy loss: 11.14945310598002
Experience 34, Iter 93, disc loss: 5.9128240539268456e-05, policy loss: 11.327197825306415
Experience 34, Iter 94, disc loss: 5.951734060304363e-05, policy loss: 11.240805879607105
Experience 34, Iter 95, disc loss: 6.998863840058204e-05, policy loss: 11.169104836738594
Experience 34, Iter 96, disc loss: 5.819247993940993e-05, policy loss: 11.43675830509693
Experience 34, Iter 97, disc loss: 6.845874254111722e-05, policy loss: 11.118340391222016
Experience 34, Iter 98, disc loss: 5.384224747247184e-05, policy loss: 11.344980552873373
Experience 34, Iter 99, disc loss: 6.962272064336854e-05, policy loss: 11.276096233364303
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0163],
        [0.2277],
        [2.0015],
        [0.0299]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0924, 0.6366, 1.3896, 0.0380, 0.0106, 6.5528]],

        [[0.0924, 0.6366, 1.3896, 0.0380, 0.0106, 6.5528]],

        [[0.0924, 0.6366, 1.3896, 0.0380, 0.0106, 6.5528]],

        [[0.0924, 0.6366, 1.3896, 0.0380, 0.0106, 6.5528]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0652, 0.9107, 8.0059, 0.1196], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0652, 0.9107, 8.0059, 0.1196], device='cuda:0')
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.876
Iter 2/2000 - Loss: 3.756
Iter 3/2000 - Loss: 3.616
Iter 4/2000 - Loss: 3.499
Iter 5/2000 - Loss: 3.370
Iter 6/2000 - Loss: 3.219
Iter 7/2000 - Loss: 3.069
Iter 8/2000 - Loss: 2.920
Iter 9/2000 - Loss: 2.756
Iter 10/2000 - Loss: 2.570
Iter 11/2000 - Loss: 2.371
Iter 12/2000 - Loss: 2.166
Iter 13/2000 - Loss: 1.955
Iter 14/2000 - Loss: 1.734
Iter 15/2000 - Loss: 1.500
Iter 16/2000 - Loss: 1.255
Iter 17/2000 - Loss: 1.002
Iter 18/2000 - Loss: 0.742
Iter 19/2000 - Loss: 0.475
Iter 20/2000 - Loss: 0.201
Iter 1981/2000 - Loss: -7.714
Iter 1982/2000 - Loss: -7.714
Iter 1983/2000 - Loss: -7.714
Iter 1984/2000 - Loss: -7.714
Iter 1985/2000 - Loss: -7.714
Iter 1986/2000 - Loss: -7.714
Iter 1987/2000 - Loss: -7.714
Iter 1988/2000 - Loss: -7.714
Iter 1989/2000 - Loss: -7.714
Iter 1990/2000 - Loss: -7.714
Iter 1991/2000 - Loss: -7.714
Iter 1992/2000 - Loss: -7.714
Iter 1993/2000 - Loss: -7.714
Iter 1994/2000 - Loss: -7.714
Iter 1995/2000 - Loss: -7.714
Iter 1996/2000 - Loss: -7.714
Iter 1997/2000 - Loss: -7.714
Iter 1998/2000 - Loss: -7.714
Iter 1999/2000 - Loss: -7.714
Iter 2000/2000 - Loss: -7.714
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.9715, 15.1777, 24.2743,  4.7619,  4.8051, 61.0045]],

        [[25.4315, 47.0269,  7.4922,  1.3497,  1.2266, 22.9123]],

        [[26.2832, 50.2629,  7.6818,  0.8925,  0.8583, 21.8874]],

        [[20.7238, 43.3775, 18.7752,  1.2477,  2.2596, 45.4525]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2294,  1.8273, 10.5355,  0.5109], device='cuda:0')
Estimated target variance: tensor([0.0652, 0.9107, 8.0059, 0.1196], device='cuda:0')
N: 350
Signal to noise ratio: tensor([25.6437, 68.5514, 68.6850, 41.0753], device='cuda:0')
Bound on condition number: tensor([ 230160.2632, 1644752.1839, 1651172.6131,  590513.6155],
       device='cuda:0')
Policy Optimizer learning rate:
0.009648114596590806
Experience 35, Iter 0, disc loss: 5.1316371202883896e-05, policy loss: 11.423808930095552
Experience 35, Iter 1, disc loss: 6.405058238785196e-05, policy loss: 11.077622233749665
Experience 35, Iter 2, disc loss: 6.630318083595136e-05, policy loss: 10.864109854240077
Experience 35, Iter 3, disc loss: 6.469404408472841e-05, policy loss: 10.897336974700671
Experience 35, Iter 4, disc loss: 6.729263105441661e-05, policy loss: 11.113564814998671
Experience 35, Iter 5, disc loss: 8.733477293538478e-05, policy loss: 11.14393159114957
Experience 35, Iter 6, disc loss: 9.061897237960551e-05, policy loss: 10.96402966612236
Experience 35, Iter 7, disc loss: 8.256688893797733e-05, policy loss: 10.856098507104159
Experience 35, Iter 8, disc loss: 6.0792469854634756e-05, policy loss: 11.297804189136876
Experience 35, Iter 9, disc loss: 7.584076094453884e-05, policy loss: 11.342995971907559
Experience 35, Iter 10, disc loss: 6.460058342848109e-05, policy loss: 11.294651520682933
Experience 35, Iter 11, disc loss: 7.444306015835886e-05, policy loss: 11.096954283123925
Experience 35, Iter 12, disc loss: 6.657958087465121e-05, policy loss: 10.972644062339178
Experience 35, Iter 13, disc loss: 6.100066642842256e-05, policy loss: 11.296108628259287
Experience 35, Iter 14, disc loss: 6.232493456831569e-05, policy loss: 11.266705063335277
Experience 35, Iter 15, disc loss: 8.03845185693665e-05, policy loss: 11.131010226461353
Experience 35, Iter 16, disc loss: 9.45089262539472e-05, policy loss: 10.792339079695994
Experience 35, Iter 17, disc loss: 7.646233643510467e-05, policy loss: 10.674611805345187
Experience 35, Iter 18, disc loss: 6.513273117941579e-05, policy loss: 11.11360156971816
Experience 35, Iter 19, disc loss: 6.256622036140136e-05, policy loss: 11.467222672346404
Experience 35, Iter 20, disc loss: 0.00011354671107624816, policy loss: 10.715821414950508
Experience 35, Iter 21, disc loss: 7.401482566639397e-05, policy loss: 10.997302620017084
Experience 35, Iter 22, disc loss: 6.398694506602314e-05, policy loss: 11.185275083029165
Experience 35, Iter 23, disc loss: 7.789266014246349e-05, policy loss: 10.823737968588306
Experience 35, Iter 24, disc loss: 6.307930465365224e-05, policy loss: 11.002269163646654
Experience 35, Iter 25, disc loss: 7.192825298094107e-05, policy loss: 11.050098162168927
Experience 35, Iter 26, disc loss: 6.938533495493769e-05, policy loss: 11.230576673826386
Experience 35, Iter 27, disc loss: 5.8835752901651656e-05, policy loss: 11.140174368577174
Experience 35, Iter 28, disc loss: 6.300994322677839e-05, policy loss: 11.081478739458909
Experience 35, Iter 29, disc loss: 7.331589702017012e-05, policy loss: 10.593974959769023
Experience 35, Iter 30, disc loss: 6.730861899219927e-05, policy loss: 11.10749736242606
Experience 35, Iter 31, disc loss: 9.220539376133008e-05, policy loss: 10.506693396313135
Experience 35, Iter 32, disc loss: 4.8791587428209815e-05, policy loss: 11.271476290969375
Experience 35, Iter 33, disc loss: 7.03403693612583e-05, policy loss: 10.813984433783542
Experience 35, Iter 34, disc loss: 5.599470162072497e-05, policy loss: 11.245860948320097
Experience 35, Iter 35, disc loss: 5.8243504943279644e-05, policy loss: 10.990654570094213
Experience 35, Iter 36, disc loss: 7.371521649249279e-05, policy loss: 10.887700745339734
Experience 35, Iter 37, disc loss: 6.97378835843587e-05, policy loss: 10.565861817150266
Experience 35, Iter 38, disc loss: 6.47490907516719e-05, policy loss: 10.770781306445466
Experience 35, Iter 39, disc loss: 9.930735470899493e-05, policy loss: 10.89130088954693
Experience 35, Iter 40, disc loss: 6.082093378446021e-05, policy loss: 11.21803866102394
Experience 35, Iter 41, disc loss: 6.224213162326915e-05, policy loss: 11.221840349171297
Experience 35, Iter 42, disc loss: 6.039322610329848e-05, policy loss: 11.250215781349326
Experience 35, Iter 43, disc loss: 5.374317372585258e-05, policy loss: 11.265899613237814
Experience 35, Iter 44, disc loss: 7.329444455020582e-05, policy loss: 10.666761476885565
Experience 35, Iter 45, disc loss: 7.828208432862645e-05, policy loss: 11.10115364120309
Experience 35, Iter 46, disc loss: 6.847653087285207e-05, policy loss: 11.125221763278228
Experience 35, Iter 47, disc loss: 5.5555293306538934e-05, policy loss: 11.040015903006063
Experience 35, Iter 48, disc loss: 5.851844325384279e-05, policy loss: 11.035174863339137
Experience 35, Iter 49, disc loss: 6.054172301675396e-05, policy loss: 10.834435827415094
Experience 35, Iter 50, disc loss: 6.678176410485023e-05, policy loss: 10.819155467451145
Experience 35, Iter 51, disc loss: 5.9329839974026594e-05, policy loss: 11.042530743507145
Experience 35, Iter 52, disc loss: 5.6154805971667815e-05, policy loss: 10.985042148289061
Experience 35, Iter 53, disc loss: 6.809476922767777e-05, policy loss: 10.906860289839635
Experience 35, Iter 54, disc loss: 5.518474828655671e-05, policy loss: 11.15708244664901
Experience 35, Iter 55, disc loss: 7.699115513716456e-05, policy loss: 10.811114416363772
Experience 35, Iter 56, disc loss: 8.22115026526494e-05, policy loss: 10.675701646575586
Experience 35, Iter 57, disc loss: 6.578063600670077e-05, policy loss: 11.168124236716892
Experience 35, Iter 58, disc loss: 6.57288206105923e-05, policy loss: 10.85085202779561
Experience 35, Iter 59, disc loss: 6.408449084113787e-05, policy loss: 10.931311858596157
Experience 35, Iter 60, disc loss: 0.00010238966693366507, policy loss: 10.540045288430141
Experience 35, Iter 61, disc loss: 6.043602631568894e-05, policy loss: 10.931102651862407
Experience 35, Iter 62, disc loss: 6.910596418494857e-05, policy loss: 11.044343342447373
Experience 35, Iter 63, disc loss: 6.218238998254216e-05, policy loss: 10.9234468629795
Experience 35, Iter 64, disc loss: 6.539022495204012e-05, policy loss: 10.924027976371754
Experience 35, Iter 65, disc loss: 7.948902240231229e-05, policy loss: 10.598585173771134
Experience 35, Iter 66, disc loss: 8.587951033354366e-05, policy loss: 10.80793631115456
Experience 35, Iter 67, disc loss: 5.913005121172291e-05, policy loss: 10.830055462274462
Experience 35, Iter 68, disc loss: 6.37157576499817e-05, policy loss: 10.920744287696
Experience 35, Iter 69, disc loss: 6.357376776001729e-05, policy loss: 10.748119902824836
Experience 35, Iter 70, disc loss: 7.362580109220376e-05, policy loss: 10.931155923096428
Experience 35, Iter 71, disc loss: 6.153465738158059e-05, policy loss: 11.031179909193359
Experience 35, Iter 72, disc loss: 5.476876238314014e-05, policy loss: 11.055863049188492
Experience 35, Iter 73, disc loss: 6.561066835320196e-05, policy loss: 10.78269676120323
Experience 35, Iter 74, disc loss: 7.264664103479875e-05, policy loss: 10.73137826940706
Experience 35, Iter 75, disc loss: 0.00010097339018772062, policy loss: 10.45733210501887
Experience 35, Iter 76, disc loss: 6.082059177673229e-05, policy loss: 10.653999238814546
Experience 35, Iter 77, disc loss: 5.707346873603928e-05, policy loss: 10.861587103451686
Experience 35, Iter 78, disc loss: 8.801883221101722e-05, policy loss: 10.49790297462493
Experience 35, Iter 79, disc loss: 5.771455921428581e-05, policy loss: 10.96752145594173
Experience 35, Iter 80, disc loss: 7.392322417075339e-05, policy loss: 10.332903806989002
Experience 35, Iter 81, disc loss: 6.457596530176515e-05, policy loss: 10.880413380682338
Experience 35, Iter 82, disc loss: 5.734589134574296e-05, policy loss: 11.027047173872933
Experience 35, Iter 83, disc loss: 6.22362664118504e-05, policy loss: 11.02740723253822
Experience 35, Iter 84, disc loss: 6.432316983193633e-05, policy loss: 11.013148016145983
Experience 35, Iter 85, disc loss: 5.648088717511902e-05, policy loss: 10.955237425160947
Experience 35, Iter 86, disc loss: 5.5334379131825715e-05, policy loss: 11.068292585643563
Experience 35, Iter 87, disc loss: 9.051420638774387e-05, policy loss: 10.870858871417573
Experience 35, Iter 88, disc loss: 7.235853527575593e-05, policy loss: 10.611379264686954
Experience 35, Iter 89, disc loss: 6.207980415520353e-05, policy loss: 10.589707092066165
Experience 35, Iter 90, disc loss: 0.00015549422823213675, policy loss: 10.30128305031247
Experience 35, Iter 91, disc loss: 6.735801431514702e-05, policy loss: 10.542901212826678
Experience 35, Iter 92, disc loss: 4.599568080804081e-05, policy loss: 11.348801985597945
Experience 35, Iter 93, disc loss: 6.498884446827915e-05, policy loss: 10.690869604176589
Experience 35, Iter 94, disc loss: 5.411337323429563e-05, policy loss: 11.089599765368586
Experience 35, Iter 95, disc loss: 6.433325737337746e-05, policy loss: 10.860819297472721
Experience 35, Iter 96, disc loss: 6.06931567369431e-05, policy loss: 10.67878855830924
Experience 35, Iter 97, disc loss: 5.630391912146201e-05, policy loss: 10.765055681518717
Experience 35, Iter 98, disc loss: 5.584514105726952e-05, policy loss: 10.840701328718353
Experience 35, Iter 99, disc loss: 6.286615416904575e-05, policy loss: 10.74369155720293
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0167],
        [0.2238],
        [1.9568],
        [0.0294]], device='cuda:0', grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0950, 0.6506, 1.3643, 0.0380, 0.0104, 6.4941]],

        [[0.0950, 0.6506, 1.3643, 0.0380, 0.0104, 6.4941]],

        [[0.0950, 0.6506, 1.3643, 0.0380, 0.0104, 6.4941]],

        [[0.0950, 0.6506, 1.3643, 0.0380, 0.0104, 6.4941]]], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0668, 0.8953, 7.8273, 0.1175], device='cuda:0',
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0668, 0.8953, 7.8273, 0.1175], device='cuda:0')
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], device='cuda:0',
       grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], device='cuda:0',
       grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.856
Iter 2/2000 - Loss: 3.725
Iter 3/2000 - Loss: 3.591
Iter 4/2000 - Loss: 3.468
Iter 5/2000 - Loss: 3.336
Iter 6/2000 - Loss: 3.187
Iter 7/2000 - Loss: 3.037
Iter 8/2000 - Loss: 2.884
Iter 9/2000 - Loss: 2.714
Iter 10/2000 - Loss: 2.523
Iter 11/2000 - Loss: 2.323
Iter 12/2000 - Loss: 2.116
Iter 13/2000 - Loss: 1.902
Iter 14/2000 - Loss: 1.675
Iter 15/2000 - Loss: 1.437
Iter 16/2000 - Loss: 1.190
Iter 17/2000 - Loss: 0.936
Iter 18/2000 - Loss: 0.675
Iter 19/2000 - Loss: 0.406
Iter 20/2000 - Loss: 0.129
Iter 1981/2000 - Loss: -7.771
Iter 1982/2000 - Loss: -7.771
Iter 1983/2000 - Loss: -7.771
Iter 1984/2000 - Loss: -7.771
Iter 1985/2000 - Loss: -7.771
Iter 1986/2000 - Loss: -7.771
Iter 1987/2000 - Loss: -7.771
Iter 1988/2000 - Loss: -7.771
Iter 1989/2000 - Loss: -7.771
Iter 1990/2000 - Loss: -7.771
Iter 1991/2000 - Loss: -7.771
Iter 1992/2000 - Loss: -7.771
Iter 1993/2000 - Loss: -7.772
Iter 1994/2000 - Loss: -7.772
Iter 1995/2000 - Loss: -7.772
Iter 1996/2000 - Loss: -7.772
Iter 1997/2000 - Loss: -7.772
Iter 1998/2000 - Loss: -7.772
Iter 1999/2000 - Loss: -7.772
Iter 2000/2000 - Loss: -7.772
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]], device='cuda:0')
Lengthscale: tensor([[[17.7342, 15.1309, 23.6917,  5.6650,  4.5282, 60.8854]],

        [[25.3858, 47.0149,  7.5133,  1.3659,  1.2154, 22.6914]],

        [[25.9602, 49.8471,  7.7661,  0.8820,  0.8449, 21.9322]],

        [[20.6829, 43.2086, 18.8775,  1.2695,  2.3279, 43.8436]]],
       device='cuda:0')
Signal Variance: tensor([ 0.2338,  1.8227, 10.5436,  0.5114], device='cuda:0')
Estimated target variance: tensor([0.0668, 0.8953, 7.8273, 0.1175], device='cuda:0')
N: 360
Signal to noise ratio: tensor([26.0042, 68.9525, 69.3625, 41.1575], device='cuda:0')
Bound on condition number: tensor([ 243440.4810, 1711603.5657, 1732016.4160,  609818.2408],
       device='cuda:0')
Policy Optimizer learning rate:
0.009637954646528333
Traceback (most recent call last):
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/scratch/aso115/final_year_project/cartpole/learn.py", line 209, in <module>
    setup, expert_samples, policy, fm, disc, disc_optimizer, policy_optimizer, init_state_distn)
  File "/scratch/aso115/final_year_project/cartpole/pathwise_grad.py", line 20, in pathwise_grad
    setup, fm, init_state_distn, policy, sample_N=N, sample_T=setup.T, with_rsample=True)
  File "/scratch/aso115/final_year_project/cartpole/utils.py", line 198, in sample_trajectories
    y, _ = fm.predict(x_t_u_t, with_rsample=True)
  File "/scratch/aso115/final_year_project/forwardmodel.py", line 48, in predict
    dyn_model = self.predictive_distn(x)
  File "/scratch/aso115/final_year_project/forwardmodel.py", line 72, in predictive_distn
    dyn_model = self.model(self.__adjust_shape(x))
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/models/exact_gp.py", line 265, in __call__
    predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(full_mean, full_covar)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/models/exact_prediction_strategies.py", line 264, in exact_prediction
    self.exact_predictive_mean(test_mean, test_train_covar),
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/models/exact_prediction_strategies.py", line 284, in exact_predictive_mean
    res = (test_train_covar @ self.mean_cache.unsqueeze(-1)).squeeze(-1)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/utils/memoize.py", line 34, in g
    add_to_cache(self, cache_name, method(self, *args, **kwargs))
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/models/exact_prediction_strategies.py", line 236, in mean_cache
    mean_cache = train_train_covar.inv_matmul(train_labels_offset).squeeze(-1)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py", line 917, in inv_matmul
    *self.representation(),
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/functions/_inv_matmul.py", line 46, in forward
    solves = _solve(lazy_tsr, right_tensor)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/functions/_inv_matmul.py", line 10, in _solve
    return lazy_tsr._cholesky()._cholesky_solve(rhs)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/utils/memoize.py", line 34, in g
    add_to_cache(self, cache_name, method(self, *args, **kwargs))
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py", line 396, in _cholesky
    evaluated_mat = self.evaluate()
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/utils/memoize.py", line 34, in g
    add_to_cache(self, cache_name, method(self, *args, **kwargs))
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py", line 62, in evaluate
    return sum(lazy_tensor.evaluate() for lazy_tensor in self.lazy_tensors)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py", line 62, in <genexpr>
    return sum(lazy_tensor.evaluate() for lazy_tensor in self.lazy_tensors)
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/utils/memoize.py", line 34, in g
    add_to_cache(self, cache_name, method(self, *args, **kwargs))
  File "/vol/bitbucket/aso115/anaconda3/envs/myvenv/lib/python3.6/site-packages/gpytorch/lazy/diag_lazy_tensor.py", line 103, in evaluate
    return self._diag.unsqueeze(-1) * torch.eye(self._diag.shape[-1], dtype=self.dtype, device=self.device)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.75 GiB total capacity; 9.15 GiB already allocated; 3.25 MiB free; 1.55 GiB cached)
