Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.0088],
        [0.8106],
        [0.0199]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0245, 0.0354, 3.2423, 0.0797], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0245, 0.0354, 3.2423, 0.0797])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 6.311
Iter 2/2000 - Loss: 4.160
Iter 3/2000 - Loss: 2.768
Iter 4/2000 - Loss: 2.066
Iter 5/2000 - Loss: 1.792
Iter 6/2000 - Loss: 1.720
Iter 7/2000 - Loss: 1.743
Iter 8/2000 - Loss: 1.805
Iter 9/2000 - Loss: 1.863
Iter 10/2000 - Loss: 1.890
Iter 11/2000 - Loss: 1.891
Iter 12/2000 - Loss: 1.886
Iter 13/2000 - Loss: 1.887
Iter 14/2000 - Loss: 1.894
Iter 15/2000 - Loss: 1.898
Iter 16/2000 - Loss: 1.893
Iter 17/2000 - Loss: 1.877
Iter 18/2000 - Loss: 1.849
Iter 19/2000 - Loss: 1.808
Iter 20/2000 - Loss: 1.751
Iter 1981/2000 - Loss: -0.182
Iter 1982/2000 - Loss: -0.182
Iter 1983/2000 - Loss: -0.182
Iter 1984/2000 - Loss: -0.182
Iter 1985/2000 - Loss: -0.182
Iter 1986/2000 - Loss: -0.183
Iter 1987/2000 - Loss: -0.183
Iter 1988/2000 - Loss: -0.183
Iter 1989/2000 - Loss: -0.183
Iter 1990/2000 - Loss: -0.183
Iter 1991/2000 - Loss: -0.183
Iter 1992/2000 - Loss: -0.183
Iter 1993/2000 - Loss: -0.183
Iter 1994/2000 - Loss: -0.183
Iter 1995/2000 - Loss: -0.183
Iter 1996/2000 - Loss: -0.183
Iter 1997/2000 - Loss: -0.184
Iter 1998/2000 - Loss: -0.184
Iter 1999/2000 - Loss: -0.184
Iter 2000/2000 - Loss: -0.184
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0044],
        [0.0063],
        [0.0001],
        [0.0142]])
Lengthscale: tensor([[[7.3874e-02, 2.6251e-01, 9.0815e-01, 1.4946e-02, 5.9710e-03,
          1.4709e-01]],

        [[7.5174e-02, 2.6308e-01, 9.1033e-01, 1.4962e-02, 6.1360e-03,
          1.4716e-01]],

        [[1.7622e+01, 2.6664e+01, 1.3772e+01, 7.1129e-01, 7.5306e+00,
          6.8021e+00]],

        [[7.3864e-02, 2.6251e-01, 9.0813e-01, 1.4946e-02, 5.9697e-03,
          1.4709e-01]]])
Signal Variance: tensor([0.0176, 0.0255, 4.0856, 0.0576])
Estimated target variance: tensor([0.0245, 0.0354, 3.2423, 0.0797])
N: 10
Signal to noise ratio: tensor([  2.0022,   2.0103, 199.7138,   2.0166])
Bound on condition number: tensor([4.1087e+01, 4.1412e+01, 3.9886e+05, 4.1668e+01])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.3050036242198821, policy loss: 0.8528824080817677
Experience 1, Iter 1, disc loss: 1.2988510744495645, policy loss: 0.8537985920497514
Experience 1, Iter 2, disc loss: 1.3015536263420495, policy loss: 0.845091976803512
Experience 1, Iter 3, disc loss: 1.3098033453862246, policy loss: 0.8266391110807256
Experience 1, Iter 4, disc loss: 1.3121824372100268, policy loss: 0.8201709851165213
Experience 1, Iter 5, disc loss: 1.364958676389426, policy loss: 0.7508719057421707
Experience 1, Iter 6, disc loss: 1.369250878245794, policy loss: 0.7429661603011989
Experience 1, Iter 7, disc loss: 1.3958880991193623, policy loss: 0.7082956398594618
Experience 1, Iter 8, disc loss: 1.38470759111097, policy loss: 0.7145052742561738
Experience 1, Iter 9, disc loss: 1.3757857958135653, policy loss: 0.719031966923328
Experience 1, Iter 10, disc loss: 1.3872370611902947, policy loss: 0.6992636148329164
Experience 1, Iter 11, disc loss: 1.342780963918221, policy loss: 0.7344264184095813
Experience 1, Iter 12, disc loss: 1.3635386959448552, policy loss: 0.7098759128974743
Experience 1, Iter 13, disc loss: 1.3409414196250793, policy loss: 0.7183949691510702
Experience 1, Iter 14, disc loss: 1.3005571530008588, policy loss: 0.7558090031055765
Experience 1, Iter 15, disc loss: 1.284094579118674, policy loss: 0.7680945574438269
Experience 1, Iter 16, disc loss: 1.2687481091732986, policy loss: 0.775952447299356
Experience 1, Iter 17, disc loss: 1.233889802526852, policy loss: 0.8061970070069755
Experience 1, Iter 18, disc loss: 1.2367222684418817, policy loss: 0.796330535542446
Experience 1, Iter 19, disc loss: 1.2261547964474033, policy loss: 0.8001807134311467
Experience 1, Iter 20, disc loss: 1.2020197483159352, policy loss: 0.824452104030739
Experience 1, Iter 21, disc loss: 1.1836249769856826, policy loss: 0.837798139143028
Experience 1, Iter 22, disc loss: 1.1558748964531314, policy loss: 0.8676053171161643
Experience 1, Iter 23, disc loss: 1.1510154898661829, policy loss: 0.8669595384221093
Experience 1, Iter 24, disc loss: 1.1315960545679398, policy loss: 0.8840408471134562
Experience 1, Iter 25, disc loss: 1.1153621120653316, policy loss: 0.8990483713423341
Experience 1, Iter 26, disc loss: 1.0902619565759686, policy loss: 0.9276621034955188
Experience 1, Iter 27, disc loss: 1.0715115913853364, policy loss: 0.9484233566413753
Experience 1, Iter 28, disc loss: 1.0518832614503282, policy loss: 0.968044516067265
Experience 1, Iter 29, disc loss: 1.0166788747978888, policy loss: 1.022571732131456
Experience 1, Iter 30, disc loss: 1.005413738125454, policy loss: 1.0304388212063051
Experience 1, Iter 31, disc loss: 0.9936008939734162, policy loss: 1.0390605161642175
Experience 1, Iter 32, disc loss: 0.9794291719824134, policy loss: 1.0559747220506748
Experience 1, Iter 33, disc loss: 0.9546721093110105, policy loss: 1.0963657386460717
Experience 1, Iter 34, disc loss: 0.9222451544950134, policy loss: 1.1516400894577967
Experience 1, Iter 35, disc loss: 0.9311474712642065, policy loss: 1.1205276725304336
Experience 1, Iter 36, disc loss: 0.8974705409670877, policy loss: 1.183350407153696
Experience 1, Iter 37, disc loss: 0.8835350883120245, policy loss: 1.2053354406348007
Experience 1, Iter 38, disc loss: 0.8743834416298061, policy loss: 1.2091737981783346
Experience 1, Iter 39, disc loss: 0.8762760842787531, policy loss: 1.1914086568634554
Experience 1, Iter 40, disc loss: 0.8431601168757308, policy loss: 1.2552090915240575
Experience 1, Iter 41, disc loss: 0.8624163164836136, policy loss: 1.1883460334319513
Experience 1, Iter 42, disc loss: 0.8664408054796571, policy loss: 1.149824858243056
Experience 1, Iter 43, disc loss: 0.862165698974594, policy loss: 1.1461183340545902
Experience 1, Iter 44, disc loss: 0.8713374818316819, policy loss: 1.1080714639129394
Experience 1, Iter 45, disc loss: 0.855511702713867, policy loss: 1.1276736426573284
Experience 1, Iter 46, disc loss: 0.8362911936686229, policy loss: 1.1624962713228935
Experience 1, Iter 47, disc loss: 0.8437408601288272, policy loss: 1.1341096150457197
Experience 1, Iter 48, disc loss: 0.8141091301529134, policy loss: 1.1891715359788868
Experience 1, Iter 49, disc loss: 0.812507960703611, policy loss: 1.1871193074521136
Experience 1, Iter 50, disc loss: 0.7776614058994797, policy loss: 1.2613894136138355
Experience 1, Iter 51, disc loss: 0.7890291100616484, policy loss: 1.218959876537565
Experience 1, Iter 52, disc loss: 0.7757704638524223, policy loss: 1.243469857862368
Experience 1, Iter 53, disc loss: 0.7564934557252776, policy loss: 1.2808280077606375
Experience 1, Iter 54, disc loss: 0.7393441271364946, policy loss: 1.314141523418309
Experience 1, Iter 55, disc loss: 0.7295881458802407, policy loss: 1.328950781447364
Experience 1, Iter 56, disc loss: 0.7165030359579317, policy loss: 1.3512116888559789
Experience 1, Iter 57, disc loss: 0.6998423060350077, policy loss: 1.3872955467716264
Experience 1, Iter 58, disc loss: 0.6863891467678127, policy loss: 1.4142865955240396
Experience 1, Iter 59, disc loss: 0.6700733665052259, policy loss: 1.444355185340954
Experience 1, Iter 60, disc loss: 0.6542228260942549, policy loss: 1.4743257785980992
Experience 1, Iter 61, disc loss: 0.6510556939738794, policy loss: 1.4649772715029314
Experience 1, Iter 62, disc loss: 0.6379839883628388, policy loss: 1.488114475432066
Experience 1, Iter 63, disc loss: 0.625869360352303, policy loss: 1.5035180432922306
Experience 1, Iter 64, disc loss: 0.5998582712956415, policy loss: 1.5741128198409882
Experience 1, Iter 65, disc loss: 0.5852785152654323, policy loss: 1.6030936248439198
Experience 1, Iter 66, disc loss: 0.5637109828756535, policy loss: 1.6629060573712744
Experience 1, Iter 67, disc loss: 0.55384565975876, policy loss: 1.663485045946027
Experience 1, Iter 68, disc loss: 0.5305379612453691, policy loss: 1.7323311255304015
Experience 1, Iter 69, disc loss: 0.5312106565578758, policy loss: 1.6965937526722414
Experience 1, Iter 70, disc loss: 0.4975319047254733, policy loss: 1.8345577786867355
Experience 1, Iter 71, disc loss: 0.508485471050459, policy loss: 1.733427243566494
Experience 1, Iter 72, disc loss: 0.4831038751372946, policy loss: 1.813262739712217
Experience 1, Iter 73, disc loss: 0.46728042193408287, policy loss: 1.86221476029659
Experience 1, Iter 74, disc loss: 0.4563760725931786, policy loss: 1.8939753524360579
Experience 1, Iter 75, disc loss: 0.4544297870876073, policy loss: 1.865419571668273
Experience 1, Iter 76, disc loss: 0.4359387036968239, policy loss: 1.9308694723668034
Experience 1, Iter 77, disc loss: 0.422741284921202, policy loss: 1.966141001339222
Experience 1, Iter 78, disc loss: 0.41140962419875826, policy loss: 1.980787287398679
Experience 1, Iter 79, disc loss: 0.4113458259458008, policy loss: 1.960889858015944
Experience 1, Iter 80, disc loss: 0.40052304850911946, policy loss: 1.951934843773596
Experience 1, Iter 81, disc loss: 0.3848086398867362, policy loss: 2.034554479128141
Experience 1, Iter 82, disc loss: 0.3820249963289991, policy loss: 1.9966855369738272
Experience 1, Iter 83, disc loss: 0.3875933000645083, policy loss: 1.9363352536438851
Experience 1, Iter 84, disc loss: 0.3770258094587772, policy loss: 1.9479234887416519
Experience 1, Iter 85, disc loss: 0.36490082770526944, policy loss: 2.002796476752361
Experience 1, Iter 86, disc loss: 0.36282362514504185, policy loss: 1.9694559529443767
Experience 1, Iter 87, disc loss: 0.3475109423960572, policy loss: 2.021671290845104
Experience 1, Iter 88, disc loss: 0.3460241281286708, policy loss: 1.9851342569459214
Experience 1, Iter 89, disc loss: 0.3390019511073979, policy loss: 1.998086468439571
Experience 1, Iter 90, disc loss: 0.33216487558330166, policy loss: 1.9836879447330296
Experience 1, Iter 91, disc loss: 0.31069463504096717, policy loss: 2.0932339351373113
Experience 1, Iter 92, disc loss: 0.30138379479068433, policy loss: 2.139873845231485
Experience 1, Iter 93, disc loss: 0.29170985574064784, policy loss: 2.153949839093136
Experience 1, Iter 94, disc loss: 0.284247778357449, policy loss: 2.1628603730501545
Experience 1, Iter 95, disc loss: 0.27536753708046496, policy loss: 2.1876197218250586
Experience 1, Iter 96, disc loss: 0.2637866302701998, policy loss: 2.25756882434071
Experience 1, Iter 97, disc loss: 0.25365568293498325, policy loss: 2.286882121822366
Experience 1, Iter 98, disc loss: 0.246039175200199, policy loss: 2.309322651034118
Experience 1, Iter 99, disc loss: 0.23602550021582064, policy loss: 2.357721442461098
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0254],
        [0.0954],
        [0.4553],
        [0.0102]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3032e-01, 9.2938e-01, 4.6899e-01, 3.4748e-02, 3.4001e-03,
          4.3985e+00]],

        [[1.3032e-01, 9.2938e-01, 4.6899e-01, 3.4748e-02, 3.4001e-03,
          4.3985e+00]],

        [[1.3032e-01, 9.2938e-01, 4.6899e-01, 3.4748e-02, 3.4001e-03,
          4.3985e+00]],

        [[1.3032e-01, 9.2938e-01, 4.6899e-01, 3.4748e-02, 3.4001e-03,
          4.3985e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.1018, 0.3816, 1.8210, 0.0409], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.1018, 0.3816, 1.8210, 0.0409])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.735
Iter 2/2000 - Loss: 2.779
Iter 3/2000 - Loss: 2.667
Iter 4/2000 - Loss: 2.662
Iter 5/2000 - Loss: 2.680
Iter 6/2000 - Loss: 2.634
Iter 7/2000 - Loss: 2.594
Iter 8/2000 - Loss: 2.588
Iter 9/2000 - Loss: 2.576
Iter 10/2000 - Loss: 2.535
Iter 11/2000 - Loss: 2.490
Iter 12/2000 - Loss: 2.457
Iter 13/2000 - Loss: 2.430
Iter 14/2000 - Loss: 2.391
Iter 15/2000 - Loss: 2.339
Iter 16/2000 - Loss: 2.281
Iter 17/2000 - Loss: 2.226
Iter 18/2000 - Loss: 2.172
Iter 19/2000 - Loss: 2.111
Iter 20/2000 - Loss: 2.041
Iter 1981/2000 - Loss: -4.446
Iter 1982/2000 - Loss: -4.446
Iter 1983/2000 - Loss: -4.446
Iter 1984/2000 - Loss: -4.446
Iter 1985/2000 - Loss: -4.447
Iter 1986/2000 - Loss: -4.447
Iter 1987/2000 - Loss: -4.447
Iter 1988/2000 - Loss: -4.447
Iter 1989/2000 - Loss: -4.447
Iter 1990/2000 - Loss: -4.447
Iter 1991/2000 - Loss: -4.447
Iter 1992/2000 - Loss: -4.447
Iter 1993/2000 - Loss: -4.447
Iter 1994/2000 - Loss: -4.447
Iter 1995/2000 - Loss: -4.447
Iter 1996/2000 - Loss: -4.447
Iter 1997/2000 - Loss: -4.447
Iter 1998/2000 - Loss: -4.447
Iter 1999/2000 - Loss: -4.448
Iter 2000/2000 - Loss: -4.448
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0013],
        [0.0002]])
Lengthscale: tensor([[[18.0969,  8.8916, 46.5567,  8.3165, 15.3807, 81.9939]],

        [[31.9810, 62.8276, 21.0794,  1.4651,  0.8529, 13.6709]],

        [[64.8037, 81.5732, 12.7326,  0.8301, 12.2557, 18.3815]],

        [[36.3535, 71.7227,  9.9207,  3.8536, 14.0693, 35.6592]]])
Signal Variance: tensor([ 0.2406,  0.9293, 12.3506,  0.3342])
Estimated target variance: tensor([0.1018, 0.3816, 1.8210, 0.0409])
N: 20
Signal to noise ratio: tensor([21.8090, 40.6469, 97.1756, 43.2618])
Bound on condition number: tensor([  9513.6497,  33044.4472, 188862.9924,  37432.7371])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.7669324446344382, policy loss: 0.7463255986258754
Experience 2, Iter 1, disc loss: 0.7631811768261193, policy loss: 0.7467485860061508
Experience 2, Iter 2, disc loss: 0.7563536151308677, policy loss: 0.7508305655450711
Experience 2, Iter 3, disc loss: 0.749499800905706, policy loss: 0.7549049207082418
Experience 2, Iter 4, disc loss: 0.7416594682958088, policy loss: 0.760374526511936
Experience 2, Iter 5, disc loss: 0.7361222902564202, policy loss: 0.7633524158295367
Experience 2, Iter 6, disc loss: 0.7401070838706003, policy loss: 0.7550164414856524
Experience 2, Iter 7, disc loss: 0.7359266037271442, policy loss: 0.7566281113856657
Experience 2, Iter 8, disc loss: 0.7422940153372906, policy loss: 0.7458618284513754
Experience 2, Iter 9, disc loss: 0.7443022912358642, policy loss: 0.7410084376923975
Experience 2, Iter 10, disc loss: 0.7384879304641307, policy loss: 0.7454870980755648
Experience 2, Iter 11, disc loss: 0.7401503582372198, policy loss: 0.7427170224323807
Experience 2, Iter 12, disc loss: 0.7258161607891728, policy loss: 0.7612448141266522
Experience 2, Iter 13, disc loss: 0.7138795272039262, policy loss: 0.7763034453444879
Experience 2, Iter 14, disc loss: 0.6628384635376521, policy loss: 0.8493148470443079
Experience 2, Iter 15, disc loss: 0.6740209902771451, policy loss: 0.8231083489177463
Experience 2, Iter 16, disc loss: 0.689781951983829, policy loss: 0.7998345460546561
Experience 2, Iter 17, disc loss: 0.6841069680350047, policy loss: 0.799148994693836
Experience 2, Iter 18, disc loss: 0.6792986704098216, policy loss: 0.8013708756402593
Experience 2, Iter 19, disc loss: 0.6700528396880164, policy loss: 0.8093015309974647
Experience 2, Iter 20, disc loss: 0.6746991193009089, policy loss: 0.801003220953084
Experience 2, Iter 21, disc loss: 0.67118693791069, policy loss: 0.8022252628888158
Experience 2, Iter 22, disc loss: 0.6574903165924182, policy loss: 0.8155482562352632
Experience 2, Iter 23, disc loss: 0.6562241684241027, policy loss: 0.8150472556251183
Experience 2, Iter 24, disc loss: 0.6439426801586963, policy loss: 0.8271956760071907
Experience 2, Iter 25, disc loss: 0.6343598404605582, policy loss: 0.8377357698206118
Experience 2, Iter 26, disc loss: 0.6344256169399152, policy loss: 0.8352523408687424
Experience 2, Iter 27, disc loss: 0.6227721264091446, policy loss: 0.8492478227046059
Experience 2, Iter 28, disc loss: 0.623364347467714, policy loss: 0.8467182051938021
Experience 2, Iter 29, disc loss: 0.6179013129496478, policy loss: 0.8517523985981836
Experience 2, Iter 30, disc loss: 0.608726195044244, policy loss: 0.8636012964699658
Experience 2, Iter 31, disc loss: 0.6064877599979354, policy loss: 0.8659014996688541
Experience 2, Iter 32, disc loss: 0.6037938900047392, policy loss: 0.8669489873618762
Experience 2, Iter 33, disc loss: 0.59779083812213, policy loss: 0.8762177520520508
Experience 2, Iter 34, disc loss: 0.6097011953246702, policy loss: 0.8602835320317047
Experience 2, Iter 35, disc loss: 0.5908517402985746, policy loss: 0.8898867191114055
Experience 2, Iter 36, disc loss: 0.6375787318913955, policy loss: 0.8446488717207371
Experience 2, Iter 37, disc loss: 0.7529000932126713, policy loss: 0.7257470382542347
Experience 2, Iter 38, disc loss: 0.8759161944486427, policy loss: 0.6124690618191222
Experience 2, Iter 39, disc loss: 0.9181137574879159, policy loss: 0.5701672112191677
Experience 2, Iter 40, disc loss: 0.8214522415619872, policy loss: 0.6969466036299048
Experience 2, Iter 41, disc loss: 0.8235995487792309, policy loss: 0.6853864276102924
Experience 2, Iter 42, disc loss: 0.8443233249175544, policy loss: 0.7006068784135331
Experience 2, Iter 43, disc loss: 0.841567864605612, policy loss: 0.6799458018225499
Experience 2, Iter 44, disc loss: 0.799634370407555, policy loss: 0.7548833736714998
Experience 2, Iter 45, disc loss: 0.861135371472979, policy loss: 0.6821116051316238
Experience 2, Iter 46, disc loss: 0.8277439146861867, policy loss: 0.7092752241505462
Experience 2, Iter 47, disc loss: 0.7854364854400672, policy loss: 0.787511257492718
Experience 2, Iter 48, disc loss: 0.8040541881114651, policy loss: 0.7586218179033869
Experience 2, Iter 49, disc loss: 0.7858157379860538, policy loss: 0.7349446652883438
Experience 2, Iter 50, disc loss: 0.7121965773956297, policy loss: 0.8765431098628348
Experience 2, Iter 51, disc loss: 0.7028088745733136, policy loss: 0.875145940333502
Experience 2, Iter 52, disc loss: 0.6337223930117192, policy loss: 0.9576118479051172
Experience 2, Iter 53, disc loss: 0.6011723432295445, policy loss: 1.0039913581402922
Experience 2, Iter 54, disc loss: 0.6205669303196872, policy loss: 0.9824797214249915
Experience 2, Iter 55, disc loss: 0.6710811920609835, policy loss: 0.9190513098647399
Experience 2, Iter 56, disc loss: 0.6536854623498702, policy loss: 0.9428402552993823
Experience 2, Iter 57, disc loss: 0.6741712513864835, policy loss: 0.908222844112154
Experience 2, Iter 58, disc loss: 0.6944088771320237, policy loss: 0.8845174481527405
Experience 2, Iter 59, disc loss: 0.6422417635673644, policy loss: 0.9640372475964156
Experience 2, Iter 60, disc loss: 0.655921512766404, policy loss: 0.9456346870858543
Experience 2, Iter 61, disc loss: 0.6358025107995158, policy loss: 0.9811089300794937
Experience 2, Iter 62, disc loss: 0.6085489049977745, policy loss: 1.0370859153106404
Experience 2, Iter 63, disc loss: 0.6342794578008216, policy loss: 0.9870809023193587
Experience 2, Iter 64, disc loss: 0.597741709004263, policy loss: 1.058439168232933
Experience 2, Iter 65, disc loss: 0.5717994351355677, policy loss: 1.1093857088373889
Experience 2, Iter 66, disc loss: 0.5712685984734324, policy loss: 1.1179938999209098
Experience 2, Iter 67, disc loss: 0.5751419657829921, policy loss: 1.1158190291622745
Experience 2, Iter 68, disc loss: 0.5730709201531623, policy loss: 1.0968854878161418
Experience 2, Iter 69, disc loss: 0.5601073359828105, policy loss: 1.1231122708906387
Experience 2, Iter 70, disc loss: 0.5518654090002317, policy loss: 1.144052338247974
Experience 2, Iter 71, disc loss: 0.5570366502632842, policy loss: 1.1437773102985591
Experience 2, Iter 72, disc loss: 0.5453453745872024, policy loss: 1.1590803709309125
Experience 2, Iter 73, disc loss: 0.5334001744035668, policy loss: 1.19370002592678
Experience 2, Iter 74, disc loss: 0.539703952990716, policy loss: 1.1645857551000225
Experience 2, Iter 75, disc loss: 0.5197781399466067, policy loss: 1.197431656814541
Experience 2, Iter 76, disc loss: 0.5129816006362312, policy loss: 1.221373482063805
Experience 2, Iter 77, disc loss: 0.48873536158259195, policy loss: 1.2775344707716532
Experience 2, Iter 78, disc loss: 0.4940865656365254, policy loss: 1.2822227066089882
Experience 2, Iter 79, disc loss: 0.4997006507132917, policy loss: 1.2275532971654473
Experience 2, Iter 80, disc loss: 0.4663609189984883, policy loss: 1.3005014435455724
Experience 2, Iter 81, disc loss: 0.476707854189908, policy loss: 1.2744007018947396
Experience 2, Iter 82, disc loss: 0.4649188052330286, policy loss: 1.2811946150063116
Experience 2, Iter 83, disc loss: 0.4605456846064437, policy loss: 1.2860542003076287
Experience 2, Iter 84, disc loss: 0.46330565991016337, policy loss: 1.2626527402568262
Experience 2, Iter 85, disc loss: 0.43802145652857916, policy loss: 1.334154755096848
Experience 2, Iter 86, disc loss: 0.42714328872044216, policy loss: 1.3588050341307343
Experience 2, Iter 87, disc loss: 0.4016070298515172, policy loss: 1.4222591318671869
Experience 2, Iter 88, disc loss: 0.4235269632923697, policy loss: 1.3573465187660685
Experience 2, Iter 89, disc loss: 0.4308701902334249, policy loss: 1.3421813659832071
Experience 2, Iter 90, disc loss: 0.42881799102393287, policy loss: 1.327087969516684
Experience 2, Iter 91, disc loss: 0.42766724515608867, policy loss: 1.337180850520904
Experience 2, Iter 92, disc loss: 0.420092924452774, policy loss: 1.3547571128466664
Experience 2, Iter 93, disc loss: 0.42664743998644805, policy loss: 1.3202133336376276
Experience 2, Iter 94, disc loss: 0.42903729150312014, policy loss: 1.336844161913323
Experience 2, Iter 95, disc loss: 0.42966574364025834, policy loss: 1.3059821665844868
Experience 2, Iter 96, disc loss: 0.4053375335846006, policy loss: 1.3924618859176965
Experience 2, Iter 97, disc loss: 0.38114891549602925, policy loss: 1.4646280595999839
Experience 2, Iter 98, disc loss: 0.37849203237685536, policy loss: 1.4472635407644598
Experience 2, Iter 99, disc loss: 0.36784228514367573, policy loss: 1.5009777764873917
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0175],
        [0.1011],
        [0.7249],
        [0.0129]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0905, 0.6420, 0.6081, 0.0339, 0.0046, 4.1235]],

        [[0.0905, 0.6420, 0.6081, 0.0339, 0.0046, 4.1235]],

        [[0.0905, 0.6420, 0.6081, 0.0339, 0.0046, 4.1235]],

        [[0.0905, 0.6420, 0.6081, 0.0339, 0.0046, 4.1235]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0701, 0.4046, 2.8995, 0.0518], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0701, 0.4046, 2.8995, 0.0518])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.939
Iter 2/2000 - Loss: 2.978
Iter 3/2000 - Loss: 2.879
Iter 4/2000 - Loss: 2.875
Iter 5/2000 - Loss: 2.894
Iter 6/2000 - Loss: 2.854
Iter 7/2000 - Loss: 2.813
Iter 8/2000 - Loss: 2.798
Iter 9/2000 - Loss: 2.779
Iter 10/2000 - Loss: 2.737
Iter 11/2000 - Loss: 2.685
Iter 12/2000 - Loss: 2.635
Iter 13/2000 - Loss: 2.586
Iter 14/2000 - Loss: 2.526
Iter 15/2000 - Loss: 2.450
Iter 16/2000 - Loss: 2.358
Iter 17/2000 - Loss: 2.256
Iter 18/2000 - Loss: 2.147
Iter 19/2000 - Loss: 2.031
Iter 20/2000 - Loss: 1.904
Iter 1981/2000 - Loss: -5.203
Iter 1982/2000 - Loss: -5.204
Iter 1983/2000 - Loss: -5.204
Iter 1984/2000 - Loss: -5.204
Iter 1985/2000 - Loss: -5.204
Iter 1986/2000 - Loss: -5.204
Iter 1987/2000 - Loss: -5.204
Iter 1988/2000 - Loss: -5.204
Iter 1989/2000 - Loss: -5.204
Iter 1990/2000 - Loss: -5.204
Iter 1991/2000 - Loss: -5.204
Iter 1992/2000 - Loss: -5.204
Iter 1993/2000 - Loss: -5.204
Iter 1994/2000 - Loss: -5.204
Iter 1995/2000 - Loss: -5.204
Iter 1996/2000 - Loss: -5.204
Iter 1997/2000 - Loss: -5.204
Iter 1998/2000 - Loss: -5.205
Iter 1999/2000 - Loss: -5.205
Iter 2000/2000 - Loss: -5.205
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0008],
        [0.0002]])
Lengthscale: tensor([[[25.8395, 10.9074, 44.3938, 12.6074, 17.3205, 69.6864]],

        [[34.8806, 62.4216, 19.5871,  1.5147,  1.2390, 18.9170]],

        [[25.5108, 49.8630, 13.4475,  0.7236,  8.8734, 18.8970]],

        [[35.9999, 56.4514, 11.2717,  2.1671,  7.1776, 24.2836]]])
Signal Variance: tensor([ 0.2665,  1.2879, 10.1829,  0.2310])
Estimated target variance: tensor([0.0701, 0.4046, 2.8995, 0.0518])
N: 30
Signal to noise ratio: tensor([ 25.3760,  50.4258, 113.9997,  36.2652])
Bound on condition number: tensor([ 19319.2451,  76283.9138, 389878.9096,  39456.0430])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.337367499423287, policy loss: 1.6167306205781122
Experience 3, Iter 1, disc loss: 0.30910777885088087, policy loss: 1.7331682558634514
Experience 3, Iter 2, disc loss: 0.3226149363600486, policy loss: 1.6490020182670082
Experience 3, Iter 3, disc loss: 0.31156294298567383, policy loss: 1.6810477159882418
Experience 3, Iter 4, disc loss: 0.30780274162631965, policy loss: 1.6918398233965752
Experience 3, Iter 5, disc loss: 0.3004435842587616, policy loss: 1.7117445115866117
Experience 3, Iter 6, disc loss: 0.29512831831619013, policy loss: 1.7388782684237358
Experience 3, Iter 7, disc loss: 0.28427773703660003, policy loss: 1.7843191306715216
Experience 3, Iter 8, disc loss: 0.29444297909384076, policy loss: 1.7222323889502489
Experience 3, Iter 9, disc loss: 0.28904102929483433, policy loss: 1.7806848673229498
Experience 3, Iter 10, disc loss: 0.2895025861216273, policy loss: 1.753881480287642
Experience 3, Iter 11, disc loss: 0.2966287124583943, policy loss: 1.7169717709016497
Experience 3, Iter 12, disc loss: 0.27528073724862084, policy loss: 1.7904470843313427
Experience 3, Iter 13, disc loss: 0.28300107872637237, policy loss: 1.7450202021918506
Experience 3, Iter 14, disc loss: 0.26994816553256623, policy loss: 1.8495593490837103
Experience 3, Iter 15, disc loss: 0.279614314934022, policy loss: 1.7653429448206526
Experience 3, Iter 16, disc loss: 0.2985647260592861, policy loss: 1.6867217484174115
Experience 3, Iter 17, disc loss: 0.2854206484357091, policy loss: 1.7389337983629003
Experience 3, Iter 18, disc loss: 0.27492262514941745, policy loss: 1.781270673803054
Experience 3, Iter 19, disc loss: 0.28426690515111763, policy loss: 1.7266445802273402
Experience 3, Iter 20, disc loss: 0.27873408284127205, policy loss: 1.7951907132820577
Experience 3, Iter 21, disc loss: 0.3002358347560046, policy loss: 1.648851393132953
Experience 3, Iter 22, disc loss: 0.2871097014368969, policy loss: 1.7122518352440559
Experience 3, Iter 23, disc loss: 0.2876573238980565, policy loss: 1.6845466994607985
Experience 3, Iter 24, disc loss: 0.27983955408727146, policy loss: 1.7265173519457142
Experience 3, Iter 25, disc loss: 0.2668372508450092, policy loss: 1.782678438306644
Experience 3, Iter 26, disc loss: 0.2686180242984903, policy loss: 1.7703363036759234
Experience 3, Iter 27, disc loss: 0.26492433952202793, policy loss: 1.8392678652342358
Experience 3, Iter 28, disc loss: 0.24092669265917302, policy loss: 1.9827874156039182
Experience 3, Iter 29, disc loss: 0.24165494054375908, policy loss: 1.9538905661684385
Experience 3, Iter 30, disc loss: 0.24827908508216953, policy loss: 1.9717923690305532
Experience 3, Iter 31, disc loss: 0.22577073195112288, policy loss: 2.1498172366111548
Experience 3, Iter 32, disc loss: 0.21626530428519608, policy loss: 2.230532376747542
Experience 3, Iter 33, disc loss: 0.21132123516056078, policy loss: 2.1881447501614604
Experience 3, Iter 34, disc loss: 0.2288250385093243, policy loss: 2.0112512890964513
Experience 3, Iter 35, disc loss: 0.22451166040851472, policy loss: 2.0172499298218254
Experience 3, Iter 36, disc loss: 0.22724224698643186, policy loss: 1.9658021596308828
Experience 3, Iter 37, disc loss: 0.21033256857144556, policy loss: 2.0622055062343962
Experience 3, Iter 38, disc loss: 0.18935243531887372, policy loss: 2.1849376966273835
Experience 3, Iter 39, disc loss: 0.18231021813670556, policy loss: 2.2403966176101777
Experience 3, Iter 40, disc loss: 0.18318054377158816, policy loss: 2.22556320975156
Experience 3, Iter 41, disc loss: 0.17865013185782097, policy loss: 2.254434732465162
Experience 3, Iter 42, disc loss: 0.174992082089382, policy loss: 2.27679880872446
Experience 3, Iter 43, disc loss: 0.17930751172604018, policy loss: 2.245562301839098
Experience 3, Iter 44, disc loss: 0.1753257334236966, policy loss: 2.24765560633695
Experience 3, Iter 45, disc loss: 0.182607240481327, policy loss: 2.1717098470270857
Experience 3, Iter 46, disc loss: 0.18890641251260964, policy loss: 2.1541181122500155
Experience 3, Iter 47, disc loss: 0.2058055426138724, policy loss: 2.016730359098212
Experience 3, Iter 48, disc loss: 0.20211312707206644, policy loss: 2.022788504677414
Experience 3, Iter 49, disc loss: 0.19728394617086586, policy loss: 2.0312630727603374
Experience 3, Iter 50, disc loss: 0.20408280380559482, policy loss: 1.9991382800542419
Experience 3, Iter 51, disc loss: 0.1949835632043456, policy loss: 2.0215289245706094
Experience 3, Iter 52, disc loss: 0.18967112068591613, policy loss: 2.04007651187173
Experience 3, Iter 53, disc loss: 0.18463173555646395, policy loss: 2.0717366705057727
Experience 3, Iter 54, disc loss: 0.19172124489260162, policy loss: 2.015046390863045
Experience 3, Iter 55, disc loss: 0.18469921255192948, policy loss: 2.052874756548176
Experience 3, Iter 56, disc loss: 0.1923386415069, policy loss: 2.005888818726959
Experience 3, Iter 57, disc loss: 0.1881564623949774, policy loss: 2.0289985039173093
Experience 3, Iter 58, disc loss: 0.18280298864464123, policy loss: 2.0710322635387435
Experience 3, Iter 59, disc loss: 0.1803935403106331, policy loss: 2.0725256976573685
Experience 3, Iter 60, disc loss: 0.18059745831407312, policy loss: 2.0463533744352964
Experience 3, Iter 61, disc loss: 0.18077127278000388, policy loss: 2.0685719193940164
Experience 3, Iter 62, disc loss: 0.1846653823256241, policy loss: 2.0171126926199925
Experience 3, Iter 63, disc loss: 0.18315649790450222, policy loss: 2.036073883659608
Experience 3, Iter 64, disc loss: 0.16652075007517395, policy loss: 2.1477035695497584
Experience 3, Iter 65, disc loss: 0.17029650637846291, policy loss: 2.1092217894356713
Experience 3, Iter 66, disc loss: 0.17563257414667566, policy loss: 2.1029150147103595
Experience 3, Iter 67, disc loss: 0.17368207698972168, policy loss: 2.0827803972951546
Experience 3, Iter 68, disc loss: 0.16934581126201942, policy loss: 2.0733895117727377
Experience 3, Iter 69, disc loss: 0.16521596415395085, policy loss: 2.141776491415363
Experience 3, Iter 70, disc loss: 0.15838824644082147, policy loss: 2.276479392468051
Experience 3, Iter 71, disc loss: 0.1698694677237092, policy loss: 2.181278359897501
Experience 3, Iter 72, disc loss: 0.14864022992576267, policy loss: 2.3569431678885704
Experience 3, Iter 73, disc loss: 0.15066346520616197, policy loss: 2.2879375985919452
Experience 3, Iter 74, disc loss: 0.16198177932205787, policy loss: 2.1864760113228745
Experience 3, Iter 75, disc loss: 0.15431459178149118, policy loss: 2.213719488076733
Experience 3, Iter 76, disc loss: 0.14998502848294135, policy loss: 2.267251497545365
Experience 3, Iter 77, disc loss: 0.12889145127494137, policy loss: 2.4927630105212186
Experience 3, Iter 78, disc loss: 0.12987833396727044, policy loss: 2.4359063134801
Experience 3, Iter 79, disc loss: 0.1457798314560539, policy loss: 2.288799605398492
Experience 3, Iter 80, disc loss: 0.14871944210137228, policy loss: 2.2255409695377697
Experience 3, Iter 81, disc loss: 0.12238203660074583, policy loss: 2.4509340216599087
Experience 3, Iter 82, disc loss: 0.1182950034471762, policy loss: 2.48248820553063
Experience 3, Iter 83, disc loss: 0.12105633690068918, policy loss: 2.45390690319316
Experience 3, Iter 84, disc loss: 0.1165914601586785, policy loss: 2.5020955910654537
Experience 3, Iter 85, disc loss: 0.1062049618687467, policy loss: 2.632595937448016
Experience 3, Iter 86, disc loss: 0.11101260266303989, policy loss: 2.569632180942569
Experience 3, Iter 87, disc loss: 0.11218344128806963, policy loss: 2.558320540648264
Experience 3, Iter 88, disc loss: 0.11364608803659847, policy loss: 2.51949295151638
Experience 3, Iter 89, disc loss: 0.12207233985659342, policy loss: 2.3875528155587022
Experience 3, Iter 90, disc loss: 0.11357560596562838, policy loss: 2.488801910645983
Experience 3, Iter 91, disc loss: 0.11417620921950855, policy loss: 2.4747357989733763
Experience 3, Iter 92, disc loss: 0.12059406646089367, policy loss: 2.40728540482134
Experience 3, Iter 93, disc loss: 0.10159611524623513, policy loss: 2.661629712013937
Experience 3, Iter 94, disc loss: 0.10602946202989243, policy loss: 2.5529188756410415
Experience 3, Iter 95, disc loss: 0.11207339274791164, policy loss: 2.487554223312217
Experience 3, Iter 96, disc loss: 0.09925035475785579, policy loss: 2.6175214943525726
Experience 3, Iter 97, disc loss: 0.09475240078012744, policy loss: 2.7128940336789773
Experience 3, Iter 98, disc loss: 0.08791206003702656, policy loss: 2.758490364491161
Experience 3, Iter 99, disc loss: 0.09104416368606288, policy loss: 2.723866661871642
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0131],
        [0.0769],
        [0.5505],
        [0.0101]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0673, 0.4776, 0.4653, 0.0254, 0.0041, 3.1105]],

        [[0.0673, 0.4776, 0.4653, 0.0254, 0.0041, 3.1105]],

        [[0.0673, 0.4776, 0.4653, 0.0254, 0.0041, 3.1105]],

        [[0.0673, 0.4776, 0.4653, 0.0254, 0.0041, 3.1105]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0522, 0.3075, 2.2018, 0.0406], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0522, 0.3075, 2.2018, 0.0406])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.349
Iter 2/2000 - Loss: 2.424
Iter 3/2000 - Loss: 2.273
Iter 4/2000 - Loss: 2.270
Iter 5/2000 - Loss: 2.290
Iter 6/2000 - Loss: 2.224
Iter 7/2000 - Loss: 2.149
Iter 8/2000 - Loss: 2.109
Iter 9/2000 - Loss: 2.074
Iter 10/2000 - Loss: 2.010
Iter 11/2000 - Loss: 1.921
Iter 12/2000 - Loss: 1.827
Iter 13/2000 - Loss: 1.736
Iter 14/2000 - Loss: 1.643
Iter 15/2000 - Loss: 1.536
Iter 16/2000 - Loss: 1.410
Iter 17/2000 - Loss: 1.265
Iter 18/2000 - Loss: 1.109
Iter 19/2000 - Loss: 0.944
Iter 20/2000 - Loss: 0.775
Iter 1981/2000 - Loss: -5.956
Iter 1982/2000 - Loss: -5.956
Iter 1983/2000 - Loss: -5.956
Iter 1984/2000 - Loss: -5.956
Iter 1985/2000 - Loss: -5.956
Iter 1986/2000 - Loss: -5.956
Iter 1987/2000 - Loss: -5.956
Iter 1988/2000 - Loss: -5.956
Iter 1989/2000 - Loss: -5.956
Iter 1990/2000 - Loss: -5.956
Iter 1991/2000 - Loss: -5.956
Iter 1992/2000 - Loss: -5.956
Iter 1993/2000 - Loss: -5.956
Iter 1994/2000 - Loss: -5.956
Iter 1995/2000 - Loss: -5.957
Iter 1996/2000 - Loss: -5.957
Iter 1997/2000 - Loss: -5.957
Iter 1998/2000 - Loss: -5.957
Iter 1999/2000 - Loss: -5.957
Iter 2000/2000 - Loss: -5.957
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[24.1790, 10.3504, 45.9029,  8.2959, 12.7654, 60.5682]],

        [[33.0472, 57.3629, 27.2086,  1.3136,  1.1313, 20.2256]],

        [[32.3524, 50.6214, 20.3773,  0.8236,  9.6946, 22.2585]],

        [[30.8638, 53.8517, 11.1308,  2.3210, 10.7500, 25.9474]]])
Signal Variance: tensor([ 0.2498,  1.7431, 15.8366,  0.2209])
Estimated target variance: tensor([0.0522, 0.3075, 2.2018, 0.0406])
N: 40
Signal to noise ratio: tensor([26.7906, 60.0112, 85.1102, 30.1252])
Bound on condition number: tensor([ 28710.5472, 144054.5379, 289750.7954,  36302.1831])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.09338476820011046, policy loss: 2.707319151997905
Experience 4, Iter 1, disc loss: 0.09650936572976106, policy loss: 2.6666642812963826
Experience 4, Iter 2, disc loss: 0.10036780847359042, policy loss: 2.6094796813055243
Experience 4, Iter 3, disc loss: 0.09555794950411553, policy loss: 2.6906702272795653
Experience 4, Iter 4, disc loss: 0.10376010828058689, policy loss: 2.562043887162902
Experience 4, Iter 5, disc loss: 0.09293739771652551, policy loss: 2.6816733258164778
Experience 4, Iter 6, disc loss: 0.10004156442686736, policy loss: 2.6067309514924046
Experience 4, Iter 7, disc loss: 0.10033238032192358, policy loss: 2.6177105081923573
Experience 4, Iter 8, disc loss: 0.09836206559675219, policy loss: 2.617204476861869
Experience 4, Iter 9, disc loss: 0.09775053284691768, policy loss: 2.644745549127239
Experience 4, Iter 10, disc loss: 0.10534860673004424, policy loss: 2.5403395479680113
Experience 4, Iter 11, disc loss: 0.09773910619064392, policy loss: 2.621881714785585
Experience 4, Iter 12, disc loss: 0.1095726510670599, policy loss: 2.5167213902544567
Experience 4, Iter 13, disc loss: 0.09859948227967945, policy loss: 2.6170952127032336
Experience 4, Iter 14, disc loss: 0.10831872512349558, policy loss: 2.536457136923386
Experience 4, Iter 15, disc loss: 0.09914388344478628, policy loss: 2.6349562929904073
Experience 4, Iter 16, disc loss: 0.10133389010859713, policy loss: 2.5989262079826374
Experience 4, Iter 17, disc loss: 0.10496291606998104, policy loss: 2.559053623954837
Experience 4, Iter 18, disc loss: 0.11176584757214877, policy loss: 2.490784093757568
Experience 4, Iter 19, disc loss: 0.09817585190727558, policy loss: 2.617078117752082
Experience 4, Iter 20, disc loss: 0.10176024482358706, policy loss: 2.6107262528755326
Experience 4, Iter 21, disc loss: 0.10045839294291752, policy loss: 2.56641389032694
Experience 4, Iter 22, disc loss: 0.09233444261800579, policy loss: 2.695749714363886
Experience 4, Iter 23, disc loss: 0.10319285683357866, policy loss: 2.5177404810199024
Experience 4, Iter 24, disc loss: 0.09998353125938, policy loss: 2.5674365715938627
Experience 4, Iter 25, disc loss: 0.0960590482084926, policy loss: 2.6219017563751157
Experience 4, Iter 26, disc loss: 0.10142247029042863, policy loss: 2.5843583073716845
Experience 4, Iter 27, disc loss: 0.09400645505106409, policy loss: 2.658610984209142
Experience 4, Iter 28, disc loss: 0.09761901233494585, policy loss: 2.631112124429404
Experience 4, Iter 29, disc loss: 0.09141147958311603, policy loss: 2.724479939399897
Experience 4, Iter 30, disc loss: 0.08752412262502106, policy loss: 2.746518392878074
Experience 4, Iter 31, disc loss: 0.0966474874691903, policy loss: 2.632818437116015
Experience 4, Iter 32, disc loss: 0.08313309344576479, policy loss: 2.8117157636860357
Experience 4, Iter 33, disc loss: 0.08292557541976545, policy loss: 2.827743284392824
Experience 4, Iter 34, disc loss: 0.08532627824132297, policy loss: 2.78841210520246
Experience 4, Iter 35, disc loss: 0.08675033219588373, policy loss: 2.7804030056511726
Experience 4, Iter 36, disc loss: 0.08956664183453183, policy loss: 2.709389429911995
Experience 4, Iter 37, disc loss: 0.07972677291608052, policy loss: 2.918672371176659
Experience 4, Iter 38, disc loss: 0.08319759383950882, policy loss: 2.8034723053379693
Experience 4, Iter 39, disc loss: 0.08513147090708066, policy loss: 2.7669225953862284
Experience 4, Iter 40, disc loss: 0.08103173580155949, policy loss: 2.8904364579975153
Experience 4, Iter 41, disc loss: 0.0780796124165054, policy loss: 2.922425315445028
Experience 4, Iter 42, disc loss: 0.08439470752704956, policy loss: 2.9160933000601306
Experience 4, Iter 43, disc loss: 0.06907641879256295, policy loss: 3.020208152608591
Experience 4, Iter 44, disc loss: 0.08682015616196884, policy loss: 2.8568197220869047
Experience 4, Iter 45, disc loss: 0.08100403579342828, policy loss: 2.885109611120463
Experience 4, Iter 46, disc loss: 0.09421571483959983, policy loss: 2.6525848056328245
Experience 4, Iter 47, disc loss: 0.08713529465507375, policy loss: 2.6799487200747896
Experience 4, Iter 48, disc loss: 0.10327137882002586, policy loss: 2.518221843895911
Experience 4, Iter 49, disc loss: 0.11068659493341161, policy loss: 2.4539426486599782
Experience 4, Iter 50, disc loss: 0.11487071855264774, policy loss: 2.393654112486865
Experience 4, Iter 51, disc loss: 0.11349787234884047, policy loss: 2.374025500790767
Experience 4, Iter 52, disc loss: 0.10728276903399647, policy loss: 2.422515443452379
Experience 4, Iter 53, disc loss: 0.1095144377769131, policy loss: 2.3959299845860365
Experience 4, Iter 54, disc loss: 0.1113320053126077, policy loss: 2.3819501570191326
Experience 4, Iter 55, disc loss: 0.11323395419190248, policy loss: 2.352816950009457
Experience 4, Iter 56, disc loss: 0.1127066293042322, policy loss: 2.350371829555704
Experience 4, Iter 57, disc loss: 0.11137069449089884, policy loss: 2.3594302920376298
Experience 4, Iter 58, disc loss: 0.11426431658935818, policy loss: 2.3302021537783197
Experience 4, Iter 59, disc loss: 0.11027675654803672, policy loss: 2.376390848454396
Experience 4, Iter 60, disc loss: 0.10765880003091227, policy loss: 2.39374212021857
Experience 4, Iter 61, disc loss: 0.11653108670359585, policy loss: 2.3128339690760282
Experience 4, Iter 62, disc loss: 0.11167167352592278, policy loss: 2.355336984239277
Experience 4, Iter 63, disc loss: 0.11027382097429003, policy loss: 2.3673979469163045
Experience 4, Iter 64, disc loss: 0.10781517950862092, policy loss: 2.4033563205223194
Experience 4, Iter 65, disc loss: 0.10544995762319616, policy loss: 2.4097024055364926
Experience 4, Iter 66, disc loss: 0.10952453113725646, policy loss: 2.3761467731607144
Experience 4, Iter 67, disc loss: 0.10334858117919381, policy loss: 2.435860312033915
Experience 4, Iter 68, disc loss: 0.10890052316227863, policy loss: 2.3908310260520285
Experience 4, Iter 69, disc loss: 0.10222203773364912, policy loss: 2.4565232290330314
Experience 4, Iter 70, disc loss: 0.10374863691070801, policy loss: 2.4358150072043854
Experience 4, Iter 71, disc loss: 0.10349340869161093, policy loss: 2.443979744179017
Experience 4, Iter 72, disc loss: 0.10534063994668222, policy loss: 2.4361038927491228
Experience 4, Iter 73, disc loss: 0.09350783760604807, policy loss: 2.540928233018939
Experience 4, Iter 74, disc loss: 0.10054131822472744, policy loss: 2.5285495395277575
Experience 4, Iter 75, disc loss: 0.09711408591288827, policy loss: 2.5118500774573995
Experience 4, Iter 76, disc loss: 0.09364532719523602, policy loss: 2.532688701587701
Experience 4, Iter 77, disc loss: 0.09552476657128875, policy loss: 2.5110584213586318
Experience 4, Iter 78, disc loss: 0.09348796306191202, policy loss: 2.527569785306232
Experience 4, Iter 79, disc loss: 0.08933757938302157, policy loss: 2.590586799126056
Experience 4, Iter 80, disc loss: 0.08771199494987622, policy loss: 2.5931882500366923
Experience 4, Iter 81, disc loss: 0.08678440723957194, policy loss: 2.6087191132524588
Experience 4, Iter 82, disc loss: 0.08141889625397598, policy loss: 2.6729933679845854
Experience 4, Iter 83, disc loss: 0.08539770696959986, policy loss: 2.694174548052411
Experience 4, Iter 84, disc loss: 0.08499755455711334, policy loss: 2.6277599432571677
Experience 4, Iter 85, disc loss: 0.0810653688248583, policy loss: 2.734591181283113
Experience 4, Iter 86, disc loss: 0.08664942589019099, policy loss: 2.6139231987633513
Experience 4, Iter 87, disc loss: 0.07958938096195363, policy loss: 2.709050680387673
Experience 4, Iter 88, disc loss: 0.08039760157572043, policy loss: 2.697687746102279
Experience 4, Iter 89, disc loss: 0.07829389891132958, policy loss: 2.708481056931527
Experience 4, Iter 90, disc loss: 0.08829070006920825, policy loss: 2.5981966473111675
Experience 4, Iter 91, disc loss: 0.08392226824425082, policy loss: 2.6757473928260778
Experience 4, Iter 92, disc loss: 0.08397156860921534, policy loss: 2.653322302869146
Experience 4, Iter 93, disc loss: 0.07473209723707659, policy loss: 2.7747723603814785
Experience 4, Iter 94, disc loss: 0.08101344867933202, policy loss: 2.696035113161967
Experience 4, Iter 95, disc loss: 0.08242856069932318, policy loss: 2.6844790077613947
Experience 4, Iter 96, disc loss: 0.07805881608909056, policy loss: 2.723459812389338
Experience 4, Iter 97, disc loss: 0.07653889945962473, policy loss: 2.7596442558785714
Experience 4, Iter 98, disc loss: 0.0827528170184759, policy loss: 2.6756663579197975
Experience 4, Iter 99, disc loss: 0.07794266219557126, policy loss: 2.834588923358419
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0104],
        [0.0654],
        [0.4840],
        [0.0082]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0537, 0.3811, 0.3798, 0.0203, 0.0036, 2.5410]],

        [[0.0537, 0.3811, 0.3798, 0.0203, 0.0036, 2.5410]],

        [[0.0537, 0.3811, 0.3798, 0.0203, 0.0036, 2.5410]],

        [[0.0537, 0.3811, 0.3798, 0.0203, 0.0036, 2.5410]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0416, 0.2618, 1.9361, 0.0329], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0416, 0.2618, 1.9361, 0.0329])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.928
Iter 2/2000 - Loss: 2.007
Iter 3/2000 - Loss: 1.829
Iter 4/2000 - Loss: 1.810
Iter 5/2000 - Loss: 1.813
Iter 6/2000 - Loss: 1.725
Iter 7/2000 - Loss: 1.623
Iter 8/2000 - Loss: 1.555
Iter 9/2000 - Loss: 1.497
Iter 10/2000 - Loss: 1.413
Iter 11/2000 - Loss: 1.302
Iter 12/2000 - Loss: 1.180
Iter 13/2000 - Loss: 1.059
Iter 14/2000 - Loss: 0.939
Iter 15/2000 - Loss: 0.811
Iter 16/2000 - Loss: 0.668
Iter 17/2000 - Loss: 0.508
Iter 18/2000 - Loss: 0.335
Iter 19/2000 - Loss: 0.153
Iter 20/2000 - Loss: -0.034
Iter 1981/2000 - Loss: -6.693
Iter 1982/2000 - Loss: -6.693
Iter 1983/2000 - Loss: -6.693
Iter 1984/2000 - Loss: -6.693
Iter 1985/2000 - Loss: -6.693
Iter 1986/2000 - Loss: -6.693
Iter 1987/2000 - Loss: -6.693
Iter 1988/2000 - Loss: -6.693
Iter 1989/2000 - Loss: -6.693
Iter 1990/2000 - Loss: -6.693
Iter 1991/2000 - Loss: -6.694
Iter 1992/2000 - Loss: -6.694
Iter 1993/2000 - Loss: -6.694
Iter 1994/2000 - Loss: -6.694
Iter 1995/2000 - Loss: -6.694
Iter 1996/2000 - Loss: -6.694
Iter 1997/2000 - Loss: -6.694
Iter 1998/2000 - Loss: -6.694
Iter 1999/2000 - Loss: -6.694
Iter 2000/2000 - Loss: -6.694
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[23.3873, 10.9525, 42.6596,  9.2914, 14.8125, 60.7826]],

        [[31.9731, 56.8472, 28.5600,  1.3421,  1.0667, 22.3967]],

        [[31.9354, 49.7950, 23.5932,  0.8580, 10.0481, 23.2292]],

        [[27.8284, 52.1164, 10.9058,  2.2547, 11.1151, 23.4464]]])
Signal Variance: tensor([ 0.2482,  1.9696, 18.8731,  0.2012])
Estimated target variance: tensor([0.0416, 0.2618, 1.9361, 0.0329])
N: 50
Signal to noise ratio: tensor([28.5486, 70.0372, 91.0495, 29.2697])
Bound on condition number: tensor([ 40751.9905, 245261.3359, 414501.7211,  42836.6338])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.07357455921236931, policy loss: 2.7953024786079963
Experience 5, Iter 1, disc loss: 0.07762702306694386, policy loss: 2.7650420745784237
Experience 5, Iter 2, disc loss: 0.07241985985383943, policy loss: 2.81441090911433
Experience 5, Iter 3, disc loss: 0.07990666653079617, policy loss: 2.76551869490742
Experience 5, Iter 4, disc loss: 0.0782094820828938, policy loss: 2.7341302705648642
Experience 5, Iter 5, disc loss: 0.07529732356692924, policy loss: 2.7806529437316727
Experience 5, Iter 6, disc loss: 0.08138020254791475, policy loss: 2.7011235907437596
Experience 5, Iter 7, disc loss: 0.08150616362517399, policy loss: 2.7135393037068245
Experience 5, Iter 8, disc loss: 0.0716630824645882, policy loss: 2.845665528632815
Experience 5, Iter 9, disc loss: 0.07175161038110382, policy loss: 2.8595457570630485
Experience 5, Iter 10, disc loss: 0.07452857846705829, policy loss: 2.8219833943905153
Experience 5, Iter 11, disc loss: 0.06956334009512422, policy loss: 2.8782506355348634
Experience 5, Iter 12, disc loss: 0.07523381041875365, policy loss: 2.7861306221963007
Experience 5, Iter 13, disc loss: 0.07027665870131684, policy loss: 2.8804845992301145
Experience 5, Iter 14, disc loss: 0.07737938236110942, policy loss: 2.777947483171669
Experience 5, Iter 15, disc loss: 0.07619524031473748, policy loss: 2.8021785616475716
Experience 5, Iter 16, disc loss: 0.07615007669085035, policy loss: 2.838570497774098
Experience 5, Iter 17, disc loss: 0.07837394065230643, policy loss: 2.797770387508826
Experience 5, Iter 18, disc loss: 0.07743624102299079, policy loss: 2.8333167945898854
Experience 5, Iter 19, disc loss: 0.07859762343219709, policy loss: 2.832708344248535
Experience 5, Iter 20, disc loss: 0.0880574956543573, policy loss: 2.710475945774118
Experience 5, Iter 21, disc loss: 0.09047424004820107, policy loss: 2.673724129192743
Experience 5, Iter 22, disc loss: 0.08734469648608517, policy loss: 2.739662936878811
Experience 5, Iter 23, disc loss: 0.10032137596128902, policy loss: 2.622906308243928
Experience 5, Iter 24, disc loss: 0.10062201620895944, policy loss: 2.5722048980401713
Experience 5, Iter 25, disc loss: 0.10327415990509584, policy loss: 2.567184099547229
Experience 5, Iter 26, disc loss: 0.1009020137813607, policy loss: 2.694244866812855
Experience 5, Iter 27, disc loss: 0.1108091486921628, policy loss: 2.5498470044593358
Experience 5, Iter 28, disc loss: 0.12389639519458685, policy loss: 2.419952452543053
Experience 5, Iter 29, disc loss: 0.1212470959216774, policy loss: 2.5145952832076226
Experience 5, Iter 30, disc loss: 0.128512981493597, policy loss: 2.4971350837364654
Experience 5, Iter 31, disc loss: 0.13115872955996594, policy loss: 2.462682945434426
Experience 5, Iter 32, disc loss: 0.14060331071623147, policy loss: 2.531596836864281
Experience 5, Iter 33, disc loss: 0.1368637551935224, policy loss: 2.493064597655156
Experience 5, Iter 34, disc loss: 0.13480154430833574, policy loss: 2.4408686533592965
Experience 5, Iter 35, disc loss: 0.15144919498331827, policy loss: 2.31163467012515
Experience 5, Iter 36, disc loss: 0.14294224814679668, policy loss: 2.468198600317018
Experience 5, Iter 37, disc loss: 0.13042640359915075, policy loss: 2.6276683510922267
Experience 5, Iter 38, disc loss: 0.14133243944417137, policy loss: 2.515123577046522
Experience 5, Iter 39, disc loss: 0.09997939436383334, policy loss: 2.9682039706954306
Experience 5, Iter 40, disc loss: 0.10192604083408296, policy loss: 2.9803129929051044
Experience 5, Iter 41, disc loss: 0.10282090857717571, policy loss: 2.9270341225548506
Experience 5, Iter 42, disc loss: 0.09960458287113215, policy loss: 3.119590394245771
Experience 5, Iter 43, disc loss: 0.09298108486538408, policy loss: 3.1720149319394055
Experience 5, Iter 44, disc loss: 0.09136524599144076, policy loss: 3.2599719980982247
Experience 5, Iter 45, disc loss: 0.08972204336386302, policy loss: 3.266195993862855
Experience 5, Iter 46, disc loss: 0.09123807393486622, policy loss: 3.155666720320294
Experience 5, Iter 47, disc loss: 0.0812857457729765, policy loss: 3.500552371446327
Experience 5, Iter 48, disc loss: 0.08198428653125694, policy loss: 3.500033694489469
Experience 5, Iter 49, disc loss: 0.08782547964050233, policy loss: 3.3149896042512172
Experience 5, Iter 50, disc loss: 0.07795137685770273, policy loss: 3.5968419836084715
Experience 5, Iter 51, disc loss: 0.07062612739667379, policy loss: 3.897227774810429
Experience 5, Iter 52, disc loss: 0.06702455632017226, policy loss: 3.832627739985411
Experience 5, Iter 53, disc loss: 0.06635003013400012, policy loss: 3.6938513667796142
Experience 5, Iter 54, disc loss: 0.06281629141832472, policy loss: 3.81109246196696
Experience 5, Iter 55, disc loss: 0.06332389019952946, policy loss: 3.7441384427150224
Experience 5, Iter 56, disc loss: 0.06788523811829278, policy loss: 3.5580425099133346
Experience 5, Iter 57, disc loss: 0.06124963331010655, policy loss: 3.6037879370529375
Experience 5, Iter 58, disc loss: 0.0583201743480309, policy loss: 3.878329590381977
Experience 5, Iter 59, disc loss: 0.05788972675143781, policy loss: 3.759775333500691
Experience 5, Iter 60, disc loss: 0.054739375382417195, policy loss: 3.7272806274199355
Experience 5, Iter 61, disc loss: 0.0594706156294635, policy loss: 3.5544513658088146
Experience 5, Iter 62, disc loss: 0.054304181274232394, policy loss: 3.790713568138007
Experience 5, Iter 63, disc loss: 0.04967027275429033, policy loss: 3.866742945670609
Experience 5, Iter 64, disc loss: 0.056863399459119474, policy loss: 3.5567616550251704
Experience 5, Iter 65, disc loss: 0.049643192558884365, policy loss: 3.9170752052667313
Experience 5, Iter 66, disc loss: 0.0521221275602327, policy loss: 3.762556173443376
Experience 5, Iter 67, disc loss: 0.04737882820039093, policy loss: 3.85854815050779
Experience 5, Iter 68, disc loss: 0.0588346305955308, policy loss: 3.4158001069125925
Experience 5, Iter 69, disc loss: 0.05371781893852787, policy loss: 3.624783569626858
Experience 5, Iter 70, disc loss: 0.04623446716936747, policy loss: 3.779183520713741
Experience 5, Iter 71, disc loss: 0.045066380150047415, policy loss: 3.8669004028661136
Experience 5, Iter 72, disc loss: 0.04116192505291181, policy loss: 3.978741215543636
Experience 5, Iter 73, disc loss: 0.046938116614833506, policy loss: 3.8177374364993693
Experience 5, Iter 74, disc loss: 0.03933001216514513, policy loss: 3.8975083751927357
Experience 5, Iter 75, disc loss: 0.039730360630668704, policy loss: 4.0244184127325635
Experience 5, Iter 76, disc loss: 0.03794716541293851, policy loss: 4.015787230774643
Experience 5, Iter 77, disc loss: 0.04148645716698081, policy loss: 3.8873153741190394
Experience 5, Iter 78, disc loss: 0.03882830450444641, policy loss: 3.98445313417224
Experience 5, Iter 79, disc loss: 0.04083777229615395, policy loss: 3.866947712012593
Experience 5, Iter 80, disc loss: 0.03365577534922998, policy loss: 4.426972199544217
Experience 5, Iter 81, disc loss: 0.03192459701179285, policy loss: 4.374849798184869
Experience 5, Iter 82, disc loss: 0.03422632480093006, policy loss: 4.321617790395737
Experience 5, Iter 83, disc loss: 0.03183465227731085, policy loss: 4.259783328182144
Experience 5, Iter 84, disc loss: 0.029219312781028624, policy loss: 4.295167873002648
Experience 5, Iter 85, disc loss: 0.029108888112571923, policy loss: 4.443989255584982
Experience 5, Iter 86, disc loss: 0.03186709822196197, policy loss: 4.389803578078361
Experience 5, Iter 87, disc loss: 0.025932130776903133, policy loss: 4.598999953675838
Experience 5, Iter 88, disc loss: 0.027704316403407998, policy loss: 4.482806642793371
Experience 5, Iter 89, disc loss: 0.026933414612107336, policy loss: 4.348991994308932
Experience 5, Iter 90, disc loss: 0.025133292162851345, policy loss: 4.453731413838669
Experience 5, Iter 91, disc loss: 0.022781256139490412, policy loss: 4.591509109244535
Experience 5, Iter 92, disc loss: 0.025409203793285373, policy loss: 4.397774670681203
Experience 5, Iter 93, disc loss: 0.024253603645496245, policy loss: 4.4391413304694485
Experience 5, Iter 94, disc loss: 0.02603798904523979, policy loss: 4.378979613453845
Experience 5, Iter 95, disc loss: 0.022467714453308724, policy loss: 4.854773496615399
Experience 5, Iter 96, disc loss: 0.02493838619771791, policy loss: 4.526512150847601
Experience 5, Iter 97, disc loss: 0.027496226926999698, policy loss: 4.29026220564417
Experience 5, Iter 98, disc loss: 0.023938537190855272, policy loss: 4.4614261778674145
Experience 5, Iter 99, disc loss: 0.023397383872918343, policy loss: 4.487618296572017
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0087],
        [0.0725],
        [0.5900],
        [0.0072]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0450, 0.3220, 0.3507, 0.0170, 0.0032, 2.4117]],

        [[0.0450, 0.3220, 0.3507, 0.0170, 0.0032, 2.4117]],

        [[0.0450, 0.3220, 0.3507, 0.0170, 0.0032, 2.4117]],

        [[0.0450, 0.3220, 0.3507, 0.0170, 0.0032, 2.4117]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0350, 0.2898, 2.3600, 0.0287], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0350, 0.2898, 2.3600, 0.0287])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.983
Iter 2/2000 - Loss: 2.086
Iter 3/2000 - Loss: 1.900
Iter 4/2000 - Loss: 1.889
Iter 5/2000 - Loss: 1.907
Iter 6/2000 - Loss: 1.824
Iter 7/2000 - Loss: 1.730
Iter 8/2000 - Loss: 1.679
Iter 9/2000 - Loss: 1.639
Iter 10/2000 - Loss: 1.563
Iter 11/2000 - Loss: 1.455
Iter 12/2000 - Loss: 1.336
Iter 13/2000 - Loss: 1.220
Iter 14/2000 - Loss: 1.102
Iter 15/2000 - Loss: 0.971
Iter 16/2000 - Loss: 0.820
Iter 17/2000 - Loss: 0.649
Iter 18/2000 - Loss: 0.463
Iter 19/2000 - Loss: 0.269
Iter 20/2000 - Loss: 0.070
Iter 1981/2000 - Loss: -7.001
Iter 1982/2000 - Loss: -7.001
Iter 1983/2000 - Loss: -7.002
Iter 1984/2000 - Loss: -7.002
Iter 1985/2000 - Loss: -7.002
Iter 1986/2000 - Loss: -7.002
Iter 1987/2000 - Loss: -7.002
Iter 1988/2000 - Loss: -7.002
Iter 1989/2000 - Loss: -7.002
Iter 1990/2000 - Loss: -7.002
Iter 1991/2000 - Loss: -7.002
Iter 1992/2000 - Loss: -7.002
Iter 1993/2000 - Loss: -7.002
Iter 1994/2000 - Loss: -7.002
Iter 1995/2000 - Loss: -7.002
Iter 1996/2000 - Loss: -7.002
Iter 1997/2000 - Loss: -7.002
Iter 1998/2000 - Loss: -7.002
Iter 1999/2000 - Loss: -7.003
Iter 2000/2000 - Loss: -7.003
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[21.7362, 11.3249, 40.8451, 10.1310, 15.9573, 56.0148]],

        [[32.0174, 55.4160, 28.9251,  1.3689,  1.1288, 26.2577]],

        [[30.6900, 47.2559, 21.7288,  0.9394, 10.6262, 22.5008]],

        [[28.5992, 49.0361, 12.4853,  2.5715, 11.8972, 23.0775]]])
Signal Variance: tensor([ 0.2273,  2.1966, 21.1675,  0.2342])
Estimated target variance: tensor([0.0350, 0.2898, 2.3600, 0.0287])
N: 60
Signal to noise ratio: tensor([26.3283, 69.8772, 97.2567, 32.7343])
Bound on condition number: tensor([ 41591.8043, 292970.3887, 567532.7505,  64293.1670])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.034936225213686035, policy loss: 4.069968198169745
Experience 6, Iter 1, disc loss: 0.044900543404672484, policy loss: 3.8658219207162903
Experience 6, Iter 2, disc loss: 0.04127269066021004, policy loss: 4.300876426790555
Experience 6, Iter 3, disc loss: 0.044708470568960795, policy loss: 3.745446952288341
Experience 6, Iter 4, disc loss: 0.03802243281583556, policy loss: 4.346024839205373
Experience 6, Iter 5, disc loss: 0.04664016214254865, policy loss: 3.5893214141217045
Experience 6, Iter 6, disc loss: 0.033072815450559394, policy loss: 4.522163949289206
Experience 6, Iter 7, disc loss: 0.034221634496475375, policy loss: 4.278452134309209
Experience 6, Iter 8, disc loss: 0.03780789747472746, policy loss: 3.956222953652576
Experience 6, Iter 9, disc loss: 0.028153025809095605, policy loss: 4.405627220578601
Experience 6, Iter 10, disc loss: 0.029712031120480216, policy loss: 4.1814027061899015
Experience 6, Iter 11, disc loss: 0.03443150133670241, policy loss: 4.0711140698812756
Experience 6, Iter 12, disc loss: 0.036781526016424745, policy loss: 4.053193306725811
Experience 6, Iter 13, disc loss: 0.0285364515127852, policy loss: 4.372254070965814
Experience 6, Iter 14, disc loss: 0.028675349702650368, policy loss: 4.311743100289915
Experience 6, Iter 15, disc loss: 0.03063902367699673, policy loss: 4.276710744950924
Experience 6, Iter 16, disc loss: 0.028357102297547142, policy loss: 4.363502763831443
Experience 6, Iter 17, disc loss: 0.029088682546312757, policy loss: 4.3913743190020265
Experience 6, Iter 18, disc loss: 0.029586085806195733, policy loss: 4.385048788706639
Experience 6, Iter 19, disc loss: 0.02496627780280175, policy loss: 4.665165262493533
Experience 6, Iter 20, disc loss: 0.036181105881611476, policy loss: 3.9419319597993847
Experience 6, Iter 21, disc loss: 0.028909362542384315, policy loss: 4.435333852105208
Experience 6, Iter 22, disc loss: 0.025050808352412142, policy loss: 4.496741995593654
Experience 6, Iter 23, disc loss: 0.025011821226434514, policy loss: 4.505347406502338
Experience 6, Iter 24, disc loss: 0.025895234857401077, policy loss: 4.740890049734203
Experience 6, Iter 25, disc loss: 0.019718401581364557, policy loss: 4.936461465212279
Experience 6, Iter 26, disc loss: 0.015472765044051127, policy loss: 5.257261187126106
Experience 6, Iter 27, disc loss: 0.01651404037231532, policy loss: 5.316040593757384
Experience 6, Iter 28, disc loss: 0.027863010698678643, policy loss: 4.504484994028486
Experience 6, Iter 29, disc loss: 0.019015038271125675, policy loss: 5.0725290455013035
Experience 6, Iter 30, disc loss: 0.016535551391304776, policy loss: 5.038301367431437
Experience 6, Iter 31, disc loss: 0.01649274735699651, policy loss: 4.992447707609329
Experience 6, Iter 32, disc loss: 0.018758204349777586, policy loss: 4.8938013751788
Experience 6, Iter 33, disc loss: 0.017730768313119562, policy loss: 4.954319713117311
Experience 6, Iter 34, disc loss: 0.022058944731525414, policy loss: 4.528771664166147
Experience 6, Iter 35, disc loss: 0.01857391375718787, policy loss: 5.003658958235295
Experience 6, Iter 36, disc loss: 0.022285310350629722, policy loss: 4.666156764969825
Experience 6, Iter 37, disc loss: 0.0222650877178759, policy loss: 4.651831940472242
Experience 6, Iter 38, disc loss: 0.01914770559982519, policy loss: 4.881416453493846
Experience 6, Iter 39, disc loss: 0.02582169211494881, policy loss: 4.417133132276705
Experience 6, Iter 40, disc loss: 0.029155527212019895, policy loss: 4.344184036156554
Experience 6, Iter 41, disc loss: 0.02601155279159423, policy loss: 4.545502036260807
Experience 6, Iter 42, disc loss: 0.014816677273892846, policy loss: 5.22024739371129
Experience 6, Iter 43, disc loss: 0.017681581604206256, policy loss: 4.860089747576479
Experience 6, Iter 44, disc loss: 0.02380962244420471, policy loss: 4.484070647033582
Experience 6, Iter 45, disc loss: 0.022544351215001715, policy loss: 4.673470129864921
Experience 6, Iter 46, disc loss: 0.02103754819751867, policy loss: 4.7246387209245775
Experience 6, Iter 47, disc loss: 0.01415689796511857, policy loss: 5.117629945283662
Experience 6, Iter 48, disc loss: 0.015609033309201097, policy loss: 5.038210151579931
Experience 6, Iter 49, disc loss: 0.015089499386348912, policy loss: 5.1674195967204986
Experience 6, Iter 50, disc loss: 0.016710838378790477, policy loss: 4.994342597565721
Experience 6, Iter 51, disc loss: 0.01854639136062492, policy loss: 4.940063128202089
Experience 6, Iter 52, disc loss: 0.018586153494355176, policy loss: 4.9211817831146405
Experience 6, Iter 53, disc loss: 0.018878700536755062, policy loss: 4.948629819408822
Experience 6, Iter 54, disc loss: 0.0195403856676726, policy loss: 4.911574814392532
Experience 6, Iter 55, disc loss: 0.024372018944264162, policy loss: 4.691466783113292
Experience 6, Iter 56, disc loss: 0.02185051271451883, policy loss: 4.668837741403227
Experience 6, Iter 57, disc loss: 0.019123883144763765, policy loss: 5.04051404253706
Experience 6, Iter 58, disc loss: 0.017531918377559994, policy loss: 5.04582106183248
Experience 6, Iter 59, disc loss: 0.015795838223941736, policy loss: 5.209705505312085
Experience 6, Iter 60, disc loss: 0.02112958068111946, policy loss: 4.882372966356438
Experience 6, Iter 61, disc loss: 0.017262818772373876, policy loss: 5.014981662037608
Experience 6, Iter 62, disc loss: 0.015387979558069706, policy loss: 5.229783282900365
Experience 6, Iter 63, disc loss: 0.020988937205628628, policy loss: 4.58111715413372
Experience 6, Iter 64, disc loss: 0.014125523633483584, policy loss: 5.53263341269327
Experience 6, Iter 65, disc loss: 0.012882838827856614, policy loss: 5.479798414038263
Experience 6, Iter 66, disc loss: 0.011135148302173124, policy loss: 5.870544660618501
Experience 6, Iter 67, disc loss: 0.014652540126487628, policy loss: 5.139657286440087
Experience 6, Iter 68, disc loss: 0.013469829190278646, policy loss: 5.21409243698011
Experience 6, Iter 69, disc loss: 0.010395567038614096, policy loss: 5.492448558078528
Experience 6, Iter 70, disc loss: 0.009412427002425438, policy loss: 5.710477845275186
Experience 6, Iter 71, disc loss: 0.009835710308494497, policy loss: 5.457187620700863
Experience 6, Iter 72, disc loss: 0.008744791102686288, policy loss: 5.74783307255646
Experience 6, Iter 73, disc loss: 0.007806932847502439, policy loss: 6.044965906552105
Experience 6, Iter 74, disc loss: 0.007459029032007168, policy loss: 6.0240934061899996
Experience 6, Iter 75, disc loss: 0.0069456785592775395, policy loss: 6.136067713883053
Experience 6, Iter 76, disc loss: 0.0072679194802048475, policy loss: 5.981790415612361
Experience 6, Iter 77, disc loss: 0.007853407291761615, policy loss: 5.8175554606111
Experience 6, Iter 78, disc loss: 0.008222794538882864, policy loss: 5.739618225521672
Experience 6, Iter 79, disc loss: 0.009128156989873543, policy loss: 5.54313203543837
Experience 6, Iter 80, disc loss: 0.010967195411925226, policy loss: 5.387063584689153
Experience 6, Iter 81, disc loss: 0.01314387743686206, policy loss: 5.1506858801314
Experience 6, Iter 82, disc loss: 0.0144160015855446, policy loss: 4.898919677531747
Experience 6, Iter 83, disc loss: 0.012589085024642444, policy loss: 5.112633007976021
Experience 6, Iter 84, disc loss: 0.014041165203843856, policy loss: 4.916290928504146
Experience 6, Iter 85, disc loss: 0.012381300501463981, policy loss: 5.094567298149071
Experience 6, Iter 86, disc loss: 0.014755915494110781, policy loss: 5.064518514901646
Experience 6, Iter 87, disc loss: 0.01638658163941181, policy loss: 5.036747421234268
Experience 6, Iter 88, disc loss: 0.013612594944530262, policy loss: 5.173542630418634
Experience 6, Iter 89, disc loss: 0.015585356284253631, policy loss: 4.949991237961378
Experience 6, Iter 90, disc loss: 0.014208992806371551, policy loss: 4.885800442098226
Experience 6, Iter 91, disc loss: 0.013656726472659083, policy loss: 5.340185547830025
Experience 6, Iter 92, disc loss: 0.012278161118128478, policy loss: 5.233619825489258
Experience 6, Iter 93, disc loss: 0.011804266279124925, policy loss: 5.587340515602934
Experience 6, Iter 94, disc loss: 0.013723508226217235, policy loss: 5.092348866569465
Experience 6, Iter 95, disc loss: 0.014569814290360856, policy loss: 5.12636263586331
Experience 6, Iter 96, disc loss: 0.015275961657223685, policy loss: 5.221132408119542
Experience 6, Iter 97, disc loss: 0.015426426245306723, policy loss: 4.975444345693074
Experience 6, Iter 98, disc loss: 0.01075204533024888, policy loss: 5.586305068046066
Experience 6, Iter 99, disc loss: 0.015220150783115683, policy loss: 5.237949654463059
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0075],
        [0.0837],
        [0.7333],
        [0.0067]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0386, 0.2795, 0.3480, 0.0148, 0.0028, 2.4170]],

        [[0.0386, 0.2795, 0.3480, 0.0148, 0.0028, 2.4170]],

        [[0.0386, 0.2795, 0.3480, 0.0148, 0.0028, 2.4170]],

        [[0.0386, 0.2795, 0.3480, 0.0148, 0.0028, 2.4170]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0301, 0.3349, 2.9332, 0.0269], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0301, 0.3349, 2.9332, 0.0269])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.071
Iter 2/2000 - Loss: 2.198
Iter 3/2000 - Loss: 1.990
Iter 4/2000 - Loss: 1.981
Iter 5/2000 - Loss: 2.002
Iter 6/2000 - Loss: 1.909
Iter 7/2000 - Loss: 1.801
Iter 8/2000 - Loss: 1.741
Iter 9/2000 - Loss: 1.694
Iter 10/2000 - Loss: 1.610
Iter 11/2000 - Loss: 1.489
Iter 12/2000 - Loss: 1.354
Iter 13/2000 - Loss: 1.223
Iter 14/2000 - Loss: 1.091
Iter 15/2000 - Loss: 0.949
Iter 16/2000 - Loss: 0.787
Iter 17/2000 - Loss: 0.605
Iter 18/2000 - Loss: 0.407
Iter 19/2000 - Loss: 0.202
Iter 20/2000 - Loss: -0.008
Iter 1981/2000 - Loss: -7.257
Iter 1982/2000 - Loss: -7.257
Iter 1983/2000 - Loss: -7.257
Iter 1984/2000 - Loss: -7.257
Iter 1985/2000 - Loss: -7.257
Iter 1986/2000 - Loss: -7.257
Iter 1987/2000 - Loss: -7.257
Iter 1988/2000 - Loss: -7.257
Iter 1989/2000 - Loss: -7.257
Iter 1990/2000 - Loss: -7.257
Iter 1991/2000 - Loss: -7.257
Iter 1992/2000 - Loss: -7.257
Iter 1993/2000 - Loss: -7.257
Iter 1994/2000 - Loss: -7.257
Iter 1995/2000 - Loss: -7.257
Iter 1996/2000 - Loss: -7.257
Iter 1997/2000 - Loss: -7.257
Iter 1998/2000 - Loss: -7.258
Iter 1999/2000 - Loss: -7.258
Iter 2000/2000 - Loss: -7.258
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[19.8736, 11.6932, 40.9564, 11.3923, 16.1122, 57.6799]],

        [[29.4698, 52.1259, 31.0763,  1.4192,  1.0790, 26.5108]],

        [[26.1845, 42.7987, 21.1653,  0.9520, 10.4751, 24.6188]],

        [[25.9652, 48.6475, 12.7281,  2.5188, 11.1909, 24.6219]]])
Signal Variance: tensor([ 0.2174,  2.2301, 22.7015,  0.2312])
Estimated target variance: tensor([0.0301, 0.3349, 2.9332, 0.0269])
N: 70
Signal to noise ratio: tensor([ 25.1701,  72.8677, 105.7028,  30.6578])
Bound on condition number: tensor([ 44348.4277, 371680.3940, 782117.1088,  65793.9203])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.011927020521757724, policy loss: 5.1445903604623595
Experience 7, Iter 1, disc loss: 0.015788009074789534, policy loss: 4.817148381826091
Experience 7, Iter 2, disc loss: 0.01408208130833489, policy loss: 4.901684587389726
Experience 7, Iter 3, disc loss: 0.013664067646477285, policy loss: 4.897477811293703
Experience 7, Iter 4, disc loss: 0.01321618220390242, policy loss: 5.104009288502156
Experience 7, Iter 5, disc loss: 0.012739767009852736, policy loss: 5.204491682753264
Experience 7, Iter 6, disc loss: 0.014111043806881364, policy loss: 5.100743757639255
Experience 7, Iter 7, disc loss: 0.012179418311378414, policy loss: 5.069281085749773
Experience 7, Iter 8, disc loss: 0.013461301258002191, policy loss: 5.125731528545442
Experience 7, Iter 9, disc loss: 0.013220352719977362, policy loss: 5.208695063773245
Experience 7, Iter 10, disc loss: 0.01234460020074835, policy loss: 5.293446002685396
Experience 7, Iter 11, disc loss: 0.01572771265536727, policy loss: 4.948365619074314
Experience 7, Iter 12, disc loss: 0.013032264829563475, policy loss: 5.094859179230447
Experience 7, Iter 13, disc loss: 0.013702687611026207, policy loss: 5.201111439672341
Experience 7, Iter 14, disc loss: 0.010734567682577315, policy loss: 5.477440748535143
Experience 7, Iter 15, disc loss: 0.013216776002421804, policy loss: 4.9863823946449966
Experience 7, Iter 16, disc loss: 0.013611619958677894, policy loss: 5.2687701683908745
Experience 7, Iter 17, disc loss: 0.011586341274059306, policy loss: 5.231565571482914
Experience 7, Iter 18, disc loss: 0.012374582008651151, policy loss: 5.109083735349574
Experience 7, Iter 19, disc loss: 0.013292989323199281, policy loss: 5.067823555087557
Experience 7, Iter 20, disc loss: 0.01077991021075501, policy loss: 5.336073241086192
Experience 7, Iter 21, disc loss: 0.011122431729901808, policy loss: 5.257268755656176
Experience 7, Iter 22, disc loss: 0.014142154449999292, policy loss: 4.964178900467706
Experience 7, Iter 23, disc loss: 0.013216278779073963, policy loss: 5.001542086200853
Experience 7, Iter 24, disc loss: 0.009170080625797505, policy loss: 5.593650252160993
Experience 7, Iter 25, disc loss: 0.009461843672719648, policy loss: 5.518121684419306
Experience 7, Iter 26, disc loss: 0.008400609026208278, policy loss: 5.699156718639921
Experience 7, Iter 27, disc loss: 0.008982890320648886, policy loss: 5.448941027411135
Experience 7, Iter 28, disc loss: 0.010595732615288407, policy loss: 5.471686925872252
Experience 7, Iter 29, disc loss: 0.014139336359703662, policy loss: 5.106524177672793
Experience 7, Iter 30, disc loss: 0.012002825782310678, policy loss: 5.2086205849951295
Experience 7, Iter 31, disc loss: 0.009460980057184773, policy loss: 5.506736049203714
Experience 7, Iter 32, disc loss: 0.0098795054142795, policy loss: 5.259838797623315
Experience 7, Iter 33, disc loss: 0.008161593629541844, policy loss: 5.662830146845561
Experience 7, Iter 34, disc loss: 0.008345037716392268, policy loss: 5.658288089312184
Experience 7, Iter 35, disc loss: 0.008940116970151223, policy loss: 5.62557884283089
Experience 7, Iter 36, disc loss: 0.008157588763978119, policy loss: 5.669277093615641
Experience 7, Iter 37, disc loss: 0.010160246049063045, policy loss: 5.4173294247051444
Experience 7, Iter 38, disc loss: 0.011407896678332466, policy loss: 5.362457636516968
Experience 7, Iter 39, disc loss: 0.009191063566194587, policy loss: 5.611433969543962
Experience 7, Iter 40, disc loss: 0.009300937007395538, policy loss: 5.409700005186222
Experience 7, Iter 41, disc loss: 0.008127757791243168, policy loss: 5.628669975905509
Experience 7, Iter 42, disc loss: 0.007931364824828056, policy loss: 5.514056774881915
Experience 7, Iter 43, disc loss: 0.008896173081359017, policy loss: 5.44460879638015
Experience 7, Iter 44, disc loss: 0.008691639059594903, policy loss: 5.7141110729960936
Experience 7, Iter 45, disc loss: 0.009320981562115966, policy loss: 5.465071138574922
Experience 7, Iter 46, disc loss: 0.010186895564292147, policy loss: 5.61696466077649
Experience 7, Iter 47, disc loss: 0.008978389630642836, policy loss: 5.439377858491953
Experience 7, Iter 48, disc loss: 0.011166698001733399, policy loss: 5.135209403050322
Experience 7, Iter 49, disc loss: 0.009875446903099192, policy loss: 5.593535333880733
Experience 7, Iter 50, disc loss: 0.00816702996823591, policy loss: 5.71276373369997
Experience 7, Iter 51, disc loss: 0.010378696243356685, policy loss: 5.4210728569965765
Experience 7, Iter 52, disc loss: 0.009344900244843992, policy loss: 5.538927627466362
Experience 7, Iter 53, disc loss: 0.012623673655963591, policy loss: 5.1062773497354685
Experience 7, Iter 54, disc loss: 0.010919716153237129, policy loss: 5.467714914420966
Experience 7, Iter 55, disc loss: 0.010691759144641374, policy loss: 5.37359522091819
Experience 7, Iter 56, disc loss: 0.009762465448900155, policy loss: 5.409242556486628
Experience 7, Iter 57, disc loss: 0.0101361574494958, policy loss: 5.251529550356474
Experience 7, Iter 58, disc loss: 0.010410607757664461, policy loss: 5.409038622052342
Experience 7, Iter 59, disc loss: 0.010440481121289123, policy loss: 5.245728632784548
Experience 7, Iter 60, disc loss: 0.009643770700506721, policy loss: 5.51829904365608
Experience 7, Iter 61, disc loss: 0.012022963748383828, policy loss: 5.313727984603658
Experience 7, Iter 62, disc loss: 0.0077138359348503235, policy loss: 5.75279481356637
Experience 7, Iter 63, disc loss: 0.007616195168042093, policy loss: 5.654366745110428
Experience 7, Iter 64, disc loss: 0.010596900727283812, policy loss: 5.467124103771722
Experience 7, Iter 65, disc loss: 0.008955639177196795, policy loss: 5.601961145792044
Experience 7, Iter 66, disc loss: 0.010406808562083697, policy loss: 5.4438981794212555
Experience 7, Iter 67, disc loss: 0.010764592948088202, policy loss: 5.280945386210406
Experience 7, Iter 68, disc loss: 0.008507975171032483, policy loss: 5.560050303768826
Experience 7, Iter 69, disc loss: 0.007945719678785124, policy loss: 5.574037864752118
Experience 7, Iter 70, disc loss: 0.00783277441210471, policy loss: 5.667475113893444
Experience 7, Iter 71, disc loss: 0.009245080342380575, policy loss: 5.395499048808693
Experience 7, Iter 72, disc loss: 0.00765248266827216, policy loss: 5.542791888507234
Experience 7, Iter 73, disc loss: 0.008698690741936332, policy loss: 5.4274007825802375
Experience 7, Iter 74, disc loss: 0.009051396115014241, policy loss: 5.6173292476704235
Experience 7, Iter 75, disc loss: 0.008863684276518388, policy loss: 5.461388161570334
Experience 7, Iter 76, disc loss: 0.009766689937199216, policy loss: 5.337027571571616
Experience 7, Iter 77, disc loss: 0.008432708035392948, policy loss: 5.611745346652871
Experience 7, Iter 78, disc loss: 0.006875445570276863, policy loss: 5.9032773688820335
Experience 7, Iter 79, disc loss: 0.004757907218539269, policy loss: 6.303823764967818
Experience 7, Iter 80, disc loss: 0.00514911077526256, policy loss: 6.0993668656261
Experience 7, Iter 81, disc loss: 0.004876179345958249, policy loss: 6.181789800999931
Experience 7, Iter 82, disc loss: 0.005009578653550164, policy loss: 6.201827422388939
Experience 7, Iter 83, disc loss: 0.006497324960230597, policy loss: 5.780018892761918
Experience 7, Iter 84, disc loss: 0.006574910480281759, policy loss: 5.861897553697945
Experience 7, Iter 85, disc loss: 0.0089249910120798, policy loss: 5.542870867017344
Experience 7, Iter 86, disc loss: 0.006485070757558905, policy loss: 5.772953439813772
Experience 7, Iter 87, disc loss: 0.005967063951164194, policy loss: 5.825327641799834
Experience 7, Iter 88, disc loss: 0.005781171800187668, policy loss: 5.884424744145403
Experience 7, Iter 89, disc loss: 0.007369053684953714, policy loss: 5.565765179461783
Experience 7, Iter 90, disc loss: 0.0066762268505412255, policy loss: 5.783714164398041
Experience 7, Iter 91, disc loss: 0.007740185788799066, policy loss: 5.647674272330692
Experience 7, Iter 92, disc loss: 0.007366166969872012, policy loss: 5.764966339482067
Experience 7, Iter 93, disc loss: 0.007570036088812239, policy loss: 5.606632690559128
Experience 7, Iter 94, disc loss: 0.0053678083529741065, policy loss: 5.984857460396061
Experience 7, Iter 95, disc loss: 0.006602331811265964, policy loss: 5.804723039993757
Experience 7, Iter 96, disc loss: 0.007820045811396076, policy loss: 5.673083056394417
Experience 7, Iter 97, disc loss: 0.006558160613133183, policy loss: 5.692234844956861
Experience 7, Iter 98, disc loss: 0.007125727645015205, policy loss: 5.6051124365255856
Experience 7, Iter 99, disc loss: 0.004891007717459874, policy loss: 6.15021392537894
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0066],
        [0.1025],
        [0.9753],
        [0.0073]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.3845e-02, 2.5164e-01, 4.0016e-01, 1.3434e-02, 2.5163e-03,
          2.5430e+00]],

        [[3.3845e-02, 2.5164e-01, 4.0016e-01, 1.3434e-02, 2.5163e-03,
          2.5430e+00]],

        [[3.3845e-02, 2.5164e-01, 4.0016e-01, 1.3434e-02, 2.5163e-03,
          2.5430e+00]],

        [[3.3845e-02, 2.5164e-01, 4.0016e-01, 1.3434e-02, 2.5163e-03,
          2.5430e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0266, 0.4098, 3.9013, 0.0291], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0266, 0.4098, 3.9013, 0.0291])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.311
Iter 2/2000 - Loss: 2.464
Iter 3/2000 - Loss: 2.246
Iter 4/2000 - Loss: 2.249
Iter 5/2000 - Loss: 2.285
Iter 6/2000 - Loss: 2.198
Iter 7/2000 - Loss: 2.096
Iter 8/2000 - Loss: 2.048
Iter 9/2000 - Loss: 2.018
Iter 10/2000 - Loss: 1.951
Iter 11/2000 - Loss: 1.843
Iter 12/2000 - Loss: 1.721
Iter 13/2000 - Loss: 1.601
Iter 14/2000 - Loss: 1.483
Iter 15/2000 - Loss: 1.351
Iter 16/2000 - Loss: 1.197
Iter 17/2000 - Loss: 1.018
Iter 18/2000 - Loss: 0.822
Iter 19/2000 - Loss: 0.614
Iter 20/2000 - Loss: 0.398
Iter 1981/2000 - Loss: -7.482
Iter 1982/2000 - Loss: -7.482
Iter 1983/2000 - Loss: -7.482
Iter 1984/2000 - Loss: -7.482
Iter 1985/2000 - Loss: -7.482
Iter 1986/2000 - Loss: -7.483
Iter 1987/2000 - Loss: -7.483
Iter 1988/2000 - Loss: -7.483
Iter 1989/2000 - Loss: -7.483
Iter 1990/2000 - Loss: -7.483
Iter 1991/2000 - Loss: -7.483
Iter 1992/2000 - Loss: -7.483
Iter 1993/2000 - Loss: -7.483
Iter 1994/2000 - Loss: -7.483
Iter 1995/2000 - Loss: -7.483
Iter 1996/2000 - Loss: -7.483
Iter 1997/2000 - Loss: -7.483
Iter 1998/2000 - Loss: -7.483
Iter 1999/2000 - Loss: -7.483
Iter 2000/2000 - Loss: -7.483
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[19.7403, 12.0761, 41.8571, 14.9994, 16.5666, 57.1869]],

        [[26.5617, 47.7172, 26.4372,  1.6379,  1.0544, 16.2911]],

        [[26.8160, 42.4353, 17.6184,  1.0452,  9.6435, 23.4290]],

        [[24.8794, 48.8004, 15.4059,  2.8975,  7.6119, 32.9567]]])
Signal Variance: tensor([ 0.2126,  1.6749, 23.0966,  0.3188])
Estimated target variance: tensor([0.0266, 0.4098, 3.9013, 0.0291])
N: 80
Signal to noise ratio: tensor([ 25.5198,  67.3921, 108.6785,  36.8733])
Bound on condition number: tensor([ 52101.9898, 363336.6284, 944882.7952, 108772.2680])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.004286962537391538, policy loss: 6.376804556086942
Experience 8, Iter 1, disc loss: 0.005768623975654702, policy loss: 5.920177985348639
Experience 8, Iter 2, disc loss: 0.005053977647565981, policy loss: 5.997538699954095
Experience 8, Iter 3, disc loss: 0.005354453288226065, policy loss: 5.895387867746399
Experience 8, Iter 4, disc loss: 0.0044586587886594154, policy loss: 6.301598189749381
Experience 8, Iter 5, disc loss: 0.005431432424581403, policy loss: 6.040870246861797
Experience 8, Iter 6, disc loss: 0.0045848270430951625, policy loss: 6.26220323419093
Experience 8, Iter 7, disc loss: 0.005126515873468498, policy loss: 6.086570165008178
Experience 8, Iter 8, disc loss: 0.005384505365084625, policy loss: 6.16219543330836
Experience 8, Iter 9, disc loss: 0.005847480192362486, policy loss: 6.180834529742751
Experience 8, Iter 10, disc loss: 0.004593973448052102, policy loss: 6.1983724097363835
Experience 8, Iter 11, disc loss: 0.005445274609906778, policy loss: 5.884101441283793
Experience 8, Iter 12, disc loss: 0.005040337816190558, policy loss: 5.984394045761667
Experience 8, Iter 13, disc loss: 0.004864429442485253, policy loss: 6.109188837287939
Experience 8, Iter 14, disc loss: 0.004864181384692638, policy loss: 6.12039643085598
Experience 8, Iter 15, disc loss: 0.006241145794990886, policy loss: 5.826589892784604
Experience 8, Iter 16, disc loss: 0.006260385397232997, policy loss: 5.767705121014321
Experience 8, Iter 17, disc loss: 0.005424064955163193, policy loss: 5.974995895601147
Experience 8, Iter 18, disc loss: 0.005905200422982713, policy loss: 5.886738264081162
Experience 8, Iter 19, disc loss: 0.005133985132981099, policy loss: 6.09891380588956
Experience 8, Iter 20, disc loss: 0.004313845800270547, policy loss: 6.317055241779087
Experience 8, Iter 21, disc loss: 0.004666390191017096, policy loss: 6.047517760802326
Experience 8, Iter 22, disc loss: 0.005041258232109145, policy loss: 6.047303282286446
Experience 8, Iter 23, disc loss: 0.004819994823352655, policy loss: 6.19218746474052
Experience 8, Iter 24, disc loss: 0.005931977294687703, policy loss: 5.808901832981688
Experience 8, Iter 25, disc loss: 0.005637808734368124, policy loss: 5.869762867667638
Experience 8, Iter 26, disc loss: 0.006279823929761345, policy loss: 5.686131352896436
Experience 8, Iter 27, disc loss: 0.006416294082645605, policy loss: 5.734890706857899
Experience 8, Iter 28, disc loss: 0.005790471696247748, policy loss: 5.842589269751841
Experience 8, Iter 29, disc loss: 0.005065575307973237, policy loss: 6.0359125264627
Experience 8, Iter 30, disc loss: 0.0047944871815357, policy loss: 6.085377770134616
Experience 8, Iter 31, disc loss: 0.006557043273921865, policy loss: 5.838570926404773
Experience 8, Iter 32, disc loss: 0.00699314547265383, policy loss: 5.632617430487657
Experience 8, Iter 33, disc loss: 0.005592963056981727, policy loss: 6.059844552029051
Experience 8, Iter 34, disc loss: 0.00606324480447741, policy loss: 5.883164341169724
Experience 8, Iter 35, disc loss: 0.0050893976818504735, policy loss: 5.988049237500116
Experience 8, Iter 36, disc loss: 0.005888467365105165, policy loss: 5.819368055527755
Experience 8, Iter 37, disc loss: 0.006524754574860368, policy loss: 5.764982444221554
Experience 8, Iter 38, disc loss: 0.006450757895754795, policy loss: 5.73090403323207
Experience 8, Iter 39, disc loss: 0.00625583669261272, policy loss: 5.857155713774943
Experience 8, Iter 40, disc loss: 0.006171313054706385, policy loss: 5.798183278122406
Experience 8, Iter 41, disc loss: 0.005200115588085992, policy loss: 5.992496906925061
Experience 8, Iter 42, disc loss: 0.00564041092194989, policy loss: 5.92717172931707
Experience 8, Iter 43, disc loss: 0.006741052366918115, policy loss: 5.658562179189879
Experience 8, Iter 44, disc loss: 0.006436132609024776, policy loss: 5.737268091067992
Experience 8, Iter 45, disc loss: 0.005140516362461304, policy loss: 6.035067507433798
Experience 8, Iter 46, disc loss: 0.005741658748874906, policy loss: 6.0534502102206025
Experience 8, Iter 47, disc loss: 0.006217765385623918, policy loss: 5.858944775777529
Experience 8, Iter 48, disc loss: 0.006233520789440982, policy loss: 5.877885372055303
Experience 8, Iter 49, disc loss: 0.005196850242437199, policy loss: 6.024213887084489
Experience 8, Iter 50, disc loss: 0.0046849807023483245, policy loss: 6.309384250781958
Experience 8, Iter 51, disc loss: 0.005797410601968476, policy loss: 6.114125072785188
Experience 8, Iter 52, disc loss: 0.007100802876077063, policy loss: 5.81815431267194
Experience 8, Iter 53, disc loss: 0.006062644097805447, policy loss: 5.840251439185451
Experience 8, Iter 54, disc loss: 0.005115558561252307, policy loss: 6.000368229834035
Experience 8, Iter 55, disc loss: 0.0048765991282163435, policy loss: 5.9921870828905375
Experience 8, Iter 56, disc loss: 0.004874140192265765, policy loss: 5.942057607274262
Experience 8, Iter 57, disc loss: 0.004921634835638175, policy loss: 5.994643702063308
Experience 8, Iter 58, disc loss: 0.005909090070285927, policy loss: 6.099650606427955
Experience 8, Iter 59, disc loss: 0.004961534093352028, policy loss: 6.052022269326324
Experience 8, Iter 60, disc loss: 0.005382139523914508, policy loss: 6.156687467237816
Experience 8, Iter 61, disc loss: 0.006333210278518372, policy loss: 5.671143661539041
Experience 8, Iter 62, disc loss: 0.005430703520650058, policy loss: 5.923004200395664
Experience 8, Iter 63, disc loss: 0.005107669207499395, policy loss: 5.86463977091061
Experience 8, Iter 64, disc loss: 0.005153580365132226, policy loss: 6.141376195589474
Experience 8, Iter 65, disc loss: 0.004383108776300012, policy loss: 6.2229010248125025
Experience 8, Iter 66, disc loss: 0.0050817379099461065, policy loss: 5.954941537706994
Experience 8, Iter 67, disc loss: 0.0055712281264303646, policy loss: 5.972102480724415
Experience 8, Iter 68, disc loss: 0.0062443016981942, policy loss: 5.886780159380804
Experience 8, Iter 69, disc loss: 0.007471987654240545, policy loss: 5.574809377225791
Experience 8, Iter 70, disc loss: 0.005630663467818413, policy loss: 5.976286971004695
Experience 8, Iter 71, disc loss: 0.004879910738502492, policy loss: 6.06347164916434
Experience 8, Iter 72, disc loss: 0.005104378946336639, policy loss: 6.022390014906639
Experience 8, Iter 73, disc loss: 0.005223358418331936, policy loss: 6.099651086642302
Experience 8, Iter 74, disc loss: 0.0039653594972873666, policy loss: 6.4107932710878925
Experience 8, Iter 75, disc loss: 0.0039004403953765655, policy loss: 6.341803240488734
Experience 8, Iter 76, disc loss: 0.0061955373565356145, policy loss: 5.732593018898909
Experience 8, Iter 77, disc loss: 0.005512575676761135, policy loss: 5.967558224208123
Experience 8, Iter 78, disc loss: 0.005120269358535094, policy loss: 6.076561953519195
Experience 8, Iter 79, disc loss: 0.00475653878199791, policy loss: 6.235497200520246
Experience 8, Iter 80, disc loss: 0.0048539324282507925, policy loss: 6.064241173304024
Experience 8, Iter 81, disc loss: 0.004906794590155642, policy loss: 6.19980230814383
Experience 8, Iter 82, disc loss: 0.004966737134695295, policy loss: 5.902058054839509
Experience 8, Iter 83, disc loss: 0.004694870910510649, policy loss: 6.162779303918211
Experience 8, Iter 84, disc loss: 0.004275576310498689, policy loss: 6.203947728837164
Experience 8, Iter 85, disc loss: 0.004921424293757168, policy loss: 6.260614445337807
Experience 8, Iter 86, disc loss: 0.005019658566449686, policy loss: 6.121062543853255
Experience 8, Iter 87, disc loss: 0.004564670931564287, policy loss: 6.342089283701152
Experience 8, Iter 88, disc loss: 0.003524034695273265, policy loss: 6.471522380572367
Experience 8, Iter 89, disc loss: 0.003964456645475466, policy loss: 6.3916410358662725
Experience 8, Iter 90, disc loss: 0.003956769557221278, policy loss: 6.235210043773461
Experience 8, Iter 91, disc loss: 0.004775192295748597, policy loss: 6.0189921344037085
Experience 8, Iter 92, disc loss: 0.004616026280873343, policy loss: 6.097831639326812
Experience 8, Iter 93, disc loss: 0.0047668754181837015, policy loss: 6.100606373481726
Experience 8, Iter 94, disc loss: 0.0045001744253016325, policy loss: 6.115161198740107
Experience 8, Iter 95, disc loss: 0.00392392961132492, policy loss: 6.658601511146377
Experience 8, Iter 96, disc loss: 0.004645963052340292, policy loss: 6.1597146400656335
Experience 8, Iter 97, disc loss: 0.0036955223883211796, policy loss: 6.482182645303965
Experience 8, Iter 98, disc loss: 0.004760050276083606, policy loss: 6.060054056598263
Experience 8, Iter 99, disc loss: 0.005613070951501769, policy loss: 5.842690239470516
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.0983],
        [0.9458],
        [0.0067]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0108e-02, 2.2484e-01, 3.7265e-01, 1.2021e-02, 2.2886e-03,
          2.3781e+00]],

        [[3.0108e-02, 2.2484e-01, 3.7265e-01, 1.2021e-02, 2.2886e-03,
          2.3781e+00]],

        [[3.0108e-02, 2.2484e-01, 3.7265e-01, 1.2021e-02, 2.2886e-03,
          2.3781e+00]],

        [[3.0108e-02, 2.2484e-01, 3.7265e-01, 1.2021e-02, 2.2886e-03,
          2.3781e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0237, 0.3932, 3.7832, 0.0267], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0237, 0.3932, 3.7832, 0.0267])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.190
Iter 2/2000 - Loss: 2.379
Iter 3/2000 - Loss: 2.131
Iter 4/2000 - Loss: 2.143
Iter 5/2000 - Loss: 2.192
Iter 6/2000 - Loss: 2.101
Iter 7/2000 - Loss: 1.991
Iter 8/2000 - Loss: 1.939
Iter 9/2000 - Loss: 1.915
Iter 10/2000 - Loss: 1.854
Iter 11/2000 - Loss: 1.744
Iter 12/2000 - Loss: 1.615
Iter 13/2000 - Loss: 1.487
Iter 14/2000 - Loss: 1.362
Iter 15/2000 - Loss: 1.225
Iter 16/2000 - Loss: 1.064
Iter 17/2000 - Loss: 0.876
Iter 18/2000 - Loss: 0.667
Iter 19/2000 - Loss: 0.444
Iter 20/2000 - Loss: 0.212
Iter 1981/2000 - Loss: -7.752
Iter 1982/2000 - Loss: -7.752
Iter 1983/2000 - Loss: -7.753
Iter 1984/2000 - Loss: -7.753
Iter 1985/2000 - Loss: -7.753
Iter 1986/2000 - Loss: -7.753
Iter 1987/2000 - Loss: -7.753
Iter 1988/2000 - Loss: -7.753
Iter 1989/2000 - Loss: -7.753
Iter 1990/2000 - Loss: -7.753
Iter 1991/2000 - Loss: -7.753
Iter 1992/2000 - Loss: -7.753
Iter 1993/2000 - Loss: -7.753
Iter 1994/2000 - Loss: -7.753
Iter 1995/2000 - Loss: -7.754
Iter 1996/2000 - Loss: -7.754
Iter 1997/2000 - Loss: -7.754
Iter 1998/2000 - Loss: -7.754
Iter 1999/2000 - Loss: -7.754
Iter 2000/2000 - Loss: -7.754
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[19.1755, 11.5714, 40.0629, 13.1350, 16.2413, 55.1341]],

        [[25.5179, 45.8436, 27.1049,  1.4545,  1.4693, 28.3288]],

        [[25.7330, 39.9545, 17.3206,  1.0357,  9.3470, 23.4966]],

        [[23.3143, 46.4077, 16.3004,  3.0492,  6.5795, 35.0785]]])
Signal Variance: tensor([ 0.1984,  3.1010, 22.7507,  0.3548])
Estimated target variance: tensor([0.0237, 0.3932, 3.7832, 0.0267])
N: 90
Signal to noise ratio: tensor([ 24.5128,  89.6176, 114.5954,  40.3330])
Bound on condition number: tensor([  54080.1339,  722819.5769, 1181890.4888,  146408.3245])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.004770999225234851, policy loss: 6.21876560728655
Experience 9, Iter 1, disc loss: 0.003976447328421484, policy loss: 6.332056630784372
Experience 9, Iter 2, disc loss: 0.0036813953195087264, policy loss: 6.478512454965427
Experience 9, Iter 3, disc loss: 0.004890002942322882, policy loss: 6.105928490418927
Experience 9, Iter 4, disc loss: 0.004473689686812942, policy loss: 6.437455479114025
Experience 9, Iter 5, disc loss: 0.005170237852974301, policy loss: 6.057651306637131
Experience 9, Iter 6, disc loss: 0.00405329936095685, policy loss: 6.278101922782874
Experience 9, Iter 7, disc loss: 0.004160106913984181, policy loss: 6.210972094784334
Experience 9, Iter 8, disc loss: 0.004311939662207534, policy loss: 6.190838345050056
Experience 9, Iter 9, disc loss: 0.005684835875639085, policy loss: 5.905554495633954
Experience 9, Iter 10, disc loss: 0.0041967217572174975, policy loss: 6.378703824091055
Experience 9, Iter 11, disc loss: 0.0041277267357979555, policy loss: 6.221642060438043
Experience 9, Iter 12, disc loss: 0.005778678883400401, policy loss: 6.012481211942899
Experience 9, Iter 13, disc loss: 0.0049487161066652414, policy loss: 6.030383193994531
Experience 9, Iter 14, disc loss: 0.0041335491130169285, policy loss: 6.280629325921776
Experience 9, Iter 15, disc loss: 0.004041204028467602, policy loss: 6.176802692049604
Experience 9, Iter 16, disc loss: 0.003936993307666159, policy loss: 6.207750433158447
Experience 9, Iter 17, disc loss: 0.004926598879857101, policy loss: 5.963543560856997
Experience 9, Iter 18, disc loss: 0.005329954470447859, policy loss: 6.095323399387667
Experience 9, Iter 19, disc loss: 0.004257192309549933, policy loss: 6.185617443618525
Experience 9, Iter 20, disc loss: 0.004500800021860471, policy loss: 6.235490661559396
Experience 9, Iter 21, disc loss: 0.004482290260375739, policy loss: 6.16535235290356
Experience 9, Iter 22, disc loss: 0.005052210482042572, policy loss: 6.040643428532737
Experience 9, Iter 23, disc loss: 0.004967700531478071, policy loss: 6.138211390418331
Experience 9, Iter 24, disc loss: 0.004968875332312121, policy loss: 6.086066118198422
Experience 9, Iter 25, disc loss: 0.004911690172017297, policy loss: 6.110366013352456
Experience 9, Iter 26, disc loss: 0.005483493794755363, policy loss: 6.140685673624814
Experience 9, Iter 27, disc loss: 0.005525505483484957, policy loss: 6.125682027954054
Experience 9, Iter 28, disc loss: 0.005251449435267145, policy loss: 5.908796239512216
Experience 9, Iter 29, disc loss: 0.004536991714620467, policy loss: 6.261266197665841
Experience 9, Iter 30, disc loss: 0.00523235075354796, policy loss: 6.132953432042906
Experience 9, Iter 31, disc loss: 0.0047620048778923285, policy loss: 6.075706766144474
Experience 9, Iter 32, disc loss: 0.0039448159175785435, policy loss: 6.260306298101173
Experience 9, Iter 33, disc loss: 0.0045584087039956854, policy loss: 6.112789981222626
Experience 9, Iter 34, disc loss: 0.004725829234831267, policy loss: 6.285517492148571
Experience 9, Iter 35, disc loss: 0.003812502565763493, policy loss: 6.466695529497047
Experience 9, Iter 36, disc loss: 0.004271178264038253, policy loss: 6.26594199719767
Experience 9, Iter 37, disc loss: 0.004386636408745068, policy loss: 6.181414562537762
Experience 9, Iter 38, disc loss: 0.00444022017409813, policy loss: 6.238438548890105
Experience 9, Iter 39, disc loss: 0.003821298045411331, policy loss: 6.6076812389750925
Experience 9, Iter 40, disc loss: 0.004044769599743313, policy loss: 6.2482313857899126
Experience 9, Iter 41, disc loss: 0.004720937569985777, policy loss: 6.076046973013887
Experience 9, Iter 42, disc loss: 0.004847790302197828, policy loss: 6.211879282369522
Experience 9, Iter 43, disc loss: 0.0048324280493901774, policy loss: 6.092357114877037
Experience 9, Iter 44, disc loss: 0.005067265881121439, policy loss: 6.018284473406773
Experience 9, Iter 45, disc loss: 0.004678768067552668, policy loss: 6.2304609353499565
Experience 9, Iter 46, disc loss: 0.003880117383573003, policy loss: 6.2675137786759585
Experience 9, Iter 47, disc loss: 0.004730211593268886, policy loss: 6.294891821428165
Experience 9, Iter 48, disc loss: 0.004478789328456715, policy loss: 6.33314306097979
Experience 9, Iter 49, disc loss: 0.004503285175239985, policy loss: 6.048383625246678
Experience 9, Iter 50, disc loss: 0.004180411634975913, policy loss: 6.326907511208706
Experience 9, Iter 51, disc loss: 0.0038462318579923853, policy loss: 6.314126551792332
Experience 9, Iter 52, disc loss: 0.004487013465727033, policy loss: 6.458450957828013
Experience 9, Iter 53, disc loss: 0.004335171370010554, policy loss: 6.2733897919473405
Experience 9, Iter 54, disc loss: 0.0037904752918138476, policy loss: 6.501080707393947
Experience 9, Iter 55, disc loss: 0.004483406595293048, policy loss: 6.376944847438299
Experience 9, Iter 56, disc loss: 0.0043354445981906835, policy loss: 6.281615038382354
Experience 9, Iter 57, disc loss: 0.0045688077432990285, policy loss: 6.084639751106251
Experience 9, Iter 58, disc loss: 0.003068831585764543, policy loss: 6.635053124928074
Experience 9, Iter 59, disc loss: 0.0038870902392108275, policy loss: 6.39900608502332
Experience 9, Iter 60, disc loss: 0.0038443692680427133, policy loss: 6.792623851573988
Experience 9, Iter 61, disc loss: 0.0036451958117627247, policy loss: 6.5773086678971495
Experience 9, Iter 62, disc loss: 0.0040809914663257, policy loss: 6.545883051346406
Experience 9, Iter 63, disc loss: 0.0035553489337858107, policy loss: 6.407279862226057
Experience 9, Iter 64, disc loss: 0.004061244871578331, policy loss: 6.294264076370196
Experience 9, Iter 65, disc loss: 0.00350732036212931, policy loss: 6.453512374476609
Experience 9, Iter 66, disc loss: 0.00355562101297165, policy loss: 6.371796350570961
Experience 9, Iter 67, disc loss: 0.0033564186954338857, policy loss: 6.560082287002765
Experience 9, Iter 68, disc loss: 0.003782623956208442, policy loss: 6.44958551051897
Experience 9, Iter 69, disc loss: 0.0030181487842175893, policy loss: 6.543002113874201
Experience 9, Iter 70, disc loss: 0.0035227962844213297, policy loss: 6.472099579832601
Experience 9, Iter 71, disc loss: 0.0034455798883789206, policy loss: 6.450438728127967
Experience 9, Iter 72, disc loss: 0.0035683939748381785, policy loss: 6.535771051159171
Experience 9, Iter 73, disc loss: 0.004425235824814548, policy loss: 6.099858645006536
Experience 9, Iter 74, disc loss: 0.003110421052636741, policy loss: 6.558808280828709
Experience 9, Iter 75, disc loss: 0.004144157466364397, policy loss: 6.282159868166164
Experience 9, Iter 76, disc loss: 0.0036041993006423276, policy loss: 6.591128846386397
Experience 9, Iter 77, disc loss: 0.003334939269891242, policy loss: 6.551416128998701
Experience 9, Iter 78, disc loss: 0.0034728273630937325, policy loss: 6.503508175832475
Experience 9, Iter 79, disc loss: 0.004260937487071556, policy loss: 6.328521416485056
Experience 9, Iter 80, disc loss: 0.003633883037759594, policy loss: 6.429261747046332
Experience 9, Iter 81, disc loss: 0.0037652288361305656, policy loss: 6.460839077213784
Experience 9, Iter 82, disc loss: 0.004463546879822731, policy loss: 6.225116547628375
Experience 9, Iter 83, disc loss: 0.003597725451836612, policy loss: 6.428096156777282
Experience 9, Iter 84, disc loss: 0.002968312995708526, policy loss: 6.791030326158684
Experience 9, Iter 85, disc loss: 0.003859407209511055, policy loss: 6.400140092559544
Experience 9, Iter 86, disc loss: 0.0034451968093653115, policy loss: 6.396103097876381
Experience 9, Iter 87, disc loss: 0.003157275103148352, policy loss: 6.60586276299861
Experience 9, Iter 88, disc loss: 0.0033521337124101354, policy loss: 6.600759859599312
Experience 9, Iter 89, disc loss: 0.004584650985566022, policy loss: 6.312805534742585
Experience 9, Iter 90, disc loss: 0.003949014008224025, policy loss: 6.339370758740643
Experience 9, Iter 91, disc loss: 0.0029631188969130007, policy loss: 6.838642614622943
Experience 9, Iter 92, disc loss: 0.0029501253957380197, policy loss: 6.704174287069433
Experience 9, Iter 93, disc loss: 0.0037473303455970077, policy loss: 6.5468186356320635
Experience 9, Iter 94, disc loss: 0.00440173526066872, policy loss: 6.360822361999503
Experience 9, Iter 95, disc loss: 0.003892389136094938, policy loss: 6.444857807336806
Experience 9, Iter 96, disc loss: 0.003273153171089147, policy loss: 6.556820318670027
Experience 9, Iter 97, disc loss: 0.004401205055211829, policy loss: 6.282305974760101
Experience 9, Iter 98, disc loss: 0.0038619656856500413, policy loss: 6.544975436622591
Experience 9, Iter 99, disc loss: 0.0034245147960289294, policy loss: 6.489065994187662
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.1000],
        [0.9869],
        [0.0066]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.7177e-02, 2.0482e-01, 3.7283e-01, 1.1100e-02, 2.0887e-03,
          2.3182e+00]],

        [[2.7177e-02, 2.0482e-01, 3.7283e-01, 1.1100e-02, 2.0887e-03,
          2.3182e+00]],

        [[2.7177e-02, 2.0482e-01, 3.7283e-01, 1.1100e-02, 2.0887e-03,
          2.3182e+00]],

        [[2.7177e-02, 2.0482e-01, 3.7283e-01, 1.1100e-02, 2.0887e-03,
          2.3182e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0214, 0.4000, 3.9476, 0.0264], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0214, 0.4000, 3.9476, 0.0264])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.172
Iter 2/2000 - Loss: 2.381
Iter 3/2000 - Loss: 2.116
Iter 4/2000 - Loss: 2.129
Iter 5/2000 - Loss: 2.185
Iter 6/2000 - Loss: 2.092
Iter 7/2000 - Loss: 1.976
Iter 8/2000 - Loss: 1.923
Iter 9/2000 - Loss: 1.902
Iter 10/2000 - Loss: 1.842
Iter 11/2000 - Loss: 1.730
Iter 12/2000 - Loss: 1.596
Iter 13/2000 - Loss: 1.466
Iter 14/2000 - Loss: 1.340
Iter 15/2000 - Loss: 1.205
Iter 16/2000 - Loss: 1.044
Iter 17/2000 - Loss: 0.854
Iter 18/2000 - Loss: 0.642
Iter 19/2000 - Loss: 0.415
Iter 20/2000 - Loss: 0.179
Iter 1981/2000 - Loss: -7.962
Iter 1982/2000 - Loss: -7.962
Iter 1983/2000 - Loss: -7.962
Iter 1984/2000 - Loss: -7.962
Iter 1985/2000 - Loss: -7.962
Iter 1986/2000 - Loss: -7.962
Iter 1987/2000 - Loss: -7.962
Iter 1988/2000 - Loss: -7.962
Iter 1989/2000 - Loss: -7.962
Iter 1990/2000 - Loss: -7.962
Iter 1991/2000 - Loss: -7.962
Iter 1992/2000 - Loss: -7.963
Iter 1993/2000 - Loss: -7.963
Iter 1994/2000 - Loss: -7.963
Iter 1995/2000 - Loss: -7.963
Iter 1996/2000 - Loss: -7.963
Iter 1997/2000 - Loss: -7.963
Iter 1998/2000 - Loss: -7.963
Iter 1999/2000 - Loss: -7.963
Iter 2000/2000 - Loss: -7.963
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[18.5117, 11.4568, 39.4310, 12.0692, 15.5372, 53.2592]],

        [[25.5441, 47.1138, 12.1061,  1.4899,  2.3578, 19.6455]],

        [[25.7240, 44.5884, 16.4496,  0.9807,  8.3501, 22.3253]],

        [[23.5765, 45.1505, 17.0928,  3.6209,  2.7864, 37.0500]]])
Signal Variance: tensor([ 0.1914,  2.2247, 21.0110,  0.3823])
Estimated target variance: tensor([0.0214, 0.4000, 3.9476, 0.0264])
N: 100
Signal to noise ratio: tensor([ 24.7381,  81.9488, 111.9145,  44.3335])
Bound on condition number: tensor([  61198.3475,  671560.9067, 1252485.4314,  196546.8995])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.0031311012707765313, policy loss: 6.630155846901774
Experience 10, Iter 1, disc loss: 0.004498020950657768, policy loss: 6.274461458882581
Experience 10, Iter 2, disc loss: 0.004042628616794266, policy loss: 6.344619766747031
Experience 10, Iter 3, disc loss: 0.004063775392080398, policy loss: 6.322154286514661
Experience 10, Iter 4, disc loss: 0.003617265807646835, policy loss: 6.399033960264656
Experience 10, Iter 5, disc loss: 0.0035306199481727114, policy loss: 6.480230004845065
Experience 10, Iter 6, disc loss: 0.003786736571719895, policy loss: 6.513111450561627
Experience 10, Iter 7, disc loss: 0.004047244606804613, policy loss: 6.500381234197205
Experience 10, Iter 8, disc loss: 0.0033855786690423573, policy loss: 6.485196836969383
Experience 10, Iter 9, disc loss: 0.0038560175754493945, policy loss: 6.236134422704365
Experience 10, Iter 10, disc loss: 0.003199090208832414, policy loss: 6.636564021364167
Experience 10, Iter 11, disc loss: 0.0035149690124809526, policy loss: 6.377054327887619
Experience 10, Iter 12, disc loss: 0.003568551867430902, policy loss: 6.599495943762964
Experience 10, Iter 13, disc loss: 0.0035157025403157976, policy loss: 6.578813545443892
Experience 10, Iter 14, disc loss: 0.003275726916731105, policy loss: 6.4703093754632635
Experience 10, Iter 15, disc loss: 0.0027161777456816503, policy loss: 6.857242633197103
Experience 10, Iter 16, disc loss: 0.0029038139078080387, policy loss: 6.656494884821594
Experience 10, Iter 17, disc loss: 0.003229078725387217, policy loss: 6.511499535698252
Experience 10, Iter 18, disc loss: 0.0032505882711123288, policy loss: 6.682162474457222
Experience 10, Iter 19, disc loss: 0.0030077226826531675, policy loss: 6.655825884064286
Experience 10, Iter 20, disc loss: 0.003763192524262228, policy loss: 6.603368529371942
Experience 10, Iter 21, disc loss: 0.002901881714750492, policy loss: 6.704686357440817
Experience 10, Iter 22, disc loss: 0.0033402485460489297, policy loss: 6.556668729140185
Experience 10, Iter 23, disc loss: 0.0032551170795942184, policy loss: 6.625069326142759
Experience 10, Iter 24, disc loss: 0.0026821607244170353, policy loss: 6.726339010718048
Experience 10, Iter 25, disc loss: 0.0029090608706078026, policy loss: 6.713318760920896
Experience 10, Iter 26, disc loss: 0.003085904936309217, policy loss: 6.6341214817901015
Experience 10, Iter 27, disc loss: 0.0026046504397839332, policy loss: 6.879258899616643
Experience 10, Iter 28, disc loss: 0.0029881565632299363, policy loss: 6.615871573880335
Experience 10, Iter 29, disc loss: 0.0025758116971362857, policy loss: 6.836258501942025
Experience 10, Iter 30, disc loss: 0.003535911422730102, policy loss: 6.454802192154111
Experience 10, Iter 31, disc loss: 0.0032250857917811328, policy loss: 6.532017644424252
Experience 10, Iter 32, disc loss: 0.002990894966625937, policy loss: 6.697269557415359
Experience 10, Iter 33, disc loss: 0.003681683061183806, policy loss: 6.3876413451570215
Experience 10, Iter 34, disc loss: 0.002548672393457609, policy loss: 6.95150133756209
Experience 10, Iter 35, disc loss: 0.002872261653024554, policy loss: 6.680551666668951
Experience 10, Iter 36, disc loss: 0.0030414374364351733, policy loss: 6.751111747359051
Experience 10, Iter 37, disc loss: 0.002994673679900736, policy loss: 6.672987307110445
Experience 10, Iter 38, disc loss: 0.002338657016588192, policy loss: 6.951001445029799
Experience 10, Iter 39, disc loss: 0.002343045908635941, policy loss: 6.905327708280494
Experience 10, Iter 40, disc loss: 0.002506530234521713, policy loss: 6.759360480541007
Experience 10, Iter 41, disc loss: 0.0027739493373006192, policy loss: 6.741993749820638
Experience 10, Iter 42, disc loss: 0.0031763390440310304, policy loss: 6.449992162017117
Experience 10, Iter 43, disc loss: 0.0036833770448681247, policy loss: 6.55235159262043
Experience 10, Iter 44, disc loss: 0.003030877353516007, policy loss: 6.823927319347398
Experience 10, Iter 45, disc loss: 0.0029767687593826014, policy loss: 6.834222601511897
Experience 10, Iter 46, disc loss: 0.00271241661449193, policy loss: 6.7577431907637155
Experience 10, Iter 47, disc loss: 0.0030607844227877766, policy loss: 6.545617486733658
Experience 10, Iter 48, disc loss: 0.0027910200112375313, policy loss: 6.921782722734115
Experience 10, Iter 49, disc loss: 0.0032870909726707065, policy loss: 6.734538505421478
Experience 10, Iter 50, disc loss: 0.0037653176246686274, policy loss: 6.4786615548841375
Experience 10, Iter 51, disc loss: 0.0029485121795300175, policy loss: 6.692580211799741
Experience 10, Iter 52, disc loss: 0.0033885119238546294, policy loss: 6.531671346624385
Experience 10, Iter 53, disc loss: 0.003076505194004253, policy loss: 6.623380475832349
Experience 10, Iter 54, disc loss: 0.0029572892732760633, policy loss: 6.675989275036578
Experience 10, Iter 55, disc loss: 0.0031089817156600354, policy loss: 6.798005377546147
Experience 10, Iter 56, disc loss: 0.0026474066768456973, policy loss: 6.644672776752691
Experience 10, Iter 57, disc loss: 0.002827812141425482, policy loss: 6.759945245937574
Experience 10, Iter 58, disc loss: 0.003427468306856635, policy loss: 6.660224408111818
Experience 10, Iter 59, disc loss: 0.0029613858265712502, policy loss: 6.700555190362695
Experience 10, Iter 60, disc loss: 0.003069580442930953, policy loss: 6.592041435671014
Experience 10, Iter 61, disc loss: 0.0031824637892015994, policy loss: 6.545934412315547
Experience 10, Iter 62, disc loss: 0.0028540200466175567, policy loss: 6.691220261718139
Experience 10, Iter 63, disc loss: 0.0029684370366591718, policy loss: 6.784539590458505
Experience 10, Iter 64, disc loss: 0.0025135607221006377, policy loss: 6.7661223775018104
Experience 10, Iter 65, disc loss: 0.00290948697467474, policy loss: 6.609753032860622
Experience 10, Iter 66, disc loss: 0.0030603357344938173, policy loss: 6.685965337018901
Experience 10, Iter 67, disc loss: 0.003007875148441094, policy loss: 6.548872793973199
Experience 10, Iter 68, disc loss: 0.002463034421297288, policy loss: 6.837001009997357
Experience 10, Iter 69, disc loss: 0.002272109465104471, policy loss: 6.888679101934162
Experience 10, Iter 70, disc loss: 0.0031278850517907175, policy loss: 6.663981593663099
Experience 10, Iter 71, disc loss: 0.002794215691775889, policy loss: 6.773788218246841
Experience 10, Iter 72, disc loss: 0.0024910136547618084, policy loss: 6.72115596512666
Experience 10, Iter 73, disc loss: 0.0021714350681195827, policy loss: 6.936176058848163
Experience 10, Iter 74, disc loss: 0.0024736564699556225, policy loss: 6.865914736497679
Experience 10, Iter 75, disc loss: 0.0022412015856820644, policy loss: 6.907738051704317
Experience 10, Iter 76, disc loss: 0.0029083337164137477, policy loss: 6.580432684046581
Experience 10, Iter 77, disc loss: 0.0025453599581640434, policy loss: 6.870009515211221
Experience 10, Iter 78, disc loss: 0.0027014136956758296, policy loss: 6.775224653853201
Experience 10, Iter 79, disc loss: 0.0030342372333973418, policy loss: 6.750477422957685
Experience 10, Iter 80, disc loss: 0.0031838627347487844, policy loss: 6.884614004068705
Experience 10, Iter 81, disc loss: 0.002697550134832855, policy loss: 6.985720341663538
Experience 10, Iter 82, disc loss: 0.0025580649092564814, policy loss: 7.0449661230686305
Experience 10, Iter 83, disc loss: 0.0027218547404227753, policy loss: 6.77230676403466
Experience 10, Iter 84, disc loss: 0.002781074865392147, policy loss: 6.654187760175981
Experience 10, Iter 85, disc loss: 0.0025278417461450904, policy loss: 6.739831210152518
Experience 10, Iter 86, disc loss: 0.0024497694105255503, policy loss: 6.8224015718496105
Experience 10, Iter 87, disc loss: 0.0029243699595775203, policy loss: 6.7887310908182945
Experience 10, Iter 88, disc loss: 0.002593478238101133, policy loss: 6.890148365401711
Experience 10, Iter 89, disc loss: 0.0026498677380863735, policy loss: 6.753082529262634
Experience 10, Iter 90, disc loss: 0.002627883758119543, policy loss: 6.721980225264343
Experience 10, Iter 91, disc loss: 0.002498427233800349, policy loss: 6.615205943770618
Experience 10, Iter 92, disc loss: 0.002317299973145371, policy loss: 6.761158959749624
Experience 10, Iter 93, disc loss: 0.0023721293401159443, policy loss: 6.90767882787787
Experience 10, Iter 94, disc loss: 0.0025558867009913295, policy loss: 6.699589237713019
Experience 10, Iter 95, disc loss: 0.002523446828562414, policy loss: 6.723373034004511
Experience 10, Iter 96, disc loss: 0.0024383909635434186, policy loss: 6.931081692548357
Experience 10, Iter 97, disc loss: 0.0022245486688710642, policy loss: 6.9929386136260145
Experience 10, Iter 98, disc loss: 0.002928054456258922, policy loss: 6.596521723454149
Experience 10, Iter 99, disc loss: 0.002392701833272244, policy loss: 6.945687699904957
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1052],
        [1.0554],
        [0.0068]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.4720e-02, 1.8945e-01, 3.8694e-01, 1.0529e-02, 1.9180e-03,
          2.3358e+00]],

        [[2.4720e-02, 1.8945e-01, 3.8694e-01, 1.0529e-02, 1.9180e-03,
          2.3358e+00]],

        [[2.4720e-02, 1.8945e-01, 3.8694e-01, 1.0529e-02, 1.9180e-03,
          2.3358e+00]],

        [[2.4720e-02, 1.8945e-01, 3.8694e-01, 1.0529e-02, 1.9180e-03,
          2.3358e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0197, 0.4209, 4.2215, 0.0271], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0197, 0.4209, 4.2215, 0.0271])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.205
Iter 2/2000 - Loss: 2.434
Iter 3/2000 - Loss: 2.154
Iter 4/2000 - Loss: 2.171
Iter 5/2000 - Loss: 2.234
Iter 6/2000 - Loss: 2.143
Iter 7/2000 - Loss: 2.024
Iter 8/2000 - Loss: 1.970
Iter 9/2000 - Loss: 1.954
Iter 10/2000 - Loss: 1.904
Iter 11/2000 - Loss: 1.798
Iter 12/2000 - Loss: 1.666
Iter 13/2000 - Loss: 1.538
Iter 14/2000 - Loss: 1.416
Iter 15/2000 - Loss: 1.286
Iter 16/2000 - Loss: 1.132
Iter 17/2000 - Loss: 0.947
Iter 18/2000 - Loss: 0.739
Iter 19/2000 - Loss: 0.513
Iter 20/2000 - Loss: 0.278
Iter 1981/2000 - Loss: -8.102
Iter 1982/2000 - Loss: -8.102
Iter 1983/2000 - Loss: -8.102
Iter 1984/2000 - Loss: -8.103
Iter 1985/2000 - Loss: -8.103
Iter 1986/2000 - Loss: -8.103
Iter 1987/2000 - Loss: -8.103
Iter 1988/2000 - Loss: -8.103
Iter 1989/2000 - Loss: -8.103
Iter 1990/2000 - Loss: -8.103
Iter 1991/2000 - Loss: -8.103
Iter 1992/2000 - Loss: -8.103
Iter 1993/2000 - Loss: -8.103
Iter 1994/2000 - Loss: -8.103
Iter 1995/2000 - Loss: -8.103
Iter 1996/2000 - Loss: -8.103
Iter 1997/2000 - Loss: -8.103
Iter 1998/2000 - Loss: -8.103
Iter 1999/2000 - Loss: -8.103
Iter 2000/2000 - Loss: -8.103
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[18.1110, 11.3985, 38.4970, 12.1305, 15.0504, 53.3476]],

        [[24.5815, 45.1160, 11.2621,  1.5590,  2.2041, 16.7399]],

        [[25.2313, 42.7818, 13.2221,  1.0012,  7.6200, 21.8350]],

        [[22.3633, 42.7437, 16.0928,  3.4005,  2.8846, 38.8271]]])
Signal Variance: tensor([ 0.1870,  1.8205, 20.3262,  0.3521])
Estimated target variance: tensor([0.0197, 0.4209, 4.2215, 0.0271])
N: 110
Signal to noise ratio: tensor([ 24.7953,  76.3439, 115.3272,  41.9618])
Bound on condition number: tensor([  67630.0027,  641124.2793, 1463039.7436,  193688.1033])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.0023544383728026875, policy loss: 7.06988959965266
Experience 11, Iter 1, disc loss: 0.002690883922701152, policy loss: 6.800485779521244
Experience 11, Iter 2, disc loss: 0.002582193002079425, policy loss: 6.752285270790818
Experience 11, Iter 3, disc loss: 0.0024301023457346284, policy loss: 6.908524465274834
Experience 11, Iter 4, disc loss: 0.0027901232228219465, policy loss: 6.667793763333029
Experience 11, Iter 5, disc loss: 0.0025950124481148398, policy loss: 6.8329049863002425
Experience 11, Iter 6, disc loss: 0.002199289875844835, policy loss: 7.042647540044296
Experience 11, Iter 7, disc loss: 0.0020824878347246154, policy loss: 6.973329724747876
Experience 11, Iter 8, disc loss: 0.002020031252013977, policy loss: 7.262805532831429
Experience 11, Iter 9, disc loss: 0.002989151440726626, policy loss: 6.857562564642823
Experience 11, Iter 10, disc loss: 0.0024756873347583285, policy loss: 6.881446701772122
Experience 11, Iter 11, disc loss: 0.0025624790871812374, policy loss: 6.97577390354817
Experience 11, Iter 12, disc loss: 0.0026011429454538652, policy loss: 6.700143251060121
Experience 11, Iter 13, disc loss: 0.002426248748784294, policy loss: 6.703075957138447
Experience 11, Iter 14, disc loss: 0.0022933061173291943, policy loss: 6.812120299386052
Experience 11, Iter 15, disc loss: 0.002656217766433255, policy loss: 6.986481480832936
Experience 11, Iter 16, disc loss: 0.0022921040008992088, policy loss: 6.921773707644426
Experience 11, Iter 17, disc loss: 0.002045703398110885, policy loss: 6.888864516434389
Experience 11, Iter 18, disc loss: 0.0024565886501302907, policy loss: 6.9318921484198555
Experience 11, Iter 19, disc loss: 0.0025698957791746527, policy loss: 7.005553902926785
Experience 11, Iter 20, disc loss: 0.002174915681416257, policy loss: 6.906986813582878
Experience 11, Iter 21, disc loss: 0.0020318071911579953, policy loss: 7.07284993920873
Experience 11, Iter 22, disc loss: 0.0021968094084484323, policy loss: 6.993831583859769
Experience 11, Iter 23, disc loss: 0.0027671467510814664, policy loss: 6.844342945404775
Experience 11, Iter 24, disc loss: 0.0025808827725340615, policy loss: 6.790221119851014
Experience 11, Iter 25, disc loss: 0.002299058101310425, policy loss: 7.0067861426545
Experience 11, Iter 26, disc loss: 0.0020070290597928525, policy loss: 7.1854136275939755
Experience 11, Iter 27, disc loss: 0.002128220901956309, policy loss: 6.971322327751569
Experience 11, Iter 28, disc loss: 0.0021387212956115215, policy loss: 6.877487937140326
Experience 11, Iter 29, disc loss: 0.002166901259409231, policy loss: 6.917096210068324
Experience 11, Iter 30, disc loss: 0.002300219507100293, policy loss: 6.918769411869074
Experience 11, Iter 31, disc loss: 0.0025300697250494695, policy loss: 6.8103373477167715
Experience 11, Iter 32, disc loss: 0.002459977606372365, policy loss: 7.017012584298519
Experience 11, Iter 33, disc loss: 0.0022719410154940934, policy loss: 7.026519711183351
Experience 11, Iter 34, disc loss: 0.002520447859641856, policy loss: 7.081853725307029
Experience 11, Iter 35, disc loss: 0.002421418680720627, policy loss: 6.896649242707612
Experience 11, Iter 36, disc loss: 0.002996746063732588, policy loss: 6.815966907152748
Experience 11, Iter 37, disc loss: 0.0022079264689329166, policy loss: 6.960653960953327
Experience 11, Iter 38, disc loss: 0.002859641794833051, policy loss: 6.616045215413543
Experience 11, Iter 39, disc loss: 0.0020016014843388637, policy loss: 7.166066610251966
Experience 11, Iter 40, disc loss: 0.0023281205756109312, policy loss: 6.966486920556852
Experience 11, Iter 41, disc loss: 0.002189693822428286, policy loss: 7.157582281777108
Experience 11, Iter 42, disc loss: 0.0025003785239880545, policy loss: 6.999946610377828
Experience 11, Iter 43, disc loss: 0.0019775491331886176, policy loss: 7.254502068321296
Experience 11, Iter 44, disc loss: 0.0022559995197706695, policy loss: 7.060085240890434
Experience 11, Iter 45, disc loss: 0.001755053034798219, policy loss: 7.173999950745343
Experience 11, Iter 46, disc loss: 0.002331226847724256, policy loss: 7.038003495876419
Experience 11, Iter 47, disc loss: 0.002520750913046437, policy loss: 6.967282037943518
Experience 11, Iter 48, disc loss: 0.002048254214245305, policy loss: 7.185195596483263
Experience 11, Iter 49, disc loss: 0.0021976607971642874, policy loss: 7.0910802227626215
Experience 11, Iter 50, disc loss: 0.0020605720878103862, policy loss: 7.00699860240296
Experience 11, Iter 51, disc loss: 0.001946017578122794, policy loss: 7.12052297079061
Experience 11, Iter 52, disc loss: 0.0021722536204282555, policy loss: 6.923079908060781
Experience 11, Iter 53, disc loss: 0.0018235270043634115, policy loss: 7.228921642352551
Experience 11, Iter 54, disc loss: 0.0017706819284775592, policy loss: 7.185199906986648
Experience 11, Iter 55, disc loss: 0.0022832494054367825, policy loss: 6.951673453616484
Experience 11, Iter 56, disc loss: 0.002049889056604737, policy loss: 7.018471207794587
Experience 11, Iter 57, disc loss: 0.002340969082297784, policy loss: 6.729127505867529
Experience 11, Iter 58, disc loss: 0.0014512780569059475, policy loss: 7.402857262267364
Experience 11, Iter 59, disc loss: 0.001608853969415206, policy loss: 7.185839587435103
Experience 11, Iter 60, disc loss: 0.001863946122460616, policy loss: 7.018805788049822
Experience 11, Iter 61, disc loss: 0.0022371469645386323, policy loss: 7.084973143178087
Experience 11, Iter 62, disc loss: 0.0023286485182672425, policy loss: 6.82815619990725
Experience 11, Iter 63, disc loss: 0.0021374811776183568, policy loss: 7.142056364688779
Experience 11, Iter 64, disc loss: 0.002373174200289872, policy loss: 6.964117655048533
Experience 11, Iter 65, disc loss: 0.002225431945013071, policy loss: 6.8656354199866545
Experience 11, Iter 66, disc loss: 0.0020083847668136874, policy loss: 7.084783515496698
Experience 11, Iter 67, disc loss: 0.0022735704856052903, policy loss: 6.888282777187738
Experience 11, Iter 68, disc loss: 0.0020002827079188463, policy loss: 7.163243181106468
Experience 11, Iter 69, disc loss: 0.0015300448807162221, policy loss: 7.435226194240112
Experience 11, Iter 70, disc loss: 0.002052759882927283, policy loss: 7.130600806066878
Experience 11, Iter 71, disc loss: 0.0024492937978977357, policy loss: 7.0416491602793165
Experience 11, Iter 72, disc loss: 0.0020146902629644267, policy loss: 7.098377179109976
Experience 11, Iter 73, disc loss: 0.002774512932510157, policy loss: 6.833282993917095
Experience 11, Iter 74, disc loss: 0.002107214694978347, policy loss: 7.085046355207082
Experience 11, Iter 75, disc loss: 0.001986341353278631, policy loss: 7.075375875612951
Experience 11, Iter 76, disc loss: 0.0019983607408505222, policy loss: 6.906412499442861
Experience 11, Iter 77, disc loss: 0.0022365222776391373, policy loss: 6.939878988486928
Experience 11, Iter 78, disc loss: 0.0021050658612554243, policy loss: 7.012204536176159
Experience 11, Iter 79, disc loss: 0.0020297018527095177, policy loss: 7.256451259596981
Experience 11, Iter 80, disc loss: 0.0016706559516453236, policy loss: 7.277718134233199
Experience 11, Iter 81, disc loss: 0.0017540821155194004, policy loss: 7.151382660402237
Experience 11, Iter 82, disc loss: 0.0014637770748720839, policy loss: 7.487924487428385
Experience 11, Iter 83, disc loss: 0.0018482148372062233, policy loss: 7.104565945173965
Experience 11, Iter 84, disc loss: 0.0016697340092590555, policy loss: 7.172432581568099
Experience 11, Iter 85, disc loss: 0.002023579138932103, policy loss: 6.9982816745328895
Experience 11, Iter 86, disc loss: 0.001832905350950467, policy loss: 6.9524627699913735
Experience 11, Iter 87, disc loss: 0.0019340588951211283, policy loss: 7.029467036121103
Experience 11, Iter 88, disc loss: 0.0019147547370120589, policy loss: 7.011365963949162
Experience 11, Iter 89, disc loss: 0.0019794291812512245, policy loss: 6.896590771108469
Experience 11, Iter 90, disc loss: 0.001796710975633245, policy loss: 7.254536819141897
Experience 11, Iter 91, disc loss: 0.001812196802382577, policy loss: 7.215207249774911
Experience 11, Iter 92, disc loss: 0.0019059678875064613, policy loss: 7.1427956905207814
Experience 11, Iter 93, disc loss: 0.0020607738970308247, policy loss: 7.010270914262888
Experience 11, Iter 94, disc loss: 0.0014443717437942565, policy loss: 7.363283551290603
Experience 11, Iter 95, disc loss: 0.001966215783443932, policy loss: 7.056204183318201
Experience 11, Iter 96, disc loss: 0.0016689609741323518, policy loss: 7.332596159345409
Experience 11, Iter 97, disc loss: 0.0018931507797697688, policy loss: 6.97827286488985
Experience 11, Iter 98, disc loss: 0.0016296507505198438, policy loss: 7.274824828053325
Experience 11, Iter 99, disc loss: 0.001933873678309988, policy loss: 7.0133896126416495
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.1047],
        [1.0623],
        [0.0065]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.2679e-02, 1.7530e-01, 3.7792e-01, 9.8137e-03, 1.7785e-03,
          2.2673e+00]],

        [[2.2679e-02, 1.7530e-01, 3.7792e-01, 9.8137e-03, 1.7785e-03,
          2.2673e+00]],

        [[2.2679e-02, 1.7530e-01, 3.7792e-01, 9.8137e-03, 1.7785e-03,
          2.2673e+00]],

        [[2.2679e-02, 1.7530e-01, 3.7792e-01, 9.8137e-03, 1.7785e-03,
          2.2673e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0181, 0.4188, 4.2493, 0.0261], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0181, 0.4188, 4.2493, 0.0261])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.156
Iter 2/2000 - Loss: 2.398
Iter 3/2000 - Loss: 2.108
Iter 4/2000 - Loss: 2.128
Iter 5/2000 - Loss: 2.195
Iter 6/2000 - Loss: 2.101
Iter 7/2000 - Loss: 1.980
Iter 8/2000 - Loss: 1.926
Iter 9/2000 - Loss: 1.912
Iter 10/2000 - Loss: 1.863
Iter 11/2000 - Loss: 1.757
Iter 12/2000 - Loss: 1.624
Iter 13/2000 - Loss: 1.494
Iter 14/2000 - Loss: 1.372
Iter 15/2000 - Loss: 1.240
Iter 16/2000 - Loss: 1.081
Iter 17/2000 - Loss: 0.890
Iter 18/2000 - Loss: 0.673
Iter 19/2000 - Loss: 0.439
Iter 20/2000 - Loss: 0.196
Iter 1981/2000 - Loss: -8.267
Iter 1982/2000 - Loss: -8.267
Iter 1983/2000 - Loss: -8.267
Iter 1984/2000 - Loss: -8.267
Iter 1985/2000 - Loss: -8.267
Iter 1986/2000 - Loss: -8.267
Iter 1987/2000 - Loss: -8.267
Iter 1988/2000 - Loss: -8.267
Iter 1989/2000 - Loss: -8.268
Iter 1990/2000 - Loss: -8.268
Iter 1991/2000 - Loss: -8.268
Iter 1992/2000 - Loss: -8.268
Iter 1993/2000 - Loss: -8.268
Iter 1994/2000 - Loss: -8.268
Iter 1995/2000 - Loss: -8.268
Iter 1996/2000 - Loss: -8.268
Iter 1997/2000 - Loss: -8.268
Iter 1998/2000 - Loss: -8.268
Iter 1999/2000 - Loss: -8.268
Iter 2000/2000 - Loss: -8.268
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[17.5450, 11.2720, 37.5229, 11.9173, 14.8230, 52.1128]],

        [[23.6240, 44.1618, 11.3597,  1.6004,  2.1502, 17.2177]],

        [[24.3999, 41.7301, 12.9524,  1.0085,  7.7775, 22.0946]],

        [[21.3697, 41.8986, 16.1058,  3.3682,  2.7948, 39.1281]]])
Signal Variance: tensor([ 0.1836,  1.8913, 20.7293,  0.3501])
Estimated target variance: tensor([0.0181, 0.4188, 4.2493, 0.0261])
N: 120
Signal to noise ratio: tensor([ 24.7502,  79.1827, 116.3606,  42.4605])
Bound on condition number: tensor([  73509.9032,  752389.2939, 1624776.3912,  216348.5934])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.0023303080261144176, policy loss: 6.947382498120482
Experience 12, Iter 1, disc loss: 0.0023444249566566626, policy loss: 7.033088930591288
Experience 12, Iter 2, disc loss: 0.0017067192835164271, policy loss: 7.153264934786456
Experience 12, Iter 3, disc loss: 0.0018443648778665382, policy loss: 7.1600749405381725
Experience 12, Iter 4, disc loss: 0.0017247970965099822, policy loss: 7.251204006735121
Experience 12, Iter 5, disc loss: 0.0018541792179063963, policy loss: 7.21068312467067
Experience 12, Iter 6, disc loss: 0.0012888069056240371, policy loss: 7.517869388055847
Experience 12, Iter 7, disc loss: 0.0021605758409920482, policy loss: 6.8607807035146635
Experience 12, Iter 8, disc loss: 0.0023081015207207655, policy loss: 6.956645285505005
Experience 12, Iter 9, disc loss: 0.002114023927661985, policy loss: 7.016632805169995
Experience 12, Iter 10, disc loss: 0.0024469589634168224, policy loss: 6.9203036609816015
Experience 12, Iter 11, disc loss: 0.0019199577246179513, policy loss: 7.370852347221329
Experience 12, Iter 12, disc loss: 0.0016631511783229593, policy loss: 7.3522279577396334
Experience 12, Iter 13, disc loss: 0.0020362897009277604, policy loss: 7.031793787548773
Experience 12, Iter 14, disc loss: 0.002157576319629451, policy loss: 7.097045049465153
Experience 12, Iter 15, disc loss: 0.0015732741076744739, policy loss: 7.236438016086777
Experience 12, Iter 16, disc loss: 0.0016454026856885188, policy loss: 7.231662978486765
Experience 12, Iter 17, disc loss: 0.001839877133159036, policy loss: 7.212674512235391
Experience 12, Iter 18, disc loss: 0.0017402858590475922, policy loss: 7.134580207886655
Experience 12, Iter 19, disc loss: 0.002272987954962902, policy loss: 6.92023236251503
Experience 12, Iter 20, disc loss: 0.0015672527030216685, policy loss: 7.224861164950262
Experience 12, Iter 21, disc loss: 0.002043359022998734, policy loss: 7.196759162302506
Experience 12, Iter 22, disc loss: 0.001876483828545521, policy loss: 7.035862484731223
Experience 12, Iter 23, disc loss: 0.001934450146879224, policy loss: 7.178517712468878
Experience 12, Iter 24, disc loss: 0.002109560473816891, policy loss: 7.050182011631056
Experience 12, Iter 25, disc loss: 0.0017659365891556987, policy loss: 7.241359470791306
Experience 12, Iter 26, disc loss: 0.0017445415917064864, policy loss: 7.308859246144803
Experience 12, Iter 27, disc loss: 0.001935270542830899, policy loss: 7.257757040363366
Experience 12, Iter 28, disc loss: 0.0019696842317816435, policy loss: 7.124984485324121
Experience 12, Iter 29, disc loss: 0.0026865077553247098, policy loss: 6.676886899099635
Experience 12, Iter 30, disc loss: 0.0019424145337602447, policy loss: 6.96264792883179
Experience 12, Iter 31, disc loss: 0.0016826032278058271, policy loss: 7.300037381772989
Experience 12, Iter 32, disc loss: 0.0017948212678662318, policy loss: 7.043023678072656
Experience 12, Iter 33, disc loss: 0.001844890718026072, policy loss: 7.074856313170998
Experience 12, Iter 34, disc loss: 0.0022155705720124626, policy loss: 6.995717506901087
Experience 12, Iter 35, disc loss: 0.002054403459058022, policy loss: 7.086104372030438
Experience 12, Iter 36, disc loss: 0.0020818391915144747, policy loss: 7.1757747247893295
Experience 12, Iter 37, disc loss: 0.0021157832942478305, policy loss: 7.10613591998294
Experience 12, Iter 38, disc loss: 0.002072958857875635, policy loss: 7.108162238816629
Experience 12, Iter 39, disc loss: 0.0017677325797213173, policy loss: 7.3069140533274615
Experience 12, Iter 40, disc loss: 0.002049506985663195, policy loss: 7.176610296999736
Experience 12, Iter 41, disc loss: 0.0017674928767182731, policy loss: 7.385360303803948
Experience 12, Iter 42, disc loss: 0.0020592588649479804, policy loss: 6.941988440511835
Experience 12, Iter 43, disc loss: 0.0022702148310257747, policy loss: 6.982119284529771
Experience 12, Iter 44, disc loss: 0.002099172399463995, policy loss: 7.016970822899761
Experience 12, Iter 45, disc loss: 0.0018920364865313549, policy loss: 7.252358117852361
Experience 12, Iter 46, disc loss: 0.002328710387895296, policy loss: 6.94435970086571
Experience 12, Iter 47, disc loss: 0.002018169842940248, policy loss: 7.1348397191005795
Experience 12, Iter 48, disc loss: 0.0014189085322275736, policy loss: 7.401162920825051
Experience 12, Iter 49, disc loss: 0.001858792172651395, policy loss: 7.055165137987812
Experience 12, Iter 50, disc loss: 0.0018259038580139583, policy loss: 7.261473209183248
Experience 12, Iter 51, disc loss: 0.0019076956334720151, policy loss: 7.200498041392561
Experience 12, Iter 52, disc loss: 0.0018651508674548357, policy loss: 7.316498963273549
Experience 12, Iter 53, disc loss: 0.0017916802997868602, policy loss: 7.429266115435487
Experience 12, Iter 54, disc loss: 0.0018218183299866548, policy loss: 7.131293158516018
Experience 12, Iter 55, disc loss: 0.0017993912875710854, policy loss: 7.092533002363786
Experience 12, Iter 56, disc loss: 0.001653243281546444, policy loss: 7.304541077092777
Experience 12, Iter 57, disc loss: 0.002291597860845019, policy loss: 6.928909612149198
Experience 12, Iter 58, disc loss: 0.0019045111362078924, policy loss: 7.2855069725518815
Experience 12, Iter 59, disc loss: 0.0018461153259900275, policy loss: 7.021668722223422
Experience 12, Iter 60, disc loss: 0.0018352615871237345, policy loss: 7.104156656872471
Experience 12, Iter 61, disc loss: 0.001959797774536174, policy loss: 7.210792648156615
Experience 12, Iter 62, disc loss: 0.0017796236556691285, policy loss: 7.160913083690071
Experience 12, Iter 63, disc loss: 0.0018058608095602698, policy loss: 7.305182576308273
Experience 12, Iter 64, disc loss: 0.0018085907043529983, policy loss: 7.137364872321712
Experience 12, Iter 65, disc loss: 0.0015630935525514302, policy loss: 7.256432939593544
Experience 12, Iter 66, disc loss: 0.0017565395462183302, policy loss: 7.335449664093408
Experience 12, Iter 67, disc loss: 0.0017714031095417066, policy loss: 7.281867384738087
Experience 12, Iter 68, disc loss: 0.0019499036847033496, policy loss: 7.1143859444688236
Experience 12, Iter 69, disc loss: 0.001775675797754332, policy loss: 7.2370597033646415
Experience 12, Iter 70, disc loss: 0.0016517620464248151, policy loss: 7.29091771869246
Experience 12, Iter 71, disc loss: 0.0019885065988628618, policy loss: 7.187187013590017
Experience 12, Iter 72, disc loss: 0.0017960334792407177, policy loss: 7.167168969851446
Experience 12, Iter 73, disc loss: 0.002177661489613405, policy loss: 6.992498631432447
Experience 12, Iter 74, disc loss: 0.0019822258492677466, policy loss: 7.116935685109977
Experience 12, Iter 75, disc loss: 0.00160771571154981, policy loss: 7.351828185530562
Experience 12, Iter 76, disc loss: 0.0014792481422145239, policy loss: 7.423048339505573
Experience 12, Iter 77, disc loss: 0.001604713328198195, policy loss: 7.464048882053444
Experience 12, Iter 78, disc loss: 0.0015774256280761266, policy loss: 7.3390464568563765
Experience 12, Iter 79, disc loss: 0.0017768110133772877, policy loss: 7.184907709012901
Experience 12, Iter 80, disc loss: 0.0018141402994192013, policy loss: 7.405125476052026
Experience 12, Iter 81, disc loss: 0.00182425194035427, policy loss: 7.3502863000459495
Experience 12, Iter 82, disc loss: 0.0017134904448003552, policy loss: 7.257833176990915
Experience 12, Iter 83, disc loss: 0.0016719327928709253, policy loss: 7.4359193353093165
Experience 12, Iter 84, disc loss: 0.0015303781396991253, policy loss: 7.4954590011503335
Experience 12, Iter 85, disc loss: 0.0016150718632518912, policy loss: 7.276112671560616
Experience 12, Iter 86, disc loss: 0.0017294547119468366, policy loss: 7.189387316377855
Experience 12, Iter 87, disc loss: 0.0017165844745125284, policy loss: 7.274893034087154
Experience 12, Iter 88, disc loss: 0.0016548286249917362, policy loss: 7.255291550959966
Experience 12, Iter 89, disc loss: 0.0019516653484578845, policy loss: 6.994405726349568
Experience 12, Iter 90, disc loss: 0.0016932832268525282, policy loss: 7.30970467706689
Experience 12, Iter 91, disc loss: 0.0014332940249692147, policy loss: 7.318366127712666
Experience 12, Iter 92, disc loss: 0.0017649017614809635, policy loss: 7.104763303440991
Experience 12, Iter 93, disc loss: 0.0015676408472027551, policy loss: 7.385611110969016
Experience 12, Iter 94, disc loss: 0.0016408322311277847, policy loss: 7.251572898202498
Experience 12, Iter 95, disc loss: 0.0017394813625645667, policy loss: 7.28055407771946
Experience 12, Iter 96, disc loss: 0.0017019608227330538, policy loss: 7.313718554717296
Experience 12, Iter 97, disc loss: 0.0016196847977155223, policy loss: 7.3944771812173435
Experience 12, Iter 98, disc loss: 0.001703580257872879, policy loss: 7.18857653575416
Experience 12, Iter 99, disc loss: 0.0014692232245374997, policy loss: 7.563450378753846
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.1017],
        [1.0366],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.0938e-02, 1.6285e-01, 3.6249e-01, 9.1301e-03, 1.6602e-03,
          2.1687e+00]],

        [[2.0938e-02, 1.6285e-01, 3.6249e-01, 9.1301e-03, 1.6602e-03,
          2.1687e+00]],

        [[2.0938e-02, 1.6285e-01, 3.6249e-01, 9.1301e-03, 1.6602e-03,
          2.1687e+00]],

        [[2.0938e-02, 1.6285e-01, 3.6249e-01, 9.1301e-03, 1.6602e-03,
          2.1687e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0168, 0.4068, 4.1465, 0.0249], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0168, 0.4068, 4.1465, 0.0249])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.074
Iter 2/2000 - Loss: 2.343
Iter 3/2000 - Loss: 2.031
Iter 4/2000 - Loss: 2.055
Iter 5/2000 - Loss: 2.131
Iter 6/2000 - Loss: 2.036
Iter 7/2000 - Loss: 1.906
Iter 8/2000 - Loss: 1.843
Iter 9/2000 - Loss: 1.825
Iter 10/2000 - Loss: 1.778
Iter 11/2000 - Loss: 1.669
Iter 12/2000 - Loss: 1.529
Iter 13/2000 - Loss: 1.389
Iter 14/2000 - Loss: 1.259
Iter 15/2000 - Loss: 1.123
Iter 16/2000 - Loss: 0.962
Iter 17/2000 - Loss: 0.768
Iter 18/2000 - Loss: 0.548
Iter 19/2000 - Loss: 0.309
Iter 20/2000 - Loss: 0.061
Iter 1981/2000 - Loss: -8.360
Iter 1982/2000 - Loss: -8.360
Iter 1983/2000 - Loss: -8.360
Iter 1984/2000 - Loss: -8.360
Iter 1985/2000 - Loss: -8.360
Iter 1986/2000 - Loss: -8.360
Iter 1987/2000 - Loss: -8.360
Iter 1988/2000 - Loss: -8.360
Iter 1989/2000 - Loss: -8.360
Iter 1990/2000 - Loss: -8.360
Iter 1991/2000 - Loss: -8.360
Iter 1992/2000 - Loss: -8.361
Iter 1993/2000 - Loss: -8.361
Iter 1994/2000 - Loss: -8.361
Iter 1995/2000 - Loss: -8.361
Iter 1996/2000 - Loss: -8.361
Iter 1997/2000 - Loss: -8.361
Iter 1998/2000 - Loss: -8.361
Iter 1999/2000 - Loss: -8.361
Iter 2000/2000 - Loss: -8.361
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[17.1870, 11.2334, 37.4643, 11.9184, 14.6884, 51.9182]],

        [[22.6201, 41.7415, 11.0591,  1.5859,  2.1602, 16.6398]],

        [[22.9375, 38.2187, 13.6997,  1.0097,  7.4660, 20.2527]],

        [[20.6830, 40.0273, 15.6459,  3.3416,  2.5755, 38.1969]]])
Signal Variance: tensor([ 0.1828,  1.7619, 19.8307,  0.3327])
Estimated target variance: tensor([0.0168, 0.4068, 4.1465, 0.0249])
N: 130
Signal to noise ratio: tensor([ 23.9381,  77.7586, 112.0003,  41.8315])
Bound on condition number: tensor([  74495.0832,  786033.5233, 1630730.5810,  227484.6649])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.001656451135957694, policy loss: 7.321766260800208
Experience 13, Iter 1, disc loss: 0.0013767732193902084, policy loss: 7.5227684998338615
Experience 13, Iter 2, disc loss: 0.0014891172913918138, policy loss: 7.5395828847379205
Experience 13, Iter 3, disc loss: 0.0019454236596220214, policy loss: 7.095008953485352
Experience 13, Iter 4, disc loss: 0.0018165168698694389, policy loss: 7.172797393833598
Experience 13, Iter 5, disc loss: 0.0014295727204590787, policy loss: 7.53588644838767
Experience 13, Iter 6, disc loss: 0.001552175260296859, policy loss: 7.551221906031413
Experience 13, Iter 7, disc loss: 0.0015245684486842794, policy loss: 7.438215838607945
Experience 13, Iter 8, disc loss: 0.0014813353127565436, policy loss: 7.300398160700595
Experience 13, Iter 9, disc loss: 0.0018060900397175617, policy loss: 7.242421762362962
Experience 13, Iter 10, disc loss: 0.0021423572076043574, policy loss: 7.040645617759915
Experience 13, Iter 11, disc loss: 0.001847346865560255, policy loss: 7.175102607213429
Experience 13, Iter 12, disc loss: 0.001674214480256951, policy loss: 7.398545197115906
Experience 13, Iter 13, disc loss: 0.0014067493111418689, policy loss: 7.517348427698888
Experience 13, Iter 14, disc loss: 0.001768188651575555, policy loss: 7.20885204064165
Experience 13, Iter 15, disc loss: 0.0012621069892945467, policy loss: 7.6990776208171035
Experience 13, Iter 16, disc loss: 0.0016539096741872987, policy loss: 7.351618815180862
Experience 13, Iter 17, disc loss: 0.0017272600460263607, policy loss: 7.29284534319427
Experience 13, Iter 18, disc loss: 0.0017293567179796094, policy loss: 7.469891357137447
Experience 13, Iter 19, disc loss: 0.0014200693287088244, policy loss: 7.703105019351208
Experience 13, Iter 20, disc loss: 0.0015443197147988422, policy loss: 7.431090048357903
Experience 13, Iter 21, disc loss: 0.0011693153230294942, policy loss: 7.641711713786636
Experience 13, Iter 22, disc loss: 0.0015541137275288668, policy loss: 7.520565755529283
Experience 13, Iter 23, disc loss: 0.0015686781464330063, policy loss: 7.36226010863269
Experience 13, Iter 24, disc loss: 0.001707535838869197, policy loss: 7.231242109783062
Experience 13, Iter 25, disc loss: 0.0012620981610867133, policy loss: 7.537538154674429
Experience 13, Iter 26, disc loss: 0.0014009742746246506, policy loss: 7.617958885724631
Experience 13, Iter 27, disc loss: 0.0014406627058021022, policy loss: 7.442013385173457
Experience 13, Iter 28, disc loss: 0.0018829976322651076, policy loss: 7.329201647114385
Experience 13, Iter 29, disc loss: 0.0016916595860441538, policy loss: 7.362877005891071
Experience 13, Iter 30, disc loss: 0.0015561065788351018, policy loss: 7.439978248040049
Experience 13, Iter 31, disc loss: 0.0016695591803344008, policy loss: 7.4008115107240435
Experience 13, Iter 32, disc loss: 0.001524881839046031, policy loss: 7.479492119848969
Experience 13, Iter 33, disc loss: 0.001522405221431358, policy loss: 7.436878570143826
Experience 13, Iter 34, disc loss: 0.0016950955425315854, policy loss: 7.244199005827847
Experience 13, Iter 35, disc loss: 0.0016362476564645066, policy loss: 7.398217453130981
Experience 13, Iter 36, disc loss: 0.001597838205436743, policy loss: 7.335622767681265
Experience 13, Iter 37, disc loss: 0.0016302684825080407, policy loss: 7.451111928211786
Experience 13, Iter 38, disc loss: 0.0013791879953808026, policy loss: 7.651182812791342
Experience 13, Iter 39, disc loss: 0.0013827026473156095, policy loss: 7.591658071216599
Experience 13, Iter 40, disc loss: 0.001371177341551755, policy loss: 7.543260926195249
Experience 13, Iter 41, disc loss: 0.0018877534796371848, policy loss: 7.210259231473306
Experience 13, Iter 42, disc loss: 0.0012470310161895606, policy loss: 7.9410346473755
Experience 13, Iter 43, disc loss: 0.001609788413192544, policy loss: 7.31370843636504
Experience 13, Iter 44, disc loss: 0.0015304731104274012, policy loss: 7.3805293604290005
Experience 13, Iter 45, disc loss: 0.0013944271230309354, policy loss: 7.586752855688725
Experience 13, Iter 46, disc loss: 0.0014786619806805425, policy loss: 7.56539768050691
Experience 13, Iter 47, disc loss: 0.001408557835823665, policy loss: 7.600164539419644
Experience 13, Iter 48, disc loss: 0.0013303056455959352, policy loss: 7.577936700549561
Experience 13, Iter 49, disc loss: 0.0015216742181337274, policy loss: 7.590188508694691
Experience 13, Iter 50, disc loss: 0.0012458841204933417, policy loss: 7.7014433341141375
Experience 13, Iter 51, disc loss: 0.0016252736473760874, policy loss: 7.59606812827178
Experience 13, Iter 52, disc loss: 0.0017203552624163252, policy loss: 7.2277229348158185
Experience 13, Iter 53, disc loss: 0.0013270658675576155, policy loss: 7.522731677807444
Experience 13, Iter 54, disc loss: 0.001458201788783498, policy loss: 7.441868434998067
Experience 13, Iter 55, disc loss: 0.0014650834287478721, policy loss: 7.609738330206468
Experience 13, Iter 56, disc loss: 0.0014168249170992413, policy loss: 7.561468676597285
Experience 13, Iter 57, disc loss: 0.0010312034327122832, policy loss: 7.889698557998621
Experience 13, Iter 58, disc loss: 0.0016388329906832683, policy loss: 7.438432130395995
Experience 13, Iter 59, disc loss: 0.0013170438025668836, policy loss: 7.558788689675607
Experience 13, Iter 60, disc loss: 0.0016503692485759507, policy loss: 7.432476111659372
Experience 13, Iter 61, disc loss: 0.001254528848299126, policy loss: 7.5968429815008065
Experience 13, Iter 62, disc loss: 0.0016612354999125736, policy loss: 7.427580331626224
Experience 13, Iter 63, disc loss: 0.001343347697325288, policy loss: 7.591594090418205
Experience 13, Iter 64, disc loss: 0.0014898196211417388, policy loss: 7.590341822658358
Experience 13, Iter 65, disc loss: 0.0015434243424163783, policy loss: 7.4295625788648625
Experience 13, Iter 66, disc loss: 0.0013232091358787775, policy loss: 7.592232769256604
Experience 13, Iter 67, disc loss: 0.0012022598524096544, policy loss: 7.645068117973095
Experience 13, Iter 68, disc loss: 0.0014047039009732456, policy loss: 7.563739018186308
Experience 13, Iter 69, disc loss: 0.001387185597687484, policy loss: 7.653232354567031
Experience 13, Iter 70, disc loss: 0.0012130588076482867, policy loss: 7.923541899448856
Experience 13, Iter 71, disc loss: 0.001268182358711086, policy loss: 7.884946595340969
Experience 13, Iter 72, disc loss: 0.001454405542686837, policy loss: 7.571015110158989
Experience 13, Iter 73, disc loss: 0.0013023647919016617, policy loss: 7.505184994039268
Experience 13, Iter 74, disc loss: 0.0012504364100704092, policy loss: 7.730780116659313
Experience 13, Iter 75, disc loss: 0.0012014836162640742, policy loss: 7.6783667326677225
Experience 13, Iter 76, disc loss: 0.0011889692618587912, policy loss: 7.626626719171714
Experience 13, Iter 77, disc loss: 0.0017126882288253935, policy loss: 7.454366489117326
Experience 13, Iter 78, disc loss: 0.0015961176807764087, policy loss: 7.53099199909189
Experience 13, Iter 79, disc loss: 0.0013201007553907231, policy loss: 7.789994650310814
Experience 13, Iter 80, disc loss: 0.00152952904248448, policy loss: 7.344955599966584
Experience 13, Iter 81, disc loss: 0.0013904502994119047, policy loss: 7.446325358164932
Experience 13, Iter 82, disc loss: 0.0016276370192988966, policy loss: 7.503299703272428
Experience 13, Iter 83, disc loss: 0.0012304056749037843, policy loss: 7.704771442211178
Experience 13, Iter 84, disc loss: 0.0013915875868157836, policy loss: 7.5524062185546255
Experience 13, Iter 85, disc loss: 0.0013873300131162737, policy loss: 7.709672551611204
Experience 13, Iter 86, disc loss: 0.0014660425550877523, policy loss: 7.562668785431092
Experience 13, Iter 87, disc loss: 0.0011919060290121338, policy loss: 7.899157981632651
Experience 13, Iter 88, disc loss: 0.0015644715614260667, policy loss: 7.5173412692901405
Experience 13, Iter 89, disc loss: 0.001518348484123062, policy loss: 7.652040226877833
Experience 13, Iter 90, disc loss: 0.0017570435206068057, policy loss: 7.281590960229786
Experience 13, Iter 91, disc loss: 0.0012869432602170645, policy loss: 7.622863987607851
Experience 13, Iter 92, disc loss: 0.0014246279947274403, policy loss: 7.404739326790715
Experience 13, Iter 93, disc loss: 0.001471616033318716, policy loss: 7.616215297935826
Experience 13, Iter 94, disc loss: 0.0014363129959351743, policy loss: 7.526995678742931
Experience 13, Iter 95, disc loss: 0.0012962485700711282, policy loss: 7.780820691136753
Experience 13, Iter 96, disc loss: 0.0012318016112961873, policy loss: 7.736384399544704
Experience 13, Iter 97, disc loss: 0.001519013834345921, policy loss: 7.546336813923027
Experience 13, Iter 98, disc loss: 0.0015089716160524714, policy loss: 7.46634646724552
Experience 13, Iter 99, disc loss: 0.001289685109531161, policy loss: 7.595212853044599
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1018],
        [1.0447],
        [0.0061]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.9458e-02, 1.5271e-01, 3.5648e-01, 8.6187e-03, 1.5539e-03,
          2.1249e+00]],

        [[1.9458e-02, 1.5271e-01, 3.5648e-01, 8.6187e-03, 1.5539e-03,
          2.1249e+00]],

        [[1.9458e-02, 1.5271e-01, 3.5648e-01, 8.6187e-03, 1.5539e-03,
          2.1249e+00]],

        [[1.9458e-02, 1.5271e-01, 3.5648e-01, 8.6187e-03, 1.5539e-03,
          2.1249e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.4072, 4.1787, 0.0244], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.4072, 4.1787, 0.0244])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.012
Iter 2/2000 - Loss: 2.294
Iter 3/2000 - Loss: 1.966
Iter 4/2000 - Loss: 1.990
Iter 5/2000 - Loss: 2.068
Iter 6/2000 - Loss: 1.969
Iter 7/2000 - Loss: 1.832
Iter 8/2000 - Loss: 1.763
Iter 9/2000 - Loss: 1.742
Iter 10/2000 - Loss: 1.689
Iter 11/2000 - Loss: 1.573
Iter 12/2000 - Loss: 1.424
Iter 13/2000 - Loss: 1.274
Iter 14/2000 - Loss: 1.133
Iter 15/2000 - Loss: 0.987
Iter 16/2000 - Loss: 0.817
Iter 17/2000 - Loss: 0.615
Iter 18/2000 - Loss: 0.384
Iter 19/2000 - Loss: 0.135
Iter 20/2000 - Loss: -0.125
Iter 1981/2000 - Loss: -8.530
Iter 1982/2000 - Loss: -8.530
Iter 1983/2000 - Loss: -8.530
Iter 1984/2000 - Loss: -8.530
Iter 1985/2000 - Loss: -8.530
Iter 1986/2000 - Loss: -8.530
Iter 1987/2000 - Loss: -8.530
Iter 1988/2000 - Loss: -8.530
Iter 1989/2000 - Loss: -8.531
Iter 1990/2000 - Loss: -8.531
Iter 1991/2000 - Loss: -8.531
Iter 1992/2000 - Loss: -8.531
Iter 1993/2000 - Loss: -8.531
Iter 1994/2000 - Loss: -8.531
Iter 1995/2000 - Loss: -8.531
Iter 1996/2000 - Loss: -8.531
Iter 1997/2000 - Loss: -8.531
Iter 1998/2000 - Loss: -8.531
Iter 1999/2000 - Loss: -8.531
Iter 2000/2000 - Loss: -8.531
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[16.8553, 11.2788, 37.1490, 11.6246, 14.3282, 51.3765]],

        [[22.1342, 40.6144, 11.2360,  1.5873,  2.1175, 17.0771]],

        [[22.5471, 36.8454, 12.9922,  1.0041,  6.5516, 19.6259]],

        [[20.2450, 39.2309, 15.1248,  3.2897,  2.4866, 37.2916]]])
Signal Variance: tensor([ 0.1811,  1.8038, 19.3312,  0.3114])
Estimated target variance: tensor([0.0157, 0.4072, 4.1787, 0.0244])
N: 140
Signal to noise ratio: tensor([ 24.3366,  81.1783, 114.1742,  40.8943])
Bound on condition number: tensor([  82918.5516,  922589.5940, 1825007.2765,  234129.1147])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.0015160094201476786, policy loss: 7.655243670739734
Experience 14, Iter 1, disc loss: 0.0011381133972518526, policy loss: 7.812757911302618
Experience 14, Iter 2, disc loss: 0.0012044118820037362, policy loss: 7.650392998105807
Experience 14, Iter 3, disc loss: 0.0010137982858261377, policy loss: 7.9986487730880675
Experience 14, Iter 4, disc loss: 0.001833888784801087, policy loss: 7.522119368140139
Experience 14, Iter 5, disc loss: 0.0015225587685322613, policy loss: 7.546857640670462
Experience 14, Iter 6, disc loss: 0.001755749069454877, policy loss: 7.182410161149329
Experience 14, Iter 7, disc loss: 0.001360394430480131, policy loss: 7.54439492244932
Experience 14, Iter 8, disc loss: 0.001112389962195304, policy loss: 7.83098827631784
Experience 14, Iter 9, disc loss: 0.001206579068915626, policy loss: 7.741651079327767
Experience 14, Iter 10, disc loss: 0.0014842935705539453, policy loss: 7.296683319707579
Experience 14, Iter 11, disc loss: 0.001152845534564226, policy loss: 7.8985973672215515
Experience 14, Iter 12, disc loss: 0.001431090096389479, policy loss: 7.397398200158978
Experience 14, Iter 13, disc loss: 0.0014959903244040852, policy loss: 7.50198266488756
Experience 14, Iter 14, disc loss: 0.001649739648851901, policy loss: 7.274096286840835
Experience 14, Iter 15, disc loss: 0.0013254400294972302, policy loss: 7.46856833855872
Experience 14, Iter 16, disc loss: 0.0014562944248520952, policy loss: 7.5655599267130835
Experience 14, Iter 17, disc loss: 0.0017223240760895317, policy loss: 7.356257343958171
Experience 14, Iter 18, disc loss: 0.0013425264063977838, policy loss: 7.565176133835652
Experience 14, Iter 19, disc loss: 0.0010738613393475322, policy loss: 7.690609268229034
Experience 14, Iter 20, disc loss: 0.0011500670169815592, policy loss: 7.665543234093036
Experience 14, Iter 21, disc loss: 0.0014562123869717972, policy loss: 7.564664727778414
Experience 14, Iter 22, disc loss: 0.0012450268905672128, policy loss: 7.772949590845147
Experience 14, Iter 23, disc loss: 0.0011818062463125872, policy loss: 7.781925660456237
Experience 14, Iter 24, disc loss: 0.0011231229274256955, policy loss: 7.646968497983204
Experience 14, Iter 25, disc loss: 0.0013789687661134235, policy loss: 7.592990303657927
Experience 14, Iter 26, disc loss: 0.0016512762170149062, policy loss: 7.4353849295441155
Experience 14, Iter 27, disc loss: 0.001211978803977198, policy loss: 7.619738515204652
Experience 14, Iter 28, disc loss: 0.0013800918371680358, policy loss: 7.513183341392035
Experience 14, Iter 29, disc loss: 0.0015454055211062074, policy loss: 7.5023454085240635
Experience 14, Iter 30, disc loss: 0.001454379463245357, policy loss: 7.540494912223583
Experience 14, Iter 31, disc loss: 0.0014935712729494816, policy loss: 7.444436050490058
Experience 14, Iter 32, disc loss: 0.001259623913704245, policy loss: 7.614071734412169
Experience 14, Iter 33, disc loss: 0.0017198550053484018, policy loss: 7.5504135418453355
Experience 14, Iter 34, disc loss: 0.001221160541920935, policy loss: 7.695507331965094
Experience 14, Iter 35, disc loss: 0.001502601993737831, policy loss: 7.484900281848483
Experience 14, Iter 36, disc loss: 0.0010145314256830573, policy loss: 7.924922331563815
Experience 14, Iter 37, disc loss: 0.0015938767034492825, policy loss: 7.501449504214229
Experience 14, Iter 38, disc loss: 0.001830848627882341, policy loss: 7.310829400493313
Experience 14, Iter 39, disc loss: 0.0016490939002307753, policy loss: 7.223285686185282
Experience 14, Iter 40, disc loss: 0.0016111740297127957, policy loss: 7.6430660866699665
Experience 14, Iter 41, disc loss: 0.0013898063064803207, policy loss: 7.570971401926915
Experience 14, Iter 42, disc loss: 0.001268959132080061, policy loss: 7.871240463325623
Experience 14, Iter 43, disc loss: 0.0013441500909975937, policy loss: 7.709512650035906
Experience 14, Iter 44, disc loss: 0.0014858210828892857, policy loss: 7.783815298860521
Experience 14, Iter 45, disc loss: 0.001452195258558656, policy loss: 7.403547000234689
Experience 14, Iter 46, disc loss: 0.0015045472887879753, policy loss: 7.662378036286566
Experience 14, Iter 47, disc loss: 0.0013659121412191125, policy loss: 7.667564763306245
Experience 14, Iter 48, disc loss: 0.0012036159344444974, policy loss: 7.552383162972508
Experience 14, Iter 49, disc loss: 0.0012351854569074392, policy loss: 7.705143440648817
Experience 14, Iter 50, disc loss: 0.0012295018937882232, policy loss: 7.699882007769986
Experience 14, Iter 51, disc loss: 0.0014669276959927416, policy loss: 7.560651710307029
Experience 14, Iter 52, disc loss: 0.0016621387198533027, policy loss: 7.488662161220429
Experience 14, Iter 53, disc loss: 0.0012657009868580213, policy loss: 7.864436408213828
Experience 14, Iter 54, disc loss: 0.0014089887990701632, policy loss: 7.687006696913459
Experience 14, Iter 55, disc loss: 0.001410469065648646, policy loss: 7.663612969143989
Experience 14, Iter 56, disc loss: 0.0013079237770878588, policy loss: 7.628312591865516
Experience 14, Iter 57, disc loss: 0.0012680847992345482, policy loss: 7.806041997656001
Experience 14, Iter 58, disc loss: 0.0015123821439891596, policy loss: 7.472257946197811
Experience 14, Iter 59, disc loss: 0.0011288958551221328, policy loss: 7.896055060375474
Experience 14, Iter 60, disc loss: 0.0011724105598000891, policy loss: 7.776525823194143
Experience 14, Iter 61, disc loss: 0.0013540687994690209, policy loss: 7.725934214583186
Experience 14, Iter 62, disc loss: 0.0011925862708525659, policy loss: 7.911624183673453
Experience 14, Iter 63, disc loss: 0.0018060592018252844, policy loss: 7.332354626505104
Experience 14, Iter 64, disc loss: 0.0014523135860955454, policy loss: 7.672169330441584
Experience 14, Iter 65, disc loss: 0.001386149446644142, policy loss: 7.577546872116956
Experience 14, Iter 66, disc loss: 0.0012018090273882146, policy loss: 7.82209931435299
Experience 14, Iter 67, disc loss: 0.0012773580581719882, policy loss: 7.775098640688972
Experience 14, Iter 68, disc loss: 0.0012506372751022558, policy loss: 7.734536991567989
Experience 14, Iter 69, disc loss: 0.001036705385196973, policy loss: 8.212707341175456
Experience 14, Iter 70, disc loss: 0.001239139205593079, policy loss: 7.836872056584377
Experience 14, Iter 71, disc loss: 0.001223004419392841, policy loss: 7.852018323200687
Experience 14, Iter 72, disc loss: 0.0013276879450249053, policy loss: 7.703714800572794
Experience 14, Iter 73, disc loss: 0.0011932588976565567, policy loss: 7.753329111940857
Experience 14, Iter 74, disc loss: 0.0010152958067733608, policy loss: 7.872893934809159
Experience 14, Iter 75, disc loss: 0.0013219049448765494, policy loss: 7.494218432559795
Experience 14, Iter 76, disc loss: 0.0014048091970013219, policy loss: 7.683006192852993
Experience 14, Iter 77, disc loss: 0.0011138072003019992, policy loss: 7.967638714595812
Experience 14, Iter 78, disc loss: 0.0014516369930924543, policy loss: 7.592240429813462
Experience 14, Iter 79, disc loss: 0.0012987438667465883, policy loss: 7.774182954702999
Experience 14, Iter 80, disc loss: 0.001192423052570285, policy loss: 7.817266708219832
Experience 14, Iter 81, disc loss: 0.0011099727409014856, policy loss: 7.984553713900986
Experience 14, Iter 82, disc loss: 0.0012301904388588433, policy loss: 7.76546028713696
Experience 14, Iter 83, disc loss: 0.0012425185958456972, policy loss: 7.739812655403789
Experience 14, Iter 84, disc loss: 0.0010718037936674606, policy loss: 7.892974418499821
Experience 14, Iter 85, disc loss: 0.0011546868308635692, policy loss: 7.851092061139864
Experience 14, Iter 86, disc loss: 0.0012607978375985336, policy loss: 7.7683457330945185
Experience 14, Iter 87, disc loss: 0.000974796742356699, policy loss: 7.864450019948503
Experience 14, Iter 88, disc loss: 0.0009857330230911396, policy loss: 7.963879742358264
Experience 14, Iter 89, disc loss: 0.0014381164252255125, policy loss: 7.6396297622114515
Experience 14, Iter 90, disc loss: 0.0012804727817343545, policy loss: 7.599005818875112
Experience 14, Iter 91, disc loss: 0.0010774233044131046, policy loss: 7.816587361138746
Experience 14, Iter 92, disc loss: 0.0013696721785318234, policy loss: 7.658674939863224
Experience 14, Iter 93, disc loss: 0.0010536876601914616, policy loss: 7.800018494064138
Experience 14, Iter 94, disc loss: 0.0009578856848089404, policy loss: 7.967064934520126
Experience 14, Iter 95, disc loss: 0.0010902782119246335, policy loss: 7.804664125002763
Experience 14, Iter 96, disc loss: 0.0013382032943220926, policy loss: 7.656795219285257
Experience 14, Iter 97, disc loss: 0.001151003231833358, policy loss: 7.7172865658661145
Experience 14, Iter 98, disc loss: 0.0011146612121608222, policy loss: 7.871938388338517
Experience 14, Iter 99, disc loss: 0.0014661593364799685, policy loss: 7.6167273401606845
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.1035],
        [1.0711],
        [0.0060]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.8164e-02, 1.4424e-01, 3.5721e-01, 8.2396e-03, 1.4578e-03,
          2.1180e+00]],

        [[1.8164e-02, 1.4424e-01, 3.5721e-01, 8.2396e-03, 1.4578e-03,
          2.1180e+00]],

        [[1.8164e-02, 1.4424e-01, 3.5721e-01, 8.2396e-03, 1.4578e-03,
          2.1180e+00]],

        [[1.8164e-02, 1.4424e-01, 3.5721e-01, 8.2396e-03, 1.4578e-03,
          2.1180e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0147, 0.4141, 4.2844, 0.0241], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0147, 0.4141, 4.2844, 0.0241])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.000
Iter 2/2000 - Loss: 2.307
Iter 3/2000 - Loss: 1.959
Iter 4/2000 - Loss: 1.989
Iter 5/2000 - Loss: 2.078
Iter 6/2000 - Loss: 1.980
Iter 7/2000 - Loss: 1.839
Iter 8/2000 - Loss: 1.770
Iter 9/2000 - Loss: 1.754
Iter 10/2000 - Loss: 1.708
Iter 11/2000 - Loss: 1.597
Iter 12/2000 - Loss: 1.445
Iter 13/2000 - Loss: 1.290
Iter 14/2000 - Loss: 1.144
Iter 15/2000 - Loss: 0.994
Iter 16/2000 - Loss: 0.821
Iter 17/2000 - Loss: 0.614
Iter 18/2000 - Loss: 0.377
Iter 19/2000 - Loss: 0.122
Iter 20/2000 - Loss: -0.142
Iter 1981/2000 - Loss: -8.557
Iter 1982/2000 - Loss: -8.557
Iter 1983/2000 - Loss: -8.557
Iter 1984/2000 - Loss: -8.557
Iter 1985/2000 - Loss: -8.557
Iter 1986/2000 - Loss: -8.557
Iter 1987/2000 - Loss: -8.557
Iter 1988/2000 - Loss: -8.557
Iter 1989/2000 - Loss: -8.557
Iter 1990/2000 - Loss: -8.557
Iter 1991/2000 - Loss: -8.557
Iter 1992/2000 - Loss: -8.557
Iter 1993/2000 - Loss: -8.557
Iter 1994/2000 - Loss: -8.557
Iter 1995/2000 - Loss: -8.558
Iter 1996/2000 - Loss: -8.558
Iter 1997/2000 - Loss: -8.558
Iter 1998/2000 - Loss: -8.558
Iter 1999/2000 - Loss: -8.558
Iter 2000/2000 - Loss: -8.558
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[16.2544, 11.1466, 37.1938, 11.2153, 13.7541, 49.9877]],

        [[21.0612, 39.0960, 12.0969,  1.5935,  2.1838, 21.0581]],

        [[22.4029, 34.9351, 13.0062,  1.0125,  6.1996, 21.4199]],

        [[19.6326, 36.7764, 14.5638,  2.4686,  5.8313, 36.2792]]])
Signal Variance: tensor([ 0.1760,  2.3053, 21.2189,  0.2705])
Estimated target variance: tensor([0.0147, 0.4141, 4.2844, 0.0241])
N: 150
Signal to noise ratio: tensor([ 24.3753,  87.2631, 116.3938,  37.0447])
Bound on condition number: tensor([  89124.4494, 1142229.1774, 2032130.1289,  205847.8048])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.001150565143522811, policy loss: 7.8937341040863656
Experience 15, Iter 1, disc loss: 0.0015818537069029871, policy loss: 7.636154835083662
Experience 15, Iter 2, disc loss: 0.0011221714188580875, policy loss: 7.63848118877714
Experience 15, Iter 3, disc loss: 0.0011226536364501126, policy loss: 7.897564131938644
Experience 15, Iter 4, disc loss: 0.0011415473903154155, policy loss: 7.982818591266964
Experience 15, Iter 5, disc loss: 0.0009536814693827362, policy loss: 8.089091250949323
Experience 15, Iter 6, disc loss: 0.0009071757418741629, policy loss: 8.055636442035665
Experience 15, Iter 7, disc loss: 0.001405704187896535, policy loss: 7.618949600556871
Experience 15, Iter 8, disc loss: 0.0010436076019237552, policy loss: 7.706677833784258
Experience 15, Iter 9, disc loss: 0.0010019503491452376, policy loss: 7.9397619211986505
Experience 15, Iter 10, disc loss: 0.0012398889665740098, policy loss: 7.7948683259474745
Experience 15, Iter 11, disc loss: 0.0010223587235115373, policy loss: 7.961914407479662
Experience 15, Iter 12, disc loss: 0.001201749004400776, policy loss: 7.735943584478743
Experience 15, Iter 13, disc loss: 0.0015086606738523522, policy loss: 7.619171959621291
Experience 15, Iter 14, disc loss: 0.0012779126161606424, policy loss: 7.7647796906907285
Experience 15, Iter 15, disc loss: 0.0008022741882206951, policy loss: 8.199566823155937
Experience 15, Iter 16, disc loss: 0.0011657700156477168, policy loss: 7.758115473567406
Experience 15, Iter 17, disc loss: 0.0011154192878481432, policy loss: 7.904690381652923
Experience 15, Iter 18, disc loss: 0.0011569282384512633, policy loss: 7.7266403866501765
Experience 15, Iter 19, disc loss: 0.0012702194346267295, policy loss: 7.8118339199854185
Experience 15, Iter 20, disc loss: 0.0011395012414281939, policy loss: 7.7685799222168015
Experience 15, Iter 21, disc loss: 0.0011651184450990424, policy loss: 7.788563984316429
Experience 15, Iter 22, disc loss: 0.0012264918261374778, policy loss: 7.853180915236248
Experience 15, Iter 23, disc loss: 0.0011230896528113068, policy loss: 7.740229402525279
Experience 15, Iter 24, disc loss: 0.0011022378347389874, policy loss: 7.893599035000433
Experience 15, Iter 25, disc loss: 0.001186122966389819, policy loss: 7.817776197744061
Experience 15, Iter 26, disc loss: 0.0010491495869304906, policy loss: 7.7559485669285095
Experience 15, Iter 27, disc loss: 0.0010515785795677402, policy loss: 7.904155653957189
Experience 15, Iter 28, disc loss: 0.0009789182486906497, policy loss: 7.953141909177767
Experience 15, Iter 29, disc loss: 0.0011319446763488929, policy loss: 7.825752924335112
Experience 15, Iter 30, disc loss: 0.0013664132291943399, policy loss: 7.571679234389734
Experience 15, Iter 31, disc loss: 0.001102642148363151, policy loss: 8.031875005131619
Experience 15, Iter 32, disc loss: 0.0010013312249538336, policy loss: 7.927032209425238
Experience 15, Iter 33, disc loss: 0.0008586420966253106, policy loss: 8.149822964908699
Experience 15, Iter 34, disc loss: 0.0012182692849793922, policy loss: 7.720187346227901
Experience 15, Iter 35, disc loss: 0.0010781434645581976, policy loss: 7.942517896893592
Experience 15, Iter 36, disc loss: 0.0011884128230942998, policy loss: 7.778261449527272
Experience 15, Iter 37, disc loss: 0.0009430021950329814, policy loss: 8.024364221180475
Experience 15, Iter 38, disc loss: 0.0010844857609183878, policy loss: 8.006526127889721
Experience 15, Iter 39, disc loss: 0.0013413012296310642, policy loss: 7.6027926618776895
Experience 15, Iter 40, disc loss: 0.0012336528711555219, policy loss: 7.873277382972868
Experience 15, Iter 41, disc loss: 0.0011602068739810223, policy loss: 7.753087395373818
Experience 15, Iter 42, disc loss: 0.0012018404892418775, policy loss: 7.6675676743402486
Experience 15, Iter 43, disc loss: 0.0012162475884772235, policy loss: 7.643959131741352
Experience 15, Iter 44, disc loss: 0.0011745862577186511, policy loss: 7.810992078609529
Experience 15, Iter 45, disc loss: 0.0009319128035437634, policy loss: 8.042611491930948
Experience 15, Iter 46, disc loss: 0.0008180547816023366, policy loss: 8.148290288696419
Experience 15, Iter 47, disc loss: 0.0010065709773539387, policy loss: 7.899494133758241
Experience 15, Iter 48, disc loss: 0.0007880433012171236, policy loss: 8.142364014409734
Experience 15, Iter 49, disc loss: 0.0010453573787257064, policy loss: 7.9461932030530384
Experience 15, Iter 50, disc loss: 0.0014215845701044277, policy loss: 7.557883419325833
Experience 15, Iter 51, disc loss: 0.0009541800619609866, policy loss: 8.13836482020937
Experience 15, Iter 52, disc loss: 0.0009985325364640341, policy loss: 7.940461294487918
Experience 15, Iter 53, disc loss: 0.0009938995480191191, policy loss: 7.896009537626592
Experience 15, Iter 54, disc loss: 0.0010605501055267052, policy loss: 7.835816636501651
Experience 15, Iter 55, disc loss: 0.0012208046335564601, policy loss: 7.768798090297192
Experience 15, Iter 56, disc loss: 0.0011114918334452373, policy loss: 7.886779120731799
Experience 15, Iter 57, disc loss: 0.0012257037582643262, policy loss: 7.745689840832215
Experience 15, Iter 58, disc loss: 0.0010630524140971937, policy loss: 7.823951977850998
Experience 15, Iter 59, disc loss: 0.001066562137614733, policy loss: 7.895305286453562
Experience 15, Iter 60, disc loss: 0.0010796821002067834, policy loss: 7.674495750941694
Experience 15, Iter 61, disc loss: 0.0010210792052538563, policy loss: 7.8646129264301425
Experience 15, Iter 62, disc loss: 0.0012076857041166122, policy loss: 7.718782219796516
Experience 15, Iter 63, disc loss: 0.0008451213495303676, policy loss: 8.176743487472379
Experience 15, Iter 64, disc loss: 0.0014080617872311775, policy loss: 7.6558343854921125
Experience 15, Iter 65, disc loss: 0.0010009129553799959, policy loss: 7.9746945274335435
Experience 15, Iter 66, disc loss: 0.0011015400319616766, policy loss: 7.834130042106115
Experience 15, Iter 67, disc loss: 0.0009400418400551655, policy loss: 8.045920338528525
Experience 15, Iter 68, disc loss: 0.0009139149010661434, policy loss: 7.895315483316849
Experience 15, Iter 69, disc loss: 0.0011954172234301356, policy loss: 7.701052644705291
Experience 15, Iter 70, disc loss: 0.0011450547279930302, policy loss: 8.034983591127975
Experience 15, Iter 71, disc loss: 0.0011506474934120875, policy loss: 8.044618202315126
Experience 15, Iter 72, disc loss: 0.0013373844107982143, policy loss: 7.820617927675262
Experience 15, Iter 73, disc loss: 0.0011271178114884198, policy loss: 7.828186618721471
Experience 15, Iter 74, disc loss: 0.001125151276073023, policy loss: 8.048886209174999
Experience 15, Iter 75, disc loss: 0.001248377602491304, policy loss: 7.655617673238071
Experience 15, Iter 76, disc loss: 0.0007463435897289512, policy loss: 8.358753070894164
Experience 15, Iter 77, disc loss: 0.0010560621157354273, policy loss: 7.921809164446798
Experience 15, Iter 78, disc loss: 0.0009471958966988822, policy loss: 8.009174356957864
Experience 15, Iter 79, disc loss: 0.0012451122878628271, policy loss: 7.716062490696221
Experience 15, Iter 80, disc loss: 0.0011739655523891467, policy loss: 7.787700788553531
Experience 15, Iter 81, disc loss: 0.000951192417166891, policy loss: 7.965278529901534
Experience 15, Iter 82, disc loss: 0.0011726316834690554, policy loss: 7.7142908126358245
Experience 15, Iter 83, disc loss: 0.0011389302680033441, policy loss: 7.857528152187738
Experience 15, Iter 84, disc loss: 0.001002660398342977, policy loss: 7.964940185827182
Experience 15, Iter 85, disc loss: 0.0010356125857238435, policy loss: 7.840614813864873
Experience 15, Iter 86, disc loss: 0.001167898254924145, policy loss: 7.817593494062818
Experience 15, Iter 87, disc loss: 0.0012813964705678045, policy loss: 7.682565937640185
Experience 15, Iter 88, disc loss: 0.0010953047933262177, policy loss: 7.920993146220351
Experience 15, Iter 89, disc loss: 0.001014705960314144, policy loss: 8.250723013944576
Experience 15, Iter 90, disc loss: 0.0008314640409460549, policy loss: 7.961906821923169
Experience 15, Iter 91, disc loss: 0.0008653996590074542, policy loss: 8.147621512076405
Experience 15, Iter 92, disc loss: 0.0008097618572104083, policy loss: 8.131510062768772
Experience 15, Iter 93, disc loss: 0.000995478211131791, policy loss: 7.9261745275780395
Experience 15, Iter 94, disc loss: 0.0008034749106126826, policy loss: 8.195370267471727
Experience 15, Iter 95, disc loss: 0.0007544818971325844, policy loss: 8.424870747543181
Experience 15, Iter 96, disc loss: 0.0010553301996472585, policy loss: 8.0321441617934
Experience 15, Iter 97, disc loss: 0.0011021614447020182, policy loss: 7.957355480064412
Experience 15, Iter 98, disc loss: 0.0008885978529658387, policy loss: 8.038816393368052
Experience 15, Iter 99, disc loss: 0.0010013418908657438, policy loss: 7.742115940344675
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0993],
        [1.0300],
        [0.0057]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.7043e-02, 1.3565e-01, 3.4089e-01, 7.7622e-03, 1.3778e-03,
          2.0201e+00]],

        [[1.7043e-02, 1.3565e-01, 3.4089e-01, 7.7622e-03, 1.3778e-03,
          2.0201e+00]],

        [[1.7043e-02, 1.3565e-01, 3.4089e-01, 7.7622e-03, 1.3778e-03,
          2.0201e+00]],

        [[1.7043e-02, 1.3565e-01, 3.4089e-01, 7.7622e-03, 1.3778e-03,
          2.0201e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0138, 0.3971, 4.1198, 0.0229], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0138, 0.3971, 4.1198, 0.0229])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.906
Iter 2/2000 - Loss: 2.234
Iter 3/2000 - Loss: 1.868
Iter 4/2000 - Loss: 1.901
Iter 5/2000 - Loss: 1.997
Iter 6/2000 - Loss: 1.897
Iter 7/2000 - Loss: 1.752
Iter 8/2000 - Loss: 1.681
Iter 9/2000 - Loss: 1.666
Iter 10/2000 - Loss: 1.624
Iter 11/2000 - Loss: 1.513
Iter 12/2000 - Loss: 1.361
Iter 13/2000 - Loss: 1.204
Iter 14/2000 - Loss: 1.056
Iter 15/2000 - Loss: 0.904
Iter 16/2000 - Loss: 0.729
Iter 17/2000 - Loss: 0.520
Iter 18/2000 - Loss: 0.279
Iter 19/2000 - Loss: 0.019
Iter 20/2000 - Loss: -0.251
Iter 1981/2000 - Loss: -8.623
Iter 1982/2000 - Loss: -8.623
Iter 1983/2000 - Loss: -8.623
Iter 1984/2000 - Loss: -8.623
Iter 1985/2000 - Loss: -8.623
Iter 1986/2000 - Loss: -8.623
Iter 1987/2000 - Loss: -8.623
Iter 1988/2000 - Loss: -8.623
Iter 1989/2000 - Loss: -8.623
Iter 1990/2000 - Loss: -8.623
Iter 1991/2000 - Loss: -8.623
Iter 1992/2000 - Loss: -8.623
Iter 1993/2000 - Loss: -8.623
Iter 1994/2000 - Loss: -8.623
Iter 1995/2000 - Loss: -8.623
Iter 1996/2000 - Loss: -8.624
Iter 1997/2000 - Loss: -8.624
Iter 1998/2000 - Loss: -8.624
Iter 1999/2000 - Loss: -8.624
Iter 2000/2000 - Loss: -8.624
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[15.5366, 10.9646, 35.8972, 11.1597, 13.6599, 48.1906]],

        [[20.3230, 37.6593, 12.0758,  1.6230,  2.0863, 20.9295]],

        [[21.4890, 33.4836, 15.6766,  0.8604,  6.0158, 15.1795]],

        [[18.9780, 35.8670, 14.5366,  2.4705,  6.1063, 36.5072]]])
Signal Variance: tensor([ 0.1701,  2.2610, 14.0462,  0.2673])
Estimated target variance: tensor([0.0138, 0.3971, 4.1198, 0.0229])
N: 160
Signal to noise ratio: tensor([24.0755, 85.5254, 96.1411, 36.4005])
Bound on condition number: tensor([  92741.4123, 1170337.1602, 1478899.9050,  212000.9253])
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 0.0011990269991021315, policy loss: 7.940798440536822
Experience 16, Iter 1, disc loss: 0.0010769874227105035, policy loss: 8.011679539869807
Experience 16, Iter 2, disc loss: 0.0010265020784622774, policy loss: 8.096869966170457
Experience 16, Iter 3, disc loss: 0.0008541062710011377, policy loss: 8.082789107131624
Experience 16, Iter 4, disc loss: 0.00118838618086943, policy loss: 7.666667851870445
Experience 16, Iter 5, disc loss: 0.0009878890274957557, policy loss: 7.967842093120455
Experience 16, Iter 6, disc loss: 0.0008666945728449677, policy loss: 8.121908625583027
Experience 16, Iter 7, disc loss: 0.0010404471910004814, policy loss: 7.93156906129832
Experience 16, Iter 8, disc loss: 0.0010338103398636485, policy loss: 8.112967646302966
Experience 16, Iter 9, disc loss: 0.0011160062393004804, policy loss: 8.038751115316526
Experience 16, Iter 10, disc loss: 0.000853945713601191, policy loss: 8.054658239520943
Experience 16, Iter 11, disc loss: 0.0008938527276967764, policy loss: 7.832781754412762
Experience 16, Iter 12, disc loss: 0.0009768714257234205, policy loss: 8.01795483458787
Experience 16, Iter 13, disc loss: 0.0008515159584646212, policy loss: 8.109343406159264
Experience 16, Iter 14, disc loss: 0.0010677006109573635, policy loss: 7.750327375574485
Experience 16, Iter 15, disc loss: 0.0011752757209519973, policy loss: 7.876073780966712
Experience 16, Iter 16, disc loss: 0.000946912513754659, policy loss: 8.102235274454504
Experience 16, Iter 17, disc loss: 0.0009086539079062126, policy loss: 8.10137587121274
Experience 16, Iter 18, disc loss: 0.0007771397873393611, policy loss: 8.18886527308388
Experience 16, Iter 19, disc loss: 0.0009592496517166912, policy loss: 8.29187971539984
Experience 16, Iter 20, disc loss: 0.0010014675674148933, policy loss: 7.894532836700525
Experience 16, Iter 21, disc loss: 0.0009312349382949395, policy loss: 8.099120908453285
Experience 16, Iter 22, disc loss: 0.0013613606303728148, policy loss: 7.631900290298029
Experience 16, Iter 23, disc loss: 0.000993521096505066, policy loss: 7.9051594744883324
Experience 16, Iter 24, disc loss: 0.0010640426916369537, policy loss: 8.005000311103485
Experience 16, Iter 25, disc loss: 0.0008081028510780677, policy loss: 8.531801666919074
Experience 16, Iter 26, disc loss: 0.0008673862404447235, policy loss: 8.290114909833981
Experience 16, Iter 27, disc loss: 0.0012471619223481064, policy loss: 7.664765737278629
Experience 16, Iter 28, disc loss: 0.0008713767673173482, policy loss: 8.100712753231297
Experience 16, Iter 29, disc loss: 0.0009671014237181185, policy loss: 8.141750400241897
Experience 16, Iter 30, disc loss: 0.000855753188538659, policy loss: 8.358841046141453
Experience 16, Iter 31, disc loss: 0.0009754164114111534, policy loss: 7.9348564387182225
Experience 16, Iter 32, disc loss: 0.0011420285994253438, policy loss: 8.02814213278967
Experience 16, Iter 33, disc loss: 0.0009103530008716705, policy loss: 8.044328528518202
Experience 16, Iter 34, disc loss: 0.0009626334763619557, policy loss: 8.07510573710768
Experience 16, Iter 35, disc loss: 0.0009571885239789555, policy loss: 7.957419951124452
Experience 16, Iter 36, disc loss: 0.0011559444483758546, policy loss: 7.849723168832564
Experience 16, Iter 37, disc loss: 0.0012465112937831192, policy loss: 7.784628697518865
Experience 16, Iter 38, disc loss: 0.0009227175113660509, policy loss: 7.996876607405731
Experience 16, Iter 39, disc loss: 0.000956692200110455, policy loss: 8.046308549924154
Experience 16, Iter 40, disc loss: 0.0009481638692244441, policy loss: 8.235450434560049
Experience 16, Iter 41, disc loss: 0.0008832080073141927, policy loss: 8.056142693781657
Experience 16, Iter 42, disc loss: 0.0015473588643091045, policy loss: 7.583424415700112
Experience 16, Iter 43, disc loss: 0.0010016818962481039, policy loss: 8.184361312300721
Experience 16, Iter 44, disc loss: 0.0010297176314772087, policy loss: 8.016649629850525
Experience 16, Iter 45, disc loss: 0.0012253234308844414, policy loss: 7.747641989401943
Experience 16, Iter 46, disc loss: 0.0009502203461858599, policy loss: 8.037710555228989
Experience 16, Iter 47, disc loss: 0.0007549200743430478, policy loss: 8.281735943373489
Experience 16, Iter 48, disc loss: 0.0007931015543059796, policy loss: 8.10684241943525
Experience 16, Iter 49, disc loss: 0.000979313628225218, policy loss: 8.198780594704298
Experience 16, Iter 50, disc loss: 0.0009239506521558667, policy loss: 8.283999526118935
Experience 16, Iter 51, disc loss: 0.0005564846540659118, policy loss: 8.662712440580933
Experience 16, Iter 52, disc loss: 0.0009388622855313291, policy loss: 8.251717853091499
Experience 16, Iter 53, disc loss: 0.001039047445261613, policy loss: 7.889179272970708
Experience 16, Iter 54, disc loss: 0.000784186447548642, policy loss: 8.410865426744515
Experience 16, Iter 55, disc loss: 0.000462993037049592, policy loss: 9.095308617676608
Experience 16, Iter 56, disc loss: 0.0006132581729944524, policy loss: 8.533739322193288
Experience 16, Iter 57, disc loss: 0.0006650148184179342, policy loss: 8.381306169048742
Experience 16, Iter 58, disc loss: 0.0006308247712075389, policy loss: 8.319996762534192
Experience 16, Iter 59, disc loss: 0.0009434346239893269, policy loss: 8.189638188605741
Experience 16, Iter 60, disc loss: 0.0007466533470994146, policy loss: 8.398516164922018
Experience 16, Iter 61, disc loss: 0.0007131191729552646, policy loss: 8.258710988986607
Experience 16, Iter 62, disc loss: 0.0007580839287985479, policy loss: 8.186007826783953
Experience 16, Iter 63, disc loss: 0.0006320317048600905, policy loss: 8.478582977655265
Experience 16, Iter 64, disc loss: 0.0008925909854351376, policy loss: 8.307342508424039
Experience 16, Iter 65, disc loss: 0.0009007216712340031, policy loss: 7.994631933069761
Experience 16, Iter 66, disc loss: 0.0009287069956777374, policy loss: 8.098390759063214
Experience 16, Iter 67, disc loss: 0.0007530143144443536, policy loss: 8.177282183238852
Experience 16, Iter 68, disc loss: 0.0009476879902683382, policy loss: 8.163517030082355
Experience 16, Iter 69, disc loss: 0.0007976776124195164, policy loss: 8.411231815707698
Experience 16, Iter 70, disc loss: 0.0007651770154729922, policy loss: 8.186379351998749
Experience 16, Iter 71, disc loss: 0.0007489114852751814, policy loss: 8.276513744201846
Experience 16, Iter 72, disc loss: 0.0006566269121615561, policy loss: 8.567181927011783
Experience 16, Iter 73, disc loss: 0.0006838968647778906, policy loss: 8.438656580020057
Experience 16, Iter 74, disc loss: 0.000920397382690105, policy loss: 8.157633143594914
Experience 16, Iter 75, disc loss: 0.0008531068789781537, policy loss: 8.18587013460086
Experience 16, Iter 76, disc loss: 0.000922687228497887, policy loss: 8.13591102274134
Experience 16, Iter 77, disc loss: 0.0008929487303761083, policy loss: 7.925556808569164
Experience 16, Iter 78, disc loss: 0.0009310752822219431, policy loss: 8.094357082843358
Experience 16, Iter 79, disc loss: 0.0009789598378191046, policy loss: 8.055750705032388
Experience 16, Iter 80, disc loss: 0.0009670919745728466, policy loss: 8.142490693804977
Experience 16, Iter 81, disc loss: 0.0010420142894238499, policy loss: 8.025317999923864
Experience 16, Iter 82, disc loss: 0.0006952726674709532, policy loss: 8.48515293574704
Experience 16, Iter 83, disc loss: 0.0007112334157866234, policy loss: 8.23008920779787
Experience 16, Iter 84, disc loss: 0.0006866737368937823, policy loss: 8.477689612552712
Experience 16, Iter 85, disc loss: 0.0006386178399884825, policy loss: 8.434077818683775
Experience 16, Iter 86, disc loss: 0.000611644994572855, policy loss: 8.64259667700154
Experience 16, Iter 87, disc loss: 0.000744943023474552, policy loss: 8.25762706477109
Experience 16, Iter 88, disc loss: 0.0007171942775407278, policy loss: 8.267359090964364
Experience 16, Iter 89, disc loss: 0.0006915910496777725, policy loss: 8.291873601823673
Experience 16, Iter 90, disc loss: 0.000635822662813225, policy loss: 8.332545055475288
Experience 16, Iter 91, disc loss: 0.0007277683851211713, policy loss: 8.163444523988804
Experience 16, Iter 92, disc loss: 0.0006868735555246059, policy loss: 8.230568775609688
Experience 16, Iter 93, disc loss: 0.0008477248744030378, policy loss: 8.076953298197386
Experience 16, Iter 94, disc loss: 0.0006538336055618405, policy loss: 8.264097972729433
Experience 16, Iter 95, disc loss: 0.0008823473254254777, policy loss: 8.08844677349349
Experience 16, Iter 96, disc loss: 0.0007923394973140588, policy loss: 8.155749073032085
Experience 16, Iter 97, disc loss: 0.0009168782829464961, policy loss: 8.027284637472178
Experience 16, Iter 98, disc loss: 0.0008894173576309901, policy loss: 8.1303932898422
Experience 16, Iter 99, disc loss: 0.0008630933638697915, policy loss: 8.096335261407445
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1029],
        [1.0762],
        [0.0059]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.6162e-02, 1.2976e-01, 3.5217e-01, 7.6175e-03, 1.3028e-03,
          2.0584e+00]],

        [[1.6162e-02, 1.2976e-01, 3.5217e-01, 7.6175e-03, 1.3028e-03,
          2.0584e+00]],

        [[1.6162e-02, 1.2976e-01, 3.5217e-01, 7.6175e-03, 1.3028e-03,
          2.0584e+00]],

        [[1.6162e-02, 1.2976e-01, 3.5217e-01, 7.6175e-03, 1.3028e-03,
          2.0584e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0131, 0.4116, 4.3047, 0.0235], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0131, 0.4116, 4.3047, 0.0235])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.937
Iter 2/2000 - Loss: 2.283
Iter 3/2000 - Loss: 1.903
Iter 4/2000 - Loss: 1.943
Iter 5/2000 - Loss: 2.048
Iter 6/2000 - Loss: 1.950
Iter 7/2000 - Loss: 1.806
Iter 8/2000 - Loss: 1.739
Iter 9/2000 - Loss: 1.733
Iter 10/2000 - Loss: 1.702
Iter 11/2000 - Loss: 1.600
Iter 12/2000 - Loss: 1.454
Iter 13/2000 - Loss: 1.303
Iter 14/2000 - Loss: 1.162
Iter 15/2000 - Loss: 1.017
Iter 16/2000 - Loss: 0.847
Iter 17/2000 - Loss: 0.639
Iter 18/2000 - Loss: 0.398
Iter 19/2000 - Loss: 0.134
Iter 20/2000 - Loss: -0.141
Iter 1981/2000 - Loss: -8.683
Iter 1982/2000 - Loss: -8.683
Iter 1983/2000 - Loss: -8.683
Iter 1984/2000 - Loss: -8.683
Iter 1985/2000 - Loss: -8.683
Iter 1986/2000 - Loss: -8.683
Iter 1987/2000 - Loss: -8.684
Iter 1988/2000 - Loss: -8.684
Iter 1989/2000 - Loss: -8.684
Iter 1990/2000 - Loss: -8.684
Iter 1991/2000 - Loss: -8.684
Iter 1992/2000 - Loss: -8.684
Iter 1993/2000 - Loss: -8.684
Iter 1994/2000 - Loss: -8.684
Iter 1995/2000 - Loss: -8.684
Iter 1996/2000 - Loss: -8.684
Iter 1997/2000 - Loss: -8.684
Iter 1998/2000 - Loss: -8.684
Iter 1999/2000 - Loss: -8.684
Iter 2000/2000 - Loss: -8.684
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[15.3704, 11.1296, 35.6497, 11.1716, 13.2772, 48.7439]],

        [[19.4135, 37.8211, 12.1144,  1.5594,  2.3374, 21.5222]],

        [[20.5712, 31.5760, 12.3249,  0.9818,  5.2658, 21.2023]],

        [[18.1354, 34.5421, 15.2355,  2.7660,  3.3603, 38.8779]]])
Signal Variance: tensor([ 0.1701,  2.2591, 20.4299,  0.3027])
Estimated target variance: tensor([0.0131, 0.4116, 4.3047, 0.0235])
N: 170
Signal to noise ratio: tensor([ 24.3397,  86.4086, 113.3940,  39.0964])
Bound on condition number: tensor([ 100712.7732, 1269295.7611, 2185895.1810,  259851.1411])
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 0.0008023108232252458, policy loss: 8.266553325500404
Experience 17, Iter 1, disc loss: 0.0007111353703287169, policy loss: 8.25813739962788
Experience 17, Iter 2, disc loss: 0.0008075907760073816, policy loss: 8.3591904561424
Experience 17, Iter 3, disc loss: 0.0008310228508550226, policy loss: 7.99325534284117
Experience 17, Iter 4, disc loss: 0.0009178196922463566, policy loss: 8.036429843390895
Experience 17, Iter 5, disc loss: 0.0009857739141762784, policy loss: 8.062688003819932
Experience 17, Iter 6, disc loss: 0.0007631938994973121, policy loss: 8.122091328357834
Experience 17, Iter 7, disc loss: 0.0007056718714137069, policy loss: 8.25532269134879
Experience 17, Iter 8, disc loss: 0.0006901209656134677, policy loss: 8.278750679587093
Experience 17, Iter 9, disc loss: 0.0006069631001722081, policy loss: 8.289374430001445
Experience 17, Iter 10, disc loss: 0.0007084359306237143, policy loss: 8.190243842036473
Experience 17, Iter 11, disc loss: 0.0007102348117390532, policy loss: 8.220209368526088
Experience 17, Iter 12, disc loss: 0.0006417648614561378, policy loss: 8.335701740862019
Experience 17, Iter 13, disc loss: 0.0007369508312402515, policy loss: 8.087010836070629
Experience 17, Iter 14, disc loss: 0.0007100895482642059, policy loss: 8.09842466708654
Experience 17, Iter 15, disc loss: 0.0007001643592308765, policy loss: 8.320876222999834
Experience 17, Iter 16, disc loss: 0.0007035807850623787, policy loss: 8.2539420640963
Experience 17, Iter 17, disc loss: 0.0005705315413654996, policy loss: 8.45312475132409
Experience 17, Iter 18, disc loss: 0.0006511182257745269, policy loss: 8.296672465519832
Experience 17, Iter 19, disc loss: 0.000981115493600293, policy loss: 7.991208034318646
Experience 17, Iter 20, disc loss: 0.0009357613092491334, policy loss: 8.169730377356423
Experience 17, Iter 21, disc loss: 0.0007539484007816401, policy loss: 8.291498655053747
Experience 17, Iter 22, disc loss: 0.0006307905867404817, policy loss: 8.501330605946377
Experience 17, Iter 23, disc loss: 0.0009181175333096603, policy loss: 8.03923845896022
Experience 17, Iter 24, disc loss: 0.0007788548932349402, policy loss: 8.14804535709829
Experience 17, Iter 25, disc loss: 0.0007795825169332672, policy loss: 8.129664693516833
Experience 17, Iter 26, disc loss: 0.0008679307248392611, policy loss: 8.118479882989497
Experience 17, Iter 27, disc loss: 0.0008226232548779017, policy loss: 8.28303805192963
Experience 17, Iter 28, disc loss: 0.0008492083948962166, policy loss: 8.085288447643991
Experience 17, Iter 29, disc loss: 0.0007584488913501986, policy loss: 8.396617358760636
Experience 17, Iter 30, disc loss: 0.0006966708667814474, policy loss: 8.354243847888444
Experience 17, Iter 31, disc loss: 0.0009565767842735503, policy loss: 7.943974132342347
Experience 17, Iter 32, disc loss: 0.000668125260062086, policy loss: 8.297745278161425
Experience 17, Iter 33, disc loss: 0.0009049296942798871, policy loss: 8.06941819026571
Experience 17, Iter 34, disc loss: 0.000941695499963744, policy loss: 8.220813661382977
Experience 17, Iter 35, disc loss: 0.000916884609265389, policy loss: 8.159647105694543
Experience 17, Iter 36, disc loss: 0.0008136329517106672, policy loss: 8.352713919381337
Experience 17, Iter 37, disc loss: 0.0008428020596084767, policy loss: 8.08221910365291
Experience 17, Iter 38, disc loss: 0.0009123379097218864, policy loss: 8.187213839788756
Experience 17, Iter 39, disc loss: 0.001064943779026418, policy loss: 8.009651664045604
Experience 17, Iter 40, disc loss: 0.0007074050044055225, policy loss: 8.212999478770334
Experience 17, Iter 41, disc loss: 0.0006441358570262034, policy loss: 8.35943498203087
Experience 17, Iter 42, disc loss: 0.00071771907413823, policy loss: 8.322735217384718
Experience 17, Iter 43, disc loss: 0.0008279129807207037, policy loss: 7.992344298030799
Experience 17, Iter 44, disc loss: 0.0008334513673608731, policy loss: 8.179333053211568
Experience 17, Iter 45, disc loss: 0.0007941385813887695, policy loss: 8.302936916957371
Experience 17, Iter 46, disc loss: 0.0007512561102793497, policy loss: 8.297040072110086
Experience 17, Iter 47, disc loss: 0.0008839611302763359, policy loss: 8.04201216326455
Experience 17, Iter 48, disc loss: 0.0008093172365540048, policy loss: 8.217915582931813
Experience 17, Iter 49, disc loss: 0.0007612764242823818, policy loss: 8.146443981714834
Experience 17, Iter 50, disc loss: 0.0006018703063009291, policy loss: 8.334205909807084
Experience 17, Iter 51, disc loss: 0.0008209639032733841, policy loss: 8.187749917518314
Experience 17, Iter 52, disc loss: 0.0006120952106079985, policy loss: 8.366802564025987
Experience 17, Iter 53, disc loss: 0.0009259095608186028, policy loss: 8.203571988810047
Experience 17, Iter 54, disc loss: 0.0008333008887603812, policy loss: 8.219191631877951
Experience 17, Iter 55, disc loss: 0.0009101712839758685, policy loss: 8.272094971084893
Experience 17, Iter 56, disc loss: 0.0008947362666003246, policy loss: 8.203613281288678
Experience 17, Iter 57, disc loss: 0.0008734667196433189, policy loss: 8.061231132128336
Experience 17, Iter 58, disc loss: 0.0008496574947593978, policy loss: 8.149387823926725
Experience 17, Iter 59, disc loss: 0.0007765127370443462, policy loss: 8.20863364653595
Experience 17, Iter 60, disc loss: 0.0007353499061942354, policy loss: 8.331230380381466
Experience 17, Iter 61, disc loss: 0.0004869858577131667, policy loss: 8.662378039634246
Experience 17, Iter 62, disc loss: 0.0008405853557297831, policy loss: 8.243964351574885
Experience 17, Iter 63, disc loss: 0.0008474895222591648, policy loss: 7.984100426972267
Experience 17, Iter 64, disc loss: 0.0005407132330989487, policy loss: 8.56288898417724
Experience 17, Iter 65, disc loss: 0.0008677450775991542, policy loss: 8.059442413561685
Experience 17, Iter 66, disc loss: 0.0007659737810919412, policy loss: 8.284508746899654
Experience 17, Iter 67, disc loss: 0.0006640482392880284, policy loss: 8.572882773207299
Experience 17, Iter 68, disc loss: 0.0006141611658965604, policy loss: 8.55880560696614
Experience 17, Iter 69, disc loss: 0.0006913783443398958, policy loss: 8.382139648082125
Experience 17, Iter 70, disc loss: 0.0007470990375187855, policy loss: 8.308191458759659
Experience 17, Iter 71, disc loss: 0.0006984434709030203, policy loss: 8.336415657468933
Experience 17, Iter 72, disc loss: 0.0005169648160600114, policy loss: 8.575960797112991
Experience 17, Iter 73, disc loss: 0.0006546101746281771, policy loss: 8.542240085618872
Experience 17, Iter 74, disc loss: 0.001100110198540876, policy loss: 8.083383784343773
Experience 17, Iter 75, disc loss: 0.0009604573149896288, policy loss: 8.22368419448176
Experience 17, Iter 76, disc loss: 0.0005432600786221451, policy loss: 8.513111716466586
Experience 17, Iter 77, disc loss: 0.0006043225019783116, policy loss: 8.343334535206925
Experience 17, Iter 78, disc loss: 0.0005404004791100133, policy loss: 8.634908405577425
Experience 17, Iter 79, disc loss: 0.0007589290712550177, policy loss: 8.44998756226483
Experience 17, Iter 80, disc loss: 0.0008200667141961559, policy loss: 8.096414247134463
Experience 17, Iter 81, disc loss: 0.0007628756537517282, policy loss: 8.25165387338433
Experience 17, Iter 82, disc loss: 0.0006445842613532156, policy loss: 8.436669654589522
Experience 17, Iter 83, disc loss: 0.0009119291358646926, policy loss: 8.220976014132193
Experience 17, Iter 84, disc loss: 0.000688188848532559, policy loss: 8.61256167814265
Experience 17, Iter 85, disc loss: 0.0006484061464193307, policy loss: 8.335515163302103
Experience 17, Iter 86, disc loss: 0.0006906199628502793, policy loss: 8.149808563431193
Experience 17, Iter 87, disc loss: 0.000532220271114689, policy loss: 8.46014636999232
Experience 17, Iter 88, disc loss: 0.0005933220268138785, policy loss: 8.500671543710123
Experience 17, Iter 89, disc loss: 0.0005423960309376739, policy loss: 8.433903337960873
Experience 17, Iter 90, disc loss: 0.0005805692999558815, policy loss: 8.36603031153474
Experience 17, Iter 91, disc loss: 0.0006571062976287032, policy loss: 8.318782781018129
Experience 17, Iter 92, disc loss: 0.0006400532284893901, policy loss: 8.457856592405314
Experience 17, Iter 93, disc loss: 0.0007371796071098216, policy loss: 8.40589336536818
Experience 17, Iter 94, disc loss: 0.0005454221747453903, policy loss: 8.5449579121979
Experience 17, Iter 95, disc loss: 0.0008196592150801764, policy loss: 8.286881364585266
Experience 17, Iter 96, disc loss: 0.0007184928962157711, policy loss: 8.259135637941364
Experience 17, Iter 97, disc loss: 0.0005879336923970169, policy loss: 8.539725151431595
Experience 17, Iter 98, disc loss: 0.0006838710391206106, policy loss: 8.507231851669376
Experience 17, Iter 99, disc loss: 0.0005805731045574574, policy loss: 8.386650291935256
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.1015],
        [1.0665],
        [0.0057]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.5265e-02, 1.2351e-01, 3.4591e-01, 7.2785e-03, 1.2370e-03,
          2.0110e+00]],

        [[1.5265e-02, 1.2351e-01, 3.4591e-01, 7.2785e-03, 1.2370e-03,
          2.0110e+00]],

        [[1.5265e-02, 1.2351e-01, 3.4591e-01, 7.2785e-03, 1.2370e-03,
          2.0110e+00]],

        [[1.5265e-02, 1.2351e-01, 3.4591e-01, 7.2785e-03, 1.2370e-03,
          2.0110e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0124, 0.4060, 4.2661, 0.0229], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0124, 0.4060, 4.2661, 0.0229])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.885
Iter 2/2000 - Loss: 2.254
Iter 3/2000 - Loss: 1.855
Iter 4/2000 - Loss: 1.898
Iter 5/2000 - Loss: 2.012
Iter 6/2000 - Loss: 1.913
Iter 7/2000 - Loss: 1.764
Iter 8/2000 - Loss: 1.692
Iter 9/2000 - Loss: 1.686
Iter 10/2000 - Loss: 1.657
Iter 11/2000 - Loss: 1.557
Iter 12/2000 - Loss: 1.408
Iter 13/2000 - Loss: 1.251
Iter 14/2000 - Loss: 1.103
Iter 15/2000 - Loss: 0.954
Iter 16/2000 - Loss: 0.781
Iter 17/2000 - Loss: 0.571
Iter 18/2000 - Loss: 0.326
Iter 19/2000 - Loss: 0.056
Iter 20/2000 - Loss: -0.226
Iter 1981/2000 - Loss: -8.788
Iter 1982/2000 - Loss: -8.788
Iter 1983/2000 - Loss: -8.788
Iter 1984/2000 - Loss: -8.788
Iter 1985/2000 - Loss: -8.788
Iter 1986/2000 - Loss: -8.788
Iter 1987/2000 - Loss: -8.788
Iter 1988/2000 - Loss: -8.788
Iter 1989/2000 - Loss: -8.788
Iter 1990/2000 - Loss: -8.788
Iter 1991/2000 - Loss: -8.788
Iter 1992/2000 - Loss: -8.788
Iter 1993/2000 - Loss: -8.788
Iter 1994/2000 - Loss: -8.788
Iter 1995/2000 - Loss: -8.788
Iter 1996/2000 - Loss: -8.788
Iter 1997/2000 - Loss: -8.788
Iter 1998/2000 - Loss: -8.788
Iter 1999/2000 - Loss: -8.788
Iter 2000/2000 - Loss: -8.788
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[14.8725, 10.9665, 34.8963, 10.8554, 12.9614, 48.0812]],

        [[18.9058, 36.5281, 11.9531,  1.5696,  2.4656, 22.4371]],

        [[20.1178, 30.2771, 12.3234,  0.9839,  5.1921, 21.9881]],

        [[17.7211, 33.8628, 16.1963,  3.1995,  2.6573, 40.3978]]])
Signal Variance: tensor([ 0.1678,  2.3782, 21.1039,  0.3419])
Estimated target variance: tensor([0.0124, 0.4060, 4.2661, 0.0229])
N: 180
Signal to noise ratio: tensor([ 24.4326,  90.2747, 117.1575,  41.7515])
Bound on condition number: tensor([ 107452.3453, 1466914.9475, 2470661.1132,  313774.9446])
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 0.0004910871119620055, policy loss: 8.672109261625778
Experience 18, Iter 1, disc loss: 0.0007629490919211731, policy loss: 8.127250314760769
Experience 18, Iter 2, disc loss: 0.0007769834229364944, policy loss: 8.189388367124693
Experience 18, Iter 3, disc loss: 0.0006933842439156513, policy loss: 8.367486457949774
Experience 18, Iter 4, disc loss: 0.0009085788664392564, policy loss: 8.08230421406632
Experience 18, Iter 5, disc loss: 0.0007094077514083763, policy loss: 8.28451614850944
Experience 18, Iter 6, disc loss: 0.0008213029488956851, policy loss: 8.257884096335292
Experience 18, Iter 7, disc loss: 0.0005296145233191077, policy loss: 8.51521848897232
Experience 18, Iter 8, disc loss: 0.000743996451023533, policy loss: 8.304553286147971
Experience 18, Iter 9, disc loss: 0.0007322165991540227, policy loss: 8.080987288018811
Experience 18, Iter 10, disc loss: 0.0007901240649764874, policy loss: 8.151991779673564
Experience 18, Iter 11, disc loss: 0.0005999380580546921, policy loss: 8.6047467330279
Experience 18, Iter 12, disc loss: 0.000783946179881123, policy loss: 8.472473098590635
Experience 18, Iter 13, disc loss: 0.0007953762310140935, policy loss: 8.214293057890751
Experience 18, Iter 14, disc loss: 0.0007362292097286606, policy loss: 8.239249346413656
Experience 18, Iter 15, disc loss: 0.0010632146585081098, policy loss: 7.979564482633431
Experience 18, Iter 16, disc loss: 0.0009687398223156252, policy loss: 8.182574026227822
Experience 18, Iter 17, disc loss: 0.0008352844712096614, policy loss: 8.11287785675821
Experience 18, Iter 18, disc loss: 0.0006561980990829276, policy loss: 8.316808284114542
Experience 18, Iter 19, disc loss: 0.0009187850181854296, policy loss: 8.101755246903632
Experience 18, Iter 20, disc loss: 0.0006750451824760965, policy loss: 8.414114376555087
Experience 18, Iter 21, disc loss: 0.0008209404927596444, policy loss: 8.353412791237321
Experience 18, Iter 22, disc loss: 0.0005200735971054931, policy loss: 8.555546569314984
Experience 18, Iter 23, disc loss: 0.0010732852460518386, policy loss: 8.034571903793598
Experience 18, Iter 24, disc loss: 0.000964518102903042, policy loss: 7.971163359547562
Experience 18, Iter 25, disc loss: 0.0007143882169261466, policy loss: 8.413125994109445
Experience 18, Iter 26, disc loss: 0.0006431454821845401, policy loss: 8.300168429819369
Experience 18, Iter 27, disc loss: 0.0007952515031498608, policy loss: 8.263304355658258
Experience 18, Iter 28, disc loss: 0.0008336613307277334, policy loss: 7.968476055634497
Experience 18, Iter 29, disc loss: 0.0007821777495277229, policy loss: 8.308463790814933
Experience 18, Iter 30, disc loss: 0.0006957607378318721, policy loss: 8.457124949870899
Experience 18, Iter 31, disc loss: 0.0007331042282886281, policy loss: 8.269784397378753
Experience 18, Iter 32, disc loss: 0.000609277048992611, policy loss: 8.459861112389852
Experience 18, Iter 33, disc loss: 0.0010372480838147168, policy loss: 8.018616328999478
Experience 18, Iter 34, disc loss: 0.0009166088222188297, policy loss: 8.241294839526642
Experience 18, Iter 35, disc loss: 0.0006793443257253646, policy loss: 8.305157411629326
Experience 18, Iter 36, disc loss: 0.0006714896631134953, policy loss: 8.73471496109987
Experience 18, Iter 37, disc loss: 0.0009767771042362295, policy loss: 8.144475268594185
Experience 18, Iter 38, disc loss: 0.0006979216081490827, policy loss: 8.439323431924262
Experience 18, Iter 39, disc loss: 0.00048818030372850616, policy loss: 8.655333938141144
Experience 18, Iter 40, disc loss: 0.0006645299080169046, policy loss: 8.42926628509672
Experience 18, Iter 41, disc loss: 0.0006134597374303486, policy loss: 8.48240521701296
Experience 18, Iter 42, disc loss: 0.0006184678308697995, policy loss: 8.416818229922853
Experience 18, Iter 43, disc loss: 0.0008658542718893395, policy loss: 8.359276545900979
Experience 18, Iter 44, disc loss: 0.0008090838243753457, policy loss: 8.180505205328483
Experience 18, Iter 45, disc loss: 0.0011710859937704484, policy loss: 8.142951990866258
Experience 18, Iter 46, disc loss: 0.0009087630760526392, policy loss: 8.033491544386285
Experience 18, Iter 47, disc loss: 0.0006217798273325703, policy loss: 8.309910267641587
Experience 18, Iter 48, disc loss: 0.0008159243734110263, policy loss: 8.324500442196669
Experience 18, Iter 49, disc loss: 0.0006981093648716907, policy loss: 8.4625455682203
Experience 18, Iter 50, disc loss: 0.0009116053851848977, policy loss: 8.224505703288981
Experience 18, Iter 51, disc loss: 0.0006545496494722683, policy loss: 8.39560002496046
Experience 18, Iter 52, disc loss: 0.0008219564782477632, policy loss: 8.146554472948061
Experience 18, Iter 53, disc loss: 0.0005479479477659765, policy loss: 8.626901364428562
Experience 18, Iter 54, disc loss: 0.0006495548337973099, policy loss: 8.74589581441354
Experience 18, Iter 55, disc loss: 0.0006002175885240102, policy loss: 8.7448065539413
Experience 18, Iter 56, disc loss: 0.0008172752851776069, policy loss: 8.21892557246668
Experience 18, Iter 57, disc loss: 0.0006813988199904652, policy loss: 8.319039452351529
Experience 18, Iter 58, disc loss: 0.000530705611105036, policy loss: 8.639835327921977
Experience 18, Iter 59, disc loss: 0.0005877353609164177, policy loss: 8.374004370664647
Experience 18, Iter 60, disc loss: 0.0006467121045366523, policy loss: 8.3650971739333
Experience 18, Iter 61, disc loss: 0.0008417602643390176, policy loss: 8.256753838822423
Experience 18, Iter 62, disc loss: 0.0007304346197265386, policy loss: 8.302542900521445
Experience 18, Iter 63, disc loss: 0.000658983010723365, policy loss: 8.58497304687052
Experience 18, Iter 64, disc loss: 0.0007656120011551962, policy loss: 8.417456034437013
Experience 18, Iter 65, disc loss: 0.0007007426192545793, policy loss: 8.569419989680974
Experience 18, Iter 66, disc loss: 0.0006921651724725525, policy loss: 8.552630529664798
Experience 18, Iter 67, disc loss: 0.0007337498047209981, policy loss: 8.491165530497227
Experience 18, Iter 68, disc loss: 0.0005947488625674771, policy loss: 8.481334116378445
Experience 18, Iter 69, disc loss: 0.0007145930980215483, policy loss: 8.227240791872433
Experience 18, Iter 70, disc loss: 0.0006686103900128808, policy loss: 8.407222170923395
Experience 18, Iter 71, disc loss: 0.000669743364747101, policy loss: 8.378019110635362
Experience 18, Iter 72, disc loss: 0.0006258458567410416, policy loss: 8.453787093592826
Experience 18, Iter 73, disc loss: 0.0005445573610676551, policy loss: 8.626588905427973
Experience 18, Iter 74, disc loss: 0.0004977513722043474, policy loss: 8.80966551943764
Experience 18, Iter 75, disc loss: 0.0006031228588131098, policy loss: 8.477301765398082
Experience 18, Iter 76, disc loss: 0.0006195947676939759, policy loss: 8.632380430790203
Experience 18, Iter 77, disc loss: 0.0006843201709985683, policy loss: 8.399630905834849
Experience 18, Iter 78, disc loss: 0.00046914911709665624, policy loss: 8.764299855123706
Experience 18, Iter 79, disc loss: 0.0005022470981901771, policy loss: 8.638213025213224
Experience 18, Iter 80, disc loss: 0.0004827641708250002, policy loss: 8.682355152751207
Experience 18, Iter 81, disc loss: 0.0005012638793323916, policy loss: 8.770638726886837
Experience 18, Iter 82, disc loss: 0.0005098502604011734, policy loss: 8.57245400191722
Experience 18, Iter 83, disc loss: 0.0006298461279317299, policy loss: 8.769967101873988
Experience 18, Iter 84, disc loss: 0.0004549415508306209, policy loss: 8.928614068193943
Experience 18, Iter 85, disc loss: 0.0005881373455117663, policy loss: 8.521472397607875
Experience 18, Iter 86, disc loss: 0.000614883156498157, policy loss: 8.40215362618138
Experience 18, Iter 87, disc loss: 0.0005171069935844189, policy loss: 8.78982758917924
Experience 18, Iter 88, disc loss: 0.0005913908804801282, policy loss: 8.690680174039311
Experience 18, Iter 89, disc loss: 0.0006237534173308915, policy loss: 8.527629948372589
Experience 18, Iter 90, disc loss: 0.0008016330984916448, policy loss: 8.184047250656695
Experience 18, Iter 91, disc loss: 0.0007850390298229301, policy loss: 8.170271874153645
Experience 18, Iter 92, disc loss: 0.000550600306780492, policy loss: 8.550847626190563
Experience 18, Iter 93, disc loss: 0.0004982596987644188, policy loss: 8.510497942254519
Experience 18, Iter 94, disc loss: 0.0005399199759094143, policy loss: 8.57438093313926
Experience 18, Iter 95, disc loss: 0.0005381268551514526, policy loss: 8.83408519101446
Experience 18, Iter 96, disc loss: 0.0005160604816919338, policy loss: 8.51720376296624
Experience 18, Iter 97, disc loss: 0.0004999726026050978, policy loss: 8.58494852074322
Experience 18, Iter 98, disc loss: 0.0004853251861886561, policy loss: 8.819509769576396
Experience 18, Iter 99, disc loss: 0.000681368015540995, policy loss: 8.660408466911338
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1047],
        [1.1028],
        [0.0058]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4549e-02, 1.1876e-01, 3.5395e-01, 7.1016e-03, 1.1764e-03,
          2.0371e+00]],

        [[1.4549e-02, 1.1876e-01, 3.5395e-01, 7.1016e-03, 1.1764e-03,
          2.0371e+00]],

        [[1.4549e-02, 1.1876e-01, 3.5395e-01, 7.1016e-03, 1.1764e-03,
          2.0371e+00]],

        [[1.4549e-02, 1.1876e-01, 3.5395e-01, 7.1016e-03, 1.1764e-03,
          2.0371e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0118, 0.4190, 4.4113, 0.0232], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0118, 0.4190, 4.4113, 0.0232])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.904
Iter 2/2000 - Loss: 2.282
Iter 3/2000 - Loss: 1.876
Iter 4/2000 - Loss: 1.922
Iter 5/2000 - Loss: 2.040
Iter 6/2000 - Loss: 1.942
Iter 7/2000 - Loss: 1.793
Iter 8/2000 - Loss: 1.726
Iter 9/2000 - Loss: 1.725
Iter 10/2000 - Loss: 1.700
Iter 11/2000 - Loss: 1.602
Iter 12/2000 - Loss: 1.456
Iter 13/2000 - Loss: 1.303
Iter 14/2000 - Loss: 1.161
Iter 15/2000 - Loss: 1.017
Iter 16/2000 - Loss: 0.848
Iter 17/2000 - Loss: 0.640
Iter 18/2000 - Loss: 0.396
Iter 19/2000 - Loss: 0.128
Iter 20/2000 - Loss: -0.153
Iter 1981/2000 - Loss: -8.847
Iter 1982/2000 - Loss: -8.847
Iter 1983/2000 - Loss: -8.847
Iter 1984/2000 - Loss: -8.847
Iter 1985/2000 - Loss: -8.848
Iter 1986/2000 - Loss: -8.848
Iter 1987/2000 - Loss: -8.848
Iter 1988/2000 - Loss: -8.848
Iter 1989/2000 - Loss: -8.848
Iter 1990/2000 - Loss: -8.848
Iter 1991/2000 - Loss: -8.848
Iter 1992/2000 - Loss: -8.848
Iter 1993/2000 - Loss: -8.848
Iter 1994/2000 - Loss: -8.848
Iter 1995/2000 - Loss: -8.848
Iter 1996/2000 - Loss: -8.848
Iter 1997/2000 - Loss: -8.848
Iter 1998/2000 - Loss: -8.848
Iter 1999/2000 - Loss: -8.848
Iter 2000/2000 - Loss: -8.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[14.8035, 10.8833, 35.1802, 10.4756, 12.4225, 47.9540]],

        [[18.9767, 36.6547, 11.6477,  1.5392,  2.4505, 20.2306]],

        [[19.6099, 28.8325, 12.0850,  1.0030,  5.1913, 23.5873]],

        [[17.2289, 33.3724, 15.9519,  3.0776,  2.8866, 40.7043]]])
Signal Variance: tensor([ 0.1624,  2.0930, 22.7134,  0.3353])
Estimated target variance: tensor([0.0118, 0.4190, 4.4113, 0.0232])
N: 190
Signal to noise ratio: tensor([ 24.4009,  84.2792, 122.1098,  41.3592])
Bound on condition number: tensor([ 113127.7220, 1349568.7708, 2833053.9334,  325011.5568])
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 0.0005588379306993728, policy loss: 8.55570734784725
Experience 19, Iter 1, disc loss: 0.0006048999015519766, policy loss: 8.581313742609344
Experience 19, Iter 2, disc loss: 0.00046343851729840885, policy loss: 8.662336595252045
Experience 19, Iter 3, disc loss: 0.000578182555109771, policy loss: 8.489706517572069
Experience 19, Iter 4, disc loss: 0.0006135471090657413, policy loss: 8.462592266766528
Experience 19, Iter 5, disc loss: 0.0005649489119447583, policy loss: 8.702233871728502
Experience 19, Iter 6, disc loss: 0.0007768347877523784, policy loss: 8.473334169218173
Experience 19, Iter 7, disc loss: 0.0006945436552620958, policy loss: 8.241956206285904
Experience 19, Iter 8, disc loss: 0.0008386213594990621, policy loss: 8.448189415601526
Experience 19, Iter 9, disc loss: 0.0005281825883679434, policy loss: 8.698374507576574
Experience 19, Iter 10, disc loss: 0.0006109530743196537, policy loss: 8.418510451887947
Experience 19, Iter 11, disc loss: 0.0004494444209517846, policy loss: 8.725233510710023
Experience 19, Iter 12, disc loss: 0.0006748278470067276, policy loss: 8.385343682513238
Experience 19, Iter 13, disc loss: 0.0006159297161023418, policy loss: 8.54084931984279
Experience 19, Iter 14, disc loss: 0.0004432595015415308, policy loss: 8.863483271124336
Experience 19, Iter 15, disc loss: 0.0005394206713018281, policy loss: 8.79213019800665
Experience 19, Iter 16, disc loss: 0.00045781414878597623, policy loss: 8.659394819255919
Experience 19, Iter 17, disc loss: 0.0007857620044399855, policy loss: 8.149710488007454
Experience 19, Iter 18, disc loss: 0.0006330180824878742, policy loss: 8.601829799568861
Experience 19, Iter 19, disc loss: 0.0004864700119263345, policy loss: 8.919196690138925
Experience 19, Iter 20, disc loss: 0.0005712835571822012, policy loss: 8.745492091286746
Experience 19, Iter 21, disc loss: 0.0005678546483868086, policy loss: 8.559967786652521
Experience 19, Iter 22, disc loss: 0.0005625064354073518, policy loss: 8.668711682296117
Experience 19, Iter 23, disc loss: 0.0006330911505671421, policy loss: 8.637868122288548
Experience 19, Iter 24, disc loss: 0.0004592926691844772, policy loss: 8.67367052150417
Experience 19, Iter 25, disc loss: 0.00044392647579410315, policy loss: 8.841627441095202
Experience 19, Iter 26, disc loss: 0.00045765792381637847, policy loss: 8.82632033815717
Experience 19, Iter 27, disc loss: 0.0003927915268333645, policy loss: 8.834343789898734
Experience 19, Iter 28, disc loss: 0.0004932769179153762, policy loss: 8.562945009166835
Experience 19, Iter 29, disc loss: 0.0005175034840330084, policy loss: 8.55298362873453
Experience 19, Iter 30, disc loss: 0.0005051033559314122, policy loss: 8.636695634458725
Experience 19, Iter 31, disc loss: 0.0005387325286825568, policy loss: 8.307653714609593
Experience 19, Iter 32, disc loss: 0.0005512328354888504, policy loss: 8.680303159942337
Experience 19, Iter 33, disc loss: 0.0006967422876486539, policy loss: 8.514840117109765
Experience 19, Iter 34, disc loss: 0.0005792829208846421, policy loss: 8.377055612317903
Experience 19, Iter 35, disc loss: 0.0005975018962161673, policy loss: 8.572826827593401
Experience 19, Iter 36, disc loss: 0.0006547985020325449, policy loss: 8.372109723338607
Experience 19, Iter 37, disc loss: 0.00040067395300210474, policy loss: 8.771703746794
Experience 19, Iter 38, disc loss: 0.00038724757423977787, policy loss: 8.739308195623913
Experience 19, Iter 39, disc loss: 0.0005348983468341682, policy loss: 8.533402192838235
Experience 19, Iter 40, disc loss: 0.0005426806784138408, policy loss: 8.490795756666621
Experience 19, Iter 41, disc loss: 0.0005470869679659839, policy loss: 8.51377035615479
Experience 19, Iter 42, disc loss: 0.0006093483176636683, policy loss: 8.349087624324937
Experience 19, Iter 43, disc loss: 0.0005961598553178756, policy loss: 8.377262883163048
Experience 19, Iter 44, disc loss: 0.0006295275223852724, policy loss: 8.466563106698707
Experience 19, Iter 45, disc loss: 0.0005070931833887955, policy loss: 8.622580938640377
Experience 19, Iter 46, disc loss: 0.00043952916092164637, policy loss: 8.698931331168694
Experience 19, Iter 47, disc loss: 0.0006005437511324781, policy loss: 8.496364136371282
Experience 19, Iter 48, disc loss: 0.00045217178182073436, policy loss: 8.62580719174493
Experience 19, Iter 49, disc loss: 0.00046470098806819114, policy loss: 8.748924833931138
Experience 19, Iter 50, disc loss: 0.0005603838037066127, policy loss: 8.685511233828464
Experience 19, Iter 51, disc loss: 0.000486873084382571, policy loss: 8.544315208885047
Experience 19, Iter 52, disc loss: 0.00045079616605436356, policy loss: 8.684055190634467
Experience 19, Iter 53, disc loss: 0.0006587472935694587, policy loss: 8.351733875262596
Experience 19, Iter 54, disc loss: 0.00041974822772900596, policy loss: 8.686307509437938
Experience 19, Iter 55, disc loss: 0.0004873025494540597, policy loss: 8.515171407716707
Experience 19, Iter 56, disc loss: 0.0005801621730359859, policy loss: 8.496043738710894
Experience 19, Iter 57, disc loss: 0.0004515871365685294, policy loss: 8.607242334451021
Experience 19, Iter 58, disc loss: 0.0003737736624262907, policy loss: 8.870633767425694
Experience 19, Iter 59, disc loss: 0.000484621735232355, policy loss: 8.674257788644868
Experience 19, Iter 60, disc loss: 0.00042338614629282506, policy loss: 8.694385415254814
Experience 19, Iter 61, disc loss: 0.00042670898966266387, policy loss: 8.725140851466094
Experience 19, Iter 62, disc loss: 0.0006542882744119669, policy loss: 8.23019158753132
Experience 19, Iter 63, disc loss: 0.00043939268279503794, policy loss: 8.738136020304596
Experience 19, Iter 64, disc loss: 0.00048064651752172046, policy loss: 8.633862929922294
Experience 19, Iter 65, disc loss: 0.0005555009692538074, policy loss: 8.316424486034094
Experience 19, Iter 66, disc loss: 0.00043633734570077093, policy loss: 8.663755950436839
Experience 19, Iter 67, disc loss: 0.0005312713787248354, policy loss: 8.475338337973561
Experience 19, Iter 68, disc loss: 0.0005145870183478901, policy loss: 8.394879205500626
Experience 19, Iter 69, disc loss: 0.0006249969244359838, policy loss: 8.238164244336236
Experience 19, Iter 70, disc loss: 0.0003674699747128534, policy loss: 8.980011388561378
Experience 19, Iter 71, disc loss: 0.0004428023631472902, policy loss: 8.55262631784023
Experience 19, Iter 72, disc loss: 0.00046548390921085704, policy loss: 8.536765091913127
Experience 19, Iter 73, disc loss: 0.00038763536910750514, policy loss: 8.679676934390507
Experience 19, Iter 74, disc loss: 0.00037802320807839426, policy loss: 8.713357476461479
Experience 19, Iter 75, disc loss: 0.00033926681071005135, policy loss: 8.94478441930633
Experience 19, Iter 76, disc loss: 0.00043594553772679595, policy loss: 8.683260869851066
Experience 19, Iter 77, disc loss: 0.0003967585714164315, policy loss: 8.761320206186491
Experience 19, Iter 78, disc loss: 0.0004456272957429676, policy loss: 8.747064470866032
Experience 19, Iter 79, disc loss: 0.00037034275239571215, policy loss: 8.862229489458208
Experience 19, Iter 80, disc loss: 0.0004399914533133704, policy loss: 8.615852279474339
Experience 19, Iter 81, disc loss: 0.000477317711441392, policy loss: 8.592579694192942
Experience 19, Iter 82, disc loss: 0.0005465818688717477, policy loss: 8.511933538455095
Experience 19, Iter 83, disc loss: 0.0004077201451242363, policy loss: 8.597811086466955
Experience 19, Iter 84, disc loss: 0.0005133162141639225, policy loss: 8.539756717488691
Experience 19, Iter 85, disc loss: 0.0005314802830035075, policy loss: 8.42313056340133
Experience 19, Iter 86, disc loss: 0.00048556226815140715, policy loss: 8.406184855230126
Experience 19, Iter 87, disc loss: 0.0003927489186859422, policy loss: 8.560945205956997
Experience 19, Iter 88, disc loss: 0.0003664517921673153, policy loss: 8.85085084483989
Experience 19, Iter 89, disc loss: 0.0003510300774867103, policy loss: 8.84071281974724
Experience 19, Iter 90, disc loss: 0.0003975403414232364, policy loss: 8.750027661964307
Experience 19, Iter 91, disc loss: 0.0004790231071758017, policy loss: 8.503320364872089
Experience 19, Iter 92, disc loss: 0.0003733176390803448, policy loss: 8.780041629394145
Experience 19, Iter 93, disc loss: 0.00037065060702674423, policy loss: 8.817533989027018
Experience 19, Iter 94, disc loss: 0.0004330394832811304, policy loss: 8.725986190486243
Experience 19, Iter 95, disc loss: 0.00036484148012055987, policy loss: 8.701441776457553
Experience 19, Iter 96, disc loss: 0.0004370786144418324, policy loss: 8.765834132829355
Experience 19, Iter 97, disc loss: 0.0003434876912440715, policy loss: 8.981313132270596
Experience 19, Iter 98, disc loss: 0.0003208071632827509, policy loss: 8.952646280793605
Experience 19, Iter 99, disc loss: 0.0004709829736222394, policy loss: 8.529786134151822
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.0998],
        [1.0535],
        [0.0056]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3844e-02, 1.1320e-01, 3.3966e-01, 6.8139e-03, 1.1230e-03,
          1.9398e+00]],

        [[1.3844e-02, 1.1320e-01, 3.3966e-01, 6.8139e-03, 1.1230e-03,
          1.9398e+00]],

        [[1.3844e-02, 1.1320e-01, 3.3966e-01, 6.8139e-03, 1.1230e-03,
          1.9398e+00]],

        [[1.3844e-02, 1.1320e-01, 3.3966e-01, 6.8139e-03, 1.1230e-03,
          1.9398e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.3993, 4.2138, 0.0224], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.3993, 4.2138, 0.0224])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.817
Iter 2/2000 - Loss: 2.226
Iter 3/2000 - Loss: 1.793
Iter 4/2000 - Loss: 1.846
Iter 5/2000 - Loss: 1.976
Iter 6/2000 - Loss: 1.878
Iter 7/2000 - Loss: 1.724
Iter 8/2000 - Loss: 1.654
Iter 9/2000 - Loss: 1.658
Iter 10/2000 - Loss: 1.642
Iter 11/2000 - Loss: 1.553
Iter 12/2000 - Loss: 1.410
Iter 13/2000 - Loss: 1.258
Iter 14/2000 - Loss: 1.118
Iter 15/2000 - Loss: 0.980
Iter 16/2000 - Loss: 0.818
Iter 17/2000 - Loss: 0.618
Iter 18/2000 - Loss: 0.379
Iter 19/2000 - Loss: 0.113
Iter 20/2000 - Loss: -0.169
Iter 1981/2000 - Loss: -8.864
Iter 1982/2000 - Loss: -8.864
Iter 1983/2000 - Loss: -8.864
Iter 1984/2000 - Loss: -8.864
Iter 1985/2000 - Loss: -8.864
Iter 1986/2000 - Loss: -8.864
Iter 1987/2000 - Loss: -8.864
Iter 1988/2000 - Loss: -8.864
Iter 1989/2000 - Loss: -8.864
Iter 1990/2000 - Loss: -8.864
Iter 1991/2000 - Loss: -8.864
Iter 1992/2000 - Loss: -8.864
Iter 1993/2000 - Loss: -8.864
Iter 1994/2000 - Loss: -8.864
Iter 1995/2000 - Loss: -8.864
Iter 1996/2000 - Loss: -8.864
Iter 1997/2000 - Loss: -8.864
Iter 1998/2000 - Loss: -8.864
Iter 1999/2000 - Loss: -8.864
Iter 2000/2000 - Loss: -8.864
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[14.4325, 10.8322, 34.2585, 10.3688, 12.0971, 47.6123]],

        [[18.4616, 35.3132, 12.1417,  1.5354,  2.4462, 21.2215]],

        [[19.2520, 29.5143, 12.4968,  1.0009,  5.5522, 23.6929]],

        [[17.0524, 31.5215, 16.7170,  3.2614,  2.8123, 41.9410]]])
Signal Variance: tensor([ 0.1598,  2.2521, 22.1947,  0.3645])
Estimated target variance: tensor([0.0113, 0.3993, 4.2138, 0.0224])
N: 200
Signal to noise ratio: tensor([ 23.7764,  87.3200, 118.3446,  42.7062])
Bound on condition number: tensor([ 113064.6485, 1524957.0048, 2801087.5721,  364764.8564])
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 0.0003696247054270214, policy loss: 8.674473763989978
Experience 20, Iter 1, disc loss: 0.0003471982931993907, policy loss: 8.813233472411113
Experience 20, Iter 2, disc loss: 0.00032489939749650866, policy loss: 8.810441094021506
Experience 20, Iter 3, disc loss: 0.0003727151174462431, policy loss: 8.737542028644988
Experience 20, Iter 4, disc loss: 0.0003237545712062568, policy loss: 8.844023026063207
Experience 20, Iter 5, disc loss: 0.00038059091118757053, policy loss: 8.531622243456178
Experience 20, Iter 6, disc loss: 0.0004055607937483515, policy loss: 8.528142585394454
Experience 20, Iter 7, disc loss: 0.00037825630062709537, policy loss: 8.533910849914115
Experience 20, Iter 8, disc loss: 0.00037424713654265064, policy loss: 8.65284400628149
Experience 20, Iter 9, disc loss: 0.00036435043703982354, policy loss: 8.812985414486201
Experience 20, Iter 10, disc loss: 0.0003808661229271373, policy loss: 8.666406788443371
Experience 20, Iter 11, disc loss: 0.00040035692117221667, policy loss: 8.536650648272019
Experience 20, Iter 12, disc loss: 0.00036433485809886997, policy loss: 8.666232060585463
Experience 20, Iter 13, disc loss: 0.00037685917100351355, policy loss: 8.530169957992335
Experience 20, Iter 14, disc loss: 0.0003737393373149917, policy loss: 8.5579193554613
Experience 20, Iter 15, disc loss: 0.00033679996186071725, policy loss: 8.581127492789149
Experience 20, Iter 16, disc loss: 0.0003716349907798083, policy loss: 8.573922326078797
Experience 20, Iter 17, disc loss: 0.0003813200756510476, policy loss: 8.456513166717256
Experience 20, Iter 18, disc loss: 0.00041606905505485085, policy loss: 8.576855199943523
Experience 20, Iter 19, disc loss: 0.00037049154236962634, policy loss: 8.616893107472594
Experience 20, Iter 20, disc loss: 0.000360496406565835, policy loss: 8.600792384649711
Experience 20, Iter 21, disc loss: 0.00037238903164269616, policy loss: 8.50650469232263
Experience 20, Iter 22, disc loss: 0.0004079240989392833, policy loss: 8.443104066021057
Experience 20, Iter 23, disc loss: 0.00040629774666838296, policy loss: 8.500138069511237
Experience 20, Iter 24, disc loss: 0.00037556245466355677, policy loss: 8.402041768975407
Experience 20, Iter 25, disc loss: 0.00032964094914533565, policy loss: 8.651777111705613
Experience 20, Iter 26, disc loss: 0.00034843588066642023, policy loss: 8.551961613499508
Experience 20, Iter 27, disc loss: 0.0003611791019447239, policy loss: 8.513745746250962
Experience 20, Iter 28, disc loss: 0.00041695896812607165, policy loss: 8.291408080113872
Experience 20, Iter 29, disc loss: 0.00043182687105734383, policy loss: 8.242272755024223
Experience 20, Iter 30, disc loss: 0.000397427998473417, policy loss: 8.378024774081638
Experience 20, Iter 31, disc loss: 0.00039663321684914657, policy loss: 8.409193743084018
Experience 20, Iter 32, disc loss: 0.000429090070855119, policy loss: 8.272643703168761
Experience 20, Iter 33, disc loss: 0.0003937214509979484, policy loss: 8.5699625444574
Experience 20, Iter 34, disc loss: 0.0004570457052343032, policy loss: 8.264781887990987
Experience 20, Iter 35, disc loss: 0.0004319524264344525, policy loss: 8.20988094121417
Experience 20, Iter 36, disc loss: 0.0004801896797711492, policy loss: 8.038601489705863
Experience 20, Iter 37, disc loss: 0.0004312301415423049, policy loss: 8.187503209347762
Experience 20, Iter 38, disc loss: 0.00047187046393926547, policy loss: 8.081704482166863
Experience 20, Iter 39, disc loss: 0.0005063540773652066, policy loss: 7.937199169462444
Experience 20, Iter 40, disc loss: 0.00047799442690715733, policy loss: 8.071446883149827
Experience 20, Iter 41, disc loss: 0.0005246730104853934, policy loss: 7.97378603609753
Experience 20, Iter 42, disc loss: 0.0005147877875586437, policy loss: 8.015386285752392
Experience 20, Iter 43, disc loss: 0.0005032769359130297, policy loss: 8.007989374106549
Experience 20, Iter 44, disc loss: 0.000526386462167711, policy loss: 7.915157983588575
Experience 20, Iter 45, disc loss: 0.0005777911284746397, policy loss: 7.840507094855538
Experience 20, Iter 46, disc loss: 0.0005877116008731936, policy loss: 7.776773876096855
Experience 20, Iter 47, disc loss: 0.0006016653718806865, policy loss: 7.752623551040423
Experience 20, Iter 48, disc loss: 0.0005837700109186669, policy loss: 7.808502668029888
Experience 20, Iter 49, disc loss: 0.0006569672365535572, policy loss: 7.674292204611827
Experience 20, Iter 50, disc loss: 0.0006492176111821964, policy loss: 7.709671472500027
Experience 20, Iter 51, disc loss: 0.0006731378892827591, policy loss: 7.6744790255122775
Experience 20, Iter 52, disc loss: 0.0007399471846932824, policy loss: 7.5241620741716275
Experience 20, Iter 53, disc loss: 0.0007727451320005302, policy loss: 7.4652803087863315
Experience 20, Iter 54, disc loss: 0.0008561795286891154, policy loss: 7.318343514789872
Experience 20, Iter 55, disc loss: 0.0008299567823598448, policy loss: 7.331829111094821
Experience 20, Iter 56, disc loss: 0.000849783251022612, policy loss: 7.349504899243055
Experience 20, Iter 57, disc loss: 0.0008849627896657066, policy loss: 7.271011486837741
Experience 20, Iter 58, disc loss: 0.0008982990379942017, policy loss: 7.249845814179607
Experience 20, Iter 59, disc loss: 0.0009034334076373615, policy loss: 7.247711559660155
Experience 20, Iter 60, disc loss: 0.0009564939305713697, policy loss: 7.193569290072146
Experience 20, Iter 61, disc loss: 0.0010547472157243023, policy loss: 7.118855162926021
Experience 20, Iter 62, disc loss: 0.0011363187728451762, policy loss: 7.028932338592401
Experience 20, Iter 63, disc loss: 0.0010915375143410538, policy loss: 7.061281232905299
Experience 20, Iter 64, disc loss: 0.0012472730466655123, policy loss: 6.929430803025852
Experience 20, Iter 65, disc loss: 0.0013538557947810352, policy loss: 6.768436032642326
Experience 20, Iter 66, disc loss: 0.0014152191653750506, policy loss: 6.763387394454725
Experience 20, Iter 67, disc loss: 0.0013386507248087263, policy loss: 6.801163312450244
Experience 20, Iter 68, disc loss: 0.001440692993250718, policy loss: 6.753847798881896
Experience 20, Iter 69, disc loss: 0.001633235381872975, policy loss: 6.628217011867558
Experience 20, Iter 70, disc loss: 0.001756954726524464, policy loss: 6.526881519224597
Experience 20, Iter 71, disc loss: 0.0019664492902909197, policy loss: 6.382122079898339
Experience 20, Iter 72, disc loss: 0.0018490788914933627, policy loss: 6.445474349156259
Experience 20, Iter 73, disc loss: 0.0019422923245218532, policy loss: 6.388527006565901
Experience 20, Iter 74, disc loss: 0.0018972285088851245, policy loss: 6.487094213773237
Experience 20, Iter 75, disc loss: 0.002253270297700837, policy loss: 6.224627990504572
Experience 20, Iter 76, disc loss: 0.002419246180287107, policy loss: 6.159633182092307
Experience 20, Iter 77, disc loss: 0.0023202799016029067, policy loss: 6.2760973206472075
Experience 20, Iter 78, disc loss: 0.002124883470774335, policy loss: 6.355665408201122
Experience 20, Iter 79, disc loss: 0.0026944355137720364, policy loss: 6.089288847150488
Experience 20, Iter 80, disc loss: 0.002350662514855773, policy loss: 6.2146239257296125
Experience 20, Iter 81, disc loss: 0.0025100084723543954, policy loss: 6.20944500097217
Experience 20, Iter 82, disc loss: 0.0026784077066104913, policy loss: 6.154822467691957
Experience 20, Iter 83, disc loss: 0.002654555703201924, policy loss: 6.103015716227604
Experience 20, Iter 84, disc loss: 0.0029542265902514054, policy loss: 5.996769803939295
Experience 20, Iter 85, disc loss: 0.002700816613506708, policy loss: 6.049523171857372
Experience 20, Iter 86, disc loss: 0.002893403420835719, policy loss: 6.065332252195124
Experience 20, Iter 87, disc loss: 0.00312396694041225, policy loss: 6.038269082165105
Experience 20, Iter 88, disc loss: 0.002944577025380287, policy loss: 6.078870897179312
Experience 20, Iter 89, disc loss: 0.0030214439679670767, policy loss: 6.0896046322643995
Experience 20, Iter 90, disc loss: 0.0029151903858424277, policy loss: 6.0798630882622176
Experience 20, Iter 91, disc loss: 0.0035746468394310866, policy loss: 5.915840261647354
Experience 20, Iter 92, disc loss: 0.003432681765062195, policy loss: 5.873845178108421
Experience 20, Iter 93, disc loss: 0.0031707891350779113, policy loss: 5.978415826140882
Experience 20, Iter 94, disc loss: 0.0033084538729345535, policy loss: 5.852356258274034
Experience 20, Iter 95, disc loss: 0.0035425241618363343, policy loss: 5.798433186060871
Experience 20, Iter 96, disc loss: 0.0036633655663827606, policy loss: 5.8020983184934325
Experience 20, Iter 97, disc loss: 0.003373490899412944, policy loss: 5.892951532252132
Experience 20, Iter 98, disc loss: 0.003584069976007841, policy loss: 5.889302017045675
Experience 20, Iter 99, disc loss: 0.0038758401400858828, policy loss: 5.766267221938923
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0953],
        [1.0064],
        [0.0054]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3391e-02, 1.0908e-01, 3.2769e-01, 6.7107e-03, 1.0721e-03,
          1.8596e+00]],

        [[1.3391e-02, 1.0908e-01, 3.2769e-01, 6.7107e-03, 1.0721e-03,
          1.8596e+00]],

        [[1.3391e-02, 1.0908e-01, 3.2769e-01, 6.7107e-03, 1.0721e-03,
          1.8596e+00]],

        [[1.3391e-02, 1.0908e-01, 3.2769e-01, 6.7107e-03, 1.0721e-03,
          1.8596e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.3812, 4.0257, 0.0217], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.3812, 4.0257, 0.0217])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.740
Iter 2/2000 - Loss: 2.181
Iter 3/2000 - Loss: 1.720
Iter 4/2000 - Loss: 1.779
Iter 5/2000 - Loss: 1.922
Iter 6/2000 - Loss: 1.825
Iter 7/2000 - Loss: 1.667
Iter 8/2000 - Loss: 1.595
Iter 9/2000 - Loss: 1.603
Iter 10/2000 - Loss: 1.597
Iter 11/2000 - Loss: 1.518
Iter 12/2000 - Loss: 1.384
Iter 13/2000 - Loss: 1.239
Iter 14/2000 - Loss: 1.105
Iter 15/2000 - Loss: 0.973
Iter 16/2000 - Loss: 0.822
Iter 17/2000 - Loss: 0.634
Iter 18/2000 - Loss: 0.407
Iter 19/2000 - Loss: 0.150
Iter 20/2000 - Loss: -0.124
Iter 1981/2000 - Loss: -8.888
Iter 1982/2000 - Loss: -8.888
Iter 1983/2000 - Loss: -8.888
Iter 1984/2000 - Loss: -8.888
Iter 1985/2000 - Loss: -8.888
Iter 1986/2000 - Loss: -8.888
Iter 1987/2000 - Loss: -8.888
Iter 1988/2000 - Loss: -8.888
Iter 1989/2000 - Loss: -8.889
Iter 1990/2000 - Loss: -8.889
Iter 1991/2000 - Loss: -8.889
Iter 1992/2000 - Loss: -8.889
Iter 1993/2000 - Loss: -8.889
Iter 1994/2000 - Loss: -8.889
Iter 1995/2000 - Loss: -8.889
Iter 1996/2000 - Loss: -8.889
Iter 1997/2000 - Loss: -8.889
Iter 1998/2000 - Loss: -8.889
Iter 1999/2000 - Loss: -8.889
Iter 2000/2000 - Loss: -8.889
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[14.5479, 10.8224, 34.0129,  9.9612, 11.5388, 47.8959]],

        [[18.2632, 34.8566, 12.0767,  1.5190,  2.8553, 21.8515]],

        [[19.0560, 29.9356, 12.2945,  1.0019,  5.3046, 23.5849]],

        [[16.8545, 30.0493, 16.8250,  3.2322,  3.0816, 42.0481]]])
Signal Variance: tensor([ 0.1570,  2.3179, 21.6551,  0.3701])
Estimated target variance: tensor([0.0109, 0.3812, 4.0257, 0.0217])
N: 210
Signal to noise ratio: tensor([ 23.9551,  87.8521, 115.4742,  42.2180])
Bound on condition number: tensor([ 120508.8013, 1620779.8141, 2800201.4613,  374296.5408])
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 0.0036929992473813712, policy loss: 5.785740252861084
Experience 21, Iter 1, disc loss: 0.0038675763120666123, policy loss: 5.701426823042029
Experience 21, Iter 2, disc loss: 0.004052459860705661, policy loss: 5.707418093870144
Experience 21, Iter 3, disc loss: 0.004209787745423942, policy loss: 5.66665259064359
Experience 21, Iter 4, disc loss: 0.0038805531408170803, policy loss: 5.789214125493089
Experience 21, Iter 5, disc loss: 0.004079399488461605, policy loss: 5.776218890795259
Experience 21, Iter 6, disc loss: 0.004556062577655042, policy loss: 5.537436682509309
Experience 21, Iter 7, disc loss: 0.004168220425808657, policy loss: 5.677765007341243
Experience 21, Iter 8, disc loss: 0.0038843389934747597, policy loss: 5.737516951173255
Experience 21, Iter 9, disc loss: 0.00447355395170489, policy loss: 5.555310583433084
Experience 21, Iter 10, disc loss: 0.004408659209901093, policy loss: 5.602130643705999
Experience 21, Iter 11, disc loss: 0.004200183568786649, policy loss: 5.698838050768224
Experience 21, Iter 12, disc loss: 0.004450267988731664, policy loss: 5.684011748026198
Experience 21, Iter 13, disc loss: 0.00437934210431716, policy loss: 5.661231553285564
Experience 21, Iter 14, disc loss: 0.004529458922023453, policy loss: 5.6871368617952145
Experience 21, Iter 15, disc loss: 0.004399159081510656, policy loss: 5.610048688713784
Experience 21, Iter 16, disc loss: 0.004627129904075123, policy loss: 5.661764296402295
Experience 21, Iter 17, disc loss: 0.004011616579756028, policy loss: 5.939619656598408
Experience 21, Iter 18, disc loss: 0.004286725308646362, policy loss: 5.690134233077649
Experience 21, Iter 19, disc loss: 0.004713657282571364, policy loss: 5.566976177797955
Experience 21, Iter 20, disc loss: 0.0044600514801665945, policy loss: 5.627289130883162
Experience 21, Iter 21, disc loss: 0.004655714116579912, policy loss: 5.602332556404894
Experience 21, Iter 22, disc loss: 0.004461511210568073, policy loss: 5.647492494063785
Experience 21, Iter 23, disc loss: 0.004601937951026664, policy loss: 5.672220593170742
Experience 21, Iter 24, disc loss: 0.0042601207720228025, policy loss: 5.692687606156159
Experience 21, Iter 25, disc loss: 0.004267278895523173, policy loss: 5.803111116030327
Experience 21, Iter 26, disc loss: 0.0043290552596745475, policy loss: 5.668631430771565
Experience 21, Iter 27, disc loss: 0.004869268063503834, policy loss: 5.604226803170935
Experience 21, Iter 28, disc loss: 0.004472255819992213, policy loss: 5.7460128965281925
Experience 21, Iter 29, disc loss: 0.004499239672208307, policy loss: 5.760180315802771
Experience 21, Iter 30, disc loss: 0.004611475911555687, policy loss: 5.654650501278882
Experience 21, Iter 31, disc loss: 0.004490254028224674, policy loss: 5.845920109715594
Experience 21, Iter 32, disc loss: 0.004341001750609096, policy loss: 5.779913096658883
Experience 21, Iter 33, disc loss: 0.004159400843727286, policy loss: 5.944029177792618
Experience 21, Iter 34, disc loss: 0.004164565419081004, policy loss: 5.919265150609009
Experience 21, Iter 35, disc loss: 0.004010533964219044, policy loss: 5.894257066946289
Experience 21, Iter 36, disc loss: 0.004182202538864536, policy loss: 5.9610214828108345
Experience 21, Iter 37, disc loss: 0.004466431999024456, policy loss: 5.853585215400414
Experience 21, Iter 38, disc loss: 0.004305868145845624, policy loss: 5.990608156145811
Experience 21, Iter 39, disc loss: 0.004337543019232887, policy loss: 5.8804808019358905
Experience 21, Iter 40, disc loss: 0.0037592306472909558, policy loss: 6.082532655109536
Experience 21, Iter 41, disc loss: 0.004381235936369287, policy loss: 5.818140341443898
Experience 21, Iter 42, disc loss: 0.003994516662253273, policy loss: 5.9422656171450985
Experience 21, Iter 43, disc loss: 0.004599474173605627, policy loss: 5.671330656693842
Experience 21, Iter 44, disc loss: 0.003894750800498865, policy loss: 5.943612372261662
Experience 21, Iter 45, disc loss: 0.00381916999385847, policy loss: 6.050125982711572
Experience 21, Iter 46, disc loss: 0.004285356436044146, policy loss: 5.827511338183273
Experience 21, Iter 47, disc loss: 0.003837038766195088, policy loss: 6.016454795070024
Experience 21, Iter 48, disc loss: 0.004274531618944788, policy loss: 5.894476404852794
Experience 21, Iter 49, disc loss: 0.0032212914121172226, policy loss: 6.381956139311933
Experience 21, Iter 50, disc loss: 0.004067427973824032, policy loss: 6.0588396691132385
Experience 21, Iter 51, disc loss: 0.004148332115727883, policy loss: 5.819602941831571
Experience 21, Iter 52, disc loss: 0.003810107233882946, policy loss: 6.122484379065023
Experience 21, Iter 53, disc loss: 0.0038299498778195996, policy loss: 6.068477121820871
Experience 21, Iter 54, disc loss: 0.0037279040376340905, policy loss: 6.086305982118905
Experience 21, Iter 55, disc loss: 0.003933409554676233, policy loss: 5.9768103293285195
Experience 21, Iter 56, disc loss: 0.0037415028239082978, policy loss: 6.170228182991208
Experience 21, Iter 57, disc loss: 0.003330107618241062, policy loss: 6.331815315866727
Experience 21, Iter 58, disc loss: 0.00355725534346404, policy loss: 6.206457488693151
Experience 21, Iter 59, disc loss: 0.003571983408289757, policy loss: 6.0990064276762
Experience 21, Iter 60, disc loss: 0.0031469438509455377, policy loss: 6.295291887866187
Experience 21, Iter 61, disc loss: 0.003593698660466382, policy loss: 6.068212445801263
Experience 21, Iter 62, disc loss: 0.003154888692674494, policy loss: 6.420069448471521
Experience 21, Iter 63, disc loss: 0.003951876344782694, policy loss: 5.994576772517857
Experience 21, Iter 64, disc loss: 0.0036340055885319238, policy loss: 6.158098128225676
Experience 21, Iter 65, disc loss: 0.0037581402674679463, policy loss: 6.100656665787622
Experience 21, Iter 66, disc loss: 0.003621022834504097, policy loss: 6.114261915481644
Experience 21, Iter 67, disc loss: 0.003274542095112287, policy loss: 6.3109768554842045
Experience 21, Iter 68, disc loss: 0.0034159108424524002, policy loss: 6.316920291164139
Experience 21, Iter 69, disc loss: 0.0034182491991862586, policy loss: 6.2370255902183045
Experience 21, Iter 70, disc loss: 0.0039021563173610952, policy loss: 5.943917353420991
Experience 21, Iter 71, disc loss: 0.00394123855721658, policy loss: 6.050732804173826
Experience 21, Iter 72, disc loss: 0.003440384902793514, policy loss: 6.241034407088309
Experience 21, Iter 73, disc loss: 0.0034886257836389788, policy loss: 6.320012264026986
Experience 21, Iter 74, disc loss: 0.0036213582044133633, policy loss: 6.265776176373295
Experience 21, Iter 75, disc loss: 0.0036552451422548135, policy loss: 6.280857227281327
Experience 21, Iter 76, disc loss: 0.0033553538450749425, policy loss: 6.446741646161483
Experience 21, Iter 77, disc loss: 0.0032249152762152955, policy loss: 6.513904265063326
Experience 21, Iter 78, disc loss: 0.002898767985947025, policy loss: 6.571567083519602
Experience 21, Iter 79, disc loss: 0.0031824127125266014, policy loss: 6.521224677749844
Experience 21, Iter 80, disc loss: 0.003112928434462335, policy loss: 6.455824157064054
Experience 21, Iter 81, disc loss: 0.0032218381415120992, policy loss: 6.360423551533296
Experience 21, Iter 82, disc loss: 0.0029903637471755825, policy loss: 6.705559223634063
Experience 21, Iter 83, disc loss: 0.003711787894455824, policy loss: 6.078019909673499
Experience 21, Iter 84, disc loss: 0.003045133196071129, policy loss: 6.498439725104193
Experience 21, Iter 85, disc loss: 0.002953067994334534, policy loss: 6.498933054957259
Experience 21, Iter 86, disc loss: 0.0031953538048529037, policy loss: 6.411599466812037
Experience 21, Iter 87, disc loss: 0.003125280426710575, policy loss: 6.534366470386122
Experience 21, Iter 88, disc loss: 0.0029440509878519627, policy loss: 6.555004510090804
Experience 21, Iter 89, disc loss: 0.002758967172244887, policy loss: 6.758653077992635
Experience 21, Iter 90, disc loss: 0.0025865590190681034, policy loss: 6.649567950766979
Experience 21, Iter 91, disc loss: 0.002961978605993459, policy loss: 6.365652612993064
Experience 21, Iter 92, disc loss: 0.002588592388069133, policy loss: 6.67520238289164
Experience 21, Iter 93, disc loss: 0.0028306009007279583, policy loss: 6.5513619484103165
Experience 21, Iter 94, disc loss: 0.0026464321400638978, policy loss: 6.918116783009582
Experience 21, Iter 95, disc loss: 0.0030915489520369982, policy loss: 6.394544064409308
Experience 21, Iter 96, disc loss: 0.002823560908824854, policy loss: 6.521946913505298
Experience 21, Iter 97, disc loss: 0.002911869938145039, policy loss: 6.518792475102162
Experience 21, Iter 98, disc loss: 0.003356321737829044, policy loss: 6.33667104060331
Experience 21, Iter 99, disc loss: 0.0027045140678388074, policy loss: 6.65875195204526
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0952],
        [1.0064],
        [0.0059]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3262e-02, 1.0745e-01, 3.4616e-01, 7.3277e-03, 1.0924e-03,
          1.8909e+00]],

        [[1.3262e-02, 1.0745e-01, 3.4616e-01, 7.3277e-03, 1.0924e-03,
          1.8909e+00]],

        [[1.3262e-02, 1.0745e-01, 3.4616e-01, 7.3277e-03, 1.0924e-03,
          1.8909e+00]],

        [[1.3262e-02, 1.0745e-01, 3.4616e-01, 7.3277e-03, 1.0924e-03,
          1.8909e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0107, 0.3810, 4.0255, 0.0236], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0107, 0.3810, 4.0255, 0.0236])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.769
Iter 2/2000 - Loss: 2.215
Iter 3/2000 - Loss: 1.749
Iter 4/2000 - Loss: 1.806
Iter 5/2000 - Loss: 1.951
Iter 6/2000 - Loss: 1.857
Iter 7/2000 - Loss: 1.699
Iter 8/2000 - Loss: 1.624
Iter 9/2000 - Loss: 1.623
Iter 10/2000 - Loss: 1.612
Iter 11/2000 - Loss: 1.538
Iter 12/2000 - Loss: 1.411
Iter 13/2000 - Loss: 1.267
Iter 14/2000 - Loss: 1.126
Iter 15/2000 - Loss: 0.985
Iter 16/2000 - Loss: 0.828
Iter 17/2000 - Loss: 0.642
Iter 18/2000 - Loss: 0.422
Iter 19/2000 - Loss: 0.172
Iter 20/2000 - Loss: -0.098
Iter 1981/2000 - Loss: -8.836
Iter 1982/2000 - Loss: -8.836
Iter 1983/2000 - Loss: -8.836
Iter 1984/2000 - Loss: -8.836
Iter 1985/2000 - Loss: -8.836
Iter 1986/2000 - Loss: -8.836
Iter 1987/2000 - Loss: -8.836
Iter 1988/2000 - Loss: -8.836
Iter 1989/2000 - Loss: -8.836
Iter 1990/2000 - Loss: -8.836
Iter 1991/2000 - Loss: -8.836
Iter 1992/2000 - Loss: -8.836
Iter 1993/2000 - Loss: -8.837
Iter 1994/2000 - Loss: -8.837
Iter 1995/2000 - Loss: -8.837
Iter 1996/2000 - Loss: -8.837
Iter 1997/2000 - Loss: -8.837
Iter 1998/2000 - Loss: -8.837
Iter 1999/2000 - Loss: -8.837
Iter 2000/2000 - Loss: -8.837
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[14.8641, 10.9776, 33.0518,  9.9579, 11.5668, 47.8878]],

        [[18.9777, 35.8638, 11.0373,  1.3243,  3.9727, 21.3188]],

        [[18.5419, 31.8998, 11.4631,  0.9790,  1.5368, 25.3986]],

        [[16.0518, 28.5727, 14.3942,  3.6593,  1.3560, 39.5573]]])
Signal Variance: tensor([ 0.1527,  1.9982, 20.1989,  0.3255])
Estimated target variance: tensor([0.0107, 0.3810, 4.0255, 0.0236])
N: 220
Signal to noise ratio: tensor([ 23.6096,  81.4267, 111.7413,  39.3850])
Bound on condition number: tensor([ 122631.5222, 1458670.0600, 2746945.2649,  341260.8451])
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 0.004207591653773726, policy loss: 5.841345809422401
Experience 22, Iter 1, disc loss: 0.003742741396969034, policy loss: 6.083500178556421
Experience 22, Iter 2, disc loss: 0.004573455305202064, policy loss: 5.78119243235774
Experience 22, Iter 3, disc loss: 0.004411383584140071, policy loss: 5.7770736592154215
Experience 22, Iter 4, disc loss: 0.004351867142524971, policy loss: 5.8876114276138924
Experience 22, Iter 5, disc loss: 0.004300063952219104, policy loss: 5.910097793199452
Experience 22, Iter 6, disc loss: 0.00477442885936648, policy loss: 5.829643643174313
Experience 22, Iter 7, disc loss: 0.004433852895890378, policy loss: 6.067494866774027
Experience 22, Iter 8, disc loss: 0.004991807503906707, policy loss: 5.698454925058232
Experience 22, Iter 9, disc loss: 0.00512840552164205, policy loss: 5.839832681818264
Experience 22, Iter 10, disc loss: 0.004384149483393991, policy loss: 6.105526343922325
Experience 22, Iter 11, disc loss: 0.005261868097849745, policy loss: 5.727564804199218
Experience 22, Iter 12, disc loss: 0.0057821526041713376, policy loss: 5.671562778302511
Experience 22, Iter 13, disc loss: 0.00601701881254414, policy loss: 5.71945027930266
Experience 22, Iter 14, disc loss: 0.007150850286172801, policy loss: 5.584409769046944
Experience 22, Iter 15, disc loss: 0.01037310395386902, policy loss: 5.2153750233378044
Experience 22, Iter 16, disc loss: 0.01223874838566766, policy loss: 5.289021240876759
Experience 22, Iter 17, disc loss: 0.028657711609355315, policy loss: 5.271640726555973
Experience 22, Iter 18, disc loss: 0.029152340899422383, policy loss: 4.92271300405544
Experience 22, Iter 19, disc loss: 0.049466092859881745, policy loss: 4.517686388474706
Experience 22, Iter 20, disc loss: 0.13720839470893967, policy loss: 3.863191881593756
Experience 22, Iter 21, disc loss: 0.12996648152021556, policy loss: 4.243559094368723
Experience 22, Iter 22, disc loss: 0.13448763611559691, policy loss: 4.965446046393742
Experience 22, Iter 23, disc loss: 0.14981860626816795, policy loss: 4.770787248020174
Experience 22, Iter 24, disc loss: 0.1173991831393053, policy loss: 6.425332353130094
Experience 22, Iter 25, disc loss: 0.17358204531139998, policy loss: 5.748116533788344
Experience 22, Iter 26, disc loss: 0.18709967604837763, policy loss: 7.219728565258959
Experience 22, Iter 27, disc loss: 0.21852717475738193, policy loss: 6.974367221386071
Experience 22, Iter 28, disc loss: 0.15924986660064705, policy loss: 8.315044498381205
Experience 22, Iter 29, disc loss: 0.15261356520374875, policy loss: 6.936728605032327
Experience 22, Iter 30, disc loss: 0.12885387159679845, policy loss: 6.123753266493231
Experience 22, Iter 31, disc loss: 0.07681507763350715, policy loss: 6.330721017385963
Experience 22, Iter 32, disc loss: 0.07785159407749034, policy loss: 6.07148118132887
Experience 22, Iter 33, disc loss: 0.03600633374079962, policy loss: 6.083730422429383
Experience 22, Iter 34, disc loss: 0.03437334885851892, policy loss: 6.452440529909934
Experience 22, Iter 35, disc loss: 0.07259425492680541, policy loss: 5.112066431142426
Experience 22, Iter 36, disc loss: 0.1260476476401182, policy loss: 5.7374752818199015
Experience 22, Iter 37, disc loss: 0.05824780547694557, policy loss: 6.4783340326760115
Experience 22, Iter 38, disc loss: 0.131368639829207, policy loss: 5.315231543548327
Experience 22, Iter 39, disc loss: 0.05598693642507181, policy loss: 7.26938083281327
Experience 22, Iter 40, disc loss: 0.024336944068452354, policy loss: 7.39823945643695
Experience 22, Iter 41, disc loss: 0.031016929246568112, policy loss: 6.863411315347743
Experience 22, Iter 42, disc loss: 0.054513296506815793, policy loss: 7.304832025648254
Experience 22, Iter 43, disc loss: 0.02565785947762326, policy loss: 8.686815303277626
Experience 22, Iter 44, disc loss: 0.03324246241203459, policy loss: 8.331471867401847
Experience 22, Iter 45, disc loss: 0.0404987898969459, policy loss: 8.157711298373716
Experience 22, Iter 46, disc loss: 0.03273319452545098, policy loss: 8.303338256199105
Experience 22, Iter 47, disc loss: 0.03059742291404942, policy loss: 8.775145059402035
Experience 22, Iter 48, disc loss: 0.0285181780896089, policy loss: 8.703958456362653
Experience 22, Iter 49, disc loss: 0.02874606708884667, policy loss: 8.987782096404054
Experience 22, Iter 50, disc loss: 0.027946956218867707, policy loss: 9.018521641551567
Experience 22, Iter 51, disc loss: 0.02600129480466196, policy loss: 8.528046384631288
Experience 22, Iter 52, disc loss: 0.018445187825966262, policy loss: 8.539045259868583
Experience 22, Iter 53, disc loss: 0.02913808912892353, policy loss: 8.94184226163426
Experience 22, Iter 54, disc loss: 0.042571277859430434, policy loss: 6.720843487546228
Experience 22, Iter 55, disc loss: 0.01897725279711382, policy loss: 8.236785984464948
Experience 22, Iter 56, disc loss: 0.029809501040474973, policy loss: 7.9376759699915596
Experience 22, Iter 57, disc loss: 0.03273457276496339, policy loss: 7.128609486373548
Experience 22, Iter 58, disc loss: 0.025855656007917394, policy loss: 7.483506925906942
Experience 22, Iter 59, disc loss: 0.02470170438172483, policy loss: 7.909988274865237
Experience 22, Iter 60, disc loss: 0.025606717910912825, policy loss: 7.112772075370119
Experience 22, Iter 61, disc loss: 0.025399593242439625, policy loss: 7.896004261911506
Experience 22, Iter 62, disc loss: 0.014394276559678836, policy loss: 7.972466422688001
Experience 22, Iter 63, disc loss: 0.027987588390240493, policy loss: 7.350475455243732
Experience 22, Iter 64, disc loss: 0.011112243818243096, policy loss: 8.884973192831138
Experience 22, Iter 65, disc loss: 0.02614845395928575, policy loss: 7.373242186177027
Experience 22, Iter 66, disc loss: 0.03587739624349499, policy loss: 6.968579684075408
Experience 22, Iter 67, disc loss: 0.017032371428891045, policy loss: 8.41842040260762
Experience 22, Iter 68, disc loss: 0.04493617791117349, policy loss: 7.699122830264363
Experience 22, Iter 69, disc loss: 0.02789583174118477, policy loss: 8.338183660814726
Experience 22, Iter 70, disc loss: 0.02910460193756407, policy loss: 7.795678692160317
Experience 22, Iter 71, disc loss: 0.021440863657983054, policy loss: 8.5218546325882
Experience 22, Iter 72, disc loss: 0.020866590955052736, policy loss: 8.462476209948024
Experience 22, Iter 73, disc loss: 0.020017587740558805, policy loss: 8.073331969791756
Experience 22, Iter 74, disc loss: 0.02540702401653207, policy loss: 8.005501042576977
Experience 22, Iter 75, disc loss: 0.02351559416282591, policy loss: 8.411011419644419
Experience 22, Iter 76, disc loss: 0.01830924712290887, policy loss: 9.130828304724673
Experience 22, Iter 77, disc loss: 0.022325853410948816, policy loss: 7.603110115597496
Experience 22, Iter 78, disc loss: 0.033366979727108545, policy loss: 7.451817716619754
Experience 22, Iter 79, disc loss: 0.018158099274064303, policy loss: 8.380878625279443
Experience 22, Iter 80, disc loss: 0.024948647965323947, policy loss: 7.99888664156515
Experience 22, Iter 81, disc loss: 0.018298172563206778, policy loss: 8.186371464765768
Experience 22, Iter 82, disc loss: 0.013878204822356723, policy loss: 8.333278304304292
Experience 22, Iter 83, disc loss: 0.019264305774435893, policy loss: 8.174993032459264
Experience 22, Iter 84, disc loss: 0.017438490315036474, policy loss: 8.196782707587351
Experience 22, Iter 85, disc loss: 0.02382465351388578, policy loss: 7.786370679967072
Experience 22, Iter 86, disc loss: 0.016890167092442773, policy loss: 8.269500443775831
Experience 22, Iter 87, disc loss: 0.013502336457931726, policy loss: 8.965965031360268
Experience 22, Iter 88, disc loss: 0.030407504717647596, policy loss: 7.775983340606043
Experience 22, Iter 89, disc loss: 0.014982968123101726, policy loss: 8.19500533903602
Experience 22, Iter 90, disc loss: 0.013425073131403046, policy loss: 8.769280361947054
Experience 22, Iter 91, disc loss: 0.011331701558127103, policy loss: 8.526653081943202
Experience 22, Iter 92, disc loss: 0.01692759387791782, policy loss: 8.32034153148158
Experience 22, Iter 93, disc loss: 0.011948689908519664, policy loss: 8.073473544253241
Experience 22, Iter 94, disc loss: 0.022255133103353864, policy loss: 7.184944007097148
Experience 22, Iter 95, disc loss: 0.018594394672879294, policy loss: 8.34234935176363
Experience 22, Iter 96, disc loss: 0.02681584786308603, policy loss: 6.992789245372508
Experience 22, Iter 97, disc loss: 0.015527658698211282, policy loss: 8.130245996940348
Experience 22, Iter 98, disc loss: 0.012844097910212376, policy loss: 8.080687782626525
Experience 22, Iter 99, disc loss: 0.012850928460561992, policy loss: 8.491949795438307
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1080],
        [1.0924],
        [0.0092]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0131, 0.1118, 0.4962, 0.0085, 0.0041, 2.1719]],

        [[0.0131, 0.1118, 0.4962, 0.0085, 0.0041, 2.1719]],

        [[0.0131, 0.1118, 0.4962, 0.0085, 0.0041, 2.1719]],

        [[0.0131, 0.1118, 0.4962, 0.0085, 0.0041, 2.1719]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.4320, 4.3697, 0.0368], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.4320, 4.3697, 0.0368])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.102
Iter 2/2000 - Loss: 2.457
Iter 3/2000 - Loss: 2.064
Iter 4/2000 - Loss: 2.094
Iter 5/2000 - Loss: 2.205
Iter 6/2000 - Loss: 2.119
Iter 7/2000 - Loss: 1.974
Iter 8/2000 - Loss: 1.883
Iter 9/2000 - Loss: 1.840
Iter 10/2000 - Loss: 1.793
Iter 11/2000 - Loss: 1.704
Iter 12/2000 - Loss: 1.570
Iter 13/2000 - Loss: 1.408
Iter 14/2000 - Loss: 1.231
Iter 15/2000 - Loss: 1.047
Iter 16/2000 - Loss: 0.853
Iter 17/2000 - Loss: 0.640
Iter 18/2000 - Loss: 0.402
Iter 19/2000 - Loss: 0.139
Iter 20/2000 - Loss: -0.145
Iter 1981/2000 - Loss: -8.638
Iter 1982/2000 - Loss: -8.638
Iter 1983/2000 - Loss: -8.638
Iter 1984/2000 - Loss: -8.638
Iter 1985/2000 - Loss: -8.638
Iter 1986/2000 - Loss: -8.638
Iter 1987/2000 - Loss: -8.638
Iter 1988/2000 - Loss: -8.638
Iter 1989/2000 - Loss: -8.638
Iter 1990/2000 - Loss: -8.638
Iter 1991/2000 - Loss: -8.638
Iter 1992/2000 - Loss: -8.638
Iter 1993/2000 - Loss: -8.638
Iter 1994/2000 - Loss: -8.638
Iter 1995/2000 - Loss: -8.638
Iter 1996/2000 - Loss: -8.638
Iter 1997/2000 - Loss: -8.638
Iter 1998/2000 - Loss: -8.638
Iter 1999/2000 - Loss: -8.638
Iter 2000/2000 - Loss: -8.638
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.9839,  4.9144, 35.6421, 10.6686, 18.8830, 40.1435]],

        [[17.7918, 32.5958, 11.3850,  1.2953,  2.9272, 19.3529]],

        [[17.4717, 30.9939, 10.8189,  1.0958,  1.1901, 24.1886]],

        [[15.2046, 26.2920, 21.8017,  5.1918,  1.5333, 45.7343]]])
Signal Variance: tensor([ 0.0959,  1.8577, 20.4993,  0.6798])
Estimated target variance: tensor([0.0109, 0.4320, 4.3697, 0.0368])
N: 230
Signal to noise ratio: tensor([ 18.5879,  79.1311, 112.8231,  57.1085])
Bound on condition number: tensor([  79468.5739, 1440200.2526, 2927685.5185,  750118.6632])
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 0.04416405389905358, policy loss: 4.279406033703003
Experience 23, Iter 1, disc loss: 0.04852259772097599, policy loss: 4.338014225611829
Experience 23, Iter 2, disc loss: 0.040000383594559895, policy loss: 4.746721408471808
Experience 23, Iter 3, disc loss: 0.06816307868728222, policy loss: 4.029708878469428
Experience 23, Iter 4, disc loss: 0.06622803759766592, policy loss: 4.298160734943928
Experience 23, Iter 5, disc loss: 0.07306766566725756, policy loss: 4.408691609317859
Experience 23, Iter 6, disc loss: 0.0689126674871885, policy loss: 4.441134450415801
Experience 23, Iter 7, disc loss: 0.09390893620836371, policy loss: 3.9262802853417584
Experience 23, Iter 8, disc loss: 0.08703465733645492, policy loss: 4.845188098048767
Experience 23, Iter 9, disc loss: 0.09799944156194068, policy loss: 4.353059276630142
Experience 23, Iter 10, disc loss: 0.10423995102086875, policy loss: 4.473364566054475
Experience 23, Iter 11, disc loss: 0.09569553837834699, policy loss: 5.9688846756798455
Experience 23, Iter 12, disc loss: 0.06511554390241299, policy loss: 6.512756810360907
Experience 23, Iter 13, disc loss: 0.048745969004797704, policy loss: 7.502123409981974
Experience 23, Iter 14, disc loss: 0.037758013941484043, policy loss: 6.180103850399406
Experience 23, Iter 15, disc loss: 0.039481020693036835, policy loss: 4.574839898741558
Experience 23, Iter 16, disc loss: 0.047667798297652, policy loss: 6.095977083580433
Experience 23, Iter 17, disc loss: 0.0644615629812714, policy loss: 5.007068657617435
Experience 23, Iter 18, disc loss: 0.05127099057448503, policy loss: 4.1650969834441245
Experience 23, Iter 19, disc loss: 0.029894989433187597, policy loss: 4.924129085459445
Experience 23, Iter 20, disc loss: 0.022449273948948557, policy loss: 5.664374611820526
Experience 23, Iter 21, disc loss: 0.051485529051382685, policy loss: 4.188045994666423
Experience 23, Iter 22, disc loss: 0.07444290762471997, policy loss: 4.428202685477843
Experience 23, Iter 23, disc loss: 0.08199387600115084, policy loss: 4.3343345935233994
Experience 23, Iter 24, disc loss: 0.0732626131035769, policy loss: 4.00116410956775
Experience 23, Iter 25, disc loss: 0.02818531061808299, policy loss: 6.260841413003833
Experience 23, Iter 26, disc loss: 0.04791208793530438, policy loss: 5.105366054496939
Experience 23, Iter 27, disc loss: 0.052877337710895755, policy loss: 5.837958182403478
Experience 23, Iter 28, disc loss: 0.06053297936642463, policy loss: 5.242313391195023
Experience 23, Iter 29, disc loss: 0.05791067709781598, policy loss: 4.957084505310506
Experience 23, Iter 30, disc loss: 0.05535692264428624, policy loss: 5.327570092831405
Experience 23, Iter 31, disc loss: 0.05820471570425178, policy loss: 5.072149578469409
Experience 23, Iter 32, disc loss: 0.058061873519549265, policy loss: 6.0055721417384875
Experience 23, Iter 33, disc loss: 0.047677872759505635, policy loss: 5.263189130566818
Experience 23, Iter 34, disc loss: 0.033863122986570175, policy loss: 5.945654000834564
Experience 23, Iter 35, disc loss: 0.03346981136972827, policy loss: 5.246346049961741
Experience 23, Iter 36, disc loss: 0.04366717824615067, policy loss: 4.727844994041639
Experience 23, Iter 37, disc loss: 0.03582797789102009, policy loss: 5.7966559541780525
Experience 23, Iter 38, disc loss: 0.030788156337330053, policy loss: 4.874990918980491
Experience 23, Iter 39, disc loss: 0.01486033849397715, policy loss: 6.149024757807531
Experience 23, Iter 40, disc loss: 0.014572420499945773, policy loss: 6.102525899818827
Experience 23, Iter 41, disc loss: 0.020130778886392905, policy loss: 5.4225596520895305
Experience 23, Iter 42, disc loss: 0.05978703162220839, policy loss: 4.439161203257332
Experience 23, Iter 43, disc loss: 0.04010838369221503, policy loss: 5.447567044754613
Experience 23, Iter 44, disc loss: 0.0346116204339495, policy loss: 4.772774762632218
Experience 23, Iter 45, disc loss: 0.024183811894821255, policy loss: 5.095896252728602
Experience 23, Iter 46, disc loss: 0.020239157471053107, policy loss: 5.099843493924805
Experience 23, Iter 47, disc loss: 0.03071840245427984, policy loss: 4.904120441660233
Experience 23, Iter 48, disc loss: 0.03546148523997112, policy loss: 4.661053962875158
Experience 23, Iter 49, disc loss: 0.024342518977325228, policy loss: 4.93166275510405
Experience 23, Iter 50, disc loss: 0.02680905020984068, policy loss: 5.112251403331401
Experience 23, Iter 51, disc loss: 0.029336118926653217, policy loss: 5.010102173279121
Experience 23, Iter 52, disc loss: 0.03085400335124225, policy loss: 5.193907339858162
Experience 23, Iter 53, disc loss: 0.027617330805229524, policy loss: 6.08239130028644
Experience 23, Iter 54, disc loss: 0.024764995025373132, policy loss: 6.172244322428396
Experience 23, Iter 55, disc loss: 0.023616028238546728, policy loss: 6.452457620153963
Experience 23, Iter 56, disc loss: 0.026383990400728655, policy loss: 6.077897866322373
Experience 23, Iter 57, disc loss: 0.029821449371641856, policy loss: 5.795942426056213
Experience 23, Iter 58, disc loss: 0.026208837801956517, policy loss: 7.206911969291741
Experience 23, Iter 59, disc loss: 0.021731001456629116, policy loss: 6.11863922328379
Experience 23, Iter 60, disc loss: 0.018728251876375685, policy loss: 6.240681616374349
Experience 23, Iter 61, disc loss: 0.016487741788671663, policy loss: 5.89290731995308
Experience 23, Iter 62, disc loss: 0.023003962552425726, policy loss: 5.142768890843946
Experience 23, Iter 63, disc loss: 0.019113937856309604, policy loss: 6.822139639053098
Experience 23, Iter 64, disc loss: 0.017298468028667045, policy loss: 5.4803956496071775
Experience 23, Iter 65, disc loss: 0.011905090550018989, policy loss: 6.11287124944045
Experience 23, Iter 66, disc loss: 0.013820695718563446, policy loss: 5.716295529959193
Experience 23, Iter 67, disc loss: 0.023629959354903477, policy loss: 4.861506955436994
Experience 23, Iter 68, disc loss: 0.03202888270223917, policy loss: 4.664328215180658
Experience 23, Iter 69, disc loss: 0.027226135561813043, policy loss: 5.21099653104133
Experience 23, Iter 70, disc loss: 0.022311357450680698, policy loss: 5.005877944225661
Experience 23, Iter 71, disc loss: 0.023208635051274847, policy loss: 4.907258544371185
Experience 23, Iter 72, disc loss: 0.02399592580264383, policy loss: 5.206774818477147
Experience 23, Iter 73, disc loss: 0.02113490256129128, policy loss: 6.0326853036817205
Experience 23, Iter 74, disc loss: 0.02023807060725738, policy loss: 5.689740648801117
Experience 23, Iter 75, disc loss: 0.01615007966494646, policy loss: 6.170218389612327
Experience 23, Iter 76, disc loss: 0.01490002831881636, policy loss: 6.267709446580092
Experience 23, Iter 77, disc loss: 0.017602463586139427, policy loss: 5.833360975483073
Experience 23, Iter 78, disc loss: 0.023292958226550184, policy loss: 5.7081246626853686
Experience 23, Iter 79, disc loss: 0.022661698866853847, policy loss: 5.69593679623895
Experience 23, Iter 80, disc loss: 0.017843288857036673, policy loss: 6.49000512124111
Experience 23, Iter 81, disc loss: 0.01773846071772225, policy loss: 5.63359499360586
Experience 23, Iter 82, disc loss: 0.01651906901901063, policy loss: 5.9761340159410885
Experience 23, Iter 83, disc loss: 0.01913316745036949, policy loss: 5.576813402637709
Experience 23, Iter 84, disc loss: 0.017867431182995767, policy loss: 6.499967015539331
Experience 23, Iter 85, disc loss: 0.014299441417252777, policy loss: 6.301861195842578
Experience 23, Iter 86, disc loss: 0.010776448670549364, policy loss: 7.1737662324762095
Experience 23, Iter 87, disc loss: 0.011180878868395068, policy loss: 6.861598082774262
Experience 23, Iter 88, disc loss: 0.014327086551729436, policy loss: 6.222518579581718
Experience 23, Iter 89, disc loss: 0.015938926603519338, policy loss: 7.014732596731168
Experience 23, Iter 90, disc loss: 0.020328875712030674, policy loss: 5.046131540740598
Experience 23, Iter 91, disc loss: 0.014956616176787457, policy loss: 5.628383649446869
Experience 23, Iter 92, disc loss: 0.014502731082305022, policy loss: 5.505133670806684
Experience 23, Iter 93, disc loss: 0.013929212846869874, policy loss: 5.549141266567329
Experience 23, Iter 94, disc loss: 0.01761992026019401, policy loss: 5.480255268146704
Experience 23, Iter 95, disc loss: 0.01834233279063255, policy loss: 5.226766285975574
Experience 23, Iter 96, disc loss: 0.011155583467630199, policy loss: 5.958703167621125
Experience 23, Iter 97, disc loss: 0.012512006490438606, policy loss: 6.12093661717992
Experience 23, Iter 98, disc loss: 0.016151519822326196, policy loss: 5.396179287358075
Experience 23, Iter 99, disc loss: 0.01365474821307076, policy loss: 6.514175069871019
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1191],
        [1.1650],
        [0.0127]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.1165, 0.6262, 0.0096, 0.0068, 2.4526]],

        [[0.0128, 0.1165, 0.6262, 0.0096, 0.0068, 2.4526]],

        [[0.0128, 0.1165, 0.6262, 0.0096, 0.0068, 2.4526]],

        [[0.0128, 0.1165, 0.6262, 0.0096, 0.0068, 2.4526]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0112, 0.4764, 4.6602, 0.0507], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0112, 0.4764, 4.6602, 0.0507])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.357
Iter 2/2000 - Loss: 2.654
Iter 3/2000 - Loss: 2.308
Iter 4/2000 - Loss: 2.318
Iter 5/2000 - Loss: 2.410
Iter 6/2000 - Loss: 2.332
Iter 7/2000 - Loss: 2.187
Iter 8/2000 - Loss: 2.083
Iter 9/2000 - Loss: 2.024
Iter 10/2000 - Loss: 1.965
Iter 11/2000 - Loss: 1.866
Iter 12/2000 - Loss: 1.721
Iter 13/2000 - Loss: 1.546
Iter 14/2000 - Loss: 1.357
Iter 15/2000 - Loss: 1.161
Iter 16/2000 - Loss: 0.955
Iter 17/2000 - Loss: 0.731
Iter 18/2000 - Loss: 0.483
Iter 19/2000 - Loss: 0.212
Iter 20/2000 - Loss: -0.079
Iter 1981/2000 - Loss: -8.537
Iter 1982/2000 - Loss: -8.538
Iter 1983/2000 - Loss: -8.538
Iter 1984/2000 - Loss: -8.538
Iter 1985/2000 - Loss: -8.538
Iter 1986/2000 - Loss: -8.538
Iter 1987/2000 - Loss: -8.538
Iter 1988/2000 - Loss: -8.538
Iter 1989/2000 - Loss: -8.538
Iter 1990/2000 - Loss: -8.538
Iter 1991/2000 - Loss: -8.538
Iter 1992/2000 - Loss: -8.538
Iter 1993/2000 - Loss: -8.538
Iter 1994/2000 - Loss: -8.538
Iter 1995/2000 - Loss: -8.538
Iter 1996/2000 - Loss: -8.538
Iter 1997/2000 - Loss: -8.538
Iter 1998/2000 - Loss: -8.538
Iter 1999/2000 - Loss: -8.538
Iter 2000/2000 - Loss: -8.538
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.3836,  5.7321, 36.1557, 10.0819, 20.8734, 44.6214]],

        [[16.7078, 27.6090, 10.6743,  1.3420,  2.7187, 22.5861]],

        [[17.7041, 29.3244, 10.1248,  1.1955,  1.2636, 25.4079]],

        [[14.8607, 26.0807, 22.2251,  3.5426,  1.4259, 43.6625]]])
Signal Variance: tensor([ 0.1051,  2.2984, 23.6619,  0.6895])
Estimated target variance: tensor([0.0112, 0.4764, 4.6602, 0.0507])
N: 240
Signal to noise ratio: tensor([ 19.1853,  86.4648, 121.1613,  57.5371])
Bound on condition number: tensor([  88339.4061, 1794279.2987, 3523214.4698,  794523.9450])
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 0.01557795374894013, policy loss: 5.3959248020698585
Experience 24, Iter 1, disc loss: 0.012572009044459233, policy loss: 6.248196926815039
Experience 24, Iter 2, disc loss: 0.013907490304769028, policy loss: 6.1800310725720555
Experience 24, Iter 3, disc loss: 0.01622187026454921, policy loss: 5.953609029761296
Experience 24, Iter 4, disc loss: 0.0151146279487772, policy loss: 6.553107245427586
Experience 24, Iter 5, disc loss: 0.015110631443365052, policy loss: 6.764801913858843
Experience 24, Iter 6, disc loss: 0.014329723199253715, policy loss: 6.49645031463522
Experience 24, Iter 7, disc loss: 0.012765575132761199, policy loss: 6.239110163866378
Experience 24, Iter 8, disc loss: 0.012487090195487147, policy loss: 6.57840528294067
Experience 24, Iter 9, disc loss: 0.01110969456927043, policy loss: 6.218745736043923
Experience 24, Iter 10, disc loss: 0.012716620508352459, policy loss: 6.407159883914447
Experience 24, Iter 11, disc loss: 0.013484961651577027, policy loss: 6.078450975032386
Experience 24, Iter 12, disc loss: 0.010925331253222384, policy loss: 6.2549789885070055
Experience 24, Iter 13, disc loss: 0.009422451231373502, policy loss: 6.622906443616239
Experience 24, Iter 14, disc loss: 0.00998145383006916, policy loss: 6.7859407421661535
Experience 24, Iter 15, disc loss: 0.010325086786145181, policy loss: 6.63772901284706
Experience 24, Iter 16, disc loss: 0.01135010890532067, policy loss: 6.825625119303263
Experience 24, Iter 17, disc loss: 0.012063047270034093, policy loss: 6.03196252027961
Experience 24, Iter 18, disc loss: 0.011398671992789901, policy loss: 5.934079282768819
Experience 24, Iter 19, disc loss: 0.008974929104232622, policy loss: 6.396009727545618
Experience 24, Iter 20, disc loss: 0.011051640704225598, policy loss: 5.880831075070997
Experience 24, Iter 21, disc loss: 0.013007296033724094, policy loss: 6.446593344486031
Experience 24, Iter 22, disc loss: 0.015005174049138299, policy loss: 5.504164092600364
Experience 24, Iter 23, disc loss: 0.0097701757677785, policy loss: 6.233119774553907
Experience 24, Iter 24, disc loss: 0.011883181183778397, policy loss: 6.22306275510557
Experience 24, Iter 25, disc loss: 0.0148824181470113, policy loss: 5.617752494684529
Experience 24, Iter 26, disc loss: 0.011097846322330242, policy loss: 6.450307886842249
Experience 24, Iter 27, disc loss: 0.010680451037134402, policy loss: 6.183714700945197
Experience 24, Iter 28, disc loss: 0.009683022547108868, policy loss: 6.554938265834579
Experience 24, Iter 29, disc loss: 0.011068138229939226, policy loss: 6.060580088170055
Experience 24, Iter 30, disc loss: 0.012016002682281337, policy loss: 6.20996430176547
Experience 24, Iter 31, disc loss: 0.012328901585142283, policy loss: 6.781392927472801
Experience 24, Iter 32, disc loss: 0.00988902476173615, policy loss: 6.33030879104766
Experience 24, Iter 33, disc loss: 0.008769766232213476, policy loss: 6.62703069794156
Experience 24, Iter 34, disc loss: 0.007603409869068617, policy loss: 6.750732039017407
Experience 24, Iter 35, disc loss: 0.00910431199949513, policy loss: 6.7012587320105785
Experience 24, Iter 36, disc loss: 0.011872485253901301, policy loss: 6.697901060469544
Experience 24, Iter 37, disc loss: 0.011468924179547396, policy loss: 7.141326709782966
Experience 24, Iter 38, disc loss: 0.008200959501561826, policy loss: 6.573034504964408
Experience 24, Iter 39, disc loss: 0.005516065151421862, policy loss: 7.9468828985307844
Experience 24, Iter 40, disc loss: 0.005235894856921391, policy loss: 8.51227494110654
Experience 24, Iter 41, disc loss: 0.006681688573378006, policy loss: 6.798110869597055
Experience 24, Iter 42, disc loss: 0.01186527931990004, policy loss: 5.617011798287324
Experience 24, Iter 43, disc loss: 0.008455231657058589, policy loss: 9.247391143865611
Experience 24, Iter 44, disc loss: 0.00990142093193471, policy loss: 5.868003043340937
Experience 24, Iter 45, disc loss: 0.0056895368127534455, policy loss: 7.511016929064175
Experience 24, Iter 46, disc loss: 0.004640462043172281, policy loss: 8.070800415509165
Experience 24, Iter 47, disc loss: 0.00671467713977463, policy loss: 6.907986308604764
Experience 24, Iter 48, disc loss: 0.008092974127361992, policy loss: 6.305311492521156
Experience 24, Iter 49, disc loss: 0.009378719499567882, policy loss: 8.933952432347567
Experience 24, Iter 50, disc loss: 0.014228692982961193, policy loss: 5.668682731407831
Experience 24, Iter 51, disc loss: 0.0067810895616730454, policy loss: 6.927343764046281
Experience 24, Iter 52, disc loss: 0.0047228687060505824, policy loss: 7.532855784972282
Experience 24, Iter 53, disc loss: 0.0051461092322469875, policy loss: 7.388015670962575
Experience 24, Iter 54, disc loss: 0.00708942581158611, policy loss: 6.5202780971038115
Experience 24, Iter 55, disc loss: 0.009891253826725545, policy loss: 7.149994884526922
Experience 24, Iter 56, disc loss: 0.011920938371822223, policy loss: 5.619391365016031
Experience 24, Iter 57, disc loss: 0.00945848097763288, policy loss: 6.161473476226435
Experience 24, Iter 58, disc loss: 0.00687595547971789, policy loss: 6.497724052368866
Experience 24, Iter 59, disc loss: 0.00863290348732524, policy loss: 6.3143597451787095
Experience 24, Iter 60, disc loss: 0.009212353670215613, policy loss: 5.986921978356328
Experience 24, Iter 61, disc loss: 0.00948681107509095, policy loss: 5.885085280487113
Experience 24, Iter 62, disc loss: 0.010214539290592956, policy loss: 5.916295356024599
Experience 24, Iter 63, disc loss: 0.008489013308569347, policy loss: 6.262844358371842
Experience 24, Iter 64, disc loss: 0.0071467894458635195, policy loss: 6.626335195973747
Experience 24, Iter 65, disc loss: 0.010606889753069728, policy loss: 5.967163159798741
Experience 24, Iter 66, disc loss: 0.009534680940113439, policy loss: 6.70558811489064
Experience 24, Iter 67, disc loss: 0.0091506544951369, policy loss: 6.630167715246349
Experience 24, Iter 68, disc loss: 0.006045292025993909, policy loss: 7.252090514164556
Experience 24, Iter 69, disc loss: 0.005562082245709779, policy loss: 7.0890859883540305
Experience 24, Iter 70, disc loss: 0.00630318816240805, policy loss: 6.888943792103185
Experience 24, Iter 71, disc loss: 0.009899803857081228, policy loss: 6.2293402640546205
Experience 24, Iter 72, disc loss: 0.009413901329503257, policy loss: 6.951724384204459
Experience 24, Iter 73, disc loss: 0.009356786500694537, policy loss: 6.307675364785741
Experience 24, Iter 74, disc loss: 0.008006758969731995, policy loss: 6.6103597682006985
Experience 24, Iter 75, disc loss: 0.006277215203022634, policy loss: 7.235316965767732
Experience 24, Iter 76, disc loss: 0.007464192886821224, policy loss: 6.595995935572937
Experience 24, Iter 77, disc loss: 0.009038691742827863, policy loss: 6.531681224660517
Experience 24, Iter 78, disc loss: 0.008233958796770868, policy loss: 6.731798701698759
Experience 24, Iter 79, disc loss: 0.008809740819919856, policy loss: 6.234878448139796
Experience 24, Iter 80, disc loss: 0.006949768842129667, policy loss: 7.104472571782468
Experience 24, Iter 81, disc loss: 0.007245431473143473, policy loss: 7.084646889495994
Experience 24, Iter 82, disc loss: 0.008571766416180428, policy loss: 6.772157637347323
Experience 24, Iter 83, disc loss: 0.008007426587289002, policy loss: 7.060253424128726
Experience 24, Iter 84, disc loss: 0.008130630567386268, policy loss: 6.565964153451399
Experience 24, Iter 85, disc loss: 0.006258814062322775, policy loss: 6.875116043012708
Experience 24, Iter 86, disc loss: 0.006041809668747251, policy loss: 7.213322791793278
Experience 24, Iter 87, disc loss: 0.007022522704068362, policy loss: 6.3326400412514765
Experience 24, Iter 88, disc loss: 0.008534773149210077, policy loss: 6.442445690183769
Experience 24, Iter 89, disc loss: 0.006597485048804583, policy loss: 7.457672023161857
Experience 24, Iter 90, disc loss: 0.00861199167848176, policy loss: 6.094529497893041
Experience 24, Iter 91, disc loss: 0.005761627727004144, policy loss: 7.3788723290020855
Experience 24, Iter 92, disc loss: 0.005247884149068049, policy loss: 7.298922573328188
Experience 24, Iter 93, disc loss: 0.008115521246268888, policy loss: 6.645386239963289
Experience 24, Iter 94, disc loss: 0.008672679586489533, policy loss: 6.580314065888025
Experience 24, Iter 95, disc loss: 0.00795695088264068, policy loss: 6.610701541271888
Experience 24, Iter 96, disc loss: 0.006598185455190354, policy loss: 6.644036387690444
Experience 24, Iter 97, disc loss: 0.004840261954935109, policy loss: 7.0100963937638205
Experience 24, Iter 98, disc loss: 0.0048432067974861145, policy loss: 7.8770997571714805
Experience 24, Iter 99, disc loss: 0.005832867462238095, policy loss: 6.8579853559872745
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1246],
        [1.2087],
        [0.0144]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0124, 0.1186, 0.7045, 0.0107, 0.0081, 2.5935]],

        [[0.0124, 0.1186, 0.7045, 0.0107, 0.0081, 2.5935]],

        [[0.0124, 0.1186, 0.7045, 0.0107, 0.0081, 2.5935]],

        [[0.0124, 0.1186, 0.7045, 0.0107, 0.0081, 2.5935]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0112, 0.4985, 4.8348, 0.0576], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0112, 0.4985, 4.8348, 0.0576])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.464
Iter 2/2000 - Loss: 2.735
Iter 3/2000 - Loss: 2.409
Iter 4/2000 - Loss: 2.411
Iter 5/2000 - Loss: 2.489
Iter 6/2000 - Loss: 2.415
Iter 7/2000 - Loss: 2.268
Iter 8/2000 - Loss: 2.152
Iter 9/2000 - Loss: 2.084
Iter 10/2000 - Loss: 2.019
Iter 11/2000 - Loss: 1.914
Iter 12/2000 - Loss: 1.761
Iter 13/2000 - Loss: 1.578
Iter 14/2000 - Loss: 1.381
Iter 15/2000 - Loss: 1.180
Iter 16/2000 - Loss: 0.969
Iter 17/2000 - Loss: 0.740
Iter 18/2000 - Loss: 0.487
Iter 19/2000 - Loss: 0.210
Iter 20/2000 - Loss: -0.086
Iter 1981/2000 - Loss: -8.409
Iter 1982/2000 - Loss: -8.409
Iter 1983/2000 - Loss: -8.409
Iter 1984/2000 - Loss: -8.409
Iter 1985/2000 - Loss: -8.409
Iter 1986/2000 - Loss: -8.409
Iter 1987/2000 - Loss: -8.409
Iter 1988/2000 - Loss: -8.409
Iter 1989/2000 - Loss: -8.409
Iter 1990/2000 - Loss: -8.409
Iter 1991/2000 - Loss: -8.409
Iter 1992/2000 - Loss: -8.409
Iter 1993/2000 - Loss: -8.410
Iter 1994/2000 - Loss: -8.410
Iter 1995/2000 - Loss: -8.410
Iter 1996/2000 - Loss: -8.410
Iter 1997/2000 - Loss: -8.410
Iter 1998/2000 - Loss: -8.410
Iter 1999/2000 - Loss: -8.410
Iter 2000/2000 - Loss: -8.410
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[10.8694,  6.5809, 34.1465,  7.4916, 10.6471, 46.4387]],

        [[16.3018, 28.2385, 10.1326,  1.6053,  1.0132, 22.3425]],

        [[16.8009, 26.5063,  8.9969,  1.1746,  0.8725, 22.2443]],

        [[14.8247, 24.0579, 21.2634,  3.8247,  2.0143, 40.6212]]])
Signal Variance: tensor([ 0.1145,  2.2930, 16.9399,  0.8767])
Estimated target variance: tensor([0.0112, 0.4985, 4.8348, 0.0576])
N: 250
Signal to noise ratio: tensor([19.7709, 87.5581, 99.1461, 64.2956])
Bound on condition number: tensor([  97722.9234, 1916607.3780, 2457490.2099, 1033482.0417])
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 0.007018811654155854, policy loss: 6.929516576941762
Experience 25, Iter 1, disc loss: 0.007639919671123922, policy loss: 7.208090285328307
Experience 25, Iter 2, disc loss: 0.0074382180360284995, policy loss: 6.500210083844758
Experience 25, Iter 3, disc loss: 0.006552847935815977, policy loss: 6.5394188127597905
Experience 25, Iter 4, disc loss: 0.0054335669266817495, policy loss: 7.003446901759457
Experience 25, Iter 5, disc loss: 0.007424872856365827, policy loss: 6.720303116121414
Experience 25, Iter 6, disc loss: 0.006726994927213822, policy loss: 6.873345663492484
Experience 25, Iter 7, disc loss: 0.006514424824513096, policy loss: 6.5999588709851515
Experience 25, Iter 8, disc loss: 0.0064269152571628865, policy loss: 6.664151368725158
Experience 25, Iter 9, disc loss: 0.005510416551633527, policy loss: 6.874804361256763
Experience 25, Iter 10, disc loss: 0.004378814054753401, policy loss: 7.363903460889048
Experience 25, Iter 11, disc loss: 0.005382009653357222, policy loss: 6.751665130133208
Experience 25, Iter 12, disc loss: 0.006626373367728655, policy loss: 6.460797621190475
Experience 25, Iter 13, disc loss: 0.005670906246141024, policy loss: 6.835071382911333
Experience 25, Iter 14, disc loss: 0.00619195532578874, policy loss: 7.232063027689028
Experience 25, Iter 15, disc loss: 0.005637162350822781, policy loss: 6.65651037053075
Experience 25, Iter 16, disc loss: 0.006287542847132397, policy loss: 6.603842219996388
Experience 25, Iter 17, disc loss: 0.006191335773709912, policy loss: 6.92242638311371
Experience 25, Iter 18, disc loss: 0.005510281238956095, policy loss: 7.220880219760456
Experience 25, Iter 19, disc loss: 0.0061338873307008514, policy loss: 6.710944611233
Experience 25, Iter 20, disc loss: 0.005274123528963074, policy loss: 7.213863650306323
Experience 25, Iter 21, disc loss: 0.00579873568451255, policy loss: 6.964428888571087
Experience 25, Iter 22, disc loss: 0.00622159938780295, policy loss: 6.887499678949966
Experience 25, Iter 23, disc loss: 0.0055529996069837995, policy loss: 6.819446922058433
Experience 25, Iter 24, disc loss: 0.0074756259304831355, policy loss: 6.572257653173242
Experience 25, Iter 25, disc loss: 0.0050814594065576296, policy loss: 7.474001922404283
Experience 25, Iter 26, disc loss: 0.006121209641013283, policy loss: 6.885258360471992
Experience 25, Iter 27, disc loss: 0.006048135781652944, policy loss: 6.8062229305918205
Experience 25, Iter 28, disc loss: 0.00536510553382941, policy loss: 7.288739345923686
Experience 25, Iter 29, disc loss: 0.005840446097404739, policy loss: 6.621131423659582
Experience 25, Iter 30, disc loss: 0.0044711020337137655, policy loss: 7.263853175735972
Experience 25, Iter 31, disc loss: 0.006547915641073022, policy loss: 6.300542773177409
Experience 25, Iter 32, disc loss: 0.005379860132893721, policy loss: 6.638861360846591
Experience 25, Iter 33, disc loss: 0.0059595447794447715, policy loss: 6.916750821200575
Experience 25, Iter 34, disc loss: 0.006637581114376754, policy loss: 6.562466170843178
Experience 25, Iter 35, disc loss: 0.0063366612032008645, policy loss: 6.902499562542434
Experience 25, Iter 36, disc loss: 0.005271768792436817, policy loss: 6.75111029054597
Experience 25, Iter 37, disc loss: 0.00542403421892757, policy loss: 7.051347956811775
Experience 25, Iter 38, disc loss: 0.006229659171284433, policy loss: 6.927107294453442
Experience 25, Iter 39, disc loss: 0.0076998743113301715, policy loss: 6.647429741681902
Experience 25, Iter 40, disc loss: 0.006703358816316337, policy loss: 6.478271943988229
Experience 25, Iter 41, disc loss: 0.0038407848912076917, policy loss: 8.027274030761388
Experience 25, Iter 42, disc loss: 0.006454845384498977, policy loss: 7.1915012660559405
Experience 25, Iter 43, disc loss: 0.005831330249076651, policy loss: 6.674040171693733
Experience 25, Iter 44, disc loss: 0.005887296289110822, policy loss: 7.867050295432939
Experience 25, Iter 45, disc loss: 0.006106061622761356, policy loss: 6.467171982908736
Experience 25, Iter 46, disc loss: 0.004862183124604647, policy loss: 6.893818128052715
Experience 25, Iter 47, disc loss: 0.0052010879070695125, policy loss: 7.617739981136109
Experience 25, Iter 48, disc loss: 0.005885601306099083, policy loss: 7.185742427735799
Experience 25, Iter 49, disc loss: 0.00511321366125965, policy loss: 7.596050244335182
Experience 25, Iter 50, disc loss: 0.005563770923239909, policy loss: 7.400137914525338
Experience 25, Iter 51, disc loss: 0.005371187286156038, policy loss: 7.271345749991684
Experience 25, Iter 52, disc loss: 0.0033169695185382344, policy loss: 8.479804035708897
Experience 25, Iter 53, disc loss: 0.0035306043852098923, policy loss: 7.88723735045922
Experience 25, Iter 54, disc loss: 0.004300969989312816, policy loss: 7.522974533120871
Experience 25, Iter 55, disc loss: 0.004891790776571544, policy loss: 7.363804457677335
Experience 25, Iter 56, disc loss: 0.005122030074663733, policy loss: 7.467526636345754
Experience 25, Iter 57, disc loss: 0.004875664372100486, policy loss: 6.70438217733772
Experience 25, Iter 58, disc loss: 0.004865077455650909, policy loss: 6.861029839342727
Experience 25, Iter 59, disc loss: 0.005072821761057422, policy loss: 6.63005724522973
Experience 25, Iter 60, disc loss: 0.004561670788967452, policy loss: 6.973782430670398
Experience 25, Iter 61, disc loss: 0.004750098612237952, policy loss: 6.797172719078894
Experience 25, Iter 62, disc loss: 0.004892383764979018, policy loss: 7.381102384305361
Experience 25, Iter 63, disc loss: 0.004769350459127588, policy loss: 7.031703266859974
Experience 25, Iter 64, disc loss: 0.004228621820640563, policy loss: 7.254704396382366
Experience 25, Iter 65, disc loss: 0.003667471104666406, policy loss: 7.537827902136613
Experience 25, Iter 66, disc loss: 0.005639620068743645, policy loss: 6.208536282600178
Experience 25, Iter 67, disc loss: 0.004939701341414891, policy loss: 7.018693797968681
Experience 25, Iter 68, disc loss: 0.0050116312670163065, policy loss: 6.8032417597748704
Experience 25, Iter 69, disc loss: 0.004899814768871855, policy loss: 6.544911931420657
Experience 25, Iter 70, disc loss: 0.003707722775887642, policy loss: 7.73097458439675
Experience 25, Iter 71, disc loss: 0.005198411842548877, policy loss: 7.099746532372349
Experience 25, Iter 72, disc loss: 0.005780319372048962, policy loss: 6.7798345542316865
Experience 25, Iter 73, disc loss: 0.0042249690186286215, policy loss: 7.763501349927403
Experience 25, Iter 74, disc loss: 0.005617550529962563, policy loss: 6.2129510626269795
Experience 25, Iter 75, disc loss: 0.00419713031575681, policy loss: 7.460714753679526
Experience 25, Iter 76, disc loss: 0.0036948387689917706, policy loss: 7.441382015831721
Experience 25, Iter 77, disc loss: 0.004799312435918532, policy loss: 7.149407837967886
Experience 25, Iter 78, disc loss: 0.004609727661000065, policy loss: 8.63290687487455
Experience 25, Iter 79, disc loss: 0.005160162388679858, policy loss: 6.791644858528798
Experience 25, Iter 80, disc loss: 0.0040583899440609205, policy loss: 7.150037518443077
Experience 25, Iter 81, disc loss: 0.004219156489920691, policy loss: 7.369846743154929
Experience 25, Iter 82, disc loss: 0.00421165601202024, policy loss: 7.079317696606422
Experience 25, Iter 83, disc loss: 0.005523195220321432, policy loss: 6.737962824691278
Experience 25, Iter 84, disc loss: 0.0044074149664076676, policy loss: 7.256352357329446
Experience 25, Iter 85, disc loss: 0.004142630492800285, policy loss: 7.356976907426573
Experience 25, Iter 86, disc loss: 0.003963303187961475, policy loss: 7.351531113093601
Experience 25, Iter 87, disc loss: 0.00435227422209168, policy loss: 7.347333358509982
Experience 25, Iter 88, disc loss: 0.004665988770643489, policy loss: 7.02311204809094
Experience 25, Iter 89, disc loss: 0.004691454864903993, policy loss: 7.426897748110759
Experience 25, Iter 90, disc loss: 0.0054345771978019965, policy loss: 6.943542568905084
Experience 25, Iter 91, disc loss: 0.003491690689997139, policy loss: 8.374911214939488
Experience 25, Iter 92, disc loss: 0.003336023999610589, policy loss: 8.22131477337887
Experience 25, Iter 93, disc loss: 0.0037870027732402204, policy loss: 7.694370178547648
Experience 25, Iter 94, disc loss: 0.0053853991609471375, policy loss: 7.1109394905243715
Experience 25, Iter 95, disc loss: 0.005720777744573614, policy loss: 7.040129409075735
Experience 25, Iter 96, disc loss: 0.005034250747086995, policy loss: 6.979225513326743
Experience 25, Iter 97, disc loss: 0.004407744156340146, policy loss: 7.365246851120551
Experience 25, Iter 98, disc loss: 0.0040275145401692395, policy loss: 7.42237531154285
Experience 25, Iter 99, disc loss: 0.004477434370345066, policy loss: 6.935758996044742
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1299],
        [1.2512],
        [0.0161]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1203, 0.7788, 0.0117, 0.0092, 2.7239]],

        [[0.0122, 0.1203, 0.7788, 0.0117, 0.0092, 2.7239]],

        [[0.0122, 0.1203, 0.7788, 0.0117, 0.0092, 2.7239]],

        [[0.0122, 0.1203, 0.7788, 0.0117, 0.0092, 2.7239]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.5195, 5.0047, 0.0642], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.5195, 5.0047, 0.0642])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.558
Iter 2/2000 - Loss: 2.808
Iter 3/2000 - Loss: 2.494
Iter 4/2000 - Loss: 2.488
Iter 5/2000 - Loss: 2.547
Iter 6/2000 - Loss: 2.468
Iter 7/2000 - Loss: 2.317
Iter 8/2000 - Loss: 2.185
Iter 9/2000 - Loss: 2.096
Iter 10/2000 - Loss: 2.013
Iter 11/2000 - Loss: 1.892
Iter 12/2000 - Loss: 1.722
Iter 13/2000 - Loss: 1.518
Iter 14/2000 - Loss: 1.300
Iter 15/2000 - Loss: 1.079
Iter 16/2000 - Loss: 0.850
Iter 17/2000 - Loss: 0.608
Iter 18/2000 - Loss: 0.343
Iter 19/2000 - Loss: 0.056
Iter 20/2000 - Loss: -0.247
Iter 1981/2000 - Loss: -8.453
Iter 1982/2000 - Loss: -8.453
Iter 1983/2000 - Loss: -8.453
Iter 1984/2000 - Loss: -8.453
Iter 1985/2000 - Loss: -8.453
Iter 1986/2000 - Loss: -8.453
Iter 1987/2000 - Loss: -8.453
Iter 1988/2000 - Loss: -8.453
Iter 1989/2000 - Loss: -8.453
Iter 1990/2000 - Loss: -8.453
Iter 1991/2000 - Loss: -8.453
Iter 1992/2000 - Loss: -8.453
Iter 1993/2000 - Loss: -8.453
Iter 1994/2000 - Loss: -8.453
Iter 1995/2000 - Loss: -8.453
Iter 1996/2000 - Loss: -8.454
Iter 1997/2000 - Loss: -8.454
Iter 1998/2000 - Loss: -8.454
Iter 1999/2000 - Loss: -8.454
Iter 2000/2000 - Loss: -8.454
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[10.7669,  7.4279, 37.4299,  7.6994, 11.9243, 47.1751]],

        [[15.6352, 28.8745, 10.0481,  1.5304,  0.9673, 21.1686]],

        [[16.3381, 27.4847,  8.7055,  1.1714,  0.8613, 22.7951]],

        [[14.2237, 23.7748, 21.9549,  4.1261,  2.0661, 41.3979]]])
Signal Variance: tensor([ 0.1238,  1.9225, 16.9100,  0.9373])
Estimated target variance: tensor([0.0113, 0.5195, 5.0047, 0.0642])
N: 260
Signal to noise ratio: tensor([20.7538, 78.8110, 99.4672, 67.3495])
Bound on condition number: tensor([ 111987.9332, 1614904.2003, 2572370.9517, 1179350.1494])
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 0.004000451547358985, policy loss: 7.785395821831962
Experience 26, Iter 1, disc loss: 0.004066004229040373, policy loss: 7.111250106795213
Experience 26, Iter 2, disc loss: 0.003207508590523048, policy loss: 7.629172248675187
Experience 26, Iter 3, disc loss: 0.003439278930426195, policy loss: 7.478858436507474
Experience 26, Iter 4, disc loss: 0.0039757455827207945, policy loss: 7.15214138629703
Experience 26, Iter 5, disc loss: 0.004382560555145135, policy loss: 7.30396059852106
Experience 26, Iter 6, disc loss: 0.003805814303151762, policy loss: 8.31343370193651
Experience 26, Iter 7, disc loss: 0.004059816611663509, policy loss: 7.278476525255597
Experience 26, Iter 8, disc loss: 0.002990829306090839, policy loss: 8.079726901053704
Experience 26, Iter 9, disc loss: 0.002527584223985484, policy loss: 8.702767679769348
Experience 26, Iter 10, disc loss: 0.004010116152814689, policy loss: 7.4564179673826185
Experience 26, Iter 11, disc loss: 0.0042409616421452655, policy loss: 7.40051061529875
Experience 26, Iter 12, disc loss: 0.004165187087267085, policy loss: 8.864272756508853
Experience 26, Iter 13, disc loss: 0.005367916520813846, policy loss: 7.151436634329947
Experience 26, Iter 14, disc loss: 0.003303634545620302, policy loss: 7.8551570169057525
Experience 26, Iter 15, disc loss: 0.0025299170230691642, policy loss: 8.538661935997546
Experience 26, Iter 16, disc loss: 0.003224149954521284, policy loss: 7.71051588661444
Experience 26, Iter 17, disc loss: 0.004376121347387798, policy loss: 6.661263527412891
Experience 26, Iter 18, disc loss: 0.0034421817134314405, policy loss: 8.917459715493706
Experience 26, Iter 19, disc loss: 0.00454895247513124, policy loss: 6.921224128274833
Experience 26, Iter 20, disc loss: 0.0037003031313516833, policy loss: 7.480699093642643
Experience 26, Iter 21, disc loss: 0.002471323127158828, policy loss: 7.985849837488529
Experience 26, Iter 22, disc loss: 0.0023596441747162876, policy loss: 8.443153219969245
Experience 26, Iter 23, disc loss: 0.004007837142778355, policy loss: 7.047284370466905
Experience 26, Iter 24, disc loss: 0.004175869113658876, policy loss: 7.672244604309105
Experience 26, Iter 25, disc loss: 0.004872770064069637, policy loss: 7.481149364825235
Experience 26, Iter 26, disc loss: 0.004253690259028458, policy loss: 6.9593606879759005
Experience 26, Iter 27, disc loss: 0.0038566058602692308, policy loss: 6.906723659834299
Experience 26, Iter 28, disc loss: 0.0035309426334439254, policy loss: 7.269493593502686
Experience 26, Iter 29, disc loss: 0.0044034756682896845, policy loss: 6.659289275732545
Experience 26, Iter 30, disc loss: 0.0042333114197495025, policy loss: 7.231695710531639
Experience 26, Iter 31, disc loss: 0.004158694401085386, policy loss: 7.333282954944193
Experience 26, Iter 32, disc loss: 0.004942148412947295, policy loss: 7.463063941820611
Experience 26, Iter 33, disc loss: 0.004303370831882427, policy loss: 7.34931225445878
Experience 26, Iter 34, disc loss: 0.0035118453106837565, policy loss: 7.269187047127538
Experience 26, Iter 35, disc loss: 0.003624842133637892, policy loss: 7.873812955075742
Experience 26, Iter 36, disc loss: 0.00456102000914333, policy loss: 6.9291254482779845
Experience 26, Iter 37, disc loss: 0.003918004669443958, policy loss: 7.260913693865188
Experience 26, Iter 38, disc loss: 0.0030953458179373058, policy loss: 7.739407393181452
Experience 26, Iter 39, disc loss: 0.0039323697709466076, policy loss: 7.511172253012981
Experience 26, Iter 40, disc loss: 0.003891780219123848, policy loss: 7.484620185962147
Experience 26, Iter 41, disc loss: 0.003997155465521926, policy loss: 7.618334816795526
Experience 26, Iter 42, disc loss: 0.003721871713452011, policy loss: 7.647933987818216
Experience 26, Iter 43, disc loss: 0.0034480273804765966, policy loss: 7.804817009333165
Experience 26, Iter 44, disc loss: 0.0030348794934237505, policy loss: 9.271706982464071
Experience 26, Iter 45, disc loss: 0.0033823949329813132, policy loss: 7.49947359059631
Experience 26, Iter 46, disc loss: 0.004859372753143987, policy loss: 7.084345104568637
Experience 26, Iter 47, disc loss: 0.003518155944857571, policy loss: 8.330907967185187
Experience 26, Iter 48, disc loss: 0.0035906803224368855, policy loss: 7.682297258986009
Experience 26, Iter 49, disc loss: 0.003538023871399641, policy loss: 7.608597539785684
Experience 26, Iter 50, disc loss: 0.0032851923885400657, policy loss: 7.720698198576447
Experience 26, Iter 51, disc loss: 0.0035635664056801668, policy loss: 7.065458480505415
Experience 26, Iter 52, disc loss: 0.003913037223551548, policy loss: 7.211536999765084
Experience 26, Iter 53, disc loss: 0.003616046622223641, policy loss: 7.786200688426849
Experience 26, Iter 54, disc loss: 0.004001435876886415, policy loss: 7.065508193447437
Experience 26, Iter 55, disc loss: 0.0035483266988754524, policy loss: 7.233252778278688
Experience 26, Iter 56, disc loss: 0.003446318155645832, policy loss: 8.26341425167891
Experience 26, Iter 57, disc loss: 0.003352157171602675, policy loss: 7.732596791275693
Experience 26, Iter 58, disc loss: 0.00397236365179904, policy loss: 7.2584942058436335
Experience 26, Iter 59, disc loss: 0.003484152686292826, policy loss: 7.397211010774319
Experience 26, Iter 60, disc loss: 0.0041666323424423436, policy loss: 7.306282426128874
Experience 26, Iter 61, disc loss: 0.003525053391916634, policy loss: 7.120599655728547
Experience 26, Iter 62, disc loss: 0.0028845327669387794, policy loss: 7.841345511469181
Experience 26, Iter 63, disc loss: 0.003075987994666453, policy loss: 7.8255087564899375
Experience 26, Iter 64, disc loss: 0.003643374771105076, policy loss: 7.389715630645757
Experience 26, Iter 65, disc loss: 0.0033510062020851526, policy loss: 7.337530702441095
Experience 26, Iter 66, disc loss: 0.0031121336674919957, policy loss: 7.56955716653647
Experience 26, Iter 67, disc loss: 0.003878095955142381, policy loss: 6.845331428119053
Experience 26, Iter 68, disc loss: 0.0035588998624647146, policy loss: 7.470054323699895
Experience 26, Iter 69, disc loss: 0.0029776969289564362, policy loss: 7.773880878762145
Experience 26, Iter 70, disc loss: 0.003746699700099366, policy loss: 7.128381610224712
Experience 26, Iter 71, disc loss: 0.004276178463385081, policy loss: 7.291011264410086
Experience 26, Iter 72, disc loss: 0.0033302575972674305, policy loss: 7.981139391326303
Experience 26, Iter 73, disc loss: 0.0041519677414371126, policy loss: 7.33990766440367
Experience 26, Iter 74, disc loss: 0.0036415842194742853, policy loss: 7.50692388781566
Experience 26, Iter 75, disc loss: 0.002629453828696003, policy loss: 8.159555927958092
Experience 26, Iter 76, disc loss: 0.0030079927701328604, policy loss: 7.3288335766997506
Experience 26, Iter 77, disc loss: 0.004073822609959393, policy loss: 7.084431451473439
Experience 26, Iter 78, disc loss: 0.003352105866846542, policy loss: 7.912212790117113
Experience 26, Iter 79, disc loss: 0.0036790058793329137, policy loss: 7.705468991021354
Experience 26, Iter 80, disc loss: 0.002842527353808021, policy loss: 7.43167805598157
Experience 26, Iter 81, disc loss: 0.0037029686086240484, policy loss: 7.06303782344325
Experience 26, Iter 82, disc loss: 0.0034433894525747076, policy loss: 7.475528590635404
Experience 26, Iter 83, disc loss: 0.003415225047965177, policy loss: 7.938005565584515
Experience 26, Iter 84, disc loss: 0.003578981136536635, policy loss: 7.051155865993508
Experience 26, Iter 85, disc loss: 0.003450071022853605, policy loss: 7.085070990656164
Experience 26, Iter 86, disc loss: 0.003094388509702848, policy loss: 7.555312274776043
Experience 26, Iter 87, disc loss: 0.003554940354133894, policy loss: 7.139958317520033
Experience 26, Iter 88, disc loss: 0.0031762617513004555, policy loss: 7.403811737653099
Experience 26, Iter 89, disc loss: 0.0032223017800092676, policy loss: 7.8131526630868295
Experience 26, Iter 90, disc loss: 0.0027998257123496777, policy loss: 8.164939834256153
Experience 26, Iter 91, disc loss: 0.0027413716726735447, policy loss: 7.575034332074132
Experience 26, Iter 92, disc loss: 0.0036898077179974655, policy loss: 6.8534827095204784
Experience 26, Iter 93, disc loss: 0.003037776432352101, policy loss: 8.134944223396797
Experience 26, Iter 94, disc loss: 0.0030189701962579506, policy loss: 7.5401689364383415
Experience 26, Iter 95, disc loss: 0.002274360461180903, policy loss: 8.468295233699308
Experience 26, Iter 96, disc loss: 0.002215662189534959, policy loss: 8.18291767442276
Experience 26, Iter 97, disc loss: 0.0023067376368882666, policy loss: 7.970527212715042
Experience 26, Iter 98, disc loss: 0.003056633156007593, policy loss: 7.688354146154658
Experience 26, Iter 99, disc loss: 0.00312927269078655, policy loss: 8.24604622383411
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1380],
        [1.3025],
        [0.0177]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0119, 0.1230, 0.8534, 0.0126, 0.0104, 2.9145]],

        [[0.0119, 0.1230, 0.8534, 0.0126, 0.0104, 2.9145]],

        [[0.0119, 0.1230, 0.8534, 0.0126, 0.0104, 2.9145]],

        [[0.0119, 0.1230, 0.8534, 0.0126, 0.0104, 2.9145]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0115, 0.5519, 5.2098, 0.0708], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0115, 0.5519, 5.2098, 0.0708])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.661
Iter 2/2000 - Loss: 2.890
Iter 3/2000 - Loss: 2.590
Iter 4/2000 - Loss: 2.583
Iter 5/2000 - Loss: 2.628
Iter 6/2000 - Loss: 2.545
Iter 7/2000 - Loss: 2.399
Iter 8/2000 - Loss: 2.268
Iter 9/2000 - Loss: 2.172
Iter 10/2000 - Loss: 2.081
Iter 11/2000 - Loss: 1.956
Iter 12/2000 - Loss: 1.785
Iter 13/2000 - Loss: 1.580
Iter 14/2000 - Loss: 1.362
Iter 15/2000 - Loss: 1.139
Iter 16/2000 - Loss: 0.911
Iter 17/2000 - Loss: 0.669
Iter 18/2000 - Loss: 0.407
Iter 19/2000 - Loss: 0.122
Iter 20/2000 - Loss: -0.179
Iter 1981/2000 - Loss: -8.442
Iter 1982/2000 - Loss: -8.442
Iter 1983/2000 - Loss: -8.442
Iter 1984/2000 - Loss: -8.442
Iter 1985/2000 - Loss: -8.442
Iter 1986/2000 - Loss: -8.443
Iter 1987/2000 - Loss: -8.443
Iter 1988/2000 - Loss: -8.443
Iter 1989/2000 - Loss: -8.443
Iter 1990/2000 - Loss: -8.443
Iter 1991/2000 - Loss: -8.443
Iter 1992/2000 - Loss: -8.443
Iter 1993/2000 - Loss: -8.443
Iter 1994/2000 - Loss: -8.443
Iter 1995/2000 - Loss: -8.443
Iter 1996/2000 - Loss: -8.443
Iter 1997/2000 - Loss: -8.443
Iter 1998/2000 - Loss: -8.443
Iter 1999/2000 - Loss: -8.443
Iter 2000/2000 - Loss: -8.443
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[11.0939,  6.0977, 37.1465,  6.9944,  9.3425, 46.7801]],

        [[13.9861, 30.0967,  9.4483,  1.5251,  1.0255, 20.8521]],

        [[15.1273, 29.2846,  8.3318,  1.2022,  0.8751, 21.9331]],

        [[15.6037, 25.7760, 21.2126,  3.4097,  2.0789, 39.3428]]])
Signal Variance: tensor([ 0.1095,  1.8072, 15.4425,  0.8408])
Estimated target variance: tensor([0.0115, 0.5519, 5.2098, 0.0708])
N: 270
Signal to noise ratio: tensor([19.5708, 75.6803, 93.9096, 63.9971])
Bound on condition number: tensor([ 103415.0136, 1546429.3549, 2381137.0184, 1105821.6766])
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 0.003073916278783967, policy loss: 7.810642174705287
Experience 27, Iter 1, disc loss: 0.0023509726746396464, policy loss: 8.17033919472608
Experience 27, Iter 2, disc loss: 0.002402285887249761, policy loss: 8.022054020123859
Experience 27, Iter 3, disc loss: 0.0025983784324958336, policy loss: 8.236257256643949
Experience 27, Iter 4, disc loss: 0.003693849224538426, policy loss: 7.591721177360624
Experience 27, Iter 5, disc loss: 0.0025533306332219848, policy loss: 9.6161217688136
Experience 27, Iter 6, disc loss: 0.0033442601930053434, policy loss: 6.786835324598714
Experience 27, Iter 7, disc loss: 0.0026816348066188832, policy loss: 7.441809612417456
Experience 27, Iter 8, disc loss: 0.002318644975790016, policy loss: 7.882702937219438
Experience 27, Iter 9, disc loss: 0.003044600835504418, policy loss: 7.830676910111485
Experience 27, Iter 10, disc loss: 0.0033180551115447286, policy loss: 7.092859481289964
Experience 27, Iter 11, disc loss: 0.003545416518361595, policy loss: 7.542495140453781
Experience 27, Iter 12, disc loss: 0.00319707173347106, policy loss: 7.31628642080172
Experience 27, Iter 13, disc loss: 0.003132901106758547, policy loss: 7.081908242905644
Experience 27, Iter 14, disc loss: 0.0031288015896394857, policy loss: 7.186019942125416
Experience 27, Iter 15, disc loss: 0.003232426458112653, policy loss: 7.533053011608582
Experience 27, Iter 16, disc loss: 0.003002988264908686, policy loss: 7.833346971905049
Experience 27, Iter 17, disc loss: 0.0026820012912699516, policy loss: 7.401620519740664
Experience 27, Iter 18, disc loss: 0.0027855086960641663, policy loss: 7.465136379728809
Experience 27, Iter 19, disc loss: 0.0028668743678044338, policy loss: 7.43422920104851
Experience 27, Iter 20, disc loss: 0.0031623985928618155, policy loss: 7.303558431338393
Experience 27, Iter 21, disc loss: 0.0030225341244723165, policy loss: 8.486106763504557
Experience 27, Iter 22, disc loss: 0.002956019845881299, policy loss: 7.5416927815672326
Experience 27, Iter 23, disc loss: 0.002145650017306632, policy loss: 8.640706844424653
Experience 27, Iter 24, disc loss: 0.0027044610277960103, policy loss: 8.013872915574225
Experience 27, Iter 25, disc loss: 0.003038376434238376, policy loss: 7.4169952108676345
Experience 27, Iter 26, disc loss: 0.002615177886489208, policy loss: 7.94524358602197
Experience 27, Iter 27, disc loss: 0.0028028235224894895, policy loss: 7.259451022227809
Experience 27, Iter 28, disc loss: 0.003016746166193793, policy loss: 7.503275298207952
Experience 27, Iter 29, disc loss: 0.0022110363891960734, policy loss: 8.25803657446539
Experience 27, Iter 30, disc loss: 0.0027871312727244238, policy loss: 7.85894148459282
Experience 27, Iter 31, disc loss: 0.0031145955730534974, policy loss: 8.203016187354352
Experience 27, Iter 32, disc loss: 0.003181712256969729, policy loss: 7.857277247557478
Experience 27, Iter 33, disc loss: 0.0031639699728378684, policy loss: 7.814027146438012
Experience 27, Iter 34, disc loss: 0.0037019121833297797, policy loss: 7.72824104937971
Experience 27, Iter 35, disc loss: 0.0030359698374729156, policy loss: 7.146016732120857
Experience 27, Iter 36, disc loss: 0.0028736744985442425, policy loss: 7.388725156038602
Experience 27, Iter 37, disc loss: 0.0030287245750551858, policy loss: 7.696732980300498
Experience 27, Iter 38, disc loss: 0.00283875407899815, policy loss: 7.974238588117858
Experience 27, Iter 39, disc loss: 0.0028975477414525277, policy loss: 7.509348812688826
Experience 27, Iter 40, disc loss: 0.0026235541892691574, policy loss: 7.664263079199838
Experience 27, Iter 41, disc loss: 0.00291338716072052, policy loss: 7.73442877163264
Experience 27, Iter 42, disc loss: 0.0032233033179347795, policy loss: 7.3521105801761575
Experience 27, Iter 43, disc loss: 0.0029721269131194965, policy loss: 7.747948199015411
Experience 27, Iter 44, disc loss: 0.0031972825370363157, policy loss: 7.702479796351678
Experience 27, Iter 45, disc loss: 0.0024443776670849985, policy loss: 8.1456719727024
Experience 27, Iter 46, disc loss: 0.002034413677306531, policy loss: 8.277451518496505
Experience 27, Iter 47, disc loss: 0.0023454836277640393, policy loss: 8.80140812041875
Experience 27, Iter 48, disc loss: 0.002422561639621458, policy loss: 7.975348888593478
Experience 27, Iter 49, disc loss: 0.0028911235517961867, policy loss: 7.790548108410703
Experience 27, Iter 50, disc loss: 0.003190654978973849, policy loss: 8.35859723067503
Experience 27, Iter 51, disc loss: 0.0030613364160113686, policy loss: 8.429616543960046
Experience 27, Iter 52, disc loss: 0.0025178454373680726, policy loss: 8.221730114649958
Experience 27, Iter 53, disc loss: 0.0017968719565671452, policy loss: 9.358821523771384
Experience 27, Iter 54, disc loss: 0.001760282794437382, policy loss: 9.488754079321884
Experience 27, Iter 55, disc loss: 0.0024360446239560275, policy loss: 8.032649838204572
Experience 27, Iter 56, disc loss: 0.002521427922548298, policy loss: 9.446958718113924
Experience 27, Iter 57, disc loss: 0.0026292919531359952, policy loss: 8.184827847601598
Experience 27, Iter 58, disc loss: 0.0025492699723402854, policy loss: 7.327338126594354
Experience 27, Iter 59, disc loss: 0.002137256510122868, policy loss: 7.832637012415075
Experience 27, Iter 60, disc loss: 0.0020935799176204223, policy loss: 7.791751094393405
Experience 27, Iter 61, disc loss: 0.0023437930905106613, policy loss: 7.905422841800748
Experience 27, Iter 62, disc loss: 0.002574077466476356, policy loss: 7.8329569653035955
Experience 27, Iter 63, disc loss: 0.002318835839541435, policy loss: 7.462437963606348
Experience 27, Iter 64, disc loss: 0.002831542362258999, policy loss: 7.098684494277505
Experience 27, Iter 65, disc loss: 0.0027112936184093654, policy loss: 7.263257154767368
Experience 27, Iter 66, disc loss: 0.0021588007975396127, policy loss: 7.78763867486289
Experience 27, Iter 67, disc loss: 0.0024510918403631244, policy loss: 7.737077056057402
Experience 27, Iter 68, disc loss: 0.0030748346813724357, policy loss: 7.341181909788641
Experience 27, Iter 69, disc loss: 0.0026493312631737528, policy loss: 8.318101377502046
Experience 27, Iter 70, disc loss: 0.0022049263994942, policy loss: 7.700801712672743
Experience 27, Iter 71, disc loss: 0.0029765870253674735, policy loss: 7.4851940779251755
Experience 27, Iter 72, disc loss: 0.0034880616362745966, policy loss: 7.3847202258590325
Experience 27, Iter 73, disc loss: 0.0024865312638732502, policy loss: 7.6876316157381925
Experience 27, Iter 74, disc loss: 0.0031865047267340867, policy loss: 7.125411993558964
Experience 27, Iter 75, disc loss: 0.003391943322419447, policy loss: 7.976898234505175
Experience 27, Iter 76, disc loss: 0.0027395630814187146, policy loss: 7.2273660459601805
Experience 27, Iter 77, disc loss: 0.002568896584575234, policy loss: 7.482872682865342
Experience 27, Iter 78, disc loss: 0.0023204064575001854, policy loss: 7.800727941300937
Experience 27, Iter 79, disc loss: 0.002502582235027706, policy loss: 8.01047667981825
Experience 27, Iter 80, disc loss: 0.0027407505729246362, policy loss: 8.413351876247578
Experience 27, Iter 81, disc loss: 0.003160767174655437, policy loss: 7.479854804828216
Experience 27, Iter 82, disc loss: 0.0024860997226871396, policy loss: 7.946946623708806
Experience 27, Iter 83, disc loss: 0.0025341153574793334, policy loss: 7.834087118814765
Experience 27, Iter 84, disc loss: 0.0025451177101059744, policy loss: 7.5517200960396185
Experience 27, Iter 85, disc loss: 0.0028012993895868473, policy loss: 7.427368504153204
Experience 27, Iter 86, disc loss: 0.002850366117213768, policy loss: 8.142760818081722
Experience 27, Iter 87, disc loss: 0.002853049572944257, policy loss: 7.759738878738289
Experience 27, Iter 88, disc loss: 0.002836592808255608, policy loss: 7.509472430227477
Experience 27, Iter 89, disc loss: 0.0026472930331862144, policy loss: 7.498004794436909
Experience 27, Iter 90, disc loss: 0.001989623080014746, policy loss: 8.76067137155053
Experience 27, Iter 91, disc loss: 0.0024894218498955566, policy loss: 7.64842765732334
Experience 27, Iter 92, disc loss: 0.0029569875920688487, policy loss: 8.283059659898154
Experience 27, Iter 93, disc loss: 0.00280005152089526, policy loss: 8.034056990243148
Experience 27, Iter 94, disc loss: 0.003143949642792707, policy loss: 7.827858328233903
Experience 27, Iter 95, disc loss: 0.0021343359352389165, policy loss: 9.063573951070058
Experience 27, Iter 96, disc loss: 0.002544424571702843, policy loss: 7.965917113522927
Experience 27, Iter 97, disc loss: 0.0022302514068969293, policy loss: 7.926954749069361
Experience 27, Iter 98, disc loss: 0.002045024590888687, policy loss: 8.573337675863744
Experience 27, Iter 99, disc loss: 0.002684933681101356, policy loss: 7.443554311058989
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1457],
        [1.3516],
        [0.0209]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0117, 0.1268, 0.9731, 0.0135, 0.0126, 3.1213]],

        [[0.0117, 0.1268, 0.9731, 0.0135, 0.0126, 3.1213]],

        [[0.0117, 0.1268, 0.9731, 0.0135, 0.0126, 3.1213]],

        [[0.0117, 0.1268, 0.9731, 0.0135, 0.0126, 3.1213]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0118, 0.5830, 5.4064, 0.0837], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0118, 0.5830, 5.4064, 0.0837])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.801
Iter 2/2000 - Loss: 3.007
Iter 3/2000 - Loss: 2.715
Iter 4/2000 - Loss: 2.710
Iter 5/2000 - Loss: 2.743
Iter 6/2000 - Loss: 2.650
Iter 7/2000 - Loss: 2.506
Iter 8/2000 - Loss: 2.377
Iter 9/2000 - Loss: 2.275
Iter 10/2000 - Loss: 2.172
Iter 11/2000 - Loss: 2.034
Iter 12/2000 - Loss: 1.853
Iter 13/2000 - Loss: 1.639
Iter 14/2000 - Loss: 1.410
Iter 15/2000 - Loss: 1.173
Iter 16/2000 - Loss: 0.930
Iter 17/2000 - Loss: 0.674
Iter 18/2000 - Loss: 0.400
Iter 19/2000 - Loss: 0.109
Iter 20/2000 - Loss: -0.197
Iter 1981/2000 - Loss: -8.417
Iter 1982/2000 - Loss: -8.417
Iter 1983/2000 - Loss: -8.417
Iter 1984/2000 - Loss: -8.417
Iter 1985/2000 - Loss: -8.417
Iter 1986/2000 - Loss: -8.417
Iter 1987/2000 - Loss: -8.417
Iter 1988/2000 - Loss: -8.417
Iter 1989/2000 - Loss: -8.417
Iter 1990/2000 - Loss: -8.417
Iter 1991/2000 - Loss: -8.417
Iter 1992/2000 - Loss: -8.417
Iter 1993/2000 - Loss: -8.417
Iter 1994/2000 - Loss: -8.417
Iter 1995/2000 - Loss: -8.417
Iter 1996/2000 - Loss: -8.417
Iter 1997/2000 - Loss: -8.417
Iter 1998/2000 - Loss: -8.417
Iter 1999/2000 - Loss: -8.417
Iter 2000/2000 - Loss: -8.417
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[10.1637,  5.0524, 34.4145,  6.1489, 12.4406, 46.0365]],

        [[14.3561, 28.4282,  8.3520,  1.4155,  1.2413, 24.0769]],

        [[16.0184, 28.2724,  8.1584,  1.1467,  0.8483, 23.6616]],

        [[15.0324, 24.7414, 17.6302,  2.3530,  1.8534, 37.7980]]])
Signal Variance: tensor([ 0.0981,  2.0219, 16.6477,  0.5975])
Estimated target variance: tensor([0.0118, 0.5830, 5.4064, 0.0837])
N: 280
Signal to noise ratio: tensor([18.6534, 80.1215, 98.7338, 53.7494])
Bound on condition number: tensor([  97426.5427, 1797447.0309, 2729541.9065,  808920.5648])
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 0.0024925299005134406, policy loss: 7.888922307705029
Experience 28, Iter 1, disc loss: 0.002478223835969443, policy loss: 8.263735900429651
Experience 28, Iter 2, disc loss: 0.0023757745099620225, policy loss: 8.134768247050438
Experience 28, Iter 3, disc loss: 0.002070442962170105, policy loss: 7.982637297922868
Experience 28, Iter 4, disc loss: 0.0021245434270440082, policy loss: 8.419228140890668
Experience 28, Iter 5, disc loss: 0.0021948515970159144, policy loss: 7.825351480663611
Experience 28, Iter 6, disc loss: 0.0024467243826577063, policy loss: 8.219251034068607
Experience 28, Iter 7, disc loss: 0.0023986807610040972, policy loss: 7.944258295084469
Experience 28, Iter 8, disc loss: 0.0026266153579713137, policy loss: 7.703145810220933
Experience 28, Iter 9, disc loss: 0.002466258232607729, policy loss: 7.9301323220882605
Experience 28, Iter 10, disc loss: 0.0022933793890959103, policy loss: 7.692417070969414
Experience 28, Iter 11, disc loss: 0.0020010424638905074, policy loss: 8.049645184624206
Experience 28, Iter 12, disc loss: 0.002394199361153021, policy loss: 8.474082287656495
Experience 28, Iter 13, disc loss: 0.0022634175075765587, policy loss: 7.875859609701518
Experience 28, Iter 14, disc loss: 0.002001903052096665, policy loss: 8.415258604331742
Experience 28, Iter 15, disc loss: 0.0020196839804125745, policy loss: 8.175696851357024
Experience 28, Iter 16, disc loss: 0.0026348416098657575, policy loss: 7.588635397078146
Experience 28, Iter 17, disc loss: 0.0023126470660274354, policy loss: 8.014568962400407
Experience 28, Iter 18, disc loss: 0.0022860794560909652, policy loss: 8.270916985755216
Experience 28, Iter 19, disc loss: 0.0025475436148941826, policy loss: 7.598281365164074
Experience 28, Iter 20, disc loss: 0.001759860624828128, policy loss: 7.9769986608864905
Experience 28, Iter 21, disc loss: 0.002473050542707407, policy loss: 7.373864476223096
Experience 28, Iter 22, disc loss: 0.002277215902855105, policy loss: 8.086328324628273
Experience 28, Iter 23, disc loss: 0.002259085912196034, policy loss: 8.368800273806388
Experience 28, Iter 24, disc loss: 0.0018295600907485615, policy loss: 8.173166770851202
Experience 28, Iter 25, disc loss: 0.0023887633883449624, policy loss: 8.012813449631723
Experience 28, Iter 26, disc loss: 0.0023575665759205363, policy loss: 8.068490646313823
Experience 28, Iter 27, disc loss: 0.0023761667861885278, policy loss: 8.322450394269708
Experience 28, Iter 28, disc loss: 0.0022802721628790755, policy loss: 8.12200689469909
Experience 28, Iter 29, disc loss: 0.0019701961345886866, policy loss: 7.9413932444231845
Experience 28, Iter 30, disc loss: 0.002469095327892824, policy loss: 7.983063612211634
Experience 28, Iter 31, disc loss: 0.0021747718957312884, policy loss: 7.75521727348012
Experience 28, Iter 32, disc loss: 0.0022083152776498924, policy loss: 8.412144451778023
Experience 28, Iter 33, disc loss: 0.0021358927207672626, policy loss: 8.451283151456684
Experience 28, Iter 34, disc loss: 0.0019158919575122143, policy loss: 8.422755119183396
Experience 28, Iter 35, disc loss: 0.0023056385327777642, policy loss: 8.16532292138524
Experience 28, Iter 36, disc loss: 0.0022363708584628352, policy loss: 8.0471346081441
Experience 28, Iter 37, disc loss: 0.0020677106504364607, policy loss: 8.132097917099637
Experience 28, Iter 38, disc loss: 0.0018543360556956935, policy loss: 8.34626246290937
Experience 28, Iter 39, disc loss: 0.002013541434624947, policy loss: 8.355102561321091
Experience 28, Iter 40, disc loss: 0.002417115501791245, policy loss: 8.291344327071863
Experience 28, Iter 41, disc loss: 0.0021853620690592663, policy loss: 7.996133699029483
Experience 28, Iter 42, disc loss: 0.0015631877064543167, policy loss: 8.406441519906913
Experience 28, Iter 43, disc loss: 0.0022257340794424197, policy loss: 7.5858097150952855
Experience 28, Iter 44, disc loss: 0.002267490137928682, policy loss: 7.880042225735264
Experience 28, Iter 45, disc loss: 0.0022002202254924608, policy loss: 8.700437606530958
Experience 28, Iter 46, disc loss: 0.0022346591482519766, policy loss: 8.010933930764807
Experience 28, Iter 47, disc loss: 0.0016718472280484449, policy loss: 8.933336808219128
Experience 28, Iter 48, disc loss: 0.0013096951681666458, policy loss: 9.423785330786547
Experience 28, Iter 49, disc loss: 0.0014888749796653797, policy loss: 8.726128276753087
Experience 28, Iter 50, disc loss: 0.002330939364959688, policy loss: 7.931930068260639
Experience 28, Iter 51, disc loss: 0.001999419338354299, policy loss: 9.797716409999097
Experience 28, Iter 52, disc loss: 0.0023167297504283237, policy loss: 7.5832760344476
Experience 28, Iter 53, disc loss: 0.0015896148992955387, policy loss: 8.26580376045701
Experience 28, Iter 54, disc loss: 0.0012147880104062024, policy loss: 9.17972072350387
Experience 28, Iter 55, disc loss: 0.0014193368932352579, policy loss: 9.326309228992388
Experience 28, Iter 56, disc loss: 0.0016591646372438565, policy loss: 8.294924672800878
Experience 28, Iter 57, disc loss: 0.001846139976983859, policy loss: 8.717476454852603
Experience 28, Iter 58, disc loss: 0.0018931282851867405, policy loss: 9.199727834635082
Experience 28, Iter 59, disc loss: 0.0017637917559089995, policy loss: 7.770783146983447
Experience 28, Iter 60, disc loss: 0.0015523216432051599, policy loss: 8.500242244412556
Experience 28, Iter 61, disc loss: 0.0013624198812585645, policy loss: 8.320210713363615
Experience 28, Iter 62, disc loss: 0.0013844753044614695, policy loss: 8.53440332862119
Experience 28, Iter 63, disc loss: 0.001977701963578821, policy loss: 8.141941222203897
Experience 28, Iter 64, disc loss: 0.0018360222313355055, policy loss: 9.625002827726952
Experience 28, Iter 65, disc loss: 0.0016557813072078241, policy loss: 9.324883242559359
Experience 28, Iter 66, disc loss: 0.0014697671731100292, policy loss: 8.305495089087634
Experience 28, Iter 67, disc loss: 0.0010293145770589461, policy loss: 9.492595231613677
Experience 28, Iter 68, disc loss: 0.0010629160138169696, policy loss: 9.477074997780024
Experience 28, Iter 69, disc loss: 0.0014910288903806025, policy loss: 8.766756797865238
Experience 28, Iter 70, disc loss: 0.0018258964425544094, policy loss: 7.5738969446466955
Experience 28, Iter 71, disc loss: 0.0016514168516575094, policy loss: 9.64583589518326
Experience 28, Iter 72, disc loss: 0.0018553506509519488, policy loss: 8.711267518389583
Experience 28, Iter 73, disc loss: 0.0016930142319023288, policy loss: 7.664022204766992
Experience 28, Iter 74, disc loss: 0.0014185565161866438, policy loss: 8.710362919713315
Experience 28, Iter 75, disc loss: 0.0011427650467897281, policy loss: 9.386859313798967
Experience 28, Iter 76, disc loss: 0.0016094069335661476, policy loss: 8.073137699690113
Experience 28, Iter 77, disc loss: 0.0022626897717059583, policy loss: 8.149462250672602
Experience 28, Iter 78, disc loss: 0.0019190059148379663, policy loss: 9.917376010224222
Experience 28, Iter 79, disc loss: 0.002262523433269917, policy loss: 7.433907185965428
Experience 28, Iter 80, disc loss: 0.0020276935910189977, policy loss: 7.795079609262417
Experience 28, Iter 81, disc loss: 0.0013490160560096873, policy loss: 8.665707271833396
Experience 28, Iter 82, disc loss: 0.0015269898491695546, policy loss: 8.394558407881783
Experience 28, Iter 83, disc loss: 0.0023329920286386196, policy loss: 7.517069987125392
Experience 28, Iter 84, disc loss: 0.0020336904573198264, policy loss: 7.840091862840566
Experience 28, Iter 85, disc loss: 0.0020836936775366164, policy loss: 8.704144404787158
Experience 28, Iter 86, disc loss: 0.002198742552721949, policy loss: 7.966876235192093
Experience 28, Iter 87, disc loss: 0.0012372879916021074, policy loss: 8.631368487632178
Experience 28, Iter 88, disc loss: 0.0011569938367012751, policy loss: 8.82333542956318
Experience 28, Iter 89, disc loss: 0.0016805542665080222, policy loss: 8.857300573327633
Experience 28, Iter 90, disc loss: 0.002591795400914733, policy loss: 7.534317279175044
Experience 28, Iter 91, disc loss: 0.0022058562346957645, policy loss: 8.232169728182459
Experience 28, Iter 92, disc loss: 0.0024819737746988165, policy loss: 8.074408425156461
Experience 28, Iter 93, disc loss: 0.001873392955757567, policy loss: 7.914502701218547
Experience 28, Iter 94, disc loss: 0.0015713549084144375, policy loss: 8.464332804133988
Experience 28, Iter 95, disc loss: 0.0021816415699581703, policy loss: 7.555468611299551
Experience 28, Iter 96, disc loss: 0.002411374521583035, policy loss: 7.956972727001355
Experience 28, Iter 97, disc loss: 0.001807457467224449, policy loss: 8.991162887110404
Experience 28, Iter 98, disc loss: 0.0018607796826179415, policy loss: 8.52400019661835
Experience 28, Iter 99, disc loss: 0.0016601681289584516, policy loss: 8.35517606331554
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1557],
        [1.4141],
        [0.0233]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0115, 0.1313, 1.0715, 0.0144, 0.0140, 3.3143]],

        [[0.0115, 0.1313, 1.0715, 0.0144, 0.0140, 3.3143]],

        [[0.0115, 0.1313, 1.0715, 0.0144, 0.0140, 3.3143]],

        [[0.0115, 0.1313, 1.0715, 0.0144, 0.0140, 3.3143]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0121, 0.6228, 5.6563, 0.0932], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0121, 0.6228, 5.6563, 0.0932])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.919
Iter 2/2000 - Loss: 3.105
Iter 3/2000 - Loss: 2.821
Iter 4/2000 - Loss: 2.817
Iter 5/2000 - Loss: 2.843
Iter 6/2000 - Loss: 2.742
Iter 7/2000 - Loss: 2.597
Iter 8/2000 - Loss: 2.471
Iter 9/2000 - Loss: 2.369
Iter 10/2000 - Loss: 2.260
Iter 11/2000 - Loss: 2.116
Iter 12/2000 - Loss: 1.929
Iter 13/2000 - Loss: 1.713
Iter 14/2000 - Loss: 1.483
Iter 15/2000 - Loss: 1.246
Iter 16/2000 - Loss: 1.001
Iter 17/2000 - Loss: 0.741
Iter 18/2000 - Loss: 0.462
Iter 19/2000 - Loss: 0.165
Iter 20/2000 - Loss: -0.147
Iter 1981/2000 - Loss: -8.381
Iter 1982/2000 - Loss: -8.381
Iter 1983/2000 - Loss: -8.381
Iter 1984/2000 - Loss: -8.381
Iter 1985/2000 - Loss: -8.381
Iter 1986/2000 - Loss: -8.381
Iter 1987/2000 - Loss: -8.382
Iter 1988/2000 - Loss: -8.382
Iter 1989/2000 - Loss: -8.382
Iter 1990/2000 - Loss: -8.382
Iter 1991/2000 - Loss: -8.382
Iter 1992/2000 - Loss: -8.382
Iter 1993/2000 - Loss: -8.382
Iter 1994/2000 - Loss: -8.382
Iter 1995/2000 - Loss: -8.382
Iter 1996/2000 - Loss: -8.382
Iter 1997/2000 - Loss: -8.382
Iter 1998/2000 - Loss: -8.382
Iter 1999/2000 - Loss: -8.382
Iter 2000/2000 - Loss: -8.382
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.8276,  5.7621, 36.5612,  5.5257, 12.2732, 45.5085]],

        [[13.9792, 29.7355,  8.6723,  1.1967,  1.4135, 26.6413]],

        [[15.9535, 29.8854,  7.8573,  1.0816,  0.8571, 24.9919]],

        [[14.9590, 23.1560, 18.1673,  2.6971,  1.8375, 38.9682]]])
Signal Variance: tensor([ 0.0996,  2.2181, 15.9277,  0.6637])
Estimated target variance: tensor([0.0121, 0.6228, 5.6563, 0.0932])
N: 290
Signal to noise ratio: tensor([18.5661, 82.0048, 96.1535, 56.6858])
Bound on condition number: tensor([  99964.4498, 1950190.8654, 2681194.4119,  931852.6712])
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.0014323088964477242, policy loss: 8.824311256168182
Experience 29, Iter 1, disc loss: 0.0021381286414475773, policy loss: 7.915291156076568
Experience 29, Iter 2, disc loss: 0.001925886161108142, policy loss: 8.121712853781407
Experience 29, Iter 3, disc loss: 0.0019775872313899283, policy loss: 9.035421455911646
Experience 29, Iter 4, disc loss: 0.0020267726266215797, policy loss: 7.486998667643663
Experience 29, Iter 5, disc loss: 0.001933000636575353, policy loss: 8.494534218207933
Experience 29, Iter 6, disc loss: 0.0016605293893473692, policy loss: 8.368610079133434
Experience 29, Iter 7, disc loss: 0.0019923175484046138, policy loss: 8.026224027641888
Experience 29, Iter 8, disc loss: 0.0017866599165477469, policy loss: 8.243299997153832
Experience 29, Iter 9, disc loss: 0.0016029409132462126, policy loss: 8.572257896422323
Experience 29, Iter 10, disc loss: 0.0017355907769295836, policy loss: 8.432442627042196
Experience 29, Iter 11, disc loss: 0.0017969345839876484, policy loss: 8.356456254865455
Experience 29, Iter 12, disc loss: 0.0018043021722810978, policy loss: 7.835231886742963
Experience 29, Iter 13, disc loss: 0.001711112568161588, policy loss: 8.576088039686324
Experience 29, Iter 14, disc loss: 0.0017088226566888648, policy loss: 8.3492880359627
Experience 29, Iter 15, disc loss: 0.001699144390403677, policy loss: 8.382422487459456
Experience 29, Iter 16, disc loss: 0.0017466201770199837, policy loss: 8.18378482869647
Experience 29, Iter 17, disc loss: 0.0015659989829109127, policy loss: 8.419602671793339
Experience 29, Iter 18, disc loss: 0.0014649248195648945, policy loss: 8.496712713491227
Experience 29, Iter 19, disc loss: 0.001905126058317309, policy loss: 7.510047616060765
Experience 29, Iter 20, disc loss: 0.0020510428314264597, policy loss: 7.814904916500072
Experience 29, Iter 21, disc loss: 0.0019402894693931207, policy loss: 8.09308112145112
Experience 29, Iter 22, disc loss: 0.0016773435391433745, policy loss: 8.536299139054819
Experience 29, Iter 23, disc loss: 0.0013373816058642996, policy loss: 8.131838903621396
Experience 29, Iter 24, disc loss: 0.001527781091044361, policy loss: 8.224029668410191
Experience 29, Iter 25, disc loss: 0.0018451695687430496, policy loss: 7.963809497321019
Experience 29, Iter 26, disc loss: 0.001986994515677948, policy loss: 8.28377926431922
Experience 29, Iter 27, disc loss: 0.0018529808077060012, policy loss: 8.017780768466723
Experience 29, Iter 28, disc loss: 0.0015002704243283682, policy loss: 9.095898390441393
Experience 29, Iter 29, disc loss: 0.0018622231848264635, policy loss: 7.664585370693684
Experience 29, Iter 30, disc loss: 0.0017168262703390917, policy loss: 8.000060778013793
Experience 29, Iter 31, disc loss: 0.0015148444452688981, policy loss: 8.636226976846846
Experience 29, Iter 32, disc loss: 0.0017058771679666772, policy loss: 7.9605739099576915
Experience 29, Iter 33, disc loss: 0.0016325796378659375, policy loss: 8.374676228467923
Experience 29, Iter 34, disc loss: 0.0017955726619526028, policy loss: 8.798250681190861
Experience 29, Iter 35, disc loss: 0.0017456361477845718, policy loss: 8.247064261989152
Experience 29, Iter 36, disc loss: 0.0017556189528059615, policy loss: 8.117880793829105
Experience 29, Iter 37, disc loss: 0.0017510506385393543, policy loss: 7.930486202040242
Experience 29, Iter 38, disc loss: 0.0018048022180749423, policy loss: 7.992456954375452
Experience 29, Iter 39, disc loss: 0.0017633202574587497, policy loss: 8.123853291856861
Experience 29, Iter 40, disc loss: 0.0018369176823220524, policy loss: 8.648440349969633
Experience 29, Iter 41, disc loss: 0.001526821168148075, policy loss: 8.710107296505846
Experience 29, Iter 42, disc loss: 0.0016888981573190923, policy loss: 7.959081835016178
Experience 29, Iter 43, disc loss: 0.0016589533683846703, policy loss: 8.216867740114525
Experience 29, Iter 44, disc loss: 0.0014493214324641727, policy loss: 8.43516119061498
Experience 29, Iter 45, disc loss: 0.0018477729276726756, policy loss: 8.386304671124382
Experience 29, Iter 46, disc loss: 0.0017633359673034372, policy loss: 8.33378352238495
Experience 29, Iter 47, disc loss: 0.0017632973602281798, policy loss: 8.184275699877709
Experience 29, Iter 48, disc loss: 0.0016445971860467867, policy loss: 8.353978193813937
Experience 29, Iter 49, disc loss: 0.0013023744030866643, policy loss: 8.940357130448351
Experience 29, Iter 50, disc loss: 0.0014162449504083268, policy loss: 9.369959970070402
Experience 29, Iter 51, disc loss: 0.0015303346636790188, policy loss: 8.466412292145208
Experience 29, Iter 52, disc loss: 0.0018734245505212868, policy loss: 7.77796947409784
Experience 29, Iter 53, disc loss: 0.001741578316159029, policy loss: 8.799262362355988
Experience 29, Iter 54, disc loss: 0.0017714789647875738, policy loss: 8.493773117052164
Experience 29, Iter 55, disc loss: 0.0016722035988614337, policy loss: 8.909005589728528
Experience 29, Iter 56, disc loss: 0.0012736062492486279, policy loss: 8.926355899100654
Experience 29, Iter 57, disc loss: 0.00128375471308317, policy loss: 8.54143943661001
Experience 29, Iter 58, disc loss: 0.0016747078153734605, policy loss: 8.19027889486809
Experience 29, Iter 59, disc loss: 0.0018507669031941792, policy loss: 8.255560317764335
Experience 29, Iter 60, disc loss: 0.0021898001015165404, policy loss: 7.831048157897415
Experience 29, Iter 61, disc loss: 0.0015987249606610313, policy loss: 8.29906208220774
Experience 29, Iter 62, disc loss: 0.0015338966457638364, policy loss: 8.194639168276918
Experience 29, Iter 63, disc loss: 0.001785633839359196, policy loss: 7.922439123598569
Experience 29, Iter 64, disc loss: 0.0016416887483387743, policy loss: 8.192947186451175
Experience 29, Iter 65, disc loss: 0.0016360215212229104, policy loss: 8.205502249451191
Experience 29, Iter 66, disc loss: 0.0016759124712566156, policy loss: 8.538732815657077
Experience 29, Iter 67, disc loss: 0.0017722086333530056, policy loss: 7.8178808186209245
Experience 29, Iter 68, disc loss: 0.001772459770859985, policy loss: 7.890557565376102
Experience 29, Iter 69, disc loss: 0.0018312385390248564, policy loss: 7.768627987827285
Experience 29, Iter 70, disc loss: 0.0012945239562732747, policy loss: 8.338935778126887
Experience 29, Iter 71, disc loss: 0.0015848601755466766, policy loss: 8.758977058083616
Experience 29, Iter 72, disc loss: 0.0016189877171604002, policy loss: 8.067267757123386
Experience 29, Iter 73, disc loss: 0.001822031627623598, policy loss: 8.035686048285047
Experience 29, Iter 74, disc loss: 0.0018765299676932671, policy loss: 7.6170157653695165
Experience 29, Iter 75, disc loss: 0.0018411181591558169, policy loss: 8.853807677400795
Experience 29, Iter 76, disc loss: 0.0016640342904507615, policy loss: 8.265404390212394
Experience 29, Iter 77, disc loss: 0.0015484657304742845, policy loss: 8.674193114196495
Experience 29, Iter 78, disc loss: 0.0014401726967011017, policy loss: 8.910807920880973
Experience 29, Iter 79, disc loss: 0.0015130835173659462, policy loss: 8.00774323788912
Experience 29, Iter 80, disc loss: 0.001622880365541206, policy loss: 8.24951931847827
Experience 29, Iter 81, disc loss: 0.0014352924448124572, policy loss: 8.73362585733356
Experience 29, Iter 82, disc loss: 0.001308906688930015, policy loss: 9.530454693895859
Experience 29, Iter 83, disc loss: 0.0014755879899762444, policy loss: 8.141721342132362
Experience 29, Iter 84, disc loss: 0.0015278797801707472, policy loss: 8.236503540059315
Experience 29, Iter 85, disc loss: 0.001474089413021302, policy loss: 8.411126125252682
Experience 29, Iter 86, disc loss: 0.001391936319250307, policy loss: 8.543242746089513
Experience 29, Iter 87, disc loss: 0.0013460433061847207, policy loss: 8.561250507953417
Experience 29, Iter 88, disc loss: 0.0015074572324770411, policy loss: 7.949547571659221
Experience 29, Iter 89, disc loss: 0.0013900706573003654, policy loss: 7.935590652018169
Experience 29, Iter 90, disc loss: 0.0014935950437349407, policy loss: 7.87108830981762
Experience 29, Iter 91, disc loss: 0.0015877000141205327, policy loss: 7.781350869707433
Experience 29, Iter 92, disc loss: 0.001532846722796259, policy loss: 7.667312517689107
Experience 29, Iter 93, disc loss: 0.0016519004700686225, policy loss: 8.827292658955697
Experience 29, Iter 94, disc loss: 0.001431218116594882, policy loss: 8.954817386255431
Experience 29, Iter 95, disc loss: 0.0011438914650447837, policy loss: 8.535149262739997
Experience 29, Iter 96, disc loss: 0.0015128320850732267, policy loss: 8.91562007380432
Experience 29, Iter 97, disc loss: 0.0017730940262427996, policy loss: 8.276209932091275
Experience 29, Iter 98, disc loss: 0.0017101787445127255, policy loss: 9.030742593973985
Experience 29, Iter 99, disc loss: 0.0017401197249218854, policy loss: 8.801534146990912
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1624],
        [1.4515],
        [0.0248]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0113, 0.1332, 1.1301, 0.0151, 0.0154, 3.4866]],

        [[0.0113, 0.1332, 1.1301, 0.0151, 0.0154, 3.4866]],

        [[0.0113, 0.1332, 1.1301, 0.0151, 0.0154, 3.4866]],

        [[0.0113, 0.1332, 1.1301, 0.0151, 0.0154, 3.4866]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0122, 0.6498, 5.8061, 0.0992], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0122, 0.6498, 5.8061, 0.0992])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.990
Iter 2/2000 - Loss: 3.169
Iter 3/2000 - Loss: 2.879
Iter 4/2000 - Loss: 2.879
Iter 5/2000 - Loss: 2.902
Iter 6/2000 - Loss: 2.794
Iter 7/2000 - Loss: 2.642
Iter 8/2000 - Loss: 2.513
Iter 9/2000 - Loss: 2.407
Iter 10/2000 - Loss: 2.294
Iter 11/2000 - Loss: 2.143
Iter 12/2000 - Loss: 1.949
Iter 13/2000 - Loss: 1.727
Iter 14/2000 - Loss: 1.490
Iter 15/2000 - Loss: 1.247
Iter 16/2000 - Loss: 0.995
Iter 17/2000 - Loss: 0.729
Iter 18/2000 - Loss: 0.445
Iter 19/2000 - Loss: 0.144
Iter 20/2000 - Loss: -0.170
Iter 1981/2000 - Loss: -8.319
Iter 1982/2000 - Loss: -8.319
Iter 1983/2000 - Loss: -8.319
Iter 1984/2000 - Loss: -8.319
Iter 1985/2000 - Loss: -8.319
Iter 1986/2000 - Loss: -8.319
Iter 1987/2000 - Loss: -8.319
Iter 1988/2000 - Loss: -8.319
Iter 1989/2000 - Loss: -8.319
Iter 1990/2000 - Loss: -8.319
Iter 1991/2000 - Loss: -8.319
Iter 1992/2000 - Loss: -8.319
Iter 1993/2000 - Loss: -8.319
Iter 1994/2000 - Loss: -8.319
Iter 1995/2000 - Loss: -8.319
Iter 1996/2000 - Loss: -8.319
Iter 1997/2000 - Loss: -8.319
Iter 1998/2000 - Loss: -8.319
Iter 1999/2000 - Loss: -8.319
Iter 2000/2000 - Loss: -8.319
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.0762,  6.9918, 35.0670,  1.8378, 23.2131, 48.3407]],

        [[16.5016, 30.2662,  7.6828,  1.2479,  1.3249, 25.3193]],

        [[15.2334, 29.7345,  8.1331,  1.0288,  0.6295, 22.7670]],

        [[14.3027, 23.2827, 15.2116,  2.2069,  1.6114, 36.8813]]])
Signal Variance: tensor([ 0.1027,  1.9812, 13.7940,  0.4999])
Estimated target variance: tensor([0.0122, 0.6498, 5.8061, 0.0992])
N: 300
Signal to noise ratio: tensor([18.9692, 77.6027, 89.6978, 47.4337])
Bound on condition number: tensor([ 107949.6897, 1806656.7618, 2413711.8209,  674988.2287])
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.0015229818934935993, policy loss: 8.029638106761919
Experience 30, Iter 1, disc loss: 0.0019329575834593857, policy loss: 8.092536167131627
Experience 30, Iter 2, disc loss: 0.0014856046647039258, policy loss: 7.9751050597875075
Experience 30, Iter 3, disc loss: 0.0016232030623601016, policy loss: 8.031221693749345
Experience 30, Iter 4, disc loss: 0.0017426985367984915, policy loss: 7.880157292646919
Experience 30, Iter 5, disc loss: 0.001549152644243782, policy loss: 7.967842708073068
Experience 30, Iter 6, disc loss: 0.0018896982372147212, policy loss: 8.477018736268008
Experience 30, Iter 7, disc loss: 0.0016053999769778005, policy loss: 7.849258500997104
Experience 30, Iter 8, disc loss: 0.0014075217623585891, policy loss: 8.315569825381244
Experience 30, Iter 9, disc loss: 0.0015174112375108569, policy loss: 8.203850198969063
Experience 30, Iter 10, disc loss: 0.0017189271828157956, policy loss: 7.820361504157532
Experience 30, Iter 11, disc loss: 0.0013629836183488618, policy loss: 8.761196634038008
Experience 30, Iter 12, disc loss: 0.0014338911268999464, policy loss: 8.451065536135005
Experience 30, Iter 13, disc loss: 0.001508351890855104, policy loss: 8.332773750615686
Experience 30, Iter 14, disc loss: 0.0014280525169366924, policy loss: 8.546050195630146
Experience 30, Iter 15, disc loss: 0.0014692326275724766, policy loss: 8.10057740524543
Experience 30, Iter 16, disc loss: 0.0015209435664009464, policy loss: 8.31290208285524
Experience 30, Iter 17, disc loss: 0.0015406181120792791, policy loss: 8.82492290080097
Experience 30, Iter 18, disc loss: 0.001694328854799043, policy loss: 8.231169644655346
Experience 30, Iter 19, disc loss: 0.0012656291435730114, policy loss: 8.403053604923123
Experience 30, Iter 20, disc loss: 0.001515762399947192, policy loss: 8.519892331766606
Experience 30, Iter 21, disc loss: 0.0014891235536138776, policy loss: 8.494663911811676
Experience 30, Iter 22, disc loss: 0.0015151825294135368, policy loss: 8.460202518271528
Experience 30, Iter 23, disc loss: 0.0013780864714084055, policy loss: 8.543738803461379
Experience 30, Iter 24, disc loss: 0.0013769066303888347, policy loss: 9.034927634858425
Experience 30, Iter 25, disc loss: 0.0015348427453050354, policy loss: 7.543320733033731
Experience 30, Iter 26, disc loss: 0.0016158183966163554, policy loss: 8.524346149223657
Experience 30, Iter 27, disc loss: 0.0015241168345729314, policy loss: 8.234292382575795
Experience 30, Iter 28, disc loss: 0.0014945177301039336, policy loss: 7.863306275179561
Experience 30, Iter 29, disc loss: 0.0013791892077240474, policy loss: 8.207083032365752
Experience 30, Iter 30, disc loss: 0.001640334495149933, policy loss: 8.30465354353218
Experience 30, Iter 31, disc loss: 0.001467032982055801, policy loss: 8.60405093676418
Experience 30, Iter 32, disc loss: 0.0017173624195510205, policy loss: 7.813498894342846
Experience 30, Iter 33, disc loss: 0.0018566206441730398, policy loss: 8.15619694997407
Experience 30, Iter 34, disc loss: 0.0016509056121580068, policy loss: 7.887821702962431
Experience 30, Iter 35, disc loss: 0.0016190369538207421, policy loss: 8.442690159464824
Experience 30, Iter 36, disc loss: 0.0018017761479403457, policy loss: 8.428330513306209
Experience 30, Iter 37, disc loss: 0.0016426937873282137, policy loss: 8.40752011716352
Experience 30, Iter 38, disc loss: 0.0014370681028662876, policy loss: 8.234579721135342
Experience 30, Iter 39, disc loss: 0.0013567384275805089, policy loss: 8.198675700591519
Experience 30, Iter 40, disc loss: 0.0013964949060614321, policy loss: 8.141339105712019
Experience 30, Iter 41, disc loss: 0.001384799836236864, policy loss: 8.522285032433619
Experience 30, Iter 42, disc loss: 0.0014786685683550746, policy loss: 8.564992945090273
Experience 30, Iter 43, disc loss: 0.001490110587655248, policy loss: 8.523781567188855
Experience 30, Iter 44, disc loss: 0.0014588353841939964, policy loss: 8.33116059971989
Experience 30, Iter 45, disc loss: 0.0012854999429747728, policy loss: 8.340866177864429
Experience 30, Iter 46, disc loss: 0.0014145896647854536, policy loss: 8.382895358230588
Experience 30, Iter 47, disc loss: 0.001478460038175541, policy loss: 8.197342427531876
Experience 30, Iter 48, disc loss: 0.0015924621652924395, policy loss: 9.063225772200866
Experience 30, Iter 49, disc loss: 0.0013482468439560981, policy loss: 8.524717819179727
Experience 30, Iter 50, disc loss: 0.0012239835402083718, policy loss: 8.974666786008527
Experience 30, Iter 51, disc loss: 0.0014708987563200808, policy loss: 8.483511912369066
Experience 30, Iter 52, disc loss: 0.0015398585398500226, policy loss: 8.43061433190638
Experience 30, Iter 53, disc loss: 0.0017125145097797196, policy loss: 8.307302179889984
Experience 30, Iter 54, disc loss: 0.0014733878904563244, policy loss: 7.9200823350269305
Experience 30, Iter 55, disc loss: 0.0014383198297024838, policy loss: 8.271496467154309
Experience 30, Iter 56, disc loss: 0.0015337506947545809, policy loss: 7.985580987360777
Experience 30, Iter 57, disc loss: 0.0014796124905250775, policy loss: 8.270332848672773
Experience 30, Iter 58, disc loss: 0.0014171928516206014, policy loss: 9.363948347675748
Experience 30, Iter 59, disc loss: 0.0014443773710321492, policy loss: 8.458236054632474
Experience 30, Iter 60, disc loss: 0.001005622446213288, policy loss: 9.301381303231029
Experience 30, Iter 61, disc loss: 0.0009666724247787742, policy loss: 9.712653820418755
Experience 30, Iter 62, disc loss: 0.0011131988419605114, policy loss: 8.897613617360028
Experience 30, Iter 63, disc loss: 0.0011230077733866705, policy loss: 10.123674704893478
Experience 30, Iter 64, disc loss: 0.0013332193819873093, policy loss: 9.226013627908292
Experience 30, Iter 65, disc loss: 0.0013680082661161812, policy loss: 9.032499582770164
Experience 30, Iter 66, disc loss: 0.0012505791234930287, policy loss: 9.576473091662068
Experience 30, Iter 67, disc loss: 0.0009449766290076391, policy loss: 9.320817422447995
Experience 30, Iter 68, disc loss: 0.0012901913089680872, policy loss: 8.349828361763599
Experience 30, Iter 69, disc loss: 0.0013284102921012697, policy loss: 8.36162527919812
Experience 30, Iter 70, disc loss: 0.0011817276380490644, policy loss: 9.476154572527722
Experience 30, Iter 71, disc loss: 0.0012299669306000075, policy loss: 9.057461618631791
Experience 30, Iter 72, disc loss: 0.0009743831671998348, policy loss: 8.910196726628998
Experience 30, Iter 73, disc loss: 0.0009410306431585487, policy loss: 8.853626986891996
Experience 30, Iter 74, disc loss: 0.0009534313669902977, policy loss: 8.830303470659624
Experience 30, Iter 75, disc loss: 0.0010516406559134715, policy loss: 8.977850259789914
Experience 30, Iter 76, disc loss: 0.0011976034480454352, policy loss: 9.126588845800281
Experience 30, Iter 77, disc loss: 0.0011411911521315736, policy loss: 8.317898558355282
Experience 30, Iter 78, disc loss: 0.0011236751287490767, policy loss: 8.716308523599892
Experience 30, Iter 79, disc loss: 0.0010450598079247652, policy loss: 8.76618228956914
Experience 30, Iter 80, disc loss: 0.0013728169500788552, policy loss: 8.188835901212997
Experience 30, Iter 81, disc loss: 0.0015380328849366126, policy loss: 8.124024600308193
Experience 30, Iter 82, disc loss: 0.0012664426611014418, policy loss: 8.527550408722494
Experience 30, Iter 83, disc loss: 0.0011545823286843026, policy loss: 8.69167512473513
Experience 30, Iter 84, disc loss: 0.001259266677919315, policy loss: 7.963028139739974
Experience 30, Iter 85, disc loss: 0.0011493463212617037, policy loss: 8.833782919074576
Experience 30, Iter 86, disc loss: 0.0010383053819869273, policy loss: 8.806292959067486
Experience 30, Iter 87, disc loss: 0.0013928925962472206, policy loss: 7.878492310147562
Experience 30, Iter 88, disc loss: 0.0011465751905208712, policy loss: 10.019159599917103
Experience 30, Iter 89, disc loss: 0.001320333748928817, policy loss: 8.144519458796486
Experience 30, Iter 90, disc loss: 0.0009081628077274855, policy loss: 8.726481319299893
Experience 30, Iter 91, disc loss: 0.0007929587829501285, policy loss: 9.388110279150371
Experience 30, Iter 92, disc loss: 0.0009038505689803518, policy loss: 9.422258271547538
Experience 30, Iter 93, disc loss: 0.001211130988517619, policy loss: 8.014493141590137
Experience 30, Iter 94, disc loss: 0.0009938775842151877, policy loss: 10.765368396134189
Experience 30, Iter 95, disc loss: 0.0014200249459923489, policy loss: 8.136748877047848
Experience 30, Iter 96, disc loss: 0.0010473099203844388, policy loss: 8.855679361709804
Experience 30, Iter 97, disc loss: 0.001009558126101238, policy loss: 9.21551506139955
Experience 30, Iter 98, disc loss: 0.0008699910128083112, policy loss: 9.340485536499715
Experience 30, Iter 99, disc loss: 0.001418269016389597, policy loss: 8.076747874403603
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.1670],
        [1.4754],
        [0.0260]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0113, 0.1353, 1.1832, 0.0158, 0.0159, 3.6147]],

        [[0.0113, 0.1353, 1.1832, 0.0158, 0.0159, 3.6147]],

        [[0.0113, 0.1353, 1.1832, 0.0158, 0.0159, 3.6147]],

        [[0.0113, 0.1353, 1.1832, 0.0158, 0.0159, 3.6147]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0123, 0.6678, 5.9014, 0.1042], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0123, 0.6678, 5.9014, 0.1042])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.040
Iter 2/2000 - Loss: 3.210
Iter 3/2000 - Loss: 2.920
Iter 4/2000 - Loss: 2.921
Iter 5/2000 - Loss: 2.943
Iter 6/2000 - Loss: 2.831
Iter 7/2000 - Loss: 2.676
Iter 8/2000 - Loss: 2.546
Iter 9/2000 - Loss: 2.441
Iter 10/2000 - Loss: 2.327
Iter 11/2000 - Loss: 2.174
Iter 12/2000 - Loss: 1.976
Iter 13/2000 - Loss: 1.749
Iter 14/2000 - Loss: 1.509
Iter 15/2000 - Loss: 1.262
Iter 16/2000 - Loss: 1.007
Iter 17/2000 - Loss: 0.737
Iter 18/2000 - Loss: 0.449
Iter 19/2000 - Loss: 0.144
Iter 20/2000 - Loss: -0.174
Iter 1981/2000 - Loss: -8.371
Iter 1982/2000 - Loss: -8.371
Iter 1983/2000 - Loss: -8.371
Iter 1984/2000 - Loss: -8.371
Iter 1985/2000 - Loss: -8.371
Iter 1986/2000 - Loss: -8.371
Iter 1987/2000 - Loss: -8.371
Iter 1988/2000 - Loss: -8.371
Iter 1989/2000 - Loss: -8.371
Iter 1990/2000 - Loss: -8.372
Iter 1991/2000 - Loss: -8.372
Iter 1992/2000 - Loss: -8.372
Iter 1993/2000 - Loss: -8.372
Iter 1994/2000 - Loss: -8.372
Iter 1995/2000 - Loss: -8.372
Iter 1996/2000 - Loss: -8.372
Iter 1997/2000 - Loss: -8.372
Iter 1998/2000 - Loss: -8.372
Iter 1999/2000 - Loss: -8.372
Iter 2000/2000 - Loss: -8.372
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.1882,  8.4389, 35.7080,  1.8697, 22.2860, 48.8499]],

        [[16.4924, 30.0453,  7.7409,  1.2657,  1.2865, 26.5153]],

        [[14.8456, 29.6384,  8.0129,  1.0570,  0.6510, 23.6711]],

        [[14.1406, 22.8702, 14.4390,  2.0350,  1.6451, 36.6408]]])
Signal Variance: tensor([ 0.1151,  2.0405, 14.3859,  0.4675])
Estimated target variance: tensor([0.0123, 0.6678, 5.9014, 0.1042])
N: 310
Signal to noise ratio: tensor([20.1961, 79.5317, 92.0703, 45.9877])
Bound on condition number: tensor([ 126444.1202, 1960841.1118, 2627855.2039,  655611.5341])
Policy Optimizer learning rate:
0.009688861611972634
Experience 31, Iter 0, disc loss: 0.0014966414584620955, policy loss: 9.151312374641705
Experience 31, Iter 1, disc loss: 0.0015333759170670066, policy loss: 8.41237012229859
Experience 31, Iter 2, disc loss: 0.001591391100283868, policy loss: 7.9040007864301405
Experience 31, Iter 3, disc loss: 0.001152901283446972, policy loss: 8.692660052853626
Experience 31, Iter 4, disc loss: 0.0011082601372362823, policy loss: 8.315533748830243
Experience 31, Iter 5, disc loss: 0.0014044693141989507, policy loss: 8.797466170248777
Experience 31, Iter 6, disc loss: 0.001520549142625432, policy loss: 8.454964181634079
Experience 31, Iter 7, disc loss: 0.0013026066458175275, policy loss: 8.64493698016523
Experience 31, Iter 8, disc loss: 0.0011676818325100605, policy loss: 8.783010214015711
Experience 31, Iter 9, disc loss: 0.0013464376864536828, policy loss: 8.501106146580968
Experience 31, Iter 10, disc loss: 0.0014160931264822752, policy loss: 7.824010151936003
Experience 31, Iter 11, disc loss: 0.0012316399341576597, policy loss: 8.910765699830886
Experience 31, Iter 12, disc loss: 0.0012523551185157575, policy loss: 8.714352258941108
Experience 31, Iter 13, disc loss: 0.001169955001561679, policy loss: 8.54773986426994
Experience 31, Iter 14, disc loss: 0.001147062104683055, policy loss: 8.950086291213031
Experience 31, Iter 15, disc loss: 0.0011892481150321732, policy loss: 8.46594824823065
Experience 31, Iter 16, disc loss: 0.0013502680311278591, policy loss: 8.336278865303983
Experience 31, Iter 17, disc loss: 0.0012951755472439708, policy loss: 9.089426860247162
Experience 31, Iter 18, disc loss: 0.0014027176162122019, policy loss: 8.799110235302052
Experience 31, Iter 19, disc loss: 0.0012162887106640032, policy loss: 8.81792108659879
Experience 31, Iter 20, disc loss: 0.001255947402768988, policy loss: 8.320237633545972
Experience 31, Iter 21, disc loss: 0.0011745822474832576, policy loss: 8.419173924339672
Experience 31, Iter 22, disc loss: 0.0013523612362321938, policy loss: 8.41704699019902
Experience 31, Iter 23, disc loss: 0.001364461595383659, policy loss: 8.81032882348925
Experience 31, Iter 24, disc loss: 0.0012034491267435483, policy loss: 8.528386965699472
Experience 31, Iter 25, disc loss: 0.0012170880183612178, policy loss: 8.181508324004263
Experience 31, Iter 26, disc loss: 0.0009393610431023649, policy loss: 8.942042958031426
Experience 31, Iter 27, disc loss: 0.0009444657531483753, policy loss: 9.858362724898273
Experience 31, Iter 28, disc loss: 0.0011583110011921215, policy loss: 9.077273789584684
Experience 31, Iter 29, disc loss: 0.0011890758360315321, policy loss: 9.391561766171025
Experience 31, Iter 30, disc loss: 0.0012628797438154443, policy loss: 8.714589667611916
Experience 31, Iter 31, disc loss: 0.0009245183499217617, policy loss: 9.32919755564366
Experience 31, Iter 32, disc loss: 0.0009929041623933817, policy loss: 9.559953664125716
Experience 31, Iter 33, disc loss: 0.0010748557079707601, policy loss: 8.758228590825127
Experience 31, Iter 34, disc loss: 0.0013509998764779968, policy loss: 8.4072481435887
Experience 31, Iter 35, disc loss: 0.0011280128877901496, policy loss: 8.508231530575301
Experience 31, Iter 36, disc loss: 0.0013891914627609872, policy loss: 8.37621448217674
Experience 31, Iter 37, disc loss: 0.0012194581914195715, policy loss: 8.432710114388298
Experience 31, Iter 38, disc loss: 0.0011094734716282414, policy loss: 8.956055455420671
Experience 31, Iter 39, disc loss: 0.001026343897788328, policy loss: 8.734424216965463
Experience 31, Iter 40, disc loss: 0.0012041357275035633, policy loss: 8.257314943019392
Experience 31, Iter 41, disc loss: 0.0011659037517056671, policy loss: 8.743243103115788
Experience 31, Iter 42, disc loss: 0.0012942682939954138, policy loss: 8.450035464399363
Experience 31, Iter 43, disc loss: 0.0011741098360695278, policy loss: 9.100808850193026
Experience 31, Iter 44, disc loss: 0.0012320646790074255, policy loss: 8.534828527774032
Experience 31, Iter 45, disc loss: 0.0012195868728370435, policy loss: 8.175771087069808
Experience 31, Iter 46, disc loss: 0.0011331162817073573, policy loss: 9.167899248225257
Experience 31, Iter 47, disc loss: 0.0014470413272187608, policy loss: 8.241060126786998
Experience 31, Iter 48, disc loss: 0.001248576346133205, policy loss: 9.311520979644689
Experience 31, Iter 49, disc loss: 0.0014070890077365885, policy loss: 9.073313913247409
Experience 31, Iter 50, disc loss: 0.0011667789019478686, policy loss: 9.206174859585953
Experience 31, Iter 51, disc loss: 0.0008754087605259888, policy loss: 9.14564802718164
Experience 31, Iter 52, disc loss: 0.0011684899091338877, policy loss: 9.217861042612121
Experience 31, Iter 53, disc loss: 0.0012203443254919285, policy loss: 8.367570231756499
Experience 31, Iter 54, disc loss: 0.0012317732961877217, policy loss: 8.343851574494915
Experience 31, Iter 55, disc loss: 0.0011158890651563414, policy loss: 8.566255962492761
Experience 31, Iter 56, disc loss: 0.0011498938066197002, policy loss: 9.120643896115139
Experience 31, Iter 57, disc loss: 0.0012190300037278382, policy loss: 7.998057179101361
Experience 31, Iter 58, disc loss: 0.0011237093848485823, policy loss: 8.390416881931955
Experience 31, Iter 59, disc loss: 0.0010927476748875617, policy loss: 8.817075032258735
Experience 31, Iter 60, disc loss: 0.0013347478803558835, policy loss: 8.680864199408965
Experience 31, Iter 61, disc loss: 0.001248370571477613, policy loss: 8.50082739340673
Experience 31, Iter 62, disc loss: 0.0011515734548898865, policy loss: 8.88165519288812
Experience 31, Iter 63, disc loss: 0.0010130583003519979, policy loss: 8.71442177103928
Experience 31, Iter 64, disc loss: 0.0009818392273255455, policy loss: 9.621549600246553
Experience 31, Iter 65, disc loss: 0.001290581387480701, policy loss: 9.31097887412933
Experience 31, Iter 66, disc loss: 0.001291851900171307, policy loss: 8.99146575812507
Experience 31, Iter 67, disc loss: 0.0013276345024103992, policy loss: 8.605049814223827
Experience 31, Iter 68, disc loss: 0.0011380711719162682, policy loss: 8.6469967729356
Experience 31, Iter 69, disc loss: 0.0011238912045085478, policy loss: 8.78086794054368
Experience 31, Iter 70, disc loss: 0.0011187218295834588, policy loss: 8.648833321496449
Experience 31, Iter 71, disc loss: 0.0012487283519428646, policy loss: 7.974906050142135
Experience 31, Iter 72, disc loss: 0.0012202920397389335, policy loss: 9.152823642621254
Experience 31, Iter 73, disc loss: 0.0011601985417249556, policy loss: 9.26783710370384
Experience 31, Iter 74, disc loss: 0.0008728580854972411, policy loss: 9.026485655955938
Experience 31, Iter 75, disc loss: 0.0007389711210898913, policy loss: 9.794829689165924
Experience 31, Iter 76, disc loss: 0.00075276625032234, policy loss: 9.57137778671158
Experience 31, Iter 77, disc loss: 0.0007660288454013624, policy loss: 9.165752175641277
Experience 31, Iter 78, disc loss: 0.0012241497830689542, policy loss: 8.496074918286187
Experience 31, Iter 79, disc loss: 0.001130765806054375, policy loss: 9.158719036394913
Experience 31, Iter 80, disc loss: 0.0011395301925615732, policy loss: 8.463258322572264
Experience 31, Iter 81, disc loss: 0.0011044268938303864, policy loss: 8.720505091879316
Experience 31, Iter 82, disc loss: 0.0010249106838822776, policy loss: 8.51509514065776
Experience 31, Iter 83, disc loss: 0.000935642303282785, policy loss: 8.527096120849095
Experience 31, Iter 84, disc loss: 0.001184376565213928, policy loss: 8.029408825985621
Experience 31, Iter 85, disc loss: 0.0010782172779426197, policy loss: 8.844338907592977
Experience 31, Iter 86, disc loss: 0.0012730327449180555, policy loss: 8.601516984008393
Experience 31, Iter 87, disc loss: 0.0009825216642367154, policy loss: 8.61007115198008
Experience 31, Iter 88, disc loss: 0.000805080220038446, policy loss: 9.74745880354699
Experience 31, Iter 89, disc loss: 0.0009637002051458642, policy loss: 9.184394226086212
Experience 31, Iter 90, disc loss: 0.0010538299578818488, policy loss: 9.107038458900636
Experience 31, Iter 91, disc loss: 0.0011061646901412297, policy loss: 9.03165267096377
Experience 31, Iter 92, disc loss: 0.0010972086195050793, policy loss: 9.10198891889228
Experience 31, Iter 93, disc loss: 0.001193559566327665, policy loss: 8.33700229074576
Experience 31, Iter 94, disc loss: 0.0010361108378022575, policy loss: 9.022630064946643
Experience 31, Iter 95, disc loss: 0.001255056791443887, policy loss: 8.660337725472083
Experience 31, Iter 96, disc loss: 0.0011679783783520488, policy loss: 8.522389900204974
Experience 31, Iter 97, disc loss: 0.001045365763876148, policy loss: 9.217759799079445
Experience 31, Iter 98, disc loss: 0.0011668154573737039, policy loss: 9.057409921260227
Experience 31, Iter 99, disc loss: 0.0012887628582498602, policy loss: 8.269562187828202
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.1750],
        [1.5252],
        [0.0283]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0111, 0.1391, 1.2747, 0.0166, 0.0172, 3.7875]],

        [[0.0111, 0.1391, 1.2747, 0.0166, 0.0172, 3.7875]],

        [[0.0111, 0.1391, 1.2747, 0.0166, 0.0172, 3.7875]],

        [[0.0111, 0.1391, 1.2747, 0.0166, 0.0172, 3.7875]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0126, 0.7002, 6.1009, 0.1134], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0126, 0.7002, 6.1009, 0.1134])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.133
Iter 2/2000 - Loss: 3.290
Iter 3/2000 - Loss: 3.000
Iter 4/2000 - Loss: 3.000
Iter 5/2000 - Loss: 3.019
Iter 6/2000 - Loss: 2.902
Iter 7/2000 - Loss: 2.740
Iter 8/2000 - Loss: 2.605
Iter 9/2000 - Loss: 2.496
Iter 10/2000 - Loss: 2.378
Iter 11/2000 - Loss: 2.218
Iter 12/2000 - Loss: 2.012
Iter 13/2000 - Loss: 1.776
Iter 14/2000 - Loss: 1.526
Iter 15/2000 - Loss: 1.269
Iter 16/2000 - Loss: 1.005
Iter 17/2000 - Loss: 0.727
Iter 18/2000 - Loss: 0.432
Iter 19/2000 - Loss: 0.121
Iter 20/2000 - Loss: -0.202
Iter 1981/2000 - Loss: -8.366
Iter 1982/2000 - Loss: -8.366
Iter 1983/2000 - Loss: -8.366
Iter 1984/2000 - Loss: -8.366
Iter 1985/2000 - Loss: -8.366
Iter 1986/2000 - Loss: -8.366
Iter 1987/2000 - Loss: -8.366
Iter 1988/2000 - Loss: -8.366
Iter 1989/2000 - Loss: -8.366
Iter 1990/2000 - Loss: -8.366
Iter 1991/2000 - Loss: -8.366
Iter 1992/2000 - Loss: -8.366
Iter 1993/2000 - Loss: -8.366
Iter 1994/2000 - Loss: -8.366
Iter 1995/2000 - Loss: -8.366
Iter 1996/2000 - Loss: -8.366
Iter 1997/2000 - Loss: -8.366
Iter 1998/2000 - Loss: -8.366
Iter 1999/2000 - Loss: -8.366
Iter 2000/2000 - Loss: -8.366
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.2273,  9.0683, 36.8761,  1.8871, 21.5039, 50.5344]],

        [[16.1346, 29.2289,  8.2287,  1.8752,  0.6429, 18.5078]],

        [[15.5306, 29.5895,  7.6293,  1.1130,  0.6184, 24.2779]],

        [[14.4446, 23.0441, 15.5945,  1.7578,  1.8148, 38.2403]]])
Signal Variance: tensor([ 0.1230,  1.4856, 15.1221,  0.4973])
Estimated target variance: tensor([0.0126, 0.7002, 6.1009, 0.1134])
N: 320
Signal to noise ratio: tensor([21.0623, 68.6984, 94.5582, 47.6385])
Bound on condition number: tensor([ 141959.0700, 1510232.7156, 2861203.0608,  726217.1684])
Policy Optimizer learning rate:
0.009678658753253001
Experience 32, Iter 0, disc loss: 0.0008958376859961942, policy loss: 8.838154352292
Experience 32, Iter 1, disc loss: 0.0009922804288673686, policy loss: 9.036657727158676
Experience 32, Iter 2, disc loss: 0.001051878193613249, policy loss: 8.634196305775998
Experience 32, Iter 3, disc loss: 0.0008943645336529885, policy loss: 9.469898754600223
Experience 32, Iter 4, disc loss: 0.0011554543528225613, policy loss: 8.750689586380979
Experience 32, Iter 5, disc loss: 0.0010646891674889291, policy loss: 8.403355529742898
Experience 32, Iter 6, disc loss: 0.0013150062136215624, policy loss: 8.48849724617185
Experience 32, Iter 7, disc loss: 0.0010031284714307354, policy loss: 8.42096769375037
Experience 32, Iter 8, disc loss: 0.0010658458534017101, policy loss: 8.679216223494056
Experience 32, Iter 9, disc loss: 0.0010638478676321013, policy loss: 8.301216019705578
Experience 32, Iter 10, disc loss: 0.0012079015241879786, policy loss: 8.590078631365017
Experience 32, Iter 11, disc loss: 0.0009957111549137524, policy loss: 8.338095878620916
Experience 32, Iter 12, disc loss: 0.001244647677696702, policy loss: 8.37968973972404
Experience 32, Iter 13, disc loss: 0.001170750160754182, policy loss: 8.28904088930115
Experience 32, Iter 14, disc loss: 0.0012285357119005893, policy loss: 8.276883560381375
Experience 32, Iter 15, disc loss: 0.001249734808234823, policy loss: 8.664968038716868
Experience 32, Iter 16, disc loss: 0.001390640981612269, policy loss: 8.182058339693722
Experience 32, Iter 17, disc loss: 0.0012712266722381782, policy loss: 8.389629185538997
Experience 32, Iter 18, disc loss: 0.001117680560649945, policy loss: 8.425785751603367
Experience 32, Iter 19, disc loss: 0.0011017034169619187, policy loss: 8.41934531901243
Experience 32, Iter 20, disc loss: 0.0011698514375507712, policy loss: 9.034822172155163
Experience 32, Iter 21, disc loss: 0.0010892176219740517, policy loss: 9.524037391585486
Experience 32, Iter 22, disc loss: 0.0011652364392731416, policy loss: 8.512260002150752
Experience 32, Iter 23, disc loss: 0.0011239622378141452, policy loss: 8.503265170489254
Experience 32, Iter 24, disc loss: 0.001341309908334004, policy loss: 8.296851915606332
Experience 32, Iter 25, disc loss: 0.0010727380745712612, policy loss: 9.275169841290364
Experience 32, Iter 26, disc loss: 0.0010104660057552199, policy loss: 9.066811321733052
Experience 32, Iter 27, disc loss: 0.0008975987856610286, policy loss: 9.148659101280822
Experience 32, Iter 28, disc loss: 0.00084783725550583, policy loss: 9.102727059725986
Experience 32, Iter 29, disc loss: 0.0012005930331250316, policy loss: 8.553188616128761
Experience 32, Iter 30, disc loss: 0.0011106294015489804, policy loss: 9.004384232710759
Experience 32, Iter 31, disc loss: 0.001216796503769386, policy loss: 8.9964933566173
Experience 32, Iter 32, disc loss: 0.0011335189493344965, policy loss: 8.955010297214283
Experience 32, Iter 33, disc loss: 0.0012062801425632378, policy loss: 8.496935640671861
Experience 32, Iter 34, disc loss: 0.0010267883547965893, policy loss: 8.593478594053758
Experience 32, Iter 35, disc loss: 0.0010843535668880197, policy loss: 8.50230741758808
Experience 32, Iter 36, disc loss: 0.0010571854362973645, policy loss: 9.01482108700057
Experience 32, Iter 37, disc loss: 0.0011395663512751338, policy loss: 8.81716253442082
Experience 32, Iter 38, disc loss: 0.0011005812469426222, policy loss: 8.832980802619012
Experience 32, Iter 39, disc loss: 0.0011764405925812258, policy loss: 8.842979172884736
Experience 32, Iter 40, disc loss: 0.0011953927675625764, policy loss: 8.268436166998368
Experience 32, Iter 41, disc loss: 0.00115223848275309, policy loss: 8.170723130537775
Experience 32, Iter 42, disc loss: 0.0010320134578835627, policy loss: 9.139189029644704
Experience 32, Iter 43, disc loss: 0.0008456489453564963, policy loss: 8.840197772450225
Experience 32, Iter 44, disc loss: 0.0008809819577644443, policy loss: 8.868445947699637
Experience 32, Iter 45, disc loss: 0.0012978875700643707, policy loss: 8.32210101791732
Experience 32, Iter 46, disc loss: 0.0009426688781186235, policy loss: 8.944292425215785
Experience 32, Iter 47, disc loss: 0.0010858819876460295, policy loss: 8.661961217629862
Experience 32, Iter 48, disc loss: 0.00099644970318165, policy loss: 8.663686129969562
Experience 32, Iter 49, disc loss: 0.0011070374042159066, policy loss: 8.796188259191766
Experience 32, Iter 50, disc loss: 0.0010387494244922897, policy loss: 8.96329649599555
Experience 32, Iter 51, disc loss: 0.0012431722415099183, policy loss: 9.482394760244068
Experience 32, Iter 52, disc loss: 0.0012767914176520213, policy loss: 8.162143150981176
Experience 32, Iter 53, disc loss: 0.0009935231598771081, policy loss: 9.954473105954996
Experience 32, Iter 54, disc loss: 0.0007967808031193423, policy loss: 9.146499373922023
Experience 32, Iter 55, disc loss: 0.000891215496959911, policy loss: 9.20079135720303
Experience 32, Iter 56, disc loss: 0.001112835016571433, policy loss: 9.098035747729211
Experience 32, Iter 57, disc loss: 0.001155262990131728, policy loss: 9.621260113312461
Experience 32, Iter 58, disc loss: 0.0010955307236908203, policy loss: 9.08975140982865
Experience 32, Iter 59, disc loss: 0.0009588779145099075, policy loss: 8.9892875105182
Experience 32, Iter 60, disc loss: 0.0009462673132276422, policy loss: 8.47534083734988
Experience 32, Iter 61, disc loss: 0.0008350315504174015, policy loss: 9.35164712864296
Experience 32, Iter 62, disc loss: 0.0012312434771448474, policy loss: 8.232786024680557
Experience 32, Iter 63, disc loss: 0.001055996588882733, policy loss: 9.119734588030918
Experience 32, Iter 64, disc loss: 0.0009233429232690246, policy loss: 9.490972162988651
Experience 32, Iter 65, disc loss: 0.0009344109400289855, policy loss: 9.006736293588157
Experience 32, Iter 66, disc loss: 0.0007691996456937989, policy loss: 9.395202756378373
Experience 32, Iter 67, disc loss: 0.0008139558746917447, policy loss: 9.236817563489348
Experience 32, Iter 68, disc loss: 0.0008841594337339839, policy loss: 9.677478225471036
Experience 32, Iter 69, disc loss: 0.0009345138948541646, policy loss: 8.816752242631368
Experience 32, Iter 70, disc loss: 0.0010080292709727173, policy loss: 8.871706876597756
Experience 32, Iter 71, disc loss: 0.0011486074280554594, policy loss: 8.992604873292306
Experience 32, Iter 72, disc loss: 0.0010362893091780834, policy loss: 8.724859170853513
Experience 32, Iter 73, disc loss: 0.0008306404567068816, policy loss: 9.088915881271934
Experience 32, Iter 74, disc loss: 0.0009454234826992312, policy loss: 8.885040442342564
Experience 32, Iter 75, disc loss: 0.0012152370179718559, policy loss: 8.538175497684097
Experience 32, Iter 76, disc loss: 0.0011726606169978496, policy loss: 9.407317973508196
Experience 32, Iter 77, disc loss: 0.0009293742168834968, policy loss: 9.03009061601739
Experience 32, Iter 78, disc loss: 0.0011099943339734127, policy loss: 8.792061379060103
Experience 32, Iter 79, disc loss: 0.0011075515224923623, policy loss: 8.7565047633231
Experience 32, Iter 80, disc loss: 0.0011834097992525034, policy loss: 8.327794116273068
Experience 32, Iter 81, disc loss: 0.0009826542461846602, policy loss: 8.996847310403457
Experience 32, Iter 82, disc loss: 0.0009324640488601311, policy loss: 8.884717870422344
Experience 32, Iter 83, disc loss: 0.000977043293133022, policy loss: 9.474180614984459
Experience 32, Iter 84, disc loss: 0.0010358784994214864, policy loss: 8.65724680874112
Experience 32, Iter 85, disc loss: 0.0008500949675075715, policy loss: 8.975999005863809
Experience 32, Iter 86, disc loss: 0.000707806343366184, policy loss: 9.657023113705858
Experience 32, Iter 87, disc loss: 0.0008913529378659995, policy loss: 8.67066660838642
Experience 32, Iter 88, disc loss: 0.00105780360551236, policy loss: 8.795904025228158
Experience 32, Iter 89, disc loss: 0.0010695838338059278, policy loss: 8.747374225263616
Experience 32, Iter 90, disc loss: 0.0011776268505936257, policy loss: 8.40508631917011
Experience 32, Iter 91, disc loss: 0.0009806490115093335, policy loss: 8.55676708773403
Experience 32, Iter 92, disc loss: 0.0008679605044059817, policy loss: 9.371870970666315
Experience 32, Iter 93, disc loss: 0.0010853766704300954, policy loss: 8.704431909256773
Experience 32, Iter 94, disc loss: 0.0010986661858119965, policy loss: 8.611366120581513
Experience 32, Iter 95, disc loss: 0.0010208356732724075, policy loss: 8.631107214731378
Experience 32, Iter 96, disc loss: 0.0008397793456317206, policy loss: 9.658160152504115
Experience 32, Iter 97, disc loss: 0.0010152963004061036, policy loss: 9.135290252355226
Experience 32, Iter 98, disc loss: 0.0012760614473211914, policy loss: 9.383217356088373
Experience 32, Iter 99, disc loss: 0.0010044620809595806, policy loss: 9.134088388799057
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1820],
        [1.5653],
        [0.0300]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0110, 0.1416, 1.3408, 0.0172, 0.0184, 3.9508]],

        [[0.0110, 0.1416, 1.3408, 0.0172, 0.0184, 3.9508]],

        [[0.0110, 0.1416, 1.3408, 0.0172, 0.0184, 3.9508]],

        [[0.0110, 0.1416, 1.3408, 0.0172, 0.0184, 3.9508]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0128, 0.7278, 6.2612, 0.1202], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0128, 0.7278, 6.2612, 0.1202])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.202
Iter 2/2000 - Loss: 3.351
Iter 3/2000 - Loss: 3.058
Iter 4/2000 - Loss: 3.057
Iter 5/2000 - Loss: 3.075
Iter 6/2000 - Loss: 2.955
Iter 7/2000 - Loss: 2.787
Iter 8/2000 - Loss: 2.646
Iter 9/2000 - Loss: 2.532
Iter 10/2000 - Loss: 2.408
Iter 11/2000 - Loss: 2.244
Iter 12/2000 - Loss: 2.033
Iter 13/2000 - Loss: 1.790
Iter 14/2000 - Loss: 1.533
Iter 15/2000 - Loss: 1.270
Iter 16/2000 - Loss: 0.999
Iter 17/2000 - Loss: 0.716
Iter 18/2000 - Loss: 0.416
Iter 19/2000 - Loss: 0.100
Iter 20/2000 - Loss: -0.227
Iter 1981/2000 - Loss: -8.360
Iter 1982/2000 - Loss: -8.360
Iter 1983/2000 - Loss: -8.360
Iter 1984/2000 - Loss: -8.360
Iter 1985/2000 - Loss: -8.360
Iter 1986/2000 - Loss: -8.361
Iter 1987/2000 - Loss: -8.361
Iter 1988/2000 - Loss: -8.361
Iter 1989/2000 - Loss: -8.361
Iter 1990/2000 - Loss: -8.361
Iter 1991/2000 - Loss: -8.361
Iter 1992/2000 - Loss: -8.361
Iter 1993/2000 - Loss: -8.361
Iter 1994/2000 - Loss: -8.361
Iter 1995/2000 - Loss: -8.361
Iter 1996/2000 - Loss: -8.361
Iter 1997/2000 - Loss: -8.361
Iter 1998/2000 - Loss: -8.361
Iter 1999/2000 - Loss: -8.361
Iter 2000/2000 - Loss: -8.361
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.3723,  8.7490, 33.0517,  1.8201, 21.5163, 51.9747]],

        [[15.6498, 28.4872,  8.2818,  1.8869,  0.6186, 17.7127]],

        [[14.7112, 28.7092,  7.6733,  1.0970,  0.6229, 23.9134]],

        [[14.1491, 22.9617, 15.1753,  1.6812,  1.6903, 39.8053]]])
Signal Variance: tensor([ 0.1182,  1.4210, 15.1138,  0.4634])
Estimated target variance: tensor([0.0128, 0.7278, 6.2612, 0.1202])
N: 330
Signal to noise ratio: tensor([19.9478, 67.2854, 95.1721, 45.8506])
Bound on condition number: tensor([ 131312.8142, 1494018.1076, 2989050.8297,  693752.3163])
Policy Optimizer learning rate:
0.009668466638656902
Experience 33, Iter 0, disc loss: 0.000893435312888913, policy loss: 8.762633093037731
Experience 33, Iter 1, disc loss: 0.0011070166298557224, policy loss: 7.99424661381517
Experience 33, Iter 2, disc loss: 0.0010500436852232066, policy loss: 9.897040281338782
Experience 33, Iter 3, disc loss: 0.0010002331153529437, policy loss: 8.905674733469077
Experience 33, Iter 4, disc loss: 0.000599663020133961, policy loss: 10.530211318505174
Experience 33, Iter 5, disc loss: 0.000662896903949041, policy loss: 9.874548983961386
Experience 33, Iter 6, disc loss: 0.0007285007629068041, policy loss: 9.862791137451707
Experience 33, Iter 7, disc loss: 0.0010110864595490525, policy loss: 9.109638471460265
Experience 33, Iter 8, disc loss: 0.0010768929144667487, policy loss: 8.619988848614204
Experience 33, Iter 9, disc loss: 0.0009170861615679358, policy loss: 8.818053771725193
Experience 33, Iter 10, disc loss: 0.000837695763894818, policy loss: 8.861633379787957
Experience 33, Iter 11, disc loss: 0.0007471668455952848, policy loss: 8.662000746630584
Experience 33, Iter 12, disc loss: 0.0008037815491281735, policy loss: 9.020773035561257
Experience 33, Iter 13, disc loss: 0.0008773918150507116, policy loss: 8.804531610861735
Experience 33, Iter 14, disc loss: 0.0009087429134914991, policy loss: 9.399260911642461
Experience 33, Iter 15, disc loss: 0.0009244358351021906, policy loss: 9.073946710085423
Experience 33, Iter 16, disc loss: 0.0007646132198238637, policy loss: 10.033281795067523
Experience 33, Iter 17, disc loss: 0.0006887319244257019, policy loss: 10.078134621280096
Experience 33, Iter 18, disc loss: 0.0008866208522385149, policy loss: 9.04886206883405
Experience 33, Iter 19, disc loss: 0.0009108731169691728, policy loss: 8.877522333088374
Experience 33, Iter 20, disc loss: 0.0009027141492855685, policy loss: 9.026857648365802
Experience 33, Iter 21, disc loss: 0.000689616243825704, policy loss: 9.515080056280071
Experience 33, Iter 22, disc loss: 0.0007807827446575106, policy loss: 8.793286301322718
Experience 33, Iter 23, disc loss: 0.0008079634538094272, policy loss: 8.782437481703198
Experience 33, Iter 24, disc loss: 0.000921023116831368, policy loss: 9.214156448969218
Experience 33, Iter 25, disc loss: 0.0008689771766532576, policy loss: 8.732649406516394
Experience 33, Iter 26, disc loss: 0.0008688330488065172, policy loss: 9.077031676016102
Experience 33, Iter 27, disc loss: 0.0008936855677556863, policy loss: 9.353785072459452
Experience 33, Iter 28, disc loss: 0.0008047323412931394, policy loss: 9.312982762262811
Experience 33, Iter 29, disc loss: 0.0007613987385981173, policy loss: 8.853680545936976
Experience 33, Iter 30, disc loss: 0.0011345698205418688, policy loss: 8.304105454636137
Experience 33, Iter 31, disc loss: 0.0009199655171436004, policy loss: 8.955733026179928
Experience 33, Iter 32, disc loss: 0.0007409864944203744, policy loss: 9.532927694150185
Experience 33, Iter 33, disc loss: 0.001078361702016528, policy loss: 8.123595928051774
Experience 33, Iter 34, disc loss: 0.0010677657881576264, policy loss: 9.023058896621778
Experience 33, Iter 35, disc loss: 0.0009308995812957201, policy loss: 8.38860215922399
Experience 33, Iter 36, disc loss: 0.0008060736403788678, policy loss: 9.580877677672309
Experience 33, Iter 37, disc loss: 0.0007359978242165181, policy loss: 9.19109954686418
Experience 33, Iter 38, disc loss: 0.0009048461927808702, policy loss: 8.643167626844463
Experience 33, Iter 39, disc loss: 0.0009946278955279004, policy loss: 8.671938967216725
Experience 33, Iter 40, disc loss: 0.000994858457732029, policy loss: 8.889994549020804
Experience 33, Iter 41, disc loss: 0.0009558168675880991, policy loss: 9.718768008489203
Experience 33, Iter 42, disc loss: 0.0007327712815268716, policy loss: 9.077338131392894
Experience 33, Iter 43, disc loss: 0.0008344941425644263, policy loss: 9.037092085442437
Experience 33, Iter 44, disc loss: 0.0007825423781445753, policy loss: 9.373616176327916
Experience 33, Iter 45, disc loss: 0.0008922164981243934, policy loss: 9.144271522475574
Experience 33, Iter 46, disc loss: 0.000969114327829773, policy loss: 8.895867970594534
Experience 33, Iter 47, disc loss: 0.0009879965728822504, policy loss: 8.501764872293077
Experience 33, Iter 48, disc loss: 0.000804875412350208, policy loss: 9.73284609585997
Experience 33, Iter 49, disc loss: 0.0009307067282204656, policy loss: 8.541371578202394
Experience 33, Iter 50, disc loss: 0.0007199581380955718, policy loss: 9.32872679744925
Experience 33, Iter 51, disc loss: 0.0007335574205424007, policy loss: 8.72352478322594
Experience 33, Iter 52, disc loss: 0.0007868370081610624, policy loss: 9.531100396741312
Experience 33, Iter 53, disc loss: 0.001058792748338136, policy loss: 8.876061502387156
Experience 33, Iter 54, disc loss: 0.000988274453448081, policy loss: 8.586223385938599
Experience 33, Iter 55, disc loss: 0.0008001904301845656, policy loss: 9.439826380922353
Experience 33, Iter 56, disc loss: 0.0006839034253625172, policy loss: 9.23403288550512
Experience 33, Iter 57, disc loss: 0.0007630263433239003, policy loss: 9.001266702322987
Experience 33, Iter 58, disc loss: 0.0007894605151197243, policy loss: 9.07313473091967
Experience 33, Iter 59, disc loss: 0.0009841595931477133, policy loss: 9.271524418028964
Experience 33, Iter 60, disc loss: 0.0008355796291325146, policy loss: 9.094666924014199
Experience 33, Iter 61, disc loss: 0.0007303730507880366, policy loss: 8.709802907441684
Experience 33, Iter 62, disc loss: 0.0006894636607139124, policy loss: 8.85706969228613
Experience 33, Iter 63, disc loss: 0.0006627682380050474, policy loss: 9.209667215651319
Experience 33, Iter 64, disc loss: 0.0009106299758319118, policy loss: 8.63715126279915
Experience 33, Iter 65, disc loss: 0.0008176328205894716, policy loss: 8.794666324696344
Experience 33, Iter 66, disc loss: 0.0009199097989128645, policy loss: 8.896881259656945
Experience 33, Iter 67, disc loss: 0.0010355342350652943, policy loss: 9.53643383692633
Experience 33, Iter 68, disc loss: 0.0010024497421789748, policy loss: 9.227452503566964
Experience 33, Iter 69, disc loss: 0.0007339211598183224, policy loss: 9.196287670850577
Experience 33, Iter 70, disc loss: 0.0006858988765034359, policy loss: 9.14033266988762
Experience 33, Iter 71, disc loss: 0.0007135396834272768, policy loss: 9.838435403313223
Experience 33, Iter 72, disc loss: 0.0007642239782341542, policy loss: 9.457001641958792
Experience 33, Iter 73, disc loss: 0.0007879095740564032, policy loss: 9.792228373154638
Experience 33, Iter 74, disc loss: 0.0008355529913626248, policy loss: 8.872488893643899
Experience 33, Iter 75, disc loss: 0.0007912265672893274, policy loss: 8.999284257703696
Experience 33, Iter 76, disc loss: 0.0008839020129471579, policy loss: 8.826776056304343
Experience 33, Iter 77, disc loss: 0.000891980603510802, policy loss: 8.818496158967351
Experience 33, Iter 78, disc loss: 0.0007434426593941337, policy loss: 9.188929067221352
Experience 33, Iter 79, disc loss: 0.0005892682908694162, policy loss: 9.222187358637022
Experience 33, Iter 80, disc loss: 0.0006841810229705147, policy loss: 9.565284857584349
Experience 33, Iter 81, disc loss: 0.0008269186999100672, policy loss: 9.322233853350472
Experience 33, Iter 82, disc loss: 0.0009505159233568571, policy loss: 9.385306564346545
Experience 33, Iter 83, disc loss: 0.0008700654724616847, policy loss: 10.117039658677832
Experience 33, Iter 84, disc loss: 0.0009582804150946714, policy loss: 9.454991985389775
Experience 33, Iter 85, disc loss: 0.0009688451702492216, policy loss: 8.649096112924315
Experience 33, Iter 86, disc loss: 0.0008845430162154673, policy loss: 9.218126098378136
Experience 33, Iter 87, disc loss: 0.0009480208292562062, policy loss: 9.457668867879164
Experience 33, Iter 88, disc loss: 0.0009878069982101178, policy loss: 8.43206933127546
Experience 33, Iter 89, disc loss: 0.0008743406097483785, policy loss: 9.016367968458805
Experience 33, Iter 90, disc loss: 0.0008167886140870654, policy loss: 9.250452303569482
Experience 33, Iter 91, disc loss: 0.0008360401637666362, policy loss: 8.664284626563957
Experience 33, Iter 92, disc loss: 0.0008970804569943655, policy loss: 8.50571954168144
Experience 33, Iter 93, disc loss: 0.0008164044029372485, policy loss: 9.478416859156578
Experience 33, Iter 94, disc loss: 0.0007508257907398451, policy loss: 9.07856347050162
Experience 33, Iter 95, disc loss: 0.000580328514675332, policy loss: 10.168545418905637
Experience 33, Iter 96, disc loss: 0.0005791512839441929, policy loss: 9.62490717029867
Experience 33, Iter 97, disc loss: 0.0006740198367102384, policy loss: 9.350043582355426
Experience 33, Iter 98, disc loss: 0.0008719422566284634, policy loss: 8.9687662959672
Experience 33, Iter 99, disc loss: 0.0008237360413036126, policy loss: 9.208645295878153
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1879],
        [1.5971],
        [0.0316]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0108, 0.1437, 1.4002, 0.0178, 0.0196, 4.0986]],

        [[0.0108, 0.1437, 1.4002, 0.0178, 0.0196, 4.0986]],

        [[0.0108, 0.1437, 1.4002, 0.0178, 0.0196, 4.0986]],

        [[0.0108, 0.1437, 1.4002, 0.0178, 0.0196, 4.0986]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0130, 0.7516, 6.3886, 0.1263], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0130, 0.7516, 6.3886, 0.1263])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.264
Iter 2/2000 - Loss: 3.409
Iter 3/2000 - Loss: 3.110
Iter 4/2000 - Loss: 3.106
Iter 5/2000 - Loss: 3.125
Iter 6/2000 - Loss: 3.003
Iter 7/2000 - Loss: 2.826
Iter 8/2000 - Loss: 2.673
Iter 9/2000 - Loss: 2.549
Iter 10/2000 - Loss: 2.418
Iter 11/2000 - Loss: 2.247
Iter 12/2000 - Loss: 2.029
Iter 13/2000 - Loss: 1.778
Iter 14/2000 - Loss: 1.512
Iter 15/2000 - Loss: 1.239
Iter 16/2000 - Loss: 0.960
Iter 17/2000 - Loss: 0.672
Iter 18/2000 - Loss: 0.370
Iter 19/2000 - Loss: 0.054
Iter 20/2000 - Loss: -0.273
Iter 1981/2000 - Loss: -8.376
Iter 1982/2000 - Loss: -8.376
Iter 1983/2000 - Loss: -8.376
Iter 1984/2000 - Loss: -8.376
Iter 1985/2000 - Loss: -8.376
Iter 1986/2000 - Loss: -8.376
Iter 1987/2000 - Loss: -8.376
Iter 1988/2000 - Loss: -8.376
Iter 1989/2000 - Loss: -8.376
Iter 1990/2000 - Loss: -8.376
Iter 1991/2000 - Loss: -8.376
Iter 1992/2000 - Loss: -8.376
Iter 1993/2000 - Loss: -8.376
Iter 1994/2000 - Loss: -8.376
Iter 1995/2000 - Loss: -8.376
Iter 1996/2000 - Loss: -8.376
Iter 1997/2000 - Loss: -8.376
Iter 1998/2000 - Loss: -8.376
Iter 1999/2000 - Loss: -8.377
Iter 2000/2000 - Loss: -8.377
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[10.0966,  9.1824, 30.9056,  1.7683, 21.7706, 54.2414]],

        [[15.4087, 28.6553,  8.3532,  1.8807,  0.6303, 19.0870]],

        [[14.3498, 29.4649,  7.6133,  1.0888,  0.6329, 24.7643]],

        [[13.9614, 22.6920, 15.8108,  1.7324,  1.7047, 40.7193]]])
Signal Variance: tensor([ 0.1236,  1.5071, 14.9236,  0.4743])
Estimated target variance: tensor([0.0130, 0.7516, 6.3886, 0.1263])
N: 340
Signal to noise ratio: tensor([20.4282, 68.6851, 92.4399, 46.2331])
Bound on condition number: tensor([ 141886.4044, 1603999.5892, 2905347.2585,  726752.1839])
Policy Optimizer learning rate:
0.009658285256870233
Experience 34, Iter 0, disc loss: 0.00084850095359401, policy loss: 8.672284854936166
Experience 34, Iter 1, disc loss: 0.0006767562705838219, policy loss: 9.303213537435518
Experience 34, Iter 2, disc loss: 0.0005838988858930266, policy loss: 9.633053107386738
Experience 34, Iter 3, disc loss: 0.0007365933691577912, policy loss: 8.877860127857323
Experience 34, Iter 4, disc loss: 0.0008557655644828659, policy loss: 9.05399387741106
Experience 34, Iter 5, disc loss: 0.0007931341116580244, policy loss: 9.301814421847524
Experience 34, Iter 6, disc loss: 0.0008392074535697351, policy loss: 9.089249411210954
Experience 34, Iter 7, disc loss: 0.0007113568718139223, policy loss: 8.5076256653113
Experience 34, Iter 8, disc loss: 0.0006648336829842043, policy loss: 9.21172473227781
Experience 34, Iter 9, disc loss: 0.0007392925929499128, policy loss: 9.196452144390456
Experience 34, Iter 10, disc loss: 0.0007121775163980885, policy loss: 9.560305272249485
Experience 34, Iter 11, disc loss: 0.0009126968931765682, policy loss: 8.872889840293727
Experience 34, Iter 12, disc loss: 0.000748778366010361, policy loss: 8.723483845260418
Experience 34, Iter 13, disc loss: 0.0006910665503951292, policy loss: 9.039114364025792
Experience 34, Iter 14, disc loss: 0.0007105340944108557, policy loss: 9.225130316324321
Experience 34, Iter 15, disc loss: 0.0007904139866633784, policy loss: 8.95925192078725
Experience 34, Iter 16, disc loss: 0.0008875852210035269, policy loss: 9.009687231300695
Experience 34, Iter 17, disc loss: 0.0008723716946938216, policy loss: 9.434593553585678
Experience 34, Iter 18, disc loss: 0.0008339497158035659, policy loss: 9.302746749619912
Experience 34, Iter 19, disc loss: 0.0006910827141206366, policy loss: 9.671243530061531
Experience 34, Iter 20, disc loss: 0.0007971556288315846, policy loss: 9.036810664338985
Experience 34, Iter 21, disc loss: 0.0006450424490542779, policy loss: 9.519730160879178
Experience 34, Iter 22, disc loss: 0.0007139482624076527, policy loss: 9.273817441330591
Experience 34, Iter 23, disc loss: 0.0007204191693117028, policy loss: 9.644368038138078
Experience 34, Iter 24, disc loss: 0.0007668137144198879, policy loss: 8.91452406400724
Experience 34, Iter 25, disc loss: 0.0008784520625614428, policy loss: 9.085798436449382
Experience 34, Iter 26, disc loss: 0.0006975981651680117, policy loss: 9.12057552887364
Experience 34, Iter 27, disc loss: 0.0008070920535402075, policy loss: 8.560195302332131
Experience 34, Iter 28, disc loss: 0.0007288117194390034, policy loss: 9.12621300673599
Experience 34, Iter 29, disc loss: 0.0008077668409168257, policy loss: 8.63621536217191
Experience 34, Iter 30, disc loss: 0.0007589262809992609, policy loss: 9.136684351936324
Experience 34, Iter 31, disc loss: 0.0007929529644799378, policy loss: 8.658719638676532
Experience 34, Iter 32, disc loss: 0.0007382198506516802, policy loss: 8.747000871547513
Experience 34, Iter 33, disc loss: 0.0007039423770626634, policy loss: 9.25609160431097
Experience 34, Iter 34, disc loss: 0.0009242397859520778, policy loss: 8.781833357727372
Experience 34, Iter 35, disc loss: 0.0008204299682810976, policy loss: 8.495258015413253
Experience 34, Iter 36, disc loss: 0.0008456396403321219, policy loss: 8.6216733687057
Experience 34, Iter 37, disc loss: 0.0008474320429021284, policy loss: 9.27928848223004
Experience 34, Iter 38, disc loss: 0.000794458177931017, policy loss: 9.212934086865054
Experience 34, Iter 39, disc loss: 0.0007662104593185585, policy loss: 9.116011982872937
Experience 34, Iter 40, disc loss: 0.0006687650300026438, policy loss: 8.574651659269458
Experience 34, Iter 41, disc loss: 0.000581260907126191, policy loss: 9.506532574991818
Experience 34, Iter 42, disc loss: 0.0007953845116290665, policy loss: 8.843981724071984
Experience 34, Iter 43, disc loss: 0.0008280989140773582, policy loss: 9.119518603896822
Experience 34, Iter 44, disc loss: 0.0007725100504756177, policy loss: 9.741685276335389
Experience 34, Iter 45, disc loss: 0.0006285321453612483, policy loss: 9.336316826997395
Experience 34, Iter 46, disc loss: 0.000642629564117493, policy loss: 9.061343231323473
Experience 34, Iter 47, disc loss: 0.0006561893598786648, policy loss: 9.175749186317429
Experience 34, Iter 48, disc loss: 0.0007699285116808469, policy loss: 8.914613241479465
Experience 34, Iter 49, disc loss: 0.0006841783877373742, policy loss: 9.247701508320622
Experience 34, Iter 50, disc loss: 0.0006730596140658208, policy loss: 8.868421822647
Experience 34, Iter 51, disc loss: 0.0007023277405910455, policy loss: 8.935334704091378
Experience 34, Iter 52, disc loss: 0.0008562044844737001, policy loss: 8.97012654255642
Experience 34, Iter 53, disc loss: 0.0007443257234817081, policy loss: 8.966641159408026
Experience 34, Iter 54, disc loss: 0.0007326695016930083, policy loss: 9.421668414174178
Experience 34, Iter 55, disc loss: 0.0007733050046247144, policy loss: 8.997156711337517
Experience 34, Iter 56, disc loss: 0.0008949639477038017, policy loss: 9.518332817971233
Experience 34, Iter 57, disc loss: 0.00071182554247585, policy loss: 9.614271013602053
Experience 34, Iter 58, disc loss: 0.0006763893263282005, policy loss: 9.066818257277955
Experience 34, Iter 59, disc loss: 0.0005900982368277716, policy loss: 9.61843528598995
Experience 34, Iter 60, disc loss: 0.00075492462719521, policy loss: 8.746879468143739
Experience 34, Iter 61, disc loss: 0.0008679909671127179, policy loss: 8.76303143453939
Experience 34, Iter 62, disc loss: 0.0007319753747514089, policy loss: 9.539062592442301
Experience 34, Iter 63, disc loss: 0.0006564220599317807, policy loss: 10.050429137851674
Experience 34, Iter 64, disc loss: 0.000633897920961909, policy loss: 9.61219747788235
Experience 34, Iter 65, disc loss: 0.0006617064483976595, policy loss: 9.209641356546744
Experience 34, Iter 66, disc loss: 0.0007310565998231487, policy loss: 9.567827062917564
Experience 34, Iter 67, disc loss: 0.0007755569795043455, policy loss: 8.283422124164787
Experience 34, Iter 68, disc loss: 0.0007270422533530586, policy loss: 9.041747068766485
Experience 34, Iter 69, disc loss: 0.0005818438587463992, policy loss: 9.563016400462276
Experience 34, Iter 70, disc loss: 0.0006125125245123799, policy loss: 9.385304534118244
Experience 34, Iter 71, disc loss: 0.000859267982750157, policy loss: 8.252137930406441
Experience 34, Iter 72, disc loss: 0.0007699440716156435, policy loss: 8.67058184679867
Experience 34, Iter 73, disc loss: 0.0009121378418286605, policy loss: 9.048453131874357
Experience 34, Iter 74, disc loss: 0.0007881099269923494, policy loss: 8.593822972868434
Experience 34, Iter 75, disc loss: 0.000541239670763442, policy loss: 9.780962673019257
Experience 34, Iter 76, disc loss: 0.0007343416940433906, policy loss: 9.43067027249652
Experience 34, Iter 77, disc loss: 0.0011045640904514737, policy loss: 8.545309672926303
Experience 34, Iter 78, disc loss: 0.0008040026628072928, policy loss: 9.5421400606715
Experience 34, Iter 79, disc loss: 0.0008176762836203462, policy loss: 8.79648829032622
Experience 34, Iter 80, disc loss: 0.0006791332691151552, policy loss: 9.403651071196593
Experience 34, Iter 81, disc loss: 0.0005333094572987619, policy loss: 9.792262327681458
Experience 34, Iter 82, disc loss: 0.0006764295909602084, policy loss: 9.798857135315235
Experience 34, Iter 83, disc loss: 0.0006630367911979447, policy loss: 9.323080692766784
Experience 34, Iter 84, disc loss: 0.0007792290529753611, policy loss: 9.475480639309007
Experience 34, Iter 85, disc loss: 0.0005845144696691025, policy loss: 9.317570610555775
Experience 34, Iter 86, disc loss: 0.0004755773610979504, policy loss: 10.083694756050395
Experience 34, Iter 87, disc loss: 0.0006060871406932246, policy loss: 9.318309161664676
Experience 34, Iter 88, disc loss: 0.0008085202712197347, policy loss: 9.23035199959989
Experience 34, Iter 89, disc loss: 0.0008162214525107913, policy loss: 9.145348654300903
Experience 34, Iter 90, disc loss: 0.0008254753062272534, policy loss: 9.440311734208812
Experience 34, Iter 91, disc loss: 0.0007325550236168299, policy loss: 8.799389900253246
Experience 34, Iter 92, disc loss: 0.00047414088275558354, policy loss: 10.02105688180535
Experience 34, Iter 93, disc loss: 0.0005220324746375946, policy loss: 9.564599206667532
Experience 34, Iter 94, disc loss: 0.0008660071669914172, policy loss: 8.935090674659333
Experience 34, Iter 95, disc loss: 0.0007612285401859332, policy loss: 9.8558110423857
Experience 34, Iter 96, disc loss: 0.0007370736045082019, policy loss: 9.616802134971195
Experience 34, Iter 97, disc loss: 0.0006554430460842706, policy loss: 9.292800228399418
Experience 34, Iter 98, disc loss: 0.00048292617020447606, policy loss: 9.900658747725302
Experience 34, Iter 99, disc loss: 0.0005429353160100205, policy loss: 9.937376399841437
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1942],
        [1.6332],
        [0.0328]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0106, 0.1458, 1.4526, 0.0184, 0.0205, 4.2349]],

        [[0.0106, 0.1458, 1.4526, 0.0184, 0.0205, 4.2349]],

        [[0.0106, 0.1458, 1.4526, 0.0184, 0.0205, 4.2349]],

        [[0.0106, 0.1458, 1.4526, 0.0184, 0.0205, 4.2349]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0132, 0.7768, 6.5328, 0.1314], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0132, 0.7768, 6.5328, 0.1314])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.315
Iter 2/2000 - Loss: 3.453
Iter 3/2000 - Loss: 3.151
Iter 4/2000 - Loss: 3.144
Iter 5/2000 - Loss: 3.163
Iter 6/2000 - Loss: 3.038
Iter 7/2000 - Loss: 2.857
Iter 8/2000 - Loss: 2.697
Iter 9/2000 - Loss: 2.565
Iter 10/2000 - Loss: 2.428
Iter 11/2000 - Loss: 2.253
Iter 12/2000 - Loss: 2.030
Iter 13/2000 - Loss: 1.773
Iter 14/2000 - Loss: 1.499
Iter 15/2000 - Loss: 1.219
Iter 16/2000 - Loss: 0.933
Iter 17/2000 - Loss: 0.639
Iter 18/2000 - Loss: 0.333
Iter 19/2000 - Loss: 0.014
Iter 20/2000 - Loss: -0.315
Iter 1981/2000 - Loss: -8.401
Iter 1982/2000 - Loss: -8.402
Iter 1983/2000 - Loss: -8.402
Iter 1984/2000 - Loss: -8.402
Iter 1985/2000 - Loss: -8.402
Iter 1986/2000 - Loss: -8.402
Iter 1987/2000 - Loss: -8.402
Iter 1988/2000 - Loss: -8.402
Iter 1989/2000 - Loss: -8.402
Iter 1990/2000 - Loss: -8.402
Iter 1991/2000 - Loss: -8.402
Iter 1992/2000 - Loss: -8.402
Iter 1993/2000 - Loss: -8.402
Iter 1994/2000 - Loss: -8.402
Iter 1995/2000 - Loss: -8.402
Iter 1996/2000 - Loss: -8.402
Iter 1997/2000 - Loss: -8.402
Iter 1998/2000 - Loss: -8.402
Iter 1999/2000 - Loss: -8.402
Iter 2000/2000 - Loss: -8.402
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.5662,  8.3305, 30.1248,  1.7331, 20.7531, 53.3879]],

        [[15.3965, 28.8437,  8.5208,  1.9041,  0.6467, 20.7372]],

        [[14.3366, 29.2578,  7.5038,  1.0952,  0.6439, 24.5221]],

        [[13.6965, 22.3009, 15.8595,  1.7684,  1.6562, 40.5669]]])
Signal Variance: tensor([ 0.1165,  1.6703, 14.9083,  0.4674])
Estimated target variance: tensor([0.0132, 0.7768, 6.5328, 0.1314])
N: 350
Signal to noise ratio: tensor([19.8105, 71.3743, 93.2836, 45.7485])
Bound on condition number: tensor([ 137360.0255, 1783000.9123, 3045642.0001,  732525.6670])
Policy Optimizer learning rate:
0.009648114596590806
Experience 35, Iter 0, disc loss: 0.000695938207729675, policy loss: 9.2022152987301
Experience 35, Iter 1, disc loss: 0.0007897998572192588, policy loss: 9.2893113025338
Experience 35, Iter 2, disc loss: 0.0006999211226325775, policy loss: 9.442131174881977
Experience 35, Iter 3, disc loss: 0.0007199144978970432, policy loss: 8.958140486091834
Experience 35, Iter 4, disc loss: 0.0007902806853460953, policy loss: 8.278978420773813
Experience 35, Iter 5, disc loss: 0.0006928588948301366, policy loss: 8.865759581586111
Experience 35, Iter 6, disc loss: 0.0008040042766799471, policy loss: 8.565962586366496
Experience 35, Iter 7, disc loss: 0.0008309494814310245, policy loss: 9.190913286341463
Experience 35, Iter 8, disc loss: 0.0007350189950283988, policy loss: 9.523545784572033
Experience 35, Iter 9, disc loss: 0.0007803393059174731, policy loss: 8.930313441167812
Experience 35, Iter 10, disc loss: 0.0005093969775903562, policy loss: 9.407715745625815
Experience 35, Iter 11, disc loss: 0.00047205969634844206, policy loss: 10.085228770460052
Experience 35, Iter 12, disc loss: 0.0006153143472177907, policy loss: 9.772642803829617
Experience 35, Iter 13, disc loss: 0.0008798639929758567, policy loss: 9.128473759293158
Experience 35, Iter 14, disc loss: 0.0008277388563779759, policy loss: 9.595299117169318
Experience 35, Iter 15, disc loss: 0.0007077108352856367, policy loss: 9.231090725943098
Experience 35, Iter 16, disc loss: 0.0005897990583079425, policy loss: 9.568924861829437
Experience 35, Iter 17, disc loss: 0.0006344861392165224, policy loss: 9.31363097194314
Experience 35, Iter 18, disc loss: 0.000687501803738929, policy loss: 9.013037474786826
Experience 35, Iter 19, disc loss: 0.00076713407007375, policy loss: 9.496626282770517
Experience 35, Iter 20, disc loss: 0.0008214450193670896, policy loss: 9.432815373116387
Experience 35, Iter 21, disc loss: 0.0008112829001757502, policy loss: 10.383317944797044
Experience 35, Iter 22, disc loss: 0.000621523027470756, policy loss: 9.464613136400924
Experience 35, Iter 23, disc loss: 0.0006615092616917936, policy loss: 8.933767948061824
Experience 35, Iter 24, disc loss: 0.0005851063252116273, policy loss: 9.389646846603869
Experience 35, Iter 25, disc loss: 0.0008047043486767643, policy loss: 8.893459327548761
Experience 35, Iter 26, disc loss: 0.0008661033621708131, policy loss: 9.597735553674656
Experience 35, Iter 27, disc loss: 0.0006834696683031819, policy loss: 9.090499683016429
Experience 35, Iter 28, disc loss: 0.0007095929755628239, policy loss: 8.818934772131053
Experience 35, Iter 29, disc loss: 0.0006486500344413022, policy loss: 9.160628846260735
Experience 35, Iter 30, disc loss: 0.000732378348216716, policy loss: 9.128632765498782
Experience 35, Iter 31, disc loss: 0.0007456783504876531, policy loss: 9.04482109567633
Experience 35, Iter 32, disc loss: 0.0007543445400144763, policy loss: 8.825072695071679
Experience 35, Iter 33, disc loss: 0.000741415830023347, policy loss: 9.31323334686501
Experience 35, Iter 34, disc loss: 0.0006354794359001753, policy loss: 9.071018429127125
Experience 35, Iter 35, disc loss: 0.000734842055942596, policy loss: 9.060162021766867
Experience 35, Iter 36, disc loss: 0.0006714702289243169, policy loss: 9.238955749237647
Experience 35, Iter 37, disc loss: 0.0006066788888526862, policy loss: 9.822798703776058
Experience 35, Iter 38, disc loss: 0.0006279927781839161, policy loss: 9.366144486964558
Experience 35, Iter 39, disc loss: 0.0006247475865948685, policy loss: 9.420283294191593
Experience 35, Iter 40, disc loss: 0.0008405576259718373, policy loss: 9.159889208469082
Experience 35, Iter 41, disc loss: 0.0007512901040541498, policy loss: 9.230468825848416
Experience 35, Iter 42, disc loss: 0.0007053726974905408, policy loss: 8.694964399180648
Experience 35, Iter 43, disc loss: 0.0006552531477532397, policy loss: 9.319425413808462
Experience 35, Iter 44, disc loss: 0.0006569074882355733, policy loss: 8.842934744542113
Experience 35, Iter 45, disc loss: 0.0006192908251753183, policy loss: 9.51460510987445
Experience 35, Iter 46, disc loss: 0.0007876839807519622, policy loss: 8.527987480046054
Experience 35, Iter 47, disc loss: 0.0007435528453376525, policy loss: 9.395559200056308
Experience 35, Iter 48, disc loss: 0.0007781982450084316, policy loss: 10.351759672773605
Experience 35, Iter 49, disc loss: 0.0006351651043875423, policy loss: 9.736449656295349
Experience 35, Iter 50, disc loss: 0.0005260981761495689, policy loss: 9.856590937660119
Experience 35, Iter 51, disc loss: 0.0005145869097832215, policy loss: 10.135273966397145
Experience 35, Iter 52, disc loss: 0.0006909007459545056, policy loss: 8.92598773862731
Experience 35, Iter 53, disc loss: 0.0007559667459732306, policy loss: 8.805330304386525
Experience 35, Iter 54, disc loss: 0.0007071006250916332, policy loss: 10.093931024686556
Experience 35, Iter 55, disc loss: 0.0007551951333895164, policy loss: 8.718698160813219
Experience 35, Iter 56, disc loss: 0.0004973508259937772, policy loss: 10.595700436052613
Experience 35, Iter 57, disc loss: 0.00044390847033465194, policy loss: 10.445811106234133
Experience 35, Iter 58, disc loss: 0.0006577182260537233, policy loss: 10.127839791481502
Experience 35, Iter 59, disc loss: 0.0006707685865171046, policy loss: 9.727460318085324
Experience 35, Iter 60, disc loss: 0.0006541394532756848, policy loss: 10.456226760368228
Experience 35, Iter 61, disc loss: 0.0007200107399761753, policy loss: 9.525219447713116
Experience 35, Iter 62, disc loss: 0.0007070092512072672, policy loss: 9.164736852373235
Experience 35, Iter 63, disc loss: 0.000681585838996833, policy loss: 9.02015646619548
Experience 35, Iter 64, disc loss: 0.0007341072056716415, policy loss: 9.438364551002913
Experience 35, Iter 65, disc loss: 0.000627407059196206, policy loss: 9.243159544535832
Experience 35, Iter 66, disc loss: 0.0006464120682748565, policy loss: 8.853289350769778
Experience 35, Iter 67, disc loss: 0.000779202194150602, policy loss: 9.173309898265664
Experience 35, Iter 68, disc loss: 0.0007816697243141235, policy loss: 8.832300722274589
Experience 35, Iter 69, disc loss: 0.0005696671264097795, policy loss: 9.954313560277864
Experience 35, Iter 70, disc loss: 0.0005083191125020889, policy loss: 10.079899406111792
Experience 35, Iter 71, disc loss: 0.0005886007171776069, policy loss: 9.154722719617077
Experience 35, Iter 72, disc loss: 0.0007105805561088592, policy loss: 8.823818398301738
Experience 35, Iter 73, disc loss: 0.0008519433057831647, policy loss: 9.724325526001444
Experience 35, Iter 74, disc loss: 0.000643341987125436, policy loss: 11.21676354915456
Experience 35, Iter 75, disc loss: 0.0004886297060853863, policy loss: 9.545900077801475
Experience 35, Iter 76, disc loss: 0.0003721325903502246, policy loss: 11.488728185218998
Experience 35, Iter 77, disc loss: 0.0003475237678758862, policy loss: 12.275413083656126
Experience 35, Iter 78, disc loss: 0.000373476991001194, policy loss: 11.139467789109812
Experience 35, Iter 79, disc loss: 0.000504126144029505, policy loss: 9.931626547999816
Experience 35, Iter 80, disc loss: 0.0005385366898546236, policy loss: 13.008696874710353
Experience 35, Iter 81, disc loss: 0.0006273028435400117, policy loss: 9.811450216106046
Experience 35, Iter 82, disc loss: 0.00042686898706871047, policy loss: 10.565386209859383
Experience 35, Iter 83, disc loss: 0.00036646575723190837, policy loss: 11.159948400364357
Experience 35, Iter 84, disc loss: 0.0003158348895923231, policy loss: 12.213908061347672
Experience 35, Iter 85, disc loss: 0.0003202911314690778, policy loss: 12.001901882026877
Experience 35, Iter 86, disc loss: 0.00046417283848766624, policy loss: 9.867942159910939
Experience 35, Iter 87, disc loss: 0.0005670274163910229, policy loss: 9.93966953924531
Experience 35, Iter 88, disc loss: 0.0006439266240490674, policy loss: 11.589126984443997
Experience 35, Iter 89, disc loss: 0.0007650507310270572, policy loss: 9.306939286647765
Experience 35, Iter 90, disc loss: 0.0005456515088648062, policy loss: 9.593789721724828
Experience 35, Iter 91, disc loss: 0.00045205725930936333, policy loss: 10.127933274819934
Experience 35, Iter 92, disc loss: 0.0005296353173904977, policy loss: 9.73677268748763
Experience 35, Iter 93, disc loss: 0.0007509770616950498, policy loss: 8.531881138517344
Experience 35, Iter 94, disc loss: 0.0008158695743088548, policy loss: 9.300519501793328
Experience 35, Iter 95, disc loss: 0.0005783066666908222, policy loss: 9.885615281022867
Experience 35, Iter 96, disc loss: 0.0006600996279836063, policy loss: 9.226714340846193
Experience 35, Iter 97, disc loss: 0.0004449526959729508, policy loss: 9.923622542276144
Experience 35, Iter 98, disc loss: 0.00047246951131853666, policy loss: 10.44263341328956
Experience 35, Iter 99, disc loss: 0.0004591287557001952, policy loss: 9.596243314164859
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.2016],
        [1.6739],
        [0.0346]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0106, 0.1491, 1.5219, 0.0190, 0.0214, 4.3861]],

        [[0.0106, 0.1491, 1.5219, 0.0190, 0.0214, 4.3861]],

        [[0.0106, 0.1491, 1.5219, 0.0190, 0.0214, 4.3861]],

        [[0.0106, 0.1491, 1.5219, 0.0190, 0.0214, 4.3861]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0135, 0.8062, 6.6956, 0.1386], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0135, 0.8062, 6.6956, 0.1386])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.382
Iter 2/2000 - Loss: 3.516
Iter 3/2000 - Loss: 3.211
Iter 4/2000 - Loss: 3.199
Iter 5/2000 - Loss: 3.218
Iter 6/2000 - Loss: 3.094
Iter 7/2000 - Loss: 2.910
Iter 8/2000 - Loss: 2.744
Iter 9/2000 - Loss: 2.607
Iter 10/2000 - Loss: 2.469
Iter 11/2000 - Loss: 2.294
Iter 12/2000 - Loss: 2.074
Iter 13/2000 - Loss: 1.819
Iter 14/2000 - Loss: 1.545
Iter 15/2000 - Loss: 1.265
Iter 16/2000 - Loss: 0.979
Iter 17/2000 - Loss: 0.686
Iter 18/2000 - Loss: 0.382
Iter 19/2000 - Loss: 0.065
Iter 20/2000 - Loss: -0.262
Iter 1981/2000 - Loss: -8.412
Iter 1982/2000 - Loss: -8.412
Iter 1983/2000 - Loss: -8.412
Iter 1984/2000 - Loss: -8.412
Iter 1985/2000 - Loss: -8.412
Iter 1986/2000 - Loss: -8.412
Iter 1987/2000 - Loss: -8.412
Iter 1988/2000 - Loss: -8.412
Iter 1989/2000 - Loss: -8.412
Iter 1990/2000 - Loss: -8.412
Iter 1991/2000 - Loss: -8.412
Iter 1992/2000 - Loss: -8.412
Iter 1993/2000 - Loss: -8.412
Iter 1994/2000 - Loss: -8.412
Iter 1995/2000 - Loss: -8.412
Iter 1996/2000 - Loss: -8.412
Iter 1997/2000 - Loss: -8.412
Iter 1998/2000 - Loss: -8.412
Iter 1999/2000 - Loss: -8.412
Iter 2000/2000 - Loss: -8.412
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 8.9577,  8.0017, 30.0701,  1.7465, 20.4500, 53.5968]],

        [[16.1410, 28.9067,  8.2100,  1.9395,  0.6704, 21.1575]],

        [[14.6133, 29.5843,  7.4020,  1.0992,  0.6474, 24.0131]],

        [[13.6186, 22.7414, 16.2620,  1.6200,  1.7576, 42.4459]]])
Signal Variance: tensor([ 0.1115,  1.6773, 14.3845,  0.4951])
Estimated target variance: tensor([0.0135, 0.8062, 6.6956, 0.1386])
N: 360
Signal to noise ratio: tensor([19.4616, 71.0347, 91.8953, 46.7295])
Bound on condition number: tensor([ 136352.7404, 1816537.1859, 3040108.0474,  786114.8245])
Policy Optimizer learning rate:
0.009637954646528333
Experience 36, Iter 0, disc loss: 0.0006947877202732111, policy loss: 9.358391666865643
Experience 36, Iter 1, disc loss: 0.0007470880956991887, policy loss: 10.379046584108018
Experience 36, Iter 2, disc loss: 0.0006933692552317317, policy loss: 10.136062683785212
Experience 36, Iter 3, disc loss: 0.0005672085175658772, policy loss: 9.22126551616456
Experience 36, Iter 4, disc loss: 0.000544920346383122, policy loss: 8.832182803582782
Experience 36, Iter 5, disc loss: 0.0006258926841465023, policy loss: 9.163908262924515
Experience 36, Iter 6, disc loss: 0.0005568931882214757, policy loss: 9.722347905696097
Experience 36, Iter 7, disc loss: 0.0006522268276237192, policy loss: 9.953769793898905
Experience 36, Iter 8, disc loss: 0.0005941366283895573, policy loss: 8.911345418362345
Experience 36, Iter 9, disc loss: 0.0005974620638206529, policy loss: 9.386621673493106
Experience 36, Iter 10, disc loss: 0.0005247767232298632, policy loss: 9.983082176949896
Experience 36, Iter 11, disc loss: 0.0006215192554685843, policy loss: 9.589337360871134
Experience 36, Iter 12, disc loss: 0.0005453504672784111, policy loss: 9.57522473881003
Experience 36, Iter 13, disc loss: 0.0005023463641505646, policy loss: 9.512284677709705
Experience 36, Iter 14, disc loss: 0.0005173273739821976, policy loss: 9.787238958309846
Experience 36, Iter 15, disc loss: 0.0006808099074252552, policy loss: 8.633471909527751
Experience 36, Iter 16, disc loss: 0.0007219956888268532, policy loss: 9.42480584856621
Experience 36, Iter 17, disc loss: 0.0006339267077409656, policy loss: 9.345313174596487
Experience 36, Iter 18, disc loss: 0.0005102290688599256, policy loss: 9.46853426612893
Experience 36, Iter 19, disc loss: 0.0005501918079010746, policy loss: 9.518070129204691
Experience 36, Iter 20, disc loss: 0.0007386723604262012, policy loss: 9.019188051558434
Experience 36, Iter 21, disc loss: 0.0005801725487949878, policy loss: 9.373250907177155
Experience 36, Iter 22, disc loss: 0.0005699445858156474, policy loss: 9.465472515722006
Experience 36, Iter 23, disc loss: 0.0006298322076525841, policy loss: 8.699301312382222
Experience 36, Iter 24, disc loss: 0.000609177259370319, policy loss: 9.049069175783472
Experience 36, Iter 25, disc loss: 0.0006702853660506428, policy loss: 9.337550111636652
Experience 36, Iter 26, disc loss: 0.0005355241338479453, policy loss: 9.473501662612577
Experience 36, Iter 27, disc loss: 0.0006177883670811647, policy loss: 9.590327348672016
Experience 36, Iter 28, disc loss: 0.0006009059964110362, policy loss: 9.046768141916631
Experience 36, Iter 29, disc loss: 0.0005376607302688304, policy loss: 9.212991114587343
Experience 36, Iter 30, disc loss: 0.0006349733355872465, policy loss: 9.445849013470069
Experience 36, Iter 31, disc loss: 0.0006854679782978938, policy loss: 9.469598331713186
Experience 36, Iter 32, disc loss: 0.0006587956872656188, policy loss: 9.141297384758369
Experience 36, Iter 33, disc loss: 0.0005918572699188706, policy loss: 9.960127074159248
Experience 36, Iter 34, disc loss: 0.00042226076069405276, policy loss: 10.088674197397689
Experience 36, Iter 35, disc loss: 0.0006036784477889083, policy loss: 8.663456106036143
Experience 36, Iter 36, disc loss: 0.000929991602799984, policy loss: 9.168212844675505
Experience 36, Iter 37, disc loss: 0.000681192024060058, policy loss: 10.203210345994595
Experience 36, Iter 38, disc loss: 0.0005512834990855511, policy loss: 9.737130816214034
Experience 36, Iter 39, disc loss: 0.0005405751592833137, policy loss: 9.318654433350343
Experience 36, Iter 40, disc loss: 0.0005063141591728802, policy loss: 9.224735143782578
Experience 36, Iter 41, disc loss: 0.0006336847533368085, policy loss: 9.317532871352487
Experience 36, Iter 42, disc loss: 0.0007315639032923317, policy loss: 8.798481702808992
Experience 36, Iter 43, disc loss: 0.0006352197411943527, policy loss: 10.325576668593344
Experience 36, Iter 44, disc loss: 0.000597280728935503, policy loss: 8.898627509029335
Experience 36, Iter 45, disc loss: 0.0004445847900932294, policy loss: 10.095067728031644
Experience 36, Iter 46, disc loss: 0.0005672567804828909, policy loss: 10.015833086103397
Experience 36, Iter 47, disc loss: 0.0006658307078770732, policy loss: 9.489328283057507
Experience 36, Iter 48, disc loss: 0.000639760725362174, policy loss: 10.73223150774324
Experience 36, Iter 49, disc loss: 0.0007131731976242333, policy loss: 10.120736349417932
Experience 36, Iter 50, disc loss: 0.0005671052523754461, policy loss: 9.504773959990064
Experience 36, Iter 51, disc loss: 0.000551867329369565, policy loss: 9.20161037017014
Experience 36, Iter 52, disc loss: 0.0005432301569606638, policy loss: 9.848364252585222
Experience 36, Iter 53, disc loss: 0.0006821608523934154, policy loss: 9.328027855579766
Experience 36, Iter 54, disc loss: 0.000630333277067238, policy loss: 10.57716892910395
Experience 36, Iter 55, disc loss: 0.0006787872386158295, policy loss: 9.531016264004274
Experience 36, Iter 56, disc loss: 0.0005154056697722537, policy loss: 9.249617047631183
Experience 36, Iter 57, disc loss: 0.000500478511454049, policy loss: 9.50817509651832
Experience 36, Iter 58, disc loss: 0.0006232336073265824, policy loss: 9.303468496503992
Experience 36, Iter 59, disc loss: 0.0006361667934469139, policy loss: 10.009058808736288
Experience 36, Iter 60, disc loss: 0.000618926586687174, policy loss: 9.194139859173742
Experience 36, Iter 61, disc loss: 0.0005514345212109602, policy loss: 9.399347729260718
Experience 36, Iter 62, disc loss: 0.0005825186686155008, policy loss: 9.04799906169852
Experience 36, Iter 63, disc loss: 0.0006109678578708383, policy loss: 9.301524160160541
Experience 36, Iter 64, disc loss: 0.0006470003369842106, policy loss: 8.844663227196534
Experience 36, Iter 65, disc loss: 0.000559598308048773, policy loss: 9.707816112644291
Experience 36, Iter 66, disc loss: 0.0005590388343152164, policy loss: 9.635387107501693
Experience 36, Iter 67, disc loss: 0.0005048588828964623, policy loss: 10.027655084257177
Experience 36, Iter 68, disc loss: 0.0005696937715495129, policy loss: 9.732581886549466
Experience 36, Iter 69, disc loss: 0.0006821287591419862, policy loss: 9.552609264875286
Experience 36, Iter 70, disc loss: 0.0006888714930261039, policy loss: 8.725344758173863
Experience 36, Iter 71, disc loss: 0.0005460293236535902, policy loss: 9.064252690258341
Experience 36, Iter 72, disc loss: 0.0006462968812929662, policy loss: 9.494263986207343
Experience 36, Iter 73, disc loss: 0.0006038499048509143, policy loss: 9.345573394532089
Experience 36, Iter 74, disc loss: 0.0006191249564497834, policy loss: 10.173717698663786
Experience 36, Iter 75, disc loss: 0.0007812595319701048, policy loss: 8.744126843197108
Experience 36, Iter 76, disc loss: 0.0006580478996572213, policy loss: 9.197199402961289
Experience 36, Iter 77, disc loss: 0.0005874178324052522, policy loss: 9.244815041064701
Experience 36, Iter 78, disc loss: 0.0005276027592810335, policy loss: 9.573571284931571
Experience 36, Iter 79, disc loss: 0.0005162100383519364, policy loss: 9.69894372919525
Experience 36, Iter 80, disc loss: 0.0005809954945641829, policy loss: 9.329321701711896
Experience 36, Iter 81, disc loss: 0.0005827612839749456, policy loss: 9.87872096395672
Experience 36, Iter 82, disc loss: 0.0006134882042881536, policy loss: 10.56169760078965
Experience 36, Iter 83, disc loss: 0.0006341636076902164, policy loss: 9.758768603526999
Experience 36, Iter 84, disc loss: 0.0005510703146871321, policy loss: 9.646884752127306
Experience 36, Iter 85, disc loss: 0.00048253803207051646, policy loss: 9.610825331802086
Experience 36, Iter 86, disc loss: 0.0006201578333276537, policy loss: 9.909165807168552
Experience 36, Iter 87, disc loss: 0.0007750042131207903, policy loss: 9.576144821366936
Experience 36, Iter 88, disc loss: 0.00060802136941976, policy loss: 10.368323069116581
Experience 36, Iter 89, disc loss: 0.0006130903444202414, policy loss: 9.648682694043252
Experience 36, Iter 90, disc loss: 0.000574039960270526, policy loss: 9.709098261580678
Experience 36, Iter 91, disc loss: 0.0006244631716072733, policy loss: 9.910812758517084
Experience 36, Iter 92, disc loss: 0.0006639251233192694, policy loss: 9.339352999039049
Experience 36, Iter 93, disc loss: 0.0006035750233395784, policy loss: 10.656348040587453
Experience 36, Iter 94, disc loss: 0.0005721322572387203, policy loss: 10.22768166870516
Experience 36, Iter 95, disc loss: 0.0004772738954208724, policy loss: 9.986259851491887
Experience 36, Iter 96, disc loss: 0.000409467225405562, policy loss: 10.596426739439252
Experience 36, Iter 97, disc loss: 0.0005144847282798508, policy loss: 9.587527971584215
Experience 36, Iter 98, disc loss: 0.0005812809501483738, policy loss: 9.861782514395303
Experience 36, Iter 99, disc loss: 0.000608462417921403, policy loss: 9.325557550425497
Experience: 37
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.2067],
        [1.7025],
        [0.0358]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0105, 0.1506, 1.5667, 0.0196, 0.0222, 4.5027]],

        [[0.0105, 0.1506, 1.5667, 0.0196, 0.0222, 4.5027]],

        [[0.0105, 0.1506, 1.5667, 0.0196, 0.0222, 4.5027]],

        [[0.0105, 0.1506, 1.5667, 0.0196, 0.0222, 4.5027]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0136, 0.8267, 6.8100, 0.1433], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0136, 0.8267, 6.8100, 0.1433])
N: 370
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1481.0000, 1481.0000, 1481.0000, 1481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.422
Iter 2/2000 - Loss: 3.552
Iter 3/2000 - Loss: 3.239
Iter 4/2000 - Loss: 3.220
Iter 5/2000 - Loss: 3.236
Iter 6/2000 - Loss: 3.107
Iter 7/2000 - Loss: 2.913
Iter 8/2000 - Loss: 2.734
Iter 9/2000 - Loss: 2.584
Iter 10/2000 - Loss: 2.434
Iter 11/2000 - Loss: 2.252
Iter 12/2000 - Loss: 2.026
Iter 13/2000 - Loss: 1.764
Iter 14/2000 - Loss: 1.483
Iter 15/2000 - Loss: 1.195
Iter 16/2000 - Loss: 0.903
Iter 17/2000 - Loss: 0.606
Iter 18/2000 - Loss: 0.301
Iter 19/2000 - Loss: -0.015
Iter 20/2000 - Loss: -0.340
Iter 1981/2000 - Loss: -8.442
Iter 1982/2000 - Loss: -8.442
Iter 1983/2000 - Loss: -8.442
Iter 1984/2000 - Loss: -8.442
Iter 1985/2000 - Loss: -8.442
Iter 1986/2000 - Loss: -8.442
Iter 1987/2000 - Loss: -8.442
Iter 1988/2000 - Loss: -8.442
Iter 1989/2000 - Loss: -8.442
Iter 1990/2000 - Loss: -8.442
Iter 1991/2000 - Loss: -8.442
Iter 1992/2000 - Loss: -8.442
Iter 1993/2000 - Loss: -8.442
Iter 1994/2000 - Loss: -8.442
Iter 1995/2000 - Loss: -8.442
Iter 1996/2000 - Loss: -8.442
Iter 1997/2000 - Loss: -8.442
Iter 1998/2000 - Loss: -8.442
Iter 1999/2000 - Loss: -8.442
Iter 2000/2000 - Loss: -8.442
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.0852,  8.1485, 30.8234,  1.8146, 19.9977, 52.9730]],

        [[15.9336, 29.1384,  8.2155,  1.9316,  0.6773, 20.7442]],

        [[14.1703, 29.4855,  7.4506,  1.0813,  0.6414, 24.2923]],

        [[13.3364, 23.7976, 15.8199,  1.5708,  1.6979, 42.5969]]])
Signal Variance: tensor([ 0.1122,  1.6426, 14.3425,  0.4686])
Estimated target variance: tensor([0.0136, 0.8267, 6.8100, 0.1433])
N: 370
Signal to noise ratio: tensor([19.4718, 70.7736, 91.4256, 45.4388])
Bound on condition number: tensor([ 140286.2589, 1853296.2203, 3092696.9102,  763932.9027])
Policy Optimizer learning rate:
0.00962780539540442
Experience 37, Iter 0, disc loss: 0.00047017626796527073, policy loss: 9.72518117949516
Experience 37, Iter 1, disc loss: 0.00041910898481511635, policy loss: 9.980375621563747
Experience 37, Iter 2, disc loss: 0.0004295919402243971, policy loss: 10.116135201432346
Experience 37, Iter 3, disc loss: 0.0005679557571692505, policy loss: 9.505202477820927
Experience 37, Iter 4, disc loss: 0.0005306424566072489, policy loss: 10.241166646445448
Experience 37, Iter 5, disc loss: 0.0005427328367205617, policy loss: 9.23964997268778
Experience 37, Iter 6, disc loss: 0.0004146385828464994, policy loss: 9.823530768598102
Experience 37, Iter 7, disc loss: 0.0003483158722790394, policy loss: 10.666886008762344
Experience 37, Iter 8, disc loss: 0.0004554432299117114, policy loss: 10.328451558056498
Experience 37, Iter 9, disc loss: 0.0005493873557600808, policy loss: 9.478387014563198
Experience 37, Iter 10, disc loss: 0.0005330194456222794, policy loss: 9.856920753696762
Experience 37, Iter 11, disc loss: 0.0005278641796075238, policy loss: 9.076790710373189
Experience 37, Iter 12, disc loss: 0.00043123008273078067, policy loss: 10.074096767686815
Experience 37, Iter 13, disc loss: 0.0004315774148286657, policy loss: 9.494585333286313
Experience 37, Iter 14, disc loss: 0.0004431173150628882, policy loss: 9.930301376414656
Experience 37, Iter 15, disc loss: 0.0005636132657699717, policy loss: 9.438067644137282
Experience 37, Iter 16, disc loss: 0.0005314613868078067, policy loss: 9.544275196144458
Experience 37, Iter 17, disc loss: 0.0005276666979198057, policy loss: 10.663289245746089
Experience 37, Iter 18, disc loss: 0.0004938304697660558, policy loss: 9.527903458304227
Experience 37, Iter 19, disc loss: 0.0003503942394503734, policy loss: 10.840988236235242
Experience 37, Iter 20, disc loss: 0.00044399554600732505, policy loss: 9.730929352704095
Experience 37, Iter 21, disc loss: 0.00047533716782922214, policy loss: 9.508268165911446
Experience 37, Iter 22, disc loss: 0.0005374927392148823, policy loss: 9.705827145626818
Experience 37, Iter 23, disc loss: 0.0004938408737270093, policy loss: 10.043581973819785
Experience 37, Iter 24, disc loss: 0.0004966278547789792, policy loss: 9.626919689350519
Experience 37, Iter 25, disc loss: 0.00034175248256717596, policy loss: 9.81324876986885
Experience 37, Iter 26, disc loss: 0.00031418760026756163, policy loss: 11.111873007228933
Experience 37, Iter 27, disc loss: 0.0004229437947985518, policy loss: 9.158318499871562
Experience 37, Iter 28, disc loss: 0.0004833282471609175, policy loss: 10.937336943804837
Experience 37, Iter 29, disc loss: 0.0005565093165608911, policy loss: 9.970684289039422
Experience 37, Iter 30, disc loss: 0.0005755194606253822, policy loss: 9.154590588445707
Experience 37, Iter 31, disc loss: 0.0005708388819412535, policy loss: 9.252770911818306
Experience 37, Iter 32, disc loss: 0.0005878383223350137, policy loss: 9.292017546020155
Experience 37, Iter 33, disc loss: 0.0005712724722381714, policy loss: 9.211867992470985
Experience 37, Iter 34, disc loss: 0.0005206789731447714, policy loss: 9.878737949375855
Experience 37, Iter 35, disc loss: 0.0004440194799233263, policy loss: 9.26094256047332
Experience 37, Iter 36, disc loss: 0.00045682499717423846, policy loss: 9.701837975045796
Experience 37, Iter 37, disc loss: 0.0006424213686485287, policy loss: 8.851255370335583
Experience 37, Iter 38, disc loss: 0.0005705436271392721, policy loss: 9.171406522486143
Experience 37, Iter 39, disc loss: 0.0005933684687712327, policy loss: 9.504145997974556
Experience 37, Iter 40, disc loss: 0.0004954424436789816, policy loss: 9.485205421472648
Experience 37, Iter 41, disc loss: 0.00036716066025735926, policy loss: 10.030684102048891
Experience 37, Iter 42, disc loss: 0.00043139378362631636, policy loss: 9.573094620335665
Experience 37, Iter 43, disc loss: 0.0005093647166379647, policy loss: 9.704000579295016
Experience 37, Iter 44, disc loss: 0.0005330752445472164, policy loss: 9.594848497730382
Experience 37, Iter 45, disc loss: 0.0005159594392032951, policy loss: 11.092050714809012
Experience 37, Iter 46, disc loss: 0.0004211655513338562, policy loss: 9.875291709404854
Experience 37, Iter 47, disc loss: 0.0003798814779412467, policy loss: 10.019538959030957
Experience 37, Iter 48, disc loss: 0.0003666749064110595, policy loss: 10.34660853921883
Experience 37, Iter 49, disc loss: 0.00047442954422163104, policy loss: 9.774186707798822
Experience 37, Iter 50, disc loss: 0.0006788050233399437, policy loss: 9.487407880062555
Experience 37, Iter 51, disc loss: 0.0006257433409685877, policy loss: 9.485333810727255
Experience 37, Iter 52, disc loss: 0.0006125255014851271, policy loss: 9.358223937959739
Experience 37, Iter 53, disc loss: 0.0003388091298536541, policy loss: 10.321729611184898
Experience 37, Iter 54, disc loss: 0.00028953378126135934, policy loss: 10.828301630605582
Experience 37, Iter 55, disc loss: 0.00034587999024540304, policy loss: 10.11537124541008
Experience 37, Iter 56, disc loss: 0.0004908452635590722, policy loss: 9.351677312134651
Experience 37, Iter 57, disc loss: 0.0005162292061730262, policy loss: 11.355622854459668
Experience 37, Iter 58, disc loss: 0.0006020352887273085, policy loss: 9.977061542672011
Experience 37, Iter 59, disc loss: 0.00039755757587030277, policy loss: 9.965699482600751
Experience 37, Iter 60, disc loss: 0.00030857157918590605, policy loss: 10.596305591196469
Experience 37, Iter 61, disc loss: 0.00033165167435580104, policy loss: 10.884734080672388
Experience 37, Iter 62, disc loss: 0.0005268576899831318, policy loss: 9.101626624032285
Experience 37, Iter 63, disc loss: 0.0005606564819597241, policy loss: 10.269862449516767
Experience 37, Iter 64, disc loss: 0.0005582121128284926, policy loss: 8.894535677838428
Experience 37, Iter 65, disc loss: 0.0004476847355790951, policy loss: 9.700695117010934
Experience 37, Iter 66, disc loss: 0.0005369790901738208, policy loss: 9.784687554710057
Experience 37, Iter 67, disc loss: 0.0004883730467956484, policy loss: 9.48363290270619
Experience 37, Iter 68, disc loss: 0.0005015755252562372, policy loss: 9.477193898253596
Experience 37, Iter 69, disc loss: 0.0005932089114455293, policy loss: 9.378106223178982
Experience 37, Iter 70, disc loss: 0.0005398316071868829, policy loss: 9.963552987011637
Experience 37, Iter 71, disc loss: 0.0005258022864226866, policy loss: 9.43706257955791
Experience 37, Iter 72, disc loss: 0.00041141245585490016, policy loss: 10.077061488447093
Experience 37, Iter 73, disc loss: 0.0006510776169466278, policy loss: 8.640525990207271
Experience 37, Iter 74, disc loss: 0.0005998994350770144, policy loss: 9.279997952781994
Experience 37, Iter 75, disc loss: 0.0006205789907089593, policy loss: 9.3829555865515
Experience 37, Iter 76, disc loss: 0.0006117803850520071, policy loss: 9.081738335717702
Experience 37, Iter 77, disc loss: 0.0005513649834337501, policy loss: 8.956414926581097
Experience 37, Iter 78, disc loss: 0.0005045099193674892, policy loss: 9.946957358771812
Experience 37, Iter 79, disc loss: 0.0005634339626928276, policy loss: 9.556424011718512
Experience 37, Iter 80, disc loss: 0.0004953777509409364, policy loss: 9.67586592662158
Experience 37, Iter 81, disc loss: 0.0005573168293400776, policy loss: 9.612305784072642
Experience 37, Iter 82, disc loss: 0.000521022917043834, policy loss: 9.825079962733184
Experience 37, Iter 83, disc loss: 0.00035747819941847343, policy loss: 10.001925928970543
Experience 37, Iter 84, disc loss: 0.0004175800004327893, policy loss: 9.78893939289772
Experience 37, Iter 85, disc loss: 0.0005371030211794337, policy loss: 9.355137443253183
Experience 37, Iter 86, disc loss: 0.0004827395372871112, policy loss: 10.006115734250882
Experience 37, Iter 87, disc loss: 0.0004499711791934391, policy loss: 9.461035134301765
Experience 37, Iter 88, disc loss: 0.00046672180623225513, policy loss: 9.081291289367877
Experience 37, Iter 89, disc loss: 0.0004546994800986488, policy loss: 9.707528641270635
Experience 37, Iter 90, disc loss: 0.0005319511170898347, policy loss: 9.682891805169609
Experience 37, Iter 91, disc loss: 0.0005265188363823655, policy loss: 10.94691540275273
Experience 37, Iter 92, disc loss: 0.0005165933502646097, policy loss: 9.203485549537367
Experience 37, Iter 93, disc loss: 0.000449123913746941, policy loss: 9.42705966315285
Experience 37, Iter 94, disc loss: 0.0003828176149398892, policy loss: 10.001819907492944
Experience 37, Iter 95, disc loss: 0.000454681280017184, policy loss: 9.673998623597864
Experience 37, Iter 96, disc loss: 0.00048124858930992983, policy loss: 10.951510255738572
Experience 37, Iter 97, disc loss: 0.0005407835741650729, policy loss: 9.161543471369813
Experience 37, Iter 98, disc loss: 0.00045548902696301825, policy loss: 9.630399561515787
Experience 37, Iter 99, disc loss: 0.0004096790685406212, policy loss: 9.59739000905448
Experience: 38
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.2122],
        [1.7361],
        [0.0367]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0104, 0.1529, 1.6096, 0.0202, 0.0226, 4.6168]],

        [[0.0104, 0.1529, 1.6096, 0.0202, 0.0226, 4.6168]],

        [[0.0104, 0.1529, 1.6096, 0.0202, 0.0226, 4.6168]],

        [[0.0104, 0.1529, 1.6096, 0.0202, 0.0226, 4.6168]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0138, 0.8488, 6.9444, 0.1467], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0138, 0.8488, 6.9444, 0.1467])
N: 380
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1521.0000, 1521.0000, 1521.0000, 1521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.463
Iter 2/2000 - Loss: 3.587
Iter 3/2000 - Loss: 3.277
Iter 4/2000 - Loss: 3.258
Iter 5/2000 - Loss: 3.272
Iter 6/2000 - Loss: 3.145
Iter 7/2000 - Loss: 2.953
Iter 8/2000 - Loss: 2.776
Iter 9/2000 - Loss: 2.627
Iter 10/2000 - Loss: 2.479
Iter 11/2000 - Loss: 2.299
Iter 12/2000 - Loss: 2.075
Iter 13/2000 - Loss: 1.816
Iter 14/2000 - Loss: 1.539
Iter 15/2000 - Loss: 1.255
Iter 16/2000 - Loss: 0.965
Iter 17/2000 - Loss: 0.669
Iter 18/2000 - Loss: 0.362
Iter 19/2000 - Loss: 0.043
Iter 20/2000 - Loss: -0.286
Iter 1981/2000 - Loss: -8.457
Iter 1982/2000 - Loss: -8.457
Iter 1983/2000 - Loss: -8.457
Iter 1984/2000 - Loss: -8.457
Iter 1985/2000 - Loss: -8.457
Iter 1986/2000 - Loss: -8.457
Iter 1987/2000 - Loss: -8.457
Iter 1988/2000 - Loss: -8.457
Iter 1989/2000 - Loss: -8.457
Iter 1990/2000 - Loss: -8.457
Iter 1991/2000 - Loss: -8.457
Iter 1992/2000 - Loss: -8.458
Iter 1993/2000 - Loss: -8.458
Iter 1994/2000 - Loss: -8.458
Iter 1995/2000 - Loss: -8.458
Iter 1996/2000 - Loss: -8.458
Iter 1997/2000 - Loss: -8.458
Iter 1998/2000 - Loss: -8.458
Iter 1999/2000 - Loss: -8.458
Iter 2000/2000 - Loss: -8.458
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 9.1295,  6.9704, 30.1491,  1.8422, 19.6593, 52.0596]],

        [[15.6280, 28.6768,  7.3875,  1.6180,  0.9234, 23.1604]],

        [[14.9175, 28.2956,  7.3669,  1.0278,  0.6548, 24.8028]],

        [[13.5776, 24.0402, 15.4466,  1.4808,  1.7264, 42.8642]]])
Signal Variance: tensor([ 0.0980,  1.7529, 14.5359,  0.4467])
Estimated target variance: tensor([0.0138, 0.8488, 6.9444, 0.1467])
N: 380
Signal to noise ratio: tensor([18.3796, 71.7793, 91.3374, 44.6997])
Bound on condition number: tensor([ 128369.2038, 1957862.8521, 3170157.6410,  759265.2626])
Policy Optimizer learning rate:
0.009617666831952545
Experience 38, Iter 0, disc loss: 0.00046747102738474905, policy loss: 9.076214475347195
Experience 38, Iter 1, disc loss: 0.0005407202841670702, policy loss: 9.768986129759375
Experience 38, Iter 2, disc loss: 0.0005169015672758111, policy loss: 10.2859174372499
Experience 38, Iter 3, disc loss: 0.0005018293743308419, policy loss: 9.211283241886852
Experience 38, Iter 4, disc loss: 0.0004522132787739056, policy loss: 10.230275227281776
Experience 38, Iter 5, disc loss: 0.000445984454361239, policy loss: 10.27253442712804
Experience 38, Iter 6, disc loss: 0.0006323612793191715, policy loss: 9.144887606499987
Experience 38, Iter 7, disc loss: 0.0006500403437195865, policy loss: 9.369074930585025
Experience 38, Iter 8, disc loss: 0.0007829922525854674, policy loss: 10.335980831176457
Experience 38, Iter 9, disc loss: 0.0007365421297848653, policy loss: 10.438440294721186
Experience 38, Iter 10, disc loss: 0.00035928220977019605, policy loss: 10.279577938341708
Experience 38, Iter 11, disc loss: 0.0002646717886059044, policy loss: 12.799727352705936
Experience 38, Iter 12, disc loss: 0.0003435152065850186, policy loss: 11.438765362726233
Experience 38, Iter 13, disc loss: 0.0005565228983602547, policy loss: 9.828519028259983
Experience 38, Iter 14, disc loss: 0.000497182664710182, policy loss: 11.231417218301704
Experience 38, Iter 15, disc loss: 0.0004998744990415173, policy loss: 10.270738019821128
Experience 38, Iter 16, disc loss: 0.00047231318980537295, policy loss: 10.329518956342465
Experience 38, Iter 17, disc loss: 0.0003811273698278078, policy loss: 10.781489321443614
Experience 38, Iter 18, disc loss: 0.0003273879508847033, policy loss: 11.104071150092999
Experience 38, Iter 19, disc loss: 0.00030754793310939664, policy loss: 11.343186038143918
Experience 38, Iter 20, disc loss: 0.0003860687089539202, policy loss: 10.181272512489736
Experience 38, Iter 21, disc loss: 0.00048219280439045317, policy loss: 9.602840523451512
Experience 38, Iter 22, disc loss: 0.0004926747125051402, policy loss: 10.528661214104687
Experience 38, Iter 23, disc loss: 0.0004161320530924222, policy loss: 9.631966301421087
Experience 38, Iter 24, disc loss: 0.00031306164890506857, policy loss: 10.699738165888483
Experience 38, Iter 25, disc loss: 0.0003380184202702765, policy loss: 10.2878190490386
Experience 38, Iter 26, disc loss: 0.0005355820970519872, policy loss: 9.231225983605759
Experience 38, Iter 27, disc loss: 0.0006602738115301401, policy loss: 10.031743633687938
Experience 38, Iter 28, disc loss: 0.0005303073704904099, policy loss: 9.543753506092997
Experience 38, Iter 29, disc loss: 0.0003568209309803852, policy loss: 10.75337987475913
Experience 38, Iter 30, disc loss: 0.0003084455534379931, policy loss: 11.128535133976069
Experience 38, Iter 31, disc loss: 0.00035461384230125504, policy loss: 10.29312373014857
Experience 38, Iter 32, disc loss: 0.0005748486320329933, policy loss: 9.846668454285158
Experience 38, Iter 33, disc loss: 0.0006005970696755035, policy loss: 10.280652726683808
Experience 38, Iter 34, disc loss: 0.0005269081824545547, policy loss: 9.353613901071533
Experience 38, Iter 35, disc loss: 0.0003873361755433452, policy loss: 10.354704764292524
Experience 38, Iter 36, disc loss: 0.0003053842333013999, policy loss: 10.599793010058816
Experience 38, Iter 37, disc loss: 0.00030198865470613495, policy loss: 10.249767993292151
Experience 38, Iter 38, disc loss: 0.000497144725159904, policy loss: 9.699285560664388
Experience 38, Iter 39, disc loss: 0.0005160322681025967, policy loss: 10.819815818571897
Experience 38, Iter 40, disc loss: 0.0005198346549520835, policy loss: 10.199255545345034
Experience 38, Iter 41, disc loss: 0.0005080016385374444, policy loss: 10.190655923239905
Experience 38, Iter 42, disc loss: 0.0003445809593332069, policy loss: 10.601068781640038
Experience 38, Iter 43, disc loss: 0.0003109201269927382, policy loss: 10.929176640660847
Experience 38, Iter 44, disc loss: 0.0003995433412668783, policy loss: 10.32661999338802
Experience 38, Iter 45, disc loss: 0.0005278441335582213, policy loss: 9.453480900646305
Experience 38, Iter 46, disc loss: 0.0006871662220620178, policy loss: 9.006258592735104
Experience 38, Iter 47, disc loss: 0.0005722936317339397, policy loss: 10.025214754967154
Experience 38, Iter 48, disc loss: 0.00043054000376966295, policy loss: 9.667295033505454
Experience 38, Iter 49, disc loss: 0.0003860437520119024, policy loss: 10.043368853348703
Experience 38, Iter 50, disc loss: 0.0003800224690955467, policy loss: 10.349761507186962
Experience 38, Iter 51, disc loss: 0.0004881622661638748, policy loss: 10.131756227718398
Experience 38, Iter 52, disc loss: 0.0005101677495251359, policy loss: 9.58182715465879
Experience 38, Iter 53, disc loss: 0.00047295681958904893, policy loss: 10.241180308077805
Experience 38, Iter 54, disc loss: 0.00039075561894356443, policy loss: 10.374811236148831
Experience 38, Iter 55, disc loss: 0.0005189693647180797, policy loss: 8.99319692651354
Experience 38, Iter 56, disc loss: 0.000470131331671963, policy loss: 9.518148709470013
Experience 38, Iter 57, disc loss: 0.0005093404967003001, policy loss: 9.640091097930192
Experience 38, Iter 58, disc loss: 0.0005628508026392605, policy loss: 9.406502964459857
Experience 38, Iter 59, disc loss: 0.0005559174007238874, policy loss: 9.397898171076559
Experience 38, Iter 60, disc loss: 0.000474789681345925, policy loss: 9.586960469423627
Experience 38, Iter 61, disc loss: 0.00039999002975222534, policy loss: 9.630895824977197
Experience 38, Iter 62, disc loss: 0.00037449376834493124, policy loss: 10.170350773829444
Experience 38, Iter 63, disc loss: 0.0004221189104306229, policy loss: 9.939072458376792
Experience 38, Iter 64, disc loss: 0.0005830253363424218, policy loss: 9.284208458118504
Experience 38, Iter 65, disc loss: 0.0006159077625086162, policy loss: 9.286974581512856
Experience 38, Iter 66, disc loss: 0.0006019857303185992, policy loss: 9.135742762198678
Experience 38, Iter 67, disc loss: 0.0005425528382414051, policy loss: 10.43282285739034
Experience 38, Iter 68, disc loss: 0.0004281170204510265, policy loss: 9.624496167400514
Experience 38, Iter 69, disc loss: 0.0003190978959288572, policy loss: 11.656126383933207
Experience 38, Iter 70, disc loss: 0.0003860287108297482, policy loss: 10.020478624255428
Experience 38, Iter 71, disc loss: 0.0004641905279761224, policy loss: 9.703980243004686
Experience 38, Iter 72, disc loss: 0.00039178067190868754, policy loss: 11.755406542146607
Experience 38, Iter 73, disc loss: 0.0004755994241746257, policy loss: 9.951702374620819
Experience 38, Iter 74, disc loss: 0.0004056393940482595, policy loss: 9.904137032021142
Experience 38, Iter 75, disc loss: 0.00036407065929242174, policy loss: 10.236475985242329
Experience 38, Iter 76, disc loss: 0.0004868915795658567, policy loss: 10.190109997677897
Experience 38, Iter 77, disc loss: 0.0006586787676012436, policy loss: 8.930217879874153
Experience 38, Iter 78, disc loss: 0.0005761155200536964, policy loss: 9.83364548149881
Experience 38, Iter 79, disc loss: 0.0006305866283370001, policy loss: 10.753598864557244
Experience 38, Iter 80, disc loss: 0.00046599277280920447, policy loss: 10.636734416223337
Experience 38, Iter 81, disc loss: 0.0003689460758258366, policy loss: 9.850967751367408
Experience 38, Iter 82, disc loss: 0.0004534894017494298, policy loss: 10.025543825660877
Experience 38, Iter 83, disc loss: 0.0005068015621298359, policy loss: 10.05628046276657
Experience 38, Iter 84, disc loss: 0.0006181987532533652, policy loss: 10.690257370837104
Experience 38, Iter 85, disc loss: 0.0003699144060029712, policy loss: 10.373664003233765
Experience 38, Iter 86, disc loss: 0.0002942286158511832, policy loss: 11.49557151794334
Experience 38, Iter 87, disc loss: 0.0002822925479168734, policy loss: 11.197158465923371
Experience 38, Iter 88, disc loss: 0.0003488681468670926, policy loss: 10.099022585809369
Experience 38, Iter 89, disc loss: 0.00044323309176205966, policy loss: 9.638202141792478
Experience 38, Iter 90, disc loss: 0.0003873090228062347, policy loss: 11.481164711443057
Experience 38, Iter 91, disc loss: 0.0003947130045959989, policy loss: 10.062901154939176
Experience 38, Iter 92, disc loss: 0.0003235843110823797, policy loss: 10.612692228615053
Experience 38, Iter 93, disc loss: 0.00027163910066009084, policy loss: 10.990240320974108
Experience 38, Iter 94, disc loss: 0.00035031447394952135, policy loss: 10.437162488007205
Experience 38, Iter 95, disc loss: 0.0005066556161877892, policy loss: 9.363801613391477
Experience 38, Iter 96, disc loss: 0.0005003490498300654, policy loss: 11.013368835020417
Experience 38, Iter 97, disc loss: 0.0005172386608903502, policy loss: 10.648249594111832
Experience 38, Iter 98, disc loss: 0.0005614687166120851, policy loss: 9.265625882508733
Experience 38, Iter 99, disc loss: 0.00037488893940463566, policy loss: 9.673629795927429
Experience: 39
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0035],
        [0.2183],
        [1.7724],
        [0.0380]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0104, 0.1555, 1.6628, 0.0208, 0.0233, 4.7352]],

        [[0.0104, 0.1555, 1.6628, 0.0208, 0.0233, 4.7352]],

        [[0.0104, 0.1555, 1.6628, 0.0208, 0.0233, 4.7352]],

        [[0.0104, 0.1555, 1.6628, 0.0208, 0.0233, 4.7352]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0139, 0.8733, 7.0897, 0.1519], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0139, 0.8733, 7.0897, 0.1519])
N: 390
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1561.0000, 1561.0000, 1561.0000, 1561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.512
Iter 2/2000 - Loss: 3.629
Iter 3/2000 - Loss: 3.322
Iter 4/2000 - Loss: 3.298
Iter 5/2000 - Loss: 3.310
Iter 6/2000 - Loss: 3.182
Iter 7/2000 - Loss: 2.990
Iter 8/2000 - Loss: 2.811
Iter 9/2000 - Loss: 2.659
Iter 10/2000 - Loss: 2.508
Iter 11/2000 - Loss: 2.326
Iter 12/2000 - Loss: 2.100
Iter 13/2000 - Loss: 1.840
Iter 14/2000 - Loss: 1.561
Iter 15/2000 - Loss: 1.274
Iter 16/2000 - Loss: 0.982
Iter 17/2000 - Loss: 0.683
Iter 18/2000 - Loss: 0.372
Iter 19/2000 - Loss: 0.050
Iter 20/2000 - Loss: -0.281
Iter 1981/2000 - Loss: -8.451
Iter 1982/2000 - Loss: -8.451
Iter 1983/2000 - Loss: -8.451
Iter 1984/2000 - Loss: -8.451
Iter 1985/2000 - Loss: -8.451
Iter 1986/2000 - Loss: -8.451
Iter 1987/2000 - Loss: -8.451
Iter 1988/2000 - Loss: -8.452
Iter 1989/2000 - Loss: -8.452
Iter 1990/2000 - Loss: -8.452
Iter 1991/2000 - Loss: -8.452
Iter 1992/2000 - Loss: -8.452
Iter 1993/2000 - Loss: -8.452
Iter 1994/2000 - Loss: -8.452
Iter 1995/2000 - Loss: -8.452
Iter 1996/2000 - Loss: -8.452
Iter 1997/2000 - Loss: -8.452
Iter 1998/2000 - Loss: -8.452
Iter 1999/2000 - Loss: -8.452
Iter 2000/2000 - Loss: -8.452
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[ 9.6783,  5.4085, 29.4222,  1.8708, 19.1036, 50.6956]],

        [[16.0177, 28.3823,  7.3115,  1.5700,  0.9196, 23.2111]],

        [[15.3144, 27.2726,  7.2073,  1.0399,  0.6732, 24.8239]],

        [[12.8289, 23.0627, 14.5999,  1.3740,  1.6874, 43.0143]]])
Signal Variance: tensor([ 0.0818,  1.6927, 14.6633,  0.4262])
Estimated target variance: tensor([0.0139, 0.8733, 7.0897, 0.1519])
N: 390
Signal to noise ratio: tensor([16.9600, 70.8629, 89.7830, 43.0855])
Bound on condition number: tensor([ 112181.1759, 1958408.2460, 3143782.8735,  723982.8149])
Policy Optimizer learning rate:
0.00960753894491805
Experience 39, Iter 0, disc loss: 0.00029736415136868744, policy loss: 10.445121228313258
Experience 39, Iter 1, disc loss: 0.000397386612189591, policy loss: 10.400462442867843
Experience 39, Iter 2, disc loss: 0.0005640025176013602, policy loss: 9.479281893377754
Experience 39, Iter 3, disc loss: 0.00046840984998128407, policy loss: 11.884182795832979
Experience 39, Iter 4, disc loss: 0.0005218610700917927, policy loss: 9.324148390927881
Experience 39, Iter 5, disc loss: 0.0002923573301805484, policy loss: 10.029118556026553
Experience 39, Iter 6, disc loss: 0.0002696773623413786, policy loss: 11.262098204939218
Experience 39, Iter 7, disc loss: 0.0002585653109830841, policy loss: 10.976852518717376
Experience 39, Iter 8, disc loss: 0.0003619020805509411, policy loss: 10.15361337494938
Experience 39, Iter 9, disc loss: 0.0003908990557258778, policy loss: 10.780078838212757
Experience 39, Iter 10, disc loss: 0.0004742447083994383, policy loss: 11.63803063202959
Experience 39, Iter 11, disc loss: 0.00043015906150074715, policy loss: 9.244309817235427
Experience 39, Iter 12, disc loss: 0.00030335836757730273, policy loss: 11.212280014222966
Experience 39, Iter 13, disc loss: 0.0002586456225541374, policy loss: 11.16471915578384
Experience 39, Iter 14, disc loss: 0.0003762084680047085, policy loss: 9.920870225920105
Experience 39, Iter 15, disc loss: 0.0005628155213881229, policy loss: 9.695264459244381
Experience 39, Iter 16, disc loss: 0.00041044214444922413, policy loss: 11.609669637793264
Experience 39, Iter 17, disc loss: 0.0004813352068788313, policy loss: 10.930952385858834
Experience 39, Iter 18, disc loss: 0.0003590454018868118, policy loss: 10.243461644323318
Experience 39, Iter 19, disc loss: 0.000416785561126196, policy loss: 10.136591587138954
Experience 39, Iter 20, disc loss: 0.00044057200774790965, policy loss: 9.836227309826425
Experience 39, Iter 21, disc loss: 0.0004570062907403287, policy loss: 9.41991821742868
Experience 39, Iter 22, disc loss: 0.00042706457580672045, policy loss: 10.071393305449082
Experience 39, Iter 23, disc loss: 0.0004222852248061086, policy loss: 9.475778713213227
Experience 39, Iter 24, disc loss: 0.0003558666074173194, policy loss: 10.271853496790566
Experience 39, Iter 25, disc loss: 0.0004460897404936209, policy loss: 9.660750562757176
Experience 39, Iter 26, disc loss: 0.00043515369014336756, policy loss: 10.881522565111299
Experience 39, Iter 27, disc loss: 0.00040861241172922687, policy loss: 9.49311339099356
Experience 39, Iter 28, disc loss: 0.0003285000022811691, policy loss: 10.898706308944409
Experience 39, Iter 29, disc loss: 0.00029121229605874263, policy loss: 10.61036463194688
Experience 39, Iter 30, disc loss: 0.00041104717580633335, policy loss: 9.586944561720077
Experience 39, Iter 31, disc loss: 0.00046500281922735756, policy loss: 10.75367168999532
Experience 39, Iter 32, disc loss: 0.0006291776561704744, policy loss: 9.650629662236785
Experience 39, Iter 33, disc loss: 0.00033222646859292816, policy loss: 9.79493135651732
Experience 39, Iter 34, disc loss: 0.00036605843803493997, policy loss: 9.819766764403324
Experience 39, Iter 35, disc loss: 0.0004732107778182548, policy loss: 10.415902096717804
Experience 39, Iter 36, disc loss: 0.0005917689263377685, policy loss: 10.111414839268956
Experience 39, Iter 37, disc loss: 0.0005636879084044788, policy loss: 9.457758234247745
Experience 39, Iter 38, disc loss: 0.0003514963311321105, policy loss: 10.115567716222833
Experience 39, Iter 39, disc loss: 0.00032422922074386236, policy loss: 10.642918097733604
Experience 39, Iter 40, disc loss: 0.00027966542933325136, policy loss: 10.511385633029887
Experience 39, Iter 41, disc loss: 0.00036487002816565004, policy loss: 9.956786256400008
Experience 39, Iter 42, disc loss: 0.00043295666304516214, policy loss: 10.907984758635946
Experience 39, Iter 43, disc loss: 0.0004949487557560964, policy loss: 9.8652566686961
Experience 39, Iter 44, disc loss: 0.0003216314755443787, policy loss: 10.064044359134366
Experience 39, Iter 45, disc loss: 0.00026171999798896793, policy loss: 11.103980815992108
Experience 39, Iter 46, disc loss: 0.0002907545994572789, policy loss: 10.45892215585203
Experience 39, Iter 47, disc loss: 0.00044040317042652767, policy loss: 9.470758548299054
Experience 39, Iter 48, disc loss: 0.0004931109833940977, policy loss: 9.966056822999906
Experience 39, Iter 49, disc loss: 0.0004920673279059054, policy loss: 11.637782508599742
Experience 39, Iter 50, disc loss: 0.0003286393491889238, policy loss: 10.478773942219133
Experience 39, Iter 51, disc loss: 0.00028130622890141816, policy loss: 10.89883739607377
Experience 39, Iter 52, disc loss: 0.00027398253043009935, policy loss: 10.92323063783893
Experience 39, Iter 53, disc loss: 0.00033873486336553514, policy loss: 10.164433791829476
Experience 39, Iter 54, disc loss: 0.0005183650368698762, policy loss: 10.466542547123957
Experience 39, Iter 55, disc loss: 0.0004787731091308156, policy loss: 9.6691704952195
Experience 39, Iter 56, disc loss: 0.00040975655274195686, policy loss: 10.123297017544697
Experience 39, Iter 57, disc loss: 0.0003073187745189416, policy loss: 9.809160544510672
Experience 39, Iter 58, disc loss: 0.0003222700802216977, policy loss: 10.02249640956105
Experience 39, Iter 59, disc loss: 0.0005136249815190562, policy loss: 9.519567137498758
Experience 39, Iter 60, disc loss: 0.00048177318944397506, policy loss: 10.642936560253474
Experience 39, Iter 61, disc loss: 0.0004832044850254159, policy loss: 10.510256761576947
Experience 39, Iter 62, disc loss: 0.0004674337723836234, policy loss: 9.902347733159445
Experience 39, Iter 63, disc loss: 0.0003095144784211918, policy loss: 10.381442907595961
Experience 39, Iter 64, disc loss: 0.00039034723714603467, policy loss: 10.301198800267972
Experience 39, Iter 65, disc loss: 0.0004842441273799579, policy loss: 10.666783905670506
Experience 39, Iter 66, disc loss: 0.0005033063408337256, policy loss: 9.75948510648668
Experience 39, Iter 67, disc loss: 0.00047137949267987586, policy loss: 9.893912506067851
Experience 39, Iter 68, disc loss: 0.00039374103894099363, policy loss: 10.254974811514147
Experience 39, Iter 69, disc loss: 0.0003193593838393188, policy loss: 10.464917834041351
Experience 39, Iter 70, disc loss: 0.000314504558461262, policy loss: 10.054079847115084
Experience 39, Iter 71, disc loss: 0.0004367394657885942, policy loss: 9.93457616724425
Experience 39, Iter 72, disc loss: 0.00048311241108747326, policy loss: 9.934727005573858
Experience 39, Iter 73, disc loss: 0.0004923232329441697, policy loss: 10.297186887312748
Experience 39, Iter 74, disc loss: 0.0004257996569085138, policy loss: 10.559538605384443
Experience 39, Iter 75, disc loss: 0.00048626467555001017, policy loss: 9.39141431194204
Experience 39, Iter 76, disc loss: 0.0003594785230634043, policy loss: 9.96394715980044
Experience 39, Iter 77, disc loss: 0.0003305191970058791, policy loss: 10.168425763734492
Experience 39, Iter 78, disc loss: 0.0004284774483499993, policy loss: 9.843122555612625
Experience 39, Iter 79, disc loss: 0.0004330198535833006, policy loss: 9.744260606909776
Experience 39, Iter 80, disc loss: 0.0004932006922816418, policy loss: 9.410803172871619
Experience 39, Iter 81, disc loss: 0.000447422420968599, policy loss: 10.07572877887456
Experience 39, Iter 82, disc loss: 0.00037787092455829135, policy loss: 9.834649757138482
Experience 39, Iter 83, disc loss: 0.0004539630937750735, policy loss: 9.519425023002917
Experience 39, Iter 84, disc loss: 0.00036766844251616524, policy loss: 9.907621473522426
Experience 39, Iter 85, disc loss: 0.00033722038534154107, policy loss: 10.473951249866529
Experience 39, Iter 86, disc loss: 0.0003532637903056476, policy loss: 9.734597278639082
Experience 39, Iter 87, disc loss: 0.0004592677760344117, policy loss: 9.171089348462274
Experience 39, Iter 88, disc loss: 0.00045715562793101745, policy loss: 10.203765556209065
Experience 39, Iter 89, disc loss: 0.00046596495510950367, policy loss: 9.736157933220927
Experience 39, Iter 90, disc loss: 0.00032563055915147744, policy loss: 9.900410139038724
Experience 39, Iter 91, disc loss: 0.0003431742165911124, policy loss: 10.151190659425156
Experience 39, Iter 92, disc loss: 0.0004921163570668812, policy loss: 9.336102207948688
Experience 39, Iter 93, disc loss: 0.0006321580352630221, policy loss: 10.051862039139971
Experience 39, Iter 94, disc loss: 0.0006390708406559609, policy loss: 9.958829195732648
Experience 39, Iter 95, disc loss: 0.0003684982400316524, policy loss: 10.341417730564217
Experience 39, Iter 96, disc loss: 0.00028011608293191113, policy loss: 11.3340630833231
Experience 39, Iter 97, disc loss: 0.0002753035958560666, policy loss: 11.24115025320377
Experience 39, Iter 98, disc loss: 0.0003043523713640731, policy loss: 10.408824954395905
Experience 39, Iter 99, disc loss: 0.0003838429692961249, policy loss: 9.667005869214824
Experience: 40
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0035],
        [0.2243],
        [1.8048],
        [0.0391]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0103, 0.1580, 1.7103, 0.0214, 0.0237, 4.8526]],

        [[0.0103, 0.1580, 1.7103, 0.0214, 0.0237, 4.8526]],

        [[0.0103, 0.1580, 1.7103, 0.0214, 0.0237, 4.8526]],

        [[0.0103, 0.1580, 1.7103, 0.0214, 0.0237, 4.8526]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0141, 0.8974, 7.2191, 0.1563], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0141, 0.8974, 7.2191, 0.1563])
N: 400
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1601.0000, 1601.0000, 1601.0000, 1601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.553
Iter 2/2000 - Loss: 3.664
Iter 3/2000 - Loss: 3.358
Iter 4/2000 - Loss: 3.330
Iter 5/2000 - Loss: 3.340
Iter 6/2000 - Loss: 3.212
Iter 7/2000 - Loss: 3.019
Iter 8/2000 - Loss: 2.838
Iter 9/2000 - Loss: 2.684
Iter 10/2000 - Loss: 2.531
Iter 11/2000 - Loss: 2.348
Iter 12/2000 - Loss: 2.123
Iter 13/2000 - Loss: 1.864
Iter 14/2000 - Loss: 1.587
Iter 15/2000 - Loss: 1.302
Iter 16/2000 - Loss: 1.011
Iter 17/2000 - Loss: 0.711
Iter 18/2000 - Loss: 0.400
Iter 19/2000 - Loss: 0.077
Iter 20/2000 - Loss: -0.255
Iter 1981/2000 - Loss: -8.422
Iter 1982/2000 - Loss: -8.422
Iter 1983/2000 - Loss: -8.422
Iter 1984/2000 - Loss: -8.423
Iter 1985/2000 - Loss: -8.423
Iter 1986/2000 - Loss: -8.423
Iter 1987/2000 - Loss: -8.423
Iter 1988/2000 - Loss: -8.423
Iter 1989/2000 - Loss: -8.423
Iter 1990/2000 - Loss: -8.423
Iter 1991/2000 - Loss: -8.423
Iter 1992/2000 - Loss: -8.423
Iter 1993/2000 - Loss: -8.423
Iter 1994/2000 - Loss: -8.423
Iter 1995/2000 - Loss: -8.423
Iter 1996/2000 - Loss: -8.423
Iter 1997/2000 - Loss: -8.423
Iter 1998/2000 - Loss: -8.423
Iter 1999/2000 - Loss: -8.423
Iter 2000/2000 - Loss: -8.423
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.7111,  6.5115, 30.6750,  1.6947, 18.9736, 53.7057]],

        [[16.2054, 28.8949,  7.8026,  1.6940,  0.7222, 21.6485]],

        [[15.7576, 29.8737,  7.2902,  1.0361,  0.6522, 24.9581]],

        [[13.0494, 23.3472, 15.3354,  1.3057,  1.7007, 45.8679]]])
Signal Variance: tensor([ 0.0952,  1.5981, 13.9063,  0.4670])
Estimated target variance: tensor([0.0141, 0.8974, 7.2191, 0.1563])
N: 400
Signal to noise ratio: tensor([18.2335, 69.2440, 84.8578, 44.3029])
Bound on condition number: tensor([ 132985.1143, 1917895.8381, 2880336.7319,  785101.3108])
Policy Optimizer learning rate:
0.009597421723058133
Experience 40, Iter 0, disc loss: 0.0003431549510952153, policy loss: 12.327164748433098
Experience 40, Iter 1, disc loss: 0.00038612124304778135, policy loss: 9.678688273032597
Experience 40, Iter 2, disc loss: 0.00029029851335120554, policy loss: 10.25696498690369
Experience 40, Iter 3, disc loss: 0.000260107846653845, policy loss: 10.761446022068467
Experience 40, Iter 4, disc loss: 0.00038300036319721545, policy loss: 9.911293858921473
Experience 40, Iter 5, disc loss: 0.0004380275299406186, policy loss: 10.248595969767608
Experience 40, Iter 6, disc loss: 0.0004811849640781649, policy loss: 9.533261325822869
Experience 40, Iter 7, disc loss: 0.0005202023329181197, policy loss: 10.0933755238307
Experience 40, Iter 8, disc loss: 0.0004596806055691564, policy loss: 9.78829020171737
Experience 40, Iter 9, disc loss: 0.0002851126819056023, policy loss: 11.35092000535593
Experience 40, Iter 10, disc loss: 0.00025451892144117235, policy loss: 11.31052405169168
Experience 40, Iter 11, disc loss: 0.00031906579628379465, policy loss: 10.585291920562291
Experience 40, Iter 12, disc loss: 0.00043203641637500295, policy loss: 9.90808993113376
Experience 40, Iter 13, disc loss: 0.0004772841340455311, policy loss: 10.983725770037218
Experience 40, Iter 14, disc loss: 0.0004566701022341382, policy loss: 9.792306149894264
Experience 40, Iter 15, disc loss: 0.00034275288111253145, policy loss: 9.856568937848644
Experience 40, Iter 16, disc loss: 0.00030018736383686265, policy loss: 10.518523115578054
Experience 40, Iter 17, disc loss: 0.00029679978511895, policy loss: 10.834447479902867
Experience 40, Iter 18, disc loss: 0.00035340358707055554, policy loss: 10.199420763760141
Experience 40, Iter 19, disc loss: 0.000404699634791697, policy loss: 10.34411448276557
Experience 40, Iter 20, disc loss: 0.0003760734353485658, policy loss: 10.390363924489037
Experience 40, Iter 21, disc loss: 0.0004082021839296517, policy loss: 10.183152941194408
Experience 40, Iter 22, disc loss: 0.00042606952418071793, policy loss: 9.752618053436596
Experience 40, Iter 23, disc loss: 0.00034277473387319416, policy loss: 10.702534431784112
Experience 40, Iter 24, disc loss: 0.00038586511410762384, policy loss: 9.610910691357844
Experience 40, Iter 25, disc loss: 0.000381636890382819, policy loss: 9.630893047209536
Experience 40, Iter 26, disc loss: 0.00042797812105245037, policy loss: 9.675294640461622
Experience 40, Iter 27, disc loss: 0.0005346162506270354, policy loss: 9.147202064069992
Experience 40, Iter 28, disc loss: 0.0004456104784238075, policy loss: 10.269407686986941
Experience 40, Iter 29, disc loss: 0.00036886717036483524, policy loss: 10.161189310050503
Experience 40, Iter 30, disc loss: 0.00041655131469700243, policy loss: 9.625560466976172
Experience 40, Iter 31, disc loss: 0.0004765460718597266, policy loss: 9.482235822151566
Experience 40, Iter 32, disc loss: 0.000511481296293903, policy loss: 9.784710178785286
Experience 40, Iter 33, disc loss: 0.00044877454895600477, policy loss: 10.214436389720753
Experience 40, Iter 34, disc loss: 0.0003817854221702655, policy loss: 9.873997481189926
Experience 40, Iter 35, disc loss: 0.00036623978076187033, policy loss: 10.214675913527058
Experience 40, Iter 36, disc loss: 0.00046806738702558613, policy loss: 9.790614205966829
Experience 40, Iter 37, disc loss: 0.00048545725535011823, policy loss: 9.689171219311074
Experience 40, Iter 38, disc loss: 0.0005032405979624295, policy loss: 10.095541646737445
Experience 40, Iter 39, disc loss: 0.00041414788949843304, policy loss: 10.387615427175785
Experience 40, Iter 40, disc loss: 0.00029147737570724624, policy loss: 10.375502729090115
Experience 40, Iter 41, disc loss: 0.00039709009186953076, policy loss: 9.861290786305583
Experience 40, Iter 42, disc loss: 0.00040064438459226864, policy loss: 10.069981375616315
Experience 40, Iter 43, disc loss: 0.0005398581421505822, policy loss: 10.01974755913865
Experience 40, Iter 44, disc loss: 0.00043416688427865403, policy loss: 10.575242010794238
Experience 40, Iter 45, disc loss: 0.0003735537125360806, policy loss: 9.723538798988088
Experience 40, Iter 46, disc loss: 0.0003100630460320681, policy loss: 10.058254171835214
Experience 40, Iter 47, disc loss: 0.00038651521038722265, policy loss: 10.123044187438822
Experience 40, Iter 48, disc loss: 0.00045279850997492093, policy loss: 9.759945556497692
Experience 40, Iter 49, disc loss: 0.00047601387967109336, policy loss: 10.571221122965316
Experience 40, Iter 50, disc loss: 0.00041353274560972493, policy loss: 10.412192940106937
Experience 40, Iter 51, disc loss: 0.00028482383081618545, policy loss: 10.647142187418405
Experience 40, Iter 52, disc loss: 0.0002866161326735659, policy loss: 10.241045579062519
Experience 40, Iter 53, disc loss: 0.00040499894876637585, policy loss: 9.784885266717353
Experience 40, Iter 54, disc loss: 0.00040750143673032606, policy loss: 9.4946728374485
Experience 40, Iter 55, disc loss: 0.00046964576601905606, policy loss: 9.074221501750124
Experience 40, Iter 56, disc loss: 0.0004323500546671702, policy loss: 10.22920143369574
Experience 40, Iter 57, disc loss: 0.00043228983666883066, policy loss: 10.251135336435016
Experience 40, Iter 58, disc loss: 0.0003535787489595041, policy loss: 9.634305437728527
Experience 40, Iter 59, disc loss: 0.0002953698035216283, policy loss: 10.07187742611613
Experience 40, Iter 60, disc loss: 0.00031361470840080734, policy loss: 11.001815809385349
Experience 40, Iter 61, disc loss: 0.00038364658260598214, policy loss: 10.27944913504151
Experience 40, Iter 62, disc loss: 0.00038828595662278537, policy loss: 10.02597200479724
Experience 40, Iter 63, disc loss: 0.0004296721432518702, policy loss: 9.794326605697647
Experience 40, Iter 64, disc loss: 0.0003767558285907098, policy loss: 11.767313933787271
Experience 40, Iter 65, disc loss: 0.0004785279939935297, policy loss: 9.682957591291764
Experience 40, Iter 66, disc loss: 0.0003279254108719214, policy loss: 10.195346852774126
Experience 40, Iter 67, disc loss: 0.00023647449405860926, policy loss: 10.878335673588964
Experience 40, Iter 68, disc loss: 0.0002672989180131657, policy loss: 10.965592862803197
Experience 40, Iter 69, disc loss: 0.0003599892085096101, policy loss: 9.981423749825282
Experience 40, Iter 70, disc loss: 0.000402415774807516, policy loss: 10.252826643258418
Experience 40, Iter 71, disc loss: 0.00036287963594879203, policy loss: 9.940342736537712
Experience 40, Iter 72, disc loss: 0.0003181387704017652, policy loss: 9.90220686703781
Experience 40, Iter 73, disc loss: 0.00029993840024058945, policy loss: 10.623132410776176
Experience 40, Iter 74, disc loss: 0.00030417908083320505, policy loss: 10.498865687030541
Experience 40, Iter 75, disc loss: 0.00038422623026263355, policy loss: 9.838352727323263
Experience 40, Iter 76, disc loss: 0.0003471069730021398, policy loss: 9.904444094950815
Experience 40, Iter 77, disc loss: 0.00039256059086231233, policy loss: 10.649003068313307
Experience 40, Iter 78, disc loss: 0.0003333996282575014, policy loss: 10.894143204013194
Experience 40, Iter 79, disc loss: 0.0002977386891553108, policy loss: 10.159415835565918
Experience 40, Iter 80, disc loss: 0.0002896582164288021, policy loss: 10.03911557526454
Experience 40, Iter 81, disc loss: 0.0003338260625658754, policy loss: 9.90669885220038
Experience 40, Iter 82, disc loss: 0.00047397308426971467, policy loss: 9.852339269984931
Experience 40, Iter 83, disc loss: 0.00038163856471343195, policy loss: 10.196237727525851
Experience 40, Iter 84, disc loss: 0.00040676522212459477, policy loss: 10.852571189612227
Experience 40, Iter 85, disc loss: 0.0003909860362446377, policy loss: 10.578726797476838
Experience 40, Iter 86, disc loss: 0.0003826198188200718, policy loss: 10.743169251929938
Experience 40, Iter 87, disc loss: 0.0003460824236647947, policy loss: 10.190515454693594
Experience 40, Iter 88, disc loss: 0.00043622948019857676, policy loss: 9.728475273051133
Experience 40, Iter 89, disc loss: 0.00036562885357501666, policy loss: 9.791973615805613
Experience 40, Iter 90, disc loss: 0.00033085595954530065, policy loss: 10.774500538668576
Experience 40, Iter 91, disc loss: 0.0003013541872263759, policy loss: 10.89962462673247
Experience 40, Iter 92, disc loss: 0.0003458861317698533, policy loss: 9.795158413891116
Experience 40, Iter 93, disc loss: 0.00029525358096657746, policy loss: 11.00932504476365
Experience 40, Iter 94, disc loss: 0.00034153659828960826, policy loss: 9.850204236746698
Experience 40, Iter 95, disc loss: 0.0003387450075753516, policy loss: 10.647484466821897
Experience 40, Iter 96, disc loss: 0.00033828795312714356, policy loss: 10.270229714105177
Experience 40, Iter 97, disc loss: 0.00021752685799790244, policy loss: 11.339297449768601
Experience 40, Iter 98, disc loss: 0.00022804940694811477, policy loss: 11.703205316993328
Experience 40, Iter 99, disc loss: 0.0002586138118478453, policy loss: 10.588851315176678
Experience: 41
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.2291],
        [1.8301],
        [0.0404]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0101, 0.1600, 1.7621, 0.0219, 0.0245, 4.9683]],

        [[0.0101, 0.1600, 1.7621, 0.0219, 0.0245, 4.9683]],

        [[0.0101, 0.1600, 1.7621, 0.0219, 0.0245, 4.9683]],

        [[0.0101, 0.1600, 1.7621, 0.0219, 0.0245, 4.9683]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0143, 0.9166, 7.3204, 0.1618], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0143, 0.9166, 7.3204, 0.1618])
N: 410
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1641.0000, 1641.0000, 1641.0000, 1641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.584
Iter 2/2000 - Loss: 3.690
Iter 3/2000 - Loss: 3.378
Iter 4/2000 - Loss: 3.343
Iter 5/2000 - Loss: 3.348
Iter 6/2000 - Loss: 3.217
Iter 7/2000 - Loss: 3.021
Iter 8/2000 - Loss: 2.833
Iter 9/2000 - Loss: 2.672
Iter 10/2000 - Loss: 2.514
Iter 11/2000 - Loss: 2.327
Iter 12/2000 - Loss: 2.099
Iter 13/2000 - Loss: 1.838
Iter 14/2000 - Loss: 1.558
Iter 15/2000 - Loss: 1.270
Iter 16/2000 - Loss: 0.974
Iter 17/2000 - Loss: 0.671
Iter 18/2000 - Loss: 0.358
Iter 19/2000 - Loss: 0.033
Iter 20/2000 - Loss: -0.300
Iter 1981/2000 - Loss: -8.448
Iter 1982/2000 - Loss: -8.448
Iter 1983/2000 - Loss: -8.448
Iter 1984/2000 - Loss: -8.448
Iter 1985/2000 - Loss: -8.448
Iter 1986/2000 - Loss: -8.448
Iter 1987/2000 - Loss: -8.448
Iter 1988/2000 - Loss: -8.448
Iter 1989/2000 - Loss: -8.448
Iter 1990/2000 - Loss: -8.448
Iter 1991/2000 - Loss: -8.448
Iter 1992/2000 - Loss: -8.448
Iter 1993/2000 - Loss: -8.448
Iter 1994/2000 - Loss: -8.448
Iter 1995/2000 - Loss: -8.448
Iter 1996/2000 - Loss: -8.448
Iter 1997/2000 - Loss: -8.448
Iter 1998/2000 - Loss: -8.448
Iter 1999/2000 - Loss: -8.448
Iter 2000/2000 - Loss: -8.448
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.7286,  6.0142, 29.2668,  1.6769, 18.8462, 53.2898]],

        [[16.0936, 28.8149,  7.7180,  1.7871,  0.6810, 21.1637]],

        [[15.7540, 29.3791,  7.1942,  1.0759,  0.6385, 24.1648]],

        [[12.9734, 23.3457, 15.0006,  1.3305,  1.6667, 46.3750]]])
Signal Variance: tensor([ 0.0889,  1.5990, 13.8149,  0.4584])
Estimated target variance: tensor([0.0143, 0.9166, 7.3204, 0.1618])
N: 410
Signal to noise ratio: tensor([17.7400, 68.8616, 85.3842, 44.2553])
Bound on condition number: tensor([ 129031.2359, 1944186.5511, 2989089.1857,  802997.3246])
Policy Optimizer learning rate:
0.009587315155141827
Experience 41, Iter 0, disc loss: 0.0004056815314385469, policy loss: 9.946126866608093
Experience 41, Iter 1, disc loss: 0.0004023311382320086, policy loss: 11.249778056789822
Experience 41, Iter 2, disc loss: 0.0004688674808289688, policy loss: 10.116123143018509
Experience 41, Iter 3, disc loss: 0.00047542066797137125, policy loss: 9.878935210234605
Experience 41, Iter 4, disc loss: 0.0002538502717080089, policy loss: 11.20392583014829
Experience 41, Iter 5, disc loss: 0.0002589054469868213, policy loss: 10.357235544424476
Experience 41, Iter 6, disc loss: 0.0003331801542972583, policy loss: 10.140840537555578
Experience 41, Iter 7, disc loss: 0.0004977149878901198, policy loss: 11.248251571811995
Experience 41, Iter 8, disc loss: 0.0004964556101814758, policy loss: 9.86270728360967
Experience 41, Iter 9, disc loss: 0.00045320311419014774, policy loss: 10.377028749147549
Experience 41, Iter 10, disc loss: 0.0004532229546769563, policy loss: 9.479897018946454
Experience 41, Iter 11, disc loss: 0.00026181412306877265, policy loss: 10.929307247364317
Experience 41, Iter 12, disc loss: 0.00029855896732386606, policy loss: 10.326684073519662
Experience 41, Iter 13, disc loss: 0.00035581023780888963, policy loss: 9.76791825880953
Experience 41, Iter 14, disc loss: 0.00035321784587059196, policy loss: 10.670767977270966
Experience 41, Iter 15, disc loss: 0.00037916066755153534, policy loss: 11.96452850307943
Experience 41, Iter 16, disc loss: 0.0003249046614666244, policy loss: 10.130938488480549
Experience 41, Iter 17, disc loss: 0.0002401664451550809, policy loss: 10.731507862121099
Experience 41, Iter 18, disc loss: 0.000246211752806359, policy loss: 11.038030357440292
Experience 41, Iter 19, disc loss: 0.0003374656504015716, policy loss: 9.764954249845939
Experience 41, Iter 20, disc loss: 0.000360595232798178, policy loss: 9.900310061849643
Experience 41, Iter 21, disc loss: 0.00039062270369072144, policy loss: 11.324005866640256
Experience 41, Iter 22, disc loss: 0.0003988828979090579, policy loss: 9.688769115082671
Experience 41, Iter 23, disc loss: 0.000255955496508485, policy loss: 10.819699923606949
Experience 41, Iter 24, disc loss: 0.00027394967559861915, policy loss: 10.38566057535121
Experience 41, Iter 25, disc loss: 0.0003786995942709991, policy loss: 9.376554372496146
Experience 41, Iter 26, disc loss: 0.0004088307273627381, policy loss: 13.433358821574252
Experience 41, Iter 27, disc loss: 0.0004265965253260741, policy loss: 10.44197572254188
Experience 41, Iter 28, disc loss: 0.0002130992695030173, policy loss: 11.18703882396336
Experience 41, Iter 29, disc loss: 0.0001747292240119121, policy loss: 14.322541080298912
Experience 41, Iter 30, disc loss: 0.00017389533976065977, policy loss: 12.908651774320031
Experience 41, Iter 31, disc loss: 0.00018724165404361846, policy loss: 12.293676731615353
Experience 41, Iter 32, disc loss: 0.0002966676978945819, policy loss: 10.658155614088018
Experience 41, Iter 33, disc loss: 0.00031675539425268497, policy loss: 11.149025744970924
Experience 41, Iter 34, disc loss: 0.0003114340454170131, policy loss: 11.04354250238541
Experience 41, Iter 35, disc loss: 0.00030567545934867676, policy loss: 10.931071572704798
Experience 41, Iter 36, disc loss: 0.000317556648100562, policy loss: 10.082516225291144
Experience 41, Iter 37, disc loss: 0.0002370396454995067, policy loss: 10.726587781135455
Experience 41, Iter 38, disc loss: 0.00030815297671777584, policy loss: 10.305256461934661
Experience 41, Iter 39, disc loss: 0.0003148072578879626, policy loss: 10.06210498571182
Experience 41, Iter 40, disc loss: 0.00041238603951286254, policy loss: 10.048034553211998
Experience 41, Iter 41, disc loss: 0.0004383275374350862, policy loss: 10.770654028677281
Experience 41, Iter 42, disc loss: 0.0003621631590370867, policy loss: 10.18710710222797
Experience 41, Iter 43, disc loss: 0.00023526288495928567, policy loss: 11.00486189655777
Experience 41, Iter 44, disc loss: 0.0002576328498041387, policy loss: 10.671059985202415
Experience 41, Iter 45, disc loss: 0.0003064101957291959, policy loss: 10.261522944442076
Experience 41, Iter 46, disc loss: 0.0003616864894350772, policy loss: 10.730759906392706
Experience 41, Iter 47, disc loss: 0.00037919408675408467, policy loss: 9.951561532749402
Experience 41, Iter 48, disc loss: 0.0003728524039088764, policy loss: 9.66435748938287
Experience 41, Iter 49, disc loss: 0.00033990854924886724, policy loss: 9.85772453086409
Experience 41, Iter 50, disc loss: 0.0003182553623793696, policy loss: 10.029549197940453
Experience 41, Iter 51, disc loss: 0.00036892191947999353, policy loss: 10.249359676333864
Experience 41, Iter 52, disc loss: 0.00045472115376384936, policy loss: 9.852034483487481
Experience 41, Iter 53, disc loss: 0.00029584060672041557, policy loss: 10.796504857836124
Experience 41, Iter 54, disc loss: 0.0002506924306524422, policy loss: 10.31557154052594
Experience 41, Iter 55, disc loss: 0.0002939694344995781, policy loss: 10.154314862626869
Experience 41, Iter 56, disc loss: 0.0003418097552960463, policy loss: 10.502087916164152
Experience 41, Iter 57, disc loss: 0.00052627138545139, policy loss: 9.617711602952976
Experience 41, Iter 58, disc loss: 0.00044021103159322254, policy loss: 11.191669940728435
Experience 41, Iter 59, disc loss: 0.0005084213934443242, policy loss: 9.512053722070199
Experience 41, Iter 60, disc loss: 0.00033738938998348605, policy loss: 10.22708857863999
Experience 41, Iter 61, disc loss: 0.00028515534550666846, policy loss: 11.715135999943744
Experience 41, Iter 62, disc loss: 0.00037204702629370574, policy loss: 9.78888031364671
Experience 41, Iter 63, disc loss: 0.0003698741361541859, policy loss: 9.980186079953413
Experience 41, Iter 64, disc loss: 0.0004686802792619311, policy loss: 9.18233779621135
Experience 41, Iter 65, disc loss: 0.00033697510831828706, policy loss: 10.04195768917117
Experience 41, Iter 66, disc loss: 0.00032955141641301613, policy loss: 10.305237105294589
Experience 41, Iter 67, disc loss: 0.0002443402584726901, policy loss: 10.69948455972925
Experience 41, Iter 68, disc loss: 0.00030112309982121595, policy loss: 10.315216461649351
Experience 41, Iter 69, disc loss: 0.00032703499540284037, policy loss: 10.765739127283098
Experience 41, Iter 70, disc loss: 0.00039634139758938766, policy loss: 10.500166869548101
Experience 41, Iter 71, disc loss: 0.0003947140446181897, policy loss: 10.15649494122751
Experience 41, Iter 72, disc loss: 0.000372628046284141, policy loss: 9.677364231368585
Experience 41, Iter 73, disc loss: 0.0003667808290942657, policy loss: 9.946871666950766
Experience 41, Iter 74, disc loss: 0.000308121636358709, policy loss: 9.717832211507606
Experience 41, Iter 75, disc loss: 0.00041639693347342115, policy loss: 10.043068787059708
Experience 41, Iter 76, disc loss: 0.00035906398954128405, policy loss: 9.648588049302651
Experience 41, Iter 77, disc loss: 0.00038637393742104495, policy loss: 10.245185535177718
Experience 41, Iter 78, disc loss: 0.00032044181716023746, policy loss: 11.336221554285322
Experience 41, Iter 79, disc loss: 0.00029080882442525423, policy loss: 10.865571103741953
Experience 41, Iter 80, disc loss: 0.0002882499837725273, policy loss: 10.261543365974166
Experience 41, Iter 81, disc loss: 0.00035397202047990925, policy loss: 9.695710150378028
Experience 41, Iter 82, disc loss: 0.000453150001221744, policy loss: 10.754488522428971
Experience 41, Iter 83, disc loss: 0.0005198024875815636, policy loss: 10.177751751844157
Experience 41, Iter 84, disc loss: 0.0004179690885851486, policy loss: 10.385515610564713
Experience 41, Iter 85, disc loss: 0.00039768109358702, policy loss: 9.558239877977684
Experience 41, Iter 86, disc loss: 0.0003387813432794186, policy loss: 10.40557039621822
Experience 41, Iter 87, disc loss: 0.0003529464787615755, policy loss: 10.692096776901511
Experience 41, Iter 88, disc loss: 0.00044493429801100137, policy loss: 11.159430229845839
Experience 41, Iter 89, disc loss: 0.0004971585377930488, policy loss: 10.412194145864149
Experience 41, Iter 90, disc loss: 0.0003414215043552899, policy loss: 10.471654958775783
Experience 41, Iter 91, disc loss: 0.000288239719961928, policy loss: 10.679857764148116
Experience 41, Iter 92, disc loss: 0.00028020965330742635, policy loss: 9.95333410384199
Experience 41, Iter 93, disc loss: 0.0003503400148008334, policy loss: 10.049446632480377
Experience 41, Iter 94, disc loss: 0.0004001330328645599, policy loss: 10.121682007092529
Experience 41, Iter 95, disc loss: 0.00043394957752749234, policy loss: 9.811509842501522
Experience 41, Iter 96, disc loss: 0.00031203057909897004, policy loss: 10.359993878535317
Experience 41, Iter 97, disc loss: 0.0002900855984264883, policy loss: 10.471230522843177
Experience 41, Iter 98, disc loss: 0.0004159907159098428, policy loss: 10.467495542432959
Experience 41, Iter 99, disc loss: 0.0004354325626010854, policy loss: 10.906079113907815
Experience: 42
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.2329],
        [1.8520],
        [0.0410]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0100, 0.1614, 1.7884, 0.0223, 0.0249, 5.0539]],

        [[0.0100, 0.1614, 1.7884, 0.0223, 0.0249, 5.0539]],

        [[0.0100, 0.1614, 1.7884, 0.0223, 0.0249, 5.0539]],

        [[0.0100, 0.1614, 1.7884, 0.0223, 0.0249, 5.0539]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0143, 0.9317, 7.4079, 0.1640], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0143, 0.9317, 7.4079, 0.1640])
N: 420
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1681.0000, 1681.0000, 1681.0000, 1681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.608
Iter 2/2000 - Loss: 3.708
Iter 3/2000 - Loss: 3.397
Iter 4/2000 - Loss: 3.358
Iter 5/2000 - Loss: 3.360
Iter 6/2000 - Loss: 3.228
Iter 7/2000 - Loss: 3.030
Iter 8/2000 - Loss: 2.840
Iter 9/2000 - Loss: 2.676
Iter 10/2000 - Loss: 2.514
Iter 11/2000 - Loss: 2.323
Iter 12/2000 - Loss: 2.091
Iter 13/2000 - Loss: 1.826
Iter 14/2000 - Loss: 1.542
Iter 15/2000 - Loss: 1.248
Iter 16/2000 - Loss: 0.948
Iter 17/2000 - Loss: 0.641
Iter 18/2000 - Loss: 0.323
Iter 19/2000 - Loss: -0.004
Iter 20/2000 - Loss: -0.339
Iter 1981/2000 - Loss: -8.473
Iter 1982/2000 - Loss: -8.473
Iter 1983/2000 - Loss: -8.473
Iter 1984/2000 - Loss: -8.473
Iter 1985/2000 - Loss: -8.473
Iter 1986/2000 - Loss: -8.473
Iter 1987/2000 - Loss: -8.473
Iter 1988/2000 - Loss: -8.473
Iter 1989/2000 - Loss: -8.473
Iter 1990/2000 - Loss: -8.473
Iter 1991/2000 - Loss: -8.473
Iter 1992/2000 - Loss: -8.473
Iter 1993/2000 - Loss: -8.473
Iter 1994/2000 - Loss: -8.473
Iter 1995/2000 - Loss: -8.473
Iter 1996/2000 - Loss: -8.473
Iter 1997/2000 - Loss: -8.473
Iter 1998/2000 - Loss: -8.474
Iter 1999/2000 - Loss: -8.474
Iter 2000/2000 - Loss: -8.474
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.8117,  5.7136, 28.8538,  1.6404, 18.4266, 54.5380]],

        [[15.7810, 28.4474,  7.6162,  1.7551,  0.6998, 21.4836]],

        [[15.4932, 29.1797,  7.2111,  1.0370,  0.6436, 24.2995]],

        [[12.9752, 23.0566, 15.3127,  1.3565,  1.6998, 46.3621]]])
Signal Variance: tensor([ 0.0865,  1.6342, 13.6198,  0.4730])
Estimated target variance: tensor([0.0143, 0.9317, 7.4079, 0.1640])
N: 420
Signal to noise ratio: tensor([17.5088, 69.7762, 84.5606, 45.0671])
Bound on condition number: tensor([ 128755.3944, 2044864.4328, 3003208.4852,  853040.9457])
Policy Optimizer learning rate:
0.009577219229949994
Experience 42, Iter 0, disc loss: 0.0004815840616918154, policy loss: 10.515404984759767
Experience 42, Iter 1, disc loss: 0.000372395331892421, policy loss: 9.904431284364165
Experience 42, Iter 2, disc loss: 0.0003328938576776456, policy loss: 10.20203613492537
Experience 42, Iter 3, disc loss: 0.0003581002508169966, policy loss: 10.63151425636002
Experience 42, Iter 4, disc loss: 0.0003700829418304789, policy loss: 10.3560272772846
Experience 42, Iter 5, disc loss: 0.0003842166600169523, policy loss: 10.594563897974744
Experience 42, Iter 6, disc loss: 0.00032465059884248797, policy loss: 9.743794013276768
Experience 42, Iter 7, disc loss: 0.00025488300680373647, policy loss: 10.853933996863457
Experience 42, Iter 8, disc loss: 0.0002645123771441681, policy loss: 10.93923575676777
Experience 42, Iter 9, disc loss: 0.0003040660044716165, policy loss: 10.333264425814942
Experience 42, Iter 10, disc loss: 0.00046828641131708005, policy loss: 9.957211135989493
Experience 42, Iter 11, disc loss: 0.0004147274255938511, policy loss: 11.036909852061239
Experience 42, Iter 12, disc loss: 0.00029719857864870874, policy loss: 10.225889357664983
Experience 42, Iter 13, disc loss: 0.0002753804339197435, policy loss: 10.91817979114294
Experience 42, Iter 14, disc loss: 0.00033784499848099034, policy loss: 10.326027948271332
Experience 42, Iter 15, disc loss: 0.0003972203410485081, policy loss: 9.818647499134073
Experience 42, Iter 16, disc loss: 0.000504400632772025, policy loss: 10.153537186813413
Experience 42, Iter 17, disc loss: 0.00051520199062039, policy loss: 10.330790829193052
Experience 42, Iter 18, disc loss: 0.0004033384501429072, policy loss: 10.439360533037181
Experience 42, Iter 19, disc loss: 0.0002399177478995305, policy loss: 10.580585385300669
Experience 42, Iter 20, disc loss: 0.0002821449518122432, policy loss: 10.535616476632969
Experience 42, Iter 21, disc loss: 0.00031747123713439247, policy loss: 10.3835090043263
Experience 42, Iter 22, disc loss: 0.0003310762813892417, policy loss: 10.990374112691248
Experience 42, Iter 23, disc loss: 0.0003903560676250539, policy loss: 11.004335678596576
Experience 42, Iter 24, disc loss: 0.0003572109908289516, policy loss: 9.705701852511009
Experience 42, Iter 25, disc loss: 0.00027613842851528575, policy loss: 11.403190460875603
Experience 42, Iter 26, disc loss: 0.00025938306740152915, policy loss: 10.583451034618836
Experience 42, Iter 27, disc loss: 0.00028487487089310666, policy loss: 10.302008454257484
Experience 42, Iter 28, disc loss: 0.0003375130048674818, policy loss: 10.01252429952227
Experience 42, Iter 29, disc loss: 0.00035681767522486587, policy loss: 11.164619565230211
Experience 42, Iter 30, disc loss: 0.0003838571325082721, policy loss: 10.316041005416963
Experience 42, Iter 31, disc loss: 0.0002294802682801584, policy loss: 10.693980151845075
Experience 42, Iter 32, disc loss: 0.00023611801640585574, policy loss: 12.589624706880274
Experience 42, Iter 33, disc loss: 0.00024687679677625005, policy loss: 11.01628135521191
Experience 42, Iter 34, disc loss: 0.0003187469924045944, policy loss: 10.279186163535455
Experience 42, Iter 35, disc loss: 0.00037667663001403557, policy loss: 11.416214686919286
Experience 42, Iter 36, disc loss: 0.00038421803923054793, policy loss: 10.31655695774029
Experience 42, Iter 37, disc loss: 0.0003771132500286653, policy loss: 9.923144390826316
Experience 42, Iter 38, disc loss: 0.00023634763250436912, policy loss: 10.965769060869313
Experience 42, Iter 39, disc loss: 0.00021920833346794575, policy loss: 11.41031063545012
Experience 42, Iter 40, disc loss: 0.0003306369124919227, policy loss: 10.471789148078313
Experience 42, Iter 41, disc loss: 0.0005240310311158751, policy loss: 10.422212427040893
Experience 42, Iter 42, disc loss: 0.0004438449730967177, policy loss: 9.760361757659997
Experience 42, Iter 43, disc loss: 0.0003915047906006537, policy loss: 10.25567865598119
Experience 42, Iter 44, disc loss: 0.000300344938511689, policy loss: 10.628344219664537
Experience 42, Iter 45, disc loss: 0.00024787129168594275, policy loss: 10.793358435670172
Experience 42, Iter 46, disc loss: 0.000290392719832257, policy loss: 10.702175687425711
Experience 42, Iter 47, disc loss: 0.0003170155428676827, policy loss: 11.402583374256382
Experience 42, Iter 48, disc loss: 0.0003819398168667669, policy loss: 11.417837595119948
Experience 42, Iter 49, disc loss: 0.0003408401724722584, policy loss: 11.626997600083453
Experience 42, Iter 50, disc loss: 0.0003384636554880586, policy loss: 11.276243402610072
Experience 42, Iter 51, disc loss: 0.00024598544486363204, policy loss: 11.234452306457401
Experience 42, Iter 52, disc loss: 0.00020658887849944494, policy loss: 11.28344198733337
Experience 42, Iter 53, disc loss: 0.00021050395079836768, policy loss: 11.717634402335662
Experience 42, Iter 54, disc loss: 0.0002445278466276739, policy loss: 11.829794980723575
Experience 42, Iter 55, disc loss: 0.0003657420115874959, policy loss: 11.502876532981173
Experience 42, Iter 56, disc loss: 0.00031490658199948777, policy loss: 13.28647024309771
Experience 42, Iter 57, disc loss: 0.00031050337864119615, policy loss: 11.801925056533081
Experience 42, Iter 58, disc loss: 0.00022790985622962842, policy loss: 10.915582842608227
Experience 42, Iter 59, disc loss: 0.00016192174251101793, policy loss: 12.89812755449713
Experience 42, Iter 60, disc loss: 0.00016089449670700367, policy loss: 14.455214238647798
Experience 42, Iter 61, disc loss: 0.00018882519231520748, policy loss: 11.838353700287467
Experience 42, Iter 62, disc loss: 0.00029434244058867195, policy loss: 10.630504084519877
Experience 42, Iter 63, disc loss: 0.00040973880645540575, policy loss: 11.300864720565405
Experience 42, Iter 64, disc loss: 0.0005075290063272803, policy loss: 10.821425089866512
Experience 42, Iter 65, disc loss: 0.0004799712979999659, policy loss: 10.026673172103488
Experience 42, Iter 66, disc loss: 0.00025381238104403494, policy loss: 11.295944732228893
Experience 42, Iter 67, disc loss: 0.0002386543238244838, policy loss: 10.886915303510476
Experience 42, Iter 68, disc loss: 0.00025428384273507926, policy loss: 10.18038125274932
Experience 42, Iter 69, disc loss: 0.0004042376694214995, policy loss: 10.222419505071835
Experience 42, Iter 70, disc loss: 0.00026771519313237584, policy loss: 13.807692803798702
Experience 42, Iter 71, disc loss: 0.0003198566137483526, policy loss: 10.688343835020646
Experience 42, Iter 72, disc loss: 0.00023181043644505565, policy loss: 10.583559662213558
Experience 42, Iter 73, disc loss: 0.00017972864464364994, policy loss: 11.817373203247376
Experience 42, Iter 74, disc loss: 0.00021373443704245286, policy loss: 11.058189875879034
Experience 42, Iter 75, disc loss: 0.0003352162066080778, policy loss: 10.132478080577798
Experience 42, Iter 76, disc loss: 0.0002908745685896294, policy loss: 11.904640572987212
Experience 42, Iter 77, disc loss: 0.00037023323377300267, policy loss: 11.896943354649101
Experience 42, Iter 78, disc loss: 0.00045825327243483613, policy loss: 10.952783464697719
Experience 42, Iter 79, disc loss: 0.00026714529277716513, policy loss: 10.156359973839447
Experience 42, Iter 80, disc loss: 0.00018868510184475449, policy loss: 12.227250293598182
Experience 42, Iter 81, disc loss: 0.00027409874720704414, policy loss: 10.696358462920706
Experience 42, Iter 82, disc loss: 0.0003520230873901819, policy loss: 10.59460408066503
Experience 42, Iter 83, disc loss: 0.00040531751693079156, policy loss: 10.384466720052584
Experience 42, Iter 84, disc loss: 0.000346695863989348, policy loss: 12.061474171747221
Experience 42, Iter 85, disc loss: 0.0003500551477623387, policy loss: 10.199290679209845
Experience 42, Iter 86, disc loss: 0.00027223862217193155, policy loss: 9.847230630977055
Experience 42, Iter 87, disc loss: 0.00020144098229674223, policy loss: 10.755743111836999
Experience 42, Iter 88, disc loss: 0.00020403378065370049, policy loss: 10.668854766926438
Experience 42, Iter 89, disc loss: 0.00025257911532195864, policy loss: 11.022823643387659
Experience 42, Iter 90, disc loss: 0.00031072611888381663, policy loss: 10.717867659315655
Experience 42, Iter 91, disc loss: 0.0002890318554943276, policy loss: 10.479306368371034
Experience 42, Iter 92, disc loss: 0.00022286367761184913, policy loss: 11.160993297743378
Experience 42, Iter 93, disc loss: 0.00020297092347210018, policy loss: 10.931485372133551
Experience 42, Iter 94, disc loss: 0.0002900902554570128, policy loss: 10.760610618001305
Experience 42, Iter 95, disc loss: 0.00035471286283929977, policy loss: 9.736150580146543
Experience 42, Iter 96, disc loss: 0.00044619246970034886, policy loss: 11.603253076650503
Experience 42, Iter 97, disc loss: 0.0004346630406950641, policy loss: 10.339000019026773
Experience 42, Iter 98, disc loss: 0.0002593000928328301, policy loss: 11.11740286540184
Experience 42, Iter 99, disc loss: 0.000211181052671334, policy loss: 10.788625029832838
Experience: 43
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.2379],
        [1.8799],
        [0.0420]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0099, 0.1634, 1.8309, 0.0227, 0.0254, 5.1505]],

        [[0.0099, 0.1634, 1.8309, 0.0227, 0.0254, 5.1505]],

        [[0.0099, 0.1634, 1.8309, 0.0227, 0.0254, 5.1505]],

        [[0.0099, 0.1634, 1.8309, 0.0227, 0.0254, 5.1505]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0145, 0.9517, 7.5196, 0.1680], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0145, 0.9517, 7.5196, 0.1680])
N: 430
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1721.0000, 1721.0000, 1721.0000, 1721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.636
Iter 2/2000 - Loss: 3.734
Iter 3/2000 - Loss: 3.420
Iter 4/2000 - Loss: 3.377
Iter 5/2000 - Loss: 3.377
Iter 6/2000 - Loss: 3.245
Iter 7/2000 - Loss: 3.046
Iter 8/2000 - Loss: 2.854
Iter 9/2000 - Loss: 2.687
Iter 10/2000 - Loss: 2.522
Iter 11/2000 - Loss: 2.329
Iter 12/2000 - Loss: 2.094
Iter 13/2000 - Loss: 1.826
Iter 14/2000 - Loss: 1.538
Iter 15/2000 - Loss: 1.240
Iter 16/2000 - Loss: 0.935
Iter 17/2000 - Loss: 0.622
Iter 18/2000 - Loss: 0.299
Iter 19/2000 - Loss: -0.032
Iter 20/2000 - Loss: -0.369
Iter 1981/2000 - Loss: -8.486
Iter 1982/2000 - Loss: -8.486
Iter 1983/2000 - Loss: -8.486
Iter 1984/2000 - Loss: -8.486
Iter 1985/2000 - Loss: -8.486
Iter 1986/2000 - Loss: -8.486
Iter 1987/2000 - Loss: -8.486
Iter 1988/2000 - Loss: -8.486
Iter 1989/2000 - Loss: -8.486
Iter 1990/2000 - Loss: -8.486
Iter 1991/2000 - Loss: -8.486
Iter 1992/2000 - Loss: -8.486
Iter 1993/2000 - Loss: -8.486
Iter 1994/2000 - Loss: -8.486
Iter 1995/2000 - Loss: -8.486
Iter 1996/2000 - Loss: -8.487
Iter 1997/2000 - Loss: -8.487
Iter 1998/2000 - Loss: -8.487
Iter 1999/2000 - Loss: -8.487
Iter 2000/2000 - Loss: -8.487
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.8011,  5.3696, 31.8900,  1.6085, 18.2503, 54.5604]],

        [[15.4759, 28.1767,  7.4467,  1.7164,  0.7145, 21.6433]],

        [[15.1499, 28.9908,  7.1595,  1.0450,  0.6442, 23.8798]],

        [[12.8292, 22.6260, 14.6947,  1.3736,  1.6270, 45.7167]]])
Signal Variance: tensor([ 0.0830,  1.6173, 13.4597,  0.4410])
Estimated target variance: tensor([0.0145, 0.9517, 7.5196, 0.1680])
N: 430
Signal to noise ratio: tensor([17.1698, 69.8811, 83.8241, 43.0570])
Bound on condition number: tensor([ 126765.9203, 2099848.3000, 3021384.3272,  797181.3314])
Policy Optimizer learning rate:
0.00956713393627531
Experience 43, Iter 0, disc loss: 0.00020580520822806298, policy loss: 11.278625378620811
Experience 43, Iter 1, disc loss: 0.00027300726296375453, policy loss: 10.541667119216633
Experience 43, Iter 2, disc loss: 0.0002913349275445768, policy loss: 10.619775678685905
Experience 43, Iter 3, disc loss: 0.00038011503201143646, policy loss: 10.38409207002626
Experience 43, Iter 4, disc loss: 0.00034570826106490235, policy loss: 10.041368111858823
Experience 43, Iter 5, disc loss: 0.00034047079976532005, policy loss: 10.168977357833938
Experience 43, Iter 6, disc loss: 0.0002577537298537959, policy loss: 10.669980704686846
Experience 43, Iter 7, disc loss: 0.0002409630002776114, policy loss: 10.06450574708659
Experience 43, Iter 8, disc loss: 0.00029404723140819444, policy loss: 10.367851889276771
Experience 43, Iter 9, disc loss: 0.00030611728449077226, policy loss: 9.967324186580075
Experience 43, Iter 10, disc loss: 0.0003561071789683987, policy loss: 10.022066353451043
Experience 43, Iter 11, disc loss: 0.00033526894174021283, policy loss: 10.029331973305423
Experience 43, Iter 12, disc loss: 0.00031978216182181263, policy loss: 10.298003185959182
Experience 43, Iter 13, disc loss: 0.00034063777319222316, policy loss: 9.94237668527281
Experience 43, Iter 14, disc loss: 0.0003940517266958512, policy loss: 9.647601555441174
Experience 43, Iter 15, disc loss: 0.000361428051176102, policy loss: 9.802249451785704
Experience 43, Iter 16, disc loss: 0.0002668058824844928, policy loss: 9.989963754115657
Experience 43, Iter 17, disc loss: 0.00033618128275823427, policy loss: 11.039822110967567
Experience 43, Iter 18, disc loss: 0.00029777673467313963, policy loss: 10.285747326346884
Experience 43, Iter 19, disc loss: 0.00034008918785332417, policy loss: 9.723288449800812
Experience 43, Iter 20, disc loss: 0.0003524848922047606, policy loss: 10.257956848506332
Experience 43, Iter 21, disc loss: 0.0003594110573187674, policy loss: 10.28284761454756
Experience 43, Iter 22, disc loss: 0.0002729466157764074, policy loss: 10.530050977258796
Experience 43, Iter 23, disc loss: 0.00021732681194193488, policy loss: 10.845279946665974
Experience 43, Iter 24, disc loss: 0.00024947523129591046, policy loss: 10.257561080233867
Experience 43, Iter 25, disc loss: 0.00031599459828807435, policy loss: 10.583399313169117
Experience 43, Iter 26, disc loss: 0.00029772310837209153, policy loss: 11.028872360298806
Experience 43, Iter 27, disc loss: 0.00025514687281337495, policy loss: 11.025741291095114
Experience 43, Iter 28, disc loss: 0.00020466089070513422, policy loss: 10.905807089290317
Experience 43, Iter 29, disc loss: 0.00017734262298473664, policy loss: 11.143488543781768
Experience 43, Iter 30, disc loss: 0.00020560579180898083, policy loss: 11.744992290289053
Experience 43, Iter 31, disc loss: 0.0003056785837363941, policy loss: 9.737594919041165
Experience 43, Iter 32, disc loss: 0.0003520943660149492, policy loss: 11.19812778919897
Experience 43, Iter 33, disc loss: 0.0003948670377548494, policy loss: 10.915110785321634
Experience 43, Iter 34, disc loss: 0.0003238934858522961, policy loss: 10.514988090314217
Experience 43, Iter 35, disc loss: 0.0002350425346702661, policy loss: 10.986736889160339
Experience 43, Iter 36, disc loss: 0.000195098670140887, policy loss: 11.315206183300308
Experience 43, Iter 37, disc loss: 0.0002516633311980961, policy loss: 10.153277721917963
Experience 43, Iter 38, disc loss: 0.00035429743646305325, policy loss: 10.154437845638508
Experience 43, Iter 39, disc loss: 0.0002757400597491373, policy loss: 11.455549722020958
Experience 43, Iter 40, disc loss: 0.00034302821209581855, policy loss: 10.651412112413965
Experience 43, Iter 41, disc loss: 0.00021901894740289804, policy loss: 10.943018958066755
Experience 43, Iter 42, disc loss: 0.00021439351838568158, policy loss: 10.609309367163124
Experience 43, Iter 43, disc loss: 0.00020671869382139532, policy loss: 11.159536958548152
Experience 43, Iter 44, disc loss: 0.00028985897945794844, policy loss: 10.48575716938755
Experience 43, Iter 45, disc loss: 0.0002571629572457596, policy loss: 10.630558091413212
Experience 43, Iter 46, disc loss: 0.00023651794537409362, policy loss: 10.59221172404379
Experience 43, Iter 47, disc loss: 0.00021017899184915814, policy loss: 10.816052248829505
Experience 43, Iter 48, disc loss: 0.00018265374431410667, policy loss: 11.311610093965554
Experience 43, Iter 49, disc loss: 0.00020031651660919523, policy loss: 10.601372939631657
Experience 43, Iter 50, disc loss: 0.00032449666183417147, policy loss: 9.90246349850811
Experience 43, Iter 51, disc loss: 0.00031726728361753797, policy loss: 10.809690373329353
Experience 43, Iter 52, disc loss: 0.00032640367038293055, policy loss: 9.787923394801178
Experience 43, Iter 53, disc loss: 0.0002773705742031726, policy loss: 9.881522395483422
Experience 43, Iter 54, disc loss: 0.00027360511194707727, policy loss: 10.316634579354306
Experience 43, Iter 55, disc loss: 0.00025339891726620305, policy loss: 10.302004276985171
Experience 43, Iter 56, disc loss: 0.00030682309988508264, policy loss: 11.063126005797912
Experience 43, Iter 57, disc loss: 0.0003102940299044188, policy loss: 11.108471284989228
Experience 43, Iter 58, disc loss: 0.0004260480828620387, policy loss: 10.036199493054408
Experience 43, Iter 59, disc loss: 0.0002889155584827063, policy loss: 9.756304878103146
Experience 43, Iter 60, disc loss: 0.0002960240501439287, policy loss: 10.604065717328798
Experience 43, Iter 61, disc loss: 0.00026874467755799177, policy loss: 10.520525601408771
Experience 43, Iter 62, disc loss: 0.0003010798129024473, policy loss: 9.548691779822114
Experience 43, Iter 63, disc loss: 0.0003337997968311436, policy loss: 9.734582605285608
Experience 43, Iter 64, disc loss: 0.00027273665700903765, policy loss: 11.520261013801557
Experience 43, Iter 65, disc loss: 0.00023655427280032065, policy loss: 10.776267597942013
Experience 43, Iter 66, disc loss: 0.00017545146436230146, policy loss: 11.113687491457872
Experience 43, Iter 67, disc loss: 0.00017548890114271623, policy loss: 11.41239273071596
Experience 43, Iter 68, disc loss: 0.00021848819222534445, policy loss: 10.32918609814643
Experience 43, Iter 69, disc loss: 0.0002745643975805366, policy loss: 10.934309353605798
Experience 43, Iter 70, disc loss: 0.0003446859740562719, policy loss: 10.538747621150039
Experience 43, Iter 71, disc loss: 0.000281337857212293, policy loss: 10.885779421278599
Experience 43, Iter 72, disc loss: 0.0002839000228924709, policy loss: 10.84712910712295
Experience 43, Iter 73, disc loss: 0.0002956044851135908, policy loss: 10.39226816702814
Experience 43, Iter 74, disc loss: 0.00034119389603244805, policy loss: 10.751634577474583
Experience 43, Iter 75, disc loss: 0.00031465930641198477, policy loss: 10.583620287809715
Experience 43, Iter 76, disc loss: 0.00046442391823352475, policy loss: 10.968048040175702
Experience 43, Iter 77, disc loss: 0.00038035605701941836, policy loss: 10.939863418574179
Experience 43, Iter 78, disc loss: 0.0002786229196559414, policy loss: 10.271595309436833
Experience 43, Iter 79, disc loss: 0.00029615490815138143, policy loss: 9.975486970323377
Experience 43, Iter 80, disc loss: 0.00029818974889194566, policy loss: 9.912106817016916
Experience 43, Iter 81, disc loss: 0.00028091805084962465, policy loss: 10.304770552329982
Experience 43, Iter 82, disc loss: 0.00021682701283579187, policy loss: 10.440761343400672
Experience 43, Iter 83, disc loss: 0.00021008977962213812, policy loss: 10.659962991597215
Experience 43, Iter 84, disc loss: 0.0002591711575969966, policy loss: 10.577387730870415
Experience 43, Iter 85, disc loss: 0.0002744927316440069, policy loss: 10.358601520980796
Experience 43, Iter 86, disc loss: 0.0002525188013854433, policy loss: 10.826619105775695
Experience 43, Iter 87, disc loss: 0.00030069533726234947, policy loss: 10.52250824587033
Experience 43, Iter 88, disc loss: 0.00028600992085878044, policy loss: 10.230419161870524
Experience 43, Iter 89, disc loss: 0.0001733617635389321, policy loss: 11.287239831312082
Experience 43, Iter 90, disc loss: 0.00020594741990370246, policy loss: 10.798017464032029
Experience 43, Iter 91, disc loss: 0.0002559045995286328, policy loss: 10.820890165689356
Experience 43, Iter 92, disc loss: 0.00039850008963121216, policy loss: 11.557933329105213
Experience 43, Iter 93, disc loss: 0.00034170560194569444, policy loss: 10.897554215821357
Experience 43, Iter 94, disc loss: 0.00022680290463190084, policy loss: 10.450760144597144
Experience 43, Iter 95, disc loss: 0.0001888728711427774, policy loss: 11.736628902538609
Experience 43, Iter 96, disc loss: 0.00021653654836984655, policy loss: 11.15861001974465
Experience 43, Iter 97, disc loss: 0.00030720270877472144, policy loss: 11.248311904911974
Experience 43, Iter 98, disc loss: 0.00027515289758702486, policy loss: 10.774925746534906
Experience 43, Iter 99, disc loss: 0.0002626713049604, policy loss: 10.572102134220113
Experience: 44
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.2423],
        [1.9075],
        [0.0425]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0098, 0.1653, 1.8618, 0.0232, 0.0256, 5.2370]],

        [[0.0098, 0.1653, 1.8618, 0.0232, 0.0256, 5.2370]],

        [[0.0098, 0.1653, 1.8618, 0.0232, 0.0256, 5.2370]],

        [[0.0098, 0.1653, 1.8618, 0.0232, 0.0256, 5.2370]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0146, 0.9693, 7.6300, 0.1702], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0146, 0.9693, 7.6300, 0.1702])
N: 440
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1761.0000, 1761.0000, 1761.0000, 1761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.662
Iter 2/2000 - Loss: 3.755
Iter 3/2000 - Loss: 3.444
Iter 4/2000 - Loss: 3.400
Iter 5/2000 - Loss: 3.398
Iter 6/2000 - Loss: 3.266
Iter 7/2000 - Loss: 3.068
Iter 8/2000 - Loss: 2.876
Iter 9/2000 - Loss: 2.708
Iter 10/2000 - Loss: 2.541
Iter 11/2000 - Loss: 2.345
Iter 12/2000 - Loss: 2.108
Iter 13/2000 - Loss: 1.837
Iter 14/2000 - Loss: 1.546
Iter 15/2000 - Loss: 1.244
Iter 16/2000 - Loss: 0.935
Iter 17/2000 - Loss: 0.618
Iter 18/2000 - Loss: 0.291
Iter 19/2000 - Loss: -0.044
Iter 20/2000 - Loss: -0.385
Iter 1981/2000 - Loss: -8.503
Iter 1982/2000 - Loss: -8.503
Iter 1983/2000 - Loss: -8.503
Iter 1984/2000 - Loss: -8.503
Iter 1985/2000 - Loss: -8.503
Iter 1986/2000 - Loss: -8.503
Iter 1987/2000 - Loss: -8.503
Iter 1988/2000 - Loss: -8.503
Iter 1989/2000 - Loss: -8.503
Iter 1990/2000 - Loss: -8.503
Iter 1991/2000 - Loss: -8.503
Iter 1992/2000 - Loss: -8.503
Iter 1993/2000 - Loss: -8.503
Iter 1994/2000 - Loss: -8.503
Iter 1995/2000 - Loss: -8.504
Iter 1996/2000 - Loss: -8.504
Iter 1997/2000 - Loss: -8.504
Iter 1998/2000 - Loss: -8.504
Iter 1999/2000 - Loss: -8.504
Iter 2000/2000 - Loss: -8.504
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.5943,  5.7019, 31.5142,  1.6315, 18.1092, 54.9191]],

        [[15.4132, 28.2353,  7.5584,  1.7504,  0.7228, 22.5393]],

        [[15.1846, 29.0072,  7.4087,  1.0075,  0.6350, 24.1084]],

        [[12.7322, 23.4106, 14.0082,  1.4091,  1.5616, 45.0080]]])
Signal Variance: tensor([ 0.0857,  1.7085, 13.4304,  0.4081])
Estimated target variance: tensor([0.0146, 0.9693, 7.6300, 0.1702])
N: 440
Signal to noise ratio: tensor([17.5784, 71.3641, 82.9909, 41.2008])
Bound on condition number: tensor([ 135960.4925, 2240845.6671, 3030493.8814,  746905.2755])
Policy Optimizer learning rate:
0.00955705926292225
Experience 44, Iter 0, disc loss: 0.0002667265773049902, policy loss: 10.407261196430376
Experience 44, Iter 1, disc loss: 0.0002547534181403719, policy loss: 10.170291987782598
Experience 44, Iter 2, disc loss: 0.00024059459870555058, policy loss: 10.803681455199484
Experience 44, Iter 3, disc loss: 0.0002627247423355593, policy loss: 10.477433100522063
Experience 44, Iter 4, disc loss: 0.0003047776197131119, policy loss: 10.034172065086945
Experience 44, Iter 5, disc loss: 0.00037294595825134824, policy loss: 10.468097608581463
Experience 44, Iter 6, disc loss: 0.0003643924833793804, policy loss: 9.528507471255542
Experience 44, Iter 7, disc loss: 0.00035525014291721137, policy loss: 9.874776297797023
Experience 44, Iter 8, disc loss: 0.00038952315927971845, policy loss: 10.239646728091936
Experience 44, Iter 9, disc loss: 0.00023735448126197483, policy loss: 11.041551497171657
Experience 44, Iter 10, disc loss: 0.000222262467741954, policy loss: 10.589720504975608
Experience 44, Iter 11, disc loss: 0.00025148446607914225, policy loss: 10.629960518618786
Experience 44, Iter 12, disc loss: 0.000365316785429217, policy loss: 10.139440611911454
Experience 44, Iter 13, disc loss: 0.0003823494034087714, policy loss: 10.775929003592758
Experience 44, Iter 14, disc loss: 0.0003040631491241732, policy loss: 10.546418934969946
Experience 44, Iter 15, disc loss: 0.0002456171933229781, policy loss: 10.77396389994985
Experience 44, Iter 16, disc loss: 0.00022569782643690433, policy loss: 11.286637600216643
Experience 44, Iter 17, disc loss: 0.000187707956074157, policy loss: 11.723533028203798
Experience 44, Iter 18, disc loss: 0.00029522497371429075, policy loss: 9.563605333817113
Experience 44, Iter 19, disc loss: 0.0002712440877086317, policy loss: 11.215404766082413
Experience 44, Iter 20, disc loss: 0.00031560699236875104, policy loss: 12.19373824817088
Experience 44, Iter 21, disc loss: 0.00021595403706271737, policy loss: 12.223918063796155
Experience 44, Iter 22, disc loss: 0.00017707560922959622, policy loss: 11.502957634328798
Experience 44, Iter 23, disc loss: 0.00017941193729506335, policy loss: 11.049572703826744
Experience 44, Iter 24, disc loss: 0.00024441599765799734, policy loss: 10.775939412568725
Experience 44, Iter 25, disc loss: 0.0003850270153468142, policy loss: 10.000470833767093
Experience 44, Iter 26, disc loss: 0.0003900528180985877, policy loss: 10.945756559529789
Experience 44, Iter 27, disc loss: 0.0003859547388970734, policy loss: 10.699692935118108
Experience 44, Iter 28, disc loss: 0.00018322893298359572, policy loss: 12.237928431307322
Experience 44, Iter 29, disc loss: 0.00016513048056146601, policy loss: 12.931145562169338
Experience 44, Iter 30, disc loss: 0.00022121160330874576, policy loss: 11.230277559030167
Experience 44, Iter 31, disc loss: 0.00035011584540899917, policy loss: 9.978139434248586
Experience 44, Iter 32, disc loss: 0.0003487842440270515, policy loss: 12.340913544907913
Experience 44, Iter 33, disc loss: 0.00025830753485185225, policy loss: 10.504082130687188
Experience 44, Iter 34, disc loss: 0.000214392452697962, policy loss: 11.263904906003855
Experience 44, Iter 35, disc loss: 0.00016775150949931906, policy loss: 11.712568836749622
Experience 44, Iter 36, disc loss: 0.00018866731087878398, policy loss: 11.715410707465622
Experience 44, Iter 37, disc loss: 0.00025759126573023496, policy loss: 10.319790201393264
Experience 44, Iter 38, disc loss: 0.00025626719456567587, policy loss: 11.164994784072356
Experience 44, Iter 39, disc loss: 0.00023452484377794764, policy loss: 12.459526932064971
Experience 44, Iter 40, disc loss: 0.00027117618273777395, policy loss: 10.377619470787778
Experience 44, Iter 41, disc loss: 0.00017527397522022842, policy loss: 11.703528466843206
Experience 44, Iter 42, disc loss: 0.00019186522437186676, policy loss: 10.758534433547103
Experience 44, Iter 43, disc loss: 0.0002513663222212355, policy loss: 10.938260268875073
Experience 44, Iter 44, disc loss: 0.0002906590214816657, policy loss: 10.704432204520668
Experience 44, Iter 45, disc loss: 0.0003037706233562686, policy loss: 11.9951201049997
Experience 44, Iter 46, disc loss: 0.00030093768821949163, policy loss: 10.863979525542568
Experience 44, Iter 47, disc loss: 0.00019893526091889304, policy loss: 10.656488111349923
Experience 44, Iter 48, disc loss: 0.0002228175952953177, policy loss: 10.663555251058344
Experience 44, Iter 49, disc loss: 0.00030247339968079355, policy loss: 10.237517941033072
Experience 44, Iter 50, disc loss: 0.00030406482513534107, policy loss: 10.682869043975849
Experience 44, Iter 51, disc loss: 0.0002905052840249772, policy loss: 10.383284938412281
Experience 44, Iter 52, disc loss: 0.0002405423125366322, policy loss: 10.343644800062318
Experience 44, Iter 53, disc loss: 0.0002189918646724877, policy loss: 10.262621721904729
Experience 44, Iter 54, disc loss: 0.0002680323167627064, policy loss: 10.656902941685884
Experience 44, Iter 55, disc loss: 0.0002258434634070972, policy loss: 12.472990759503165
Experience 44, Iter 56, disc loss: 0.00024291278758049924, policy loss: 11.050491977641059
Experience 44, Iter 57, disc loss: 0.00023168110479281531, policy loss: 10.692407670141906
Experience 44, Iter 58, disc loss: 0.00023513410836641777, policy loss: 10.195557872988644
Experience 44, Iter 59, disc loss: 0.00023437412195977168, policy loss: 10.581917973104524
Experience 44, Iter 60, disc loss: 0.0002755098349405498, policy loss: 10.798170978422764
Experience 44, Iter 61, disc loss: 0.00028168420071482826, policy loss: 11.989348441654975
Experience 44, Iter 62, disc loss: 0.0002803208081393727, policy loss: 11.025915168526087
Experience 44, Iter 63, disc loss: 0.0002035320056609373, policy loss: 10.669166953738664
Experience 44, Iter 64, disc loss: 0.000151825576177213, policy loss: 12.275368854663265
Experience 44, Iter 65, disc loss: 0.0001857372086504516, policy loss: 10.619203922853043
Experience 44, Iter 66, disc loss: 0.0002571721359158236, policy loss: 10.622008763225354
Experience 44, Iter 67, disc loss: 0.00028121187455704664, policy loss: 11.216220863025827
Experience 44, Iter 68, disc loss: 0.00025086000071179363, policy loss: 12.284423217216458
Experience 44, Iter 69, disc loss: 0.00031523738762537733, policy loss: 11.02760824303545
Experience 44, Iter 70, disc loss: 0.00025165144383045484, policy loss: 11.370258094203432
Experience 44, Iter 71, disc loss: 0.00022808131981723895, policy loss: 10.97007371387075
Experience 44, Iter 72, disc loss: 0.00023736175022743422, policy loss: 10.533362943974776
Experience 44, Iter 73, disc loss: 0.00032710833279812646, policy loss: 10.738191870220419
Experience 44, Iter 74, disc loss: 0.000361116571204833, policy loss: 11.14772533474115
Experience 44, Iter 75, disc loss: 0.0002761724601095629, policy loss: 10.732993027049728
Experience 44, Iter 76, disc loss: 0.000228709687124934, policy loss: 10.30380382297416
Experience 44, Iter 77, disc loss: 0.00021754465241811822, policy loss: 10.308521719918545
Experience 44, Iter 78, disc loss: 0.00022949814000785827, policy loss: 10.488179817578654
Experience 44, Iter 79, disc loss: 0.00023886694498036112, policy loss: 10.972159671086121
Experience 44, Iter 80, disc loss: 0.0002796529682583505, policy loss: 10.15448119683234
Experience 44, Iter 81, disc loss: 0.0002528568521171893, policy loss: 10.544366583875352
Experience 44, Iter 82, disc loss: 0.0002574373176337052, policy loss: 10.334253084664109
Experience 44, Iter 83, disc loss: 0.000315415690367704, policy loss: 9.959326186853625
Experience 44, Iter 84, disc loss: 0.0003431230135461291, policy loss: 11.18626610254156
Experience 44, Iter 85, disc loss: 0.0003156089201519899, policy loss: 10.933255911224006
Experience 44, Iter 86, disc loss: 0.00029683002156539085, policy loss: 10.226139026875114
Experience 44, Iter 87, disc loss: 0.0001999900826964983, policy loss: 11.017682901219889
Experience 44, Iter 88, disc loss: 0.00021611223691541997, policy loss: 10.660036784733261
Experience 44, Iter 89, disc loss: 0.00022519509433595702, policy loss: 10.199101149443324
Experience 44, Iter 90, disc loss: 0.0003193720206898529, policy loss: 10.44395576686991
Experience 44, Iter 91, disc loss: 0.0002973242913188204, policy loss: 11.819882773284672
Experience 44, Iter 92, disc loss: 0.0003315022631444068, policy loss: 10.90140354131741
Experience 44, Iter 93, disc loss: 0.00017631052225825742, policy loss: 10.533197963671888
Experience 44, Iter 94, disc loss: 0.00016935234662488437, policy loss: 11.35693818358137
Experience 44, Iter 95, disc loss: 0.00016844515978034342, policy loss: 11.40783734893433
Experience 44, Iter 96, disc loss: 0.00017279071967624007, policy loss: 11.41398533319821
Experience 44, Iter 97, disc loss: 0.00026389540644085384, policy loss: 10.561233703501035
Experience 44, Iter 98, disc loss: 0.00025747038211575965, policy loss: 10.31487568614216
Experience 44, Iter 99, disc loss: 0.00024292902246989415, policy loss: 11.85842300003015
Experience: 45
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.2465],
        [1.9338],
        [0.0432]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0097, 0.1674, 1.8921, 0.0236, 0.0258, 5.3187]],

        [[0.0097, 0.1674, 1.8921, 0.0236, 0.0258, 5.3187]],

        [[0.0097, 0.1674, 1.8921, 0.0236, 0.0258, 5.3187]],

        [[0.0097, 0.1674, 1.8921, 0.0236, 0.0258, 5.3187]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0148, 0.9862, 7.7352, 0.1726], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0148, 0.9862, 7.7352, 0.1726])
N: 450
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1801.0000, 1801.0000, 1801.0000, 1801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.688
Iter 2/2000 - Loss: 3.771
Iter 3/2000 - Loss: 3.464
Iter 4/2000 - Loss: 3.416
Iter 5/2000 - Loss: 3.407
Iter 6/2000 - Loss: 3.272
Iter 7/2000 - Loss: 3.071
Iter 8/2000 - Loss: 2.876
Iter 9/2000 - Loss: 2.704
Iter 10/2000 - Loss: 2.530
Iter 11/2000 - Loss: 2.328
Iter 12/2000 - Loss: 2.085
Iter 13/2000 - Loss: 1.810
Iter 14/2000 - Loss: 1.516
Iter 15/2000 - Loss: 1.211
Iter 16/2000 - Loss: 0.900
Iter 17/2000 - Loss: 0.580
Iter 18/2000 - Loss: 0.251
Iter 19/2000 - Loss: -0.084
Iter 20/2000 - Loss: -0.425
Iter 1981/2000 - Loss: -8.502
Iter 1982/2000 - Loss: -8.502
Iter 1983/2000 - Loss: -8.502
Iter 1984/2000 - Loss: -8.502
Iter 1985/2000 - Loss: -8.502
Iter 1986/2000 - Loss: -8.502
Iter 1987/2000 - Loss: -8.502
Iter 1988/2000 - Loss: -8.502
Iter 1989/2000 - Loss: -8.502
Iter 1990/2000 - Loss: -8.502
Iter 1991/2000 - Loss: -8.502
Iter 1992/2000 - Loss: -8.502
Iter 1993/2000 - Loss: -8.502
Iter 1994/2000 - Loss: -8.502
Iter 1995/2000 - Loss: -8.503
Iter 1996/2000 - Loss: -8.503
Iter 1997/2000 - Loss: -8.503
Iter 1998/2000 - Loss: -8.503
Iter 1999/2000 - Loss: -8.503
Iter 2000/2000 - Loss: -8.503
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[ 9.7335,  4.9974, 33.4787,  1.5659, 17.4364, 54.9970]],

        [[15.3556, 27.7347,  7.3680,  1.7358,  0.7270, 22.0659]],

        [[14.9888, 28.7397,  7.4643,  0.9928,  0.6186, 23.6530]],

        [[12.5592, 23.9384, 14.1781,  1.3421,  1.6415, 44.4757]]])
Signal Variance: tensor([ 0.0775,  1.6415, 12.8281,  0.4140])
Estimated target variance: tensor([0.0148, 0.9862, 7.7352, 0.1726])
N: 450
Signal to noise ratio: tensor([16.7734, 70.5307, 79.4196, 41.1326])
Bound on condition number: tensor([ 126606.8286, 2238562.2573, 2838366.1287,  761352.0417])
Policy Optimizer learning rate:
0.009546995198707083
Experience 45, Iter 0, disc loss: 0.00022750590143889247, policy loss: 10.717097755851869
Experience 45, Iter 1, disc loss: 0.00017120078784542148, policy loss: 11.047272145962076
Experience 45, Iter 2, disc loss: 0.00014146958311979493, policy loss: 12.160650506263949
Experience 45, Iter 3, disc loss: 0.00018756786171000853, policy loss: 11.479338465215328
Experience 45, Iter 4, disc loss: 0.0003153711924005957, policy loss: 10.195076058587096
Experience 45, Iter 5, disc loss: 0.00025238079415522473, policy loss: 12.1210680648344
Experience 45, Iter 6, disc loss: 0.0002622207709115907, policy loss: 10.674816375453737
Experience 45, Iter 7, disc loss: 0.000230953205047259, policy loss: 11.105946224684974
Experience 45, Iter 8, disc loss: 0.000295250345971308, policy loss: 10.76687768197055
Experience 45, Iter 9, disc loss: 0.00026302885281029325, policy loss: 11.159947595000393
Experience 45, Iter 10, disc loss: 0.00029136151696593877, policy loss: 11.433461441303155
Experience 45, Iter 11, disc loss: 0.00032894494617713476, policy loss: 10.255879967432252
Experience 45, Iter 12, disc loss: 0.0002998908925145458, policy loss: 10.745746925569069
Experience 45, Iter 13, disc loss: 0.00021441667626304623, policy loss: 10.987482093559377
Experience 45, Iter 14, disc loss: 0.00023229855508645768, policy loss: 9.956793090005448
Experience 45, Iter 15, disc loss: 0.0003311912148214834, policy loss: 9.895200723309225
Experience 45, Iter 16, disc loss: 0.00028822340312596815, policy loss: 9.897572403219153
Experience 45, Iter 17, disc loss: 0.0002595940831274301, policy loss: 10.463960148583066
Experience 45, Iter 18, disc loss: 0.00026209974197687036, policy loss: 9.994806877989928
Experience 45, Iter 19, disc loss: 0.000263449033378886, policy loss: 10.101382876710016
Experience 45, Iter 20, disc loss: 0.0002041661209753508, policy loss: 11.014717986992055
Experience 45, Iter 21, disc loss: 0.0002010824227938988, policy loss: 10.432886899009024
Experience 45, Iter 22, disc loss: 0.00017631973802273475, policy loss: 11.419723093920153
Experience 45, Iter 23, disc loss: 0.0002350722903370878, policy loss: 10.238153391142022
Experience 45, Iter 24, disc loss: 0.00025537851562202176, policy loss: 11.138809212787057
Experience 45, Iter 25, disc loss: 0.00032858100327239076, policy loss: 10.727118369970983
Experience 45, Iter 26, disc loss: 0.0002711777457351039, policy loss: 10.86363978579814
Experience 45, Iter 27, disc loss: 0.00022878212369598812, policy loss: 10.950982509963202
Experience 45, Iter 28, disc loss: 0.00021914591426049944, policy loss: 11.0228827484563
Experience 45, Iter 29, disc loss: 0.00031497984984655273, policy loss: 10.840414997152228
Experience 45, Iter 30, disc loss: 0.0003008850383996081, policy loss: 10.496658348752383
Experience 45, Iter 31, disc loss: 0.000310141350086033, policy loss: 11.11269664612357
Experience 45, Iter 32, disc loss: 0.00029505768721107596, policy loss: 10.800192984864045
Experience 45, Iter 33, disc loss: 0.0002264548254750649, policy loss: 10.177937147013342
Experience 45, Iter 34, disc loss: 0.00018967922971476018, policy loss: 11.989013390178496
Experience 45, Iter 35, disc loss: 0.0002133040699130061, policy loss: 10.640664580928531
Experience 45, Iter 36, disc loss: 0.0002710769859242386, policy loss: 9.816289509877414
Experience 45, Iter 37, disc loss: 0.0002939933892187372, policy loss: 11.436970072124408
Experience 45, Iter 38, disc loss: 0.00026707854900054855, policy loss: 11.006003751910576
Experience 45, Iter 39, disc loss: 0.00026751237826483853, policy loss: 11.08815153016707
Experience 45, Iter 40, disc loss: 0.00023191727356772535, policy loss: 10.113819805180068
Experience 45, Iter 41, disc loss: 0.00021907099727313926, policy loss: 10.452760706080369
Experience 45, Iter 42, disc loss: 0.00024333466494519702, policy loss: 11.147876129108216
Experience 45, Iter 43, disc loss: 0.0002857682208722695, policy loss: 10.743407862846706
Experience 45, Iter 44, disc loss: 0.00022000143127224448, policy loss: 10.579088571691004
Experience 45, Iter 45, disc loss: 0.0002468197482874796, policy loss: 10.21429420600828
Experience 45, Iter 46, disc loss: 0.00025839060369300883, policy loss: 10.193405634870395
Experience 45, Iter 47, disc loss: 0.00026289986806837145, policy loss: 9.969775732884315
Experience 45, Iter 48, disc loss: 0.00026466386035102073, policy loss: 10.832481489470577
Experience 45, Iter 49, disc loss: 0.0002584362222004104, policy loss: 10.33379891584995
Experience 45, Iter 50, disc loss: 0.0002012060403902929, policy loss: 10.305512142550798
Experience 45, Iter 51, disc loss: 0.0002429503747449716, policy loss: 10.293464962472214
Experience 45, Iter 52, disc loss: 0.0002557808006838094, policy loss: 9.741056372552881
Experience 45, Iter 53, disc loss: 0.00032248266392610135, policy loss: 10.46125841174177
Experience 45, Iter 54, disc loss: 0.0002981054116267197, policy loss: 11.669361347489383
Experience 45, Iter 55, disc loss: 0.00021055375316153928, policy loss: 10.89565404633477
Experience 45, Iter 56, disc loss: 0.00020245198719865006, policy loss: 10.578810423649797
Experience 45, Iter 57, disc loss: 0.0002062751929955478, policy loss: 10.483336321637442
Experience 45, Iter 58, disc loss: 0.00020799901275252885, policy loss: 10.615800955888428
Experience 45, Iter 59, disc loss: 0.00026408015285738927, policy loss: 10.720392218137302
Experience 45, Iter 60, disc loss: 0.00023966970805902317, policy loss: 11.159739090563406
Experience 45, Iter 61, disc loss: 0.00021648283340420745, policy loss: 11.388847684761593
Experience 45, Iter 62, disc loss: 0.0002024379104659237, policy loss: 10.489992448393107
Experience 45, Iter 63, disc loss: 0.0001556704825792934, policy loss: 11.850230347391784
Experience 45, Iter 64, disc loss: 0.00015183747634012104, policy loss: 12.027221544377486
Experience 45, Iter 65, disc loss: 0.0001735474567737714, policy loss: 11.852523452215882
Experience 45, Iter 66, disc loss: 0.00028754564165818535, policy loss: 10.098107500139307
Experience 45, Iter 67, disc loss: 0.00023726798617846313, policy loss: 11.591223485881883
Experience 45, Iter 68, disc loss: 0.0002770884403630162, policy loss: 11.597536295592633
Experience 45, Iter 69, disc loss: 0.0002095957995192691, policy loss: 10.745315825644859
Experience 45, Iter 70, disc loss: 0.0001865113654811474, policy loss: 10.605403076594728
Experience 45, Iter 71, disc loss: 0.00021143049871715042, policy loss: 10.234376017976128
Experience 45, Iter 72, disc loss: 0.0002562992876448369, policy loss: 10.670591473193305
Experience 45, Iter 73, disc loss: 0.0002567559467992453, policy loss: 11.79307408512745
Experience 45, Iter 74, disc loss: 0.00025132339238793494, policy loss: 10.042312394255797
Experience 45, Iter 75, disc loss: 0.00020932363833793945, policy loss: 11.171808885360395
Experience 45, Iter 76, disc loss: 0.0002600257210329471, policy loss: 9.891622388949909
Experience 45, Iter 77, disc loss: 0.0002825448381418893, policy loss: 11.166214673209653
Experience 45, Iter 78, disc loss: 0.0002953594907653257, policy loss: 10.394816471949586
Experience 45, Iter 79, disc loss: 0.00021305390467681756, policy loss: 10.960384440075412
Experience 45, Iter 80, disc loss: 0.0002177339960756347, policy loss: 10.724426064279623
Experience 45, Iter 81, disc loss: 0.00031023735994652614, policy loss: 10.692955677977366
Experience 45, Iter 82, disc loss: 0.00038946608112908965, policy loss: 10.181812783314456
Experience 45, Iter 83, disc loss: 0.0002573244176082417, policy loss: 13.31789644470778
Experience 45, Iter 84, disc loss: 0.00034622021743173343, policy loss: 10.064673067965355
Experience 45, Iter 85, disc loss: 0.00018026194222047458, policy loss: 11.431198943938512
Experience 45, Iter 86, disc loss: 0.00014254684375293808, policy loss: 12.272132813155991
Experience 45, Iter 87, disc loss: 0.0001536913269046817, policy loss: 11.350178677570753
Experience 45, Iter 88, disc loss: 0.00031813410075216353, policy loss: 9.531047678147509
Experience 45, Iter 89, disc loss: 0.00026732825950731575, policy loss: 11.46081820889599
Experience 45, Iter 90, disc loss: 0.00030957499320237713, policy loss: 11.335065542308723
Experience 45, Iter 91, disc loss: 0.0002961459772734105, policy loss: 10.372678963217087
Experience 45, Iter 92, disc loss: 0.00019131389541608213, policy loss: 10.825813960590803
Experience 45, Iter 93, disc loss: 0.00015069444594908153, policy loss: 11.39548805913082
Experience 45, Iter 94, disc loss: 0.00017690832318622565, policy loss: 11.16037950555867
Experience 45, Iter 95, disc loss: 0.00027745073207460726, policy loss: 9.754424931021486
Experience 45, Iter 96, disc loss: 0.000320651366625928, policy loss: 10.837903809359402
Experience 45, Iter 97, disc loss: 0.00026920589671321243, policy loss: 11.188544801349094
Experience 45, Iter 98, disc loss: 0.00020017207936459002, policy loss: 10.367521422725721
Experience 45, Iter 99, disc loss: 0.00018690586432016924, policy loss: 10.742980994596538
Experience: 46
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.2487],
        [1.9410],
        [0.0438]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0097, 0.1686, 1.9196, 0.0240, 0.0264, 5.3789]],

        [[0.0097, 0.1686, 1.9196, 0.0240, 0.0264, 5.3789]],

        [[0.0097, 0.1686, 1.9196, 0.0240, 0.0264, 5.3789]],

        [[0.0097, 0.1686, 1.9196, 0.0240, 0.0264, 5.3789]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0150, 0.9950, 7.7641, 0.1750], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0150, 0.9950, 7.7641, 0.1750])
N: 460
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1841.0000, 1841.0000, 1841.0000, 1841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.709
Iter 2/2000 - Loss: 3.796
Iter 3/2000 - Loss: 3.484
Iter 4/2000 - Loss: 3.433
Iter 5/2000 - Loss: 3.427
Iter 6/2000 - Loss: 3.294
Iter 7/2000 - Loss: 3.094
Iter 8/2000 - Loss: 2.897
Iter 9/2000 - Loss: 2.722
Iter 10/2000 - Loss: 2.548
Iter 11/2000 - Loss: 2.347
Iter 12/2000 - Loss: 2.106
Iter 13/2000 - Loss: 1.831
Iter 14/2000 - Loss: 1.536
Iter 15/2000 - Loss: 1.229
Iter 16/2000 - Loss: 0.914
Iter 17/2000 - Loss: 0.592
Iter 18/2000 - Loss: 0.262
Iter 19/2000 - Loss: -0.075
Iter 20/2000 - Loss: -0.416
Iter 1981/2000 - Loss: -8.496
Iter 1982/2000 - Loss: -8.496
Iter 1983/2000 - Loss: -8.496
Iter 1984/2000 - Loss: -8.496
Iter 1985/2000 - Loss: -8.496
Iter 1986/2000 - Loss: -8.496
Iter 1987/2000 - Loss: -8.496
Iter 1988/2000 - Loss: -8.496
Iter 1989/2000 - Loss: -8.496
Iter 1990/2000 - Loss: -8.496
Iter 1991/2000 - Loss: -8.496
Iter 1992/2000 - Loss: -8.496
Iter 1993/2000 - Loss: -8.496
Iter 1994/2000 - Loss: -8.496
Iter 1995/2000 - Loss: -8.496
Iter 1996/2000 - Loss: -8.496
Iter 1997/2000 - Loss: -8.496
Iter 1998/2000 - Loss: -8.496
Iter 1999/2000 - Loss: -8.496
Iter 2000/2000 - Loss: -8.497
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[ 9.9512,  4.8349, 32.8282,  1.5900, 17.2212, 56.7776]],

        [[15.2093, 28.2408,  7.4473,  1.7295,  0.7137, 21.9206]],

        [[15.3665, 28.9593,  7.4211,  1.0275,  0.6266, 23.9279]],

        [[12.6734, 23.2965, 14.5441,  1.3364,  1.7496, 44.1919]]])
Signal Variance: tensor([ 0.0757,  1.6046, 12.8413,  0.4401])
Estimated target variance: tensor([0.0150, 0.9950, 7.7641, 0.1750])
N: 460
Signal to noise ratio: tensor([16.5157, 69.7161, 78.1897, 42.1475])
Bound on condition number: tensor([ 125474.9000, 2235751.9790, 2812267.4501,  817148.5939])
Policy Optimizer learning rate:
0.009536941732457853
Experience 46, Iter 0, disc loss: 0.00019839000752807783, policy loss: 10.502073459752381
Experience 46, Iter 1, disc loss: 0.0002508392095065154, policy loss: 11.079028070342769
Experience 46, Iter 2, disc loss: 0.0002965282402986924, policy loss: 11.301325442914546
Experience 46, Iter 3, disc loss: 0.00028168640091122497, policy loss: 12.674089594585212
Experience 46, Iter 4, disc loss: 0.00030729338321864555, policy loss: 11.51828620628454
Experience 46, Iter 5, disc loss: 0.00022003029772076427, policy loss: 11.796614458687783
Experience 46, Iter 6, disc loss: 0.0001634649414214128, policy loss: 12.01869949084728
Experience 46, Iter 7, disc loss: 0.00015802386404253628, policy loss: 11.616447309517799
Experience 46, Iter 8, disc loss: 0.00019055832970439593, policy loss: 11.01936002364879
Experience 46, Iter 9, disc loss: 0.0002461952496403364, policy loss: 12.19302577801134
Experience 46, Iter 10, disc loss: 0.00027164300657064286, policy loss: 11.06796947961119
Experience 46, Iter 11, disc loss: 0.00023434812016697885, policy loss: 10.628153734777602
Experience 46, Iter 12, disc loss: 0.0001974717966717402, policy loss: 10.927167119633923
Experience 46, Iter 13, disc loss: 0.00016548987737348395, policy loss: 11.736354296012147
Experience 46, Iter 14, disc loss: 0.00014798658841486603, policy loss: 11.57915445440815
Experience 46, Iter 15, disc loss: 0.0002245946071191479, policy loss: 10.217588096182627
Experience 46, Iter 16, disc loss: 0.000359864907541509, policy loss: 9.88709722301694
Experience 46, Iter 17, disc loss: 0.00024350818511467974, policy loss: 12.635453295013907
Experience 46, Iter 18, disc loss: 0.00026682213537917234, policy loss: 11.378218351352345
Experience 46, Iter 19, disc loss: 0.00014042976721880966, policy loss: 12.577053662290783
Experience 46, Iter 20, disc loss: 0.00012508449029413005, policy loss: 13.507656524572266
Experience 46, Iter 21, disc loss: 0.00012241061042958835, policy loss: 12.878260879247861
Experience 46, Iter 22, disc loss: 0.00014966329042536788, policy loss: 11.890713875959195
Experience 46, Iter 23, disc loss: 0.00017051561254649632, policy loss: 11.392758203695998
Experience 46, Iter 24, disc loss: 0.00023763906679186842, policy loss: 11.85453806571852
Experience 46, Iter 25, disc loss: 0.00025156697144883256, policy loss: 12.196533278225779
Experience 46, Iter 26, disc loss: 0.0002144949466698906, policy loss: 10.776574181873922
Experience 46, Iter 27, disc loss: 0.00015007498510845527, policy loss: 11.64971053925943
Experience 46, Iter 28, disc loss: 0.00012660158805507144, policy loss: 12.741414511755611
Experience 46, Iter 29, disc loss: 0.00016310944318090892, policy loss: 11.72164438591989
Experience 46, Iter 30, disc loss: 0.0002830375745383099, policy loss: 10.526495189588704
Experience 46, Iter 31, disc loss: 0.0002710616240064981, policy loss: 11.217356679819922
Experience 46, Iter 32, disc loss: 0.0002923878141818594, policy loss: 11.076991718493783
Experience 46, Iter 33, disc loss: 0.000241242732514162, policy loss: 10.96250722504352
Experience 46, Iter 34, disc loss: 0.0001633967768338745, policy loss: 11.185566483882361
Experience 46, Iter 35, disc loss: 0.00018361618196212424, policy loss: 11.391965163143382
Experience 46, Iter 36, disc loss: 0.00022170707395900697, policy loss: 11.122434601821281
Experience 46, Iter 37, disc loss: 0.0003795406771967859, policy loss: 10.677173181315318
Experience 46, Iter 38, disc loss: 0.00027077487320203856, policy loss: 11.924470210980315
Experience 46, Iter 39, disc loss: 0.00024720043394250383, policy loss: 10.32617527228945
Experience 46, Iter 40, disc loss: 0.00016528381076852294, policy loss: 11.187147022799152
Experience 46, Iter 41, disc loss: 0.00015194619031658953, policy loss: 10.853308151782135
Experience 46, Iter 42, disc loss: 0.00016035598233904726, policy loss: 10.93968515111113
Experience 46, Iter 43, disc loss: 0.0002794312593965428, policy loss: 9.908610846437991
Experience 46, Iter 44, disc loss: 0.00023654113544933436, policy loss: 12.081494428267872
Experience 46, Iter 45, disc loss: 0.0002556929985886413, policy loss: 11.577508407036877
Experience 46, Iter 46, disc loss: 0.0002544820693876456, policy loss: 9.969321252041055
Experience 46, Iter 47, disc loss: 0.00015476783865335904, policy loss: 12.338401148196683
Experience 46, Iter 48, disc loss: 0.0001379089198042398, policy loss: 12.53999363395486
Experience 46, Iter 49, disc loss: 0.0001870247212050076, policy loss: 11.586512474531656
Experience 46, Iter 50, disc loss: 0.0002294342079029326, policy loss: 10.896389681236473
Experience 46, Iter 51, disc loss: 0.00026464320783731583, policy loss: 11.15114060487095
Experience 46, Iter 52, disc loss: 0.00024804264336865725, policy loss: 10.535782348949777
Experience 46, Iter 53, disc loss: 0.0001838334754221185, policy loss: 10.85348421640577
Experience 46, Iter 54, disc loss: 0.00016555085734761357, policy loss: 11.609914707893674
Experience 46, Iter 55, disc loss: 0.00014592963957773126, policy loss: 11.350628164490269
Experience 46, Iter 56, disc loss: 0.00014272412588395628, policy loss: 11.199845434079945
Experience 46, Iter 57, disc loss: 0.00020115528141518885, policy loss: 10.544342492025734
Experience 46, Iter 58, disc loss: 0.00021099186904344422, policy loss: 11.269249030081916
Experience 46, Iter 59, disc loss: 0.0002186718510152086, policy loss: 11.039999464123683
Experience 46, Iter 60, disc loss: 0.00022867266030180937, policy loss: 9.92504001817771
Experience 46, Iter 61, disc loss: 0.00015824767475591033, policy loss: 10.928056555839104
Experience 46, Iter 62, disc loss: 0.00013940683870508673, policy loss: 11.20713905160419
Experience 46, Iter 63, disc loss: 0.00017861469754470977, policy loss: 11.69827435768595
Experience 46, Iter 64, disc loss: 0.0002796731646694871, policy loss: 10.45288479067227
Experience 46, Iter 65, disc loss: 0.00027886102244624504, policy loss: 10.927876738410346
Experience 46, Iter 66, disc loss: 0.0002836870583930313, policy loss: 11.78044251887679
Experience 46, Iter 67, disc loss: 0.00020539114586195812, policy loss: 10.711388872557833
Experience 46, Iter 68, disc loss: 0.00018599512642290545, policy loss: 11.600030955591544
Experience 46, Iter 69, disc loss: 0.00013694180669843813, policy loss: 12.049769074661613
Experience 46, Iter 70, disc loss: 0.0001746116285636385, policy loss: 11.742103321548985
Experience 46, Iter 71, disc loss: 0.0003073884402753932, policy loss: 10.822680567349163
Experience 46, Iter 72, disc loss: 0.00027673756348397334, policy loss: 12.733665372042397
Experience 46, Iter 73, disc loss: 0.0003171813297477748, policy loss: 11.180888870032776
Experience 46, Iter 74, disc loss: 0.0001606544179880338, policy loss: 11.013339994324868
Experience 46, Iter 75, disc loss: 0.00012680252215150533, policy loss: 12.561492680048879
Experience 46, Iter 76, disc loss: 0.0001705307524390541, policy loss: 12.203660939700363
Experience 46, Iter 77, disc loss: 0.00019407160522299702, policy loss: 11.801524309319516
Experience 46, Iter 78, disc loss: 0.0002420407655255934, policy loss: 10.34155332370841
Experience 46, Iter 79, disc loss: 0.00024438306587824967, policy loss: 11.83485573526336
Experience 46, Iter 80, disc loss: 0.00019608499294774187, policy loss: 13.337436189816145
Experience 46, Iter 81, disc loss: 0.00033989411059749295, policy loss: 10.701129442573167
Experience 46, Iter 82, disc loss: 0.00015301548024203198, policy loss: 12.485826377439398
Experience 46, Iter 83, disc loss: 0.00013740208559981793, policy loss: 13.140640829765186
Experience 46, Iter 84, disc loss: 0.00012328989809667082, policy loss: 12.338288448100872
Experience 46, Iter 85, disc loss: 0.00015764296706986114, policy loss: 11.441305171552994
Experience 46, Iter 86, disc loss: 0.00027599995529630254, policy loss: 10.712081951305455
Experience 46, Iter 87, disc loss: 0.00023331176225363108, policy loss: 12.926095302355861
Experience 46, Iter 88, disc loss: 0.00021680251746128095, policy loss: 11.29613607887367
Experience 46, Iter 89, disc loss: 0.00020694234160779525, policy loss: 10.359034940017633
Experience 46, Iter 90, disc loss: 0.00020426203751254486, policy loss: 11.354227661045218
Experience 46, Iter 91, disc loss: 0.000140117903106116, policy loss: 11.39149950118574
Experience 46, Iter 92, disc loss: 0.00018208686601032877, policy loss: 11.074609399148324
Experience 46, Iter 93, disc loss: 0.00023396822640606027, policy loss: 10.978616383104416
Experience 46, Iter 94, disc loss: 0.00024180037288811705, policy loss: 10.712529867401319
Experience 46, Iter 95, disc loss: 0.0002809754422253042, policy loss: 10.489403830311074
Experience 46, Iter 96, disc loss: 0.00024345695364006654, policy loss: 10.597102810688579
Experience 46, Iter 97, disc loss: 0.0001832496922290729, policy loss: 10.636917558729445
Experience 46, Iter 98, disc loss: 0.0001453846355556848, policy loss: 11.458148499196065
Experience 46, Iter 99, disc loss: 0.00018124623571863822, policy loss: 10.894303926127385
Experience: 47
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.2520],
        [1.9596],
        [0.0442]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0096, 0.1696, 1.9415, 0.0243, 0.0267, 5.4500]],

        [[0.0096, 0.1696, 1.9415, 0.0243, 0.0267, 5.4500]],

        [[0.0096, 0.1696, 1.9415, 0.0243, 0.0267, 5.4500]],

        [[0.0096, 0.1696, 1.9415, 0.0243, 0.0267, 5.4500]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0151, 1.0082, 7.8382, 0.1769], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0151, 1.0082, 7.8382, 0.1769])
N: 470
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1881.0000, 1881.0000, 1881.0000, 1881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.726
Iter 2/2000 - Loss: 3.805
Iter 3/2000 - Loss: 3.493
Iter 4/2000 - Loss: 3.439
Iter 5/2000 - Loss: 3.427
Iter 6/2000 - Loss: 3.292
Iter 7/2000 - Loss: 3.090
Iter 8/2000 - Loss: 2.892
Iter 9/2000 - Loss: 2.714
Iter 10/2000 - Loss: 2.537
Iter 11/2000 - Loss: 2.331
Iter 12/2000 - Loss: 2.086
Iter 13/2000 - Loss: 1.807
Iter 14/2000 - Loss: 1.509
Iter 15/2000 - Loss: 1.200
Iter 16/2000 - Loss: 0.884
Iter 17/2000 - Loss: 0.561
Iter 18/2000 - Loss: 0.230
Iter 19/2000 - Loss: -0.108
Iter 20/2000 - Loss: -0.449
Iter 1981/2000 - Loss: -8.491
Iter 1982/2000 - Loss: -8.491
Iter 1983/2000 - Loss: -8.491
Iter 1984/2000 - Loss: -8.491
Iter 1985/2000 - Loss: -8.491
Iter 1986/2000 - Loss: -8.491
Iter 1987/2000 - Loss: -8.491
Iter 1988/2000 - Loss: -8.491
Iter 1989/2000 - Loss: -8.491
Iter 1990/2000 - Loss: -8.491
Iter 1991/2000 - Loss: -8.491
Iter 1992/2000 - Loss: -8.491
Iter 1993/2000 - Loss: -8.491
Iter 1994/2000 - Loss: -8.491
Iter 1995/2000 - Loss: -8.491
Iter 1996/2000 - Loss: -8.491
Iter 1997/2000 - Loss: -8.491
Iter 1998/2000 - Loss: -8.491
Iter 1999/2000 - Loss: -8.491
Iter 2000/2000 - Loss: -8.492
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 9.8140,  5.1070, 32.3454,  1.6094, 16.9725, 57.2953]],

        [[15.0104, 28.1364,  7.5975,  1.7266,  0.7086, 21.8786]],

        [[15.1185, 28.2496,  7.3724,  1.0379,  0.6128, 23.2942]],

        [[12.5428, 23.3862, 14.9064,  1.3408,  1.7048, 44.4713]]])
Signal Variance: tensor([ 0.0803,  1.6143, 12.3805,  0.4533])
Estimated target variance: tensor([0.0151, 1.0082, 7.8382, 0.1769])
N: 470
Signal to noise ratio: tensor([16.8567, 70.3017, 76.0488, 42.1612])
Bound on condition number: tensor([ 133550.5232, 2322897.7920, 2718205.7373,  835456.3957])
Policy Optimizer learning rate:
0.00952689885301437
Experience 47, Iter 0, disc loss: 0.0002202425488654212, policy loss: 11.591627250899613
Experience 47, Iter 1, disc loss: 0.0002976174658095536, policy loss: 10.358991249234144
Experience 47, Iter 2, disc loss: 0.00029002027044467316, policy loss: 10.096017115643992
Experience 47, Iter 3, disc loss: 0.00020210835531999605, policy loss: 10.689862611073053
Experience 47, Iter 4, disc loss: 0.00016397051675931128, policy loss: 11.643729608788817
Experience 47, Iter 5, disc loss: 0.00018780798174772642, policy loss: 11.01353396183155
Experience 47, Iter 6, disc loss: 0.0002429378709993384, policy loss: 10.622904774944368
Experience 47, Iter 7, disc loss: 0.0002785839914620601, policy loss: 11.44887009214145
Experience 47, Iter 8, disc loss: 0.00028294232960858717, policy loss: 10.84305959589163
Experience 47, Iter 9, disc loss: 0.00020111434683236026, policy loss: 10.740564418227182
Experience 47, Iter 10, disc loss: 0.00017776840633487473, policy loss: 10.993641963284018
Experience 47, Iter 11, disc loss: 0.00020588937984869602, policy loss: 11.049013445797858
Experience 47, Iter 12, disc loss: 0.0002436440974615246, policy loss: 10.922092923111547
Experience 47, Iter 13, disc loss: 0.0002921974412234649, policy loss: 10.76167612021775
Experience 47, Iter 14, disc loss: 0.00024275073943299145, policy loss: 10.080744043193512
Experience 47, Iter 15, disc loss: 0.00019358818755616723, policy loss: 10.453506114682282
Experience 47, Iter 16, disc loss: 0.0002243820979142468, policy loss: 10.684009868102502
Experience 47, Iter 17, disc loss: 0.00019227831863674486, policy loss: 11.336982950472931
Experience 47, Iter 18, disc loss: 0.0002477398172487014, policy loss: 11.048764298531914
Experience 47, Iter 19, disc loss: 0.0002907418246161495, policy loss: 10.183379908235622
Experience 47, Iter 20, disc loss: 0.00020270843091902292, policy loss: 10.882148336925967
Experience 47, Iter 21, disc loss: 0.00017495560619926786, policy loss: 11.881895190022359
Experience 47, Iter 22, disc loss: 0.00022281298835284197, policy loss: 11.224653260945137
Experience 47, Iter 23, disc loss: 0.00019938027859741766, policy loss: 11.551825027987135
Experience 47, Iter 24, disc loss: 0.0002447256585301266, policy loss: 10.655279203287337
Experience 47, Iter 25, disc loss: 0.0002034758936218938, policy loss: 10.504309983653881
Experience 47, Iter 26, disc loss: 0.00020015771611003272, policy loss: 10.272562504628269
Experience 47, Iter 27, disc loss: 0.0001791302291302843, policy loss: 10.632501752974854
Experience 47, Iter 28, disc loss: 0.00019172782570435514, policy loss: 10.948673434174115
Experience 47, Iter 29, disc loss: 0.0001929731427833344, policy loss: 10.322208923033358
Experience 47, Iter 30, disc loss: 0.00026378068996213856, policy loss: 10.521769552254085
Experience 47, Iter 31, disc loss: 0.0002632185049981274, policy loss: 10.322600305227478
Experience 47, Iter 32, disc loss: 0.0002322675124689376, policy loss: 10.31676632196254
Experience 47, Iter 33, disc loss: 0.00017380382083017248, policy loss: 11.156628353777954
Experience 47, Iter 34, disc loss: 0.00019374162220121082, policy loss: 11.461890359974984
Experience 47, Iter 35, disc loss: 0.00025094691306568767, policy loss: 11.561346445782396
Experience 47, Iter 36, disc loss: 0.000265659954227633, policy loss: 10.724612650587265
Experience 47, Iter 37, disc loss: 0.0002555158918312664, policy loss: 10.09786614518698
Experience 47, Iter 38, disc loss: 0.000193464456982674, policy loss: 11.187456850337005
Experience 47, Iter 39, disc loss: 0.00021072582039008699, policy loss: 10.724919448664167
Experience 47, Iter 40, disc loss: 0.00023736108516839914, policy loss: 10.302785931658821
Experience 47, Iter 41, disc loss: 0.00021992817639272187, policy loss: 11.384322887693688
Experience 47, Iter 42, disc loss: 0.00017890210527498647, policy loss: 11.387742752838122
Experience 47, Iter 43, disc loss: 0.00017606468139190406, policy loss: 10.294288535212102
Experience 47, Iter 44, disc loss: 0.00021283191886574865, policy loss: 10.354348807396878
Experience 47, Iter 45, disc loss: 0.0002072627135357077, policy loss: 10.452768829535705
Experience 47, Iter 46, disc loss: 0.0002148949164906081, policy loss: 10.225511341937022
Experience 47, Iter 47, disc loss: 0.00023334871174287316, policy loss: 11.027285114319637
Experience 47, Iter 48, disc loss: 0.00025441909873999744, policy loss: 10.876991110708142
Experience 47, Iter 49, disc loss: 0.0002946877665910977, policy loss: 10.214865933411758
Experience 47, Iter 50, disc loss: 0.0002485071084866496, policy loss: 10.531736082749994
Experience 47, Iter 51, disc loss: 0.0002103910695435414, policy loss: 11.9735576154514
Experience 47, Iter 52, disc loss: 0.00021110934765141047, policy loss: 10.85118528768987
Experience 47, Iter 53, disc loss: 0.0002002323422044223, policy loss: 11.296875638173551
Experience 47, Iter 54, disc loss: 0.0002513444925068184, policy loss: 12.228752675745636
Experience 47, Iter 55, disc loss: 0.000213402940344148, policy loss: 11.112102724415617
Experience 47, Iter 56, disc loss: 0.00020664931386350332, policy loss: 11.671170233459826
Experience 47, Iter 57, disc loss: 0.0001554302368637823, policy loss: 12.3303162132082
Experience 47, Iter 58, disc loss: 0.00017906015556815318, policy loss: 12.083648402230374
Experience 47, Iter 59, disc loss: 0.0001764282621919801, policy loss: 10.764373685481884
Experience 47, Iter 60, disc loss: 0.00013760917345105017, policy loss: 12.439517821539145
Experience 47, Iter 61, disc loss: 0.00013336987399004368, policy loss: 11.51978787479577
Experience 47, Iter 62, disc loss: 0.00017121667811838178, policy loss: 11.444183397352763
Experience 47, Iter 63, disc loss: 0.00024429450131162843, policy loss: 11.409725853851821
Experience 47, Iter 64, disc loss: 0.00023481713560686993, policy loss: 10.679690689566979
Experience 47, Iter 65, disc loss: 0.00017112382543671678, policy loss: 10.76185624412183
Experience 47, Iter 66, disc loss: 0.00018997060369301826, policy loss: 11.728528701115309
Experience 47, Iter 67, disc loss: 0.00013802722146559975, policy loss: 12.015977014789208
Experience 47, Iter 68, disc loss: 0.00019239467470770656, policy loss: 10.43097034079219
Experience 47, Iter 69, disc loss: 0.00024256904348262615, policy loss: 10.841532010293044
Experience 47, Iter 70, disc loss: 0.0002718187500795673, policy loss: 13.053154006706148
Experience 47, Iter 71, disc loss: 0.0002840547980558213, policy loss: 10.666415040076958
Experience 47, Iter 72, disc loss: 0.00017981732192483022, policy loss: 11.947124426806916
Experience 47, Iter 73, disc loss: 0.00012578058270828506, policy loss: 12.063447562549413
Experience 47, Iter 74, disc loss: 0.000133410356637279, policy loss: 11.809244900970931
Experience 47, Iter 75, disc loss: 0.00015731363705110424, policy loss: 11.375483474660648
Experience 47, Iter 76, disc loss: 0.00022491035638127138, policy loss: 11.765548315811781
Experience 47, Iter 77, disc loss: 0.00022524392709484092, policy loss: 10.63994043188343
Experience 47, Iter 78, disc loss: 0.00017080612877457782, policy loss: 13.773120724747908
Experience 47, Iter 79, disc loss: 0.00019261155324380144, policy loss: 10.397224718137068
Experience 47, Iter 80, disc loss: 0.00013076344030403097, policy loss: 11.69676530795118
Experience 47, Iter 81, disc loss: 0.000146460303072446, policy loss: 11.785734101756574
Experience 47, Iter 82, disc loss: 0.000184575685084509, policy loss: 10.9793245679422
Experience 47, Iter 83, disc loss: 0.0001887509238280342, policy loss: 11.35095094132187
Experience 47, Iter 84, disc loss: 0.0003216584445121102, policy loss: 11.167548833516355
Experience 47, Iter 85, disc loss: 0.00022419581804766515, policy loss: 10.996062047262622
Experience 47, Iter 86, disc loss: 0.00018271562598810279, policy loss: 10.199996683741785
Experience 47, Iter 87, disc loss: 0.0001507641336896511, policy loss: 11.002435614545016
Experience 47, Iter 88, disc loss: 0.00020011845288275902, policy loss: 11.149557989450336
Experience 47, Iter 89, disc loss: 0.00033628271081527424, policy loss: 10.337577784294059
Experience 47, Iter 90, disc loss: 0.00027273960782977195, policy loss: 10.507872263216868
Experience 47, Iter 91, disc loss: 0.0002845454495921816, policy loss: 10.47248659170317
Experience 47, Iter 92, disc loss: 0.00022192942224461858, policy loss: 11.636933244139748
Experience 47, Iter 93, disc loss: 0.00023528901448584838, policy loss: 10.983660035597078
Experience 47, Iter 94, disc loss: 0.00022618442631526823, policy loss: 10.358959217790568
Experience 47, Iter 95, disc loss: 0.00020339234154671742, policy loss: 10.592657017121308
Experience 47, Iter 96, disc loss: 0.0001675729314769837, policy loss: 11.130838216878836
Experience 47, Iter 97, disc loss: 0.00018789462014516932, policy loss: 10.548144772583287
Experience 47, Iter 98, disc loss: 0.00018930057027160985, policy loss: 11.650117437668623
Experience 47, Iter 99, disc loss: 0.0001962445534693129, policy loss: 11.069157956233507
Experience: 48
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.2542],
        [1.9775],
        [0.0444]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0095, 0.1702, 1.9553, 0.0247, 0.0268, 5.4992]],

        [[0.0095, 0.1702, 1.9553, 0.0247, 0.0268, 5.4992]],

        [[0.0095, 0.1702, 1.9553, 0.0247, 0.0268, 5.4992]],

        [[0.0095, 0.1702, 1.9553, 0.0247, 0.0268, 5.4992]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0151, 1.0166, 7.9099, 0.1776], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0151, 1.0166, 7.9099, 0.1776])
N: 480
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1921.0000, 1921.0000, 1921.0000, 1921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.732
Iter 2/2000 - Loss: 3.808
Iter 3/2000 - Loss: 3.496
Iter 4/2000 - Loss: 3.439
Iter 5/2000 - Loss: 3.424
Iter 6/2000 - Loss: 3.287
Iter 7/2000 - Loss: 3.083
Iter 8/2000 - Loss: 2.882
Iter 9/2000 - Loss: 2.701
Iter 10/2000 - Loss: 2.520
Iter 11/2000 - Loss: 2.310
Iter 12/2000 - Loss: 2.062
Iter 13/2000 - Loss: 1.781
Iter 14/2000 - Loss: 1.482
Iter 15/2000 - Loss: 1.172
Iter 16/2000 - Loss: 0.855
Iter 17/2000 - Loss: 0.532
Iter 18/2000 - Loss: 0.202
Iter 19/2000 - Loss: -0.135
Iter 20/2000 - Loss: -0.476
Iter 1981/2000 - Loss: -8.517
Iter 1982/2000 - Loss: -8.517
Iter 1983/2000 - Loss: -8.517
Iter 1984/2000 - Loss: -8.517
Iter 1985/2000 - Loss: -8.517
Iter 1986/2000 - Loss: -8.517
Iter 1987/2000 - Loss: -8.518
Iter 1988/2000 - Loss: -8.518
Iter 1989/2000 - Loss: -8.518
Iter 1990/2000 - Loss: -8.518
Iter 1991/2000 - Loss: -8.518
Iter 1992/2000 - Loss: -8.518
Iter 1993/2000 - Loss: -8.518
Iter 1994/2000 - Loss: -8.518
Iter 1995/2000 - Loss: -8.518
Iter 1996/2000 - Loss: -8.518
Iter 1997/2000 - Loss: -8.518
Iter 1998/2000 - Loss: -8.518
Iter 1999/2000 - Loss: -8.518
Iter 2000/2000 - Loss: -8.518
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 9.7883,  5.0854, 32.8005,  1.5894, 16.5625, 57.5173]],

        [[14.9730, 27.8547,  7.4833,  1.7562,  0.7021, 22.7036]],

        [[15.1139, 28.1560,  7.4161,  1.0291,  0.6024, 23.5560]],

        [[12.4196, 23.1066, 14.4896,  1.3291,  1.7984, 43.1662]]])
Signal Variance: tensor([ 0.0784,  1.6612, 12.4021,  0.4514])
Estimated target variance: tensor([0.0151, 1.0166, 7.9099, 0.1776])
N: 480
Signal to noise ratio: tensor([16.6240, 71.8340, 76.4788, 42.3127])
Bound on condition number: tensor([ 132651.8201, 2476858.9732, 2807522.5809,  859375.3777])
Policy Optimizer learning rate:
0.009516866549228193
Experience 48, Iter 0, disc loss: 0.00017359328519804762, policy loss: 10.522376969122227
Experience 48, Iter 1, disc loss: 0.00013642665223613968, policy loss: 12.229966794273128
Experience 48, Iter 2, disc loss: 0.00021229629634346745, policy loss: 10.724326751510645
Experience 48, Iter 3, disc loss: 0.00021399475078245813, policy loss: 10.201209839197784
Experience 48, Iter 4, disc loss: 0.00021811614982110525, policy loss: 12.54149151696744
Experience 48, Iter 5, disc loss: 0.00020766323092868266, policy loss: 10.854591351933221
Experience 48, Iter 6, disc loss: 0.00018597917329135063, policy loss: 10.595636394966776
Experience 48, Iter 7, disc loss: 0.00022533358560138404, policy loss: 10.167522562583438
Experience 48, Iter 8, disc loss: 0.0002105033935358936, policy loss: 10.569093523296509
Experience 48, Iter 9, disc loss: 0.0002485217460648674, policy loss: 12.12425105865763
Experience 48, Iter 10, disc loss: 0.0002210169037172031, policy loss: 10.634273157830151
Experience 48, Iter 11, disc loss: 0.00021333257816166134, policy loss: 10.819387160409173
Experience 48, Iter 12, disc loss: 0.00015584029460066138, policy loss: 11.681751106655575
Experience 48, Iter 13, disc loss: 0.00012005682072503129, policy loss: 12.898969610501622
Experience 48, Iter 14, disc loss: 0.00016863523541771837, policy loss: 11.0812304117895
Experience 48, Iter 15, disc loss: 0.00019998173686850923, policy loss: 10.787096672481624
Experience 48, Iter 16, disc loss: 0.0002455354015416286, policy loss: 11.273165529955246
Experience 48, Iter 17, disc loss: 0.00025281668065691695, policy loss: 10.679895095382497
Experience 48, Iter 18, disc loss: 0.00021095482363083109, policy loss: 10.055929535452728
Experience 48, Iter 19, disc loss: 0.00017478940847112363, policy loss: 10.746854103520905
Experience 48, Iter 20, disc loss: 0.00017510303964090064, policy loss: 11.140814753963518
Experience 48, Iter 21, disc loss: 0.00018304801714028637, policy loss: 10.777378963918968
Experience 48, Iter 22, disc loss: 0.00017432312679793813, policy loss: 11.14215940232992
Experience 48, Iter 23, disc loss: 0.00021501509481549214, policy loss: 11.238458634710462
Experience 48, Iter 24, disc loss: 0.00021104712011207476, policy loss: 10.079920425220275
Experience 48, Iter 25, disc loss: 0.0002676393099905468, policy loss: 9.927578490122523
Experience 48, Iter 26, disc loss: 0.00021028505308490702, policy loss: 10.900815647495534
Experience 48, Iter 27, disc loss: 0.00021419034323519955, policy loss: 10.54237569417258
Experience 48, Iter 28, disc loss: 0.0002576131694886644, policy loss: 10.464191141104244
Experience 48, Iter 29, disc loss: 0.0002089342830052162, policy loss: 10.630361095049054
Experience 48, Iter 30, disc loss: 0.00024243224722001128, policy loss: 10.91334403083188
Experience 48, Iter 31, disc loss: 0.0002259723702272987, policy loss: 10.73669308994755
Experience 48, Iter 32, disc loss: 0.000296957810211569, policy loss: 10.451431908502492
Experience 48, Iter 33, disc loss: 0.00021081173977379838, policy loss: 11.611044044783284
Experience 48, Iter 34, disc loss: 0.00014030988072637158, policy loss: 11.938629917610967
Experience 48, Iter 35, disc loss: 0.00013828687018441384, policy loss: 11.554635761904315
Experience 48, Iter 36, disc loss: 0.00016831752882353736, policy loss: 10.880382048714601
Experience 48, Iter 37, disc loss: 0.00019593320008097416, policy loss: 10.818224608660607
Experience 48, Iter 38, disc loss: 0.00026250829415316466, policy loss: 11.529419632644592
Experience 48, Iter 39, disc loss: 0.0002384060625752246, policy loss: 10.775372260648643
Experience 48, Iter 40, disc loss: 0.00022062190502752168, policy loss: 10.460637431972449
Experience 48, Iter 41, disc loss: 0.00016974989161282954, policy loss: 11.589243867499466
Experience 48, Iter 42, disc loss: 0.00015577326636906965, policy loss: 11.69134981134945
Experience 48, Iter 43, disc loss: 0.00018815234878769629, policy loss: 10.54251518936011
Experience 48, Iter 44, disc loss: 0.00019853732251191798, policy loss: 10.622864002359787
Experience 48, Iter 45, disc loss: 0.0001970949570284693, policy loss: 11.286557662026457
Experience 48, Iter 46, disc loss: 0.00020458295801491303, policy loss: 10.844303294779012
Experience 48, Iter 47, disc loss: 0.00015632687232628004, policy loss: 11.434173584070686
Experience 48, Iter 48, disc loss: 0.0001976977922619576, policy loss: 10.686784977658338
Experience 48, Iter 49, disc loss: 0.00017918786167766857, policy loss: 10.402458657797915
Experience 48, Iter 50, disc loss: 0.000201219815287077, policy loss: 10.715082170190051
Experience 48, Iter 51, disc loss: 0.00020756463975553808, policy loss: 11.31978267688511
Experience 48, Iter 52, disc loss: 0.0002442577325890557, policy loss: 10.330111801186721
Experience 48, Iter 53, disc loss: 0.00021501242892829576, policy loss: 11.008153352856988
Experience 48, Iter 54, disc loss: 0.0001601046497166777, policy loss: 12.628935580915911
Experience 48, Iter 55, disc loss: 0.0001444420017018461, policy loss: 11.50063971976543
Experience 48, Iter 56, disc loss: 0.0001523102799689385, policy loss: 10.711724737088243
Experience 48, Iter 57, disc loss: 0.00015362162859991143, policy loss: 11.448888935484305
Experience 48, Iter 58, disc loss: 0.00019029253509918865, policy loss: 11.698410475205655
Experience 48, Iter 59, disc loss: 0.00019624344118857815, policy loss: 11.695361599441298
Experience 48, Iter 60, disc loss: 0.00021025196617795882, policy loss: 11.312022685714652
Experience 48, Iter 61, disc loss: 0.00016271161303550155, policy loss: 10.926901757463899
Experience 48, Iter 62, disc loss: 0.00012489924138600058, policy loss: 11.768257065792007
Experience 48, Iter 63, disc loss: 0.0001282761475105314, policy loss: 11.218225174795213
Experience 48, Iter 64, disc loss: 0.00017367093914129545, policy loss: 11.994580171196919
Experience 48, Iter 65, disc loss: 0.0002463633526989587, policy loss: 10.486089943507503
Experience 48, Iter 66, disc loss: 0.00020534050531842727, policy loss: 11.328753886427124
Experience 48, Iter 67, disc loss: 0.00017610662552937335, policy loss: 10.86793444827638
Experience 48, Iter 68, disc loss: 0.0001663670903431609, policy loss: 10.719714250033839
Experience 48, Iter 69, disc loss: 0.00020391538136623329, policy loss: 10.727672453039355
Experience 48, Iter 70, disc loss: 0.0002595424358257886, policy loss: 11.568506765015261
Experience 48, Iter 71, disc loss: 0.0002597715101585338, policy loss: 11.643690041122223
Experience 48, Iter 72, disc loss: 0.00018263196957092907, policy loss: 11.589099459919792
Experience 48, Iter 73, disc loss: 0.00013444922078936734, policy loss: 11.72238688475345
Experience 48, Iter 74, disc loss: 0.00011431699997917036, policy loss: 12.497054871765314
Experience 48, Iter 75, disc loss: 0.0001325223887307049, policy loss: 11.249523347714584
Experience 48, Iter 76, disc loss: 0.0002394154550411315, policy loss: 10.584351493164894
Experience 48, Iter 77, disc loss: 0.00022521997142989505, policy loss: 11.73261909356983
Experience 48, Iter 78, disc loss: 0.00031604841601515714, policy loss: 10.507640552318144
Experience 48, Iter 79, disc loss: 0.00023870484451255297, policy loss: 11.053339095988498
Experience 48, Iter 80, disc loss: 0.00016965298506832553, policy loss: 12.07891650807689
Experience 48, Iter 81, disc loss: 0.0001581653596550823, policy loss: 11.208516201925109
Experience 48, Iter 82, disc loss: 0.00016434045194685425, policy loss: 10.727593020414952
Experience 48, Iter 83, disc loss: 0.00020150037587448637, policy loss: 10.794352637397553
Experience 48, Iter 84, disc loss: 0.00022181884508066718, policy loss: 11.138804387281949
Experience 48, Iter 85, disc loss: 0.0001881530681590303, policy loss: 11.868602553136649
Experience 48, Iter 86, disc loss: 0.00014577344793880131, policy loss: 11.432177151774344
Experience 48, Iter 87, disc loss: 0.000185647336980713, policy loss: 11.320508273991464
Experience 48, Iter 88, disc loss: 0.00018386533807975878, policy loss: 10.317654630069676
Experience 48, Iter 89, disc loss: 0.000175563547529744, policy loss: 11.342226160948156
Experience 48, Iter 90, disc loss: 0.0002274983096123243, policy loss: 10.745355259609969
Experience 48, Iter 91, disc loss: 0.00018332488110847918, policy loss: 10.80353664553093
Experience 48, Iter 92, disc loss: 0.0001612941358466438, policy loss: 10.880694526928238
Experience 48, Iter 93, disc loss: 0.00015368100458993365, policy loss: 11.181097436741915
Experience 48, Iter 94, disc loss: 0.00018852090782188634, policy loss: 10.835232012534085
Experience 48, Iter 95, disc loss: 0.00019869833151352545, policy loss: 11.23423831826351
Experience 48, Iter 96, disc loss: 0.00019352392721825012, policy loss: 11.464635306609003
Experience 48, Iter 97, disc loss: 0.00020180980009206086, policy loss: 11.953585003401681
Experience 48, Iter 98, disc loss: 0.00016540357277015845, policy loss: 11.215842540615661
Experience 48, Iter 99, disc loss: 0.00020954865668424072, policy loss: 10.676397460315577
Experience: 49
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.2586],
        [2.0015],
        [0.0452]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0094, 0.1721, 1.9895, 0.0250, 0.0271, 5.5784]],

        [[0.0094, 0.1721, 1.9895, 0.0250, 0.0271, 5.5784]],

        [[0.0094, 0.1721, 1.9895, 0.0250, 0.0271, 5.5784]],

        [[0.0094, 0.1721, 1.9895, 0.0250, 0.0271, 5.5784]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0153, 1.0342, 8.0060, 0.1809], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0153, 1.0342, 8.0060, 0.1809])
N: 490
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1961.0000, 1961.0000, 1961.0000, 1961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.752
Iter 2/2000 - Loss: 3.820
Iter 3/2000 - Loss: 3.506
Iter 4/2000 - Loss: 3.442
Iter 5/2000 - Loss: 3.418
Iter 6/2000 - Loss: 3.274
Iter 7/2000 - Loss: 3.065
Iter 8/2000 - Loss: 2.859
Iter 9/2000 - Loss: 2.672
Iter 10/2000 - Loss: 2.484
Iter 11/2000 - Loss: 2.268
Iter 12/2000 - Loss: 2.015
Iter 13/2000 - Loss: 1.732
Iter 14/2000 - Loss: 1.432
Iter 15/2000 - Loss: 1.122
Iter 16/2000 - Loss: 0.807
Iter 17/2000 - Loss: 0.485
Iter 18/2000 - Loss: 0.155
Iter 19/2000 - Loss: -0.181
Iter 20/2000 - Loss: -0.520
Iter 1981/2000 - Loss: -8.521
Iter 1982/2000 - Loss: -8.521
Iter 1983/2000 - Loss: -8.521
Iter 1984/2000 - Loss: -8.521
Iter 1985/2000 - Loss: -8.521
Iter 1986/2000 - Loss: -8.521
Iter 1987/2000 - Loss: -8.521
Iter 1988/2000 - Loss: -8.521
Iter 1989/2000 - Loss: -8.521
Iter 1990/2000 - Loss: -8.521
Iter 1991/2000 - Loss: -8.521
Iter 1992/2000 - Loss: -8.521
Iter 1993/2000 - Loss: -8.521
Iter 1994/2000 - Loss: -8.521
Iter 1995/2000 - Loss: -8.521
Iter 1996/2000 - Loss: -8.521
Iter 1997/2000 - Loss: -8.521
Iter 1998/2000 - Loss: -8.521
Iter 1999/2000 - Loss: -8.521
Iter 2000/2000 - Loss: -8.521
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.7001,  5.0443, 31.1175,  1.6321, 16.3422, 56.8555]],

        [[14.8830, 27.9771,  7.4007,  1.7325,  0.7106, 22.3232]],

        [[15.2317, 28.0544,  7.5234,  1.0249,  0.5827, 23.0732]],

        [[12.3457, 23.1148, 14.5732,  1.3252,  1.8428, 43.0542]]])
Signal Variance: tensor([ 0.0769,  1.6197, 11.9136,  0.4599])
Estimated target variance: tensor([0.0153, 1.0342, 8.0060, 0.1809])
N: 490
Signal to noise ratio: tensor([16.4784, 70.5017, 74.3383, 42.5872])
Bound on condition number: tensor([ 133054.5554, 2435539.3356, 2707827.3698,  888697.6449])
Policy Optimizer learning rate:
0.009506844809962623
Experience 49, Iter 0, disc loss: 0.00024671886120406156, policy loss: 10.544857628173185
Experience 49, Iter 1, disc loss: 0.00021134053251944002, policy loss: 11.36678876259214
Experience 49, Iter 2, disc loss: 0.00019696774537800984, policy loss: 11.57784402293893
Experience 49, Iter 3, disc loss: 0.00015248703523114478, policy loss: 11.066281844959418
Experience 49, Iter 4, disc loss: 0.00015742792245106526, policy loss: 12.343990635129078
Experience 49, Iter 5, disc loss: 0.0001916779242076598, policy loss: 11.06708363483382
Experience 49, Iter 6, disc loss: 0.0001790256264102685, policy loss: 11.649629499523249
Experience 49, Iter 7, disc loss: 0.00021793700677040859, policy loss: 11.563118877908055
Experience 49, Iter 8, disc loss: 0.00016828709963283777, policy loss: 10.835143203280001
Experience 49, Iter 9, disc loss: 0.00011792235649613677, policy loss: 12.448703732285676
Experience 49, Iter 10, disc loss: 0.00010920914818067959, policy loss: 12.362736140510691
Experience 49, Iter 11, disc loss: 0.00013691402663491836, policy loss: 12.147419231408485
Experience 49, Iter 12, disc loss: 0.0001599416068900274, policy loss: 12.107714979185277
Experience 49, Iter 13, disc loss: 0.0001476640486061999, policy loss: 11.424096583715123
Experience 49, Iter 14, disc loss: 0.0001528114250520562, policy loss: 12.76287762238379
Experience 49, Iter 15, disc loss: 0.0001642055075323532, policy loss: 11.171361985989282
Experience 49, Iter 16, disc loss: 0.0001384368328360675, policy loss: 11.20929862830533
Experience 49, Iter 17, disc loss: 0.0001363864044576147, policy loss: 12.029754835994204
Experience 49, Iter 18, disc loss: 0.0001566345150629497, policy loss: 11.630170108709336
Experience 49, Iter 19, disc loss: 0.00016162715175862836, policy loss: 11.839303622006991
Experience 49, Iter 20, disc loss: 0.00019000108239579445, policy loss: 11.229647997691405
Experience 49, Iter 21, disc loss: 0.00017518789823111271, policy loss: 9.98702291256421
Experience 49, Iter 22, disc loss: 0.0001440913141241683, policy loss: 11.356642637505704
Experience 49, Iter 23, disc loss: 0.0001792611205392399, policy loss: 10.552149694718365
Experience 49, Iter 24, disc loss: 0.00016301038776902089, policy loss: 10.83892788118325
Experience 49, Iter 25, disc loss: 0.0001793092668174503, policy loss: 10.789690517435133
Experience 49, Iter 26, disc loss: 0.00026795152738493834, policy loss: 10.896861639663673
Experience 49, Iter 27, disc loss: 0.00023821561872065984, policy loss: 11.052051126947566
Experience 49, Iter 28, disc loss: 0.00026649139447746815, policy loss: 9.906068989847027
Experience 49, Iter 29, disc loss: 0.00015496123200718707, policy loss: 11.302945138147846
Experience 49, Iter 30, disc loss: 0.00014407218634570186, policy loss: 11.38593068237607
Experience 49, Iter 31, disc loss: 0.00015943809290613415, policy loss: 11.099937502329123
Experience 49, Iter 32, disc loss: 0.00018054333554250727, policy loss: 11.172976362382986
Experience 49, Iter 33, disc loss: 0.000194868331700215, policy loss: 11.56434336847775
Experience 49, Iter 34, disc loss: 0.000218043169917982, policy loss: 11.223611247164834
Experience 49, Iter 35, disc loss: 0.00016544177544297654, policy loss: 10.792686554961087
Experience 49, Iter 36, disc loss: 0.00016445470628228485, policy loss: 11.188688119865422
Experience 49, Iter 37, disc loss: 0.00015424202530667079, policy loss: 11.82909345132662
Experience 49, Iter 38, disc loss: 0.00019942342302279876, policy loss: 10.816021715036904
Experience 49, Iter 39, disc loss: 0.0001838068284100067, policy loss: 12.061977992873937
Experience 49, Iter 40, disc loss: 0.00021045151772150096, policy loss: 11.067246763212799
Experience 49, Iter 41, disc loss: 0.00017242217336174004, policy loss: 10.902849350076787
Experience 49, Iter 42, disc loss: 0.00013026252509595806, policy loss: 11.315389910925932
Experience 49, Iter 43, disc loss: 0.00014610707402379716, policy loss: 11.059729735175443
Experience 49, Iter 44, disc loss: 0.00017179722905049196, policy loss: 11.113490456229705
Experience 49, Iter 45, disc loss: 0.0001575605899354778, policy loss: 12.066541020926918
Experience 49, Iter 46, disc loss: 0.0001903006817055611, policy loss: 11.115485167581877
Experience 49, Iter 47, disc loss: 0.00019759306042696414, policy loss: 11.037604809568036
Experience 49, Iter 48, disc loss: 0.00022120692798280065, policy loss: 11.577782082081766
Experience 49, Iter 49, disc loss: 0.0002095463750481743, policy loss: 11.750504211710545
Experience 49, Iter 50, disc loss: 0.0002127661527970261, policy loss: 11.695072480159073
Experience 49, Iter 51, disc loss: 0.00017419236322538376, policy loss: 11.084037052672993
Experience 49, Iter 52, disc loss: 0.00014218101874740664, policy loss: 11.574780665042113
Experience 49, Iter 53, disc loss: 0.00023544404010107114, policy loss: 11.094581117466102
Experience 49, Iter 54, disc loss: 0.00018154989122125143, policy loss: 11.83625503551043
Experience 49, Iter 55, disc loss: 0.00021683369022476795, policy loss: 10.472236034917795
Experience 49, Iter 56, disc loss: 0.00018394929339830775, policy loss: 11.170610007307642
Experience 49, Iter 57, disc loss: 0.00014703037510452766, policy loss: 11.3677054998997
Experience 49, Iter 58, disc loss: 0.00016097338247866464, policy loss: 10.670788759977354
Experience 49, Iter 59, disc loss: 0.00015743408934966018, policy loss: 11.710009788174109
Experience 49, Iter 60, disc loss: 0.00019099278253311292, policy loss: 10.816134266167827
Experience 49, Iter 61, disc loss: 0.0001906523507746387, policy loss: 9.980261127026075
Experience 49, Iter 62, disc loss: 0.0001703172936992282, policy loss: 13.126526101288404
Experience 49, Iter 63, disc loss: 0.0001749384880158405, policy loss: 10.903989165805728
Experience 49, Iter 64, disc loss: 0.00012653424776713234, policy loss: 12.047981390461471
Experience 49, Iter 65, disc loss: 0.00011855090221508512, policy loss: 11.996243854286845
Experience 49, Iter 66, disc loss: 0.00018014151455036286, policy loss: 11.232497065780247
Experience 49, Iter 67, disc loss: 0.00023308731117112488, policy loss: 10.614610148026191
Experience 49, Iter 68, disc loss: 0.00018567338712082088, policy loss: 10.633034447202174
Experience 49, Iter 69, disc loss: 0.00016360554056315152, policy loss: 12.462727602465119
Experience 49, Iter 70, disc loss: 0.00014507355546581664, policy loss: 11.41317382719269
Experience 49, Iter 71, disc loss: 0.00011504975718928685, policy loss: 11.545942874625752
Experience 49, Iter 72, disc loss: 0.00011028849006557842, policy loss: 12.378966517757249
Experience 49, Iter 73, disc loss: 0.00014485205830141, policy loss: 11.356277982547605
Experience 49, Iter 74, disc loss: 0.00018871025196568543, policy loss: 10.495456093620163
Experience 49, Iter 75, disc loss: 0.00023520150911029297, policy loss: 11.021778479186061
Experience 49, Iter 76, disc loss: 0.0002344868143707443, policy loss: 12.907204575884744
Experience 49, Iter 77, disc loss: 0.00017463382486025057, policy loss: 13.69785074437037
Experience 49, Iter 78, disc loss: 0.00014285586082134614, policy loss: 11.60456131880537
Experience 49, Iter 79, disc loss: 9.15350398740519e-05, policy loss: 14.602712248559655
Experience 49, Iter 80, disc loss: 9.15845280015748e-05, policy loss: 13.695623931008619
Experience 49, Iter 81, disc loss: 0.00010113983644804223, policy loss: 14.185273032694017
Experience 49, Iter 82, disc loss: 0.00011430283195396297, policy loss: 12.628124493752953
Experience 49, Iter 83, disc loss: 0.00014845620801098527, policy loss: 12.17223670208095
Experience 49, Iter 84, disc loss: 0.00014358678753344787, policy loss: 12.660426003108526
Experience 49, Iter 85, disc loss: 0.00014198196104472544, policy loss: 13.1626510268116
Experience 49, Iter 86, disc loss: 0.00015919877344387968, policy loss: 10.827681879016904
Experience 49, Iter 87, disc loss: 0.00010812174796743059, policy loss: 12.45709558462405
Experience 49, Iter 88, disc loss: 0.00010809536170997758, policy loss: 12.883251260896749
Experience 49, Iter 89, disc loss: 0.0001344797545006738, policy loss: 11.257634676693474
Experience 49, Iter 90, disc loss: 0.0001829812711562616, policy loss: 11.337458432728447
Experience 49, Iter 91, disc loss: 0.00023679157559554847, policy loss: 11.073959581979448
Experience 49, Iter 92, disc loss: 0.0002562123333120914, policy loss: 10.262345607830817
Experience 49, Iter 93, disc loss: 0.00020416189602630365, policy loss: 10.235441335984147
Experience 49, Iter 94, disc loss: 0.00017035456147175863, policy loss: 11.317130146060649
Experience 49, Iter 95, disc loss: 0.00014414721611936715, policy loss: 11.185411479061248
Experience 49, Iter 96, disc loss: 0.00015564764966632374, policy loss: 11.344487825350603
Experience 49, Iter 97, disc loss: 0.00022572069377805768, policy loss: 10.639143251106752
Experience 49, Iter 98, disc loss: 0.00022689794312620655, policy loss: 11.944659148660874
Experience 49, Iter 99, disc loss: 0.0002018290591209931, policy loss: 11.543627083481288
Experience: 50
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.2609],
        [2.0109],
        [0.0456]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0093, 0.1725, 2.0065, 0.0254, 0.0275, 5.6344]],

        [[0.0093, 0.1725, 2.0065, 0.0254, 0.0275, 5.6344]],

        [[0.0093, 0.1725, 2.0065, 0.0254, 0.0275, 5.6344]],

        [[0.0093, 0.1725, 2.0065, 0.0254, 0.0275, 5.6344]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0153, 1.0434, 8.0436, 0.1824], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0153, 1.0434, 8.0436, 0.1824])
N: 500
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2001.0000, 2001.0000, 2001.0000, 2001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.764
Iter 2/2000 - Loss: 3.832
Iter 3/2000 - Loss: 3.513
Iter 4/2000 - Loss: 3.445
Iter 5/2000 - Loss: 3.420
Iter 6/2000 - Loss: 3.275
Iter 7/2000 - Loss: 3.064
Iter 8/2000 - Loss: 2.854
Iter 9/2000 - Loss: 2.663
Iter 10/2000 - Loss: 2.472
Iter 11/2000 - Loss: 2.254
Iter 12/2000 - Loss: 2.000
Iter 13/2000 - Loss: 1.715
Iter 14/2000 - Loss: 1.413
Iter 15/2000 - Loss: 1.101
Iter 16/2000 - Loss: 0.784
Iter 17/2000 - Loss: 0.460
Iter 18/2000 - Loss: 0.130
Iter 19/2000 - Loss: -0.205
Iter 20/2000 - Loss: -0.543
Iter 1981/2000 - Loss: -8.546
Iter 1982/2000 - Loss: -8.546
Iter 1983/2000 - Loss: -8.546
Iter 1984/2000 - Loss: -8.546
Iter 1985/2000 - Loss: -8.546
Iter 1986/2000 - Loss: -8.546
Iter 1987/2000 - Loss: -8.546
Iter 1988/2000 - Loss: -8.546
Iter 1989/2000 - Loss: -8.546
Iter 1990/2000 - Loss: -8.546
Iter 1991/2000 - Loss: -8.546
Iter 1992/2000 - Loss: -8.547
Iter 1993/2000 - Loss: -8.547
Iter 1994/2000 - Loss: -8.547
Iter 1995/2000 - Loss: -8.547
Iter 1996/2000 - Loss: -8.547
Iter 1997/2000 - Loss: -8.547
Iter 1998/2000 - Loss: -8.547
Iter 1999/2000 - Loss: -8.547
Iter 2000/2000 - Loss: -8.547
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.6783,  4.9370, 32.0877,  1.6613, 16.2792, 56.4018]],

        [[14.7701, 27.8477,  7.3403,  1.5990,  0.7215, 22.8286]],

        [[15.3844, 28.7058,  7.5158,  1.0048,  0.5878, 23.4115]],

        [[12.3590, 22.7977, 14.7899,  1.3080,  1.8697, 43.0502]]])
Signal Variance: tensor([ 0.0755,  1.6147, 11.8864,  0.4627])
Estimated target variance: tensor([0.0153, 1.0434, 8.0436, 0.1824])
N: 500
Signal to noise ratio: tensor([16.4051, 70.5703, 74.3167, 42.8119])
Bound on condition number: tensor([ 134564.2243, 2490086.8613, 2761487.9076,  916431.1841])
Policy Optimizer learning rate:
0.00949683362409269
Experience 50, Iter 0, disc loss: 0.0001418586141624297, policy loss: 11.632421502689073
Experience 50, Iter 1, disc loss: 0.00012554377103091548, policy loss: 11.479995488162178
Experience 50, Iter 2, disc loss: 0.00012818013194479256, policy loss: 11.988530762652804
Experience 50, Iter 3, disc loss: 0.00014554944811039693, policy loss: 11.962364607880328
Experience 50, Iter 4, disc loss: 0.00020637777759868747, policy loss: 11.506385017111167
Experience 50, Iter 5, disc loss: 0.00022926556496646262, policy loss: 10.92887509223949
Experience 50, Iter 6, disc loss: 0.0001834787894642324, policy loss: 10.838519234461133
Experience 50, Iter 7, disc loss: 0.0002042130991673266, policy loss: 10.529639267142505
Experience 50, Iter 8, disc loss: 0.00017888977840779487, policy loss: 11.159740456044204
Experience 50, Iter 9, disc loss: 0.00014030129695504956, policy loss: 11.518553143194536
Experience 50, Iter 10, disc loss: 0.0001859506100015832, policy loss: 10.731781230653752
Experience 50, Iter 11, disc loss: 0.0002086456806453373, policy loss: 10.861951797513534
Experience 50, Iter 12, disc loss: 0.00015196391446119716, policy loss: 10.714916878442637
Experience 50, Iter 13, disc loss: 0.00012031259898479362, policy loss: 11.423337493365665
Experience 50, Iter 14, disc loss: 0.00013974995149268615, policy loss: 11.31869097909409
Experience 50, Iter 15, disc loss: 0.00013912229989852289, policy loss: 11.415653589070082
Experience 50, Iter 16, disc loss: 0.000180955271073381, policy loss: 10.652123595394823
Experience 50, Iter 17, disc loss: 0.00017818301733469151, policy loss: 11.656144426366396
Experience 50, Iter 18, disc loss: 0.0001506982853513948, policy loss: 11.142463987328483
Experience 50, Iter 19, disc loss: 0.00017938190555318864, policy loss: 10.497587115489617
Experience 50, Iter 20, disc loss: 0.00013088055267502296, policy loss: 11.147177164975231
Experience 50, Iter 21, disc loss: 0.0001494097612049819, policy loss: 11.042800401032842
Experience 50, Iter 22, disc loss: 0.00014207286636126686, policy loss: 11.366495142592798
Experience 50, Iter 23, disc loss: 0.00016340709665713956, policy loss: 11.420337691340066
Experience 50, Iter 24, disc loss: 0.0001593213771572267, policy loss: 11.415332410943728
Experience 50, Iter 25, disc loss: 0.00014427186668755952, policy loss: 11.048436432594166
Experience 50, Iter 26, disc loss: 0.00013375647148005255, policy loss: 11.341842205648083
Experience 50, Iter 27, disc loss: 0.0001741133463091915, policy loss: 10.859314142326733
Experience 50, Iter 28, disc loss: 0.00022787697528216212, policy loss: 10.299955247232916
Experience 50, Iter 29, disc loss: 0.0002840291079521216, policy loss: 10.216536569808056
Experience 50, Iter 30, disc loss: 0.00023042740939830983, policy loss: 11.057227076611305
Experience 50, Iter 31, disc loss: 0.00014705232047806404, policy loss: 10.812754868649591
Experience 50, Iter 32, disc loss: 0.0001237219216625295, policy loss: 12.00641988527885
Experience 50, Iter 33, disc loss: 0.00016342909915392502, policy loss: 10.923907878333317
Experience 50, Iter 34, disc loss: 0.00018211277751331145, policy loss: 11.10903157908703
Experience 50, Iter 35, disc loss: 0.00016740213819394693, policy loss: 11.40621773051268
Experience 50, Iter 36, disc loss: 0.00016372401562734417, policy loss: 11.479235534036444
Experience 50, Iter 37, disc loss: 0.00016814766704162945, policy loss: 10.761044520610879
Experience 50, Iter 38, disc loss: 0.00019876345033125617, policy loss: 10.465958726950458
Experience 50, Iter 39, disc loss: 0.00016223459844317672, policy loss: 11.441012372475406
Experience 50, Iter 40, disc loss: 0.0001703432701064696, policy loss: 11.868232841607288
Experience 50, Iter 41, disc loss: 0.00014692561285777365, policy loss: 11.287240123348681
Experience 50, Iter 42, disc loss: 0.0001518638636425909, policy loss: 11.150425395175958
Experience 50, Iter 43, disc loss: 0.00016729587130331956, policy loss: 10.412818079416633
Experience 50, Iter 44, disc loss: 0.00017067107628904556, policy loss: 10.934182832046865
Experience 50, Iter 45, disc loss: 0.00016782432059829315, policy loss: 11.749637773810026
Experience 50, Iter 46, disc loss: 0.00013459354960875788, policy loss: 11.761807295090785
Experience 50, Iter 47, disc loss: 0.00020038081549857216, policy loss: 10.858595779169754
Experience 50, Iter 48, disc loss: 0.00013666541702209926, policy loss: 10.983442211540718
Experience 50, Iter 49, disc loss: 0.0001831338986692579, policy loss: 11.636217084932817
Experience 50, Iter 50, disc loss: 0.00016586794217825175, policy loss: 11.261976634228104
Experience 50, Iter 51, disc loss: 0.00015584297900999536, policy loss: 11.137558953893782
Experience 50, Iter 52, disc loss: 0.00013490465179122856, policy loss: 11.248038553817196
Experience 50, Iter 53, disc loss: 0.00013914733813850144, policy loss: 11.19840335672848
Experience 50, Iter 54, disc loss: 0.00022510549542681724, policy loss: 10.49339650216755
Experience 50, Iter 55, disc loss: 0.00020540705554197185, policy loss: 11.597659993059331
Experience 50, Iter 56, disc loss: 0.00019931532668692186, policy loss: 10.555602812488008
Experience 50, Iter 57, disc loss: 0.0001420578486578835, policy loss: 11.373757677916764
Experience 50, Iter 58, disc loss: 0.00011204332298936305, policy loss: 12.34876326292402
Experience 50, Iter 59, disc loss: 0.00012165705847687808, policy loss: 12.109875913507917
Experience 50, Iter 60, disc loss: 0.00016703954963613469, policy loss: 10.969882489792003
Experience 50, Iter 61, disc loss: 0.00020700937024960192, policy loss: 11.259673252090241
Experience 50, Iter 62, disc loss: 0.00019803286888776936, policy loss: 10.233019581563177
Experience 50, Iter 63, disc loss: 0.00014987095550067565, policy loss: 11.277602389032078
Experience 50, Iter 64, disc loss: 0.00013975247787683472, policy loss: 10.570168511688006
Experience 50, Iter 65, disc loss: 0.00015791833634045766, policy loss: 11.064764287116475
Experience 50, Iter 66, disc loss: 0.00016509888138663845, policy loss: 11.805293598159144
Experience 50, Iter 67, disc loss: 0.00021583785246032577, policy loss: 10.249543812817226
Experience 50, Iter 68, disc loss: 0.00021111966900909973, policy loss: 10.250513627447205
Experience 50, Iter 69, disc loss: 0.00022114168785725155, policy loss: 11.072587747752141
Experience 50, Iter 70, disc loss: 0.00021265819328997662, policy loss: 11.258770758299775
Experience 50, Iter 71, disc loss: 0.00019331066570773886, policy loss: 10.718603192264816
Experience 50, Iter 72, disc loss: 0.00018182607991417072, policy loss: 11.549099025019279
Experience 50, Iter 73, disc loss: 0.00015215078396929794, policy loss: 11.557671229894964
Experience 50, Iter 74, disc loss: 0.00013733933779044372, policy loss: 10.721035353263073
Experience 50, Iter 75, disc loss: 0.0001589749287753, policy loss: 10.917873668034158
Experience 50, Iter 76, disc loss: 0.00018480177836174827, policy loss: 10.604820624288962
Experience 50, Iter 77, disc loss: 0.00023475977000712067, policy loss: 11.00372257542781
Experience 50, Iter 78, disc loss: 0.00017734453401274704, policy loss: 11.872598504853555
Experience 50, Iter 79, disc loss: 0.00017159759101635514, policy loss: 10.321885620515898
Experience 50, Iter 80, disc loss: 0.0001466658861020881, policy loss: 10.92303041745264
Experience 50, Iter 81, disc loss: 0.00014297314163302311, policy loss: 10.892347719275937
Experience 50, Iter 82, disc loss: 0.00013249519681300515, policy loss: 11.055982148871554
Experience 50, Iter 83, disc loss: 0.00016919825071754293, policy loss: 11.202930244813668
Experience 50, Iter 84, disc loss: 0.000170181820729495, policy loss: 12.338592375216457
Experience 50, Iter 85, disc loss: 0.00016430039636138223, policy loss: 10.896606643102484
Experience 50, Iter 86, disc loss: 0.00013362014457108426, policy loss: 11.747247623381366
Experience 50, Iter 87, disc loss: 0.0001375538705758106, policy loss: 11.157212806027047
Experience 50, Iter 88, disc loss: 0.00021205838536088184, policy loss: 10.432675460921038
Experience 50, Iter 89, disc loss: 0.00022754923732411507, policy loss: 11.835944893645253
Experience 50, Iter 90, disc loss: 0.00017311111156531795, policy loss: 12.779287008145236
Experience 50, Iter 91, disc loss: 0.0002286600014166789, policy loss: 12.510144190375986
Experience 50, Iter 92, disc loss: 0.00013357678125110542, policy loss: 11.015139128311239
Experience 50, Iter 93, disc loss: 9.971998319559076e-05, policy loss: 13.309458865030921
Experience 50, Iter 94, disc loss: 0.00010177803263732699, policy loss: 12.958358318816948
Experience 50, Iter 95, disc loss: 0.00012471729603579252, policy loss: 11.80878293529417
Experience 50, Iter 96, disc loss: 0.00019133150814915355, policy loss: 10.957849244820405
Experience 50, Iter 97, disc loss: 0.00015091810534121513, policy loss: 13.58936419710226
Experience 50, Iter 98, disc loss: 0.00015232070030135283, policy loss: 13.765600720822231
Experience 50, Iter 99, disc loss: 0.00011342296266031107, policy loss: 12.394096916369996
