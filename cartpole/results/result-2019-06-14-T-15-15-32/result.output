Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.0088],
        [0.8106],
        [0.0199]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]],

        [[0.0738, 0.2625, 0.9079, 0.0149, 0.0060, 0.1471]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0245, 0.0354, 3.2423, 0.0797], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0245, 0.0354, 3.2423, 0.0797])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 6.311
Iter 2/2000 - Loss: 4.160
Iter 3/2000 - Loss: 2.768
Iter 4/2000 - Loss: 2.066
Iter 5/2000 - Loss: 1.792
Iter 6/2000 - Loss: 1.720
Iter 7/2000 - Loss: 1.743
Iter 8/2000 - Loss: 1.805
Iter 9/2000 - Loss: 1.863
Iter 10/2000 - Loss: 1.890
Iter 11/2000 - Loss: 1.891
Iter 12/2000 - Loss: 1.886
Iter 13/2000 - Loss: 1.887
Iter 14/2000 - Loss: 1.894
Iter 15/2000 - Loss: 1.898
Iter 16/2000 - Loss: 1.893
Iter 17/2000 - Loss: 1.877
Iter 18/2000 - Loss: 1.849
Iter 19/2000 - Loss: 1.808
Iter 20/2000 - Loss: 1.751
Iter 1981/2000 - Loss: -0.182
Iter 1982/2000 - Loss: -0.182
Iter 1983/2000 - Loss: -0.182
Iter 1984/2000 - Loss: -0.182
Iter 1985/2000 - Loss: -0.182
Iter 1986/2000 - Loss: -0.183
Iter 1987/2000 - Loss: -0.183
Iter 1988/2000 - Loss: -0.183
Iter 1989/2000 - Loss: -0.183
Iter 1990/2000 - Loss: -0.183
Iter 1991/2000 - Loss: -0.183
Iter 1992/2000 - Loss: -0.183
Iter 1993/2000 - Loss: -0.183
Iter 1994/2000 - Loss: -0.183
Iter 1995/2000 - Loss: -0.183
Iter 1996/2000 - Loss: -0.183
Iter 1997/2000 - Loss: -0.184
Iter 1998/2000 - Loss: -0.184
Iter 1999/2000 - Loss: -0.184
Iter 2000/2000 - Loss: -0.184
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0044],
        [0.0063],
        [0.0001],
        [0.0142]])
Lengthscale: tensor([[[7.3874e-02, 2.6251e-01, 9.0815e-01, 1.4946e-02, 5.9710e-03,
          1.4709e-01]],

        [[7.5174e-02, 2.6308e-01, 9.1033e-01, 1.4962e-02, 6.1360e-03,
          1.4716e-01]],

        [[1.7623e+01, 2.6664e+01, 1.3772e+01, 7.1129e-01, 7.5305e+00,
          6.8021e+00]],

        [[7.3864e-02, 2.6251e-01, 9.0813e-01, 1.4946e-02, 5.9697e-03,
          1.4709e-01]]])
Signal Variance: tensor([0.0176, 0.0255, 4.0857, 0.0576])
Estimated target variance: tensor([0.0245, 0.0354, 3.2423, 0.0797])
N: 10
Signal to noise ratio: tensor([  2.0022,   2.0103, 199.7144,   2.0167])
Bound on condition number: tensor([4.1087e+01, 4.1412e+01, 3.9886e+05, 4.1673e+01])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.3050071883669232, policy loss: 0.8528782229896873
Experience 1, Iter 1, disc loss: 1.2988566392767604, policy loss: 0.8537921210880233
Experience 1, Iter 2, disc loss: 1.301558656186621, policy loss: 0.8450862396580429
Experience 1, Iter 3, disc loss: 1.3098101331490253, policy loss: 0.8266310900261158
Experience 1, Iter 4, disc loss: 1.3123269750681903, policy loss: 0.8200074235172948
Experience 1, Iter 5, disc loss: 1.365256466624749, policy loss: 0.7505643401411702
Experience 1, Iter 6, disc loss: 1.3696014668671626, policy loss: 0.7426120430615325
Experience 1, Iter 7, disc loss: 1.3961557632726138, policy loss: 0.7080602865686119
Experience 1, Iter 8, disc loss: 1.3848683598968936, policy loss: 0.7143733073305555
Experience 1, Iter 9, disc loss: 1.3758985546979159, policy loss: 0.7189401139909468
Experience 1, Iter 10, disc loss: 1.387344836274663, policy loss: 0.6991742300356216
Experience 1, Iter 11, disc loss: 1.3428243291169957, policy loss: 0.7343856741758907
Experience 1, Iter 12, disc loss: 1.3635343873771848, policy loss: 0.7098247513795264
Experience 1, Iter 13, disc loss: 1.3409484986150662, policy loss: 0.7183665702924141
Experience 1, Iter 14, disc loss: 1.3005871270999512, policy loss: 0.7557589778834435
Experience 1, Iter 15, disc loss: 1.2842003252386343, policy loss: 0.7680422748134881
Experience 1, Iter 16, disc loss: 1.2687794319166656, policy loss: 0.7759565026947551
Experience 1, Iter 17, disc loss: 1.2339032885703156, policy loss: 0.8062084881789816
Experience 1, Iter 18, disc loss: 1.2367066402873168, policy loss: 0.7963824792473567
Experience 1, Iter 19, disc loss: 1.2261191429642848, policy loss: 0.8002388725926827
Experience 1, Iter 20, disc loss: 1.2019553021200609, policy loss: 0.824548672996146
Experience 1, Iter 21, disc loss: 1.183564840085892, policy loss: 0.8378692393524984
Experience 1, Iter 22, disc loss: 1.155796557878704, policy loss: 0.8677010369708542
Experience 1, Iter 23, disc loss: 1.1508685523115179, policy loss: 0.8671454971820838
Experience 1, Iter 24, disc loss: 1.1314298337150057, policy loss: 0.8842524448497209
Experience 1, Iter 25, disc loss: 1.1151913173662422, policy loss: 0.8992777884602471
Experience 1, Iter 26, disc loss: 1.0900739358582112, policy loss: 0.9279304103462631
Experience 1, Iter 27, disc loss: 1.0713099725009316, policy loss: 0.94871521138523
Experience 1, Iter 28, disc loss: 1.0516542063283298, policy loss: 0.9683868827748661
Experience 1, Iter 29, disc loss: 1.0163743786657709, policy loss: 1.0230950748686687
Experience 1, Iter 30, disc loss: 1.0051644384381442, policy loss: 1.0308897501390513
Experience 1, Iter 31, disc loss: 0.9934332363464461, policy loss: 1.0393471309405926
Experience 1, Iter 32, disc loss: 0.9792474958662309, policy loss: 1.0563154178867427
Experience 1, Iter 33, disc loss: 0.9545294077607348, policy loss: 1.0967187927697184
Experience 1, Iter 34, disc loss: 0.9220476292659094, policy loss: 1.1521231389526614
Experience 1, Iter 35, disc loss: 0.9310253559791261, policy loss: 1.120889755631325
Experience 1, Iter 36, disc loss: 0.8973203255053578, policy loss: 1.183906860197261
Experience 1, Iter 37, disc loss: 0.8831276020331122, policy loss: 1.2066069824846974
Experience 1, Iter 38, disc loss: 0.8740417979855065, policy loss: 1.2103785327685666
Experience 1, Iter 39, disc loss: 0.875601933238545, policy loss: 1.1933099377014476
Experience 1, Iter 40, disc loss: 0.8418402700382516, policy loss: 1.2594054438213709
Experience 1, Iter 41, disc loss: 0.8611774151228769, policy loss: 1.1916801938104706
Experience 1, Iter 42, disc loss: 0.8687095753364826, policy loss: 1.14233632975923
Experience 1, Iter 43, disc loss: 0.8668333288016778, policy loss: 1.1349956603714788
Experience 1, Iter 44, disc loss: 0.8699390279677325, policy loss: 1.1103616676681387
Experience 1, Iter 45, disc loss: 0.856775339613621, policy loss: 1.1244187914778203
Experience 1, Iter 46, disc loss: 0.836485937036114, policy loss: 1.1629265758289387
Experience 1, Iter 47, disc loss: 0.8440508792634701, policy loss: 1.1336675486932128
Experience 1, Iter 48, disc loss: 0.8156700677684768, policy loss: 1.1850471522080566
Experience 1, Iter 49, disc loss: 0.8148785355201773, policy loss: 1.1813832761070224
Experience 1, Iter 50, disc loss: 0.7799048721503044, policy loss: 1.2576527660963182
Experience 1, Iter 51, disc loss: 0.7897403419659275, policy loss: 1.2166827499489254
Experience 1, Iter 52, disc loss: 0.7747248614788051, policy loss: 1.2459304613702744
Experience 1, Iter 53, disc loss: 0.7559172864465846, policy loss: 1.2826258620941473
Experience 1, Iter 54, disc loss: 0.7424168077901189, policy loss: 1.3055546889522023
Experience 1, Iter 55, disc loss: 0.7313765704913886, policy loss: 1.3243338203176895
Experience 1, Iter 56, disc loss: 0.7188752556856279, policy loss: 1.3440934329121035
Experience 1, Iter 57, disc loss: 0.7010267276292862, policy loss: 1.3849517348985336
Experience 1, Iter 58, disc loss: 0.6846171815138293, policy loss: 1.4211621279892515
Experience 1, Iter 59, disc loss: 0.6762581415907688, policy loss: 1.424434864602563
Experience 1, Iter 60, disc loss: 0.6610673895235513, policy loss: 1.4511860741631502
Experience 1, Iter 61, disc loss: 0.6541878777917147, policy loss: 1.4542849745681221
Experience 1, Iter 62, disc loss: 0.6489999026569692, policy loss: 1.4470247162364596
Experience 1, Iter 63, disc loss: 0.6299889198598054, policy loss: 1.4872583669723118
Experience 1, Iter 64, disc loss: 0.6137681223736213, policy loss: 1.5199920595915133
Experience 1, Iter 65, disc loss: 0.5926170755963803, policy loss: 1.5668148307183727
Experience 1, Iter 66, disc loss: 0.5813831351795156, policy loss: 1.5875028896406114
Experience 1, Iter 67, disc loss: 0.5709305074183301, policy loss: 1.5928334941335143
Experience 1, Iter 68, disc loss: 0.5478859278850796, policy loss: 1.6545274483444885
Experience 1, Iter 69, disc loss: 0.5472916706041502, policy loss: 1.635729169987164
Experience 1, Iter 70, disc loss: 0.5172800862069034, policy loss: 1.7330290961710548
Experience 1, Iter 71, disc loss: 0.5199048044758863, policy loss: 1.6831507458699293
Experience 1, Iter 72, disc loss: 0.5020662727405398, policy loss: 1.72213896123448
Experience 1, Iter 73, disc loss: 0.48361119874851305, policy loss: 1.783365815356659
Experience 1, Iter 74, disc loss: 0.46962425946219555, policy loss: 1.821426327618695
Experience 1, Iter 75, disc loss: 0.4577358232162426, policy loss: 1.848993735061455
Experience 1, Iter 76, disc loss: 0.4520257704312817, policy loss: 1.8405874052756568
Experience 1, Iter 77, disc loss: 0.4314108270390073, policy loss: 1.9100059093227872
Experience 1, Iter 78, disc loss: 0.4111205433682577, policy loss: 1.9829901983008869
Experience 1, Iter 79, disc loss: 0.40797147009891477, policy loss: 1.960458399274299
Experience 1, Iter 80, disc loss: 0.3922456366154551, policy loss: 2.0057486472123056
Experience 1, Iter 81, disc loss: 0.3787887602501335, policy loss: 2.0595291722390736
Experience 1, Iter 82, disc loss: 0.3626012628344717, policy loss: 2.1182946623622287
Experience 1, Iter 83, disc loss: 0.35849354409395734, policy loss: 2.1131053824258226
Experience 1, Iter 84, disc loss: 0.3524294407846874, policy loss: 2.0909013373230456
Experience 1, Iter 85, disc loss: 0.3357495973909042, policy loss: 2.1822498905384204
Experience 1, Iter 86, disc loss: 0.3342629157694394, policy loss: 2.13370003063231
Experience 1, Iter 87, disc loss: 0.31301313920646445, policy loss: 2.2513993477031313
Experience 1, Iter 88, disc loss: 0.3045890762419708, policy loss: 2.2773077606668855
Experience 1, Iter 89, disc loss: 0.29648754765529417, policy loss: 2.295920577098528
Experience 1, Iter 90, disc loss: 0.2952246134119038, policy loss: 2.2419119747742493
Experience 1, Iter 91, disc loss: 0.28668109862227115, policy loss: 2.2780956702764974
Experience 1, Iter 92, disc loss: 0.2758708010193194, policy loss: 2.33635522008881
Experience 1, Iter 93, disc loss: 0.2556727386921971, policy loss: 2.485709314852402
Experience 1, Iter 94, disc loss: 0.25756698836311614, policy loss: 2.3865124081368774
Experience 1, Iter 95, disc loss: 0.2613293317051903, policy loss: 2.30505294034619
Experience 1, Iter 96, disc loss: 0.23797526160865404, policy loss: 2.477478734710736
Experience 1, Iter 97, disc loss: 0.2341010113005487, policy loss: 2.5045743309148882
Experience 1, Iter 98, disc loss: 0.221918929416303, policy loss: 2.5831366152058024
Experience 1, Iter 99, disc loss: 0.22387996460701254, policy loss: 2.4862597681113123
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.1493],
        [1.8023],
        [0.0153]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0474, 0.2427, 0.8328, 0.0123, 0.0040, 2.9596]],

        [[0.0474, 0.2427, 0.8328, 0.0123, 0.0040, 2.9596]],

        [[0.0474, 0.2427, 0.8328, 0.0123, 0.0040, 2.9596]],

        [[0.0474, 0.2427, 0.8328, 0.0123, 0.0040, 2.9596]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0223, 0.5972, 7.2090, 0.0610], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0223, 0.5972, 7.2090, 0.0610])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.849
Iter 2/2000 - Loss: 3.313
Iter 3/2000 - Loss: 3.071
Iter 4/2000 - Loss: 3.112
Iter 5/2000 - Loss: 3.223
Iter 6/2000 - Loss: 3.249
Iter 7/2000 - Loss: 3.203
Iter 8/2000 - Loss: 3.135
Iter 9/2000 - Loss: 3.065
Iter 10/2000 - Loss: 3.002
Iter 11/2000 - Loss: 2.964
Iter 12/2000 - Loss: 2.960
Iter 13/2000 - Loss: 2.974
Iter 14/2000 - Loss: 2.979
Iter 15/2000 - Loss: 2.955
Iter 16/2000 - Loss: 2.905
Iter 17/2000 - Loss: 2.847
Iter 18/2000 - Loss: 2.802
Iter 19/2000 - Loss: 2.776
Iter 20/2000 - Loss: 2.763
Iter 1981/2000 - Loss: -4.015
Iter 1982/2000 - Loss: -4.015
Iter 1983/2000 - Loss: -4.015
Iter 1984/2000 - Loss: -4.015
Iter 1985/2000 - Loss: -4.015
Iter 1986/2000 - Loss: -4.015
Iter 1987/2000 - Loss: -4.015
Iter 1988/2000 - Loss: -4.015
Iter 1989/2000 - Loss: -4.015
Iter 1990/2000 - Loss: -4.015
Iter 1991/2000 - Loss: -4.016
Iter 1992/2000 - Loss: -4.016
Iter 1993/2000 - Loss: -4.016
Iter 1994/2000 - Loss: -4.016
Iter 1995/2000 - Loss: -4.016
Iter 1996/2000 - Loss: -4.016
Iter 1997/2000 - Loss: -4.016
Iter 1998/2000 - Loss: -4.016
Iter 1999/2000 - Loss: -4.016
Iter 2000/2000 - Loss: -4.016
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0008],
        [0.0008],
        [0.0002]])
Lengthscale: tensor([[[19.4896,  5.8860, 59.9271,  5.0942, 15.7664, 20.1129]],

        [[36.9354, 58.7019, 24.4111,  1.9297,  0.9996, 24.0079]],

        [[ 9.6549, 45.6608, 20.1431,  1.0041, 14.4064, 25.0751]],

        [[39.6484, 43.3879, 10.9826,  4.2145,  9.6190, 37.6623]]])
Signal Variance: tensor([ 0.0900,  2.4820, 25.0534,  0.3963])
Estimated target variance: tensor([0.0223, 0.5972, 7.2090, 0.0610])
N: 20
Signal to noise ratio: tensor([ 14.7530,  56.3635, 171.8134,  49.8348])
Bound on condition number: tensor([  4354.0440,  63537.8103, 590397.9654,  49671.2031])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.3800972847632158, policy loss: 1.6126094759105376
Experience 2, Iter 1, disc loss: 0.36594182134744835, policy loss: 1.6385573643766524
Experience 2, Iter 2, disc loss: 0.3653182064363369, policy loss: 1.668048425309375
Experience 2, Iter 3, disc loss: 0.34787747887899545, policy loss: 1.7406318616980614
Experience 2, Iter 4, disc loss: 0.3085280473198144, policy loss: 1.9550350947001571
Experience 2, Iter 5, disc loss: 0.29181830161792544, policy loss: 2.0115274397678244
Experience 2, Iter 6, disc loss: 0.303956729928805, policy loss: 1.8900916534665233
Experience 2, Iter 7, disc loss: 0.2585808096231383, policy loss: 2.190172594699366
Experience 2, Iter 8, disc loss: 0.2653167540137008, policy loss: 2.083394142625867
Experience 2, Iter 9, disc loss: 0.26970315958301594, policy loss: 2.081344734606862
Experience 2, Iter 10, disc loss: 0.2291639225699713, policy loss: 2.323356073792272
Experience 2, Iter 11, disc loss: 0.23124698480414568, policy loss: 2.32866300143903
Experience 2, Iter 12, disc loss: 0.22387805627299573, policy loss: 2.337624999029784
Experience 2, Iter 13, disc loss: 0.22517863253592474, policy loss: 2.283682148636659
Experience 2, Iter 14, disc loss: 0.24753260204786592, policy loss: 2.189224460404203
Experience 2, Iter 15, disc loss: 0.23378732454854012, policy loss: 2.15656938410054
Experience 2, Iter 16, disc loss: 0.23042195523534256, policy loss: 2.296642738493737
Experience 2, Iter 17, disc loss: 0.21081712507435874, policy loss: 2.369147298274523
Experience 2, Iter 18, disc loss: 0.2235258963229197, policy loss: 2.191968359716348
Experience 2, Iter 19, disc loss: 0.22699673362581482, policy loss: 2.164110498102429
Experience 2, Iter 20, disc loss: 0.23105023992211757, policy loss: 2.157785447311498
Experience 2, Iter 21, disc loss: 0.20926728065523578, policy loss: 2.2425809172464337
Experience 2, Iter 22, disc loss: 0.23202239819083115, policy loss: 2.2385891013204446
Experience 2, Iter 23, disc loss: 0.19466932976536017, policy loss: 2.3112346711382408
Experience 2, Iter 24, disc loss: 0.23221173439609896, policy loss: 2.171216577258019
Experience 2, Iter 25, disc loss: 0.22727953522030375, policy loss: 2.2143028047685136
Experience 2, Iter 26, disc loss: 0.2201231476029667, policy loss: 2.2040062538786733
Experience 2, Iter 27, disc loss: 0.1685690414937123, policy loss: 2.505435829597034
Experience 2, Iter 28, disc loss: 0.20573421953399101, policy loss: 2.3008014117867615
Experience 2, Iter 29, disc loss: 0.1833499905860347, policy loss: 2.3049226822166977
Experience 2, Iter 30, disc loss: 0.21906405993697597, policy loss: 2.3557161821949744
Experience 2, Iter 31, disc loss: 0.21154961416622356, policy loss: 2.444521596105488
Experience 2, Iter 32, disc loss: 0.2801003854182564, policy loss: 2.0555586156318433
Experience 2, Iter 33, disc loss: 0.28152020673537825, policy loss: 2.086911394610835
Experience 2, Iter 34, disc loss: 0.267373669847017, policy loss: 2.110901161490797
Experience 2, Iter 35, disc loss: 0.2706790742786107, policy loss: 2.1357164367171064
Experience 2, Iter 36, disc loss: 0.30858099220695795, policy loss: 1.96411767057451
Experience 2, Iter 37, disc loss: 0.2899273110940395, policy loss: 2.0476960805081093
Experience 2, Iter 38, disc loss: 0.33015502526837875, policy loss: 1.9138400769518753
Experience 2, Iter 39, disc loss: 0.3380381874688085, policy loss: 1.866564493626446
Experience 2, Iter 40, disc loss: 0.34723604173764594, policy loss: 1.786409467662566
Experience 2, Iter 41, disc loss: 0.2723831232243568, policy loss: 2.0985783307630985
Experience 2, Iter 42, disc loss: 0.2536503647772907, policy loss: 2.0872782010985027
Experience 2, Iter 43, disc loss: 0.2670221943644959, policy loss: 1.9081010159307876
Experience 2, Iter 44, disc loss: 0.18645740071713116, policy loss: 2.3504926636835592
Experience 2, Iter 45, disc loss: 0.21919324986217908, policy loss: 2.0902977720308615
Experience 2, Iter 46, disc loss: 0.18419784878161202, policy loss: 2.2260673248019955
Experience 2, Iter 47, disc loss: 0.1568592747598426, policy loss: 2.4392201357384655
Experience 2, Iter 48, disc loss: 0.1348891059055946, policy loss: 2.613739587359916
Experience 2, Iter 49, disc loss: 0.1376222102838608, policy loss: 2.5337696500512
Experience 2, Iter 50, disc loss: 0.11926851250877757, policy loss: 2.7393261317441153
Experience 2, Iter 51, disc loss: 0.11181474704784727, policy loss: 2.7809435458747527
Experience 2, Iter 52, disc loss: 0.11345183184980627, policy loss: 2.757060841068366
Experience 2, Iter 53, disc loss: 0.10276499694857723, policy loss: 2.8564458562649064
Experience 2, Iter 54, disc loss: 0.10652822819117991, policy loss: 2.8440016754682
Experience 2, Iter 55, disc loss: 0.10398569419864495, policy loss: 2.8071521431933117
Experience 2, Iter 56, disc loss: 0.1066676398352473, policy loss: 2.8042047655351148
Experience 2, Iter 57, disc loss: 0.09918190643685335, policy loss: 2.9209972971496363
Experience 2, Iter 58, disc loss: 0.09865822088660293, policy loss: 2.916397579809158
Experience 2, Iter 59, disc loss: 0.09229371595639732, policy loss: 3.024404228952032
Experience 2, Iter 60, disc loss: 0.09375645017457827, policy loss: 3.0324556638605173
Experience 2, Iter 61, disc loss: 0.1024200843288822, policy loss: 2.9026581452735876
Experience 2, Iter 62, disc loss: 0.09272673037103056, policy loss: 3.0083981464888856
Experience 2, Iter 63, disc loss: 0.08821888296576949, policy loss: 3.10812557612314
Experience 2, Iter 64, disc loss: 0.08416805889066459, policy loss: 3.1258149444573746
Experience 2, Iter 65, disc loss: 0.07591879955829242, policy loss: 3.1511069046512246
Experience 2, Iter 66, disc loss: 0.07321725856066705, policy loss: 3.2338279488562582
Experience 2, Iter 67, disc loss: 0.07130606932688371, policy loss: 3.2777476474238387
Experience 2, Iter 68, disc loss: 0.0700011080605836, policy loss: 3.2655215947028724
Experience 2, Iter 69, disc loss: 0.06841289989084637, policy loss: 3.267646207991514
Experience 2, Iter 70, disc loss: 0.06465242984611169, policy loss: 3.397338863793478
Experience 2, Iter 71, disc loss: 0.06520656613404151, policy loss: 3.3458287573471517
Experience 2, Iter 72, disc loss: 0.06410586572218145, policy loss: 3.286770897384498
Experience 2, Iter 73, disc loss: 0.06420512830093289, policy loss: 3.3200066367235817
Experience 2, Iter 74, disc loss: 0.059576856940697656, policy loss: 3.4361763471357136
Experience 2, Iter 75, disc loss: 0.06255049835121286, policy loss: 3.3579655034871214
Experience 2, Iter 76, disc loss: 0.058384938165111123, policy loss: 3.4432319147721793
Experience 2, Iter 77, disc loss: 0.05990136969104233, policy loss: 3.3926412424618926
Experience 2, Iter 78, disc loss: 0.05680852652171671, policy loss: 3.46584821459186
Experience 2, Iter 79, disc loss: 0.05898920829228507, policy loss: 3.3242100578767255
Experience 2, Iter 80, disc loss: 0.055374312712137524, policy loss: 3.463284426831765
Experience 2, Iter 81, disc loss: 0.05493810853937252, policy loss: 3.4858119975774877
Experience 2, Iter 82, disc loss: 0.06289945579854322, policy loss: 3.4570340336123584
Experience 2, Iter 83, disc loss: 0.05413427004979537, policy loss: 3.4187938449143096
Experience 2, Iter 84, disc loss: 0.05105427108331731, policy loss: 3.4986504926620183
Experience 2, Iter 85, disc loss: 0.04808447998896244, policy loss: 3.6844392116397047
Experience 2, Iter 86, disc loss: 0.0508442070413479, policy loss: 3.529109702421647
Experience 2, Iter 87, disc loss: 0.04842150788314106, policy loss: 3.582139652904557
Experience 2, Iter 88, disc loss: 0.0496134023120107, policy loss: 3.53227790228655
Experience 2, Iter 89, disc loss: 0.04894882866412447, policy loss: 3.553497637466414
Experience 2, Iter 90, disc loss: 0.05235058225387003, policy loss: 3.390895511700679
Experience 2, Iter 91, disc loss: 0.04898093062728461, policy loss: 3.478437802982529
Experience 2, Iter 92, disc loss: 0.05044355122342189, policy loss: 3.4945973490671127
Experience 2, Iter 93, disc loss: 0.05349589252037293, policy loss: 3.422016802527563
Experience 2, Iter 94, disc loss: 0.04802745222184151, policy loss: 3.576738127030727
Experience 2, Iter 95, disc loss: 0.051788637476805746, policy loss: 3.3983445861497215
Experience 2, Iter 96, disc loss: 0.05360121620409507, policy loss: 3.351992724345874
Experience 2, Iter 97, disc loss: 0.06914913155184108, policy loss: 3.330120690275854
Experience 2, Iter 98, disc loss: 0.05355748463148742, policy loss: 3.3875083745290384
Experience 2, Iter 99, disc loss: 0.05328102916843139, policy loss: 3.4555154284646954
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1988],
        [2.2820],
        [0.0175]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.6406e-02, 2.1457e-01, 9.4691e-01, 1.2221e-02, 3.2558e-03,
          3.8222e+00]],

        [[3.6406e-02, 2.1457e-01, 9.4691e-01, 1.2221e-02, 3.2558e-03,
          3.8222e+00]],

        [[3.6406e-02, 2.1457e-01, 9.4691e-01, 1.2221e-02, 3.2558e-03,
          3.8222e+00]],

        [[3.6406e-02, 2.1457e-01, 9.4691e-01, 1.2221e-02, 3.2558e-03,
          3.8222e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0195, 0.7953, 9.1281, 0.0700], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0195, 0.7953, 9.1281, 0.0700])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.963
Iter 2/2000 - Loss: 3.482
Iter 3/2000 - Loss: 3.353
Iter 4/2000 - Loss: 3.452
Iter 5/2000 - Loss: 3.543
Iter 6/2000 - Loss: 3.515
Iter 7/2000 - Loss: 3.427
Iter 8/2000 - Loss: 3.345
Iter 9/2000 - Loss: 3.287
Iter 10/2000 - Loss: 3.258
Iter 11/2000 - Loss: 3.256
Iter 12/2000 - Loss: 3.260
Iter 13/2000 - Loss: 3.244
Iter 14/2000 - Loss: 3.195
Iter 15/2000 - Loss: 3.123
Iter 16/2000 - Loss: 3.052
Iter 17/2000 - Loss: 3.000
Iter 18/2000 - Loss: 2.965
Iter 19/2000 - Loss: 2.930
Iter 20/2000 - Loss: 2.876
Iter 1981/2000 - Loss: -4.791
Iter 1982/2000 - Loss: -4.791
Iter 1983/2000 - Loss: -4.791
Iter 1984/2000 - Loss: -4.791
Iter 1985/2000 - Loss: -4.791
Iter 1986/2000 - Loss: -4.791
Iter 1987/2000 - Loss: -4.791
Iter 1988/2000 - Loss: -4.791
Iter 1989/2000 - Loss: -4.791
Iter 1990/2000 - Loss: -4.791
Iter 1991/2000 - Loss: -4.791
Iter 1992/2000 - Loss: -4.791
Iter 1993/2000 - Loss: -4.791
Iter 1994/2000 - Loss: -4.792
Iter 1995/2000 - Loss: -4.792
Iter 1996/2000 - Loss: -4.792
Iter 1997/2000 - Loss: -4.792
Iter 1998/2000 - Loss: -4.792
Iter 1999/2000 - Loss: -4.792
Iter 2000/2000 - Loss: -4.792
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[28.6494,  7.7011, 21.9423,  7.3305, 14.3087, 58.3075]],

        [[32.2450, 63.5894, 20.1509,  1.3035,  2.3320, 34.0123]],

        [[12.3461, 51.4605, 18.2121,  0.9355, 11.2484, 27.4114]],

        [[36.7820, 56.7329, 16.2805,  3.9691,  1.9526, 37.7825]]])
Signal Variance: tensor([ 0.1471,  3.6700, 22.3632,  0.6074])
Estimated target variance: tensor([0.0195, 0.7953, 9.1281, 0.0700])
N: 30
Signal to noise ratio: tensor([ 20.4846,  71.2941, 117.8378,  53.9724])
Bound on condition number: tensor([ 12589.5535, 152486.3659, 416573.6201,  87391.6259])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.06358081785596498, policy loss: 3.836228126408475
Experience 3, Iter 1, disc loss: 0.09308314965834386, policy loss: 3.6472584355517768
Experience 3, Iter 2, disc loss: 0.11726325795078552, policy loss: 3.4489384521756
Experience 3, Iter 3, disc loss: 0.6600293982804268, policy loss: 2.164202987879656
Experience 3, Iter 4, disc loss: 0.758606631133572, policy loss: 1.8035760031597028
Experience 3, Iter 5, disc loss: 1.2770880850167061, policy loss: 1.132959876525385
Experience 3, Iter 6, disc loss: 1.1249190397948765, policy loss: 1.449203232298731
Experience 3, Iter 7, disc loss: 0.9508161285177787, policy loss: 1.5343618541458033
Experience 3, Iter 8, disc loss: 1.002520339980139, policy loss: 1.4930976112046082
Experience 3, Iter 9, disc loss: 0.8795790356856316, policy loss: 1.9800837148202128
Experience 3, Iter 10, disc loss: 0.554455232303423, policy loss: 3.1715912661794823
Experience 3, Iter 11, disc loss: 0.4466257110277289, policy loss: 4.383672804032569
Experience 3, Iter 12, disc loss: 0.4771070928042978, policy loss: 3.6185122578178244
Experience 3, Iter 13, disc loss: 0.554352953317167, policy loss: 3.308209356261866
Experience 3, Iter 14, disc loss: 0.35381683683905213, policy loss: 3.2072970236734477
Experience 3, Iter 15, disc loss: 0.31849843435252334, policy loss: 4.4202135063719865
Experience 3, Iter 16, disc loss: 0.11474112876326276, policy loss: 6.1447821014826935
Experience 3, Iter 17, disc loss: 0.10383818673736583, policy loss: 6.15164519290428
Experience 3, Iter 18, disc loss: 0.08202633921200057, policy loss: 6.335150124105914
Experience 3, Iter 19, disc loss: 0.09562513348675888, policy loss: 5.870659726398005
Experience 3, Iter 20, disc loss: 0.09427247455292506, policy loss: 5.573942235596295
Experience 3, Iter 21, disc loss: 0.09911787074538539, policy loss: 5.524692386455096
Experience 3, Iter 22, disc loss: 0.10770099366850905, policy loss: 5.261963243901136
Experience 3, Iter 23, disc loss: 0.09843250266764278, policy loss: 5.435570190836215
Experience 3, Iter 24, disc loss: 0.10412506074666436, policy loss: 4.99307830211611
Experience 3, Iter 25, disc loss: 0.11306886060064801, policy loss: 4.58153074124162
Experience 3, Iter 26, disc loss: 0.10202947596411283, policy loss: 4.763545900436214
Experience 3, Iter 27, disc loss: 0.10384933470218455, policy loss: 4.5195931261127065
Experience 3, Iter 28, disc loss: 0.09416084852004837, policy loss: 4.801476541946867
Experience 3, Iter 29, disc loss: 0.08384616436176662, policy loss: 4.8959269048882605
Experience 3, Iter 30, disc loss: 0.08395301538006782, policy loss: 5.00680322487319
Experience 3, Iter 31, disc loss: 0.0782831768777919, policy loss: 6.005593266769233
Experience 3, Iter 32, disc loss: 0.07278870147175716, policy loss: 6.050036354717452
Experience 3, Iter 33, disc loss: 0.07306459553643123, policy loss: 6.380746244304554
Experience 3, Iter 34, disc loss: 0.0648031318607435, policy loss: 7.43916906492593
Experience 3, Iter 35, disc loss: 0.058201598701990556, policy loss: 7.470972822029907
Experience 3, Iter 36, disc loss: 0.05470253039963366, policy loss: 7.87353984326305
Experience 3, Iter 37, disc loss: 0.06485432422509081, policy loss: 6.660680310583693
Experience 3, Iter 38, disc loss: 0.05463861117749447, policy loss: 7.225501763261924
Experience 3, Iter 39, disc loss: 0.0624662107580247, policy loss: 6.42708331889493
Experience 3, Iter 40, disc loss: 0.08199407863812883, policy loss: 5.185156154956731
Experience 3, Iter 41, disc loss: 0.058045265067357854, policy loss: 5.232970883054199
Experience 3, Iter 42, disc loss: 0.08423682437526311, policy loss: 5.0864200946164715
Experience 3, Iter 43, disc loss: 0.08871889907023983, policy loss: 4.976218435047768
Experience 3, Iter 44, disc loss: 0.08064768394685373, policy loss: 5.15718552809765
Experience 3, Iter 45, disc loss: 0.09148199492128614, policy loss: 5.208941242307301
Experience 3, Iter 46, disc loss: 0.12921480933463797, policy loss: 4.3186096622551915
Experience 3, Iter 47, disc loss: 0.10891395331996861, policy loss: 3.9545394563250142
Experience 3, Iter 48, disc loss: 0.11711805773263739, policy loss: 3.765476319776882
Experience 3, Iter 49, disc loss: 0.12803438163397812, policy loss: 3.9295672833364312
Experience 3, Iter 50, disc loss: 0.15263515316584195, policy loss: 3.2825571711096915
Experience 3, Iter 51, disc loss: 0.14880548054187956, policy loss: 3.204599876322099
Experience 3, Iter 52, disc loss: 0.15787795047758557, policy loss: 3.110441820363443
Experience 3, Iter 53, disc loss: 0.08434445928668077, policy loss: 4.087146308168256
Experience 3, Iter 54, disc loss: 0.0965880080494603, policy loss: 3.729658225128299
Experience 3, Iter 55, disc loss: 0.11802341347366489, policy loss: 3.9224232361972
Experience 3, Iter 56, disc loss: 0.07922132859909219, policy loss: 4.182373096829395
Experience 3, Iter 57, disc loss: 0.06959026320687675, policy loss: 4.599408004168381
Experience 3, Iter 58, disc loss: 0.07648607060897201, policy loss: 4.121734831824872
Experience 3, Iter 59, disc loss: 0.08439323648675608, policy loss: 4.160535397886823
Experience 3, Iter 60, disc loss: 0.057068965746543666, policy loss: 4.70393880845252
Experience 3, Iter 61, disc loss: 0.060302320820488586, policy loss: 4.423246651118815
Experience 3, Iter 62, disc loss: 0.07586207220800158, policy loss: 4.064759056762182
Experience 3, Iter 63, disc loss: 0.07123192326152827, policy loss: 4.143779641363989
Experience 3, Iter 64, disc loss: 0.07012292308651494, policy loss: 4.202342877449822
Experience 3, Iter 65, disc loss: 0.07733898253910143, policy loss: 4.232327925932482
Experience 3, Iter 66, disc loss: 0.06895085151828766, policy loss: 4.217923795022733
Experience 3, Iter 67, disc loss: 0.05913032466758454, policy loss: 4.575106854009148
Experience 3, Iter 68, disc loss: 0.06718632246750147, policy loss: 4.327072368565239
Experience 3, Iter 69, disc loss: 0.04570233979269811, policy loss: 4.407338899847721
Experience 3, Iter 70, disc loss: 0.061156821434052155, policy loss: 4.625887123967561
Experience 3, Iter 71, disc loss: 0.07188351043586638, policy loss: 5.293551000307636
Experience 3, Iter 72, disc loss: 0.08046909241583554, policy loss: 4.866342088826226
Experience 3, Iter 73, disc loss: 0.0668782954080294, policy loss: 5.087314027066617
Experience 3, Iter 74, disc loss: 0.046926566052138144, policy loss: 5.4617434324245036
Experience 3, Iter 75, disc loss: 0.06364802055517174, policy loss: 5.345585380844037
Experience 3, Iter 76, disc loss: 0.05580096490604255, policy loss: 5.80121815972287
Experience 3, Iter 77, disc loss: 0.04706303512977457, policy loss: 5.750192268455395
Experience 3, Iter 78, disc loss: 0.04116001885540978, policy loss: 5.459636719088931
Experience 3, Iter 79, disc loss: 0.04005200504325953, policy loss: 5.8100738442785556
Experience 3, Iter 80, disc loss: 0.02950170789443701, policy loss: 6.322958576326629
Experience 3, Iter 81, disc loss: 0.02655417094285456, policy loss: 6.955494007055309
Experience 3, Iter 82, disc loss: 0.02802823896911448, policy loss: 6.27122738041694
Experience 3, Iter 83, disc loss: 0.04055406758348941, policy loss: 6.440617722057386
Experience 3, Iter 84, disc loss: 0.03842642403157406, policy loss: 5.312249852215661
Experience 3, Iter 85, disc loss: 0.040818921235620254, policy loss: 6.380771609912828
Experience 3, Iter 86, disc loss: 0.029210434349248093, policy loss: 6.559730401187258
Experience 3, Iter 87, disc loss: 0.023391973335068494, policy loss: 7.801479750433342
Experience 3, Iter 88, disc loss: 0.023932262131421616, policy loss: 8.07585788998086
Experience 3, Iter 89, disc loss: 0.016666882462957177, policy loss: 8.801328842010104
Experience 3, Iter 90, disc loss: 0.01651348396957454, policy loss: 8.659463287933267
Experience 3, Iter 91, disc loss: 0.0178241193660846, policy loss: 8.229466867920541
Experience 3, Iter 92, disc loss: 0.020462760246628343, policy loss: 8.305044119228844
Experience 3, Iter 93, disc loss: 0.024766278347136857, policy loss: 7.747797072357631
Experience 3, Iter 94, disc loss: 0.02261856406062411, policy loss: 7.255813324840614
Experience 3, Iter 95, disc loss: 0.023903869219433337, policy loss: 7.199534613487948
Experience 3, Iter 96, disc loss: 0.024209274751698612, policy loss: 6.884517217990362
Experience 3, Iter 97, disc loss: 0.0282033924928858, policy loss: 6.3956444168845605
Experience 3, Iter 98, disc loss: 0.029068629799515945, policy loss: 5.577251699702961
Experience 3, Iter 99, disc loss: 0.026961846766307272, policy loss: 5.919981902355414
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2090],
        [2.2685],
        [0.0215]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.0999e-02, 2.0088e-01, 1.1202e+00, 1.4507e-02, 3.9638e-03,
          4.4442e+00]],

        [[4.0999e-02, 2.0088e-01, 1.1202e+00, 1.4507e-02, 3.9638e-03,
          4.4442e+00]],

        [[4.0999e-02, 2.0088e-01, 1.1202e+00, 1.4507e-02, 3.9638e-03,
          4.4442e+00]],

        [[4.0999e-02, 2.0088e-01, 1.1202e+00, 1.4507e-02, 3.9638e-03,
          4.4442e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0168, 0.8361, 9.0740, 0.0860], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0168, 0.8361, 9.0740, 0.0860])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.254
Iter 2/2000 - Loss: 3.620
Iter 3/2000 - Loss: 3.401
Iter 4/2000 - Loss: 3.499
Iter 5/2000 - Loss: 3.631
Iter 6/2000 - Loss: 3.640
Iter 7/2000 - Loss: 3.556
Iter 8/2000 - Loss: 3.450
Iter 9/2000 - Loss: 3.363
Iter 10/2000 - Loss: 3.305
Iter 11/2000 - Loss: 3.275
Iter 12/2000 - Loss: 3.269
Iter 13/2000 - Loss: 3.268
Iter 14/2000 - Loss: 3.251
Iter 15/2000 - Loss: 3.203
Iter 16/2000 - Loss: 3.127
Iter 17/2000 - Loss: 3.037
Iter 18/2000 - Loss: 2.952
Iter 19/2000 - Loss: 2.882
Iter 20/2000 - Loss: 2.825
Iter 1981/2000 - Loss: -5.116
Iter 1982/2000 - Loss: -5.116
Iter 1983/2000 - Loss: -5.116
Iter 1984/2000 - Loss: -5.116
Iter 1985/2000 - Loss: -5.116
Iter 1986/2000 - Loss: -5.116
Iter 1987/2000 - Loss: -5.117
Iter 1988/2000 - Loss: -5.117
Iter 1989/2000 - Loss: -5.117
Iter 1990/2000 - Loss: -5.117
Iter 1991/2000 - Loss: -5.117
Iter 1992/2000 - Loss: -5.117
Iter 1993/2000 - Loss: -5.117
Iter 1994/2000 - Loss: -5.117
Iter 1995/2000 - Loss: -5.117
Iter 1996/2000 - Loss: -5.117
Iter 1997/2000 - Loss: -5.117
Iter 1998/2000 - Loss: -5.117
Iter 1999/2000 - Loss: -5.117
Iter 2000/2000 - Loss: -5.117
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0025],
        [0.0002]])
Lengthscale: tensor([[[21.9260,  9.4209, 35.7211,  8.4944, 13.5129, 66.3202]],

        [[32.1596, 56.3235, 13.1806,  1.2625,  8.9167, 34.7072]],

        [[32.6284, 57.7669, 11.6249,  1.1262,  2.1354, 24.9114]],

        [[32.9334, 47.6171, 19.0992,  4.6887,  1.8719, 38.2833]]])
Signal Variance: tensor([ 0.1950,  3.4511, 21.9434,  0.6900])
Estimated target variance: tensor([0.0168, 0.8361, 9.0740, 0.0860])
N: 40
Signal to noise ratio: tensor([23.6219, 82.8171, 93.4110, 54.7130])
Bound on condition number: tensor([ 22320.8106, 274347.8702, 349025.9172, 119741.6123])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.02288705333806533, policy loss: 6.277703641801905
Experience 4, Iter 1, disc loss: 0.026304161533102263, policy loss: 6.075079548770141
Experience 4, Iter 2, disc loss: 0.02754506857919967, policy loss: 5.7233615538143905
Experience 4, Iter 3, disc loss: 0.02830859241324587, policy loss: 5.334391472448894
Experience 4, Iter 4, disc loss: 0.0249726874225935, policy loss: 5.482743098573884
Experience 4, Iter 5, disc loss: 0.02951175164123279, policy loss: 6.0115101533493664
Experience 4, Iter 6, disc loss: 0.03508145012503791, policy loss: 4.849824067973281
Experience 4, Iter 7, disc loss: 0.02600860115284713, policy loss: 5.268399712855253
Experience 4, Iter 8, disc loss: 0.021870688540669876, policy loss: 6.011247612589666
Experience 4, Iter 9, disc loss: 0.027205918411475648, policy loss: 5.269347141077219
Experience 4, Iter 10, disc loss: 0.027930145200750628, policy loss: 5.068781312207044
Experience 4, Iter 11, disc loss: 0.025056436100204298, policy loss: 5.359672685573636
Experience 4, Iter 12, disc loss: 0.026863581716550476, policy loss: 5.741933284742107
Experience 4, Iter 13, disc loss: 0.024373116583147208, policy loss: 5.527046411346785
Experience 4, Iter 14, disc loss: 0.029928620087959136, policy loss: 5.870375558503156
Experience 4, Iter 15, disc loss: 0.026742558233593578, policy loss: 6.274554222086057
Experience 4, Iter 16, disc loss: 0.02259361592293265, policy loss: 5.729163626843748
Experience 4, Iter 17, disc loss: 0.02484067893840379, policy loss: 5.811884179625108
Experience 4, Iter 18, disc loss: 0.029720530501346563, policy loss: 5.117484465242636
Experience 4, Iter 19, disc loss: 0.02918746591698903, policy loss: 5.689736561475818
Experience 4, Iter 20, disc loss: 0.024133617325244553, policy loss: 6.1066652765861775
Experience 4, Iter 21, disc loss: 0.02280584497365896, policy loss: 5.870144186058831
Experience 4, Iter 22, disc loss: 0.023051819370468577, policy loss: 5.924833004883345
Experience 4, Iter 23, disc loss: 0.019892238267217276, policy loss: 6.568013333392583
Experience 4, Iter 24, disc loss: 0.024360406442255053, policy loss: 6.195983020034102
Experience 4, Iter 25, disc loss: 0.025306849658846775, policy loss: 6.142475401582951
Experience 4, Iter 26, disc loss: 0.025687384647133896, policy loss: 5.861213523577495
Experience 4, Iter 27, disc loss: 0.026988344180483983, policy loss: 5.676553209681649
Experience 4, Iter 28, disc loss: 0.019239907537253673, policy loss: 6.3191179060444895
Experience 4, Iter 29, disc loss: 0.0233678789572608, policy loss: 6.252582535550214
Experience 4, Iter 30, disc loss: 0.023877483028204242, policy loss: 6.45977314982174
Experience 4, Iter 31, disc loss: 0.024341249448065455, policy loss: 5.869300423557981
Experience 4, Iter 32, disc loss: 0.023057728318700295, policy loss: 5.849518429539447
Experience 4, Iter 33, disc loss: 0.022401637179153253, policy loss: 6.213897279663101
Experience 4, Iter 34, disc loss: 0.020348595297717847, policy loss: 6.658930257562066
Experience 4, Iter 35, disc loss: 0.020573824567231037, policy loss: 6.2424306135271195
Experience 4, Iter 36, disc loss: 0.020656373488232634, policy loss: 6.216146779098807
Experience 4, Iter 37, disc loss: 0.020058433788030763, policy loss: 6.530526784267031
Experience 4, Iter 38, disc loss: 0.02655911377982826, policy loss: 5.5288487199241505
Experience 4, Iter 39, disc loss: 0.018233838850341688, policy loss: 6.2739952210484855
Experience 4, Iter 40, disc loss: 0.023364007618015784, policy loss: 6.094602107769999
Experience 4, Iter 41, disc loss: 0.02102336613322974, policy loss: 6.725409940751115
Experience 4, Iter 42, disc loss: 0.024809309575514908, policy loss: 6.144973860211534
Experience 4, Iter 43, disc loss: 0.018227443999138733, policy loss: 6.84743827052529
Experience 4, Iter 44, disc loss: 0.01906304619260728, policy loss: 5.9748756263466944
Experience 4, Iter 45, disc loss: 0.021381702988099535, policy loss: 6.533778696290187
Experience 4, Iter 46, disc loss: 0.01819241773679049, policy loss: 6.449393536625686
Experience 4, Iter 47, disc loss: 0.019751089632243937, policy loss: 6.42293274669837
Experience 4, Iter 48, disc loss: 0.014848987995120248, policy loss: 6.975498698103721
Experience 4, Iter 49, disc loss: 0.018611588985986856, policy loss: 6.7183671278269514
Experience 4, Iter 50, disc loss: 0.019856887311689327, policy loss: 6.56321190644338
Experience 4, Iter 51, disc loss: 0.01692753840061836, policy loss: 6.553662571365136
Experience 4, Iter 52, disc loss: 0.015738666837430262, policy loss: 6.860466299726841
Experience 4, Iter 53, disc loss: 0.020983499863530614, policy loss: 6.3483148967379
Experience 4, Iter 54, disc loss: 0.016779974182577262, policy loss: 6.732981081829112
Experience 4, Iter 55, disc loss: 0.01652328005826039, policy loss: 6.669143683516488
Experience 4, Iter 56, disc loss: 0.015705059594512332, policy loss: 6.689427817934557
Experience 4, Iter 57, disc loss: 0.01460986898146523, policy loss: 6.907324407141491
Experience 4, Iter 58, disc loss: 0.01853887920773664, policy loss: 6.4284799904324395
Experience 4, Iter 59, disc loss: 0.01605556253067296, policy loss: 7.323354017438495
Experience 4, Iter 60, disc loss: 0.017523346186705785, policy loss: 6.934524529436346
Experience 4, Iter 61, disc loss: 0.01665834537420363, policy loss: 6.526871639942598
Experience 4, Iter 62, disc loss: 0.015732263164706953, policy loss: 6.582972820493093
Experience 4, Iter 63, disc loss: 0.015727785277010048, policy loss: 6.648711956750197
Experience 4, Iter 64, disc loss: 0.015502834167247325, policy loss: 6.838673435562107
Experience 4, Iter 65, disc loss: 0.01584534304805532, policy loss: 7.221696085726551
Experience 4, Iter 66, disc loss: 0.015664063990806026, policy loss: 6.447771386712059
Experience 4, Iter 67, disc loss: 0.015363331093267901, policy loss: 6.462285951622029
Experience 4, Iter 68, disc loss: 0.014801115580904543, policy loss: 6.579071600812809
Experience 4, Iter 69, disc loss: 0.012548406827724075, policy loss: 7.755627514042606
Experience 4, Iter 70, disc loss: 0.01245031843594047, policy loss: 7.035405482084672
Experience 4, Iter 71, disc loss: 0.017804480107392093, policy loss: 6.648806004005292
Experience 4, Iter 72, disc loss: 0.014440359468890999, policy loss: 6.942987456986592
Experience 4, Iter 73, disc loss: 0.013044062710043013, policy loss: 6.966249065431818
Experience 4, Iter 74, disc loss: 0.014965396009598031, policy loss: 7.026700111020592
Experience 4, Iter 75, disc loss: 0.011713428628161566, policy loss: 7.258681160340938
Experience 4, Iter 76, disc loss: 0.015222901169779531, policy loss: 6.873533579996823
Experience 4, Iter 77, disc loss: 0.015508380586448267, policy loss: 7.3040009605744585
Experience 4, Iter 78, disc loss: 0.013343975967331237, policy loss: 6.768271021885484
Experience 4, Iter 79, disc loss: 0.011632707039282623, policy loss: 7.514162912031521
Experience 4, Iter 80, disc loss: 0.01313310122035316, policy loss: 7.03327566571356
Experience 4, Iter 81, disc loss: 0.014700363377022517, policy loss: 7.386233644147498
Experience 4, Iter 82, disc loss: 0.010948620018715943, policy loss: 8.12717829501755
Experience 4, Iter 83, disc loss: 0.016302180301195825, policy loss: 6.657261873052878
Experience 4, Iter 84, disc loss: 0.011475142891862633, policy loss: 7.233838200185721
Experience 4, Iter 85, disc loss: 0.011730464393067805, policy loss: 7.0706933714562465
Experience 4, Iter 86, disc loss: 0.016784814473924742, policy loss: 7.23700494081565
Experience 4, Iter 87, disc loss: 0.0111677484304183, policy loss: 8.245092441651138
Experience 4, Iter 88, disc loss: 0.01350929131220069, policy loss: 7.1440702257760185
Experience 4, Iter 89, disc loss: 0.01083160502036197, policy loss: 6.958980051878687
Experience 4, Iter 90, disc loss: 0.01405385822281999, policy loss: 6.154771771432004
Experience 4, Iter 91, disc loss: 0.012292173708156901, policy loss: 7.123003837167964
Experience 4, Iter 92, disc loss: 0.013666013256083112, policy loss: 6.928319461321143
Experience 4, Iter 93, disc loss: 0.010817848520697018, policy loss: 7.21914967674682
Experience 4, Iter 94, disc loss: 0.01337215871789784, policy loss: 6.989080519832355
Experience 4, Iter 95, disc loss: 0.012641498139066273, policy loss: 7.402513948551363
Experience 4, Iter 96, disc loss: 0.011837639636275708, policy loss: 7.13835292969754
Experience 4, Iter 97, disc loss: 0.014298792485949719, policy loss: 6.4409047521131555
Experience 4, Iter 98, disc loss: 0.010994198365625915, policy loss: 7.408565008701778
Experience 4, Iter 99, disc loss: 0.01332250467093174, policy loss: 7.687591215710824
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2384],
        [2.5171],
        [0.0183]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.7287e-02, 2.0214e-01, 9.9815e-01, 1.3377e-02, 3.5155e-03,
          4.7824e+00]],

        [[3.7287e-02, 2.0214e-01, 9.9815e-01, 1.3377e-02, 3.5155e-03,
          4.7824e+00]],

        [[3.7287e-02, 2.0214e-01, 9.9815e-01, 1.3377e-02, 3.5155e-03,
          4.7824e+00]],

        [[3.7287e-02, 2.0214e-01, 9.9815e-01, 1.3377e-02, 3.5155e-03,
          4.7824e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0168,  0.9538, 10.0686,  0.0733], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0168,  0.9538, 10.0686,  0.0733])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.069
Iter 2/2000 - Loss: 3.586
Iter 3/2000 - Loss: 3.463
Iter 4/2000 - Loss: 3.583
Iter 5/2000 - Loss: 3.665
Iter 6/2000 - Loss: 3.608
Iter 7/2000 - Loss: 3.493
Iter 8/2000 - Loss: 3.402
Iter 9/2000 - Loss: 3.358
Iter 10/2000 - Loss: 3.347
Iter 11/2000 - Loss: 3.343
Iter 12/2000 - Loss: 3.325
Iter 13/2000 - Loss: 3.279
Iter 14/2000 - Loss: 3.208
Iter 15/2000 - Loss: 3.122
Iter 16/2000 - Loss: 3.036
Iter 17/2000 - Loss: 2.958
Iter 18/2000 - Loss: 2.885
Iter 19/2000 - Loss: 2.806
Iter 20/2000 - Loss: 2.705
Iter 1981/2000 - Loss: -5.877
Iter 1982/2000 - Loss: -5.877
Iter 1983/2000 - Loss: -5.877
Iter 1984/2000 - Loss: -5.877
Iter 1985/2000 - Loss: -5.877
Iter 1986/2000 - Loss: -5.877
Iter 1987/2000 - Loss: -5.877
Iter 1988/2000 - Loss: -5.877
Iter 1989/2000 - Loss: -5.877
Iter 1990/2000 - Loss: -5.877
Iter 1991/2000 - Loss: -5.877
Iter 1992/2000 - Loss: -5.877
Iter 1993/2000 - Loss: -5.877
Iter 1994/2000 - Loss: -5.878
Iter 1995/2000 - Loss: -5.878
Iter 1996/2000 - Loss: -5.878
Iter 1997/2000 - Loss: -5.878
Iter 1998/2000 - Loss: -5.878
Iter 1999/2000 - Loss: -5.878
Iter 2000/2000 - Loss: -5.878
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[18.9555, 10.3812, 29.6872,  6.4321, 14.7737, 65.9562]],

        [[31.1526, 52.5086, 13.6633,  1.4058,  4.6220, 35.7028]],

        [[27.3063, 54.0692, 11.7662,  1.1211,  1.9353, 26.5825]],

        [[30.7599, 47.4998, 15.4270,  3.4174,  1.9387, 32.1778]]])
Signal Variance: tensor([ 0.2012,  3.7504, 24.6544,  0.4388])
Estimated target variance: tensor([ 0.0168,  0.9538, 10.0686,  0.0733])
N: 50
Signal to noise ratio: tensor([ 26.3011,  91.3889, 112.6559,  42.4946])
Bound on condition number: tensor([ 34588.3242, 417597.1545, 634568.6348,  90290.3856])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.016711743211809935, policy loss: 6.018114890963812
Experience 5, Iter 1, disc loss: 0.011733209768613896, policy loss: 7.127998859757724
Experience 5, Iter 2, disc loss: 0.010975278756031503, policy loss: 7.331578541788792
Experience 5, Iter 3, disc loss: 0.012938192521323172, policy loss: 6.68788391784552
Experience 5, Iter 4, disc loss: 0.01605470664763389, policy loss: 6.480003510637682
Experience 5, Iter 5, disc loss: 0.012787829777135315, policy loss: 7.306131626440772
Experience 5, Iter 6, disc loss: 0.013666435278844875, policy loss: 6.6197342077136785
Experience 5, Iter 7, disc loss: 0.013835811518844516, policy loss: 7.105341855419571
Experience 5, Iter 8, disc loss: 0.012559826777833528, policy loss: 7.607066287710579
Experience 5, Iter 9, disc loss: 0.010023524540951583, policy loss: 7.159370965246669
Experience 5, Iter 10, disc loss: 0.012079987499289443, policy loss: 6.525392285446008
Experience 5, Iter 11, disc loss: 0.012189099381547632, policy loss: 7.281384011281352
Experience 5, Iter 12, disc loss: 0.012266337957749314, policy loss: 7.001827612866462
Experience 5, Iter 13, disc loss: 0.014196463154683751, policy loss: 7.576572512036556
Experience 5, Iter 14, disc loss: 0.011893589999057978, policy loss: 6.389443397709462
Experience 5, Iter 15, disc loss: 0.011211084400255663, policy loss: 7.248354395143877
Experience 5, Iter 16, disc loss: 0.00870888076102914, policy loss: 8.000426971838916
Experience 5, Iter 17, disc loss: 0.01150623236770011, policy loss: 7.0210091162593065
Experience 5, Iter 18, disc loss: 0.010225511296297026, policy loss: 7.134366221377608
Experience 5, Iter 19, disc loss: 0.011880707231609073, policy loss: 7.951373584594563
Experience 5, Iter 20, disc loss: 0.012572907588061817, policy loss: 7.843816151524241
Experience 5, Iter 21, disc loss: 0.010062915181564283, policy loss: 7.240575097263919
Experience 5, Iter 22, disc loss: 0.010873726547316786, policy loss: 7.872190129073102
Experience 5, Iter 23, disc loss: 0.009599224637204326, policy loss: 7.806023949124514
Experience 5, Iter 24, disc loss: 0.011253664599593274, policy loss: 7.452357855464664
Experience 5, Iter 25, disc loss: 0.009283671705647428, policy loss: 7.692657612625797
Experience 5, Iter 26, disc loss: 0.008923296427724893, policy loss: 8.015222485646639
Experience 5, Iter 27, disc loss: 0.012670298379175323, policy loss: 8.581067690245735
Experience 5, Iter 28, disc loss: 0.009555054853841153, policy loss: 7.225614268087936
Experience 5, Iter 29, disc loss: 0.012623396573427849, policy loss: 7.2586376925207565
Experience 5, Iter 30, disc loss: 0.015660359198206204, policy loss: 7.136191663016791
Experience 5, Iter 31, disc loss: 0.010725645344582178, policy loss: 7.3618484240704545
Experience 5, Iter 32, disc loss: 0.010237551859985254, policy loss: 7.892646422324885
Experience 5, Iter 33, disc loss: 0.014032015119629666, policy loss: 6.753829098163323
Experience 5, Iter 34, disc loss: 0.007428759200121108, policy loss: 8.50034069052483
Experience 5, Iter 35, disc loss: 0.009877802325784599, policy loss: 6.814437598410251
Experience 5, Iter 36, disc loss: 0.008870450191100887, policy loss: 7.338384120939562
Experience 5, Iter 37, disc loss: 0.007514678484892578, policy loss: 8.049647044517943
Experience 5, Iter 38, disc loss: 0.009766829384321354, policy loss: 7.014496553125591
Experience 5, Iter 39, disc loss: 0.009801567601701816, policy loss: 7.413636896398565
Experience 5, Iter 40, disc loss: 0.009862125028206822, policy loss: 7.340877769361783
Experience 5, Iter 41, disc loss: 0.01022934733062332, policy loss: 7.290666218617156
Experience 5, Iter 42, disc loss: 0.011334733981412368, policy loss: 7.044892476043695
Experience 5, Iter 43, disc loss: 0.01051324639384027, policy loss: 7.0059858698810435
Experience 5, Iter 44, disc loss: 0.010776589485450272, policy loss: 7.160509677529326
Experience 5, Iter 45, disc loss: 0.010381714715071348, policy loss: 6.698936646606157
Experience 5, Iter 46, disc loss: 0.01085398906980865, policy loss: 7.02889054446311
Experience 5, Iter 47, disc loss: 0.011432552868684682, policy loss: 7.683280453681171
Experience 5, Iter 48, disc loss: 0.00935244604112558, policy loss: 7.920927989350488
Experience 5, Iter 49, disc loss: 0.011555730428966359, policy loss: 6.966703597398876
Experience 5, Iter 50, disc loss: 0.01127194205844758, policy loss: 7.479959783215137
Experience 5, Iter 51, disc loss: 0.009949603033700922, policy loss: 7.319934640613814
Experience 5, Iter 52, disc loss: 0.009168164088097465, policy loss: 7.358034661722481
Experience 5, Iter 53, disc loss: 0.00981971275603806, policy loss: 7.183072932790878
Experience 5, Iter 54, disc loss: 0.00877207715161997, policy loss: 7.72307080791294
Experience 5, Iter 55, disc loss: 0.007579090767755269, policy loss: 7.703803622045395
Experience 5, Iter 56, disc loss: 0.010011975807440962, policy loss: 7.724461832743703
Experience 5, Iter 57, disc loss: 0.010654139561023925, policy loss: 7.657894030605335
Experience 5, Iter 58, disc loss: 0.008781377353036857, policy loss: 7.904829217950038
Experience 5, Iter 59, disc loss: 0.010624360990625647, policy loss: 7.703372948643666
Experience 5, Iter 60, disc loss: 0.011658842126417842, policy loss: 7.076675182663265
Experience 5, Iter 61, disc loss: 0.008408888885248465, policy loss: 7.536906773108685
Experience 5, Iter 62, disc loss: 0.009612789711041595, policy loss: 7.326761129664803
Experience 5, Iter 63, disc loss: 0.007727433385251708, policy loss: 8.236702882392654
Experience 5, Iter 64, disc loss: 0.007893983642122064, policy loss: 7.485317719274208
Experience 5, Iter 65, disc loss: 0.00839778763840103, policy loss: 7.363098433364492
Experience 5, Iter 66, disc loss: 0.008305118882776952, policy loss: 7.276513992372851
Experience 5, Iter 67, disc loss: 0.007850783795932542, policy loss: 7.760561618650777
Experience 5, Iter 68, disc loss: 0.007864645834410657, policy loss: 7.2604748713466325
Experience 5, Iter 69, disc loss: 0.010384322385712541, policy loss: 6.719063785655993
Experience 5, Iter 70, disc loss: 0.0066394733395783966, policy loss: 8.940102923466867
Experience 5, Iter 71, disc loss: 0.00971775317423556, policy loss: 7.526846194070002
Experience 5, Iter 72, disc loss: 0.006728959718035074, policy loss: 8.525983928256391
Experience 5, Iter 73, disc loss: 0.009075699625262492, policy loss: 7.351442054416552
Experience 5, Iter 74, disc loss: 0.007792657873339263, policy loss: 7.758330011548588
Experience 5, Iter 75, disc loss: 0.008148287154499724, policy loss: 7.388760260509972
Experience 5, Iter 76, disc loss: 0.008513701226947466, policy loss: 7.171389698603459
Experience 5, Iter 77, disc loss: 0.009669909525499183, policy loss: 8.075750430535166
Experience 5, Iter 78, disc loss: 0.007484008946442211, policy loss: 6.976259979307015
Experience 5, Iter 79, disc loss: 0.00932077538154633, policy loss: 6.862254965219712
Experience 5, Iter 80, disc loss: 0.010343292831902166, policy loss: 7.216075345426236
Experience 5, Iter 81, disc loss: 0.009060998141407912, policy loss: 7.948771346668821
Experience 5, Iter 82, disc loss: 0.006059516450653021, policy loss: 7.928285989313247
Experience 5, Iter 83, disc loss: 0.008239441472330963, policy loss: 7.6145378412326075
Experience 5, Iter 84, disc loss: 0.007338061356767703, policy loss: 8.012727942733532
Experience 5, Iter 85, disc loss: 0.012002022195041043, policy loss: 7.925506098449201
Experience 5, Iter 86, disc loss: 0.007248480902345784, policy loss: 7.484869789740704
Experience 5, Iter 87, disc loss: 0.00802829543057147, policy loss: 7.715259840999587
Experience 5, Iter 88, disc loss: 0.006495372888778545, policy loss: 8.464703625712238
Experience 5, Iter 89, disc loss: 0.006841916538825037, policy loss: 8.154498808804382
Experience 5, Iter 90, disc loss: 0.006467990986677983, policy loss: 8.32346935202226
Experience 5, Iter 91, disc loss: 0.008300767753503701, policy loss: 7.815710455153885
Experience 5, Iter 92, disc loss: 0.007397265117429214, policy loss: 7.916115678204301
Experience 5, Iter 93, disc loss: 0.00790490090475602, policy loss: 7.97662752308362
Experience 5, Iter 94, disc loss: 0.007316814245911973, policy loss: 7.687570603987485
Experience 5, Iter 95, disc loss: 0.007232356972935696, policy loss: 7.8594740292704595
Experience 5, Iter 96, disc loss: 0.012698847478642491, policy loss: 7.665305315747036
Experience 5, Iter 97, disc loss: 0.007430094475600193, policy loss: 7.10921570708318
Experience 5, Iter 98, disc loss: 0.005641417111259244, policy loss: 7.802483158365074
Experience 5, Iter 99, disc loss: 0.006861619634002978, policy loss: 8.688915681320317
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0065],
        [0.2092],
        [2.1804],
        [0.0168]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.0072e-02, 2.8888e-01, 9.0165e-01, 1.3733e-02, 3.6544e-03,
          4.4046e+00]],

        [[5.0072e-02, 2.8888e-01, 9.0165e-01, 1.3733e-02, 3.6544e-03,
          4.4046e+00]],

        [[5.0072e-02, 2.8888e-01, 9.0165e-01, 1.3733e-02, 3.6544e-03,
          4.4046e+00]],

        [[5.0072e-02, 2.8888e-01, 9.0165e-01, 1.3733e-02, 3.6544e-03,
          4.4046e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0260, 0.8368, 8.7217, 0.0673], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0260, 0.8368, 8.7217, 0.0673])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.102
Iter 2/2000 - Loss: 3.685
Iter 3/2000 - Loss: 3.488
Iter 4/2000 - Loss: 3.534
Iter 5/2000 - Loss: 3.627
Iter 6/2000 - Loss: 3.629
Iter 7/2000 - Loss: 3.561
Iter 8/2000 - Loss: 3.477
Iter 9/2000 - Loss: 3.404
Iter 10/2000 - Loss: 3.344
Iter 11/2000 - Loss: 3.300
Iter 12/2000 - Loss: 3.272
Iter 13/2000 - Loss: 3.246
Iter 14/2000 - Loss: 3.202
Iter 15/2000 - Loss: 3.127
Iter 16/2000 - Loss: 3.025
Iter 17/2000 - Loss: 2.908
Iter 18/2000 - Loss: 2.793
Iter 19/2000 - Loss: 2.686
Iter 20/2000 - Loss: 2.580
Iter 1981/2000 - Loss: -6.385
Iter 1982/2000 - Loss: -6.385
Iter 1983/2000 - Loss: -6.385
Iter 1984/2000 - Loss: -6.385
Iter 1985/2000 - Loss: -6.385
Iter 1986/2000 - Loss: -6.385
Iter 1987/2000 - Loss: -6.386
Iter 1988/2000 - Loss: -6.386
Iter 1989/2000 - Loss: -6.386
Iter 1990/2000 - Loss: -6.386
Iter 1991/2000 - Loss: -6.386
Iter 1992/2000 - Loss: -6.386
Iter 1993/2000 - Loss: -6.386
Iter 1994/2000 - Loss: -6.386
Iter 1995/2000 - Loss: -6.386
Iter 1996/2000 - Loss: -6.386
Iter 1997/2000 - Loss: -6.386
Iter 1998/2000 - Loss: -6.386
Iter 1999/2000 - Loss: -6.386
Iter 2000/2000 - Loss: -6.386
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[25.4654, 10.0274, 24.2660,  5.2867, 13.6634, 66.3517]],

        [[32.3381, 52.7291, 13.4049,  1.4186,  4.1505, 33.3140]],

        [[30.2539, 53.5820, 11.9317,  1.0927,  1.9315, 26.6654]],

        [[30.1358, 49.0852, 17.2243,  3.1904,  2.0636, 37.5374]]])
Signal Variance: tensor([ 0.1792,  3.5043, 22.3190,  0.5606])
Estimated target variance: tensor([0.0260, 0.8368, 8.7217, 0.0673])
N: 60
Signal to noise ratio: tensor([ 24.3099,  90.5175, 111.4170,  50.3618])
Bound on condition number: tensor([ 35459.3773, 491606.5105, 744825.4662, 152179.6323])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.00916132395338499, policy loss: 7.577475988982849
Experience 6, Iter 1, disc loss: 0.0076958237642784574, policy loss: 7.224640460020425
Experience 6, Iter 2, disc loss: 0.007504827883170421, policy loss: 7.958102452524958
Experience 6, Iter 3, disc loss: 0.013821391510868131, policy loss: 7.249663530423474
Experience 6, Iter 4, disc loss: 0.007619324375002545, policy loss: 7.521630691360649
Experience 6, Iter 5, disc loss: 0.008043153646075397, policy loss: 7.573491365855045
Experience 6, Iter 6, disc loss: 0.005052699488432661, policy loss: 8.903647378956835
Experience 6, Iter 7, disc loss: 0.005888705080793843, policy loss: 7.841601311369949
Experience 6, Iter 8, disc loss: 0.008361417725427464, policy loss: 8.379556827541032
Experience 6, Iter 9, disc loss: 0.006011389860909152, policy loss: 8.379075877257803
Experience 6, Iter 10, disc loss: 0.012741085877263641, policy loss: 7.961796996631064
Experience 6, Iter 11, disc loss: 0.006100555676916157, policy loss: 8.373374764880316
Experience 6, Iter 12, disc loss: 0.007865042045231448, policy loss: 7.231538971513134
Experience 6, Iter 13, disc loss: 0.005664645376092261, policy loss: 8.343204860926729
Experience 6, Iter 14, disc loss: 0.0061830930541333425, policy loss: 7.42774006115023
Experience 6, Iter 15, disc loss: 0.007646723110520961, policy loss: 7.759939971834049
Experience 6, Iter 16, disc loss: 0.005919105752503168, policy loss: 8.264809658603912
Experience 6, Iter 17, disc loss: 0.005098751992125245, policy loss: 8.180284831225421
Experience 6, Iter 18, disc loss: 0.006388308343456484, policy loss: 8.448113217720994
Experience 6, Iter 19, disc loss: 0.0069140744770136335, policy loss: 8.089637008586253
Experience 6, Iter 20, disc loss: 0.007763534822821845, policy loss: 7.9577375973925415
Experience 6, Iter 21, disc loss: 0.0070905755437407235, policy loss: 7.265864702814321
Experience 6, Iter 22, disc loss: 0.00778335205478602, policy loss: 7.824192625288498
Experience 6, Iter 23, disc loss: 0.006131885362090597, policy loss: 8.634699647279614
Experience 6, Iter 24, disc loss: 0.004426694927152553, policy loss: 8.739675158774315
Experience 6, Iter 25, disc loss: 0.015511275662376018, policy loss: 8.27103026970654
Experience 6, Iter 26, disc loss: 0.005962685927297632, policy loss: 8.295920995528736
Experience 6, Iter 27, disc loss: 0.005708592275900024, policy loss: 8.06646744023621
Experience 6, Iter 28, disc loss: 0.007227780111604104, policy loss: 8.674415892699656
Experience 6, Iter 29, disc loss: 0.005302300111371491, policy loss: 8.10234111126768
Experience 6, Iter 30, disc loss: 0.004163607866584521, policy loss: 8.6203851821009
Experience 6, Iter 31, disc loss: 0.005731311472871126, policy loss: 8.501982111519665
Experience 6, Iter 32, disc loss: 0.005845710400515621, policy loss: 8.610620763667992
Experience 6, Iter 33, disc loss: 0.006821293287787573, policy loss: 8.147187312785999
Experience 6, Iter 34, disc loss: 0.0076681514725565215, policy loss: 8.247699589690885
Experience 6, Iter 35, disc loss: 0.005732844921378384, policy loss: 8.60618931763372
Experience 6, Iter 36, disc loss: 0.004988774177099227, policy loss: 8.136446415752532
Experience 6, Iter 37, disc loss: 0.005989008764509894, policy loss: 7.632327830824839
Experience 6, Iter 38, disc loss: 0.006766283587427655, policy loss: 8.432298533572933
Experience 6, Iter 39, disc loss: 0.006248013332735407, policy loss: 8.030096219088113
Experience 6, Iter 40, disc loss: 0.006381808889815628, policy loss: 7.962167123227237
Experience 6, Iter 41, disc loss: 0.005025995368640063, policy loss: 9.564127704569234
Experience 6, Iter 42, disc loss: 0.006133528786982674, policy loss: 8.244528437574218
Experience 6, Iter 43, disc loss: 0.0049562296441230825, policy loss: 8.932547388780714
Experience 6, Iter 44, disc loss: 0.00555312884331983, policy loss: 8.064586196221878
Experience 6, Iter 45, disc loss: 0.004425231915518937, policy loss: 9.086645973043158
Experience 6, Iter 46, disc loss: 0.004370185281304189, policy loss: 8.882247685575448
Experience 6, Iter 47, disc loss: 0.004879181921134173, policy loss: 9.235462432857739
Experience 6, Iter 48, disc loss: 0.005894879832742662, policy loss: 8.740442935916032
Experience 6, Iter 49, disc loss: 0.005566543616698378, policy loss: 8.450243021883832
Experience 6, Iter 50, disc loss: 0.005198256097687287, policy loss: 8.303467843363968
Experience 6, Iter 51, disc loss: 0.004552534254115998, policy loss: 8.881444687384949
Experience 6, Iter 52, disc loss: 0.005026028942655369, policy loss: 8.343026883194637
Experience 6, Iter 53, disc loss: 0.005849955482179434, policy loss: 7.91113585849484
Experience 6, Iter 54, disc loss: 0.0054080714588801895, policy loss: 8.191409489216323
Experience 6, Iter 55, disc loss: 0.004536257078543991, policy loss: 8.819507634989847
Experience 6, Iter 56, disc loss: 0.006097353302518364, policy loss: 8.559270873271974
Experience 6, Iter 57, disc loss: 0.004723850701907378, policy loss: 8.559562432872765
Experience 6, Iter 58, disc loss: 0.004567260289442616, policy loss: 9.424105963179832
Experience 6, Iter 59, disc loss: 0.0046600440237534345, policy loss: 8.262537280657213
Experience 6, Iter 60, disc loss: 0.005582174245847409, policy loss: 8.378201559008037
Experience 6, Iter 61, disc loss: 0.004758167767099419, policy loss: 8.686948752289862
Experience 6, Iter 62, disc loss: 0.0047281310273942995, policy loss: 9.3327714973824
Experience 6, Iter 63, disc loss: 0.005402069712997183, policy loss: 8.040354692379141
Experience 6, Iter 64, disc loss: 0.006490300898167808, policy loss: 7.895791618728927
Experience 6, Iter 65, disc loss: 0.004340382814951499, policy loss: 8.710555175079662
Experience 6, Iter 66, disc loss: 0.010853564119319257, policy loss: 9.020130159906495
Experience 6, Iter 67, disc loss: 0.004509276416175757, policy loss: 9.303122876357623
Experience 6, Iter 68, disc loss: 0.005416515282813903, policy loss: 8.096353821365705
Experience 6, Iter 69, disc loss: 0.00434483788423112, policy loss: 8.532907582010123
Experience 6, Iter 70, disc loss: 0.005316399250768202, policy loss: 8.449374691066332
Experience 6, Iter 71, disc loss: 0.005873169488339899, policy loss: 7.677594000393427
Experience 6, Iter 72, disc loss: 0.005752091376770041, policy loss: 7.489352500047159
Experience 6, Iter 73, disc loss: 0.004410127549674988, policy loss: 8.559708240203218
Experience 6, Iter 74, disc loss: 0.004325234100247037, policy loss: 8.466478216891241
Experience 6, Iter 75, disc loss: 0.003857874724763192, policy loss: 9.055765638925665
Experience 6, Iter 76, disc loss: 0.004921510557415316, policy loss: 8.180975541871932
Experience 6, Iter 77, disc loss: 0.0043911874418166245, policy loss: 8.798063410610087
Experience 6, Iter 78, disc loss: 0.006522853672245543, policy loss: 8.176883348724125
Experience 6, Iter 79, disc loss: 0.006595859438217863, policy loss: 7.996245498390671
Experience 6, Iter 80, disc loss: 0.003996704068087005, policy loss: 9.136455416178336
Experience 6, Iter 81, disc loss: 0.004607817923338409, policy loss: 8.908563835268968
Experience 6, Iter 82, disc loss: 0.004230937408426479, policy loss: 9.084654134470025
Experience 6, Iter 83, disc loss: 0.00397581453233381, policy loss: 9.139478871941012
Experience 6, Iter 84, disc loss: 0.004014603212871454, policy loss: 8.76098325997553
Experience 6, Iter 85, disc loss: 0.004455298070773573, policy loss: 9.411182219292623
Experience 6, Iter 86, disc loss: 0.0043613406185004835, policy loss: 8.93598412555103
Experience 6, Iter 87, disc loss: 0.004195559066592556, policy loss: 7.988202022937264
Experience 6, Iter 88, disc loss: 0.0040782618103952185, policy loss: 8.794188637866839
Experience 6, Iter 89, disc loss: 0.005712951105697346, policy loss: 8.42087318841278
Experience 6, Iter 90, disc loss: 0.0048242084749694075, policy loss: 8.87104175921937
Experience 6, Iter 91, disc loss: 0.004224900137927466, policy loss: 9.055980813144288
Experience 6, Iter 92, disc loss: 0.0039264756783927235, policy loss: 8.648576840411796
Experience 6, Iter 93, disc loss: 0.00471949734776852, policy loss: 8.282392712398762
Experience 6, Iter 94, disc loss: 0.0039051471749108222, policy loss: 9.783761926727784
Experience 6, Iter 95, disc loss: 0.003512173493969926, policy loss: 9.223284334608632
Experience 6, Iter 96, disc loss: 0.004018488335093022, policy loss: 10.284510275513004
Experience 6, Iter 97, disc loss: 0.005487405655289203, policy loss: 8.298920608154347
Experience 6, Iter 98, disc loss: 0.003864411174620717, policy loss: 9.195756761512063
Experience 6, Iter 99, disc loss: 0.007233618490950876, policy loss: 10.294920697783425
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0058],
        [0.2124],
        [2.1606],
        [0.0173]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5250e-02, 2.5880e-01, 9.2214e-01, 1.5536e-02, 3.3908e-03,
          4.6487e+00]],

        [[4.5250e-02, 2.5880e-01, 9.2214e-01, 1.5536e-02, 3.3908e-03,
          4.6487e+00]],

        [[4.5250e-02, 2.5880e-01, 9.2214e-01, 1.5536e-02, 3.3908e-03,
          4.6487e+00]],

        [[4.5250e-02, 2.5880e-01, 9.2214e-01, 1.5536e-02, 3.3908e-03,
          4.6487e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0234, 0.8497, 8.6423, 0.0691], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0234, 0.8497, 8.6423, 0.0691])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.043
Iter 2/2000 - Loss: 3.619
Iter 3/2000 - Loss: 3.447
Iter 4/2000 - Loss: 3.521
Iter 5/2000 - Loss: 3.613
Iter 6/2000 - Loss: 3.594
Iter 7/2000 - Loss: 3.505
Iter 8/2000 - Loss: 3.411
Iter 9/2000 - Loss: 3.338
Iter 10/2000 - Loss: 3.287
Iter 11/2000 - Loss: 3.253
Iter 12/2000 - Loss: 3.228
Iter 13/2000 - Loss: 3.194
Iter 14/2000 - Loss: 3.136
Iter 15/2000 - Loss: 3.049
Iter 16/2000 - Loss: 2.941
Iter 17/2000 - Loss: 2.826
Iter 18/2000 - Loss: 2.716
Iter 19/2000 - Loss: 2.613
Iter 20/2000 - Loss: 2.507
Iter 1981/2000 - Loss: -6.414
Iter 1982/2000 - Loss: -6.414
Iter 1983/2000 - Loss: -6.414
Iter 1984/2000 - Loss: -6.415
Iter 1985/2000 - Loss: -6.415
Iter 1986/2000 - Loss: -6.415
Iter 1987/2000 - Loss: -6.415
Iter 1988/2000 - Loss: -6.415
Iter 1989/2000 - Loss: -6.415
Iter 1990/2000 - Loss: -6.415
Iter 1991/2000 - Loss: -6.415
Iter 1992/2000 - Loss: -6.415
Iter 1993/2000 - Loss: -6.415
Iter 1994/2000 - Loss: -6.415
Iter 1995/2000 - Loss: -6.415
Iter 1996/2000 - Loss: -6.415
Iter 1997/2000 - Loss: -6.415
Iter 1998/2000 - Loss: -6.415
Iter 1999/2000 - Loss: -6.416
Iter 2000/2000 - Loss: -6.416
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[26.1754,  9.9996, 26.4892,  5.2802, 13.2761, 67.7372]],

        [[32.0520, 51.2933, 12.6232,  1.2672,  5.8690, 25.9496]],

        [[30.8821, 53.1552, 12.0958,  0.9845,  2.4839, 22.0908]],

        [[29.7122, 49.6398, 17.3577,  3.8879,  1.9270, 39.3767]]])
Signal Variance: tensor([ 0.1753,  2.6311, 18.1354,  0.5872])
Estimated target variance: tensor([0.0234, 0.8497, 8.6423, 0.0691])
N: 70
Signal to noise ratio: tensor([22.7901, 72.2857, 97.6043, 48.5792])
Bound on condition number: tensor([ 36358.3358, 365766.3943, 666863.2229, 165196.8198])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.0041286496609889735, policy loss: 9.835731078747031
Experience 7, Iter 1, disc loss: 0.0035849236814302, policy loss: 10.42239990641551
Experience 7, Iter 2, disc loss: 0.0038189874627000955, policy loss: 10.524080015644026
Experience 7, Iter 3, disc loss: 0.003882736532008401, policy loss: 9.163095224212103
Experience 7, Iter 4, disc loss: 0.0041189189435587945, policy loss: 10.19006456856686
Experience 7, Iter 5, disc loss: 0.005405385077001423, policy loss: 9.905072832914279
Experience 7, Iter 6, disc loss: 0.004838455901552935, policy loss: 10.670895716029182
Experience 7, Iter 7, disc loss: 0.002987311498455829, policy loss: 11.303888036741906
Experience 7, Iter 8, disc loss: 0.003647251069894924, policy loss: 10.462551710056188
Experience 7, Iter 9, disc loss: 0.00388531062806197, policy loss: 10.42093003748678
Experience 7, Iter 10, disc loss: 0.0034439438986451276, policy loss: 10.418487169632147
Experience 7, Iter 11, disc loss: 0.003140278158361538, policy loss: 10.912589972886234
Experience 7, Iter 12, disc loss: 0.004711383670130976, policy loss: 9.151675356891463
Experience 7, Iter 13, disc loss: 0.005157432940313079, policy loss: 10.342623358159466
Experience 7, Iter 14, disc loss: 0.002938226565837758, policy loss: 10.650447027192019
Experience 7, Iter 15, disc loss: 0.0035446235736098924, policy loss: 11.762566048121688
Experience 7, Iter 16, disc loss: 0.004146193736474146, policy loss: 10.173250480493602
Experience 7, Iter 17, disc loss: 0.0037126740444428296, policy loss: 11.567296484778415
Experience 7, Iter 18, disc loss: 0.004567426942881242, policy loss: 9.77474039979094
Experience 7, Iter 19, disc loss: 0.0037354461318078952, policy loss: 10.210483689301485
Experience 7, Iter 20, disc loss: 0.0038873128052889457, policy loss: 10.59296925276421
Experience 7, Iter 21, disc loss: 0.005958396155031641, policy loss: 10.538190876069141
Experience 7, Iter 22, disc loss: 0.0030735154235267534, policy loss: 11.437148086379597
Experience 7, Iter 23, disc loss: 0.0037517256502333936, policy loss: 10.884279155422176
Experience 7, Iter 24, disc loss: 0.0037336765569248812, policy loss: 10.841350594150539
Experience 7, Iter 25, disc loss: 0.00358643237716873, policy loss: 10.746532470851449
Experience 7, Iter 26, disc loss: 0.003538365878307627, policy loss: 10.872166589709405
Experience 7, Iter 27, disc loss: 0.003120275029036346, policy loss: 11.119659417250041
Experience 7, Iter 28, disc loss: 0.003768721453226578, policy loss: 10.943089666798258
Experience 7, Iter 29, disc loss: 0.0033679351965432884, policy loss: 10.015320758347588
Experience 7, Iter 30, disc loss: 0.004315716787848228, policy loss: 11.29643483551435
Experience 7, Iter 31, disc loss: 0.0027839989013535315, policy loss: 11.62395604807968
Experience 7, Iter 32, disc loss: 0.003476183861376085, policy loss: 10.166092928282861
Experience 7, Iter 33, disc loss: 0.0028304542047091824, policy loss: 10.994110096382478
Experience 7, Iter 34, disc loss: 0.013327436769846027, policy loss: 11.377173656516646
Experience 7, Iter 35, disc loss: 0.002859671093880668, policy loss: 11.079077606289147
Experience 7, Iter 36, disc loss: 0.003419286161209234, policy loss: 10.132996791120519
Experience 7, Iter 37, disc loss: 0.005083606246582634, policy loss: 9.522455858452073
Experience 7, Iter 38, disc loss: 0.003054347100856317, policy loss: 10.27403671912657
Experience 7, Iter 39, disc loss: 0.003126097509813488, policy loss: 10.759342659757989
Experience 7, Iter 40, disc loss: 0.00441124095569778, policy loss: 9.892018344678569
Experience 7, Iter 41, disc loss: 0.004392411775386643, policy loss: 9.95728658789942
Experience 7, Iter 42, disc loss: 0.0035998911328985906, policy loss: 10.537960466605746
Experience 7, Iter 43, disc loss: 0.0027223037155382217, policy loss: 11.225191636054825
Experience 7, Iter 44, disc loss: 0.0027457146550177135, policy loss: 11.756414355552883
Experience 7, Iter 45, disc loss: 0.0036495801731563898, policy loss: 9.952642886780616
Experience 7, Iter 46, disc loss: 0.0033166160336066865, policy loss: 9.37608016271914
Experience 7, Iter 47, disc loss: 0.01603471971066583, policy loss: 9.971491424575866
Experience 7, Iter 48, disc loss: 0.002801701329737847, policy loss: 11.736250971241336
Experience 7, Iter 49, disc loss: 0.0024223790984443097, policy loss: 11.139105378614792
Experience 7, Iter 50, disc loss: 0.0032047435029336256, policy loss: 10.542159760268781
Experience 7, Iter 51, disc loss: 0.006840900111122873, policy loss: 10.28760409789515
Experience 7, Iter 52, disc loss: 0.004833417803926118, policy loss: 10.071061618447294
Experience 7, Iter 53, disc loss: 0.0027926154159631867, policy loss: 11.71444268898441
Experience 7, Iter 54, disc loss: 0.0033630988277415287, policy loss: 11.193066185206765
Experience 7, Iter 55, disc loss: 0.002978448131710226, policy loss: 12.13594010571267
Experience 7, Iter 56, disc loss: 0.003319196576771673, policy loss: 11.90032618299913
Experience 7, Iter 57, disc loss: 0.0064353944038656645, policy loss: 11.214787528881264
Experience 7, Iter 58, disc loss: 0.0028445568327444977, policy loss: 11.874089715291676
Experience 7, Iter 59, disc loss: 0.0033136682927506827, policy loss: 11.440261404393366
Experience 7, Iter 60, disc loss: 0.003566005968026846, policy loss: 12.280911204421773
Experience 7, Iter 61, disc loss: 0.0023274232626064153, policy loss: 12.790155120889569
Experience 7, Iter 62, disc loss: 0.003308393025215751, policy loss: 12.201645266674777
Experience 7, Iter 63, disc loss: 0.003663917795325796, policy loss: 13.208222316360946
Experience 7, Iter 64, disc loss: 0.003684263125194211, policy loss: 11.648037200652887
Experience 7, Iter 65, disc loss: 0.003259016011502654, policy loss: 12.708145758722365
Experience 7, Iter 66, disc loss: 0.0026061808168545337, policy loss: 11.691217035573619
Experience 7, Iter 67, disc loss: 0.0025212189655443046, policy loss: 11.77789968334099
Experience 7, Iter 68, disc loss: 0.001791911556229449, policy loss: 13.235918221824113
Experience 7, Iter 69, disc loss: 0.0022419866663592174, policy loss: 13.52336254350539
Experience 7, Iter 70, disc loss: 0.004203997002227932, policy loss: 10.845421570780452
Experience 7, Iter 71, disc loss: 0.00298420638324794, policy loss: 10.856607807311619
Experience 7, Iter 72, disc loss: 0.006206850857481646, policy loss: 10.968059352090666
Experience 7, Iter 73, disc loss: 0.0028547894544912306, policy loss: 11.213925059729512
Experience 7, Iter 74, disc loss: 0.0026892600559313726, policy loss: 11.140667260585452
Experience 7, Iter 75, disc loss: 0.005423797663862506, policy loss: 10.336358157820085
Experience 7, Iter 76, disc loss: 0.0020180144200499035, policy loss: 11.615919652293638
Experience 7, Iter 77, disc loss: 0.002383348653704491, policy loss: 11.396014075134108
Experience 7, Iter 78, disc loss: 0.003648483111897929, policy loss: 11.793743184383317
Experience 7, Iter 79, disc loss: 0.0036494956746841614, policy loss: 11.876909436917074
Experience 7, Iter 80, disc loss: 0.004114758378221034, policy loss: 11.888707318649356
Experience 7, Iter 81, disc loss: 0.0026952142175685314, policy loss: 12.437935427726616
Experience 7, Iter 82, disc loss: 0.003487470714054419, policy loss: 11.34481900313698
Experience 7, Iter 83, disc loss: 0.002782352358754705, policy loss: 11.206559300933234
Experience 7, Iter 84, disc loss: 0.002620131060133997, policy loss: 12.086435196336726
Experience 7, Iter 85, disc loss: 0.002373418341869305, policy loss: 11.115842202788308
Experience 7, Iter 86, disc loss: 0.014658665975492991, policy loss: 10.379357205579804
Experience 7, Iter 87, disc loss: 0.005500148251599239, policy loss: 12.034654676563182
Experience 7, Iter 88, disc loss: 0.0027736050097114055, policy loss: 12.18386919748457
Experience 7, Iter 89, disc loss: 0.004443558976920864, policy loss: 10.703706320475575
Experience 7, Iter 90, disc loss: 0.0031239041688114573, policy loss: 12.651730226404098
Experience 7, Iter 91, disc loss: 0.0021285854048579524, policy loss: 12.080816726375794
Experience 7, Iter 92, disc loss: 0.004720866803016266, policy loss: 11.904574483318727
Experience 7, Iter 93, disc loss: 0.002783013344218872, policy loss: 11.523160825338813
Experience 7, Iter 94, disc loss: 0.002489246474769588, policy loss: 12.902455275393956
Experience 7, Iter 95, disc loss: 0.00207517128229409, policy loss: 12.223020565527404
Experience 7, Iter 96, disc loss: 0.0024829728791457917, policy loss: 11.252639033686258
Experience 7, Iter 97, disc loss: 0.0020347072160820053, policy loss: 12.55316783690828
Experience 7, Iter 98, disc loss: 0.002680096396898044, policy loss: 12.490421074757098
Experience 7, Iter 99, disc loss: 0.002411327480805375, policy loss: 11.40723397398823
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2141],
        [2.1536],
        [0.0220]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0455, 0.2386, 1.1208, 0.0180, 0.0060, 4.9922]],

        [[0.0455, 0.2386, 1.1208, 0.0180, 0.0060, 4.9922]],

        [[0.0455, 0.2386, 1.1208, 0.0180, 0.0060, 4.9922]],

        [[0.0455, 0.2386, 1.1208, 0.0180, 0.0060, 4.9922]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0209, 0.8566, 8.6144, 0.0878], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0209, 0.8566, 8.6144, 0.0878])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.172
Iter 2/2000 - Loss: 3.663
Iter 3/2000 - Loss: 3.490
Iter 4/2000 - Loss: 3.560
Iter 5/2000 - Loss: 3.649
Iter 6/2000 - Loss: 3.635
Iter 7/2000 - Loss: 3.549
Iter 8/2000 - Loss: 3.446
Iter 9/2000 - Loss: 3.352
Iter 10/2000 - Loss: 3.277
Iter 11/2000 - Loss: 3.227
Iter 12/2000 - Loss: 3.195
Iter 13/2000 - Loss: 3.158
Iter 14/2000 - Loss: 3.093
Iter 15/2000 - Loss: 2.993
Iter 16/2000 - Loss: 2.867
Iter 17/2000 - Loss: 2.733
Iter 18/2000 - Loss: 2.604
Iter 19/2000 - Loss: 2.482
Iter 20/2000 - Loss: 2.358
Iter 1981/2000 - Loss: -6.340
Iter 1982/2000 - Loss: -6.341
Iter 1983/2000 - Loss: -6.341
Iter 1984/2000 - Loss: -6.341
Iter 1985/2000 - Loss: -6.341
Iter 1986/2000 - Loss: -6.341
Iter 1987/2000 - Loss: -6.341
Iter 1988/2000 - Loss: -6.341
Iter 1989/2000 - Loss: -6.341
Iter 1990/2000 - Loss: -6.341
Iter 1991/2000 - Loss: -6.341
Iter 1992/2000 - Loss: -6.341
Iter 1993/2000 - Loss: -6.341
Iter 1994/2000 - Loss: -6.341
Iter 1995/2000 - Loss: -6.341
Iter 1996/2000 - Loss: -6.341
Iter 1997/2000 - Loss: -6.341
Iter 1998/2000 - Loss: -6.341
Iter 1999/2000 - Loss: -6.341
Iter 2000/2000 - Loss: -6.342
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[24.6252, 10.1453, 25.9504,  4.8842, 13.8157, 69.4892]],

        [[25.9785, 41.9968, 10.3274,  1.1086,  1.9677, 10.1087]],

        [[30.5570, 48.4168, 10.5407,  1.0213,  1.5468, 22.5403]],

        [[27.7049, 44.2660, 14.3362,  2.2435,  2.1387, 40.9122]]])
Signal Variance: tensor([ 0.1702,  0.7264, 16.7352,  0.5150])
Estimated target variance: tensor([0.0209, 0.8566, 8.6144, 0.0878])
N: 80
Signal to noise ratio: tensor([ 23.5876,  42.0341, 103.2869,  46.5470])
Bound on condition number: tensor([ 44511.1094, 141350.3292, 853455.4703, 173330.8352])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.0031170679450614617, policy loss: 11.632144075828663
Experience 8, Iter 1, disc loss: 0.005633313573633831, policy loss: 11.814792717171002
Experience 8, Iter 2, disc loss: 0.0018451942247629432, policy loss: 13.027470019164006
Experience 8, Iter 3, disc loss: 0.0021754363759609488, policy loss: 13.191579463036621
Experience 8, Iter 4, disc loss: 0.004459276126135021, policy loss: 13.02877034382577
Experience 8, Iter 5, disc loss: 0.0018979338316370913, policy loss: 12.552832661415495
Experience 8, Iter 6, disc loss: 0.0019014514416088022, policy loss: 12.226522472771013
Experience 8, Iter 7, disc loss: 0.0027063292472415106, policy loss: 12.902549940739663
Experience 8, Iter 8, disc loss: 0.003986239914509651, policy loss: 12.535999065586218
Experience 8, Iter 9, disc loss: 0.0024425902874128315, policy loss: 12.141093832095603
Experience 8, Iter 10, disc loss: 0.003536906385220774, policy loss: 12.316077496549184
Experience 8, Iter 11, disc loss: 0.0027390253621421244, policy loss: 12.87102106720804
Experience 8, Iter 12, disc loss: 0.002554587587240296, policy loss: 12.887204485103151
Experience 8, Iter 13, disc loss: 0.0023211596168892655, policy loss: 12.984645539830021
Experience 8, Iter 14, disc loss: 0.0026413637126917987, policy loss: 11.755320762286555
Experience 8, Iter 15, disc loss: 0.0025332153231285405, policy loss: 12.400390046210982
Experience 8, Iter 16, disc loss: 0.0019741371800075396, policy loss: 12.562269880674684
Experience 8, Iter 17, disc loss: 0.0038312359030878594, policy loss: 12.319091187214697
Experience 8, Iter 18, disc loss: 0.001882379693641845, policy loss: 12.639268055570426
Experience 8, Iter 19, disc loss: 0.0045434826368325335, policy loss: 12.850837596393276
Experience 8, Iter 20, disc loss: 0.0026944199264335483, policy loss: 12.385237193697392
Experience 8, Iter 21, disc loss: 0.003724429107943992, policy loss: 11.643625165822701
Experience 8, Iter 22, disc loss: 0.0022208383823489044, policy loss: 10.652948643469852
Experience 8, Iter 23, disc loss: 0.0045813133164882575, policy loss: 9.594241032782872
Experience 8, Iter 24, disc loss: 0.002340002079144016, policy loss: 9.3060905519866
Experience 8, Iter 25, disc loss: 0.013093044080622899, policy loss: 8.913747296042352
Experience 8, Iter 26, disc loss: 0.0016398839097516481, policy loss: 9.975572621740204
Experience 8, Iter 27, disc loss: 0.001948174361496236, policy loss: 10.998825174600528
Experience 8, Iter 28, disc loss: 0.0021203724429489394, policy loss: 12.86431326202051
Experience 8, Iter 29, disc loss: 0.00175061244179847, policy loss: 12.630389489917803
Experience 8, Iter 30, disc loss: 0.0012689811459995353, policy loss: 13.48456395763667
Experience 8, Iter 31, disc loss: 0.0015919309236465955, policy loss: 13.357893463972177
Experience 8, Iter 32, disc loss: 0.007383669098056807, policy loss: 13.049470467647232
Experience 8, Iter 33, disc loss: 0.0022124821333838283, policy loss: 14.611902726986363
Experience 8, Iter 34, disc loss: 0.001719874021904723, policy loss: 14.070419697338433
Experience 8, Iter 35, disc loss: 0.0014795299068485332, policy loss: 15.263441585433089
Experience 8, Iter 36, disc loss: 0.0014644213406467604, policy loss: 13.987339874585484
Experience 8, Iter 37, disc loss: 0.0017433543135723145, policy loss: 13.763802885728454
Experience 8, Iter 38, disc loss: 0.002120978769781545, policy loss: 13.365341640809747
Experience 8, Iter 39, disc loss: 0.0017380985271047585, policy loss: 13.204451009904897
Experience 8, Iter 40, disc loss: 0.0018479450417903053, policy loss: 12.408451725798356
Experience 8, Iter 41, disc loss: 0.0025103657485870813, policy loss: 12.55365535666936
Experience 8, Iter 42, disc loss: 0.0019739553490642253, policy loss: 13.537019715770029
Experience 8, Iter 43, disc loss: 0.004101451507469307, policy loss: 12.132761247047121
Experience 8, Iter 44, disc loss: 0.0031040863729366493, policy loss: 10.816773137647338
Experience 8, Iter 45, disc loss: 0.007694939337032475, policy loss: 12.735743894553854
Experience 8, Iter 46, disc loss: 0.003415498905270734, policy loss: 10.37540066348815
Experience 8, Iter 47, disc loss: 0.0034251558242506712, policy loss: 10.413053674655984
Experience 8, Iter 48, disc loss: 0.00719545845050025, policy loss: 10.781408538733572
Experience 8, Iter 49, disc loss: 0.002852821910296957, policy loss: 11.164841102489135
Experience 8, Iter 50, disc loss: 0.003822000688904044, policy loss: 12.053587101085133
Experience 8, Iter 51, disc loss: 0.00307300905023366, policy loss: 10.947147462358501
Experience 8, Iter 52, disc loss: 0.0036601554963755713, policy loss: 10.817031578768825
Experience 8, Iter 53, disc loss: 0.0025838100066107902, policy loss: 12.13529062618452
Experience 8, Iter 54, disc loss: 0.005504544420414016, policy loss: 11.481070696214879
Experience 8, Iter 55, disc loss: 0.004369980778238803, policy loss: 11.948151365368542
Experience 8, Iter 56, disc loss: 0.004916419631029283, policy loss: 10.976867364675499
Experience 8, Iter 57, disc loss: 0.004486804435602069, policy loss: 11.477437262040954
Experience 8, Iter 58, disc loss: 0.002976425911049496, policy loss: 11.421740169956559
Experience 8, Iter 59, disc loss: 0.003109945509330328, policy loss: 10.914117018763662
Experience 8, Iter 60, disc loss: 0.002516294182392291, policy loss: 10.477167912932632
Experience 8, Iter 61, disc loss: 0.0031779345110851953, policy loss: 10.486328193498572
Experience 8, Iter 62, disc loss: 0.0039896469720499, policy loss: 9.473326478396677
Experience 8, Iter 63, disc loss: 0.0030382911851622165, policy loss: 10.299637946020624
Experience 8, Iter 64, disc loss: 0.004562459619320358, policy loss: 10.463958120313356
Experience 8, Iter 65, disc loss: 0.0031113369795742157, policy loss: 11.18203766041107
Experience 8, Iter 66, disc loss: 0.003282890092042438, policy loss: 11.216412561014902
Experience 8, Iter 67, disc loss: 0.0032872848514432203, policy loss: 10.522302090525285
Experience 8, Iter 68, disc loss: 0.00332992660639739, policy loss: 11.034810103663546
Experience 8, Iter 69, disc loss: 0.0031454845436891623, policy loss: 10.59565316918127
Experience 8, Iter 70, disc loss: 0.005335418274781838, policy loss: 9.966730310539418
Experience 8, Iter 71, disc loss: 0.005382970266763728, policy loss: 9.730360368625723
Experience 8, Iter 72, disc loss: 0.004054735501103118, policy loss: 10.366378552571778
Experience 8, Iter 73, disc loss: 0.005500102713790448, policy loss: 10.004948634630859
Experience 8, Iter 74, disc loss: 0.00495940772796699, policy loss: 10.02631138452779
Experience 8, Iter 75, disc loss: 0.004019012758201013, policy loss: 9.026391341726864
Experience 8, Iter 76, disc loss: 0.003395497829758298, policy loss: 11.187620851047862
Experience 8, Iter 77, disc loss: 0.0036404787682323856, policy loss: 10.722511317828346
Experience 8, Iter 78, disc loss: 0.003837877177429634, policy loss: 9.909897577272552
Experience 8, Iter 79, disc loss: 0.005450154738311484, policy loss: 10.798165314429781
Experience 8, Iter 80, disc loss: 0.002927165637337182, policy loss: 11.142582326737521
Experience 8, Iter 81, disc loss: 0.004148951039093542, policy loss: 11.379384003191255
Experience 8, Iter 82, disc loss: 0.0044422874549173335, policy loss: 9.739597703630277
Experience 8, Iter 83, disc loss: 0.00348431584332272, policy loss: 10.841339622504877
Experience 8, Iter 84, disc loss: 0.002525911214563697, policy loss: 12.117035309121594
Experience 8, Iter 85, disc loss: 0.003717799428415782, policy loss: 9.900539103981636
Experience 8, Iter 86, disc loss: 0.0031844582008672484, policy loss: 10.171694791750278
Experience 8, Iter 87, disc loss: 0.002779635447443033, policy loss: 11.47852548660196
Experience 8, Iter 88, disc loss: 0.005805362203880511, policy loss: 9.9155547260677
Experience 8, Iter 89, disc loss: 0.0033789915810411423, policy loss: 11.7186557988129
Experience 8, Iter 90, disc loss: 0.0027085578439712044, policy loss: 10.777260881016847
Experience 8, Iter 91, disc loss: 0.0027385266904017182, policy loss: 10.523603629929006
Experience 8, Iter 92, disc loss: 0.0031344423369594927, policy loss: 10.588796369691957
Experience 8, Iter 93, disc loss: 0.0038916094040613654, policy loss: 9.981117486933346
Experience 8, Iter 94, disc loss: 0.00297519372887486, policy loss: 10.763468228332878
Experience 8, Iter 95, disc loss: 0.003193093977714737, policy loss: 11.142927824074011
Experience 8, Iter 96, disc loss: 0.0027311511227548443, policy loss: 10.904049341011795
Experience 8, Iter 97, disc loss: 0.004969631437643458, policy loss: 10.281827458911865
Experience 8, Iter 98, disc loss: 0.0031898039767741587, policy loss: 9.942777441964257
Experience 8, Iter 99, disc loss: 0.0028349314776845193, policy loss: 10.52499243444117
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.2121],
        [2.1594],
        [0.0204]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0416, 0.2256, 1.0598, 0.0165, 0.0055, 4.7971]],

        [[0.0416, 0.2256, 1.0598, 0.0165, 0.0055, 4.7971]],

        [[0.0416, 0.2256, 1.0598, 0.0165, 0.0055, 4.7971]],

        [[0.0416, 0.2256, 1.0598, 0.0165, 0.0055, 4.7971]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0196, 0.8485, 8.6377, 0.0815], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0196, 0.8485, 8.6377, 0.0815])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.083
Iter 2/2000 - Loss: 3.584
Iter 3/2000 - Loss: 3.430
Iter 4/2000 - Loss: 3.518
Iter 5/2000 - Loss: 3.603
Iter 6/2000 - Loss: 3.571
Iter 7/2000 - Loss: 3.470
Iter 8/2000 - Loss: 3.364
Iter 9/2000 - Loss: 3.279
Iter 10/2000 - Loss: 3.216
Iter 11/2000 - Loss: 3.175
Iter 12/2000 - Loss: 3.140
Iter 13/2000 - Loss: 3.090
Iter 14/2000 - Loss: 3.008
Iter 15/2000 - Loss: 2.891
Iter 16/2000 - Loss: 2.755
Iter 17/2000 - Loss: 2.614
Iter 18/2000 - Loss: 2.481
Iter 19/2000 - Loss: 2.352
Iter 20/2000 - Loss: 2.216
Iter 1981/2000 - Loss: -6.686
Iter 1982/2000 - Loss: -6.686
Iter 1983/2000 - Loss: -6.686
Iter 1984/2000 - Loss: -6.686
Iter 1985/2000 - Loss: -6.686
Iter 1986/2000 - Loss: -6.686
Iter 1987/2000 - Loss: -6.686
Iter 1988/2000 - Loss: -6.687
Iter 1989/2000 - Loss: -6.687
Iter 1990/2000 - Loss: -6.687
Iter 1991/2000 - Loss: -6.687
Iter 1992/2000 - Loss: -6.687
Iter 1993/2000 - Loss: -6.687
Iter 1994/2000 - Loss: -6.687
Iter 1995/2000 - Loss: -6.687
Iter 1996/2000 - Loss: -6.687
Iter 1997/2000 - Loss: -6.687
Iter 1998/2000 - Loss: -6.687
Iter 1999/2000 - Loss: -6.687
Iter 2000/2000 - Loss: -6.687
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[22.8286, 10.6644, 26.9970,  8.1663,  3.3895, 68.3454]],

        [[27.4029, 43.4004, 10.4606,  1.1364,  1.8488, 11.8401]],

        [[30.6161, 45.7068, 11.0375,  0.9979,  1.5114, 21.0533]],

        [[26.3823, 42.8255, 12.9949,  2.1761,  2.0354, 41.6842]]])
Signal Variance: tensor([ 0.1838,  0.8705, 15.4706,  0.4595])
Estimated target variance: tensor([0.0196, 0.8485, 8.6377, 0.0815])
N: 90
Signal to noise ratio: tensor([ 24.4539,  45.4591, 100.0464,  45.8257])
Bound on condition number: tensor([ 53820.3280, 185988.6253, 900836.1143, 189000.2527])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.002096935826570336, policy loss: 11.934343725327054
Experience 9, Iter 1, disc loss: 0.0031945247580001147, policy loss: 10.819728259372042
Experience 9, Iter 2, disc loss: 0.0033746731840751227, policy loss: 9.951278236213257
Experience 9, Iter 3, disc loss: 0.0035890171456822192, policy loss: 11.052912838896752
Experience 9, Iter 4, disc loss: 0.004322429537771838, policy loss: 11.240901419098304
Experience 9, Iter 5, disc loss: 0.0035959585943578067, policy loss: 10.516092306284845
Experience 9, Iter 6, disc loss: 0.004273651093467239, policy loss: 9.664957496493628
Experience 9, Iter 7, disc loss: 0.004150122445162254, policy loss: 9.609570191948759
Experience 9, Iter 8, disc loss: 0.0033637430336713845, policy loss: 10.911897551034663
Experience 9, Iter 9, disc loss: 0.005078433672774692, policy loss: 9.626465920310391
Experience 9, Iter 10, disc loss: 0.0026557210143657974, policy loss: 9.605090476319504
Experience 9, Iter 11, disc loss: 0.0030966765125770472, policy loss: 10.713676292186381
Experience 9, Iter 12, disc loss: 0.002648129958288661, policy loss: 10.27140257232636
Experience 9, Iter 13, disc loss: 0.0025331261614952526, policy loss: 10.720868557589979
Experience 9, Iter 14, disc loss: 0.004502480723868647, policy loss: 11.605543811466584
Experience 9, Iter 15, disc loss: 0.0031401151909379445, policy loss: 11.741371829349037
Experience 9, Iter 16, disc loss: 0.0028240668136498524, policy loss: 10.91879025294012
Experience 9, Iter 17, disc loss: 0.002832976099658704, policy loss: 12.211283795069088
Experience 9, Iter 18, disc loss: 0.0031226032542400316, policy loss: 10.661165097761911
Experience 9, Iter 19, disc loss: 0.005425621551318061, policy loss: 11.36016673323519
Experience 9, Iter 20, disc loss: 0.0037499608933730154, policy loss: 11.04140050990748
Experience 9, Iter 21, disc loss: 0.005492227933312757, policy loss: 11.659009580054821
Experience 9, Iter 22, disc loss: 0.0025276269111223805, policy loss: 11.689229701320466
Experience 9, Iter 23, disc loss: 0.003059967440375992, policy loss: 11.508984699209556
Experience 9, Iter 24, disc loss: 0.002576629172812993, policy loss: 11.3905221628652
Experience 9, Iter 25, disc loss: 0.0031526845141499034, policy loss: 10.557559380233041
Experience 9, Iter 26, disc loss: 0.002807352299743302, policy loss: 9.935469899683438
Experience 9, Iter 27, disc loss: 0.001996513608033112, policy loss: 11.907393551781833
Experience 9, Iter 28, disc loss: 0.0029869463558864703, policy loss: 11.688735130673177
Experience 9, Iter 29, disc loss: 0.003551592856531763, policy loss: 12.295457251517387
Experience 9, Iter 30, disc loss: 0.0038698292545057836, policy loss: 11.173638063455217
Experience 9, Iter 31, disc loss: 0.0031728680542886913, policy loss: 9.785915349016255
Experience 9, Iter 32, disc loss: 0.002467183265482273, policy loss: 11.6939569302234
Experience 9, Iter 33, disc loss: 0.0024693447243545868, policy loss: 10.47696189342525
Experience 9, Iter 34, disc loss: 0.0035870370270925196, policy loss: 11.667605569418148
Experience 9, Iter 35, disc loss: 0.002054930279877653, policy loss: 11.558025837074627
Experience 9, Iter 36, disc loss: 0.003024335932886222, policy loss: 12.655783950008125
Experience 9, Iter 37, disc loss: 0.0034222517049474193, policy loss: 11.911066780979308
Experience 9, Iter 38, disc loss: 0.002210452438959759, policy loss: 10.957919769547434
Experience 9, Iter 39, disc loss: 0.004815612898863873, policy loss: 10.579964601243047
Experience 9, Iter 40, disc loss: 0.0022106461007250226, policy loss: 11.034711146988506
Experience 9, Iter 41, disc loss: 0.002185747406977299, policy loss: 12.439706771710657
Experience 9, Iter 42, disc loss: 0.0027989893106860372, policy loss: 10.398427176409031
Experience 9, Iter 43, disc loss: 0.002441155414919902, policy loss: 10.633444732974974
Experience 9, Iter 44, disc loss: 0.0031467226401325774, policy loss: 11.667418660469433
Experience 9, Iter 45, disc loss: 0.0022235235775562635, policy loss: 10.571674672460636
Experience 9, Iter 46, disc loss: 0.002330366567275495, policy loss: 10.195062350399407
Experience 9, Iter 47, disc loss: 0.0034613461665992046, policy loss: 9.74637024925758
Experience 9, Iter 48, disc loss: 0.004520003010308805, policy loss: 11.795291359925237
Experience 9, Iter 49, disc loss: 0.002335235476435936, policy loss: 11.313123083318974
Experience 9, Iter 50, disc loss: 0.0030765287399101204, policy loss: 9.78136056030981
Experience 9, Iter 51, disc loss: 0.0027104533445334485, policy loss: 12.324798211794826
Experience 9, Iter 52, disc loss: 0.003380502881281864, policy loss: 12.602982695941698
Experience 9, Iter 53, disc loss: 0.0019277506120116972, policy loss: 11.826168519789036
Experience 9, Iter 54, disc loss: 0.0017859107907316988, policy loss: 12.808550105323476
Experience 9, Iter 55, disc loss: 0.002847033162956427, policy loss: 9.256248777752454
Experience 9, Iter 56, disc loss: 0.003152814519287034, policy loss: 11.1085355023132
Experience 9, Iter 57, disc loss: 0.004508960913601368, policy loss: 10.774712359043152
Experience 9, Iter 58, disc loss: 0.0018833666805976252, policy loss: 12.503224664873489
Experience 9, Iter 59, disc loss: 0.0021445846889709417, policy loss: 11.450254757873811
Experience 9, Iter 60, disc loss: 0.004391331688811816, policy loss: 12.890051610473636
Experience 9, Iter 61, disc loss: 0.0028424860800248435, policy loss: 10.966767646139758
Experience 9, Iter 62, disc loss: 0.002187805448230577, policy loss: 10.986810790561352
Experience 9, Iter 63, disc loss: 0.0022903320049264837, policy loss: 11.337171858821176
Experience 9, Iter 64, disc loss: 0.002042612272450872, policy loss: 10.815961690003352
Experience 9, Iter 65, disc loss: 0.0021185083150492166, policy loss: 11.331716722056708
Experience 9, Iter 66, disc loss: 0.003041433663118432, policy loss: 10.421700994694321
Experience 9, Iter 67, disc loss: 0.00195999500817384, policy loss: 11.915114849849715
Experience 9, Iter 68, disc loss: 0.0017782094453354742, policy loss: 11.120754164475287
Experience 9, Iter 69, disc loss: 0.003924316180756935, policy loss: 11.753778215745971
Experience 9, Iter 70, disc loss: 0.006698295699000655, policy loss: 12.092696799791344
Experience 9, Iter 71, disc loss: 0.00196874297733856, policy loss: 10.229649006998702
Experience 9, Iter 72, disc loss: 0.002667131127322764, policy loss: 11.26061175385432
Experience 9, Iter 73, disc loss: 0.002555978692073098, policy loss: 10.704565029940465
Experience 9, Iter 74, disc loss: 0.0019204238003904966, policy loss: 11.883057820766838
Experience 9, Iter 75, disc loss: 0.0028950842921997982, policy loss: 10.303852060785697
Experience 9, Iter 76, disc loss: 0.0020378635202922355, policy loss: 10.964554572340317
Experience 9, Iter 77, disc loss: 0.002500944554886664, policy loss: 10.566619215342758
Experience 9, Iter 78, disc loss: 0.002984013688335017, policy loss: 10.117838532267367
Experience 9, Iter 79, disc loss: 0.0018520929400776417, policy loss: 11.07687907366933
Experience 9, Iter 80, disc loss: 0.0015962477763669584, policy loss: 13.038030996039263
Experience 9, Iter 81, disc loss: 0.002509090734267562, policy loss: 11.263529637759396
Experience 9, Iter 82, disc loss: 0.0020803428409726784, policy loss: 10.73089014166284
Experience 9, Iter 83, disc loss: 0.0017385155939888379, policy loss: 11.71185124387211
Experience 9, Iter 84, disc loss: 0.0016589536858988563, policy loss: 12.040089475429085
Experience 9, Iter 85, disc loss: 0.004683299143921583, policy loss: 10.896510542553065
Experience 9, Iter 86, disc loss: 0.0019389449630428733, policy loss: 11.99414508738982
Experience 9, Iter 87, disc loss: 0.0018687021365154412, policy loss: 11.560173068202777
Experience 9, Iter 88, disc loss: 0.00201875255382107, policy loss: 10.702981438112225
Experience 9, Iter 89, disc loss: 0.001982593565364241, policy loss: 12.352977993111608
Experience 9, Iter 90, disc loss: 0.001961392439701565, policy loss: 11.027925500813609
Experience 9, Iter 91, disc loss: 0.0024066131737638533, policy loss: 11.45996769508652
Experience 9, Iter 92, disc loss: 0.0020054338771439617, policy loss: 11.21368955575459
Experience 9, Iter 93, disc loss: 0.0017672644795254829, policy loss: 10.895642513362095
Experience 9, Iter 94, disc loss: 0.002750495629758876, policy loss: 10.589158822371985
Experience 9, Iter 95, disc loss: 0.0017308236843323254, policy loss: 12.004514539831588
Experience 9, Iter 96, disc loss: 0.0025283151551673063, policy loss: 12.170870922948628
Experience 9, Iter 97, disc loss: 0.002465742792349413, policy loss: 11.16630780215113
Experience 9, Iter 98, disc loss: 0.003407011982702795, policy loss: 10.133233902745932
Experience 9, Iter 99, disc loss: 0.0023183176346432254, policy loss: 13.159361883300804
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.2117],
        [2.1763],
        [0.0199]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0405, 0.2131, 1.0401, 0.0163, 0.0051, 4.7080]],

        [[0.0405, 0.2131, 1.0401, 0.0163, 0.0051, 4.7080]],

        [[0.0405, 0.2131, 1.0401, 0.0163, 0.0051, 4.7080]],

        [[0.0405, 0.2131, 1.0401, 0.0163, 0.0051, 4.7080]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0184, 0.8470, 8.7051, 0.0796], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0184, 0.8470, 8.7051, 0.0796])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.098
Iter 2/2000 - Loss: 3.553
Iter 3/2000 - Loss: 3.385
Iter 4/2000 - Loss: 3.475
Iter 5/2000 - Loss: 3.567
Iter 6/2000 - Loss: 3.541
Iter 7/2000 - Loss: 3.442
Iter 8/2000 - Loss: 3.333
Iter 9/2000 - Loss: 3.242
Iter 10/2000 - Loss: 3.174
Iter 11/2000 - Loss: 3.130
Iter 12/2000 - Loss: 3.099
Iter 13/2000 - Loss: 3.056
Iter 14/2000 - Loss: 2.982
Iter 15/2000 - Loss: 2.872
Iter 16/2000 - Loss: 2.738
Iter 17/2000 - Loss: 2.597
Iter 18/2000 - Loss: 2.461
Iter 19/2000 - Loss: 2.331
Iter 20/2000 - Loss: 2.196
Iter 1981/2000 - Loss: -6.881
Iter 1982/2000 - Loss: -6.881
Iter 1983/2000 - Loss: -6.881
Iter 1984/2000 - Loss: -6.882
Iter 1985/2000 - Loss: -6.882
Iter 1986/2000 - Loss: -6.882
Iter 1987/2000 - Loss: -6.882
Iter 1988/2000 - Loss: -6.882
Iter 1989/2000 - Loss: -6.882
Iter 1990/2000 - Loss: -6.882
Iter 1991/2000 - Loss: -6.882
Iter 1992/2000 - Loss: -6.882
Iter 1993/2000 - Loss: -6.882
Iter 1994/2000 - Loss: -6.882
Iter 1995/2000 - Loss: -6.882
Iter 1996/2000 - Loss: -6.882
Iter 1997/2000 - Loss: -6.882
Iter 1998/2000 - Loss: -6.882
Iter 1999/2000 - Loss: -6.882
Iter 2000/2000 - Loss: -6.882
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[22.7805, 11.1668, 26.1811, 10.7486,  2.9280, 69.2651]],

        [[28.9367, 42.6004, 10.5618,  1.4243,  1.0258, 17.2134]],

        [[29.5045, 41.3263, 10.3181,  0.9807,  1.4495, 21.3856]],

        [[25.0374, 42.2177, 12.6079,  1.8578,  2.1035, 41.0139]]])
Signal Variance: tensor([ 0.1847,  1.6539, 16.0272,  0.4044])
Estimated target variance: tensor([0.0184, 0.8470, 8.7051, 0.0796])
N: 100
Signal to noise ratio: tensor([ 24.9401,  65.8155, 102.2646,  44.7549])
Bound on condition number: tensor([  62202.0864,  433169.5187, 1045805.4865,  200300.7492])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.002829909667669518, policy loss: 11.78252465389327
Experience 10, Iter 1, disc loss: 0.001731210709913319, policy loss: 11.669268882802953
Experience 10, Iter 2, disc loss: 0.002052147557709283, policy loss: 11.033853079582117
Experience 10, Iter 3, disc loss: 0.0017125341218444797, policy loss: 11.24728808701909
Experience 10, Iter 4, disc loss: 0.003853968257716636, policy loss: 11.010048513960875
Experience 10, Iter 5, disc loss: 0.002124644790409179, policy loss: 11.205723921120365
Experience 10, Iter 6, disc loss: 0.0017824528830972493, policy loss: 11.699202520136438
Experience 10, Iter 7, disc loss: 0.0017830725581685588, policy loss: 10.503659012612701
Experience 10, Iter 8, disc loss: 0.002246080437535319, policy loss: 12.45867185427957
Experience 10, Iter 9, disc loss: 0.0018098037803254375, policy loss: 11.513944168156385
Experience 10, Iter 10, disc loss: 0.00139637694517833, policy loss: 12.381707377696827
Experience 10, Iter 11, disc loss: 0.0015336058876833508, policy loss: 11.61289123932313
Experience 10, Iter 12, disc loss: 0.0016331859845669643, policy loss: 12.020544213003163
Experience 10, Iter 13, disc loss: 0.0028073209483597513, policy loss: 11.201191217345189
Experience 10, Iter 14, disc loss: 0.0014540468035011973, policy loss: 11.360523587655706
Experience 10, Iter 15, disc loss: 0.0018721729467429215, policy loss: 11.382398131586966
Experience 10, Iter 16, disc loss: 0.00392041681067672, policy loss: 11.509256417704103
Experience 10, Iter 17, disc loss: 0.0014833707945547722, policy loss: 12.041452689522632
Experience 10, Iter 18, disc loss: 0.0020228537892373493, policy loss: 11.28725229503673
Experience 10, Iter 19, disc loss: 0.0014382929007724448, policy loss: 12.588339889527656
Experience 10, Iter 20, disc loss: 0.00298652535718456, policy loss: 11.605442350512432
Experience 10, Iter 21, disc loss: 0.0020784877518087567, policy loss: 11.403901198253454
Experience 10, Iter 22, disc loss: 0.004939225958464824, policy loss: 11.939077142222843
Experience 10, Iter 23, disc loss: 0.0032864071027877565, policy loss: 11.420882260354922
Experience 10, Iter 24, disc loss: 0.0034172121239664163, policy loss: 10.970776380856336
Experience 10, Iter 25, disc loss: 0.0017274834109951526, policy loss: 12.32215206176538
Experience 10, Iter 26, disc loss: 0.0018198046948472739, policy loss: 12.27967795803579
Experience 10, Iter 27, disc loss: 0.004092014847148784, policy loss: 11.948882883567958
Experience 10, Iter 28, disc loss: 0.0024523496943081686, policy loss: 10.87037272898337
Experience 10, Iter 29, disc loss: 0.0019062694304274558, policy loss: 12.764860022190078
Experience 10, Iter 30, disc loss: 0.0014290169455318516, policy loss: 11.805510840373179
Experience 10, Iter 31, disc loss: 0.00289359620074731, policy loss: 11.736476501387383
Experience 10, Iter 32, disc loss: 0.0016932984741194902, policy loss: 11.8137581948876
Experience 10, Iter 33, disc loss: 0.0031417554378308328, policy loss: 10.37000766659656
Experience 10, Iter 34, disc loss: 0.00273642739737084, policy loss: 11.203425949500964
Experience 10, Iter 35, disc loss: 0.0017599029770248237, policy loss: 11.482701397419127
Experience 10, Iter 36, disc loss: 0.001650609524096575, policy loss: 12.132883297015399
Experience 10, Iter 37, disc loss: 0.0015858193869124587, policy loss: 11.376016705873422
Experience 10, Iter 38, disc loss: 0.001472929483818704, policy loss: 12.079534006719973
Experience 10, Iter 39, disc loss: 0.0014135538309991702, policy loss: 11.911257449910217
Experience 10, Iter 40, disc loss: 0.0011599349153385069, policy loss: 11.85322836443423
Experience 10, Iter 41, disc loss: 0.001569143194453117, policy loss: 11.413632636897072
Experience 10, Iter 42, disc loss: 0.001701701746378985, policy loss: 10.934786884985748
Experience 10, Iter 43, disc loss: 0.0016342355328363128, policy loss: 11.933010593930417
Experience 10, Iter 44, disc loss: 0.001485786915768194, policy loss: 11.902352414824108
Experience 10, Iter 45, disc loss: 0.0015507036844745617, policy loss: 11.513398807168583
Experience 10, Iter 46, disc loss: 0.002099128867411592, policy loss: 12.201205105548397
Experience 10, Iter 47, disc loss: 0.001885481396092255, policy loss: 10.229852263658776
Experience 10, Iter 48, disc loss: 0.0014688187708302458, policy loss: 11.582810868402309
Experience 10, Iter 49, disc loss: 0.001921114170518413, policy loss: 11.285626394021735
Experience 10, Iter 50, disc loss: 0.0014975880444844155, policy loss: 11.479701535096957
Experience 10, Iter 51, disc loss: 0.001867573150291071, policy loss: 12.20334403370719
Experience 10, Iter 52, disc loss: 0.0015114593766622612, policy loss: 11.227601383212203
Experience 10, Iter 53, disc loss: 0.002490184137320157, policy loss: 11.43066832705253
Experience 10, Iter 54, disc loss: 0.0015234047206532865, policy loss: 11.454043952862662
Experience 10, Iter 55, disc loss: 0.0017117343440908887, policy loss: 11.295593068137272
Experience 10, Iter 56, disc loss: 0.0012528724522103112, policy loss: 13.121328002339542
Experience 10, Iter 57, disc loss: 0.0014123798945923118, policy loss: 12.329036233089177
Experience 10, Iter 58, disc loss: 0.001494258666978463, policy loss: 11.963288040552882
Experience 10, Iter 59, disc loss: 0.001259141844053813, policy loss: 13.547258412217937
Experience 10, Iter 60, disc loss: 0.0017146305381647309, policy loss: 12.230143731904073
Experience 10, Iter 61, disc loss: 0.0013712090977565142, policy loss: 11.95576947112989
Experience 10, Iter 62, disc loss: 0.0012669741105759916, policy loss: 11.668489004527231
Experience 10, Iter 63, disc loss: 0.001494951662871513, policy loss: 11.638785126797664
Experience 10, Iter 64, disc loss: 0.0014450443370819122, policy loss: 11.485071784237928
Experience 10, Iter 65, disc loss: 0.0016290773747270678, policy loss: 12.976042716466644
Experience 10, Iter 66, disc loss: 0.0026385117679337167, policy loss: 11.986087078310714
Experience 10, Iter 67, disc loss: 0.001693977894184404, policy loss: 11.51116302643683
Experience 10, Iter 68, disc loss: 0.0011061243452085453, policy loss: 13.451005493894886
Experience 10, Iter 69, disc loss: 0.001833002390528091, policy loss: 12.194552814456731
Experience 10, Iter 70, disc loss: 0.0013309659768581611, policy loss: 12.117598796703014
Experience 10, Iter 71, disc loss: 0.001454230692833612, policy loss: 11.630973216429032
Experience 10, Iter 72, disc loss: 0.0010497450553555554, policy loss: 13.882010846037431
Experience 10, Iter 73, disc loss: 0.002314364770887221, policy loss: 12.106342820776717
Experience 10, Iter 74, disc loss: 0.003103953878485709, policy loss: 11.557120420370168
Experience 10, Iter 75, disc loss: 0.0017702749546468816, policy loss: 11.857781176205497
Experience 10, Iter 76, disc loss: 0.0014278705255501038, policy loss: 11.79858515346237
Experience 10, Iter 77, disc loss: 0.0015378585271703434, policy loss: 11.945441730832908
Experience 10, Iter 78, disc loss: 0.0013274830695120796, policy loss: 11.629227398865913
Experience 10, Iter 79, disc loss: 0.0012079504606422773, policy loss: 12.866132452854373
Experience 10, Iter 80, disc loss: 0.001385594388411552, policy loss: 12.319905542975686
Experience 10, Iter 81, disc loss: 0.0013748053757343544, policy loss: 11.804046432576271
Experience 10, Iter 82, disc loss: 0.001589214406602223, policy loss: 12.016907940706956
Experience 10, Iter 83, disc loss: 0.0030246449526934215, policy loss: 11.821666080927544
Experience 10, Iter 84, disc loss: 0.0016452753780940121, policy loss: 10.89409197071276
Experience 10, Iter 85, disc loss: 0.0017932286691283465, policy loss: 13.560399709456503
Experience 10, Iter 86, disc loss: 0.0023208206016594, policy loss: 12.06600752022126
Experience 10, Iter 87, disc loss: 0.0029511805464661465, policy loss: 12.374728143520766
Experience 10, Iter 88, disc loss: 0.0013216887391357367, policy loss: 13.013649894954966
Experience 10, Iter 89, disc loss: 0.0010062880939823529, policy loss: 12.78545357178712
Experience 10, Iter 90, disc loss: 0.003024017312753265, policy loss: 12.544912432164265
Experience 10, Iter 91, disc loss: 0.0017315959831760517, policy loss: 11.896075741446204
Experience 10, Iter 92, disc loss: 0.0012247595412173534, policy loss: 11.737872147525756
Experience 10, Iter 93, disc loss: 0.0010664967815719989, policy loss: 12.69015569088588
Experience 10, Iter 94, disc loss: 0.0010342645189246658, policy loss: 13.089194379158334
Experience 10, Iter 95, disc loss: 0.001101013539208765, policy loss: 13.205604641310314
Experience 10, Iter 96, disc loss: 0.0013402377607122803, policy loss: 12.635224275495963
Experience 10, Iter 97, disc loss: 0.0012397797372172923, policy loss: 13.092069045145557
Experience 10, Iter 98, disc loss: 0.0028661946333805193, policy loss: 11.701811039152496
Experience 10, Iter 99, disc loss: 0.0012706064550811421, policy loss: 11.9953985039527
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2125],
        [2.1668],
        [0.0196]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0396, 0.2242, 1.0164, 0.0163, 0.0057, 4.7949]],

        [[0.0396, 0.2242, 1.0164, 0.0163, 0.0057, 4.7949]],

        [[0.0396, 0.2242, 1.0164, 0.0163, 0.0057, 4.7949]],

        [[0.0396, 0.2242, 1.0164, 0.0163, 0.0057, 4.7949]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0192, 0.8498, 8.6672, 0.0783], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0192, 0.8498, 8.6672, 0.0783])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.065
Iter 2/2000 - Loss: 3.554
Iter 3/2000 - Loss: 3.379
Iter 4/2000 - Loss: 3.464
Iter 5/2000 - Loss: 3.553
Iter 6/2000 - Loss: 3.521
Iter 7/2000 - Loss: 3.415
Iter 8/2000 - Loss: 3.303
Iter 9/2000 - Loss: 3.213
Iter 10/2000 - Loss: 3.144
Iter 11/2000 - Loss: 3.094
Iter 12/2000 - Loss: 3.051
Iter 13/2000 - Loss: 2.996
Iter 14/2000 - Loss: 2.913
Iter 15/2000 - Loss: 2.796
Iter 16/2000 - Loss: 2.654
Iter 17/2000 - Loss: 2.501
Iter 18/2000 - Loss: 2.350
Iter 19/2000 - Loss: 2.203
Iter 20/2000 - Loss: 2.052
Iter 1981/2000 - Loss: -7.127
Iter 1982/2000 - Loss: -7.127
Iter 1983/2000 - Loss: -7.127
Iter 1984/2000 - Loss: -7.127
Iter 1985/2000 - Loss: -7.127
Iter 1986/2000 - Loss: -7.127
Iter 1987/2000 - Loss: -7.127
Iter 1988/2000 - Loss: -7.127
Iter 1989/2000 - Loss: -7.127
Iter 1990/2000 - Loss: -7.127
Iter 1991/2000 - Loss: -7.127
Iter 1992/2000 - Loss: -7.127
Iter 1993/2000 - Loss: -7.127
Iter 1994/2000 - Loss: -7.128
Iter 1995/2000 - Loss: -7.128
Iter 1996/2000 - Loss: -7.128
Iter 1997/2000 - Loss: -7.128
Iter 1998/2000 - Loss: -7.128
Iter 1999/2000 - Loss: -7.128
Iter 2000/2000 - Loss: -7.128
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[21.9814, 11.4696, 25.1464,  9.7480,  3.5571, 65.4605]],

        [[28.1504, 42.7225, 10.5717,  1.2572,  1.2082, 17.0249]],

        [[28.4305, 45.9188, 10.7017,  0.9566,  1.4847, 20.3574]],

        [[23.7786, 42.2005, 12.4368,  1.9203,  1.9826, 40.8410]]])
Signal Variance: tensor([ 0.1927,  1.5861, 15.2069,  0.3778])
Estimated target variance: tensor([0.0192, 0.8498, 8.6672, 0.0783])
N: 110
Signal to noise ratio: tensor([ 25.8797,  67.1396, 100.2482,  43.1344])
Bound on condition number: tensor([  73674.5005,  495851.5371, 1105467.3072,  204664.8712])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.0019450541228891074, policy loss: 11.71952795382454
Experience 11, Iter 1, disc loss: 0.0013490758494919832, policy loss: 12.161998867375605
Experience 11, Iter 2, disc loss: 0.0014157494189694342, policy loss: 11.895953781182381
Experience 11, Iter 3, disc loss: 0.0015382290541653028, policy loss: 11.328806439519004
Experience 11, Iter 4, disc loss: 0.002137310257906272, policy loss: 11.51953303371399
Experience 11, Iter 5, disc loss: 0.0010197184315701127, policy loss: 12.119296840608564
Experience 11, Iter 6, disc loss: 0.0010673322989618373, policy loss: 12.001362872468963
Experience 11, Iter 7, disc loss: 0.001268763660748914, policy loss: 11.676337230661431
Experience 11, Iter 8, disc loss: 0.0027130967662678377, policy loss: 12.445982815556047
Experience 11, Iter 9, disc loss: 0.0014034489753960073, policy loss: 10.365463571256608
Experience 11, Iter 10, disc loss: 0.0011928166200588651, policy loss: 12.429166442739394
Experience 11, Iter 11, disc loss: 0.001090340231696369, policy loss: 12.160871116984197
Experience 11, Iter 12, disc loss: 0.0010724301744984915, policy loss: 12.245582721843974
Experience 11, Iter 13, disc loss: 0.0017709764514655464, policy loss: 11.297998247271373
Experience 11, Iter 14, disc loss: 0.003073996606986621, policy loss: 13.117578209287416
Experience 11, Iter 15, disc loss: 0.0014813219603323942, policy loss: 11.738777127482228
Experience 11, Iter 16, disc loss: 0.0013177460639569416, policy loss: 12.545756925565726
Experience 11, Iter 17, disc loss: 0.0010533013035195946, policy loss: 13.194978242118573
Experience 11, Iter 18, disc loss: 0.001382186374822177, policy loss: 11.458010363120579
Experience 11, Iter 19, disc loss: 0.0012961293853829279, policy loss: 11.98717715575626
Experience 11, Iter 20, disc loss: 0.001664820514280472, policy loss: 11.7825510727403
Experience 11, Iter 21, disc loss: 0.003247704284537827, policy loss: 11.600700784776759
Experience 11, Iter 22, disc loss: 0.000906947616162535, policy loss: 14.719807769207812
Experience 11, Iter 23, disc loss: 0.0012683932836508854, policy loss: 12.288697669041039
Experience 11, Iter 24, disc loss: 0.001294091611154935, policy loss: 11.18140359739571
Experience 11, Iter 25, disc loss: 0.0019779781823442938, policy loss: 11.206017790319514
Experience 11, Iter 26, disc loss: 0.0028402305957639265, policy loss: 12.550674268460076
Experience 11, Iter 27, disc loss: 0.00128139198543325, policy loss: 13.418886902042718
Experience 11, Iter 28, disc loss: 0.0013703734319941991, policy loss: 13.482551679849191
Experience 11, Iter 29, disc loss: 0.0012073312849830392, policy loss: 12.388047388868552
Experience 11, Iter 30, disc loss: 0.0013541636663322696, policy loss: 12.153488859691667
Experience 11, Iter 31, disc loss: 0.0013525673629291202, policy loss: 12.212813424095675
Experience 11, Iter 32, disc loss: 0.001091216764914514, policy loss: 13.304077013261134
Experience 11, Iter 33, disc loss: 0.0009387045569360496, policy loss: 13.643334269802304
Experience 11, Iter 34, disc loss: 0.0021561997521463627, policy loss: 11.289899721102358
Experience 11, Iter 35, disc loss: 0.001510083668466064, policy loss: 11.256791046744981
Experience 11, Iter 36, disc loss: 0.001218836431723884, policy loss: 10.939214678239848
Experience 11, Iter 37, disc loss: 0.00099748782315034, policy loss: 12.38645143946215
Experience 11, Iter 38, disc loss: 0.0010145768743974005, policy loss: 12.479762927435294
Experience 11, Iter 39, disc loss: 0.00134544132012659, policy loss: 12.894556967651138
Experience 11, Iter 40, disc loss: 0.0011644193655624472, policy loss: 12.969062815929474
Experience 11, Iter 41, disc loss: 0.0009420819059170354, policy loss: 12.561705030489382
Experience 11, Iter 42, disc loss: 0.001404266053988773, policy loss: 11.206718147049273
Experience 11, Iter 43, disc loss: 0.0011189367548879996, policy loss: 12.544559991776726
Experience 11, Iter 44, disc loss: 0.00090052394481803, policy loss: 12.949544613164317
Experience 11, Iter 45, disc loss: 0.002585698982713865, policy loss: 12.88407255333637
Experience 11, Iter 46, disc loss: 0.001083782405778565, policy loss: 11.88556996841028
Experience 11, Iter 47, disc loss: 0.001317080620319123, policy loss: 11.69994637900028
Experience 11, Iter 48, disc loss: 0.0010854354325851635, policy loss: 13.159876235498892
Experience 11, Iter 49, disc loss: 0.0009683781440845647, policy loss: 12.152892903181959
Experience 11, Iter 50, disc loss: 0.0017686068864208259, policy loss: 10.957384464926244
Experience 11, Iter 51, disc loss: 0.0022766765022354963, policy loss: 12.091739055605256
Experience 11, Iter 52, disc loss: 0.001464523488635027, policy loss: 12.545293254928787
Experience 11, Iter 53, disc loss: 0.0011008588017921716, policy loss: 12.597644418648382
Experience 11, Iter 54, disc loss: 0.0012407622994758188, policy loss: 12.246366690540158
Experience 11, Iter 55, disc loss: 0.001009420219114124, policy loss: 11.815040483808989
Experience 11, Iter 56, disc loss: 0.0010322512125794945, policy loss: 13.298619211879586
Experience 11, Iter 57, disc loss: 0.0010443504238789557, policy loss: 10.956655554476376
Experience 11, Iter 58, disc loss: 0.0016259222578255284, policy loss: 11.690021704024826
Experience 11, Iter 59, disc loss: 0.0012110490229536666, policy loss: 12.558025314193507
Experience 11, Iter 60, disc loss: 0.001207924883184179, policy loss: 12.153046816259524
Experience 11, Iter 61, disc loss: 0.001051597696388276, policy loss: 12.282073360362949
Experience 11, Iter 62, disc loss: 0.000969477365841264, policy loss: 12.709322167079737
Experience 11, Iter 63, disc loss: 0.0011174547375379883, policy loss: 11.62905462070524
Experience 11, Iter 64, disc loss: 0.0013878033174562479, policy loss: 11.676913139393111
Experience 11, Iter 65, disc loss: 0.0013272745921933354, policy loss: 12.030996369731437
Experience 11, Iter 66, disc loss: 0.0015614697480845164, policy loss: 12.732928178877147
Experience 11, Iter 67, disc loss: 0.001073773475144333, policy loss: 12.392027330494267
Experience 11, Iter 68, disc loss: 0.0009587192782928072, policy loss: 12.728756043341825
Experience 11, Iter 69, disc loss: 0.00105049475763731, policy loss: 12.99638705260206
Experience 11, Iter 70, disc loss: 0.0010500322289161259, policy loss: 12.102357738875662
Experience 11, Iter 71, disc loss: 0.0023351477831437175, policy loss: 11.345940223939763
Experience 11, Iter 72, disc loss: 0.0015387276206521054, policy loss: 12.0634914007844
Experience 11, Iter 73, disc loss: 0.0009597456839221145, policy loss: 11.708938296904678
Experience 11, Iter 74, disc loss: 0.0008634558124993575, policy loss: 12.233089344038273
Experience 11, Iter 75, disc loss: 0.0013244835880577516, policy loss: 11.55946069724428
Experience 11, Iter 76, disc loss: 0.0011020204106191246, policy loss: 12.446349331028348
Experience 11, Iter 77, disc loss: 0.0010670264210693712, policy loss: 13.703529483475267
Experience 11, Iter 78, disc loss: 0.0010616478530213655, policy loss: 11.67442256431127
Experience 11, Iter 79, disc loss: 0.0009196279404371043, policy loss: 12.476864445107852
Experience 11, Iter 80, disc loss: 0.0012089650908052893, policy loss: 13.320973467639607
Experience 11, Iter 81, disc loss: 0.0009097848938267364, policy loss: 13.774836758809094
Experience 11, Iter 82, disc loss: 0.0008201201647452178, policy loss: 13.387712559339846
Experience 11, Iter 83, disc loss: 0.0015021438888051066, policy loss: 11.28488409047422
Experience 11, Iter 84, disc loss: 0.0008172755364496314, policy loss: 14.131347374548948
Experience 11, Iter 85, disc loss: 0.00090916046172631, policy loss: 11.047470572410685
Experience 11, Iter 86, disc loss: 0.0010191618731246153, policy loss: 11.976915072662894
Experience 11, Iter 87, disc loss: 0.0010470900267618385, policy loss: 13.117498043187076
Experience 11, Iter 88, disc loss: 0.000880635969334885, policy loss: 12.890969984039112
Experience 11, Iter 89, disc loss: 0.0010881865586211993, policy loss: 11.854586323790574
Experience 11, Iter 90, disc loss: 0.0008433706237722492, policy loss: 12.590099364229701
Experience 11, Iter 91, disc loss: 0.0015196204616885268, policy loss: 11.732037454916803
Experience 11, Iter 92, disc loss: 0.001341468581635235, policy loss: 11.996570985379071
Experience 11, Iter 93, disc loss: 0.001001535488846454, policy loss: 12.125454725390144
Experience 11, Iter 94, disc loss: 0.000855131033042268, policy loss: 14.45619064325414
Experience 11, Iter 95, disc loss: 0.0008375541518938112, policy loss: 11.739821587145968
Experience 11, Iter 96, disc loss: 0.0008528193878286427, policy loss: 12.899988531389887
Experience 11, Iter 97, disc loss: 0.0008315750123466749, policy loss: 13.690573683479736
Experience 11, Iter 98, disc loss: 0.0022801595497522597, policy loss: 12.700540980393527
Experience 11, Iter 99, disc loss: 0.000982294286865367, policy loss: 13.984456828272483
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.2050],
        [2.1166],
        [0.0214]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0413, 0.2185, 1.0863, 0.0174, 0.0062, 4.7136]],

        [[0.0413, 0.2185, 1.0863, 0.0174, 0.0062, 4.7136]],

        [[0.0413, 0.2185, 1.0863, 0.0174, 0.0062, 4.7136]],

        [[0.0413, 0.2185, 1.0863, 0.0174, 0.0062, 4.7136]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0183, 0.8201, 8.4663, 0.0855], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0183, 0.8201, 8.4663, 0.0855])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.130
Iter 2/2000 - Loss: 3.560
Iter 3/2000 - Loss: 3.369
Iter 4/2000 - Loss: 3.451
Iter 5/2000 - Loss: 3.549
Iter 6/2000 - Loss: 3.532
Iter 7/2000 - Loss: 3.434
Iter 8/2000 - Loss: 3.318
Iter 9/2000 - Loss: 3.214
Iter 10/2000 - Loss: 3.130
Iter 11/2000 - Loss: 3.068
Iter 12/2000 - Loss: 3.019
Iter 13/2000 - Loss: 2.964
Iter 14/2000 - Loss: 2.881
Iter 15/2000 - Loss: 2.760
Iter 16/2000 - Loss: 2.607
Iter 17/2000 - Loss: 2.438
Iter 18/2000 - Loss: 2.266
Iter 19/2000 - Loss: 2.097
Iter 20/2000 - Loss: 1.926
Iter 1981/2000 - Loss: -7.292
Iter 1982/2000 - Loss: -7.292
Iter 1983/2000 - Loss: -7.292
Iter 1984/2000 - Loss: -7.292
Iter 1985/2000 - Loss: -7.292
Iter 1986/2000 - Loss: -7.292
Iter 1987/2000 - Loss: -7.292
Iter 1988/2000 - Loss: -7.292
Iter 1989/2000 - Loss: -7.293
Iter 1990/2000 - Loss: -7.293
Iter 1991/2000 - Loss: -7.293
Iter 1992/2000 - Loss: -7.293
Iter 1993/2000 - Loss: -7.293
Iter 1994/2000 - Loss: -7.293
Iter 1995/2000 - Loss: -7.293
Iter 1996/2000 - Loss: -7.293
Iter 1997/2000 - Loss: -7.293
Iter 1998/2000 - Loss: -7.293
Iter 1999/2000 - Loss: -7.293
Iter 2000/2000 - Loss: -7.293
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[21.9597, 11.1790, 26.9289,  6.2457,  6.1919, 62.1803]],

        [[28.2231, 41.5904, 10.5456,  1.0609,  1.6197, 17.3446]],

        [[27.1442, 42.8645, 11.1925,  0.9111,  1.4688, 21.4348]],

        [[23.5418, 39.9221, 12.3850,  1.8567,  1.9464, 41.7659]]])
Signal Variance: tensor([ 0.1843,  1.5778, 16.0334,  0.3713])
Estimated target variance: tensor([0.0183, 0.8201, 8.4663, 0.0855])
N: 120
Signal to noise ratio: tensor([ 25.2450,  68.6646, 100.6668,  43.7058])
Bound on condition number: tensor([  76478.4873,  565780.7559, 1216056.4412,  229224.5687])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.0009207685224247874, policy loss: 12.450827563843593
Experience 12, Iter 1, disc loss: 0.0013721600999756559, policy loss: 11.997189528167917
Experience 12, Iter 2, disc loss: 0.0009882187743324852, policy loss: 13.004907045969903
Experience 12, Iter 3, disc loss: 0.0024143154179827077, policy loss: 10.979436915868469
Experience 12, Iter 4, disc loss: 0.0009623561010330509, policy loss: 12.459777956232568
Experience 12, Iter 5, disc loss: 0.000842121170197303, policy loss: 12.204174760588101
Experience 12, Iter 6, disc loss: 0.0012750361942773093, policy loss: 12.379791623287048
Experience 12, Iter 7, disc loss: 0.0012119011093704379, policy loss: 11.96479006595646
Experience 12, Iter 8, disc loss: 0.0010397766411428377, policy loss: 12.485816783620827
Experience 12, Iter 9, disc loss: 0.0010891095851603971, policy loss: 11.575223305692395
Experience 12, Iter 10, disc loss: 0.0015686467707539638, policy loss: 12.484211180587312
Experience 12, Iter 11, disc loss: 0.0019851186885063276, policy loss: 11.487980117840484
Experience 12, Iter 12, disc loss: 0.0010572063853651586, policy loss: 13.034254853047301
Experience 12, Iter 13, disc loss: 0.0012339342127034158, policy loss: 12.002301254783028
Experience 12, Iter 14, disc loss: 0.0009816238935219164, policy loss: 12.625034131032688
Experience 12, Iter 15, disc loss: 0.0007138543577556406, policy loss: 13.73139261059594
Experience 12, Iter 16, disc loss: 0.0017360625058954666, policy loss: 12.67809191624587
Experience 12, Iter 17, disc loss: 0.001665236919468009, policy loss: 12.392650205009224
Experience 12, Iter 18, disc loss: 0.0008394722418551353, policy loss: 12.48738274681458
Experience 12, Iter 19, disc loss: 0.0010935016530179446, policy loss: 11.336604803715861
Experience 12, Iter 20, disc loss: 0.0013187394527058994, policy loss: 12.0222429642307
Experience 12, Iter 21, disc loss: 0.0011742311541358653, policy loss: 12.648394713889086
Experience 12, Iter 22, disc loss: 0.002172357594485233, policy loss: 11.663377335772214
Experience 12, Iter 23, disc loss: 0.001267229896528117, policy loss: 12.228580822137058
Experience 12, Iter 24, disc loss: 0.0009800891044497062, policy loss: 13.602966110287609
Experience 12, Iter 25, disc loss: 0.0011062651668906913, policy loss: 12.140544860186576
Experience 12, Iter 26, disc loss: 0.0007592172153392438, policy loss: 12.592936756828564
Experience 12, Iter 27, disc loss: 0.0008319832412192472, policy loss: 12.619667082452036
Experience 12, Iter 28, disc loss: 0.0011094851226125773, policy loss: 11.431728677534307
Experience 12, Iter 29, disc loss: 0.002094555167300826, policy loss: 12.151674204281182
Experience 12, Iter 30, disc loss: 0.0010296825491259154, policy loss: 12.700313782047692
Experience 12, Iter 31, disc loss: 0.0008956109861110175, policy loss: 12.56152259330279
Experience 12, Iter 32, disc loss: 0.000754054278354383, policy loss: 13.539472092723155
Experience 12, Iter 33, disc loss: 0.0010479443827917784, policy loss: 13.281889489329915
Experience 12, Iter 34, disc loss: 0.0011531812969074813, policy loss: 13.565136458734287
Experience 12, Iter 35, disc loss: 0.0012916489933427588, policy loss: 11.762917595924113
Experience 12, Iter 36, disc loss: 0.0008794122084466254, policy loss: 12.15139574294659
Experience 12, Iter 37, disc loss: 0.0009878011007073513, policy loss: 11.015921382281537
Experience 12, Iter 38, disc loss: 0.0008812352603261341, policy loss: 11.348023495342677
Experience 12, Iter 39, disc loss: 0.0011745052741140233, policy loss: 13.653332888726464
Experience 12, Iter 40, disc loss: 0.0012176752912022174, policy loss: 11.602195741372881
Experience 12, Iter 41, disc loss: 0.0020084141212082175, policy loss: 12.323393031035463
Experience 12, Iter 42, disc loss: 0.0007292621619104231, policy loss: 12.10664227703143
Experience 12, Iter 43, disc loss: 0.0009254975642235987, policy loss: 11.91358217166994
Experience 12, Iter 44, disc loss: 0.001050270895610128, policy loss: 14.112036413036131
Experience 12, Iter 45, disc loss: 0.0008072234796634035, policy loss: 12.707640047626029
Experience 12, Iter 46, disc loss: 0.0008601394089240659, policy loss: 10.993919477842415
Experience 12, Iter 47, disc loss: 0.0009072572174883568, policy loss: 12.605570902323635
Experience 12, Iter 48, disc loss: 0.000763372782204338, policy loss: 13.975514244862529
Experience 12, Iter 49, disc loss: 0.0007784097932565721, policy loss: 13.601636065886069
Experience 12, Iter 50, disc loss: 0.0019403515606132968, policy loss: 12.338042564248772
Experience 12, Iter 51, disc loss: 0.0013448673789854108, policy loss: 13.665471535326954
Experience 12, Iter 52, disc loss: 0.0007756450903150165, policy loss: 12.80338304017684
Experience 12, Iter 53, disc loss: 0.0008623155100711855, policy loss: 12.027066707164481
Experience 12, Iter 54, disc loss: 0.0008861436845521771, policy loss: 12.095954158380207
Experience 12, Iter 55, disc loss: 0.0007022076330861367, policy loss: 13.438575639708798
Experience 12, Iter 56, disc loss: 0.0016234258791321112, policy loss: 12.182088744049533
Experience 12, Iter 57, disc loss: 0.0008954847835462149, policy loss: 12.186556840772223
Experience 12, Iter 58, disc loss: 0.000867286983496054, policy loss: 12.26934518511715
Experience 12, Iter 59, disc loss: 0.0014997263175641064, policy loss: 12.259327715825103
Experience 12, Iter 60, disc loss: 0.0006751083914607096, policy loss: 13.446000072481286
Experience 12, Iter 61, disc loss: 0.0009225735334210426, policy loss: 13.143018695882056
Experience 12, Iter 62, disc loss: 0.0009523003305079079, policy loss: 11.702359317273526
Experience 12, Iter 63, disc loss: 0.0016479067719785499, policy loss: 13.250262931242254
Experience 12, Iter 64, disc loss: 0.0009067940218832548, policy loss: 13.390522323971659
Experience 12, Iter 65, disc loss: 0.0012272911913845624, policy loss: 12.90085722836884
Experience 12, Iter 66, disc loss: 0.0025762702743016043, policy loss: 11.897897911704561
Experience 12, Iter 67, disc loss: 0.0009871082652255611, policy loss: 13.914347141885107
Experience 12, Iter 68, disc loss: 0.0008277823504912637, policy loss: 13.130392400286055
Experience 12, Iter 69, disc loss: 0.0012659691532708678, policy loss: 13.076708899506945
Experience 12, Iter 70, disc loss: 0.002007849227132638, policy loss: 12.9909291193678
Experience 12, Iter 71, disc loss: 0.0010008808113148165, policy loss: 11.50092811010942
Experience 12, Iter 72, disc loss: 0.0009027665657687722, policy loss: 11.822289616626339
Experience 12, Iter 73, disc loss: 0.0007506413742042638, policy loss: 13.175508540953686
Experience 12, Iter 74, disc loss: 0.0009683570977435105, policy loss: 12.13977009782629
Experience 12, Iter 75, disc loss: 0.0008438554051419483, policy loss: 12.963980058356544
Experience 12, Iter 76, disc loss: 0.0007479663603958513, policy loss: 13.353764666837417
Experience 12, Iter 77, disc loss: 0.0008813384465447998, policy loss: 11.82371647643816
Experience 12, Iter 78, disc loss: 0.0013927459317157817, policy loss: 13.287156237710832
Experience 12, Iter 79, disc loss: 0.0007203124601970002, policy loss: 12.447425786332303
Experience 12, Iter 80, disc loss: 0.0008271981220019523, policy loss: 11.650518690748882
Experience 12, Iter 81, disc loss: 0.0008147947940351265, policy loss: 12.883949316562026
Experience 12, Iter 82, disc loss: 0.0008275268678709349, policy loss: 13.221282650517724
Experience 12, Iter 83, disc loss: 0.0008814682908545321, policy loss: 12.80939697844368
Experience 12, Iter 84, disc loss: 0.0006640083116493381, policy loss: 12.718513724876846
Experience 12, Iter 85, disc loss: 0.0007980246257254907, policy loss: 12.667916309388488
Experience 12, Iter 86, disc loss: 0.0007319904922676573, policy loss: 12.768681260660394
Experience 12, Iter 87, disc loss: 0.0007074864842504447, policy loss: 12.821507688031385
Experience 12, Iter 88, disc loss: 0.0007182810637294829, policy loss: 12.902650369773982
Experience 12, Iter 89, disc loss: 0.0009177794514214006, policy loss: 12.802000395618702
Experience 12, Iter 90, disc loss: 0.0008756627803982655, policy loss: 13.716312566210467
Experience 12, Iter 91, disc loss: 0.0015202996596683344, policy loss: 14.306504636760458
Experience 12, Iter 92, disc loss: 0.0008324243582636502, policy loss: 12.027448918829274
Experience 12, Iter 93, disc loss: 0.0006826586536258167, policy loss: 12.98975910502655
Experience 12, Iter 94, disc loss: 0.0007040259771089255, policy loss: 12.576174504421552
Experience 12, Iter 95, disc loss: 0.0016513136412812273, policy loss: 12.282093143120516
Experience 12, Iter 96, disc loss: 0.0005960727378760544, policy loss: 14.812051375602584
Experience 12, Iter 97, disc loss: 0.000964357941621816, policy loss: 12.485740986655262
Experience 12, Iter 98, disc loss: 0.000693349177597188, policy loss: 11.834548288403312
Experience 12, Iter 99, disc loss: 0.0008260047592627679, policy loss: 12.415571676602482
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0044],
        [0.2105],
        [2.1709],
        [0.0212]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0394, 0.2095, 1.0789, 0.0170, 0.0059, 4.7690]],

        [[0.0394, 0.2095, 1.0789, 0.0170, 0.0059, 4.7690]],

        [[0.0394, 0.2095, 1.0789, 0.0170, 0.0059, 4.7690]],

        [[0.0394, 0.2095, 1.0789, 0.0170, 0.0059, 4.7690]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0174, 0.8420, 8.6836, 0.0849], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0174, 0.8420, 8.6836, 0.0849])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.134
Iter 2/2000 - Loss: 3.548
Iter 3/2000 - Loss: 3.361
Iter 4/2000 - Loss: 3.450
Iter 5/2000 - Loss: 3.548
Iter 6/2000 - Loss: 3.525
Iter 7/2000 - Loss: 3.422
Iter 8/2000 - Loss: 3.303
Iter 9/2000 - Loss: 3.200
Iter 10/2000 - Loss: 3.118
Iter 11/2000 - Loss: 3.059
Iter 12/2000 - Loss: 3.015
Iter 13/2000 - Loss: 2.961
Iter 14/2000 - Loss: 2.875
Iter 15/2000 - Loss: 2.749
Iter 16/2000 - Loss: 2.591
Iter 17/2000 - Loss: 2.416
Iter 18/2000 - Loss: 2.239
Iter 19/2000 - Loss: 2.064
Iter 20/2000 - Loss: 1.886
Iter 1981/2000 - Loss: -7.444
Iter 1982/2000 - Loss: -7.444
Iter 1983/2000 - Loss: -7.445
Iter 1984/2000 - Loss: -7.445
Iter 1985/2000 - Loss: -7.445
Iter 1986/2000 - Loss: -7.445
Iter 1987/2000 - Loss: -7.445
Iter 1988/2000 - Loss: -7.445
Iter 1989/2000 - Loss: -7.445
Iter 1990/2000 - Loss: -7.445
Iter 1991/2000 - Loss: -7.445
Iter 1992/2000 - Loss: -7.445
Iter 1993/2000 - Loss: -7.445
Iter 1994/2000 - Loss: -7.445
Iter 1995/2000 - Loss: -7.445
Iter 1996/2000 - Loss: -7.445
Iter 1997/2000 - Loss: -7.445
Iter 1998/2000 - Loss: -7.445
Iter 1999/2000 - Loss: -7.445
Iter 2000/2000 - Loss: -7.445
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[21.4890, 11.0250, 23.8178,  4.7617,  9.2361, 61.3322]],

        [[27.6253, 42.1930, 10.5033,  1.0434,  1.4730, 16.9777]],

        [[27.6118, 43.3928, 11.0134,  0.9247,  1.4647, 21.7458]],

        [[22.5827, 38.3148, 11.1797,  1.5177,  2.1351, 41.6636]]])
Signal Variance: tensor([ 0.1670,  1.4643, 15.5880,  0.3213])
Estimated target variance: tensor([0.0174, 0.8420, 8.6836, 0.0849])
N: 130
Signal to noise ratio: tensor([23.8316, 67.8164, 96.3406, 41.3984])
Bound on condition number: tensor([  73833.8242,  597878.6804, 1206597.8739,  222798.6740])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.0007013758376021842, policy loss: 13.086559302689675
Experience 13, Iter 1, disc loss: 0.0010672448635100275, policy loss: 12.694724960365278
Experience 13, Iter 2, disc loss: 0.000615551361449739, policy loss: 13.549421732954597
Experience 13, Iter 3, disc loss: 0.0006823136667259391, policy loss: 12.2811441671501
Experience 13, Iter 4, disc loss: 0.0009588526720421789, policy loss: 11.59029326335714
Experience 13, Iter 5, disc loss: 0.0008999751302373874, policy loss: 12.273071678992542
Experience 13, Iter 6, disc loss: 0.0007605607094431153, policy loss: 14.407701859820783
Experience 13, Iter 7, disc loss: 0.001969238443140124, policy loss: 13.13786281776295
Experience 13, Iter 8, disc loss: 0.0007840257216298171, policy loss: 13.090142803228558
Experience 13, Iter 9, disc loss: 0.001293626268776864, policy loss: 11.033887260446988
Experience 13, Iter 10, disc loss: 0.000909742009956526, policy loss: 11.333905806332425
Experience 13, Iter 11, disc loss: 0.0006668851351870678, policy loss: 12.702770118522968
Experience 13, Iter 12, disc loss: 0.0010104791337324566, policy loss: 11.719056300114492
Experience 13, Iter 13, disc loss: 0.0007690297528711615, policy loss: 11.87382503675132
Experience 13, Iter 14, disc loss: 0.0018726880965623575, policy loss: 11.940247746871211
Experience 13, Iter 15, disc loss: 0.0010798506130274322, policy loss: 11.671344819606624
Experience 13, Iter 16, disc loss: 0.0016961451880674013, policy loss: 13.85378578392902
Experience 13, Iter 17, disc loss: 0.0005569779586730543, policy loss: 13.703702934998478
Experience 13, Iter 18, disc loss: 0.0016044592277840938, policy loss: 12.376349504836375
Experience 13, Iter 19, disc loss: 0.0006112419458667793, policy loss: 14.291987020499402
Experience 13, Iter 20, disc loss: 0.001162766920058987, policy loss: 11.235972444291288
Experience 13, Iter 21, disc loss: 0.000786538664411282, policy loss: 11.79669174037896
Experience 13, Iter 22, disc loss: 0.000946987231250981, policy loss: 11.88179829963651
Experience 13, Iter 23, disc loss: 0.0008896425243123589, policy loss: 12.440672383696867
Experience 13, Iter 24, disc loss: 0.0007909133330920823, policy loss: 12.343628249397508
Experience 13, Iter 25, disc loss: 0.0008110547506940075, policy loss: 12.185128975723181
Experience 13, Iter 26, disc loss: 0.0006335703521746371, policy loss: 14.766327868420841
Experience 13, Iter 27, disc loss: 0.0019966034881073458, policy loss: 12.126026248577908
Experience 13, Iter 28, disc loss: 0.000761426814621662, policy loss: 12.716418578751925
Experience 13, Iter 29, disc loss: 0.000870841581808573, policy loss: 11.89317610870207
Experience 13, Iter 30, disc loss: 0.0018389146854714037, policy loss: 11.839867774170237
Experience 13, Iter 31, disc loss: 0.0007955504444838977, policy loss: 12.657810804381153
Experience 13, Iter 32, disc loss: 0.0022137721476168676, policy loss: 11.361449651732535
Experience 13, Iter 33, disc loss: 0.0005511778163402138, policy loss: 13.560117958425513
Experience 13, Iter 34, disc loss: 0.0006342823349278824, policy loss: 12.643602353764564
Experience 13, Iter 35, disc loss: 0.0009610504766219866, policy loss: 12.60688579569047
Experience 13, Iter 36, disc loss: 0.0006261279209537066, policy loss: 12.517003452340774
Experience 13, Iter 37, disc loss: 0.000879402061622966, policy loss: 13.167300609488038
Experience 13, Iter 38, disc loss: 0.0005715581932971398, policy loss: 13.303920126604465
Experience 13, Iter 39, disc loss: 0.0006141038185702797, policy loss: 13.194677418616793
Experience 13, Iter 40, disc loss: 0.0008775917129511574, policy loss: 13.275799784131713
Experience 13, Iter 41, disc loss: 0.0010092116783282034, policy loss: 10.7567166985175
Experience 13, Iter 42, disc loss: 0.0018039857072028008, policy loss: 12.11260230131523
Experience 13, Iter 43, disc loss: 0.0008203891261955195, policy loss: 11.751219717111375
Experience 13, Iter 44, disc loss: 0.0009503405341374738, policy loss: 13.552041638309149
Experience 13, Iter 45, disc loss: 0.0005941193238000338, policy loss: 13.500822669035866
Experience 13, Iter 46, disc loss: 0.0007860727568262714, policy loss: 12.20530429588355
Experience 13, Iter 47, disc loss: 0.0007292869358128187, policy loss: 12.940774842589516
Experience 13, Iter 48, disc loss: 0.0006286274106656075, policy loss: 13.314872174420453
Experience 13, Iter 49, disc loss: 0.0008424224477414613, policy loss: 12.416481031025867
Experience 13, Iter 50, disc loss: 0.0008140833766493881, policy loss: 12.944469413872742
Experience 13, Iter 51, disc loss: 0.0011240178644820664, policy loss: 13.95666624672872
Experience 13, Iter 52, disc loss: 0.0007235821256236473, policy loss: 11.568824977639412
Experience 13, Iter 53, disc loss: 0.000681261927814176, policy loss: 11.609688457581264
Experience 13, Iter 54, disc loss: 0.0005934642447741287, policy loss: 14.13211861484563
Experience 13, Iter 55, disc loss: 0.0005778960836859994, policy loss: 13.752301556405126
Experience 13, Iter 56, disc loss: 0.0008396751394395243, policy loss: 12.042496252931446
Experience 13, Iter 57, disc loss: 0.0005781297638565676, policy loss: 13.755493395493476
Experience 13, Iter 58, disc loss: 0.0005404195364022887, policy loss: 12.549339620880492
Experience 13, Iter 59, disc loss: 0.0006609003522640291, policy loss: 13.217872503483035
Experience 13, Iter 60, disc loss: 0.0006523408389465592, policy loss: 11.539309914557466
Experience 13, Iter 61, disc loss: 0.00083489648278041, policy loss: 14.290168352720963
Experience 13, Iter 62, disc loss: 0.0010292668070182947, policy loss: 11.251688655376402
Experience 13, Iter 63, disc loss: 0.0006303769813359669, policy loss: 12.423453582017485
Experience 13, Iter 64, disc loss: 0.0007690511196474305, policy loss: 13.20208661496745
Experience 13, Iter 65, disc loss: 0.0005726475664357374, policy loss: 13.98516462246014
Experience 13, Iter 66, disc loss: 0.000478989840690689, policy loss: 13.949422874222229
Experience 13, Iter 67, disc loss: 0.0005898977380564161, policy loss: 13.198311276682876
Experience 13, Iter 68, disc loss: 0.0006553009622028204, policy loss: 13.484815130315138
Experience 13, Iter 69, disc loss: 0.0007874514776065684, policy loss: 12.349698236162794
Experience 13, Iter 70, disc loss: 0.0006995072824502372, policy loss: 14.125688411734115
Experience 13, Iter 71, disc loss: 0.00136098554199024, policy loss: 14.436475690429198
Experience 13, Iter 72, disc loss: 0.0005787671745784964, policy loss: 14.165703894168505
Experience 13, Iter 73, disc loss: 0.0011456503048654953, policy loss: 14.011477342884577
Experience 13, Iter 74, disc loss: 0.0006732275649225917, policy loss: 12.49606586045108
Experience 13, Iter 75, disc loss: 0.0006231787638272142, policy loss: 12.578521808060067
Experience 13, Iter 76, disc loss: 0.000864450071244838, policy loss: 12.858709277743674
Experience 13, Iter 77, disc loss: 0.0022088774368926563, policy loss: 12.618735433689446
Experience 13, Iter 78, disc loss: 0.002017250169527118, policy loss: 13.557033946386767
Experience 13, Iter 79, disc loss: 0.0012046889341962102, policy loss: 13.273785694198033
Experience 13, Iter 80, disc loss: 0.00082457481574764, policy loss: 12.759740829957126
Experience 13, Iter 81, disc loss: 0.0009703701657304421, policy loss: 11.710129612169775
Experience 13, Iter 82, disc loss: 0.0016243195923365044, policy loss: 14.02386924886951
Experience 13, Iter 83, disc loss: 0.0009597652082296643, policy loss: 14.701444137265321
Experience 13, Iter 84, disc loss: 0.0006605744963096579, policy loss: 12.72950586582096
Experience 13, Iter 85, disc loss: 0.0008755390499842178, policy loss: 12.196036147328828
Experience 13, Iter 86, disc loss: 0.0004915415351221146, policy loss: 13.11918043581762
Experience 13, Iter 87, disc loss: 0.0018412985362846915, policy loss: 12.150931114368284
Experience 13, Iter 88, disc loss: 0.0007058953377266877, policy loss: 12.784233514603738
Experience 13, Iter 89, disc loss: 0.0011024159471730143, policy loss: 12.045513240266882
Experience 13, Iter 90, disc loss: 0.0006120327948958288, policy loss: 12.244903172180472
Experience 13, Iter 91, disc loss: 0.0010568330214748248, policy loss: 13.40733123360122
Experience 13, Iter 92, disc loss: 0.0018193846586047955, policy loss: 11.754883111719476
Experience 13, Iter 93, disc loss: 0.0009030659378526972, policy loss: 14.056544856607271
Experience 13, Iter 94, disc loss: 0.000684064374109811, policy loss: 12.511848868138086
Experience 13, Iter 95, disc loss: 0.0010353151559904464, policy loss: 13.686131435388237
Experience 13, Iter 96, disc loss: 0.0007715070683020724, policy loss: 14.218003434608185
Experience 13, Iter 97, disc loss: 0.002016665831152262, policy loss: 12.912132537056978
Experience 13, Iter 98, disc loss: 0.0005155284374852577, policy loss: 14.623972104545732
Experience 13, Iter 99, disc loss: 0.0007289602147905673, policy loss: 13.171742232776241
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0043],
        [0.2144],
        [2.2057],
        [0.0201]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0382, 0.2055, 1.0394, 0.0163, 0.0056, 4.7707]],

        [[0.0382, 0.2055, 1.0394, 0.0163, 0.0056, 4.7707]],

        [[0.0382, 0.2055, 1.0394, 0.0163, 0.0056, 4.7707]],

        [[0.0382, 0.2055, 1.0394, 0.0163, 0.0056, 4.7707]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0171, 0.8577, 8.8226, 0.0804], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0171, 0.8577, 8.8226, 0.0804])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.066
Iter 2/2000 - Loss: 3.510
Iter 3/2000 - Loss: 3.345
Iter 4/2000 - Loss: 3.442
Iter 5/2000 - Loss: 3.526
Iter 6/2000 - Loss: 3.480
Iter 7/2000 - Loss: 3.362
Iter 8/2000 - Loss: 3.241
Iter 9/2000 - Loss: 3.143
Iter 10/2000 - Loss: 3.070
Iter 11/2000 - Loss: 3.016
Iter 12/2000 - Loss: 2.966
Iter 13/2000 - Loss: 2.898
Iter 14/2000 - Loss: 2.795
Iter 15/2000 - Loss: 2.653
Iter 16/2000 - Loss: 2.484
Iter 17/2000 - Loss: 2.305
Iter 18/2000 - Loss: 2.126
Iter 19/2000 - Loss: 1.947
Iter 20/2000 - Loss: 1.763
Iter 1981/2000 - Loss: -7.646
Iter 1982/2000 - Loss: -7.646
Iter 1983/2000 - Loss: -7.646
Iter 1984/2000 - Loss: -7.646
Iter 1985/2000 - Loss: -7.646
Iter 1986/2000 - Loss: -7.646
Iter 1987/2000 - Loss: -7.646
Iter 1988/2000 - Loss: -7.646
Iter 1989/2000 - Loss: -7.646
Iter 1990/2000 - Loss: -7.646
Iter 1991/2000 - Loss: -7.646
Iter 1992/2000 - Loss: -7.646
Iter 1993/2000 - Loss: -7.646
Iter 1994/2000 - Loss: -7.646
Iter 1995/2000 - Loss: -7.647
Iter 1996/2000 - Loss: -7.647
Iter 1997/2000 - Loss: -7.647
Iter 1998/2000 - Loss: -7.647
Iter 1999/2000 - Loss: -7.647
Iter 2000/2000 - Loss: -7.647
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[20.6985, 11.1537, 23.4345,  4.9116,  9.2442, 61.1200]],

        [[26.7469, 41.6118, 10.6282,  1.1421,  1.3292, 18.4187]],

        [[27.4608, 43.1450, 11.0694,  0.9132,  1.4725, 21.5505]],

        [[21.8359, 37.2177, 11.4969,  1.7048,  1.9659, 42.3477]]])
Signal Variance: tensor([ 0.1699,  1.6438, 15.3139,  0.3378])
Estimated target variance: tensor([0.0171, 0.8577, 8.8226, 0.0804])
N: 140
Signal to noise ratio: tensor([24.6207, 71.5539, 99.5975, 41.9606])
Bound on condition number: tensor([  84866.1159,  716795.6998, 1388753.9590,  246497.5410])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.000749406781697173, policy loss: 13.345551458805417
Experience 14, Iter 1, disc loss: 0.001104862147395148, policy loss: 12.02694082865714
Experience 14, Iter 2, disc loss: 0.0006945302195852342, policy loss: 12.403464057574702
Experience 14, Iter 3, disc loss: 0.0008738818580337308, policy loss: 13.009257832917623
Experience 14, Iter 4, disc loss: 0.0008446513235229292, policy loss: 14.200548484383283
Experience 14, Iter 5, disc loss: 0.0009674683262585257, policy loss: 11.736302478222218
Experience 14, Iter 6, disc loss: 0.0005718926245886407, policy loss: 12.574265027590481
Experience 14, Iter 7, disc loss: 0.0006009461032395562, policy loss: 12.927411264043789
Experience 14, Iter 8, disc loss: 0.0014019274683367707, policy loss: 12.41793942691108
Experience 14, Iter 9, disc loss: 0.0005628579006427873, policy loss: 12.883544460961485
Experience 14, Iter 10, disc loss: 0.0006872296952308695, policy loss: 12.020521152003262
Experience 14, Iter 11, disc loss: 0.000699043556459172, policy loss: 12.717845650301182
Experience 14, Iter 12, disc loss: 0.0011812998818311476, policy loss: 15.245326260561729
Experience 14, Iter 13, disc loss: 0.0007119207032834705, policy loss: 12.153527083062148
Experience 14, Iter 14, disc loss: 0.000675165686489385, policy loss: 11.35366889999771
Experience 14, Iter 15, disc loss: 0.00047879478853980805, policy loss: 13.982491172867814
Experience 14, Iter 16, disc loss: 0.0006052994072350688, policy loss: 12.2768608054071
Experience 14, Iter 17, disc loss: 0.001543297661027666, policy loss: 11.190095863135443
Experience 14, Iter 18, disc loss: 0.0010725239692511695, policy loss: 13.196070823269967
Experience 14, Iter 19, disc loss: 0.0010305567437776805, policy loss: 12.769110372517213
Experience 14, Iter 20, disc loss: 0.0006169623412795599, policy loss: 13.869047802596866
Experience 14, Iter 21, disc loss: 0.0005698380303246292, policy loss: 13.17704558270489
Experience 14, Iter 22, disc loss: 0.0012549554039101571, policy loss: 12.668202939397528
Experience 14, Iter 23, disc loss: 0.0009200936000460676, policy loss: 12.949429102027963
Experience 14, Iter 24, disc loss: 0.0006593882908095209, policy loss: 12.136037296180552
Experience 14, Iter 25, disc loss: 0.0005379000733018364, policy loss: 13.06519427958419
Experience 14, Iter 26, disc loss: 0.0005619498839500723, policy loss: 12.617142316159068
Experience 14, Iter 27, disc loss: 0.0005650822930583488, policy loss: 13.6341941618587
Experience 14, Iter 28, disc loss: 0.0004869620648947766, policy loss: 13.893584940361182
Experience 14, Iter 29, disc loss: 0.0010484345685611162, policy loss: 12.657956220919598
Experience 14, Iter 30, disc loss: 0.0006628808763657633, policy loss: 12.692175338307965
Experience 14, Iter 31, disc loss: 0.0010272179535841008, policy loss: 12.606207712166794
Experience 14, Iter 32, disc loss: 0.0005910246199753446, policy loss: 12.60898849359737
Experience 14, Iter 33, disc loss: 0.0009187757300502199, policy loss: 12.217029430248424
Experience 14, Iter 34, disc loss: 0.0006237966349049704, policy loss: 12.932749334839404
Experience 14, Iter 35, disc loss: 0.0005414814715133544, policy loss: 13.209197294878823
Experience 14, Iter 36, disc loss: 0.00043300845465848466, policy loss: 14.672516695098462
Experience 14, Iter 37, disc loss: 0.0006048249102509448, policy loss: 12.450807748833558
Experience 14, Iter 38, disc loss: 0.0005634630357352575, policy loss: 13.190695414627529
Experience 14, Iter 39, disc loss: 0.0009254248963870009, policy loss: 12.098412411051541
Experience 14, Iter 40, disc loss: 0.0007044962859960109, policy loss: 12.079030132067562
Experience 14, Iter 41, disc loss: 0.0005227539966640934, policy loss: 12.472097873193555
Experience 14, Iter 42, disc loss: 0.0006387247976553536, policy loss: 12.3271981199097
Experience 14, Iter 43, disc loss: 0.0006939344683409075, policy loss: 12.945916573112612
Experience 14, Iter 44, disc loss: 0.0006269456576705366, policy loss: 12.855785872604189
Experience 14, Iter 45, disc loss: 0.0006579314402732156, policy loss: 12.019911060320116
Experience 14, Iter 46, disc loss: 0.0005758940049347566, policy loss: 12.407711793471355
Experience 14, Iter 47, disc loss: 0.0007550453705629087, policy loss: 11.777243594442945
Experience 14, Iter 48, disc loss: 0.0005299738696061592, policy loss: 13.361545602755651
Experience 14, Iter 49, disc loss: 0.0008784629158107423, policy loss: 14.908152247424056
Experience 14, Iter 50, disc loss: 0.0005570503813078377, policy loss: 12.074231114373884
Experience 14, Iter 51, disc loss: 0.00042902355627400066, policy loss: 14.122157339195246
Experience 14, Iter 52, disc loss: 0.0011144451875180685, policy loss: 12.28128492309728
Experience 14, Iter 53, disc loss: 0.00047186215715476937, policy loss: 14.12267968132159
Experience 14, Iter 54, disc loss: 0.0008708060832925429, policy loss: 13.389204009743759
Experience 14, Iter 55, disc loss: 0.0004922882759369362, policy loss: 13.908502445962714
Experience 14, Iter 56, disc loss: 0.0006323555917830732, policy loss: 12.656310521533392
Experience 14, Iter 57, disc loss: 0.0006257791579258308, policy loss: 14.049429445919058
Experience 14, Iter 58, disc loss: 0.0005469887816466246, policy loss: 12.899486174926336
Experience 14, Iter 59, disc loss: 0.0013026528672902655, policy loss: 13.32628237361023
Experience 14, Iter 60, disc loss: 0.0008742159465838506, policy loss: 13.107191255343627
Experience 14, Iter 61, disc loss: 0.0005912586247490769, policy loss: 13.302046302133874
Experience 14, Iter 62, disc loss: 0.0006355463867226698, policy loss: 12.337513299750466
Experience 14, Iter 63, disc loss: 0.0004449973630524202, policy loss: 14.039368397164218
Experience 14, Iter 64, disc loss: 0.00048697378117147606, policy loss: 13.335872172325939
Experience 14, Iter 65, disc loss: 0.0004918386368999806, policy loss: 13.522624537842614
Experience 14, Iter 66, disc loss: 0.0007125927691710994, policy loss: 12.536101750898016
Experience 14, Iter 67, disc loss: 0.00046192271530471837, policy loss: 15.22772848710848
Experience 14, Iter 68, disc loss: 0.0008578211573912208, policy loss: 12.055923351011838
Experience 14, Iter 69, disc loss: 0.000662622151795871, policy loss: 14.171056282502796
Experience 14, Iter 70, disc loss: 0.0006224786505037254, policy loss: 12.131894053802032
Experience 14, Iter 71, disc loss: 0.0008539050152874615, policy loss: 13.934265047203683
Experience 14, Iter 72, disc loss: 0.00048678375016185474, policy loss: 13.637698952267076
Experience 14, Iter 73, disc loss: 0.00047909211974945414, policy loss: 13.778685428552047
Experience 14, Iter 74, disc loss: 0.00044770954563800555, policy loss: 14.130101294864286
Experience 14, Iter 75, disc loss: 0.0004465146861228397, policy loss: 14.212040554886434
Experience 14, Iter 76, disc loss: 0.00046766402954227635, policy loss: 14.20105670185366
Experience 14, Iter 77, disc loss: 0.0005344737718058359, policy loss: 13.84952175524411
Experience 14, Iter 78, disc loss: 0.0006609393934886822, policy loss: 13.062245729577004
Experience 14, Iter 79, disc loss: 0.0013796823679531262, policy loss: 12.508257202466751
Experience 14, Iter 80, disc loss: 0.0005687624102149844, policy loss: 12.449389815155374
Experience 14, Iter 81, disc loss: 0.00037222868146525353, policy loss: 14.08498056272495
Experience 14, Iter 82, disc loss: 0.0010382202615940076, policy loss: 12.238812535947616
Experience 14, Iter 83, disc loss: 0.0005494412093768957, policy loss: 12.419207558064851
Experience 14, Iter 84, disc loss: 0.0005381126160825536, policy loss: 12.475789167622004
Experience 14, Iter 85, disc loss: 0.000671970977547268, policy loss: 13.967169051170526
Experience 14, Iter 86, disc loss: 0.000767452230979384, policy loss: 12.909825332035027
Experience 14, Iter 87, disc loss: 0.0004431826150273212, policy loss: 14.205173032110915
Experience 14, Iter 88, disc loss: 0.0011117936109009642, policy loss: 13.58192286768407
Experience 14, Iter 89, disc loss: 0.001719609025891202, policy loss: 11.993102280685163
Experience 14, Iter 90, disc loss: 0.0007340500337138309, policy loss: 12.561607124937627
Experience 14, Iter 91, disc loss: 0.0005640090174452634, policy loss: 12.77283209577867
Experience 14, Iter 92, disc loss: 0.000492967235013241, policy loss: 13.746358543146972
Experience 14, Iter 93, disc loss: 0.0009518689478935831, policy loss: 12.47426915460698
Experience 14, Iter 94, disc loss: 0.00043144045182021015, policy loss: 13.750740701441181
Experience 14, Iter 95, disc loss: 0.0007712471540902198, policy loss: 13.558676616941709
Experience 14, Iter 96, disc loss: 0.00041309260590508326, policy loss: 12.52080787172695
Experience 14, Iter 97, disc loss: 0.0005318294694180019, policy loss: 12.583353132571785
Experience 14, Iter 98, disc loss: 0.0004282529225699822, policy loss: 14.262707134445355
Experience 14, Iter 99, disc loss: 0.0004962814469225727, policy loss: 13.44785819352624
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2167],
        [2.2300],
        [0.0204]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0369, 0.2037, 1.0498, 0.0162, 0.0055, 4.7961]],

        [[0.0369, 0.2037, 1.0498, 0.0162, 0.0055, 4.7961]],

        [[0.0369, 0.2037, 1.0498, 0.0162, 0.0055, 4.7961]],

        [[0.0369, 0.2037, 1.0498, 0.0162, 0.0055, 4.7961]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0170, 0.8667, 8.9199, 0.0817], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0170, 0.8667, 8.9199, 0.0817])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.020
Iter 2/2000 - Loss: 3.488
Iter 3/2000 - Loss: 3.342
Iter 4/2000 - Loss: 3.440
Iter 5/2000 - Loss: 3.508
Iter 6/2000 - Loss: 3.446
Iter 7/2000 - Loss: 3.318
Iter 8/2000 - Loss: 3.195
Iter 9/2000 - Loss: 3.100
Iter 10/2000 - Loss: 3.030
Iter 11/2000 - Loss: 2.975
Iter 12/2000 - Loss: 2.918
Iter 13/2000 - Loss: 2.836
Iter 14/2000 - Loss: 2.717
Iter 15/2000 - Loss: 2.563
Iter 16/2000 - Loss: 2.387
Iter 17/2000 - Loss: 2.206
Iter 18/2000 - Loss: 2.028
Iter 19/2000 - Loss: 1.848
Iter 20/2000 - Loss: 1.657
Iter 1981/2000 - Loss: -7.673
Iter 1982/2000 - Loss: -7.673
Iter 1983/2000 - Loss: -7.673
Iter 1984/2000 - Loss: -7.673
Iter 1985/2000 - Loss: -7.673
Iter 1986/2000 - Loss: -7.673
Iter 1987/2000 - Loss: -7.673
Iter 1988/2000 - Loss: -7.673
Iter 1989/2000 - Loss: -7.673
Iter 1990/2000 - Loss: -7.673
Iter 1991/2000 - Loss: -7.673
Iter 1992/2000 - Loss: -7.673
Iter 1993/2000 - Loss: -7.673
Iter 1994/2000 - Loss: -7.673
Iter 1995/2000 - Loss: -7.673
Iter 1996/2000 - Loss: -7.673
Iter 1997/2000 - Loss: -7.674
Iter 1998/2000 - Loss: -7.674
Iter 1999/2000 - Loss: -7.674
Iter 2000/2000 - Loss: -7.674
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[20.1461, 10.8938, 24.7202,  4.5428, 10.7391, 61.4346]],

        [[26.0148, 41.4537, 10.4543,  1.1152,  1.4630, 17.7609]],

        [[27.3173, 43.8128, 10.7949,  0.8733,  1.4121, 22.0195]],

        [[21.4954, 36.4704, 12.9192,  1.9551,  2.0043, 42.0699]]])
Signal Variance: tensor([ 0.1595,  1.4750, 14.7888,  0.3886])
Estimated target variance: tensor([0.0170, 0.8667, 8.9199, 0.0817])
N: 150
Signal to noise ratio: tensor([23.9289, 65.9385, 89.9381, 43.0602])
Bound on condition number: tensor([  85889.7461,  652184.3712, 1213329.0820,  278128.3139])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.000447367365984582, policy loss: 12.374870808848193
Experience 15, Iter 1, disc loss: 0.0008949985063806172, policy loss: 13.615617875874593
Experience 15, Iter 2, disc loss: 0.00048705709184672293, policy loss: 12.901236208088122
Experience 15, Iter 3, disc loss: 0.00044570111654560927, policy loss: 14.390624599674972
Experience 15, Iter 4, disc loss: 0.00047227996355100877, policy loss: 14.308404650349194
Experience 15, Iter 5, disc loss: 0.0008095375205417624, policy loss: 14.47439764415445
Experience 15, Iter 6, disc loss: 0.0005859140596030284, policy loss: 14.30610544786026
Experience 15, Iter 7, disc loss: 0.000790444477887776, policy loss: 13.506376251368486
Experience 15, Iter 8, disc loss: 0.0004250725957492561, policy loss: 13.191566967498357
Experience 15, Iter 9, disc loss: 0.0004887501618597128, policy loss: 13.539041177329509
Experience 15, Iter 10, disc loss: 0.00044746405713980294, policy loss: 13.184384046984558
Experience 15, Iter 11, disc loss: 0.0005314707565224209, policy loss: 15.167091532262475
Experience 15, Iter 12, disc loss: 0.0004398160950386482, policy loss: 13.190600076307895
Experience 15, Iter 13, disc loss: 0.0006916578924388351, policy loss: 11.697034625961027
Experience 15, Iter 14, disc loss: 0.0004631078559260886, policy loss: 11.969893456014114
Experience 15, Iter 15, disc loss: 0.0004283979604562363, policy loss: 13.556157020973124
Experience 15, Iter 16, disc loss: 0.00045318714272335323, policy loss: 11.50289322364033
Experience 15, Iter 17, disc loss: 0.00044898395573428264, policy loss: 13.858671145250762
Experience 15, Iter 18, disc loss: 0.00041873579352620486, policy loss: 12.348593999328568
Experience 15, Iter 19, disc loss: 0.0005304104728056158, policy loss: 13.340582539592395
Experience 15, Iter 20, disc loss: 0.0003957107083684759, policy loss: 13.393333786557221
Experience 15, Iter 21, disc loss: 0.0004988613804172092, policy loss: 11.961722954254505
Experience 15, Iter 22, disc loss: 0.0004473476766893741, policy loss: 13.21431020687674
Experience 15, Iter 23, disc loss: 0.00048106941802097296, policy loss: 14.557185988746738
Experience 15, Iter 24, disc loss: 0.00044821914219988525, policy loss: 14.569831570222606
Experience 15, Iter 25, disc loss: 0.0004153882359340952, policy loss: 14.071600909114931
Experience 15, Iter 26, disc loss: 0.00044241302558618216, policy loss: 14.622728206543908
Experience 15, Iter 27, disc loss: 0.0004525566980360729, policy loss: 13.665136979457284
Experience 15, Iter 28, disc loss: 0.000585331698282462, policy loss: 13.569206093121
Experience 15, Iter 29, disc loss: 0.0005907754628753731, policy loss: 13.813432940067766
Experience 15, Iter 30, disc loss: 0.0004908081592096441, policy loss: 13.321060347352672
Experience 15, Iter 31, disc loss: 0.000563041160302509, policy loss: 13.387535179560368
Experience 15, Iter 32, disc loss: 0.0009972123198692207, policy loss: 12.749293223068157
Experience 15, Iter 33, disc loss: 0.0008358056577355345, policy loss: 14.483609865357751
Experience 15, Iter 34, disc loss: 0.0005282534547947012, policy loss: 12.672375785368201
Experience 15, Iter 35, disc loss: 0.00035124255033008227, policy loss: 13.900696031522177
Experience 15, Iter 36, disc loss: 0.0005643827792689878, policy loss: 11.804995600681128
Experience 15, Iter 37, disc loss: 0.00045197566346088917, policy loss: 13.829263787777053
Experience 15, Iter 38, disc loss: 0.0004611636600257258, policy loss: 13.734473455510138
Experience 15, Iter 39, disc loss: 0.0004095318202337819, policy loss: 14.047923797513064
Experience 15, Iter 40, disc loss: 0.00044387094359131415, policy loss: 14.76089125762267
Experience 15, Iter 41, disc loss: 0.0010726988524097376, policy loss: 12.16659444959701
Experience 15, Iter 42, disc loss: 0.00040566366504214837, policy loss: 14.063018603569516
Experience 15, Iter 43, disc loss: 0.0005681734477468896, policy loss: 12.23979226100493
Experience 15, Iter 44, disc loss: 0.0005603992855892452, policy loss: 13.469746296809603
Experience 15, Iter 45, disc loss: 0.0003696216848859995, policy loss: 15.017827571818035
Experience 15, Iter 46, disc loss: 0.00035520058052308665, policy loss: 14.625884878015118
Experience 15, Iter 47, disc loss: 0.0004475459155420991, policy loss: 15.201244292753078
Experience 15, Iter 48, disc loss: 0.00042696110641558674, policy loss: 13.490187829371797
Experience 15, Iter 49, disc loss: 0.00043323420464149375, policy loss: 13.318436977172224
Experience 15, Iter 50, disc loss: 0.0005776008502255862, policy loss: 11.65305090452623
Experience 15, Iter 51, disc loss: 0.0004746803984183159, policy loss: 13.044955833510976
Experience 15, Iter 52, disc loss: 0.0008615995695949396, policy loss: 13.292453453200663
Experience 15, Iter 53, disc loss: 0.0004144675428830446, policy loss: 14.39084035698459
Experience 15, Iter 54, disc loss: 0.0005201949606058892, policy loss: 12.78677330092496
Experience 15, Iter 55, disc loss: 0.000450083681085445, policy loss: 12.729377049503665
Experience 15, Iter 56, disc loss: 0.0009015657005779017, policy loss: 12.73618417891797
Experience 15, Iter 57, disc loss: 0.000408032716679181, policy loss: 14.39866798890043
Experience 15, Iter 58, disc loss: 0.0004242994468635645, policy loss: 13.419546083549768
Experience 15, Iter 59, disc loss: 0.001967675157310316, policy loss: 13.10498865586057
Experience 15, Iter 60, disc loss: 0.0003952707020774295, policy loss: 13.071438831441858
Experience 15, Iter 61, disc loss: 0.00041289112213228045, policy loss: 14.311213421683226
Experience 15, Iter 62, disc loss: 0.0005745986534070657, policy loss: 12.370315871906165
Experience 15, Iter 63, disc loss: 0.00042914033549069556, policy loss: 14.283342133371821
Experience 15, Iter 64, disc loss: 0.00044474885017023, policy loss: 12.803590404144304
Experience 15, Iter 65, disc loss: 0.0006420512897810583, policy loss: 14.390725677943227
Experience 15, Iter 66, disc loss: 0.0004717991115640861, policy loss: 13.362159219502988
Experience 15, Iter 67, disc loss: 0.0006771280654465283, policy loss: 11.83252148376901
Experience 15, Iter 68, disc loss: 0.00040824863015537943, policy loss: 12.784694574053725
Experience 15, Iter 69, disc loss: 0.0006700857241833139, policy loss: 12.851744386186162
Experience 15, Iter 70, disc loss: 0.0008958713157747347, policy loss: 12.874885115775555
Experience 15, Iter 71, disc loss: 0.00039136545612921287, policy loss: 12.271881459639241
Experience 15, Iter 72, disc loss: 0.0004280593862785121, policy loss: 12.504453095971982
Experience 15, Iter 73, disc loss: 0.0003640619797178592, policy loss: 13.283916426489732
Experience 15, Iter 74, disc loss: 0.0004011323808466075, policy loss: 13.31168884222318
Experience 15, Iter 75, disc loss: 0.00042729645028878827, policy loss: 12.328705341146811
Experience 15, Iter 76, disc loss: 0.00034621967747886054, policy loss: 15.65056733576497
Experience 15, Iter 77, disc loss: 0.00043440020385517193, policy loss: 12.895501381398308
Experience 15, Iter 78, disc loss: 0.00036785044573624015, policy loss: 14.206490684688983
Experience 15, Iter 79, disc loss: 0.00041061566182665405, policy loss: 13.458952197917217
Experience 15, Iter 80, disc loss: 0.00039971069456355707, policy loss: 13.555393197716306
Experience 15, Iter 81, disc loss: 0.0003703860688391051, policy loss: 13.804838597191313
Experience 15, Iter 82, disc loss: 0.00036605875323263983, policy loss: 12.942090403097644
Experience 15, Iter 83, disc loss: 0.00035658793830630224, policy loss: 13.865346933391486
Experience 15, Iter 84, disc loss: 0.0011743259850712478, policy loss: 14.711368382565894
Experience 15, Iter 85, disc loss: 0.0005281250512759352, policy loss: 13.158797260542514
Experience 15, Iter 86, disc loss: 0.0003427581458480132, policy loss: 12.598851817390067
Experience 15, Iter 87, disc loss: 0.0006532246648760415, policy loss: 12.668839263263155
Experience 15, Iter 88, disc loss: 0.0004424713361001661, policy loss: 12.574378928200318
Experience 15, Iter 89, disc loss: 0.0008826089011993103, policy loss: 13.698753111571584
Experience 15, Iter 90, disc loss: 0.0004545641163545158, policy loss: 13.60844925252291
Experience 15, Iter 91, disc loss: 0.0006029069941820792, policy loss: 14.090817102664971
Experience 15, Iter 92, disc loss: 0.00036310741982158255, policy loss: 13.350020287088997
Experience 15, Iter 93, disc loss: 0.0003768197457281677, policy loss: 14.045470234111088
Experience 15, Iter 94, disc loss: 0.00041352686216228546, policy loss: 12.279355379220252
Experience 15, Iter 95, disc loss: 0.0006747808400430664, policy loss: 13.9432872563063
Experience 15, Iter 96, disc loss: 0.00036448421491545326, policy loss: 13.511102882243904
Experience 15, Iter 97, disc loss: 0.00039247593528435613, policy loss: 13.66061697496352
Experience 15, Iter 98, disc loss: 0.0006282917082900782, policy loss: 12.578529670804325
Experience 15, Iter 99, disc loss: 0.0011503843045715509, policy loss: 12.931445050126346
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0043],
        [0.2146],
        [2.2483],
        [0.0215]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0358, 0.2028, 1.0861, 0.0161, 0.0054, 4.7154]],

        [[0.0358, 0.2028, 1.0861, 0.0161, 0.0054, 4.7154]],

        [[0.0358, 0.2028, 1.0861, 0.0161, 0.0054, 4.7154]],

        [[0.0358, 0.2028, 1.0861, 0.0161, 0.0054, 4.7154]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0171, 0.8586, 8.9931, 0.0861], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0171, 0.8586, 8.9931, 0.0861])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.059
Iter 2/2000 - Loss: 3.525
Iter 3/2000 - Loss: 3.366
Iter 4/2000 - Loss: 3.460
Iter 5/2000 - Loss: 3.533
Iter 6/2000 - Loss: 3.473
Iter 7/2000 - Loss: 3.343
Iter 8/2000 - Loss: 3.214
Iter 9/2000 - Loss: 3.114
Iter 10/2000 - Loss: 3.040
Iter 11/2000 - Loss: 2.980
Iter 12/2000 - Loss: 2.919
Iter 13/2000 - Loss: 2.837
Iter 14/2000 - Loss: 2.720
Iter 15/2000 - Loss: 2.569
Iter 16/2000 - Loss: 2.395
Iter 17/2000 - Loss: 2.212
Iter 18/2000 - Loss: 2.028
Iter 19/2000 - Loss: 1.844
Iter 20/2000 - Loss: 1.651
Iter 1981/2000 - Loss: -7.757
Iter 1982/2000 - Loss: -7.757
Iter 1983/2000 - Loss: -7.757
Iter 1984/2000 - Loss: -7.757
Iter 1985/2000 - Loss: -7.757
Iter 1986/2000 - Loss: -7.757
Iter 1987/2000 - Loss: -7.757
Iter 1988/2000 - Loss: -7.757
Iter 1989/2000 - Loss: -7.757
Iter 1990/2000 - Loss: -7.757
Iter 1991/2000 - Loss: -7.757
Iter 1992/2000 - Loss: -7.758
Iter 1993/2000 - Loss: -7.758
Iter 1994/2000 - Loss: -7.758
Iter 1995/2000 - Loss: -7.758
Iter 1996/2000 - Loss: -7.758
Iter 1997/2000 - Loss: -7.758
Iter 1998/2000 - Loss: -7.758
Iter 1999/2000 - Loss: -7.758
Iter 2000/2000 - Loss: -7.758
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[19.6783, 11.1972, 28.0097,  4.4508, 10.0715, 61.5321]],

        [[26.4064, 41.5292,  9.8447,  1.0773,  1.7322, 18.2693]],

        [[27.7145, 43.7547, 10.6197,  0.8783,  1.4005, 21.4978]],

        [[21.0523, 35.2622,  9.8524,  1.6904,  1.9541, 39.3943]]])
Signal Variance: tensor([ 0.1640,  1.4299, 13.8072,  0.2768])
Estimated target variance: tensor([0.0171, 0.8586, 8.9931, 0.0861])
N: 160
Signal to noise ratio: tensor([24.1488, 63.7082, 89.1553, 36.8209])
Bound on condition number: tensor([  93307.0096,  649397.9960, 1271788.3089,  216926.0981])
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 0.0008031553571798571, policy loss: 12.377709895201367
Experience 16, Iter 1, disc loss: 0.00044705874887151766, policy loss: 13.960784683335037
Experience 16, Iter 2, disc loss: 0.00037049951833958126, policy loss: 14.142198886675237
Experience 16, Iter 3, disc loss: 0.0007253158423318573, policy loss: 12.422052815913368
Experience 16, Iter 4, disc loss: 0.00040283468088901094, policy loss: 14.621372223556355
Experience 16, Iter 5, disc loss: 0.0003357382511209442, policy loss: 14.545427713381935
Experience 16, Iter 6, disc loss: 0.0003472734182144551, policy loss: 14.129099849911428
Experience 16, Iter 7, disc loss: 0.00037729679711059796, policy loss: 13.657371801921203
Experience 16, Iter 8, disc loss: 0.0005205512568191703, policy loss: 13.381582347592618
Experience 16, Iter 9, disc loss: 0.0003061381494831091, policy loss: 14.97721260734145
Experience 16, Iter 10, disc loss: 0.0003401591943570384, policy loss: 14.183169881925512
Experience 16, Iter 11, disc loss: 0.00039524379174190856, policy loss: 13.573763811843254
Experience 16, Iter 12, disc loss: 0.0007551222257312068, policy loss: 12.363797173734518
Experience 16, Iter 13, disc loss: 0.00044865472721082063, policy loss: 13.700092169503463
Experience 16, Iter 14, disc loss: 0.0013469867134048832, policy loss: 13.415243604633286
Experience 16, Iter 15, disc loss: 0.00039058705352474444, policy loss: 13.760443477064662
Experience 16, Iter 16, disc loss: 0.00034345727306647343, policy loss: 13.886904589578162
Experience 16, Iter 17, disc loss: 0.0003615171544367702, policy loss: 14.235260372938818
Experience 16, Iter 18, disc loss: 0.00037694127166424177, policy loss: 14.622867818670837
Experience 16, Iter 19, disc loss: 0.00044335604275399837, policy loss: 12.958995018148181
Experience 16, Iter 20, disc loss: 0.0005125637453073337, policy loss: 14.268841403666642
Experience 16, Iter 21, disc loss: 0.0005660682596721931, policy loss: 14.8078721103604
Experience 16, Iter 22, disc loss: 0.00031861906629714607, policy loss: 14.646955020171296
Experience 16, Iter 23, disc loss: 0.0007059941712474348, policy loss: 12.174056822705955
Experience 16, Iter 24, disc loss: 0.00037711862199061295, policy loss: 12.908200989457317
Experience 16, Iter 25, disc loss: 0.00039692583395830314, policy loss: 12.88328046738663
Experience 16, Iter 26, disc loss: 0.0003260313613761745, policy loss: 14.764530754367044
Experience 16, Iter 27, disc loss: 0.0007581556802470264, policy loss: 12.258145798214116
Experience 16, Iter 28, disc loss: 0.0007718683629574124, policy loss: 13.053620905141536
Experience 16, Iter 29, disc loss: 0.00034737369857863645, policy loss: 13.976015191470836
Experience 16, Iter 30, disc loss: 0.0006971357344535835, policy loss: 13.779319471740783
Experience 16, Iter 31, disc loss: 0.00036104242714614387, policy loss: 13.483037307404489
Experience 16, Iter 32, disc loss: 0.0003639420022462104, policy loss: 12.884726864143754
Experience 16, Iter 33, disc loss: 0.0006730152063951131, policy loss: 15.693093243936266
Experience 16, Iter 34, disc loss: 0.00035117961233697807, policy loss: 13.705334879983798
Experience 16, Iter 35, disc loss: 0.0007532675175882617, policy loss: 14.432428091548555
Experience 16, Iter 36, disc loss: 0.00036412872957531137, policy loss: 13.336764338421705
Experience 16, Iter 37, disc loss: 0.0003400490339386064, policy loss: 13.544633865753932
Experience 16, Iter 38, disc loss: 0.00036154959137236874, policy loss: 14.388385400629325
Experience 16, Iter 39, disc loss: 0.00040048474443834555, policy loss: 15.538760313140303
Experience 16, Iter 40, disc loss: 0.000387430189077827, policy loss: 15.397119769479067
Experience 16, Iter 41, disc loss: 0.00043821114654731913, policy loss: 14.959915437445574
Experience 16, Iter 42, disc loss: 0.0007799161694854377, policy loss: 12.876792309119299
Experience 16, Iter 43, disc loss: 0.00037357678824590423, policy loss: 12.90228785122877
Experience 16, Iter 44, disc loss: 0.00040497423896966724, policy loss: 13.357055378356467
Experience 16, Iter 45, disc loss: 0.001038628046789068, policy loss: 12.506591286876226
Experience 16, Iter 46, disc loss: 0.0004735723894544598, policy loss: 13.193843403007847
Experience 16, Iter 47, disc loss: 0.00038134324911090554, policy loss: 15.142612973314177
Experience 16, Iter 48, disc loss: 0.00035351714062461473, policy loss: 13.006948594170096
Experience 16, Iter 49, disc loss: 0.0004664458196109397, policy loss: 14.059408313225806
Experience 16, Iter 50, disc loss: 0.00041445034166562805, policy loss: 12.783470407949821
Experience 16, Iter 51, disc loss: 0.0003512510763962883, policy loss: 15.05868847071604
Experience 16, Iter 52, disc loss: 0.0003614397768054857, policy loss: 12.21778927756753
Experience 16, Iter 53, disc loss: 0.0003360814658367704, policy loss: 14.34343929766923
Experience 16, Iter 54, disc loss: 0.0004040213663099283, policy loss: 14.518158811476928
Experience 16, Iter 55, disc loss: 0.0003866751107882995, policy loss: 14.075189995755991
Experience 16, Iter 56, disc loss: 0.00036107940921034924, policy loss: 13.863047106742876
Experience 16, Iter 57, disc loss: 0.0003489414596806583, policy loss: 14.02244570686218
Experience 16, Iter 58, disc loss: 0.0003527087266556668, policy loss: 13.496407749822826
Experience 16, Iter 59, disc loss: 0.00033414539696568785, policy loss: 14.566214686580492
Experience 16, Iter 60, disc loss: 0.00047647431415614885, policy loss: 14.573342050952487
Experience 16, Iter 61, disc loss: 0.0003567253336716267, policy loss: 13.43641380491039
Experience 16, Iter 62, disc loss: 0.0003468614487648192, policy loss: 13.366470600796633
Experience 16, Iter 63, disc loss: 0.0003580282721484504, policy loss: 14.526346090955109
Experience 16, Iter 64, disc loss: 0.0004454709892154255, policy loss: 13.201956199582291
Experience 16, Iter 65, disc loss: 0.0003379225682307484, policy loss: 13.333208666325701
Experience 16, Iter 66, disc loss: 0.0004462779386297612, policy loss: 14.376212122167994
Experience 16, Iter 67, disc loss: 0.0003514575886779959, policy loss: 12.75105551526602
Experience 16, Iter 68, disc loss: 0.0005361635095480306, policy loss: 13.07447627744402
Experience 16, Iter 69, disc loss: 0.0004873283478994503, policy loss: 13.359006528495716
Experience 16, Iter 70, disc loss: 0.0004454041100182129, policy loss: 14.900893452089457
Experience 16, Iter 71, disc loss: 0.00039976907160418456, policy loss: 13.107661964306843
Experience 16, Iter 72, disc loss: 0.0004346081202006492, policy loss: 15.061707832528121
Experience 16, Iter 73, disc loss: 0.00034322908845168577, policy loss: 13.701448012132051
Experience 16, Iter 74, disc loss: 0.0008247328257954071, policy loss: 13.286791279813263
Experience 16, Iter 75, disc loss: 0.00038956540738812473, policy loss: 13.870583935711068
Experience 16, Iter 76, disc loss: 0.00042680610943017733, policy loss: 14.12653761079866
Experience 16, Iter 77, disc loss: 0.0003689716720199856, policy loss: 13.663575908057958
Experience 16, Iter 78, disc loss: 0.00032067621492345776, policy loss: 13.143194108006783
Experience 16, Iter 79, disc loss: 0.0004069307974507175, policy loss: 12.420097487595744
Experience 16, Iter 80, disc loss: 0.00032923923140488304, policy loss: 13.463322065235477
Experience 16, Iter 81, disc loss: 0.0005960664649157548, policy loss: 14.627865565172318
Experience 16, Iter 82, disc loss: 0.00037453474854072646, policy loss: 13.555638585896906
Experience 16, Iter 83, disc loss: 0.00044706544579741587, policy loss: 13.99153797648573
Experience 16, Iter 84, disc loss: 0.0006023647250305555, policy loss: 14.549272466656856
Experience 16, Iter 85, disc loss: 0.00038846455328670057, policy loss: 14.246893013008224
Experience 16, Iter 86, disc loss: 0.001508517753287655, policy loss: 12.664324388766548
Experience 16, Iter 87, disc loss: 0.0013681398641751518, policy loss: 13.11883795327253
Experience 16, Iter 88, disc loss: 0.00030980551255602705, policy loss: 14.486922452663844
Experience 16, Iter 89, disc loss: 0.0007960016031824078, policy loss: 13.90223937099404
Experience 16, Iter 90, disc loss: 0.0004476452726717372, policy loss: 14.654953195971403
Experience 16, Iter 91, disc loss: 0.00047597504692340495, policy loss: 13.574200300202445
Experience 16, Iter 92, disc loss: 0.000955027777549032, policy loss: 12.969229170779263
Experience 16, Iter 93, disc loss: 0.0013974810430787215, policy loss: 12.850200058293819
Experience 16, Iter 94, disc loss: 0.0003365943163702683, policy loss: 13.91061137500218
Experience 16, Iter 95, disc loss: 0.0005864541998101239, policy loss: 13.537665499675457
Experience 16, Iter 96, disc loss: 0.0004133911219558836, policy loss: 13.769538823355663
Experience 16, Iter 97, disc loss: 0.00035040687028139723, policy loss: 13.927131845083691
Experience 16, Iter 98, disc loss: 0.0006881828313194216, policy loss: 12.797185124033064
Experience 16, Iter 99, disc loss: 0.00031208987160493014, policy loss: 14.097068003772131
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2169],
        [2.2532],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0351, 0.2006, 1.0666, 0.0154, 0.0052, 4.7546]],

        [[0.0351, 0.2006, 1.0666, 0.0154, 0.0052, 4.7546]],

        [[0.0351, 0.2006, 1.0666, 0.0154, 0.0052, 4.7546]],

        [[0.0351, 0.2006, 1.0666, 0.0154, 0.0052, 4.7546]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0168, 0.8676, 9.0129, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0168, 0.8676, 9.0129, 0.0841])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.038
Iter 2/2000 - Loss: 3.519
Iter 3/2000 - Loss: 3.369
Iter 4/2000 - Loss: 3.472
Iter 5/2000 - Loss: 3.545
Iter 6/2000 - Loss: 3.480
Iter 7/2000 - Loss: 3.345
Iter 8/2000 - Loss: 3.217
Iter 9/2000 - Loss: 3.126
Iter 10/2000 - Loss: 3.063
Iter 11/2000 - Loss: 3.011
Iter 12/2000 - Loss: 2.949
Iter 13/2000 - Loss: 2.859
Iter 14/2000 - Loss: 2.736
Iter 15/2000 - Loss: 2.583
Iter 16/2000 - Loss: 2.410
Iter 17/2000 - Loss: 2.229
Iter 18/2000 - Loss: 2.045
Iter 19/2000 - Loss: 1.857
Iter 20/2000 - Loss: 1.658
Iter 1981/2000 - Loss: -7.891
Iter 1982/2000 - Loss: -7.891
Iter 1983/2000 - Loss: -7.891
Iter 1984/2000 - Loss: -7.891
Iter 1985/2000 - Loss: -7.891
Iter 1986/2000 - Loss: -7.891
Iter 1987/2000 - Loss: -7.891
Iter 1988/2000 - Loss: -7.891
Iter 1989/2000 - Loss: -7.891
Iter 1990/2000 - Loss: -7.891
Iter 1991/2000 - Loss: -7.891
Iter 1992/2000 - Loss: -7.891
Iter 1993/2000 - Loss: -7.891
Iter 1994/2000 - Loss: -7.891
Iter 1995/2000 - Loss: -7.892
Iter 1996/2000 - Loss: -7.892
Iter 1997/2000 - Loss: -7.892
Iter 1998/2000 - Loss: -7.892
Iter 1999/2000 - Loss: -7.892
Iter 2000/2000 - Loss: -7.892
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[18.9295, 10.7687, 27.7914,  4.4456,  9.2800, 60.7815]],

        [[25.6173, 41.0369,  9.6762,  1.0634,  1.7592, 18.4329]],

        [[27.0035, 43.2223, 10.4509,  0.8565,  1.4143, 20.2998]],

        [[21.0397, 34.8299,  9.9427,  1.6693,  1.9507, 39.6371]]])
Signal Variance: tensor([ 0.1512,  1.4067, 12.7390,  0.2748])
Estimated target variance: tensor([0.0168, 0.8676, 9.0129, 0.0841])
N: 170
Signal to noise ratio: tensor([23.4894, 64.4469, 85.8526, 37.4885])
Bound on condition number: tensor([  93799.1495,  706079.4816, 1253015.5318,  238917.3714])
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 0.0002748536685920575, policy loss: 13.665130762918018
Experience 17, Iter 1, disc loss: 0.0003014878570379822, policy loss: 13.82903356174078
Experience 17, Iter 2, disc loss: 0.00033959965913811063, policy loss: 14.017468413920275
Experience 17, Iter 3, disc loss: 0.00046511566044545743, policy loss: 13.1514880942738
Experience 17, Iter 4, disc loss: 0.0003624781452173758, policy loss: 13.026488616331106
Experience 17, Iter 5, disc loss: 0.00044286046913474895, policy loss: 14.514606811678679
Experience 17, Iter 6, disc loss: 0.0003475303549651804, policy loss: 12.841550891624312
Experience 17, Iter 7, disc loss: 0.00045832611193600853, policy loss: 14.238703233793295
Experience 17, Iter 8, disc loss: 0.00035952079466777397, policy loss: 12.657861773555759
Experience 17, Iter 9, disc loss: 0.0003605157341571976, policy loss: 14.97619731772302
Experience 17, Iter 10, disc loss: 0.0005675253377481605, policy loss: 14.075575268839621
Experience 17, Iter 11, disc loss: 0.000619002002808804, policy loss: 15.060322780728043
Experience 17, Iter 12, disc loss: 0.0003389012088954134, policy loss: 14.03256057681694
Experience 17, Iter 13, disc loss: 0.00029893273236158275, policy loss: 13.771341371594136
Experience 17, Iter 14, disc loss: 0.0003220715557161462, policy loss: 13.707333477368962
Experience 17, Iter 15, disc loss: 0.0014306643727211626, policy loss: 12.114365172307057
Experience 17, Iter 16, disc loss: 0.0005887656859222939, policy loss: 12.721041104307705
Experience 17, Iter 17, disc loss: 0.00028290874112513854, policy loss: 14.79658151809781
Experience 17, Iter 18, disc loss: 0.0002865804622866333, policy loss: 13.92322628676653
Experience 17, Iter 19, disc loss: 0.00032295418411055506, policy loss: 12.79676272919984
Experience 17, Iter 20, disc loss: 0.00033273797950148323, policy loss: 14.203659229282664
Experience 17, Iter 21, disc loss: 0.001409736211200576, policy loss: 12.718817984123
Experience 17, Iter 22, disc loss: 0.0003883804011780455, policy loss: 13.718632663425542
Experience 17, Iter 23, disc loss: 0.0003466184128415777, policy loss: 13.973711514311757
Experience 17, Iter 24, disc loss: 0.00025114584713952545, policy loss: 14.8090426104851
Experience 17, Iter 25, disc loss: 0.0009200510501827776, policy loss: 13.042618225936627
Experience 17, Iter 26, disc loss: 0.0002841182166017541, policy loss: 12.455203155629881
Experience 17, Iter 27, disc loss: 0.000482344286263508, policy loss: 14.786623438762653
Experience 17, Iter 28, disc loss: 0.0009217778775842462, policy loss: 12.950122399409953
Experience 17, Iter 29, disc loss: 0.0004715469967010508, policy loss: 13.208321723418933
Experience 17, Iter 30, disc loss: 0.0005161462155937769, policy loss: 13.461529484650338
Experience 17, Iter 31, disc loss: 0.00048735168464775105, policy loss: 12.975463700179333
Experience 17, Iter 32, disc loss: 0.0004303239574170043, policy loss: 12.872009268878909
Experience 17, Iter 33, disc loss: 0.0007951919163990809, policy loss: 12.824641003460785
Experience 17, Iter 34, disc loss: 0.0005605976464619481, policy loss: 14.048081304761888
Experience 17, Iter 35, disc loss: 0.0003400408609365386, policy loss: 13.484045657235395
Experience 17, Iter 36, disc loss: 0.00036936446215713424, policy loss: 14.406422875454872
Experience 17, Iter 37, disc loss: 0.00033620024904666614, policy loss: 13.491208583658905
Experience 17, Iter 38, disc loss: 0.0003622885352724625, policy loss: 12.72868542462864
Experience 17, Iter 39, disc loss: 0.0003490732741356021, policy loss: 12.882487556652672
Experience 17, Iter 40, disc loss: 0.0002951114234644775, policy loss: 13.934062816555961
Experience 17, Iter 41, disc loss: 0.0005305532972614456, policy loss: 13.678401485655032
Experience 17, Iter 42, disc loss: 0.0003783687321389636, policy loss: 11.863633502075377
Experience 17, Iter 43, disc loss: 0.0004101901731148142, policy loss: 13.236179585602127
Experience 17, Iter 44, disc loss: 0.00040544060969464516, policy loss: 14.726565340011367
Experience 17, Iter 45, disc loss: 0.0005562003494629263, policy loss: 13.94308397648353
Experience 17, Iter 46, disc loss: 0.0004142380389426653, policy loss: 12.96786262297315
Experience 17, Iter 47, disc loss: 0.0004240607188657722, policy loss: 12.918823694912199
Experience 17, Iter 48, disc loss: 0.00028302340842560137, policy loss: 12.886206597669336
Experience 17, Iter 49, disc loss: 0.00042525533648417227, policy loss: 12.53730812611198
Experience 17, Iter 50, disc loss: 0.0002680453274302148, policy loss: 14.485848090064724
Experience 17, Iter 51, disc loss: 0.0005647651597400726, policy loss: 13.63360913456901
Experience 17, Iter 52, disc loss: 0.0005929856984049334, policy loss: 14.289021104687066
Experience 17, Iter 53, disc loss: 0.0003178188137271254, policy loss: 14.003417412481074
Experience 17, Iter 54, disc loss: 0.0003691146157277682, policy loss: 13.629663680519842
Experience 17, Iter 55, disc loss: 0.000368726938446473, policy loss: 12.68736781083302
Experience 17, Iter 56, disc loss: 0.0004744334719281995, policy loss: 13.338256639510885
Experience 17, Iter 57, disc loss: 0.00030308748459751105, policy loss: 12.67208971681842
Experience 17, Iter 58, disc loss: 0.0002860344214127491, policy loss: 14.711684066602164
Experience 17, Iter 59, disc loss: 0.00037397240497897785, policy loss: 13.481676340847928
Experience 17, Iter 60, disc loss: 0.0003659838833275693, policy loss: 13.355873341080393
Experience 17, Iter 61, disc loss: 0.0002969248988297318, policy loss: 15.674165859652458
Experience 17, Iter 62, disc loss: 0.0005000546968553987, policy loss: 12.448506344428608
Experience 17, Iter 63, disc loss: 0.0008137610698537937, policy loss: 13.104829305794436
Experience 17, Iter 64, disc loss: 0.0014500552547355027, policy loss: 13.642550958848723
Experience 17, Iter 65, disc loss: 0.0004950667816897886, policy loss: 12.1042821922026
Experience 17, Iter 66, disc loss: 0.000707753688505483, policy loss: 11.693972604765131
Experience 17, Iter 67, disc loss: 0.010190595752830746, policy loss: 11.902390414228474
Experience 17, Iter 68, disc loss: 0.014320121642481464, policy loss: 11.705030253045123
Experience 17, Iter 69, disc loss: 0.02652018405857732, policy loss: 12.645130332803728
Experience 17, Iter 70, disc loss: 0.05260842058894291, policy loss: 11.829515048598285
Experience 17, Iter 71, disc loss: 0.12528903585230594, policy loss: 11.799535975432248
Experience 17, Iter 72, disc loss: 0.1665543730881571, policy loss: 10.83553852043254
Experience 17, Iter 73, disc loss: 0.03301429372810073, policy loss: 13.278241491177248
Experience 17, Iter 74, disc loss: 0.13160606785819257, policy loss: 9.995320509924381
Experience 17, Iter 75, disc loss: 0.10096542982510842, policy loss: 10.878105484977922
Experience 17, Iter 76, disc loss: 0.032872016907413974, policy loss: 12.467313978785477
Experience 17, Iter 77, disc loss: 0.03785592542249379, policy loss: 10.352007759009167
Experience 17, Iter 78, disc loss: 0.00859777216267318, policy loss: 13.072070916364526
Experience 17, Iter 79, disc loss: 0.0074843190234077355, policy loss: 13.465934797530156
Experience 17, Iter 80, disc loss: 0.018041123781790017, policy loss: 11.759225489561945
Experience 17, Iter 81, disc loss: 0.015594557856871567, policy loss: 10.688920501084949
Experience 17, Iter 82, disc loss: 0.009315968097215941, policy loss: 11.564745607831355
Experience 17, Iter 83, disc loss: 0.009859461163032032, policy loss: 11.073106235481582
Experience 17, Iter 84, disc loss: 0.008180606947179926, policy loss: 11.103515915074016
Experience 17, Iter 85, disc loss: 0.01149233662743756, policy loss: 11.776533704565306
Experience 17, Iter 86, disc loss: 0.008516549247729918, policy loss: 11.286318308721746
Experience 17, Iter 87, disc loss: 0.009489433264691503, policy loss: 12.110465912868621
Experience 17, Iter 88, disc loss: 0.009612980402008316, policy loss: 10.388613314976524
Experience 17, Iter 89, disc loss: 0.013127078247988803, policy loss: 9.795286199171588
Experience 17, Iter 90, disc loss: 0.01177145086993814, policy loss: 10.02379987858656
Experience 17, Iter 91, disc loss: 0.015222070424051076, policy loss: 8.628555708177567
Experience 17, Iter 92, disc loss: 0.03161771297163656, policy loss: 8.350093352275055
Experience 17, Iter 93, disc loss: 0.029217443858964014, policy loss: 8.535364310599645
Experience 17, Iter 94, disc loss: 0.03241513268940883, policy loss: 7.809925090024992
Experience 17, Iter 95, disc loss: 0.03289750000487182, policy loss: 10.472566883048707
Experience 17, Iter 96, disc loss: 0.043904911107622485, policy loss: 10.234304537123279
Experience 17, Iter 97, disc loss: 0.052056734135581935, policy loss: 11.28447023496888
Experience 17, Iter 98, disc loss: 0.04163639238926013, policy loss: 10.587367722062806
Experience 17, Iter 99, disc loss: 0.0139967315679256, policy loss: 11.953191178403856
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.2253],
        [2.2781],
        [0.0208]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.3980e-02, 2.0759e-01, 1.0630e+00, 1.6323e-02, 4.9857e-03,
          5.0329e+00]],

        [[3.3980e-02, 2.0759e-01, 1.0630e+00, 1.6323e-02, 4.9857e-03,
          5.0329e+00]],

        [[3.3980e-02, 2.0759e-01, 1.0630e+00, 1.6323e-02, 4.9857e-03,
          5.0329e+00]],

        [[3.3980e-02, 2.0759e-01, 1.0630e+00, 1.6323e-02, 4.9857e-03,
          5.0329e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0180, 0.9011, 9.1126, 0.0831], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0180, 0.9011, 9.1126, 0.0831])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.976
Iter 2/2000 - Loss: 3.533
Iter 3/2000 - Loss: 3.427
Iter 4/2000 - Loss: 3.524
Iter 5/2000 - Loss: 3.563
Iter 6/2000 - Loss: 3.475
Iter 7/2000 - Loss: 3.341
Iter 8/2000 - Loss: 3.229
Iter 9/2000 - Loss: 3.156
Iter 10/2000 - Loss: 3.103
Iter 11/2000 - Loss: 3.050
Iter 12/2000 - Loss: 2.975
Iter 13/2000 - Loss: 2.867
Iter 14/2000 - Loss: 2.729
Iter 15/2000 - Loss: 2.571
Iter 16/2000 - Loss: 2.407
Iter 17/2000 - Loss: 2.243
Iter 18/2000 - Loss: 2.076
Iter 19/2000 - Loss: 1.898
Iter 20/2000 - Loss: 1.699
Iter 1981/2000 - Loss: -7.947
Iter 1982/2000 - Loss: -7.947
Iter 1983/2000 - Loss: -7.947
Iter 1984/2000 - Loss: -7.947
Iter 1985/2000 - Loss: -7.947
Iter 1986/2000 - Loss: -7.947
Iter 1987/2000 - Loss: -7.947
Iter 1988/2000 - Loss: -7.947
Iter 1989/2000 - Loss: -7.947
Iter 1990/2000 - Loss: -7.947
Iter 1991/2000 - Loss: -7.947
Iter 1992/2000 - Loss: -7.947
Iter 1993/2000 - Loss: -7.947
Iter 1994/2000 - Loss: -7.947
Iter 1995/2000 - Loss: -7.947
Iter 1996/2000 - Loss: -7.948
Iter 1997/2000 - Loss: -7.948
Iter 1998/2000 - Loss: -7.948
Iter 1999/2000 - Loss: -7.948
Iter 2000/2000 - Loss: -7.948
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[18.7655, 11.7806, 28.7190,  4.9768,  8.8303, 62.6701]],

        [[25.4253, 41.6675,  9.4657,  1.0967,  1.7379, 19.0377]],

        [[26.8186, 44.0505, 10.2970,  0.8540,  1.4058, 20.6114]],

        [[20.7171, 36.1627, 10.9926,  1.7057,  1.9475, 41.4095]]])
Signal Variance: tensor([ 0.1792,  1.3966, 12.6464,  0.3012])
Estimated target variance: tensor([0.0180, 0.9011, 9.1126, 0.0831])
N: 180
Signal to noise ratio: tensor([25.7647, 64.6554, 87.6729, 38.9392])
Bound on condition number: tensor([ 119488.5612,  752458.4815, 1383578.8485,  272928.3529])
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 0.034035717086951946, policy loss: 9.77667126721737
Experience 18, Iter 1, disc loss: 0.022192834274242533, policy loss: 11.589628721581956
Experience 18, Iter 2, disc loss: 0.04844437028560265, policy loss: 11.010378389646721
Experience 18, Iter 3, disc loss: 0.024016901963033778, policy loss: 9.23509666837912
Experience 18, Iter 4, disc loss: 0.019582621813961483, policy loss: 10.988995548523336
Experience 18, Iter 5, disc loss: 0.01603752884702301, policy loss: 9.932991011772103
Experience 18, Iter 6, disc loss: 0.01500698319263634, policy loss: 12.093442599356628
Experience 18, Iter 7, disc loss: 0.016385457896383435, policy loss: 11.905741665683719
Experience 18, Iter 8, disc loss: 0.014806989033792501, policy loss: 11.845017032185552
Experience 18, Iter 9, disc loss: 0.014589073331991869, policy loss: 11.013242744899959
Experience 18, Iter 10, disc loss: 0.012341277158727224, policy loss: 11.173141318250883
Experience 18, Iter 11, disc loss: 0.011646535206216724, policy loss: 11.821082179818932
Experience 18, Iter 12, disc loss: 0.010634654884052425, policy loss: 11.88909196092432
Experience 18, Iter 13, disc loss: 0.00990798311126475, policy loss: 11.197176300777835
Experience 18, Iter 14, disc loss: 0.00895360669921803, policy loss: 14.625087898273623
Experience 18, Iter 15, disc loss: 0.008219760361140565, policy loss: 14.77091368251573
Experience 18, Iter 16, disc loss: 0.00754017881285777, policy loss: 15.165409606630494
Experience 18, Iter 17, disc loss: 0.006800015861194141, policy loss: 14.837731797546642
Experience 18, Iter 18, disc loss: 0.006128100644615856, policy loss: 17.672515209050538
Experience 18, Iter 19, disc loss: 0.005515637041779108, policy loss: 16.223757432096235
Experience 18, Iter 20, disc loss: 0.005031950101520029, policy loss: 15.35520608309997
Experience 18, Iter 21, disc loss: 0.004493323117402788, policy loss: 16.07946275956339
Experience 18, Iter 22, disc loss: 0.004050893673348376, policy loss: 18.266480439387866
Experience 18, Iter 23, disc loss: 0.003633879197165385, policy loss: 17.299594173098527
Experience 18, Iter 24, disc loss: 0.003275857059780994, policy loss: 16.48458178674482
Experience 18, Iter 25, disc loss: 0.0029503089965958263, policy loss: 19.461761042103305
Experience 18, Iter 26, disc loss: 0.00271315002998829, policy loss: 16.854762913475938
Experience 18, Iter 27, disc loss: 0.0024784833207240234, policy loss: 18.037391643459177
Experience 18, Iter 28, disc loss: 0.002255914499774311, policy loss: 18.220358531576515
Experience 18, Iter 29, disc loss: 0.0021227781424036224, policy loss: 16.709964161395618
Experience 18, Iter 30, disc loss: 0.0020057955196124367, policy loss: 17.86929144016278
Experience 18, Iter 31, disc loss: 0.001837569414611408, policy loss: 18.742784628220384
Experience 18, Iter 32, disc loss: 0.019244534138783097, policy loss: 17.726272013270552
Experience 18, Iter 33, disc loss: 0.0016055159267104146, policy loss: 15.62834009971915
Experience 18, Iter 34, disc loss: 0.0015054058266353394, policy loss: 18.3082044236913
Experience 18, Iter 35, disc loss: 0.0015418929101404407, policy loss: 16.837152165432737
Experience 18, Iter 36, disc loss: 0.0014373376987290606, policy loss: 17.602545346831576
Experience 18, Iter 37, disc loss: 0.0013121392592390548, policy loss: 18.250121618571317
Experience 18, Iter 38, disc loss: 0.0012735806670132867, policy loss: 15.882587363590082
Experience 18, Iter 39, disc loss: 0.0012012605964110109, policy loss: 17.977926170744123
Experience 18, Iter 40, disc loss: 0.0011625140687533736, policy loss: 18.96371642807269
Experience 18, Iter 41, disc loss: 0.0023331028989505556, policy loss: 20.180060144591575
Experience 18, Iter 42, disc loss: 0.0010821813943211406, policy loss: 18.20091772790896
Experience 18, Iter 43, disc loss: 0.001021126503954727, policy loss: 17.192224395303697
Experience 18, Iter 44, disc loss: 0.0010250096831815176, policy loss: 16.536045295511066
Experience 18, Iter 45, disc loss: 0.0009974819758549512, policy loss: 15.269367617220267
Experience 18, Iter 46, disc loss: 0.0009542256286578224, policy loss: 17.02714681426896
Experience 18, Iter 47, disc loss: 0.0009209882083025385, policy loss: 17.44347292014428
Experience 18, Iter 48, disc loss: 0.0011251216754306031, policy loss: 14.589524760184744
Experience 18, Iter 49, disc loss: 0.0008915857556567622, policy loss: 16.825255643393795
Experience 18, Iter 50, disc loss: 0.000981869225597396, policy loss: 18.209662879746745
Experience 18, Iter 51, disc loss: 0.0008132253271594789, policy loss: 19.162557561508617
Experience 18, Iter 52, disc loss: 0.0008150393291157731, policy loss: 17.44755187970891
Experience 18, Iter 53, disc loss: 0.0008112063483192842, policy loss: 17.423977494437306
Experience 18, Iter 54, disc loss: 0.0010476946456492349, policy loss: 17.039540670866845
Experience 18, Iter 55, disc loss: 0.0008177724260420139, policy loss: 19.864734705920817
Experience 18, Iter 56, disc loss: 0.0007322982968229599, policy loss: 17.862681756827392
Experience 18, Iter 57, disc loss: 0.0007318132568844769, policy loss: 16.25506677452373
Experience 18, Iter 58, disc loss: 0.0006952985440570475, policy loss: 17.78106299851187
Experience 18, Iter 59, disc loss: 0.0007540361624469052, policy loss: 17.97082948045253
Experience 18, Iter 60, disc loss: 0.0006747621387869964, policy loss: 16.75495733190388
Experience 18, Iter 61, disc loss: 0.0006551698755874036, policy loss: 18.214469692212035
Experience 18, Iter 62, disc loss: 0.0006505693931748327, policy loss: 18.90706343019864
Experience 18, Iter 63, disc loss: 0.0006648707428280021, policy loss: 17.715444952499134
Experience 18, Iter 64, disc loss: 0.0025053448031496475, policy loss: 15.336469245461434
Experience 18, Iter 65, disc loss: 0.0007101447249732882, policy loss: 18.01508467927192
Experience 18, Iter 66, disc loss: 0.0007105195298006399, policy loss: 17.71522106669806
Experience 18, Iter 67, disc loss: 0.0006075773791367365, policy loss: 18.713759931433138
Experience 18, Iter 68, disc loss: 0.0006021598325780584, policy loss: 17.2743207238964
Experience 18, Iter 69, disc loss: 0.0006113748682149769, policy loss: 16.576802010114974
Experience 18, Iter 70, disc loss: 0.0005789247484014769, policy loss: 18.744936535122815
Experience 18, Iter 71, disc loss: 0.0005925236102050028, policy loss: 17.809286101224405
Experience 18, Iter 72, disc loss: 0.0006271413634984153, policy loss: 16.524159546766626
Experience 18, Iter 73, disc loss: 0.0005479024971536969, policy loss: 20.015805195276585
Experience 18, Iter 74, disc loss: 0.000561752319594796, policy loss: 17.270870302901486
Experience 18, Iter 75, disc loss: 0.0005561872209139511, policy loss: 17.004303200610096
Experience 18, Iter 76, disc loss: 0.0006687352465700065, policy loss: 18.514590930052275
Experience 18, Iter 77, disc loss: 0.0006669953580720062, policy loss: 17.18955738760497
Experience 18, Iter 78, disc loss: 0.0005431644882749853, policy loss: 17.76811134698029
Experience 18, Iter 79, disc loss: 0.0006480695919731467, policy loss: 18.11027840037412
Experience 18, Iter 80, disc loss: 0.0025529046477258586, policy loss: 16.10871476036965
Experience 18, Iter 81, disc loss: 0.0005126149088744538, policy loss: 17.285649398667257
Experience 18, Iter 82, disc loss: 0.0005334035303824687, policy loss: 14.99281846417946
Experience 18, Iter 83, disc loss: 0.0005354400768276698, policy loss: 15.074669105447864
Experience 18, Iter 84, disc loss: 0.0005067899747064465, policy loss: 16.336899686734128
Experience 18, Iter 85, disc loss: 0.0005532682318087075, policy loss: 16.887752211331268
Experience 18, Iter 86, disc loss: 0.0005123083470195945, policy loss: 17.075817696856284
Experience 18, Iter 87, disc loss: 0.0007622653550889652, policy loss: 18.485692649280754
Experience 18, Iter 88, disc loss: 0.0005119376552927956, policy loss: 17.20155990776447
Experience 18, Iter 89, disc loss: 0.0004678807589880699, policy loss: 17.17860778650133
Experience 18, Iter 90, disc loss: 0.0005043851166347996, policy loss: 17.88317560935171
Experience 18, Iter 91, disc loss: 0.00047488986506811907, policy loss: 17.754082471850754
Experience 18, Iter 92, disc loss: 0.0005105018767115279, policy loss: 17.68818914657635
Experience 18, Iter 93, disc loss: 0.000523688134609398, policy loss: 18.53896256037031
Experience 18, Iter 94, disc loss: 0.00045185714686370783, policy loss: 17.999870061097003
Experience 18, Iter 95, disc loss: 0.000570149878243632, policy loss: 17.46665437712017
Experience 18, Iter 96, disc loss: 0.00044596218686089215, policy loss: 18.277614559589864
Experience 18, Iter 97, disc loss: 0.00046032710099156457, policy loss: 16.550795052634257
Experience 18, Iter 98, disc loss: 0.0004646781843596579, policy loss: 17.3296135528316
Experience 18, Iter 99, disc loss: 0.000504535019314185, policy loss: 15.909429733976769
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0047],
        [0.2263],
        [2.2975],
        [0.0229]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0329, 0.2101, 1.1541, 0.0181, 0.0062, 5.2174]],

        [[0.0329, 0.2101, 1.1541, 0.0181, 0.0062, 5.2174]],

        [[0.0329, 0.2101, 1.1541, 0.0181, 0.0062, 5.2174]],

        [[0.0329, 0.2101, 1.1541, 0.0181, 0.0062, 5.2174]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0187, 0.9051, 9.1901, 0.0916], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0187, 0.9051, 9.1901, 0.0916])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.977
Iter 2/2000 - Loss: 3.570
Iter 3/2000 - Loss: 3.499
Iter 4/2000 - Loss: 3.584
Iter 5/2000 - Loss: 3.594
Iter 6/2000 - Loss: 3.494
Iter 7/2000 - Loss: 3.366
Iter 8/2000 - Loss: 3.266
Iter 9/2000 - Loss: 3.198
Iter 10/2000 - Loss: 3.146
Iter 11/2000 - Loss: 3.086
Iter 12/2000 - Loss: 2.996
Iter 13/2000 - Loss: 2.871
Iter 14/2000 - Loss: 2.721
Iter 15/2000 - Loss: 2.560
Iter 16/2000 - Loss: 2.401
Iter 17/2000 - Loss: 2.244
Iter 18/2000 - Loss: 2.078
Iter 19/2000 - Loss: 1.892
Iter 20/2000 - Loss: 1.681
Iter 1981/2000 - Loss: -7.882
Iter 1982/2000 - Loss: -7.882
Iter 1983/2000 - Loss: -7.882
Iter 1984/2000 - Loss: -7.882
Iter 1985/2000 - Loss: -7.883
Iter 1986/2000 - Loss: -7.883
Iter 1987/2000 - Loss: -7.883
Iter 1988/2000 - Loss: -7.883
Iter 1989/2000 - Loss: -7.883
Iter 1990/2000 - Loss: -7.883
Iter 1991/2000 - Loss: -7.883
Iter 1992/2000 - Loss: -7.883
Iter 1993/2000 - Loss: -7.883
Iter 1994/2000 - Loss: -7.883
Iter 1995/2000 - Loss: -7.883
Iter 1996/2000 - Loss: -7.883
Iter 1997/2000 - Loss: -7.883
Iter 1998/2000 - Loss: -7.883
Iter 1999/2000 - Loss: -7.883
Iter 2000/2000 - Loss: -7.883
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[17.7502, 11.3909, 23.5821,  5.1513,  6.3219, 62.3787]],

        [[24.8196, 40.9629,  8.9047,  1.1265,  1.8066, 17.8684]],

        [[27.9073, 45.2426,  9.7838,  0.8822,  1.4186, 21.0653]],

        [[20.0426, 35.1523, 11.4202,  1.9580,  2.0637, 41.9702]]])
Signal Variance: tensor([ 0.1512,  1.2076, 12.6738,  0.3786])
Estimated target variance: tensor([0.0187, 0.9051, 9.1901, 0.0916])
N: 190
Signal to noise ratio: tensor([23.8625, 59.3566, 88.8430, 43.8583])
Bound on condition number: tensor([ 108190.5179,  669410.6201, 1499685.7595,  365476.1103])
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 0.0005627096784135811, policy loss: 16.777659105529317
Experience 19, Iter 1, disc loss: 0.0005555300693304511, policy loss: 16.22731372830428
Experience 19, Iter 2, disc loss: 0.0005393375398166696, policy loss: 17.10696289226613
Experience 19, Iter 3, disc loss: 0.0004614136080227088, policy loss: 16.401823486023662
Experience 19, Iter 4, disc loss: 0.003272927559102321, policy loss: 15.126479713758735
Experience 19, Iter 5, disc loss: 0.0005450019652964893, policy loss: 16.2840209795128
Experience 19, Iter 6, disc loss: 0.0004916811489314876, policy loss: 15.406944853875673
Experience 19, Iter 7, disc loss: 0.0004884995753773412, policy loss: 15.224573659791355
Experience 19, Iter 8, disc loss: 0.00045539706809070437, policy loss: 18.36557434660831
Experience 19, Iter 9, disc loss: 0.00042038351312845724, policy loss: 17.373584508367873
Experience 19, Iter 10, disc loss: 0.0004220593406989644, policy loss: 16.87191851764646
Experience 19, Iter 11, disc loss: 0.0005676406825749841, policy loss: 16.78062501852594
Experience 19, Iter 12, disc loss: 0.0005433806102533734, policy loss: 15.69500197832788
Experience 19, Iter 13, disc loss: 0.0004999858933700207, policy loss: 15.412980026837268
Experience 19, Iter 14, disc loss: 0.00046240194635768393, policy loss: 17.302728410168235
Experience 19, Iter 15, disc loss: 0.00042064386725905445, policy loss: 15.896956217033399
Experience 19, Iter 16, disc loss: 0.0004838970250724639, policy loss: 17.108354398503298
Experience 19, Iter 17, disc loss: 0.00040242604298695395, policy loss: 17.188847520443822
Experience 19, Iter 18, disc loss: 0.0004578919869945761, policy loss: 17.473941397339942
Experience 19, Iter 19, disc loss: 0.0003994062403507772, policy loss: 17.808978343759826
Experience 19, Iter 20, disc loss: 0.00042400147333650207, policy loss: 16.819251311992666
Experience 19, Iter 21, disc loss: 0.0004930408838019273, policy loss: 16.792690665634204
Experience 19, Iter 22, disc loss: 0.00037701917287353187, policy loss: 18.01548341701504
Experience 19, Iter 23, disc loss: 0.0007254661155031048, policy loss: 16.270318727161367
Experience 19, Iter 24, disc loss: 0.00041169635841790226, policy loss: 17.284236331556585
Experience 19, Iter 25, disc loss: 0.0005231454672710242, policy loss: 16.224904465073294
Experience 19, Iter 26, disc loss: 0.0015582572424312582, policy loss: 16.669053838338975
Experience 19, Iter 27, disc loss: 0.00041788504249096707, policy loss: 17.119881257517495
Experience 19, Iter 28, disc loss: 0.0004648954110463027, policy loss: 16.12801169411289
Experience 19, Iter 29, disc loss: 0.0004298389749264686, policy loss: 14.656368807540042
Experience 19, Iter 30, disc loss: 0.0004624202341111538, policy loss: 17.04749321763439
Experience 19, Iter 31, disc loss: 0.0005557508396281119, policy loss: 16.032665493008047
Experience 19, Iter 32, disc loss: 0.0005583262590967602, policy loss: 16.411587866497587
Experience 19, Iter 33, disc loss: 0.0005744787890225038, policy loss: 14.905497997703243
Experience 19, Iter 34, disc loss: 0.0018945419019952437, policy loss: 16.334200506844542
Experience 19, Iter 35, disc loss: 0.0006674140971121626, policy loss: 15.922704436815783
Experience 19, Iter 36, disc loss: 0.0005901922243591079, policy loss: 16.42603725345292
Experience 19, Iter 37, disc loss: 0.0004888897046639031, policy loss: 18.008691426108477
Experience 19, Iter 38, disc loss: 0.0004173663893084186, policy loss: 17.02055321324474
Experience 19, Iter 39, disc loss: 0.0005310062975646216, policy loss: 16.17896384376715
Experience 19, Iter 40, disc loss: 0.0003866278458552857, policy loss: 17.011841529865972
Experience 19, Iter 41, disc loss: 0.0004648310623623807, policy loss: 16.140454056852473
Experience 19, Iter 42, disc loss: 0.00034774850022793177, policy loss: 17.16964453521236
Experience 19, Iter 43, disc loss: 0.00040593205394571217, policy loss: 17.666569021629705
Experience 19, Iter 44, disc loss: 0.00040053094894446415, policy loss: 16.35413251462049
Experience 19, Iter 45, disc loss: 0.0003843150008877993, policy loss: 16.439387176833293
Experience 19, Iter 46, disc loss: 0.0003793213655475878, policy loss: 16.970323106268268
Experience 19, Iter 47, disc loss: 0.00039372251777758784, policy loss: 16.624952026441203
Experience 19, Iter 48, disc loss: 0.0003745849897132624, policy loss: 18.176307670994845
Experience 19, Iter 49, disc loss: 0.0004452807798366913, policy loss: 17.108439093028874
Experience 19, Iter 50, disc loss: 0.00037851552728240904, policy loss: 15.748944202996169
Experience 19, Iter 51, disc loss: 0.0005456137769554858, policy loss: 15.48078331360261
Experience 19, Iter 52, disc loss: 0.00046702900547581624, policy loss: 16.215495623911025
Experience 19, Iter 53, disc loss: 0.0004642071370978689, policy loss: 16.379774039665307
Experience 19, Iter 54, disc loss: 0.0004727122942992554, policy loss: 15.740481337550829
Experience 19, Iter 55, disc loss: 0.00039594537840715704, policy loss: 16.737171938664286
Experience 19, Iter 56, disc loss: 0.0004365348166843977, policy loss: 16.592634139572397
Experience 19, Iter 57, disc loss: 0.0003291533747232583, policy loss: 17.257772820904364
Experience 19, Iter 58, disc loss: 0.00038661617341310744, policy loss: 17.771172250035313
Experience 19, Iter 59, disc loss: 0.00032967371853425597, policy loss: 17.442606333095192
Experience 19, Iter 60, disc loss: 0.00043267162882355055, policy loss: 16.26755671840311
Experience 19, Iter 61, disc loss: 0.00036402276226616553, policy loss: 16.317554420248506
Experience 19, Iter 62, disc loss: 0.00036687286767919793, policy loss: 15.56563782866365
Experience 19, Iter 63, disc loss: 0.0004335362482215007, policy loss: 16.04762390788808
Experience 19, Iter 64, disc loss: 0.0004647352364198778, policy loss: 15.455798634762923
Experience 19, Iter 65, disc loss: 0.00042822522201728895, policy loss: 15.417825647572903
Experience 19, Iter 66, disc loss: 0.00043866497459939713, policy loss: 15.325802886107166
Experience 19, Iter 67, disc loss: 0.0004774499899163324, policy loss: 15.08981716336879
Experience 19, Iter 68, disc loss: 0.0004441238843513747, policy loss: 14.561868233466337
Experience 19, Iter 69, disc loss: 0.0005485856286250271, policy loss: 15.691894226595748
Experience 19, Iter 70, disc loss: 0.0003034707425604265, policy loss: 16.971474069825266
Experience 19, Iter 71, disc loss: 0.0003360052714973344, policy loss: 16.02746177384321
Experience 19, Iter 72, disc loss: 0.0005091785366147081, policy loss: 16.983441654485834
Experience 19, Iter 73, disc loss: 0.0005127674468798411, policy loss: 16.567882934799258
Experience 19, Iter 74, disc loss: 0.0003847158281983045, policy loss: 16.993598665308124
Experience 19, Iter 75, disc loss: 0.0005246543254831667, policy loss: 15.02155930222374
Experience 19, Iter 76, disc loss: 0.00038908509801758884, policy loss: 16.10708665753628
Experience 19, Iter 77, disc loss: 0.00031526044883846535, policy loss: 16.88681837486178
Experience 19, Iter 78, disc loss: 0.0003747563491014687, policy loss: 15.563692210197342
Experience 19, Iter 79, disc loss: 0.0005362210938946708, policy loss: 15.717307627473843
Experience 19, Iter 80, disc loss: 0.0004274684472973198, policy loss: 17.06524451596542
Experience 19, Iter 81, disc loss: 0.00032510274776749784, policy loss: 17.593511923410674
Experience 19, Iter 82, disc loss: 0.00048115493713460475, policy loss: 15.561576066698514
Experience 19, Iter 83, disc loss: 0.00045389118777547984, policy loss: 17.027568084108765
Experience 19, Iter 84, disc loss: 0.0003805285405752036, policy loss: 17.102388423071268
Experience 19, Iter 85, disc loss: 0.0004409216911870481, policy loss: 15.822091425433255
Experience 19, Iter 86, disc loss: 0.00033773104844710556, policy loss: 14.764105243264934
Experience 19, Iter 87, disc loss: 0.00046359023288039736, policy loss: 15.671786888640803
Experience 19, Iter 88, disc loss: 0.0004203828370468989, policy loss: 15.050506793349765
Experience 19, Iter 89, disc loss: 0.0003911454408761368, policy loss: 15.391924341521921
Experience 19, Iter 90, disc loss: 0.000620566701358167, policy loss: 16.431638589975208
Experience 19, Iter 91, disc loss: 0.0006014245429402309, policy loss: 14.046571139579585
Experience 19, Iter 92, disc loss: 0.0005410347065015295, policy loss: 14.892451729034544
Experience 19, Iter 93, disc loss: 0.0005465091525875823, policy loss: 14.290996466770316
Experience 19, Iter 94, disc loss: 0.0005479751914825741, policy loss: 15.475089194690167
Experience 19, Iter 95, disc loss: 0.0010622825181344272, policy loss: 13.32978075782092
Experience 19, Iter 96, disc loss: 0.0012747440193046392, policy loss: 13.529790991164573
Experience 19, Iter 97, disc loss: 0.0021209211503076973, policy loss: 12.292840725480907
Experience 19, Iter 98, disc loss: 0.0031976406358512964, policy loss: 13.442153244880215
Experience 19, Iter 99, disc loss: 0.0033004670112645367, policy loss: 12.405224964130623
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.2276],
        [2.3125],
        [0.0243]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0350, 0.2432, 1.1886, 0.0207, 0.0063, 5.3150]],

        [[0.0350, 0.2432, 1.1886, 0.0207, 0.0063, 5.3150]],

        [[0.0350, 0.2432, 1.1886, 0.0207, 0.0063, 5.3150]],

        [[0.0350, 0.2432, 1.1886, 0.0207, 0.0063, 5.3150]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0218, 0.9105, 9.2501, 0.0971], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0218, 0.9105, 9.2501, 0.0971])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.938
Iter 2/2000 - Loss: 3.638
Iter 3/2000 - Loss: 3.596
Iter 4/2000 - Loss: 3.651
Iter 5/2000 - Loss: 3.619
Iter 6/2000 - Loss: 3.505
Iter 7/2000 - Loss: 3.388
Iter 8/2000 - Loss: 3.312
Iter 9/2000 - Loss: 3.266
Iter 10/2000 - Loss: 3.212
Iter 11/2000 - Loss: 3.125
Iter 12/2000 - Loss: 3.004
Iter 13/2000 - Loss: 2.866
Iter 14/2000 - Loss: 2.725
Iter 15/2000 - Loss: 2.586
Iter 16/2000 - Loss: 2.441
Iter 17/2000 - Loss: 2.279
Iter 18/2000 - Loss: 2.093
Iter 19/2000 - Loss: 1.885
Iter 20/2000 - Loss: 1.659
Iter 1981/2000 - Loss: -7.803
Iter 1982/2000 - Loss: -7.803
Iter 1983/2000 - Loss: -7.803
Iter 1984/2000 - Loss: -7.803
Iter 1985/2000 - Loss: -7.803
Iter 1986/2000 - Loss: -7.803
Iter 1987/2000 - Loss: -7.803
Iter 1988/2000 - Loss: -7.803
Iter 1989/2000 - Loss: -7.804
Iter 1990/2000 - Loss: -7.804
Iter 1991/2000 - Loss: -7.804
Iter 1992/2000 - Loss: -7.804
Iter 1993/2000 - Loss: -7.804
Iter 1994/2000 - Loss: -7.804
Iter 1995/2000 - Loss: -7.804
Iter 1996/2000 - Loss: -7.804
Iter 1997/2000 - Loss: -7.804
Iter 1998/2000 - Loss: -7.804
Iter 1999/2000 - Loss: -7.804
Iter 2000/2000 - Loss: -7.804
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[17.4680, 13.4536, 23.8391,  7.0638,  4.7881, 60.2854]],

        [[24.9559, 42.4193,  9.2186,  1.0794,  1.6964, 17.7011]],

        [[27.2003, 44.1161,  9.3951,  0.8965,  1.2898, 20.5045]],

        [[21.3071, 38.6086, 13.5159,  2.1114,  1.6493, 42.9716]]])
Signal Variance: tensor([ 0.2002,  1.1792, 11.8198,  0.4507])
Estimated target variance: tensor([0.0218, 0.9105, 9.2501, 0.0971])
N: 200
Signal to noise ratio: tensor([26.9406, 59.3515, 83.8903, 47.2926])
Bound on condition number: tensor([ 145159.6505,  704521.5561, 1407516.5755,  447318.5065])
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 0.0025377362529410198, policy loss: 13.331713055735758
Experience 20, Iter 1, disc loss: 0.0037364901987691434, policy loss: 13.62212045306909
Experience 20, Iter 2, disc loss: 0.0021726515035951173, policy loss: 14.49420602802681
Experience 20, Iter 3, disc loss: 0.0061941390862573055, policy loss: 13.391885833472656
Experience 20, Iter 4, disc loss: 0.0035456508580779486, policy loss: 12.875189597683567
Experience 20, Iter 5, disc loss: 0.003649270443655059, policy loss: 12.097928758069251
Experience 20, Iter 6, disc loss: 0.0019612192743531567, policy loss: 14.914483014469841
Experience 20, Iter 7, disc loss: 0.0011334603229782038, policy loss: 13.695519587404247
Experience 20, Iter 8, disc loss: 0.0014732027233274089, policy loss: 14.639771511070187
Experience 20, Iter 9, disc loss: 0.0015527736372338116, policy loss: 12.271143089481473
Experience 20, Iter 10, disc loss: 0.0008810643601946795, policy loss: 12.527959541170157
Experience 20, Iter 11, disc loss: 0.000523033280594297, policy loss: 14.316523689390795
Experience 20, Iter 12, disc loss: 0.0006205999046147125, policy loss: 14.43692497002525
Experience 20, Iter 13, disc loss: 0.0004411551764060737, policy loss: 16.138610759452767
Experience 20, Iter 14, disc loss: 0.0005224674932472761, policy loss: 13.752297476175208
Experience 20, Iter 15, disc loss: 0.00043440641632239786, policy loss: 14.105017215776435
Experience 20, Iter 16, disc loss: 0.00045989043860328137, policy loss: 14.388332594070956
Experience 20, Iter 17, disc loss: 0.00044101181263190247, policy loss: 14.273563393531624
Experience 20, Iter 18, disc loss: 0.00047401173750321083, policy loss: 16.206029222763945
Experience 20, Iter 19, disc loss: 0.000548880823491754, policy loss: 14.613520072881983
Experience 20, Iter 20, disc loss: 0.00041838009345820796, policy loss: 13.910688760018052
Experience 20, Iter 21, disc loss: 0.0004331991312549814, policy loss: 14.4332984504914
Experience 20, Iter 22, disc loss: 0.00046234249656564345, policy loss: 14.804659277729057
Experience 20, Iter 23, disc loss: 0.00044682569915097445, policy loss: 14.795557596834833
Experience 20, Iter 24, disc loss: 0.0004133978295377453, policy loss: 16.220940844175782
Experience 20, Iter 25, disc loss: 0.00046274091226033504, policy loss: 15.28349324687019
Experience 20, Iter 26, disc loss: 0.00038083635968178545, policy loss: 15.601084585924312
Experience 20, Iter 27, disc loss: 0.0005399929896523684, policy loss: 14.51406335408251
Experience 20, Iter 28, disc loss: 0.000559353685392226, policy loss: 13.203200233840484
Experience 20, Iter 29, disc loss: 0.00045361175169983826, policy loss: 14.96166210568828
Experience 20, Iter 30, disc loss: 0.0004366163368302015, policy loss: 15.705601391233259
Experience 20, Iter 31, disc loss: 0.00046255047842071477, policy loss: 16.487677637598146
Experience 20, Iter 32, disc loss: 0.0005235600928988721, policy loss: 14.562033298474494
Experience 20, Iter 33, disc loss: 0.0006556141036474115, policy loss: 15.524969246381087
Experience 20, Iter 34, disc loss: 0.0004015445696551801, policy loss: 14.930233229178135
Experience 20, Iter 35, disc loss: 0.0005761596097312864, policy loss: 12.907845946767509
Experience 20, Iter 36, disc loss: 0.0003512133327160762, policy loss: 15.179724486266833
Experience 20, Iter 37, disc loss: 0.0006015527040766618, policy loss: 13.914349589885076
Experience 20, Iter 38, disc loss: 0.0007369812227872293, policy loss: 14.124148157638047
Experience 20, Iter 39, disc loss: 0.0004194340722583352, policy loss: 16.568776771154617
Experience 20, Iter 40, disc loss: 0.0004927533011093842, policy loss: 13.883247941206898
Experience 20, Iter 41, disc loss: 0.0004218831064765714, policy loss: 15.70328028405141
Experience 20, Iter 42, disc loss: 0.0005449726718245068, policy loss: 14.194718570562362
Experience 20, Iter 43, disc loss: 0.0005317190366889351, policy loss: 13.425897414113164
Experience 20, Iter 44, disc loss: 0.0005123439452729267, policy loss: 14.0982380608598
Experience 20, Iter 45, disc loss: 0.00043962590289038596, policy loss: 14.939957369229278
Experience 20, Iter 46, disc loss: 0.0004973232767724158, policy loss: 14.267163233764295
Experience 20, Iter 47, disc loss: 0.0004451432874698613, policy loss: 16.526242035094505
Experience 20, Iter 48, disc loss: 0.0005333937184922541, policy loss: 15.073476903046664
Experience 20, Iter 49, disc loss: 0.0004859766961802315, policy loss: 16.875736162237615
Experience 20, Iter 50, disc loss: 0.0005186174025976034, policy loss: 15.516679382829285
Experience 20, Iter 51, disc loss: 0.0005955460550577616, policy loss: 14.030271330159495
Experience 20, Iter 52, disc loss: 0.00048637095428317057, policy loss: 15.083826306129616
Experience 20, Iter 53, disc loss: 0.0006125966667855591, policy loss: 15.7883480047654
Experience 20, Iter 54, disc loss: 0.0006854113840132497, policy loss: 15.954559659567357
Experience 20, Iter 55, disc loss: 0.0005570012844563296, policy loss: 14.312947820232804
Experience 20, Iter 56, disc loss: 0.0004841861128482037, policy loss: 15.924592317144302
Experience 20, Iter 57, disc loss: 0.0005574397003672254, policy loss: 15.416868880444634
Experience 20, Iter 58, disc loss: 0.0007171872924260986, policy loss: 14.820697377215936
Experience 20, Iter 59, disc loss: 0.0006092973269350294, policy loss: 13.460730556981394
Experience 20, Iter 60, disc loss: 0.0006821722497081814, policy loss: 14.0025299259685
Experience 20, Iter 61, disc loss: 0.0006110980831975131, policy loss: 14.36957329523101
Experience 20, Iter 62, disc loss: 0.0006202527771716238, policy loss: 15.693938710773761
Experience 20, Iter 63, disc loss: 0.0007243563905648769, policy loss: 13.764881290155333
Experience 20, Iter 64, disc loss: 0.0008016081192576594, policy loss: 13.839900003945328
Experience 20, Iter 65, disc loss: 0.0007524857964181615, policy loss: 15.530730997929986
Experience 20, Iter 66, disc loss: 0.0008720279086434977, policy loss: 12.284145218730144
Experience 20, Iter 67, disc loss: 0.000823072848980196, policy loss: 12.473405751164425
Experience 20, Iter 68, disc loss: 0.0005463261879719696, policy loss: 13.464442456614227
Experience 20, Iter 69, disc loss: 0.000843835952184471, policy loss: 14.028757759866487
Experience 20, Iter 70, disc loss: 0.0006125758402723851, policy loss: 14.258359859809826
Experience 20, Iter 71, disc loss: 0.0004969899408824012, policy loss: 16.9776424839061
Experience 20, Iter 72, disc loss: 0.0009417948941186896, policy loss: 12.438875312014119
Experience 20, Iter 73, disc loss: 0.0009251351819512879, policy loss: 13.350068110356524
Experience 20, Iter 74, disc loss: 0.0006943997077196048, policy loss: 13.857923227024887
Experience 20, Iter 75, disc loss: 0.0008956679273201066, policy loss: 14.08089945205867
Experience 20, Iter 76, disc loss: 0.0007834778954159981, policy loss: 14.647320401044073
Experience 20, Iter 77, disc loss: 0.001016740577626834, policy loss: 13.172671935081222
Experience 20, Iter 78, disc loss: 0.0007281033501811307, policy loss: 13.216343131671842
Experience 20, Iter 79, disc loss: 0.0006102860435573652, policy loss: 13.875329121022052
Experience 20, Iter 80, disc loss: 0.000950865040858792, policy loss: 13.338753872600961
Experience 20, Iter 81, disc loss: 0.0010002909162345982, policy loss: 14.227492260325228
Experience 20, Iter 82, disc loss: 0.0009246961572554418, policy loss: 13.541193547435547
Experience 20, Iter 83, disc loss: 0.0009976773205415549, policy loss: 13.881747021629504
Experience 20, Iter 84, disc loss: 0.0009216423713482357, policy loss: 14.780196744087782
Experience 20, Iter 85, disc loss: 0.0006951076605659768, policy loss: 16.651504328544686
Experience 20, Iter 86, disc loss: 0.0005480886661429989, policy loss: 14.459301529003458
Experience 20, Iter 87, disc loss: 0.0008123197374580669, policy loss: 13.075564208520044
Experience 20, Iter 88, disc loss: 0.0005627325605516444, policy loss: 14.918423086262695
Experience 20, Iter 89, disc loss: 0.0010233471027644942, policy loss: 13.678998793419908
Experience 20, Iter 90, disc loss: 0.001071352848182806, policy loss: 13.846286332287416
Experience 20, Iter 91, disc loss: 0.0010552473087863399, policy loss: 14.962396090611605
Experience 20, Iter 92, disc loss: 0.0007011336380813871, policy loss: 15.623510142252574
Experience 20, Iter 93, disc loss: 0.0012767642426745197, policy loss: 11.801041050654808
Experience 20, Iter 94, disc loss: 0.0010875457387024305, policy loss: 12.915628820350953
Experience 20, Iter 95, disc loss: 0.000722411908567826, policy loss: 14.417088233457468
Experience 20, Iter 96, disc loss: 0.0010564087413940527, policy loss: 13.453088217794111
Experience 20, Iter 97, disc loss: 0.0008685757742936092, policy loss: 14.143581689284582
Experience 20, Iter 98, disc loss: 0.0014804464842394778, policy loss: 13.350628947987628
Experience 20, Iter 99, disc loss: 0.0006099889941938256, policy loss: 15.07799133422595
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.2313],
        [2.3181],
        [0.0248]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0338, 0.2367, 1.2163, 0.0217, 0.0062, 5.4872]],

        [[0.0338, 0.2367, 1.2163, 0.0217, 0.0062, 5.4872]],

        [[0.0338, 0.2367, 1.2163, 0.0217, 0.0062, 5.4872]],

        [[0.0338, 0.2367, 1.2163, 0.0217, 0.0062, 5.4872]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0213, 0.9251, 9.2722, 0.0992], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0213, 0.9251, 9.2722, 0.0992])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.919
Iter 2/2000 - Loss: 3.619
Iter 3/2000 - Loss: 3.589
Iter 4/2000 - Loss: 3.634
Iter 5/2000 - Loss: 3.591
Iter 6/2000 - Loss: 3.474
Iter 7/2000 - Loss: 3.363
Iter 8/2000 - Loss: 3.293
Iter 9/2000 - Loss: 3.248
Iter 10/2000 - Loss: 3.189
Iter 11/2000 - Loss: 3.097
Iter 12/2000 - Loss: 2.972
Iter 13/2000 - Loss: 2.834
Iter 14/2000 - Loss: 2.695
Iter 15/2000 - Loss: 2.559
Iter 16/2000 - Loss: 2.415
Iter 17/2000 - Loss: 2.251
Iter 18/2000 - Loss: 2.062
Iter 19/2000 - Loss: 1.852
Iter 20/2000 - Loss: 1.625
Iter 1981/2000 - Loss: -7.870
Iter 1982/2000 - Loss: -7.870
Iter 1983/2000 - Loss: -7.871
Iter 1984/2000 - Loss: -7.871
Iter 1985/2000 - Loss: -7.871
Iter 1986/2000 - Loss: -7.871
Iter 1987/2000 - Loss: -7.871
Iter 1988/2000 - Loss: -7.871
Iter 1989/2000 - Loss: -7.871
Iter 1990/2000 - Loss: -7.871
Iter 1991/2000 - Loss: -7.871
Iter 1992/2000 - Loss: -7.871
Iter 1993/2000 - Loss: -7.871
Iter 1994/2000 - Loss: -7.871
Iter 1995/2000 - Loss: -7.871
Iter 1996/2000 - Loss: -7.871
Iter 1997/2000 - Loss: -7.871
Iter 1998/2000 - Loss: -7.871
Iter 1999/2000 - Loss: -7.871
Iter 2000/2000 - Loss: -7.871
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[17.3209, 13.2252, 24.6068,  6.7626,  4.9552, 60.0179]],

        [[24.6967, 42.3407,  9.2356,  1.0987,  1.6556, 18.4243]],

        [[26.9911, 44.0055,  9.3690,  0.8756,  1.2893, 20.4648]],

        [[21.1108, 38.1973, 14.0730,  2.0567,  1.6612, 42.5545]]])
Signal Variance: tensor([ 0.1941,  1.2645, 11.3743,  0.4669])
Estimated target variance: tensor([0.0213, 0.9251, 9.2722, 0.0992])
N: 210
Signal to noise ratio: tensor([26.7628, 61.5062, 84.3741, 46.7366])
Bound on condition number: tensor([ 150413.5176,  794434.0859, 1494988.5825,  458706.7501])
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 0.0009064958886780485, policy loss: 12.282661718596257
Experience 21, Iter 1, disc loss: 0.000648765028081176, policy loss: 15.177083406327148
Experience 21, Iter 2, disc loss: 0.0008379361162872565, policy loss: 14.475259005399321
Experience 21, Iter 3, disc loss: 0.0009366537047354515, policy loss: 14.000169931151792
Experience 21, Iter 4, disc loss: 0.0011226611391893803, policy loss: 13.399429603459954
Experience 21, Iter 5, disc loss: 0.000874299538108602, policy loss: 14.964699361543055
Experience 21, Iter 6, disc loss: 0.000715409149478814, policy loss: 13.997350129345968
Experience 21, Iter 7, disc loss: 0.0008641069321751816, policy loss: 13.507089776601001
Experience 21, Iter 8, disc loss: 0.0011862691407666893, policy loss: 15.142462199541585
Experience 21, Iter 9, disc loss: 0.0008042632677405069, policy loss: 15.518737301623243
Experience 21, Iter 10, disc loss: 0.0006594452703953828, policy loss: 14.583405687710906
Experience 21, Iter 11, disc loss: 0.0009418805005445682, policy loss: 13.782633942683335
Experience 21, Iter 12, disc loss: 0.0009363930228414569, policy loss: 16.14137149043397
Experience 21, Iter 13, disc loss: 0.0010877281381581705, policy loss: 16.040662248035723
Experience 21, Iter 14, disc loss: 0.0011651430729936044, policy loss: 14.121956877377226
Experience 21, Iter 15, disc loss: 0.0008839470737881971, policy loss: 14.718611489281209
Experience 21, Iter 16, disc loss: 0.0012990555313088126, policy loss: 14.40276219479358
Experience 21, Iter 17, disc loss: 0.0010799361497397446, policy loss: 14.174479527422388
Experience 21, Iter 18, disc loss: 0.0010465896316499198, policy loss: 14.932486126084695
Experience 21, Iter 19, disc loss: 0.0009243113461445793, policy loss: 14.79769583728803
Experience 21, Iter 20, disc loss: 0.0009900145302858773, policy loss: 14.146290788798753
Experience 21, Iter 21, disc loss: 0.0010270518238077431, policy loss: 14.610531062243119
Experience 21, Iter 22, disc loss: 0.0012754991556378867, policy loss: 13.411705414918174
Experience 21, Iter 23, disc loss: 0.000993253505260042, policy loss: 15.303434204565962
Experience 21, Iter 24, disc loss: 0.0009444237396606855, policy loss: 12.760102486988552
Experience 21, Iter 25, disc loss: 0.0011643458150333765, policy loss: 14.03213783281268
Experience 21, Iter 26, disc loss: 0.0010695823876233695, policy loss: 12.593912878250613
Experience 21, Iter 27, disc loss: 0.0010485688618406092, policy loss: 14.443935333689508
Experience 21, Iter 28, disc loss: 0.0012211174497429875, policy loss: 12.430919103275187
Experience 21, Iter 29, disc loss: 0.0009581683103224311, policy loss: 16.648326283395765
Experience 21, Iter 30, disc loss: 0.0009074093949351417, policy loss: 13.80278934023518
Experience 21, Iter 31, disc loss: 0.0013311099665785312, policy loss: 14.99259936222288
Experience 21, Iter 32, disc loss: 0.0010622170744561531, policy loss: 13.529164986366354
Experience 21, Iter 33, disc loss: 0.0011415067456917068, policy loss: 13.903635482463658
Experience 21, Iter 34, disc loss: 0.0013113981508961533, policy loss: 13.320579182735248
Experience 21, Iter 35, disc loss: 0.0012269762656124303, policy loss: 12.702382684734328
Experience 21, Iter 36, disc loss: 0.0013371267793809635, policy loss: 14.213373524008556
Experience 21, Iter 37, disc loss: 0.00151154566948653, policy loss: 15.495998833529656
Experience 21, Iter 38, disc loss: 0.0013365817878870487, policy loss: 15.047490816699238
Experience 21, Iter 39, disc loss: 0.0014067828659225273, policy loss: 14.110798934150086
Experience 21, Iter 40, disc loss: 0.0014917093323720547, policy loss: 12.960929948745314
Experience 21, Iter 41, disc loss: 0.0013103273899235995, policy loss: 14.278242261864065
Experience 21, Iter 42, disc loss: 0.0014149933910228888, policy loss: 14.156347675807613
Experience 21, Iter 43, disc loss: 0.0011102848299391179, policy loss: 15.102822964364005
Experience 21, Iter 44, disc loss: 0.002071150409206536, policy loss: 13.184140089655168
Experience 21, Iter 45, disc loss: 0.0014174893524914228, policy loss: 14.96055361520196
Experience 21, Iter 46, disc loss: 0.002017378560205887, policy loss: 12.655849845214494
Experience 21, Iter 47, disc loss: 0.0020425822245549612, policy loss: 13.851803627487742
Experience 21, Iter 48, disc loss: 0.0020015490914558074, policy loss: 14.706875013159081
Experience 21, Iter 49, disc loss: 0.0028094250790479435, policy loss: 13.641683787755316
Experience 21, Iter 50, disc loss: 0.002633152724053808, policy loss: 13.619369849687185
Experience 21, Iter 51, disc loss: 0.0027624510161434667, policy loss: 13.664228867084226
Experience 21, Iter 52, disc loss: 0.00338998135807233, policy loss: 14.282154398436582
Experience 21, Iter 53, disc loss: 0.004184570147863071, policy loss: 13.449227908483637
Experience 21, Iter 54, disc loss: 0.0029404489641791424, policy loss: 14.36094151954348
Experience 21, Iter 55, disc loss: 0.0027396891342830218, policy loss: 14.324120757216082
Experience 21, Iter 56, disc loss: 0.0026126679468840798, policy loss: 13.438568096840008
Experience 21, Iter 57, disc loss: 0.004478866250869028, policy loss: 14.067520209465851
Experience 21, Iter 58, disc loss: 0.002616845139076562, policy loss: 15.033325524757036
Experience 21, Iter 59, disc loss: 0.0035982084626201083, policy loss: 13.206083979806419
Experience 21, Iter 60, disc loss: 0.003131438297718102, policy loss: 12.939392527008682
Experience 21, Iter 61, disc loss: 0.003578328574137974, policy loss: 14.793829814420544
Experience 21, Iter 62, disc loss: 0.006033658576248716, policy loss: 14.622887797934979
Experience 21, Iter 63, disc loss: 0.002197301269983751, policy loss: 15.757801353563067
Experience 21, Iter 64, disc loss: 0.00912027414751021, policy loss: 13.590063566919982
Experience 21, Iter 65, disc loss: 0.007266790506899032, policy loss: 15.22353996947437
Experience 21, Iter 66, disc loss: 0.0042570114047415335, policy loss: 12.456979294261828
Experience 21, Iter 67, disc loss: 0.005777449464383856, policy loss: 14.209148515041566
Experience 21, Iter 68, disc loss: 0.008074275491824308, policy loss: 12.829242065077151
Experience 21, Iter 69, disc loss: 0.005954240584943639, policy loss: 13.373031130514471
Experience 21, Iter 70, disc loss: 0.006031460925363008, policy loss: 13.977142180383373
Experience 21, Iter 71, disc loss: 0.008575261895067838, policy loss: 13.480030248421285
Experience 21, Iter 72, disc loss: 0.006102161416319258, policy loss: 13.619685939803556
Experience 21, Iter 73, disc loss: 0.0059955734053170895, policy loss: 17.191216622915142
Experience 21, Iter 74, disc loss: 0.00835685427295277, policy loss: 12.935642198666894
Experience 21, Iter 75, disc loss: 0.004863242848714896, policy loss: 15.265969479156704
Experience 21, Iter 76, disc loss: 0.004062630452245235, policy loss: 17.115961578169053
Experience 21, Iter 77, disc loss: 0.006103852641505806, policy loss: 14.147918137515445
Experience 21, Iter 78, disc loss: 0.0070604816667697605, policy loss: 13.239713672305546
Experience 21, Iter 79, disc loss: 0.005598734897325562, policy loss: 13.36974869525091
Experience 21, Iter 80, disc loss: 0.005471435336251223, policy loss: 12.936543121668569
Experience 21, Iter 81, disc loss: 0.0032756269113836415, policy loss: 15.274951727692502
Experience 21, Iter 82, disc loss: 0.007436848674738326, policy loss: 11.241695557273966
Experience 21, Iter 83, disc loss: 0.005015573928701513, policy loss: 14.482348298906334
Experience 21, Iter 84, disc loss: 0.004289025517509626, policy loss: 16.23933454968882
Experience 21, Iter 85, disc loss: 0.010978132840316186, policy loss: 14.723328625605
Experience 21, Iter 86, disc loss: 0.0064042055622258245, policy loss: 16.753617527766913
Experience 21, Iter 87, disc loss: 0.003689915741917939, policy loss: 14.234760085589297
Experience 21, Iter 88, disc loss: 0.0033222008565879195, policy loss: 14.293115507594514
Experience 21, Iter 89, disc loss: 0.0032561901231084826, policy loss: 13.702021767926016
Experience 21, Iter 90, disc loss: 0.0030382548768553906, policy loss: 15.851304082510108
Experience 21, Iter 91, disc loss: 0.002884232288078121, policy loss: 15.335154360305859
Experience 21, Iter 92, disc loss: 0.0025880849881371568, policy loss: 14.231473156293767
Experience 21, Iter 93, disc loss: 0.002320902224090875, policy loss: 14.155709354483509
Experience 21, Iter 94, disc loss: 0.0023194773871200443, policy loss: 14.93956964302234
Experience 21, Iter 95, disc loss: 0.002390073099702648, policy loss: 14.631184977508113
Experience 21, Iter 96, disc loss: 0.002371805991729964, policy loss: 14.936252764545234
Experience 21, Iter 97, disc loss: 0.0026481351142398383, policy loss: 15.323969952167944
Experience 21, Iter 98, disc loss: 0.0025817902437956218, policy loss: 16.99271963178709
Experience 21, Iter 99, disc loss: 0.002596816691125042, policy loss: 16.336153152968542
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2336],
        [2.3143],
        [0.0252]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0326, 0.2310, 1.2382, 0.0223, 0.0062, 5.6249]],

        [[0.0326, 0.2310, 1.2382, 0.0223, 0.0062, 5.6249]],

        [[0.0326, 0.2310, 1.2382, 0.0223, 0.0062, 5.6249]],

        [[0.0326, 0.2310, 1.2382, 0.0223, 0.0062, 5.6249]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0209, 0.9343, 9.2574, 0.1009], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0209, 0.9343, 9.2574, 0.1009])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.915
Iter 2/2000 - Loss: 3.617
Iter 3/2000 - Loss: 3.595
Iter 4/2000 - Loss: 3.634
Iter 5/2000 - Loss: 3.583
Iter 6/2000 - Loss: 3.464
Iter 7/2000 - Loss: 3.355
Iter 8/2000 - Loss: 3.289
Iter 9/2000 - Loss: 3.243
Iter 10/2000 - Loss: 3.180
Iter 11/2000 - Loss: 3.081
Iter 12/2000 - Loss: 2.953
Iter 13/2000 - Loss: 2.814
Iter 14/2000 - Loss: 2.678
Iter 15/2000 - Loss: 2.543
Iter 16/2000 - Loss: 2.398
Iter 17/2000 - Loss: 2.231
Iter 18/2000 - Loss: 2.038
Iter 19/2000 - Loss: 1.825
Iter 20/2000 - Loss: 1.598
Iter 1981/2000 - Loss: -7.914
Iter 1982/2000 - Loss: -7.914
Iter 1983/2000 - Loss: -7.914
Iter 1984/2000 - Loss: -7.914
Iter 1985/2000 - Loss: -7.914
Iter 1986/2000 - Loss: -7.914
Iter 1987/2000 - Loss: -7.914
Iter 1988/2000 - Loss: -7.914
Iter 1989/2000 - Loss: -7.914
Iter 1990/2000 - Loss: -7.914
Iter 1991/2000 - Loss: -7.914
Iter 1992/2000 - Loss: -7.914
Iter 1993/2000 - Loss: -7.914
Iter 1994/2000 - Loss: -7.914
Iter 1995/2000 - Loss: -7.914
Iter 1996/2000 - Loss: -7.915
Iter 1997/2000 - Loss: -7.915
Iter 1998/2000 - Loss: -7.915
Iter 1999/2000 - Loss: -7.915
Iter 2000/2000 - Loss: -7.915
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[17.0582, 13.1012, 24.2570,  6.5403,  4.5222, 60.6080]],

        [[24.2955, 41.6293,  9.0456,  1.1184,  1.6470, 16.8084]],

        [[26.7460, 43.1387,  9.3930,  0.8500,  1.2594, 18.3479]],

        [[20.8321, 37.6840, 13.2060,  1.9306,  1.6295, 42.8708]]])
Signal Variance: tensor([ 0.1889,  1.1363, 10.1216,  0.4219])
Estimated target variance: tensor([0.0209, 0.9343, 9.2574, 0.1009])
N: 220
Signal to noise ratio: tensor([26.5403, 59.2101, 79.7372, 44.3054])
Bound on condition number: tensor([ 154966.3056,  771283.8198, 1398765.4475,  431855.0367])
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 0.0023100827865798514, policy loss: 16.095576363342236
Experience 22, Iter 1, disc loss: 0.004397915112787133, policy loss: 12.955287998025069
Experience 22, Iter 2, disc loss: 0.0032035368618310955, policy loss: 14.94024759659289
Experience 22, Iter 3, disc loss: 0.003011887563520791, policy loss: 13.983823823422123
Experience 22, Iter 4, disc loss: 0.002384903101385247, policy loss: 15.127605300794146
Experience 22, Iter 5, disc loss: 0.00405908817973913, policy loss: 15.532046986734542
Experience 22, Iter 6, disc loss: 0.0030182653449922743, policy loss: 16.367653042756857
Experience 22, Iter 7, disc loss: 0.006702557279642462, policy loss: 13.33972795906205
Experience 22, Iter 8, disc loss: 0.006810636064009503, policy loss: 16.3362387964905
Experience 22, Iter 9, disc loss: 0.014400728807622434, policy loss: 15.403050722846633
Experience 22, Iter 10, disc loss: 0.00644443803725986, policy loss: 14.293745005257138
Experience 22, Iter 11, disc loss: 0.0040160208070444126, policy loss: 16.516259139832485
Experience 22, Iter 12, disc loss: 0.02522190085868842, policy loss: 15.496661031671595
Experience 22, Iter 13, disc loss: 0.006375459747696776, policy loss: 13.287476828591867
Experience 22, Iter 14, disc loss: 0.003388793518871938, policy loss: 13.96246427262404
Experience 22, Iter 15, disc loss: 0.006189659585832358, policy loss: 15.504335188813943
Experience 22, Iter 16, disc loss: 0.003222102708247453, policy loss: 14.506912630741835
Experience 22, Iter 17, disc loss: 0.004496607421370124, policy loss: 13.864352612418436
Experience 22, Iter 18, disc loss: 0.0057917082239991435, policy loss: 14.380114084003353
Experience 22, Iter 19, disc loss: 0.00445898268874508, policy loss: 14.728106029858969
Experience 22, Iter 20, disc loss: 0.0032754520525277294, policy loss: 15.07464120603088
Experience 22, Iter 21, disc loss: 0.003331280814039175, policy loss: 17.101928880817333
Experience 22, Iter 22, disc loss: 0.0051307400357519, policy loss: 15.29835585240145
Experience 22, Iter 23, disc loss: 0.0051287691586410205, policy loss: 15.07386650538819
Experience 22, Iter 24, disc loss: 0.004949822648435938, policy loss: 17.890173743491815
Experience 22, Iter 25, disc loss: 0.006069116607492214, policy loss: 17.430018693791634
Experience 22, Iter 26, disc loss: 0.011845191397768788, policy loss: 16.63124322290414
Experience 22, Iter 27, disc loss: 0.005829214047975413, policy loss: 15.166797685590819
Experience 22, Iter 28, disc loss: 0.01314045626230451, policy loss: 15.746363640713373
Experience 22, Iter 29, disc loss: 0.006010489528552043, policy loss: 16.772990568190586
Experience 22, Iter 30, disc loss: 0.005436436002855474, policy loss: 18.095959616985464
Experience 22, Iter 31, disc loss: 0.007982112275960171, policy loss: 17.389221784857156
Experience 22, Iter 32, disc loss: 0.01592666460576324, policy loss: 13.57276313478034
Experience 22, Iter 33, disc loss: 0.012922941809347796, policy loss: 15.474203526928656
Experience 22, Iter 34, disc loss: 0.01116897700494879, policy loss: 14.659198983192946
Experience 22, Iter 35, disc loss: 0.010727924603946178, policy loss: 13.276147292251844
Experience 22, Iter 36, disc loss: 0.016515722805947108, policy loss: 17.759819674939433
Experience 22, Iter 37, disc loss: 0.007138464216243897, policy loss: 14.793876319888438
Experience 22, Iter 38, disc loss: 0.015779097762842278, policy loss: 16.451980439386467
Experience 22, Iter 39, disc loss: 0.0123094100862468, policy loss: 15.122701257712
Experience 22, Iter 40, disc loss: 0.01895432012978677, policy loss: 17.52115710447348
Experience 22, Iter 41, disc loss: 0.007567546799231859, policy loss: 18.759843348666063
Experience 22, Iter 42, disc loss: 0.008904785155416935, policy loss: 16.068677785121828
Experience 22, Iter 43, disc loss: 0.01589111098624029, policy loss: 16.155259927083804
Experience 22, Iter 44, disc loss: 0.010600131052024137, policy loss: 17.499273277724207
Experience 22, Iter 45, disc loss: 0.009715972605563715, policy loss: 16.221479839987893
Experience 22, Iter 46, disc loss: 0.009451514015949646, policy loss: 15.525590920346728
Experience 22, Iter 47, disc loss: 0.0073917150749912355, policy loss: 17.2218838616135
Experience 22, Iter 48, disc loss: 0.0071677399736175, policy loss: 17.75967255498102
Experience 22, Iter 49, disc loss: 0.007733700722892114, policy loss: 17.776185988673248
Experience 22, Iter 50, disc loss: 0.009502664097469075, policy loss: 15.688665865665536
Experience 22, Iter 51, disc loss: 0.009295253624860212, policy loss: 16.80960778874644
Experience 22, Iter 52, disc loss: 0.016300718635201064, policy loss: 14.116136331641078
Experience 22, Iter 53, disc loss: 0.005148179232332086, policy loss: 16.893965342731434
Experience 22, Iter 54, disc loss: 0.007769651511792059, policy loss: 16.30020839725005
Experience 22, Iter 55, disc loss: 0.013219745953085489, policy loss: 16.179846149145888
Experience 22, Iter 56, disc loss: 0.006266699302304557, policy loss: 17.264131389228147
Experience 22, Iter 57, disc loss: 0.006862357478334311, policy loss: 17.75064894599262
Experience 22, Iter 58, disc loss: 0.005201907868916859, policy loss: 15.505517439247333
Experience 22, Iter 59, disc loss: 0.004168562918805034, policy loss: 16.397328404907164
Experience 22, Iter 60, disc loss: 0.00593072473089586, policy loss: 17.40974210712706
Experience 22, Iter 61, disc loss: 0.008184043286942135, policy loss: 16.422525157650984
Experience 22, Iter 62, disc loss: 0.009885878573043558, policy loss: 16.52379661653018
Experience 22, Iter 63, disc loss: 0.007970910231131627, policy loss: 16.678397751686486
Experience 22, Iter 64, disc loss: 0.010184066756471998, policy loss: 15.376975647788587
Experience 22, Iter 65, disc loss: 0.004937530546760459, policy loss: 15.519355741439622
Experience 22, Iter 66, disc loss: 0.008468320547226703, policy loss: 17.473470602922397
Experience 22, Iter 67, disc loss: 0.005636065179151813, policy loss: 16.97503479008008
Experience 22, Iter 68, disc loss: 0.008530838797283932, policy loss: 17.126454547068256
Experience 22, Iter 69, disc loss: 0.01293863855351468, policy loss: 16.471826443345684
Experience 22, Iter 70, disc loss: 0.00368714754179969, policy loss: 17.489163176574884
Experience 22, Iter 71, disc loss: 0.007113392947505051, policy loss: 16.854526134263452
Experience 22, Iter 72, disc loss: 0.006820695575502345, policy loss: 17.079977868872525
Experience 22, Iter 73, disc loss: 0.007097730310294165, policy loss: 15.674195900188616
Experience 22, Iter 74, disc loss: 0.006897288863141936, policy loss: 14.566413807055936
Experience 22, Iter 75, disc loss: 0.008041638585417472, policy loss: 14.290318726042944
Experience 22, Iter 76, disc loss: 0.004928377301652661, policy loss: 17.431617323033564
Experience 22, Iter 77, disc loss: 0.005305271616843851, policy loss: 17.611005778317782
Experience 22, Iter 78, disc loss: 0.008994845477613832, policy loss: 16.97971369801135
Experience 22, Iter 79, disc loss: 0.006445860362384386, policy loss: 17.051126708419034
Experience 22, Iter 80, disc loss: 0.010924108898200778, policy loss: 16.021316625053817
Experience 22, Iter 81, disc loss: 0.011324981934875439, policy loss: 16.522175312530713
Experience 22, Iter 82, disc loss: 0.011893985495915891, policy loss: 15.39159051485855
Experience 22, Iter 83, disc loss: 0.004931645181142154, policy loss: 14.984844474210304
Experience 22, Iter 84, disc loss: 0.005372108122305767, policy loss: 15.579215128399255
Experience 22, Iter 85, disc loss: 0.008159961949113443, policy loss: 16.575135625435767
Experience 22, Iter 86, disc loss: 0.009543772841019276, policy loss: 15.879719469566714
Experience 22, Iter 87, disc loss: 0.004226332229290305, policy loss: 18.447477744798555
Experience 22, Iter 88, disc loss: 0.01010074396212795, policy loss: 15.276978742268662
Experience 22, Iter 89, disc loss: 0.006132332954783556, policy loss: 14.813964679276406
Experience 22, Iter 90, disc loss: 0.0050348641583370934, policy loss: 15.821597402323524
Experience 22, Iter 91, disc loss: 0.006053998052014226, policy loss: 17.65093517030637
Experience 22, Iter 92, disc loss: 0.003982427589318958, policy loss: 17.676054453577834
Experience 22, Iter 93, disc loss: 0.0035895704680083415, policy loss: 16.360801251471017
Experience 22, Iter 94, disc loss: 0.003350192466877381, policy loss: 18.68776842990872
Experience 22, Iter 95, disc loss: 0.0031324168184881694, policy loss: 18.17628847986814
Experience 22, Iter 96, disc loss: 0.0029320493018447184, policy loss: 18.617220145801472
Experience 22, Iter 97, disc loss: 0.0027260436037972887, policy loss: 20.255546489498897
Experience 22, Iter 98, disc loss: 0.0025222025977181084, policy loss: 18.45082120031887
Experience 22, Iter 99, disc loss: 0.0023230504218709493, policy loss: 19.400878471032552
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0068],
        [0.2351],
        [2.3335],
        [0.0265]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0392, 0.2946, 1.2615, 0.0250, 0.0069, 5.7700]],

        [[0.0392, 0.2946, 1.2615, 0.0250, 0.0069, 5.7700]],

        [[0.0392, 0.2946, 1.2615, 0.0250, 0.0069, 5.7700]],

        [[0.0392, 0.2946, 1.2615, 0.0250, 0.0069, 5.7700]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0273, 0.9402, 9.3342, 0.1059], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0273, 0.9402, 9.3342, 0.1059])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.912
Iter 2/2000 - Loss: 3.721
Iter 3/2000 - Loss: 3.701
Iter 4/2000 - Loss: 3.681
Iter 5/2000 - Loss: 3.596
Iter 6/2000 - Loss: 3.486
Iter 7/2000 - Loss: 3.406
Iter 8/2000 - Loss: 3.358
Iter 9/2000 - Loss: 3.294
Iter 10/2000 - Loss: 3.190
Iter 11/2000 - Loss: 3.062
Iter 12/2000 - Loss: 2.934
Iter 13/2000 - Loss: 2.807
Iter 14/2000 - Loss: 2.674
Iter 15/2000 - Loss: 2.524
Iter 16/2000 - Loss: 2.354
Iter 17/2000 - Loss: 2.168
Iter 18/2000 - Loss: 1.966
Iter 19/2000 - Loss: 1.751
Iter 20/2000 - Loss: 1.523
Iter 1981/2000 - Loss: -7.857
Iter 1982/2000 - Loss: -7.857
Iter 1983/2000 - Loss: -7.857
Iter 1984/2000 - Loss: -7.857
Iter 1985/2000 - Loss: -7.857
Iter 1986/2000 - Loss: -7.857
Iter 1987/2000 - Loss: -7.857
Iter 1988/2000 - Loss: -7.857
Iter 1989/2000 - Loss: -7.857
Iter 1990/2000 - Loss: -7.858
Iter 1991/2000 - Loss: -7.858
Iter 1992/2000 - Loss: -7.858
Iter 1993/2000 - Loss: -7.858
Iter 1994/2000 - Loss: -7.858
Iter 1995/2000 - Loss: -7.858
Iter 1996/2000 - Loss: -7.858
Iter 1997/2000 - Loss: -7.858
Iter 1998/2000 - Loss: -7.858
Iter 1999/2000 - Loss: -7.858
Iter 2000/2000 - Loss: -7.858
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[18.3991, 17.2305, 28.3164,  8.0544,  6.2054, 65.2687]],

        [[24.4826, 42.3051,  8.8512,  1.1108,  1.8425, 17.5967]],

        [[27.0313, 43.8233,  9.2496,  0.9012,  1.3541, 19.1318]],

        [[20.9953, 38.9377, 14.0874,  1.6353,  1.8775, 45.4070]]])
Signal Variance: tensor([ 0.3389,  1.2981, 11.3445,  0.4443])
Estimated target variance: tensor([0.0273, 0.9402, 9.3342, 0.1059])
N: 230
Signal to noise ratio: tensor([34.6904, 63.6213, 82.8003, 45.3642])
Bound on condition number: tensor([ 276788.6143,  930965.7409, 1576854.6797,  473320.4489])
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 0.0021336926221158787, policy loss: 18.41325385001275
Experience 23, Iter 1, disc loss: 0.0019566387615985274, policy loss: 19.591415604973875
Experience 23, Iter 2, disc loss: 0.001791713448030968, policy loss: 18.921831432159127
Experience 23, Iter 3, disc loss: 0.0016413964848472852, policy loss: 20.24578246002061
Experience 23, Iter 4, disc loss: 0.0015046525884199688, policy loss: 20.363517669496694
Experience 23, Iter 5, disc loss: 0.0013813746552107484, policy loss: 20.789161275599472
Experience 23, Iter 6, disc loss: 0.001270394197389821, policy loss: 20.043608686253222
Experience 23, Iter 7, disc loss: 0.0011708408735529965, policy loss: 22.15492060011349
Experience 23, Iter 8, disc loss: 0.001081660008431664, policy loss: 19.959458033586603
Experience 23, Iter 9, disc loss: 0.0010019789264492794, policy loss: 18.821086545268763
Experience 23, Iter 10, disc loss: 0.0009306610276997833, policy loss: 19.589011173204106
Experience 23, Iter 11, disc loss: 0.0008667097221769409, policy loss: 19.646138843740992
Experience 23, Iter 12, disc loss: 0.0008094159301444063, policy loss: 21.073145338349306
Experience 23, Iter 13, disc loss: 0.0007582015453942489, policy loss: 20.015028293347463
Experience 23, Iter 14, disc loss: 0.0007124761432156525, policy loss: 21.537215187378358
Experience 23, Iter 15, disc loss: 0.0006706310751284913, policy loss: 19.801928929793533
Experience 23, Iter 16, disc loss: 0.0006333218730862718, policy loss: 18.250413517595554
Experience 23, Iter 17, disc loss: 0.0005994921082706347, policy loss: 20.186191925113324
Experience 23, Iter 18, disc loss: 0.0005704718822241423, policy loss: 21.38478772175339
Experience 23, Iter 19, disc loss: 0.0005420625954010525, policy loss: 22.097015521129812
Experience 23, Iter 20, disc loss: 0.0005160879489859012, policy loss: 21.19341442577639
Experience 23, Iter 21, disc loss: 0.0004934075206261442, policy loss: 20.15366902128506
Experience 23, Iter 22, disc loss: 0.0004725894193753372, policy loss: 20.801492956266657
Experience 23, Iter 23, disc loss: 0.0004540777133491572, policy loss: 19.560400800704926
Experience 23, Iter 24, disc loss: 0.00043550919250284867, policy loss: 21.699530697773213
Experience 23, Iter 25, disc loss: 0.00041985489683770446, policy loss: 19.094445227923153
Experience 23, Iter 26, disc loss: 0.0004045901863951178, policy loss: 19.998339568568575
Experience 23, Iter 27, disc loss: 0.00039111990830847785, policy loss: 18.661403558532076
Experience 23, Iter 28, disc loss: 0.00037831281710826035, policy loss: 21.13812296594836
Experience 23, Iter 29, disc loss: 0.00036647083625841303, policy loss: 20.803659827235805
Experience 23, Iter 30, disc loss: 0.00035569244659115496, policy loss: 20.157574355518847
Experience 23, Iter 31, disc loss: 0.0003477145349396526, policy loss: 20.63414577507635
Experience 23, Iter 32, disc loss: 0.00033588824884513396, policy loss: 19.710256176380724
Experience 23, Iter 33, disc loss: 0.0003269970482759959, policy loss: 20.912858339988254
Experience 23, Iter 34, disc loss: 0.0003186998350953619, policy loss: 19.729106385672477
Experience 23, Iter 35, disc loss: 0.0003108411408872506, policy loss: 20.604879347107804
Experience 23, Iter 36, disc loss: 0.00030362721156838466, policy loss: 19.883440622777066
Experience 23, Iter 37, disc loss: 0.0002996622439811586, policy loss: 20.07349452287354
Experience 23, Iter 38, disc loss: 0.0003215441668608799, policy loss: 21.01696399008568
Experience 23, Iter 39, disc loss: 0.00028576692990113826, policy loss: 19.68319932073581
Experience 23, Iter 40, disc loss: 0.00032859743834198464, policy loss: 20.830888069146802
Experience 23, Iter 41, disc loss: 0.0002732471665421208, policy loss: 19.18918187863285
Experience 23, Iter 42, disc loss: 0.0002674796719355766, policy loss: 20.851877070660642
Experience 23, Iter 43, disc loss: 0.0002626768753979553, policy loss: 20.578050685045746
Experience 23, Iter 44, disc loss: 0.0002578108039943888, policy loss: 20.2966058542714
Experience 23, Iter 45, disc loss: 0.0002533822027614673, policy loss: 20.44527398162164
Experience 23, Iter 46, disc loss: 0.00025063600295748673, policy loss: 19.479340706648134
Experience 23, Iter 47, disc loss: 0.000244997033400287, policy loss: 21.059135842585977
Experience 23, Iter 48, disc loss: 0.00024072356426415456, policy loss: 21.467554152049445
Experience 23, Iter 49, disc loss: 0.00023690833997558518, policy loss: 21.261254348391336
Experience 23, Iter 50, disc loss: 0.00023340891983381873, policy loss: 21.14379589957738
Experience 23, Iter 51, disc loss: 0.00023022712455341172, policy loss: 20.21042021026959
Experience 23, Iter 52, disc loss: 0.00022671336635390202, policy loss: 20.699580236926213
Experience 23, Iter 53, disc loss: 0.00023888371183185687, policy loss: 19.603662746596413
Experience 23, Iter 54, disc loss: 0.00022026095512220627, policy loss: 18.82798386738161
Experience 23, Iter 55, disc loss: 0.00021751275265371022, policy loss: 19.865554839681174
Experience 23, Iter 56, disc loss: 0.0002146977099275482, policy loss: 20.595108367658057
Experience 23, Iter 57, disc loss: 0.00021133026365787155, policy loss: 19.64708105199899
Experience 23, Iter 58, disc loss: 0.0002089389690008645, policy loss: 20.48659165981647
Experience 23, Iter 59, disc loss: 0.00020642430471763096, policy loss: 19.11508387086121
Experience 23, Iter 60, disc loss: 0.00020398858554111437, policy loss: 19.208947083061645
Experience 23, Iter 61, disc loss: 0.00020102722086576044, policy loss: 20.42100522479794
Experience 23, Iter 62, disc loss: 0.00019833573521325787, policy loss: 20.50052145744716
Experience 23, Iter 63, disc loss: 0.0001961435338246028, policy loss: 19.094928654145356
Experience 23, Iter 64, disc loss: 0.00019399195575078818, policy loss: 21.034379990136692
Experience 23, Iter 65, disc loss: 0.00019187420917993596, policy loss: 19.18233636720508
Experience 23, Iter 66, disc loss: 0.00018985662469036795, policy loss: 20.005727245840923
Experience 23, Iter 67, disc loss: 0.00018746734640163763, policy loss: 21.830467655461902
Experience 23, Iter 68, disc loss: 0.00018516465106841582, policy loss: 21.712663903118653
Experience 23, Iter 69, disc loss: 0.00018329546592552132, policy loss: 19.41190257681036
Experience 23, Iter 70, disc loss: 0.0001811968977851611, policy loss: 20.303810501059765
Experience 23, Iter 71, disc loss: 0.0001794619334731929, policy loss: 19.977163001962147
Experience 23, Iter 72, disc loss: 0.00017776971913426623, policy loss: 19.63240657527787
Experience 23, Iter 73, disc loss: 0.00017586411919803168, policy loss: 20.288367098672115
Experience 23, Iter 74, disc loss: 0.00017429613093182956, policy loss: 20.443025788213202
Experience 23, Iter 75, disc loss: 0.0001721052316359875, policy loss: 19.835652443330016
Experience 23, Iter 76, disc loss: 0.0001705765543543541, policy loss: 19.01243324457961
Experience 23, Iter 77, disc loss: 0.00016884005551969517, policy loss: 19.990865108953066
Experience 23, Iter 78, disc loss: 0.0001669466662276278, policy loss: 20.420553032919774
Experience 23, Iter 79, disc loss: 0.00016560883732785473, policy loss: 19.41429629127667
Experience 23, Iter 80, disc loss: 0.00020097755435592054, policy loss: 21.249065836902915
Experience 23, Iter 81, disc loss: 0.00016234693579130982, policy loss: 21.193199153867905
Experience 23, Iter 82, disc loss: 0.00016098605607001745, policy loss: 19.898975523804573
Experience 23, Iter 83, disc loss: 0.00015980036357795245, policy loss: 19.76603987673455
Experience 23, Iter 84, disc loss: 0.00015808985808996717, policy loss: 20.65793922101776
Experience 23, Iter 85, disc loss: 0.00016244235070697667, policy loss: 20.72810149412075
Experience 23, Iter 86, disc loss: 0.00015568487663440256, policy loss: 19.299219254518967
Experience 23, Iter 87, disc loss: 0.00015384111647500975, policy loss: 20.621814691622895
Experience 23, Iter 88, disc loss: 0.00015259141243307968, policy loss: 18.84454492131306
Experience 23, Iter 89, disc loss: 0.00015169548064879722, policy loss: 19.316151256032285
Experience 23, Iter 90, disc loss: 0.00014993057529316052, policy loss: 18.6776600863147
Experience 23, Iter 91, disc loss: 0.00014868183349822523, policy loss: 18.93493300924911
Experience 23, Iter 92, disc loss: 0.00014743586381726673, policy loss: 22.192989284247666
Experience 23, Iter 93, disc loss: 0.00014629615646548672, policy loss: 19.426814134168982
Experience 23, Iter 94, disc loss: 0.0001479273824991863, policy loss: 18.49615207832222
Experience 23, Iter 95, disc loss: 0.00014391633547708544, policy loss: 20.254292452286904
Experience 23, Iter 96, disc loss: 0.00014272953279772797, policy loss: 19.362378974305283
Experience 23, Iter 97, disc loss: 0.00014170330881662575, policy loss: 19.72702693159814
Experience 23, Iter 98, disc loss: 0.0001403999963787694, policy loss: 19.233041636751473
Experience 23, Iter 99, disc loss: 0.00013995853228816884, policy loss: 19.413413555520837
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0088],
        [0.2373],
        [2.3325],
        [0.0273]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0471, 0.3673, 1.2939, 0.0273, 0.0078, 6.0462]],

        [[0.0471, 0.3673, 1.2939, 0.0273, 0.0078, 6.0462]],

        [[0.0471, 0.3673, 1.2939, 0.0273, 0.0078, 6.0462]],

        [[0.0471, 0.3673, 1.2939, 0.0273, 0.0078, 6.0462]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0351, 0.9494, 9.3302, 0.1091], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0351, 0.9494, 9.3302, 0.1091])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.936
Iter 2/2000 - Loss: 3.795
Iter 3/2000 - Loss: 3.774
Iter 4/2000 - Loss: 3.701
Iter 5/2000 - Loss: 3.608
Iter 6/2000 - Loss: 3.530
Iter 7/2000 - Loss: 3.466
Iter 8/2000 - Loss: 3.391
Iter 9/2000 - Loss: 3.287
Iter 10/2000 - Loss: 3.165
Iter 11/2000 - Loss: 3.039
Iter 12/2000 - Loss: 2.911
Iter 13/2000 - Loss: 2.774
Iter 14/2000 - Loss: 2.619
Iter 15/2000 - Loss: 2.446
Iter 16/2000 - Loss: 2.258
Iter 17/2000 - Loss: 2.058
Iter 18/2000 - Loss: 1.845
Iter 19/2000 - Loss: 1.620
Iter 20/2000 - Loss: 1.379
Iter 1981/2000 - Loss: -7.886
Iter 1982/2000 - Loss: -7.886
Iter 1983/2000 - Loss: -7.886
Iter 1984/2000 - Loss: -7.886
Iter 1985/2000 - Loss: -7.887
Iter 1986/2000 - Loss: -7.887
Iter 1987/2000 - Loss: -7.887
Iter 1988/2000 - Loss: -7.887
Iter 1989/2000 - Loss: -7.887
Iter 1990/2000 - Loss: -7.887
Iter 1991/2000 - Loss: -7.887
Iter 1992/2000 - Loss: -7.887
Iter 1993/2000 - Loss: -7.887
Iter 1994/2000 - Loss: -7.887
Iter 1995/2000 - Loss: -7.887
Iter 1996/2000 - Loss: -7.887
Iter 1997/2000 - Loss: -7.887
Iter 1998/2000 - Loss: -7.887
Iter 1999/2000 - Loss: -7.887
Iter 2000/2000 - Loss: -7.887
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[18.0829, 17.2734, 27.8054,  8.3210,  6.4499, 64.5682]],

        [[24.7164, 44.0285,  8.6333,  1.1324,  1.9535, 18.2028]],

        [[27.0408, 45.0006,  9.1523,  0.9161,  1.3367, 20.2348]],

        [[20.9472, 40.7565, 13.8733,  1.6717,  1.7832, 46.1693]]])
Signal Variance: tensor([ 0.3407,  1.3409, 12.1704,  0.4278])
Estimated target variance: tensor([0.0351, 0.9494, 9.3302, 0.1091])
N: 240
Signal to noise ratio: tensor([34.7891, 65.0136, 85.4414, 44.7421])
Bound on condition number: tensor([ 290468.3846, 1014424.8187, 1752056.2123,  480447.1795])
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 0.00013852243612171326, policy loss: 17.211271915492762
Experience 24, Iter 1, disc loss: 0.0001379335292674345, policy loss: 19.352647042488442
Experience 24, Iter 2, disc loss: 0.00013640216771883853, policy loss: 19.097712787280734
Experience 24, Iter 3, disc loss: 0.00013556952331310483, policy loss: 19.346441978206222
Experience 24, Iter 4, disc loss: 0.00013429706050123094, policy loss: 18.251464992132057
Experience 24, Iter 5, disc loss: 0.00013353074096364374, policy loss: 19.27534655456906
Experience 24, Iter 6, disc loss: 0.00014321946122621034, policy loss: 20.710054636613883
Experience 24, Iter 7, disc loss: 0.00013186168896375482, policy loss: 18.387396361299036
Experience 24, Iter 8, disc loss: 0.00013024437638366735, policy loss: 21.55965126477909
Experience 24, Iter 9, disc loss: 0.00013095937051051964, policy loss: 18.677067466876125
Experience 24, Iter 10, disc loss: 0.0001288543056057205, policy loss: 18.755587507526755
Experience 24, Iter 11, disc loss: 0.00012794269629902243, policy loss: 18.022125696813966
Experience 24, Iter 12, disc loss: 0.0001267184661765091, policy loss: 19.87587946660412
Experience 24, Iter 13, disc loss: 0.00012633772402304033, policy loss: 18.413986811186156
Experience 24, Iter 14, disc loss: 0.00012528220107121865, policy loss: 19.646717661039723
Experience 24, Iter 15, disc loss: 0.00012449076552819137, policy loss: 18.2681628126148
Experience 24, Iter 16, disc loss: 0.00012395903110549357, policy loss: 17.610884839855263
Experience 24, Iter 17, disc loss: 0.00012275359377379236, policy loss: 19.040343826419946
Experience 24, Iter 18, disc loss: 0.00012139491389275932, policy loss: 20.229807212734226
Experience 24, Iter 19, disc loss: 0.00012176155750094737, policy loss: 16.911674719792828
Experience 24, Iter 20, disc loss: 0.00012065659901450226, policy loss: 17.959123320470546
Experience 24, Iter 21, disc loss: 0.00011982241917274493, policy loss: 18.44644940309756
Experience 24, Iter 22, disc loss: 0.00011877298708316578, policy loss: 18.89497705425471
Experience 24, Iter 23, disc loss: 0.00011800503074536897, policy loss: 17.42501915382131
Experience 24, Iter 24, disc loss: 0.00011877866612847755, policy loss: 17.584511153031016
Experience 24, Iter 25, disc loss: 0.00011716207011936233, policy loss: 17.46605462230732
Experience 24, Iter 26, disc loss: 0.00011649552231919349, policy loss: 17.985903808300524
Experience 24, Iter 27, disc loss: 0.0001156396393039513, policy loss: 17.618382713557008
Experience 24, Iter 28, disc loss: 0.00011419668657021224, policy loss: 19.088347451284434
Experience 24, Iter 29, disc loss: 0.00011400215933272412, policy loss: 19.139256297781394
Experience 24, Iter 30, disc loss: 0.0001146110572299828, policy loss: 18.748468014447237
Experience 24, Iter 31, disc loss: 0.00011291046720799673, policy loss: 17.95652996499278
Experience 24, Iter 32, disc loss: 0.0001124544513350232, policy loss: 18.229275128826764
Experience 24, Iter 33, disc loss: 0.00011086683095445091, policy loss: 18.38443533527877
Experience 24, Iter 34, disc loss: 0.00011077268020420348, policy loss: 18.997475888057526
Experience 24, Iter 35, disc loss: 0.00010969793104581331, policy loss: 19.106750609807957
Experience 24, Iter 36, disc loss: 0.0001095238187809639, policy loss: 19.177390525733486
Experience 24, Iter 37, disc loss: 0.00010817026967902162, policy loss: 17.664304893674007
Experience 24, Iter 38, disc loss: 0.00010823084574569209, policy loss: 19.256414418395035
Experience 24, Iter 39, disc loss: 0.00010780868165915342, policy loss: 17.953657581808592
Experience 24, Iter 40, disc loss: 0.00010713855655685075, policy loss: 18.679752842179372
Experience 24, Iter 41, disc loss: 0.00010611093215113965, policy loss: 18.69509298227413
Experience 24, Iter 42, disc loss: 0.00012240165873879087, policy loss: 17.066082925121762
Experience 24, Iter 43, disc loss: 0.00010546660147597699, policy loss: 19.817178572726633
Experience 24, Iter 44, disc loss: 0.0001038488511928564, policy loss: 18.898986747912982
Experience 24, Iter 45, disc loss: 0.00010390441429561833, policy loss: 19.174014872343513
Experience 24, Iter 46, disc loss: 0.00010304326847159367, policy loss: 18.076603918656243
Experience 24, Iter 47, disc loss: 0.00010238638606575104, policy loss: 20.51167963731168
Experience 24, Iter 48, disc loss: 0.00010319668170367351, policy loss: 16.91801330134953
Experience 24, Iter 49, disc loss: 0.00010112936462140519, policy loss: 19.01380773829102
Experience 24, Iter 50, disc loss: 0.00010145187968189523, policy loss: 19.92548361676419
Experience 24, Iter 51, disc loss: 0.00010051217150254818, policy loss: 19.483785060080734
Experience 24, Iter 52, disc loss: 9.98773425941773e-05, policy loss: 18.992345785883003
Experience 24, Iter 53, disc loss: 9.95654940810478e-05, policy loss: 18.18683678348352
Experience 24, Iter 54, disc loss: 0.00010029440528809728, policy loss: 16.844420861218655
Experience 24, Iter 55, disc loss: 9.851568691314191e-05, policy loss: 19.02368123595349
Experience 24, Iter 56, disc loss: 9.789700331713713e-05, policy loss: 18.86748179884969
Experience 24, Iter 57, disc loss: 9.72733965498637e-05, policy loss: 18.09133562290736
Experience 24, Iter 58, disc loss: 9.782233840028793e-05, policy loss: 17.946749510910443
Experience 24, Iter 59, disc loss: 9.694441961293611e-05, policy loss: 18.226398862053948
Experience 24, Iter 60, disc loss: 9.626113489826485e-05, policy loss: 17.456644479551336
Experience 24, Iter 61, disc loss: 9.553968042464575e-05, policy loss: 18.395047522101883
Experience 24, Iter 62, disc loss: 9.52074872621718e-05, policy loss: 20.93994423720863
Experience 24, Iter 63, disc loss: 9.461330235852294e-05, policy loss: 19.527803554852227
Experience 24, Iter 64, disc loss: 9.504145767545282e-05, policy loss: 17.50374646478516
Experience 24, Iter 65, disc loss: 9.385745534028328e-05, policy loss: 17.82361318966833
Experience 24, Iter 66, disc loss: 9.324915835204804e-05, policy loss: 17.988896083542453
Experience 24, Iter 67, disc loss: 9.31446466705037e-05, policy loss: 18.1992704480472
Experience 24, Iter 68, disc loss: 9.228806952365633e-05, policy loss: 18.91351502474957
Experience 24, Iter 69, disc loss: 9.164644667072892e-05, policy loss: 18.71504344151247
Experience 24, Iter 70, disc loss: 9.150234600910963e-05, policy loss: 17.875697006963716
Experience 24, Iter 71, disc loss: 9.126719873729078e-05, policy loss: 18.54699651561704
Experience 24, Iter 72, disc loss: 9.080416902803921e-05, policy loss: 18.405328015870705
Experience 24, Iter 73, disc loss: 9.111834284993876e-05, policy loss: 18.49684781840598
Experience 24, Iter 74, disc loss: 9.137038079937684e-05, policy loss: 17.780872167610106
Experience 24, Iter 75, disc loss: 9.024831767167717e-05, policy loss: 18.39226647045329
Experience 24, Iter 76, disc loss: 9.030897868761723e-05, policy loss: 18.597967218321973
Experience 24, Iter 77, disc loss: 8.817646811313364e-05, policy loss: 18.99501871501981
Experience 24, Iter 78, disc loss: 8.947684472204334e-05, policy loss: 18.02418670574057
Experience 24, Iter 79, disc loss: 8.703023655990397e-05, policy loss: 19.29445187098894
Experience 24, Iter 80, disc loss: 8.765112223358258e-05, policy loss: 18.101362929449405
Experience 24, Iter 81, disc loss: 8.690352305878334e-05, policy loss: 18.8711446598279
Experience 24, Iter 82, disc loss: 8.783134162130472e-05, policy loss: 17.166417620642434
Experience 24, Iter 83, disc loss: 8.727940146873594e-05, policy loss: 18.426147055947258
Experience 24, Iter 84, disc loss: 8.577181925909935e-05, policy loss: 18.346991869270884
Experience 24, Iter 85, disc loss: 8.719045701242551e-05, policy loss: 18.452000433041107
Experience 24, Iter 86, disc loss: 8.856856372267448e-05, policy loss: 16.521493043252065
Experience 24, Iter 87, disc loss: 8.58287469379867e-05, policy loss: 16.91720951499441
Experience 24, Iter 88, disc loss: 8.651111546696292e-05, policy loss: 17.78080089692796
Experience 24, Iter 89, disc loss: 8.650200666582685e-05, policy loss: 17.383978658709342
Experience 24, Iter 90, disc loss: 8.582007193837837e-05, policy loss: 16.876380675435612
Experience 24, Iter 91, disc loss: 8.416889840741972e-05, policy loss: 18.91164626221039
Experience 24, Iter 92, disc loss: 8.557611074876335e-05, policy loss: 17.144880915408145
Experience 24, Iter 93, disc loss: 8.563563648486121e-05, policy loss: 18.446232529923286
Experience 24, Iter 94, disc loss: 8.219613879907013e-05, policy loss: 18.673378177460936
Experience 24, Iter 95, disc loss: 8.27969132088111e-05, policy loss: 18.623652001943714
Experience 24, Iter 96, disc loss: 8.154118187478173e-05, policy loss: 19.22655283583435
Experience 24, Iter 97, disc loss: 8.270293752793272e-05, policy loss: 17.942534481067675
Experience 24, Iter 98, disc loss: 9.319935559482962e-05, policy loss: 16.346395960935112
Experience 24, Iter 99, disc loss: 8.36403815020368e-05, policy loss: 16.990176928366466
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0087],
        [0.2418],
        [2.3802],
        [0.0273]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0455, 0.3637, 1.3012, 0.0282, 0.0077, 6.1307]],

        [[0.0455, 0.3637, 1.3012, 0.0282, 0.0077, 6.1307]],

        [[0.0455, 0.3637, 1.3012, 0.0282, 0.0077, 6.1307]],

        [[0.0455, 0.3637, 1.3012, 0.0282, 0.0077, 6.1307]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0348, 0.9673, 9.5208, 0.1092], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0348, 0.9673, 9.5208, 0.1092])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.934
Iter 2/2000 - Loss: 3.802
Iter 3/2000 - Loss: 3.781
Iter 4/2000 - Loss: 3.699
Iter 5/2000 - Loss: 3.610
Iter 6/2000 - Loss: 3.540
Iter 7/2000 - Loss: 3.476
Iter 8/2000 - Loss: 3.393
Iter 9/2000 - Loss: 3.286
Iter 10/2000 - Loss: 3.165
Iter 11/2000 - Loss: 3.040
Iter 12/2000 - Loss: 2.911
Iter 13/2000 - Loss: 2.770
Iter 14/2000 - Loss: 2.610
Iter 15/2000 - Loss: 2.432
Iter 16/2000 - Loss: 2.242
Iter 17/2000 - Loss: 2.040
Iter 18/2000 - Loss: 1.827
Iter 19/2000 - Loss: 1.600
Iter 20/2000 - Loss: 1.358
Iter 1981/2000 - Loss: -7.903
Iter 1982/2000 - Loss: -7.903
Iter 1983/2000 - Loss: -7.903
Iter 1984/2000 - Loss: -7.903
Iter 1985/2000 - Loss: -7.903
Iter 1986/2000 - Loss: -7.903
Iter 1987/2000 - Loss: -7.903
Iter 1988/2000 - Loss: -7.903
Iter 1989/2000 - Loss: -7.903
Iter 1990/2000 - Loss: -7.903
Iter 1991/2000 - Loss: -7.903
Iter 1992/2000 - Loss: -7.903
Iter 1993/2000 - Loss: -7.903
Iter 1994/2000 - Loss: -7.903
Iter 1995/2000 - Loss: -7.904
Iter 1996/2000 - Loss: -7.904
Iter 1997/2000 - Loss: -7.904
Iter 1998/2000 - Loss: -7.904
Iter 1999/2000 - Loss: -7.904
Iter 2000/2000 - Loss: -7.904
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[17.6830, 17.0938, 28.2082,  7.7447,  6.5746, 65.4465]],

        [[25.1227, 44.8258,  8.6271,  1.0899,  1.9134, 17.5338]],

        [[27.7529, 45.3762,  9.2829,  0.8680,  1.2178, 21.0415]],

        [[20.8371, 40.3587, 14.1494,  1.6827,  1.7122, 45.7404]]])
Signal Variance: tensor([ 0.3297,  1.2362, 12.1440,  0.4061])
Estimated target variance: tensor([0.0348, 0.9673, 9.5208, 0.1092])
N: 250
Signal to noise ratio: tensor([34.0069, 61.6249, 85.6509, 42.9331])
Bound on condition number: tensor([ 289118.1179,  949408.2100, 1834018.2026,  460812.9215])
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 0.00010109882192824918, policy loss: 18.95347117056209
Experience 25, Iter 1, disc loss: 8.17493142336217e-05, policy loss: 18.236917489800007
Experience 25, Iter 2, disc loss: 8.373363802817975e-05, policy loss: 16.00302164373733
Experience 25, Iter 3, disc loss: 8.326870052162502e-05, policy loss: 18.649055052899065
Experience 25, Iter 4, disc loss: 8.324073029659052e-05, policy loss: 17.017377532710604
Experience 25, Iter 5, disc loss: 8.046669192435722e-05, policy loss: 17.804013504835005
Experience 25, Iter 6, disc loss: 8.033812396921612e-05, policy loss: 17.981342349211868
Experience 25, Iter 7, disc loss: 8.07936095008264e-05, policy loss: 17.542604884748513
Experience 25, Iter 8, disc loss: 7.899150346965712e-05, policy loss: 17.79929083569016
Experience 25, Iter 9, disc loss: 7.834759911391344e-05, policy loss: 19.61419741188336
Experience 25, Iter 10, disc loss: 7.998696692216984e-05, policy loss: 17.15379036551444
Experience 25, Iter 11, disc loss: 7.916840567634287e-05, policy loss: 18.63820884779382
Experience 25, Iter 12, disc loss: 8.011521992371897e-05, policy loss: 17.38516775398515
Experience 25, Iter 13, disc loss: 7.912103673622377e-05, policy loss: 18.23812347792982
Experience 25, Iter 14, disc loss: 7.747307327096885e-05, policy loss: 18.4509228263453
Experience 25, Iter 15, disc loss: 8.084342521284323e-05, policy loss: 16.458473562256767
Experience 25, Iter 16, disc loss: 7.673938764426182e-05, policy loss: 18.635721767212814
Experience 25, Iter 17, disc loss: 7.801017848118446e-05, policy loss: 17.499298811795537
Experience 25, Iter 18, disc loss: 7.958302767163868e-05, policy loss: 17.246325095699095
Experience 25, Iter 19, disc loss: 7.583704891376208e-05, policy loss: 19.730708194901755
Experience 25, Iter 20, disc loss: 7.750030378031564e-05, policy loss: 18.27379822198038
Experience 25, Iter 21, disc loss: 7.68753246534893e-05, policy loss: 18.475558893617986
Experience 25, Iter 22, disc loss: 7.509693738356328e-05, policy loss: 19.930912182671747
Experience 25, Iter 23, disc loss: 7.601039101038222e-05, policy loss: 19.208522879199947
Experience 25, Iter 24, disc loss: 7.55872185976072e-05, policy loss: 19.515915423459198
Experience 25, Iter 25, disc loss: 8.10305742163612e-05, policy loss: 16.403625573284906
Experience 25, Iter 26, disc loss: 7.474930229598734e-05, policy loss: 18.3436620352345
Experience 25, Iter 27, disc loss: 7.607850531325639e-05, policy loss: 17.890187864925835
Experience 25, Iter 28, disc loss: 7.387339881053497e-05, policy loss: 18.405031600310522
Experience 25, Iter 29, disc loss: 7.783989903155403e-05, policy loss: 18.14144950269947
Experience 25, Iter 30, disc loss: 7.904584335595739e-05, policy loss: 16.519503700667677
Experience 25, Iter 31, disc loss: 7.69568419346295e-05, policy loss: 17.90066348041085
Experience 25, Iter 32, disc loss: 7.36252333844024e-05, policy loss: 17.24821507375721
Experience 25, Iter 33, disc loss: 7.785351764213516e-05, policy loss: 16.158426759952224
Experience 25, Iter 34, disc loss: 7.435771153959107e-05, policy loss: 17.64202802522333
Experience 25, Iter 35, disc loss: 7.624370741574608e-05, policy loss: 18.85857137226206
Experience 25, Iter 36, disc loss: 7.455473904067506e-05, policy loss: 17.463707149044787
Experience 25, Iter 37, disc loss: 7.681901612099806e-05, policy loss: 16.568962960961983
Experience 25, Iter 38, disc loss: 7.290853745045017e-05, policy loss: 18.319169646364614
Experience 25, Iter 39, disc loss: 7.54392420989401e-05, policy loss: 18.208146840769256
Experience 25, Iter 40, disc loss: 7.470653492432803e-05, policy loss: 18.696751333391106
Experience 25, Iter 41, disc loss: 7.69599295513358e-05, policy loss: 16.43939996156102
Experience 25, Iter 42, disc loss: 7.460565135616755e-05, policy loss: 18.351811574137322
Experience 25, Iter 43, disc loss: 7.808561999985997e-05, policy loss: 16.681361667195958
Experience 25, Iter 44, disc loss: 7.463508280864146e-05, policy loss: 18.207170054282184
Experience 25, Iter 45, disc loss: 7.678845206199663e-05, policy loss: 18.351571928679576
Experience 25, Iter 46, disc loss: 7.628617573660782e-05, policy loss: 18.503254738917406
Experience 25, Iter 47, disc loss: 7.649571790203807e-05, policy loss: 17.65576263671149
Experience 25, Iter 48, disc loss: 7.50416356173622e-05, policy loss: 17.603629130651953
Experience 25, Iter 49, disc loss: 8.125211907032413e-05, policy loss: 16.800831508330376
Experience 25, Iter 50, disc loss: 7.730554081331894e-05, policy loss: 17.8188398132405
Experience 25, Iter 51, disc loss: 8.382772660368523e-05, policy loss: 17.566785954976375
Experience 25, Iter 52, disc loss: 8.44990896478092e-05, policy loss: 17.83569590950686
Experience 25, Iter 53, disc loss: 8.40847218136953e-05, policy loss: 17.721235954211078
Experience 25, Iter 54, disc loss: 8.758486143790952e-05, policy loss: 16.756501252857895
Experience 25, Iter 55, disc loss: 7.600985512932217e-05, policy loss: 18.139919081807687
Experience 25, Iter 56, disc loss: 8.355723929015993e-05, policy loss: 16.504138449705202
Experience 25, Iter 57, disc loss: 9.118932437694254e-05, policy loss: 16.33297897671809
Experience 25, Iter 58, disc loss: 8.785499012567981e-05, policy loss: 18.78168467510963
Experience 25, Iter 59, disc loss: 8.799116662432946e-05, policy loss: 16.616783412312458
Experience 25, Iter 60, disc loss: 8.288287430954589e-05, policy loss: 18.348332158973946
Experience 25, Iter 61, disc loss: 8.293203904020713e-05, policy loss: 17.05825584612075
Experience 25, Iter 62, disc loss: 0.0001362484617116908, policy loss: 18.644456206909055
Experience 25, Iter 63, disc loss: 0.00011008810893793672, policy loss: 17.803361920075147
Experience 25, Iter 64, disc loss: 0.00010508470539269895, policy loss: 17.00866396431149
Experience 25, Iter 65, disc loss: 0.00011900324583887127, policy loss: 15.639513422386564
Experience 25, Iter 66, disc loss: 0.00012146661391601059, policy loss: 17.257987334805318
Experience 25, Iter 67, disc loss: 0.00015834591992286908, policy loss: 16.097180195800206
Experience 25, Iter 68, disc loss: 0.00015476638133490617, policy loss: 16.372933926709308
Experience 25, Iter 69, disc loss: 0.00015767376828276439, policy loss: 16.983785837272144
Experience 25, Iter 70, disc loss: 0.00016373882155343406, policy loss: 16.83965757033839
Experience 25, Iter 71, disc loss: 0.00016547896314336177, policy loss: 17.71951913689156
Experience 25, Iter 72, disc loss: 0.00015520339844146013, policy loss: 17.50511466415775
Experience 25, Iter 73, disc loss: 0.000185011359129098, policy loss: 16.98546661966678
Experience 25, Iter 74, disc loss: 0.00019345028062852878, policy loss: 17.175028759549775
Experience 25, Iter 75, disc loss: 0.00014398666398463786, policy loss: 16.13863323656281
Experience 25, Iter 76, disc loss: 0.00011341983187388379, policy loss: 16.89004367887088
Experience 25, Iter 77, disc loss: 0.0002378450850000916, policy loss: 14.975703951723304
Experience 25, Iter 78, disc loss: 0.0002297473053752654, policy loss: 16.5531053441182
Experience 25, Iter 79, disc loss: 0.0003072262018139044, policy loss: 16.82240792259907
Experience 25, Iter 80, disc loss: 0.00017297599508697838, policy loss: 16.727318526788952
Experience 25, Iter 81, disc loss: 0.00012211860751221577, policy loss: 19.293838312778774
Experience 25, Iter 82, disc loss: 0.00031270085500540487, policy loss: 15.95632468003332
Experience 25, Iter 83, disc loss: 0.00014172674917928329, policy loss: 16.42597949533713
Experience 25, Iter 84, disc loss: 0.00012687364741189629, policy loss: 15.77363190483108
Experience 25, Iter 85, disc loss: 0.0001410128959540149, policy loss: 18.50886911844205
Experience 25, Iter 86, disc loss: 0.00010336989574996963, policy loss: 16.91408698957648
Experience 25, Iter 87, disc loss: 0.00010866561847343003, policy loss: 17.424398318925032
Experience 25, Iter 88, disc loss: 0.0001139119682245989, policy loss: 15.823786513446755
Experience 25, Iter 89, disc loss: 0.00011800496908788805, policy loss: 16.272521373664077
Experience 25, Iter 90, disc loss: 8.814069353169143e-05, policy loss: 18.699874939792135
Experience 25, Iter 91, disc loss: 9.206690426805607e-05, policy loss: 17.836469959735044
Experience 25, Iter 92, disc loss: 8.244765203807205e-05, policy loss: 18.085141991787108
Experience 25, Iter 93, disc loss: 0.00010101338239143312, policy loss: 17.488910946821754
Experience 25, Iter 94, disc loss: 8.210423436571627e-05, policy loss: 18.35279223053424
Experience 25, Iter 95, disc loss: 9.798904344298382e-05, policy loss: 16.438012486590473
Experience 25, Iter 96, disc loss: 8.48712571058242e-05, policy loss: 15.88217349303184
Experience 25, Iter 97, disc loss: 8.967608114951914e-05, policy loss: 15.587592917972433
Experience 25, Iter 98, disc loss: 9.318281532349202e-05, policy loss: 16.18747639388831
Experience 25, Iter 99, disc loss: 9.356831835386588e-05, policy loss: 15.661116903373667
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0086],
        [0.2450],
        [2.3922],
        [0.0268]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0440, 0.3581, 1.2867, 0.0286, 0.0075, 6.2071]],

        [[0.0440, 0.3581, 1.2867, 0.0286, 0.0075, 6.2071]],

        [[0.0440, 0.3581, 1.2867, 0.0286, 0.0075, 6.2071]],

        [[0.0440, 0.3581, 1.2867, 0.0286, 0.0075, 6.2071]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0344, 0.9800, 9.5688, 0.1073], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0344, 0.9800, 9.5688, 0.1073])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.914
Iter 2/2000 - Loss: 3.785
Iter 3/2000 - Loss: 3.760
Iter 4/2000 - Loss: 3.669
Iter 5/2000 - Loss: 3.581
Iter 6/2000 - Loss: 3.514
Iter 7/2000 - Loss: 3.445
Iter 8/2000 - Loss: 3.353
Iter 9/2000 - Loss: 3.242
Iter 10/2000 - Loss: 3.121
Iter 11/2000 - Loss: 2.996
Iter 12/2000 - Loss: 2.865
Iter 13/2000 - Loss: 2.719
Iter 14/2000 - Loss: 2.554
Iter 15/2000 - Loss: 2.374
Iter 16/2000 - Loss: 2.183
Iter 17/2000 - Loss: 1.981
Iter 18/2000 - Loss: 1.767
Iter 19/2000 - Loss: 1.539
Iter 20/2000 - Loss: 1.294
Iter 1981/2000 - Loss: -7.989
Iter 1982/2000 - Loss: -7.989
Iter 1983/2000 - Loss: -7.989
Iter 1984/2000 - Loss: -7.989
Iter 1985/2000 - Loss: -7.989
Iter 1986/2000 - Loss: -7.989
Iter 1987/2000 - Loss: -7.989
Iter 1988/2000 - Loss: -7.989
Iter 1989/2000 - Loss: -7.989
Iter 1990/2000 - Loss: -7.989
Iter 1991/2000 - Loss: -7.989
Iter 1992/2000 - Loss: -7.989
Iter 1993/2000 - Loss: -7.989
Iter 1994/2000 - Loss: -7.989
Iter 1995/2000 - Loss: -7.989
Iter 1996/2000 - Loss: -7.990
Iter 1997/2000 - Loss: -7.990
Iter 1998/2000 - Loss: -7.990
Iter 1999/2000 - Loss: -7.990
Iter 2000/2000 - Loss: -7.990
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[17.3032, 17.0651, 28.6014,  7.8914,  6.4582, 65.5391]],

        [[24.8995, 44.5719,  8.6093,  1.0720,  2.0034, 18.4025]],

        [[27.1923, 44.9782,  9.1769,  0.8533,  1.2219, 21.7661]],

        [[20.7029, 40.0756, 14.0146,  1.7156,  1.6770, 45.4154]]])
Signal Variance: tensor([ 0.3333,  1.3077, 12.3696,  0.3989])
Estimated target variance: tensor([0.0344, 0.9800, 9.5688, 0.1073])
N: 260
Signal to noise ratio: tensor([34.5889, 63.5864, 87.4277, 42.9976])
Bound on condition number: tensor([ 311062.2914, 1051241.0558, 1987337.3080,  480687.9222])
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 8.101540487771985e-05, policy loss: 16.512808902059074
Experience 26, Iter 1, disc loss: 9.366718835538382e-05, policy loss: 16.698812207126636
Experience 26, Iter 2, disc loss: 0.00010220862243833076, policy loss: 15.256200757687552
Experience 26, Iter 3, disc loss: 9.241310069848675e-05, policy loss: 17.6748600834533
Experience 26, Iter 4, disc loss: 9.416842483504939e-05, policy loss: 15.837128242657375
Experience 26, Iter 5, disc loss: 8.210538137383896e-05, policy loss: 17.66127291773032
Experience 26, Iter 6, disc loss: 8.405827523765041e-05, policy loss: 16.709136645927803
Experience 26, Iter 7, disc loss: 8.250294789731837e-05, policy loss: 17.054916648062147
Experience 26, Iter 8, disc loss: 8.697281232327109e-05, policy loss: 16.669046697433668
Experience 26, Iter 9, disc loss: 0.00011101425454355165, policy loss: 15.692875020988751
Experience 26, Iter 10, disc loss: 9.906230212542747e-05, policy loss: 16.548040803258964
Experience 26, Iter 11, disc loss: 0.00010970838464760966, policy loss: 15.800831419955
Experience 26, Iter 12, disc loss: 0.00010388518850755843, policy loss: 17.031842767702464
Experience 26, Iter 13, disc loss: 0.00011708974235785552, policy loss: 15.896845858779802
Experience 26, Iter 14, disc loss: 9.3169260095753e-05, policy loss: 16.71932951036961
Experience 26, Iter 15, disc loss: 0.00013351534067548209, policy loss: 15.799624360152372
Experience 26, Iter 16, disc loss: 0.0001317760816036488, policy loss: 15.257855486923145
Experience 26, Iter 17, disc loss: 0.00012356841122674098, policy loss: 15.154140289230616
Experience 26, Iter 18, disc loss: 0.00013553235169031045, policy loss: 15.075138645594148
Experience 26, Iter 19, disc loss: 0.0001649172922328274, policy loss: 16.38970498626833
Experience 26, Iter 20, disc loss: 0.00013437032199303914, policy loss: 17.17102531695087
Experience 26, Iter 21, disc loss: 0.00015386005386511692, policy loss: 14.767339849499102
Experience 26, Iter 22, disc loss: 0.00016402378312490603, policy loss: 16.34638339184312
Experience 26, Iter 23, disc loss: 0.00013514072374098255, policy loss: 16.541784955735743
Experience 26, Iter 24, disc loss: 0.00015933496099349633, policy loss: 17.208883939093585
Experience 26, Iter 25, disc loss: 0.00016164107454968274, policy loss: 16.719832457849982
Experience 26, Iter 26, disc loss: 0.00016239596787219514, policy loss: 15.435145397617966
Experience 26, Iter 27, disc loss: 0.00011818179733845627, policy loss: 17.804646692701496
Experience 26, Iter 28, disc loss: 0.00021447695072949498, policy loss: 16.524872814807757
Experience 26, Iter 29, disc loss: 0.00023615962089994268, policy loss: 15.803272495330326
Experience 26, Iter 30, disc loss: 0.0002053611606112913, policy loss: 15.656393919964096
Experience 26, Iter 31, disc loss: 0.00015363660168345235, policy loss: 17.550709643294493
Experience 26, Iter 32, disc loss: 0.00016993057117269572, policy loss: 16.554514612947585
Experience 26, Iter 33, disc loss: 0.00013991744255672093, policy loss: 16.03315125929428
Experience 26, Iter 34, disc loss: 0.00019315352919868102, policy loss: 14.709050280012054
Experience 26, Iter 35, disc loss: 0.00014988077551955932, policy loss: 16.20473810327125
Experience 26, Iter 36, disc loss: 0.00018576033867032262, policy loss: 17.833271832892407
Experience 26, Iter 37, disc loss: 9.993330260048995e-05, policy loss: 18.15078666525431
Experience 26, Iter 38, disc loss: 0.00011273286984436451, policy loss: 18.020481462255617
Experience 26, Iter 39, disc loss: 0.00015130324960517985, policy loss: 17.064611557542918
Experience 26, Iter 40, disc loss: 0.0002502561012701778, policy loss: 15.057982121182421
Experience 26, Iter 41, disc loss: 0.00015248314531798488, policy loss: 15.98667547675313
Experience 26, Iter 42, disc loss: 0.00018244099981673024, policy loss: 16.758972477925784
Experience 26, Iter 43, disc loss: 0.00021883018943534755, policy loss: 17.382845680429405
Experience 26, Iter 44, disc loss: 0.0002721076764764208, policy loss: 14.651491829037763
Experience 26, Iter 45, disc loss: 0.00022804892087110169, policy loss: 15.908937821522127
Experience 26, Iter 46, disc loss: 0.00019636585773737678, policy loss: 17.678953434882324
Experience 26, Iter 47, disc loss: 0.00023619464780078443, policy loss: 14.474510727647418
Experience 26, Iter 48, disc loss: 0.00015383891655975425, policy loss: 16.518584352911525
Experience 26, Iter 49, disc loss: 0.00018190063169579705, policy loss: 16.430443869460877
Experience 26, Iter 50, disc loss: 0.00011088305240279447, policy loss: 17.192640377945168
Experience 26, Iter 51, disc loss: 0.00012918515410702895, policy loss: 16.847745932098487
Experience 26, Iter 52, disc loss: 0.00011431628185986609, policy loss: 18.003938799422436
Experience 26, Iter 53, disc loss: 0.00013192890214524028, policy loss: 16.211055922028876
Experience 26, Iter 54, disc loss: 0.00010630291146767617, policy loss: 17.340626301473186
Experience 26, Iter 55, disc loss: 0.00014242177478195043, policy loss: 16.00342582925641
Experience 26, Iter 56, disc loss: 0.00013803877631754318, policy loss: 15.775642881751663
Experience 26, Iter 57, disc loss: 0.0001420830092080209, policy loss: 15.740838804899909
Experience 26, Iter 58, disc loss: 0.00013110598256102295, policy loss: 16.53156179286878
Experience 26, Iter 59, disc loss: 0.00011404786881707722, policy loss: 15.516852704934696
Experience 26, Iter 60, disc loss: 0.0001444491561012168, policy loss: 17.955843088648272
Experience 26, Iter 61, disc loss: 0.00011457327527127602, policy loss: 16.791377026626847
Experience 26, Iter 62, disc loss: 0.0001296611833178297, policy loss: 16.36146799208351
Experience 26, Iter 63, disc loss: 0.00013112945392895452, policy loss: 16.43920820509846
Experience 26, Iter 64, disc loss: 0.00013346439962446277, policy loss: 17.46259086812269
Experience 26, Iter 65, disc loss: 0.00016367288660726424, policy loss: 14.104968218729743
Experience 26, Iter 66, disc loss: 0.00012458590482511013, policy loss: 16.10101764848669
Experience 26, Iter 67, disc loss: 0.0001341121681819095, policy loss: 16.689295570053613
Experience 26, Iter 68, disc loss: 0.00011586056815084681, policy loss: 17.709097606153318
Experience 26, Iter 69, disc loss: 0.00012853719018855497, policy loss: 15.51763735758234
Experience 26, Iter 70, disc loss: 0.0002030466440584905, policy loss: 16.068889112696013
Experience 26, Iter 71, disc loss: 0.0001888867130618961, policy loss: 16.41112989181463
Experience 26, Iter 72, disc loss: 0.0001622427043574034, policy loss: 15.722925949968939
Experience 26, Iter 73, disc loss: 0.00020424325451870207, policy loss: 15.785923844853526
Experience 26, Iter 74, disc loss: 0.00017839039681674022, policy loss: 16.956902461324567
Experience 26, Iter 75, disc loss: 0.00019735752184660087, policy loss: 15.440015097356241
Experience 26, Iter 76, disc loss: 0.0001695983045396501, policy loss: 17.613618160951532
Experience 26, Iter 77, disc loss: 0.00017231436756120526, policy loss: 17.740183379406908
Experience 26, Iter 78, disc loss: 0.0002712626931477012, policy loss: 15.352277151248924
Experience 26, Iter 79, disc loss: 0.00019178304898168132, policy loss: 14.382857124070387
Experience 26, Iter 80, disc loss: 0.00013914446596794544, policy loss: 15.761831227769475
Experience 26, Iter 81, disc loss: 0.00010932113344958166, policy loss: 17.667490668499866
Experience 26, Iter 82, disc loss: 8.702478325158186e-05, policy loss: 14.670080591620536
Experience 26, Iter 83, disc loss: 7.90105059496925e-05, policy loss: 18.386167597552237
Experience 26, Iter 84, disc loss: 6.70877229820668e-05, policy loss: 17.67844963812029
Experience 26, Iter 85, disc loss: 7.108359628159815e-05, policy loss: 18.481088535245014
Experience 26, Iter 86, disc loss: 6.692792655513075e-05, policy loss: 16.217424609132614
Experience 26, Iter 87, disc loss: 5.877335236218232e-05, policy loss: 17.523009735520777
Experience 26, Iter 88, disc loss: 6.24961128756097e-05, policy loss: 16.72826317440568
Experience 26, Iter 89, disc loss: 5.9279600599983585e-05, policy loss: 17.183167851545246
Experience 26, Iter 90, disc loss: 6.50549067572462e-05, policy loss: 15.026123449104855
Experience 26, Iter 91, disc loss: 5.79970740124292e-05, policy loss: 16.44169360695905
Experience 26, Iter 92, disc loss: 5.680739125721892e-05, policy loss: 18.60253799165409
Experience 26, Iter 93, disc loss: 5.6688451197476045e-05, policy loss: 18.086039782681173
Experience 26, Iter 94, disc loss: 5.5938329305092444e-05, policy loss: 18.759936943817916
Experience 26, Iter 95, disc loss: 5.6842363632376586e-05, policy loss: 16.677167382735405
Experience 26, Iter 96, disc loss: 5.73050664499832e-05, policy loss: 18.966300620754097
Experience 26, Iter 97, disc loss: 5.952647880582526e-05, policy loss: 15.976834920781837
Experience 26, Iter 98, disc loss: 5.696991392539781e-05, policy loss: 17.182570003657567
Experience 26, Iter 99, disc loss: 5.6888847116684526e-05, policy loss: 17.833472618542896
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0095],
        [0.2462],
        [2.3810],
        [0.0269]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0459, 0.3913, 1.2888, 0.0300, 0.0077, 6.3494]],

        [[0.0459, 0.3913, 1.2888, 0.0300, 0.0077, 6.3494]],

        [[0.0459, 0.3913, 1.2888, 0.0300, 0.0077, 6.3494]],

        [[0.0459, 0.3913, 1.2888, 0.0300, 0.0077, 6.3494]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0381, 0.9849, 9.5240, 0.1076], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0381, 0.9849, 9.5240, 0.1076])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.929
Iter 2/2000 - Loss: 3.826
Iter 3/2000 - Loss: 3.780
Iter 4/2000 - Loss: 3.683
Iter 5/2000 - Loss: 3.608
Iter 6/2000 - Loss: 3.544
Iter 7/2000 - Loss: 3.455
Iter 8/2000 - Loss: 3.349
Iter 9/2000 - Loss: 3.237
Iter 10/2000 - Loss: 3.118
Iter 11/2000 - Loss: 2.990
Iter 12/2000 - Loss: 2.849
Iter 13/2000 - Loss: 2.692
Iter 14/2000 - Loss: 2.520
Iter 15/2000 - Loss: 2.338
Iter 16/2000 - Loss: 2.145
Iter 17/2000 - Loss: 1.940
Iter 18/2000 - Loss: 1.721
Iter 19/2000 - Loss: 1.484
Iter 20/2000 - Loss: 1.233
Iter 1981/2000 - Loss: -8.047
Iter 1982/2000 - Loss: -8.047
Iter 1983/2000 - Loss: -8.047
Iter 1984/2000 - Loss: -8.047
Iter 1985/2000 - Loss: -8.047
Iter 1986/2000 - Loss: -8.047
Iter 1987/2000 - Loss: -8.047
Iter 1988/2000 - Loss: -8.047
Iter 1989/2000 - Loss: -8.047
Iter 1990/2000 - Loss: -8.047
Iter 1991/2000 - Loss: -8.047
Iter 1992/2000 - Loss: -8.047
Iter 1993/2000 - Loss: -8.047
Iter 1994/2000 - Loss: -8.048
Iter 1995/2000 - Loss: -8.048
Iter 1996/2000 - Loss: -8.048
Iter 1997/2000 - Loss: -8.048
Iter 1998/2000 - Loss: -8.048
Iter 1999/2000 - Loss: -8.048
Iter 2000/2000 - Loss: -8.048
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[16.9958, 17.5836, 29.5587,  7.5663,  6.6426, 64.9067]],

        [[24.5886, 45.1386,  8.6605,  1.0704,  1.9413, 18.3258]],

        [[26.8558, 45.0535,  9.3463,  0.8482,  1.2494, 21.6207]],

        [[20.5859, 40.6000, 12.5367,  1.8673,  1.6416, 42.9635]]])
Signal Variance: tensor([ 0.3444,  1.2899, 12.4119,  0.3691])
Estimated target variance: tensor([0.0381, 0.9849, 9.5240, 0.1076])
N: 270
Signal to noise ratio: tensor([35.2456, 62.9055, 87.7414, 42.0390])
Bound on condition number: tensor([ 335408.7889, 1068418.2467, 2078610.2655,  477165.4482])
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 5.95944191498336e-05, policy loss: 15.893348510376846
Experience 27, Iter 1, disc loss: 5.605231910122487e-05, policy loss: 16.322119546820375
Experience 27, Iter 2, disc loss: 5.662989922291366e-05, policy loss: 17.817367198598966
Experience 27, Iter 3, disc loss: 5.596855966233903e-05, policy loss: 18.856328227921693
Experience 27, Iter 4, disc loss: 5.466037018992868e-05, policy loss: 18.62986090548863
Experience 27, Iter 5, disc loss: 5.6348101722174246e-05, policy loss: 18.750045903550095
Experience 27, Iter 6, disc loss: 6.015797118068347e-05, policy loss: 17.158096304334947
Experience 27, Iter 7, disc loss: 5.689782619475349e-05, policy loss: 16.811374798182303
Experience 27, Iter 8, disc loss: 5.6619316926620894e-05, policy loss: 17.54629276370096
Experience 27, Iter 9, disc loss: 5.94302647218895e-05, policy loss: 16.915915461666692
Experience 27, Iter 10, disc loss: 5.770354876789328e-05, policy loss: 16.17061559729041
Experience 27, Iter 11, disc loss: 5.705150894550805e-05, policy loss: 18.203407812866104
Experience 27, Iter 12, disc loss: 5.423710131285616e-05, policy loss: 17.650626637162205
Experience 27, Iter 13, disc loss: 5.772720632167107e-05, policy loss: 17.59134928991654
Experience 27, Iter 14, disc loss: 5.836871682251267e-05, policy loss: 16.593403509077937
Experience 27, Iter 15, disc loss: 5.416350751807475e-05, policy loss: 16.896946542142828
Experience 27, Iter 16, disc loss: 5.8648159824630064e-05, policy loss: 19.413004505969155
Experience 27, Iter 17, disc loss: 5.771396313464049e-05, policy loss: 17.274365748344227
Experience 27, Iter 18, disc loss: 5.5574815247120944e-05, policy loss: 17.187066468942938
Experience 27, Iter 19, disc loss: 5.8613058670026834e-05, policy loss: 15.969875130222235
Experience 27, Iter 20, disc loss: 6.338187274198143e-05, policy loss: 15.982724012751362
Experience 27, Iter 21, disc loss: 5.631981117225488e-05, policy loss: 19.104648231658054
Experience 27, Iter 22, disc loss: 6.125752196559835e-05, policy loss: 17.208545758205503
Experience 27, Iter 23, disc loss: 6.143730203629853e-05, policy loss: 15.551696841100783
Experience 27, Iter 24, disc loss: 6.218997046752517e-05, policy loss: 17.22961003557676
Experience 27, Iter 25, disc loss: 8.103597754485744e-05, policy loss: 17.311195026079908
Experience 27, Iter 26, disc loss: 6.067020703355673e-05, policy loss: 16.84550041487919
Experience 27, Iter 27, disc loss: 6.866976866721795e-05, policy loss: 17.676948894390794
Experience 27, Iter 28, disc loss: 6.903229039894084e-05, policy loss: 17.894918205227164
Experience 27, Iter 29, disc loss: 7.572010290553254e-05, policy loss: 16.66917603213288
Experience 27, Iter 30, disc loss: 6.37324870845087e-05, policy loss: 16.69019721938604
Experience 27, Iter 31, disc loss: 6.943354143562593e-05, policy loss: 18.14294830041272
Experience 27, Iter 32, disc loss: 7.47230789387658e-05, policy loss: 15.522422284261742
Experience 27, Iter 33, disc loss: 7.036641667762851e-05, policy loss: 17.77250988063997
Experience 27, Iter 34, disc loss: 7.7376929619919e-05, policy loss: 15.821888645316106
Experience 27, Iter 35, disc loss: 7.278546451951163e-05, policy loss: 16.068581812293356
Experience 27, Iter 36, disc loss: 6.54726923193201e-05, policy loss: 17.532129111984794
Experience 27, Iter 37, disc loss: 6.897018271412723e-05, policy loss: 16.31178929464365
Experience 27, Iter 38, disc loss: 7.740355875338932e-05, policy loss: 17.973343579103584
Experience 27, Iter 39, disc loss: 6.899903605128311e-05, policy loss: 17.873942833117493
Experience 27, Iter 40, disc loss: 7.527485096667566e-05, policy loss: 17.98447550348398
Experience 27, Iter 41, disc loss: 9.320587818323377e-05, policy loss: 16.995508348286705
Experience 27, Iter 42, disc loss: 9.607439483049073e-05, policy loss: 17.312397421549633
Experience 27, Iter 43, disc loss: 9.53091619962009e-05, policy loss: 17.122341976127817
Experience 27, Iter 44, disc loss: 9.359252351283986e-05, policy loss: 16.151081442577382
Experience 27, Iter 45, disc loss: 0.00010767985523297211, policy loss: 15.224767619664
Experience 27, Iter 46, disc loss: 0.00011130381945906892, policy loss: 15.323197874738261
Experience 27, Iter 47, disc loss: 0.00012299505421150918, policy loss: 16.880503854925585
Experience 27, Iter 48, disc loss: 8.620750925830073e-05, policy loss: 16.57997542872855
Experience 27, Iter 49, disc loss: 0.0001196296243034385, policy loss: 15.614103341646295
Experience 27, Iter 50, disc loss: 0.0001439101952345659, policy loss: 15.055649405999574
Experience 27, Iter 51, disc loss: 0.00011369521668342712, policy loss: 17.67937984958769
Experience 27, Iter 52, disc loss: 0.00012000284068921613, policy loss: 16.799307125316773
Experience 27, Iter 53, disc loss: 0.00011776934803555783, policy loss: 15.610952905961149
Experience 27, Iter 54, disc loss: 0.00015273704852172002, policy loss: 16.289325511329565
Experience 27, Iter 55, disc loss: 0.00011379652427209402, policy loss: 16.77420403553821
Experience 27, Iter 56, disc loss: 0.00011568612597795158, policy loss: 17.210767014613424
Experience 27, Iter 57, disc loss: 0.00017778400716509282, policy loss: 16.189095027743438
Experience 27, Iter 58, disc loss: 0.00010139901592290068, policy loss: 18.446933699099752
Experience 27, Iter 59, disc loss: 0.00020146226301917384, policy loss: 15.612994106027504
Experience 27, Iter 60, disc loss: 0.0001483704556602743, policy loss: 18.181291533023714
Experience 27, Iter 61, disc loss: 0.00019559344187855445, policy loss: 15.218172143582942
Experience 27, Iter 62, disc loss: 0.00014603470181654023, policy loss: 18.068477072691806
Experience 27, Iter 63, disc loss: 0.00018840319936392296, policy loss: 16.338562890699627
Experience 27, Iter 64, disc loss: 0.0002199803259465733, policy loss: 15.264424956581928
Experience 27, Iter 65, disc loss: 0.00018720313822361717, policy loss: 17.023306699132114
Experience 27, Iter 66, disc loss: 0.00019585194225503236, policy loss: 16.10564483629705
Experience 27, Iter 67, disc loss: 0.00016305119425195084, policy loss: 16.593351081618295
Experience 27, Iter 68, disc loss: 0.00014135363745135864, policy loss: 16.46029019774698
Experience 27, Iter 69, disc loss: 0.0001054503180745341, policy loss: 16.97394332343761
Experience 27, Iter 70, disc loss: 0.00016259129874845904, policy loss: 15.737692357328184
Experience 27, Iter 71, disc loss: 0.00011157061745816751, policy loss: 17.996847892643896
Experience 27, Iter 72, disc loss: 0.00012180782843656628, policy loss: 17.40481582318197
Experience 27, Iter 73, disc loss: 9.157557608202631e-05, policy loss: 16.12942015780159
Experience 27, Iter 74, disc loss: 8.251542762668587e-05, policy loss: 19.16175235863411
Experience 27, Iter 75, disc loss: 9.13726548296677e-05, policy loss: 16.972378198389087
Experience 27, Iter 76, disc loss: 7.943559757922776e-05, policy loss: 18.17311489727311
Experience 27, Iter 77, disc loss: 8.339147770112589e-05, policy loss: 18.078933977572817
Experience 27, Iter 78, disc loss: 8.374000401874148e-05, policy loss: 18.572394875033357
Experience 27, Iter 79, disc loss: 7.953427565174722e-05, policy loss: 16.76705438596467
Experience 27, Iter 80, disc loss: 8.010174946537511e-05, policy loss: 19.130802297713117
Experience 27, Iter 81, disc loss: 7.727681566351406e-05, policy loss: 18.35135027606722
Experience 27, Iter 82, disc loss: 0.00010060882341144144, policy loss: 16.48921529716565
Experience 27, Iter 83, disc loss: 8.512283805630022e-05, policy loss: 18.938093993841573
Experience 27, Iter 84, disc loss: 9.17472222475787e-05, policy loss: 19.35658005982301
Experience 27, Iter 85, disc loss: 7.453829865926472e-05, policy loss: 16.853690508965858
Experience 27, Iter 86, disc loss: 9.171270251729732e-05, policy loss: 17.450879244836596
Experience 27, Iter 87, disc loss: 7.395048387527127e-05, policy loss: 19.116589265573793
Experience 27, Iter 88, disc loss: 7.776876090143456e-05, policy loss: 21.283175018210105
Experience 27, Iter 89, disc loss: 7.949072215828365e-05, policy loss: 19.370226490111012
Experience 27, Iter 90, disc loss: 9.120359898272457e-05, policy loss: 16.326708541292156
Experience 27, Iter 91, disc loss: 9.411095633396556e-05, policy loss: 17.580965880635127
Experience 27, Iter 92, disc loss: 0.0001210629980196565, policy loss: 18.234449679172705
Experience 27, Iter 93, disc loss: 0.00013531393334404669, policy loss: 15.773720510619743
Experience 27, Iter 94, disc loss: 0.00014914569016271137, policy loss: 17.72698646773688
Experience 27, Iter 95, disc loss: 0.00011366859812392738, policy loss: 17.360356149833414
Experience 27, Iter 96, disc loss: 9.485300129990424e-05, policy loss: 19.448305489234766
Experience 27, Iter 97, disc loss: 9.132140529838345e-05, policy loss: 17.68327644972168
Experience 27, Iter 98, disc loss: 0.00015104196122874708, policy loss: 17.529698910646097
Experience 27, Iter 99, disc loss: 0.00010367362834951899, policy loss: 19.69664920568853
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0100],
        [0.2482],
        [2.3706],
        [0.0263]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0472, 0.4100, 1.2647, 0.0306, 0.0075, 6.4310]],

        [[0.0472, 0.4100, 1.2647, 0.0306, 0.0075, 6.4310]],

        [[0.0472, 0.4100, 1.2647, 0.0306, 0.0075, 6.4310]],

        [[0.0472, 0.4100, 1.2647, 0.0306, 0.0075, 6.4310]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0401, 0.9930, 9.4825, 0.1051], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0401, 0.9930, 9.4825, 0.1051])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.912
Iter 2/2000 - Loss: 3.826
Iter 3/2000 - Loss: 3.760
Iter 4/2000 - Loss: 3.663
Iter 5/2000 - Loss: 3.600
Iter 6/2000 - Loss: 3.531
Iter 7/2000 - Loss: 3.431
Iter 8/2000 - Loss: 3.325
Iter 9/2000 - Loss: 3.220
Iter 10/2000 - Loss: 3.103
Iter 11/2000 - Loss: 2.969
Iter 12/2000 - Loss: 2.823
Iter 13/2000 - Loss: 2.666
Iter 14/2000 - Loss: 2.498
Iter 15/2000 - Loss: 2.318
Iter 16/2000 - Loss: 2.124
Iter 17/2000 - Loss: 1.915
Iter 18/2000 - Loss: 1.692
Iter 19/2000 - Loss: 1.454
Iter 20/2000 - Loss: 1.203
Iter 1981/2000 - Loss: -8.119
Iter 1982/2000 - Loss: -8.119
Iter 1983/2000 - Loss: -8.119
Iter 1984/2000 - Loss: -8.119
Iter 1985/2000 - Loss: -8.119
Iter 1986/2000 - Loss: -8.119
Iter 1987/2000 - Loss: -8.119
Iter 1988/2000 - Loss: -8.119
Iter 1989/2000 - Loss: -8.119
Iter 1990/2000 - Loss: -8.119
Iter 1991/2000 - Loss: -8.119
Iter 1992/2000 - Loss: -8.119
Iter 1993/2000 - Loss: -8.119
Iter 1994/2000 - Loss: -8.119
Iter 1995/2000 - Loss: -8.119
Iter 1996/2000 - Loss: -8.119
Iter 1997/2000 - Loss: -8.119
Iter 1998/2000 - Loss: -8.119
Iter 1999/2000 - Loss: -8.120
Iter 2000/2000 - Loss: -8.120
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[16.8542, 18.1413, 28.8323,  8.1959,  6.4134, 63.8103]],

        [[24.7724, 45.4495,  8.5489,  1.0852,  1.9568, 17.9844]],

        [[27.3074, 46.7529,  9.2162,  0.8505,  1.2354, 21.7107]],

        [[20.3589, 39.8147, 12.9579,  1.7720,  1.7070, 44.1260]]])
Signal Variance: tensor([ 0.3591,  1.2672, 12.2191,  0.3827])
Estimated target variance: tensor([0.0401, 0.9930, 9.4825, 0.1051])
N: 280
Signal to noise ratio: tensor([36.2317, 62.8980, 88.8724, 43.0405])
Bound on condition number: tensor([ 367566.7445, 1107727.0939, 2211525.4277,  518697.8878])
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 9.401993898355272e-05, policy loss: 20.009111274118446
Experience 28, Iter 1, disc loss: 0.00010815695867702401, policy loss: 19.21911651473681
Experience 28, Iter 2, disc loss: 0.0001289415745279792, policy loss: 19.436098077426184
Experience 28, Iter 3, disc loss: 0.00011822201509081688, policy loss: 19.965024601987857
Experience 28, Iter 4, disc loss: 0.0001037614039151607, policy loss: 20.731315152505914
Experience 28, Iter 5, disc loss: 0.00013787450413358494, policy loss: 17.033823035324517
Experience 28, Iter 6, disc loss: 0.0006336484068783519, policy loss: 19.52615843183365
Experience 28, Iter 7, disc loss: 0.00015800910784255346, policy loss: 18.829014296058226
Experience 28, Iter 8, disc loss: 0.00011457871760413399, policy loss: 19.465519259532584
Experience 28, Iter 9, disc loss: 0.00015592860734645134, policy loss: 17.595645498277896
Experience 28, Iter 10, disc loss: 0.00013114770644197563, policy loss: 16.129814192597962
Experience 28, Iter 11, disc loss: 0.00012656930637814466, policy loss: 18.190983499425123
Experience 28, Iter 12, disc loss: 0.00011849424035122286, policy loss: 17.97205871914762
Experience 28, Iter 13, disc loss: 0.00014408117097987783, policy loss: 16.56332045717378
Experience 28, Iter 14, disc loss: 0.00015753577837333968, policy loss: 17.066602547364248
Experience 28, Iter 15, disc loss: 0.0001401455610404402, policy loss: 16.8459235666209
Experience 28, Iter 16, disc loss: 0.00010805030333662316, policy loss: 18.00769718279415
Experience 28, Iter 17, disc loss: 0.00012622571998308952, policy loss: 19.579325412345522
Experience 28, Iter 18, disc loss: 7.174498685979306e-05, policy loss: 21.021341439128268
Experience 28, Iter 19, disc loss: 6.682079748771976e-05, policy loss: 20.03449139946929
Experience 28, Iter 20, disc loss: 7.201434036494087e-05, policy loss: 17.18378781709903
Experience 28, Iter 21, disc loss: 7.064836364049595e-05, policy loss: 19.62321406757752
Experience 28, Iter 22, disc loss: 9.811725945258224e-05, policy loss: 16.22766397093647
Experience 28, Iter 23, disc loss: 0.00010204823855380712, policy loss: 17.162317632701843
Experience 28, Iter 24, disc loss: 0.00011207899997595047, policy loss: 16.695082010829402
Experience 28, Iter 25, disc loss: 7.628268609691864e-05, policy loss: 18.820978819571906
Experience 28, Iter 26, disc loss: 0.00010646342759135958, policy loss: 16.859524650930034
Experience 28, Iter 27, disc loss: 0.00011126529219188248, policy loss: 16.89514256216187
Experience 28, Iter 28, disc loss: 9.197846175702699e-05, policy loss: 17.404287577643665
Experience 28, Iter 29, disc loss: 9.737892738677396e-05, policy loss: 15.200942688553695
Experience 28, Iter 30, disc loss: 0.00010060207681654199, policy loss: 16.279794706968975
Experience 28, Iter 31, disc loss: 0.00011815177061859788, policy loss: 17.064999212263494
Experience 28, Iter 32, disc loss: 8.22424253888122e-05, policy loss: 17.58217437550931
Experience 28, Iter 33, disc loss: 9.118088786007549e-05, policy loss: 17.016212049825413
Experience 28, Iter 34, disc loss: 0.00010775705113943698, policy loss: 17.058952460281446
Experience 28, Iter 35, disc loss: 9.301924954610684e-05, policy loss: 18.138212936245573
Experience 28, Iter 36, disc loss: 0.00010829430535532007, policy loss: 16.43327839825601
Experience 28, Iter 37, disc loss: 9.444243924205977e-05, policy loss: 18.17206321369835
Experience 28, Iter 38, disc loss: 7.944840782948263e-05, policy loss: 16.34474503118654
Experience 28, Iter 39, disc loss: 8.968586267603497e-05, policy loss: 17.162866555489025
Experience 28, Iter 40, disc loss: 7.235439713364772e-05, policy loss: 18.658628198185603
Experience 28, Iter 41, disc loss: 9.232568567863749e-05, policy loss: 17.986899844783622
Experience 28, Iter 42, disc loss: 8.852808524198634e-05, policy loss: 17.624105583681626
Experience 28, Iter 43, disc loss: 9.656906458289726e-05, policy loss: 18.77867618916886
Experience 28, Iter 44, disc loss: 7.911266399461957e-05, policy loss: 16.406548128407803
Experience 28, Iter 45, disc loss: 7.869442806398153e-05, policy loss: 17.249122992065345
Experience 28, Iter 46, disc loss: 9.8457765223555e-05, policy loss: 18.300512036526033
Experience 28, Iter 47, disc loss: 0.00011785673805561934, policy loss: 17.604555223416682
Experience 28, Iter 48, disc loss: 0.0001359661937771747, policy loss: 15.431478883545624
Experience 28, Iter 49, disc loss: 0.00011293020775933395, policy loss: 17.768394652417836
Experience 28, Iter 50, disc loss: 8.968667957240555e-05, policy loss: 17.111714277467634
Experience 28, Iter 51, disc loss: 8.752382011698831e-05, policy loss: 17.850319635740185
Experience 28, Iter 52, disc loss: 0.0001053671910013219, policy loss: 17.697128569627754
Experience 28, Iter 53, disc loss: 0.0001550754391128334, policy loss: 17.49675389369731
Experience 28, Iter 54, disc loss: 0.0001696992455904718, policy loss: 15.085320099068158
Experience 28, Iter 55, disc loss: 0.00011223078169034037, policy loss: 16.297533377683827
Experience 28, Iter 56, disc loss: 0.00015944099934604733, policy loss: 15.544061848490747
Experience 28, Iter 57, disc loss: 0.00020454615200363342, policy loss: 14.871382815474316
Experience 28, Iter 58, disc loss: 0.00013282589578752688, policy loss: 16.257099069827888
Experience 28, Iter 59, disc loss: 0.00013225968380621935, policy loss: 16.189920238756386
Experience 28, Iter 60, disc loss: 0.00017128867987604044, policy loss: 15.487951732906852
Experience 28, Iter 61, disc loss: 0.0001209117693357822, policy loss: 16.38236467679708
Experience 28, Iter 62, disc loss: 0.00015530972176295874, policy loss: 16.169739965207278
Experience 28, Iter 63, disc loss: 0.0002662331234734851, policy loss: 15.885773710130543
Experience 28, Iter 64, disc loss: 0.00020327431666536952, policy loss: 15.639292711909292
Experience 28, Iter 65, disc loss: 0.002565751111109576, policy loss: 16.22150866494723
Experience 28, Iter 66, disc loss: 0.006511573921354787, policy loss: 14.879356393413143
Experience 28, Iter 67, disc loss: 0.0005004100080230843, policy loss: 15.506710499159823
Experience 28, Iter 68, disc loss: 0.012099809932505118, policy loss: 14.655940092351258
Experience 28, Iter 69, disc loss: 0.027120464760545426, policy loss: 15.537559919557966
Experience 28, Iter 70, disc loss: 0.00031669254551408094, policy loss: 15.26653448342799
Experience 28, Iter 71, disc loss: 0.03949335217284155, policy loss: 16.17466382299366
Experience 28, Iter 72, disc loss: 0.00986167768563565, policy loss: 15.447693926876461
Experience 28, Iter 73, disc loss: 0.00021132963360006487, policy loss: 16.393676082728298
Experience 28, Iter 74, disc loss: 0.00016943369898550818, policy loss: 15.72086598545615
Experience 28, Iter 75, disc loss: 0.00015788500172329053, policy loss: 16.726241817939513
Experience 28, Iter 76, disc loss: 0.00015493339694562644, policy loss: 16.489602001629347
Experience 28, Iter 77, disc loss: 0.00017139451801051798, policy loss: 17.905253032226774
Experience 28, Iter 78, disc loss: 0.00020001800441524038, policy loss: 17.091065166504194
Experience 28, Iter 79, disc loss: 0.00020881631916781524, policy loss: 15.872021939160128
Experience 28, Iter 80, disc loss: 0.00022723398344513007, policy loss: 17.213779666285948
Experience 28, Iter 81, disc loss: 0.0002461754676775023, policy loss: 16.73263917614554
Experience 28, Iter 82, disc loss: 0.00026447002646736573, policy loss: 16.431057655932555
Experience 28, Iter 83, disc loss: 0.00028159125468769875, policy loss: 17.823697664791997
Experience 28, Iter 84, disc loss: 0.0002978639645271447, policy loss: 16.73097029503738
Experience 28, Iter 85, disc loss: 0.0003121469916336996, policy loss: 17.158998358221012
Experience 28, Iter 86, disc loss: 0.00032466388560256865, policy loss: 19.960336995620548
Experience 28, Iter 87, disc loss: 0.00033664930744678543, policy loss: 17.52547098341572
Experience 28, Iter 88, disc loss: 0.000346444377485875, policy loss: 17.218668955995827
Experience 28, Iter 89, disc loss: 0.00035461258941551853, policy loss: 17.59605001138028
Experience 28, Iter 90, disc loss: 0.00036107433461461864, policy loss: 19.066475368559146
Experience 28, Iter 91, disc loss: 0.000366182782183628, policy loss: 18.04418507540483
Experience 28, Iter 92, disc loss: 0.000369997077468377, policy loss: 19.795536145699707
Experience 28, Iter 93, disc loss: 0.0003726509311756855, policy loss: 19.26089733798342
Experience 28, Iter 94, disc loss: 0.00037385762375641095, policy loss: 18.57666548004205
Experience 28, Iter 95, disc loss: 0.0003741161737778442, policy loss: 19.041270959323004
Experience 28, Iter 96, disc loss: 0.00038120624535770424, policy loss: 18.72494681509597
Experience 28, Iter 97, disc loss: 0.00037172760098918157, policy loss: 18.00256327699219
Experience 28, Iter 98, disc loss: 0.00038667954723190727, policy loss: 17.29323783592142
Experience 28, Iter 99, disc loss: 0.0003659523538860618, policy loss: 18.163773165871035
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0109],
        [0.2496],
        [2.3674],
        [0.0265]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0496, 0.4410, 1.2748, 0.0318, 0.0078, 6.5496]],

        [[0.0496, 0.4410, 1.2748, 0.0318, 0.0078, 6.5496]],

        [[0.0496, 0.4410, 1.2748, 0.0318, 0.0078, 6.5496]],

        [[0.0496, 0.4410, 1.2748, 0.0318, 0.0078, 6.5496]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0437, 0.9983, 9.4696, 0.1061], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0437, 0.9983, 9.4696, 0.1061])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.930
Iter 2/2000 - Loss: 3.859
Iter 3/2000 - Loss: 3.771
Iter 4/2000 - Loss: 3.678
Iter 5/2000 - Loss: 3.618
Iter 6/2000 - Loss: 3.532
Iter 7/2000 - Loss: 3.424
Iter 8/2000 - Loss: 3.319
Iter 9/2000 - Loss: 3.210
Iter 10/2000 - Loss: 3.082
Iter 11/2000 - Loss: 2.939
Iter 12/2000 - Loss: 2.787
Iter 13/2000 - Loss: 2.628
Iter 14/2000 - Loss: 2.457
Iter 15/2000 - Loss: 2.271
Iter 16/2000 - Loss: 2.069
Iter 17/2000 - Loss: 1.853
Iter 18/2000 - Loss: 1.624
Iter 19/2000 - Loss: 1.381
Iter 20/2000 - Loss: 1.124
Iter 1981/2000 - Loss: -8.142
Iter 1982/2000 - Loss: -8.142
Iter 1983/2000 - Loss: -8.142
Iter 1984/2000 - Loss: -8.142
Iter 1985/2000 - Loss: -8.142
Iter 1986/2000 - Loss: -8.142
Iter 1987/2000 - Loss: -8.142
Iter 1988/2000 - Loss: -8.142
Iter 1989/2000 - Loss: -8.142
Iter 1990/2000 - Loss: -8.142
Iter 1991/2000 - Loss: -8.142
Iter 1992/2000 - Loss: -8.142
Iter 1993/2000 - Loss: -8.142
Iter 1994/2000 - Loss: -8.142
Iter 1995/2000 - Loss: -8.143
Iter 1996/2000 - Loss: -8.143
Iter 1997/2000 - Loss: -8.143
Iter 1998/2000 - Loss: -8.143
Iter 1999/2000 - Loss: -8.143
Iter 2000/2000 - Loss: -8.143
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[16.3265, 17.7354, 27.3095,  8.4712,  6.2362, 64.3196]],

        [[24.5845, 45.7145,  8.6742,  1.0819,  1.9014, 18.4118]],

        [[27.3025, 47.6596,  9.2374,  0.8296,  1.1681, 21.9438]],

        [[19.8880, 39.9267, 12.8745,  1.8877,  1.6036, 43.7519]]])
Signal Variance: tensor([ 0.3503,  1.3019, 11.9354,  0.3818])
Estimated target variance: tensor([0.0437, 0.9983, 9.4696, 0.1061])
N: 290
Signal to noise ratio: tensor([35.6944, 62.6658, 89.1532, 42.3168])
Bound on condition number: tensor([ 369488.1427, 1138832.9406, 2305004.8010,  519308.4437])
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.00036415113506955277, policy loss: 18.22205974366296
Experience 29, Iter 1, disc loss: 0.00035781128925815663, policy loss: 21.253704331289153
Experience 29, Iter 2, disc loss: 0.0003529538052073208, policy loss: 19.39770890695867
Experience 29, Iter 3, disc loss: 0.0003481617033002269, policy loss: 19.367206330191273
Experience 29, Iter 4, disc loss: 0.000342810798718494, policy loss: 18.82501952889081
Experience 29, Iter 5, disc loss: 0.00033728369600958896, policy loss: 19.573124565776
Experience 29, Iter 6, disc loss: 0.0003317674319659704, policy loss: 19.449478277323422
Experience 29, Iter 7, disc loss: 0.000325760666428373, policy loss: 17.980428746510313
Experience 29, Iter 8, disc loss: 0.0003196830575472939, policy loss: 18.287389637093572
Experience 29, Iter 9, disc loss: 0.00031414863657755937, policy loss: 17.203893861738194
Experience 29, Iter 10, disc loss: 0.0003122726712637273, policy loss: 18.98848849539464
Experience 29, Iter 11, disc loss: 0.0003019358585747919, policy loss: 19.741136779448514
Experience 29, Iter 12, disc loss: 0.00029617873918334184, policy loss: 18.21419995139426
Experience 29, Iter 13, disc loss: 0.0002903877605653269, policy loss: 17.493476082055007
Experience 29, Iter 14, disc loss: 0.00028459728273574246, policy loss: 17.725087928638317
Experience 29, Iter 15, disc loss: 0.0003157910805422872, policy loss: 16.86815771947979
Experience 29, Iter 16, disc loss: 0.00027347367851847924, policy loss: 19.190775678783105
Experience 29, Iter 17, disc loss: 0.00026837159084892924, policy loss: 16.815175199164056
Experience 29, Iter 18, disc loss: 0.0002631324732430846, policy loss: 18.45918834616404
Experience 29, Iter 19, disc loss: 0.00025797559344330217, policy loss: 18.09722648039762
Experience 29, Iter 20, disc loss: 0.0002532014137016108, policy loss: 17.469948967220276
Experience 29, Iter 21, disc loss: 0.0002481200242897784, policy loss: 18.92989329808077
Experience 29, Iter 22, disc loss: 0.00024356702422447717, policy loss: 18.161897100312473
Experience 29, Iter 23, disc loss: 0.00023878956569905132, policy loss: 17.85415714176422
Experience 29, Iter 24, disc loss: 0.00023446189440786837, policy loss: 18.762583080529293
Experience 29, Iter 25, disc loss: 0.0002299559683777307, policy loss: 19.82062634258328
Experience 29, Iter 26, disc loss: 0.0002259998966970505, policy loss: 18.292021752022336
Experience 29, Iter 27, disc loss: 0.00022177419997239552, policy loss: 17.785712381229573
Experience 29, Iter 28, disc loss: 0.0009035504735321454, policy loss: 17.078689565185595
Experience 29, Iter 29, disc loss: 0.00021534049253640007, policy loss: 17.117544779458473
Experience 29, Iter 30, disc loss: 0.00021111004021794226, policy loss: 19.95526385423291
Experience 29, Iter 31, disc loss: 0.0003896336561926703, policy loss: 18.347733032764523
Experience 29, Iter 32, disc loss: 0.0002050500357287186, policy loss: 17.66162586522399
Experience 29, Iter 33, disc loss: 0.0002019963939626391, policy loss: 17.460824757863577
Experience 29, Iter 34, disc loss: 0.00019951896062594696, policy loss: 17.74448471329454
Experience 29, Iter 35, disc loss: 0.00019638939381131845, policy loss: 16.47902207018241
Experience 29, Iter 36, disc loss: 0.00019376297426704052, policy loss: 19.359780337029875
Experience 29, Iter 37, disc loss: 0.00019113151028980977, policy loss: 17.090538112759088
Experience 29, Iter 38, disc loss: 0.00018801076229253211, policy loss: 19.151597278493444
Experience 29, Iter 39, disc loss: 0.00018528936685482944, policy loss: 18.199096552199094
Experience 29, Iter 40, disc loss: 0.00018322083014493936, policy loss: 17.710953819866788
Experience 29, Iter 41, disc loss: 0.00018100331632381095, policy loss: 17.57958515207789
Experience 29, Iter 42, disc loss: 0.00017773142514838295, policy loss: 17.936275429468672
Experience 29, Iter 43, disc loss: 0.00017563069433432178, policy loss: 18.106263896016454
Experience 29, Iter 44, disc loss: 0.00017274740241376768, policy loss: 16.733121338159098
Experience 29, Iter 45, disc loss: 0.00017150862471508732, policy loss: 17.23272125917444
Experience 29, Iter 46, disc loss: 0.00016900523671309757, policy loss: 17.40462533419701
Experience 29, Iter 47, disc loss: 0.0001664996819151606, policy loss: 16.077015654188152
Experience 29, Iter 48, disc loss: 0.00016489883482134255, policy loss: 16.96005268216199
Experience 29, Iter 49, disc loss: 0.00016226650779529596, policy loss: 17.74675920751442
Experience 29, Iter 50, disc loss: 0.00015982504315052828, policy loss: 17.9481684786372
Experience 29, Iter 51, disc loss: 0.0001576682290941336, policy loss: 19.080893129657237
Experience 29, Iter 52, disc loss: 0.00015632487981044325, policy loss: 18.561120571496446
Experience 29, Iter 53, disc loss: 0.00015447703064617693, policy loss: 17.76722853171988
Experience 29, Iter 54, disc loss: 0.00015302918670577663, policy loss: 17.65212010122349
Experience 29, Iter 55, disc loss: 0.0001499041351297843, policy loss: 18.93268215054126
Experience 29, Iter 56, disc loss: 0.0001489394846252821, policy loss: 18.25615302147023
Experience 29, Iter 57, disc loss: 0.00014758148457463474, policy loss: 17.641813117793312
Experience 29, Iter 58, disc loss: 0.0001470642074760454, policy loss: 16.690069750109867
Experience 29, Iter 59, disc loss: 0.00014448117674333802, policy loss: 18.197090495386576
Experience 29, Iter 60, disc loss: 0.00014238765487558763, policy loss: 18.71830618423393
Experience 29, Iter 61, disc loss: 0.00014309529818651474, policy loss: 15.290498390637763
Experience 29, Iter 62, disc loss: 0.0001405195400285745, policy loss: 18.433218974029568
Experience 29, Iter 63, disc loss: 0.00013933367748199572, policy loss: 18.481760891471577
Experience 29, Iter 64, disc loss: 0.00014506555522269214, policy loss: 18.225690029339404
Experience 29, Iter 65, disc loss: 0.00013718330117780156, policy loss: 17.10031040848901
Experience 29, Iter 66, disc loss: 0.0001350250664413831, policy loss: 17.85631728758805
Experience 29, Iter 67, disc loss: 0.00013390475376720304, policy loss: 17.465185788328974
Experience 29, Iter 68, disc loss: 0.00013225857346859936, policy loss: 16.454782245391588
Experience 29, Iter 69, disc loss: 0.00013247838177084478, policy loss: 18.588377452654427
Experience 29, Iter 70, disc loss: 0.0001313421275235482, policy loss: 17.15495780717225
Experience 29, Iter 71, disc loss: 0.0001314971003293923, policy loss: 17.460635313141474
Experience 29, Iter 72, disc loss: 0.00012863276248271268, policy loss: 17.696224321057105
Experience 29, Iter 73, disc loss: 0.00012638801589125577, policy loss: 17.098354890840326
Experience 29, Iter 74, disc loss: 0.00012614786828775784, policy loss: 15.071710182235433
Experience 29, Iter 75, disc loss: 0.00012479500751193603, policy loss: 18.82508220350341
Experience 29, Iter 76, disc loss: 0.00012484331943750073, policy loss: 16.81528440665891
Experience 29, Iter 77, disc loss: 0.00012409518826485838, policy loss: 16.675302423786153
Experience 29, Iter 78, disc loss: 0.00012606446477064263, policy loss: 15.405332028516101
Experience 29, Iter 79, disc loss: 0.00012246968404603677, policy loss: 16.018714902170444
Experience 29, Iter 80, disc loss: 0.00011900218202893866, policy loss: 17.55353669815235
Experience 29, Iter 81, disc loss: 0.00011877527570346859, policy loss: 19.16957211429169
Experience 29, Iter 82, disc loss: 0.00012156480398948705, policy loss: 16.560627316620835
Experience 29, Iter 83, disc loss: 0.0001225242593455306, policy loss: 15.78536441985856
Experience 29, Iter 84, disc loss: 0.00012061514158827727, policy loss: 14.989150417280008
Experience 29, Iter 85, disc loss: 0.00011796943521856744, policy loss: 18.2782028862826
Experience 29, Iter 86, disc loss: 0.0001222933138360748, policy loss: 15.34486487748617
Experience 29, Iter 87, disc loss: 0.00011742501115159046, policy loss: 17.052046657879224
Experience 29, Iter 88, disc loss: 0.00011468691263221751, policy loss: 17.12142390099426
Experience 29, Iter 89, disc loss: 0.00011670378924301011, policy loss: 16.42683114817153
Experience 29, Iter 90, disc loss: 0.00011553832196976619, policy loss: 16.611148016192985
Experience 29, Iter 91, disc loss: 0.0001198522000128576, policy loss: 16.29373782912902
Experience 29, Iter 92, disc loss: 0.00011665982361983, policy loss: 16.29545141703039
Experience 29, Iter 93, disc loss: 0.00014706167739691344, policy loss: 17.686401416725342
Experience 29, Iter 94, disc loss: 0.00011405942330517524, policy loss: 17.227746073757345
Experience 29, Iter 95, disc loss: 0.00011444872247332831, policy loss: 16.577370896578504
Experience 29, Iter 96, disc loss: 0.00011604913003830388, policy loss: 16.630588283374315
Experience 29, Iter 97, disc loss: 0.00010985656770869264, policy loss: 17.674278311355966
Experience 29, Iter 98, disc loss: 0.00011269817313717675, policy loss: 16.627680604906303
Experience 29, Iter 99, disc loss: 0.00011331522559792514, policy loss: 16.545989090504726
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0113],
        [0.2493],
        [2.3451],
        [0.0262]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0510, 0.4549, 1.2582, 0.0323, 0.0077, 6.5926]],

        [[0.0510, 0.4549, 1.2582, 0.0323, 0.0077, 6.5926]],

        [[0.0510, 0.4549, 1.2582, 0.0323, 0.0077, 6.5926]],

        [[0.0510, 0.4549, 1.2582, 0.0323, 0.0077, 6.5926]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0453, 0.9974, 9.3804, 0.1047], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0453, 0.9974, 9.3804, 0.1047])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.919
Iter 2/2000 - Loss: 3.859
Iter 3/2000 - Loss: 3.756
Iter 4/2000 - Loss: 3.668
Iter 5/2000 - Loss: 3.605
Iter 6/2000 - Loss: 3.511
Iter 7/2000 - Loss: 3.401
Iter 8/2000 - Loss: 3.296
Iter 9/2000 - Loss: 3.185
Iter 10/2000 - Loss: 3.052
Iter 11/2000 - Loss: 2.905
Iter 12/2000 - Loss: 2.752
Iter 13/2000 - Loss: 2.591
Iter 14/2000 - Loss: 2.417
Iter 15/2000 - Loss: 2.228
Iter 16/2000 - Loss: 2.023
Iter 17/2000 - Loss: 1.804
Iter 18/2000 - Loss: 1.572
Iter 19/2000 - Loss: 1.326
Iter 20/2000 - Loss: 1.065
Iter 1981/2000 - Loss: -8.157
Iter 1982/2000 - Loss: -8.157
Iter 1983/2000 - Loss: -8.157
Iter 1984/2000 - Loss: -8.157
Iter 1985/2000 - Loss: -8.157
Iter 1986/2000 - Loss: -8.158
Iter 1987/2000 - Loss: -8.158
Iter 1988/2000 - Loss: -8.158
Iter 1989/2000 - Loss: -8.158
Iter 1990/2000 - Loss: -8.158
Iter 1991/2000 - Loss: -8.158
Iter 1992/2000 - Loss: -8.158
Iter 1993/2000 - Loss: -8.158
Iter 1994/2000 - Loss: -8.158
Iter 1995/2000 - Loss: -8.158
Iter 1996/2000 - Loss: -8.158
Iter 1997/2000 - Loss: -8.158
Iter 1998/2000 - Loss: -8.158
Iter 1999/2000 - Loss: -8.158
Iter 2000/2000 - Loss: -8.158
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[16.2731, 18.0102, 27.3080,  9.2598,  6.1928, 64.7676]],

        [[24.6477, 46.1094,  8.6911,  1.0784,  1.9289, 18.9154]],

        [[27.5317, 47.6171,  9.3464,  0.8224,  1.1902, 22.4940]],

        [[19.9968, 39.7756, 13.0269,  1.5953,  1.7973, 45.2238]]])
Signal Variance: tensor([ 0.3642,  1.3278, 12.2364,  0.3862])
Estimated target variance: tensor([0.0453, 0.9974, 9.3804, 0.1047])
N: 300
Signal to noise ratio: tensor([35.7512, 63.3586, 89.4418, 41.9006])
Bound on condition number: tensor([ 383444.8352, 1204296.4144, 2399952.0513,  526698.0572])
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.0001171124140413622, policy loss: 18.04746419460625
Experience 30, Iter 1, disc loss: 0.00012138224636577336, policy loss: 16.072814491987607
Experience 30, Iter 2, disc loss: 0.00011221553203921372, policy loss: 15.064844404540445
Experience 30, Iter 3, disc loss: 0.00011435244963703564, policy loss: 15.006649073847125
Experience 30, Iter 4, disc loss: 0.0001257468379070539, policy loss: 17.613859638442854
Experience 30, Iter 5, disc loss: 0.00012618695094423148, policy loss: 15.445108791067753
Experience 30, Iter 6, disc loss: 0.00012685971808766463, policy loss: 16.695221350332176
Experience 30, Iter 7, disc loss: 0.00011704432863218178, policy loss: 16.237436564424897
Experience 30, Iter 8, disc loss: 0.000115031718509866, policy loss: 15.080921566062099
Experience 30, Iter 9, disc loss: 0.0001146871480373436, policy loss: 16.829346304641227
Experience 30, Iter 10, disc loss: 0.00011336816227634273, policy loss: 16.620179956899225
Experience 30, Iter 11, disc loss: 0.00011411163493536543, policy loss: 16.194228395503444
Experience 30, Iter 12, disc loss: 0.0001190516500229152, policy loss: 15.182418237870902
Experience 30, Iter 13, disc loss: 0.00010746174101541824, policy loss: 18.218341112211366
Experience 30, Iter 14, disc loss: 0.0001118122612748521, policy loss: 16.264548530232354
