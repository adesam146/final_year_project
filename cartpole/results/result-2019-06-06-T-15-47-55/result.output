Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0210],
        [0.0147],
        [1.0646],
        [0.0211]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.4105, 0.9102, 0.9909, 0.0166, 0.0083, 0.4054]],

        [[0.4105, 0.9102, 0.9909, 0.0166, 0.0083, 0.4054]],

        [[0.4105, 0.9102, 0.9909, 0.0166, 0.0083, 0.4054]],

        [[0.4105, 0.9102, 0.9909, 0.0166, 0.0083, 0.4054]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0840, 0.0587, 4.2584, 0.0845], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0840, 0.0587, 4.2584, 0.0845])
N: 15
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([61.0000, 61.0000, 61.0000, 61.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 6.446
Iter 2/2000 - Loss: 4.942
Iter 3/2000 - Loss: 3.967
Iter 4/2000 - Loss: 3.325
Iter 5/2000 - Loss: 2.921
Iter 6/2000 - Loss: 2.713
Iter 7/2000 - Loss: 2.648
Iter 8/2000 - Loss: 2.657
Iter 9/2000 - Loss: 2.695
Iter 10/2000 - Loss: 2.749
Iter 11/2000 - Loss: 2.808
Iter 12/2000 - Loss: 2.858
Iter 13/2000 - Loss: 2.887
Iter 14/2000 - Loss: 2.889
Iter 15/2000 - Loss: 2.868
Iter 16/2000 - Loss: 2.833
Iter 17/2000 - Loss: 2.791
Iter 18/2000 - Loss: 2.746
Iter 19/2000 - Loss: 2.699
Iter 20/2000 - Loss: 2.651
Iter 1981/2000 - Loss: -3.763
Iter 1982/2000 - Loss: -3.763
Iter 1983/2000 - Loss: -3.763
Iter 1984/2000 - Loss: -3.763
Iter 1985/2000 - Loss: -3.763
Iter 1986/2000 - Loss: -3.763
Iter 1987/2000 - Loss: -3.763
Iter 1988/2000 - Loss: -3.763
Iter 1989/2000 - Loss: -3.763
Iter 1990/2000 - Loss: -3.763
Iter 1991/2000 - Loss: -3.764
Iter 1992/2000 - Loss: -3.764
Iter 1993/2000 - Loss: -3.764
Iter 1994/2000 - Loss: -3.764
Iter 1995/2000 - Loss: -3.764
Iter 1996/2000 - Loss: -3.764
Iter 1997/2000 - Loss: -3.764
Iter 1998/2000 - Loss: -3.764
Iter 1999/2000 - Loss: -3.764
Iter 2000/2000 - Loss: -3.764
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0001],
        [0.0001],
        [0.0002]])
Lengthscale: tensor([[[ 9.4596,  9.0064, 63.9841, 29.2718, 20.3584, 40.8099]],

        [[60.1999, 66.6986, 10.4731,  0.9597,  0.7273,  4.9090]],

        [[61.5968, 68.7483, 10.0674,  0.7799, 12.4920, 11.8961]],

        [[65.6424, 61.9688, 10.5559,  4.4254,  3.6495, 60.6083]]])
Signal Variance: tensor([0.1523, 0.1381, 8.2208, 0.3079])
Estimated target variance: tensor([0.0840, 0.0587, 4.2584, 0.0845])
N: 15
Signal to noise ratio: tensor([ 18.4536,  37.0810, 283.7465,  38.0734])
Bound on condition number: tensor([   5109.0335,   20626.0177, 1207681.7318,   21744.8078])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.3385634755800004, policy loss: 0.6352250791518386
Experience 1, Iter 1, disc loss: 1.285972508345365, policy loss: 0.6833450057999416
Experience 1, Iter 2, disc loss: 1.2394924603643973, policy loss: 0.7268740609762896
Experience 1, Iter 3, disc loss: 1.208282896324015, policy loss: 0.7558884531178286
Experience 1, Iter 4, disc loss: 1.1892136054255755, policy loss: 0.7692530048605928
Experience 1, Iter 5, disc loss: 1.1740221072414432, policy loss: 0.7809358460128372
Experience 1, Iter 6, disc loss: 1.1732753935195834, policy loss: 0.7723633757087814
Experience 1, Iter 7, disc loss: 1.1932851360353147, policy loss: 0.7392549074515412
Experience 1, Iter 8, disc loss: 1.2374919136948836, policy loss: 0.6811681855842635
Experience 1, Iter 9, disc loss: 1.2195006614976145, policy loss: 0.6900045190998872
Experience 1, Iter 10, disc loss: 1.1803501190718395, policy loss: 0.7268165082178222
Experience 1, Iter 11, disc loss: 1.1472934967047794, policy loss: 0.7577120399181339
Experience 1, Iter 12, disc loss: 1.1253930124197036, policy loss: 0.7818103656676083
Experience 1, Iter 13, disc loss: 1.0876747420001966, policy loss: 0.8251713585871066
Experience 1, Iter 14, disc loss: 1.0373952074148078, policy loss: 0.8832613084579004
Experience 1, Iter 15, disc loss: 1.0329646388665974, policy loss: 0.8852678186902498
Experience 1, Iter 16, disc loss: 0.9853131716894193, policy loss: 0.9456161509183493
Experience 1, Iter 17, disc loss: 0.9948681168395773, policy loss: 0.9375864506826482
Experience 1, Iter 18, disc loss: 0.9206748237962873, policy loss: 1.042640797064064
Experience 1, Iter 19, disc loss: 0.9075523984251168, policy loss: 1.0505162067505835
Experience 1, Iter 20, disc loss: 0.8989338160829003, policy loss: 1.0675488159623683
Experience 1, Iter 21, disc loss: 0.9011370076143419, policy loss: 1.0678453805699968
Experience 1, Iter 22, disc loss: 0.8768775282424881, policy loss: 1.0980282192731647
Experience 1, Iter 23, disc loss: 0.8752921400749314, policy loss: 1.0749585212394175
Experience 1, Iter 24, disc loss: 0.8549385727511507, policy loss: 1.1105631234109716
Experience 1, Iter 25, disc loss: 0.8580490689408087, policy loss: 1.078778535012645
Experience 1, Iter 26, disc loss: 0.8501248385360689, policy loss: 1.0997777974681857
Experience 1, Iter 27, disc loss: 0.821130025252317, policy loss: 1.1169359928878064
Experience 1, Iter 28, disc loss: 0.7678793938100144, policy loss: 1.2288985591648884
Experience 1, Iter 29, disc loss: 0.757721182990303, policy loss: 1.2353530113643338
Experience 1, Iter 30, disc loss: 0.7561451543709279, policy loss: 1.2453312794453957
Experience 1, Iter 31, disc loss: 0.7198208050139845, policy loss: 1.3104811455137164
Experience 1, Iter 32, disc loss: 0.6883744997790069, policy loss: 1.3867760600997763
Experience 1, Iter 33, disc loss: 0.6955335241427703, policy loss: 1.3726979500586691
Experience 1, Iter 34, disc loss: 0.6164554158083462, policy loss: 1.5683664519456642
Experience 1, Iter 35, disc loss: 0.6199750485541335, policy loss: 1.5559981059910695
Experience 1, Iter 36, disc loss: 0.600168554497214, policy loss: 1.6095289599698994
Experience 1, Iter 37, disc loss: 0.5601326172161648, policy loss: 1.7535023313392017
Experience 1, Iter 38, disc loss: 0.5361841079173908, policy loss: 1.786864474573722
Experience 1, Iter 39, disc loss: 0.5044221466608239, policy loss: 1.880326487368293
Experience 1, Iter 40, disc loss: 0.4974067151938587, policy loss: 1.9189610976003217
Experience 1, Iter 41, disc loss: 0.48628223486929056, policy loss: 1.9913348567851945
Experience 1, Iter 42, disc loss: 0.48614699460886557, policy loss: 1.890549645206224
Experience 1, Iter 43, disc loss: 0.44748245648143015, policy loss: 1.99108926132111
Experience 1, Iter 44, disc loss: 0.43017000383686305, policy loss: 2.0043229875054074
Experience 1, Iter 45, disc loss: 0.4286552563518441, policy loss: 2.1666897994381684
Experience 1, Iter 46, disc loss: 0.39591742650096273, policy loss: 2.2292282434222126
Experience 1, Iter 47, disc loss: 0.4138336841270097, policy loss: 2.1781183454710678
Experience 1, Iter 48, disc loss: 0.39636422040266417, policy loss: 2.20402371612995
Experience 1, Iter 49, disc loss: 0.34154834671674195, policy loss: 2.356711123193708
Experience 1, Iter 50, disc loss: 0.3238708696543879, policy loss: 2.5023739660721684
Experience 1, Iter 51, disc loss: 0.3479800978773079, policy loss: 2.3605484699494337
Experience 1, Iter 52, disc loss: 0.33262700950562424, policy loss: 2.4027928515561348
Experience 1, Iter 53, disc loss: 0.29734323037439886, policy loss: 2.6187590360776447
Experience 1, Iter 54, disc loss: 0.33310204724836323, policy loss: 2.4684461122597323
Experience 1, Iter 55, disc loss: 0.3166232524264825, policy loss: 2.4409292805412974
Experience 1, Iter 56, disc loss: 0.29346926933741635, policy loss: 2.666705943955386
Experience 1, Iter 57, disc loss: 0.2513444702068881, policy loss: 2.869774419756915
Experience 1, Iter 58, disc loss: 0.24171341432308416, policy loss: 2.8007317270554406
Experience 1, Iter 59, disc loss: 0.24424005981751043, policy loss: 2.8606185553439225
Experience 1, Iter 60, disc loss: 0.251464320610137, policy loss: 2.6514656840655184
Experience 1, Iter 61, disc loss: 0.2600955068410037, policy loss: 2.7103248335325336
Experience 1, Iter 62, disc loss: 0.23581469598773838, policy loss: 2.893736404271299
Experience 1, Iter 63, disc loss: 0.2320977312679775, policy loss: 2.7584687830515158
Experience 1, Iter 64, disc loss: 0.24029823755077287, policy loss: 2.6861072350420603
Experience 1, Iter 65, disc loss: 0.21526531410778918, policy loss: 2.825908943696624
Experience 1, Iter 66, disc loss: 0.2330745124732867, policy loss: 2.7681569876214747
Experience 1, Iter 67, disc loss: 0.260274107517976, policy loss: 2.4032789524635145
Experience 1, Iter 68, disc loss: 0.23976910211083163, policy loss: 2.5632579976130017
Experience 1, Iter 69, disc loss: 0.22840250135954798, policy loss: 2.6725628053778325
Experience 1, Iter 70, disc loss: 0.195493393641188, policy loss: 2.741279706944735
Experience 1, Iter 71, disc loss: 0.2430350705250609, policy loss: 2.6224870356624277
Experience 1, Iter 72, disc loss: 0.1973426027644498, policy loss: 2.73292098329546
Experience 1, Iter 73, disc loss: 0.18212485927888414, policy loss: 2.8341004407014276
Experience 1, Iter 74, disc loss: 0.1711269974149548, policy loss: 3.025999315013875
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0421],
        [0.0764],
        [1.0477],
        [0.0201]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.5544, 1.6860, 0.9261, 0.0344, 0.0059, 3.2369]],

        [[0.5544, 1.6860, 0.9261, 0.0344, 0.0059, 3.2369]],

        [[0.5544, 1.6860, 0.9261, 0.0344, 0.0059, 3.2369]],

        [[0.5544, 1.6860, 0.9261, 0.0344, 0.0059, 3.2369]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.1686, 0.3056, 4.1906, 0.0803], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.1686, 0.3056, 4.1906, 0.0803])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.743
Iter 2/2000 - Loss: 3.653
Iter 3/2000 - Loss: 3.605
Iter 4/2000 - Loss: 3.611
Iter 5/2000 - Loss: 3.624
Iter 6/2000 - Loss: 3.605
Iter 7/2000 - Loss: 3.584
Iter 8/2000 - Loss: 3.577
Iter 9/2000 - Loss: 3.566
Iter 10/2000 - Loss: 3.549
Iter 11/2000 - Loss: 3.534
Iter 12/2000 - Loss: 3.525
Iter 13/2000 - Loss: 3.513
Iter 14/2000 - Loss: 3.490
Iter 15/2000 - Loss: 3.458
Iter 16/2000 - Loss: 3.418
Iter 17/2000 - Loss: 3.375
Iter 18/2000 - Loss: 3.326
Iter 19/2000 - Loss: 3.272
Iter 20/2000 - Loss: 3.213
Iter 1981/2000 - Loss: -4.355
Iter 1982/2000 - Loss: -4.355
Iter 1983/2000 - Loss: -4.355
Iter 1984/2000 - Loss: -4.355
Iter 1985/2000 - Loss: -4.355
Iter 1986/2000 - Loss: -4.355
Iter 1987/2000 - Loss: -4.356
Iter 1988/2000 - Loss: -4.356
Iter 1989/2000 - Loss: -4.356
Iter 1990/2000 - Loss: -4.356
Iter 1991/2000 - Loss: -4.356
Iter 1992/2000 - Loss: -4.356
Iter 1993/2000 - Loss: -4.356
Iter 1994/2000 - Loss: -4.356
Iter 1995/2000 - Loss: -4.356
Iter 1996/2000 - Loss: -4.356
Iter 1997/2000 - Loss: -4.356
Iter 1998/2000 - Loss: -4.356
Iter 1999/2000 - Loss: -4.356
Iter 2000/2000 - Loss: -4.356
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0002],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[ 7.2099, 13.9055, 42.7776, 26.0934, 16.1914, 59.0315]],

        [[42.3235, 66.9896, 12.2229,  1.8936,  1.0114, 17.6912]],

        [[43.3359, 70.6612, 11.3129,  1.0603, 10.3429, 14.2616]],

        [[56.0387, 87.3861, 14.0981,  5.0747,  8.8044, 56.8002]]])
Signal Variance: tensor([ 0.2542,  0.9603, 12.4535,  0.5211])
Estimated target variance: tensor([0.1686, 0.3056, 4.1906, 0.0803])
N: 30
Signal to noise ratio: tensor([22.3648, 76.3802, 64.0828, 42.0617])
Bound on condition number: tensor([ 15006.5503, 175019.0322, 123199.3008,  53076.6199])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 1.4081756329097983, policy loss: 0.3075623428496037
Experience 2, Iter 1, disc loss: 1.392598552787024, policy loss: 0.31422139896757995
Experience 2, Iter 2, disc loss: 1.372587025111741, policy loss: 0.3217714203175527
Experience 2, Iter 3, disc loss: 1.3881099915546122, policy loss: 0.317070992214558
Experience 2, Iter 4, disc loss: 1.3894804427138643, policy loss: 0.32831625602113407
Experience 2, Iter 5, disc loss: 1.2506531345372032, policy loss: 0.40912684683374706
Experience 2, Iter 6, disc loss: 1.2500839960369656, policy loss: 0.432704585846303
Experience 2, Iter 7, disc loss: 1.343237856131743, policy loss: 0.35951962635727475
Experience 2, Iter 8, disc loss: 1.462543700880055, policy loss: 0.31740070569683604
Experience 2, Iter 9, disc loss: 1.4708672161872047, policy loss: 0.3202299080981642
Experience 2, Iter 10, disc loss: 1.4950410308328994, policy loss: 0.3463695207834928
Experience 2, Iter 11, disc loss: 1.4905663421605857, policy loss: 0.3925625749545028
Experience 2, Iter 12, disc loss: 1.529152708841877, policy loss: 0.33054545301095467
Experience 2, Iter 13, disc loss: 1.5865006959382442, policy loss: 0.35459201213033814
Experience 2, Iter 14, disc loss: 1.554322838867453, policy loss: 0.38390554177435743
Experience 2, Iter 15, disc loss: 1.5181673644951414, policy loss: 0.3932211058674003
Experience 2, Iter 16, disc loss: 1.4877157175418412, policy loss: 0.38899258539814136
Experience 2, Iter 17, disc loss: 1.4495919282275875, policy loss: 0.4211348949637416
Experience 2, Iter 18, disc loss: 1.4755511239218975, policy loss: 0.4007839563537362
Experience 2, Iter 19, disc loss: 1.486254408434267, policy loss: 0.4167146283994329
Experience 2, Iter 20, disc loss: 1.3271419686332875, policy loss: 0.4644581047918172
Experience 2, Iter 21, disc loss: 1.2948904276808155, policy loss: 0.474237847500962
Experience 2, Iter 22, disc loss: 1.3555558750221721, policy loss: 0.4631014940172432
Experience 2, Iter 23, disc loss: 1.2878811511003456, policy loss: 0.4992657806298261
Experience 2, Iter 24, disc loss: 1.3328443298778223, policy loss: 0.49637632432893497
Experience 2, Iter 25, disc loss: 1.300382490328282, policy loss: 0.5200494818036187
Experience 2, Iter 26, disc loss: 1.2604805697361754, policy loss: 0.5535702982077483
Experience 2, Iter 27, disc loss: 1.2455851761543246, policy loss: 0.5690165724601103
Experience 2, Iter 28, disc loss: 1.2309670130464196, policy loss: 0.578974621148793
Experience 2, Iter 29, disc loss: 1.223431406723448, policy loss: 0.5818646280255808
Experience 2, Iter 30, disc loss: 1.1907383958083022, policy loss: 0.6064450437675281
Experience 2, Iter 31, disc loss: 1.1481812892570962, policy loss: 0.6394606387138291
Experience 2, Iter 32, disc loss: 1.1851820981990904, policy loss: 0.6189162259931429
Experience 2, Iter 33, disc loss: 1.1162281894217154, policy loss: 0.6553894981762591
Experience 2, Iter 34, disc loss: 1.1130932475637922, policy loss: 0.6558235013851432
Experience 2, Iter 35, disc loss: 1.0996264458523763, policy loss: 0.6608880975037038
Experience 2, Iter 36, disc loss: 1.066596475679345, policy loss: 0.6876724339007917
Experience 2, Iter 37, disc loss: 1.0260437789687544, policy loss: 0.711826156624253
Experience 2, Iter 38, disc loss: 0.9986026357934279, policy loss: 0.7309928809470074
Experience 2, Iter 39, disc loss: 0.9699474081952068, policy loss: 0.7529428088930701
Experience 2, Iter 40, disc loss: 0.9494580594095394, policy loss: 0.7665014459352614
Experience 2, Iter 41, disc loss: 0.9224490228845815, policy loss: 0.786505096812246
Experience 2, Iter 42, disc loss: 0.894929630999501, policy loss: 0.8089132151366637
Experience 2, Iter 43, disc loss: 0.8662407275475181, policy loss: 0.8327096282368521
Experience 2, Iter 44, disc loss: 0.8471337961269443, policy loss: 0.8466685126423941
Experience 2, Iter 45, disc loss: 0.8342210336168212, policy loss: 0.8536740938872815
Experience 2, Iter 46, disc loss: 0.8240044988942676, policy loss: 0.8574071719335162
Experience 2, Iter 47, disc loss: 0.8109584251270798, policy loss: 0.8621060024941237
Experience 2, Iter 48, disc loss: 0.7949126766933052, policy loss: 0.8689852152965633
Experience 2, Iter 49, disc loss: 0.7561432588370286, policy loss: 0.9069337238716845
Experience 2, Iter 50, disc loss: 0.7219100702325545, policy loss: 0.9427233863471105
Experience 2, Iter 51, disc loss: 0.705461695028467, policy loss: 0.9556245650969026
Experience 2, Iter 52, disc loss: 0.6946418927218, policy loss: 0.9594906810290532
Experience 2, Iter 53, disc loss: 0.6720908521433884, policy loss: 0.9852877612258639
Experience 2, Iter 54, disc loss: 0.6640666252830771, policy loss: 0.9875312218553592
Experience 2, Iter 55, disc loss: 0.6517479742703567, policy loss: 0.9975439290789068
Experience 2, Iter 56, disc loss: 0.6481179248187006, policy loss: 0.9921527061068898
Experience 2, Iter 57, disc loss: 0.6327553986842399, policy loss: 1.0081427392984048
Experience 2, Iter 58, disc loss: 0.6304049423784608, policy loss: 1.0025463531287302
Experience 2, Iter 59, disc loss: 0.6174522047377747, policy loss: 1.0151453857792632
Experience 2, Iter 60, disc loss: 0.6102069474236893, policy loss: 1.0175134490782018
Experience 2, Iter 61, disc loss: 0.6051263018391215, policy loss: 1.0162580772137562
Experience 2, Iter 62, disc loss: 0.6038868858605788, policy loss: 1.008352763339764
Experience 2, Iter 63, disc loss: 0.6031330789112014, policy loss: 0.9998452928742235
Experience 2, Iter 64, disc loss: 0.6105458608146062, policy loss: 0.9780103418454329
Experience 2, Iter 65, disc loss: 0.6122871884435008, policy loss: 0.9657022265365975
Experience 2, Iter 66, disc loss: 0.6214879690272004, policy loss: 0.9424685472780979
Experience 2, Iter 67, disc loss: 0.6106265102632766, policy loss: 0.951682677769806
Experience 2, Iter 68, disc loss: 0.6120347303347445, policy loss: 0.9415728106680794
Experience 2, Iter 69, disc loss: 0.6028685821863642, policy loss: 0.9487948371733828
Experience 2, Iter 70, disc loss: 0.5901038869208874, policy loss: 0.9631929112313589
Experience 2, Iter 71, disc loss: 0.5812179027012244, policy loss: 0.9708498029679407
Experience 2, Iter 72, disc loss: 0.5868989746840034, policy loss: 0.9567947706083406
Experience 2, Iter 73, disc loss: 0.5891874974080809, policy loss: 0.950012868230241
Experience 2, Iter 74, disc loss: 0.600563321601595, policy loss: 0.9288045749718403
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0281],
        [0.0556],
        [0.7339],
        [0.0137]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.3690, 1.1233, 0.6279, 0.0231, 0.0048, 2.2713]],

        [[0.3690, 1.1233, 0.6279, 0.0231, 0.0048, 2.2713]],

        [[0.3690, 1.1233, 0.6279, 0.0231, 0.0048, 2.2713]],

        [[0.3690, 1.1233, 0.6279, 0.0231, 0.0048, 2.2713]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.1125, 0.2226, 2.9355, 0.0547], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.1125, 0.2226, 2.9355, 0.0547])
N: 45
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([181.0000, 181.0000, 181.0000, 181.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.901
Iter 2/2000 - Loss: 2.810
Iter 3/2000 - Loss: 2.734
Iter 4/2000 - Loss: 2.717
Iter 5/2000 - Loss: 2.687
Iter 6/2000 - Loss: 2.619
Iter 7/2000 - Loss: 2.554
Iter 8/2000 - Loss: 2.502
Iter 9/2000 - Loss: 2.445
Iter 10/2000 - Loss: 2.374
Iter 11/2000 - Loss: 2.298
Iter 12/2000 - Loss: 2.222
Iter 13/2000 - Loss: 2.143
Iter 14/2000 - Loss: 2.056
Iter 15/2000 - Loss: 1.956
Iter 16/2000 - Loss: 1.844
Iter 17/2000 - Loss: 1.723
Iter 18/2000 - Loss: 1.596
Iter 19/2000 - Loss: 1.465
Iter 20/2000 - Loss: 1.327
Iter 1981/2000 - Loss: -5.848
Iter 1982/2000 - Loss: -5.848
Iter 1983/2000 - Loss: -5.848
Iter 1984/2000 - Loss: -5.848
Iter 1985/2000 - Loss: -5.848
Iter 1986/2000 - Loss: -5.848
Iter 1987/2000 - Loss: -5.848
Iter 1988/2000 - Loss: -5.848
Iter 1989/2000 - Loss: -5.848
Iter 1990/2000 - Loss: -5.848
Iter 1991/2000 - Loss: -5.849
Iter 1992/2000 - Loss: -5.849
Iter 1993/2000 - Loss: -5.849
Iter 1994/2000 - Loss: -5.849
Iter 1995/2000 - Loss: -5.849
Iter 1996/2000 - Loss: -5.849
Iter 1997/2000 - Loss: -5.849
Iter 1998/2000 - Loss: -5.849
Iter 1999/2000 - Loss: -5.849
Iter 2000/2000 - Loss: -5.849
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[15.6577, 15.9151, 38.5355, 18.2772, 16.6440, 54.6213]],

        [[43.0615, 58.4541,  8.8278,  1.2122,  8.2748, 17.8662]],

        [[48.9011, 60.9213,  9.7228,  1.0372, 10.0017, 12.7367]],

        [[51.9517, 74.3886, 14.2031,  4.4640,  6.5167, 50.0462]]])
Signal Variance: tensor([ 0.3590,  0.7660, 10.0593,  0.4429])
Estimated target variance: tensor([0.1125, 0.2226, 2.9355, 0.0547])
N: 45
Signal to noise ratio: tensor([29.6070, 42.9132, 85.5051, 38.2992])
Bound on condition number: tensor([ 39446.7953,  82870.4801, 329001.2440,  66008.1848])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.556178676099009, policy loss: 0.9956911438805045
Experience 3, Iter 1, disc loss: 0.5207241566482942, policy loss: 1.0569640508693618
Experience 3, Iter 2, disc loss: 0.48565950467841534, policy loss: 1.1245203075024537
Experience 3, Iter 3, disc loss: 0.46884479247481864, policy loss: 1.1539753008660885
Experience 3, Iter 4, disc loss: 0.4566395839423474, policy loss: 1.1779404439656698
Experience 3, Iter 5, disc loss: 0.4502228868421853, policy loss: 1.1895133042670336
Experience 3, Iter 6, disc loss: 0.446607929904873, policy loss: 1.1949637028135829
Experience 3, Iter 7, disc loss: 0.44647984847018435, policy loss: 1.1928672493389731
Experience 3, Iter 8, disc loss: 0.44597391537690234, policy loss: 1.1917483031748666
Experience 3, Iter 9, disc loss: 0.45329870976561115, policy loss: 1.1738906277491195
Experience 3, Iter 10, disc loss: 0.47745012762264827, policy loss: 1.1212785624560686
Experience 3, Iter 11, disc loss: 0.4761154519623706, policy loss: 1.1221968632813912
Experience 3, Iter 12, disc loss: 0.4929457180919736, policy loss: 1.0892575062398184
Experience 3, Iter 13, disc loss: 0.5210444405043975, policy loss: 1.0377919397476973
Experience 3, Iter 14, disc loss: 0.5383791655396439, policy loss: 1.0064266326367988
Experience 3, Iter 15, disc loss: 0.5463932635275042, policy loss: 0.9930177111155906
Experience 3, Iter 16, disc loss: 0.567614869208151, policy loss: 0.9599877536623493
Experience 3, Iter 17, disc loss: 0.5894182431807115, policy loss: 0.92873561288524
Experience 3, Iter 18, disc loss: 0.5450742138460267, policy loss: 1.0124752469213978
Experience 3, Iter 19, disc loss: 0.5604762854296057, policy loss: 0.9879566927342379
Experience 3, Iter 20, disc loss: 0.52967423368354, policy loss: 1.0425471293281705
Experience 3, Iter 21, disc loss: 0.5489841372466588, policy loss: 1.0124253371365177
Experience 3, Iter 22, disc loss: 0.5455542932735465, policy loss: 1.0128107813631977
Experience 3, Iter 23, disc loss: 0.4890469227654746, policy loss: 1.1207423025543406
Experience 3, Iter 24, disc loss: 0.47756457055738677, policy loss: 1.1512501542414704
Experience 3, Iter 25, disc loss: 0.48864208277815757, policy loss: 1.133142977933801
Experience 3, Iter 26, disc loss: 0.4683357609436051, policy loss: 1.1909861979988576
Experience 3, Iter 27, disc loss: 0.37139824118463216, policy loss: 1.469781635586492
Experience 3, Iter 28, disc loss: 0.3653769536316642, policy loss: 1.4877036586659096
Experience 3, Iter 29, disc loss: 0.40981240999988683, policy loss: 1.3411551859415538
Experience 3, Iter 30, disc loss: 0.39220404550033544, policy loss: 1.3891160978419184
Experience 3, Iter 31, disc loss: 0.38099073438142456, policy loss: 1.4103207365906196
Experience 3, Iter 32, disc loss: 0.36177588453930487, policy loss: 1.4839357220446456
Experience 3, Iter 33, disc loss: 0.3774975029209543, policy loss: 1.4288011766709618
Experience 3, Iter 34, disc loss: 0.35071818881369854, policy loss: 1.5258816929075585
Experience 3, Iter 35, disc loss: 0.3429995873731842, policy loss: 1.5527824441710485
Experience 3, Iter 36, disc loss: 0.3528457658304864, policy loss: 1.5078989800677187
Experience 3, Iter 37, disc loss: 0.33656292354770667, policy loss: 1.5619222234934642
Experience 3, Iter 38, disc loss: 0.34174407454405853, policy loss: 1.5375643588841543
Experience 3, Iter 39, disc loss: 0.3433652213083365, policy loss: 1.5254145335100993
Experience 3, Iter 40, disc loss: 0.337147594530105, policy loss: 1.542814602212866
Experience 3, Iter 41, disc loss: 0.3384783315161471, policy loss: 1.530104144048527
Experience 3, Iter 42, disc loss: 0.33676228609339154, policy loss: 1.532261170002007
Experience 3, Iter 43, disc loss: 0.3256868648687413, policy loss: 1.5655746001938116
Experience 3, Iter 44, disc loss: 0.32190286675235824, policy loss: 1.5730844953230414
Experience 3, Iter 45, disc loss: 0.3105631613643356, policy loss: 1.609868422049138
Experience 3, Iter 46, disc loss: 0.30871796235778615, policy loss: 1.6054815083639062
Experience 3, Iter 47, disc loss: 0.3015492919864857, policy loss: 1.6236558962464844
Experience 3, Iter 48, disc loss: 0.2908563898581539, policy loss: 1.6592418793427521
Experience 3, Iter 49, disc loss: 0.2883015422091887, policy loss: 1.6561946684586024
Experience 3, Iter 50, disc loss: 0.2844328934753162, policy loss: 1.6613702899370417
Experience 3, Iter 51, disc loss: 0.2756988144279797, policy loss: 1.6876323066755718
Experience 3, Iter 52, disc loss: 0.2721799478402095, policy loss: 1.6923925797883188
Experience 3, Iter 53, disc loss: 0.26505397662842733, policy loss: 1.7114006432422861
Experience 3, Iter 54, disc loss: 0.25937726456109955, policy loss: 1.729329111797177
Experience 3, Iter 55, disc loss: 0.24655719538914744, policy loss: 1.7785887695912015
Experience 3, Iter 56, disc loss: 0.2474787799726759, policy loss: 1.7644684495006269
Experience 3, Iter 57, disc loss: 0.24161070575301205, policy loss: 1.7865982037968124
Experience 3, Iter 58, disc loss: 0.23254036447590565, policy loss: 1.8191363406076126
Experience 3, Iter 59, disc loss: 0.2251913135270745, policy loss: 1.8501942951077455
Experience 3, Iter 60, disc loss: 0.22470131981913327, policy loss: 1.8439370771188304
Experience 3, Iter 61, disc loss: 0.21543775413292485, policy loss: 1.8880062011002814
Experience 3, Iter 62, disc loss: 0.2159730753973003, policy loss: 1.8727646697416676
Experience 3, Iter 63, disc loss: 0.2091946066603878, policy loss: 1.9082235045951548
Experience 3, Iter 64, disc loss: 0.20243878419396683, policy loss: 1.9381816979509072
Experience 3, Iter 65, disc loss: 0.19724721958123764, policy loss: 1.966313739835219
Experience 3, Iter 66, disc loss: 0.19033927589465954, policy loss: 2.002422460674185
Experience 3, Iter 67, disc loss: 0.18526366324327503, policy loss: 2.030241410963196
Experience 3, Iter 68, disc loss: 0.18547807132802044, policy loss: 2.014535735362177
Experience 3, Iter 69, disc loss: 0.18978490708407786, policy loss: 1.9813977744041604
Experience 3, Iter 70, disc loss: 0.18839709433901514, policy loss: 1.9842256889840932
Experience 3, Iter 71, disc loss: 0.17628386214340958, policy loss: 2.0608125161833004
Experience 3, Iter 72, disc loss: 0.1774996064558392, policy loss: 2.04676421922209
Experience 3, Iter 73, disc loss: 0.1743904908954609, policy loss: 2.0583547586607684
Experience 3, Iter 74, disc loss: 0.17350497844902357, policy loss: 2.0637993717997634
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0214],
        [0.0500],
        [0.6310],
        [0.0105]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.2761, 0.8536, 0.4869, 0.0174, 0.0040, 1.8554]],

        [[0.2761, 0.8536, 0.4869, 0.0174, 0.0040, 1.8554]],

        [[0.2761, 0.8536, 0.4869, 0.0174, 0.0040, 1.8554]],

        [[0.2761, 0.8536, 0.4869, 0.0174, 0.0040, 1.8554]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0857, 0.2001, 2.5239, 0.0418], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0857, 0.2001, 2.5239, 0.0418])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.420
Iter 2/2000 - Loss: 2.339
Iter 3/2000 - Loss: 2.246
Iter 4/2000 - Loss: 2.198
Iter 5/2000 - Loss: 2.135
Iter 6/2000 - Loss: 2.040
Iter 7/2000 - Loss: 1.945
Iter 8/2000 - Loss: 1.858
Iter 9/2000 - Loss: 1.768
Iter 10/2000 - Loss: 1.666
Iter 11/2000 - Loss: 1.553
Iter 12/2000 - Loss: 1.438
Iter 13/2000 - Loss: 1.321
Iter 14/2000 - Loss: 1.199
Iter 15/2000 - Loss: 1.070
Iter 16/2000 - Loss: 0.932
Iter 17/2000 - Loss: 0.785
Iter 18/2000 - Loss: 0.632
Iter 19/2000 - Loss: 0.474
Iter 20/2000 - Loss: 0.310
Iter 1981/2000 - Loss: -6.546
Iter 1982/2000 - Loss: -6.546
Iter 1983/2000 - Loss: -6.546
Iter 1984/2000 - Loss: -6.546
Iter 1985/2000 - Loss: -6.546
Iter 1986/2000 - Loss: -6.546
Iter 1987/2000 - Loss: -6.546
Iter 1988/2000 - Loss: -6.546
Iter 1989/2000 - Loss: -6.546
Iter 1990/2000 - Loss: -6.546
Iter 1991/2000 - Loss: -6.547
Iter 1992/2000 - Loss: -6.547
Iter 1993/2000 - Loss: -6.547
Iter 1994/2000 - Loss: -6.547
Iter 1995/2000 - Loss: -6.547
Iter 1996/2000 - Loss: -6.547
Iter 1997/2000 - Loss: -6.547
Iter 1998/2000 - Loss: -6.547
Iter 1999/2000 - Loss: -6.547
Iter 2000/2000 - Loss: -6.547
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[24.2095, 16.0158, 36.6746, 14.6974, 13.8535, 48.9258]],

        [[41.8464, 49.2993, 10.2324,  1.2075,  8.1154, 22.7439]],

        [[46.3585, 51.9048, 11.6245,  1.0469, 10.0884, 13.2375]],

        [[48.2961, 64.5621, 14.1425,  4.1077,  4.5753, 46.9116]]])
Signal Variance: tensor([ 0.3980,  1.2788, 11.7048,  0.4031])
Estimated target variance: tensor([0.0857, 0.2001, 2.5239, 0.0418])
N: 60
Signal to noise ratio: tensor([30.6497, 56.8177, 76.2201, 37.8553])
Bound on condition number: tensor([ 56365.0665, 193695.7308, 348570.8271,  85982.5683])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.17444393341019748, policy loss: 2.055941610563006
Experience 4, Iter 1, disc loss: 0.16838058732579436, policy loss: 2.093233865169796
Experience 4, Iter 2, disc loss: 0.15766129341701124, policy loss: 2.1670260486684425
Experience 4, Iter 3, disc loss: 0.16110308270191262, policy loss: 2.1383602189235527
Experience 4, Iter 4, disc loss: 0.16882081105489136, policy loss: 2.0895511502417707
Experience 4, Iter 5, disc loss: 0.15385244724349662, policy loss: 2.20428827921294
Experience 4, Iter 6, disc loss: 0.16381918747077115, policy loss: 2.1163401968014224
Experience 4, Iter 7, disc loss: 0.15224645918611956, policy loss: 2.1946732300043763
Experience 4, Iter 8, disc loss: 0.14692708015661982, policy loss: 2.240009342075612
Experience 4, Iter 9, disc loss: 0.14954265335385855, policy loss: 2.2157283081673373
Experience 4, Iter 10, disc loss: 0.14427283256141216, policy loss: 2.2573680009401205
Experience 4, Iter 11, disc loss: 0.14026220606256593, policy loss: 2.2929246823358813
Experience 4, Iter 12, disc loss: 0.13577281512401357, policy loss: 2.3267548130792703
Experience 4, Iter 13, disc loss: 0.1375334960712406, policy loss: 2.301466632202753
Experience 4, Iter 14, disc loss: 0.12626851491485494, policy loss: 2.4109029594519633
Experience 4, Iter 15, disc loss: 0.12309280647528015, policy loss: 2.436804559850577
Experience 4, Iter 16, disc loss: 0.12503101189937044, policy loss: 2.415450318820649
Experience 4, Iter 17, disc loss: 0.1170694989416183, policy loss: 2.498737657026721
Experience 4, Iter 18, disc loss: 0.11515221230983375, policy loss: 2.500031959170062
Experience 4, Iter 19, disc loss: 0.11013309221248568, policy loss: 2.559479848828834
Experience 4, Iter 20, disc loss: 0.10703672157569395, policy loss: 2.60114517572651
Experience 4, Iter 21, disc loss: 0.10443252230113748, policy loss: 2.6297786343177023
Experience 4, Iter 22, disc loss: 0.10579307380694948, policy loss: 2.5946468196630823
Experience 4, Iter 23, disc loss: 0.10614650654139075, policy loss: 2.5924914391505176
Experience 4, Iter 24, disc loss: 0.10425742232047, policy loss: 2.5957500713827564
Experience 4, Iter 25, disc loss: 0.10214890935131088, policy loss: 2.6279833441978115
Experience 4, Iter 26, disc loss: 0.0987810689120725, policy loss: 2.654054955229453
Experience 4, Iter 27, disc loss: 0.09886355887445113, policy loss: 2.6545716496840366
Experience 4, Iter 28, disc loss: 0.09658715291424508, policy loss: 2.6675796665647713
Experience 4, Iter 29, disc loss: 0.09443097116833472, policy loss: 2.6883408021357376
Experience 4, Iter 30, disc loss: 0.09074844196990908, policy loss: 2.7469683996688676
Experience 4, Iter 31, disc loss: 0.0921332693000356, policy loss: 2.7276902879267113
Experience 4, Iter 32, disc loss: 0.0921230964856467, policy loss: 2.712709837043028
Experience 4, Iter 33, disc loss: 0.09217872109793583, policy loss: 2.711704320520035
Experience 4, Iter 34, disc loss: 0.08635097737519487, policy loss: 2.783702448534604
Experience 4, Iter 35, disc loss: 0.09032000488076461, policy loss: 2.724458728453114
Experience 4, Iter 36, disc loss: 0.08738388738286729, policy loss: 2.767295810351465
Experience 4, Iter 37, disc loss: 0.08599011574296586, policy loss: 2.780816007759057
Experience 4, Iter 38, disc loss: 0.08371935567147337, policy loss: 2.8115999124982185
Experience 4, Iter 39, disc loss: 0.08002881555789003, policy loss: 2.8772578768815524
Experience 4, Iter 40, disc loss: 0.07848263416795818, policy loss: 2.869683508101122
Experience 4, Iter 41, disc loss: 0.07909574467926575, policy loss: 2.8707186945060217
Experience 4, Iter 42, disc loss: 0.07507022630832998, policy loss: 2.948240025280395
Experience 4, Iter 43, disc loss: 0.07511639403270036, policy loss: 2.9169064640683526
Experience 4, Iter 44, disc loss: 0.07297165202112067, policy loss: 2.955800060346382
Experience 4, Iter 45, disc loss: 0.07056939695461009, policy loss: 2.995340429910905
Experience 4, Iter 46, disc loss: 0.06819364981337567, policy loss: 3.0283282500168722
Experience 4, Iter 47, disc loss: 0.06890902932295515, policy loss: 3.0096215364432686
Experience 4, Iter 48, disc loss: 0.06608484917855972, policy loss: 3.061448205125579
Experience 4, Iter 49, disc loss: 0.06429103902492875, policy loss: 3.0997783931083864
Experience 4, Iter 50, disc loss: 0.06433000847602104, policy loss: 3.0897262975205493
Experience 4, Iter 51, disc loss: 0.06539451976177903, policy loss: 3.0583265897349197
Experience 4, Iter 52, disc loss: 0.0609211018798724, policy loss: 3.1504591276526304
Experience 4, Iter 53, disc loss: 0.06258594521828073, policy loss: 3.0981143705769183
Experience 4, Iter 54, disc loss: 0.06358683197452443, policy loss: 3.0843052313413337
Experience 4, Iter 55, disc loss: 0.06216934195262985, policy loss: 3.1059173911617224
Experience 4, Iter 56, disc loss: 0.06052973588192308, policy loss: 3.137076049563384
Experience 4, Iter 57, disc loss: 0.05858953370349422, policy loss: 3.173053024124475
Experience 4, Iter 58, disc loss: 0.0579979105113426, policy loss: 3.1759063246155956
Experience 4, Iter 59, disc loss: 0.05532238944218108, policy loss: 3.2438346122854553
Experience 4, Iter 60, disc loss: 0.0587699952342551, policy loss: 3.1479945994657665
Experience 4, Iter 61, disc loss: 0.053918116581635755, policy loss: 3.2897185437905145
Experience 4, Iter 62, disc loss: 0.05428664683460829, policy loss: 3.2537311637930744
Experience 4, Iter 63, disc loss: 0.0560751521662833, policy loss: 3.2012752706824172
Experience 4, Iter 64, disc loss: 0.05204353613900434, policy loss: 3.3063257746002863
Experience 4, Iter 65, disc loss: 0.05018547161236064, policy loss: 3.336367789407532
Experience 4, Iter 66, disc loss: 0.05040487170008641, policy loss: 3.3175969014475886
Experience 4, Iter 67, disc loss: 0.049918998230823186, policy loss: 3.357201676751858
Experience 4, Iter 68, disc loss: 0.048784304954661795, policy loss: 3.356556989946933
Experience 4, Iter 69, disc loss: 0.04988194718530594, policy loss: 3.3274294646685685
Experience 4, Iter 70, disc loss: 0.04793390670533268, policy loss: 3.3810122038416908
Experience 4, Iter 71, disc loss: 0.0467669306123874, policy loss: 3.4037468236353297
Experience 4, Iter 72, disc loss: 0.04744902781322917, policy loss: 3.3745981616089313
Experience 4, Iter 73, disc loss: 0.04871422168365311, policy loss: 3.337891646714085
Experience 4, Iter 74, disc loss: 0.04667922378920808, policy loss: 3.3885092499485934
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0173],
        [0.0523],
        [0.6270],
        [0.0086]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.2208, 0.6909, 0.4113, 0.0140, 0.0033, 1.6903]],

        [[0.2208, 0.6909, 0.4113, 0.0140, 0.0033, 1.6903]],

        [[0.2208, 0.6909, 0.4113, 0.0140, 0.0033, 1.6903]],

        [[0.2208, 0.6909, 0.4113, 0.0140, 0.0033, 1.6903]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0693, 0.2093, 2.5081, 0.0346], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0693, 0.2093, 2.5081, 0.0346])
N: 75
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([301.0000, 301.0000, 301.0000, 301.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.247
Iter 2/2000 - Loss: 2.201
Iter 3/2000 - Loss: 2.091
Iter 4/2000 - Loss: 2.028
Iter 5/2000 - Loss: 1.965
Iter 6/2000 - Loss: 1.864
Iter 7/2000 - Loss: 1.748
Iter 8/2000 - Loss: 1.634
Iter 9/2000 - Loss: 1.520
Iter 10/2000 - Loss: 1.391
Iter 11/2000 - Loss: 1.247
Iter 12/2000 - Loss: 1.093
Iter 13/2000 - Loss: 0.936
Iter 14/2000 - Loss: 0.776
Iter 15/2000 - Loss: 0.613
Iter 16/2000 - Loss: 0.444
Iter 17/2000 - Loss: 0.266
Iter 18/2000 - Loss: 0.081
Iter 19/2000 - Loss: -0.110
Iter 20/2000 - Loss: -0.303
Iter 1981/2000 - Loss: -7.006
Iter 1982/2000 - Loss: -7.006
Iter 1983/2000 - Loss: -7.006
Iter 1984/2000 - Loss: -7.006
Iter 1985/2000 - Loss: -7.006
Iter 1986/2000 - Loss: -7.006
Iter 1987/2000 - Loss: -7.006
Iter 1988/2000 - Loss: -7.006
Iter 1989/2000 - Loss: -7.006
Iter 1990/2000 - Loss: -7.006
Iter 1991/2000 - Loss: -7.006
Iter 1992/2000 - Loss: -7.006
Iter 1993/2000 - Loss: -7.006
Iter 1994/2000 - Loss: -7.006
Iter 1995/2000 - Loss: -7.006
Iter 1996/2000 - Loss: -7.006
Iter 1997/2000 - Loss: -7.007
Iter 1998/2000 - Loss: -7.007
Iter 1999/2000 - Loss: -7.007
Iter 2000/2000 - Loss: -7.007
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[17.9207, 12.2269, 31.9619, 12.2780, 13.4567, 41.7737]],

        [[39.6146, 48.5566, 14.3173,  1.1632,  1.9104, 10.1972]],

        [[44.0911, 51.3704, 12.0000,  1.0547, 10.1683, 12.6796]],

        [[45.7081, 61.0204, 13.6703,  3.9280,  4.2062, 43.2757]]])
Signal Variance: tensor([ 0.3228,  0.7829, 11.6480,  0.3618])
Estimated target variance: tensor([0.0693, 0.2093, 2.5081, 0.0346])
N: 75
Signal to noise ratio: tensor([30.8070, 46.5612, 69.4487, 34.3376])
Bound on condition number: tensor([ 71181.3708, 162596.6929, 361735.1518,  88431.0850])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.04158227789438252, policy loss: 3.541720857713312
Experience 5, Iter 1, disc loss: 0.04356434122854651, policy loss: 3.4701139536010137
Experience 5, Iter 2, disc loss: 0.042018815206984785, policy loss: 3.516272620516845
Experience 5, Iter 3, disc loss: 0.036231483608604544, policy loss: 3.7429844662616816
Experience 5, Iter 4, disc loss: 0.042029244544216665, policy loss: 3.5109623733360755
Experience 5, Iter 5, disc loss: 0.04126825756195956, policy loss: 3.5253698353084673
Experience 5, Iter 6, disc loss: 0.04066116616698556, policy loss: 3.538343256328308
Experience 5, Iter 7, disc loss: 0.04026666128501166, policy loss: 3.541762820488181
Experience 5, Iter 8, disc loss: 0.04029679909541881, policy loss: 3.5403872085033212
Experience 5, Iter 9, disc loss: 0.038782986205809986, policy loss: 3.614623496278244
Experience 5, Iter 10, disc loss: 0.03872238793810997, policy loss: 3.5913771371261203
Experience 5, Iter 11, disc loss: 0.03967264980656879, policy loss: 3.556767477589704
Experience 5, Iter 12, disc loss: 0.0382334114910408, policy loss: 3.5904538186425614
Experience 5, Iter 13, disc loss: 0.03754148139973996, policy loss: 3.6146491760613393
Experience 5, Iter 14, disc loss: 0.03725058905757572, policy loss: 3.6177385632468697
Experience 5, Iter 15, disc loss: 0.036728488746009924, policy loss: 3.641228081204927
Experience 5, Iter 16, disc loss: 0.034357476404734556, policy loss: 3.749472360993845
Experience 5, Iter 17, disc loss: 0.03424539487688337, policy loss: 3.735272395832847
Experience 5, Iter 18, disc loss: 0.03636647372542199, policy loss: 3.6397546434388013
Experience 5, Iter 19, disc loss: 0.03417568636428341, policy loss: 3.7230946164379874
Experience 5, Iter 20, disc loss: 0.0339590206965029, policy loss: 3.7250125361488946
Experience 5, Iter 21, disc loss: 0.03393097665467466, policy loss: 3.7276438524714535
Experience 5, Iter 22, disc loss: 0.03295932008825329, policy loss: 3.7695772625814605
Experience 5, Iter 23, disc loss: 0.03027219898240617, policy loss: 3.905045067708612
Experience 5, Iter 24, disc loss: 0.031488002842332066, policy loss: 3.8347253194851554
Experience 5, Iter 25, disc loss: 0.03338605551998376, policy loss: 3.7412337751384053
Experience 5, Iter 26, disc loss: 0.031456142704600704, policy loss: 3.8098745522076127
Experience 5, Iter 27, disc loss: 0.031437021480971106, policy loss: 3.807016314472231
Experience 5, Iter 28, disc loss: 0.032502211003839133, policy loss: 3.7554484366168746
Experience 5, Iter 29, disc loss: 0.030850770791653, policy loss: 3.834951580363156
Experience 5, Iter 30, disc loss: 0.029439843927717937, policy loss: 3.9058696554174306
Experience 5, Iter 31, disc loss: 0.03062329075302931, policy loss: 3.8437300801934344
Experience 5, Iter 32, disc loss: 0.03046483259549752, policy loss: 3.8370719385986822
Experience 5, Iter 33, disc loss: 0.029829962244661966, policy loss: 3.854131124638163
Experience 5, Iter 34, disc loss: 0.029357808402905333, policy loss: 3.876195841449781
Experience 5, Iter 35, disc loss: 0.02981508114767452, policy loss: 3.855204294237554
Experience 5, Iter 36, disc loss: 0.028993946567558503, policy loss: 3.898768224861557
Experience 5, Iter 37, disc loss: 0.028695024109856616, policy loss: 3.9116084593146625
Experience 5, Iter 38, disc loss: 0.02849056066069976, policy loss: 3.922375308043235
Experience 5, Iter 39, disc loss: 0.028542851677931302, policy loss: 3.898948819562195
Experience 5, Iter 40, disc loss: 0.028543782968358886, policy loss: 3.8914818948063585
Experience 5, Iter 41, disc loss: 0.02780552888375222, policy loss: 3.918414482209131
Experience 5, Iter 42, disc loss: 0.027392285675910857, policy loss: 3.945441952157946
Experience 5, Iter 43, disc loss: 0.02643982738606481, policy loss: 4.000903441140959
Experience 5, Iter 44, disc loss: 0.027017284849091737, policy loss: 3.9717785486787367
Experience 5, Iter 45, disc loss: 0.02650980216220288, policy loss: 3.993998683733962
Experience 5, Iter 46, disc loss: 0.026400404504579804, policy loss: 3.9915462308281793
Experience 5, Iter 47, disc loss: 0.026178311416237242, policy loss: 3.986364765180409
Experience 5, Iter 48, disc loss: 0.026444565068042547, policy loss: 3.975194824124209
Experience 5, Iter 49, disc loss: 0.024662316931459097, policy loss: 4.080822853544353
Experience 5, Iter 50, disc loss: 0.025260563934657247, policy loss: 4.046007373396433
Experience 5, Iter 51, disc loss: 0.025459270917280202, policy loss: 4.02716062880136
Experience 5, Iter 52, disc loss: 0.025101062315554493, policy loss: 4.036935003632463
Experience 5, Iter 53, disc loss: 0.02355469590592567, policy loss: 4.135207311001494
Experience 5, Iter 54, disc loss: 0.02484636597078339, policy loss: 4.0487148990146045
Experience 5, Iter 55, disc loss: 0.023947388464907737, policy loss: 4.108778967476681
Experience 5, Iter 56, disc loss: 0.023375093697667583, policy loss: 4.134698388319004
Experience 5, Iter 57, disc loss: 0.02356847884085074, policy loss: 4.11368969946717
Experience 5, Iter 58, disc loss: 0.024467767645966077, policy loss: 4.0519437568883285
Experience 5, Iter 59, disc loss: 0.023370471823955703, policy loss: 4.122241598895876
Experience 5, Iter 60, disc loss: 0.023706309573520754, policy loss: 4.094953918751216
Experience 5, Iter 61, disc loss: 0.02296975251046669, policy loss: 4.1481273414787685
Experience 5, Iter 62, disc loss: 0.02207788509583606, policy loss: 4.187315577612195
Experience 5, Iter 63, disc loss: 0.02305304823234125, policy loss: 4.1257914045103465
Experience 5, Iter 64, disc loss: 0.022235711596929397, policy loss: 4.176338173421568
Experience 5, Iter 65, disc loss: 0.02197931490270338, policy loss: 4.190719082858517
Experience 5, Iter 66, disc loss: 0.022076559076396114, policy loss: 4.1755039184978315
Experience 5, Iter 67, disc loss: 0.02134273772230357, policy loss: 4.229732583199015
Experience 5, Iter 68, disc loss: 0.02095794255166275, policy loss: 4.254899213116108
Experience 5, Iter 69, disc loss: 0.019964772297383415, policy loss: 4.327148107369862
Experience 5, Iter 70, disc loss: 0.02088983438258226, policy loss: 4.244461836280299
Experience 5, Iter 71, disc loss: 0.02022336985900385, policy loss: 4.295236396495351
Experience 5, Iter 72, disc loss: 0.021606829169160825, policy loss: 4.188153200742767
Experience 5, Iter 73, disc loss: 0.01997711933336061, policy loss: 4.299593943996287
Experience 5, Iter 74, disc loss: 0.019797662587950755, policy loss: 4.3145727833775265
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0146],
        [0.0581],
        [0.6921],
        [0.0080]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1843, 0.5810, 0.3936, 0.0120, 0.0029, 1.6258]],

        [[0.1843, 0.5810, 0.3936, 0.0120, 0.0029, 1.6258]],

        [[0.1843, 0.5810, 0.3936, 0.0120, 0.0029, 1.6258]],

        [[0.1843, 0.5810, 0.3936, 0.0120, 0.0029, 1.6258]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0582, 0.2324, 2.7683, 0.0321], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0582, 0.2324, 2.7683, 0.0321])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.272
Iter 2/2000 - Loss: 2.277
Iter 3/2000 - Loss: 2.155
Iter 4/2000 - Loss: 2.104
Iter 5/2000 - Loss: 2.072
Iter 6/2000 - Loss: 1.987
Iter 7/2000 - Loss: 1.874
Iter 8/2000 - Loss: 1.766
Iter 9/2000 - Loss: 1.662
Iter 10/2000 - Loss: 1.541
Iter 11/2000 - Loss: 1.397
Iter 12/2000 - Loss: 1.235
Iter 13/2000 - Loss: 1.067
Iter 14/2000 - Loss: 0.894
Iter 15/2000 - Loss: 0.714
Iter 16/2000 - Loss: 0.524
Iter 17/2000 - Loss: 0.322
Iter 18/2000 - Loss: 0.109
Iter 19/2000 - Loss: -0.111
Iter 20/2000 - Loss: -0.334
Iter 1981/2000 - Loss: -7.361
Iter 1982/2000 - Loss: -7.362
Iter 1983/2000 - Loss: -7.362
Iter 1984/2000 - Loss: -7.362
Iter 1985/2000 - Loss: -7.362
Iter 1986/2000 - Loss: -7.362
Iter 1987/2000 - Loss: -7.362
Iter 1988/2000 - Loss: -7.362
Iter 1989/2000 - Loss: -7.362
Iter 1990/2000 - Loss: -7.362
Iter 1991/2000 - Loss: -7.362
Iter 1992/2000 - Loss: -7.362
Iter 1993/2000 - Loss: -7.362
Iter 1994/2000 - Loss: -7.362
Iter 1995/2000 - Loss: -7.362
Iter 1996/2000 - Loss: -7.362
Iter 1997/2000 - Loss: -7.362
Iter 1998/2000 - Loss: -7.362
Iter 1999/2000 - Loss: -7.362
Iter 2000/2000 - Loss: -7.362
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[22.6286, 15.7519, 32.1485, 14.1148, 15.7322, 46.0182]],

        [[39.8028, 43.0493, 14.4574,  1.1750,  3.2357, 15.0857]],

        [[37.8925, 40.4823, 11.7551,  0.9854,  8.4627, 13.9592]],

        [[43.0625, 56.7956, 13.7203,  3.8482,  3.1518, 42.7103]]])
Signal Variance: tensor([ 0.3414,  1.3448, 12.7418,  0.3449])
Estimated target variance: tensor([0.0582, 0.2324, 2.7683, 0.0321])
N: 90
Signal to noise ratio: tensor([29.4657, 63.6601, 76.3710, 35.8924])
Bound on condition number: tensor([ 78141.2298, 364735.3186, 524928.1029, 115945.0121])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.01989302791151508, policy loss: 4.2995429793906155
Experience 6, Iter 1, disc loss: 0.020944197605357654, policy loss: 4.223486051488072
Experience 6, Iter 2, disc loss: 0.020565351087352496, policy loss: 4.240817806198215
Experience 6, Iter 3, disc loss: 0.020991165913605402, policy loss: 4.207190568891837
Experience 6, Iter 4, disc loss: 0.019550153598490827, policy loss: 4.321586408147175
Experience 6, Iter 5, disc loss: 0.01898304343844314, policy loss: 4.3549968856836285
Experience 6, Iter 6, disc loss: 0.01911927701839096, policy loss: 4.343448553651138
Experience 6, Iter 7, disc loss: 0.01890583603717555, policy loss: 4.3527101944958435
Experience 6, Iter 8, disc loss: 0.019120936947404625, policy loss: 4.333974161815634
Experience 6, Iter 9, disc loss: 0.01863353510957077, policy loss: 4.359665665899389
Experience 6, Iter 10, disc loss: 0.017996759609625906, policy loss: 4.421723768969182
Experience 6, Iter 11, disc loss: 0.017098897810328972, policy loss: 4.482575134952089
Experience 6, Iter 12, disc loss: 0.017484847604989664, policy loss: 4.459128956356678
Experience 6, Iter 13, disc loss: 0.01820841170262764, policy loss: 4.388166463000415
Experience 6, Iter 14, disc loss: 0.017494038728031326, policy loss: 4.4488982761536855
Experience 6, Iter 15, disc loss: 0.017727046436663587, policy loss: 4.430450030392954
Experience 6, Iter 16, disc loss: 0.017525155423596974, policy loss: 4.436120008715612
Experience 6, Iter 17, disc loss: 0.018255002044918586, policy loss: 4.373380312879622
Experience 6, Iter 18, disc loss: 0.017324970235268727, policy loss: 4.446912530980459
Experience 6, Iter 19, disc loss: 0.01747026393580292, policy loss: 4.428300796545081
Experience 6, Iter 20, disc loss: 0.01799340169733682, policy loss: 4.393407764029301
Experience 6, Iter 21, disc loss: 0.01670047239724054, policy loss: 4.493681590292358
Experience 6, Iter 22, disc loss: 0.016958921433676392, policy loss: 4.475232140593695
Experience 6, Iter 23, disc loss: 0.016393686659450356, policy loss: 4.518441816348358
Experience 6, Iter 24, disc loss: 0.01611122423135224, policy loss: 4.541814308296934
Experience 6, Iter 25, disc loss: 0.016037171683825957, policy loss: 4.5505617112804675
Experience 6, Iter 26, disc loss: 0.015753163304613724, policy loss: 4.569393574953069
Experience 6, Iter 27, disc loss: 0.01568909977919184, policy loss: 4.568130416648916
Experience 6, Iter 28, disc loss: 0.015966994580417726, policy loss: 4.541606505701045
Experience 6, Iter 29, disc loss: 0.015502673104431127, policy loss: 4.587211547772627
Experience 6, Iter 30, disc loss: 0.015858117533810834, policy loss: 4.544907305972583
Experience 6, Iter 31, disc loss: 0.015705831649621076, policy loss: 4.558930492782833
Experience 6, Iter 32, disc loss: 0.015793160303783967, policy loss: 4.548154520451958
Experience 6, Iter 33, disc loss: 0.015324211047074925, policy loss: 4.583482411367326
Experience 6, Iter 34, disc loss: 0.015493278869044431, policy loss: 4.568908939405357
Experience 6, Iter 35, disc loss: 0.015391076973497668, policy loss: 4.572422544597259
Experience 6, Iter 36, disc loss: 0.014847373444658554, policy loss: 4.623677629852429
Experience 6, Iter 37, disc loss: 0.015140420011778753, policy loss: 4.596573751286103
Experience 6, Iter 38, disc loss: 0.015132326082461452, policy loss: 4.591079938953655
Experience 6, Iter 39, disc loss: 0.014955167571677447, policy loss: 4.598606264416528
Experience 6, Iter 40, disc loss: 0.014986238274646447, policy loss: 4.5974795583100105
Experience 6, Iter 41, disc loss: 0.015304170061960805, policy loss: 4.561851210801336
Experience 6, Iter 42, disc loss: 0.014938774447621903, policy loss: 4.602153382425317
Experience 6, Iter 43, disc loss: 0.014740456346515121, policy loss: 4.614314940372557
Experience 6, Iter 44, disc loss: 0.014565560108672342, policy loss: 4.637625030443569
Experience 6, Iter 45, disc loss: 0.014430821421558255, policy loss: 4.640789964692493
Experience 6, Iter 46, disc loss: 0.01401974045426682, policy loss: 4.676818417952472
Experience 6, Iter 47, disc loss: 0.014112161304846882, policy loss: 4.6670255543289265
Experience 6, Iter 48, disc loss: 0.01383097634494367, policy loss: 4.693013353033546
Experience 6, Iter 49, disc loss: 0.014496132807774533, policy loss: 4.613261107307728
Experience 6, Iter 50, disc loss: 0.013932149390254103, policy loss: 4.681770672888155
Experience 6, Iter 51, disc loss: 0.0138532419102008, policy loss: 4.685406984997309
Experience 6, Iter 52, disc loss: 0.013378892975732784, policy loss: 4.72820742332196
Experience 6, Iter 53, disc loss: 0.013743024774170692, policy loss: 4.68505058074705
Experience 6, Iter 54, disc loss: 0.012947503335311037, policy loss: 4.776774866504172
Experience 6, Iter 55, disc loss: 0.012760724204843133, policy loss: 4.788154744606201
Experience 6, Iter 56, disc loss: 0.013065826307131722, policy loss: 4.755242033979389
Experience 6, Iter 57, disc loss: 0.012659881256683818, policy loss: 4.794729711805214
Experience 6, Iter 58, disc loss: 0.012554585449980022, policy loss: 4.8063598677911505
Experience 6, Iter 59, disc loss: 0.012637900029122967, policy loss: 4.787722044175262
Experience 6, Iter 60, disc loss: 0.012725046417736651, policy loss: 4.780393180037373
Experience 6, Iter 61, disc loss: 0.012906798751938009, policy loss: 4.7569195497335865
Experience 6, Iter 62, disc loss: 0.012545807181719573, policy loss: 4.7984208841174585
Experience 6, Iter 63, disc loss: 0.012429535155106714, policy loss: 4.805247631786571
Experience 6, Iter 64, disc loss: 0.012265479869628568, policy loss: 4.821025042755459
Experience 6, Iter 65, disc loss: 0.011767885581493578, policy loss: 4.885051113655229
Experience 6, Iter 66, disc loss: 0.011509499617288645, policy loss: 4.913060151578572
Experience 6, Iter 67, disc loss: 0.012249711880218568, policy loss: 4.8227739147319655
Experience 6, Iter 68, disc loss: 0.01203881101241803, policy loss: 4.84331310412709
Experience 6, Iter 69, disc loss: 0.011850152407107618, policy loss: 4.857519147468292
Experience 6, Iter 70, disc loss: 0.01189583210800977, policy loss: 4.854568866772188
Experience 6, Iter 71, disc loss: 0.010953098286198117, policy loss: 4.969566135188163
Experience 6, Iter 72, disc loss: 0.011421767675790204, policy loss: 4.898002894659092
Experience 6, Iter 73, disc loss: 0.011705011061838848, policy loss: 4.874818045109231
Experience 6, Iter 74, disc loss: 0.01126765995970863, policy loss: 4.930348984824351
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0126],
        [0.0701],
        [0.8502],
        [0.0085]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1578, 0.5053, 0.4298, 0.0114, 0.0025, 1.7062]],

        [[0.1578, 0.5053, 0.4298, 0.0114, 0.0025, 1.7062]],

        [[0.1578, 0.5053, 0.4298, 0.0114, 0.0025, 1.7062]],

        [[0.1578, 0.5053, 0.4298, 0.0114, 0.0025, 1.7062]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0505, 0.2804, 3.4009, 0.0341], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0505, 0.2804, 3.4009, 0.0341])
N: 105
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([421.0000, 421.0000, 421.0000, 421.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.444
Iter 2/2000 - Loss: 2.470
Iter 3/2000 - Loss: 2.343
Iter 4/2000 - Loss: 2.302
Iter 5/2000 - Loss: 2.287
Iter 6/2000 - Loss: 2.208
Iter 7/2000 - Loss: 2.102
Iter 8/2000 - Loss: 2.009
Iter 9/2000 - Loss: 1.921
Iter 10/2000 - Loss: 1.814
Iter 11/2000 - Loss: 1.682
Iter 12/2000 - Loss: 1.534
Iter 13/2000 - Loss: 1.380
Iter 14/2000 - Loss: 1.223
Iter 15/2000 - Loss: 1.057
Iter 16/2000 - Loss: 0.880
Iter 17/2000 - Loss: 0.690
Iter 18/2000 - Loss: 0.488
Iter 19/2000 - Loss: 0.277
Iter 20/2000 - Loss: 0.060
Iter 1981/2000 - Loss: -7.518
Iter 1982/2000 - Loss: -7.518
Iter 1983/2000 - Loss: -7.518
Iter 1984/2000 - Loss: -7.518
Iter 1985/2000 - Loss: -7.518
Iter 1986/2000 - Loss: -7.518
Iter 1987/2000 - Loss: -7.518
Iter 1988/2000 - Loss: -7.518
Iter 1989/2000 - Loss: -7.518
Iter 1990/2000 - Loss: -7.518
Iter 1991/2000 - Loss: -7.518
Iter 1992/2000 - Loss: -7.519
Iter 1993/2000 - Loss: -7.519
Iter 1994/2000 - Loss: -7.519
Iter 1995/2000 - Loss: -7.519
Iter 1996/2000 - Loss: -7.519
Iter 1997/2000 - Loss: -7.519
Iter 1998/2000 - Loss: -7.519
Iter 1999/2000 - Loss: -7.519
Iter 2000/2000 - Loss: -7.519
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[21.2611, 15.7051, 29.9830, 15.0190, 15.0259, 44.3494]],

        [[36.8901, 39.1320, 11.7175,  1.2605,  4.4220, 22.9828]],

        [[36.7142, 36.4070, 12.5765,  1.0061,  6.7487, 15.0968]],

        [[39.7931, 52.6675, 12.5280,  3.4513,  2.1363, 31.7343]]])
Signal Variance: tensor([ 0.3278,  1.9783, 15.0529,  0.2559])
Estimated target variance: tensor([0.0505, 0.2804, 3.4009, 0.0341])
N: 105
Signal to noise ratio: tensor([28.4603, 81.3039, 83.9160, 30.8309])
Bound on condition number: tensor([ 85049.7129, 694084.9052, 739400.7962,  99807.8402])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.011540387396559968, policy loss: 4.887191584685315
Experience 7, Iter 1, disc loss: 0.01218674906028168, policy loss: 4.813615155023593
Experience 7, Iter 2, disc loss: 0.012356659931262988, policy loss: 4.790057011538011
Experience 7, Iter 3, disc loss: 0.01085350895574188, policy loss: 5.002081577085219
Experience 7, Iter 4, disc loss: 0.010685845077073184, policy loss: 5.0146947445434655
Experience 7, Iter 5, disc loss: 0.011604083412357944, policy loss: 4.887792767784796
Experience 7, Iter 6, disc loss: 0.012004478881723091, policy loss: 4.820445311298107
Experience 7, Iter 7, disc loss: 0.012328787846888272, policy loss: 4.788796742089881
Experience 7, Iter 8, disc loss: 0.012471795105735744, policy loss: 4.768143760592277
Experience 7, Iter 9, disc loss: 0.013837436435983211, policy loss: 4.62462242422864
Experience 7, Iter 10, disc loss: 0.013505006634389364, policy loss: 4.648866056771943
Experience 7, Iter 11, disc loss: 0.014453533038556508, policy loss: 4.579306099741539
Experience 7, Iter 12, disc loss: 0.014152783356040892, policy loss: 4.59711860484116
Experience 7, Iter 13, disc loss: 0.013532582036157691, policy loss: 4.662894887915331
Experience 7, Iter 14, disc loss: 0.014179603526755317, policy loss: 4.599183099810146
Experience 7, Iter 15, disc loss: 0.01432284227510637, policy loss: 4.5881478367619035
Experience 7, Iter 16, disc loss: 0.013369884538707332, policy loss: 4.696714286460877
Experience 7, Iter 17, disc loss: 0.01216806639282804, policy loss: 4.837273926578316
Experience 7, Iter 18, disc loss: 0.014113883084528274, policy loss: 4.619048420158265
Experience 7, Iter 19, disc loss: 0.013311747038311658, policy loss: 4.727082038749737
Experience 7, Iter 20, disc loss: 0.013110267753071354, policy loss: 4.719269358017842
Experience 7, Iter 21, disc loss: 0.012186896265819282, policy loss: 4.842408718578542
Experience 7, Iter 22, disc loss: 0.010899953425068384, policy loss: 5.01369869399844
Experience 7, Iter 23, disc loss: 0.012241384575436538, policy loss: 4.816989544577474
Experience 7, Iter 24, disc loss: 0.012985749732169538, policy loss: 4.724649220841375
Experience 7, Iter 25, disc loss: 0.012408686350279776, policy loss: 4.810959088163723
Experience 7, Iter 26, disc loss: 0.012839469252253667, policy loss: 4.793086221656487
Experience 7, Iter 27, disc loss: 0.01542684303843847, policy loss: 4.546820021251362
Experience 7, Iter 28, disc loss: 0.015791595773745948, policy loss: 4.561665921503421
Experience 7, Iter 29, disc loss: 0.01713742228248387, policy loss: 4.512124405683837
Experience 7, Iter 30, disc loss: 0.01644368347057272, policy loss: 4.4835722203067165
Experience 7, Iter 31, disc loss: 0.016046686003238414, policy loss: 4.58609604254349
Experience 7, Iter 32, disc loss: 0.02132285132969054, policy loss: 4.315013415213429
Experience 7, Iter 33, disc loss: 0.021270879172539114, policy loss: 4.331866954161219
Experience 7, Iter 34, disc loss: 0.017594657049880624, policy loss: 4.497789937547113
Experience 7, Iter 35, disc loss: 0.017139891753664444, policy loss: 4.518153697002966
Experience 7, Iter 36, disc loss: 0.014897861946080355, policy loss: 4.698847233480459
Experience 7, Iter 37, disc loss: 0.015328740937402423, policy loss: 4.613862712539273
Experience 7, Iter 38, disc loss: 0.014719012842264444, policy loss: 4.620746713596835
Experience 7, Iter 39, disc loss: 0.019972826914855475, policy loss: 4.347482410099925
Experience 7, Iter 40, disc loss: 0.01622040090536233, policy loss: 4.560214469493355
Experience 7, Iter 41, disc loss: 0.01472806211006042, policy loss: 4.879900735349224
Experience 7, Iter 42, disc loss: 0.011016860216556545, policy loss: 5.080306201114547
Experience 7, Iter 43, disc loss: 0.012258170822256914, policy loss: 4.9249319526354896
Experience 7, Iter 44, disc loss: 0.012718141520821558, policy loss: 4.832553899350492
Experience 7, Iter 45, disc loss: 0.019370504181972243, policy loss: 4.467250730010179
Experience 7, Iter 46, disc loss: 0.02894676439131462, policy loss: 4.127150789299385
Experience 7, Iter 47, disc loss: 0.019055119722952533, policy loss: 4.480956524715442
Experience 7, Iter 48, disc loss: 0.013457373720264336, policy loss: 4.845671825531465
Experience 7, Iter 49, disc loss: 0.015013111660075169, policy loss: 4.774056168404998
Experience 7, Iter 50, disc loss: 0.012500843399841843, policy loss: 4.89640280956499
Experience 7, Iter 51, disc loss: 0.013285527690671532, policy loss: 4.8397005596092155
Experience 7, Iter 52, disc loss: 0.020894018210041297, policy loss: 4.581824163277908
Experience 7, Iter 53, disc loss: 0.02207770490953398, policy loss: 4.365314014729968
Experience 7, Iter 54, disc loss: 0.021810998735556024, policy loss: 4.535721319613227
Experience 7, Iter 55, disc loss: 0.01602745232184363, policy loss: 4.63829224965996
Experience 7, Iter 56, disc loss: 0.013851551064596984, policy loss: 4.806512788350023
Experience 7, Iter 57, disc loss: 0.012274714105046203, policy loss: 4.89601106138808
Experience 7, Iter 58, disc loss: 0.012002390879530407, policy loss: 4.9571058352153505
Experience 7, Iter 59, disc loss: 0.011423630757789197, policy loss: 5.007733525535373
Experience 7, Iter 60, disc loss: 0.010424009639756603, policy loss: 5.129644087465294
Experience 7, Iter 61, disc loss: 0.010802223860979374, policy loss: 5.1664312569938335
Experience 7, Iter 62, disc loss: 0.009652677440024739, policy loss: 5.242395276130443
Experience 7, Iter 63, disc loss: 0.009544753304420773, policy loss: 5.259808624717236
Experience 7, Iter 64, disc loss: 0.00926139033323339, policy loss: 5.334910824294799
Experience 7, Iter 65, disc loss: 0.009154583727626512, policy loss: 5.3410906040807955
Experience 7, Iter 66, disc loss: 0.008610198700450006, policy loss: 5.4001733363644275
Experience 7, Iter 67, disc loss: 0.009326510534079006, policy loss: 5.261096493120299
Experience 7, Iter 68, disc loss: 0.009457850352877386, policy loss: 5.226751283709677
Experience 7, Iter 69, disc loss: 0.008970846504765541, policy loss: 5.350906962572152
Experience 7, Iter 70, disc loss: 0.009964993682692967, policy loss: 5.141144710455768
Experience 7, Iter 71, disc loss: 0.009694399928569332, policy loss: 5.175826905946234
Experience 7, Iter 72, disc loss: 0.00944544905831923, policy loss: 5.201187419968818
Experience 7, Iter 73, disc loss: 0.009483004158646119, policy loss: 5.19177599375945
Experience 7, Iter 74, disc loss: 0.010861341718101663, policy loss: 5.022917429606104
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0112],
        [0.0697],
        [0.8760],
        [0.0087]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1395, 0.4493, 0.4298, 0.0112, 0.0022, 1.6216]],

        [[0.1395, 0.4493, 0.4298, 0.0112, 0.0022, 1.6216]],

        [[0.1395, 0.4493, 0.4298, 0.0112, 0.0022, 1.6216]],

        [[0.1395, 0.4493, 0.4298, 0.0112, 0.0022, 1.6216]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0448, 0.2788, 3.5041, 0.0350], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0448, 0.2788, 3.5041, 0.0350])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.421
Iter 2/2000 - Loss: 2.474
Iter 3/2000 - Loss: 2.329
Iter 4/2000 - Loss: 2.300
Iter 5/2000 - Loss: 2.298
Iter 6/2000 - Loss: 2.218
Iter 7/2000 - Loss: 2.115
Iter 8/2000 - Loss: 2.030
Iter 9/2000 - Loss: 1.950
Iter 10/2000 - Loss: 1.851
Iter 11/2000 - Loss: 1.725
Iter 12/2000 - Loss: 1.583
Iter 13/2000 - Loss: 1.433
Iter 14/2000 - Loss: 1.276
Iter 15/2000 - Loss: 1.109
Iter 16/2000 - Loss: 0.929
Iter 17/2000 - Loss: 0.735
Iter 18/2000 - Loss: 0.527
Iter 19/2000 - Loss: 0.309
Iter 20/2000 - Loss: 0.085
Iter 1981/2000 - Loss: -7.757
Iter 1982/2000 - Loss: -7.757
Iter 1983/2000 - Loss: -7.757
Iter 1984/2000 - Loss: -7.757
Iter 1985/2000 - Loss: -7.757
Iter 1986/2000 - Loss: -7.757
Iter 1987/2000 - Loss: -7.757
Iter 1988/2000 - Loss: -7.757
Iter 1989/2000 - Loss: -7.757
Iter 1990/2000 - Loss: -7.757
Iter 1991/2000 - Loss: -7.757
Iter 1992/2000 - Loss: -7.757
Iter 1993/2000 - Loss: -7.757
Iter 1994/2000 - Loss: -7.757
Iter 1995/2000 - Loss: -7.757
Iter 1996/2000 - Loss: -7.757
Iter 1997/2000 - Loss: -7.757
Iter 1998/2000 - Loss: -7.757
Iter 1999/2000 - Loss: -7.757
Iter 2000/2000 - Loss: -7.757
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[16.5928, 12.3768, 28.8920, 14.0147, 13.7220, 41.6707]],

        [[34.3013, 37.5693, 13.1792,  1.3520,  2.2900, 20.4961]],

        [[33.0005, 38.7289, 12.3888,  0.9586,  6.2577, 16.5947]],

        [[37.3277, 51.2866, 12.8718,  3.3855,  1.9745, 31.7699]]])
Signal Variance: tensor([ 0.2936,  1.7947, 15.0873,  0.2497])
Estimated target variance: tensor([0.0448, 0.2788, 3.5041, 0.0350])
N: 120
Signal to noise ratio: tensor([28.3011, 76.1978, 86.7644, 30.6129])
Bound on condition number: tensor([ 96114.9470, 696733.4298, 903367.5747, 112458.8033])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.011351785105421251, policy loss: 4.96182808851239
Experience 8, Iter 1, disc loss: 0.01058206447163684, policy loss: 5.016901090440444
Experience 8, Iter 2, disc loss: 0.01051909146221575, policy loss: 5.017247806434611
Experience 8, Iter 3, disc loss: 0.009746496494657034, policy loss: 5.118480084498005
Experience 8, Iter 4, disc loss: 0.0089444557803787, policy loss: 5.279733704977991
Experience 8, Iter 5, disc loss: 0.009248076611960096, policy loss: 5.189519190148039
Experience 8, Iter 6, disc loss: 0.00973908975969531, policy loss: 5.107764986639555
Experience 8, Iter 7, disc loss: 0.00926516000304808, policy loss: 5.165414908180763
Experience 8, Iter 8, disc loss: 0.009033553966216844, policy loss: 5.195195575932836
Experience 8, Iter 9, disc loss: 0.00898758071014384, policy loss: 5.199592277860554
Experience 8, Iter 10, disc loss: 0.008404169783717342, policy loss: 5.318058896810867
Experience 8, Iter 11, disc loss: 0.009676927965700256, policy loss: 5.066776463264675
Experience 8, Iter 12, disc loss: 0.009373401540866414, policy loss: 5.11532865457613
Experience 8, Iter 13, disc loss: 0.009916553176034355, policy loss: 5.025240945957199
Experience 8, Iter 14, disc loss: 0.010234149851728581, policy loss: 4.9842475979050045
Experience 8, Iter 15, disc loss: 0.011523801356780002, policy loss: 4.843246167507559
Experience 8, Iter 16, disc loss: 0.010811480436915695, policy loss: 4.968773201169718
Experience 8, Iter 17, disc loss: 0.009911913184604937, policy loss: 5.036825217787835
Experience 8, Iter 18, disc loss: 0.010494167640884127, policy loss: 5.0174803811202935
Experience 8, Iter 19, disc loss: 0.010432602582704951, policy loss: 4.984620307417693
Experience 8, Iter 20, disc loss: 0.0100776817369208, policy loss: 5.007196140273718
Experience 8, Iter 21, disc loss: 0.010013691726026525, policy loss: 5.005228000446933
Experience 8, Iter 22, disc loss: 0.009389231449311452, policy loss: 5.127461611083465
Experience 8, Iter 23, disc loss: 0.00901788641259888, policy loss: 5.16350234227846
Experience 8, Iter 24, disc loss: 0.01135273298856565, policy loss: 5.065604486176326
Experience 8, Iter 25, disc loss: 0.01047556479462547, policy loss: 4.932533492177484
Experience 8, Iter 26, disc loss: 0.011460568987315848, policy loss: 4.841064138528482
Experience 8, Iter 27, disc loss: 0.014939727449698764, policy loss: 4.888508996195027
Experience 8, Iter 28, disc loss: 0.010160295017618191, policy loss: 4.997187549221901
Experience 8, Iter 29, disc loss: 0.013656725942262901, policy loss: 5.03536809550401
Experience 8, Iter 30, disc loss: 0.013050106250678942, policy loss: 5.1220108405718054
Experience 8, Iter 31, disc loss: 0.009750760255225255, policy loss: 5.120217170849106
Experience 8, Iter 32, disc loss: 0.011132297615231716, policy loss: 5.099754396576103
Experience 8, Iter 33, disc loss: 0.018986049907727672, policy loss: 5.014236940724599
Experience 8, Iter 34, disc loss: 0.011126971510452516, policy loss: 4.973176933902919
Experience 8, Iter 35, disc loss: 0.01124566934790142, policy loss: 4.997633514502175
Experience 8, Iter 36, disc loss: 0.012849749153716783, policy loss: 4.993302150095177
Experience 8, Iter 37, disc loss: 0.02456713557091974, policy loss: 4.991466266444989
Experience 8, Iter 38, disc loss: 0.009242619414477838, policy loss: 5.127491200033436
Experience 8, Iter 39, disc loss: 0.010785588239007129, policy loss: 5.102386124879627
Experience 8, Iter 40, disc loss: 0.02724002966870688, policy loss: 4.880185093981996
Experience 8, Iter 41, disc loss: 0.01123488494319389, policy loss: 5.031470716027336
Experience 8, Iter 42, disc loss: 0.011177056040764313, policy loss: 5.142543413259649
Experience 8, Iter 43, disc loss: 0.009159920905682023, policy loss: 5.265645119344331
Experience 8, Iter 44, disc loss: 0.009780068649012192, policy loss: 5.265423232909197
Experience 8, Iter 45, disc loss: 0.007794440152670333, policy loss: 5.383043956141517
Experience 8, Iter 46, disc loss: 0.028047885392608062, policy loss: 5.283832388276277
Experience 8, Iter 47, disc loss: 0.00927654204042719, policy loss: 5.2800669109118115
Experience 8, Iter 48, disc loss: 0.009770511006599685, policy loss: 5.23212318102469
Experience 8, Iter 49, disc loss: 0.024230178128677997, policy loss: 5.09108015072548
Experience 8, Iter 50, disc loss: 0.06341598197723852, policy loss: 4.688051406792018
Experience 8, Iter 51, disc loss: 0.039068243374580584, policy loss: 5.055641861472134
Experience 8, Iter 52, disc loss: 0.04418995695041159, policy loss: 5.253683054756229
Experience 8, Iter 53, disc loss: 0.01780897857910524, policy loss: 5.514157619138807
Experience 8, Iter 54, disc loss: 0.011033796229622568, policy loss: 5.6980290754142935
Experience 8, Iter 55, disc loss: 0.006602402218375699, policy loss: 5.942236960784573
Experience 8, Iter 56, disc loss: 0.005946441216504182, policy loss: 6.276213688057318
Experience 8, Iter 57, disc loss: 0.0056900665066698655, policy loss: 6.478869262504142
Experience 8, Iter 58, disc loss: 0.0054803836830806165, policy loss: 6.655170070457851
Experience 8, Iter 59, disc loss: 0.005313448426105125, policy loss: 6.855027603348099
Experience 8, Iter 60, disc loss: 0.005412778390325431, policy loss: 6.8228816556119
Experience 8, Iter 61, disc loss: 0.005351016519482716, policy loss: 6.892328406772898
Experience 8, Iter 62, disc loss: 0.005268235287206236, policy loss: 6.957575186893301
Experience 8, Iter 63, disc loss: 0.0053130530760949965, policy loss: 6.909071194543156
Experience 8, Iter 64, disc loss: 0.005306075155292787, policy loss: 6.8967117036817225
Experience 8, Iter 65, disc loss: 0.005308230099701876, policy loss: 6.894063518717148
Experience 8, Iter 66, disc loss: 0.0053115273128654565, policy loss: 6.798906573095752
Experience 8, Iter 67, disc loss: 0.0055860354892983514, policy loss: 6.5867347944239585
Experience 8, Iter 68, disc loss: 0.005529623119095016, policy loss: 6.572960051878965
Experience 8, Iter 69, disc loss: 0.005541937630490429, policy loss: 6.484852138253017
Experience 8, Iter 70, disc loss: 0.00625652644119872, policy loss: 6.263953031113699
Experience 8, Iter 71, disc loss: 0.005656809615389699, policy loss: 6.34322936230079
Experience 8, Iter 72, disc loss: 0.005860766854845205, policy loss: 6.183845473324542
Experience 8, Iter 73, disc loss: 0.006035074383835284, policy loss: 6.0698736931048956
Experience 8, Iter 74, disc loss: 0.006185524058978175, policy loss: 5.974858701845645
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0100],
        [0.0689],
        [0.8630],
        [0.0093]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1246, 0.4029, 0.4520, 0.0113, 0.0020, 1.6267]],

        [[0.1246, 0.4029, 0.4520, 0.0113, 0.0020, 1.6267]],

        [[0.1246, 0.4029, 0.4520, 0.0113, 0.0020, 1.6267]],

        [[0.1246, 0.4029, 0.4520, 0.0113, 0.0020, 1.6267]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0402, 0.2757, 3.4519, 0.0373], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0402, 0.2757, 3.4519, 0.0373])
N: 135
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([541.0000, 541.0000, 541.0000, 541.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.382
Iter 2/2000 - Loss: 2.466
Iter 3/2000 - Loss: 2.297
Iter 4/2000 - Loss: 2.273
Iter 5/2000 - Loss: 2.282
Iter 6/2000 - Loss: 2.200
Iter 7/2000 - Loss: 2.089
Iter 8/2000 - Loss: 1.999
Iter 9/2000 - Loss: 1.920
Iter 10/2000 - Loss: 1.824
Iter 11/2000 - Loss: 1.699
Iter 12/2000 - Loss: 1.555
Iter 13/2000 - Loss: 1.401
Iter 14/2000 - Loss: 1.240
Iter 15/2000 - Loss: 1.071
Iter 16/2000 - Loss: 0.891
Iter 17/2000 - Loss: 0.697
Iter 18/2000 - Loss: 0.491
Iter 19/2000 - Loss: 0.274
Iter 20/2000 - Loss: 0.050
Iter 1981/2000 - Loss: -7.871
Iter 1982/2000 - Loss: -7.871
Iter 1983/2000 - Loss: -7.871
Iter 1984/2000 - Loss: -7.871
Iter 1985/2000 - Loss: -7.871
Iter 1986/2000 - Loss: -7.871
Iter 1987/2000 - Loss: -7.871
Iter 1988/2000 - Loss: -7.871
Iter 1989/2000 - Loss: -7.871
Iter 1990/2000 - Loss: -7.871
Iter 1991/2000 - Loss: -7.871
Iter 1992/2000 - Loss: -7.871
Iter 1993/2000 - Loss: -7.871
Iter 1994/2000 - Loss: -7.871
Iter 1995/2000 - Loss: -7.871
Iter 1996/2000 - Loss: -7.871
Iter 1997/2000 - Loss: -7.871
Iter 1998/2000 - Loss: -7.871
Iter 1999/2000 - Loss: -7.871
Iter 2000/2000 - Loss: -7.871
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.6796,  9.5508, 29.4776, 14.1387, 13.3060, 41.2854]],

        [[35.0794, 38.2814, 11.6477,  1.3440,  4.4986, 26.6642]],

        [[33.0592, 47.5941, 10.4075,  1.0125,  2.6162, 20.1177]],

        [[36.2795, 47.6260, 14.7941,  3.4910,  1.6721, 26.4952]]])
Signal Variance: tensor([ 0.2232,  2.3300, 16.3133,  0.2949])
Estimated target variance: tensor([0.0402, 0.2757, 3.4519, 0.0373])
N: 135
Signal to noise ratio: tensor([25.1031, 88.5821, 89.5452, 33.5438])
Bound on condition number: tensor([  85073.5716, 1059316.6893, 1082476.4983,  151900.7655])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.00610739847206318, policy loss: 5.969743032708406
Experience 9, Iter 1, disc loss: 0.006451221560516768, policy loss: 5.823540011932852
Experience 9, Iter 2, disc loss: 0.006654582183582493, policy loss: 5.7502865368488925
Experience 9, Iter 3, disc loss: 0.007539391148077848, policy loss: 5.505769256694674
Experience 9, Iter 4, disc loss: 0.0075725162154321405, policy loss: 5.48789740740966
Experience 9, Iter 5, disc loss: 0.007915523984501675, policy loss: 5.405253064425179
Experience 9, Iter 6, disc loss: 0.00815205725052277, policy loss: 5.368583168885049
Experience 9, Iter 7, disc loss: 0.008368043960118973, policy loss: 5.297283141444818
Experience 9, Iter 8, disc loss: 0.008864228564336891, policy loss: 5.239972610455208
Experience 9, Iter 9, disc loss: 0.01104876227747047, policy loss: 4.948024424302906
Experience 9, Iter 10, disc loss: 0.011255052480432482, policy loss: 4.958786537464416
Experience 9, Iter 11, disc loss: 0.012676885113685907, policy loss: 4.834079909054079
Experience 9, Iter 12, disc loss: 0.009228784550958187, policy loss: 5.15024917908535
Experience 9, Iter 13, disc loss: 0.009602024816483522, policy loss: 5.143687222379949
Experience 9, Iter 14, disc loss: 0.008584536324138872, policy loss: 5.2473411884232695
Experience 9, Iter 15, disc loss: 0.009824775796442982, policy loss: 5.294149908186757
Experience 9, Iter 16, disc loss: 0.00832675411153207, policy loss: 5.44543348727398
Experience 9, Iter 17, disc loss: 0.07168240752613574, policy loss: 5.472282601251436
Experience 9, Iter 18, disc loss: 0.013257603916299278, policy loss: 5.9298991109290675
Experience 9, Iter 19, disc loss: 0.03895431719072756, policy loss: 6.203339741903703
Experience 9, Iter 20, disc loss: 0.042296678705469534, policy loss: 6.256550892889173
Experience 9, Iter 21, disc loss: 0.06704626367485353, policy loss: 6.757785407574145
Experience 9, Iter 22, disc loss: 0.1932835488817104, policy loss: 6.337505185416347
Experience 9, Iter 23, disc loss: 0.2704082244224637, policy loss: 6.12426333903635
Experience 9, Iter 24, disc loss: 0.28906950311090834, policy loss: 6.040179224607772
Experience 9, Iter 25, disc loss: 0.30549570944207965, policy loss: 6.328291864172492
Experience 9, Iter 26, disc loss: 0.16853720936594813, policy loss: 6.510814586638293
Experience 9, Iter 27, disc loss: 0.14684545177209984, policy loss: 6.745558987002578
Experience 9, Iter 28, disc loss: 0.3192993364756109, policy loss: 6.371675014196167
Experience 9, Iter 29, disc loss: 0.5924283328805912, policy loss: 5.435998093133965
Experience 9, Iter 30, disc loss: 0.8474351905638331, policy loss: 5.082342081067016
Experience 9, Iter 31, disc loss: 1.3636925013362502, policy loss: 4.507716163090294
Experience 9, Iter 32, disc loss: 0.5267121009081752, policy loss: 5.329284521600053
Experience 9, Iter 33, disc loss: 0.9694978763306953, policy loss: 5.559249344404403
Experience 9, Iter 34, disc loss: 0.4298684678999015, policy loss: 6.8415198025437025
Experience 9, Iter 35, disc loss: 0.25097837464544176, policy loss: 6.9950539019701505
Experience 9, Iter 36, disc loss: 0.23108112603881514, policy loss: 8.15529269795674
Experience 9, Iter 37, disc loss: 0.3250578907167673, policy loss: 7.351176100749833
Experience 9, Iter 38, disc loss: 0.17765107925604806, policy loss: 8.195401011559898
Experience 9, Iter 39, disc loss: 0.41090312727511225, policy loss: 7.923699100580448
Experience 9, Iter 40, disc loss: 0.31441626470516143, policy loss: 8.152104890820485
Experience 9, Iter 41, disc loss: 0.1866421303853215, policy loss: 8.84252288842323
Experience 9, Iter 42, disc loss: 0.14905000991358094, policy loss: 8.104312518889246
Experience 9, Iter 43, disc loss: 0.1439093566267614, policy loss: 8.074230173062972
Experience 9, Iter 44, disc loss: 0.13529538833823218, policy loss: 7.910913982115334
Experience 9, Iter 45, disc loss: 0.10081520899779367, policy loss: 7.74203416992422
Experience 9, Iter 46, disc loss: 0.13989281111243573, policy loss: 7.593790097460519
Experience 9, Iter 47, disc loss: 0.05409340776401447, policy loss: 7.394966499270802
Experience 9, Iter 48, disc loss: 0.12222128012643127, policy loss: 7.233528463712682
Experience 9, Iter 49, disc loss: 0.25138011175168595, policy loss: 6.67458355647541
Experience 9, Iter 50, disc loss: 0.10542811232943303, policy loss: 6.376754530082463
Experience 9, Iter 51, disc loss: 0.12744349876502634, policy loss: 6.984207524973519
Experience 9, Iter 52, disc loss: 0.09723512177079427, policy loss: 6.775930944466307
Experience 9, Iter 53, disc loss: 0.01912861297103717, policy loss: 6.81798495066986
Experience 9, Iter 54, disc loss: 0.01814434348678189, policy loss: 6.607788602071154
Experience 9, Iter 55, disc loss: 0.016244070784035263, policy loss: 6.554528834300187
Experience 9, Iter 56, disc loss: 0.014802530543032925, policy loss: 6.778054362937893
Experience 9, Iter 57, disc loss: 0.04225377763904115, policy loss: 6.675490681467888
Experience 9, Iter 58, disc loss: 0.09583704367820195, policy loss: 6.889803468403842
Experience 9, Iter 59, disc loss: 0.23677735442651743, policy loss: 6.604104879239291
Experience 9, Iter 60, disc loss: 0.36767507169112434, policy loss: 6.246184356908181
Experience 9, Iter 61, disc loss: 0.3827603495966839, policy loss: 6.321125500584871
Experience 9, Iter 62, disc loss: 0.09397821872962264, policy loss: 7.309407061799474
Experience 9, Iter 63, disc loss: 0.2668486129668604, policy loss: 6.307136483262223
Experience 9, Iter 64, disc loss: 0.14867847799066813, policy loss: 6.865033394365035
Experience 9, Iter 65, disc loss: 0.05858485567382267, policy loss: 6.989733581225837
Experience 9, Iter 66, disc loss: 0.04675220735552209, policy loss: 7.1710349185978925
Experience 9, Iter 67, disc loss: 0.16179207228870876, policy loss: 6.5223122699933285
Experience 9, Iter 68, disc loss: 0.28723267632788546, policy loss: 6.687808770236471
Experience 9, Iter 69, disc loss: 0.2275837075400269, policy loss: 6.884982058268001
Experience 9, Iter 70, disc loss: 0.2501935271495318, policy loss: 7.045842379874221
Experience 9, Iter 71, disc loss: 0.19857431211989543, policy loss: 7.18005576645687
Experience 9, Iter 72, disc loss: 0.16605435752247233, policy loss: 7.009076647276613
Experience 9, Iter 73, disc loss: 0.044172290756022976, policy loss: 7.33048449687989
Experience 9, Iter 74, disc loss: 0.24524574326420562, policy loss: 6.536009411848634
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0093],
        [0.0761],
        [0.9229],
        [0.0116]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1160, 0.3751, 0.5532, 0.0139, 0.0030, 1.9130]],

        [[0.1160, 0.3751, 0.5532, 0.0139, 0.0030, 1.9130]],

        [[0.1160, 0.3751, 0.5532, 0.0139, 0.0030, 1.9130]],

        [[0.1160, 0.3751, 0.5532, 0.0139, 0.0030, 1.9130]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0372, 0.3044, 3.6915, 0.0463], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0372, 0.3044, 3.6915, 0.0463])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.506
Iter 2/2000 - Loss: 2.584
Iter 3/2000 - Loss: 2.409
Iter 4/2000 - Loss: 2.374
Iter 5/2000 - Loss: 2.368
Iter 6/2000 - Loss: 2.274
Iter 7/2000 - Loss: 2.154
Iter 8/2000 - Loss: 2.055
Iter 9/2000 - Loss: 1.966
Iter 10/2000 - Loss: 1.858
Iter 11/2000 - Loss: 1.722
Iter 12/2000 - Loss: 1.568
Iter 13/2000 - Loss: 1.407
Iter 14/2000 - Loss: 1.242
Iter 15/2000 - Loss: 1.071
Iter 16/2000 - Loss: 0.890
Iter 17/2000 - Loss: 0.697
Iter 18/2000 - Loss: 0.492
Iter 19/2000 - Loss: 0.278
Iter 20/2000 - Loss: 0.056
Iter 1981/2000 - Loss: -7.808
Iter 1982/2000 - Loss: -7.809
Iter 1983/2000 - Loss: -7.809
Iter 1984/2000 - Loss: -7.809
Iter 1985/2000 - Loss: -7.809
Iter 1986/2000 - Loss: -7.809
Iter 1987/2000 - Loss: -7.809
Iter 1988/2000 - Loss: -7.809
Iter 1989/2000 - Loss: -7.809
Iter 1990/2000 - Loss: -7.809
Iter 1991/2000 - Loss: -7.809
Iter 1992/2000 - Loss: -7.809
Iter 1993/2000 - Loss: -7.809
Iter 1994/2000 - Loss: -7.809
Iter 1995/2000 - Loss: -7.809
Iter 1996/2000 - Loss: -7.809
Iter 1997/2000 - Loss: -7.809
Iter 1998/2000 - Loss: -7.809
Iter 1999/2000 - Loss: -7.809
Iter 2000/2000 - Loss: -7.809
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.0176, 11.0008, 33.1283, 14.4236, 13.2488, 44.4530]],

        [[33.2357, 36.2960, 11.8494,  1.2388,  7.2816, 28.9884]],

        [[34.2343, 49.0416, 10.7487,  1.0425,  1.9181, 18.8287]],

        [[35.3852, 46.7117, 17.3888,  4.0692,  1.7255, 30.2348]]])
Signal Variance: tensor([ 0.2241,  2.5167, 15.2095,  0.3711])
Estimated target variance: tensor([0.0372, 0.3044, 3.6915, 0.0463])
N: 150
Signal to noise ratio: tensor([25.2373, 88.9728, 88.2592, 38.4405])
Bound on condition number: tensor([  95539.5611, 1187425.0791, 1168453.1195,  221651.8240])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.026375663647545077, policy loss: 7.32054182360473
Experience 10, Iter 1, disc loss: 0.028271480882839296, policy loss: 7.399187276380664
Experience 10, Iter 2, disc loss: 0.029958357914974594, policy loss: 7.342218582151369
Experience 10, Iter 3, disc loss: 0.031169257260859697, policy loss: 7.392674063792718
Experience 10, Iter 4, disc loss: 0.031985844345575165, policy loss: 7.382053840204112
Experience 10, Iter 5, disc loss: 0.032351142511919895, policy loss: 7.407123716890691
Experience 10, Iter 6, disc loss: 0.03238596901015811, policy loss: 7.323486670858351
Experience 10, Iter 7, disc loss: 0.03206964599178757, policy loss: 7.257232621790674
Experience 10, Iter 8, disc loss: 0.03130384907554072, policy loss: 7.292518174515662
Experience 10, Iter 9, disc loss: 0.03036028019235819, policy loss: 7.271637598775553
Experience 10, Iter 10, disc loss: 0.029245372060203084, policy loss: 7.2314681834943695
Experience 10, Iter 11, disc loss: 0.02809951171091666, policy loss: 7.1723482752648335
Experience 10, Iter 12, disc loss: 0.026648068696385058, policy loss: 7.294632554186134
Experience 10, Iter 13, disc loss: 0.025338105140758318, policy loss: 7.298130536240807
Experience 10, Iter 14, disc loss: 0.024094939697328448, policy loss: 7.224013195749928
Experience 10, Iter 15, disc loss: 0.022714562916209068, policy loss: 7.312268528865919
Experience 10, Iter 16, disc loss: 0.021635738637485073, policy loss: 7.179377377851823
Experience 10, Iter 17, disc loss: 0.02041048036242207, policy loss: 7.215678308000135
Experience 10, Iter 18, disc loss: 0.01941793082676334, policy loss: 7.132216345799749
Experience 10, Iter 19, disc loss: 0.0182463339487368, policy loss: 7.232015846919053
Experience 10, Iter 20, disc loss: 0.017357931468653352, policy loss: 7.150015887121674
Experience 10, Iter 21, disc loss: 0.016439365403207874, policy loss: 7.148348032151472
Experience 10, Iter 22, disc loss: 0.015695736171354686, policy loss: 7.103781783937224
Experience 10, Iter 23, disc loss: 0.01495034964613483, policy loss: 7.105512367961254
Experience 10, Iter 24, disc loss: 0.014277050287215027, policy loss: 7.052490906101097
Experience 10, Iter 25, disc loss: 0.01409246058141921, policy loss: 6.775476426081365
Experience 10, Iter 26, disc loss: 0.013218428074346771, policy loss: 6.979998483814594
Experience 10, Iter 27, disc loss: 0.03975054773252244, policy loss: 6.812111411354792
Experience 10, Iter 28, disc loss: 0.013151449145324892, policy loss: 6.807698864883248
Experience 10, Iter 29, disc loss: 0.019651785356477144, policy loss: 6.766611260670585
Experience 10, Iter 30, disc loss: 0.017163638830532135, policy loss: 6.661583134437865
Experience 10, Iter 31, disc loss: 0.019541260294158515, policy loss: 6.641704938757005
Experience 10, Iter 32, disc loss: 0.01066353205702012, policy loss: 6.930054946848277
Experience 10, Iter 33, disc loss: 0.010770804975986304, policy loss: 6.728105595516311
Experience 10, Iter 34, disc loss: 0.04467115162147789, policy loss: 6.526232207808922
Experience 10, Iter 35, disc loss: 0.01513004935709257, policy loss: 6.5059148496090184
Experience 10, Iter 36, disc loss: 0.030982447862574328, policy loss: 6.247944298082699
Experience 10, Iter 37, disc loss: 0.1769726263091449, policy loss: 5.8151452882672015
Experience 10, Iter 38, disc loss: 0.1307825601150563, policy loss: 6.214662586482698
Experience 10, Iter 39, disc loss: 0.046551793951298104, policy loss: 6.656450192542652
Experience 10, Iter 40, disc loss: 0.010647938889479805, policy loss: 6.645690282284037
Experience 10, Iter 41, disc loss: 0.22576958716168638, policy loss: 5.893821555197938
Experience 10, Iter 42, disc loss: 0.28017410250182734, policy loss: 5.559275169121038
Experience 10, Iter 43, disc loss: 0.22491614388274983, policy loss: 5.707641032323904
Experience 10, Iter 44, disc loss: 0.09254164015857287, policy loss: 6.162906097594089
Experience 10, Iter 45, disc loss: 0.013986192562367397, policy loss: 6.710456085741033
Experience 10, Iter 46, disc loss: 0.06874106882780726, policy loss: 6.8981930414782555
Experience 10, Iter 47, disc loss: 0.03984917387370087, policy loss: 6.9370970120649895
Experience 10, Iter 48, disc loss: 0.015362276911741192, policy loss: 6.814470185628339
Experience 10, Iter 49, disc loss: 0.013348280872196817, policy loss: 7.508844946783602
Experience 10, Iter 50, disc loss: 0.014152561546281566, policy loss: 7.310030081627625
Experience 10, Iter 51, disc loss: 0.014442627931621573, policy loss: 7.605081550852123
Experience 10, Iter 52, disc loss: 0.015262521831851129, policy loss: 7.582209539493519
Experience 10, Iter 53, disc loss: 0.014554303435320179, policy loss: 7.361823803186111
Experience 10, Iter 54, disc loss: 0.014275130347166086, policy loss: 7.6921209246862885
Experience 10, Iter 55, disc loss: 0.014270510553536103, policy loss: 7.715652191780499
Experience 10, Iter 56, disc loss: 0.014319850790750146, policy loss: 7.5489812455539225
Experience 10, Iter 57, disc loss: 0.014252172614446928, policy loss: 7.427526147157372
Experience 10, Iter 58, disc loss: 0.014003445521116757, policy loss: 7.526630800919728
Experience 10, Iter 59, disc loss: 0.013690477500050294, policy loss: 7.567872784093137
Experience 10, Iter 60, disc loss: 0.013391102356487647, policy loss: 7.6406347860469515
Experience 10, Iter 61, disc loss: 0.01307588251005424, policy loss: 7.5498928502436975
Experience 10, Iter 62, disc loss: 0.012712791430805484, policy loss: 7.610120588643159
Experience 10, Iter 63, disc loss: 0.012350727243878035, policy loss: 7.538019313690061
Experience 10, Iter 64, disc loss: 0.011988027797223836, policy loss: 7.498452022387158
Experience 10, Iter 65, disc loss: 0.011636924897383346, policy loss: 7.48662977817013
Experience 10, Iter 66, disc loss: 0.011117861398722555, policy loss: 7.77941340959697
Experience 10, Iter 67, disc loss: 0.01082894020273449, policy loss: 7.571269198492507
Experience 10, Iter 68, disc loss: 0.01050323997665598, policy loss: 7.649286534221191
Experience 10, Iter 69, disc loss: 0.010127802959834722, policy loss: 7.582645380826456
Experience 10, Iter 70, disc loss: 0.009832708647369112, policy loss: 7.535062173262444
Experience 10, Iter 71, disc loss: 0.009681188286381759, policy loss: 7.345507958681329
Experience 10, Iter 72, disc loss: 0.009314583577250176, policy loss: 7.406513150548973
Experience 10, Iter 73, disc loss: 0.00905984268333671, policy loss: 7.327151908395587
Experience 10, Iter 74, disc loss: 0.008862964643040712, policy loss: 7.202247895986842
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0089],
        [0.0897],
        [1.0373],
        [0.0155]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1078, 0.3626, 0.7336, 0.0174, 0.0075, 2.3831]],

        [[0.1078, 0.3626, 0.7336, 0.0174, 0.0075, 2.3831]],

        [[0.1078, 0.3626, 0.7336, 0.0174, 0.0075, 2.3831]],

        [[0.1078, 0.3626, 0.7336, 0.0174, 0.0075, 2.3831]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0357, 0.3586, 4.1491, 0.0620], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0357, 0.3586, 4.1491, 0.0620])
N: 165
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([661.0000, 661.0000, 661.0000, 661.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.720
Iter 2/2000 - Loss: 2.784
Iter 3/2000 - Loss: 2.603
Iter 4/2000 - Loss: 2.558
Iter 5/2000 - Loss: 2.534
Iter 6/2000 - Loss: 2.426
Iter 7/2000 - Loss: 2.292
Iter 8/2000 - Loss: 2.176
Iter 9/2000 - Loss: 2.072
Iter 10/2000 - Loss: 1.953
Iter 11/2000 - Loss: 1.808
Iter 12/2000 - Loss: 1.644
Iter 13/2000 - Loss: 1.472
Iter 14/2000 - Loss: 1.295
Iter 15/2000 - Loss: 1.113
Iter 16/2000 - Loss: 0.924
Iter 17/2000 - Loss: 0.723
Iter 18/2000 - Loss: 0.509
Iter 19/2000 - Loss: 0.284
Iter 20/2000 - Loss: 0.050
Iter 1981/2000 - Loss: -7.648
Iter 1982/2000 - Loss: -7.648
Iter 1983/2000 - Loss: -7.648
Iter 1984/2000 - Loss: -7.648
Iter 1985/2000 - Loss: -7.648
Iter 1986/2000 - Loss: -7.648
Iter 1987/2000 - Loss: -7.648
Iter 1988/2000 - Loss: -7.649
Iter 1989/2000 - Loss: -7.649
Iter 1990/2000 - Loss: -7.649
Iter 1991/2000 - Loss: -7.649
Iter 1992/2000 - Loss: -7.649
Iter 1993/2000 - Loss: -7.649
Iter 1994/2000 - Loss: -7.649
Iter 1995/2000 - Loss: -7.649
Iter 1996/2000 - Loss: -7.649
Iter 1997/2000 - Loss: -7.649
Iter 1998/2000 - Loss: -7.649
Iter 1999/2000 - Loss: -7.649
Iter 2000/2000 - Loss: -7.649
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[16.0544, 11.4644, 32.3644, 14.0435, 13.3622, 45.3070]],

        [[30.4128, 39.1241, 12.1191,  1.2921,  2.7153, 27.3782]],

        [[32.0049, 45.9408, 11.6749,  1.0083,  1.5778, 17.9975]],

        [[32.9484, 45.4826, 17.3231,  3.1148,  1.8226, 31.6200]]])
Signal Variance: tensor([ 0.2385,  2.6885, 14.4022,  0.4370])
Estimated target variance: tensor([0.0357, 0.3586, 4.1491, 0.0620])
N: 165
Signal to noise ratio: tensor([26.3143, 90.3084, 84.5723, 42.2217])
Bound on condition number: tensor([ 114254.1870, 1345675.0100, 1180160.1470,  294142.4905])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.009430676786707318, policy loss: 6.534070319938854
Experience 11, Iter 1, disc loss: 0.009334755301584112, policy loss: 6.459821728332075
Experience 11, Iter 2, disc loss: 0.009285249176249157, policy loss: 6.347829605748361
Experience 11, Iter 3, disc loss: 0.00889499373790849, policy loss: 6.423991171630963
Experience 11, Iter 4, disc loss: 0.008679307371902346, policy loss: 6.4508900402112035
Experience 11, Iter 5, disc loss: 0.00889647479645122, policy loss: 6.307353588698465
Experience 11, Iter 6, disc loss: 0.008840716310315522, policy loss: 6.2628083526243925
Experience 11, Iter 7, disc loss: 0.010078090969232633, policy loss: 5.8690034965621845
Experience 11, Iter 8, disc loss: 0.2652911030540009, policy loss: 3.9808432574438988
Experience 11, Iter 9, disc loss: 0.5419130311244365, policy loss: 3.2450806804557866
Experience 11, Iter 10, disc loss: 1.2767247827361508, policy loss: 2.5174517777961842
Experience 11, Iter 11, disc loss: 1.248948990613581, policy loss: 2.354942765723883
Experience 11, Iter 12, disc loss: 1.0998908310302637, policy loss: 1.8184802643546127
Experience 11, Iter 13, disc loss: 0.30532946206071326, policy loss: 4.767694415171001
Experience 11, Iter 14, disc loss: 0.0735342317950589, policy loss: 7.820983280053008
Experience 11, Iter 15, disc loss: 0.06731289202857374, policy loss: 9.47381330164729
Experience 11, Iter 16, disc loss: 0.10029992009515949, policy loss: 9.39116045930784
Experience 11, Iter 17, disc loss: 0.1402179455207775, policy loss: 9.658448517735001
Experience 11, Iter 18, disc loss: 0.1737279567427224, policy loss: 9.898834016190595
Experience 11, Iter 19, disc loss: 0.19588082404526416, policy loss: 9.709326179716065
Experience 11, Iter 20, disc loss: 0.19967699079325024, policy loss: 9.387415603819479
Experience 11, Iter 21, disc loss: 0.1863519612598001, policy loss: 8.846266566086026
Experience 11, Iter 22, disc loss: 0.16185838562535204, policy loss: 8.439729582701165
Experience 11, Iter 23, disc loss: 0.13304544962541381, policy loss: 8.195953202253975
Experience 11, Iter 24, disc loss: 0.10508034738019839, policy loss: 8.024733330657018
Experience 11, Iter 25, disc loss: 0.08104191253370908, policy loss: 8.027462371879729
Experience 11, Iter 26, disc loss: 0.06176382273293462, policy loss: 8.419202081169123
Experience 11, Iter 27, disc loss: 0.04701628899772079, policy loss: 8.782461020272823
Experience 11, Iter 28, disc loss: 0.036275827185821834, policy loss: 8.370387931742183
Experience 11, Iter 29, disc loss: 0.02829078612704474, policy loss: 8.608199512879848
Experience 11, Iter 30, disc loss: 0.022646242013061414, policy loss: 8.613012770719735
Experience 11, Iter 31, disc loss: 0.01857652102294899, policy loss: 8.549598706893928
Experience 11, Iter 32, disc loss: 0.015686026637909467, policy loss: 8.4286610904056
Experience 11, Iter 33, disc loss: 0.013589138949605405, policy loss: 8.326170904108814
Experience 11, Iter 34, disc loss: 0.011938304844348253, policy loss: 8.454616463275862
Experience 11, Iter 35, disc loss: 0.01076298314144271, policy loss: 8.41506818846479
Experience 11, Iter 36, disc loss: 0.009767668376907514, policy loss: 8.238017156996722
Experience 11, Iter 37, disc loss: 0.010195858590845576, policy loss: 7.156420947889817
Experience 11, Iter 38, disc loss: 0.044048533430110275, policy loss: 6.68500523086106
Experience 11, Iter 39, disc loss: 0.007995212835181668, policy loss: 7.709968310543405
Experience 11, Iter 40, disc loss: 0.007404004961507475, policy loss: 8.106320378011972
Experience 11, Iter 41, disc loss: 0.007018397981551924, policy loss: 8.163685531456096
Experience 11, Iter 42, disc loss: 0.006729793927529164, policy loss: 8.08812297455833
Experience 11, Iter 43, disc loss: 0.006560014719433656, policy loss: 7.803540627568203
Experience 11, Iter 44, disc loss: 0.006513933277047348, policy loss: 7.435890192602991
Experience 11, Iter 45, disc loss: 0.006249390756320453, policy loss: 7.47111428740852
Experience 11, Iter 46, disc loss: 0.006074319736574519, policy loss: 7.445503594365435
Experience 11, Iter 47, disc loss: 0.006109287527336709, policy loss: 7.291442914260583
Experience 11, Iter 48, disc loss: 0.0062943805094640036, policy loss: 7.082114417468409
Experience 11, Iter 49, disc loss: 0.006576917461174914, policy loss: 7.0125019382697555
Experience 11, Iter 50, disc loss: 0.008518909842653943, policy loss: 6.3155904619489736
Experience 11, Iter 51, disc loss: 0.009895265841528018, policy loss: 6.178206838997081
Experience 11, Iter 52, disc loss: 0.12062446183692375, policy loss: 5.099147845641612
Experience 11, Iter 53, disc loss: 0.5114043949745531, policy loss: 3.7579858019253267
Experience 11, Iter 54, disc loss: 0.6463403966643259, policy loss: 3.9622805933062826
Experience 11, Iter 55, disc loss: 0.9233163044197145, policy loss: 3.1034491331821936
Experience 11, Iter 56, disc loss: 0.5648236012150483, policy loss: 3.736049633953633
Experience 11, Iter 57, disc loss: 1.0886534670757644, policy loss: 3.322919296423783
Experience 11, Iter 58, disc loss: 0.8478335509847126, policy loss: 4.11494676304462
Experience 11, Iter 59, disc loss: 0.560969160942558, policy loss: 4.506330522437747
Experience 11, Iter 60, disc loss: 0.39474518180355744, policy loss: 4.965789871594186
Experience 11, Iter 61, disc loss: 0.32778480475240324, policy loss: 6.269225791727734
Experience 11, Iter 62, disc loss: 0.044857007395679084, policy loss: 10.78704243291492
Experience 11, Iter 63, disc loss: 0.044112548811651574, policy loss: 10.250695491473952
Experience 11, Iter 64, disc loss: 0.06038950774843251, policy loss: 10.28567598095725
Experience 11, Iter 65, disc loss: 0.07867314453651124, policy loss: 10.24543869671147
Experience 11, Iter 66, disc loss: 0.097040322554291, policy loss: 10.351275795230169
Experience 11, Iter 67, disc loss: 0.11282394256926276, policy loss: 10.32912477551754
Experience 11, Iter 68, disc loss: 0.12364092261179203, policy loss: 10.119435335533355
Experience 11, Iter 69, disc loss: 0.1279881047898957, policy loss: 9.848384155800861
Experience 11, Iter 70, disc loss: 0.12586593285225198, policy loss: 9.234965642881422
Experience 11, Iter 71, disc loss: 0.11813584191415386, policy loss: 9.400999001372428
Experience 11, Iter 72, disc loss: 0.10737826780846045, policy loss: 8.96532591641386
Experience 11, Iter 73, disc loss: 0.09457609629054009, policy loss: 9.163757503542156
Experience 11, Iter 74, disc loss: 0.08149419523250552, policy loss: 9.520747087862196
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0083],
        [0.0909],
        [1.0385],
        [0.0164]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0993, 0.3408, 0.7601, 0.0186, 0.0085, 2.5358]],

        [[0.0993, 0.3408, 0.7601, 0.0186, 0.0085, 2.5358]],

        [[0.0993, 0.3408, 0.7601, 0.0186, 0.0085, 2.5358]],

        [[0.0993, 0.3408, 0.7601, 0.0186, 0.0085, 2.5358]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0334, 0.3637, 4.1542, 0.0658], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0334, 0.3637, 4.1542, 0.0658])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.716
Iter 2/2000 - Loss: 2.768
Iter 3/2000 - Loss: 2.591
Iter 4/2000 - Loss: 2.548
Iter 5/2000 - Loss: 2.517
Iter 6/2000 - Loss: 2.402
Iter 7/2000 - Loss: 2.264
Iter 8/2000 - Loss: 2.145
Iter 9/2000 - Loss: 2.035
Iter 10/2000 - Loss: 1.906
Iter 11/2000 - Loss: 1.752
Iter 12/2000 - Loss: 1.579
Iter 13/2000 - Loss: 1.399
Iter 14/2000 - Loss: 1.214
Iter 15/2000 - Loss: 1.023
Iter 16/2000 - Loss: 0.824
Iter 17/2000 - Loss: 0.613
Iter 18/2000 - Loss: 0.391
Iter 19/2000 - Loss: 0.159
Iter 20/2000 - Loss: -0.081
Iter 1981/2000 - Loss: -7.794
Iter 1982/2000 - Loss: -7.794
Iter 1983/2000 - Loss: -7.794
Iter 1984/2000 - Loss: -7.794
Iter 1985/2000 - Loss: -7.794
Iter 1986/2000 - Loss: -7.794
Iter 1987/2000 - Loss: -7.794
Iter 1988/2000 - Loss: -7.795
Iter 1989/2000 - Loss: -7.795
Iter 1990/2000 - Loss: -7.795
Iter 1991/2000 - Loss: -7.795
Iter 1992/2000 - Loss: -7.795
Iter 1993/2000 - Loss: -7.795
Iter 1994/2000 - Loss: -7.795
Iter 1995/2000 - Loss: -7.795
Iter 1996/2000 - Loss: -7.795
Iter 1997/2000 - Loss: -7.795
Iter 1998/2000 - Loss: -7.795
Iter 1999/2000 - Loss: -7.795
Iter 2000/2000 - Loss: -7.795
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[20.5572, 13.1626, 32.3279, 13.7377, 10.7143, 47.5323]],

        [[30.3168, 39.8667, 11.4871,  1.2366,  2.5021, 27.8851]],

        [[32.7144, 47.1671, 11.8555,  0.9919,  1.5655, 17.2214]],

        [[30.5262, 42.8519, 16.6476,  2.6399,  2.0229, 33.7417]]])
Signal Variance: tensor([ 0.2636,  2.4234, 13.1002,  0.4427])
Estimated target variance: tensor([0.0334, 0.3637, 4.1542, 0.0658])
N: 180
Signal to noise ratio: tensor([28.2899, 87.2474, 82.0512, 43.5922])
Bound on condition number: tensor([ 144057.9067, 1370179.3288, 1211831.6982,  342051.5809])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.07007783062345943, policy loss: 8.601337987391297
Experience 12, Iter 1, disc loss: 0.0599278849438702, policy loss: 8.152080218498664
Experience 12, Iter 2, disc loss: 0.050933643215669726, policy loss: 8.62923368875082
Experience 12, Iter 3, disc loss: 0.043902909879214795, policy loss: 8.545460754710726
Experience 12, Iter 4, disc loss: 0.03799121937455265, policy loss: 8.463308799995328
Experience 12, Iter 5, disc loss: 0.032871191087905725, policy loss: 9.386228099356114
Experience 12, Iter 6, disc loss: 0.028880871758499696, policy loss: 9.376674632076579
Experience 12, Iter 7, disc loss: 0.02623408713658524, policy loss: 9.279894991315073
Experience 12, Iter 8, disc loss: 0.024719364301422474, policy loss: 7.383480231754428
Experience 12, Iter 9, disc loss: 0.021349992349170878, policy loss: 9.426305755060444
Experience 12, Iter 10, disc loss: 0.019510502761429915, policy loss: 9.760900912324985
Experience 12, Iter 11, disc loss: 0.01795843759009429, policy loss: 9.588153424125029
Experience 12, Iter 12, disc loss: 0.016625566343086436, policy loss: 9.407847269141806
Experience 12, Iter 13, disc loss: 0.015407224667150207, policy loss: 9.254046678762894
Experience 12, Iter 14, disc loss: 0.014482124420623832, policy loss: 9.172637109733484
Experience 12, Iter 15, disc loss: 0.013643639886784149, policy loss: 9.148167361951394
Experience 12, Iter 16, disc loss: 0.013080637331836982, policy loss: 8.814729533065808
Experience 12, Iter 17, disc loss: 0.011938796801146378, policy loss: 9.204948781044983
Experience 12, Iter 18, disc loss: 0.01159455467533312, policy loss: 8.803842294285099
Experience 12, Iter 19, disc loss: 0.010722749557992229, policy loss: 9.171462540528982
Experience 12, Iter 20, disc loss: 0.010319841939997507, policy loss: 8.859938726847865
Experience 12, Iter 21, disc loss: 0.00983181187318315, policy loss: 8.899832075766174
Experience 12, Iter 22, disc loss: 0.009417879093821269, policy loss: 8.877365940410122
Experience 12, Iter 23, disc loss: 0.008937403632993612, policy loss: 8.881845104505569
Experience 12, Iter 24, disc loss: 0.008751822848703197, policy loss: 8.749101055613314
Experience 12, Iter 25, disc loss: 0.00827998098835362, policy loss: 8.930443402788011
Experience 12, Iter 26, disc loss: 0.00828786787837452, policy loss: 8.602532176932957
Experience 12, Iter 27, disc loss: 0.007815234853940896, policy loss: 8.833450607910127
Experience 12, Iter 28, disc loss: 0.007973963426600146, policy loss: 8.786460260164489
Experience 12, Iter 29, disc loss: 0.007297404376352864, policy loss: 9.075106499780562
Experience 12, Iter 30, disc loss: 0.0071111139456080255, policy loss: 9.043041339302594
Experience 12, Iter 31, disc loss: 0.00696804153531785, policy loss: 8.936788267699061
Experience 12, Iter 32, disc loss: 0.006776486216336295, policy loss: 8.946057548062008
Experience 12, Iter 33, disc loss: 0.006699915087628977, policy loss: 8.911995317322098
Experience 12, Iter 34, disc loss: 0.006488262207505161, policy loss: 8.921515024496186
Experience 12, Iter 35, disc loss: 0.00639201266446722, policy loss: 8.881190274197989
Experience 12, Iter 36, disc loss: 0.006352472944350072, policy loss: 8.705911977164824
Experience 12, Iter 37, disc loss: 0.006253899657414213, policy loss: 8.63876244711973
Experience 12, Iter 38, disc loss: 0.00601804226464718, policy loss: 8.811710099878267
Experience 12, Iter 39, disc loss: 0.005951715056574389, policy loss: 8.582255652017214
Experience 12, Iter 40, disc loss: 0.006010991664164757, policy loss: 8.586064704278176
Experience 12, Iter 41, disc loss: 0.005901243030465297, policy loss: 8.581298564271972
Experience 12, Iter 42, disc loss: 0.005671945979238327, policy loss: 8.60830249786256
Experience 12, Iter 43, disc loss: 0.005687128955157114, policy loss: 8.521791093093775
Experience 12, Iter 44, disc loss: 0.005722342150112377, policy loss: 8.51298872167733
Experience 12, Iter 45, disc loss: 0.006378868503389055, policy loss: 8.361144091088047
Experience 12, Iter 46, disc loss: 0.0053631893002755535, policy loss: 8.546932018276175
Experience 12, Iter 47, disc loss: 0.005349548636981772, policy loss: 8.44689877735113
Experience 12, Iter 48, disc loss: 0.005358642114556014, policy loss: 8.423958462172978
Experience 12, Iter 49, disc loss: 0.00517809567925934, policy loss: 8.492464081414212
Experience 12, Iter 50, disc loss: 0.005111495436885723, policy loss: 8.52441546918687
Experience 12, Iter 51, disc loss: 0.005058405001542675, policy loss: 8.529839470702969
Experience 12, Iter 52, disc loss: 0.004999086784179963, policy loss: 8.511211015725868
Experience 12, Iter 53, disc loss: 0.004942809078905344, policy loss: 8.48852875243551
Experience 12, Iter 54, disc loss: 0.004879874828203703, policy loss: 8.559417166969581
Experience 12, Iter 55, disc loss: 0.004830262947929572, policy loss: 8.54564834881493
Experience 12, Iter 56, disc loss: 0.004794672350111621, policy loss: 8.441994386663712
Experience 12, Iter 57, disc loss: 0.00475271230368509, policy loss: 8.410957007095643
Experience 12, Iter 58, disc loss: 0.00469300162862401, policy loss: 8.457953513478682
Experience 12, Iter 59, disc loss: 0.004679920787016239, policy loss: 8.313709036612883
Experience 12, Iter 60, disc loss: 0.004632524443095211, policy loss: 8.358871559438908
Experience 12, Iter 61, disc loss: 0.0045935929282203405, policy loss: 8.35815796068454
Experience 12, Iter 62, disc loss: 0.0045462906565933055, policy loss: 8.365964959003971
Experience 12, Iter 63, disc loss: 0.004513993234411664, policy loss: 8.321677770849327
Experience 12, Iter 64, disc loss: 0.004490030438950881, policy loss: 8.239910013574981
Experience 12, Iter 65, disc loss: 0.004421496409868374, policy loss: 8.359372786301336
Experience 12, Iter 66, disc loss: 0.0043751056932137345, policy loss: 8.410277230527683
Experience 12, Iter 67, disc loss: 0.004313310223178703, policy loss: 8.49740350206136
Experience 12, Iter 68, disc loss: 0.004283758429763481, policy loss: 8.487920299827755
Experience 12, Iter 69, disc loss: 0.0042412692397824645, policy loss: 8.545513727053375
Experience 12, Iter 70, disc loss: 0.004217125811725943, policy loss: 8.462430074491664
Experience 12, Iter 71, disc loss: 0.004189551979184954, policy loss: 8.396211922088373
Experience 12, Iter 72, disc loss: 0.004167675256525502, policy loss: 8.345187114502075
Experience 12, Iter 73, disc loss: 0.004147620994452879, policy loss: 8.297609858003169
Experience 12, Iter 74, disc loss: 0.004139793613901982, policy loss: 8.185048595558229
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0079],
        [0.0987],
        [1.0975],
        [0.0178]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0918, 0.3239, 0.8283, 0.0198, 0.0090, 2.7954]],

        [[0.0918, 0.3239, 0.8283, 0.0198, 0.0090, 2.7954]],

        [[0.0918, 0.3239, 0.8283, 0.0198, 0.0090, 2.7954]],

        [[0.0918, 0.3239, 0.8283, 0.0198, 0.0090, 2.7954]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0316, 0.3949, 4.3901, 0.0713], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0316, 0.3949, 4.3901, 0.0713])
N: 195
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([781.0000, 781.0000, 781.0000, 781.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.792
Iter 2/2000 - Loss: 2.846
Iter 3/2000 - Loss: 2.665
Iter 4/2000 - Loss: 2.621
Iter 5/2000 - Loss: 2.588
Iter 6/2000 - Loss: 2.472
Iter 7/2000 - Loss: 2.332
Iter 8/2000 - Loss: 2.210
Iter 9/2000 - Loss: 2.098
Iter 10/2000 - Loss: 1.969
Iter 11/2000 - Loss: 1.812
Iter 12/2000 - Loss: 1.635
Iter 13/2000 - Loss: 1.449
Iter 14/2000 - Loss: 1.257
Iter 15/2000 - Loss: 1.060
Iter 16/2000 - Loss: 0.853
Iter 17/2000 - Loss: 0.634
Iter 18/2000 - Loss: 0.403
Iter 19/2000 - Loss: 0.161
Iter 20/2000 - Loss: -0.090
Iter 1981/2000 - Loss: -7.864
Iter 1982/2000 - Loss: -7.864
Iter 1983/2000 - Loss: -7.864
Iter 1984/2000 - Loss: -7.864
Iter 1985/2000 - Loss: -7.864
Iter 1986/2000 - Loss: -7.864
Iter 1987/2000 - Loss: -7.864
Iter 1988/2000 - Loss: -7.864
Iter 1989/2000 - Loss: -7.864
Iter 1990/2000 - Loss: -7.864
Iter 1991/2000 - Loss: -7.865
Iter 1992/2000 - Loss: -7.865
Iter 1993/2000 - Loss: -7.865
Iter 1994/2000 - Loss: -7.865
Iter 1995/2000 - Loss: -7.865
Iter 1996/2000 - Loss: -7.865
Iter 1997/2000 - Loss: -7.865
Iter 1998/2000 - Loss: -7.865
Iter 1999/2000 - Loss: -7.865
Iter 2000/2000 - Loss: -7.865
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[19.8843, 12.3568, 32.9967, 13.4616,  8.3380, 48.8503]],

        [[27.1405, 38.5776, 11.0682,  1.2522,  2.4336, 30.0857]],

        [[30.3826, 43.9401, 11.9442,  1.0320,  1.5890, 17.7278]],

        [[28.0007, 40.0688, 15.9214,  2.3653,  2.1227, 34.9940]]])
Signal Variance: tensor([ 0.2516,  2.6747, 13.5404,  0.4479])
Estimated target variance: tensor([0.0316, 0.3949, 4.3901, 0.0713])
N: 195
Signal to noise ratio: tensor([28.3681, 91.9580, 83.2677, 42.4262])
Bound on condition number: tensor([ 156927.1050, 1648974.7188, 1352036.5927,  350996.7992])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.004125625153661196, policy loss: 8.148359877703175
Experience 13, Iter 1, disc loss: 0.004063370860456545, policy loss: 8.243218109757885
Experience 13, Iter 2, disc loss: 0.004035348159182719, policy loss: 8.212140508452993
Experience 13, Iter 3, disc loss: 0.003965477980627193, policy loss: 8.347398898711154
Experience 13, Iter 4, disc loss: 0.003929876881905225, policy loss: 8.384795634007938
Experience 13, Iter 5, disc loss: 0.003883664504868701, policy loss: 8.422220772236464
Experience 13, Iter 6, disc loss: 0.0038629628448455964, policy loss: 8.402240693917939
Experience 13, Iter 7, disc loss: 0.003845084258277161, policy loss: 8.376868914057399
Experience 13, Iter 8, disc loss: 0.003836358322976247, policy loss: 8.292962586440396
Experience 13, Iter 9, disc loss: 0.00382326218033156, policy loss: 8.210879096771283
Experience 13, Iter 10, disc loss: 0.0037884806163189576, policy loss: 8.256862322861739
Experience 13, Iter 11, disc loss: 0.003771466311502202, policy loss: 8.193488766657065
Experience 13, Iter 12, disc loss: 0.0037374124140274285, policy loss: 8.236183642610685
Experience 13, Iter 13, disc loss: 0.00368732360934378, policy loss: 8.309977387352532
Experience 13, Iter 14, disc loss: 0.0036425332475904194, policy loss: 8.375036126553475
Experience 13, Iter 15, disc loss: 0.003628446330055649, policy loss: 8.325631294029067
Experience 13, Iter 16, disc loss: 0.0035887760276343064, policy loss: 8.394653812680955
Experience 13, Iter 17, disc loss: 0.003568913367750982, policy loss: 8.405567006817673
Experience 13, Iter 18, disc loss: 0.003594727487228318, policy loss: 8.198292658478374
Experience 13, Iter 19, disc loss: 0.0035530104894855624, policy loss: 8.247945642044261
Experience 13, Iter 20, disc loss: 0.003515510332217689, policy loss: 8.300660032153353
Experience 13, Iter 21, disc loss: 0.0034961201670185883, policy loss: 8.289919148113396
Experience 13, Iter 22, disc loss: 0.003454509469874626, policy loss: 8.362011427899288
Experience 13, Iter 23, disc loss: 0.003424861676747384, policy loss: 8.412414349532106
Experience 13, Iter 24, disc loss: 0.003433144155971114, policy loss: 8.242235763901332
Experience 13, Iter 25, disc loss: 0.003385571360708649, policy loss: 8.377981889074427
Experience 13, Iter 26, disc loss: 0.0033839851021822027, policy loss: 8.280909668401424
Experience 13, Iter 27, disc loss: 0.0033816877169405295, policy loss: 8.220071226291498
Experience 13, Iter 28, disc loss: 0.0033482651876373974, policy loss: 8.259886662535127
Experience 13, Iter 29, disc loss: 0.0033053355430433503, policy loss: 8.356868431371034
Experience 13, Iter 30, disc loss: 0.003328606515383059, policy loss: 8.186324449942365
Experience 13, Iter 31, disc loss: 0.0032669867564030478, policy loss: 8.355014121408898
Experience 13, Iter 32, disc loss: 0.0032666832521487054, policy loss: 8.232033429568478
Experience 13, Iter 33, disc loss: 0.003232677423972972, policy loss: 8.308602714298537
Experience 13, Iter 34, disc loss: 0.003217122641437164, policy loss: 8.283222605620123
Experience 13, Iter 35, disc loss: 0.0032023342550770995, policy loss: 8.269061329866343
Experience 13, Iter 36, disc loss: 0.003180062064238097, policy loss: 8.310409830381623
Experience 13, Iter 37, disc loss: 0.0031492777270693394, policy loss: 8.321415428897788
Experience 13, Iter 38, disc loss: 0.00314917922041116, policy loss: 8.249110694353469
Experience 13, Iter 39, disc loss: 0.0031445509630154246, policy loss: 8.339291592399377
Experience 13, Iter 40, disc loss: 0.003121338530329317, policy loss: 8.288794376582574
Experience 13, Iter 41, disc loss: 0.0031127881800260024, policy loss: 8.20859532076323
Experience 13, Iter 42, disc loss: 0.003195794362010356, policy loss: 8.205696683877232
Experience 13, Iter 43, disc loss: 0.0030617831161824653, policy loss: 8.257805591168125
Experience 13, Iter 44, disc loss: 0.0030213894531212794, policy loss: 8.333896047261444
Experience 13, Iter 45, disc loss: 0.003008901081434874, policy loss: 8.307134176948036
Experience 13, Iter 46, disc loss: 0.003021713659790691, policy loss: 8.291114511165912
Experience 13, Iter 47, disc loss: 0.003436082581844045, policy loss: 8.202328837545355
Experience 13, Iter 48, disc loss: 0.0029511412149129212, policy loss: 8.355288049149342
Experience 13, Iter 49, disc loss: 0.003253996501929122, policy loss: 8.225872723323906
Experience 13, Iter 50, disc loss: 0.003961235880140954, policy loss: 8.2424985304737
Experience 13, Iter 51, disc loss: 0.004231822386794551, policy loss: 8.096687129116003
Experience 13, Iter 52, disc loss: 0.002978338884548248, policy loss: 8.179085249556064
Experience 13, Iter 53, disc loss: 0.010034627422620733, policy loss: 8.263763962800613
Experience 13, Iter 54, disc loss: 0.00286885160994386, policy loss: 8.345951707624856
Experience 13, Iter 55, disc loss: 0.0028307074274503815, policy loss: 8.461059893751987
Experience 13, Iter 56, disc loss: 0.002824178017409273, policy loss: 8.42819044043027
Experience 13, Iter 57, disc loss: 0.0028829812456459486, policy loss: 8.360426793509113
Experience 13, Iter 58, disc loss: 0.0037063281397492994, policy loss: 8.222508000373427
Experience 13, Iter 59, disc loss: 0.002882457763740809, policy loss: 8.245209204515069
Experience 13, Iter 60, disc loss: 0.0028101873698658146, policy loss: 8.344316407532013
Experience 13, Iter 61, disc loss: 0.002894712607740309, policy loss: 8.158494612774014
Experience 13, Iter 62, disc loss: 0.045119211712936313, policy loss: 7.997597001179039
Experience 13, Iter 63, disc loss: 0.01562644870073005, policy loss: 7.844005332467894
Experience 13, Iter 64, disc loss: 0.0030002108699814815, policy loss: 8.24258110179729
Experience 13, Iter 65, disc loss: 0.004951700148178331, policy loss: 8.098472524397836
Experience 13, Iter 66, disc loss: 0.0034187944769710652, policy loss: 8.166542453265617
Experience 13, Iter 67, disc loss: 0.002809691010818563, policy loss: 8.275016577596972
Experience 13, Iter 68, disc loss: 0.026818565186164545, policy loss: 8.183961717167564
Experience 13, Iter 69, disc loss: 0.002993355285684003, policy loss: 8.310353609284979
Experience 13, Iter 70, disc loss: 0.03385550479782658, policy loss: 7.921194138781748
Experience 13, Iter 71, disc loss: 0.01117196407403641, policy loss: 7.921638149100349
Experience 13, Iter 72, disc loss: 0.009802368498275118, policy loss: 8.158947204820777
Experience 13, Iter 73, disc loss: 0.033962564264449505, policy loss: 7.679174975056235
Experience 13, Iter 74, disc loss: 0.003019879907779726, policy loss: 8.306413066322378
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0082],
        [0.1074],
        [1.1382],
        [0.0194]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0864, 0.3358, 0.8866, 0.0218, 0.0142, 3.1416]],

        [[0.0864, 0.3358, 0.8866, 0.0218, 0.0142, 3.1416]],

        [[0.0864, 0.3358, 0.8866, 0.0218, 0.0142, 3.1416]],

        [[0.0864, 0.3358, 0.8866, 0.0218, 0.0142, 3.1416]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0327, 0.4296, 4.5529, 0.0775], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0327, 0.4296, 4.5529, 0.0775])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.878
Iter 2/2000 - Loss: 2.935
Iter 3/2000 - Loss: 2.739
Iter 4/2000 - Loss: 2.687
Iter 5/2000 - Loss: 2.647
Iter 6/2000 - Loss: 2.524
Iter 7/2000 - Loss: 2.373
Iter 8/2000 - Loss: 2.236
Iter 9/2000 - Loss: 2.110
Iter 10/2000 - Loss: 1.969
Iter 11/2000 - Loss: 1.802
Iter 12/2000 - Loss: 1.613
Iter 13/2000 - Loss: 1.412
Iter 14/2000 - Loss: 1.204
Iter 15/2000 - Loss: 0.991
Iter 16/2000 - Loss: 0.771
Iter 17/2000 - Loss: 0.540
Iter 18/2000 - Loss: 0.299
Iter 19/2000 - Loss: 0.046
Iter 20/2000 - Loss: -0.215
Iter 1981/2000 - Loss: -7.883
Iter 1982/2000 - Loss: -7.883
Iter 1983/2000 - Loss: -7.883
Iter 1984/2000 - Loss: -7.883
Iter 1985/2000 - Loss: -7.883
Iter 1986/2000 - Loss: -7.883
Iter 1987/2000 - Loss: -7.883
Iter 1988/2000 - Loss: -7.883
Iter 1989/2000 - Loss: -7.883
Iter 1990/2000 - Loss: -7.883
Iter 1991/2000 - Loss: -7.883
Iter 1992/2000 - Loss: -7.884
Iter 1993/2000 - Loss: -7.884
Iter 1994/2000 - Loss: -7.884
Iter 1995/2000 - Loss: -7.884
Iter 1996/2000 - Loss: -7.884
Iter 1997/2000 - Loss: -7.884
Iter 1998/2000 - Loss: -7.884
Iter 1999/2000 - Loss: -7.884
Iter 2000/2000 - Loss: -7.884
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[18.3200, 10.4695, 35.3145, 12.3694, 12.5285, 50.9819]],

        [[25.1249, 37.4655, 11.5700,  1.2136,  2.2816, 26.6833]],

        [[29.3963, 43.5348, 11.9137,  1.0193,  1.4407, 16.3173]],

        [[27.4699, 38.6258, 16.0973,  2.3285,  2.1602, 36.4053]]])
Signal Variance: tensor([ 0.2127,  2.5173, 12.1876,  0.4562])
Estimated target variance: tensor([0.0327, 0.4296, 4.5529, 0.0775])
N: 210
Signal to noise ratio: tensor([25.5910, 92.1462, 79.8054, 41.6944])
Bound on condition number: tensor([ 137529.8404, 1783093.9088, 1337471.0702,  365069.0883])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.13086687289489463, policy loss: 7.875146523763247
Experience 14, Iter 1, disc loss: 0.5397205695594981, policy loss: 6.635326367785812
Experience 14, Iter 2, disc loss: 0.7791721751007037, policy loss: 5.476771894522356
Experience 14, Iter 3, disc loss: 0.979981143382278, policy loss: 5.390593983764111
Experience 14, Iter 4, disc loss: 0.797414330586615, policy loss: 6.057883474794178
Experience 14, Iter 5, disc loss: 0.4934656350928197, policy loss: 7.415942452778743
Experience 14, Iter 6, disc loss: 0.13260751170383558, policy loss: 7.9541991713514255
Experience 14, Iter 7, disc loss: 0.07051575860575021, policy loss: 9.235973402477265
Experience 14, Iter 8, disc loss: 0.14137188892562857, policy loss: 8.616691512499179
Experience 14, Iter 9, disc loss: 0.3316945518654282, policy loss: 8.459931764498343
Experience 14, Iter 10, disc loss: 0.03809934744233596, policy loss: 10.189896428177985
Experience 14, Iter 11, disc loss: 0.01984194137950616, policy loss: 10.203214096300734
Experience 14, Iter 12, disc loss: 0.02471670455343625, policy loss: 9.73303706700604
Experience 14, Iter 13, disc loss: 0.029896366907004493, policy loss: 10.251693682004557
Experience 14, Iter 14, disc loss: 0.035189897699676564, policy loss: 10.50307104011792
Experience 14, Iter 15, disc loss: 0.04011649469411222, policy loss: 10.859877624683786
Experience 14, Iter 16, disc loss: 0.04452222073107674, policy loss: 10.524110748290202
Experience 14, Iter 17, disc loss: 0.04808604060611605, policy loss: 10.226755895296224
Experience 14, Iter 18, disc loss: 0.05056973489706377, policy loss: 9.928410228331199
Experience 14, Iter 19, disc loss: 0.051832065206395085, policy loss: 9.75564678838959
Experience 14, Iter 20, disc loss: 0.05187055340355581, policy loss: 9.61993270113356
Experience 14, Iter 21, disc loss: 0.050785657329460554, policy loss: 9.661958404193607
Experience 14, Iter 22, disc loss: 0.0487914827801098, policy loss: 9.730010497057204
Experience 14, Iter 23, disc loss: 0.04611808066882876, policy loss: 10.058277787331818
Experience 14, Iter 24, disc loss: 0.043028456997828105, policy loss: 10.355268403195682
Experience 14, Iter 25, disc loss: 0.03972929629877743, policy loss: 10.630556759856036
Experience 14, Iter 26, disc loss: 0.036509852606243634, policy loss: 10.243820483158935
Experience 14, Iter 27, disc loss: 0.033211240321152136, policy loss: 10.184424278540094
Experience 14, Iter 28, disc loss: 0.03016460645042738, policy loss: 10.05812660082409
Experience 14, Iter 29, disc loss: 0.02737931724094572, policy loss: 9.946461279917134
Experience 14, Iter 30, disc loss: 0.024843997852728405, policy loss: 10.224534544149694
Experience 14, Iter 31, disc loss: 0.022596223564888405, policy loss: 10.271229811088487
Experience 14, Iter 32, disc loss: 0.020604482114647896, policy loss: 10.07411063292576
Experience 14, Iter 33, disc loss: 0.018855840333464904, policy loss: 9.720479492685616
Experience 14, Iter 34, disc loss: 0.01736430442900187, policy loss: 9.39152307144385
Experience 14, Iter 35, disc loss: 0.016063229960697836, policy loss: 9.378775787219668
Experience 14, Iter 36, disc loss: 0.01494611385662455, policy loss: 9.283903866938314
Experience 14, Iter 37, disc loss: 0.013957722356634046, policy loss: 9.34027238002908
Experience 14, Iter 38, disc loss: 0.01309351131017057, policy loss: 9.359142147802151
Experience 14, Iter 39, disc loss: 0.012344998653306877, policy loss: 9.362212706578152
Experience 14, Iter 40, disc loss: 0.011680735351289995, policy loss: 9.468438422260448
Experience 14, Iter 41, disc loss: 0.011109589885768724, policy loss: 9.31363907176252
Experience 14, Iter 42, disc loss: 0.010587001705444351, policy loss: 9.227587559731099
Experience 14, Iter 43, disc loss: 0.010095210244551735, policy loss: 9.386085889209388
Experience 14, Iter 44, disc loss: 0.009636905164452283, policy loss: 9.574607678724421
Experience 14, Iter 45, disc loss: 0.009239222793975066, policy loss: 9.641847465905798
Experience 14, Iter 46, disc loss: 0.00888416639988138, policy loss: 9.526100345768986
Experience 14, Iter 47, disc loss: 0.00856811893877849, policy loss: 9.336799612646287
Experience 14, Iter 48, disc loss: 0.008289515748921785, policy loss: 9.066182181189134
Experience 14, Iter 49, disc loss: 0.007979864557646306, policy loss: 9.30881557563769
Experience 14, Iter 50, disc loss: 0.007716204831920232, policy loss: 9.33205737205865
Experience 14, Iter 51, disc loss: 0.00747751303210047, policy loss: 9.27347416294604
Experience 14, Iter 52, disc loss: 0.007253087310492012, policy loss: 9.283642837064294
Experience 14, Iter 53, disc loss: 0.007054298628147638, policy loss: 9.194439777194724
Experience 14, Iter 54, disc loss: 0.006850055994922066, policy loss: 9.255528739291387
Experience 14, Iter 55, disc loss: 0.006655556772751668, policy loss: 9.36880845906733
Experience 14, Iter 56, disc loss: 0.0064870178639461025, policy loss: 9.506840578377584
Experience 14, Iter 57, disc loss: 0.006324567464816821, policy loss: 9.558479852022046
Experience 14, Iter 58, disc loss: 0.006180867210908533, policy loss: 9.324180709529248
Experience 14, Iter 59, disc loss: 0.00605650173502293, policy loss: 9.129939901200473
Experience 14, Iter 60, disc loss: 0.005930108484001171, policy loss: 9.07536247314857
Experience 14, Iter 61, disc loss: 0.0058019846721444595, policy loss: 9.134018403939837
Experience 14, Iter 62, disc loss: 0.005692508960272867, policy loss: 9.080352686108094
Experience 14, Iter 63, disc loss: 0.005582040199193277, policy loss: 9.126798408130856
Experience 14, Iter 64, disc loss: 0.005451151185300278, policy loss: 9.304743416928272
Experience 14, Iter 65, disc loss: 0.0053607936953828155, policy loss: 9.222787661359131
Experience 14, Iter 66, disc loss: 0.005274322022898833, policy loss: 9.163358733531776
Experience 14, Iter 67, disc loss: 0.0051786847198386655, policy loss: 9.25703220105774
Experience 14, Iter 68, disc loss: 0.005117334001865091, policy loss: 9.066325079738348
Experience 14, Iter 69, disc loss: 0.005057340614485904, policy loss: 8.907133752017117
Experience 14, Iter 70, disc loss: 0.004942249181243346, policy loss: 9.229662476473202
Experience 14, Iter 71, disc loss: 0.009932821655732412, policy loss: 9.190300451154929
Experience 14, Iter 72, disc loss: 0.004777180466833708, policy loss: 9.477501165592859
Experience 14, Iter 73, disc loss: 0.004729214858428761, policy loss: 9.298595416136916
Experience 14, Iter 74, disc loss: 0.00467047479403506, policy loss: 9.281649030071659
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0078],
        [0.1130],
        [1.1745],
        [0.0204]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0813, 0.3214, 0.9331, 0.0226, 0.0143, 3.3426]],

        [[0.0813, 0.3214, 0.9331, 0.0226, 0.0143, 3.3426]],

        [[0.0813, 0.3214, 0.9331, 0.0226, 0.0143, 3.3426]],

        [[0.0813, 0.3214, 0.9331, 0.0226, 0.0143, 3.3426]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0312, 0.4520, 4.6980, 0.0814], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0312, 0.4520, 4.6980, 0.0814])
N: 225
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([901.0000, 901.0000, 901.0000, 901.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.926
Iter 2/2000 - Loss: 2.984
Iter 3/2000 - Loss: 2.788
Iter 4/2000 - Loss: 2.736
Iter 5/2000 - Loss: 2.695
Iter 6/2000 - Loss: 2.569
Iter 7/2000 - Loss: 2.415
Iter 8/2000 - Loss: 2.275
Iter 9/2000 - Loss: 2.142
Iter 10/2000 - Loss: 1.993
Iter 11/2000 - Loss: 1.816
Iter 12/2000 - Loss: 1.616
Iter 13/2000 - Loss: 1.404
Iter 14/2000 - Loss: 1.186
Iter 15/2000 - Loss: 0.963
Iter 16/2000 - Loss: 0.733
Iter 17/2000 - Loss: 0.494
Iter 18/2000 - Loss: 0.244
Iter 19/2000 - Loss: -0.016
Iter 20/2000 - Loss: -0.284
Iter 1981/2000 - Loss: -7.980
Iter 1982/2000 - Loss: -7.980
Iter 1983/2000 - Loss: -7.980
Iter 1984/2000 - Loss: -7.980
Iter 1985/2000 - Loss: -7.980
Iter 1986/2000 - Loss: -7.980
Iter 1987/2000 - Loss: -7.980
Iter 1988/2000 - Loss: -7.980
Iter 1989/2000 - Loss: -7.980
Iter 1990/2000 - Loss: -7.980
Iter 1991/2000 - Loss: -7.980
Iter 1992/2000 - Loss: -7.981
Iter 1993/2000 - Loss: -7.981
Iter 1994/2000 - Loss: -7.981
Iter 1995/2000 - Loss: -7.981
Iter 1996/2000 - Loss: -7.981
Iter 1997/2000 - Loss: -7.981
Iter 1998/2000 - Loss: -7.981
Iter 1999/2000 - Loss: -7.981
Iter 2000/2000 - Loss: -7.981
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[17.4165,  9.1937, 36.0591, 12.4729, 12.7827, 51.8311]],

        [[25.3680, 32.0758, 11.2866,  1.2529,  2.0313, 22.0763]],

        [[28.4560, 38.8800, 11.9708,  1.0407,  1.4051, 16.1056]],

        [[26.1542, 37.0191, 15.8553,  2.2188,  2.2098, 37.5530]]])
Signal Variance: tensor([ 0.1855,  2.1134, 12.1205,  0.4450])
Estimated target variance: tensor([0.0312, 0.4520, 4.6980, 0.0814])
N: 225
Signal to noise ratio: tensor([24.2032, 84.2335, 80.0372, 42.1757])
Bound on condition number: tensor([ 131804.6878, 1596439.9909, 1441341.1974,  400227.9463])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.0046562531087286085, policy loss: 8.905717996292084
Experience 15, Iter 1, disc loss: 0.0046355324319495095, policy loss: 8.760477461225772
Experience 15, Iter 2, disc loss: 0.004513559808111894, policy loss: 9.226933767118958
Experience 15, Iter 3, disc loss: 0.0220657784628442, policy loss: 9.150041531470968
Experience 15, Iter 4, disc loss: 0.09929606726633179, policy loss: 8.466892811542856
Experience 15, Iter 5, disc loss: 0.04746073132335816, policy loss: 8.521156068688892
Experience 15, Iter 6, disc loss: 0.004429571267011479, policy loss: 9.550110209469569
Experience 15, Iter 7, disc loss: 0.004496032504799925, policy loss: 9.138152195549392
Experience 15, Iter 8, disc loss: 0.004553338596897262, policy loss: 8.806826035860098
Experience 15, Iter 9, disc loss: 0.004622840980091715, policy loss: 8.480443116905288
Experience 15, Iter 10, disc loss: 0.004609435943506364, policy loss: 8.5834119986731
Experience 15, Iter 11, disc loss: 0.0045447123386190115, policy loss: 9.043906700890888
Experience 15, Iter 12, disc loss: 0.004515765184161471, policy loss: 9.46786960525302
Experience 15, Iter 13, disc loss: 0.0443581723450228, policy loss: 9.04302084131363
Experience 15, Iter 14, disc loss: 0.19164663016989306, policy loss: 8.133978725099938
Experience 15, Iter 15, disc loss: 0.2949454985354453, policy loss: 8.10717996428452
Experience 15, Iter 16, disc loss: 0.01622433942420188, policy loss: 8.681036589386611
Experience 15, Iter 17, disc loss: 0.020312687298548786, policy loss: 9.068185599232434
Experience 15, Iter 18, disc loss: 0.005303557196336598, policy loss: 8.914978726641753
Experience 15, Iter 19, disc loss: 0.005446499942027848, policy loss: 8.917275701368787
Experience 15, Iter 20, disc loss: 0.005596032168083183, policy loss: 9.010483157948254
Experience 15, Iter 21, disc loss: 0.005740034026498539, policy loss: 9.332809920885357
Experience 15, Iter 22, disc loss: 0.005819383744875397, policy loss: 9.574783294623478
Experience 15, Iter 23, disc loss: 0.005928992323229415, policy loss: 9.557610504692688
Experience 15, Iter 24, disc loss: 0.0060446090272836565, policy loss: 9.319345069925188
Experience 15, Iter 25, disc loss: 0.00616700563040029, policy loss: 8.899125253809192
Experience 15, Iter 26, disc loss: 0.006246838424984759, policy loss: 8.815096037629857
Experience 15, Iter 27, disc loss: 0.006240775402071178, policy loss: 9.104033929517371
Experience 15, Iter 28, disc loss: 0.006261265454649149, policy loss: 9.180992195441549
Experience 15, Iter 29, disc loss: 0.006283279816370307, policy loss: 9.12948294636934
Experience 15, Iter 30, disc loss: 0.0062787298289347865, policy loss: 9.442268683316227
Experience 15, Iter 31, disc loss: 0.00624107096522008, policy loss: 9.621219737612378
Experience 15, Iter 32, disc loss: 0.006201394160599339, policy loss: 9.994359464605983
Experience 15, Iter 33, disc loss: 0.006177579113646888, policy loss: 9.96540692070436
Experience 15, Iter 34, disc loss: 0.006154954916234972, policy loss: 9.667120063656451
Experience 15, Iter 35, disc loss: 0.0061250837591509556, policy loss: 9.524269054994761
Experience 15, Iter 36, disc loss: 0.006137300413447924, policy loss: 9.003321940920312
Experience 15, Iter 37, disc loss: 0.006076706956467924, policy loss: 9.540881529941293
Experience 15, Iter 38, disc loss: 0.12471070605657882, policy loss: 8.17130621278504
Experience 15, Iter 39, disc loss: 0.3435643895686196, policy loss: 6.886351651731774
Experience 15, Iter 40, disc loss: 0.2885647174934387, policy loss: 6.645308559911351
Experience 15, Iter 41, disc loss: 0.2004284543360848, policy loss: 7.562813739669248
Experience 15, Iter 42, disc loss: 0.007759041637815471, policy loss: 9.965341100979742
Experience 15, Iter 43, disc loss: 0.008482403710271003, policy loss: 9.303233804246718
Experience 15, Iter 44, disc loss: 0.009373253109320492, policy loss: 8.802268379099665
Experience 15, Iter 45, disc loss: 0.010147672148446753, policy loss: 9.037364985636993
Experience 15, Iter 46, disc loss: 0.010830571372425872, policy loss: 9.706475688655498
Experience 15, Iter 47, disc loss: 0.011684137433604927, policy loss: 9.959923354754045
Experience 15, Iter 48, disc loss: 0.03351159299366781, policy loss: 8.461747054219833
Experience 15, Iter 49, disc loss: 0.37882094231923724, policy loss: 6.080799149417704
Experience 15, Iter 50, disc loss: 0.46809521310727403, policy loss: 5.627977976701157
Experience 15, Iter 51, disc loss: 0.08127749080502317, policy loss: 8.341171899347751
Experience 15, Iter 52, disc loss: 0.1271178791619466, policy loss: 7.952958409804522
Experience 15, Iter 53, disc loss: 0.02867612456285291, policy loss: 9.734887033217674
Experience 15, Iter 54, disc loss: 0.028437845291897867, policy loss: 9.811589966750532
Experience 15, Iter 55, disc loss: 0.03199183134315692, policy loss: 9.39700675838167
Experience 15, Iter 56, disc loss: 0.03497524778140578, policy loss: 9.193059997394194
Experience 15, Iter 57, disc loss: 0.037151220769036465, policy loss: 9.50245621475242
Experience 15, Iter 58, disc loss: 0.03846504426429035, policy loss: 9.797391610039764
Experience 15, Iter 59, disc loss: 0.0435245887627807, policy loss: 9.2267884694265
Experience 15, Iter 60, disc loss: 0.0386714292936876, policy loss: 9.06457220251692
Experience 15, Iter 61, disc loss: 0.03764355018558751, policy loss: 8.771944354769769
Experience 15, Iter 62, disc loss: 0.0394009001740818, policy loss: 8.682114722291619
Experience 15, Iter 63, disc loss: 0.07037693322362953, policy loss: 9.335046485818928
Experience 15, Iter 64, disc loss: 0.03175374876729027, policy loss: 9.787398760855918
Experience 15, Iter 65, disc loss: 0.029485141713240567, policy loss: 9.345575118350489
Experience 15, Iter 66, disc loss: 0.027244294181135654, policy loss: 8.637397493327033
Experience 15, Iter 67, disc loss: 0.024938454399239196, policy loss: 8.676356006340974
Experience 15, Iter 68, disc loss: 0.022788086259393974, policy loss: 8.605567717388322
Experience 15, Iter 69, disc loss: 0.020750597413851418, policy loss: 9.020762667031304
Experience 15, Iter 70, disc loss: 0.01895903410716468, policy loss: 8.914120741254568
Experience 15, Iter 71, disc loss: 0.023582445820694006, policy loss: 8.281257739244431
Experience 15, Iter 72, disc loss: 0.016682536531689357, policy loss: 8.251826294821658
Experience 15, Iter 73, disc loss: 0.014933728244574652, policy loss: 8.446306318830537
Experience 15, Iter 74, disc loss: 0.013882210964958215, policy loss: 8.49478869150757
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0075],
        [0.1131],
        [1.1663],
        [0.0211]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0766, 0.3095, 0.9633, 0.0234, 0.0142, 3.4263]],

        [[0.0766, 0.3095, 0.9633, 0.0234, 0.0142, 3.4263]],

        [[0.0766, 0.3095, 0.9633, 0.0234, 0.0142, 3.4263]],

        [[0.0766, 0.3095, 0.9633, 0.0234, 0.0142, 3.4263]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0299, 0.4523, 4.6651, 0.0845], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0299, 0.4523, 4.6651, 0.0845])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.929
Iter 2/2000 - Loss: 2.987
Iter 3/2000 - Loss: 2.793
Iter 4/2000 - Loss: 2.743
Iter 5/2000 - Loss: 2.703
Iter 6/2000 - Loss: 2.577
Iter 7/2000 - Loss: 2.424
Iter 8/2000 - Loss: 2.286
Iter 9/2000 - Loss: 2.156
Iter 10/2000 - Loss: 2.007
Iter 11/2000 - Loss: 1.829
Iter 12/2000 - Loss: 1.628
Iter 13/2000 - Loss: 1.415
Iter 14/2000 - Loss: 1.196
Iter 15/2000 - Loss: 0.972
Iter 16/2000 - Loss: 0.740
Iter 17/2000 - Loss: 0.499
Iter 18/2000 - Loss: 0.246
Iter 19/2000 - Loss: -0.018
Iter 20/2000 - Loss: -0.290
Iter 1981/2000 - Loss: -8.018
Iter 1982/2000 - Loss: -8.018
Iter 1983/2000 - Loss: -8.018
Iter 1984/2000 - Loss: -8.018
Iter 1985/2000 - Loss: -8.018
Iter 1986/2000 - Loss: -8.018
Iter 1987/2000 - Loss: -8.018
Iter 1988/2000 - Loss: -8.019
Iter 1989/2000 - Loss: -8.019
Iter 1990/2000 - Loss: -8.019
Iter 1991/2000 - Loss: -8.019
Iter 1992/2000 - Loss: -8.019
Iter 1993/2000 - Loss: -8.019
Iter 1994/2000 - Loss: -8.019
Iter 1995/2000 - Loss: -8.019
Iter 1996/2000 - Loss: -8.019
Iter 1997/2000 - Loss: -8.019
Iter 1998/2000 - Loss: -8.019
Iter 1999/2000 - Loss: -8.019
Iter 2000/2000 - Loss: -8.019
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[17.5707, 10.0336, 35.6454, 11.9530, 13.1153, 51.0823]],

        [[25.0892, 32.7783, 10.9536,  1.2203,  2.0248, 24.1234]],

        [[27.2428, 36.1950, 12.5067,  1.0155,  1.3655, 15.6379]],

        [[25.6153, 36.4775, 16.7770,  1.7505,  2.4886, 39.6229]]])
Signal Variance: tensor([ 0.2009,  2.2041, 11.9116,  0.4594])
Estimated target variance: tensor([0.0299, 0.4523, 4.6651, 0.0845])
N: 240
Signal to noise ratio: tensor([25.2683, 83.1185, 80.4497, 42.8115])
Bound on condition number: tensor([ 153237.7774, 1658084.1434, 1553319.1098,  439879.7174])
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 0.012949619175042773, policy loss: 8.698739357439473
Experience 16, Iter 1, disc loss: 0.012150289897810054, policy loss: 8.879029374116804
Experience 16, Iter 2, disc loss: 0.011484980509846537, policy loss: 8.77617621023812
Experience 16, Iter 3, disc loss: 0.010957800540268792, policy loss: 8.325106223035718
Experience 16, Iter 4, disc loss: 0.010618389236989993, policy loss: 7.737459950388391
Experience 16, Iter 5, disc loss: 0.010191913425551502, policy loss: 7.589950179335069
Experience 16, Iter 6, disc loss: 0.18685599528350225, policy loss: 6.199693213234821
Experience 16, Iter 7, disc loss: 0.6979222926369925, policy loss: 4.255109904300992
Experience 16, Iter 8, disc loss: 1.1158267466838878, policy loss: 2.748959327546231
Experience 16, Iter 9, disc loss: 0.030101882792490462, policy loss: 8.390896575600152
Experience 16, Iter 10, disc loss: 0.01430190195826693, policy loss: 9.63406497433162
Experience 16, Iter 11, disc loss: 0.01687234164397994, policy loss: 9.73632576452454
Experience 16, Iter 12, disc loss: 0.019667024446177368, policy loss: 8.897989852230351
Experience 16, Iter 13, disc loss: 0.022484161858467393, policy loss: 8.562971982209252
Experience 16, Iter 14, disc loss: 0.02511088719546348, policy loss: 8.730520819631924
Experience 16, Iter 15, disc loss: 0.027430165033079334, policy loss: 9.262625858500927
Experience 16, Iter 16, disc loss: 0.029527188554283983, policy loss: 8.789575346392336
Experience 16, Iter 17, disc loss: 0.03163884515677309, policy loss: 7.575114154591425
Experience 16, Iter 18, disc loss: 0.032923485667658736, policy loss: 7.190129549968471
Experience 16, Iter 19, disc loss: 0.03544174677656791, policy loss: 6.879250581124511
Experience 16, Iter 20, disc loss: 0.09319156754094379, policy loss: 6.076991857745007
Experience 16, Iter 21, disc loss: 0.12803248583979485, policy loss: 6.137270476920877
Experience 16, Iter 22, disc loss: 0.18524434721567978, policy loss: 5.346289823516354
Experience 16, Iter 23, disc loss: 0.23247123933713276, policy loss: 4.83795583807463
Experience 16, Iter 24, disc loss: 0.22405251453492925, policy loss: 5.5078533876379785
Experience 16, Iter 25, disc loss: 0.20056604515407234, policy loss: 5.663279284264884
Experience 16, Iter 26, disc loss: 0.24878072497624434, policy loss: 5.886376536752104
Experience 16, Iter 27, disc loss: 0.1080790401435664, policy loss: 7.542630198615198
Experience 16, Iter 28, disc loss: 0.05199708275165096, policy loss: 8.809232200703995
Experience 16, Iter 29, disc loss: 0.05556129073733335, policy loss: 7.943090364706652
Experience 16, Iter 30, disc loss: 0.05991921512291508, policy loss: 7.524282798204181
Experience 16, Iter 31, disc loss: 0.05812993489779891, policy loss: 7.309321818291528
Experience 16, Iter 32, disc loss: 0.06089050387084882, policy loss: 6.94820896567184
Experience 16, Iter 33, disc loss: 0.07943862660557566, policy loss: 6.457841896907002
Experience 16, Iter 34, disc loss: 0.05914896719563664, policy loss: 6.494610342949516
Experience 16, Iter 35, disc loss: 0.05012399728081064, policy loss: 6.70899193243659
Experience 16, Iter 36, disc loss: 0.0613508783376962, policy loss: 6.552722522573866
Experience 16, Iter 37, disc loss: 0.06940177674550521, policy loss: 6.412732154076247
Experience 16, Iter 38, disc loss: 0.03994883445326756, policy loss: 7.408537538810066
Experience 16, Iter 39, disc loss: 0.03642415377797264, policy loss: 8.43319815084166
Experience 16, Iter 40, disc loss: 0.03359509571890869, policy loss: 9.009155218938522
Experience 16, Iter 41, disc loss: 0.031066024490707483, policy loss: 8.936129855676267
Experience 16, Iter 42, disc loss: 0.028767317162491295, policy loss: 8.708163130437583
Experience 16, Iter 43, disc loss: 0.026625381869192437, policy loss: 8.453070816064248
Experience 16, Iter 44, disc loss: 0.024634759791158752, policy loss: 8.54042869437539
Experience 16, Iter 45, disc loss: 0.022904458455371035, policy loss: 8.32106718592644
Experience 16, Iter 46, disc loss: 0.021309565341249535, policy loss: 8.183543594395822
Experience 16, Iter 47, disc loss: 0.02133044273397711, policy loss: 6.513548381221197
Experience 16, Iter 48, disc loss: 0.20132644786719234, policy loss: 4.545558633147261
Experience 16, Iter 49, disc loss: 0.3957894323719315, policy loss: 3.4417755050128096
Experience 16, Iter 50, disc loss: 0.1879455609331154, policy loss: 4.319521226117803
Experience 16, Iter 51, disc loss: 0.05358199074419568, policy loss: 5.326409780142804
Experience 16, Iter 52, disc loss: 0.02594142667289543, policy loss: 5.762056117493897
Experience 16, Iter 53, disc loss: 0.07907073735329534, policy loss: 4.900275847331102
Experience 16, Iter 54, disc loss: 0.26094087384059694, policy loss: 4.300840673044243
Experience 16, Iter 55, disc loss: 0.32314336189497944, policy loss: 4.141111411179526
Experience 16, Iter 56, disc loss: 0.21739771980933467, policy loss: 4.482018566523986
Experience 16, Iter 57, disc loss: 0.08035172398114301, policy loss: 5.4473400781161
Experience 16, Iter 58, disc loss: 0.052887883602323035, policy loss: 5.846912373391198
Experience 16, Iter 59, disc loss: 0.2943277279671129, policy loss: 4.1614389421546845
Experience 16, Iter 60, disc loss: 1.0922471196301604, policy loss: 2.5820618550183623
Experience 16, Iter 61, disc loss: 0.5967942485560501, policy loss: 4.213586542694324
Experience 16, Iter 62, disc loss: 0.9711410268354577, policy loss: 2.9776148031266914
Experience 16, Iter 63, disc loss: 0.7124878564431214, policy loss: 2.9993947393538463
Experience 16, Iter 64, disc loss: 0.17630662690876436, policy loss: 8.294220792486367
Experience 16, Iter 65, disc loss: 0.2303695981872636, policy loss: 9.434287293280612
Experience 16, Iter 66, disc loss: 0.2643243758553929, policy loss: 10.31446285606467
Experience 16, Iter 67, disc loss: 0.2684860511808357, policy loss: 10.652013129802194
Experience 16, Iter 68, disc loss: 0.2462163092793901, policy loss: 10.230019258225674
Experience 16, Iter 69, disc loss: 0.20969036626664034, policy loss: 9.755268687646465
Experience 16, Iter 70, disc loss: 0.17190287410375427, policy loss: 9.43735556299631
Experience 16, Iter 71, disc loss: 0.13958062139161687, policy loss: 9.424334263899816
Experience 16, Iter 72, disc loss: 0.11400084039214364, policy loss: 9.363569263571174
Experience 16, Iter 73, disc loss: 0.09454925630899745, policy loss: 7.763360828321461
Experience 16, Iter 74, disc loss: 0.07873208109438153, policy loss: 7.320589098282084
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0072],
        [0.1194],
        [1.2276],
        [0.0236]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0723, 0.2985, 1.0692, 0.0245, 0.0162, 3.6556]],

        [[0.0723, 0.2985, 1.0692, 0.0245, 0.0162, 3.6556]],

        [[0.0723, 0.2985, 1.0692, 0.0245, 0.0162, 3.6556]],

        [[0.0723, 0.2985, 1.0692, 0.0245, 0.0162, 3.6556]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0287, 0.4775, 4.9104, 0.0943], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0287, 0.4775, 4.9104, 0.0943])
N: 255
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1021.0000, 1021.0000, 1021.0000, 1021.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.011
Iter 2/2000 - Loss: 3.065
Iter 3/2000 - Loss: 2.873
Iter 4/2000 - Loss: 2.818
Iter 5/2000 - Loss: 2.777
Iter 6/2000 - Loss: 2.650
Iter 7/2000 - Loss: 2.495
Iter 8/2000 - Loss: 2.353
Iter 9/2000 - Loss: 2.218
Iter 10/2000 - Loss: 2.067
Iter 11/2000 - Loss: 1.888
Iter 12/2000 - Loss: 1.689
Iter 13/2000 - Loss: 1.478
Iter 14/2000 - Loss: 1.260
Iter 15/2000 - Loss: 1.038
Iter 16/2000 - Loss: 0.806
Iter 17/2000 - Loss: 0.563
Iter 18/2000 - Loss: 0.308
Iter 19/2000 - Loss: 0.043
Iter 20/2000 - Loss: -0.230
Iter 1981/2000 - Loss: -7.951
Iter 1982/2000 - Loss: -7.951
Iter 1983/2000 - Loss: -7.951
Iter 1984/2000 - Loss: -7.951
Iter 1985/2000 - Loss: -7.951
Iter 1986/2000 - Loss: -7.951
Iter 1987/2000 - Loss: -7.951
Iter 1988/2000 - Loss: -7.951
Iter 1989/2000 - Loss: -7.951
Iter 1990/2000 - Loss: -7.951
Iter 1991/2000 - Loss: -7.951
Iter 1992/2000 - Loss: -7.951
Iter 1993/2000 - Loss: -7.952
Iter 1994/2000 - Loss: -7.952
Iter 1995/2000 - Loss: -7.952
Iter 1996/2000 - Loss: -7.952
Iter 1997/2000 - Loss: -7.952
Iter 1998/2000 - Loss: -7.952
Iter 1999/2000 - Loss: -7.952
Iter 2000/2000 - Loss: -7.952
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[17.4655, 11.7685, 33.2298, 11.8282,  8.1998, 54.5665]],

        [[24.9158, 35.3669,  8.7920,  1.2227,  2.5899, 24.2880]],

        [[27.0467, 36.0441, 11.2644,  1.0718,  1.2907, 15.6911]],

        [[24.3906, 35.5857, 14.2972,  2.2947,  1.8781, 36.1356]]])
Signal Variance: tensor([ 0.2441,  1.7997, 11.8242,  0.3938])
Estimated target variance: tensor([0.0287, 0.4775, 4.9104, 0.0943])
N: 255
Signal to noise ratio: tensor([27.3414, 73.0313, 80.6417, 39.1409])
Bound on condition number: tensor([ 190626.4707, 1360059.8813, 1658288.9562,  390662.7369])
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 0.08555609157072519, policy loss: 6.135685935590171
Experience 17, Iter 1, disc loss: 0.17064882242975613, policy loss: 5.008010871425227
Experience 17, Iter 2, disc loss: 0.20112152687166504, policy loss: 5.152260681887734
Experience 17, Iter 3, disc loss: 0.04899678245582617, policy loss: 6.301775589896042
Experience 17, Iter 4, disc loss: 0.0402507970287231, policy loss: 6.680305972789301
Experience 17, Iter 5, disc loss: 0.12349842084867177, policy loss: 5.535897902278071
Experience 17, Iter 6, disc loss: 0.268453368251213, policy loss: 4.126543356201633
Experience 17, Iter 7, disc loss: 0.19150908185787013, policy loss: 4.163427151079956
Experience 17, Iter 8, disc loss: 0.17626670292863386, policy loss: 4.781652836818987
Experience 17, Iter 9, disc loss: 0.09570198489532436, policy loss: 5.147194366875421
Experience 17, Iter 10, disc loss: 0.1076922055815003, policy loss: 5.077261411732566
Experience 17, Iter 11, disc loss: 0.20228485790208076, policy loss: 5.098816665236835
Experience 17, Iter 12, disc loss: 0.18094978649437776, policy loss: 5.562609392976926
Experience 17, Iter 13, disc loss: 0.0330098271666476, policy loss: 6.928169748463066
Experience 17, Iter 14, disc loss: 0.02551767368662584, policy loss: 8.126112696015149
Experience 17, Iter 15, disc loss: 0.04549905121845926, policy loss: 7.872885183317185
Experience 17, Iter 16, disc loss: 0.02592736024578238, policy loss: 7.84007282283332
Experience 17, Iter 17, disc loss: 0.18531351030877208, policy loss: 5.724374307054646
Experience 17, Iter 18, disc loss: 0.2078494328089667, policy loss: 6.1410531659528385
Experience 17, Iter 19, disc loss: 0.20921716241996322, policy loss: 6.086524401739284
Experience 17, Iter 20, disc loss: 0.10897291735683373, policy loss: 6.706051357626896
Experience 17, Iter 21, disc loss: 0.14083734374321655, policy loss: 7.164526057279526
Experience 17, Iter 22, disc loss: 0.17932184913032595, policy loss: 6.647562442315956
Experience 17, Iter 23, disc loss: 0.18564051858590122, policy loss: 6.318856273896651
Experience 17, Iter 24, disc loss: 0.21506516002568551, policy loss: 6.525916057387018
Experience 17, Iter 25, disc loss: 0.09072518950579679, policy loss: 7.522240648876261
Experience 17, Iter 26, disc loss: 0.0860694105905741, policy loss: 7.431118509080941
Experience 17, Iter 27, disc loss: 0.16963230837589305, policy loss: 7.246093592310167
Experience 17, Iter 28, disc loss: 0.059049574175977615, policy loss: 7.448989861392943
Experience 17, Iter 29, disc loss: 0.04628991492314045, policy loss: 7.998283499067091
Experience 17, Iter 30, disc loss: 0.14808508055764638, policy loss: 8.045126682248045
Experience 17, Iter 31, disc loss: 0.2167201810155728, policy loss: 7.505841871941861
Experience 17, Iter 32, disc loss: 0.22610868083831712, policy loss: 7.732448290361535
Experience 17, Iter 33, disc loss: 0.15423542244234084, policy loss: 8.232359859992632
Experience 17, Iter 34, disc loss: 0.21127210866501714, policy loss: 5.909316965408388
Experience 17, Iter 35, disc loss: 0.14111440822437643, policy loss: 7.1440089447416675
Experience 17, Iter 36, disc loss: 0.05090144973198525, policy loss: 8.640859917183924
Experience 17, Iter 37, disc loss: 0.15413813833806156, policy loss: 6.361844214572372
Experience 17, Iter 38, disc loss: 0.05899336109298715, policy loss: 8.689900441271167
Experience 17, Iter 39, disc loss: 0.08770483016642255, policy loss: 8.043118744115866
Experience 17, Iter 40, disc loss: 0.09982610494453684, policy loss: 7.045439926810458
Experience 17, Iter 41, disc loss: 0.05505448937772154, policy loss: 8.366049020804638
Experience 17, Iter 42, disc loss: 0.053344493683827286, policy loss: 10.497449129477367
Experience 17, Iter 43, disc loss: 0.05198451105646405, policy loss: 10.91973816451462
Experience 17, Iter 44, disc loss: 0.05010403931653477, policy loss: 10.935559696081086
Experience 17, Iter 45, disc loss: 0.04780523648269468, policy loss: 10.670006620950042
Experience 17, Iter 46, disc loss: 0.04519520269248039, policy loss: 10.543919951470723
Experience 17, Iter 47, disc loss: 0.0423742138892423, policy loss: 11.11157355161027
Experience 17, Iter 48, disc loss: 0.03949220492510666, policy loss: 11.634581268078296
Experience 17, Iter 49, disc loss: 0.03665207180599382, policy loss: 11.335557211851812
Experience 17, Iter 50, disc loss: 0.033913435565093, policy loss: 10.521815981281167
Experience 17, Iter 51, disc loss: 0.031360911081417676, policy loss: 9.547248535729885
Experience 17, Iter 52, disc loss: 0.030685385109000294, policy loss: 7.8291007423536065
Experience 17, Iter 53, disc loss: 0.04297472441873673, policy loss: 6.94578017014151
Experience 17, Iter 54, disc loss: 0.05332682614767133, policy loss: 7.239228231089241
Experience 17, Iter 55, disc loss: 0.10821070520867543, policy loss: 6.420729484907031
Experience 17, Iter 56, disc loss: 0.1668766334354536, policy loss: 5.759365736095106
Experience 17, Iter 57, disc loss: 0.1395885090622408, policy loss: 6.266814177933966
Experience 17, Iter 58, disc loss: 0.1175823517557203, policy loss: 6.807770029091609
Experience 17, Iter 59, disc loss: 0.0379324640861983, policy loss: 8.875053019612713
Experience 17, Iter 60, disc loss: 0.022774854287920976, policy loss: 8.865544749116921
Experience 17, Iter 61, disc loss: 0.038440736177435354, policy loss: 7.976328278259101
Experience 17, Iter 62, disc loss: 0.18149681868035658, policy loss: 6.376921828253754
Experience 17, Iter 63, disc loss: 0.10542237974279543, policy loss: 6.337321665138895
Experience 17, Iter 64, disc loss: 0.08197158801635447, policy loss: 7.412859130043811
Experience 17, Iter 65, disc loss: 0.04907041803023199, policy loss: 8.530931459843712
Experience 17, Iter 66, disc loss: 0.05362406554169727, policy loss: 7.208238210836887
Experience 17, Iter 67, disc loss: 0.47685742363871514, policy loss: 3.4715923907364
Experience 17, Iter 68, disc loss: 0.3114406769244973, policy loss: 4.518950954420454
Experience 17, Iter 69, disc loss: 0.029596508098951077, policy loss: 9.462706635728132
Experience 17, Iter 70, disc loss: 0.03319661195604559, policy loss: 10.324205957071282
Experience 17, Iter 71, disc loss: 0.03678047589800547, policy loss: 10.746609917672963
Experience 17, Iter 72, disc loss: 0.039748601318537864, policy loss: 11.12486777211226
Experience 17, Iter 73, disc loss: 0.04186451478177131, policy loss: 11.461494124858167
Experience 17, Iter 74, disc loss: 0.04300999488375608, policy loss: 11.436783541717357
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0070],
        [0.1240],
        [1.2823],
        [0.0245]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0692, 0.2920, 1.1178, 0.0251, 0.0159, 3.7599]],

        [[0.0692, 0.2920, 1.1178, 0.0251, 0.0159, 3.7599]],

        [[0.0692, 0.2920, 1.1178, 0.0251, 0.0159, 3.7599]],

        [[0.0692, 0.2920, 1.1178, 0.0251, 0.0159, 3.7599]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0279, 0.4961, 5.1290, 0.0982], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0279, 0.4961, 5.1290, 0.0982])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.068
Iter 2/2000 - Loss: 3.113
Iter 3/2000 - Loss: 2.932
Iter 4/2000 - Loss: 2.880
Iter 5/2000 - Loss: 2.837
Iter 6/2000 - Loss: 2.713
Iter 7/2000 - Loss: 2.563
Iter 8/2000 - Loss: 2.428
Iter 9/2000 - Loss: 2.297
Iter 10/2000 - Loss: 2.145
Iter 11/2000 - Loss: 1.964
Iter 12/2000 - Loss: 1.764
Iter 13/2000 - Loss: 1.554
Iter 14/2000 - Loss: 1.338
Iter 15/2000 - Loss: 1.115
Iter 16/2000 - Loss: 0.881
Iter 17/2000 - Loss: 0.634
Iter 18/2000 - Loss: 0.374
Iter 19/2000 - Loss: 0.105
Iter 20/2000 - Loss: -0.171
Iter 1981/2000 - Loss: -7.967
Iter 1982/2000 - Loss: -7.967
Iter 1983/2000 - Loss: -7.967
Iter 1984/2000 - Loss: -7.967
Iter 1985/2000 - Loss: -7.967
Iter 1986/2000 - Loss: -7.967
Iter 1987/2000 - Loss: -7.967
Iter 1988/2000 - Loss: -7.967
Iter 1989/2000 - Loss: -7.967
Iter 1990/2000 - Loss: -7.967
Iter 1991/2000 - Loss: -7.967
Iter 1992/2000 - Loss: -7.967
Iter 1993/2000 - Loss: -7.967
Iter 1994/2000 - Loss: -7.967
Iter 1995/2000 - Loss: -7.967
Iter 1996/2000 - Loss: -7.967
Iter 1997/2000 - Loss: -7.967
Iter 1998/2000 - Loss: -7.967
Iter 1999/2000 - Loss: -7.967
Iter 2000/2000 - Loss: -7.967
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[16.9647, 12.6926, 32.7914, 12.3154,  7.7553, 54.2106]],

        [[24.4182, 34.4968,  9.6598,  1.2021,  2.3015, 21.7528]],

        [[26.8092, 34.3382, 11.1115,  1.1099,  1.3265, 16.1002]],

        [[23.7343, 35.4744, 14.2200,  2.3713,  1.9334, 32.7724]]])
Signal Variance: tensor([ 0.2551,  1.7167, 12.9589,  0.3829])
Estimated target variance: tensor([0.0279, 0.4961, 5.1290, 0.0982])
N: 270
Signal to noise ratio: tensor([27.6933, 70.8056, 86.0642, 37.9731])
Bound on condition number: tensor([ 207069.5559, 1353627.5702, 1999903.9460,  389330.0014])
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 0.04315520934470178, policy loss: 10.831903151981116
Experience 18, Iter 1, disc loss: 0.0423803793306873, policy loss: 10.330365672933315
Experience 18, Iter 2, disc loss: 0.04097887288722968, policy loss: 9.736482829902286
Experience 18, Iter 3, disc loss: 0.10321943505904613, policy loss: 6.313448729965631
Experience 18, Iter 4, disc loss: 0.20902664427927378, policy loss: 4.944503880290741
Experience 18, Iter 5, disc loss: 0.2589463683858802, policy loss: 5.333827219074831
Experience 18, Iter 6, disc loss: 0.38255359863556015, policy loss: 5.556620976671471
Experience 18, Iter 7, disc loss: 0.34241451960128927, policy loss: 5.447537467432586
Experience 18, Iter 8, disc loss: 0.30278726515382215, policy loss: 3.980726917824245
Experience 18, Iter 9, disc loss: 0.5584339043223289, policy loss: 3.565809266735412
Experience 18, Iter 10, disc loss: 0.6972055171939283, policy loss: 3.4241808620528147
Experience 18, Iter 11, disc loss: 0.6054957840786133, policy loss: 2.648821724160598
Experience 18, Iter 12, disc loss: 0.547223033843703, policy loss: 2.7496955173289885
Experience 18, Iter 13, disc loss: 0.15240895154842587, policy loss: 9.251357732672007
Experience 18, Iter 14, disc loss: 0.1763092874090407, policy loss: 11.990995372514456
Experience 18, Iter 15, disc loss: 0.19534641288684015, policy loss: 13.162848914460108
Experience 18, Iter 16, disc loss: 0.19605673705951207, policy loss: 12.82724177482564
Experience 18, Iter 17, disc loss: 0.18070830250954786, policy loss: 12.422211746154192
Experience 18, Iter 18, disc loss: 0.1557002319749534, policy loss: 12.276204030225294
Experience 18, Iter 19, disc loss: 0.1283797942495354, policy loss: 12.102643288018967
Experience 18, Iter 20, disc loss: 0.10362164085562832, policy loss: 12.314745800758958
Experience 18, Iter 21, disc loss: 0.0838975901099485, policy loss: 12.304073219417862
Experience 18, Iter 22, disc loss: 0.06870934229949362, policy loss: 11.856911089707305
Experience 18, Iter 23, disc loss: 0.057417342417493133, policy loss: 11.458742122947438
Experience 18, Iter 24, disc loss: 0.048706459674425315, policy loss: 10.491401503783315
Experience 18, Iter 25, disc loss: 0.045423720965890445, policy loss: 8.466006102307484
Experience 18, Iter 26, disc loss: 0.21090513302958117, policy loss: 5.639798717992019
Experience 18, Iter 27, disc loss: 0.2171833730104961, policy loss: 6.43483996903913
Experience 18, Iter 28, disc loss: 0.19813833232738934, policy loss: 5.979927556406098
Experience 18, Iter 29, disc loss: 0.366345880212318, policy loss: 5.314253074976904
Experience 18, Iter 30, disc loss: 0.3115699051126409, policy loss: 5.701048272360647
Experience 18, Iter 31, disc loss: 0.1965737873200604, policy loss: 6.799899782995432
Experience 18, Iter 32, disc loss: 0.27960519132897654, policy loss: 5.428125044862417
Experience 18, Iter 33, disc loss: 0.25458423685731124, policy loss: 6.146911563674275
Experience 18, Iter 34, disc loss: 0.17978824571986396, policy loss: 7.281772881112687
Experience 18, Iter 35, disc loss: 0.157256152040985, policy loss: 6.169412061247023
Experience 18, Iter 36, disc loss: 0.08988225293083663, policy loss: 7.756458918842553
Experience 18, Iter 37, disc loss: 0.09270273829838681, policy loss: 8.347049489667144
Experience 18, Iter 38, disc loss: 0.10319508339415798, policy loss: 6.863020556035389
Experience 18, Iter 39, disc loss: 0.09712049926455285, policy loss: 7.60259935873262
Experience 18, Iter 40, disc loss: 0.05406141348403848, policy loss: 10.525164156983642
Experience 18, Iter 41, disc loss: 0.05323442951894307, policy loss: 11.680827220150661
Experience 18, Iter 42, disc loss: 0.054116435858472034, policy loss: 11.990176817543686
Experience 18, Iter 43, disc loss: 0.05418935491449774, policy loss: 12.522984954645375
Experience 18, Iter 44, disc loss: 0.05351746839312768, policy loss: 11.929695062249802
Experience 18, Iter 45, disc loss: 0.05218760835216262, policy loss: 11.648950982846422
Experience 18, Iter 46, disc loss: 0.05031895746164005, policy loss: 12.72711124805554
Experience 18, Iter 47, disc loss: 0.048058899058924666, policy loss: 12.701453952235486
Experience 18, Iter 48, disc loss: 0.04553102112548962, policy loss: 12.646801094865737
Experience 18, Iter 49, disc loss: 0.04284819704714982, policy loss: 12.36292865470864
Experience 18, Iter 50, disc loss: 0.040156453592405654, policy loss: 12.006531165971523
Experience 18, Iter 51, disc loss: 0.0390198102573663, policy loss: 11.197129104243414
Experience 18, Iter 52, disc loss: 0.03476758962676331, policy loss: 11.740002409368818
Experience 18, Iter 53, disc loss: 0.032267577295778165, policy loss: 11.814015279447837
Experience 18, Iter 54, disc loss: 0.029907072790173284, policy loss: 11.889691280525234
Experience 18, Iter 55, disc loss: 0.027707413198715246, policy loss: 12.132575604876417
Experience 18, Iter 56, disc loss: 0.025676392046072954, policy loss: 12.01762994486469
Experience 18, Iter 57, disc loss: 0.023812949875773114, policy loss: 11.602333008891929
Experience 18, Iter 58, disc loss: 0.02210696040679942, policy loss: 11.385765604016195
Experience 18, Iter 59, disc loss: 0.020551537622147693, policy loss: 11.41893159007997
Experience 18, Iter 60, disc loss: 0.019144475866620277, policy loss: 11.374695218366679
Experience 18, Iter 61, disc loss: 0.017869718887100493, policy loss: 11.231764749343304
Experience 18, Iter 62, disc loss: 0.016716836256867137, policy loss: 11.143210112684791
Experience 18, Iter 63, disc loss: 0.01567107209680238, policy loss: 11.102450243353555
Experience 18, Iter 64, disc loss: 0.014725868074498156, policy loss: 11.006350575038603
Experience 18, Iter 65, disc loss: 0.01386652107748091, policy loss: 11.006759295161793
Experience 18, Iter 66, disc loss: 0.013090577329709311, policy loss: 11.07930284597529
Experience 18, Iter 67, disc loss: 0.01238569077618988, policy loss: 11.174891398332589
Experience 18, Iter 68, disc loss: 0.011748318736660605, policy loss: 11.049539375582947
Experience 18, Iter 69, disc loss: 0.011167813448851658, policy loss: 11.08599227710837
Experience 18, Iter 70, disc loss: 0.010641197623689687, policy loss: 11.178328021398993
Experience 18, Iter 71, disc loss: 0.010166353858899172, policy loss: 11.059917601035883
Experience 18, Iter 72, disc loss: 0.009727634873634601, policy loss: 11.162251424985898
Experience 18, Iter 73, disc loss: 0.009328348443951386, policy loss: 11.073575615911665
Experience 18, Iter 74, disc loss: 0.008971489131809523, policy loss: 10.671625814442658
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.1259],
        [1.2892],
        [0.0246]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0664, 0.2821, 1.1200, 0.0252, 0.0154, 3.8098]],

        [[0.0664, 0.2821, 1.1200, 0.0252, 0.0154, 3.8098]],

        [[0.0664, 0.2821, 1.1200, 0.0252, 0.0154, 3.8098]],

        [[0.0664, 0.2821, 1.1200, 0.0252, 0.0154, 3.8098]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0269, 0.5035, 5.1568, 0.0982], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0269, 0.5035, 5.1568, 0.0982])
N: 285
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1141.0000, 1141.0000, 1141.0000, 1141.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.068
Iter 2/2000 - Loss: 3.118
Iter 3/2000 - Loss: 2.935
Iter 4/2000 - Loss: 2.884
Iter 5/2000 - Loss: 2.843
Iter 6/2000 - Loss: 2.719
Iter 7/2000 - Loss: 2.570
Iter 8/2000 - Loss: 2.436
Iter 9/2000 - Loss: 2.308
Iter 10/2000 - Loss: 2.157
Iter 11/2000 - Loss: 1.977
Iter 12/2000 - Loss: 1.776
Iter 13/2000 - Loss: 1.566
Iter 14/2000 - Loss: 1.349
Iter 15/2000 - Loss: 1.124
Iter 16/2000 - Loss: 0.888
Iter 17/2000 - Loss: 0.637
Iter 18/2000 - Loss: 0.373
Iter 19/2000 - Loss: 0.099
Iter 20/2000 - Loss: -0.183
Iter 1981/2000 - Loss: -8.022
Iter 1982/2000 - Loss: -8.022
Iter 1983/2000 - Loss: -8.022
Iter 1984/2000 - Loss: -8.022
Iter 1985/2000 - Loss: -8.022
Iter 1986/2000 - Loss: -8.022
Iter 1987/2000 - Loss: -8.022
Iter 1988/2000 - Loss: -8.022
Iter 1989/2000 - Loss: -8.022
Iter 1990/2000 - Loss: -8.022
Iter 1991/2000 - Loss: -8.022
Iter 1992/2000 - Loss: -8.023
Iter 1993/2000 - Loss: -8.023
Iter 1994/2000 - Loss: -8.023
Iter 1995/2000 - Loss: -8.023
Iter 1996/2000 - Loss: -8.023
Iter 1997/2000 - Loss: -8.023
Iter 1998/2000 - Loss: -8.023
Iter 1999/2000 - Loss: -8.023
Iter 2000/2000 - Loss: -8.023
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[15.9256, 12.4645, 32.4334, 12.1427,  8.0601, 53.7489]],

        [[24.6193, 34.3329,  9.3431,  1.2344,  2.4190, 21.3002]],

        [[25.6518, 34.0503, 11.0186,  1.0665,  1.2823, 16.1474]],

        [[23.4090, 34.1054, 14.6588,  2.1177,  1.9626, 34.0619]]])
Signal Variance: tensor([ 0.2515,  1.6670, 12.0774,  0.3971])
Estimated target variance: tensor([0.0269, 0.5035, 5.1568, 0.0982])
N: 285
Signal to noise ratio: tensor([27.6090, 69.7125, 82.6629, 38.5978])
Bound on condition number: tensor([ 217243.7502, 1385051.9319, 1947452.4315,  424591.8075])
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 0.008634659102969204, policy loss: 10.999427491501654
Experience 19, Iter 1, disc loss: 0.008317886522844802, policy loss: 11.290022193197636
Experience 19, Iter 2, disc loss: 0.008034607066443544, policy loss: 11.44415841407823
Experience 19, Iter 3, disc loss: 0.00778070441226989, policy loss: 11.183974884016468
Experience 19, Iter 4, disc loss: 0.007539530916845304, policy loss: 11.314731352161953
Experience 19, Iter 5, disc loss: 0.012091211731500467, policy loss: 10.443572564630937
Experience 19, Iter 6, disc loss: 0.007132333200325325, policy loss: 10.561641005248527
Experience 19, Iter 7, disc loss: 0.006943679857175448, policy loss: 10.696740580705345
Experience 19, Iter 8, disc loss: 0.0067680950290783535, policy loss: 10.907907637698763
Experience 19, Iter 9, disc loss: 0.006604051389474937, policy loss: 10.997143897793887
Experience 19, Iter 10, disc loss: 0.006454043063435808, policy loss: 10.870345035401305
Experience 19, Iter 11, disc loss: 0.006313216733351451, policy loss: 10.870469914270124
Experience 19, Iter 12, disc loss: 0.006183116906504867, policy loss: 10.906493900897534
Experience 19, Iter 13, disc loss: 0.00605025430725692, policy loss: 10.933978326913293
Experience 19, Iter 14, disc loss: 0.0059500385507602685, policy loss: 10.711008885356522
Experience 19, Iter 15, disc loss: 0.005825426271343065, policy loss: 10.5302134861847
Experience 19, Iter 16, disc loss: 0.00571302458096455, policy loss: 10.576845768278641
Experience 19, Iter 17, disc loss: 0.005616466185394794, policy loss: 10.34676582206993
Experience 19, Iter 18, disc loss: 0.005512831394793263, policy loss: 10.461333458878258
Experience 19, Iter 19, disc loss: 0.005416587784813923, policy loss: 10.584458389254586
Experience 19, Iter 20, disc loss: 0.005327236215592682, policy loss: 10.549313952002576
Experience 19, Iter 21, disc loss: 0.0052414753128642265, policy loss: 10.518011100446703
Experience 19, Iter 22, disc loss: 0.005163030951307511, policy loss: 10.407696116392156
Experience 19, Iter 23, disc loss: 0.005086759740649265, policy loss: 10.403817107482523
Experience 19, Iter 24, disc loss: 0.005009495241636051, policy loss: 10.399721055933638
Experience 19, Iter 25, disc loss: 0.004938734525338474, policy loss: 10.386431249984138
Experience 19, Iter 26, disc loss: 0.004863660237032377, policy loss: 10.515404186177488
Experience 19, Iter 27, disc loss: 0.0047940837513461235, policy loss: 10.555394006500643
Experience 19, Iter 28, disc loss: 0.004728219706372464, policy loss: 10.513816392506723
Experience 19, Iter 29, disc loss: 0.004674997917670391, policy loss: 10.29545927509288
Experience 19, Iter 30, disc loss: 0.004613754867534813, policy loss: 10.302877839391861
Experience 19, Iter 31, disc loss: 0.004555029819220537, policy loss: 10.323425359752317
Experience 19, Iter 32, disc loss: 0.004498192564971933, policy loss: 10.359941281798259
Experience 19, Iter 33, disc loss: 0.004450711864402938, policy loss: 10.270812771699294
Experience 19, Iter 34, disc loss: 0.004390633588884646, policy loss: 10.357688703269085
Experience 19, Iter 35, disc loss: 0.004346475681401196, policy loss: 10.204202703530864
Experience 19, Iter 36, disc loss: 0.004297682596207241, policy loss: 10.242439350015148
Experience 19, Iter 37, disc loss: 0.004248725601042012, policy loss: 10.310469594596206
Experience 19, Iter 38, disc loss: 0.004197705626139571, policy loss: 10.340689681208696
Experience 19, Iter 39, disc loss: 0.0041630424637558285, policy loss: 10.189528944493809
Experience 19, Iter 40, disc loss: 0.004120665513767809, policy loss: 10.110022425957348
Experience 19, Iter 41, disc loss: 0.004080059142104967, policy loss: 10.037223632570068
Experience 19, Iter 42, disc loss: 0.00403858132023385, policy loss: 10.117386246507593
Experience 19, Iter 43, disc loss: 0.003988660758864823, policy loss: 10.291336471522339
Experience 19, Iter 44, disc loss: 0.003953896791563604, policy loss: 10.153411872038147
Experience 19, Iter 45, disc loss: 0.00391918679951977, policy loss: 10.176333909860624
Experience 19, Iter 46, disc loss: 0.003885033930448385, policy loss: 10.033385581132277
Experience 19, Iter 47, disc loss: 0.003845732201616622, policy loss: 10.157044894319013
Experience 19, Iter 48, disc loss: 0.00380319385440424, policy loss: 10.327122687545256
Experience 19, Iter 49, disc loss: 0.0037669883120376815, policy loss: 10.330316707360971
Experience 19, Iter 50, disc loss: 0.003733250543504013, policy loss: 10.3374802988115
Experience 19, Iter 51, disc loss: 0.0037094070575312277, policy loss: 10.033055206533824
Experience 19, Iter 52, disc loss: 0.00367682679695476, policy loss: 10.06937323710125
Experience 19, Iter 53, disc loss: 0.003643654609688201, policy loss: 10.172199875290655
Experience 19, Iter 54, disc loss: 0.003605938393329073, policy loss: 10.288612045574038
Experience 19, Iter 55, disc loss: 0.003567998936725541, policy loss: 10.368307283725182
Experience 19, Iter 56, disc loss: 0.003531346768155443, policy loss: 10.608758771779307
Experience 19, Iter 57, disc loss: 0.003504287532501177, policy loss: 10.494217808413683
Experience 19, Iter 58, disc loss: 0.0034815254676260907, policy loss: 10.40970139508598
Experience 19, Iter 59, disc loss: 0.003463321836327393, policy loss: 10.076909354937717
Experience 19, Iter 60, disc loss: 0.003441713249762674, policy loss: 10.049460677361932
Experience 19, Iter 61, disc loss: 0.00341010403636505, policy loss: 10.131227857303838
Experience 19, Iter 62, disc loss: 0.003387330617963009, policy loss: 9.977136867787431
Experience 19, Iter 63, disc loss: 0.0033447424688385816, policy loss: 10.372018609133656
Experience 19, Iter 64, disc loss: 0.003312028112297543, policy loss: 10.49518550731008
Experience 19, Iter 65, disc loss: 0.00329432785455201, policy loss: 10.378667515984757
Experience 19, Iter 66, disc loss: 0.0032764459556796784, policy loss: 10.167687721406686
Experience 19, Iter 67, disc loss: 0.0032587780973751808, policy loss: 10.11975377636266
Experience 19, Iter 68, disc loss: 0.003228479980930505, policy loss: 10.219011271372993
Experience 19, Iter 69, disc loss: 0.0032227039867456465, policy loss: 9.849122323657184
Experience 19, Iter 70, disc loss: 0.00318416966924114, policy loss: 10.140403799749993
Experience 19, Iter 71, disc loss: 0.0031547394704425444, policy loss: 10.20000330780615
Experience 19, Iter 72, disc loss: 0.003137286382685154, policy loss: 10.107746065231701
Experience 19, Iter 73, disc loss: 0.0031192266988617403, policy loss: 10.00707903556441
Experience 19, Iter 74, disc loss: 0.003106492329998504, policy loss: 9.834857986906549
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0065],
        [0.1291],
        [1.3189],
        [0.0248]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0631, 0.2736, 1.1358, 0.0253, 0.0151, 3.8784]],

        [[0.0631, 0.2736, 1.1358, 0.0253, 0.0151, 3.8784]],

        [[0.0631, 0.2736, 1.1358, 0.0253, 0.0151, 3.8784]],

        [[0.0631, 0.2736, 1.1358, 0.0253, 0.0151, 3.8784]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0260, 0.5164, 5.2756, 0.0993], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0260, 0.5164, 5.2756, 0.0993])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.078
Iter 2/2000 - Loss: 3.131
Iter 3/2000 - Loss: 2.944
Iter 4/2000 - Loss: 2.893
Iter 5/2000 - Loss: 2.851
Iter 6/2000 - Loss: 2.724
Iter 7/2000 - Loss: 2.571
Iter 8/2000 - Loss: 2.432
Iter 9/2000 - Loss: 2.300
Iter 10/2000 - Loss: 2.146
Iter 11/2000 - Loss: 1.961
Iter 12/2000 - Loss: 1.755
Iter 13/2000 - Loss: 1.539
Iter 14/2000 - Loss: 1.316
Iter 15/2000 - Loss: 1.086
Iter 16/2000 - Loss: 0.845
Iter 17/2000 - Loss: 0.590
Iter 18/2000 - Loss: 0.321
Iter 19/2000 - Loss: 0.043
Iter 20/2000 - Loss: -0.243
Iter 1981/2000 - Loss: -8.080
Iter 1982/2000 - Loss: -8.080
Iter 1983/2000 - Loss: -8.080
Iter 1984/2000 - Loss: -8.080
Iter 1985/2000 - Loss: -8.080
Iter 1986/2000 - Loss: -8.080
Iter 1987/2000 - Loss: -8.080
Iter 1988/2000 - Loss: -8.080
Iter 1989/2000 - Loss: -8.080
Iter 1990/2000 - Loss: -8.080
Iter 1991/2000 - Loss: -8.080
Iter 1992/2000 - Loss: -8.080
Iter 1993/2000 - Loss: -8.080
Iter 1994/2000 - Loss: -8.080
Iter 1995/2000 - Loss: -8.080
Iter 1996/2000 - Loss: -8.080
Iter 1997/2000 - Loss: -8.080
Iter 1998/2000 - Loss: -8.080
Iter 1999/2000 - Loss: -8.080
Iter 2000/2000 - Loss: -8.080
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[15.9756, 11.5470, 31.7086, 11.5903,  9.1855, 53.5811]],

        [[23.9551, 33.7308,  9.4364,  1.2370,  2.4406, 21.1772]],

        [[24.7345, 34.7166, 11.0312,  1.0511,  1.2335, 16.4587]],

        [[22.5735, 33.1488, 14.1530,  1.8901,  2.0063, 34.7667]]])
Signal Variance: tensor([ 0.2342,  1.6648, 11.7706,  0.3942])
Estimated target variance: tensor([0.0260, 0.5164, 5.2756, 0.0993])
N: 300
Signal to noise ratio: tensor([26.6024, 69.6515, 81.2143, 38.6220])
Bound on condition number: tensor([ 212306.6000, 1455399.4418, 1978730.5524,  447498.2160])
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 0.003087055450689352, policy loss: 9.8250089572952
Experience 20, Iter 1, disc loss: 0.003057815431311556, policy loss: 9.995756595457461
Experience 20, Iter 2, disc loss: 0.00303587918104863, policy loss: 10.012867856485293
Experience 20, Iter 3, disc loss: 0.0030135891433567094, policy loss: 9.988351112695856
Experience 20, Iter 4, disc loss: 0.0029999660153075423, policy loss: 9.87017216956913
Experience 20, Iter 5, disc loss: 0.002981112346932256, policy loss: 9.867916446441921
Experience 20, Iter 6, disc loss: 0.002966351032965689, policy loss: 9.77354237515941
Experience 20, Iter 7, disc loss: 0.0029482292856693845, policy loss: 9.822853082025826
Experience 20, Iter 8, disc loss: 0.0029219550594965244, policy loss: 9.920096649167007
Experience 20, Iter 9, disc loss: 0.0029060717796169358, policy loss: 9.829163777176095
Experience 20, Iter 10, disc loss: 0.0028885990331990134, policy loss: 9.836650483951471
Experience 20, Iter 11, disc loss: 0.002865135726962223, policy loss: 9.88298836325074
Experience 20, Iter 12, disc loss: 0.002844898126194658, policy loss: 9.893979630771955
Experience 20, Iter 13, disc loss: 0.002819267422883141, policy loss: 10.045991270055277
Experience 20, Iter 14, disc loss: 0.002807856745734838, policy loss: 9.942120839997628
Experience 20, Iter 15, disc loss: 0.0027988974386002627, policy loss: 9.840766989610843
Experience 20, Iter 16, disc loss: 0.0027760131717204047, policy loss: 9.910609592345786
Experience 20, Iter 17, disc loss: 0.002758847716457162, policy loss: 9.922035343668824
Experience 20, Iter 18, disc loss: 0.0027482568883394607, policy loss: 9.835238073920971
Experience 20, Iter 19, disc loss: 0.002727558699458648, policy loss: 9.811487931580096
Experience 20, Iter 20, disc loss: 0.0027021315866294824, policy loss: 10.007946209449326
Experience 20, Iter 21, disc loss: 0.0026966832783632223, policy loss: 9.848708179452254
Experience 20, Iter 22, disc loss: 0.0026901407401963406, policy loss: 9.732707689527036
Experience 20, Iter 23, disc loss: 0.00266561822897946, policy loss: 9.899367184967522
Experience 20, Iter 24, disc loss: 0.002640055215001526, policy loss: 10.229212138696035
Experience 20, Iter 25, disc loss: 0.0026207100175005007, policy loss: 10.170474663579231
Experience 20, Iter 26, disc loss: 0.0026235235479143684, policy loss: 9.885546757117844
Experience 20, Iter 27, disc loss: 0.0026039544465171656, policy loss: 9.929217182318883
Experience 20, Iter 28, disc loss: 0.0025896323511909233, policy loss: 9.807876706794941
Experience 20, Iter 29, disc loss: 0.002588035857175019, policy loss: 9.675112807914283
Experience 20, Iter 30, disc loss: 0.002566226052540343, policy loss: 9.749841777518123
Experience 20, Iter 31, disc loss: 0.00254711593914518, policy loss: 9.806882905950268
Experience 20, Iter 32, disc loss: 0.0025347660961701833, policy loss: 9.867412931773249
Experience 20, Iter 33, disc loss: 0.0025302653963268666, policy loss: 9.621216521920672
Experience 20, Iter 34, disc loss: 0.0025098905415806686, policy loss: 9.69448303955095
Experience 20, Iter 35, disc loss: 0.0025085528452792644, policy loss: 9.532707981679096
Experience 20, Iter 36, disc loss: 0.0024955519840162152, policy loss: 9.53950428549761
Experience 20, Iter 37, disc loss: 0.0024724991434915454, policy loss: 9.65010091956949
Experience 20, Iter 38, disc loss: 0.0024786659819836126, policy loss: 9.420378663166616
Experience 20, Iter 39, disc loss: 0.0024642785827053084, policy loss: 9.54669539261278
Experience 20, Iter 40, disc loss: 0.0024382268490746126, policy loss: 9.568232247000413
Experience 20, Iter 41, disc loss: 0.002413303847181706, policy loss: 9.771129770731102
Experience 20, Iter 42, disc loss: 0.0024117879149017497, policy loss: 9.631228005740962
Experience 20, Iter 43, disc loss: 0.0024015351012106035, policy loss: 9.647763175810756
Experience 20, Iter 44, disc loss: 0.002388669285146098, policy loss: 9.608263710308945
Experience 20, Iter 45, disc loss: 0.002383966951878121, policy loss: 9.534404393816672
Experience 20, Iter 46, disc loss: 0.0023709791450061803, policy loss: 9.587938768986852
Experience 20, Iter 47, disc loss: 0.0023449521180427123, policy loss: 9.719612816760199
Experience 20, Iter 48, disc loss: 0.0023327717375063884, policy loss: 9.803321984757567
Experience 20, Iter 49, disc loss: 0.0023352940936559098, policy loss: 9.546700752224567
Experience 20, Iter 50, disc loss: 0.0023249204242954905, policy loss: 9.538967011914488
Experience 20, Iter 51, disc loss: 0.002314591584427781, policy loss: 9.417033238114964
Experience 20, Iter 52, disc loss: 0.0022959719313674905, policy loss: 9.547509287943136
Experience 20, Iter 53, disc loss: 0.0022898990417193742, policy loss: 9.515668698839113
Experience 20, Iter 54, disc loss: 0.002270073974560403, policy loss: 9.622565096050579
Experience 20, Iter 55, disc loss: 0.0022592780946898237, policy loss: 9.796739600625749
Experience 20, Iter 56, disc loss: 0.0022515496448369606, policy loss: 9.579210990801705
Experience 20, Iter 57, disc loss: 0.0022464464731204383, policy loss: 9.680149641811326
Experience 20, Iter 58, disc loss: 0.002236979743058324, policy loss: 9.549425907990852
Experience 20, Iter 59, disc loss: 0.0022271240398840098, policy loss: 9.528289938364786
Experience 20, Iter 60, disc loss: 0.0022098347105820337, policy loss: 9.597971289150937
Experience 20, Iter 61, disc loss: 0.0021938095641758884, policy loss: 9.695426361986538
Experience 20, Iter 62, disc loss: 0.0021807546296427646, policy loss: 9.752560922094615
Experience 20, Iter 63, disc loss: 0.0021720792842480937, policy loss: 9.717278117230267
Experience 20, Iter 64, disc loss: 0.002179081030719505, policy loss: 9.500489527627002
Experience 20, Iter 65, disc loss: 0.0021718246889634208, policy loss: 9.472252673825933
Experience 20, Iter 66, disc loss: 0.0021642901881901147, policy loss: 9.484254214629035
Experience 20, Iter 67, disc loss: 0.002145734211436478, policy loss: 9.566899852549966
Experience 20, Iter 68, disc loss: 0.00212930020617426, policy loss: 9.682637338557669
Experience 20, Iter 69, disc loss: 0.0021227947364277217, policy loss: 9.651284284406762
Experience 20, Iter 70, disc loss: 0.002131751754973774, policy loss: 9.459751800273093
Experience 20, Iter 71, disc loss: 0.002124257001359, policy loss: 9.400234885106903
Experience 20, Iter 72, disc loss: 0.0021091897029432093, policy loss: 9.433489295935928
Experience 20, Iter 73, disc loss: 0.0021062180840664873, policy loss: 9.315640852219886
Experience 20, Iter 74, disc loss: 0.002101856202987676, policy loss: 9.283860940139908
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0063],
        [0.1348],
        [1.3672],
        [0.0252]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0602, 0.2677, 1.1618, 0.0255, 0.0150, 4.0076]],

        [[0.0602, 0.2677, 1.1618, 0.0255, 0.0150, 4.0076]],

        [[0.0602, 0.2677, 1.1618, 0.0255, 0.0150, 4.0076]],

        [[0.0602, 0.2677, 1.1618, 0.0255, 0.0150, 4.0076]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0254, 0.5391, 5.4689, 0.1010], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0254, 0.5391, 5.4689, 0.1010])
N: 315
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1261.0000, 1261.0000, 1261.0000, 1261.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.102
Iter 2/2000 - Loss: 3.154
Iter 3/2000 - Loss: 2.961
Iter 4/2000 - Loss: 2.904
Iter 5/2000 - Loss: 2.859
Iter 6/2000 - Loss: 2.728
Iter 7/2000 - Loss: 2.571
Iter 8/2000 - Loss: 2.428
Iter 9/2000 - Loss: 2.293
Iter 10/2000 - Loss: 2.137
Iter 11/2000 - Loss: 1.950
Iter 12/2000 - Loss: 1.741
Iter 13/2000 - Loss: 1.522
Iter 14/2000 - Loss: 1.296
Iter 15/2000 - Loss: 1.061
Iter 16/2000 - Loss: 0.815
Iter 17/2000 - Loss: 0.555
Iter 18/2000 - Loss: 0.282
Iter 19/2000 - Loss: -0.002
Iter 20/2000 - Loss: -0.292
Iter 1981/2000 - Loss: -8.107
Iter 1982/2000 - Loss: -8.107
Iter 1983/2000 - Loss: -8.107
Iter 1984/2000 - Loss: -8.107
Iter 1985/2000 - Loss: -8.107
Iter 1986/2000 - Loss: -8.107
Iter 1987/2000 - Loss: -8.107
Iter 1988/2000 - Loss: -8.107
Iter 1989/2000 - Loss: -8.107
Iter 1990/2000 - Loss: -8.107
Iter 1991/2000 - Loss: -8.107
Iter 1992/2000 - Loss: -8.107
Iter 1993/2000 - Loss: -8.107
Iter 1994/2000 - Loss: -8.107
Iter 1995/2000 - Loss: -8.107
Iter 1996/2000 - Loss: -8.108
Iter 1997/2000 - Loss: -8.108
Iter 1998/2000 - Loss: -8.108
Iter 1999/2000 - Loss: -8.108
Iter 2000/2000 - Loss: -8.108
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[15.6449, 10.9740, 31.9897, 11.0801,  8.8870, 53.4339]],

        [[23.6657, 32.9265,  9.2979,  1.2619,  2.4587, 21.1927]],

        [[23.6905, 34.4107, 11.0171,  1.0483,  1.1764, 17.1886]],

        [[22.4135, 33.3281, 14.8675,  1.8045,  2.0446, 36.4118]]])
Signal Variance: tensor([ 0.2244,  1.6803, 12.1825,  0.4146])
Estimated target variance: tensor([0.0254, 0.5391, 5.4689, 0.1010])
N: 315
Signal to noise ratio: tensor([26.1455, 70.2547, 80.1438, 39.2598])
Bound on condition number: tensor([ 215331.2057, 1554752.5182, 2023256.9305,  485520.1178])
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 0.0020873854456729766, policy loss: 9.327596173954532
Experience 21, Iter 1, disc loss: 0.002084195795574357, policy loss: 9.351065466515822
Experience 21, Iter 2, disc loss: 0.0020582685343443472, policy loss: 9.536959599023735
Experience 21, Iter 3, disc loss: 0.002058727530640331, policy loss: 9.33210279010465
Experience 21, Iter 4, disc loss: 0.002055230017058447, policy loss: 9.262759287578678
Experience 21, Iter 5, disc loss: 0.002040683891886742, policy loss: 9.399384714206793
Experience 21, Iter 6, disc loss: 0.0020429270045072933, policy loss: 9.336464944814495
Experience 21, Iter 7, disc loss: 0.002029199816936969, policy loss: 9.397691749584752
Experience 21, Iter 8, disc loss: 0.0019655952409589304, policy loss: 9.975774433584773
Experience 21, Iter 9, disc loss: 0.001976072185110078, policy loss: 9.716942168905035
Experience 21, Iter 10, disc loss: 0.0019598391090511363, policy loss: 9.813899403851192
Experience 21, Iter 11, disc loss: 0.0019149485685777201, policy loss: 10.725780811691521
Experience 21, Iter 12, disc loss: 0.0019188988088237895, policy loss: 10.379240736984
Experience 21, Iter 13, disc loss: 0.0019529046493044944, policy loss: 9.935201987501866
Experience 21, Iter 14, disc loss: 0.0019146939568417335, policy loss: 10.198123481856882
Experience 21, Iter 15, disc loss: 0.0019182679272380032, policy loss: 9.820658119993247
Experience 21, Iter 16, disc loss: 0.0019061970527967222, policy loss: 9.882473425766147
Experience 21, Iter 17, disc loss: 0.0018716430652430563, policy loss: 10.78493104391203
Experience 21, Iter 18, disc loss: 0.0023103121630157603, policy loss: 11.25485406896294
Experience 21, Iter 19, disc loss: 0.0032477700662648214, policy loss: 10.338596212077219
Experience 21, Iter 20, disc loss: 0.005093621745712291, policy loss: 10.955901985816931
Experience 21, Iter 21, disc loss: 0.06324643306032085, policy loss: 9.384157173861023
Experience 21, Iter 22, disc loss: 0.04093396809503494, policy loss: 10.737625647073344
Experience 21, Iter 23, disc loss: 0.0019582450196681464, policy loss: 11.77001173886043
Experience 21, Iter 24, disc loss: 0.0018941102537798724, policy loss: 9.994540027235761
Experience 21, Iter 25, disc loss: 0.0019131356956170282, policy loss: 9.793028861306551
Experience 21, Iter 26, disc loss: 0.0019283148241390918, policy loss: 9.66765947855529
Experience 21, Iter 27, disc loss: 0.0019163775509134404, policy loss: 10.172978774974315
Experience 21, Iter 28, disc loss: 0.0019299306365234419, policy loss: 10.219422557993443
Experience 21, Iter 29, disc loss: 0.0019177785863931274, policy loss: 10.26363330015328
Experience 21, Iter 30, disc loss: 0.0019164871732418735, policy loss: 10.05915891398587
Experience 21, Iter 31, disc loss: 0.0019348609167755353, policy loss: 9.677346316539731
Experience 21, Iter 32, disc loss: 0.0019399795295469884, policy loss: 9.592095164311395
Experience 21, Iter 33, disc loss: 0.001954928731772319, policy loss: 9.44252316229223
Experience 21, Iter 34, disc loss: 0.0019424825697497332, policy loss: 9.669031380261497
Experience 21, Iter 35, disc loss: 0.001953455917665545, policy loss: 9.433361796305077
Experience 21, Iter 36, disc loss: 0.0019371394149177727, policy loss: 9.680726392185115
Experience 21, Iter 37, disc loss: 0.0019186066871628659, policy loss: 9.859313707332538
Experience 21, Iter 38, disc loss: 0.001914753780103743, policy loss: 9.750799464935653
Experience 21, Iter 39, disc loss: 0.001929237669697812, policy loss: 9.579430742984194
Experience 21, Iter 40, disc loss: 0.0019089075324436576, policy loss: 9.703119767936105
Experience 21, Iter 41, disc loss: 0.0019028246287946043, policy loss: 9.768787817340819
Experience 21, Iter 42, disc loss: 0.0019052812200320908, policy loss: 9.750357824475692
Experience 21, Iter 43, disc loss: 0.0019002331086892322, policy loss: 9.873589171784136
Experience 21, Iter 44, disc loss: 0.0019012846982201256, policy loss: 10.903832899975942
Experience 21, Iter 45, disc loss: 0.03992981073131775, policy loss: 8.657994929433473
Experience 21, Iter 46, disc loss: 0.028478470590596047, policy loss: 9.751464678069105
Experience 21, Iter 47, disc loss: 0.14410057678389027, policy loss: 7.621574987341112
Experience 21, Iter 48, disc loss: 0.43453610648431423, policy loss: 5.685726597350035
Experience 21, Iter 49, disc loss: 0.19137801743413457, policy loss: 7.3859247327752975
Experience 21, Iter 50, disc loss: 0.033340180726407265, policy loss: 9.227706109067116
Experience 21, Iter 51, disc loss: 0.12152364843333915, policy loss: 9.915912049686167
Experience 21, Iter 52, disc loss: 0.17883685844717698, policy loss: 10.133912759422806
Experience 21, Iter 53, disc loss: 0.016438021647282412, policy loss: 10.647682957279592
Experience 21, Iter 54, disc loss: 0.003529121857326787, policy loss: 10.474927129285808
Experience 21, Iter 55, disc loss: 0.0039193389437720775, policy loss: 10.313093805786743
Experience 21, Iter 56, disc loss: 0.0043129089867420325, policy loss: 10.284426162734524
Experience 21, Iter 57, disc loss: 0.00469748155246132, policy loss: 10.386138975930146
Experience 21, Iter 58, disc loss: 0.005066466872258147, policy loss: 10.690237277969917
Experience 21, Iter 59, disc loss: 0.005429614597438818, policy loss: 11.028527963269408
Experience 21, Iter 60, disc loss: 0.005764525564778748, policy loss: 12.126928473804737
Experience 21, Iter 61, disc loss: 0.006090028770254331, policy loss: 11.47659135638013
Experience 21, Iter 62, disc loss: 0.006457377514758378, policy loss: 11.73150745149696
Experience 21, Iter 63, disc loss: 0.008533759910095131, policy loss: 9.678679996030763
Experience 21, Iter 64, disc loss: 0.11582672364623375, policy loss: 7.804081592474152
Experience 21, Iter 65, disc loss: 0.2359346505656877, policy loss: 6.377709277314056
Experience 21, Iter 66, disc loss: 0.27178360955960995, policy loss: 6.298916889576771
Experience 21, Iter 67, disc loss: 0.43395873194546586, policy loss: 4.075679242362689
Experience 21, Iter 68, disc loss: 0.9546885678947642, policy loss: 2.1973173743408063
Experience 21, Iter 69, disc loss: 0.7601656929153298, policy loss: 2.074985148610218
Experience 21, Iter 70, disc loss: 0.25219650408352795, policy loss: 4.080816309525503
Experience 21, Iter 71, disc loss: 0.04112819320801862, policy loss: 8.507282443546096
Experience 21, Iter 72, disc loss: 0.052290572295104726, policy loss: 12.00667535535826
Experience 21, Iter 73, disc loss: 0.0719506501115308, policy loss: 15.730484754450561
Experience 21, Iter 74, disc loss: 0.09391171815779424, policy loss: 19.1389942808691
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0062],
        [0.1432],
        [1.4721],
        [0.0267]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0577, 0.2645, 1.2401, 0.0265, 0.0148, 4.0925]],

        [[0.0577, 0.2645, 1.2401, 0.0265, 0.0148, 4.0925]],

        [[0.0577, 0.2645, 1.2401, 0.0265, 0.0148, 4.0925]],

        [[0.0577, 0.2645, 1.2401, 0.0265, 0.0148, 4.0925]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0249, 0.5729, 5.8883, 0.1069], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0249, 0.5729, 5.8883, 0.1069])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.178
Iter 2/2000 - Loss: 3.233
Iter 3/2000 - Loss: 3.037
Iter 4/2000 - Loss: 2.982
Iter 5/2000 - Loss: 2.938
Iter 6/2000 - Loss: 2.810
Iter 7/2000 - Loss: 2.654
Iter 8/2000 - Loss: 2.515
Iter 9/2000 - Loss: 2.384
Iter 10/2000 - Loss: 2.233
Iter 11/2000 - Loss: 2.053
Iter 12/2000 - Loss: 1.851
Iter 13/2000 - Loss: 1.637
Iter 14/2000 - Loss: 1.417
Iter 15/2000 - Loss: 1.190
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.697
Iter 18/2000 - Loss: 0.430
Iter 19/2000 - Loss: 0.151
Iter 20/2000 - Loss: -0.135
Iter 1981/2000 - Loss: -8.079
Iter 1982/2000 - Loss: -8.079
Iter 1983/2000 - Loss: -8.079
Iter 1984/2000 - Loss: -8.079
Iter 1985/2000 - Loss: -8.079
Iter 1986/2000 - Loss: -8.079
Iter 1987/2000 - Loss: -8.079
Iter 1988/2000 - Loss: -8.079
Iter 1989/2000 - Loss: -8.079
Iter 1990/2000 - Loss: -8.079
Iter 1991/2000 - Loss: -8.079
Iter 1992/2000 - Loss: -8.079
Iter 1993/2000 - Loss: -8.079
Iter 1994/2000 - Loss: -8.079
Iter 1995/2000 - Loss: -8.079
Iter 1996/2000 - Loss: -8.079
Iter 1997/2000 - Loss: -8.079
Iter 1998/2000 - Loss: -8.079
Iter 1999/2000 - Loss: -8.079
Iter 2000/2000 - Loss: -8.080
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[15.3026, 13.0896, 30.6800, 10.0167,  5.5817, 55.7882]],

        [[23.3833, 33.3820,  8.3140,  1.3468,  2.3035, 22.4921]],

        [[24.4479, 35.7628,  9.2375,  1.0754,  1.0602, 19.5398]],

        [[22.2511, 33.1283, 14.1297,  1.7242,  2.1969, 37.5549]]])
Signal Variance: tensor([ 0.2383,  1.7610, 12.3624,  0.4021])
Estimated target variance: tensor([0.0249, 0.5729, 5.8883, 0.1069])
N: 330
Signal to noise ratio: tensor([27.2339, 71.2527, 81.8541, 38.8553])
Bound on condition number: tensor([ 244756.5685, 1675392.7059, 2211032.3352,  498214.2650])
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 0.11387885174926532, policy loss: 20.447023274634038
Experience 22, Iter 1, disc loss: 0.12823135357763782, policy loss: 19.717887592719727
Experience 22, Iter 2, disc loss: 0.1345043202369355, policy loss: 17.044986815000122
Experience 22, Iter 3, disc loss: 0.13220737733814117, policy loss: 14.377030295866145
Experience 22, Iter 4, disc loss: 0.12282940541336775, policy loss: 13.165238849190047
Experience 22, Iter 5, disc loss: 0.10907386765413205, policy loss: 12.86300856495048
Experience 22, Iter 6, disc loss: 0.09373800460701305, policy loss: 12.881364171293068
Experience 22, Iter 7, disc loss: 0.07901342543109152, policy loss: 13.604078491371661
Experience 22, Iter 8, disc loss: 0.06583765168317722, policy loss: 12.860829596259181
Experience 22, Iter 9, disc loss: 0.054643499083889505, policy loss: 17.260204809533736
Experience 22, Iter 10, disc loss: 0.045506148053626745, policy loss: 14.153373374858333
Experience 22, Iter 11, disc loss: 0.03803630660131269, policy loss: 13.830392154492833
Experience 22, Iter 12, disc loss: 0.03193340417691819, policy loss: 13.834421522031295
Experience 22, Iter 13, disc loss: 0.026977473037308346, policy loss: 14.301050862201407
Experience 22, Iter 14, disc loss: 0.022988388933998724, policy loss: 14.325314485309011
Experience 22, Iter 15, disc loss: 0.019847625367087115, policy loss: 13.634791024442489
Experience 22, Iter 16, disc loss: 0.01723107988981634, policy loss: 13.5113387032088
Experience 22, Iter 17, disc loss: 0.015159807065177094, policy loss: 12.67590404424352
Experience 22, Iter 18, disc loss: 0.013471022779823904, policy loss: 11.925485099992796
Experience 22, Iter 19, disc loss: 0.012086257890340884, policy loss: 11.86284621008752
Experience 22, Iter 20, disc loss: 0.010945770007152973, policy loss: 11.87748848157272
Experience 22, Iter 21, disc loss: 0.009989738542916519, policy loss: 12.948884560124313
Experience 22, Iter 22, disc loss: 0.009189986076295625, policy loss: 12.962346829165746
Experience 22, Iter 23, disc loss: 0.008513816695422862, policy loss: 12.34754077647407
Experience 22, Iter 24, disc loss: 0.007939717691447723, policy loss: 11.825592050760703
Experience 22, Iter 25, disc loss: 0.0074322424859506854, policy loss: 11.598981122127725
Experience 22, Iter 26, disc loss: 0.006997868350083799, policy loss: 11.064626073178275
Experience 22, Iter 27, disc loss: 0.006602876796575286, policy loss: 11.593332425777252
Experience 22, Iter 28, disc loss: 0.0062676610946771375, policy loss: 11.161172577369074
Experience 22, Iter 29, disc loss: 0.005965987553852188, policy loss: 11.37671418876797
Experience 22, Iter 30, disc loss: 0.0057013995545351385, policy loss: 11.776327606958567
Experience 22, Iter 31, disc loss: 0.0054668489668772, policy loss: 12.325067915330276
Experience 22, Iter 32, disc loss: 0.005259649434555184, policy loss: 12.309493539686443
Experience 22, Iter 33, disc loss: 0.005074591293709478, policy loss: 12.076125395965446
Experience 22, Iter 34, disc loss: 0.004908326556153386, policy loss: 11.959529455735915
Experience 22, Iter 35, disc loss: 0.004758353495307506, policy loss: 11.769737192930652
Experience 22, Iter 36, disc loss: 0.0046234125904759055, policy loss: 11.65155985747597
Experience 22, Iter 37, disc loss: 0.004501759005795375, policy loss: 11.449758239714342
Experience 22, Iter 38, disc loss: 0.004389035474448251, policy loss: 11.413348000739935
Experience 22, Iter 39, disc loss: 0.004287137061296723, policy loss: 11.306298442463419
Experience 22, Iter 40, disc loss: 0.004189548570270405, policy loss: 11.442011899235743
Experience 22, Iter 41, disc loss: 0.0041037916479474785, policy loss: 11.227984646386616
Experience 22, Iter 42, disc loss: 0.00401808388843929, policy loss: 11.527604062483775
Experience 22, Iter 43, disc loss: 0.003941005100602819, policy loss: 11.650863910600997
Experience 22, Iter 44, disc loss: 0.0038718266175866356, policy loss: 11.47861436368149
Experience 22, Iter 45, disc loss: 0.003808891981542857, policy loss: 11.179197868660607
Experience 22, Iter 46, disc loss: 0.0037456871731167745, policy loss: 11.305113913745974
Experience 22, Iter 47, disc loss: 0.0036897633553002666, policy loss: 11.209050896962733
Experience 22, Iter 48, disc loss: 0.003636522864862662, policy loss: 11.132218755935817
Experience 22, Iter 49, disc loss: 0.0035850872322817974, policy loss: 11.153543285353258
Experience 22, Iter 50, disc loss: 0.003539770772926055, policy loss: 10.955084610969521
Experience 22, Iter 51, disc loss: 0.0034920024705453194, policy loss: 11.198284340772686
Experience 22, Iter 52, disc loss: 0.0034473300556633964, policy loss: 11.287469965115772
Experience 22, Iter 53, disc loss: 0.0034070808386609873, policy loss: 11.132379285766955
Experience 22, Iter 54, disc loss: 0.00336660811932512, policy loss: 11.209687206903647
Experience 22, Iter 55, disc loss: 0.003328194080449893, policy loss: 11.244563970797856
Experience 22, Iter 56, disc loss: 0.0032931612667801255, policy loss: 11.151376552048175
Experience 22, Iter 57, disc loss: 0.0032598163847866544, policy loss: 11.180276165160823
Experience 22, Iter 58, disc loss: 0.003226145194243869, policy loss: 11.0553695223342
Experience 22, Iter 59, disc loss: 0.0031935174596722502, policy loss: 11.083767233610029
Experience 22, Iter 60, disc loss: 0.00316379880338465, policy loss: 11.039632773891617
Experience 22, Iter 61, disc loss: 0.0031326024493131114, policy loss: 11.133730593814446
Experience 22, Iter 62, disc loss: 0.0031084617815265716, policy loss: 11.022742621872343
Experience 22, Iter 63, disc loss: 0.0030728399493646706, policy loss: 11.311762539685194
Experience 22, Iter 64, disc loss: 0.003042234684763371, policy loss: 11.533362197000727
Experience 22, Iter 65, disc loss: 0.0030128409151280464, policy loss: 11.689191040456834
Experience 22, Iter 66, disc loss: 0.0029888081019203705, policy loss: 11.639033043170002
Experience 22, Iter 67, disc loss: 0.002966418456572979, policy loss: 11.402698848740048
Experience 22, Iter 68, disc loss: 0.0029435793215306875, policy loss: 11.162996728896049
Experience 22, Iter 69, disc loss: 0.002924118955847466, policy loss: 10.934191239429422
Experience 22, Iter 70, disc loss: 0.002906774875437162, policy loss: 10.713704096142365
Experience 22, Iter 71, disc loss: 0.002888400277513554, policy loss: 10.56342376938294
Experience 22, Iter 72, disc loss: 0.0028600259039573035, policy loss: 10.837605629466328
Experience 22, Iter 73, disc loss: 0.0028307844008444447, policy loss: 11.024179994187943
Experience 22, Iter 74, disc loss: 0.002806212794103926, policy loss: 11.275360890291072
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.1448],
        [1.4834],
        [0.0271]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0553, 0.2587, 1.2596, 0.0268, 0.0149, 4.1497]],

        [[0.0553, 0.2587, 1.2596, 0.0268, 0.0149, 4.1497]],

        [[0.0553, 0.2587, 1.2596, 0.0268, 0.0149, 4.1497]],

        [[0.0553, 0.2587, 1.2596, 0.0268, 0.0149, 4.1497]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0243, 0.5792, 5.9335, 0.1086], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0243, 0.5792, 5.9335, 0.1086])
N: 345
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1381.0000, 1381.0000, 1381.0000, 1381.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.183
Iter 2/2000 - Loss: 3.238
Iter 3/2000 - Loss: 3.041
Iter 4/2000 - Loss: 2.986
Iter 5/2000 - Loss: 2.942
Iter 6/2000 - Loss: 2.812
Iter 7/2000 - Loss: 2.656
Iter 8/2000 - Loss: 2.515
Iter 9/2000 - Loss: 2.383
Iter 10/2000 - Loss: 2.231
Iter 11/2000 - Loss: 2.049
Iter 12/2000 - Loss: 1.844
Iter 13/2000 - Loss: 1.628
Iter 14/2000 - Loss: 1.405
Iter 15/2000 - Loss: 1.174
Iter 16/2000 - Loss: 0.931
Iter 17/2000 - Loss: 0.674
Iter 18/2000 - Loss: 0.402
Iter 19/2000 - Loss: 0.119
Iter 20/2000 - Loss: -0.172
Iter 1981/2000 - Loss: -8.135
Iter 1982/2000 - Loss: -8.135
Iter 1983/2000 - Loss: -8.135
Iter 1984/2000 - Loss: -8.135
Iter 1985/2000 - Loss: -8.135
Iter 1986/2000 - Loss: -8.135
Iter 1987/2000 - Loss: -8.135
Iter 1988/2000 - Loss: -8.135
Iter 1989/2000 - Loss: -8.135
Iter 1990/2000 - Loss: -8.135
Iter 1991/2000 - Loss: -8.135
Iter 1992/2000 - Loss: -8.135
Iter 1993/2000 - Loss: -8.135
Iter 1994/2000 - Loss: -8.135
Iter 1995/2000 - Loss: -8.135
Iter 1996/2000 - Loss: -8.135
Iter 1997/2000 - Loss: -8.135
Iter 1998/2000 - Loss: -8.135
Iter 1999/2000 - Loss: -8.135
Iter 2000/2000 - Loss: -8.135
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[14.9967, 12.4731, 31.1863,  9.5174,  5.3824, 55.4203]],

        [[23.0530, 33.1026,  8.3577,  1.3517,  2.3115, 21.3653]],

        [[24.0375, 35.5120,  9.4085,  1.0654,  1.0229, 19.2104]],

        [[21.7338, 32.0618, 13.6713,  1.7609,  2.1457, 37.8338]]])
Signal Variance: tensor([ 0.2235,  1.6474, 11.7774,  0.3822])
Estimated target variance: tensor([0.0243, 0.5792, 5.9335, 0.1086])
N: 345
Signal to noise ratio: tensor([26.4720, 68.1714, 80.2911, 38.0803])
Bound on condition number: tensor([ 241764.9186, 1603332.2895, 2224100.7390,  500289.5152])
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 0.002784846266630863, policy loss: 11.312403501492422
Experience 23, Iter 1, disc loss: 0.002760723445551077, policy loss: 11.804640622862598
Experience 23, Iter 2, disc loss: 0.002738110703988144, policy loss: 11.976586628766986
Experience 23, Iter 3, disc loss: 0.0027172115817705456, policy loss: 12.020723929283866
Experience 23, Iter 4, disc loss: 0.0026955070232007696, policy loss: 12.351147995368137
Experience 23, Iter 5, disc loss: 0.002676500640815912, policy loss: 12.38106968062002
Experience 23, Iter 6, disc loss: 0.002657807073011285, policy loss: 12.290406845338634
Experience 23, Iter 7, disc loss: 0.002643316930231469, policy loss: 11.832223246917879
Experience 23, Iter 8, disc loss: 0.002627870163959679, policy loss: 11.643871188118641
Experience 23, Iter 9, disc loss: 0.0026092973407386938, policy loss: 11.687735547014386
Experience 23, Iter 10, disc loss: 0.0025927467273143393, policy loss: 11.422047479594534
Experience 23, Iter 11, disc loss: 0.002574021888803636, policy loss: 11.566485089125301
Experience 23, Iter 12, disc loss: 0.0025561244878889965, policy loss: 11.712570592874206
Experience 23, Iter 13, disc loss: 0.002538862303786083, policy loss: 11.845967994332927
Experience 23, Iter 14, disc loss: 0.002521277635241652, policy loss: 11.862021705797918
Experience 23, Iter 15, disc loss: 0.0025046695661384445, policy loss: 11.85568163026736
Experience 23, Iter 16, disc loss: 0.002487279540714905, policy loss: 12.08172945026585
Experience 23, Iter 17, disc loss: 0.002474801426284527, policy loss: 11.764711710867168
Experience 23, Iter 18, disc loss: 0.002459702016655669, policy loss: 11.652793184245308
Experience 23, Iter 19, disc loss: 0.0024452350849268212, policy loss: 11.614820873042706
Experience 23, Iter 20, disc loss: 0.0024332437860930243, policy loss: 11.3999701820322
Experience 23, Iter 21, disc loss: 0.0024221431775467116, policy loss: 11.089752944298862
Experience 23, Iter 22, disc loss: 0.002408945231785302, policy loss: 11.015987386627257
Experience 23, Iter 23, disc loss: 0.0023952386159063555, policy loss: 11.006811990401369
Experience 23, Iter 24, disc loss: 0.0023798791315588455, policy loss: 11.03827559984331
Experience 23, Iter 25, disc loss: 0.0023632920778615314, policy loss: 11.21710407574842
Experience 23, Iter 26, disc loss: 0.0023495368190596375, policy loss: 11.213962581484072
Experience 23, Iter 27, disc loss: 0.0023346919722926508, policy loss: 11.276182587074237
Experience 23, Iter 28, disc loss: 0.002320617565264348, policy loss: 11.351586533891354
Experience 23, Iter 29, disc loss: 0.0023081756368588096, policy loss: 11.307810044327898
Experience 23, Iter 30, disc loss: 0.0022941891799857935, policy loss: 11.399914586644538
Experience 23, Iter 31, disc loss: 0.0022807519187386634, policy loss: 11.423567780464566
Experience 23, Iter 32, disc loss: 0.002268788159403314, policy loss: 11.42403477196996
Experience 23, Iter 33, disc loss: 0.0022570386965417425, policy loss: 11.364084595609933
Experience 23, Iter 34, disc loss: 0.002247184220470141, policy loss: 11.110699855200615
Experience 23, Iter 35, disc loss: 0.0022359397611768895, policy loss: 11.183444160960397
Experience 23, Iter 36, disc loss: 0.002223551690950334, policy loss: 11.12648116186185
Experience 23, Iter 37, disc loss: 0.002210795061077717, policy loss: 11.117367214525569
Experience 23, Iter 38, disc loss: 0.0021989666864775785, policy loss: 11.144452409985387
Experience 23, Iter 39, disc loss: 0.0021881220704168306, policy loss: 11.094711524228023
Experience 23, Iter 40, disc loss: 0.002175690899390548, policy loss: 11.204237991546846
Experience 23, Iter 41, disc loss: 0.002166903065583896, policy loss: 11.029646808478077
Experience 23, Iter 42, disc loss: 0.002157110520851455, policy loss: 10.95140050673933
Experience 23, Iter 43, disc loss: 0.0021465177906491223, policy loss: 11.004321317633337
Experience 23, Iter 44, disc loss: 0.002138004789774853, policy loss: 10.75898390292508
Experience 23, Iter 45, disc loss: 0.002126630134470649, policy loss: 10.84339530045351
Experience 23, Iter 46, disc loss: 0.002117431426505537, policy loss: 10.762535185877285
Experience 23, Iter 47, disc loss: 0.0021055631978935344, policy loss: 10.917669490015665
Experience 23, Iter 48, disc loss: 0.002100353217592378, policy loss: 10.777821709437797
Experience 23, Iter 49, disc loss: 0.00208679172299536, policy loss: 10.700235690734669
Experience 23, Iter 50, disc loss: 0.0020767715804136657, policy loss: 10.774695935185488
Experience 23, Iter 51, disc loss: 0.0020648677957770525, policy loss: 10.87638529920366
Experience 23, Iter 52, disc loss: 0.0020571491060917485, policy loss: 10.67674583596619
Experience 23, Iter 53, disc loss: 0.00204703142326592, policy loss: 10.693522586730248
Experience 23, Iter 54, disc loss: 0.0020367803032836627, policy loss: 10.71709020365843
Experience 23, Iter 55, disc loss: 0.002027853699804709, policy loss: 10.630953756136071
Experience 23, Iter 56, disc loss: 0.0020186197933964893, policy loss: 10.697395062094918
Experience 23, Iter 57, disc loss: 0.0020071478505009443, policy loss: 10.746559145984346
Experience 23, Iter 58, disc loss: 0.001998971216455544, policy loss: 10.698560871468594
Experience 23, Iter 59, disc loss: 0.0019893644353877254, policy loss: 10.71005446645887
Experience 23, Iter 60, disc loss: 0.001984006924669681, policy loss: 10.517879997558051
Experience 23, Iter 61, disc loss: 0.001972327102594382, policy loss: 10.645343850453147
Experience 23, Iter 62, disc loss: 0.0019627615197902446, policy loss: 10.699748025167217
Experience 23, Iter 63, disc loss: 0.0019545782100805816, policy loss: 10.709670600298754
Experience 23, Iter 64, disc loss: 0.0019458250768829668, policy loss: 10.606596901915923
Experience 23, Iter 65, disc loss: 0.001937978220523704, policy loss: 10.575802265796165
Experience 23, Iter 66, disc loss: 0.001927114549207909, policy loss: 10.706046638104063
Experience 23, Iter 67, disc loss: 0.0019212670943678355, policy loss: 10.548840210145215
Experience 23, Iter 68, disc loss: 0.001907889586579523, policy loss: 10.771268322884765
Experience 23, Iter 69, disc loss: 0.0018972874427907885, policy loss: 10.791671472912888
Experience 23, Iter 70, disc loss: 0.001886971629600112, policy loss: 10.893362483230732
Experience 23, Iter 71, disc loss: 0.0018769574563339394, policy loss: 10.963254816234885
Experience 23, Iter 72, disc loss: 0.0018721090065635718, policy loss: 10.924422579791912
Experience 23, Iter 73, disc loss: 0.0018661564273162944, policy loss: 10.718165015892671
Experience 23, Iter 74, disc loss: 0.0018619445663730924, policy loss: 10.522118778679095
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.1481],
        [1.5203],
        [0.0272]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0531, 0.2516, 1.2682, 0.0268, 0.0145, 4.1742]],

        [[0.0531, 0.2516, 1.2682, 0.0268, 0.0145, 4.1742]],

        [[0.0531, 0.2516, 1.2682, 0.0268, 0.0145, 4.1742]],

        [[0.0531, 0.2516, 1.2682, 0.0268, 0.0145, 4.1742]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0235, 0.5923, 6.0814, 0.1089], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0235, 0.5923, 6.0814, 0.1089])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.198
Iter 2/2000 - Loss: 3.261
Iter 3/2000 - Loss: 3.059
Iter 4/2000 - Loss: 3.006
Iter 5/2000 - Loss: 2.967
Iter 6/2000 - Loss: 2.839
Iter 7/2000 - Loss: 2.683
Iter 8/2000 - Loss: 2.544
Iter 9/2000 - Loss: 2.415
Iter 10/2000 - Loss: 2.267
Iter 11/2000 - Loss: 2.087
Iter 12/2000 - Loss: 1.883
Iter 13/2000 - Loss: 1.667
Iter 14/2000 - Loss: 1.445
Iter 15/2000 - Loss: 1.214
Iter 16/2000 - Loss: 0.970
Iter 17/2000 - Loss: 0.711
Iter 18/2000 - Loss: 0.436
Iter 19/2000 - Loss: 0.149
Iter 20/2000 - Loss: -0.147
Iter 1981/2000 - Loss: -8.169
Iter 1982/2000 - Loss: -8.170
Iter 1983/2000 - Loss: -8.170
Iter 1984/2000 - Loss: -8.170
Iter 1985/2000 - Loss: -8.170
Iter 1986/2000 - Loss: -8.170
Iter 1987/2000 - Loss: -8.170
Iter 1988/2000 - Loss: -8.170
Iter 1989/2000 - Loss: -8.170
Iter 1990/2000 - Loss: -8.170
Iter 1991/2000 - Loss: -8.170
Iter 1992/2000 - Loss: -8.170
Iter 1993/2000 - Loss: -8.170
Iter 1994/2000 - Loss: -8.170
Iter 1995/2000 - Loss: -8.170
Iter 1996/2000 - Loss: -8.170
Iter 1997/2000 - Loss: -8.170
Iter 1998/2000 - Loss: -8.170
Iter 1999/2000 - Loss: -8.170
Iter 2000/2000 - Loss: -8.170
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[14.7238, 12.4263, 31.9322,  9.8649,  5.7870, 54.0354]],

        [[22.9564, 32.8051,  8.4508,  1.3614,  2.0563, 22.7844]],

        [[24.0226, 35.4901,  9.4522,  1.1027,  0.9902, 19.3235]],

        [[21.5146, 31.3417, 13.9918,  1.7446,  2.1623, 38.2481]]])
Signal Variance: tensor([ 0.2264,  1.7179, 11.6392,  0.3899])
Estimated target variance: tensor([0.0235, 0.5923, 6.0814, 0.1089])
N: 360
Signal to noise ratio: tensor([26.4716, 69.8647, 78.6032, 38.4790])
Bound on condition number: tensor([ 252269.0767, 1757189.5623, 2224245.0112,  533029.4755])
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 0.0018514495815828591, policy loss: 10.743882123163718
Experience 24, Iter 1, disc loss: 0.0018468551125976947, policy loss: 10.660555888957619
Experience 24, Iter 2, disc loss: 0.0018345197049963575, policy loss: 10.766148774860097
Experience 24, Iter 3, disc loss: 0.0018291233228308096, policy loss: 10.607899802325385
Experience 24, Iter 4, disc loss: 0.001821623284433194, policy loss: 10.60055200481498
Experience 24, Iter 5, disc loss: 0.001810728914484288, policy loss: 10.720251367247108
Experience 24, Iter 6, disc loss: 0.0018021454303281555, policy loss: 10.690128311599308
Experience 24, Iter 7, disc loss: 0.00179316609940582, policy loss: 10.787654443591443
Experience 24, Iter 8, disc loss: 0.0017908936001932334, policy loss: 10.529157245367088
Experience 24, Iter 9, disc loss: 0.0017839393628125105, policy loss: 10.591459427368099
Experience 24, Iter 10, disc loss: 0.0017766736165599497, policy loss: 10.576397922182075
Experience 24, Iter 11, disc loss: 0.0017735793672314757, policy loss: 10.438309281397213
Experience 24, Iter 12, disc loss: 0.001765297512542367, policy loss: 10.457617393324352
Experience 24, Iter 13, disc loss: 0.001757996465681953, policy loss: 10.446546320344094
Experience 24, Iter 14, disc loss: 0.001745958917069236, policy loss: 10.658265883123345
Experience 24, Iter 15, disc loss: 0.0017424357936482195, policy loss: 10.439774436654044
Experience 24, Iter 16, disc loss: 0.001735353207003867, policy loss: 10.513294710026463
Experience 24, Iter 17, disc loss: 0.0017307210827356987, policy loss: 10.408885712668383
Experience 24, Iter 18, disc loss: 0.0017220635019720382, policy loss: 10.453170890012128
Experience 24, Iter 19, disc loss: 0.0017138645919687015, policy loss: 10.49347189872065
Experience 24, Iter 20, disc loss: 0.001708863250303917, policy loss: 10.450439317769973
Experience 24, Iter 21, disc loss: 0.001704581694536084, policy loss: 10.338836081074549
Experience 24, Iter 22, disc loss: 0.001696430664659264, policy loss: 10.404779580893775
Experience 24, Iter 23, disc loss: 0.0016923744621697718, policy loss: 10.370002682041616
Experience 24, Iter 24, disc loss: 0.001688623573224903, policy loss: 10.235819591665084
Experience 24, Iter 25, disc loss: 0.0016792767671958825, policy loss: 10.334979548864894
Experience 24, Iter 26, disc loss: 0.0016724142830452616, policy loss: 10.30250999641871
Experience 24, Iter 27, disc loss: 0.0016631255283276886, policy loss: 10.417727022854962
Experience 24, Iter 28, disc loss: 0.001663325794632985, policy loss: 10.216521849505238
Experience 24, Iter 29, disc loss: 0.0016586675331231402, policy loss: 10.19363535184051
Experience 24, Iter 30, disc loss: 0.001650057103516506, policy loss: 10.36249699275961
Experience 24, Iter 31, disc loss: 0.0016457410873795988, policy loss: 10.24817316623351
Experience 24, Iter 32, disc loss: 0.0016426353593669348, policy loss: 10.176582612956711
Experience 24, Iter 33, disc loss: 0.0016314730636857403, policy loss: 10.299995460228732
Experience 24, Iter 34, disc loss: 0.0016310503474736935, policy loss: 10.123652583725757
Experience 24, Iter 35, disc loss: 0.0016263281266071519, policy loss: 10.106121996807051
Experience 24, Iter 36, disc loss: 0.00161427048302083, policy loss: 10.209712620337307
Experience 24, Iter 37, disc loss: 0.0016060750609918134, policy loss: 10.30687743510894
Experience 24, Iter 38, disc loss: 0.0016023289534564872, policy loss: 10.204258806524308
Experience 24, Iter 39, disc loss: 0.0015934243382318756, policy loss: 10.264910796826932
Experience 24, Iter 40, disc loss: 0.001588582173867978, policy loss: 10.318296979362293
Experience 24, Iter 41, disc loss: 0.0015815126737080672, policy loss: 10.3869337588832
Experience 24, Iter 42, disc loss: 0.001577252304640548, policy loss: 10.26986887279961
Experience 24, Iter 43, disc loss: 0.00157460412368066, policy loss: 10.22463668637728
Experience 24, Iter 44, disc loss: 0.0015679308423161371, policy loss: 10.192162400336276
Experience 24, Iter 45, disc loss: 0.001556219399363807, policy loss: 10.315119896761736
Experience 24, Iter 46, disc loss: 0.0015485679917645614, policy loss: 10.382250913729875
Experience 24, Iter 47, disc loss: 0.0015536034262557219, policy loss: 10.126855890625176
Experience 24, Iter 48, disc loss: 0.0015530315802907404, policy loss: 10.060306183452703
Experience 24, Iter 49, disc loss: 0.0015453809198236518, policy loss: 10.208075747565278
Experience 24, Iter 50, disc loss: 0.0015349484233279605, policy loss: 10.163251379640483
Experience 24, Iter 51, disc loss: 0.0015314127365953618, policy loss: 10.114632663476518
Experience 24, Iter 52, disc loss: 0.0015253293489852977, policy loss: 10.203517570846882
Experience 24, Iter 53, disc loss: 0.0015199383581537239, policy loss: 10.193252450619458
Experience 24, Iter 54, disc loss: 0.001516475811140749, policy loss: 10.103191278688815
Experience 24, Iter 55, disc loss: 0.0015062295346058307, policy loss: 10.261926170621456
Experience 24, Iter 56, disc loss: 0.001503025457114814, policy loss: 10.259310908668496
Experience 24, Iter 57, disc loss: 0.001506689545703281, policy loss: 9.992226730933206
Experience 24, Iter 58, disc loss: 0.0014958827763230003, policy loss: 10.155355612452032
Experience 24, Iter 59, disc loss: 0.0014939489661237035, policy loss: 10.098550538934578
Experience 24, Iter 60, disc loss: 0.001485074401508504, policy loss: 10.157864995236986
Experience 24, Iter 61, disc loss: 0.001491542011111805, policy loss: 9.920111710304582
Experience 24, Iter 62, disc loss: 0.001476362046530347, policy loss: 10.11947127110249
Experience 24, Iter 63, disc loss: 0.0014774181565572733, policy loss: 9.999347842435782
Experience 24, Iter 64, disc loss: 0.0014643403402837052, policy loss: 10.150407563398327
Experience 24, Iter 65, disc loss: 0.00146180351225886, policy loss: 10.242812261726282
Experience 24, Iter 66, disc loss: 0.0014512388274934866, policy loss: 10.293999600395658
Experience 24, Iter 67, disc loss: 0.0014424377003056546, policy loss: 10.340765124914363
Experience 24, Iter 68, disc loss: 0.001434246439826052, policy loss: 10.452628263841348
Experience 24, Iter 69, disc loss: 0.0014277272256447613, policy loss: 10.62076693236446
Experience 24, Iter 70, disc loss: 0.001435352206778445, policy loss: 10.518527256813428
Experience 24, Iter 71, disc loss: 0.0018339668237988434, policy loss: 10.03342255435841
Experience 24, Iter 72, disc loss: 0.011546245376249731, policy loss: 8.840347280098896
Experience 24, Iter 73, disc loss: 0.045597251338662964, policy loss: 7.236385106052563
Experience 24, Iter 74, disc loss: 0.044313192371716194, policy loss: 6.767063653175676
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0058],
        [0.1499],
        [1.5309],
        [0.0284]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0512, 0.2478, 1.3099, 0.0274, 0.0167, 4.2698]],

        [[0.0512, 0.2478, 1.3099, 0.0274, 0.0167, 4.2698]],

        [[0.0512, 0.2478, 1.3099, 0.0274, 0.0167, 4.2698]],

        [[0.0512, 0.2478, 1.3099, 0.0274, 0.0167, 4.2698]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0231, 0.5996, 6.1236, 0.1134], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0231, 0.5996, 6.1236, 0.1134])
N: 375
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1501.0000, 1501.0000, 1501.0000, 1501.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.221
Iter 2/2000 - Loss: 3.290
Iter 3/2000 - Loss: 3.079
Iter 4/2000 - Loss: 3.025
Iter 5/2000 - Loss: 2.985
Iter 6/2000 - Loss: 2.851
Iter 7/2000 - Loss: 2.688
Iter 8/2000 - Loss: 2.540
Iter 9/2000 - Loss: 2.404
Iter 10/2000 - Loss: 2.250
Iter 11/2000 - Loss: 2.065
Iter 12/2000 - Loss: 1.854
Iter 13/2000 - Loss: 1.630
Iter 14/2000 - Loss: 1.398
Iter 15/2000 - Loss: 1.160
Iter 16/2000 - Loss: 0.909
Iter 17/2000 - Loss: 0.645
Iter 18/2000 - Loss: 0.365
Iter 19/2000 - Loss: 0.073
Iter 20/2000 - Loss: -0.227
Iter 1981/2000 - Loss: -8.194
Iter 1982/2000 - Loss: -8.194
Iter 1983/2000 - Loss: -8.194
Iter 1984/2000 - Loss: -8.194
Iter 1985/2000 - Loss: -8.194
Iter 1986/2000 - Loss: -8.194
Iter 1987/2000 - Loss: -8.194
Iter 1988/2000 - Loss: -8.194
Iter 1989/2000 - Loss: -8.194
Iter 1990/2000 - Loss: -8.194
Iter 1991/2000 - Loss: -8.194
Iter 1992/2000 - Loss: -8.194
Iter 1993/2000 - Loss: -8.195
Iter 1994/2000 - Loss: -8.195
Iter 1995/2000 - Loss: -8.195
Iter 1996/2000 - Loss: -8.195
Iter 1997/2000 - Loss: -8.195
Iter 1998/2000 - Loss: -8.195
Iter 1999/2000 - Loss: -8.195
Iter 2000/2000 - Loss: -8.195
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[14.5300, 12.7752, 31.2249,  9.0933,  5.4366, 54.3787]],

        [[22.2090, 32.1301,  8.4028,  1.3776,  1.9120, 22.8123]],

        [[22.7445, 34.8683,  8.7674,  1.1399,  1.0156, 19.8051]],

        [[20.9138, 30.7614, 14.3131,  1.7839,  1.9873, 39.7820]]])
Signal Variance: tensor([ 0.2285,  1.6892, 11.5845,  0.3914])
Estimated target variance: tensor([0.0231, 0.5996, 6.1236, 0.1134])
N: 375
Signal to noise ratio: tensor([26.8930, 70.5669, 77.5819, 38.4887])
Bound on condition number: tensor([ 271213.6950, 1867386.0262, 2257107.2208,  555517.4983])
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 0.12196960929015538, policy loss: 8.120917704634625
Experience 25, Iter 1, disc loss: 0.0014532308931147994, policy loss: 11.595425618222638
Experience 25, Iter 2, disc loss: 0.0014799164394123684, policy loss: 12.26936901877735
Experience 25, Iter 3, disc loss: 0.0015134940080837784, policy loss: 11.961291991847208
Experience 25, Iter 4, disc loss: 0.0015446901579886135, policy loss: 11.809826990132596
Experience 25, Iter 5, disc loss: 0.0015731378853421493, policy loss: 11.527222779500015
Experience 25, Iter 6, disc loss: 0.0015974946095297208, policy loss: 11.469437952654106
Experience 25, Iter 7, disc loss: 0.0016144758392751923, policy loss: 11.935495859142783
Experience 25, Iter 8, disc loss: 0.0016315148589308695, policy loss: 12.203512969406608
Experience 25, Iter 9, disc loss: 0.0016516936991768378, policy loss: 12.160787772494665
Experience 25, Iter 10, disc loss: 0.0016632142728570556, policy loss: 12.351501010233903
Experience 25, Iter 11, disc loss: 0.0016770222315337464, policy loss: 12.141054844499278
Experience 25, Iter 12, disc loss: 0.001689890672338608, policy loss: 11.966579817328766
Experience 25, Iter 13, disc loss: 0.0017026224437072669, policy loss: 11.530646117640718
Experience 25, Iter 14, disc loss: 0.0017112673044957884, policy loss: 11.39572436086365
Experience 25, Iter 15, disc loss: 0.0017191840224787286, policy loss: 11.353541145185151
Experience 25, Iter 16, disc loss: 0.0017250479920783673, policy loss: 11.32236950929694
Experience 25, Iter 17, disc loss: 0.001726748063005566, policy loss: 11.487644504292113
Experience 25, Iter 18, disc loss: 0.0017316555631141871, policy loss: 11.338483588582546
Experience 25, Iter 19, disc loss: 0.0017373913764043092, policy loss: 11.152124176566497
Experience 25, Iter 20, disc loss: 0.0017466902467957871, policy loss: 11.04212613005654
Experience 25, Iter 21, disc loss: 0.0017579446851794507, policy loss: 11.087209505746788
Experience 25, Iter 22, disc loss: 0.001735061756230979, policy loss: 11.194402786081401
Experience 25, Iter 23, disc loss: 0.0017303308299435112, policy loss: 11.566973828990315
Experience 25, Iter 24, disc loss: 0.0017262785331747105, policy loss: 11.857284214995762
Experience 25, Iter 25, disc loss: 0.0017207747562486943, policy loss: 11.476800978940224
Experience 25, Iter 26, disc loss: 0.001717464768477139, policy loss: 11.4413798575163
Experience 25, Iter 27, disc loss: 0.0017334098189394653, policy loss: 11.375183317840625
Experience 25, Iter 28, disc loss: 0.0017279710605118614, policy loss: 11.278391391527745
Experience 25, Iter 29, disc loss: 0.0018003919562704976, policy loss: 11.47903308975399
Experience 25, Iter 30, disc loss: 0.0017734233190399155, policy loss: 11.568865003085815
Experience 25, Iter 31, disc loss: 0.0017067006476891016, policy loss: 11.506068280863982
Experience 25, Iter 32, disc loss: 0.001686534792960865, policy loss: 11.724565293246357
Experience 25, Iter 33, disc loss: 0.0016795633759022046, policy loss: 11.659505657897776
Experience 25, Iter 34, disc loss: 0.0016728315615903927, policy loss: 11.766364776266638
Experience 25, Iter 35, disc loss: 0.001761286141941961, policy loss: 11.166712902957983
Experience 25, Iter 36, disc loss: 0.0019721998078543333, policy loss: 9.685480055324318
Experience 25, Iter 37, disc loss: 0.009637194726767058, policy loss: 7.32692483061836
Experience 25, Iter 38, disc loss: 0.11468133737796604, policy loss: 5.295549725249126
Experience 25, Iter 39, disc loss: 0.4159733000950504, policy loss: 4.4030759036714535
Experience 25, Iter 40, disc loss: 0.20157778658558093, policy loss: 6.747687742680537
Experience 25, Iter 41, disc loss: 0.002018676930189025, policy loss: 12.357326701464352
Experience 25, Iter 42, disc loss: 0.0022311756599587237, policy loss: 12.401077927372743
Experience 25, Iter 43, disc loss: 0.002457528714831619, policy loss: 11.97642285102957
Experience 25, Iter 44, disc loss: 0.0026834906085433626, policy loss: 12.135626388669284
Experience 25, Iter 45, disc loss: 0.0029105403386215727, policy loss: 12.142673613672397
Experience 25, Iter 46, disc loss: 0.0031400773163102573, policy loss: 11.718724209732342
Experience 25, Iter 47, disc loss: 0.003359901722069626, policy loss: 11.726312199039329
Experience 25, Iter 48, disc loss: 0.0035710043301257987, policy loss: 12.128457190117095
Experience 25, Iter 49, disc loss: 0.003772544525183068, policy loss: 12.43333756313887
Experience 25, Iter 50, disc loss: 0.00396428790627085, policy loss: 12.350603926889722
Experience 25, Iter 51, disc loss: 0.004139675892725957, policy loss: 12.587880856727601
Experience 25, Iter 52, disc loss: 0.0042990855135439845, policy loss: 12.365793054215542
Experience 25, Iter 53, disc loss: 0.004439735666560165, policy loss: 12.435615066470545
Experience 25, Iter 54, disc loss: 0.004563506059847977, policy loss: 12.368073051880279
Experience 25, Iter 55, disc loss: 0.0046694316765066145, policy loss: 12.266255587946878
Experience 25, Iter 56, disc loss: 0.004756981874028037, policy loss: 12.1494858732101
Experience 25, Iter 57, disc loss: 0.004828064960211056, policy loss: 12.08582051486801
Experience 25, Iter 58, disc loss: 0.004885277510094865, policy loss: 12.115621427764388
Experience 25, Iter 59, disc loss: 0.004921348244805393, policy loss: 12.225509189162974
Experience 25, Iter 60, disc loss: 0.004951875752939296, policy loss: 12.008697781338613
Experience 25, Iter 61, disc loss: 0.0049593304953907775, policy loss: 12.237303675924606
Experience 25, Iter 62, disc loss: 0.004953372729566672, policy loss: 12.231606277285664
Experience 25, Iter 63, disc loss: 0.004941249365313885, policy loss: 12.420069205674737
Experience 25, Iter 64, disc loss: 0.004921920390019951, policy loss: 12.117770612043353
Experience 25, Iter 65, disc loss: 0.004888010427512848, policy loss: 12.424280374390532
Experience 25, Iter 66, disc loss: 0.0048507592269346304, policy loss: 12.199506599008819
Experience 25, Iter 67, disc loss: 0.004805023777536043, policy loss: 12.320855466715917
Experience 25, Iter 68, disc loss: 0.004753405165657747, policy loss: 12.476127073899994
Experience 25, Iter 69, disc loss: 0.004696082695555308, policy loss: 12.79117943075975
Experience 25, Iter 70, disc loss: 0.004635409499241867, policy loss: 13.343871850867718
Experience 25, Iter 71, disc loss: 0.00457340264938491, policy loss: 13.091614965233255
Experience 25, Iter 72, disc loss: 0.0045090536799967575, policy loss: 13.076394569459653
Experience 25, Iter 73, disc loss: 0.004441266587603874, policy loss: 13.197370247920357
Experience 25, Iter 74, disc loss: 0.004372918331121238, policy loss: 13.307238930418187
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.1517],
        [1.5452],
        [0.0289]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0495, 0.2446, 1.3359, 0.0279, 0.0173, 4.3497]],

        [[0.0495, 0.2446, 1.3359, 0.0279, 0.0173, 4.3497]],

        [[0.0495, 0.2446, 1.3359, 0.0279, 0.0173, 4.3497]],

        [[0.0495, 0.2446, 1.3359, 0.0279, 0.0173, 4.3497]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0227, 0.6067, 6.1808, 0.1158], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0227, 0.6067, 6.1808, 0.1158])
N: 390
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1561.0000, 1561.0000, 1561.0000, 1561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.226
Iter 2/2000 - Loss: 3.301
Iter 3/2000 - Loss: 3.081
Iter 4/2000 - Loss: 3.026
Iter 5/2000 - Loss: 2.985
Iter 6/2000 - Loss: 2.851
Iter 7/2000 - Loss: 2.683
Iter 8/2000 - Loss: 2.531
Iter 9/2000 - Loss: 2.392
Iter 10/2000 - Loss: 2.237
Iter 11/2000 - Loss: 2.051
Iter 12/2000 - Loss: 1.839
Iter 13/2000 - Loss: 1.611
Iter 14/2000 - Loss: 1.376
Iter 15/2000 - Loss: 1.134
Iter 16/2000 - Loss: 0.881
Iter 17/2000 - Loss: 0.614
Iter 18/2000 - Loss: 0.332
Iter 19/2000 - Loss: 0.038
Iter 20/2000 - Loss: -0.265
Iter 1981/2000 - Loss: -8.224
Iter 1982/2000 - Loss: -8.224
Iter 1983/2000 - Loss: -8.224
Iter 1984/2000 - Loss: -8.224
Iter 1985/2000 - Loss: -8.224
Iter 1986/2000 - Loss: -8.224
Iter 1987/2000 - Loss: -8.224
Iter 1988/2000 - Loss: -8.224
Iter 1989/2000 - Loss: -8.224
Iter 1990/2000 - Loss: -8.224
Iter 1991/2000 - Loss: -8.224
Iter 1992/2000 - Loss: -8.224
Iter 1993/2000 - Loss: -8.224
Iter 1994/2000 - Loss: -8.224
Iter 1995/2000 - Loss: -8.224
Iter 1996/2000 - Loss: -8.224
Iter 1997/2000 - Loss: -8.224
Iter 1998/2000 - Loss: -8.225
Iter 1999/2000 - Loss: -8.225
Iter 2000/2000 - Loss: -8.225
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[14.7472, 13.0310, 30.3581,  9.1821,  5.3873, 54.2188]],

        [[22.1269, 31.6526,  8.5058,  1.3803,  1.9068, 23.1386]],

        [[22.5092, 34.6838,  8.8727,  1.1377,  1.0213, 20.0825]],

        [[20.4757, 30.2427, 14.4290,  1.7917,  2.0038, 39.7686]]])
Signal Variance: tensor([ 0.2291,  1.7257, 11.5936,  0.3940])
Estimated target variance: tensor([0.0227, 0.6067, 6.1808, 0.1158])
N: 390
Signal to noise ratio: tensor([26.8058, 71.4689, 77.3924, 38.0908])
Bound on condition number: tensor([ 280235.0529, 1992045.4233, 2335938.3370,  565856.6865])
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 0.004304142893412308, policy loss: 13.174394818043547
Experience 26, Iter 1, disc loss: 0.004235126664860356, policy loss: 12.884959075097148
Experience 26, Iter 2, disc loss: 0.0041660738169811105, policy loss: 12.738915363018862
Experience 26, Iter 3, disc loss: 0.004096097637720161, policy loss: 12.871872326454593
Experience 26, Iter 4, disc loss: 0.004026149898347897, policy loss: 12.908317524222905
Experience 26, Iter 5, disc loss: 0.0039578226055252, policy loss: 12.974989930476362
Experience 26, Iter 6, disc loss: 0.0038896230495095753, policy loss: 13.228632464084473
Experience 26, Iter 7, disc loss: 0.003822878675456024, policy loss: 13.412475834720803
Experience 26, Iter 8, disc loss: 0.0037580323945779087, policy loss: 13.330208327225531
Experience 26, Iter 9, disc loss: 0.0036947254549482534, policy loss: 13.154128026493904
Experience 26, Iter 10, disc loss: 0.0036322031140116322, policy loss: 13.265138028965369
Experience 26, Iter 11, disc loss: 0.003573518605818183, policy loss: 12.701849574223019
Experience 26, Iter 12, disc loss: 0.003514513129099202, policy loss: 12.547417394145624
Experience 26, Iter 13, disc loss: 0.0034579384924584553, policy loss: 12.39325530916525
Experience 26, Iter 14, disc loss: 0.0034015462181789602, policy loss: 12.21207848956156
Experience 26, Iter 15, disc loss: 0.0033465688741503167, policy loss: 12.111692648471674
Experience 26, Iter 16, disc loss: 0.0032918339271130184, policy loss: 12.498998170563043
Experience 26, Iter 17, disc loss: 0.0032408859209961676, policy loss: 12.258154923513484
Experience 26, Iter 18, disc loss: 0.0031907734820640237, policy loss: 12.026411674935563
Experience 26, Iter 19, disc loss: 0.0031425591685908987, policy loss: 11.917003185909806
Experience 26, Iter 20, disc loss: 0.003094496098018165, policy loss: 12.138519393282065
Experience 26, Iter 21, disc loss: 0.0030504748586911816, policy loss: 11.74082188311493
Experience 26, Iter 22, disc loss: 0.003004480509798035, policy loss: 11.941134921757792
Experience 26, Iter 23, disc loss: 0.0029611334256910773, policy loss: 11.982024475590633
Experience 26, Iter 24, disc loss: 0.0029181328366027505, policy loss: 12.022642726741855
Experience 26, Iter 25, disc loss: 0.0028760350948511663, policy loss: 12.213002648918717
Experience 26, Iter 26, disc loss: 0.002837655119858892, policy loss: 12.074675186143697
Experience 26, Iter 27, disc loss: 0.002798676703071334, policy loss: 12.14053830805877
Experience 26, Iter 28, disc loss: 0.0027613376515612104, policy loss: 12.188392035490489
Experience 26, Iter 29, disc loss: 0.002724649653532416, policy loss: 12.214367410044055
Experience 26, Iter 30, disc loss: 0.0026895208640993844, policy loss: 12.272499868977983
Experience 26, Iter 31, disc loss: 0.002656051559924808, policy loss: 12.279891303287858
Experience 26, Iter 32, disc loss: 0.0026238290449139053, policy loss: 12.063608144187945
Experience 26, Iter 33, disc loss: 0.0025925185620041488, policy loss: 11.992968587877796
Experience 26, Iter 34, disc loss: 0.0025605798301603527, policy loss: 12.179801866811287
Experience 26, Iter 35, disc loss: 0.0025316556078188034, policy loss: 11.972247246541407
Experience 26, Iter 36, disc loss: 0.002502192388297566, policy loss: 12.011507836290097
Experience 26, Iter 37, disc loss: 0.002473355181880513, policy loss: 12.195277486351134
Experience 26, Iter 38, disc loss: 0.002446740320950428, policy loss: 11.982788597771153
Experience 26, Iter 39, disc loss: 0.0024200147370025105, policy loss: 11.996411302529596
Experience 26, Iter 40, disc loss: 0.0023932267540604756, policy loss: 12.073393483456822
Experience 26, Iter 41, disc loss: 0.002368105042713039, policy loss: 12.117290771439649
Experience 26, Iter 42, disc loss: 0.0023434088038772967, policy loss: 12.047605533357668
Experience 26, Iter 43, disc loss: 0.0023187959894179623, policy loss: 11.997648144229151
Experience 26, Iter 44, disc loss: 0.0022965058852372354, policy loss: 11.925559302644388
Experience 26, Iter 45, disc loss: 0.0022737126944312154, policy loss: 11.832884487285728
Experience 26, Iter 46, disc loss: 0.002251785441450211, policy loss: 11.76900650552816
Experience 26, Iter 47, disc loss: 0.002229388756429546, policy loss: 11.882716503108563
Experience 26, Iter 48, disc loss: 0.0022078629495735514, policy loss: 11.85836040543773
Experience 26, Iter 49, disc loss: 0.0021866319732275277, policy loss: 11.952009540119668
Experience 26, Iter 50, disc loss: 0.0021657689953341955, policy loss: 12.044730864716293
Experience 26, Iter 51, disc loss: 0.002147084393879729, policy loss: 11.812541281893076
Experience 26, Iter 52, disc loss: 0.002128796180614205, policy loss: 11.60527003858866
Experience 26, Iter 53, disc loss: 0.002109606007141243, policy loss: 11.669218737245911
Experience 26, Iter 54, disc loss: 0.0020910433173972607, policy loss: 11.712721147281238
Experience 26, Iter 55, disc loss: 0.00207078771685237, policy loss: 12.086811670966807
Experience 26, Iter 56, disc loss: 0.002052937751724709, policy loss: 12.010205994900073
Experience 26, Iter 57, disc loss: 0.002037452841475729, policy loss: 11.702178408370493
Experience 26, Iter 58, disc loss: 0.002019972245083188, policy loss: 11.763418501392994
Experience 26, Iter 59, disc loss: 0.0020038884373303405, policy loss: 11.905072611529738
Experience 26, Iter 60, disc loss: 0.0019873336706798385, policy loss: 11.680973167619394
Experience 26, Iter 61, disc loss: 0.0019676063332180038, policy loss: 12.518439748338585
Experience 26, Iter 62, disc loss: 0.0019517970368943531, policy loss: 12.550147258636583
Experience 26, Iter 63, disc loss: 0.0019354884484545274, policy loss: 12.633984959126582
Experience 26, Iter 64, disc loss: 0.0019214607774745709, policy loss: 12.405586082121186
Experience 26, Iter 65, disc loss: 0.001903989707856932, policy loss: 12.995205885521127
Experience 26, Iter 66, disc loss: 0.0018951937380923618, policy loss: 12.762693923261017
Experience 26, Iter 67, disc loss: 0.0018763584019801964, policy loss: 12.71896472582366
Experience 26, Iter 68, disc loss: 0.0018618332941911875, policy loss: 13.085640224484493
Experience 26, Iter 69, disc loss: 0.0018505363456837723, policy loss: 12.56814799831469
Experience 26, Iter 70, disc loss: 0.0018366055892478992, policy loss: 12.392721657970998
Experience 26, Iter 71, disc loss: 0.0018248326162369083, policy loss: 11.892774467699212
Experience 26, Iter 72, disc loss: 0.0018123232966947677, policy loss: 11.906192485603368
Experience 26, Iter 73, disc loss: 0.001800056969779338, policy loss: 11.708198351213557
Experience 26, Iter 74, disc loss: 0.0017893061379033902, policy loss: 11.583061515615071
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.1530],
        [1.5636],
        [0.0289]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0477, 0.2395, 1.3364, 0.0277, 0.0168, 4.3398]],

        [[0.0477, 0.2395, 1.3364, 0.0277, 0.0168, 4.3398]],

        [[0.0477, 0.2395, 1.3364, 0.0277, 0.0168, 4.3398]],

        [[0.0477, 0.2395, 1.3364, 0.0277, 0.0168, 4.3398]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0222, 0.6118, 6.2544, 0.1156], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0222, 0.6118, 6.2544, 0.1156])
N: 405
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1621.0000, 1621.0000, 1621.0000, 1621.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.226
Iter 2/2000 - Loss: 3.304
Iter 3/2000 - Loss: 3.080
Iter 4/2000 - Loss: 3.024
Iter 5/2000 - Loss: 2.983
Iter 6/2000 - Loss: 2.847
Iter 7/2000 - Loss: 2.677
Iter 8/2000 - Loss: 2.523
Iter 9/2000 - Loss: 2.382
Iter 10/2000 - Loss: 2.226
Iter 11/2000 - Loss: 2.038
Iter 12/2000 - Loss: 1.823
Iter 13/2000 - Loss: 1.594
Iter 14/2000 - Loss: 1.356
Iter 15/2000 - Loss: 1.111
Iter 16/2000 - Loss: 0.855
Iter 17/2000 - Loss: 0.586
Iter 18/2000 - Loss: 0.302
Iter 19/2000 - Loss: 0.006
Iter 20/2000 - Loss: -0.300
Iter 1981/2000 - Loss: -8.273
Iter 1982/2000 - Loss: -8.273
Iter 1983/2000 - Loss: -8.273
Iter 1984/2000 - Loss: -8.273
Iter 1985/2000 - Loss: -8.273
Iter 1986/2000 - Loss: -8.273
Iter 1987/2000 - Loss: -8.273
Iter 1988/2000 - Loss: -8.273
Iter 1989/2000 - Loss: -8.273
Iter 1990/2000 - Loss: -8.274
Iter 1991/2000 - Loss: -8.274
Iter 1992/2000 - Loss: -8.274
Iter 1993/2000 - Loss: -8.274
Iter 1994/2000 - Loss: -8.274
Iter 1995/2000 - Loss: -8.274
Iter 1996/2000 - Loss: -8.274
Iter 1997/2000 - Loss: -8.274
Iter 1998/2000 - Loss: -8.274
Iter 1999/2000 - Loss: -8.274
Iter 2000/2000 - Loss: -8.274
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[14.8527, 13.1282, 30.3442,  8.5326,  5.0702, 52.9822]],

        [[21.9188, 30.6115,  8.5702,  1.3893,  1.8875, 23.1142]],

        [[21.7322, 33.5956,  8.9793,  1.1256,  1.0325, 20.1485]],

        [[20.1335, 29.3451, 14.4555,  1.7739,  2.0248, 39.3069]]])
Signal Variance: tensor([ 0.2277,  1.7368, 11.6814,  0.3895])
Estimated target variance: tensor([0.0222, 0.6118, 6.2544, 0.1156])
N: 405
Signal to noise ratio: tensor([26.7406, 72.2501, 77.5961, 38.0897])
Bound on condition number: tensor([ 289601.0705, 2114132.7367, 2438567.0173,  587584.0735])
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 0.0017759244707260258, policy loss: 11.652120616297234
Experience 27, Iter 1, disc loss: 0.0017632655603406985, policy loss: 11.730739413107559
Experience 27, Iter 2, disc loss: 0.0017517333333410686, policy loss: 11.776835167763963
Experience 27, Iter 3, disc loss: 0.0017426689841519986, policy loss: 11.618089798926714
Experience 27, Iter 4, disc loss: 0.0017286480996978784, policy loss: 11.617805690204385
Experience 27, Iter 5, disc loss: 0.0017174280920197664, policy loss: 11.612561787598851
Experience 27, Iter 6, disc loss: 0.0017068249047408255, policy loss: 11.573917222529367
Experience 27, Iter 7, disc loss: 0.0016954080729920554, policy loss: 11.689121655988501
Experience 27, Iter 8, disc loss: 0.0016848366803558202, policy loss: 11.608695587538374
Experience 27, Iter 9, disc loss: 0.001675391478294643, policy loss: 11.552873147054104
Experience 27, Iter 10, disc loss: 0.0016651880352752278, policy loss: 11.409118766211662
Experience 27, Iter 11, disc loss: 0.0016524757984343328, policy loss: 11.83183944554924
Experience 27, Iter 12, disc loss: 0.0016460147390197919, policy loss: 11.371673398096995
Experience 27, Iter 13, disc loss: 0.0016320070429309225, policy loss: 11.911544793941431
Experience 27, Iter 14, disc loss: 0.0016226271237986044, policy loss: 11.803339256571764
Experience 27, Iter 15, disc loss: 0.0016135293183286137, policy loss: 11.744640397972624
Experience 27, Iter 16, disc loss: 0.0016034654169392545, policy loss: 11.71901705783378
Experience 27, Iter 17, disc loss: 0.0015920992395599678, policy loss: 11.97172178471157
Experience 27, Iter 18, disc loss: 0.0015822985427525133, policy loss: 11.953080465487766
Experience 27, Iter 19, disc loss: 0.0015734909067736667, policy loss: 11.920107410749452
Experience 27, Iter 20, disc loss: 0.0015676259346310097, policy loss: 11.609184908887201
Experience 27, Iter 21, disc loss: 0.001558960840422214, policy loss: 11.555940500638208
Experience 27, Iter 22, disc loss: 0.0015472575495881033, policy loss: 11.980003433830607
Experience 27, Iter 23, disc loss: 0.0015377350269820342, policy loss: 12.212649839023584
Experience 27, Iter 24, disc loss: 0.0015313570450522208, policy loss: 11.74656317364926
Experience 27, Iter 25, disc loss: 0.0015208497488230874, policy loss: 12.051814764115916
Experience 27, Iter 26, disc loss: 0.001511633703401213, policy loss: 12.223762539890405
Experience 27, Iter 27, disc loss: 0.001502114336946799, policy loss: 12.320280901961365
Experience 27, Iter 28, disc loss: 0.001495395288485688, policy loss: 12.34580783595051
Experience 27, Iter 29, disc loss: 0.0014863609716969965, policy loss: 12.32782370508648
Experience 27, Iter 30, disc loss: 0.0014789210774096677, policy loss: 12.31342521375548
Experience 27, Iter 31, disc loss: 0.001471298953036728, policy loss: 12.172901916501218
Experience 27, Iter 32, disc loss: 0.0014642433068648032, policy loss: 12.249271479085053
Experience 27, Iter 33, disc loss: 0.0014577671285891359, policy loss: 11.919529602696235
Experience 27, Iter 34, disc loss: 0.0014504106681272493, policy loss: 11.708321814523032
Experience 27, Iter 35, disc loss: 0.0014453173910258861, policy loss: 11.579888759029506
Experience 27, Iter 36, disc loss: 0.0014397982443571347, policy loss: 11.440131292018215
Experience 27, Iter 37, disc loss: 0.0014330147686479565, policy loss: 11.311044998258607
Experience 27, Iter 38, disc loss: 0.0014221550512555675, policy loss: 11.45773336322252
Experience 27, Iter 39, disc loss: 0.0014149795774978254, policy loss: 11.538483409849393
Experience 27, Iter 40, disc loss: 0.001411532465939444, policy loss: 11.248184917910528
Experience 27, Iter 41, disc loss: 0.0014040012195124917, policy loss: 11.239822811966711
Experience 27, Iter 42, disc loss: 0.0013962942557406994, policy loss: 11.356526078573783
Experience 27, Iter 43, disc loss: 0.0013885052986691378, policy loss: 11.429509723344514
Experience 27, Iter 44, disc loss: 0.001381844067473083, policy loss: 11.364376817657863
Experience 27, Iter 45, disc loss: 0.001376872458661775, policy loss: 11.311484713804564
Experience 27, Iter 46, disc loss: 0.0013673623818935042, policy loss: 11.507805898220557
Experience 27, Iter 47, disc loss: 0.0013622890284895066, policy loss: 11.3507372665071
Experience 27, Iter 48, disc loss: 0.0013537477402533566, policy loss: 11.45970326962175
Experience 27, Iter 49, disc loss: 0.0013497529051424513, policy loss: 11.314191827434687
Experience 27, Iter 50, disc loss: 0.001342148364262017, policy loss: 11.37267134932058
Experience 27, Iter 51, disc loss: 0.0013361751739446336, policy loss: 11.31547533907728
Experience 27, Iter 52, disc loss: 0.0013282124769449493, policy loss: 11.484747758912377
Experience 27, Iter 53, disc loss: 0.0013215725908682211, policy loss: 11.624822952052748
Experience 27, Iter 54, disc loss: 0.0013155823611634809, policy loss: 11.492682555993095
Experience 27, Iter 55, disc loss: 0.0013143484097092647, policy loss: 11.32624752345473
Experience 27, Iter 56, disc loss: 0.0013045047020060678, policy loss: 11.462581949133812
Experience 27, Iter 57, disc loss: 0.001299417392239565, policy loss: 11.355478966414886
Experience 27, Iter 58, disc loss: 0.001296515313395794, policy loss: 11.149195718764485
Experience 27, Iter 59, disc loss: 0.0012889505099989084, policy loss: 11.326002193849519
Experience 27, Iter 60, disc loss: 0.0012835725827623104, policy loss: 11.248768214348537
Experience 27, Iter 61, disc loss: 0.0012782966861565738, policy loss: 11.156574455457378
Experience 27, Iter 62, disc loss: 0.0012732723301524718, policy loss: 11.169136537947677
Experience 27, Iter 63, disc loss: 0.001266563861815514, policy loss: 11.171959857243465
Experience 27, Iter 64, disc loss: 0.001260407090229003, policy loss: 11.287523396114448
Experience 27, Iter 65, disc loss: 0.0012545795640256116, policy loss: 11.337857808470702
Experience 27, Iter 66, disc loss: 0.0012503925298814273, policy loss: 11.163166086058173
Experience 27, Iter 67, disc loss: 0.001244268450600971, policy loss: 11.261037060542847
Experience 27, Iter 68, disc loss: 0.0012390872610830582, policy loss: 11.212920214283017
Experience 27, Iter 69, disc loss: 0.0012349043856138447, policy loss: 11.17672946776459
Experience 27, Iter 70, disc loss: 0.0012309106521354886, policy loss: 11.133188724751514
Experience 27, Iter 71, disc loss: 0.0012247176208486036, policy loss: 11.16148200857224
Experience 27, Iter 72, disc loss: 0.0012182312836520601, policy loss: 11.30009632047333
Experience 27, Iter 73, disc loss: 0.0012136000747616279, policy loss: 11.176280437066405
Experience 27, Iter 74, disc loss: 0.0012082945441018666, policy loss: 11.238756110496977
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.1547],
        [1.5898],
        [0.0288]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0462, 0.2356, 1.3356, 0.0273, 0.0164, 4.3327]],

        [[0.0462, 0.2356, 1.3356, 0.0273, 0.0164, 4.3327]],

        [[0.0462, 0.2356, 1.3356, 0.0273, 0.0164, 4.3327]],

        [[0.0462, 0.2356, 1.3356, 0.0273, 0.0164, 4.3327]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0218, 0.6186, 6.3592, 0.1152], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0218, 0.6186, 6.3592, 0.1152])
N: 420
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1681.0000, 1681.0000, 1681.0000, 1681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.232
Iter 2/2000 - Loss: 3.310
Iter 3/2000 - Loss: 3.087
Iter 4/2000 - Loss: 3.032
Iter 5/2000 - Loss: 2.992
Iter 6/2000 - Loss: 2.856
Iter 7/2000 - Loss: 2.688
Iter 8/2000 - Loss: 2.536
Iter 9/2000 - Loss: 2.398
Iter 10/2000 - Loss: 2.243
Iter 11/2000 - Loss: 2.055
Iter 12/2000 - Loss: 1.841
Iter 13/2000 - Loss: 1.611
Iter 14/2000 - Loss: 1.374
Iter 15/2000 - Loss: 1.130
Iter 16/2000 - Loss: 0.874
Iter 17/2000 - Loss: 0.602
Iter 18/2000 - Loss: 0.316
Iter 19/2000 - Loss: 0.017
Iter 20/2000 - Loss: -0.291
Iter 1981/2000 - Loss: -8.320
Iter 1982/2000 - Loss: -8.320
Iter 1983/2000 - Loss: -8.320
Iter 1984/2000 - Loss: -8.320
Iter 1985/2000 - Loss: -8.320
Iter 1986/2000 - Loss: -8.321
Iter 1987/2000 - Loss: -8.321
Iter 1988/2000 - Loss: -8.321
Iter 1989/2000 - Loss: -8.321
Iter 1990/2000 - Loss: -8.321
Iter 1991/2000 - Loss: -8.321
Iter 1992/2000 - Loss: -8.321
Iter 1993/2000 - Loss: -8.321
Iter 1994/2000 - Loss: -8.321
Iter 1995/2000 - Loss: -8.321
Iter 1996/2000 - Loss: -8.321
Iter 1997/2000 - Loss: -8.321
Iter 1998/2000 - Loss: -8.321
Iter 1999/2000 - Loss: -8.321
Iter 2000/2000 - Loss: -8.321
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.7491, 12.9261, 29.9860,  8.8986,  5.1814, 52.6986]],

        [[21.8049, 30.2912,  8.7766,  1.4110,  1.8742, 24.0310]],

        [[21.9654, 33.2266,  9.0536,  1.1130,  1.0488, 20.2517]],

        [[19.7806, 29.1214, 14.1712,  1.7888,  1.9883, 39.3422]]])
Signal Variance: tensor([ 0.2188,  1.8708, 11.8698,  0.3856])
Estimated target variance: tensor([0.0218, 0.6186, 6.3592, 0.1152])
N: 420
Signal to noise ratio: tensor([26.2733, 75.5214, 77.6110, 38.1110])
Bound on condition number: tensor([ 289921.2390, 2395462.9427, 2529854.0353,  610030.4190])
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 0.0012039198079785043, policy loss: 11.22948771035757
Experience 28, Iter 1, disc loss: 0.0011986122376628013, policy loss: 11.294826150125903
Experience 28, Iter 2, disc loss: 0.00119515278190106, policy loss: 11.155146640330274
Experience 28, Iter 3, disc loss: 0.0011902730578915734, policy loss: 11.10995458254797
Experience 28, Iter 4, disc loss: 0.001185036300020541, policy loss: 11.200229192560768
Experience 28, Iter 5, disc loss: 0.0011807368876674566, policy loss: 11.086390625880952
Experience 28, Iter 6, disc loss: 0.001174293412945385, policy loss: 11.36506438810045
Experience 28, Iter 7, disc loss: 0.0011723514514097538, policy loss: 11.098818037966405
Experience 28, Iter 8, disc loss: 0.0011662540598845366, policy loss: 11.137812385854659
Experience 28, Iter 9, disc loss: 0.0011621520906265614, policy loss: 11.112649319738898
Experience 28, Iter 10, disc loss: 0.0011588223974752531, policy loss: 11.036214770340191
Experience 28, Iter 11, disc loss: 0.001152543608810973, policy loss: 11.19272752110291
Experience 28, Iter 12, disc loss: 0.001149806188447244, policy loss: 11.044627688902558
Experience 28, Iter 13, disc loss: 0.0011461580255202736, policy loss: 10.976883447280326
Experience 28, Iter 14, disc loss: 0.0011411314353184632, policy loss: 11.087094064366667
Experience 28, Iter 15, disc loss: 0.001136775005205364, policy loss: 11.093178781657766
Experience 28, Iter 16, disc loss: 0.0011336569354566787, policy loss: 10.899407797667152
Experience 28, Iter 17, disc loss: 0.0011288623513247091, policy loss: 10.937193191833778
Experience 28, Iter 18, disc loss: 0.0011254001548859742, policy loss: 10.890852819923865
Experience 28, Iter 19, disc loss: 0.0011192776134996564, policy loss: 10.983277356787422
Experience 28, Iter 20, disc loss: 0.0011160930078220152, policy loss: 11.02388442826135
Experience 28, Iter 21, disc loss: 0.001111789920113005, policy loss: 11.005625924058513
Experience 28, Iter 22, disc loss: 0.0011068094587511092, policy loss: 11.061148084860765
Experience 28, Iter 23, disc loss: 0.0011035433702508376, policy loss: 10.987447979993151
Experience 28, Iter 24, disc loss: 0.0010996259274750262, policy loss: 10.979468848107501
Experience 28, Iter 25, disc loss: 0.001094678737217156, policy loss: 10.984177142548535
Experience 28, Iter 26, disc loss: 0.0010894275576933945, policy loss: 11.103298839987186
Experience 28, Iter 27, disc loss: 0.001089238106243341, policy loss: 10.867387434067968
Experience 28, Iter 28, disc loss: 0.0010848084891916785, policy loss: 10.879268192108839
Experience 28, Iter 29, disc loss: 0.001079917843984121, policy loss: 11.014605620885412
Experience 28, Iter 30, disc loss: 0.001078068940863752, policy loss: 10.84961437754067
Experience 28, Iter 31, disc loss: 0.001073989379333409, policy loss: 10.865127579565193
Experience 28, Iter 32, disc loss: 0.0010675541658687307, policy loss: 10.96048941747112
Experience 28, Iter 33, disc loss: 0.00106103296702779, policy loss: 11.128810701431092
Experience 28, Iter 34, disc loss: 0.0010587190926663408, policy loss: 11.129946913691928
Experience 28, Iter 35, disc loss: 0.0010545703821090882, policy loss: 11.109652847066087
Experience 28, Iter 36, disc loss: 0.0010528748858677718, policy loss: 10.906239573107525
Experience 28, Iter 37, disc loss: 0.0010494743605644508, policy loss: 10.958006842107393
Experience 28, Iter 38, disc loss: 0.0010457028557481227, policy loss: 10.975923202617745
Experience 28, Iter 39, disc loss: 0.0010412239305050985, policy loss: 11.040989327310374
Experience 28, Iter 40, disc loss: 0.0010379591416192976, policy loss: 10.968741949768301
Experience 28, Iter 41, disc loss: 0.0010353582711286162, policy loss: 10.902809818301924
Experience 28, Iter 42, disc loss: 0.0010318181381874828, policy loss: 10.899936403729766
Experience 28, Iter 43, disc loss: 0.0010283250205972226, policy loss: 10.970169468421304
Experience 28, Iter 44, disc loss: 0.0010248135750235653, policy loss: 10.958014093968986
Experience 28, Iter 45, disc loss: 0.0010182966834592975, policy loss: 11.134632852223856
Experience 28, Iter 46, disc loss: 0.0010144832792515635, policy loss: 11.115165906485213
Experience 28, Iter 47, disc loss: 0.0010115994206199572, policy loss: 11.096034055687646
Experience 28, Iter 48, disc loss: 0.0010072577000045106, policy loss: 11.118721505813594
Experience 28, Iter 49, disc loss: 0.0010066134098353162, policy loss: 10.968216779734457
Experience 28, Iter 50, disc loss: 0.001003761992057914, policy loss: 10.944274174945374
Experience 28, Iter 51, disc loss: 0.0010007239663667841, policy loss: 10.866703584128004
Experience 28, Iter 52, disc loss: 0.0009980853220177238, policy loss: 10.851577975481987
Experience 28, Iter 53, disc loss: 0.0009935666270849475, policy loss: 10.93763912755272
Experience 28, Iter 54, disc loss: 0.0009923435445459406, policy loss: 10.878172932928589
Experience 28, Iter 55, disc loss: 0.0009887486796402332, policy loss: 10.842110770160643
Experience 28, Iter 56, disc loss: 0.00098432848522825, policy loss: 10.875700112574535
Experience 28, Iter 57, disc loss: 0.0009807017365882915, policy loss: 10.900133646969973
Experience 28, Iter 58, disc loss: 0.0009764286823833036, policy loss: 11.076749659717139
Experience 28, Iter 59, disc loss: 0.000973923614142707, policy loss: 10.95769010944113
Experience 28, Iter 60, disc loss: 0.000972670547621451, policy loss: 10.881596683640069
Experience 28, Iter 61, disc loss: 0.0009699066988732218, policy loss: 10.870943623244212
Experience 28, Iter 62, disc loss: 0.0009681227717786358, policy loss: 10.86236251835102
Experience 28, Iter 63, disc loss: 0.00096528154562276, policy loss: 10.792528572646935
Experience 28, Iter 64, disc loss: 0.0009590383775867983, policy loss: 10.907689928288882
Experience 28, Iter 65, disc loss: 0.0009549716835223112, policy loss: 11.004544825109846
Experience 28, Iter 66, disc loss: 0.0009502807192026751, policy loss: 11.089598190388738
Experience 28, Iter 67, disc loss: 0.0009502633950235179, policy loss: 10.861328670684113
Experience 28, Iter 68, disc loss: 0.0009479781578659871, policy loss: 10.861092413703666
Experience 28, Iter 69, disc loss: 0.0009437277254291943, policy loss: 10.98519426714475
Experience 28, Iter 70, disc loss: 0.0009408865694384176, policy loss: 10.914397648097196
Experience 28, Iter 71, disc loss: 0.0009365426290722849, policy loss: 11.011472891852623
Experience 28, Iter 72, disc loss: 0.0009314409367915951, policy loss: 11.144057403046672
Experience 28, Iter 73, disc loss: 0.0009297630126987418, policy loss: 10.99747805611451
Experience 28, Iter 74, disc loss: 0.0009281042433052322, policy loss: 10.951474086836283
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.1570],
        [1.6237],
        [0.0288]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0449, 0.2320, 1.3426, 0.0271, 0.0161, 4.3310]],

        [[0.0449, 0.2320, 1.3426, 0.0271, 0.0161, 4.3310]],

        [[0.0449, 0.2320, 1.3426, 0.0271, 0.0161, 4.3310]],

        [[0.0449, 0.2320, 1.3426, 0.0271, 0.0161, 4.3310]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0214, 0.6280, 6.4950, 0.1154], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0214, 0.6280, 6.4950, 0.1154])
N: 435
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1741.0000, 1741.0000, 1741.0000, 1741.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.249
Iter 2/2000 - Loss: 3.327
Iter 3/2000 - Loss: 3.105
Iter 4/2000 - Loss: 3.050
Iter 5/2000 - Loss: 3.010
Iter 6/2000 - Loss: 2.873
Iter 7/2000 - Loss: 2.705
Iter 8/2000 - Loss: 2.553
Iter 9/2000 - Loss: 2.414
Iter 10/2000 - Loss: 2.258
Iter 11/2000 - Loss: 2.068
Iter 12/2000 - Loss: 1.851
Iter 13/2000 - Loss: 1.620
Iter 14/2000 - Loss: 1.382
Iter 15/2000 - Loss: 1.136
Iter 16/2000 - Loss: 0.878
Iter 17/2000 - Loss: 0.605
Iter 18/2000 - Loss: 0.316
Iter 19/2000 - Loss: 0.014
Iter 20/2000 - Loss: -0.296
Iter 1981/2000 - Loss: -8.365
Iter 1982/2000 - Loss: -8.365
Iter 1983/2000 - Loss: -8.365
Iter 1984/2000 - Loss: -8.366
Iter 1985/2000 - Loss: -8.366
Iter 1986/2000 - Loss: -8.366
Iter 1987/2000 - Loss: -8.366
Iter 1988/2000 - Loss: -8.366
Iter 1989/2000 - Loss: -8.366
Iter 1990/2000 - Loss: -8.366
Iter 1991/2000 - Loss: -8.366
Iter 1992/2000 - Loss: -8.366
Iter 1993/2000 - Loss: -8.366
Iter 1994/2000 - Loss: -8.366
Iter 1995/2000 - Loss: -8.366
Iter 1996/2000 - Loss: -8.366
Iter 1997/2000 - Loss: -8.366
Iter 1998/2000 - Loss: -8.366
Iter 1999/2000 - Loss: -8.366
Iter 2000/2000 - Loss: -8.366
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.4201, 12.8354, 29.3071,  8.8724,  5.3738, 53.0300]],

        [[21.6797, 29.9564,  8.7377,  1.4076,  1.8594, 24.2706]],

        [[21.8418, 32.7139,  9.1475,  1.1007,  1.0576, 20.5758]],

        [[19.3183, 28.6638, 14.2439,  1.7688,  1.9997, 39.5152]]])
Signal Variance: tensor([ 0.2171,  1.8803, 12.0145,  0.3902])
Estimated target variance: tensor([0.0214, 0.6280, 6.4950, 0.1154])
N: 435
Signal to noise ratio: tensor([26.2896, 76.2128, 77.8137, 38.4893])
Bound on condition number: tensor([ 300649.2802, 2526651.8923, 2633911.1035,  644420.5386])
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.0009269892081517589, policy loss: 10.884865643655258
Experience 29, Iter 1, disc loss: 0.0009234398635921242, policy loss: 10.930435763795607
Experience 29, Iter 2, disc loss: 0.0009210041183791496, policy loss: 10.908730562819127
Experience 29, Iter 3, disc loss: 0.0009188634845058534, policy loss: 10.83155110944199
Experience 29, Iter 4, disc loss: 0.0009158641827943645, policy loss: 10.888417898672953
Experience 29, Iter 5, disc loss: 0.0009120503366103571, policy loss: 10.926501592495985
Experience 29, Iter 6, disc loss: 0.0009104315748065088, policy loss: 10.905549826039637
Experience 29, Iter 7, disc loss: 0.0009098085479179324, policy loss: 10.743282905135446
Experience 29, Iter 8, disc loss: 0.0009068123955036757, policy loss: 10.77765720068688
Experience 29, Iter 9, disc loss: 0.0009045419494182831, policy loss: 10.72663955466849
Experience 29, Iter 10, disc loss: 0.0008983610051994645, policy loss: 10.996010128379758
Experience 29, Iter 11, disc loss: 0.000893117570855093, policy loss: 11.134099630604686
Experience 29, Iter 12, disc loss: 0.0008913655816005054, policy loss: 11.05232299907147
Experience 29, Iter 13, disc loss: 0.0008913108519217151, policy loss: 10.880293745623467
Experience 29, Iter 14, disc loss: 0.0008890974384634542, policy loss: 10.90029819260755
Experience 29, Iter 15, disc loss: 0.0008887279217955648, policy loss: 10.767405126554653
Experience 29, Iter 16, disc loss: 0.0008853081183453453, policy loss: 10.821720930533765
Experience 29, Iter 17, disc loss: 0.0008813683734415347, policy loss: 10.909570084891216
Experience 29, Iter 18, disc loss: 0.000878709985934539, policy loss: 10.848961842590104
Experience 29, Iter 19, disc loss: 0.0008735206127389383, policy loss: 10.985089106730484
Experience 29, Iter 20, disc loss: 0.0008697399344494345, policy loss: 11.142998326177413
Experience 29, Iter 21, disc loss: 0.0008694861727278476, policy loss: 10.920764871306986
Experience 29, Iter 22, disc loss: 0.0008683262017129466, policy loss: 10.882353313104364
Experience 29, Iter 23, disc loss: 0.0008657489217739659, policy loss: 10.901765189275523
Experience 29, Iter 24, disc loss: 0.0008618609408549086, policy loss: 10.902055321760875
Experience 29, Iter 25, disc loss: 0.000859019258085336, policy loss: 11.024671395644638
Experience 29, Iter 26, disc loss: 0.0008551497365870188, policy loss: 11.11216044501214
Experience 29, Iter 27, disc loss: 0.0008522366900835372, policy loss: 11.097327156531838
Experience 29, Iter 28, disc loss: 0.0008518739167807371, policy loss: 11.00420205861667
Experience 29, Iter 29, disc loss: 0.0008531583200953268, policy loss: 10.73096556682873
Experience 29, Iter 30, disc loss: 0.0008494472210868595, policy loss: 10.817434410823552
Experience 29, Iter 31, disc loss: 0.000846960356737591, policy loss: 10.85848854244491
Experience 29, Iter 32, disc loss: 0.0008433486723570652, policy loss: 10.940607490058843
Experience 29, Iter 33, disc loss: 0.0008404863429834622, policy loss: 10.960960401207924
Experience 29, Iter 34, disc loss: 0.0008373396842384281, policy loss: 10.93885091967783
Experience 29, Iter 35, disc loss: 0.0008364374892707976, policy loss: 10.88120934709242
Experience 29, Iter 36, disc loss: 0.0008373179043867124, policy loss: 10.697175888716002
Experience 29, Iter 37, disc loss: 0.0008369005162451148, policy loss: 10.614739634294441
Experience 29, Iter 38, disc loss: 0.0008324421166419863, policy loss: 10.767890962400031
Experience 29, Iter 39, disc loss: 0.0008282266667687505, policy loss: 10.842619284068075
Experience 29, Iter 40, disc loss: 0.0008235671075060588, policy loss: 10.924086922398471
Experience 29, Iter 41, disc loss: 0.0008236604283925768, policy loss: 10.811730788432268
Experience 29, Iter 42, disc loss: 0.0008203160952384604, policy loss: 10.919236172003483
Experience 29, Iter 43, disc loss: 0.00082011021171297, policy loss: 10.770436407958577
Experience 29, Iter 44, disc loss: 0.0008218272067183964, policy loss: 10.589649568555407
Experience 29, Iter 45, disc loss: 0.0008185369275859606, policy loss: 10.68402980996392
Experience 29, Iter 46, disc loss: 0.0008170334776938693, policy loss: 10.693782261028922
Experience 29, Iter 47, disc loss: 0.0008100934452121, policy loss: 10.86264888872958
Experience 29, Iter 48, disc loss: 0.0008075863663059552, policy loss: 10.924306608210589
Experience 29, Iter 49, disc loss: 0.0008053838480788828, policy loss: 10.858457271829554
Experience 29, Iter 50, disc loss: 0.0008042082820420188, policy loss: 10.849603031062998
Experience 29, Iter 51, disc loss: 0.0008053035873709518, policy loss: 10.71842629930463
Experience 29, Iter 52, disc loss: 0.0008013034863610239, policy loss: 10.756376753910605
Experience 29, Iter 53, disc loss: 0.0008034384405749108, policy loss: 10.666637835626183
Experience 29, Iter 54, disc loss: 0.0007974652125847658, policy loss: 10.778269613582655
Experience 29, Iter 55, disc loss: 0.0007970697428867316, policy loss: 10.666421438801114
Experience 29, Iter 56, disc loss: 0.0007928802635586922, policy loss: 10.747447411058271
Experience 29, Iter 57, disc loss: 0.0007911183000680889, policy loss: 10.751491641092885
Experience 29, Iter 58, disc loss: 0.0007859605980036189, policy loss: 10.843464362598315
Experience 29, Iter 59, disc loss: 0.0007844642801554373, policy loss: 10.954127715874831
Experience 29, Iter 60, disc loss: 0.0007845351856663032, policy loss: 10.73800355140622
Experience 29, Iter 61, disc loss: 0.0007794196459670085, policy loss: 10.941495745690656
Experience 29, Iter 62, disc loss: 0.0007786098880942993, policy loss: 10.880244673380297
Experience 29, Iter 63, disc loss: 0.0007761069689487582, policy loss: 10.908090967341813
Experience 29, Iter 64, disc loss: 0.0007786064351125311, policy loss: 10.656766673017305
Experience 29, Iter 65, disc loss: 0.0007785589774201872, policy loss: 10.593646703673748
Experience 29, Iter 66, disc loss: 0.0007753968608082581, policy loss: 10.628608458031007
Experience 29, Iter 67, disc loss: 0.0007698032712790304, policy loss: 10.822291755477137
Experience 29, Iter 68, disc loss: 0.0007685344278338303, policy loss: 10.780316238268188
Experience 29, Iter 69, disc loss: 0.0007705430029490198, policy loss: 10.680167788407795
Experience 29, Iter 70, disc loss: 0.0007690871385170003, policy loss: 10.616893336972396
Experience 29, Iter 71, disc loss: 0.0007625299115747216, policy loss: 10.795475911548015
Experience 29, Iter 72, disc loss: 0.0007613046049632088, policy loss: 10.790121202186604
Experience 29, Iter 73, disc loss: 0.0007573637767251638, policy loss: 10.854460472157463
Experience 29, Iter 74, disc loss: 0.0007575096974400071, policy loss: 10.774033634541276
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.1595],
        [1.6523],
        [0.0289]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0435, 0.2281, 1.3485, 0.0270, 0.0157, 4.3479]],

        [[0.0435, 0.2281, 1.3485, 0.0270, 0.0157, 4.3479]],

        [[0.0435, 0.2281, 1.3485, 0.0270, 0.0157, 4.3479]],

        [[0.0435, 0.2281, 1.3485, 0.0270, 0.0157, 4.3479]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0210, 0.6380, 6.6090, 0.1155], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0210, 0.6380, 6.6090, 0.1155])
N: 450
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1801.0000, 1801.0000, 1801.0000, 1801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.246
Iter 2/2000 - Loss: 3.326
Iter 3/2000 - Loss: 3.100
Iter 4/2000 - Loss: 3.044
Iter 5/2000 - Loss: 3.003
Iter 6/2000 - Loss: 2.865
Iter 7/2000 - Loss: 2.694
Iter 8/2000 - Loss: 2.541
Iter 9/2000 - Loss: 2.400
Iter 10/2000 - Loss: 2.241
Iter 11/2000 - Loss: 2.048
Iter 12/2000 - Loss: 1.827
Iter 13/2000 - Loss: 1.591
Iter 14/2000 - Loss: 1.349
Iter 15/2000 - Loss: 1.099
Iter 16/2000 - Loss: 0.838
Iter 17/2000 - Loss: 0.563
Iter 18/2000 - Loss: 0.272
Iter 19/2000 - Loss: -0.030
Iter 20/2000 - Loss: -0.341
Iter 1981/2000 - Loss: -8.392
Iter 1982/2000 - Loss: -8.392
Iter 1983/2000 - Loss: -8.392
Iter 1984/2000 - Loss: -8.393
Iter 1985/2000 - Loss: -8.393
Iter 1986/2000 - Loss: -8.393
Iter 1987/2000 - Loss: -8.393
Iter 1988/2000 - Loss: -8.393
Iter 1989/2000 - Loss: -8.393
Iter 1990/2000 - Loss: -8.393
Iter 1991/2000 - Loss: -8.393
Iter 1992/2000 - Loss: -8.393
Iter 1993/2000 - Loss: -8.393
Iter 1994/2000 - Loss: -8.393
Iter 1995/2000 - Loss: -8.393
Iter 1996/2000 - Loss: -8.393
Iter 1997/2000 - Loss: -8.393
Iter 1998/2000 - Loss: -8.393
Iter 1999/2000 - Loss: -8.393
Iter 2000/2000 - Loss: -8.393
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.2049, 12.7070, 29.5735,  7.8276,  5.0805, 53.0052]],

        [[21.5554, 29.3243,  8.7313,  1.4520,  1.6052, 23.7920]],

        [[21.6616, 32.3796,  9.1331,  1.0903,  1.0314, 20.1945]],

        [[18.9710, 28.4143, 14.3163,  1.7324,  2.0318, 39.4866]]])
Signal Variance: tensor([ 0.2124,  1.7936, 11.4202,  0.3896])
Estimated target variance: tensor([0.0210, 0.6380, 6.6090, 0.1155])
N: 450
Signal to noise ratio: tensor([25.9833, 73.7112, 76.2280, 38.6606])
Bound on condition number: tensor([ 303810.7223, 2445003.2832, 2614820.2339,  672591.4048])
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.0007563137081884072, policy loss: 10.737062552750551
Experience 30, Iter 1, disc loss: 0.0007548902536290317, policy loss: 10.735731442433949
Experience 30, Iter 2, disc loss: 0.0007532114299467849, policy loss: 10.672033844233507
Experience 30, Iter 3, disc loss: 0.0007480685189878984, policy loss: 10.81182174162219
Experience 30, Iter 4, disc loss: 0.0007463333355717359, policy loss: 10.837891217813038
Experience 30, Iter 5, disc loss: 0.0007446210849909382, policy loss: 10.816294705405616
Experience 30, Iter 6, disc loss: 0.0007419819926800623, policy loss: 10.873690408189814
Experience 30, Iter 7, disc loss: 0.0007390379542099876, policy loss: 10.906634110848724
Experience 30, Iter 8, disc loss: 0.0007385552648238891, policy loss: 10.877547374168833
Experience 30, Iter 9, disc loss: 0.0007378551405937568, policy loss: 10.783197234759921
Experience 30, Iter 10, disc loss: 0.0007366260088878288, policy loss: 10.765823416648953
Experience 30, Iter 11, disc loss: 0.0007337568832170761, policy loss: 10.816271154694832
Experience 30, Iter 12, disc loss: 0.0007308065752043649, policy loss: 10.856339529845043
Experience 30, Iter 13, disc loss: 0.0007303112008042605, policy loss: 10.813262415972751
Experience 30, Iter 14, disc loss: 0.0007342784570399671, policy loss: 10.616893370946855
Experience 30, Iter 15, disc loss: 0.0007292347591610201, policy loss: 10.667554536738745
Experience 30, Iter 16, disc loss: 0.0007244449135013875, policy loss: 10.844040923991976
Experience 30, Iter 17, disc loss: 0.0007250252303238991, policy loss: 10.720809250285852
Experience 30, Iter 18, disc loss: 0.0007226015235559724, policy loss: 10.73722013085009
Experience 30, Iter 19, disc loss: 0.0007208811609370848, policy loss: 10.726316503127169
Experience 30, Iter 20, disc loss: 0.0007203747324404997, policy loss: 10.687349332407454
Experience 30, Iter 21, disc loss: 0.0007202413232176904, policy loss: 10.602993234328547
Experience 30, Iter 22, disc loss: 0.0007176040569555854, policy loss: 10.74638308054122
Experience 30, Iter 23, disc loss: 0.0007153019277131863, policy loss: 10.685480583586806
Experience 30, Iter 24, disc loss: 0.0007105417581849423, policy loss: 10.917668538832098
Experience 30, Iter 25, disc loss: 0.0007060197600204509, policy loss: 11.109364701246882
Experience 30, Iter 26, disc loss: 0.0007074615224180041, policy loss: 10.894286201408118
Experience 30, Iter 27, disc loss: 0.0007042945027535279, policy loss: 10.933821457983804
Experience 30, Iter 28, disc loss: 0.0007077844301724015, policy loss: 10.739907403986042
Experience 30, Iter 29, disc loss: 0.0007046148080197855, policy loss: 10.735884521682856
Experience 30, Iter 30, disc loss: 0.0007009298573334901, policy loss: 10.83613657564448
Experience 30, Iter 31, disc loss: 0.0007011694047711943, policy loss: 10.717234258499069
Experience 30, Iter 32, disc loss: 0.0006989798841047385, policy loss: 10.750518928709752
Experience 30, Iter 33, disc loss: 0.0006933555626057158, policy loss: 11.020083733275843
Experience 30, Iter 34, disc loss: 0.0006963369032636739, policy loss: 10.680619189610747
Experience 30, Iter 35, disc loss: 0.0006941714908592802, policy loss: 10.772826225615084
Experience 30, Iter 36, disc loss: 0.0006932982250560624, policy loss: 10.743674864228252
Experience 30, Iter 37, disc loss: 0.0006921258327073109, policy loss: 10.695749179692658
Experience 30, Iter 38, disc loss: 0.0006911735529653521, policy loss: 10.653740644288728
Experience 30, Iter 39, disc loss: 0.0006890493612087818, policy loss: 10.793498162135915
Experience 30, Iter 40, disc loss: 0.000689597093264557, policy loss: 10.623361298087662
Experience 30, Iter 41, disc loss: 0.0006899899083038569, policy loss: 10.675088717901604
Experience 30, Iter 42, disc loss: 0.0006870903835105238, policy loss: 10.59277683475375
Experience 30, Iter 43, disc loss: 0.0006857645248188504, policy loss: 10.547535012685596
Experience 30, Iter 44, disc loss: 0.0006833144598361457, policy loss: 10.632855202290282
Experience 30, Iter 45, disc loss: 0.0006805736050262406, policy loss: 10.73826313359116
Experience 30, Iter 46, disc loss: 0.0006782322062031238, policy loss: 10.785831477281413
Experience 30, Iter 47, disc loss: 0.0006775642753853557, policy loss: 10.623338691326612
Experience 30, Iter 48, disc loss: 0.0006738840231120935, policy loss: 10.754288660109147
Experience 30, Iter 49, disc loss: 0.0006727420473941463, policy loss: 10.898544298543154
Experience 30, Iter 50, disc loss: 0.0006709885673565246, policy loss: 10.754271560011869
Experience 30, Iter 51, disc loss: 0.0006727664246265908, policy loss: 10.59922944253832
Experience 30, Iter 52, disc loss: 0.0006646286293079249, policy loss: 10.890687058371654
Experience 30, Iter 53, disc loss: 0.0006621697860694112, policy loss: 10.927286505780803
Experience 30, Iter 54, disc loss: 0.0006558064992644564, policy loss: 11.377482615402329
Experience 30, Iter 55, disc loss: 0.0006556450208929566, policy loss: 11.214050873103815
Experience 30, Iter 56, disc loss: 0.0006477433692137464, policy loss: 11.711290948413694
Experience 30, Iter 57, disc loss: 0.0006446343455990491, policy loss: 11.89827168543605
Experience 30, Iter 58, disc loss: 0.0006421433869493515, policy loss: 12.074090157191293
Experience 30, Iter 59, disc loss: 0.000642329740208355, policy loss: 11.816987758604599
Experience 30, Iter 60, disc loss: 0.000640616063125344, policy loss: 11.955634346183027
Experience 30, Iter 61, disc loss: 0.0006420801802755447, policy loss: 11.611195268057163
Experience 30, Iter 62, disc loss: 0.0006433724998679594, policy loss: 11.404827627867004
Experience 30, Iter 63, disc loss: 0.0006434920345324149, policy loss: 11.261577255795935
Experience 30, Iter 64, disc loss: 0.0006433767392935932, policy loss: 11.162740320242673
Experience 30, Iter 65, disc loss: 0.0006427262339530438, policy loss: 11.136132261545534
Experience 30, Iter 66, disc loss: 0.00063933658180542, policy loss: 11.316132869592947
Experience 30, Iter 67, disc loss: 0.0006395922626490375, policy loss: 11.16428057759336
Experience 30, Iter 68, disc loss: 0.0006396300517766102, policy loss: 11.151390096378133
Experience 30, Iter 69, disc loss: 0.0006383427278960628, policy loss: 11.142516660711374
Experience 30, Iter 70, disc loss: 0.000640784050719908, policy loss: 10.820323118374123
Experience 30, Iter 71, disc loss: 0.0006432498290740387, policy loss: 10.751918440397814
Experience 30, Iter 72, disc loss: 0.0006351779942691846, policy loss: 10.907982479593233
Experience 30, Iter 73, disc loss: 0.0006392817722609791, policy loss: 10.65639169049479
Experience 30, Iter 74, disc loss: 0.0006344363853526584, policy loss: 10.769848949742274
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.1609],
        [1.6628],
        [0.0290]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0422, 0.2256, 1.3561, 0.0270, 0.0156, 4.3866]],

        [[0.0422, 0.2256, 1.3561, 0.0270, 0.0156, 4.3866]],

        [[0.0422, 0.2256, 1.3561, 0.0270, 0.0156, 4.3866]],

        [[0.0422, 0.2256, 1.3561, 0.0270, 0.0156, 4.3866]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0207, 0.6437, 6.6512, 0.1161], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0207, 0.6437, 6.6512, 0.1161])
N: 465
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1861.0000, 1861.0000, 1861.0000, 1861.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.244
Iter 2/2000 - Loss: 3.322
Iter 3/2000 - Loss: 3.094
Iter 4/2000 - Loss: 3.036
Iter 5/2000 - Loss: 2.994
Iter 6/2000 - Loss: 2.855
Iter 7/2000 - Loss: 2.682
Iter 8/2000 - Loss: 2.527
Iter 9/2000 - Loss: 2.384
Iter 10/2000 - Loss: 2.224
Iter 11/2000 - Loss: 2.028
Iter 12/2000 - Loss: 1.804
Iter 13/2000 - Loss: 1.565
Iter 14/2000 - Loss: 1.320
Iter 15/2000 - Loss: 1.068
Iter 16/2000 - Loss: 0.804
Iter 17/2000 - Loss: 0.527
Iter 18/2000 - Loss: 0.234
Iter 19/2000 - Loss: -0.070
Iter 20/2000 - Loss: -0.382
Iter 1981/2000 - Loss: -8.420
Iter 1982/2000 - Loss: -8.420
Iter 1983/2000 - Loss: -8.420
Iter 1984/2000 - Loss: -8.420
Iter 1985/2000 - Loss: -8.420
Iter 1986/2000 - Loss: -8.421
Iter 1987/2000 - Loss: -8.421
Iter 1988/2000 - Loss: -8.421
Iter 1989/2000 - Loss: -8.421
Iter 1990/2000 - Loss: -8.421
Iter 1991/2000 - Loss: -8.421
Iter 1992/2000 - Loss: -8.421
Iter 1993/2000 - Loss: -8.421
Iter 1994/2000 - Loss: -8.421
Iter 1995/2000 - Loss: -8.421
Iter 1996/2000 - Loss: -8.421
Iter 1997/2000 - Loss: -8.421
Iter 1998/2000 - Loss: -8.421
Iter 1999/2000 - Loss: -8.421
Iter 2000/2000 - Loss: -8.421
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.0732, 12.5560, 29.4960,  7.3539,  5.0741, 52.9740]],

        [[21.1388, 29.2353,  8.6134,  1.4556,  1.5754, 23.7427]],

        [[21.4061, 32.2658,  9.0816,  1.0681,  1.0105, 20.5542]],

        [[18.6854, 28.0715, 14.3414,  1.7219,  2.0361, 39.7057]]])
Signal Variance: tensor([ 0.2071,  1.7728, 11.3308,  0.3904])
Estimated target variance: tensor([0.0207, 0.6437, 6.6512, 0.1161])
N: 465
Signal to noise ratio: tensor([25.6470, 73.7501, 75.3787, 38.7455])
Bound on condition number: tensor([ 305862.7749, 2529169.4669, 2642109.5743,  698064.0453])
Policy Optimizer learning rate:
0.009688861611972634
Experience 31, Iter 0, disc loss: 0.0006296124311764595, policy loss: 10.977473887537027
Experience 31, Iter 1, disc loss: 0.0006269423645945822, policy loss: 11.139802588738927
Experience 31, Iter 2, disc loss: 0.0006295370903271111, policy loss: 11.113296925325892
Experience 31, Iter 3, disc loss: 0.0006252988934203307, policy loss: 11.046504024600194
Experience 31, Iter 4, disc loss: 0.0006296874928606002, policy loss: 10.82422118944581
Experience 31, Iter 5, disc loss: 0.0006294463380395542, policy loss: 10.786591766225783
Experience 31, Iter 6, disc loss: 0.0006321397956198749, policy loss: 10.594555394951207
Experience 31, Iter 7, disc loss: 0.0006319901733495627, policy loss: 10.491341991966753
Experience 31, Iter 8, disc loss: 0.0006256221085525678, policy loss: 10.709677414295285
Experience 31, Iter 9, disc loss: 0.000627145828131282, policy loss: 10.578082411776347
Experience 31, Iter 10, disc loss: 0.0006277314429550845, policy loss: 10.556735772407068
Experience 31, Iter 11, disc loss: 0.0006243870601814556, policy loss: 10.61218511286369
Experience 31, Iter 12, disc loss: 0.0006236305518354116, policy loss: 10.618506684636966
Experience 31, Iter 13, disc loss: 0.0006205913311635887, policy loss: 10.63963691926974
Experience 31, Iter 14, disc loss: 0.0006202492544972217, policy loss: 10.573813529247591
Experience 31, Iter 15, disc loss: 0.0006164025138864311, policy loss: 10.71090061727483
Experience 31, Iter 16, disc loss: 0.0006214878961000897, policy loss: 10.521965280387862
Experience 31, Iter 17, disc loss: 0.0006213091304255584, policy loss: 10.450533285807914
Experience 31, Iter 18, disc loss: 0.0006222400617155672, policy loss: 10.32022684390283
Experience 31, Iter 19, disc loss: 0.0006128678120538979, policy loss: 10.666427677071006
Experience 31, Iter 20, disc loss: 0.000612185537219598, policy loss: 10.605757481173825
Experience 31, Iter 21, disc loss: 0.000612925872139946, policy loss: 10.530308456415238
Experience 31, Iter 22, disc loss: 0.0006108034937302963, policy loss: 10.550215670679709
Experience 31, Iter 23, disc loss: 0.0006096004552437407, policy loss: 10.556229447457802
Experience 31, Iter 24, disc loss: 0.0006082288735384168, policy loss: 10.536342234712087
Experience 31, Iter 25, disc loss: 0.0006082718589233832, policy loss: 10.495640673381164
Experience 31, Iter 26, disc loss: 0.0006015659160726878, policy loss: 10.781637895975566
Experience 31, Iter 27, disc loss: 0.0006023984973263535, policy loss: 10.582786724774337
Experience 31, Iter 28, disc loss: 0.0005995297210402077, policy loss: 10.715917550663475
Experience 31, Iter 29, disc loss: 0.000603315949070028, policy loss: 10.531618073268858
Experience 31, Iter 30, disc loss: 0.0006020769036806854, policy loss: 10.551292420041719
Experience 31, Iter 31, disc loss: 0.0006015857891503692, policy loss: 10.467736968584546
Experience 31, Iter 32, disc loss: 0.000596780149755023, policy loss: 10.700510554704058
Experience 31, Iter 33, disc loss: 0.0005969640767103141, policy loss: 10.59912587243158
Experience 31, Iter 34, disc loss: 0.0005968244345478114, policy loss: 10.511763915691153
Experience 31, Iter 35, disc loss: 0.0006031641293576544, policy loss: 10.264428717277303
Experience 31, Iter 36, disc loss: 0.0005952650617876776, policy loss: 10.664407115652082
Experience 31, Iter 37, disc loss: 0.0005954795718512043, policy loss: 10.551659696976298
Experience 31, Iter 38, disc loss: 0.0005948864188809633, policy loss: 10.501368589648186
Experience 31, Iter 39, disc loss: 0.0005863283726167824, policy loss: 10.790653549506
Experience 31, Iter 40, disc loss: 0.0005896834546731342, policy loss: 10.575064803740823
Experience 31, Iter 41, disc loss: 0.0005903490469735425, policy loss: 10.472124050373008
Experience 31, Iter 42, disc loss: 0.0005923817263469899, policy loss: 10.413603618342604
Experience 31, Iter 43, disc loss: 0.0005905561970317246, policy loss: 10.451721889695246
Experience 31, Iter 44, disc loss: 0.0005835540730582554, policy loss: 10.595635974770946
Experience 31, Iter 45, disc loss: 0.000577919956945432, policy loss: 10.881931401289515
Experience 31, Iter 46, disc loss: 0.0005762793088572662, policy loss: 10.79101096940362
Experience 31, Iter 47, disc loss: 0.0005772592595759481, policy loss: 10.77630606168114
Experience 31, Iter 48, disc loss: 0.0005851793976442102, policy loss: 10.400825194698925
Experience 31, Iter 49, disc loss: 0.0005812987229958306, policy loss: 10.520777619446097
Experience 31, Iter 50, disc loss: 0.0005799564485950893, policy loss: 10.481269571611346
Experience 31, Iter 51, disc loss: 0.0005788858150525572, policy loss: 10.575746956485496
Experience 31, Iter 52, disc loss: 0.0005790329848935728, policy loss: 10.394398974583986
Experience 31, Iter 53, disc loss: 0.0005777403429400436, policy loss: 10.502025159349632
Experience 31, Iter 54, disc loss: 0.0005744650625160637, policy loss: 10.534169355815376
Experience 31, Iter 55, disc loss: 0.000575886332161979, policy loss: 10.42768860740545
Experience 31, Iter 56, disc loss: 0.0005738005184567763, policy loss: 10.466649154979557
Experience 31, Iter 57, disc loss: 0.000570590005405891, policy loss: 10.494641710142115
Experience 31, Iter 58, disc loss: 0.0005706870291511919, policy loss: 10.558333195668652
Experience 31, Iter 59, disc loss: 0.0005654216802241853, policy loss: 10.702282984946761
Experience 31, Iter 60, disc loss: 0.000567455750055493, policy loss: 10.556566839628523
Experience 31, Iter 61, disc loss: 0.0005721597734715236, policy loss: 10.393442502320696
Experience 31, Iter 62, disc loss: 0.0005715113484742353, policy loss: 10.346699943412682
Experience 31, Iter 63, disc loss: 0.0005674097123409855, policy loss: 10.557631785305412
Experience 31, Iter 64, disc loss: 0.0005589650581508426, policy loss: 10.794931536369306
Experience 31, Iter 65, disc loss: 0.0005578113084419726, policy loss: 10.816601050349789
Experience 31, Iter 66, disc loss: 0.0005607796583494826, policy loss: 10.609112920270885
Experience 31, Iter 67, disc loss: 0.0005643118384234892, policy loss: 10.482206148547139
Experience 31, Iter 68, disc loss: 0.0005673445302482585, policy loss: 10.310453337892003
Experience 31, Iter 69, disc loss: 0.0005599745449867023, policy loss: 10.531360948673232
Experience 31, Iter 70, disc loss: 0.0005588388086369458, policy loss: 10.576343326148795
Experience 31, Iter 71, disc loss: 0.0005576319580080707, policy loss: 10.534029468407686
Experience 31, Iter 72, disc loss: 0.0005575522271159087, policy loss: 10.573581787618139
Experience 31, Iter 73, disc loss: 0.0005535336112022698, policy loss: 10.667534916408227
Experience 31, Iter 74, disc loss: 0.0005582950863840577, policy loss: 10.364270785027808
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1627],
        [1.6774],
        [0.0292]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0409, 0.2231, 1.3642, 0.0270, 0.0154, 4.4310]],

        [[0.0409, 0.2231, 1.3642, 0.0270, 0.0154, 4.4310]],

        [[0.0409, 0.2231, 1.3642, 0.0270, 0.0154, 4.4310]],

        [[0.0409, 0.2231, 1.3642, 0.0270, 0.0154, 4.4310]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0204, 0.6508, 6.7097, 0.1168], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0204, 0.6508, 6.7097, 0.1168])
N: 480
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1921.0000, 1921.0000, 1921.0000, 1921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.232
Iter 2/2000 - Loss: 3.306
Iter 3/2000 - Loss: 3.073
Iter 4/2000 - Loss: 3.009
Iter 5/2000 - Loss: 2.961
Iter 6/2000 - Loss: 2.816
Iter 7/2000 - Loss: 2.639
Iter 8/2000 - Loss: 2.479
Iter 9/2000 - Loss: 2.331
Iter 10/2000 - Loss: 2.167
Iter 11/2000 - Loss: 1.968
Iter 12/2000 - Loss: 1.740
Iter 13/2000 - Loss: 1.499
Iter 14/2000 - Loss: 1.251
Iter 15/2000 - Loss: 0.997
Iter 16/2000 - Loss: 0.732
Iter 17/2000 - Loss: 0.454
Iter 18/2000 - Loss: 0.161
Iter 19/2000 - Loss: -0.143
Iter 20/2000 - Loss: -0.454
Iter 1981/2000 - Loss: -8.446
Iter 1982/2000 - Loss: -8.446
Iter 1983/2000 - Loss: -8.446
Iter 1984/2000 - Loss: -8.446
Iter 1985/2000 - Loss: -8.446
Iter 1986/2000 - Loss: -8.447
Iter 1987/2000 - Loss: -8.447
Iter 1988/2000 - Loss: -8.447
Iter 1989/2000 - Loss: -8.447
Iter 1990/2000 - Loss: -8.447
Iter 1991/2000 - Loss: -8.447
Iter 1992/2000 - Loss: -8.447
Iter 1993/2000 - Loss: -8.447
Iter 1994/2000 - Loss: -8.447
Iter 1995/2000 - Loss: -8.447
Iter 1996/2000 - Loss: -8.447
Iter 1997/2000 - Loss: -8.447
Iter 1998/2000 - Loss: -8.447
Iter 1999/2000 - Loss: -8.447
Iter 2000/2000 - Loss: -8.447
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.9368, 12.3529, 29.3479,  7.1677,  4.7874, 52.7184]],

        [[20.8729, 28.9832,  8.5711,  1.4621,  1.5845, 23.7095]],

        [[20.9630, 32.1644,  9.1078,  1.0771,  1.0265, 20.9278]],

        [[18.3513, 27.5868, 14.4457,  1.7419,  2.0333, 39.9981]]])
Signal Variance: tensor([ 0.2033,  1.7737, 11.5310,  0.3918])
Estimated target variance: tensor([0.0204, 0.6508, 6.7097, 0.1168])
N: 480
Signal to noise ratio: tensor([25.3363, 74.4257, 75.1298, 38.7057])
Bound on condition number: tensor([ 308127.3067, 2658812.1725, 2709356.4802,  719102.6047])
Policy Optimizer learning rate:
0.009678658753253001
Experience 32, Iter 0, disc loss: 0.0005537492521337536, policy loss: 10.559468707678926
Experience 32, Iter 1, disc loss: 0.0005533458100551811, policy loss: 10.458310179782924
Experience 32, Iter 2, disc loss: 0.0005497914749397981, policy loss: 10.524059474681845
Experience 32, Iter 3, disc loss: 0.0005514568729617919, policy loss: 10.492580014768883
Experience 32, Iter 4, disc loss: 0.0005520069797564626, policy loss: 10.372065255893013
Experience 32, Iter 5, disc loss: 0.0005502276283243652, policy loss: 10.404252419424804
Experience 32, Iter 6, disc loss: 0.0005497594942080638, policy loss: 10.405309880964735
Experience 32, Iter 7, disc loss: 0.0005449141232730262, policy loss: 10.543607515429951
Experience 32, Iter 8, disc loss: 0.0005428287569809519, policy loss: 10.68765452926265
Experience 32, Iter 9, disc loss: 0.000546279456170191, policy loss: 10.467407540490978
Experience 32, Iter 10, disc loss: 0.0005445076186473627, policy loss: 10.465118038255689
Experience 32, Iter 11, disc loss: 0.0005453056639705795, policy loss: 10.41412433397669
Experience 32, Iter 12, disc loss: 0.0005391822046925336, policy loss: 10.587219393735545
Experience 32, Iter 13, disc loss: 0.0005340185099767629, policy loss: 10.753692827939126
Experience 32, Iter 14, disc loss: 0.0005407673885306665, policy loss: 10.450156196176312
Experience 32, Iter 15, disc loss: 0.0005374495995365106, policy loss: 10.614146222963479
Experience 32, Iter 16, disc loss: 0.0005377056505715465, policy loss: 10.442042180272207
Experience 32, Iter 17, disc loss: 0.000535365452789767, policy loss: 10.572959022327197
Experience 32, Iter 18, disc loss: 0.0005340089031404559, policy loss: 10.629362444675
Experience 32, Iter 19, disc loss: 0.0005318726610077205, policy loss: 10.704129441902388
Experience 32, Iter 20, disc loss: 0.0005317471255498988, policy loss: 10.545995659757128
Experience 32, Iter 21, disc loss: 0.0005338502707636542, policy loss: 10.494754042867132
Experience 32, Iter 22, disc loss: 0.0005360244545190135, policy loss: 10.394867363736687
Experience 32, Iter 23, disc loss: 0.00053280282365944, policy loss: 10.53365351932392
Experience 32, Iter 24, disc loss: 0.0005297607351763695, policy loss: 10.577919510528329
Experience 32, Iter 25, disc loss: 0.0005295756732107175, policy loss: 10.537559474641496
Experience 32, Iter 26, disc loss: 0.0005304731740502852, policy loss: 10.46006430407213
Experience 32, Iter 27, disc loss: 0.0005282308308428584, policy loss: 10.702838613670988
Experience 32, Iter 28, disc loss: 0.0005301747197692083, policy loss: 10.444789868760008
Experience 32, Iter 29, disc loss: 0.0005281035266467253, policy loss: 10.509502156573067
Experience 32, Iter 30, disc loss: 0.0005242099333008762, policy loss: 10.60554923990482
Experience 32, Iter 31, disc loss: 0.0005272997245793517, policy loss: 10.333540383621445
Experience 32, Iter 32, disc loss: 0.0005273699389515853, policy loss: 10.334305117555001
Experience 32, Iter 33, disc loss: 0.0005248948298570279, policy loss: 10.422551579903244
Experience 32, Iter 34, disc loss: 0.0005195358873240014, policy loss: 10.48256664385945
Experience 32, Iter 35, disc loss: 0.000522950021008613, policy loss: 10.365800353345122
Experience 32, Iter 36, disc loss: 0.000520093539342333, policy loss: 10.479348691778979
Experience 32, Iter 37, disc loss: 0.0005265940106462994, policy loss: 10.351597079194775
Experience 32, Iter 38, disc loss: 0.0005159611098908273, policy loss: 10.689801498538756
Experience 32, Iter 39, disc loss: 0.0005236624841363853, policy loss: 10.315635830121334
Experience 32, Iter 40, disc loss: 0.0005250320923681434, policy loss: 10.268730818019181
Experience 32, Iter 41, disc loss: 0.0005205131307300507, policy loss: 10.339427296978577
Experience 32, Iter 42, disc loss: 0.0005135345733839004, policy loss: 10.679174064155395
Experience 32, Iter 43, disc loss: 0.0005201925657772236, policy loss: 10.413629268519472
Experience 32, Iter 44, disc loss: 0.0005107844002219285, policy loss: 10.650126964140988
Experience 32, Iter 45, disc loss: 0.0005180637564619804, policy loss: 10.231455762766746
Experience 32, Iter 46, disc loss: 0.0005222868296253913, policy loss: 10.1432940960031
Experience 32, Iter 47, disc loss: 0.0005155384298303219, policy loss: 10.358759187394117
Experience 32, Iter 48, disc loss: 0.0005171250329173141, policy loss: 10.282202581258105
Experience 32, Iter 49, disc loss: 0.0005075313409604592, policy loss: 10.764501578172009
Experience 32, Iter 50, disc loss: 0.0005129961118286564, policy loss: 10.470937168461756
Experience 32, Iter 51, disc loss: 0.0005115484648791442, policy loss: 10.6407790645117
Experience 32, Iter 52, disc loss: 0.0005087998679849169, policy loss: 10.491438117678442
Experience 32, Iter 53, disc loss: 0.0005101552886002499, policy loss: 10.383471229613146
Experience 32, Iter 54, disc loss: 0.0005023235412414395, policy loss: 10.625568199194657
Experience 32, Iter 55, disc loss: 0.0004991740068740058, policy loss: 10.577529875741796
Experience 32, Iter 56, disc loss: 0.0005053433795016568, policy loss: 10.456208936054935
Experience 32, Iter 57, disc loss: 0.0004906525183873934, policy loss: 11.054124287186553
Experience 32, Iter 58, disc loss: 0.0004954519629908325, policy loss: 10.698091968842391
Experience 32, Iter 59, disc loss: 0.0004988014413018445, policy loss: 10.599365628869869
Experience 32, Iter 60, disc loss: 0.00048459750465004915, policy loss: 11.253153489451185
Experience 32, Iter 61, disc loss: 0.0004777495064876903, policy loss: 11.644215935055515
Experience 32, Iter 62, disc loss: 0.00048653801112206134, policy loss: 11.066054216110558
Experience 32, Iter 63, disc loss: 0.0004918598944680021, policy loss: 10.727953889266189
Experience 32, Iter 64, disc loss: 0.0004837657440832764, policy loss: 11.152504558973945
Experience 32, Iter 65, disc loss: 0.0004847442811125807, policy loss: 11.09730609293728
Experience 32, Iter 66, disc loss: 0.0004918053247261947, policy loss: 10.659233116045698
Experience 32, Iter 67, disc loss: 0.0004843813615108691, policy loss: 11.161218121251531
Experience 32, Iter 68, disc loss: 0.0004708980520962363, policy loss: 11.869610293548423
Experience 32, Iter 69, disc loss: 0.000484485306542892, policy loss: 11.12490525158157
Experience 32, Iter 70, disc loss: 0.00048480503140544047, policy loss: 10.755183773588302
Experience 32, Iter 71, disc loss: 0.00048352957891698603, policy loss: 10.942397987598902
Experience 32, Iter 72, disc loss: 0.0004798081812935034, policy loss: 11.170700426044563
Experience 32, Iter 73, disc loss: 0.00048271681586651736, policy loss: 10.974616123123656
Experience 32, Iter 74, disc loss: 0.00048026021482157523, policy loss: 11.082764369794253
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1641],
        [1.6852],
        [0.0293]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0399, 0.2210, 1.3707, 0.0271, 0.0153, 4.4757]],

        [[0.0399, 0.2210, 1.3707, 0.0271, 0.0153, 4.4757]],

        [[0.0399, 0.2210, 1.3707, 0.0271, 0.0153, 4.4757]],

        [[0.0399, 0.2210, 1.3707, 0.0271, 0.0153, 4.4757]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0202, 0.6566, 6.7408, 0.1174], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0202, 0.6566, 6.7408, 0.1174])
N: 495
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1981.0000, 1981.0000, 1981.0000, 1981.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.240
Iter 2/2000 - Loss: 3.316
Iter 3/2000 - Loss: 3.082
Iter 4/2000 - Loss: 3.019
Iter 5/2000 - Loss: 2.971
Iter 6/2000 - Loss: 2.825
Iter 7/2000 - Loss: 2.646
Iter 8/2000 - Loss: 2.484
Iter 9/2000 - Loss: 2.335
Iter 10/2000 - Loss: 2.168
Iter 11/2000 - Loss: 1.966
Iter 12/2000 - Loss: 1.735
Iter 13/2000 - Loss: 1.489
Iter 14/2000 - Loss: 1.236
Iter 15/2000 - Loss: 0.977
Iter 16/2000 - Loss: 0.708
Iter 17/2000 - Loss: 0.426
Iter 18/2000 - Loss: 0.131
Iter 19/2000 - Loss: -0.176
Iter 20/2000 - Loss: -0.491
Iter 1981/2000 - Loss: -8.487
Iter 1982/2000 - Loss: -8.487
Iter 1983/2000 - Loss: -8.487
Iter 1984/2000 - Loss: -8.487
Iter 1985/2000 - Loss: -8.487
Iter 1986/2000 - Loss: -8.487
Iter 1987/2000 - Loss: -8.487
Iter 1988/2000 - Loss: -8.487
Iter 1989/2000 - Loss: -8.487
Iter 1990/2000 - Loss: -8.487
Iter 1991/2000 - Loss: -8.487
Iter 1992/2000 - Loss: -8.487
Iter 1993/2000 - Loss: -8.487
Iter 1994/2000 - Loss: -8.487
Iter 1995/2000 - Loss: -8.487
Iter 1996/2000 - Loss: -8.487
Iter 1997/2000 - Loss: -8.487
Iter 1998/2000 - Loss: -8.487
Iter 1999/2000 - Loss: -8.487
Iter 2000/2000 - Loss: -8.487
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.9982, 12.2553, 28.6103,  7.0878,  4.8070, 52.5561]],

        [[20.2986, 28.5670,  8.6533,  1.4754,  1.5387, 24.2219]],

        [[20.4895, 32.0098,  9.1207,  1.0859,  1.0262, 20.9734]],

        [[17.9386, 27.2359, 14.5410,  1.7327,  2.0306, 39.9250]]])
Signal Variance: tensor([ 0.2026,  1.8048, 11.5176,  0.3942])
Estimated target variance: tensor([0.0202, 0.6566, 6.7408, 0.1174])
N: 495
Signal to noise ratio: tensor([25.5111, 74.5999, 75.8882, 38.8277])
Bound on condition number: tensor([ 322153.8855, 2754748.6875, 2850714.6872,  746258.5825])
Policy Optimizer learning rate:
0.009668466638656902
Experience 33, Iter 0, disc loss: 0.0004792482714589595, policy loss: 10.959829682035947
Experience 33, Iter 1, disc loss: 0.0004753454064448716, policy loss: 11.103468225817636
Experience 33, Iter 2, disc loss: 0.0004782874974026304, policy loss: 10.93959764649978
Experience 33, Iter 3, disc loss: 0.0004861868138747013, policy loss: 10.500488500983773
Experience 33, Iter 4, disc loss: 0.0004788194683046919, policy loss: 10.765605666652265
Experience 33, Iter 5, disc loss: 0.000484491929222501, policy loss: 10.432490041078424
Experience 33, Iter 6, disc loss: 0.0004865480244619201, policy loss: 10.414677754087574
Experience 33, Iter 7, disc loss: 0.000478671138871849, policy loss: 10.75889470289134
Experience 33, Iter 8, disc loss: 0.0004754597004487061, policy loss: 10.693281828231541
Experience 33, Iter 9, disc loss: 0.00047777919258624524, policy loss: 10.683739249776345
Experience 33, Iter 10, disc loss: 0.00048484506935007844, policy loss: 10.287550826088662
Experience 33, Iter 11, disc loss: 0.00048608563710079235, policy loss: 10.28652589737331
Experience 33, Iter 12, disc loss: 0.00048363001656596705, policy loss: 10.297136294170183
Experience 33, Iter 13, disc loss: 0.00048478289727662627, policy loss: 10.213398002440247
Experience 33, Iter 14, disc loss: 0.0004791675316976947, policy loss: 10.43408052896529
Experience 33, Iter 15, disc loss: 0.00047428643089693974, policy loss: 10.501954538440053
Experience 33, Iter 16, disc loss: 0.0004773352770820921, policy loss: 10.350833191195912
Experience 33, Iter 17, disc loss: 0.0004751083438697497, policy loss: 10.44083643168409
Experience 33, Iter 18, disc loss: 0.00047068072646892203, policy loss: 10.673465677912061
Experience 33, Iter 19, disc loss: 0.0004763680023171814, policy loss: 10.349191065578308
Experience 33, Iter 20, disc loss: 0.0004706300370416548, policy loss: 10.567735697332864
Experience 33, Iter 21, disc loss: 0.00047062087618059806, policy loss: 10.803622377388255
Experience 33, Iter 22, disc loss: 0.00047083671665640856, policy loss: 10.64941963596102
Experience 33, Iter 23, disc loss: 0.00047575874615907063, policy loss: 10.324734198088208
Experience 33, Iter 24, disc loss: 0.0004705784790214652, policy loss: 10.63288277579204
Experience 33, Iter 25, disc loss: 0.00046892741428707644, policy loss: 10.562891315671875
Experience 33, Iter 26, disc loss: 0.0004705838979369418, policy loss: 10.384110477220041
Experience 33, Iter 27, disc loss: 0.00046761812392331437, policy loss: 10.503141623402392
Experience 33, Iter 28, disc loss: 0.00046654763509853116, policy loss: 10.424512350732801
Experience 33, Iter 29, disc loss: 0.000470716228454347, policy loss: 10.21719133065443
Experience 33, Iter 30, disc loss: 0.0004692846745558277, policy loss: 10.340918372620056
Experience 33, Iter 31, disc loss: 0.0004666351058829632, policy loss: 10.402689055624073
Experience 33, Iter 32, disc loss: 0.0004671134346587586, policy loss: 10.351712823148144
Experience 33, Iter 33, disc loss: 0.00046718292313526486, policy loss: 10.39988377777281
Experience 33, Iter 34, disc loss: 0.00047000933803937093, policy loss: 10.266516942548943
Experience 33, Iter 35, disc loss: 0.00046118054549065774, policy loss: 10.496689745118505
Experience 33, Iter 36, disc loss: 0.0004618326009889366, policy loss: 10.463210990946255
Experience 33, Iter 37, disc loss: 0.000467549098435499, policy loss: 10.198269162427092
Experience 33, Iter 38, disc loss: 0.00046027952620658077, policy loss: 10.50608214464415
Experience 33, Iter 39, disc loss: 0.000466004739701104, policy loss: 10.337999834167388
Experience 33, Iter 40, disc loss: 0.0004655001841110338, policy loss: 10.32699397555022
Experience 33, Iter 41, disc loss: 0.0004610773370240822, policy loss: 10.50204483357463
Experience 33, Iter 42, disc loss: 0.0004554374647365901, policy loss: 10.595435042136682
Experience 33, Iter 43, disc loss: 0.00045168328137096465, policy loss: 10.940489642998708
Experience 33, Iter 44, disc loss: 0.00045816976115583077, policy loss: 10.593849924556698
Experience 33, Iter 45, disc loss: 0.00045836204327364733, policy loss: 10.461248044138685
Experience 33, Iter 46, disc loss: 0.00045432870046978904, policy loss: 10.654096508436645
Experience 33, Iter 47, disc loss: 0.0004592642606065464, policy loss: 10.51627467912019
Experience 33, Iter 48, disc loss: 0.00045573044808429916, policy loss: 10.318091627613256
Experience 33, Iter 49, disc loss: 0.00045933271643007657, policy loss: 10.282201190745258
Experience 33, Iter 50, disc loss: 0.000455711831654738, policy loss: 10.327004041524331
Experience 33, Iter 51, disc loss: 0.0004564845815565055, policy loss: 10.353840228073722
Experience 33, Iter 52, disc loss: 0.00044880017579199507, policy loss: 10.682243673539904
Experience 33, Iter 53, disc loss: 0.0004599146082818605, policy loss: 10.177401253197132
Experience 33, Iter 54, disc loss: 0.0004491935552683337, policy loss: 10.478915457952168
Experience 33, Iter 55, disc loss: 0.000456864013587895, policy loss: 10.540525675189901
Experience 33, Iter 56, disc loss: 0.0004555755648472999, policy loss: 10.354253628801446
Experience 33, Iter 57, disc loss: 0.00044735232180865246, policy loss: 10.592691240397029
Experience 33, Iter 58, disc loss: 0.0004530771631273533, policy loss: 10.280250679001929
Experience 33, Iter 59, disc loss: 0.0004391599170772562, policy loss: 11.120231754718805
Experience 33, Iter 60, disc loss: 0.0004419801173092722, policy loss: 10.532144763478769
Experience 33, Iter 61, disc loss: 0.00044329216808168385, policy loss: 10.588960938195871
Experience 33, Iter 62, disc loss: 0.00044538580987757437, policy loss: 10.74189535502823
Experience 33, Iter 63, disc loss: 0.0004510579575697205, policy loss: 10.326491073915811
Experience 33, Iter 64, disc loss: 0.0004511931912182822, policy loss: 10.280774278620399
Experience 33, Iter 65, disc loss: 0.00044652390592202175, policy loss: 10.84518869327801
Experience 33, Iter 66, disc loss: 0.00045005630717422204, policy loss: 10.21087160200673
Experience 33, Iter 67, disc loss: 0.00044362131166318235, policy loss: 10.636650367247958
Experience 33, Iter 68, disc loss: 0.0004454420312946141, policy loss: 10.769370210381155
Experience 33, Iter 69, disc loss: 0.00045126019488963, policy loss: 10.106788317973773
Experience 33, Iter 70, disc loss: 0.00045099251091633614, policy loss: 10.067169070196817
Experience 33, Iter 71, disc loss: 0.0004485400106221178, policy loss: 10.109770405002703
Experience 33, Iter 72, disc loss: 0.0004465758061185241, policy loss: 10.271758740769434
Experience 33, Iter 73, disc loss: 0.00044150675797968643, policy loss: 10.367408011764377
Experience 33, Iter 74, disc loss: 0.0004334287780139618, policy loss: 11.021512660755675
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.1660],
        [1.7063],
        [0.0294]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0389, 0.2188, 1.3775, 0.0270, 0.0151, 4.4958]],

        [[0.0389, 0.2188, 1.3775, 0.0270, 0.0151, 4.4958]],

        [[0.0389, 0.2188, 1.3775, 0.0270, 0.0151, 4.4958]],

        [[0.0389, 0.2188, 1.3775, 0.0270, 0.0151, 4.4958]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.6638, 6.8252, 0.1177], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.6638, 6.8252, 0.1177])
N: 510
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2041.0000, 2041.0000, 2041.0000, 2041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.249
Iter 2/2000 - Loss: 3.328
Iter 3/2000 - Loss: 3.091
Iter 4/2000 - Loss: 3.028
Iter 5/2000 - Loss: 2.980
Iter 6/2000 - Loss: 2.833
Iter 7/2000 - Loss: 2.652
Iter 8/2000 - Loss: 2.489
Iter 9/2000 - Loss: 2.339
Iter 10/2000 - Loss: 2.171
Iter 11/2000 - Loss: 1.968
Iter 12/2000 - Loss: 1.735
Iter 13/2000 - Loss: 1.487
Iter 14/2000 - Loss: 1.232
Iter 15/2000 - Loss: 0.971
Iter 16/2000 - Loss: 0.701
Iter 17/2000 - Loss: 0.417
Iter 18/2000 - Loss: 0.119
Iter 19/2000 - Loss: -0.189
Iter 20/2000 - Loss: -0.506
Iter 1981/2000 - Loss: -8.511
Iter 1982/2000 - Loss: -8.511
Iter 1983/2000 - Loss: -8.511
Iter 1984/2000 - Loss: -8.511
Iter 1985/2000 - Loss: -8.511
Iter 1986/2000 - Loss: -8.511
Iter 1987/2000 - Loss: -8.511
Iter 1988/2000 - Loss: -8.511
Iter 1989/2000 - Loss: -8.511
Iter 1990/2000 - Loss: -8.511
Iter 1991/2000 - Loss: -8.511
Iter 1992/2000 - Loss: -8.511
Iter 1993/2000 - Loss: -8.511
Iter 1994/2000 - Loss: -8.511
Iter 1995/2000 - Loss: -8.511
Iter 1996/2000 - Loss: -8.511
Iter 1997/2000 - Loss: -8.511
Iter 1998/2000 - Loss: -8.511
Iter 1999/2000 - Loss: -8.512
Iter 2000/2000 - Loss: -8.512
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.7773, 11.9828, 28.1021,  7.0492,  4.8540, 52.4891]],

        [[19.9257, 27.9955,  8.5480,  1.4767,  1.5652, 24.1132]],

        [[20.3306, 31.8763,  9.0931,  1.0965,  1.0015, 20.8416]],

        [[17.6646, 27.2611, 14.4051,  1.6851,  2.0621, 39.4546]]])
Signal Variance: tensor([ 0.1967,  1.7959, 11.2763,  0.3969])
Estimated target variance: tensor([0.0200, 0.6638, 6.8252, 0.1177])
N: 510
Signal to noise ratio: tensor([25.3420, 74.3203, 74.9469, 38.7836])
Bound on condition number: tensor([ 327531.9621, 2816989.5138, 2864687.2641,  767124.9677])
Policy Optimizer learning rate:
0.009658285256870233
Experience 34, Iter 0, disc loss: 0.0004409881568267139, policy loss: 10.27386630582729
Experience 34, Iter 1, disc loss: 0.00044508897593534545, policy loss: 10.17938106389147
Experience 34, Iter 2, disc loss: 0.00043406991486553753, policy loss: 10.747425362960575
Experience 34, Iter 3, disc loss: 0.00043879472895853215, policy loss: 10.264060661248548
Experience 34, Iter 4, disc loss: 0.0004370183881829243, policy loss: 10.313714297831746
Experience 34, Iter 5, disc loss: 0.00043315448705585295, policy loss: 10.522961230535847
Experience 34, Iter 6, disc loss: 0.00042968118028443805, policy loss: 10.839377503127986
Experience 34, Iter 7, disc loss: 0.00042416235965601586, policy loss: 11.014122691322573
Experience 34, Iter 8, disc loss: 0.00041833457668274813, policy loss: 11.394380155135877
Experience 34, Iter 9, disc loss: 0.00042218355935121966, policy loss: 10.63231448648581
Experience 34, Iter 10, disc loss: 0.00043009332424912456, policy loss: 10.474700513654323
Experience 34, Iter 11, disc loss: 0.0004250607521583237, policy loss: 10.706368608460881
Experience 34, Iter 12, disc loss: 0.0004102159312273333, policy loss: 11.271645658360661
Experience 34, Iter 13, disc loss: 0.0004070301741038768, policy loss: 11.519577965843332
Experience 34, Iter 14, disc loss: 0.0004280653096171248, policy loss: 10.40402126428766
Experience 34, Iter 15, disc loss: 0.0004178060509487458, policy loss: 10.565054721200728
Experience 34, Iter 16, disc loss: 0.0004051337134915803, policy loss: 12.03113567042823
Experience 34, Iter 17, disc loss: 0.0030181359728708584, policy loss: 11.636498316111084
Experience 34, Iter 18, disc loss: 0.00039891064443345103, policy loss: 12.324626799622063
Experience 34, Iter 19, disc loss: 0.0004007428776378205, policy loss: 11.934329064717325
Experience 34, Iter 20, disc loss: 0.0004076681944509172, policy loss: 11.025578369144624
Experience 34, Iter 21, disc loss: 0.00041002136344215023, policy loss: 11.229998011806853
Experience 34, Iter 22, disc loss: 0.0004134482593818249, policy loss: 10.963745007497195
Experience 34, Iter 23, disc loss: 0.0004149583925846925, policy loss: 10.764769551917215
Experience 34, Iter 24, disc loss: 0.00041290693865263044, policy loss: 10.943277718831352
Experience 34, Iter 25, disc loss: 0.00041906713213969254, policy loss: 10.445249513916387
Experience 34, Iter 26, disc loss: 0.00041384002382304896, policy loss: 10.608945733804731
Experience 34, Iter 27, disc loss: 0.0004067986464717581, policy loss: 10.936470832218358
Experience 34, Iter 28, disc loss: 0.00039800194326418, policy loss: 11.387485694616451
Experience 34, Iter 29, disc loss: 0.0718962767330257, policy loss: 11.499165231557619
Experience 34, Iter 30, disc loss: 0.0004000536485519737, policy loss: 11.764484733265856
Experience 34, Iter 31, disc loss: 0.0006813014412402385, policy loss: 11.964357479837446
Experience 34, Iter 32, disc loss: 0.00039962288276315904, policy loss: 11.91582421307324
Experience 34, Iter 33, disc loss: 0.00040294802939731627, policy loss: 11.880454394441003
Experience 34, Iter 34, disc loss: 0.0004005803114645091, policy loss: 11.98679999254848
Experience 34, Iter 35, disc loss: 0.0004038370573384597, policy loss: 11.730242876105638
Experience 34, Iter 36, disc loss: 0.00040959308720739927, policy loss: 11.388527483027413
Experience 34, Iter 37, disc loss: 0.0004065326689059903, policy loss: 11.487234953854845
Experience 34, Iter 38, disc loss: 0.00040980792616968304, policy loss: 11.278062522745483
Experience 34, Iter 39, disc loss: 0.00041330686363193545, policy loss: 10.983447954326877
Experience 34, Iter 40, disc loss: 0.0004134735156817769, policy loss: 11.062520920605582
Experience 34, Iter 41, disc loss: 0.0004149894608824046, policy loss: 11.03081544236275
Experience 34, Iter 42, disc loss: 0.0004147444882528069, policy loss: 11.048313230809356
Experience 34, Iter 43, disc loss: 0.000417783744779667, policy loss: 11.055822023354725
Experience 34, Iter 44, disc loss: 0.00042403254240654836, policy loss: 10.71553447119142
Experience 34, Iter 45, disc loss: 0.00041896257688830513, policy loss: 11.047642648426683
Experience 34, Iter 46, disc loss: 0.0004191795114509752, policy loss: 11.01107510983396
Experience 34, Iter 47, disc loss: 0.00042703223629531354, policy loss: 10.71294922302582
Experience 34, Iter 48, disc loss: 0.00043020990099018657, policy loss: 10.56719192800018
Experience 34, Iter 49, disc loss: 0.0004241718146250477, policy loss: 10.67957542924446
Experience 34, Iter 50, disc loss: 0.0004298638292642622, policy loss: 10.60420278782115
Experience 34, Iter 51, disc loss: 0.0004241274140703466, policy loss: 10.83016552507011
Experience 34, Iter 52, disc loss: 0.0004192306613032842, policy loss: 10.994211351885212
Experience 34, Iter 53, disc loss: 0.0004150101645438733, policy loss: 11.113812869569518
Experience 34, Iter 54, disc loss: 0.0004126659308143778, policy loss: 11.095044670824544
Experience 34, Iter 55, disc loss: 0.00041529769272961013, policy loss: 10.869929622446639
Experience 34, Iter 56, disc loss: 0.00042132689206996875, policy loss: 10.7057450085862
Experience 34, Iter 57, disc loss: 0.0004120970018140967, policy loss: 11.14064674720833
Experience 34, Iter 58, disc loss: 0.0004176633723487675, policy loss: 10.783884644238498
Experience 34, Iter 59, disc loss: 0.0004140215989862789, policy loss: 11.042708710622101
Experience 34, Iter 60, disc loss: 0.0004135291935478137, policy loss: 11.049078147262403
Experience 34, Iter 61, disc loss: 0.0004196106036163037, policy loss: 10.580804965928436
Experience 34, Iter 62, disc loss: 0.0004262567435100954, policy loss: 10.347302709408124
Experience 34, Iter 63, disc loss: 0.00042095008845306536, policy loss: 10.480934590163232
Experience 34, Iter 64, disc loss: 0.0004213474409612972, policy loss: 10.504291874653063
Experience 34, Iter 65, disc loss: 0.00041977249341752944, policy loss: 10.48372572690312
Experience 34, Iter 66, disc loss: 0.00042119545229566996, policy loss: 10.504831242489722
Experience 34, Iter 67, disc loss: 0.00042285343282411054, policy loss: 10.396574682359699
Experience 34, Iter 68, disc loss: 0.0004233823531963615, policy loss: 10.54426588580633
Experience 34, Iter 69, disc loss: 0.0004244061781717375, policy loss: 10.364381230058754
Experience 34, Iter 70, disc loss: 0.00041517892821948466, policy loss: 10.621309873052677
Experience 34, Iter 71, disc loss: 0.00041610125427934996, policy loss: 10.613558696496856
Experience 34, Iter 72, disc loss: 0.00041207602485724494, policy loss: 11.06466488834219
Experience 34, Iter 73, disc loss: 0.0004087625566600484, policy loss: 11.012127203514012
Experience 34, Iter 74, disc loss: 0.00041258746603637915, policy loss: 10.715885264329684
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.1686],
        [1.7334],
        [0.0297]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0379, 0.2174, 1.3913, 0.0271, 0.0149, 4.5480]],

        [[0.0379, 0.2174, 1.3913, 0.0271, 0.0149, 4.5480]],

        [[0.0379, 0.2174, 1.3913, 0.0271, 0.0149, 4.5480]],

        [[0.0379, 0.2174, 1.3913, 0.0271, 0.0149, 4.5480]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0198, 0.6746, 6.9337, 0.1187], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0198, 0.6746, 6.9337, 0.1187])
N: 525
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2101.0000, 2101.0000, 2101.0000, 2101.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.259
Iter 2/2000 - Loss: 3.334
Iter 3/2000 - Loss: 3.092
Iter 4/2000 - Loss: 3.024
Iter 5/2000 - Loss: 2.970
Iter 6/2000 - Loss: 2.817
Iter 7/2000 - Loss: 2.630
Iter 8/2000 - Loss: 2.462
Iter 9/2000 - Loss: 2.307
Iter 10/2000 - Loss: 2.135
Iter 11/2000 - Loss: 1.929
Iter 12/2000 - Loss: 1.693
Iter 13/2000 - Loss: 1.442
Iter 14/2000 - Loss: 1.184
Iter 15/2000 - Loss: 0.921
Iter 16/2000 - Loss: 0.649
Iter 17/2000 - Loss: 0.364
Iter 18/2000 - Loss: 0.065
Iter 19/2000 - Loss: -0.244
Iter 20/2000 - Loss: -0.560
Iter 1981/2000 - Loss: -8.516
Iter 1982/2000 - Loss: -8.516
Iter 1983/2000 - Loss: -8.516
Iter 1984/2000 - Loss: -8.516
Iter 1985/2000 - Loss: -8.516
Iter 1986/2000 - Loss: -8.516
Iter 1987/2000 - Loss: -8.517
Iter 1988/2000 - Loss: -8.517
Iter 1989/2000 - Loss: -8.517
Iter 1990/2000 - Loss: -8.517
Iter 1991/2000 - Loss: -8.517
Iter 1992/2000 - Loss: -8.517
Iter 1993/2000 - Loss: -8.517
Iter 1994/2000 - Loss: -8.517
Iter 1995/2000 - Loss: -8.517
Iter 1996/2000 - Loss: -8.517
Iter 1997/2000 - Loss: -8.517
Iter 1998/2000 - Loss: -8.517
Iter 1999/2000 - Loss: -8.517
Iter 2000/2000 - Loss: -8.517
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.5143, 12.1379, 28.6301,  7.0721,  4.8228, 52.5013]],

        [[19.7273, 27.8437,  8.5502,  1.4526,  1.5932, 24.7094]],

        [[20.0294, 31.8221,  9.1230,  1.0889,  1.0014, 20.6479]],

        [[17.3351, 27.2427, 14.5727,  1.6974,  2.0952, 38.9408]]])
Signal Variance: tensor([ 0.1992,  1.8257, 11.1401,  0.3977])
Estimated target variance: tensor([0.0198, 0.6746, 6.9337, 0.1187])
N: 525
Signal to noise ratio: tensor([25.5324, 73.5086, 74.5756, 38.5103])
Bound on condition number: tensor([ 342250.3329, 2836846.9540, 2919801.1399,  778598.2034])
Policy Optimizer learning rate:
0.009648114596590806
Experience 35, Iter 0, disc loss: 0.000413044301007377, policy loss: 10.5449642051324
Experience 35, Iter 1, disc loss: 0.00041391392978894746, policy loss: 10.79958224811451
Experience 35, Iter 2, disc loss: 0.0004133467422393519, policy loss: 10.596195939874235
Experience 35, Iter 3, disc loss: 0.00041991044642218124, policy loss: 10.345186113034657
Experience 35, Iter 4, disc loss: 0.0004199814029297475, policy loss: 10.367201501949738
Experience 35, Iter 5, disc loss: 0.0004214590279224614, policy loss: 10.328799907187936
Experience 35, Iter 6, disc loss: 0.000417699526309917, policy loss: 10.417114933503887
Experience 35, Iter 7, disc loss: 0.0004121487532502588, policy loss: 10.508367691343096
Experience 35, Iter 8, disc loss: 0.00041564325738115913, policy loss: 10.39631981556166
Experience 35, Iter 9, disc loss: 0.00041358146119060225, policy loss: 10.633073589876183
Experience 35, Iter 10, disc loss: 0.00040702805363123886, policy loss: 10.755229212681598
Experience 35, Iter 11, disc loss: 0.00040981365748467334, policy loss: 10.55997206771167
Experience 35, Iter 12, disc loss: 0.0004013803578121203, policy loss: 10.689473312674153
Experience 35, Iter 13, disc loss: 0.0004069785154434502, policy loss: 10.61824062776816
Experience 35, Iter 14, disc loss: 0.00041376609312230683, policy loss: 10.407126171102433
Experience 35, Iter 15, disc loss: 0.00041320203406479015, policy loss: 10.37449926204809
Experience 35, Iter 16, disc loss: 0.0004086935837928513, policy loss: 10.511740519589653
Experience 35, Iter 17, disc loss: 0.0004101240880572472, policy loss: 10.662591302490723
Experience 35, Iter 18, disc loss: 0.00040585283122100127, policy loss: 10.798266866286255
Experience 35, Iter 19, disc loss: 0.0004017795565750142, policy loss: 10.688673206867213
Experience 35, Iter 20, disc loss: 0.00040821454857564206, policy loss: 10.599219413738961
Experience 35, Iter 21, disc loss: 0.0004021281609338575, policy loss: 10.680299773168784
Experience 35, Iter 22, disc loss: 0.00040197029245300385, policy loss: 10.675975101247545
Experience 35, Iter 23, disc loss: 0.00041089025171939995, policy loss: 10.272480056009577
Experience 35, Iter 24, disc loss: 0.00040716272424679673, policy loss: 10.297346725368271
Experience 35, Iter 25, disc loss: 0.00040189999939250825, policy loss: 10.548768440069967
Experience 35, Iter 26, disc loss: 0.0004063545581145824, policy loss: 10.324990042897165
Experience 35, Iter 27, disc loss: 0.0004005284116424462, policy loss: 10.570749813387364
Experience 35, Iter 28, disc loss: 0.00040590153510892816, policy loss: 10.282521138174555
Experience 35, Iter 29, disc loss: 0.00040148021524843514, policy loss: 10.579155379132482
Experience 35, Iter 30, disc loss: 0.000408334875699862, policy loss: 10.333659436509118
Experience 35, Iter 31, disc loss: 0.00039836089461084667, policy loss: 10.76377490486797
Experience 35, Iter 32, disc loss: 0.00039945598001458495, policy loss: 10.593085725812358
Experience 35, Iter 33, disc loss: 0.0004025811693985736, policy loss: 10.416962537820066
Experience 35, Iter 34, disc loss: 0.00039834986020854355, policy loss: 10.434895316057707
Experience 35, Iter 35, disc loss: 0.0004010039417071986, policy loss: 10.337458358749608
Experience 35, Iter 36, disc loss: 0.00040822910788203433, policy loss: 10.181500275082431
Experience 35, Iter 37, disc loss: 0.00040430320841194293, policy loss: 10.259694503413485
Experience 35, Iter 38, disc loss: 0.0003966478491428812, policy loss: 10.47633896478402
Experience 35, Iter 39, disc loss: 0.00038862860924943136, policy loss: 10.797511336119104
Experience 35, Iter 40, disc loss: 0.000393939310203633, policy loss: 10.432229013338855
Experience 35, Iter 41, disc loss: 0.0003977005761560831, policy loss: 10.516005592489865
Experience 35, Iter 42, disc loss: 0.00039621966154711294, policy loss: 10.372364222083768
Experience 35, Iter 43, disc loss: 0.00040530954377273774, policy loss: 10.140237482901522
Experience 35, Iter 44, disc loss: 0.0003936562415428843, policy loss: 10.664141378780728
Experience 35, Iter 45, disc loss: 0.000391918018324957, policy loss: 10.567054020032401
Experience 35, Iter 46, disc loss: 0.0003892724467144613, policy loss: 10.52967772164531
Experience 35, Iter 47, disc loss: 0.0003962707379232492, policy loss: 10.297242342841885
Experience 35, Iter 48, disc loss: 0.0003924175166828923, policy loss: 10.557542913830364
Experience 35, Iter 49, disc loss: 0.0003998722448477312, policy loss: 10.178125836434488
Experience 35, Iter 50, disc loss: 0.0003978486803872549, policy loss: 10.255933875231694
Experience 35, Iter 51, disc loss: 0.0003893856202090242, policy loss: 10.560858970654241
Experience 35, Iter 52, disc loss: 0.0003944252257105473, policy loss: 10.41516249115189
Experience 35, Iter 53, disc loss: 0.00038876004012717, policy loss: 10.623116003112575
Experience 35, Iter 54, disc loss: 0.00039272102833757324, policy loss: 10.45333261538146
Experience 35, Iter 55, disc loss: 0.0003894033579674771, policy loss: 10.765377672886945
Experience 35, Iter 56, disc loss: 0.0003915613955482508, policy loss: 10.48537109490102
Experience 35, Iter 57, disc loss: 0.0003973399706039291, policy loss: 10.155081538982984
Experience 35, Iter 58, disc loss: 0.00039033109607359743, policy loss: 10.308444986340701
Experience 35, Iter 59, disc loss: 0.00038153127685113593, policy loss: 10.899903444161321
Experience 35, Iter 60, disc loss: 0.0003867353789551819, policy loss: 10.72157716988167
Experience 35, Iter 61, disc loss: 0.00039147837703348284, policy loss: 10.264161036177121
Experience 35, Iter 62, disc loss: 0.00039369093612506796, policy loss: 10.228932137304703
Experience 35, Iter 63, disc loss: 0.0003882706639617026, policy loss: 10.348267947283423
Experience 35, Iter 64, disc loss: 0.0003812217110446765, policy loss: 10.507469393127177
Experience 35, Iter 65, disc loss: 0.000381922085037174, policy loss: 10.565446536089606
Experience 35, Iter 66, disc loss: 0.00038388329010936986, policy loss: 10.47881469073786
Experience 35, Iter 67, disc loss: 0.00038873403794811905, policy loss: 10.325947920446705
Experience 35, Iter 68, disc loss: 0.0003936909810477744, policy loss: 10.161148449506413
Experience 35, Iter 69, disc loss: 0.0003917597711171878, policy loss: 10.161741198944043
Experience 35, Iter 70, disc loss: 0.0003939455603486164, policy loss: 10.118337536174865
Experience 35, Iter 71, disc loss: 0.0003785223599519061, policy loss: 10.507534325530504
Experience 35, Iter 72, disc loss: 0.0003761505357525661, policy loss: 10.680773728180057
Experience 35, Iter 73, disc loss: 0.00038127966953267083, policy loss: 10.506445942293483
Experience 35, Iter 74, disc loss: 0.00038366258468206156, policy loss: 10.541594413864505
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1709],
        [1.7570],
        [0.0299]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0370, 0.2156, 1.4010, 0.0272, 0.0148, 4.5843]],

        [[0.0370, 0.2156, 1.4010, 0.0272, 0.0148, 4.5843]],

        [[0.0370, 0.2156, 1.4010, 0.0272, 0.0148, 4.5843]],

        [[0.0370, 0.2156, 1.4010, 0.0272, 0.0148, 4.5843]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0196, 0.6838, 7.0280, 0.1194], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0196, 0.6838, 7.0280, 0.1194])
N: 540
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2161.0000, 2161.0000, 2161.0000, 2161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.265
Iter 2/2000 - Loss: 3.341
Iter 3/2000 - Loss: 3.094
Iter 4/2000 - Loss: 3.024
Iter 5/2000 - Loss: 2.968
Iter 6/2000 - Loss: 2.812
Iter 7/2000 - Loss: 2.623
Iter 8/2000 - Loss: 2.452
Iter 9/2000 - Loss: 2.295
Iter 10/2000 - Loss: 2.122
Iter 11/2000 - Loss: 1.914
Iter 12/2000 - Loss: 1.676
Iter 13/2000 - Loss: 1.422
Iter 14/2000 - Loss: 1.162
Iter 15/2000 - Loss: 0.897
Iter 16/2000 - Loss: 0.623
Iter 17/2000 - Loss: 0.336
Iter 18/2000 - Loss: 0.037
Iter 19/2000 - Loss: -0.272
Iter 20/2000 - Loss: -0.588
Iter 1981/2000 - Loss: -8.544
Iter 1982/2000 - Loss: -8.544
Iter 1983/2000 - Loss: -8.544
Iter 1984/2000 - Loss: -8.545
Iter 1985/2000 - Loss: -8.545
Iter 1986/2000 - Loss: -8.545
Iter 1987/2000 - Loss: -8.545
Iter 1988/2000 - Loss: -8.545
Iter 1989/2000 - Loss: -8.545
Iter 1990/2000 - Loss: -8.545
Iter 1991/2000 - Loss: -8.545
Iter 1992/2000 - Loss: -8.545
Iter 1993/2000 - Loss: -8.545
Iter 1994/2000 - Loss: -8.545
Iter 1995/2000 - Loss: -8.545
Iter 1996/2000 - Loss: -8.545
Iter 1997/2000 - Loss: -8.545
Iter 1998/2000 - Loss: -8.545
Iter 1999/2000 - Loss: -8.545
Iter 2000/2000 - Loss: -8.545
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.4060, 12.0562, 28.6055,  6.9793,  4.6303, 52.5150]],

        [[19.4666, 27.1566,  8.5378,  1.4758,  1.5665, 23.9113]],

        [[19.9293, 31.6132,  9.0336,  1.0898,  1.0035, 20.4312]],

        [[17.1185, 27.0221, 14.6770,  1.6839,  2.1111, 38.9348]]])
Signal Variance: tensor([ 0.1962,  1.7770, 10.8773,  0.4031])
Estimated target variance: tensor([0.0196, 0.6838, 7.0280, 0.1194])
N: 540
Signal to noise ratio: tensor([25.3525, 72.7462, 73.7124, 38.9192])
Bound on condition number: tensor([ 347086.1461, 2857683.1492, 2934101.3127,  817939.2765])
Policy Optimizer learning rate:
0.009637954646528333
Experience 36, Iter 0, disc loss: 0.00038151046553216474, policy loss: 10.401563463348007
Experience 36, Iter 1, disc loss: 0.00037731285902510147, policy loss: 10.47257370114568
Experience 36, Iter 2, disc loss: 0.00037149756613307833, policy loss: 10.681686339792686
Experience 36, Iter 3, disc loss: 0.00040544232995368514, policy loss: 10.502874483480483
Experience 36, Iter 4, disc loss: 0.0003365311621895961, policy loss: 19.607486443074798
Experience 36, Iter 5, disc loss: 0.00033508555134964893, policy loss: 24.540917744120954
Experience 36, Iter 6, disc loss: 0.0003345633414267827, policy loss: 25.261169942014114
Experience 36, Iter 7, disc loss: 0.0003342031001823398, policy loss: 22.032076984586514
Experience 36, Iter 8, disc loss: 0.0004079936595327037, policy loss: 12.610114794313695
Experience 36, Iter 9, disc loss: 0.0009982029215552034, policy loss: 9.046290746741956
Experience 36, Iter 10, disc loss: 0.0010838005781916094, policy loss: 8.521722972382701
Experience 36, Iter 11, disc loss: 0.0005917565854811663, policy loss: 10.616438092002031
Experience 36, Iter 12, disc loss: 0.0003538227301769682, policy loss: 12.545016682523471
Experience 36, Iter 13, disc loss: 0.0012701328452750416, policy loss: 11.241961588544255
Experience 36, Iter 14, disc loss: 0.0017925360380441758, policy loss: 17.21519861441996
Experience 36, Iter 15, disc loss: 0.009581352132139169, policy loss: 11.2995276831374
Experience 36, Iter 16, disc loss: 0.0005808918567576, policy loss: 12.461497507662427
Experience 36, Iter 17, disc loss: 0.0003471101802205296, policy loss: 13.361315729097656
Experience 36, Iter 18, disc loss: 0.000369133350061643, policy loss: 12.85319253284132
Experience 36, Iter 19, disc loss: 0.000425377237644693, policy loss: 14.38904666283132
Experience 36, Iter 20, disc loss: 0.000485525075286008, policy loss: 19.56596191605334
Experience 36, Iter 21, disc loss: 0.00034105163786919267, policy loss: 23.161021594424234
Experience 36, Iter 22, disc loss: 0.0003397936582178319, policy loss: 20.88926035777465
Experience 36, Iter 23, disc loss: 0.00035024999406699556, policy loss: 19.859589170634056
Experience 36, Iter 24, disc loss: 0.000654909400968809, policy loss: 13.727355632576632
Experience 36, Iter 25, disc loss: 0.0005354089469260437, policy loss: 11.322077210787022
Experience 36, Iter 26, disc loss: 0.0018319928461182637, policy loss: 9.688223358889246
Experience 36, Iter 27, disc loss: 0.006279466580972143, policy loss: 6.733499326425544
Experience 36, Iter 28, disc loss: 0.00785263170952319, policy loss: 6.749329636547565
Experience 36, Iter 29, disc loss: 0.02299499139717913, policy loss: 14.319386336521816
Experience 36, Iter 30, disc loss: 0.0003372956400181244, policy loss: 22.08101654414436
Experience 36, Iter 31, disc loss: 0.0003395720679590404, policy loss: 33.12063793943633
Experience 36, Iter 32, disc loss: 0.00034158953279503324, policy loss: 31.257913282677798
Experience 36, Iter 33, disc loss: 0.0019490974200764974, policy loss: 25.031429427498594
Experience 36, Iter 34, disc loss: 0.0546981740008352, policy loss: 16.27477161866821
Experience 36, Iter 35, disc loss: 0.02958237323194032, policy loss: 12.186853970834347
Experience 36, Iter 36, disc loss: 0.02957762575811139, policy loss: 10.42757199895847
Experience 36, Iter 37, disc loss: 0.051319061766723455, policy loss: 12.748826111311171
Experience 36, Iter 38, disc loss: 0.016510131840972192, policy loss: 15.065425638591174
Experience 36, Iter 39, disc loss: 0.03892379526203446, policy loss: 15.947594727354291
Experience 36, Iter 40, disc loss: 0.03398728940766865, policy loss: 15.819472401852696
Experience 36, Iter 41, disc loss: 0.03442194061613728, policy loss: 16.388321353934415
Experience 36, Iter 42, disc loss: 0.005793754377386002, policy loss: 17.038393496235454
Experience 36, Iter 43, disc loss: 0.010466375966847527, policy loss: 16.637367763857903
Experience 36, Iter 44, disc loss: 0.002781423800237008, policy loss: 19.43167564211566
Experience 36, Iter 45, disc loss: 0.01727273196807647, policy loss: 18.16900391372638
Experience 36, Iter 46, disc loss: 0.005821294721343218, policy loss: 21.114614542961295
Experience 36, Iter 47, disc loss: 0.00357460657719193, policy loss: 18.884623550566346
Experience 36, Iter 48, disc loss: 0.011063918272648757, policy loss: 18.749616702812386
Experience 36, Iter 49, disc loss: 0.0022253047753997388, policy loss: 19.703026591575153
Experience 36, Iter 50, disc loss: 0.010849948672536639, policy loss: 18.90231337092514
Experience 36, Iter 51, disc loss: 0.006931730334243655, policy loss: 19.5914659924479
Experience 36, Iter 52, disc loss: 0.0037555124231153543, policy loss: 20.47880493711947
Experience 36, Iter 53, disc loss: 0.0038039733575307246, policy loss: 19.481405614325055
Experience 36, Iter 54, disc loss: 0.0016173078878515834, policy loss: 20.545640616033765
Experience 36, Iter 55, disc loss: 0.0026427372132354562, policy loss: 18.991636464320848
Experience 36, Iter 56, disc loss: 0.0013294015236428743, policy loss: 18.094365918347407
Experience 36, Iter 57, disc loss: 0.0037148951091941582, policy loss: 20.49186303124513
Experience 36, Iter 58, disc loss: 0.004556307444511602, policy loss: 17.64428506942742
Experience 36, Iter 59, disc loss: 0.0051148690261098115, policy loss: 17.587587723649328
Experience 36, Iter 60, disc loss: 0.002986044430955047, policy loss: 19.69447428045696
Experience 36, Iter 61, disc loss: 0.0026936757942171937, policy loss: 18.8790019740298
Experience 36, Iter 62, disc loss: 0.0022279632070890216, policy loss: 17.82591997677676
Experience 36, Iter 63, disc loss: 0.0032763910484019058, policy loss: 18.878411036938402
Experience 36, Iter 64, disc loss: 0.002275060059715917, policy loss: 20.887572407329852
Experience 36, Iter 65, disc loss: 0.002108642557037722, policy loss: 20.60590282181644
Experience 36, Iter 66, disc loss: 0.0013620951857518178, policy loss: 21.5266751771862
Experience 36, Iter 67, disc loss: 0.0015637180408713406, policy loss: 19.105171737555327
Experience 36, Iter 68, disc loss: 0.006546986168895167, policy loss: 18.354092805125937
Experience 36, Iter 69, disc loss: 0.002220561524950808, policy loss: 19.809496055725177
Experience 36, Iter 70, disc loss: 0.002227139538969195, policy loss: 19.873716340136422
Experience 36, Iter 71, disc loss: 0.00432878883552321, policy loss: 17.193876759621546
Experience 36, Iter 72, disc loss: 0.0011694934540026846, policy loss: 19.71776863144789
Experience 36, Iter 73, disc loss: 0.0023352430409672533, policy loss: 18.34691700208683
Experience 36, Iter 74, disc loss: 0.003164213053567328, policy loss: 16.41466521191217
Experience: 37
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1734],
        [1.8003],
        [0.0305]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0413, 0.2246, 1.4347, 0.0272, 0.0148, 4.5959]],

        [[0.0413, 0.2246, 1.4347, 0.0272, 0.0148, 4.5959]],

        [[0.0413, 0.2246, 1.4347, 0.0272, 0.0148, 4.5959]],

        [[0.0413, 0.2246, 1.4347, 0.0272, 0.0148, 4.5959]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0204, 0.6936, 7.2011, 0.1221], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0204, 0.6936, 7.2011, 0.1221])
N: 555
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2221.0000, 2221.0000, 2221.0000, 2221.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.298
Iter 2/2000 - Loss: 3.339
Iter 3/2000 - Loss: 3.114
Iter 4/2000 - Loss: 3.036
Iter 5/2000 - Loss: 2.966
Iter 6/2000 - Loss: 2.807
Iter 7/2000 - Loss: 2.625
Iter 8/2000 - Loss: 2.460
Iter 9/2000 - Loss: 2.303
Iter 10/2000 - Loss: 2.123
Iter 11/2000 - Loss: 1.909
Iter 12/2000 - Loss: 1.672
Iter 13/2000 - Loss: 1.424
Iter 14/2000 - Loss: 1.172
Iter 15/2000 - Loss: 0.913
Iter 16/2000 - Loss: 0.642
Iter 17/2000 - Loss: 0.359
Iter 18/2000 - Loss: 0.064
Iter 19/2000 - Loss: -0.240
Iter 20/2000 - Loss: -0.549
Iter 1981/2000 - Loss: -8.541
Iter 1982/2000 - Loss: -8.541
Iter 1983/2000 - Loss: -8.541
Iter 1984/2000 - Loss: -8.541
Iter 1985/2000 - Loss: -8.541
Iter 1986/2000 - Loss: -8.541
Iter 1987/2000 - Loss: -8.541
Iter 1988/2000 - Loss: -8.541
Iter 1989/2000 - Loss: -8.541
Iter 1990/2000 - Loss: -8.541
Iter 1991/2000 - Loss: -8.542
Iter 1992/2000 - Loss: -8.542
Iter 1993/2000 - Loss: -8.542
Iter 1994/2000 - Loss: -8.542
Iter 1995/2000 - Loss: -8.542
Iter 1996/2000 - Loss: -8.542
Iter 1997/2000 - Loss: -8.542
Iter 1998/2000 - Loss: -8.542
Iter 1999/2000 - Loss: -8.542
Iter 2000/2000 - Loss: -8.542
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.2344, 12.2892, 28.5373,  6.7350,  4.8380, 52.8568]],

        [[20.0216, 28.0073,  8.2146,  1.5623,  1.4161, 22.0229]],

        [[21.1021, 31.8905,  8.8802,  1.0962,  1.0040, 21.0550]],

        [[18.0606, 26.8702, 14.4338,  1.7627,  2.1263, 38.0753]]])
Signal Variance: tensor([ 0.1983,  1.5396, 11.0255,  0.3964])
Estimated target variance: tensor([0.0204, 0.6936, 7.2011, 0.1221])
N: 555
Signal to noise ratio: tensor([25.5486, 68.1195, 73.9595, 38.4328])
Bound on condition number: tensor([ 362266.1702, 2575345.6712, 3035853.2364,  819780.7462])
Policy Optimizer learning rate:
0.00962780539540442
Experience 37, Iter 0, disc loss: 0.0018542349741042514, policy loss: 18.588891091448893
Experience 37, Iter 1, disc loss: 0.005796041992509474, policy loss: 17.591813363589935
Experience 37, Iter 2, disc loss: 0.004457761095576131, policy loss: 17.268802671955147
Experience 37, Iter 3, disc loss: 0.0039807375494689025, policy loss: 17.759170840312354
Experience 37, Iter 4, disc loss: 0.0037751795280067503, policy loss: 17.184243343589998
Experience 37, Iter 5, disc loss: 0.0024632514798432655, policy loss: 18.169773369757472
Experience 37, Iter 6, disc loss: 0.0070746059224336445, policy loss: 15.131318806069036
Experience 37, Iter 7, disc loss: 0.002774680883923848, policy loss: 15.558229926032539
Experience 37, Iter 8, disc loss: 0.0019533497480487784, policy loss: 14.865732258520557
Experience 37, Iter 9, disc loss: 0.0030777135083715056, policy loss: 15.895027218747334
Experience 37, Iter 10, disc loss: 0.002246532335654551, policy loss: 15.375491313295953
Experience 37, Iter 11, disc loss: 0.002775259743614365, policy loss: 13.546333807243052
Experience 37, Iter 12, disc loss: 0.0025961058588899228, policy loss: 13.133316650585186
Experience 37, Iter 13, disc loss: 0.0038021585598184363, policy loss: 13.49113938920199
Experience 37, Iter 14, disc loss: 0.009067936762735786, policy loss: 13.026901161850098
Experience 37, Iter 15, disc loss: 0.009746900590460409, policy loss: 12.961077206264932
Experience 37, Iter 16, disc loss: 0.001720292944973016, policy loss: 14.70274984448374
Experience 37, Iter 17, disc loss: 0.007229089108722578, policy loss: 13.721376978615396
Experience 37, Iter 18, disc loss: 0.001672508520590562, policy loss: 13.989823929746692
Experience 37, Iter 19, disc loss: 0.0087339669288867, policy loss: 12.82340190096196
Experience 37, Iter 20, disc loss: 0.0021455612588968374, policy loss: 14.367977340730949
Experience 37, Iter 21, disc loss: 0.0025261155146600557, policy loss: 13.89740268968934
Experience 37, Iter 22, disc loss: 0.001536855246504181, policy loss: 12.90468552879963
Experience 37, Iter 23, disc loss: 0.0036032168889800686, policy loss: 14.220100362079034
Experience 37, Iter 24, disc loss: 0.006021865099612278, policy loss: 13.37167892731538
Experience 37, Iter 25, disc loss: 0.0015700379581694522, policy loss: 14.196674775254273
Experience 37, Iter 26, disc loss: 0.0016453283895026575, policy loss: 13.482002812710519
Experience 37, Iter 27, disc loss: 0.0020110863946801337, policy loss: 13.276069386054251
Experience 37, Iter 28, disc loss: 0.002074514286728358, policy loss: 14.360413539060763
Experience 37, Iter 29, disc loss: 0.001796349634944388, policy loss: 14.758795584094557
Experience 37, Iter 30, disc loss: 0.0036180709206892732, policy loss: 13.457641260873604
Experience 37, Iter 31, disc loss: 0.0049703933879051716, policy loss: 12.527327746498202
Experience 37, Iter 32, disc loss: 0.004443072637440342, policy loss: 13.03422040258227
Experience 37, Iter 33, disc loss: 0.0033684345590047553, policy loss: 12.68838993120294
Experience 37, Iter 34, disc loss: 0.0035315406761075785, policy loss: 13.342232256662417
Experience 37, Iter 35, disc loss: 0.0023066461046509163, policy loss: 10.773704359255504
Experience 37, Iter 36, disc loss: 0.006288921650652739, policy loss: 11.85588699011321
Experience 37, Iter 37, disc loss: 0.0022994659520545685, policy loss: 12.679842314502798
Experience 37, Iter 38, disc loss: 0.002090494083591873, policy loss: 12.744536388640128
Experience 37, Iter 39, disc loss: 0.002479417934301658, policy loss: 13.130926232135561
Experience 37, Iter 40, disc loss: 0.004701415458228013, policy loss: 13.75594014308091
Experience 37, Iter 41, disc loss: 0.0019582881295427145, policy loss: 13.287470990277935
Experience 37, Iter 42, disc loss: 0.0019226943786595078, policy loss: 13.065407422879321
Experience 37, Iter 43, disc loss: 0.001875056192498132, policy loss: 12.637948162745795
Experience 37, Iter 44, disc loss: 0.002222604145334442, policy loss: 12.753002525288403
Experience 37, Iter 45, disc loss: 0.0021918047289494853, policy loss: 12.57371230683046
Experience 37, Iter 46, disc loss: 0.001985904703313306, policy loss: 12.502411263730094
Experience 37, Iter 47, disc loss: 0.003921330617671984, policy loss: 12.904090089448037
Experience 37, Iter 48, disc loss: 0.0037437633849955985, policy loss: 11.401333899949266
Experience 37, Iter 49, disc loss: 0.001620856040294227, policy loss: 13.197947974114852
Experience 37, Iter 50, disc loss: 0.0019512575608474046, policy loss: 11.032753645051375
Experience 37, Iter 51, disc loss: 0.0017348246916593195, policy loss: 13.793220114051694
Experience 37, Iter 52, disc loss: 0.002146733380349042, policy loss: 12.441925965360305
Experience 37, Iter 53, disc loss: 0.0032961833908924276, policy loss: 11.564253120361347
Experience 37, Iter 54, disc loss: 0.0028695461556023007, policy loss: 13.048896056734684
Experience 37, Iter 55, disc loss: 0.002645478884776813, policy loss: 11.38079173141792
Experience 37, Iter 56, disc loss: 0.00229030476919062, policy loss: 12.290610624079335
Experience 37, Iter 57, disc loss: 0.0021405176670292246, policy loss: 13.988787224170814
Experience 37, Iter 58, disc loss: 0.0024399040628050477, policy loss: 12.518704000541401
Experience 37, Iter 59, disc loss: 0.002523103219529009, policy loss: 13.555216248057533
Experience 37, Iter 60, disc loss: 0.0017331912165027752, policy loss: 14.607266263038035
Experience 37, Iter 61, disc loss: 0.0016675406277434524, policy loss: 17.24258606629367
Experience 37, Iter 62, disc loss: 0.004185198892009341, policy loss: 12.9812566631978
Experience 37, Iter 63, disc loss: 0.001697019090105222, policy loss: 13.531144174767103
Experience 37, Iter 64, disc loss: 0.0015107335011550198, policy loss: 13.232638517041316
Experience 37, Iter 65, disc loss: 0.0018201441125435956, policy loss: 12.726308023638143
Experience 37, Iter 66, disc loss: 0.0015623269212343546, policy loss: 14.291105433625653
Experience 37, Iter 67, disc loss: 0.001530938190826187, policy loss: 13.428435368530764
Experience 37, Iter 68, disc loss: 0.001178946615990863, policy loss: 14.19458336762479
Experience 37, Iter 69, disc loss: 0.001130186690603346, policy loss: 17.373058626143397
Experience 37, Iter 70, disc loss: 0.0011117808932715656, policy loss: 17.502447403145766
Experience 37, Iter 71, disc loss: 0.0011028739331353234, policy loss: 20.16254411552156
Experience 37, Iter 72, disc loss: 0.0010905822599106224, policy loss: 22.659099977841365
Experience 37, Iter 73, disc loss: 0.0010827303952043348, policy loss: 21.025082200320604
Experience 37, Iter 74, disc loss: 0.001075043325733518, policy loss: 21.951021073090136
Experience: 38
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1800],
        [1.8627],
        [0.0301]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0439, 0.2263, 1.4231, 0.0267, 0.0146, 4.6651]],

        [[0.0439, 0.2263, 1.4231, 0.0267, 0.0146, 4.6651]],

        [[0.0439, 0.2263, 1.4231, 0.0267, 0.0146, 4.6651]],

        [[0.0439, 0.2263, 1.4231, 0.0267, 0.0146, 4.6651]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.7199, 7.4509, 0.1202], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.7199, 7.4509, 0.1202])
N: 570
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2281.0000, 2281.0000, 2281.0000, 2281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.327
Iter 2/2000 - Loss: 3.354
Iter 3/2000 - Loss: 3.141
Iter 4/2000 - Loss: 3.061
Iter 5/2000 - Loss: 2.989
Iter 6/2000 - Loss: 2.833
Iter 7/2000 - Loss: 2.658
Iter 8/2000 - Loss: 2.501
Iter 9/2000 - Loss: 2.347
Iter 10/2000 - Loss: 2.167
Iter 11/2000 - Loss: 1.955
Iter 12/2000 - Loss: 1.723
Iter 13/2000 - Loss: 1.485
Iter 14/2000 - Loss: 1.241
Iter 15/2000 - Loss: 0.988
Iter 16/2000 - Loss: 0.723
Iter 17/2000 - Loss: 0.444
Iter 18/2000 - Loss: 0.154
Iter 19/2000 - Loss: -0.145
Iter 20/2000 - Loss: -0.448
Iter 1981/2000 - Loss: -8.540
Iter 1982/2000 - Loss: -8.540
Iter 1983/2000 - Loss: -8.540
Iter 1984/2000 - Loss: -8.540
Iter 1985/2000 - Loss: -8.540
Iter 1986/2000 - Loss: -8.540
Iter 1987/2000 - Loss: -8.540
Iter 1988/2000 - Loss: -8.540
Iter 1989/2000 - Loss: -8.540
Iter 1990/2000 - Loss: -8.540
Iter 1991/2000 - Loss: -8.540
Iter 1992/2000 - Loss: -8.540
Iter 1993/2000 - Loss: -8.540
Iter 1994/2000 - Loss: -8.540
Iter 1995/2000 - Loss: -8.540
Iter 1996/2000 - Loss: -8.540
Iter 1997/2000 - Loss: -8.540
Iter 1998/2000 - Loss: -8.540
Iter 1999/2000 - Loss: -8.540
Iter 2000/2000 - Loss: -8.540
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.5882, 11.1485, 28.1216,  6.7191,  4.8776, 53.0596]],

        [[20.5982, 29.1286,  8.3364,  1.4038,  1.3521, 21.0753]],

        [[21.7468, 33.2852,  8.9567,  1.0030,  0.9538, 21.7665]],

        [[18.3588, 26.7818, 15.3748,  1.7643,  2.1255, 39.5625]]])
Signal Variance: tensor([ 0.1793,  1.4261, 10.9696,  0.4249])
Estimated target variance: tensor([0.0205, 0.7199, 7.4509, 0.1202])
N: 570
Signal to noise ratio: tensor([24.4313, 65.0525, 73.4264, 39.8095])
Bound on condition number: tensor([ 340227.9512, 2412143.2859, 3073118.2754,  903336.5986])
Policy Optimizer learning rate:
0.009617666831952545
Experience 38, Iter 0, disc loss: 0.00106666124564103, policy loss: 20.990954516062285
Experience 38, Iter 1, disc loss: 0.0010583351873166477, policy loss: 21.3727059565504
Experience 38, Iter 2, disc loss: 0.001050251269393915, policy loss: 19.475982292650006
Experience 38, Iter 3, disc loss: 0.0010418460375693141, policy loss: 19.338715337563645
Experience 38, Iter 4, disc loss: 0.001033868725273715, policy loss: 17.81755224440286
Experience 38, Iter 5, disc loss: 0.0010250474033898693, policy loss: 20.253143354686593
Experience 38, Iter 6, disc loss: 0.0010168406877624104, policy loss: 19.31047774101266
Experience 38, Iter 7, disc loss: 0.0010089752013934812, policy loss: 17.013493269060355
Experience 38, Iter 8, disc loss: 0.0010011488783815514, policy loss: 16.851629812673515
Experience 38, Iter 9, disc loss: 0.0009932740781274888, policy loss: 15.224465534526571
Experience 38, Iter 10, disc loss: 0.0009851568864206587, policy loss: 15.535941645286686
Experience 38, Iter 11, disc loss: 0.0009768475244275965, policy loss: 15.111390246517605
Experience 38, Iter 12, disc loss: 0.0009689980949551586, policy loss: 14.826351749024685
Experience 38, Iter 13, disc loss: 0.0009619031131242087, policy loss: 14.562994569556238
Experience 38, Iter 14, disc loss: 0.0009541978701654595, policy loss: 14.319234660993692
Experience 38, Iter 15, disc loss: 0.0009465842789790617, policy loss: 14.50094749240932
Experience 38, Iter 16, disc loss: 0.0009388808111769669, policy loss: 14.581600768618404
Experience 38, Iter 17, disc loss: 0.0009322001250849824, policy loss: 14.246433899515544
Experience 38, Iter 18, disc loss: 0.0009244661054994054, policy loss: 14.418001487718087
Experience 38, Iter 19, disc loss: 0.0009178265617940141, policy loss: 14.412396025106233
Experience 38, Iter 20, disc loss: 0.0009105753426593305, policy loss: 14.78169378432278
Experience 38, Iter 21, disc loss: 0.0009032456003457093, policy loss: 14.623415874388611
Experience 38, Iter 22, disc loss: 0.0008971174450248808, policy loss: 14.233502381419228
Experience 38, Iter 23, disc loss: 0.0008897873980677463, policy loss: 14.834111763840692
Experience 38, Iter 24, disc loss: 0.0008832920678272887, policy loss: 15.087819069381329
Experience 38, Iter 25, disc loss: 0.0008770231567477194, policy loss: 14.896306755075189
Experience 38, Iter 26, disc loss: 0.0008707319868657357, policy loss: 14.940247169602838
Experience 38, Iter 27, disc loss: 0.00086512030289296, policy loss: 14.670262188974931
Experience 38, Iter 28, disc loss: 0.0008581613038515221, policy loss: 15.237221528073569
Experience 38, Iter 29, disc loss: 0.000852335807241054, policy loss: 14.6683912697032
Experience 38, Iter 30, disc loss: 0.0008465711562379389, policy loss: 14.691150539177842
Experience 38, Iter 31, disc loss: 0.0008406274515529816, policy loss: 15.180967408745166
Experience 38, Iter 32, disc loss: 0.0008350814595941603, policy loss: 15.08176364219181
Experience 38, Iter 33, disc loss: 0.00082942603938191, policy loss: 14.866874524026764
Experience 38, Iter 34, disc loss: 0.0008239576467614822, policy loss: 14.830070227814637
Experience 38, Iter 35, disc loss: 0.0008184117969140129, policy loss: 15.09858629355625
Experience 38, Iter 36, disc loss: 0.0008132193303954487, policy loss: 14.958443238855537
Experience 38, Iter 37, disc loss: 0.000808046914944871, policy loss: 15.0419911352718
Experience 38, Iter 38, disc loss: 0.0008029346597129071, policy loss: 14.884025677924086
Experience 38, Iter 39, disc loss: 0.0007977842777478812, policy loss: 14.91011852569397
Experience 38, Iter 40, disc loss: 0.0007927511385820201, policy loss: 14.99299075399928
Experience 38, Iter 41, disc loss: 0.0007880022781028737, policy loss: 14.856178919089901
Experience 38, Iter 42, disc loss: 0.0007829318689991182, policy loss: 15.32189650841843
Experience 38, Iter 43, disc loss: 0.0007783868466800592, policy loss: 15.262522257726484
Experience 38, Iter 44, disc loss: 0.0007734323695770126, policy loss: 15.412715035734
Experience 38, Iter 45, disc loss: 0.0007689536964357788, policy loss: 15.231590145638156
Experience 38, Iter 46, disc loss: 0.0007645248178655999, policy loss: 15.206912368068716
Experience 38, Iter 47, disc loss: 0.0007600439406025573, policy loss: 15.136683501910099
Experience 38, Iter 48, disc loss: 0.0007562914011965734, policy loss: 14.328940007036996
Experience 38, Iter 49, disc loss: 0.0007533337398295508, policy loss: 13.492078888775957
Experience 38, Iter 50, disc loss: 0.0007504393049870516, policy loss: 13.603074577346131
Experience 38, Iter 51, disc loss: 0.0007465453828067698, policy loss: 13.261132841335343
Experience 38, Iter 52, disc loss: 0.000741784375607574, policy loss: 13.68959013760323
Experience 38, Iter 53, disc loss: 0.0007399226978527934, policy loss: 12.971471389430057
Experience 38, Iter 54, disc loss: 0.0007385801963310638, policy loss: 12.928308581406508
Experience 38, Iter 55, disc loss: 0.0007418804983607318, policy loss: 12.183038928622832
Experience 38, Iter 56, disc loss: 0.0007392781234048147, policy loss: 11.848971702780933
Experience 38, Iter 57, disc loss: 0.0007456035793229585, policy loss: 11.742222296230093
Experience 38, Iter 58, disc loss: 0.0007322688641161245, policy loss: 12.343420688180904
Experience 38, Iter 59, disc loss: 0.0007328093716911916, policy loss: 12.219282750923846
Experience 38, Iter 60, disc loss: 0.0007509368840229212, policy loss: 11.828274731419567
Experience 38, Iter 61, disc loss: 0.0015125533665060625, policy loss: 12.179714857223667
Experience 38, Iter 62, disc loss: 0.0008115687081273969, policy loss: 11.676910473400607
Experience 38, Iter 63, disc loss: 0.0007949439258221094, policy loss: 11.944722810619112
Experience 38, Iter 64, disc loss: 0.0007327137388267888, policy loss: 11.87296886458617
Experience 38, Iter 65, disc loss: 0.0007433083522713553, policy loss: 12.927263855463993
Experience 38, Iter 66, disc loss: 0.0007028124769639598, policy loss: 12.823838384507251
Experience 38, Iter 67, disc loss: 0.0007191251153567591, policy loss: 13.173036412880244
Experience 38, Iter 68, disc loss: 0.0007312391795256763, policy loss: 11.731003207999454
Experience 38, Iter 69, disc loss: 0.0007281641407969807, policy loss: 12.901241041705639
Experience 38, Iter 70, disc loss: 0.0006918108339075643, policy loss: 12.907747406523075
Experience 38, Iter 71, disc loss: 0.0008190325458454912, policy loss: 12.82440674180329
Experience 38, Iter 72, disc loss: 0.0007314840421829499, policy loss: 13.276853908315966
Experience 38, Iter 73, disc loss: 0.0006884632123261148, policy loss: 12.68037719970834
Experience 38, Iter 74, disc loss: 0.0007705108007485858, policy loss: 12.974690792506046
Experience: 39
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1841],
        [1.8951],
        [0.0297]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0432, 0.2264, 1.4134, 0.0269, 0.0142, 4.7325]],

        [[0.0432, 0.2264, 1.4134, 0.0269, 0.0142, 4.7325]],

        [[0.0432, 0.2264, 1.4134, 0.0269, 0.0142, 4.7325]],

        [[0.0432, 0.2264, 1.4134, 0.0269, 0.0142, 4.7325]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.7364, 7.5805, 0.1188], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.7364, 7.5805, 0.1188])
N: 585
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2341.0000, 2341.0000, 2341.0000, 2341.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.346
Iter 2/2000 - Loss: 3.385
Iter 3/2000 - Loss: 3.168
Iter 4/2000 - Loss: 3.092
Iter 5/2000 - Loss: 3.026
Iter 6/2000 - Loss: 2.873
Iter 7/2000 - Loss: 2.699
Iter 8/2000 - Loss: 2.545
Iter 9/2000 - Loss: 2.397
Iter 10/2000 - Loss: 2.223
Iter 11/2000 - Loss: 2.015
Iter 12/2000 - Loss: 1.787
Iter 13/2000 - Loss: 1.551
Iter 14/2000 - Loss: 1.311
Iter 15/2000 - Loss: 1.062
Iter 16/2000 - Loss: 0.800
Iter 17/2000 - Loss: 0.524
Iter 18/2000 - Loss: 0.236
Iter 19/2000 - Loss: -0.062
Iter 20/2000 - Loss: -0.365
Iter 1981/2000 - Loss: -8.559
Iter 1982/2000 - Loss: -8.559
Iter 1983/2000 - Loss: -8.559
Iter 1984/2000 - Loss: -8.559
Iter 1985/2000 - Loss: -8.559
Iter 1986/2000 - Loss: -8.560
Iter 1987/2000 - Loss: -8.560
Iter 1988/2000 - Loss: -8.560
Iter 1989/2000 - Loss: -8.560
Iter 1990/2000 - Loss: -8.560
Iter 1991/2000 - Loss: -8.560
Iter 1992/2000 - Loss: -8.560
Iter 1993/2000 - Loss: -8.560
Iter 1994/2000 - Loss: -8.560
Iter 1995/2000 - Loss: -8.560
Iter 1996/2000 - Loss: -8.560
Iter 1997/2000 - Loss: -8.560
Iter 1998/2000 - Loss: -8.560
Iter 1999/2000 - Loss: -8.560
Iter 2000/2000 - Loss: -8.560
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.3132, 11.3116, 27.4318,  5.8097,  4.6178, 53.2737]],

        [[20.7399, 29.7841,  8.2532,  1.2995,  1.3672, 20.7321]],

        [[21.9931, 34.3362,  8.8759,  1.0105,  0.9631, 21.9418]],

        [[18.3744, 27.3246, 15.5399,  1.7800,  2.0797, 40.0500]]])
Signal Variance: tensor([ 0.1780,  1.3834, 11.0479,  0.4260])
Estimated target variance: tensor([0.0205, 0.7364, 7.5805, 0.1188])
N: 585
Signal to noise ratio: tensor([24.3863, 64.6532, 74.1375, 40.0392])
Bound on condition number: tensor([ 347895.5271, 2445325.3976, 3215372.8418,  937835.8197])
Policy Optimizer learning rate:
0.00960753894491805
Experience 39, Iter 0, disc loss: 0.000703874672887581, policy loss: 12.195703333637812
Experience 39, Iter 1, disc loss: 0.0007310833059837438, policy loss: 11.955491395270382
Experience 39, Iter 2, disc loss: 0.0009408711845519303, policy loss: 12.573886603877314
Experience 39, Iter 3, disc loss: 0.0009577678903150656, policy loss: 12.195210054019912
Experience 39, Iter 4, disc loss: 0.0006786464991052337, policy loss: 11.886826911889473
Experience 39, Iter 5, disc loss: 0.0007368122281954562, policy loss: 11.534556100097515
Experience 39, Iter 6, disc loss: 0.0007103558277837506, policy loss: 11.521828600376589
Experience 39, Iter 7, disc loss: 0.0007193536774403354, policy loss: 11.720342819704886
Experience 39, Iter 8, disc loss: 0.0006908526413722034, policy loss: 11.776798181305216
Experience 39, Iter 9, disc loss: 0.0010525303260543162, policy loss: 11.23611748876328
Experience 39, Iter 10, disc loss: 0.0007496771435000431, policy loss: 11.448200926275906
Experience 39, Iter 11, disc loss: 0.0007397883418810197, policy loss: 11.496389576683512
Experience 39, Iter 12, disc loss: 0.000688046444967732, policy loss: 11.236634999388729
Experience 39, Iter 13, disc loss: 0.0007104255101957149, policy loss: 11.416396948266957
Experience 39, Iter 14, disc loss: 0.00092409469944174, policy loss: 11.384358422822896
Experience 39, Iter 15, disc loss: 0.0007555197441193872, policy loss: 11.039123259630854
Experience 39, Iter 16, disc loss: 0.0009259284184389071, policy loss: 10.852810046707914
Experience 39, Iter 17, disc loss: 0.000767601602536827, policy loss: 10.664787086873448
Experience 39, Iter 18, disc loss: 0.0007011148266264529, policy loss: 10.529949638066217
Experience 39, Iter 19, disc loss: 0.0007719329416503512, policy loss: 10.962155552919358
Experience 39, Iter 20, disc loss: 0.0009305899807261199, policy loss: 10.515430634858937
Experience 39, Iter 21, disc loss: 0.0007814857135771504, policy loss: 10.246691054509297
Experience 39, Iter 22, disc loss: 0.0011045265173681897, policy loss: 11.439188146292398
Experience 39, Iter 23, disc loss: 0.0010952000867108722, policy loss: 10.488262103424738
Experience 39, Iter 24, disc loss: 0.011237161965274661, policy loss: 10.148486915389254
Experience 39, Iter 25, disc loss: 0.001191861599165301, policy loss: 10.266664637506734
Experience 39, Iter 26, disc loss: 0.000920001000141655, policy loss: 10.797243498318544
Experience 39, Iter 27, disc loss: 0.0007482933895534209, policy loss: 11.143979485530314
Experience 39, Iter 28, disc loss: 0.001980916405731503, policy loss: 10.888852646013646
Experience 39, Iter 29, disc loss: 0.001604245211089121, policy loss: 10.210930479865127
Experience 39, Iter 30, disc loss: 0.0009718688748516571, policy loss: 10.426792029963405
Experience 39, Iter 31, disc loss: 0.018152422909105365, policy loss: 10.517166870368207
Experience 39, Iter 32, disc loss: 0.003924452079203372, policy loss: 10.998252447470739
Experience 39, Iter 33, disc loss: 0.0025126865561290682, policy loss: 10.466344910927397
Experience 39, Iter 34, disc loss: 0.0008496464916683678, policy loss: 10.324771294312054
Experience 39, Iter 35, disc loss: 0.002895876068847193, policy loss: 10.21196778154988
Experience 39, Iter 36, disc loss: 0.0017995180397019405, policy loss: 10.593432204863456
Experience 39, Iter 37, disc loss: 0.0010122925453711204, policy loss: 11.184153493489605
Experience 39, Iter 38, disc loss: 0.0009192591919402442, policy loss: 10.881537540064631
Experience 39, Iter 39, disc loss: 0.0010772245572495769, policy loss: 11.321245297511986
Experience 39, Iter 40, disc loss: 0.0013664468396437572, policy loss: 10.832831333414783
Experience 39, Iter 41, disc loss: 0.0010006669923290465, policy loss: 10.831214863869478
Experience 39, Iter 42, disc loss: 0.001303061109404481, policy loss: 10.938246233096555
Experience 39, Iter 43, disc loss: 0.0011014914309959187, policy loss: 10.683290672795529
Experience 39, Iter 44, disc loss: 0.001034234305950766, policy loss: 10.711841230897022
Experience 39, Iter 45, disc loss: 0.004093232070826033, policy loss: 10.136868947620147
Experience 39, Iter 46, disc loss: 0.0012732248763405174, policy loss: 11.230531760946992
Experience 39, Iter 47, disc loss: 0.0009607434392699533, policy loss: 10.949246413153896
Experience 39, Iter 48, disc loss: 0.0074023052194927, policy loss: 10.753319347185064
Experience 39, Iter 49, disc loss: 0.0023056901005492018, policy loss: 11.111313279384778
Experience 39, Iter 50, disc loss: 0.004166780568206469, policy loss: 11.595622701814687
Experience 39, Iter 51, disc loss: 0.0046954119186978175, policy loss: 11.458467977694914
Experience 39, Iter 52, disc loss: 0.00158154925845871, policy loss: 11.09739082507453
Experience 39, Iter 53, disc loss: 0.01331935168184262, policy loss: 10.28458445334767
Experience 39, Iter 54, disc loss: 0.002776230095814513, policy loss: 11.208823213086923
Experience 39, Iter 55, disc loss: 0.0044423374414205895, policy loss: 11.890239372071365
Experience 39, Iter 56, disc loss: 0.012182498116078565, policy loss: 11.555642790263892
Experience 39, Iter 57, disc loss: 0.009213627893203806, policy loss: 11.282318902844104
Experience 39, Iter 58, disc loss: 0.005969588872961943, policy loss: 11.484244331443875
Experience 39, Iter 59, disc loss: 0.004283682890916176, policy loss: 11.049640751250529
Experience 39, Iter 60, disc loss: 0.003110564621613571, policy loss: 10.590538333708125
Experience 39, Iter 61, disc loss: 0.006482716982947435, policy loss: 10.676495149031549
Experience 39, Iter 62, disc loss: 0.0018264855265385557, policy loss: 10.99344001680081
Experience 39, Iter 63, disc loss: 0.004663526160891665, policy loss: 11.04945363728433
Experience 39, Iter 64, disc loss: 0.0016173886425386434, policy loss: 11.259741767794122
Experience 39, Iter 65, disc loss: 0.0026502421231822636, policy loss: 13.213105396736202
Experience 39, Iter 66, disc loss: 0.0010469623467087082, policy loss: 12.761545443903728
Experience 39, Iter 67, disc loss: 0.0009866787338256812, policy loss: 14.601013049102598
Experience 39, Iter 68, disc loss: 0.007055991005647875, policy loss: 13.985838044343486
Experience 39, Iter 69, disc loss: 0.0011298382056868183, policy loss: 15.33657142731133
Experience 39, Iter 70, disc loss: 0.000993256728357732, policy loss: 13.152525765883366
Experience 39, Iter 71, disc loss: 0.0010277411051750403, policy loss: 12.623494025352645
Experience 39, Iter 72, disc loss: 0.0011055920734037623, policy loss: 11.87086601117067
Experience 39, Iter 73, disc loss: 0.0011190436304359657, policy loss: 12.079193353619623
Experience 39, Iter 74, disc loss: 0.0018177373790766514, policy loss: 11.435948468813727
Experience: 40
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1880],
        [1.9290],
        [0.0295]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0424, 0.2270, 1.3992, 0.0270, 0.0139, 4.7881]],

        [[0.0424, 0.2270, 1.3992, 0.0270, 0.0139, 4.7881]],

        [[0.0424, 0.2270, 1.3992, 0.0270, 0.0139, 4.7881]],

        [[0.0424, 0.2270, 1.3992, 0.0270, 0.0139, 4.7881]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0204, 0.7521, 7.7161, 0.1179], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0204, 0.7521, 7.7161, 0.1179])
N: 600
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2401.0000, 2401.0000, 2401.0000, 2401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.368
Iter 2/2000 - Loss: 3.419
Iter 3/2000 - Loss: 3.197
Iter 4/2000 - Loss: 3.126
Iter 5/2000 - Loss: 3.067
Iter 6/2000 - Loss: 2.917
Iter 7/2000 - Loss: 2.743
Iter 8/2000 - Loss: 2.589
Iter 9/2000 - Loss: 2.445
Iter 10/2000 - Loss: 2.277
Iter 11/2000 - Loss: 2.074
Iter 12/2000 - Loss: 1.847
Iter 13/2000 - Loss: 1.612
Iter 14/2000 - Loss: 1.372
Iter 15/2000 - Loss: 1.125
Iter 16/2000 - Loss: 0.866
Iter 17/2000 - Loss: 0.591
Iter 18/2000 - Loss: 0.303
Iter 19/2000 - Loss: 0.004
Iter 20/2000 - Loss: -0.302
Iter 1981/2000 - Loss: -8.562
Iter 1982/2000 - Loss: -8.562
Iter 1983/2000 - Loss: -8.562
Iter 1984/2000 - Loss: -8.562
Iter 1985/2000 - Loss: -8.563
Iter 1986/2000 - Loss: -8.563
Iter 1987/2000 - Loss: -8.563
Iter 1988/2000 - Loss: -8.563
Iter 1989/2000 - Loss: -8.563
Iter 1990/2000 - Loss: -8.563
Iter 1991/2000 - Loss: -8.563
Iter 1992/2000 - Loss: -8.563
Iter 1993/2000 - Loss: -8.563
Iter 1994/2000 - Loss: -8.563
Iter 1995/2000 - Loss: -8.563
Iter 1996/2000 - Loss: -8.563
Iter 1997/2000 - Loss: -8.563
Iter 1998/2000 - Loss: -8.563
Iter 1999/2000 - Loss: -8.563
Iter 2000/2000 - Loss: -8.563
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.3420, 11.6616, 27.6792,  5.5812,  4.3571, 54.3783]],

        [[20.6864, 30.3694,  8.1883,  1.3034,  1.3861, 21.3433]],

        [[21.9693, 34.6808,  8.8693,  0.9670,  0.9407, 21.5118]],

        [[18.2503, 27.4989, 15.2482,  1.8036,  2.0058, 40.6926]]])
Signal Variance: tensor([ 0.1807,  1.3904, 10.3375,  0.4193])
Estimated target variance: tensor([0.0204, 0.7521, 7.7161, 0.1179])
N: 600
Signal to noise ratio: tensor([24.5406, 64.2108, 71.7502, 39.5701])
Bound on condition number: tensor([ 361346.1364, 2473818.2826, 3088854.2183,  939477.8492])
Policy Optimizer learning rate:
0.009597421723058133
Experience 40, Iter 0, disc loss: 0.0019561646911357236, policy loss: 11.699641308090834
Experience 40, Iter 1, disc loss: 0.001975828125425444, policy loss: 11.590517664647912
Experience 40, Iter 2, disc loss: 0.001039453432704074, policy loss: 12.16721024559072
Experience 40, Iter 3, disc loss: 0.0016813941778378136, policy loss: 12.205732273919256
Experience 40, Iter 4, disc loss: 0.0011726386356955662, policy loss: 11.943304324447258
Experience 40, Iter 5, disc loss: 0.0022940274032768907, policy loss: 12.640181757808977
Experience 40, Iter 6, disc loss: 0.0018299738383379786, policy loss: 13.16873166126116
Experience 40, Iter 7, disc loss: 0.0019704694668764653, policy loss: 13.025673620899394
Experience 40, Iter 8, disc loss: 0.007280785616891467, policy loss: 13.270130895402005
Experience 40, Iter 9, disc loss: 0.004368667546644766, policy loss: 13.977184226483324
Experience 40, Iter 10, disc loss: 0.0010760149885970843, policy loss: 15.16055625041226
Experience 40, Iter 11, disc loss: 0.0016483990298918525, policy loss: 14.423030188706473
Experience 40, Iter 12, disc loss: 0.005848521238383824, policy loss: 14.43918622589836
Experience 40, Iter 13, disc loss: 0.002054121311207961, policy loss: 15.622206853139833
Experience 40, Iter 14, disc loss: 0.0024916445163654585, policy loss: 17.67896135793611
Experience 40, Iter 15, disc loss: 0.0019423172149100137, policy loss: 21.89029660036541
Experience 40, Iter 16, disc loss: 0.0010095319661446907, policy loss: 24.722154023624235
Experience 40, Iter 17, disc loss: 0.0035069732062147155, policy loss: 30.305756552790122
Experience 40, Iter 18, disc loss: 0.0010169403327463676, policy loss: 28.43036221792258
Experience 40, Iter 19, disc loss: 0.0010216260231538574, policy loss: 25.79137290675113
Experience 40, Iter 20, disc loss: 0.0015032133846158552, policy loss: 24.502311832110696
Experience 40, Iter 21, disc loss: 0.0010326272147445168, policy loss: 19.998068079242636
Experience 40, Iter 22, disc loss: 0.004650098858365443, policy loss: 20.97277804986505
Experience 40, Iter 23, disc loss: 0.0010399265474764063, policy loss: 22.60414197456447
Experience 40, Iter 24, disc loss: 0.0011132340065728137, policy loss: 25.21361912701262
Experience 40, Iter 25, disc loss: 0.0010222469406782344, policy loss: 27.07726655110706
Experience 40, Iter 26, disc loss: 0.001022694600664848, policy loss: 22.98929086517812
Experience 40, Iter 27, disc loss: 0.009417009933577927, policy loss: 25.910577352655643
Experience 40, Iter 28, disc loss: 0.0012156673359255304, policy loss: 24.572394618842267
Experience 40, Iter 29, disc loss: 0.001022679705489537, policy loss: 25.70335693596069
Experience 40, Iter 30, disc loss: 0.0010262406428889473, policy loss: 27.929717770750084
Experience 40, Iter 31, disc loss: 0.0010232359016460358, policy loss: 32.124575954841546
Experience 40, Iter 32, disc loss: 0.0011202072691868408, policy loss: 27.41530654199203
Experience 40, Iter 33, disc loss: 0.0010775077264624246, policy loss: 18.771698118203332
Experience 40, Iter 34, disc loss: 0.0010188784492799281, policy loss: 18.50539203086663
Experience 40, Iter 35, disc loss: 0.001015393296498155, policy loss: 20.241493390355608
Experience 40, Iter 36, disc loss: 0.0010070941305211816, policy loss: 20.516069678448005
Experience 40, Iter 37, disc loss: 0.0010041506927780286, policy loss: 19.163135710401185
Experience 40, Iter 38, disc loss: 0.0010020566246951917, policy loss: 19.69592920352808
Experience 40, Iter 39, disc loss: 0.000998479385998798, policy loss: 15.64519929268196
Experience 40, Iter 40, disc loss: 0.0009867237326978743, policy loss: 16.7555054949811
Experience 40, Iter 41, disc loss: 0.0009760773665995382, policy loss: 20.019085016581556
Experience 40, Iter 42, disc loss: 0.0009686904505515668, policy loss: 20.452528788166042
Experience 40, Iter 43, disc loss: 0.0009612342783437974, policy loss: 21.770769281172104
Experience 40, Iter 44, disc loss: 0.0009536683602601625, policy loss: 25.881322774144486
Experience 40, Iter 45, disc loss: 0.0009459933407328865, policy loss: 29.430719836428928
Experience 40, Iter 46, disc loss: 0.0009382167088236224, policy loss: 28.53520909511594
Experience 40, Iter 47, disc loss: 0.0009303853595579264, policy loss: 28.472782829548102
Experience 40, Iter 48, disc loss: 0.0009225172873297826, policy loss: 27.850187404921066
Experience 40, Iter 49, disc loss: 0.0009146438359979591, policy loss: 27.816760189423835
Experience 40, Iter 50, disc loss: 0.0009067692453060603, policy loss: 28.304910852502964
Experience 40, Iter 51, disc loss: 0.0008989124586354397, policy loss: 27.58883012564755
Experience 40, Iter 52, disc loss: 0.0008910948068369107, policy loss: 26.45409388562424
Experience 40, Iter 53, disc loss: 0.0008833225085141467, policy loss: 26.395832465380064
Experience 40, Iter 54, disc loss: 0.0008756038522668163, policy loss: 26.355592368630266
Experience 40, Iter 55, disc loss: 0.0008679705064757272, policy loss: 26.656982235750483
Experience 40, Iter 56, disc loss: 0.0008604216191024221, policy loss: 25.80176065160059
Experience 40, Iter 57, disc loss: 0.0008532357240705015, policy loss: 24.115558886302438
Experience 40, Iter 58, disc loss: 0.0008458346514189195, policy loss: 23.986118091735428
Experience 40, Iter 59, disc loss: 0.0008396465750551489, policy loss: 24.339748187168766
Experience 40, Iter 60, disc loss: 0.0008325041223475822, policy loss: 24.599085993119154
Experience 40, Iter 61, disc loss: 0.0008251542703263591, policy loss: 23.966391079352153
Experience 40, Iter 62, disc loss: 0.0008244615502262997, policy loss: 22.522342865676865
Experience 40, Iter 63, disc loss: 0.0008140011619048708, policy loss: 22.769847362315417
Experience 40, Iter 64, disc loss: 0.0008051983867411554, policy loss: 22.160045713658086
Experience 40, Iter 65, disc loss: 0.0008006963160009968, policy loss: 20.357375082218745
Experience 40, Iter 66, disc loss: 0.0007975901096309147, policy loss: 20.518144531802996
Experience 40, Iter 67, disc loss: 0.0007910253712505009, policy loss: 18.338111674824923
Experience 40, Iter 68, disc loss: 0.0007827915211374601, policy loss: 20.285193475347242
Experience 40, Iter 69, disc loss: 0.0007811299752314473, policy loss: 18.662933267207578
Experience 40, Iter 70, disc loss: 0.0007809346302437155, policy loss: 20.013875990900303
Experience 40, Iter 71, disc loss: 0.000761975656014632, policy loss: 19.1127967273152
Experience 40, Iter 72, disc loss: 0.0007638731268900006, policy loss: 17.63771430862881
Experience 40, Iter 73, disc loss: 0.0007517213875282453, policy loss: 17.748273074695604
Experience 40, Iter 74, disc loss: 0.0007490076514117742, policy loss: 19.29596737637885
Experience: 41
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1870],
        [1.9101],
        [0.0289]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0416, 0.2246, 1.3731, 0.0264, 0.0137, 4.7604]],

        [[0.0416, 0.2246, 1.3731, 0.0264, 0.0137, 4.7604]],

        [[0.0416, 0.2246, 1.3731, 0.0264, 0.0137, 4.7604]],

        [[0.0416, 0.2246, 1.3731, 0.0264, 0.0137, 4.7604]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0203, 0.7480, 7.6402, 0.1155], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0203, 0.7480, 7.6402, 0.1155])
N: 615
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2461.0000, 2461.0000, 2461.0000, 2461.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.349
Iter 2/2000 - Loss: 3.396
Iter 3/2000 - Loss: 3.177
Iter 4/2000 - Loss: 3.105
Iter 5/2000 - Loss: 3.044
Iter 6/2000 - Loss: 2.894
Iter 7/2000 - Loss: 2.721
Iter 8/2000 - Loss: 2.568
Iter 9/2000 - Loss: 2.423
Iter 10/2000 - Loss: 2.253
Iter 11/2000 - Loss: 2.049
Iter 12/2000 - Loss: 1.823
Iter 13/2000 - Loss: 1.588
Iter 14/2000 - Loss: 1.348
Iter 15/2000 - Loss: 1.100
Iter 16/2000 - Loss: 0.839
Iter 17/2000 - Loss: 0.563
Iter 18/2000 - Loss: 0.273
Iter 19/2000 - Loss: -0.026
Iter 20/2000 - Loss: -0.332
Iter 1981/2000 - Loss: -8.580
Iter 1982/2000 - Loss: -8.580
Iter 1983/2000 - Loss: -8.580
Iter 1984/2000 - Loss: -8.580
Iter 1985/2000 - Loss: -8.581
Iter 1986/2000 - Loss: -8.581
Iter 1987/2000 - Loss: -8.581
Iter 1988/2000 - Loss: -8.581
Iter 1989/2000 - Loss: -8.581
Iter 1990/2000 - Loss: -8.581
Iter 1991/2000 - Loss: -8.581
Iter 1992/2000 - Loss: -8.581
Iter 1993/2000 - Loss: -8.581
Iter 1994/2000 - Loss: -8.581
Iter 1995/2000 - Loss: -8.581
Iter 1996/2000 - Loss: -8.581
Iter 1997/2000 - Loss: -8.581
Iter 1998/2000 - Loss: -8.581
Iter 1999/2000 - Loss: -8.581
Iter 2000/2000 - Loss: -8.581
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.3564, 12.1096, 26.9937,  5.5316,  4.4729, 54.5082]],

        [[20.5707, 30.1952,  8.3507,  1.3240,  1.3687, 21.7555]],

        [[21.5315, 34.3665,  9.0621,  0.9727,  0.9287, 21.3485]],

        [[18.0843, 27.3831, 15.2078,  1.8298,  2.0238, 40.8243]]])
Signal Variance: tensor([ 0.1863,  1.4359, 10.3541,  0.4222])
Estimated target variance: tensor([0.0203, 0.7480, 7.6402, 0.1155])
N: 615
Signal to noise ratio: tensor([25.0168, 64.8779, 71.3391, 39.7027])
Bound on condition number: tensor([ 384892.4298, 2588620.6634, 3129899.5545,  969428.4088])
Policy Optimizer learning rate:
0.009587315155141827
Experience 41, Iter 0, disc loss: 0.0007515489372109717, policy loss: 17.3482702309515
Experience 41, Iter 1, disc loss: 0.0007386588415746287, policy loss: 17.354680647804773
Experience 41, Iter 2, disc loss: 0.0007376402172524819, policy loss: 19.435611472556822
Experience 41, Iter 3, disc loss: 0.0007539614003393189, policy loss: 16.46363913142485
Experience 41, Iter 4, disc loss: 0.0007219440774111028, policy loss: 17.702109493525363
Experience 41, Iter 5, disc loss: 0.0007201879393507788, policy loss: 19.96918141409757
Experience 41, Iter 6, disc loss: 0.0007212226951363792, policy loss: 18.64456508601862
Experience 41, Iter 7, disc loss: 0.0007128894788656361, policy loss: 17.895605109778973
Experience 41, Iter 8, disc loss: 0.0007278022591870974, policy loss: 16.924335226118583
Experience 41, Iter 9, disc loss: 0.000704639628963746, policy loss: 17.48187168344475
Experience 41, Iter 10, disc loss: 0.000700148720866732, policy loss: 17.44634207004453
Experience 41, Iter 11, disc loss: 0.0006973407353011334, policy loss: 18.95735791972169
Experience 41, Iter 12, disc loss: 0.0006849727094277066, policy loss: 18.591520510124003
Experience 41, Iter 13, disc loss: 0.0006812542490826071, policy loss: 17.74030158714084
Experience 41, Iter 14, disc loss: 0.0006757904477174272, policy loss: 17.713726894502727
Experience 41, Iter 15, disc loss: 0.0006741326995251551, policy loss: 17.70829483615986
Experience 41, Iter 16, disc loss: 0.0006744463038766261, policy loss: 18.08786184343014
Experience 41, Iter 17, disc loss: 0.0006739057190515238, policy loss: 17.773793431492198
Experience 41, Iter 18, disc loss: 0.0006858002412677821, policy loss: 18.57934861141587
Experience 41, Iter 19, disc loss: 0.0006758511867322105, policy loss: 17.67509883063992
Experience 41, Iter 20, disc loss: 0.0006590274393178569, policy loss: 17.147522882127276
Experience 41, Iter 21, disc loss: 0.000660997151818165, policy loss: 16.498472770272564
Experience 41, Iter 22, disc loss: 0.0006492636523304415, policy loss: 17.137753953736073
Experience 41, Iter 23, disc loss: 0.0006430747720330507, policy loss: 17.320309014749725
Experience 41, Iter 24, disc loss: 0.0006403564492747362, policy loss: 17.27603541874606
Experience 41, Iter 25, disc loss: 0.0006335929027843642, policy loss: 17.924174856010747
Experience 41, Iter 26, disc loss: 0.00063111516641973, policy loss: 17.670641038820378
Experience 41, Iter 27, disc loss: 0.0006315488944118317, policy loss: 17.421734809189417
Experience 41, Iter 28, disc loss: 0.0006272625704840834, policy loss: 17.713161041250963
Experience 41, Iter 29, disc loss: 0.0006337268372309472, policy loss: 19.021425230145276
Experience 41, Iter 30, disc loss: 0.0006241441827458684, policy loss: 17.770121541025407
Experience 41, Iter 31, disc loss: 0.0006202680126786584, policy loss: 16.74976887735353
Experience 41, Iter 32, disc loss: 0.0006130933779549419, policy loss: 17.42616677888139
Experience 41, Iter 33, disc loss: 0.0006195365533450424, policy loss: 17.200540957657132
Experience 41, Iter 34, disc loss: 0.0006080538115447699, policy loss: 17.376789396877044
Experience 41, Iter 35, disc loss: 0.0006050271727465701, policy loss: 17.933177194860622
Experience 41, Iter 36, disc loss: 0.0006093194424116398, policy loss: 16.75814217882274
Experience 41, Iter 37, disc loss: 0.0006000984406807535, policy loss: 18.487925894027548
Experience 41, Iter 38, disc loss: 0.0006008188512325139, policy loss: 19.324920909864403
Experience 41, Iter 39, disc loss: 0.0005958627879908813, policy loss: 17.060418851696998
Experience 41, Iter 40, disc loss: 0.0005877529889765923, policy loss: 19.1944810132223
Experience 41, Iter 41, disc loss: 0.0005932634371373563, policy loss: 17.374925845962256
Experience 41, Iter 42, disc loss: 0.0005902134630312595, policy loss: 17.426252588699505
Experience 41, Iter 43, disc loss: 0.0005791231534817398, policy loss: 18.84106205512003
Experience 41, Iter 44, disc loss: 0.0005767789985107497, policy loss: 17.966621700914256
Experience 41, Iter 45, disc loss: 0.0005798598202059301, policy loss: 17.95851980992604
Experience 41, Iter 46, disc loss: 0.0005743150692062599, policy loss: 19.366070082759386
Experience 41, Iter 47, disc loss: 0.0005709086598116674, policy loss: 18.148197121571734
Experience 41, Iter 48, disc loss: 0.000573280574348171, policy loss: 17.646894934826356
Experience 41, Iter 49, disc loss: 0.0005659003547816292, policy loss: 18.662504236679005
Experience 41, Iter 50, disc loss: 0.0005702973215554662, policy loss: 18.691954202088958
Experience 41, Iter 51, disc loss: 0.0005785205404659299, policy loss: 19.644804746627518
Experience 41, Iter 52, disc loss: 0.0005571320532929958, policy loss: 19.430621926830106
Experience 41, Iter 53, disc loss: 0.0005995707755309264, policy loss: 16.59811132680917
Experience 41, Iter 54, disc loss: 0.0005551061091961603, policy loss: 17.65876803111838
Experience 41, Iter 55, disc loss: 0.0005562189750142777, policy loss: 19.45868882192725
Experience 41, Iter 56, disc loss: 0.0005502997018554881, policy loss: 17.521412297753066
Experience 41, Iter 57, disc loss: 0.000552428508158229, policy loss: 18.619970007204255
Experience 41, Iter 58, disc loss: 0.0005447585001798238, policy loss: 20.188379161455163
Experience 41, Iter 59, disc loss: 0.0005499549634188914, policy loss: 18.988796383418773
Experience 41, Iter 60, disc loss: 0.0005413351274245829, policy loss: 18.398479563664505
Experience 41, Iter 61, disc loss: 0.0005388565598693392, policy loss: 18.73518578767112
Experience 41, Iter 62, disc loss: 0.0005466044175108336, policy loss: 19.525769061967807
Experience 41, Iter 63, disc loss: 0.000533392976847131, policy loss: 19.53167924350986
Experience 41, Iter 64, disc loss: 0.0005334320935066969, policy loss: 18.526435430664645
Experience 41, Iter 65, disc loss: 0.0005310983419794549, policy loss: 20.245088232625836
Experience 41, Iter 66, disc loss: 0.0005295941914675063, policy loss: 19.713170797025775
Experience 41, Iter 67, disc loss: 0.0005239647896238128, policy loss: 19.701747278347014
Experience 41, Iter 68, disc loss: 0.0005212990301211229, policy loss: 19.027641658441773
Experience 41, Iter 69, disc loss: 0.0005243009711823751, policy loss: 17.9961213563157
Experience 41, Iter 70, disc loss: 0.0005205017689705753, policy loss: 19.412721788368195
Experience 41, Iter 71, disc loss: 0.000526485558311516, policy loss: 19.829181403756785
Experience 41, Iter 72, disc loss: 0.0005152448152494113, policy loss: 20.644693325635206
Experience 41, Iter 73, disc loss: 0.0005116253573067969, policy loss: 19.90497576614314
Experience 41, Iter 74, disc loss: 0.0005123241991907046, policy loss: 20.90754748443895
Experience: 42
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.1842],
        [1.8768],
        [0.0282]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0407, 0.2201, 1.3431, 0.0259, 0.0135, 4.6839]],

        [[0.0407, 0.2201, 1.3431, 0.0259, 0.0135, 4.6839]],

        [[0.0407, 0.2201, 1.3431, 0.0259, 0.0135, 4.6839]],

        [[0.0407, 0.2201, 1.3431, 0.0259, 0.0135, 4.6839]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.7367, 7.5072, 0.1130], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.7367, 7.5072, 0.1130])
N: 630
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2521.0000, 2521.0000, 2521.0000, 2521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.322
Iter 2/2000 - Loss: 3.373
Iter 3/2000 - Loss: 3.153
Iter 4/2000 - Loss: 3.082
Iter 5/2000 - Loss: 3.024
Iter 6/2000 - Loss: 2.874
Iter 7/2000 - Loss: 2.701
Iter 8/2000 - Loss: 2.549
Iter 9/2000 - Loss: 2.404
Iter 10/2000 - Loss: 2.235
Iter 11/2000 - Loss: 2.031
Iter 12/2000 - Loss: 1.805
Iter 13/2000 - Loss: 1.569
Iter 14/2000 - Loss: 1.328
Iter 15/2000 - Loss: 1.079
Iter 16/2000 - Loss: 0.817
Iter 17/2000 - Loss: 0.539
Iter 18/2000 - Loss: 0.249
Iter 19/2000 - Loss: -0.052
Iter 20/2000 - Loss: -0.359
Iter 1981/2000 - Loss: -8.557
Iter 1982/2000 - Loss: -8.557
Iter 1983/2000 - Loss: -8.557
Iter 1984/2000 - Loss: -8.557
Iter 1985/2000 - Loss: -8.557
Iter 1986/2000 - Loss: -8.557
Iter 1987/2000 - Loss: -8.557
Iter 1988/2000 - Loss: -8.557
Iter 1989/2000 - Loss: -8.557
Iter 1990/2000 - Loss: -8.557
Iter 1991/2000 - Loss: -8.557
Iter 1992/2000 - Loss: -8.557
Iter 1993/2000 - Loss: -8.557
Iter 1994/2000 - Loss: -8.557
Iter 1995/2000 - Loss: -8.557
Iter 1996/2000 - Loss: -8.557
Iter 1997/2000 - Loss: -8.557
Iter 1998/2000 - Loss: -8.557
Iter 1999/2000 - Loss: -8.557
Iter 2000/2000 - Loss: -8.557
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.2580, 12.0379, 26.4087,  5.5997,  4.6490, 53.7168]],

        [[20.4878, 30.0784,  8.3902,  1.3380,  1.3674, 22.2611]],

        [[21.6946, 34.0252,  9.0387,  0.9722,  0.9322, 21.5522]],

        [[17.8786, 27.0754, 15.5671,  1.8582,  2.0090, 41.2122]]])
Signal Variance: tensor([ 0.1830,  1.4784, 10.3050,  0.4351])
Estimated target variance: tensor([0.0200, 0.7367, 7.5072, 0.1130])
N: 630
Signal to noise ratio: tensor([24.3493, 65.5655, 69.7704, 39.7896])
Bound on condition number: tensor([ 373520.7624, 2708269.2952, 3066781.5723,  997423.0114])
Policy Optimizer learning rate:
0.009577219229949994
Experience 42, Iter 0, disc loss: 0.0005170742885790754, policy loss: 19.93818354565485
Experience 42, Iter 1, disc loss: 0.0005062167935697332, policy loss: 20.892889244376768
Experience 42, Iter 2, disc loss: 0.0005073840177003721, policy loss: 20.537304228574495
Experience 42, Iter 3, disc loss: 0.0005030620934057365, policy loss: 20.47459920054193
Experience 42, Iter 4, disc loss: 0.0005033836240099739, policy loss: 19.812771024553218
Experience 42, Iter 5, disc loss: 0.0005032994875784679, policy loss: 20.76944310873077
Experience 42, Iter 6, disc loss: 0.0004963623689620242, policy loss: 20.701473331494128
Experience 42, Iter 7, disc loss: 0.0004983429119430814, policy loss: 21.775809128398926
Experience 42, Iter 8, disc loss: 0.0004923023974208457, policy loss: 20.66505141676483
Experience 42, Iter 9, disc loss: 0.0004905259360301797, policy loss: 22.018351852466765
Experience 42, Iter 10, disc loss: 0.0004896626733150604, policy loss: 20.85371538744131
Experience 42, Iter 11, disc loss: 0.0004888091786813797, policy loss: 20.500731004805054
Experience 42, Iter 12, disc loss: 0.0004878524769152056, policy loss: 19.666110692509342
Experience 42, Iter 13, disc loss: 0.000483549784528949, policy loss: 22.1751204534854
Experience 42, Iter 14, disc loss: 0.0005052460341964987, policy loss: 21.773832645539578
Experience 42, Iter 15, disc loss: 0.000482592169341416, policy loss: 22.360325134343235
Experience 42, Iter 16, disc loss: 0.00047826417464266323, policy loss: 22.00516416560645
Experience 42, Iter 17, disc loss: 0.00047677547181619373, policy loss: 21.775679831811193
Experience 42, Iter 18, disc loss: 0.0004796991218339357, policy loss: 20.257557439506126
Experience 42, Iter 19, disc loss: 0.00047324064505259965, policy loss: 22.812418309457666
Experience 42, Iter 20, disc loss: 0.0004826097395337272, policy loss: 21.00069719758111
Experience 42, Iter 21, disc loss: 0.00047131488161801933, policy loss: 21.91315545333251
Experience 42, Iter 22, disc loss: 0.00047307796309923266, policy loss: 20.509183207829437
Experience 42, Iter 23, disc loss: 0.0004685718535741682, policy loss: 20.380209367942218
Experience 42, Iter 24, disc loss: 0.0004648999967365989, policy loss: 22.034002953723117
Experience 42, Iter 25, disc loss: 0.00047589016845184516, policy loss: 20.85439809382715
Experience 42, Iter 26, disc loss: 0.00046415536560565345, policy loss: 21.433381270020984
Experience 42, Iter 27, disc loss: 0.00047414938489121367, policy loss: 19.321189784842392
Experience 42, Iter 28, disc loss: 0.0004610317014424221, policy loss: 20.74748791197603
Experience 42, Iter 29, disc loss: 0.00045911320665000617, policy loss: 19.47804760847633
Experience 42, Iter 30, disc loss: 0.00046011328887284476, policy loss: 21.446575149246144
Experience 42, Iter 31, disc loss: 0.00045760092202528724, policy loss: 21.10402026085867
Experience 42, Iter 32, disc loss: 0.0004560847882501979, policy loss: 19.99718335443709
Experience 42, Iter 33, disc loss: 0.00045774294612108216, policy loss: 21.795426434210526
Experience 42, Iter 34, disc loss: 0.0004554230895815444, policy loss: 21.007298502231592
Experience 42, Iter 35, disc loss: 0.00045155595176701884, policy loss: 19.076402182894427
Experience 42, Iter 36, disc loss: 0.00045062651540894207, policy loss: 21.13260105852067
Experience 42, Iter 37, disc loss: 0.00045475308963997417, policy loss: 20.252941759623702
Experience 42, Iter 38, disc loss: 0.00045135001744481745, policy loss: 20.78672486438433
Experience 42, Iter 39, disc loss: 0.000445854636503396, policy loss: 20.16110153276643
Experience 42, Iter 40, disc loss: 0.0004452687765389423, policy loss: 20.313307497349594
Experience 42, Iter 41, disc loss: 0.0004421519377263748, policy loss: 21.29279613877035
Experience 42, Iter 42, disc loss: 0.00044937246640429227, policy loss: 19.485215732375167
Experience 42, Iter 43, disc loss: 0.000438412477056038, policy loss: 20.527630286626618
Experience 42, Iter 44, disc loss: 0.0004388397469093009, policy loss: 20.38852979104066
Experience 42, Iter 45, disc loss: 0.0004373350038373129, policy loss: 20.93499617800254
Experience 42, Iter 46, disc loss: 0.0004354176722199078, policy loss: 20.91791101043622
Experience 42, Iter 47, disc loss: 0.00043585752966489153, policy loss: 20.335558818643534
Experience 42, Iter 48, disc loss: 0.00043437772918698234, policy loss: 19.736675727411566
Experience 42, Iter 49, disc loss: 0.0004338417057875186, policy loss: 20.348189058649506
Experience 42, Iter 50, disc loss: 0.00043194055462340245, policy loss: 21.39399303625388
Experience 42, Iter 51, disc loss: 0.000429000654080078, policy loss: 21.18734048973649
Experience 42, Iter 52, disc loss: 0.00042973924501652665, policy loss: 20.733660251976332
Experience 42, Iter 53, disc loss: 0.0004269946191016528, policy loss: 20.611797535104238
Experience 42, Iter 54, disc loss: 0.00042480792529362777, policy loss: 21.97063239992688
Experience 42, Iter 55, disc loss: 0.0004250925444484815, policy loss: 20.862558167733226
Experience 42, Iter 56, disc loss: 0.0004232638338759381, policy loss: 20.34629781131465
Experience 42, Iter 57, disc loss: 0.0004251069860048576, policy loss: 22.94654480688557
Experience 42, Iter 58, disc loss: 0.0004203380597349099, policy loss: 20.929832993267087
Experience 42, Iter 59, disc loss: 0.00042038616800514086, policy loss: 21.554429228103423
Experience 42, Iter 60, disc loss: 0.00041969941208939125, policy loss: 21.371954437679687
Experience 42, Iter 61, disc loss: 0.0004214311278313365, policy loss: 21.097925739580365
Experience 42, Iter 62, disc loss: 0.00041702232721270064, policy loss: 22.207605740734117
Experience 42, Iter 63, disc loss: 0.0004144728022361385, policy loss: 23.36637180973113
Experience 42, Iter 64, disc loss: 0.0004147488617622998, policy loss: 20.67534828260376
Experience 42, Iter 65, disc loss: 0.00041211990937474485, policy loss: 21.697937408134877
Experience 42, Iter 66, disc loss: 0.0004098685190572212, policy loss: 21.34913404466543
Experience 42, Iter 67, disc loss: 0.00041086428884219017, policy loss: 21.367581117960725
Experience 42, Iter 68, disc loss: 0.00041235122488903636, policy loss: 23.06414162587926
Experience 42, Iter 69, disc loss: 0.0004096670468914191, policy loss: 22.813499959874918
Experience 42, Iter 70, disc loss: 0.00040556254847653463, policy loss: 21.828534752590514
Experience 42, Iter 71, disc loss: 0.0004121385676126923, policy loss: 22.151428321387396
Experience 42, Iter 72, disc loss: 0.0004068243300066036, policy loss: 20.985869776834644
Experience 42, Iter 73, disc loss: 0.00040541943496964056, policy loss: 22.94866408867595
Experience 42, Iter 74, disc loss: 0.000401180643228789, policy loss: 21.517899376245687
Experience: 43
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1848],
        [1.8759],
        [0.0277]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0399, 0.2176, 1.3215, 0.0254, 0.0133, 4.6803]],

        [[0.0399, 0.2176, 1.3215, 0.0254, 0.0133, 4.6803]],

        [[0.0399, 0.2176, 1.3215, 0.0254, 0.0133, 4.6803]],

        [[0.0399, 0.2176, 1.3215, 0.0254, 0.0133, 4.6803]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0198, 0.7393, 7.5035, 0.1109], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0198, 0.7393, 7.5035, 0.1109])
N: 645
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2581.0000, 2581.0000, 2581.0000, 2581.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.308
Iter 2/2000 - Loss: 3.357
Iter 3/2000 - Loss: 3.137
Iter 4/2000 - Loss: 3.060
Iter 5/2000 - Loss: 3.000
Iter 6/2000 - Loss: 2.851
Iter 7/2000 - Loss: 2.678
Iter 8/2000 - Loss: 2.527
Iter 9/2000 - Loss: 2.384
Iter 10/2000 - Loss: 2.214
Iter 11/2000 - Loss: 2.009
Iter 12/2000 - Loss: 1.782
Iter 13/2000 - Loss: 1.546
Iter 14/2000 - Loss: 1.304
Iter 15/2000 - Loss: 1.055
Iter 16/2000 - Loss: 0.792
Iter 17/2000 - Loss: 0.514
Iter 18/2000 - Loss: 0.223
Iter 19/2000 - Loss: -0.079
Iter 20/2000 - Loss: -0.386
Iter 1981/2000 - Loss: -8.588
Iter 1982/2000 - Loss: -8.588
Iter 1983/2000 - Loss: -8.588
Iter 1984/2000 - Loss: -8.588
Iter 1985/2000 - Loss: -8.588
Iter 1986/2000 - Loss: -8.588
Iter 1987/2000 - Loss: -8.588
Iter 1988/2000 - Loss: -8.588
Iter 1989/2000 - Loss: -8.588
Iter 1990/2000 - Loss: -8.588
Iter 1991/2000 - Loss: -8.588
Iter 1992/2000 - Loss: -8.588
Iter 1993/2000 - Loss: -8.588
Iter 1994/2000 - Loss: -8.588
Iter 1995/2000 - Loss: -8.588
Iter 1996/2000 - Loss: -8.588
Iter 1997/2000 - Loss: -8.588
Iter 1998/2000 - Loss: -8.588
Iter 1999/2000 - Loss: -8.588
Iter 2000/2000 - Loss: -8.588
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.2373, 11.9804, 26.4379,  5.6178,  4.5932, 53.7891]],

        [[20.4204, 29.8554,  8.4013,  1.3292,  1.3620, 22.1357]],

        [[21.6686, 33.8713,  8.9869,  0.9631,  0.9366, 21.4495]],

        [[17.7217, 26.7640, 15.6460,  1.8578,  2.0266, 41.2476]]])
Signal Variance: tensor([ 0.1805,  1.4677, 10.2171,  0.4364])
Estimated target variance: tensor([0.0198, 0.7393, 7.5035, 0.1109])
N: 645
Signal to noise ratio: tensor([24.2540, 65.5984, 69.6918, 40.0242])
Bound on condition number: tensor([ 379425.6772, 2775530.8338, 3132735.3619, 1033247.7634])
Policy Optimizer learning rate:
0.00956713393627531
Experience 43, Iter 0, disc loss: 0.0004050277098501164, policy loss: 21.77625471497634
Experience 43, Iter 1, disc loss: 0.0004207590151364638, policy loss: 22.01530114955981
Experience 43, Iter 2, disc loss: 0.00039695160327411663, policy loss: 22.681524031353334
Experience 43, Iter 3, disc loss: 0.00039903414702327944, policy loss: 21.225448372735624
Experience 43, Iter 4, disc loss: 0.0003958851392384839, policy loss: 21.63802154279216
Experience 43, Iter 5, disc loss: 0.0003945377207760576, policy loss: 21.954113284003633
Experience 43, Iter 6, disc loss: 0.00039411549962680364, policy loss: 21.889468086278676
Experience 43, Iter 7, disc loss: 0.00039243967475323784, policy loss: 22.488451540956426
Experience 43, Iter 8, disc loss: 0.000394387271626738, policy loss: 20.591288872350162
Experience 43, Iter 9, disc loss: 0.0003899873243349511, policy loss: 22.550536231058295
Experience 43, Iter 10, disc loss: 0.0003890048826321515, policy loss: 21.053310750469183
Experience 43, Iter 11, disc loss: 0.0003883102144535129, policy loss: 21.973270178123038
Experience 43, Iter 12, disc loss: 0.00038837690924045664, policy loss: 22.036984696033016
Experience 43, Iter 13, disc loss: 0.00038565404422728625, policy loss: 22.415668569298543
Experience 43, Iter 14, disc loss: 0.00038547861122991474, policy loss: 21.653306589436653
Experience 43, Iter 15, disc loss: 0.0003843631402465402, policy loss: 22.724058159495797
Experience 43, Iter 16, disc loss: 0.0003828710861353966, policy loss: 22.06089199736848
Experience 43, Iter 17, disc loss: 0.0003835413547396917, policy loss: 21.33482555331368
Experience 43, Iter 18, disc loss: 0.0004042602696073616, policy loss: 22.884461953031114
Experience 43, Iter 19, disc loss: 0.0003806665608608678, policy loss: 22.8675951534277
Experience 43, Iter 20, disc loss: 0.00038136041764957564, policy loss: 21.33854012662102
Experience 43, Iter 21, disc loss: 0.00038106954266757824, policy loss: 21.85808342673549
Experience 43, Iter 22, disc loss: 0.00038499679867139445, policy loss: 19.966269560130677
Experience 43, Iter 23, disc loss: 0.00037890147938773313, policy loss: 20.653094713490873
Experience 43, Iter 24, disc loss: 0.0003774425501362965, policy loss: 21.782498965530564
Experience 43, Iter 25, disc loss: 0.00037436352568584724, policy loss: 23.599648356000905
Experience 43, Iter 26, disc loss: 0.00037462505460931513, policy loss: 22.257517210287475
Experience 43, Iter 27, disc loss: 0.000371449129434344, policy loss: 23.23864891780013
Experience 43, Iter 28, disc loss: 0.00037035294572599264, policy loss: 23.32742530491904
Experience 43, Iter 29, disc loss: 0.00036979004328974673, policy loss: 22.38490313330896
Experience 43, Iter 30, disc loss: 0.00036942842500257853, policy loss: 23.653555148569612
Experience 43, Iter 31, disc loss: 0.00036990389601806805, policy loss: 22.884820225725402
Experience 43, Iter 32, disc loss: 0.00036895466232889226, policy loss: 22.002960258115106
Experience 43, Iter 33, disc loss: 0.0003662165703485763, policy loss: 21.759742803694426
Experience 43, Iter 34, disc loss: 0.0003670050673143624, policy loss: 23.121704817497577
Experience 43, Iter 35, disc loss: 0.00036445811685735974, policy loss: 22.560204901439498
Experience 43, Iter 36, disc loss: 0.00036387050989195155, policy loss: 22.33933684695705
Experience 43, Iter 37, disc loss: 0.00036305302393253424, policy loss: 23.150123042034775
Experience 43, Iter 38, disc loss: 0.00036154219433695876, policy loss: 23.78165871671528
Experience 43, Iter 39, disc loss: 0.0003617866524805757, policy loss: 22.531685410776607
Experience 43, Iter 40, disc loss: 0.00035974311053230606, policy loss: 23.09474463301938
Experience 43, Iter 41, disc loss: 0.0003597328243925664, policy loss: 22.330711684515023
Experience 43, Iter 42, disc loss: 0.0003579383243170176, policy loss: 23.081689878582857
Experience 43, Iter 43, disc loss: 0.00035648928075635375, policy loss: 23.13399525039062
Experience 43, Iter 44, disc loss: 0.0003585184863084134, policy loss: 21.449516539825545
Experience 43, Iter 45, disc loss: 0.0003547742388478741, policy loss: 23.50187962461098
Experience 43, Iter 46, disc loss: 0.0003550128163567321, policy loss: 23.110792407072008
Experience 43, Iter 47, disc loss: 0.0003529239933383095, policy loss: 23.91464373578639
Experience 43, Iter 48, disc loss: 0.0003523832816631809, policy loss: 23.511482427557343
Experience 43, Iter 49, disc loss: 0.0003516701043111083, policy loss: 22.36883485968534
Experience 43, Iter 50, disc loss: 0.00035090123770559586, policy loss: 23.11232181412094
Experience 43, Iter 51, disc loss: 0.00034953713077203336, policy loss: 23.459959204436487
Experience 43, Iter 52, disc loss: 0.00034889437883081235, policy loss: 23.306077096696683
Experience 43, Iter 53, disc loss: 0.0003480626237296024, policy loss: 23.00630637093061
Experience 43, Iter 54, disc loss: 0.0003540044451346224, policy loss: 22.770711564790254
Experience 43, Iter 55, disc loss: 0.00034702865328677595, policy loss: 22.899028594739388
Experience 43, Iter 56, disc loss: 0.0003454558683881198, policy loss: 23.043457550470634
Experience 43, Iter 57, disc loss: 0.0003453621047566238, policy loss: 22.630980236483104
Experience 43, Iter 58, disc loss: 0.0003445178544238647, policy loss: 23.181250155347023
Experience 43, Iter 59, disc loss: 0.00034309311511000116, policy loss: 22.04505251095257
Experience 43, Iter 60, disc loss: 0.0003451309106092771, policy loss: 22.245753636788987
Experience 43, Iter 61, disc loss: 0.000342320638847776, policy loss: 21.266946020053926
Experience 43, Iter 62, disc loss: 0.0003408489671207448, policy loss: 21.752240602719233
Experience 43, Iter 63, disc loss: 0.00034031365500806826, policy loss: 22.16074918633359
Experience 43, Iter 64, disc loss: 0.0003401933256449707, policy loss: 22.587557155557032
Experience 43, Iter 65, disc loss: 0.0003381869785272788, policy loss: 22.08640546406542
Experience 43, Iter 66, disc loss: 0.00033796216425738463, policy loss: 23.213694707740732
Experience 43, Iter 67, disc loss: 0.0003377933217884752, policy loss: 22.761578428483862
Experience 43, Iter 68, disc loss: 0.0003381462853388925, policy loss: 20.68876011276201
Experience 43, Iter 69, disc loss: 0.0003354485343527379, policy loss: 21.004822664951455
Experience 43, Iter 70, disc loss: 0.0003343740286184846, policy loss: 22.223415045750265
Experience 43, Iter 71, disc loss: 0.0003367239751377463, policy loss: 21.236112943397252
Experience 43, Iter 72, disc loss: 0.00033339596375549994, policy loss: 21.71231980821055
Experience 43, Iter 73, disc loss: 0.0003327569691146993, policy loss: 22.377486514676576
Experience 43, Iter 74, disc loss: 0.00033509697757520997, policy loss: 20.158915359701908
Experience: 44
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.1865],
        [1.8917],
        [0.0277]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0441, 0.2283, 1.3221, 0.0252, 0.0131, 4.7003]],

        [[0.0441, 0.2283, 1.3221, 0.0252, 0.0131, 4.7003]],

        [[0.0441, 0.2283, 1.3221, 0.0252, 0.0131, 4.7003]],

        [[0.0441, 0.2283, 1.3221, 0.0252, 0.0131, 4.7003]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0208, 0.7458, 7.5667, 0.1107], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0208, 0.7458, 7.5667, 0.1107])
N: 660
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2641.0000, 2641.0000, 2641.0000, 2641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.315
Iter 2/2000 - Loss: 3.331
Iter 3/2000 - Loss: 3.127
Iter 4/2000 - Loss: 3.043
Iter 5/2000 - Loss: 2.972
Iter 6/2000 - Loss: 2.820
Iter 7/2000 - Loss: 2.648
Iter 8/2000 - Loss: 2.495
Iter 9/2000 - Loss: 2.344
Iter 10/2000 - Loss: 2.165
Iter 11/2000 - Loss: 1.957
Iter 12/2000 - Loss: 1.732
Iter 13/2000 - Loss: 1.500
Iter 14/2000 - Loss: 1.261
Iter 15/2000 - Loss: 1.011
Iter 16/2000 - Loss: 0.749
Iter 17/2000 - Loss: 0.473
Iter 18/2000 - Loss: 0.185
Iter 19/2000 - Loss: -0.110
Iter 20/2000 - Loss: -0.410
Iter 1981/2000 - Loss: -8.582
Iter 1982/2000 - Loss: -8.582
Iter 1983/2000 - Loss: -8.582
Iter 1984/2000 - Loss: -8.582
Iter 1985/2000 - Loss: -8.582
Iter 1986/2000 - Loss: -8.582
Iter 1987/2000 - Loss: -8.582
Iter 1988/2000 - Loss: -8.582
Iter 1989/2000 - Loss: -8.582
Iter 1990/2000 - Loss: -8.582
Iter 1991/2000 - Loss: -8.582
Iter 1992/2000 - Loss: -8.582
Iter 1993/2000 - Loss: -8.582
Iter 1994/2000 - Loss: -8.582
Iter 1995/2000 - Loss: -8.582
Iter 1996/2000 - Loss: -8.582
Iter 1997/2000 - Loss: -8.582
Iter 1998/2000 - Loss: -8.582
Iter 1999/2000 - Loss: -8.582
Iter 2000/2000 - Loss: -8.582
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.2927, 12.8587, 25.5262,  5.0338,  4.5930, 53.5699]],

        [[20.5305, 29.5596,  8.5505,  1.3536,  1.2194, 22.0143]],

        [[22.2234, 34.6761,  8.8092,  0.9745,  0.9069, 21.4166]],

        [[18.1515, 27.0424, 15.5677,  1.8427,  2.0149, 41.4060]]])
Signal Variance: tensor([0.1877, 1.4367, 9.7851, 0.4257])
Estimated target variance: tensor([0.0208, 0.7458, 7.5667, 0.1107])
N: 660
Signal to noise ratio: tensor([24.4791, 64.8931, 68.4166, 39.3876])
Bound on condition number: tensor([ 395491.6746, 2779339.4529, 3089352.1929, 1023912.6105])
Policy Optimizer learning rate:
0.00955705926292225
Experience 44, Iter 0, disc loss: 0.00033050330916821274, policy loss: 21.897254950058993
Experience 44, Iter 1, disc loss: 0.0003354588517042744, policy loss: 21.795530991835648
Experience 44, Iter 2, disc loss: 0.00032929474447240613, policy loss: 21.528581596133932
Experience 44, Iter 3, disc loss: 0.0003280626655707123, policy loss: 23.005637669733517
Experience 44, Iter 4, disc loss: 0.0003293956024018308, policy loss: 21.564981270444218
Experience 44, Iter 5, disc loss: 0.00032699040872863144, policy loss: 22.14950252521684
Experience 44, Iter 6, disc loss: 0.00032567731210857956, policy loss: 23.70818175305503
Experience 44, Iter 7, disc loss: 0.00032494771632727854, policy loss: 23.58476145106368
Experience 44, Iter 8, disc loss: 0.00032539166788374914, policy loss: 21.515078896207434
Experience 44, Iter 9, disc loss: 0.0003241911981310041, policy loss: 21.11360764737186
Experience 44, Iter 10, disc loss: 0.0003230207907733935, policy loss: 23.42381370555557
Experience 44, Iter 11, disc loss: 0.00032408388118478646, policy loss: 22.03136329409293
Experience 44, Iter 12, disc loss: 0.0003234684359410795, policy loss: 21.420464208472783
Experience 44, Iter 13, disc loss: 0.00032086837061907306, policy loss: 22.430138038627486
Experience 44, Iter 14, disc loss: 0.00032005998912998775, policy loss: 22.781681633307745
Experience 44, Iter 15, disc loss: 0.0003199770194531417, policy loss: 22.789645992547808
Experience 44, Iter 16, disc loss: 0.0003212812432591599, policy loss: 22.673557860908772
Experience 44, Iter 17, disc loss: 0.01387001442564594, policy loss: 23.285998121775577
Experience 44, Iter 18, disc loss: 0.0003199644367236383, policy loss: 22.974575244057363
Experience 44, Iter 19, disc loss: 0.00031994598350643924, policy loss: 23.07218990056064
Experience 44, Iter 20, disc loss: 0.00032134064889415, policy loss: 20.54741655405453
Experience 44, Iter 21, disc loss: 0.00032144496977116224, policy loss: 22.15028858146838
Experience 44, Iter 22, disc loss: 0.00032287018688689854, policy loss: 21.948212059676507
Experience 44, Iter 23, disc loss: 0.00032267083544818735, policy loss: 21.99920913036133
Experience 44, Iter 24, disc loss: 0.00032301218259536793, policy loss: 21.858463908542472
Experience 44, Iter 25, disc loss: 0.0003245386200979266, policy loss: 21.085860474484505
Experience 44, Iter 26, disc loss: 0.0003229158742692855, policy loss: 21.960160415685497
Experience 44, Iter 27, disc loss: 0.0003235653670019476, policy loss: 21.502819121468903
Experience 44, Iter 28, disc loss: 0.0003228267943347484, policy loss: 22.45507599228045
Experience 44, Iter 29, disc loss: 0.000322998087043454, policy loss: 22.25757634535965
Experience 44, Iter 30, disc loss: 0.00032449712873756367, policy loss: 21.560875598909966
Experience 44, Iter 31, disc loss: 0.00032398495790175865, policy loss: 22.60653428214695
Experience 44, Iter 32, disc loss: 0.0003218882865608462, policy loss: 21.8975392422113
Experience 44, Iter 33, disc loss: 0.0003229226604637906, policy loss: 21.127908678022273
Experience 44, Iter 34, disc loss: 0.00032263083096584607, policy loss: 21.782102909459915
Experience 44, Iter 35, disc loss: 0.0003224925096656697, policy loss: 21.439293461595952
Experience 44, Iter 36, disc loss: 0.0003220257311734212, policy loss: 21.393557112337447
Experience 44, Iter 37, disc loss: 0.00032018102593686935, policy loss: 22.137640267116975
Experience 44, Iter 38, disc loss: 0.0003304811372960694, policy loss: 21.609832117493866
Experience 44, Iter 39, disc loss: 0.00032053066799156297, policy loss: 21.88977551464168
Experience 44, Iter 40, disc loss: 0.0003322912905722614, policy loss: 21.035331920201834
Experience 44, Iter 41, disc loss: 0.0003186888192781556, policy loss: 22.92526661864562
Experience 44, Iter 42, disc loss: 0.0003175783993751885, policy loss: 22.493973549196628
Experience 44, Iter 43, disc loss: 0.00031825714240017885, policy loss: 21.405261470821067
Experience 44, Iter 44, disc loss: 0.00031615413922370697, policy loss: 21.67905436485606
Experience 44, Iter 45, disc loss: 0.00031574344714057294, policy loss: 21.64668799613692
Experience 44, Iter 46, disc loss: 0.0003155602641436343, policy loss: 21.617152510041635
Experience 44, Iter 47, disc loss: 0.0003151817899764687, policy loss: 21.01249655949999
Experience 44, Iter 48, disc loss: 0.00031398600155928524, policy loss: 22.078899737953307
Experience 44, Iter 49, disc loss: 0.0003143536868579232, policy loss: 21.84956865506394
Experience 44, Iter 50, disc loss: 0.0003146715498315636, policy loss: 20.87433415975792
Experience 44, Iter 51, disc loss: 0.0003118419032098302, policy loss: 21.24325784068504
Experience 44, Iter 52, disc loss: 0.00031181342580914855, policy loss: 21.42420721299861
Experience 44, Iter 53, disc loss: 0.00031196609285107416, policy loss: 22.61003411231013
Experience 44, Iter 54, disc loss: 0.00030992589455712706, policy loss: 21.848372248887934
Experience 44, Iter 55, disc loss: 0.0003091505480344118, policy loss: 22.453778474740588
Experience 44, Iter 56, disc loss: 0.00030894701373231694, policy loss: 20.884043734317483
Experience 44, Iter 57, disc loss: 0.00030866507582422224, policy loss: 20.663435938783948
Experience 44, Iter 58, disc loss: 0.0003083581162929596, policy loss: 21.350997144124165
Experience 44, Iter 59, disc loss: 0.0003069049319717392, policy loss: 21.822755062568145
Experience 44, Iter 60, disc loss: 0.00030602212466302874, policy loss: 22.76913667957656
Experience 44, Iter 61, disc loss: 0.00030708464288043433, policy loss: 20.12107198198435
Experience 44, Iter 62, disc loss: 0.00030548706714190663, policy loss: 20.710271303419688
Experience 44, Iter 63, disc loss: 0.00030416601850550933, policy loss: 20.56687443106443
Experience 44, Iter 64, disc loss: 0.00030374747801333236, policy loss: 20.475949938039996
Experience 44, Iter 65, disc loss: 0.00030328819696368215, policy loss: 21.382079264903098
Experience 44, Iter 66, disc loss: 0.0003018503977749022, policy loss: 21.29114168999704
Experience 44, Iter 67, disc loss: 0.00030191407893997855, policy loss: 21.540353856533223
Experience 44, Iter 68, disc loss: 0.00030098174842282634, policy loss: 20.895856429032765
Experience 44, Iter 69, disc loss: 0.0003084642206626174, policy loss: 22.504980203762333
Experience 44, Iter 70, disc loss: 0.00029956146017319865, policy loss: 22.697995032975495
Experience 44, Iter 71, disc loss: 0.0003023130369108342, policy loss: 21.58002565278923
Experience 44, Iter 72, disc loss: 0.0002981816679467103, policy loss: 21.418028539052514
Experience 44, Iter 73, disc loss: 0.000297424470460905, policy loss: 22.08062869642128
Experience 44, Iter 74, disc loss: 0.0002967217262773732, policy loss: 22.47310145162594
Experience: 45
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.1871],
        [1.8831],
        [0.0281]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0473, 0.2337, 1.3398, 0.0256, 0.0142, 4.7202]],

        [[0.0473, 0.2337, 1.3398, 0.0256, 0.0142, 4.7202]],

        [[0.0473, 0.2337, 1.3398, 0.0256, 0.0142, 4.7202]],

        [[0.0473, 0.2337, 1.3398, 0.0256, 0.0142, 4.7202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0213, 0.7484, 7.5324, 0.1123], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0213, 0.7484, 7.5324, 0.1123])
N: 675
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2701.0000, 2701.0000, 2701.0000, 2701.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.318
Iter 2/2000 - Loss: 3.307
Iter 3/2000 - Loss: 3.119
Iter 4/2000 - Loss: 3.025
Iter 5/2000 - Loss: 2.943
Iter 6/2000 - Loss: 2.790
Iter 7/2000 - Loss: 2.620
Iter 8/2000 - Loss: 2.467
Iter 9/2000 - Loss: 2.308
Iter 10/2000 - Loss: 2.122
Iter 11/2000 - Loss: 1.909
Iter 12/2000 - Loss: 1.683
Iter 13/2000 - Loss: 1.451
Iter 14/2000 - Loss: 1.211
Iter 15/2000 - Loss: 0.960
Iter 16/2000 - Loss: 0.696
Iter 17/2000 - Loss: 0.419
Iter 18/2000 - Loss: 0.133
Iter 19/2000 - Loss: -0.159
Iter 20/2000 - Loss: -0.455
Iter 1981/2000 - Loss: -8.523
Iter 1982/2000 - Loss: -8.524
Iter 1983/2000 - Loss: -8.524
Iter 1984/2000 - Loss: -8.524
Iter 1985/2000 - Loss: -8.524
Iter 1986/2000 - Loss: -8.524
Iter 1987/2000 - Loss: -8.524
Iter 1988/2000 - Loss: -8.524
Iter 1989/2000 - Loss: -8.524
Iter 1990/2000 - Loss: -8.524
Iter 1991/2000 - Loss: -8.524
Iter 1992/2000 - Loss: -8.524
Iter 1993/2000 - Loss: -8.524
Iter 1994/2000 - Loss: -8.524
Iter 1995/2000 - Loss: -8.524
Iter 1996/2000 - Loss: -8.524
Iter 1997/2000 - Loss: -8.524
Iter 1998/2000 - Loss: -8.524
Iter 1999/2000 - Loss: -8.524
Iter 2000/2000 - Loss: -8.524
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.1499, 11.4934, 24.9761,  7.1472,  5.3533, 52.4950]],

        [[21.0720, 31.0449,  8.5201,  1.4333,  1.1756, 21.6583]],

        [[22.2815, 34.8910,  8.7215,  1.0374,  0.9286, 21.6312]],

        [[18.5835, 26.8774, 17.2416,  1.7812,  2.0727, 40.5629]]])
Signal Variance: tensor([ 0.1776,  1.4726, 10.4786,  0.4581])
Estimated target variance: tensor([0.0213, 0.7484, 7.5324, 0.1123])
N: 675
Signal to noise ratio: tensor([23.6465, 65.2402, 70.1195, 40.8474])
Bound on condition number: tensor([ 377432.1949, 2872990.8557, 3318801.6020, 1126244.4225])
Policy Optimizer learning rate:
0.009546995198707083
Experience 45, Iter 0, disc loss: 0.00029612090493868484, policy loss: 22.05289288116818
Experience 45, Iter 1, disc loss: 0.00029540354188397407, policy loss: 23.50384205106699
Experience 45, Iter 2, disc loss: 0.00029539172684760613, policy loss: 22.491142098267353
Experience 45, Iter 3, disc loss: 0.00029424262498139374, policy loss: 22.83155412961866
Experience 45, Iter 4, disc loss: 0.0002934562259087572, policy loss: 22.320008965098694
Experience 45, Iter 5, disc loss: 0.00029295583192326743, policy loss: 24.30053168441072
Experience 45, Iter 6, disc loss: 0.00029969363671333605, policy loss: 22.97833313246713
Experience 45, Iter 7, disc loss: 0.00029441268451978853, policy loss: 22.224064994623355
Experience 45, Iter 8, disc loss: 0.00029235937696942183, policy loss: 23.167564185377984
Experience 45, Iter 9, disc loss: 0.0002925890368278175, policy loss: 23.203906510237868
Experience 45, Iter 10, disc loss: 0.0086405478402296, policy loss: 20.6926841319711
Experience 45, Iter 11, disc loss: 0.00029106016365522327, policy loss: 22.899126224582407
Experience 45, Iter 12, disc loss: 0.0002924150347833808, policy loss: 22.489065021811037
Experience 45, Iter 13, disc loss: 0.0002914257788216406, policy loss: 22.08031717815283
Experience 45, Iter 14, disc loss: 0.0002919889532612495, policy loss: 22.837885681191825
Experience 45, Iter 15, disc loss: 0.00029293395529995535, policy loss: 22.12498313314688
Experience 45, Iter 16, disc loss: 0.0002927542091422193, policy loss: 22.526307363168815
Experience 45, Iter 17, disc loss: 0.0002937061610329956, policy loss: 22.082074703392543
Experience 45, Iter 18, disc loss: 0.0002925595345749953, policy loss: 21.89272466935745
Experience 45, Iter 19, disc loss: 0.0002937225429862852, policy loss: 23.09860781511462
Experience 45, Iter 20, disc loss: 0.0002938501419487722, policy loss: 22.012358664955308
Experience 45, Iter 21, disc loss: 0.0002928154517495778, policy loss: 21.52914245301357
Experience 45, Iter 22, disc loss: 0.0002919106094924839, policy loss: 22.519585858932384
Experience 45, Iter 23, disc loss: 0.00030219760499252264, policy loss: 22.648275660112503
Experience 45, Iter 24, disc loss: 0.0002916526198952246, policy loss: 21.52938139969808
Experience 45, Iter 25, disc loss: 0.0002911128397598552, policy loss: 23.327433564932463
Experience 45, Iter 26, disc loss: 0.0002912316651400342, policy loss: 21.798883088751893
Experience 45, Iter 27, disc loss: 0.0002908929101878175, policy loss: 23.265036306207328
Experience 45, Iter 28, disc loss: 0.00029070861878189853, policy loss: 21.693030085693245
Experience 45, Iter 29, disc loss: 0.0002902069705259051, policy loss: 22.645048475673743
Experience 45, Iter 30, disc loss: 0.00028918643751910344, policy loss: 22.905164596022395
Experience 45, Iter 31, disc loss: 0.0002944879572658258, policy loss: 21.84376224050644
Experience 45, Iter 32, disc loss: 0.0002897586649149112, policy loss: 21.603588798023775
Experience 45, Iter 33, disc loss: 0.00028765305052440743, policy loss: 22.565885510534393
Experience 45, Iter 34, disc loss: 0.00028813960596308757, policy loss: 22.445083360329402
Experience 45, Iter 35, disc loss: 0.000286979574565921, policy loss: 22.1169710561992
Experience 45, Iter 36, disc loss: 0.0002860887410055992, policy loss: 23.006099900020075
Experience 45, Iter 37, disc loss: 0.00028645945499584696, policy loss: 21.777872850342952
Experience 45, Iter 38, disc loss: 0.00028701213156754246, policy loss: 21.23314552566841
Experience 45, Iter 39, disc loss: 0.0002856909588389474, policy loss: 22.157159919718396
Experience 45, Iter 40, disc loss: 0.00029478026825059797, policy loss: 21.058873207710775
Experience 45, Iter 41, disc loss: 0.0002833014389962667, policy loss: 23.132488167603846
Experience 45, Iter 42, disc loss: 0.0002831564591656152, policy loss: 20.79165297663292
Experience 45, Iter 43, disc loss: 0.0002826963686693615, policy loss: 22.350848193065502
Experience 45, Iter 44, disc loss: 0.0002820641368977284, policy loss: 22.948664034199155
Experience 45, Iter 45, disc loss: 0.0002825045627733108, policy loss: 21.459096480859596
Experience 45, Iter 46, disc loss: 0.00028067855207251913, policy loss: 21.575098270922027
Experience 45, Iter 47, disc loss: 0.0002811385713793741, policy loss: 21.254495080983087
Experience 45, Iter 48, disc loss: 0.00028791897493398615, policy loss: 21.33783787989119
Experience 45, Iter 49, disc loss: 0.0002817918696174864, policy loss: 20.520592570196268
Experience 45, Iter 50, disc loss: 0.0002783386135575637, policy loss: 21.595909324019246
Experience 45, Iter 51, disc loss: 0.0002780263715072562, policy loss: 21.358370945809323
Experience 45, Iter 52, disc loss: 0.00027818964731293186, policy loss: 21.19315464254691
Experience 45, Iter 53, disc loss: 0.00027686665880295497, policy loss: 21.483606751447695
Experience 45, Iter 54, disc loss: 0.007428714840624969, policy loss: 21.03780603361119
Experience 45, Iter 55, disc loss: 0.00027868467180960715, policy loss: 21.15357503343423
Experience 45, Iter 56, disc loss: 0.00028257596355209144, policy loss: 20.750966844302233
Experience 45, Iter 57, disc loss: 0.0002780647436270001, policy loss: 21.508981210182505
Experience 45, Iter 58, disc loss: 0.00027901319591507935, policy loss: 20.811083466690373
Experience 45, Iter 59, disc loss: 0.00027995144527013274, policy loss: 21.825911734723995
Experience 45, Iter 60, disc loss: 0.005958669202582253, policy loss: 21.74304028842644
Experience 45, Iter 61, disc loss: 0.00028114039501031274, policy loss: 20.870450736660047
Experience 45, Iter 62, disc loss: 0.0002818085451174812, policy loss: 21.847193679716298
Experience 45, Iter 63, disc loss: 0.0002825542525346848, policy loss: 22.17230435548726
Experience 45, Iter 64, disc loss: 0.00028406330679986463, policy loss: 21.43184982216357
Experience 45, Iter 65, disc loss: 0.0002888001373897751, policy loss: 22.075014586232022
Experience 45, Iter 66, disc loss: 0.0002837059708826081, policy loss: 21.62013084471285
Experience 45, Iter 67, disc loss: 0.00028408076504172916, policy loss: 21.449870284433366
Experience 45, Iter 68, disc loss: 0.00028461617240798173, policy loss: 21.552753690523126
Experience 45, Iter 69, disc loss: 0.0002849185768889148, policy loss: 21.338466662749155
Experience 45, Iter 70, disc loss: 0.00028577554407250455, policy loss: 21.119683910975464
Experience 45, Iter 71, disc loss: 0.0002847528883909702, policy loss: 22.000617839758792
Experience 45, Iter 72, disc loss: 0.0002862575880625043, policy loss: 21.517001459554542
Experience 45, Iter 73, disc loss: 0.00028546539581669696, policy loss: 23.0750065421035
Experience 45, Iter 74, disc loss: 0.00028496894964367053, policy loss: 21.362505380169814
Experience: 46
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.1889],
        [1.8960],
        [0.0279]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0516, 0.2426, 1.3343, 0.0254, 0.0140, 4.7426]],

        [[0.0516, 0.2426, 1.3343, 0.0254, 0.0140, 4.7426]],

        [[0.0516, 0.2426, 1.3343, 0.0254, 0.0140, 4.7426]],

        [[0.0516, 0.2426, 1.3343, 0.0254, 0.0140, 4.7426]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0222, 0.7555, 7.5839, 0.1115], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0222, 0.7555, 7.5839, 0.1115])
N: 690
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2761.0000, 2761.0000, 2761.0000, 2761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.319
Iter 2/2000 - Loss: 3.284
Iter 3/2000 - Loss: 3.109
Iter 4/2000 - Loss: 3.004
Iter 5/2000 - Loss: 2.915
Iter 6/2000 - Loss: 2.763
Iter 7/2000 - Loss: 2.593
Iter 8/2000 - Loss: 2.436
Iter 9/2000 - Loss: 2.272
Iter 10/2000 - Loss: 2.081
Iter 11/2000 - Loss: 1.866
Iter 12/2000 - Loss: 1.639
Iter 13/2000 - Loss: 1.405
Iter 14/2000 - Loss: 1.163
Iter 15/2000 - Loss: 0.909
Iter 16/2000 - Loss: 0.643
Iter 17/2000 - Loss: 0.365
Iter 18/2000 - Loss: 0.079
Iter 19/2000 - Loss: -0.212
Iter 20/2000 - Loss: -0.507
Iter 1981/2000 - Loss: -8.552
Iter 1982/2000 - Loss: -8.552
Iter 1983/2000 - Loss: -8.552
Iter 1984/2000 - Loss: -8.552
Iter 1985/2000 - Loss: -8.553
Iter 1986/2000 - Loss: -8.553
Iter 1987/2000 - Loss: -8.553
Iter 1988/2000 - Loss: -8.553
Iter 1989/2000 - Loss: -8.553
Iter 1990/2000 - Loss: -8.553
Iter 1991/2000 - Loss: -8.553
Iter 1992/2000 - Loss: -8.553
Iter 1993/2000 - Loss: -8.553
Iter 1994/2000 - Loss: -8.553
Iter 1995/2000 - Loss: -8.553
Iter 1996/2000 - Loss: -8.553
Iter 1997/2000 - Loss: -8.553
Iter 1998/2000 - Loss: -8.553
Iter 1999/2000 - Loss: -8.553
Iter 2000/2000 - Loss: -8.553
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.4700, 11.6132, 24.5358,  7.1237,  5.4065, 52.0648]],

        [[21.5121, 31.4105,  8.3400,  1.4302,  1.1993, 21.0033]],

        [[22.8272, 35.1722,  8.7154,  1.0324,  0.9336, 21.5102]],

        [[18.7776, 27.0904, 17.3581,  1.7775,  2.0691, 40.6955]]])
Signal Variance: tensor([ 0.1790,  1.4199, 10.3464,  0.4559])
Estimated target variance: tensor([0.0222, 0.7555, 7.5839, 0.1115])
N: 690
Signal to noise ratio: tensor([23.8051, 64.5618, 70.1347, 40.7912])
Bound on condition number: tensor([ 391013.2737, 2876076.2015, 3394025.1347, 1148105.3347])
Policy Optimizer learning rate:
0.009536941732457853
Experience 46, Iter 0, disc loss: 0.00028456103004524647, policy loss: 20.920817653025416
Experience 46, Iter 1, disc loss: 0.00028668528866248206, policy loss: 22.376910936439273
Experience 46, Iter 2, disc loss: 0.0002837167045901326, policy loss: 21.535366464470478
Experience 46, Iter 3, disc loss: 0.0002833541874410644, policy loss: 23.169464909991994
Experience 46, Iter 4, disc loss: 0.0002845379287584922, policy loss: 21.67865418866408
Experience 46, Iter 5, disc loss: 0.0002825528759304569, policy loss: 22.200359358719865
Experience 46, Iter 6, disc loss: 0.00028316872520204055, policy loss: 21.10282856068187
Experience 46, Iter 7, disc loss: 0.00028731684566018643, policy loss: 21.121054489059183
Experience 46, Iter 8, disc loss: 0.0002818020466566716, policy loss: 20.06557654481788
Experience 46, Iter 9, disc loss: 0.00028094362317380577, policy loss: 21.618626258113306
Experience 46, Iter 10, disc loss: 0.0002807505202617386, policy loss: 21.768547429130415
Experience 46, Iter 11, disc loss: 0.0002799172309950478, policy loss: 22.06611480966218
Experience 46, Iter 12, disc loss: 0.0002795880639798831, policy loss: 21.51448559724457
Experience 46, Iter 13, disc loss: 0.0002788108622186881, policy loss: 22.99023852040634
Experience 46, Iter 14, disc loss: 0.0002783939886147848, policy loss: 22.042393383950195
Experience 46, Iter 15, disc loss: 0.0002787244619830821, policy loss: 21.070989117151015
Experience 46, Iter 16, disc loss: 0.0002772830194323627, policy loss: 21.43687357599956
Experience 46, Iter 17, disc loss: 0.00027746587258162383, policy loss: 21.61161856015315
Experience 46, Iter 18, disc loss: 0.0002760820676394257, policy loss: 22.694763674259846
Experience 46, Iter 19, disc loss: 0.0002758452932212759, policy loss: 21.244974344811965
Experience 46, Iter 20, disc loss: 0.0002751248583570877, policy loss: 21.61197025772821
Experience 46, Iter 21, disc loss: 0.00027506327127986373, policy loss: 22.07759137030734
Experience 46, Iter 22, disc loss: 0.00027387516695666115, policy loss: 21.482028043178154
Experience 46, Iter 23, disc loss: 0.00027552147929299557, policy loss: 22.166587951571007
Experience 46, Iter 24, disc loss: 0.00027422438667126597, policy loss: 22.198699383574255
Experience 46, Iter 25, disc loss: 0.00027284234938326833, policy loss: 20.97825199371134
Experience 46, Iter 26, disc loss: 0.00027159846478939654, policy loss: 22.69141248899739
Experience 46, Iter 27, disc loss: 0.00027927909581980436, policy loss: 22.791346169257857
Experience 46, Iter 28, disc loss: 0.00027083267070740215, policy loss: 22.104353283290404
Experience 46, Iter 29, disc loss: 0.0002699521285152232, policy loss: 22.65705452689184
Experience 46, Iter 30, disc loss: 0.00030543428656483256, policy loss: 21.7027488539671
Experience 46, Iter 31, disc loss: 0.00026865310585837604, policy loss: 21.871608622813067
Experience 46, Iter 32, disc loss: 0.00026924664293264967, policy loss: 22.07700199953732
Experience 46, Iter 33, disc loss: 0.0002674693328915152, policy loss: 21.080089886482764
Experience 46, Iter 34, disc loss: 0.000267104291877026, policy loss: 22.99496743661072
Experience 46, Iter 35, disc loss: 0.011237836652553772, policy loss: 21.91822972717167
Experience 46, Iter 36, disc loss: 0.00026750369110673764, policy loss: 22.45038185791407
Experience 46, Iter 37, disc loss: 0.00026834455617929067, policy loss: 22.69718565020215
Experience 46, Iter 38, disc loss: 0.00026903410677712883, policy loss: 23.445499210207473
Experience 46, Iter 39, disc loss: 0.00028825935863369774, policy loss: 21.965347069185356
Experience 46, Iter 40, disc loss: 0.00027009435829275356, policy loss: 23.501909127019
Experience 46, Iter 41, disc loss: 0.00027110006806200886, policy loss: 21.265318857522544
Experience 46, Iter 42, disc loss: 0.00028583225475334984, policy loss: 22.76743698092503
Experience 46, Iter 43, disc loss: 0.0002714942092998014, policy loss: 21.97391551763514
Experience 46, Iter 44, disc loss: 0.0002720159915477028, policy loss: 23.276893457785004
Experience 46, Iter 45, disc loss: 0.00027151027677967566, policy loss: 22.43959865709279
Experience 46, Iter 46, disc loss: 0.00027136560685779647, policy loss: 22.815118995327964
Experience 46, Iter 47, disc loss: 0.0002713694278466717, policy loss: 22.497806645519027
Experience 46, Iter 48, disc loss: 0.00027247578200559396, policy loss: 22.51020590627011
Experience 46, Iter 49, disc loss: 0.00027105660804892804, policy loss: 22.736595681066717
Experience 46, Iter 50, disc loss: 0.0002714054094855238, policy loss: 22.289321972993342
Experience 46, Iter 51, disc loss: 0.0002706823785345038, policy loss: 21.96403212675812
Experience 46, Iter 52, disc loss: 0.000270672863015745, policy loss: 22.11736600835554
Experience 46, Iter 53, disc loss: 0.0002700866527461571, policy loss: 22.722446588273673
Experience 46, Iter 54, disc loss: 0.00026942657774160127, policy loss: 22.891678065290794
Experience 46, Iter 55, disc loss: 0.0002692433383489591, policy loss: 21.96990719871481
Experience 46, Iter 56, disc loss: 0.014390007405589409, policy loss: 22.336938006424585
Experience 46, Iter 57, disc loss: 0.00027047475055092364, policy loss: 22.745519940739236
Experience 46, Iter 58, disc loss: 0.00027291766506229854, policy loss: 23.25328376399831
Experience 46, Iter 59, disc loss: 0.0002739186110077813, policy loss: 22.077476179298948
Experience 46, Iter 60, disc loss: 0.00027576459653018474, policy loss: 23.46556197299077
Experience 46, Iter 61, disc loss: 0.0002753965711440324, policy loss: 22.57257571001901
Experience 46, Iter 62, disc loss: 0.0002764334496483732, policy loss: 22.011909365718196
Experience 46, Iter 63, disc loss: 0.00027728923165765097, policy loss: 23.163096271395506
Experience 46, Iter 64, disc loss: 0.0002804908113143116, policy loss: 21.337147755251248
Experience 46, Iter 65, disc loss: 0.00027788860551239723, policy loss: 22.664164533012
Experience 46, Iter 66, disc loss: 0.0002786200552344435, policy loss: 22.399978140321718
Experience 46, Iter 67, disc loss: 0.00028254848425813227, policy loss: 22.09965966104732
Experience 46, Iter 68, disc loss: 0.00027861760314676153, policy loss: 22.838374695646916
Experience 46, Iter 69, disc loss: 0.00027965466118387946, policy loss: 21.805541061975607
Experience 46, Iter 70, disc loss: 0.0002792547881313089, policy loss: 22.255317430339367
Experience 46, Iter 71, disc loss: 0.0002828033225553149, policy loss: 21.920042322074707
Experience 46, Iter 72, disc loss: 0.0002785949312846057, policy loss: 22.140766276059964
Experience 46, Iter 73, disc loss: 0.000278671712369532, policy loss: 22.288470496230214
Experience 46, Iter 74, disc loss: 0.00027874151758134453, policy loss: 22.069654470235125
Experience: 47
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.1869],
        [1.8747],
        [0.0278]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0524, 0.2438, 1.3293, 0.0253, 0.0138, 4.7084]],

        [[0.0524, 0.2438, 1.3293, 0.0253, 0.0138, 4.7084]],

        [[0.0524, 0.2438, 1.3293, 0.0253, 0.0138, 4.7084]],

        [[0.0524, 0.2438, 1.3293, 0.0253, 0.0138, 4.7084]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0223, 0.7475, 7.4987, 0.1112], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0223, 0.7475, 7.4987, 0.1112])
N: 705
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2821.0000, 2821.0000, 2821.0000, 2821.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.298
Iter 2/2000 - Loss: 3.252
Iter 3/2000 - Loss: 3.082
Iter 4/2000 - Loss: 2.970
Iter 5/2000 - Loss: 2.879
Iter 6/2000 - Loss: 2.726
Iter 7/2000 - Loss: 2.555
Iter 8/2000 - Loss: 2.396
Iter 9/2000 - Loss: 2.229
Iter 10/2000 - Loss: 2.035
Iter 11/2000 - Loss: 1.818
Iter 12/2000 - Loss: 1.591
Iter 13/2000 - Loss: 1.357
Iter 14/2000 - Loss: 1.116
Iter 15/2000 - Loss: 0.862
Iter 16/2000 - Loss: 0.595
Iter 17/2000 - Loss: 0.318
Iter 18/2000 - Loss: 0.034
Iter 19/2000 - Loss: -0.255
Iter 20/2000 - Loss: -0.548
Iter 1981/2000 - Loss: -8.567
Iter 1982/2000 - Loss: -8.567
Iter 1983/2000 - Loss: -8.567
Iter 1984/2000 - Loss: -8.567
Iter 1985/2000 - Loss: -8.567
Iter 1986/2000 - Loss: -8.567
Iter 1987/2000 - Loss: -8.567
Iter 1988/2000 - Loss: -8.567
Iter 1989/2000 - Loss: -8.567
Iter 1990/2000 - Loss: -8.567
Iter 1991/2000 - Loss: -8.567
Iter 1992/2000 - Loss: -8.567
Iter 1993/2000 - Loss: -8.567
Iter 1994/2000 - Loss: -8.567
Iter 1995/2000 - Loss: -8.567
Iter 1996/2000 - Loss: -8.567
Iter 1997/2000 - Loss: -8.567
Iter 1998/2000 - Loss: -8.567
Iter 1999/2000 - Loss: -8.567
Iter 2000/2000 - Loss: -8.568
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.4209, 11.6630, 24.5938,  7.2413,  5.2536, 51.2550]],

        [[21.5376, 31.5528,  8.4605,  1.4433,  1.1992, 21.2179]],

        [[22.5827, 35.6025,  8.5962,  1.0096,  0.9266, 20.8813]],

        [[18.7361, 26.8324, 17.2021,  1.7647,  2.0982, 41.0197]]])
Signal Variance: tensor([0.1782, 1.4579, 9.8602, 0.4472])
Estimated target variance: tensor([0.0223, 0.7475, 7.4987, 0.1112])
N: 705
Signal to noise ratio: tensor([23.8442, 65.3417, 68.7702, 40.5351])
Bound on condition number: tensor([ 400825.8650, 3010025.9012, 3334187.1910, 1158383.7739])
Policy Optimizer learning rate:
0.00952689885301437
Experience 47, Iter 0, disc loss: 0.00027768052496734085, policy loss: 22.12843598249
Experience 47, Iter 1, disc loss: 0.00027845982252234526, policy loss: 21.49204328807506
Experience 47, Iter 2, disc loss: 0.0002964903891366772, policy loss: 22.373725702237742
Experience 47, Iter 3, disc loss: 0.0002766725031728857, policy loss: 21.362411276628976
Experience 47, Iter 4, disc loss: 0.0002798201772934172, policy loss: 22.06813876233577
Experience 47, Iter 5, disc loss: 0.0002772018845351761, policy loss: 21.870787491651647
Experience 47, Iter 6, disc loss: 0.0002764216205230128, policy loss: 21.773432215874806
Experience 47, Iter 7, disc loss: 0.0002748878901219014, policy loss: 22.78820023806994
Experience 47, Iter 8, disc loss: 0.00027760521619165485, policy loss: 21.832072260687237
Experience 47, Iter 9, disc loss: 0.0002738448875862825, policy loss: 22.29454208349744
Experience 47, Iter 10, disc loss: 0.0002731822947669031, policy loss: 21.99452411187012
Experience 47, Iter 11, disc loss: 0.0002726004454840086, policy loss: 21.18398867974338
Experience 47, Iter 12, disc loss: 0.00027205697126900583, policy loss: 22.28292092251641
Experience 47, Iter 13, disc loss: 0.0002715021732563571, policy loss: 22.256217696839514
Experience 47, Iter 14, disc loss: 0.0002840934522024928, policy loss: 20.58437540891778
Experience 47, Iter 15, disc loss: 0.00027034775062827714, policy loss: 21.779866734227003
Experience 47, Iter 16, disc loss: 0.0002704261427891346, policy loss: 21.922864168035918
Experience 47, Iter 17, disc loss: 0.00026931324688532374, policy loss: 21.40423714871136
Experience 47, Iter 18, disc loss: 0.000268536101505331, policy loss: 21.960053906041907
Experience 47, Iter 19, disc loss: 0.00026792350272181447, policy loss: 21.29353424059608
Experience 47, Iter 20, disc loss: 0.00026724539951512855, policy loss: 22.346578947779275
Experience 47, Iter 21, disc loss: 0.00027214062572108896, policy loss: 21.108799630071374
Experience 47, Iter 22, disc loss: 0.0012068021558292884, policy loss: 21.442194571640183
Experience 47, Iter 23, disc loss: 0.00026599571547726586, policy loss: 21.644136261587594
Experience 47, Iter 24, disc loss: 0.00026530282221838623, policy loss: 20.965742669136304
Experience 47, Iter 25, disc loss: 0.0002648409370377054, policy loss: 22.230770053943274
Experience 47, Iter 26, disc loss: 0.0002648174746388007, policy loss: 21.121687966723794
Experience 47, Iter 27, disc loss: 0.0002646689663256206, policy loss: 20.611129093988716
Experience 47, Iter 28, disc loss: 0.0002643773567747726, policy loss: 21.141200334638878
Experience 47, Iter 29, disc loss: 0.00026301901062694233, policy loss: 20.937301560648795
Experience 47, Iter 30, disc loss: 0.0002643634777224668, policy loss: 20.280873534294642
Experience 47, Iter 31, disc loss: 0.00026247591967000267, policy loss: 20.369720496708137
Experience 47, Iter 32, disc loss: 0.0002622872732091893, policy loss: 20.9671320453535
Experience 47, Iter 33, disc loss: 0.00026115657033585457, policy loss: 20.877331041980153
Experience 47, Iter 34, disc loss: 0.0002632955212058342, policy loss: 21.221456456886006
Experience 47, Iter 35, disc loss: 0.00027632089279835406, policy loss: 21.222884835468665
Experience 47, Iter 36, disc loss: 0.00027390293143623074, policy loss: 19.399785284763503
Experience 47, Iter 37, disc loss: 0.00027449908682258936, policy loss: 20.400450977198282
Experience 47, Iter 38, disc loss: 0.0002631421798987484, policy loss: 20.505854369876495
Experience 47, Iter 39, disc loss: 0.00028216944507245445, policy loss: 20.747891968983403
Experience 47, Iter 40, disc loss: 0.0002577198654130849, policy loss: 20.401585835995768
Experience 47, Iter 41, disc loss: 0.0002564923993218351, policy loss: 21.19916102505805
Experience 47, Iter 42, disc loss: 0.00025628383392784643, policy loss: 20.43959859836558
Experience 47, Iter 43, disc loss: 0.00025519680606002056, policy loss: 22.42262764345131
Experience 47, Iter 44, disc loss: 0.0002549366675923611, policy loss: 21.101943777820598
Experience 47, Iter 45, disc loss: 0.0002590746507223366, policy loss: 21.539418091069525
Experience 47, Iter 46, disc loss: 0.00025923459615840227, policy loss: 20.810110670164217
Experience 47, Iter 47, disc loss: 0.00025360969952772174, policy loss: 20.24054446367106
Experience 47, Iter 48, disc loss: 0.0002631715426438455, policy loss: 20.327631119473974
Experience 47, Iter 49, disc loss: 0.0002534383609579731, policy loss: 19.527839853788212
Experience 47, Iter 50, disc loss: 0.00026424154950611435, policy loss: 20.587738150268386
Experience 47, Iter 51, disc loss: 0.0002567101529710725, policy loss: 21.267692784600932
Experience 47, Iter 52, disc loss: 0.00025310973618542616, policy loss: 20.09954089503302
Experience 47, Iter 53, disc loss: 0.000255600246209387, policy loss: 20.405023051763585
Experience 47, Iter 54, disc loss: 0.0002508391793001203, policy loss: 19.11862710567732
Experience 47, Iter 55, disc loss: 0.00024880580745486285, policy loss: 20.636213327410026
Experience 47, Iter 56, disc loss: 0.0002482857083631191, policy loss: 20.497236946523323
Experience 47, Iter 57, disc loss: 0.00024773787291459384, policy loss: 21.143840261280296
Experience 47, Iter 58, disc loss: 0.00024753930226093955, policy loss: 20.087307328535452
Experience 47, Iter 59, disc loss: 0.00024692311263044666, policy loss: 20.712384915880456
Experience 47, Iter 60, disc loss: 0.0002462200718734256, policy loss: 20.635671205156232
Experience 47, Iter 61, disc loss: 0.0002469600520597737, policy loss: 20.339533055140457
Experience 47, Iter 62, disc loss: 0.00024519741218114404, policy loss: 19.902981072790624
Experience 47, Iter 63, disc loss: 0.00024487619487066457, policy loss: 21.37848755985592
Experience 47, Iter 64, disc loss: 0.00024478854719140353, policy loss: 20.17574404555907
Experience 47, Iter 65, disc loss: 0.00024502249781447477, policy loss: 20.20987292448768
Experience 47, Iter 66, disc loss: 0.00377753987994237, policy loss: 20.643237914198348
Experience 47, Iter 67, disc loss: 0.00025555901831233064, policy loss: 21.10133719456875
Experience 47, Iter 68, disc loss: 0.00024584594844271024, policy loss: 21.042380888570662
Experience 47, Iter 69, disc loss: 0.00024346181065322808, policy loss: 21.811473411766013
Experience 47, Iter 70, disc loss: 0.0002428606723592672, policy loss: 21.32884290400242
Experience 47, Iter 71, disc loss: 0.0002445696691916421, policy loss: 20.686818590073372
Experience 47, Iter 72, disc loss: 0.00024329928876084606, policy loss: 22.365674737508947
Experience 47, Iter 73, disc loss: 0.00024243171299918808, policy loss: 21.378780958353765
Experience 47, Iter 74, disc loss: 0.0002493532928235122, policy loss: 22.15157810146671
Experience: 48
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.1871],
        [1.8715],
        [0.0278]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0545, 0.2463, 1.3289, 0.0257, 0.0136, 4.7407]],

        [[0.0545, 0.2463, 1.3289, 0.0257, 0.0136, 4.7407]],

        [[0.0545, 0.2463, 1.3289, 0.0257, 0.0136, 4.7407]],

        [[0.0545, 0.2463, 1.3289, 0.0257, 0.0136, 4.7407]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0225, 0.7484, 7.4859, 0.1113], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0225, 0.7484, 7.4859, 0.1113])
N: 720
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2881.0000, 2881.0000, 2881.0000, 2881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.296
Iter 2/2000 - Loss: 3.236
Iter 3/2000 - Loss: 3.075
Iter 4/2000 - Loss: 2.955
Iter 5/2000 - Loss: 2.860
Iter 6/2000 - Loss: 2.708
Iter 7/2000 - Loss: 2.536
Iter 8/2000 - Loss: 2.373
Iter 9/2000 - Loss: 2.202
Iter 10/2000 - Loss: 2.006
Iter 11/2000 - Loss: 1.788
Iter 12/2000 - Loss: 1.560
Iter 13/2000 - Loss: 1.325
Iter 14/2000 - Loss: 1.083
Iter 15/2000 - Loss: 0.829
Iter 16/2000 - Loss: 0.563
Iter 17/2000 - Loss: 0.287
Iter 18/2000 - Loss: 0.003
Iter 19/2000 - Loss: -0.284
Iter 20/2000 - Loss: -0.574
Iter 1981/2000 - Loss: -8.575
Iter 1982/2000 - Loss: -8.575
Iter 1983/2000 - Loss: -8.575
Iter 1984/2000 - Loss: -8.575
Iter 1985/2000 - Loss: -8.575
Iter 1986/2000 - Loss: -8.575
Iter 1987/2000 - Loss: -8.575
Iter 1988/2000 - Loss: -8.575
Iter 1989/2000 - Loss: -8.575
Iter 1990/2000 - Loss: -8.575
Iter 1991/2000 - Loss: -8.575
Iter 1992/2000 - Loss: -8.575
Iter 1993/2000 - Loss: -8.575
Iter 1994/2000 - Loss: -8.575
Iter 1995/2000 - Loss: -8.575
Iter 1996/2000 - Loss: -8.575
Iter 1997/2000 - Loss: -8.575
Iter 1998/2000 - Loss: -8.575
Iter 1999/2000 - Loss: -8.575
Iter 2000/2000 - Loss: -8.576
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.1247, 11.9232, 24.4864,  3.8968,  5.7287, 50.0028]],

        [[21.8344, 31.5273,  8.5411,  1.4198,  1.1818, 21.3847]],

        [[23.3076, 35.7648,  8.5421,  1.0074,  0.8481, 21.0741]],

        [[18.8890, 26.9182, 17.4048,  1.7675,  2.1088, 41.5852]]])
Signal Variance: tensor([0.1815, 1.4196, 9.3531, 0.4509])
Estimated target variance: tensor([0.0225, 0.7484, 7.4859, 0.1113])
N: 720
Signal to noise ratio: tensor([24.2056, 64.6929, 67.4133, 40.6980])
Bound on condition number: tensor([ 421858.4651, 3013322.0517, 3272075.4111, 1192556.9480])
Policy Optimizer learning rate:
0.009516866549228193
