Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0089],
        [0.3404],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0354, 1.3616, 0.0250], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0354, 1.3616, 0.0250])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.138
Iter 2/2000 - Loss: -0.154
Iter 3/2000 - Loss: -0.358
Iter 4/2000 - Loss: 0.185
Iter 5/2000 - Loss: 0.371
Iter 6/2000 - Loss: 0.133
Iter 7/2000 - Loss: -0.173
Iter 8/2000 - Loss: -0.334
Iter 9/2000 - Loss: -0.361
Iter 10/2000 - Loss: -0.320
Iter 11/2000 - Loss: -0.260
Iter 12/2000 - Loss: -0.222
Iter 13/2000 - Loss: -0.238
Iter 14/2000 - Loss: -0.305
Iter 15/2000 - Loss: -0.388
Iter 16/2000 - Loss: -0.448
Iter 17/2000 - Loss: -0.469
Iter 18/2000 - Loss: -0.462
Iter 19/2000 - Loss: -0.447
Iter 20/2000 - Loss: -0.439
Iter 1981/2000 - Loss: -0.693
Iter 1982/2000 - Loss: -0.702
Iter 1983/2000 - Loss: -0.693
Iter 1984/2000 - Loss: -0.701
Iter 1985/2000 - Loss: -0.694
Iter 1986/2000 - Loss: -0.701
Iter 1987/2000 - Loss: -0.695
Iter 1988/2000 - Loss: -0.702
Iter 1989/2000 - Loss: -0.697
Iter 1990/2000 - Loss: -0.702
Iter 1991/2000 - Loss: -0.699
Iter 1992/2000 - Loss: -0.702
Iter 1993/2000 - Loss: -0.700
Iter 1994/2000 - Loss: -0.701
Iter 1995/2000 - Loss: -0.701
Iter 1996/2000 - Loss: -0.700
Iter 1997/2000 - Loss: -0.702
Iter 1998/2000 - Loss: -0.701
Iter 1999/2000 - Loss: -0.703
Iter 2000/2000 - Loss: -0.701
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0063],
        [0.1966],
        [0.0045]])
Lengthscale: tensor([[[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]],

        [[0.0151, 0.0379, 0.2745, 0.0064, 0.0003, 0.0327]]])
Signal Variance: tensor([0.0027, 0.0256, 1.0289, 0.0180])
Estimated target variance: tensor([0.0036, 0.0354, 1.3616, 0.0250])
N: 10
Signal to noise ratio: tensor([2.0045, 2.0093, 2.2876, 2.0075])
Bound on condition number: tensor([41.1806, 41.3728, 53.3311, 41.3005])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.3705471752514495, policy loss: 0.5985781910106107
Experience 1, Iter 1, disc loss: 1.3628257214710726, policy loss: 0.599487645628304
Experience 1, Iter 2, disc loss: 1.3538488940639084, policy loss: 0.6009163307687725
Experience 1, Iter 3, disc loss: 1.339269244969018, policy loss: 0.6073704386797076
Experience 1, Iter 4, disc loss: 1.3287954515902758, policy loss: 0.6103715375863683
Experience 1, Iter 5, disc loss: 1.3216701476687773, policy loss: 0.6108305609939109
Experience 1, Iter 6, disc loss: 1.3066360642335395, policy loss: 0.6184965413090923
Experience 1, Iter 7, disc loss: 1.2950616412446825, policy loss: 0.6232659048573774
Experience 1, Iter 8, disc loss: 1.2836778836718237, policy loss: 0.6276608095192744
Experience 1, Iter 9, disc loss: 1.2761392809236773, policy loss: 0.6288013902019918
Experience 1, Iter 10, disc loss: 1.2634016994111512, policy loss: 0.6350759910626506
Experience 1, Iter 11, disc loss: 1.2489047579390697, policy loss: 0.6427834400907246
Experience 1, Iter 12, disc loss: 1.2360441984718737, policy loss: 0.648946042353086
Experience 1, Iter 13, disc loss: 1.2252004112095198, policy loss: 0.6539093356708483
Experience 1, Iter 14, disc loss: 1.2130370805237003, policy loss: 0.6600212505521524
Experience 1, Iter 15, disc loss: 1.2048303596221093, policy loss: 0.6620477140480869
Experience 1, Iter 16, disc loss: 1.19836663096838, policy loss: 0.6628193921862504
Experience 1, Iter 17, disc loss: 1.1804516723552938, policy loss: 0.6744654651978886
Experience 1, Iter 18, disc loss: 1.173266144979638, policy loss: 0.6756225879522104
Experience 1, Iter 19, disc loss: 1.147489234542655, policy loss: 0.6959364816527973
Experience 1, Iter 20, disc loss: 1.1421189815271893, policy loss: 0.6953191421279528
Experience 1, Iter 21, disc loss: 1.1352790497713081, policy loss: 0.6961447822666542
Experience 1, Iter 22, disc loss: 1.115030940943413, policy loss: 0.7123309555052341
Experience 1, Iter 23, disc loss: 1.1062076941672032, policy loss: 0.7149481309188128
Experience 1, Iter 24, disc loss: 1.0871725519212112, policy loss: 0.7293392559162999
Experience 1, Iter 25, disc loss: 1.0874290480315532, policy loss: 0.7232378983053711
Experience 1, Iter 26, disc loss: 1.0710663343277254, policy loss: 0.7354919540181386
Experience 1, Iter 27, disc loss: 1.0509457630030363, policy loss: 0.7524802073503767
Experience 1, Iter 28, disc loss: 1.0453592560179628, policy loss: 0.7539439920410823
Experience 1, Iter 29, disc loss: 1.0271572723982567, policy loss: 0.7699027516781178
Experience 1, Iter 30, disc loss: 1.0094325776645572, policy loss: 0.7842216336450449
Experience 1, Iter 31, disc loss: 0.9989040480068265, policy loss: 0.7923756199345893
Experience 1, Iter 32, disc loss: 0.9870872820617055, policy loss: 0.7987810574818627
Experience 1, Iter 33, disc loss: 0.977891878890915, policy loss: 0.8081516164028744
Experience 1, Iter 34, disc loss: 0.9610887474106039, policy loss: 0.8241270976566473
Experience 1, Iter 35, disc loss: 0.944437974497818, policy loss: 0.8371613136990811
Experience 1, Iter 36, disc loss: 0.9230987373651501, policy loss: 0.8592450363768023
Experience 1, Iter 37, disc loss: 0.9112443866822513, policy loss: 0.87207105081918
Experience 1, Iter 38, disc loss: 0.898856944249468, policy loss: 0.8812297387813636
Experience 1, Iter 39, disc loss: 0.8913679965556972, policy loss: 0.8853590770012952
Experience 1, Iter 40, disc loss: 0.8775760521556271, policy loss: 0.8921753763594711
Experience 1, Iter 41, disc loss: 0.8570293355293501, policy loss: 0.920101989709577
Experience 1, Iter 42, disc loss: 0.8387315968903587, policy loss: 0.939626704548872
Experience 1, Iter 43, disc loss: 0.8317889161163804, policy loss: 0.9414784828359923
Experience 1, Iter 44, disc loss: 0.8067234080842207, policy loss: 0.9753610468216268
Experience 1, Iter 45, disc loss: 0.8031480885045688, policy loss: 0.9788287559139168
Experience 1, Iter 46, disc loss: 0.7860683008948868, policy loss: 0.98980571869931
Experience 1, Iter 47, disc loss: 0.7572782777783611, policy loss: 1.038972823424979
Experience 1, Iter 48, disc loss: 0.7508464393207981, policy loss: 1.0431015114466191
Experience 1, Iter 49, disc loss: 0.7328898020720963, policy loss: 1.0728685728815694
Experience 1, Iter 50, disc loss: 0.7091272520008863, policy loss: 1.1142705335045058
Experience 1, Iter 51, disc loss: 0.6882430225454605, policy loss: 1.1432359167971815
Experience 1, Iter 52, disc loss: 0.6745025990150033, policy loss: 1.159808116564215
Experience 1, Iter 53, disc loss: 0.6847676829942155, policy loss: 1.1177386626426105
Experience 1, Iter 54, disc loss: 0.6412565964393402, policy loss: 1.220116972015366
Experience 1, Iter 55, disc loss: 0.6376015015322581, policy loss: 1.2044167965340193
Experience 1, Iter 56, disc loss: 0.6279033103445356, policy loss: 1.2272826168729738
Experience 1, Iter 57, disc loss: 0.6160023181906017, policy loss: 1.2554533738149265
Experience 1, Iter 58, disc loss: 0.5911549249026578, policy loss: 1.2939675901521195
Experience 1, Iter 59, disc loss: 0.5873883146009846, policy loss: 1.3018245650841869
Experience 1, Iter 60, disc loss: 0.569866014365976, policy loss: 1.3239232945591284
Experience 1, Iter 61, disc loss: 0.5667688464458644, policy loss: 1.3278523363834918
Experience 1, Iter 62, disc loss: 0.5198615589503158, policy loss: 1.452046119912847
Experience 1, Iter 63, disc loss: 0.5214084846360469, policy loss: 1.4504883498087993
Experience 1, Iter 64, disc loss: 0.49865379126334036, policy loss: 1.49933867546113
Experience 1, Iter 65, disc loss: 0.492523831263149, policy loss: 1.4975453565170895
Experience 1, Iter 66, disc loss: 0.4706416004564251, policy loss: 1.5475236159185868
Experience 1, Iter 67, disc loss: 0.4580450519012241, policy loss: 1.6048892440562854
Experience 1, Iter 68, disc loss: 0.4421764600187186, policy loss: 1.6168980775940152
Experience 1, Iter 69, disc loss: 0.44545192553106666, policy loss: 1.639405001814701
Experience 1, Iter 70, disc loss: 0.42445315640673187, policy loss: 1.698102216762564
Experience 1, Iter 71, disc loss: 0.42117630378910664, policy loss: 1.703076315002828
Experience 1, Iter 72, disc loss: 0.3950350954781342, policy loss: 1.7633085962372985
Experience 1, Iter 73, disc loss: 0.400842065553251, policy loss: 1.757579966068516
Experience 1, Iter 74, disc loss: 0.41702855082457824, policy loss: 1.6845806523733982
Experience 1, Iter 75, disc loss: 0.38163720724887806, policy loss: 1.7945237906321903
Experience 1, Iter 76, disc loss: 0.35940957131809226, policy loss: 1.8854415888675484
Experience 1, Iter 77, disc loss: 0.3454874801233979, policy loss: 2.033368438233527
Experience 1, Iter 78, disc loss: 0.3426175159542473, policy loss: 1.9391721980662262
Experience 1, Iter 79, disc loss: 0.3172022395401991, policy loss: 2.085245325251042
Experience 1, Iter 80, disc loss: 0.3044217665871305, policy loss: 2.0984282295501844
Experience 1, Iter 81, disc loss: 0.30839754652450935, policy loss: 2.085774206370717
Experience 1, Iter 82, disc loss: 0.2908374478960837, policy loss: 2.155814180967054
Experience 1, Iter 83, disc loss: 0.2828742277968441, policy loss: 2.2823883008483805
Experience 1, Iter 84, disc loss: 0.26765183060794806, policy loss: 2.24812122443294
Experience 1, Iter 85, disc loss: 0.2876237205965791, policy loss: 2.1872880384440037
Experience 1, Iter 86, disc loss: 0.27226983005531874, policy loss: 2.2982704279843
Experience 1, Iter 87, disc loss: 0.24294058707577149, policy loss: 2.4555733432166154
Experience 1, Iter 88, disc loss: 0.24660495119515569, policy loss: 2.4380991567387227
Experience 1, Iter 89, disc loss: 0.23437011309270828, policy loss: 2.545808978242895
Experience 1, Iter 90, disc loss: 0.23201125700221267, policy loss: 2.499957082774417
Experience 1, Iter 91, disc loss: 0.22667312487807612, policy loss: 2.5601168297989076
Experience 1, Iter 92, disc loss: 0.20613382342645087, policy loss: 2.695228756490189
Experience 1, Iter 93, disc loss: 0.20920452274715384, policy loss: 2.693218075212924
Experience 1, Iter 94, disc loss: 0.19777678511621158, policy loss: 2.727637091019692
Experience 1, Iter 95, disc loss: 0.19550764880748575, policy loss: 2.751760962500303
Experience 1, Iter 96, disc loss: 0.19048289814366465, policy loss: 2.7130198449082084
Experience 1, Iter 97, disc loss: 0.19117567899907628, policy loss: 2.844492788763189
Experience 1, Iter 98, disc loss: 0.17347538356332962, policy loss: 2.9128676794290476
Experience 1, Iter 99, disc loss: 0.16661521592931772, policy loss: 3.008263026904356
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0068],
        [0.2231],
        [0.0040]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0040, 0.0272, 0.8923, 0.0160], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0040, 0.0272, 0.8923, 0.0160])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.563
Iter 2/2000 - Loss: -0.776
Iter 3/2000 - Loss: -0.612
Iter 4/2000 - Loss: -0.425
Iter 5/2000 - Loss: -0.512
Iter 6/2000 - Loss: -0.717
Iter 7/2000 - Loss: -0.811
Iter 8/2000 - Loss: -0.755
Iter 9/2000 - Loss: -0.698
Iter 10/2000 - Loss: -0.741
Iter 11/2000 - Loss: -0.834
Iter 12/2000 - Loss: -0.897
Iter 13/2000 - Loss: -0.908
Iter 14/2000 - Loss: -0.891
Iter 15/2000 - Loss: -0.873
Iter 16/2000 - Loss: -0.876
Iter 17/2000 - Loss: -0.914
Iter 18/2000 - Loss: -0.979
Iter 19/2000 - Loss: -1.030
Iter 20/2000 - Loss: -1.032
Iter 1981/2000 - Loss: -1.101
Iter 1982/2000 - Loss: -1.106
Iter 1983/2000 - Loss: -1.105
Iter 1984/2000 - Loss: -1.102
Iter 1985/2000 - Loss: -1.106
Iter 1986/2000 - Loss: -1.105
Iter 1987/2000 - Loss: -1.104
Iter 1988/2000 - Loss: -1.107
Iter 1989/2000 - Loss: -1.106
Iter 1990/2000 - Loss: -1.104
Iter 1991/2000 - Loss: -1.107
Iter 1992/2000 - Loss: -1.106
Iter 1993/2000 - Loss: -1.105
Iter 1994/2000 - Loss: -1.107
Iter 1995/2000 - Loss: -1.106
Iter 1996/2000 - Loss: -1.106
Iter 1997/2000 - Loss: -1.107
Iter 1998/2000 - Loss: -1.106
Iter 1999/2000 - Loss: -1.106
Iter 2000/2000 - Loss: -1.107
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0051],
        [0.1481],
        [0.0030]])
Lengthscale: tensor([[[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]],

        [[0.0089, 0.0400, 0.1752, 0.0044, 0.0002, 0.0588]]])
Signal Variance: tensor([0.0031, 0.0207, 0.6996, 0.0120])
Estimated target variance: tensor([0.0040, 0.0272, 0.8923, 0.0160])
N: 20
Signal to noise ratio: tensor([2.0042, 2.0066, 2.1731, 2.0100])
Bound on condition number: tensor([81.3390, 81.5312, 95.4499, 81.8008])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.21257358125204429, policy loss: 2.4134332815309016
Experience 2, Iter 1, disc loss: 0.2082753697101809, policy loss: 2.4964731253843118
Experience 2, Iter 2, disc loss: 0.1879952790035739, policy loss: 2.5656751819406525
Experience 2, Iter 3, disc loss: 0.19948897042562444, policy loss: 2.4778890103956734
Experience 2, Iter 4, disc loss: 0.19243422567129384, policy loss: 2.5453914422636936
Experience 2, Iter 5, disc loss: 0.16660825223024306, policy loss: 2.811593578046886
Experience 2, Iter 6, disc loss: 0.17671139118732998, policy loss: 2.6761416189284524
Experience 2, Iter 7, disc loss: 0.18824983220340144, policy loss: 2.563551284654892
Experience 2, Iter 8, disc loss: 0.16082970304162025, policy loss: 2.7777101320472877
Experience 2, Iter 9, disc loss: 0.1480224452330704, policy loss: 2.924679897488082
Experience 2, Iter 10, disc loss: 0.1528628782847585, policy loss: 2.900521135110462
Experience 2, Iter 11, disc loss: 0.146177317094543, policy loss: 2.910729624159951
Experience 2, Iter 12, disc loss: 0.155464222683959, policy loss: 2.7758120379353706
Experience 2, Iter 13, disc loss: 0.13156562042506498, policy loss: 2.9850597550026206
Experience 2, Iter 14, disc loss: 0.13145347200804985, policy loss: 3.0018368563189846
Experience 2, Iter 15, disc loss: 0.11595379044638277, policy loss: 3.4259144508899615
Experience 2, Iter 16, disc loss: 0.1242795547546575, policy loss: 3.217492629899181
Experience 2, Iter 17, disc loss: 0.11769865969865922, policy loss: 3.1654193954142458
Experience 2, Iter 18, disc loss: 0.11860198949516947, policy loss: 3.134910707931131
Experience 2, Iter 19, disc loss: 0.10961222279767718, policy loss: 3.382565100644781
Experience 2, Iter 20, disc loss: 0.11979090778818188, policy loss: 3.2083611702835175
Experience 2, Iter 21, disc loss: 0.09858763184115034, policy loss: 3.4411863079215235
Experience 2, Iter 22, disc loss: 0.10196001486815753, policy loss: 3.4528835342450694
Experience 2, Iter 23, disc loss: 0.104759631240918, policy loss: 3.4200027295322286
Experience 2, Iter 24, disc loss: 0.10085236535127592, policy loss: 3.4243325685286776
Experience 2, Iter 25, disc loss: 0.09820613089246259, policy loss: 3.389668893381299
Experience 2, Iter 26, disc loss: 0.08460725571568053, policy loss: 3.609047541242986
Experience 2, Iter 27, disc loss: 0.09053279434740381, policy loss: 3.4968996476010616
Experience 2, Iter 28, disc loss: 0.091197636559528, policy loss: 3.438552524347733
Experience 2, Iter 29, disc loss: 0.0778821029326142, policy loss: 3.7412015501179807
Experience 2, Iter 30, disc loss: 0.07523169554794121, policy loss: 3.894962686334825
Experience 2, Iter 31, disc loss: 0.07884118023492355, policy loss: 3.818983447858617
Experience 2, Iter 32, disc loss: 0.07601310297383673, policy loss: 3.6935744806004474
Experience 2, Iter 33, disc loss: 0.07373328689737911, policy loss: 3.845891267926777
Experience 2, Iter 34, disc loss: 0.06633717625450387, policy loss: 3.890297335781082
Experience 2, Iter 35, disc loss: 0.07278052871874621, policy loss: 3.957747460232896
Experience 2, Iter 36, disc loss: 0.06843173605114054, policy loss: 3.8824797806877
Experience 2, Iter 37, disc loss: 0.06767247311701141, policy loss: 3.897104761029561
Experience 2, Iter 38, disc loss: 0.06200133219918323, policy loss: 4.051789677596061
Experience 2, Iter 39, disc loss: 0.06331549866602365, policy loss: 4.046855538258989
Experience 2, Iter 40, disc loss: 0.05972940765433962, policy loss: 4.2642973741746815
Experience 2, Iter 41, disc loss: 0.05897112619441289, policy loss: 4.107138514313195
Experience 2, Iter 42, disc loss: 0.05355245333614461, policy loss: 4.140967821203216
Experience 2, Iter 43, disc loss: 0.052332865514891865, policy loss: 4.191776408020452
Experience 2, Iter 44, disc loss: 0.05902085322831872, policy loss: 4.2677058024258425
Experience 2, Iter 45, disc loss: 0.05523059707578905, policy loss: 4.349481289272668
Experience 2, Iter 46, disc loss: 0.05562750324093779, policy loss: 4.263856002075913
Experience 2, Iter 47, disc loss: 0.04815583309475332, policy loss: 4.406705962493184
Experience 2, Iter 48, disc loss: 0.0547722147001645, policy loss: 4.263488401689623
Experience 2, Iter 49, disc loss: 0.045507365551078406, policy loss: 4.463396231681757
Experience 2, Iter 50, disc loss: 0.046546365258777485, policy loss: 4.325473507319064
Experience 2, Iter 51, disc loss: 0.04704523481503823, policy loss: 4.527056878788446
Experience 2, Iter 52, disc loss: 0.04635778964913878, policy loss: 4.486765195703541
Experience 2, Iter 53, disc loss: 0.04173638369358606, policy loss: 4.607889494765626
Experience 2, Iter 54, disc loss: 0.045874335814645326, policy loss: 4.577709194488593
Experience 2, Iter 55, disc loss: 0.04183694059595122, policy loss: 4.546216633249854
Experience 2, Iter 56, disc loss: 0.03661862435987404, policy loss: 4.897207487854374
Experience 2, Iter 57, disc loss: 0.040204357648569264, policy loss: 4.481245158044906
Experience 2, Iter 58, disc loss: 0.04309803898635449, policy loss: 4.596806951075427
Experience 2, Iter 59, disc loss: 0.033728452224062615, policy loss: 4.771506085583439
Experience 2, Iter 60, disc loss: 0.03218909592110223, policy loss: 4.929770520766387
Experience 2, Iter 61, disc loss: 0.03451018772971813, policy loss: 4.753885726847619
Experience 2, Iter 62, disc loss: 0.03522868656134645, policy loss: 4.693431225244144
Experience 2, Iter 63, disc loss: 0.03150202854161971, policy loss: 4.903351058084006
Experience 2, Iter 64, disc loss: 0.03323745350533183, policy loss: 4.759636436956136
Experience 2, Iter 65, disc loss: 0.03318066454889493, policy loss: 4.7995214633612076
Experience 2, Iter 66, disc loss: 0.031990054285455136, policy loss: 4.807695037324726
Experience 2, Iter 67, disc loss: 0.034698087490101844, policy loss: 5.05387873539299
Experience 2, Iter 68, disc loss: 0.02971339832075352, policy loss: 5.202490649716241
Experience 2, Iter 69, disc loss: 0.02697698489768334, policy loss: 4.923472334130675
Experience 2, Iter 70, disc loss: 0.029743791164595974, policy loss: 4.990426117639412
Experience 2, Iter 71, disc loss: 0.03133234507994013, policy loss: 4.961012687066217
Experience 2, Iter 72, disc loss: 0.027304019621362693, policy loss: 5.174593943199415
Experience 2, Iter 73, disc loss: 0.02917923177404659, policy loss: 4.92687513508522
Experience 2, Iter 74, disc loss: 0.026252242900892273, policy loss: 4.92955440246406
Experience 2, Iter 75, disc loss: 0.024876510495663258, policy loss: 5.281398020176771
Experience 2, Iter 76, disc loss: 0.02434959722199299, policy loss: 5.291127665259195
Experience 2, Iter 77, disc loss: 0.021225979074456336, policy loss: 5.483175275630975
Experience 2, Iter 78, disc loss: 0.025198601835205076, policy loss: 5.159113601991827
Experience 2, Iter 79, disc loss: 0.0208136597684169, policy loss: 5.593686032776313
Experience 2, Iter 80, disc loss: 0.02323834971916454, policy loss: 5.312783259471851
Experience 2, Iter 81, disc loss: 0.024409524502964106, policy loss: 5.031566782658763
Experience 2, Iter 82, disc loss: 0.02215296208378427, policy loss: 5.255477403693924
Experience 2, Iter 83, disc loss: 0.01846902115612779, policy loss: 5.7463630623926925
Experience 2, Iter 84, disc loss: 0.018219391299141567, policy loss: 5.846308865615049
Experience 2, Iter 85, disc loss: 0.01836243015457833, policy loss: 5.623152105967767
Experience 2, Iter 86, disc loss: 0.023035800695120102, policy loss: 5.358693425845797
Experience 2, Iter 87, disc loss: 0.01961090407186514, policy loss: 5.498452841398926
Experience 2, Iter 88, disc loss: 0.018875834434703198, policy loss: 5.39154049891904
Experience 2, Iter 89, disc loss: 0.01767153617499572, policy loss: 5.776599012889976
Experience 2, Iter 90, disc loss: 0.01968238587713075, policy loss: 5.602997663637615
Experience 2, Iter 91, disc loss: 0.020736714105925605, policy loss: 5.211309984053289
Experience 2, Iter 92, disc loss: 0.01733431940218829, policy loss: 5.850120435899935
Experience 2, Iter 93, disc loss: 0.016266112589054, policy loss: 5.593075776191736
Experience 2, Iter 94, disc loss: 0.017909972247874637, policy loss: 5.567295054978626
Experience 2, Iter 95, disc loss: 0.018108190594846717, policy loss: 5.44156405462746
Experience 2, Iter 96, disc loss: 0.017986940695016867, policy loss: 5.4289472146585505
Experience 2, Iter 97, disc loss: 0.016279397762873654, policy loss: 5.74692850281723
Experience 2, Iter 98, disc loss: 0.02408927538701265, policy loss: 5.661070159407032
Experience 2, Iter 99, disc loss: 0.018282068600142413, policy loss: 5.505747487865484
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0089],
        [0.3876],
        [0.0080]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0043, 0.0356, 1.5502, 0.0319], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0043, 0.0356, 1.5502, 0.0319])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.517
Iter 2/2000 - Loss: 0.073
Iter 3/2000 - Loss: 0.128
Iter 4/2000 - Loss: 0.411
Iter 5/2000 - Loss: 0.364
Iter 6/2000 - Loss: 0.150
Iter 7/2000 - Loss: 0.013
Iter 8/2000 - Loss: 0.025
Iter 9/2000 - Loss: 0.104
Iter 10/2000 - Loss: 0.127
Iter 11/2000 - Loss: 0.064
Iter 12/2000 - Loss: -0.033
Iter 13/2000 - Loss: -0.099
Iter 14/2000 - Loss: -0.109
Iter 15/2000 - Loss: -0.082
Iter 16/2000 - Loss: -0.053
Iter 17/2000 - Loss: -0.052
Iter 18/2000 - Loss: -0.088
Iter 19/2000 - Loss: -0.147
Iter 20/2000 - Loss: -0.198
Iter 1981/2000 - Loss: -0.288
Iter 1982/2000 - Loss: -0.288
Iter 1983/2000 - Loss: -0.288
Iter 1984/2000 - Loss: -0.288
Iter 1985/2000 - Loss: -0.288
Iter 1986/2000 - Loss: -0.288
Iter 1987/2000 - Loss: -0.288
Iter 1988/2000 - Loss: -0.288
Iter 1989/2000 - Loss: -0.288
Iter 1990/2000 - Loss: -0.288
Iter 1991/2000 - Loss: -0.288
Iter 1992/2000 - Loss: -0.288
Iter 1993/2000 - Loss: -0.288
Iter 1994/2000 - Loss: -0.288
Iter 1995/2000 - Loss: -0.288
Iter 1996/2000 - Loss: -0.288
Iter 1997/2000 - Loss: -0.288
Iter 1998/2000 - Loss: -0.288
Iter 1999/2000 - Loss: -0.288
Iter 2000/2000 - Loss: -0.288
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0068],
        [0.2447],
        [0.0061]])
Lengthscale: tensor([[[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]],

        [[0.0109, 0.0437, 0.3595, 0.0081, 0.0008, 0.0811]]])
Signal Variance: tensor([0.0033, 0.0276, 1.2539, 0.0247])
Estimated target variance: tensor([0.0043, 0.0356, 1.5502, 0.0319])
N: 30
Signal to noise ratio: tensor([2.0030, 2.0087, 2.2637, 2.0100])
Bound on condition number: tensor([121.3656, 122.0495, 154.7314, 122.2043])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.015421191899360354, policy loss: 6.31731579209462
Experience 3, Iter 1, disc loss: 0.012956299070716493, policy loss: 6.426671829219392
Experience 3, Iter 2, disc loss: 0.014246812668981689, policy loss: 6.314363937417845
Experience 3, Iter 3, disc loss: 0.015796343867823526, policy loss: 6.321130268846977
Experience 3, Iter 4, disc loss: 0.013930121701177408, policy loss: 6.180655234272298
Experience 3, Iter 5, disc loss: 0.013697792607955402, policy loss: 6.3085574724486175
Experience 3, Iter 6, disc loss: 0.01110105327221977, policy loss: 6.696079929410931
Experience 3, Iter 7, disc loss: 0.011957266740239116, policy loss: 6.21840368696098
Experience 3, Iter 8, disc loss: 0.012364184120691093, policy loss: 6.661885334681879
Experience 3, Iter 9, disc loss: 0.011686982037155813, policy loss: 6.482995690878878
Experience 3, Iter 10, disc loss: 0.011615353703685872, policy loss: 6.5227813206762955
Experience 3, Iter 11, disc loss: 0.013196786515780311, policy loss: 6.345517601011662
Experience 3, Iter 12, disc loss: 0.012706860494688926, policy loss: 6.468046757101028
Experience 3, Iter 13, disc loss: 0.01342596321859455, policy loss: 6.490253393045038
Experience 3, Iter 14, disc loss: 0.011608758291908591, policy loss: 7.0026268749689695
Experience 3, Iter 15, disc loss: 0.013277883085570805, policy loss: 6.184289918560859
Experience 3, Iter 16, disc loss: 0.010405588403708636, policy loss: 6.904289424482503
Experience 3, Iter 17, disc loss: 0.01179674988177208, policy loss: 6.69088158453813
Experience 3, Iter 18, disc loss: 0.009835183338055623, policy loss: 6.805639649876461
Experience 3, Iter 19, disc loss: 0.008835891666858248, policy loss: 7.125482006985903
Experience 3, Iter 20, disc loss: 0.011515319932833319, policy loss: 6.8640552831837764
Experience 3, Iter 21, disc loss: 0.009431019406039342, policy loss: 6.908504143489316
Experience 3, Iter 22, disc loss: 0.009072295077778495, policy loss: 6.654386720869816
Experience 3, Iter 23, disc loss: 0.010093405270056728, policy loss: 6.487509986500237
Experience 3, Iter 24, disc loss: 0.012942793525074908, policy loss: 6.567979335079718
Experience 3, Iter 25, disc loss: 0.009709521681825702, policy loss: 6.651047970196331
Experience 3, Iter 26, disc loss: 0.00913455324398153, policy loss: 6.794495585432122
Experience 3, Iter 27, disc loss: 0.0097613073019431, policy loss: 7.01427974648181
Experience 3, Iter 28, disc loss: 0.009156691249423594, policy loss: 6.502888858060498
Experience 3, Iter 29, disc loss: 0.009104590052132832, policy loss: 6.81843526388201
Experience 3, Iter 30, disc loss: 0.00935569621947653, policy loss: 6.769428430142487
Experience 3, Iter 31, disc loss: 0.009255151185227331, policy loss: 6.830136641584341
Experience 3, Iter 32, disc loss: 0.00785824819185285, policy loss: 6.946426813279475
Experience 3, Iter 33, disc loss: 0.00884389151791308, policy loss: 6.96268159376282
Experience 3, Iter 34, disc loss: 0.007991703675972257, policy loss: 7.058450948581162
Experience 3, Iter 35, disc loss: 0.00905394069489246, policy loss: 6.853811402218426
Experience 3, Iter 36, disc loss: 0.00673007879939091, policy loss: 7.406826059529882
Experience 3, Iter 37, disc loss: 0.008398067291085335, policy loss: 7.140502236513283
Experience 3, Iter 38, disc loss: 0.008435066898617467, policy loss: 6.921650787105794
Experience 3, Iter 39, disc loss: 0.008361312749332986, policy loss: 6.865257597175992
Experience 3, Iter 40, disc loss: 0.00893933751597939, policy loss: 7.130411965772147
Experience 3, Iter 41, disc loss: 0.008005962957314604, policy loss: 6.951373457205948
Experience 3, Iter 42, disc loss: 0.009513172240440834, policy loss: 7.043362323598694
Experience 3, Iter 43, disc loss: 0.007131479107425654, policy loss: 6.850235640910913
Experience 3, Iter 44, disc loss: 0.007570404401928823, policy loss: 6.786338179353403
Experience 3, Iter 45, disc loss: 0.008957345556098091, policy loss: 6.956835771735076
Experience 3, Iter 46, disc loss: 0.007178137761133623, policy loss: 7.09683915795954
Experience 3, Iter 47, disc loss: 0.006760481019875256, policy loss: 7.134902064823329
Experience 3, Iter 48, disc loss: 0.006480320815776463, policy loss: 6.970572624789954
Experience 3, Iter 49, disc loss: 0.008195537575274268, policy loss: 6.959643133890128
Experience 3, Iter 50, disc loss: 0.009535472065101669, policy loss: 6.622169415976315
Experience 3, Iter 51, disc loss: 0.0076164385622078095, policy loss: 6.981133560078048
Experience 3, Iter 52, disc loss: 0.0076570202063636385, policy loss: 7.046364505900895
Experience 3, Iter 53, disc loss: 0.007363335795941976, policy loss: 7.290947251696334
Experience 3, Iter 54, disc loss: 0.0068793516945352485, policy loss: 7.447002570616521
Experience 3, Iter 55, disc loss: 0.007367239561946589, policy loss: 7.239688751665733
Experience 3, Iter 56, disc loss: 0.0067809148155903295, policy loss: 6.983762146443792
Experience 3, Iter 57, disc loss: 0.006207503393957853, policy loss: 7.276426594939471
Experience 3, Iter 58, disc loss: 0.0065918330344196165, policy loss: 7.1172094752451835
Experience 3, Iter 59, disc loss: 0.008726928595515701, policy loss: 6.847390083996552
Experience 3, Iter 60, disc loss: 0.005563058263906837, policy loss: 7.513059583650726
Experience 3, Iter 61, disc loss: 0.006373730713977462, policy loss: 7.02410296726092
Experience 3, Iter 62, disc loss: 0.006446130110588909, policy loss: 7.137790440949197
Experience 3, Iter 63, disc loss: 0.005934810826224048, policy loss: 7.417240244471946
Experience 3, Iter 64, disc loss: 0.006167028132184875, policy loss: 7.532243786962703
Experience 3, Iter 65, disc loss: 0.007166908732086117, policy loss: 6.660305718825744
Experience 3, Iter 66, disc loss: 0.0064248359094167835, policy loss: 7.328925207652102
Experience 3, Iter 67, disc loss: 0.005926150926162183, policy loss: 7.4634024827680685
Experience 3, Iter 68, disc loss: 0.006467002557963717, policy loss: 7.117228204227261
Experience 3, Iter 69, disc loss: 0.006834712790216105, policy loss: 7.086454689632582
Experience 3, Iter 70, disc loss: 0.006673507253110307, policy loss: 7.180198889621343
Experience 3, Iter 71, disc loss: 0.006298343207025617, policy loss: 6.9281944439900105
Experience 3, Iter 72, disc loss: 0.005053005358134776, policy loss: 7.68070330646793
Experience 3, Iter 73, disc loss: 0.004999868816197559, policy loss: 7.988549594671777
Experience 3, Iter 74, disc loss: 0.006153074483360409, policy loss: 7.422172474926601
Experience 3, Iter 75, disc loss: 0.006252412036178861, policy loss: 7.376042603367441
Experience 3, Iter 76, disc loss: 0.005023387031491148, policy loss: 7.290610898342743
Experience 3, Iter 77, disc loss: 0.004929256817299783, policy loss: 7.3794538705541495
Experience 3, Iter 78, disc loss: 0.006236549712278803, policy loss: 7.437660744332348
Experience 3, Iter 79, disc loss: 0.010258624939428494, policy loss: 7.407537147882408
Experience 3, Iter 80, disc loss: 0.005294702991303354, policy loss: 7.415298163965749
Experience 3, Iter 81, disc loss: 0.004718381985632998, policy loss: 7.226323912114633
Experience 3, Iter 82, disc loss: 0.004710071778297771, policy loss: 7.363749833106705
Experience 3, Iter 83, disc loss: 0.0063275367156281, policy loss: 7.415543192328568
Experience 3, Iter 84, disc loss: 0.004853852635669652, policy loss: 7.564861869579663
Experience 3, Iter 85, disc loss: 0.005211692271320903, policy loss: 7.585279778410271
Experience 3, Iter 86, disc loss: 0.004482357283168065, policy loss: 7.718190442125903
Experience 3, Iter 87, disc loss: 0.004349686484343888, policy loss: 7.501510301135733
Experience 3, Iter 88, disc loss: 0.006228831570544019, policy loss: 7.1864659774240405
Experience 3, Iter 89, disc loss: 0.006903219193194939, policy loss: 7.2336819655007725
Experience 3, Iter 90, disc loss: 0.0048883387993544514, policy loss: 7.498050266580815
Experience 3, Iter 91, disc loss: 0.006170949352853576, policy loss: 7.288710642022387
Experience 3, Iter 92, disc loss: 0.004008585671130136, policy loss: 8.126344738175455
Experience 3, Iter 93, disc loss: 0.00502508399048, policy loss: 7.409755876832693
Experience 3, Iter 94, disc loss: 0.0036454416284502854, policy loss: 7.721210063870014
Experience 3, Iter 95, disc loss: 0.005273140051066598, policy loss: 7.017902491311247
Experience 3, Iter 96, disc loss: 0.005126343234386724, policy loss: 7.447623325646543
Experience 3, Iter 97, disc loss: 0.004779349948295558, policy loss: 7.919952514624031
Experience 3, Iter 98, disc loss: 0.003964752390386752, policy loss: 7.510462401918943
Experience 3, Iter 99, disc loss: 0.0046072077243187355, policy loss: 7.650191587459714
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0073],
        [0.3209],
        [0.0065]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0293, 1.2838, 0.0261], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0293, 1.2838, 0.0261])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.262
Iter 2/2000 - Loss: -0.245
Iter 3/2000 - Loss: -0.147
Iter 4/2000 - Loss: 0.127
Iter 5/2000 - Loss: 0.051
Iter 6/2000 - Loss: -0.179
Iter 7/2000 - Loss: -0.317
Iter 8/2000 - Loss: -0.296
Iter 9/2000 - Loss: -0.208
Iter 10/2000 - Loss: -0.186
Iter 11/2000 - Loss: -0.267
Iter 12/2000 - Loss: -0.385
Iter 13/2000 - Loss: -0.455
Iter 14/2000 - Loss: -0.446
Iter 15/2000 - Loss: -0.392
Iter 16/2000 - Loss: -0.354
Iter 17/2000 - Loss: -0.374
Iter 18/2000 - Loss: -0.441
Iter 19/2000 - Loss: -0.515
Iter 20/2000 - Loss: -0.554
Iter 1981/2000 - Loss: -0.645
Iter 1982/2000 - Loss: -0.645
Iter 1983/2000 - Loss: -0.645
Iter 1984/2000 - Loss: -0.645
Iter 1985/2000 - Loss: -0.645
Iter 1986/2000 - Loss: -0.645
Iter 1987/2000 - Loss: -0.645
Iter 1988/2000 - Loss: -0.645
Iter 1989/2000 - Loss: -0.645
Iter 1990/2000 - Loss: -0.645
Iter 1991/2000 - Loss: -0.645
Iter 1992/2000 - Loss: -0.645
Iter 1993/2000 - Loss: -0.645
Iter 1994/2000 - Loss: -0.645
Iter 1995/2000 - Loss: -0.645
Iter 1996/2000 - Loss: -0.645
Iter 1997/2000 - Loss: -0.645
Iter 1998/2000 - Loss: -0.645
Iter 1999/2000 - Loss: -0.645
Iter 2000/2000 - Loss: -0.645
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0057],
        [0.2112],
        [0.0050]])
Lengthscale: tensor([[[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]],

        [[0.0097, 0.0391, 0.2948, 0.0069, 0.0006, 0.0771]]])
Signal Variance: tensor([0.0029, 0.0229, 1.0405, 0.0204])
Estimated target variance: tensor([0.0037, 0.0293, 1.2838, 0.0261])
N: 40
Signal to noise ratio: tensor([2.0012, 2.0074, 2.2197, 2.0091])
Bound on condition number: tensor([161.1922, 162.1915, 198.0850, 162.4672])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.005979496599366323, policy loss: 6.769580567307081
Experience 4, Iter 1, disc loss: 0.004869354753821508, policy loss: 7.159672248316892
Experience 4, Iter 2, disc loss: 0.005782925905874985, policy loss: 6.951993455844656
Experience 4, Iter 3, disc loss: 0.006600093515845045, policy loss: 6.885137184987293
Experience 4, Iter 4, disc loss: 0.0055978635878315836, policy loss: 7.164028528972052
Experience 4, Iter 5, disc loss: 0.0044907265882804534, policy loss: 7.280492620717709
Experience 4, Iter 6, disc loss: 0.004550887149464068, policy loss: 7.479858577918521
Experience 4, Iter 7, disc loss: 0.005644215839650027, policy loss: 7.06124299016048
Experience 4, Iter 8, disc loss: 0.004529944003402828, policy loss: 7.263938902627965
Experience 4, Iter 9, disc loss: 0.005864054874757246, policy loss: 6.649595765896761
Experience 4, Iter 10, disc loss: 0.005259778652015009, policy loss: 6.993041005335259
Experience 4, Iter 11, disc loss: 0.004826018493355186, policy loss: 7.089243591184856
Experience 4, Iter 12, disc loss: 0.003807155706810478, policy loss: 7.6731106739833095
Experience 4, Iter 13, disc loss: 0.004531467036419308, policy loss: 7.131098233940068
Experience 4, Iter 14, disc loss: 0.003430016380837395, policy loss: 7.670303157315761
Experience 4, Iter 15, disc loss: 0.004021116393666738, policy loss: 7.446948272522028
Experience 4, Iter 16, disc loss: 0.004240796399110105, policy loss: 7.2192670841784095
Experience 4, Iter 17, disc loss: 0.0037848345870224033, policy loss: 7.6570936339503195
Experience 4, Iter 18, disc loss: 0.0038210792026158722, policy loss: 7.478159573109624
Experience 4, Iter 19, disc loss: 0.0064893428105124865, policy loss: 6.644192578742773
Experience 4, Iter 20, disc loss: 0.003475068036081208, policy loss: 7.704079439388197
Experience 4, Iter 21, disc loss: 0.004699733630617714, policy loss: 7.3276115859234086
Experience 4, Iter 22, disc loss: 0.003940233323019952, policy loss: 7.340547203073482
Experience 4, Iter 23, disc loss: 0.004052489159996337, policy loss: 7.1034662525009225
Experience 4, Iter 24, disc loss: 0.005000661061044035, policy loss: 7.250667822419035
Experience 4, Iter 25, disc loss: 0.00621024401974667, policy loss: 7.620382174589545
Experience 4, Iter 26, disc loss: 0.003881617077225041, policy loss: 7.51100997477125
Experience 4, Iter 27, disc loss: 0.004326326410317051, policy loss: 7.365657528758102
Experience 4, Iter 28, disc loss: 0.004082824917073195, policy loss: 7.674968308072591
Experience 4, Iter 29, disc loss: 0.003641943060273642, policy loss: 7.258053076089123
Experience 4, Iter 30, disc loss: 0.004254537810930581, policy loss: 7.225541693352039
Experience 4, Iter 31, disc loss: 0.0056903246186682886, policy loss: 7.001502328296514
Experience 4, Iter 32, disc loss: 0.0038864567550972234, policy loss: 7.2315475351154515
Experience 4, Iter 33, disc loss: 0.003869870991174474, policy loss: 7.126217429777831
Experience 4, Iter 34, disc loss: 0.004271546518477375, policy loss: 7.112618368927014
Experience 4, Iter 35, disc loss: 0.00479177041718255, policy loss: 7.52350537390125
Experience 4, Iter 36, disc loss: 0.004176279177815446, policy loss: 7.294426893705597
Experience 4, Iter 37, disc loss: 0.003850035245229516, policy loss: 7.524368528642503
Experience 4, Iter 38, disc loss: 0.0034364506622071465, policy loss: 7.464413905965263
Experience 4, Iter 39, disc loss: 0.003781822034890546, policy loss: 7.481397560175459
Experience 4, Iter 40, disc loss: 0.003803943358510028, policy loss: 7.330605245165867
Experience 4, Iter 41, disc loss: 0.003018366384234237, policy loss: 7.637135574966228
Experience 4, Iter 42, disc loss: 0.00349429452433298, policy loss: 7.4066201727398475
Experience 4, Iter 43, disc loss: 0.004964525213174985, policy loss: 6.918842563375438
Experience 4, Iter 44, disc loss: 0.0032144215107795587, policy loss: 7.506757524532589
Experience 4, Iter 45, disc loss: 0.004470426784688203, policy loss: 7.391356207624032
Experience 4, Iter 46, disc loss: 0.0037510728671896615, policy loss: 7.447153349975878
Experience 4, Iter 47, disc loss: 0.0031401766867272065, policy loss: 8.030722814341114
Experience 4, Iter 48, disc loss: 0.0029667113817163984, policy loss: 7.6890104090011135
Experience 4, Iter 49, disc loss: 0.002874121358290519, policy loss: 7.87591083810807
Experience 4, Iter 50, disc loss: 0.0033002164036695335, policy loss: 7.6992370495664275
Experience 4, Iter 51, disc loss: 0.0029786329769721696, policy loss: 7.709826760776774
Experience 4, Iter 52, disc loss: 0.003554044611906011, policy loss: 7.693768138677941
Experience 4, Iter 53, disc loss: 0.0030156376275484367, policy loss: 8.012741742335052
Experience 4, Iter 54, disc loss: 0.0038492208501143097, policy loss: 7.663560204093879
Experience 4, Iter 55, disc loss: 0.004823596096740617, policy loss: 7.423102316927722
Experience 4, Iter 56, disc loss: 0.003041239950120935, policy loss: 7.688544494297088
Experience 4, Iter 57, disc loss: 0.003548380079761669, policy loss: 7.575422234302039
Experience 4, Iter 58, disc loss: 0.0027849205221252295, policy loss: 7.880946796983488
Experience 4, Iter 59, disc loss: 0.003947682425553097, policy loss: 7.246772482805159
Experience 4, Iter 60, disc loss: 0.0030463849355808854, policy loss: 7.438811709339992
Experience 4, Iter 61, disc loss: 0.003665956490638581, policy loss: 7.940215897838745
Experience 4, Iter 62, disc loss: 0.00305665239177104, policy loss: 7.701019498052011
Experience 4, Iter 63, disc loss: 0.0027986991337889955, policy loss: 8.1648805293406
Experience 4, Iter 64, disc loss: 0.002568411694004861, policy loss: 7.812813198063545
Experience 4, Iter 65, disc loss: 0.003412193879978398, policy loss: 7.392611533315365
Experience 4, Iter 66, disc loss: 0.0028720509088498696, policy loss: 8.010817919421395
Experience 4, Iter 67, disc loss: 0.0028334699025088462, policy loss: 8.030854658901527
Experience 4, Iter 68, disc loss: 0.004180375274421852, policy loss: 7.426374988895887
Experience 4, Iter 69, disc loss: 0.003999082866294793, policy loss: 7.360756446597294
Experience 4, Iter 70, disc loss: 0.0026931236212361775, policy loss: 7.6610470240400215
Experience 4, Iter 71, disc loss: 0.003117027677430522, policy loss: 8.036153491710476
Experience 4, Iter 72, disc loss: 0.00255920424704922, policy loss: 8.004379226467778
Experience 4, Iter 73, disc loss: 0.003083220382342663, policy loss: 7.737546510038495
Experience 4, Iter 74, disc loss: 0.002874290718217201, policy loss: 7.9724875529681025
Experience 4, Iter 75, disc loss: 0.0032179745337187053, policy loss: 8.039760114132573
Experience 4, Iter 76, disc loss: 0.0025138733800206675, policy loss: 7.818380283851495
Experience 4, Iter 77, disc loss: 0.0025767995058438424, policy loss: 8.189511620930531
Experience 4, Iter 78, disc loss: 0.0038403691956063156, policy loss: 7.746208454271491
Experience 4, Iter 79, disc loss: 0.003599287441616881, policy loss: 7.286820624923644
Experience 4, Iter 80, disc loss: 0.002799505737945469, policy loss: 7.972917464819379
Experience 4, Iter 81, disc loss: 0.0026970004417168686, policy loss: 7.818774388225465
Experience 4, Iter 82, disc loss: 0.002790372707795855, policy loss: 8.026517552668569
Experience 4, Iter 83, disc loss: 0.002882832598194671, policy loss: 7.658810069850797
Experience 4, Iter 84, disc loss: 0.0023559007498180655, policy loss: 7.947929001465599
Experience 4, Iter 85, disc loss: 0.0023008434316900456, policy loss: 8.046623779291883
Experience 4, Iter 86, disc loss: 0.0028406829178300947, policy loss: 8.139801668222317
Experience 4, Iter 87, disc loss: 0.0026755494757298593, policy loss: 8.263635742479188
Experience 4, Iter 88, disc loss: 0.0028337727864376277, policy loss: 7.823938803758554
Experience 4, Iter 89, disc loss: 0.002641852634661991, policy loss: 7.932025304196021
Experience 4, Iter 90, disc loss: 0.0027249009164000506, policy loss: 8.003929589764082
Experience 4, Iter 91, disc loss: 0.002970332245475638, policy loss: 7.698051015602466
Experience 4, Iter 92, disc loss: 0.002161733874439093, policy loss: 7.935282942550253
Experience 4, Iter 93, disc loss: 0.0030236085286041376, policy loss: 7.739464767757155
Experience 4, Iter 94, disc loss: 0.003862116006682652, policy loss: 7.79388064476702
Experience 4, Iter 95, disc loss: 0.0024489893189876067, policy loss: 7.8533487187062185
Experience 4, Iter 96, disc loss: 0.0020750344542294352, policy loss: 8.088727493395394
Experience 4, Iter 97, disc loss: 0.003228925709002646, policy loss: 8.374706523701214
Experience 4, Iter 98, disc loss: 0.0021749769915425958, policy loss: 8.157483958327889
Experience 4, Iter 99, disc loss: 0.0026993425782650465, policy loss: 7.379934673939273
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0065],
        [0.2560],
        [0.0052]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0088, 0.0398, 0.2350, 0.0056, 0.0005, 0.0914]],

        [[0.0088, 0.0398, 0.2350, 0.0056, 0.0005, 0.0914]],

        [[0.0088, 0.0398, 0.2350, 0.0056, 0.0005, 0.0914]],

        [[0.0088, 0.0398, 0.2350, 0.0056, 0.0005, 0.0914]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0261, 1.0241, 0.0208], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0261, 1.0241, 0.0208])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.526
Iter 2/2000 - Loss: -0.458
Iter 3/2000 - Loss: -0.366
Iter 4/2000 - Loss: -0.265
Iter 5/2000 - Loss: -0.336
Iter 6/2000 - Loss: -0.472
Iter 7/2000 - Loss: -0.532
Iter 8/2000 - Loss: -0.514
Iter 9/2000 - Loss: -0.505
Iter 10/2000 - Loss: -0.557
Iter 11/2000 - Loss: -0.652
Iter 12/2000 - Loss: -0.723
Iter 13/2000 - Loss: -0.732
Iter 14/2000 - Loss: -0.698
Iter 15/2000 - Loss: -0.685
Iter 16/2000 - Loss: -0.741
Iter 17/2000 - Loss: -0.850
Iter 18/2000 - Loss: -0.945
Iter 19/2000 - Loss: -0.977
Iter 20/2000 - Loss: -0.959
Iter 1981/2000 - Loss: -8.093
Iter 1982/2000 - Loss: -8.093
Iter 1983/2000 - Loss: -8.093
Iter 1984/2000 - Loss: -8.093
Iter 1985/2000 - Loss: -8.093
Iter 1986/2000 - Loss: -8.093
Iter 1987/2000 - Loss: -8.093
Iter 1988/2000 - Loss: -8.093
Iter 1989/2000 - Loss: -8.093
Iter 1990/2000 - Loss: -8.093
Iter 1991/2000 - Loss: -8.093
Iter 1992/2000 - Loss: -8.093
Iter 1993/2000 - Loss: -8.093
Iter 1994/2000 - Loss: -8.093
Iter 1995/2000 - Loss: -8.093
Iter 1996/2000 - Loss: -8.093
Iter 1997/2000 - Loss: -8.093
Iter 1998/2000 - Loss: -8.093
Iter 1999/2000 - Loss: -8.093
Iter 2000/2000 - Loss: -8.093
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0011],
        [0.0003]])
Lengthscale: tensor([[[ 6.7406,  0.8973, 34.0489, 11.8450,  1.6675, 10.9029]],

        [[18.0043, 26.5461, 12.4117,  0.7239,  4.8582,  5.8304]],

        [[28.6926, 36.7442, 19.4935,  0.8447,  0.8568, 12.0810]],

        [[24.2103, 35.3588, 12.3799,  4.1078, 11.9304, 30.1068]]])
Signal Variance: tensor([0.0086, 0.1597, 8.4957, 0.3409])
Estimated target variance: tensor([0.0041, 0.0261, 1.0241, 0.0208])
N: 50
Signal to noise ratio: tensor([ 6.5081, 20.6083, 86.3723, 36.7450])
Bound on condition number: tensor([  2118.7357,  21236.0293, 373009.5613,  67510.8784])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.06522609098959715, policy loss: 3.0422367999203312
Experience 5, Iter 1, disc loss: 0.09540346499370424, policy loss: 2.8415101623971055
Experience 5, Iter 2, disc loss: 0.15476047641084273, policy loss: 2.5243024681989974
Experience 5, Iter 3, disc loss: 0.2651098331704728, policy loss: 2.047875948836411
Experience 5, Iter 4, disc loss: 0.39587426905467205, policy loss: 1.8146498911932953
Experience 5, Iter 5, disc loss: 0.7572092723935868, policy loss: 1.4408538856461015
Experience 5, Iter 6, disc loss: 1.0295615102014306, policy loss: 1.3974064974609104
Experience 5, Iter 7, disc loss: 1.4334618213947539, policy loss: 1.3870964138338446
Experience 5, Iter 8, disc loss: 1.2322040685794537, policy loss: 1.5538246856455113
Experience 5, Iter 9, disc loss: 1.0655562973474877, policy loss: 2.3224253831973902
Experience 5, Iter 10, disc loss: 1.0453528171168522, policy loss: 2.573690149413549
Experience 5, Iter 11, disc loss: 0.7374552401756069, policy loss: 3.4574610505016277
Experience 5, Iter 12, disc loss: 0.5515106308219866, policy loss: 2.362333938422577
Experience 5, Iter 13, disc loss: 0.3642819880014027, policy loss: 3.537198461634735
Experience 5, Iter 14, disc loss: 0.4706039043779768, policy loss: 3.0235660530504247
Experience 5, Iter 15, disc loss: 0.36094556323332505, policy loss: 2.929135803127086
Experience 5, Iter 16, disc loss: 0.39927993698624886, policy loss: 2.5704711682853723
Experience 5, Iter 17, disc loss: 0.4064393073700448, policy loss: 2.7441016099774895
Experience 5, Iter 18, disc loss: 0.39248504046277966, policy loss: 2.9529482155608813
Experience 5, Iter 19, disc loss: 0.36779425735671345, policy loss: 3.063691182934397
Experience 5, Iter 20, disc loss: 0.33866130076867024, policy loss: 3.022558665009041
Experience 5, Iter 21, disc loss: 0.30197894224178523, policy loss: 2.940135285210128
Experience 5, Iter 22, disc loss: 0.26006426738688393, policy loss: 2.922227125962522
Experience 5, Iter 23, disc loss: 0.22768160269939292, policy loss: 2.8043317881434353
Experience 5, Iter 24, disc loss: 0.22785736699671527, policy loss: 2.4871947321700074
Experience 5, Iter 25, disc loss: 0.2524959741645943, policy loss: 2.2176486106593187
Experience 5, Iter 26, disc loss: 0.2996922719180445, policy loss: 2.0224394431269928
Experience 5, Iter 27, disc loss: 0.3248686494961853, policy loss: 1.8218367343813646
Experience 5, Iter 28, disc loss: 0.33251837123531225, policy loss: 1.8791700213525735
Experience 5, Iter 29, disc loss: 0.33138161301925345, policy loss: 2.1305049788865174
Experience 5, Iter 30, disc loss: 0.44326489087766013, policy loss: 1.8505454416843254
Experience 5, Iter 31, disc loss: 0.4056739918595529, policy loss: 1.7419475345363598
Experience 5, Iter 32, disc loss: 0.3342774743849746, policy loss: 1.8690608299779512
Experience 5, Iter 33, disc loss: 0.42052621412030566, policy loss: 1.6304795897705813
Experience 5, Iter 34, disc loss: 0.3906303060083449, policy loss: 1.5623194209662348
Experience 5, Iter 35, disc loss: 0.4500723051581364, policy loss: 1.596737547241644
Experience 5, Iter 36, disc loss: 0.35273693188991134, policy loss: 1.740065291142768
Experience 5, Iter 37, disc loss: 0.37539643223165575, policy loss: 1.66861055886505
Experience 5, Iter 38, disc loss: 0.36833611887168044, policy loss: 1.6192642631084717
Experience 5, Iter 39, disc loss: 0.42108992423416486, policy loss: 1.4592588190626437
Experience 5, Iter 40, disc loss: 0.3998480701507208, policy loss: 1.606520842389573
Experience 5, Iter 41, disc loss: 0.37874321882644807, policy loss: 1.6269921835475163
Experience 5, Iter 42, disc loss: 0.3591023040609165, policy loss: 1.7381118561633948
Experience 5, Iter 43, disc loss: 0.29072634259535163, policy loss: 1.9999940933039904
Experience 5, Iter 44, disc loss: 0.41816594139444263, policy loss: 1.682491558401261
Experience 5, Iter 45, disc loss: 0.3556223003772858, policy loss: 1.8390431635778923
Experience 5, Iter 46, disc loss: 0.39362831575197443, policy loss: 1.7880029178351409
Experience 5, Iter 47, disc loss: 0.32364237424384706, policy loss: 1.818611820237142
Experience 5, Iter 48, disc loss: 0.3420028367223077, policy loss: 1.8812494040657106
Experience 5, Iter 49, disc loss: 0.31179981795441364, policy loss: 1.9121950729881503
Experience 5, Iter 50, disc loss: 0.3301278034549888, policy loss: 1.93308524794389
Experience 5, Iter 51, disc loss: 0.25464961939080005, policy loss: 2.087403478351882
Experience 5, Iter 52, disc loss: 0.23962868164723924, policy loss: 2.1723078737974824
Experience 5, Iter 53, disc loss: 0.23776049224378273, policy loss: 2.1289622576936766
Experience 5, Iter 54, disc loss: 0.22898884720095458, policy loss: 2.334169293303895
Experience 5, Iter 55, disc loss: 0.2106349934390579, policy loss: 2.346179608919962
Experience 5, Iter 56, disc loss: 0.23746666280392462, policy loss: 2.3550499889054075
Experience 5, Iter 57, disc loss: 0.1584668632175321, policy loss: 2.5943482038569634
Experience 5, Iter 58, disc loss: 0.20991446533982505, policy loss: 2.3145460382327263
Experience 5, Iter 59, disc loss: 0.16786369243297392, policy loss: 2.568299991074047
Experience 5, Iter 60, disc loss: 0.18277873985359397, policy loss: 2.45416960074117
Experience 5, Iter 61, disc loss: 0.21480807393504908, policy loss: 2.372301938009622
Experience 5, Iter 62, disc loss: 0.16102941928890827, policy loss: 2.7203011169799325
Experience 5, Iter 63, disc loss: 0.13390203778929763, policy loss: 2.8176589985342666
Experience 5, Iter 64, disc loss: 0.17439280664354195, policy loss: 2.5815402481004255
Experience 5, Iter 65, disc loss: 0.13290160312885574, policy loss: 2.8340676810224563
Experience 5, Iter 66, disc loss: 0.16111528923847548, policy loss: 2.657959047666008
Experience 5, Iter 67, disc loss: 0.15003132693991195, policy loss: 2.815102180337009
Experience 5, Iter 68, disc loss: 0.1260587212553186, policy loss: 2.96260943936422
Experience 5, Iter 69, disc loss: 0.14998327684170681, policy loss: 2.6938372852474446
Experience 5, Iter 70, disc loss: 0.1354311877959682, policy loss: 2.830091368697081
Experience 5, Iter 71, disc loss: 0.1256955008706194, policy loss: 2.8495383889051515
Experience 5, Iter 72, disc loss: 0.12447398177499931, policy loss: 3.1312860679636008
Experience 5, Iter 73, disc loss: 0.10601228765323062, policy loss: 3.245164279620349
Experience 5, Iter 74, disc loss: 0.14850479982296824, policy loss: 2.9906459996428287
Experience 5, Iter 75, disc loss: 0.09583785273035939, policy loss: 3.2654651615030392
Experience 5, Iter 76, disc loss: 0.10732728787282772, policy loss: 3.1685272146634915
Experience 5, Iter 77, disc loss: 0.10729833159552264, policy loss: 3.116376001113644
Experience 5, Iter 78, disc loss: 0.14377485810929477, policy loss: 2.892031790631317
Experience 5, Iter 79, disc loss: 0.1000837728632367, policy loss: 3.301644303795803
Experience 5, Iter 80, disc loss: 0.10321040924538302, policy loss: 3.0904819566187154
Experience 5, Iter 81, disc loss: 0.0881951258949335, policy loss: 3.563660110286533
Experience 5, Iter 82, disc loss: 0.09458527399136829, policy loss: 3.1578065373636686
Experience 5, Iter 83, disc loss: 0.09067965091329018, policy loss: 3.3527512019894044
Experience 5, Iter 84, disc loss: 0.08539449676912671, policy loss: 3.486132834770448
Experience 5, Iter 85, disc loss: 0.0712454125339638, policy loss: 3.516850895981092
Experience 5, Iter 86, disc loss: 0.07175183061452917, policy loss: 3.5704962811457177
Experience 5, Iter 87, disc loss: 0.09638661251108331, policy loss: 3.379603170915016
Experience 5, Iter 88, disc loss: 0.06783800240144008, policy loss: 3.6164600368724154
Experience 5, Iter 89, disc loss: 0.09240829156846694, policy loss: 3.191088106861831
Experience 5, Iter 90, disc loss: 0.08225007926465369, policy loss: 3.36471854315039
Experience 5, Iter 91, disc loss: 0.06240765407846879, policy loss: 3.587987130793411
Experience 5, Iter 92, disc loss: 0.06284839339883236, policy loss: 3.723962462893585
Experience 5, Iter 93, disc loss: 0.08147989432104338, policy loss: 3.4172807202741966
Experience 5, Iter 94, disc loss: 0.058110257060288, policy loss: 3.700226203710732
Experience 5, Iter 95, disc loss: 0.07493446386468927, policy loss: 3.6046101169361493
Experience 5, Iter 96, disc loss: 0.060135777210131684, policy loss: 3.782362841977246
Experience 5, Iter 97, disc loss: 0.05422109689302143, policy loss: 3.717702570599007
Experience 5, Iter 98, disc loss: 0.06130150659674474, policy loss: 3.603969253424019
Experience 5, Iter 99, disc loss: 0.0596051245038719, policy loss: 3.815132903823058
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0223],
        [0.2215],
        [0.0047]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.7527e-02, 1.1286e-01, 2.0744e-01, 8.9254e-03, 6.0348e-04,
          8.5702e-01]],

        [[1.7527e-02, 1.1286e-01, 2.0744e-01, 8.9254e-03, 6.0348e-04,
          8.5702e-01]],

        [[1.7527e-02, 1.1286e-01, 2.0744e-01, 8.9254e-03, 6.0348e-04,
          8.5702e-01]],

        [[1.7527e-02, 1.1286e-01, 2.0744e-01, 8.9254e-03, 6.0348e-04,
          8.5702e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0130, 0.0892, 0.8862, 0.0188], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0130, 0.0892, 0.8862, 0.0188])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.386
Iter 2/2000 - Loss: 0.473
Iter 3/2000 - Loss: 0.311
Iter 4/2000 - Loss: 0.264
Iter 5/2000 - Loss: 0.330
Iter 6/2000 - Loss: 0.302
Iter 7/2000 - Loss: 0.218
Iter 8/2000 - Loss: 0.196
Iter 9/2000 - Loss: 0.228
Iter 10/2000 - Loss: 0.224
Iter 11/2000 - Loss: 0.167
Iter 12/2000 - Loss: 0.117
Iter 13/2000 - Loss: 0.102
Iter 14/2000 - Loss: 0.096
Iter 15/2000 - Loss: 0.061
Iter 16/2000 - Loss: -0.001
Iter 17/2000 - Loss: -0.063
Iter 18/2000 - Loss: -0.118
Iter 19/2000 - Loss: -0.177
Iter 20/2000 - Loss: -0.252
Iter 1981/2000 - Loss: -8.039
Iter 1982/2000 - Loss: -8.039
Iter 1983/2000 - Loss: -8.039
Iter 1984/2000 - Loss: -8.039
Iter 1985/2000 - Loss: -8.039
Iter 1986/2000 - Loss: -8.039
Iter 1987/2000 - Loss: -8.039
Iter 1988/2000 - Loss: -8.039
Iter 1989/2000 - Loss: -8.039
Iter 1990/2000 - Loss: -8.039
Iter 1991/2000 - Loss: -8.039
Iter 1992/2000 - Loss: -8.039
Iter 1993/2000 - Loss: -8.039
Iter 1994/2000 - Loss: -8.039
Iter 1995/2000 - Loss: -8.039
Iter 1996/2000 - Loss: -8.040
Iter 1997/2000 - Loss: -8.040
Iter 1998/2000 - Loss: -8.040
Iter 1999/2000 - Loss: -8.040
Iter 2000/2000 - Loss: -8.040
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0011],
        [0.0003]])
Lengthscale: tensor([[[17.2891, 10.3147, 30.9141, 18.2359, 10.3058, 53.1507]],

        [[20.8190, 37.6266, 17.0219,  0.8884,  4.5330, 12.8848]],

        [[25.1549, 31.6476, 18.1563,  0.7873,  0.9470, 10.1643]],

        [[19.4200, 38.3083, 10.6832,  3.9059, 10.2691, 29.6423]]])
Signal Variance: tensor([0.2858, 0.5837, 7.3742, 0.2793])
Estimated target variance: tensor([0.0130, 0.0892, 0.8862, 0.0188])
N: 60
Signal to noise ratio: tensor([36.8475, 40.2369, 81.7032, 33.1130])
Bound on condition number: tensor([ 81465.0979,  97141.4742, 400525.7842,  65789.1769])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.30246596178699525, policy loss: 1.7803461392877291
Experience 6, Iter 1, disc loss: 0.34866956161957663, policy loss: 1.7047894608512073
Experience 6, Iter 2, disc loss: 0.3416739902505854, policy loss: 1.6892359612410581
Experience 6, Iter 3, disc loss: 0.36204289291399694, policy loss: 1.454766682261952
Experience 6, Iter 4, disc loss: 0.36867514369874554, policy loss: 1.4947528039937739
Experience 6, Iter 5, disc loss: 0.35413772208557304, policy loss: 1.5468353874896745
Experience 6, Iter 6, disc loss: 0.3125113563156631, policy loss: 1.6267184016579863
Experience 6, Iter 7, disc loss: 0.2855501769172964, policy loss: 1.707739383343284
Experience 6, Iter 8, disc loss: 0.2180284375987248, policy loss: 2.136391557011671
Experience 6, Iter 9, disc loss: 0.21560239164431616, policy loss: 2.200528004444254
Experience 6, Iter 10, disc loss: 0.17432682053850143, policy loss: 2.420867307122557
Experience 6, Iter 11, disc loss: 0.17879834009836518, policy loss: 2.3886538811134987
Experience 6, Iter 12, disc loss: 0.1690400385839303, policy loss: 2.3751558111267084
Experience 6, Iter 13, disc loss: 0.14655612830038983, policy loss: 2.626495191574704
Experience 6, Iter 14, disc loss: 0.14105467449826872, policy loss: 2.7841344406947695
Experience 6, Iter 15, disc loss: 0.1327107163272869, policy loss: 2.7983749018428146
Experience 6, Iter 16, disc loss: 0.11133490771701846, policy loss: 3.171777384601326
Experience 6, Iter 17, disc loss: 0.10982018330866744, policy loss: 3.235554371320074
Experience 6, Iter 18, disc loss: 0.10050119909741212, policy loss: 3.339365468358327
Experience 6, Iter 19, disc loss: 0.09568863250357523, policy loss: 3.477751585654869
Experience 6, Iter 20, disc loss: 0.08314239224706921, policy loss: 3.7835690124229595
Experience 6, Iter 21, disc loss: 0.08178038262831167, policy loss: 3.7604147940853245
Experience 6, Iter 22, disc loss: 0.07587439728055342, policy loss: 3.913471549319988
Experience 6, Iter 23, disc loss: 0.07063441175778101, policy loss: 4.010051365006694
Experience 6, Iter 24, disc loss: 0.0665064960190498, policy loss: 4.071740311796322
Experience 6, Iter 25, disc loss: 0.06363607132535849, policy loss: 4.031313972369966
Experience 6, Iter 26, disc loss: 0.05734877896848729, policy loss: 4.194579357012484
Experience 6, Iter 27, disc loss: 0.054373970603134675, policy loss: 4.262708380255694
Experience 6, Iter 28, disc loss: 0.05169118522856679, policy loss: 4.314332036982847
Experience 6, Iter 29, disc loss: 0.047970396080620516, policy loss: 4.343187834788603
Experience 6, Iter 30, disc loss: 0.042089519283860406, policy loss: 4.490216169718898
Experience 6, Iter 31, disc loss: 0.03868191744468367, policy loss: 4.565667375097956
Experience 6, Iter 32, disc loss: 0.03994509218769443, policy loss: 4.398149023560773
Experience 6, Iter 33, disc loss: 0.03751391295821534, policy loss: 4.440021800216712
Experience 6, Iter 34, disc loss: 0.03214400742763262, policy loss: 4.779848779208331
Experience 6, Iter 35, disc loss: 0.03516602860794818, policy loss: 4.529545833941013
Experience 6, Iter 36, disc loss: 0.029408896753039454, policy loss: 4.908204906553785
Experience 6, Iter 37, disc loss: 0.03087569828874296, policy loss: 4.7599711305689425
Experience 6, Iter 38, disc loss: 0.028487554936079323, policy loss: 4.785448800196791
Experience 6, Iter 39, disc loss: 0.030275995872214204, policy loss: 4.549581884755547
Experience 6, Iter 40, disc loss: 0.02906613356457695, policy loss: 4.656906385246471
Experience 6, Iter 41, disc loss: 0.027504550646201484, policy loss: 4.74245160032102
Experience 6, Iter 42, disc loss: 0.024456504187360208, policy loss: 5.0729760570804645
Experience 6, Iter 43, disc loss: 0.02150635018124835, policy loss: 5.1565996021115
Experience 6, Iter 44, disc loss: 0.02412384821885355, policy loss: 4.797331483537407
Experience 6, Iter 45, disc loss: 0.01950157349056525, policy loss: 5.211001332633765
Experience 6, Iter 46, disc loss: 0.02015726665450327, policy loss: 4.995822303364706
Experience 6, Iter 47, disc loss: 0.02563802677108561, policy loss: 5.070888449369171
Experience 6, Iter 48, disc loss: 0.020354314667968137, policy loss: 5.241437841728664
Experience 6, Iter 49, disc loss: 0.018803079059910112, policy loss: 5.215681018574378
Experience 6, Iter 50, disc loss: 0.016738093782855824, policy loss: 5.321425813077142
Experience 6, Iter 51, disc loss: 0.02218411969014332, policy loss: 5.484134664723722
Experience 6, Iter 52, disc loss: 0.0185093392160035, policy loss: 5.220189107931039
Experience 6, Iter 53, disc loss: 0.017082163630355213, policy loss: 5.413809395372596
Experience 6, Iter 54, disc loss: 0.01390353140873782, policy loss: 5.6509753200336394
Experience 6, Iter 55, disc loss: 0.013486130189661764, policy loss: 5.4751896726660245
Experience 6, Iter 56, disc loss: 0.013139885194103251, policy loss: 5.639450289789374
Experience 6, Iter 57, disc loss: 0.013678816888229275, policy loss: 5.634675001674232
Experience 6, Iter 58, disc loss: 0.016966341753529887, policy loss: 5.638987016614851
Experience 6, Iter 59, disc loss: 0.01553521153112036, policy loss: 5.224298825664285
Experience 6, Iter 60, disc loss: 0.01357771937277918, policy loss: 5.763789750144305
Experience 6, Iter 61, disc loss: 0.011968553777590981, policy loss: 5.505791611737522
Experience 6, Iter 62, disc loss: 0.016977814158452093, policy loss: 5.631750325138394
Experience 6, Iter 63, disc loss: 0.013655089802392894, policy loss: 5.374850735866584
Experience 6, Iter 64, disc loss: 0.01379712169640868, policy loss: 5.563970473401955
Experience 6, Iter 65, disc loss: 0.01282279690452374, policy loss: 5.402571186430494
Experience 6, Iter 66, disc loss: 0.011527149058813822, policy loss: 5.691733796563017
Experience 6, Iter 67, disc loss: 0.013046947039089393, policy loss: 5.642196964824443
Experience 6, Iter 68, disc loss: 0.010261351453257942, policy loss: 5.91801224611012
Experience 6, Iter 69, disc loss: 0.011044498745471039, policy loss: 5.862504001845284
Experience 6, Iter 70, disc loss: 0.011154947143211068, policy loss: 5.684014494374402
Experience 6, Iter 71, disc loss: 0.010668809373851411, policy loss: 5.86427354250859
Experience 6, Iter 72, disc loss: 0.009960573071495976, policy loss: 5.742735020842813
Experience 6, Iter 73, disc loss: 0.011353651444835404, policy loss: 5.704907637964016
Experience 6, Iter 74, disc loss: 0.010235479642303077, policy loss: 6.066817006499461
Experience 6, Iter 75, disc loss: 0.010401178549201522, policy loss: 5.8763461598827025
Experience 6, Iter 76, disc loss: 0.009898155800128743, policy loss: 6.056909627493878
Experience 6, Iter 77, disc loss: 0.008921984603352575, policy loss: 6.031976523831453
Experience 6, Iter 78, disc loss: 0.011448635116699336, policy loss: 6.063479492254363
Experience 6, Iter 79, disc loss: 0.008600061712962188, policy loss: 6.066967020670609
Experience 6, Iter 80, disc loss: 0.009360880990917145, policy loss: 6.057687767264666
Experience 6, Iter 81, disc loss: 0.0090646662933224, policy loss: 6.087916508241181
Experience 6, Iter 82, disc loss: 0.00930930167106675, policy loss: 6.022440020725998
Experience 6, Iter 83, disc loss: 0.008710772430948723, policy loss: 5.901356087105687
Experience 6, Iter 84, disc loss: 0.008222426084269117, policy loss: 6.11626871417391
Experience 6, Iter 85, disc loss: 0.007748124974659961, policy loss: 6.205467229530619
Experience 6, Iter 86, disc loss: 0.009853841282104227, policy loss: 5.941968210997594
Experience 6, Iter 87, disc loss: 0.007803042214814449, policy loss: 6.125260226717824
Experience 6, Iter 88, disc loss: 0.008242555956013326, policy loss: 6.084571982931486
Experience 6, Iter 89, disc loss: 0.007575060389982596, policy loss: 6.200700806311916
Experience 6, Iter 90, disc loss: 0.008242030893078327, policy loss: 6.114006109815173
Experience 6, Iter 91, disc loss: 0.009198532904504543, policy loss: 5.953208138386684
Experience 6, Iter 92, disc loss: 0.009512625580999572, policy loss: 6.234231091020813
Experience 6, Iter 93, disc loss: 0.009062207977564312, policy loss: 5.750751456641085
Experience 6, Iter 94, disc loss: 0.015708757065173207, policy loss: 6.0752126507280675
Experience 6, Iter 95, disc loss: 0.008411884649564811, policy loss: 5.9972015497372
Experience 6, Iter 96, disc loss: 0.007422891441835128, policy loss: 6.157281342228232
Experience 6, Iter 97, disc loss: 0.008692572830642672, policy loss: 6.04881459566704
Experience 6, Iter 98, disc loss: 0.008808859995934084, policy loss: 6.3773258607331655
Experience 6, Iter 99, disc loss: 0.007412487526919318, policy loss: 6.144697666836089
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0047],
        [0.0586],
        [0.4730],
        [0.0046]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.1209e-02, 1.6523e-01, 2.3262e-01, 1.0758e-02, 8.4806e-04,
          1.7836e+00]],

        [[2.1209e-02, 1.6523e-01, 2.3262e-01, 1.0758e-02, 8.4806e-04,
          1.7836e+00]],

        [[2.1209e-02, 1.6523e-01, 2.3262e-01, 1.0758e-02, 8.4806e-04,
          1.7836e+00]],

        [[2.1209e-02, 1.6523e-01, 2.3262e-01, 1.0758e-02, 8.4806e-04,
          1.7836e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0186, 0.2344, 1.8919, 0.0183], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0186, 0.2344, 1.8919, 0.0183])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.318
Iter 2/2000 - Loss: 1.525
Iter 3/2000 - Loss: 1.266
Iter 4/2000 - Loss: 1.281
Iter 5/2000 - Loss: 1.361
Iter 6/2000 - Loss: 1.280
Iter 7/2000 - Loss: 1.196
Iter 8/2000 - Loss: 1.206
Iter 9/2000 - Loss: 1.237
Iter 10/2000 - Loss: 1.201
Iter 11/2000 - Loss: 1.127
Iter 12/2000 - Loss: 1.078
Iter 13/2000 - Loss: 1.060
Iter 14/2000 - Loss: 1.031
Iter 15/2000 - Loss: 0.964
Iter 16/2000 - Loss: 0.873
Iter 17/2000 - Loss: 0.784
Iter 18/2000 - Loss: 0.702
Iter 19/2000 - Loss: 0.611
Iter 20/2000 - Loss: 0.497
Iter 1981/2000 - Loss: -7.890
Iter 1982/2000 - Loss: -7.890
Iter 1983/2000 - Loss: -7.890
Iter 1984/2000 - Loss: -7.890
Iter 1985/2000 - Loss: -7.890
Iter 1986/2000 - Loss: -7.890
Iter 1987/2000 - Loss: -7.890
Iter 1988/2000 - Loss: -7.890
Iter 1989/2000 - Loss: -7.890
Iter 1990/2000 - Loss: -7.890
Iter 1991/2000 - Loss: -7.890
Iter 1992/2000 - Loss: -7.890
Iter 1993/2000 - Loss: -7.890
Iter 1994/2000 - Loss: -7.890
Iter 1995/2000 - Loss: -7.891
Iter 1996/2000 - Loss: -7.891
Iter 1997/2000 - Loss: -7.891
Iter 1998/2000 - Loss: -7.891
Iter 1999/2000 - Loss: -7.891
Iter 2000/2000 - Loss: -7.891
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0012],
        [0.0003]])
Lengthscale: tensor([[[15.1624,  9.4786, 24.5499, 20.6679,  3.7602, 60.2367]],

        [[17.9172, 41.6121, 19.4576,  1.0935,  6.9082, 25.8135]],

        [[26.6031, 37.1797, 20.6197,  0.8634,  1.6129, 15.1114]],

        [[18.2654, 35.0326, 12.0265,  3.8186, 11.3817, 37.6899]]])
Signal Variance: tensor([ 0.2548,  1.7012, 17.6635,  0.3123])
Estimated target variance: tensor([0.0186, 0.2344, 1.8919, 0.0183])
N: 70
Signal to noise ratio: tensor([ 34.5935,  69.4432, 120.3172,  34.7998])
Bound on condition number: tensor([  83770.5046,  337565.8208, 1013336.9868,   84772.6262])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.004129912337620554, policy loss: 8.027687681774356
Experience 7, Iter 1, disc loss: 0.004133192664213115, policy loss: 7.969459346369459
Experience 7, Iter 2, disc loss: 0.004219800565504413, policy loss: 7.699127252813591
Experience 7, Iter 3, disc loss: 0.004002505998644005, policy loss: 7.939991140103303
Experience 7, Iter 4, disc loss: 0.003967833093167475, policy loss: 7.902655318957064
Experience 7, Iter 5, disc loss: 0.004027011182636173, policy loss: 7.768460308721728
Experience 7, Iter 6, disc loss: 0.0040243018394917705, policy loss: 7.7657726225349535
Experience 7, Iter 7, disc loss: 0.003982077812015197, policy loss: 7.671125685571167
Experience 7, Iter 8, disc loss: 0.003980939226144394, policy loss: 7.660811463144538
Experience 7, Iter 9, disc loss: 0.0041305371162994615, policy loss: 7.628335285370021
Experience 7, Iter 10, disc loss: 0.003984186352533807, policy loss: 7.609029318408509
Experience 7, Iter 11, disc loss: 0.0039965884502936875, policy loss: 7.62752825408206
Experience 7, Iter 12, disc loss: 0.0038695997543787795, policy loss: 7.671965240939841
Experience 7, Iter 13, disc loss: 0.004039201096333513, policy loss: 7.338136091328213
Experience 7, Iter 14, disc loss: 0.004080316990814672, policy loss: 7.3209501696649015
Experience 7, Iter 15, disc loss: 0.0038906938671527847, policy loss: 7.4321323466893
Experience 7, Iter 16, disc loss: 0.004142682340885936, policy loss: 7.3818031900334775
Experience 7, Iter 17, disc loss: 0.0037654994232314, policy loss: 7.804120000649155
Experience 7, Iter 18, disc loss: 0.00404024809570762, policy loss: 7.303575685791605
Experience 7, Iter 19, disc loss: 0.004198862615816643, policy loss: 7.047314278266561
Experience 7, Iter 20, disc loss: 0.0038837960423392832, policy loss: 7.27018767284782
Experience 7, Iter 21, disc loss: 0.004143811157864786, policy loss: 7.059592621835259
Experience 7, Iter 22, disc loss: 0.004047277863663136, policy loss: 7.165426193644187
Experience 7, Iter 23, disc loss: 0.004503012314091255, policy loss: 6.860781150993329
Experience 7, Iter 24, disc loss: 0.004335584851501974, policy loss: 6.88175358643479
Experience 7, Iter 25, disc loss: 0.004536090147361931, policy loss: 6.95107807837677
Experience 7, Iter 26, disc loss: 0.0048850858358784055, policy loss: 6.716416389898766
Experience 7, Iter 27, disc loss: 0.005890219552622498, policy loss: 6.5606758389901945
Experience 7, Iter 28, disc loss: 0.006016933556016435, policy loss: 6.425972235810936
Experience 7, Iter 29, disc loss: 0.005990698883953158, policy loss: 6.47753807384777
Experience 7, Iter 30, disc loss: 0.005838991131605139, policy loss: 6.37169969904037
Experience 7, Iter 31, disc loss: 0.006926273746513964, policy loss: 6.132988632170964
Experience 7, Iter 32, disc loss: 0.006399198138238541, policy loss: 6.339629622712621
Experience 7, Iter 33, disc loss: 0.004719881655183384, policy loss: 6.883373356421649
Experience 7, Iter 34, disc loss: 0.003565786494953477, policy loss: 7.28595248090433
Experience 7, Iter 35, disc loss: 0.0037213386094566364, policy loss: 7.149302370208126
Experience 7, Iter 36, disc loss: 0.0032245208896425183, policy loss: 7.471661314944062
Experience 7, Iter 37, disc loss: 0.003501195150018467, policy loss: 7.240345446322253
Experience 7, Iter 38, disc loss: 0.0030010489788697, policy loss: 7.687523232513692
Experience 7, Iter 39, disc loss: 0.003182121054788869, policy loss: 7.336298857599116
Experience 7, Iter 40, disc loss: 0.0029810863621598125, policy loss: 7.446794646747643
Experience 7, Iter 41, disc loss: 0.002853421777088896, policy loss: 7.633757407839264
Experience 7, Iter 42, disc loss: 0.0031305529707167778, policy loss: 7.317724637164206
Experience 7, Iter 43, disc loss: 0.0030662537352384254, policy loss: 7.321747423773364
Experience 7, Iter 44, disc loss: 0.0029084866268556595, policy loss: 7.383808164889878
Experience 7, Iter 45, disc loss: 0.0027270819612709926, policy loss: 7.73979912738277
Experience 7, Iter 46, disc loss: 0.0026630066572225987, policy loss: 7.820629278515306
Experience 7, Iter 47, disc loss: 0.002873729715856746, policy loss: 7.435680950708867
Experience 7, Iter 48, disc loss: 0.002656556004335735, policy loss: 7.6560321357391965
Experience 7, Iter 49, disc loss: 0.0026959232115393045, policy loss: 7.53091780468767
Experience 7, Iter 50, disc loss: 0.002686034363949019, policy loss: 7.486723251185996
Experience 7, Iter 51, disc loss: 0.002482887974187644, policy loss: 7.940113764963881
Experience 7, Iter 52, disc loss: 0.0025361715619540855, policy loss: 7.648851851687235
Experience 7, Iter 53, disc loss: 0.002398104593426838, policy loss: 7.813637891354043
Experience 7, Iter 54, disc loss: 0.0023575638111991686, policy loss: 7.8739144640175285
Experience 7, Iter 55, disc loss: 0.002493022446947541, policy loss: 7.704766203198988
Experience 7, Iter 56, disc loss: 0.002353271583648255, policy loss: 7.789426014421333
Experience 7, Iter 57, disc loss: 0.0022408384493987645, policy loss: 7.9146145068133595
Experience 7, Iter 58, disc loss: 0.002317258605365819, policy loss: 7.813170636053096
Experience 7, Iter 59, disc loss: 0.0024009246062157404, policy loss: 7.552517114034485
Experience 7, Iter 60, disc loss: 0.002267873266396953, policy loss: 7.773420765132479
Experience 7, Iter 61, disc loss: 0.0023160737376966504, policy loss: 7.696916331170269
Experience 7, Iter 62, disc loss: 0.0023558254364413845, policy loss: 7.541449712060761
Experience 7, Iter 63, disc loss: 0.002212674593515594, policy loss: 7.775578942282704
Experience 7, Iter 64, disc loss: 0.002295079807515046, policy loss: 7.601934988668839
Experience 7, Iter 65, disc loss: 0.0023198584766062425, policy loss: 7.558005847947236
Experience 7, Iter 66, disc loss: 0.0022836144952793408, policy loss: 7.550529820607745
Experience 7, Iter 67, disc loss: 0.00225759241005329, policy loss: 7.544303096527326
Experience 7, Iter 68, disc loss: 0.0023361141204607467, policy loss: 7.3844291801881665
Experience 7, Iter 69, disc loss: 0.0023934444077104633, policy loss: 7.327540735686942
Experience 7, Iter 70, disc loss: 0.0023751106808738744, policy loss: 7.27113288691568
Experience 7, Iter 71, disc loss: 0.0024005676798112175, policy loss: 7.265560529962259
Experience 7, Iter 72, disc loss: 0.002374167304519072, policy loss: 7.210182760377682
Experience 7, Iter 73, disc loss: 0.0023626195715443746, policy loss: 7.221201109711478
Experience 7, Iter 74, disc loss: 0.002323991093173625, policy loss: 7.238513587482248
Experience 7, Iter 75, disc loss: 0.0023637854169127606, policy loss: 7.1891604141539265
Experience 7, Iter 76, disc loss: 0.002411505188083473, policy loss: 7.08185884767248
Experience 7, Iter 77, disc loss: 0.0023430763988349997, policy loss: 7.158888871753878
Experience 7, Iter 78, disc loss: 0.002446843259054042, policy loss: 7.070499877586419
Experience 7, Iter 79, disc loss: 0.0022990573620670926, policy loss: 7.183175085696278
Experience 7, Iter 80, disc loss: 0.00247112317792874, policy loss: 6.986485389274614
Experience 7, Iter 81, disc loss: 0.0023482199744546055, policy loss: 7.09432966223796
Experience 7, Iter 82, disc loss: 0.0024018777647285836, policy loss: 7.019355567526879
Experience 7, Iter 83, disc loss: 0.0024712059576579544, policy loss: 6.947216191201016
Experience 7, Iter 84, disc loss: 0.002429249612298743, policy loss: 7.009163867550171
Experience 7, Iter 85, disc loss: 0.00247794978464322, policy loss: 6.944651653597005
Experience 7, Iter 86, disc loss: 0.0024147386535484146, policy loss: 6.967556951102622
Experience 7, Iter 87, disc loss: 0.0025379130093673378, policy loss: 6.8898043258490365
Experience 7, Iter 88, disc loss: 0.0023608430440448624, policy loss: 6.9990571817170455
Experience 7, Iter 89, disc loss: 0.0024144020209266766, policy loss: 6.986866490454783
Experience 7, Iter 90, disc loss: 0.0024067347386716132, policy loss: 6.943292073316202
Experience 7, Iter 91, disc loss: 0.0024826529366489165, policy loss: 6.862870015850181
Experience 7, Iter 92, disc loss: 0.002427174916682257, policy loss: 6.931579082409314
Experience 7, Iter 93, disc loss: 0.002403562123736813, policy loss: 6.94171375609975
Experience 7, Iter 94, disc loss: 0.002349279934627904, policy loss: 6.982767024934359
Experience 7, Iter 95, disc loss: 0.002394061045996821, policy loss: 6.924910144420986
Experience 7, Iter 96, disc loss: 0.002360536028065306, policy loss: 6.933007261160585
Experience 7, Iter 97, disc loss: 0.0024362875644238237, policy loss: 6.8901279634262265
Experience 7, Iter 98, disc loss: 0.0025498383733004344, policy loss: 6.780993988166491
Experience 7, Iter 99, disc loss: 0.0024863817379607712, policy loss: 6.834440119899247
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0521],
        [0.4228],
        [0.0041]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.8973e-02, 1.4489e-01, 2.0716e-01, 9.4631e-03, 7.7387e-04,
          1.5738e+00]],

        [[1.8973e-02, 1.4489e-01, 2.0716e-01, 9.4631e-03, 7.7387e-04,
          1.5738e+00]],

        [[1.8973e-02, 1.4489e-01, 2.0716e-01, 9.4631e-03, 7.7387e-04,
          1.5738e+00]],

        [[1.8973e-02, 1.4489e-01, 2.0716e-01, 9.4631e-03, 7.7387e-04,
          1.5738e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0164, 0.2083, 1.6913, 0.0163], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0164, 0.2083, 1.6913, 0.0163])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.078
Iter 2/2000 - Loss: 1.356
Iter 3/2000 - Loss: 1.033
Iter 4/2000 - Loss: 1.065
Iter 5/2000 - Loss: 1.164
Iter 6/2000 - Loss: 1.073
Iter 7/2000 - Loss: 0.965
Iter 8/2000 - Loss: 0.959
Iter 9/2000 - Loss: 0.999
Iter 10/2000 - Loss: 0.976
Iter 11/2000 - Loss: 0.893
Iter 12/2000 - Loss: 0.818
Iter 13/2000 - Loss: 0.783
Iter 14/2000 - Loss: 0.757
Iter 15/2000 - Loss: 0.696
Iter 16/2000 - Loss: 0.596
Iter 17/2000 - Loss: 0.482
Iter 18/2000 - Loss: 0.375
Iter 19/2000 - Loss: 0.269
Iter 20/2000 - Loss: 0.149
Iter 1981/2000 - Loss: -8.035
Iter 1982/2000 - Loss: -8.036
Iter 1983/2000 - Loss: -8.036
Iter 1984/2000 - Loss: -8.036
Iter 1985/2000 - Loss: -8.036
Iter 1986/2000 - Loss: -8.036
Iter 1987/2000 - Loss: -8.036
Iter 1988/2000 - Loss: -8.036
Iter 1989/2000 - Loss: -8.036
Iter 1990/2000 - Loss: -8.036
Iter 1991/2000 - Loss: -8.036
Iter 1992/2000 - Loss: -8.036
Iter 1993/2000 - Loss: -8.036
Iter 1994/2000 - Loss: -8.036
Iter 1995/2000 - Loss: -8.036
Iter 1996/2000 - Loss: -8.036
Iter 1997/2000 - Loss: -8.036
Iter 1998/2000 - Loss: -8.036
Iter 1999/2000 - Loss: -8.036
Iter 2000/2000 - Loss: -8.036
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0014],
        [0.0002]])
Lengthscale: tensor([[[14.0090,  9.5856, 26.3177, 18.2348,  4.7640, 57.8818]],

        [[16.3541, 39.3464, 20.2967,  1.1048,  6.7139, 27.4185]],

        [[23.8225, 38.6843, 20.1823,  0.8749,  1.9958, 16.5980]],

        [[17.5917, 33.4976, 11.9393,  3.7747, 11.2985, 37.0239]]])
Signal Variance: tensor([ 0.2522,  1.9021, 18.7515,  0.3011])
Estimated target variance: tensor([0.0164, 0.2083, 1.6913, 0.0163])
N: 80
Signal to noise ratio: tensor([ 30.5837,  75.2353, 116.4867,  34.8422])
Bound on condition number: tensor([  74829.9685,  452828.8365, 1085532.8542,   97119.4814])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.0026609120702220015, policy loss: 6.75159280504727
Experience 8, Iter 1, disc loss: 0.0027694195317637775, policy loss: 6.679124147296938
Experience 8, Iter 2, disc loss: 0.0028663895345229927, policy loss: 6.613219494221104
Experience 8, Iter 3, disc loss: 0.003021054712220009, policy loss: 6.51773224696213
Experience 8, Iter 4, disc loss: 0.0031179330322625405, policy loss: 6.487414494857572
Experience 8, Iter 5, disc loss: 0.003362919250947199, policy loss: 6.353835629668318
Experience 8, Iter 6, disc loss: 0.0034089948566218982, policy loss: 6.3529936666545765
Experience 8, Iter 7, disc loss: 0.0038726544004896923, policy loss: 6.063712842869101
Experience 8, Iter 8, disc loss: 0.003609555859581292, policy loss: 6.188281171037602
Experience 8, Iter 9, disc loss: 0.0038596151940390455, policy loss: 6.0642602551255
Experience 8, Iter 10, disc loss: 0.003929107794836378, policy loss: 6.022055381992374
Experience 8, Iter 11, disc loss: 0.0040120997433795574, policy loss: 6.005575793149541
Experience 8, Iter 12, disc loss: 0.003964560646137365, policy loss: 5.989051822393085
Experience 8, Iter 13, disc loss: 0.003875150494712557, policy loss: 6.024407295479391
Experience 8, Iter 14, disc loss: 0.0041853886470439245, policy loss: 5.925902973178504
Experience 8, Iter 15, disc loss: 0.0041788690651566466, policy loss: 5.903426734751816
Experience 8, Iter 16, disc loss: 0.003890920589909191, policy loss: 6.013196874699441
Experience 8, Iter 17, disc loss: 0.003955115596595575, policy loss: 5.976799404333182
Experience 8, Iter 18, disc loss: 0.004010565960617192, policy loss: 5.928825175874236
Experience 8, Iter 19, disc loss: 0.003982540234410202, policy loss: 5.9792509703096455
Experience 8, Iter 20, disc loss: 0.0041174641159820715, policy loss: 5.907771113814414
Experience 8, Iter 21, disc loss: 0.003943553541015424, policy loss: 5.992731434837996
Experience 8, Iter 22, disc loss: 0.0037513799357536126, policy loss: 6.045416426425957
Experience 8, Iter 23, disc loss: 0.0040299575967427176, policy loss: 5.943953611282464
Experience 8, Iter 24, disc loss: 0.0040906421971242165, policy loss: 5.9131044333017755
Experience 8, Iter 25, disc loss: 0.003956245394042154, policy loss: 5.996374299405281
Experience 8, Iter 26, disc loss: 0.0040846537103868975, policy loss: 5.935017733446374
Experience 8, Iter 27, disc loss: 0.004073126654743749, policy loss: 5.92142412296953
Experience 8, Iter 28, disc loss: 0.004178830699249396, policy loss: 5.872619020145071
Experience 8, Iter 29, disc loss: 0.00423625090592035, policy loss: 5.862735402273004
Experience 8, Iter 30, disc loss: 0.0042106429204127735, policy loss: 5.890924050134868
Experience 8, Iter 31, disc loss: 0.003869413512062589, policy loss: 5.9891380961968235
Experience 8, Iter 32, disc loss: 0.003905277941187054, policy loss: 5.979219045878581
Experience 8, Iter 33, disc loss: 0.004156113046728225, policy loss: 5.892763425224173
Experience 8, Iter 34, disc loss: 0.004434494146466902, policy loss: 5.783973360612633
Experience 8, Iter 35, disc loss: 0.0039949493066871195, policy loss: 5.935382579724759
Experience 8, Iter 36, disc loss: 0.004130216473062913, policy loss: 5.86560315912868
Experience 8, Iter 37, disc loss: 0.004415966212291462, policy loss: 5.778270274009891
Experience 8, Iter 38, disc loss: 0.003938041142241754, policy loss: 5.954618700014929
Experience 8, Iter 39, disc loss: 0.0038617463366465397, policy loss: 5.993923386236237
Experience 8, Iter 40, disc loss: 0.0039693273616953064, policy loss: 5.940087909794596
Experience 8, Iter 41, disc loss: 0.003922665954023115, policy loss: 5.945444133507316
Experience 8, Iter 42, disc loss: 0.003867023136096417, policy loss: 5.952513519918162
Experience 8, Iter 43, disc loss: 0.00401487161638525, policy loss: 5.894703740988022
Experience 8, Iter 44, disc loss: 0.003936695151136033, policy loss: 5.903194497406295
Experience 8, Iter 45, disc loss: 0.004270310034829904, policy loss: 5.8469881052711
Experience 8, Iter 46, disc loss: 0.0040022453339030265, policy loss: 5.919847465418098
Experience 8, Iter 47, disc loss: 0.003984603147167413, policy loss: 5.908618560048433
Experience 8, Iter 48, disc loss: 0.003824481069549009, policy loss: 5.984323829843922
Experience 8, Iter 49, disc loss: 0.004054132047341607, policy loss: 5.903498063216441
Experience 8, Iter 50, disc loss: 0.00384361922313234, policy loss: 5.947966053222515
Experience 8, Iter 51, disc loss: 0.004124222530859562, policy loss: 5.873389576846355
Experience 8, Iter 52, disc loss: 0.004033950614696401, policy loss: 5.893193551280403
Experience 8, Iter 53, disc loss: 0.004149100251169936, policy loss: 5.829237965878192
Experience 8, Iter 54, disc loss: 0.004108834306284842, policy loss: 5.85657645290459
Experience 8, Iter 55, disc loss: 0.004334623166999618, policy loss: 5.826631799248725
Experience 8, Iter 56, disc loss: 0.004228551194776465, policy loss: 5.860908850965628
Experience 8, Iter 57, disc loss: 0.0038535638620575607, policy loss: 5.958255295625101
Experience 8, Iter 58, disc loss: 0.004130665511046589, policy loss: 5.887588778060951
Experience 8, Iter 59, disc loss: 0.004211760235741516, policy loss: 5.869865202417488
Experience 8, Iter 60, disc loss: 0.0038988044475958064, policy loss: 5.928959000692407
Experience 8, Iter 61, disc loss: 0.00449455012023551, policy loss: 5.773938910337938
Experience 8, Iter 62, disc loss: 0.00400272703700793, policy loss: 5.9177785207077624
Experience 8, Iter 63, disc loss: 0.004666111824407996, policy loss: 5.74915628452583
Experience 8, Iter 64, disc loss: 0.004479030738466112, policy loss: 5.811215007574928
Experience 8, Iter 65, disc loss: 0.004098340199246975, policy loss: 5.9113999387598195
Experience 8, Iter 66, disc loss: 0.0046778051678656694, policy loss: 5.747388818905815
Experience 8, Iter 67, disc loss: 0.003977669338346182, policy loss: 5.988964011339868
Experience 8, Iter 68, disc loss: 0.004671041213990009, policy loss: 5.8465384431883685
Experience 8, Iter 69, disc loss: 0.003978642850007446, policy loss: 5.972512155732505
Experience 8, Iter 70, disc loss: 0.00445382776090293, policy loss: 5.879165017850422
Experience 8, Iter 71, disc loss: 0.004660865253010388, policy loss: 5.824615320365846
Experience 8, Iter 72, disc loss: 0.004538271898225962, policy loss: 5.866200626902243
Experience 8, Iter 73, disc loss: 0.004337126053424818, policy loss: 5.9614302534002785
Experience 8, Iter 74, disc loss: 0.0045409241523495635, policy loss: 5.850462814729523
Experience 8, Iter 75, disc loss: 0.0051442939293617545, policy loss: 5.825485187701633
Experience 8, Iter 76, disc loss: 0.0048151584157593175, policy loss: 5.89533472672264
Experience 8, Iter 77, disc loss: 0.004852053571389702, policy loss: 5.899869663375658
Experience 8, Iter 78, disc loss: 0.006520731207552134, policy loss: 5.700191100970396
Experience 8, Iter 79, disc loss: 0.008552633735132865, policy loss: 5.463686737568386
Experience 8, Iter 80, disc loss: 0.009874062107223873, policy loss: 5.556207538459549
Experience 8, Iter 81, disc loss: 0.013837867437749006, policy loss: 5.017759617369814
Experience 8, Iter 82, disc loss: 0.023086640803067153, policy loss: 4.647056391969522
Experience 8, Iter 83, disc loss: 0.02944345398060125, policy loss: 4.279937198834362
Experience 8, Iter 84, disc loss: 0.0386177698403809, policy loss: 4.106052605096151
Experience 8, Iter 85, disc loss: 0.054927639091542156, policy loss: 3.5041704234835125
Experience 8, Iter 86, disc loss: 0.055025132643747296, policy loss: 3.370168772000988
Experience 8, Iter 87, disc loss: 0.053811105735862425, policy loss: 3.3723494320557856
Experience 8, Iter 88, disc loss: 0.05488646860414783, policy loss: 3.3249643519627545
Experience 8, Iter 89, disc loss: 0.05275065672509344, policy loss: 3.2749719932839065
Experience 8, Iter 90, disc loss: 0.04708759097917287, policy loss: 3.53947518957147
Experience 8, Iter 91, disc loss: 0.0442019671503926, policy loss: 3.460820640545199
Experience 8, Iter 92, disc loss: 0.043179934977819294, policy loss: 3.506845421942769
Experience 8, Iter 93, disc loss: 0.04478850403413483, policy loss: 3.508779997831644
Experience 8, Iter 94, disc loss: 0.04077109628397622, policy loss: 3.6327539106698805
Experience 8, Iter 95, disc loss: 0.04195186614487757, policy loss: 3.5964181521355156
Experience 8, Iter 96, disc loss: 0.04279024448184215, policy loss: 3.5985635792014623
Experience 8, Iter 97, disc loss: 0.04023894916766198, policy loss: 3.6399650898020157
Experience 8, Iter 98, disc loss: 0.039282365333759624, policy loss: 3.644387520932715
Experience 8, Iter 99, disc loss: 0.03652336242667576, policy loss: 3.743335540903588
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.0496],
        [0.4282],
        [0.0046]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.9618e-02, 1.3996e-01, 2.2920e-01, 1.0159e-02, 7.4783e-04,
          1.5118e+00]],

        [[1.9618e-02, 1.3996e-01, 2.2920e-01, 1.0159e-02, 7.4783e-04,
          1.5118e+00]],

        [[1.9618e-02, 1.3996e-01, 2.2920e-01, 1.0159e-02, 7.4783e-04,
          1.5118e+00]],

        [[1.9618e-02, 1.3996e-01, 2.2920e-01, 1.0159e-02, 7.4783e-04,
          1.5118e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0154, 0.1985, 1.7127, 0.0186], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0154, 0.1985, 1.7127, 0.0186])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.120
Iter 2/2000 - Loss: 1.302
Iter 3/2000 - Loss: 1.060
Iter 4/2000 - Loss: 1.058
Iter 5/2000 - Loss: 1.139
Iter 6/2000 - Loss: 1.063
Iter 7/2000 - Loss: 0.981
Iter 8/2000 - Loss: 0.991
Iter 9/2000 - Loss: 1.015
Iter 10/2000 - Loss: 0.971
Iter 11/2000 - Loss: 0.897
Iter 12/2000 - Loss: 0.850
Iter 13/2000 - Loss: 0.830
Iter 14/2000 - Loss: 0.791
Iter 15/2000 - Loss: 0.714
Iter 16/2000 - Loss: 0.618
Iter 17/2000 - Loss: 0.524
Iter 18/2000 - Loss: 0.434
Iter 19/2000 - Loss: 0.332
Iter 20/2000 - Loss: 0.206
Iter 1981/2000 - Loss: -8.038
Iter 1982/2000 - Loss: -8.038
Iter 1983/2000 - Loss: -8.038
Iter 1984/2000 - Loss: -8.038
Iter 1985/2000 - Loss: -8.038
Iter 1986/2000 - Loss: -8.038
Iter 1987/2000 - Loss: -8.038
Iter 1988/2000 - Loss: -8.038
Iter 1989/2000 - Loss: -8.038
Iter 1990/2000 - Loss: -8.038
Iter 1991/2000 - Loss: -8.038
Iter 1992/2000 - Loss: -8.038
Iter 1993/2000 - Loss: -8.038
Iter 1994/2000 - Loss: -8.038
Iter 1995/2000 - Loss: -8.038
Iter 1996/2000 - Loss: -8.039
Iter 1997/2000 - Loss: -8.039
Iter 1998/2000 - Loss: -8.039
Iter 1999/2000 - Loss: -8.039
Iter 2000/2000 - Loss: -8.039
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[14.3257,  4.4279, 31.6643, 13.2443,  7.1248, 47.1902]],

        [[22.8463, 39.2130, 21.8595,  1.2056,  5.9214, 29.0328]],

        [[27.7863, 44.1962, 13.6865,  1.0580,  6.5943, 17.5954]],

        [[15.6720, 32.1309, 10.8052,  2.6243,  8.8168, 34.6307]]])
Signal Variance: tensor([ 0.1112,  1.9568, 17.2577,  0.2271])
Estimated target variance: tensor([0.0154, 0.1985, 1.7127, 0.0186])
N: 90
Signal to noise ratio: tensor([20.3936, 76.1216, 97.0466, 29.2991])
Bound on condition number: tensor([ 37431.8574, 521505.4368, 847624.8786,  77260.4121])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.036178654669317384, policy loss: 3.812075513228728
Experience 9, Iter 1, disc loss: 0.03485953018790475, policy loss: 3.8544899354459057
Experience 9, Iter 2, disc loss: 0.0373218195599198, policy loss: 3.8204536257891393
Experience 9, Iter 3, disc loss: 0.03548143491410366, policy loss: 3.8912693179446967
Experience 9, Iter 4, disc loss: 0.036653457359824324, policy loss: 3.896644907283347
Experience 9, Iter 5, disc loss: 0.039136890669178095, policy loss: 3.7329802156000107
Experience 9, Iter 6, disc loss: 0.03625424535525313, policy loss: 3.895299415877518
Experience 9, Iter 7, disc loss: 0.03658647902299965, policy loss: 3.8554939076345676
Experience 9, Iter 8, disc loss: 0.03763106777449928, policy loss: 3.9567566539495065
Experience 9, Iter 9, disc loss: 0.03920746810707196, policy loss: 3.899872231986072
Experience 9, Iter 10, disc loss: 0.03989602938979929, policy loss: 3.9937226281035985
Experience 9, Iter 11, disc loss: 0.04704813011974119, policy loss: 3.824250330808009
Experience 9, Iter 12, disc loss: 0.05050027364641198, policy loss: 3.6653154217392796
Experience 9, Iter 13, disc loss: 0.054570948205772345, policy loss: 3.7160377876870068
Experience 9, Iter 14, disc loss: 0.057903204984891286, policy loss: 4.056323981699856
Experience 9, Iter 15, disc loss: 0.06147346251954283, policy loss: 3.8099838667462698
Experience 9, Iter 16, disc loss: 0.06808097784839075, policy loss: 3.5457262783774515
Experience 9, Iter 17, disc loss: 0.06575747732135948, policy loss: 3.839787591923694
Experience 9, Iter 18, disc loss: 0.07785670483234394, policy loss: 3.4518471646047946
Experience 9, Iter 19, disc loss: 0.08965918127050942, policy loss: 3.345223841982659
Experience 9, Iter 20, disc loss: 0.09929097751491087, policy loss: 3.6096691296698906
Experience 9, Iter 21, disc loss: 0.09435361545178955, policy loss: 3.6580719211437325
Experience 9, Iter 22, disc loss: 0.11900087415362953, policy loss: 3.863721847258438
Experience 9, Iter 23, disc loss: 0.13663996798309214, policy loss: 3.6796965546177325
Experience 9, Iter 24, disc loss: 0.1491584610800542, policy loss: 3.2737790763897903
Experience 9, Iter 25, disc loss: 0.17886174832546262, policy loss: 3.2746712664103117
Experience 9, Iter 26, disc loss: 0.19102681931153725, policy loss: 3.6064013953091933
Experience 9, Iter 27, disc loss: 0.20126290888461715, policy loss: 3.471858561538975
Experience 9, Iter 28, disc loss: 0.23173724788711805, policy loss: 3.5914354573702987
Experience 9, Iter 29, disc loss: 0.25271519116721125, policy loss: 3.3700251770243916
Experience 9, Iter 30, disc loss: 0.23787188922876798, policy loss: 3.6564609735195583
Experience 9, Iter 31, disc loss: 0.2717401788843223, policy loss: 3.1830206819387907
Experience 9, Iter 32, disc loss: 0.26925622633037627, policy loss: 3.184990349551227
Experience 9, Iter 33, disc loss: 0.2740225828347713, policy loss: 2.8313413076715896
Experience 9, Iter 34, disc loss: 0.2591174363783711, policy loss: 3.071543869640662
Experience 9, Iter 35, disc loss: 0.24577137534947613, policy loss: 3.441658308917753
Experience 9, Iter 36, disc loss: 0.2516720011100706, policy loss: 4.146965103387789
Experience 9, Iter 37, disc loss: 0.20721563051916642, policy loss: 3.8358349409537165
Experience 9, Iter 38, disc loss: 0.20971714582978518, policy loss: 4.015347946708658
Experience 9, Iter 39, disc loss: 0.18964053957198634, policy loss: 3.4153084436048964
Experience 9, Iter 40, disc loss: 0.1726307327582695, policy loss: 3.529924152159252
Experience 9, Iter 41, disc loss: 0.184908039324549, policy loss: 3.186159354112126
Experience 9, Iter 42, disc loss: 0.16812098901379416, policy loss: 3.174291597548332
Experience 9, Iter 43, disc loss: 0.16621818248292838, policy loss: 3.251116843732664
Experience 9, Iter 44, disc loss: 0.14700892282324785, policy loss: 3.257842291901148
Experience 9, Iter 45, disc loss: 0.1437930837748397, policy loss: 3.2420229834093313
Experience 9, Iter 46, disc loss: 0.13880785809937418, policy loss: 3.47115757560681
Experience 9, Iter 47, disc loss: 0.12265709595483634, policy loss: 4.1387883888544765
Experience 9, Iter 48, disc loss: 0.10388619297256574, policy loss: 4.254500827717466
Experience 9, Iter 49, disc loss: 0.11395688294036273, policy loss: 4.217784010882927
Experience 9, Iter 50, disc loss: 0.10413693749313988, policy loss: 4.028338735672444
Experience 9, Iter 51, disc loss: 0.09942536475995717, policy loss: 4.5345318454544525
Experience 9, Iter 52, disc loss: 0.10328175481445924, policy loss: 4.2501050461493435
Experience 9, Iter 53, disc loss: 0.10091468217564632, policy loss: 3.915943091602494
Experience 9, Iter 54, disc loss: 0.089368367523155, policy loss: 5.193131134452822
Experience 9, Iter 55, disc loss: 0.09676670691912216, policy loss: 3.987117465883708
Experience 9, Iter 56, disc loss: 0.08276059144946954, policy loss: 4.844117174869518
Experience 9, Iter 57, disc loss: 0.09004400350938738, policy loss: 4.6544089926788015
Experience 9, Iter 58, disc loss: 0.06940059117261285, policy loss: 4.756113229668323
Experience 9, Iter 59, disc loss: 0.07535391345075226, policy loss: 3.9669033186225464
Experience 9, Iter 60, disc loss: 0.06510450760492917, policy loss: 5.086784596341852
Experience 9, Iter 61, disc loss: 0.05800949920157814, policy loss: 5.182293382825721
Experience 9, Iter 62, disc loss: 0.0677718925923832, policy loss: 4.714199679452492
Experience 9, Iter 63, disc loss: 0.056546885229203134, policy loss: 4.746721687412403
Experience 9, Iter 64, disc loss: 0.05602163726061876, policy loss: 4.494375661564266
Experience 9, Iter 65, disc loss: 0.051896504715293015, policy loss: 4.641766386049669
Experience 9, Iter 66, disc loss: 0.05112660370671436, policy loss: 4.443805127024116
Experience 9, Iter 67, disc loss: 0.04982756087198306, policy loss: 4.4795462111440205
Experience 9, Iter 68, disc loss: 0.04729491076751912, policy loss: 4.58650750271952
Experience 9, Iter 69, disc loss: 0.05684587274993679, policy loss: 4.6086172175689235
Experience 9, Iter 70, disc loss: 0.05322814066581267, policy loss: 4.591146209990889
Experience 9, Iter 71, disc loss: 0.043385721063598136, policy loss: 5.804012385209731
Experience 9, Iter 72, disc loss: 0.05099257783785008, policy loss: 4.907893382430457
Experience 9, Iter 73, disc loss: 0.04780067500378879, policy loss: 4.58023828094416
Experience 9, Iter 74, disc loss: 0.045696663914009415, policy loss: 4.5911715693669315
Experience 9, Iter 75, disc loss: 0.04850468391807816, policy loss: 4.457053080967208
Experience 9, Iter 76, disc loss: 0.045583908945097784, policy loss: 4.750004573445664
Experience 9, Iter 77, disc loss: 0.051285130165019954, policy loss: 4.676043765681141
Experience 9, Iter 78, disc loss: 0.04248748099105416, policy loss: 5.51973036932845
Experience 9, Iter 79, disc loss: 0.03745819935371258, policy loss: 5.548600614329336
Experience 9, Iter 80, disc loss: 0.046957578848219904, policy loss: 4.991459422544553
Experience 9, Iter 81, disc loss: 0.04948838365481398, policy loss: 5.07274147669744
Experience 9, Iter 82, disc loss: 0.05129717111141824, policy loss: 4.492004874358264
Experience 9, Iter 83, disc loss: 0.04182117773157086, policy loss: 4.976440633310185
Experience 9, Iter 84, disc loss: 0.04322074431470321, policy loss: 4.798302261986843
Experience 9, Iter 85, disc loss: 0.04407289184292279, policy loss: 5.343629287533979
Experience 9, Iter 86, disc loss: 0.041968334366769736, policy loss: 5.7037817209099275
Experience 9, Iter 87, disc loss: 0.04241250383432869, policy loss: 4.59341965817578
Experience 9, Iter 88, disc loss: 0.046571025696001016, policy loss: 4.606892570316491
Experience 9, Iter 89, disc loss: 0.04206866755001771, policy loss: 4.661024156204773
Experience 9, Iter 90, disc loss: 0.0371498701064315, policy loss: 4.780732195308257
Experience 9, Iter 91, disc loss: 0.04093879594140645, policy loss: 4.679257384098812
Experience 9, Iter 92, disc loss: 0.040653419596758576, policy loss: 4.697383389046151
Experience 9, Iter 93, disc loss: 0.04201782625327639, policy loss: 5.567335513676081
Experience 9, Iter 94, disc loss: 0.04613284508446454, policy loss: 4.260962544761448
Experience 9, Iter 95, disc loss: 0.040566986564962576, policy loss: 4.432990919452073
Experience 9, Iter 96, disc loss: 0.04240104855016832, policy loss: 5.102861464156597
Experience 9, Iter 97, disc loss: 0.04266651326782103, policy loss: 5.027596846935243
Experience 9, Iter 98, disc loss: 0.045027002413846676, policy loss: 4.493568861538333
Experience 9, Iter 99, disc loss: 0.036380009150885406, policy loss: 5.261107436347459
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0659],
        [0.6594],
        [0.0120]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0184, 0.1529, 0.5681, 0.0135, 0.0058, 1.8215]],

        [[0.0184, 0.1529, 0.5681, 0.0135, 0.0058, 1.8215]],

        [[0.0184, 0.1529, 0.5681, 0.0135, 0.0058, 1.8215]],

        [[0.0184, 0.1529, 0.5681, 0.0135, 0.0058, 1.8215]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0166, 0.2636, 2.6375, 0.0479], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0166, 0.2636, 2.6375, 0.0479])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.964
Iter 2/2000 - Loss: 2.064
Iter 3/2000 - Loss: 1.900
Iter 4/2000 - Loss: 1.882
Iter 5/2000 - Loss: 1.929
Iter 6/2000 - Loss: 1.875
Iter 7/2000 - Loss: 1.797
Iter 8/2000 - Loss: 1.772
Iter 9/2000 - Loss: 1.766
Iter 10/2000 - Loss: 1.718
Iter 11/2000 - Loss: 1.635
Iter 12/2000 - Loss: 1.552
Iter 13/2000 - Loss: 1.480
Iter 14/2000 - Loss: 1.401
Iter 15/2000 - Loss: 1.294
Iter 16/2000 - Loss: 1.159
Iter 17/2000 - Loss: 1.006
Iter 18/2000 - Loss: 0.841
Iter 19/2000 - Loss: 0.665
Iter 20/2000 - Loss: 0.471
Iter 1981/2000 - Loss: -7.559
Iter 1982/2000 - Loss: -7.559
Iter 1983/2000 - Loss: -7.559
Iter 1984/2000 - Loss: -7.559
Iter 1985/2000 - Loss: -7.559
Iter 1986/2000 - Loss: -7.559
Iter 1987/2000 - Loss: -7.559
Iter 1988/2000 - Loss: -7.560
Iter 1989/2000 - Loss: -7.560
Iter 1990/2000 - Loss: -7.560
Iter 1991/2000 - Loss: -7.560
Iter 1992/2000 - Loss: -7.560
Iter 1993/2000 - Loss: -7.560
Iter 1994/2000 - Loss: -7.560
Iter 1995/2000 - Loss: -7.560
Iter 1996/2000 - Loss: -7.560
Iter 1997/2000 - Loss: -7.560
Iter 1998/2000 - Loss: -7.560
Iter 1999/2000 - Loss: -7.560
Iter 2000/2000 - Loss: -7.560
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.5671,  3.5569, 38.3250, 14.3608,  7.6587, 40.4198]],

        [[20.0480, 37.4682, 10.5748,  1.4142,  8.5969, 34.7362]],

        [[22.8032, 41.5645,  9.8834,  1.0188,  2.3279, 23.6156]],

        [[16.1752, 31.9386, 19.8855,  4.3521,  1.8573, 39.2617]]])
Signal Variance: tensor([ 0.0874,  4.1590, 22.1020,  0.6423])
Estimated target variance: tensor([0.0166, 0.2636, 2.6375, 0.0479])
N: 100
Signal to noise ratio: tensor([ 18.5998, 112.6668, 117.2956,  51.2088])
Bound on condition number: tensor([  34596.2690, 1269382.8856, 1375825.7906,  262235.3416])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.03608373433739319, policy loss: 5.245852220734754
Experience 10, Iter 1, disc loss: 0.0357701637575404, policy loss: 4.669622857777856
Experience 10, Iter 2, disc loss: 0.03409983320135199, policy loss: 5.543467468789662
Experience 10, Iter 3, disc loss: 0.029206192097081415, policy loss: 5.333542792054096
Experience 10, Iter 4, disc loss: 0.03462242099033187, policy loss: 4.692065024870423
Experience 10, Iter 5, disc loss: 0.03248443606987042, policy loss: 5.328912338442327
Experience 10, Iter 6, disc loss: 0.035035811638122846, policy loss: 4.590048201499138
Experience 10, Iter 7, disc loss: 0.03411230008651616, policy loss: 4.904171169689655
Experience 10, Iter 8, disc loss: 0.03537841241003606, policy loss: 5.315326711506802
Experience 10, Iter 9, disc loss: 0.03750864755284694, policy loss: 4.66850069618726
Experience 10, Iter 10, disc loss: 0.033864260338182475, policy loss: 4.734967067168629
Experience 10, Iter 11, disc loss: 0.03585870993322088, policy loss: 5.0138378093345874
Experience 10, Iter 12, disc loss: 0.031024387260253257, policy loss: 5.036333885780024
Experience 10, Iter 13, disc loss: 0.03231600194416041, policy loss: 4.967832640847307
Experience 10, Iter 14, disc loss: 0.03195158283392817, policy loss: 5.184597177802843
Experience 10, Iter 15, disc loss: 0.03431169248221598, policy loss: 5.209802835834234
Experience 10, Iter 16, disc loss: 0.030928530917609137, policy loss: 5.531578509331595
Experience 10, Iter 17, disc loss: 0.03219955567200989, policy loss: 5.309572041607373
Experience 10, Iter 18, disc loss: 0.02971820821227877, policy loss: 5.016657033046427
Experience 10, Iter 19, disc loss: 0.03464648067705571, policy loss: 4.514170332103354
Experience 10, Iter 20, disc loss: 0.032234352593542404, policy loss: 5.181872670310913
Experience 10, Iter 21, disc loss: 0.03464452821586195, policy loss: 4.815338710666032
Experience 10, Iter 22, disc loss: 0.03609022264656889, policy loss: 4.6184351981702205
Experience 10, Iter 23, disc loss: 0.033634013847488306, policy loss: 4.842646388041652
Experience 10, Iter 24, disc loss: 0.036556760212025495, policy loss: 4.807187430587997
Experience 10, Iter 25, disc loss: 0.03616951266685392, policy loss: 4.924101385304583
Experience 10, Iter 26, disc loss: 0.033113279414956426, policy loss: 5.350785370327991
Experience 10, Iter 27, disc loss: 0.031562902080316854, policy loss: 5.621834869156161
Experience 10, Iter 28, disc loss: 0.029742833574285005, policy loss: 5.467229193842704
Experience 10, Iter 29, disc loss: 0.03191286392943277, policy loss: 5.417412071908233
Experience 10, Iter 30, disc loss: 0.027575739135449175, policy loss: 5.1080991469942365
Experience 10, Iter 31, disc loss: 0.0317521868992329, policy loss: 5.071027515526696
Experience 10, Iter 32, disc loss: 0.030394364423686062, policy loss: 4.879245281313077
Experience 10, Iter 33, disc loss: 0.02776871554187648, policy loss: 5.577777015233185
Experience 10, Iter 34, disc loss: 0.032859493963254265, policy loss: 4.72148935582891
Experience 10, Iter 35, disc loss: 0.036725253392805274, policy loss: 4.657176622884299
Experience 10, Iter 36, disc loss: 0.03372725513666379, policy loss: 4.834609847812542
Experience 10, Iter 37, disc loss: 0.03406645087453547, policy loss: 4.880957213845283
Experience 10, Iter 38, disc loss: 0.031240872433902285, policy loss: 5.530572356998944
Experience 10, Iter 39, disc loss: 0.034909666111935585, policy loss: 4.923982199962686
Experience 10, Iter 40, disc loss: 0.03750324759962685, policy loss: 4.8640382464287635
Experience 10, Iter 41, disc loss: 0.03714398648908511, policy loss: 4.989601730800617
Experience 10, Iter 42, disc loss: 0.03892699123583131, policy loss: 5.34208424984063
Experience 10, Iter 43, disc loss: 0.03461756446586273, policy loss: 5.359231680623399
Experience 10, Iter 44, disc loss: 0.033795088803492995, policy loss: 5.536854679560808
Experience 10, Iter 45, disc loss: 0.03457716221196447, policy loss: 4.883538422661642
Experience 10, Iter 46, disc loss: 0.02933375653943341, policy loss: 5.749632835095346
Experience 10, Iter 47, disc loss: 0.030763620205268728, policy loss: 5.192861334767168
Experience 10, Iter 48, disc loss: 0.0327530147179463, policy loss: 4.884799622594985
Experience 10, Iter 49, disc loss: 0.03107470866721168, policy loss: 5.064048015035693
Experience 10, Iter 50, disc loss: 0.030746346348259092, policy loss: 5.080517723213595
Experience 10, Iter 51, disc loss: 0.035091410952918276, policy loss: 4.986990702695704
Experience 10, Iter 52, disc loss: 0.03049227767558805, policy loss: 5.674479986403912
Experience 10, Iter 53, disc loss: 0.03470352828675149, policy loss: 5.398928383956315
Experience 10, Iter 54, disc loss: 0.031011967458576026, policy loss: 5.5087982436262966
Experience 10, Iter 55, disc loss: 0.035977048663918085, policy loss: 4.791795453250524
Experience 10, Iter 56, disc loss: 0.03471180962063761, policy loss: 5.011028692390655
Experience 10, Iter 57, disc loss: 0.031134764230990326, policy loss: 4.917595351065618
Experience 10, Iter 58, disc loss: 0.03202489996128011, policy loss: 5.102552304110048
Experience 10, Iter 59, disc loss: 0.03195625983828328, policy loss: 5.137324618495999
Experience 10, Iter 60, disc loss: 0.033537684475699554, policy loss: 4.880566072063379
Experience 10, Iter 61, disc loss: 0.03172968806459913, policy loss: 5.169082207052911
Experience 10, Iter 62, disc loss: 0.03314079191012826, policy loss: 5.2245355135253435
Experience 10, Iter 63, disc loss: 0.03282392987127937, policy loss: 5.4242153524550965
Experience 10, Iter 64, disc loss: 0.02802933382593207, policy loss: 5.868184086983055
Experience 10, Iter 65, disc loss: 0.03423979806355379, policy loss: 4.9121925065147485
Experience 10, Iter 66, disc loss: 0.026803458678916633, policy loss: 5.596491830822606
Experience 10, Iter 67, disc loss: 0.02831645075273879, policy loss: 5.9314016927647835
Experience 10, Iter 68, disc loss: 0.027788385894031098, policy loss: 5.396286163457424
Experience 10, Iter 69, disc loss: 0.02731375730461785, policy loss: 5.437916805882437
Experience 10, Iter 70, disc loss: 0.029763819969659183, policy loss: 5.5331869119657675
Experience 10, Iter 71, disc loss: 0.03520988058678239, policy loss: 5.294714358544628
Experience 10, Iter 72, disc loss: 0.029374176274587518, policy loss: 5.152057175808881
Experience 10, Iter 73, disc loss: 0.03251357841239526, policy loss: 5.183641158537849
Experience 10, Iter 74, disc loss: 0.030299693953169973, policy loss: 5.343035878212532
Experience 10, Iter 75, disc loss: 0.02731619247814652, policy loss: 5.725372289560529
Experience 10, Iter 76, disc loss: 0.031028270330563115, policy loss: 5.151370970116972
Experience 10, Iter 77, disc loss: 0.027595240643239295, policy loss: 5.582604457971584
Experience 10, Iter 78, disc loss: 0.026702324525234643, policy loss: 5.849234230685276
Experience 10, Iter 79, disc loss: 0.034265433583122215, policy loss: 5.202247075258477
Experience 10, Iter 80, disc loss: 0.03221368140811307, policy loss: 5.588851008778113
Experience 10, Iter 81, disc loss: 0.02878696584684378, policy loss: 5.234620203541013
Experience 10, Iter 82, disc loss: 0.02516365970435, policy loss: 5.671757191268379
Experience 10, Iter 83, disc loss: 0.02924828028243679, policy loss: 4.942154936162497
Experience 10, Iter 84, disc loss: 0.03182882900832311, policy loss: 5.517374115813038
Experience 10, Iter 85, disc loss: 0.02658477812511089, policy loss: 5.465524296404931
Experience 10, Iter 86, disc loss: 0.028485437883466806, policy loss: 5.600778823654255
Experience 10, Iter 87, disc loss: 0.031202262858191533, policy loss: 5.383422383273244
Experience 10, Iter 88, disc loss: 0.029883534037954178, policy loss: 5.782804859219032
Experience 10, Iter 89, disc loss: 0.030764453647189324, policy loss: 5.6601187948903595
Experience 10, Iter 90, disc loss: 0.026215398087975905, policy loss: 5.902969222221969
Experience 10, Iter 91, disc loss: 0.02545286266428929, policy loss: 6.128473169534755
Experience 10, Iter 92, disc loss: 0.026193411439581475, policy loss: 5.725766036015177
Experience 10, Iter 93, disc loss: 0.02641301264408151, policy loss: 5.67585329786683
Experience 10, Iter 94, disc loss: 0.02494921647823388, policy loss: 5.74078178033975
Experience 10, Iter 95, disc loss: 0.026687591977209043, policy loss: 5.607152766288001
Experience 10, Iter 96, disc loss: 0.022454917899033693, policy loss: 6.193088073625035
Experience 10, Iter 97, disc loss: 0.022548069360989704, policy loss: 5.697691944589785
Experience 10, Iter 98, disc loss: 0.029327743549764584, policy loss: 5.553743843634018
Experience 10, Iter 99, disc loss: 0.026103141144804096, policy loss: 6.216819173000581
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0796],
        [0.7652],
        [0.0159]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0171, 0.1546, 0.7397, 0.0156, 0.0112, 2.1972]],

        [[0.0171, 0.1546, 0.7397, 0.0156, 0.0112, 2.1972]],

        [[0.0171, 0.1546, 0.7397, 0.0156, 0.0112, 2.1972]],

        [[0.0171, 0.1546, 0.7397, 0.0156, 0.0112, 2.1972]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0163, 0.3182, 3.0609, 0.0635], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0163, 0.3182, 3.0609, 0.0635])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.277
Iter 2/2000 - Loss: 2.358
Iter 3/2000 - Loss: 2.210
Iter 4/2000 - Loss: 2.197
Iter 5/2000 - Loss: 2.218
Iter 6/2000 - Loss: 2.166
Iter 7/2000 - Loss: 2.099
Iter 8/2000 - Loss: 2.061
Iter 9/2000 - Loss: 2.038
Iter 10/2000 - Loss: 1.987
Iter 11/2000 - Loss: 1.906
Iter 12/2000 - Loss: 1.815
Iter 13/2000 - Loss: 1.726
Iter 14/2000 - Loss: 1.631
Iter 15/2000 - Loss: 1.515
Iter 16/2000 - Loss: 1.372
Iter 17/2000 - Loss: 1.208
Iter 18/2000 - Loss: 1.028
Iter 19/2000 - Loss: 0.832
Iter 20/2000 - Loss: 0.618
Iter 1981/2000 - Loss: -7.284
Iter 1982/2000 - Loss: -7.284
Iter 1983/2000 - Loss: -7.284
Iter 1984/2000 - Loss: -7.284
Iter 1985/2000 - Loss: -7.284
Iter 1986/2000 - Loss: -7.284
Iter 1987/2000 - Loss: -7.284
Iter 1988/2000 - Loss: -7.284
Iter 1989/2000 - Loss: -7.284
Iter 1990/2000 - Loss: -7.284
Iter 1991/2000 - Loss: -7.284
Iter 1992/2000 - Loss: -7.284
Iter 1993/2000 - Loss: -7.284
Iter 1994/2000 - Loss: -7.284
Iter 1995/2000 - Loss: -7.284
Iter 1996/2000 - Loss: -7.284
Iter 1997/2000 - Loss: -7.284
Iter 1998/2000 - Loss: -7.284
Iter 1999/2000 - Loss: -7.284
Iter 2000/2000 - Loss: -7.284
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.3586,  3.5411, 36.0527, 10.0217,  6.5062, 41.0353]],

        [[17.2718, 31.1332,  8.0610,  1.6525,  1.5925, 16.6583]],

        [[18.5214, 34.6219,  9.2059,  1.2420,  1.2469, 19.7382]],

        [[16.0417, 33.9491, 19.8305,  4.0192,  1.8112, 40.4560]]])
Signal Variance: tensor([ 0.0804,  1.4727, 19.9014,  0.6316])
Estimated target variance: tensor([0.0163, 0.3182, 3.0609, 0.0635])
N: 110
Signal to noise ratio: tensor([ 17.2866,  66.9660, 102.0208,  52.4489])
Bound on condition number: tensor([  32871.9821,  493289.4333, 1144906.7668,  302598.7368])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.10714765910538174, policy loss: 3.4150051389317846
Experience 11, Iter 1, disc loss: 0.10093863467769956, policy loss: 3.6508136651304888
Experience 11, Iter 2, disc loss: 0.1297370839295753, policy loss: 3.3764233465937794
Experience 11, Iter 3, disc loss: 0.09171897759547061, policy loss: 3.6831993347815493
Experience 11, Iter 4, disc loss: 0.08660843634111468, policy loss: 4.554900898283078
Experience 11, Iter 5, disc loss: 0.09245108635668785, policy loss: 4.4928930438449095
Experience 11, Iter 6, disc loss: 0.08609544330783175, policy loss: 4.858763492037871
Experience 11, Iter 7, disc loss: 0.0900729751489192, policy loss: 5.176081073280525
Experience 11, Iter 8, disc loss: 0.09329293951067974, policy loss: 5.465598400932939
Experience 11, Iter 9, disc loss: 0.09684239116509101, policy loss: 5.1844051543255745
Experience 11, Iter 10, disc loss: 0.1038670414858082, policy loss: 3.9745857585634754
Experience 11, Iter 11, disc loss: 0.08915246886734877, policy loss: 4.331677439995155
Experience 11, Iter 12, disc loss: 0.08921402429454955, policy loss: 4.567340699900289
Experience 11, Iter 13, disc loss: 0.08039977329922822, policy loss: 4.233830363583898
Experience 11, Iter 14, disc loss: 0.09143636391695072, policy loss: 3.628351628084946
Experience 11, Iter 15, disc loss: 0.08949270299512715, policy loss: 3.8911952916343475
Experience 11, Iter 16, disc loss: 0.08374720989634189, policy loss: 3.8218072325497188
Experience 11, Iter 17, disc loss: 0.07436579983217806, policy loss: 3.93141762665671
Experience 11, Iter 18, disc loss: 0.08017063805598743, policy loss: 4.148618347484012
Experience 11, Iter 19, disc loss: 0.09189683703289439, policy loss: 3.6224083625309538
Experience 11, Iter 20, disc loss: 0.07233090118380042, policy loss: 4.258791576469203
Experience 11, Iter 21, disc loss: 0.0802552367357594, policy loss: 3.9632147333286722
Experience 11, Iter 22, disc loss: 0.07672588491429894, policy loss: 4.403346377459388
Experience 11, Iter 23, disc loss: 0.08352671526698935, policy loss: 3.8886683557812036
Experience 11, Iter 24, disc loss: 0.08521672474742004, policy loss: 4.580780973076958
Experience 11, Iter 25, disc loss: 0.07857615719395786, policy loss: 4.611596622599546
Experience 11, Iter 26, disc loss: 0.07744635587713397, policy loss: 5.195427466715363
Experience 11, Iter 27, disc loss: 0.07414379414796338, policy loss: 5.263670913695551
Experience 11, Iter 28, disc loss: 0.06684067381960354, policy loss: 5.093442271070245
Experience 11, Iter 29, disc loss: 0.07285903426860013, policy loss: 4.231588078743835
Experience 11, Iter 30, disc loss: 0.06890287786647455, policy loss: 4.708516582168119
Experience 11, Iter 31, disc loss: 0.062252164126599945, policy loss: 4.501783739679947
Experience 11, Iter 32, disc loss: 0.06604338524684664, policy loss: 4.722869134322222
Experience 11, Iter 33, disc loss: 0.06828150069613914, policy loss: 4.0902337164462255
Experience 11, Iter 34, disc loss: 0.06279107536852774, policy loss: 4.549586728098238
Experience 11, Iter 35, disc loss: 0.05895853181929753, policy loss: 5.020823369396618
Experience 11, Iter 36, disc loss: 0.05809175516187135, policy loss: 4.468486359528197
Experience 11, Iter 37, disc loss: 0.06131325909224836, policy loss: 4.3964027824547935
Experience 11, Iter 38, disc loss: 0.05595521198214218, policy loss: 4.488037470616787
Experience 11, Iter 39, disc loss: 0.06429302959646371, policy loss: 4.31315212853304
Experience 11, Iter 40, disc loss: 0.05490395472856169, policy loss: 4.893347949643282
Experience 11, Iter 41, disc loss: 0.061614463581971306, policy loss: 4.287298338567675
Experience 11, Iter 42, disc loss: 0.05286331177050797, policy loss: 4.676163979967038
Experience 11, Iter 43, disc loss: 0.06246748419485086, policy loss: 4.844017565312504
Experience 11, Iter 44, disc loss: 0.06255533051204545, policy loss: 4.972581578985257
Experience 11, Iter 45, disc loss: 0.050097132910614614, policy loss: 5.196737064943482
Experience 11, Iter 46, disc loss: 0.06007393234913667, policy loss: 5.2424588328830275
Experience 11, Iter 47, disc loss: 0.05491407929618901, policy loss: 4.66086258423158
Experience 11, Iter 48, disc loss: 0.05488440637673967, policy loss: 4.484496967927559
Experience 11, Iter 49, disc loss: 0.056649350353343716, policy loss: 4.070800844864696
Experience 11, Iter 50, disc loss: 0.05462284693195312, policy loss: 4.204492841331349
Experience 11, Iter 51, disc loss: 0.054113594410914644, policy loss: 4.65018180980502
Experience 11, Iter 52, disc loss: 0.055021737073662536, policy loss: 4.931090222274458
Experience 11, Iter 53, disc loss: 0.052083637296004355, policy loss: 5.210718947310275
Experience 11, Iter 54, disc loss: 0.05994691304254836, policy loss: 4.843143550840804
Experience 11, Iter 55, disc loss: 0.06115720231980716, policy loss: 5.502037819007656
Experience 11, Iter 56, disc loss: 0.057091002760752504, policy loss: 4.595143142929471
Experience 11, Iter 57, disc loss: 0.06491346516723462, policy loss: 4.336903424229789
Experience 11, Iter 58, disc loss: 0.05326567283620939, policy loss: 4.816575842419269
Experience 11, Iter 59, disc loss: 0.05851290198515881, policy loss: 4.597281457804584
Experience 11, Iter 60, disc loss: 0.05452235829938585, policy loss: 4.47924305496
Experience 11, Iter 61, disc loss: 0.044915895850163125, policy loss: 5.442967967943715
Experience 11, Iter 62, disc loss: 0.048619974367488564, policy loss: 5.651759987263607
Experience 11, Iter 63, disc loss: 0.0572745028327158, policy loss: 5.141218782415613
Experience 11, Iter 64, disc loss: 0.04899908362472276, policy loss: 5.1091840699485
Experience 11, Iter 65, disc loss: 0.051543861612742506, policy loss: 4.607932906151821
Experience 11, Iter 66, disc loss: 0.052324082610777076, policy loss: 5.562898695792933
Experience 11, Iter 67, disc loss: 0.06507690175785602, policy loss: 4.453383243548429
Experience 11, Iter 68, disc loss: 0.049936683392120644, policy loss: 5.34254369743824
Experience 11, Iter 69, disc loss: 0.04758503063960781, policy loss: 5.458963699704626
Experience 11, Iter 70, disc loss: 0.0503190333558966, policy loss: 4.931380526761183
Experience 11, Iter 71, disc loss: 0.052914258006983406, policy loss: 4.772627657830214
Experience 11, Iter 72, disc loss: 0.04459661215829419, policy loss: 5.001016141979031
Experience 11, Iter 73, disc loss: 0.04732241374272162, policy loss: 5.49899211487564
Experience 11, Iter 74, disc loss: 0.04269749449013458, policy loss: 5.032202765802946
Experience 11, Iter 75, disc loss: 0.041201527666314824, policy loss: 5.1353621624735935
Experience 11, Iter 76, disc loss: 0.040056079379797126, policy loss: 5.658839474015179
Experience 11, Iter 77, disc loss: 0.048352753818940844, policy loss: 4.69463567242925
Experience 11, Iter 78, disc loss: 0.04687096772899991, policy loss: 4.966916858874031
Experience 11, Iter 79, disc loss: 0.04562930506781733, policy loss: 5.212223418961722
Experience 11, Iter 80, disc loss: 0.051818981828326297, policy loss: 4.865831404336998
Experience 11, Iter 81, disc loss: 0.047480762444276625, policy loss: 4.5858427184792045
Experience 11, Iter 82, disc loss: 0.05306004581675097, policy loss: 4.586305018181445
Experience 11, Iter 83, disc loss: 0.03528823761872453, policy loss: 5.580794937640396
Experience 11, Iter 84, disc loss: 0.04985948900935913, policy loss: 4.972812666323565
Experience 11, Iter 85, disc loss: 0.0544420955281354, policy loss: 4.5355142826896
Experience 11, Iter 86, disc loss: 0.04550396529934351, policy loss: 4.940427624292319
Experience 11, Iter 87, disc loss: 0.04798264203232468, policy loss: 4.830977817280873
Experience 11, Iter 88, disc loss: 0.05150913545597097, policy loss: 5.004285302532994
Experience 11, Iter 89, disc loss: 0.05890580022958199, policy loss: 4.5711592688141005
Experience 11, Iter 90, disc loss: 0.04323809705381177, policy loss: 5.633653225916767
Experience 11, Iter 91, disc loss: 0.05035102354053231, policy loss: 5.0043275667794465
Experience 11, Iter 92, disc loss: 0.05179810383409443, policy loss: 5.641108054413629
Experience 11, Iter 93, disc loss: 0.05200908310517237, policy loss: 4.912822512806025
Experience 11, Iter 94, disc loss: 0.05075807603969759, policy loss: 5.004228806571507
Experience 11, Iter 95, disc loss: 0.049799504798309066, policy loss: 4.763983382078578
Experience 11, Iter 96, disc loss: 0.05237907410647905, policy loss: 5.074414424259433
Experience 11, Iter 97, disc loss: 0.03836054511658766, policy loss: 5.74902037992765
Experience 11, Iter 98, disc loss: 0.04165417687162154, policy loss: 5.814552841839588
Experience 11, Iter 99, disc loss: 0.047739001707122655, policy loss: 4.804435226776372
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.0923],
        [0.8679],
        [0.0184]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0163, 0.1548, 0.8539, 0.0175, 0.0133, 2.5338]],

        [[0.0163, 0.1548, 0.8539, 0.0175, 0.0133, 2.5338]],

        [[0.0163, 0.1548, 0.8539, 0.0175, 0.0133, 2.5338]],

        [[0.0163, 0.1548, 0.8539, 0.0175, 0.0133, 2.5338]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0161, 0.3691, 3.4716, 0.0735], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0161, 0.3691, 3.4716, 0.0735])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.488
Iter 2/2000 - Loss: 2.565
Iter 3/2000 - Loss: 2.411
Iter 4/2000 - Loss: 2.413
Iter 5/2000 - Loss: 2.429
Iter 6/2000 - Loss: 2.363
Iter 7/2000 - Loss: 2.297
Iter 8/2000 - Loss: 2.268
Iter 9/2000 - Loss: 2.248
Iter 10/2000 - Loss: 2.193
Iter 11/2000 - Loss: 2.105
Iter 12/2000 - Loss: 2.011
Iter 13/2000 - Loss: 1.922
Iter 14/2000 - Loss: 1.827
Iter 15/2000 - Loss: 1.707
Iter 16/2000 - Loss: 1.555
Iter 17/2000 - Loss: 1.378
Iter 18/2000 - Loss: 1.183
Iter 19/2000 - Loss: 0.973
Iter 20/2000 - Loss: 0.743
Iter 1981/2000 - Loss: -7.314
Iter 1982/2000 - Loss: -7.314
Iter 1983/2000 - Loss: -7.314
Iter 1984/2000 - Loss: -7.314
Iter 1985/2000 - Loss: -7.314
Iter 1986/2000 - Loss: -7.314
Iter 1987/2000 - Loss: -7.314
Iter 1988/2000 - Loss: -7.314
Iter 1989/2000 - Loss: -7.314
Iter 1990/2000 - Loss: -7.314
Iter 1991/2000 - Loss: -7.314
Iter 1992/2000 - Loss: -7.314
Iter 1993/2000 - Loss: -7.314
Iter 1994/2000 - Loss: -7.314
Iter 1995/2000 - Loss: -7.314
Iter 1996/2000 - Loss: -7.314
Iter 1997/2000 - Loss: -7.314
Iter 1998/2000 - Loss: -7.314
Iter 1999/2000 - Loss: -7.314
Iter 2000/2000 - Loss: -7.315
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.1589,  3.8689, 37.8105, 11.0646,  6.2083, 44.5528]],

        [[16.9321, 31.0378,  8.9405,  1.9464,  1.1781, 19.1491]],

        [[19.9715, 30.9315,  9.1350,  1.2882,  1.2750, 19.4506]],

        [[16.1586, 33.1481, 20.4406,  3.9078,  1.5606, 44.3392]]])
Signal Variance: tensor([ 0.0809,  1.7594, 18.1415,  0.6168])
Estimated target variance: tensor([0.0161, 0.3691, 3.4716, 0.0735])
N: 120
Signal to noise ratio: tensor([16.4919, 74.5691, 93.7896, 52.5768])
Bound on condition number: tensor([  32639.0512,  667267.6136, 1055579.2256,  331719.7155])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.047121375827654306, policy loss: 4.687664555070344
Experience 12, Iter 1, disc loss: 0.041231771217998425, policy loss: 4.936076994962457
Experience 12, Iter 2, disc loss: 0.04796029200244582, policy loss: 4.7022228363150145
Experience 12, Iter 3, disc loss: 0.05015643944376153, policy loss: 4.283939015016583
Experience 12, Iter 4, disc loss: 0.04386370690330283, policy loss: 4.840180937615128
Experience 12, Iter 5, disc loss: 0.05467845458284423, policy loss: 4.421098687260153
Experience 12, Iter 6, disc loss: 0.04363663373280949, policy loss: 5.210981291485572
Experience 12, Iter 7, disc loss: 0.047535629355541745, policy loss: 4.724457936060858
Experience 12, Iter 8, disc loss: 0.048503088425739714, policy loss: 4.564669600123479
Experience 12, Iter 9, disc loss: 0.048435349097073505, policy loss: 5.127400217849909
Experience 12, Iter 10, disc loss: 0.0458516395351478, policy loss: 4.7664754967464615
Experience 12, Iter 11, disc loss: 0.047355951985014126, policy loss: 4.988920189103861
Experience 12, Iter 12, disc loss: 0.0517529716867604, policy loss: 4.67217574884411
Experience 12, Iter 13, disc loss: 0.05014786864691255, policy loss: 4.852026057218492
Experience 12, Iter 14, disc loss: 0.05290995975417799, policy loss: 4.838819727439189
Experience 12, Iter 15, disc loss: 0.04682762129983635, policy loss: 5.754254658465901
Experience 12, Iter 16, disc loss: 0.04506859407825073, policy loss: 5.407156078878188
Experience 12, Iter 17, disc loss: 0.050505342408667664, policy loss: 5.781916605685843
Experience 12, Iter 18, disc loss: 0.045492669254658266, policy loss: 5.630838227271176
Experience 12, Iter 19, disc loss: 0.04194892854316652, policy loss: 6.495385028901725
Experience 12, Iter 20, disc loss: 0.050943344837662025, policy loss: 5.024702271543054
Experience 12, Iter 21, disc loss: 0.042540469960877965, policy loss: 5.808039788417754
Experience 12, Iter 22, disc loss: 0.03818283037144565, policy loss: 5.854347447052534
Experience 12, Iter 23, disc loss: 0.04752901620891663, policy loss: 5.591053768947592
Experience 12, Iter 24, disc loss: 0.04180110562935692, policy loss: 5.625401077681605
Experience 12, Iter 25, disc loss: 0.0410466584696392, policy loss: 5.353052740769728
Experience 12, Iter 26, disc loss: 0.0380281841302309, policy loss: 5.254142834316237
Experience 12, Iter 27, disc loss: 0.03566597210786106, policy loss: 5.385252816001054
Experience 12, Iter 28, disc loss: 0.04305826505828177, policy loss: 4.822554429667641
Experience 12, Iter 29, disc loss: 0.03902852228053297, policy loss: 5.699743390786997
Experience 12, Iter 30, disc loss: 0.03758999294172857, policy loss: 5.333171988257239
Experience 12, Iter 31, disc loss: 0.0375191719159849, policy loss: 5.171159753658831
Experience 12, Iter 32, disc loss: 0.038701981550922714, policy loss: 5.2307578739291785
Experience 12, Iter 33, disc loss: 0.044124153209564115, policy loss: 4.712692043921578
Experience 12, Iter 34, disc loss: 0.040804039311955545, policy loss: 5.491645937494443
Experience 12, Iter 35, disc loss: 0.03433632985992354, policy loss: 5.287604934334704
Experience 12, Iter 36, disc loss: 0.030672741082163522, policy loss: 5.222237215857696
Experience 12, Iter 37, disc loss: 0.03230576417282372, policy loss: 5.62098534865563
Experience 12, Iter 38, disc loss: 0.03549949069144657, policy loss: 5.090382701088773
Experience 12, Iter 39, disc loss: 0.03802576912806389, policy loss: 4.965982034117187
Experience 12, Iter 40, disc loss: 0.039564515904179344, policy loss: 5.403076603067042
Experience 12, Iter 41, disc loss: 0.029672943785922022, policy loss: 5.469832942070424
Experience 12, Iter 42, disc loss: 0.03093325272558558, policy loss: 5.569160344040459
Experience 12, Iter 43, disc loss: 0.042358699227407585, policy loss: 5.115324247506288
Experience 12, Iter 44, disc loss: 0.0359498425874723, policy loss: 4.888919385351645
Experience 12, Iter 45, disc loss: 0.039452936788680545, policy loss: 4.5292506238253365
Experience 12, Iter 46, disc loss: 0.04456362233685901, policy loss: 4.801610478871455
Experience 12, Iter 47, disc loss: 0.03843925382268132, policy loss: 6.204798583030186
Experience 12, Iter 48, disc loss: 0.038454226733987545, policy loss: 5.266568212724937
Experience 12, Iter 49, disc loss: 0.0317280851496835, policy loss: 5.56626260897137
Experience 12, Iter 50, disc loss: 0.03625660265024276, policy loss: 5.1653013466671425
Experience 12, Iter 51, disc loss: 0.042265612524138824, policy loss: 5.577563705536173
Experience 12, Iter 52, disc loss: 0.0353722670701988, policy loss: 5.4990824193886345
Experience 12, Iter 53, disc loss: 0.035736297862868735, policy loss: 5.483345493670248
Experience 12, Iter 54, disc loss: 0.036784375914573064, policy loss: 5.455649484518681
Experience 12, Iter 55, disc loss: 0.034355947674590846, policy loss: 5.032470274872874
Experience 12, Iter 56, disc loss: 0.0421361351261817, policy loss: 4.989550745861566
Experience 12, Iter 57, disc loss: 0.03412069477034886, policy loss: 5.074191377839355
Experience 12, Iter 58, disc loss: 0.03269819196261242, policy loss: 5.467804993069011
Experience 12, Iter 59, disc loss: 0.03278282512706458, policy loss: 5.537256129216168
Experience 12, Iter 60, disc loss: 0.03545747662266671, policy loss: 5.145932089861123
Experience 12, Iter 61, disc loss: 0.032751409083080545, policy loss: 5.152457348271088
Experience 12, Iter 62, disc loss: 0.03245136657114668, policy loss: 5.129041319712652
Experience 12, Iter 63, disc loss: 0.0323558382015432, policy loss: 5.1591195229080755
Experience 12, Iter 64, disc loss: 0.037301074468621076, policy loss: 4.956122213566629
Experience 12, Iter 65, disc loss: 0.02922450420784152, policy loss: 5.211185616075002
Experience 12, Iter 66, disc loss: 0.028571468508065308, policy loss: 5.704929422922344
Experience 12, Iter 67, disc loss: 0.029870752640715353, policy loss: 6.483164311218839
Experience 12, Iter 68, disc loss: 0.027426322564943727, policy loss: 6.122602112940703
Experience 12, Iter 69, disc loss: 0.034069706380067455, policy loss: 4.7936302472745425
Experience 12, Iter 70, disc loss: 0.035022064459369645, policy loss: 5.411548948659387
Experience 12, Iter 71, disc loss: 0.03318007806091255, policy loss: 5.080301479528806
Experience 12, Iter 72, disc loss: 0.030895354259194607, policy loss: 5.120805520327773
Experience 12, Iter 73, disc loss: 0.027500675636777296, policy loss: 5.271486744156181
Experience 12, Iter 74, disc loss: 0.03301352359618538, policy loss: 5.051572954680671
Experience 12, Iter 75, disc loss: 0.03011220841577624, policy loss: 5.176901676702432
Experience 12, Iter 76, disc loss: 0.025998896331182325, policy loss: 5.518555341176853
Experience 12, Iter 77, disc loss: 0.029087782482324034, policy loss: 5.245432424322698
Experience 12, Iter 78, disc loss: 0.0290742577853723, policy loss: 5.227945080039389
Experience 12, Iter 79, disc loss: 0.03236443425793053, policy loss: 5.4971256717184
Experience 12, Iter 80, disc loss: 0.03356822649019875, policy loss: 4.709795480192911
Experience 12, Iter 81, disc loss: 0.03403577465750161, policy loss: 5.114099834635939
Experience 12, Iter 82, disc loss: 0.030930415675968787, policy loss: 5.004005814625748
Experience 12, Iter 83, disc loss: 0.027691887882288206, policy loss: 5.17419852328875
Experience 12, Iter 84, disc loss: 0.02624243909610938, policy loss: 5.576153661269348
Experience 12, Iter 85, disc loss: 0.028528315979300896, policy loss: 5.046545072838727
Experience 12, Iter 86, disc loss: 0.030705522766034055, policy loss: 5.17579372481841
Experience 12, Iter 87, disc loss: 0.028668310953965027, policy loss: 5.268126709191689
Experience 12, Iter 88, disc loss: 0.029468237655462425, policy loss: 5.094472248867291
Experience 12, Iter 89, disc loss: 0.0343900252871325, policy loss: 4.693370282049775
Experience 12, Iter 90, disc loss: 0.027030823447453934, policy loss: 5.328274519965102
Experience 12, Iter 91, disc loss: 0.03133091300535427, policy loss: 5.621687824668413
Experience 12, Iter 92, disc loss: 0.02761884620104295, policy loss: 5.530149606780283
Experience 12, Iter 93, disc loss: 0.032212870409076076, policy loss: 5.003902262584436
Experience 12, Iter 94, disc loss: 0.0372928866288159, policy loss: 4.632594022054984
Experience 12, Iter 95, disc loss: 0.034525402361979894, policy loss: 5.479567703818745
Experience 12, Iter 96, disc loss: 0.032437518541500454, policy loss: 5.469478513035785
Experience 12, Iter 97, disc loss: 0.03360601453925194, policy loss: 4.825764478870262
Experience 12, Iter 98, disc loss: 0.03454265133078086, policy loss: 5.184516271747489
Experience 12, Iter 99, disc loss: 0.028946834812205374, policy loss: 5.205712265690431
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1087],
        [0.9881],
        [0.0207]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.1570, 0.9633, 0.0191, 0.0159, 2.8951]],

        [[0.0153, 0.1570, 0.9633, 0.0191, 0.0159, 2.8951]],

        [[0.0153, 0.1570, 0.9633, 0.0191, 0.0159, 2.8951]],

        [[0.0153, 0.1570, 0.9633, 0.0191, 0.0159, 2.8951]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.4347, 3.9522, 0.0828], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.4347, 3.9522, 0.0828])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.696
Iter 2/2000 - Loss: 2.776
Iter 3/2000 - Loss: 2.606
Iter 4/2000 - Loss: 2.623
Iter 5/2000 - Loss: 2.643
Iter 6/2000 - Loss: 2.567
Iter 7/2000 - Loss: 2.491
Iter 8/2000 - Loss: 2.462
Iter 9/2000 - Loss: 2.447
Iter 10/2000 - Loss: 2.393
Iter 11/2000 - Loss: 2.296
Iter 12/2000 - Loss: 2.186
Iter 13/2000 - Loss: 2.080
Iter 14/2000 - Loss: 1.972
Iter 15/2000 - Loss: 1.843
Iter 16/2000 - Loss: 1.680
Iter 17/2000 - Loss: 1.486
Iter 18/2000 - Loss: 1.271
Iter 19/2000 - Loss: 1.040
Iter 20/2000 - Loss: 0.793
Iter 1981/2000 - Loss: -7.336
Iter 1982/2000 - Loss: -7.336
Iter 1983/2000 - Loss: -7.336
Iter 1984/2000 - Loss: -7.337
Iter 1985/2000 - Loss: -7.337
Iter 1986/2000 - Loss: -7.337
Iter 1987/2000 - Loss: -7.337
Iter 1988/2000 - Loss: -7.337
Iter 1989/2000 - Loss: -7.337
Iter 1990/2000 - Loss: -7.337
Iter 1991/2000 - Loss: -7.337
Iter 1992/2000 - Loss: -7.337
Iter 1993/2000 - Loss: -7.337
Iter 1994/2000 - Loss: -7.337
Iter 1995/2000 - Loss: -7.337
Iter 1996/2000 - Loss: -7.337
Iter 1997/2000 - Loss: -7.337
Iter 1998/2000 - Loss: -7.337
Iter 1999/2000 - Loss: -7.337
Iter 2000/2000 - Loss: -7.337
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[12.1238,  7.0836, 35.6237, 10.2447,  3.6294, 51.3392]],

        [[15.5521, 34.7534,  8.4590,  1.9021,  1.2857, 15.8664]],

        [[14.8167, 32.2924,  8.6166,  1.3281,  1.2319, 18.9867]],

        [[14.9878, 33.3113, 19.9317,  3.4800,  1.5131, 47.7530]]])
Signal Variance: tensor([ 0.1107,  1.3602, 16.8110,  0.5919])
Estimated target variance: tensor([0.0159, 0.4347, 3.9522, 0.0828])
N: 130
Signal to noise ratio: tensor([18.0370, 65.9152, 90.5200, 52.4316])
Bound on condition number: tensor([  42294.3942,  564825.9528, 1065204.2809,  357379.9142])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.027679880092264065, policy loss: 5.0912198766756465
Experience 13, Iter 1, disc loss: 0.03279831046990836, policy loss: 5.181202166840314
Experience 13, Iter 2, disc loss: 0.031351736722601595, policy loss: 5.05961318169485
Experience 13, Iter 3, disc loss: 0.02929277867800465, policy loss: 5.720577848432026
Experience 13, Iter 4, disc loss: 0.030256448265717165, policy loss: 5.848732206578216
Experience 13, Iter 5, disc loss: 0.0280495204684215, policy loss: 6.147972971511603
Experience 13, Iter 6, disc loss: 0.03727792017211391, policy loss: 4.645681579037081
Experience 13, Iter 7, disc loss: 0.029003732069490237, policy loss: 5.446431531985592
Experience 13, Iter 8, disc loss: 0.028310179564100185, policy loss: 5.569580887422774
Experience 13, Iter 9, disc loss: 0.033367058255987556, policy loss: 4.566607329716954
Experience 13, Iter 10, disc loss: 0.026621420039324287, policy loss: 5.200719008353005
Experience 13, Iter 11, disc loss: 0.02526286788244215, policy loss: 5.161714468849236
Experience 13, Iter 12, disc loss: 0.02900909269167481, policy loss: 4.7394732863799645
Experience 13, Iter 13, disc loss: 0.029828786500442656, policy loss: 5.02152194570464
Experience 13, Iter 14, disc loss: 0.030903813953519925, policy loss: 4.734357310933283
Experience 13, Iter 15, disc loss: 0.02865360762231135, policy loss: 4.959012753326551
Experience 13, Iter 16, disc loss: 0.027176628935362032, policy loss: 5.100594861167001
Experience 13, Iter 17, disc loss: 0.02820823785301431, policy loss: 5.0250028396323625
Experience 13, Iter 18, disc loss: 0.026842832009347603, policy loss: 6.161612207304681
Experience 13, Iter 19, disc loss: 0.028601388388258413, policy loss: 4.931636448864557
Experience 13, Iter 20, disc loss: 0.027074269368857366, policy loss: 5.059254835428285
Experience 13, Iter 21, disc loss: 0.027531342080848894, policy loss: 4.946742122496602
Experience 13, Iter 22, disc loss: 0.027848696050213208, policy loss: 5.3626358485933405
Experience 13, Iter 23, disc loss: 0.026204040459864082, policy loss: 5.661052128869736
Experience 13, Iter 24, disc loss: 0.03235784679315601, policy loss: 4.9271366829152425
Experience 13, Iter 25, disc loss: 0.025074415776372577, policy loss: 5.3353468607304215
Experience 13, Iter 26, disc loss: 0.027254608351089456, policy loss: 5.003868652277412
Experience 13, Iter 27, disc loss: 0.029503404195786207, policy loss: 4.701426915248296
Experience 13, Iter 28, disc loss: 0.02816639139705029, policy loss: 4.736599983645567
Experience 13, Iter 29, disc loss: 0.029026225427646903, policy loss: 4.712556799205376
Experience 13, Iter 30, disc loss: 0.02874241259420417, policy loss: 5.129328624368754
Experience 13, Iter 31, disc loss: 0.028487077227048158, policy loss: 5.173196625091492
Experience 13, Iter 32, disc loss: 0.031045397325740447, policy loss: 5.121533174034699
Experience 13, Iter 33, disc loss: 0.02904117320666516, policy loss: 5.899552378552745
Experience 13, Iter 34, disc loss: 0.03163750358926355, policy loss: 5.920385443819404
Experience 13, Iter 35, disc loss: 0.028709628912742775, policy loss: 6.3460824336832005
Experience 13, Iter 36, disc loss: 0.02747582550991463, policy loss: 5.006998565612991
Experience 13, Iter 37, disc loss: 0.023869120346849893, policy loss: 6.953806929867383
Experience 13, Iter 38, disc loss: 0.024880588830899053, policy loss: 5.968456231046562
Experience 13, Iter 39, disc loss: 0.023635513079140594, policy loss: 5.417892146104713
Experience 13, Iter 40, disc loss: 0.030294536798133674, policy loss: 5.1320316976602225
Experience 13, Iter 41, disc loss: 0.030911230870557936, policy loss: 4.856697975256971
Experience 13, Iter 42, disc loss: 0.029683667530746173, policy loss: 4.907712286813743
Experience 13, Iter 43, disc loss: 0.023346690485517234, policy loss: 5.083362973017199
Experience 13, Iter 44, disc loss: 0.021783404906143583, policy loss: 5.671771597513513
Experience 13, Iter 45, disc loss: 0.023247695604430216, policy loss: 6.002372583649314
Experience 13, Iter 46, disc loss: 0.0161460542850191, policy loss: 7.1247277947892
Experience 13, Iter 47, disc loss: 0.015244038745432304, policy loss: 7.131566024054833
Experience 13, Iter 48, disc loss: 0.015238002280996244, policy loss: 6.979694287055529
Experience 13, Iter 49, disc loss: 0.015452690312299514, policy loss: 6.728050414545138
Experience 13, Iter 50, disc loss: 0.020133954314599765, policy loss: 5.667075752338043
Experience 13, Iter 51, disc loss: 0.021196622854386282, policy loss: 5.004260368025489
Experience 13, Iter 52, disc loss: 0.010894905632997231, policy loss: 7.678936467129775
Experience 13, Iter 53, disc loss: 0.019594514860790373, policy loss: 5.046465376530772
Experience 13, Iter 54, disc loss: 0.009693005242445617, policy loss: 7.981206478693918
Experience 13, Iter 55, disc loss: 0.008295520429586735, policy loss: 7.797169119140612
Experience 13, Iter 56, disc loss: 0.007625432092493747, policy loss: 8.355575058290048
Experience 13, Iter 57, disc loss: 0.006760847015016258, policy loss: 9.118419024605686
Experience 13, Iter 58, disc loss: 0.007527580782898718, policy loss: 8.17186357800084
Experience 13, Iter 59, disc loss: 0.007944935424414145, policy loss: 7.296718235681357
Experience 13, Iter 60, disc loss: 0.011937305870909914, policy loss: 6.493222817853217
Experience 13, Iter 61, disc loss: 0.011228331337036098, policy loss: 6.081715488882345
Experience 13, Iter 62, disc loss: 0.009616209618006888, policy loss: 5.984015802737829
Experience 13, Iter 63, disc loss: 0.008737135296104672, policy loss: 5.998150826919737
Experience 13, Iter 64, disc loss: 0.020246040477562538, policy loss: 4.896399499836667
Experience 13, Iter 65, disc loss: 0.030628491860375556, policy loss: 4.29521746974284
Experience 13, Iter 66, disc loss: 0.018985326526332055, policy loss: 5.133805986473802
Experience 13, Iter 67, disc loss: 0.014330857005765356, policy loss: 5.364438331780713
Experience 13, Iter 68, disc loss: 0.01539473472855275, policy loss: 5.689710673630339
Experience 13, Iter 69, disc loss: 0.0153768618203697, policy loss: 5.321793099757272
Experience 13, Iter 70, disc loss: 0.020274654584708576, policy loss: 5.099543311068722
Experience 13, Iter 71, disc loss: 0.02212842396649701, policy loss: 5.4669039645199975
Experience 13, Iter 72, disc loss: 0.018047472867789474, policy loss: 5.1642571052270885
Experience 13, Iter 73, disc loss: 0.020850492211800454, policy loss: 5.171107570854156
Experience 13, Iter 74, disc loss: 0.01671989744276388, policy loss: 6.151668994916321
Experience 13, Iter 75, disc loss: 0.019395820546928042, policy loss: 5.457913154715425
Experience 13, Iter 76, disc loss: 0.01962548142914968, policy loss: 5.754078776622148
Experience 13, Iter 77, disc loss: 0.02034334226777343, policy loss: 5.692134983612695
Experience 13, Iter 78, disc loss: 0.0195732965488327, policy loss: 5.953657457885464
Experience 13, Iter 79, disc loss: 0.01770362314971063, policy loss: 6.147418545013666
Experience 13, Iter 80, disc loss: 0.02240934154841029, policy loss: 5.437843509433061
Experience 13, Iter 81, disc loss: 0.0202305147833671, policy loss: 5.702083741701972
Experience 13, Iter 82, disc loss: 0.020647477293026135, policy loss: 5.5672972755894685
Experience 13, Iter 83, disc loss: 0.020728316585635356, policy loss: 6.227364566925946
Experience 13, Iter 84, disc loss: 0.019267638793175102, policy loss: 6.577522917695568
Experience 13, Iter 85, disc loss: 0.018685774113416275, policy loss: 5.500195207293547
Experience 13, Iter 86, disc loss: 0.025504220397359414, policy loss: 5.103321908496959
Experience 13, Iter 87, disc loss: 0.02544307580884165, policy loss: 5.501368617136389
Experience 13, Iter 88, disc loss: 0.017085602034137806, policy loss: 6.156997282520031
Experience 13, Iter 89, disc loss: 0.020881999892721714, policy loss: 5.749263796650562
Experience 13, Iter 90, disc loss: 0.019579798350936664, policy loss: 5.2042128960914305
Experience 13, Iter 91, disc loss: 0.01933743982734773, policy loss: 5.343225249607201
Experience 13, Iter 92, disc loss: 0.026975798328916514, policy loss: 5.264525444712289
Experience 13, Iter 93, disc loss: 0.02087569336827715, policy loss: 4.899638952954295
Experience 13, Iter 94, disc loss: 0.020897860153632423, policy loss: 5.295164970786727
Experience 13, Iter 95, disc loss: 0.02121907884605422, policy loss: 5.9237105588760635
Experience 13, Iter 96, disc loss: 0.021808433182160747, policy loss: 5.064372471227458
Experience 13, Iter 97, disc loss: 0.020672918752898308, policy loss: 5.573384854563059
Experience 13, Iter 98, disc loss: 0.020776926626507933, policy loss: 5.59548970963483
Experience 13, Iter 99, disc loss: 0.01934149376639139, policy loss: 5.5438099538944705
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1223],
        [1.0954],
        [0.0227]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0144, 0.1590, 1.0601, 0.0205, 0.0175, 3.1729]],

        [[0.0144, 0.1590, 1.0601, 0.0205, 0.0175, 3.1729]],

        [[0.0144, 0.1590, 1.0601, 0.0205, 0.0175, 3.1729]],

        [[0.0144, 0.1590, 1.0601, 0.0205, 0.0175, 3.1729]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.4891, 4.3816, 0.0908], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.4891, 4.3816, 0.0908])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.855
Iter 2/2000 - Loss: 2.941
Iter 3/2000 - Loss: 2.752
Iter 4/2000 - Loss: 2.776
Iter 5/2000 - Loss: 2.800
Iter 6/2000 - Loss: 2.716
Iter 7/2000 - Loss: 2.624
Iter 8/2000 - Loss: 2.584
Iter 9/2000 - Loss: 2.566
Iter 10/2000 - Loss: 2.513
Iter 11/2000 - Loss: 2.410
Iter 12/2000 - Loss: 2.284
Iter 13/2000 - Loss: 2.159
Iter 14/2000 - Loss: 2.035
Iter 15/2000 - Loss: 1.894
Iter 16/2000 - Loss: 1.722
Iter 17/2000 - Loss: 1.516
Iter 18/2000 - Loss: 1.283
Iter 19/2000 - Loss: 1.031
Iter 20/2000 - Loss: 0.761
Iter 1981/2000 - Loss: -7.442
Iter 1982/2000 - Loss: -7.442
Iter 1983/2000 - Loss: -7.442
Iter 1984/2000 - Loss: -7.443
Iter 1985/2000 - Loss: -7.443
Iter 1986/2000 - Loss: -7.443
Iter 1987/2000 - Loss: -7.443
Iter 1988/2000 - Loss: -7.443
Iter 1989/2000 - Loss: -7.443
Iter 1990/2000 - Loss: -7.443
Iter 1991/2000 - Loss: -7.443
Iter 1992/2000 - Loss: -7.443
Iter 1993/2000 - Loss: -7.443
Iter 1994/2000 - Loss: -7.443
Iter 1995/2000 - Loss: -7.443
Iter 1996/2000 - Loss: -7.443
Iter 1997/2000 - Loss: -7.443
Iter 1998/2000 - Loss: -7.443
Iter 1999/2000 - Loss: -7.443
Iter 2000/2000 - Loss: -7.443
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[12.7531,  7.3902, 37.8259, 10.1998,  3.5051, 52.3829]],

        [[16.0140, 34.5556,  8.1132,  2.0480,  1.3149, 18.0916]],

        [[16.6752, 33.3705,  8.0789,  1.2821,  1.1362, 22.4831]],

        [[13.4571, 32.2599, 20.0174,  3.5686,  1.5197, 48.3261]]])
Signal Variance: tensor([ 0.1095,  1.6056, 16.9750,  0.5979])
Estimated target variance: tensor([0.0158, 0.4891, 4.3816, 0.0908])
N: 140
Signal to noise ratio: tensor([17.7658, 71.7327, 90.4514, 53.2012])
Bound on condition number: tensor([  44188.2570,  720381.5891, 1145403.6336,  396253.1462])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.01838553954460553, policy loss: 6.2383785142712735
Experience 14, Iter 1, disc loss: 0.02020927696898594, policy loss: 5.441225893039161
Experience 14, Iter 2, disc loss: 0.020686660416438005, policy loss: 5.895817672033932
Experience 14, Iter 3, disc loss: 0.02075653999041462, policy loss: 5.603157793991843
Experience 14, Iter 4, disc loss: 0.017549564006425367, policy loss: 5.669771126542254
Experience 14, Iter 5, disc loss: 0.020926007544952125, policy loss: 5.473062806658823
Experience 14, Iter 6, disc loss: 0.018777862291028936, policy loss: 6.024666788256473
Experience 14, Iter 7, disc loss: 0.020691522082780314, policy loss: 5.539230835102496
Experience 14, Iter 8, disc loss: 0.016945977508700805, policy loss: 6.981038609954609
Experience 14, Iter 9, disc loss: 0.017760948656203264, policy loss: 5.457842962077549
Experience 14, Iter 10, disc loss: 0.01751618786863586, policy loss: 5.66483353159229
Experience 14, Iter 11, disc loss: 0.017593762755050372, policy loss: 5.302239831022396
Experience 14, Iter 12, disc loss: 0.018219452463863542, policy loss: 5.584823005971862
Experience 14, Iter 13, disc loss: 0.019590576709857047, policy loss: 5.209280786911985
Experience 14, Iter 14, disc loss: 0.020175314627751437, policy loss: 5.686206019227262
Experience 14, Iter 15, disc loss: 0.018401847299668324, policy loss: 5.324053839147922
Experience 14, Iter 16, disc loss: 0.02174932110723873, policy loss: 5.474264450133351
Experience 14, Iter 17, disc loss: 0.024319183215030866, policy loss: 4.941906598756619
Experience 14, Iter 18, disc loss: 0.02103492557987991, policy loss: 5.218032951766693
Experience 14, Iter 19, disc loss: 0.018318208219599078, policy loss: 6.063246585954185
Experience 14, Iter 20, disc loss: 0.0203631084504047, policy loss: 5.288093824051333
Experience 14, Iter 21, disc loss: 0.01949839979277119, policy loss: 5.515020533994048
Experience 14, Iter 22, disc loss: 0.018626670612360945, policy loss: 6.135773642315867
Experience 14, Iter 23, disc loss: 0.020665801018374848, policy loss: 5.144081733347557
Experience 14, Iter 24, disc loss: 0.019651278553103668, policy loss: 5.892052311395494
Experience 14, Iter 25, disc loss: 0.019766604087830077, policy loss: 5.148352708182093
Experience 14, Iter 26, disc loss: 0.01932995986778211, policy loss: 5.172452078216228
Experience 14, Iter 27, disc loss: 0.01914026969802523, policy loss: 6.019261951652976
Experience 14, Iter 28, disc loss: 0.019915013156189022, policy loss: 5.548390770659525
Experience 14, Iter 29, disc loss: 0.022447678723028285, policy loss: 5.184055975553938
Experience 14, Iter 30, disc loss: 0.019959121759920803, policy loss: 5.593750750939995
Experience 14, Iter 31, disc loss: 0.019157222897922564, policy loss: 5.810758040586307
Experience 14, Iter 32, disc loss: 0.019239220071571424, policy loss: 5.28353048533066
Experience 14, Iter 33, disc loss: 0.020271954016026127, policy loss: 6.022052169517687
Experience 14, Iter 34, disc loss: 0.01978588592166943, policy loss: 5.4319079767899705
Experience 14, Iter 35, disc loss: 0.0198711202196653, policy loss: 5.17237826668651
Experience 14, Iter 36, disc loss: 0.018856329674833222, policy loss: 5.48485600678044
Experience 14, Iter 37, disc loss: 0.01968117653044758, policy loss: 6.290628866750031
Experience 14, Iter 38, disc loss: 0.018506244910376078, policy loss: 5.925715720839712
Experience 14, Iter 39, disc loss: 0.01850122660556236, policy loss: 5.519974997662354
Experience 14, Iter 40, disc loss: 0.016298027844911203, policy loss: 5.964618461967048
Experience 14, Iter 41, disc loss: 0.017427047082598287, policy loss: 5.492524332659826
Experience 14, Iter 42, disc loss: 0.017984798954429664, policy loss: 5.3911601422055675
Experience 14, Iter 43, disc loss: 0.018680058971750953, policy loss: 5.392483636350052
Experience 14, Iter 44, disc loss: 0.016776358139786694, policy loss: 5.999951078716742
Experience 14, Iter 45, disc loss: 0.017114767516996993, policy loss: 5.7286283859641385
Experience 14, Iter 46, disc loss: 0.018599988326838172, policy loss: 5.3727269020543185
Experience 14, Iter 47, disc loss: 0.01721000131962496, policy loss: 5.426816347139141
Experience 14, Iter 48, disc loss: 0.018691001172945025, policy loss: 6.094848071718435
Experience 14, Iter 49, disc loss: 0.0169192807794252, policy loss: 5.453167178410624
Experience 14, Iter 50, disc loss: 0.015711859714246434, policy loss: 5.772482618889249
Experience 14, Iter 51, disc loss: 0.015146013844865141, policy loss: 5.820922587335193
Experience 14, Iter 52, disc loss: 0.01776062617063357, policy loss: 6.368217029693836
Experience 14, Iter 53, disc loss: 0.016323475270949402, policy loss: 5.501195534997878
Experience 14, Iter 54, disc loss: 0.017106165286282456, policy loss: 5.4855091199840675
Experience 14, Iter 55, disc loss: 0.017160973396501095, policy loss: 5.460711762978053
Experience 14, Iter 56, disc loss: 0.01785530442463657, policy loss: 5.380610147373689
Experience 14, Iter 57, disc loss: 0.016655004813927315, policy loss: 5.200724930389412
Experience 14, Iter 58, disc loss: 0.016427497874521857, policy loss: 6.139366873777288
Experience 14, Iter 59, disc loss: 0.015000942744705457, policy loss: 5.659250166334068
Experience 14, Iter 60, disc loss: 0.01706624797222222, policy loss: 5.681727815287509
Experience 14, Iter 61, disc loss: 0.015151794105679003, policy loss: 6.194843911838928
Experience 14, Iter 62, disc loss: 0.017045911526920747, policy loss: 7.529229021444703
Experience 14, Iter 63, disc loss: 0.016342645255820264, policy loss: 6.884276745112373
Experience 14, Iter 64, disc loss: 0.016062435077092888, policy loss: 6.105054713921548
Experience 14, Iter 65, disc loss: 0.014977158938730684, policy loss: 6.094898178358281
Experience 14, Iter 66, disc loss: 0.01576751662380759, policy loss: 5.627760688716018
Experience 14, Iter 67, disc loss: 0.015601229616017362, policy loss: 5.494666660366472
Experience 14, Iter 68, disc loss: 0.012818938300134315, policy loss: 6.170550818396304
Experience 14, Iter 69, disc loss: 0.017411775116959766, policy loss: 5.798189011176001
Experience 14, Iter 70, disc loss: 0.0177263085729054, policy loss: 5.884654862222553
Experience 14, Iter 71, disc loss: 0.013455674439426961, policy loss: 6.036940261709263
Experience 14, Iter 72, disc loss: 0.016589005916119517, policy loss: 5.42313395051317
Experience 14, Iter 73, disc loss: 0.017425476857517865, policy loss: 5.653123935433349
Experience 14, Iter 74, disc loss: 0.014666245611592245, policy loss: 6.161603562188562
Experience 14, Iter 75, disc loss: 0.01637246824160047, policy loss: 5.673687427361625
Experience 14, Iter 76, disc loss: 0.01316155262742046, policy loss: 5.932004887390214
Experience 14, Iter 77, disc loss: 0.012507360983949136, policy loss: 6.2273499270031625
Experience 14, Iter 78, disc loss: 0.01301842357565066, policy loss: 7.290767739350136
Experience 14, Iter 79, disc loss: 0.01621555167346992, policy loss: 5.5878362284375775
Experience 14, Iter 80, disc loss: 0.012998163035529003, policy loss: 6.472567605311386
Experience 14, Iter 81, disc loss: 0.01310620565637125, policy loss: 5.80926671809705
Experience 14, Iter 82, disc loss: 0.012864430996247227, policy loss: 6.350917844971727
Experience 14, Iter 83, disc loss: 0.012388299570762424, policy loss: 5.743535048000062
Experience 14, Iter 84, disc loss: 0.012779567707529107, policy loss: 5.782368212532438
Experience 14, Iter 85, disc loss: 0.01200050602174772, policy loss: 6.094303025106448
Experience 14, Iter 86, disc loss: 0.01361697514932177, policy loss: 5.65624418504755
Experience 14, Iter 87, disc loss: 0.011025585368112987, policy loss: 6.556821616906932
Experience 14, Iter 88, disc loss: 0.012661727618972291, policy loss: 5.913420701133028
Experience 14, Iter 89, disc loss: 0.01269174576380033, policy loss: 5.682183611737603
Experience 14, Iter 90, disc loss: 0.012928876113959127, policy loss: 5.9447335590617625
Experience 14, Iter 91, disc loss: 0.015023689992552864, policy loss: 5.421363247603632
Experience 14, Iter 92, disc loss: 0.014433701257000057, policy loss: 6.200376594629675
Experience 14, Iter 93, disc loss: 0.013541793698070915, policy loss: 5.8017008949374205
Experience 14, Iter 94, disc loss: 0.014744472815428834, policy loss: 6.356347407306409
Experience 14, Iter 95, disc loss: 0.012669751396388748, policy loss: 5.750637245271831
Experience 14, Iter 96, disc loss: 0.015093417101801522, policy loss: 5.796685696547295
Experience 14, Iter 97, disc loss: 0.012893940227885394, policy loss: 6.117842857322412
Experience 14, Iter 98, disc loss: 0.0119279043672603, policy loss: 6.411545939362663
Experience 14, Iter 99, disc loss: 0.013006952769592933, policy loss: 5.997495919511974
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1325],
        [1.1628],
        [0.0239]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0137, 0.1590, 1.1202, 0.0215, 0.0192, 3.4314]],

        [[0.0137, 0.1590, 1.1202, 0.0215, 0.0192, 3.4314]],

        [[0.0137, 0.1590, 1.1202, 0.0215, 0.0192, 3.4314]],

        [[0.0137, 0.1590, 1.1202, 0.0215, 0.0192, 3.4314]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.5299, 4.6512, 0.0958], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.5299, 4.6512, 0.0958])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.951
Iter 2/2000 - Loss: 3.033
Iter 3/2000 - Loss: 2.827
Iter 4/2000 - Loss: 2.848
Iter 5/2000 - Loss: 2.870
Iter 6/2000 - Loss: 2.773
Iter 7/2000 - Loss: 2.659
Iter 8/2000 - Loss: 2.596
Iter 9/2000 - Loss: 2.561
Iter 10/2000 - Loss: 2.494
Iter 11/2000 - Loss: 2.375
Iter 12/2000 - Loss: 2.224
Iter 13/2000 - Loss: 2.070
Iter 14/2000 - Loss: 1.916
Iter 15/2000 - Loss: 1.751
Iter 16/2000 - Loss: 1.558
Iter 17/2000 - Loss: 1.333
Iter 18/2000 - Loss: 1.080
Iter 19/2000 - Loss: 0.806
Iter 20/2000 - Loss: 0.516
Iter 1981/2000 - Loss: -7.525
Iter 1982/2000 - Loss: -7.525
Iter 1983/2000 - Loss: -7.525
Iter 1984/2000 - Loss: -7.525
Iter 1985/2000 - Loss: -7.525
Iter 1986/2000 - Loss: -7.525
Iter 1987/2000 - Loss: -7.525
Iter 1988/2000 - Loss: -7.525
Iter 1989/2000 - Loss: -7.525
Iter 1990/2000 - Loss: -7.525
Iter 1991/2000 - Loss: -7.525
Iter 1992/2000 - Loss: -7.525
Iter 1993/2000 - Loss: -7.525
Iter 1994/2000 - Loss: -7.525
Iter 1995/2000 - Loss: -7.525
Iter 1996/2000 - Loss: -7.525
Iter 1997/2000 - Loss: -7.525
Iter 1998/2000 - Loss: -7.525
Iter 1999/2000 - Loss: -7.525
Iter 2000/2000 - Loss: -7.525
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[10.9404,  3.8567, 31.2826,  9.2674,  6.8543, 47.3095]],

        [[15.8793, 33.5539,  8.0995,  2.0639,  1.3065, 18.0109]],

        [[16.3771, 32.3132,  8.2310,  1.2753,  1.1494, 22.4584]],

        [[13.4216, 31.1520, 20.3797,  3.7926,  1.5516, 49.3189]]])
Signal Variance: tensor([ 0.0747,  1.6087, 17.7436,  0.6339])
Estimated target variance: tensor([0.0157, 0.5299, 4.6512, 0.0958])
N: 150
Signal to noise ratio: tensor([14.5145, 73.2149, 90.6036, 54.4953])
Bound on condition number: tensor([  31601.4947,  804064.9696, 1231352.4893,  445462.1362])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.012854884086364603, policy loss: 5.9957020887300985
Experience 15, Iter 1, disc loss: 0.012777678512640354, policy loss: 6.523550811226823
Experience 15, Iter 2, disc loss: 0.011990015566934126, policy loss: 6.160837652682373
Experience 15, Iter 3, disc loss: 0.01338383743062745, policy loss: 6.062209018078018
Experience 15, Iter 4, disc loss: 0.012342647142076505, policy loss: 5.99347239777718
Experience 15, Iter 5, disc loss: 0.014013409026958211, policy loss: 5.770994332143565
Experience 15, Iter 6, disc loss: 0.012625042935518003, policy loss: 6.238102401474647
Experience 15, Iter 7, disc loss: 0.014290747693161613, policy loss: 5.702595326326734
Experience 15, Iter 8, disc loss: 0.014736463410675946, policy loss: 5.923521615071319
Experience 15, Iter 9, disc loss: 0.012184080941149852, policy loss: 5.6444547524664195
Experience 15, Iter 10, disc loss: 0.012152360753583965, policy loss: 6.5049851696639145
Experience 15, Iter 11, disc loss: 0.012470743105436038, policy loss: 6.432411882413962
Experience 15, Iter 12, disc loss: 0.012187888430980195, policy loss: 6.122258510399999
Experience 15, Iter 13, disc loss: 0.011596071590397543, policy loss: 6.614138227330958
Experience 15, Iter 14, disc loss: 0.010547817607437856, policy loss: 6.006552589451966
Experience 15, Iter 15, disc loss: 0.011932868716725456, policy loss: 5.94521708738134
Experience 15, Iter 16, disc loss: 0.01227575501553997, policy loss: 6.1393748398347
Experience 15, Iter 17, disc loss: 0.01183405945492793, policy loss: 5.9777041615481945
Experience 15, Iter 18, disc loss: 0.011012158441031459, policy loss: 5.965374363514643
Experience 15, Iter 19, disc loss: 0.010624782159326539, policy loss: 6.28838724917594
Experience 15, Iter 20, disc loss: 0.010418698527540904, policy loss: 5.983786941620197
Experience 15, Iter 21, disc loss: 0.011435098844303219, policy loss: 5.8775563413682885
Experience 15, Iter 22, disc loss: 0.011834478584387485, policy loss: 5.688800651912846
Experience 15, Iter 23, disc loss: 0.010525742256415628, policy loss: 6.029162033049401
Experience 15, Iter 24, disc loss: 0.010119797328896538, policy loss: 7.54878856449063
Experience 15, Iter 25, disc loss: 0.012897963346090988, policy loss: 5.6621162006589065
Experience 15, Iter 26, disc loss: 0.012731287911867324, policy loss: 6.16421444017335
Experience 15, Iter 27, disc loss: 0.009629785135612173, policy loss: 6.129929343961793
Experience 15, Iter 28, disc loss: 0.011358420382916689, policy loss: 6.764265495472123
Experience 15, Iter 29, disc loss: 0.011462236435053936, policy loss: 5.958837755989491
Experience 15, Iter 30, disc loss: 0.01067274664664605, policy loss: 6.099470632584666
Experience 15, Iter 31, disc loss: 0.011211703654361159, policy loss: 6.403902606773248
Experience 15, Iter 32, disc loss: 0.010403302927332964, policy loss: 6.331959041779119
Experience 15, Iter 33, disc loss: 0.0116599035044427, policy loss: 5.94885718265781
Experience 15, Iter 34, disc loss: 0.010616092217995842, policy loss: 6.327222927560389
Experience 15, Iter 35, disc loss: 0.012212619917130314, policy loss: 6.097959714543046
Experience 15, Iter 36, disc loss: 0.01157639887906193, policy loss: 6.379082126096686
Experience 15, Iter 37, disc loss: 0.0106405575025221, policy loss: 6.049922220623207
Experience 15, Iter 38, disc loss: 0.00923847380950562, policy loss: 6.977902065406678
Experience 15, Iter 39, disc loss: 0.011305165376309562, policy loss: 6.166476923873091
Experience 15, Iter 40, disc loss: 0.011064296368580109, policy loss: 6.870890514849386
Experience 15, Iter 41, disc loss: 0.011215325252481548, policy loss: 6.902000603262945
Experience 15, Iter 42, disc loss: 0.011056280052387395, policy loss: 6.178995396195688
Experience 15, Iter 43, disc loss: 0.010689492578859398, policy loss: 6.237745369243749
Experience 15, Iter 44, disc loss: 0.01062221053389073, policy loss: 6.4759683966315675
Experience 15, Iter 45, disc loss: 0.010268282718660674, policy loss: 6.457475022690623
Experience 15, Iter 46, disc loss: 0.009529156554983046, policy loss: 6.4489165703889375
Experience 15, Iter 47, disc loss: 0.009566823946960542, policy loss: 6.694406147470323
Experience 15, Iter 48, disc loss: 0.010184341178723957, policy loss: 6.265709776568104
Experience 15, Iter 49, disc loss: 0.010430805334929946, policy loss: 5.984533343452323
Experience 15, Iter 50, disc loss: 0.00776258055191652, policy loss: 6.693515038989796
Experience 15, Iter 51, disc loss: 0.009888535172183176, policy loss: 6.45229377545898
Experience 15, Iter 52, disc loss: 0.008788735450506464, policy loss: 7.098233144512665
Experience 15, Iter 53, disc loss: 0.007064026563064326, policy loss: 7.125555743825789
Experience 15, Iter 54, disc loss: 0.0061067139424446845, policy loss: 7.026294167716657
Experience 15, Iter 55, disc loss: 0.008140093721904443, policy loss: 6.896145586559949
Experience 15, Iter 56, disc loss: 0.008669339706270256, policy loss: 6.4401670410157195
Experience 15, Iter 57, disc loss: 0.01014593204557749, policy loss: 5.943037032408135
Experience 15, Iter 58, disc loss: 0.005264509041149756, policy loss: 9.329073390394893
Experience 15, Iter 59, disc loss: 0.005002187463167044, policy loss: 8.261369458195258
Experience 15, Iter 60, disc loss: 0.003786522313005491, policy loss: 11.951900509802552
Experience 15, Iter 61, disc loss: 0.003504317836720539, policy loss: 14.05233110895099
Experience 15, Iter 62, disc loss: 0.0032980866109351054, policy loss: 15.644373630445205
Experience 15, Iter 63, disc loss: 0.003084279940406868, policy loss: 15.572058115719301
Experience 15, Iter 64, disc loss: 0.0028682272242330858, policy loss: 14.06498965379473
Experience 15, Iter 65, disc loss: 0.0026587934755658575, policy loss: 16.59402571433248
Experience 15, Iter 66, disc loss: 0.0024593912392816596, policy loss: 16.791125755391196
Experience 15, Iter 67, disc loss: 0.0022718108771253498, policy loss: 15.493157980093397
Experience 15, Iter 68, disc loss: 0.002097986330330203, policy loss: 14.495318681593288
Experience 15, Iter 69, disc loss: 0.0019420708205289039, policy loss: 14.884051914746975
Experience 15, Iter 70, disc loss: 0.0017938085798562568, policy loss: 14.18717656594688
Experience 15, Iter 71, disc loss: 0.0016661682145492989, policy loss: 14.22250199779454
Experience 15, Iter 72, disc loss: 0.0015730965612584804, policy loss: 12.388156800121825
Experience 15, Iter 73, disc loss: 0.001521039148693413, policy loss: 10.142969693521845
Experience 15, Iter 74, disc loss: 0.0015924650029564514, policy loss: 9.590131544484368
Experience 15, Iter 75, disc loss: 0.0018375958220699636, policy loss: 8.120007670986757
Experience 15, Iter 76, disc loss: 0.002597291067436798, policy loss: 7.265897159903612
Experience 15, Iter 77, disc loss: 0.0020896878642007134, policy loss: 7.264784516732034
Experience 15, Iter 78, disc loss: 0.00144417988918156, policy loss: 8.57139162177971
Experience 15, Iter 79, disc loss: 0.0018592344817711412, policy loss: 8.01250575737185
Experience 15, Iter 80, disc loss: 0.00892633021034807, policy loss: 5.438681411333833
Experience 15, Iter 81, disc loss: 0.009982017153964283, policy loss: 5.537479377730236
Experience 15, Iter 82, disc loss: 0.010892534711930102, policy loss: 5.575902572466835
Experience 15, Iter 83, disc loss: 0.00821394662439625, policy loss: 6.439158915802753
Experience 15, Iter 84, disc loss: 0.0063935088052194215, policy loss: 6.039602913347061
Experience 15, Iter 85, disc loss: 0.00542714057164524, policy loss: 6.105413139910348
Experience 15, Iter 86, disc loss: 0.003998298798135072, policy loss: 6.549672225394143
Experience 15, Iter 87, disc loss: 0.004632544864038548, policy loss: 6.913747797004659
Experience 15, Iter 88, disc loss: 0.005630715314991309, policy loss: 6.111063615697757
Experience 15, Iter 89, disc loss: 0.003939084452712869, policy loss: 7.214464377175189
Experience 15, Iter 90, disc loss: 0.005193195269897673, policy loss: 6.350524596316978
Experience 15, Iter 91, disc loss: 0.004268588105478364, policy loss: 7.111076541470988
Experience 15, Iter 92, disc loss: 0.004451657090073366, policy loss: 6.579343010756873
Experience 15, Iter 93, disc loss: 0.004504106490414134, policy loss: 7.30754689420726
Experience 15, Iter 94, disc loss: 0.004262172328934154, policy loss: 6.891429560342017
Experience 15, Iter 95, disc loss: 0.0034850625260846113, policy loss: 7.855491284236509
Experience 15, Iter 96, disc loss: 0.004253068663178846, policy loss: 6.885711701565864
Experience 15, Iter 97, disc loss: 0.0050953324877219385, policy loss: 6.6944307425062295
Experience 15, Iter 98, disc loss: 0.0065173556250335985, policy loss: 6.66386731120522
Experience 15, Iter 99, disc loss: 0.004867756944196919, policy loss: 7.0868406346568165
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1410],
        [1.2090],
        [0.0247]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0131, 0.1608, 1.1573, 0.0224, 0.0210, 3.6918]],

        [[0.0131, 0.1608, 1.1573, 0.0224, 0.0210, 3.6918]],

        [[0.0131, 0.1608, 1.1573, 0.0224, 0.0210, 3.6918]],

        [[0.0131, 0.1608, 1.1573, 0.0224, 0.0210, 3.6918]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0156, 0.5640, 4.8361, 0.0990], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0156, 0.5640, 4.8361, 0.0990])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.025
Iter 2/2000 - Loss: 3.119
Iter 3/2000 - Loss: 2.891
Iter 4/2000 - Loss: 2.919
Iter 5/2000 - Loss: 2.954
Iter 6/2000 - Loss: 2.857
Iter 7/2000 - Loss: 2.734
Iter 8/2000 - Loss: 2.659
Iter 9/2000 - Loss: 2.620
Iter 10/2000 - Loss: 2.559
Iter 11/2000 - Loss: 2.444
Iter 12/2000 - Loss: 2.288
Iter 13/2000 - Loss: 2.118
Iter 14/2000 - Loss: 1.948
Iter 15/2000 - Loss: 1.770
Iter 16/2000 - Loss: 1.568
Iter 17/2000 - Loss: 1.334
Iter 18/2000 - Loss: 1.069
Iter 19/2000 - Loss: 0.779
Iter 20/2000 - Loss: 0.473
Iter 1981/2000 - Loss: -7.563
Iter 1982/2000 - Loss: -7.563
Iter 1983/2000 - Loss: -7.563
Iter 1984/2000 - Loss: -7.563
Iter 1985/2000 - Loss: -7.563
Iter 1986/2000 - Loss: -7.563
Iter 1987/2000 - Loss: -7.563
Iter 1988/2000 - Loss: -7.563
Iter 1989/2000 - Loss: -7.563
Iter 1990/2000 - Loss: -7.563
Iter 1991/2000 - Loss: -7.563
Iter 1992/2000 - Loss: -7.563
Iter 1993/2000 - Loss: -7.563
Iter 1994/2000 - Loss: -7.563
Iter 1995/2000 - Loss: -7.563
Iter 1996/2000 - Loss: -7.563
Iter 1997/2000 - Loss: -7.563
Iter 1998/2000 - Loss: -7.563
Iter 1999/2000 - Loss: -7.563
Iter 2000/2000 - Loss: -7.563
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[11.6234,  3.4077, 32.4533,  9.6892,  7.3497, 45.5395]],

        [[16.3637, 33.0322,  7.5310,  1.6194,  1.8005, 18.4761]],

        [[17.0817, 29.9845,  8.2889,  1.2289,  1.1416, 22.5055]],

        [[12.8594, 29.4392, 20.2602,  3.5473,  1.5094, 49.0270]]])
Signal Variance: tensor([ 0.0660,  1.5429, 17.7886,  0.6060])
Estimated target variance: tensor([0.0156, 0.5640, 4.8361, 0.0990])
N: 160
Signal to noise ratio: tensor([13.6876, 70.0217, 88.7894, 52.5519])
Bound on condition number: tensor([  29977.0186,  784486.1782, 1261371.4109,  441873.8118])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.006050001534700027, policy loss: 7.1160416253137555
Experience 16, Iter 1, disc loss: 0.005891049820271178, policy loss: 6.525045025231786
Experience 16, Iter 2, disc loss: 0.006151204764050606, policy loss: 6.9158452450440455
Experience 16, Iter 3, disc loss: 0.005404475810917108, policy loss: 7.768006710060683
Experience 16, Iter 4, disc loss: 0.005431713807496778, policy loss: 7.1553098536898405
Experience 16, Iter 5, disc loss: 0.005446392836366384, policy loss: 6.686064711530641
Experience 16, Iter 6, disc loss: 0.006407282374764606, policy loss: 6.868559833928428
Experience 16, Iter 7, disc loss: 0.006288644310814206, policy loss: 7.71465764097072
Experience 16, Iter 8, disc loss: 0.006433622833631588, policy loss: 8.392563035452564
Experience 16, Iter 9, disc loss: 0.007967125778356137, policy loss: 7.508562892628255
Experience 16, Iter 10, disc loss: 0.006916735778552775, policy loss: 6.387904998264167
Experience 16, Iter 11, disc loss: 0.009200606359460885, policy loss: 6.202331643276898
Experience 16, Iter 12, disc loss: 0.007954813419952595, policy loss: 6.849161025593432
Experience 16, Iter 13, disc loss: 0.007566240744102948, policy loss: 6.65143164029542
Experience 16, Iter 14, disc loss: 0.0076759626200679845, policy loss: 6.649574815872823
Experience 16, Iter 15, disc loss: 0.0075908970866420174, policy loss: 7.396594100366599
Experience 16, Iter 16, disc loss: 0.0069826347747546855, policy loss: 6.8943800212224
Experience 16, Iter 17, disc loss: 0.006977293218644147, policy loss: 6.275732168874794
Experience 16, Iter 18, disc loss: 0.007003103858699594, policy loss: 6.684572716135433
Experience 16, Iter 19, disc loss: 0.00808087589117937, policy loss: 6.596575768809208
Experience 16, Iter 20, disc loss: 0.007777952994216805, policy loss: 6.268698542422948
Experience 16, Iter 21, disc loss: 0.006004059171590786, policy loss: 7.651974111861949
Experience 16, Iter 22, disc loss: 0.006070146364963653, policy loss: 7.736678799198257
Experience 16, Iter 23, disc loss: 0.0067242978803203, policy loss: 6.347725781637248
Experience 16, Iter 24, disc loss: 0.006401936234007226, policy loss: 7.012309121794735
Experience 16, Iter 25, disc loss: 0.00658408680944759, policy loss: 7.09508998795426
Experience 16, Iter 26, disc loss: 0.00701380318975998, policy loss: 6.939331577571395
Experience 16, Iter 27, disc loss: 0.006710779278860892, policy loss: 7.129785816945373
Experience 16, Iter 28, disc loss: 0.0070699700472349235, policy loss: 6.485013620246677
Experience 16, Iter 29, disc loss: 0.007058872429125399, policy loss: 6.903887135800174
Experience 16, Iter 30, disc loss: 0.007053615892457404, policy loss: 6.396028146833262
Experience 16, Iter 31, disc loss: 0.006818653522184742, policy loss: 6.693247051129507
Experience 16, Iter 32, disc loss: 0.006580582046988295, policy loss: 6.826418160649569
Experience 16, Iter 33, disc loss: 0.006481867159849816, policy loss: 6.872967500489557
Experience 16, Iter 34, disc loss: 0.0062073971201203944, policy loss: 6.742724722763771
Experience 16, Iter 35, disc loss: 0.006750170366349361, policy loss: 6.594147195952987
Experience 16, Iter 36, disc loss: 0.007482768481569146, policy loss: 6.600492262469827
Experience 16, Iter 37, disc loss: 0.006563588432533176, policy loss: 6.372272634824341
Experience 16, Iter 38, disc loss: 0.0070290485505094995, policy loss: 6.32351459115463
Experience 16, Iter 39, disc loss: 0.007509173653419261, policy loss: 5.97075012885278
Experience 16, Iter 40, disc loss: 0.007407987362859485, policy loss: 6.331728797817379
Experience 16, Iter 41, disc loss: 0.007485020212281314, policy loss: 6.482972076221197
Experience 16, Iter 42, disc loss: 0.007208926776716729, policy loss: 6.4106029479695295
Experience 16, Iter 43, disc loss: 0.006867770364665009, policy loss: 6.707044089354683
Experience 16, Iter 44, disc loss: 0.006723020440059807, policy loss: 6.2973556469442915
Experience 16, Iter 45, disc loss: 0.008035947379710856, policy loss: 6.639864243710493
Experience 16, Iter 46, disc loss: 0.008017196821055794, policy loss: 6.286121485105673
Experience 16, Iter 47, disc loss: 0.007014318831167858, policy loss: 6.624378066591831
Experience 16, Iter 48, disc loss: 0.00709846212103774, policy loss: 6.455814016286168
Experience 16, Iter 49, disc loss: 0.007719982138312677, policy loss: 6.421828754968029
Experience 16, Iter 50, disc loss: 0.010487389625593556, policy loss: 6.3988998006271895
Experience 16, Iter 51, disc loss: 0.007526922911446299, policy loss: 6.287626068199092
Experience 16, Iter 52, disc loss: 0.006818059736390986, policy loss: 6.780997268795156
Experience 16, Iter 53, disc loss: 0.007729309843396232, policy loss: 6.647537722522716
Experience 16, Iter 54, disc loss: 0.007213003817643398, policy loss: 6.734114567752201
Experience 16, Iter 55, disc loss: 0.007632777001422538, policy loss: 6.408480384389106
Experience 16, Iter 56, disc loss: 0.00825094049395555, policy loss: 6.9739149634291
Experience 16, Iter 57, disc loss: 0.006930063860591269, policy loss: 7.6153985248109874
Experience 16, Iter 58, disc loss: 0.007496638350947477, policy loss: 7.121952592853907
Experience 16, Iter 59, disc loss: 0.007446023992919486, policy loss: 6.804139073588107
Experience 16, Iter 60, disc loss: 0.007793893358610934, policy loss: 6.736020537860272
Experience 16, Iter 61, disc loss: 0.007131080440887925, policy loss: 7.083202956123286
Experience 16, Iter 62, disc loss: 0.006413644832562614, policy loss: 6.808881463395359
Experience 16, Iter 63, disc loss: 0.00763457019263438, policy loss: 6.596542076208828
Experience 16, Iter 64, disc loss: 0.006593257669876663, policy loss: 6.747870103048964
Experience 16, Iter 65, disc loss: 0.006530516927713703, policy loss: 6.587954410028163
Experience 16, Iter 66, disc loss: 0.005985826577176376, policy loss: 7.02183712656835
Experience 16, Iter 67, disc loss: 0.005802236512878495, policy loss: 7.062488963041315
Experience 16, Iter 68, disc loss: 0.006952024245580319, policy loss: 6.9470593881998175
Experience 16, Iter 69, disc loss: 0.00582735279942986, policy loss: 7.564219640302589
Experience 16, Iter 70, disc loss: 0.006745545931352242, policy loss: 7.357694905799345
Experience 16, Iter 71, disc loss: 0.007391781892808038, policy loss: 6.848747330346411
Experience 16, Iter 72, disc loss: 0.006222683915162559, policy loss: 6.499771506328992
Experience 16, Iter 73, disc loss: 0.006671962865202388, policy loss: 6.402353694957764
Experience 16, Iter 74, disc loss: 0.006887068604310447, policy loss: 6.3399393630553735
Experience 16, Iter 75, disc loss: 0.006009106837127968, policy loss: 7.0910529707682
Experience 16, Iter 76, disc loss: 0.00807279606314802, policy loss: 6.295213688407622
Experience 16, Iter 77, disc loss: 0.006404265365772927, policy loss: 6.203240065572625
Experience 16, Iter 78, disc loss: 0.007787221474539559, policy loss: 6.3006532658409595
Experience 16, Iter 79, disc loss: 0.005831191090102489, policy loss: 6.54247058777914
Experience 16, Iter 80, disc loss: 0.008095673556628005, policy loss: 6.283551553785703
Experience 16, Iter 81, disc loss: 0.006851398075033281, policy loss: 6.527105016695057
Experience 16, Iter 82, disc loss: 0.006874269532847753, policy loss: 6.42755325046209
Experience 16, Iter 83, disc loss: 0.006575586902483053, policy loss: 6.516219611174769
Experience 16, Iter 84, disc loss: 0.006902862293032368, policy loss: 7.368413941920107
Experience 16, Iter 85, disc loss: 0.006604764899001133, policy loss: 6.563259616146633
Experience 16, Iter 86, disc loss: 0.006443412005297265, policy loss: 7.823407831401304
Experience 16, Iter 87, disc loss: 0.006238264136486241, policy loss: 8.235446686548087
Experience 16, Iter 88, disc loss: 0.00568028397482242, policy loss: 7.511580560380992
Experience 16, Iter 89, disc loss: 0.007410533444158224, policy loss: 6.707625581072195
Experience 16, Iter 90, disc loss: 0.006115626762201938, policy loss: 6.842875578974419
Experience 16, Iter 91, disc loss: 0.005102470065080283, policy loss: 6.903805637073416
Experience 16, Iter 92, disc loss: 0.004924817007326323, policy loss: 7.102502306988878
Experience 16, Iter 93, disc loss: 0.006816848204604451, policy loss: 6.205839686379424
Experience 16, Iter 94, disc loss: 0.007142730748275151, policy loss: 6.727788807745
Experience 16, Iter 95, disc loss: 0.005820380919396446, policy loss: 7.204159907994462
Experience 16, Iter 96, disc loss: 0.005950193012279545, policy loss: 6.82606767769385
Experience 16, Iter 97, disc loss: 0.00621873215555651, policy loss: 6.790480393004906
Experience 16, Iter 98, disc loss: 0.0071116985998136125, policy loss: 6.301517831006121
Experience 16, Iter 99, disc loss: 0.006077373538008496, policy loss: 6.9334965423747175
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1495],
        [1.2699],
        [0.0256]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0126, 0.1615, 1.2047, 0.0231, 0.0221, 3.8869]],

        [[0.0126, 0.1615, 1.2047, 0.0231, 0.0221, 3.8869]],

        [[0.0126, 0.1615, 1.2047, 0.0231, 0.0221, 3.8869]],

        [[0.0126, 0.1615, 1.2047, 0.0231, 0.0221, 3.8869]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0156, 0.5982, 5.0794, 0.1025], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0156, 0.5982, 5.0794, 0.1025])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.086
Iter 2/2000 - Loss: 3.178
Iter 3/2000 - Loss: 2.930
Iter 4/2000 - Loss: 2.949
Iter 5/2000 - Loss: 2.982
Iter 6/2000 - Loss: 2.878
Iter 7/2000 - Loss: 2.735
Iter 8/2000 - Loss: 2.635
Iter 9/2000 - Loss: 2.575
Iter 10/2000 - Loss: 2.500
Iter 11/2000 - Loss: 2.373
Iter 12/2000 - Loss: 2.201
Iter 13/2000 - Loss: 2.008
Iter 14/2000 - Loss: 1.812
Iter 15/2000 - Loss: 1.610
Iter 16/2000 - Loss: 1.391
Iter 17/2000 - Loss: 1.145
Iter 18/2000 - Loss: 0.869
Iter 19/2000 - Loss: 0.570
Iter 20/2000 - Loss: 0.256
Iter 1981/2000 - Loss: -7.663
Iter 1982/2000 - Loss: -7.663
Iter 1983/2000 - Loss: -7.663
Iter 1984/2000 - Loss: -7.663
Iter 1985/2000 - Loss: -7.663
Iter 1986/2000 - Loss: -7.663
Iter 1987/2000 - Loss: -7.663
Iter 1988/2000 - Loss: -7.663
Iter 1989/2000 - Loss: -7.663
Iter 1990/2000 - Loss: -7.663
Iter 1991/2000 - Loss: -7.663
Iter 1992/2000 - Loss: -7.663
Iter 1993/2000 - Loss: -7.663
Iter 1994/2000 - Loss: -7.664
Iter 1995/2000 - Loss: -7.664
Iter 1996/2000 - Loss: -7.664
Iter 1997/2000 - Loss: -7.664
Iter 1998/2000 - Loss: -7.664
Iter 1999/2000 - Loss: -7.664
Iter 2000/2000 - Loss: -7.664
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[11.6607,  5.8750, 34.0115,  1.7519,  9.7362, 52.8640]],

        [[16.5248, 32.9560,  7.3240,  1.6045,  1.8814, 18.0222]],

        [[17.4311, 30.0906,  8.1744,  1.2007,  1.1199, 22.4066]],

        [[12.7156, 28.8137, 20.1010,  3.4369,  1.5357, 49.0259]]])
Signal Variance: tensor([ 0.0851,  1.4880, 17.4483,  0.6007])
Estimated target variance: tensor([0.0156, 0.5982, 5.0794, 0.1025])
N: 170
Signal to noise ratio: tensor([15.2455, 70.5046, 90.0178, 52.9665])
Bound on condition number: tensor([  39513.4942,  845054.2791, 1377544.5350,  476927.0724])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.00662748475978526, policy loss: 7.47975601914686
Experience 17, Iter 1, disc loss: 0.006363609276013992, policy loss: 6.870249235807567
Experience 17, Iter 2, disc loss: 0.006385610065727425, policy loss: 6.676643455997737
Experience 17, Iter 3, disc loss: 0.005991642128113745, policy loss: 6.933935063291953
Experience 17, Iter 4, disc loss: 0.006041484930269722, policy loss: 7.787201941888098
Experience 17, Iter 5, disc loss: 0.00687987954171326, policy loss: 8.347727062299409
Experience 17, Iter 6, disc loss: 0.006918810147361358, policy loss: 6.677366255392428
Experience 17, Iter 7, disc loss: 0.005763629594211606, policy loss: 6.539816967660264
Experience 17, Iter 8, disc loss: 0.006701909592406359, policy loss: 6.627395992058874
Experience 17, Iter 9, disc loss: 0.007108626658295644, policy loss: 6.27406732247754
Experience 17, Iter 10, disc loss: 0.005489366756197678, policy loss: 6.829237827140132
Experience 17, Iter 11, disc loss: 0.006183560817121138, policy loss: 7.238392128974474
Experience 17, Iter 12, disc loss: 0.005969293014162004, policy loss: 6.56310267442931
Experience 17, Iter 13, disc loss: 0.006671090207195504, policy loss: 7.159780514926055
Experience 17, Iter 14, disc loss: 0.005605166883586253, policy loss: 7.1499496278850785
Experience 17, Iter 15, disc loss: 0.006807102647267672, policy loss: 7.30145113379578
Experience 17, Iter 16, disc loss: 0.0062437836516164285, policy loss: 6.5823014038808605
Experience 17, Iter 17, disc loss: 0.00550164998400028, policy loss: 6.8310409066647075
Experience 17, Iter 18, disc loss: 0.006453081302895187, policy loss: 6.530162440989584
Experience 17, Iter 19, disc loss: 0.006120395078852701, policy loss: 6.5483604262870365
Experience 17, Iter 20, disc loss: 0.008078929076555717, policy loss: 6.238583057583952
Experience 17, Iter 21, disc loss: 0.005974133829029804, policy loss: 6.434020224165477
Experience 17, Iter 22, disc loss: 0.0066368464719099435, policy loss: 7.222587862848262
Experience 17, Iter 23, disc loss: 0.006332415722350951, policy loss: 6.622252417359843
Experience 17, Iter 24, disc loss: 0.006406122851981581, policy loss: 7.750388787104727
Experience 17, Iter 25, disc loss: 0.005938134478399998, policy loss: 6.705080202405293
Experience 17, Iter 26, disc loss: 0.005918090747316541, policy loss: 7.36361491432044
Experience 17, Iter 27, disc loss: 0.006075734431638171, policy loss: 6.5433650706203625
Experience 17, Iter 28, disc loss: 0.0058414893690849935, policy loss: 6.9052139662595895
Experience 17, Iter 29, disc loss: 0.005881269481633235, policy loss: 6.80422972342579
Experience 17, Iter 30, disc loss: 0.0052517831640343585, policy loss: 7.2340825860275455
Experience 17, Iter 31, disc loss: 0.005522863815101554, policy loss: 6.910817092543583
Experience 17, Iter 32, disc loss: 0.007130952651585759, policy loss: 6.233999171036194
Experience 17, Iter 33, disc loss: 0.00507498788604287, policy loss: 7.712271442183662
Experience 17, Iter 34, disc loss: 0.006918421194330466, policy loss: 6.598122748056401
Experience 17, Iter 35, disc loss: 0.0060594344728917456, policy loss: 7.019800279552639
Experience 17, Iter 36, disc loss: 0.005329327995415691, policy loss: 7.0151857664357
Experience 17, Iter 37, disc loss: 0.005824613697067965, policy loss: 6.515306081889862
Experience 17, Iter 38, disc loss: 0.005718653459736561, policy loss: 7.324241575278564
Experience 17, Iter 39, disc loss: 0.0055626632327522236, policy loss: 7.064687995026267
Experience 17, Iter 40, disc loss: 0.005976798579101009, policy loss: 7.037420539727195
Experience 17, Iter 41, disc loss: 0.006212577956376425, policy loss: 6.92429671372591
Experience 17, Iter 42, disc loss: 0.006632136330684973, policy loss: 6.712757900588992
Experience 17, Iter 43, disc loss: 0.005745994165071044, policy loss: 7.382186653463065
Experience 17, Iter 44, disc loss: 0.006975471857271599, policy loss: 7.260393491993182
Experience 17, Iter 45, disc loss: 0.00606878053287854, policy loss: 6.878211978339807
Experience 17, Iter 46, disc loss: 0.005491539794354407, policy loss: 6.988675923157253
Experience 17, Iter 47, disc loss: 0.006095394356921958, policy loss: 7.287230851864597
Experience 17, Iter 48, disc loss: 0.005207379762513817, policy loss: 7.58605178728427
Experience 17, Iter 49, disc loss: 0.00616950365489121, policy loss: 7.336412729521457
Experience 17, Iter 50, disc loss: 0.0053498645339337845, policy loss: 6.813062037099446
Experience 17, Iter 51, disc loss: 0.005860090721216981, policy loss: 7.175994364710362
Experience 17, Iter 52, disc loss: 0.006087771776126841, policy loss: 6.903724178089192
Experience 17, Iter 53, disc loss: 0.006228766875688678, policy loss: 6.880342036480375
Experience 17, Iter 54, disc loss: 0.005753299260558016, policy loss: 7.207473226951612
Experience 17, Iter 55, disc loss: 0.005294838125316304, policy loss: 6.890211721496638
Experience 17, Iter 56, disc loss: 0.0051541950440806925, policy loss: 7.329756644206132
Experience 17, Iter 57, disc loss: 0.004564515227064753, policy loss: 7.305464430501312
Experience 17, Iter 58, disc loss: 0.0053798090596099005, policy loss: 6.453597649331595
Experience 17, Iter 59, disc loss: 0.006419789607961006, policy loss: 7.034967259323183
Experience 17, Iter 60, disc loss: 0.00473950528466922, policy loss: 7.781893478448672
Experience 17, Iter 61, disc loss: 0.005259242357993121, policy loss: 7.538207954500564
Experience 17, Iter 62, disc loss: 0.005182917101078698, policy loss: 7.732107256718082
Experience 17, Iter 63, disc loss: 0.0047526201727733286, policy loss: 7.26484777168894
Experience 17, Iter 64, disc loss: 0.005425614688480885, policy loss: 6.971710821379415
Experience 17, Iter 65, disc loss: 0.00580970695874727, policy loss: 6.893776622187688
Experience 17, Iter 66, disc loss: 0.004798372172599817, policy loss: 7.970645608184235
Experience 17, Iter 67, disc loss: 0.0050224805154986465, policy loss: 7.268628993850319
Experience 17, Iter 68, disc loss: 0.006332007730189759, policy loss: 6.767046960871306
Experience 17, Iter 69, disc loss: 0.0052099169247739495, policy loss: 8.011831089648023
Experience 17, Iter 70, disc loss: 0.0051865574459838875, policy loss: 6.9639200541225055
Experience 17, Iter 71, disc loss: 0.004712703709330475, policy loss: 6.886389320605156
Experience 17, Iter 72, disc loss: 0.005919401558365208, policy loss: 7.087789760660897
Experience 17, Iter 73, disc loss: 0.004190834141127516, policy loss: 7.074859905570952
Experience 17, Iter 74, disc loss: 0.00555674702797688, policy loss: 7.116573917823886
Experience 17, Iter 75, disc loss: 0.004991760223978129, policy loss: 6.856313158089121
Experience 17, Iter 76, disc loss: 0.0047054819494102024, policy loss: 7.999201694466988
Experience 17, Iter 77, disc loss: 0.004240648623273262, policy loss: 7.718578262508961
Experience 17, Iter 78, disc loss: 0.00404620359690275, policy loss: 8.363709967970442
Experience 17, Iter 79, disc loss: 0.004991400135753712, policy loss: 6.769532386496805
Experience 17, Iter 80, disc loss: 0.005726408747075225, policy loss: 6.7194686823775855
Experience 17, Iter 81, disc loss: 0.005224054959562239, policy loss: 7.075755879213805
Experience 17, Iter 82, disc loss: 0.004937954615411291, policy loss: 6.629397961450356
Experience 17, Iter 83, disc loss: 0.004995438422048302, policy loss: 7.675442110819842
Experience 17, Iter 84, disc loss: 0.004690670313296035, policy loss: 7.669758744652741
Experience 17, Iter 85, disc loss: 0.004892522140105519, policy loss: 7.682688287741721
Experience 17, Iter 86, disc loss: 0.004529642116609939, policy loss: 7.0182450942249535
Experience 17, Iter 87, disc loss: 0.005133335595578629, policy loss: 6.848628285815741
Experience 17, Iter 88, disc loss: 0.004661169164187248, policy loss: 7.3584962302557635
Experience 17, Iter 89, disc loss: 0.004315247766229031, policy loss: 7.490047055404897
Experience 17, Iter 90, disc loss: 0.004839905796788765, policy loss: 8.041531425597022
Experience 17, Iter 91, disc loss: 0.005138566732433143, policy loss: 6.679730754393805
Experience 17, Iter 92, disc loss: 0.004596315676037484, policy loss: 7.1784068881471566
Experience 17, Iter 93, disc loss: 0.00527169262326499, policy loss: 6.484213226503836
Experience 17, Iter 94, disc loss: 0.0047316839394719555, policy loss: 6.887662698325604
Experience 17, Iter 95, disc loss: 0.0057809661014633, policy loss: 6.893365505654831
Experience 17, Iter 96, disc loss: 0.00509310953195981, policy loss: 6.763455353746297
Experience 17, Iter 97, disc loss: 0.005037041982811854, policy loss: 7.344178117337274
Experience 17, Iter 98, disc loss: 0.004416211243945664, policy loss: 7.20281472930383
Experience 17, Iter 99, disc loss: 0.004289382168934481, policy loss: 7.942797826736656
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1591],
        [1.3453],
        [0.0271]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1641, 1.2737, 0.0238, 0.0229, 4.0562]],

        [[0.0122, 0.1641, 1.2737, 0.0238, 0.0229, 4.0562]],

        [[0.0122, 0.1641, 1.2737, 0.0238, 0.0229, 4.0562]],

        [[0.0122, 0.1641, 1.2737, 0.0238, 0.0229, 4.0562]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.6365, 5.3814, 0.1083], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.6365, 5.3814, 0.1083])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.175
Iter 2/2000 - Loss: 3.277
Iter 3/2000 - Loss: 3.011
Iter 4/2000 - Loss: 3.027
Iter 5/2000 - Loss: 3.069
Iter 6/2000 - Loss: 2.968
Iter 7/2000 - Loss: 2.820
Iter 8/2000 - Loss: 2.711
Iter 9/2000 - Loss: 2.645
Iter 10/2000 - Loss: 2.572
Iter 11/2000 - Loss: 2.450
Iter 12/2000 - Loss: 2.278
Iter 13/2000 - Loss: 2.078
Iter 14/2000 - Loss: 1.870
Iter 15/2000 - Loss: 1.656
Iter 16/2000 - Loss: 1.428
Iter 17/2000 - Loss: 1.174
Iter 18/2000 - Loss: 0.891
Iter 19/2000 - Loss: 0.584
Iter 20/2000 - Loss: 0.259
Iter 1981/2000 - Loss: -7.753
Iter 1982/2000 - Loss: -7.753
Iter 1983/2000 - Loss: -7.753
Iter 1984/2000 - Loss: -7.753
Iter 1985/2000 - Loss: -7.753
Iter 1986/2000 - Loss: -7.753
Iter 1987/2000 - Loss: -7.753
Iter 1988/2000 - Loss: -7.753
Iter 1989/2000 - Loss: -7.753
Iter 1990/2000 - Loss: -7.753
Iter 1991/2000 - Loss: -7.753
Iter 1992/2000 - Loss: -7.753
Iter 1993/2000 - Loss: -7.753
Iter 1994/2000 - Loss: -7.753
Iter 1995/2000 - Loss: -7.753
Iter 1996/2000 - Loss: -7.753
Iter 1997/2000 - Loss: -7.753
Iter 1998/2000 - Loss: -7.753
Iter 1999/2000 - Loss: -7.753
Iter 2000/2000 - Loss: -7.753
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[12.1936,  6.4888, 36.8907,  1.6552, 12.0506, 57.9552]],

        [[16.5677, 32.7259,  7.5164,  1.5329,  1.8661, 18.6230]],

        [[17.3957, 31.9396,  8.2402,  1.1053,  1.0869, 21.2892]],

        [[13.4993, 27.1442, 19.3817,  3.2065,  1.6686, 47.9818]]])
Signal Variance: tensor([ 0.1007,  1.5156, 15.2366,  0.6149])
Estimated target variance: tensor([0.0157, 0.6365, 5.3814, 0.1083])
N: 180
Signal to noise ratio: tensor([16.8880, 71.3629, 84.3077, 53.7104])
Bound on condition number: tensor([  51337.7729,  916681.3829, 1279403.3323,  519266.2962])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.005098087556701114, policy loss: 6.752401146059034
Experience 18, Iter 1, disc loss: 0.0041491588090946255, policy loss: 7.777553539898249
Experience 18, Iter 2, disc loss: 0.00636571952302027, policy loss: 6.963162158132791
Experience 18, Iter 3, disc loss: 0.007368774938300071, policy loss: 6.644595095537384
Experience 18, Iter 4, disc loss: 0.004954656516682091, policy loss: 7.323778997743292
Experience 18, Iter 5, disc loss: 0.005300119821291154, policy loss: 7.110040901996594
Experience 18, Iter 6, disc loss: 0.004499668502221802, policy loss: 6.919938157521648
Experience 18, Iter 7, disc loss: 0.006591780060293035, policy loss: 6.848790351040034
Experience 18, Iter 8, disc loss: 0.004282216542254925, policy loss: 8.411524201112151
Experience 18, Iter 9, disc loss: 0.005373621637950155, policy loss: 7.647308686408295
Experience 18, Iter 10, disc loss: 0.004469658160235897, policy loss: 7.565193464909433
Experience 18, Iter 11, disc loss: 0.004449626047474068, policy loss: 8.837738387883464
Experience 18, Iter 12, disc loss: 0.004421021777915153, policy loss: 7.439195324402164
Experience 18, Iter 13, disc loss: 0.0048327240141847125, policy loss: 6.953847757327621
Experience 18, Iter 14, disc loss: 0.004061777053613564, policy loss: 7.2455801227598
Experience 18, Iter 15, disc loss: 0.004557459239558529, policy loss: 7.293715360283661
Experience 18, Iter 16, disc loss: 0.005140187955760475, policy loss: 6.970999664007289
Experience 18, Iter 17, disc loss: 0.005469784144208055, policy loss: 7.187719523684035
Experience 18, Iter 18, disc loss: 0.0042578726494056675, policy loss: 7.53433028380975
Experience 18, Iter 19, disc loss: 0.004309334618961727, policy loss: 7.826887970541669
Experience 18, Iter 20, disc loss: 0.004725316002426265, policy loss: 7.468543020592524
Experience 18, Iter 21, disc loss: 0.004924492386447829, policy loss: 6.822720305149366
Experience 18, Iter 22, disc loss: 0.003729099251578044, policy loss: 7.372713034577277
Experience 18, Iter 23, disc loss: 0.004450105581874555, policy loss: 7.223030307964047
Experience 18, Iter 24, disc loss: 0.005189971202982604, policy loss: 7.642625464379098
Experience 18, Iter 25, disc loss: 0.003941969164128743, policy loss: 8.70944753730296
Experience 18, Iter 26, disc loss: 0.004279088859578367, policy loss: 6.914929746943544
Experience 18, Iter 27, disc loss: 0.005751006419302729, policy loss: 6.684271411704511
Experience 18, Iter 28, disc loss: 0.004386682955035363, policy loss: 7.264915815297691
Experience 18, Iter 29, disc loss: 0.00401737207009934, policy loss: 7.4372769297003885
Experience 18, Iter 30, disc loss: 0.004156205086735283, policy loss: 9.236866638518636
Experience 18, Iter 31, disc loss: 0.004072513924185598, policy loss: 7.032460477383413
Experience 18, Iter 32, disc loss: 0.004893984084373244, policy loss: 7.172492814111406
Experience 18, Iter 33, disc loss: 0.006826977960152337, policy loss: 6.614189353417549
Experience 18, Iter 34, disc loss: 0.00572701923153695, policy loss: 6.506771453634795
Experience 18, Iter 35, disc loss: 0.005088310107643495, policy loss: 7.83809134239788
Experience 18, Iter 36, disc loss: 0.0039560667082289705, policy loss: 8.256613906453262
Experience 18, Iter 37, disc loss: 0.004838953239131822, policy loss: 7.303434882611397
Experience 18, Iter 38, disc loss: 0.004980644909468616, policy loss: 7.2077891210895615
Experience 18, Iter 39, disc loss: 0.004420708057730718, policy loss: 7.39684209553091
Experience 18, Iter 40, disc loss: 0.005134657649510578, policy loss: 6.886523445543802
Experience 18, Iter 41, disc loss: 0.003942028852344157, policy loss: 8.094165083220005
Experience 18, Iter 42, disc loss: 0.005435876995999978, policy loss: 6.921321686048907
Experience 18, Iter 43, disc loss: 0.004804425994074088, policy loss: 7.561071259613109
Experience 18, Iter 44, disc loss: 0.004310443284002229, policy loss: 7.386516660345003
Experience 18, Iter 45, disc loss: 0.005172298097871681, policy loss: 6.9479938725460215
Experience 18, Iter 46, disc loss: 0.004585590563404819, policy loss: 7.412062302957157
Experience 18, Iter 47, disc loss: 0.004495206679321084, policy loss: 7.258364193431765
Experience 18, Iter 48, disc loss: 0.0045955212197140004, policy loss: 7.389860188564958
Experience 18, Iter 49, disc loss: 0.0047605769348403664, policy loss: 7.146085368737666
Experience 18, Iter 50, disc loss: 0.005160080299029449, policy loss: 7.220955453269528
Experience 18, Iter 51, disc loss: 0.004777958361344746, policy loss: 7.118795511231335
Experience 18, Iter 52, disc loss: 0.004528981144766225, policy loss: 7.232840773012995
Experience 18, Iter 53, disc loss: 0.0042995798701579175, policy loss: 7.805636926656712
Experience 18, Iter 54, disc loss: 0.004290103521156205, policy loss: 7.573044167025806
Experience 18, Iter 55, disc loss: 0.004062953228605391, policy loss: 7.859448513836232
Experience 18, Iter 56, disc loss: 0.00452890734614532, policy loss: 7.46491323171843
Experience 18, Iter 57, disc loss: 0.004484983054077572, policy loss: 7.305519992251677
Experience 18, Iter 58, disc loss: 0.004203141064097259, policy loss: 6.99294451346937
Experience 18, Iter 59, disc loss: 0.004375902342703834, policy loss: 7.717873042346242
Experience 18, Iter 60, disc loss: 0.005181787122193384, policy loss: 7.038570486769442
Experience 18, Iter 61, disc loss: 0.004192773092919542, policy loss: 7.004776626424016
Experience 18, Iter 62, disc loss: 0.004138305260579243, policy loss: 7.150953262105939
Experience 18, Iter 63, disc loss: 0.0036297361441296515, policy loss: 7.366961673242736
Experience 18, Iter 64, disc loss: 0.005223167037478228, policy loss: 7.2888923100098255
Experience 18, Iter 65, disc loss: 0.004554360658872876, policy loss: 7.321674344989283
Experience 18, Iter 66, disc loss: 0.004158838420401732, policy loss: 7.135372458260882
Experience 18, Iter 67, disc loss: 0.00454802925826342, policy loss: 7.034401906974581
Experience 18, Iter 68, disc loss: 0.004423761457307886, policy loss: 7.8316211688884145
Experience 18, Iter 69, disc loss: 0.004680880426718345, policy loss: 7.057443405894798
Experience 18, Iter 70, disc loss: 0.004963840451803989, policy loss: 6.784178478409879
Experience 18, Iter 71, disc loss: 0.004028883179645196, policy loss: 6.994700193783361
Experience 18, Iter 72, disc loss: 0.00408456879051455, policy loss: 7.504787580190454
Experience 18, Iter 73, disc loss: 0.004417407454397647, policy loss: 7.19761654387682
Experience 18, Iter 74, disc loss: 0.004151006721640309, policy loss: 7.395906224565567
Experience 18, Iter 75, disc loss: 0.003929768056604307, policy loss: 8.22692286889982
Experience 18, Iter 76, disc loss: 0.004070356692266387, policy loss: 7.364572635048401
Experience 18, Iter 77, disc loss: 0.0040586168864365346, policy loss: 7.568195062791778
Experience 18, Iter 78, disc loss: 0.004053364633236431, policy loss: 7.1479689078235005
Experience 18, Iter 79, disc loss: 0.004822286460063151, policy loss: 7.0293990099073955
Experience 18, Iter 80, disc loss: 0.004934276333772271, policy loss: 7.311615005532615
Experience 18, Iter 81, disc loss: 0.00417416116763604, policy loss: 7.361458123105658
Experience 18, Iter 82, disc loss: 0.004418241521699914, policy loss: 7.193350416886993
Experience 18, Iter 83, disc loss: 0.0037125721132978172, policy loss: 8.550578463327273
Experience 18, Iter 84, disc loss: 0.003984513283831811, policy loss: 7.682122506138959
Experience 18, Iter 85, disc loss: 0.004813109814345115, policy loss: 7.71118607472121
Experience 18, Iter 86, disc loss: 0.004933538911874272, policy loss: 7.094439863517254
Experience 18, Iter 87, disc loss: 0.00427653663619984, policy loss: 7.760207792133578
Experience 18, Iter 88, disc loss: 0.004212117451077085, policy loss: 7.359965024071709
Experience 18, Iter 89, disc loss: 0.004122579327140684, policy loss: 7.070900096805509
Experience 18, Iter 90, disc loss: 0.004196690294299796, policy loss: 8.022680570748205
Experience 18, Iter 91, disc loss: 0.0037594722034250136, policy loss: 7.5718997577580005
Experience 18, Iter 92, disc loss: 0.004267393995483457, policy loss: 7.331040764160543
Experience 18, Iter 93, disc loss: 0.0041650951152467815, policy loss: 7.019369170374087
Experience 18, Iter 94, disc loss: 0.003934718253382432, policy loss: 7.107203013086663
Experience 18, Iter 95, disc loss: 0.004293268069665823, policy loss: 8.291650772075684
Experience 18, Iter 96, disc loss: 0.004667447954292726, policy loss: 8.321309602038244
Experience 18, Iter 97, disc loss: 0.003270707851412574, policy loss: 7.79656834041516
Experience 18, Iter 98, disc loss: 0.004213225704240051, policy loss: 7.306527134742682
Experience 18, Iter 99, disc loss: 0.003777042411866144, policy loss: 8.706739006725257
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1659],
        [1.3875],
        [0.0276]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0117, 0.1638, 1.3012, 0.0244, 0.0241, 4.2495]],

        [[0.0117, 0.1638, 1.3012, 0.0244, 0.0241, 4.2495]],

        [[0.0117, 0.1638, 1.3012, 0.0244, 0.0241, 4.2495]],

        [[0.0117, 0.1638, 1.3012, 0.0244, 0.0241, 4.2495]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.6636, 5.5499, 0.1105], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.6636, 5.5499, 0.1105])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.220
Iter 2/2000 - Loss: 3.320
Iter 3/2000 - Loss: 3.044
Iter 4/2000 - Loss: 3.054
Iter 5/2000 - Loss: 3.095
Iter 6/2000 - Loss: 2.990
Iter 7/2000 - Loss: 2.832
Iter 8/2000 - Loss: 2.708
Iter 9/2000 - Loss: 2.628
Iter 10/2000 - Loss: 2.543
Iter 11/2000 - Loss: 2.411
Iter 12/2000 - Loss: 2.227
Iter 13/2000 - Loss: 2.016
Iter 14/2000 - Loss: 1.796
Iter 15/2000 - Loss: 1.571
Iter 16/2000 - Loss: 1.333
Iter 17/2000 - Loss: 1.072
Iter 18/2000 - Loss: 0.784
Iter 19/2000 - Loss: 0.474
Iter 20/2000 - Loss: 0.148
Iter 1981/2000 - Loss: -7.808
Iter 1982/2000 - Loss: -7.808
Iter 1983/2000 - Loss: -7.808
Iter 1984/2000 - Loss: -7.808
Iter 1985/2000 - Loss: -7.808
Iter 1986/2000 - Loss: -7.808
Iter 1987/2000 - Loss: -7.808
Iter 1988/2000 - Loss: -7.808
Iter 1989/2000 - Loss: -7.808
Iter 1990/2000 - Loss: -7.808
Iter 1991/2000 - Loss: -7.808
Iter 1992/2000 - Loss: -7.808
Iter 1993/2000 - Loss: -7.808
Iter 1994/2000 - Loss: -7.808
Iter 1995/2000 - Loss: -7.808
Iter 1996/2000 - Loss: -7.808
Iter 1997/2000 - Loss: -7.808
Iter 1998/2000 - Loss: -7.808
Iter 1999/2000 - Loss: -7.808
Iter 2000/2000 - Loss: -7.808
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[12.0063,  6.7552, 37.3229,  1.6740, 12.0367, 58.4696]],

        [[15.7959, 32.0250,  7.8172,  1.4959,  1.7962, 19.5881]],

        [[17.4217, 31.7296,  8.1600,  1.1101,  1.1020, 21.3037]],

        [[13.0855, 25.4395, 19.3140,  3.2641,  1.6503, 48.5596]]])
Signal Variance: tensor([ 0.1031,  1.6189, 15.6894,  0.6132])
Estimated target variance: tensor([0.0155, 0.6636, 5.5499, 0.1105])
N: 190
Signal to noise ratio: tensor([17.4553, 71.2256, 85.3166, 52.9329])
Bound on condition number: tensor([  57891.4381,  963887.6381, 1382997.2077,  532360.0239])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.004469653434022895, policy loss: 8.242906825598684
Experience 19, Iter 1, disc loss: 0.004065953710759827, policy loss: 6.904070763614223
Experience 19, Iter 2, disc loss: 0.0030854819326502194, policy loss: 7.872877754463103
Experience 19, Iter 3, disc loss: 0.003596281051767709, policy loss: 7.920440671927179
Experience 19, Iter 4, disc loss: 0.003739633557581114, policy loss: 7.414819764991477
Experience 19, Iter 5, disc loss: 0.003298668403848851, policy loss: 8.238961079276931
Experience 19, Iter 6, disc loss: 0.0035053757641415224, policy loss: 7.332229544961836
Experience 19, Iter 7, disc loss: 0.00351497982742627, policy loss: 7.310204683693953
Experience 19, Iter 8, disc loss: 0.0032969082630546357, policy loss: 7.126200355173341
Experience 19, Iter 9, disc loss: 0.003182263933458528, policy loss: 8.301353291650543
Experience 19, Iter 10, disc loss: 0.003411791029716007, policy loss: 7.390728312497677
Experience 19, Iter 11, disc loss: 0.0036859633903042377, policy loss: 7.365561748849731
Experience 19, Iter 12, disc loss: 0.00400330072963094, policy loss: 7.488781341861031
Experience 19, Iter 13, disc loss: 0.0038671330171573272, policy loss: 6.9393325902923415
Experience 19, Iter 14, disc loss: 0.003723325869540946, policy loss: 7.464740966511796
Experience 19, Iter 15, disc loss: 0.004013526012604982, policy loss: 7.079633426795473
Experience 19, Iter 16, disc loss: 0.0034907720766564373, policy loss: 7.098767919892849
Experience 19, Iter 17, disc loss: 0.0034721872251621023, policy loss: 7.398676206988746
Experience 19, Iter 18, disc loss: 0.004145155189476105, policy loss: 7.023177492422473
Experience 19, Iter 19, disc loss: 0.0035385563044000013, policy loss: 8.27349635755677
Experience 19, Iter 20, disc loss: 0.004047352618905423, policy loss: 6.985249801686468
Experience 19, Iter 21, disc loss: 0.0035513868916841407, policy loss: 7.184710552093015
Experience 19, Iter 22, disc loss: 0.0031678105130089804, policy loss: 7.309636639691632
Experience 19, Iter 23, disc loss: 0.003935603642943305, policy loss: 7.515020684300536
Experience 19, Iter 24, disc loss: 0.0031873920728517444, policy loss: 7.803602558095134
Experience 19, Iter 25, disc loss: 0.0031313676067597955, policy loss: 7.2997495832374515
Experience 19, Iter 26, disc loss: 0.003836071512814243, policy loss: 7.175261550573229
Experience 19, Iter 27, disc loss: 0.0036740309945218156, policy loss: 7.2630708953333425
Experience 19, Iter 28, disc loss: 0.0033941279939922056, policy loss: 7.283686720693037
Experience 19, Iter 29, disc loss: 0.0034931791249401024, policy loss: 7.590874308033073
Experience 19, Iter 30, disc loss: 0.0037086791361775296, policy loss: 7.264739370220546
Experience 19, Iter 31, disc loss: 0.0038447226870172956, policy loss: 7.752987023327921
Experience 19, Iter 32, disc loss: 0.0039396307919996485, policy loss: 7.232645424281483
Experience 19, Iter 33, disc loss: 0.0035188097352776247, policy loss: 7.767576724642411
Experience 19, Iter 34, disc loss: 0.00465522758376999, policy loss: 7.186886311760605
Experience 19, Iter 35, disc loss: 0.004457419370723637, policy loss: 7.695249860323868
Experience 19, Iter 36, disc loss: 0.0036697806380346283, policy loss: 7.645400661397126
Experience 19, Iter 37, disc loss: 0.0035311983769164786, policy loss: 7.288025229615004
Experience 19, Iter 38, disc loss: 0.0035446551421681053, policy loss: 7.967082367288113
Experience 19, Iter 39, disc loss: 0.003740969036480377, policy loss: 7.7310999667025
Experience 19, Iter 40, disc loss: 0.003439081541918414, policy loss: 8.236630050191753
Experience 19, Iter 41, disc loss: 0.003659175655908848, policy loss: 7.549785359112091
Experience 19, Iter 42, disc loss: 0.003889655447675471, policy loss: 8.388967806203244
Experience 19, Iter 43, disc loss: 0.003030457922858167, policy loss: 8.258398615046318
Experience 19, Iter 44, disc loss: 0.0037034666490126378, policy loss: 7.715748035504294
Experience 19, Iter 45, disc loss: 0.0029493594083990957, policy loss: 7.959518818671009
Experience 19, Iter 46, disc loss: 0.00386769012639713, policy loss: 7.315396983432554
Experience 19, Iter 47, disc loss: 0.003905049382388461, policy loss: 7.181800086672187
Experience 19, Iter 48, disc loss: 0.004215529058685726, policy loss: 7.987344770543679
Experience 19, Iter 49, disc loss: 0.004023627419865232, policy loss: 7.406730442038304
Experience 19, Iter 50, disc loss: 0.0034446757651803066, policy loss: 7.446889526800884
Experience 19, Iter 51, disc loss: 0.004054850911214645, policy loss: 7.077354362584817
Experience 19, Iter 52, disc loss: 0.0038870405056588616, policy loss: 8.30322947562371
Experience 19, Iter 53, disc loss: 0.003300597471030925, policy loss: 7.453739067920031
Experience 19, Iter 54, disc loss: 0.002956758226665082, policy loss: 7.6992280638872845
Experience 19, Iter 55, disc loss: 0.0029066786936187373, policy loss: 8.876838066609633
Experience 19, Iter 56, disc loss: 0.0032775941235965167, policy loss: 8.507343564020037
Experience 19, Iter 57, disc loss: 0.0027772751324341175, policy loss: 8.549778779191346
Experience 19, Iter 58, disc loss: 0.0028242125780510788, policy loss: 8.288723130138338
Experience 19, Iter 59, disc loss: 0.004093353829157771, policy loss: 7.801800500464441
Experience 19, Iter 60, disc loss: 0.0031302555594654533, policy loss: 8.561928777539979
Experience 19, Iter 61, disc loss: 0.002656475157896799, policy loss: 8.081761263457754
Experience 19, Iter 62, disc loss: 0.0030871759210085925, policy loss: 7.869391316395719
Experience 19, Iter 63, disc loss: 0.0030692241489412124, policy loss: 7.690545859753914
Experience 19, Iter 64, disc loss: 0.002655085404092655, policy loss: 7.925472562214764
Experience 19, Iter 65, disc loss: 0.0032978607980782013, policy loss: 8.039247746810776
Experience 19, Iter 66, disc loss: 0.0026929208040742643, policy loss: 7.678735144887176
Experience 19, Iter 67, disc loss: 0.002833041699996061, policy loss: 7.311289528930583
Experience 19, Iter 68, disc loss: 0.0028824182371908775, policy loss: 7.402185933212717
Experience 19, Iter 69, disc loss: 0.0026728830884776584, policy loss: 7.83966615030592
Experience 19, Iter 70, disc loss: 0.0028157181636040644, policy loss: 7.884036733420434
Experience 19, Iter 71, disc loss: 0.00282783591321796, policy loss: 7.991282433992987
Experience 19, Iter 72, disc loss: 0.0029232935381723222, policy loss: 7.648567970725798
Experience 19, Iter 73, disc loss: 0.003270492031071284, policy loss: 7.490446704054422
Experience 19, Iter 74, disc loss: 0.0024661275327404553, policy loss: 7.309852140199753
Experience 19, Iter 75, disc loss: 0.0034238282406547525, policy loss: 7.31724454691547
Experience 19, Iter 76, disc loss: 0.0028890509581029934, policy loss: 7.229941330219833
Experience 19, Iter 77, disc loss: 0.003994863469417468, policy loss: 7.409012442925852
Experience 19, Iter 78, disc loss: 0.002472203116972149, policy loss: 7.911543140899904
Experience 19, Iter 79, disc loss: 0.0024899879004389707, policy loss: 7.606941669358369
Experience 19, Iter 80, disc loss: 0.0027667527051955694, policy loss: 7.887311562367012
Experience 19, Iter 81, disc loss: 0.0024458501083710824, policy loss: 8.293989613599056
Experience 19, Iter 82, disc loss: 0.003814952869843853, policy loss: 7.18932520057984
Experience 19, Iter 83, disc loss: 0.002450767068423449, policy loss: 7.804998081406332
Experience 19, Iter 84, disc loss: 0.0027364914043999143, policy loss: 8.390320598736237
Experience 19, Iter 85, disc loss: 0.002864957371654133, policy loss: 7.332851057003781
Experience 19, Iter 86, disc loss: 0.0033780455554406953, policy loss: 7.948812627894727
Experience 19, Iter 87, disc loss: 0.0022212694429464214, policy loss: 8.84718364303928
Experience 19, Iter 88, disc loss: 0.002357710328154297, policy loss: 9.318199182061122
Experience 19, Iter 89, disc loss: 0.002226517695664689, policy loss: 8.429502491357223
Experience 19, Iter 90, disc loss: 0.0023389082172609573, policy loss: 8.221116873724778
Experience 19, Iter 91, disc loss: 0.0023725546220575407, policy loss: 7.82489996949286
Experience 19, Iter 92, disc loss: 0.0027955251671383506, policy loss: 8.139331154618468
Experience 19, Iter 93, disc loss: 0.002819178135705805, policy loss: 7.624071138891759
Experience 19, Iter 94, disc loss: 0.0018415706094346257, policy loss: 9.27260116619906
Experience 19, Iter 95, disc loss: 0.002202568257802027, policy loss: 8.005107169708648
Experience 19, Iter 96, disc loss: 0.0016385617071887893, policy loss: 9.974740751452504
Experience 19, Iter 97, disc loss: 0.0013297728105659131, policy loss: 11.149174615492942
Experience 19, Iter 98, disc loss: 0.0012685925699091169, policy loss: 12.097969630734728
Experience 19, Iter 99, disc loss: 0.0012225902547132962, policy loss: 12.212297861603172
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1731],
        [1.4407],
        [0.0286]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0113, 0.1646, 1.3511, 0.0253, 0.0251, 4.4066]],

        [[0.0113, 0.1646, 1.3511, 0.0253, 0.0251, 4.4066]],

        [[0.0113, 0.1646, 1.3511, 0.0253, 0.0251, 4.4066]],

        [[0.0113, 0.1646, 1.3511, 0.0253, 0.0251, 4.4066]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0156, 0.6925, 5.7630, 0.1145], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0156, 0.6925, 5.7630, 0.1145])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.284
Iter 2/2000 - Loss: 3.380
Iter 3/2000 - Loss: 3.101
Iter 4/2000 - Loss: 3.106
Iter 5/2000 - Loss: 3.146
Iter 6/2000 - Loss: 3.043
Iter 7/2000 - Loss: 2.883
Iter 8/2000 - Loss: 2.754
Iter 9/2000 - Loss: 2.668
Iter 10/2000 - Loss: 2.580
Iter 11/2000 - Loss: 2.446
Iter 12/2000 - Loss: 2.262
Iter 13/2000 - Loss: 2.049
Iter 14/2000 - Loss: 1.825
Iter 15/2000 - Loss: 1.595
Iter 16/2000 - Loss: 1.351
Iter 17/2000 - Loss: 1.084
Iter 18/2000 - Loss: 0.791
Iter 19/2000 - Loss: 0.478
Iter 20/2000 - Loss: 0.150
Iter 1981/2000 - Loss: -7.794
Iter 1982/2000 - Loss: -7.794
Iter 1983/2000 - Loss: -7.794
Iter 1984/2000 - Loss: -7.794
Iter 1985/2000 - Loss: -7.794
Iter 1986/2000 - Loss: -7.794
Iter 1987/2000 - Loss: -7.794
Iter 1988/2000 - Loss: -7.794
Iter 1989/2000 - Loss: -7.794
Iter 1990/2000 - Loss: -7.794
Iter 1991/2000 - Loss: -7.794
Iter 1992/2000 - Loss: -7.794
Iter 1993/2000 - Loss: -7.794
Iter 1994/2000 - Loss: -7.794
Iter 1995/2000 - Loss: -7.794
Iter 1996/2000 - Loss: -7.794
Iter 1997/2000 - Loss: -7.794
Iter 1998/2000 - Loss: -7.794
Iter 1999/2000 - Loss: -7.794
Iter 2000/2000 - Loss: -7.794
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.7099,  6.6909, 37.0987,  1.6238, 12.8035, 58.4444]],

        [[14.8839, 32.1728,  7.5339,  1.3497,  2.0025, 18.0564]],

        [[16.8223, 31.6684,  8.3133,  1.0407,  1.0541, 21.9392]],

        [[11.6789, 23.7826, 18.1882,  3.0276,  1.7133, 44.8215]]])
Signal Variance: tensor([ 0.1041,  1.3861, 15.3467,  0.5545])
Estimated target variance: tensor([0.0156, 0.6925, 5.7630, 0.1145])
N: 200
Signal to noise ratio: tensor([17.3958, 64.8100, 85.1254, 47.9814])
Bound on condition number: tensor([  60523.8222,  840067.5237, 1449268.2017,  460443.5124])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0011897257172681626, policy loss: 12.114195325229048
Experience 20, Iter 1, disc loss: 0.0012051531084502922, policy loss: 11.144978090306843
Experience 20, Iter 2, disc loss: 0.0011940023627575941, policy loss: 9.985939462709005
Experience 20, Iter 3, disc loss: 0.0012536734880755846, policy loss: 9.434509384574344
Experience 20, Iter 4, disc loss: 0.001350378979071858, policy loss: 8.776389818175765
Experience 20, Iter 5, disc loss: 0.0012620625871065906, policy loss: 9.367830466704246
Experience 20, Iter 6, disc loss: 0.0010227213852450666, policy loss: 9.608287106262887
Experience 20, Iter 7, disc loss: 0.0012484366720434817, policy loss: 8.42407088696281
Experience 20, Iter 8, disc loss: 0.0028560365128706346, policy loss: 7.0415667226742205
Experience 20, Iter 9, disc loss: 0.0022585957466452434, policy loss: 7.8513246687297205
Experience 20, Iter 10, disc loss: 0.0019107715173822257, policy loss: 8.463954019775251
Experience 20, Iter 11, disc loss: 0.0023305030963842593, policy loss: 7.596602732732562
Experience 20, Iter 12, disc loss: 0.001982650848436137, policy loss: 7.713938232574786
Experience 20, Iter 13, disc loss: 0.002336061738871437, policy loss: 8.790624806712177
Experience 20, Iter 14, disc loss: 0.0024727621279015772, policy loss: 7.686003960335743
Experience 20, Iter 15, disc loss: 0.001860879677035856, policy loss: 8.104747975979087
Experience 20, Iter 16, disc loss: 0.0018167093306837062, policy loss: 8.394384314446311
Experience 20, Iter 17, disc loss: 0.0018201298592155312, policy loss: 8.029364127349197
Experience 20, Iter 18, disc loss: 0.0014419634290119966, policy loss: 8.232315688255827
Experience 20, Iter 19, disc loss: 0.0018734004237132744, policy loss: 8.324702260288442
Experience 20, Iter 20, disc loss: 0.0014323823579457198, policy loss: 8.379204208031233
Experience 20, Iter 21, disc loss: 0.001680429888410296, policy loss: 8.597675335205452
Experience 20, Iter 22, disc loss: 0.001861483865380471, policy loss: 7.7131723617780406
Experience 20, Iter 23, disc loss: 0.0021684354117907974, policy loss: 8.092225999864416
Experience 20, Iter 24, disc loss: 0.0016843510896595033, policy loss: 7.93389717462265
Experience 20, Iter 25, disc loss: 0.001747123559046203, policy loss: 8.645407172331517
Experience 20, Iter 26, disc loss: 0.0017231311098663815, policy loss: 8.234649133902645
Experience 20, Iter 27, disc loss: 0.0018620661501857188, policy loss: 7.659334921917289
Experience 20, Iter 28, disc loss: 0.0019075949664196573, policy loss: 8.472879154721307
Experience 20, Iter 29, disc loss: 0.0019525254398213621, policy loss: 8.274287290708592
Experience 20, Iter 30, disc loss: 0.002001306355312443, policy loss: 7.761266498294622
Experience 20, Iter 31, disc loss: 0.0023130682642194564, policy loss: 7.43538070133058
Experience 20, Iter 32, disc loss: 0.002114047310760721, policy loss: 7.697291257579075
Experience 20, Iter 33, disc loss: 0.002181546614984174, policy loss: 7.676639568879448
Experience 20, Iter 34, disc loss: 0.0019892614305652982, policy loss: 7.72034625286733
Experience 20, Iter 35, disc loss: 0.00194213777523538, policy loss: 8.013715541416062
Experience 20, Iter 36, disc loss: 0.0027856823334340853, policy loss: 7.358947986868726
Experience 20, Iter 37, disc loss: 0.002081373851573185, policy loss: 7.874473754997783
Experience 20, Iter 38, disc loss: 0.00208790626979567, policy loss: 7.991867926301681
Experience 20, Iter 39, disc loss: 0.0022488798013313906, policy loss: 7.6289122266211775
Experience 20, Iter 40, disc loss: 0.002206148806212895, policy loss: 7.561918839635544
Experience 20, Iter 41, disc loss: 0.002412586627005268, policy loss: 7.917464257453425
Experience 20, Iter 42, disc loss: 0.002397686509312385, policy loss: 7.6897118006806915
Experience 20, Iter 43, disc loss: 0.002125348401309669, policy loss: 8.129646249273332
Experience 20, Iter 44, disc loss: 0.0029107956882037755, policy loss: 7.71770533981069
Experience 20, Iter 45, disc loss: 0.0028960151712114244, policy loss: 7.202438118729058
Experience 20, Iter 46, disc loss: 0.002655345759243922, policy loss: 7.084191235704858
Experience 20, Iter 47, disc loss: 0.002515338826518588, policy loss: 7.80706191050619
Experience 20, Iter 48, disc loss: 0.0021121040509871954, policy loss: 7.643924410634293
Experience 20, Iter 49, disc loss: 0.0025722659309673998, policy loss: 7.882737283372294
Experience 20, Iter 50, disc loss: 0.0022258343837532456, policy loss: 7.690380082545854
Experience 20, Iter 51, disc loss: 0.0027942070086915648, policy loss: 7.519956199666626
Experience 20, Iter 52, disc loss: 0.0030646659416413763, policy loss: 7.073503165932712
Experience 20, Iter 53, disc loss: 0.0037321974025264454, policy loss: 7.239747257694594
Experience 20, Iter 54, disc loss: 0.002533648319947083, policy loss: 8.388420187393963
Experience 20, Iter 55, disc loss: 0.0027271651127236925, policy loss: 7.496656071769118
Experience 20, Iter 56, disc loss: 0.002548894704864372, policy loss: 7.57568502723108
Experience 20, Iter 57, disc loss: 0.0029393451524210843, policy loss: 7.430392011777412
Experience 20, Iter 58, disc loss: 0.002719039142825345, policy loss: 7.927006295807201
Experience 20, Iter 59, disc loss: 0.003340428624578111, policy loss: 8.02263843353457
Experience 20, Iter 60, disc loss: 0.003048690899181315, policy loss: 8.205321567004251
Experience 20, Iter 61, disc loss: 0.0028038155075728642, policy loss: 8.557226442420253
Experience 20, Iter 62, disc loss: 0.0027129204857007103, policy loss: 8.491132595511626
Experience 20, Iter 63, disc loss: 0.0027979966673199255, policy loss: 8.088504551580186
Experience 20, Iter 64, disc loss: 0.002909791183285616, policy loss: 8.35132276206532
Experience 20, Iter 65, disc loss: 0.0024912082936292814, policy loss: 8.606177437864186
Experience 20, Iter 66, disc loss: 0.002613996880104644, policy loss: 7.732197974919169
Experience 20, Iter 67, disc loss: 0.0025312469770485124, policy loss: 8.40172219850982
Experience 20, Iter 68, disc loss: 0.0025294985307871467, policy loss: 7.44330032254294
Experience 20, Iter 69, disc loss: 0.0027655595984883595, policy loss: 8.067032304075868
Experience 20, Iter 70, disc loss: 0.002506007089996698, policy loss: 7.948126720078665
Experience 20, Iter 71, disc loss: 0.0025784058543797347, policy loss: 7.737242074670839
Experience 20, Iter 72, disc loss: 0.00302907974686032, policy loss: 7.87621996105316
Experience 20, Iter 73, disc loss: 0.0025039960533085534, policy loss: 7.816839479977444
Experience 20, Iter 74, disc loss: 0.0026376939821972684, policy loss: 7.640968838640976
Experience 20, Iter 75, disc loss: 0.0024444880397513098, policy loss: 7.759701986876563
Experience 20, Iter 76, disc loss: 0.0027925182989109922, policy loss: 7.249234231992702
Experience 20, Iter 77, disc loss: 0.0026673511349647955, policy loss: 7.634305724968929
Experience 20, Iter 78, disc loss: 0.0026744516209765176, policy loss: 7.612687006390142
Experience 20, Iter 79, disc loss: 0.002534030891971203, policy loss: 7.825664956313651
Experience 20, Iter 80, disc loss: 0.0032015679102192665, policy loss: 7.549064564873397
Experience 20, Iter 81, disc loss: 0.0025246845622950748, policy loss: 7.575037268974304
Experience 20, Iter 82, disc loss: 0.0025052448906366133, policy loss: 8.032057208333022
Experience 20, Iter 83, disc loss: 0.0027465331449164822, policy loss: 7.648847552788523
Experience 20, Iter 84, disc loss: 0.0023595089170516358, policy loss: 7.672650132410491
Experience 20, Iter 85, disc loss: 0.003179180748536823, policy loss: 7.640349140015369
Experience 20, Iter 86, disc loss: 0.003462716550134012, policy loss: 7.360952604488762
Experience 20, Iter 87, disc loss: 0.002318808116891692, policy loss: 8.521090108040731
Experience 20, Iter 88, disc loss: 0.003010419553302737, policy loss: 9.160031148216435
Experience 20, Iter 89, disc loss: 0.002418741733243853, policy loss: 7.794139396598291
Experience 20, Iter 90, disc loss: 0.0024281755578204425, policy loss: 7.712182103374594
Experience 20, Iter 91, disc loss: 0.002669658441406091, policy loss: 8.311586648991485
Experience 20, Iter 92, disc loss: 0.0025898147359710074, policy loss: 7.93913728487094
Experience 20, Iter 93, disc loss: 0.0027855116211779043, policy loss: 7.4680208555319
Experience 20, Iter 94, disc loss: 0.0028467508231736446, policy loss: 8.962329460147785
Experience 20, Iter 95, disc loss: 0.003126368729434447, policy loss: 7.088649966472441
Experience 20, Iter 96, disc loss: 0.0031499012331484815, policy loss: 7.5966987038022475
Experience 20, Iter 97, disc loss: 0.003134090806604867, policy loss: 7.4909535678521095
Experience 20, Iter 98, disc loss: 0.003464500677746681, policy loss: 7.805400282794889
Experience 20, Iter 99, disc loss: 0.002537666290935698, policy loss: 8.106572935950581
