Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0036],
        [0.2650],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0030, 0.0143, 1.0599, 0.0199], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0030, 0.0143, 1.0599, 0.0199])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.796
Iter 2/2000 - Loss: -0.784
Iter 3/2000 - Loss: -0.901
Iter 4/2000 - Loss: -0.460
Iter 5/2000 - Loss: -0.434
Iter 6/2000 - Loss: -0.694
Iter 7/2000 - Loss: -0.899
Iter 8/2000 - Loss: -0.912
Iter 9/2000 - Loss: -0.831
Iter 10/2000 - Loss: -0.818
Iter 11/2000 - Loss: -0.907
Iter 12/2000 - Loss: -1.030
Iter 13/2000 - Loss: -1.115
Iter 14/2000 - Loss: -1.142
Iter 15/2000 - Loss: -1.123
Iter 16/2000 - Loss: -1.074
Iter 17/2000 - Loss: -1.027
Iter 18/2000 - Loss: -1.018
Iter 19/2000 - Loss: -1.072
Iter 20/2000 - Loss: -1.175
Iter 1981/2000 - Loss: -1.493
Iter 1982/2000 - Loss: -1.493
Iter 1983/2000 - Loss: -1.493
Iter 1984/2000 - Loss: -1.493
Iter 1985/2000 - Loss: -1.493
Iter 1986/2000 - Loss: -1.493
Iter 1987/2000 - Loss: -1.493
Iter 1988/2000 - Loss: -1.493
Iter 1989/2000 - Loss: -1.493
Iter 1990/2000 - Loss: -1.493
Iter 1991/2000 - Loss: -1.493
Iter 1992/2000 - Loss: -1.493
Iter 1993/2000 - Loss: -1.493
Iter 1994/2000 - Loss: -1.493
Iter 1995/2000 - Loss: -1.493
Iter 1996/2000 - Loss: -1.493
Iter 1997/2000 - Loss: -1.493
Iter 1998/2000 - Loss: -1.493
Iter 1999/2000 - Loss: -1.493
Iter 2000/2000 - Loss: -1.493
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0026],
        [0.1596],
        [0.0036]])
Lengthscale: tensor([[[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]],

        [[0.0118, 0.0398, 0.2480, 0.0056, 0.0003, 0.0266]]])
Signal Variance: tensor([0.0022, 0.0103, 0.7943, 0.0144])
Estimated target variance: tensor([0.0030, 0.0143, 1.0599, 0.0199])
N: 10
Signal to noise ratio: tensor([2.0007, 2.0051, 2.2308, 2.0074])
Bound on condition number: tensor([41.0289, 41.2048, 50.7645, 41.2983])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4011275890437629, policy loss: 0.6772172373192411
Experience 1, Iter 1, disc loss: 1.396895272812414, policy loss: 0.6759952916939471
Experience 1, Iter 2, disc loss: 1.3911063186192596, policy loss: 0.6756638637940409
Experience 1, Iter 3, disc loss: 1.3859384864225228, policy loss: 0.6749676820503205
Experience 1, Iter 4, disc loss: 1.3749755199201066, policy loss: 0.6795707214362645
Experience 1, Iter 5, disc loss: 1.3620082796120345, policy loss: 0.68597096226595
Experience 1, Iter 6, disc loss: 1.3590511827657552, policy loss: 0.6838322117024485
Experience 1, Iter 7, disc loss: 1.3495660070850093, policy loss: 0.6881424736924915
Experience 1, Iter 8, disc loss: 1.3485709662964644, policy loss: 0.6851305971213388
Experience 1, Iter 9, disc loss: 1.3341744210327522, policy loss: 0.694634224467105
Experience 1, Iter 10, disc loss: 1.3272071537147814, policy loss: 0.6971027597131738
Experience 1, Iter 11, disc loss: 1.320945037869063, policy loss: 0.6987671093224401
Experience 1, Iter 12, disc loss: 1.3165087115723697, policy loss: 0.6983876041602529
Experience 1, Iter 13, disc loss: 1.310005786498643, policy loss: 0.7004328676099263
Experience 1, Iter 14, disc loss: 1.2994170474882916, policy loss: 0.7062908327918871
Experience 1, Iter 15, disc loss: 1.2955621106819502, policy loss: 0.7054989529392692
Experience 1, Iter 16, disc loss: 1.2827044529133969, policy loss: 0.714032459983901
Experience 1, Iter 17, disc loss: 1.281402032127286, policy loss: 0.7104708917718423
Experience 1, Iter 18, disc loss: 1.2744975880379532, policy loss: 0.7126001138538652
Experience 1, Iter 19, disc loss: 1.268155638599893, policy loss: 0.7141658409238265
Experience 1, Iter 20, disc loss: 1.2581993640554563, policy loss: 0.7193930905169008
Experience 1, Iter 21, disc loss: 1.2501482763022964, policy loss: 0.7225098377912827
Experience 1, Iter 22, disc loss: 1.240987736982695, policy loss: 0.7270631117750198
Experience 1, Iter 23, disc loss: 1.239620151806554, policy loss: 0.7231676248683216
Experience 1, Iter 24, disc loss: 1.2268531626834345, policy loss: 0.7308874038310932
Experience 1, Iter 25, disc loss: 1.2228433891117387, policy loss: 0.7297631611679403
Experience 1, Iter 26, disc loss: 1.2098911241216141, policy loss: 0.7377239597910717
Experience 1, Iter 27, disc loss: 1.2007247488736925, policy loss: 0.741768431841149
Experience 1, Iter 28, disc loss: 1.1985558346409078, policy loss: 0.7379752367294805
Experience 1, Iter 29, disc loss: 1.187743934405355, policy loss: 0.7440035265242584
Experience 1, Iter 30, disc loss: 1.1788193320163627, policy loss: 0.7472581621556605
Experience 1, Iter 31, disc loss: 1.1702500772855027, policy loss: 0.7507441629281915
Experience 1, Iter 32, disc loss: 1.1602627740517617, policy loss: 0.7551397599985619
Experience 1, Iter 33, disc loss: 1.1536825087302267, policy loss: 0.7558781626599479
Experience 1, Iter 34, disc loss: 1.1394144370656485, policy loss: 0.7654650771668357
Experience 1, Iter 35, disc loss: 1.1375400081608804, policy loss: 0.7602715419555373
Experience 1, Iter 36, disc loss: 1.1273779593034967, policy loss: 0.7648647299502773
Experience 1, Iter 37, disc loss: 1.111976694330047, policy loss: 0.7761540133511622
Experience 1, Iter 38, disc loss: 1.0982395097811914, policy loss: 0.7847382081003966
Experience 1, Iter 39, disc loss: 1.097888175291771, policy loss: 0.7773109115744011
Experience 1, Iter 40, disc loss: 1.0802817844057913, policy loss: 0.7911280325763316
Experience 1, Iter 41, disc loss: 1.0754038575473894, policy loss: 0.788237774933361
Experience 1, Iter 42, disc loss: 1.070246119714769, policy loss: 0.7865145044074701
Experience 1, Iter 43, disc loss: 1.0513940292041168, policy loss: 0.8021310276986564
Experience 1, Iter 44, disc loss: 1.0461784046574292, policy loss: 0.8010107311980118
Experience 1, Iter 45, disc loss: 1.0287542454367755, policy loss: 0.815679874104773
Experience 1, Iter 46, disc loss: 1.018823599189301, policy loss: 0.8198568874072074
Experience 1, Iter 47, disc loss: 1.01102522441569, policy loss: 0.8222138138214543
Experience 1, Iter 48, disc loss: 0.991874591675445, policy loss: 0.8388402610308724
Experience 1, Iter 49, disc loss: 0.9992467675577735, policy loss: 0.8209094012762019
Experience 1, Iter 50, disc loss: 0.9758222241268072, policy loss: 0.8435259415496453
Experience 1, Iter 51, disc loss: 0.9697295752600291, policy loss: 0.84267481100326
Experience 1, Iter 52, disc loss: 0.9576371379435468, policy loss: 0.8507176523141936
Experience 1, Iter 53, disc loss: 0.938775484766862, policy loss: 0.867263019281915
Experience 1, Iter 54, disc loss: 0.9367292800784239, policy loss: 0.8602957073793303
Experience 1, Iter 55, disc loss: 0.9234466544809776, policy loss: 0.8693483772907558
Experience 1, Iter 56, disc loss: 0.908395148954204, policy loss: 0.8816805018605233
Experience 1, Iter 57, disc loss: 0.8967008206911046, policy loss: 0.8882762622277504
Experience 1, Iter 58, disc loss: 0.8854066095232349, policy loss: 0.8941802111557973
Experience 1, Iter 59, disc loss: 0.8709812923200493, policy loss: 0.9058414778278663
Experience 1, Iter 60, disc loss: 0.8520882273516681, policy loss: 0.9242698959214724
Experience 1, Iter 61, disc loss: 0.8452610560267606, policy loss: 0.9229764576189059
Experience 1, Iter 62, disc loss: 0.8339977302921284, policy loss: 0.9306057293754436
Experience 1, Iter 63, disc loss: 0.820392652298463, policy loss: 0.9431853368190857
Experience 1, Iter 64, disc loss: 0.7973312842582451, policy loss: 0.9684961940750427
Experience 1, Iter 65, disc loss: 0.7897241671431656, policy loss: 0.9694909602838218
Experience 1, Iter 66, disc loss: 0.7923842146837436, policy loss: 0.9539596377493192
Experience 1, Iter 67, disc loss: 0.7721281373677166, policy loss: 0.9742271930137429
Experience 1, Iter 68, disc loss: 0.7568848482649553, policy loss: 0.9891571460215277
Experience 1, Iter 69, disc loss: 0.7430132146237202, policy loss: 1.0001327749002846
Experience 1, Iter 70, disc loss: 0.7316003964581168, policy loss: 1.0083619635612058
Experience 1, Iter 71, disc loss: 0.713590165323884, policy loss: 1.0279263425036576
Experience 1, Iter 72, disc loss: 0.7005460113455557, policy loss: 1.0397465328098874
Experience 1, Iter 73, disc loss: 0.6911760432065833, policy loss: 1.0456353475129583
Experience 1, Iter 74, disc loss: 0.6738011834588538, policy loss: 1.066094145476609
Experience 1, Iter 75, disc loss: 0.6583518968380245, policy loss: 1.0827127729424757
Experience 1, Iter 76, disc loss: 0.6479659109644977, policy loss: 1.091829575999928
Experience 1, Iter 77, disc loss: 0.6371265064965445, policy loss: 1.1006602822156555
Experience 1, Iter 78, disc loss: 0.6366135189430577, policy loss: 1.0928022411562155
Experience 1, Iter 79, disc loss: 0.5992666490549623, policy loss: 1.1574317599736657
Experience 1, Iter 80, disc loss: 0.5967787473033539, policy loss: 1.1486510463036528
Experience 1, Iter 81, disc loss: 0.5996579342949, policy loss: 1.1300969367554452
Experience 1, Iter 82, disc loss: 0.5781035250389319, policy loss: 1.1665358073062684
Experience 1, Iter 83, disc loss: 0.5679856604354648, policy loss: 1.177216467550701
Experience 1, Iter 84, disc loss: 0.5338397296107305, policy loss: 1.241970365554087
Experience 1, Iter 85, disc loss: 0.5437275167146985, policy loss: 1.2066993687129821
Experience 1, Iter 86, disc loss: 0.5112153717991282, policy loss: 1.2733159120822597
Experience 1, Iter 87, disc loss: 0.5119515144390552, policy loss: 1.2581122105146623
Experience 1, Iter 88, disc loss: 0.49742301272014894, policy loss: 1.2793210180301067
Experience 1, Iter 89, disc loss: 0.4879044020123664, policy loss: 1.2962199898049152
Experience 1, Iter 90, disc loss: 0.48212392991012576, policy loss: 1.2963852073339575
Experience 1, Iter 91, disc loss: 0.45870276006084193, policy loss: 1.3542297619136958
Experience 1, Iter 92, disc loss: 0.45256231540875863, policy loss: 1.354735099863017
Experience 1, Iter 93, disc loss: 0.4486054772399226, policy loss: 1.3583068067031194
Experience 1, Iter 94, disc loss: 0.4334267843205185, policy loss: 1.3886881300050962
Experience 1, Iter 95, disc loss: 0.4312672646227902, policy loss: 1.3876910063581893
Experience 1, Iter 96, disc loss: 0.4181316092414895, policy loss: 1.415269920440915
Experience 1, Iter 97, disc loss: 0.4078070856476662, policy loss: 1.438890710018344
Experience 1, Iter 98, disc loss: 0.3947807928174407, policy loss: 1.467094324507746
Experience 1, Iter 99, disc loss: 0.3866639525165889, policy loss: 1.480546753434841
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0048],
        [0.3266],
        [0.0068]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0192, 1.3062, 0.0272], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0192, 1.3062, 0.0272])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.010
Iter 2/2000 - Loss: -0.282
Iter 3/2000 - Loss: -0.443
Iter 4/2000 - Loss: -0.016
Iter 5/2000 - Loss: 0.060
Iter 6/2000 - Loss: -0.155
Iter 7/2000 - Loss: -0.358
Iter 8/2000 - Loss: -0.410
Iter 9/2000 - Loss: -0.365
Iter 10/2000 - Loss: -0.333
Iter 11/2000 - Loss: -0.360
Iter 12/2000 - Loss: -0.432
Iter 13/2000 - Loss: -0.512
Iter 14/2000 - Loss: -0.571
Iter 15/2000 - Loss: -0.592
Iter 16/2000 - Loss: -0.573
Iter 17/2000 - Loss: -0.529
Iter 18/2000 - Loss: -0.500
Iter 19/2000 - Loss: -0.519
Iter 20/2000 - Loss: -0.591
Iter 1981/2000 - Loss: -0.877
Iter 1982/2000 - Loss: -0.877
Iter 1983/2000 - Loss: -0.877
Iter 1984/2000 - Loss: -0.877
Iter 1985/2000 - Loss: -0.877
Iter 1986/2000 - Loss: -0.877
Iter 1987/2000 - Loss: -0.877
Iter 1988/2000 - Loss: -0.877
Iter 1989/2000 - Loss: -0.877
Iter 1990/2000 - Loss: -0.877
Iter 1991/2000 - Loss: -0.877
Iter 1992/2000 - Loss: -0.877
Iter 1993/2000 - Loss: -0.877
Iter 1994/2000 - Loss: -0.877
Iter 1995/2000 - Loss: -0.877
Iter 1996/2000 - Loss: -0.877
Iter 1997/2000 - Loss: -0.877
Iter 1998/2000 - Loss: -0.877
Iter 1999/2000 - Loss: -0.877
Iter 2000/2000 - Loss: -0.877
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0036],
        [0.2060],
        [0.0051]])
Lengthscale: tensor([[[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]],

        [[0.0136, 0.0413, 0.3201, 0.0073, 0.0005, 0.0439]]])
Signal Variance: tensor([0.0028, 0.0146, 1.0350, 0.0207])
Estimated target variance: tensor([0.0037, 0.0192, 1.3062, 0.0272])
N: 20
Signal to noise ratio: tensor([2.0012, 2.0065, 2.2416, 2.0094])
Bound on condition number: tensor([ 81.0944,  81.5175, 101.4995,  81.7519])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.3649387349973737, policy loss: 1.5536518784820033
Experience 2, Iter 1, disc loss: 0.3370593426847185, policy loss: 1.6443186127388327
Experience 2, Iter 2, disc loss: 0.34428964146097846, policy loss: 1.6096848639449366
Experience 2, Iter 3, disc loss: 0.3345106046538305, policy loss: 1.6519637599214692
Experience 2, Iter 4, disc loss: 0.30905714907519255, policy loss: 1.7277412596100277
Experience 2, Iter 5, disc loss: 0.3004509354728777, policy loss: 1.748304808656108
Experience 2, Iter 6, disc loss: 0.30537171004477415, policy loss: 1.7249245247890153
Experience 2, Iter 7, disc loss: 0.29321450424263096, policy loss: 1.7669094310633273
Experience 2, Iter 8, disc loss: 0.2856974507491311, policy loss: 1.7892406458114887
Experience 2, Iter 9, disc loss: 0.28267673530915116, policy loss: 1.8023435829601708
Experience 2, Iter 10, disc loss: 0.25940579940352204, policy loss: 1.8982311904146745
Experience 2, Iter 11, disc loss: 0.26037553573930017, policy loss: 1.8794409695478214
Experience 2, Iter 12, disc loss: 0.26012527747005804, policy loss: 1.8603826101130059
Experience 2, Iter 13, disc loss: 0.241375829225218, policy loss: 1.9546863839774973
Experience 2, Iter 14, disc loss: 0.23287534666533882, policy loss: 2.0038958205428172
Experience 2, Iter 15, disc loss: 0.2123886245584583, policy loss: 2.109120155507527
Experience 2, Iter 16, disc loss: 0.22565726826436183, policy loss: 2.0130679748300317
Experience 2, Iter 17, disc loss: 0.22062862569904157, policy loss: 2.0328965186590464
Experience 2, Iter 18, disc loss: 0.20825531960242546, policy loss: 2.099217724820265
Experience 2, Iter 19, disc loss: 0.21255412489565748, policy loss: 2.053816340638721
Experience 2, Iter 20, disc loss: 0.20724519852295822, policy loss: 2.0872524558788648
Experience 2, Iter 21, disc loss: 0.19000312682762843, policy loss: 2.1748866256059847
Experience 2, Iter 22, disc loss: 0.18899121328655252, policy loss: 2.1734928537216813
Experience 2, Iter 23, disc loss: 0.17611328826677525, policy loss: 2.2601233085990087
Experience 2, Iter 24, disc loss: 0.16789928565234424, policy loss: 2.2898498287097713
Experience 2, Iter 25, disc loss: 0.1558754998681067, policy loss: 2.3767331655378845
Experience 2, Iter 26, disc loss: 0.15515773117038967, policy loss: 2.3589113930829395
Experience 2, Iter 27, disc loss: 0.1448128049436393, policy loss: 2.464384693799402
Experience 2, Iter 28, disc loss: 0.1392271001073092, policy loss: 2.487036867503556
Experience 2, Iter 29, disc loss: 0.14111096950413554, policy loss: 2.439101128322064
Experience 2, Iter 30, disc loss: 0.14069760651589938, policy loss: 2.4507951768198586
Experience 2, Iter 31, disc loss: 0.12635852109672172, policy loss: 2.5703479888745147
Experience 2, Iter 32, disc loss: 0.12158420543479545, policy loss: 2.619297160432474
Experience 2, Iter 33, disc loss: 0.12464577558677498, policy loss: 2.6142086457245997
Experience 2, Iter 34, disc loss: 0.11870823537861816, policy loss: 2.5827538212646655
Experience 2, Iter 35, disc loss: 0.11670479778040012, policy loss: 2.638582457289788
Experience 2, Iter 36, disc loss: 0.1092297914320455, policy loss: 2.693780179358381
Experience 2, Iter 37, disc loss: 0.09933500961146813, policy loss: 2.7774854616435025
Experience 2, Iter 38, disc loss: 0.09654298389566174, policy loss: 2.844748913974355
Experience 2, Iter 39, disc loss: 0.09607494426202746, policy loss: 2.790532988142024
Experience 2, Iter 40, disc loss: 0.08977413939640612, policy loss: 2.895213271233596
Experience 2, Iter 41, disc loss: 0.08465679828084163, policy loss: 2.9647947495565425
Experience 2, Iter 42, disc loss: 0.0902320410107641, policy loss: 2.8933162049032304
Experience 2, Iter 43, disc loss: 0.08541397110450452, policy loss: 3.003323866091617
Experience 2, Iter 44, disc loss: 0.08258400046283244, policy loss: 2.9689469654429885
Experience 2, Iter 45, disc loss: 0.08542366548650325, policy loss: 2.9295269149793457
Experience 2, Iter 46, disc loss: 0.08280400235103616, policy loss: 2.9480356402107457
Experience 2, Iter 47, disc loss: 0.08535583058293193, policy loss: 2.9203820475343156
Experience 2, Iter 48, disc loss: 0.07030337005322376, policy loss: 3.1381148501958176
Experience 2, Iter 49, disc loss: 0.07078050984945404, policy loss: 3.1200954764754374
Experience 2, Iter 50, disc loss: 0.06920072146214704, policy loss: 3.1865216368513867
Experience 2, Iter 51, disc loss: 0.0657878989679174, policy loss: 3.190095152588735
Experience 2, Iter 52, disc loss: 0.06371511963897153, policy loss: 3.2617526712856075
Experience 2, Iter 53, disc loss: 0.06650063800875036, policy loss: 3.2087941717895516
Experience 2, Iter 54, disc loss: 0.06489634315378005, policy loss: 3.201270173900167
Experience 2, Iter 55, disc loss: 0.06864966005484327, policy loss: 3.175731871688675
Experience 2, Iter 56, disc loss: 0.05474144366026064, policy loss: 3.4075335745952864
Experience 2, Iter 57, disc loss: 0.05567683873733248, policy loss: 3.3705161018076546
Experience 2, Iter 58, disc loss: 0.051338492636893274, policy loss: 3.525626228862391
Experience 2, Iter 59, disc loss: 0.05521480159868018, policy loss: 3.3571244950616483
Experience 2, Iter 60, disc loss: 0.0550073430923276, policy loss: 3.3765647146061477
Experience 2, Iter 61, disc loss: 0.04829120595167692, policy loss: 3.495007376941968
Experience 2, Iter 62, disc loss: 0.04622595258170275, policy loss: 3.610852795497814
Experience 2, Iter 63, disc loss: 0.04661725781747017, policy loss: 3.55233845079554
Experience 2, Iter 64, disc loss: 0.04828377154834752, policy loss: 3.5294259837355986
Experience 2, Iter 65, disc loss: 0.048979120722450295, policy loss: 3.548554678379695
Experience 2, Iter 66, disc loss: 0.04533504458300235, policy loss: 3.602304861146756
Experience 2, Iter 67, disc loss: 0.0437496471234783, policy loss: 3.6431861512179786
Experience 2, Iter 68, disc loss: 0.04244490768426107, policy loss: 3.723445840653256
Experience 2, Iter 69, disc loss: 0.04168707328774791, policy loss: 3.6846513216542203
Experience 2, Iter 70, disc loss: 0.04176195521373101, policy loss: 3.642506783935764
Experience 2, Iter 71, disc loss: 0.04028673465159349, policy loss: 3.726268365354897
Experience 2, Iter 72, disc loss: 0.03710088430341171, policy loss: 3.8796359272308827
Experience 2, Iter 73, disc loss: 0.036384585569328734, policy loss: 3.91766835114632
Experience 2, Iter 74, disc loss: 0.039612133699855195, policy loss: 3.7918552968415016
Experience 2, Iter 75, disc loss: 0.03342760464524384, policy loss: 3.9249162111448084
Experience 2, Iter 76, disc loss: 0.03372202194172456, policy loss: 3.965943365687536
Experience 2, Iter 77, disc loss: 0.03052349213940628, policy loss: 3.965088739655335
Experience 2, Iter 78, disc loss: 0.035661331467363916, policy loss: 3.8335103479551376
Experience 2, Iter 79, disc loss: 0.034497244071212185, policy loss: 3.8662131037002117
Experience 2, Iter 80, disc loss: 0.03369512164283046, policy loss: 3.8792425432534747
Experience 2, Iter 81, disc loss: 0.030125079111027988, policy loss: 4.068836625451798
Experience 2, Iter 82, disc loss: 0.031242499157562487, policy loss: 3.9900391606418
Experience 2, Iter 83, disc loss: 0.03154579002467535, policy loss: 4.036102297259773
Experience 2, Iter 84, disc loss: 0.026857775105200918, policy loss: 4.159326072346623
Experience 2, Iter 85, disc loss: 0.028329774857743024, policy loss: 4.13159821255148
Experience 2, Iter 86, disc loss: 0.030635376363983825, policy loss: 4.041753575466292
Experience 2, Iter 87, disc loss: 0.03050462791953585, policy loss: 3.977586549630131
Experience 2, Iter 88, disc loss: 0.027738416497383105, policy loss: 4.149596730387928
Experience 2, Iter 89, disc loss: 0.02882285930594209, policy loss: 4.202870482125132
Experience 2, Iter 90, disc loss: 0.02271114671117885, policy loss: 4.398022509809618
Experience 2, Iter 91, disc loss: 0.02805640963710842, policy loss: 4.126988791633927
Experience 2, Iter 92, disc loss: 0.026302594289775237, policy loss: 4.171784316122132
Experience 2, Iter 93, disc loss: 0.026214618499064105, policy loss: 4.211218855257315
Experience 2, Iter 94, disc loss: 0.02704734151479695, policy loss: 4.189061460025155
Experience 2, Iter 95, disc loss: 0.02359847044829326, policy loss: 4.3570861518122195
Experience 2, Iter 96, disc loss: 0.023212674218066227, policy loss: 4.303113621815659
Experience 2, Iter 97, disc loss: 0.02583202010289904, policy loss: 4.182639484850049
Experience 2, Iter 98, disc loss: 0.020466367962659444, policy loss: 4.444025850654194
Experience 2, Iter 99, disc loss: 0.02453444922339975, policy loss: 4.260424708194153
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0055],
        [0.2297],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0118, 0.0395, 0.2254, 0.0058, 0.0004, 0.1321]],

        [[0.0118, 0.0395, 0.2254, 0.0058, 0.0004, 0.1321]],

        [[0.0118, 0.0395, 0.2254, 0.0058, 0.0004, 0.1321]],

        [[0.0118, 0.0395, 0.2254, 0.0058, 0.0004, 0.1321]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0221, 0.9187, 0.0193], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0221, 0.9187, 0.0193])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.568
Iter 2/2000 - Loss: -0.816
Iter 3/2000 - Loss: -0.593
Iter 4/2000 - Loss: -0.353
Iter 5/2000 - Loss: -0.476
Iter 6/2000 - Loss: -0.735
Iter 7/2000 - Loss: -0.859
Iter 8/2000 - Loss: -0.804
Iter 9/2000 - Loss: -0.721
Iter 10/2000 - Loss: -0.735
Iter 11/2000 - Loss: -0.819
Iter 12/2000 - Loss: -0.893
Iter 13/2000 - Loss: -0.918
Iter 14/2000 - Loss: -0.913
Iter 15/2000 - Loss: -0.914
Iter 16/2000 - Loss: -0.937
Iter 17/2000 - Loss: -0.978
Iter 18/2000 - Loss: -1.024
Iter 19/2000 - Loss: -1.066
Iter 20/2000 - Loss: -1.098
Iter 1981/2000 - Loss: -7.342
Iter 1982/2000 - Loss: -7.342
Iter 1983/2000 - Loss: -7.342
Iter 1984/2000 - Loss: -7.342
Iter 1985/2000 - Loss: -7.342
Iter 1986/2000 - Loss: -7.342
Iter 1987/2000 - Loss: -7.342
Iter 1988/2000 - Loss: -7.342
Iter 1989/2000 - Loss: -7.342
Iter 1990/2000 - Loss: -7.342
Iter 1991/2000 - Loss: -7.342
Iter 1992/2000 - Loss: -7.342
Iter 1993/2000 - Loss: -7.342
Iter 1994/2000 - Loss: -7.342
Iter 1995/2000 - Loss: -7.342
Iter 1996/2000 - Loss: -7.342
Iter 1997/2000 - Loss: -7.342
Iter 1998/2000 - Loss: -7.342
Iter 1999/2000 - Loss: -7.342
Iter 2000/2000 - Loss: -7.342
***AFTER OPTIMATION***
Noise Variance: tensor([[5.5182e-06],
        [2.3507e-04],
        [1.8670e-03],
        [2.9566e-04]])
Lengthscale: tensor([[[4.0917e-01, 1.4672e+00, 4.0968e+00, 5.8621e-01, 2.0343e-02,
          2.0475e+01]],

        [[2.8601e+01, 4.4244e+01, 1.2688e+01, 1.5918e+00, 8.0027e+00,
          4.7634e+00]],

        [[2.5909e+01, 3.4406e+01, 1.2861e+01, 8.0820e-01, 5.1900e-01,
          9.8202e+00]],

        [[2.6627e+01, 4.7588e+01, 9.6337e+00, 2.9329e+00, 8.6594e+00,
          2.1056e+01]]])
Signal Variance: tensor([3.5223e-03, 2.5284e-01, 4.7198e+00, 2.6777e-01])
Estimated target variance: tensor([0.0036, 0.0221, 0.9187, 0.0193])
N: 30
Signal to noise ratio: tensor([25.2649, 32.7963, 50.2796, 30.0943])
Bound on condition number: tensor([19150.4172, 32268.9009, 75842.2911, 27171.0907])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.10966763811972993, policy loss: 2.4934005995454505
Experience 3, Iter 1, disc loss: 0.1343828188779546, policy loss: 2.3066047302944734
Experience 3, Iter 2, disc loss: 0.1522051110348287, policy loss: 2.157012121345288
Experience 3, Iter 3, disc loss: 0.18048900483774652, policy loss: 1.9736679430237989
Experience 3, Iter 4, disc loss: 0.20249261885476139, policy loss: 1.797625278571946
Experience 3, Iter 5, disc loss: 0.23704624697971247, policy loss: 1.6424425416534103
Experience 3, Iter 6, disc loss: 0.2703403263511393, policy loss: 1.5393826853382766
Experience 3, Iter 7, disc loss: 0.3023046057750721, policy loss: 1.4650040702519382
Experience 3, Iter 8, disc loss: 0.30843416351367126, policy loss: 1.415861476710027
Experience 3, Iter 9, disc loss: 0.33242057987023993, policy loss: 1.3798107131775041
Experience 3, Iter 10, disc loss: 0.3377717922653867, policy loss: 1.3592231895927196
Experience 3, Iter 11, disc loss: 0.3092353174563867, policy loss: 1.512140881722988
Experience 3, Iter 12, disc loss: 0.31194700757179145, policy loss: 1.4975724103060286
Experience 3, Iter 13, disc loss: 0.29866966310301446, policy loss: 1.5010434893202635
Experience 3, Iter 14, disc loss: 0.3238441480403345, policy loss: 1.4256251054520712
Experience 3, Iter 15, disc loss: 0.28137556479628545, policy loss: 1.5310935539326218
Experience 3, Iter 16, disc loss: 0.29030193096201573, policy loss: 1.5415502510515562
Experience 3, Iter 17, disc loss: 0.3115698747897764, policy loss: 1.423958927266767
Experience 3, Iter 18, disc loss: 0.3131124728565648, policy loss: 1.408713837668802
Experience 3, Iter 19, disc loss: 0.29911264547567273, policy loss: 1.4621291472612576
Experience 3, Iter 20, disc loss: 0.29482433586589196, policy loss: 1.4623651495925327
Experience 3, Iter 21, disc loss: 0.2942503172860481, policy loss: 1.474845427129919
Experience 3, Iter 22, disc loss: 0.2806207561896196, policy loss: 1.5164352846762725
Experience 3, Iter 23, disc loss: 0.28381606610097315, policy loss: 1.5191145430957333
Experience 3, Iter 24, disc loss: 0.2741549230640544, policy loss: 1.5201168146133077
Experience 3, Iter 25, disc loss: 0.2434912675411774, policy loss: 1.6749219373101307
Experience 3, Iter 26, disc loss: 0.2635121451699479, policy loss: 1.609464775286352
Experience 3, Iter 27, disc loss: 0.2608307564917218, policy loss: 1.5683374009899245
Experience 3, Iter 28, disc loss: 0.2555062919480737, policy loss: 1.5918288500932942
Experience 3, Iter 29, disc loss: 0.26660366525492574, policy loss: 1.5484543895909335
Experience 3, Iter 30, disc loss: 0.22634958127656496, policy loss: 1.6946274224047035
Experience 3, Iter 31, disc loss: 0.24059101220343887, policy loss: 1.6476120512323456
Experience 3, Iter 32, disc loss: 0.2431733334375441, policy loss: 1.6347363854060935
Experience 3, Iter 33, disc loss: 0.2216805629736026, policy loss: 1.723202353184539
Experience 3, Iter 34, disc loss: 0.22016764751761475, policy loss: 1.780425991035762
Experience 3, Iter 35, disc loss: 0.20943255648558892, policy loss: 1.7952833439541798
Experience 3, Iter 36, disc loss: 0.21596124168649694, policy loss: 1.7514437332828283
Experience 3, Iter 37, disc loss: 0.21259629000114724, policy loss: 1.7757740637089416
Experience 3, Iter 38, disc loss: 0.20284139261113002, policy loss: 1.8159031445286753
Experience 3, Iter 39, disc loss: 0.20873824199535862, policy loss: 1.775709483217729
Experience 3, Iter 40, disc loss: 0.19304170226695433, policy loss: 1.8763021085810823
Experience 3, Iter 41, disc loss: 0.19257697408847985, policy loss: 1.8704294660709353
Experience 3, Iter 42, disc loss: 0.18270599048931724, policy loss: 1.8941313774338047
Experience 3, Iter 43, disc loss: 0.1935112618915698, policy loss: 1.8501521308562345
Experience 3, Iter 44, disc loss: 0.19049995982112636, policy loss: 1.8516262858851635
Experience 3, Iter 45, disc loss: 0.19332158275071157, policy loss: 1.8499913602166136
Experience 3, Iter 46, disc loss: 0.17668014752471603, policy loss: 1.9811570795907194
Experience 3, Iter 47, disc loss: 0.17506144413523364, policy loss: 1.9717094768543113
Experience 3, Iter 48, disc loss: 0.1777384760288516, policy loss: 1.9493775305414855
Experience 3, Iter 49, disc loss: 0.16324621837621803, policy loss: 2.046456958399685
Experience 3, Iter 50, disc loss: 0.17992679140412848, policy loss: 1.9104386154996573
Experience 3, Iter 51, disc loss: 0.16350209025323167, policy loss: 2.0121362662843305
Experience 3, Iter 52, disc loss: 0.15893118279802315, policy loss: 2.0905911923113827
Experience 3, Iter 53, disc loss: 0.15647054314775957, policy loss: 2.0442388208735123
Experience 3, Iter 54, disc loss: 0.1545878420649885, policy loss: 2.0593082099327895
Experience 3, Iter 55, disc loss: 0.15501127528249714, policy loss: 2.0494944135363573
Experience 3, Iter 56, disc loss: 0.15132748926895687, policy loss: 2.1057245576907238
Experience 3, Iter 57, disc loss: 0.14626476009832642, policy loss: 2.129897219381761
Experience 3, Iter 58, disc loss: 0.1418856474150173, policy loss: 2.139311621821227
Experience 3, Iter 59, disc loss: 0.14279867085634967, policy loss: 2.161522101329436
Experience 3, Iter 60, disc loss: 0.13935481433850427, policy loss: 2.1871644699400328
Experience 3, Iter 61, disc loss: 0.1458469153568202, policy loss: 2.150476079227255
Experience 3, Iter 62, disc loss: 0.13189203765774, policy loss: 2.2432334594004733
Experience 3, Iter 63, disc loss: 0.12986254003392986, policy loss: 2.259071426829804
Experience 3, Iter 64, disc loss: 0.1338227598960636, policy loss: 2.244110048791838
Experience 3, Iter 65, disc loss: 0.1281993028947379, policy loss: 2.2557836739468753
Experience 3, Iter 66, disc loss: 0.1233356221882909, policy loss: 2.350521523178139
Experience 3, Iter 67, disc loss: 0.11430256861905314, policy loss: 2.411095995709042
Experience 3, Iter 68, disc loss: 0.11742065099390928, policy loss: 2.3436043410005043
Experience 3, Iter 69, disc loss: 0.10789481157414002, policy loss: 2.5076189142023417
Experience 3, Iter 70, disc loss: 0.12513648745109662, policy loss: 2.281769366503177
Experience 3, Iter 71, disc loss: 0.10983640863116269, policy loss: 2.4625200267754965
Experience 3, Iter 72, disc loss: 0.11943013009866658, policy loss: 2.3184491331048434
Experience 3, Iter 73, disc loss: 0.10991595776099515, policy loss: 2.3886565997050253
Experience 3, Iter 74, disc loss: 0.10466887293499598, policy loss: 2.519568564161434
Experience 3, Iter 75, disc loss: 0.1095531938551757, policy loss: 2.486832441225528
Experience 3, Iter 76, disc loss: 0.09972223217156707, policy loss: 2.478277072430595
Experience 3, Iter 77, disc loss: 0.10759101431002574, policy loss: 2.422892432812749
Experience 3, Iter 78, disc loss: 0.1082602602695028, policy loss: 2.449816713117275
Experience 3, Iter 79, disc loss: 0.09315872487685618, policy loss: 2.6078393555995025
Experience 3, Iter 80, disc loss: 0.09251873284743148, policy loss: 2.5908262446486465
Experience 3, Iter 81, disc loss: 0.1025858551433574, policy loss: 2.474834010366604
Experience 3, Iter 82, disc loss: 0.09772170792039162, policy loss: 2.5452334698251704
Experience 3, Iter 83, disc loss: 0.08688446570612618, policy loss: 2.656088091574456
Experience 3, Iter 84, disc loss: 0.0931609353885597, policy loss: 2.5716079908028853
Experience 3, Iter 85, disc loss: 0.09084557716628024, policy loss: 2.619392747615725
Experience 3, Iter 86, disc loss: 0.08775756952042092, policy loss: 2.6748554903498727
Experience 3, Iter 87, disc loss: 0.08566227487394075, policy loss: 2.7091259153255214
Experience 3, Iter 88, disc loss: 0.0884397085089522, policy loss: 2.631396516725115
Experience 3, Iter 89, disc loss: 0.0846139734297151, policy loss: 2.694280719988268
Experience 3, Iter 90, disc loss: 0.08397521053421239, policy loss: 2.7155278441610062
Experience 3, Iter 91, disc loss: 0.08723632596206263, policy loss: 2.618607106378586
Experience 3, Iter 92, disc loss: 0.07446997842104443, policy loss: 2.8178786077533946
Experience 3, Iter 93, disc loss: 0.08172378101351888, policy loss: 2.738534272053837
Experience 3, Iter 94, disc loss: 0.07436511751337403, policy loss: 2.8256573291647036
Experience 3, Iter 95, disc loss: 0.07934342589393095, policy loss: 2.779535787422186
Experience 3, Iter 96, disc loss: 0.07447782459475823, policy loss: 2.8120519283560284
Experience 3, Iter 97, disc loss: 0.07410417795111189, policy loss: 2.807215749453289
Experience 3, Iter 98, disc loss: 0.07400996865722005, policy loss: 2.8980612648740913
Experience 3, Iter 99, disc loss: 0.07947595945955287, policy loss: 2.868096193086719
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0065],
        [0.1876],
        [0.0041]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0129, 0.0580, 0.1900, 0.0059, 0.0003, 0.2102]],

        [[0.0129, 0.0580, 0.1900, 0.0059, 0.0003, 0.2102]],

        [[0.0129, 0.0580, 0.1900, 0.0059, 0.0003, 0.2102]],

        [[0.0129, 0.0580, 0.1900, 0.0059, 0.0003, 0.2102]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0058, 0.0258, 0.7506, 0.0165], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0058, 0.0258, 0.7506, 0.0165])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.370
Iter 2/2000 - Loss: -0.632
Iter 3/2000 - Loss: -0.608
Iter 4/2000 - Loss: -0.642
Iter 5/2000 - Loss: -0.698
Iter 6/2000 - Loss: -0.764
Iter 7/2000 - Loss: -0.797
Iter 8/2000 - Loss: -0.816
Iter 9/2000 - Loss: -0.824
Iter 10/2000 - Loss: -0.836
Iter 11/2000 - Loss: -0.852
Iter 12/2000 - Loss: -0.883
Iter 13/2000 - Loss: -0.925
Iter 14/2000 - Loss: -0.939
Iter 15/2000 - Loss: -0.913
Iter 16/2000 - Loss: -0.900
Iter 17/2000 - Loss: -0.945
Iter 18/2000 - Loss: -1.007
Iter 19/2000 - Loss: -1.023
Iter 20/2000 - Loss: -1.012
Iter 1981/2000 - Loss: -7.766
Iter 1982/2000 - Loss: -7.766
Iter 1983/2000 - Loss: -7.766
Iter 1984/2000 - Loss: -7.766
Iter 1985/2000 - Loss: -7.766
Iter 1986/2000 - Loss: -7.766
Iter 1987/2000 - Loss: -7.766
Iter 1988/2000 - Loss: -7.766
Iter 1989/2000 - Loss: -7.766
Iter 1990/2000 - Loss: -7.766
Iter 1991/2000 - Loss: -7.766
Iter 1992/2000 - Loss: -7.766
Iter 1993/2000 - Loss: -7.766
Iter 1994/2000 - Loss: -7.766
Iter 1995/2000 - Loss: -7.766
Iter 1996/2000 - Loss: -7.766
Iter 1997/2000 - Loss: -7.766
Iter 1998/2000 - Loss: -7.766
Iter 1999/2000 - Loss: -7.766
Iter 2000/2000 - Loss: -7.766
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[ 9.1264,  0.3894, 28.4314,  3.8284,  0.0508,  9.5997]],

        [[28.3336, 44.7242, 32.2149,  2.4277,  8.4690, 11.7935]],

        [[26.1217, 43.8734, 15.4681,  0.9183,  0.5256, 11.8497]],

        [[26.8198, 46.2340, 10.2271,  3.1461,  9.6614, 25.0885]]])
Signal Variance: tensor([4.5511e-03, 7.5898e-01, 5.7476e+00, 2.8939e-01])
Estimated target variance: tensor([0.0058, 0.0258, 0.7506, 0.0165])
N: 40
Signal to noise ratio: tensor([ 6.4596, 53.0279, 54.2190, 33.6652])
Bound on condition number: tensor([  1670.0454, 112479.3753, 117588.8925,  45334.9600])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.16643450134912713, policy loss: 1.971020075634479
Experience 4, Iter 1, disc loss: 0.18880275588162412, policy loss: 1.8315196660813275
Experience 4, Iter 2, disc loss: 0.19395795690420414, policy loss: 1.813992740628688
Experience 4, Iter 3, disc loss: 0.19318280252100636, policy loss: 1.8230844191063378
Experience 4, Iter 4, disc loss: 0.19251199287258855, policy loss: 1.8128702369830707
Experience 4, Iter 5, disc loss: 0.20101413183569752, policy loss: 1.7629091522503968
Experience 4, Iter 6, disc loss: 0.18461227334566943, policy loss: 1.8527721734954063
Experience 4, Iter 7, disc loss: 0.1804846860907126, policy loss: 1.8766997487910506
Experience 4, Iter 8, disc loss: 0.17314455104709578, policy loss: 1.9295832744915602
Experience 4, Iter 9, disc loss: 0.1679952941802328, policy loss: 1.958390794475216
Experience 4, Iter 10, disc loss: 0.16401676839298115, policy loss: 1.9990317954160544
Experience 4, Iter 11, disc loss: 0.1740654260656631, policy loss: 1.9321207589200884
Experience 4, Iter 12, disc loss: 0.16446353119840007, policy loss: 1.990009286389169
Experience 4, Iter 13, disc loss: 0.15114646930101397, policy loss: 2.067950254040933
Experience 4, Iter 14, disc loss: 0.1480000047464115, policy loss: 2.097648840790338
Experience 4, Iter 15, disc loss: 0.14629825268426197, policy loss: 2.1031285796466666
Experience 4, Iter 16, disc loss: 0.13659247909074326, policy loss: 2.1644644848449377
Experience 4, Iter 17, disc loss: 0.1388419388165685, policy loss: 2.1449763383767233
Experience 4, Iter 18, disc loss: 0.14182341909737328, policy loss: 2.119530805687025
Experience 4, Iter 19, disc loss: 0.13544119080586406, policy loss: 2.169063152531694
Experience 4, Iter 20, disc loss: 0.12998030209836275, policy loss: 2.2127169225507166
Experience 4, Iter 21, disc loss: 0.1239345168962364, policy loss: 2.2694386278729364
Experience 4, Iter 22, disc loss: 0.12538210163898422, policy loss: 2.2768405127250486
Experience 4, Iter 23, disc loss: 0.13990133701127644, policy loss: 2.1380667025047044
Experience 4, Iter 24, disc loss: 0.12362154760557796, policy loss: 2.288823719778866
Experience 4, Iter 25, disc loss: 0.12522457832018263, policy loss: 2.276147029477242
Experience 4, Iter 26, disc loss: 0.12546257211987485, policy loss: 2.26081924786108
Experience 4, Iter 27, disc loss: 0.12141445657659154, policy loss: 2.2959174786269743
Experience 4, Iter 28, disc loss: 0.11222887957166067, policy loss: 2.374805750162028
Experience 4, Iter 29, disc loss: 0.11546662501781076, policy loss: 2.3453665515874675
Experience 4, Iter 30, disc loss: 0.11854553570842842, policy loss: 2.3222878480282763
Experience 4, Iter 31, disc loss: 0.10977108803138652, policy loss: 2.4005845410286595
Experience 4, Iter 32, disc loss: 0.10138899746677847, policy loss: 2.4887038663747365
Experience 4, Iter 33, disc loss: 0.11020581302002805, policy loss: 2.381105420898324
Experience 4, Iter 34, disc loss: 0.09217993012032474, policy loss: 2.56829648098981
Experience 4, Iter 35, disc loss: 0.09975976094352297, policy loss: 2.4797545521071602
Experience 4, Iter 36, disc loss: 0.09636655695244858, policy loss: 2.5245761040246286
Experience 4, Iter 37, disc loss: 0.09487520826411, policy loss: 2.536603685092511
Experience 4, Iter 38, disc loss: 0.09842161436036817, policy loss: 2.4847306684227144
Experience 4, Iter 39, disc loss: 0.09136694068135238, policy loss: 2.578000650846959
Experience 4, Iter 40, disc loss: 0.09101627441609579, policy loss: 2.5617638961081988
Experience 4, Iter 41, disc loss: 0.08484618717746925, policy loss: 2.6491674170638624
Experience 4, Iter 42, disc loss: 0.08560728237503239, policy loss: 2.631511339585315
Experience 4, Iter 43, disc loss: 0.08288071843706857, policy loss: 2.6543366238197104
Experience 4, Iter 44, disc loss: 0.08000647034318953, policy loss: 2.692947159096318
Experience 4, Iter 45, disc loss: 0.0795966087573445, policy loss: 2.6927406063710486
Experience 4, Iter 46, disc loss: 0.07518867898443489, policy loss: 2.7611314011318804
Experience 4, Iter 47, disc loss: 0.0790484673384608, policy loss: 2.716181324188007
Experience 4, Iter 48, disc loss: 0.07061126990027013, policy loss: 2.823039809568149
Experience 4, Iter 49, disc loss: 0.07261512689615819, policy loss: 2.7954942994474754
Experience 4, Iter 50, disc loss: 0.07084625995987472, policy loss: 2.826230013301831
Experience 4, Iter 51, disc loss: 0.066728304938177, policy loss: 2.880088706128041
Experience 4, Iter 52, disc loss: 0.06596475215338508, policy loss: 2.904292417229261
Experience 4, Iter 53, disc loss: 0.06275390295339747, policy loss: 2.9281150456871328
Experience 4, Iter 54, disc loss: 0.06375857077792003, policy loss: 2.906028593844692
Experience 4, Iter 55, disc loss: 0.06365591322951816, policy loss: 2.8997007285901533
Experience 4, Iter 56, disc loss: 0.06511475235117115, policy loss: 2.8840740214908993
Experience 4, Iter 57, disc loss: 0.06412686660132917, policy loss: 2.8895598026483786
Experience 4, Iter 58, disc loss: 0.06008813399703017, policy loss: 2.9763938212846215
Experience 4, Iter 59, disc loss: 0.05506403647429087, policy loss: 3.0514565661014474
Experience 4, Iter 60, disc loss: 0.05819075206175539, policy loss: 2.990330685901534
Experience 4, Iter 61, disc loss: 0.057240660625510406, policy loss: 3.0226963168294922
Experience 4, Iter 62, disc loss: 0.05298644172107187, policy loss: 3.112783997998401
Experience 4, Iter 63, disc loss: 0.05260984263446999, policy loss: 3.1033767789937676
Experience 4, Iter 64, disc loss: 0.053903028158378545, policy loss: 3.076082448005831
Experience 4, Iter 65, disc loss: 0.05157651614282372, policy loss: 3.1104025326715137
Experience 4, Iter 66, disc loss: 0.04830284179611217, policy loss: 3.1993074854367403
Experience 4, Iter 67, disc loss: 0.04782107571026363, policy loss: 3.1827974118709315
Experience 4, Iter 68, disc loss: 0.04720549441552286, policy loss: 3.222823164540867
Experience 4, Iter 69, disc loss: 0.04607686436911102, policy loss: 3.226463986029513
Experience 4, Iter 70, disc loss: 0.045366365209954254, policy loss: 3.25060453253163
Experience 4, Iter 71, disc loss: 0.048408104402471626, policy loss: 3.170391790958645
Experience 4, Iter 72, disc loss: 0.0439128630946221, policy loss: 3.291191161696251
Experience 4, Iter 73, disc loss: 0.04305607807546952, policy loss: 3.2878775221403265
Experience 4, Iter 74, disc loss: 0.04162318237865177, policy loss: 3.3413543646258352
Experience 4, Iter 75, disc loss: 0.04262412131208432, policy loss: 3.295307397150874
Experience 4, Iter 76, disc loss: 0.044542089366284406, policy loss: 3.2582231049989945
Experience 4, Iter 77, disc loss: 0.04037256225760516, policy loss: 3.379906889094479
Experience 4, Iter 78, disc loss: 0.04047941423669281, policy loss: 3.4176317445989097
Experience 4, Iter 79, disc loss: 0.03992092752321278, policy loss: 3.379599307677053
Experience 4, Iter 80, disc loss: 0.04086355973217614, policy loss: 3.3553442429689104
Experience 4, Iter 81, disc loss: 0.0394656916954645, policy loss: 3.4286341917525074
Experience 4, Iter 82, disc loss: 0.035652102070885004, policy loss: 3.5031161125859653
Experience 4, Iter 83, disc loss: 0.03548287130160941, policy loss: 3.499026002415372
Experience 4, Iter 84, disc loss: 0.03250686386370368, policy loss: 3.621121689834173
Experience 4, Iter 85, disc loss: 0.035037513675974345, policy loss: 3.495173280049154
Experience 4, Iter 86, disc loss: 0.034691535121528366, policy loss: 3.4982388423513364
Experience 4, Iter 87, disc loss: 0.03303573553903393, policy loss: 3.5387368538822104
Experience 4, Iter 88, disc loss: 0.031955282602477965, policy loss: 3.589443047185094
Experience 4, Iter 89, disc loss: 0.032335912123471866, policy loss: 3.608483301904993
Experience 4, Iter 90, disc loss: 0.03365858832611074, policy loss: 3.5563447475961363
Experience 4, Iter 91, disc loss: 0.031714752581048604, policy loss: 3.586840819920436
Experience 4, Iter 92, disc loss: 0.03445164630942787, policy loss: 3.4859578607146586
Experience 4, Iter 93, disc loss: 0.03191206097237272, policy loss: 3.5864380754071723
Experience 4, Iter 94, disc loss: 0.03154045738153351, policy loss: 3.6215239868818436
Experience 4, Iter 95, disc loss: 0.030011546235995022, policy loss: 3.679188945828575
Experience 4, Iter 96, disc loss: 0.031286298982669526, policy loss: 3.6474318691580017
Experience 4, Iter 97, disc loss: 0.031760399057238396, policy loss: 3.591335412933109
Experience 4, Iter 98, disc loss: 0.03160234946200921, policy loss: 3.6071006738283424
Experience 4, Iter 99, disc loss: 0.03193799012598442, policy loss: 3.5944276570400646
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0069],
        [0.1618],
        [0.0037]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0124, 0.0611, 0.1713, 0.0058, 0.0003, 0.2463]],

        [[0.0124, 0.0611, 0.1713, 0.0058, 0.0003, 0.2463]],

        [[0.0124, 0.0611, 0.1713, 0.0058, 0.0003, 0.2463]],

        [[0.0124, 0.0611, 0.1713, 0.0058, 0.0003, 0.2463]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0063, 0.0275, 0.6473, 0.0149], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0063, 0.0275, 0.6473, 0.0149])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.686
Iter 2/2000 - Loss: -0.505
Iter 3/2000 - Loss: -0.755
Iter 4/2000 - Loss: -0.814
Iter 5/2000 - Loss: -0.742
Iter 6/2000 - Loss: -0.772
Iter 7/2000 - Loss: -0.894
Iter 8/2000 - Loss: -0.958
Iter 9/2000 - Loss: -0.917
Iter 10/2000 - Loss: -0.881
Iter 11/2000 - Loss: -0.939
Iter 12/2000 - Loss: -1.002
Iter 13/2000 - Loss: -1.005
Iter 14/2000 - Loss: -1.013
Iter 15/2000 - Loss: -1.051
Iter 16/2000 - Loss: -1.077
Iter 17/2000 - Loss: -1.088
Iter 18/2000 - Loss: -1.125
Iter 19/2000 - Loss: -1.186
Iter 20/2000 - Loss: -1.234
Iter 1981/2000 - Loss: -7.844
Iter 1982/2000 - Loss: -7.844
Iter 1983/2000 - Loss: -7.844
Iter 1984/2000 - Loss: -7.844
Iter 1985/2000 - Loss: -7.844
Iter 1986/2000 - Loss: -7.844
Iter 1987/2000 - Loss: -7.844
Iter 1988/2000 - Loss: -7.844
Iter 1989/2000 - Loss: -7.844
Iter 1990/2000 - Loss: -7.844
Iter 1991/2000 - Loss: -7.844
Iter 1992/2000 - Loss: -7.844
Iter 1993/2000 - Loss: -7.844
Iter 1994/2000 - Loss: -7.844
Iter 1995/2000 - Loss: -7.844
Iter 1996/2000 - Loss: -7.844
Iter 1997/2000 - Loss: -7.844
Iter 1998/2000 - Loss: -7.844
Iter 1999/2000 - Loss: -7.844
Iter 2000/2000 - Loss: -7.844
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[10.3839,  0.4345, 26.7314,  3.5618,  0.0562, 13.2066]],

        [[28.8905, 39.2112, 36.5258,  2.8738,  7.9297, 14.8201]],

        [[26.5039, 37.4102, 16.0138,  0.9209,  0.4616, 12.7207]],

        [[24.9823, 40.2329,  8.8116,  2.5676,  8.0111, 22.1096]]])
Signal Variance: tensor([4.6224e-03, 1.1599e+00, 6.4009e+00, 1.9072e-01])
Estimated target variance: tensor([0.0063, 0.0275, 0.6473, 0.0149])
N: 50
Signal to noise ratio: tensor([ 5.1850, 62.5455, 55.5570, 24.1513])
Bound on condition number: tensor([  1345.2006, 195597.8457, 154330.2645,  29165.3249])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.03580689760279967, policy loss: 3.439051325191074
Experience 5, Iter 1, disc loss: 0.032718495078281944, policy loss: 3.5597732628908174
Experience 5, Iter 2, disc loss: 0.029816047870490144, policy loss: 3.639098098490564
Experience 5, Iter 3, disc loss: 0.03141668288365815, policy loss: 3.6072523173039497
Experience 5, Iter 4, disc loss: 0.027480477925531464, policy loss: 3.722874296139824
Experience 5, Iter 5, disc loss: 0.029200918679901107, policy loss: 3.65289511702927
Experience 5, Iter 6, disc loss: 0.02995962101325006, policy loss: 3.6469761696340397
Experience 5, Iter 7, disc loss: 0.03019232964872716, policy loss: 3.629463307745264
Experience 5, Iter 8, disc loss: 0.029894563510537233, policy loss: 3.6403282593659427
Experience 5, Iter 9, disc loss: 0.030065285028137496, policy loss: 3.660052761428415
Experience 5, Iter 10, disc loss: 0.02964387937725843, policy loss: 3.6450928970641945
Experience 5, Iter 11, disc loss: 0.02784890166362532, policy loss: 3.7151257630804624
Experience 5, Iter 12, disc loss: 0.028699073764841647, policy loss: 3.682407466918919
Experience 5, Iter 13, disc loss: 0.028656569602784086, policy loss: 3.6817315908230293
Experience 5, Iter 14, disc loss: 0.028060668221512357, policy loss: 3.7111117545288197
Experience 5, Iter 15, disc loss: 0.02837568050924804, policy loss: 3.693809474127386
Experience 5, Iter 16, disc loss: 0.027597203582167263, policy loss: 3.7191167069608295
Experience 5, Iter 17, disc loss: 0.026920299383570774, policy loss: 3.7746680179219974
Experience 5, Iter 18, disc loss: 0.028511873424420443, policy loss: 3.6771992797255413
Experience 5, Iter 19, disc loss: 0.0277998577993877, policy loss: 3.7133668584148527
Experience 5, Iter 20, disc loss: 0.027402463950395594, policy loss: 3.721577134194669
Experience 5, Iter 21, disc loss: 0.02736452495179109, policy loss: 3.7385445281638017
Experience 5, Iter 22, disc loss: 0.02658858481075826, policy loss: 3.7682128401868518
Experience 5, Iter 23, disc loss: 0.02700933399336196, policy loss: 3.7584234667077
Experience 5, Iter 24, disc loss: 0.025991924699067134, policy loss: 3.805567940241444
Experience 5, Iter 25, disc loss: 0.025317769755774933, policy loss: 3.824026703237926
Experience 5, Iter 26, disc loss: 0.025151926508072055, policy loss: 3.8397873775047473
Experience 5, Iter 27, disc loss: 0.025426547280606168, policy loss: 3.837768911153726
Experience 5, Iter 28, disc loss: 0.02582306541713855, policy loss: 3.825697841880458
Experience 5, Iter 29, disc loss: 0.025796560906945314, policy loss: 3.819544995600814
Experience 5, Iter 30, disc loss: 0.024617383753508762, policy loss: 3.870058836275863
Experience 5, Iter 31, disc loss: 0.02364180806712039, policy loss: 3.9131745842898664
Experience 5, Iter 32, disc loss: 0.02382109869034111, policy loss: 3.913293867928239
Experience 5, Iter 33, disc loss: 0.02409911262674494, policy loss: 3.9177034301209326
Experience 5, Iter 34, disc loss: 0.024762664087578098, policy loss: 3.889294908616136
Experience 5, Iter 35, disc loss: 0.022857267348480544, policy loss: 3.990488038664479
Experience 5, Iter 36, disc loss: 0.024131441519147014, policy loss: 3.9280982179931643
Experience 5, Iter 37, disc loss: 0.022767989007057267, policy loss: 4.006604350323189
Experience 5, Iter 38, disc loss: 0.02315667624500444, policy loss: 3.9843471828593473
Experience 5, Iter 39, disc loss: 0.0231057282087903, policy loss: 3.9661534399131937
Experience 5, Iter 40, disc loss: 0.023463489321536142, policy loss: 3.954714988102331
Experience 5, Iter 41, disc loss: 0.023349811506379433, policy loss: 3.9537262267552613
Experience 5, Iter 42, disc loss: 0.025068847847129143, policy loss: 3.88115108131761
Experience 5, Iter 43, disc loss: 0.022445189447941345, policy loss: 4.058196700313046
Experience 5, Iter 44, disc loss: 0.023067840125180972, policy loss: 3.9993373090742628
Experience 5, Iter 45, disc loss: 0.024386088074956173, policy loss: 3.9053900801872428
Experience 5, Iter 46, disc loss: 0.023210631148136873, policy loss: 3.978062766108571
Experience 5, Iter 47, disc loss: 0.022051541379343002, policy loss: 4.062929682848473
Experience 5, Iter 48, disc loss: 0.022685583401355798, policy loss: 4.037800210474671
Experience 5, Iter 49, disc loss: 0.02238910075327306, policy loss: 4.05199185524778
Experience 5, Iter 50, disc loss: 0.0246042500893615, policy loss: 3.9138100760027648
Experience 5, Iter 51, disc loss: 0.023486728831400877, policy loss: 3.9685147147674753
Experience 5, Iter 52, disc loss: 0.023417889015032882, policy loss: 4.007432714486727
Experience 5, Iter 53, disc loss: 0.023748096303149314, policy loss: 3.9537202128224123
Experience 5, Iter 54, disc loss: 0.021926595742462696, policy loss: 4.074718381060735
Experience 5, Iter 55, disc loss: 0.023472483749253887, policy loss: 3.965123572015144
Experience 5, Iter 56, disc loss: 0.0224005154163536, policy loss: 4.068291822711463
Experience 5, Iter 57, disc loss: 0.02284538472397124, policy loss: 4.06817043628142
Experience 5, Iter 58, disc loss: 0.02469016862582883, policy loss: 3.9452834165187696
Experience 5, Iter 59, disc loss: 0.022903855960404, policy loss: 4.070420217937521
Experience 5, Iter 60, disc loss: 0.020595763775964206, policy loss: 4.2604989258606025
Experience 5, Iter 61, disc loss: 0.02298871966365095, policy loss: 4.029499920259259
Experience 5, Iter 62, disc loss: 0.022082643942369706, policy loss: 4.102831762482862
Experience 5, Iter 63, disc loss: 0.021365835511149987, policy loss: 4.16527678225525
Experience 5, Iter 64, disc loss: 0.0221579685302444, policy loss: 4.044180852705949
Experience 5, Iter 65, disc loss: 0.021957242240300698, policy loss: 4.075358670383556
Experience 5, Iter 66, disc loss: 0.02195917884472341, policy loss: 4.088041139022197
Experience 5, Iter 67, disc loss: 0.022586543447742875, policy loss: 4.088985785306697
Experience 5, Iter 68, disc loss: 0.02350173929638763, policy loss: 4.025081051143662
Experience 5, Iter 69, disc loss: 0.02433338342990899, policy loss: 3.9860480138926686
Experience 5, Iter 70, disc loss: 0.023818084611911212, policy loss: 4.070123775680484
Experience 5, Iter 71, disc loss: 0.023646506253314183, policy loss: 4.131857071797128
Experience 5, Iter 72, disc loss: 0.022552151023614762, policy loss: 4.173650651008574
Experience 5, Iter 73, disc loss: 0.024036872856959705, policy loss: 4.065581269490542
Experience 5, Iter 74, disc loss: 0.02337101825521661, policy loss: 4.108372241391279
Experience 5, Iter 75, disc loss: 0.023430565504923662, policy loss: 4.083786316595502
Experience 5, Iter 76, disc loss: 0.022490537810377718, policy loss: 4.224695452104107
Experience 5, Iter 77, disc loss: 0.02457132859801282, policy loss: 4.129715250085587
Experience 5, Iter 78, disc loss: 0.02536670357500734, policy loss: 4.088015812162926
Experience 5, Iter 79, disc loss: 0.024926603428585947, policy loss: 4.095009623995709
Experience 5, Iter 80, disc loss: 0.02370613322966609, policy loss: 4.190939155497588
Experience 5, Iter 81, disc loss: 0.02472818092877302, policy loss: 4.133206006118743
Experience 5, Iter 82, disc loss: 0.024837552040086117, policy loss: 4.20603224946327
Experience 5, Iter 83, disc loss: 0.026998667517540667, policy loss: 4.096422185665656
Experience 5, Iter 84, disc loss: 0.02622223032222899, policy loss: 4.196227706237599
Experience 5, Iter 85, disc loss: 0.024691851916453112, policy loss: 4.232537507926489
Experience 5, Iter 86, disc loss: 0.0250929384000826, policy loss: 4.2041564498984165
Experience 5, Iter 87, disc loss: 0.027264649414761345, policy loss: 4.138136705326339
Experience 5, Iter 88, disc loss: 0.02590342512386159, policy loss: 4.179564625586918
Experience 5, Iter 89, disc loss: 0.026561153682054757, policy loss: 4.288829648251981
Experience 5, Iter 90, disc loss: 0.0292362909131781, policy loss: 4.111405086997978
Experience 5, Iter 91, disc loss: 0.028126214674491206, policy loss: 4.120299137254845
Experience 5, Iter 92, disc loss: 0.030774910025942938, policy loss: 4.054930347559704
Experience 5, Iter 93, disc loss: 0.0320120754953496, policy loss: 3.9934010666833895
Experience 5, Iter 94, disc loss: 0.030185616206759816, policy loss: 4.154309766523401
Experience 5, Iter 95, disc loss: 0.03057918624460805, policy loss: 4.154158021553572
Experience 5, Iter 96, disc loss: 0.027959413396795703, policy loss: 4.301472349611137
Experience 5, Iter 97, disc loss: 0.03006501352851331, policy loss: 4.158409943191865
Experience 5, Iter 98, disc loss: 0.027464884935001016, policy loss: 4.263039392616814
Experience 5, Iter 99, disc loss: 0.03150480308957681, policy loss: 4.118967267687007
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0158],
        [0.3694],
        [0.0072]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0111, 0.0637, 0.3263, 0.0100, 0.0019, 0.4041]],

        [[0.0111, 0.0637, 0.3263, 0.0100, 0.0019, 0.4041]],

        [[0.0111, 0.0637, 0.3263, 0.0100, 0.0019, 0.4041]],

        [[0.0111, 0.0637, 0.3263, 0.0100, 0.0019, 0.4041]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0064, 0.0631, 1.4776, 0.0288], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0064, 0.0631, 1.4776, 0.0288])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.386
Iter 2/2000 - Loss: 0.502
Iter 3/2000 - Loss: 0.327
Iter 4/2000 - Loss: 0.252
Iter 5/2000 - Loss: 0.313
Iter 6/2000 - Loss: 0.290
Iter 7/2000 - Loss: 0.192
Iter 8/2000 - Loss: 0.164
Iter 9/2000 - Loss: 0.216
Iter 10/2000 - Loss: 0.210
Iter 11/2000 - Loss: 0.124
Iter 12/2000 - Loss: 0.079
Iter 13/2000 - Loss: 0.095
Iter 14/2000 - Loss: 0.079
Iter 15/2000 - Loss: 0.013
Iter 16/2000 - Loss: -0.050
Iter 17/2000 - Loss: -0.098
Iter 18/2000 - Loss: -0.162
Iter 19/2000 - Loss: -0.251
Iter 20/2000 - Loss: -0.349
Iter 1981/2000 - Loss: -7.511
Iter 1982/2000 - Loss: -7.511
Iter 1983/2000 - Loss: -7.511
Iter 1984/2000 - Loss: -7.511
Iter 1985/2000 - Loss: -7.511
Iter 1986/2000 - Loss: -7.511
Iter 1987/2000 - Loss: -7.511
Iter 1988/2000 - Loss: -7.511
Iter 1989/2000 - Loss: -7.511
Iter 1990/2000 - Loss: -7.511
Iter 1991/2000 - Loss: -7.511
Iter 1992/2000 - Loss: -7.511
Iter 1993/2000 - Loss: -7.511
Iter 1994/2000 - Loss: -7.511
Iter 1995/2000 - Loss: -7.511
Iter 1996/2000 - Loss: -7.511
Iter 1997/2000 - Loss: -7.511
Iter 1998/2000 - Loss: -7.511
Iter 1999/2000 - Loss: -7.511
Iter 2000/2000 - Loss: -7.511
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0022],
        [0.0004]])
Lengthscale: tensor([[[13.1625,  4.6853, 40.7987,  8.8750, 12.7891, 30.7306]],

        [[20.5221, 30.1981,  9.8206,  2.9290,  1.4880,  9.5556]],

        [[24.5901, 35.1887, 12.3756,  0.9719,  8.4407, 16.1390]],

        [[21.3281, 35.8258, 13.0404,  4.5839, 12.9941, 35.3976]]])
Signal Variance: tensor([0.0508, 1.0260, 9.6917, 0.4115])
Estimated target variance: tensor([0.0064, 0.0631, 1.4776, 0.0288])
N: 60
Signal to noise ratio: tensor([14.5252, 61.4912, 66.3350, 31.6412])
Bound on condition number: tensor([ 12659.9476, 226871.3030, 264021.2686,  60071.0077])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.037703310205810414, policy loss: 3.7213755110781928
Experience 6, Iter 1, disc loss: 0.0438883731836451, policy loss: 3.5674208589895238
Experience 6, Iter 2, disc loss: 0.04292657434647393, policy loss: 3.628493053732147
Experience 6, Iter 3, disc loss: 0.03976250671226819, policy loss: 3.7777364053421927
Experience 6, Iter 4, disc loss: 0.03664978063515671, policy loss: 3.898435596612521
Experience 6, Iter 5, disc loss: 0.037422837944349155, policy loss: 3.821477223009658
Experience 6, Iter 6, disc loss: 0.03572259761406423, policy loss: 3.9098186568846076
Experience 6, Iter 7, disc loss: 0.037165534509762434, policy loss: 3.8917339758544824
Experience 6, Iter 8, disc loss: 0.035322478902723276, policy loss: 4.009181554730223
Experience 6, Iter 9, disc loss: 0.033891266711581894, policy loss: 4.114963614421195
Experience 6, Iter 10, disc loss: 0.03614907248799368, policy loss: 4.0303857612757925
Experience 6, Iter 11, disc loss: 0.03741224211629553, policy loss: 3.867206280893262
Experience 6, Iter 12, disc loss: 0.033557133512656226, policy loss: 4.055301624022211
Experience 6, Iter 13, disc loss: 0.03224389789447982, policy loss: 4.090623167235568
Experience 6, Iter 14, disc loss: 0.03302177301692541, policy loss: 3.95461762029918
Experience 6, Iter 15, disc loss: 0.033730347613641215, policy loss: 3.9390787293985063
Experience 6, Iter 16, disc loss: 0.0329334758249489, policy loss: 3.913927483950456
Experience 6, Iter 17, disc loss: 0.03145238562990546, policy loss: 3.9373244106316783
Experience 6, Iter 18, disc loss: 0.031628389570325745, policy loss: 3.932616671703171
Experience 6, Iter 19, disc loss: 0.03196920824001401, policy loss: 3.888580061435363
Experience 6, Iter 20, disc loss: 0.03161566052839612, policy loss: 3.9326293073549046
Experience 6, Iter 21, disc loss: 0.02961924237522167, policy loss: 4.033733822893478
Experience 6, Iter 22, disc loss: 0.02943217277771608, policy loss: 4.002829315872297
Experience 6, Iter 23, disc loss: 0.03083257605496255, policy loss: 3.899916458950182
Experience 6, Iter 24, disc loss: 0.0297530532299997, policy loss: 3.935972456339795
Experience 6, Iter 25, disc loss: 0.02876779567968374, policy loss: 3.983902786336796
Experience 6, Iter 26, disc loss: 0.026856907694110872, policy loss: 4.110212478289625
Experience 6, Iter 27, disc loss: 0.028401240734815925, policy loss: 4.066760197769571
Experience 6, Iter 28, disc loss: 0.024830726751672326, policy loss: 4.311298285162789
Experience 6, Iter 29, disc loss: 0.026706004360349922, policy loss: 4.115108417256245
Experience 6, Iter 30, disc loss: 0.025881630069807156, policy loss: 4.182487398592004
Experience 6, Iter 31, disc loss: 0.025683080392047253, policy loss: 4.18581798384489
Experience 6, Iter 32, disc loss: 0.024655524395426447, policy loss: 4.252349135786305
Experience 6, Iter 33, disc loss: 0.02403391153196948, policy loss: 4.264407252773037
Experience 6, Iter 34, disc loss: 0.025499979878662794, policy loss: 4.229327222139956
Experience 6, Iter 35, disc loss: 0.024213278062554446, policy loss: 4.270752331201271
Experience 6, Iter 36, disc loss: 0.02462374334113562, policy loss: 4.1967105531343325
Experience 6, Iter 37, disc loss: 0.021940341412112707, policy loss: 4.427259991810608
Experience 6, Iter 38, disc loss: 0.02251925283688159, policy loss: 4.340779077228041
Experience 6, Iter 39, disc loss: 0.023188883037916733, policy loss: 4.310511658559543
Experience 6, Iter 40, disc loss: 0.022758180697506535, policy loss: 4.404553270838382
Experience 6, Iter 41, disc loss: 0.021362473736667293, policy loss: 4.387585842947602
Experience 6, Iter 42, disc loss: 0.02279025005580918, policy loss: 4.236234116109669
Experience 6, Iter 43, disc loss: 0.01935840674775311, policy loss: 4.514325596063202
Experience 6, Iter 44, disc loss: 0.020219270263547008, policy loss: 4.465552525238349
Experience 6, Iter 45, disc loss: 0.021106436815428203, policy loss: 4.34231084517085
Experience 6, Iter 46, disc loss: 0.018334823579261302, policy loss: 4.673507120289167
Experience 6, Iter 47, disc loss: 0.01975163619395441, policy loss: 4.414858109125877
Experience 6, Iter 48, disc loss: 0.018175758102515198, policy loss: 4.549613123761117
Experience 6, Iter 49, disc loss: 0.018035658774575677, policy loss: 4.611782649242405
Experience 6, Iter 50, disc loss: 0.01626747084737204, policy loss: 4.630527352590913
Experience 6, Iter 51, disc loss: 0.017023304946157683, policy loss: 4.566310362659681
Experience 6, Iter 52, disc loss: 0.016207552487612957, policy loss: 4.61547891675867
Experience 6, Iter 53, disc loss: 0.016355646971415866, policy loss: 4.607627731097915
Experience 6, Iter 54, disc loss: 0.01737078276100449, policy loss: 4.546235985775503
Experience 6, Iter 55, disc loss: 0.016232925384738167, policy loss: 4.68106401583295
Experience 6, Iter 56, disc loss: 0.01586727072351173, policy loss: 4.634883605103855
Experience 6, Iter 57, disc loss: 0.014475627327732775, policy loss: 4.7149134764143
Experience 6, Iter 58, disc loss: 0.015452336854198726, policy loss: 4.5794473050042415
Experience 6, Iter 59, disc loss: 0.013576056150855885, policy loss: 4.738269620575489
Experience 6, Iter 60, disc loss: 0.013417782229285632, policy loss: 4.756098210813882
Experience 6, Iter 61, disc loss: 0.014092602120847623, policy loss: 4.682604190478641
Experience 6, Iter 62, disc loss: 0.014488031225212448, policy loss: 4.597660451947554
Experience 6, Iter 63, disc loss: 0.014319687181595195, policy loss: 4.6080681567562385
Experience 6, Iter 64, disc loss: 0.013213786377982033, policy loss: 4.790460296217433
Experience 6, Iter 65, disc loss: 0.014218091702685984, policy loss: 4.6485271988713155
Experience 6, Iter 66, disc loss: 0.01383551645136069, policy loss: 4.665771132508195
Experience 6, Iter 67, disc loss: 0.01294293226931099, policy loss: 4.745455074805181
Experience 6, Iter 68, disc loss: 0.013383265729174084, policy loss: 4.665104236579179
Experience 6, Iter 69, disc loss: 0.012864087636502795, policy loss: 4.68611972279024
Experience 6, Iter 70, disc loss: 0.011194568851706648, policy loss: 4.887722724018014
Experience 6, Iter 71, disc loss: 0.011139206793826, policy loss: 4.939892091896601
Experience 6, Iter 72, disc loss: 0.012597771179635788, policy loss: 4.721333863644972
Experience 6, Iter 73, disc loss: 0.012872592669127651, policy loss: 4.676483020203042
Experience 6, Iter 74, disc loss: 0.01249254322187239, policy loss: 4.740982192680838
Experience 6, Iter 75, disc loss: 0.012239656837932312, policy loss: 4.697866000693464
Experience 6, Iter 76, disc loss: 0.01164175385029614, policy loss: 4.7686499347913305
Experience 6, Iter 77, disc loss: 0.011898280704804441, policy loss: 4.714229302225306
Experience 6, Iter 78, disc loss: 0.012313592197571251, policy loss: 4.682235793551152
Experience 6, Iter 79, disc loss: 0.011735921387356876, policy loss: 4.723139909621451
Experience 6, Iter 80, disc loss: 0.012271824446346532, policy loss: 4.703094996657183
Experience 6, Iter 81, disc loss: 0.011765988003674989, policy loss: 4.697186169658805
Experience 6, Iter 82, disc loss: 0.01092856829810879, policy loss: 4.837560501402462
Experience 6, Iter 83, disc loss: 0.011703991584635134, policy loss: 4.733730725753624
Experience 6, Iter 84, disc loss: 0.012250550423302125, policy loss: 4.622239540190069
Experience 6, Iter 85, disc loss: 0.011412922521670643, policy loss: 4.777889183360995
Experience 6, Iter 86, disc loss: 0.011733107754600678, policy loss: 4.692699326311038
Experience 6, Iter 87, disc loss: 0.01022492998561155, policy loss: 4.90442991727372
Experience 6, Iter 88, disc loss: 0.01027947670937403, policy loss: 4.889553551261565
Experience 6, Iter 89, disc loss: 0.01128122411846156, policy loss: 4.738056645815998
Experience 6, Iter 90, disc loss: 0.011479481360007239, policy loss: 4.713830248510515
Experience 6, Iter 91, disc loss: 0.009971076539413933, policy loss: 4.888395910129985
Experience 6, Iter 92, disc loss: 0.01106936333283965, policy loss: 4.698177206002489
Experience 6, Iter 93, disc loss: 0.01069687246433422, policy loss: 4.773739169167122
Experience 6, Iter 94, disc loss: 0.009025251723360495, policy loss: 4.953155993178417
Experience 6, Iter 95, disc loss: 0.010312742150519168, policy loss: 4.863182890555445
Experience 6, Iter 96, disc loss: 0.009200119468484971, policy loss: 4.921377932017538
Experience 6, Iter 97, disc loss: 0.009557057156482075, policy loss: 4.877112826072505
Experience 6, Iter 98, disc loss: 0.009051964459243764, policy loss: 4.951404093082736
Experience 6, Iter 99, disc loss: 0.009978705123666106, policy loss: 4.790500156864965
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0202],
        [0.3981],
        [0.0076]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0119, 0.0653, 0.3411, 0.0110, 0.0018, 0.5460]],

        [[0.0119, 0.0653, 0.3411, 0.0110, 0.0018, 0.5460]],

        [[0.0119, 0.0653, 0.3411, 0.0110, 0.0018, 0.5460]],

        [[0.0119, 0.0653, 0.3411, 0.0110, 0.0018, 0.5460]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0065, 0.0806, 1.5925, 0.0302], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0065, 0.0806, 1.5925, 0.0302])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.513
Iter 2/2000 - Loss: 0.780
Iter 3/2000 - Loss: 0.434
Iter 4/2000 - Loss: 0.447
Iter 5/2000 - Loss: 0.560
Iter 6/2000 - Loss: 0.474
Iter 7/2000 - Loss: 0.371
Iter 8/2000 - Loss: 0.375
Iter 9/2000 - Loss: 0.424
Iter 10/2000 - Loss: 0.408
Iter 11/2000 - Loss: 0.329
Iter 12/2000 - Loss: 0.265
Iter 13/2000 - Loss: 0.257
Iter 14/2000 - Loss: 0.259
Iter 15/2000 - Loss: 0.208
Iter 16/2000 - Loss: 0.113
Iter 17/2000 - Loss: 0.025
Iter 18/2000 - Loss: -0.036
Iter 19/2000 - Loss: -0.106
Iter 20/2000 - Loss: -0.217
Iter 1981/2000 - Loss: -7.605
Iter 1982/2000 - Loss: -7.605
Iter 1983/2000 - Loss: -7.605
Iter 1984/2000 - Loss: -7.605
Iter 1985/2000 - Loss: -7.605
Iter 1986/2000 - Loss: -7.605
Iter 1987/2000 - Loss: -7.605
Iter 1988/2000 - Loss: -7.605
Iter 1989/2000 - Loss: -7.605
Iter 1990/2000 - Loss: -7.605
Iter 1991/2000 - Loss: -7.605
Iter 1992/2000 - Loss: -7.605
Iter 1993/2000 - Loss: -7.605
Iter 1994/2000 - Loss: -7.605
Iter 1995/2000 - Loss: -7.605
Iter 1996/2000 - Loss: -7.605
Iter 1997/2000 - Loss: -7.605
Iter 1998/2000 - Loss: -7.605
Iter 1999/2000 - Loss: -7.605
Iter 2000/2000 - Loss: -7.606
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0022],
        [0.0004]])
Lengthscale: tensor([[[12.7884,  1.4051, 40.6180,  7.6581, 11.9205, 20.7325]],

        [[23.9310, 34.4982,  9.8947,  2.1894,  2.1245, 22.5736]],

        [[22.4662, 38.0137, 10.4139,  1.0128,  4.1397, 16.8544]],

        [[20.7189, 32.7900, 12.1044,  4.6187, 10.2894, 26.1051]]])
Signal Variance: tensor([0.0173, 1.6940, 9.5585, 0.3914])
Estimated target variance: tensor([0.0065, 0.0806, 1.5925, 0.0302])
N: 70
Signal to noise ratio: tensor([ 8.6911, 76.3212, 65.5400, 31.8479])
Bound on condition number: tensor([  5288.4333, 407745.7644, 300685.7422,  71001.4067])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.010875471203476674, policy loss: 4.7265340122827215
Experience 7, Iter 1, disc loss: 0.010143557738454653, policy loss: 4.777144521334901
Experience 7, Iter 2, disc loss: 0.010086345089147805, policy loss: 4.801533785244531
Experience 7, Iter 3, disc loss: 0.010422691819472078, policy loss: 4.7684173500267555
Experience 7, Iter 4, disc loss: 0.009674329415639414, policy loss: 4.847390254860125
Experience 7, Iter 5, disc loss: 0.009739320121335666, policy loss: 4.840178852705699
Experience 7, Iter 6, disc loss: 0.010412318812286683, policy loss: 4.729555710078973
Experience 7, Iter 7, disc loss: 0.009882885241765806, policy loss: 4.810193959481593
Experience 7, Iter 8, disc loss: 0.009858202783995027, policy loss: 4.823438362094391
Experience 7, Iter 9, disc loss: 0.009831598648629238, policy loss: 4.811401151855478
Experience 7, Iter 10, disc loss: 0.009765443826995096, policy loss: 4.809701635367187
Experience 7, Iter 11, disc loss: 0.009939237852244792, policy loss: 4.796241778109116
Experience 7, Iter 12, disc loss: 0.01074336460518488, policy loss: 4.727180388462888
Experience 7, Iter 13, disc loss: 0.009965360238135037, policy loss: 4.796792145873528
Experience 7, Iter 14, disc loss: 0.009427573503348646, policy loss: 4.845200792298524
Experience 7, Iter 15, disc loss: 0.009107370039229583, policy loss: 4.921625648835699
Experience 7, Iter 16, disc loss: 0.00893196853576274, policy loss: 4.927254428400715
Experience 7, Iter 17, disc loss: 0.008708472607677733, policy loss: 4.952756833479534
Experience 7, Iter 18, disc loss: 0.00878201738069642, policy loss: 4.941126173735055
Experience 7, Iter 19, disc loss: 0.008977648828788458, policy loss: 4.92790518976831
Experience 7, Iter 20, disc loss: 0.00856256592384627, policy loss: 4.95962861244641
Experience 7, Iter 21, disc loss: 0.00885516447953398, policy loss: 4.922781920353195
Experience 7, Iter 22, disc loss: 0.00889313584604317, policy loss: 4.919159123796659
Experience 7, Iter 23, disc loss: 0.008123620784723012, policy loss: 5.031241038725191
Experience 7, Iter 24, disc loss: 0.00825492077251754, policy loss: 5.026510152259097
Experience 7, Iter 25, disc loss: 0.00855889175470756, policy loss: 4.943797357461383
Experience 7, Iter 26, disc loss: 0.007872615737839109, policy loss: 5.075398123699782
Experience 7, Iter 27, disc loss: 0.00790426850014081, policy loss: 5.081611928788741
Experience 7, Iter 28, disc loss: 0.008151549954344039, policy loss: 5.012365462684019
Experience 7, Iter 29, disc loss: 0.00772307346023861, policy loss: 5.09362542728008
Experience 7, Iter 30, disc loss: 0.007731404111670777, policy loss: 5.074392081739618
Experience 7, Iter 31, disc loss: 0.007830036865362506, policy loss: 5.040288509372767
Experience 7, Iter 32, disc loss: 0.007408825610119011, policy loss: 5.129749749111536
Experience 7, Iter 33, disc loss: 0.007447240009993417, policy loss: 5.099224159508846
Experience 7, Iter 34, disc loss: 0.0074117247243904, policy loss: 5.121475408992116
Experience 7, Iter 35, disc loss: 0.007390321367090271, policy loss: 5.129722237501127
Experience 7, Iter 36, disc loss: 0.006946124897261251, policy loss: 5.19117542060612
Experience 7, Iter 37, disc loss: 0.007208517112328266, policy loss: 5.139137486546581
Experience 7, Iter 38, disc loss: 0.0069853298232020284, policy loss: 5.179261353972694
Experience 7, Iter 39, disc loss: 0.007261005859595501, policy loss: 5.153354427311818
Experience 7, Iter 40, disc loss: 0.006603435141552058, policy loss: 5.22919532271108
Experience 7, Iter 41, disc loss: 0.006844881234194368, policy loss: 5.185198324927689
Experience 7, Iter 42, disc loss: 0.006508466428919687, policy loss: 5.27311032848857
Experience 7, Iter 43, disc loss: 0.006279847667868772, policy loss: 5.2791338042279445
Experience 7, Iter 44, disc loss: 0.006421035742328546, policy loss: 5.250478123650531
Experience 7, Iter 45, disc loss: 0.006170338500702358, policy loss: 5.308610015466559
Experience 7, Iter 46, disc loss: 0.006462148626315842, policy loss: 5.245631898363925
Experience 7, Iter 47, disc loss: 0.006308010476467765, policy loss: 5.27205603518439
Experience 7, Iter 48, disc loss: 0.00600223868519513, policy loss: 5.336480472507748
Experience 7, Iter 49, disc loss: 0.006159739831901, policy loss: 5.312749784463856
Experience 7, Iter 50, disc loss: 0.006281905635357821, policy loss: 5.2556010622719525
Experience 7, Iter 51, disc loss: 0.005868371033070583, policy loss: 5.386761842423924
Experience 7, Iter 52, disc loss: 0.005708771738941166, policy loss: 5.409019949029405
Experience 7, Iter 53, disc loss: 0.0055523362174371696, policy loss: 5.416106523110402
Experience 7, Iter 54, disc loss: 0.005718670661383909, policy loss: 5.365884770207922
Experience 7, Iter 55, disc loss: 0.0058120550387147345, policy loss: 5.352885357075511
Experience 7, Iter 56, disc loss: 0.005644647186378363, policy loss: 5.3982177742706385
Experience 7, Iter 57, disc loss: 0.005896542799010973, policy loss: 5.3249043068893425
Experience 7, Iter 58, disc loss: 0.005906550196838192, policy loss: 5.325501276593732
Experience 7, Iter 59, disc loss: 0.005589902670654067, policy loss: 5.400010227280974
Experience 7, Iter 60, disc loss: 0.005582167055579993, policy loss: 5.389009664259351
Experience 7, Iter 61, disc loss: 0.005240657048344113, policy loss: 5.4720323870311045
Experience 7, Iter 62, disc loss: 0.005378505982427114, policy loss: 5.427482656821606
Experience 7, Iter 63, disc loss: 0.0052182502315635615, policy loss: 5.46880942904593
Experience 7, Iter 64, disc loss: 0.005596940647732954, policy loss: 5.385142487736447
Experience 7, Iter 65, disc loss: 0.005181467281981786, policy loss: 5.475424196868742
Experience 7, Iter 66, disc loss: 0.005073773274890053, policy loss: 5.500019792087949
Experience 7, Iter 67, disc loss: 0.0051512829656468785, policy loss: 5.47878204485006
Experience 7, Iter 68, disc loss: 0.005166879277924146, policy loss: 5.4944664987090786
Experience 7, Iter 69, disc loss: 0.005097543221439808, policy loss: 5.505730246629803
Experience 7, Iter 70, disc loss: 0.00517200410751767, policy loss: 5.443453947906381
Experience 7, Iter 71, disc loss: 0.004867757040127965, policy loss: 5.526889209062756
Experience 7, Iter 72, disc loss: 0.00513625791301152, policy loss: 5.457670253782297
Experience 7, Iter 73, disc loss: 0.005055513163995442, policy loss: 5.485703896361653
Experience 7, Iter 74, disc loss: 0.004371868202959057, policy loss: 5.675538606790733
Experience 7, Iter 75, disc loss: 0.004726068749900406, policy loss: 5.550202693250586
Experience 7, Iter 76, disc loss: 0.004799850796278599, policy loss: 5.563965009243466
Experience 7, Iter 77, disc loss: 0.0047801647105008655, policy loss: 5.53783961748147
Experience 7, Iter 78, disc loss: 0.004675843460447707, policy loss: 5.583357646337032
Experience 7, Iter 79, disc loss: 0.004718729755353958, policy loss: 5.548549943877776
Experience 7, Iter 80, disc loss: 0.0047874118903757385, policy loss: 5.520252456388677
Experience 7, Iter 81, disc loss: 0.004578218506390839, policy loss: 5.580847061575611
Experience 7, Iter 82, disc loss: 0.00442213106577725, policy loss: 5.626627317167275
Experience 7, Iter 83, disc loss: 0.0043538404150909055, policy loss: 5.667805175561879
Experience 7, Iter 84, disc loss: 0.004779330284769963, policy loss: 5.527080477738581
Experience 7, Iter 85, disc loss: 0.0046055978589095515, policy loss: 5.584889664551545
Experience 7, Iter 86, disc loss: 0.004235282704916296, policy loss: 5.670072639435238
Experience 7, Iter 87, disc loss: 0.004440552144355047, policy loss: 5.613592746109861
Experience 7, Iter 88, disc loss: 0.004412538623403291, policy loss: 5.629064040752479
Experience 7, Iter 89, disc loss: 0.004660950771561478, policy loss: 5.543138113544027
Experience 7, Iter 90, disc loss: 0.004567358411510283, policy loss: 5.597174435006588
Experience 7, Iter 91, disc loss: 0.004125974470570402, policy loss: 5.715697256176616
Experience 7, Iter 92, disc loss: 0.004025646422645746, policy loss: 5.766733356828982
Experience 7, Iter 93, disc loss: 0.004457273844665768, policy loss: 5.61742798349036
Experience 7, Iter 94, disc loss: 0.004247180892682327, policy loss: 5.666397693037363
Experience 7, Iter 95, disc loss: 0.004263613412754771, policy loss: 5.647120978233496
Experience 7, Iter 96, disc loss: 0.0042265362399775365, policy loss: 5.669271317594996
Experience 7, Iter 97, disc loss: 0.004446097942836521, policy loss: 5.609655230858861
Experience 7, Iter 98, disc loss: 0.0038610275675552506, policy loss: 5.793353134117488
Experience 7, Iter 99, disc loss: 0.00438815007196688, policy loss: 5.633085436796171
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0212],
        [0.3751],
        [0.0072]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0149, 0.0659, 0.3258, 0.0112, 0.0016, 0.6172]],

        [[0.0149, 0.0659, 0.3258, 0.0112, 0.0016, 0.6172]],

        [[0.0149, 0.0659, 0.3258, 0.0112, 0.0016, 0.6172]],

        [[0.0149, 0.0659, 0.3258, 0.0112, 0.0016, 0.6172]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0066, 0.0848, 1.5004, 0.0289], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0066, 0.0848, 1.5004, 0.0289])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.465
Iter 2/2000 - Loss: 0.839
Iter 3/2000 - Loss: 0.408
Iter 4/2000 - Loss: 0.455
Iter 5/2000 - Loss: 0.590
Iter 6/2000 - Loss: 0.490
Iter 7/2000 - Loss: 0.369
Iter 8/2000 - Loss: 0.360
Iter 9/2000 - Loss: 0.410
Iter 10/2000 - Loss: 0.416
Iter 11/2000 - Loss: 0.354
Iter 12/2000 - Loss: 0.279
Iter 13/2000 - Loss: 0.237
Iter 14/2000 - Loss: 0.225
Iter 15/2000 - Loss: 0.201
Iter 16/2000 - Loss: 0.135
Iter 17/2000 - Loss: 0.039
Iter 18/2000 - Loss: -0.053
Iter 19/2000 - Loss: -0.132
Iter 20/2000 - Loss: -0.220
Iter 1981/2000 - Loss: -7.821
Iter 1982/2000 - Loss: -7.821
Iter 1983/2000 - Loss: -7.821
Iter 1984/2000 - Loss: -7.821
Iter 1985/2000 - Loss: -7.821
Iter 1986/2000 - Loss: -7.821
Iter 1987/2000 - Loss: -7.821
Iter 1988/2000 - Loss: -7.821
Iter 1989/2000 - Loss: -7.821
Iter 1990/2000 - Loss: -7.821
Iter 1991/2000 - Loss: -7.821
Iter 1992/2000 - Loss: -7.821
Iter 1993/2000 - Loss: -7.821
Iter 1994/2000 - Loss: -7.821
Iter 1995/2000 - Loss: -7.821
Iter 1996/2000 - Loss: -7.821
Iter 1997/2000 - Loss: -7.821
Iter 1998/2000 - Loss: -7.821
Iter 1999/2000 - Loss: -7.821
Iter 2000/2000 - Loss: -7.821
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[14.4264,  1.5688, 38.8259,  6.9242, 11.3227, 23.9945]],

        [[23.5528, 32.7109,  9.6132,  1.9149,  2.2090, 20.3642]],

        [[24.8779, 35.4893, 10.1641,  1.0061,  4.2236, 17.7732]],

        [[22.9071, 33.5547, 12.8809,  4.0197,  2.6170, 26.1051]]])
Signal Variance: tensor([0.0196, 1.3864, 9.6224, 0.3915])
Estimated target variance: tensor([0.0066, 0.0848, 1.5004, 0.0289])
N: 80
Signal to noise ratio: tensor([ 9.2522, 68.1876, 66.3094, 34.2273])
Bound on condition number: tensor([  6849.3265, 371964.6240, 351755.5873,  93721.7416])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.00442893776945286, policy loss: 5.605293796319675
Experience 8, Iter 1, disc loss: 0.004246337464742171, policy loss: 5.65671045922519
Experience 8, Iter 2, disc loss: 0.004280200967441714, policy loss: 5.645496214482466
Experience 8, Iter 3, disc loss: 0.004250722157600265, policy loss: 5.6973785879906345
Experience 8, Iter 4, disc loss: 0.004154275365027176, policy loss: 5.697829093262446
Experience 8, Iter 5, disc loss: 0.004162807779935366, policy loss: 5.675868392502251
Experience 8, Iter 6, disc loss: 0.0040535756411542445, policy loss: 5.731172485933373
Experience 8, Iter 7, disc loss: 0.004156656485195434, policy loss: 5.690810474199448
Experience 8, Iter 8, disc loss: 0.004294064924510704, policy loss: 5.643946572042335
Experience 8, Iter 9, disc loss: 0.003996782832119991, policy loss: 5.748227663008711
Experience 8, Iter 10, disc loss: 0.004127166141615274, policy loss: 5.705380047425661
Experience 8, Iter 11, disc loss: 0.0040226805647654, policy loss: 5.7602600080128425
Experience 8, Iter 12, disc loss: 0.004203150990116505, policy loss: 5.684625766974774
Experience 8, Iter 13, disc loss: 0.003998796290687507, policy loss: 5.748548283442185
Experience 8, Iter 14, disc loss: 0.004032016997521559, policy loss: 5.7311552172067906
Experience 8, Iter 15, disc loss: 0.0042032267576496305, policy loss: 5.708012488346236
Experience 8, Iter 16, disc loss: 0.004397644361074166, policy loss: 5.6582621914965054
Experience 8, Iter 17, disc loss: 0.00414901327372293, policy loss: 5.761837668966786
Experience 8, Iter 18, disc loss: 0.0041653960061863796, policy loss: 5.761086902915302
Experience 8, Iter 19, disc loss: 0.0044311498397527115, policy loss: 5.680741374662013
Experience 8, Iter 20, disc loss: 0.004533420298768824, policy loss: 5.664814942553545
Experience 8, Iter 21, disc loss: 0.005048844575133477, policy loss: 5.521968154182961
Experience 8, Iter 22, disc loss: 0.004780298921662393, policy loss: 5.625352313883974
Experience 8, Iter 23, disc loss: 0.004886849930715207, policy loss: 5.576001545378431
Experience 8, Iter 24, disc loss: 0.005314556507354532, policy loss: 5.4848091510913966
Experience 8, Iter 25, disc loss: 0.005606565774867086, policy loss: 5.435775232492128
Experience 8, Iter 26, disc loss: 0.005380811174149295, policy loss: 5.568580363484508
Experience 8, Iter 27, disc loss: 0.0057657573602916115, policy loss: 5.477582610863242
Experience 8, Iter 28, disc loss: 0.005999786449728993, policy loss: 5.3860489317343525
Experience 8, Iter 29, disc loss: 0.005727999641036497, policy loss: 5.443075489405241
Experience 8, Iter 30, disc loss: 0.005803723272966286, policy loss: 5.51225649871405
Experience 8, Iter 31, disc loss: 0.006481539740604921, policy loss: 5.371516999530272
Experience 8, Iter 32, disc loss: 0.006487032529338909, policy loss: 5.42727473688151
Experience 8, Iter 33, disc loss: 0.006780874188432648, policy loss: 5.367596606028222
Experience 8, Iter 34, disc loss: 0.006529630443602627, policy loss: 5.4240156584326815
Experience 8, Iter 35, disc loss: 0.006941268944476516, policy loss: 5.415954181703491
Experience 8, Iter 36, disc loss: 0.00727586032493421, policy loss: 5.335914235935293
Experience 8, Iter 37, disc loss: 0.007166866258430817, policy loss: 5.384769193654843
Experience 8, Iter 38, disc loss: 0.006963747589537111, policy loss: 5.415461537070644
Experience 8, Iter 39, disc loss: 0.007205913060984882, policy loss: 5.408655963046427
Experience 8, Iter 40, disc loss: 0.006414017154627713, policy loss: 5.647155210613431
Experience 8, Iter 41, disc loss: 0.0068782969738745816, policy loss: 5.561283166750298
Experience 8, Iter 42, disc loss: 0.006465401339642133, policy loss: 5.679432950338017
Experience 8, Iter 43, disc loss: 0.00647309111741852, policy loss: 5.759732269936718
Experience 8, Iter 44, disc loss: 0.006527041659161404, policy loss: 5.70641621281139
Experience 8, Iter 45, disc loss: 0.006874810847719052, policy loss: 5.652460396257179
Experience 8, Iter 46, disc loss: 0.006699495536683737, policy loss: 5.731331325551171
Experience 8, Iter 47, disc loss: 0.006141147076059582, policy loss: 5.857072472336749
Experience 8, Iter 48, disc loss: 0.006890205132844462, policy loss: 5.671248924925816
Experience 8, Iter 49, disc loss: 0.006825559864337423, policy loss: 5.863108026143277
Experience 8, Iter 50, disc loss: 0.006950748151917077, policy loss: 5.654305064035075
Experience 8, Iter 51, disc loss: 0.006773315972137684, policy loss: 5.737465204444275
Experience 8, Iter 52, disc loss: 0.006626650093515089, policy loss: 5.709417469903608
Experience 8, Iter 53, disc loss: 0.006814789284146447, policy loss: 5.683972659805482
Experience 8, Iter 54, disc loss: 0.006351015559925878, policy loss: 5.785768245394849
Experience 8, Iter 55, disc loss: 0.006275106941967423, policy loss: 5.785635785163757
Experience 8, Iter 56, disc loss: 0.006263379147636741, policy loss: 5.761119113412845
Experience 8, Iter 57, disc loss: 0.0054484850075302085, policy loss: 6.104610655853097
Experience 8, Iter 58, disc loss: 0.0055759188287102645, policy loss: 5.894952816615121
Experience 8, Iter 59, disc loss: 0.005467119863734515, policy loss: 5.9175633560004
Experience 8, Iter 60, disc loss: 0.00597835642031201, policy loss: 5.775098181122971
Experience 8, Iter 61, disc loss: 0.005676973823427243, policy loss: 5.794939129105591
Experience 8, Iter 62, disc loss: 0.005926077243904187, policy loss: 5.816724853613
Experience 8, Iter 63, disc loss: 0.00655059951027111, policy loss: 5.689287458017935
Experience 8, Iter 64, disc loss: 0.006111845874630901, policy loss: 5.62740182541185
Experience 8, Iter 65, disc loss: 0.006120595009220827, policy loss: 5.7537461883040395
Experience 8, Iter 66, disc loss: 0.006604162521181966, policy loss: 5.555709382877161
Experience 8, Iter 67, disc loss: 0.005242820363085261, policy loss: 5.877229134285894
Experience 8, Iter 68, disc loss: 0.005510296601124266, policy loss: 5.88447104861477
Experience 8, Iter 69, disc loss: 0.005486009297756968, policy loss: 5.910750302336698
Experience 8, Iter 70, disc loss: 0.005972916828034899, policy loss: 5.820068377714344
Experience 8, Iter 71, disc loss: 0.004893860937136721, policy loss: 6.0655001476550705
Experience 8, Iter 72, disc loss: 0.005692345267376442, policy loss: 5.81806573164125
Experience 8, Iter 73, disc loss: 0.005564947186475295, policy loss: 5.853387330950124
Experience 8, Iter 74, disc loss: 0.005373425526461225, policy loss: 5.862525919741762
Experience 8, Iter 75, disc loss: 0.00534089017771099, policy loss: 5.929401559118361
Experience 8, Iter 76, disc loss: 0.00479302542852412, policy loss: 6.016481144502922
Experience 8, Iter 77, disc loss: 0.005147617157313744, policy loss: 6.003404173150645
Experience 8, Iter 78, disc loss: 0.004992711069866294, policy loss: 5.973568748774937
Experience 8, Iter 79, disc loss: 0.004649832094853121, policy loss: 6.119343277675345
Experience 8, Iter 80, disc loss: 0.004601267717160684, policy loss: 6.260736418470884
Experience 8, Iter 81, disc loss: 0.004680106228220526, policy loss: 6.101344537556386
Experience 8, Iter 82, disc loss: 0.004306396666777752, policy loss: 6.280464072042358
Experience 8, Iter 83, disc loss: 0.004637805176774368, policy loss: 6.110238278588784
Experience 8, Iter 84, disc loss: 0.0038722351707667375, policy loss: 6.359279813726137
Experience 8, Iter 85, disc loss: 0.00446507145493079, policy loss: 6.133671782273744
Experience 8, Iter 86, disc loss: 0.004511063590143886, policy loss: 6.141057385907656
Experience 8, Iter 87, disc loss: 0.0042554366961208775, policy loss: 6.213138131247289
Experience 8, Iter 88, disc loss: 0.00419713547381048, policy loss: 6.166916478217581
Experience 8, Iter 89, disc loss: 0.0040807151549882145, policy loss: 6.195418040545334
Experience 8, Iter 90, disc loss: 0.004385132091249194, policy loss: 6.104165334056436
Experience 8, Iter 91, disc loss: 0.003825925539582003, policy loss: 6.275144575795011
Experience 8, Iter 92, disc loss: 0.0037400378995891955, policy loss: 6.25099560874673
Experience 8, Iter 93, disc loss: 0.003989006344289106, policy loss: 6.1498035442655326
Experience 8, Iter 94, disc loss: 0.0040894449447711435, policy loss: 6.142128564045294
Experience 8, Iter 95, disc loss: 0.0034397147786537807, policy loss: 6.398868460466109
Experience 8, Iter 96, disc loss: 0.003979503420173407, policy loss: 6.152896026674625
Experience 8, Iter 97, disc loss: 0.003981657267384111, policy loss: 6.110941750391333
Experience 8, Iter 98, disc loss: 0.0037537805814581407, policy loss: 6.293875030777386
Experience 8, Iter 99, disc loss: 0.0035541080707139202, policy loss: 6.2702562771064745
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0418],
        [0.6111],
        [0.0100]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0137, 0.0707, 0.4687, 0.0138, 0.0037, 1.0613]],

        [[0.0137, 0.0707, 0.4687, 0.0138, 0.0037, 1.0613]],

        [[0.0137, 0.0707, 0.4687, 0.0138, 0.0037, 1.0613]],

        [[0.0137, 0.0707, 0.4687, 0.0138, 0.0037, 1.0613]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0068, 0.1672, 2.4445, 0.0401], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0068, 0.1672, 2.4445, 0.0401])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.221
Iter 2/2000 - Loss: 1.530
Iter 3/2000 - Loss: 1.163
Iter 4/2000 - Loss: 1.188
Iter 5/2000 - Loss: 1.313
Iter 6/2000 - Loss: 1.233
Iter 7/2000 - Loss: 1.120
Iter 8/2000 - Loss: 1.103
Iter 9/2000 - Loss: 1.145
Iter 10/2000 - Loss: 1.148
Iter 11/2000 - Loss: 1.088
Iter 12/2000 - Loss: 1.012
Iter 13/2000 - Loss: 0.965
Iter 14/2000 - Loss: 0.945
Iter 15/2000 - Loss: 0.910
Iter 16/2000 - Loss: 0.829
Iter 17/2000 - Loss: 0.716
Iter 18/2000 - Loss: 0.601
Iter 19/2000 - Loss: 0.493
Iter 20/2000 - Loss: 0.374
Iter 1981/2000 - Loss: -7.571
Iter 1982/2000 - Loss: -7.571
Iter 1983/2000 - Loss: -7.571
Iter 1984/2000 - Loss: -7.571
Iter 1985/2000 - Loss: -7.571
Iter 1986/2000 - Loss: -7.571
Iter 1987/2000 - Loss: -7.571
Iter 1988/2000 - Loss: -7.571
Iter 1989/2000 - Loss: -7.571
Iter 1990/2000 - Loss: -7.571
Iter 1991/2000 - Loss: -7.571
Iter 1992/2000 - Loss: -7.571
Iter 1993/2000 - Loss: -7.571
Iter 1994/2000 - Loss: -7.571
Iter 1995/2000 - Loss: -7.571
Iter 1996/2000 - Loss: -7.571
Iter 1997/2000 - Loss: -7.571
Iter 1998/2000 - Loss: -7.571
Iter 1999/2000 - Loss: -7.572
Iter 2000/2000 - Loss: -7.572
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.8073,  5.2559, 46.0929,  9.6798, 16.0885, 42.8470]],

        [[20.9628, 29.6252, 11.5386,  2.4046,  2.1442, 13.2146]],

        [[22.4209, 33.6532, 11.7344,  1.0615,  1.4551, 14.2577]],

        [[20.4820, 30.1538, 13.2278,  3.0656,  1.5304, 40.6274]]])
Signal Variance: tensor([ 0.0542,  1.6840, 12.6562,  0.3622])
Estimated target variance: tensor([0.0068, 0.1672, 2.4445, 0.0401])
N: 90
Signal to noise ratio: tensor([15.5818, 75.8656, 75.8183, 32.6997])
Bound on condition number: tensor([ 21852.3268, 518003.7596, 517358.7901,  96235.4398])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.004431056248853655, policy loss: 5.852917101055745
Experience 9, Iter 1, disc loss: 0.005128231560108445, policy loss: 5.686229805375552
Experience 9, Iter 2, disc loss: 0.006270883120370787, policy loss: 5.414406907426079
Experience 9, Iter 3, disc loss: 0.006727417544784648, policy loss: 5.589106531441604
Experience 9, Iter 4, disc loss: 0.009615051518586673, policy loss: 5.417066217367159
Experience 9, Iter 5, disc loss: 0.015502954311085962, policy loss: 5.204963974457527
Experience 9, Iter 6, disc loss: 0.007851579797509865, policy loss: 5.744885160941328
Experience 9, Iter 7, disc loss: 0.01506795232502173, policy loss: 5.391253858924694
Experience 9, Iter 8, disc loss: 0.016800302594336243, policy loss: 5.16065772507775
Experience 9, Iter 9, disc loss: 0.012325674059457281, policy loss: 5.48605165112131
Experience 9, Iter 10, disc loss: 0.0132211105843238, policy loss: 5.324538601126158
Experience 9, Iter 11, disc loss: 0.016149027346186662, policy loss: 5.1930824009650225
Experience 9, Iter 12, disc loss: 0.01489621171618033, policy loss: 5.5307087522054985
Experience 9, Iter 13, disc loss: 0.016373242972924515, policy loss: 5.5429179321192255
Experience 9, Iter 14, disc loss: 0.018402759894442833, policy loss: 5.297615199555184
Experience 9, Iter 15, disc loss: 0.020442277254911424, policy loss: 5.416282361321388
Experience 9, Iter 16, disc loss: 0.01953514940275491, policy loss: 5.437597037969658
Experience 9, Iter 17, disc loss: 0.019482003072565958, policy loss: 5.30739332229138
Experience 9, Iter 18, disc loss: 0.019873800055616862, policy loss: 4.991294364427086
Experience 9, Iter 19, disc loss: 0.019412840685587032, policy loss: 4.97793780032619
Experience 9, Iter 20, disc loss: 0.01973043339167165, policy loss: 5.0184337351970525
Experience 9, Iter 21, disc loss: 0.023111028837610962, policy loss: 4.696855830013208
Experience 9, Iter 22, disc loss: 0.022323143048197797, policy loss: 4.7902861655951
Experience 9, Iter 23, disc loss: 0.017800580135369583, policy loss: 5.042165236483632
Experience 9, Iter 24, disc loss: 0.02268000561002049, policy loss: 4.746146274745357
Experience 9, Iter 25, disc loss: 0.021783481287042022, policy loss: 4.892103595490688
Experience 9, Iter 26, disc loss: 0.027600069688297736, policy loss: 5.039723160519239
Experience 9, Iter 27, disc loss: 0.024337485492364527, policy loss: 4.9691759880471515
Experience 9, Iter 28, disc loss: 0.025163507519516282, policy loss: 5.150646562998279
Experience 9, Iter 29, disc loss: 0.020363810998860702, policy loss: 5.334666144512896
Experience 9, Iter 30, disc loss: 0.025469332432809254, policy loss: 5.259288557689976
Experience 9, Iter 31, disc loss: 0.022389382614493296, policy loss: 5.455306030403932
Experience 9, Iter 32, disc loss: 0.021557878603459202, policy loss: 5.445807546117635
Experience 9, Iter 33, disc loss: 0.02488666408210029, policy loss: 5.194124630769001
Experience 9, Iter 34, disc loss: 0.024037054395772806, policy loss: 5.0253748278916754
Experience 9, Iter 35, disc loss: 0.02606772831780576, policy loss: 5.149228732434844
Experience 9, Iter 36, disc loss: 0.020553271636278334, policy loss: 5.383438492463645
Experience 9, Iter 37, disc loss: 0.020848393465173413, policy loss: 5.277813898109173
Experience 9, Iter 38, disc loss: 0.027162566522481228, policy loss: 4.801774445092377
Experience 9, Iter 39, disc loss: 0.020873292530083697, policy loss: 5.227667259275645
Experience 9, Iter 40, disc loss: 0.018579676416090162, policy loss: 5.451534151356908
Experience 9, Iter 41, disc loss: 0.01998429499725423, policy loss: 5.5181454083954495
Experience 9, Iter 42, disc loss: 0.028614757340599737, policy loss: 5.390558915984065
Experience 9, Iter 43, disc loss: 0.019913500188290644, policy loss: 5.83463016179583
Experience 9, Iter 44, disc loss: 0.017790341012229482, policy loss: 5.700147198069066
Experience 9, Iter 45, disc loss: 0.030457457123966282, policy loss: 5.789601076763688
Experience 9, Iter 46, disc loss: 0.0210732699362072, policy loss: 5.915955917596795
Experience 9, Iter 47, disc loss: 0.027544398614095814, policy loss: 5.684025323039739
Experience 9, Iter 48, disc loss: 0.017825278905954603, policy loss: 6.042015107859974
Experience 9, Iter 49, disc loss: 0.021872028174085012, policy loss: 5.44367177809935
Experience 9, Iter 50, disc loss: 0.025852307816955203, policy loss: 5.986741510335659
Experience 9, Iter 51, disc loss: 0.016867291591884524, policy loss: 6.074707539625429
Experience 9, Iter 52, disc loss: 0.02095434838133041, policy loss: 5.628427743217081
Experience 9, Iter 53, disc loss: 0.017940180592662152, policy loss: 5.844309337756864
Experience 9, Iter 54, disc loss: 0.021601716906820004, policy loss: 5.804529625563184
Experience 9, Iter 55, disc loss: 0.014701670599959023, policy loss: 5.921638918376118
Experience 9, Iter 56, disc loss: 0.02568819577879626, policy loss: 5.5891180021837314
Experience 9, Iter 57, disc loss: 0.015764239847677105, policy loss: 5.793384409054189
Experience 9, Iter 58, disc loss: 0.013755828653920946, policy loss: 6.003552836261029
Experience 9, Iter 59, disc loss: 0.015076262087606988, policy loss: 5.993690165669172
Experience 9, Iter 60, disc loss: 0.01628988148362626, policy loss: 5.9946329513407175
Experience 9, Iter 61, disc loss: 0.015193039332558105, policy loss: 6.193505129532565
Experience 9, Iter 62, disc loss: 0.014783303505476672, policy loss: 6.182234038335978
Experience 9, Iter 63, disc loss: 0.01702450242554148, policy loss: 6.0844312495101045
Experience 9, Iter 64, disc loss: 0.017495167549997002, policy loss: 6.086566931204572
Experience 9, Iter 65, disc loss: 0.02882278623884041, policy loss: 6.000073527841094
Experience 9, Iter 66, disc loss: 0.013213704125511056, policy loss: 6.60259265015057
Experience 9, Iter 67, disc loss: 0.021938895560561028, policy loss: 6.025477071494171
Experience 9, Iter 68, disc loss: 0.01423715186360909, policy loss: 6.5181048159305
Experience 9, Iter 69, disc loss: 0.01510876517945257, policy loss: 6.495970037668637
Experience 9, Iter 70, disc loss: 0.01570732287620933, policy loss: 6.562314081681996
Experience 9, Iter 71, disc loss: 0.014720201066040188, policy loss: 6.590020931766527
Experience 9, Iter 72, disc loss: 0.01772601291402999, policy loss: 6.421013817236945
Experience 9, Iter 73, disc loss: 0.021481537625610222, policy loss: 6.374211005675555
Experience 9, Iter 74, disc loss: 0.014386517900727185, policy loss: 6.458308930099151
Experience 9, Iter 75, disc loss: 0.011824744556500304, policy loss: 6.486508350030771
Experience 9, Iter 76, disc loss: 0.0131057536713649, policy loss: 6.540011753340629
Experience 9, Iter 77, disc loss: 0.012403842219777535, policy loss: 6.578550435810163
Experience 9, Iter 78, disc loss: 0.018447010567171594, policy loss: 6.346102574897355
Experience 9, Iter 79, disc loss: 0.014643969413179853, policy loss: 6.2478326216478965
Experience 9, Iter 80, disc loss: 0.015379992106905112, policy loss: 6.54516099638157
Experience 9, Iter 81, disc loss: 0.01734151697228499, policy loss: 6.459846690694373
Experience 9, Iter 82, disc loss: 0.011927648434662905, policy loss: 6.442899377456374
Experience 9, Iter 83, disc loss: 0.014694846900849125, policy loss: 6.819177976997404
Experience 9, Iter 84, disc loss: 0.01288983103100174, policy loss: 6.818826599227691
Experience 9, Iter 85, disc loss: 0.017254677718857008, policy loss: 6.7418606069568625
Experience 9, Iter 86, disc loss: 0.012613404071124403, policy loss: 7.022302918661167
Experience 9, Iter 87, disc loss: 0.010298525537204263, policy loss: 7.077259076547472
Experience 9, Iter 88, disc loss: 0.00954268352639758, policy loss: 7.0594990689036825
Experience 9, Iter 89, disc loss: 0.008933925985831113, policy loss: 6.855211870358042
Experience 9, Iter 90, disc loss: 0.009981792185337878, policy loss: 6.644519498868312
Experience 9, Iter 91, disc loss: 0.009408447324055848, policy loss: 6.562782654962875
Experience 9, Iter 92, disc loss: 0.02566392520463818, policy loss: 6.469204898243557
Experience 9, Iter 93, disc loss: 0.009398280140316084, policy loss: 6.578752830729238
Experience 9, Iter 94, disc loss: 0.010524233700237352, policy loss: 6.610846634346332
Experience 9, Iter 95, disc loss: 0.013550853540241551, policy loss: 6.874863603176296
Experience 9, Iter 96, disc loss: 0.014321400922301598, policy loss: 6.514631808149988
Experience 9, Iter 97, disc loss: 0.01220922360030366, policy loss: 7.005606630892923
Experience 9, Iter 98, disc loss: 0.010400032043243919, policy loss: 7.273546425165668
Experience 9, Iter 99, disc loss: 0.009394634964960347, policy loss: 6.899206779224735
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0683],
        [0.7655],
        [0.0139]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0134, 0.0785, 0.6532, 0.0160, 0.0078, 1.7445]],

        [[0.0134, 0.0785, 0.6532, 0.0160, 0.0078, 1.7445]],

        [[0.0134, 0.0785, 0.6532, 0.0160, 0.0078, 1.7445]],

        [[0.0134, 0.0785, 0.6532, 0.0160, 0.0078, 1.7445]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0074, 0.2732, 3.0620, 0.0558], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0074, 0.2732, 3.0620, 0.0558])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.774
Iter 2/2000 - Loss: 2.045
Iter 3/2000 - Loss: 1.728
Iter 4/2000 - Loss: 1.739
Iter 5/2000 - Loss: 1.843
Iter 6/2000 - Loss: 1.791
Iter 7/2000 - Loss: 1.684
Iter 8/2000 - Loss: 1.643
Iter 9/2000 - Loss: 1.672
Iter 10/2000 - Loss: 1.684
Iter 11/2000 - Loss: 1.630
Iter 12/2000 - Loss: 1.545
Iter 13/2000 - Loss: 1.479
Iter 14/2000 - Loss: 1.444
Iter 15/2000 - Loss: 1.401
Iter 16/2000 - Loss: 1.311
Iter 17/2000 - Loss: 1.178
Iter 18/2000 - Loss: 1.028
Iter 19/2000 - Loss: 0.878
Iter 20/2000 - Loss: 0.717
Iter 1981/2000 - Loss: -7.455
Iter 1982/2000 - Loss: -7.455
Iter 1983/2000 - Loss: -7.455
Iter 1984/2000 - Loss: -7.455
Iter 1985/2000 - Loss: -7.456
Iter 1986/2000 - Loss: -7.456
Iter 1987/2000 - Loss: -7.456
Iter 1988/2000 - Loss: -7.456
Iter 1989/2000 - Loss: -7.456
Iter 1990/2000 - Loss: -7.456
Iter 1991/2000 - Loss: -7.456
Iter 1992/2000 - Loss: -7.456
Iter 1993/2000 - Loss: -7.456
Iter 1994/2000 - Loss: -7.456
Iter 1995/2000 - Loss: -7.456
Iter 1996/2000 - Loss: -7.456
Iter 1997/2000 - Loss: -7.456
Iter 1998/2000 - Loss: -7.456
Iter 1999/2000 - Loss: -7.456
Iter 2000/2000 - Loss: -7.456
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[14.6517,  4.3728, 41.5107,  7.2565, 13.2067, 38.6257]],

        [[20.3826, 33.2083, 12.1858,  1.4847,  5.7867, 15.0945]],

        [[20.3156, 31.7862, 13.1971,  1.1338,  1.2793, 14.6915]],

        [[18.0188, 28.2639, 13.3771,  3.3615,  1.5415, 43.2889]]])
Signal Variance: tensor([ 0.0427,  1.9463, 13.9011,  0.4418])
Estimated target variance: tensor([0.0074, 0.2732, 3.0620, 0.0558])
N: 100
Signal to noise ratio: tensor([13.7390, 79.5069, 79.2824, 37.0088])
Bound on condition number: tensor([ 18877.0782, 632136.3030, 628570.1797, 136965.9404])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.2517041345610489, policy loss: 2.342333716921674
Experience 10, Iter 1, disc loss: 0.19455983515494488, policy loss: 2.93935407012599
Experience 10, Iter 2, disc loss: 0.12714352163642817, policy loss: 3.8450422252650163
Experience 10, Iter 3, disc loss: 0.12739981361294064, policy loss: 4.289447080350353
Experience 10, Iter 4, disc loss: 0.1946533715316051, policy loss: 4.559102049813418
Experience 10, Iter 5, disc loss: 0.22724739682251135, policy loss: 4.791883540586506
Experience 10, Iter 6, disc loss: 0.19109380199352874, policy loss: 4.119672130117318
Experience 10, Iter 7, disc loss: 0.13281851196474928, policy loss: 3.5562776137706784
Experience 10, Iter 8, disc loss: 0.1168489851616396, policy loss: 2.887827392917548
Experience 10, Iter 9, disc loss: 0.13784151520769883, policy loss: 2.659593502914615
Experience 10, Iter 10, disc loss: 0.11002600552257921, policy loss: 2.8593230074357594
Experience 10, Iter 11, disc loss: 0.17227207411546316, policy loss: 2.4671863106261
Experience 10, Iter 12, disc loss: 0.13356794011884582, policy loss: 3.0765095746678313
Experience 10, Iter 13, disc loss: 0.1267607186695992, policy loss: 3.421317547911197
Experience 10, Iter 14, disc loss: 0.10095247255164064, policy loss: 3.673035634753811
Experience 10, Iter 15, disc loss: 0.09143244697269266, policy loss: 3.9885736689629194
Experience 10, Iter 16, disc loss: 0.09305657381015844, policy loss: 4.320139865776208
Experience 10, Iter 17, disc loss: 0.09831535809267218, policy loss: 4.548588410761751
Experience 10, Iter 18, disc loss: 0.10369845531217156, policy loss: 4.635088150938952
Experience 10, Iter 19, disc loss: 0.10109801372997795, policy loss: 4.3702647895514115
Experience 10, Iter 20, disc loss: 0.0879990438755826, policy loss: 4.346986808017501
Experience 10, Iter 21, disc loss: 0.07522640044701054, policy loss: 4.174744899406789
Experience 10, Iter 22, disc loss: 0.06256819808186195, policy loss: 4.153680284805626
Experience 10, Iter 23, disc loss: 0.05695641237611617, policy loss: 3.72273277724036
Experience 10, Iter 24, disc loss: 0.062408325812034045, policy loss: 3.592632142654309
Experience 10, Iter 25, disc loss: 0.055230935320159324, policy loss: 3.781404517899114
Experience 10, Iter 26, disc loss: 0.076550759099044, policy loss: 3.4568407394364917
Experience 10, Iter 27, disc loss: 0.06897673752481416, policy loss: 3.5434797961267077
Experience 10, Iter 28, disc loss: 0.05671786328776879, policy loss: 3.7508673804747388
Experience 10, Iter 29, disc loss: 0.06012355836848465, policy loss: 3.9206620516622914
Experience 10, Iter 30, disc loss: 0.043583464257272206, policy loss: 4.607457925053499
Experience 10, Iter 31, disc loss: 0.04536006294847393, policy loss: 4.586218859179505
Experience 10, Iter 32, disc loss: 0.03781759531446642, policy loss: 4.812678426324081
Experience 10, Iter 33, disc loss: 0.04122885384393378, policy loss: 4.825707961802657
Experience 10, Iter 34, disc loss: 0.03968188847021639, policy loss: 4.777502926599922
Experience 10, Iter 35, disc loss: 0.03824937487287929, policy loss: 5.068343390669343
Experience 10, Iter 36, disc loss: 0.03711568071411672, policy loss: 5.123760456692859
Experience 10, Iter 37, disc loss: 0.035105146444738285, policy loss: 5.044674219212114
Experience 10, Iter 38, disc loss: 0.033673229583972446, policy loss: 5.246509982661085
Experience 10, Iter 39, disc loss: 0.035120665765391364, policy loss: 4.7534786175890655
Experience 10, Iter 40, disc loss: 0.03263846916542877, policy loss: 4.866983856859662
Experience 10, Iter 41, disc loss: 0.030076636086000735, policy loss: 4.74850023417643
Experience 10, Iter 42, disc loss: 0.027310785363731372, policy loss: 4.993856634364928
Experience 10, Iter 43, disc loss: 0.030021621795214365, policy loss: 4.6598491665667305
Experience 10, Iter 44, disc loss: 0.02695977229956282, policy loss: 4.707058037775214
Experience 10, Iter 45, disc loss: 0.027369030140422128, policy loss: 4.564352847298499
Experience 10, Iter 46, disc loss: 0.022876425116468934, policy loss: 4.721057462118532
Experience 10, Iter 47, disc loss: 0.02735077443720381, policy loss: 4.644124747482909
Experience 10, Iter 48, disc loss: 0.02510997441899459, policy loss: 4.939531250348944
Experience 10, Iter 49, disc loss: 0.02275360879922067, policy loss: 4.844349750166852
Experience 10, Iter 50, disc loss: 0.02593920307616865, policy loss: 4.603878365884346
Experience 10, Iter 51, disc loss: 0.02290706766124028, policy loss: 4.801639984729913
Experience 10, Iter 52, disc loss: 0.023467114451474213, policy loss: 4.76366951333403
Experience 10, Iter 53, disc loss: 0.02240732065791512, policy loss: 4.901003807177364
Experience 10, Iter 54, disc loss: 0.020505554944009633, policy loss: 5.09841592644303
Experience 10, Iter 55, disc loss: 0.022123413757819704, policy loss: 4.986260670760501
Experience 10, Iter 56, disc loss: 0.01815031127888718, policy loss: 5.150442785010544
Experience 10, Iter 57, disc loss: 0.020103613772119572, policy loss: 5.08287422643607
Experience 10, Iter 58, disc loss: 0.021299760029770888, policy loss: 5.00414419254019
Experience 10, Iter 59, disc loss: 0.021980426669368093, policy loss: 5.0638199179924
Experience 10, Iter 60, disc loss: 0.01789387665121331, policy loss: 5.226118472584451
Experience 10, Iter 61, disc loss: 0.02049610523250998, policy loss: 5.091430739328813
Experience 10, Iter 62, disc loss: 0.01861700726336401, policy loss: 5.1596513904824155
Experience 10, Iter 63, disc loss: 0.018172528539547837, policy loss: 5.2033575857280105
Experience 10, Iter 64, disc loss: 0.019166928609924902, policy loss: 5.14807249308155
Experience 10, Iter 65, disc loss: 0.01712725038075385, policy loss: 5.385986505749141
Experience 10, Iter 66, disc loss: 0.019664872532687463, policy loss: 5.168694186955999
Experience 10, Iter 67, disc loss: 0.018799828435959676, policy loss: 5.178430301228642
Experience 10, Iter 68, disc loss: 0.01808893270417689, policy loss: 5.288910853386066
Experience 10, Iter 69, disc loss: 0.01686105944971834, policy loss: 5.374352603533982
Experience 10, Iter 70, disc loss: 0.01866449709226836, policy loss: 5.104497031682218
Experience 10, Iter 71, disc loss: 0.01786130399656935, policy loss: 5.2184758083534355
Experience 10, Iter 72, disc loss: 0.01715408877277033, policy loss: 5.111983528894364
Experience 10, Iter 73, disc loss: 0.01843039483467035, policy loss: 5.1595274097846255
Experience 10, Iter 74, disc loss: 0.01744243461335967, policy loss: 5.03991541683965
Experience 10, Iter 75, disc loss: 0.017149329854653235, policy loss: 5.0972517264326775
Experience 10, Iter 76, disc loss: 0.016236120860306444, policy loss: 5.200887133123039
Experience 10, Iter 77, disc loss: 0.016430127511073554, policy loss: 5.393664486644621
Experience 10, Iter 78, disc loss: 0.014286230402511126, policy loss: 5.614682471769734
Experience 10, Iter 79, disc loss: 0.015486652342154076, policy loss: 5.255770625251919
Experience 10, Iter 80, disc loss: 0.016355533559340864, policy loss: 5.226770299798039
Experience 10, Iter 81, disc loss: 0.014282351278128455, policy loss: 5.349343971612569
Experience 10, Iter 82, disc loss: 0.014390792109884941, policy loss: 5.394662634727492
Experience 10, Iter 83, disc loss: 0.015518737413719508, policy loss: 5.150505161922139
Experience 10, Iter 84, disc loss: 0.015010982122731727, policy loss: 5.295739450340731
Experience 10, Iter 85, disc loss: 0.01533524312055648, policy loss: 5.305038973836298
Experience 10, Iter 86, disc loss: 0.013811533623407757, policy loss: 5.331560677493349
Experience 10, Iter 87, disc loss: 0.014025612385057394, policy loss: 5.399252358553399
Experience 10, Iter 88, disc loss: 0.0144165417169363, policy loss: 5.290102326089318
Experience 10, Iter 89, disc loss: 0.013872054690629947, policy loss: 5.346860633946017
Experience 10, Iter 90, disc loss: 0.012518058758631979, policy loss: 5.450682211288937
Experience 10, Iter 91, disc loss: 0.014019050991821115, policy loss: 5.234185326121651
Experience 10, Iter 92, disc loss: 0.015032038385273264, policy loss: 5.056386523487911
Experience 10, Iter 93, disc loss: 0.015935254453522566, policy loss: 5.070660359107927
Experience 10, Iter 94, disc loss: 0.015596381158959744, policy loss: 5.194970484967756
Experience 10, Iter 95, disc loss: 0.014332351013131989, policy loss: 5.354638692458435
Experience 10, Iter 96, disc loss: 0.015160207102635066, policy loss: 5.212712320532371
Experience 10, Iter 97, disc loss: 0.014462676295666787, policy loss: 5.2247818819950425
Experience 10, Iter 98, disc loss: 0.015643396643469587, policy loss: 5.408950653291932
Experience 10, Iter 99, disc loss: 0.013361975490649117, policy loss: 5.401319646876994
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0020],
        [0.0901],
        [0.8859],
        [0.0182]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0135, 0.0861, 0.8242, 0.0179, 0.0119, 2.2736]],

        [[0.0135, 0.0861, 0.8242, 0.0179, 0.0119, 2.2736]],

        [[0.0135, 0.0861, 0.8242, 0.0179, 0.0119, 2.2736]],

        [[0.0135, 0.0861, 0.8242, 0.0179, 0.0119, 2.2736]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.3603, 3.5438, 0.0728], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.3603, 3.5438, 0.0728])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.164
Iter 2/2000 - Loss: 2.428
Iter 3/2000 - Loss: 2.114
Iter 4/2000 - Loss: 2.147
Iter 5/2000 - Loss: 2.235
Iter 6/2000 - Loss: 2.167
Iter 7/2000 - Loss: 2.069
Iter 8/2000 - Loss: 2.030
Iter 9/2000 - Loss: 2.042
Iter 10/2000 - Loss: 2.040
Iter 11/2000 - Loss: 1.984
Iter 12/2000 - Loss: 1.895
Iter 13/2000 - Loss: 1.810
Iter 14/2000 - Loss: 1.743
Iter 15/2000 - Loss: 1.671
Iter 16/2000 - Loss: 1.564
Iter 17/2000 - Loss: 1.413
Iter 18/2000 - Loss: 1.234
Iter 19/2000 - Loss: 1.044
Iter 20/2000 - Loss: 0.846
Iter 1981/2000 - Loss: -7.392
Iter 1982/2000 - Loss: -7.392
Iter 1983/2000 - Loss: -7.392
Iter 1984/2000 - Loss: -7.392
Iter 1985/2000 - Loss: -7.392
Iter 1986/2000 - Loss: -7.392
Iter 1987/2000 - Loss: -7.392
Iter 1988/2000 - Loss: -7.393
Iter 1989/2000 - Loss: -7.393
Iter 1990/2000 - Loss: -7.393
Iter 1991/2000 - Loss: -7.393
Iter 1992/2000 - Loss: -7.393
Iter 1993/2000 - Loss: -7.393
Iter 1994/2000 - Loss: -7.393
Iter 1995/2000 - Loss: -7.393
Iter 1996/2000 - Loss: -7.393
Iter 1997/2000 - Loss: -7.393
Iter 1998/2000 - Loss: -7.393
Iter 1999/2000 - Loss: -7.393
Iter 2000/2000 - Loss: -7.393
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[13.3104,  3.4053, 38.9760,  6.1102, 13.1645, 35.3436]],

        [[17.3876, 32.7960, 10.4151,  1.4102,  2.7619, 14.9618]],

        [[19.4710, 32.2452, 11.4498,  1.2524,  1.3404, 16.3537]],

        [[16.8371, 25.1969, 15.0416,  3.7182,  2.2100, 45.2778]]])
Signal Variance: tensor([ 0.0385,  1.5588, 17.9701,  0.6222])
Estimated target variance: tensor([0.0082, 0.3603, 3.5438, 0.0728])
N: 110
Signal to noise ratio: tensor([12.8697, 71.0024, 92.3800, 42.8239])
Bound on condition number: tensor([ 18220.2927, 554548.4774, 938747.8627, 201728.5055])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.025300358147513115, policy loss: 4.536675345498251
Experience 11, Iter 1, disc loss: 0.022530178010979868, policy loss: 4.776443913704578
Experience 11, Iter 2, disc loss: 0.020996468851663494, policy loss: 4.7887014332472875
Experience 11, Iter 3, disc loss: 0.022060997831907736, policy loss: 4.894932642151841
Experience 11, Iter 4, disc loss: 0.02382602983930178, policy loss: 4.8780206965737865
Experience 11, Iter 5, disc loss: 0.02279654193422674, policy loss: 5.067027694030837
Experience 11, Iter 6, disc loss: 0.023186632264126872, policy loss: 5.077539045409655
Experience 11, Iter 7, disc loss: 0.025252643947590796, policy loss: 4.955981982478276
Experience 11, Iter 8, disc loss: 0.023097657565522785, policy loss: 4.878584227495791
Experience 11, Iter 9, disc loss: 0.02392581281946531, policy loss: 4.97248696600281
Experience 11, Iter 10, disc loss: 0.024149721826312816, policy loss: 5.054610602961641
Experience 11, Iter 11, disc loss: 0.021840476573981783, policy loss: 5.53397519239855
Experience 11, Iter 12, disc loss: 0.02299273631796061, policy loss: 5.289731305522699
Experience 11, Iter 13, disc loss: 0.024888330297712428, policy loss: 5.427027338219057
Experience 11, Iter 14, disc loss: 0.023036221923705767, policy loss: 5.128167175737133
Experience 11, Iter 15, disc loss: 0.021434974443201023, policy loss: 5.348843136683244
Experience 11, Iter 16, disc loss: 0.021890785552056702, policy loss: 5.385554375854774
Experience 11, Iter 17, disc loss: 0.021707373110263592, policy loss: 5.471228328062537
Experience 11, Iter 18, disc loss: 0.020275177992053507, policy loss: 5.410275299175217
Experience 11, Iter 19, disc loss: 0.0198365450135573, policy loss: 5.454534062265319
Experience 11, Iter 20, disc loss: 0.02061370279103201, policy loss: 5.116596484531388
Experience 11, Iter 21, disc loss: 0.021731252892356407, policy loss: 4.841401318524691
Experience 11, Iter 22, disc loss: 0.021254022548950098, policy loss: 5.0682602973797435
Experience 11, Iter 23, disc loss: 0.019452354540916764, policy loss: 5.167960211616969
Experience 11, Iter 24, disc loss: 0.02013919833374605, policy loss: 4.954829642922961
Experience 11, Iter 25, disc loss: 0.01925017975863174, policy loss: 5.100396379948376
Experience 11, Iter 26, disc loss: 0.017509313850780507, policy loss: 5.140783743093421
Experience 11, Iter 27, disc loss: 0.01868055912988626, policy loss: 5.311377576068943
Experience 11, Iter 28, disc loss: 0.018509198490115113, policy loss: 5.252160472015843
Experience 11, Iter 29, disc loss: 0.019002734650141555, policy loss: 5.072317981690829
Experience 11, Iter 30, disc loss: 0.017535343900738166, policy loss: 5.357994847405264
Experience 11, Iter 31, disc loss: 0.017395960743220133, policy loss: 5.336070606221208
Experience 11, Iter 32, disc loss: 0.017101755928989194, policy loss: 5.396261293141283
Experience 11, Iter 33, disc loss: 0.01875944297416335, policy loss: 5.015480261137157
Experience 11, Iter 34, disc loss: 0.01613343450187816, policy loss: 5.502523993042694
Experience 11, Iter 35, disc loss: 0.016095787279380554, policy loss: 5.4876190512326275
Experience 11, Iter 36, disc loss: 0.015348697570364628, policy loss: 5.426158394620223
Experience 11, Iter 37, disc loss: 0.015537175725235745, policy loss: 5.321793041599895
Experience 11, Iter 38, disc loss: 0.015011765443300098, policy loss: 5.623108845989574
Experience 11, Iter 39, disc loss: 0.015927860473827733, policy loss: 5.242959220210206
Experience 11, Iter 40, disc loss: 0.015868665268733734, policy loss: 5.435019319793071
Experience 11, Iter 41, disc loss: 0.015295269644895873, policy loss: 5.575464561740032
Experience 11, Iter 42, disc loss: 0.016839324167094306, policy loss: 5.259078283618983
Experience 11, Iter 43, disc loss: 0.013563222214451082, policy loss: 5.799760013487716
Experience 11, Iter 44, disc loss: 0.013405237815463579, policy loss: 5.5306575294108935
Experience 11, Iter 45, disc loss: 0.014241696686729706, policy loss: 5.579632706976274
Experience 11, Iter 46, disc loss: 0.0143421539478291, policy loss: 5.59221624235572
Experience 11, Iter 47, disc loss: 0.013458366501094894, policy loss: 5.6444571688897405
Experience 11, Iter 48, disc loss: 0.014603409558727899, policy loss: 5.754243749175619
Experience 11, Iter 49, disc loss: 0.01205825719665966, policy loss: 5.782988591724797
Experience 11, Iter 50, disc loss: 0.012812784476222354, policy loss: 5.514365192862096
Experience 11, Iter 51, disc loss: 0.01315234423261445, policy loss: 5.435669307215759
Experience 11, Iter 52, disc loss: 0.0120943184486785, policy loss: 5.706801307989271
Experience 11, Iter 53, disc loss: 0.011655917313971548, policy loss: 5.819190517893975
Experience 11, Iter 54, disc loss: 0.013807823081370543, policy loss: 5.664385471353111
Experience 11, Iter 55, disc loss: 0.011286844757555156, policy loss: 5.801747559861783
Experience 11, Iter 56, disc loss: 0.012376137122663977, policy loss: 5.779915343875132
Experience 11, Iter 57, disc loss: 0.013317952164628557, policy loss: 5.840942130359473
Experience 11, Iter 58, disc loss: 0.010569206472916275, policy loss: 5.900075010432097
Experience 11, Iter 59, disc loss: 0.010592515440325271, policy loss: 6.121349758396008
Experience 11, Iter 60, disc loss: 0.01140201893349883, policy loss: 5.736357960471264
Experience 11, Iter 61, disc loss: 0.012378883143407262, policy loss: 5.558434126210987
Experience 11, Iter 62, disc loss: 0.010600244832738217, policy loss: 5.691227912864921
Experience 11, Iter 63, disc loss: 0.010611918905102121, policy loss: 5.849290429152386
Experience 11, Iter 64, disc loss: 0.010117043555003948, policy loss: 5.980335236914187
Experience 11, Iter 65, disc loss: 0.012711123792948075, policy loss: 5.884190490286418
Experience 11, Iter 66, disc loss: 0.011022415415726374, policy loss: 5.708865838483712
Experience 11, Iter 67, disc loss: 0.009816483306901871, policy loss: 5.857246440012099
Experience 11, Iter 68, disc loss: 0.011395009592823888, policy loss: 5.585961744952697
Experience 11, Iter 69, disc loss: 0.010418319412114985, policy loss: 6.201010598697844
Experience 11, Iter 70, disc loss: 0.012496620084740295, policy loss: 5.573188281155821
Experience 11, Iter 71, disc loss: 0.010430235247079256, policy loss: 5.678104871675316
Experience 11, Iter 72, disc loss: 0.010956906674004027, policy loss: 5.847632431251142
Experience 11, Iter 73, disc loss: 0.009455823616081507, policy loss: 6.016028893765485
Experience 11, Iter 74, disc loss: 0.009647654088348714, policy loss: 5.909417452302536
Experience 11, Iter 75, disc loss: 0.00961125277418353, policy loss: 6.009501112138413
Experience 11, Iter 76, disc loss: 0.00985671837489557, policy loss: 5.833666155748393
Experience 11, Iter 77, disc loss: 0.008932048670406935, policy loss: 5.993173501331994
Experience 11, Iter 78, disc loss: 0.009622807638082688, policy loss: 5.841646975143361
Experience 11, Iter 79, disc loss: 0.009075481069037172, policy loss: 5.95318977490056
Experience 11, Iter 80, disc loss: 0.009573779342275476, policy loss: 6.130657548126715
Experience 11, Iter 81, disc loss: 0.010369501399543995, policy loss: 5.9295922031712145
Experience 11, Iter 82, disc loss: 0.009039267647132747, policy loss: 5.970405762959912
Experience 11, Iter 83, disc loss: 0.009516285724032257, policy loss: 5.947952171910039
Experience 11, Iter 84, disc loss: 0.008964730166067202, policy loss: 6.055250817280244
Experience 11, Iter 85, disc loss: 0.009397769031380773, policy loss: 6.068642661574904
Experience 11, Iter 86, disc loss: 0.009042243414399737, policy loss: 6.074046118038092
Experience 11, Iter 87, disc loss: 0.009624932823184214, policy loss: 5.69431915934582
Experience 11, Iter 88, disc loss: 0.010847151919305032, policy loss: 5.510027529629304
Experience 11, Iter 89, disc loss: 0.01054000778697312, policy loss: 5.640962105140293
Experience 11, Iter 90, disc loss: 0.009007665139285548, policy loss: 6.029457418277753
Experience 11, Iter 91, disc loss: 0.011990659378797223, policy loss: 5.778808769908657
Experience 11, Iter 92, disc loss: 0.009096512932544869, policy loss: 6.170176074709326
Experience 11, Iter 93, disc loss: 0.008873223363404105, policy loss: 6.592304811731118
Experience 11, Iter 94, disc loss: 0.010440362609135095, policy loss: 6.0249048773981375
Experience 11, Iter 95, disc loss: 0.008125407132602255, policy loss: 6.376423564454137
Experience 11, Iter 96, disc loss: 0.009020286144454357, policy loss: 6.091550967216964
Experience 11, Iter 97, disc loss: 0.00862162773495978, policy loss: 6.227644215067815
Experience 11, Iter 98, disc loss: 0.008928445014121341, policy loss: 6.286882974214357
Experience 11, Iter 99, disc loss: 0.008518716500012744, policy loss: 6.169172772380557
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1159],
        [1.0709],
        [0.0238]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0131, 0.0999, 1.0526, 0.0194, 0.0156, 2.7799]],

        [[0.0131, 0.0999, 1.0526, 0.0194, 0.0156, 2.7799]],

        [[0.0131, 0.0999, 1.0526, 0.0194, 0.0156, 2.7799]],

        [[0.0131, 0.0999, 1.0526, 0.0194, 0.0156, 2.7799]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0093, 0.4637, 4.2835, 0.0953], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0093, 0.4637, 4.2835, 0.0953])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.594
Iter 2/2000 - Loss: 2.789
Iter 3/2000 - Loss: 2.519
Iter 4/2000 - Loss: 2.559
Iter 5/2000 - Loss: 2.629
Iter 6/2000 - Loss: 2.551
Iter 7/2000 - Loss: 2.458
Iter 8/2000 - Loss: 2.434
Iter 9/2000 - Loss: 2.449
Iter 10/2000 - Loss: 2.433
Iter 11/2000 - Loss: 2.360
Iter 12/2000 - Loss: 2.265
Iter 13/2000 - Loss: 2.186
Iter 14/2000 - Loss: 2.122
Iter 15/2000 - Loss: 2.042
Iter 16/2000 - Loss: 1.916
Iter 17/2000 - Loss: 1.749
Iter 18/2000 - Loss: 1.565
Iter 19/2000 - Loss: 1.374
Iter 20/2000 - Loss: 1.172
Iter 1981/2000 - Loss: -7.248
Iter 1982/2000 - Loss: -7.248
Iter 1983/2000 - Loss: -7.248
Iter 1984/2000 - Loss: -7.248
Iter 1985/2000 - Loss: -7.248
Iter 1986/2000 - Loss: -7.248
Iter 1987/2000 - Loss: -7.249
Iter 1988/2000 - Loss: -7.249
Iter 1989/2000 - Loss: -7.249
Iter 1990/2000 - Loss: -7.249
Iter 1991/2000 - Loss: -7.249
Iter 1992/2000 - Loss: -7.249
Iter 1993/2000 - Loss: -7.249
Iter 1994/2000 - Loss: -7.249
Iter 1995/2000 - Loss: -7.249
Iter 1996/2000 - Loss: -7.249
Iter 1997/2000 - Loss: -7.249
Iter 1998/2000 - Loss: -7.249
Iter 1999/2000 - Loss: -7.249
Iter 2000/2000 - Loss: -7.249
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.5572,  4.8798, 28.7078,  1.4907, 21.2691, 37.2288]],

        [[17.4242, 31.1741, 10.6175,  1.9586,  0.9287, 18.6056]],

        [[16.9864, 30.1099, 11.0228,  1.0281,  0.5992, 18.4004]],

        [[16.8497, 28.5803, 16.8510,  2.8829,  1.7046, 50.3188]]])
Signal Variance: tensor([ 0.0434,  2.0185, 12.4027,  0.5906])
Estimated target variance: tensor([0.0093, 0.4637, 4.2835, 0.0953])
N: 120
Signal to noise ratio: tensor([14.0438, 82.5060, 79.6926, 42.5866])
Bound on condition number: tensor([ 23668.3893, 816869.5281, 762110.1776, 217634.9475])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.00909032477637464, policy loss: 5.9248424245437885
Experience 12, Iter 1, disc loss: 0.010985582242823151, policy loss: 5.569691151840512
Experience 12, Iter 2, disc loss: 0.008766151977166521, policy loss: 6.07985506044761
Experience 12, Iter 3, disc loss: 0.008559885342109339, policy loss: 6.09873983616187
Experience 12, Iter 4, disc loss: 0.009293549277557234, policy loss: 6.220399342827038
Experience 12, Iter 5, disc loss: 0.009314886052895571, policy loss: 6.297928124193422
Experience 12, Iter 6, disc loss: 0.009160036140310808, policy loss: 5.967073980895771
Experience 12, Iter 7, disc loss: 0.00850604069076265, policy loss: 6.203596282132066
Experience 12, Iter 8, disc loss: 0.008978084628157423, policy loss: 6.037389905945086
Experience 12, Iter 9, disc loss: 0.00871686845362131, policy loss: 6.08887987121407
Experience 12, Iter 10, disc loss: 0.010480841630833484, policy loss: 5.9294635763510435
Experience 12, Iter 11, disc loss: 0.008921461228435543, policy loss: 6.006195261827438
Experience 12, Iter 12, disc loss: 0.010551142504594851, policy loss: 5.8747185715733234
Experience 12, Iter 13, disc loss: 0.009647863389747883, policy loss: 5.7667075916773385
Experience 12, Iter 14, disc loss: 0.009362227954771013, policy loss: 5.856213323375579
Experience 12, Iter 15, disc loss: 0.010963533434470225, policy loss: 5.6085287018544765
Experience 12, Iter 16, disc loss: 0.00907861005983552, policy loss: 6.092751758486428
Experience 12, Iter 17, disc loss: 0.009567457417183834, policy loss: 5.786661599118634
Experience 12, Iter 18, disc loss: 0.00948935607799076, policy loss: 5.860497994326449
Experience 12, Iter 19, disc loss: 0.01092838198767217, policy loss: 5.975954230011524
Experience 12, Iter 20, disc loss: 0.00965766650106094, policy loss: 5.83222303800893
Experience 12, Iter 21, disc loss: 0.009453805520668579, policy loss: 6.0285421343068375
Experience 12, Iter 22, disc loss: 0.009211818492802684, policy loss: 5.882950268824721
Experience 12, Iter 23, disc loss: 0.009215664937277337, policy loss: 6.294169229613359
Experience 12, Iter 24, disc loss: 0.010284056117821856, policy loss: 6.020597592019193
Experience 12, Iter 25, disc loss: 0.00959531094466955, policy loss: 5.918673554579675
Experience 12, Iter 26, disc loss: 0.009692514894081896, policy loss: 6.0000280146248794
Experience 12, Iter 27, disc loss: 0.008209833252499785, policy loss: 6.197513796774067
Experience 12, Iter 28, disc loss: 0.009135898946494525, policy loss: 6.158429321400209
Experience 12, Iter 29, disc loss: 0.009837047704951519, policy loss: 5.7966570861727424
Experience 12, Iter 30, disc loss: 0.008954553218256504, policy loss: 6.233542841981105
Experience 12, Iter 31, disc loss: 0.009767560953710439, policy loss: 6.004394138945919
Experience 12, Iter 32, disc loss: 0.009253501925702508, policy loss: 5.945013004279087
Experience 12, Iter 33, disc loss: 0.009264731427842683, policy loss: 5.792130810070272
Experience 12, Iter 34, disc loss: 0.00971107793541303, policy loss: 6.0101952072635285
Experience 12, Iter 35, disc loss: 0.009366954287755636, policy loss: 5.948134207646714
Experience 12, Iter 36, disc loss: 0.0101573487606691, policy loss: 5.730973598617884
Experience 12, Iter 37, disc loss: 0.008484349412035578, policy loss: 6.002114081343449
Experience 12, Iter 38, disc loss: 0.009205354285680721, policy loss: 5.946773182538973
Experience 12, Iter 39, disc loss: 0.008928924763428476, policy loss: 5.976738927348691
Experience 12, Iter 40, disc loss: 0.009096384747526335, policy loss: 6.342819473061868
Experience 12, Iter 41, disc loss: 0.010154720706636335, policy loss: 5.73197946106691
Experience 12, Iter 42, disc loss: 0.010011096994148622, policy loss: 6.165482229269901
Experience 12, Iter 43, disc loss: 0.009798378775482394, policy loss: 5.886636624268167
Experience 12, Iter 44, disc loss: 0.008182577148021616, policy loss: 6.091898804832285
Experience 12, Iter 45, disc loss: 0.007965410156868321, policy loss: 6.403167078952696
Experience 12, Iter 46, disc loss: 0.008782153208369385, policy loss: 6.177002797676249
Experience 12, Iter 47, disc loss: 0.009948038792463004, policy loss: 5.718143398053666
Experience 12, Iter 48, disc loss: 0.00908260061923926, policy loss: 6.121923384042538
Experience 12, Iter 49, disc loss: 0.009163143823706336, policy loss: 5.896675806979499
Experience 12, Iter 50, disc loss: 0.009884687361478671, policy loss: 5.965897071680688
Experience 12, Iter 51, disc loss: 0.009835140619258189, policy loss: 5.75080272410131
Experience 12, Iter 52, disc loss: 0.00775285395401469, policy loss: 6.682547901849108
Experience 12, Iter 53, disc loss: 0.009581833976873743, policy loss: 5.894208059943493
Experience 12, Iter 54, disc loss: 0.010236961901257918, policy loss: 6.132502346728785
Experience 12, Iter 55, disc loss: 0.008588244579543746, policy loss: 6.3706757643567204
Experience 12, Iter 56, disc loss: 0.007830286942339105, policy loss: 6.371271345435419
Experience 12, Iter 57, disc loss: 0.009527074656868516, policy loss: 6.054626410028548
Experience 12, Iter 58, disc loss: 0.008341400016711523, policy loss: 6.529363886024684
Experience 12, Iter 59, disc loss: 0.010052375907193212, policy loss: 5.825088354182249
Experience 12, Iter 60, disc loss: 0.008318195519483776, policy loss: 6.3657997344363135
Experience 12, Iter 61, disc loss: 0.009987583501428258, policy loss: 5.855941154998339
Experience 12, Iter 62, disc loss: 0.009111103703106554, policy loss: 6.2714980299146
Experience 12, Iter 63, disc loss: 0.008818906305598388, policy loss: 6.171588429283696
Experience 12, Iter 64, disc loss: 0.008183876399633968, policy loss: 6.059601723676787
Experience 12, Iter 65, disc loss: 0.009913231927437666, policy loss: 6.3573756305575415
Experience 12, Iter 66, disc loss: 0.008261706741435157, policy loss: 6.324606153438005
Experience 12, Iter 67, disc loss: 0.007818050980481057, policy loss: 6.500887842299511
Experience 12, Iter 68, disc loss: 0.007989518497488484, policy loss: 6.24252379196084
Experience 12, Iter 69, disc loss: 0.007930994421882071, policy loss: 6.0099269752533715
Experience 12, Iter 70, disc loss: 0.00804801056343241, policy loss: 6.519232534063459
Experience 12, Iter 71, disc loss: 0.008144219638459545, policy loss: 6.248267725576783
Experience 12, Iter 72, disc loss: 0.008359749873747387, policy loss: 6.37042508841984
Experience 12, Iter 73, disc loss: 0.007565800158789403, policy loss: 6.1079100865488645
Experience 12, Iter 74, disc loss: 0.00857847970456853, policy loss: 6.022511863157263
Experience 12, Iter 75, disc loss: 0.008369583386265544, policy loss: 6.099819566863048
Experience 12, Iter 76, disc loss: 0.008213165911028662, policy loss: 6.14241343964787
Experience 12, Iter 77, disc loss: 0.008892257163963989, policy loss: 6.046512695195148
Experience 12, Iter 78, disc loss: 0.007859044027228583, policy loss: 6.129017899580809
Experience 12, Iter 79, disc loss: 0.008370955719966992, policy loss: 6.285222939290504
Experience 12, Iter 80, disc loss: 0.008369369743104867, policy loss: 6.474550275458359
Experience 12, Iter 81, disc loss: 0.008016865284639163, policy loss: 5.960252543086371
Experience 12, Iter 82, disc loss: 0.008753276365773867, policy loss: 5.918711840915298
Experience 12, Iter 83, disc loss: 0.007224765733928535, policy loss: 6.370193522281612
Experience 12, Iter 84, disc loss: 0.00825019583163095, policy loss: 6.313881029617271
Experience 12, Iter 85, disc loss: 0.008110309500656703, policy loss: 6.120703130578558
Experience 12, Iter 86, disc loss: 0.007936100418978149, policy loss: 6.205083579270783
Experience 12, Iter 87, disc loss: 0.00745365341217003, policy loss: 6.234714179804856
Experience 12, Iter 88, disc loss: 0.006976436454325213, policy loss: 6.21647542952587
Experience 12, Iter 89, disc loss: 0.007830680965354456, policy loss: 6.087457951958378
Experience 12, Iter 90, disc loss: 0.006320060710026398, policy loss: 6.557712274650513
Experience 12, Iter 91, disc loss: 0.007789701729005391, policy loss: 5.930306910165913
Experience 12, Iter 92, disc loss: 0.007684637092286386, policy loss: 6.122519901446129
Experience 12, Iter 93, disc loss: 0.0082223314311221, policy loss: 6.503103735637214
Experience 12, Iter 94, disc loss: 0.00727408872863914, policy loss: 6.353515008737026
Experience 12, Iter 95, disc loss: 0.008329314296965006, policy loss: 5.936647879809582
Experience 12, Iter 96, disc loss: 0.007930531984344132, policy loss: 6.052519657484098
Experience 12, Iter 97, disc loss: 0.007463673393084168, policy loss: 6.5187552427638575
Experience 12, Iter 98, disc loss: 0.00906078143287601, policy loss: 5.953756943264981
Experience 12, Iter 99, disc loss: 0.006828831154271559, policy loss: 6.620429828177141
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1334],
        [1.1737],
        [0.0267]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0133, 0.1053, 1.1821, 0.0210, 0.0179, 3.1762]],

        [[0.0133, 0.1053, 1.1821, 0.0210, 0.0179, 3.1762]],

        [[0.0133, 0.1053, 1.1821, 0.0210, 0.0179, 3.1762]],

        [[0.0133, 0.1053, 1.1821, 0.0210, 0.0179, 3.1762]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0097, 0.5334, 4.6948, 0.1069], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0097, 0.5334, 4.6948, 0.1069])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.780
Iter 2/2000 - Loss: 2.973
Iter 3/2000 - Loss: 2.681
Iter 4/2000 - Loss: 2.731
Iter 5/2000 - Loss: 2.803
Iter 6/2000 - Loss: 2.712
Iter 7/2000 - Loss: 2.599
Iter 8/2000 - Loss: 2.558
Iter 9/2000 - Loss: 2.566
Iter 10/2000 - Loss: 2.544
Iter 11/2000 - Loss: 2.458
Iter 12/2000 - Loss: 2.339
Iter 13/2000 - Loss: 2.230
Iter 14/2000 - Loss: 2.137
Iter 15/2000 - Loss: 2.034
Iter 16/2000 - Loss: 1.888
Iter 17/2000 - Loss: 1.697
Iter 18/2000 - Loss: 1.480
Iter 19/2000 - Loss: 1.253
Iter 20/2000 - Loss: 1.015
Iter 1981/2000 - Loss: -7.341
Iter 1982/2000 - Loss: -7.341
Iter 1983/2000 - Loss: -7.341
Iter 1984/2000 - Loss: -7.341
Iter 1985/2000 - Loss: -7.341
Iter 1986/2000 - Loss: -7.341
Iter 1987/2000 - Loss: -7.341
Iter 1988/2000 - Loss: -7.341
Iter 1989/2000 - Loss: -7.341
Iter 1990/2000 - Loss: -7.341
Iter 1991/2000 - Loss: -7.341
Iter 1992/2000 - Loss: -7.341
Iter 1993/2000 - Loss: -7.341
Iter 1994/2000 - Loss: -7.341
Iter 1995/2000 - Loss: -7.341
Iter 1996/2000 - Loss: -7.341
Iter 1997/2000 - Loss: -7.341
Iter 1998/2000 - Loss: -7.341
Iter 1999/2000 - Loss: -7.341
Iter 2000/2000 - Loss: -7.341
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.5826,  5.5937, 20.2161,  1.5064, 20.5439, 43.9868]],

        [[16.5083, 30.3914, 10.0062,  1.2677,  1.1554, 30.9019]],

        [[18.3061, 34.0191,  9.8491,  1.0888,  0.6831, 19.3283]],

        [[16.5280, 27.4743, 18.1325,  2.8512,  1.7232, 50.6117]]])
Signal Variance: tensor([ 0.0532,  2.6270, 12.6268,  0.6102])
Estimated target variance: tensor([0.0097, 0.5334, 4.6948, 0.1069])
N: 130
Signal to noise ratio: tensor([15.8068, 90.3489, 79.4978, 44.1550])
Bound on condition number: tensor([  32482.1886, 1061181.2180,  821587.8325,  253456.9523])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0076918190256805696, policy loss: 6.1765808226160726
Experience 13, Iter 1, disc loss: 0.007082239378103719, policy loss: 6.727127518724799
Experience 13, Iter 2, disc loss: 0.008968282626007467, policy loss: 6.28815703346681
Experience 13, Iter 3, disc loss: 0.007381927474243157, policy loss: 6.252794999975578
Experience 13, Iter 4, disc loss: 0.00797567384779033, policy loss: 6.562882665437768
Experience 13, Iter 5, disc loss: 0.007667641040395565, policy loss: 6.289207731459915
Experience 13, Iter 6, disc loss: 0.0073222222378586865, policy loss: 6.511595371790815
Experience 13, Iter 7, disc loss: 0.007968780688344251, policy loss: 6.23230281890042
Experience 13, Iter 8, disc loss: 0.006725143847425367, policy loss: 6.49262360566327
Experience 13, Iter 9, disc loss: 0.006289679059134743, policy loss: 6.620143351362456
Experience 13, Iter 10, disc loss: 0.007161276070114237, policy loss: 6.26505789662197
Experience 13, Iter 11, disc loss: 0.007175757681636932, policy loss: 6.494868772325459
Experience 13, Iter 12, disc loss: 0.007147734250694078, policy loss: 6.252895384584517
Experience 13, Iter 13, disc loss: 0.007365997539785504, policy loss: 6.613808078692445
Experience 13, Iter 14, disc loss: 0.007111918788674888, policy loss: 6.603391013248107
Experience 13, Iter 15, disc loss: 0.006955176781189033, policy loss: 6.482633553894992
Experience 13, Iter 16, disc loss: 0.00702379261112677, policy loss: 6.455555547635807
Experience 13, Iter 17, disc loss: 0.006564332824855605, policy loss: 6.361698691007665
Experience 13, Iter 18, disc loss: 0.006192331128164593, policy loss: 6.755483615169449
Experience 13, Iter 19, disc loss: 0.006180959463892072, policy loss: 6.627479759727524
Experience 13, Iter 20, disc loss: 0.008000183848802906, policy loss: 6.08449556608816
Experience 13, Iter 21, disc loss: 0.007128234472648446, policy loss: 6.319061837806453
Experience 13, Iter 22, disc loss: 0.0072522908896270576, policy loss: 6.226743881396867
Experience 13, Iter 23, disc loss: 0.005828397622485885, policy loss: 6.755482084782248
Experience 13, Iter 24, disc loss: 0.007649610614480234, policy loss: 6.23973340275316
Experience 13, Iter 25, disc loss: 0.006759291017143862, policy loss: 6.4406781907563495
Experience 13, Iter 26, disc loss: 0.007226643723868501, policy loss: 6.060867574381815
Experience 13, Iter 27, disc loss: 0.0068325068860809815, policy loss: 6.556957548024605
Experience 13, Iter 28, disc loss: 0.005569932021419457, policy loss: 6.692459002716101
Experience 13, Iter 29, disc loss: 0.0060157232285588695, policy loss: 6.3863187319250345
Experience 13, Iter 30, disc loss: 0.005532024006267913, policy loss: 7.078942453809973
Experience 13, Iter 31, disc loss: 0.0064771187324260335, policy loss: 6.28059173054568
Experience 13, Iter 32, disc loss: 0.00629579733206718, policy loss: 6.866030261993595
Experience 13, Iter 33, disc loss: 0.00748069851176982, policy loss: 6.2427392060658375
Experience 13, Iter 34, disc loss: 0.005513862472451513, policy loss: 6.7543348877437435
Experience 13, Iter 35, disc loss: 0.007041231313212234, policy loss: 6.555991683523845
Experience 13, Iter 36, disc loss: 0.005719889195006407, policy loss: 6.77561308503374
Experience 13, Iter 37, disc loss: 0.006428828628581022, policy loss: 6.354648991276328
Experience 13, Iter 38, disc loss: 0.006063838722496379, policy loss: 6.776306417603625
Experience 13, Iter 39, disc loss: 0.0064663923533988775, policy loss: 6.208093688604479
Experience 13, Iter 40, disc loss: 0.006427813019390697, policy loss: 6.491627776280924
Experience 13, Iter 41, disc loss: 0.006415462605123488, policy loss: 6.194659269020117
Experience 13, Iter 42, disc loss: 0.006446988314940661, policy loss: 6.614599219768817
Experience 13, Iter 43, disc loss: 0.006256332082797772, policy loss: 6.506877588082177
Experience 13, Iter 44, disc loss: 0.008579271204212697, policy loss: 6.262407857427011
Experience 13, Iter 45, disc loss: 0.006042867648944716, policy loss: 6.557240207831449
Experience 13, Iter 46, disc loss: 0.006330355850679285, policy loss: 6.372874165593285
Experience 13, Iter 47, disc loss: 0.006889687240031557, policy loss: 6.579336296689519
Experience 13, Iter 48, disc loss: 0.005801908928696668, policy loss: 6.876545556868855
Experience 13, Iter 49, disc loss: 0.007230232330518275, policy loss: 6.44779277267925
Experience 13, Iter 50, disc loss: 0.0053688703140820325, policy loss: 6.84859157362216
Experience 13, Iter 51, disc loss: 0.005735412336112645, policy loss: 6.658331746316369
Experience 13, Iter 52, disc loss: 0.006096848661951507, policy loss: 6.543466951792472
Experience 13, Iter 53, disc loss: 0.006601436982712992, policy loss: 6.477704032606098
Experience 13, Iter 54, disc loss: 0.006204067525498595, policy loss: 6.632941297358498
Experience 13, Iter 55, disc loss: 0.006390772939225183, policy loss: 6.42207605801652
Experience 13, Iter 56, disc loss: 0.005344874915789841, policy loss: 6.769159840326271
Experience 13, Iter 57, disc loss: 0.006456459961279226, policy loss: 6.512923145791371
Experience 13, Iter 58, disc loss: 0.0064551484912036845, policy loss: 6.434215245817376
Experience 13, Iter 59, disc loss: 0.006241652779995253, policy loss: 6.529767851875876
Experience 13, Iter 60, disc loss: 0.006885550484715161, policy loss: 6.6396242073051255
Experience 13, Iter 61, disc loss: 0.006602961582834102, policy loss: 6.69764359498978
Experience 13, Iter 62, disc loss: 0.006467671692365658, policy loss: 6.678080216450962
Experience 13, Iter 63, disc loss: 0.006187834275101747, policy loss: 6.351930886607405
Experience 13, Iter 64, disc loss: 0.007082497350635388, policy loss: 6.231424180325535
Experience 13, Iter 65, disc loss: 0.006182995910045629, policy loss: 6.654662923525873
Experience 13, Iter 66, disc loss: 0.005900806250399486, policy loss: 7.013689323839756
Experience 13, Iter 67, disc loss: 0.006131806432904599, policy loss: 6.549474582501011
Experience 13, Iter 68, disc loss: 0.006778103063758945, policy loss: 6.682160122551172
Experience 13, Iter 69, disc loss: 0.00647630182186033, policy loss: 6.524544864471126
Experience 13, Iter 70, disc loss: 0.006474865626165534, policy loss: 7.001128876946202
Experience 13, Iter 71, disc loss: 0.006121672701519561, policy loss: 6.526317350469558
Experience 13, Iter 72, disc loss: 0.0059069027120434246, policy loss: 6.903787596673862
Experience 13, Iter 73, disc loss: 0.006156994999671275, policy loss: 6.60624810366442
Experience 13, Iter 74, disc loss: 0.006620777387885078, policy loss: 6.44511421321082
Experience 13, Iter 75, disc loss: 0.00627382018379151, policy loss: 7.003158697188914
Experience 13, Iter 76, disc loss: 0.006093877994253427, policy loss: 6.598717576375778
Experience 13, Iter 77, disc loss: 0.0056199338011619065, policy loss: 6.741138787337965
Experience 13, Iter 78, disc loss: 0.006345041049340619, policy loss: 6.43222750681892
Experience 13, Iter 79, disc loss: 0.005933621402665512, policy loss: 6.394247372467043
Experience 13, Iter 80, disc loss: 0.006258747193647498, policy loss: 6.641159479117319
Experience 13, Iter 81, disc loss: 0.005851409447327017, policy loss: 6.945418516043772
Experience 13, Iter 82, disc loss: 0.005678469966923889, policy loss: 6.827052933846433
Experience 13, Iter 83, disc loss: 0.005166728593742851, policy loss: 6.757602426209624
Experience 13, Iter 84, disc loss: 0.005666175188971677, policy loss: 6.644190277908557
Experience 13, Iter 85, disc loss: 0.006211109625641706, policy loss: 6.766366155448916
Experience 13, Iter 86, disc loss: 0.006153278040056151, policy loss: 6.723155616312214
Experience 13, Iter 87, disc loss: 0.0056936873282290214, policy loss: 6.8663115191856665
Experience 13, Iter 88, disc loss: 0.005963571709186557, policy loss: 6.350705399175078
Experience 13, Iter 89, disc loss: 0.006202309297141141, policy loss: 6.785255730141023
Experience 13, Iter 90, disc loss: 0.005329768237863561, policy loss: 6.837984733876364
Experience 13, Iter 91, disc loss: 0.005415293545728766, policy loss: 6.427168919946251
Experience 13, Iter 92, disc loss: 0.005946044011959239, policy loss: 6.430513896274583
Experience 13, Iter 93, disc loss: 0.005251166612820188, policy loss: 6.880311093169049
Experience 13, Iter 94, disc loss: 0.005723282549160485, policy loss: 6.458811900085031
Experience 13, Iter 95, disc loss: 0.006895371826514174, policy loss: 6.3191454103000435
Experience 13, Iter 96, disc loss: 0.0057497983875333105, policy loss: 6.606295334852424
Experience 13, Iter 97, disc loss: 0.005358440839490999, policy loss: 6.684102632714509
Experience 13, Iter 98, disc loss: 0.0059495587607471204, policy loss: 6.640482024272293
Experience 13, Iter 99, disc loss: 0.005943306568836127, policy loss: 7.052526068227238
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1491],
        [1.2718],
        [0.0294]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0131, 0.1098, 1.2994, 0.0223, 0.0197, 3.5041]],

        [[0.0131, 0.1098, 1.2994, 0.0223, 0.0197, 3.5041]],

        [[0.0131, 0.1098, 1.2994, 0.0223, 0.0197, 3.5041]],

        [[0.0131, 0.1098, 1.2994, 0.0223, 0.0197, 3.5041]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0100, 0.5966, 5.0873, 0.1177], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0100, 0.5966, 5.0873, 0.1177])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.934
Iter 2/2000 - Loss: 3.115
Iter 3/2000 - Loss: 2.813
Iter 4/2000 - Loss: 2.860
Iter 5/2000 - Loss: 2.931
Iter 6/2000 - Loss: 2.832
Iter 7/2000 - Loss: 2.704
Iter 8/2000 - Loss: 2.646
Iter 9/2000 - Loss: 2.638
Iter 10/2000 - Loss: 2.603
Iter 11/2000 - Loss: 2.501
Iter 12/2000 - Loss: 2.361
Iter 13/2000 - Loss: 2.223
Iter 14/2000 - Loss: 2.099
Iter 15/2000 - Loss: 1.964
Iter 16/2000 - Loss: 1.789
Iter 17/2000 - Loss: 1.569
Iter 18/2000 - Loss: 1.321
Iter 19/2000 - Loss: 1.062
Iter 20/2000 - Loss: 0.793
Iter 1981/2000 - Loss: -7.434
Iter 1982/2000 - Loss: -7.434
Iter 1983/2000 - Loss: -7.434
Iter 1984/2000 - Loss: -7.434
Iter 1985/2000 - Loss: -7.434
Iter 1986/2000 - Loss: -7.434
Iter 1987/2000 - Loss: -7.434
Iter 1988/2000 - Loss: -7.434
Iter 1989/2000 - Loss: -7.434
Iter 1990/2000 - Loss: -7.434
Iter 1991/2000 - Loss: -7.434
Iter 1992/2000 - Loss: -7.434
Iter 1993/2000 - Loss: -7.434
Iter 1994/2000 - Loss: -7.434
Iter 1995/2000 - Loss: -7.434
Iter 1996/2000 - Loss: -7.434
Iter 1997/2000 - Loss: -7.434
Iter 1998/2000 - Loss: -7.435
Iter 1999/2000 - Loss: -7.435
Iter 2000/2000 - Loss: -7.435
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[13.1655,  6.0694, 18.8923,  1.5717, 19.1636, 49.9762]],

        [[16.8954, 30.1405,  9.2211,  1.2560,  1.0696, 31.2097]],

        [[17.6363, 34.6615,  8.5900,  1.1476,  0.7888, 20.0415]],

        [[15.7089, 26.5873, 18.0183,  2.4407,  1.7838, 48.0446]]])
Signal Variance: tensor([ 0.0622,  2.4139, 13.2310,  0.5897])
Estimated target variance: tensor([0.0100, 0.5966, 5.0873, 0.1177])
N: 140
Signal to noise ratio: tensor([16.9873, 86.7446, 81.0456, 44.3778])
Bound on condition number: tensor([  40400.5030, 1053449.3549,  919574.5416,  275716.0849])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.007206071795032881, policy loss: 6.695928967311891
Experience 14, Iter 1, disc loss: 0.005893123509787828, policy loss: 6.670459997020662
Experience 14, Iter 2, disc loss: 0.0076845430171498755, policy loss: 6.388059242087815
Experience 14, Iter 3, disc loss: 0.006450918521614805, policy loss: 6.472047877353025
Experience 14, Iter 4, disc loss: 0.006045534488544406, policy loss: 6.743465962706288
Experience 14, Iter 5, disc loss: 0.005665305796691427, policy loss: 7.139244386925057
Experience 14, Iter 6, disc loss: 0.007109457797852193, policy loss: 6.695936759631385
Experience 14, Iter 7, disc loss: 0.005647084879196766, policy loss: 7.609350456649434
Experience 14, Iter 8, disc loss: 0.005981453924442193, policy loss: 6.809272248699392
Experience 14, Iter 9, disc loss: 0.005667398272767604, policy loss: 7.057059547389738
Experience 14, Iter 10, disc loss: 0.005987883545792197, policy loss: 6.639965057498992
Experience 14, Iter 11, disc loss: 0.005913949447362081, policy loss: 6.7688997580616945
Experience 14, Iter 12, disc loss: 0.006413130652920112, policy loss: 6.602578036451611
Experience 14, Iter 13, disc loss: 0.005473690945723177, policy loss: 7.25252102847499
Experience 14, Iter 14, disc loss: 0.0061038444594750055, policy loss: 6.568721574591375
Experience 14, Iter 15, disc loss: 0.005267577726042029, policy loss: 6.987960441270733
Experience 14, Iter 16, disc loss: 0.005815512475487136, policy loss: 7.015435495117521
Experience 14, Iter 17, disc loss: 0.005740558723656497, policy loss: 7.056648460763665
Experience 14, Iter 18, disc loss: 0.005373752913968422, policy loss: 6.982595509343152
Experience 14, Iter 19, disc loss: 0.005833309441024616, policy loss: 6.7948307159242205
Experience 14, Iter 20, disc loss: 0.004982976221842378, policy loss: 7.108718888112076
Experience 14, Iter 21, disc loss: 0.005153598797799274, policy loss: 7.131307403358982
Experience 14, Iter 22, disc loss: 0.005976573494716679, policy loss: 6.641317104493595
Experience 14, Iter 23, disc loss: 0.005444764331081517, policy loss: 6.634490455854862
Experience 14, Iter 24, disc loss: 0.00618440126277142, policy loss: 6.668999090015093
Experience 14, Iter 25, disc loss: 0.005482784644757182, policy loss: 6.891994682912991
Experience 14, Iter 26, disc loss: 0.006432348479981639, policy loss: 6.661462625230669
Experience 14, Iter 27, disc loss: 0.005252924401526216, policy loss: 6.782095687864478
Experience 14, Iter 28, disc loss: 0.004664451169290857, policy loss: 7.2326888382456485
Experience 14, Iter 29, disc loss: 0.005715914881018986, policy loss: 6.902273574851101
Experience 14, Iter 30, disc loss: 0.00522383600519612, policy loss: 6.703493869955692
Experience 14, Iter 31, disc loss: 0.005498430598342239, policy loss: 6.5280849615594505
Experience 14, Iter 32, disc loss: 0.005153947449686717, policy loss: 6.670887786653145
Experience 14, Iter 33, disc loss: 0.005813635581822085, policy loss: 6.661557833084334
Experience 14, Iter 34, disc loss: 0.004734349955094081, policy loss: 6.818694897166829
Experience 14, Iter 35, disc loss: 0.005051949151877615, policy loss: 7.130209292581217
Experience 14, Iter 36, disc loss: 0.005110593254949616, policy loss: 7.068698486631524
Experience 14, Iter 37, disc loss: 0.005389144573300624, policy loss: 6.685541939618753
Experience 14, Iter 38, disc loss: 0.005134230062783013, policy loss: 6.656433726670553
Experience 14, Iter 39, disc loss: 0.0048630679992252325, policy loss: 7.1323927657276585
Experience 14, Iter 40, disc loss: 0.004968465601699933, policy loss: 6.978558940649403
Experience 14, Iter 41, disc loss: 0.005276282621964964, policy loss: 6.762377250828063
Experience 14, Iter 42, disc loss: 0.004670264958450522, policy loss: 6.874450063571662
Experience 14, Iter 43, disc loss: 0.004539047347147414, policy loss: 7.450587354590459
Experience 14, Iter 44, disc loss: 0.004968884207642951, policy loss: 6.985662569102101
Experience 14, Iter 45, disc loss: 0.005139997244248601, policy loss: 6.8016245677624205
Experience 14, Iter 46, disc loss: 0.004409355426326094, policy loss: 6.919260325022784
Experience 14, Iter 47, disc loss: 0.005232802835562499, policy loss: 7.3869359405538875
Experience 14, Iter 48, disc loss: 0.004807928598774536, policy loss: 6.977712804466942
Experience 14, Iter 49, disc loss: 0.005047654730824889, policy loss: 6.8472579779104485
Experience 14, Iter 50, disc loss: 0.005433725483780691, policy loss: 6.698955118807833
Experience 14, Iter 51, disc loss: 0.0047628985318617575, policy loss: 6.719415455271271
Experience 14, Iter 52, disc loss: 0.004392735549716228, policy loss: 6.79133371907494
Experience 14, Iter 53, disc loss: 0.004959868707833485, policy loss: 6.56513329493723
Experience 14, Iter 54, disc loss: 0.004899904364642812, policy loss: 6.951901169814327
Experience 14, Iter 55, disc loss: 0.006674344490052731, policy loss: 6.696629390264883
Experience 14, Iter 56, disc loss: 0.004214601338633431, policy loss: 7.7015427155342735
Experience 14, Iter 57, disc loss: 0.005315366625554806, policy loss: 6.600115880016704
Experience 14, Iter 58, disc loss: 0.004468379103031607, policy loss: 7.203560922557445
Experience 14, Iter 59, disc loss: 0.004630205582284117, policy loss: 6.760848917141035
Experience 14, Iter 60, disc loss: 0.004339683192453033, policy loss: 7.180197794198486
Experience 14, Iter 61, disc loss: 0.00539362630423266, policy loss: 6.797684858367612
Experience 14, Iter 62, disc loss: 0.004879048818844266, policy loss: 6.833805351470297
Experience 14, Iter 63, disc loss: 0.004399550289167139, policy loss: 7.21885650247224
Experience 14, Iter 64, disc loss: 0.004918284328069104, policy loss: 7.192037736336995
Experience 14, Iter 65, disc loss: 0.004431047921396778, policy loss: 7.390483485780414
Experience 14, Iter 66, disc loss: 0.005595849196559879, policy loss: 6.837329356706073
Experience 14, Iter 67, disc loss: 0.006026202044036935, policy loss: 6.705110374849211
Experience 14, Iter 68, disc loss: 0.004592214792130947, policy loss: 6.996412534370464
Experience 14, Iter 69, disc loss: 0.004935041346711362, policy loss: 7.196891275954325
Experience 14, Iter 70, disc loss: 0.004556448003717014, policy loss: 6.949509241383453
Experience 14, Iter 71, disc loss: 0.004514954992642276, policy loss: 7.092944361441072
Experience 14, Iter 72, disc loss: 0.0053921938228227995, policy loss: 6.914792175321129
Experience 14, Iter 73, disc loss: 0.004744064330018033, policy loss: 7.471405400414886
Experience 14, Iter 74, disc loss: 0.005462576084066144, policy loss: 6.756679710840196
Experience 14, Iter 75, disc loss: 0.004367862620457144, policy loss: 7.281014939159773
Experience 14, Iter 76, disc loss: 0.004569749022982659, policy loss: 7.076538254501365
Experience 14, Iter 77, disc loss: 0.0039771850014869116, policy loss: 7.406519477837809
Experience 14, Iter 78, disc loss: 0.004565253380775418, policy loss: 7.108303591640961
Experience 14, Iter 79, disc loss: 0.004585056123680986, policy loss: 6.832741153678771
Experience 14, Iter 80, disc loss: 0.004807754082777957, policy loss: 6.844864888114002
Experience 14, Iter 81, disc loss: 0.006073328698814804, policy loss: 6.632330466828519
Experience 14, Iter 82, disc loss: 0.005152294592133927, policy loss: 6.980967906808553
Experience 14, Iter 83, disc loss: 0.0051246467331931295, policy loss: 6.803342874696066
Experience 14, Iter 84, disc loss: 0.0054921987822645485, policy loss: 6.849262195043911
Experience 14, Iter 85, disc loss: 0.0047261006426626735, policy loss: 6.830086696670161
Experience 14, Iter 86, disc loss: 0.003814490610300751, policy loss: 7.587305470807552
Experience 14, Iter 87, disc loss: 0.004381210371185943, policy loss: 7.043008064364795
Experience 14, Iter 88, disc loss: 0.005176737993176697, policy loss: 7.2794653926660295
Experience 14, Iter 89, disc loss: 0.00471600456927451, policy loss: 7.31979316487724
Experience 14, Iter 90, disc loss: 0.004742444424576145, policy loss: 6.778976818365932
Experience 14, Iter 91, disc loss: 0.004696209697779726, policy loss: 7.230987829548068
Experience 14, Iter 92, disc loss: 0.00427343180758375, policy loss: 7.335925681044205
Experience 14, Iter 93, disc loss: 0.004516986793456449, policy loss: 7.0927189271360485
Experience 14, Iter 94, disc loss: 0.0045112287488891884, policy loss: 7.122290778991864
Experience 14, Iter 95, disc loss: 0.0045525840727793365, policy loss: 6.988298435293309
Experience 14, Iter 96, disc loss: 0.0049687077595432, policy loss: 6.848570472469692
Experience 14, Iter 97, disc loss: 0.004076006986302741, policy loss: 7.370708560399745
Experience 14, Iter 98, disc loss: 0.00452515431045767, policy loss: 7.000257582649514
Experience 14, Iter 99, disc loss: 0.004012103487114704, policy loss: 7.470732183843014
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1607],
        [1.3463],
        [0.0311]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0125, 0.1127, 1.3848, 0.0236, 0.0208, 3.7628]],

        [[0.0125, 0.1127, 1.3848, 0.0236, 0.0208, 3.7628]],

        [[0.0125, 0.1127, 1.3848, 0.0236, 0.0208, 3.7628]],

        [[0.0125, 0.1127, 1.3848, 0.0236, 0.0208, 3.7628]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0103, 0.6426, 5.3853, 0.1244], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0103, 0.6426, 5.3853, 0.1244])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.046
Iter 2/2000 - Loss: 3.234
Iter 3/2000 - Loss: 2.920
Iter 4/2000 - Loss: 2.970
Iter 5/2000 - Loss: 3.050
Iter 6/2000 - Loss: 2.956
Iter 7/2000 - Loss: 2.825
Iter 8/2000 - Loss: 2.761
Iter 9/2000 - Loss: 2.752
Iter 10/2000 - Loss: 2.725
Iter 11/2000 - Loss: 2.635
Iter 12/2000 - Loss: 2.499
Iter 13/2000 - Loss: 2.359
Iter 14/2000 - Loss: 2.230
Iter 15/2000 - Loss: 2.096
Iter 16/2000 - Loss: 1.928
Iter 17/2000 - Loss: 1.715
Iter 18/2000 - Loss: 1.470
Iter 19/2000 - Loss: 1.208
Iter 20/2000 - Loss: 0.937
Iter 1981/2000 - Loss: -7.510
Iter 1982/2000 - Loss: -7.510
Iter 1983/2000 - Loss: -7.510
Iter 1984/2000 - Loss: -7.510
Iter 1985/2000 - Loss: -7.510
Iter 1986/2000 - Loss: -7.510
Iter 1987/2000 - Loss: -7.510
Iter 1988/2000 - Loss: -7.510
Iter 1989/2000 - Loss: -7.510
Iter 1990/2000 - Loss: -7.511
Iter 1991/2000 - Loss: -7.511
Iter 1992/2000 - Loss: -7.511
Iter 1993/2000 - Loss: -7.511
Iter 1994/2000 - Loss: -7.511
Iter 1995/2000 - Loss: -7.511
Iter 1996/2000 - Loss: -7.511
Iter 1997/2000 - Loss: -7.511
Iter 1998/2000 - Loss: -7.511
Iter 1999/2000 - Loss: -7.511
Iter 2000/2000 - Loss: -7.511
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[12.8023,  6.3098, 18.9182,  1.5418, 19.6536, 49.8339]],

        [[12.5714, 31.2973,  8.8241,  1.2999,  1.1035, 30.4087]],

        [[16.7249, 36.5244,  8.8824,  1.1101,  0.7448, 19.7342]],

        [[15.6334, 26.2329, 16.9954,  2.2683,  1.8121, 44.7718]]])
Signal Variance: tensor([ 0.0621,  2.2801, 12.3496,  0.5114])
Estimated target variance: tensor([0.0103, 0.6426, 5.3853, 0.1244])
N: 150
Signal to noise ratio: tensor([17.3041, 81.9594, 78.6684, 41.2304])
Bound on condition number: tensor([  44915.5733, 1007602.3104,  928309.4483,  254992.7797])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0036993447442259524, policy loss: 7.8017691691065165
Experience 15, Iter 1, disc loss: 0.0040732177662004595, policy loss: 6.821337115418861
Experience 15, Iter 2, disc loss: 0.004614415103110718, policy loss: 7.326098017840797
Experience 15, Iter 3, disc loss: 0.003952452675291438, policy loss: 7.332535989101618
Experience 15, Iter 4, disc loss: 0.0048169162498815365, policy loss: 6.896903827848999
Experience 15, Iter 5, disc loss: 0.003908865733521783, policy loss: 7.257842671305399
Experience 15, Iter 6, disc loss: 0.004568872459319719, policy loss: 6.976051236614123
Experience 15, Iter 7, disc loss: 0.0039057145548343167, policy loss: 7.238201780645217
Experience 15, Iter 8, disc loss: 0.004265982103714171, policy loss: 6.9790317801119
Experience 15, Iter 9, disc loss: 0.00394675876501877, policy loss: 6.989057697257415
Experience 15, Iter 10, disc loss: 0.003807964833414799, policy loss: 7.096630152798036
Experience 15, Iter 11, disc loss: 0.00405610993392738, policy loss: 7.236964620857859
Experience 15, Iter 12, disc loss: 0.00474442667915625, policy loss: 7.119343365067907
Experience 15, Iter 13, disc loss: 0.003978688479645677, policy loss: 7.381687048535575
Experience 15, Iter 14, disc loss: 0.004310825605732812, policy loss: 6.91194797222414
Experience 15, Iter 15, disc loss: 0.003696933014420405, policy loss: 7.200308461052119
Experience 15, Iter 16, disc loss: 0.0047661112230454415, policy loss: 6.977786923407181
Experience 15, Iter 17, disc loss: 0.0048642331103970395, policy loss: 7.055095112828927
Experience 15, Iter 18, disc loss: 0.0044576643208378616, policy loss: 7.3683304850503255
Experience 15, Iter 19, disc loss: 0.004606232595103366, policy loss: 6.901236941089019
Experience 15, Iter 20, disc loss: 0.004157777208401096, policy loss: 7.263185449722432
Experience 15, Iter 21, disc loss: 0.004908122900313156, policy loss: 6.7811511557928
Experience 15, Iter 22, disc loss: 0.004071487183768234, policy loss: 7.075244739013864
Experience 15, Iter 23, disc loss: 0.005004505255647733, policy loss: 6.989795096783851
Experience 15, Iter 24, disc loss: 0.0036155886493389995, policy loss: 7.468676271115834
Experience 15, Iter 25, disc loss: 0.0035574857831689784, policy loss: 7.548178021442485
Experience 15, Iter 26, disc loss: 0.003565699381597098, policy loss: 7.342275819144673
Experience 15, Iter 27, disc loss: 0.0036788247179068002, policy loss: 7.49675785168103
Experience 15, Iter 28, disc loss: 0.00408220873417834, policy loss: 7.3373112341999365
Experience 15, Iter 29, disc loss: 0.005092301669226815, policy loss: 7.108695878694327
Experience 15, Iter 30, disc loss: 0.0037392718123386106, policy loss: 7.517259563619193
Experience 15, Iter 31, disc loss: 0.0050407530364861885, policy loss: 6.730271101615069
Experience 15, Iter 32, disc loss: 0.004331867336138648, policy loss: 7.55739176598053
Experience 15, Iter 33, disc loss: 0.004595053993815297, policy loss: 7.158485387302741
Experience 15, Iter 34, disc loss: 0.0042768305064253154, policy loss: 7.47723541555761
Experience 15, Iter 35, disc loss: 0.00369103079725027, policy loss: 7.444789116007309
Experience 15, Iter 36, disc loss: 0.004117550937577394, policy loss: 7.657643347958791
Experience 15, Iter 37, disc loss: 0.003965306684255696, policy loss: 7.0299821830009614
Experience 15, Iter 38, disc loss: 0.004105184683729772, policy loss: 7.439414401985442
Experience 15, Iter 39, disc loss: 0.004517786810041425, policy loss: 7.032863470923557
Experience 15, Iter 40, disc loss: 0.003661557254198064, policy loss: 7.389752801078138
Experience 15, Iter 41, disc loss: 0.003960547304811583, policy loss: 7.166318201316155
Experience 15, Iter 42, disc loss: 0.004432804176244797, policy loss: 7.28772161100485
Experience 15, Iter 43, disc loss: 0.0036827257450306986, policy loss: 7.256368735294395
Experience 15, Iter 44, disc loss: 0.0038455334263301595, policy loss: 7.455187786066068
Experience 15, Iter 45, disc loss: 0.0041050142525504985, policy loss: 7.427577971652386
Experience 15, Iter 46, disc loss: 0.004323272557475092, policy loss: 7.246677451291168
Experience 15, Iter 47, disc loss: 0.003572581243930968, policy loss: 7.467330716596498
Experience 15, Iter 48, disc loss: 0.004008502730533957, policy loss: 7.183836933975904
Experience 15, Iter 49, disc loss: 0.0038480957795836183, policy loss: 7.467782133352532
Experience 15, Iter 50, disc loss: 0.004174064132910726, policy loss: 6.958034695608847
Experience 15, Iter 51, disc loss: 0.003930455496722848, policy loss: 7.469400905792108
Experience 15, Iter 52, disc loss: 0.004263038042506973, policy loss: 7.206261287416553
Experience 15, Iter 53, disc loss: 0.004600622174478571, policy loss: 7.001732593081572
Experience 15, Iter 54, disc loss: 0.004141251483803843, policy loss: 7.042750584776671
Experience 15, Iter 55, disc loss: 0.0037734908419070484, policy loss: 7.861929911908333
Experience 15, Iter 56, disc loss: 0.003811186458527769, policy loss: 7.522519836370211
Experience 15, Iter 57, disc loss: 0.0037581020777626654, policy loss: 7.158570143472749
Experience 15, Iter 58, disc loss: 0.0034964238626678246, policy loss: 7.034800858871209
Experience 15, Iter 59, disc loss: 0.004048397318937446, policy loss: 7.114961842762927
Experience 15, Iter 60, disc loss: 0.003126158034952118, policy loss: 7.56108378146065
Experience 15, Iter 61, disc loss: 0.004539199806073179, policy loss: 6.984119534843402
Experience 15, Iter 62, disc loss: 0.003491500166474186, policy loss: 7.267655761947228
Experience 15, Iter 63, disc loss: 0.003618778264986584, policy loss: 7.567095473502093
Experience 15, Iter 64, disc loss: 0.0037268793019395, policy loss: 7.395798287786178
Experience 15, Iter 65, disc loss: 0.003706692624025342, policy loss: 7.295007320352529
Experience 15, Iter 66, disc loss: 0.004042013604142883, policy loss: 7.295868349013704
Experience 15, Iter 67, disc loss: 0.004509283408630787, policy loss: 7.061065790903085
Experience 15, Iter 68, disc loss: 0.0038448348415127277, policy loss: 7.652928599262199
Experience 15, Iter 69, disc loss: 0.0039966539938946334, policy loss: 7.077790367746363
Experience 15, Iter 70, disc loss: 0.003321485955500905, policy loss: 7.149941424632011
Experience 15, Iter 71, disc loss: 0.0043238109184469825, policy loss: 7.141826862461834
Experience 15, Iter 72, disc loss: 0.003947403835906561, policy loss: 7.225251085900757
Experience 15, Iter 73, disc loss: 0.0032063504174312633, policy loss: 7.781137170061708
Experience 15, Iter 74, disc loss: 0.0033663747545743285, policy loss: 7.5457252208028756
Experience 15, Iter 75, disc loss: 0.0034959771597808265, policy loss: 7.307127198045487
Experience 15, Iter 76, disc loss: 0.003338359111807601, policy loss: 7.176415959494657
Experience 15, Iter 77, disc loss: 0.003946320630115667, policy loss: 7.3007930333188344
Experience 15, Iter 78, disc loss: 0.0036305572459006067, policy loss: 7.528852620632133
Experience 15, Iter 79, disc loss: 0.0031561037707749894, policy loss: 7.8887932907679765
Experience 15, Iter 80, disc loss: 0.0031039518704397686, policy loss: 7.823132286409519
Experience 15, Iter 81, disc loss: 0.00460416763599511, policy loss: 7.117741157222493
Experience 15, Iter 82, disc loss: 0.0030996996186718083, policy loss: 8.23277593567024
Experience 15, Iter 83, disc loss: 0.003082425242492484, policy loss: 7.59373232725378
Experience 15, Iter 84, disc loss: 0.003835417904694204, policy loss: 7.452700641936898
Experience 15, Iter 85, disc loss: 0.0032905325096618176, policy loss: 7.251291338252301
Experience 15, Iter 86, disc loss: 0.0029499609367436746, policy loss: 8.020597993214789
Experience 15, Iter 87, disc loss: 0.0034901918325198986, policy loss: 7.462818338425707
Experience 15, Iter 88, disc loss: 0.003914476139235344, policy loss: 7.099345191058407
Experience 15, Iter 89, disc loss: 0.0029083612593000293, policy loss: 7.633498454963147
Experience 15, Iter 90, disc loss: 0.003348284569730493, policy loss: 7.707708848849864
Experience 15, Iter 91, disc loss: 0.003121032088347005, policy loss: 7.444989253218693
Experience 15, Iter 92, disc loss: 0.003705238991957749, policy loss: 7.605316369207651
Experience 15, Iter 93, disc loss: 0.002912254370461049, policy loss: 7.663249005712421
Experience 15, Iter 94, disc loss: 0.0029990810187336073, policy loss: 7.603677590079467
Experience 15, Iter 95, disc loss: 0.0036368466705726174, policy loss: 7.539860227020323
Experience 15, Iter 96, disc loss: 0.0030459831603569494, policy loss: 7.50619831262875
Experience 15, Iter 97, disc loss: 0.0028839660920854954, policy loss: 7.611556676093736
Experience 15, Iter 98, disc loss: 0.003595094770178683, policy loss: 7.152154671313838
Experience 15, Iter 99, disc loss: 0.003431666664362323, policy loss: 7.0041827124855125
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1671],
        [1.4028],
        [0.0321]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0118, 0.1138, 1.4402, 0.0246, 0.0215, 3.9237]],

        [[0.0118, 0.1138, 1.4402, 0.0246, 0.0215, 3.9237]],

        [[0.0118, 0.1138, 1.4402, 0.0246, 0.0215, 3.9237]],

        [[0.0118, 0.1138, 1.4402, 0.0246, 0.0215, 3.9237]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0103, 0.6682, 5.6111, 0.1285], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0103, 0.6682, 5.6111, 0.1285])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.110
Iter 2/2000 - Loss: 3.296
Iter 3/2000 - Loss: 2.977
Iter 4/2000 - Loss: 3.022
Iter 5/2000 - Loss: 3.100
Iter 6/2000 - Loss: 3.002
Iter 7/2000 - Loss: 2.863
Iter 8/2000 - Loss: 2.787
Iter 9/2000 - Loss: 2.767
Iter 10/2000 - Loss: 2.732
Iter 11/2000 - Loss: 2.635
Iter 12/2000 - Loss: 2.491
Iter 13/2000 - Loss: 2.337
Iter 14/2000 - Loss: 2.193
Iter 15/2000 - Loss: 2.044
Iter 16/2000 - Loss: 1.863
Iter 17/2000 - Loss: 1.637
Iter 18/2000 - Loss: 1.375
Iter 19/2000 - Loss: 1.095
Iter 20/2000 - Loss: 0.807
Iter 1981/2000 - Loss: -7.610
Iter 1982/2000 - Loss: -7.611
Iter 1983/2000 - Loss: -7.611
Iter 1984/2000 - Loss: -7.611
Iter 1985/2000 - Loss: -7.611
Iter 1986/2000 - Loss: -7.611
Iter 1987/2000 - Loss: -7.611
Iter 1988/2000 - Loss: -7.611
Iter 1989/2000 - Loss: -7.611
Iter 1990/2000 - Loss: -7.611
Iter 1991/2000 - Loss: -7.611
Iter 1992/2000 - Loss: -7.611
Iter 1993/2000 - Loss: -7.611
Iter 1994/2000 - Loss: -7.611
Iter 1995/2000 - Loss: -7.611
Iter 1996/2000 - Loss: -7.611
Iter 1997/2000 - Loss: -7.611
Iter 1998/2000 - Loss: -7.611
Iter 1999/2000 - Loss: -7.611
Iter 2000/2000 - Loss: -7.611
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[12.0154,  6.3561, 20.9701,  1.6907, 16.9351, 48.7933]],

        [[12.5920, 27.7035,  8.0908,  1.9718,  1.0099, 17.3570]],

        [[16.9749, 35.7756,  7.9082,  1.2000,  0.8566, 19.0792]],

        [[15.1204, 25.1859, 16.6610,  2.1829,  1.8506, 45.2361]]])
Signal Variance: tensor([ 0.0634,  1.5199, 12.4440,  0.5025])
Estimated target variance: tensor([0.0103, 0.6682, 5.6111, 0.1285])
N: 160
Signal to noise ratio: tensor([17.2091, 66.6573, 79.6344, 41.9047])
Bound on condition number: tensor([  47385.3771,  710911.6285, 1014661.7808,  280961.2794])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.003686103741631856, policy loss: 7.62388509512401
Experience 16, Iter 1, disc loss: 0.0034212119040240335, policy loss: 7.232593798441201
Experience 16, Iter 2, disc loss: 0.0030416554221090165, policy loss: 7.58925511192488
Experience 16, Iter 3, disc loss: 0.0041773069723190556, policy loss: 6.694726623312542
Experience 16, Iter 4, disc loss: 0.003244772248603996, policy loss: 7.812706875573334
Experience 16, Iter 5, disc loss: 0.002993788639972363, policy loss: 7.656734360701478
Experience 16, Iter 6, disc loss: 0.003507149752699148, policy loss: 7.599947036049571
Experience 16, Iter 7, disc loss: 0.0035457928079574933, policy loss: 7.866809008458012
Experience 16, Iter 8, disc loss: 0.005896899906932767, policy loss: 7.145251093527756
Experience 16, Iter 9, disc loss: 0.0033777460061701096, policy loss: 7.715554313331422
Experience 16, Iter 10, disc loss: 0.003445707128136041, policy loss: 7.234703724207959
Experience 16, Iter 11, disc loss: 0.003947487697540861, policy loss: 7.20457320911792
Experience 16, Iter 12, disc loss: 0.0033357038432865714, policy loss: 7.731093116344894
Experience 16, Iter 13, disc loss: 0.003401424818457465, policy loss: 8.331694400007933
Experience 16, Iter 14, disc loss: 0.004184541217843745, policy loss: 7.534841675612866
Experience 16, Iter 15, disc loss: 0.002997546521962656, policy loss: 7.844450052940346
Experience 16, Iter 16, disc loss: 0.002763427844084597, policy loss: 8.05601310738326
Experience 16, Iter 17, disc loss: 0.0034867391760364276, policy loss: 7.685168530745065
Experience 16, Iter 18, disc loss: 0.0038826429553551075, policy loss: 7.709931771003357
Experience 16, Iter 19, disc loss: 0.003329155624170822, policy loss: 7.317309387942609
Experience 16, Iter 20, disc loss: 0.0027118408537630976, policy loss: 7.699166667339607
Experience 16, Iter 21, disc loss: 0.003519023036313653, policy loss: 7.267365694975874
Experience 16, Iter 22, disc loss: 0.0031886261877222726, policy loss: 7.870460131489141
Experience 16, Iter 23, disc loss: 0.003613455500826151, policy loss: 7.799234293596518
Experience 16, Iter 24, disc loss: 0.0026917172212718435, policy loss: 7.754009064185964
Experience 16, Iter 25, disc loss: 0.002938912887784139, policy loss: 7.591464788308608
Experience 16, Iter 26, disc loss: 0.004010026136240774, policy loss: 7.775082371450319
Experience 16, Iter 27, disc loss: 0.0035026527303649553, policy loss: 7.401134375104193
Experience 16, Iter 28, disc loss: 0.003373673895673026, policy loss: 7.540553822378612
Experience 16, Iter 29, disc loss: 0.0032086701352636547, policy loss: 7.576883897329056
Experience 16, Iter 30, disc loss: 0.003207911746655809, policy loss: 7.738467742351409
Experience 16, Iter 31, disc loss: 0.003311127763309474, policy loss: 8.288281529843287
Experience 16, Iter 32, disc loss: 0.0038601437887110296, policy loss: 7.166930359938302
Experience 16, Iter 33, disc loss: 0.0032987116319164594, policy loss: 7.216558316989748
Experience 16, Iter 34, disc loss: 0.003135855082594347, policy loss: 7.694488385346981
Experience 16, Iter 35, disc loss: 0.00380639185715861, policy loss: 7.511147462252646
Experience 16, Iter 36, disc loss: 0.002836170056204179, policy loss: 7.627800893973182
Experience 16, Iter 37, disc loss: 0.003053975542815075, policy loss: 8.180959592966659
Experience 16, Iter 38, disc loss: 0.003996675432659131, policy loss: 7.354482388816573
Experience 16, Iter 39, disc loss: 0.0027517802742887103, policy loss: 8.537927253465499
Experience 16, Iter 40, disc loss: 0.003116915385977412, policy loss: 7.618961354938833
Experience 16, Iter 41, disc loss: 0.003631266633255909, policy loss: 7.198048888191435
Experience 16, Iter 42, disc loss: 0.0034514966500096963, policy loss: 7.684424414179903
Experience 16, Iter 43, disc loss: 0.0026114528957742276, policy loss: 8.21177483576622
Experience 16, Iter 44, disc loss: 0.003189873115899443, policy loss: 7.900066176742972
Experience 16, Iter 45, disc loss: 0.0028206398198423036, policy loss: 7.843619442678305
Experience 16, Iter 46, disc loss: 0.003837900465231434, policy loss: 7.513135890361397
Experience 16, Iter 47, disc loss: 0.0029152256155402607, policy loss: 7.763363781439525
Experience 16, Iter 48, disc loss: 0.003848289631213543, policy loss: 8.091262516130172
Experience 16, Iter 49, disc loss: 0.003173659151313777, policy loss: 7.7624432960224485
Experience 16, Iter 50, disc loss: 0.003445105845369623, policy loss: 7.54806058602756
Experience 16, Iter 51, disc loss: 0.0033306473344106866, policy loss: 7.681889536656174
Experience 16, Iter 52, disc loss: 0.002883146910810413, policy loss: 8.139928845781117
Experience 16, Iter 53, disc loss: 0.003007002756322553, policy loss: 7.9164163086849015
Experience 16, Iter 54, disc loss: 0.0030018562078980146, policy loss: 7.494549136198385
Experience 16, Iter 55, disc loss: 0.003459749914493172, policy loss: 7.518344449720077
Experience 16, Iter 56, disc loss: 0.0026719464073760952, policy loss: 8.127979134920912
Experience 16, Iter 57, disc loss: 0.003821773784993696, policy loss: 7.298428262767308
Experience 16, Iter 58, disc loss: 0.003346263150148186, policy loss: 7.4033097925057705
Experience 16, Iter 59, disc loss: 0.003394474902382455, policy loss: 7.5680626657628896
Experience 16, Iter 60, disc loss: 0.0036176835731045447, policy loss: 7.312768974730007
Experience 16, Iter 61, disc loss: 0.002620872045947409, policy loss: 7.585676771932657
Experience 16, Iter 62, disc loss: 0.003067983480282114, policy loss: 7.661532238179752
Experience 16, Iter 63, disc loss: 0.0026474842033946413, policy loss: 7.4617746560224605
Experience 16, Iter 64, disc loss: 0.002843616497918977, policy loss: 7.54476710524728
Experience 16, Iter 65, disc loss: 0.003137466255573298, policy loss: 7.394371585356707
Experience 16, Iter 66, disc loss: 0.0025162253039744, policy loss: 7.801713625724245
Experience 16, Iter 67, disc loss: 0.004027079309916807, policy loss: 7.259469882280424
Experience 16, Iter 68, disc loss: 0.0028713298248588034, policy loss: 7.754318900537319
Experience 16, Iter 69, disc loss: 0.0032412902329262477, policy loss: 7.734245241308143
Experience 16, Iter 70, disc loss: 0.002846044255157453, policy loss: 7.939203820161385
Experience 16, Iter 71, disc loss: 0.003317522764606653, policy loss: 7.540032103992919
Experience 16, Iter 72, disc loss: 0.0027148309419762987, policy loss: 7.7190260303937075
Experience 16, Iter 73, disc loss: 0.0027554355703487896, policy loss: 7.568856317904595
Experience 16, Iter 74, disc loss: 0.0028736900808280993, policy loss: 7.795235487836422
Experience 16, Iter 75, disc loss: 0.0037409908285716224, policy loss: 7.460908456678498
Experience 16, Iter 76, disc loss: 0.00301634499400511, policy loss: 7.695180620044285
Experience 16, Iter 77, disc loss: 0.0030809283867201096, policy loss: 7.605334795794936
Experience 16, Iter 78, disc loss: 0.0033689232277103794, policy loss: 7.400349454553476
Experience 16, Iter 79, disc loss: 0.0027887420663900717, policy loss: 7.73183626248586
Experience 16, Iter 80, disc loss: 0.0030733643790218533, policy loss: 7.7141635178111425
Experience 16, Iter 81, disc loss: 0.0029548836290553084, policy loss: 7.792151992406909
Experience 16, Iter 82, disc loss: 0.0031915834824623925, policy loss: 7.4331523110007485
Experience 16, Iter 83, disc loss: 0.0024944210754144895, policy loss: 7.844383445517507
Experience 16, Iter 84, disc loss: 0.002783904243154294, policy loss: 7.806128638638463
Experience 16, Iter 85, disc loss: 0.003066965433433569, policy loss: 7.841304312416931
Experience 16, Iter 86, disc loss: 0.002786814826837975, policy loss: 8.089971811159762
Experience 16, Iter 87, disc loss: 0.0025916860375661144, policy loss: 7.689408342101871
Experience 16, Iter 88, disc loss: 0.002744587354548935, policy loss: 7.872196028839502
Experience 16, Iter 89, disc loss: 0.002553825349506292, policy loss: 7.89465040504782
Experience 16, Iter 90, disc loss: 0.0028894501373165965, policy loss: 7.409939023885697
Experience 16, Iter 91, disc loss: 0.002579983754155384, policy loss: 7.482996844979777
Experience 16, Iter 92, disc loss: 0.0025680678913965966, policy loss: 7.6652133509723885
Experience 16, Iter 93, disc loss: 0.0032881970711880737, policy loss: 7.521698396191443
Experience 16, Iter 94, disc loss: 0.002572177655628255, policy loss: 7.8263419411306465
Experience 16, Iter 95, disc loss: 0.0032815601968961576, policy loss: 7.5634583456276605
Experience 16, Iter 96, disc loss: 0.0033649360415204817, policy loss: 7.271906749797438
Experience 16, Iter 97, disc loss: 0.0023916821292314016, policy loss: 7.970633593706865
Experience 16, Iter 98, disc loss: 0.0033640491822437063, policy loss: 7.564591755720819
Experience 16, Iter 99, disc loss: 0.0025691868457586803, policy loss: 7.631055353040084
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1751],
        [1.4688],
        [0.0337]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0114, 0.1168, 1.5124, 0.0255, 0.0224, 4.0878]],

        [[0.0114, 0.1168, 1.5124, 0.0255, 0.0224, 4.0878]],

        [[0.0114, 0.1168, 1.5124, 0.0255, 0.0224, 4.0878]],

        [[0.0114, 0.1168, 1.5124, 0.0255, 0.0224, 4.0878]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0107, 0.7006, 5.8752, 0.1348], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0107, 0.7006, 5.8752, 0.1348])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.200
Iter 2/2000 - Loss: 3.379
Iter 3/2000 - Loss: 3.060
Iter 4/2000 - Loss: 3.101
Iter 5/2000 - Loss: 3.179
Iter 6/2000 - Loss: 3.084
Iter 7/2000 - Loss: 2.944
Iter 8/2000 - Loss: 2.862
Iter 9/2000 - Loss: 2.837
Iter 10/2000 - Loss: 2.799
Iter 11/2000 - Loss: 2.700
Iter 12/2000 - Loss: 2.553
Iter 13/2000 - Loss: 2.392
Iter 14/2000 - Loss: 2.236
Iter 15/2000 - Loss: 2.074
Iter 16/2000 - Loss: 1.880
Iter 17/2000 - Loss: 1.642
Iter 18/2000 - Loss: 1.368
Iter 19/2000 - Loss: 1.076
Iter 20/2000 - Loss: 0.774
Iter 1981/2000 - Loss: -7.683
Iter 1982/2000 - Loss: -7.683
Iter 1983/2000 - Loss: -7.683
Iter 1984/2000 - Loss: -7.683
Iter 1985/2000 - Loss: -7.683
Iter 1986/2000 - Loss: -7.683
Iter 1987/2000 - Loss: -7.683
Iter 1988/2000 - Loss: -7.683
Iter 1989/2000 - Loss: -7.683
Iter 1990/2000 - Loss: -7.683
Iter 1991/2000 - Loss: -7.683
Iter 1992/2000 - Loss: -7.683
Iter 1993/2000 - Loss: -7.683
Iter 1994/2000 - Loss: -7.683
Iter 1995/2000 - Loss: -7.683
Iter 1996/2000 - Loss: -7.683
Iter 1997/2000 - Loss: -7.683
Iter 1998/2000 - Loss: -7.683
Iter 1999/2000 - Loss: -7.683
Iter 2000/2000 - Loss: -7.683
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[11.1515,  5.9717, 23.7004,  1.5545, 17.2770, 47.6610]],

        [[11.8418, 28.4974,  8.1767,  1.9244,  0.9796, 17.5589]],

        [[17.4967, 34.2245,  7.8205,  1.1053,  0.7893, 21.0566]],

        [[14.8714, 25.3544, 16.7950,  2.0777,  1.7667, 44.7892]]])
Signal Variance: tensor([ 0.0570,  1.4717, 11.7922,  0.4736])
Estimated target variance: tensor([0.0107, 0.7006, 5.8752, 0.1348])
N: 170
Signal to noise ratio: tensor([15.7535, 66.1661, 77.3312, 41.2611])
Bound on condition number: tensor([  42190.1530,  744253.2272, 1016621.2312,  289421.7270])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.002302811484738216, policy loss: 8.464405456454429
Experience 17, Iter 1, disc loss: 0.00266275048528985, policy loss: 7.7494091692723615
Experience 17, Iter 2, disc loss: 0.0025775532875696675, policy loss: 8.097689487825466
Experience 17, Iter 3, disc loss: 0.002673877640516073, policy loss: 8.232388590231166
Experience 17, Iter 4, disc loss: 0.002743241616446226, policy loss: 7.723001133201242
Experience 17, Iter 5, disc loss: 0.0029919000192014985, policy loss: 7.48338402714126
Experience 17, Iter 6, disc loss: 0.0018982514481429293, policy loss: 8.262161485061961
Experience 17, Iter 7, disc loss: 0.002560978344194687, policy loss: 7.757514101044034
Experience 17, Iter 8, disc loss: 0.0028614864149155454, policy loss: 7.773416459824353
Experience 17, Iter 9, disc loss: 0.002751599607175111, policy loss: 7.792818001090507
Experience 17, Iter 10, disc loss: 0.0027370465597869537, policy loss: 7.703873474036243
Experience 17, Iter 11, disc loss: 0.0026821110321900024, policy loss: 7.928809978502192
Experience 17, Iter 12, disc loss: 0.0028637489819066725, policy loss: 7.689265458356433
Experience 17, Iter 13, disc loss: 0.0027839951567783963, policy loss: 7.7877041987203315
Experience 17, Iter 14, disc loss: 0.0027872701139989037, policy loss: 7.824786495865227
Experience 17, Iter 15, disc loss: 0.0038780189542169836, policy loss: 7.3048848292349
Experience 17, Iter 16, disc loss: 0.0034052827851302966, policy loss: 7.656093752807031
Experience 17, Iter 17, disc loss: 0.0026262682865924097, policy loss: 7.996751626963739
Experience 17, Iter 18, disc loss: 0.002765534906964447, policy loss: 8.131978221048783
Experience 17, Iter 19, disc loss: 0.003001307600460832, policy loss: 7.654085455455491
Experience 17, Iter 20, disc loss: 0.0030256742482517966, policy loss: 8.080881957383168
Experience 17, Iter 21, disc loss: 0.00242874238076364, policy loss: 8.141342293231544
Experience 17, Iter 22, disc loss: 0.002343294091930896, policy loss: 7.548949254694591
Experience 17, Iter 23, disc loss: 0.0028516977706924754, policy loss: 7.538846770373726
Experience 17, Iter 24, disc loss: 0.002345828016287888, policy loss: 8.171339010596245
Experience 17, Iter 25, disc loss: 0.002541361416841992, policy loss: 8.191549025782352
Experience 17, Iter 26, disc loss: 0.0024379693017812206, policy loss: 7.822549404493644
Experience 17, Iter 27, disc loss: 0.0023382404284471303, policy loss: 7.711934850287179
Experience 17, Iter 28, disc loss: 0.0025837408405823296, policy loss: 7.839893640565622
Experience 17, Iter 29, disc loss: 0.0034631468923593325, policy loss: 7.657954322839625
Experience 17, Iter 30, disc loss: 0.0030537586834290596, policy loss: 7.589136625758355
Experience 17, Iter 31, disc loss: 0.0023057966153963616, policy loss: 7.868083831954733
Experience 17, Iter 32, disc loss: 0.002606469064630417, policy loss: 8.403674826002304
Experience 17, Iter 33, disc loss: 0.002505186375494668, policy loss: 7.800601019572079
Experience 17, Iter 34, disc loss: 0.0023200918632823317, policy loss: 7.820462473657569
Experience 17, Iter 35, disc loss: 0.0024483444558508075, policy loss: 7.722096252368831
Experience 17, Iter 36, disc loss: 0.0029676188492790308, policy loss: 8.014982639653573
Experience 17, Iter 37, disc loss: 0.0024871707570876906, policy loss: 7.524891443993095
Experience 17, Iter 38, disc loss: 0.0023015568321754325, policy loss: 7.867892569324605
Experience 17, Iter 39, disc loss: 0.0032907735579774285, policy loss: 8.20250400335599
Experience 17, Iter 40, disc loss: 0.002633841401597582, policy loss: 7.972355009088411
Experience 17, Iter 41, disc loss: 0.0025027981431113067, policy loss: 7.710131956732635
Experience 17, Iter 42, disc loss: 0.0024194282202891926, policy loss: 8.113823580288672
Experience 17, Iter 43, disc loss: 0.0022412675363005243, policy loss: 7.68879104894798
Experience 17, Iter 44, disc loss: 0.0022231483498311054, policy loss: 8.247420977407586
Experience 17, Iter 45, disc loss: 0.0022041725296946158, policy loss: 8.216068124800703
Experience 17, Iter 46, disc loss: 0.0037495185661378027, policy loss: 8.067842141862208
Experience 17, Iter 47, disc loss: 0.0023243245521613696, policy loss: 7.85170733359274
Experience 17, Iter 48, disc loss: 0.002420964231874716, policy loss: 8.2428965124041
Experience 17, Iter 49, disc loss: 0.002452316202393514, policy loss: 8.392600663480682
Experience 17, Iter 50, disc loss: 0.002119343676948619, policy loss: 8.176026979316688
Experience 17, Iter 51, disc loss: 0.0023223487018348746, policy loss: 7.940473053976476
Experience 17, Iter 52, disc loss: 0.00294400394857059, policy loss: 7.515358668681014
Experience 17, Iter 53, disc loss: 0.0025421203683374666, policy loss: 7.8447916453399955
Experience 17, Iter 54, disc loss: 0.0022962511211565787, policy loss: 7.974631044630456
Experience 17, Iter 55, disc loss: 0.002621831769415687, policy loss: 7.806022646016819
Experience 17, Iter 56, disc loss: 0.002779587189561804, policy loss: 8.066523102539058
Experience 17, Iter 57, disc loss: 0.0022699472414550617, policy loss: 8.18004530261697
Experience 17, Iter 58, disc loss: 0.0024838174102300836, policy loss: 8.298196736486354
Experience 17, Iter 59, disc loss: 0.0025362943399347736, policy loss: 8.020357818049927
Experience 17, Iter 60, disc loss: 0.0024227745327104876, policy loss: 7.5347818120802925
Experience 17, Iter 61, disc loss: 0.0023583993490888123, policy loss: 8.068043025215179
Experience 17, Iter 62, disc loss: 0.0022197075498761576, policy loss: 8.18382768473935
Experience 17, Iter 63, disc loss: 0.0024896620436495064, policy loss: 7.858478180759784
Experience 17, Iter 64, disc loss: 0.0021506210974918376, policy loss: 8.241877097047176
Experience 17, Iter 65, disc loss: 0.0021571917048995755, policy loss: 8.704567682730376
Experience 17, Iter 66, disc loss: 0.0019014130704837382, policy loss: 8.354533336361254
Experience 17, Iter 67, disc loss: 0.0023802735359522093, policy loss: 7.701052481052855
Experience 17, Iter 68, disc loss: 0.0022179273907754846, policy loss: 8.236369249631318
Experience 17, Iter 69, disc loss: 0.001986930311124714, policy loss: 8.469806260436275
Experience 17, Iter 70, disc loss: 0.0020993218587908747, policy loss: 7.8300725029388945
Experience 17, Iter 71, disc loss: 0.0021067053567890314, policy loss: 8.005062032759557
Experience 17, Iter 72, disc loss: 0.0020310416916663877, policy loss: 8.693905867224327
Experience 17, Iter 73, disc loss: 0.0017682140941275293, policy loss: 8.380904109269581
Experience 17, Iter 74, disc loss: 0.002245251634479644, policy loss: 8.101142292920299
Experience 17, Iter 75, disc loss: 0.0017804289376423393, policy loss: 8.291872311178118
Experience 17, Iter 76, disc loss: 0.0018198277697141513, policy loss: 8.396358812105646
Experience 17, Iter 77, disc loss: 0.001739452304936748, policy loss: 8.714156928204487
Experience 17, Iter 78, disc loss: 0.0016587712368477934, policy loss: 8.131043911960555
Experience 17, Iter 79, disc loss: 0.0025597899600193637, policy loss: 8.168523081867537
Experience 17, Iter 80, disc loss: 0.0027782561175748134, policy loss: 7.745344505332522
Experience 17, Iter 81, disc loss: 0.002596197498716338, policy loss: 7.749658040716636
Experience 17, Iter 82, disc loss: 0.0016063727307037224, policy loss: 8.391723926555573
Experience 17, Iter 83, disc loss: 0.0015081410390375151, policy loss: 9.407677691849852
Experience 17, Iter 84, disc loss: 0.0010529462010448166, policy loss: 10.48730454960472
Experience 17, Iter 85, disc loss: 0.0009834516867353348, policy loss: 13.005361525470448
Experience 17, Iter 86, disc loss: 0.0009641683517021891, policy loss: 14.257881564752823
Experience 17, Iter 87, disc loss: 0.0009439824913148373, policy loss: 15.143200171224398
Experience 17, Iter 88, disc loss: 0.0009214131535256157, policy loss: 15.236687174038881
Experience 17, Iter 89, disc loss: 0.0008968310587092847, policy loss: 15.42158877256635
Experience 17, Iter 90, disc loss: 0.0008708146643838894, policy loss: 15.207910675214434
Experience 17, Iter 91, disc loss: 0.0008438048747373474, policy loss: 14.880301907875355
Experience 17, Iter 92, disc loss: 0.0008164163940838777, policy loss: 14.334822490482264
Experience 17, Iter 93, disc loss: 0.000789345776276695, policy loss: 13.625501611695544
Experience 17, Iter 94, disc loss: 0.000764907531736131, policy loss: 12.414974704784495
Experience 17, Iter 95, disc loss: 0.0007541086923330626, policy loss: 11.08497448056012
Experience 17, Iter 96, disc loss: 0.0009052172817346704, policy loss: 8.908922015754968
Experience 17, Iter 97, disc loss: 0.002253617687745345, policy loss: 10.956403480054924
Experience 17, Iter 98, disc loss: 0.0015807406212366, policy loss: 10.661800176022922
Experience 17, Iter 99, disc loss: 0.0009623064944107287, policy loss: 8.346615697676345
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1799],
        [1.5148],
        [0.0344]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0110, 0.1176, 1.5536, 0.0263, 0.0228, 4.2012]],

        [[0.0110, 0.1176, 1.5536, 0.0263, 0.0228, 4.2012]],

        [[0.0110, 0.1176, 1.5536, 0.0263, 0.0228, 4.2012]],

        [[0.0110, 0.1176, 1.5536, 0.0263, 0.0228, 4.2012]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.7197, 6.0590, 0.1376], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.7197, 6.0590, 0.1376])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.248
Iter 2/2000 - Loss: 3.442
Iter 3/2000 - Loss: 3.107
Iter 4/2000 - Loss: 3.148
Iter 5/2000 - Loss: 3.234
Iter 6/2000 - Loss: 3.141
Iter 7/2000 - Loss: 2.995
Iter 8/2000 - Loss: 2.903
Iter 9/2000 - Loss: 2.870
Iter 10/2000 - Loss: 2.833
Iter 11/2000 - Loss: 2.738
Iter 12/2000 - Loss: 2.589
Iter 13/2000 - Loss: 2.419
Iter 14/2000 - Loss: 2.251
Iter 15/2000 - Loss: 2.081
Iter 16/2000 - Loss: 1.885
Iter 17/2000 - Loss: 1.648
Iter 18/2000 - Loss: 1.371
Iter 19/2000 - Loss: 1.071
Iter 20/2000 - Loss: 0.761
Iter 1981/2000 - Loss: -7.799
Iter 1982/2000 - Loss: -7.799
Iter 1983/2000 - Loss: -7.799
Iter 1984/2000 - Loss: -7.799
Iter 1985/2000 - Loss: -7.799
Iter 1986/2000 - Loss: -7.799
Iter 1987/2000 - Loss: -7.799
Iter 1988/2000 - Loss: -7.799
Iter 1989/2000 - Loss: -7.799
Iter 1990/2000 - Loss: -7.799
Iter 1991/2000 - Loss: -7.799
Iter 1992/2000 - Loss: -7.799
Iter 1993/2000 - Loss: -7.799
Iter 1994/2000 - Loss: -7.799
Iter 1995/2000 - Loss: -7.799
Iter 1996/2000 - Loss: -7.800
Iter 1997/2000 - Loss: -7.800
Iter 1998/2000 - Loss: -7.800
Iter 1999/2000 - Loss: -7.800
Iter 2000/2000 - Loss: -7.800
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.8160,  5.7611, 23.8963,  1.5506, 15.6330, 46.7608]],

        [[13.4024, 29.0172,  8.7076,  1.5105,  1.0306, 23.4080]],

        [[18.5653, 33.6240,  7.3394,  1.1512,  0.8237, 21.0266]],

        [[14.5267, 25.4016, 15.7956,  2.0918,  1.8930, 44.7215]]])
Signal Variance: tensor([ 0.0538,  1.8844, 11.7706,  0.4519])
Estimated target variance: tensor([0.0108, 0.7197, 6.0590, 0.1376])
N: 180
Signal to noise ratio: tensor([15.5716, 74.5302, 78.9867, 40.4052])
Bound on condition number: tensor([  43646.3601,  999855.6517, 1123002.6611,  293865.4106])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0006951713850928977, policy loss: 10.026719245580221
Experience 18, Iter 1, disc loss: 0.0006644758671709115, policy loss: 10.200446626946462
Experience 18, Iter 2, disc loss: 0.0006341251332760485, policy loss: 10.802803374095188
Experience 18, Iter 3, disc loss: 0.0006587070081079623, policy loss: 10.12206089891122
Experience 18, Iter 4, disc loss: 0.0008221497862647444, policy loss: 8.998443609257073
Experience 18, Iter 5, disc loss: 0.0014134780411383843, policy loss: 7.958520074675118
Experience 18, Iter 6, disc loss: 0.0021025742147459036, policy loss: 9.214283316708757
Experience 18, Iter 7, disc loss: 0.0009761590324862202, policy loss: 8.462092619210578
Experience 18, Iter 8, disc loss: 0.0007736624094599247, policy loss: 9.264049783913602
Experience 18, Iter 9, disc loss: 0.0007233117753661872, policy loss: 9.28016604471618
Experience 18, Iter 10, disc loss: 0.0006815307001350373, policy loss: 9.226016045927235
Experience 18, Iter 11, disc loss: 0.0007480224617657448, policy loss: 9.312551918196494
Experience 18, Iter 12, disc loss: 0.001027065962767873, policy loss: 8.108642588313122
Experience 18, Iter 13, disc loss: 0.0020398787718122467, policy loss: 8.258250128800675
Experience 18, Iter 14, disc loss: 0.0021661343148817484, policy loss: 7.800575956647287
Experience 18, Iter 15, disc loss: 0.0017126821525763332, policy loss: 8.24604106546985
Experience 18, Iter 16, disc loss: 0.0015740482182206444, policy loss: 8.410658515316824
Experience 18, Iter 17, disc loss: 0.0008943663232510898, policy loss: 9.262166806521273
Experience 18, Iter 18, disc loss: 0.0013332539768659882, policy loss: 8.404022591769447
Experience 18, Iter 19, disc loss: 0.0017058144132926383, policy loss: 8.279812366408695
Experience 18, Iter 20, disc loss: 0.0011138241470030528, policy loss: 8.515114684917481
Experience 18, Iter 21, disc loss: 0.0012643079630882312, policy loss: 8.60086963672224
Experience 18, Iter 22, disc loss: 0.0012902685680570957, policy loss: 8.657577393687065
Experience 18, Iter 23, disc loss: 0.001234904487860671, policy loss: 8.7140798890816
Experience 18, Iter 24, disc loss: 0.00163482506372687, policy loss: 8.295800256481645
Experience 18, Iter 25, disc loss: 0.0015371584774232646, policy loss: 9.223100430185195
Experience 18, Iter 26, disc loss: 0.0017430273985101405, policy loss: 8.966334747927014
Experience 18, Iter 27, disc loss: 0.0017277257671894555, policy loss: 8.44938153114547
Experience 18, Iter 28, disc loss: 0.002056293602996495, policy loss: 7.884807267869132
Experience 18, Iter 29, disc loss: 0.0011677751446919889, policy loss: 8.663123290769343
Experience 18, Iter 30, disc loss: 0.0012142243760588306, policy loss: 8.117094006625857
Experience 18, Iter 31, disc loss: 0.001854395714852687, policy loss: 7.829198070414434
Experience 18, Iter 32, disc loss: 0.0015121045942637443, policy loss: 8.2857886252125
Experience 18, Iter 33, disc loss: 0.0025178144204858607, policy loss: 7.925719378486056
Experience 18, Iter 34, disc loss: 0.002169408389324056, policy loss: 8.032788847162946
Experience 18, Iter 35, disc loss: 0.0021160521846118126, policy loss: 8.144801160423022
Experience 18, Iter 36, disc loss: 0.0016835434038281437, policy loss: 8.589522518203914
Experience 18, Iter 37, disc loss: 0.002062551069429738, policy loss: 8.58689351724519
Experience 18, Iter 38, disc loss: 0.002359412880535383, policy loss: 7.941907104119037
Experience 18, Iter 39, disc loss: 0.001794183037582928, policy loss: 7.705545544231644
Experience 18, Iter 40, disc loss: 0.002066776333781421, policy loss: 8.132113715161774
Experience 18, Iter 41, disc loss: 0.0020440191062826977, policy loss: 7.69084032428394
Experience 18, Iter 42, disc loss: 0.0024026351442951004, policy loss: 8.105305131906176
Experience 18, Iter 43, disc loss: 0.002090435162304602, policy loss: 8.016035191350607
Experience 18, Iter 44, disc loss: 0.0019224873113108266, policy loss: 8.750887293301885
Experience 18, Iter 45, disc loss: 0.0020998411539064833, policy loss: 7.982430867880893
Experience 18, Iter 46, disc loss: 0.001775148385064492, policy loss: 8.33228738186625
Experience 18, Iter 47, disc loss: 0.0017050177647235476, policy loss: 8.772722643845821
Experience 18, Iter 48, disc loss: 0.0017054359359718343, policy loss: 8.50834181216125
Experience 18, Iter 49, disc loss: 0.0016473223771281144, policy loss: 8.366407537503584
Experience 18, Iter 50, disc loss: 0.0016894409295909467, policy loss: 8.827265944074037
Experience 18, Iter 51, disc loss: 0.0018025239324114869, policy loss: 8.055100904633496
Experience 18, Iter 52, disc loss: 0.0019059217038051453, policy loss: 8.458552027424714
Experience 18, Iter 53, disc loss: 0.0019914712815087034, policy loss: 8.35221674399816
Experience 18, Iter 54, disc loss: 0.0018096957780954562, policy loss: 8.791315345861111
Experience 18, Iter 55, disc loss: 0.0015893002789095004, policy loss: 8.534621347940911
Experience 18, Iter 56, disc loss: 0.0015442327147853516, policy loss: 8.350552576040617
Experience 18, Iter 57, disc loss: 0.001990506432003364, policy loss: 8.359924509579859
Experience 18, Iter 58, disc loss: 0.002002249890670273, policy loss: 8.230121810175557
Experience 18, Iter 59, disc loss: 0.0018802809805138093, policy loss: 8.344980164456295
Experience 18, Iter 60, disc loss: 0.0019427448555102333, policy loss: 8.55391827252722
Experience 18, Iter 61, disc loss: 0.0018704852209127842, policy loss: 8.458454269653654
Experience 18, Iter 62, disc loss: 0.0018341929838249012, policy loss: 8.65838339727095
Experience 18, Iter 63, disc loss: 0.001993461027757112, policy loss: 8.1931106379152
Experience 18, Iter 64, disc loss: 0.0018512094839985812, policy loss: 8.155876477251443
Experience 18, Iter 65, disc loss: 0.001903236896728575, policy loss: 8.438627435322392
Experience 18, Iter 66, disc loss: 0.00221186725567177, policy loss: 8.526403250826522
Experience 18, Iter 67, disc loss: 0.001543637217659957, policy loss: 8.764780296242865
Experience 18, Iter 68, disc loss: 0.0016961784187641145, policy loss: 8.500257585146135
Experience 18, Iter 69, disc loss: 0.002237079896144364, policy loss: 8.370071978850003
Experience 18, Iter 70, disc loss: 0.002278123085107412, policy loss: 8.287332635915615
Experience 18, Iter 71, disc loss: 0.0020551117387033213, policy loss: 8.791862010146676
Experience 18, Iter 72, disc loss: 0.0019283103602615792, policy loss: 8.48810675230444
Experience 18, Iter 73, disc loss: 0.0018403315738350445, policy loss: 8.555103104606669
Experience 18, Iter 74, disc loss: 0.0016020593511284619, policy loss: 8.604980224757574
Experience 18, Iter 75, disc loss: 0.001621386540808201, policy loss: 8.639527645568702
Experience 18, Iter 76, disc loss: 0.0015777293764421274, policy loss: 8.29684796236291
Experience 18, Iter 77, disc loss: 0.0017093582504964884, policy loss: 8.177031929852497
Experience 18, Iter 78, disc loss: 0.0015979990053354202, policy loss: 8.711355506748337
Experience 18, Iter 79, disc loss: 0.0014236299078052143, policy loss: 8.874123007682797
Experience 18, Iter 80, disc loss: 0.0026916637538462345, policy loss: 8.2513093360778
Experience 18, Iter 81, disc loss: 0.001790018822959182, policy loss: 8.269141003054472
Experience 18, Iter 82, disc loss: 0.001866065645436381, policy loss: 8.094507238886552
Experience 18, Iter 83, disc loss: 0.0013617184136064551, policy loss: 8.653827414223546
Experience 18, Iter 84, disc loss: 0.0016553376998739077, policy loss: 8.594532888299026
Experience 18, Iter 85, disc loss: 0.001499685030107328, policy loss: 8.158769938905758
Experience 18, Iter 86, disc loss: 0.0016676334441623397, policy loss: 8.729754240527221
Experience 18, Iter 87, disc loss: 0.001691539136270995, policy loss: 8.652943331071873
Experience 18, Iter 88, disc loss: 0.0015071753436157972, policy loss: 8.274396167294935
Experience 18, Iter 89, disc loss: 0.0017318692525998196, policy loss: 7.97030781545379
Experience 18, Iter 90, disc loss: 0.0016006251288832197, policy loss: 8.473922654905952
Experience 18, Iter 91, disc loss: 0.0024170775194575023, policy loss: 8.152285552594169
Experience 18, Iter 92, disc loss: 0.0015709585020034769, policy loss: 8.33708011485988
Experience 18, Iter 93, disc loss: 0.0013396521495014518, policy loss: 8.473214294999266
Experience 18, Iter 94, disc loss: 0.001764210585304822, policy loss: 8.221825930828006
Experience 18, Iter 95, disc loss: 0.0016482390109118737, policy loss: 8.27050313283845
Experience 18, Iter 96, disc loss: 0.0014536232643948014, policy loss: 8.456699583726135
Experience 18, Iter 97, disc loss: 0.002384885281056526, policy loss: 7.826626513670142
Experience 18, Iter 98, disc loss: 0.0015391414747945881, policy loss: 8.761787321833728
Experience 18, Iter 99, disc loss: 0.0016509668571941002, policy loss: 8.395051809270601
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1865],
        [1.5533],
        [0.0352]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0106, 0.1191, 1.5943, 0.0271, 0.0234, 4.3692]],

        [[0.0106, 0.1191, 1.5943, 0.0271, 0.0234, 4.3692]],

        [[0.0106, 0.1191, 1.5943, 0.0271, 0.0234, 4.3692]],

        [[0.0106, 0.1191, 1.5943, 0.0271, 0.0234, 4.3692]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.7461, 6.2130, 0.1406], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.7461, 6.2130, 0.1406])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.290
Iter 2/2000 - Loss: 3.491
Iter 3/2000 - Loss: 3.139
Iter 4/2000 - Loss: 3.173
Iter 5/2000 - Loss: 3.257
Iter 6/2000 - Loss: 3.160
Iter 7/2000 - Loss: 3.002
Iter 8/2000 - Loss: 2.892
Iter 9/2000 - Loss: 2.842
Iter 10/2000 - Loss: 2.795
Iter 11/2000 - Loss: 2.695
Iter 12/2000 - Loss: 2.537
Iter 13/2000 - Loss: 2.352
Iter 14/2000 - Loss: 2.164
Iter 15/2000 - Loss: 1.977
Iter 16/2000 - Loss: 1.771
Iter 17/2000 - Loss: 1.528
Iter 18/2000 - Loss: 1.248
Iter 19/2000 - Loss: 0.941
Iter 20/2000 - Loss: 0.622
Iter 1981/2000 - Loss: -7.893
Iter 1982/2000 - Loss: -7.893
Iter 1983/2000 - Loss: -7.893
Iter 1984/2000 - Loss: -7.893
Iter 1985/2000 - Loss: -7.894
Iter 1986/2000 - Loss: -7.894
Iter 1987/2000 - Loss: -7.894
Iter 1988/2000 - Loss: -7.894
Iter 1989/2000 - Loss: -7.894
Iter 1990/2000 - Loss: -7.894
Iter 1991/2000 - Loss: -7.894
Iter 1992/2000 - Loss: -7.894
Iter 1993/2000 - Loss: -7.894
Iter 1994/2000 - Loss: -7.894
Iter 1995/2000 - Loss: -7.894
Iter 1996/2000 - Loss: -7.894
Iter 1997/2000 - Loss: -7.894
Iter 1998/2000 - Loss: -7.894
Iter 1999/2000 - Loss: -7.894
Iter 2000/2000 - Loss: -7.894
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[11.3073,  6.1754, 23.8039,  2.4083,  2.6537, 48.4555]],

        [[15.6345, 30.1661,  8.5839,  1.3738,  1.0491, 24.7778]],

        [[18.6410, 33.3469,  7.0956,  1.0918,  0.8567, 21.9693]],

        [[14.1424, 24.6442, 15.4718,  2.0637,  1.9413, 42.9213]]])
Signal Variance: tensor([ 0.0602,  1.8044, 11.6556,  0.4397])
Estimated target variance: tensor([0.0109, 0.7461, 6.2130, 0.1406])
N: 190
Signal to noise ratio: tensor([16.5187, 72.4883, 79.8397, 40.1551])
Bound on condition number: tensor([  51846.0024,  998364.8584, 1211133.1278,  306363.3448])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.001874128822403263, policy loss: 8.045391864091822
Experience 19, Iter 1, disc loss: 0.0023312861398144744, policy loss: 8.289141894222166
Experience 19, Iter 2, disc loss: 0.0015362231180228027, policy loss: 8.276372566949448
Experience 19, Iter 3, disc loss: 0.0015829842167494014, policy loss: 8.31299593219365
Experience 19, Iter 4, disc loss: 0.001506329979229774, policy loss: 8.24256399675891
Experience 19, Iter 5, disc loss: 0.002018234057208232, policy loss: 8.646088411856073
Experience 19, Iter 6, disc loss: 0.0017185951198804028, policy loss: 8.302228765171904
Experience 19, Iter 7, disc loss: 0.0014259599792708004, policy loss: 8.545523168910677
Experience 19, Iter 8, disc loss: 0.0016527221147551816, policy loss: 8.19645026639035
Experience 19, Iter 9, disc loss: 0.0018895526453741116, policy loss: 7.969654248168501
Experience 19, Iter 10, disc loss: 0.0020300007824172444, policy loss: 8.093211568985737
Experience 19, Iter 11, disc loss: 0.002014254953411013, policy loss: 8.275206983117888
Experience 19, Iter 12, disc loss: 0.002132692830224235, policy loss: 8.798730205487752
Experience 19, Iter 13, disc loss: 0.0015255448774091967, policy loss: 8.247514780625938
Experience 19, Iter 14, disc loss: 0.0016361396856531879, policy loss: 8.561916119425522
Experience 19, Iter 15, disc loss: 0.001390172432404696, policy loss: 8.870873985609652
Experience 19, Iter 16, disc loss: 0.0021821666269564604, policy loss: 8.353827390860054
Experience 19, Iter 17, disc loss: 0.001800988332853091, policy loss: 8.366811907719642
Experience 19, Iter 18, disc loss: 0.001813617709408619, policy loss: 7.998871679049772
Experience 19, Iter 19, disc loss: 0.0021190644851588583, policy loss: 8.843862412049717
Experience 19, Iter 20, disc loss: 0.0019043431145584623, policy loss: 8.441544658548775
Experience 19, Iter 21, disc loss: 0.0014044712613576601, policy loss: 8.476269649050275
Experience 19, Iter 22, disc loss: 0.0016607579453915239, policy loss: 8.283748055657405
Experience 19, Iter 23, disc loss: 0.0016174019610287263, policy loss: 8.310329402845731
Experience 19, Iter 24, disc loss: 0.0017223740793432568, policy loss: 8.149144209496113
Experience 19, Iter 25, disc loss: 0.0017706028101622364, policy loss: 8.49643641460262
Experience 19, Iter 26, disc loss: 0.001890001228941496, policy loss: 8.27143759478631
Experience 19, Iter 27, disc loss: 0.0014667250479609567, policy loss: 8.480427214784651
Experience 19, Iter 28, disc loss: 0.0017142459665732586, policy loss: 8.39486194269513
Experience 19, Iter 29, disc loss: 0.002394266298706346, policy loss: 8.629266436648514
Experience 19, Iter 30, disc loss: 0.0018646234045773934, policy loss: 8.288152812824565
Experience 19, Iter 31, disc loss: 0.001860709542869224, policy loss: 8.87403618077349
Experience 19, Iter 32, disc loss: 0.0015364588686761552, policy loss: 8.641819835710997
Experience 19, Iter 33, disc loss: 0.0018214141608349755, policy loss: 8.154370397755866
Experience 19, Iter 34, disc loss: 0.0020484060771684457, policy loss: 8.28956237814098
Experience 19, Iter 35, disc loss: 0.0018349474153029717, policy loss: 8.425920835378545
Experience 19, Iter 36, disc loss: 0.0018515049332962917, policy loss: 8.203288370418896
Experience 19, Iter 37, disc loss: 0.0015103386194215522, policy loss: 8.392159708201438
Experience 19, Iter 38, disc loss: 0.0016830519112722348, policy loss: 8.469823437686022
Experience 19, Iter 39, disc loss: 0.0016803782909948963, policy loss: 8.488658511196348
Experience 19, Iter 40, disc loss: 0.0017946866270876901, policy loss: 8.19207079809741
Experience 19, Iter 41, disc loss: 0.001446000127757385, policy loss: 8.597844676525963
Experience 19, Iter 42, disc loss: 0.0017815285190607958, policy loss: 8.408293849901169
Experience 19, Iter 43, disc loss: 0.0020280510566117096, policy loss: 8.380186084279167
Experience 19, Iter 44, disc loss: 0.001955481570990499, policy loss: 8.629880035036253
Experience 19, Iter 45, disc loss: 0.0014116880568287807, policy loss: 8.607315465885504
Experience 19, Iter 46, disc loss: 0.0015300820436897427, policy loss: 8.640958927395904
Experience 19, Iter 47, disc loss: 0.0017494606515166028, policy loss: 8.47905081143892
Experience 19, Iter 48, disc loss: 0.0013612297237783344, policy loss: 9.155277771554768
Experience 19, Iter 49, disc loss: 0.0017564451455099037, policy loss: 8.48653737355432
Experience 19, Iter 50, disc loss: 0.001970528066027555, policy loss: 8.227383590282152
Experience 19, Iter 51, disc loss: 0.002075016221713683, policy loss: 8.048617456850645
Experience 19, Iter 52, disc loss: 0.0019139022218405443, policy loss: 8.403820776162519
Experience 19, Iter 53, disc loss: 0.001535577846843033, policy loss: 8.535188487041857
Experience 19, Iter 54, disc loss: 0.0018634952941707173, policy loss: 9.121712435599491
Experience 19, Iter 55, disc loss: 0.001469236144841954, policy loss: 8.688235720745567
Experience 19, Iter 56, disc loss: 0.0016686200177327107, policy loss: 8.87837653649786
Experience 19, Iter 57, disc loss: 0.0015205724583295622, policy loss: 8.47380788544002
Experience 19, Iter 58, disc loss: 0.0015205120570100175, policy loss: 8.563813819222661
Experience 19, Iter 59, disc loss: 0.001903821072002006, policy loss: 8.548271211778257
Experience 19, Iter 60, disc loss: 0.0013410464791696825, policy loss: 8.737367914642117
Experience 19, Iter 61, disc loss: 0.0015032367447761986, policy loss: 8.481116129340482
Experience 19, Iter 62, disc loss: 0.0016846382574672554, policy loss: 8.393667776338063
Experience 19, Iter 63, disc loss: 0.002049159288780234, policy loss: 8.29384004723372
Experience 19, Iter 64, disc loss: 0.0016073710558228006, policy loss: 8.418863663405817
Experience 19, Iter 65, disc loss: 0.0019489535375407754, policy loss: 8.405875464925376
Experience 19, Iter 66, disc loss: 0.0016186221024378294, policy loss: 8.554649201303086
Experience 19, Iter 67, disc loss: 0.001535071142026276, policy loss: 8.330986066289935
Experience 19, Iter 68, disc loss: 0.0016300669549279634, policy loss: 8.634613765259846
Experience 19, Iter 69, disc loss: 0.0017507470976602345, policy loss: 8.723607640701514
Experience 19, Iter 70, disc loss: 0.0019201401605237283, policy loss: 8.212691075644784
Experience 19, Iter 71, disc loss: 0.0020081333804777804, policy loss: 8.436602088896652
Experience 19, Iter 72, disc loss: 0.0014246117161417664, policy loss: 9.103956892908236
Experience 19, Iter 73, disc loss: 0.0015460503496838632, policy loss: 8.622844045651018
Experience 19, Iter 74, disc loss: 0.002324695620121711, policy loss: 8.72949930028725
Experience 19, Iter 75, disc loss: 0.0018887291866971027, policy loss: 8.403221474715034
Experience 19, Iter 76, disc loss: 0.0014721963991797349, policy loss: 8.620221225224995
Experience 19, Iter 77, disc loss: 0.0013174572074317755, policy loss: 9.187974686488733
Experience 19, Iter 78, disc loss: 0.0014857347755483146, policy loss: 8.990922356034886
Experience 19, Iter 79, disc loss: 0.001458980515504571, policy loss: 8.956677152477548
Experience 19, Iter 80, disc loss: 0.0013329622186220714, policy loss: 8.742210080287961
Experience 19, Iter 81, disc loss: 0.0015024198691070737, policy loss: 8.957681689661438
Experience 19, Iter 82, disc loss: 0.0018545046736865214, policy loss: 8.30273301323221
Experience 19, Iter 83, disc loss: 0.0015663072208867068, policy loss: 8.47505753056865
Experience 19, Iter 84, disc loss: 0.0014214704033885038, policy loss: 8.406369832054677
Experience 19, Iter 85, disc loss: 0.001423290130724055, policy loss: 8.089850228470194
Experience 19, Iter 86, disc loss: 0.0019200942722401785, policy loss: 8.906053775748823
Experience 19, Iter 87, disc loss: 0.0014461762427079485, policy loss: 8.482876796745447
Experience 19, Iter 88, disc loss: 0.0020811132466631204, policy loss: 8.869487886825738
Experience 19, Iter 89, disc loss: 0.0013729837253212288, policy loss: 8.700143281682031
Experience 19, Iter 90, disc loss: 0.0014103184103780725, policy loss: 8.66474736198111
Experience 19, Iter 91, disc loss: 0.0016988396964299336, policy loss: 8.530074068616313
Experience 19, Iter 92, disc loss: 0.0019746952071462995, policy loss: 8.105032645293765
Experience 19, Iter 93, disc loss: 0.0015744338662277667, policy loss: 8.681562809097475
Experience 19, Iter 94, disc loss: 0.0012690908144069302, policy loss: 9.162683845349815
Experience 19, Iter 95, disc loss: 0.0016398734703909395, policy loss: 8.991952481997117
Experience 19, Iter 96, disc loss: 0.001440754505631246, policy loss: 8.480865954354883
Experience 19, Iter 97, disc loss: 0.0016091741609696997, policy loss: 8.532711451571068
Experience 19, Iter 98, disc loss: 0.0016529000349356881, policy loss: 8.185510358576805
Experience 19, Iter 99, disc loss: 0.0018386606376792933, policy loss: 8.82954841957189
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1916],
        [1.5850],
        [0.0359]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0103, 0.1203, 1.6285, 0.0277, 0.0240, 4.4963]],

        [[0.0103, 0.1203, 1.6285, 0.0277, 0.0240, 4.4963]],

        [[0.0103, 0.1203, 1.6285, 0.0277, 0.0240, 4.4963]],

        [[0.0103, 0.1203, 1.6285, 0.0277, 0.0240, 4.4963]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.7662, 6.3401, 0.1434], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.7662, 6.3401, 0.1434])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.328
Iter 2/2000 - Loss: 3.535
Iter 3/2000 - Loss: 3.171
Iter 4/2000 - Loss: 3.202
Iter 5/2000 - Loss: 3.290
Iter 6/2000 - Loss: 3.195
Iter 7/2000 - Loss: 3.031
Iter 8/2000 - Loss: 2.910
Iter 9/2000 - Loss: 2.848
Iter 10/2000 - Loss: 2.792
Iter 11/2000 - Loss: 2.687
Iter 12/2000 - Loss: 2.522
Iter 13/2000 - Loss: 2.324
Iter 14/2000 - Loss: 2.119
Iter 15/2000 - Loss: 1.915
Iter 16/2000 - Loss: 1.696
Iter 17/2000 - Loss: 1.445
Iter 18/2000 - Loss: 1.159
Iter 19/2000 - Loss: 0.847
Iter 20/2000 - Loss: 0.522
Iter 1981/2000 - Loss: -7.898
Iter 1982/2000 - Loss: -7.898
Iter 1983/2000 - Loss: -7.898
Iter 1984/2000 - Loss: -7.898
Iter 1985/2000 - Loss: -7.898
Iter 1986/2000 - Loss: -7.898
Iter 1987/2000 - Loss: -7.898
Iter 1988/2000 - Loss: -7.898
Iter 1989/2000 - Loss: -7.898
Iter 1990/2000 - Loss: -7.898
Iter 1991/2000 - Loss: -7.898
Iter 1992/2000 - Loss: -7.898
Iter 1993/2000 - Loss: -7.898
Iter 1994/2000 - Loss: -7.898
Iter 1995/2000 - Loss: -7.898
Iter 1996/2000 - Loss: -7.898
Iter 1997/2000 - Loss: -7.898
Iter 1998/2000 - Loss: -7.898
Iter 1999/2000 - Loss: -7.898
Iter 2000/2000 - Loss: -7.898
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[10.7788,  6.3569, 22.1580,  2.4250,  2.6036, 48.3465]],

        [[15.5530, 29.8720,  8.6990,  1.3143,  1.1404, 27.2621]],

        [[18.9279, 32.4146,  7.2855,  1.0620,  0.8628, 21.7423]],

        [[13.7146, 24.0021, 15.7976,  2.1974,  2.0394, 44.1220]]])
Signal Variance: tensor([ 0.0648,  2.0651, 11.4148,  0.4702])
Estimated target variance: tensor([0.0109, 0.7662, 6.3401, 0.1434])
N: 200
Signal to noise ratio: tensor([17.1078, 77.5575, 74.7414, 39.8995])
Bound on condition number: tensor([  58536.6652, 1203035.3378, 1117257.4811,  318395.0000])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0017892950657668219, policy loss: 9.061994520281466
Experience 20, Iter 1, disc loss: 0.0017149495590340499, policy loss: 8.916288732009452
Experience 20, Iter 2, disc loss: 0.0012534637857828299, policy loss: 8.908072663972396
Experience 20, Iter 3, disc loss: 0.0014461440468561933, policy loss: 8.585231246463174
Experience 20, Iter 4, disc loss: 0.0017213560016041125, policy loss: 8.688423103765306
Experience 20, Iter 5, disc loss: 0.0015362239650646542, policy loss: 8.546783298883815
Experience 20, Iter 6, disc loss: 0.001702045157051896, policy loss: 8.497245307933909
Experience 20, Iter 7, disc loss: 0.0011016309774666943, policy loss: 9.150534933513566
Experience 20, Iter 8, disc loss: 0.001352338757187249, policy loss: 9.03130199098591
Experience 20, Iter 9, disc loss: 0.0016008310003759081, policy loss: 9.033805425316618
Experience 20, Iter 10, disc loss: 0.001515289713472074, policy loss: 8.406043824676507
Experience 20, Iter 11, disc loss: 0.001529971649939789, policy loss: 8.655188725164015
Experience 20, Iter 12, disc loss: 0.0014883405688663341, policy loss: 8.93191049924006
Experience 20, Iter 13, disc loss: 0.0015135024820725866, policy loss: 8.312827983616511
Experience 20, Iter 14, disc loss: 0.0016016889292542525, policy loss: 9.047408287002419
Experience 20, Iter 15, disc loss: 0.0015571724626567071, policy loss: 8.380114134502158
Experience 20, Iter 16, disc loss: 0.002652616152671306, policy loss: 7.977726166817874
Experience 20, Iter 17, disc loss: 0.0017606686289724323, policy loss: 8.668192590870625
Experience 20, Iter 18, disc loss: 0.0016803042599183248, policy loss: 8.739345428174609
Experience 20, Iter 19, disc loss: 0.0014290251480132605, policy loss: 8.6993066948746
Experience 20, Iter 20, disc loss: 0.001514014560159511, policy loss: 8.9508004484098
Experience 20, Iter 21, disc loss: 0.0013820207153569632, policy loss: 9.227226320342933
Experience 20, Iter 22, disc loss: 0.0018897177345892377, policy loss: 8.473553286563835
Experience 20, Iter 23, disc loss: 0.0017065920740724513, policy loss: 8.654368354519951
Experience 20, Iter 24, disc loss: 0.0017496287162240447, policy loss: 9.213652061452656
Experience 20, Iter 25, disc loss: 0.0016410831140815925, policy loss: 9.013475640060058
Experience 20, Iter 26, disc loss: 0.001357725566041391, policy loss: 8.998072047052313
Experience 20, Iter 27, disc loss: 0.001330027229455067, policy loss: 9.178039973685458
Experience 20, Iter 28, disc loss: 0.0015787467126149373, policy loss: 9.075804125422177
Experience 20, Iter 29, disc loss: 0.0014919673477055391, policy loss: 9.262160147204211
Experience 20, Iter 30, disc loss: 0.0017547450682020628, policy loss: 9.029501095611536
Experience 20, Iter 31, disc loss: 0.001381783539940326, policy loss: 8.429691732034314
Experience 20, Iter 32, disc loss: 0.0015935976495474674, policy loss: 9.015158507124308
Experience 20, Iter 33, disc loss: 0.0015751205102827208, policy loss: 8.7696968248748
Experience 20, Iter 34, disc loss: 0.0014932924809926312, policy loss: 8.610360249665849
Experience 20, Iter 35, disc loss: 0.0016479197400835614, policy loss: 8.227999217148952
Experience 20, Iter 36, disc loss: 0.001338053225509213, policy loss: 9.227026383568829
Experience 20, Iter 37, disc loss: 0.0013382336389219224, policy loss: 8.980628607871537
Experience 20, Iter 38, disc loss: 0.0012279851875071693, policy loss: 8.758676183436956
Experience 20, Iter 39, disc loss: 0.0012702597285808833, policy loss: 8.900353372245277
Experience 20, Iter 40, disc loss: 0.001345762748195952, policy loss: 8.736904049647226
Experience 20, Iter 41, disc loss: 0.0018193504814498426, policy loss: 8.296042926503016
Experience 20, Iter 42, disc loss: 0.0013373305533356597, policy loss: 9.164920339485251
Experience 20, Iter 43, disc loss: 0.0014573514115013872, policy loss: 8.543481422656237
Experience 20, Iter 44, disc loss: 0.00139840167472133, policy loss: 8.525463188496932
Experience 20, Iter 45, disc loss: 0.0017113112135981859, policy loss: 8.24771571069437
Experience 20, Iter 46, disc loss: 0.0013485301914299827, policy loss: 8.9235854475689
Experience 20, Iter 47, disc loss: 0.0016693639767428776, policy loss: 8.86658595186734
Experience 20, Iter 48, disc loss: 0.0011702284242879673, policy loss: 9.0802538130758
Experience 20, Iter 49, disc loss: 0.0012634443591387095, policy loss: 9.000529855455408
Experience 20, Iter 50, disc loss: 0.0014214525325300085, policy loss: 8.791924431181858
Experience 20, Iter 51, disc loss: 0.0013251892059186472, policy loss: 8.59940637282158
Experience 20, Iter 52, disc loss: 0.0014650423811047408, policy loss: 8.656227005925103
Experience 20, Iter 53, disc loss: 0.0012042538080264524, policy loss: 8.715976777642418
Experience 20, Iter 54, disc loss: 0.0016780422351244397, policy loss: 8.841246860771825
Experience 20, Iter 55, disc loss: 0.0015667912816685952, policy loss: 8.506679375462722
Experience 20, Iter 56, disc loss: 0.001310661144477786, policy loss: 8.79994924451352
Experience 20, Iter 57, disc loss: 0.0012347441686696714, policy loss: 8.664914279617598
Experience 20, Iter 58, disc loss: 0.00218710618536427, policy loss: 8.649685565913098
Experience 20, Iter 59, disc loss: 0.0017900738550050022, policy loss: 8.87877514766051
Experience 20, Iter 60, disc loss: 0.0018438084391494765, policy loss: 9.23343586842141
Experience 20, Iter 61, disc loss: 0.0013078482544405159, policy loss: 9.003773318491202
Experience 20, Iter 62, disc loss: 0.001263483038350439, policy loss: 8.844856403416472
Experience 20, Iter 63, disc loss: 0.0012157523794897795, policy loss: 9.059856609940244
Experience 20, Iter 64, disc loss: 0.0017990380925799717, policy loss: 8.145823860849994
Experience 20, Iter 65, disc loss: 0.0014000956685778915, policy loss: 8.315248019430477
Experience 20, Iter 66, disc loss: 0.001142509221593911, policy loss: 8.867374993563697
Experience 20, Iter 67, disc loss: 0.0014473786850545199, policy loss: 8.780631673595444
Experience 20, Iter 68, disc loss: 0.0011069362849707122, policy loss: 8.885318466861577
Experience 20, Iter 69, disc loss: 0.001290435693755778, policy loss: 8.84957702822826
Experience 20, Iter 70, disc loss: 0.0013820111487670723, policy loss: 8.799364326903024
Experience 20, Iter 71, disc loss: 0.0010344053440068176, policy loss: 8.266194445995271
Experience 20, Iter 72, disc loss: 0.00618135739298164, policy loss: 9.558013425655352
Experience 20, Iter 73, disc loss: 0.004750531767382339, policy loss: 7.1294297893061
Experience 20, Iter 74, disc loss: 0.006047731143052952, policy loss: 7.060900015769231
Experience 20, Iter 75, disc loss: 0.007704496761483129, policy loss: 6.924822391455677
Experience 20, Iter 76, disc loss: 0.017049111520384214, policy loss: 5.797614652328206
Experience 20, Iter 77, disc loss: 0.023001851928254765, policy loss: 5.608463173650447
Experience 20, Iter 78, disc loss: 0.02664856096226481, policy loss: 5.576034738689195
Experience 20, Iter 79, disc loss: 0.013155899459505802, policy loss: 6.682779934613393
Experience 20, Iter 80, disc loss: 0.011154474209074975, policy loss: 6.738733979878996
Experience 20, Iter 81, disc loss: 0.01356968322738011, policy loss: 6.199158764651012
Experience 20, Iter 82, disc loss: 0.0064700364055437914, policy loss: 7.339691439331435
Experience 20, Iter 83, disc loss: 0.00492890538071882, policy loss: 7.564989585427516
Experience 20, Iter 84, disc loss: 0.003865707234310438, policy loss: 7.776851519650561
Experience 20, Iter 85, disc loss: 0.0029033924250597134, policy loss: 8.637100943881643
Experience 20, Iter 86, disc loss: 0.002741641059283835, policy loss: 8.630069399751417
Experience 20, Iter 87, disc loss: 0.0023552883279038144, policy loss: 9.681453437690855
Experience 20, Iter 88, disc loss: 0.0023561409654520686, policy loss: 9.412849842321716
Experience 20, Iter 89, disc loss: 0.002252895525227579, policy loss: 10.014784308591448
Experience 20, Iter 90, disc loss: 0.0021645926169854947, policy loss: 10.630579232245776
Experience 20, Iter 91, disc loss: 0.002092608928082987, policy loss: 10.682108529429087
Experience 20, Iter 92, disc loss: 0.0020390709374829416, policy loss: 10.591594647643529
Experience 20, Iter 93, disc loss: 0.001918817680494993, policy loss: 11.11355529876772
Experience 20, Iter 94, disc loss: 0.001838290899712953, policy loss: 10.798599821173527
Experience 20, Iter 95, disc loss: 0.0017296448874251685, policy loss: 10.967442729839574
Experience 20, Iter 96, disc loss: 0.0016302434096076887, policy loss: 10.600438492342132
Experience 20, Iter 97, disc loss: 0.0015090579794414928, policy loss: 11.140634084231378
Experience 20, Iter 98, disc loss: 0.0014090825690660914, policy loss: 10.841916068471681
Experience 20, Iter 99, disc loss: 0.0013340386064973192, policy loss: 10.987592585560769
