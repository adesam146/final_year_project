Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0022],
        [0.0965],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0009, 0.0086, 0.3860, 0.0093], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0009, 0.0086, 0.3860, 0.0093])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.623
Iter 2/2000 - Loss: -0.572
Iter 3/2000 - Loss: -2.619
Iter 4/2000 - Loss: -2.810
Iter 5/2000 - Loss: -2.052
Iter 6/2000 - Loss: -2.178
Iter 7/2000 - Loss: -2.770
Iter 8/2000 - Loss: -3.049
Iter 9/2000 - Loss: -2.838
Iter 10/2000 - Loss: -2.539
Iter 11/2000 - Loss: -2.554
Iter 12/2000 - Loss: -2.816
Iter 13/2000 - Loss: -3.004
Iter 14/2000 - Loss: -2.977
Iter 15/2000 - Loss: -2.863
Iter 16/2000 - Loss: -2.812
Iter 17/2000 - Loss: -2.844
Iter 18/2000 - Loss: -2.906
Iter 19/2000 - Loss: -2.962
Iter 20/2000 - Loss: -3.002
Iter 1981/2000 - Loss: -3.234
Iter 1982/2000 - Loss: -3.234
Iter 1983/2000 - Loss: -3.234
Iter 1984/2000 - Loss: -3.234
Iter 1985/2000 - Loss: -3.234
Iter 1986/2000 - Loss: -3.234
Iter 1987/2000 - Loss: -3.234
Iter 1988/2000 - Loss: -3.234
Iter 1989/2000 - Loss: -3.234
Iter 1990/2000 - Loss: -3.234
Iter 1991/2000 - Loss: -3.234
Iter 1992/2000 - Loss: -3.234
Iter 1993/2000 - Loss: -3.234
Iter 1994/2000 - Loss: -3.234
Iter 1995/2000 - Loss: -3.234
Iter 1996/2000 - Loss: -3.234
Iter 1997/2000 - Loss: -3.234
Iter 1998/2000 - Loss: -3.234
Iter 1999/2000 - Loss: -3.234
Iter 2000/2000 - Loss: -3.234
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0015],
        [0.0647],
        [0.0017]])
Lengthscale: tensor([[[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]],

        [[3.8061e-04, 2.4205e-03, 8.1405e-02, 2.1866e-03, 8.8648e-06,
          1.1605e-02]]])
Signal Variance: tensor([0.0006, 0.0062, 0.2827, 0.0067])
Estimated target variance: tensor([0.0009, 0.0086, 0.3860, 0.0093])
N: 10
Signal to noise ratio: tensor([1.9987, 2.0067, 2.0895, 2.0034])
Bound on condition number: tensor([40.9472, 41.2695, 44.6598, 41.1346])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2831570519856585, policy loss: 0.6695059920942437
Experience 1, Iter 1, disc loss: 1.2720327324924265, policy loss: 0.6725531590887273
Experience 1, Iter 2, disc loss: 1.26320613340221, policy loss: 0.673741524653965
Experience 1, Iter 3, disc loss: 1.2564196255242743, policy loss: 0.673245855197065
Experience 1, Iter 4, disc loss: 1.2451234493291286, policy loss: 0.6763616519785249
Experience 1, Iter 5, disc loss: 1.2345355468112649, policy loss: 0.6794354758965442
Experience 1, Iter 6, disc loss: 1.2255946813595844, policy loss: 0.6804496948917972
Experience 1, Iter 7, disc loss: 1.2165947153253727, policy loss: 0.6818491775550866
Experience 1, Iter 8, disc loss: 1.2078278644110103, policy loss: 0.6823044707941892
Experience 1, Iter 9, disc loss: 1.1992756280747554, policy loss: 0.6834071759049614
Experience 1, Iter 10, disc loss: 1.1879236521461207, policy loss: 0.6866889646245569
Experience 1, Iter 11, disc loss: 1.176420811938916, policy loss: 0.6905465461154648
Experience 1, Iter 12, disc loss: 1.1652526619040646, policy loss: 0.6937524372136192
Experience 1, Iter 13, disc loss: 1.1554955495705708, policy loss: 0.695837986971533
Experience 1, Iter 14, disc loss: 1.147968555684617, policy loss: 0.6951389552612893
Experience 1, Iter 15, disc loss: 1.1376839404501773, policy loss: 0.697200814327744
Experience 1, Iter 16, disc loss: 1.1249626293403368, policy loss: 0.7024346387429492
Experience 1, Iter 17, disc loss: 1.1136647614897406, policy loss: 0.7060397912765873
Experience 1, Iter 18, disc loss: 1.1093693459810767, policy loss: 0.7013960222582569
Experience 1, Iter 19, disc loss: 1.098819977351918, policy loss: 0.7042956892028118
Experience 1, Iter 20, disc loss: 1.0885656849799084, policy loss: 0.7066255144234118
Experience 1, Iter 21, disc loss: 1.078586912220679, policy loss: 0.7084223651840073
Experience 1, Iter 22, disc loss: 1.063969209578834, policy loss: 0.7155718304187644
Experience 1, Iter 23, disc loss: 1.0542879398829894, policy loss: 0.7180062804314793
Experience 1, Iter 24, disc loss: 1.0436134557668562, policy loss: 0.7206604353084463
Experience 1, Iter 25, disc loss: 1.0355076389386628, policy loss: 0.720393693291825
Experience 1, Iter 26, disc loss: 1.0274477077656992, policy loss: 0.7214355886898809
Experience 1, Iter 27, disc loss: 1.0195592776433662, policy loss: 0.7202607568759346
Experience 1, Iter 28, disc loss: 1.0065981533051442, policy loss: 0.7269669903357536
Experience 1, Iter 29, disc loss: 0.982731570157424, policy loss: 0.7446451434869599
Experience 1, Iter 30, disc loss: 0.9792085520713784, policy loss: 0.7400283967411847
Experience 1, Iter 31, disc loss: 0.9673431102511015, policy loss: 0.7445945997099853
Experience 1, Iter 32, disc loss: 0.9559914466202236, policy loss: 0.7478387255415094
Experience 1, Iter 33, disc loss: 0.933955487913239, policy loss: 0.7660641421595565
Experience 1, Iter 34, disc loss: 0.9428738324483618, policy loss: 0.7461894871959442
Experience 1, Iter 35, disc loss: 0.9300978826014531, policy loss: 0.7520893653165166
Experience 1, Iter 36, disc loss: 0.9162280258071757, policy loss: 0.7594487204615454
Experience 1, Iter 37, disc loss: 0.8910291736606116, policy loss: 0.781286944863498
Experience 1, Iter 38, disc loss: 0.8963047581030458, policy loss: 0.7659616364301665
Experience 1, Iter 39, disc loss: 0.8855238348478589, policy loss: 0.7697224447183395
Experience 1, Iter 40, disc loss: 0.8762046365972008, policy loss: 0.773669005209567
Experience 1, Iter 41, disc loss: 0.8509208602770948, policy loss: 0.7947502859746961
Experience 1, Iter 42, disc loss: 0.8404390458463548, policy loss: 0.8007240569019227
Experience 1, Iter 43, disc loss: 0.8446739941190128, policy loss: 0.7855638264028932
Experience 1, Iter 44, disc loss: 0.8190438921293354, policy loss: 0.8129342568937138
Experience 1, Iter 45, disc loss: 0.8063970018386755, policy loss: 0.8201725778107063
Experience 1, Iter 46, disc loss: 0.8249774316764604, policy loss: 0.7847608208414731
Experience 1, Iter 47, disc loss: 0.7894767508340809, policy loss: 0.8251148009751585
Experience 1, Iter 48, disc loss: 0.8064714975532677, policy loss: 0.7937934564919189
Experience 1, Iter 49, disc loss: 0.7871200374826486, policy loss: 0.8115022107642385
Experience 1, Iter 50, disc loss: 0.7671420747927271, policy loss: 0.8294985533532955
Experience 1, Iter 51, disc loss: 0.7544091386398464, policy loss: 0.8405819552695843
Experience 1, Iter 52, disc loss: 0.7447113156276854, policy loss: 0.8452545480964606
Experience 1, Iter 53, disc loss: 0.7226783711852819, policy loss: 0.8757346847330806
Experience 1, Iter 54, disc loss: 0.7374272263091435, policy loss: 0.8421747377875837
Experience 1, Iter 55, disc loss: 0.7207239164740045, policy loss: 0.8634237697748565
Experience 1, Iter 56, disc loss: 0.68185798611645, policy loss: 0.9122887103597841
Experience 1, Iter 57, disc loss: 0.6969419411711416, policy loss: 0.8982296153590568
Experience 1, Iter 58, disc loss: 0.6892124237865553, policy loss: 0.8915660737054376
Experience 1, Iter 59, disc loss: 0.6846873445565991, policy loss: 0.894039885975319
Experience 1, Iter 60, disc loss: 0.6841975580829994, policy loss: 0.8857180696045194
Experience 1, Iter 61, disc loss: 0.6344619564414101, policy loss: 0.9642846248641432
Experience 1, Iter 62, disc loss: 0.6679310934245376, policy loss: 0.9004930038535484
Experience 1, Iter 63, disc loss: 0.6629266698223617, policy loss: 0.9171929029932478
Experience 1, Iter 64, disc loss: 0.6789207076054099, policy loss: 0.8671006050180686
Experience 1, Iter 65, disc loss: 0.5904538208344685, policy loss: 1.016429519999008
Experience 1, Iter 66, disc loss: 0.6505590935044258, policy loss: 0.9090971137188807
Experience 1, Iter 67, disc loss: 0.6474850924413804, policy loss: 0.9111929737501796
Experience 1, Iter 68, disc loss: 0.5825134812591706, policy loss: 1.007588562464983
Experience 1, Iter 69, disc loss: 0.5668926223526028, policy loss: 1.0541515062497029
Experience 1, Iter 70, disc loss: 0.6205991831216097, policy loss: 0.9523446607603854
Experience 1, Iter 71, disc loss: 0.5907149100810654, policy loss: 1.0083204372600305
Experience 1, Iter 72, disc loss: 0.5492406184005808, policy loss: 1.06908901980914
Experience 1, Iter 73, disc loss: 0.5514910222241645, policy loss: 1.0667256997052652
Experience 1, Iter 74, disc loss: 0.6154854185837522, policy loss: 0.9358341951496774
Experience 1, Iter 75, disc loss: 0.5226771439224001, policy loss: 1.144679189696743
Experience 1, Iter 76, disc loss: 0.5354574228169667, policy loss: 1.0732436181261535
Experience 1, Iter 77, disc loss: 0.535237185589792, policy loss: 1.0523883718949811
Experience 1, Iter 78, disc loss: 0.513356677749622, policy loss: 1.1156478341713572
Experience 1, Iter 79, disc loss: 0.5194252595095289, policy loss: 1.10326513923766
Experience 1, Iter 80, disc loss: 0.501092592206701, policy loss: 1.128820854571166
Experience 1, Iter 81, disc loss: 0.5277346474300073, policy loss: 1.1178648072119535
Experience 1, Iter 82, disc loss: 0.5323254758574458, policy loss: 1.0815456793154146
Experience 1, Iter 83, disc loss: 0.5159310705647683, policy loss: 1.08902554845553
Experience 1, Iter 84, disc loss: 0.47144125294681605, policy loss: 1.1949566279375141
Experience 1, Iter 85, disc loss: 0.50244097844882, policy loss: 1.1263620470377154
Experience 1, Iter 86, disc loss: 0.46331247885773297, policy loss: 1.2311314613026114
Experience 1, Iter 87, disc loss: 0.4886271751781661, policy loss: 1.1545420518554563
Experience 1, Iter 88, disc loss: 0.47378407479994933, policy loss: 1.2400459960750347
Experience 1, Iter 89, disc loss: 0.4727037377876124, policy loss: 1.2605473679690635
Experience 1, Iter 90, disc loss: 0.49011232032207575, policy loss: 1.200412539306668
Experience 1, Iter 91, disc loss: 0.47358359961663615, policy loss: 1.2523078037918423
Experience 1, Iter 92, disc loss: 0.4424475926836562, policy loss: 1.3123414024840745
Experience 1, Iter 93, disc loss: 0.4412105524510047, policy loss: 1.2876201499192272
Experience 1, Iter 94, disc loss: 0.40246749099750967, policy loss: 1.3840384724858377
Experience 1, Iter 95, disc loss: 0.43438213292669725, policy loss: 1.326284014068451
Experience 1, Iter 96, disc loss: 0.42105146551491446, policy loss: 1.4085828663918616
Experience 1, Iter 97, disc loss: 0.4639135847941069, policy loss: 1.2260459731528912
Experience 1, Iter 98, disc loss: 0.3926301038031517, policy loss: 1.4732981253759707
Experience 1, Iter 99, disc loss: 0.35886930787661975, policy loss: 1.6494762257179871
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0024],
        [0.0969],
        [0.0021]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0009, 0.0096, 0.3877, 0.0082], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0009, 0.0096, 0.3877, 0.0082])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.536
Iter 2/2000 - Loss: -0.511
Iter 3/2000 - Loss: -2.534
Iter 4/2000 - Loss: -2.695
Iter 5/2000 - Loss: -1.952
Iter 6/2000 - Loss: -2.084
Iter 7/2000 - Loss: -2.646
Iter 8/2000 - Loss: -2.925
Iter 9/2000 - Loss: -2.754
Iter 10/2000 - Loss: -2.464
Iter 11/2000 - Loss: -2.425
Iter 12/2000 - Loss: -2.659
Iter 13/2000 - Loss: -2.896
Iter 14/2000 - Loss: -2.921
Iter 15/2000 - Loss: -2.777
Iter 16/2000 - Loss: -2.668
Iter 17/2000 - Loss: -2.705
Iter 18/2000 - Loss: -2.824
Iter 19/2000 - Loss: -2.906
Iter 20/2000 - Loss: -2.907
Iter 1981/2000 - Loss: -3.119
Iter 1982/2000 - Loss: -3.119
Iter 1983/2000 - Loss: -3.119
Iter 1984/2000 - Loss: -3.119
Iter 1985/2000 - Loss: -3.119
Iter 1986/2000 - Loss: -3.119
Iter 1987/2000 - Loss: -3.119
Iter 1988/2000 - Loss: -3.119
Iter 1989/2000 - Loss: -3.119
Iter 1990/2000 - Loss: -3.119
Iter 1991/2000 - Loss: -3.119
Iter 1992/2000 - Loss: -3.119
Iter 1993/2000 - Loss: -3.119
Iter 1994/2000 - Loss: -3.119
Iter 1995/2000 - Loss: -3.119
Iter 1996/2000 - Loss: -3.119
Iter 1997/2000 - Loss: -3.119
Iter 1998/2000 - Loss: -3.119
Iter 1999/2000 - Loss: -3.119
Iter 2000/2000 - Loss: -3.119
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0018],
        [0.0692],
        [0.0016]])
Lengthscale: tensor([[[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]],

        [[1.0851e-03, 2.7698e-03, 8.2835e-02, 2.1677e-03, 1.0714e-05,
          1.4611e-02]]])
Signal Variance: tensor([0.0007, 0.0073, 0.2991, 0.0062])
Estimated target variance: tensor([0.0009, 0.0096, 0.3877, 0.0082])
N: 20
Signal to noise ratio: tensor([2.0029, 2.0049, 2.0798, 2.0021])
Bound on condition number: tensor([81.2294, 81.3951, 87.5155, 81.1642])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.41417560768068173, policy loss: 1.4579080889306704
Experience 2, Iter 1, disc loss: 0.4844120255800237, policy loss: 1.2052586540395231
Experience 2, Iter 2, disc loss: 0.41099098643728693, policy loss: 1.428446883271618
Experience 2, Iter 3, disc loss: 0.4620888143629261, policy loss: 1.252792430365944
Experience 2, Iter 4, disc loss: 0.4182497898448245, policy loss: 1.3663608978814383
Experience 2, Iter 5, disc loss: 0.37799689177799617, policy loss: 1.5184482332690667
Experience 2, Iter 6, disc loss: 0.39738241963277166, policy loss: 1.4625816948042158
Experience 2, Iter 7, disc loss: 0.422294600733788, policy loss: 1.3776492845278374
Experience 2, Iter 8, disc loss: 0.3522511035365779, policy loss: 1.658535366055399
Experience 2, Iter 9, disc loss: 0.40147952664319503, policy loss: 1.4397482979129486
Experience 2, Iter 10, disc loss: 0.40364315469790407, policy loss: 1.4702721526748954
Experience 2, Iter 11, disc loss: 0.3449051371367844, policy loss: 1.664605456206174
Experience 2, Iter 12, disc loss: 0.4247094913166516, policy loss: 1.3965537653155846
Experience 2, Iter 13, disc loss: 0.3884644685393505, policy loss: 1.4237312119466379
Experience 2, Iter 14, disc loss: 0.41330137839957043, policy loss: 1.4745108137394245
Experience 2, Iter 15, disc loss: 0.34817283202037747, policy loss: 1.6870033029217217
Experience 2, Iter 16, disc loss: 0.38888355912109396, policy loss: 1.4196595096383366
Experience 2, Iter 17, disc loss: 0.3315227835198984, policy loss: 1.813737780086266
Experience 2, Iter 18, disc loss: 0.38255075794496907, policy loss: 1.6768166148301524
Experience 2, Iter 19, disc loss: 0.32336691708360354, policy loss: 1.8522944227532576
Experience 2, Iter 20, disc loss: 0.3350116038387611, policy loss: 1.6289891640196061
Experience 2, Iter 21, disc loss: 0.39321279182795266, policy loss: 1.5476415808156814
Experience 2, Iter 22, disc loss: 0.3212581909111725, policy loss: 1.9362697020937762
Experience 2, Iter 23, disc loss: 0.3441528457207573, policy loss: 1.6918600780881707
Experience 2, Iter 24, disc loss: 0.3510341569874633, policy loss: 1.648722206375488
Experience 2, Iter 25, disc loss: 0.32105484741609847, policy loss: 1.6754213777860552
Experience 2, Iter 26, disc loss: 0.35690444519332093, policy loss: 1.6452049339043138
Experience 2, Iter 27, disc loss: 0.27690129554402587, policy loss: 1.9282871262935202
Experience 2, Iter 28, disc loss: 0.35573347931239113, policy loss: 1.7453618517728673
Experience 2, Iter 29, disc loss: 0.287548749561579, policy loss: 2.032361438374149
Experience 2, Iter 30, disc loss: 0.3185457030594673, policy loss: 1.7897317875656742
Experience 2, Iter 31, disc loss: 0.2782961949776233, policy loss: 1.869649273370555
Experience 2, Iter 32, disc loss: 0.3485506486217709, policy loss: 1.7805005181044038
Experience 2, Iter 33, disc loss: 0.3155759978894291, policy loss: 1.9324811146187746
Experience 2, Iter 34, disc loss: 0.30685719501617514, policy loss: 2.1103794015991513
Experience 2, Iter 35, disc loss: 0.30737958520954767, policy loss: 2.0287263382090495
Experience 2, Iter 36, disc loss: 0.30646022752719015, policy loss: 2.059724434581211
Experience 2, Iter 37, disc loss: 0.2676527067956178, policy loss: 2.302559229315033
Experience 2, Iter 38, disc loss: 0.26984908772670463, policy loss: 2.204282126609246
Experience 2, Iter 39, disc loss: 0.252302781554185, policy loss: 2.211801351537444
Experience 2, Iter 40, disc loss: 0.3244991964956828, policy loss: 1.8262770847981429
Experience 2, Iter 41, disc loss: 0.2653450315250032, policy loss: 2.1896852921600223
Experience 2, Iter 42, disc loss: 0.2868607562774695, policy loss: 2.1951585824035518
Experience 2, Iter 43, disc loss: 0.32064912311215016, policy loss: 2.0020854054693
Experience 2, Iter 44, disc loss: 0.3129654776055309, policy loss: 2.0890804512116983
Experience 2, Iter 45, disc loss: 0.2219734604606483, policy loss: 2.336529269297241
Experience 2, Iter 46, disc loss: 0.2664836977784789, policy loss: 2.096191870335382
Experience 2, Iter 47, disc loss: 0.24307058142639934, policy loss: 2.5162141114943797
Experience 2, Iter 48, disc loss: 0.27557517994931924, policy loss: 2.0149573548777413
Experience 2, Iter 49, disc loss: 0.23137655292680512, policy loss: 2.5940018386147123
Experience 2, Iter 50, disc loss: 0.2453278463455301, policy loss: 2.591983195529174
Experience 2, Iter 51, disc loss: 0.23845588914739022, policy loss: 2.4603918356219014
Experience 2, Iter 52, disc loss: 0.2580426796806744, policy loss: 2.3146682090964417
Experience 2, Iter 53, disc loss: 0.22621453774689274, policy loss: 2.435501017420047
Experience 2, Iter 54, disc loss: 0.2519588147673856, policy loss: 2.4447315826110954
Experience 2, Iter 55, disc loss: 0.2501961186790833, policy loss: 2.3475451044401794
Experience 2, Iter 56, disc loss: 0.20376622645320622, policy loss: 2.504850321313073
Experience 2, Iter 57, disc loss: 0.24655872960941072, policy loss: 2.202260647988102
Experience 2, Iter 58, disc loss: 0.22805047515290672, policy loss: 2.5749463219565287
Experience 2, Iter 59, disc loss: 0.1974857872591574, policy loss: 2.495903185835293
Experience 2, Iter 60, disc loss: 0.25126771123904457, policy loss: 2.580152706146848
Experience 2, Iter 61, disc loss: 0.3051398439840043, policy loss: 2.3504804497485465
Experience 2, Iter 62, disc loss: 0.2132386798144326, policy loss: 2.6696065952182346
Experience 2, Iter 63, disc loss: 0.2056361461011341, policy loss: 2.6255034838434814
Experience 2, Iter 64, disc loss: 0.21172795775869807, policy loss: 2.748783416956753
Experience 2, Iter 65, disc loss: 0.21347413726706171, policy loss: 2.791556253266115
Experience 2, Iter 66, disc loss: 0.20398553996159588, policy loss: 2.9719589233109955
Experience 2, Iter 67, disc loss: 0.16424204047405763, policy loss: 3.242730005694535
Experience 2, Iter 68, disc loss: 0.23316476003677827, policy loss: 2.8260174835817833
Experience 2, Iter 69, disc loss: 0.2646058402874756, policy loss: 2.529909303538728
Experience 2, Iter 70, disc loss: 0.23013151213425928, policy loss: 2.507616757080312
Experience 2, Iter 71, disc loss: 0.17186929255591105, policy loss: 3.2803731088784676
Experience 2, Iter 72, disc loss: 0.23507882716296247, policy loss: 2.4859602527964437
Experience 2, Iter 73, disc loss: 0.1936754024654973, policy loss: 2.8856348811705477
Experience 2, Iter 74, disc loss: 0.22887848018683915, policy loss: 2.5590979221031094
Experience 2, Iter 75, disc loss: 0.19129492130555958, policy loss: 2.7005209936939343
Experience 2, Iter 76, disc loss: 0.1985807825365609, policy loss: 2.8268541966491627
Experience 2, Iter 77, disc loss: 0.20210306257769517, policy loss: 2.736805400294963
Experience 2, Iter 78, disc loss: 0.204782191665814, policy loss: 2.8272773982370825
Experience 2, Iter 79, disc loss: 0.21538456129501923, policy loss: 2.867329947095127
Experience 2, Iter 80, disc loss: 0.2064612255373246, policy loss: 2.9329969614725506
Experience 2, Iter 81, disc loss: 0.1594006821820402, policy loss: 3.411991150149951
Experience 2, Iter 82, disc loss: 0.19741569859989958, policy loss: 2.983167916963157
Experience 2, Iter 83, disc loss: 0.2112197056089549, policy loss: 3.100099762913722
Experience 2, Iter 84, disc loss: 0.18887703252843283, policy loss: 2.932973819759424
Experience 2, Iter 85, disc loss: 0.20925223104704851, policy loss: 2.8847596003549114
Experience 2, Iter 86, disc loss: 0.190915869879041, policy loss: 2.5877962275033264
Experience 2, Iter 87, disc loss: 0.15427704842260476, policy loss: 3.461771640908184
Experience 2, Iter 88, disc loss: 0.19235653990471566, policy loss: 2.8963101002995195
Experience 2, Iter 89, disc loss: 0.14546108130164537, policy loss: 3.309716578576507
Experience 2, Iter 90, disc loss: 0.1991126891028313, policy loss: 3.0079443383140747
Experience 2, Iter 91, disc loss: 0.1753045130642944, policy loss: 3.25076439879242
Experience 2, Iter 92, disc loss: 0.1479270845616168, policy loss: 3.4376390088787687
Experience 2, Iter 93, disc loss: 0.17012142856844323, policy loss: 3.456678541131977
Experience 2, Iter 94, disc loss: 0.14886131089058122, policy loss: 3.223097811238935
Experience 2, Iter 95, disc loss: 0.16603787870982537, policy loss: 3.3934988829841863
Experience 2, Iter 96, disc loss: 0.14735720719173243, policy loss: 3.404042850972688
Experience 2, Iter 97, disc loss: 0.15982192818798022, policy loss: 3.424494740206453
Experience 2, Iter 98, disc loss: 0.13319382695759951, policy loss: 3.586391018451187
Experience 2, Iter 99, disc loss: 0.14784210256418848, policy loss: 3.468389752462942
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0024],
        [0.0941],
        [0.0019]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0095, 0.3765, 0.0077], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0095, 0.3765, 0.0077])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.594
Iter 2/2000 - Loss: -0.198
Iter 3/2000 - Loss: -2.586
Iter 4/2000 - Loss: -2.747
Iter 5/2000 - Loss: -1.874
Iter 6/2000 - Loss: -2.044
Iter 7/2000 - Loss: -2.701
Iter 8/2000 - Loss: -3.018
Iter 9/2000 - Loss: -2.825
Iter 10/2000 - Loss: -2.500
Iter 11/2000 - Loss: -2.445
Iter 12/2000 - Loss: -2.692
Iter 13/2000 - Loss: -2.963
Iter 14/2000 - Loss: -3.016
Iter 15/2000 - Loss: -2.870
Iter 16/2000 - Loss: -2.734
Iter 17/2000 - Loss: -2.745
Iter 18/2000 - Loss: -2.863
Iter 19/2000 - Loss: -2.966
Iter 20/2000 - Loss: -2.988
Iter 1981/2000 - Loss: -3.191
Iter 1982/2000 - Loss: -3.210
Iter 1983/2000 - Loss: -3.214
Iter 1984/2000 - Loss: -3.204
Iter 1985/2000 - Loss: -3.195
Iter 1986/2000 - Loss: -3.196
Iter 1987/2000 - Loss: -3.204
Iter 1988/2000 - Loss: -3.212
Iter 1989/2000 - Loss: -3.217
Iter 1990/2000 - Loss: -3.217
Iter 1991/2000 - Loss: -3.214
Iter 1992/2000 - Loss: -3.207
Iter 1993/2000 - Loss: -3.194
Iter 1994/2000 - Loss: -3.172
Iter 1995/2000 - Loss: -3.141
Iter 1996/2000 - Loss: -3.118
Iter 1997/2000 - Loss: -3.140
Iter 1998/2000 - Loss: -3.197
Iter 1999/2000 - Loss: -3.201
Iter 2000/2000 - Loss: -3.166
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0018],
        [0.0686],
        [0.0015]])
Lengthscale: tensor([[[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]],

        [[8.6250e-04, 2.6559e-03, 8.1316e-02, 2.1265e-03, 1.1225e-05,
          1.4030e-02]]])
Signal Variance: tensor([0.0007, 0.0073, 0.2953, 0.0060])
Estimated target variance: tensor([0.0008, 0.0095, 0.3765, 0.0077])
N: 30
Signal to noise ratio: tensor([2.0001, 2.0049, 2.0743, 2.0018])
Bound on condition number: tensor([121.0177, 121.5885, 130.0768, 121.2213])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.1292415651649528, policy loss: 3.5575591558682302
Experience 3, Iter 1, disc loss: 0.154600019566554, policy loss: 3.4326590234209333
Experience 3, Iter 2, disc loss: 0.19095399000750432, policy loss: 3.067260253008712
Experience 3, Iter 3, disc loss: 0.1535366885167443, policy loss: 3.605598330841075
Experience 3, Iter 4, disc loss: 0.15965183168944622, policy loss: 3.165009421115905
Experience 3, Iter 5, disc loss: 0.15729161447773624, policy loss: 3.6438843664179252
Experience 3, Iter 6, disc loss: 0.16276678574720937, policy loss: 3.6188023405829
Experience 3, Iter 7, disc loss: 0.1329222878035208, policy loss: 4.099967993819348
Experience 3, Iter 8, disc loss: 0.17936282215592125, policy loss: 3.2377913528557003
Experience 3, Iter 9, disc loss: 0.16237130349822645, policy loss: 3.6823694160941893
Experience 3, Iter 10, disc loss: 0.13115992710702193, policy loss: 3.6526635485096786
Experience 3, Iter 11, disc loss: 0.1361209083844666, policy loss: 3.411016072110484
Experience 3, Iter 12, disc loss: 0.167754342801261, policy loss: 3.314183453824203
Experience 3, Iter 13, disc loss: 0.1068398596081954, policy loss: 3.844716441182796
Experience 3, Iter 14, disc loss: 0.15896425181138418, policy loss: 3.6323279344089188
Experience 3, Iter 15, disc loss: 0.1210611821960521, policy loss: 4.177242657134364
Experience 3, Iter 16, disc loss: 0.1430842897987386, policy loss: 3.8506575957300266
Experience 3, Iter 17, disc loss: 0.11962063571747854, policy loss: 4.005018352448339
Experience 3, Iter 18, disc loss: 0.11385510099107789, policy loss: 4.0655506865993125
Experience 3, Iter 19, disc loss: 0.15527517233590293, policy loss: 3.738262401271583
Experience 3, Iter 20, disc loss: 0.11857460828327601, policy loss: 4.444431100822992
Experience 3, Iter 21, disc loss: 0.1432134601134615, policy loss: 3.949267212503227
Experience 3, Iter 22, disc loss: 0.13719340675022682, policy loss: 3.7984451765177543
Experience 3, Iter 23, disc loss: 0.1210685299146863, policy loss: 3.9495589252644248
Experience 3, Iter 24, disc loss: 0.14869158815167677, policy loss: 3.6497561946291004
Experience 3, Iter 25, disc loss: 0.12062984864384475, policy loss: 4.014866146597486
Experience 3, Iter 26, disc loss: 0.13358221126803743, policy loss: 4.069977121841182
Experience 3, Iter 27, disc loss: 0.1614437709021749, policy loss: 3.342268806715293
Experience 3, Iter 28, disc loss: 0.14937565648580972, policy loss: 3.341680110981481
Experience 3, Iter 29, disc loss: 0.15171950334925516, policy loss: 3.3828180239357515
Experience 3, Iter 30, disc loss: 0.1398143636019925, policy loss: 3.524869637673137
Experience 3, Iter 31, disc loss: 0.11327159210771064, policy loss: 4.010620507447885
Experience 3, Iter 32, disc loss: 0.11777860801277226, policy loss: 4.306704547515994
Experience 3, Iter 33, disc loss: 0.13307739400856677, policy loss: 3.86225023886072
Experience 3, Iter 34, disc loss: 0.09667369073148596, policy loss: 4.125722727501062
Experience 3, Iter 35, disc loss: 0.12227310578869066, policy loss: 4.2005685393187475
Experience 3, Iter 36, disc loss: 0.15044948769214453, policy loss: 3.3770369125183723
Experience 3, Iter 37, disc loss: 0.11620201280896095, policy loss: 4.684136110338282
Experience 3, Iter 38, disc loss: 0.106538482095175, policy loss: 4.671265651781968
Experience 3, Iter 39, disc loss: 0.10242824571770769, policy loss: 4.858351212541343
Experience 3, Iter 40, disc loss: 0.10821655739084879, policy loss: 4.668270474693588
Experience 3, Iter 41, disc loss: 0.09541002508981145, policy loss: 4.24779269974333
Experience 3, Iter 42, disc loss: 0.09179300594199113, policy loss: 4.665595159775281
Experience 3, Iter 43, disc loss: 0.09133900016818634, policy loss: 4.4864803915697955
Experience 3, Iter 44, disc loss: 0.08467703536457616, policy loss: 4.427629361472034
Experience 3, Iter 45, disc loss: 0.1237762226635138, policy loss: 3.9142460585566026
Experience 3, Iter 46, disc loss: 0.09795354172975838, policy loss: 4.691969542009527
Experience 3, Iter 47, disc loss: 0.11636520335240261, policy loss: 4.750016233470957
Experience 3, Iter 48, disc loss: 0.0880998211954806, policy loss: 4.849320778091263
Experience 3, Iter 49, disc loss: 0.08720309082491558, policy loss: 4.311298921140406
Experience 3, Iter 50, disc loss: 0.07148154585283231, policy loss: 4.9478585176175685
Experience 3, Iter 51, disc loss: 0.11458123530448003, policy loss: 4.366399848220221
Experience 3, Iter 52, disc loss: 0.1385419987299545, policy loss: 4.004591314952206
Experience 3, Iter 53, disc loss: 0.07136118304130934, policy loss: 5.180803496825326
Experience 3, Iter 54, disc loss: 0.13605172317622358, policy loss: 4.257460669027476
Experience 3, Iter 55, disc loss: 0.09778210062145684, policy loss: 4.8265567318170195
Experience 3, Iter 56, disc loss: 0.11088298643877094, policy loss: 3.9633630113858422
Experience 3, Iter 57, disc loss: 0.09291466949580422, policy loss: 4.226524966368034
Experience 3, Iter 58, disc loss: 0.08333533530064, policy loss: 4.910898124430046
Experience 3, Iter 59, disc loss: 0.08274822074446585, policy loss: 4.407085396611385
Experience 3, Iter 60, disc loss: 0.07512313866885983, policy loss: 5.096574175242724
Experience 3, Iter 61, disc loss: 0.07665638796352157, policy loss: 4.852189065395234
Experience 3, Iter 62, disc loss: 0.06336885253371602, policy loss: 4.90808875224215
Experience 3, Iter 63, disc loss: 0.06855796151745217, policy loss: 4.946343391035112
Experience 3, Iter 64, disc loss: 0.0630246629776144, policy loss: 4.761651124014656
Experience 3, Iter 65, disc loss: 0.05460516267691937, policy loss: 5.558060897244548
Experience 3, Iter 66, disc loss: 0.05227169020047498, policy loss: 5.652626086321456
Experience 3, Iter 67, disc loss: 0.08259431029466004, policy loss: 4.856896548716316
Experience 3, Iter 68, disc loss: 0.08546970720244616, policy loss: 4.668164514914965
Experience 3, Iter 69, disc loss: 0.0674262092880714, policy loss: 5.06253352403248
Experience 3, Iter 70, disc loss: 0.08596331301150086, policy loss: 4.194728292299384
Experience 3, Iter 71, disc loss: 0.0716884718864391, policy loss: 4.713589813781558
Experience 3, Iter 72, disc loss: 0.06927276022310623, policy loss: 4.818959528424571
Experience 3, Iter 73, disc loss: 0.05174735806469859, policy loss: 5.293493981770823
Experience 3, Iter 74, disc loss: 0.08074936431541957, policy loss: 4.289596568514685
Experience 3, Iter 75, disc loss: 0.05367527029559216, policy loss: 5.213888888430634
Experience 3, Iter 76, disc loss: 0.07951706284630816, policy loss: 5.325930010316003
Experience 3, Iter 77, disc loss: 0.06306858409259689, policy loss: 5.22222566374376
Experience 3, Iter 78, disc loss: 0.06404749749474865, policy loss: 4.9977939647212
Experience 3, Iter 79, disc loss: 0.07360562919424112, policy loss: 4.815742458000409
Experience 3, Iter 80, disc loss: 0.061650227591289054, policy loss: 5.548747122194204
Experience 3, Iter 81, disc loss: 0.05586838446696507, policy loss: 5.217933979065549
Experience 3, Iter 82, disc loss: 0.055533586857828304, policy loss: 4.525490126783341
Experience 3, Iter 83, disc loss: 0.08244319569565316, policy loss: 4.986597566694522
Experience 3, Iter 84, disc loss: 0.04832404408464057, policy loss: 5.361101135413077
Experience 3, Iter 85, disc loss: 0.05099272361632921, policy loss: 5.123021387541019
Experience 3, Iter 86, disc loss: 0.0626302864786234, policy loss: 5.061107921669853
Experience 3, Iter 87, disc loss: 0.054827637386460314, policy loss: 5.746549607233483
Experience 3, Iter 88, disc loss: 0.06304541264731366, policy loss: 5.26078246799789
Experience 3, Iter 89, disc loss: 0.05998798440367076, policy loss: 5.051885735657877
Experience 3, Iter 90, disc loss: 0.047487283628585224, policy loss: 5.31286845202587
Experience 3, Iter 91, disc loss: 0.06971209375029414, policy loss: 5.270881111271972
Experience 3, Iter 92, disc loss: 0.05060273213424256, policy loss: 5.23674921892543
Experience 3, Iter 93, disc loss: 0.04851001667769532, policy loss: 5.653316969137277
Experience 3, Iter 94, disc loss: 0.04699033711542062, policy loss: 5.3652077652342784
Experience 3, Iter 95, disc loss: 0.04325879358327771, policy loss: 5.351069427916398
Experience 3, Iter 96, disc loss: 0.060460374895803824, policy loss: 5.653902163122005
Experience 3, Iter 97, disc loss: 0.06796392225164403, policy loss: 4.906403712397822
Experience 3, Iter 98, disc loss: 0.06581466349746062, policy loss: 5.556808904970695
Experience 3, Iter 99, disc loss: 0.08660054875477924, policy loss: 4.479244328262886
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0023],
        [0.0850],
        [0.0017]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0011, 0.0091, 0.3399, 0.0070], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0011, 0.0091, 0.3399, 0.0070])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.354
Iter 2/2000 - Loss: -1.178
Iter 3/2000 - Loss: -2.471
Iter 4/2000 - Loss: -2.771
Iter 5/2000 - Loss: -2.274
Iter 6/2000 - Loss: -2.296
Iter 7/2000 - Loss: -2.672
Iter 8/2000 - Loss: -2.882
Iter 9/2000 - Loss: -2.801
Iter 10/2000 - Loss: -2.633
Iter 11/2000 - Loss: -2.599
Iter 12/2000 - Loss: -2.719
Iter 13/2000 - Loss: -2.877
Iter 14/2000 - Loss: -2.948
Iter 15/2000 - Loss: -2.888
Iter 16/2000 - Loss: -2.786
Iter 17/2000 - Loss: -2.779
Iter 18/2000 - Loss: -2.895
Iter 19/2000 - Loss: -3.015
Iter 20/2000 - Loss: -3.019
Iter 1981/2000 - Loss: -3.150
Iter 1982/2000 - Loss: -3.150
Iter 1983/2000 - Loss: -3.149
Iter 1984/2000 - Loss: -3.150
Iter 1985/2000 - Loss: -3.150
Iter 1986/2000 - Loss: -3.150
Iter 1987/2000 - Loss: -3.150
Iter 1988/2000 - Loss: -3.150
Iter 1989/2000 - Loss: -3.150
Iter 1990/2000 - Loss: -3.150
Iter 1991/2000 - Loss: -3.150
Iter 1992/2000 - Loss: -3.150
Iter 1993/2000 - Loss: -3.150
Iter 1994/2000 - Loss: -3.150
Iter 1995/2000 - Loss: -3.150
Iter 1996/2000 - Loss: -3.150
Iter 1997/2000 - Loss: -3.150
Iter 1998/2000 - Loss: -3.150
Iter 1999/2000 - Loss: -3.150
Iter 2000/2000 - Loss: -3.150
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0018],
        [0.0629],
        [0.0014]])
Lengthscale: tensor([[[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]],

        [[2.0637e-03, 6.6453e-03, 7.3792e-02, 1.9985e-03, 1.3930e-05,
          2.0286e-02]]])
Signal Variance: tensor([0.0009, 0.0071, 0.2686, 0.0055])
Estimated target variance: tensor([0.0011, 0.0091, 0.3399, 0.0070])
N: 40
Signal to noise ratio: tensor([1.9997, 2.0032, 2.0658, 2.0018])
Bound on condition number: tensor([160.9591, 161.5201, 171.7026, 161.2806])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.05166753593135717, policy loss: 5.395199564546723
Experience 4, Iter 1, disc loss: 0.052617069307405326, policy loss: 5.514984478753999
Experience 4, Iter 2, disc loss: 0.055306820386006615, policy loss: 5.339602631714163
Experience 4, Iter 3, disc loss: 0.04587240015896757, policy loss: 5.69473004434534
Experience 4, Iter 4, disc loss: 0.04361851605719481, policy loss: 5.931807868400131
Experience 4, Iter 5, disc loss: 0.05295251799409028, policy loss: 5.586242881116194
Experience 4, Iter 6, disc loss: 0.035772298470698384, policy loss: 6.108185005735409
Experience 4, Iter 7, disc loss: 0.043814012883001796, policy loss: 5.867048409742534
Experience 4, Iter 8, disc loss: 0.04691531254972453, policy loss: 5.333836358163776
Experience 4, Iter 9, disc loss: 0.044590132727298046, policy loss: 6.136973743734815
Experience 4, Iter 10, disc loss: 0.04150173058546067, policy loss: 5.8483956965398285
Experience 4, Iter 11, disc loss: 0.04120346184844877, policy loss: 6.462986082044851
Experience 4, Iter 12, disc loss: 0.028934408529366774, policy loss: 6.223331571964129
Experience 4, Iter 13, disc loss: 0.030593167280655576, policy loss: 5.783151162197333
Experience 4, Iter 14, disc loss: 0.03966690911738585, policy loss: 5.800170077616596
Experience 4, Iter 15, disc loss: 0.039546885973173795, policy loss: 6.201394054053297
Experience 4, Iter 16, disc loss: 0.0332680665605187, policy loss: 5.564802360413084
Experience 4, Iter 17, disc loss: 0.04540009059633617, policy loss: 5.370183977270662
Experience 4, Iter 18, disc loss: 0.040889098906188796, policy loss: 5.866371208117099
Experience 4, Iter 19, disc loss: 0.05179673318861273, policy loss: 5.746654470778987
Experience 4, Iter 20, disc loss: 0.037958101615952804, policy loss: 6.49981270838217
Experience 4, Iter 21, disc loss: 0.036286980410340466, policy loss: 5.551774343876862
Experience 4, Iter 22, disc loss: 0.044455630073170445, policy loss: 5.467484391164272
Experience 4, Iter 23, disc loss: 0.031302862870176605, policy loss: 5.880943653732165
Experience 4, Iter 24, disc loss: 0.033063683726702535, policy loss: 6.170914792748433
Experience 4, Iter 25, disc loss: 0.034450119640350996, policy loss: 6.056055611663537
Experience 4, Iter 26, disc loss: 0.029491039517034422, policy loss: 5.438993185752666
Experience 4, Iter 27, disc loss: 0.028164355734133145, policy loss: 5.577219444265218
Experience 4, Iter 28, disc loss: 0.024348964086039555, policy loss: 6.376243920340858
Experience 4, Iter 29, disc loss: 0.030102680640986295, policy loss: 6.247465286641489
Experience 4, Iter 30, disc loss: 0.036292932591700224, policy loss: 5.288459025109046
Experience 4, Iter 31, disc loss: 0.02507594534237094, policy loss: 6.204626113072681
Experience 4, Iter 32, disc loss: 0.04272802322896819, policy loss: 6.207865673191346
Experience 4, Iter 33, disc loss: 0.02840367993040556, policy loss: 6.098297465299398
Experience 4, Iter 34, disc loss: 0.025996251290673875, policy loss: 5.975091687944101
Experience 4, Iter 35, disc loss: 0.03326959896573205, policy loss: 5.85430456909461
Experience 4, Iter 36, disc loss: 0.036774186810980485, policy loss: 5.661956063306619
Experience 4, Iter 37, disc loss: 0.04266709186618557, policy loss: 5.912022260042841
Experience 4, Iter 38, disc loss: 0.019025109821401057, policy loss: 7.127823401795852
Experience 4, Iter 39, disc loss: 0.03122877194085921, policy loss: 6.347192273508966
Experience 4, Iter 40, disc loss: 0.05278656059680418, policy loss: 5.187317260447042
Experience 4, Iter 41, disc loss: 0.021750251837995477, policy loss: 6.975669598689747
Experience 4, Iter 42, disc loss: 0.033859606200637014, policy loss: 6.9633674433992745
Experience 4, Iter 43, disc loss: 0.02767314316473328, policy loss: 6.509028844086898
Experience 4, Iter 44, disc loss: 0.035947245169859934, policy loss: 5.66822674203962
Experience 4, Iter 45, disc loss: 0.025945471752687223, policy loss: 6.68782851084134
Experience 4, Iter 46, disc loss: 0.018769517801778882, policy loss: 7.275201714104899
Experience 4, Iter 47, disc loss: 0.0266969814324836, policy loss: 5.742159474457919
Experience 4, Iter 48, disc loss: 0.018418383380717142, policy loss: 6.715576466382571
Experience 4, Iter 49, disc loss: 0.026858036427264684, policy loss: 6.653759576628658
Experience 4, Iter 50, disc loss: 0.024156224698728895, policy loss: 6.421860860740609
Experience 4, Iter 51, disc loss: 0.029754055560944355, policy loss: 6.484754769297269
Experience 4, Iter 52, disc loss: 0.0304000896118742, policy loss: 5.914113581456853
Experience 4, Iter 53, disc loss: 0.020375109673731146, policy loss: 7.044607929889282
Experience 4, Iter 54, disc loss: 0.022610618256199366, policy loss: 6.056456980668621
Experience 4, Iter 55, disc loss: 0.04500286523433441, policy loss: 5.771257879139336
Experience 4, Iter 56, disc loss: 0.013830003879741816, policy loss: 7.0120712067092565
Experience 4, Iter 57, disc loss: 0.019838022699257864, policy loss: 7.606703828923742
Experience 4, Iter 58, disc loss: 0.03399982775507662, policy loss: 5.1949223582578465
Experience 4, Iter 59, disc loss: 0.03786004469448669, policy loss: 6.638439333349224
Experience 4, Iter 60, disc loss: 0.01745845460203628, policy loss: 7.006563742976475
Experience 4, Iter 61, disc loss: 0.01710194729417367, policy loss: 7.14553710112083
Experience 4, Iter 62, disc loss: 0.012293207547029507, policy loss: 7.099478807998668
Experience 4, Iter 63, disc loss: 0.02777233597187217, policy loss: 6.461952927871578
Experience 4, Iter 64, disc loss: 0.021183931317271643, policy loss: 7.166677264628007
Experience 4, Iter 65, disc loss: 0.02690979848721123, policy loss: 6.605425687794625
Experience 4, Iter 66, disc loss: 0.015865881584945948, policy loss: 6.412669204101991
Experience 4, Iter 67, disc loss: 0.024587286994289793, policy loss: 6.209188893066367
Experience 4, Iter 68, disc loss: 0.023121979013206756, policy loss: 6.189090365919388
Experience 4, Iter 69, disc loss: 0.02853336236680191, policy loss: 6.090292402295033
Experience 4, Iter 70, disc loss: 0.01457402859513695, policy loss: 6.873924132489693
Experience 4, Iter 71, disc loss: 0.01694178147414851, policy loss: 6.673896118821855
Experience 4, Iter 72, disc loss: 0.01477143002688682, policy loss: 6.638983799554047
Experience 4, Iter 73, disc loss: 0.011921006042128118, policy loss: 7.709426344169598
Experience 4, Iter 74, disc loss: 0.04376603117608246, policy loss: 6.191801098463387
Experience 4, Iter 75, disc loss: 0.019194178337344485, policy loss: 6.845509680803961
Experience 4, Iter 76, disc loss: 0.01971729569744972, policy loss: 6.202438356719419
Experience 4, Iter 77, disc loss: 0.021743333623379973, policy loss: 6.941437017428742
Experience 4, Iter 78, disc loss: 0.02135849551652478, policy loss: 6.436570752005231
Experience 4, Iter 79, disc loss: 0.020265819868858184, policy loss: 6.863125217935582
Experience 4, Iter 80, disc loss: 0.02877511488081751, policy loss: 5.703665948474853
Experience 4, Iter 81, disc loss: 0.020000358219985435, policy loss: 6.889133711921657
Experience 4, Iter 82, disc loss: 0.019465918016294124, policy loss: 6.761470295129088
Experience 4, Iter 83, disc loss: 0.020027507032367702, policy loss: 6.984462631828443
Experience 4, Iter 84, disc loss: 0.015612276235835155, policy loss: 6.903838812789721
Experience 4, Iter 85, disc loss: 0.020282804347329195, policy loss: 6.192207818766842
Experience 4, Iter 86, disc loss: 0.01849451603211378, policy loss: 7.573208510929771
Experience 4, Iter 87, disc loss: 0.022703831291067747, policy loss: 6.984092456225196
Experience 4, Iter 88, disc loss: 0.013144222744521452, policy loss: 6.644134432934008
Experience 4, Iter 89, disc loss: 0.015721503030966068, policy loss: 6.912428997691357
Experience 4, Iter 90, disc loss: 0.014030371463693622, policy loss: 6.8974086245795
Experience 4, Iter 91, disc loss: 0.015023069614610833, policy loss: 6.867664547005354
Experience 4, Iter 92, disc loss: 0.015723735151403694, policy loss: 7.14116863724961
Experience 4, Iter 93, disc loss: 0.018280894484092935, policy loss: 6.73143460903073
Experience 4, Iter 94, disc loss: 0.014927460456897253, policy loss: 6.829232653213103
Experience 4, Iter 95, disc loss: 0.011870315486065498, policy loss: 8.438495787857047
Experience 4, Iter 96, disc loss: 0.01430986730038769, policy loss: 6.99665348234243
Experience 4, Iter 97, disc loss: 0.01989074012693218, policy loss: 6.705247162735902
Experience 4, Iter 98, disc loss: 0.010972166267443996, policy loss: 7.395395935435184
Experience 4, Iter 99, disc loss: 0.013289589931129171, policy loss: 7.646087356251078
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0023],
        [0.0698],
        [0.0014]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0018, 0.0091, 0.2791, 0.0057], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0018, 0.0091, 0.2791, 0.0057])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.772
Iter 2/2000 - Loss: -0.838
Iter 3/2000 - Loss: -2.759
Iter 4/2000 - Loss: -2.597
Iter 5/2000 - Loss: -1.975
Iter 6/2000 - Loss: -2.234
Iter 7/2000 - Loss: -2.760
Iter 8/2000 - Loss: -2.970
Iter 9/2000 - Loss: -2.782
Iter 10/2000 - Loss: -2.513
Iter 11/2000 - Loss: -2.490
Iter 12/2000 - Loss: -2.710
Iter 13/2000 - Loss: -2.923
Iter 14/2000 - Loss: -2.953
Iter 15/2000 - Loss: -2.841
Iter 16/2000 - Loss: -2.741
Iter 17/2000 - Loss: -2.746
Iter 18/2000 - Loss: -2.837
Iter 19/2000 - Loss: -2.936
Iter 20/2000 - Loss: -2.976
Iter 1981/2000 - Loss: -3.105
Iter 1982/2000 - Loss: -3.106
Iter 1983/2000 - Loss: -3.106
Iter 1984/2000 - Loss: -3.106
Iter 1985/2000 - Loss: -3.106
Iter 1986/2000 - Loss: -3.105
Iter 1987/2000 - Loss: -3.105
Iter 1988/2000 - Loss: -3.105
Iter 1989/2000 - Loss: -3.105
Iter 1990/2000 - Loss: -3.105
Iter 1991/2000 - Loss: -3.106
Iter 1992/2000 - Loss: -3.106
Iter 1993/2000 - Loss: -3.106
Iter 1994/2000 - Loss: -3.106
Iter 1995/2000 - Loss: -3.106
Iter 1996/2000 - Loss: -3.106
Iter 1997/2000 - Loss: -3.106
Iter 1998/2000 - Loss: -3.106
Iter 1999/2000 - Loss: -3.105
Iter 2000/2000 - Loss: -3.105
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0018],
        [0.0524],
        [0.0011]])
Lengthscale: tensor([[[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]],

        [[2.9346e-03, 1.1973e-02, 5.9896e-02, 1.8213e-03, 1.2776e-05,
          3.6072e-02]]])
Signal Variance: tensor([0.0014, 0.0072, 0.2211, 0.0044])
Estimated target variance: tensor([0.0018, 0.0091, 0.2791, 0.0057])
N: 50
Signal to noise ratio: tensor([2.0003, 2.0052, 2.0533, 2.0020])
Bound on condition number: tensor([201.0564, 202.0386, 211.7963, 201.4058])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.029734714848213453, policy loss: 5.690138355413336
Experience 5, Iter 1, disc loss: 0.021385733257768484, policy loss: 6.172722743985374
Experience 5, Iter 2, disc loss: 0.016593850225466215, policy loss: 5.928885762117948
Experience 5, Iter 3, disc loss: 0.01641181981156926, policy loss: 5.841129921582716
Experience 5, Iter 4, disc loss: 0.01975997918353686, policy loss: 6.757359557651795
Experience 5, Iter 5, disc loss: 0.01757755084746803, policy loss: 6.541432260366674
Experience 5, Iter 6, disc loss: 0.02323135080149669, policy loss: 5.6284391237495015
Experience 5, Iter 7, disc loss: 0.03230848679580117, policy loss: 5.383530397262333
Experience 5, Iter 8, disc loss: 0.01901482537235886, policy loss: 6.688120991584425
Experience 5, Iter 9, disc loss: 0.015408000582560252, policy loss: 6.60566556145986
Experience 5, Iter 10, disc loss: 0.023807520164854972, policy loss: 5.713833406066637
Experience 5, Iter 11, disc loss: 0.015290933036061924, policy loss: 6.367136941200979
Experience 5, Iter 12, disc loss: 0.020442409259728088, policy loss: 5.853440623732366
Experience 5, Iter 13, disc loss: 0.014915482509931428, policy loss: 6.340315543714835
Experience 5, Iter 14, disc loss: 0.01411145756164174, policy loss: 6.83150310446466
Experience 5, Iter 15, disc loss: 0.012615206248273317, policy loss: 7.5711075440451765
Experience 5, Iter 16, disc loss: 0.014177663212296583, policy loss: 6.642858655214962
Experience 5, Iter 17, disc loss: 0.02229222456164451, policy loss: 6.042973869528806
Experience 5, Iter 18, disc loss: 0.023886609117300842, policy loss: 6.119341346949599
Experience 5, Iter 19, disc loss: 0.022471415235129105, policy loss: 6.408436006048261
Experience 5, Iter 20, disc loss: 0.019337709786786936, policy loss: 6.506767623307272
Experience 5, Iter 21, disc loss: 0.017102077884793877, policy loss: 6.912505460751027
Experience 5, Iter 22, disc loss: 0.019142724525945023, policy loss: 6.477521469641856
Experience 5, Iter 23, disc loss: 0.015721240683881144, policy loss: 6.180749402661465
Experience 5, Iter 24, disc loss: 0.019629632669785645, policy loss: 6.177529050973181
Experience 5, Iter 25, disc loss: 0.024793943173698516, policy loss: 6.292454419137821
Experience 5, Iter 26, disc loss: 0.019243633059428254, policy loss: 6.44323478695314
Experience 5, Iter 27, disc loss: 0.02240047042789397, policy loss: 6.467881931870881
Experience 5, Iter 28, disc loss: 0.020552289600959693, policy loss: 5.797333555772525
Experience 5, Iter 29, disc loss: 0.015117468749124159, policy loss: 6.3972994048078755
Experience 5, Iter 30, disc loss: 0.014126443934767774, policy loss: 6.258103510144173
Experience 5, Iter 31, disc loss: 0.01584590595763375, policy loss: 6.0688410146240805
Experience 5, Iter 32, disc loss: 0.01349625383654173, policy loss: 6.597001486215376
Experience 5, Iter 33, disc loss: 0.0217370522015962, policy loss: 6.856905664619768
Experience 5, Iter 34, disc loss: 0.016548760393645785, policy loss: 6.514536932396554
Experience 5, Iter 35, disc loss: 0.016924533866585996, policy loss: 6.737347953907967
Experience 5, Iter 36, disc loss: 0.02225096258960907, policy loss: 6.0933607569864705
Experience 5, Iter 37, disc loss: 0.02136681777525966, policy loss: 5.732654596788316
Experience 5, Iter 38, disc loss: 0.01735646570561708, policy loss: 6.354250530048743
Experience 5, Iter 39, disc loss: 0.012297226443532754, policy loss: 6.7117311283763055
Experience 5, Iter 40, disc loss: 0.021729785031409644, policy loss: 6.441328213261262
Experience 5, Iter 41, disc loss: 0.021978547119072697, policy loss: 5.829623435594793
Experience 5, Iter 42, disc loss: 0.008104760771867999, policy loss: 7.11688386904472
Experience 5, Iter 43, disc loss: 0.019044112841673892, policy loss: 5.890583823264767
Experience 5, Iter 44, disc loss: 0.012217032845004331, policy loss: 6.889286880610444
Experience 5, Iter 45, disc loss: 0.015028472804177307, policy loss: 6.335958064303882
Experience 5, Iter 46, disc loss: 0.013945406442303004, policy loss: 7.0949277697853095
Experience 5, Iter 47, disc loss: 0.010538969548739854, policy loss: 6.620302894014451
Experience 5, Iter 48, disc loss: 0.015540309792001446, policy loss: 6.218757115747536
Experience 5, Iter 49, disc loss: 0.016724253384975393, policy loss: 6.607781045989736
Experience 5, Iter 50, disc loss: 0.01493587704305261, policy loss: 6.551836494403793
Experience 5, Iter 51, disc loss: 0.016780564880825036, policy loss: 6.6282835282145545
Experience 5, Iter 52, disc loss: 0.014376240831257922, policy loss: 6.602689708077797
Experience 5, Iter 53, disc loss: 0.014090313373826605, policy loss: 6.843078820500045
Experience 5, Iter 54, disc loss: 0.011813084330116488, policy loss: 7.216130079876324
Experience 5, Iter 55, disc loss: 0.010250072341751562, policy loss: 6.377937930414125
Experience 5, Iter 56, disc loss: 0.01109236565108347, policy loss: 6.8521643674771155
Experience 5, Iter 57, disc loss: 0.010231149524671676, policy loss: 6.894958416533141
Experience 5, Iter 58, disc loss: 0.009053386868266733, policy loss: 6.9327307391918165
Experience 5, Iter 59, disc loss: 0.009532262117135992, policy loss: 7.364433550945004
Experience 5, Iter 60, disc loss: 0.012978893181658792, policy loss: 6.426293054537757
Experience 5, Iter 61, disc loss: 0.008463501062470907, policy loss: 7.201094110657307
Experience 5, Iter 62, disc loss: 0.00983666984249762, policy loss: 6.26216471072728
Experience 5, Iter 63, disc loss: 0.012146672453947637, policy loss: 6.6492818587713565
Experience 5, Iter 64, disc loss: 0.012667784094643567, policy loss: 6.732601955993956
Experience 5, Iter 65, disc loss: 0.013807493231094833, policy loss: 6.605633156105782
Experience 5, Iter 66, disc loss: 0.014207452655904071, policy loss: 7.330262676843928
Experience 5, Iter 67, disc loss: 0.01377738871235928, policy loss: 6.581218708613165
Experience 5, Iter 68, disc loss: 0.010147058175297211, policy loss: 6.7168347939144954
Experience 5, Iter 69, disc loss: 0.00918126133599584, policy loss: 7.0578698173206895
Experience 5, Iter 70, disc loss: 0.01133434229884603, policy loss: 7.022213168601056
Experience 5, Iter 71, disc loss: 0.0159958868105847, policy loss: 6.2786285674863205
Experience 5, Iter 72, disc loss: 0.009976487744292004, policy loss: 6.618284533107163
Experience 5, Iter 73, disc loss: 0.00815857224970459, policy loss: 7.060442511891022
Experience 5, Iter 74, disc loss: 0.011637651796378259, policy loss: 6.683633490792738
Experience 5, Iter 75, disc loss: 0.01095589047749646, policy loss: 7.745213089470159
Experience 5, Iter 76, disc loss: 0.008698353004655155, policy loss: 7.125592979390921
Experience 5, Iter 77, disc loss: 0.010548610603111599, policy loss: 7.2176648535966175
Experience 5, Iter 78, disc loss: 0.013126605729498364, policy loss: 7.132024192276811
Experience 5, Iter 79, disc loss: 0.016178315756278015, policy loss: 7.171644275459974
Experience 5, Iter 80, disc loss: 0.007339673834564841, policy loss: 7.247217833430318
Experience 5, Iter 81, disc loss: 0.011617023813001035, policy loss: 6.5648319793648735
Experience 5, Iter 82, disc loss: 0.010995963921175538, policy loss: 6.66234301794816
Experience 5, Iter 83, disc loss: 0.011817141853743152, policy loss: 6.897929046311212
Experience 5, Iter 84, disc loss: 0.006005508287541612, policy loss: 7.7683781792090905
Experience 5, Iter 85, disc loss: 0.008819247642563, policy loss: 6.850446148692055
Experience 5, Iter 86, disc loss: 0.011882608426976776, policy loss: 6.8495539130212055
Experience 5, Iter 87, disc loss: 0.012850518235403818, policy loss: 6.607490465651933
Experience 5, Iter 88, disc loss: 0.008201918715780543, policy loss: 7.201969133617299
Experience 5, Iter 89, disc loss: 0.016201780313582773, policy loss: 6.926430930600497
Experience 5, Iter 90, disc loss: 0.009903826036379527, policy loss: 7.132082906050362
Experience 5, Iter 91, disc loss: 0.0097651169402175, policy loss: 6.453054115514237
Experience 5, Iter 92, disc loss: 0.007348188817108208, policy loss: 7.126011826665969
Experience 5, Iter 93, disc loss: 0.008854646458522882, policy loss: 6.907940846152773
Experience 5, Iter 94, disc loss: 0.012446408209844568, policy loss: 7.0035236849527545
Experience 5, Iter 95, disc loss: 0.008531330461072663, policy loss: 7.290157324742746
Experience 5, Iter 96, disc loss: 0.007664869920666001, policy loss: 7.429386574899868
Experience 5, Iter 97, disc loss: 0.007262990612578951, policy loss: 7.519266189247955
Experience 5, Iter 98, disc loss: 0.007070772209782531, policy loss: 6.9357032475713085
Experience 5, Iter 99, disc loss: 0.014987381418819189, policy loss: 8.044718792099037
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0102],
        [0.0361],
        [0.2963],
        [0.0051]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0543, 0.3713, 0.2323, 0.0099, 0.0033, 1.4554]],

        [[0.0543, 0.3713, 0.2323, 0.0099, 0.0033, 1.4554]],

        [[0.0543, 0.3713, 0.2323, 0.0099, 0.0033, 1.4554]],

        [[0.0543, 0.3713, 0.2323, 0.0099, 0.0033, 1.4554]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0406, 0.1443, 1.1853, 0.0205], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0406, 0.1443, 1.1853, 0.0205])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.247
Iter 2/2000 - Loss: 1.287
Iter 3/2000 - Loss: 1.112
Iter 4/2000 - Loss: 1.068
Iter 5/2000 - Loss: 1.057
Iter 6/2000 - Loss: 0.972
Iter 7/2000 - Loss: 0.847
Iter 8/2000 - Loss: 0.733
Iter 9/2000 - Loss: 0.637
Iter 10/2000 - Loss: 0.529
Iter 11/2000 - Loss: 0.386
Iter 12/2000 - Loss: 0.211
Iter 13/2000 - Loss: 0.019
Iter 14/2000 - Loss: -0.181
Iter 15/2000 - Loss: -0.389
Iter 16/2000 - Loss: -0.609
Iter 17/2000 - Loss: -0.848
Iter 18/2000 - Loss: -1.106
Iter 19/2000 - Loss: -1.381
Iter 20/2000 - Loss: -1.666
Iter 1981/2000 - Loss: -7.436
Iter 1982/2000 - Loss: -7.436
Iter 1983/2000 - Loss: -7.436
Iter 1984/2000 - Loss: -7.436
Iter 1985/2000 - Loss: -7.436
Iter 1986/2000 - Loss: -7.436
Iter 1987/2000 - Loss: -7.436
Iter 1988/2000 - Loss: -7.436
Iter 1989/2000 - Loss: -7.436
Iter 1990/2000 - Loss: -7.436
Iter 1991/2000 - Loss: -7.436
Iter 1992/2000 - Loss: -7.436
Iter 1993/2000 - Loss: -7.436
Iter 1994/2000 - Loss: -7.436
Iter 1995/2000 - Loss: -7.436
Iter 1996/2000 - Loss: -7.436
Iter 1997/2000 - Loss: -7.436
Iter 1998/2000 - Loss: -7.436
Iter 1999/2000 - Loss: -7.436
Iter 2000/2000 - Loss: -7.436
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[19.4185, 12.2601, 16.4954, 10.8340, 20.1777, 50.4038]],

        [[20.4352, 44.3434, 20.4024,  2.1919,  0.8687, 17.1062]],

        [[24.4581, 56.5825, 17.3722,  0.9747, 11.8512, 18.2026]],

        [[22.7609, 55.4659, 14.7115,  3.3750, 16.9892, 39.4802]]])
Signal Variance: tensor([ 0.3462,  1.1585, 11.2200,  0.4639])
Estimated target variance: tensor([0.0406, 0.1443, 1.1853, 0.0205])
N: 60
Signal to noise ratio: tensor([31.3094, 54.2711, 70.1525, 40.8383])
Bound on condition number: tensor([ 58817.6373, 176722.4161, 295283.1389, 100066.8541])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.0009663808687879958, policy loss: 17.83378260336244
Experience 6, Iter 1, disc loss: 0.0009615800466033625, policy loss: 17.325977869473753
Experience 6, Iter 2, disc loss: 0.0009564397238478007, policy loss: 16.808061184539
Experience 6, Iter 3, disc loss: 0.000939079656476336, policy loss: 16.43675677684431
Experience 6, Iter 4, disc loss: 0.0009292038442632302, policy loss: 14.50951119804888
Experience 6, Iter 5, disc loss: 0.0009524210902209378, policy loss: 12.721420466643252
Experience 6, Iter 6, disc loss: 0.002494669236094811, policy loss: 9.947348190752084
Experience 6, Iter 7, disc loss: 0.0053334747027471326, policy loss: 7.825150858819717
Experience 6, Iter 8, disc loss: 0.014806107314445139, policy loss: 5.067520552636755
Experience 6, Iter 9, disc loss: 0.0298594461703738, policy loss: 4.3164132398411805
Experience 6, Iter 10, disc loss: 0.06227590067667578, policy loss: 3.190535253435468
Experience 6, Iter 11, disc loss: 0.045240533840248696, policy loss: 3.4256863668306745
Experience 6, Iter 12, disc loss: 0.021910769851849813, policy loss: 4.127549181329135
Experience 6, Iter 13, disc loss: 0.020548892541688818, policy loss: 4.28428132441333
Experience 6, Iter 14, disc loss: 0.02415569729754168, policy loss: 4.12388755058903
Experience 6, Iter 15, disc loss: 0.047795918375797934, policy loss: 3.348769804389603
Experience 6, Iter 16, disc loss: 0.06917670099972889, policy loss: 2.826566633799845
Experience 6, Iter 17, disc loss: 0.08032251266167509, policy loss: 2.6469466314309393
Experience 6, Iter 18, disc loss: 0.0691910739822157, policy loss: 2.7784483323789617
Experience 6, Iter 19, disc loss: 0.05853704650110968, policy loss: 2.945533566594114
Experience 6, Iter 20, disc loss: 0.05107918503999608, policy loss: 3.1136790862503285
Experience 6, Iter 21, disc loss: 0.04739466886741995, policy loss: 3.1795132164184916
Experience 6, Iter 22, disc loss: 0.04783569745250307, policy loss: 3.173076853808853
Experience 6, Iter 23, disc loss: 0.04639354516087743, policy loss: 3.198604750428678
Experience 6, Iter 24, disc loss: 0.044178420745785506, policy loss: 3.265315540230869
Experience 6, Iter 25, disc loss: 0.04723120149885655, policy loss: 3.192359383081949
Experience 6, Iter 26, disc loss: 0.0499101312191107, policy loss: 3.1406874675784007
Experience 6, Iter 27, disc loss: 0.05016630294456548, policy loss: 3.1433633356681767
Experience 6, Iter 28, disc loss: 0.05419924434832848, policy loss: 3.0556179033863815
Experience 6, Iter 29, disc loss: 0.054210026820327016, policy loss: 3.06432933617139
Experience 6, Iter 30, disc loss: 0.048297523154880714, policy loss: 3.2101073987765623
Experience 6, Iter 31, disc loss: 0.04558537582349018, policy loss: 3.28338279342895
Experience 6, Iter 32, disc loss: 0.043604742248720164, policy loss: 3.350267601994458
Experience 6, Iter 33, disc loss: 0.04380909025168415, policy loss: 3.3396566893135144
Experience 6, Iter 34, disc loss: 0.04368534389105969, policy loss: 3.3178265712784976
Experience 6, Iter 35, disc loss: 0.04207575160579871, policy loss: 3.3646075419699657
Experience 6, Iter 36, disc loss: 0.04150490828526537, policy loss: 3.3937976417121822
Experience 6, Iter 37, disc loss: 0.04182758563984461, policy loss: 3.354024051310054
Experience 6, Iter 38, disc loss: 0.04157234189766326, policy loss: 3.3425460539382486
Experience 6, Iter 39, disc loss: 0.03848472976567194, policy loss: 3.455908655093922
Experience 6, Iter 40, disc loss: 0.037143702194226635, policy loss: 3.4761383716650585
Experience 6, Iter 41, disc loss: 0.03459239648111775, policy loss: 3.5612348419667335
Experience 6, Iter 42, disc loss: 0.03356337778660326, policy loss: 3.593686717908951
Experience 6, Iter 43, disc loss: 0.03253072521488997, policy loss: 3.6115358662525807
Experience 6, Iter 44, disc loss: 0.03212243293614487, policy loss: 3.624935109774939
Experience 6, Iter 45, disc loss: 0.031899393833621864, policy loss: 3.6373178508085884
Experience 6, Iter 46, disc loss: 0.03286478069933647, policy loss: 3.5907641804359596
Experience 6, Iter 47, disc loss: 0.03106398789957645, policy loss: 3.6683506449896326
Experience 6, Iter 48, disc loss: 0.029229372489773923, policy loss: 3.731240998068131
Experience 6, Iter 49, disc loss: 0.028382780294809166, policy loss: 3.7461600943745013
Experience 6, Iter 50, disc loss: 0.02813294394239592, policy loss: 3.7584765546618577
Experience 6, Iter 51, disc loss: 0.02743519796101215, policy loss: 3.806032920183907
Experience 6, Iter 52, disc loss: 0.026057290327907705, policy loss: 3.8311380307231717
Experience 6, Iter 53, disc loss: 0.025498352085775958, policy loss: 3.860039431335279
Experience 6, Iter 54, disc loss: 0.02455918886696858, policy loss: 3.916526753211873
Experience 6, Iter 55, disc loss: 0.023213267530299602, policy loss: 3.968157161663149
Experience 6, Iter 56, disc loss: 0.02535471547362791, policy loss: 3.818546644626192
Experience 6, Iter 57, disc loss: 0.023214771085562056, policy loss: 3.911292293153952
Experience 6, Iter 58, disc loss: 0.022725523338855108, policy loss: 3.935503194269511
Experience 6, Iter 59, disc loss: 0.021512298063939204, policy loss: 4.014673667105686
Experience 6, Iter 60, disc loss: 0.02100316011855958, policy loss: 4.019296786337833
Experience 6, Iter 61, disc loss: 0.020960772407466063, policy loss: 4.014580880415002
Experience 6, Iter 62, disc loss: 0.020296436631863493, policy loss: 4.043757197632385
Experience 6, Iter 63, disc loss: 0.0193255801376203, policy loss: 4.107249820081254
Experience 6, Iter 64, disc loss: 0.018284400859047503, policy loss: 4.179691157246293
Experience 6, Iter 65, disc loss: 0.01917323392897026, policy loss: 4.098973708728021
Experience 6, Iter 66, disc loss: 0.01797208656618974, policy loss: 4.162995156881114
Experience 6, Iter 67, disc loss: 0.01754722355820707, policy loss: 4.1955469035416995
Experience 6, Iter 68, disc loss: 0.01878106864734017, policy loss: 4.100726669931549
Experience 6, Iter 69, disc loss: 0.017909462014014797, policy loss: 4.172948224642981
Experience 6, Iter 70, disc loss: 0.016464776103125387, policy loss: 4.2501819322403085
Experience 6, Iter 71, disc loss: 0.015630013053968765, policy loss: 4.301922312548848
Experience 6, Iter 72, disc loss: 0.016095761485586793, policy loss: 4.2895835374945355
Experience 6, Iter 73, disc loss: 0.015111176639519702, policy loss: 4.347033492623277
Experience 6, Iter 74, disc loss: 0.016453410183280504, policy loss: 4.238472990205855
Experience 6, Iter 75, disc loss: 0.014740649666763311, policy loss: 4.404686095027529
Experience 6, Iter 76, disc loss: 0.014721219362675786, policy loss: 4.358555679184548
Experience 6, Iter 77, disc loss: 0.015186867861003362, policy loss: 4.315288042538967
Experience 6, Iter 78, disc loss: 0.014058680146055375, policy loss: 4.40145610117449
Experience 6, Iter 79, disc loss: 0.012851502592415931, policy loss: 4.5261564646050285
Experience 6, Iter 80, disc loss: 0.014113244456504073, policy loss: 4.401979278042267
Experience 6, Iter 81, disc loss: 0.013268372701078182, policy loss: 4.451087025755192
Experience 6, Iter 82, disc loss: 0.01314316289509223, policy loss: 4.476330376636749
Experience 6, Iter 83, disc loss: 0.012725988327709749, policy loss: 4.519987409598517
Experience 6, Iter 84, disc loss: 0.012223309947903944, policy loss: 4.556552399726746
Experience 6, Iter 85, disc loss: 0.012410830692785773, policy loss: 4.543055263211505
Experience 6, Iter 86, disc loss: 0.01190572846646052, policy loss: 4.624728933533553
Experience 6, Iter 87, disc loss: 0.01113264263330868, policy loss: 4.677188310054332
Experience 6, Iter 88, disc loss: 0.012251070709061689, policy loss: 4.525572144193653
Experience 6, Iter 89, disc loss: 0.010299475406803113, policy loss: 4.761985035814584
Experience 6, Iter 90, disc loss: 0.011010354792557913, policy loss: 4.664087568353236
Experience 6, Iter 91, disc loss: 0.011322688290608337, policy loss: 4.622999068368429
Experience 6, Iter 92, disc loss: 0.010878241030080211, policy loss: 4.657156494599795
Experience 6, Iter 93, disc loss: 0.010461549795789211, policy loss: 4.709657415855904
Experience 6, Iter 94, disc loss: 0.010285861303782584, policy loss: 4.729000795238287
Experience 6, Iter 95, disc loss: 0.010650850491886887, policy loss: 4.680746538282209
Experience 6, Iter 96, disc loss: 0.009887794633000036, policy loss: 4.751919503234079
Experience 6, Iter 97, disc loss: 0.010254599519903233, policy loss: 4.718214732965247
Experience 6, Iter 98, disc loss: 0.00903939043852666, policy loss: 4.881372919923926
Experience 6, Iter 99, disc loss: 0.00952605349118848, policy loss: 4.795427746137469
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0087],
        [0.0312],
        [0.2545],
        [0.0044]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0468, 0.3180, 0.1989, 0.0086, 0.0029, 1.2561]],

        [[0.0468, 0.3180, 0.1989, 0.0086, 0.0029, 1.2561]],

        [[0.0468, 0.3180, 0.1989, 0.0086, 0.0029, 1.2561]],

        [[0.0468, 0.3180, 0.1989, 0.0086, 0.0029, 1.2561]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0349, 0.1246, 1.0181, 0.0176], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0349, 0.1246, 1.0181, 0.0176])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.959
Iter 2/2000 - Loss: 1.033
Iter 3/2000 - Loss: 0.841
Iter 4/2000 - Loss: 0.794
Iter 5/2000 - Loss: 0.797
Iter 6/2000 - Loss: 0.714
Iter 7/2000 - Loss: 0.584
Iter 8/2000 - Loss: 0.467
Iter 9/2000 - Loss: 0.371
Iter 10/2000 - Loss: 0.264
Iter 11/2000 - Loss: 0.120
Iter 12/2000 - Loss: -0.056
Iter 13/2000 - Loss: -0.249
Iter 14/2000 - Loss: -0.451
Iter 15/2000 - Loss: -0.661
Iter 16/2000 - Loss: -0.885
Iter 17/2000 - Loss: -1.129
Iter 18/2000 - Loss: -1.393
Iter 19/2000 - Loss: -1.673
Iter 20/2000 - Loss: -1.964
Iter 1981/2000 - Loss: -7.738
Iter 1982/2000 - Loss: -7.739
Iter 1983/2000 - Loss: -7.739
Iter 1984/2000 - Loss: -7.739
Iter 1985/2000 - Loss: -7.739
Iter 1986/2000 - Loss: -7.739
Iter 1987/2000 - Loss: -7.739
Iter 1988/2000 - Loss: -7.739
Iter 1989/2000 - Loss: -7.739
Iter 1990/2000 - Loss: -7.739
Iter 1991/2000 - Loss: -7.739
Iter 1992/2000 - Loss: -7.739
Iter 1993/2000 - Loss: -7.739
Iter 1994/2000 - Loss: -7.739
Iter 1995/2000 - Loss: -7.739
Iter 1996/2000 - Loss: -7.739
Iter 1997/2000 - Loss: -7.739
Iter 1998/2000 - Loss: -7.739
Iter 1999/2000 - Loss: -7.739
Iter 2000/2000 - Loss: -7.739
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[18.5036, 11.6026, 16.3511, 10.0384, 19.0801, 47.5194]],

        [[19.7079, 42.4403, 20.3588,  2.1926,  0.8821, 17.6433]],

        [[22.9659, 53.4841, 17.1090,  0.9641, 11.3408, 18.1921]],

        [[22.0436, 52.8486, 14.6723,  3.3519, 17.1325, 39.5458]]])
Signal Variance: tensor([ 0.3098,  1.1746, 10.8141,  0.4476])
Estimated target variance: tensor([0.0349, 0.1246, 1.0181, 0.0176])
N: 70
Signal to noise ratio: tensor([27.2959, 56.1262, 74.4428, 42.2821])
Bound on condition number: tensor([ 52155.5719, 220511.3935, 387921.6289, 125145.1239])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.009581413495593232, policy loss: 4.781912307499766
Experience 7, Iter 1, disc loss: 0.009748824135997591, policy loss: 4.755954781454077
Experience 7, Iter 2, disc loss: 0.009040008301413521, policy loss: 4.855979671808291
Experience 7, Iter 3, disc loss: 0.009005019744656636, policy loss: 4.840931110770422
Experience 7, Iter 4, disc loss: 0.00904560628962763, policy loss: 4.891343312619972
Experience 7, Iter 5, disc loss: 0.009111295219904465, policy loss: 4.838246074692339
Experience 7, Iter 6, disc loss: 0.009224938582339377, policy loss: 4.814775936669879
Experience 7, Iter 7, disc loss: 0.008839412215058736, policy loss: 4.853892586822241
Experience 7, Iter 8, disc loss: 0.00862203020091341, policy loss: 4.902653128715381
Experience 7, Iter 9, disc loss: 0.008338111104659889, policy loss: 4.92084933253772
Experience 7, Iter 10, disc loss: 0.008307261983316024, policy loss: 4.918464186073115
Experience 7, Iter 11, disc loss: 0.008234220467167782, policy loss: 4.942637176898019
Experience 7, Iter 12, disc loss: 0.008551860972075023, policy loss: 4.881379091142514
Experience 7, Iter 13, disc loss: 0.008269338674503376, policy loss: 4.920129249727122
Experience 7, Iter 14, disc loss: 0.00784289828248571, policy loss: 5.001137238190196
Experience 7, Iter 15, disc loss: 0.008155148431141453, policy loss: 4.938805817698763
Experience 7, Iter 16, disc loss: 0.007776741444214937, policy loss: 5.003140282463233
Experience 7, Iter 17, disc loss: 0.00808711592393168, policy loss: 4.939594092200398
Experience 7, Iter 18, disc loss: 0.00738024978105316, policy loss: 5.046113221579546
Experience 7, Iter 19, disc loss: 0.008033798968807248, policy loss: 4.9475719060531365
Experience 7, Iter 20, disc loss: 0.007648398526214736, policy loss: 5.0257887685970415
Experience 7, Iter 21, disc loss: 0.0070902690492443896, policy loss: 5.115293339970715
Experience 7, Iter 22, disc loss: 0.007456241993930118, policy loss: 5.056398041050695
Experience 7, Iter 23, disc loss: 0.007329132380049372, policy loss: 5.040702016970556
Experience 7, Iter 24, disc loss: 0.007300136076505427, policy loss: 5.049471540577011
Experience 7, Iter 25, disc loss: 0.00740935441108982, policy loss: 5.039010910822351
Experience 7, Iter 26, disc loss: 0.007617743015284403, policy loss: 5.001816093071435
Experience 7, Iter 27, disc loss: 0.007060206412717019, policy loss: 5.110114324360941
Experience 7, Iter 28, disc loss: 0.0071997256919382555, policy loss: 5.05553061289331
Experience 7, Iter 29, disc loss: 0.007104711014396906, policy loss: 5.097246792026706
Experience 7, Iter 30, disc loss: 0.0076766124245105875, policy loss: 4.994058984968858
Experience 7, Iter 31, disc loss: 0.00734589291028463, policy loss: 5.049508016733711
Experience 7, Iter 32, disc loss: 0.0067566431419039275, policy loss: 5.1293278915777085
Experience 7, Iter 33, disc loss: 0.007071243600695995, policy loss: 5.077639183704537
Experience 7, Iter 34, disc loss: 0.006842528140762121, policy loss: 5.159471099144663
Experience 7, Iter 35, disc loss: 0.006463304058947194, policy loss: 5.200011677476235
Experience 7, Iter 36, disc loss: 0.00693933246591477, policy loss: 5.108321635764574
Experience 7, Iter 37, disc loss: 0.0067454184412668846, policy loss: 5.152482619285424
Experience 7, Iter 38, disc loss: 0.006565537648646591, policy loss: 5.169860748431563
Experience 7, Iter 39, disc loss: 0.007153122653771676, policy loss: 5.091996307247266
Experience 7, Iter 40, disc loss: 0.007197335631027879, policy loss: 5.071522820978516
Experience 7, Iter 41, disc loss: 0.00669622353298601, policy loss: 5.174814842861142
Experience 7, Iter 42, disc loss: 0.006956422698980791, policy loss: 5.114391389739207
Experience 7, Iter 43, disc loss: 0.0064769741722996545, policy loss: 5.173125083857402
Experience 7, Iter 44, disc loss: 0.006949350900761184, policy loss: 5.144707307780784
Experience 7, Iter 45, disc loss: 0.006749198867118669, policy loss: 5.199625292324072
Experience 7, Iter 46, disc loss: 0.006599388978956764, policy loss: 5.215164605459055
Experience 7, Iter 47, disc loss: 0.006506068018024144, policy loss: 5.188572586992273
Experience 7, Iter 48, disc loss: 0.006757106751201287, policy loss: 5.166666702402723
Experience 7, Iter 49, disc loss: 0.007133949047245845, policy loss: 5.129590196227994
Experience 7, Iter 50, disc loss: 0.007045391512349951, policy loss: 5.1383540671471915
Experience 7, Iter 51, disc loss: 0.006592822956002927, policy loss: 5.209397501671531
Experience 7, Iter 52, disc loss: 0.006670948831255997, policy loss: 5.225855045759251
Experience 7, Iter 53, disc loss: 0.006407419319684816, policy loss: 5.2493227353758165
Experience 7, Iter 54, disc loss: 0.0059941091616772324, policy loss: 5.321033914650016
Experience 7, Iter 55, disc loss: 0.006368277777638608, policy loss: 5.242612807746255
Experience 7, Iter 56, disc loss: 0.006340136588804882, policy loss: 5.231145678136761
Experience 7, Iter 57, disc loss: 0.006256132840984662, policy loss: 5.284796184752521
Experience 7, Iter 58, disc loss: 0.005996111428826372, policy loss: 5.320933515737954
Experience 7, Iter 59, disc loss: 0.006243396741876618, policy loss: 5.2929670703345035
Experience 7, Iter 60, disc loss: 0.005969516287986351, policy loss: 5.340687536184351
Experience 7, Iter 61, disc loss: 0.006622344030967276, policy loss: 5.225176721910684
Experience 7, Iter 62, disc loss: 0.005655371934800141, policy loss: 5.367039033694231
Experience 7, Iter 63, disc loss: 0.006007534871543142, policy loss: 5.3151939110365225
Experience 7, Iter 64, disc loss: 0.005989622937935768, policy loss: 5.348096098122301
Experience 7, Iter 65, disc loss: 0.005956809900336528, policy loss: 5.325121583246964
Experience 7, Iter 66, disc loss: 0.0058341104813454175, policy loss: 5.387425444862589
Experience 7, Iter 67, disc loss: 0.005874913248807066, policy loss: 5.370504322915431
Experience 7, Iter 68, disc loss: 0.006095473964550293, policy loss: 5.304107468265197
Experience 7, Iter 69, disc loss: 0.00597831146034999, policy loss: 5.326683593481783
Experience 7, Iter 70, disc loss: 0.006344849131808222, policy loss: 5.306311295056137
Experience 7, Iter 71, disc loss: 0.005571492239502634, policy loss: 5.405070021201356
Experience 7, Iter 72, disc loss: 0.005894979964051043, policy loss: 5.344221090510479
Experience 7, Iter 73, disc loss: 0.005562622066813237, policy loss: 5.435578815393729
Experience 7, Iter 74, disc loss: 0.006256986411342655, policy loss: 5.334017732764824
Experience 7, Iter 75, disc loss: 0.005589036146428711, policy loss: 5.485949356174032
Experience 7, Iter 76, disc loss: 0.005352492244401573, policy loss: 5.523964314934375
Experience 7, Iter 77, disc loss: 0.0052589800543843945, policy loss: 5.485707865157162
Experience 7, Iter 78, disc loss: 0.006571375930653625, policy loss: 5.343808211621337
Experience 7, Iter 79, disc loss: 0.0056927553473904765, policy loss: 5.394616558308581
Experience 7, Iter 80, disc loss: 0.005167133479722492, policy loss: 5.531923943006594
Experience 7, Iter 81, disc loss: 0.005559258440673011, policy loss: 5.455291673210865
Experience 7, Iter 82, disc loss: 0.00531897729062734, policy loss: 5.562055526214573
Experience 7, Iter 83, disc loss: 0.005647735615296503, policy loss: 5.524660743915181
Experience 7, Iter 84, disc loss: 0.005470789607515597, policy loss: 5.52262061072805
Experience 7, Iter 85, disc loss: 0.007273569543008261, policy loss: 5.244112615671628
Experience 7, Iter 86, disc loss: 0.005924811803210978, policy loss: 5.436213851556853
Experience 7, Iter 87, disc loss: 0.006255318905087755, policy loss: 5.442808685885618
Experience 7, Iter 88, disc loss: 0.005578319258744256, policy loss: 5.602251350993582
Experience 7, Iter 89, disc loss: 0.007524752799841361, policy loss: 5.298543255762352
Experience 7, Iter 90, disc loss: 0.007287485362617617, policy loss: 5.397310662708543
Experience 7, Iter 91, disc loss: 0.007460377517652463, policy loss: 5.353383666507748
Experience 7, Iter 92, disc loss: 0.005998060529636529, policy loss: 5.557033057584263
Experience 7, Iter 93, disc loss: 0.006705301503137797, policy loss: 5.5682241015275356
Experience 7, Iter 94, disc loss: 0.006507546659014228, policy loss: 5.4162728058939384
Experience 7, Iter 95, disc loss: 0.006793469562862989, policy loss: 5.394341138347821
Experience 7, Iter 96, disc loss: 0.0056391981550089525, policy loss: 5.538755156356652
Experience 7, Iter 97, disc loss: 0.006220097432893876, policy loss: 5.584243788107807
Experience 7, Iter 98, disc loss: 0.007123621238699256, policy loss: 5.371219707813136
Experience 7, Iter 99, disc loss: 0.006849759814351364, policy loss: 5.380494617302073
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0077],
        [0.0304],
        [0.2559],
        [0.0041]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0413, 0.2810, 0.1882, 0.0080, 0.0026, 1.1606]],

        [[0.0413, 0.2810, 0.1882, 0.0080, 0.0026, 1.1606]],

        [[0.0413, 0.2810, 0.1882, 0.0080, 0.0026, 1.1606]],

        [[0.0413, 0.2810, 0.1882, 0.0080, 0.0026, 1.1606]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0308, 0.1216, 1.0236, 0.0164], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0308, 0.1216, 1.0236, 0.0164])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.872
Iter 2/2000 - Loss: 0.981
Iter 3/2000 - Loss: 0.779
Iter 4/2000 - Loss: 0.740
Iter 5/2000 - Loss: 0.770
Iter 6/2000 - Loss: 0.696
Iter 7/2000 - Loss: 0.573
Iter 8/2000 - Loss: 0.478
Iter 9/2000 - Loss: 0.411
Iter 10/2000 - Loss: 0.325
Iter 11/2000 - Loss: 0.197
Iter 12/2000 - Loss: 0.042
Iter 13/2000 - Loss: -0.122
Iter 14/2000 - Loss: -0.289
Iter 15/2000 - Loss: -0.467
Iter 16/2000 - Loss: -0.664
Iter 17/2000 - Loss: -0.886
Iter 18/2000 - Loss: -1.131
Iter 19/2000 - Loss: -1.395
Iter 20/2000 - Loss: -1.672
Iter 1981/2000 - Loss: -7.886
Iter 1982/2000 - Loss: -7.886
Iter 1983/2000 - Loss: -7.886
Iter 1984/2000 - Loss: -7.886
Iter 1985/2000 - Loss: -7.886
Iter 1986/2000 - Loss: -7.886
Iter 1987/2000 - Loss: -7.886
Iter 1988/2000 - Loss: -7.886
Iter 1989/2000 - Loss: -7.886
Iter 1990/2000 - Loss: -7.886
Iter 1991/2000 - Loss: -7.886
Iter 1992/2000 - Loss: -7.887
Iter 1993/2000 - Loss: -7.887
Iter 1994/2000 - Loss: -7.887
Iter 1995/2000 - Loss: -7.887
Iter 1996/2000 - Loss: -7.887
Iter 1997/2000 - Loss: -7.887
Iter 1998/2000 - Loss: -7.887
Iter 1999/2000 - Loss: -7.887
Iter 2000/2000 - Loss: -7.887
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[18.1856, 11.3117, 16.2296, 10.1916, 16.9175, 44.7987]],

        [[17.7764, 32.6009, 21.0634,  2.6277,  1.0972, 22.1278]],

        [[21.1155, 44.9352, 17.5093,  0.9419,  9.1972, 24.6690]],

        [[21.8859, 46.6667, 14.4615,  3.2045, 15.4283, 39.1938]]])
Signal Variance: tensor([ 0.2836,  2.0597, 16.5850,  0.4177])
Estimated target variance: tensor([0.0308, 0.1216, 1.0236, 0.0164])
N: 80
Signal to noise ratio: tensor([26.4030, 77.2800, 91.6598, 41.8785])
Bound on condition number: tensor([ 55770.3216, 477776.4787, 672123.0578, 140305.8609])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.006983507430114687, policy loss: 5.470761701558253
Experience 8, Iter 1, disc loss: 0.007148674336321206, policy loss: 5.450718055498595
Experience 8, Iter 2, disc loss: 0.008858586552455087, policy loss: 5.25993674284837
Experience 8, Iter 3, disc loss: 0.006123175857975753, policy loss: 5.749692212248563
Experience 8, Iter 4, disc loss: 0.005183882256528166, policy loss: 6.093462914223522
Experience 8, Iter 5, disc loss: 0.006124251587369048, policy loss: 6.1261773895535185
Experience 8, Iter 6, disc loss: 0.0052228668908924835, policy loss: 5.855023718335324
Experience 8, Iter 7, disc loss: 0.0075589503172971036, policy loss: 5.7490010795769475
Experience 8, Iter 8, disc loss: 0.007262259618165244, policy loss: 5.6994461858397605
Experience 8, Iter 9, disc loss: 0.006898049684530611, policy loss: 5.614782980904209
Experience 8, Iter 10, disc loss: 0.0071741408344723956, policy loss: 5.588581715276921
Experience 8, Iter 11, disc loss: 0.007953625216005828, policy loss: 5.511465234201436
Experience 8, Iter 12, disc loss: 0.006938222506210004, policy loss: 5.519466736270691
Experience 8, Iter 13, disc loss: 0.005841033165015731, policy loss: 5.596815734391551
Experience 8, Iter 14, disc loss: 0.007063926223867236, policy loss: 5.53385869281207
Experience 8, Iter 15, disc loss: 0.005799183685989795, policy loss: 5.644419383865467
Experience 8, Iter 16, disc loss: 0.005108747978670245, policy loss: 5.771543430449962
Experience 8, Iter 17, disc loss: 0.004919081376056353, policy loss: 5.868624255740424
Experience 8, Iter 18, disc loss: 0.00557619763257337, policy loss: 5.823359119638683
Experience 8, Iter 19, disc loss: 0.004322349571139146, policy loss: 5.937884817667581
Experience 8, Iter 20, disc loss: 0.005618491666420804, policy loss: 5.78765869289733
Experience 8, Iter 21, disc loss: 0.005020530961999907, policy loss: 5.8859584568476535
Experience 8, Iter 22, disc loss: 0.005990103327976266, policy loss: 5.67764311265132
Experience 8, Iter 23, disc loss: 0.0057938140049939675, policy loss: 5.630946266297974
Experience 8, Iter 24, disc loss: 0.0065976286650047174, policy loss: 5.522938407256927
Experience 8, Iter 25, disc loss: 0.006619849540619188, policy loss: 5.764538724675435
Experience 8, Iter 26, disc loss: 0.007204833421754456, policy loss: 5.735934950240788
Experience 8, Iter 27, disc loss: 0.007059260994293927, policy loss: 5.705971908335087
Experience 8, Iter 28, disc loss: 0.007752919961139749, policy loss: 5.7497333257488865
Experience 8, Iter 29, disc loss: 0.006151013521555182, policy loss: 5.843477362772357
Experience 8, Iter 30, disc loss: 0.005861665041670482, policy loss: 5.9709445825496985
Experience 8, Iter 31, disc loss: 0.006441986194296682, policy loss: 5.752335122344867
Experience 8, Iter 32, disc loss: 0.006575567106226855, policy loss: 5.721696642626794
Experience 8, Iter 33, disc loss: 0.008158322765426596, policy loss: 5.423991418062468
Experience 8, Iter 34, disc loss: 0.007170646598318243, policy loss: 5.875443268748612
Experience 8, Iter 35, disc loss: 0.007172647775214026, policy loss: 5.829784388166537
Experience 8, Iter 36, disc loss: 0.006416882852712345, policy loss: 5.788174923710908
Experience 8, Iter 37, disc loss: 0.006488193677511092, policy loss: 5.730656048473847
Experience 8, Iter 38, disc loss: 0.0059339672078047336, policy loss: 6.023650534402471
Experience 8, Iter 39, disc loss: 0.009749463062254942, policy loss: 5.522367915216366
Experience 8, Iter 40, disc loss: 0.009997303981756578, policy loss: 5.507186358563704
Experience 8, Iter 41, disc loss: 0.01002203357728575, policy loss: 5.797970889776039
Experience 8, Iter 42, disc loss: 0.007833494136636796, policy loss: 5.9660180436316645
Experience 8, Iter 43, disc loss: 0.005316616018641846, policy loss: 6.03881150606487
Experience 8, Iter 44, disc loss: 0.007281758149885852, policy loss: 6.028136081609829
Experience 8, Iter 45, disc loss: 0.008729646008253308, policy loss: 5.863511807174333
Experience 8, Iter 46, disc loss: 0.0072860758619903146, policy loss: 6.054978919305217
Experience 8, Iter 47, disc loss: 0.006505348645912599, policy loss: 5.918003530446265
Experience 8, Iter 48, disc loss: 0.006500594275245141, policy loss: 6.136237268109282
Experience 8, Iter 49, disc loss: 0.007195766479367876, policy loss: 6.0183349056136155
Experience 8, Iter 50, disc loss: 0.013586526357192636, policy loss: 5.344632178207126
Experience 8, Iter 51, disc loss: 0.011347024931803958, policy loss: 5.605610189597032
Experience 8, Iter 52, disc loss: 0.010177039709325934, policy loss: 5.732520993195307
Experience 8, Iter 53, disc loss: 0.008991897587061615, policy loss: 5.772003633007451
Experience 8, Iter 54, disc loss: 0.00789286832074528, policy loss: 5.86471891763827
Experience 8, Iter 55, disc loss: 0.006173931824871845, policy loss: 6.294203137350887
Experience 8, Iter 56, disc loss: 0.006143574120418669, policy loss: 6.4371691627351435
Experience 8, Iter 57, disc loss: 0.004263196814954608, policy loss: 6.818977143706115
Experience 8, Iter 58, disc loss: 0.004409531453420518, policy loss: 6.970983795452326
Experience 8, Iter 59, disc loss: 0.004737853903382846, policy loss: 6.825326350201202
Experience 8, Iter 60, disc loss: 0.005216927629539028, policy loss: 6.418750379536635
Experience 8, Iter 61, disc loss: 0.006139502383209969, policy loss: 6.395186962672217
Experience 8, Iter 62, disc loss: 0.0055976122350779684, policy loss: 6.56439561448097
Experience 8, Iter 63, disc loss: 0.006927366345821487, policy loss: 5.7289229199973155
Experience 8, Iter 64, disc loss: 0.0076523042137222105, policy loss: 5.775734258589812
Experience 8, Iter 65, disc loss: 0.00972800164279939, policy loss: 5.8755730758634
Experience 8, Iter 66, disc loss: 0.013195047591985195, policy loss: 5.982070770296309
Experience 8, Iter 67, disc loss: 0.008606046394731226, policy loss: 6.060195352755741
Experience 8, Iter 68, disc loss: 0.011386850630430836, policy loss: 6.478804367505205
Experience 8, Iter 69, disc loss: 0.010238974190693662, policy loss: 6.4012548963915314
Experience 8, Iter 70, disc loss: 0.009858710609223057, policy loss: 6.3499549751094495
Experience 8, Iter 71, disc loss: 0.008871116989817202, policy loss: 6.573847927569954
Experience 8, Iter 72, disc loss: 0.008004114919146767, policy loss: 6.370026465104682
Experience 8, Iter 73, disc loss: 0.006187667453719757, policy loss: 6.57593071895862
Experience 8, Iter 74, disc loss: 0.0073470216277514155, policy loss: 6.569135709637478
Experience 8, Iter 75, disc loss: 0.008627612054600546, policy loss: 6.14978131318571
Experience 8, Iter 76, disc loss: 0.006834942637446103, policy loss: 6.305584657168483
Experience 8, Iter 77, disc loss: 0.00734867528965385, policy loss: 6.138954199212773
Experience 8, Iter 78, disc loss: 0.00914506794648776, policy loss: 5.941135224070153
Experience 8, Iter 79, disc loss: 0.006459203662037616, policy loss: 6.563704764009559
Experience 8, Iter 80, disc loss: 0.008322246457443193, policy loss: 6.111199127741228
Experience 8, Iter 81, disc loss: 0.007652753918196515, policy loss: 6.240069769223026
Experience 8, Iter 82, disc loss: 0.007016219497235783, policy loss: 6.131583041108436
Experience 8, Iter 83, disc loss: 0.005562797613394746, policy loss: 6.666648172966504
Experience 8, Iter 84, disc loss: 0.005532013515383459, policy loss: 6.742597924216923
Experience 8, Iter 85, disc loss: 0.0049136531808178984, policy loss: 7.155029946005048
Experience 8, Iter 86, disc loss: 0.004737939151844159, policy loss: 6.644417673738321
Experience 8, Iter 87, disc loss: 0.004301595912023108, policy loss: 6.705050827402086
Experience 8, Iter 88, disc loss: 0.005425507215764655, policy loss: 6.701023424542987
Experience 8, Iter 89, disc loss: 0.00617241715186583, policy loss: 6.718255369822845
Experience 8, Iter 90, disc loss: 0.004495847810085201, policy loss: 6.950757669479833
Experience 8, Iter 91, disc loss: 0.00454955024123394, policy loss: 7.010163040644303
Experience 8, Iter 92, disc loss: 0.005053185958228514, policy loss: 6.8549970292043145
Experience 8, Iter 93, disc loss: 0.00403231562344126, policy loss: 7.474370805841614
Experience 8, Iter 94, disc loss: 0.005468275858858232, policy loss: 6.735152398024841
Experience 8, Iter 95, disc loss: 0.004207940354032581, policy loss: 6.965786673099839
Experience 8, Iter 96, disc loss: 0.006384222261417286, policy loss: 6.348411824452086
Experience 8, Iter 97, disc loss: 0.0070556056477454505, policy loss: 6.416644450801195
Experience 8, Iter 98, disc loss: 0.007539276855938326, policy loss: 6.34819857203033
Experience 8, Iter 99, disc loss: 0.007011944634570005, policy loss: 6.377658784275107
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0070],
        [0.0393],
        [0.3661],
        [0.0042]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0369, 0.2552, 0.2020, 0.0075, 0.0023, 1.2454]],

        [[0.0369, 0.2552, 0.2020, 0.0075, 0.0023, 1.2454]],

        [[0.0369, 0.2552, 0.2020, 0.0075, 0.0023, 1.2454]],

        [[0.0369, 0.2552, 0.2020, 0.0075, 0.0023, 1.2454]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0280, 0.1571, 1.4642, 0.0166], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0280, 0.1571, 1.4642, 0.0166])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.136
Iter 2/2000 - Loss: 1.302
Iter 3/2000 - Loss: 1.064
Iter 4/2000 - Loss: 1.046
Iter 5/2000 - Loss: 1.107
Iter 6/2000 - Loss: 1.031
Iter 7/2000 - Loss: 0.912
Iter 8/2000 - Loss: 0.841
Iter 9/2000 - Loss: 0.804
Iter 10/2000 - Loss: 0.741
Iter 11/2000 - Loss: 0.633
Iter 12/2000 - Loss: 0.500
Iter 13/2000 - Loss: 0.362
Iter 14/2000 - Loss: 0.223
Iter 15/2000 - Loss: 0.071
Iter 16/2000 - Loss: -0.106
Iter 17/2000 - Loss: -0.312
Iter 18/2000 - Loss: -0.545
Iter 19/2000 - Loss: -0.798
Iter 20/2000 - Loss: -1.066
Iter 1981/2000 - Loss: -8.014
Iter 1982/2000 - Loss: -8.014
Iter 1983/2000 - Loss: -8.014
Iter 1984/2000 - Loss: -8.014
Iter 1985/2000 - Loss: -8.014
Iter 1986/2000 - Loss: -8.014
Iter 1987/2000 - Loss: -8.014
Iter 1988/2000 - Loss: -8.014
Iter 1989/2000 - Loss: -8.014
Iter 1990/2000 - Loss: -8.014
Iter 1991/2000 - Loss: -8.014
Iter 1992/2000 - Loss: -8.014
Iter 1993/2000 - Loss: -8.014
Iter 1994/2000 - Loss: -8.015
Iter 1995/2000 - Loss: -8.015
Iter 1996/2000 - Loss: -8.015
Iter 1997/2000 - Loss: -8.015
Iter 1998/2000 - Loss: -8.015
Iter 1999/2000 - Loss: -8.015
Iter 2000/2000 - Loss: -8.015
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[18.5866, 11.5070, 15.9161, 10.5110, 16.0607, 46.4054]],

        [[21.3928, 34.7426, 21.8137,  3.4989,  0.9485, 23.0921]],

        [[17.8295, 42.7115, 17.2571,  0.9393,  6.6899, 22.4934]],

        [[22.1244, 43.2310, 14.4244,  3.1219, 14.2139, 42.2298]]])
Signal Variance: tensor([ 0.2788,  2.0214, 15.0448,  0.4006])
Estimated target variance: tensor([0.0280, 0.1571, 1.4642, 0.0166])
N: 90
Signal to noise ratio: tensor([27.4803, 77.6073, 89.5242, 39.5200])
Bound on condition number: tensor([ 67966.2086, 542061.5357, 721312.9741, 140566.0021])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.008811287444099983, policy loss: 6.6750931023777245
Experience 9, Iter 1, disc loss: 0.005669137320121646, policy loss: 6.810717594668657
Experience 9, Iter 2, disc loss: 0.005091111081545047, policy loss: 6.991838175317015
Experience 9, Iter 3, disc loss: 0.0058740095420975694, policy loss: 6.897966499147837
Experience 9, Iter 4, disc loss: 0.0045829064301249785, policy loss: 6.768190692092225
Experience 9, Iter 5, disc loss: 0.005648732487587225, policy loss: 6.539357274657511
Experience 9, Iter 6, disc loss: 0.006233977005630182, policy loss: 6.204075668353898
Experience 9, Iter 7, disc loss: 0.003483116632944471, policy loss: 7.264456872234298
Experience 9, Iter 8, disc loss: 0.0035878607448173493, policy loss: 7.062741548961759
Experience 9, Iter 9, disc loss: 0.0029328063140279734, policy loss: 7.4668786591388425
Experience 9, Iter 10, disc loss: 0.002008224881475582, policy loss: 7.842668897313189
Experience 9, Iter 11, disc loss: 0.002434405564152733, policy loss: 7.68951517362685
Experience 9, Iter 12, disc loss: 0.0021081957381617524, policy loss: 7.752335361838604
Experience 9, Iter 13, disc loss: 0.00296707975425652, policy loss: 6.98849478382192
Experience 9, Iter 14, disc loss: 0.0034531113480001582, policy loss: 6.728222632687699
Experience 9, Iter 15, disc loss: 0.004875114826010098, policy loss: 6.206431331704055
Experience 9, Iter 16, disc loss: 0.004389716857542709, policy loss: 6.151111744053904
Experience 9, Iter 17, disc loss: 0.00360957924690014, policy loss: 6.6361868271005005
Experience 9, Iter 18, disc loss: 0.0035307610004006408, policy loss: 6.700691351186192
Experience 9, Iter 19, disc loss: 0.00412168525298807, policy loss: 6.56932997857213
Experience 9, Iter 20, disc loss: 0.004398635946422697, policy loss: 6.535751747103978
Experience 9, Iter 21, disc loss: 0.004437975722789058, policy loss: 6.470887712598093
Experience 9, Iter 22, disc loss: 0.004884722284744655, policy loss: 6.238946403429686
Experience 9, Iter 23, disc loss: 0.004854751977544699, policy loss: 6.196867930359058
Experience 9, Iter 24, disc loss: 0.004617488735380463, policy loss: 6.388879177702346
Experience 9, Iter 25, disc loss: 0.004173533500600588, policy loss: 6.90345954425103
Experience 9, Iter 26, disc loss: 0.00496862499677734, policy loss: 6.145841168125509
Experience 9, Iter 27, disc loss: 0.004313108798568493, policy loss: 6.437177767534732
Experience 9, Iter 28, disc loss: 0.0048742488886204085, policy loss: 6.42180694471012
Experience 9, Iter 29, disc loss: 0.003683250926938112, policy loss: 6.541214465355339
Experience 9, Iter 30, disc loss: 0.004052749057922888, policy loss: 6.4838014140693545
Experience 9, Iter 31, disc loss: 0.0036835596338331503, policy loss: 6.738804776575928
Experience 9, Iter 32, disc loss: 0.004228653863447659, policy loss: 6.24892102461944
Experience 9, Iter 33, disc loss: 0.003544131449850903, policy loss: 6.723189109913378
Experience 9, Iter 34, disc loss: 0.004678355235701223, policy loss: 6.210579888127753
Experience 9, Iter 35, disc loss: 0.004861171577028371, policy loss: 6.408464021816004
Experience 9, Iter 36, disc loss: 0.004570061627403404, policy loss: 6.38219231156887
Experience 9, Iter 37, disc loss: 0.005310075264138403, policy loss: 6.116584507202164
Experience 9, Iter 38, disc loss: 0.0043722945730143405, policy loss: 6.647329230680502
Experience 9, Iter 39, disc loss: 0.0038261575947637558, policy loss: 6.464111358125778
Experience 9, Iter 40, disc loss: 0.005017624956026316, policy loss: 6.53047624827256
Experience 9, Iter 41, disc loss: 0.004477402805165981, policy loss: 6.603684597774807
Experience 9, Iter 42, disc loss: 0.0059312024881335475, policy loss: 6.209631228593578
Experience 9, Iter 43, disc loss: 0.00412990484316334, policy loss: 6.53472483384556
Experience 9, Iter 44, disc loss: 0.0044685147308804124, policy loss: 6.5811224481532875
Experience 9, Iter 45, disc loss: 0.004507026045820016, policy loss: 6.317135044259503
Experience 9, Iter 46, disc loss: 0.0035023820457520238, policy loss: 6.701990953501243
Experience 9, Iter 47, disc loss: 0.0034046490755853184, policy loss: 7.16854043231395
Experience 9, Iter 48, disc loss: 0.003463834536685785, policy loss: 7.1761240025623065
Experience 9, Iter 49, disc loss: 0.003906411486290041, policy loss: 6.35938742328228
Experience 9, Iter 50, disc loss: 0.004629351649670621, policy loss: 6.358319880482991
Experience 9, Iter 51, disc loss: 0.004889017561982923, policy loss: 6.239907167756885
Experience 9, Iter 52, disc loss: 0.004510584200083792, policy loss: 6.69863015330535
Experience 9, Iter 53, disc loss: 0.004335826526581401, policy loss: 6.637200824322022
Experience 9, Iter 54, disc loss: 0.00525565608234527, policy loss: 6.679836470543543
Experience 9, Iter 55, disc loss: 0.00421138111507157, policy loss: 6.9644175386082985
Experience 9, Iter 56, disc loss: 0.0051057644597512514, policy loss: 6.782469270242975
Experience 9, Iter 57, disc loss: 0.004711764392469516, policy loss: 6.962241944285235
Experience 9, Iter 58, disc loss: 0.005012736066787578, policy loss: 7.22917564169858
Experience 9, Iter 59, disc loss: 0.00488350688929658, policy loss: 6.94076477998299
Experience 9, Iter 60, disc loss: 0.004094331730267001, policy loss: 7.3429880262154725
Experience 9, Iter 61, disc loss: 0.004529711820923378, policy loss: 6.855273130602768
Experience 9, Iter 62, disc loss: 0.004005069187646788, policy loss: 6.8120050621993
Experience 9, Iter 63, disc loss: 0.0031105686203708074, policy loss: 6.978224215645634
Experience 9, Iter 64, disc loss: 0.003456771458607684, policy loss: 6.906342505421966
Experience 9, Iter 65, disc loss: 0.0029786510219446307, policy loss: 6.917022210518789
Experience 9, Iter 66, disc loss: 0.0029851907581811277, policy loss: 6.8714774463813875
Experience 9, Iter 67, disc loss: 0.002702936074833685, policy loss: 7.193253714714034
Experience 9, Iter 68, disc loss: 0.0029208366651041653, policy loss: 6.873588320880854
Experience 9, Iter 69, disc loss: 0.002955038094937739, policy loss: 6.960984115980852
Experience 9, Iter 70, disc loss: 0.002474416110716857, policy loss: 7.032064794067529
Experience 9, Iter 71, disc loss: 0.002105887867432339, policy loss: 7.403105530998328
Experience 9, Iter 72, disc loss: 0.002227590399362209, policy loss: 7.688130630289907
Experience 9, Iter 73, disc loss: 0.001938911489802314, policy loss: 7.553096495529814
Experience 9, Iter 74, disc loss: 0.001931720738103276, policy loss: 7.70157873137252
Experience 9, Iter 75, disc loss: 0.0016379814588120627, policy loss: 8.26683433961333
Experience 9, Iter 76, disc loss: 0.0014094372410013978, policy loss: 7.907227795503492
Experience 9, Iter 77, disc loss: 0.0015281012189922046, policy loss: 7.85597518766475
Experience 9, Iter 78, disc loss: 0.0013240332574896425, policy loss: 8.116718988106268
Experience 9, Iter 79, disc loss: 0.0012055332830017305, policy loss: 8.208122445746039
Experience 9, Iter 80, disc loss: 0.0012844722699764245, policy loss: 8.161762074518684
Experience 9, Iter 81, disc loss: 0.0016035950203768905, policy loss: 7.845406983418961
Experience 9, Iter 82, disc loss: 0.0016488681198039107, policy loss: 7.83729915746758
Experience 9, Iter 83, disc loss: 0.0016675137825200772, policy loss: 8.117166487470591
Experience 9, Iter 84, disc loss: 0.0015674891440672315, policy loss: 8.106082156559596
Experience 9, Iter 85, disc loss: 0.0017516934600887452, policy loss: 7.780818215733888
Experience 9, Iter 86, disc loss: 0.001659915539643745, policy loss: 7.574869195121968
Experience 9, Iter 87, disc loss: 0.0013675962551185344, policy loss: 8.343133341272642
Experience 9, Iter 88, disc loss: 0.0014400681133763245, policy loss: 7.7728493076992695
Experience 9, Iter 89, disc loss: 0.0012478562741329974, policy loss: 7.926176588485391
Experience 9, Iter 90, disc loss: 0.0009885159403581257, policy loss: 8.436448081506402
Experience 9, Iter 91, disc loss: 0.000921346093576257, policy loss: 8.390901481104496
Experience 9, Iter 92, disc loss: 0.0011166439359988943, policy loss: 7.949902603551857
Experience 9, Iter 93, disc loss: 0.000921953771815559, policy loss: 8.408955294551092
Experience 9, Iter 94, disc loss: 0.0010264569941524186, policy loss: 8.257375955625697
Experience 9, Iter 95, disc loss: 0.001063266774869512, policy loss: 8.040606916191972
Experience 9, Iter 96, disc loss: 0.0009634817610001531, policy loss: 8.263166026560905
Experience 9, Iter 97, disc loss: 0.0008002458541906932, policy loss: 8.383615358981949
Experience 9, Iter 98, disc loss: 0.0008052736947898677, policy loss: 8.226355522698947
Experience 9, Iter 99, disc loss: 0.0007136342653882762, policy loss: 8.36828677422711
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0063],
        [0.0360],
        [0.3345],
        [0.0037]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0337, 0.2307, 0.1827, 0.0068, 0.0021, 1.1353]],

        [[0.0337, 0.2307, 0.1827, 0.0068, 0.0021, 1.1353]],

        [[0.0337, 0.2307, 0.1827, 0.0068, 0.0021, 1.1353]],

        [[0.0337, 0.2307, 0.1827, 0.0068, 0.0021, 1.1353]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0253, 0.1440, 1.3380, 0.0150], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0253, 0.1440, 1.3380, 0.0150])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.953
Iter 2/2000 - Loss: 1.159
Iter 3/2000 - Loss: 0.889
Iter 4/2000 - Loss: 0.884
Iter 5/2000 - Loss: 0.961
Iter 6/2000 - Loss: 0.880
Iter 7/2000 - Loss: 0.761
Iter 8/2000 - Loss: 0.701
Iter 9/2000 - Loss: 0.676
Iter 10/2000 - Loss: 0.620
Iter 11/2000 - Loss: 0.517
Iter 12/2000 - Loss: 0.391
Iter 13/2000 - Loss: 0.262
Iter 14/2000 - Loss: 0.131
Iter 15/2000 - Loss: -0.014
Iter 16/2000 - Loss: -0.186
Iter 17/2000 - Loss: -0.390
Iter 18/2000 - Loss: -0.623
Iter 19/2000 - Loss: -0.880
Iter 20/2000 - Loss: -1.153
Iter 1981/2000 - Loss: -8.140
Iter 1982/2000 - Loss: -8.140
Iter 1983/2000 - Loss: -8.140
Iter 1984/2000 - Loss: -8.140
Iter 1985/2000 - Loss: -8.140
Iter 1986/2000 - Loss: -8.140
Iter 1987/2000 - Loss: -8.140
Iter 1988/2000 - Loss: -8.140
Iter 1989/2000 - Loss: -8.140
Iter 1990/2000 - Loss: -8.140
Iter 1991/2000 - Loss: -8.140
Iter 1992/2000 - Loss: -8.140
Iter 1993/2000 - Loss: -8.140
Iter 1994/2000 - Loss: -8.140
Iter 1995/2000 - Loss: -8.140
Iter 1996/2000 - Loss: -8.140
Iter 1997/2000 - Loss: -8.140
Iter 1998/2000 - Loss: -8.141
Iter 1999/2000 - Loss: -8.141
Iter 2000/2000 - Loss: -8.141
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[17.5954, 11.2923, 15.8682,  9.8780, 16.3559, 44.6652]],

        [[20.8361, 32.4784, 21.1993,  3.3695,  0.8937, 21.3189]],

        [[19.3083, 41.1753, 18.0927,  0.9247,  6.7373, 23.7626]],

        [[20.9512, 39.4307, 13.9920,  2.9716, 13.2942, 40.2827]]])
Signal Variance: tensor([ 0.2569,  1.7330, 15.1211,  0.3742])
Estimated target variance: tensor([0.0253, 0.1440, 1.3380, 0.0150])
N: 100
Signal to noise ratio: tensor([27.0756, 71.4283, 84.5445, 38.6664])
Bound on condition number: tensor([ 73309.6116, 510201.7707, 714777.5035, 149510.0737])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0006718443373835328, policy loss: 8.381196805502112
Experience 10, Iter 1, disc loss: 0.0006229199677936836, policy loss: 8.464443277817278
Experience 10, Iter 2, disc loss: 0.0005468794241019242, policy loss: 8.691535467045508
Experience 10, Iter 3, disc loss: 0.0005072836694659516, policy loss: 8.891296558512828
Experience 10, Iter 4, disc loss: 0.0005008927106909508, policy loss: 8.880129190268203
Experience 10, Iter 5, disc loss: 0.00047993413177189315, policy loss: 8.923372926935693
Experience 10, Iter 6, disc loss: 0.00045329687903810246, policy loss: 9.071132302262303
Experience 10, Iter 7, disc loss: 0.00043512724433181866, policy loss: 9.172978498569663
Experience 10, Iter 8, disc loss: 0.00043752572537012754, policy loss: 9.08193842218492
Experience 10, Iter 9, disc loss: 0.0004109352804236899, policy loss: 9.298464975834246
Experience 10, Iter 10, disc loss: 0.0004242125935026986, policy loss: 9.069001106898906
Experience 10, Iter 11, disc loss: 0.00044829429254449597, policy loss: 8.863578890217356
Experience 10, Iter 12, disc loss: 0.0004314541225478453, policy loss: 8.949954580526704
Experience 10, Iter 13, disc loss: 0.0004374891295344217, policy loss: 8.830889379115224
Experience 10, Iter 14, disc loss: 0.00044907854674416004, policy loss: 8.7421324995777
Experience 10, Iter 15, disc loss: 0.0004787479430441863, policy loss: 8.641895534998513
Experience 10, Iter 16, disc loss: 0.0005058476304218378, policy loss: 8.462866377906543
Experience 10, Iter 17, disc loss: 0.0005364315191541386, policy loss: 8.378109998505048
Experience 10, Iter 18, disc loss: 0.000560068687132225, policy loss: 8.301797909706767
Experience 10, Iter 19, disc loss: 0.0006004242442905739, policy loss: 8.192370804698328
Experience 10, Iter 20, disc loss: 0.0005511838499464143, policy loss: 8.384946461160222
Experience 10, Iter 21, disc loss: 0.0006763258289403549, policy loss: 8.123606607781648
Experience 10, Iter 22, disc loss: 0.0006280935739758428, policy loss: 8.25412035489867
Experience 10, Iter 23, disc loss: 0.0005929244205584856, policy loss: 8.287829153227797
Experience 10, Iter 24, disc loss: 0.0006465400874549505, policy loss: 8.29741861788785
Experience 10, Iter 25, disc loss: 0.0006899518117647661, policy loss: 8.206342789866076
Experience 10, Iter 26, disc loss: 0.0007767327239578256, policy loss: 8.12671278146427
Experience 10, Iter 27, disc loss: 0.0009124722587499417, policy loss: 7.935264264945572
Experience 10, Iter 28, disc loss: 0.0008907175165272668, policy loss: 8.038972657339684
Experience 10, Iter 29, disc loss: 0.000681141184890061, policy loss: 8.407554167512737
Experience 10, Iter 30, disc loss: 0.0007531810181238909, policy loss: 8.343695631867954
Experience 10, Iter 31, disc loss: 0.0009470491972425639, policy loss: 7.920451559115471
Experience 10, Iter 32, disc loss: 0.0007829721616073855, policy loss: 8.152797316199779
Experience 10, Iter 33, disc loss: 0.0009948935647969845, policy loss: 7.957637578739082
Experience 10, Iter 34, disc loss: 0.0010991841863951977, policy loss: 7.9771146915957
Experience 10, Iter 35, disc loss: 0.0009441170321124596, policy loss: 8.014046720689745
Experience 10, Iter 36, disc loss: 0.0007229160455425919, policy loss: 8.434021898160978
Experience 10, Iter 37, disc loss: 0.001151401070766023, policy loss: 7.832022486731716
Experience 10, Iter 38, disc loss: 0.0008936492615683595, policy loss: 8.028977730179149
Experience 10, Iter 39, disc loss: 0.0010258015898093257, policy loss: 8.00712755097308
Experience 10, Iter 40, disc loss: 0.0009579998272796627, policy loss: 8.0311966235477
Experience 10, Iter 41, disc loss: 0.0008616490803595558, policy loss: 8.000641015647501
Experience 10, Iter 42, disc loss: 0.0009055335372488878, policy loss: 8.110950848346494
Experience 10, Iter 43, disc loss: 0.001016901154148842, policy loss: 7.974118059604661
Experience 10, Iter 44, disc loss: 0.0008158663847429366, policy loss: 8.214050536704857
Experience 10, Iter 45, disc loss: 0.0008882823120612225, policy loss: 8.035559046259602
Experience 10, Iter 46, disc loss: 0.0008260310000854311, policy loss: 8.213831190054966
Experience 10, Iter 47, disc loss: 0.0008953665238197407, policy loss: 7.95888018653661
Experience 10, Iter 48, disc loss: 0.0012069618648361392, policy loss: 7.74601026157893
Experience 10, Iter 49, disc loss: 0.0008570578796209235, policy loss: 7.9853322241340425
Experience 10, Iter 50, disc loss: 0.000825344532635413, policy loss: 7.98764710049299
Experience 10, Iter 51, disc loss: 0.0009798217604875378, policy loss: 7.923641832258204
Experience 10, Iter 52, disc loss: 0.0009277219879552932, policy loss: 7.932236427723497
Experience 10, Iter 53, disc loss: 0.0009587730169969368, policy loss: 7.746391937976214
Experience 10, Iter 54, disc loss: 0.0007994890906061927, policy loss: 8.11604348648266
Experience 10, Iter 55, disc loss: 0.0009309303374839765, policy loss: 7.894802281568772
Experience 10, Iter 56, disc loss: 0.0008081664564601131, policy loss: 8.088931517315213
Experience 10, Iter 57, disc loss: 0.0010269761891756309, policy loss: 7.718806238238472
Experience 10, Iter 58, disc loss: 0.0008811907063440023, policy loss: 8.039680117230471
Experience 10, Iter 59, disc loss: 0.0007253930447808678, policy loss: 8.48181645377282
Experience 10, Iter 60, disc loss: 0.0009391908774338263, policy loss: 7.901574327532755
Experience 10, Iter 61, disc loss: 0.0009079442123234691, policy loss: 7.835076346998412
Experience 10, Iter 62, disc loss: 0.0008677241200801701, policy loss: 8.013532414674886
Experience 10, Iter 63, disc loss: 0.0009607768713491376, policy loss: 7.944466058391471
Experience 10, Iter 64, disc loss: 0.0008801332943637687, policy loss: 7.960830943796853
Experience 10, Iter 65, disc loss: 0.0008624873768048275, policy loss: 8.054979991506826
Experience 10, Iter 66, disc loss: 0.000972121777793226, policy loss: 7.888946609739859
Experience 10, Iter 67, disc loss: 0.0007803954907505472, policy loss: 8.124625066530538
Experience 10, Iter 68, disc loss: 0.0007467038295370248, policy loss: 8.330222170809547
Experience 10, Iter 69, disc loss: 0.0007818425236454092, policy loss: 8.206285090439133
Experience 10, Iter 70, disc loss: 0.0009805336240840082, policy loss: 7.940504469481399
Experience 10, Iter 71, disc loss: 0.0011440472510840806, policy loss: 8.205615930941004
Experience 10, Iter 72, disc loss: 0.0009087375031773696, policy loss: 7.94421811730625
Experience 10, Iter 73, disc loss: 0.000777458727698718, policy loss: 8.16791597663316
Experience 10, Iter 74, disc loss: 0.0011145026548692394, policy loss: 7.656694705954294
Experience 10, Iter 75, disc loss: 0.0009335985626850289, policy loss: 7.8764939300191
Experience 10, Iter 76, disc loss: 0.00095219295897332, policy loss: 8.012064490768939
Experience 10, Iter 77, disc loss: 0.0011471583653997982, policy loss: 7.932877514495481
Experience 10, Iter 78, disc loss: 0.0010664787514517307, policy loss: 7.968080775193871
Experience 10, Iter 79, disc loss: 0.0011818977549875123, policy loss: 7.669089025784839
Experience 10, Iter 80, disc loss: 0.0010335345419623602, policy loss: 7.86607786159829
Experience 10, Iter 81, disc loss: 0.0010236039908253476, policy loss: 7.823548031090604
Experience 10, Iter 82, disc loss: 0.001049476404468576, policy loss: 7.758519105868725
Experience 10, Iter 83, disc loss: 0.0010742526819627167, policy loss: 7.717819093517525
Experience 10, Iter 84, disc loss: 0.0010451220043751833, policy loss: 7.62463728566562
Experience 10, Iter 85, disc loss: 0.0010691320624220016, policy loss: 7.69980504519324
Experience 10, Iter 86, disc loss: 0.0011686376057507425, policy loss: 7.560018579275187
Experience 10, Iter 87, disc loss: 0.0013372830370362015, policy loss: 7.448031555147768
Experience 10, Iter 88, disc loss: 0.0012366991908072326, policy loss: 7.657627090748665
Experience 10, Iter 89, disc loss: 0.0011690318967995227, policy loss: 7.746685448745136
Experience 10, Iter 90, disc loss: 0.0012986675253437215, policy loss: 7.645076421201989
Experience 10, Iter 91, disc loss: 0.0014200491408861205, policy loss: 7.660291153336399
Experience 10, Iter 92, disc loss: 0.001450706636932259, policy loss: 7.495505036268325
Experience 10, Iter 93, disc loss: 0.0016136417326432374, policy loss: 7.713638458268816
Experience 10, Iter 94, disc loss: 0.0016860223171590387, policy loss: 7.697361890152721
Experience 10, Iter 95, disc loss: 0.0015379643963979487, policy loss: 7.648135914565234
Experience 10, Iter 96, disc loss: 0.0018093837710636805, policy loss: 7.289300302610768
Experience 10, Iter 97, disc loss: 0.001380683625934444, policy loss: 7.961162977325958
Experience 10, Iter 98, disc loss: 0.00149652458566902, policy loss: 7.585773455151852
Experience 10, Iter 99, disc loss: 0.0014540490799561509, policy loss: 7.846856953250946
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.0340],
        [0.3505],
        [0.0043]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0312, 0.2145, 0.2089, 0.0075, 0.0020, 1.0657]],

        [[0.0312, 0.2145, 0.2089, 0.0075, 0.0020, 1.0657]],

        [[0.0312, 0.2145, 0.2089, 0.0075, 0.0020, 1.0657]],

        [[0.0312, 0.2145, 0.2089, 0.0075, 0.0020, 1.0657]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0234, 0.1360, 1.4021, 0.0173], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0234, 0.1360, 1.4021, 0.0173])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.975
Iter 2/2000 - Loss: 1.219
Iter 3/2000 - Loss: 0.921
Iter 4/2000 - Loss: 0.926
Iter 5/2000 - Loss: 1.014
Iter 6/2000 - Loss: 0.935
Iter 7/2000 - Loss: 0.812
Iter 8/2000 - Loss: 0.748
Iter 9/2000 - Loss: 0.723
Iter 10/2000 - Loss: 0.676
Iter 11/2000 - Loss: 0.587
Iter 12/2000 - Loss: 0.467
Iter 13/2000 - Loss: 0.336
Iter 14/2000 - Loss: 0.197
Iter 15/2000 - Loss: 0.047
Iter 16/2000 - Loss: -0.124
Iter 17/2000 - Loss: -0.320
Iter 18/2000 - Loss: -0.544
Iter 19/2000 - Loss: -0.792
Iter 20/2000 - Loss: -1.058
Iter 1981/2000 - Loss: -8.152
Iter 1982/2000 - Loss: -8.152
Iter 1983/2000 - Loss: -8.152
Iter 1984/2000 - Loss: -8.152
Iter 1985/2000 - Loss: -8.152
Iter 1986/2000 - Loss: -8.152
Iter 1987/2000 - Loss: -8.152
Iter 1988/2000 - Loss: -8.152
Iter 1989/2000 - Loss: -8.152
Iter 1990/2000 - Loss: -8.152
Iter 1991/2000 - Loss: -8.152
Iter 1992/2000 - Loss: -8.152
Iter 1993/2000 - Loss: -8.152
Iter 1994/2000 - Loss: -8.152
Iter 1995/2000 - Loss: -8.152
Iter 1996/2000 - Loss: -8.152
Iter 1997/2000 - Loss: -8.152
Iter 1998/2000 - Loss: -8.152
Iter 1999/2000 - Loss: -8.152
Iter 2000/2000 - Loss: -8.152
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[15.8942, 10.5963, 18.1031,  8.9514, 13.0274, 39.3464]],

        [[18.7409, 17.0454, 19.3637,  1.3883,  0.9313, 17.4474]],

        [[18.8735, 36.4496, 14.2449,  0.9085,  6.0644, 20.2475]],

        [[19.0823, 32.9154, 13.8755,  2.2167, 10.2661, 37.9225]]])
Signal Variance: tensor([ 0.2279,  0.9385, 14.3472,  0.3416])
Estimated target variance: tensor([0.0234, 0.1360, 1.4021, 0.0173])
N: 110
Signal to noise ratio: tensor([25.5734, 52.4307, 86.8582, 37.6471])
Bound on condition number: tensor([ 71940.8244, 302388.4719, 829879.2621, 155904.7882])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0030270553255098242, policy loss: 7.305508721783642
Experience 11, Iter 1, disc loss: 0.0037640408493270594, policy loss: 6.916581510542229
Experience 11, Iter 2, disc loss: 0.004328127206769802, policy loss: 6.974831280367342
Experience 11, Iter 3, disc loss: 0.004115149994228089, policy loss: 6.9146000078811545
Experience 11, Iter 4, disc loss: 0.003770595604706402, policy loss: 7.135430407901991
Experience 11, Iter 5, disc loss: 0.003568173148547682, policy loss: 6.934007441568027
Experience 11, Iter 6, disc loss: 0.004260825338678207, policy loss: 6.766304051666664
Experience 11, Iter 7, disc loss: 0.005198304345582548, policy loss: 6.195357374138479
Experience 11, Iter 8, disc loss: 0.0039060484797064903, policy loss: 6.960727295516357
Experience 11, Iter 9, disc loss: 0.005117242085892979, policy loss: 6.653726764073543
Experience 11, Iter 10, disc loss: 0.005447143291877576, policy loss: 6.340372165073818
Experience 11, Iter 11, disc loss: 0.005511376049832867, policy loss: 6.472787762586196
Experience 11, Iter 12, disc loss: 0.005170623429631518, policy loss: 6.852405576401745
Experience 11, Iter 13, disc loss: 0.00622566198562506, policy loss: 6.325444552951009
Experience 11, Iter 14, disc loss: 0.007291798515709434, policy loss: 5.763263789199871
Experience 11, Iter 15, disc loss: 0.007529696325218399, policy loss: 6.1188302222051725
Experience 11, Iter 16, disc loss: 0.007504922720466596, policy loss: 6.693398984673969
Experience 11, Iter 17, disc loss: 0.008046184940248658, policy loss: 6.019239185043125
Experience 11, Iter 18, disc loss: 0.011497980650030572, policy loss: 5.828844210293481
Experience 11, Iter 19, disc loss: 0.007878699505763587, policy loss: 6.469442492567911
Experience 11, Iter 20, disc loss: 0.012224861623899535, policy loss: 5.549428623823982
Experience 11, Iter 21, disc loss: 0.012673720462977493, policy loss: 5.789132860036892
Experience 11, Iter 22, disc loss: 0.016202907845854538, policy loss: 5.533328411863028
Experience 11, Iter 23, disc loss: 0.015892325659713453, policy loss: 5.576786122874646
Experience 11, Iter 24, disc loss: 0.015985715729015373, policy loss: 5.555149815434429
Experience 11, Iter 25, disc loss: 0.01768634440902801, policy loss: 5.640742684068106
Experience 11, Iter 26, disc loss: 0.018970516111609652, policy loss: 5.52843135650755
Experience 11, Iter 27, disc loss: 0.019806469754069345, policy loss: 5.357204363556492
Experience 11, Iter 28, disc loss: 0.02112977132472378, policy loss: 5.708137761631381
Experience 11, Iter 29, disc loss: 0.0201443503608847, policy loss: 5.277481954150546
Experience 11, Iter 30, disc loss: 0.01918352625669148, policy loss: 5.407779190083035
Experience 11, Iter 31, disc loss: 0.020674184383943542, policy loss: 5.787213183116638
Experience 11, Iter 32, disc loss: 0.02036707562126521, policy loss: 5.1991617739839615
Experience 11, Iter 33, disc loss: 0.023908475338338057, policy loss: 5.07046957238751
Experience 11, Iter 34, disc loss: 0.020839056862553232, policy loss: 5.251601490882479
Experience 11, Iter 35, disc loss: 0.021022702212352994, policy loss: 5.279844615271915
Experience 11, Iter 36, disc loss: 0.024352798817239226, policy loss: 5.259021763470697
Experience 11, Iter 37, disc loss: 0.018274675853801298, policy loss: 5.697382081832159
Experience 11, Iter 38, disc loss: 0.0175737509465023, policy loss: 5.650931485817382
Experience 11, Iter 39, disc loss: 0.016480957579571503, policy loss: 5.868858164977097
Experience 11, Iter 40, disc loss: 0.018015156822812135, policy loss: 6.284811286757218
Experience 11, Iter 41, disc loss: 0.01771628482138568, policy loss: 5.667840547681218
Experience 11, Iter 42, disc loss: 0.01508475042359456, policy loss: 6.225051831914339
Experience 11, Iter 43, disc loss: 0.011750351858501712, policy loss: 6.798905699212899
Experience 11, Iter 44, disc loss: 0.013395115927435124, policy loss: 5.805133106363057
Experience 11, Iter 45, disc loss: 0.013383224667318323, policy loss: 6.148611159450004
Experience 11, Iter 46, disc loss: 0.011487749069215295, policy loss: 6.347569556615847
Experience 11, Iter 47, disc loss: 0.011232030025009528, policy loss: 5.847299365846336
Experience 11, Iter 48, disc loss: 0.009760994363590781, policy loss: 6.252277352057545
Experience 11, Iter 49, disc loss: 0.008892365100938588, policy loss: 6.320205624455935
Experience 11, Iter 50, disc loss: 0.013161191224715965, policy loss: 5.989048644714934
Experience 11, Iter 51, disc loss: 0.011372815854040381, policy loss: 6.038063619351224
Experience 11, Iter 52, disc loss: 0.010337440078928051, policy loss: 6.015851948923208
Experience 11, Iter 53, disc loss: 0.007727011331054372, policy loss: 6.482442961340124
Experience 11, Iter 54, disc loss: 0.0077304109894765684, policy loss: 6.887751993051353
Experience 11, Iter 55, disc loss: 0.008290812000896743, policy loss: 6.50246504882555
Experience 11, Iter 56, disc loss: 0.008934423934913218, policy loss: 7.207339743629573
Experience 11, Iter 57, disc loss: 0.00879242446724109, policy loss: 6.929573112488932
Experience 11, Iter 58, disc loss: 0.006925153740605015, policy loss: 6.730568089162629
Experience 11, Iter 59, disc loss: 0.006575477248641846, policy loss: 6.615534723433164
Experience 11, Iter 60, disc loss: 0.006122232293680758, policy loss: 7.422862035750892
Experience 11, Iter 61, disc loss: 0.007329053782223298, policy loss: 6.846746081680418
Experience 11, Iter 62, disc loss: 0.008382610701597857, policy loss: 6.7769488434005964
Experience 11, Iter 63, disc loss: 0.007448593237956988, policy loss: 7.107929849301365
Experience 11, Iter 64, disc loss: 0.0061569960116089305, policy loss: 6.764747781738929
Experience 11, Iter 65, disc loss: 0.00732751497559498, policy loss: 7.059907615367967
Experience 11, Iter 66, disc loss: 0.0068170952583529705, policy loss: 6.7262221361604295
Experience 11, Iter 67, disc loss: 0.005790668895372073, policy loss: 6.8467921140661865
Experience 11, Iter 68, disc loss: 0.00695052735495485, policy loss: 6.633011630429532
Experience 11, Iter 69, disc loss: 0.004715800335885707, policy loss: 7.475928377295949
Experience 11, Iter 70, disc loss: 0.005933287353564664, policy loss: 6.594215165656079
Experience 11, Iter 71, disc loss: 0.005006688553557475, policy loss: 7.0211095435978645
Experience 11, Iter 72, disc loss: 0.0061194967410882475, policy loss: 7.1421923402375995
Experience 11, Iter 73, disc loss: 0.006481880551311568, policy loss: 6.5331491272844895
Experience 11, Iter 74, disc loss: 0.005939728399214882, policy loss: 6.72445361080176
Experience 11, Iter 75, disc loss: 0.006433608107421827, policy loss: 6.666908793905342
Experience 11, Iter 76, disc loss: 0.005834516737450604, policy loss: 6.901768224940369
Experience 11, Iter 77, disc loss: 0.006120811723182291, policy loss: 6.553329371647797
Experience 11, Iter 78, disc loss: 0.006115171584848223, policy loss: 6.722565199191571
Experience 11, Iter 79, disc loss: 0.007001838751017985, policy loss: 6.816864583710114
Experience 11, Iter 80, disc loss: 0.006769159619830771, policy loss: 6.7876138768655965
Experience 11, Iter 81, disc loss: 0.0057252073800893545, policy loss: 6.910090959491054
Experience 11, Iter 82, disc loss: 0.005038280375763782, policy loss: 7.291410600477098
Experience 11, Iter 83, disc loss: 0.005561458669713129, policy loss: 6.983247932447517
Experience 11, Iter 84, disc loss: 0.004433971004461015, policy loss: 7.534942105801008
Experience 11, Iter 85, disc loss: 0.005658424819168607, policy loss: 7.467990158492354
Experience 11, Iter 86, disc loss: 0.006360220400094393, policy loss: 6.913168953137701
Experience 11, Iter 87, disc loss: 0.004981681087196343, policy loss: 7.088900439323607
Experience 11, Iter 88, disc loss: 0.005884786995336745, policy loss: 7.261215022811747
Experience 11, Iter 89, disc loss: 0.004164849980187558, policy loss: 7.739762398131999
Experience 11, Iter 90, disc loss: 0.005707818890781998, policy loss: 7.489394809320801
Experience 11, Iter 91, disc loss: 0.005443397911260938, policy loss: 7.259214152361533
Experience 11, Iter 92, disc loss: 0.0055199090114659895, policy loss: 7.464880896069174
Experience 11, Iter 93, disc loss: 0.004496424132270558, policy loss: 7.532878357290626
Experience 11, Iter 94, disc loss: 0.0055028924473726065, policy loss: 7.241863052938065
Experience 11, Iter 95, disc loss: 0.0042504558840916265, policy loss: 7.552166243179935
Experience 11, Iter 96, disc loss: 0.004978727596154031, policy loss: 7.791687168756103
Experience 11, Iter 97, disc loss: 0.0053998167189202965, policy loss: 7.178903000458925
Experience 11, Iter 98, disc loss: 0.003932562521747511, policy loss: 7.858045104251504
Experience 11, Iter 99, disc loss: 0.00392081174802038, policy loss: 7.387567138461138
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.0455],
        [0.5336],
        [0.0082]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0294, 0.2204, 0.3860, 0.0098, 0.0053, 1.4225]],

        [[0.0294, 0.2204, 0.3860, 0.0098, 0.0053, 1.4225]],

        [[0.0294, 0.2204, 0.3860, 0.0098, 0.0053, 1.4225]],

        [[0.0294, 0.2204, 0.3860, 0.0098, 0.0053, 1.4225]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0243, 0.1819, 2.1343, 0.0327], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0243, 0.1819, 2.1343, 0.0327])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.662
Iter 2/2000 - Loss: 1.738
Iter 3/2000 - Loss: 1.579
Iter 4/2000 - Loss: 1.559
Iter 5/2000 - Loss: 1.579
Iter 6/2000 - Loss: 1.502
Iter 7/2000 - Loss: 1.409
Iter 8/2000 - Loss: 1.344
Iter 9/2000 - Loss: 1.283
Iter 10/2000 - Loss: 1.189
Iter 11/2000 - Loss: 1.063
Iter 12/2000 - Loss: 0.918
Iter 13/2000 - Loss: 0.760
Iter 14/2000 - Loss: 0.588
Iter 15/2000 - Loss: 0.393
Iter 16/2000 - Loss: 0.174
Iter 17/2000 - Loss: -0.067
Iter 18/2000 - Loss: -0.327
Iter 19/2000 - Loss: -0.599
Iter 20/2000 - Loss: -0.880
Iter 1981/2000 - Loss: -7.817
Iter 1982/2000 - Loss: -7.817
Iter 1983/2000 - Loss: -7.817
Iter 1984/2000 - Loss: -7.817
Iter 1985/2000 - Loss: -7.817
Iter 1986/2000 - Loss: -7.817
Iter 1987/2000 - Loss: -7.817
Iter 1988/2000 - Loss: -7.817
Iter 1989/2000 - Loss: -7.817
Iter 1990/2000 - Loss: -7.817
Iter 1991/2000 - Loss: -7.817
Iter 1992/2000 - Loss: -7.817
Iter 1993/2000 - Loss: -7.817
Iter 1994/2000 - Loss: -7.817
Iter 1995/2000 - Loss: -7.818
Iter 1996/2000 - Loss: -7.818
Iter 1997/2000 - Loss: -7.818
Iter 1998/2000 - Loss: -7.818
Iter 1999/2000 - Loss: -7.818
Iter 2000/2000 - Loss: -7.818
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[14.2364,  4.9672, 31.3322,  8.9449, 14.9457, 32.6872]],

        [[17.1956, 30.0566, 11.6221,  1.3050,  3.8274, 25.7292]],

        [[19.6350, 39.0843, 13.3256,  0.9612,  2.2558, 22.7816]],

        [[20.0114, 34.0848, 21.0597,  3.4343,  1.4412, 40.5898]]])
Signal Variance: tensor([ 0.1187,  2.0522, 18.4737,  0.6399])
Estimated target variance: tensor([0.0243, 0.1819, 2.1343, 0.0327])
N: 120
Signal to noise ratio: tensor([17.9933, 76.0508, 99.3319, 49.4477])
Bound on condition number: tensor([  38852.1911,  694048.2981, 1184020.7756,  293410.0989])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0022209488551986336, policy loss: 9.394211796080869
Experience 12, Iter 1, disc loss: 0.002138735236860698, policy loss: 9.354448324865123
Experience 12, Iter 2, disc loss: 0.0020537014778312115, policy loss: 8.862221724917347
Experience 12, Iter 3, disc loss: 0.001916344813229626, policy loss: 9.218447632881976
Experience 12, Iter 4, disc loss: 0.0018131766314546023, policy loss: 9.0964966316849
Experience 12, Iter 5, disc loss: 0.0017008927334300622, policy loss: 8.918127049413076
Experience 12, Iter 6, disc loss: 0.0015822145608405309, policy loss: 9.088591357974225
Experience 12, Iter 7, disc loss: 0.0014977199275927429, policy loss: 8.475286588653985
Experience 12, Iter 8, disc loss: 0.0013235775874939832, policy loss: 8.890827073708223
Experience 12, Iter 9, disc loss: 0.001326427485010606, policy loss: 8.662104670140629
Experience 12, Iter 10, disc loss: 0.0012432428715423, policy loss: 8.324679439300267
Experience 12, Iter 11, disc loss: 0.001282991638540136, policy loss: 7.994336395120689
Experience 12, Iter 12, disc loss: 0.0011154930783788392, policy loss: 8.208173724431427
Experience 12, Iter 13, disc loss: 0.0010875556855311618, policy loss: 8.312742886411838
Experience 12, Iter 14, disc loss: 0.0011105209310349152, policy loss: 8.089979468961973
Experience 12, Iter 15, disc loss: 0.0010202186518047643, policy loss: 8.03913027981317
Experience 12, Iter 16, disc loss: 0.001053237232216151, policy loss: 8.0592356224566
Experience 12, Iter 17, disc loss: 0.0012795792660224062, policy loss: 7.84494722444585
Experience 12, Iter 18, disc loss: 0.0010407779672403803, policy loss: 7.879253741395935
Experience 12, Iter 19, disc loss: 0.0011485611833324102, policy loss: 7.86344270816253
Experience 12, Iter 20, disc loss: 0.0013271081697604826, policy loss: 7.324155790634923
Experience 12, Iter 21, disc loss: 0.001389448733082712, policy loss: 7.526814329466926
Experience 12, Iter 22, disc loss: 0.0014998064794955058, policy loss: 7.540779060019554
Experience 12, Iter 23, disc loss: 0.0014802304523935036, policy loss: 7.355521002912487
Experience 12, Iter 24, disc loss: 0.0012796160505818065, policy loss: 7.598508211156208
Experience 12, Iter 25, disc loss: 0.001655960602322366, policy loss: 7.251193389069546
Experience 12, Iter 26, disc loss: 0.0019954820595003368, policy loss: 7.154691247994551
Experience 12, Iter 27, disc loss: 0.0015291404362217653, policy loss: 7.332991970160388
Experience 12, Iter 28, disc loss: 0.0015391553840376673, policy loss: 7.744643232453111
Experience 12, Iter 29, disc loss: 0.0016918519379548623, policy loss: 7.8749821241781905
Experience 12, Iter 30, disc loss: 0.001665784379918927, policy loss: 8.023775021284377
Experience 12, Iter 31, disc loss: 0.0016096761728891977, policy loss: 7.795545716130394
Experience 12, Iter 32, disc loss: 0.0014273390724185342, policy loss: 7.935407122287085
Experience 12, Iter 33, disc loss: 0.0015502992957198087, policy loss: 7.656271512817638
Experience 12, Iter 34, disc loss: 0.0014678085951803418, policy loss: 7.839560435057275
Experience 12, Iter 35, disc loss: 0.0017692272811238655, policy loss: 7.79462204771736
Experience 12, Iter 36, disc loss: 0.002036714229799307, policy loss: 7.723315000188297
Experience 12, Iter 37, disc loss: 0.0016364542122020427, policy loss: 7.569900318008797
Experience 12, Iter 38, disc loss: 0.0018520100359439087, policy loss: 7.397849489769311
Experience 12, Iter 39, disc loss: 0.0017421762246071016, policy loss: 7.930298655838122
Experience 12, Iter 40, disc loss: 0.0017327824303351629, policy loss: 7.749610936008138
Experience 12, Iter 41, disc loss: 0.0019740732651518677, policy loss: 7.992374443641848
Experience 12, Iter 42, disc loss: 0.0017729282985921315, policy loss: 7.87675642008219
Experience 12, Iter 43, disc loss: 0.0024405239534192654, policy loss: 7.750015234151515
Experience 12, Iter 44, disc loss: 0.00192070936618569, policy loss: 7.857933694704131
Experience 12, Iter 45, disc loss: 0.0021229343952996757, policy loss: 8.1631452794382
Experience 12, Iter 46, disc loss: 0.0017970243047641878, policy loss: 8.142621736833297
Experience 12, Iter 47, disc loss: 0.0021088944319009708, policy loss: 7.756420659011569
Experience 12, Iter 48, disc loss: 0.001927664602697044, policy loss: 8.374919223551426
Experience 12, Iter 49, disc loss: 0.0019053451587570346, policy loss: 8.363534121836262
Experience 12, Iter 50, disc loss: 0.00238791149073523, policy loss: 8.187326125353515
Experience 12, Iter 51, disc loss: 0.001974789479100897, policy loss: 8.504936594829758
Experience 12, Iter 52, disc loss: 0.0017887313618110256, policy loss: 8.098363173437225
Experience 12, Iter 53, disc loss: 0.0027952941869939358, policy loss: 7.7922260113876085
Experience 12, Iter 54, disc loss: 0.002086078564136875, policy loss: 8.186478225293268
Experience 12, Iter 55, disc loss: 0.0020413529949439005, policy loss: 8.417837539992885
Experience 12, Iter 56, disc loss: 0.0020920834326070047, policy loss: 8.156726748345248
Experience 12, Iter 57, disc loss: 0.0020038089812395464, policy loss: 8.48028454619074
Experience 12, Iter 58, disc loss: 0.0021745279254817692, policy loss: 8.247758128286593
Experience 12, Iter 59, disc loss: 0.0018458655772042416, policy loss: 8.366925718002246
Experience 12, Iter 60, disc loss: 0.002206918473074809, policy loss: 8.347983216261339
Experience 12, Iter 61, disc loss: 0.0020511274088684207, policy loss: 8.308618308517774
Experience 12, Iter 62, disc loss: 0.00277871510272205, policy loss: 8.234848188447481
Experience 12, Iter 63, disc loss: 0.0021134171881623774, policy loss: 7.949371335911612
Experience 12, Iter 64, disc loss: 0.0017436223905753056, policy loss: 8.506128377196223
Experience 12, Iter 65, disc loss: 0.00204460137820779, policy loss: 8.083783259611307
Experience 12, Iter 66, disc loss: 0.0024478341809417567, policy loss: 7.920841588062168
Experience 12, Iter 67, disc loss: 0.0025107393944727168, policy loss: 8.303746928952613
Experience 12, Iter 68, disc loss: 0.0026519888949822932, policy loss: 7.845196901844318
Experience 12, Iter 69, disc loss: 0.0020593787008941255, policy loss: 8.302707262645134
Experience 12, Iter 70, disc loss: 0.0022349511978495633, policy loss: 7.808519974688945
Experience 12, Iter 71, disc loss: 0.003031722584775426, policy loss: 7.9061585453123095
Experience 12, Iter 72, disc loss: 0.0023069420324063543, policy loss: 7.81737227332342
Experience 12, Iter 73, disc loss: 0.0027584289716956183, policy loss: 7.526174667204699
Experience 12, Iter 74, disc loss: 0.002962098240915794, policy loss: 7.947542971211256
Experience 12, Iter 75, disc loss: 0.0032493961907223397, policy loss: 8.023946748721812
Experience 12, Iter 76, disc loss: 0.0022230775645632707, policy loss: 8.075890686520808
Experience 12, Iter 77, disc loss: 0.0028130311259289952, policy loss: 8.065163384532575
Experience 12, Iter 78, disc loss: 0.002109168342362717, policy loss: 8.098440750118314
Experience 12, Iter 79, disc loss: 0.0021355563526293145, policy loss: 8.155331511938334
Experience 12, Iter 80, disc loss: 0.002845448532297048, policy loss: 7.813488682906332
Experience 12, Iter 81, disc loss: 0.0029086715151974696, policy loss: 7.739611858303156
Experience 12, Iter 82, disc loss: 0.002456373941277964, policy loss: 7.886634766173762
Experience 12, Iter 83, disc loss: 0.002905138111639266, policy loss: 7.813080844118615
Experience 12, Iter 84, disc loss: 0.0023392388962974915, policy loss: 8.153064985004255
Experience 12, Iter 85, disc loss: 0.001983215112658413, policy loss: 8.140813967485029
Experience 12, Iter 86, disc loss: 0.0031093229921074655, policy loss: 7.983931670691236
Experience 12, Iter 87, disc loss: 0.0029335698377875585, policy loss: 7.631204677500245
Experience 12, Iter 88, disc loss: 0.0026383862799579712, policy loss: 8.403451423897728
Experience 12, Iter 89, disc loss: 0.0033588052913019815, policy loss: 8.330770942537686
Experience 12, Iter 90, disc loss: 0.002023287462840696, policy loss: 8.389943024871986
Experience 12, Iter 91, disc loss: 0.002179949547363359, policy loss: 8.068572647466773
Experience 12, Iter 92, disc loss: 0.0027211443870138697, policy loss: 7.994525730989616
Experience 12, Iter 93, disc loss: 0.002033726796964068, policy loss: 8.059288554358071
Experience 12, Iter 94, disc loss: 0.0025774307696232518, policy loss: 7.628063256929714
Experience 12, Iter 95, disc loss: 0.002197866444536268, policy loss: 7.762859606388654
Experience 12, Iter 96, disc loss: 0.002033972245140757, policy loss: 7.882626130146129
Experience 12, Iter 97, disc loss: 0.0026809163272064757, policy loss: 7.614385512952968
Experience 12, Iter 98, disc loss: 0.0018865360711945705, policy loss: 8.113980221410344
Experience 12, Iter 99, disc loss: 0.002630713500194906, policy loss: 7.313189022560074
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.0507],
        [0.6416],
        [0.0104]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0274, 0.2144, 0.4903, 0.0121, 0.0066, 1.5319]],

        [[0.0274, 0.2144, 0.4903, 0.0121, 0.0066, 1.5319]],

        [[0.0274, 0.2144, 0.4903, 0.0121, 0.0066, 1.5319]],

        [[0.0274, 0.2144, 0.4903, 0.0121, 0.0066, 1.5319]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0234, 0.2027, 2.5663, 0.0414], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0234, 0.2027, 2.5663, 0.0414])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.908
Iter 2/2000 - Loss: 1.940
Iter 3/2000 - Loss: 1.814
Iter 4/2000 - Loss: 1.785
Iter 5/2000 - Loss: 1.790
Iter 6/2000 - Loss: 1.715
Iter 7/2000 - Loss: 1.630
Iter 8/2000 - Loss: 1.567
Iter 9/2000 - Loss: 1.495
Iter 10/2000 - Loss: 1.387
Iter 11/2000 - Loss: 1.251
Iter 12/2000 - Loss: 1.100
Iter 13/2000 - Loss: 0.938
Iter 14/2000 - Loss: 0.757
Iter 15/2000 - Loss: 0.551
Iter 16/2000 - Loss: 0.319
Iter 17/2000 - Loss: 0.064
Iter 18/2000 - Loss: -0.205
Iter 19/2000 - Loss: -0.485
Iter 20/2000 - Loss: -0.770
Iter 1981/2000 - Loss: -7.846
Iter 1982/2000 - Loss: -7.846
Iter 1983/2000 - Loss: -7.846
Iter 1984/2000 - Loss: -7.846
Iter 1985/2000 - Loss: -7.846
Iter 1986/2000 - Loss: -7.846
Iter 1987/2000 - Loss: -7.846
Iter 1988/2000 - Loss: -7.846
Iter 1989/2000 - Loss: -7.846
Iter 1990/2000 - Loss: -7.846
Iter 1991/2000 - Loss: -7.846
Iter 1992/2000 - Loss: -7.846
Iter 1993/2000 - Loss: -7.846
Iter 1994/2000 - Loss: -7.846
Iter 1995/2000 - Loss: -7.846
Iter 1996/2000 - Loss: -7.846
Iter 1997/2000 - Loss: -7.846
Iter 1998/2000 - Loss: -7.846
Iter 1999/2000 - Loss: -7.846
Iter 2000/2000 - Loss: -7.846
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[14.0947, 10.7444, 34.1432,  8.5094, 17.1439, 38.4357]],

        [[14.0969, 31.1322, 11.0007,  1.3284,  4.3230, 25.5789]],

        [[17.9436, 41.2272, 10.9517,  1.0580,  1.7643, 22.4902]],

        [[17.8575, 35.3819, 23.1188,  3.6476,  1.8166, 44.8863]]])
Signal Variance: tensor([ 0.2084,  1.9602, 16.0413,  0.7080])
Estimated target variance: tensor([0.0234, 0.2027, 2.5663, 0.0414])
N: 130
Signal to noise ratio: tensor([24.2667, 76.0821, 94.9541, 51.6635])
Bound on condition number: tensor([  76554.4656,  752504.4976, 1172116.5387,  346986.2009])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0020235427350628445, policy loss: 7.78456248144207
Experience 13, Iter 1, disc loss: 0.002006587923437393, policy loss: 7.390865578159822
Experience 13, Iter 2, disc loss: 0.002116007312866281, policy loss: 7.473266293330658
Experience 13, Iter 3, disc loss: 0.00224564677079062, policy loss: 7.411151673713541
Experience 13, Iter 4, disc loss: 0.002350791649442437, policy loss: 7.187431938642588
Experience 13, Iter 5, disc loss: 0.002429753017766969, policy loss: 7.15200157994922
Experience 13, Iter 6, disc loss: 0.0021684298472102468, policy loss: 7.445730516208831
Experience 13, Iter 7, disc loss: 0.002027095028659224, policy loss: 7.537811756268056
Experience 13, Iter 8, disc loss: 0.0021827316491497095, policy loss: 7.175801239221357
Experience 13, Iter 9, disc loss: 0.00224502567962751, policy loss: 7.1494389141117765
Experience 13, Iter 10, disc loss: 0.002758291677364205, policy loss: 7.1900218108366465
Experience 13, Iter 11, disc loss: 0.002700476425172699, policy loss: 7.203510652780338
Experience 13, Iter 12, disc loss: 0.0026713186759088348, policy loss: 6.8923886088939
Experience 13, Iter 13, disc loss: 0.003498649931842176, policy loss: 6.481340836561155
Experience 13, Iter 14, disc loss: 0.005100179112125932, policy loss: 6.696807287757533
Experience 13, Iter 15, disc loss: 0.0034615119032014334, policy loss: 6.868988293627879
Experience 13, Iter 16, disc loss: 0.006422595683327764, policy loss: 6.335209247606979
Experience 13, Iter 17, disc loss: 0.004759855554782328, policy loss: 6.826228631093608
Experience 13, Iter 18, disc loss: 0.0061811221825684905, policy loss: 7.2932613526673205
Experience 13, Iter 19, disc loss: 0.006073873426642367, policy loss: 7.302055338846932
Experience 13, Iter 20, disc loss: 0.0051658619966435705, policy loss: 7.229642051830277
Experience 13, Iter 21, disc loss: 0.007323965891482449, policy loss: 6.960067242682621
Experience 13, Iter 22, disc loss: 0.006290115620433473, policy loss: 7.727925650594242
Experience 13, Iter 23, disc loss: 0.005581330245623328, policy loss: 7.325427006030269
Experience 13, Iter 24, disc loss: 0.005674033690616602, policy loss: 7.971985109662443
Experience 13, Iter 25, disc loss: 0.00596839673231246, policy loss: 7.694087422916045
Experience 13, Iter 26, disc loss: 0.008965555356691676, policy loss: 7.574077031992095
Experience 13, Iter 27, disc loss: 0.006747877328845926, policy loss: 7.550819371304129
Experience 13, Iter 28, disc loss: 0.00797107984057434, policy loss: 7.673210866260096
Experience 13, Iter 29, disc loss: 0.007844556843516561, policy loss: 7.15493425950329
Experience 13, Iter 30, disc loss: 0.007238392767129697, policy loss: 7.146930792075466
Experience 13, Iter 31, disc loss: 0.0062846298607913504, policy loss: 7.632069787918369
Experience 13, Iter 32, disc loss: 0.008457278742793133, policy loss: 7.031523407589152
Experience 13, Iter 33, disc loss: 0.006748026059335638, policy loss: 8.130303540153024
Experience 13, Iter 34, disc loss: 0.006609953387949148, policy loss: 7.815870217546399
Experience 13, Iter 35, disc loss: 0.008550987037869408, policy loss: 7.43252307743008
Experience 13, Iter 36, disc loss: 0.015056752287114901, policy loss: 7.4717872344929885
Experience 13, Iter 37, disc loss: 0.0056947340247582075, policy loss: 7.785841489223502
Experience 13, Iter 38, disc loss: 0.00527537742593014, policy loss: 8.045836512722168
Experience 13, Iter 39, disc loss: 0.009222643657793902, policy loss: 7.645002111703414
Experience 13, Iter 40, disc loss: 0.004653347728681511, policy loss: 8.511353170798333
Experience 13, Iter 41, disc loss: 0.006057233164579191, policy loss: 8.28805182268173
Experience 13, Iter 42, disc loss: 0.007191051754529996, policy loss: 7.3356916428811925
Experience 13, Iter 43, disc loss: 0.0060288304517753626, policy loss: 7.578367285185735
Experience 13, Iter 44, disc loss: 0.005863787136940582, policy loss: 7.695396981475817
Experience 13, Iter 45, disc loss: 0.007404023823518156, policy loss: 7.393367610621887
Experience 13, Iter 46, disc loss: 0.011939280047859378, policy loss: 7.367923346349718
Experience 13, Iter 47, disc loss: 0.006352716208322954, policy loss: 7.869303483954129
Experience 13, Iter 48, disc loss: 0.007515632488334595, policy loss: 7.8155741780615
Experience 13, Iter 49, disc loss: 0.006062453888947784, policy loss: 7.588252082033103
Experience 13, Iter 50, disc loss: 0.006077493445704409, policy loss: 7.8737095042444345
Experience 13, Iter 51, disc loss: 0.006553444882717067, policy loss: 8.04506199790353
Experience 13, Iter 52, disc loss: 0.005231873040328483, policy loss: 7.941641309657346
Experience 13, Iter 53, disc loss: 0.005818814925179037, policy loss: 7.966279571652324
Experience 13, Iter 54, disc loss: 0.00562227529203998, policy loss: 8.213806633329007
Experience 13, Iter 55, disc loss: 0.004916411699652073, policy loss: 7.957088233522442
Experience 13, Iter 56, disc loss: 0.004762476798772161, policy loss: 8.371880293063528
Experience 13, Iter 57, disc loss: 0.004811697734489671, policy loss: 7.746681023974115
Experience 13, Iter 58, disc loss: 0.006634263829006952, policy loss: 7.7311965127447815
Experience 13, Iter 59, disc loss: 0.004674713134668282, policy loss: 8.055258887648069
Experience 13, Iter 60, disc loss: 0.006994125242852023, policy loss: 8.000227988574073
Experience 13, Iter 61, disc loss: 0.004411804576705967, policy loss: 7.8450634462647635
Experience 13, Iter 62, disc loss: 0.006031559444466615, policy loss: 7.416378143749132
Experience 13, Iter 63, disc loss: 0.007935156753025412, policy loss: 7.509570428408628
Experience 13, Iter 64, disc loss: 0.00462541408488285, policy loss: 8.534917554108821
Experience 13, Iter 65, disc loss: 0.004203123771601288, policy loss: 8.339626219561227
Experience 13, Iter 66, disc loss: 0.00943362204848884, policy loss: 8.121661566792902
Experience 13, Iter 67, disc loss: 0.005279481425700847, policy loss: 8.258571768208851
Experience 13, Iter 68, disc loss: 0.00578399784626266, policy loss: 8.083096287329191
Experience 13, Iter 69, disc loss: 0.008124817938075894, policy loss: 8.442769200254592
Experience 13, Iter 70, disc loss: 0.004958095928654664, policy loss: 8.604511895385693
Experience 13, Iter 71, disc loss: 0.00538134784919856, policy loss: 7.9065816007890275
Experience 13, Iter 72, disc loss: 0.0075493465341986855, policy loss: 8.433625433358461
Experience 13, Iter 73, disc loss: 0.006021520552023792, policy loss: 8.385245702380079
Experience 13, Iter 74, disc loss: 0.006098578111288895, policy loss: 8.616267787657245
Experience 13, Iter 75, disc loss: 0.0066728822142034915, policy loss: 8.003367602418614
Experience 13, Iter 76, disc loss: 0.006469669444485682, policy loss: 7.731182860974711
Experience 13, Iter 77, disc loss: 0.006710032270608323, policy loss: 8.166367983276798
Experience 13, Iter 78, disc loss: 0.008137300562918342, policy loss: 8.621733333172505
Experience 13, Iter 79, disc loss: 0.007711663163281091, policy loss: 7.986784508445177
Experience 13, Iter 80, disc loss: 0.005611682533134965, policy loss: 8.181399330607952
Experience 13, Iter 81, disc loss: 0.01039640087266926, policy loss: 9.254321013922375
Experience 13, Iter 82, disc loss: 0.006047876510244808, policy loss: 8.084130688545123
Experience 13, Iter 83, disc loss: 0.004923304472800337, policy loss: 8.757005626567942
Experience 13, Iter 84, disc loss: 0.006519471458956825, policy loss: 8.63136539458438
Experience 13, Iter 85, disc loss: 0.007594571079202761, policy loss: 8.832806684518525
Experience 13, Iter 86, disc loss: 0.005977009033763979, policy loss: 8.012179225402832
Experience 13, Iter 87, disc loss: 0.0058584773918322515, policy loss: 8.52119553696593
Experience 13, Iter 88, disc loss: 0.0051423934928545465, policy loss: 8.83212029502275
Experience 13, Iter 89, disc loss: 0.004687865846107843, policy loss: 8.380978055991477
Experience 13, Iter 90, disc loss: 0.0039779527845336445, policy loss: 8.73922971413765
Experience 13, Iter 91, disc loss: 0.0036550353120274885, policy loss: 8.928673742584607
Experience 13, Iter 92, disc loss: 0.004423155510975631, policy loss: 8.570672651995114
Experience 13, Iter 93, disc loss: 0.008098797862831849, policy loss: 8.247978466272851
Experience 13, Iter 94, disc loss: 0.00447755578526516, policy loss: 8.720353383739862
Experience 13, Iter 95, disc loss: 0.0035692009520680405, policy loss: 8.883016519711116
Experience 13, Iter 96, disc loss: 0.003237939688218487, policy loss: 8.486904386820102
Experience 13, Iter 97, disc loss: 0.0035985304615793173, policy loss: 8.295926958308952
Experience 13, Iter 98, disc loss: 0.004170949357492871, policy loss: 8.621567813475192
Experience 13, Iter 99, disc loss: 0.005940308337408955, policy loss: 9.237839209373352
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.0642],
        [0.7637],
        [0.0152]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0260, 0.2133, 0.7075, 0.0140, 0.0112, 1.8439]],

        [[0.0260, 0.2133, 0.7075, 0.0140, 0.0112, 1.8439]],

        [[0.0260, 0.2133, 0.7075, 0.0140, 0.0112, 1.8439]],

        [[0.0260, 0.2133, 0.7075, 0.0140, 0.0112, 1.8439]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0229, 0.2568, 3.0548, 0.0607], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0229, 0.2568, 3.0548, 0.0607])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.290
Iter 2/2000 - Loss: 2.274
Iter 3/2000 - Loss: 2.186
Iter 4/2000 - Loss: 2.136
Iter 5/2000 - Loss: 2.128
Iter 6/2000 - Loss: 2.065
Iter 7/2000 - Loss: 1.977
Iter 8/2000 - Loss: 1.897
Iter 9/2000 - Loss: 1.810
Iter 10/2000 - Loss: 1.695
Iter 11/2000 - Loss: 1.551
Iter 12/2000 - Loss: 1.389
Iter 13/2000 - Loss: 1.212
Iter 14/2000 - Loss: 1.019
Iter 15/2000 - Loss: 0.804
Iter 16/2000 - Loss: 0.566
Iter 17/2000 - Loss: 0.308
Iter 18/2000 - Loss: 0.038
Iter 19/2000 - Loss: -0.240
Iter 20/2000 - Loss: -0.523
Iter 1981/2000 - Loss: -7.610
Iter 1982/2000 - Loss: -7.610
Iter 1983/2000 - Loss: -7.610
Iter 1984/2000 - Loss: -7.610
Iter 1985/2000 - Loss: -7.610
Iter 1986/2000 - Loss: -7.610
Iter 1987/2000 - Loss: -7.610
Iter 1988/2000 - Loss: -7.610
Iter 1989/2000 - Loss: -7.610
Iter 1990/2000 - Loss: -7.610
Iter 1991/2000 - Loss: -7.611
Iter 1992/2000 - Loss: -7.611
Iter 1993/2000 - Loss: -7.611
Iter 1994/2000 - Loss: -7.611
Iter 1995/2000 - Loss: -7.611
Iter 1996/2000 - Loss: -7.611
Iter 1997/2000 - Loss: -7.611
Iter 1998/2000 - Loss: -7.611
Iter 1999/2000 - Loss: -7.611
Iter 2000/2000 - Loss: -7.611
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.4670,  4.7808, 33.2397,  9.9498, 17.5652, 43.6602]],

        [[16.6272, 30.4063,  9.2341,  1.3638,  4.4921, 22.1278]],

        [[15.7402, 36.7217,  9.2211,  1.2915,  1.3463, 24.0961]],

        [[16.3685, 34.6835, 24.2464,  2.9768,  1.8601, 42.0097]]])
Signal Variance: tensor([ 0.1124,  1.6360, 20.0312,  0.8491])
Estimated target variance: tensor([0.0229, 0.2568, 3.0548, 0.0607])
N: 140
Signal to noise ratio: tensor([ 18.2194,  70.2959, 104.2120,  55.3184])
Bound on condition number: tensor([  46473.6746,  691812.4510, 1520421.4293,  428418.2097])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.09760024229575108, policy loss: 3.287340963182321
Experience 14, Iter 1, disc loss: 0.09830121415276191, policy loss: 3.820432609212885
Experience 14, Iter 2, disc loss: 0.05426064834484186, policy loss: 4.489015237845833
Experience 14, Iter 3, disc loss: 0.03314901930163425, policy loss: 5.6110467179622
Experience 14, Iter 4, disc loss: 0.047085746160885886, policy loss: 5.442884166469026
Experience 14, Iter 5, disc loss: 0.06304383917797915, policy loss: 6.31650979559671
Experience 14, Iter 6, disc loss: 0.08153426256596122, policy loss: 6.162311897565381
Experience 14, Iter 7, disc loss: 0.08169269219535816, policy loss: 6.20283014236266
Experience 14, Iter 8, disc loss: 0.06742373385724103, policy loss: 5.204225151802243
Experience 14, Iter 9, disc loss: 0.0522490653240942, policy loss: 4.993058766644674
Experience 14, Iter 10, disc loss: 0.04229401285571488, policy loss: 4.837020457235645
Experience 14, Iter 11, disc loss: 0.04175898912331503, policy loss: 3.931075034064023
Experience 14, Iter 12, disc loss: 0.043812661665844674, policy loss: 4.007436746497475
Experience 14, Iter 13, disc loss: 0.050338205625153405, policy loss: 3.7090997859644492
Experience 14, Iter 14, disc loss: 0.05187981041924891, policy loss: 3.922902512938216
Experience 14, Iter 15, disc loss: 0.04308617416300886, policy loss: 4.188301129011664
Experience 14, Iter 16, disc loss: 0.03573007142505338, policy loss: 4.824696644837484
Experience 14, Iter 17, disc loss: 0.03887053848579974, policy loss: 5.18811197730718
Experience 14, Iter 18, disc loss: 0.031739198251418535, policy loss: 5.468162152913761
Experience 14, Iter 19, disc loss: 0.0274898827613589, policy loss: 5.376296179437765
Experience 14, Iter 20, disc loss: 0.029368264650590614, policy loss: 5.3459613443413225
Experience 14, Iter 21, disc loss: 0.02837335709431575, policy loss: 5.807830646405645
Experience 14, Iter 22, disc loss: 0.02889976531113076, policy loss: 5.904515284228667
Experience 14, Iter 23, disc loss: 0.028478833785085284, policy loss: 6.0700925492305755
Experience 14, Iter 24, disc loss: 0.027695361159910118, policy loss: 5.712244313094576
Experience 14, Iter 25, disc loss: 0.025794831598479746, policy loss: 5.613593887306294
Experience 14, Iter 26, disc loss: 0.024196042151680475, policy loss: 5.434438238531177
Experience 14, Iter 27, disc loss: 0.023165071626246658, policy loss: 5.41909251734724
Experience 14, Iter 28, disc loss: 0.021049865440712225, policy loss: 5.467359150064033
Experience 14, Iter 29, disc loss: 0.0221459337875153, policy loss: 5.1622318339197255
Experience 14, Iter 30, disc loss: 0.01920006883825787, policy loss: 5.488947685023822
Experience 14, Iter 31, disc loss: 0.01813519291195284, policy loss: 5.528239870441527
Experience 14, Iter 32, disc loss: 0.018316132065157007, policy loss: 5.0537558777088964
Experience 14, Iter 33, disc loss: 0.01890526465234686, policy loss: 5.018780210224416
Experience 14, Iter 34, disc loss: 0.018492600296047823, policy loss: 4.952649011383487
Experience 14, Iter 35, disc loss: 0.017644439409435927, policy loss: 5.156041871274955
Experience 14, Iter 36, disc loss: 0.016615489530777055, policy loss: 5.133712157258292
Experience 14, Iter 37, disc loss: 0.01620691897980517, policy loss: 5.897488514639068
Experience 14, Iter 38, disc loss: 0.015080616901457813, policy loss: 5.850629519239478
Experience 14, Iter 39, disc loss: 0.015742490680321596, policy loss: 5.697274511039727
Experience 14, Iter 40, disc loss: 0.017224817818241908, policy loss: 5.651409000629428
Experience 14, Iter 41, disc loss: 0.015098432390394559, policy loss: 5.783843129802671
Experience 14, Iter 42, disc loss: 0.013862488234592246, policy loss: 5.884972548546147
Experience 14, Iter 43, disc loss: 0.014401064646323049, policy loss: 5.832177211948824
Experience 14, Iter 44, disc loss: 0.013440194182097909, policy loss: 5.8541973517546495
Experience 14, Iter 45, disc loss: 0.013888114813635694, policy loss: 6.006097345942102
Experience 14, Iter 46, disc loss: 0.012730498158585405, policy loss: 5.981280176936261
Experience 14, Iter 47, disc loss: 0.012674272736469702, policy loss: 6.578582787661021
Experience 14, Iter 48, disc loss: 0.013420056668590252, policy loss: 5.703679341749398
Experience 14, Iter 49, disc loss: 0.012614610767913616, policy loss: 6.046099676877825
Experience 14, Iter 50, disc loss: 0.012366521039036559, policy loss: 6.250472750256223
Experience 14, Iter 51, disc loss: 0.012312058910466253, policy loss: 6.271312269586883
Experience 14, Iter 52, disc loss: 0.012499543767609974, policy loss: 5.712627008125185
Experience 14, Iter 53, disc loss: 0.012296127801602802, policy loss: 6.465119094629742
Experience 14, Iter 54, disc loss: 0.012571518804591005, policy loss: 5.459153246580927
Experience 14, Iter 55, disc loss: 0.012998671542034091, policy loss: 5.573636683087267
Experience 14, Iter 56, disc loss: 0.012412526625735706, policy loss: 5.575436026123025
Experience 14, Iter 57, disc loss: 0.011969731812973245, policy loss: 5.760316402767555
Experience 14, Iter 58, disc loss: 0.011894776103002989, policy loss: 6.17861503578616
Experience 14, Iter 59, disc loss: 0.011468136834620852, policy loss: 6.125507288129578
Experience 14, Iter 60, disc loss: 0.011074008781407872, policy loss: 6.298630414881854
Experience 14, Iter 61, disc loss: 0.01161189300721215, policy loss: 5.590810538925635
Experience 14, Iter 62, disc loss: 0.01103228004632557, policy loss: 5.746593166600693
Experience 14, Iter 63, disc loss: 0.011644582882442685, policy loss: 5.730617376279451
Experience 14, Iter 64, disc loss: 0.011133633861525528, policy loss: 6.268289647953746
Experience 14, Iter 65, disc loss: 0.011193457636180732, policy loss: 6.051451482893535
Experience 14, Iter 66, disc loss: 0.01309486132346478, policy loss: 5.905977972637363
Experience 14, Iter 67, disc loss: 0.012314858616125995, policy loss: 5.69077827779199
Experience 14, Iter 68, disc loss: 0.011429301144554076, policy loss: 6.298119955854261
Experience 14, Iter 69, disc loss: 0.01221314337005073, policy loss: 5.741742388551816
Experience 14, Iter 70, disc loss: 0.011687130163444175, policy loss: 5.970635159074487
Experience 14, Iter 71, disc loss: 0.011784540662602299, policy loss: 5.881007044727033
Experience 14, Iter 72, disc loss: 0.011180994399024279, policy loss: 5.809330395624987
Experience 14, Iter 73, disc loss: 0.012077468143829092, policy loss: 5.760145066467887
Experience 14, Iter 74, disc loss: 0.011250347668106033, policy loss: 5.905938807761817
Experience 14, Iter 75, disc loss: 0.013741392028400058, policy loss: 5.920977523073383
Experience 14, Iter 76, disc loss: 0.012456919067493744, policy loss: 5.663408390388614
Experience 14, Iter 77, disc loss: 0.01394198815755521, policy loss: 5.4495828679769245
Experience 14, Iter 78, disc loss: 0.012429914658404758, policy loss: 5.849059105902947
Experience 14, Iter 79, disc loss: 0.01172468547575211, policy loss: 5.794658062738492
Experience 14, Iter 80, disc loss: 0.012054092670195284, policy loss: 5.984365938180964
Experience 14, Iter 81, disc loss: 0.01121941898482618, policy loss: 6.726318405110646
Experience 14, Iter 82, disc loss: 0.012130307943064224, policy loss: 5.705508887195771
Experience 14, Iter 83, disc loss: 0.011199743815941214, policy loss: 5.856284633939401
Experience 14, Iter 84, disc loss: 0.012519584070740827, policy loss: 5.613394518828754
Experience 14, Iter 85, disc loss: 0.012047335758157439, policy loss: 6.188238149103507
Experience 14, Iter 86, disc loss: 0.011812159271660103, policy loss: 5.69423271504068
Experience 14, Iter 87, disc loss: 0.011862592560531413, policy loss: 5.911110568661449
Experience 14, Iter 88, disc loss: 0.012463649741503505, policy loss: 5.884627166930608
Experience 14, Iter 89, disc loss: 0.01185617645239381, policy loss: 5.970813838485334
Experience 14, Iter 90, disc loss: 0.009442333961755166, policy loss: 6.878627619795236
Experience 14, Iter 91, disc loss: 0.009840015628667837, policy loss: 6.139151754848012
Experience 14, Iter 92, disc loss: 0.01127442157822681, policy loss: 6.220400525313171
Experience 14, Iter 93, disc loss: 0.0061990736106247515, policy loss: 8.320007461044902
Experience 14, Iter 94, disc loss: 0.006035656897521737, policy loss: 7.33186138386575
Experience 14, Iter 95, disc loss: 0.004854431348541592, policy loss: 10.0311398058683
Experience 14, Iter 96, disc loss: 0.004414759166683869, policy loss: 10.734677661551867
Experience 14, Iter 97, disc loss: 0.0041020834829587575, policy loss: 10.840540675091248
Experience 14, Iter 98, disc loss: 0.0037942930345193845, policy loss: 10.561679751471189
Experience 14, Iter 99, disc loss: 0.003867550707717004, policy loss: 8.406560178567705
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.0802],
        [0.8876],
        [0.0196]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0246, 0.2135, 0.8982, 0.0155, 0.0143, 2.1474]],

        [[0.0246, 0.2135, 0.8982, 0.0155, 0.0143, 2.1474]],

        [[0.0246, 0.2135, 0.8982, 0.0155, 0.0143, 2.1474]],

        [[0.0246, 0.2135, 0.8982, 0.0155, 0.0143, 2.1474]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0227, 0.3209, 3.5503, 0.0782], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0227, 0.3209, 3.5503, 0.0782])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.597
Iter 2/2000 - Loss: 2.561
Iter 3/2000 - Loss: 2.486
Iter 4/2000 - Loss: 2.432
Iter 5/2000 - Loss: 2.403
Iter 6/2000 - Loss: 2.339
Iter 7/2000 - Loss: 2.256
Iter 8/2000 - Loss: 2.166
Iter 9/2000 - Loss: 2.063
Iter 10/2000 - Loss: 1.935
Iter 11/2000 - Loss: 1.786
Iter 12/2000 - Loss: 1.620
Iter 13/2000 - Loss: 1.437
Iter 14/2000 - Loss: 1.235
Iter 15/2000 - Loss: 1.011
Iter 16/2000 - Loss: 0.766
Iter 17/2000 - Loss: 0.505
Iter 18/2000 - Loss: 0.231
Iter 19/2000 - Loss: -0.050
Iter 20/2000 - Loss: -0.337
Iter 1981/2000 - Loss: -7.536
Iter 1982/2000 - Loss: -7.536
Iter 1983/2000 - Loss: -7.536
Iter 1984/2000 - Loss: -7.536
Iter 1985/2000 - Loss: -7.536
Iter 1986/2000 - Loss: -7.536
Iter 1987/2000 - Loss: -7.536
Iter 1988/2000 - Loss: -7.536
Iter 1989/2000 - Loss: -7.536
Iter 1990/2000 - Loss: -7.537
Iter 1991/2000 - Loss: -7.537
Iter 1992/2000 - Loss: -7.537
Iter 1993/2000 - Loss: -7.537
Iter 1994/2000 - Loss: -7.537
Iter 1995/2000 - Loss: -7.537
Iter 1996/2000 - Loss: -7.537
Iter 1997/2000 - Loss: -7.537
Iter 1998/2000 - Loss: -7.537
Iter 1999/2000 - Loss: -7.537
Iter 2000/2000 - Loss: -7.537
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.6834, 10.3040, 38.2865,  7.1619, 16.9166, 39.6270]],

        [[15.5691, 32.1431,  8.2968,  1.4499,  3.8704, 23.2700]],

        [[17.3231, 36.3330,  7.9693,  1.4702,  1.2302, 25.9664]],

        [[11.7628, 33.8060, 26.2344,  3.5329,  1.8048, 46.3668]]])
Signal Variance: tensor([ 0.1932,  1.8179, 21.3049,  1.0846])
Estimated target variance: tensor([0.0227, 0.3209, 3.5503, 0.0782])
N: 150
Signal to noise ratio: tensor([ 23.5916,  73.0471, 107.7530,  64.0719])
Bound on condition number: tensor([  83485.6291,  800382.0881, 1741606.6101,  615782.6127])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0053265877403789645, policy loss: 6.979695702490815
Experience 15, Iter 1, disc loss: 0.004093952494905369, policy loss: 7.198193203578888
Experience 15, Iter 2, disc loss: 0.00799535355228538, policy loss: 5.518872592010258
Experience 15, Iter 3, disc loss: 0.010826811279852341, policy loss: 5.881199760163606
Experience 15, Iter 4, disc loss: 0.007787699088056601, policy loss: 6.225386269940014
Experience 15, Iter 5, disc loss: 0.006518988070434814, policy loss: 6.689627052888009
Experience 15, Iter 6, disc loss: 0.007779097181516367, policy loss: 6.19065035917187
Experience 15, Iter 7, disc loss: 0.009118753523028014, policy loss: 6.479154732833471
Experience 15, Iter 8, disc loss: 0.014942715602959374, policy loss: 5.081299593972119
Experience 15, Iter 9, disc loss: 0.014164155078213935, policy loss: 5.2962516468235705
Experience 15, Iter 10, disc loss: 0.0065318060629933625, policy loss: 6.677151483725142
Experience 15, Iter 11, disc loss: 0.004328015120792691, policy loss: 7.66285630292262
Experience 15, Iter 12, disc loss: 0.0039451351912164, policy loss: 8.046415834165703
Experience 15, Iter 13, disc loss: 0.005216098938512078, policy loss: 6.953366407368637
Experience 15, Iter 14, disc loss: 0.006146279578969277, policy loss: 6.8148875854972975
Experience 15, Iter 15, disc loss: 0.0058230797871144935, policy loss: 7.200708882997964
Experience 15, Iter 16, disc loss: 0.0070277465517269225, policy loss: 7.175567667546281
Experience 15, Iter 17, disc loss: 0.005477277931665017, policy loss: 7.407466890662731
Experience 15, Iter 18, disc loss: 0.005024964342700621, policy loss: 7.586348566319607
Experience 15, Iter 19, disc loss: 0.0054390158514400705, policy loss: 6.966690146487598
Experience 15, Iter 20, disc loss: 0.006510361847594622, policy loss: 6.720467314185163
Experience 15, Iter 21, disc loss: 0.01245719843249017, policy loss: 6.358999723634273
Experience 15, Iter 22, disc loss: 0.012936035579728519, policy loss: 5.727240496793329
Experience 15, Iter 23, disc loss: 0.010986127047775393, policy loss: 6.1113880583307765
Experience 15, Iter 24, disc loss: 0.010426815529490995, policy loss: 6.0250786978423125
Experience 15, Iter 25, disc loss: 0.013847114802487761, policy loss: 6.130743808145546
Experience 15, Iter 26, disc loss: 0.00897045504654654, policy loss: 6.624686562796791
Experience 15, Iter 27, disc loss: 0.009686212569601714, policy loss: 6.454856841187109
Experience 15, Iter 28, disc loss: 0.012671039999477475, policy loss: 6.770054898716012
Experience 15, Iter 29, disc loss: 0.009173250507947144, policy loss: 6.785033794257408
Experience 15, Iter 30, disc loss: 0.007959519044342089, policy loss: 7.271304565052751
Experience 15, Iter 31, disc loss: 0.006948939192304014, policy loss: 7.655748056567609
Experience 15, Iter 32, disc loss: 0.006986396792597773, policy loss: 7.962866500415828
Experience 15, Iter 33, disc loss: 0.00756217143974945, policy loss: 7.345024464494939
Experience 15, Iter 34, disc loss: 0.00873547689604964, policy loss: 6.419162049908492
Experience 15, Iter 35, disc loss: 0.005228624254482018, policy loss: 10.64949283906492
Experience 15, Iter 36, disc loss: 0.006953100437946795, policy loss: 7.58410356943182
Experience 15, Iter 37, disc loss: 0.0047706254860888355, policy loss: 9.101877415791275
Experience 15, Iter 38, disc loss: 0.004235160197259899, policy loss: 10.500886121614862
Experience 15, Iter 39, disc loss: 0.0038134633643538466, policy loss: 11.413560992449366
Experience 15, Iter 40, disc loss: 0.003519112544772133, policy loss: 10.873099347960164
Experience 15, Iter 41, disc loss: 0.0032818962987762905, policy loss: 10.329784871058308
Experience 15, Iter 42, disc loss: 0.0032158414521658786, policy loss: 9.69384393282412
Experience 15, Iter 43, disc loss: 0.00465254918255552, policy loss: 7.6063595107850235
Experience 15, Iter 44, disc loss: 0.008912613223777276, policy loss: 5.786841633747235
Experience 15, Iter 45, disc loss: 0.0021555837991756943, policy loss: 11.732714983970272
Experience 15, Iter 46, disc loss: 0.008194849812554987, policy loss: 5.825629730024614
Experience 15, Iter 47, disc loss: 0.005071423586989179, policy loss: 6.995732971105202
Experience 15, Iter 48, disc loss: 0.0031722901600006476, policy loss: 7.655617740650738
Experience 15, Iter 49, disc loss: 0.0026785071003895093, policy loss: 8.61237495259868
Experience 15, Iter 50, disc loss: 0.003078982657685732, policy loss: 8.085579795051839
Experience 15, Iter 51, disc loss: 0.003231161468678364, policy loss: 7.359074616967606
Experience 15, Iter 52, disc loss: 0.003689422765846234, policy loss: 8.175962400034827
Experience 15, Iter 53, disc loss: 0.006234605814085009, policy loss: 6.256956466714913
Experience 15, Iter 54, disc loss: 0.006883983116507535, policy loss: 6.0774469626175645
Experience 15, Iter 55, disc loss: 0.0038919681207588886, policy loss: 7.021812307872237
Experience 15, Iter 56, disc loss: 0.009733055555183915, policy loss: 5.458894230801007
Experience 15, Iter 57, disc loss: 0.007219084595942093, policy loss: 6.019071580862709
Experience 15, Iter 58, disc loss: 0.005541292309601478, policy loss: 7.308884705827722
Experience 15, Iter 59, disc loss: 0.005717956411565422, policy loss: 6.9714274046990505
Experience 15, Iter 60, disc loss: 0.009008783082716569, policy loss: 5.800961867962416
Experience 15, Iter 61, disc loss: 0.010963621462660056, policy loss: 6.181731630060015
Experience 15, Iter 62, disc loss: 0.009301223437541915, policy loss: 6.208909264247146
Experience 15, Iter 63, disc loss: 0.007254897797228762, policy loss: 6.515699080328664
Experience 15, Iter 64, disc loss: 0.0059301793445020404, policy loss: 7.0275798027888055
Experience 15, Iter 65, disc loss: 0.006147740925503133, policy loss: 6.690552481058998
Experience 15, Iter 66, disc loss: 0.007477477859612468, policy loss: 6.638532575186431
Experience 15, Iter 67, disc loss: 0.008367793878818773, policy loss: 6.871398571810163
Experience 15, Iter 68, disc loss: 0.007385549251895944, policy loss: 6.766531953233651
Experience 15, Iter 69, disc loss: 0.006518892569240882, policy loss: 7.087738154129737
Experience 15, Iter 70, disc loss: 0.00711920068566596, policy loss: 7.035748289536457
Experience 15, Iter 71, disc loss: 0.009169208684486549, policy loss: 6.107129991734841
Experience 15, Iter 72, disc loss: 0.008343326392613459, policy loss: 7.17908269769952
Experience 15, Iter 73, disc loss: 0.00840530149656267, policy loss: 6.757364059048605
Experience 15, Iter 74, disc loss: 0.008235013511252451, policy loss: 7.022481093629737
Experience 15, Iter 75, disc loss: 0.007963341624108819, policy loss: 7.208408566338576
Experience 15, Iter 76, disc loss: 0.009007922724927156, policy loss: 6.4205519174856285
Experience 15, Iter 77, disc loss: 0.008219711900610892, policy loss: 6.830772883348059
Experience 15, Iter 78, disc loss: 0.007437996420611805, policy loss: 7.655319731432689
Experience 15, Iter 79, disc loss: 0.006152455067820526, policy loss: 7.204543269854445
Experience 15, Iter 80, disc loss: 0.006323671758436874, policy loss: 8.036942587582008
Experience 15, Iter 81, disc loss: 0.009008829461521428, policy loss: 6.538624995389187
Experience 15, Iter 82, disc loss: 0.007590447410279899, policy loss: 7.02743599967471
Experience 15, Iter 83, disc loss: 0.0076194156016200325, policy loss: 6.423783931940011
Experience 15, Iter 84, disc loss: 0.006606467029951388, policy loss: 6.534568075163975
Experience 15, Iter 85, disc loss: 0.006982594515044254, policy loss: 6.450645701119025
Experience 15, Iter 86, disc loss: 0.008965220511677776, policy loss: 6.186404467948291
Experience 15, Iter 87, disc loss: 0.008126196054214777, policy loss: 7.237001936251073
Experience 15, Iter 88, disc loss: 0.007290666076566337, policy loss: 6.560029713815679
Experience 15, Iter 89, disc loss: 0.006218439784795118, policy loss: 6.961230101470353
Experience 15, Iter 90, disc loss: 0.008237859284107245, policy loss: 6.629083570977586
Experience 15, Iter 91, disc loss: 0.00769551682959151, policy loss: 6.381077540158271
Experience 15, Iter 92, disc loss: 0.007086300178730666, policy loss: 6.449153515077932
Experience 15, Iter 93, disc loss: 0.005455429569074985, policy loss: 7.19992344451426
Experience 15, Iter 94, disc loss: 0.00710412116676511, policy loss: 7.222844515702764
Experience 15, Iter 95, disc loss: 0.006083090765867373, policy loss: 7.1254807052955424
Experience 15, Iter 96, disc loss: 0.006451408294177029, policy loss: 6.787294556724236
Experience 15, Iter 97, disc loss: 0.006993832215857768, policy loss: 6.5264766559738305
Experience 15, Iter 98, disc loss: 0.007403209260919495, policy loss: 6.415815091665023
Experience 15, Iter 99, disc loss: 0.005922298158799704, policy loss: 6.6710435265947154
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.0985],
        [1.0099],
        [0.0235]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0234, 0.2147, 1.0792, 0.0169, 0.0171, 2.5049]],

        [[0.0234, 0.2147, 1.0792, 0.0169, 0.0171, 2.5049]],

        [[0.0234, 0.2147, 1.0792, 0.0169, 0.0171, 2.5049]],

        [[0.0234, 0.2147, 1.0792, 0.0169, 0.0171, 2.5049]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0224, 0.3939, 4.0396, 0.0938], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0224, 0.3939, 4.0396, 0.0938])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.845
Iter 2/2000 - Loss: 2.793
Iter 3/2000 - Loss: 2.709
Iter 4/2000 - Loss: 2.654
Iter 5/2000 - Loss: 2.607
Iter 6/2000 - Loss: 2.518
Iter 7/2000 - Loss: 2.420
Iter 8/2000 - Loss: 2.319
Iter 9/2000 - Loss: 2.198
Iter 10/2000 - Loss: 2.046
Iter 11/2000 - Loss: 1.870
Iter 12/2000 - Loss: 1.679
Iter 13/2000 - Loss: 1.475
Iter 14/2000 - Loss: 1.253
Iter 15/2000 - Loss: 1.011
Iter 16/2000 - Loss: 0.748
Iter 17/2000 - Loss: 0.470
Iter 18/2000 - Loss: 0.182
Iter 19/2000 - Loss: -0.109
Iter 20/2000 - Loss: -0.402
Iter 1981/2000 - Loss: -7.538
Iter 1982/2000 - Loss: -7.538
Iter 1983/2000 - Loss: -7.538
Iter 1984/2000 - Loss: -7.538
Iter 1985/2000 - Loss: -7.539
Iter 1986/2000 - Loss: -7.539
Iter 1987/2000 - Loss: -7.539
Iter 1988/2000 - Loss: -7.539
Iter 1989/2000 - Loss: -7.539
Iter 1990/2000 - Loss: -7.539
Iter 1991/2000 - Loss: -7.539
Iter 1992/2000 - Loss: -7.539
Iter 1993/2000 - Loss: -7.539
Iter 1994/2000 - Loss: -7.539
Iter 1995/2000 - Loss: -7.539
Iter 1996/2000 - Loss: -7.539
Iter 1997/2000 - Loss: -7.539
Iter 1998/2000 - Loss: -7.539
Iter 1999/2000 - Loss: -7.539
Iter 2000/2000 - Loss: -7.539
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.7391,  4.9073, 37.9524, 10.2396, 16.3828, 43.5799]],

        [[14.6832, 32.8806,  8.3544,  1.4645,  2.4259, 25.7580]],

        [[16.9157, 31.3922,  8.0838,  1.4633,  1.1869, 26.9485]],

        [[10.7511, 27.9212, 27.7363,  3.7676,  1.7563, 47.9733]]])
Signal Variance: tensor([ 0.1112,  2.0570, 23.2149,  1.1849])
Estimated target variance: tensor([0.0224, 0.3939, 4.0396, 0.0938])
N: 160
Signal to noise ratio: tensor([ 18.1832,  79.5310, 112.0012,  66.5907])
Bound on condition number: tensor([  52901.4354, 1012030.0625, 2007084.9597,  709492.6570])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0046543682302189774, policy loss: 8.05672132948487
Experience 16, Iter 1, disc loss: 0.005128689592627322, policy loss: 7.363210505698317
Experience 16, Iter 2, disc loss: 0.007057790513108024, policy loss: 7.281410236261385
Experience 16, Iter 3, disc loss: 0.00489576643950487, policy loss: 8.756202317772157
Experience 16, Iter 4, disc loss: 0.006226745554684147, policy loss: 6.5740996506463985
Experience 16, Iter 5, disc loss: 0.004688002574920638, policy loss: 7.512616160761146
Experience 16, Iter 6, disc loss: 0.004962892189631012, policy loss: 7.323841111370076
Experience 16, Iter 7, disc loss: 0.004809598221318951, policy loss: 7.005316593291697
Experience 16, Iter 8, disc loss: 0.004266591280259227, policy loss: 7.365186812791304
Experience 16, Iter 9, disc loss: 0.005054057828021366, policy loss: 7.193063522241912
Experience 16, Iter 10, disc loss: 0.004916434149827403, policy loss: 6.977073336324006
Experience 16, Iter 11, disc loss: 0.005624712252411349, policy loss: 6.685909746494261
Experience 16, Iter 12, disc loss: 0.005366497711735713, policy loss: 7.161851839964282
Experience 16, Iter 13, disc loss: 0.005663323867192536, policy loss: 6.9392315255294585
Experience 16, Iter 14, disc loss: 0.004953334266528191, policy loss: 7.279409044946529
Experience 16, Iter 15, disc loss: 0.005907550185932841, policy loss: 6.254860012802547
Experience 16, Iter 16, disc loss: 0.007391036547790922, policy loss: 7.134113021400941
Experience 16, Iter 17, disc loss: 0.005867693083588194, policy loss: 6.427195747894286
Experience 16, Iter 18, disc loss: 0.004982946389062062, policy loss: 6.863547572611088
Experience 16, Iter 19, disc loss: 0.004039872340084608, policy loss: 8.315929578882503
Experience 16, Iter 20, disc loss: 0.005107176292652681, policy loss: 6.464601417929148
Experience 16, Iter 21, disc loss: 0.0033325800469795816, policy loss: 8.962341253407349
Experience 16, Iter 22, disc loss: 0.005107337581340202, policy loss: 6.740778738631091
Experience 16, Iter 23, disc loss: 0.0034719621455264883, policy loss: 7.871881991700529
Experience 16, Iter 24, disc loss: 0.003604597562022645, policy loss: 7.806095832760056
Experience 16, Iter 25, disc loss: 0.004438752998644217, policy loss: 7.260386776411272
Experience 16, Iter 26, disc loss: 0.005144605805514239, policy loss: 6.70014724025418
Experience 16, Iter 27, disc loss: 0.005163392897363604, policy loss: 7.010405150414279
Experience 16, Iter 28, disc loss: 0.004955342285344741, policy loss: 7.13261769686331
Experience 16, Iter 29, disc loss: 0.004704588570718572, policy loss: 7.08375383532664
Experience 16, Iter 30, disc loss: 0.005708174292466521, policy loss: 6.764465618174672
Experience 16, Iter 31, disc loss: 0.005367335851415541, policy loss: 8.076231743672775
Experience 16, Iter 32, disc loss: 0.005034055582518721, policy loss: 6.900617518935508
Experience 16, Iter 33, disc loss: 0.004726498853394694, policy loss: 6.826321822708258
Experience 16, Iter 34, disc loss: 0.005458218816807551, policy loss: 6.873941853892141
Experience 16, Iter 35, disc loss: 0.005590232123241961, policy loss: 6.656501780426285
Experience 16, Iter 36, disc loss: 0.004763847872123602, policy loss: 7.195215882396509
Experience 16, Iter 37, disc loss: 0.004821385825085617, policy loss: 6.848452282368313
Experience 16, Iter 38, disc loss: 0.0052116587582635345, policy loss: 6.627058477569243
Experience 16, Iter 39, disc loss: 0.0045734592792406985, policy loss: 6.89412644989013
Experience 16, Iter 40, disc loss: 0.0048416791943871685, policy loss: 7.048354086094154
Experience 16, Iter 41, disc loss: 0.005034453364977289, policy loss: 7.00385756235055
Experience 16, Iter 42, disc loss: 0.005272096605779949, policy loss: 7.927410686388015
Experience 16, Iter 43, disc loss: 0.005102484906914985, policy loss: 7.183683005564046
Experience 16, Iter 44, disc loss: 0.005370503198262116, policy loss: 7.003998538700781
Experience 16, Iter 45, disc loss: 0.00521256098792452, policy loss: 6.915950805033132
Experience 16, Iter 46, disc loss: 0.004987565788585205, policy loss: 7.106823744504813
Experience 16, Iter 47, disc loss: 0.005281508260737083, policy loss: 6.6496414254341545
Experience 16, Iter 48, disc loss: 0.005073959480749213, policy loss: 6.648505634207321
Experience 16, Iter 49, disc loss: 0.005064426431362046, policy loss: 7.418284447943407
Experience 16, Iter 50, disc loss: 0.005188500797533888, policy loss: 7.393148119874411
Experience 16, Iter 51, disc loss: 0.003977294997802747, policy loss: 7.205015657210055
Experience 16, Iter 52, disc loss: 0.004723405373887007, policy loss: 6.964493571143793
Experience 16, Iter 53, disc loss: 0.004822646877681982, policy loss: 6.937055381644722
Experience 16, Iter 54, disc loss: 0.004226237396022689, policy loss: 7.901413272198596
Experience 16, Iter 55, disc loss: 0.0036922148540458823, policy loss: 7.688553492334019
Experience 16, Iter 56, disc loss: 0.004235203556340833, policy loss: 7.355510757943035
Experience 16, Iter 57, disc loss: 0.004709149176000358, policy loss: 7.209367283499492
Experience 16, Iter 58, disc loss: 0.004254026794117997, policy loss: 7.45208201911891
Experience 16, Iter 59, disc loss: 0.003581135273573756, policy loss: 7.949902433610418
Experience 16, Iter 60, disc loss: 0.004481072249549901, policy loss: 6.9497296286401316
Experience 16, Iter 61, disc loss: 0.0043953727616921155, policy loss: 7.721685791586256
Experience 16, Iter 62, disc loss: 0.004773878633781991, policy loss: 6.642318202497035
Experience 16, Iter 63, disc loss: 0.003567637061967289, policy loss: 7.545150589824433
Experience 16, Iter 64, disc loss: 0.0041064401441271575, policy loss: 7.2378458751195955
Experience 16, Iter 65, disc loss: 0.004302414696461613, policy loss: 7.040784061288191
Experience 16, Iter 66, disc loss: 0.004449041791878636, policy loss: 7.3995191502685245
Experience 16, Iter 67, disc loss: 0.004213808008841123, policy loss: 7.21797275321078
Experience 16, Iter 68, disc loss: 0.004157622365863209, policy loss: 7.291698434035542
Experience 16, Iter 69, disc loss: 0.00509269022459131, policy loss: 6.5120262848523875
Experience 16, Iter 70, disc loss: 0.004473450743465399, policy loss: 7.030661413172832
Experience 16, Iter 71, disc loss: 0.004053237451192722, policy loss: 7.578338596322862
Experience 16, Iter 72, disc loss: 0.0041699828913815615, policy loss: 6.944199467945586
Experience 16, Iter 73, disc loss: 0.005027997103313224, policy loss: 6.947098990985566
Experience 16, Iter 74, disc loss: 0.004653836555037653, policy loss: 7.9794003310690105
Experience 16, Iter 75, disc loss: 0.003547487273278224, policy loss: 7.729206555715869
Experience 16, Iter 76, disc loss: 0.0036101669959772553, policy loss: 7.917273913287953
Experience 16, Iter 77, disc loss: 0.0037987560805994905, policy loss: 7.082982432775395
Experience 16, Iter 78, disc loss: 0.0031397866994165966, policy loss: 8.219599302039118
Experience 16, Iter 79, disc loss: 0.003357734064088863, policy loss: 8.112767464470647
Experience 16, Iter 80, disc loss: 0.0034049896414621297, policy loss: 8.179896300485694
Experience 16, Iter 81, disc loss: 0.00322960338001978, policy loss: 8.392988968012723
Experience 16, Iter 82, disc loss: 0.0032368784046689913, policy loss: 7.851099894044452
Experience 16, Iter 83, disc loss: 0.0022218211949102265, policy loss: 9.336596880594422
Experience 16, Iter 84, disc loss: 0.0034810729951000683, policy loss: 8.88752799545701
Experience 16, Iter 85, disc loss: 0.002495080764186889, policy loss: 8.620700518504846
Experience 16, Iter 86, disc loss: 0.0024792443363868114, policy loss: 8.551717970795039
Experience 16, Iter 87, disc loss: 0.0026336932054343456, policy loss: 8.239702341456118
Experience 16, Iter 88, disc loss: 0.003240752524243804, policy loss: 7.288581422437755
Experience 16, Iter 89, disc loss: 0.0024517502971621386, policy loss: 8.516800758426953
Experience 16, Iter 90, disc loss: 0.003134885997117264, policy loss: 7.310664827735221
Experience 16, Iter 91, disc loss: 0.003054055380746306, policy loss: 8.11020158590799
Experience 16, Iter 92, disc loss: 0.002752874647170273, policy loss: 7.888361832004923
Experience 16, Iter 93, disc loss: 0.002660841597638814, policy loss: 7.692931916082906
Experience 16, Iter 94, disc loss: 0.003616737713558654, policy loss: 7.056543003897636
Experience 16, Iter 95, disc loss: 0.0027661212289047717, policy loss: 7.653860173821306
Experience 16, Iter 96, disc loss: 0.0038317303623339903, policy loss: 6.9297451913114685
Experience 16, Iter 97, disc loss: 0.0033868751900111155, policy loss: 6.748710024636283
Experience 16, Iter 98, disc loss: 0.003692827079602552, policy loss: 7.362756937789166
Experience 16, Iter 99, disc loss: 0.004045902279688702, policy loss: 7.030681786523898
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.1011],
        [1.0320],
        [0.0233]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0226, 0.2081, 1.0703, 0.0179, 0.0173, 2.6464]],

        [[0.0226, 0.2081, 1.0703, 0.0179, 0.0173, 2.6464]],

        [[0.0226, 0.2081, 1.0703, 0.0179, 0.0173, 2.6464]],

        [[0.0226, 0.2081, 1.0703, 0.0179, 0.0173, 2.6464]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0216, 0.4046, 4.1280, 0.0931], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0216, 0.4046, 4.1280, 0.0931])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.853
Iter 2/2000 - Loss: 2.807
Iter 3/2000 - Loss: 2.716
Iter 4/2000 - Loss: 2.667
Iter 5/2000 - Loss: 2.623
Iter 6/2000 - Loss: 2.531
Iter 7/2000 - Loss: 2.431
Iter 8/2000 - Loss: 2.334
Iter 9/2000 - Loss: 2.217
Iter 10/2000 - Loss: 2.067
Iter 11/2000 - Loss: 1.892
Iter 12/2000 - Loss: 1.703
Iter 13/2000 - Loss: 1.501
Iter 14/2000 - Loss: 1.284
Iter 15/2000 - Loss: 1.047
Iter 16/2000 - Loss: 0.789
Iter 17/2000 - Loss: 0.515
Iter 18/2000 - Loss: 0.230
Iter 19/2000 - Loss: -0.058
Iter 20/2000 - Loss: -0.348
Iter 1981/2000 - Loss: -7.607
Iter 1982/2000 - Loss: -7.607
Iter 1983/2000 - Loss: -7.607
Iter 1984/2000 - Loss: -7.607
Iter 1985/2000 - Loss: -7.607
Iter 1986/2000 - Loss: -7.607
Iter 1987/2000 - Loss: -7.608
Iter 1988/2000 - Loss: -7.608
Iter 1989/2000 - Loss: -7.608
Iter 1990/2000 - Loss: -7.608
Iter 1991/2000 - Loss: -7.608
Iter 1992/2000 - Loss: -7.608
Iter 1993/2000 - Loss: -7.608
Iter 1994/2000 - Loss: -7.608
Iter 1995/2000 - Loss: -7.608
Iter 1996/2000 - Loss: -7.608
Iter 1997/2000 - Loss: -7.608
Iter 1998/2000 - Loss: -7.608
Iter 1999/2000 - Loss: -7.608
Iter 2000/2000 - Loss: -7.608
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.8758,  4.8227, 37.6513, 10.5459, 15.6222, 48.0612]],

        [[14.9175, 31.3522,  8.9107,  1.3680,  2.0518, 25.9171]],

        [[17.9849, 30.3244,  8.5311,  1.2976,  1.1142, 27.9783]],

        [[11.0470, 28.9952, 27.9919,  3.3855,  1.8612, 48.1490]]])
Signal Variance: tensor([ 0.1072,  2.1011, 21.8489,  1.1904])
Estimated target variance: tensor([0.0216, 0.4046, 4.1280, 0.0931])
N: 170
Signal to noise ratio: tensor([ 18.0791,  81.6228, 109.2724,  67.4036])
Bound on condition number: tensor([  55566.2650, 1132589.6451, 2029879.5018,  772351.5950])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.004080744613132148, policy loss: 7.310250967846029
Experience 17, Iter 1, disc loss: 0.00259277771818506, policy loss: 7.766283309386625
Experience 17, Iter 2, disc loss: 0.0024876800478149646, policy loss: 7.91470470727411
Experience 17, Iter 3, disc loss: 0.003145514938382488, policy loss: 7.045986061944381
Experience 17, Iter 4, disc loss: 0.00325854788557589, policy loss: 8.666799306018584
Experience 17, Iter 5, disc loss: 0.003376049867208702, policy loss: 7.0494113033652965
Experience 17, Iter 6, disc loss: 0.003121787095113554, policy loss: 7.766635892451321
Experience 17, Iter 7, disc loss: 0.0037951154542294188, policy loss: 7.373819526346017
Experience 17, Iter 8, disc loss: 0.002698751480276063, policy loss: 8.4103350456823
Experience 17, Iter 9, disc loss: 0.0033757524189613596, policy loss: 7.224967762798389
Experience 17, Iter 10, disc loss: 0.0027220082773320416, policy loss: 7.682429697350958
Experience 17, Iter 11, disc loss: 0.002675006470551939, policy loss: 8.127709670237337
Experience 17, Iter 12, disc loss: 0.003759064938670202, policy loss: 7.103178884451404
Experience 17, Iter 13, disc loss: 0.0020783598777765542, policy loss: 8.82163403509033
Experience 17, Iter 14, disc loss: 0.0026962180388104585, policy loss: 7.896321544916728
Experience 17, Iter 15, disc loss: 0.002086233845336395, policy loss: 8.641379838403008
Experience 17, Iter 16, disc loss: 0.0019043626425613962, policy loss: 9.815709175055341
Experience 17, Iter 17, disc loss: 0.00200698526739333, policy loss: 8.999052831991886
Experience 17, Iter 18, disc loss: 0.002121332960247315, policy loss: 8.424919654694438
Experience 17, Iter 19, disc loss: 0.0028142809464200094, policy loss: 8.06325715127032
Experience 17, Iter 20, disc loss: 0.0023235542173950926, policy loss: 7.620452083856767
Experience 17, Iter 21, disc loss: 0.0020390496740532653, policy loss: 7.926558712767935
Experience 17, Iter 22, disc loss: 0.0026372164174088587, policy loss: 8.120707621270734
Experience 17, Iter 23, disc loss: 0.0020205545104498997, policy loss: 9.084304503668914
Experience 17, Iter 24, disc loss: 0.0019446284070056678, policy loss: 8.481465124358248
Experience 17, Iter 25, disc loss: 0.0020930686816723115, policy loss: 8.784433185021681
Experience 17, Iter 26, disc loss: 0.002868120915980721, policy loss: 7.998885952263195
Experience 17, Iter 27, disc loss: 0.0023955420177565706, policy loss: 7.614562229285578
Experience 17, Iter 28, disc loss: 0.0020806393554277354, policy loss: 8.228028066471392
Experience 17, Iter 29, disc loss: 0.002698103387599249, policy loss: 8.292880670081688
Experience 17, Iter 30, disc loss: 0.0020800868472281575, policy loss: 8.513966601148748
Experience 17, Iter 31, disc loss: 0.0019987857774239554, policy loss: 8.856496017115639
Experience 17, Iter 32, disc loss: 0.0025963066617613493, policy loss: 8.168508035040107
Experience 17, Iter 33, disc loss: 0.0036414974044444355, policy loss: 7.851605325791193
Experience 17, Iter 34, disc loss: 0.0024525428789627314, policy loss: 8.418611645017242
Experience 17, Iter 35, disc loss: 0.0022224456770285356, policy loss: 8.1892519743336
Experience 17, Iter 36, disc loss: 0.0016786244217231014, policy loss: 8.25290029237529
Experience 17, Iter 37, disc loss: 0.0018367522386195088, policy loss: 8.3397864060356
Experience 17, Iter 38, disc loss: 0.0018674477555445921, policy loss: 8.592976080805261
Experience 17, Iter 39, disc loss: 0.0026170517548901777, policy loss: 8.504298191248028
Experience 17, Iter 40, disc loss: 0.003640931009214958, policy loss: 6.550208224492444
Experience 17, Iter 41, disc loss: 0.002105288155688256, policy loss: 9.334310168758076
Experience 17, Iter 42, disc loss: 0.002270805410966594, policy loss: 7.512741495216348
Experience 17, Iter 43, disc loss: 0.0016109360564001336, policy loss: 9.381680217688128
Experience 17, Iter 44, disc loss: 0.0015013656412795817, policy loss: 9.753136821064079
Experience 17, Iter 45, disc loss: 0.0013659717640636141, policy loss: 9.536372105563691
Experience 17, Iter 46, disc loss: 0.0015809441475157725, policy loss: 9.021800823302069
Experience 17, Iter 47, disc loss: 0.0016453660821519774, policy loss: 9.346627693584946
Experience 17, Iter 48, disc loss: 0.0027384813941399243, policy loss: 8.25056076388985
Experience 17, Iter 49, disc loss: 0.0017716842457589448, policy loss: 7.60093851297761
Experience 17, Iter 50, disc loss: 0.0020830712835079182, policy loss: 7.9329983520095695
Experience 17, Iter 51, disc loss: 0.0025544762231781487, policy loss: 7.380324823357332
Experience 17, Iter 52, disc loss: 0.002310079430098068, policy loss: 8.053890731252277
Experience 17, Iter 53, disc loss: 0.002769457694403063, policy loss: 7.141028170927946
Experience 17, Iter 54, disc loss: 0.0026836120047650652, policy loss: 7.94351916632049
Experience 17, Iter 55, disc loss: 0.002210488875501174, policy loss: 7.506750949046852
Experience 17, Iter 56, disc loss: 0.0029243736094386684, policy loss: 6.9692313465477564
Experience 17, Iter 57, disc loss: 0.002391333428205615, policy loss: 7.747793708786081
Experience 17, Iter 58, disc loss: 0.0026314470921794285, policy loss: 7.343211045773006
Experience 17, Iter 59, disc loss: 0.0034388437512669944, policy loss: 6.685253455625165
Experience 17, Iter 60, disc loss: 0.0030023523511861563, policy loss: 7.949363829014111
Experience 17, Iter 61, disc loss: 0.0033904557575300995, policy loss: 7.425473052896692
Experience 17, Iter 62, disc loss: 0.002390002452270925, policy loss: 7.9012928565105325
Experience 17, Iter 63, disc loss: 0.0019619090021711872, policy loss: 8.95207546553532
Experience 17, Iter 64, disc loss: 0.001936015460890656, policy loss: 9.475541733715747
Experience 17, Iter 65, disc loss: 0.002619134341330435, policy loss: 7.551072620607922
Experience 17, Iter 66, disc loss: 0.0023479657041734488, policy loss: 9.947141779341774
Experience 17, Iter 67, disc loss: 0.0021613043596366522, policy loss: 7.826726793790483
Experience 17, Iter 68, disc loss: 0.0015363437079190597, policy loss: 9.271413969527398
Experience 17, Iter 69, disc loss: 0.001403391825205226, policy loss: 9.79445168943991
Experience 17, Iter 70, disc loss: 0.0013231675813536467, policy loss: 10.03691521769469
Experience 17, Iter 71, disc loss: 0.001396266098356912, policy loss: 9.918890415644368
Experience 17, Iter 72, disc loss: 0.0019146490166156705, policy loss: 9.37993920823573
Experience 17, Iter 73, disc loss: 0.0020651926427513816, policy loss: 8.60964039655216
Experience 17, Iter 74, disc loss: 0.0022564740007093007, policy loss: 8.407488537056459
Experience 17, Iter 75, disc loss: 0.001266981217893361, policy loss: 11.568724124301127
Experience 17, Iter 76, disc loss: 0.002362753317836601, policy loss: 7.943579677680351
Experience 17, Iter 77, disc loss: 0.0017118347588549964, policy loss: 8.973474707947585
Experience 17, Iter 78, disc loss: 0.001492484034578085, policy loss: 9.033936461648057
Experience 17, Iter 79, disc loss: 0.0014089466441704163, policy loss: 9.14167387104336
Experience 17, Iter 80, disc loss: 0.001436908854841651, policy loss: 9.215942623033504
Experience 17, Iter 81, disc loss: 0.001992212097784256, policy loss: 8.02259086924678
Experience 17, Iter 82, disc loss: 0.0023978522439231575, policy loss: 7.269626494754692
Experience 17, Iter 83, disc loss: 0.0013219025014118372, policy loss: 8.784937961758324
Experience 17, Iter 84, disc loss: 0.0015928213444177503, policy loss: 8.499293363101893
Experience 17, Iter 85, disc loss: 0.0023902104325927214, policy loss: 7.5645680647533124
Experience 17, Iter 86, disc loss: 0.0015764677400766066, policy loss: 8.49287437617777
Experience 17, Iter 87, disc loss: 0.0019285134029737637, policy loss: 7.796484699697524
Experience 17, Iter 88, disc loss: 0.0020207218375368607, policy loss: 7.781095623912664
Experience 17, Iter 89, disc loss: 0.0026169890922428307, policy loss: 7.278887613600257
Experience 17, Iter 90, disc loss: 0.001703788193567509, policy loss: 8.162387631073658
Experience 17, Iter 91, disc loss: 0.0022341937989307547, policy loss: 8.22101161010208
Experience 17, Iter 92, disc loss: 0.002271435579682728, policy loss: 7.330470948526457
Experience 17, Iter 93, disc loss: 0.002201763042642435, policy loss: 8.063035128586577
Experience 17, Iter 94, disc loss: 0.0028618691749661552, policy loss: 7.5557826779434585
Experience 17, Iter 95, disc loss: 0.002574268409343804, policy loss: 8.186712566043488
Experience 17, Iter 96, disc loss: 0.0028260105801711953, policy loss: 7.548854950767328
Experience 17, Iter 97, disc loss: 0.002411551491281864, policy loss: 7.7616918471553085
Experience 17, Iter 98, disc loss: 0.0024003855669692906, policy loss: 7.609864225281926
Experience 17, Iter 99, disc loss: 0.00262213148153464, policy loss: 8.0555206336409
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.1089],
        [1.0845],
        [0.0245]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0215, 0.2048, 1.1281, 0.0188, 0.0187, 2.8433]],

        [[0.0215, 0.2048, 1.1281, 0.0188, 0.0187, 2.8433]],

        [[0.0215, 0.2048, 1.1281, 0.0188, 0.0187, 2.8433]],

        [[0.0215, 0.2048, 1.1281, 0.0188, 0.0187, 2.8433]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0211, 0.4355, 4.3381, 0.0979], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0211, 0.4355, 4.3381, 0.0979])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.932
Iter 2/2000 - Loss: 2.891
Iter 3/2000 - Loss: 2.787
Iter 4/2000 - Loss: 2.748
Iter 5/2000 - Loss: 2.708
Iter 6/2000 - Loss: 2.609
Iter 7/2000 - Loss: 2.502
Iter 8/2000 - Loss: 2.405
Iter 9/2000 - Loss: 2.290
Iter 10/2000 - Loss: 2.140
Iter 11/2000 - Loss: 1.959
Iter 12/2000 - Loss: 1.760
Iter 13/2000 - Loss: 1.549
Iter 14/2000 - Loss: 1.323
Iter 15/2000 - Loss: 1.078
Iter 16/2000 - Loss: 0.812
Iter 17/2000 - Loss: 0.529
Iter 18/2000 - Loss: 0.236
Iter 19/2000 - Loss: -0.062
Iter 20/2000 - Loss: -0.359
Iter 1981/2000 - Loss: -7.625
Iter 1982/2000 - Loss: -7.625
Iter 1983/2000 - Loss: -7.625
Iter 1984/2000 - Loss: -7.626
Iter 1985/2000 - Loss: -7.626
Iter 1986/2000 - Loss: -7.626
Iter 1987/2000 - Loss: -7.626
Iter 1988/2000 - Loss: -7.626
Iter 1989/2000 - Loss: -7.626
Iter 1990/2000 - Loss: -7.626
Iter 1991/2000 - Loss: -7.626
Iter 1992/2000 - Loss: -7.626
Iter 1993/2000 - Loss: -7.626
Iter 1994/2000 - Loss: -7.626
Iter 1995/2000 - Loss: -7.626
Iter 1996/2000 - Loss: -7.626
Iter 1997/2000 - Loss: -7.626
Iter 1998/2000 - Loss: -7.626
Iter 1999/2000 - Loss: -7.626
Iter 2000/2000 - Loss: -7.626
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.7076,  5.2406, 37.0830, 10.4699, 16.8572, 47.9833]],

        [[17.0823, 30.4040,  8.6752,  1.6262,  1.6088, 25.5971]],

        [[16.8985, 34.4462,  8.3804,  1.1673,  0.8568, 25.8722]],

        [[11.2733, 29.7189, 26.9147,  2.7855,  1.8205, 46.5863]]])
Signal Variance: tensor([ 0.1184,  2.2653, 16.2107,  1.0436])
Estimated target variance: tensor([0.0211, 0.4355, 4.3381, 0.0979])
N: 180
Signal to noise ratio: tensor([19.0279, 85.5426, 94.0153, 62.3219])
Bound on condition number: tensor([  65171.7988, 1317156.8411, 1590998.3322,  699125.5796])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.002557620140004643, policy loss: 8.325261851986038
Experience 18, Iter 1, disc loss: 0.0029028863688467994, policy loss: 8.087161032607627
Experience 18, Iter 2, disc loss: 0.0027173667367522336, policy loss: 8.028231159489774
Experience 18, Iter 3, disc loss: 0.0022363552493146997, policy loss: 7.951891036715272
Experience 18, Iter 4, disc loss: 0.002253787067303627, policy loss: 7.990062774304378
Experience 18, Iter 5, disc loss: 0.002681524601303796, policy loss: 8.52429012754024
Experience 18, Iter 6, disc loss: 0.0023707177109428103, policy loss: 8.343392226327452
Experience 18, Iter 7, disc loss: 0.0028618894301789414, policy loss: 8.266717924459204
Experience 18, Iter 8, disc loss: 0.002431956438642264, policy loss: 8.067927080722463
Experience 18, Iter 9, disc loss: 0.002221101194903143, policy loss: 7.983357397352866
Experience 18, Iter 10, disc loss: 0.002559112356472904, policy loss: 7.696665614010629
Experience 18, Iter 11, disc loss: 0.0021399235420152696, policy loss: 8.07139251917722
Experience 18, Iter 12, disc loss: 0.002380034971888457, policy loss: 8.564143910226187
Experience 18, Iter 13, disc loss: 0.002875914987697616, policy loss: 8.045059032426291
Experience 18, Iter 14, disc loss: 0.0024694739219480646, policy loss: 8.065350775864594
Experience 18, Iter 15, disc loss: 0.002776877028214188, policy loss: 7.314970798783952
Experience 18, Iter 16, disc loss: 0.0023835253131660144, policy loss: 8.470983377576625
Experience 18, Iter 17, disc loss: 0.0022693487981155685, policy loss: 7.613272822216967
Experience 18, Iter 18, disc loss: 0.002451850228457482, policy loss: 8.263438828120691
Experience 18, Iter 19, disc loss: 0.0026357910371958932, policy loss: 7.644423092698308
Experience 18, Iter 20, disc loss: 0.002361383754347494, policy loss: 8.060660249165105
Experience 18, Iter 21, disc loss: 0.0027030361938421253, policy loss: 7.734981799573994
Experience 18, Iter 22, disc loss: 0.002230437351982537, policy loss: 7.862352418990873
Experience 18, Iter 23, disc loss: 0.0021027947521538564, policy loss: 8.953219650442106
Experience 18, Iter 24, disc loss: 0.002094534864259305, policy loss: 8.358588862213006
Experience 18, Iter 25, disc loss: 0.0020921503057999375, policy loss: 8.366640670379448
Experience 18, Iter 26, disc loss: 0.002386082464472517, policy loss: 8.46032551469773
Experience 18, Iter 27, disc loss: 0.002210644140108303, policy loss: 7.592431009198316
Experience 18, Iter 28, disc loss: 0.0020808086917509373, policy loss: 7.675828601539146
Experience 18, Iter 29, disc loss: 0.0021001117322913022, policy loss: 8.831241105034996
Experience 18, Iter 30, disc loss: 0.0027241068219959717, policy loss: 7.644704335730684
Experience 18, Iter 31, disc loss: 0.002432301039687561, policy loss: 7.912027808669144
Experience 18, Iter 32, disc loss: 0.002387438944351995, policy loss: 7.715358728082263
Experience 18, Iter 33, disc loss: 0.002391163934672353, policy loss: 8.66873523183691
Experience 18, Iter 34, disc loss: 0.002192986952418185, policy loss: 7.924706073385634
Experience 18, Iter 35, disc loss: 0.002164471765101372, policy loss: 8.983840556265841
Experience 18, Iter 36, disc loss: 0.002207335160379292, policy loss: 7.983447622419076
Experience 18, Iter 37, disc loss: 0.002354179401152722, policy loss: 8.194838090040413
Experience 18, Iter 38, disc loss: 0.002334987183822943, policy loss: 7.910998750924422
Experience 18, Iter 39, disc loss: 0.002307175523403337, policy loss: 7.779344370467513
Experience 18, Iter 40, disc loss: 0.0019829580884665457, policy loss: 7.6701354623211255
Experience 18, Iter 41, disc loss: 0.0024880390304373493, policy loss: 7.401293651633581
Experience 18, Iter 42, disc loss: 0.002023830211812698, policy loss: 8.358885916645825
Experience 18, Iter 43, disc loss: 0.0025545456117346396, policy loss: 8.095120087939613
Experience 18, Iter 44, disc loss: 0.001734570435226648, policy loss: 8.48341652335321
Experience 18, Iter 45, disc loss: 0.0024597419184093946, policy loss: 7.746900272999335
Experience 18, Iter 46, disc loss: 0.002029652063301495, policy loss: 8.560926829194878
Experience 18, Iter 47, disc loss: 0.002238674204963346, policy loss: 7.978392926896937
Experience 18, Iter 48, disc loss: 0.0019106311340598584, policy loss: 8.311874607053166
Experience 18, Iter 49, disc loss: 0.0017296134291692967, policy loss: 8.411756433404232
Experience 18, Iter 50, disc loss: 0.0018780348639725083, policy loss: 8.703338457923547
Experience 18, Iter 51, disc loss: 0.0020965103340703207, policy loss: 8.161168271108751
Experience 18, Iter 52, disc loss: 0.0019276174476855479, policy loss: 8.390406906971663
Experience 18, Iter 53, disc loss: 0.0019153836261562658, policy loss: 7.9821180724858305
Experience 18, Iter 54, disc loss: 0.0018869671506388588, policy loss: 8.239362427368574
Experience 18, Iter 55, disc loss: 0.0019178304562607253, policy loss: 8.976343388951108
Experience 18, Iter 56, disc loss: 0.0020372334073026506, policy loss: 7.606436457078962
Experience 18, Iter 57, disc loss: 0.0018128775460010604, policy loss: 8.572615558387156
Experience 18, Iter 58, disc loss: 0.0019326076398806162, policy loss: 8.193618536405978
Experience 18, Iter 59, disc loss: 0.001888024974257055, policy loss: 8.804500734573779
Experience 18, Iter 60, disc loss: 0.0017398632548809995, policy loss: 8.09149269996242
Experience 18, Iter 61, disc loss: 0.0021049602093776744, policy loss: 8.120120423969306
Experience 18, Iter 62, disc loss: 0.0017580745792362728, policy loss: 8.80749460646469
Experience 18, Iter 63, disc loss: 0.0017084472383078078, policy loss: 9.20664106028093
Experience 18, Iter 64, disc loss: 0.0018794603175812585, policy loss: 8.260082291348843
Experience 18, Iter 65, disc loss: 0.0016031553633511612, policy loss: 8.028521230385012
Experience 18, Iter 66, disc loss: 0.001889325064157526, policy loss: 7.718514071823891
Experience 18, Iter 67, disc loss: 0.0018609028854824753, policy loss: 8.39377175869564
Experience 18, Iter 68, disc loss: 0.0020903927756572988, policy loss: 8.069702901838939
Experience 18, Iter 69, disc loss: 0.0019176634634493774, policy loss: 7.733724993381029
Experience 18, Iter 70, disc loss: 0.0019190959428647572, policy loss: 7.848484493849798
Experience 18, Iter 71, disc loss: 0.0019606311857585704, policy loss: 7.500167100589376
Experience 18, Iter 72, disc loss: 0.0016499174122362249, policy loss: 8.939049518716917
Experience 18, Iter 73, disc loss: 0.0020334338827965415, policy loss: 8.252097719599394
Experience 18, Iter 74, disc loss: 0.001527661210449829, policy loss: 8.293065264880548
Experience 18, Iter 75, disc loss: 0.0016035110897039474, policy loss: 8.265996353569603
Experience 18, Iter 76, disc loss: 0.00188849694234558, policy loss: 8.11285012393825
Experience 18, Iter 77, disc loss: 0.0019190029765004404, policy loss: 8.532521253735036
Experience 18, Iter 78, disc loss: 0.002024567010339448, policy loss: 7.534607048962171
Experience 18, Iter 79, disc loss: 0.0014899646802473752, policy loss: 8.498519157110387
Experience 18, Iter 80, disc loss: 0.0016317533193632937, policy loss: 8.42433015541221
Experience 18, Iter 81, disc loss: 0.0016999572242806442, policy loss: 7.75323842884162
Experience 18, Iter 82, disc loss: 0.0016372709745439548, policy loss: 8.775188776823168
Experience 18, Iter 83, disc loss: 0.001862912997847519, policy loss: 7.957180540611719
Experience 18, Iter 84, disc loss: 0.001515608208885329, policy loss: 8.551305826178847
Experience 18, Iter 85, disc loss: 0.0015300535575704485, policy loss: 8.740922241677323
Experience 18, Iter 86, disc loss: 0.0017336497594206786, policy loss: 7.797997034704663
Experience 18, Iter 87, disc loss: 0.0015357405699876155, policy loss: 8.881000554426375
Experience 18, Iter 88, disc loss: 0.001364667846233558, policy loss: 9.268472614411676
Experience 18, Iter 89, disc loss: 0.0013509160769576245, policy loss: 9.169417838472356
Experience 18, Iter 90, disc loss: 0.0016574332090196846, policy loss: 8.450348887749584
Experience 18, Iter 91, disc loss: 0.0015926629948713516, policy loss: 7.95016692660906
Experience 18, Iter 92, disc loss: 0.0016133737021696033, policy loss: 8.21808909623644
Experience 18, Iter 93, disc loss: 0.0015867788706836917, policy loss: 8.341841676633177
Experience 18, Iter 94, disc loss: 0.001823232278006603, policy loss: 8.068900956892008
Experience 18, Iter 95, disc loss: 0.0015799496658599047, policy loss: 8.365854704932733
Experience 18, Iter 96, disc loss: 0.0018379951806163503, policy loss: 7.944636095538629
Experience 18, Iter 97, disc loss: 0.001617215134001835, policy loss: 8.683388640028538
Experience 18, Iter 98, disc loss: 0.0015531753775515499, policy loss: 8.480739266898055
Experience 18, Iter 99, disc loss: 0.0016873023146826505, policy loss: 7.9236002164532175
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.1214],
        [1.1612],
        [0.0260]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0206, 0.2041, 1.2002, 0.0196, 0.0200, 3.1088]],

        [[0.0206, 0.2041, 1.2002, 0.0196, 0.0200, 3.1088]],

        [[0.0206, 0.2041, 1.2002, 0.0196, 0.0200, 3.1088]],

        [[0.0206, 0.2041, 1.2002, 0.0196, 0.0200, 3.1088]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0209, 0.4854, 4.6450, 0.1041], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0209, 0.4854, 4.6450, 0.1041])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.033
Iter 2/2000 - Loss: 2.988
Iter 3/2000 - Loss: 2.874
Iter 4/2000 - Loss: 2.836
Iter 5/2000 - Loss: 2.796
Iter 6/2000 - Loss: 2.691
Iter 7/2000 - Loss: 2.575
Iter 8/2000 - Loss: 2.472
Iter 9/2000 - Loss: 2.355
Iter 10/2000 - Loss: 2.199
Iter 11/2000 - Loss: 2.011
Iter 12/2000 - Loss: 1.804
Iter 13/2000 - Loss: 1.585
Iter 14/2000 - Loss: 1.352
Iter 15/2000 - Loss: 1.100
Iter 16/2000 - Loss: 0.830
Iter 17/2000 - Loss: 0.543
Iter 18/2000 - Loss: 0.246
Iter 19/2000 - Loss: -0.055
Iter 20/2000 - Loss: -0.355
Iter 1981/2000 - Loss: -7.612
Iter 1982/2000 - Loss: -7.612
Iter 1983/2000 - Loss: -7.612
Iter 1984/2000 - Loss: -7.612
Iter 1985/2000 - Loss: -7.612
Iter 1986/2000 - Loss: -7.612
Iter 1987/2000 - Loss: -7.612
Iter 1988/2000 - Loss: -7.612
Iter 1989/2000 - Loss: -7.612
Iter 1990/2000 - Loss: -7.612
Iter 1991/2000 - Loss: -7.612
Iter 1992/2000 - Loss: -7.612
Iter 1993/2000 - Loss: -7.612
Iter 1994/2000 - Loss: -7.612
Iter 1995/2000 - Loss: -7.612
Iter 1996/2000 - Loss: -7.612
Iter 1997/2000 - Loss: -7.613
Iter 1998/2000 - Loss: -7.613
Iter 1999/2000 - Loss: -7.613
Iter 2000/2000 - Loss: -7.613
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.4579,  9.5427, 35.3966,  6.3150, 19.3406, 49.6352]],

        [[17.1911, 30.9000,  8.7287,  1.4906,  1.7485, 21.6757]],

        [[18.2504, 33.5372,  8.5980,  1.1443,  0.7922, 24.1927]],

        [[15.0174, 30.5769, 23.4684,  2.3254,  1.9739, 44.6379]]])
Signal Variance: tensor([ 0.1717,  2.1113, 14.7720,  0.8367])
Estimated target variance: tensor([0.0209, 0.4854, 4.6450, 0.1041])
N: 190
Signal to noise ratio: tensor([22.9212, 82.6959, 88.0188, 53.3311])
Bound on condition number: tensor([  99823.1276, 1299338.7819, 1471990.7589,  540400.4498])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0018986526354010551, policy loss: 9.103803853862999
Experience 19, Iter 1, disc loss: 0.002068467853309705, policy loss: 7.913081798324944
Experience 19, Iter 2, disc loss: 0.002076794827767782, policy loss: 8.013887884942037
Experience 19, Iter 3, disc loss: 0.0021872602062764463, policy loss: 7.9392170802153466
Experience 19, Iter 4, disc loss: 0.002262561030892362, policy loss: 7.208723561564219
Experience 19, Iter 5, disc loss: 0.0018907999875541004, policy loss: 8.191799639721072
Experience 19, Iter 6, disc loss: 0.0021646437645039245, policy loss: 7.559415380575771
Experience 19, Iter 7, disc loss: 0.0021850250434659064, policy loss: 8.36538562625963
Experience 19, Iter 8, disc loss: 0.0023942430043768985, policy loss: 7.400110842305959
Experience 19, Iter 9, disc loss: 0.001998496532529168, policy loss: 8.36623873986445
Experience 19, Iter 10, disc loss: 0.0017840399239005096, policy loss: 7.834432539418753
Experience 19, Iter 11, disc loss: 0.0020640289833009553, policy loss: 7.579993942747282
Experience 19, Iter 12, disc loss: 0.002061671929547225, policy loss: 7.530921988090534
Experience 19, Iter 13, disc loss: 0.001857294571150528, policy loss: 8.341401807807715
Experience 19, Iter 14, disc loss: 0.0020330754514488155, policy loss: 7.997128926988964
Experience 19, Iter 15, disc loss: 0.0017499182080201745, policy loss: 8.411314484496422
Experience 19, Iter 16, disc loss: 0.0017627492814011644, policy loss: 7.765859241938468
Experience 19, Iter 17, disc loss: 0.001980348622457531, policy loss: 7.946057970389382
Experience 19, Iter 18, disc loss: 0.0017049674609034148, policy loss: 8.451231396517803
Experience 19, Iter 19, disc loss: 0.001702577492358118, policy loss: 8.379491790255887
Experience 19, Iter 20, disc loss: 0.002098724827450485, policy loss: 7.927526466649871
Experience 19, Iter 21, disc loss: 0.0018045842324926034, policy loss: 8.415253908908687
Experience 19, Iter 22, disc loss: 0.0018660845451927489, policy loss: 8.067129094082615
Experience 19, Iter 23, disc loss: 0.0016018308580981102, policy loss: 8.600540421787889
Experience 19, Iter 24, disc loss: 0.0019069640581002348, policy loss: 8.851096990262942
Experience 19, Iter 25, disc loss: 0.0016875192005741209, policy loss: 9.321450191587132
Experience 19, Iter 26, disc loss: 0.0016583438363351815, policy loss: 8.545903792395386
Experience 19, Iter 27, disc loss: 0.0017165927774896446, policy loss: 8.245173647486006
Experience 19, Iter 28, disc loss: 0.0013772204592816073, policy loss: 8.706322261645173
Experience 19, Iter 29, disc loss: 0.0013883034540106375, policy loss: 9.488400176787664
Experience 19, Iter 30, disc loss: 0.001870275757909839, policy loss: 8.278086428258927
Experience 19, Iter 31, disc loss: 0.0015682534796572607, policy loss: 9.20763090228809
Experience 19, Iter 32, disc loss: 0.0014060618789605607, policy loss: 9.018295662320678
Experience 19, Iter 33, disc loss: 0.0011566477307837152, policy loss: 9.620950599999212
Experience 19, Iter 34, disc loss: 0.0011324365846265827, policy loss: 9.61642646753585
Experience 19, Iter 35, disc loss: 0.0013624290312919144, policy loss: 8.515383729704059
Experience 19, Iter 36, disc loss: 0.0017612873351422291, policy loss: 8.06208517450305
Experience 19, Iter 37, disc loss: 0.0011936521605957783, policy loss: 9.38781856816595
Experience 19, Iter 38, disc loss: 0.0013138593579187265, policy loss: 8.921407527909638
Experience 19, Iter 39, disc loss: 0.0012535675124900926, policy loss: 8.819934334787973
Experience 19, Iter 40, disc loss: 0.001195897271796095, policy loss: 9.032286496043653
Experience 19, Iter 41, disc loss: 0.0016951963152728804, policy loss: 8.046562615225257
Experience 19, Iter 42, disc loss: 0.0015527423006925366, policy loss: 8.511136586993432
Experience 19, Iter 43, disc loss: 0.0013426660199884642, policy loss: 8.451607530835199
Experience 19, Iter 44, disc loss: 0.0019253672480792463, policy loss: 8.533708321800258
Experience 19, Iter 45, disc loss: 0.0014922543660213518, policy loss: 8.033009456740334
Experience 19, Iter 46, disc loss: 0.001442985373761641, policy loss: 8.829541091989752
Experience 19, Iter 47, disc loss: 0.0015819373268763878, policy loss: 7.8980639953049625
Experience 19, Iter 48, disc loss: 0.0018627684765984735, policy loss: 8.074963616830988
Experience 19, Iter 49, disc loss: 0.002091915994717901, policy loss: 8.100933867638243
Experience 19, Iter 50, disc loss: 0.001787277608087794, policy loss: 7.738669323564229
Experience 19, Iter 51, disc loss: 0.0014018215347434662, policy loss: 8.54421849417831
Experience 19, Iter 52, disc loss: 0.001547579335199744, policy loss: 8.498530057123947
Experience 19, Iter 53, disc loss: 0.0015849078118624882, policy loss: 8.151630925676814
Experience 19, Iter 54, disc loss: 0.0016154096248724961, policy loss: 7.928998785056206
Experience 19, Iter 55, disc loss: 0.0017905111638722424, policy loss: 8.108083041227925
Experience 19, Iter 56, disc loss: 0.0015469842344031709, policy loss: 8.395163814455945
Experience 19, Iter 57, disc loss: 0.0016868184635639924, policy loss: 8.268128704024315
Experience 19, Iter 58, disc loss: 0.0017094173881605876, policy loss: 8.638298246022245
Experience 19, Iter 59, disc loss: 0.0018908441622218592, policy loss: 7.6025756798889255
Experience 19, Iter 60, disc loss: 0.0016861511614945889, policy loss: 8.511266767807482
Experience 19, Iter 61, disc loss: 0.0017466212980398854, policy loss: 8.167014777826303
Experience 19, Iter 62, disc loss: 0.0019004058465669242, policy loss: 9.341773431779178
Experience 19, Iter 63, disc loss: 0.0016517165982072638, policy loss: 8.052459190173483
Experience 19, Iter 64, disc loss: 0.0016069666193667133, policy loss: 8.586096972774847
Experience 19, Iter 65, disc loss: 0.0017208096747081771, policy loss: 8.36935648413873
Experience 19, Iter 66, disc loss: 0.0014518544011404785, policy loss: 8.436221033691563
Experience 19, Iter 67, disc loss: 0.0018288414939837154, policy loss: 8.48870476979428
Experience 19, Iter 68, disc loss: 0.0015741623845690568, policy loss: 9.080871235142112
Experience 19, Iter 69, disc loss: 0.0016492578759265664, policy loss: 7.905714731000291
Experience 19, Iter 70, disc loss: 0.0016544010705207973, policy loss: 8.135909006084633
Experience 19, Iter 71, disc loss: 0.001590882713492325, policy loss: 8.119167004637747
Experience 19, Iter 72, disc loss: 0.0015981404346690725, policy loss: 8.329634117274406
Experience 19, Iter 73, disc loss: 0.0013822911490519638, policy loss: 8.566546560300974
Experience 19, Iter 74, disc loss: 0.0011719527126109208, policy loss: 9.194234416388792
Experience 19, Iter 75, disc loss: 0.0013555691124349386, policy loss: 8.775105654239622
Experience 19, Iter 76, disc loss: 0.0015897235877478883, policy loss: 7.952143163477379
Experience 19, Iter 77, disc loss: 0.0014816508930206211, policy loss: 9.612131533289586
Experience 19, Iter 78, disc loss: 0.0014888545769850183, policy loss: 8.52724742460931
Experience 19, Iter 79, disc loss: 0.0014694202965979822, policy loss: 8.609835101863613
Experience 19, Iter 80, disc loss: 0.0013796836248890056, policy loss: 8.904033868206575
Experience 19, Iter 81, disc loss: 0.001256672349263918, policy loss: 9.334293061400516
Experience 19, Iter 82, disc loss: 0.001511724515111684, policy loss: 8.40099257983512
Experience 19, Iter 83, disc loss: 0.0016084261542105233, policy loss: 8.322866632606136
Experience 19, Iter 84, disc loss: 0.0014908055636432354, policy loss: 9.077873847320667
Experience 19, Iter 85, disc loss: 0.001414621982506531, policy loss: 8.40076644817082
Experience 19, Iter 86, disc loss: 0.0014510560796502864, policy loss: 8.195542282183286
Experience 19, Iter 87, disc loss: 0.001520460050882529, policy loss: 7.831756029877171
Experience 19, Iter 88, disc loss: 0.0015317776557271664, policy loss: 8.634227373956028
Experience 19, Iter 89, disc loss: 0.0015596156059369936, policy loss: 7.8867614158664905
Experience 19, Iter 90, disc loss: 0.001361853676068761, policy loss: 8.692983277688228
Experience 19, Iter 91, disc loss: 0.001280787994475971, policy loss: 8.651747907842608
Experience 19, Iter 92, disc loss: 0.0014690254484248497, policy loss: 8.118980326104012
Experience 19, Iter 93, disc loss: 0.0013765315895096536, policy loss: 8.798222426792684
Experience 19, Iter 94, disc loss: 0.0016502372246698903, policy loss: 8.209535268828283
Experience 19, Iter 95, disc loss: 0.0014552333049791953, policy loss: 9.421086178011674
Experience 19, Iter 96, disc loss: 0.0014743650139563038, policy loss: 8.305460152950753
Experience 19, Iter 97, disc loss: 0.0016755312123293282, policy loss: 8.839023115860067
Experience 19, Iter 98, disc loss: 0.0014846990792811805, policy loss: 8.62098773448293
Experience 19, Iter 99, disc loss: 0.001435386883070705, policy loss: 8.463156379772137
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1295],
        [1.2095],
        [0.0270]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0200, 0.2023, 1.2462, 0.0203, 0.0213, 3.3220]],

        [[0.0200, 0.2023, 1.2462, 0.0203, 0.0213, 3.3220]],

        [[0.0200, 0.2023, 1.2462, 0.0203, 0.0213, 3.3220]],

        [[0.0200, 0.2023, 1.2462, 0.0203, 0.0213, 3.3220]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0206, 0.5181, 4.8380, 0.1080], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0206, 0.5181, 4.8380, 0.1080])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.103
Iter 2/2000 - Loss: 3.057
Iter 3/2000 - Loss: 2.935
Iter 4/2000 - Loss: 2.899
Iter 5/2000 - Loss: 2.863
Iter 6/2000 - Loss: 2.754
Iter 7/2000 - Loss: 2.630
Iter 8/2000 - Loss: 2.521
Iter 9/2000 - Loss: 2.402
Iter 10/2000 - Loss: 2.245
Iter 11/2000 - Loss: 2.053
Iter 12/2000 - Loss: 1.839
Iter 13/2000 - Loss: 1.612
Iter 14/2000 - Loss: 1.370
Iter 15/2000 - Loss: 1.111
Iter 16/2000 - Loss: 0.833
Iter 17/2000 - Loss: 0.540
Iter 18/2000 - Loss: 0.237
Iter 19/2000 - Loss: -0.070
Iter 20/2000 - Loss: -0.377
Iter 1981/2000 - Loss: -7.700
Iter 1982/2000 - Loss: -7.700
Iter 1983/2000 - Loss: -7.700
Iter 1984/2000 - Loss: -7.700
Iter 1985/2000 - Loss: -7.700
Iter 1986/2000 - Loss: -7.700
Iter 1987/2000 - Loss: -7.700
Iter 1988/2000 - Loss: -7.700
Iter 1989/2000 - Loss: -7.700
Iter 1990/2000 - Loss: -7.700
Iter 1991/2000 - Loss: -7.700
Iter 1992/2000 - Loss: -7.700
Iter 1993/2000 - Loss: -7.700
Iter 1994/2000 - Loss: -7.700
Iter 1995/2000 - Loss: -7.700
Iter 1996/2000 - Loss: -7.700
Iter 1997/2000 - Loss: -7.700
Iter 1998/2000 - Loss: -7.700
Iter 1999/2000 - Loss: -7.701
Iter 2000/2000 - Loss: -7.701
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.3196,  5.4906, 38.3575,  9.4617, 17.5157, 49.5244]],

        [[16.9710, 31.2276,  8.6235,  1.4882,  1.5997, 21.0105]],

        [[18.2444, 32.7162,  8.6080,  1.1425,  0.8066, 24.0848]],

        [[12.8567, 30.0075, 22.8379,  2.1148,  1.9443, 43.1710]]])
Signal Variance: tensor([ 0.1198,  1.9759, 14.6648,  0.8113])
Estimated target variance: tensor([0.0206, 0.5181, 4.8380, 0.1080])
N: 200
Signal to noise ratio: tensor([19.2947, 81.6170, 88.9350, 52.8284])
Bound on condition number: tensor([  74457.9665, 1332269.3219, 1581889.0245,  558168.4613])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0014766303111275673, policy loss: 8.61880497683856
Experience 20, Iter 1, disc loss: 0.0013134780356931306, policy loss: 8.751302025453313
Experience 20, Iter 2, disc loss: 0.001482695923414132, policy loss: 8.017496622700735
Experience 20, Iter 3, disc loss: 0.0015589924941918252, policy loss: 8.128324115704913
Experience 20, Iter 4, disc loss: 0.001458135837469792, policy loss: 8.29241402287957
Experience 20, Iter 5, disc loss: 0.0013820887548570606, policy loss: 8.764778619401175
Experience 20, Iter 6, disc loss: 0.0012505029557085085, policy loss: 9.483259591253969
Experience 20, Iter 7, disc loss: 0.0014263436857840637, policy loss: 9.674898565721046
Experience 20, Iter 8, disc loss: 0.0013232837075340966, policy loss: 8.510609122697907
Experience 20, Iter 9, disc loss: 0.0013891305325652605, policy loss: 8.211459115102647
Experience 20, Iter 10, disc loss: 0.0013529902578800358, policy loss: 8.33563205999927
Experience 20, Iter 11, disc loss: 0.0014703989385861497, policy loss: 8.444327692241211
Experience 20, Iter 12, disc loss: 0.0015178907307306982, policy loss: 8.464619859416166
Experience 20, Iter 13, disc loss: 0.0011191430388781576, policy loss: 9.333596543138754
Experience 20, Iter 14, disc loss: 0.0011145135357777418, policy loss: 9.028925914582999
Experience 20, Iter 15, disc loss: 0.0015437117642626598, policy loss: 8.105806346769057
Experience 20, Iter 16, disc loss: 0.001290082424489344, policy loss: 8.768822489279383
Experience 20, Iter 17, disc loss: 0.001189230973851731, policy loss: 8.918836097448937
Experience 20, Iter 18, disc loss: 0.0010594446518853602, policy loss: 8.903848849723548
Experience 20, Iter 19, disc loss: 0.000971274480700279, policy loss: 9.72407073606934
Experience 20, Iter 20, disc loss: 0.001251278711817401, policy loss: 8.574830028390899
Experience 20, Iter 21, disc loss: 0.001307480102858378, policy loss: 8.72882633139471
Experience 20, Iter 22, disc loss: 0.0013874287421657243, policy loss: 8.589884726557834
Experience 20, Iter 23, disc loss: 0.00143052961887414, policy loss: 8.60618473748685
Experience 20, Iter 24, disc loss: 0.0012536468965760688, policy loss: 8.659945192302185
Experience 20, Iter 25, disc loss: 0.001416199534004309, policy loss: 7.633867892336122
Experience 20, Iter 26, disc loss: 0.0011227621707629505, policy loss: 8.895472411034845
Experience 20, Iter 27, disc loss: 0.0012715949920912941, policy loss: 8.766166633289824
Experience 20, Iter 28, disc loss: 0.0015431071591469842, policy loss: 8.017416517156857
Experience 20, Iter 29, disc loss: 0.0011490046328425584, policy loss: 8.682521237446304
Experience 20, Iter 30, disc loss: 0.001176834063108887, policy loss: 8.40802224387349
Experience 20, Iter 31, disc loss: 0.0013394846095328109, policy loss: 8.89364578211326
Experience 20, Iter 32, disc loss: 0.0016548337305520037, policy loss: 7.757472845115939
Experience 20, Iter 33, disc loss: 0.0013473343154744445, policy loss: 8.186401602730704
Experience 20, Iter 34, disc loss: 0.001302046930348381, policy loss: 9.169054623485508
Experience 20, Iter 35, disc loss: 0.0012830098688123405, policy loss: 8.869586360588798
Experience 20, Iter 36, disc loss: 0.0014171749111756612, policy loss: 8.455871895334269
Experience 20, Iter 37, disc loss: 0.0013608321518263147, policy loss: 7.849572656415611
Experience 20, Iter 38, disc loss: 0.0011751992992346664, policy loss: 8.09157188783057
Experience 20, Iter 39, disc loss: 0.0010306930936252517, policy loss: 8.781956543267174
Experience 20, Iter 40, disc loss: 0.00135200761187155, policy loss: 8.306146740035636
Experience 20, Iter 41, disc loss: 0.0010693226077081216, policy loss: 9.126673343387397
Experience 20, Iter 42, disc loss: 0.0015410662585846934, policy loss: 8.128703032555434
Experience 20, Iter 43, disc loss: 0.001258281945861314, policy loss: 8.426331283965915
Experience 20, Iter 44, disc loss: 0.0011545864619538489, policy loss: 8.694817766624531
Experience 20, Iter 45, disc loss: 0.0011295136482654647, policy loss: 8.946697452049275
Experience 20, Iter 46, disc loss: 0.0012544002106500865, policy loss: 7.9175936584320725
Experience 20, Iter 47, disc loss: 0.0011694343179919756, policy loss: 8.716099804763267
Experience 20, Iter 48, disc loss: 0.0012053370001168817, policy loss: 8.204083417215724
Experience 20, Iter 49, disc loss: 0.001690479232349426, policy loss: 7.729449696478111
Experience 20, Iter 50, disc loss: 0.0012522588444765933, policy loss: 8.468591623938085
Experience 20, Iter 51, disc loss: 0.0011596970072227726, policy loss: 8.612983577506693
Experience 20, Iter 52, disc loss: 0.0012841754681605054, policy loss: 8.359459502639705
Experience 20, Iter 53, disc loss: 0.0013725277040531625, policy loss: 8.488852596162326
Experience 20, Iter 54, disc loss: 0.0012283612296499712, policy loss: 8.5853896108573
Experience 20, Iter 55, disc loss: 0.0013034432225543723, policy loss: 8.73647222446274
Experience 20, Iter 56, disc loss: 0.0011647388505853147, policy loss: 8.969455388536371
Experience 20, Iter 57, disc loss: 0.0010225931612455376, policy loss: 9.601544940159782
Experience 20, Iter 58, disc loss: 0.0012819265724262068, policy loss: 8.935454743491572
Experience 20, Iter 59, disc loss: 0.0011945593692774977, policy loss: 8.573414164206326
Experience 20, Iter 60, disc loss: 0.0012371913315886056, policy loss: 8.685393159090292
Experience 20, Iter 61, disc loss: 0.0011273150263475243, policy loss: 8.990604072598504
Experience 20, Iter 62, disc loss: 0.0011061435479099199, policy loss: 9.308433094647784
Experience 20, Iter 63, disc loss: 0.0012142457528848164, policy loss: 8.388071050913457
Experience 20, Iter 64, disc loss: 0.0013697012474375393, policy loss: 8.330658694573518
Experience 20, Iter 65, disc loss: 0.0012155053795440485, policy loss: 9.663586829325086
Experience 20, Iter 66, disc loss: 0.0012795186036779881, policy loss: 7.916246011114819
Experience 20, Iter 67, disc loss: 0.0012641341016949611, policy loss: 9.153250696049216
Experience 20, Iter 68, disc loss: 0.0011798332794297271, policy loss: 8.52271255370207
Experience 20, Iter 69, disc loss: 0.0013196202622959435, policy loss: 8.284190291017808
Experience 20, Iter 70, disc loss: 0.0011662909851981817, policy loss: 8.435230072764146
Experience 20, Iter 71, disc loss: 0.0011856108596110947, policy loss: 8.3974553747042
Experience 20, Iter 72, disc loss: 0.0011856102006386473, policy loss: 8.943774530325914
Experience 20, Iter 73, disc loss: 0.0012261608300911868, policy loss: 8.900463015648974
Experience 20, Iter 74, disc loss: 0.0012649654118216985, policy loss: 7.784615711086087
Experience 20, Iter 75, disc loss: 0.0012413708588544753, policy loss: 8.328955226724714
Experience 20, Iter 76, disc loss: 0.0011738936668982225, policy loss: 9.609485693855964
Experience 20, Iter 77, disc loss: 0.0012379808409312973, policy loss: 8.202637598762013
Experience 20, Iter 78, disc loss: 0.0012840819558121261, policy loss: 8.237570413509019
Experience 20, Iter 79, disc loss: 0.0010610950167133522, policy loss: 8.800792344582993
Experience 20, Iter 80, disc loss: 0.0011277567446141968, policy loss: 8.510710616015107
Experience 20, Iter 81, disc loss: 0.0011385033505447464, policy loss: 8.416839253579374
Experience 20, Iter 82, disc loss: 0.0010762231268918636, policy loss: 8.624131649658883
Experience 20, Iter 83, disc loss: 0.0010053981705423713, policy loss: 8.793526257132434
Experience 20, Iter 84, disc loss: 0.0010466647609531235, policy loss: 8.986024412164243
Experience 20, Iter 85, disc loss: 0.0011652351488685674, policy loss: 8.260954923468947
Experience 20, Iter 86, disc loss: 0.0011574212494822869, policy loss: 8.524767062529014
Experience 20, Iter 87, disc loss: 0.0012096482263881055, policy loss: 8.14876480919088
Experience 20, Iter 88, disc loss: 0.001133275658327894, policy loss: 8.139621803803701
Experience 20, Iter 89, disc loss: 0.0009576687436575046, policy loss: 8.980033167889701
Experience 20, Iter 90, disc loss: 0.0012579771027973455, policy loss: 7.8603019166200285
Experience 20, Iter 91, disc loss: 0.0011036553338167527, policy loss: 8.455972005849414
Experience 20, Iter 92, disc loss: 0.0010480828102818244, policy loss: 9.426916997212395
Experience 20, Iter 93, disc loss: 0.001167749644987736, policy loss: 8.62632775012443
Experience 20, Iter 94, disc loss: 0.0011152200561699063, policy loss: 8.4921753287078
Experience 20, Iter 95, disc loss: 0.0009508911018278094, policy loss: 8.76441186441607
Experience 20, Iter 96, disc loss: 0.0010048301907637054, policy loss: 9.470282342412712
Experience 20, Iter 97, disc loss: 0.0011258909291835056, policy loss: 8.423215066172252
Experience 20, Iter 98, disc loss: 0.0008969341067957175, policy loss: 8.796059904021988
Experience 20, Iter 99, disc loss: 0.0010123177686098598, policy loss: 8.548453700186057
