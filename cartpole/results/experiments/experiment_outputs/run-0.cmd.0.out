Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0060],
        [0.2773],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0019, 0.0240, 1.1090, 0.0198], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0019, 0.0240, 1.1090, 0.0198])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.059
Iter 2/2000 - Loss: -0.809
Iter 3/2000 - Loss: -0.627
Iter 4/2000 - Loss: -0.892
Iter 5/2000 - Loss: -0.998
Iter 6/2000 - Loss: -0.948
Iter 7/2000 - Loss: -0.916
Iter 8/2000 - Loss: -1.010
Iter 9/2000 - Loss: -1.133
Iter 10/2000 - Loss: -1.186
Iter 11/2000 - Loss: -1.155
Iter 12/2000 - Loss: -1.099
Iter 13/2000 - Loss: -1.098
Iter 14/2000 - Loss: -1.172
Iter 15/2000 - Loss: -1.265
Iter 16/2000 - Loss: -1.306
Iter 17/2000 - Loss: -1.273
Iter 18/2000 - Loss: -1.228
Iter 19/2000 - Loss: -1.246
Iter 20/2000 - Loss: -1.318
Iter 1981/2000 - Loss: -1.432
Iter 1982/2000 - Loss: -1.432
Iter 1983/2000 - Loss: -1.432
Iter 1984/2000 - Loss: -1.432
Iter 1985/2000 - Loss: -1.432
Iter 1986/2000 - Loss: -1.432
Iter 1987/2000 - Loss: -1.432
Iter 1988/2000 - Loss: -1.432
Iter 1989/2000 - Loss: -1.432
Iter 1990/2000 - Loss: -1.432
Iter 1991/2000 - Loss: -1.432
Iter 1992/2000 - Loss: -1.432
Iter 1993/2000 - Loss: -1.432
Iter 1994/2000 - Loss: -1.432
Iter 1995/2000 - Loss: -1.432
Iter 1996/2000 - Loss: -1.432
Iter 1997/2000 - Loss: -1.432
Iter 1998/2000 - Loss: -1.432
Iter 1999/2000 - Loss: -1.432
Iter 2000/2000 - Loss: -1.432
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0043],
        [0.1659],
        [0.0035]])
Lengthscale: tensor([[[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]],

        [[4.1206e-03, 1.3357e-02, 2.3637e-01, 5.6998e-03, 1.3060e-04,
          1.5856e-02]]])
Signal Variance: tensor([0.0014, 0.0173, 0.8323, 0.0143])
Estimated target variance: tensor([0.0019, 0.0240, 1.1090, 0.0198])
N: 10
Signal to noise ratio: tensor([2.0038, 2.0053, 2.2402, 2.0069])
Bound on condition number: tensor([41.1528, 41.2136, 51.1834, 41.2767])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.3910735912999121, policy loss: 0.6906952613717878
Experience 1, Iter 1, disc loss: 1.3822488635551284, policy loss: 0.6943034697328653
Experience 1, Iter 2, disc loss: 1.381857886558008, policy loss: 0.6903031204099923
Experience 1, Iter 3, disc loss: 1.3760807776297495, policy loss: 0.6912753092877146
Experience 1, Iter 4, disc loss: 1.3654916878316152, policy loss: 0.6965751364540647
Experience 1, Iter 5, disc loss: 1.3602859683133381, policy loss: 0.6971039018743138
Experience 1, Iter 6, disc loss: 1.3498888481495874, policy loss: 0.7023726764954981
Experience 1, Iter 7, disc loss: 1.3427999217410806, policy loss: 0.704756696955074
Experience 1, Iter 8, disc loss: 1.3358731065319402, policy loss: 0.7069111508986624
Experience 1, Iter 9, disc loss: 1.333861068439663, policy loss: 0.7043014502401606
Experience 1, Iter 10, disc loss: 1.326073102914361, policy loss: 0.7073003377692453
Experience 1, Iter 11, disc loss: 1.3192227153776463, policy loss: 0.7094855048451316
Experience 1, Iter 12, disc loss: 1.3113810651750801, policy loss: 0.7124743999517226
Experience 1, Iter 13, disc loss: 1.3073270322102228, policy loss: 0.7119979136415715
Experience 1, Iter 14, disc loss: 1.3017390406780887, policy loss: 0.7131607889545638
Experience 1, Iter 15, disc loss: 1.2934267853005372, policy loss: 0.7168106701520947
Experience 1, Iter 16, disc loss: 1.2895535420133404, policy loss: 0.7163963313334096
Experience 1, Iter 17, disc loss: 1.2843289810837342, policy loss: 0.7170999268003881
Experience 1, Iter 18, disc loss: 1.276671711359564, policy loss: 0.7204070415082682
Experience 1, Iter 19, disc loss: 1.2724198377045526, policy loss: 0.7203376861574823
Experience 1, Iter 20, disc loss: 1.2683016334883002, policy loss: 0.7201929436401484
Experience 1, Iter 21, disc loss: 1.2612678441671967, policy loss: 0.7230682274603293
Experience 1, Iter 22, disc loss: 1.2572262990312744, policy loss: 0.7226794534169205
Experience 1, Iter 23, disc loss: 1.2503306033074488, policy loss: 0.725249800683073
Experience 1, Iter 24, disc loss: 1.245957966077938, policy loss: 0.725370177646552
Experience 1, Iter 25, disc loss: 1.240922989140075, policy loss: 0.7259851214440984
Experience 1, Iter 26, disc loss: 1.234586288599378, policy loss: 0.728157070270254
Experience 1, Iter 27, disc loss: 1.2286982060382297, policy loss: 0.7295647498005693
Experience 1, Iter 28, disc loss: 1.2242623407680155, policy loss: 0.7295078393091539
Experience 1, Iter 29, disc loss: 1.2169467389451307, policy loss: 0.7325077593482938
Experience 1, Iter 30, disc loss: 1.2133098094873411, policy loss: 0.7316515082078174
Experience 1, Iter 31, disc loss: 1.2070699820961805, policy loss: 0.7332992512533723
Experience 1, Iter 32, disc loss: 1.201936282134997, policy loss: 0.733605727820769
Experience 1, Iter 33, disc loss: 1.1992130964974566, policy loss: 0.7316760906165083
Experience 1, Iter 34, disc loss: 1.1892925417067493, policy loss: 0.7373458800663989
Experience 1, Iter 35, disc loss: 1.1843920472980045, policy loss: 0.7372606931293642
Experience 1, Iter 36, disc loss: 1.1794932016232766, policy loss: 0.7372211229143272
Experience 1, Iter 37, disc loss: 1.1731905114065087, policy loss: 0.7386414408380113
Experience 1, Iter 38, disc loss: 1.1660502060430251, policy loss: 0.741144884218081
Experience 1, Iter 39, disc loss: 1.1604899508238709, policy loss: 0.7416028095999323
Experience 1, Iter 40, disc loss: 1.153399150246841, policy loss: 0.7437390615035971
Experience 1, Iter 41, disc loss: 1.149338400826027, policy loss: 0.7423530502382529
Experience 1, Iter 42, disc loss: 1.1412538823456138, policy loss: 0.7455088668898201
Experience 1, Iter 43, disc loss: 1.130812660172697, policy loss: 0.7513847870887829
Experience 1, Iter 44, disc loss: 1.126206507765209, policy loss: 0.7505355104573498
Experience 1, Iter 45, disc loss: 1.1196173970982919, policy loss: 0.7516461929973723
Experience 1, Iter 46, disc loss: 1.116119202514798, policy loss: 0.7495902219101424
Experience 1, Iter 47, disc loss: 1.10308308238181, policy loss: 0.7579282509156402
Experience 1, Iter 48, disc loss: 1.0960118736942717, policy loss: 0.7595394237078408
Experience 1, Iter 49, disc loss: 1.0913100375726736, policy loss: 0.7581448104112752
Experience 1, Iter 50, disc loss: 1.0841353484179204, policy loss: 0.7599589896915409
Experience 1, Iter 51, disc loss: 1.0749071293544872, policy loss: 0.763845219520331
Experience 1, Iter 52, disc loss: 1.0658662231511788, policy loss: 0.7676152873540864
Experience 1, Iter 53, disc loss: 1.0578229268642687, policy loss: 0.7698803751649588
Experience 1, Iter 54, disc loss: 1.0494089142997012, policy loss: 0.7725690155133168
Experience 1, Iter 55, disc loss: 1.041729299724465, policy loss: 0.7738015679302324
Experience 1, Iter 56, disc loss: 1.0361824559739228, policy loss: 0.7713918409984813
Experience 1, Iter 57, disc loss: 1.024358096843534, policy loss: 0.7764961900324283
Experience 1, Iter 58, disc loss: 1.0131992376283345, policy loss: 0.7802397341605904
Experience 1, Iter 59, disc loss: 1.0036837267344851, policy loss: 0.7816449992391574
Experience 1, Iter 60, disc loss: 0.9977958755373679, policy loss: 0.778820897567172
Experience 1, Iter 61, disc loss: 0.9829295024879596, policy loss: 0.7865751580164826
Experience 1, Iter 62, disc loss: 0.9673708287244975, policy loss: 0.7950391509211413
Experience 1, Iter 63, disc loss: 0.9654346653626033, policy loss: 0.7856772890361746
Experience 1, Iter 64, disc loss: 0.9446931519084816, policy loss: 0.7993693062825575
Experience 1, Iter 65, disc loss: 0.9383328698871455, policy loss: 0.7947950109923696
Experience 1, Iter 66, disc loss: 0.9216142823149408, policy loss: 0.8024921775468667
Experience 1, Iter 67, disc loss: 0.9179114026162127, policy loss: 0.7940766902234578
Experience 1, Iter 68, disc loss: 0.8935668347792676, policy loss: 0.8114341489760217
Experience 1, Iter 69, disc loss: 0.8927618346310636, policy loss: 0.799618871496817
Experience 1, Iter 70, disc loss: 0.8747614375603365, policy loss: 0.8096938368333493
Experience 1, Iter 71, disc loss: 0.8605220522575159, policy loss: 0.8143336101685795
Experience 1, Iter 72, disc loss: 0.848669684264563, policy loss: 0.8171581360978944
Experience 1, Iter 73, disc loss: 0.8361465609611629, policy loss: 0.8202639846862072
Experience 1, Iter 74, disc loss: 0.8313071751828687, policy loss: 0.8141735008665529
Experience 1, Iter 75, disc loss: 0.8070801491558706, policy loss: 0.834601825069641
Experience 1, Iter 76, disc loss: 0.8035278128214284, policy loss: 0.8268518647108833
Experience 1, Iter 77, disc loss: 0.7874320204369336, policy loss: 0.8366066909896033
Experience 1, Iter 78, disc loss: 0.7710135794416334, policy loss: 0.846973460557854
Experience 1, Iter 79, disc loss: 0.769209019233047, policy loss: 0.8379313818894452
Experience 1, Iter 80, disc loss: 0.7573744698918575, policy loss: 0.8428981882366978
Experience 1, Iter 81, disc loss: 0.7493634987542425, policy loss: 0.843595850018517
Experience 1, Iter 82, disc loss: 0.7362139104726169, policy loss: 0.8522515846469771
Experience 1, Iter 83, disc loss: 0.725199177653715, policy loss: 0.8577241722926915
Experience 1, Iter 84, disc loss: 0.7097331058699419, policy loss: 0.8683034469138754
Experience 1, Iter 85, disc loss: 0.7029268851796135, policy loss: 0.8684402932574431
Experience 1, Iter 86, disc loss: 0.6871073761194854, policy loss: 0.8819006157611471
Experience 1, Iter 87, disc loss: 0.6857375694171566, policy loss: 0.8754993308777501
Experience 1, Iter 88, disc loss: 0.6787260527549815, policy loss: 0.8771144401274718
Experience 1, Iter 89, disc loss: 0.6702396175415433, policy loss: 0.8806529127308002
Experience 1, Iter 90, disc loss: 0.654218984224453, policy loss: 0.8959433977512602
Experience 1, Iter 91, disc loss: 0.6517291816529788, policy loss: 0.8919386626373095
Experience 1, Iter 92, disc loss: 0.6387147004496099, policy loss: 0.9049859337051764
Experience 1, Iter 93, disc loss: 0.624565443755263, policy loss: 0.9189190298540612
Experience 1, Iter 94, disc loss: 0.6179452846073848, policy loss: 0.9213589732074942
Experience 1, Iter 95, disc loss: 0.6097923572864778, policy loss: 0.928460641743722
Experience 1, Iter 96, disc loss: 0.6073220549612778, policy loss: 0.9272776620667962
Experience 1, Iter 97, disc loss: 0.6009684307307064, policy loss: 0.9292670347436829
Experience 1, Iter 98, disc loss: 0.5924824071593828, policy loss: 0.9374221594842094
Experience 1, Iter 99, disc loss: 0.5861792264475058, policy loss: 0.9448926515702979
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0037],
        [0.1324],
        [0.0024]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0013, 0.0147, 0.5297, 0.0095], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0013, 0.0147, 0.5297, 0.0095])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.037
Iter 2/2000 - Loss: -1.493
Iter 3/2000 - Loss: -1.442
Iter 4/2000 - Loss: -1.881
Iter 5/2000 - Loss: -1.986
Iter 6/2000 - Loss: -1.889
Iter 7/2000 - Loss: -1.883
Iter 8/2000 - Loss: -2.015
Iter 9/2000 - Loss: -2.126
Iter 10/2000 - Loss: -2.139
Iter 11/2000 - Loss: -2.096
Iter 12/2000 - Loss: -2.078
Iter 13/2000 - Loss: -2.119
Iter 14/2000 - Loss: -2.192
Iter 15/2000 - Loss: -2.247
Iter 16/2000 - Loss: -2.259
Iter 17/2000 - Loss: -2.242
Iter 18/2000 - Loss: -2.235
Iter 19/2000 - Loss: -2.264
Iter 20/2000 - Loss: -2.318
Iter 1981/2000 - Loss: -2.492
Iter 1982/2000 - Loss: -2.492
Iter 1983/2000 - Loss: -2.492
Iter 1984/2000 - Loss: -2.493
Iter 1985/2000 - Loss: -2.493
Iter 1986/2000 - Loss: -2.493
Iter 1987/2000 - Loss: -2.493
Iter 1988/2000 - Loss: -2.493
Iter 1989/2000 - Loss: -2.493
Iter 1990/2000 - Loss: -2.493
Iter 1991/2000 - Loss: -2.493
Iter 1992/2000 - Loss: -2.493
Iter 1993/2000 - Loss: -2.493
Iter 1994/2000 - Loss: -2.493
Iter 1995/2000 - Loss: -2.493
Iter 1996/2000 - Loss: -2.493
Iter 1997/2000 - Loss: -2.493
Iter 1998/2000 - Loss: -2.493
Iter 1999/2000 - Loss: -2.493
Iter 2000/2000 - Loss: -2.493
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0028],
        [0.0925],
        [0.0018]])
Lengthscale: tensor([[[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]],

        [[3.8783e-03, 8.9998e-03, 1.1305e-01, 2.8844e-03, 8.7027e-05,
          3.3699e-02]]])
Signal Variance: tensor([0.0010, 0.0112, 0.4108, 0.0073])
Estimated target variance: tensor([0.0013, 0.0147, 0.5297, 0.0095])
N: 20
Signal to noise ratio: tensor([1.9998, 2.0047, 2.1075, 2.0025])
Bound on condition number: tensor([80.9850, 81.3730, 89.8277, 81.1976])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.579126839101571, policy loss: 0.9462006950970553
Experience 2, Iter 1, disc loss: 0.5811042238464171, policy loss: 0.9381411059465207
Experience 2, Iter 2, disc loss: 0.5779051361034222, policy loss: 0.9382416264833209
Experience 2, Iter 3, disc loss: 0.571502217386104, policy loss: 0.9448672163586904
Experience 2, Iter 4, disc loss: 0.570060461931713, policy loss: 0.942478458004584
Experience 2, Iter 5, disc loss: 0.5596216649116409, policy loss: 0.954149668211101
Experience 2, Iter 6, disc loss: 0.5484695943192692, policy loss: 0.9696969469545792
Experience 2, Iter 7, disc loss: 0.5412072445116406, policy loss: 0.9782458183319788
Experience 2, Iter 8, disc loss: 0.5381614347039079, policy loss: 0.978820307205295
Experience 2, Iter 9, disc loss: 0.5319048282006347, policy loss: 0.9852143577269338
Experience 2, Iter 10, disc loss: 0.5232996421146235, policy loss: 0.9958600227963983
Experience 2, Iter 11, disc loss: 0.5246548631745609, policy loss: 0.9909028614827617
Experience 2, Iter 12, disc loss: 0.5177792866531623, policy loss: 0.9996942283730398
Experience 2, Iter 13, disc loss: 0.5042310095500636, policy loss: 1.0227301494034833
Experience 2, Iter 14, disc loss: 0.50633210268613, policy loss: 1.0163173934923555
Experience 2, Iter 15, disc loss: 0.49327598386093546, policy loss: 1.0338105660835084
Experience 2, Iter 16, disc loss: 0.49775261474985644, policy loss: 1.0262011801304254
Experience 2, Iter 17, disc loss: 0.49080414913368614, policy loss: 1.0364473118248139
Experience 2, Iter 18, disc loss: 0.48763005245151614, policy loss: 1.039580888940479
Experience 2, Iter 19, disc loss: 0.48593500488962793, policy loss: 1.0394569002358465
Experience 2, Iter 20, disc loss: 0.4669279236109402, policy loss: 1.0743394452432795
Experience 2, Iter 21, disc loss: 0.45203052604781435, policy loss: 1.1026887326233274
Experience 2, Iter 22, disc loss: 0.4669364038084213, policy loss: 1.0705817966114757
Experience 2, Iter 23, disc loss: 0.4505291651805305, policy loss: 1.0997623613148768
Experience 2, Iter 24, disc loss: 0.45051775598413085, policy loss: 1.097877752222749
Experience 2, Iter 25, disc loss: 0.44767367944107755, policy loss: 1.1034619163595794
Experience 2, Iter 26, disc loss: 0.43119164805270466, policy loss: 1.1345216318027238
Experience 2, Iter 27, disc loss: 0.4272961160957253, policy loss: 1.140146675343662
Experience 2, Iter 28, disc loss: 0.438258623800822, policy loss: 1.1147338161142868
Experience 2, Iter 29, disc loss: 0.42904737591189834, policy loss: 1.1334239597139184
Experience 2, Iter 30, disc loss: 0.4120279632325265, policy loss: 1.1719237974329477
Experience 2, Iter 31, disc loss: 0.4252117652848181, policy loss: 1.1389404784475106
Experience 2, Iter 32, disc loss: 0.4109452273388361, policy loss: 1.1695393022577867
Experience 2, Iter 33, disc loss: 0.4196977225140724, policy loss: 1.1502957644442346
Experience 2, Iter 34, disc loss: 0.3969835166248159, policy loss: 1.199377650966469
Experience 2, Iter 35, disc loss: 0.39000748126851414, policy loss: 1.214853959565151
Experience 2, Iter 36, disc loss: 0.3986904751238032, policy loss: 1.1969845353350006
Experience 2, Iter 37, disc loss: 0.3962293223022742, policy loss: 1.202197041076461
Experience 2, Iter 38, disc loss: 0.3786144889288023, policy loss: 1.2363074934274334
Experience 2, Iter 39, disc loss: 0.38443384750075416, policy loss: 1.2242642515760866
Experience 2, Iter 40, disc loss: 0.38113781184237333, policy loss: 1.2319133363835642
Experience 2, Iter 41, disc loss: 0.3479286798325676, policy loss: 1.315922634431946
Experience 2, Iter 42, disc loss: 0.37119653017199994, policy loss: 1.2549889231443907
Experience 2, Iter 43, disc loss: 0.3371672357015334, policy loss: 1.3468064960014439
Experience 2, Iter 44, disc loss: 0.3634057595870412, policy loss: 1.2759007540182525
Experience 2, Iter 45, disc loss: 0.3378525427827011, policy loss: 1.3379830616653245
Experience 2, Iter 46, disc loss: 0.344351674140968, policy loss: 1.3230644827426834
Experience 2, Iter 47, disc loss: 0.338560764262302, policy loss: 1.3319157739944105
Experience 2, Iter 48, disc loss: 0.3342324571637528, policy loss: 1.3480219969543643
Experience 2, Iter 49, disc loss: 0.3269243253487943, policy loss: 1.3708007969944573
Experience 2, Iter 50, disc loss: 0.32454113854125693, policy loss: 1.3784658966495704
Experience 2, Iter 51, disc loss: 0.32312736923211016, policy loss: 1.3808119251723494
Experience 2, Iter 52, disc loss: 0.30506106623050794, policy loss: 1.438261267420142
Experience 2, Iter 53, disc loss: 0.2927369318141021, policy loss: 1.4779182258077048
Experience 2, Iter 54, disc loss: 0.3136725931542818, policy loss: 1.4057266443026912
Experience 2, Iter 55, disc loss: 0.28964401107106685, policy loss: 1.4838835923597986
Experience 2, Iter 56, disc loss: 0.3066440069456163, policy loss: 1.428429979812078
Experience 2, Iter 57, disc loss: 0.30269380779352306, policy loss: 1.4367371997714085
Experience 2, Iter 58, disc loss: 0.2822195389636283, policy loss: 1.5077198186727296
Experience 2, Iter 59, disc loss: 0.2829991239624297, policy loss: 1.495715290381606
Experience 2, Iter 60, disc loss: 0.27435127065085385, policy loss: 1.5392901717992178
Experience 2, Iter 61, disc loss: 0.28403007620395326, policy loss: 1.491586587785491
Experience 2, Iter 62, disc loss: 0.25768609053785513, policy loss: 1.5929070192160375
Experience 2, Iter 63, disc loss: 0.2670295960393182, policy loss: 1.5589478091266036
Experience 2, Iter 64, disc loss: 0.2655919483988952, policy loss: 1.567247770843516
Experience 2, Iter 65, disc loss: 0.24022206767352056, policy loss: 1.6771743139737432
Experience 2, Iter 66, disc loss: 0.2604185076348213, policy loss: 1.591571024445654
Experience 2, Iter 67, disc loss: 0.2510324461535201, policy loss: 1.6146983035495035
Experience 2, Iter 68, disc loss: 0.2372577221998055, policy loss: 1.6856245392519713
Experience 2, Iter 69, disc loss: 0.24891538463267027, policy loss: 1.6344096071156242
Experience 2, Iter 70, disc loss: 0.23942717036615319, policy loss: 1.6671932703050272
Experience 2, Iter 71, disc loss: 0.23827451061753183, policy loss: 1.674636353422191
Experience 2, Iter 72, disc loss: 0.2407413628291791, policy loss: 1.659356447054924
Experience 2, Iter 73, disc loss: 0.22561649573618076, policy loss: 1.7191831195914502
Experience 2, Iter 74, disc loss: 0.22281539196180897, policy loss: 1.7292182046002327
Experience 2, Iter 75, disc loss: 0.22233442744978205, policy loss: 1.7384319768595708
Experience 2, Iter 76, disc loss: 0.20205905924444661, policy loss: 1.8413413844904698
Experience 2, Iter 77, disc loss: 0.2081315306356704, policy loss: 1.807229273841826
Experience 2, Iter 78, disc loss: 0.20026879756668325, policy loss: 1.8290771434755433
Experience 2, Iter 79, disc loss: 0.19771574064715763, policy loss: 1.863357270176941
Experience 2, Iter 80, disc loss: 0.2021039556344622, policy loss: 1.8400364929780615
Experience 2, Iter 81, disc loss: 0.18278952359842965, policy loss: 1.9397543937079003
Experience 2, Iter 82, disc loss: 0.17104253913804682, policy loss: 2.0244569729741784
Experience 2, Iter 83, disc loss: 0.19067531319028713, policy loss: 1.8993985769845647
Experience 2, Iter 84, disc loss: 0.18304815385504689, policy loss: 1.9404722416195352
Experience 2, Iter 85, disc loss: 0.18993909976370582, policy loss: 1.8835145340443913
Experience 2, Iter 86, disc loss: 0.18787192623425808, policy loss: 1.9004092350373687
Experience 2, Iter 87, disc loss: 0.17570802247947595, policy loss: 1.9914312354085648
Experience 2, Iter 88, disc loss: 0.170268793669607, policy loss: 1.993913073601366
Experience 2, Iter 89, disc loss: 0.1675788578944418, policy loss: 2.0262842444284956
Experience 2, Iter 90, disc loss: 0.1606254533000478, policy loss: 2.0736830183960535
Experience 2, Iter 91, disc loss: 0.15361911116067792, policy loss: 2.1308689649890926
Experience 2, Iter 92, disc loss: 0.15395027585122614, policy loss: 2.1088961742850643
Experience 2, Iter 93, disc loss: 0.15710204711647122, policy loss: 2.10026288105184
Experience 2, Iter 94, disc loss: 0.15066146146205167, policy loss: 2.128381039957942
Experience 2, Iter 95, disc loss: 0.15522656972069196, policy loss: 2.0855728573110515
Experience 2, Iter 96, disc loss: 0.14669330910792913, policy loss: 2.186048437286906
Experience 2, Iter 97, disc loss: 0.1445639927839867, policy loss: 2.182105352143773
Experience 2, Iter 98, disc loss: 0.14373008424027536, policy loss: 2.1728144559599576
Experience 2, Iter 99, disc loss: 0.14132309851984756, policy loss: 2.2182147402745764
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0034],
        [0.0907],
        [0.0016]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0639e-03, 1.3828e-02, 7.6290e-02, 2.2447e-03, 6.3811e-05,
          5.8597e-02]],

        [[3.0639e-03, 1.3828e-02, 7.6290e-02, 2.2447e-03, 6.3811e-05,
          5.8597e-02]],

        [[3.0639e-03, 1.3828e-02, 7.6290e-02, 2.2447e-03, 6.3811e-05,
          5.8597e-02]],

        [[3.0639e-03, 1.3828e-02, 7.6290e-02, 2.2447e-03, 6.3811e-05,
          5.8597e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0017, 0.0137, 0.3626, 0.0065], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0017, 0.0137, 0.3626, 0.0065])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.135
Iter 2/2000 - Loss: -1.258
Iter 3/2000 - Loss: -2.190
Iter 4/2000 - Loss: -2.449
Iter 5/2000 - Loss: -2.090
Iter 6/2000 - Loss: -2.024
Iter 7/2000 - Loss: -2.283
Iter 8/2000 - Loss: -2.524
Iter 9/2000 - Loss: -2.532
Iter 10/2000 - Loss: -2.366
Iter 11/2000 - Loss: -2.258
Iter 12/2000 - Loss: -2.343
Iter 13/2000 - Loss: -2.523
Iter 14/2000 - Loss: -2.613
Iter 15/2000 - Loss: -2.560
Iter 16/2000 - Loss: -2.482
Iter 17/2000 - Loss: -2.501
Iter 18/2000 - Loss: -2.590
Iter 19/2000 - Loss: -2.644
Iter 20/2000 - Loss: -2.637
Iter 1981/2000 - Loss: -4.509
Iter 1982/2000 - Loss: -4.509
Iter 1983/2000 - Loss: -4.509
Iter 1984/2000 - Loss: -4.509
Iter 1985/2000 - Loss: -4.509
Iter 1986/2000 - Loss: -4.509
Iter 1987/2000 - Loss: -4.509
Iter 1988/2000 - Loss: -4.509
Iter 1989/2000 - Loss: -4.509
Iter 1990/2000 - Loss: -4.509
Iter 1991/2000 - Loss: -4.509
Iter 1992/2000 - Loss: -4.509
Iter 1993/2000 - Loss: -4.509
Iter 1994/2000 - Loss: -4.509
Iter 1995/2000 - Loss: -4.509
Iter 1996/2000 - Loss: -4.509
Iter 1997/2000 - Loss: -4.509
Iter 1998/2000 - Loss: -4.509
Iter 1999/2000 - Loss: -4.509
Iter 2000/2000 - Loss: -4.509
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0662],
        [0.0013]])
Lengthscale: tensor([[[7.5752e+00, 1.0369e+00, 5.6033e+01, 3.1439e+00, 2.7875e+00,
          3.9064e+01]],

        [[1.4675e+01, 1.7533e+01, 6.1637e+00, 8.8739e-01, 5.1232e+00,
          4.0866e+00]],

        [[3.0639e-03, 1.3845e-02, 7.6290e-02, 2.2449e-03, 6.3832e-05,
          5.8599e-02]],

        [[3.0640e-03, 1.3900e-02, 7.6291e-02, 2.2457e-03, 6.3896e-05,
          5.8602e-02]]])
Signal Variance: tensor([0.0069, 0.1436, 0.2843, 0.0050])
Estimated target variance: tensor([0.0017, 0.0137, 0.3626, 0.0065])
N: 30
Signal to noise ratio: tensor([ 5.0641, 16.9120,  2.0717,  2.0019])
Bound on condition number: tensor([ 770.3578, 8581.5184,  129.7597,  121.2250])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.23018525648190968, policy loss: 1.7828318986060567
Experience 3, Iter 1, disc loss: 0.23844408221312577, policy loss: 1.7412080140715038
Experience 3, Iter 2, disc loss: 0.22434616715394, policy loss: 1.7877827524801473
Experience 3, Iter 3, disc loss: 0.22502495620577256, policy loss: 1.760992819685646
Experience 3, Iter 4, disc loss: 0.23559668686043128, policy loss: 1.692525406431518
Experience 3, Iter 5, disc loss: 0.2069557429724645, policy loss: 1.8077986825392407
Experience 3, Iter 6, disc loss: 0.21827975195453816, policy loss: 1.7606829666027182
Experience 3, Iter 7, disc loss: 0.21189051109513277, policy loss: 1.8055252264479573
Experience 3, Iter 8, disc loss: 0.20898016746752202, policy loss: 1.814400386662877
Experience 3, Iter 9, disc loss: 0.21105630325946542, policy loss: 1.7757469152600385
Experience 3, Iter 10, disc loss: 0.19233388923315833, policy loss: 1.8659054116172074
Experience 3, Iter 11, disc loss: 0.18042837185220106, policy loss: 1.9470749426289218
Experience 3, Iter 12, disc loss: 0.1880187638352854, policy loss: 1.9154993784003707
Experience 3, Iter 13, disc loss: 0.19644640827711407, policy loss: 1.8629321473057827
Experience 3, Iter 14, disc loss: 0.19497440098406377, policy loss: 1.8956877964643577
Experience 3, Iter 15, disc loss: 0.2037898584215722, policy loss: 1.8760462134814098
Experience 3, Iter 16, disc loss: 0.2081666399695295, policy loss: 1.8356184653766419
Experience 3, Iter 17, disc loss: 0.20435730414881578, policy loss: 1.8913449762929933
Experience 3, Iter 18, disc loss: 0.21372896903445793, policy loss: 1.8567911546265492
Experience 3, Iter 19, disc loss: 0.20041112006331852, policy loss: 1.941350293828831
Experience 3, Iter 20, disc loss: 0.2142052548134796, policy loss: 1.8512532110417523
Experience 3, Iter 21, disc loss: 0.2102663642179828, policy loss: 1.8802227225457604
Experience 3, Iter 22, disc loss: 0.2117700284280687, policy loss: 1.8417888657730614
Experience 3, Iter 23, disc loss: 0.19911437094032966, policy loss: 1.8765304815970432
Experience 3, Iter 24, disc loss: 0.1822552202366141, policy loss: 2.007107437294203
Experience 3, Iter 25, disc loss: 0.18945501443018528, policy loss: 1.9088389232776224
Experience 3, Iter 26, disc loss: 0.18388529757756636, policy loss: 1.9997016301930355
Experience 3, Iter 27, disc loss: 0.1736036818366027, policy loss: 2.0388002823080473
Experience 3, Iter 28, disc loss: 0.1783951778918221, policy loss: 2.0060754340959277
Experience 3, Iter 29, disc loss: 0.1733237146164318, policy loss: 2.050572986044574
Experience 3, Iter 30, disc loss: 0.175816214941773, policy loss: 2.043233193146028
Experience 3, Iter 31, disc loss: 0.16520978925420143, policy loss: 2.101610049026612
Experience 3, Iter 32, disc loss: 0.17389565361222223, policy loss: 2.036307803512886
Experience 3, Iter 33, disc loss: 0.1667147513420968, policy loss: 2.0397856055830363
Experience 3, Iter 34, disc loss: 0.16447705482256228, policy loss: 2.176058897229064
Experience 3, Iter 35, disc loss: 0.15946124452963048, policy loss: 2.1700070942845624
Experience 3, Iter 36, disc loss: 0.1513424837658454, policy loss: 2.22090188200605
Experience 3, Iter 37, disc loss: 0.16442626674395705, policy loss: 2.1273796013955946
Experience 3, Iter 38, disc loss: 0.15207230271653951, policy loss: 2.200418967583019
Experience 3, Iter 39, disc loss: 0.14147702693834982, policy loss: 2.2917478054172213
Experience 3, Iter 40, disc loss: 0.15130532880794403, policy loss: 2.20962374609352
Experience 3, Iter 41, disc loss: 0.14863217141814752, policy loss: 2.2106640992953035
Experience 3, Iter 42, disc loss: 0.1370839176346221, policy loss: 2.3185115562439984
Experience 3, Iter 43, disc loss: 0.14025335112551582, policy loss: 2.232265588663873
Experience 3, Iter 44, disc loss: 0.1375167479419211, policy loss: 2.3102518025186054
Experience 3, Iter 45, disc loss: 0.13901876795468288, policy loss: 2.2508595795644517
Experience 3, Iter 46, disc loss: 0.14233247196930934, policy loss: 2.2487267386211016
Experience 3, Iter 47, disc loss: 0.1421743220296217, policy loss: 2.26123638603784
Experience 3, Iter 48, disc loss: 0.13117089333698578, policy loss: 2.3119485023277195
Experience 3, Iter 49, disc loss: 0.12126876540600848, policy loss: 2.4529626145448513
Experience 3, Iter 50, disc loss: 0.1313917967194112, policy loss: 2.347520887613975
Experience 3, Iter 51, disc loss: 0.12283210775349829, policy loss: 2.435691875270512
Experience 3, Iter 52, disc loss: 0.1026356879084003, policy loss: 2.5407684848650325
Experience 3, Iter 53, disc loss: 0.12781405644158628, policy loss: 2.3470833325721605
Experience 3, Iter 54, disc loss: 0.10063476435737924, policy loss: 2.59791918567635
Experience 3, Iter 55, disc loss: 0.11565794307624495, policy loss: 2.458010667088194
Experience 3, Iter 56, disc loss: 0.09669452114490665, policy loss: 2.60895490087934
Experience 3, Iter 57, disc loss: 0.11741237156681367, policy loss: 2.4406784208993804
Experience 3, Iter 58, disc loss: 0.1116505032291063, policy loss: 2.5148838928996793
Experience 3, Iter 59, disc loss: 0.10502217135643785, policy loss: 2.5403378142864548
Experience 3, Iter 60, disc loss: 0.10331187559699863, policy loss: 2.6022234014272336
Experience 3, Iter 61, disc loss: 0.10018574646213611, policy loss: 2.588140610025418
Experience 3, Iter 62, disc loss: 0.10396249974413607, policy loss: 2.5929583859053054
Experience 3, Iter 63, disc loss: 0.11491297577163956, policy loss: 2.499454665192672
Experience 3, Iter 64, disc loss: 0.09868231063334386, policy loss: 2.6156668061463417
Experience 3, Iter 65, disc loss: 0.10505284076458307, policy loss: 2.5186054548449395
Experience 3, Iter 66, disc loss: 0.11004223750520159, policy loss: 2.5394155045950857
Experience 3, Iter 67, disc loss: 0.09839997062134682, policy loss: 2.629825734121816
Experience 3, Iter 68, disc loss: 0.08895191906168622, policy loss: 2.694254450983131
Experience 3, Iter 69, disc loss: 0.08507945184506312, policy loss: 2.7855458627531724
Experience 3, Iter 70, disc loss: 0.10384159179144853, policy loss: 2.5642432751805946
Experience 3, Iter 71, disc loss: 0.08433819151891406, policy loss: 2.7839210669031766
Experience 3, Iter 72, disc loss: 0.0989387005170277, policy loss: 2.5878214098560344
Experience 3, Iter 73, disc loss: 0.08639586061732997, policy loss: 2.703915509581394
Experience 3, Iter 74, disc loss: 0.09885381283566051, policy loss: 2.590586696637816
Experience 3, Iter 75, disc loss: 0.07958635351891279, policy loss: 2.8729908437628318
Experience 3, Iter 76, disc loss: 0.08011443372703744, policy loss: 2.845074197311145
Experience 3, Iter 77, disc loss: 0.0944066680056587, policy loss: 2.6606434689311254
Experience 3, Iter 78, disc loss: 0.0861599645014798, policy loss: 2.844552280544069
Experience 3, Iter 79, disc loss: 0.08310793365666298, policy loss: 2.757113685266127
Experience 3, Iter 80, disc loss: 0.08487862690471025, policy loss: 2.753486139151267
Experience 3, Iter 81, disc loss: 0.08351279741628806, policy loss: 2.7493458673446174
Experience 3, Iter 82, disc loss: 0.09238018388025505, policy loss: 2.6894268331804794
Experience 3, Iter 83, disc loss: 0.0819126139483571, policy loss: 2.8043185551258842
Experience 3, Iter 84, disc loss: 0.07863735098721472, policy loss: 2.8427147664282266
Experience 3, Iter 85, disc loss: 0.07449646625319935, policy loss: 2.9310582097416704
Experience 3, Iter 86, disc loss: 0.07730414234754134, policy loss: 2.8353190728751594
Experience 3, Iter 87, disc loss: 0.07737609377826961, policy loss: 2.8825901599290567
Experience 3, Iter 88, disc loss: 0.07698933836790757, policy loss: 2.8636017172309804
Experience 3, Iter 89, disc loss: 0.07554768287287385, policy loss: 2.8501465594743904
Experience 3, Iter 90, disc loss: 0.07692383789723503, policy loss: 2.858206800325661
Experience 3, Iter 91, disc loss: 0.073594582924516, policy loss: 2.8929165926061406
Experience 3, Iter 92, disc loss: 0.06948300738072262, policy loss: 2.9689803957315726
Experience 3, Iter 93, disc loss: 0.06784544984313728, policy loss: 3.0412548248747022
Experience 3, Iter 94, disc loss: 0.07208909817910261, policy loss: 2.9018793192321715
Experience 3, Iter 95, disc loss: 0.06383248703783648, policy loss: 3.024600785618248
Experience 3, Iter 96, disc loss: 0.056676199881334724, policy loss: 3.194108250640065
Experience 3, Iter 97, disc loss: 0.05661646405703681, policy loss: 3.2293294272636994
Experience 3, Iter 98, disc loss: 0.0675142159914287, policy loss: 2.9862617358433408
Experience 3, Iter 99, disc loss: 0.059004486903384556, policy loss: 3.1354216097341814
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0029],
        [0.0754],
        [0.0015]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.8281e-03, 1.2963e-02, 6.8834e-02, 2.2057e-03, 4.8933e-05,
          6.2174e-02]],

        [[3.8281e-03, 1.2963e-02, 6.8834e-02, 2.2057e-03, 4.8933e-05,
          6.2174e-02]],

        [[3.8281e-03, 1.2963e-02, 6.8834e-02, 2.2057e-03, 4.8933e-05,
          6.2174e-02]],

        [[3.8281e-03, 1.2963e-02, 6.8834e-02, 2.2057e-03, 4.8933e-05,
          6.2174e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0016, 0.0115, 0.3017, 0.0060], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0016, 0.0115, 0.3017, 0.0060])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.553
Iter 2/2000 - Loss: -1.131
Iter 3/2000 - Loss: -2.538
Iter 4/2000 - Loss: -2.696
Iter 5/2000 - Loss: -2.134
Iter 6/2000 - Loss: -2.151
Iter 7/2000 - Loss: -2.585
Iter 8/2000 - Loss: -2.836
Iter 9/2000 - Loss: -2.703
Iter 10/2000 - Loss: -2.466
Iter 11/2000 - Loss: -2.443
Iter 12/2000 - Loss: -2.613
Iter 13/2000 - Loss: -2.769
Iter 14/2000 - Loss: -2.792
Iter 15/2000 - Loss: -2.722
Iter 16/2000 - Loss: -2.673
Iter 17/2000 - Loss: -2.706
Iter 18/2000 - Loss: -2.786
Iter 19/2000 - Loss: -2.842
Iter 20/2000 - Loss: -2.845
Iter 1981/2000 - Loss: -6.051
Iter 1982/2000 - Loss: -6.051
Iter 1983/2000 - Loss: -6.051
Iter 1984/2000 - Loss: -6.051
Iter 1985/2000 - Loss: -6.051
Iter 1986/2000 - Loss: -6.051
Iter 1987/2000 - Loss: -6.051
Iter 1988/2000 - Loss: -6.051
Iter 1989/2000 - Loss: -6.051
Iter 1990/2000 - Loss: -6.051
Iter 1991/2000 - Loss: -6.051
Iter 1992/2000 - Loss: -6.051
Iter 1993/2000 - Loss: -6.051
Iter 1994/2000 - Loss: -6.051
Iter 1995/2000 - Loss: -6.051
Iter 1996/2000 - Loss: -6.051
Iter 1997/2000 - Loss: -6.051
Iter 1998/2000 - Loss: -6.051
Iter 1999/2000 - Loss: -6.051
Iter 2000/2000 - Loss: -6.051
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0562],
        [0.0002]])
Lengthscale: tensor([[[1.2868e+01, 9.0832e-01, 3.0679e+01, 9.9140e+00, 5.1180e-01,
          3.7349e+01]],

        [[3.0066e+01, 2.1296e+01, 7.7573e+00, 1.0612e+00, 5.2416e+00,
          5.8017e+00]],

        [[3.8332e-03, 1.2964e-02, 6.8834e-02, 2.2058e-03, 4.8937e-05,
          6.2175e-02]],

        [[3.1012e+01, 2.5537e+01, 8.0472e+00, 2.7244e+00, 8.0641e+00,
          2.6631e+01]]])
Signal Variance: tensor([0.0053, 0.2481, 0.2380, 0.1963])
Estimated target variance: tensor([0.0016, 0.0115, 0.3017, 0.0060])
N: 40
Signal to noise ratio: tensor([ 4.0386, 24.5976,  2.0579, 29.3019])
Bound on condition number: tensor([  653.4229, 24202.6475,   170.3979, 34344.9994])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.06307800468430232, policy loss: 3.1176117730826043
Experience 4, Iter 1, disc loss: 0.061405825675338595, policy loss: 3.080618938184804
Experience 4, Iter 2, disc loss: 0.05911779017516922, policy loss: 3.225575837359952
Experience 4, Iter 3, disc loss: 0.06121804251529287, policy loss: 3.1458284630128244
Experience 4, Iter 4, disc loss: 0.05258124758712, policy loss: 3.2954999650711803
Experience 4, Iter 5, disc loss: 0.06132993882001143, policy loss: 3.1283870578170365
Experience 4, Iter 6, disc loss: 0.05632935526509612, policy loss: 3.284448939775659
Experience 4, Iter 7, disc loss: 0.05952007607683189, policy loss: 3.1944131523272095
Experience 4, Iter 8, disc loss: 0.06330952741108513, policy loss: 3.1135057411104032
Experience 4, Iter 9, disc loss: 0.05049208076648448, policy loss: 3.4052510977391086
Experience 4, Iter 10, disc loss: 0.0559518421216096, policy loss: 3.2424175100387913
Experience 4, Iter 11, disc loss: 0.052146151082377676, policy loss: 3.2707869291464062
Experience 4, Iter 12, disc loss: 0.05535254582240184, policy loss: 3.237404549290057
Experience 4, Iter 13, disc loss: 0.05344956784463111, policy loss: 3.2781407152379023
Experience 4, Iter 14, disc loss: 0.052081400316937324, policy loss: 3.3214153011540186
Experience 4, Iter 15, disc loss: 0.04809925054300334, policy loss: 3.3965637005403106
Experience 4, Iter 16, disc loss: 0.0552896179600405, policy loss: 3.2805306738672475
Experience 4, Iter 17, disc loss: 0.060629164851620315, policy loss: 3.229210977986263
Experience 4, Iter 18, disc loss: 0.046417910189638464, policy loss: 3.4278156766087053
Experience 4, Iter 19, disc loss: 0.0484664342579646, policy loss: 3.4000125017497664
Experience 4, Iter 20, disc loss: 0.04887772492190564, policy loss: 3.333009313040499
Experience 4, Iter 21, disc loss: 0.05029981149487347, policy loss: 3.3479758337016445
Experience 4, Iter 22, disc loss: 0.049899040258894675, policy loss: 3.2941388719798206
Experience 4, Iter 23, disc loss: 0.04586396660742027, policy loss: 3.520010004595376
Experience 4, Iter 24, disc loss: 0.061972077734129716, policy loss: 3.132393540024419
Experience 4, Iter 25, disc loss: 0.047034257066173105, policy loss: 3.4472551008442127
Experience 4, Iter 26, disc loss: 0.04376535068938208, policy loss: 3.435024742607711
Experience 4, Iter 27, disc loss: 0.05274737649031511, policy loss: 3.2984097884580184
Experience 4, Iter 28, disc loss: 0.044573053979973035, policy loss: 3.4490664596245217
Experience 4, Iter 29, disc loss: 0.043878659459910097, policy loss: 3.4993454245216133
Experience 4, Iter 30, disc loss: 0.04747284293227513, policy loss: 3.4332046375769387
Experience 4, Iter 31, disc loss: 0.05243469987898649, policy loss: 3.277243192334483
Experience 4, Iter 32, disc loss: 0.043975667396148034, policy loss: 3.4852373961568723
Experience 4, Iter 33, disc loss: 0.04368474914893037, policy loss: 3.508165057581137
Experience 4, Iter 34, disc loss: 0.039062651099076515, policy loss: 3.6091826932193403
Experience 4, Iter 35, disc loss: 0.04179499581415491, policy loss: 3.530478178989325
Experience 4, Iter 36, disc loss: 0.03516149222047008, policy loss: 3.727790129907815
Experience 4, Iter 37, disc loss: 0.045822203168972016, policy loss: 3.4989389681164536
Experience 4, Iter 38, disc loss: 0.041807996892873664, policy loss: 3.5155614722817345
Experience 4, Iter 39, disc loss: 0.04134983839397052, policy loss: 3.5511262088920788
Experience 4, Iter 40, disc loss: 0.045138782720632674, policy loss: 3.501099006659495
Experience 4, Iter 41, disc loss: 0.040004831150555836, policy loss: 3.6069761896265
Experience 4, Iter 42, disc loss: 0.03585626336253247, policy loss: 3.6473733975567053
Experience 4, Iter 43, disc loss: 0.03320838862327989, policy loss: 3.7528728264695768
Experience 4, Iter 44, disc loss: 0.0316750497049863, policy loss: 3.7532023970572785
Experience 4, Iter 45, disc loss: 0.04184187273932741, policy loss: 3.5312099826853194
Experience 4, Iter 46, disc loss: 0.04080023313578731, policy loss: 3.5303951542559364
Experience 4, Iter 47, disc loss: 0.04336395391360014, policy loss: 3.510788200128312
Experience 4, Iter 48, disc loss: 0.039073719465977576, policy loss: 3.657241946448191
Experience 4, Iter 49, disc loss: 0.03672901361581505, policy loss: 3.636072545059342
Experience 4, Iter 50, disc loss: 0.03856032978309935, policy loss: 3.5930792121281123
Experience 4, Iter 51, disc loss: 0.037449518878435735, policy loss: 3.764696910394676
Experience 4, Iter 52, disc loss: 0.039553065177341286, policy loss: 3.6585829077225784
Experience 4, Iter 53, disc loss: 0.03879868144486197, policy loss: 3.5511761610312074
Experience 4, Iter 54, disc loss: 0.03270414293465103, policy loss: 3.750092590967231
Experience 4, Iter 55, disc loss: 0.0352204505100627, policy loss: 3.762959553923444
Experience 4, Iter 56, disc loss: 0.03545758996124574, policy loss: 3.6707482285287902
Experience 4, Iter 57, disc loss: 0.03687791484068045, policy loss: 3.6414721807028627
Experience 4, Iter 58, disc loss: 0.034793357486688326, policy loss: 3.7236141357979773
Experience 4, Iter 59, disc loss: 0.035600797856422624, policy loss: 3.6327719895129182
Experience 4, Iter 60, disc loss: 0.03342773296227028, policy loss: 3.7576056456942277
Experience 4, Iter 61, disc loss: 0.0325913412658107, policy loss: 3.786758834414349
Experience 4, Iter 62, disc loss: 0.03242722422262609, policy loss: 3.817911525188061
Experience 4, Iter 63, disc loss: 0.03526553636862966, policy loss: 3.681660689336541
Experience 4, Iter 64, disc loss: 0.031830941408658844, policy loss: 3.76696566155148
Experience 4, Iter 65, disc loss: 0.03431431889956873, policy loss: 3.6924721591861496
Experience 4, Iter 66, disc loss: 0.034557293551178966, policy loss: 3.7326279409220233
Experience 4, Iter 67, disc loss: 0.03481985175405072, policy loss: 3.733366686244195
Experience 4, Iter 68, disc loss: 0.030250111963732004, policy loss: 3.852846581595826
Experience 4, Iter 69, disc loss: 0.02657860272616887, policy loss: 4.01290579839464
Experience 4, Iter 70, disc loss: 0.031008504429261386, policy loss: 3.868915895825037
Experience 4, Iter 71, disc loss: 0.033909351903859526, policy loss: 3.785902692568207
Experience 4, Iter 72, disc loss: 0.030727682589227274, policy loss: 3.88050911092143
Experience 4, Iter 73, disc loss: 0.028175172250736562, policy loss: 3.989027753143222
Experience 4, Iter 74, disc loss: 0.034692859815635, policy loss: 3.8175303991698604
Experience 4, Iter 75, disc loss: 0.028696179631626014, policy loss: 3.95026817339189
Experience 4, Iter 76, disc loss: 0.03573982433504483, policy loss: 3.728046921717611
Experience 4, Iter 77, disc loss: 0.028420534916464846, policy loss: 3.9051695936207778
Experience 4, Iter 78, disc loss: 0.026630359216817504, policy loss: 4.030945290172385
Experience 4, Iter 79, disc loss: 0.026371670476434426, policy loss: 3.9625506987592014
Experience 4, Iter 80, disc loss: 0.030675532376600437, policy loss: 3.7992851242147028
Experience 4, Iter 81, disc loss: 0.028045130533724136, policy loss: 3.9687794780611934
Experience 4, Iter 82, disc loss: 0.026683667012796862, policy loss: 3.939603785119096
Experience 4, Iter 83, disc loss: 0.02763279311331524, policy loss: 3.981126082101419
Experience 4, Iter 84, disc loss: 0.028072045700324517, policy loss: 3.9846685852302843
Experience 4, Iter 85, disc loss: 0.026690684822138603, policy loss: 4.081264067932812
Experience 4, Iter 86, disc loss: 0.02704531750776039, policy loss: 3.96950173200546
Experience 4, Iter 87, disc loss: 0.026967787424713438, policy loss: 3.9963951984148083
Experience 4, Iter 88, disc loss: 0.025139132837492386, policy loss: 4.068541469686021
Experience 4, Iter 89, disc loss: 0.023922547518319723, policy loss: 4.116021104228739
Experience 4, Iter 90, disc loss: 0.0301701117552644, policy loss: 3.9462773203659336
Experience 4, Iter 91, disc loss: 0.02921749740761143, policy loss: 3.874252252495093
Experience 4, Iter 92, disc loss: 0.0224642836019038, policy loss: 4.174179229782798
Experience 4, Iter 93, disc loss: 0.029381285860976925, policy loss: 3.839240215636015
Experience 4, Iter 94, disc loss: 0.02918677172577367, policy loss: 3.9468902128395724
Experience 4, Iter 95, disc loss: 0.025060150169937633, policy loss: 3.9945407441837664
Experience 4, Iter 96, disc loss: 0.02294586954666447, policy loss: 4.131792583917553
Experience 4, Iter 97, disc loss: 0.023736220381420894, policy loss: 4.17181305397125
Experience 4, Iter 98, disc loss: 0.0239304705617051, policy loss: 4.101272412863548
Experience 4, Iter 99, disc loss: 0.023297000753831093, policy loss: 4.164874282159211
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0024],
        [0.0603],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0668e-03, 1.1160e-02, 5.5101e-02, 1.7982e-03, 4.1181e-05,
          5.5408e-02]],

        [[3.0668e-03, 1.1160e-02, 5.5101e-02, 1.7982e-03, 4.1181e-05,
          5.5408e-02]],

        [[3.0668e-03, 1.1160e-02, 5.5101e-02, 1.7982e-03, 4.1181e-05,
          5.5408e-02]],

        [[3.0668e-03, 1.1160e-02, 5.5101e-02, 1.7982e-03, 4.1181e-05,
          5.5408e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0015, 0.0098, 0.2412, 0.0048], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0015, 0.0098, 0.2412, 0.0048])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.974
Iter 2/2000 - Loss: -0.950
Iter 3/2000 - Loss: -2.950
Iter 4/2000 - Loss: -2.958
Iter 5/2000 - Loss: -2.174
Iter 6/2000 - Loss: -2.323
Iter 7/2000 - Loss: -2.909
Iter 8/2000 - Loss: -3.161
Iter 9/2000 - Loss: -2.976
Iter 10/2000 - Loss: -2.731
Iter 11/2000 - Loss: -2.703
Iter 12/2000 - Loss: -2.849
Iter 13/2000 - Loss: -3.010
Iter 14/2000 - Loss: -3.085
Iter 15/2000 - Loss: -3.054
Iter 16/2000 - Loss: -2.982
Iter 17/2000 - Loss: -2.961
Iter 18/2000 - Loss: -3.026
Iter 19/2000 - Loss: -3.124
Iter 20/2000 - Loss: -3.179
Iter 1981/2000 - Loss: -8.263
Iter 1982/2000 - Loss: -8.263
Iter 1983/2000 - Loss: -8.263
Iter 1984/2000 - Loss: -8.263
Iter 1985/2000 - Loss: -8.263
Iter 1986/2000 - Loss: -8.263
Iter 1987/2000 - Loss: -8.263
Iter 1988/2000 - Loss: -8.263
Iter 1989/2000 - Loss: -8.263
Iter 1990/2000 - Loss: -8.263
Iter 1991/2000 - Loss: -8.263
Iter 1992/2000 - Loss: -8.263
Iter 1993/2000 - Loss: -8.263
Iter 1994/2000 - Loss: -8.263
Iter 1995/2000 - Loss: -8.263
Iter 1996/2000 - Loss: -8.263
Iter 1997/2000 - Loss: -8.263
Iter 1998/2000 - Loss: -8.263
Iter 1999/2000 - Loss: -8.263
Iter 2000/2000 - Loss: -8.263
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[11.3163,  0.8997, 36.4200,  8.6729,  0.5937, 36.7593]],

        [[21.7801, 18.5563,  7.6520,  1.0450,  4.7943,  5.7615]],

        [[24.7730, 30.8729, 19.1018,  1.8438,  6.2885, 15.3993]],

        [[22.3591, 19.6360,  6.8443,  2.3123,  6.8675, 20.2570]]])
Signal Variance: tensor([5.3465e-03, 2.3371e-01, 1.8086e+01, 1.3557e-01])
Estimated target variance: tensor([0.0015, 0.0098, 0.2412, 0.0048])
N: 50
Signal to noise ratio: tensor([ 3.8857, 23.6119, 91.2424, 26.5469])
Bound on condition number: tensor([   755.9274,  27877.0458, 416259.5641,  35237.9166])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.018037096736539047, policy loss: 4.216604498739319
Experience 5, Iter 1, disc loss: 0.019762036777598677, policy loss: 4.119510026329687
Experience 5, Iter 2, disc loss: 0.019563449949715497, policy loss: 4.110599734186901
Experience 5, Iter 3, disc loss: 0.019848771614919034, policy loss: 4.092980736440134
Experience 5, Iter 4, disc loss: 0.01980808454728185, policy loss: 4.094831359103884
Experience 5, Iter 5, disc loss: 0.0194610695319298, policy loss: 4.103866598774598
Experience 5, Iter 6, disc loss: 0.02160679894383893, policy loss: 4.006292832123895
Experience 5, Iter 7, disc loss: 0.021883287391085648, policy loss: 3.983766724621897
Experience 5, Iter 8, disc loss: 0.02554657869301371, policy loss: 3.814631500814662
Experience 5, Iter 9, disc loss: 0.04179626398842322, policy loss: 3.3970654349966694
Experience 5, Iter 10, disc loss: 0.09858815467475758, policy loss: 2.7308971547500147
Experience 5, Iter 11, disc loss: 0.12462498170534786, policy loss: 2.6254850060161496
Experience 5, Iter 12, disc loss: 0.17461999371916717, policy loss: 2.3511906729599796
Experience 5, Iter 13, disc loss: 0.16500223676095643, policy loss: 2.5605895718055285
Experience 5, Iter 14, disc loss: 0.13791422701025566, policy loss: 2.9388707841667028
Experience 5, Iter 15, disc loss: 0.15195457458649583, policy loss: 2.9335642123393435
Experience 5, Iter 16, disc loss: 0.1399163896750793, policy loss: 2.8174473083454346
Experience 5, Iter 17, disc loss: 0.1138142406913346, policy loss: 2.806967519522562
Experience 5, Iter 18, disc loss: 0.10721096948026411, policy loss: 2.8613215327033084
Experience 5, Iter 19, disc loss: 0.10502510484963021, policy loss: 2.7649285298390773
Experience 5, Iter 20, disc loss: 0.08606454321454386, policy loss: 3.045701119912114
Experience 5, Iter 21, disc loss: 0.0716533672092474, policy loss: 3.2121782242718195
Experience 5, Iter 22, disc loss: 0.08001608259409113, policy loss: 3.1807193337337294
Experience 5, Iter 23, disc loss: 0.07692300715804228, policy loss: 3.2572822619227257
Experience 5, Iter 24, disc loss: 0.07455464041816931, policy loss: 3.366193360203718
Experience 5, Iter 25, disc loss: 0.0809728989623891, policy loss: 3.2958111967845465
Experience 5, Iter 26, disc loss: 0.07789171698679839, policy loss: 3.4693898313118714
Experience 5, Iter 27, disc loss: 0.08053382829563752, policy loss: 3.3976004148604244
Experience 5, Iter 28, disc loss: 0.08534542046162079, policy loss: 3.2988369153599466
Experience 5, Iter 29, disc loss: 0.08190735261906146, policy loss: 3.5153289469976152
Experience 5, Iter 30, disc loss: 0.07931606723465937, policy loss: 3.5604764503688315
Experience 5, Iter 31, disc loss: 0.06948088321791013, policy loss: 3.708665158026897
Experience 5, Iter 32, disc loss: 0.07845427358556245, policy loss: 3.6286273532926696
Experience 5, Iter 33, disc loss: 0.08091287848152265, policy loss: 3.610501397434746
Experience 5, Iter 34, disc loss: 0.07960139095068733, policy loss: 3.3393721056262233
Experience 5, Iter 35, disc loss: 0.07460294952390006, policy loss: 3.5590938665788574
Experience 5, Iter 36, disc loss: 0.08289483118562774, policy loss: 3.2405287412162886
Experience 5, Iter 37, disc loss: 0.07228248239719831, policy loss: 3.432599909966252
Experience 5, Iter 38, disc loss: 0.06896504344152345, policy loss: 3.5112525003382435
Experience 5, Iter 39, disc loss: 0.06531930841187292, policy loss: 3.48123037255786
Experience 5, Iter 40, disc loss: 0.059533790684695345, policy loss: 3.8316413551943005
Experience 5, Iter 41, disc loss: 0.05502285213583529, policy loss: 3.791445162865977
Experience 5, Iter 42, disc loss: 0.056602049725546714, policy loss: 3.71196022874954
Experience 5, Iter 43, disc loss: 0.06451961673946502, policy loss: 3.604203559001181
Experience 5, Iter 44, disc loss: 0.058054151813594815, policy loss: 3.7900049511114644
Experience 5, Iter 45, disc loss: 0.059033281905150795, policy loss: 3.658110889847877
Experience 5, Iter 46, disc loss: 0.05700660850006534, policy loss: 3.9407710030752328
Experience 5, Iter 47, disc loss: 0.05720284378867735, policy loss: 3.801093408254096
Experience 5, Iter 48, disc loss: 0.052058904286320824, policy loss: 3.7735235152755453
Experience 5, Iter 49, disc loss: 0.05250842275469413, policy loss: 3.7640025699735737
Experience 5, Iter 50, disc loss: 0.053712401179917574, policy loss: 3.8995621033675567
Experience 5, Iter 51, disc loss: 0.05160183035022402, policy loss: 3.9501838686273705
Experience 5, Iter 52, disc loss: 0.04923102637993167, policy loss: 3.8262423989975325
Experience 5, Iter 53, disc loss: 0.04689131853569865, policy loss: 3.9532956376762183
Experience 5, Iter 54, disc loss: 0.04977590481896265, policy loss: 3.773052321865584
Experience 5, Iter 55, disc loss: 0.04820783783167715, policy loss: 3.870833672701276
Experience 5, Iter 56, disc loss: 0.046947068049568344, policy loss: 4.034931187436586
Experience 5, Iter 57, disc loss: 0.043797725293173675, policy loss: 3.9060115115353415
Experience 5, Iter 58, disc loss: 0.045473212881490394, policy loss: 3.97828450853223
Experience 5, Iter 59, disc loss: 0.04639377276526477, policy loss: 3.9114049152848214
Experience 5, Iter 60, disc loss: 0.04716718743937492, policy loss: 3.836436705772048
Experience 5, Iter 61, disc loss: 0.04344041359269455, policy loss: 4.02298548933367
Experience 5, Iter 62, disc loss: 0.04597584094581067, policy loss: 3.929195859100895
Experience 5, Iter 63, disc loss: 0.04514029343473659, policy loss: 3.9314025624378384
Experience 5, Iter 64, disc loss: 0.037463138274171685, policy loss: 4.126699348423644
Experience 5, Iter 65, disc loss: 0.043681462294001196, policy loss: 3.9744333929692606
Experience 5, Iter 66, disc loss: 0.0450981372016994, policy loss: 3.9332402878054955
Experience 5, Iter 67, disc loss: 0.039902399671436856, policy loss: 4.04996505971118
Experience 5, Iter 68, disc loss: 0.03888498926532757, policy loss: 4.078978124223945
Experience 5, Iter 69, disc loss: 0.03783000101541671, policy loss: 4.043905611823352
Experience 5, Iter 70, disc loss: 0.03609713763671334, policy loss: 4.167580593880478
Experience 5, Iter 71, disc loss: 0.034260869426459145, policy loss: 4.164361353369841
Experience 5, Iter 72, disc loss: 0.03582643092694427, policy loss: 4.208012450981968
Experience 5, Iter 73, disc loss: 0.033458048020428136, policy loss: 4.3948760952009716
Experience 5, Iter 74, disc loss: 0.032675080233165586, policy loss: 4.371010086349723
Experience 5, Iter 75, disc loss: 0.03517939819549479, policy loss: 4.083192914378454
Experience 5, Iter 76, disc loss: 0.03219064003978277, policy loss: 4.406678792428436
Experience 5, Iter 77, disc loss: 0.033028251369560925, policy loss: 4.342025454515368
Experience 5, Iter 78, disc loss: 0.03737240931050594, policy loss: 4.133288839945662
Experience 5, Iter 79, disc loss: 0.030024730491553523, policy loss: 4.256691580395412
Experience 5, Iter 80, disc loss: 0.031233231955899013, policy loss: 4.255934634070268
Experience 5, Iter 81, disc loss: 0.02745440038484666, policy loss: 4.372711064204004
Experience 5, Iter 82, disc loss: 0.028848927941805342, policy loss: 4.392760806418851
Experience 5, Iter 83, disc loss: 0.028440221517242167, policy loss: 4.444582502993672
Experience 5, Iter 84, disc loss: 0.02867315494119486, policy loss: 4.393830428878014
Experience 5, Iter 85, disc loss: 0.027664493366948595, policy loss: 4.41081120662654
Experience 5, Iter 86, disc loss: 0.027250123085236472, policy loss: 4.359311069369889
Experience 5, Iter 87, disc loss: 0.028286098303570616, policy loss: 4.318844372790391
Experience 5, Iter 88, disc loss: 0.024474205857382192, policy loss: 4.616236263807595
Experience 5, Iter 89, disc loss: 0.025636011677238404, policy loss: 4.4827763707124015
Experience 5, Iter 90, disc loss: 0.028034151210977163, policy loss: 4.350775617887193
Experience 5, Iter 91, disc loss: 0.027928281546870534, policy loss: 4.361518592738884
Experience 5, Iter 92, disc loss: 0.024974301958699277, policy loss: 4.661171590973428
Experience 5, Iter 93, disc loss: 0.025927781521581615, policy loss: 4.42024604763961
Experience 5, Iter 94, disc loss: 0.026315860070338582, policy loss: 4.347284416460525
Experience 5, Iter 95, disc loss: 0.027503386858631258, policy loss: 4.286586201658833
Experience 5, Iter 96, disc loss: 0.023685289534589524, policy loss: 4.540545972324571
Experience 5, Iter 97, disc loss: 0.02428269375778878, policy loss: 4.54339435174358
Experience 5, Iter 98, disc loss: 0.02384337275814355, policy loss: 4.511208758080289
Experience 5, Iter 99, disc loss: 0.02333199634599461, policy loss: 4.531558531545393
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0127],
        [0.2570],
        [0.0038]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0044, 0.0184, 0.1682, 0.0053, 0.0004, 0.2426]],

        [[0.0044, 0.0184, 0.1682, 0.0053, 0.0004, 0.2426]],

        [[0.0044, 0.0184, 0.1682, 0.0053, 0.0004, 0.2426]],

        [[0.0044, 0.0184, 0.1682, 0.0053, 0.0004, 0.2426]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0021, 0.0507, 1.0281, 0.0151], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0021, 0.0507, 1.0281, 0.0151])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.891
Iter 2/2000 - Loss: 0.715
Iter 3/2000 - Loss: -0.873
Iter 4/2000 - Loss: -0.646
Iter 5/2000 - Loss: -0.120
Iter 6/2000 - Loss: -0.379
Iter 7/2000 - Loss: -0.817
Iter 8/2000 - Loss: -0.942
Iter 9/2000 - Loss: -0.780
Iter 10/2000 - Loss: -0.618
Iter 11/2000 - Loss: -0.633
Iter 12/2000 - Loss: -0.776
Iter 13/2000 - Loss: -0.914
Iter 14/2000 - Loss: -0.971
Iter 15/2000 - Loss: -0.962
Iter 16/2000 - Loss: -0.945
Iter 17/2000 - Loss: -0.962
Iter 18/2000 - Loss: -1.024
Iter 19/2000 - Loss: -1.119
Iter 20/2000 - Loss: -1.227
Iter 1981/2000 - Loss: -7.595
Iter 1982/2000 - Loss: -7.595
Iter 1983/2000 - Loss: -7.595
Iter 1984/2000 - Loss: -7.595
Iter 1985/2000 - Loss: -7.595
Iter 1986/2000 - Loss: -7.595
Iter 1987/2000 - Loss: -7.595
Iter 1988/2000 - Loss: -7.596
Iter 1989/2000 - Loss: -7.596
Iter 1990/2000 - Loss: -7.596
Iter 1991/2000 - Loss: -7.596
Iter 1992/2000 - Loss: -7.596
Iter 1993/2000 - Loss: -7.596
Iter 1994/2000 - Loss: -7.596
Iter 1995/2000 - Loss: -7.596
Iter 1996/2000 - Loss: -7.596
Iter 1997/2000 - Loss: -7.596
Iter 1998/2000 - Loss: -7.596
Iter 1999/2000 - Loss: -7.596
Iter 2000/2000 - Loss: -7.596
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0025],
        [0.0002]])
Lengthscale: tensor([[[ 9.7155,  0.5629, 38.1898,  1.9922,  6.6359, 43.1356]],

        [[19.8948, 25.5869, 25.4900,  1.4300,  1.2232, 12.1511]],

        [[17.5399, 27.2646, 16.8717,  0.8991,  4.8956, 17.5278]],

        [[16.2776, 31.9201, 14.6011,  3.3483, 11.1361, 35.9089]]])
Signal Variance: tensor([5.8940e-03, 7.0532e-01, 1.0467e+01, 4.1819e-01])
Estimated target variance: tensor([0.0021, 0.0507, 1.0281, 0.0151])
N: 60
Signal to noise ratio: tensor([ 3.8020, 34.2142, 64.4972, 47.4181])
Bound on condition number: tensor([   868.3342,  70237.6469, 249594.4087, 134909.6569])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.023798494778661562, policy loss: 4.273860119374383
Experience 6, Iter 1, disc loss: 0.02534307512884436, policy loss: 4.183144041913637
Experience 6, Iter 2, disc loss: 0.0319295150675992, policy loss: 3.959389968714766
Experience 6, Iter 3, disc loss: 0.034957775970778066, policy loss: 3.805486382806747
Experience 6, Iter 4, disc loss: 0.03393677903758478, policy loss: 3.9597734741490296
Experience 6, Iter 5, disc loss: 0.03127503101157941, policy loss: 4.254973388173567
Experience 6, Iter 6, disc loss: 0.03127805841176271, policy loss: 4.196706470316439
Experience 6, Iter 7, disc loss: 0.03206661752613385, policy loss: 3.9955945014976297
Experience 6, Iter 8, disc loss: 0.03566770264448143, policy loss: 3.7984788487556838
Experience 6, Iter 9, disc loss: 0.029992507703349464, policy loss: 4.076078634043534
Experience 6, Iter 10, disc loss: 0.02996897771725404, policy loss: 4.0752790143841855
Experience 6, Iter 11, disc loss: 0.02967963308766974, policy loss: 4.095365473002154
Experience 6, Iter 12, disc loss: 0.0281626989355026, policy loss: 4.241821577730152
Experience 6, Iter 13, disc loss: 0.031290310471566676, policy loss: 4.116731309776901
Experience 6, Iter 14, disc loss: 0.03026722499055793, policy loss: 4.063857139732721
Experience 6, Iter 15, disc loss: 0.028785685453068754, policy loss: 4.174527677002277
Experience 6, Iter 16, disc loss: 0.028306335504658556, policy loss: 4.435377823585338
Experience 6, Iter 17, disc loss: 0.030065833862394167, policy loss: 4.17806211788165
Experience 6, Iter 18, disc loss: 0.031276381514532166, policy loss: 4.15429425939423
Experience 6, Iter 19, disc loss: 0.026863535888111873, policy loss: 4.400930543553964
Experience 6, Iter 20, disc loss: 0.025286658082707532, policy loss: 4.453581644802442
Experience 6, Iter 21, disc loss: 0.026421072284985102, policy loss: 4.436893607715478
Experience 6, Iter 22, disc loss: 0.025174369022912303, policy loss: 4.4893152783475685
Experience 6, Iter 23, disc loss: 0.025801891152647555, policy loss: 4.416400380629181
Experience 6, Iter 24, disc loss: 0.02571601518547669, policy loss: 4.458304676413107
Experience 6, Iter 25, disc loss: 0.02648314844447928, policy loss: 4.331002853857908
Experience 6, Iter 26, disc loss: 0.025628174872819858, policy loss: 4.40086538093591
Experience 6, Iter 27, disc loss: 0.02528352774559957, policy loss: 4.449368758633811
Experience 6, Iter 28, disc loss: 0.025618652327552838, policy loss: 4.406879586217489
Experience 6, Iter 29, disc loss: 0.02475107934688282, policy loss: 4.562822800448723
Experience 6, Iter 30, disc loss: 0.027711997960180784, policy loss: 4.210381969367933
Experience 6, Iter 31, disc loss: 0.026574678344283485, policy loss: 4.3163320542799015
Experience 6, Iter 32, disc loss: 0.026757047291108976, policy loss: 4.346895852648546
Experience 6, Iter 33, disc loss: 0.02460429260137756, policy loss: 4.3708384989744005
Experience 6, Iter 34, disc loss: 0.02432864811577547, policy loss: 4.466692130921211
Experience 6, Iter 35, disc loss: 0.026070295415505325, policy loss: 4.324749216888981
Experience 6, Iter 36, disc loss: 0.02486873634273138, policy loss: 4.438490194176492
Experience 6, Iter 37, disc loss: 0.02860558502200663, policy loss: 4.134811913911069
Experience 6, Iter 38, disc loss: 0.025727398907323448, policy loss: 4.3499294902488
Experience 6, Iter 39, disc loss: 0.02440327791298575, policy loss: 4.322839020371079
Experience 6, Iter 40, disc loss: 0.02263920139234594, policy loss: 4.525907265117963
Experience 6, Iter 41, disc loss: 0.02353750895837356, policy loss: 4.417612091366651
Experience 6, Iter 42, disc loss: 0.02489019416155701, policy loss: 4.381482614850229
Experience 6, Iter 43, disc loss: 0.025059094315404963, policy loss: 4.312992356983757
Experience 6, Iter 44, disc loss: 0.023218475421451806, policy loss: 4.44068242169624
Experience 6, Iter 45, disc loss: 0.024208898044809243, policy loss: 4.391968259664623
Experience 6, Iter 46, disc loss: 0.021882089629574466, policy loss: 4.536794444319616
Experience 6, Iter 47, disc loss: 0.02211593687739879, policy loss: 4.522739690468978
Experience 6, Iter 48, disc loss: 0.02290918063002027, policy loss: 4.418364402971472
Experience 6, Iter 49, disc loss: 0.021286381355001882, policy loss: 4.657982490939098
Experience 6, Iter 50, disc loss: 0.023582628795742248, policy loss: 4.506652456291118
Experience 6, Iter 51, disc loss: 0.021501100819369512, policy loss: 4.491011597713149
Experience 6, Iter 52, disc loss: 0.021350965623352015, policy loss: 4.561890804985737
Experience 6, Iter 53, disc loss: 0.0191878909095239, policy loss: 4.755022678741007
Experience 6, Iter 54, disc loss: 0.022884120292701128, policy loss: 4.456114792478933
Experience 6, Iter 55, disc loss: 0.020103640877709102, policy loss: 4.6117771964754875
Experience 6, Iter 56, disc loss: 0.02017222110763106, policy loss: 4.670552692440639
Experience 6, Iter 57, disc loss: 0.019857470798038575, policy loss: 4.587110910265402
Experience 6, Iter 58, disc loss: 0.019452135457369627, policy loss: 4.759887502279554
Experience 6, Iter 59, disc loss: 0.018394785613554943, policy loss: 4.855337379443695
Experience 6, Iter 60, disc loss: 0.01923481875186335, policy loss: 4.666924898974093
Experience 6, Iter 61, disc loss: 0.020236862413266346, policy loss: 4.57508398757067
Experience 6, Iter 62, disc loss: 0.01816211724435744, policy loss: 4.7377802205227
Experience 6, Iter 63, disc loss: 0.017026092045520778, policy loss: 4.78072398065092
Experience 6, Iter 64, disc loss: 0.01665224027146525, policy loss: 4.84976842972639
Experience 6, Iter 65, disc loss: 0.018412008425539646, policy loss: 4.621348943561575
Experience 6, Iter 66, disc loss: 0.0189552333649875, policy loss: 4.597374093136105
Experience 6, Iter 67, disc loss: 0.016456720445505935, policy loss: 4.807676397899142
Experience 6, Iter 68, disc loss: 0.017094397535591638, policy loss: 4.71751487753076
Experience 6, Iter 69, disc loss: 0.016850180979059243, policy loss: 4.71917793150965
Experience 6, Iter 70, disc loss: 0.016978142598505774, policy loss: 4.654591181771785
Experience 6, Iter 71, disc loss: 0.016744139521437735, policy loss: 4.722912686058624
Experience 6, Iter 72, disc loss: 0.015292926513904435, policy loss: 4.880998764474274
Experience 6, Iter 73, disc loss: 0.017018288929008328, policy loss: 4.660510340058638
Experience 6, Iter 74, disc loss: 0.016353285093059382, policy loss: 4.797456701179397
Experience 6, Iter 75, disc loss: 0.016709647071822566, policy loss: 4.659487962414814
Experience 6, Iter 76, disc loss: 0.014265191485219158, policy loss: 4.932456970991866
Experience 6, Iter 77, disc loss: 0.01637638743949848, policy loss: 4.646221481404751
Experience 6, Iter 78, disc loss: 0.015038611524654957, policy loss: 4.802113767705791
Experience 6, Iter 79, disc loss: 0.014661887733802618, policy loss: 4.876462434153386
Experience 6, Iter 80, disc loss: 0.014714578394611816, policy loss: 4.839689516558665
Experience 6, Iter 81, disc loss: 0.014256256752314596, policy loss: 4.994003020890268
Experience 6, Iter 82, disc loss: 0.014191568233679449, policy loss: 4.893759065042337
Experience 6, Iter 83, disc loss: 0.01359152338751252, policy loss: 4.945515185791158
Experience 6, Iter 84, disc loss: 0.014884086217684389, policy loss: 4.751852021024907
Experience 6, Iter 85, disc loss: 0.013938421524848133, policy loss: 4.891797452229574
Experience 6, Iter 86, disc loss: 0.014023445677302375, policy loss: 4.8332694670010135
Experience 6, Iter 87, disc loss: 0.01403180316082209, policy loss: 4.889538464071638
Experience 6, Iter 88, disc loss: 0.014739950363819905, policy loss: 4.782560281024608
Experience 6, Iter 89, disc loss: 0.013793985734604684, policy loss: 4.955840011242164
Experience 6, Iter 90, disc loss: 0.013752350154337118, policy loss: 4.868437109745521
Experience 6, Iter 91, disc loss: 0.014325463328248526, policy loss: 4.819690119479125
Experience 6, Iter 92, disc loss: 0.014626403155412197, policy loss: 4.80205002726179
Experience 6, Iter 93, disc loss: 0.013498630339118488, policy loss: 4.868931665508833
Experience 6, Iter 94, disc loss: 0.012528308153860681, policy loss: 4.964386539700328
Experience 6, Iter 95, disc loss: 0.012592470414052924, policy loss: 4.984868363963009
Experience 6, Iter 96, disc loss: 0.012390138022539998, policy loss: 5.008543855131502
Experience 6, Iter 97, disc loss: 0.012855785144436357, policy loss: 4.921406309721678
Experience 6, Iter 98, disc loss: 0.011723798854983869, policy loss: 5.061314430212838
Experience 6, Iter 99, disc loss: 0.011539530825529583, policy loss: 5.066972809773154
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0204],
        [0.3870],
        [0.0059]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0042, 0.0238, 0.2644, 0.0081, 0.0007, 0.4210]],

        [[0.0042, 0.0238, 0.2644, 0.0081, 0.0007, 0.4210]],

        [[0.0042, 0.0238, 0.2644, 0.0081, 0.0007, 0.4210]],

        [[0.0042, 0.0238, 0.2644, 0.0081, 0.0007, 0.4210]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.0816, 1.5481, 0.0235], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.0816, 1.5481, 0.0235])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.161
Iter 2/2000 - Loss: 1.251
Iter 3/2000 - Loss: -0.138
Iter 4/2000 - Loss: 0.062
Iter 5/2000 - Loss: 0.525
Iter 6/2000 - Loss: 0.316
Iter 7/2000 - Loss: -0.063
Iter 8/2000 - Loss: -0.201
Iter 9/2000 - Loss: -0.105
Iter 10/2000 - Loss: 0.022
Iter 11/2000 - Loss: 0.041
Iter 12/2000 - Loss: -0.047
Iter 13/2000 - Loss: -0.172
Iter 14/2000 - Loss: -0.272
Iter 15/2000 - Loss: -0.326
Iter 16/2000 - Loss: -0.344
Iter 17/2000 - Loss: -0.355
Iter 18/2000 - Loss: -0.393
Iter 19/2000 - Loss: -0.474
Iter 20/2000 - Loss: -0.593
Iter 1981/2000 - Loss: -7.627
Iter 1982/2000 - Loss: -7.627
Iter 1983/2000 - Loss: -7.627
Iter 1984/2000 - Loss: -7.627
Iter 1985/2000 - Loss: -7.627
Iter 1986/2000 - Loss: -7.627
Iter 1987/2000 - Loss: -7.627
Iter 1988/2000 - Loss: -7.627
Iter 1989/2000 - Loss: -7.627
Iter 1990/2000 - Loss: -7.628
Iter 1991/2000 - Loss: -7.628
Iter 1992/2000 - Loss: -7.628
Iter 1993/2000 - Loss: -7.628
Iter 1994/2000 - Loss: -7.628
Iter 1995/2000 - Loss: -7.628
Iter 1996/2000 - Loss: -7.628
Iter 1997/2000 - Loss: -7.628
Iter 1998/2000 - Loss: -7.628
Iter 1999/2000 - Loss: -7.628
Iter 2000/2000 - Loss: -7.628
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0024],
        [0.0002]])
Lengthscale: tensor([[[14.0152,  3.0519, 48.1213,  5.2425, 12.3567, 33.8367]],

        [[17.3026,  4.8766, 30.4614,  1.5261,  6.8758, 15.8770]],

        [[16.8166, 27.6529, 14.2194,  0.9940,  6.1524, 18.6621]],

        [[14.8666, 25.0649, 16.0992,  3.7218,  9.2669, 41.8513]]])
Signal Variance: tensor([ 0.0287,  0.9318, 11.5275,  0.4870])
Estimated target variance: tensor([0.0026, 0.0816, 1.5481, 0.0235])
N: 70
Signal to noise ratio: tensor([ 8.0890, 41.0002, 69.7360, 50.5042])
Bound on condition number: tensor([  4581.1985, 117672.0874, 340418.9454, 178548.1564])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.013459723062946877, policy loss: 4.826187299812613
Experience 7, Iter 1, disc loss: 0.013644759929989812, policy loss: 4.825761369708002
Experience 7, Iter 2, disc loss: 0.012998294748708242, policy loss: 4.879767806976331
Experience 7, Iter 3, disc loss: 0.0125151497853854, policy loss: 5.023213639592635
Experience 7, Iter 4, disc loss: 0.014224301981681935, policy loss: 4.745280628518094
Experience 7, Iter 5, disc loss: 0.011808185983116434, policy loss: 5.059189648429265
Experience 7, Iter 6, disc loss: 0.012195770152492758, policy loss: 4.959801778514542
Experience 7, Iter 7, disc loss: 0.01211289076158449, policy loss: 5.008507361049664
Experience 7, Iter 8, disc loss: 0.012882471242032548, policy loss: 4.873034533863656
Experience 7, Iter 9, disc loss: 0.013282046594917819, policy loss: 4.86377969239287
Experience 7, Iter 10, disc loss: 0.013458902799709547, policy loss: 4.795996578199294
Experience 7, Iter 11, disc loss: 0.0133288689923468, policy loss: 4.885903051354964
Experience 7, Iter 12, disc loss: 0.01310138336133906, policy loss: 4.836358249417113
Experience 7, Iter 13, disc loss: 0.012543792861855071, policy loss: 4.979053473257725
Experience 7, Iter 14, disc loss: 0.013212069077222069, policy loss: 4.876644232424155
Experience 7, Iter 15, disc loss: 0.01227033950074155, policy loss: 4.966355844173118
Experience 7, Iter 16, disc loss: 0.01314597590907493, policy loss: 4.849804337681426
Experience 7, Iter 17, disc loss: 0.011843992740922847, policy loss: 4.9913611487407525
Experience 7, Iter 18, disc loss: 0.012596587175579162, policy loss: 4.922238950576544
Experience 7, Iter 19, disc loss: 0.012859317839978214, policy loss: 4.90475186656777
Experience 7, Iter 20, disc loss: 0.01279665752215772, policy loss: 4.9910554992603435
Experience 7, Iter 21, disc loss: 0.01182778572838994, policy loss: 5.108265705412519
Experience 7, Iter 22, disc loss: 0.012020654275953824, policy loss: 5.01298207212397
Experience 7, Iter 23, disc loss: 0.011282913443179745, policy loss: 5.084576329835041
Experience 7, Iter 24, disc loss: 0.01225577580981663, policy loss: 4.975129190698192
Experience 7, Iter 25, disc loss: 0.01214391761893621, policy loss: 4.966830085793186
Experience 7, Iter 26, disc loss: 0.011681889866698028, policy loss: 5.050707081216238
Experience 7, Iter 27, disc loss: 0.011013506021286843, policy loss: 5.121783424830989
Experience 7, Iter 28, disc loss: 0.01162766721194485, policy loss: 5.00963432826965
Experience 7, Iter 29, disc loss: 0.010709164423576018, policy loss: 5.171317452672515
Experience 7, Iter 30, disc loss: 0.010677721120251848, policy loss: 5.181633114783416
Experience 7, Iter 31, disc loss: 0.011191847674629619, policy loss: 5.061048191601533
Experience 7, Iter 32, disc loss: 0.011131921954096156, policy loss: 5.119581301160489
Experience 7, Iter 33, disc loss: 0.010319336878722893, policy loss: 5.2314456253394175
Experience 7, Iter 34, disc loss: 0.010736619250390529, policy loss: 5.176582928031533
Experience 7, Iter 35, disc loss: 0.009982188727405295, policy loss: 5.279788000035831
Experience 7, Iter 36, disc loss: 0.010036332078685883, policy loss: 5.26746124033615
Experience 7, Iter 37, disc loss: 0.010747475717795952, policy loss: 5.161883433621635
Experience 7, Iter 38, disc loss: 0.011330325242626876, policy loss: 5.060509201650136
Experience 7, Iter 39, disc loss: 0.010545295046089313, policy loss: 5.170266144336631
Experience 7, Iter 40, disc loss: 0.009750848097976089, policy loss: 5.221401436727582
Experience 7, Iter 41, disc loss: 0.009941099527929596, policy loss: 5.254426188268424
Experience 7, Iter 42, disc loss: 0.009429743829371576, policy loss: 5.334048239863384
Experience 7, Iter 43, disc loss: 0.010103971824833732, policy loss: 5.191354073707623
Experience 7, Iter 44, disc loss: 0.009299677070830858, policy loss: 5.315568583659431
Experience 7, Iter 45, disc loss: 0.010046657262181433, policy loss: 5.219973602579293
Experience 7, Iter 46, disc loss: 0.009819325586364565, policy loss: 5.204797460339129
Experience 7, Iter 47, disc loss: 0.009259849543534239, policy loss: 5.38126779323397
Experience 7, Iter 48, disc loss: 0.009544171593844561, policy loss: 5.254560130596433
Experience 7, Iter 49, disc loss: 0.009434201420192564, policy loss: 5.290966031400387
Experience 7, Iter 50, disc loss: 0.010094677289386062, policy loss: 5.17260942319983
Experience 7, Iter 51, disc loss: 0.009327542795860328, policy loss: 5.281557184657724
Experience 7, Iter 52, disc loss: 0.009041707911446505, policy loss: 5.304467584116594
Experience 7, Iter 53, disc loss: 0.009167260615058886, policy loss: 5.280024491691434
Experience 7, Iter 54, disc loss: 0.00904834925301905, policy loss: 5.323168177204947
Experience 7, Iter 55, disc loss: 0.008452905400292931, policy loss: 5.404172028241955
Experience 7, Iter 56, disc loss: 0.008910781298571493, policy loss: 5.276679625427595
Experience 7, Iter 57, disc loss: 0.008932204529807428, policy loss: 5.281548295871399
Experience 7, Iter 58, disc loss: 0.009017800695160299, policy loss: 5.239115923710508
Experience 7, Iter 59, disc loss: 0.0085235247702155, policy loss: 5.336060185845618
Experience 7, Iter 60, disc loss: 0.008728923583006941, policy loss: 5.343006938832691
Experience 7, Iter 61, disc loss: 0.008231644258275665, policy loss: 5.396821268332749
Experience 7, Iter 62, disc loss: 0.00826844564932255, policy loss: 5.362747062394661
Experience 7, Iter 63, disc loss: 0.009234526021003265, policy loss: 5.25381620297741
Experience 7, Iter 64, disc loss: 0.008055009648658403, policy loss: 5.429336022759121
Experience 7, Iter 65, disc loss: 0.009118333647294094, policy loss: 5.216150541589823
Experience 7, Iter 66, disc loss: 0.008388030357677436, policy loss: 5.318875839228368
Experience 7, Iter 67, disc loss: 0.00825938269505844, policy loss: 5.35692186197844
Experience 7, Iter 68, disc loss: 0.008144242793706286, policy loss: 5.362285060984919
Experience 7, Iter 69, disc loss: 0.008225397031262814, policy loss: 5.326113411838456
Experience 7, Iter 70, disc loss: 0.008225149590780897, policy loss: 5.396570866023499
Experience 7, Iter 71, disc loss: 0.0083636800504901, policy loss: 5.308276728544573
Experience 7, Iter 72, disc loss: 0.00828463186715264, policy loss: 5.3154709314923805
Experience 7, Iter 73, disc loss: 0.007983423235839943, policy loss: 5.357078701295134
Experience 7, Iter 74, disc loss: 0.008267276491573985, policy loss: 5.34566925820433
Experience 7, Iter 75, disc loss: 0.007733162071395672, policy loss: 5.421593403736303
Experience 7, Iter 76, disc loss: 0.00800852070680199, policy loss: 5.3570675315249385
Experience 7, Iter 77, disc loss: 0.006901729440143734, policy loss: 5.594250900740743
Experience 7, Iter 78, disc loss: 0.00790939665882279, policy loss: 5.45455984397492
Experience 7, Iter 79, disc loss: 0.00767564092705633, policy loss: 5.456771957328538
Experience 7, Iter 80, disc loss: 0.00755194561423885, policy loss: 5.471921839262476
Experience 7, Iter 81, disc loss: 0.006912613914436412, policy loss: 5.528954951744277
Experience 7, Iter 82, disc loss: 0.007862880321255705, policy loss: 5.382762578875203
Experience 7, Iter 83, disc loss: 0.006491095211705303, policy loss: 5.6089227717935
Experience 7, Iter 84, disc loss: 0.00714480776322995, policy loss: 5.485010365957546
Experience 7, Iter 85, disc loss: 0.00719356374847839, policy loss: 5.472375531789852
Experience 7, Iter 86, disc loss: 0.00757515010849609, policy loss: 5.367425178474728
Experience 7, Iter 87, disc loss: 0.007318262249984417, policy loss: 5.416032088508873
Experience 7, Iter 88, disc loss: 0.00742984207287793, policy loss: 5.381069235751734
Experience 7, Iter 89, disc loss: 0.007029305882095865, policy loss: 5.52581816613153
Experience 7, Iter 90, disc loss: 0.006979112999184724, policy loss: 5.451755409073102
Experience 7, Iter 91, disc loss: 0.00753614340688635, policy loss: 5.359633371719376
Experience 7, Iter 92, disc loss: 0.0070417581609318065, policy loss: 5.4505667477739825
Experience 7, Iter 93, disc loss: 0.006693257527252366, policy loss: 5.545298367976573
Experience 7, Iter 94, disc loss: 0.00688976877892968, policy loss: 5.5439948605013765
Experience 7, Iter 95, disc loss: 0.00684104389739789, policy loss: 5.529888250195851
Experience 7, Iter 96, disc loss: 0.006520213738911576, policy loss: 5.543768108692839
Experience 7, Iter 97, disc loss: 0.006920570587812275, policy loss: 5.49309757234835
Experience 7, Iter 98, disc loss: 0.0068738566199973246, policy loss: 5.4364006643039575
Experience 7, Iter 99, disc loss: 0.006344579414984255, policy loss: 5.58255345077086
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0308],
        [0.5071],
        [0.0073]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0041, 0.0270, 0.3413, 0.0099, 0.0009, 0.6576]],

        [[0.0041, 0.0270, 0.3413, 0.0099, 0.0009, 0.6576]],

        [[0.0041, 0.0270, 0.3413, 0.0099, 0.0009, 0.6576]],

        [[0.0041, 0.0270, 0.3413, 0.0099, 0.0009, 0.6576]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.1232, 2.0285, 0.0292], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.1232, 2.0285, 0.0292])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.341
Iter 2/2000 - Loss: 1.627
Iter 3/2000 - Loss: 0.361
Iter 4/2000 - Loss: 0.537
Iter 5/2000 - Loss: 0.965
Iter 6/2000 - Loss: 0.789
Iter 7/2000 - Loss: 0.444
Iter 8/2000 - Loss: 0.299
Iter 9/2000 - Loss: 0.364
Iter 10/2000 - Loss: 0.477
Iter 11/2000 - Loss: 0.512
Iter 12/2000 - Loss: 0.445
Iter 13/2000 - Loss: 0.327
Iter 14/2000 - Loss: 0.215
Iter 15/2000 - Loss: 0.146
Iter 16/2000 - Loss: 0.119
Iter 17/2000 - Loss: 0.109
Iter 18/2000 - Loss: 0.077
Iter 19/2000 - Loss: -0.001
Iter 20/2000 - Loss: -0.122
Iter 1981/2000 - Loss: -7.648
Iter 1982/2000 - Loss: -7.648
Iter 1983/2000 - Loss: -7.649
Iter 1984/2000 - Loss: -7.649
Iter 1985/2000 - Loss: -7.649
Iter 1986/2000 - Loss: -7.649
Iter 1987/2000 - Loss: -7.649
Iter 1988/2000 - Loss: -7.649
Iter 1989/2000 - Loss: -7.649
Iter 1990/2000 - Loss: -7.649
Iter 1991/2000 - Loss: -7.649
Iter 1992/2000 - Loss: -7.649
Iter 1993/2000 - Loss: -7.649
Iter 1994/2000 - Loss: -7.649
Iter 1995/2000 - Loss: -7.649
Iter 1996/2000 - Loss: -7.649
Iter 1997/2000 - Loss: -7.649
Iter 1998/2000 - Loss: -7.649
Iter 1999/2000 - Loss: -7.649
Iter 2000/2000 - Loss: -7.649
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[14.2589,  3.3068, 50.3396,  6.2494, 11.3831, 38.3073]],

        [[16.9465, 23.0323, 21.5509,  1.0332,  6.3947, 12.8983]],

        [[12.5542, 26.7758, 13.7586,  0.9831,  6.7841, 17.2674]],

        [[ 3.2081, 19.0612, 16.2630,  4.9043, 13.9847, 42.9215]]])
Signal Variance: tensor([ 0.0336,  1.1416, 13.2327,  0.5042])
Estimated target variance: tensor([0.0029, 0.1232, 2.0285, 0.0292])
N: 80
Signal to noise ratio: tensor([ 8.9102, 48.4243, 76.1498, 51.3665])
Bound on condition number: tensor([  6352.3051, 187594.0171, 463904.0045, 211082.5741])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.006560680436227005, policy loss: 5.528710224011492
Experience 8, Iter 1, disc loss: 0.006477946920724731, policy loss: 5.525080756378074
Experience 8, Iter 2, disc loss: 0.006575641068543695, policy loss: 5.470490217329735
Experience 8, Iter 3, disc loss: 0.006547710693270968, policy loss: 5.481572185968977
Experience 8, Iter 4, disc loss: 0.006905041002547681, policy loss: 5.385141686919043
Experience 8, Iter 5, disc loss: 0.006682771270768845, policy loss: 5.439247951765334
Experience 8, Iter 6, disc loss: 0.006507730822454715, policy loss: 5.491692339430534
Experience 8, Iter 7, disc loss: 0.006617599086791853, policy loss: 5.431345878086708
Experience 8, Iter 8, disc loss: 0.006524141087577425, policy loss: 5.447512071837933
Experience 8, Iter 9, disc loss: 0.006844539005218809, policy loss: 5.4109711981853215
Experience 8, Iter 10, disc loss: 0.0066797913812182115, policy loss: 5.4996185709000684
Experience 8, Iter 11, disc loss: 0.006494149712700862, policy loss: 5.457190570546377
Experience 8, Iter 12, disc loss: 0.0067388828690825965, policy loss: 5.389547136720111
Experience 8, Iter 13, disc loss: 0.007101414791970336, policy loss: 5.317732019771547
Experience 8, Iter 14, disc loss: 0.006824405642188627, policy loss: 5.3638546679725785
Experience 8, Iter 15, disc loss: 0.006477231699520642, policy loss: 5.417127323927554
Experience 8, Iter 16, disc loss: 0.006729829231494004, policy loss: 5.355414333050186
Experience 8, Iter 17, disc loss: 0.006145692587447368, policy loss: 5.597022760462395
Experience 8, Iter 18, disc loss: 0.00649421086914573, policy loss: 5.413642885657033
Experience 8, Iter 19, disc loss: 0.0065889043810199634, policy loss: 5.400352735797712
Experience 8, Iter 20, disc loss: 0.006248371878647248, policy loss: 5.453828172493469
Experience 8, Iter 21, disc loss: 0.006538979025872051, policy loss: 5.394050324861221
Experience 8, Iter 22, disc loss: 0.0063777337658804546, policy loss: 5.410798519473881
Experience 8, Iter 23, disc loss: 0.006418095987560405, policy loss: 5.410272162638629
Experience 8, Iter 24, disc loss: 0.00669790073167076, policy loss: 5.330834534979164
Experience 8, Iter 25, disc loss: 0.0063346971788918235, policy loss: 5.426587120512593
Experience 8, Iter 26, disc loss: 0.006573643671032388, policy loss: 5.347196316548566
Experience 8, Iter 27, disc loss: 0.006119363877575999, policy loss: 5.4471201217266945
Experience 8, Iter 28, disc loss: 0.006253227229779633, policy loss: 5.432276051524908
Experience 8, Iter 29, disc loss: 0.0064433564057003925, policy loss: 5.353512358385419
Experience 8, Iter 30, disc loss: 0.006535980746108349, policy loss: 5.365752854464604
Experience 8, Iter 31, disc loss: 0.006549288078683411, policy loss: 5.328889567112137
Experience 8, Iter 32, disc loss: 0.0063665730957648235, policy loss: 5.400551224305344
Experience 8, Iter 33, disc loss: 0.006211504389050533, policy loss: 5.399375573526977
Experience 8, Iter 34, disc loss: 0.0066362852992983565, policy loss: 5.295061099438493
Experience 8, Iter 35, disc loss: 0.005843362218910428, policy loss: 5.559758842145358
Experience 8, Iter 36, disc loss: 0.006342197548676217, policy loss: 5.348044273324878
Experience 8, Iter 37, disc loss: 0.006533830048278002, policy loss: 5.342455404051268
Experience 8, Iter 38, disc loss: 0.006078395058468694, policy loss: 5.4038397728126455
Experience 8, Iter 39, disc loss: 0.006498842967884963, policy loss: 5.320293640490908
Experience 8, Iter 40, disc loss: 0.0063307488513592814, policy loss: 5.347121583835721
Experience 8, Iter 41, disc loss: 0.006459494675079953, policy loss: 5.341560517391395
Experience 8, Iter 42, disc loss: 0.0064606036348272364, policy loss: 5.323474047246304
Experience 8, Iter 43, disc loss: 0.006310714488434925, policy loss: 5.336912231677228
Experience 8, Iter 44, disc loss: 0.006443281178869793, policy loss: 5.31994316417084
Experience 8, Iter 45, disc loss: 0.006083509070066221, policy loss: 5.404612796609051
Experience 8, Iter 46, disc loss: 0.006800452177748571, policy loss: 5.245524228933734
Experience 8, Iter 47, disc loss: 0.006495386245008139, policy loss: 5.294065758921311
Experience 8, Iter 48, disc loss: 0.006091376176550933, policy loss: 5.376252674628529
Experience 8, Iter 49, disc loss: 0.006599577066882522, policy loss: 5.254303657515196
Experience 8, Iter 50, disc loss: 0.006825362278307681, policy loss: 5.196969640531416
Experience 8, Iter 51, disc loss: 0.00689798165723379, policy loss: 5.212910643952355
Experience 8, Iter 52, disc loss: 0.006803382309472538, policy loss: 5.219552946695488
Experience 8, Iter 53, disc loss: 0.006028457319255784, policy loss: 5.38402827584503
Experience 8, Iter 54, disc loss: 0.00658280424442683, policy loss: 5.257077908662118
Experience 8, Iter 55, disc loss: 0.006466202017305758, policy loss: 5.262573468014158
Experience 8, Iter 56, disc loss: 0.0068529133492796716, policy loss: 5.218514279312617
Experience 8, Iter 57, disc loss: 0.0067212100499530305, policy loss: 5.219797428152992
Experience 8, Iter 58, disc loss: 0.0063154910370860524, policy loss: 5.2954601727484025
Experience 8, Iter 59, disc loss: 0.006564835547111547, policy loss: 5.2544234559395875
Experience 8, Iter 60, disc loss: 0.006765467609530497, policy loss: 5.199786134547935
Experience 8, Iter 61, disc loss: 0.006831585950374897, policy loss: 5.197953957176067
Experience 8, Iter 62, disc loss: 0.006781085238227234, policy loss: 5.220134046677535
Experience 8, Iter 63, disc loss: 0.006158270113996327, policy loss: 5.3309426270002875
Experience 8, Iter 64, disc loss: 0.006485152110046009, policy loss: 5.262739018693449
Experience 8, Iter 65, disc loss: 0.00636077179159418, policy loss: 5.28675372370418
Experience 8, Iter 66, disc loss: 0.006521056552403059, policy loss: 5.22474155428092
Experience 8, Iter 67, disc loss: 0.006342785748210402, policy loss: 5.264662571668176
Experience 8, Iter 68, disc loss: 0.0068680366443196065, policy loss: 5.174647060817074
Experience 8, Iter 69, disc loss: 0.006535462244314027, policy loss: 5.225051004461866
Experience 8, Iter 70, disc loss: 0.006538412804860826, policy loss: 5.230765838071348
Experience 8, Iter 71, disc loss: 0.006433415797553627, policy loss: 5.249067008520851
Experience 8, Iter 72, disc loss: 0.006492857346072176, policy loss: 5.245743910109115
Experience 8, Iter 73, disc loss: 0.006307578536913185, policy loss: 5.279913723301779
Experience 8, Iter 74, disc loss: 0.006480408917891023, policy loss: 5.241801348747959
Experience 8, Iter 75, disc loss: 0.0061278086009043595, policy loss: 5.291399530194033
Experience 8, Iter 76, disc loss: 0.006191883648097991, policy loss: 5.295929208820012
Experience 8, Iter 77, disc loss: 0.006701189098569607, policy loss: 5.179731092746994
Experience 8, Iter 78, disc loss: 0.006608041954125567, policy loss: 5.219236842399402
Experience 8, Iter 79, disc loss: 0.006496808281212789, policy loss: 5.221603754329062
Experience 8, Iter 80, disc loss: 0.006583359802701091, policy loss: 5.196399884010749
Experience 8, Iter 81, disc loss: 0.0063878436262335535, policy loss: 5.241090004952272
Experience 8, Iter 82, disc loss: 0.006004640369946871, policy loss: 5.316072077718205
Experience 8, Iter 83, disc loss: 0.006460049435724266, policy loss: 5.220718911547243
Experience 8, Iter 84, disc loss: 0.006310083475670707, policy loss: 5.248829072515528
Experience 8, Iter 85, disc loss: 0.006499398822069906, policy loss: 5.20679342988885
Experience 8, Iter 86, disc loss: 0.006122396040470364, policy loss: 5.285337559715119
Experience 8, Iter 87, disc loss: 0.0059215227816976235, policy loss: 5.336762844430763
Experience 8, Iter 88, disc loss: 0.0061645759133673785, policy loss: 5.28165535013135
Experience 8, Iter 89, disc loss: 0.005938292011189154, policy loss: 5.3172449603283445
Experience 8, Iter 90, disc loss: 0.005346460296531464, policy loss: 5.456308606828243
Experience 8, Iter 91, disc loss: 0.005265819278990352, policy loss: 5.483548038999837
Experience 8, Iter 92, disc loss: 0.006283426195513116, policy loss: 5.238523513489836
Experience 8, Iter 93, disc loss: 0.005911825008690403, policy loss: 5.301694533328762
Experience 8, Iter 94, disc loss: 0.0055494093810847765, policy loss: 5.4116395970403275
Experience 8, Iter 95, disc loss: 0.005294352202668972, policy loss: 5.455581976926325
Experience 8, Iter 96, disc loss: 0.0064788863800106095, policy loss: 5.216338739950686
Experience 8, Iter 97, disc loss: 0.005355619000813917, policy loss: 5.490404578223816
Experience 8, Iter 98, disc loss: 0.005107067607403239, policy loss: 5.5316041730630205
Experience 8, Iter 99, disc loss: 0.00492216314728071, policy loss: 5.539067273718828
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0355],
        [0.5442],
        [0.0070]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0037, 0.0281, 0.3340, 0.0096, 0.0008, 0.7346]],

        [[0.0037, 0.0281, 0.3340, 0.0096, 0.0008, 0.7346]],

        [[0.0037, 0.0281, 0.3340, 0.0096, 0.0008, 0.7346]],

        [[0.0037, 0.0281, 0.3340, 0.0096, 0.0008, 0.7346]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0030, 0.1420, 2.1769, 0.0280], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0030, 0.1420, 2.1769, 0.0280])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.451
Iter 2/2000 - Loss: 1.632
Iter 3/2000 - Loss: 0.462
Iter 4/2000 - Loss: 0.624
Iter 5/2000 - Loss: 1.018
Iter 6/2000 - Loss: 0.843
Iter 7/2000 - Loss: 0.523
Iter 8/2000 - Loss: 0.406
Iter 9/2000 - Loss: 0.486
Iter 10/2000 - Loss: 0.596
Iter 11/2000 - Loss: 0.614
Iter 12/2000 - Loss: 0.536
Iter 13/2000 - Loss: 0.423
Iter 14/2000 - Loss: 0.331
Iter 15/2000 - Loss: 0.285
Iter 16/2000 - Loss: 0.277
Iter 17/2000 - Loss: 0.271
Iter 18/2000 - Loss: 0.232
Iter 19/2000 - Loss: 0.146
Iter 20/2000 - Loss: 0.026
Iter 1981/2000 - Loss: -7.795
Iter 1982/2000 - Loss: -7.795
Iter 1983/2000 - Loss: -7.795
Iter 1984/2000 - Loss: -7.795
Iter 1985/2000 - Loss: -7.796
Iter 1986/2000 - Loss: -7.796
Iter 1987/2000 - Loss: -7.796
Iter 1988/2000 - Loss: -7.796
Iter 1989/2000 - Loss: -7.796
Iter 1990/2000 - Loss: -7.796
Iter 1991/2000 - Loss: -7.796
Iter 1992/2000 - Loss: -7.796
Iter 1993/2000 - Loss: -7.796
Iter 1994/2000 - Loss: -7.796
Iter 1995/2000 - Loss: -7.796
Iter 1996/2000 - Loss: -7.796
Iter 1997/2000 - Loss: -7.796
Iter 1998/2000 - Loss: -7.796
Iter 1999/2000 - Loss: -7.796
Iter 2000/2000 - Loss: -7.796
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[14.1380,  3.6515, 51.5316,  6.0990, 12.5091, 36.3626]],

        [[16.3459, 23.8652, 22.7222,  1.0926,  6.0052, 14.3495]],

        [[14.4198, 28.8761, 12.9024,  0.9941,  6.9298, 19.0674]],

        [[ 3.1787, 21.6827, 18.9726,  5.4956, 11.2108, 41.1844]]])
Signal Variance: tensor([ 0.0425,  1.3621, 16.4163,  0.7116])
Estimated target variance: tensor([0.0030, 0.1420, 2.1769, 0.0280])
N: 90
Signal to noise ratio: tensor([10.0112, 53.6901, 89.2051, 61.7228])
Bound on condition number: tensor([  9021.1745, 259437.3983, 716179.7603, 342874.0995])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0058818177072184775, policy loss: 5.307707026698402
Experience 9, Iter 1, disc loss: 0.006186160942095719, policy loss: 5.248015812154611
Experience 9, Iter 2, disc loss: 0.004988188907108393, policy loss: 5.545114093820241
Experience 9, Iter 3, disc loss: 0.004723812664625169, policy loss: 5.593633364347651
Experience 9, Iter 4, disc loss: 0.004859557166704158, policy loss: 5.559935850198757
Experience 9, Iter 5, disc loss: 0.005956103138203845, policy loss: 5.282732342378445
Experience 9, Iter 6, disc loss: 0.005353867955874103, policy loss: 5.459384171640547
Experience 9, Iter 7, disc loss: 0.0053193135014160935, policy loss: 5.462395937557907
Experience 9, Iter 8, disc loss: 0.00569791468530835, policy loss: 5.353187571228338
Experience 9, Iter 9, disc loss: 0.005609265711939277, policy loss: 5.356416840658087
Experience 9, Iter 10, disc loss: 0.006190346218143841, policy loss: 5.220684991867826
Experience 9, Iter 11, disc loss: 0.005734320010768748, policy loss: 5.31711695325324
Experience 9, Iter 12, disc loss: 0.005487467363676973, policy loss: 5.387725959219932
Experience 9, Iter 13, disc loss: 0.005625755100647102, policy loss: 5.3710589342548465
Experience 9, Iter 14, disc loss: 0.005488751948846694, policy loss: 5.392526262444852
Experience 9, Iter 15, disc loss: 0.0055810209689251475, policy loss: 5.380204569700908
Experience 9, Iter 16, disc loss: 0.005471484480431641, policy loss: 5.399488023153706
Experience 9, Iter 17, disc loss: 0.005348924156348156, policy loss: 5.424909087540218
Experience 9, Iter 18, disc loss: 0.005430552646316981, policy loss: 5.397783387877557
Experience 9, Iter 19, disc loss: 0.005142229119212633, policy loss: 5.478113982542645
Experience 9, Iter 20, disc loss: 0.005396748317035843, policy loss: 5.427233982691502
Experience 9, Iter 21, disc loss: 0.0057175210374204, policy loss: 5.31597970520785
Experience 9, Iter 22, disc loss: 0.005780349734844885, policy loss: 5.3069720987733255
Experience 9, Iter 23, disc loss: 0.00525602908934189, policy loss: 5.424866794363252
Experience 9, Iter 24, disc loss: 0.004955982005911562, policy loss: 5.48851152169084
Experience 9, Iter 25, disc loss: 0.005491241944155652, policy loss: 5.350954298721467
Experience 9, Iter 26, disc loss: 0.005271606749078732, policy loss: 5.4015258169975775
Experience 9, Iter 27, disc loss: 0.005281626898505237, policy loss: 5.421660738697129
Experience 9, Iter 28, disc loss: 0.004574342536230297, policy loss: 5.620595681852346
Experience 9, Iter 29, disc loss: 0.005154968907572175, policy loss: 5.453905399614834
Experience 9, Iter 30, disc loss: 0.005429120918900721, policy loss: 5.386440351126748
Experience 9, Iter 31, disc loss: 0.005085000586761166, policy loss: 5.44779251803253
Experience 9, Iter 32, disc loss: 0.004794775827174472, policy loss: 5.522094616849653
Experience 9, Iter 33, disc loss: 0.004695532269097299, policy loss: 5.553773415737319
Experience 9, Iter 34, disc loss: 0.005133211254009051, policy loss: 5.427727257246824
Experience 9, Iter 35, disc loss: 0.005197625320550172, policy loss: 5.42923687631712
Experience 9, Iter 36, disc loss: 0.004701930988186872, policy loss: 5.569963919262109
Experience 9, Iter 37, disc loss: 0.0046250687187210805, policy loss: 5.598095472737109
Experience 9, Iter 38, disc loss: 0.005132722994429958, policy loss: 5.46435291518874
Experience 9, Iter 39, disc loss: 0.0051069855973659845, policy loss: 5.437152007766215
Experience 9, Iter 40, disc loss: 0.004583678293854838, policy loss: 5.622399264380379
Experience 9, Iter 41, disc loss: 0.004438422806759071, policy loss: 5.628948932488886
Experience 9, Iter 42, disc loss: 0.005110695036514308, policy loss: 5.436593247584648
Experience 9, Iter 43, disc loss: 0.005387904458589611, policy loss: 5.370020611033624
Experience 9, Iter 44, disc loss: 0.00504757431193967, policy loss: 5.448056027152254
Experience 9, Iter 45, disc loss: 0.005213544481160672, policy loss: 5.408401909025505
Experience 9, Iter 46, disc loss: 0.005072921969418278, policy loss: 5.456733154265928
Experience 9, Iter 47, disc loss: 0.005087539131877417, policy loss: 5.427931014819238
Experience 9, Iter 48, disc loss: 0.004827904290148959, policy loss: 5.518221191422617
Experience 9, Iter 49, disc loss: 0.004757874062950154, policy loss: 5.511501278854043
Experience 9, Iter 50, disc loss: 0.0049178667999616454, policy loss: 5.475139864069711
Experience 9, Iter 51, disc loss: 0.004664258543853781, policy loss: 5.5292554575690005
Experience 9, Iter 52, disc loss: 0.0045915077906606965, policy loss: 5.545791305040735
Experience 9, Iter 53, disc loss: 0.004677740506258594, policy loss: 5.558733511686925
Experience 9, Iter 54, disc loss: 0.005176755203739934, policy loss: 5.40948850716062
Experience 9, Iter 55, disc loss: 0.004539907769721074, policy loss: 5.57330493251025
Experience 9, Iter 56, disc loss: 0.004411871031262921, policy loss: 5.616452662130269
Experience 9, Iter 57, disc loss: 0.00453573009240817, policy loss: 5.5560102611058095
Experience 9, Iter 58, disc loss: 0.004788501482770026, policy loss: 5.505059330913347
Experience 9, Iter 59, disc loss: 0.004927295689797426, policy loss: 5.472324277703774
Experience 9, Iter 60, disc loss: 0.004523146298285202, policy loss: 5.571760438897188
Experience 9, Iter 61, disc loss: 0.004579855919423773, policy loss: 5.565380627782475
Experience 9, Iter 62, disc loss: 0.004799907500524756, policy loss: 5.535298813522755
Experience 9, Iter 63, disc loss: 0.004800410039391932, policy loss: 5.484667443077593
Experience 9, Iter 64, disc loss: 0.004229510856612777, policy loss: 5.663846502713525
Experience 9, Iter 65, disc loss: 0.0038635532586949197, policy loss: 5.7828019175915255
Experience 9, Iter 66, disc loss: 0.0036545692860197186, policy loss: 5.833325053309846
Experience 9, Iter 67, disc loss: 0.0040650928462643765, policy loss: 5.693779928341932
Experience 9, Iter 68, disc loss: 0.004397379921619048, policy loss: 5.608736612035381
Experience 9, Iter 69, disc loss: 0.004521092477609751, policy loss: 5.563924705503911
Experience 9, Iter 70, disc loss: 0.004415947554246994, policy loss: 5.6194893697957875
Experience 9, Iter 71, disc loss: 0.004027784827892769, policy loss: 5.718257937178008
Experience 9, Iter 72, disc loss: 0.00437924281988615, policy loss: 5.596185046122425
Experience 9, Iter 73, disc loss: 0.004646647274944173, policy loss: 5.528792729565518
Experience 9, Iter 74, disc loss: 0.004580738645090777, policy loss: 5.571079560782193
Experience 9, Iter 75, disc loss: 0.004119846730209873, policy loss: 5.674270412647246
Experience 9, Iter 76, disc loss: 0.004533981049353199, policy loss: 5.5507183733875625
Experience 9, Iter 77, disc loss: 0.00472826931137726, policy loss: 5.4894441947097565
Experience 9, Iter 78, disc loss: 0.004709291434444163, policy loss: 5.486692956234979
Experience 9, Iter 79, disc loss: 0.004401654739658204, policy loss: 5.56743311704764
Experience 9, Iter 80, disc loss: 0.00439320561648488, policy loss: 5.606746105144225
Experience 9, Iter 81, disc loss: 0.004791670339420019, policy loss: 5.477799070292031
Experience 9, Iter 82, disc loss: 0.00442195756959561, policy loss: 5.5660916816277
Experience 9, Iter 83, disc loss: 0.004561814870653961, policy loss: 5.529262102341371
Experience 9, Iter 84, disc loss: 0.004567976180008508, policy loss: 5.530589605396579
Experience 9, Iter 85, disc loss: 0.0042744024327770426, policy loss: 5.6073378169424615
Experience 9, Iter 86, disc loss: 0.004211287152852864, policy loss: 5.619966001101652
Experience 9, Iter 87, disc loss: 0.004662699415205343, policy loss: 5.528883104715557
Experience 9, Iter 88, disc loss: 0.004291269288738254, policy loss: 5.607701464407934
Experience 9, Iter 89, disc loss: 0.004409972195341902, policy loss: 5.569266173335868
Experience 9, Iter 90, disc loss: 0.004352149213074897, policy loss: 5.579898308792993
Experience 9, Iter 91, disc loss: 0.004404561373087367, policy loss: 5.566566178457971
Experience 9, Iter 92, disc loss: 0.0041045709595977, policy loss: 5.658619789996335
Experience 9, Iter 93, disc loss: 0.0043788756438582765, policy loss: 5.586954964785296
Experience 9, Iter 94, disc loss: 0.004374968247972551, policy loss: 5.58064435182982
Experience 9, Iter 95, disc loss: 0.004157029732427635, policy loss: 5.649990497644029
Experience 9, Iter 96, disc loss: 0.004245262853857873, policy loss: 5.611375864435907
Experience 9, Iter 97, disc loss: 0.004443047232871121, policy loss: 5.54925678955431
Experience 9, Iter 98, disc loss: 0.004260623040953208, policy loss: 5.601063196086127
Experience 9, Iter 99, disc loss: 0.0041821144379136695, policy loss: 5.616909047720105
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0329],
        [0.5123],
        [0.0069]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0034, 0.0260, 0.3249, 0.0092, 0.0007, 0.6815]],

        [[0.0034, 0.0260, 0.3249, 0.0092, 0.0007, 0.6815]],

        [[0.0034, 0.0260, 0.3249, 0.0092, 0.0007, 0.6815]],

        [[0.0034, 0.0260, 0.3249, 0.0092, 0.0007, 0.6815]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0028, 0.1315, 2.0490, 0.0274], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0028, 0.1315, 2.0490, 0.0274])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.344
Iter 2/2000 - Loss: 1.619
Iter 3/2000 - Loss: 0.361
Iter 4/2000 - Loss: 0.535
Iter 5/2000 - Loss: 0.962
Iter 6/2000 - Loss: 0.778
Iter 7/2000 - Loss: 0.435
Iter 8/2000 - Loss: 0.308
Iter 9/2000 - Loss: 0.395
Iter 10/2000 - Loss: 0.519
Iter 11/2000 - Loss: 0.547
Iter 12/2000 - Loss: 0.472
Iter 13/2000 - Loss: 0.358
Iter 14/2000 - Loss: 0.265
Iter 15/2000 - Loss: 0.223
Iter 16/2000 - Loss: 0.224
Iter 17/2000 - Loss: 0.231
Iter 18/2000 - Loss: 0.205
Iter 19/2000 - Loss: 0.128
Iter 20/2000 - Loss: 0.016
Iter 1981/2000 - Loss: -7.964
Iter 1982/2000 - Loss: -7.964
Iter 1983/2000 - Loss: -7.964
Iter 1984/2000 - Loss: -7.964
Iter 1985/2000 - Loss: -7.964
Iter 1986/2000 - Loss: -7.964
Iter 1987/2000 - Loss: -7.964
Iter 1988/2000 - Loss: -7.964
Iter 1989/2000 - Loss: -7.964
Iter 1990/2000 - Loss: -7.964
Iter 1991/2000 - Loss: -7.964
Iter 1992/2000 - Loss: -7.964
Iter 1993/2000 - Loss: -7.964
Iter 1994/2000 - Loss: -7.964
Iter 1995/2000 - Loss: -7.964
Iter 1996/2000 - Loss: -7.964
Iter 1997/2000 - Loss: -7.964
Iter 1998/2000 - Loss: -7.964
Iter 1999/2000 - Loss: -7.964
Iter 2000/2000 - Loss: -7.964
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[13.1817,  3.5426, 50.5467,  5.4687, 11.7749, 35.0540]],

        [[16.2176, 25.3179, 16.1238,  0.9848,  5.4312, 19.8305]],

        [[13.1918, 29.1603, 13.0582,  1.0210,  6.9082, 17.3101]],

        [[10.9645, 20.5012, 19.6748,  5.3433,  1.3463, 43.0902]]])
Signal Variance: tensor([ 0.0406,  1.5719, 16.5338,  0.6759])
Estimated target variance: tensor([0.0028, 0.1315, 2.0490, 0.0274])
N: 100
Signal to noise ratio: tensor([ 9.8470, 58.6750, 90.4040, 62.8519])
Bound on condition number: tensor([  9697.4091, 344277.0522, 817289.5010, 395037.1502])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.004049250672856302, policy loss: 5.670833836105788
Experience 10, Iter 1, disc loss: 0.003838921456450263, policy loss: 5.722843230241046
Experience 10, Iter 2, disc loss: 0.00434981137911469, policy loss: 5.56847134677086
Experience 10, Iter 3, disc loss: 0.004144562189224616, policy loss: 5.638850741565027
Experience 10, Iter 4, disc loss: 0.004146366324944461, policy loss: 5.637572925704397
Experience 10, Iter 5, disc loss: 0.004119744901500774, policy loss: 5.636158200762261
Experience 10, Iter 6, disc loss: 0.004078748450427094, policy loss: 5.679040147325328
Experience 10, Iter 7, disc loss: 0.004180143068499611, policy loss: 5.635437191530555
Experience 10, Iter 8, disc loss: 0.00398626871801148, policy loss: 5.68083769063769
Experience 10, Iter 9, disc loss: 0.004088895746331793, policy loss: 5.655390276311326
Experience 10, Iter 10, disc loss: 0.0040717401061387834, policy loss: 5.662286040888994
Experience 10, Iter 11, disc loss: 0.003997900072735996, policy loss: 5.692868578008917
Experience 10, Iter 12, disc loss: 0.0037485923728727342, policy loss: 5.780884636433166
Experience 10, Iter 13, disc loss: 0.0039031943748854183, policy loss: 5.716289552953315
Experience 10, Iter 14, disc loss: 0.003861613320843887, policy loss: 5.715765738747326
Experience 10, Iter 15, disc loss: 0.003950911712640172, policy loss: 5.694212439243215
Experience 10, Iter 16, disc loss: 0.00388503677130485, policy loss: 5.710836194828877
Experience 10, Iter 17, disc loss: 0.004134736672118065, policy loss: 5.623050798841539
Experience 10, Iter 18, disc loss: 0.0037944176285487734, policy loss: 5.740030421301016
Experience 10, Iter 19, disc loss: 0.003794654305120698, policy loss: 5.732672652198433
Experience 10, Iter 20, disc loss: 0.003934093464003701, policy loss: 5.689717464071165
Experience 10, Iter 21, disc loss: 0.003640311369119514, policy loss: 5.811430551115901
Experience 10, Iter 22, disc loss: 0.00398704292186272, policy loss: 5.6805264362447705
Experience 10, Iter 23, disc loss: 0.003969072725973841, policy loss: 5.6763574811622135
Experience 10, Iter 24, disc loss: 0.0036933538280468593, policy loss: 5.760436370990443
Experience 10, Iter 25, disc loss: 0.003963062996366996, policy loss: 5.671284263634994
Experience 10, Iter 26, disc loss: 0.00410626944348393, policy loss: 5.622565645665164
Experience 10, Iter 27, disc loss: 0.0037846605161747943, policy loss: 5.734888682891761
Experience 10, Iter 28, disc loss: 0.0038011457675070095, policy loss: 5.735466374264979
Experience 10, Iter 29, disc loss: 0.0037238302332007604, policy loss: 5.747132898547443
Experience 10, Iter 30, disc loss: 0.0037802366987355848, policy loss: 5.752378285078679
Experience 10, Iter 31, disc loss: 0.003910915627468822, policy loss: 5.6948733316337385
Experience 10, Iter 32, disc loss: 0.0038151821981473755, policy loss: 5.72476712887271
Experience 10, Iter 33, disc loss: 0.0037357309965102533, policy loss: 5.730620925606181
Experience 10, Iter 34, disc loss: 0.0037923432421806532, policy loss: 5.7240819432830055
Experience 10, Iter 35, disc loss: 0.003693949489554487, policy loss: 5.7593766136773965
Experience 10, Iter 36, disc loss: 0.0037632677066378457, policy loss: 5.740870693136795
Experience 10, Iter 37, disc loss: 0.00380224889751162, policy loss: 5.7238745028045175
Experience 10, Iter 38, disc loss: 0.0036707427612949262, policy loss: 5.767668146121004
Experience 10, Iter 39, disc loss: 0.0035853138098486267, policy loss: 5.803065123016016
Experience 10, Iter 40, disc loss: 0.0036073796831640245, policy loss: 5.776140249798738
Experience 10, Iter 41, disc loss: 0.003783701702814415, policy loss: 5.730388920701728
Experience 10, Iter 42, disc loss: 0.0035588528978291365, policy loss: 5.783212865175714
Experience 10, Iter 43, disc loss: 0.003529548128316343, policy loss: 5.806749764850497
Experience 10, Iter 44, disc loss: 0.00340961014433098, policy loss: 5.839046249926139
Experience 10, Iter 45, disc loss: 0.0035810700279691383, policy loss: 5.792001014645205
Experience 10, Iter 46, disc loss: 0.0035553517073759595, policy loss: 5.788402060949568
Experience 10, Iter 47, disc loss: 0.003463244627837112, policy loss: 5.837103251956684
Experience 10, Iter 48, disc loss: 0.0035013580949206768, policy loss: 5.8359974600548625
Experience 10, Iter 49, disc loss: 0.003507152904930023, policy loss: 5.8119876788859886
Experience 10, Iter 50, disc loss: 0.003621970707636369, policy loss: 5.763250135850155
Experience 10, Iter 51, disc loss: 0.0033003064816922254, policy loss: 5.897140936225167
Experience 10, Iter 52, disc loss: 0.003348144429221181, policy loss: 5.8941876186007836
Experience 10, Iter 53, disc loss: 0.003415027831656911, policy loss: 5.8551327915408145
Experience 10, Iter 54, disc loss: 0.003712456424965205, policy loss: 5.746185618443059
Experience 10, Iter 55, disc loss: 0.003387766817204706, policy loss: 5.846971453569803
Experience 10, Iter 56, disc loss: 0.003195973569682356, policy loss: 5.918161024332775
Experience 10, Iter 57, disc loss: 0.0032012914836638956, policy loss: 5.905021956552888
Experience 10, Iter 58, disc loss: 0.003365945004485726, policy loss: 5.855889422638269
Experience 10, Iter 59, disc loss: 0.0034291396616793887, policy loss: 5.827394853369379
Experience 10, Iter 60, disc loss: 0.0035010846145547437, policy loss: 5.789576532044606
Experience 10, Iter 61, disc loss: 0.003418056994677799, policy loss: 5.8364877312087895
Experience 10, Iter 62, disc loss: 0.003099560287537031, policy loss: 5.957407104050493
Experience 10, Iter 63, disc loss: 0.003448005267144415, policy loss: 5.819417940412135
Experience 10, Iter 64, disc loss: 0.003401607425855174, policy loss: 5.836635280688645
Experience 10, Iter 65, disc loss: 0.003344866443557038, policy loss: 5.85046011933918
Experience 10, Iter 66, disc loss: 0.00345095692375539, policy loss: 5.820823403165401
Experience 10, Iter 67, disc loss: 0.0032949114448277144, policy loss: 5.873474642118746
Experience 10, Iter 68, disc loss: 0.0033412557911498704, policy loss: 5.864892257748427
Experience 10, Iter 69, disc loss: 0.0033744711497368787, policy loss: 5.845842503490215
Experience 10, Iter 70, disc loss: 0.0031541781465129638, policy loss: 5.92567517100224
Experience 10, Iter 71, disc loss: 0.0032634542800037804, policy loss: 5.883374465073413
Experience 10, Iter 72, disc loss: 0.0033895824995060627, policy loss: 5.849745093260331
Experience 10, Iter 73, disc loss: 0.0034720576674630676, policy loss: 5.81481872880927
Experience 10, Iter 74, disc loss: 0.0033369491323473758, policy loss: 5.858403734068657
Experience 10, Iter 75, disc loss: 0.003304364952426924, policy loss: 5.864871731474961
Experience 10, Iter 76, disc loss: 0.002866104062563187, policy loss: 6.035509189923117
Experience 10, Iter 77, disc loss: 0.0031925167565928624, policy loss: 5.884936297763807
Experience 10, Iter 78, disc loss: 0.0032203043538585824, policy loss: 5.888534956070838
Experience 10, Iter 79, disc loss: 0.0032212746249767645, policy loss: 5.892579247898103
Experience 10, Iter 80, disc loss: 0.00329923912410847, policy loss: 5.864447645982455
Experience 10, Iter 81, disc loss: 0.0032278137359140384, policy loss: 5.884178493021641
Experience 10, Iter 82, disc loss: 0.0031797518403918913, policy loss: 5.892850236269263
Experience 10, Iter 83, disc loss: 0.003005914167553456, policy loss: 5.993427759404629
Experience 10, Iter 84, disc loss: 0.0031000466090918594, policy loss: 5.93735084176194
Experience 10, Iter 85, disc loss: 0.003068493894150819, policy loss: 5.940105648868486
Experience 10, Iter 86, disc loss: 0.0029958215618701876, policy loss: 5.990120829511538
Experience 10, Iter 87, disc loss: 0.0028778420763079867, policy loss: 6.0360655202225075
Experience 10, Iter 88, disc loss: 0.0030960044283404663, policy loss: 5.928826260706263
Experience 10, Iter 89, disc loss: 0.0031804399149873187, policy loss: 5.903848409050004
Experience 10, Iter 90, disc loss: 0.0030851732931576415, policy loss: 5.943880097425934
Experience 10, Iter 91, disc loss: 0.0028646320029925227, policy loss: 6.028548377439647
Experience 10, Iter 92, disc loss: 0.0030750299308812947, policy loss: 5.939545402687492
Experience 10, Iter 93, disc loss: 0.00306547821025972, policy loss: 5.935284556640797
Experience 10, Iter 94, disc loss: 0.0027062796375422965, policy loss: 6.115686274007226
Experience 10, Iter 95, disc loss: 0.002826235668152364, policy loss: 6.059935199395807
Experience 10, Iter 96, disc loss: 0.0032229345797708587, policy loss: 5.911589340296677
Experience 10, Iter 97, disc loss: 0.003063128158916055, policy loss: 5.943269146967173
Experience 10, Iter 98, disc loss: 0.002836609539134554, policy loss: 6.022034332356967
Experience 10, Iter 99, disc loss: 0.0026490095318130427, policy loss: 6.120292654775762
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0306],
        [0.4741],
        [0.0064]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0032, 0.0247, 0.3021, 0.0090, 0.0007, 0.6525]],

        [[0.0032, 0.0247, 0.3021, 0.0090, 0.0007, 0.6525]],

        [[0.0032, 0.0247, 0.3021, 0.0090, 0.0007, 0.6525]],

        [[0.0032, 0.0247, 0.3021, 0.0090, 0.0007, 0.6525]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0027, 0.1222, 1.8966, 0.0256], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0027, 0.1222, 1.8966, 0.0256])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.224
Iter 2/2000 - Loss: 1.526
Iter 3/2000 - Loss: 0.238
Iter 4/2000 - Loss: 0.412
Iter 5/2000 - Loss: 0.851
Iter 6/2000 - Loss: 0.662
Iter 7/2000 - Loss: 0.309
Iter 8/2000 - Loss: 0.184
Iter 9/2000 - Loss: 0.280
Iter 10/2000 - Loss: 0.409
Iter 11/2000 - Loss: 0.437
Iter 12/2000 - Loss: 0.360
Iter 13/2000 - Loss: 0.245
Iter 14/2000 - Loss: 0.156
Iter 15/2000 - Loss: 0.122
Iter 16/2000 - Loss: 0.134
Iter 17/2000 - Loss: 0.150
Iter 18/2000 - Loss: 0.127
Iter 19/2000 - Loss: 0.050
Iter 20/2000 - Loss: -0.059
Iter 1981/2000 - Loss: -8.080
Iter 1982/2000 - Loss: -8.080
Iter 1983/2000 - Loss: -8.080
Iter 1984/2000 - Loss: -8.080
Iter 1985/2000 - Loss: -8.080
Iter 1986/2000 - Loss: -8.080
Iter 1987/2000 - Loss: -8.080
Iter 1988/2000 - Loss: -8.080
Iter 1989/2000 - Loss: -8.080
Iter 1990/2000 - Loss: -8.080
Iter 1991/2000 - Loss: -8.080
Iter 1992/2000 - Loss: -8.080
Iter 1993/2000 - Loss: -8.080
Iter 1994/2000 - Loss: -8.080
Iter 1995/2000 - Loss: -8.080
Iter 1996/2000 - Loss: -8.080
Iter 1997/2000 - Loss: -8.080
Iter 1998/2000 - Loss: -8.080
Iter 1999/2000 - Loss: -8.080
Iter 2000/2000 - Loss: -8.080
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[13.2200,  3.4970, 48.3163,  5.1636, 10.8407, 34.3776]],

        [[15.5001, 22.5159, 21.8020,  1.0246,  5.4539, 18.7698]],

        [[13.7600, 28.9323, 12.0454,  1.0401,  6.5484, 17.1453]],

        [[ 3.3713, 19.0597, 19.6618,  5.6165,  1.9683, 41.2156]]])
Signal Variance: tensor([ 0.0392,  1.5631, 16.3924,  0.7082])
Estimated target variance: tensor([0.0027, 0.1222, 1.8966, 0.0256])
N: 110
Signal to noise ratio: tensor([ 9.7506, 57.6562, 94.5601, 61.5357])
Bound on condition number: tensor([ 10459.2408, 365666.6052, 983578.9967, 416531.6746])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.002829618105068521, policy loss: 6.022760180733377
Experience 11, Iter 1, disc loss: 0.0029532097573235446, policy loss: 5.975541577357264
Experience 11, Iter 2, disc loss: 0.002602953506959151, policy loss: 6.145373390007711
Experience 11, Iter 3, disc loss: 0.002424924538496444, policy loss: 6.231879086894898
Experience 11, Iter 4, disc loss: 0.0025863051532166908, policy loss: 6.1336963412975924
Experience 11, Iter 5, disc loss: 0.00276510721425026, policy loss: 6.057099022780722
Experience 11, Iter 6, disc loss: 0.002967589322135725, policy loss: 5.962964902318589
Experience 11, Iter 7, disc loss: 0.002716268198692756, policy loss: 6.087460884418338
Experience 11, Iter 8, disc loss: 0.0027494003656121794, policy loss: 6.078726515242663
Experience 11, Iter 9, disc loss: 0.0028305172783169357, policy loss: 6.0135267717963625
Experience 11, Iter 10, disc loss: 0.002779101431587851, policy loss: 6.047990363503116
Experience 11, Iter 11, disc loss: 0.0027772480770472912, policy loss: 6.045079379186462
Experience 11, Iter 12, disc loss: 0.002855822708595221, policy loss: 6.012895499775883
Experience 11, Iter 13, disc loss: 0.0029420709290661185, policy loss: 5.984810245168159
Experience 11, Iter 14, disc loss: 0.002956106302895637, policy loss: 5.971048814136939
Experience 11, Iter 15, disc loss: 0.0027895517512580748, policy loss: 6.0373824859703795
Experience 11, Iter 16, disc loss: 0.002886539771677752, policy loss: 6.00311411403062
Experience 11, Iter 17, disc loss: 0.002832750139309323, policy loss: 6.017166850550348
Experience 11, Iter 18, disc loss: 0.002882245062962252, policy loss: 6.012232568947295
Experience 11, Iter 19, disc loss: 0.0027407330904262285, policy loss: 6.059796190623613
Experience 11, Iter 20, disc loss: 0.0027148146549063794, policy loss: 6.062168155825537
Experience 11, Iter 21, disc loss: 0.002978624744713923, policy loss: 5.951318716522847
Experience 11, Iter 22, disc loss: 0.0027136566522710393, policy loss: 6.050128916923756
Experience 11, Iter 23, disc loss: 0.002724819540276121, policy loss: 6.0463161659942815
Experience 11, Iter 24, disc loss: 0.0027197888902434956, policy loss: 6.078008918128749
Experience 11, Iter 25, disc loss: 0.002701051887912912, policy loss: 6.077723838409272
Experience 11, Iter 26, disc loss: 0.0027287589371520577, policy loss: 6.05333875235756
Experience 11, Iter 27, disc loss: 0.002600285475110797, policy loss: 6.119093604498946
Experience 11, Iter 28, disc loss: 0.0027134100472474766, policy loss: 6.079104135320072
Experience 11, Iter 29, disc loss: 0.002728478898413032, policy loss: 6.052216799115972
Experience 11, Iter 30, disc loss: 0.002563469763362821, policy loss: 6.126048505772192
Experience 11, Iter 31, disc loss: 0.002448408679068156, policy loss: 6.171086614711536
Experience 11, Iter 32, disc loss: 0.002679992489486006, policy loss: 6.077893719547043
Experience 11, Iter 33, disc loss: 0.0026870319744828927, policy loss: 6.070639700839369
Experience 11, Iter 34, disc loss: 0.0026890255219471373, policy loss: 6.085828156279338
Experience 11, Iter 35, disc loss: 0.0027199917757137013, policy loss: 6.044017136215226
Experience 11, Iter 36, disc loss: 0.0026824788582906745, policy loss: 6.073943919945826
Experience 11, Iter 37, disc loss: 0.002655255084492506, policy loss: 6.0789813976342675
Experience 11, Iter 38, disc loss: 0.002726558250692765, policy loss: 6.042752899955965
Experience 11, Iter 39, disc loss: 0.002743115668581537, policy loss: 6.044418659639635
Experience 11, Iter 40, disc loss: 0.0026033143878592705, policy loss: 6.1073043856001785
Experience 11, Iter 41, disc loss: 0.0027654979922319195, policy loss: 6.037643872737386
Experience 11, Iter 42, disc loss: 0.002731185852504037, policy loss: 6.0598317520717995
Experience 11, Iter 43, disc loss: 0.0024894734517410125, policy loss: 6.150350039325758
Experience 11, Iter 44, disc loss: 0.0027191003163242345, policy loss: 6.057020607614184
Experience 11, Iter 45, disc loss: 0.002675628996475816, policy loss: 6.068930989053065
Experience 11, Iter 46, disc loss: 0.0026742276352167017, policy loss: 6.058463606003153
Experience 11, Iter 47, disc loss: 0.0027042689719621632, policy loss: 6.075834726034135
Experience 11, Iter 48, disc loss: 0.002702029497493155, policy loss: 6.059469950726837
Experience 11, Iter 49, disc loss: 0.0025871279493206896, policy loss: 6.099339857509877
Experience 11, Iter 50, disc loss: 0.002636506029386723, policy loss: 6.09649681874335
Experience 11, Iter 51, disc loss: 0.0025099424984690484, policy loss: 6.152642623405505
Experience 11, Iter 52, disc loss: 0.0025755076957644915, policy loss: 6.10494172380994
Experience 11, Iter 53, disc loss: 0.002539330967045646, policy loss: 6.114760041137808
Experience 11, Iter 54, disc loss: 0.002580891032176514, policy loss: 6.0996429229467575
Experience 11, Iter 55, disc loss: 0.0024563873601813804, policy loss: 6.197547359331747
Experience 11, Iter 56, disc loss: 0.0025408811529528813, policy loss: 6.123676856326862
Experience 11, Iter 57, disc loss: 0.002435440922028166, policy loss: 6.18823077776483
Experience 11, Iter 58, disc loss: 0.002538294588501162, policy loss: 6.121459610964193
Experience 11, Iter 59, disc loss: 0.0024767557200068794, policy loss: 6.14969843188859
Experience 11, Iter 60, disc loss: 0.002454574691999684, policy loss: 6.160211914300712
Experience 11, Iter 61, disc loss: 0.002476976955719058, policy loss: 6.15879870226347
Experience 11, Iter 62, disc loss: 0.0023534305041489475, policy loss: 6.206111905336575
Experience 11, Iter 63, disc loss: 0.002629102976275884, policy loss: 6.082695801345765
Experience 11, Iter 64, disc loss: 0.0024288111321785254, policy loss: 6.175994298634228
Experience 11, Iter 65, disc loss: 0.0023899142329973557, policy loss: 6.1982864262758754
Experience 11, Iter 66, disc loss: 0.0024373685314546707, policy loss: 6.1749486161292815
Experience 11, Iter 67, disc loss: 0.0024447743280246333, policy loss: 6.174523161620499
Experience 11, Iter 68, disc loss: 0.0023817155608977405, policy loss: 6.195734612438914
Experience 11, Iter 69, disc loss: 0.0024115408210956943, policy loss: 6.185977512625529
Experience 11, Iter 70, disc loss: 0.0023796125353081725, policy loss: 6.199052822108567
Experience 11, Iter 71, disc loss: 0.002334565027262779, policy loss: 6.2372159025308065
Experience 11, Iter 72, disc loss: 0.0024025150875311894, policy loss: 6.182550460848316
Experience 11, Iter 73, disc loss: 0.0024634542935496246, policy loss: 6.1498006656552775
Experience 11, Iter 74, disc loss: 0.0022439712479008947, policy loss: 6.261508834532934
Experience 11, Iter 75, disc loss: 0.0023806666386148767, policy loss: 6.198273125527459
Experience 11, Iter 76, disc loss: 0.002376540260893518, policy loss: 6.220632383047589
Experience 11, Iter 77, disc loss: 0.0023716588110020828, policy loss: 6.197095817731769
Experience 11, Iter 78, disc loss: 0.0023077994066381066, policy loss: 6.22766552303197
Experience 11, Iter 79, disc loss: 0.002392829290312623, policy loss: 6.194014462441126
Experience 11, Iter 80, disc loss: 0.0022718824433766173, policy loss: 6.23644296376523
Experience 11, Iter 81, disc loss: 0.0021671567073568085, policy loss: 6.295429971888026
Experience 11, Iter 82, disc loss: 0.00226125466765854, policy loss: 6.241945637856128
Experience 11, Iter 83, disc loss: 0.0023432872290136512, policy loss: 6.221319741037693
Experience 11, Iter 84, disc loss: 0.0022677556860240446, policy loss: 6.242575075420131
Experience 11, Iter 85, disc loss: 0.0022236786891135503, policy loss: 6.253968702007324
Experience 11, Iter 86, disc loss: 0.002190105805441103, policy loss: 6.280953193089663
Experience 11, Iter 87, disc loss: 0.002235024322140832, policy loss: 6.255506033357039
Experience 11, Iter 88, disc loss: 0.0024350307324762666, policy loss: 6.156815577233086
Experience 11, Iter 89, disc loss: 0.0023116996419187625, policy loss: 6.228245439527543
Experience 11, Iter 90, disc loss: 0.0022565886022099793, policy loss: 6.253961870877539
Experience 11, Iter 91, disc loss: 0.0023298322428518567, policy loss: 6.220057560691478
Experience 11, Iter 92, disc loss: 0.0024037114736935646, policy loss: 6.175732340590346
Experience 11, Iter 93, disc loss: 0.002329495108536273, policy loss: 6.227310368264175
Experience 11, Iter 94, disc loss: 0.002268078755526823, policy loss: 6.257308146304836
Experience 11, Iter 95, disc loss: 0.002455902710648578, policy loss: 6.1465600513466425
Experience 11, Iter 96, disc loss: 0.0023241471182350723, policy loss: 6.206974703190106
Experience 11, Iter 97, disc loss: 0.002370838536123077, policy loss: 6.185032973075572
Experience 11, Iter 98, disc loss: 0.002364280046950183, policy loss: 6.187830862645694
Experience 11, Iter 99, disc loss: 0.002280959623460264, policy loss: 6.228481223239294
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0324],
        [0.4877],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.1070e-03, 2.5522e-02, 2.9519e-01, 8.6663e-03, 6.4605e-04,
          6.7595e-01]],

        [[3.1070e-03, 2.5522e-02, 2.9519e-01, 8.6663e-03, 6.4605e-04,
          6.7595e-01]],

        [[3.1070e-03, 2.5522e-02, 2.9519e-01, 8.6663e-03, 6.4605e-04,
          6.7595e-01]],

        [[3.1070e-03, 2.5522e-02, 2.9519e-01, 8.6663e-03, 6.4605e-04,
          6.7595e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0027, 0.1297, 1.9508, 0.0247], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0027, 0.1297, 1.9508, 0.0247])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.265
Iter 2/2000 - Loss: 1.479
Iter 3/2000 - Loss: 0.269
Iter 4/2000 - Loss: 0.429
Iter 5/2000 - Loss: 0.838
Iter 6/2000 - Loss: 0.650
Iter 7/2000 - Loss: 0.318
Iter 8/2000 - Loss: 0.216
Iter 9/2000 - Loss: 0.325
Iter 10/2000 - Loss: 0.446
Iter 11/2000 - Loss: 0.454
Iter 12/2000 - Loss: 0.366
Iter 13/2000 - Loss: 0.258
Iter 14/2000 - Loss: 0.189
Iter 15/2000 - Loss: 0.176
Iter 16/2000 - Loss: 0.196
Iter 17/2000 - Loss: 0.206
Iter 18/2000 - Loss: 0.170
Iter 19/2000 - Loss: 0.086
Iter 20/2000 - Loss: -0.016
Iter 1981/2000 - Loss: -8.113
Iter 1982/2000 - Loss: -8.114
Iter 1983/2000 - Loss: -8.114
Iter 1984/2000 - Loss: -8.114
Iter 1985/2000 - Loss: -8.114
Iter 1986/2000 - Loss: -8.114
Iter 1987/2000 - Loss: -8.114
Iter 1988/2000 - Loss: -8.114
Iter 1989/2000 - Loss: -8.114
Iter 1990/2000 - Loss: -8.114
Iter 1991/2000 - Loss: -8.114
Iter 1992/2000 - Loss: -8.114
Iter 1993/2000 - Loss: -8.114
Iter 1994/2000 - Loss: -8.114
Iter 1995/2000 - Loss: -8.114
Iter 1996/2000 - Loss: -8.114
Iter 1997/2000 - Loss: -8.114
Iter 1998/2000 - Loss: -8.114
Iter 1999/2000 - Loss: -8.114
Iter 2000/2000 - Loss: -8.114
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[12.4385,  3.5825, 47.7699,  6.2184, 10.7429, 38.1830]],

        [[14.9844, 23.9819, 18.9750,  1.0417,  5.2221, 15.9860]],

        [[ 6.9401, 25.1734, 11.8507,  1.1158,  6.4037, 16.9287]],

        [[11.4238, 16.3493, 19.5485,  5.8459,  1.3027, 38.5215]]])
Signal Variance: tensor([ 0.0394,  1.3618, 19.0037,  0.7177])
Estimated target variance: tensor([0.0027, 0.1297, 1.9508, 0.0247])
N: 120
Signal to noise ratio: tensor([ 9.9194, 56.4655, 99.9315, 57.2934])
Bound on condition number: tensor([  11808.4093,  382603.3931, 1198357.5837,  393904.9791])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.002284487819282655, policy loss: 6.2200154576656494
Experience 12, Iter 1, disc loss: 0.0022689968237123216, policy loss: 6.238470589101216
Experience 12, Iter 2, disc loss: 0.002252660510487659, policy loss: 6.254338121373166
Experience 12, Iter 3, disc loss: 0.0022959819930954504, policy loss: 6.215248857990898
Experience 12, Iter 4, disc loss: 0.0023790073815848625, policy loss: 6.198584301376858
Experience 12, Iter 5, disc loss: 0.002179399211641062, policy loss: 6.305528808881613
Experience 12, Iter 6, disc loss: 0.0022647343295821975, policy loss: 6.243846559222075
Experience 12, Iter 7, disc loss: 0.0023244082354064366, policy loss: 6.226944572529729
Experience 12, Iter 8, disc loss: 0.0022574307050747273, policy loss: 6.247827877379662
Experience 12, Iter 9, disc loss: 0.002346531271351644, policy loss: 6.2056960563840535
Experience 12, Iter 10, disc loss: 0.002262635518269913, policy loss: 6.228716309495762
Experience 12, Iter 11, disc loss: 0.0022232000966050346, policy loss: 6.260076695266476
Experience 12, Iter 12, disc loss: 0.0021670783202596498, policy loss: 6.283918740093245
Experience 12, Iter 13, disc loss: 0.002162358181933464, policy loss: 6.290959189810368
Experience 12, Iter 14, disc loss: 0.0021991413095983963, policy loss: 6.268747790984548
Experience 12, Iter 15, disc loss: 0.0021391853296680234, policy loss: 6.297750866815428
Experience 12, Iter 16, disc loss: 0.0021331548810334115, policy loss: 6.29691245055036
Experience 12, Iter 17, disc loss: 0.0021804046125966777, policy loss: 6.270096139194009
Experience 12, Iter 18, disc loss: 0.002301378248748058, policy loss: 6.2288764234921725
Experience 12, Iter 19, disc loss: 0.002218219722564346, policy loss: 6.253824165451826
Experience 12, Iter 20, disc loss: 0.0021157768619737443, policy loss: 6.3171504220569945
Experience 12, Iter 21, disc loss: 0.002128803991344465, policy loss: 6.291825423943222
Experience 12, Iter 22, disc loss: 0.0022447506694191865, policy loss: 6.2399403663518695
Experience 12, Iter 23, disc loss: 0.0021131001704244846, policy loss: 6.309695932511629
Experience 12, Iter 24, disc loss: 0.0021519853529376153, policy loss: 6.292336251771207
Experience 12, Iter 25, disc loss: 0.002062823159603876, policy loss: 6.34007640498737
Experience 12, Iter 26, disc loss: 0.0022129938048307987, policy loss: 6.254628446656729
Experience 12, Iter 27, disc loss: 0.002110344903953827, policy loss: 6.3082030480371625
Experience 12, Iter 28, disc loss: 0.0020861773906829854, policy loss: 6.314006865673062
Experience 12, Iter 29, disc loss: 0.0021055857211533533, policy loss: 6.34476081196181
Experience 12, Iter 30, disc loss: 0.0022179819367282474, policy loss: 6.260851023715215
Experience 12, Iter 31, disc loss: 0.0019920750191386115, policy loss: 6.371380268680841
Experience 12, Iter 32, disc loss: 0.002117695682940111, policy loss: 6.30412645253167
Experience 12, Iter 33, disc loss: 0.0020905981919933786, policy loss: 6.328950722901709
Experience 12, Iter 34, disc loss: 0.0020752910257322496, policy loss: 6.330425164013246
Experience 12, Iter 35, disc loss: 0.0020811781530667263, policy loss: 6.311275162449141
Experience 12, Iter 36, disc loss: 0.0020217022720814613, policy loss: 6.355005039202782
Experience 12, Iter 37, disc loss: 0.0020123607509935806, policy loss: 6.3643292321231195
Experience 12, Iter 38, disc loss: 0.002033483568274635, policy loss: 6.350408464754219
Experience 12, Iter 39, disc loss: 0.0020634073720721775, policy loss: 6.341706937951304
Experience 12, Iter 40, disc loss: 0.002052084811853463, policy loss: 6.335132228294896
Experience 12, Iter 41, disc loss: 0.0020554577126236703, policy loss: 6.3315690264095466
Experience 12, Iter 42, disc loss: 0.0020867442899738797, policy loss: 6.310208434692541
Experience 12, Iter 43, disc loss: 0.0019935893158763694, policy loss: 6.399418507600002
Experience 12, Iter 44, disc loss: 0.0019812315642285076, policy loss: 6.3867402698988505
Experience 12, Iter 45, disc loss: 0.002026679104855102, policy loss: 6.347012016665552
Experience 12, Iter 46, disc loss: 0.002041193569389856, policy loss: 6.35151207076204
Experience 12, Iter 47, disc loss: 0.001883339598293931, policy loss: 6.433560229976206
Experience 12, Iter 48, disc loss: 0.0019635303598493515, policy loss: 6.390412938540857
Experience 12, Iter 49, disc loss: 0.002054234557232621, policy loss: 6.338606442208635
Experience 12, Iter 50, disc loss: 0.0019677115197975796, policy loss: 6.378285174162686
Experience 12, Iter 51, disc loss: 0.0019527167098640317, policy loss: 6.382300440832782
Experience 12, Iter 52, disc loss: 0.0020405369543168303, policy loss: 6.336838256719
Experience 12, Iter 53, disc loss: 0.001991649958954188, policy loss: 6.376347753782821
Experience 12, Iter 54, disc loss: 0.001945014815612045, policy loss: 6.390820200350632
Experience 12, Iter 55, disc loss: 0.001954555913534458, policy loss: 6.3839924645440504
Experience 12, Iter 56, disc loss: 0.0019514686104137576, policy loss: 6.390166304833006
Experience 12, Iter 57, disc loss: 0.0020477019677733513, policy loss: 6.335045609882326
Experience 12, Iter 58, disc loss: 0.0020063050538657373, policy loss: 6.372137596711219
Experience 12, Iter 59, disc loss: 0.001920560870504462, policy loss: 6.401702972867562
Experience 12, Iter 60, disc loss: 0.0020407919346598314, policy loss: 6.335706510959177
Experience 12, Iter 61, disc loss: 0.001922619045401461, policy loss: 6.409909350936363
Experience 12, Iter 62, disc loss: 0.0019129010403557461, policy loss: 6.417843323153474
Experience 12, Iter 63, disc loss: 0.002069819747663496, policy loss: 6.317539107219345
Experience 12, Iter 64, disc loss: 0.0019483645062925326, policy loss: 6.3721957470192665
Experience 12, Iter 65, disc loss: 0.0018966046630863528, policy loss: 6.424279077328001
Experience 12, Iter 66, disc loss: 0.0019902614324096442, policy loss: 6.381883325590965
Experience 12, Iter 67, disc loss: 0.0019796205778090698, policy loss: 6.378227462783304
Experience 12, Iter 68, disc loss: 0.0018664270042564684, policy loss: 6.438753962281814
Experience 12, Iter 69, disc loss: 0.001997306483184448, policy loss: 6.372181304460987
Experience 12, Iter 70, disc loss: 0.0019466006978608053, policy loss: 6.392256480277833
Experience 12, Iter 71, disc loss: 0.002017194975612541, policy loss: 6.344215870966332
Experience 12, Iter 72, disc loss: 0.0019695221184607365, policy loss: 6.376926619600821
Experience 12, Iter 73, disc loss: 0.0018254883126907692, policy loss: 6.479439830615809
Experience 12, Iter 74, disc loss: 0.0018363603625998802, policy loss: 6.460605934948027
Experience 12, Iter 75, disc loss: 0.0018711663848985627, policy loss: 6.431663953594416
Experience 12, Iter 76, disc loss: 0.0018284564279251619, policy loss: 6.458439408543153
Experience 12, Iter 77, disc loss: 0.0017590763245124129, policy loss: 6.505596052219465
Experience 12, Iter 78, disc loss: 0.0019112112539176804, policy loss: 6.4078132617986565
Experience 12, Iter 79, disc loss: 0.0018206433439045089, policy loss: 6.455114324453906
Experience 12, Iter 80, disc loss: 0.0019427901821698122, policy loss: 6.391043128093131
Experience 12, Iter 81, disc loss: 0.0018709972558424977, policy loss: 6.435201599523233
Experience 12, Iter 82, disc loss: 0.0018259456554030496, policy loss: 6.46940344066282
Experience 12, Iter 83, disc loss: 0.0018527821509304301, policy loss: 6.441791840194224
Experience 12, Iter 84, disc loss: 0.0018555763925398156, policy loss: 6.4388406118665795
Experience 12, Iter 85, disc loss: 0.0018782206855340888, policy loss: 6.427563278042831
Experience 12, Iter 86, disc loss: 0.0019238824364533353, policy loss: 6.403265311892874
Experience 12, Iter 87, disc loss: 0.001894537230725351, policy loss: 6.422275083401036
Experience 12, Iter 88, disc loss: 0.0018542317359568957, policy loss: 6.459432222763556
Experience 12, Iter 89, disc loss: 0.001764598784173878, policy loss: 6.5094680466610955
Experience 12, Iter 90, disc loss: 0.0018146828123242393, policy loss: 6.472871271004998
Experience 12, Iter 91, disc loss: 0.00192313830043276, policy loss: 6.39335950589914
Experience 12, Iter 92, disc loss: 0.0018005683956967432, policy loss: 6.471540653514151
Experience 12, Iter 93, disc loss: 0.0018150681435487074, policy loss: 6.465547896204578
Experience 12, Iter 94, disc loss: 0.001914805606881033, policy loss: 6.411795563104976
Experience 12, Iter 95, disc loss: 0.0018446654672470157, policy loss: 6.449268735632634
Experience 12, Iter 96, disc loss: 0.0018586977072376416, policy loss: 6.43647154843209
Experience 12, Iter 97, disc loss: 0.0018194717103168741, policy loss: 6.456645702677362
Experience 12, Iter 98, disc loss: 0.001748004325110407, policy loss: 6.51366506514805
Experience 12, Iter 99, disc loss: 0.001898979514464548, policy loss: 6.4039679600617525
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0308],
        [0.4639],
        [0.0059]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.9015e-03, 2.5005e-02, 2.8086e-01, 8.5261e-03, 6.1552e-04,
          6.5154e-01]],

        [[2.9015e-03, 2.5005e-02, 2.8086e-01, 8.5261e-03, 6.1552e-04,
          6.5154e-01]],

        [[2.9015e-03, 2.5005e-02, 2.8086e-01, 8.5261e-03, 6.1552e-04,
          6.5154e-01]],

        [[2.9015e-03, 2.5005e-02, 2.8086e-01, 8.5261e-03, 6.1552e-04,
          6.5154e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.1231, 1.8558, 0.0237], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.1231, 1.8558, 0.0237])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.188
Iter 2/2000 - Loss: 1.418
Iter 3/2000 - Loss: 0.191
Iter 4/2000 - Loss: 0.351
Iter 5/2000 - Loss: 0.767
Iter 6/2000 - Loss: 0.576
Iter 7/2000 - Loss: 0.239
Iter 8/2000 - Loss: 0.137
Iter 9/2000 - Loss: 0.251
Iter 10/2000 - Loss: 0.376
Iter 11/2000 - Loss: 0.383
Iter 12/2000 - Loss: 0.293
Iter 13/2000 - Loss: 0.186
Iter 14/2000 - Loss: 0.121
Iter 15/2000 - Loss: 0.113
Iter 16/2000 - Loss: 0.138
Iter 17/2000 - Loss: 0.151
Iter 18/2000 - Loss: 0.116
Iter 19/2000 - Loss: 0.032
Iter 20/2000 - Loss: -0.069
Iter 1981/2000 - Loss: -8.226
Iter 1982/2000 - Loss: -8.226
Iter 1983/2000 - Loss: -8.227
Iter 1984/2000 - Loss: -8.227
Iter 1985/2000 - Loss: -8.227
Iter 1986/2000 - Loss: -8.227
Iter 1987/2000 - Loss: -8.227
Iter 1988/2000 - Loss: -8.227
Iter 1989/2000 - Loss: -8.227
Iter 1990/2000 - Loss: -8.227
Iter 1991/2000 - Loss: -8.227
Iter 1992/2000 - Loss: -8.227
Iter 1993/2000 - Loss: -8.227
Iter 1994/2000 - Loss: -8.227
Iter 1995/2000 - Loss: -8.227
Iter 1996/2000 - Loss: -8.227
Iter 1997/2000 - Loss: -8.227
Iter 1998/2000 - Loss: -8.227
Iter 1999/2000 - Loss: -8.227
Iter 2000/2000 - Loss: -8.227
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[12.1695,  3.6302, 47.3172,  6.5407, 10.5531, 38.8257]],

        [[14.7171, 23.9154, 17.1821,  1.0285,  5.0691, 16.9270]],

        [[ 6.9680, 26.3515, 11.1611,  1.1086,  6.0697, 15.5972]],

        [[10.7572, 15.6805, 19.6063,  5.8904,  1.3327, 37.8623]]])
Signal Variance: tensor([ 0.0392,  1.4763, 18.1024,  0.7109])
Estimated target variance: tensor([0.0026, 0.1231, 1.8558, 0.0237])
N: 130
Signal to noise ratio: tensor([10.0343, 60.5770, 97.8137, 57.0481])
Bound on condition number: tensor([  13090.3307,  477045.6972, 1243778.5022,  423084.4474])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0017374943259728741, policy loss: 6.503116778891548
Experience 13, Iter 1, disc loss: 0.001728175452454993, policy loss: 6.526042193432853
Experience 13, Iter 2, disc loss: 0.0017376940012383858, policy loss: 6.502208307021209
Experience 13, Iter 3, disc loss: 0.0017971107751112532, policy loss: 6.477902989657348
Experience 13, Iter 4, disc loss: 0.0016875141569535782, policy loss: 6.54655444477879
Experience 13, Iter 5, disc loss: 0.001668506330153566, policy loss: 6.5709731347538565
Experience 13, Iter 6, disc loss: 0.0018490679140958067, policy loss: 6.436744770997972
Experience 13, Iter 7, disc loss: 0.0017549186300039438, policy loss: 6.506988209359941
Experience 13, Iter 8, disc loss: 0.001743182032776729, policy loss: 6.497500524253854
Experience 13, Iter 9, disc loss: 0.0017303222508145036, policy loss: 6.531939070126767
Experience 13, Iter 10, disc loss: 0.0017244933607638768, policy loss: 6.515204418988025
Experience 13, Iter 11, disc loss: 0.0018067947983786499, policy loss: 6.457722828311632
Experience 13, Iter 12, disc loss: 0.001758848170653639, policy loss: 6.4852203513618365
Experience 13, Iter 13, disc loss: 0.001780931610901568, policy loss: 6.466320497569045
Experience 13, Iter 14, disc loss: 0.0016899014044532308, policy loss: 6.534083216101173
Experience 13, Iter 15, disc loss: 0.001630845880522551, policy loss: 6.58832710432908
Experience 13, Iter 16, disc loss: 0.0017126888472430257, policy loss: 6.532086759632862
Experience 13, Iter 17, disc loss: 0.0015874119533427606, policy loss: 6.604490088726272
Experience 13, Iter 18, disc loss: 0.001719218225841251, policy loss: 6.532093791494813
Experience 13, Iter 19, disc loss: 0.0016434539400591035, policy loss: 6.56900173901166
Experience 13, Iter 20, disc loss: 0.0016669639272850042, policy loss: 6.5423853744063525
Experience 13, Iter 21, disc loss: 0.001623998538189394, policy loss: 6.589336797391707
Experience 13, Iter 22, disc loss: 0.001564133782788024, policy loss: 6.638412090869636
Experience 13, Iter 23, disc loss: 0.0016205584290703025, policy loss: 6.586153157737573
Experience 13, Iter 24, disc loss: 0.0016842504510789023, policy loss: 6.5417323005153625
Experience 13, Iter 25, disc loss: 0.0016682451762381845, policy loss: 6.5426208377521275
Experience 13, Iter 26, disc loss: 0.0016950964360618731, policy loss: 6.515873324476702
Experience 13, Iter 27, disc loss: 0.001688779815973079, policy loss: 6.528533259175718
Experience 13, Iter 28, disc loss: 0.0016751101883272972, policy loss: 6.542364445531954
Experience 13, Iter 29, disc loss: 0.0016323060797470423, policy loss: 6.56509118407307
Experience 13, Iter 30, disc loss: 0.0016553328671495356, policy loss: 6.557360318372407
Experience 13, Iter 31, disc loss: 0.0016826704845736744, policy loss: 6.528050770683035
Experience 13, Iter 32, disc loss: 0.001692679477044723, policy loss: 6.527144213888595
Experience 13, Iter 33, disc loss: 0.0016670402540635855, policy loss: 6.5510569075487926
Experience 13, Iter 34, disc loss: 0.001662826422085975, policy loss: 6.5485603146720806
Experience 13, Iter 35, disc loss: 0.0016876006251656584, policy loss: 6.539116337386236
Experience 13, Iter 36, disc loss: 0.0015861963992793993, policy loss: 6.619471301371618
Experience 13, Iter 37, disc loss: 0.001609730614401855, policy loss: 6.580277015712472
Experience 13, Iter 38, disc loss: 0.0016452138329995617, policy loss: 6.562785370201019
Experience 13, Iter 39, disc loss: 0.0015855309613393095, policy loss: 6.60376711136097
Experience 13, Iter 40, disc loss: 0.0016219956166950657, policy loss: 6.591125886924049
Experience 13, Iter 41, disc loss: 0.0016329383481035874, policy loss: 6.580151167564729
Experience 13, Iter 42, disc loss: 0.0016164400478087804, policy loss: 6.572446441904357
Experience 13, Iter 43, disc loss: 0.001635440831428042, policy loss: 6.569603581125201
Experience 13, Iter 44, disc loss: 0.0016804625751163582, policy loss: 6.532839013531992
Experience 13, Iter 45, disc loss: 0.0015303520563880871, policy loss: 6.63921201772205
Experience 13, Iter 46, disc loss: 0.00161817716133242, policy loss: 6.581721595592254
Experience 13, Iter 47, disc loss: 0.0015455365566165332, policy loss: 6.636695175939847
Experience 13, Iter 48, disc loss: 0.0015999458139830205, policy loss: 6.597776271106829
Experience 13, Iter 49, disc loss: 0.0016011321126712598, policy loss: 6.619339860515366
Experience 13, Iter 50, disc loss: 0.001632299576617689, policy loss: 6.560634916387051
Experience 13, Iter 51, disc loss: 0.001593361438262539, policy loss: 6.592324285711976
Experience 13, Iter 52, disc loss: 0.0015323200105446537, policy loss: 6.634775280900231
Experience 13, Iter 53, disc loss: 0.0016208501350028458, policy loss: 6.570139631325375
Experience 13, Iter 54, disc loss: 0.0016569827919605334, policy loss: 6.538405554935309
Experience 13, Iter 55, disc loss: 0.0015575126990819929, policy loss: 6.61883217898254
Experience 13, Iter 56, disc loss: 0.0016225166083877357, policy loss: 6.596859623443494
Experience 13, Iter 57, disc loss: 0.0015530751692095824, policy loss: 6.619822590728755
Experience 13, Iter 58, disc loss: 0.0015048166251516878, policy loss: 6.662804667166014
Experience 13, Iter 59, disc loss: 0.0014702472399944716, policy loss: 6.686039713367514
Experience 13, Iter 60, disc loss: 0.0015990695083702262, policy loss: 6.591349325714171
Experience 13, Iter 61, disc loss: 0.0015481162723198403, policy loss: 6.620355844683093
Experience 13, Iter 62, disc loss: 0.0015627883679229986, policy loss: 6.609855479530534
Experience 13, Iter 63, disc loss: 0.0015229368199796056, policy loss: 6.634377649414373
Experience 13, Iter 64, disc loss: 0.0015636919960775098, policy loss: 6.607861566964642
Experience 13, Iter 65, disc loss: 0.001459460789526351, policy loss: 6.690427069920331
Experience 13, Iter 66, disc loss: 0.0016084025235609761, policy loss: 6.578431571716296
Experience 13, Iter 67, disc loss: 0.001564590917950391, policy loss: 6.604858917771175
Experience 13, Iter 68, disc loss: 0.0014964113771042012, policy loss: 6.669038160688072
Experience 13, Iter 69, disc loss: 0.0015704467003183704, policy loss: 6.600762210175439
Experience 13, Iter 70, disc loss: 0.0015364524660535126, policy loss: 6.639812327299175
Experience 13, Iter 71, disc loss: 0.0015232617458598118, policy loss: 6.6396244555153565
Experience 13, Iter 72, disc loss: 0.001488421608148285, policy loss: 6.662074459058598
Experience 13, Iter 73, disc loss: 0.0015467918100773056, policy loss: 6.616167645368776
Experience 13, Iter 74, disc loss: 0.0016229114208858029, policy loss: 6.577104740574483
Experience 13, Iter 75, disc loss: 0.0013932166795004252, policy loss: 6.739219266298774
Experience 13, Iter 76, disc loss: 0.0015113336894431915, policy loss: 6.651739054821025
Experience 13, Iter 77, disc loss: 0.0015424123071287117, policy loss: 6.625203563984677
Experience 13, Iter 78, disc loss: 0.001380063925427171, policy loss: 6.762158858211877
Experience 13, Iter 79, disc loss: 0.0014834347075971326, policy loss: 6.672578005797433
Experience 13, Iter 80, disc loss: 0.0014755798005398682, policy loss: 6.672754997791575
Experience 13, Iter 81, disc loss: 0.0014364519732790004, policy loss: 6.701286404263353
Experience 13, Iter 82, disc loss: 0.0013777328558333476, policy loss: 6.740893090318843
Experience 13, Iter 83, disc loss: 0.0015777032631969154, policy loss: 6.58521016088058
Experience 13, Iter 84, disc loss: 0.0015175232234522782, policy loss: 6.661893605480966
Experience 13, Iter 85, disc loss: 0.0013219391323095622, policy loss: 6.803984660664318
Experience 13, Iter 86, disc loss: 0.0014343098171524767, policy loss: 6.707252767565994
Experience 13, Iter 87, disc loss: 0.0015560629516628234, policy loss: 6.609220496838123
Experience 13, Iter 88, disc loss: 0.0014189078318805096, policy loss: 6.703348058142303
Experience 13, Iter 89, disc loss: 0.0014078992379201545, policy loss: 6.722880079646392
Experience 13, Iter 90, disc loss: 0.0014919426156011068, policy loss: 6.656030203236059
Experience 13, Iter 91, disc loss: 0.0013379992195271407, policy loss: 6.79026832630401
Experience 13, Iter 92, disc loss: 0.0013632313737245983, policy loss: 6.757012896809874
Experience 13, Iter 93, disc loss: 0.0014315186275646768, policy loss: 6.699775627879953
Experience 13, Iter 94, disc loss: 0.0013754637449001831, policy loss: 6.751696228477678
Experience 13, Iter 95, disc loss: 0.001439193607417599, policy loss: 6.703046882012924
Experience 13, Iter 96, disc loss: 0.0014106438895362988, policy loss: 6.731519381618004
Experience 13, Iter 97, disc loss: 0.0015013712326776563, policy loss: 6.649545319892519
Experience 13, Iter 98, disc loss: 0.0014321892226677366, policy loss: 6.7105524281413516
Experience 13, Iter 99, disc loss: 0.0014928279250817163, policy loss: 6.659409209247939
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0312],
        [0.4662],
        [0.0057]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.8228e-03, 2.4761e-02, 2.7456e-01, 8.2136e-03, 5.7989e-04,
          6.4994e-01]],

        [[2.8228e-03, 2.4761e-02, 2.7456e-01, 8.2136e-03, 5.7989e-04,
          6.4994e-01]],

        [[2.8228e-03, 2.4761e-02, 2.7456e-01, 8.2136e-03, 5.7989e-04,
          6.4994e-01]],

        [[2.8228e-03, 2.4761e-02, 2.7456e-01, 8.2136e-03, 5.7989e-04,
          6.4994e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.1249, 1.8649, 0.0229], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.1249, 1.8649, 0.0229])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.176
Iter 2/2000 - Loss: 1.413
Iter 3/2000 - Loss: 0.177
Iter 4/2000 - Loss: 0.337
Iter 5/2000 - Loss: 0.754
Iter 6/2000 - Loss: 0.560
Iter 7/2000 - Loss: 0.220
Iter 8/2000 - Loss: 0.122
Iter 9/2000 - Loss: 0.243
Iter 10/2000 - Loss: 0.369
Iter 11/2000 - Loss: 0.372
Iter 12/2000 - Loss: 0.277
Iter 13/2000 - Loss: 0.172
Iter 14/2000 - Loss: 0.114
Iter 15/2000 - Loss: 0.113
Iter 16/2000 - Loss: 0.142
Iter 17/2000 - Loss: 0.155
Iter 18/2000 - Loss: 0.118
Iter 19/2000 - Loss: 0.034
Iter 20/2000 - Loss: -0.063
Iter 1981/2000 - Loss: -8.314
Iter 1982/2000 - Loss: -8.314
Iter 1983/2000 - Loss: -8.314
Iter 1984/2000 - Loss: -8.314
Iter 1985/2000 - Loss: -8.314
Iter 1986/2000 - Loss: -8.314
Iter 1987/2000 - Loss: -8.314
Iter 1988/2000 - Loss: -8.314
Iter 1989/2000 - Loss: -8.314
Iter 1990/2000 - Loss: -8.314
Iter 1991/2000 - Loss: -8.314
Iter 1992/2000 - Loss: -8.314
Iter 1993/2000 - Loss: -8.314
Iter 1994/2000 - Loss: -8.314
Iter 1995/2000 - Loss: -8.314
Iter 1996/2000 - Loss: -8.314
Iter 1997/2000 - Loss: -8.314
Iter 1998/2000 - Loss: -8.314
Iter 1999/2000 - Loss: -8.314
Iter 2000/2000 - Loss: -8.314
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[12.0400,  3.6417, 46.4083,  6.2467, 10.2564, 37.8016]],

        [[14.6132, 23.4736, 16.6626,  1.0427,  5.0002, 19.0401]],

        [[11.5828, 26.9081, 10.8127,  1.0801,  5.6624, 15.4431]],

        [[11.1064, 16.3883, 19.4102,  6.0785,  1.2448, 36.9377]]])
Signal Variance: tensor([ 0.0391,  1.6572, 16.0408,  0.7138])
Estimated target variance: tensor([0.0026, 0.1249, 1.8649, 0.0229])
N: 140
Signal to noise ratio: tensor([10.1626, 65.3272, 90.9709, 55.9756])
Bound on condition number: tensor([  14459.9570,  597471.4291, 1158599.6382,  438658.3193])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0014407061514695705, policy loss: 6.687776975728086
Experience 14, Iter 1, disc loss: 0.0014625630164629068, policy loss: 6.68741417414587
Experience 14, Iter 2, disc loss: 0.0013979386568752255, policy loss: 6.724752952166195
Experience 14, Iter 3, disc loss: 0.001464649532431007, policy loss: 6.667696649941718
Experience 14, Iter 4, disc loss: 0.0014111594815542682, policy loss: 6.716588204541358
Experience 14, Iter 5, disc loss: 0.001402803316332219, policy loss: 6.719426687227723
Experience 14, Iter 6, disc loss: 0.0013924755076325538, policy loss: 6.737762953578576
Experience 14, Iter 7, disc loss: 0.0014449586640480404, policy loss: 6.706482821703658
Experience 14, Iter 8, disc loss: 0.0013804588518648068, policy loss: 6.753403570624065
Experience 14, Iter 9, disc loss: 0.0013985378771850252, policy loss: 6.717273167710939
Experience 14, Iter 10, disc loss: 0.001357986423390276, policy loss: 6.769326088899559
Experience 14, Iter 11, disc loss: 0.001420163345392531, policy loss: 6.7034522924165
Experience 14, Iter 12, disc loss: 0.0013700421402713294, policy loss: 6.749668068725975
Experience 14, Iter 13, disc loss: 0.0014279659823620214, policy loss: 6.710781861933081
Experience 14, Iter 14, disc loss: 0.0013840063151155575, policy loss: 6.7488169319975615
Experience 14, Iter 15, disc loss: 0.0013566577490786804, policy loss: 6.756664948075603
Experience 14, Iter 16, disc loss: 0.00135332287318565, policy loss: 6.7631261479879425
Experience 14, Iter 17, disc loss: 0.0014383554789531264, policy loss: 6.687630080804013
Experience 14, Iter 18, disc loss: 0.0013518802871700926, policy loss: 6.7582112996332455
Experience 14, Iter 19, disc loss: 0.0013375833564981479, policy loss: 6.781992715918626
Experience 14, Iter 20, disc loss: 0.0013356472276396841, policy loss: 6.782495324976136
Experience 14, Iter 21, disc loss: 0.0013893331703259681, policy loss: 6.720742057523022
Experience 14, Iter 22, disc loss: 0.0013534974071389959, policy loss: 6.754653850372864
Experience 14, Iter 23, disc loss: 0.0014150339366138906, policy loss: 6.699747622622393
Experience 14, Iter 24, disc loss: 0.0014019017700271012, policy loss: 6.715559433459827
Experience 14, Iter 25, disc loss: 0.0013368815729907068, policy loss: 6.776707935879841
Experience 14, Iter 26, disc loss: 0.0013179244288947138, policy loss: 6.792591806333753
Experience 14, Iter 27, disc loss: 0.001293228981117222, policy loss: 6.803307785434781
Experience 14, Iter 28, disc loss: 0.0013455488283546345, policy loss: 6.781915222779235
Experience 14, Iter 29, disc loss: 0.0013663519491640705, policy loss: 6.742522320135711
Experience 14, Iter 30, disc loss: 0.001315545094345722, policy loss: 6.800340470804684
Experience 14, Iter 31, disc loss: 0.0013150145851668267, policy loss: 6.798261506628737
Experience 14, Iter 32, disc loss: 0.0013757781643390266, policy loss: 6.74105203270504
Experience 14, Iter 33, disc loss: 0.0013346369357181867, policy loss: 6.762662535007862
Experience 14, Iter 34, disc loss: 0.0013218485105514965, policy loss: 6.775266866565119
Experience 14, Iter 35, disc loss: 0.0012924734199335497, policy loss: 6.812553283018083
Experience 14, Iter 36, disc loss: 0.001351600813989991, policy loss: 6.772923083202336
Experience 14, Iter 37, disc loss: 0.00133463878403897, policy loss: 6.780888434460372
Experience 14, Iter 38, disc loss: 0.0013401079359603339, policy loss: 6.7552225269307185
Experience 14, Iter 39, disc loss: 0.0013795151149431338, policy loss: 6.733199478653866
Experience 14, Iter 40, disc loss: 0.0012804700778708783, policy loss: 6.824149068243197
Experience 14, Iter 41, disc loss: 0.001279138898682239, policy loss: 6.8319776783387045
Experience 14, Iter 42, disc loss: 0.0013180692530348095, policy loss: 6.815572481401427
Experience 14, Iter 43, disc loss: 0.0013252526693841387, policy loss: 6.780749701771343
Experience 14, Iter 44, disc loss: 0.0012028678642843074, policy loss: 6.903601049774106
Experience 14, Iter 45, disc loss: 0.0013296830830360593, policy loss: 6.7794341613490925
Experience 14, Iter 46, disc loss: 0.0013245487889741463, policy loss: 6.7808286360271275
Experience 14, Iter 47, disc loss: 0.001275659121260974, policy loss: 6.827442658615632
Experience 14, Iter 48, disc loss: 0.001323876450919419, policy loss: 6.776914083515694
Experience 14, Iter 49, disc loss: 0.001373656646215059, policy loss: 6.734666378052355
Experience 14, Iter 50, disc loss: 0.0013182455635322885, policy loss: 6.7883808835975685
Experience 14, Iter 51, disc loss: 0.001226302964922201, policy loss: 6.866716981183398
Experience 14, Iter 52, disc loss: 0.0013166526141449666, policy loss: 6.786511010186025
Experience 14, Iter 53, disc loss: 0.001384144344663675, policy loss: 6.739481346548212
Experience 14, Iter 54, disc loss: 0.0012274263462986235, policy loss: 6.872008390025137
Experience 14, Iter 55, disc loss: 0.0012881241647629128, policy loss: 6.812087487293276
Experience 14, Iter 56, disc loss: 0.001248598446865678, policy loss: 6.83839393790864
Experience 14, Iter 57, disc loss: 0.00125450385802686, policy loss: 6.837070563113031
Experience 14, Iter 58, disc loss: 0.001319860617864134, policy loss: 6.7707074709781505
Experience 14, Iter 59, disc loss: 0.0013063251075769063, policy loss: 6.806266199817234
Experience 14, Iter 60, disc loss: 0.0012185438813989825, policy loss: 6.873152963160796
Experience 14, Iter 61, disc loss: 0.0013361558873626214, policy loss: 6.763619063472419
Experience 14, Iter 62, disc loss: 0.0012786068048913396, policy loss: 6.817372888792742
Experience 14, Iter 63, disc loss: 0.001262858715446498, policy loss: 6.8180593713164805
Experience 14, Iter 64, disc loss: 0.001246877080035279, policy loss: 6.838956843312211
Experience 14, Iter 65, disc loss: 0.0012727407180972165, policy loss: 6.817046060311158
Experience 14, Iter 66, disc loss: 0.0012871935140304517, policy loss: 6.805942116552416
Experience 14, Iter 67, disc loss: 0.001278538062679442, policy loss: 6.8187955348043
Experience 14, Iter 68, disc loss: 0.001310325676173176, policy loss: 6.786989297626523
Experience 14, Iter 69, disc loss: 0.0012370829938595116, policy loss: 6.858714896108819
Experience 14, Iter 70, disc loss: 0.0012746544970042156, policy loss: 6.818715240558419
Experience 14, Iter 71, disc loss: 0.001336535868738527, policy loss: 6.772692688578944
Experience 14, Iter 72, disc loss: 0.0012538932153104865, policy loss: 6.82914461163306
Experience 14, Iter 73, disc loss: 0.0012479467281482808, policy loss: 6.839259027722516
Experience 14, Iter 74, disc loss: 0.0011859336922527015, policy loss: 6.892996425041811
Experience 14, Iter 75, disc loss: 0.0012483111197571315, policy loss: 6.833124695226309
Experience 14, Iter 76, disc loss: 0.0011846738650779307, policy loss: 6.902804741976809
Experience 14, Iter 77, disc loss: 0.0012563718907228785, policy loss: 6.827825841379592
Experience 14, Iter 78, disc loss: 0.0012575164944020022, policy loss: 6.826998597470206
Experience 14, Iter 79, disc loss: 0.0011253771145724486, policy loss: 6.952710841324301
Experience 14, Iter 80, disc loss: 0.0012319434045631322, policy loss: 6.855596935731051
Experience 14, Iter 81, disc loss: 0.0012712478070807205, policy loss: 6.814683533490436
Experience 14, Iter 82, disc loss: 0.001221107663807788, policy loss: 6.873539640279582
Experience 14, Iter 83, disc loss: 0.0011790912477577101, policy loss: 6.9048951716616065
Experience 14, Iter 84, disc loss: 0.0011288755953738762, policy loss: 6.961838506968435
Experience 14, Iter 85, disc loss: 0.001250405056526357, policy loss: 6.828304197399709
Experience 14, Iter 86, disc loss: 0.001248464681543246, policy loss: 6.84307430387986
Experience 14, Iter 87, disc loss: 0.0012137042253312687, policy loss: 6.864295431535797
Experience 14, Iter 88, disc loss: 0.001204338032967569, policy loss: 6.871326248988069
Experience 14, Iter 89, disc loss: 0.0011767593097615193, policy loss: 6.898982827016714
Experience 14, Iter 90, disc loss: 0.0012451766013872568, policy loss: 6.8257101469990005
Experience 14, Iter 91, disc loss: 0.0011597067252124451, policy loss: 6.917712825892516
Experience 14, Iter 92, disc loss: 0.0012279876152842429, policy loss: 6.848382353890939
Experience 14, Iter 93, disc loss: 0.0011650198605690902, policy loss: 6.93244345237389
Experience 14, Iter 94, disc loss: 0.0011857709975754717, policy loss: 6.903105496388497
Experience 14, Iter 95, disc loss: 0.0011628992735298774, policy loss: 6.9054620015348895
Experience 14, Iter 96, disc loss: 0.0012275969150062904, policy loss: 6.842589540497954
Experience 14, Iter 97, disc loss: 0.0011861628428348953, policy loss: 6.893893398198379
Experience 14, Iter 98, disc loss: 0.0012366859645293196, policy loss: 6.8319919442145824
Experience 14, Iter 99, disc loss: 0.0011411365136205346, policy loss: 6.936869792485547
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0298],
        [0.4469],
        [0.0056]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.7250e-03, 2.4394e-02, 2.6498e-01, 8.2746e-03, 5.6551e-04,
          6.3347e-01]],

        [[2.7250e-03, 2.4394e-02, 2.6498e-01, 8.2746e-03, 5.6551e-04,
          6.3347e-01]],

        [[2.7250e-03, 2.4394e-02, 2.6498e-01, 8.2746e-03, 5.6551e-04,
          6.3347e-01]],

        [[2.7250e-03, 2.4394e-02, 2.6498e-01, 8.2746e-03, 5.6551e-04,
          6.3347e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0025, 0.1190, 1.7875, 0.0222], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0025, 0.1190, 1.7875, 0.0222])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.118
Iter 2/2000 - Loss: 1.354
Iter 3/2000 - Loss: 0.118
Iter 4/2000 - Loss: 0.276
Iter 5/2000 - Loss: 0.693
Iter 6/2000 - Loss: 0.499
Iter 7/2000 - Loss: 0.159
Iter 8/2000 - Loss: 0.061
Iter 9/2000 - Loss: 0.183
Iter 10/2000 - Loss: 0.310
Iter 11/2000 - Loss: 0.313
Iter 12/2000 - Loss: 0.218
Iter 13/2000 - Loss: 0.113
Iter 14/2000 - Loss: 0.057
Iter 15/2000 - Loss: 0.059
Iter 16/2000 - Loss: 0.091
Iter 17/2000 - Loss: 0.106
Iter 18/2000 - Loss: 0.070
Iter 19/2000 - Loss: -0.013
Iter 20/2000 - Loss: -0.106
Iter 1981/2000 - Loss: -8.402
Iter 1982/2000 - Loss: -8.402
Iter 1983/2000 - Loss: -8.402
Iter 1984/2000 - Loss: -8.402
Iter 1985/2000 - Loss: -8.402
Iter 1986/2000 - Loss: -8.402
Iter 1987/2000 - Loss: -8.402
Iter 1988/2000 - Loss: -8.402
Iter 1989/2000 - Loss: -8.402
Iter 1990/2000 - Loss: -8.402
Iter 1991/2000 - Loss: -8.402
Iter 1992/2000 - Loss: -8.402
Iter 1993/2000 - Loss: -8.402
Iter 1994/2000 - Loss: -8.402
Iter 1995/2000 - Loss: -8.402
Iter 1996/2000 - Loss: -8.402
Iter 1997/2000 - Loss: -8.402
Iter 1998/2000 - Loss: -8.402
Iter 1999/2000 - Loss: -8.402
Iter 2000/2000 - Loss: -8.402
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.6299,  3.5438, 44.7851,  5.9255,  9.8579, 36.4699]],

        [[14.2071, 23.2331, 14.3815,  1.0617,  5.0929, 20.8624]],

        [[11.2058, 25.8534, 10.9417,  1.0892,  5.5934, 15.9556]],

        [[11.1896, 14.8841, 19.0180,  5.9687,  1.2300, 37.2684]]])
Signal Variance: tensor([ 0.0374,  1.8544, 17.0183,  0.6856])
Estimated target variance: tensor([0.0025, 0.1190, 1.7875, 0.0222])
N: 150
Signal to noise ratio: tensor([10.0781, 69.9139, 94.2155, 55.2092])
Bound on condition number: tensor([  15236.2662,  733194.5843, 1331485.1098,  457210.0779])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0012370849507339654, policy loss: 6.83520106211919
Experience 15, Iter 1, disc loss: 0.0011611792272973765, policy loss: 6.926047549346229
Experience 15, Iter 2, disc loss: 0.001132551756529352, policy loss: 6.93786730336949
Experience 15, Iter 3, disc loss: 0.0011969413536313948, policy loss: 6.880819668921917
Experience 15, Iter 4, disc loss: 0.001120451358358718, policy loss: 6.950544648151684
Experience 15, Iter 5, disc loss: 0.0012232459291824163, policy loss: 6.870348449879617
Experience 15, Iter 6, disc loss: 0.0011730040326245919, policy loss: 6.905228528790995
Experience 15, Iter 7, disc loss: 0.0012186083562875163, policy loss: 6.858639301493207
Experience 15, Iter 8, disc loss: 0.0011675542699264086, policy loss: 6.910551699142133
Experience 15, Iter 9, disc loss: 0.001201072240748124, policy loss: 6.8641773096699925
Experience 15, Iter 10, disc loss: 0.001170460245524005, policy loss: 6.903952684482477
Experience 15, Iter 11, disc loss: 0.0011323701106665982, policy loss: 6.940009914621065
Experience 15, Iter 12, disc loss: 0.0011835085612143894, policy loss: 6.890418554909587
Experience 15, Iter 13, disc loss: 0.0011848993658517399, policy loss: 6.878502973507007
Experience 15, Iter 14, disc loss: 0.0011622169703361588, policy loss: 6.917517851850307
Experience 15, Iter 15, disc loss: 0.0011326842389268235, policy loss: 6.935233894750003
Experience 15, Iter 16, disc loss: 0.001167877717668962, policy loss: 6.898252761447175
Experience 15, Iter 17, disc loss: 0.0010804128269205557, policy loss: 6.989060970077235
Experience 15, Iter 18, disc loss: 0.001161842157212897, policy loss: 6.900957474225389
Experience 15, Iter 19, disc loss: 0.0011369625801851968, policy loss: 6.936153097608838
Experience 15, Iter 20, disc loss: 0.0010976345785120806, policy loss: 6.96502335633501
Experience 15, Iter 21, disc loss: 0.0011057357220303399, policy loss: 6.970458681359467
Experience 15, Iter 22, disc loss: 0.0011449401295745899, policy loss: 6.928657083645277
Experience 15, Iter 23, disc loss: 0.0011322563250592726, policy loss: 6.934336640837403
Experience 15, Iter 24, disc loss: 0.0011361402270632804, policy loss: 6.929643410941619
Experience 15, Iter 25, disc loss: 0.0011508963775488784, policy loss: 6.927000095322688
Experience 15, Iter 26, disc loss: 0.0011595223763798592, policy loss: 6.904106545383817
Experience 15, Iter 27, disc loss: 0.0010914485324408465, policy loss: 6.988902247386987
Experience 15, Iter 28, disc loss: 0.0010911386748474325, policy loss: 7.000810666371105
Experience 15, Iter 29, disc loss: 0.001054041639225833, policy loss: 7.025005395598246
Experience 15, Iter 30, disc loss: 0.001113778644138509, policy loss: 6.954455410007148
Experience 15, Iter 31, disc loss: 0.0010866787029514809, policy loss: 6.982255185526129
Experience 15, Iter 32, disc loss: 0.001114859081559707, policy loss: 6.946440713959111
Experience 15, Iter 33, disc loss: 0.0011116059010658776, policy loss: 6.965649421527067
Experience 15, Iter 34, disc loss: 0.001150598745203012, policy loss: 6.91959637403148
Experience 15, Iter 35, disc loss: 0.0010953740792111655, policy loss: 6.971985769733007
Experience 15, Iter 36, disc loss: 0.0011098199594188822, policy loss: 6.978154572652337
Experience 15, Iter 37, disc loss: 0.0010822883677759567, policy loss: 6.988121026862636
Experience 15, Iter 38, disc loss: 0.0011334131340169674, policy loss: 6.951137800436714
Experience 15, Iter 39, disc loss: 0.0011094913858947259, policy loss: 6.956235851273214
Experience 15, Iter 40, disc loss: 0.0010686314828145963, policy loss: 7.008218466380399
Experience 15, Iter 41, disc loss: 0.0010939810203971392, policy loss: 6.966328905531205
Experience 15, Iter 42, disc loss: 0.0011345947965218115, policy loss: 6.9283318217997945
Experience 15, Iter 43, disc loss: 0.0010615185670140167, policy loss: 7.001527847304972
Experience 15, Iter 44, disc loss: 0.0011096368374547189, policy loss: 6.951648478164861
Experience 15, Iter 45, disc loss: 0.0010660075431948469, policy loss: 6.998961357707692
Experience 15, Iter 46, disc loss: 0.0010929122943382132, policy loss: 6.97948953855164
Experience 15, Iter 47, disc loss: 0.001075740285749514, policy loss: 7.000342870783559
Experience 15, Iter 48, disc loss: 0.0010768594212709462, policy loss: 6.987840485287344
Experience 15, Iter 49, disc loss: 0.0010590704097932424, policy loss: 7.02750923834474
Experience 15, Iter 50, disc loss: 0.0011082123064134553, policy loss: 6.958831325269767
Experience 15, Iter 51, disc loss: 0.001045984009075024, policy loss: 7.0251273581964275
Experience 15, Iter 52, disc loss: 0.0010678197074736663, policy loss: 7.009125524514686
Experience 15, Iter 53, disc loss: 0.0010559714909202514, policy loss: 7.005664450221763
Experience 15, Iter 54, disc loss: 0.0010898049893179774, policy loss: 6.982201586928325
Experience 15, Iter 55, disc loss: 0.0010309107325390083, policy loss: 7.038742932694759
Experience 15, Iter 56, disc loss: 0.0010540192776180705, policy loss: 7.0188287766944
Experience 15, Iter 57, disc loss: 0.001132193048075408, policy loss: 6.924098835172113
Experience 15, Iter 58, disc loss: 0.0010347541934813435, policy loss: 7.024333645035707
Experience 15, Iter 59, disc loss: 0.001047054188642596, policy loss: 7.010608264309071
Experience 15, Iter 60, disc loss: 0.0010850689199520653, policy loss: 6.982411770333938
Experience 15, Iter 61, disc loss: 0.0010204109524261464, policy loss: 7.046795060894063
Experience 15, Iter 62, disc loss: 0.0010802615542407944, policy loss: 6.973630838307823
Experience 15, Iter 63, disc loss: 0.0010922317421552235, policy loss: 6.971178141690153
Experience 15, Iter 64, disc loss: 0.0010043802434736946, policy loss: 7.07958828890507
Experience 15, Iter 65, disc loss: 0.0010705938512946525, policy loss: 6.994334737403674
Experience 15, Iter 66, disc loss: 0.0010674256536763722, policy loss: 6.999297606166307
Experience 15, Iter 67, disc loss: 0.0010422521587540928, policy loss: 7.025181583966033
Experience 15, Iter 68, disc loss: 0.0011085832937509305, policy loss: 6.967229521519323
Experience 15, Iter 69, disc loss: 0.001069361250982424, policy loss: 6.99179063466141
Experience 15, Iter 70, disc loss: 0.0010540058623945943, policy loss: 7.01192347870488
Experience 15, Iter 71, disc loss: 0.0010139243144462935, policy loss: 7.055016578947207
Experience 15, Iter 72, disc loss: 0.0009804460741698728, policy loss: 7.110588886259076
Experience 15, Iter 73, disc loss: 0.0010779157930868916, policy loss: 6.982464444949073
Experience 15, Iter 74, disc loss: 0.0010163584795879988, policy loss: 7.035377991039216
Experience 15, Iter 75, disc loss: 0.0010502843310415345, policy loss: 7.008520259366242
Experience 15, Iter 76, disc loss: 0.000990704009042, policy loss: 7.07918179490322
Experience 15, Iter 77, disc loss: 0.0010004192270208557, policy loss: 7.063391789500849
Experience 15, Iter 78, disc loss: 0.0010099113898881156, policy loss: 7.051999903984141
Experience 15, Iter 79, disc loss: 0.0010193074228917979, policy loss: 7.044808833098824
Experience 15, Iter 80, disc loss: 0.0010534095243382089, policy loss: 7.004975818835346
Experience 15, Iter 81, disc loss: 0.0009889980053068986, policy loss: 7.091280225217343
Experience 15, Iter 82, disc loss: 0.000972635945444166, policy loss: 7.103217290050722
Experience 15, Iter 83, disc loss: 0.0009907940748359921, policy loss: 7.076583022655727
Experience 15, Iter 84, disc loss: 0.001032561774341324, policy loss: 7.046167969343943
Experience 15, Iter 85, disc loss: 0.0009872277169137907, policy loss: 7.104508981091351
Experience 15, Iter 86, disc loss: 0.0010127078079285193, policy loss: 7.056081899857713
Experience 15, Iter 87, disc loss: 0.0010435436136384875, policy loss: 7.005613913240149
Experience 15, Iter 88, disc loss: 0.001016915713141294, policy loss: 7.035478027837029
Experience 15, Iter 89, disc loss: 0.0010377328539709196, policy loss: 7.04661621980603
Experience 15, Iter 90, disc loss: 0.0009923857954954808, policy loss: 7.067578682535697
Experience 15, Iter 91, disc loss: 0.0009594563523228146, policy loss: 7.10672218035446
Experience 15, Iter 92, disc loss: 0.0009997194195460672, policy loss: 7.055745389003706
Experience 15, Iter 93, disc loss: 0.0009793818779474045, policy loss: 7.076671868354289
Experience 15, Iter 94, disc loss: 0.0009495684875187342, policy loss: 7.115141306697073
Experience 15, Iter 95, disc loss: 0.0009710075130706687, policy loss: 7.09998984174268
Experience 15, Iter 96, disc loss: 0.001003322775902477, policy loss: 7.053158826540687
Experience 15, Iter 97, disc loss: 0.0009657188655630586, policy loss: 7.088946323350532
Experience 15, Iter 98, disc loss: 0.0010219658314451082, policy loss: 7.052381135688101
Experience 15, Iter 99, disc loss: 0.0009281996564174693, policy loss: 7.156202811396824
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0286],
        [0.4297],
        [0.0054]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.7106e-03, 2.3965e-02, 2.5546e-01, 8.1568e-03, 5.4334e-04,
          6.1592e-01]],

        [[2.7106e-03, 2.3965e-02, 2.5546e-01, 8.1568e-03, 5.4334e-04,
          6.1592e-01]],

        [[2.7106e-03, 2.3965e-02, 2.5546e-01, 8.1568e-03, 5.4334e-04,
          6.1592e-01]],

        [[2.7106e-03, 2.3965e-02, 2.5546e-01, 8.1568e-03, 5.4334e-04,
          6.1592e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.1142, 1.7190, 0.0216], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.1142, 1.7190, 0.0216])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.076
Iter 2/2000 - Loss: 1.260
Iter 3/2000 - Loss: 0.068
Iter 4/2000 - Loss: 0.215
Iter 5/2000 - Loss: 0.616
Iter 6/2000 - Loss: 0.427
Iter 7/2000 - Loss: 0.099
Iter 8/2000 - Loss: 0.011
Iter 9/2000 - Loss: 0.135
Iter 10/2000 - Loss: 0.257
Iter 11/2000 - Loss: 0.252
Iter 12/2000 - Loss: 0.155
Iter 13/2000 - Loss: 0.054
Iter 14/2000 - Loss: 0.006
Iter 15/2000 - Loss: 0.016
Iter 16/2000 - Loss: 0.050
Iter 17/2000 - Loss: 0.059
Iter 18/2000 - Loss: 0.015
Iter 19/2000 - Loss: -0.070
Iter 20/2000 - Loss: -0.157
Iter 1981/2000 - Loss: -8.425
Iter 1982/2000 - Loss: -8.425
Iter 1983/2000 - Loss: -8.425
Iter 1984/2000 - Loss: -8.425
Iter 1985/2000 - Loss: -8.425
Iter 1986/2000 - Loss: -8.425
Iter 1987/2000 - Loss: -8.425
Iter 1988/2000 - Loss: -8.425
Iter 1989/2000 - Loss: -8.425
Iter 1990/2000 - Loss: -8.425
Iter 1991/2000 - Loss: -8.425
Iter 1992/2000 - Loss: -8.425
Iter 1993/2000 - Loss: -8.425
Iter 1994/2000 - Loss: -8.425
Iter 1995/2000 - Loss: -8.426
Iter 1996/2000 - Loss: -8.426
Iter 1997/2000 - Loss: -8.426
Iter 1998/2000 - Loss: -8.426
Iter 1999/2000 - Loss: -8.426
Iter 2000/2000 - Loss: -8.426
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.0813,  3.5253, 43.3209,  5.6282,  9.7870, 35.4865]],

        [[14.0805, 21.4456, 15.7848,  1.0772,  4.6608, 19.3145]],

        [[10.0951, 24.6536, 11.0361,  1.1201,  5.8025, 14.7334]],

        [[10.4371, 14.5591, 18.9073,  5.7276,  1.2810, 36.1546]]])
Signal Variance: tensor([ 0.0392,  1.8064, 16.8264,  0.6711])
Estimated target variance: tensor([0.0026, 0.1142, 1.7190, 0.0216])
N: 160
Signal to noise ratio: tensor([10.2028, 68.4421, 93.4547, 53.9752])
Bound on condition number: tensor([  16656.6611,  749491.2772, 1397404.6833,  466133.3681])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0009595216010644717, policy loss: 7.110106849158321
Experience 16, Iter 1, disc loss: 0.0009616130558943282, policy loss: 7.1028283843762825
Experience 16, Iter 2, disc loss: 0.0009559755195815042, policy loss: 7.104017268302512
Experience 16, Iter 3, disc loss: 0.0009725946072779056, policy loss: 7.101517204967541
Experience 16, Iter 4, disc loss: 0.0009276204352088226, policy loss: 7.155216125181729
Experience 16, Iter 5, disc loss: 0.0009435629465211502, policy loss: 7.1278547835829436
Experience 16, Iter 6, disc loss: 0.0009434797733254895, policy loss: 7.127945654024156
Experience 16, Iter 7, disc loss: 0.0009106732296497748, policy loss: 7.15889538919368
Experience 16, Iter 8, disc loss: 0.0009626489945124172, policy loss: 7.126037323255764
Experience 16, Iter 9, disc loss: 0.0009596462618917975, policy loss: 7.109659292830227
Experience 16, Iter 10, disc loss: 0.0009444236270813656, policy loss: 7.134553964815005
Experience 16, Iter 11, disc loss: 0.0009292348315952778, policy loss: 7.144062023908383
Experience 16, Iter 12, disc loss: 0.0009194865107370235, policy loss: 7.152242049079208
Experience 16, Iter 13, disc loss: 0.0009864118016551411, policy loss: 7.089480047634892
Experience 16, Iter 14, disc loss: 0.0009322153581261542, policy loss: 7.137067513536218
Experience 16, Iter 15, disc loss: 0.0009670153096018589, policy loss: 7.089726445520213
Experience 16, Iter 16, disc loss: 0.0009447758067396576, policy loss: 7.119043484631926
Experience 16, Iter 17, disc loss: 0.000939931390616498, policy loss: 7.117340669004729
Experience 16, Iter 18, disc loss: 0.0009154548567694731, policy loss: 7.156494231352124
Experience 16, Iter 19, disc loss: 0.0009258932382711227, policy loss: 7.14532497958567
Experience 16, Iter 20, disc loss: 0.000928292712185378, policy loss: 7.157132090291843
Experience 16, Iter 21, disc loss: 0.0009480550157871004, policy loss: 7.125520159978693
Experience 16, Iter 22, disc loss: 0.0008688293570165125, policy loss: 7.215294021036893
Experience 16, Iter 23, disc loss: 0.0009055845137046979, policy loss: 7.163152041006841
Experience 16, Iter 24, disc loss: 0.0009228060911962676, policy loss: 7.157179187083095
Experience 16, Iter 25, disc loss: 0.00096527206449533, policy loss: 7.093988672121145
Experience 16, Iter 26, disc loss: 0.000995845879390784, policy loss: 7.059048169052548
Experience 16, Iter 27, disc loss: 0.0009351456586231975, policy loss: 7.129794685824597
Experience 16, Iter 28, disc loss: 0.0009482615545009644, policy loss: 7.1084294021119545
Experience 16, Iter 29, disc loss: 0.0009509628833977802, policy loss: 7.102529312092782
Experience 16, Iter 30, disc loss: 0.0009491133944507903, policy loss: 7.107397806501515
Experience 16, Iter 31, disc loss: 0.0008948467611979194, policy loss: 7.184204734744405
Experience 16, Iter 32, disc loss: 0.0009360147839610931, policy loss: 7.117907989191783
Experience 16, Iter 33, disc loss: 0.0009228824045712088, policy loss: 7.133188275795364
Experience 16, Iter 34, disc loss: 0.0008962273141042921, policy loss: 7.177469255703366
Experience 16, Iter 35, disc loss: 0.0009611643940579623, policy loss: 7.08545700345992
Experience 16, Iter 36, disc loss: 0.0009089116479794231, policy loss: 7.181451986953981
Experience 16, Iter 37, disc loss: 0.0009078403407118075, policy loss: 7.186787960806051
Experience 16, Iter 38, disc loss: 0.0009126315983661318, policy loss: 7.159238751526129
Experience 16, Iter 39, disc loss: 0.0008594909356480306, policy loss: 7.223979256145338
Experience 16, Iter 40, disc loss: 0.0009315116268267872, policy loss: 7.120961342560456
Experience 16, Iter 41, disc loss: 0.0009149928891745765, policy loss: 7.141996637140622
Experience 16, Iter 42, disc loss: 0.0009546019070297925, policy loss: 7.11533274034112
Experience 16, Iter 43, disc loss: 0.0008871649227864662, policy loss: 7.182304914617279
Experience 16, Iter 44, disc loss: 0.0009023310509531345, policy loss: 7.163356265811551
Experience 16, Iter 45, disc loss: 0.0009232454021676158, policy loss: 7.136636732647578
Experience 16, Iter 46, disc loss: 0.0009045311869954657, policy loss: 7.170650135138046
Experience 16, Iter 47, disc loss: 0.000921207277563886, policy loss: 7.139117152347941
Experience 16, Iter 48, disc loss: 0.0008932688568730508, policy loss: 7.166675487139118
Experience 16, Iter 49, disc loss: 0.0008993591784527779, policy loss: 7.16375162130209
Experience 16, Iter 50, disc loss: 0.0009072308763696796, policy loss: 7.166311474812831
Experience 16, Iter 51, disc loss: 0.0009030989446863572, policy loss: 7.162500899931724
Experience 16, Iter 52, disc loss: 0.0009031669834081491, policy loss: 7.152576918250315
Experience 16, Iter 53, disc loss: 0.0009240596509834006, policy loss: 7.136864439282077
Experience 16, Iter 54, disc loss: 0.0008723106846948037, policy loss: 7.213142139717336
Experience 16, Iter 55, disc loss: 0.0009071318538849166, policy loss: 7.1594215513974575
Experience 16, Iter 56, disc loss: 0.000885622099757172, policy loss: 7.176031817126583
Experience 16, Iter 57, disc loss: 0.0008893678775155678, policy loss: 7.164835812691315
Experience 16, Iter 58, disc loss: 0.0009076933244423674, policy loss: 7.143225499866119
Experience 16, Iter 59, disc loss: 0.000880415874747269, policy loss: 7.224730341694197
Experience 16, Iter 60, disc loss: 0.0008944211798526375, policy loss: 7.162280497222342
Experience 16, Iter 61, disc loss: 0.0009101778825809044, policy loss: 7.1524027426330035
Experience 16, Iter 62, disc loss: 0.0008322672788763923, policy loss: 7.2415295454379915
Experience 16, Iter 63, disc loss: 0.000843313984730824, policy loss: 7.237064800442018
Experience 16, Iter 64, disc loss: 0.0008342400934010439, policy loss: 7.255822993868849
Experience 16, Iter 65, disc loss: 0.0008814748766538551, policy loss: 7.180524796552318
Experience 16, Iter 66, disc loss: 0.0008754445524800888, policy loss: 7.194856185825511
Experience 16, Iter 67, disc loss: 0.0009276788310040572, policy loss: 7.1312775456553545
Experience 16, Iter 68, disc loss: 0.0008597086808233505, policy loss: 7.2090836236629015
Experience 16, Iter 69, disc loss: 0.0008721898144386103, policy loss: 7.2055580478171875
Experience 16, Iter 70, disc loss: 0.0008473292330445554, policy loss: 7.2443705176656135
Experience 16, Iter 71, disc loss: 0.0008935546814071906, policy loss: 7.176493040222587
Experience 16, Iter 72, disc loss: 0.0008278921790201683, policy loss: 7.264619779661033
Experience 16, Iter 73, disc loss: 0.0008591262737458553, policy loss: 7.2068408097967245
Experience 16, Iter 74, disc loss: 0.000855543037928652, policy loss: 7.228136789546036
Experience 16, Iter 75, disc loss: 0.0008711806506031478, policy loss: 7.186918767517987
Experience 16, Iter 76, disc loss: 0.0008489015903663655, policy loss: 7.233682078767265
Experience 16, Iter 77, disc loss: 0.000922034473318687, policy loss: 7.126751329770772
Experience 16, Iter 78, disc loss: 0.0008407431771774447, policy loss: 7.253034538755268
Experience 16, Iter 79, disc loss: 0.0008565228229042821, policy loss: 7.2116568765108156
Experience 16, Iter 80, disc loss: 0.0008623747292797325, policy loss: 7.218246160257012
Experience 16, Iter 81, disc loss: 0.0008674023384024689, policy loss: 7.20668248730415
Experience 16, Iter 82, disc loss: 0.0008683729624447358, policy loss: 7.19343128914281
Experience 16, Iter 83, disc loss: 0.0008252457588431812, policy loss: 7.267271861029247
Experience 16, Iter 84, disc loss: 0.0008491071697317815, policy loss: 7.230041151897157
Experience 16, Iter 85, disc loss: 0.000827588500750021, policy loss: 7.251352300710726
Experience 16, Iter 86, disc loss: 0.0008924333790472813, policy loss: 7.176997155922141
Experience 16, Iter 87, disc loss: 0.0008157590824705398, policy loss: 7.283904503976431
Experience 16, Iter 88, disc loss: 0.0008023502254428537, policy loss: 7.290232685212982
Experience 16, Iter 89, disc loss: 0.0008213357867704839, policy loss: 7.259116716593979
Experience 16, Iter 90, disc loss: 0.0008492651858221094, policy loss: 7.232330910849235
Experience 16, Iter 91, disc loss: 0.0008095222736835673, policy loss: 7.276621306619015
Experience 16, Iter 92, disc loss: 0.00083782880601054, policy loss: 7.245428419552425
Experience 16, Iter 93, disc loss: 0.0008101335777100925, policy loss: 7.276882063009948
Experience 16, Iter 94, disc loss: 0.0008166295467356983, policy loss: 7.277175248172145
Experience 16, Iter 95, disc loss: 0.0008190558557000793, policy loss: 7.271218726787328
Experience 16, Iter 96, disc loss: 0.0008126946868495253, policy loss: 7.279029108503962
Experience 16, Iter 97, disc loss: 0.0008253520688255476, policy loss: 7.2646133948777125
Experience 16, Iter 98, disc loss: 0.0008207524602086257, policy loss: 7.247284702682188
Experience 16, Iter 99, disc loss: 0.0008272015435774757, policy loss: 7.252123461002521
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0279],
        [0.4227],
        [0.0053]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.9756e-03, 2.3410e-02, 2.5074e-01, 8.1370e-03, 5.2654e-04,
          6.0668e-01]],

        [[2.9756e-03, 2.3410e-02, 2.5074e-01, 8.1370e-03, 5.2654e-04,
          6.0668e-01]],

        [[2.9756e-03, 2.3410e-02, 2.5074e-01, 8.1370e-03, 5.2654e-04,
          6.0668e-01]],

        [[2.9756e-03, 2.3410e-02, 2.5074e-01, 8.1370e-03, 5.2654e-04,
          6.0668e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0025, 0.1117, 1.6907, 0.0212], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0025, 0.1117, 1.6907, 0.0212])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.040
Iter 2/2000 - Loss: 1.247
Iter 3/2000 - Loss: 0.032
Iter 4/2000 - Loss: 0.179
Iter 5/2000 - Loss: 0.589
Iter 6/2000 - Loss: 0.397
Iter 7/2000 - Loss: 0.063
Iter 8/2000 - Loss: -0.028
Iter 9/2000 - Loss: 0.098
Iter 10/2000 - Loss: 0.222
Iter 11/2000 - Loss: 0.217
Iter 12/2000 - Loss: 0.118
Iter 13/2000 - Loss: 0.016
Iter 14/2000 - Loss: -0.034
Iter 15/2000 - Loss: -0.024
Iter 16/2000 - Loss: 0.011
Iter 17/2000 - Loss: 0.022
Iter 18/2000 - Loss: -0.022
Iter 19/2000 - Loss: -0.108
Iter 20/2000 - Loss: -0.197
Iter 1981/2000 - Loss: -8.462
Iter 1982/2000 - Loss: -8.462
Iter 1983/2000 - Loss: -8.462
Iter 1984/2000 - Loss: -8.462
Iter 1985/2000 - Loss: -8.462
Iter 1986/2000 - Loss: -8.462
Iter 1987/2000 - Loss: -8.462
Iter 1988/2000 - Loss: -8.462
Iter 1989/2000 - Loss: -8.462
Iter 1990/2000 - Loss: -8.462
Iter 1991/2000 - Loss: -8.462
Iter 1992/2000 - Loss: -8.462
Iter 1993/2000 - Loss: -8.462
Iter 1994/2000 - Loss: -8.462
Iter 1995/2000 - Loss: -8.462
Iter 1996/2000 - Loss: -8.462
Iter 1997/2000 - Loss: -8.462
Iter 1998/2000 - Loss: -8.462
Iter 1999/2000 - Loss: -8.462
Iter 2000/2000 - Loss: -8.462
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[11.7966,  3.4537, 41.9429,  5.5909,  9.6076, 34.8541]],

        [[13.7797, 19.8650, 16.8759,  1.0574,  4.2078, 16.2183]],

        [[14.2305, 24.9800, 11.1784,  1.0671,  5.5701, 13.8245]],

        [[ 9.8810, 15.5561, 18.8450,  5.4619,  1.3675, 34.5606]]])
Signal Variance: tensor([ 0.0365,  1.5165, 13.8874,  0.6674])
Estimated target variance: tensor([0.0025, 0.1117, 1.6907, 0.0212])
N: 170
Signal to noise ratio: tensor([ 9.9222, 63.1965, 84.2027, 52.2087])
Bound on condition number: tensor([  16737.6655,  678946.0915, 1205316.3428,  463378.4956])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0008307572000143525, policy loss: 7.236562355794984
Experience 17, Iter 1, disc loss: 0.0008412226506846997, policy loss: 7.228088941437642
Experience 17, Iter 2, disc loss: 0.0008350271289719592, policy loss: 7.2356699894967855
Experience 17, Iter 3, disc loss: 0.0008231011896834678, policy loss: 7.266427961260311
Experience 17, Iter 4, disc loss: 0.0008239920719131603, policy loss: 7.283215724420065
Experience 17, Iter 5, disc loss: 0.0008381458653414666, policy loss: 7.2338158969525495
Experience 17, Iter 6, disc loss: 0.0008342307830323754, policy loss: 7.237589429867302
Experience 17, Iter 7, disc loss: 0.000813453726702647, policy loss: 7.291622475595958
Experience 17, Iter 8, disc loss: 0.0008131795212219312, policy loss: 7.274008450931065
Experience 17, Iter 9, disc loss: 0.0008214387283956769, policy loss: 7.247508418977418
Experience 17, Iter 10, disc loss: 0.0008045847877285243, policy loss: 7.287187398102917
Experience 17, Iter 11, disc loss: 0.0007880790747233142, policy loss: 7.300889479955599
Experience 17, Iter 12, disc loss: 0.0008287575581680574, policy loss: 7.231894334641627
Experience 17, Iter 13, disc loss: 0.0007836335687033463, policy loss: 7.319403961225875
Experience 17, Iter 14, disc loss: 0.0007974222237927548, policy loss: 7.305446391243334
Experience 17, Iter 15, disc loss: 0.0007928829920760823, policy loss: 7.305262345264488
Experience 17, Iter 16, disc loss: 0.0008006352628678074, policy loss: 7.284204694949361
Experience 17, Iter 17, disc loss: 0.000762095238206412, policy loss: 7.354865774767921
Experience 17, Iter 18, disc loss: 0.0007643616942047347, policy loss: 7.33623588666539
Experience 17, Iter 19, disc loss: 0.0008054549498420095, policy loss: 7.2723942763470575
Experience 17, Iter 20, disc loss: 0.0007933676273034662, policy loss: 7.294512021951992
Experience 17, Iter 21, disc loss: 0.0007776970093474691, policy loss: 7.3104528913512175
Experience 17, Iter 22, disc loss: 0.0008060455597445076, policy loss: 7.288513867092757
Experience 17, Iter 23, disc loss: 0.000818385679870845, policy loss: 7.258265963426123
Experience 17, Iter 24, disc loss: 0.0007929950855029204, policy loss: 7.302702562027936
Experience 17, Iter 25, disc loss: 0.0008458300968101496, policy loss: 7.222712389499851
Experience 17, Iter 26, disc loss: 0.0008301664553756338, policy loss: 7.242965153525847
Experience 17, Iter 27, disc loss: 0.0007426160116137726, policy loss: 7.371841579257331
Experience 17, Iter 28, disc loss: 0.0007863896448492591, policy loss: 7.328469034295634
Experience 17, Iter 29, disc loss: 0.0008022748843186697, policy loss: 7.289475267792722
Experience 17, Iter 30, disc loss: 0.0007722991205663969, policy loss: 7.3207668599932685
Experience 17, Iter 31, disc loss: 0.0007813671890175898, policy loss: 7.321388994036366
Experience 17, Iter 32, disc loss: 0.000781529560326136, policy loss: 7.31301234827917
Experience 17, Iter 33, disc loss: 0.0007938703002703421, policy loss: 7.300264498252912
Experience 17, Iter 34, disc loss: 0.0007508078304654356, policy loss: 7.3553256572792085
Experience 17, Iter 35, disc loss: 0.0008202720614885314, policy loss: 7.2530802030472294
Experience 17, Iter 36, disc loss: 0.0007535355482476086, policy loss: 7.347505761079443
Experience 17, Iter 37, disc loss: 0.0007565082571486968, policy loss: 7.3598551132632135
Experience 17, Iter 38, disc loss: 0.0007572373147275934, policy loss: 7.34777504721836
Experience 17, Iter 39, disc loss: 0.0007779276986066895, policy loss: 7.3137370853953545
Experience 17, Iter 40, disc loss: 0.0007440267409772559, policy loss: 7.36626811952119
Experience 17, Iter 41, disc loss: 0.0007641741790131412, policy loss: 7.338893838093474
Experience 17, Iter 42, disc loss: 0.0007471026088637764, policy loss: 7.377070253952232
Experience 17, Iter 43, disc loss: 0.0007889445484543637, policy loss: 7.301991737929241
Experience 17, Iter 44, disc loss: 0.0007353333792012503, policy loss: 7.384447476119612
Experience 17, Iter 45, disc loss: 0.0007139772931578124, policy loss: 7.401195401584936
Experience 17, Iter 46, disc loss: 0.0007295149818132308, policy loss: 7.381907218343025
Experience 17, Iter 47, disc loss: 0.0007487241171428557, policy loss: 7.374794610728781
Experience 17, Iter 48, disc loss: 0.0007480222500215436, policy loss: 7.368411838251932
Experience 17, Iter 49, disc loss: 0.0007642918090566757, policy loss: 7.321146099093478
Experience 17, Iter 50, disc loss: 0.0007646306369976611, policy loss: 7.326194363680036
Experience 17, Iter 51, disc loss: 0.0007665314260461698, policy loss: 7.319017263943625
Experience 17, Iter 52, disc loss: 0.0007530194809507352, policy loss: 7.374104484963455
Experience 17, Iter 53, disc loss: 0.0007790712542185396, policy loss: 7.308384212625711
Experience 17, Iter 54, disc loss: 0.0008103280270081863, policy loss: 7.267102316797096
Experience 17, Iter 55, disc loss: 0.0007585993106252626, policy loss: 7.3358704596973014
Experience 17, Iter 56, disc loss: 0.0007252546150538258, policy loss: 7.396594302463011
Experience 17, Iter 57, disc loss: 0.0007798962266012037, policy loss: 7.312167693780523
Experience 17, Iter 58, disc loss: 0.0007439112917117199, policy loss: 7.351048852936219
Experience 17, Iter 59, disc loss: 0.0007646368476873363, policy loss: 7.340246303870673
Experience 17, Iter 60, disc loss: 0.0007171327012287396, policy loss: 7.410389471735397
Experience 17, Iter 61, disc loss: 0.0007314873926347738, policy loss: 7.386708292984471
Experience 17, Iter 62, disc loss: 0.0007198810969281291, policy loss: 7.393225195140907
Experience 17, Iter 63, disc loss: 0.0007272391096258645, policy loss: 7.386547222950938
Experience 17, Iter 64, disc loss: 0.0007630147593213181, policy loss: 7.332572782011406
Experience 17, Iter 65, disc loss: 0.0007477242709869746, policy loss: 7.35440831367237
Experience 17, Iter 66, disc loss: 0.0007202283082831783, policy loss: 7.401015668443624
Experience 17, Iter 67, disc loss: 0.0007126363407843086, policy loss: 7.423513270224992
Experience 17, Iter 68, disc loss: 0.0007382646843243901, policy loss: 7.37263744139964
Experience 17, Iter 69, disc loss: 0.0007174028005839205, policy loss: 7.4086302605723855
Experience 17, Iter 70, disc loss: 0.0007281326378327777, policy loss: 7.3788554189512086
Experience 17, Iter 71, disc loss: 0.0007572118786403697, policy loss: 7.334468396120695
Experience 17, Iter 72, disc loss: 0.0007227854485152243, policy loss: 7.394236526087999
Experience 17, Iter 73, disc loss: 0.0007318236357154124, policy loss: 7.380283436282036
Experience 17, Iter 74, disc loss: 0.0006852401004796324, policy loss: 7.454317893735707
Experience 17, Iter 75, disc loss: 0.0007485217320716997, policy loss: 7.344922188823579
Experience 17, Iter 76, disc loss: 0.000707642553918586, policy loss: 7.415820206305167
Experience 17, Iter 77, disc loss: 0.0007187930082282722, policy loss: 7.411041775543171
Experience 17, Iter 78, disc loss: 0.0007384778876515798, policy loss: 7.362370693266048
Experience 17, Iter 79, disc loss: 0.0007072865721277528, policy loss: 7.417464581042174
Experience 17, Iter 80, disc loss: 0.0007276283677485468, policy loss: 7.383429317706314
Experience 17, Iter 81, disc loss: 0.000730322212105675, policy loss: 7.401658653849159
Experience 17, Iter 82, disc loss: 0.000721976044808572, policy loss: 7.38876764676508
Experience 17, Iter 83, disc loss: 0.0007685064822729871, policy loss: 7.314957467900698
Experience 17, Iter 84, disc loss: 0.0007361366441040688, policy loss: 7.377060745552177
Experience 17, Iter 85, disc loss: 0.0007200136201833253, policy loss: 7.388165724277956
Experience 17, Iter 86, disc loss: 0.000725981762631271, policy loss: 7.382188068293045
Experience 17, Iter 87, disc loss: 0.0007056038100094419, policy loss: 7.433696302377177
Experience 17, Iter 88, disc loss: 0.0007720648921024402, policy loss: 7.313170880247899
Experience 17, Iter 89, disc loss: 0.0006924767303381877, policy loss: 7.4378045899954035
Experience 17, Iter 90, disc loss: 0.0006975440944905966, policy loss: 7.430422891420364
Experience 17, Iter 91, disc loss: 0.000705772007743685, policy loss: 7.415105625680351
Experience 17, Iter 92, disc loss: 0.0007113964588283523, policy loss: 7.397087191778482
Experience 17, Iter 93, disc loss: 0.0007182882953703236, policy loss: 7.4289231549335
Experience 17, Iter 94, disc loss: 0.0006705290573712339, policy loss: 7.473745146811868
Experience 17, Iter 95, disc loss: 0.0006668336584820467, policy loss: 7.474291960666056
Experience 17, Iter 96, disc loss: 0.0007111197212700481, policy loss: 7.416593656355032
Experience 17, Iter 97, disc loss: 0.000700773885457378, policy loss: 7.431315827044599
Experience 17, Iter 98, disc loss: 0.0007207640750856448, policy loss: 7.396008376613889
Experience 17, Iter 99, disc loss: 0.0006727852587629428, policy loss: 7.478165124335296
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0271],
        [0.4108],
        [0.0052]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0967e-03, 2.3251e-02, 2.4370e-01, 8.0408e-03, 5.0970e-04,
          5.9346e-01]],

        [[3.0967e-03, 2.3251e-02, 2.4370e-01, 8.0408e-03, 5.0970e-04,
          5.9346e-01]],

        [[3.0967e-03, 2.3251e-02, 2.4370e-01, 8.0408e-03, 5.0970e-04,
          5.9346e-01]],

        [[3.0967e-03, 2.3251e-02, 2.4370e-01, 8.0408e-03, 5.0970e-04,
          5.9346e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0025, 0.1085, 1.6434, 0.0207], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0025, 0.1085, 1.6434, 0.0207])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.003
Iter 2/2000 - Loss: 1.185
Iter 3/2000 - Loss: -0.011
Iter 4/2000 - Loss: 0.129
Iter 5/2000 - Loss: 0.531
Iter 6/2000 - Loss: 0.340
Iter 7/2000 - Loss: 0.012
Iter 8/2000 - Loss: -0.073
Iter 9/2000 - Loss: 0.055
Iter 10/2000 - Loss: 0.175
Iter 11/2000 - Loss: 0.166
Iter 12/2000 - Loss: 0.064
Iter 13/2000 - Loss: -0.036
Iter 14/2000 - Loss: -0.080
Iter 15/2000 - Loss: -0.065
Iter 16/2000 - Loss: -0.029
Iter 17/2000 - Loss: -0.023
Iter 18/2000 - Loss: -0.072
Iter 19/2000 - Loss: -0.160
Iter 20/2000 - Loss: -0.245
Iter 1981/2000 - Loss: -8.521
Iter 1982/2000 - Loss: -8.521
Iter 1983/2000 - Loss: -8.521
Iter 1984/2000 - Loss: -8.521
Iter 1985/2000 - Loss: -8.521
Iter 1986/2000 - Loss: -8.521
Iter 1987/2000 - Loss: -8.521
Iter 1988/2000 - Loss: -8.521
Iter 1989/2000 - Loss: -8.521
Iter 1990/2000 - Loss: -8.521
Iter 1991/2000 - Loss: -8.521
Iter 1992/2000 - Loss: -8.521
Iter 1993/2000 - Loss: -8.521
Iter 1994/2000 - Loss: -8.521
Iter 1995/2000 - Loss: -8.521
Iter 1996/2000 - Loss: -8.521
Iter 1997/2000 - Loss: -8.521
Iter 1998/2000 - Loss: -8.521
Iter 1999/2000 - Loss: -8.521
Iter 2000/2000 - Loss: -8.521
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[11.2613,  3.4797, 41.6212,  5.4128,  9.4193, 35.2459]],

        [[13.5942, 20.3301, 15.6486,  1.0560,  4.3367, 18.3969]],

        [[13.2164, 23.7753, 10.6748,  1.0890,  5.5722, 14.4562]],

        [[ 9.8827, 15.3930, 18.6289,  5.1297,  1.2989, 33.1140]]])
Signal Variance: tensor([ 0.0371,  1.6753, 14.6835,  0.6267])
Estimated target variance: tensor([0.0025, 0.1085, 1.6434, 0.0207])
N: 180
Signal to noise ratio: tensor([10.1780, 66.6990, 86.3485, 50.5944])
Bound on condition number: tensor([  18647.6856,  800776.3827, 1342092.8475,  460763.1859])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.000682817617348547, policy loss: 7.4581215946863715
Experience 18, Iter 1, disc loss: 0.000699382338446853, policy loss: 7.412641297228477
Experience 18, Iter 2, disc loss: 0.0006829944627723138, policy loss: 7.458665453040901
Experience 18, Iter 3, disc loss: 0.0006954774618494655, policy loss: 7.422433794650805
Experience 18, Iter 4, disc loss: 0.0007025676739388423, policy loss: 7.423854975289448
Experience 18, Iter 5, disc loss: 0.0006937709507440739, policy loss: 7.430040673071183
Experience 18, Iter 6, disc loss: 0.0006972767776013181, policy loss: 7.4269061962001395
Experience 18, Iter 7, disc loss: 0.0006766665959716163, policy loss: 7.464983843539538
Experience 18, Iter 8, disc loss: 0.0007159980046662893, policy loss: 7.399366877294763
Experience 18, Iter 9, disc loss: 0.0007292790454636426, policy loss: 7.371886580538815
Experience 18, Iter 10, disc loss: 0.0006913209749253046, policy loss: 7.4327962656458535
Experience 18, Iter 11, disc loss: 0.0006858337704924566, policy loss: 7.445385894819641
Experience 18, Iter 12, disc loss: 0.0006741058266944018, policy loss: 7.468947082668097
Experience 18, Iter 13, disc loss: 0.0006846230324714641, policy loss: 7.447831346300573
Experience 18, Iter 14, disc loss: 0.0006657588408649714, policy loss: 7.4866385515022955
Experience 18, Iter 15, disc loss: 0.0006968961715131854, policy loss: 7.420633174695194
Experience 18, Iter 16, disc loss: 0.000728254457564433, policy loss: 7.378615630820904
Experience 18, Iter 17, disc loss: 0.0006560581633189316, policy loss: 7.484518527108628
Experience 18, Iter 18, disc loss: 0.0006852215807563851, policy loss: 7.440547527014655
Experience 18, Iter 19, disc loss: 0.0006850243497945679, policy loss: 7.440812740296391
Experience 18, Iter 20, disc loss: 0.0006635872709321772, policy loss: 7.5155106665921245
Experience 18, Iter 21, disc loss: 0.0006944425927341195, policy loss: 7.424974552084862
Experience 18, Iter 22, disc loss: 0.0006700018219537233, policy loss: 7.458571495410419
Experience 18, Iter 23, disc loss: 0.0006865986810874606, policy loss: 7.427903571056291
Experience 18, Iter 24, disc loss: 0.0006704721250397926, policy loss: 7.468982057060267
Experience 18, Iter 25, disc loss: 0.0006836176825783223, policy loss: 7.442224809703555
Experience 18, Iter 26, disc loss: 0.0006651838928741863, policy loss: 7.478370563760885
Experience 18, Iter 27, disc loss: 0.0007011452355890157, policy loss: 7.4134030417796595
Experience 18, Iter 28, disc loss: 0.0006551255095817791, policy loss: 7.4835960407941755
Experience 18, Iter 29, disc loss: 0.0006355186807081612, policy loss: 7.522701687514393
Experience 18, Iter 30, disc loss: 0.0006516021038921977, policy loss: 7.497766802844913
Experience 18, Iter 31, disc loss: 0.0006500669173511686, policy loss: 7.518575818643077
Experience 18, Iter 32, disc loss: 0.0006593407498337963, policy loss: 7.495537093240665
Experience 18, Iter 33, disc loss: 0.0006426614959099723, policy loss: 7.509404302288529
Experience 18, Iter 34, disc loss: 0.0006248655811297721, policy loss: 7.549163490644835
Experience 18, Iter 35, disc loss: 0.0006568666249589768, policy loss: 7.499791543568826
Experience 18, Iter 36, disc loss: 0.0006530003113757586, policy loss: 7.493061623973139
Experience 18, Iter 37, disc loss: 0.0006374529796026952, policy loss: 7.51726578111619
Experience 18, Iter 38, disc loss: 0.0005942586983278632, policy loss: 7.602683766668027
Experience 18, Iter 39, disc loss: 0.0006492875181015647, policy loss: 7.513070936521883
Experience 18, Iter 40, disc loss: 0.0005672156453961682, policy loss: 7.664587390506162
Experience 18, Iter 41, disc loss: 0.000563297505232726, policy loss: 7.679836673267243
Experience 18, Iter 42, disc loss: 0.0005737198838200062, policy loss: 7.6441853756567895
Experience 18, Iter 43, disc loss: 0.0006261527128891152, policy loss: 7.551009968048978
Experience 18, Iter 44, disc loss: 0.0005855264747911679, policy loss: 7.59545989814687
Experience 18, Iter 45, disc loss: 0.0006069083511862924, policy loss: 7.56406625347879
Experience 18, Iter 46, disc loss: 0.0006346563046212392, policy loss: 7.519244341905303
Experience 18, Iter 47, disc loss: 0.0005907420873883088, policy loss: 7.592668471238051
Experience 18, Iter 48, disc loss: 0.0006032677333187954, policy loss: 7.618116289496472
Experience 18, Iter 49, disc loss: 0.000637014390518944, policy loss: 7.528172254668281
Experience 18, Iter 50, disc loss: 0.0006526858308541313, policy loss: 7.496402099895995
Experience 18, Iter 51, disc loss: 0.0006120844307271811, policy loss: 7.562321713816404
Experience 18, Iter 52, disc loss: 0.0006050446971293457, policy loss: 7.573350451983199
Experience 18, Iter 53, disc loss: 0.0006419246886443377, policy loss: 7.510116855035278
Experience 18, Iter 54, disc loss: 0.0005844614216625741, policy loss: 7.619430925151926
Experience 18, Iter 55, disc loss: 0.000572790911928438, policy loss: 7.661531131613278
Experience 18, Iter 56, disc loss: 0.0006522081052714026, policy loss: 7.490825569287624
Experience 18, Iter 57, disc loss: 0.0006697484600350595, policy loss: 7.451705725645395
Experience 18, Iter 58, disc loss: 0.0005613468936186035, policy loss: 7.676144251222103
Experience 18, Iter 59, disc loss: 0.0006700896410283171, policy loss: 7.450995479775905
Experience 18, Iter 60, disc loss: 0.0005970983864445806, policy loss: 7.5884036124743846
Experience 18, Iter 61, disc loss: 0.0005666800889963438, policy loss: 7.650717833723666
Experience 18, Iter 62, disc loss: 0.0005850385451755624, policy loss: 7.620390769074674
Experience 18, Iter 63, disc loss: 0.0006235716791427184, policy loss: 7.541958985888734
Experience 18, Iter 64, disc loss: 0.0005758851709335647, policy loss: 7.631482616909881
Experience 18, Iter 65, disc loss: 0.0006596727631451393, policy loss: 7.474266127262799
Experience 18, Iter 66, disc loss: 0.0005904213077742787, policy loss: 7.601082848157325
Experience 18, Iter 67, disc loss: 0.000560346266691231, policy loss: 7.686059212729017
Experience 18, Iter 68, disc loss: 0.0005920194718685683, policy loss: 7.602239100919586
Experience 18, Iter 69, disc loss: 0.0006187676295398484, policy loss: 7.540780465464207
Experience 18, Iter 70, disc loss: 0.0005692941646472375, policy loss: 7.637387226253212
Experience 18, Iter 71, disc loss: 0.0005781087885940515, policy loss: 7.622782852780917
Experience 18, Iter 72, disc loss: 0.0006211113956237494, policy loss: 7.53752552087793
Experience 18, Iter 73, disc loss: 0.0006164926876596917, policy loss: 7.553988939629472
Experience 18, Iter 74, disc loss: 0.0005792797885596866, policy loss: 7.617503902386067
Experience 18, Iter 75, disc loss: 0.0005733395258747265, policy loss: 7.629692615226299
Experience 18, Iter 76, disc loss: 0.0005975652587317993, policy loss: 7.596244391255457
Experience 18, Iter 77, disc loss: 0.0006063148695119851, policy loss: 7.560554843008355
Experience 18, Iter 78, disc loss: 0.0005793005172714028, policy loss: 7.6185831138288265
Experience 18, Iter 79, disc loss: 0.0006174746395813206, policy loss: 7.547826186411997
Experience 18, Iter 80, disc loss: 0.0006141126973992755, policy loss: 7.547424559336389
Experience 18, Iter 81, disc loss: 0.000627873811470491, policy loss: 7.52637988752363
Experience 18, Iter 82, disc loss: 0.0006186662656107477, policy loss: 7.551743433998442
Experience 18, Iter 83, disc loss: 0.0006464861462180296, policy loss: 7.492325887950925
Experience 18, Iter 84, disc loss: 0.0006091267843251956, policy loss: 7.567668887751441
Experience 18, Iter 85, disc loss: 0.000607202519867868, policy loss: 7.553743368268465
Experience 18, Iter 86, disc loss: 0.0005585829393284252, policy loss: 7.668475825994383
Experience 18, Iter 87, disc loss: 0.0006182707118603569, policy loss: 7.544537187022567
Experience 18, Iter 88, disc loss: 0.0006127678744789877, policy loss: 7.564372191477963
Experience 18, Iter 89, disc loss: 0.0005651717487285919, policy loss: 7.6545386785410265
Experience 18, Iter 90, disc loss: 0.0006374173357066619, policy loss: 7.502792033954817
Experience 18, Iter 91, disc loss: 0.0006272401586281937, policy loss: 7.52020219317564
Experience 18, Iter 92, disc loss: 0.0005943244463140166, policy loss: 7.5841690381007645
Experience 18, Iter 93, disc loss: 0.0005991117328094299, policy loss: 7.590627958781394
Experience 18, Iter 94, disc loss: 0.0006164462546610208, policy loss: 7.5362897718121555
Experience 18, Iter 95, disc loss: 0.0005969795040593894, policy loss: 7.577413409165609
Experience 18, Iter 96, disc loss: 0.0006004349171654467, policy loss: 7.569134227305227
Experience 18, Iter 97, disc loss: 0.000624844703365202, policy loss: 7.5264882963565185
Experience 18, Iter 98, disc loss: 0.0005936720467502717, policy loss: 7.599276357292274
Experience 18, Iter 99, disc loss: 0.0006071423356906862, policy loss: 7.574527483298935
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0261],
        [0.3945],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0118e-03, 2.2770e-02, 2.3500e-01, 7.9935e-03, 4.9596e-04,
          5.8007e-01]],

        [[3.0118e-03, 2.2770e-02, 2.3500e-01, 7.9935e-03, 4.9596e-04,
          5.8007e-01]],

        [[3.0118e-03, 2.2770e-02, 2.3500e-01, 7.9935e-03, 4.9596e-04,
          5.8007e-01]],

        [[3.0118e-03, 2.2770e-02, 2.3500e-01, 7.9935e-03, 4.9596e-04,
          5.8007e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0024, 0.1042, 1.5780, 0.0201], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0024, 0.1042, 1.5780, 0.0201])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.055
Iter 2/2000 - Loss: 1.136
Iter 3/2000 - Loss: -0.070
Iter 4/2000 - Loss: 0.068
Iter 5/2000 - Loss: 0.474
Iter 6/2000 - Loss: 0.282
Iter 7/2000 - Loss: -0.049
Iter 8/2000 - Loss: -0.135
Iter 9/2000 - Loss: -0.006
Iter 10/2000 - Loss: 0.116
Iter 11/2000 - Loss: 0.106
Iter 12/2000 - Loss: 0.004
Iter 13/2000 - Loss: -0.098
Iter 14/2000 - Loss: -0.143
Iter 15/2000 - Loss: -0.128
Iter 16/2000 - Loss: -0.092
Iter 17/2000 - Loss: -0.086
Iter 18/2000 - Loss: -0.137
Iter 19/2000 - Loss: -0.228
Iter 20/2000 - Loss: -0.315
Iter 1981/2000 - Loss: -8.501
Iter 1982/2000 - Loss: -8.501
Iter 1983/2000 - Loss: -8.501
Iter 1984/2000 - Loss: -8.501
Iter 1985/2000 - Loss: -8.501
Iter 1986/2000 - Loss: -8.501
Iter 1987/2000 - Loss: -8.501
Iter 1988/2000 - Loss: -8.501
Iter 1989/2000 - Loss: -8.501
Iter 1990/2000 - Loss: -8.501
Iter 1991/2000 - Loss: -8.501
Iter 1992/2000 - Loss: -8.501
Iter 1993/2000 - Loss: -8.501
Iter 1994/2000 - Loss: -8.501
Iter 1995/2000 - Loss: -8.501
Iter 1996/2000 - Loss: -8.501
Iter 1997/2000 - Loss: -8.501
Iter 1998/2000 - Loss: -8.501
Iter 1999/2000 - Loss: -8.501
Iter 2000/2000 - Loss: -8.501
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[11.3130,  3.5259, 41.3267,  5.3337,  9.2167, 35.1716]],

        [[13.3829, 18.9569, 14.5668,  1.0973,  4.5721, 19.7014]],

        [[13.1919, 23.9150, 10.3443,  1.0863,  5.3881, 14.1284]],

        [[10.6215, 15.9237, 18.1330,  4.8781,  1.2573, 32.6776]]])
Signal Variance: tensor([ 0.0377,  1.7918, 14.0569,  0.5801])
Estimated target variance: tensor([0.0024, 0.1042, 1.5780, 0.0201])
N: 190
Signal to noise ratio: tensor([10.3692, 68.8993, 82.7616, 45.8031])
Bound on condition number: tensor([  20429.7384,  901953.5087, 1301402.8303,  398606.6862])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0005762601476985045, policy loss: 7.640680361012288
Experience 19, Iter 1, disc loss: 0.0006129403703581782, policy loss: 7.563170942232804
Experience 19, Iter 2, disc loss: 0.0005956102287962126, policy loss: 7.574629825170772
Experience 19, Iter 3, disc loss: 0.000609815748984309, policy loss: 7.5528766006790455
Experience 19, Iter 4, disc loss: 0.0005972007353084476, policy loss: 7.583840350759431
Experience 19, Iter 5, disc loss: 0.0005984448240789589, policy loss: 7.588682705063912
Experience 19, Iter 6, disc loss: 0.0006124103661590158, policy loss: 7.557160026251347
Experience 19, Iter 7, disc loss: 0.0005586582460216794, policy loss: 7.651477800599427
Experience 19, Iter 8, disc loss: 0.00046436999028987887, policy loss: 7.89759110953809
Experience 19, Iter 9, disc loss: 0.0004860370124933318, policy loss: 7.824974292245425
Experience 19, Iter 10, disc loss: 0.0005231856614746271, policy loss: 7.734075934264697
Experience 19, Iter 11, disc loss: 0.0005882718190746927, policy loss: 7.599502243610635
Experience 19, Iter 12, disc loss: 0.000545373079515023, policy loss: 7.692800995916375
Experience 19, Iter 13, disc loss: 0.0005301025858727987, policy loss: 7.712758196963607
Experience 19, Iter 14, disc loss: 0.0005763149049547632, policy loss: 7.628445436659337
Experience 19, Iter 15, disc loss: 0.0005235858467352159, policy loss: 7.7332443549113306
Experience 19, Iter 16, disc loss: 0.0004882928126743466, policy loss: 7.810541643974924
Experience 19, Iter 17, disc loss: 0.0005216168351568891, policy loss: 7.73633518218027
Experience 19, Iter 18, disc loss: 0.0005639218278901704, policy loss: 7.656985252081817
Experience 19, Iter 19, disc loss: 0.0005472895752331817, policy loss: 7.6829134188817205
Experience 19, Iter 20, disc loss: 0.0004962399117915931, policy loss: 7.7889022708744475
Experience 19, Iter 21, disc loss: 0.0005544320533415353, policy loss: 7.652812178565817
Experience 19, Iter 22, disc loss: 0.0005893979534121041, policy loss: 7.583081631297548
Experience 19, Iter 23, disc loss: 0.0005427109363527581, policy loss: 7.689906448483825
Experience 19, Iter 24, disc loss: 0.0005158927131722139, policy loss: 7.7526958490275595
Experience 19, Iter 25, disc loss: 0.0004892459821389203, policy loss: 7.807967623925897
Experience 19, Iter 26, disc loss: 0.0005404659905656108, policy loss: 7.692459734403865
Experience 19, Iter 27, disc loss: 0.0005750458060244073, policy loss: 7.614446225969829
Experience 19, Iter 28, disc loss: 0.000518270862752691, policy loss: 7.730935490550127
Experience 19, Iter 29, disc loss: 0.0005452635189119332, policy loss: 7.689621475012988
Experience 19, Iter 30, disc loss: 0.0005663259441046422, policy loss: 7.626368285792574
Experience 19, Iter 31, disc loss: 0.0005682286867130624, policy loss: 7.623098681263916
Experience 19, Iter 32, disc loss: 0.0005617396000825388, policy loss: 7.640292785411296
Experience 19, Iter 33, disc loss: 0.0005598341989982856, policy loss: 7.6479891805879365
Experience 19, Iter 34, disc loss: 0.0005755704311201069, policy loss: 7.613250207595129
Experience 19, Iter 35, disc loss: 0.0005714332278633105, policy loss: 7.62800601665654
Experience 19, Iter 36, disc loss: 0.0005564867295379874, policy loss: 7.642420745443237
Experience 19, Iter 37, disc loss: 0.0005861634512524848, policy loss: 7.589357380761642
Experience 19, Iter 38, disc loss: 0.0005543937763541112, policy loss: 7.665661506701337
Experience 19, Iter 39, disc loss: 0.0005621341990201007, policy loss: 7.642140221497311
Experience 19, Iter 40, disc loss: 0.0005717946953629821, policy loss: 7.627708396211309
Experience 19, Iter 41, disc loss: 0.0005922095860288479, policy loss: 7.576887014345811
Experience 19, Iter 42, disc loss: 0.0005655134372145066, policy loss: 7.624746136200635
Experience 19, Iter 43, disc loss: 0.0006033333375958181, policy loss: 7.568651284843103
Experience 19, Iter 44, disc loss: 0.0005664735401603415, policy loss: 7.6277790415275195
Experience 19, Iter 45, disc loss: 0.0005921765791286076, policy loss: 7.585287161111646
Experience 19, Iter 46, disc loss: 0.0005544551193602729, policy loss: 7.641404639187606
Experience 19, Iter 47, disc loss: 0.0005800872225601178, policy loss: 7.59315887720921
Experience 19, Iter 48, disc loss: 0.000568718018332629, policy loss: 7.62131314882101
Experience 19, Iter 49, disc loss: 0.0005700156620806066, policy loss: 7.629230211230427
Experience 19, Iter 50, disc loss: 0.0005686380450822916, policy loss: 7.625510025116929
Experience 19, Iter 51, disc loss: 0.0005992898267063703, policy loss: 7.576410205308553
Experience 19, Iter 52, disc loss: 0.0005572876132639124, policy loss: 7.647011155060902
Experience 19, Iter 53, disc loss: 0.0005248889863691352, policy loss: 7.719690080047055
Experience 19, Iter 54, disc loss: 0.000552674848068842, policy loss: 7.687278685133204
Experience 19, Iter 55, disc loss: 0.0005783057132763653, policy loss: 7.612496388762848
Experience 19, Iter 56, disc loss: 0.0005574673258473525, policy loss: 7.64439042894916
Experience 19, Iter 57, disc loss: 0.0005700504165617594, policy loss: 7.625005525751273
Experience 19, Iter 58, disc loss: 0.0005771537812816409, policy loss: 7.616320480229698
Experience 19, Iter 59, disc loss: 0.0005490364310170423, policy loss: 7.680322436735404
Experience 19, Iter 60, disc loss: 0.0005520036371950296, policy loss: 7.666094321531425
Experience 19, Iter 61, disc loss: 0.0005459497032417841, policy loss: 7.686310818373906
Experience 19, Iter 62, disc loss: 0.0005891692619678722, policy loss: 7.580373610623268
Experience 19, Iter 63, disc loss: 0.0005558118688619019, policy loss: 7.6686833233425755
Experience 19, Iter 64, disc loss: 0.0005163995610090197, policy loss: 7.729205657884448
Experience 19, Iter 65, disc loss: 0.0005588669547018071, policy loss: 7.648991341744997
Experience 19, Iter 66, disc loss: 0.0005648221959742148, policy loss: 7.644955530421536
Experience 19, Iter 67, disc loss: 0.0005740135494313517, policy loss: 7.610479719253831
Experience 19, Iter 68, disc loss: 0.0005639053694845725, policy loss: 7.630547820167854
Experience 19, Iter 69, disc loss: 0.0005588873893913983, policy loss: 7.648667936089637
Experience 19, Iter 70, disc loss: 0.0005663432304848551, policy loss: 7.640678699968266
Experience 19, Iter 71, disc loss: 0.0005315692158026444, policy loss: 7.707963853208886
Experience 19, Iter 72, disc loss: 0.0005727688176278653, policy loss: 7.630626422401956
Experience 19, Iter 73, disc loss: 0.0005498426843016472, policy loss: 7.656828869090582
Experience 19, Iter 74, disc loss: 0.0005463769478237533, policy loss: 7.668599760869304
Experience 19, Iter 75, disc loss: 0.0005591891319021918, policy loss: 7.654451594583503
Experience 19, Iter 76, disc loss: 0.0005867354741979748, policy loss: 7.606719640540174
Experience 19, Iter 77, disc loss: 0.0005802221615307435, policy loss: 7.59804836552288
Experience 19, Iter 78, disc loss: 0.0005263074148708667, policy loss: 7.707670811092454
Experience 19, Iter 79, disc loss: 0.0005328483930649746, policy loss: 7.698882002959618
Experience 19, Iter 80, disc loss: 0.0005341932197416056, policy loss: 7.696420811030997
Experience 19, Iter 81, disc loss: 0.0005169790027567861, policy loss: 7.729882291532531
Experience 19, Iter 82, disc loss: 0.0005298409951697323, policy loss: 7.707141732380714
Experience 19, Iter 83, disc loss: 0.0005401697306805381, policy loss: 7.672874822415176
Experience 19, Iter 84, disc loss: 0.0005479656460558839, policy loss: 7.660005060986119
Experience 19, Iter 85, disc loss: 0.0005265366198716384, policy loss: 7.707187246933566
Experience 19, Iter 86, disc loss: 0.0005183137648546396, policy loss: 7.739632871562653
Experience 19, Iter 87, disc loss: 0.0005653857787724999, policy loss: 7.619593584939591
Experience 19, Iter 88, disc loss: 0.0005569775438960041, policy loss: 7.635316805705159
Experience 19, Iter 89, disc loss: 0.0005447129856011647, policy loss: 7.669357665234282
Experience 19, Iter 90, disc loss: 0.0005575223948556081, policy loss: 7.640705133980945
Experience 19, Iter 91, disc loss: 0.0005098147309963969, policy loss: 7.744862247636394
Experience 19, Iter 92, disc loss: 0.0005363426267044886, policy loss: 7.679321031005156
Experience 19, Iter 93, disc loss: 0.0005435518698638137, policy loss: 7.672333613601245
Experience 19, Iter 94, disc loss: 0.0005438323813693581, policy loss: 7.6872929823668805
Experience 19, Iter 95, disc loss: 0.0005632107707515159, policy loss: 7.630437902738467
Experience 19, Iter 96, disc loss: 0.0005359147412407321, policy loss: 7.689060548939127
Experience 19, Iter 97, disc loss: 0.0005347492011745936, policy loss: 7.686464166269316
Experience 19, Iter 98, disc loss: 0.0005265246153024408, policy loss: 7.709763205823402
Experience 19, Iter 99, disc loss: 0.0005333775123405192, policy loss: 7.687354279359488
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0267],
        [0.4013],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0452e-03, 2.2764e-02, 2.3419e-01, 7.8643e-03, 4.7839e-04,
          5.8542e-01]],

        [[3.0452e-03, 2.2764e-02, 2.3419e-01, 7.8643e-03, 4.7839e-04,
          5.8542e-01]],

        [[3.0452e-03, 2.2764e-02, 2.3419e-01, 7.8643e-03, 4.7839e-04,
          5.8542e-01]],

        [[3.0452e-03, 2.2764e-02, 2.3419e-01, 7.8643e-03, 4.7839e-04,
          5.8542e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0024, 0.1068, 1.6050, 0.0199], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0024, 0.1068, 1.6050, 0.0199])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.038
Iter 2/2000 - Loss: 1.149
Iter 3/2000 - Loss: -0.055
Iter 4/2000 - Loss: 0.083
Iter 5/2000 - Loss: 0.487
Iter 6/2000 - Loss: 0.294
Iter 7/2000 - Loss: -0.036
Iter 8/2000 - Loss: -0.119
Iter 9/2000 - Loss: 0.012
Iter 10/2000 - Loss: 0.132
Iter 11/2000 - Loss: 0.120
Iter 12/2000 - Loss: 0.017
Iter 13/2000 - Loss: -0.083
Iter 14/2000 - Loss: -0.125
Iter 15/2000 - Loss: -0.107
Iter 16/2000 - Loss: -0.071
Iter 17/2000 - Loss: -0.067
Iter 18/2000 - Loss: -0.120
Iter 19/2000 - Loss: -0.211
Iter 20/2000 - Loss: -0.297
Iter 1981/2000 - Loss: -8.561
Iter 1982/2000 - Loss: -8.561
Iter 1983/2000 - Loss: -8.561
Iter 1984/2000 - Loss: -8.561
Iter 1985/2000 - Loss: -8.561
Iter 1986/2000 - Loss: -8.561
Iter 1987/2000 - Loss: -8.561
Iter 1988/2000 - Loss: -8.561
Iter 1989/2000 - Loss: -8.561
Iter 1990/2000 - Loss: -8.561
Iter 1991/2000 - Loss: -8.561
Iter 1992/2000 - Loss: -8.561
Iter 1993/2000 - Loss: -8.561
Iter 1994/2000 - Loss: -8.561
Iter 1995/2000 - Loss: -8.561
Iter 1996/2000 - Loss: -8.561
Iter 1997/2000 - Loss: -8.561
Iter 1998/2000 - Loss: -8.561
Iter 1999/2000 - Loss: -8.561
Iter 2000/2000 - Loss: -8.561
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[10.9336,  3.4319, 40.3258,  5.0514,  8.9376, 34.3060]],

        [[12.9370, 18.0084, 15.0663,  1.0861,  4.5813, 17.0540]],

        [[12.7712, 23.5028, 10.4318,  1.0990,  5.2317, 14.0868]],

        [[10.6727, 16.0837, 18.2669,  5.0073,  1.2247, 32.2288]]])
Signal Variance: tensor([ 0.0362,  1.5751, 14.2601,  0.5834])
Estimated target variance: tensor([0.0024, 0.1068, 1.6050, 0.0199])
N: 200
Signal to noise ratio: tensor([10.2564, 64.7709, 84.4106, 46.0454])
Bound on condition number: tensor([  21039.8013,  839055.7806, 1425030.9026,  424036.7795])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0005626909206418622, policy loss: 7.626992379726037
Experience 20, Iter 1, disc loss: 0.0005006762859760918, policy loss: 7.7571147558799325
Experience 20, Iter 2, disc loss: 0.0005479157983669056, policy loss: 7.6546106449928875
Experience 20, Iter 3, disc loss: 0.000459340862403732, policy loss: 7.856365201300641
Experience 20, Iter 4, disc loss: 0.0004599535642225148, policy loss: 7.8604601679069015
Experience 20, Iter 5, disc loss: 0.00045638135224271563, policy loss: 7.879575939714892
Experience 20, Iter 6, disc loss: 0.0005277319118338479, policy loss: 7.6926661053796686
Experience 20, Iter 7, disc loss: 0.00039100708621487657, policy loss: 8.071191254936828
Experience 20, Iter 8, disc loss: 0.0005141732923123153, policy loss: 7.736798015970315
Experience 20, Iter 9, disc loss: 0.0005195319100018752, policy loss: 7.716322699141613
Experience 20, Iter 10, disc loss: 0.0004705341426423854, policy loss: 7.849840727648399
Experience 20, Iter 11, disc loss: 0.00046791342853538755, policy loss: 7.842227104643771
Experience 20, Iter 12, disc loss: 0.00048453675210715096, policy loss: 7.794762346520201
Experience 20, Iter 13, disc loss: 0.0005266794177504969, policy loss: 7.696461516376847
Experience 20, Iter 14, disc loss: 0.0004846001714649598, policy loss: 7.795372395541193
Experience 20, Iter 15, disc loss: 0.00048715675609750544, policy loss: 7.787197558064641
Experience 20, Iter 16, disc loss: 0.0005239255482176189, policy loss: 7.69815615018565
Experience 20, Iter 17, disc loss: 0.0005211983839559608, policy loss: 7.711124681698948
Experience 20, Iter 18, disc loss: 0.0005037790799728971, policy loss: 7.762181173216816
Experience 20, Iter 19, disc loss: 0.0004919629415915852, policy loss: 7.777570987508418
Experience 20, Iter 20, disc loss: 0.0005100968541606561, policy loss: 7.74899872830667
Experience 20, Iter 21, disc loss: 0.0004731477920987408, policy loss: 7.811595722014379
Experience 20, Iter 22, disc loss: 0.0005201518909606436, policy loss: 7.70355406846225
Experience 20, Iter 23, disc loss: 0.00048748019631574343, policy loss: 7.785619207964754
Experience 20, Iter 24, disc loss: 0.0004411234261536968, policy loss: 7.941836310137841
Experience 20, Iter 25, disc loss: 0.000492396015996186, policy loss: 7.787883858769842
Experience 20, Iter 26, disc loss: 0.0005433256508601594, policy loss: 7.664591557404555
Experience 20, Iter 27, disc loss: 0.0004987735254787245, policy loss: 7.754062532370736
Experience 20, Iter 28, disc loss: 0.0005091039170083395, policy loss: 7.7382250487268305
Experience 20, Iter 29, disc loss: 0.000522380172766943, policy loss: 7.701308264480993
Experience 20, Iter 30, disc loss: 0.0005003396277791432, policy loss: 7.757395050088409
Experience 20, Iter 31, disc loss: 0.0004985256940455228, policy loss: 7.756054260602048
Experience 20, Iter 32, disc loss: 0.000499975350467971, policy loss: 7.74983588023117
Experience 20, Iter 33, disc loss: 0.0005131585536162436, policy loss: 7.722714064758777
Experience 20, Iter 34, disc loss: 0.00048496368211970944, policy loss: 7.7828455152382245
Experience 20, Iter 35, disc loss: 0.0005055963383361325, policy loss: 7.744198230396718
Experience 20, Iter 36, disc loss: 0.0005211721121811989, policy loss: 7.711556001502482
Experience 20, Iter 37, disc loss: 0.0005149740218338138, policy loss: 7.725016926626587
Experience 20, Iter 38, disc loss: 0.0005054963615152856, policy loss: 7.751852971084983
Experience 20, Iter 39, disc loss: 0.0005215819780053777, policy loss: 7.7122314897801365
Experience 20, Iter 40, disc loss: 0.000505784080067708, policy loss: 7.750552527985818
Experience 20, Iter 41, disc loss: 0.0005130456870311288, policy loss: 7.7286135350998615
Experience 20, Iter 42, disc loss: 0.0004929538849841417, policy loss: 7.769288403818887
Experience 20, Iter 43, disc loss: 0.0004862526668783391, policy loss: 7.792052357473985
Experience 20, Iter 44, disc loss: 0.0005178167961486897, policy loss: 7.715022810958824
Experience 20, Iter 45, disc loss: 0.0005076148230631, policy loss: 7.738214260629208
Experience 20, Iter 46, disc loss: 0.00046674707001465276, policy loss: 7.836043350894483
Experience 20, Iter 47, disc loss: 0.0004898962942705659, policy loss: 7.792545512252328
Experience 20, Iter 48, disc loss: 0.0004654940061792416, policy loss: 7.845494165273864
Experience 20, Iter 49, disc loss: 0.0004377505651865515, policy loss: 7.907275217265991
Experience 20, Iter 50, disc loss: 0.000440419216708838, policy loss: 7.896355047557268
Experience 20, Iter 51, disc loss: 0.0005093969668747522, policy loss: 7.742716093651234
Experience 20, Iter 52, disc loss: 0.0004913308049883245, policy loss: 7.772558014848894
Experience 20, Iter 53, disc loss: 0.00047819756361564686, policy loss: 7.814861284952125
Experience 20, Iter 54, disc loss: 0.00047561280090160435, policy loss: 7.8295408382024565
Experience 20, Iter 55, disc loss: 0.00044491087054101614, policy loss: 7.899914423970208
Experience 20, Iter 56, disc loss: 0.00039814376860450106, policy loss: 8.02457135407379
Experience 20, Iter 57, disc loss: 0.00042355260203196866, policy loss: 7.954879596184865
Experience 20, Iter 58, disc loss: 0.0004540427248344817, policy loss: 7.862371674639137
Experience 20, Iter 59, disc loss: 0.00046886215329505754, policy loss: 7.831052751191162
Experience 20, Iter 60, disc loss: 0.00044743676844468256, policy loss: 7.880451490372947
Experience 20, Iter 61, disc loss: 0.0004487728607800161, policy loss: 7.881479273481537
Experience 20, Iter 62, disc loss: 0.0004617204362481032, policy loss: 7.849786388122694
Experience 20, Iter 63, disc loss: 0.0004404130594506377, policy loss: 7.9232869672631825
Experience 20, Iter 64, disc loss: 0.0004406046991935227, policy loss: 7.909747163040872
Experience 20, Iter 65, disc loss: 0.00043825861454231283, policy loss: 7.905596651102237
Experience 20, Iter 66, disc loss: 0.0004374038974019419, policy loss: 7.916103425046083
Experience 20, Iter 67, disc loss: 0.0004816362647994296, policy loss: 7.801043107561305
Experience 20, Iter 68, disc loss: 0.0004605059344358141, policy loss: 7.851804843363998
Experience 20, Iter 69, disc loss: 0.0004666641955697804, policy loss: 7.824315537016961
Experience 20, Iter 70, disc loss: 0.0004718514311698428, policy loss: 7.8189543866932105
Experience 20, Iter 71, disc loss: 0.0004881913818702683, policy loss: 7.78608022713482
Experience 20, Iter 72, disc loss: 0.00047065872423433417, policy loss: 7.813102780168555
Experience 20, Iter 73, disc loss: 0.00046232305899137217, policy loss: 7.853311137408289
Experience 20, Iter 74, disc loss: 0.0004633656146560472, policy loss: 7.83395724920566
Experience 20, Iter 75, disc loss: 0.0004676527746886599, policy loss: 7.843257376523026
Experience 20, Iter 76, disc loss: 0.0004558555924840531, policy loss: 7.855940675999512
Experience 20, Iter 77, disc loss: 0.00046850347352564735, policy loss: 7.8308863645662115
Experience 20, Iter 78, disc loss: 0.0004459287862942035, policy loss: 7.88102003978493
Experience 20, Iter 79, disc loss: 0.0004441536933306081, policy loss: 7.875973989381464
Experience 20, Iter 80, disc loss: 0.00047385812363871625, policy loss: 7.8261454600434845
Experience 20, Iter 81, disc loss: 0.00046900532025427003, policy loss: 7.837469754544401
Experience 20, Iter 82, disc loss: 0.0004869032297590217, policy loss: 7.7828258068796465
Experience 20, Iter 83, disc loss: 0.00047490038824937557, policy loss: 7.802832888658584
Experience 20, Iter 84, disc loss: 0.0005066734825902503, policy loss: 7.726284752870969
Experience 20, Iter 85, disc loss: 0.00045324233717105255, policy loss: 7.8615634015974525
Experience 20, Iter 86, disc loss: 0.00046396450879528084, policy loss: 7.864575024221147
Experience 20, Iter 87, disc loss: 0.0004642435660701072, policy loss: 7.838203821789666
Experience 20, Iter 88, disc loss: 0.000476459836993864, policy loss: 7.832406322588817
Experience 20, Iter 89, disc loss: 0.0005049905212107615, policy loss: 7.733939977405229
Experience 20, Iter 90, disc loss: 0.000426789590562732, policy loss: 7.932330503898994
Experience 20, Iter 91, disc loss: 0.00046938040181272386, policy loss: 7.821401009116141
Experience 20, Iter 92, disc loss: 0.0004139164596926059, policy loss: 7.983085819647967
Experience 20, Iter 93, disc loss: 0.00044471223079766277, policy loss: 7.889193367420205
Experience 20, Iter 94, disc loss: 0.0004341163986911658, policy loss: 7.908574353392345
Experience 20, Iter 95, disc loss: 0.00044832849193445704, policy loss: 7.875887220104983
Experience 20, Iter 96, disc loss: 0.0004791316213288753, policy loss: 7.804325920935506
Experience 20, Iter 97, disc loss: 0.0003946983113857977, policy loss: 8.010833850045813
Experience 20, Iter 98, disc loss: 0.0004915228345076216, policy loss: 7.763251168998547
Experience 20, Iter 99, disc loss: 0.0004427268753257203, policy loss: 7.8883352879495225
