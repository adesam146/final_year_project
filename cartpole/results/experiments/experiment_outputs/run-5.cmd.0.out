Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0011],
        [0.0916],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0052, 0.0042, 0.3664, 0.0094], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0052, 0.0042, 0.3664, 0.0094])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 5.527
Iter 2/2000 - Loss: 0.375
Iter 3/2000 - Loss: -1.953
Iter 4/2000 - Loss: -1.974
Iter 5/2000 - Loss: -1.400
Iter 6/2000 - Loss: -1.043
Iter 7/2000 - Loss: -0.949
Iter 8/2000 - Loss: -0.997
Iter 9/2000 - Loss: -1.104
Iter 10/2000 - Loss: -1.276
Iter 11/2000 - Loss: -1.548
Iter 12/2000 - Loss: -1.886
Iter 13/2000 - Loss: -2.187
Iter 14/2000 - Loss: -2.357
Iter 15/2000 - Loss: -2.363
Iter 16/2000 - Loss: -2.253
Iter 17/2000 - Loss: -2.121
Iter 18/2000 - Loss: -2.033
Iter 19/2000 - Loss: -2.004
Iter 20/2000 - Loss: -2.017
Iter 1981/2000 - Loss: -2.732
Iter 1982/2000 - Loss: -2.732
Iter 1983/2000 - Loss: -2.732
Iter 1984/2000 - Loss: -2.732
Iter 1985/2000 - Loss: -2.732
Iter 1986/2000 - Loss: -2.732
Iter 1987/2000 - Loss: -2.732
Iter 1988/2000 - Loss: -2.732
Iter 1989/2000 - Loss: -2.732
Iter 1990/2000 - Loss: -2.732
Iter 1991/2000 - Loss: -2.732
Iter 1992/2000 - Loss: -2.732
Iter 1993/2000 - Loss: -2.732
Iter 1994/2000 - Loss: -2.732
Iter 1995/2000 - Loss: -2.732
Iter 1996/2000 - Loss: -2.732
Iter 1997/2000 - Loss: -2.732
Iter 1998/2000 - Loss: -2.732
Iter 1999/2000 - Loss: -2.732
Iter 2000/2000 - Loss: -2.732
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0008],
        [0.0617],
        [0.0017]])
Lengthscale: tensor([[[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]],

        [[0.0140, 0.0607, 0.0939, 0.0023, 0.0002, 0.0219]]])
Signal Variance: tensor([0.0038, 0.0030, 0.2681, 0.0068])
Estimated target variance: tensor([0.0052, 0.0042, 0.3664, 0.0094])
N: 10
Signal to noise ratio: tensor([2.0017, 2.0013, 2.0845, 2.0023])
Bound on condition number: tensor([41.0667, 41.0521, 44.4499, 41.0908])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.5117474957531472, policy loss: 0.7465729888971037
Experience 1, Iter 1, disc loss: 1.4998986607960376, policy loss: 0.7428866444294835
Experience 1, Iter 2, disc loss: 1.4840517233875707, policy loss: 0.7441949438958122
Experience 1, Iter 3, disc loss: 1.4704344664536584, policy loss: 0.743400853018076
Experience 1, Iter 4, disc loss: 1.4589652690488322, policy loss: 0.7410111971824775
Experience 1, Iter 5, disc loss: 1.441210529861428, policy loss: 0.7479588543008511
Experience 1, Iter 6, disc loss: 1.4345173620468659, policy loss: 0.7481300473193485
Experience 1, Iter 7, disc loss: 1.430137579529948, policy loss: 0.7470081451018618
Experience 1, Iter 8, disc loss: 1.4263159368152132, policy loss: 0.7454236141138867
Experience 1, Iter 9, disc loss: 1.4209075712480044, policy loss: 0.7456423541854647
Experience 1, Iter 10, disc loss: 1.4132551149482862, policy loss: 0.7485339478991004
Experience 1, Iter 11, disc loss: 1.409430585369026, policy loss: 0.747294969145334
Experience 1, Iter 12, disc loss: 1.4020911123163047, policy loss: 0.7500599182078894
Experience 1, Iter 13, disc loss: 1.4013784805192167, policy loss: 0.7454058758983978
Experience 1, Iter 14, disc loss: 1.388068154027033, policy loss: 0.7551213357079117
Experience 1, Iter 15, disc loss: 1.38512660129367, policy loss: 0.7533053392943567
Experience 1, Iter 16, disc loss: 1.38235815840097, policy loss: 0.7513403637522731
Experience 1, Iter 17, disc loss: 1.3758685871068332, policy loss: 0.7536431449252917
Experience 1, Iter 18, disc loss: 1.3724361036272332, policy loss: 0.752559402766227
Experience 1, Iter 19, disc loss: 1.364935790390199, policy loss: 0.7563709809442997
Experience 1, Iter 20, disc loss: 1.3580864834869943, policy loss: 0.7593576420866789
Experience 1, Iter 21, disc loss: 1.3543463367040323, policy loss: 0.7589958015335014
Experience 1, Iter 22, disc loss: 1.3509885966849402, policy loss: 0.7580385541068617
Experience 1, Iter 23, disc loss: 1.3458409341649686, policy loss: 0.7593735498358358
Experience 1, Iter 24, disc loss: 1.3387851437068097, policy loss: 0.762884523277234
Experience 1, Iter 25, disc loss: 1.3312055184536196, policy loss: 0.7670102787138584
Experience 1, Iter 26, disc loss: 1.3279178791799948, policy loss: 0.7664098647369192
Experience 1, Iter 27, disc loss: 1.3247619437123475, policy loss: 0.7653180388605005
Experience 1, Iter 28, disc loss: 1.3114039103809658, policy loss: 0.7765268850652801
Experience 1, Iter 29, disc loss: 1.3063219512408923, policy loss: 0.7779058644011962
Experience 1, Iter 30, disc loss: 1.3022508105107473, policy loss: 0.7784157551831563
Experience 1, Iter 31, disc loss: 1.2993394124477926, policy loss: 0.7788206940839761
Experience 1, Iter 32, disc loss: 1.2962535749015056, policy loss: 0.7804954553883559
Experience 1, Iter 33, disc loss: 1.2902293122309394, policy loss: 0.7865968837838264
Experience 1, Iter 34, disc loss: 1.2887081065976518, policy loss: 0.7867975735586592
Experience 1, Iter 35, disc loss: 1.2834551075290763, policy loss: 0.7914060738506277
Experience 1, Iter 36, disc loss: 1.2757169111234834, policy loss: 0.7987939988176043
Experience 1, Iter 37, disc loss: 1.2726325586639384, policy loss: 0.8004097142943397
Experience 1, Iter 38, disc loss: 1.271381349230239, policy loss: 0.7990081477669652
Experience 1, Iter 39, disc loss: 1.2710170807676122, policy loss: 0.7967588221088879
Experience 1, Iter 40, disc loss: 1.2616179890460892, policy loss: 0.8054553116695736
Experience 1, Iter 41, disc loss: 1.2576294053820032, policy loss: 0.8073043987386453
Experience 1, Iter 42, disc loss: 1.2526746783024012, policy loss: 0.809957462456978
Experience 1, Iter 43, disc loss: 1.247311130575048, policy loss: 0.8129968986769995
Experience 1, Iter 44, disc loss: 1.2352059365391677, policy loss: 0.8245206205446923
Experience 1, Iter 45, disc loss: 1.2306179879102088, policy loss: 0.8263331579962326
Experience 1, Iter 46, disc loss: 1.229534554488267, policy loss: 0.8233234422995938
Experience 1, Iter 47, disc loss: 1.2196401429981294, policy loss: 0.8311841697683067
Experience 1, Iter 48, disc loss: 1.20657744032306, policy loss: 0.8428150512750658
Experience 1, Iter 49, disc loss: 1.2020629951172874, policy loss: 0.8437967871637839
Experience 1, Iter 50, disc loss: 1.1992357628466306, policy loss: 0.8416322356010624
Experience 1, Iter 51, disc loss: 1.1842229664264017, policy loss: 0.8559320485260244
Experience 1, Iter 52, disc loss: 1.1782044940780914, policy loss: 0.8577653411239498
Experience 1, Iter 53, disc loss: 1.171893546849192, policy loss: 0.8598523243677552
Experience 1, Iter 54, disc loss: 1.1617991099982445, policy loss: 0.86630262111266
Experience 1, Iter 55, disc loss: 1.1536902883838847, policy loss: 0.8697695880613296
Experience 1, Iter 56, disc loss: 1.1579958656778695, policy loss: 0.8568054248220541
Experience 1, Iter 57, disc loss: 1.1362899090782734, policy loss: 0.8786700761363059
Experience 1, Iter 58, disc loss: 1.1228602151194536, policy loss: 0.8891282143510002
Experience 1, Iter 59, disc loss: 1.1181041155050386, policy loss: 0.8873032090795137
Experience 1, Iter 60, disc loss: 1.1105931678063425, policy loss: 0.8887315770707671
Experience 1, Iter 61, disc loss: 1.0888374963472938, policy loss: 0.9108052927769007
Experience 1, Iter 62, disc loss: 1.0783524419909436, policy loss: 0.9162987190048602
Experience 1, Iter 63, disc loss: 1.0713235792538631, policy loss: 0.9164024092153651
Experience 1, Iter 64, disc loss: 1.0617191941233193, policy loss: 0.9200231534107104
Experience 1, Iter 65, disc loss: 1.0447859352123576, policy loss: 0.9342055991201546
Experience 1, Iter 66, disc loss: 1.02872842613119, policy loss: 0.9473061002473172
Experience 1, Iter 67, disc loss: 1.027007644770019, policy loss: 0.9380394884910976
Experience 1, Iter 68, disc loss: 1.0125641719720986, policy loss: 0.9484064424268084
Experience 1, Iter 69, disc loss: 1.0055143891746243, policy loss: 0.9461930914587904
Experience 1, Iter 70, disc loss: 0.9832688827107263, policy loss: 0.9680607390580652
Experience 1, Iter 71, disc loss: 0.9609850765598276, policy loss: 0.9927623918389347
Experience 1, Iter 72, disc loss: 0.9608368142682702, policy loss: 0.9765083466881603
Experience 1, Iter 73, disc loss: 0.9393294929936868, policy loss: 0.9981954529458552
Experience 1, Iter 74, disc loss: 0.9242356293067646, policy loss: 1.0080131770215393
Experience 1, Iter 75, disc loss: 0.9091107725236144, policy loss: 1.0197310643168158
Experience 1, Iter 76, disc loss: 0.8850791622123682, policy loss: 1.0461642354783067
Experience 1, Iter 77, disc loss: 0.8931518978114554, policy loss: 1.016476737106474
Experience 1, Iter 78, disc loss: 0.8721782021668971, policy loss: 1.03600082130037
Experience 1, Iter 79, disc loss: 0.8479617739670356, policy loss: 1.0627665527745853
Experience 1, Iter 80, disc loss: 0.8406008009685182, policy loss: 1.0600161689739362
Experience 1, Iter 81, disc loss: 0.8192859797556916, policy loss: 1.0822043200553082
Experience 1, Iter 82, disc loss: 0.804642580374724, policy loss: 1.0920758298090796
Experience 1, Iter 83, disc loss: 0.7899303690641288, policy loss: 1.1011012784604768
Experience 1, Iter 84, disc loss: 0.7814011166473736, policy loss: 1.0998826135231006
Experience 1, Iter 85, disc loss: 0.7614388894880348, policy loss: 1.1200134792403909
Experience 1, Iter 86, disc loss: 0.7387347772048616, policy loss: 1.147725256949097
Experience 1, Iter 87, disc loss: 0.7276602250172624, policy loss: 1.1503526613890278
Experience 1, Iter 88, disc loss: 0.7126132435646104, policy loss: 1.161419507388792
Experience 1, Iter 89, disc loss: 0.6947991383347242, policy loss: 1.1806422153312102
Experience 1, Iter 90, disc loss: 0.6804147106049424, policy loss: 1.1902271736921253
Experience 1, Iter 91, disc loss: 0.6681767967469352, policy loss: 1.194540669304986
Experience 1, Iter 92, disc loss: 0.6467736468512301, policy loss: 1.2271940160299861
Experience 1, Iter 93, disc loss: 0.6389259611866396, policy loss: 1.2198861633136118
Experience 1, Iter 94, disc loss: 0.6227201634635053, policy loss: 1.2411430918962663
Experience 1, Iter 95, disc loss: 0.5952500850922403, policy loss: 1.2896078973541438
Experience 1, Iter 96, disc loss: 0.5868321739878564, policy loss: 1.2852531333511723
Experience 1, Iter 97, disc loss: 0.5772130612236559, policy loss: 1.2890370003729454
Experience 1, Iter 98, disc loss: 0.5645991413321487, policy loss: 1.3044511937034984
Experience 1, Iter 99, disc loss: 0.5389153236557459, policy loss: 1.3476612213182633
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0027],
        [0.1713],
        [0.0037]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0070, 0.0110, 0.6852, 0.0146], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0070, 0.0110, 0.6852, 0.0146])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.106
Iter 2/2000 - Loss: 0.339
Iter 3/2000 - Loss: -0.933
Iter 4/2000 - Loss: -0.903
Iter 5/2000 - Loss: -0.626
Iter 6/2000 - Loss: -0.546
Iter 7/2000 - Loss: -0.577
Iter 8/2000 - Loss: -0.599
Iter 9/2000 - Loss: -0.582
Iter 10/2000 - Loss: -0.574
Iter 11/2000 - Loss: -0.637
Iter 12/2000 - Loss: -0.776
Iter 13/2000 - Loss: -0.939
Iter 14/2000 - Loss: -1.068
Iter 15/2000 - Loss: -1.137
Iter 16/2000 - Loss: -1.159
Iter 17/2000 - Loss: -1.151
Iter 18/2000 - Loss: -1.114
Iter 19/2000 - Loss: -1.057
Iter 20/2000 - Loss: -1.008
Iter 1981/2000 - Loss: -1.468
Iter 1982/2000 - Loss: -1.468
Iter 1983/2000 - Loss: -1.469
Iter 1984/2000 - Loss: -1.469
Iter 1985/2000 - Loss: -1.468
Iter 1986/2000 - Loss: -1.469
Iter 1987/2000 - Loss: -1.469
Iter 1988/2000 - Loss: -1.469
Iter 1989/2000 - Loss: -1.469
Iter 1990/2000 - Loss: -1.469
Iter 1991/2000 - Loss: -1.469
Iter 1992/2000 - Loss: -1.469
Iter 1993/2000 - Loss: -1.469
Iter 1994/2000 - Loss: -1.469
Iter 1995/2000 - Loss: -1.469
Iter 1996/2000 - Loss: -1.469
Iter 1997/2000 - Loss: -1.469
Iter 1998/2000 - Loss: -1.469
Iter 1999/2000 - Loss: -1.469
Iter 2000/2000 - Loss: -1.469
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0021],
        [0.1170],
        [0.0028]])
Lengthscale: tensor([[[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]],

        [[0.0198, 0.0679, 0.1639, 0.0038, 0.0004, 0.0358]]])
Signal Variance: tensor([0.0053, 0.0083, 0.5339, 0.0111])
Estimated target variance: tensor([0.0070, 0.0110, 0.6852, 0.0146])
N: 20
Signal to noise ratio: tensor([2.0024, 2.0045, 2.1364, 2.0048])
Bound on condition number: tensor([81.1940, 81.3635, 92.2810, 81.3819])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.5210696096666083, policy loss: 1.3788930619522124
Experience 2, Iter 1, disc loss: 0.48068026391880425, policy loss: 1.4804460262078498
Experience 2, Iter 2, disc loss: 0.4941907156151383, policy loss: 1.4216559026440714
Experience 2, Iter 3, disc loss: 0.4612516772372792, policy loss: 1.5067389301227134
Experience 2, Iter 4, disc loss: 0.4465831824241193, policy loss: 1.5222503170179666
Experience 2, Iter 5, disc loss: 0.43306200069170453, policy loss: 1.5497252181052774
Experience 2, Iter 6, disc loss: 0.4376861861197426, policy loss: 1.507857109642412
Experience 2, Iter 7, disc loss: 0.41933505104405217, policy loss: 1.5472702045833648
Experience 2, Iter 8, disc loss: 0.41062177030442915, policy loss: 1.5561390849824548
Experience 2, Iter 9, disc loss: 0.3787633372697798, policy loss: 1.6610568566865327
Experience 2, Iter 10, disc loss: 0.35978549087056166, policy loss: 1.7086131541317193
Experience 2, Iter 11, disc loss: 0.3592358653623409, policy loss: 1.6904318704228436
Experience 2, Iter 12, disc loss: 0.34276651348344034, policy loss: 1.7274121972964012
Experience 2, Iter 13, disc loss: 0.3335285923494631, policy loss: 1.750573469820828
Experience 2, Iter 14, disc loss: 0.32314660368140957, policy loss: 1.7721851025254824
Experience 2, Iter 15, disc loss: 0.31543700362505545, policy loss: 1.7895624225355513
Experience 2, Iter 16, disc loss: 0.30874825697863617, policy loss: 1.808960749947868
Experience 2, Iter 17, disc loss: 0.28757736527970423, policy loss: 1.8780821401373573
Experience 2, Iter 18, disc loss: 0.2861099168925722, policy loss: 1.8826048626771068
Experience 2, Iter 19, disc loss: 0.28398831255481555, policy loss: 1.8554109579363347
Experience 2, Iter 20, disc loss: 0.26609904773247206, policy loss: 1.9316765850266227
Experience 2, Iter 21, disc loss: 0.25968516501434086, policy loss: 1.9337007016124361
Experience 2, Iter 22, disc loss: 0.241167251225976, policy loss: 2.0240282974792123
Experience 2, Iter 23, disc loss: 0.2436231898940202, policy loss: 1.978655163845357
Experience 2, Iter 24, disc loss: 0.23077711668195366, policy loss: 2.051479845313082
Experience 2, Iter 25, disc loss: 0.23292708658448236, policy loss: 2.018666936426282
Experience 2, Iter 26, disc loss: 0.22552064746288142, policy loss: 2.0409181276530064
Experience 2, Iter 27, disc loss: 0.20606856512639454, policy loss: 2.1368526967294543
Experience 2, Iter 28, disc loss: 0.21576610090307524, policy loss: 2.07026053320591
Experience 2, Iter 29, disc loss: 0.20973492927781698, policy loss: 2.06765845544877
Experience 2, Iter 30, disc loss: 0.18364984087708686, policy loss: 2.2526201639141816
Experience 2, Iter 31, disc loss: 0.18254640826632593, policy loss: 2.2522431615964185
Experience 2, Iter 32, disc loss: 0.17767183154854416, policy loss: 2.2670578859061736
Experience 2, Iter 33, disc loss: 0.1646522418307794, policy loss: 2.3638151328660926
Experience 2, Iter 34, disc loss: 0.17803035089868074, policy loss: 2.234669674538962
Experience 2, Iter 35, disc loss: 0.17638379092200424, policy loss: 2.222898996080052
Experience 2, Iter 36, disc loss: 0.15527864809628095, policy loss: 2.3822623921298445
Experience 2, Iter 37, disc loss: 0.1600346440506673, policy loss: 2.3226512766043745
Experience 2, Iter 38, disc loss: 0.1558983177345938, policy loss: 2.3536418055527344
Experience 2, Iter 39, disc loss: 0.1426961300141387, policy loss: 2.445008599178361
Experience 2, Iter 40, disc loss: 0.13960342615801158, policy loss: 2.4676874740253383
Experience 2, Iter 41, disc loss: 0.1312449408840246, policy loss: 2.548447358282563
Experience 2, Iter 42, disc loss: 0.1320118585426979, policy loss: 2.5347938490653203
Experience 2, Iter 43, disc loss: 0.12595619282220566, policy loss: 2.5942949473439336
Experience 2, Iter 44, disc loss: 0.12469914528656496, policy loss: 2.561349827165154
Experience 2, Iter 45, disc loss: 0.11406237282366255, policy loss: 2.690013143998719
Experience 2, Iter 46, disc loss: 0.1105366645994911, policy loss: 2.7227133766162996
Experience 2, Iter 47, disc loss: 0.10801082412443361, policy loss: 2.74286545529932
Experience 2, Iter 48, disc loss: 0.11197102278210484, policy loss: 2.6546806178200653
Experience 2, Iter 49, disc loss: 0.1064914343184794, policy loss: 2.741137790591215
Experience 2, Iter 50, disc loss: 0.10468270912544742, policy loss: 2.7581086066311933
Experience 2, Iter 51, disc loss: 0.09211192355138748, policy loss: 2.894969220812561
Experience 2, Iter 52, disc loss: 0.09558143708700262, policy loss: 2.8349621865488217
Experience 2, Iter 53, disc loss: 0.09862616172244566, policy loss: 2.8040462450860755
Experience 2, Iter 54, disc loss: 0.09471457617021624, policy loss: 2.930673322466773
Experience 2, Iter 55, disc loss: 0.0863016164904724, policy loss: 2.9494971861351633
Experience 2, Iter 56, disc loss: 0.07978870217733788, policy loss: 3.0442317687412954
Experience 2, Iter 57, disc loss: 0.08009288239776818, policy loss: 3.047795485470701
Experience 2, Iter 58, disc loss: 0.07829105692425445, policy loss: 3.0487778525546525
Experience 2, Iter 59, disc loss: 0.0744073420951939, policy loss: 3.1143914650784383
Experience 2, Iter 60, disc loss: 0.07174329751439287, policy loss: 3.1687409513765834
Experience 2, Iter 61, disc loss: 0.07166978103847135, policy loss: 3.1271643208743756
Experience 2, Iter 62, disc loss: 0.07302501960828786, policy loss: 3.094316741758237
Experience 2, Iter 63, disc loss: 0.06877322076099668, policy loss: 3.2061105508970886
Experience 2, Iter 64, disc loss: 0.06737769854622554, policy loss: 3.20723587840921
Experience 2, Iter 65, disc loss: 0.06748273867810911, policy loss: 3.2072818953194324
Experience 2, Iter 66, disc loss: 0.06166744173566137, policy loss: 3.2846381509089753
Experience 2, Iter 67, disc loss: 0.06266387875216894, policy loss: 3.259991425506896
Experience 2, Iter 68, disc loss: 0.062101109671902424, policy loss: 3.2579840765571064
Experience 2, Iter 69, disc loss: 0.05802042065116708, policy loss: 3.331572322481674
Experience 2, Iter 70, disc loss: 0.05570491824966639, policy loss: 3.399030379664116
Experience 2, Iter 71, disc loss: 0.05841472409203585, policy loss: 3.351905472343386
Experience 2, Iter 72, disc loss: 0.05523178313495185, policy loss: 3.37942506915051
Experience 2, Iter 73, disc loss: 0.05010443317971577, policy loss: 3.540376117643278
Experience 2, Iter 74, disc loss: 0.051577676431252065, policy loss: 3.489236951487358
Experience 2, Iter 75, disc loss: 0.0530682801256888, policy loss: 3.470455579440287
Experience 2, Iter 76, disc loss: 0.05006580219675718, policy loss: 3.4910696333057687
Experience 2, Iter 77, disc loss: 0.045680610261830845, policy loss: 3.6390949724529174
Experience 2, Iter 78, disc loss: 0.04572964361285944, policy loss: 3.6504680955478026
Experience 2, Iter 79, disc loss: 0.045856844529823165, policy loss: 3.6231477333143465
Experience 2, Iter 80, disc loss: 0.042793556160053974, policy loss: 3.7074516987305666
Experience 2, Iter 81, disc loss: 0.04313012659750209, policy loss: 3.6907127600558507
Experience 2, Iter 82, disc loss: 0.04444648724678629, policy loss: 3.645179834233593
Experience 2, Iter 83, disc loss: 0.039299262215009276, policy loss: 3.7796347209174055
Experience 2, Iter 84, disc loss: 0.03787357227306907, policy loss: 3.8468333990767736
Experience 2, Iter 85, disc loss: 0.037350601344362744, policy loss: 3.8759198321074235
Experience 2, Iter 86, disc loss: 0.03927141597268038, policy loss: 3.765259946063824
Experience 2, Iter 87, disc loss: 0.034598998004409555, policy loss: 3.95811424351501
Experience 2, Iter 88, disc loss: 0.03748330193725537, policy loss: 3.8561068843993818
Experience 2, Iter 89, disc loss: 0.03528460679864932, policy loss: 3.923203442790467
Experience 2, Iter 90, disc loss: 0.035897083507160005, policy loss: 3.9080571501606123
Experience 2, Iter 91, disc loss: 0.033341654118564254, policy loss: 3.9897088813867265
Experience 2, Iter 92, disc loss: 0.03348969694422529, policy loss: 3.9701209733497707
Experience 2, Iter 93, disc loss: 0.03253132597361556, policy loss: 3.99635149064882
Experience 2, Iter 94, disc loss: 0.032350498306548923, policy loss: 4.03286557083824
Experience 2, Iter 95, disc loss: 0.03037486608528759, policy loss: 4.083944025363539
Experience 2, Iter 96, disc loss: 0.02867166963310058, policy loss: 4.189597830745059
Experience 2, Iter 97, disc loss: 0.0297560552507974, policy loss: 4.12253367774287
Experience 2, Iter 98, disc loss: 0.02786455535629614, policy loss: 4.291872961898008
Experience 2, Iter 99, disc loss: 0.02736946602640395, policy loss: 4.265329467981275
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0035],
        [0.2160],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0207, 0.0744, 0.2082, 0.0048, 0.0006, 0.0423]],

        [[0.0207, 0.0744, 0.2082, 0.0048, 0.0006, 0.0423]],

        [[0.0207, 0.0744, 0.2082, 0.0048, 0.0006, 0.0423]],

        [[0.0207, 0.0744, 0.2082, 0.0048, 0.0006, 0.0423]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0076, 0.0141, 0.8641, 0.0190], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0076, 0.0141, 0.8641, 0.0190])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.125
Iter 2/2000 - Loss: 0.620
Iter 3/2000 - Loss: -0.510
Iter 4/2000 - Loss: -0.504
Iter 5/2000 - Loss: -0.273
Iter 6/2000 - Loss: -0.215
Iter 7/2000 - Loss: -0.250
Iter 8/2000 - Loss: -0.270
Iter 9/2000 - Loss: -0.247
Iter 10/2000 - Loss: -0.229
Iter 11/2000 - Loss: -0.266
Iter 12/2000 - Loss: -0.358
Iter 13/2000 - Loss: -0.473
Iter 14/2000 - Loss: -0.578
Iter 15/2000 - Loss: -0.660
Iter 16/2000 - Loss: -0.721
Iter 17/2000 - Loss: -0.751
Iter 18/2000 - Loss: -0.737
Iter 19/2000 - Loss: -0.682
Iter 20/2000 - Loss: -0.622
Iter 1981/2000 - Loss: -7.362
Iter 1982/2000 - Loss: -7.362
Iter 1983/2000 - Loss: -7.362
Iter 1984/2000 - Loss: -7.362
Iter 1985/2000 - Loss: -7.362
Iter 1986/2000 - Loss: -7.362
Iter 1987/2000 - Loss: -7.362
Iter 1988/2000 - Loss: -7.362
Iter 1989/2000 - Loss: -7.362
Iter 1990/2000 - Loss: -7.362
Iter 1991/2000 - Loss: -7.362
Iter 1992/2000 - Loss: -7.362
Iter 1993/2000 - Loss: -7.362
Iter 1994/2000 - Loss: -7.362
Iter 1995/2000 - Loss: -7.362
Iter 1996/2000 - Loss: -7.362
Iter 1997/2000 - Loss: -7.362
Iter 1998/2000 - Loss: -7.362
Iter 1999/2000 - Loss: -7.362
Iter 2000/2000 - Loss: -7.362
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[16.1181,  3.3785, 49.3382, 15.9500, 11.2474, 27.3039]],

        [[21.5216, 29.7320, 10.3390,  0.3982,  0.6263,  2.1248]],

        [[32.4818, 30.6353,  8.6928,  0.5735,  4.9478,  5.5092]],

        [[32.0880, 60.6166,  9.6399,  2.7800,  8.8271, 35.8072]]])
Signal Variance: tensor([0.0430, 0.0318, 2.2315, 0.2720])
Estimated target variance: tensor([0.0076, 0.0141, 0.8641, 0.0190])
N: 30
Signal to noise ratio: tensor([ 9.8307,  9.4144, 38.1745, 34.2363])
Bound on condition number: tensor([ 2900.2941,  2659.9279, 43719.8673, 35164.6606])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.028438193441871065, policy loss: 4.097217937453317
Experience 3, Iter 1, disc loss: 0.029719940616205073, policy loss: 4.014903131285115
Experience 3, Iter 2, disc loss: 0.03311711085275175, policy loss: 3.834698677834883
Experience 3, Iter 3, disc loss: 0.03472888669334179, policy loss: 3.7565704302807936
Experience 3, Iter 4, disc loss: 0.0376217684807192, policy loss: 3.656982770124143
Experience 3, Iter 5, disc loss: 0.03985308974570338, policy loss: 3.5838696097250846
Experience 3, Iter 6, disc loss: 0.04072850062487461, policy loss: 3.536208038896141
Experience 3, Iter 7, disc loss: 0.04259921471524862, policy loss: 3.471148152448473
Experience 3, Iter 8, disc loss: 0.04597910091031648, policy loss: 3.3970759037782403
Experience 3, Iter 9, disc loss: 0.04636449446634842, policy loss: 3.4021030951130524
Experience 3, Iter 10, disc loss: 0.040187020025658486, policy loss: 3.5869542888795256
Experience 3, Iter 11, disc loss: 0.03934771712462795, policy loss: 3.6138898748935167
Experience 3, Iter 12, disc loss: 0.04103150074081073, policy loss: 3.571261993909079
Experience 3, Iter 13, disc loss: 0.04051572290704228, policy loss: 3.635430024703574
Experience 3, Iter 14, disc loss: 0.039247083457985305, policy loss: 3.694823995426556
Experience 3, Iter 15, disc loss: 0.03493128753651322, policy loss: 3.8134561636469653
Experience 3, Iter 16, disc loss: 0.035304914385626836, policy loss: 3.781480780172303
Experience 3, Iter 17, disc loss: 0.033951936567587326, policy loss: 3.8615061734063865
Experience 3, Iter 18, disc loss: 0.032830982816063455, policy loss: 3.8465053339886004
Experience 3, Iter 19, disc loss: 0.03116984912538126, policy loss: 3.8936787429983046
Experience 3, Iter 20, disc loss: 0.02990207794317315, policy loss: 3.9162572132156814
Experience 3, Iter 21, disc loss: 0.03254987412530515, policy loss: 3.852011815341204
Experience 3, Iter 22, disc loss: 0.03207863814033864, policy loss: 3.8290372458163757
Experience 3, Iter 23, disc loss: 0.029878635366465536, policy loss: 3.9118782446930713
Experience 3, Iter 24, disc loss: 0.028201739907893972, policy loss: 3.984218328201485
Experience 3, Iter 25, disc loss: 0.030163159428577543, policy loss: 3.9162961653231294
Experience 3, Iter 26, disc loss: 0.029582786512137308, policy loss: 3.9162257301253702
Experience 3, Iter 27, disc loss: 0.026402565444308426, policy loss: 4.068548713564829
Experience 3, Iter 28, disc loss: 0.02386323925148961, policy loss: 4.1763858101688065
Experience 3, Iter 29, disc loss: 0.026338028467857773, policy loss: 4.09686770467005
Experience 3, Iter 30, disc loss: 0.024605024086935275, policy loss: 4.191984536602351
Experience 3, Iter 31, disc loss: 0.02218597854947438, policy loss: 4.238791524571353
Experience 3, Iter 32, disc loss: 0.023652964907047946, policy loss: 4.236851467450629
Experience 3, Iter 33, disc loss: 0.026351676316435944, policy loss: 4.08085044475656
Experience 3, Iter 34, disc loss: 0.023120603221685507, policy loss: 4.216384134978144
Experience 3, Iter 35, disc loss: 0.03101048617128139, policy loss: 4.24954981427018
Experience 3, Iter 36, disc loss: 0.022337982999564028, policy loss: 4.2768445094411405
Experience 3, Iter 37, disc loss: 0.019134752502476374, policy loss: 4.490035763194543
Experience 3, Iter 38, disc loss: 0.01924111209258715, policy loss: 4.478034012806724
Experience 3, Iter 39, disc loss: 0.025225057158634535, policy loss: 4.2839508433452815
Experience 3, Iter 40, disc loss: 0.023896638162911897, policy loss: 4.355442600272425
Experience 3, Iter 41, disc loss: 0.02292440266263037, policy loss: 4.345260956028809
Experience 3, Iter 42, disc loss: 0.035484038445318396, policy loss: 4.411518886885913
Experience 3, Iter 43, disc loss: 0.03234397308313737, policy loss: 4.470138836386127
Experience 3, Iter 44, disc loss: 0.020039147279588512, policy loss: 4.443359963902177
Experience 3, Iter 45, disc loss: 0.017900231038177936, policy loss: 4.620388212892523
Experience 3, Iter 46, disc loss: 0.01662427490015073, policy loss: 4.686575823825834
Experience 3, Iter 47, disc loss: 0.019073865899410306, policy loss: 4.568114574300106
Experience 3, Iter 48, disc loss: 0.018041384887299512, policy loss: 4.632702221090255
Experience 3, Iter 49, disc loss: 0.0163458140894207, policy loss: 4.752952124875735
Experience 3, Iter 50, disc loss: 0.019408067502819946, policy loss: 4.635757723963516
Experience 3, Iter 51, disc loss: 0.02136689951836918, policy loss: 4.640418106625069
Experience 3, Iter 52, disc loss: 0.026565612255499783, policy loss: 4.55175439104663
Experience 3, Iter 53, disc loss: 0.02434311011320611, policy loss: 4.810647162299918
Experience 3, Iter 54, disc loss: 0.05003428866225988, policy loss: 4.387903667221542
Experience 3, Iter 55, disc loss: 0.03663574888690137, policy loss: 4.768423381000569
Experience 3, Iter 56, disc loss: 0.04097317060542505, policy loss: 4.43428722406305
Experience 3, Iter 57, disc loss: 0.0449142993658206, policy loss: 4.477743680674722
Experience 3, Iter 58, disc loss: 0.07701379101156137, policy loss: 4.29517370332137
Experience 3, Iter 59, disc loss: 0.06498769332366194, policy loss: 4.526465238150921
Experience 3, Iter 60, disc loss: 0.09372758150489005, policy loss: 4.220455471902088
Experience 3, Iter 61, disc loss: 0.04901969616055163, policy loss: 4.781939957971068
Experience 3, Iter 62, disc loss: 0.06918820907195529, policy loss: 4.644678852505888
Experience 3, Iter 63, disc loss: 0.04481564862673415, policy loss: 5.08163187781544
Experience 3, Iter 64, disc loss: 0.042360305794026595, policy loss: 4.973309314434677
Experience 3, Iter 65, disc loss: 0.04709086664507178, policy loss: 4.948252072292901
Experience 3, Iter 66, disc loss: 0.04021938231824447, policy loss: 5.0698616427146845
Experience 3, Iter 67, disc loss: 0.02569844739504499, policy loss: 5.321694708794733
Experience 3, Iter 68, disc loss: 0.05564636818587685, policy loss: 4.926962990811789
Experience 3, Iter 69, disc loss: 0.02437986599379259, policy loss: 5.5545140381969
Experience 3, Iter 70, disc loss: 0.02568195260763799, policy loss: 5.614768958435644
Experience 3, Iter 71, disc loss: 0.025735325028186054, policy loss: 5.7674506013565
Experience 3, Iter 72, disc loss: 0.018943738418090966, policy loss: 5.948718798784821
Experience 3, Iter 73, disc loss: 0.016063584549671453, policy loss: 6.434130664639337
Experience 3, Iter 74, disc loss: 0.0199267146592528, policy loss: 5.947629287965754
Experience 3, Iter 75, disc loss: 0.023625867944708893, policy loss: 5.605492755796104
Experience 3, Iter 76, disc loss: 0.01899537933298444, policy loss: 6.041849285656841
Experience 3, Iter 77, disc loss: 0.01201550984819414, policy loss: 6.210167750701845
Experience 3, Iter 78, disc loss: 0.012206509663478879, policy loss: 6.427674486178068
Experience 3, Iter 79, disc loss: 0.021288858817413898, policy loss: 5.976582106962704
Experience 3, Iter 80, disc loss: 0.010389836811910167, policy loss: 6.526676449170194
Experience 3, Iter 81, disc loss: 0.010948095246902503, policy loss: 6.249719317953546
Experience 3, Iter 82, disc loss: 0.011794550827527537, policy loss: 6.432117587231724
Experience 3, Iter 83, disc loss: 0.010238139606304257, policy loss: 6.633097983491484
Experience 3, Iter 84, disc loss: 0.01017210420507313, policy loss: 6.481324519704587
Experience 3, Iter 85, disc loss: 0.010108356434975345, policy loss: 6.534547008655972
Experience 3, Iter 86, disc loss: 0.009955436438399749, policy loss: 6.531998869880264
Experience 3, Iter 87, disc loss: 0.009931598347646602, policy loss: 6.495722483684982
Experience 3, Iter 88, disc loss: 0.009987736606812328, policy loss: 6.372981586242028
Experience 3, Iter 89, disc loss: 0.010031858022452319, policy loss: 6.363569118450259
Experience 3, Iter 90, disc loss: 0.010617015464943569, policy loss: 6.144417405132469
Experience 3, Iter 91, disc loss: 0.010284116928543773, policy loss: 6.2731164938588755
Experience 3, Iter 92, disc loss: 0.01023160516992562, policy loss: 6.2438197229793335
Experience 3, Iter 93, disc loss: 0.010292710369135453, policy loss: 6.244094643401317
Experience 3, Iter 94, disc loss: 0.009615228995912432, policy loss: 6.250427190490702
Experience 3, Iter 95, disc loss: 0.011607932837915863, policy loss: 5.937738193011558
Experience 3, Iter 96, disc loss: 0.009538002801611094, policy loss: 6.202148638654073
Experience 3, Iter 97, disc loss: 0.00986152647021549, policy loss: 6.143846948501312
Experience 3, Iter 98, disc loss: 0.009987507084527298, policy loss: 6.104529901195873
Experience 3, Iter 99, disc loss: 0.009553800067482266, policy loss: 6.188181216307026
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0039],
        [0.1747],
        [0.0040]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0188, 0.0725, 0.1707, 0.0043, 0.0005, 0.0863]],

        [[0.0188, 0.0725, 0.1707, 0.0043, 0.0005, 0.0863]],

        [[0.0188, 0.0725, 0.1707, 0.0043, 0.0005, 0.0863]],

        [[0.0188, 0.0725, 0.1707, 0.0043, 0.0005, 0.0863]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0075, 0.0154, 0.6990, 0.0158], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0075, 0.0154, 0.6990, 0.0158])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.921
Iter 2/2000 - Loss: -0.019
Iter 3/2000 - Loss: -0.796
Iter 4/2000 - Loss: -0.645
Iter 5/2000 - Loss: -0.428
Iter 6/2000 - Loss: -0.426
Iter 7/2000 - Loss: -0.505
Iter 8/2000 - Loss: -0.543
Iter 9/2000 - Loss: -0.521
Iter 10/2000 - Loss: -0.509
Iter 11/2000 - Loss: -0.569
Iter 12/2000 - Loss: -0.691
Iter 13/2000 - Loss: -0.817
Iter 14/2000 - Loss: -0.897
Iter 15/2000 - Loss: -0.920
Iter 16/2000 - Loss: -0.907
Iter 17/2000 - Loss: -0.875
Iter 18/2000 - Loss: -0.836
Iter 19/2000 - Loss: -0.809
Iter 20/2000 - Loss: -0.818
Iter 1981/2000 - Loss: -7.541
Iter 1982/2000 - Loss: -7.541
Iter 1983/2000 - Loss: -7.541
Iter 1984/2000 - Loss: -7.541
Iter 1985/2000 - Loss: -7.541
Iter 1986/2000 - Loss: -7.541
Iter 1987/2000 - Loss: -7.541
Iter 1988/2000 - Loss: -7.541
Iter 1989/2000 - Loss: -7.541
Iter 1990/2000 - Loss: -7.541
Iter 1991/2000 - Loss: -7.541
Iter 1992/2000 - Loss: -7.541
Iter 1993/2000 - Loss: -7.542
Iter 1994/2000 - Loss: -7.542
Iter 1995/2000 - Loss: -7.542
Iter 1996/2000 - Loss: -7.542
Iter 1997/2000 - Loss: -7.542
Iter 1998/2000 - Loss: -7.542
Iter 1999/2000 - Loss: -7.542
Iter 2000/2000 - Loss: -7.542
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[17.0901,  3.8502, 51.7586, 10.1511,  9.0885, 49.7439]],

        [[24.0626, 33.3863, 12.4724,  0.5002,  2.1150,  3.8473]],

        [[27.3195, 46.3193,  9.7537,  0.6124,  4.6289,  6.6697]],

        [[22.5797, 40.3682,  8.2024,  2.3973,  6.5868, 27.5948]]])
Signal Variance: tensor([0.0507, 0.0747, 2.5772, 0.1715])
Estimated target variance: tensor([0.0075, 0.0154, 0.6990, 0.0158])
N: 40
Signal to noise ratio: tensor([11.5035, 12.0457, 34.9161, 26.0972])
Bound on condition number: tensor([ 5294.2032,  5804.9633, 48766.3973, 27243.5830])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.01914946199935217, policy loss: 4.583110211860644
Experience 4, Iter 1, disc loss: 0.019487061963273954, policy loss: 4.548880478315334
Experience 4, Iter 2, disc loss: 0.0192996598621816, policy loss: 4.537546536428536
Experience 4, Iter 3, disc loss: 0.019634407177697776, policy loss: 4.522222018994839
Experience 4, Iter 4, disc loss: 0.02168273575559262, policy loss: 4.361724444350772
Experience 4, Iter 5, disc loss: 0.02279183078326342, policy loss: 4.260290012071556
Experience 4, Iter 6, disc loss: 0.027185658921849307, policy loss: 4.099936457255611
Experience 4, Iter 7, disc loss: 0.02718333436960994, policy loss: 4.055503213074189
Experience 4, Iter 8, disc loss: 0.026153758599644816, policy loss: 4.1101940117839275
Experience 4, Iter 9, disc loss: 0.03358311590634216, policy loss: 3.900123606853237
Experience 4, Iter 10, disc loss: 0.04454184988446946, policy loss: 3.919513909153654
Experience 4, Iter 11, disc loss: 0.04610993608940342, policy loss: 3.729714515204269
Experience 4, Iter 12, disc loss: 0.045776969550360425, policy loss: 3.794413520017856
Experience 4, Iter 13, disc loss: 0.07526729815549188, policy loss: 3.4188993199792685
Experience 4, Iter 14, disc loss: 0.08624655871047006, policy loss: 3.7985259355679504
Experience 4, Iter 15, disc loss: 0.06576446661470035, policy loss: 3.865081746413666
Experience 4, Iter 16, disc loss: 0.059225651738435206, policy loss: 3.9968190151280565
Experience 4, Iter 17, disc loss: 0.06100210796867108, policy loss: 4.064537322262706
Experience 4, Iter 18, disc loss: 0.06979481739514316, policy loss: 4.1479413797863565
Experience 4, Iter 19, disc loss: 0.15826170948750382, policy loss: 3.8016763878241004
Experience 4, Iter 20, disc loss: 0.12115889925015455, policy loss: 4.312225396482552
Experience 4, Iter 21, disc loss: 0.08778965367581225, policy loss: 4.498774653992344
Experience 4, Iter 22, disc loss: 0.14337833118330517, policy loss: 4.169794391103891
Experience 4, Iter 23, disc loss: 0.10472497804387637, policy loss: 4.519500156144712
Experience 4, Iter 24, disc loss: 0.12382807071673146, policy loss: 4.091993316281351
Experience 4, Iter 25, disc loss: 0.14098275627729118, policy loss: 4.05505238764155
Experience 4, Iter 26, disc loss: 0.04754462472721499, policy loss: 4.73442019624113
Experience 4, Iter 27, disc loss: 0.10058954224435421, policy loss: 4.613522860280441
Experience 4, Iter 28, disc loss: 0.12683154047517245, policy loss: 4.270016651318127
Experience 4, Iter 29, disc loss: 0.14942038656216042, policy loss: 4.778922568810164
Experience 4, Iter 30, disc loss: 0.12251924623050206, policy loss: 4.568436033074301
Experience 4, Iter 31, disc loss: 0.10411656688746426, policy loss: 4.753901066353593
Experience 4, Iter 32, disc loss: 0.060657183421596365, policy loss: 4.90705927767667
Experience 4, Iter 33, disc loss: 0.06154442791626939, policy loss: 4.892320812579449
Experience 4, Iter 34, disc loss: 0.03937418861908703, policy loss: 5.210053918221386
Experience 4, Iter 35, disc loss: 0.04358405376027669, policy loss: 4.914853145101726
Experience 4, Iter 36, disc loss: 0.03691806073377873, policy loss: 5.037481001944101
Experience 4, Iter 37, disc loss: 0.02429677957933206, policy loss: 5.213603273834427
Experience 4, Iter 38, disc loss: 0.028884677470445916, policy loss: 5.08212355066671
Experience 4, Iter 39, disc loss: 0.022697052173796393, policy loss: 5.28766775385942
Experience 4, Iter 40, disc loss: 0.03031364250211516, policy loss: 5.057641806325259
Experience 4, Iter 41, disc loss: 0.02605193954003095, policy loss: 5.088156237870825
Experience 4, Iter 42, disc loss: 0.024581949296811595, policy loss: 5.266431700644683
Experience 4, Iter 43, disc loss: 0.024067225242724335, policy loss: 5.277406522507399
Experience 4, Iter 44, disc loss: 0.025191053566159704, policy loss: 5.142369300659903
Experience 4, Iter 45, disc loss: 0.023177500595883888, policy loss: 5.099121939879488
Experience 4, Iter 46, disc loss: 0.022556235949333488, policy loss: 5.315920352741128
Experience 4, Iter 47, disc loss: 0.02130147936936695, policy loss: 5.182479140487914
Experience 4, Iter 48, disc loss: 0.020674784191286877, policy loss: 5.208335892536186
Experience 4, Iter 49, disc loss: 0.01740853106932698, policy loss: 5.371029874205367
Experience 4, Iter 50, disc loss: 0.020726334772355023, policy loss: 5.2895091675429855
Experience 4, Iter 51, disc loss: 0.0162645045415631, policy loss: 5.376956750081567
Experience 4, Iter 52, disc loss: 0.015669315691205832, policy loss: 5.385933373446042
Experience 4, Iter 53, disc loss: 0.01639236925205139, policy loss: 5.342433421663541
Experience 4, Iter 54, disc loss: 0.013613666393418496, policy loss: 5.551374206790296
Experience 4, Iter 55, disc loss: 0.015600902867713517, policy loss: 5.341482401353324
Experience 4, Iter 56, disc loss: 0.015895582986455298, policy loss: 5.434385790380624
Experience 4, Iter 57, disc loss: 0.014725521055349083, policy loss: 5.203961534313961
Experience 4, Iter 58, disc loss: 0.0163638782124833, policy loss: 5.53511095907311
Experience 4, Iter 59, disc loss: 0.015740595668325405, policy loss: 5.341407657671086
Experience 4, Iter 60, disc loss: 0.015067154578349597, policy loss: 5.257934412178946
Experience 4, Iter 61, disc loss: 0.012278461392451416, policy loss: 5.395823829029251
Experience 4, Iter 62, disc loss: 0.01325696913126612, policy loss: 5.548112192233264
Experience 4, Iter 63, disc loss: 0.014228757454580201, policy loss: 5.445618886419482
Experience 4, Iter 64, disc loss: 0.015245780007480291, policy loss: 5.303799179106899
Experience 4, Iter 65, disc loss: 0.01272914473527189, policy loss: 5.403683329267888
Experience 4, Iter 66, disc loss: 0.013956193781982772, policy loss: 5.406253375790039
Experience 4, Iter 67, disc loss: 0.01177726214876276, policy loss: 5.470989833327547
Experience 4, Iter 68, disc loss: 0.01414728965704813, policy loss: 5.301850840876148
Experience 4, Iter 69, disc loss: 0.011686319991138068, policy loss: 5.743731662097732
Experience 4, Iter 70, disc loss: 0.01665546963264082, policy loss: 5.534290916367938
Experience 4, Iter 71, disc loss: 0.012438292837748843, policy loss: 5.517891672411425
Experience 4, Iter 72, disc loss: 0.012461209806601828, policy loss: 5.516232578988205
Experience 4, Iter 73, disc loss: 0.010332140607975444, policy loss: 5.745499689699095
Experience 4, Iter 74, disc loss: 0.013014163906260713, policy loss: 5.5757742985761105
Experience 4, Iter 75, disc loss: 0.01083821912041837, policy loss: 5.653567565311035
Experience 4, Iter 76, disc loss: 0.009662237807114633, policy loss: 5.825595535367378
Experience 4, Iter 77, disc loss: 0.013443644815216546, policy loss: 5.606781897072072
Experience 4, Iter 78, disc loss: 0.011002954519684251, policy loss: 5.783296475540599
Experience 4, Iter 79, disc loss: 0.0149009608238243, policy loss: 5.556429688868526
Experience 4, Iter 80, disc loss: 0.01317962510347677, policy loss: 5.5663338938154485
Experience 4, Iter 81, disc loss: 0.013363947697598643, policy loss: 5.275299338428749
Experience 4, Iter 82, disc loss: 0.009007483243575849, policy loss: 5.827001995591669
Experience 4, Iter 83, disc loss: 0.010994087890474864, policy loss: 5.5944120665950186
Experience 4, Iter 84, disc loss: 0.012973504918208943, policy loss: 5.477607103459611
Experience 4, Iter 85, disc loss: 0.012733299048063636, policy loss: 5.662260826937755
Experience 4, Iter 86, disc loss: 0.012313563154050538, policy loss: 5.687163700969081
Experience 4, Iter 87, disc loss: 0.00992464115568033, policy loss: 5.830709819412092
Experience 4, Iter 88, disc loss: 0.012372538318936808, policy loss: 5.565407703031038
Experience 4, Iter 89, disc loss: 0.011982674002783411, policy loss: 5.474859324048637
Experience 4, Iter 90, disc loss: 0.012204201924211995, policy loss: 5.468868216491394
Experience 4, Iter 91, disc loss: 0.011167962580006154, policy loss: 5.707417406449288
Experience 4, Iter 92, disc loss: 0.013278954415680512, policy loss: 5.60209107871064
Experience 4, Iter 93, disc loss: 0.03231898464381615, policy loss: 5.645699829565963
Experience 4, Iter 94, disc loss: 0.012364500732538256, policy loss: 5.515319746595158
Experience 4, Iter 95, disc loss: 0.010172598120673029, policy loss: 5.660471409841385
Experience 4, Iter 96, disc loss: 0.022556243712491562, policy loss: 5.9150684768527935
Experience 4, Iter 97, disc loss: 0.009716080815898377, policy loss: 6.09409412544175
Experience 4, Iter 98, disc loss: 0.007957235219516063, policy loss: 5.923671478248642
Experience 4, Iter 99, disc loss: 0.011673042435503121, policy loss: 5.875081825591702
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.0057],
        [0.1444],
        [0.0032]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0180, 0.0780, 0.1400, 0.0045, 0.0004, 0.1853]],

        [[0.0180, 0.0780, 0.1400, 0.0045, 0.0004, 0.1853]],

        [[0.0180, 0.0780, 0.1400, 0.0045, 0.0004, 0.1853]],

        [[0.0180, 0.0780, 0.1400, 0.0045, 0.0004, 0.1853]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0083, 0.0228, 0.5778, 0.0130], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0083, 0.0228, 0.5778, 0.0130])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.496
Iter 2/2000 - Loss: -0.442
Iter 3/2000 - Loss: -0.817
Iter 4/2000 - Loss: -0.625
Iter 5/2000 - Loss: -0.486
Iter 6/2000 - Loss: -0.564
Iter 7/2000 - Loss: -0.689
Iter 8/2000 - Loss: -0.738
Iter 9/2000 - Loss: -0.717
Iter 10/2000 - Loss: -0.718
Iter 11/2000 - Loss: -0.801
Iter 12/2000 - Loss: -0.928
Iter 13/2000 - Loss: -1.018
Iter 14/2000 - Loss: -1.020
Iter 15/2000 - Loss: -0.961
Iter 16/2000 - Loss: -0.929
Iter 17/2000 - Loss: -0.986
Iter 18/2000 - Loss: -1.114
Iter 19/2000 - Loss: -1.244
Iter 20/2000 - Loss: -1.328
Iter 1981/2000 - Loss: -7.874
Iter 1982/2000 - Loss: -7.874
Iter 1983/2000 - Loss: -7.874
Iter 1984/2000 - Loss: -7.874
Iter 1985/2000 - Loss: -7.874
Iter 1986/2000 - Loss: -7.874
Iter 1987/2000 - Loss: -7.874
Iter 1988/2000 - Loss: -7.874
Iter 1989/2000 - Loss: -7.874
Iter 1990/2000 - Loss: -7.874
Iter 1991/2000 - Loss: -7.875
Iter 1992/2000 - Loss: -7.875
Iter 1993/2000 - Loss: -7.875
Iter 1994/2000 - Loss: -7.875
Iter 1995/2000 - Loss: -7.875
Iter 1996/2000 - Loss: -7.875
Iter 1997/2000 - Loss: -7.875
Iter 1998/2000 - Loss: -7.875
Iter 1999/2000 - Loss: -7.875
Iter 2000/2000 - Loss: -7.875
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[14.9456,  3.7753, 35.6212,  8.9948,  7.5572, 33.5131]],

        [[23.7101, 29.7567, 14.7649,  0.5754,  0.9815,  5.2421]],

        [[24.7207, 35.2018, 10.3688,  0.5695,  4.1267,  8.0623]],

        [[25.7704, 42.6553,  9.2438,  2.6142,  5.6745, 23.8915]]])
Signal Variance: tensor([0.0448, 0.1281, 2.8570, 0.2106])
Estimated target variance: tensor([0.0083, 0.0228, 0.5778, 0.0130])
N: 50
Signal to noise ratio: tensor([11.3509, 16.1449, 38.0137, 30.9750])
Bound on condition number: tensor([ 6443.1419, 13033.9691, 72253.1078, 47973.6682])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.023218364481192688, policy loss: 4.4270009303304665
Experience 5, Iter 1, disc loss: 0.02157088606574977, policy loss: 4.526223297510716
Experience 5, Iter 2, disc loss: 0.03800801413400186, policy loss: 4.359384020542826
Experience 5, Iter 3, disc loss: 0.03785586082215611, policy loss: 4.175666425632134
Experience 5, Iter 4, disc loss: 0.03261630323672212, policy loss: 4.282557642017758
Experience 5, Iter 5, disc loss: 0.052499205935561066, policy loss: 3.976240517254764
Experience 5, Iter 6, disc loss: 0.07964256093976277, policy loss: 3.806341359238667
Experience 5, Iter 7, disc loss: 0.06941960192050185, policy loss: 3.733729788223293
Experience 5, Iter 8, disc loss: 0.08845189376363162, policy loss: 3.6631103807285443
Experience 5, Iter 9, disc loss: 0.06646229313703653, policy loss: 4.146924806201586
Experience 5, Iter 10, disc loss: 0.12114754460291702, policy loss: 4.1476018675215425
Experience 5, Iter 11, disc loss: 0.18391919893547032, policy loss: 3.6419762121956745
Experience 5, Iter 12, disc loss: 0.09914803812349743, policy loss: 4.87626798722054
Experience 5, Iter 13, disc loss: 0.1094409082240519, policy loss: 4.584768544217411
Experience 5, Iter 14, disc loss: 0.0871758831457817, policy loss: 4.685697017703836
Experience 5, Iter 15, disc loss: 0.07821140718297923, policy loss: 4.835862019134927
Experience 5, Iter 16, disc loss: 0.05974576702777468, policy loss: 4.865145678930157
Experience 5, Iter 17, disc loss: 0.08922770803800291, policy loss: 4.30351743195731
Experience 5, Iter 18, disc loss: 0.05797725057509468, policy loss: 4.7608972976732264
Experience 5, Iter 19, disc loss: 0.0593205712716676, policy loss: 4.816108724986776
Experience 5, Iter 20, disc loss: 0.0659753831758127, policy loss: 4.351675788789763
Experience 5, Iter 21, disc loss: 0.05924746649871821, policy loss: 4.454885062725934
Experience 5, Iter 22, disc loss: 0.04669464582314809, policy loss: 4.7846191698453175
Experience 5, Iter 23, disc loss: 0.034562759390504974, policy loss: 4.870485279770541
Experience 5, Iter 24, disc loss: 0.04578897625079899, policy loss: 4.834933208406031
Experience 5, Iter 25, disc loss: 0.03788120978653073, policy loss: 4.599848743216521
Experience 5, Iter 26, disc loss: 0.023787561925520756, policy loss: 5.060830296877434
Experience 5, Iter 27, disc loss: 0.02429622500869366, policy loss: 4.776938597639777
Experience 5, Iter 28, disc loss: 0.038877091084220314, policy loss: 4.343164236710568
Experience 5, Iter 29, disc loss: 0.03586988082109814, policy loss: 4.954077735474138
Experience 5, Iter 30, disc loss: 0.023408653953292075, policy loss: 4.952331514893365
Experience 5, Iter 31, disc loss: 0.02643831804705065, policy loss: 4.84857772727556
Experience 5, Iter 32, disc loss: 0.026881697626382928, policy loss: 4.758666583066765
Experience 5, Iter 33, disc loss: 0.027645637930670697, policy loss: 5.063534386356
Experience 5, Iter 34, disc loss: 0.02501586799398263, policy loss: 4.966781666863961
Experience 5, Iter 35, disc loss: 0.04873687755301634, policy loss: 4.5822060205435164
Experience 5, Iter 36, disc loss: 0.029327685550279432, policy loss: 5.058048617298303
Experience 5, Iter 37, disc loss: 0.029839617356143596, policy loss: 4.882151405143587
Experience 5, Iter 38, disc loss: 0.026228536987785007, policy loss: 5.256946465943082
Experience 5, Iter 39, disc loss: 0.020000620407195174, policy loss: 5.470663735045857
Experience 5, Iter 40, disc loss: 0.026383872957215476, policy loss: 5.332038149480052
Experience 5, Iter 41, disc loss: 0.02757644007124969, policy loss: 5.016315251703869
Experience 5, Iter 42, disc loss: 0.0401746024689978, policy loss: 5.138640261242765
Experience 5, Iter 43, disc loss: 0.030794694761468407, policy loss: 4.937265585576379
Experience 5, Iter 44, disc loss: 0.028478472434431377, policy loss: 5.24748345629075
Experience 5, Iter 45, disc loss: 0.06033712623167732, policy loss: 5.200065537111556
Experience 5, Iter 46, disc loss: 0.028219727770217886, policy loss: 5.085487921930932
Experience 5, Iter 47, disc loss: 0.02054314825840085, policy loss: 5.33044113708115
Experience 5, Iter 48, disc loss: 0.025864244037431677, policy loss: 5.3094859839094415
Experience 5, Iter 49, disc loss: 0.020820851072758714, policy loss: 5.243248303918737
Experience 5, Iter 50, disc loss: 0.020600928823706195, policy loss: 5.3840717538053395
Experience 5, Iter 51, disc loss: 0.0155129002340616, policy loss: 5.4800520760688665
Experience 5, Iter 52, disc loss: 0.012485721515867885, policy loss: 5.856314317203584
Experience 5, Iter 53, disc loss: 0.018800882761107276, policy loss: 5.56902668996458
Experience 5, Iter 54, disc loss: 0.020385693146258883, policy loss: 5.519371598382028
Experience 5, Iter 55, disc loss: 0.013764605955822625, policy loss: 5.745790444498779
Experience 5, Iter 56, disc loss: 0.023028133834032433, policy loss: 5.491174239921287
Experience 5, Iter 57, disc loss: 0.014713970748648265, policy loss: 5.573670511805736
Experience 5, Iter 58, disc loss: 0.014270702025654546, policy loss: 5.371716938330303
Experience 5, Iter 59, disc loss: 0.014680953882188898, policy loss: 5.47736574331562
Experience 5, Iter 60, disc loss: 0.01823168817783871, policy loss: 5.722533497938392
Experience 5, Iter 61, disc loss: 0.013856263145962956, policy loss: 5.428949132870119
Experience 5, Iter 62, disc loss: 0.02175290433681099, policy loss: 5.170713629853756
Experience 5, Iter 63, disc loss: 0.011820625808092254, policy loss: 5.500434299318819
Experience 5, Iter 64, disc loss: 0.01298324693885924, policy loss: 5.4093902818334545
Experience 5, Iter 65, disc loss: 0.015351147679616378, policy loss: 5.5077970580667275
Experience 5, Iter 66, disc loss: 0.009791020794158008, policy loss: 5.638157509179136
Experience 5, Iter 67, disc loss: 0.015756722147140037, policy loss: 5.444768481456867
Experience 5, Iter 68, disc loss: 0.013051052089040574, policy loss: 5.575902894119151
Experience 5, Iter 69, disc loss: 0.01241711143791991, policy loss: 5.412460230496507
Experience 5, Iter 70, disc loss: 0.01267274009680117, policy loss: 5.446487790262406
Experience 5, Iter 71, disc loss: 0.01051102121531636, policy loss: 5.53153313965954
Experience 5, Iter 72, disc loss: 0.009785874658900753, policy loss: 5.49233755710746
Experience 5, Iter 73, disc loss: 0.012103043649682513, policy loss: 5.4672251630066615
Experience 5, Iter 74, disc loss: 0.012531563245951496, policy loss: 5.67660521879327
Experience 5, Iter 75, disc loss: 0.009805579107086248, policy loss: 5.5316540309987605
Experience 5, Iter 76, disc loss: 0.008648226759251403, policy loss: 5.667941693462885
Experience 5, Iter 77, disc loss: 0.013076262611712915, policy loss: 5.504666217898925
Experience 5, Iter 78, disc loss: 0.008903039094605554, policy loss: 5.734794208591794
Experience 5, Iter 79, disc loss: 0.0096011109997555, policy loss: 5.499093907448358
Experience 5, Iter 80, disc loss: 0.007962500944666396, policy loss: 5.763744559599449
Experience 5, Iter 81, disc loss: 0.009433117875488735, policy loss: 5.712374276914628
Experience 5, Iter 82, disc loss: 0.0072753835930097575, policy loss: 5.978100156041758
Experience 5, Iter 83, disc loss: 0.008156106602625814, policy loss: 5.754762683176056
Experience 5, Iter 84, disc loss: 0.006872017926441092, policy loss: 5.92094870895
Experience 5, Iter 85, disc loss: 0.006458799486933542, policy loss: 6.013361220771939
Experience 5, Iter 86, disc loss: 0.01249285344825844, policy loss: 5.758930879776451
Experience 5, Iter 87, disc loss: 0.007577207841112569, policy loss: 5.696458985069056
Experience 5, Iter 88, disc loss: 0.007625860522538939, policy loss: 5.856987802858311
Experience 5, Iter 89, disc loss: 0.006759767705319448, policy loss: 5.959547171178805
Experience 5, Iter 90, disc loss: 0.008682471337700249, policy loss: 5.877874264037635
Experience 5, Iter 91, disc loss: 0.006907098758798245, policy loss: 5.992963401308269
Experience 5, Iter 92, disc loss: 0.00692961871041762, policy loss: 5.989285951392277
Experience 5, Iter 93, disc loss: 0.007305355391090244, policy loss: 5.929168509821946
Experience 5, Iter 94, disc loss: 0.007125725693685002, policy loss: 5.732967209853189
Experience 5, Iter 95, disc loss: 0.006936831411770508, policy loss: 5.880192486536456
Experience 5, Iter 96, disc loss: 0.006946991076210343, policy loss: 5.930638534557996
Experience 5, Iter 97, disc loss: 0.006238487594447964, policy loss: 5.9490634171257275
Experience 5, Iter 98, disc loss: 0.006075052947521569, policy loss: 6.003824481028965
Experience 5, Iter 99, disc loss: 0.00619251615130472, policy loss: 6.093653762577491
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0187],
        [0.1214],
        [0.0030]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.0684e-02, 1.4116e-01, 1.2455e-01, 7.5675e-03, 3.9966e-04,
          7.9262e-01]],

        [[2.0684e-02, 1.4116e-01, 1.2455e-01, 7.5675e-03, 3.9966e-04,
          7.9262e-01]],

        [[2.0684e-02, 1.4116e-01, 1.2455e-01, 7.5675e-03, 3.9966e-04,
          7.9262e-01]],

        [[2.0684e-02, 1.4116e-01, 1.2455e-01, 7.5675e-03, 3.9966e-04,
          7.9262e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0165, 0.0748, 0.4856, 0.0118], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0165, 0.0748, 0.4856, 0.0118])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.011
Iter 2/2000 - Loss: 0.111
Iter 3/2000 - Loss: -0.127
Iter 4/2000 - Loss: -0.098
Iter 5/2000 - Loss: -0.055
Iter 6/2000 - Loss: -0.114
Iter 7/2000 - Loss: -0.189
Iter 8/2000 - Loss: -0.244
Iter 9/2000 - Loss: -0.243
Iter 10/2000 - Loss: -0.205
Iter 11/2000 - Loss: -0.203
Iter 12/2000 - Loss: -0.270
Iter 13/2000 - Loss: -0.360
Iter 14/2000 - Loss: -0.409
Iter 15/2000 - Loss: -0.413
Iter 16/2000 - Loss: -0.426
Iter 17/2000 - Loss: -0.490
Iter 18/2000 - Loss: -0.598
Iter 19/2000 - Loss: -0.711
Iter 20/2000 - Loss: -0.805
Iter 1981/2000 - Loss: -7.988
Iter 1982/2000 - Loss: -7.988
Iter 1983/2000 - Loss: -7.988
Iter 1984/2000 - Loss: -7.988
Iter 1985/2000 - Loss: -7.988
Iter 1986/2000 - Loss: -7.988
Iter 1987/2000 - Loss: -7.988
Iter 1988/2000 - Loss: -7.988
Iter 1989/2000 - Loss: -7.988
Iter 1990/2000 - Loss: -7.988
Iter 1991/2000 - Loss: -7.988
Iter 1992/2000 - Loss: -7.988
Iter 1993/2000 - Loss: -7.988
Iter 1994/2000 - Loss: -7.988
Iter 1995/2000 - Loss: -7.988
Iter 1996/2000 - Loss: -7.988
Iter 1997/2000 - Loss: -7.988
Iter 1998/2000 - Loss: -7.988
Iter 1999/2000 - Loss: -7.988
Iter 2000/2000 - Loss: -7.988
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[20.7178,  6.7366, 29.0018, 11.7100,  7.7225, 48.2555]],

        [[26.1184, 37.1880, 16.2008,  3.5271,  9.0921, 18.2648]],

        [[25.1197, 39.9540, 10.4626,  0.6437,  3.8967,  9.0607]],

        [[25.4592, 44.8127,  9.0045,  2.6468,  4.5896, 26.8370]]])
Signal Variance: tensor([0.1286, 1.7473, 3.0046, 0.1934])
Estimated target variance: tensor([0.0165, 0.0748, 0.4856, 0.0118])
N: 60
Signal to noise ratio: tensor([20.0862, 60.2261, 39.7813, 29.6281])
Bound on condition number: tensor([ 24208.3592, 217632.3471,  94954.2089,  52670.4557])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.00804669599355001, policy loss: 5.5973853919805565
Experience 6, Iter 1, disc loss: 0.009938855121293682, policy loss: 5.291933794419653
Experience 6, Iter 2, disc loss: 0.011477640593612951, policy loss: 5.119578103980368
Experience 6, Iter 3, disc loss: 0.011776360432829633, policy loss: 4.97105152666208
Experience 6, Iter 4, disc loss: 0.013231880559286424, policy loss: 4.87756720161719
Experience 6, Iter 5, disc loss: 0.01461940049853783, policy loss: 4.862951769644901
Experience 6, Iter 6, disc loss: 0.013566035767150462, policy loss: 4.983836886914032
Experience 6, Iter 7, disc loss: 0.010679976442115562, policy loss: 5.272526078303603
Experience 6, Iter 8, disc loss: 0.01060729109497013, policy loss: 5.2466990182210544
Experience 6, Iter 9, disc loss: 0.00987748091988222, policy loss: 5.33638512050263
Experience 6, Iter 10, disc loss: 0.009425262066346926, policy loss: 5.381528861187911
Experience 6, Iter 11, disc loss: 0.010042834998350285, policy loss: 5.3401707141076
Experience 6, Iter 12, disc loss: 0.010386471335344982, policy loss: 5.297580628423902
Experience 6, Iter 13, disc loss: 0.009228337765886874, policy loss: 5.544207852939511
Experience 6, Iter 14, disc loss: 0.009510533850101094, policy loss: 5.453297489475126
Experience 6, Iter 15, disc loss: 0.00869048172529986, policy loss: 5.666982343995661
Experience 6, Iter 16, disc loss: 0.008676522120393593, policy loss: 5.776297334620962
Experience 6, Iter 17, disc loss: 0.008855534773042273, policy loss: 5.620318360324319
Experience 6, Iter 18, disc loss: 0.008327410993930617, policy loss: 5.795352857480276
Experience 6, Iter 19, disc loss: 0.007982572808518622, policy loss: 5.884440008139258
Experience 6, Iter 20, disc loss: 0.007268330360251542, policy loss: 6.077917652187076
Experience 6, Iter 21, disc loss: 0.00742328579492165, policy loss: 5.9322024741202615
Experience 6, Iter 22, disc loss: 0.008108995646802897, policy loss: 5.7148896462285235
Experience 6, Iter 23, disc loss: 0.007096004465120853, policy loss: 5.922115896501616
Experience 6, Iter 24, disc loss: 0.006736746743423205, policy loss: 6.0878091827243574
Experience 6, Iter 25, disc loss: 0.006735697383521316, policy loss: 6.001949026990943
Experience 6, Iter 26, disc loss: 0.006990308638257951, policy loss: 5.831428574018215
Experience 6, Iter 27, disc loss: 0.006542445990012535, policy loss: 5.869345746095038
Experience 6, Iter 28, disc loss: 0.006214354607319175, policy loss: 6.078697023288357
Experience 6, Iter 29, disc loss: 0.006157705362357225, policy loss: 5.853391663390086
Experience 6, Iter 30, disc loss: 0.006257317840411233, policy loss: 5.9029050491448425
Experience 6, Iter 31, disc loss: 0.005670883622568476, policy loss: 6.113350608079006
Experience 6, Iter 32, disc loss: 0.005929144050857241, policy loss: 5.891512501468631
Experience 6, Iter 33, disc loss: 0.005875739068058559, policy loss: 5.968618660193752
Experience 6, Iter 34, disc loss: 0.0058731077470370084, policy loss: 5.909824914437189
Experience 6, Iter 35, disc loss: 0.0062057301836330545, policy loss: 5.7880492570468105
Experience 6, Iter 36, disc loss: 0.005661366120868174, policy loss: 5.931830823763239
Experience 6, Iter 37, disc loss: 0.006126604093645707, policy loss: 5.837986756299773
Experience 6, Iter 38, disc loss: 0.0056410461865637005, policy loss: 5.998061443396237
Experience 6, Iter 39, disc loss: 0.0059441589557026435, policy loss: 6.033178072963774
Experience 6, Iter 40, disc loss: 0.005698536952849288, policy loss: 5.932618153876704
Experience 6, Iter 41, disc loss: 0.004941934390741422, policy loss: 6.337754478500989
Experience 6, Iter 42, disc loss: 0.005601970969994552, policy loss: 5.952164455960707
Experience 6, Iter 43, disc loss: 0.005746252560325947, policy loss: 5.902613152109688
Experience 6, Iter 44, disc loss: 0.006218139605513128, policy loss: 5.805490500903805
Experience 6, Iter 45, disc loss: 0.0063714271783078345, policy loss: 5.776654737831093
Experience 6, Iter 46, disc loss: 0.0065569499385878275, policy loss: 5.774621402079995
Experience 6, Iter 47, disc loss: 0.0061952030436762204, policy loss: 5.908120686236291
Experience 6, Iter 48, disc loss: 0.007120958644803428, policy loss: 5.649150341581919
Experience 6, Iter 49, disc loss: 0.006962116713337799, policy loss: 5.799302366354768
Experience 6, Iter 50, disc loss: 0.007705077777769022, policy loss: 5.819236300065864
Experience 6, Iter 51, disc loss: 0.006161078930676545, policy loss: 5.919798087617283
Experience 6, Iter 52, disc loss: 0.007398457091356865, policy loss: 5.766960245053146
Experience 6, Iter 53, disc loss: 0.006801345669825163, policy loss: 5.887344264577557
Experience 6, Iter 54, disc loss: 0.008445597114143203, policy loss: 5.742576702804687
Experience 6, Iter 55, disc loss: 0.009990250356840591, policy loss: 5.594173627209257
Experience 6, Iter 56, disc loss: 0.0072203784533659875, policy loss: 5.873353787852366
Experience 6, Iter 57, disc loss: 0.007113986710995438, policy loss: 5.961323997670487
Experience 6, Iter 58, disc loss: 0.007434998985023793, policy loss: 6.098232636215867
Experience 6, Iter 59, disc loss: 0.010137436752937972, policy loss: 5.8487852633659845
Experience 6, Iter 60, disc loss: 0.007948413039864429, policy loss: 6.182225397474692
Experience 6, Iter 61, disc loss: 0.010135328050144256, policy loss: 5.960724827741416
Experience 6, Iter 62, disc loss: 0.008613794737064976, policy loss: 5.914093985208891
Experience 6, Iter 63, disc loss: 0.01183016372657757, policy loss: 5.784103305497714
Experience 6, Iter 64, disc loss: 0.013376107431253663, policy loss: 5.5782845700822055
Experience 6, Iter 65, disc loss: 0.018357408330127396, policy loss: 5.486584332439733
Experience 6, Iter 66, disc loss: 0.010708864852000118, policy loss: 6.0205369269246365
Experience 6, Iter 67, disc loss: 0.019087022230313676, policy loss: 5.542686767459967
Experience 6, Iter 68, disc loss: 0.012589784894378567, policy loss: 6.093927405778549
Experience 6, Iter 69, disc loss: 0.018373783414857695, policy loss: 5.856976445092434
Experience 6, Iter 70, disc loss: 0.010794266312214758, policy loss: 6.044055173391728
Experience 6, Iter 71, disc loss: 0.01675171860892364, policy loss: 6.072617124821692
Experience 6, Iter 72, disc loss: 0.01616266387676505, policy loss: 6.295105396124993
Experience 6, Iter 73, disc loss: 0.013614108912409478, policy loss: 5.9647449885187775
Experience 6, Iter 74, disc loss: 0.011918245596966005, policy loss: 6.388487008358705
Experience 6, Iter 75, disc loss: 0.010133908363145167, policy loss: 6.695424583922827
Experience 6, Iter 76, disc loss: 0.02750192241361408, policy loss: 6.28663200753382
Experience 6, Iter 77, disc loss: 0.010720852028861144, policy loss: 6.53718966451262
Experience 6, Iter 78, disc loss: 0.010070304398682155, policy loss: 6.618167834903581
Experience 6, Iter 79, disc loss: 0.010764268450744113, policy loss: 6.744185515332307
Experience 6, Iter 80, disc loss: 0.011321373717699014, policy loss: 6.402225854973441
Experience 6, Iter 81, disc loss: 0.009416914850902087, policy loss: 6.995934027948601
Experience 6, Iter 82, disc loss: 0.009471253438681038, policy loss: 6.801325566695014
Experience 6, Iter 83, disc loss: 0.018866537688267394, policy loss: 6.55000915337385
Experience 6, Iter 84, disc loss: 0.015610198409951832, policy loss: 6.49717158652131
Experience 6, Iter 85, disc loss: 0.010591255296810957, policy loss: 6.593692449098247
Experience 6, Iter 86, disc loss: 0.00686961989348471, policy loss: 6.877725117889505
Experience 6, Iter 87, disc loss: 0.012704307586800374, policy loss: 6.625848139334053
Experience 6, Iter 88, disc loss: 0.0076824710225369824, policy loss: 7.181870056189441
Experience 6, Iter 89, disc loss: 0.009335458148677603, policy loss: 6.653716129196031
Experience 6, Iter 90, disc loss: 0.008181318485997655, policy loss: 6.854084150721095
Experience 6, Iter 91, disc loss: 0.011218709566990169, policy loss: 6.426432979099272
Experience 6, Iter 92, disc loss: 0.009900925944519325, policy loss: 6.535397608843526
Experience 6, Iter 93, disc loss: 0.010409166881676847, policy loss: 6.680518402242251
Experience 6, Iter 94, disc loss: 0.0073910879203506405, policy loss: 6.70179215986289
Experience 6, Iter 95, disc loss: 0.005657317089417236, policy loss: 7.091328383344079
Experience 6, Iter 96, disc loss: 0.005186056208580247, policy loss: 7.390493099730743
Experience 6, Iter 97, disc loss: 0.007587005119208595, policy loss: 7.038017730714568
Experience 6, Iter 98, disc loss: 0.005751027352961659, policy loss: 7.317521468406136
Experience 6, Iter 99, disc loss: 0.005843152009460005, policy loss: 7.302747015457977
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0981],
        [0.8840],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.9294e-02, 1.3875e-01, 1.8905e-01, 6.8413e-03, 3.6900e-04,
          2.0724e+00]],

        [[1.9294e-02, 1.3875e-01, 1.8905e-01, 6.8413e-03, 3.6900e-04,
          2.0724e+00]],

        [[1.9294e-02, 1.3875e-01, 1.8905e-01, 6.8413e-03, 3.6900e-04,
          2.0724e+00]],

        [[1.9294e-02, 1.3875e-01, 1.8905e-01, 6.8413e-03, 3.6900e-04,
          2.0724e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0151, 0.3925, 3.5359, 0.0104], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0151, 0.3925, 3.5359, 0.0104])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.591
Iter 2/2000 - Loss: 1.873
Iter 3/2000 - Loss: 1.546
Iter 4/2000 - Loss: 1.530
Iter 5/2000 - Loss: 1.662
Iter 6/2000 - Loss: 1.640
Iter 7/2000 - Loss: 1.509
Iter 8/2000 - Loss: 1.430
Iter 9/2000 - Loss: 1.455
Iter 10/2000 - Loss: 1.504
Iter 11/2000 - Loss: 1.482
Iter 12/2000 - Loss: 1.393
Iter 13/2000 - Loss: 1.303
Iter 14/2000 - Loss: 1.245
Iter 15/2000 - Loss: 1.202
Iter 16/2000 - Loss: 1.140
Iter 17/2000 - Loss: 1.042
Iter 18/2000 - Loss: 0.916
Iter 19/2000 - Loss: 0.777
Iter 20/2000 - Loss: 0.638
Iter 1981/2000 - Loss: -7.849
Iter 1982/2000 - Loss: -7.849
Iter 1983/2000 - Loss: -7.849
Iter 1984/2000 - Loss: -7.849
Iter 1985/2000 - Loss: -7.850
Iter 1986/2000 - Loss: -7.850
Iter 1987/2000 - Loss: -7.850
Iter 1988/2000 - Loss: -7.850
Iter 1989/2000 - Loss: -7.850
Iter 1990/2000 - Loss: -7.850
Iter 1991/2000 - Loss: -7.850
Iter 1992/2000 - Loss: -7.850
Iter 1993/2000 - Loss: -7.850
Iter 1994/2000 - Loss: -7.850
Iter 1995/2000 - Loss: -7.850
Iter 1996/2000 - Loss: -7.850
Iter 1997/2000 - Loss: -7.850
Iter 1998/2000 - Loss: -7.850
Iter 1999/2000 - Loss: -7.850
Iter 2000/2000 - Loss: -7.850
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[18.3377,  7.5099, 35.1747, 12.2169,  6.9827, 58.5098]],

        [[27.2979, 35.7119, 21.3384,  5.1159, 11.4912, 33.1029]],

        [[26.1435, 40.5118, 18.3658,  1.2250,  6.2538, 29.7699]],

        [[25.2185, 41.7284, 11.4998,  3.7343,  7.0275, 44.0201]]])
Signal Variance: tensor([ 0.1324,  4.6088, 27.1811,  0.3724])
Estimated target variance: tensor([0.0151, 0.3925, 3.5359, 0.0104])
N: 70
Signal to noise ratio: tensor([ 19.9589, 100.5000, 108.8430,  40.4772])
Bound on condition number: tensor([ 27886.0953, 707018.3776, 829277.1370, 114689.5193])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.002548027540211573, policy loss: 9.627424045049551
Experience 7, Iter 1, disc loss: 0.0032457562684722408, policy loss: 9.46716286870205
Experience 7, Iter 2, disc loss: 0.0028610111942745688, policy loss: 9.788131019729025
Experience 7, Iter 3, disc loss: 0.001956875574861492, policy loss: 10.028529869261515
Experience 7, Iter 4, disc loss: 0.00411320918913584, policy loss: 9.270636533012386
Experience 7, Iter 5, disc loss: 0.004689127887780014, policy loss: 9.305466837484055
Experience 7, Iter 6, disc loss: 0.005585100370040872, policy loss: 9.44255925016964
Experience 7, Iter 7, disc loss: 0.006720415320770767, policy loss: 9.023048155226707
Experience 7, Iter 8, disc loss: 0.009414362847440247, policy loss: 8.586440237795744
Experience 7, Iter 9, disc loss: 0.00674112750680152, policy loss: 8.739389463341897
Experience 7, Iter 10, disc loss: 0.00780825802827745, policy loss: 8.264224203779396
Experience 7, Iter 11, disc loss: 0.007249642587784546, policy loss: 8.057624487570761
Experience 7, Iter 12, disc loss: 0.004318428452325362, policy loss: 8.89543144548842
Experience 7, Iter 13, disc loss: 0.004212943658485499, policy loss: 8.781126476867971
Experience 7, Iter 14, disc loss: 0.0036566353313248917, policy loss: 8.448456863696586
Experience 7, Iter 15, disc loss: 0.003631677762385926, policy loss: 8.156646783754695
Experience 7, Iter 16, disc loss: 0.0030455803021968595, policy loss: 7.8832253947501405
Experience 7, Iter 17, disc loss: 0.00286168416815905, policy loss: 9.01140828033181
Experience 7, Iter 18, disc loss: 0.0027523436015440914, policy loss: 8.0377823776776
Experience 7, Iter 19, disc loss: 0.002903845491265328, policy loss: 7.93499005859526
Experience 7, Iter 20, disc loss: 0.0024665601330425974, policy loss: 8.127610839898924
Experience 7, Iter 21, disc loss: 0.002212672481709777, policy loss: 7.884745035386208
Experience 7, Iter 22, disc loss: 0.002313091097854754, policy loss: 7.773455278931903
Experience 7, Iter 23, disc loss: 0.0019298801860895296, policy loss: 7.993417622576455
Experience 7, Iter 24, disc loss: 0.002045336601180971, policy loss: 8.031469324190144
Experience 7, Iter 25, disc loss: 0.0018095429504322727, policy loss: 8.094704029156425
Experience 7, Iter 26, disc loss: 0.0016298637514208063, policy loss: 8.6266124309635
Experience 7, Iter 27, disc loss: 0.0016320140971927749, policy loss: 8.704407834004183
Experience 7, Iter 28, disc loss: 0.0015699497183942251, policy loss: 8.651092592529471
Experience 7, Iter 29, disc loss: 0.001522091091656231, policy loss: 8.76166685677575
Experience 7, Iter 30, disc loss: 0.0014915709558773475, policy loss: 8.793408202461052
Experience 7, Iter 31, disc loss: 0.0014383104988068048, policy loss: 9.145860782791846
Experience 7, Iter 32, disc loss: 0.0014795089326528712, policy loss: 8.682761187502368
Experience 7, Iter 33, disc loss: 0.001421494886435992, policy loss: 8.908687941012005
Experience 7, Iter 34, disc loss: 0.001381329462009424, policy loss: 8.874072854902128
Experience 7, Iter 35, disc loss: 0.0014201128225186084, policy loss: 8.83672187850475
Experience 7, Iter 36, disc loss: 0.0013572135480500042, policy loss: 9.053175151632217
Experience 7, Iter 37, disc loss: 0.0013349674598291785, policy loss: 9.075179542265293
Experience 7, Iter 38, disc loss: 0.0012920167147117483, policy loss: 9.175149317759479
Experience 7, Iter 39, disc loss: 0.001304923594941919, policy loss: 9.066923294011325
Experience 7, Iter 40, disc loss: 0.0012664350440607347, policy loss: 9.150862695131671
Experience 7, Iter 41, disc loss: 0.0012496975146193134, policy loss: 9.194595871403868
Experience 7, Iter 42, disc loss: 0.001301639686915066, policy loss: 8.884909360972324
Experience 7, Iter 43, disc loss: 0.0012585052149799846, policy loss: 9.142216215836578
Experience 7, Iter 44, disc loss: 0.0012046405358848338, policy loss: 9.32042376386573
Experience 7, Iter 45, disc loss: 0.0012334924994253013, policy loss: 9.134222306169068
Experience 7, Iter 46, disc loss: 0.0012050194962498573, policy loss: 9.428205411252476
Experience 7, Iter 47, disc loss: 0.0012220914385464784, policy loss: 9.158678100961898
Experience 7, Iter 48, disc loss: 0.001185685753266631, policy loss: 9.414991072901204
Experience 7, Iter 49, disc loss: 0.0011668639936620096, policy loss: 9.352874943129464
Experience 7, Iter 50, disc loss: 0.0011682102505900717, policy loss: 9.38713589540677
Experience 7, Iter 51, disc loss: 0.001171651431903709, policy loss: 9.205141432626544
Experience 7, Iter 52, disc loss: 0.001156324229338127, policy loss: 9.241545922012882
Experience 7, Iter 53, disc loss: 0.0011856985823429195, policy loss: 9.164425431978117
Experience 7, Iter 54, disc loss: 0.0011482450688599121, policy loss: 9.23123120165069
Experience 7, Iter 55, disc loss: 0.001120640817107241, policy loss: 9.437227577934518
Experience 7, Iter 56, disc loss: 0.001162966230551085, policy loss: 9.344232301227061
Experience 7, Iter 57, disc loss: 0.0011592378090723896, policy loss: 9.080920402043942
Experience 7, Iter 58, disc loss: 0.0011485865660761335, policy loss: 9.173277656622384
Experience 7, Iter 59, disc loss: 0.0011492416582957999, policy loss: 9.316862474240466
Experience 7, Iter 60, disc loss: 0.0011161894500903223, policy loss: 9.180746196627318
Experience 7, Iter 61, disc loss: 0.0011681504109513903, policy loss: 9.177887580238101
Experience 7, Iter 62, disc loss: 0.0011034225229759657, policy loss: 9.136409448595774
Experience 7, Iter 63, disc loss: 0.001099798178520644, policy loss: 9.27376950620982
Experience 7, Iter 64, disc loss: 0.001149471046634115, policy loss: 9.059596668974871
Experience 7, Iter 65, disc loss: 0.0011090997602663352, policy loss: 9.137106608433323
Experience 7, Iter 66, disc loss: 0.0010897255147253309, policy loss: 9.182245162063104
Experience 7, Iter 67, disc loss: 0.0010877190059082659, policy loss: 9.237706222285766
Experience 7, Iter 68, disc loss: 0.0010562247111919618, policy loss: 9.311781257034012
Experience 7, Iter 69, disc loss: 0.0010432313610556842, policy loss: 9.356048914631314
Experience 7, Iter 70, disc loss: 0.0010892019581172422, policy loss: 9.15011951500814
Experience 7, Iter 71, disc loss: 0.0010816518675227297, policy loss: 9.168720033030704
Experience 7, Iter 72, disc loss: 0.001061712000660888, policy loss: 9.15053273607909
Experience 7, Iter 73, disc loss: 0.0010301686599836104, policy loss: 9.309075270989164
Experience 7, Iter 74, disc loss: 0.0010444382772456884, policy loss: 9.212534817105952
Experience 7, Iter 75, disc loss: 0.0010280880295642613, policy loss: 9.458660468822355
Experience 7, Iter 76, disc loss: 0.0010967089405759644, policy loss: 8.973426048006882
Experience 7, Iter 77, disc loss: 0.0010439566145323618, policy loss: 9.12666737710411
Experience 7, Iter 78, disc loss: 0.0010537067911695413, policy loss: 9.00679536736629
Experience 7, Iter 79, disc loss: 0.0010794270303254967, policy loss: 9.001484718970355
Experience 7, Iter 80, disc loss: 0.0010457658759101313, policy loss: 9.221243900089453
Experience 7, Iter 81, disc loss: 0.0010423364198786508, policy loss: 9.086370077468505
Experience 7, Iter 82, disc loss: 0.0010316146326432732, policy loss: 9.004056927699308
Experience 7, Iter 83, disc loss: 0.0010346151647535166, policy loss: 9.182871716270029
Experience 7, Iter 84, disc loss: 0.0010526957948880002, policy loss: 8.95169001760811
Experience 7, Iter 85, disc loss: 0.0010213446913974783, policy loss: 8.984371063033954
Experience 7, Iter 86, disc loss: 0.00103761264849869, policy loss: 9.041535577504867
Experience 7, Iter 87, disc loss: 0.001096267403523006, policy loss: 8.723446113754534
Experience 7, Iter 88, disc loss: 0.0011231254813216292, policy loss: 8.922855133489502
Experience 7, Iter 89, disc loss: 0.0010396210163942372, policy loss: 9.025746592092988
Experience 7, Iter 90, disc loss: 0.0010622379494610256, policy loss: 8.905398701662275
Experience 7, Iter 91, disc loss: 0.0010587505858249352, policy loss: 8.73890728097001
Experience 7, Iter 92, disc loss: 0.0010254674144509145, policy loss: 8.894763206965177
Experience 7, Iter 93, disc loss: 0.0011397479556301908, policy loss: 8.687620409503648
Experience 7, Iter 94, disc loss: 0.0010521979401654116, policy loss: 8.87091771491209
Experience 7, Iter 95, disc loss: 0.0010033548499734314, policy loss: 9.011788799562474
Experience 7, Iter 96, disc loss: 0.0010556623567414994, policy loss: 8.971005870376704
Experience 7, Iter 97, disc loss: 0.0009952876582551323, policy loss: 9.10276408511775
Experience 7, Iter 98, disc loss: 0.001045988366810027, policy loss: 8.747302168185751
Experience 7, Iter 99, disc loss: 0.001071610095922217, policy loss: 8.639614260087757
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1427],
        [1.3019],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.8286e-02, 1.2884e-01, 2.2166e-01, 5.9902e-03, 3.3580e-04,
          2.7864e+00]],

        [[1.8286e-02, 1.2884e-01, 2.2166e-01, 5.9902e-03, 3.3580e-04,
          2.7864e+00]],

        [[1.8286e-02, 1.2884e-01, 2.2166e-01, 5.9902e-03, 3.3580e-04,
          2.7864e+00]],

        [[1.8286e-02, 1.2884e-01, 2.2166e-01, 5.9902e-03, 3.3580e-04,
          2.7864e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0133, 0.5708, 5.2076, 0.0092], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0133, 0.5708, 5.2076, 0.0092])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.856
Iter 2/2000 - Loss: 2.209
Iter 3/2000 - Loss: 1.822
Iter 4/2000 - Loss: 1.806
Iter 5/2000 - Loss: 1.964
Iter 6/2000 - Loss: 1.944
Iter 7/2000 - Loss: 1.799
Iter 8/2000 - Loss: 1.706
Iter 9/2000 - Loss: 1.731
Iter 10/2000 - Loss: 1.792
Iter 11/2000 - Loss: 1.783
Iter 12/2000 - Loss: 1.704
Iter 13/2000 - Loss: 1.620
Iter 14/2000 - Loss: 1.566
Iter 15/2000 - Loss: 1.529
Iter 16/2000 - Loss: 1.485
Iter 17/2000 - Loss: 1.416
Iter 18/2000 - Loss: 1.316
Iter 19/2000 - Loss: 1.191
Iter 20/2000 - Loss: 1.053
Iter 1981/2000 - Loss: -7.869
Iter 1982/2000 - Loss: -7.869
Iter 1983/2000 - Loss: -7.869
Iter 1984/2000 - Loss: -7.869
Iter 1985/2000 - Loss: -7.869
Iter 1986/2000 - Loss: -7.869
Iter 1987/2000 - Loss: -7.869
Iter 1988/2000 - Loss: -7.869
Iter 1989/2000 - Loss: -7.869
Iter 1990/2000 - Loss: -7.869
Iter 1991/2000 - Loss: -7.869
Iter 1992/2000 - Loss: -7.870
Iter 1993/2000 - Loss: -7.870
Iter 1994/2000 - Loss: -7.870
Iter 1995/2000 - Loss: -7.870
Iter 1996/2000 - Loss: -7.870
Iter 1997/2000 - Loss: -7.870
Iter 1998/2000 - Loss: -7.870
Iter 1999/2000 - Loss: -7.870
Iter 2000/2000 - Loss: -7.870
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0024],
        [0.0002]])
Lengthscale: tensor([[[18.5524,  7.2912, 35.3709, 10.8766,  7.0357, 60.9345]],

        [[21.7337, 33.8860, 18.0044,  4.1212,  8.5728, 21.8975]],

        [[22.7637, 35.6687, 19.6349,  1.0503,  5.4285, 33.0695]],

        [[23.1597, 40.0077, 11.7996,  3.8537,  7.4783, 48.3643]]])
Signal Variance: tensor([ 0.1282,  3.3345, 27.1458,  0.3904])
Estimated target variance: tensor([0.0133, 0.5708, 5.2076, 0.0092])
N: 80
Signal to noise ratio: tensor([ 19.7339,  84.0829, 107.0429,  40.5547])
Bound on condition number: tensor([ 31155.1737, 565596.2724, 916655.5099, 131576.0097])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.001021452471621857, policy loss: 8.897986884295772
Experience 8, Iter 1, disc loss: 0.0009804993487625886, policy loss: 8.986004212163005
Experience 8, Iter 2, disc loss: 0.0010689064812642193, policy loss: 8.629363974213287
Experience 8, Iter 3, disc loss: 0.0010267199539741876, policy loss: 8.829435879079245
Experience 8, Iter 4, disc loss: 0.0012273281201483086, policy loss: 8.47281309523759
Experience 8, Iter 5, disc loss: 0.0010387336013427263, policy loss: 8.819301803885363
Experience 8, Iter 6, disc loss: 0.0010434928234584254, policy loss: 8.657873312729285
Experience 8, Iter 7, disc loss: 0.0010601550438504121, policy loss: 8.626402756683344
Experience 8, Iter 8, disc loss: 0.0010587726562395813, policy loss: 8.532006892726521
Experience 8, Iter 9, disc loss: 0.0010606200392463791, policy loss: 8.860385285338667
Experience 8, Iter 10, disc loss: 0.0010900546091008362, policy loss: 8.69876496884107
Experience 8, Iter 11, disc loss: 0.0010316590596924666, policy loss: 8.653220942026982
Experience 8, Iter 12, disc loss: 0.0010157732545954385, policy loss: 8.880561516111083
Experience 8, Iter 13, disc loss: 0.0010396995593461495, policy loss: 8.61199360114274
Experience 8, Iter 14, disc loss: 0.0010796739548472791, policy loss: 8.465348328428945
Experience 8, Iter 15, disc loss: 0.001037203145871102, policy loss: 8.66993661156594
Experience 8, Iter 16, disc loss: 0.0010219873687248029, policy loss: 8.937130076511309
Experience 8, Iter 17, disc loss: 0.0010230892325686, policy loss: 8.778758491968587
Experience 8, Iter 18, disc loss: 0.0010555356078011933, policy loss: 8.491536033446081
Experience 8, Iter 19, disc loss: 0.0010111517567641978, policy loss: 8.583022911543104
Experience 8, Iter 20, disc loss: 0.001061216455316219, policy loss: 8.551882995194335
Experience 8, Iter 21, disc loss: 0.0010613889651144622, policy loss: 8.497180428193081
Experience 8, Iter 22, disc loss: 0.001071063367005739, policy loss: 8.341484399792845
Experience 8, Iter 23, disc loss: 0.0010602418645193814, policy loss: 8.547346885175942
Experience 8, Iter 24, disc loss: 0.0009775063492312535, policy loss: 8.82740716108275
Experience 8, Iter 25, disc loss: 0.000969190032461736, policy loss: 8.723925827605159
Experience 8, Iter 26, disc loss: 0.0010353832682624946, policy loss: 8.520440853256165
Experience 8, Iter 27, disc loss: 0.0010641835166611926, policy loss: 8.62086770849793
Experience 8, Iter 28, disc loss: 0.0010214191595976376, policy loss: 8.512649005055415
Experience 8, Iter 29, disc loss: 0.0010078837573341247, policy loss: 8.638598364424306
Experience 8, Iter 30, disc loss: 0.001039806064188983, policy loss: 8.432102240272386
Experience 8, Iter 31, disc loss: 0.0010127451840628934, policy loss: 8.560365162811543
Experience 8, Iter 32, disc loss: 0.0010675269635450344, policy loss: 8.475164399707278
Experience 8, Iter 33, disc loss: 0.0009056490859482301, policy loss: 9.206523523303197
Experience 8, Iter 34, disc loss: 0.0009397726036962538, policy loss: 8.95039249703952
Experience 8, Iter 35, disc loss: 0.0009875955215430748, policy loss: 8.606374128588119
Experience 8, Iter 36, disc loss: 0.0008946278274186097, policy loss: 8.908230110725787
Experience 8, Iter 37, disc loss: 0.0010134716108396013, policy loss: 8.507599011331237
Experience 8, Iter 38, disc loss: 0.0009112342286374051, policy loss: 8.898625095097971
Experience 8, Iter 39, disc loss: 0.0008969145896341871, policy loss: 8.894413253026805
Experience 8, Iter 40, disc loss: 0.0009140563471043753, policy loss: 8.795874286460126
Experience 8, Iter 41, disc loss: 0.0009178807008243361, policy loss: 8.94556976411967
Experience 8, Iter 42, disc loss: 0.0008478960548490488, policy loss: 9.102752316434545
Experience 8, Iter 43, disc loss: 0.0008900218724360422, policy loss: 8.777321337746432
Experience 8, Iter 44, disc loss: 0.0008863789055502267, policy loss: 8.958127366778607
Experience 8, Iter 45, disc loss: 0.0008920910556378938, policy loss: 8.858880750140957
Experience 8, Iter 46, disc loss: 0.0009209874990304286, policy loss: 8.630813869772133
Experience 8, Iter 47, disc loss: 0.0009241577653136034, policy loss: 8.630063045076573
Experience 8, Iter 48, disc loss: 0.000866346386929577, policy loss: 8.811368427130787
Experience 8, Iter 49, disc loss: 0.0008452788998745334, policy loss: 8.926344295019979
Experience 8, Iter 50, disc loss: 0.0009694551122503862, policy loss: 8.601174213606113
Experience 8, Iter 51, disc loss: 0.0008485388320455363, policy loss: 9.12430077400559
Experience 8, Iter 52, disc loss: 0.0008353332690864944, policy loss: 8.786647438735617
Experience 8, Iter 53, disc loss: 0.000922388897146964, policy loss: 8.477148561245746
Experience 8, Iter 54, disc loss: 0.0009035046412733216, policy loss: 8.769629964718957
Experience 8, Iter 55, disc loss: 0.0009119700601855634, policy loss: 8.824905065885659
Experience 8, Iter 56, disc loss: 0.0008840239847146728, policy loss: 8.69097895892006
Experience 8, Iter 57, disc loss: 0.0008969951195206207, policy loss: 8.808099783873827
Experience 8, Iter 58, disc loss: 0.0009450886986347707, policy loss: 8.464040008799808
Experience 8, Iter 59, disc loss: 0.0009070166580604165, policy loss: 8.575716385637463
Experience 8, Iter 60, disc loss: 0.000914218453771905, policy loss: 8.658916999087758
Experience 8, Iter 61, disc loss: 0.0009780236540868816, policy loss: 8.620591200412534
Experience 8, Iter 62, disc loss: 0.0008921157288967999, policy loss: 8.629304406223357
Experience 8, Iter 63, disc loss: 0.0009320562655690061, policy loss: 8.512885651909098
Experience 8, Iter 64, disc loss: 0.0008593681583788624, policy loss: 8.76500761518084
Experience 8, Iter 65, disc loss: 0.0008796906341200842, policy loss: 8.868361363578796
Experience 8, Iter 66, disc loss: 0.0009153448080566869, policy loss: 8.530525723383604
Experience 8, Iter 67, disc loss: 0.0008867560842175966, policy loss: 8.799357362284233
Experience 8, Iter 68, disc loss: 0.000929386599931143, policy loss: 8.487538985415231
Experience 8, Iter 69, disc loss: 0.0008911915293247433, policy loss: 8.622224066336187
Experience 8, Iter 70, disc loss: 0.0009275209384889311, policy loss: 8.684678091772085
Experience 8, Iter 71, disc loss: 0.0009282083539204264, policy loss: 8.497073865241427
Experience 8, Iter 72, disc loss: 0.0009253463547807789, policy loss: 8.580414520126086
Experience 8, Iter 73, disc loss: 0.000963961082109159, policy loss: 8.314405846893056
Experience 8, Iter 74, disc loss: 0.0009200465114577491, policy loss: 8.58512411784971
Experience 8, Iter 75, disc loss: 0.0009252786685371438, policy loss: 8.62090328639463
Experience 8, Iter 76, disc loss: 0.000901986282864032, policy loss: 8.526058669097434
Experience 8, Iter 77, disc loss: 0.000908425916998937, policy loss: 8.612490911305741
Experience 8, Iter 78, disc loss: 0.0009316409672197795, policy loss: 8.486125833700335
Experience 8, Iter 79, disc loss: 0.0009258654841179772, policy loss: 8.605079631678013
Experience 8, Iter 80, disc loss: 0.0009417218905488343, policy loss: 8.46008889313448
Experience 8, Iter 81, disc loss: 0.0008868615728279702, policy loss: 8.661211370919625
Experience 8, Iter 82, disc loss: 0.0008866420598933134, policy loss: 8.78914752342349
Experience 8, Iter 83, disc loss: 0.0008544416896962807, policy loss: 8.499425499124463
Experience 8, Iter 84, disc loss: 0.0009040834001081033, policy loss: 8.337686803890396
Experience 8, Iter 85, disc loss: 0.0008279419319051828, policy loss: 8.770083943117958
Experience 8, Iter 86, disc loss: 0.0009366511134758147, policy loss: 8.313793517354906
Experience 8, Iter 87, disc loss: 0.0008625958450404366, policy loss: 8.485327984264345
Experience 8, Iter 88, disc loss: 0.0008609340366039007, policy loss: 8.471294456996254
Experience 8, Iter 89, disc loss: 0.0009054604247484739, policy loss: 8.447959737151216
Experience 8, Iter 90, disc loss: 0.0008664172735471005, policy loss: 8.60073407237783
Experience 8, Iter 91, disc loss: 0.0008568418439314251, policy loss: 8.48996892228321
Experience 8, Iter 92, disc loss: 0.0007958808378685622, policy loss: 8.74638920988269
Experience 8, Iter 93, disc loss: 0.000890454504876103, policy loss: 8.360462375151727
Experience 8, Iter 94, disc loss: 0.0008580625964906712, policy loss: 8.654426130553265
Experience 8, Iter 95, disc loss: 0.0008567592022938558, policy loss: 8.413412524694468
Experience 8, Iter 96, disc loss: 0.0008788918990662411, policy loss: 8.722958678797813
Experience 8, Iter 97, disc loss: 0.0008229705402983626, policy loss: 8.620790036631302
Experience 8, Iter 98, disc loss: 0.0009025376189699306, policy loss: 8.20782549035906
Experience 8, Iter 99, disc loss: 0.0008677570043956101, policy loss: 8.283327284104267
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1684],
        [1.5302],
        [0.0021]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.6767e-02, 1.2029e-01, 2.3849e-01, 5.4273e-03, 3.0595e-04,
          3.1846e+00]],

        [[1.6767e-02, 1.2029e-01, 2.3849e-01, 5.4273e-03, 3.0595e-04,
          3.1846e+00]],

        [[1.6767e-02, 1.2029e-01, 2.3849e-01, 5.4273e-03, 3.0595e-04,
          3.1846e+00]],

        [[1.6767e-02, 1.2029e-01, 2.3849e-01, 5.4273e-03, 3.0595e-04,
          3.1846e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0119, 0.6737, 6.1207, 0.0086], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0119, 0.6737, 6.1207, 0.0086])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.945
Iter 2/2000 - Loss: 2.347
Iter 3/2000 - Loss: 1.944
Iter 4/2000 - Loss: 1.922
Iter 5/2000 - Loss: 2.053
Iter 6/2000 - Loss: 2.051
Iter 7/2000 - Loss: 1.920
Iter 8/2000 - Loss: 1.800
Iter 9/2000 - Loss: 1.805
Iter 10/2000 - Loss: 1.886
Iter 11/2000 - Loss: 1.903
Iter 12/2000 - Loss: 1.830
Iter 13/2000 - Loss: 1.740
Iter 14/2000 - Loss: 1.685
Iter 15/2000 - Loss: 1.660
Iter 16/2000 - Loss: 1.634
Iter 17/2000 - Loss: 1.585
Iter 18/2000 - Loss: 1.504
Iter 19/2000 - Loss: 1.399
Iter 20/2000 - Loss: 1.279
Iter 1981/2000 - Loss: -8.056
Iter 1982/2000 - Loss: -8.056
Iter 1983/2000 - Loss: -8.056
Iter 1984/2000 - Loss: -8.056
Iter 1985/2000 - Loss: -8.056
Iter 1986/2000 - Loss: -8.056
Iter 1987/2000 - Loss: -8.056
Iter 1988/2000 - Loss: -8.056
Iter 1989/2000 - Loss: -8.056
Iter 1990/2000 - Loss: -8.056
Iter 1991/2000 - Loss: -8.056
Iter 1992/2000 - Loss: -8.056
Iter 1993/2000 - Loss: -8.056
Iter 1994/2000 - Loss: -8.056
Iter 1995/2000 - Loss: -8.056
Iter 1996/2000 - Loss: -8.056
Iter 1997/2000 - Loss: -8.056
Iter 1998/2000 - Loss: -8.056
Iter 1999/2000 - Loss: -8.056
Iter 2000/2000 - Loss: -8.057
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[18.4396,  7.2868, 36.3015,  9.5124,  7.6524, 60.8941]],

        [[21.1705, 32.9035, 18.3975,  3.8984,  8.5253, 25.4940]],

        [[22.9585, 40.2673, 21.9232,  0.8970,  5.1219, 36.6351]],

        [[22.8867, 47.9041, 12.9579,  4.1969,  7.4895, 50.2418]]])
Signal Variance: tensor([ 0.1252,  3.6434, 28.0833,  0.4312])
Estimated target variance: tensor([0.0119, 0.6737, 6.1207, 0.0086])
N: 90
Signal to noise ratio: tensor([ 20.5737,  91.6482, 110.0613,  41.7194])
Bound on condition number: tensor([  38096.0260,  755945.9024, 1090214.9162,  156647.0479])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0008948437959713854, policy loss: 8.307654573935096
Experience 9, Iter 1, disc loss: 0.0008662144207363855, policy loss: 8.442202927266262
Experience 9, Iter 2, disc loss: 0.0008486549074179647, policy loss: 8.599372528250125
Experience 9, Iter 3, disc loss: 0.0008921505286613072, policy loss: 8.360605263310525
Experience 9, Iter 4, disc loss: 0.0009118724762162401, policy loss: 8.276938518553909
Experience 9, Iter 5, disc loss: 0.0008911209759458545, policy loss: 8.30433456817964
Experience 9, Iter 6, disc loss: 0.0009032964861088061, policy loss: 8.21235874464927
Experience 9, Iter 7, disc loss: 0.0009069723452658906, policy loss: 8.363819881037923
Experience 9, Iter 8, disc loss: 0.0008922073643214435, policy loss: 8.438240529938795
Experience 9, Iter 9, disc loss: 0.0009252779487752458, policy loss: 8.118696984101529
Experience 9, Iter 10, disc loss: 0.0008820977715259669, policy loss: 8.336806134986794
Experience 9, Iter 11, disc loss: 0.0009197690827961516, policy loss: 8.219518445934183
Experience 9, Iter 12, disc loss: 0.0009049388597590457, policy loss: 8.278368111522301
Experience 9, Iter 13, disc loss: 0.0008853929629062667, policy loss: 8.381320690392101
Experience 9, Iter 14, disc loss: 0.0008984842344254744, policy loss: 8.28781971507042
Experience 9, Iter 15, disc loss: 0.0009243016071309795, policy loss: 8.10484554138894
Experience 9, Iter 16, disc loss: 0.000969644939136261, policy loss: 8.155408287416751
Experience 9, Iter 17, disc loss: 0.0009671367227583739, policy loss: 8.141307213023053
Experience 9, Iter 18, disc loss: 0.0009236384782874993, policy loss: 8.167974038859995
Experience 9, Iter 19, disc loss: 0.0009363288704041864, policy loss: 8.135578495660727
Experience 9, Iter 20, disc loss: 0.0010108501878310341, policy loss: 8.062313405941975
Experience 9, Iter 21, disc loss: 0.0009674708859207688, policy loss: 8.061445259941879
Experience 9, Iter 22, disc loss: 0.0009414155727538811, policy loss: 8.164809240460432
Experience 9, Iter 23, disc loss: 0.0010845365012137939, policy loss: 7.869740476886533
Experience 9, Iter 24, disc loss: 0.0010579885557467199, policy loss: 8.00069058345289
Experience 9, Iter 25, disc loss: 0.0010169419985752268, policy loss: 8.188982355867262
Experience 9, Iter 26, disc loss: 0.0010939236630687606, policy loss: 7.950278242700825
Experience 9, Iter 27, disc loss: 0.001080615566928335, policy loss: 7.922431543389791
Experience 9, Iter 28, disc loss: 0.0014624988667323716, policy loss: 7.44959919635541
Experience 9, Iter 29, disc loss: 0.0018124709827213705, policy loss: 7.347132620903984
Experience 9, Iter 30, disc loss: 0.0028313831023978327, policy loss: 6.984622672441017
Experience 9, Iter 31, disc loss: 0.004101966710865536, policy loss: 6.502628554502943
Experience 9, Iter 32, disc loss: 0.0042081067068864214, policy loss: 6.457480052266581
Experience 9, Iter 33, disc loss: 0.004041320833998537, policy loss: 6.78913764511552
Experience 9, Iter 34, disc loss: 0.004532375292983001, policy loss: 6.592425804825119
Experience 9, Iter 35, disc loss: 0.00460917425798936, policy loss: 6.571323510267366
Experience 9, Iter 36, disc loss: 0.004769988860168964, policy loss: 6.597012275523147
Experience 9, Iter 37, disc loss: 0.004023193844353767, policy loss: 6.350049059433501
Experience 9, Iter 38, disc loss: 0.004693455893512202, policy loss: 6.139968194576367
Experience 9, Iter 39, disc loss: 0.003975989296116661, policy loss: 6.692826721549879
Experience 9, Iter 40, disc loss: 0.004523100014418549, policy loss: 6.508347000272487
Experience 9, Iter 41, disc loss: 0.004210075134185785, policy loss: 6.5510882740627805
Experience 9, Iter 42, disc loss: 0.005103093922162816, policy loss: 6.370706419980661
Experience 9, Iter 43, disc loss: 0.00468582119475307, policy loss: 6.513644355164437
Experience 9, Iter 44, disc loss: 0.005098792132650118, policy loss: 6.189101512194017
Experience 9, Iter 45, disc loss: 0.005340040145269173, policy loss: 6.779274979694177
Experience 9, Iter 46, disc loss: 0.00838257307294677, policy loss: 6.062566188915918
Experience 9, Iter 47, disc loss: 0.01033495421611593, policy loss: 6.492211234567417
Experience 9, Iter 48, disc loss: 0.017949339002341684, policy loss: 5.731147932981489
Experience 9, Iter 49, disc loss: 0.01610375652881944, policy loss: 6.286122474638708
Experience 9, Iter 50, disc loss: 0.011331917449365528, policy loss: 6.169511036702979
Experience 9, Iter 51, disc loss: 0.006938710625073054, policy loss: 7.026027384137424
Experience 9, Iter 52, disc loss: 0.006056347473665916, policy loss: 6.664331934660091
Experience 9, Iter 53, disc loss: 0.0089039990994036, policy loss: 6.68274702812939
Experience 9, Iter 54, disc loss: 0.007380526248855716, policy loss: 6.466796978274916
Experience 9, Iter 55, disc loss: 0.012770882231490506, policy loss: 6.207733146671784
Experience 9, Iter 56, disc loss: 0.010747400467192941, policy loss: 6.434533544001363
Experience 9, Iter 57, disc loss: 0.00983065089229393, policy loss: 6.849551375185892
Experience 9, Iter 58, disc loss: 0.00746276102221588, policy loss: 7.0976265958636215
Experience 9, Iter 59, disc loss: 0.007595800341151682, policy loss: 7.096739623004517
Experience 9, Iter 60, disc loss: 0.006850198101990621, policy loss: 7.5042808786055915
Experience 9, Iter 61, disc loss: 0.009988631476489845, policy loss: 7.072121971591934
Experience 9, Iter 62, disc loss: 0.00797616967942692, policy loss: 7.201347299619257
Experience 9, Iter 63, disc loss: 0.007906120236856868, policy loss: 7.0042123522124875
Experience 9, Iter 64, disc loss: 0.0065631568437507296, policy loss: 7.714008387208565
Experience 9, Iter 65, disc loss: 0.008100082339809685, policy loss: 6.860167145735842
Experience 9, Iter 66, disc loss: 0.009828143249178313, policy loss: 6.925146387882391
Experience 9, Iter 67, disc loss: 0.013463817611964394, policy loss: 6.379246967406516
Experience 9, Iter 68, disc loss: 0.01591314316709888, policy loss: 6.927029128260838
Experience 9, Iter 69, disc loss: 0.02148285491426202, policy loss: 6.250035066228937
Experience 9, Iter 70, disc loss: 0.011970430081636816, policy loss: 7.184707881049146
Experience 9, Iter 71, disc loss: 0.012065573150553375, policy loss: 7.065980883016418
Experience 9, Iter 72, disc loss: 0.017135854014580645, policy loss: 6.471669562210067
Experience 9, Iter 73, disc loss: 0.0093943557469038, policy loss: 7.256754816527626
Experience 9, Iter 74, disc loss: 0.010175092230506803, policy loss: 7.306347123323053
Experience 9, Iter 75, disc loss: 0.008797920812043027, policy loss: 7.323628352568405
Experience 9, Iter 76, disc loss: 0.007168849531412126, policy loss: 7.629309770979679
Experience 9, Iter 77, disc loss: 0.0064677800044660415, policy loss: 7.59051651858487
Experience 9, Iter 78, disc loss: 0.007170058039870754, policy loss: 7.352088905240311
Experience 9, Iter 79, disc loss: 0.005594829633358231, policy loss: 8.312279436890359
Experience 9, Iter 80, disc loss: 0.006303478588294442, policy loss: 7.241269819005872
Experience 9, Iter 81, disc loss: 0.006328964198452278, policy loss: 7.760328522172033
Experience 9, Iter 82, disc loss: 0.008332716895834905, policy loss: 6.89826462259261
Experience 9, Iter 83, disc loss: 0.009777769991154577, policy loss: 6.706674454148284
Experience 9, Iter 84, disc loss: 0.017695320361343733, policy loss: 6.413783282973266
Experience 9, Iter 85, disc loss: 0.023222745107574265, policy loss: 5.737681261278428
Experience 9, Iter 86, disc loss: 0.015411227675869537, policy loss: 6.399127643982113
Experience 9, Iter 87, disc loss: 0.01650410957910097, policy loss: 7.281058627614069
Experience 9, Iter 88, disc loss: 0.010952294577725234, policy loss: 8.123504392656958
Experience 9, Iter 89, disc loss: 0.011093812648911901, policy loss: 7.34455457627111
Experience 9, Iter 90, disc loss: 0.015556459253183209, policy loss: 8.204481951895454
Experience 9, Iter 91, disc loss: 0.00954060267750758, policy loss: 8.628054267320323
Experience 9, Iter 92, disc loss: 0.016412166144330942, policy loss: 6.773628647972157
Experience 9, Iter 93, disc loss: 0.008877357781266783, policy loss: 8.406285595728708
Experience 9, Iter 94, disc loss: 0.009714523714277537, policy loss: 7.320733317242171
Experience 9, Iter 95, disc loss: 0.009197744250340258, policy loss: 7.117826181150846
Experience 9, Iter 96, disc loss: 0.008561943155570635, policy loss: 8.046594508707434
Experience 9, Iter 97, disc loss: 0.007548144590459513, policy loss: 8.074620158206013
Experience 9, Iter 98, disc loss: 0.0072333138864550915, policy loss: 7.843537684462818
Experience 9, Iter 99, disc loss: 0.006364982008435962, policy loss: 7.648105648574149
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1695],
        [1.5591],
        [0.0024]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.5121e-02, 1.1250e-01, 2.5138e-01, 5.4301e-03, 2.8597e-04,
          3.1863e+00]],

        [[1.5121e-02, 1.1250e-01, 2.5138e-01, 5.4301e-03, 2.8597e-04,
          3.1863e+00]],

        [[1.5121e-02, 1.1250e-01, 2.5138e-01, 5.4301e-03, 2.8597e-04,
          3.1863e+00]],

        [[1.5121e-02, 1.1250e-01, 2.5138e-01, 5.4301e-03, 2.8597e-04,
          3.1863e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0110, 0.6781, 6.2364, 0.0094], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0110, 0.6781, 6.2364, 0.0094])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.958
Iter 2/2000 - Loss: 2.312
Iter 3/2000 - Loss: 1.955
Iter 4/2000 - Loss: 1.918
Iter 5/2000 - Loss: 2.043
Iter 6/2000 - Loss: 2.042
Iter 7/2000 - Loss: 1.905
Iter 8/2000 - Loss: 1.804
Iter 9/2000 - Loss: 1.838
Iter 10/2000 - Loss: 1.908
Iter 11/2000 - Loss: 1.893
Iter 12/2000 - Loss: 1.812
Iter 13/2000 - Loss: 1.743
Iter 14/2000 - Loss: 1.711
Iter 15/2000 - Loss: 1.693
Iter 16/2000 - Loss: 1.662
Iter 17/2000 - Loss: 1.602
Iter 18/2000 - Loss: 1.515
Iter 19/2000 - Loss: 1.411
Iter 20/2000 - Loss: 1.298
Iter 1981/2000 - Loss: -8.072
Iter 1982/2000 - Loss: -8.072
Iter 1983/2000 - Loss: -8.072
Iter 1984/2000 - Loss: -8.072
Iter 1985/2000 - Loss: -8.072
Iter 1986/2000 - Loss: -8.072
Iter 1987/2000 - Loss: -8.072
Iter 1988/2000 - Loss: -8.072
Iter 1989/2000 - Loss: -8.072
Iter 1990/2000 - Loss: -8.072
Iter 1991/2000 - Loss: -8.072
Iter 1992/2000 - Loss: -8.072
Iter 1993/2000 - Loss: -8.072
Iter 1994/2000 - Loss: -8.072
Iter 1995/2000 - Loss: -8.072
Iter 1996/2000 - Loss: -8.073
Iter 1997/2000 - Loss: -8.073
Iter 1998/2000 - Loss: -8.073
Iter 1999/2000 - Loss: -8.073
Iter 2000/2000 - Loss: -8.073
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[18.3249,  7.6169, 36.9374, 11.8973,  7.1478, 60.7887]],

        [[21.3020, 39.1740, 28.6504,  4.6697,  1.0104, 19.8814]],

        [[21.6083, 40.0388, 23.0942,  0.8437,  5.0236, 36.0968]],

        [[21.7849, 45.7422, 13.5250,  3.8877,  7.0539, 36.9387]]])
Signal Variance: tensor([ 0.1270,  2.7675, 26.3033,  0.3521])
Estimated target variance: tensor([0.0110, 0.6781, 6.2364, 0.0094])
N: 100
Signal to noise ratio: tensor([ 20.8027,  79.8819, 108.1869,  36.4187])
Bound on condition number: tensor([  43276.3170,  638112.6173, 1170440.8806,  132632.8081])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.005706433060596722, policy loss: 7.87965505468336
Experience 10, Iter 1, disc loss: 0.0050029831658535955, policy loss: 8.220930268531738
Experience 10, Iter 2, disc loss: 0.004754976063840173, policy loss: 7.482320550544462
Experience 10, Iter 3, disc loss: 0.004139307086668158, policy loss: 8.06607070944657
Experience 10, Iter 4, disc loss: 0.004688326285134961, policy loss: 8.135714905115215
Experience 10, Iter 5, disc loss: 0.0034884951168023598, policy loss: 8.16775042134285
Experience 10, Iter 6, disc loss: 0.003352546425091567, policy loss: 8.192773898232689
Experience 10, Iter 7, disc loss: 0.0027769449088376206, policy loss: 8.491486362615312
Experience 10, Iter 8, disc loss: 0.002833501341211177, policy loss: 8.181720724079504
Experience 10, Iter 9, disc loss: 0.0028178966493938125, policy loss: 7.849342005862317
Experience 10, Iter 10, disc loss: 0.0032048611850162283, policy loss: 7.67148945869021
Experience 10, Iter 11, disc loss: 0.0036006820200405448, policy loss: 7.637667387642416
Experience 10, Iter 12, disc loss: 0.002793026520703738, policy loss: 7.717277048417972
Experience 10, Iter 13, disc loss: 0.0033894084738104783, policy loss: 7.6215060805905654
Experience 10, Iter 14, disc loss: 0.0029283249927110813, policy loss: 8.154054616601695
Experience 10, Iter 15, disc loss: 0.0030938075098443903, policy loss: 7.625705342995673
Experience 10, Iter 16, disc loss: 0.003319699767500339, policy loss: 7.371765020027517
Experience 10, Iter 17, disc loss: 0.0034139687199992884, policy loss: 7.202061002226934
Experience 10, Iter 18, disc loss: 0.0023916925956804893, policy loss: 8.316497592881896
Experience 10, Iter 19, disc loss: 0.0027015778822543877, policy loss: 7.884896165771151
Experience 10, Iter 20, disc loss: 0.0036968669841007584, policy loss: 7.369656624413124
Experience 10, Iter 21, disc loss: 0.002653839471915901, policy loss: 7.584983400120578
Experience 10, Iter 22, disc loss: 0.0022417923608607166, policy loss: 8.064143634929222
Experience 10, Iter 23, disc loss: 0.002487695449148024, policy loss: 7.5945510137265675
Experience 10, Iter 24, disc loss: 0.0024926615526967145, policy loss: 8.201982582936425
Experience 10, Iter 25, disc loss: 0.0025628070830571066, policy loss: 7.828311151170293
Experience 10, Iter 26, disc loss: 0.002787845262405504, policy loss: 7.164785599991671
Experience 10, Iter 27, disc loss: 0.002122555182914941, policy loss: 8.29337014527162
Experience 10, Iter 28, disc loss: 0.002403638179266417, policy loss: 7.632781834794066
Experience 10, Iter 29, disc loss: 0.0024980223272734704, policy loss: 7.467345962096946
Experience 10, Iter 30, disc loss: 0.0024474452658032842, policy loss: 8.172248763330334
Experience 10, Iter 31, disc loss: 0.002394448697225729, policy loss: 7.399814552576488
Experience 10, Iter 32, disc loss: 0.0020314185248931325, policy loss: 7.906869549035008
Experience 10, Iter 33, disc loss: 0.0020951149820106763, policy loss: 7.2999576953614
Experience 10, Iter 34, disc loss: 0.0018876475906124549, policy loss: 8.114528521123287
Experience 10, Iter 35, disc loss: 0.002365304310735364, policy loss: 8.193352622062719
Experience 10, Iter 36, disc loss: 0.002539507235245678, policy loss: 8.393802988129282
Experience 10, Iter 37, disc loss: 0.002361461401934919, policy loss: 7.970506334800682
Experience 10, Iter 38, disc loss: 0.002510375703788201, policy loss: 7.841899443050826
Experience 10, Iter 39, disc loss: 0.0028638979863492586, policy loss: 7.537504895214222
Experience 10, Iter 40, disc loss: 0.0025442853274296254, policy loss: 7.565743497750128
Experience 10, Iter 41, disc loss: 0.002602555472078038, policy loss: 8.692075945433366
Experience 10, Iter 42, disc loss: 0.00258411683378501, policy loss: 8.030310453463784
Experience 10, Iter 43, disc loss: 0.003063680258708975, policy loss: 7.858312310482578
Experience 10, Iter 44, disc loss: 0.0025127213275807146, policy loss: 8.013678718279014
Experience 10, Iter 45, disc loss: 0.0027863095364663564, policy loss: 7.863181903063403
Experience 10, Iter 46, disc loss: 0.002881719480333197, policy loss: 7.85737822965284
Experience 10, Iter 47, disc loss: 0.0024202899232787563, policy loss: 8.105674872317685
Experience 10, Iter 48, disc loss: 0.002200419081915681, policy loss: 8.692391168828092
Experience 10, Iter 49, disc loss: 0.002120578932831972, policy loss: 9.138654584731299
Experience 10, Iter 50, disc loss: 0.0027301942977554093, policy loss: 8.207053708929287
Experience 10, Iter 51, disc loss: 0.002235561513987835, policy loss: 9.006613063669302
Experience 10, Iter 52, disc loss: 0.002122517979312192, policy loss: 8.685617829489367
Experience 10, Iter 53, disc loss: 0.002199609712653694, policy loss: 8.13132477556805
Experience 10, Iter 54, disc loss: 0.0021361034923559866, policy loss: 8.900682367778346
Experience 10, Iter 55, disc loss: 0.0024511950126403746, policy loss: 8.923787829549997
Experience 10, Iter 56, disc loss: 0.0019340553958451313, policy loss: 8.461304988577444
Experience 10, Iter 57, disc loss: 0.001806946784563767, policy loss: 8.615985509196001
Experience 10, Iter 58, disc loss: 0.0016220801410559843, policy loss: 9.14525499494019
Experience 10, Iter 59, disc loss: 0.001821865463739952, policy loss: 8.44524481463312
Experience 10, Iter 60, disc loss: 0.0014083993727498564, policy loss: 9.516745659798959
Experience 10, Iter 61, disc loss: 0.0014052699046852484, policy loss: 9.226024174016295
Experience 10, Iter 62, disc loss: 0.0018787461282001139, policy loss: 8.221697461149201
Experience 10, Iter 63, disc loss: 0.0016360725668792095, policy loss: 8.403454348968626
Experience 10, Iter 64, disc loss: 0.0018198410319502857, policy loss: 8.462004271824508
Experience 10, Iter 65, disc loss: 0.0015399470373241666, policy loss: 8.858364098989682
Experience 10, Iter 66, disc loss: 0.0015829359258310227, policy loss: 8.326485744892016
Experience 10, Iter 67, disc loss: 0.0014954251941276047, policy loss: 8.37288931664157
Experience 10, Iter 68, disc loss: 0.0015483905574733576, policy loss: 8.74511803959069
Experience 10, Iter 69, disc loss: 0.0016901896490051646, policy loss: 8.42357130913306
Experience 10, Iter 70, disc loss: 0.0015419665974092372, policy loss: 8.421678128593378
Experience 10, Iter 71, disc loss: 0.0014316924453064352, policy loss: 8.420573301832
Experience 10, Iter 72, disc loss: 0.0017698476218569476, policy loss: 8.02055102396988
Experience 10, Iter 73, disc loss: 0.0018013426171638969, policy loss: 7.748801990080549
Experience 10, Iter 74, disc loss: 0.0018403817981871972, policy loss: 7.450305338970447
Experience 10, Iter 75, disc loss: 0.0019675411720559005, policy loss: 7.9374243252916665
Experience 10, Iter 76, disc loss: 0.0014364845557412916, policy loss: 8.303433023820162
Experience 10, Iter 77, disc loss: 0.0014254575132753048, policy loss: 8.323252823525188
Experience 10, Iter 78, disc loss: 0.0015198260882699408, policy loss: 8.344510704503861
Experience 10, Iter 79, disc loss: 0.0014002332819835366, policy loss: 8.333159242635702
Experience 10, Iter 80, disc loss: 0.001257693860466329, policy loss: 8.498051782103023
Experience 10, Iter 81, disc loss: 0.0016829963743399637, policy loss: 8.27211062567542
Experience 10, Iter 82, disc loss: 0.0017099550758157473, policy loss: 7.873284305135457
Experience 10, Iter 83, disc loss: 0.001961082108258215, policy loss: 8.107721370445791
Experience 10, Iter 84, disc loss: 0.0014809914200575266, policy loss: 8.391941151676699
Experience 10, Iter 85, disc loss: 0.00161681068113312, policy loss: 8.281147565896251
Experience 10, Iter 86, disc loss: 0.001361091556112845, policy loss: 8.75257075906202
Experience 10, Iter 87, disc loss: 0.0013004803644930867, policy loss: 8.21943172475433
Experience 10, Iter 88, disc loss: 0.0014105516459232653, policy loss: 8.159282078270046
Experience 10, Iter 89, disc loss: 0.0015016107903445036, policy loss: 8.127823043603174
Experience 10, Iter 90, disc loss: 0.0014849720183930588, policy loss: 8.181145216865948
Experience 10, Iter 91, disc loss: 0.0018330332240446645, policy loss: 7.354221806208359
Experience 10, Iter 92, disc loss: 0.0016551721956757095, policy loss: 8.283673895563313
Experience 10, Iter 93, disc loss: 0.0017692300080558413, policy loss: 7.770124405092624
Experience 10, Iter 94, disc loss: 0.001373860356374083, policy loss: 8.421522292787298
Experience 10, Iter 95, disc loss: 0.0014298711024393943, policy loss: 8.340119643146302
Experience 10, Iter 96, disc loss: 0.0018862636481141125, policy loss: 7.702211575835253
Experience 10, Iter 97, disc loss: 0.0014953283641622153, policy loss: 8.258903546075512
Experience 10, Iter 98, disc loss: 0.0015334903996123313, policy loss: 8.293564764683852
Experience 10, Iter 99, disc loss: 0.0016268404208964814, policy loss: 8.24766920171498
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1700],
        [1.5907],
        [0.0029]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3879e-02, 1.0540e-01, 2.7625e-01, 6.0578e-03, 2.9384e-04,
          3.1954e+00]],

        [[1.3879e-02, 1.0540e-01, 2.7625e-01, 6.0578e-03, 2.9384e-04,
          3.1954e+00]],

        [[1.3879e-02, 1.0540e-01, 2.7625e-01, 6.0578e-03, 2.9384e-04,
          3.1954e+00]],

        [[1.3879e-02, 1.0540e-01, 2.7625e-01, 6.0578e-03, 2.9384e-04,
          3.1954e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0102, 0.6799, 6.3629, 0.0117], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0102, 0.6799, 6.3629, 0.0117])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.035
Iter 2/2000 - Loss: 2.314
Iter 3/2000 - Loss: 2.025
Iter 4/2000 - Loss: 1.970
Iter 5/2000 - Loss: 2.077
Iter 6/2000 - Loss: 2.077
Iter 7/2000 - Loss: 1.945
Iter 8/2000 - Loss: 1.862
Iter 9/2000 - Loss: 1.905
Iter 10/2000 - Loss: 1.954
Iter 11/2000 - Loss: 1.914
Iter 12/2000 - Loss: 1.831
Iter 13/2000 - Loss: 1.777
Iter 14/2000 - Loss: 1.758
Iter 15/2000 - Loss: 1.738
Iter 16/2000 - Loss: 1.690
Iter 17/2000 - Loss: 1.612
Iter 18/2000 - Loss: 1.518
Iter 19/2000 - Loss: 1.417
Iter 20/2000 - Loss: 1.307
Iter 1981/2000 - Loss: -8.134
Iter 1982/2000 - Loss: -8.134
Iter 1983/2000 - Loss: -8.134
Iter 1984/2000 - Loss: -8.134
Iter 1985/2000 - Loss: -8.134
Iter 1986/2000 - Loss: -8.134
Iter 1987/2000 - Loss: -8.134
Iter 1988/2000 - Loss: -8.134
Iter 1989/2000 - Loss: -8.134
Iter 1990/2000 - Loss: -8.134
Iter 1991/2000 - Loss: -8.134
Iter 1992/2000 - Loss: -8.135
Iter 1993/2000 - Loss: -8.135
Iter 1994/2000 - Loss: -8.135
Iter 1995/2000 - Loss: -8.135
Iter 1996/2000 - Loss: -8.135
Iter 1997/2000 - Loss: -8.135
Iter 1998/2000 - Loss: -8.135
Iter 1999/2000 - Loss: -8.135
Iter 2000/2000 - Loss: -8.135
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[16.7935,  7.4362, 36.4677, 14.9606,  6.4154, 59.9725]],

        [[20.6751, 36.9694, 28.5899,  3.7221,  0.7517, 18.7401]],

        [[19.8267, 38.2459, 14.9281,  0.9278,  4.5644, 18.6879]],

        [[19.6117, 44.1869, 14.1378,  3.9613,  6.6768, 40.0993]]])
Signal Variance: tensor([ 0.1171,  2.2891, 17.0409,  0.4037])
Estimated target variance: tensor([0.0102, 0.6799, 6.3629, 0.0117])
N: 110
Signal to noise ratio: tensor([20.3901, 74.1849, 90.5837, 39.5043])
Bound on condition number: tensor([ 45733.9710, 605375.2624, 902596.1317, 171666.0115])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0013972711923613678, policy loss: 9.380349344675931
Experience 11, Iter 1, disc loss: 0.001446591656425882, policy loss: 8.588619081667566
Experience 11, Iter 2, disc loss: 0.0016452565650422434, policy loss: 8.60890464261032
Experience 11, Iter 3, disc loss: 0.001572228153175715, policy loss: 8.512552493077834
Experience 11, Iter 4, disc loss: 0.001664967677341324, policy loss: 8.810430866518352
Experience 11, Iter 5, disc loss: 0.0015023178909462599, policy loss: 8.690313903497753
Experience 11, Iter 6, disc loss: 0.002015165180297041, policy loss: 7.680276656640868
Experience 11, Iter 7, disc loss: 0.00162812802089086, policy loss: 8.36920282045007
Experience 11, Iter 8, disc loss: 0.0015310836402085302, policy loss: 8.02478969778906
Experience 11, Iter 9, disc loss: 0.0015657761200828333, policy loss: 8.666304714988794
Experience 11, Iter 10, disc loss: 0.0016419354665056967, policy loss: 8.190848333865919
Experience 11, Iter 11, disc loss: 0.0012516415153759697, policy loss: 8.83391506805238
Experience 11, Iter 12, disc loss: 0.0014101869353380315, policy loss: 9.050350103062387
Experience 11, Iter 13, disc loss: 0.0012037354105541892, policy loss: 8.86444402607301
Experience 11, Iter 14, disc loss: 0.001327341040995547, policy loss: 8.743767698317809
Experience 11, Iter 15, disc loss: 0.0010960848927113512, policy loss: 8.365867474260714
Experience 11, Iter 16, disc loss: 0.00139068405077484, policy loss: 8.41701233177424
Experience 11, Iter 17, disc loss: 0.0013092657797605404, policy loss: 8.345593954938737
Experience 11, Iter 18, disc loss: 0.0016348356166837172, policy loss: 7.5559700249222574
Experience 11, Iter 19, disc loss: 0.0013754834258955166, policy loss: 8.450522185362061
Experience 11, Iter 20, disc loss: 0.0013087352160136697, policy loss: 7.984307185610051
Experience 11, Iter 21, disc loss: 0.001550818318832844, policy loss: 7.76737179315926
Experience 11, Iter 22, disc loss: 0.0018703680459010114, policy loss: 7.399205268971627
Experience 11, Iter 23, disc loss: 0.0013236925276143992, policy loss: 8.74735375994104
Experience 11, Iter 24, disc loss: 0.0013756076036485427, policy loss: 8.287675091970044
Experience 11, Iter 25, disc loss: 0.0012784081209319192, policy loss: 8.792837330924975
Experience 11, Iter 26, disc loss: 0.0018492422951596988, policy loss: 8.13510526979152
Experience 11, Iter 27, disc loss: 0.0020435050911627364, policy loss: 8.464336512739738
Experience 11, Iter 28, disc loss: 0.0015734855195039953, policy loss: 8.40647880683424
Experience 11, Iter 29, disc loss: 0.0014031080699889273, policy loss: 8.280669621000643
Experience 11, Iter 30, disc loss: 0.0013576490474337811, policy loss: 9.312223841460282
Experience 11, Iter 31, disc loss: 0.002229830414205688, policy loss: 8.707140355893056
Experience 11, Iter 32, disc loss: 0.0015150503527851824, policy loss: 8.833318562693416
Experience 11, Iter 33, disc loss: 0.0027639816098346624, policy loss: 8.55442910165716
Experience 11, Iter 34, disc loss: 0.00140389275831418, policy loss: 8.285982658940458
Experience 11, Iter 35, disc loss: 0.0027315166343272962, policy loss: 8.570166134472823
Experience 11, Iter 36, disc loss: 0.0017294138283462982, policy loss: 8.56615935589468
Experience 11, Iter 37, disc loss: 0.002068140108087385, policy loss: 8.49400766646655
Experience 11, Iter 38, disc loss: 0.001632187199510028, policy loss: 8.586069078688366
Experience 11, Iter 39, disc loss: 0.0016147967754414213, policy loss: 8.469272819428676
Experience 11, Iter 40, disc loss: 0.0013115839533234083, policy loss: 8.828886958118478
Experience 11, Iter 41, disc loss: 0.0013609388564151993, policy loss: 8.374362423943492
Experience 11, Iter 42, disc loss: 0.0014600979220202564, policy loss: 8.882671213582896
Experience 11, Iter 43, disc loss: 0.0014076387015572163, policy loss: 9.07035601127078
Experience 11, Iter 44, disc loss: 0.001438080711365016, policy loss: 8.52142897705965
Experience 11, Iter 45, disc loss: 0.0013334380541625802, policy loss: 8.531673773276236
Experience 11, Iter 46, disc loss: 0.0012272762500133941, policy loss: 8.654032513728179
Experience 11, Iter 47, disc loss: 0.0011589295831682634, policy loss: 8.970347377130883
Experience 11, Iter 48, disc loss: 0.0013544899639836724, policy loss: 8.32885272851549
Experience 11, Iter 49, disc loss: 0.0012033650120689024, policy loss: 8.697076414813376
Experience 11, Iter 50, disc loss: 0.0013344167027617019, policy loss: 8.85819259048948
Experience 11, Iter 51, disc loss: 0.0011553954909974068, policy loss: 9.140585503523125
Experience 11, Iter 52, disc loss: 0.0011802788106999355, policy loss: 8.710774688760004
Experience 11, Iter 53, disc loss: 0.0011189474210475934, policy loss: 8.799049131499551
Experience 11, Iter 54, disc loss: 0.001353068013786246, policy loss: 8.039364152440896
Experience 11, Iter 55, disc loss: 0.0012327742560156914, policy loss: 8.647630757631202
Experience 11, Iter 56, disc loss: 0.001118224607101533, policy loss: 8.671116603176763
Experience 11, Iter 57, disc loss: 0.0010571620670607885, policy loss: 8.744070286741803
Experience 11, Iter 58, disc loss: 0.001206822235222291, policy loss: 8.073598507440037
Experience 11, Iter 59, disc loss: 0.0012311321018449498, policy loss: 8.176566957484697
Experience 11, Iter 60, disc loss: 0.0011294816190650083, policy loss: 8.414024755690914
Experience 11, Iter 61, disc loss: 0.0011124196228084493, policy loss: 8.40748177270217
Experience 11, Iter 62, disc loss: 0.0010495123158830784, policy loss: 8.94010572329507
Experience 11, Iter 63, disc loss: 0.000953926922248277, policy loss: 9.235179003542422
Experience 11, Iter 64, disc loss: 0.0011304128257264207, policy loss: 8.841301970738032
Experience 11, Iter 65, disc loss: 0.0010703459774436923, policy loss: 8.801093971372481
Experience 11, Iter 66, disc loss: 0.0011615521361301071, policy loss: 9.075451864277039
Experience 11, Iter 67, disc loss: 0.0011689399890932803, policy loss: 8.97331801151359
Experience 11, Iter 68, disc loss: 0.0009949078679208317, policy loss: 8.841211182830733
Experience 11, Iter 69, disc loss: 0.0012143945005212617, policy loss: 8.81855210913719
Experience 11, Iter 70, disc loss: 0.001039838100890375, policy loss: 8.425406834190412
Experience 11, Iter 71, disc loss: 0.0010432349954212314, policy loss: 8.932933470053062
Experience 11, Iter 72, disc loss: 0.0011489570846043788, policy loss: 8.491106334209197
Experience 11, Iter 73, disc loss: 0.001042289238773834, policy loss: 8.283142070647884
Experience 11, Iter 74, disc loss: 0.000994869431490799, policy loss: 8.238816788103566
Experience 11, Iter 75, disc loss: 0.0011466029995703694, policy loss: 8.677174050162845
Experience 11, Iter 76, disc loss: 0.001090306994650732, policy loss: 8.386208559750669
Experience 11, Iter 77, disc loss: 0.0010681956801364412, policy loss: 8.706686554129334
Experience 11, Iter 78, disc loss: 0.0010894066686912675, policy loss: 8.851023475396186
Experience 11, Iter 79, disc loss: 0.0011410421428823894, policy loss: 8.941828798812617
Experience 11, Iter 80, disc loss: 0.0012096373500481588, policy loss: 8.40299487342646
Experience 11, Iter 81, disc loss: 0.0014596893032754249, policy loss: 8.017950395671718
Experience 11, Iter 82, disc loss: 0.0011827118747599406, policy loss: 8.401945664550455
Experience 11, Iter 83, disc loss: 0.001047481892953274, policy loss: 8.320448048418461
Experience 11, Iter 84, disc loss: 0.0012324250709170807, policy loss: 8.652999646999739
Experience 11, Iter 85, disc loss: 0.0009059471178311067, policy loss: 9.157553240932932
Experience 11, Iter 86, disc loss: 0.0009182741629106676, policy loss: 9.097545722947896
Experience 11, Iter 87, disc loss: 0.0011857054468482702, policy loss: 8.870634173093471
Experience 11, Iter 88, disc loss: 0.0010361032177260369, policy loss: 8.297270734828961
Experience 11, Iter 89, disc loss: 0.0010927241158253684, policy loss: 8.597824783543505
Experience 11, Iter 90, disc loss: 0.0009831030270093172, policy loss: 8.730410502371106
Experience 11, Iter 91, disc loss: 0.0010571681961227622, policy loss: 9.173994768690372
Experience 11, Iter 92, disc loss: 0.001018866203074619, policy loss: 8.791238475442956
Experience 11, Iter 93, disc loss: 0.0010612662697357727, policy loss: 8.666726226323583
Experience 11, Iter 94, disc loss: 0.0011061694452625614, policy loss: 8.366660275740156
Experience 11, Iter 95, disc loss: 0.00098855961062529, policy loss: 8.676923674447638
Experience 11, Iter 96, disc loss: 0.000937706401195205, policy loss: 9.546780245168101
Experience 11, Iter 97, disc loss: 0.0010830141825645013, policy loss: 8.619040960834147
Experience 11, Iter 98, disc loss: 0.0012449904671656565, policy loss: 8.22929675244363
Experience 11, Iter 99, disc loss: 0.001135692570419104, policy loss: 8.643302263400141
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1750],
        [1.6640],
        [0.0039]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2871e-02, 1.0113e-01, 3.1919e-01, 6.7829e-03, 3.2997e-04,
          3.2799e+00]],

        [[1.2871e-02, 1.0113e-01, 3.1919e-01, 6.7829e-03, 3.2997e-04,
          3.2799e+00]],

        [[1.2871e-02, 1.0113e-01, 3.1919e-01, 6.7829e-03, 3.2997e-04,
          3.2799e+00]],

        [[1.2871e-02, 1.0113e-01, 3.1919e-01, 6.7829e-03, 3.2997e-04,
          3.2799e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0096, 0.7002, 6.6560, 0.0154], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0096, 0.7002, 6.6560, 0.0154])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.178
Iter 2/2000 - Loss: 2.386
Iter 3/2000 - Loss: 2.162
Iter 4/2000 - Loss: 2.091
Iter 5/2000 - Loss: 2.182
Iter 6/2000 - Loss: 2.181
Iter 7/2000 - Loss: 2.058
Iter 8/2000 - Loss: 1.994
Iter 9/2000 - Loss: 2.040
Iter 10/2000 - Loss: 2.066
Iter 11/2000 - Loss: 2.007
Iter 12/2000 - Loss: 1.927
Iter 13/2000 - Loss: 1.887
Iter 14/2000 - Loss: 1.870
Iter 15/2000 - Loss: 1.835
Iter 16/2000 - Loss: 1.765
Iter 17/2000 - Loss: 1.674
Iter 18/2000 - Loss: 1.579
Iter 19/2000 - Loss: 1.476
Iter 20/2000 - Loss: 1.358
Iter 1981/2000 - Loss: -8.030
Iter 1982/2000 - Loss: -8.030
Iter 1983/2000 - Loss: -8.030
Iter 1984/2000 - Loss: -8.031
Iter 1985/2000 - Loss: -8.031
Iter 1986/2000 - Loss: -8.031
Iter 1987/2000 - Loss: -8.031
Iter 1988/2000 - Loss: -8.031
Iter 1989/2000 - Loss: -8.031
Iter 1990/2000 - Loss: -8.031
Iter 1991/2000 - Loss: -8.031
Iter 1992/2000 - Loss: -8.031
Iter 1993/2000 - Loss: -8.031
Iter 1994/2000 - Loss: -8.031
Iter 1995/2000 - Loss: -8.031
Iter 1996/2000 - Loss: -8.031
Iter 1997/2000 - Loss: -8.031
Iter 1998/2000 - Loss: -8.031
Iter 1999/2000 - Loss: -8.031
Iter 2000/2000 - Loss: -8.031
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.9295,  7.3809, 39.0927, 12.0138,  7.0133, 59.1234]],

        [[17.1376, 34.1281, 20.1896,  7.3508,  1.3004, 26.5219]],

        [[17.3986, 36.2202, 12.7497,  0.9986,  5.2450, 25.4317]],

        [[17.9630, 33.7173, 11.8904,  3.5624,  1.3840, 43.1598]]])
Signal Variance: tensor([ 0.1147,  5.0768, 24.8126,  0.3360])
Estimated target variance: tensor([0.0096, 0.7002, 6.6560, 0.0154])
N: 120
Signal to noise ratio: tensor([ 20.2634, 107.3261, 108.5224,  33.9774])
Bound on condition number: tensor([  49273.4930, 1382267.3637, 1413255.4036,  138536.8702])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0012608218592177712, policy loss: 8.549115956498252
Experience 12, Iter 1, disc loss: 0.0011253875551073943, policy loss: 8.90724710125972
Experience 12, Iter 2, disc loss: 0.0009676124834375316, policy loss: 8.829156632333437
Experience 12, Iter 3, disc loss: 0.0012053114195268934, policy loss: 8.663249780040136
Experience 12, Iter 4, disc loss: 0.00093177572042037, policy loss: 8.942367008017165
Experience 12, Iter 5, disc loss: 0.0010230961548895437, policy loss: 8.91641072922664
Experience 12, Iter 6, disc loss: 0.0011591564279448932, policy loss: 8.297020919644917
Experience 12, Iter 7, disc loss: 0.0010834165423662372, policy loss: 8.547121711778045
Experience 12, Iter 8, disc loss: 0.0010413654954129728, policy loss: 8.76231028394813
Experience 12, Iter 9, disc loss: 0.0009857825779913904, policy loss: 8.61383342826128
Experience 12, Iter 10, disc loss: 0.0010005334742830873, policy loss: 8.77614428956619
Experience 12, Iter 11, disc loss: 0.0010347707532150467, policy loss: 9.054210738470855
Experience 12, Iter 12, disc loss: 0.0010443396895215694, policy loss: 8.80621614756253
Experience 12, Iter 13, disc loss: 0.0010358265374607089, policy loss: 8.498897308404484
Experience 12, Iter 14, disc loss: 0.0010006107908045497, policy loss: 9.003929454033154
Experience 12, Iter 15, disc loss: 0.0008942434791547969, policy loss: 9.22455220174298
Experience 12, Iter 16, disc loss: 0.0010723746317198128, policy loss: 8.417860327229402
Experience 12, Iter 17, disc loss: 0.0009853523801312712, policy loss: 8.6100720486831
Experience 12, Iter 18, disc loss: 0.0010749279833889062, policy loss: 8.646363203133944
Experience 12, Iter 19, disc loss: 0.0007987863968244968, policy loss: 8.876554705453024
Experience 12, Iter 20, disc loss: 0.0010142389462761685, policy loss: 8.611973068773423
Experience 12, Iter 21, disc loss: 0.0009461988949095591, policy loss: 9.107949859997117
Experience 12, Iter 22, disc loss: 0.0009473485216626553, policy loss: 8.608632524097011
Experience 12, Iter 23, disc loss: 0.0009985104703058843, policy loss: 8.337083918197727
Experience 12, Iter 24, disc loss: 0.0009403338631198418, policy loss: 8.519419113997852
Experience 12, Iter 25, disc loss: 0.0010573293409568065, policy loss: 8.768007457718639
Experience 12, Iter 26, disc loss: 0.000833099356372721, policy loss: 9.20674857701799
Experience 12, Iter 27, disc loss: 0.0008572740406769029, policy loss: 8.732616639263243
Experience 12, Iter 28, disc loss: 0.0008182436395866817, policy loss: 8.5762120050018
Experience 12, Iter 29, disc loss: 0.0008967860507141009, policy loss: 8.779009687546658
Experience 12, Iter 30, disc loss: 0.0008985275059773103, policy loss: 8.436034886983883
Experience 12, Iter 31, disc loss: 0.0007116231532063893, policy loss: 9.402354597845475
Experience 12, Iter 32, disc loss: 0.0007932871737635007, policy loss: 8.990808545328948
Experience 12, Iter 33, disc loss: 0.0009945895875271254, policy loss: 8.302932749861355
Experience 12, Iter 34, disc loss: 0.0008974162483990165, policy loss: 8.881059747895488
Experience 12, Iter 35, disc loss: 0.0009103256303260559, policy loss: 8.904351985548509
Experience 12, Iter 36, disc loss: 0.00096317214757263, policy loss: 8.708353307980719
Experience 12, Iter 37, disc loss: 0.0009454815149229406, policy loss: 8.8001091226577
Experience 12, Iter 38, disc loss: 0.0010163700891439279, policy loss: 8.747880919482938
Experience 12, Iter 39, disc loss: 0.0011395027089335953, policy loss: 8.687002131080202
Experience 12, Iter 40, disc loss: 0.0011978599466916538, policy loss: 8.208453452704433
Experience 12, Iter 41, disc loss: 0.0011337922725984805, policy loss: 8.600756570404261
Experience 12, Iter 42, disc loss: 0.0007967971464478828, policy loss: 9.440217812916403
Experience 12, Iter 43, disc loss: 0.0008711132776150143, policy loss: 8.983950006261283
Experience 12, Iter 44, disc loss: 0.0010595123877727506, policy loss: 8.08227959059224
Experience 12, Iter 45, disc loss: 0.0010050017360083774, policy loss: 9.007714239591166
Experience 12, Iter 46, disc loss: 0.0009734114184864816, policy loss: 8.805080618734902
Experience 12, Iter 47, disc loss: 0.0009023397220579376, policy loss: 9.059542237274888
Experience 12, Iter 48, disc loss: 0.0009849364700091601, policy loss: 8.951797941782308
Experience 12, Iter 49, disc loss: 0.0009486061894284777, policy loss: 8.86148734400897
Experience 12, Iter 50, disc loss: 0.0010343125756442416, policy loss: 8.807743289858541
Experience 12, Iter 51, disc loss: 0.0009999124445028962, policy loss: 8.931075342767954
Experience 12, Iter 52, disc loss: 0.0009017171901457188, policy loss: 8.888294018703952
Experience 12, Iter 53, disc loss: 0.0008341643846725517, policy loss: 9.180378053498274
Experience 12, Iter 54, disc loss: 0.0009454963236403308, policy loss: 8.774485291144542
Experience 12, Iter 55, disc loss: 0.0010173623207476383, policy loss: 9.11198098040163
Experience 12, Iter 56, disc loss: 0.0010900158631684015, policy loss: 8.703845327735966
Experience 12, Iter 57, disc loss: 0.0010249608947770282, policy loss: 8.695833048713636
Experience 12, Iter 58, disc loss: 0.0009379688814200765, policy loss: 8.611552125543342
Experience 12, Iter 59, disc loss: 0.0007674411862301749, policy loss: 9.282331750225284
Experience 12, Iter 60, disc loss: 0.0009488461139952807, policy loss: 8.521254091063454
Experience 12, Iter 61, disc loss: 0.0010268332082686476, policy loss: 8.59639051163207
Experience 12, Iter 62, disc loss: 0.0010578597490769571, policy loss: 8.98234299265092
Experience 12, Iter 63, disc loss: 0.0008592384783565389, policy loss: 9.124314208162872
Experience 12, Iter 64, disc loss: 0.001386951133440529, policy loss: 8.863229690983422
Experience 12, Iter 65, disc loss: 0.0011503748750418459, policy loss: 8.66805436122101
Experience 12, Iter 66, disc loss: 0.0010979373623897111, policy loss: 8.872412527051383
Experience 12, Iter 67, disc loss: 0.0007629702318775397, policy loss: 9.56634373483514
Experience 12, Iter 68, disc loss: 0.0009843728680787823, policy loss: 8.322334030888824
Experience 12, Iter 69, disc loss: 0.0011205413337567393, policy loss: 8.558271162967644
Experience 12, Iter 70, disc loss: 0.0009585101780557944, policy loss: 8.449463558576788
Experience 12, Iter 71, disc loss: 0.0010069372326047153, policy loss: 8.94178985943508
Experience 12, Iter 72, disc loss: 0.001129750265282785, policy loss: 8.73236575448383
Experience 12, Iter 73, disc loss: 0.0010297173636609923, policy loss: 9.233568977813107
Experience 12, Iter 74, disc loss: 0.000791357978136257, policy loss: 9.245426134591849
Experience 12, Iter 75, disc loss: 0.0008953325701364554, policy loss: 9.818685223286508
Experience 12, Iter 76, disc loss: 0.0009404278021208928, policy loss: 9.017398524146866
Experience 12, Iter 77, disc loss: 0.0008979490340708639, policy loss: 9.011471717256445
Experience 12, Iter 78, disc loss: 0.00101754273600207, policy loss: 8.496095091596292
Experience 12, Iter 79, disc loss: 0.0008413818851188287, policy loss: 9.344515302267032
Experience 12, Iter 80, disc loss: 0.0010916171441119585, policy loss: 8.940564911938477
Experience 12, Iter 81, disc loss: 0.0009281544475487137, policy loss: 9.172686800509595
Experience 12, Iter 82, disc loss: 0.0008597020765564738, policy loss: 8.551541780578873
Experience 12, Iter 83, disc loss: 0.0008568886263285938, policy loss: 9.02852638396952
Experience 12, Iter 84, disc loss: 0.0008947789621709531, policy loss: 9.211693325185015
Experience 12, Iter 85, disc loss: 0.0008356429918306773, policy loss: 9.314333256495466
Experience 12, Iter 86, disc loss: 0.0010028331708645906, policy loss: 8.796043785771035
Experience 12, Iter 87, disc loss: 0.0009208245072552119, policy loss: 9.009313751814345
Experience 12, Iter 88, disc loss: 0.000995949443415158, policy loss: 8.615839176068018
Experience 12, Iter 89, disc loss: 0.0009168810543455893, policy loss: 9.417395851483963
Experience 12, Iter 90, disc loss: 0.0009340279734236059, policy loss: 9.086144626172583
Experience 12, Iter 91, disc loss: 0.0008685525907221268, policy loss: 9.55995169354316
Experience 12, Iter 92, disc loss: 0.0008343134614094119, policy loss: 9.372982199500271
Experience 12, Iter 93, disc loss: 0.000655582188970448, policy loss: 9.392046141261808
Experience 12, Iter 94, disc loss: 0.0008231533161600644, policy loss: 9.119776421199772
Experience 12, Iter 95, disc loss: 0.0008157746234377806, policy loss: 9.6453366398244
Experience 12, Iter 96, disc loss: 0.0009577662808698923, policy loss: 9.419934712603862
Experience 12, Iter 97, disc loss: 0.0011866492174259012, policy loss: 9.27904488623021
Experience 12, Iter 98, disc loss: 0.0009077151339416106, policy loss: 8.698818685069135
Experience 12, Iter 99, disc loss: 0.0008224672536955311, policy loss: 9.230673048003979
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1790],
        [1.7037],
        [0.0040]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2041e-02, 9.7433e-02, 3.3076e-01, 6.9559e-03, 3.3196e-04,
          3.3550e+00]],

        [[1.2041e-02, 9.7433e-02, 3.3076e-01, 6.9559e-03, 3.3196e-04,
          3.3550e+00]],

        [[1.2041e-02, 9.7433e-02, 3.3076e-01, 6.9559e-03, 3.3196e-04,
          3.3550e+00]],

        [[1.2041e-02, 9.7433e-02, 3.3076e-01, 6.9559e-03, 3.3196e-04,
          3.3550e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0091, 0.7159, 6.8147, 0.0162], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0091, 0.7159, 6.8147, 0.0162])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.191
Iter 2/2000 - Loss: 2.407
Iter 3/2000 - Loss: 2.136
Iter 4/2000 - Loss: 2.103
Iter 5/2000 - Loss: 2.215
Iter 6/2000 - Loss: 2.181
Iter 7/2000 - Loss: 2.062
Iter 8/2000 - Loss: 2.022
Iter 9/2000 - Loss: 2.072
Iter 10/2000 - Loss: 2.084
Iter 11/2000 - Loss: 2.019
Iter 12/2000 - Loss: 1.955
Iter 13/2000 - Loss: 1.932
Iter 14/2000 - Loss: 1.914
Iter 15/2000 - Loss: 1.871
Iter 16/2000 - Loss: 1.805
Iter 17/2000 - Loss: 1.728
Iter 18/2000 - Loss: 1.641
Iter 19/2000 - Loss: 1.539
Iter 20/2000 - Loss: 1.424
Iter 1981/2000 - Loss: -8.145
Iter 1982/2000 - Loss: -8.145
Iter 1983/2000 - Loss: -8.145
Iter 1984/2000 - Loss: -8.145
Iter 1985/2000 - Loss: -8.145
Iter 1986/2000 - Loss: -8.145
Iter 1987/2000 - Loss: -8.145
Iter 1988/2000 - Loss: -8.145
Iter 1989/2000 - Loss: -8.146
Iter 1990/2000 - Loss: -8.146
Iter 1991/2000 - Loss: -8.146
Iter 1992/2000 - Loss: -8.146
Iter 1993/2000 - Loss: -8.146
Iter 1994/2000 - Loss: -8.146
Iter 1995/2000 - Loss: -8.146
Iter 1996/2000 - Loss: -8.146
Iter 1997/2000 - Loss: -8.146
Iter 1998/2000 - Loss: -8.146
Iter 1999/2000 - Loss: -8.146
Iter 2000/2000 - Loss: -8.146
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[16.3653,  7.5906, 38.5420, 11.8883,  7.3840, 60.0972]],

        [[18.3527, 32.9746, 17.1025,  6.5973,  1.2653, 24.8170]],

        [[19.6563, 38.4894, 11.2042,  0.9961,  4.6722, 23.4709]],

        [[17.1776, 33.2164, 11.9689,  3.6106,  1.2814, 44.1439]]])
Signal Variance: tensor([ 0.1142,  4.1153, 22.6017,  0.3370])
Estimated target variance: tensor([0.0091, 0.7159, 6.8147, 0.0162])
N: 130
Signal to noise ratio: tensor([ 20.6455,  97.7025, 105.2476,  34.9856])
Bound on condition number: tensor([  55411.5371, 1240952.6752, 1440017.6664,  159120.2628])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0017065668101499012, policy loss: 9.186764859416764
Experience 13, Iter 1, disc loss: 0.0011660688254117603, policy loss: 8.619704502228775
Experience 13, Iter 2, disc loss: 0.0012674911336449676, policy loss: 8.859652378557435
Experience 13, Iter 3, disc loss: 0.0011664596650692333, policy loss: 9.21224080097015
Experience 13, Iter 4, disc loss: 0.0008989214941004761, policy loss: 9.18436066060512
Experience 13, Iter 5, disc loss: 0.0012092679356299877, policy loss: 8.877074249420641
Experience 13, Iter 6, disc loss: 0.0015055509075572772, policy loss: 9.461539214934865
Experience 13, Iter 7, disc loss: 0.0016228667010502395, policy loss: 9.882200623684255
Experience 13, Iter 8, disc loss: 0.006560621316665782, policy loss: 8.547211990316224
Experience 13, Iter 9, disc loss: 0.001083748330393105, policy loss: 9.079671100413943
Experience 13, Iter 10, disc loss: 0.0050681789797212135, policy loss: 9.409743931718364
Experience 13, Iter 11, disc loss: 0.0067554510370100036, policy loss: 8.610738832751222
Experience 13, Iter 12, disc loss: 0.0029518776543489304, policy loss: 10.458324200003013
Experience 13, Iter 13, disc loss: 0.004725015727423906, policy loss: 9.652521149759806
Experience 13, Iter 14, disc loss: 0.004709319665808875, policy loss: 8.923836020302398
Experience 13, Iter 15, disc loss: 0.002313609629986835, policy loss: 9.646243880577977
Experience 13, Iter 16, disc loss: 0.002953427679104956, policy loss: 9.438008811528057
Experience 13, Iter 17, disc loss: 0.0025858616309467463, policy loss: 8.880336946899632
Experience 13, Iter 18, disc loss: 0.003583953642543493, policy loss: 9.890501758072274
Experience 13, Iter 19, disc loss: 0.00530947352933485, policy loss: 9.186764468775188
Experience 13, Iter 20, disc loss: 0.004145541851498657, policy loss: 9.537672324632869
Experience 13, Iter 21, disc loss: 0.005288694816142586, policy loss: 8.969501460956666
Experience 13, Iter 22, disc loss: 0.0036053898885718017, policy loss: 9.702523493403852
Experience 13, Iter 23, disc loss: 0.0036753393853772623, policy loss: 9.665886526017813
Experience 13, Iter 24, disc loss: 0.004192046331417709, policy loss: 9.79755446978396
Experience 13, Iter 25, disc loss: 0.005849995592608186, policy loss: 9.048717837598932
Experience 13, Iter 26, disc loss: 0.0046779728855813815, policy loss: 9.602765881723947
Experience 13, Iter 27, disc loss: 0.004770389497432944, policy loss: 9.569109938662208
Experience 13, Iter 28, disc loss: 0.004033605355619903, policy loss: 9.720492027013627
Experience 13, Iter 29, disc loss: 0.004706221577790939, policy loss: 10.16600444209205
Experience 13, Iter 30, disc loss: 0.004206990798687495, policy loss: 9.919812129483104
Experience 13, Iter 31, disc loss: 0.004435984129654631, policy loss: 10.092706407919046
Experience 13, Iter 32, disc loss: 0.004314797945984695, policy loss: 9.717123874685882
Experience 13, Iter 33, disc loss: 0.004564520688497023, policy loss: 9.782471426295988
Experience 13, Iter 34, disc loss: 0.0041913016557259914, policy loss: 9.67671510190968
Experience 13, Iter 35, disc loss: 0.004523313835156, policy loss: 8.983695054411347
Experience 13, Iter 36, disc loss: 0.005304798058764149, policy loss: 9.068735740868396
Experience 13, Iter 37, disc loss: 0.0038206267168663416, policy loss: 9.88613021145427
Experience 13, Iter 38, disc loss: 0.004149178335396136, policy loss: 9.57950944008104
Experience 13, Iter 39, disc loss: 0.0036037726344460997, policy loss: 10.144513180775807
Experience 13, Iter 40, disc loss: 0.003995262724362594, policy loss: 8.722789371050325
Experience 13, Iter 41, disc loss: 0.003341203369383735, policy loss: 9.805784410682653
Experience 13, Iter 42, disc loss: 0.0029787468881664085, policy loss: 10.04343898761073
Experience 13, Iter 43, disc loss: 0.0035404719785327972, policy loss: 11.385708335942935
Experience 13, Iter 44, disc loss: 0.003144237331593173, policy loss: 10.330148323053713
Experience 13, Iter 45, disc loss: 0.0031869261482725115, policy loss: 9.921505063032514
Experience 13, Iter 46, disc loss: 0.0025067464947719324, policy loss: 10.996793351962985
Experience 13, Iter 47, disc loss: 0.00308838175626388, policy loss: 9.77204765150977
Experience 13, Iter 48, disc loss: 0.003008042328446055, policy loss: 11.778206153878875
Experience 13, Iter 49, disc loss: 0.003082982810593327, policy loss: 9.377391397242379
Experience 13, Iter 50, disc loss: 0.0025908973341894834, policy loss: 10.032252844623564
Experience 13, Iter 51, disc loss: 0.0031910236246582976, policy loss: 10.56414219186595
Experience 13, Iter 52, disc loss: 0.0025299729127070637, policy loss: 10.548907165644591
Experience 13, Iter 53, disc loss: 0.0019875760157760888, policy loss: 10.423946903024234
Experience 13, Iter 54, disc loss: 0.0034655308052502828, policy loss: 9.656210686254203
Experience 13, Iter 55, disc loss: 0.0023220466846133126, policy loss: 10.442220200034294
Experience 13, Iter 56, disc loss: 0.0034813225601600054, policy loss: 8.9940572934741
Experience 13, Iter 57, disc loss: 0.0025770704209726876, policy loss: 9.800930610890955
Experience 13, Iter 58, disc loss: 0.0026111079083077247, policy loss: 9.328436898531358
Experience 13, Iter 59, disc loss: 0.0020727705297652597, policy loss: 10.150308665528033
Experience 13, Iter 60, disc loss: 0.0023819029239258065, policy loss: 9.278459650855115
Experience 13, Iter 61, disc loss: 0.002670354888486849, policy loss: 9.297879055344168
Experience 13, Iter 62, disc loss: 0.002592007116442453, policy loss: 8.755754187831425
Experience 13, Iter 63, disc loss: 0.002825057019187134, policy loss: 8.785796590656476
Experience 13, Iter 64, disc loss: 0.0021634703092444237, policy loss: 10.23847731286344
Experience 13, Iter 65, disc loss: 0.0023092582107042074, policy loss: 9.219837484001967
Experience 13, Iter 66, disc loss: 0.002503141209270355, policy loss: 10.14841561381835
Experience 13, Iter 67, disc loss: 0.002485766809974541, policy loss: 9.06887412105646
Experience 13, Iter 68, disc loss: 0.0023428945504185084, policy loss: 9.902568680201348
Experience 13, Iter 69, disc loss: 0.0021556551480324676, policy loss: 10.07239848024198
Experience 13, Iter 70, disc loss: 0.0027804243039922945, policy loss: 9.221838695748966
Experience 13, Iter 71, disc loss: 0.0022178234311771, policy loss: 10.004773491747056
Experience 13, Iter 72, disc loss: 0.002417709362203607, policy loss: 9.261646993963085
Experience 13, Iter 73, disc loss: 0.0024622462049867103, policy loss: 9.203558724676611
Experience 13, Iter 74, disc loss: 0.002075858625937263, policy loss: 9.922750424124594
Experience 13, Iter 75, disc loss: 0.0023114688808749204, policy loss: 9.703306103115196
Experience 13, Iter 76, disc loss: 0.002618813216038582, policy loss: 9.084420229402749
Experience 13, Iter 77, disc loss: 0.00227663800197098, policy loss: 9.318807909198629
Experience 13, Iter 78, disc loss: 0.002236247782723825, policy loss: 9.439643046025898
Experience 13, Iter 79, disc loss: 0.002740268738535723, policy loss: 9.884555498459232
Experience 13, Iter 80, disc loss: 0.0021873035185756774, policy loss: 9.028546265330078
Experience 13, Iter 81, disc loss: 0.0022222139661158425, policy loss: 9.112850145182648
Experience 13, Iter 82, disc loss: 0.0023307205935695725, policy loss: 9.27496728898956
Experience 13, Iter 83, disc loss: 0.0018390507998680208, policy loss: 10.166687603581835
Experience 13, Iter 84, disc loss: 0.0018217632509484154, policy loss: 9.839037402078038
Experience 13, Iter 85, disc loss: 0.0018616193613001312, policy loss: 10.078852052353263
Experience 13, Iter 86, disc loss: 0.0021191638944001367, policy loss: 9.429170791580862
Experience 13, Iter 87, disc loss: 0.001927412026655085, policy loss: 9.331882391996956
Experience 13, Iter 88, disc loss: 0.001965261534645337, policy loss: 9.572322952082057
Experience 13, Iter 89, disc loss: 0.0017335966521922176, policy loss: 9.895795255588691
Experience 13, Iter 90, disc loss: 0.0018868020287546364, policy loss: 9.947692989845677
Experience 13, Iter 91, disc loss: 0.0018527464968147236, policy loss: 9.813080220722286
Experience 13, Iter 92, disc loss: 0.001908202533085259, policy loss: 8.978091698450283
Experience 13, Iter 93, disc loss: 0.0019207008873125561, policy loss: 9.330065659986264
Experience 13, Iter 94, disc loss: 0.0019304759033204428, policy loss: 9.52948397656116
Experience 13, Iter 95, disc loss: 0.0016053873585287382, policy loss: 10.449677068439382
Experience 13, Iter 96, disc loss: 0.0014894053250135873, policy loss: 10.441176696419037
Experience 13, Iter 97, disc loss: 0.0018285363406523704, policy loss: 9.253259452721679
Experience 13, Iter 98, disc loss: 0.0017420351095589952, policy loss: 9.098521765793166
Experience 13, Iter 99, disc loss: 0.0016992503447806379, policy loss: 9.423476529963189
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1820],
        [1.7403],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1816e-02, 1.0657e-01, 3.6678e-01, 8.7355e-03, 1.0859e-03,
          3.5114e+00]],

        [[1.1816e-02, 1.0657e-01, 3.6678e-01, 8.7355e-03, 1.0859e-03,
          3.5114e+00]],

        [[1.1816e-02, 1.0657e-01, 3.6678e-01, 8.7355e-03, 1.0859e-03,
          3.5114e+00]],

        [[1.1816e-02, 1.0657e-01, 3.6678e-01, 8.7355e-03, 1.0859e-03,
          3.5114e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0101, 0.7278, 6.9611, 0.0194], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0101, 0.7278, 6.9611, 0.0194])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.317
Iter 2/2000 - Loss: 2.530
Iter 3/2000 - Loss: 2.264
Iter 4/2000 - Loss: 2.251
Iter 5/2000 - Loss: 2.352
Iter 6/2000 - Loss: 2.288
Iter 7/2000 - Loss: 2.185
Iter 8/2000 - Loss: 2.178
Iter 9/2000 - Loss: 2.218
Iter 10/2000 - Loss: 2.190
Iter 11/2000 - Loss: 2.106
Iter 12/2000 - Loss: 2.045
Iter 13/2000 - Loss: 2.019
Iter 14/2000 - Loss: 1.976
Iter 15/2000 - Loss: 1.889
Iter 16/2000 - Loss: 1.775
Iter 17/2000 - Loss: 1.656
Iter 18/2000 - Loss: 1.530
Iter 19/2000 - Loss: 1.382
Iter 20/2000 - Loss: 1.206
Iter 1981/2000 - Loss: -8.087
Iter 1982/2000 - Loss: -8.087
Iter 1983/2000 - Loss: -8.087
Iter 1984/2000 - Loss: -8.088
Iter 1985/2000 - Loss: -8.088
Iter 1986/2000 - Loss: -8.088
Iter 1987/2000 - Loss: -8.088
Iter 1988/2000 - Loss: -8.088
Iter 1989/2000 - Loss: -8.088
Iter 1990/2000 - Loss: -8.088
Iter 1991/2000 - Loss: -8.088
Iter 1992/2000 - Loss: -8.088
Iter 1993/2000 - Loss: -8.088
Iter 1994/2000 - Loss: -8.088
Iter 1995/2000 - Loss: -8.088
Iter 1996/2000 - Loss: -8.088
Iter 1997/2000 - Loss: -8.088
Iter 1998/2000 - Loss: -8.088
Iter 1999/2000 - Loss: -8.088
Iter 2000/2000 - Loss: -8.088
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[13.1840,  7.4838, 29.9095, 10.7334,  8.9924, 55.2490]],

        [[18.3467, 34.9071, 11.0064,  1.4608,  6.2312, 23.3345]],

        [[17.3195, 34.8118, 11.5778,  1.1054,  2.4758, 20.0757]],

        [[15.8415, 31.4590, 11.1987,  3.3275,  1.5737, 35.9154]]])
Signal Variance: tensor([ 0.1096,  2.1456, 20.2876,  0.2417])
Estimated target variance: tensor([0.0101, 0.7278, 6.9611, 0.0194])
N: 140
Signal to noise ratio: tensor([20.1235, 75.0769, 98.7250, 29.2279])
Bound on condition number: tensor([  56694.5507,  789117.1668, 1364528.6112,  119599.2256])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0014986427979497142, policy loss: 10.599330890765074
Experience 14, Iter 1, disc loss: 0.0016455528941928164, policy loss: 9.2709768753044
Experience 14, Iter 2, disc loss: 0.0015340442507634053, policy loss: 9.637418501056317
Experience 14, Iter 3, disc loss: 0.0015715802659910009, policy loss: 9.232673769344116
Experience 14, Iter 4, disc loss: 0.0012985147014819502, policy loss: 10.707417178396845
Experience 14, Iter 5, disc loss: 0.0012716530440723983, policy loss: 10.62732279166757
Experience 14, Iter 6, disc loss: 0.001237201370477652, policy loss: 10.512583102765364
Experience 14, Iter 7, disc loss: 0.0012494144199656076, policy loss: 10.482081765408424
Experience 14, Iter 8, disc loss: 0.0012577919559501284, policy loss: 9.904748623509878
Experience 14, Iter 9, disc loss: 0.0015323366187917661, policy loss: 8.995619950753493
Experience 14, Iter 10, disc loss: 0.0013285794036924834, policy loss: 10.238324438550396
Experience 14, Iter 11, disc loss: 0.001270460824070133, policy loss: 10.405233521879504
Experience 14, Iter 12, disc loss: 0.0014965344572816738, policy loss: 9.063780915077334
Experience 14, Iter 13, disc loss: 0.001357931880142082, policy loss: 9.774589555268918
Experience 14, Iter 14, disc loss: 0.0013694610051058305, policy loss: 10.14271357384998
Experience 14, Iter 15, disc loss: 0.0014411579731950406, policy loss: 9.31674344214304
Experience 14, Iter 16, disc loss: 0.001363123158786581, policy loss: 9.671472067420522
Experience 14, Iter 17, disc loss: 0.0014208739075078345, policy loss: 9.478926932580196
Experience 14, Iter 18, disc loss: 0.001497135620615703, policy loss: 9.06311790618015
Experience 14, Iter 19, disc loss: 0.0014154112368340457, policy loss: 9.545388003004103
Experience 14, Iter 20, disc loss: 0.0015584876768908857, policy loss: 9.27282229130001
Experience 14, Iter 21, disc loss: 0.0018994554152693402, policy loss: 9.193148219938081
Experience 14, Iter 22, disc loss: 0.0015029420679027467, policy loss: 9.051904527036031
Experience 14, Iter 23, disc loss: 0.0010918602852724904, policy loss: 11.46639639250803
Experience 14, Iter 24, disc loss: 0.0013122701068832212, policy loss: 9.252804963846305
Experience 14, Iter 25, disc loss: 0.001397692688585936, policy loss: 9.049487616995501
Experience 14, Iter 26, disc loss: 0.0012860657992306164, policy loss: 9.514026687627346
Experience 14, Iter 27, disc loss: 0.0012357434948024907, policy loss: 9.787668102752619
Experience 14, Iter 28, disc loss: 0.0011994592218749949, policy loss: 10.074083849723792
Experience 14, Iter 29, disc loss: 0.001095517597108484, policy loss: 9.32916657991991
Experience 14, Iter 30, disc loss: 0.001055730065861629, policy loss: 10.209706987739521
Experience 14, Iter 31, disc loss: 0.0010862396505163664, policy loss: 9.918405775302238
Experience 14, Iter 32, disc loss: 0.0010345395749159429, policy loss: 9.909259260706238
Experience 14, Iter 33, disc loss: 0.001056720581848651, policy loss: 10.447194209249389
Experience 14, Iter 34, disc loss: 0.0011415618634062286, policy loss: 9.887730669902611
Experience 14, Iter 35, disc loss: 0.0012074335461139908, policy loss: 9.499639405250146
Experience 14, Iter 36, disc loss: 0.0009529948196246553, policy loss: 10.836297806866217
Experience 14, Iter 37, disc loss: 0.0009561664010068477, policy loss: 10.05042197460413
Experience 14, Iter 38, disc loss: 0.0010524473922487445, policy loss: 9.989750446883868
Experience 14, Iter 39, disc loss: 0.0009436975757205966, policy loss: 10.927560798398947
Experience 14, Iter 40, disc loss: 0.0010480993373058922, policy loss: 10.66833332824053
Experience 14, Iter 41, disc loss: 0.0008930884328552807, policy loss: 10.113994798503718
Experience 14, Iter 42, disc loss: 0.0010572380295180424, policy loss: 10.177273655451874
Experience 14, Iter 43, disc loss: 0.0011975599021993547, policy loss: 9.017204026120954
Experience 14, Iter 44, disc loss: 0.0009810704729334469, policy loss: 11.03103131244345
Experience 14, Iter 45, disc loss: 0.0010569325025044497, policy loss: 10.121882403895988
Experience 14, Iter 46, disc loss: 0.0010554005929556943, policy loss: 9.315784453955288
Experience 14, Iter 47, disc loss: 0.0011295134085305106, policy loss: 9.630665337716705
Experience 14, Iter 48, disc loss: 0.0011914293550276042, policy loss: 10.906797542408128
Experience 14, Iter 49, disc loss: 0.001062998662466699, policy loss: 9.736756837154593
Experience 14, Iter 50, disc loss: 0.0011930455656037867, policy loss: 9.611846441927646
Experience 14, Iter 51, disc loss: 0.0009914170006316827, policy loss: 11.677108091022635
Experience 14, Iter 52, disc loss: 0.00119428056864576, policy loss: 10.772042040732785
Experience 14, Iter 53, disc loss: 0.0010183208994524644, policy loss: 10.293577152001784
Experience 14, Iter 54, disc loss: 0.0011479489107778364, policy loss: 11.095127358230732
Experience 14, Iter 55, disc loss: 0.0009561054007769593, policy loss: 11.541171826955761
Experience 14, Iter 56, disc loss: 0.0009542691987103305, policy loss: 11.823862606032996
Experience 14, Iter 57, disc loss: 0.0009306707666927132, policy loss: 12.864231132202505
Experience 14, Iter 58, disc loss: 0.0009053868191999445, policy loss: 11.823413851380419
Experience 14, Iter 59, disc loss: 0.0011508615725079603, policy loss: 11.286218868967925
Experience 14, Iter 60, disc loss: 0.0008329133537268991, policy loss: 11.266365045419194
Experience 14, Iter 61, disc loss: 0.0008868392861020101, policy loss: 10.5613093425254
Experience 14, Iter 62, disc loss: 0.0008401822617255831, policy loss: 10.328073337869093
Experience 14, Iter 63, disc loss: 0.000866578341295895, policy loss: 10.163265661999848
Experience 14, Iter 64, disc loss: 0.0008048689840430192, policy loss: 10.710686987027874
Experience 14, Iter 65, disc loss: 0.0007653388935315975, policy loss: 10.81868103445704
Experience 14, Iter 66, disc loss: 0.0007696034096723457, policy loss: 10.559592180804636
Experience 14, Iter 67, disc loss: 0.0007507881769109606, policy loss: 10.71957637815682
Experience 14, Iter 68, disc loss: 0.0006421936444969375, policy loss: 11.023732479284579
Experience 14, Iter 69, disc loss: 0.000612450232332703, policy loss: 10.851879431949358
Experience 14, Iter 70, disc loss: 0.0006273351819258643, policy loss: 10.007471869015738
Experience 14, Iter 71, disc loss: 0.0006035534571099217, policy loss: 10.199051015311323
Experience 14, Iter 72, disc loss: 0.0006487868792427712, policy loss: 10.892192268141729
Experience 14, Iter 73, disc loss: 0.0006133682698769991, policy loss: 9.725986632554111
Experience 14, Iter 74, disc loss: 0.0005863599904799725, policy loss: 10.114066690847336
Experience 14, Iter 75, disc loss: 0.000561016911092166, policy loss: 10.72688030523623
Experience 14, Iter 76, disc loss: 0.0005559850890433588, policy loss: 10.532228472493717
Experience 14, Iter 77, disc loss: 0.0005338118369554083, policy loss: 11.155224492183981
Experience 14, Iter 78, disc loss: 0.0007669339380810014, policy loss: 10.066651695929547
Experience 14, Iter 79, disc loss: 0.0006203758185465111, policy loss: 9.214803603739702
Experience 14, Iter 80, disc loss: 0.0005947905532701879, policy loss: 10.40948384140584
Experience 14, Iter 81, disc loss: 0.0006055448435227485, policy loss: 9.830352852934169
Experience 14, Iter 82, disc loss: 0.0008693227330069383, policy loss: 10.145257051850713
Experience 14, Iter 83, disc loss: 0.0005879328922737061, policy loss: 9.926578923359799
Experience 14, Iter 84, disc loss: 0.000678331040550424, policy loss: 9.75706290332433
Experience 14, Iter 85, disc loss: 0.0006445020189932022, policy loss: 9.856072242544828
Experience 14, Iter 86, disc loss: 0.0006396800956932871, policy loss: 9.87055466013241
Experience 14, Iter 87, disc loss: 0.0006421932097251189, policy loss: 9.838472472290954
Experience 14, Iter 88, disc loss: 0.0006810662454297906, policy loss: 10.295371142489747
Experience 14, Iter 89, disc loss: 0.0007466786796059512, policy loss: 9.353902449275848
Experience 14, Iter 90, disc loss: 0.0007292936099168234, policy loss: 10.064987487465098
Experience 14, Iter 91, disc loss: 0.0006592525923062558, policy loss: 9.832950184500632
Experience 14, Iter 92, disc loss: 0.0007003578204707312, policy loss: 9.601370967565384
Experience 14, Iter 93, disc loss: 0.0005985956871513518, policy loss: 10.237938976113455
Experience 14, Iter 94, disc loss: 0.0006604160453368939, policy loss: 10.070749416276753
Experience 14, Iter 95, disc loss: 0.000652951531563152, policy loss: 9.497074819025267
Experience 14, Iter 96, disc loss: 0.0005999370936516068, policy loss: 9.94117491692467
Experience 14, Iter 97, disc loss: 0.0007603909252869549, policy loss: 9.535282046687099
Experience 14, Iter 98, disc loss: 0.0006891138992507922, policy loss: 9.929378368819
Experience 14, Iter 99, disc loss: 0.0006473085994914047, policy loss: 10.471066058938197
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1871],
        [1.8077],
        [0.0058]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1416e-02, 1.1193e-01, 4.1018e-01, 1.0378e-02, 1.9133e-03,
          3.6837e+00]],

        [[1.1416e-02, 1.1193e-01, 4.1018e-01, 1.0378e-02, 1.9133e-03,
          3.6837e+00]],

        [[1.1416e-02, 1.1193e-01, 4.1018e-01, 1.0378e-02, 1.9133e-03,
          3.6837e+00]],

        [[1.1416e-02, 1.1193e-01, 4.1018e-01, 1.0378e-02, 1.9133e-03,
          3.6837e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0104, 0.7485, 7.2309, 0.0233], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0104, 0.7485, 7.2309, 0.0233])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.438
Iter 2/2000 - Loss: 2.648
Iter 3/2000 - Loss: 2.385
Iter 4/2000 - Loss: 2.381
Iter 5/2000 - Loss: 2.472
Iter 6/2000 - Loss: 2.398
Iter 7/2000 - Loss: 2.303
Iter 8/2000 - Loss: 2.300
Iter 9/2000 - Loss: 2.326
Iter 10/2000 - Loss: 2.283
Iter 11/2000 - Loss: 2.192
Iter 12/2000 - Loss: 2.121
Iter 13/2000 - Loss: 2.079
Iter 14/2000 - Loss: 2.018
Iter 15/2000 - Loss: 1.907
Iter 16/2000 - Loss: 1.764
Iter 17/2000 - Loss: 1.616
Iter 18/2000 - Loss: 1.463
Iter 19/2000 - Loss: 1.290
Iter 20/2000 - Loss: 1.084
Iter 1981/2000 - Loss: -8.078
Iter 1982/2000 - Loss: -8.078
Iter 1983/2000 - Loss: -8.078
Iter 1984/2000 - Loss: -8.078
Iter 1985/2000 - Loss: -8.078
Iter 1986/2000 - Loss: -8.078
Iter 1987/2000 - Loss: -8.078
Iter 1988/2000 - Loss: -8.078
Iter 1989/2000 - Loss: -8.078
Iter 1990/2000 - Loss: -8.078
Iter 1991/2000 - Loss: -8.078
Iter 1992/2000 - Loss: -8.078
Iter 1993/2000 - Loss: -8.078
Iter 1994/2000 - Loss: -8.078
Iter 1995/2000 - Loss: -8.079
Iter 1996/2000 - Loss: -8.079
Iter 1997/2000 - Loss: -8.079
Iter 1998/2000 - Loss: -8.079
Iter 1999/2000 - Loss: -8.079
Iter 2000/2000 - Loss: -8.079
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[10.5383,  3.3044, 27.8728, 10.9466, 12.4451, 53.0007]],

        [[16.2236, 32.9807, 10.4683,  1.6450,  2.7198, 23.3906]],

        [[16.3671, 32.6684, 12.3011,  0.9921,  1.4275, 21.2331]],

        [[14.8074, 30.7876, 11.5994,  3.0589,  1.5239, 38.0186]]])
Signal Variance: tensor([ 0.0622,  1.9570, 18.3766,  0.2284])
Estimated target variance: tensor([0.0104, 0.7485, 7.2309, 0.0233])
N: 150
Signal to noise ratio: tensor([15.6233, 68.9761, 96.2240, 28.4393])
Bound on condition number: tensor([  36614.3434,  713655.9135, 1388859.6964,  121320.4342])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.001367989959913256, policy loss: 9.854562799366292
Experience 15, Iter 1, disc loss: 0.0007520826922845311, policy loss: 9.082367531838607
Experience 15, Iter 2, disc loss: 0.0005974845937150087, policy loss: 9.784330398524899
Experience 15, Iter 3, disc loss: 0.0005903289762627871, policy loss: 10.282373181451103
Experience 15, Iter 4, disc loss: 0.0006505980850071633, policy loss: 9.699427246374151
Experience 15, Iter 5, disc loss: 0.0006376565239420656, policy loss: 10.200373985746234
Experience 15, Iter 6, disc loss: 0.0006279074856108822, policy loss: 9.731484156684
Experience 15, Iter 7, disc loss: 0.0006717794014749251, policy loss: 9.560070128545908
Experience 15, Iter 8, disc loss: 0.0005930992676092095, policy loss: 10.212869191531022
Experience 15, Iter 9, disc loss: 0.0005874854771379734, policy loss: 10.229887131257804
Experience 15, Iter 10, disc loss: 0.0005430854019616156, policy loss: 10.6194038595225
Experience 15, Iter 11, disc loss: 0.0006835363951103532, policy loss: 9.277157552279132
Experience 15, Iter 12, disc loss: 0.0005900509450572598, policy loss: 9.810414936633421
Experience 15, Iter 13, disc loss: 0.0005967908107002029, policy loss: 9.954087421223582
Experience 15, Iter 14, disc loss: 0.0005483533749758866, policy loss: 10.46411161510499
Experience 15, Iter 15, disc loss: 0.0006302804015410686, policy loss: 9.40965242854239
Experience 15, Iter 16, disc loss: 0.0005686788164610315, policy loss: 10.041592319289073
Experience 15, Iter 17, disc loss: 0.0005835646226790938, policy loss: 9.936971491500406
Experience 15, Iter 18, disc loss: 0.0006463371644933137, policy loss: 9.515349067565936
Experience 15, Iter 19, disc loss: 0.0006027202440420222, policy loss: 9.851967656500372
Experience 15, Iter 20, disc loss: 0.000559046803791232, policy loss: 9.898371055182654
Experience 15, Iter 21, disc loss: 0.0005529117396905564, policy loss: 10.083920221352614
Experience 15, Iter 22, disc loss: 0.0006141327767998602, policy loss: 9.400985588396917
Experience 15, Iter 23, disc loss: 0.0006366892411755344, policy loss: 9.640178115072466
Experience 15, Iter 24, disc loss: 0.0006577299466221599, policy loss: 9.882715051470171
Experience 15, Iter 25, disc loss: 0.0006317008824118434, policy loss: 10.507196920671078
Experience 15, Iter 26, disc loss: 0.0006469175391676618, policy loss: 9.79336241186886
Experience 15, Iter 27, disc loss: 0.0006611815379367067, policy loss: 9.91740455483304
Experience 15, Iter 28, disc loss: 0.0007315532563055495, policy loss: 8.994307542989706
Experience 15, Iter 29, disc loss: 0.0006608794658899611, policy loss: 9.916293885986782
Experience 15, Iter 30, disc loss: 0.0007826910001616297, policy loss: 9.622326148906104
Experience 15, Iter 31, disc loss: 0.0006481033006776797, policy loss: 9.916617108719667
Experience 15, Iter 32, disc loss: 0.0007889738020829883, policy loss: 8.922936046978183
Experience 15, Iter 33, disc loss: 0.00069175927030953, policy loss: 9.790796084399645
Experience 15, Iter 34, disc loss: 0.0011785254456197394, policy loss: 9.827155640778326
Experience 15, Iter 35, disc loss: 0.0007586508836147604, policy loss: 9.984663729201326
Experience 15, Iter 36, disc loss: 0.0006844589328082074, policy loss: 9.626145984479875
Experience 15, Iter 37, disc loss: 0.0016905048832589805, policy loss: 9.50666422559374
Experience 15, Iter 38, disc loss: 0.0007455831657792976, policy loss: 9.772829751560836
Experience 15, Iter 39, disc loss: 0.0007381774340376004, policy loss: 9.898595888159205
Experience 15, Iter 40, disc loss: 0.0006325216145510772, policy loss: 10.336185862840015
Experience 15, Iter 41, disc loss: 0.0006210407855100984, policy loss: 10.632794740571097
Experience 15, Iter 42, disc loss: 0.0005566471813797129, policy loss: 11.279527476304867
Experience 15, Iter 43, disc loss: 0.0006705088066023977, policy loss: 11.445137260023973
Experience 15, Iter 44, disc loss: 0.0008174909706262542, policy loss: 12.580240512210842
Experience 15, Iter 45, disc loss: 0.0011131921322091894, policy loss: 12.248284170658012
Experience 15, Iter 46, disc loss: 0.0007835513258764204, policy loss: 12.996346901275551
Experience 15, Iter 47, disc loss: 0.0008824989041445487, policy loss: 12.761491418347532
Experience 15, Iter 48, disc loss: 0.0012152393278988198, policy loss: 12.961388210195224
Experience 15, Iter 49, disc loss: 0.0011110117483687626, policy loss: 12.110090106534752
Experience 15, Iter 50, disc loss: 0.0011109759311112347, policy loss: 12.326701343020734
Experience 15, Iter 51, disc loss: 0.0017607170917883323, policy loss: 13.009547257507547
Experience 15, Iter 52, disc loss: 0.002360897054967285, policy loss: 13.899817404197686
Experience 15, Iter 53, disc loss: 0.0013007038239035542, policy loss: 13.177291499597644
Experience 15, Iter 54, disc loss: 0.0017557304433085475, policy loss: 12.940871558429926
Experience 15, Iter 55, disc loss: 0.004659114500009427, policy loss: 12.19348455510247
Experience 15, Iter 56, disc loss: 0.0015571345057572503, policy loss: 13.042610201519242
Experience 15, Iter 57, disc loss: 0.002867925456288262, policy loss: 11.043370694008745
Experience 15, Iter 58, disc loss: 0.0029708763798842316, policy loss: 12.418209823004581
Experience 15, Iter 59, disc loss: 0.0031794257960567575, policy loss: 13.318230597559424
Experience 15, Iter 60, disc loss: 0.0033004168866039765, policy loss: 13.086206231705113
Experience 15, Iter 61, disc loss: 0.00483152531740543, policy loss: 12.147948254433343
Experience 15, Iter 62, disc loss: 0.0030599906842357573, policy loss: 11.665840731506968
Experience 15, Iter 63, disc loss: 0.0020846310918150182, policy loss: 12.17646500733452
Experience 15, Iter 64, disc loss: 0.003160655292335642, policy loss: 12.109748230228524
Experience 15, Iter 65, disc loss: 0.004318814963663422, policy loss: 12.563660001308175
Experience 15, Iter 66, disc loss: 0.0030571411786799656, policy loss: 13.168762957406688
Experience 15, Iter 67, disc loss: 0.004080926818662193, policy loss: 12.999046346763519
Experience 15, Iter 68, disc loss: 0.0033470632147561876, policy loss: 13.68191710588427
Experience 15, Iter 69, disc loss: 0.004180405588363009, policy loss: 12.034866480043123
Experience 15, Iter 70, disc loss: 0.0027831531280148626, policy loss: 12.947360292164774
Experience 15, Iter 71, disc loss: 0.004018405995883701, policy loss: 14.130352856400393
Experience 15, Iter 72, disc loss: 0.002714640438297137, policy loss: 14.946508088501552
Experience 15, Iter 73, disc loss: 0.002153728527268954, policy loss: 15.42315145900703
Experience 15, Iter 74, disc loss: 0.00293517725339415, policy loss: 16.624666372375426
Experience 15, Iter 75, disc loss: 0.002018846347378583, policy loss: 19.430043971897476
Experience 15, Iter 76, disc loss: 0.0020888246824931757, policy loss: 17.265867524961337
Experience 15, Iter 77, disc loss: 0.0018018975797727992, policy loss: 20.32218036693981
Experience 15, Iter 78, disc loss: 0.0015052414569429887, policy loss: 21.004041887233836
Experience 15, Iter 79, disc loss: 0.002117304196185759, policy loss: 20.96198717269832
Experience 15, Iter 80, disc loss: 0.0014667485054465196, policy loss: 21.0415201419638
Experience 15, Iter 81, disc loss: 0.0011487478956678362, policy loss: 19.087650673649026
Experience 15, Iter 82, disc loss: 0.0009882406906865692, policy loss: 21.392969971174068
Experience 15, Iter 83, disc loss: 0.0007984343447570027, policy loss: 19.40861619303923
Experience 15, Iter 84, disc loss: 0.0006495454247798799, policy loss: 19.98643620704214
Experience 15, Iter 85, disc loss: 0.0013256975340609214, policy loss: 19.064949833813266
Experience 15, Iter 86, disc loss: 0.0011726284594962208, policy loss: 17.80017396552431
Experience 15, Iter 87, disc loss: 0.0006744056619014144, policy loss: 18.159758894742488
Experience 15, Iter 88, disc loss: 0.0004839015467501194, policy loss: 17.889509936868098
Experience 15, Iter 89, disc loss: 0.0005558157540910809, policy loss: 17.587223119779733
Experience 15, Iter 90, disc loss: 0.00034891889344201935, policy loss: 22.27212374658138
Experience 15, Iter 91, disc loss: 0.00042316555039537834, policy loss: 18.022921877790466
Experience 15, Iter 92, disc loss: 0.0005430580133717557, policy loss: 21.254460168324982
Experience 15, Iter 93, disc loss: 0.0003879117793810229, policy loss: 23.292309687744567
Experience 15, Iter 94, disc loss: 0.0003503957464293682, policy loss: 22.701156715748382
Experience 15, Iter 95, disc loss: 0.0003078518851097535, policy loss: 22.04943210984881
Experience 15, Iter 96, disc loss: 0.0004037808847796319, policy loss: 22.705040206924725
Experience 15, Iter 97, disc loss: 0.00046170881833180707, policy loss: 25.083987055021026
Experience 15, Iter 98, disc loss: 0.0004215936380858983, policy loss: 24.248612791247915
Experience 15, Iter 99, disc loss: 0.0006135680137324311, policy loss: 22.102319696704306
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.1808],
        [1.7501],
        [0.0079]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.6305e-02, 1.5340e-01, 4.6456e-01, 1.2682e-02, 2.7950e-03,
          3.7122e+00]],

        [[1.6305e-02, 1.5340e-01, 4.6456e-01, 1.2682e-02, 2.7950e-03,
          3.7122e+00]],

        [[1.6305e-02, 1.5340e-01, 4.6456e-01, 1.2682e-02, 2.7950e-03,
          3.7122e+00]],

        [[1.6305e-02, 1.5340e-01, 4.6456e-01, 1.2682e-02, 2.7950e-03,
          3.7122e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0144, 0.7234, 7.0005, 0.0315], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0144, 0.7234, 7.0005, 0.0315])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.662
Iter 2/2000 - Loss: 2.894
Iter 3/2000 - Loss: 2.615
Iter 4/2000 - Loss: 2.636
Iter 5/2000 - Loss: 2.705
Iter 6/2000 - Loss: 2.614
Iter 7/2000 - Loss: 2.502
Iter 8/2000 - Loss: 2.462
Iter 9/2000 - Loss: 2.452
Iter 10/2000 - Loss: 2.393
Iter 11/2000 - Loss: 2.277
Iter 12/2000 - Loss: 2.145
Iter 13/2000 - Loss: 2.022
Iter 14/2000 - Loss: 1.904
Iter 15/2000 - Loss: 1.762
Iter 16/2000 - Loss: 1.585
Iter 17/2000 - Loss: 1.376
Iter 18/2000 - Loss: 1.152
Iter 19/2000 - Loss: 0.918
Iter 20/2000 - Loss: 0.675
Iter 1981/2000 - Loss: -8.061
Iter 1982/2000 - Loss: -8.061
Iter 1983/2000 - Loss: -8.061
Iter 1984/2000 - Loss: -8.061
Iter 1985/2000 - Loss: -8.061
Iter 1986/2000 - Loss: -8.061
Iter 1987/2000 - Loss: -8.061
Iter 1988/2000 - Loss: -8.061
Iter 1989/2000 - Loss: -8.061
Iter 1990/2000 - Loss: -8.061
Iter 1991/2000 - Loss: -8.061
Iter 1992/2000 - Loss: -8.061
Iter 1993/2000 - Loss: -8.061
Iter 1994/2000 - Loss: -8.061
Iter 1995/2000 - Loss: -8.061
Iter 1996/2000 - Loss: -8.061
Iter 1997/2000 - Loss: -8.061
Iter 1998/2000 - Loss: -8.061
Iter 1999/2000 - Loss: -8.061
Iter 2000/2000 - Loss: -8.061
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.5635,  7.9639, 27.4547,  9.4334, 11.5260, 55.6757]],

        [[16.7404, 35.9643,  9.9129,  1.5207,  1.9650, 21.8949]],

        [[17.5224, 36.2654, 11.7067,  0.9909,  1.5676, 20.3993]],

        [[16.2399, 34.3655, 14.8747,  3.1457,  1.9338, 36.7934]]])
Signal Variance: tensor([ 0.1083,  1.6844, 16.4610,  0.3198])
Estimated target variance: tensor([0.0144, 0.7234, 7.0005, 0.0315])
N: 160
Signal to noise ratio: tensor([20.4164, 65.2678, 92.2496, 33.4812])
Bound on condition number: tensor([  66693.7670,  681583.4433, 1361599.3572,  179359.1870])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0002825955184847877, policy loss: 26.199448754201228
Experience 16, Iter 1, disc loss: 0.00030489003709333246, policy loss: 24.69108956866562
Experience 16, Iter 2, disc loss: 0.0002827209715499839, policy loss: 26.106485250824775
Experience 16, Iter 3, disc loss: 0.0002378032839717739, policy loss: 24.82328046111619
Experience 16, Iter 4, disc loss: 0.00029067501600637483, policy loss: 21.781797674017007
Experience 16, Iter 5, disc loss: 0.00023865824911250576, policy loss: 25.38078173092912
Experience 16, Iter 6, disc loss: 0.0002551813530189992, policy loss: 23.975600904192852
Experience 16, Iter 7, disc loss: 0.00039980544745870045, policy loss: 25.553767377046032
Experience 16, Iter 8, disc loss: 0.00026870440045791697, policy loss: 26.860297348900097
Experience 16, Iter 9, disc loss: 0.0003534115196774214, policy loss: 25.99714122526879
Experience 16, Iter 10, disc loss: 0.0002341931791011461, policy loss: 24.430461989823833
Experience 16, Iter 11, disc loss: 0.0012790949336254711, policy loss: 23.248825000825295
Experience 16, Iter 12, disc loss: 0.00024832949924123856, policy loss: 21.017892883178064
Experience 16, Iter 13, disc loss: 0.000327125649276773, policy loss: 22.25243188418926
Experience 16, Iter 14, disc loss: 0.0002444048821198238, policy loss: 22.47070702367997
Experience 16, Iter 15, disc loss: 0.0003891721532613974, policy loss: 23.586424393249565
Experience 16, Iter 16, disc loss: 0.0003198684873125994, policy loss: 25.079595521303595
Experience 16, Iter 17, disc loss: 0.0005049411949874167, policy loss: 25.531968049545604
Experience 16, Iter 18, disc loss: 0.00032359595242030417, policy loss: 20.167631134646427
Experience 16, Iter 19, disc loss: 0.0002710001890887358, policy loss: 21.0178613841879
Experience 16, Iter 20, disc loss: 0.0004002226453195617, policy loss: 19.12298702434529
Experience 16, Iter 21, disc loss: 0.00031827571406203197, policy loss: 22.127471608765845
Experience 16, Iter 22, disc loss: 0.0004199487553014411, policy loss: 17.812281747830376
Experience 16, Iter 23, disc loss: 0.0004091306977639979, policy loss: 22.123031944611014
Experience 16, Iter 24, disc loss: 0.00028759382820369155, policy loss: 22.641248839158933
Experience 16, Iter 25, disc loss: 0.00025603483221888716, policy loss: 19.655011999554926
Experience 16, Iter 26, disc loss: 0.00024103916042695953, policy loss: 22.476404276247983
Experience 16, Iter 27, disc loss: 0.00023515424411124205, policy loss: 23.960107650973562
Experience 16, Iter 28, disc loss: 0.0004969384984794506, policy loss: 20.474044166303635
Experience 16, Iter 29, disc loss: 0.0002897959356586793, policy loss: 19.16629772563485
Experience 16, Iter 30, disc loss: 0.00027414347344069346, policy loss: 19.922547781215876
Experience 16, Iter 31, disc loss: 0.0002909042720786787, policy loss: 22.58069085628235
Experience 16, Iter 32, disc loss: 0.00032510000757831204, policy loss: 19.739765551102213
Experience 16, Iter 33, disc loss: 0.0002436082992330938, policy loss: 21.61355615023225
Experience 16, Iter 34, disc loss: 0.0002265494276760854, policy loss: 25.917917149315343
Experience 16, Iter 35, disc loss: 0.00023829425082914397, policy loss: 23.09146996358191
Experience 16, Iter 36, disc loss: 0.00022743562648189808, policy loss: 23.479111773087595
Experience 16, Iter 37, disc loss: 0.000238321776674555, policy loss: 25.76248707555669
Experience 16, Iter 38, disc loss: 0.00043488848210392056, policy loss: 25.247577716096966
Experience 16, Iter 39, disc loss: 0.00035276046432382763, policy loss: 24.697262981492557
Experience 16, Iter 40, disc loss: 0.00026392810844432093, policy loss: 25.67882826130859
Experience 16, Iter 41, disc loss: 0.0002708977253512622, policy loss: 26.314092106834174
Experience 16, Iter 42, disc loss: 0.0002478325082411617, policy loss: 21.629452150086614
Experience 16, Iter 43, disc loss: 0.00023185164554775337, policy loss: 26.551588398365773
Experience 16, Iter 44, disc loss: 0.00021245548258139215, policy loss: 27.336193517060472
Experience 16, Iter 45, disc loss: 0.0002113001509543767, policy loss: 28.14490882515223
Experience 16, Iter 46, disc loss: 0.00034285120202117716, policy loss: 25.68319519727895
Experience 16, Iter 47, disc loss: 0.0002291429765772376, policy loss: 27.931611862066948
Experience 16, Iter 48, disc loss: 0.00043600037263341325, policy loss: 26.65873016625022
Experience 16, Iter 49, disc loss: 0.0003014251370804668, policy loss: 29.430901789395143
Experience 16, Iter 50, disc loss: 0.0002234267807086396, policy loss: 31.643836755196688
Experience 16, Iter 51, disc loss: 0.00021546170386845122, policy loss: 30.86468254194884
Experience 16, Iter 52, disc loss: 0.0002159257595612987, policy loss: 30.493931057598903
Experience 16, Iter 53, disc loss: 0.00020826948367258342, policy loss: 32.848197365872906
Experience 16, Iter 54, disc loss: 0.00020556277216214337, policy loss: 35.144944738733614
Experience 16, Iter 55, disc loss: 0.00025629015397356567, policy loss: 31.952886953367983
Experience 16, Iter 56, disc loss: 0.00021806940797687802, policy loss: 33.46422217104226
Experience 16, Iter 57, disc loss: 0.0011559336031304133, policy loss: 31.808979000910988
Experience 16, Iter 58, disc loss: 0.0002063409260261741, policy loss: 36.01922944086856
Experience 16, Iter 59, disc loss: 0.0002649463796432963, policy loss: 36.47667138894474
Experience 16, Iter 60, disc loss: 0.00023747286873413136, policy loss: 34.956047336895665
Experience 16, Iter 61, disc loss: 0.0002119830053361854, policy loss: 29.313815849997745
Experience 16, Iter 62, disc loss: 0.00041430256137586834, policy loss: 34.58243844680917
Experience 16, Iter 63, disc loss: 0.0002503183118419823, policy loss: 34.73378479097575
Experience 16, Iter 64, disc loss: 0.0002034427257977788, policy loss: 35.45047980563461
Experience 16, Iter 65, disc loss: 0.00020500656793171352, policy loss: 37.35194475019624
Experience 16, Iter 66, disc loss: 0.0002035625468847522, policy loss: 35.41928677445948
Experience 16, Iter 67, disc loss: 0.0004428857908421721, policy loss: 36.234912467610904
Experience 16, Iter 68, disc loss: 0.0006239418773127008, policy loss: 32.830182986371696
Experience 16, Iter 69, disc loss: 0.00026621573020896457, policy loss: 32.71359480308532
Experience 16, Iter 70, disc loss: 0.00020706565887301163, policy loss: 32.849964662801284
Experience 16, Iter 71, disc loss: 0.00021561900899806698, policy loss: 32.77632379527947
Experience 16, Iter 72, disc loss: 0.00042517039682936526, policy loss: 33.78080981584172
Experience 16, Iter 73, disc loss: 0.0002063230726354044, policy loss: 34.961348844429246
Experience 16, Iter 74, disc loss: 0.00022315388300140455, policy loss: 35.034439192415704
Experience 16, Iter 75, disc loss: 0.0003914195456164638, policy loss: 35.57427966091737
Experience 16, Iter 76, disc loss: 0.0005922296741109467, policy loss: 36.22061648276236
Experience 16, Iter 77, disc loss: 0.0002058316994762333, policy loss: 35.114297807513715
Experience 16, Iter 78, disc loss: 0.00021060075941364774, policy loss: 31.835816024134395
Experience 16, Iter 79, disc loss: 0.0002444617273289468, policy loss: 33.29877240333591
Experience 16, Iter 80, disc loss: 0.00019996342326886825, policy loss: 33.491075670047934
Experience 16, Iter 81, disc loss: 0.00024432534402834454, policy loss: 34.00213518517776
Experience 16, Iter 82, disc loss: 0.00039780822904613724, policy loss: 34.75811313585196
Experience 16, Iter 83, disc loss: 0.00021300799102948588, policy loss: 32.82311311429517
Experience 16, Iter 84, disc loss: 0.00022760104386951877, policy loss: 32.85482305550468
Experience 16, Iter 85, disc loss: 0.00020474303019959257, policy loss: 34.97484195345152
Experience 16, Iter 86, disc loss: 0.00019959724697378556, policy loss: 33.77103642085608
Experience 16, Iter 87, disc loss: 0.00028349620745023966, policy loss: 33.73225766095035
Experience 16, Iter 88, disc loss: 0.00020203629742736654, policy loss: 33.23341895539762
Experience 16, Iter 89, disc loss: 0.00019831013226028957, policy loss: 34.14647954653462
Experience 16, Iter 90, disc loss: 0.00019547051706702046, policy loss: 38.694644756363175
Experience 16, Iter 91, disc loss: 0.00026284264544149306, policy loss: 35.848243422199374
Experience 16, Iter 92, disc loss: 0.000196281438794851, policy loss: 33.482264357539684
Experience 16, Iter 93, disc loss: 0.00019271447254113212, policy loss: 34.91624926273979
Experience 16, Iter 94, disc loss: 0.00019433659264862083, policy loss: 34.04723790730635
Experience 16, Iter 95, disc loss: 0.00019287083748713415, policy loss: 34.69248377081962
Experience 16, Iter 96, disc loss: 0.0001920608490505519, policy loss: 33.298051305123245
Experience 16, Iter 97, disc loss: 0.0002059963552290667, policy loss: 32.1752545181875
Experience 16, Iter 98, disc loss: 0.0001896332411430548, policy loss: 34.44701060763268
Experience 16, Iter 99, disc loss: 0.00024315545610098044, policy loss: 31.99927285296979
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.1825],
        [1.7751],
        [0.0124]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0216, 0.1775, 0.6003, 0.0148, 0.0042, 3.8310]],

        [[0.0216, 0.1775, 0.6003, 0.0148, 0.0042, 3.8310]],

        [[0.0216, 0.1775, 0.6003, 0.0148, 0.0042, 3.8310]],

        [[0.0216, 0.1775, 0.6003, 0.0148, 0.0042, 3.8310]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0164, 0.7299, 7.1005, 0.0498], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0164, 0.7299, 7.1005, 0.0498])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.941
Iter 2/2000 - Loss: 3.178
Iter 3/2000 - Loss: 2.886
Iter 4/2000 - Loss: 2.897
Iter 5/2000 - Loss: 2.956
Iter 6/2000 - Loss: 2.863
Iter 7/2000 - Loss: 2.729
Iter 8/2000 - Loss: 2.640
Iter 9/2000 - Loss: 2.588
Iter 10/2000 - Loss: 2.518
Iter 11/2000 - Loss: 2.400
Iter 12/2000 - Loss: 2.242
Iter 13/2000 - Loss: 2.069
Iter 14/2000 - Loss: 1.893
Iter 15/2000 - Loss: 1.714
Iter 16/2000 - Loss: 1.519
Iter 17/2000 - Loss: 1.300
Iter 18/2000 - Loss: 1.055
Iter 19/2000 - Loss: 0.791
Iter 20/2000 - Loss: 0.515
Iter 1981/2000 - Loss: -7.938
Iter 1982/2000 - Loss: -7.938
Iter 1983/2000 - Loss: -7.938
Iter 1984/2000 - Loss: -7.938
Iter 1985/2000 - Loss: -7.938
Iter 1986/2000 - Loss: -7.938
Iter 1987/2000 - Loss: -7.938
Iter 1988/2000 - Loss: -7.938
Iter 1989/2000 - Loss: -7.938
Iter 1990/2000 - Loss: -7.938
Iter 1991/2000 - Loss: -7.938
Iter 1992/2000 - Loss: -7.938
Iter 1993/2000 - Loss: -7.938
Iter 1994/2000 - Loss: -7.938
Iter 1995/2000 - Loss: -7.938
Iter 1996/2000 - Loss: -7.938
Iter 1997/2000 - Loss: -7.938
Iter 1998/2000 - Loss: -7.938
Iter 1999/2000 - Loss: -7.938
Iter 2000/2000 - Loss: -7.938
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.1004,  8.4590, 20.8060, 10.5856, 11.9399, 57.8345]],

        [[18.1970, 36.3755,  8.1092,  1.5654,  2.4634, 23.4028]],

        [[18.9303, 37.0399, 11.0278,  1.0167,  1.0069, 20.6323]],

        [[16.8941, 33.7790, 18.9997,  3.8724,  1.7778, 45.3941]]])
Signal Variance: tensor([ 0.1099,  2.3442, 17.3143,  0.4631])
Estimated target variance: tensor([0.0164, 0.7299, 7.1005, 0.0498])
N: 170
Signal to noise ratio: tensor([20.3058, 78.8937, 96.0376, 40.3773])
Bound on condition number: tensor([  70096.5479, 1058117.9315, 1567949.7396,  277155.9411])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0003488420545115692, policy loss: 30.57142684226779
Experience 17, Iter 1, disc loss: 0.00033385995312031536, policy loss: 32.41445037664835
Experience 17, Iter 2, disc loss: 0.0002743566384168384, policy loss: 34.575693113630294
Experience 17, Iter 3, disc loss: 0.00019885594072810108, policy loss: 34.18468569306504
Experience 17, Iter 4, disc loss: 0.000253758079642897, policy loss: 36.70926255346099
Experience 17, Iter 5, disc loss: 0.000270092664968982, policy loss: 33.35562084881495
Experience 17, Iter 6, disc loss: 0.00018858438438386512, policy loss: 33.34593089079435
Experience 17, Iter 7, disc loss: 0.00018476555803684743, policy loss: 35.61617250500838
Experience 17, Iter 8, disc loss: 0.00026927617892202806, policy loss: 36.35117802342007
Experience 17, Iter 9, disc loss: 0.0001896268690707235, policy loss: 38.41218660513648
Experience 17, Iter 10, disc loss: 0.00018222061232114195, policy loss: 38.27688560319284
Experience 17, Iter 11, disc loss: 0.002374293521553962, policy loss: 34.54042898607783
Experience 17, Iter 12, disc loss: 0.00018304309184278284, policy loss: 37.10362279923476
Experience 17, Iter 13, disc loss: 0.00018488277517123464, policy loss: 34.823026051418694
Experience 17, Iter 14, disc loss: 0.00018618549888739535, policy loss: 35.80380116828471
Experience 17, Iter 15, disc loss: 0.00018765566233626987, policy loss: 37.64977281695664
Experience 17, Iter 16, disc loss: 0.0001897918619756763, policy loss: 36.50291281593873
Experience 17, Iter 17, disc loss: 0.00018553897679510182, policy loss: 37.87582967024278
Experience 17, Iter 18, disc loss: 0.00018600653231907189, policy loss: 41.215013106320285
Experience 17, Iter 19, disc loss: 0.0001872197295200266, policy loss: 39.715709586358415
Experience 17, Iter 20, disc loss: 0.0001861781397385161, policy loss: 38.5822481971634
Experience 17, Iter 21, disc loss: 0.00018596159460341276, policy loss: 41.292657640890596
Experience 17, Iter 22, disc loss: 0.0002067797873236688, policy loss: 40.303401493702594
Experience 17, Iter 23, disc loss: 0.00019831126381051917, policy loss: 41.901054243552224
Experience 17, Iter 24, disc loss: 0.00018569108129873926, policy loss: 36.5022499107676
Experience 17, Iter 25, disc loss: 0.00018506489644500712, policy loss: 38.85081409706178
Experience 17, Iter 26, disc loss: 0.00018480698655452382, policy loss: 39.58562324996359
Experience 17, Iter 27, disc loss: 0.0001840105738062237, policy loss: 38.46326473285578
Experience 17, Iter 28, disc loss: 0.00018402024515984193, policy loss: 35.824569062319924
Experience 17, Iter 29, disc loss: 0.0001829068959043015, policy loss: 37.2116907011882
Experience 17, Iter 30, disc loss: 0.00021187948439170773, policy loss: 29.36715509725019
Experience 17, Iter 31, disc loss: 0.00018191897695937257, policy loss: 30.2390518277532
Experience 17, Iter 32, disc loss: 0.00022372772666404763, policy loss: 28.653554383518365
Experience 17, Iter 33, disc loss: 0.000187873379023225, policy loss: 26.97572955981184
Experience 17, Iter 34, disc loss: 0.00019083076525895832, policy loss: 25.63237131368316
Experience 17, Iter 35, disc loss: 0.00017971585755002537, policy loss: 26.49665200169101
Experience 17, Iter 36, disc loss: 0.00018302687118016366, policy loss: 19.9527714227357
Experience 17, Iter 37, disc loss: 0.00017936146360697137, policy loss: 22.105976096630535
Experience 17, Iter 38, disc loss: 0.00018478033513762607, policy loss: 21.2489217416596
Experience 17, Iter 39, disc loss: 0.00017918547835357946, policy loss: 20.481242418437784
Experience 17, Iter 40, disc loss: 0.0001811643643439867, policy loss: 20.396294341923237
Experience 17, Iter 41, disc loss: 0.00018096390797679195, policy loss: 22.526320924648573
Experience 17, Iter 42, disc loss: 0.0001758891085332891, policy loss: 22.432607428771593
Experience 17, Iter 43, disc loss: 0.0001839980054946351, policy loss: 22.21555645226477
Experience 17, Iter 44, disc loss: 0.00017426855467001556, policy loss: 21.13752931228076
Experience 17, Iter 45, disc loss: 0.00017833455191354697, policy loss: 22.37465054396819
Experience 17, Iter 46, disc loss: 0.00017525538121962057, policy loss: 21.545460413502035
Experience 17, Iter 47, disc loss: 0.00017127574922104567, policy loss: 22.938339732379283
Experience 17, Iter 48, disc loss: 0.00018797531160584738, policy loss: 20.132978147884845
Experience 17, Iter 49, disc loss: 0.00017512172993829688, policy loss: 22.026552001079537
Experience 17, Iter 50, disc loss: 0.0001798780177687599, policy loss: 24.45800515930367
Experience 17, Iter 51, disc loss: 0.0001901133492346539, policy loss: 22.879937072289906
Experience 17, Iter 52, disc loss: 0.00017177732208852232, policy loss: 23.41512092070161
Experience 17, Iter 53, disc loss: 0.00017276316482043065, policy loss: 21.5354687985007
Experience 17, Iter 54, disc loss: 0.00017692950398932796, policy loss: 22.81881755811584
Experience 17, Iter 55, disc loss: 0.00018542548854252276, policy loss: 20.754728048380212
Experience 17, Iter 56, disc loss: 0.00017975379990593828, policy loss: 21.900459941700372
Experience 17, Iter 57, disc loss: 0.00016807661758589805, policy loss: 20.10556948059783
Experience 17, Iter 58, disc loss: 0.00017276814101381137, policy loss: 21.423586529266522
Experience 17, Iter 59, disc loss: 0.00017242500492648866, policy loss: 20.440646921902704
Experience 17, Iter 60, disc loss: 0.00017126084634277528, policy loss: 19.526361270100097
Experience 17, Iter 61, disc loss: 0.00016362579308022387, policy loss: 23.12726045670425
Experience 17, Iter 62, disc loss: 0.0001675317694810316, policy loss: 20.084969797735074
Experience 17, Iter 63, disc loss: 0.00017063543670842063, policy loss: 20.148684889972728
Experience 17, Iter 64, disc loss: 0.0001914416217028455, policy loss: 17.938521541137924
Experience 17, Iter 65, disc loss: 0.0001639084930388889, policy loss: 19.141449868235636
Experience 17, Iter 66, disc loss: 0.00016147511475297528, policy loss: 22.050491019905817
Experience 17, Iter 67, disc loss: 0.00016740510029868177, policy loss: 20.098955853568725
Experience 17, Iter 68, disc loss: 0.00020462122535947142, policy loss: 19.494874508156514
Experience 17, Iter 69, disc loss: 0.00018508294036222595, policy loss: 19.6860219454583
Experience 17, Iter 70, disc loss: 0.00017568318544067496, policy loss: 20.045383267722528
Experience 17, Iter 71, disc loss: 0.00021836825434643926, policy loss: 20.570330065393833
Experience 17, Iter 72, disc loss: 0.00017557884380495214, policy loss: 19.851642319288093
Experience 17, Iter 73, disc loss: 0.0001586225231251777, policy loss: 20.350751023175214
Experience 17, Iter 74, disc loss: 0.00016360428141870654, policy loss: 19.579035110033143
Experience 17, Iter 75, disc loss: 0.0001937738729619384, policy loss: 19.629424085827434
Experience 17, Iter 76, disc loss: 0.00030820378336724217, policy loss: 18.771471910023713
Experience 17, Iter 77, disc loss: 0.00015790655640411937, policy loss: 20.419151875995667
Experience 17, Iter 78, disc loss: 0.00015956477372301768, policy loss: 19.589500906792175
Experience 17, Iter 79, disc loss: 0.00016704848057197226, policy loss: 19.90916167567689
Experience 17, Iter 80, disc loss: 0.00015956870261517672, policy loss: 19.784564208564
Experience 17, Iter 81, disc loss: 0.00021783615686195605, policy loss: 17.716330871113005
Experience 17, Iter 82, disc loss: 0.00017957773237685677, policy loss: 19.09878545771293
Experience 17, Iter 83, disc loss: 0.00018117717378772345, policy loss: 19.739163970592926
Experience 17, Iter 84, disc loss: 0.00018209239094880525, policy loss: 16.820872262448425
Experience 17, Iter 85, disc loss: 0.00022858929312442752, policy loss: 17.95414205616994
Experience 17, Iter 86, disc loss: 0.0001534689206246392, policy loss: 18.64983346290119
Experience 17, Iter 87, disc loss: 0.00027953558187071554, policy loss: 16.648107369940355
Experience 17, Iter 88, disc loss: 0.00016275860524182128, policy loss: 17.85807028609637
Experience 17, Iter 89, disc loss: 0.00020684909596211363, policy loss: 18.726512572224593
Experience 17, Iter 90, disc loss: 0.0001536371395712731, policy loss: 19.692848235780595
Experience 17, Iter 91, disc loss: 0.0002495403080326644, policy loss: 18.755447077441467
Experience 17, Iter 92, disc loss: 0.0002169963650446279, policy loss: 17.484821870041756
Experience 17, Iter 93, disc loss: 0.0001585636627136412, policy loss: 19.591562201269493
Experience 17, Iter 94, disc loss: 0.00016389541149081657, policy loss: 15.876413263898698
Experience 17, Iter 95, disc loss: 0.00016043304466674616, policy loss: 18.335597891900274
Experience 17, Iter 96, disc loss: 0.00016096657168355105, policy loss: 19.667290752139298
Experience 17, Iter 97, disc loss: 0.00016903291115839773, policy loss: 17.225333998908127
Experience 17, Iter 98, disc loss: 0.0002026206147502232, policy loss: 17.75787997426484
Experience 17, Iter 99, disc loss: 0.00023467631673261335, policy loss: 17.14410182587662
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.1870],
        [1.8264],
        [0.0136]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0208, 0.1765, 0.6532, 0.0162, 0.0063, 3.9970]],

        [[0.0208, 0.1765, 0.6532, 0.0162, 0.0063, 3.9970]],

        [[0.0208, 0.1765, 0.6532, 0.0162, 0.0063, 3.9970]],

        [[0.0208, 0.1765, 0.6532, 0.0162, 0.0063, 3.9970]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0162, 0.7480, 7.3056, 0.0543], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0162, 0.7480, 7.3056, 0.0543])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.003
Iter 2/2000 - Loss: 3.237
Iter 3/2000 - Loss: 2.946
Iter 4/2000 - Loss: 2.950
Iter 5/2000 - Loss: 3.008
Iter 6/2000 - Loss: 2.919
Iter 7/2000 - Loss: 2.784
Iter 8/2000 - Loss: 2.685
Iter 9/2000 - Loss: 2.619
Iter 10/2000 - Loss: 2.543
Iter 11/2000 - Loss: 2.425
Iter 12/2000 - Loss: 2.268
Iter 13/2000 - Loss: 2.088
Iter 14/2000 - Loss: 1.899
Iter 15/2000 - Loss: 1.704
Iter 16/2000 - Loss: 1.497
Iter 17/2000 - Loss: 1.271
Iter 18/2000 - Loss: 1.022
Iter 19/2000 - Loss: 0.753
Iter 20/2000 - Loss: 0.469
Iter 1981/2000 - Loss: -7.888
Iter 1982/2000 - Loss: -7.888
Iter 1983/2000 - Loss: -7.888
Iter 1984/2000 - Loss: -7.888
Iter 1985/2000 - Loss: -7.888
Iter 1986/2000 - Loss: -7.888
Iter 1987/2000 - Loss: -7.888
Iter 1988/2000 - Loss: -7.888
Iter 1989/2000 - Loss: -7.888
Iter 1990/2000 - Loss: -7.888
Iter 1991/2000 - Loss: -7.888
Iter 1992/2000 - Loss: -7.888
Iter 1993/2000 - Loss: -7.888
Iter 1994/2000 - Loss: -7.888
Iter 1995/2000 - Loss: -7.889
Iter 1996/2000 - Loss: -7.889
Iter 1997/2000 - Loss: -7.889
Iter 1998/2000 - Loss: -7.889
Iter 1999/2000 - Loss: -7.889
Iter 2000/2000 - Loss: -7.889
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.6169,  8.8031, 23.2268,  9.8536,  7.4294, 55.1864]],

        [[18.2106, 34.6633,  8.2463,  1.3720,  3.6651, 24.8542]],

        [[18.8375, 37.3864, 10.7151,  1.0371,  1.1751, 23.1331]],

        [[16.6733, 33.4433, 19.6296,  3.5501,  2.0144, 44.0260]]])
Signal Variance: tensor([ 0.1125,  2.7240, 20.4127,  0.5377])
Estimated target variance: tensor([0.0162, 0.7480, 7.3056, 0.0543])
N: 180
Signal to noise ratio: tensor([ 20.4085,  84.1778, 104.2264,  43.6258])
Bound on condition number: tensor([  74972.5908, 1275462.7223, 1955367.7075,  342579.3525])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.00018288848404252595, policy loss: 18.080928301773675
Experience 18, Iter 1, disc loss: 0.00019559599517913803, policy loss: 19.232279481076024
Experience 18, Iter 2, disc loss: 0.00015261705757676206, policy loss: 18.324318365100677
Experience 18, Iter 3, disc loss: 0.00016938358157807395, policy loss: 17.87566818674579
Experience 18, Iter 4, disc loss: 0.0001771336187541369, policy loss: 17.462094647156846
Experience 18, Iter 5, disc loss: 0.00022669017828474595, policy loss: 18.377721487856377
Experience 18, Iter 6, disc loss: 0.00016319739002147615, policy loss: 18.172194773030252
Experience 18, Iter 7, disc loss: 0.00019278884778995027, policy loss: 18.760243925081504
Experience 18, Iter 8, disc loss: 0.00015590968726679233, policy loss: 17.747711075714456
Experience 18, Iter 9, disc loss: 0.00021138378186038483, policy loss: 16.135911428705594
Experience 18, Iter 10, disc loss: 0.00020625482200101842, policy loss: 17.20015509237606
Experience 18, Iter 11, disc loss: 0.0001923686916139776, policy loss: 16.941853927894122
Experience 18, Iter 12, disc loss: 0.0002521865253897971, policy loss: 17.73348970715998
Experience 18, Iter 13, disc loss: 0.00024660267968392357, policy loss: 18.499361184477532
Experience 18, Iter 14, disc loss: 0.00018195957943114132, policy loss: 16.784779349470398
Experience 18, Iter 15, disc loss: 0.00020534626527332492, policy loss: 18.775391824375728
Experience 18, Iter 16, disc loss: 0.0001749519048163101, policy loss: 16.423600619151102
Experience 18, Iter 17, disc loss: 0.00025505431638722523, policy loss: 18.74063170568712
Experience 18, Iter 18, disc loss: 0.00042285537575014804, policy loss: 17.389115534802457
Experience 18, Iter 19, disc loss: 0.00014836086841784978, policy loss: 18.643178231384844
Experience 18, Iter 20, disc loss: 0.00015849448675839424, policy loss: 16.71559963864628
Experience 18, Iter 21, disc loss: 0.0001963662148714978, policy loss: 16.858712745981485
Experience 18, Iter 22, disc loss: 0.0001540565486333507, policy loss: 17.78772416517895
Experience 18, Iter 23, disc loss: 0.00031886505500305137, policy loss: 18.18618539269005
Experience 18, Iter 24, disc loss: 0.000331185451493546, policy loss: 18.50388724619163
Experience 18, Iter 25, disc loss: 0.00029049791221165275, policy loss: 18.1347263943803
Experience 18, Iter 26, disc loss: 0.000319974242585825, policy loss: 17.450810567552658
Experience 18, Iter 27, disc loss: 0.00015952412059556302, policy loss: 16.87789509213915
Experience 18, Iter 28, disc loss: 0.00021455892595555897, policy loss: 17.78949281692286
Experience 18, Iter 29, disc loss: 0.0005482226818942672, policy loss: 17.398771066569008
Experience 18, Iter 30, disc loss: 0.0001911671440757118, policy loss: 16.651980686070836
Experience 18, Iter 31, disc loss: 0.00017507388724956528, policy loss: 16.1482006836291
Experience 18, Iter 32, disc loss: 0.00024058390395864163, policy loss: 17.441396608965164
Experience 18, Iter 33, disc loss: 0.00016865856950935742, policy loss: 19.572646448919116
Experience 18, Iter 34, disc loss: 0.0002588065164700404, policy loss: 17.365849450879338
Experience 18, Iter 35, disc loss: 0.0005782116360343022, policy loss: 16.888832803137028
Experience 18, Iter 36, disc loss: 0.000163845185106344, policy loss: 17.032887866781515
Experience 18, Iter 37, disc loss: 0.00018394504814928068, policy loss: 17.83474113499707
Experience 18, Iter 38, disc loss: 0.0002999770843645722, policy loss: 16.97056862302974
Experience 18, Iter 39, disc loss: 0.00022192493736178772, policy loss: 16.126535441914875
Experience 18, Iter 40, disc loss: 0.0001531997540580836, policy loss: 17.75934604939728
Experience 18, Iter 41, disc loss: 0.00028733907289454734, policy loss: 18.280362617650333
Experience 18, Iter 42, disc loss: 0.00017476353494686643, policy loss: 17.304279316203026
Experience 18, Iter 43, disc loss: 0.0002495285339678376, policy loss: 17.796444148181106
Experience 18, Iter 44, disc loss: 0.0001674164239772063, policy loss: 15.51463492654921
Experience 18, Iter 45, disc loss: 0.00016829533806887354, policy loss: 16.047252547044167
Experience 18, Iter 46, disc loss: 0.00015866441411693138, policy loss: 17.735095877383703
Experience 18, Iter 47, disc loss: 0.00017681799849290363, policy loss: 17.340304378927677
Experience 18, Iter 48, disc loss: 0.00018203670681465168, policy loss: 16.18029422839069
Experience 18, Iter 49, disc loss: 0.00016121295674621547, policy loss: 17.936948512434874
Experience 18, Iter 50, disc loss: 0.00022029555660786182, policy loss: 16.163081850642286
Experience 18, Iter 51, disc loss: 0.000339360997429896, policy loss: 17.51676290457769
Experience 18, Iter 52, disc loss: 0.00015605746829537654, policy loss: 18.27538817590892
Experience 18, Iter 53, disc loss: 0.00018152182829624596, policy loss: 16.700550549301845
Experience 18, Iter 54, disc loss: 0.00016265261578094325, policy loss: 17.4676305085256
Experience 18, Iter 55, disc loss: 0.0001688750367111273, policy loss: 17.568606401583956
Experience 18, Iter 56, disc loss: 0.0003840181674383302, policy loss: 15.64744184547095
Experience 18, Iter 57, disc loss: 0.00044485818987601006, policy loss: 16.917364581400673
Experience 18, Iter 58, disc loss: 0.00019664414958877086, policy loss: 17.37102739048169
Experience 18, Iter 59, disc loss: 0.0001644695324719285, policy loss: 16.89756605156489
Experience 18, Iter 60, disc loss: 0.00028325466789194773, policy loss: 16.65414541709203
Experience 18, Iter 61, disc loss: 0.00019497708202791646, policy loss: 16.356845204948335
Experience 18, Iter 62, disc loss: 0.00022691972128974634, policy loss: 16.91274511281866
Experience 18, Iter 63, disc loss: 0.0001797737040125255, policy loss: 17.315675914790774
Experience 18, Iter 64, disc loss: 0.0002233489667957394, policy loss: 15.751272026403283
Experience 18, Iter 65, disc loss: 0.00026372851384351263, policy loss: 17.970509237088603
Experience 18, Iter 66, disc loss: 0.0003213284881553197, policy loss: 17.513024988792793
Experience 18, Iter 67, disc loss: 0.000181626771936967, policy loss: 15.893133357082355
Experience 18, Iter 68, disc loss: 0.0003834526718501154, policy loss: 15.95630368925885
Experience 18, Iter 69, disc loss: 0.000933163126776853, policy loss: 16.473151230247474
Experience 18, Iter 70, disc loss: 0.00018171509614343988, policy loss: 16.86100221645194
Experience 18, Iter 71, disc loss: 0.0003289512485759941, policy loss: 18.29177024679995
Experience 18, Iter 72, disc loss: 0.0002045668589405741, policy loss: 17.159473232323048
Experience 18, Iter 73, disc loss: 0.00018229100480349382, policy loss: 16.850864634971558
Experience 18, Iter 74, disc loss: 0.0001691491021379673, policy loss: 17.732967305560933
Experience 18, Iter 75, disc loss: 0.00016933904096119528, policy loss: 16.120321915307237
Experience 18, Iter 76, disc loss: 0.00018953542765836175, policy loss: 16.52679191377793
Experience 18, Iter 77, disc loss: 0.00019361075365684866, policy loss: 17.278024276751296
Experience 18, Iter 78, disc loss: 0.00017701834596777353, policy loss: 16.95727042281923
Experience 18, Iter 79, disc loss: 0.0002715988271490205, policy loss: 17.84985340417419
Experience 18, Iter 80, disc loss: 0.0002160840939537551, policy loss: 18.3234291153777
Experience 18, Iter 81, disc loss: 0.000154653205735054, policy loss: 16.64444687836213
Experience 18, Iter 82, disc loss: 0.0001657592762781912, policy loss: 15.80376270560642
Experience 18, Iter 83, disc loss: 0.0002001280005259321, policy loss: 17.63972921350565
Experience 18, Iter 84, disc loss: 0.00023609345173869725, policy loss: 16.40679014325599
Experience 18, Iter 85, disc loss: 0.00035129200920044326, policy loss: 15.851171788349601
Experience 18, Iter 86, disc loss: 0.0003542122054908522, policy loss: 16.636901760108348
Experience 18, Iter 87, disc loss: 0.00020152495659265282, policy loss: 17.132015627246997
Experience 18, Iter 88, disc loss: 0.00023158937458728817, policy loss: 16.89725056173562
Experience 18, Iter 89, disc loss: 0.000377284006432098, policy loss: 16.8346451511654
Experience 18, Iter 90, disc loss: 0.00021153863014219674, policy loss: 15.9863633498435
Experience 18, Iter 91, disc loss: 0.00014860944627370404, policy loss: 18.40597578612477
Experience 18, Iter 92, disc loss: 0.0002922432601431686, policy loss: 17.054207952844326
Experience 18, Iter 93, disc loss: 0.00017253707088147282, policy loss: 16.906038209724226
Experience 18, Iter 94, disc loss: 0.00034636144720286914, policy loss: 15.143386302168597
Experience 18, Iter 95, disc loss: 0.00018482590297797105, policy loss: 15.934708589277193
Experience 18, Iter 96, disc loss: 0.0004388584612266993, policy loss: 17.50523416760781
Experience 18, Iter 97, disc loss: 0.00020428972174664088, policy loss: 19.597899878961044
Experience 18, Iter 98, disc loss: 0.0001494460206759495, policy loss: 17.84747307225224
Experience 18, Iter 99, disc loss: 0.00016825267042048088, policy loss: 17.262883337906764
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1869],
        [1.8334],
        [0.0135]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0199, 0.1702, 0.6485, 0.0161, 0.0060, 3.9858]],

        [[0.0199, 0.1702, 0.6485, 0.0161, 0.0060, 3.9858]],

        [[0.0199, 0.1702, 0.6485, 0.0161, 0.0060, 3.9858]],

        [[0.0199, 0.1702, 0.6485, 0.0161, 0.0060, 3.9858]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0156, 0.7476, 7.3336, 0.0539], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0156, 0.7476, 7.3336, 0.0539])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.985
Iter 2/2000 - Loss: 3.233
Iter 3/2000 - Loss: 2.930
Iter 4/2000 - Loss: 2.936
Iter 5/2000 - Loss: 3.002
Iter 6/2000 - Loss: 2.916
Iter 7/2000 - Loss: 2.780
Iter 8/2000 - Loss: 2.681
Iter 9/2000 - Loss: 2.616
Iter 10/2000 - Loss: 2.544
Iter 11/2000 - Loss: 2.433
Iter 12/2000 - Loss: 2.280
Iter 13/2000 - Loss: 2.101
Iter 14/2000 - Loss: 1.912
Iter 15/2000 - Loss: 1.716
Iter 16/2000 - Loss: 1.509
Iter 17/2000 - Loss: 1.285
Iter 18/2000 - Loss: 1.038
Iter 19/2000 - Loss: 0.771
Iter 20/2000 - Loss: 0.487
Iter 1981/2000 - Loss: -7.940
Iter 1982/2000 - Loss: -7.940
Iter 1983/2000 - Loss: -7.940
Iter 1984/2000 - Loss: -7.940
Iter 1985/2000 - Loss: -7.940
Iter 1986/2000 - Loss: -7.940
Iter 1987/2000 - Loss: -7.940
Iter 1988/2000 - Loss: -7.940
Iter 1989/2000 - Loss: -7.940
Iter 1990/2000 - Loss: -7.940
Iter 1991/2000 - Loss: -7.940
Iter 1992/2000 - Loss: -7.940
Iter 1993/2000 - Loss: -7.940
Iter 1994/2000 - Loss: -7.940
Iter 1995/2000 - Loss: -7.940
Iter 1996/2000 - Loss: -7.940
Iter 1997/2000 - Loss: -7.940
Iter 1998/2000 - Loss: -7.941
Iter 1999/2000 - Loss: -7.941
Iter 2000/2000 - Loss: -7.941
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.0606,  8.5198, 23.6716,  8.8098,  7.9207, 53.6917]],

        [[17.6302, 33.5692,  8.1046,  1.3737,  3.8942, 23.9260]],

        [[18.0228, 36.6865, 10.2043,  1.0298,  1.2405, 22.7935]],

        [[16.5374, 32.4553, 19.4828,  3.3224,  1.9633, 43.3192]]])
Signal Variance: tensor([ 0.1090,  2.6566, 20.3876,  0.5082])
Estimated target variance: tensor([0.0156, 0.7476, 7.3336, 0.0539])
N: 190
Signal to noise ratio: tensor([ 19.7154,  85.2442, 103.9398,  42.4311])
Bound on condition number: tensor([  73853.2379, 1380649.7838, 2052662.2997,  342077.0606])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.00015386149530135924, policy loss: 16.46623032315464
Experience 19, Iter 1, disc loss: 0.0001721362668284998, policy loss: 17.200052162714897
Experience 19, Iter 2, disc loss: 0.00017968001429153466, policy loss: 16.68636650455746
Experience 19, Iter 3, disc loss: 0.00015734323041881185, policy loss: 17.976387035437238
Experience 19, Iter 4, disc loss: 0.00015200420518727076, policy loss: 19.380357269985012
Experience 19, Iter 5, disc loss: 0.00023051205371558904, policy loss: 18.608655860588488
Experience 19, Iter 6, disc loss: 0.00014597412550469997, policy loss: 17.808403060577874
Experience 19, Iter 7, disc loss: 0.00017580873431463412, policy loss: 17.98552440878997
Experience 19, Iter 8, disc loss: 0.00014903783154640774, policy loss: 18.438737029506278
Experience 19, Iter 9, disc loss: 0.00016039711254391102, policy loss: 18.41822614308196
Experience 19, Iter 10, disc loss: 0.00019062038419077643, policy loss: 17.20076202102798
Experience 19, Iter 11, disc loss: 0.0002916048644076095, policy loss: 17.575414711046598
Experience 19, Iter 12, disc loss: 0.0001578294039243931, policy loss: 18.033464710319414
Experience 19, Iter 13, disc loss: 0.0002511531371774888, policy loss: 17.256124483983854
Experience 19, Iter 14, disc loss: 0.00015739498354104444, policy loss: 17.256362542270445
Experience 19, Iter 15, disc loss: 0.00029615300254975973, policy loss: 17.72031911361485
Experience 19, Iter 16, disc loss: 0.00018820996421256304, policy loss: 16.93367794493071
Experience 19, Iter 17, disc loss: 0.00014762713256255935, policy loss: 16.83688810453731
Experience 19, Iter 18, disc loss: 0.000168753638603382, policy loss: 18.805656072588945
Experience 19, Iter 19, disc loss: 0.00016220027210171324, policy loss: 15.15796200347716
Experience 19, Iter 20, disc loss: 0.0002155173282581529, policy loss: 17.51011165242936
Experience 19, Iter 21, disc loss: 0.00020639964352178915, policy loss: 16.44792465483485
Experience 19, Iter 22, disc loss: 0.00015396820125848927, policy loss: 16.65599851400853
Experience 19, Iter 23, disc loss: 0.00015399051610270333, policy loss: 16.183149442941623
Experience 19, Iter 24, disc loss: 0.00016003831113543137, policy loss: 17.367898189613264
Experience 19, Iter 25, disc loss: 0.0002248828997537197, policy loss: 16.304016565431265
Experience 19, Iter 26, disc loss: 0.0001715438180019028, policy loss: 16.733695770047717
Experience 19, Iter 27, disc loss: 0.00017868208574611455, policy loss: 16.770726628245093
Experience 19, Iter 28, disc loss: 0.00015709816670215893, policy loss: 17.6953813379577
Experience 19, Iter 29, disc loss: 0.0002454679631790544, policy loss: 14.917939381964539
Experience 19, Iter 30, disc loss: 0.0003206223237172532, policy loss: 15.418736229944688
Experience 19, Iter 31, disc loss: 0.00038073820905759213, policy loss: 16.75806218186994
Experience 19, Iter 32, disc loss: 0.00018218484902246917, policy loss: 16.506544660176218
Experience 19, Iter 33, disc loss: 0.000299389917399437, policy loss: 17.707871162877066
Experience 19, Iter 34, disc loss: 0.0003436912473832687, policy loss: 15.402432001144792
Experience 19, Iter 35, disc loss: 0.0006708393524124145, policy loss: 15.843603082206993
Experience 19, Iter 36, disc loss: 0.0001716301948193175, policy loss: 16.22242059621793
Experience 19, Iter 37, disc loss: 0.0001592487224773937, policy loss: 16.384022219560983
Experience 19, Iter 38, disc loss: 0.0001733875738353107, policy loss: 15.800784950400704
Experience 19, Iter 39, disc loss: 0.0001414404293973555, policy loss: 19.10622632689804
Experience 19, Iter 40, disc loss: 0.00019283386851019722, policy loss: 18.34289254305417
Experience 19, Iter 41, disc loss: 0.00013955046236165014, policy loss: 17.863624269121036
Experience 19, Iter 42, disc loss: 0.000307721540913409, policy loss: 16.647482483721078
Experience 19, Iter 43, disc loss: 0.0004012566847230014, policy loss: 16.240233601009088
Experience 19, Iter 44, disc loss: 0.0003327382597701937, policy loss: 15.673430575694136
Experience 19, Iter 45, disc loss: 0.00016735489109268484, policy loss: 17.955075482574333
Experience 19, Iter 46, disc loss: 0.0001543507045610886, policy loss: 17.289762513417745
Experience 19, Iter 47, disc loss: 0.00020641449075266144, policy loss: 17.06742262543166
Experience 19, Iter 48, disc loss: 0.00020448515579653433, policy loss: 15.175738475306911
Experience 19, Iter 49, disc loss: 0.00022392336550954798, policy loss: 15.807284789847461
Experience 19, Iter 50, disc loss: 0.0001462784481586712, policy loss: 16.801342993455407
Experience 19, Iter 51, disc loss: 0.0005439634610665018, policy loss: 15.265787005083581
Experience 19, Iter 52, disc loss: 0.0001881996681054682, policy loss: 15.816735129335166
Experience 19, Iter 53, disc loss: 0.00019667306168795516, policy loss: 16.744191630726476
Experience 19, Iter 54, disc loss: 0.00020571876334850336, policy loss: 15.258670107085093
Experience 19, Iter 55, disc loss: 0.00015747705241626436, policy loss: 16.396748457688094
Experience 19, Iter 56, disc loss: 0.00020687960813920277, policy loss: 16.626553478873852
Experience 19, Iter 57, disc loss: 0.00026661971652691165, policy loss: 15.659255688303213
Experience 19, Iter 58, disc loss: 0.00026790541659586114, policy loss: 15.758659231938378
Experience 19, Iter 59, disc loss: 0.0002897169444160793, policy loss: 16.314875305250993
Experience 19, Iter 60, disc loss: 0.0003052360929625785, policy loss: 15.666071139613582
Experience 19, Iter 61, disc loss: 0.000388621671302932, policy loss: 15.966109783223521
Experience 19, Iter 62, disc loss: 0.00016685870605254004, policy loss: 16.480574721605883
Experience 19, Iter 63, disc loss: 0.0001781905447950205, policy loss: 16.500051779636706
Experience 19, Iter 64, disc loss: 0.00016031778845509553, policy loss: 17.0066277689875
Experience 19, Iter 65, disc loss: 0.0002515438913629204, policy loss: 15.40962839301782
Experience 19, Iter 66, disc loss: 0.00023409914162179338, policy loss: 15.901688908674602
Experience 19, Iter 67, disc loss: 0.000504786593110735, policy loss: 16.172395587192028
Experience 19, Iter 68, disc loss: 0.0006124382747176706, policy loss: 15.747561042084715
Experience 19, Iter 69, disc loss: 0.00015772597143159638, policy loss: 15.957687956332048
Experience 19, Iter 70, disc loss: 0.00037567170257155606, policy loss: 15.985202428465584
Experience 19, Iter 71, disc loss: 0.00035663701133539826, policy loss: 15.90145083243548
Experience 19, Iter 72, disc loss: 0.0003625470324312731, policy loss: 15.307648796265577
Experience 19, Iter 73, disc loss: 0.0002926947552238242, policy loss: 15.812652830704298
Experience 19, Iter 74, disc loss: 0.00015121865059363195, policy loss: 15.0471334771888
Experience 19, Iter 75, disc loss: 0.00027531451885968886, policy loss: 16.503550675314806
Experience 19, Iter 76, disc loss: 0.00017431642014515835, policy loss: 16.155651221243147
Experience 19, Iter 77, disc loss: 0.0002464385592978504, policy loss: 14.825274517383328
Experience 19, Iter 78, disc loss: 0.00031900103538043075, policy loss: 15.471525801727857
Experience 19, Iter 79, disc loss: 0.0002166544000224822, policy loss: 15.467968165360649
Experience 19, Iter 80, disc loss: 0.00039999424207847785, policy loss: 15.556091109412835
Experience 19, Iter 81, disc loss: 0.0001727110303739427, policy loss: 15.875588871495022
Experience 19, Iter 82, disc loss: 0.0003917729771057273, policy loss: 16.19767498037213
Experience 19, Iter 83, disc loss: 0.00024001398948343098, policy loss: 14.866311382599124
Experience 19, Iter 84, disc loss: 0.00014988937338352888, policy loss: 16.515684718451293
Experience 19, Iter 85, disc loss: 0.00016885058633011937, policy loss: 16.464730732949093
Experience 19, Iter 86, disc loss: 0.00024639568332015914, policy loss: 16.73913898259951
Experience 19, Iter 87, disc loss: 0.00016256167421128088, policy loss: 16.226837606085628
Experience 19, Iter 88, disc loss: 0.0002356114577154784, policy loss: 14.974734458112616
Experience 19, Iter 89, disc loss: 0.00025954807738622695, policy loss: 15.510268803574188
Experience 19, Iter 90, disc loss: 0.00022642820270985387, policy loss: 16.893955490470134
Experience 19, Iter 91, disc loss: 0.0002756548888613403, policy loss: 14.065422430133998
Experience 19, Iter 92, disc loss: 0.0002186201932582692, policy loss: 15.625184309267858
Experience 19, Iter 93, disc loss: 0.00018061831881800386, policy loss: 17.553758890089
Experience 19, Iter 94, disc loss: 0.00018152395971813006, policy loss: 16.046374169955175
Experience 19, Iter 95, disc loss: 0.00017265988222210776, policy loss: 16.874472644186298
Experience 19, Iter 96, disc loss: 0.00026243431817304745, policy loss: 15.926426415901258
Experience 19, Iter 97, disc loss: 0.0001802431338946783, policy loss: 15.925537810880178
Experience 19, Iter 98, disc loss: 0.00021402786912921526, policy loss: 15.720334285941485
Experience 19, Iter 99, disc loss: 0.00020248521894117416, policy loss: 15.51339428842864
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.1889],
        [1.8642],
        [0.0143]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0192, 0.1685, 0.6833, 0.0171, 0.0070, 4.0845]],

        [[0.0192, 0.1685, 0.6833, 0.0171, 0.0070, 4.0845]],

        [[0.0192, 0.1685, 0.6833, 0.0171, 0.0070, 4.0845]],

        [[0.0192, 0.1685, 0.6833, 0.0171, 0.0070, 4.0845]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0153, 0.7558, 7.4570, 0.0570], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0153, 0.7558, 7.4570, 0.0570])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.019
Iter 2/2000 - Loss: 3.271
Iter 3/2000 - Loss: 2.962
Iter 4/2000 - Loss: 2.962
Iter 5/2000 - Loss: 3.029
Iter 6/2000 - Loss: 2.946
Iter 7/2000 - Loss: 2.805
Iter 8/2000 - Loss: 2.695
Iter 9/2000 - Loss: 2.618
Iter 10/2000 - Loss: 2.541
Iter 11/2000 - Loss: 2.429
Iter 12/2000 - Loss: 2.275
Iter 13/2000 - Loss: 2.091
Iter 14/2000 - Loss: 1.892
Iter 15/2000 - Loss: 1.685
Iter 16/2000 - Loss: 1.470
Iter 17/2000 - Loss: 1.242
Iter 18/2000 - Loss: 0.994
Iter 19/2000 - Loss: 0.724
Iter 20/2000 - Loss: 0.437
Iter 1981/2000 - Loss: -7.997
Iter 1982/2000 - Loss: -7.997
Iter 1983/2000 - Loss: -7.997
Iter 1984/2000 - Loss: -7.997
Iter 1985/2000 - Loss: -7.997
Iter 1986/2000 - Loss: -7.997
Iter 1987/2000 - Loss: -7.998
Iter 1988/2000 - Loss: -7.998
Iter 1989/2000 - Loss: -7.998
Iter 1990/2000 - Loss: -7.998
Iter 1991/2000 - Loss: -7.998
Iter 1992/2000 - Loss: -7.998
Iter 1993/2000 - Loss: -7.998
Iter 1994/2000 - Loss: -7.998
Iter 1995/2000 - Loss: -7.998
Iter 1996/2000 - Loss: -7.998
Iter 1997/2000 - Loss: -7.998
Iter 1998/2000 - Loss: -7.998
Iter 1999/2000 - Loss: -7.998
Iter 2000/2000 - Loss: -7.998
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[11.2246,  8.4174, 29.1753,  9.4117,  8.4339, 53.3867]],

        [[17.2377, 33.4030,  7.5485,  1.4148,  3.7570, 24.1818]],

        [[17.5675, 36.0336,  9.3796,  1.0043,  1.2524, 22.6340]],

        [[15.9193, 32.1505, 19.8073,  3.3212,  1.8058, 42.5274]]])
Signal Variance: tensor([ 0.1054,  2.5321, 18.5562,  0.5000])
Estimated target variance: tensor([0.0153, 0.7558, 7.4570, 0.0570])
N: 200
Signal to noise ratio: tensor([ 19.4010,  85.3171, 100.5938,  41.7377])
Bound on condition number: tensor([  75280.8985, 1455802.7871, 2023825.1254,  348408.0447])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0001811755925795927, policy loss: 15.8695241900775
Experience 20, Iter 1, disc loss: 0.0003046971575774547, policy loss: 15.266526284961714
Experience 20, Iter 2, disc loss: 0.00015032375591623285, policy loss: 15.623302450026554
Experience 20, Iter 3, disc loss: 0.00023135465534587625, policy loss: 16.21190620683128
Experience 20, Iter 4, disc loss: 0.00025091567956974763, policy loss: 14.96718662395695
Experience 20, Iter 5, disc loss: 0.0002783500020122012, policy loss: 14.446453346522441
Experience 20, Iter 6, disc loss: 0.00016542705379538953, policy loss: 15.41173956305025
Experience 20, Iter 7, disc loss: 0.0002938666423229351, policy loss: 14.942875360237043
Experience 20, Iter 8, disc loss: 0.00020033030298901268, policy loss: 16.41661874427496
Experience 20, Iter 9, disc loss: 0.00020826262438174178, policy loss: 15.03680594029803
Experience 20, Iter 10, disc loss: 0.0002082734592316167, policy loss: 15.35081915009134
Experience 20, Iter 11, disc loss: 0.0001828249694237606, policy loss: 15.454113619130077
Experience 20, Iter 12, disc loss: 0.000202514588102613, policy loss: 14.8501810317155
Experience 20, Iter 13, disc loss: 0.0001479071995994108, policy loss: 15.529070454696406
Experience 20, Iter 14, disc loss: 0.00016264673436102884, policy loss: 15.699281159871223
Experience 20, Iter 15, disc loss: 0.00021374657972584156, policy loss: 14.682078487087296
Experience 20, Iter 16, disc loss: 0.0004801692437001413, policy loss: 14.712103040174705
Experience 20, Iter 17, disc loss: 0.0003628716837094248, policy loss: 15.02110573956575
Experience 20, Iter 18, disc loss: 0.00017188057865791264, policy loss: 15.953290226892197
Experience 20, Iter 19, disc loss: 0.00017206514010077815, policy loss: 15.942085215277658
Experience 20, Iter 20, disc loss: 0.00029620292603836987, policy loss: 15.182536473136317
Experience 20, Iter 21, disc loss: 0.00017403999512408222, policy loss: 15.911222487279938
Experience 20, Iter 22, disc loss: 0.0005056483800445309, policy loss: 14.223708030980623
Experience 20, Iter 23, disc loss: 0.0003135042709813969, policy loss: 14.715385583693376
Experience 20, Iter 24, disc loss: 0.00026142483939691136, policy loss: 15.484706204519993
Experience 20, Iter 25, disc loss: 0.0002422100742808543, policy loss: 15.274096904339224
Experience 20, Iter 26, disc loss: 0.00015682222823161084, policy loss: 15.15463308886494
Experience 20, Iter 27, disc loss: 0.00023314371035018566, policy loss: 15.368131070612334
Experience 20, Iter 28, disc loss: 0.00033040271414784653, policy loss: 13.917277303028957
Experience 20, Iter 29, disc loss: 0.00017309437813540064, policy loss: 15.544384289452232
Experience 20, Iter 30, disc loss: 0.0002715858694338684, policy loss: 14.850867896533709
Experience 20, Iter 31, disc loss: 0.0005356864323689836, policy loss: 13.779891336188523
Experience 20, Iter 32, disc loss: 0.00014583411727947494, policy loss: 15.811366290793561
Experience 20, Iter 33, disc loss: 0.00021549366962035112, policy loss: 14.281279154426308
Experience 20, Iter 34, disc loss: 0.00025162823711767017, policy loss: 15.50419365816194
Experience 20, Iter 35, disc loss: 0.00022821907806189535, policy loss: 16.258778534611583
Experience 20, Iter 36, disc loss: 0.00023410470203894725, policy loss: 15.135571028683202
Experience 20, Iter 37, disc loss: 0.00017074933866204013, policy loss: 14.834588694978601
Experience 20, Iter 38, disc loss: 0.00018173311867431834, policy loss: 16.51229118954448
Experience 20, Iter 39, disc loss: 0.0002470981261163826, policy loss: 15.070767193531431
Experience 20, Iter 40, disc loss: 0.000162483688153259, policy loss: 15.189956621349241
Experience 20, Iter 41, disc loss: 0.000173522789587002, policy loss: 15.861835822843881
Experience 20, Iter 42, disc loss: 0.00020173324352734169, policy loss: 16.47962370933542
Experience 20, Iter 43, disc loss: 0.00016023291498814223, policy loss: 15.059121386087206
Experience 20, Iter 44, disc loss: 0.00017240642568163247, policy loss: 14.896124776462958
Experience 20, Iter 45, disc loss: 0.00019984801845268407, policy loss: 15.485228383059619
Experience 20, Iter 46, disc loss: 0.0004502920108647168, policy loss: 15.446077187709264
Experience 20, Iter 47, disc loss: 0.00018280295394224304, policy loss: 14.076527728087175
Experience 20, Iter 48, disc loss: 0.00016961633588201343, policy loss: 15.153694066862181
Experience 20, Iter 49, disc loss: 0.000399016248275693, policy loss: 14.814776935387059
Experience 20, Iter 50, disc loss: 0.00027888644432481804, policy loss: 14.542637150222063
Experience 20, Iter 51, disc loss: 0.00019055525986755236, policy loss: 15.73725429576633
Experience 20, Iter 52, disc loss: 0.00014290799676923774, policy loss: 15.883467852306737
Experience 20, Iter 53, disc loss: 0.00015115749963919502, policy loss: 16.321735115097336
Experience 20, Iter 54, disc loss: 0.00028549662262845557, policy loss: 15.053727623817874
Experience 20, Iter 55, disc loss: 0.00021475206806147447, policy loss: 14.544480038051349
Experience 20, Iter 56, disc loss: 0.00014921394666986813, policy loss: 15.215418213751349
Experience 20, Iter 57, disc loss: 0.0001647391615606238, policy loss: 14.764429136364928
Experience 20, Iter 58, disc loss: 0.0002282820555609468, policy loss: 15.408611986249962
Experience 20, Iter 59, disc loss: 0.0002110575312637529, policy loss: 15.35091079629858
Experience 20, Iter 60, disc loss: 0.0002944653949147326, policy loss: 15.089066707267527
Experience 20, Iter 61, disc loss: 0.00048798676688263177, policy loss: 14.060546034713138
Experience 20, Iter 62, disc loss: 0.0002458941968201357, policy loss: 14.62296524549445
Experience 20, Iter 63, disc loss: 0.00020699238447263152, policy loss: 15.673076830218278
Experience 20, Iter 64, disc loss: 0.00027007566562715663, policy loss: 14.124707656340892
Experience 20, Iter 65, disc loss: 0.0003820404239476587, policy loss: 15.194683848815657
Experience 20, Iter 66, disc loss: 0.00016270224567200526, policy loss: 14.681225586981771
Experience 20, Iter 67, disc loss: 0.00016809736277079428, policy loss: 14.497024122764566
Experience 20, Iter 68, disc loss: 0.00022720829616447747, policy loss: 14.716303145401326
Experience 20, Iter 69, disc loss: 0.00023696431848982936, policy loss: 14.798779057399845
Experience 20, Iter 70, disc loss: 0.00026533387204123954, policy loss: 15.53593168004473
Experience 20, Iter 71, disc loss: 0.0001575968082698668, policy loss: 15.120003799765549
Experience 20, Iter 72, disc loss: 0.00015608362155445643, policy loss: 15.853740567625414
Experience 20, Iter 73, disc loss: 0.00017218670327750268, policy loss: 15.598474277602046
Experience 20, Iter 74, disc loss: 0.00042937652043495536, policy loss: 16.312091722556502
Experience 20, Iter 75, disc loss: 0.00015805696127753352, policy loss: 14.568999214436555
Experience 20, Iter 76, disc loss: 0.0005365377200912348, policy loss: 14.659102527763391
Experience 20, Iter 77, disc loss: 0.00030182423710950257, policy loss: 14.173017107592901
Experience 20, Iter 78, disc loss: 0.00035139457390260447, policy loss: 13.872078100091585
Experience 20, Iter 79, disc loss: 0.0003597057045731928, policy loss: 14.484236797535498
Experience 20, Iter 80, disc loss: 0.0005511843993347907, policy loss: 14.148976069249938
Experience 20, Iter 81, disc loss: 0.00031779236410879504, policy loss: 14.37306858452925
Experience 20, Iter 82, disc loss: 0.0002451487303943786, policy loss: 13.60995534698595
Experience 20, Iter 83, disc loss: 0.000256796532882623, policy loss: 15.304818696290772
Experience 20, Iter 84, disc loss: 0.00019671058280461545, policy loss: 15.225765531286175
Experience 20, Iter 85, disc loss: 0.00015550414524899468, policy loss: 15.631436803204961
Experience 20, Iter 86, disc loss: 0.00032726771327948345, policy loss: 14.919524296456151
Experience 20, Iter 87, disc loss: 0.00040265080837500577, policy loss: 15.353768573522103
Experience 20, Iter 88, disc loss: 0.00017544017392831968, policy loss: 15.039970736911847
Experience 20, Iter 89, disc loss: 0.0002771273172898668, policy loss: 14.624324961541392
Experience 20, Iter 90, disc loss: 0.00016528934340477152, policy loss: 13.679332292564498
Experience 20, Iter 91, disc loss: 0.0002051462652292346, policy loss: 14.873024525588606
Experience 20, Iter 92, disc loss: 0.0004966687929395932, policy loss: 14.112340108973777
Experience 20, Iter 93, disc loss: 0.00015825260369451728, policy loss: 14.509345233920524
Experience 20, Iter 94, disc loss: 0.00018168588178294013, policy loss: 14.9122951172875
Experience 20, Iter 95, disc loss: 0.0001696926897294585, policy loss: 15.772178344949022
Experience 20, Iter 96, disc loss: 0.0001971786306413628, policy loss: 14.98152757261083
Experience 20, Iter 97, disc loss: 0.0002777285631707069, policy loss: 14.3521783586915
Experience 20, Iter 98, disc loss: 0.00017980141461316723, policy loss: 14.212905963839955
Experience 20, Iter 99, disc loss: 0.00027657274780602527, policy loss: 15.56312424007527
