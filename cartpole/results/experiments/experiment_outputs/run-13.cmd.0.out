Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0007],
        [0.0096],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0023, 0.0029, 0.0385, 0.0007], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0023, 0.0029, 0.0385, 0.0007])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.278
Iter 2/2000 - Loss: 0.097
Iter 3/2000 - Loss: -4.482
Iter 4/2000 - Loss: -3.189
Iter 5/2000 - Loss: -1.558
Iter 6/2000 - Loss: -2.514
Iter 7/2000 - Loss: -4.150
Iter 8/2000 - Loss: -4.980
Iter 9/2000 - Loss: -4.857
Iter 10/2000 - Loss: -4.364
Iter 11/2000 - Loss: -4.071
Iter 12/2000 - Loss: -4.138
Iter 13/2000 - Loss: -4.409
Iter 14/2000 - Loss: -4.692
Iter 15/2000 - Loss: -4.881
Iter 16/2000 - Loss: -4.949
Iter 17/2000 - Loss: -4.924
Iter 18/2000 - Loss: -4.858
Iter 19/2000 - Loss: -4.812
Iter 20/2000 - Loss: -4.834
Iter 1981/2000 - Loss: -5.745
Iter 1982/2000 - Loss: -5.742
Iter 1983/2000 - Loss: -5.740
Iter 1984/2000 - Loss: -5.738
Iter 1985/2000 - Loss: -5.739
Iter 1986/2000 - Loss: -5.742
Iter 1987/2000 - Loss: -5.746
Iter 1988/2000 - Loss: -5.748
Iter 1989/2000 - Loss: -5.747
Iter 1990/2000 - Loss: -5.742
Iter 1991/2000 - Loss: -5.734
Iter 1992/2000 - Loss: -5.723
Iter 1993/2000 - Loss: -5.708
Iter 1994/2000 - Loss: -5.690
Iter 1995/2000 - Loss: -5.679
Iter 1996/2000 - Loss: -5.686
Iter 1997/2000 - Loss: -5.712
Iter 1998/2000 - Loss: -5.737
Iter 1999/2000 - Loss: -5.741
Iter 2000/2000 - Loss: -5.731
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0069],
        [0.0001]])
Lengthscale: tensor([[[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]],

        [[5.8411e-03, 2.3174e-02, 6.7898e-03, 1.5560e-04, 3.3221e-06,
          5.5849e-03]]])
Signal Variance: tensor([0.0017, 0.0021, 0.0278, 0.0005])
Estimated target variance: tensor([0.0023, 0.0029, 0.0385, 0.0007])
N: 10
Signal to noise ratio: tensor([2.0003, 2.0007, 2.0106, 1.9988])
Bound on condition number: tensor([41.0136, 41.0287, 41.4235, 40.9503])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2793775557281528, policy loss: 0.8145309623958518
Experience 1, Iter 1, disc loss: 1.268326924469405, policy loss: 0.8176478832536183
Experience 1, Iter 2, disc loss: 1.2605429816028466, policy loss: 0.8170889493479698
Experience 1, Iter 3, disc loss: 1.2505843901770526, policy loss: 0.8189693441342552
Experience 1, Iter 4, disc loss: 1.2394769679470143, policy loss: 0.8223031719705985
Experience 1, Iter 5, disc loss: 1.2303080871067966, policy loss: 0.8230066760251846
Experience 1, Iter 6, disc loss: 1.2200649910825256, policy loss: 0.8251471619308673
Experience 1, Iter 7, disc loss: 1.2124548261330865, policy loss: 0.8241975446067131
Experience 1, Iter 8, disc loss: 1.2021208389733826, policy loss: 0.8263485071329073
Experience 1, Iter 9, disc loss: 1.1922838159218399, policy loss: 0.827820739541
Experience 1, Iter 10, disc loss: 1.1833054129381266, policy loss: 0.8282797577413071
Experience 1, Iter 11, disc loss: 1.1741332185396376, policy loss: 0.8291453891411931
Experience 1, Iter 12, disc loss: 1.1654965632281857, policy loss: 0.8294834315781681
Experience 1, Iter 13, disc loss: 1.1563604459465993, policy loss: 0.8302655677176298
Experience 1, Iter 14, disc loss: 1.1471624100779492, policy loss: 0.8311148865509994
Experience 1, Iter 15, disc loss: 1.1386613659016986, policy loss: 0.8313076304116216
Experience 1, Iter 16, disc loss: 1.129281636766183, policy loss: 0.8327071369586776
Experience 1, Iter 17, disc loss: 1.120510151392339, policy loss: 0.8334346196240632
Experience 1, Iter 18, disc loss: 1.1125429128843032, policy loss: 0.8331726102329036
Experience 1, Iter 19, disc loss: 1.1032288690093808, policy loss: 0.8347923094406249
Experience 1, Iter 20, disc loss: 1.0943294271995065, policy loss: 0.8359321414240044
Experience 1, Iter 21, disc loss: 1.0857007293646088, policy loss: 0.8367259508227689
Experience 1, Iter 22, disc loss: 1.0773422978297376, policy loss: 0.8370970127147548
Experience 1, Iter 23, disc loss: 1.0676804367098205, policy loss: 0.8389565653013273
Experience 1, Iter 24, disc loss: 1.0574658603662197, policy loss: 0.8412158981776523
Experience 1, Iter 25, disc loss: 1.046963236994395, policy loss: 0.8425943913839161
Experience 1, Iter 26, disc loss: 1.0347127955296167, policy loss: 0.8444555100514605
Experience 1, Iter 27, disc loss: 1.0217786155252118, policy loss: 0.8456370854292807
Experience 1, Iter 28, disc loss: 1.0076178080170966, policy loss: 0.8479922212031332
Experience 1, Iter 29, disc loss: 0.9927106921083522, policy loss: 0.8510096293263439
Experience 1, Iter 30, disc loss: 0.9789223147603476, policy loss: 0.8522419315375407
Experience 1, Iter 31, disc loss: 0.9651661628579811, policy loss: 0.8534374948790677
Experience 1, Iter 32, disc loss: 0.9507644119833006, policy loss: 0.8553403576248146
Experience 1, Iter 33, disc loss: 0.9355685208749251, policy loss: 0.8586248216847339
Experience 1, Iter 34, disc loss: 0.9202251425951077, policy loss: 0.8622027845408762
Experience 1, Iter 35, disc loss: 0.9060498268994567, policy loss: 0.8642864487685932
Experience 1, Iter 36, disc loss: 0.8904871069934888, policy loss: 0.8686475261041966
Experience 1, Iter 37, disc loss: 0.8804436512226753, policy loss: 0.8653002470936484
Experience 1, Iter 38, disc loss: 0.8637401059186858, policy loss: 0.8716827180771075
Experience 1, Iter 39, disc loss: 0.8499980491144046, policy loss: 0.87413551571739
Experience 1, Iter 40, disc loss: 0.8363262065947693, policy loss: 0.8766623049596778
Experience 1, Iter 41, disc loss: 0.8204062678028472, policy loss: 0.8829650938301474
Experience 1, Iter 42, disc loss: 0.8069570836378674, policy loss: 0.8859036335353501
Experience 1, Iter 43, disc loss: 0.7946427789665053, policy loss: 0.8875477301933479
Experience 1, Iter 44, disc loss: 0.7814096491544346, policy loss: 0.8911250289810506
Experience 1, Iter 45, disc loss: 0.7705713904940943, policy loss: 0.8913978795465289
Experience 1, Iter 46, disc loss: 0.7571760759465476, policy loss: 0.8959449948847522
Experience 1, Iter 47, disc loss: 0.7481494007880493, policy loss: 0.8949096342399872
Experience 1, Iter 48, disc loss: 0.7334895658564792, policy loss: 0.9023789398098314
Experience 1, Iter 49, disc loss: 0.7245951249259632, policy loss: 0.901935503173246
Experience 1, Iter 50, disc loss: 0.7102143747725882, policy loss: 0.9101146296774587
Experience 1, Iter 51, disc loss: 0.7013385061393842, policy loss: 0.9106059825529237
Experience 1, Iter 52, disc loss: 0.6891132713160424, policy loss: 0.9165927536149596
Experience 1, Iter 53, disc loss: 0.6797448770427819, policy loss: 0.9187441091756738
Experience 1, Iter 54, disc loss: 0.6665463657557951, policy loss: 0.9275617589678992
Experience 1, Iter 55, disc loss: 0.657262260466268, policy loss: 0.93090828167011
Experience 1, Iter 56, disc loss: 0.6461416296778999, policy loss: 0.9374688425201444
Experience 1, Iter 57, disc loss: 0.6397715568198119, policy loss: 0.9372583155372634
Experience 1, Iter 58, disc loss: 0.6304547735493498, policy loss: 0.9419452294968091
Experience 1, Iter 59, disc loss: 0.6198628421460282, policy loss: 0.9496014936508556
Experience 1, Iter 60, disc loss: 0.614013970318547, policy loss: 0.9501106670580757
Experience 1, Iter 61, disc loss: 0.599289773508064, policy loss: 0.9652479817872995
Experience 1, Iter 62, disc loss: 0.5931864898978978, policy loss: 0.9672246341055416
Experience 1, Iter 63, disc loss: 0.5846011334213995, policy loss: 0.9732854347052801
Experience 1, Iter 64, disc loss: 0.5740976742686574, policy loss: 0.9838001840923254
Experience 1, Iter 65, disc loss: 0.5648390072490094, policy loss: 0.992354893682206
Experience 1, Iter 66, disc loss: 0.5531787694042445, policy loss: 1.0039780360358068
Experience 1, Iter 67, disc loss: 0.5441212841587934, policy loss: 1.0128337810402228
Experience 1, Iter 68, disc loss: 0.535944313263478, policy loss: 1.0186678739485897
Experience 1, Iter 69, disc loss: 0.5192442088510711, policy loss: 1.0423484284030853
Experience 1, Iter 70, disc loss: 0.5089502283161724, policy loss: 1.054325785075642
Experience 1, Iter 71, disc loss: 0.5020377549133267, policy loss: 1.060476177551304
Experience 1, Iter 72, disc loss: 0.4908275290436014, policy loss: 1.0746368637425534
Experience 1, Iter 73, disc loss: 0.47671513887938916, policy loss: 1.0962295004863618
Experience 1, Iter 74, disc loss: 0.4632759507187681, policy loss: 1.1168012694621883
Experience 1, Iter 75, disc loss: 0.45145354336580257, policy loss: 1.1357782310012339
Experience 1, Iter 76, disc loss: 0.4371873896083601, policy loss: 1.1593678103482423
Experience 1, Iter 77, disc loss: 0.4252963224823675, policy loss: 1.180843584734165
Experience 1, Iter 78, disc loss: 0.41367718888291133, policy loss: 1.2018539117934013
Experience 1, Iter 79, disc loss: 0.4005976029945014, policy loss: 1.2268815948447949
Experience 1, Iter 80, disc loss: 0.3947440414943724, policy loss: 1.2341925459829244
Experience 1, Iter 81, disc loss: 0.3804415917685431, policy loss: 1.2652450224366902
Experience 1, Iter 82, disc loss: 0.3663825430443859, policy loss: 1.2966123316965525
Experience 1, Iter 83, disc loss: 0.3587867124995611, policy loss: 1.3120907329328804
Experience 1, Iter 84, disc loss: 0.34369185427058946, policy loss: 1.3487259179095805
Experience 1, Iter 85, disc loss: 0.3228762431303531, policy loss: 1.4055604858748416
Experience 1, Iter 86, disc loss: 0.3193776098024377, policy loss: 1.4121335742417434
Experience 1, Iter 87, disc loss: 0.30334298931798365, policy loss: 1.4577053435683245
Experience 1, Iter 88, disc loss: 0.29362723643828886, policy loss: 1.4864158103559646
Experience 1, Iter 89, disc loss: 0.2814621946726217, policy loss: 1.5231724095023114
Experience 1, Iter 90, disc loss: 0.27198116449112564, policy loss: 1.5531287322122225
Experience 1, Iter 91, disc loss: 0.24990242583613348, policy loss: 1.6325507275613216
Experience 1, Iter 92, disc loss: 0.25790248903277085, policy loss: 1.6001526048464298
Experience 1, Iter 93, disc loss: 0.24782501240062466, policy loss: 1.634294261139915
Experience 1, Iter 94, disc loss: 0.23651412411964798, policy loss: 1.6789021792617547
Experience 1, Iter 95, disc loss: 0.2232198916975367, policy loss: 1.735953929275968
Experience 1, Iter 96, disc loss: 0.2164034267700104, policy loss: 1.760766454706129
Experience 1, Iter 97, disc loss: 0.2085787370656767, policy loss: 1.7986199356439747
Experience 1, Iter 98, disc loss: 0.20154794585041563, policy loss: 1.8265945818354932
Experience 1, Iter 99, disc loss: 0.18958346795540465, policy loss: 1.8874859351005533
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0009],
        [0.0140],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.0034, 0.0560, 0.0008], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.0034, 0.0560, 0.0008])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.188
Iter 2/2000 - Loss: -0.550
Iter 3/2000 - Loss: -4.138
Iter 4/2000 - Loss: -2.969
Iter 5/2000 - Loss: -1.714
Iter 6/2000 - Loss: -2.620
Iter 7/2000 - Loss: -3.991
Iter 8/2000 - Loss: -4.592
Iter 9/2000 - Loss: -4.390
Iter 10/2000 - Loss: -3.959
Iter 11/2000 - Loss: -3.779
Iter 12/2000 - Loss: -3.919
Iter 13/2000 - Loss: -4.184
Iter 14/2000 - Loss: -4.383
Iter 15/2000 - Loss: -4.456
Iter 16/2000 - Loss: -4.449
Iter 17/2000 - Loss: -4.431
Iter 18/2000 - Loss: -4.441
Iter 19/2000 - Loss: -4.477
Iter 20/2000 - Loss: -4.519
Iter 1981/2000 - Loss: -5.214
Iter 1982/2000 - Loss: -5.213
Iter 1983/2000 - Loss: -5.216
Iter 1984/2000 - Loss: -5.222
Iter 1985/2000 - Loss: -5.229
Iter 1986/2000 - Loss: -5.234
Iter 1987/2000 - Loss: -5.235
Iter 1988/2000 - Loss: -5.232
Iter 1989/2000 - Loss: -5.229
Iter 1990/2000 - Loss: -5.226
Iter 1991/2000 - Loss: -5.225
Iter 1992/2000 - Loss: -5.226
Iter 1993/2000 - Loss: -5.227
Iter 1994/2000 - Loss: -5.229
Iter 1995/2000 - Loss: -5.230
Iter 1996/2000 - Loss: -5.232
Iter 1997/2000 - Loss: -5.233
Iter 1998/2000 - Loss: -5.233
Iter 1999/2000 - Loss: -5.234
Iter 2000/2000 - Loss: -5.234
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0105],
        [0.0002]])
Lengthscale: tensor([[[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]],

        [[6.0841e-03, 2.2323e-02, 9.2483e-03, 2.1802e-04, 4.3944e-06,
          7.7062e-03]]])
Signal Variance: tensor([0.0020, 0.0026, 0.0427, 0.0006])
Estimated target variance: tensor([0.0026, 0.0034, 0.0560, 0.0008])
N: 20
Signal to noise ratio: tensor([2.0007, 2.0011, 2.0139, 1.9989])
Bound on condition number: tensor([81.0532, 81.0895, 82.1197, 80.9116])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.18893875503256033, policy loss: 1.8870507514817403
Experience 2, Iter 1, disc loss: 0.17550260003103213, policy loss: 1.9617038801843225
Experience 2, Iter 2, disc loss: 0.17103057679038478, policy loss: 1.9890284176281157
Experience 2, Iter 3, disc loss: 0.1630314572553258, policy loss: 2.027071918317247
Experience 2, Iter 4, disc loss: 0.16235855164141041, policy loss: 2.0342951070638904
Experience 2, Iter 5, disc loss: 0.149301319644802, policy loss: 2.110889872780258
Experience 2, Iter 6, disc loss: 0.14119139871206215, policy loss: 2.1743196583539977
Experience 2, Iter 7, disc loss: 0.13706368739349398, policy loss: 2.2004096992140587
Experience 2, Iter 8, disc loss: 0.13204016802882831, policy loss: 2.245880046887813
Experience 2, Iter 9, disc loss: 0.12547217127739746, policy loss: 2.287163121570579
Experience 2, Iter 10, disc loss: 0.11978581147209821, policy loss: 2.3233082008513763
Experience 2, Iter 11, disc loss: 0.11894651357958978, policy loss: 2.3366809218856877
Experience 2, Iter 12, disc loss: 0.10630365234176782, policy loss: 2.449670395905796
Experience 2, Iter 13, disc loss: 0.10863233058623051, policy loss: 2.4214311824403953
Experience 2, Iter 14, disc loss: 0.10159539525276169, policy loss: 2.4865624535354867
Experience 2, Iter 15, disc loss: 0.09814220910820204, policy loss: 2.529195527716708
Experience 2, Iter 16, disc loss: 0.09118488801831388, policy loss: 2.6098782658639204
Experience 2, Iter 17, disc loss: 0.089789963113128, policy loss: 2.6306704190115626
Experience 2, Iter 18, disc loss: 0.08811682792641966, policy loss: 2.643450682375625
Experience 2, Iter 19, disc loss: 0.0863297518913143, policy loss: 2.6531403222326606
Experience 2, Iter 20, disc loss: 0.07820322725816746, policy loss: 2.7615993348399286
Experience 2, Iter 21, disc loss: 0.07611602232997984, policy loss: 2.7957749557870235
Experience 2, Iter 22, disc loss: 0.07467901141555341, policy loss: 2.8060026604733106
Experience 2, Iter 23, disc loss: 0.07208827756024003, policy loss: 2.8481438651564965
Experience 2, Iter 24, disc loss: 0.06826800759244822, policy loss: 2.887295013863844
Experience 2, Iter 25, disc loss: 0.06752286008000778, policy loss: 2.913691390379358
Experience 2, Iter 26, disc loss: 0.06321489889098417, policy loss: 2.968332807120113
Experience 2, Iter 27, disc loss: 0.060841671402231116, policy loss: 3.0051431452546007
Experience 2, Iter 28, disc loss: 0.05599357390769586, policy loss: 3.0923172387680244
Experience 2, Iter 29, disc loss: 0.05558854080442696, policy loss: 3.0951172203357693
Experience 2, Iter 30, disc loss: 0.05320563930602097, policy loss: 3.1479891205404584
Experience 2, Iter 31, disc loss: 0.051329580941598736, policy loss: 3.1848426085096064
Experience 2, Iter 32, disc loss: 0.04841321674422906, policy loss: 3.2541368346941777
Experience 2, Iter 33, disc loss: 0.04450262596159546, policy loss: 3.3332874918372046
Experience 2, Iter 34, disc loss: 0.04614251117159889, policy loss: 3.2831812188626124
Experience 2, Iter 35, disc loss: 0.04194313596599575, policy loss: 3.390605202087542
Experience 2, Iter 36, disc loss: 0.04302789632966916, policy loss: 3.360017648355467
Experience 2, Iter 37, disc loss: 0.04239273332779211, policy loss: 3.384408216555327
Experience 2, Iter 38, disc loss: 0.036553489364958745, policy loss: 3.5412207492976173
Experience 2, Iter 39, disc loss: 0.03748121010021004, policy loss: 3.5142677591193143
Experience 2, Iter 40, disc loss: 0.03739741573600392, policy loss: 3.5154876875285845
Experience 2, Iter 41, disc loss: 0.03640781011476809, policy loss: 3.531913046558056
Experience 2, Iter 42, disc loss: 0.034423872457620124, policy loss: 3.609801917092798
Experience 2, Iter 43, disc loss: 0.034241821785926725, policy loss: 3.5955383851445673
Experience 2, Iter 44, disc loss: 0.034418814880517956, policy loss: 3.5940415323922945
Experience 2, Iter 45, disc loss: 0.0306486474756731, policy loss: 3.7264479041626264
Experience 2, Iter 46, disc loss: 0.02928033929286502, policy loss: 3.760380384683221
Experience 2, Iter 47, disc loss: 0.02872638282757355, policy loss: 3.788597937109557
Experience 2, Iter 48, disc loss: 0.02703337408960533, policy loss: 3.8406573970837194
Experience 2, Iter 49, disc loss: 0.026685592666309606, policy loss: 3.883902247589732
Experience 2, Iter 50, disc loss: 0.026373635799384308, policy loss: 3.8800908988292466
Experience 2, Iter 51, disc loss: 0.024925431710093638, policy loss: 3.9377970455741007
Experience 2, Iter 52, disc loss: 0.0248230954428063, policy loss: 3.944101040554555
Experience 2, Iter 53, disc loss: 0.024462639494371444, policy loss: 3.9589023680648916
Experience 2, Iter 54, disc loss: 0.022721144620193964, policy loss: 4.037319778259429
Experience 2, Iter 55, disc loss: 0.02233737047438086, policy loss: 4.057861471365568
Experience 2, Iter 56, disc loss: 0.022068504290433163, policy loss: 4.069449792982754
Experience 2, Iter 57, disc loss: 0.02156156852435919, policy loss: 4.092209484212454
Experience 2, Iter 58, disc loss: 0.02050252917583663, policy loss: 4.161584469680586
Experience 2, Iter 59, disc loss: 0.021109810230771707, policy loss: 4.118385086344318
Experience 2, Iter 60, disc loss: 0.019165676473781097, policy loss: 4.2193897487067265
Experience 2, Iter 61, disc loss: 0.018824803385043484, policy loss: 4.2540538342809056
Experience 2, Iter 62, disc loss: 0.01868594068020485, policy loss: 4.248552573419893
Experience 2, Iter 63, disc loss: 0.01779876786741513, policy loss: 4.3149115420831645
Experience 2, Iter 64, disc loss: 0.01734320128545231, policy loss: 4.351337707297203
Experience 2, Iter 65, disc loss: 0.01718961329715053, policy loss: 4.340410723986962
Experience 2, Iter 66, disc loss: 0.016012923777585, policy loss: 4.429903341142688
Experience 2, Iter 67, disc loss: 0.016368530655686858, policy loss: 4.399315408068799
Experience 2, Iter 68, disc loss: 0.016051930734586307, policy loss: 4.420069035265669
Experience 2, Iter 69, disc loss: 0.015802656704269082, policy loss: 4.423271817491742
Experience 2, Iter 70, disc loss: 0.014572504804416677, policy loss: 4.528140068500142
Experience 2, Iter 71, disc loss: 0.013518649437315074, policy loss: 4.604900989340554
Experience 2, Iter 72, disc loss: 0.014151282758188756, policy loss: 4.572827556680875
Experience 2, Iter 73, disc loss: 0.013360580851610213, policy loss: 4.640611129892222
Experience 2, Iter 74, disc loss: 0.014373078297676164, policy loss: 4.545328327521718
Experience 2, Iter 75, disc loss: 0.012635775062673705, policy loss: 4.704955711729129
Experience 2, Iter 76, disc loss: 0.013099206955364552, policy loss: 4.637463234031532
Experience 2, Iter 77, disc loss: 0.013211676470874104, policy loss: 4.617515893114984
Experience 2, Iter 78, disc loss: 0.01239909414481827, policy loss: 4.691788211558597
Experience 2, Iter 79, disc loss: 0.013012142624415473, policy loss: 4.652022070954054
Experience 2, Iter 80, disc loss: 0.012295379658341144, policy loss: 4.713315539761708
Experience 2, Iter 81, disc loss: 0.011629546367666645, policy loss: 4.7629688818436335
Experience 2, Iter 82, disc loss: 0.010595291818494413, policy loss: 4.906948182044648
Experience 2, Iter 83, disc loss: 0.011185860732131053, policy loss: 4.797366715745996
Experience 2, Iter 84, disc loss: 0.011530304095780711, policy loss: 4.766716312432406
Experience 2, Iter 85, disc loss: 0.010526809535846442, policy loss: 4.887003431890833
Experience 2, Iter 86, disc loss: 0.010797915684971439, policy loss: 4.8830421590322555
Experience 2, Iter 87, disc loss: 0.009850617244045852, policy loss: 4.950505276302424
Experience 2, Iter 88, disc loss: 0.011055805837803231, policy loss: 4.820587063492313
Experience 2, Iter 89, disc loss: 0.010121645418480738, policy loss: 4.934582974260659
Experience 2, Iter 90, disc loss: 0.009680375480519943, policy loss: 4.967926451917308
Experience 2, Iter 91, disc loss: 0.009918110375092591, policy loss: 4.955930856721844
Experience 2, Iter 92, disc loss: 0.009884391989493816, policy loss: 4.965311350347605
Experience 2, Iter 93, disc loss: 0.008818103700398287, policy loss: 5.079284502156987
Experience 2, Iter 94, disc loss: 0.008967741660663695, policy loss: 5.0539009860906825
Experience 2, Iter 95, disc loss: 0.009612720299453982, policy loss: 4.977828971787877
Experience 2, Iter 96, disc loss: 0.008463426892719771, policy loss: 5.1363850176536285
Experience 2, Iter 97, disc loss: 0.008135458691459955, policy loss: 5.184769984198786
Experience 2, Iter 98, disc loss: 0.008509292992657631, policy loss: 5.154422163264703
Experience 2, Iter 99, disc loss: 0.008259825311270943, policy loss: 5.1955673434078
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0030],
        [0.0375],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0050, 0.0121, 0.1501, 0.0018], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0050, 0.0121, 0.1501, 0.0018])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.022
Iter 2/2000 - Loss: -1.291
Iter 3/2000 - Loss: -2.862
Iter 4/2000 - Loss: -2.327
Iter 5/2000 - Loss: -1.755
Iter 6/2000 - Loss: -2.116
Iter 7/2000 - Loss: -2.672
Iter 8/2000 - Loss: -2.865
Iter 9/2000 - Loss: -2.719
Iter 10/2000 - Loss: -2.556
Iter 11/2000 - Loss: -2.588
Iter 12/2000 - Loss: -2.768
Iter 13/2000 - Loss: -2.930
Iter 14/2000 - Loss: -2.963
Iter 15/2000 - Loss: -2.876
Iter 16/2000 - Loss: -2.773
Iter 17/2000 - Loss: -2.761
Iter 18/2000 - Loss: -2.869
Iter 19/2000 - Loss: -3.030
Iter 20/2000 - Loss: -3.140
Iter 1981/2000 - Loss: -3.348
Iter 1982/2000 - Loss: -3.348
Iter 1983/2000 - Loss: -3.348
Iter 1984/2000 - Loss: -3.348
Iter 1985/2000 - Loss: -3.348
Iter 1986/2000 - Loss: -3.348
Iter 1987/2000 - Loss: -3.348
Iter 1988/2000 - Loss: -3.348
Iter 1989/2000 - Loss: -3.348
Iter 1990/2000 - Loss: -3.348
Iter 1991/2000 - Loss: -3.348
Iter 1992/2000 - Loss: -3.348
Iter 1993/2000 - Loss: -3.348
Iter 1994/2000 - Loss: -3.348
Iter 1995/2000 - Loss: -3.348
Iter 1996/2000 - Loss: -3.348
Iter 1997/2000 - Loss: -3.348
Iter 1998/2000 - Loss: -3.348
Iter 1999/2000 - Loss: -3.348
Iter 2000/2000 - Loss: -3.348
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0023],
        [0.0283],
        [0.0004]])
Lengthscale: tensor([[[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]],

        [[1.0165e-02, 4.5277e-02, 2.2549e-02, 9.1033e-04, 4.6671e-05,
          7.3846e-02]]])
Signal Variance: tensor([0.0039, 0.0094, 0.1168, 0.0014])
Estimated target variance: tensor([0.0050, 0.0121, 0.1501, 0.0018])
N: 30
Signal to noise ratio: tensor([2.0028, 2.0042, 2.0300, 2.0014])
Bound on condition number: tensor([121.3389, 121.5037, 124.6239, 121.1683])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.004778350390589818, policy loss: 5.993733234375859
Experience 3, Iter 1, disc loss: 0.0057621517016609464, policy loss: 5.818529370903314
Experience 3, Iter 2, disc loss: 0.005402242984583553, policy loss: 5.904751715688176
Experience 3, Iter 3, disc loss: 0.005075231239419501, policy loss: 5.914665417150954
Experience 3, Iter 4, disc loss: 0.0052168841416329005, policy loss: 5.841675386780295
Experience 3, Iter 5, disc loss: 0.005075910452286033, policy loss: 5.981764368243535
Experience 3, Iter 6, disc loss: 0.0050399170472769095, policy loss: 6.016486693041014
Experience 3, Iter 7, disc loss: 0.005057727494818842, policy loss: 5.984007251225325
Experience 3, Iter 8, disc loss: 0.004869311086559292, policy loss: 5.940211552263798
Experience 3, Iter 9, disc loss: 0.0047088423579940706, policy loss: 6.0543442721477785
Experience 3, Iter 10, disc loss: 0.005409837930295168, policy loss: 5.9701454574282735
Experience 3, Iter 11, disc loss: 0.004531557653002392, policy loss: 6.1210689176363156
Experience 3, Iter 12, disc loss: 0.004235462457463238, policy loss: 6.1128605734051265
Experience 3, Iter 13, disc loss: 0.00417415871278692, policy loss: 6.20112655798955
Experience 3, Iter 14, disc loss: 0.004258011114139305, policy loss: 6.189390068612196
Experience 3, Iter 15, disc loss: 0.004164549182608935, policy loss: 6.201141553377933
Experience 3, Iter 16, disc loss: 0.003829665232928419, policy loss: 6.313936651360723
Experience 3, Iter 17, disc loss: 0.004455034407700004, policy loss: 6.097382417257645
Experience 3, Iter 18, disc loss: 0.003923737333684972, policy loss: 6.320201573876439
Experience 3, Iter 19, disc loss: 0.005068049347486796, policy loss: 6.197280266174543
Experience 3, Iter 20, disc loss: 0.003529940280306714, policy loss: 6.368935110669164
Experience 3, Iter 21, disc loss: 0.004116493802681874, policy loss: 6.328989504941295
Experience 3, Iter 22, disc loss: 0.0039022958090240236, policy loss: 6.3219676431006215
Experience 3, Iter 23, disc loss: 0.00380474399158722, policy loss: 6.261048918808959
Experience 3, Iter 24, disc loss: 0.003651562916231756, policy loss: 6.327458503391823
Experience 3, Iter 25, disc loss: 0.004237936129225373, policy loss: 6.288453160414964
Experience 3, Iter 26, disc loss: 0.0037812436711548384, policy loss: 6.313414450261401
Experience 3, Iter 27, disc loss: 0.00450886015198786, policy loss: 6.13479320181815
Experience 3, Iter 28, disc loss: 0.004175143096870481, policy loss: 6.248906094250167
Experience 3, Iter 29, disc loss: 0.0037008072628450708, policy loss: 6.3001966912083365
Experience 3, Iter 30, disc loss: 0.003392427814563348, policy loss: 6.49535391782723
Experience 3, Iter 31, disc loss: 0.003955823471662829, policy loss: 6.322343329977116
Experience 3, Iter 32, disc loss: 0.003433841429693698, policy loss: 6.466002474721739
Experience 3, Iter 33, disc loss: 0.0034042997759006384, policy loss: 6.38800664534658
Experience 3, Iter 34, disc loss: 0.003252445128433262, policy loss: 6.565874559675953
Experience 3, Iter 35, disc loss: 0.004134597877831045, policy loss: 6.363656490400375
Experience 3, Iter 36, disc loss: 0.0034822948664124786, policy loss: 6.430454080985377
Experience 3, Iter 37, disc loss: 0.003321218095262983, policy loss: 6.4349583800584345
Experience 3, Iter 38, disc loss: 0.0031201030606335706, policy loss: 6.5138453419733215
Experience 3, Iter 39, disc loss: 0.003073894787407691, policy loss: 6.581869523721238
Experience 3, Iter 40, disc loss: 0.0033762806391303673, policy loss: 6.393072319669551
Experience 3, Iter 41, disc loss: 0.0029098156395711353, policy loss: 6.632990561221489
Experience 3, Iter 42, disc loss: 0.0032433250664981907, policy loss: 6.476495737351815
Experience 3, Iter 43, disc loss: 0.0029990822681997074, policy loss: 6.5525539736804514
Experience 3, Iter 44, disc loss: 0.003556316675039532, policy loss: 6.458358337816867
Experience 3, Iter 45, disc loss: 0.003359871599244473, policy loss: 6.514018890414551
Experience 3, Iter 46, disc loss: 0.0038131681731525926, policy loss: 6.312871193628808
Experience 3, Iter 47, disc loss: 0.004002078405971738, policy loss: 6.27320342692656
Experience 3, Iter 48, disc loss: 0.0028915640180054185, policy loss: 6.652510631254962
Experience 3, Iter 49, disc loss: 0.003507554667109859, policy loss: 6.468034322788291
Experience 3, Iter 50, disc loss: 0.003441198580587705, policy loss: 6.491090410887713
Experience 3, Iter 51, disc loss: 0.0035551815884216904, policy loss: 6.381699683469439
Experience 3, Iter 52, disc loss: 0.0028206814865487937, policy loss: 6.6211016175011
Experience 3, Iter 53, disc loss: 0.003585407995248562, policy loss: 6.403033145784169
Experience 3, Iter 54, disc loss: 0.0027579594610048974, policy loss: 6.714904015886469
Experience 3, Iter 55, disc loss: 0.002617352837242555, policy loss: 6.814175622394404
Experience 3, Iter 56, disc loss: 0.0031728348437135537, policy loss: 6.532096635128524
Experience 3, Iter 57, disc loss: 0.002798223059915128, policy loss: 6.717202198987067
Experience 3, Iter 58, disc loss: 0.0023986442163404977, policy loss: 6.824900088837753
Experience 3, Iter 59, disc loss: 0.002994657922896953, policy loss: 6.59493155397346
Experience 3, Iter 60, disc loss: 0.003059650960536837, policy loss: 6.5166901823949726
Experience 3, Iter 61, disc loss: 0.003243797314864831, policy loss: 6.549192636071975
Experience 3, Iter 62, disc loss: 0.0026202377867386604, policy loss: 6.683359070410983
Experience 3, Iter 63, disc loss: 0.002605837506321273, policy loss: 6.678175926566713
Experience 3, Iter 64, disc loss: 0.003060709368444786, policy loss: 6.671626304877706
Experience 3, Iter 65, disc loss: 0.002743206859649493, policy loss: 6.63157766261981
Experience 3, Iter 66, disc loss: 0.0027576934619631356, policy loss: 6.616972454226376
Experience 3, Iter 67, disc loss: 0.0027001516082100965, policy loss: 6.714076055856461
Experience 3, Iter 68, disc loss: 0.0025954368603690638, policy loss: 6.784674405696964
Experience 3, Iter 69, disc loss: 0.0024663696643599654, policy loss: 6.862015542509385
Experience 3, Iter 70, disc loss: 0.0025086227900555183, policy loss: 6.789974061846801
Experience 3, Iter 71, disc loss: 0.0027137162929762957, policy loss: 6.7188921460556355
Experience 3, Iter 72, disc loss: 0.00278496114672939, policy loss: 6.657935195316746
Experience 3, Iter 73, disc loss: 0.0023910059340829922, policy loss: 6.834329233473683
Experience 3, Iter 74, disc loss: 0.0030464907031345044, policy loss: 6.830761469916994
Experience 3, Iter 75, disc loss: 0.0022667081844809983, policy loss: 6.966695731269643
Experience 3, Iter 76, disc loss: 0.002457589087676723, policy loss: 6.793435116181791
Experience 3, Iter 77, disc loss: 0.0022410628236422835, policy loss: 7.014640514678672
Experience 3, Iter 78, disc loss: 0.002584843277955484, policy loss: 6.771487488218321
Experience 3, Iter 79, disc loss: 0.0024093086262324923, policy loss: 6.899992732123513
Experience 3, Iter 80, disc loss: 0.0020950559892217277, policy loss: 7.064457773441544
Experience 3, Iter 81, disc loss: 0.001898028884459132, policy loss: 7.102021654351711
Experience 3, Iter 82, disc loss: 0.002442434545964677, policy loss: 6.936518037044158
Experience 3, Iter 83, disc loss: 0.0022831487767729615, policy loss: 6.921978824146589
Experience 3, Iter 84, disc loss: 0.002220177696071742, policy loss: 6.873792567771783
Experience 3, Iter 85, disc loss: 0.002290697783309107, policy loss: 6.883988591778012
Experience 3, Iter 86, disc loss: 0.0020711739570806065, policy loss: 6.971506618302179
Experience 3, Iter 87, disc loss: 0.0021526310176692106, policy loss: 6.933427964612674
Experience 3, Iter 88, disc loss: 0.0022593511071210344, policy loss: 6.8819048236303715
Experience 3, Iter 89, disc loss: 0.0024265833761173408, policy loss: 6.75100512823837
Experience 3, Iter 90, disc loss: 0.0022802544750427068, policy loss: 6.8117830157681425
Experience 3, Iter 91, disc loss: 0.0022271093183087215, policy loss: 7.05770336592041
Experience 3, Iter 92, disc loss: 0.0022240787944178926, policy loss: 6.900965192212729
Experience 3, Iter 93, disc loss: 0.0021655143713365105, policy loss: 7.1076883892798195
Experience 3, Iter 94, disc loss: 0.002408603036699179, policy loss: 6.8071705583126505
Experience 3, Iter 95, disc loss: 0.0023867868305321835, policy loss: 6.913409513222964
Experience 3, Iter 96, disc loss: 0.0020631354427305395, policy loss: 6.965984298217038
Experience 3, Iter 97, disc loss: 0.0020908303032564244, policy loss: 7.040042347737371
Experience 3, Iter 98, disc loss: 0.0020874278678429804, policy loss: 7.191849394396553
Experience 3, Iter 99, disc loss: 0.0017354956390829464, policy loss: 7.186951299693501
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.0065],
        [0.0496],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3612e-02, 8.0427e-02, 2.8862e-02, 1.8076e-03, 1.4724e-04,
          2.1191e-01]],

        [[1.3612e-02, 8.0427e-02, 2.8862e-02, 1.8076e-03, 1.4724e-04,
          2.1191e-01]],

        [[1.3612e-02, 8.0427e-02, 2.8862e-02, 1.8076e-03, 1.4724e-04,
          2.1191e-01]],

        [[1.3612e-02, 8.0427e-02, 2.8862e-02, 1.8076e-03, 1.4724e-04,
          2.1191e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0083, 0.0260, 0.1985, 0.0022], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0083, 0.0260, 0.1985, 0.0022])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.547
Iter 2/2000 - Loss: -0.827
Iter 3/2000 - Loss: -2.123
Iter 4/2000 - Loss: -1.827
Iter 5/2000 - Loss: -1.378
Iter 6/2000 - Loss: -1.588
Iter 7/2000 - Loss: -1.945
Iter 8/2000 - Loss: -2.043
Iter 9/2000 - Loss: -1.907
Iter 10/2000 - Loss: -1.791
Iter 11/2000 - Loss: -1.854
Iter 12/2000 - Loss: -2.039
Iter 13/2000 - Loss: -2.200
Iter 14/2000 - Loss: -2.243
Iter 15/2000 - Loss: -2.176
Iter 16/2000 - Loss: -2.087
Iter 17/2000 - Loss: -2.062
Iter 18/2000 - Loss: -2.126
Iter 19/2000 - Loss: -2.230
Iter 20/2000 - Loss: -2.308
Iter 1981/2000 - Loss: -8.279
Iter 1982/2000 - Loss: -8.279
Iter 1983/2000 - Loss: -8.279
Iter 1984/2000 - Loss: -8.279
Iter 1985/2000 - Loss: -8.279
Iter 1986/2000 - Loss: -8.279
Iter 1987/2000 - Loss: -8.279
Iter 1988/2000 - Loss: -8.279
Iter 1989/2000 - Loss: -8.279
Iter 1990/2000 - Loss: -8.279
Iter 1991/2000 - Loss: -8.279
Iter 1992/2000 - Loss: -8.279
Iter 1993/2000 - Loss: -8.279
Iter 1994/2000 - Loss: -8.279
Iter 1995/2000 - Loss: -8.279
Iter 1996/2000 - Loss: -8.279
Iter 1997/2000 - Loss: -8.279
Iter 1998/2000 - Loss: -8.279
Iter 1999/2000 - Loss: -8.279
Iter 2000/2000 - Loss: -8.280
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0002],
        [0.0014],
        [0.0002]])
Lengthscale: tensor([[[ 5.8910,  5.7120, 38.6387, 16.4077,  6.7528, 25.3875]],

        [[27.6292, 49.6520, 14.7520,  1.0722,  4.3764, 10.0810]],

        [[37.0486, 47.2978, 21.3081,  1.5140,  5.7035, 15.6193]],

        [[26.9756, 61.4315,  3.3858,  1.1375,  3.5851, 12.8733]]])
Signal Variance: tensor([ 0.0730,  0.5677, 15.2872,  0.0333])
Estimated target variance: tensor([0.0083, 0.0260, 0.1985, 0.0022])
N: 40
Signal to noise ratio: tensor([ 13.9048,  51.3212, 104.2348,  11.6065])
Bound on condition number: tensor([  7734.7078, 105355.5621, 434596.3907,   5389.4773])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.0007720185844772239, policy loss: 12.287877747041833
Experience 4, Iter 1, disc loss: 0.0007704677979645046, policy loss: 11.807290708556666
Experience 4, Iter 2, disc loss: 0.0007748150093856736, policy loss: 11.124447702542646
Experience 4, Iter 3, disc loss: 0.0007912062961000179, policy loss: 10.388707346699423
Experience 4, Iter 4, disc loss: 0.0008456752167045295, policy loss: 9.370007686920898
Experience 4, Iter 5, disc loss: 0.000968142393477568, policy loss: 8.539523586599875
Experience 4, Iter 6, disc loss: 0.0012067850059877018, policy loss: 7.790299488727445
Experience 4, Iter 7, disc loss: 0.0017753794570312086, policy loss: 7.0610778776670315
Experience 4, Iter 8, disc loss: 0.003237067265487318, policy loss: 6.099825862652603
Experience 4, Iter 9, disc loss: 0.00570458189429601, policy loss: 5.420850063195881
Experience 4, Iter 10, disc loss: 0.01083382843982765, policy loss: 4.704103150191678
Experience 4, Iter 11, disc loss: 0.023509883807045103, policy loss: 4.074979762407024
Experience 4, Iter 12, disc loss: 0.060954188190166965, policy loss: 3.351948045485923
Experience 4, Iter 13, disc loss: 0.08451791151895975, policy loss: 3.2606034641382857
Experience 4, Iter 14, disc loss: 0.09111358528724148, policy loss: 3.650351788070468
Experience 4, Iter 15, disc loss: 0.09465032981061594, policy loss: 3.48946985426986
Experience 4, Iter 16, disc loss: 0.11185322273604122, policy loss: 3.1667979880827435
Experience 4, Iter 17, disc loss: 0.09576002151028852, policy loss: 3.1243798699801535
Experience 4, Iter 18, disc loss: 0.09218819564239257, policy loss: 2.96168311794556
Experience 4, Iter 19, disc loss: 0.08209061530995328, policy loss: 3.3452083267397406
Experience 4, Iter 20, disc loss: 0.06909011459358023, policy loss: 3.1951150472830148
Experience 4, Iter 21, disc loss: 0.05717913929520269, policy loss: 3.3096924458725776
Experience 4, Iter 22, disc loss: 0.07271134324191529, policy loss: 3.334405904949498
Experience 4, Iter 23, disc loss: 0.059758479575315014, policy loss: 3.354831084570465
Experience 4, Iter 24, disc loss: 0.0694378297398912, policy loss: 3.346853847215318
Experience 4, Iter 25, disc loss: 0.045182974720848174, policy loss: 3.623395413250713
Experience 4, Iter 26, disc loss: 0.05015862671027529, policy loss: 3.6280944673324074
Experience 4, Iter 27, disc loss: 0.050745398709338904, policy loss: 3.725962775183213
Experience 4, Iter 28, disc loss: 0.04699638426602604, policy loss: 3.6207532182750617
Experience 4, Iter 29, disc loss: 0.05323434258152942, policy loss: 3.533046247253615
Experience 4, Iter 30, disc loss: 0.047633355485344336, policy loss: 3.722217107900546
Experience 4, Iter 31, disc loss: 0.04603847670368858, policy loss: 3.688981382940601
Experience 4, Iter 32, disc loss: 0.051571135136658376, policy loss: 3.7449939614274887
Experience 4, Iter 33, disc loss: 0.03528055422115824, policy loss: 4.025128833697142
Experience 4, Iter 34, disc loss: 0.03926159653070912, policy loss: 4.007154952684544
Experience 4, Iter 35, disc loss: 0.03637703533789263, policy loss: 4.021254374199556
Experience 4, Iter 36, disc loss: 0.05123545175985876, policy loss: 3.844148485417884
Experience 4, Iter 37, disc loss: 0.029690398218428952, policy loss: 4.4072413684319685
Experience 4, Iter 38, disc loss: 0.03427770943654245, policy loss: 4.357003900706639
Experience 4, Iter 39, disc loss: 0.043001466994468336, policy loss: 4.0888828279648735
Experience 4, Iter 40, disc loss: 0.03913445207940041, policy loss: 4.203620902383688
Experience 4, Iter 41, disc loss: 0.05138745962050654, policy loss: 4.256526409757521
Experience 4, Iter 42, disc loss: 0.03616263383811043, policy loss: 4.26873677534198
Experience 4, Iter 43, disc loss: 0.040905107708366056, policy loss: 4.2709042279661285
Experience 4, Iter 44, disc loss: 0.04926379890801818, policy loss: 4.37813280644144
Experience 4, Iter 45, disc loss: 0.05112799879182403, policy loss: 4.560123035671074
Experience 4, Iter 46, disc loss: 0.04208345668598013, policy loss: 4.541961265600748
Experience 4, Iter 47, disc loss: 0.023923519600564595, policy loss: 4.680505504638893
Experience 4, Iter 48, disc loss: 0.03636831886612645, policy loss: 4.671129606019461
Experience 4, Iter 49, disc loss: 0.056456195364687586, policy loss: 4.378052317268566
Experience 4, Iter 50, disc loss: 0.044169267768041774, policy loss: 4.662964968838283
Experience 4, Iter 51, disc loss: 0.03433665180173909, policy loss: 4.633035681308822
Experience 4, Iter 52, disc loss: 0.03566085535653543, policy loss: 4.76493746927847
Experience 4, Iter 53, disc loss: 0.0438854719800043, policy loss: 4.735982834434055
Experience 4, Iter 54, disc loss: 0.048506555099206136, policy loss: 4.753226232792353
Experience 4, Iter 55, disc loss: 0.030772350472831875, policy loss: 4.726697012676011
Experience 4, Iter 56, disc loss: 0.058563985002073785, policy loss: 4.888875064663054
Experience 4, Iter 57, disc loss: 0.03633480637033742, policy loss: 4.822908884740408
Experience 4, Iter 58, disc loss: 0.031583570743505054, policy loss: 4.897150320801675
Experience 4, Iter 59, disc loss: 0.02962589570453642, policy loss: 5.133277733975287
Experience 4, Iter 60, disc loss: 0.044446553620043794, policy loss: 4.95144783124633
Experience 4, Iter 61, disc loss: 0.03148497844897222, policy loss: 5.347267282779821
Experience 4, Iter 62, disc loss: 0.023797571296535237, policy loss: 5.264438803226212
Experience 4, Iter 63, disc loss: 0.029248165352582702, policy loss: 5.078805639507966
Experience 4, Iter 64, disc loss: 0.041210777806685646, policy loss: 5.136209975188677
Experience 4, Iter 65, disc loss: 0.02929581583054217, policy loss: 5.133357425385662
Experience 4, Iter 66, disc loss: 0.03286534490780821, policy loss: 5.045546856742329
Experience 4, Iter 67, disc loss: 0.02726316519371036, policy loss: 5.399344522011236
Experience 4, Iter 68, disc loss: 0.022432559898573956, policy loss: 5.498407744838499
Experience 4, Iter 69, disc loss: 0.023907603914908104, policy loss: 5.089148263108351
Experience 4, Iter 70, disc loss: 0.02511299547758456, policy loss: 5.211277725939297
Experience 4, Iter 71, disc loss: 0.017001632430730648, policy loss: 5.3296859999086434
Experience 4, Iter 72, disc loss: 0.030740541907444057, policy loss: 5.371876826703037
Experience 4, Iter 73, disc loss: 0.020290711828086578, policy loss: 5.272878391582256
Experience 4, Iter 74, disc loss: 0.022066039582384248, policy loss: 5.263536126705613
Experience 4, Iter 75, disc loss: 0.016706899922039654, policy loss: 5.477003745525711
Experience 4, Iter 76, disc loss: 0.015143712339106583, policy loss: 5.459045437089703
Experience 4, Iter 77, disc loss: 0.024951088799352383, policy loss: 5.568198601522435
Experience 4, Iter 78, disc loss: 0.024913214456062184, policy loss: 5.2497335491210135
Experience 4, Iter 79, disc loss: 0.027357495875685867, policy loss: 5.28971895638422
Experience 4, Iter 80, disc loss: 0.028892572879391662, policy loss: 5.2706086183086125
Experience 4, Iter 81, disc loss: 0.034540707471438434, policy loss: 4.953459730273281
Experience 4, Iter 82, disc loss: 0.0262940670895266, policy loss: 5.271494317965008
Experience 4, Iter 83, disc loss: 0.023849656975102128, policy loss: 5.493743024480385
Experience 4, Iter 84, disc loss: 0.029665393440841813, policy loss: 5.350003761952339
Experience 4, Iter 85, disc loss: 0.031039879562887732, policy loss: 5.4605238307780155
Experience 4, Iter 86, disc loss: 0.022562156260633213, policy loss: 5.762345807646087
Experience 4, Iter 87, disc loss: 0.018443514650591245, policy loss: 5.8911341510909585
Experience 4, Iter 88, disc loss: 0.020598714022908464, policy loss: 5.469253171111773
Experience 4, Iter 89, disc loss: 0.013107060090208926, policy loss: 6.12041755958527
Experience 4, Iter 90, disc loss: 0.015514546833460756, policy loss: 5.828538364875891
Experience 4, Iter 91, disc loss: 0.015570062669014565, policy loss: 5.7346991980790945
Experience 4, Iter 92, disc loss: 0.014127416197526044, policy loss: 5.810887120375055
Experience 4, Iter 93, disc loss: 0.01419310342847836, policy loss: 5.82468811778421
Experience 4, Iter 94, disc loss: 0.01336318108511258, policy loss: 5.674058042852961
Experience 4, Iter 95, disc loss: 0.014989452099969647, policy loss: 5.658651401184507
Experience 4, Iter 96, disc loss: 0.014971664858599264, policy loss: 5.9521585063814815
Experience 4, Iter 97, disc loss: 0.013498707608569261, policy loss: 5.60026249371958
Experience 4, Iter 98, disc loss: 0.016976816994165173, policy loss: 5.4956537484828285
Experience 4, Iter 99, disc loss: 0.012673718953153019, policy loss: 5.688375390762545
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0101],
        [0.0893],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2698e-02, 9.3515e-02, 5.8180e-02, 3.8934e-03, 1.4544e-04,
          3.6679e-01]],

        [[1.2698e-02, 9.3515e-02, 5.8180e-02, 3.8934e-03, 1.4544e-04,
          3.6679e-01]],

        [[1.2698e-02, 9.3515e-02, 5.8180e-02, 3.8934e-03, 1.4544e-04,
          3.6679e-01]],

        [[1.2698e-02, 9.3515e-02, 5.8180e-02, 3.8934e-03, 1.4544e-04,
          3.6679e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.0405, 0.3572, 0.0050], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.0405, 0.3572, 0.0050])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.557
Iter 2/2000 - Loss: -0.819
Iter 3/2000 - Loss: -1.149
Iter 4/2000 - Loss: -1.030
Iter 5/2000 - Loss: -0.917
Iter 6/2000 - Loss: -1.057
Iter 7/2000 - Loss: -1.209
Iter 8/2000 - Loss: -1.218
Iter 9/2000 - Loss: -1.162
Iter 10/2000 - Loss: -1.179
Iter 11/2000 - Loss: -1.277
Iter 12/2000 - Loss: -1.351
Iter 13/2000 - Loss: -1.333
Iter 14/2000 - Loss: -1.269
Iter 15/2000 - Loss: -1.266
Iter 16/2000 - Loss: -1.352
Iter 17/2000 - Loss: -1.441
Iter 18/2000 - Loss: -1.443
Iter 19/2000 - Loss: -1.384
Iter 20/2000 - Loss: -1.365
Iter 1981/2000 - Loss: -8.269
Iter 1982/2000 - Loss: -8.269
Iter 1983/2000 - Loss: -8.269
Iter 1984/2000 - Loss: -8.269
Iter 1985/2000 - Loss: -8.269
Iter 1986/2000 - Loss: -8.269
Iter 1987/2000 - Loss: -8.270
Iter 1988/2000 - Loss: -8.270
Iter 1989/2000 - Loss: -8.270
Iter 1990/2000 - Loss: -8.270
Iter 1991/2000 - Loss: -8.270
Iter 1992/2000 - Loss: -8.270
Iter 1993/2000 - Loss: -8.270
Iter 1994/2000 - Loss: -8.270
Iter 1995/2000 - Loss: -8.270
Iter 1996/2000 - Loss: -8.270
Iter 1997/2000 - Loss: -8.270
Iter 1998/2000 - Loss: -8.270
Iter 1999/2000 - Loss: -8.270
Iter 2000/2000 - Loss: -8.270
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[ 5.9358,  5.8077, 36.3916, 20.1117,  6.9665, 26.1541]],

        [[28.2295, 54.9631, 32.4340,  1.9021,  5.5698, 13.2934]],

        [[32.7138, 67.5022, 24.5296,  1.3751,  5.0450, 15.5557]],

        [[28.5950, 58.4428,  6.6230,  2.5685,  7.9319, 19.2703]]])
Signal Variance: tensor([ 0.0675,  1.0330, 13.9777,  0.1361])
Estimated target variance: tensor([0.0099, 0.0405, 0.3572, 0.0050])
N: 50
Signal to noise ratio: tensor([13.8982, 65.4324, 96.8698, 23.2872])
Bound on condition number: tensor([  9658.9683, 214071.2317, 469188.5648,  27115.6431])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.031821819750979585, policy loss: 3.7804501980325638
Experience 5, Iter 1, disc loss: 0.04400540638171226, policy loss: 3.6748206800408196
Experience 5, Iter 2, disc loss: 0.07613564732209101, policy loss: 2.909698722119488
Experience 5, Iter 3, disc loss: 0.11112416925091462, policy loss: 2.759642044170912
Experience 5, Iter 4, disc loss: 0.09379467960793343, policy loss: 3.202953682879036
Experience 5, Iter 5, disc loss: 0.10611975457765226, policy loss: 2.9513154035159745
Experience 5, Iter 6, disc loss: 0.10457686772278918, policy loss: 3.0031646692430156
Experience 5, Iter 7, disc loss: 0.10699076754889678, policy loss: 3.1884682755832365
Experience 5, Iter 8, disc loss: 0.11367041481291798, policy loss: 3.063809424418409
Experience 5, Iter 9, disc loss: 0.11410062031728417, policy loss: 3.1945499393104106
Experience 5, Iter 10, disc loss: 0.12745823033879292, policy loss: 3.3202352627012965
Experience 5, Iter 11, disc loss: 0.12963337041022585, policy loss: 3.401066505106834
Experience 5, Iter 12, disc loss: 0.1328644598872915, policy loss: 3.4725209374497163
Experience 5, Iter 13, disc loss: 0.1175001992566027, policy loss: 3.689834685108708
Experience 5, Iter 14, disc loss: 0.1029104344077385, policy loss: 3.6413473093859317
Experience 5, Iter 15, disc loss: 0.09607451867940475, policy loss: 3.4121774361505324
Experience 5, Iter 16, disc loss: 0.08683020715427717, policy loss: 3.436915332729888
Experience 5, Iter 17, disc loss: 0.11244325562880902, policy loss: 3.0557133567585786
Experience 5, Iter 18, disc loss: 0.09415305811020955, policy loss: 3.344066024896191
Experience 5, Iter 19, disc loss: 0.11625309290401142, policy loss: 3.0201915862905055
Experience 5, Iter 20, disc loss: 0.09775887552761198, policy loss: 3.311189505214665
Experience 5, Iter 21, disc loss: 0.07656315990801896, policy loss: 3.344352554638445
Experience 5, Iter 22, disc loss: 0.07513573869947614, policy loss: 3.5068090889456265
Experience 5, Iter 23, disc loss: 0.07112724209023726, policy loss: 3.326201073798007
Experience 5, Iter 24, disc loss: 0.06486282868505881, policy loss: 3.5861847750181948
Experience 5, Iter 25, disc loss: 0.05162600171024785, policy loss: 3.9135220057701665
Experience 5, Iter 26, disc loss: 0.05189238277011453, policy loss: 3.9293550495195224
Experience 5, Iter 27, disc loss: 0.04714984134066814, policy loss: 4.044605642162909
Experience 5, Iter 28, disc loss: 0.04476225273027422, policy loss: 4.1584272018198
Experience 5, Iter 29, disc loss: 0.04190318711092962, policy loss: 4.359299131836869
Experience 5, Iter 30, disc loss: 0.04687733032461679, policy loss: 4.239826097895852
Experience 5, Iter 31, disc loss: 0.04633735946794623, policy loss: 4.382174678804996
Experience 5, Iter 32, disc loss: 0.039544821965709694, policy loss: 4.488335484180852
Experience 5, Iter 33, disc loss: 0.04414316084277403, policy loss: 4.3321468198189095
Experience 5, Iter 34, disc loss: 0.04126810283562962, policy loss: 4.37633601895959
Experience 5, Iter 35, disc loss: 0.040607484338776687, policy loss: 4.17146255046312
Experience 5, Iter 36, disc loss: 0.03476120960758201, policy loss: 4.550444595943938
Experience 5, Iter 37, disc loss: 0.032126059774804386, policy loss: 4.667608601944417
Experience 5, Iter 38, disc loss: 0.034410625097229655, policy loss: 4.416841104378557
Experience 5, Iter 39, disc loss: 0.0336184839576228, policy loss: 4.386753925088863
Experience 5, Iter 40, disc loss: 0.028635677268675747, policy loss: 4.590693738971296
Experience 5, Iter 41, disc loss: 0.0329340267085627, policy loss: 4.438820936014195
Experience 5, Iter 42, disc loss: 0.03493106445205934, policy loss: 4.164905925455518
Experience 5, Iter 43, disc loss: 0.03417718229806815, policy loss: 4.590087780768259
Experience 5, Iter 44, disc loss: 0.02679592796763189, policy loss: 4.41853846733567
Experience 5, Iter 45, disc loss: 0.02848796112120853, policy loss: 4.474585159733262
Experience 5, Iter 46, disc loss: 0.032761549567781156, policy loss: 4.26622726368571
Experience 5, Iter 47, disc loss: 0.030554795556096755, policy loss: 4.230707653825407
Experience 5, Iter 48, disc loss: 0.029907733074516035, policy loss: 4.400586932854697
Experience 5, Iter 49, disc loss: 0.027618177728058545, policy loss: 4.324453455339813
Experience 5, Iter 50, disc loss: 0.032161867718625864, policy loss: 4.255424445576108
Experience 5, Iter 51, disc loss: 0.02663986248291002, policy loss: 4.384373072106123
Experience 5, Iter 52, disc loss: 0.026540630563367293, policy loss: 4.420113746215168
Experience 5, Iter 53, disc loss: 0.02892370616204288, policy loss: 4.277349195684713
Experience 5, Iter 54, disc loss: 0.0299986634922877, policy loss: 4.292610786132149
Experience 5, Iter 55, disc loss: 0.028604836469604145, policy loss: 4.172900699790351
Experience 5, Iter 56, disc loss: 0.031250391663889814, policy loss: 4.185852251966367
Experience 5, Iter 57, disc loss: 0.028508192107959726, policy loss: 4.430542711508733
Experience 5, Iter 58, disc loss: 0.02679348690058699, policy loss: 4.370556629949211
Experience 5, Iter 59, disc loss: 0.02711559217006764, policy loss: 4.341766683477224
Experience 5, Iter 60, disc loss: 0.025904808888546904, policy loss: 4.4992186301979835
Experience 5, Iter 61, disc loss: 0.02694577123443897, policy loss: 4.330465140500687
Experience 5, Iter 62, disc loss: 0.027987680913406268, policy loss: 4.318554525819263
Experience 5, Iter 63, disc loss: 0.026489651192624822, policy loss: 4.299978784274455
Experience 5, Iter 64, disc loss: 0.030331717300662933, policy loss: 4.186377168284606
Experience 5, Iter 65, disc loss: 0.025475196047900425, policy loss: 4.347771976306219
Experience 5, Iter 66, disc loss: 0.02829718039003107, policy loss: 4.183052017275628
Experience 5, Iter 67, disc loss: 0.026668574201932565, policy loss: 4.306132974986383
Experience 5, Iter 68, disc loss: 0.027706291110645924, policy loss: 4.161630632060321
Experience 5, Iter 69, disc loss: 0.026487482453220128, policy loss: 4.27366701384574
Experience 5, Iter 70, disc loss: 0.027867875415765655, policy loss: 4.19010398115002
Experience 5, Iter 71, disc loss: 0.023303270620799663, policy loss: 4.444611005840857
Experience 5, Iter 72, disc loss: 0.024677351653200463, policy loss: 4.377896287198766
Experience 5, Iter 73, disc loss: 0.02346722707708644, policy loss: 4.35216530923898
Experience 5, Iter 74, disc loss: 0.026781692924549946, policy loss: 4.207927563055694
Experience 5, Iter 75, disc loss: 0.025417527745255932, policy loss: 4.342493074518938
Experience 5, Iter 76, disc loss: 0.024500242677676455, policy loss: 4.314314315073359
Experience 5, Iter 77, disc loss: 0.026899933584166796, policy loss: 4.119750559090118
Experience 5, Iter 78, disc loss: 0.02507141104103712, policy loss: 4.262101922547796
Experience 5, Iter 79, disc loss: 0.02423957550349744, policy loss: 4.2804605881530335
Experience 5, Iter 80, disc loss: 0.026144931991329758, policy loss: 4.219036603887597
Experience 5, Iter 81, disc loss: 0.024761964127355048, policy loss: 4.269091782471056
Experience 5, Iter 82, disc loss: 0.0265176419459944, policy loss: 4.1956318725650235
Experience 5, Iter 83, disc loss: 0.024210423782227028, policy loss: 4.372615966409423
Experience 5, Iter 84, disc loss: 0.022683030040785153, policy loss: 4.451090985725298
Experience 5, Iter 85, disc loss: 0.021486484478447847, policy loss: 4.455182401598878
Experience 5, Iter 86, disc loss: 0.021005464270993472, policy loss: 4.463573789899085
Experience 5, Iter 87, disc loss: 0.020646391887710145, policy loss: 4.488579182775653
Experience 5, Iter 88, disc loss: 0.021229601967350766, policy loss: 4.479022966510152
Experience 5, Iter 89, disc loss: 0.02384793041475783, policy loss: 4.30157203824043
Experience 5, Iter 90, disc loss: 0.021226724907116322, policy loss: 4.464451666289683
Experience 5, Iter 91, disc loss: 0.020480974074838516, policy loss: 4.484226402467086
Experience 5, Iter 92, disc loss: 0.020127919720046007, policy loss: 4.534562204237799
Experience 5, Iter 93, disc loss: 0.021348272926269656, policy loss: 4.588659288003343
Experience 5, Iter 94, disc loss: 0.02141204230353535, policy loss: 4.391066343873611
Experience 5, Iter 95, disc loss: 0.01993278008376561, policy loss: 4.497901765143605
Experience 5, Iter 96, disc loss: 0.0186551016009222, policy loss: 4.737101560377871
Experience 5, Iter 97, disc loss: 0.017157145532390877, policy loss: 4.770726643288436
Experience 5, Iter 98, disc loss: 0.01776316594375859, policy loss: 4.663482025403804
Experience 5, Iter 99, disc loss: 0.019062861376056214, policy loss: 4.706727867684388
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.0222],
        [0.2540],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.6141e-02, 1.2105e-01, 2.3143e-01, 9.0525e-03, 6.2644e-04,
          7.3522e-01]],

        [[1.6141e-02, 1.2105e-01, 2.3143e-01, 9.0525e-03, 6.2644e-04,
          7.3522e-01]],

        [[1.6141e-02, 1.2105e-01, 2.3143e-01, 9.0525e-03, 6.2644e-04,
          7.3522e-01]],

        [[1.6141e-02, 1.2105e-01, 2.3143e-01, 9.0525e-03, 6.2644e-04,
          7.3522e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0125, 0.0888, 1.0160, 0.0200], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0125, 0.0888, 1.0160, 0.0200])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.652
Iter 2/2000 - Loss: 0.485
Iter 3/2000 - Loss: 0.481
Iter 4/2000 - Loss: 0.423
Iter 5/2000 - Loss: 0.402
Iter 6/2000 - Loss: 0.385
Iter 7/2000 - Loss: 0.375
Iter 8/2000 - Loss: 0.347
Iter 9/2000 - Loss: 0.315
Iter 10/2000 - Loss: 0.312
Iter 11/2000 - Loss: 0.330
Iter 12/2000 - Loss: 0.322
Iter 13/2000 - Loss: 0.282
Iter 14/2000 - Loss: 0.242
Iter 15/2000 - Loss: 0.222
Iter 16/2000 - Loss: 0.208
Iter 17/2000 - Loss: 0.182
Iter 18/2000 - Loss: 0.135
Iter 19/2000 - Loss: 0.074
Iter 20/2000 - Loss: 0.006
Iter 1981/2000 - Loss: -7.527
Iter 1982/2000 - Loss: -7.527
Iter 1983/2000 - Loss: -7.527
Iter 1984/2000 - Loss: -7.527
Iter 1985/2000 - Loss: -7.527
Iter 1986/2000 - Loss: -7.527
Iter 1987/2000 - Loss: -7.527
Iter 1988/2000 - Loss: -7.527
Iter 1989/2000 - Loss: -7.527
Iter 1990/2000 - Loss: -7.527
Iter 1991/2000 - Loss: -7.527
Iter 1992/2000 - Loss: -7.527
Iter 1993/2000 - Loss: -7.527
Iter 1994/2000 - Loss: -7.527
Iter 1995/2000 - Loss: -7.528
Iter 1996/2000 - Loss: -7.528
Iter 1997/2000 - Loss: -7.528
Iter 1998/2000 - Loss: -7.528
Iter 1999/2000 - Loss: -7.528
Iter 2000/2000 - Loss: -7.528
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[11.7262,  7.6322, 46.8735, 18.5981, 12.1482, 44.1338]],

        [[19.3530,  6.8986, 32.1092,  3.5765,  0.3965, 23.2027]],

        [[25.4827, 49.7602, 17.8166,  1.0462,  1.2494, 12.1559]],

        [[21.7898, 44.6049, 11.1521,  3.1725, 10.3771, 32.1858]]])
Signal Variance: tensor([ 0.1211,  1.7677, 12.8043,  0.3005])
Estimated target variance: tensor([0.0125, 0.0888, 1.0160, 0.0200])
N: 60
Signal to noise ratio: tensor([18.1676, 88.1329, 88.9183, 33.5081])
Bound on condition number: tensor([ 19804.7106, 466045.3428, 474388.6538,  67368.4175])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.014635628157959824, policy loss: 4.8778553367427495
Experience 6, Iter 1, disc loss: 0.013963905823113023, policy loss: 4.956485716500145
Experience 6, Iter 2, disc loss: 0.014721128171070599, policy loss: 4.906182174893779
Experience 6, Iter 3, disc loss: 0.016065199882180265, policy loss: 4.693129902327567
Experience 6, Iter 4, disc loss: 0.016603100497250844, policy loss: 4.851104201239873
Experience 6, Iter 5, disc loss: 0.015138065967126994, policy loss: 4.878521000040212
Experience 6, Iter 6, disc loss: 0.0156782529414015, policy loss: 4.751052704538168
Experience 6, Iter 7, disc loss: 0.014934814945688952, policy loss: 4.779995411193006
Experience 6, Iter 8, disc loss: 0.01604004545418792, policy loss: 4.764767236618937
Experience 6, Iter 9, disc loss: 0.016450634020474923, policy loss: 4.758879105136541
Experience 6, Iter 10, disc loss: 0.015237763530838867, policy loss: 4.752073736451613
Experience 6, Iter 11, disc loss: 0.015239674041407289, policy loss: 4.695635350832491
Experience 6, Iter 12, disc loss: 0.014492467931688102, policy loss: 4.806463726314657
Experience 6, Iter 13, disc loss: 0.014070633751621608, policy loss: 4.932687565552799
Experience 6, Iter 14, disc loss: 0.01499766713635671, policy loss: 4.791570495113693
Experience 6, Iter 15, disc loss: 0.015247203817242774, policy loss: 4.73779879233157
Experience 6, Iter 16, disc loss: 0.013680417320763394, policy loss: 4.946529933831703
Experience 6, Iter 17, disc loss: 0.014288484883927206, policy loss: 4.93555131058039
Experience 6, Iter 18, disc loss: 0.01485593815439137, policy loss: 4.931480982238311
Experience 6, Iter 19, disc loss: 0.014501180575764938, policy loss: 4.936071826805428
Experience 6, Iter 20, disc loss: 0.014244116954414535, policy loss: 4.968647059219094
Experience 6, Iter 21, disc loss: 0.01382133957441495, policy loss: 4.907111547509248
Experience 6, Iter 22, disc loss: 0.013425344721996892, policy loss: 4.968462162744152
Experience 6, Iter 23, disc loss: 0.013373738070619832, policy loss: 4.904155086285372
Experience 6, Iter 24, disc loss: 0.013156918464713344, policy loss: 4.905382958884916
Experience 6, Iter 25, disc loss: 0.012695567784402862, policy loss: 5.089414376752355
Experience 6, Iter 26, disc loss: 0.01375301399070479, policy loss: 4.839728074560327
Experience 6, Iter 27, disc loss: 0.011906936157348395, policy loss: 5.156910358290293
Experience 6, Iter 28, disc loss: 0.013988934885029248, policy loss: 5.126784557052543
Experience 6, Iter 29, disc loss: 0.012184366741715313, policy loss: 5.266412091242248
Experience 6, Iter 30, disc loss: 0.013545393742537366, policy loss: 4.861760024934717
Experience 6, Iter 31, disc loss: 0.012869565059259585, policy loss: 4.935486469641153
Experience 6, Iter 32, disc loss: 0.013182707763171025, policy loss: 5.034077081783317
Experience 6, Iter 33, disc loss: 0.012318299195172083, policy loss: 5.042119415704562
Experience 6, Iter 34, disc loss: 0.01195524236109227, policy loss: 5.084763419478852
Experience 6, Iter 35, disc loss: 0.012068530188011187, policy loss: 5.201716503508706
Experience 6, Iter 36, disc loss: 0.012006331427345996, policy loss: 5.100444590683686
Experience 6, Iter 37, disc loss: 0.011876161691915809, policy loss: 5.104929448029906
Experience 6, Iter 38, disc loss: 0.012179302896685357, policy loss: 5.0437049314077225
Experience 6, Iter 39, disc loss: 0.011281998972672437, policy loss: 5.349636865738315
Experience 6, Iter 40, disc loss: 0.011313861992383833, policy loss: 5.121181931272907
Experience 6, Iter 41, disc loss: 0.011915007844799272, policy loss: 5.007010348241224
Experience 6, Iter 42, disc loss: 0.009570905796922238, policy loss: 5.461698994114612
Experience 6, Iter 43, disc loss: 0.012062482292836348, policy loss: 5.002481024796433
Experience 6, Iter 44, disc loss: 0.01098259991123256, policy loss: 5.193829428590975
Experience 6, Iter 45, disc loss: 0.011192004640082423, policy loss: 5.0355339516202555
Experience 6, Iter 46, disc loss: 0.010723185671632478, policy loss: 5.152942211345458
Experience 6, Iter 47, disc loss: 0.0105133857271617, policy loss: 5.273276740469473
Experience 6, Iter 48, disc loss: 0.011126662310487492, policy loss: 5.229808956687732
Experience 6, Iter 49, disc loss: 0.01130692682751589, policy loss: 5.1746854876700175
Experience 6, Iter 50, disc loss: 0.010836292879015509, policy loss: 5.258670011038663
Experience 6, Iter 51, disc loss: 0.011027955629671068, policy loss: 5.2297111761343364
Experience 6, Iter 52, disc loss: 0.009681537852897694, policy loss: 5.5762062686022595
Experience 6, Iter 53, disc loss: 0.009227262462158214, policy loss: 5.453312641987679
Experience 6, Iter 54, disc loss: 0.01048725218761263, policy loss: 5.262190919790216
Experience 6, Iter 55, disc loss: 0.010374898482478663, policy loss: 5.220512372694685
Experience 6, Iter 56, disc loss: 0.009295526564881156, policy loss: 5.491736787161899
Experience 6, Iter 57, disc loss: 0.009764758973646967, policy loss: 5.258336028938561
Experience 6, Iter 58, disc loss: 0.009219432684992694, policy loss: 5.395398694334645
Experience 6, Iter 59, disc loss: 0.008957748638449873, policy loss: 5.414117527114769
Experience 6, Iter 60, disc loss: 0.008969782495804345, policy loss: 5.5244195567811705
Experience 6, Iter 61, disc loss: 0.008708902063207442, policy loss: 5.450053314659317
Experience 6, Iter 62, disc loss: 0.00891745764114531, policy loss: 5.4846174509613945
Experience 6, Iter 63, disc loss: 0.008459687273588732, policy loss: 5.584778260832378
Experience 6, Iter 64, disc loss: 0.008999285959029124, policy loss: 5.3854121155313806
Experience 6, Iter 65, disc loss: 0.008339590905129767, policy loss: 5.435583086538355
Experience 6, Iter 66, disc loss: 0.00808334344354148, policy loss: 5.621559471431672
Experience 6, Iter 67, disc loss: 0.008132955913848126, policy loss: 5.684593996306058
Experience 6, Iter 68, disc loss: 0.008410830885355013, policy loss: 5.445151999149639
Experience 6, Iter 69, disc loss: 0.009323262749854903, policy loss: 5.281853199558614
Experience 6, Iter 70, disc loss: 0.00828398969190781, policy loss: 5.534147945701269
Experience 6, Iter 71, disc loss: 0.008692882484020488, policy loss: 5.449704265249943
Experience 6, Iter 72, disc loss: 0.008243761522803674, policy loss: 5.600182827994175
Experience 6, Iter 73, disc loss: 0.008453907859091593, policy loss: 5.341304321313709
Experience 6, Iter 74, disc loss: 0.007410358510862066, policy loss: 6.0808481130523395
Experience 6, Iter 75, disc loss: 0.008352341306580538, policy loss: 5.360744500763242
Experience 6, Iter 76, disc loss: 0.0077086731340518955, policy loss: 5.591364083464525
Experience 6, Iter 77, disc loss: 0.007712342988507712, policy loss: 5.553752600300637
Experience 6, Iter 78, disc loss: 0.007736324997915233, policy loss: 5.5761559645463565
Experience 6, Iter 79, disc loss: 0.00741345784220524, policy loss: 5.672081355312315
Experience 6, Iter 80, disc loss: 0.0077149369982455794, policy loss: 5.560177150412436
Experience 6, Iter 81, disc loss: 0.007790819179741758, policy loss: 5.511346095922125
Experience 6, Iter 82, disc loss: 0.006902127001044282, policy loss: 5.757366560495372
Experience 6, Iter 83, disc loss: 0.008182636391749074, policy loss: 5.363475895527055
Experience 6, Iter 84, disc loss: 0.007532822121467403, policy loss: 5.61741930098173
Experience 6, Iter 85, disc loss: 0.007264375637029325, policy loss: 5.600435863464762
Experience 6, Iter 86, disc loss: 0.006951316322155697, policy loss: 5.780680424976647
Experience 6, Iter 87, disc loss: 0.006463705510545838, policy loss: 5.790571176116817
Experience 6, Iter 88, disc loss: 0.006331707462218686, policy loss: 5.860723563997562
Experience 6, Iter 89, disc loss: 0.007265435294237212, policy loss: 5.585992302316779
Experience 6, Iter 90, disc loss: 0.007720343952776008, policy loss: 5.509885134906194
Experience 6, Iter 91, disc loss: 0.006648097689011777, policy loss: 5.8301859412520205
Experience 6, Iter 92, disc loss: 0.006935429034968105, policy loss: 5.798875829802052
Experience 6, Iter 93, disc loss: 0.006512876665158564, policy loss: 5.721301884066223
Experience 6, Iter 94, disc loss: 0.006534740136071474, policy loss: 5.740775911577777
Experience 6, Iter 95, disc loss: 0.006153905732250129, policy loss: 5.8125778404612305
Experience 6, Iter 96, disc loss: 0.006450333565256876, policy loss: 5.7581878863088765
Experience 6, Iter 97, disc loss: 0.006633005294494209, policy loss: 5.741889948146742
Experience 6, Iter 98, disc loss: 0.006137453187070897, policy loss: 5.845895712991242
Experience 6, Iter 99, disc loss: 0.006851613009870094, policy loss: 5.70126543845775
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.0294],
        [0.3143],
        [0.0060]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.7786e-02, 1.2046e-01, 2.8120e-01, 1.1044e-02, 9.9506e-04,
          1.0189e+00]],

        [[1.7786e-02, 1.2046e-01, 2.8120e-01, 1.1044e-02, 9.9506e-04,
          1.0189e+00]],

        [[1.7786e-02, 1.2046e-01, 2.8120e-01, 1.1044e-02, 9.9506e-04,
          1.0189e+00]],

        [[1.7786e-02, 1.2046e-01, 2.8120e-01, 1.1044e-02, 9.9506e-04,
          1.0189e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0126, 0.1176, 1.2570, 0.0240], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0126, 0.1176, 1.2570, 0.0240])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.933
Iter 2/2000 - Loss: 0.786
Iter 3/2000 - Loss: 0.780
Iter 4/2000 - Loss: 0.684
Iter 5/2000 - Loss: 0.704
Iter 6/2000 - Loss: 0.735
Iter 7/2000 - Loss: 0.672
Iter 8/2000 - Loss: 0.615
Iter 9/2000 - Loss: 0.629
Iter 10/2000 - Loss: 0.651
Iter 11/2000 - Loss: 0.625
Iter 12/2000 - Loss: 0.577
Iter 13/2000 - Loss: 0.547
Iter 14/2000 - Loss: 0.526
Iter 15/2000 - Loss: 0.488
Iter 16/2000 - Loss: 0.428
Iter 17/2000 - Loss: 0.362
Iter 18/2000 - Loss: 0.296
Iter 19/2000 - Loss: 0.220
Iter 20/2000 - Loss: 0.121
Iter 1981/2000 - Loss: -7.676
Iter 1982/2000 - Loss: -7.676
Iter 1983/2000 - Loss: -7.676
Iter 1984/2000 - Loss: -7.676
Iter 1985/2000 - Loss: -7.676
Iter 1986/2000 - Loss: -7.676
Iter 1987/2000 - Loss: -7.676
Iter 1988/2000 - Loss: -7.676
Iter 1989/2000 - Loss: -7.676
Iter 1990/2000 - Loss: -7.676
Iter 1991/2000 - Loss: -7.676
Iter 1992/2000 - Loss: -7.676
Iter 1993/2000 - Loss: -7.676
Iter 1994/2000 - Loss: -7.676
Iter 1995/2000 - Loss: -7.676
Iter 1996/2000 - Loss: -7.676
Iter 1997/2000 - Loss: -7.676
Iter 1998/2000 - Loss: -7.676
Iter 1999/2000 - Loss: -7.676
Iter 2000/2000 - Loss: -7.677
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[12.7138,  7.9876, 44.5596, 17.2718, 13.6526, 48.7610]],

        [[22.8610, 44.1782,  6.4027,  1.1496,  6.9153, 21.9907]],

        [[24.9451, 49.5500, 11.5183,  0.9802,  5.3157, 23.6255]],

        [[21.4242, 43.0169, 11.2453,  3.3485,  8.2151, 40.9498]]])
Signal Variance: tensor([ 0.1243,  1.4958, 19.8448,  0.3201])
Estimated target variance: tensor([0.0126, 0.1176, 1.2570, 0.0240])
N: 70
Signal to noise ratio: tensor([ 19.4999,  80.8408, 115.9465,  35.0327])
Bound on condition number: tensor([ 26618.2955, 457467.1459, 941052.6829,  85911.3538])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.006078481961464429, policy loss: 5.923264347778416
Experience 7, Iter 1, disc loss: 0.006332866270047079, policy loss: 5.83138601813114
Experience 7, Iter 2, disc loss: 0.005894498546749951, policy loss: 5.858372647809211
Experience 7, Iter 3, disc loss: 0.006094696289192054, policy loss: 5.733806537052969
Experience 7, Iter 4, disc loss: 0.00531116616548399, policy loss: 6.003184610027453
Experience 7, Iter 5, disc loss: 0.00573777630896475, policy loss: 6.014013565596229
Experience 7, Iter 6, disc loss: 0.006609451906637868, policy loss: 5.668277924318822
Experience 7, Iter 7, disc loss: 0.006056797812787561, policy loss: 5.876666859758933
Experience 7, Iter 8, disc loss: 0.005940999450723551, policy loss: 5.854173336320526
Experience 7, Iter 9, disc loss: 0.006276775931181263, policy loss: 5.705595121594548
Experience 7, Iter 10, disc loss: 0.0059776559574697555, policy loss: 5.844725916994431
Experience 7, Iter 11, disc loss: 0.006584058064214734, policy loss: 5.582033628610775
Experience 7, Iter 12, disc loss: 0.006429705600974382, policy loss: 5.769555969183308
Experience 7, Iter 13, disc loss: 0.006813353724977204, policy loss: 5.553302076861531
Experience 7, Iter 14, disc loss: 0.00623271356909846, policy loss: 5.716265657627441
Experience 7, Iter 15, disc loss: 0.006526455064580883, policy loss: 5.692819209674695
Experience 7, Iter 16, disc loss: 0.0069026208941411205, policy loss: 5.561383711018211
Experience 7, Iter 17, disc loss: 0.0070836913947543234, policy loss: 5.732352119579593
Experience 7, Iter 18, disc loss: 0.005922970978743216, policy loss: 6.309532490367456
Experience 7, Iter 19, disc loss: 0.007047423890936447, policy loss: 5.8405809342963355
Experience 7, Iter 20, disc loss: 0.007016002981795388, policy loss: 5.686860474404591
Experience 7, Iter 21, disc loss: 0.0076975952128346785, policy loss: 5.534179095510932
Experience 7, Iter 22, disc loss: 0.007494204119840638, policy loss: 5.593830502164649
Experience 7, Iter 23, disc loss: 0.007251819081254923, policy loss: 5.664534062537264
Experience 7, Iter 24, disc loss: 0.007898458479404834, policy loss: 5.570386099141086
Experience 7, Iter 25, disc loss: 0.007868682053953708, policy loss: 5.711161354882183
Experience 7, Iter 26, disc loss: 0.00771118586605264, policy loss: 5.615276200773725
Experience 7, Iter 27, disc loss: 0.008334936344987415, policy loss: 5.500720753645638
Experience 7, Iter 28, disc loss: 0.008466076636977352, policy loss: 5.521886497520042
Experience 7, Iter 29, disc loss: 0.008355152794097235, policy loss: 5.74167617838668
Experience 7, Iter 30, disc loss: 0.011124285718523662, policy loss: 5.4149311232990245
Experience 7, Iter 31, disc loss: 0.009177917247303546, policy loss: 5.903417693835365
Experience 7, Iter 32, disc loss: 0.008509060937228962, policy loss: 5.827244943940393
Experience 7, Iter 33, disc loss: 0.009053316639285658, policy loss: 5.554170479856406
Experience 7, Iter 34, disc loss: 0.00910857807933641, policy loss: 5.462680579583441
Experience 7, Iter 35, disc loss: 0.009281499019550997, policy loss: 5.8815592684452165
Experience 7, Iter 36, disc loss: 0.00932446890643688, policy loss: 5.614740161980305
Experience 7, Iter 37, disc loss: 0.009399118019262602, policy loss: 6.043523718906902
Experience 7, Iter 38, disc loss: 0.00874479115061931, policy loss: 6.007868709304816
Experience 7, Iter 39, disc loss: 0.009083327836191975, policy loss: 5.872630359829659
Experience 7, Iter 40, disc loss: 0.008921866315415436, policy loss: 5.766040441058902
Experience 7, Iter 41, disc loss: 0.008091532827683903, policy loss: 6.057391203191965
Experience 7, Iter 42, disc loss: 0.009066480336693973, policy loss: 5.772205107572839
Experience 7, Iter 43, disc loss: 0.00957225730490853, policy loss: 5.888174835771531
Experience 7, Iter 44, disc loss: 0.009102674698019585, policy loss: 5.815989364615333
Experience 7, Iter 45, disc loss: 0.011486740917839109, policy loss: 5.655488787690245
Experience 7, Iter 46, disc loss: 0.01132293748824437, policy loss: 5.780366917204087
Experience 7, Iter 47, disc loss: 0.009840848011623965, policy loss: 5.958861971945234
Experience 7, Iter 48, disc loss: 0.009561750009698732, policy loss: 5.967206136656741
Experience 7, Iter 49, disc loss: 0.010484070615961016, policy loss: 5.95480634586825
Experience 7, Iter 50, disc loss: 0.008619244599075161, policy loss: 6.462510719016145
Experience 7, Iter 51, disc loss: 0.01157512933534117, policy loss: 5.7658699838014185
Experience 7, Iter 52, disc loss: 0.00813201580878007, policy loss: 6.140506744351268
Experience 7, Iter 53, disc loss: 0.009908273002866667, policy loss: 6.0352017392024635
Experience 7, Iter 54, disc loss: 0.009188027773785757, policy loss: 6.027074007715484
Experience 7, Iter 55, disc loss: 0.009738512703340283, policy loss: 5.9922825557683534
Experience 7, Iter 56, disc loss: 0.009610968773980356, policy loss: 5.831642221913347
Experience 7, Iter 57, disc loss: 0.009974274286687033, policy loss: 5.98921829576462
Experience 7, Iter 58, disc loss: 0.008367094200727372, policy loss: 6.247753108699969
Experience 7, Iter 59, disc loss: 0.00984415995263923, policy loss: 5.875937865586913
Experience 7, Iter 60, disc loss: 0.008074991705713899, policy loss: 6.195988766543537
Experience 7, Iter 61, disc loss: 0.010131107820094852, policy loss: 5.92840389095686
Experience 7, Iter 62, disc loss: 0.00883065129997777, policy loss: 6.182139276532217
Experience 7, Iter 63, disc loss: 0.011443120171642656, policy loss: 5.8763826859917625
Experience 7, Iter 64, disc loss: 0.008670664371178086, policy loss: 6.245480891675645
Experience 7, Iter 65, disc loss: 0.00884767316213364, policy loss: 5.939939180660982
Experience 7, Iter 66, disc loss: 0.007152368155493959, policy loss: 6.321364669363155
Experience 7, Iter 67, disc loss: 0.009040931964382581, policy loss: 5.86811844798087
Experience 7, Iter 68, disc loss: 0.013367619357868244, policy loss: 6.057805921861867
Experience 7, Iter 69, disc loss: 0.009144716393341159, policy loss: 5.806887190394751
Experience 7, Iter 70, disc loss: 0.011982412489192077, policy loss: 6.098877752774378
Experience 7, Iter 71, disc loss: 0.010180160337773518, policy loss: 6.023777387791609
Experience 7, Iter 72, disc loss: 0.007594606302560409, policy loss: 6.3194879197472
Experience 7, Iter 73, disc loss: 0.009685533009563993, policy loss: 6.124596532228431
Experience 7, Iter 74, disc loss: 0.009214380048365969, policy loss: 6.338263907015868
Experience 7, Iter 75, disc loss: 0.008832706712800465, policy loss: 6.5069180533467295
Experience 7, Iter 76, disc loss: 0.009698827837117393, policy loss: 6.24012131473157
Experience 7, Iter 77, disc loss: 0.008124776524059265, policy loss: 6.515935301506504
Experience 7, Iter 78, disc loss: 0.00814818852687619, policy loss: 6.513704197915014
Experience 7, Iter 79, disc loss: 0.009253242521788274, policy loss: 6.358097396222909
Experience 7, Iter 80, disc loss: 0.007459025367806884, policy loss: 6.454217997310684
Experience 7, Iter 81, disc loss: 0.006964475531311839, policy loss: 6.497534026137145
Experience 7, Iter 82, disc loss: 0.006747312942270528, policy loss: 6.420030376094896
Experience 7, Iter 83, disc loss: 0.007708513880854929, policy loss: 6.414369045961481
Experience 7, Iter 84, disc loss: 0.008872660760749641, policy loss: 6.073119234884189
Experience 7, Iter 85, disc loss: 0.008882488624315989, policy loss: 6.225486789095554
Experience 7, Iter 86, disc loss: 0.008597942882769348, policy loss: 6.506325998753157
Experience 7, Iter 87, disc loss: 0.006764587383294719, policy loss: 6.597521482419445
Experience 7, Iter 88, disc loss: 0.00930712588328636, policy loss: 6.054420213527554
Experience 7, Iter 89, disc loss: 0.009705537513845165, policy loss: 6.486691701355417
Experience 7, Iter 90, disc loss: 0.007414214431385187, policy loss: 6.395492503311746
Experience 7, Iter 91, disc loss: 0.007010643684090101, policy loss: 6.612650477457877
Experience 7, Iter 92, disc loss: 0.006608536207893835, policy loss: 6.470130384255761
Experience 7, Iter 93, disc loss: 0.00647218618292873, policy loss: 6.570130986242245
Experience 7, Iter 94, disc loss: 0.006531846184195051, policy loss: 6.656966991947731
Experience 7, Iter 95, disc loss: 0.00626228628434842, policy loss: 6.757267569877191
Experience 7, Iter 96, disc loss: 0.00583190967119512, policy loss: 7.087003165001246
Experience 7, Iter 97, disc loss: 0.021185786232301503, policy loss: 6.560176238222622
Experience 7, Iter 98, disc loss: 0.007510640250568312, policy loss: 6.413143681025373
Experience 7, Iter 99, disc loss: 0.008640972484169802, policy loss: 6.576782807528749
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.0616],
        [0.7103],
        [0.0107]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0166, 0.1283, 0.5193, 0.0148, 0.0039, 1.5813]],

        [[0.0166, 0.1283, 0.5193, 0.0148, 0.0039, 1.5813]],

        [[0.0166, 0.1283, 0.5193, 0.0148, 0.0039, 1.5813]],

        [[0.0166, 0.1283, 0.5193, 0.0148, 0.0039, 1.5813]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0131, 0.2466, 2.8411, 0.0430], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0131, 0.2466, 2.8411, 0.0430])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.957
Iter 2/2000 - Loss: 1.837
Iter 3/2000 - Loss: 1.828
Iter 4/2000 - Loss: 1.738
Iter 5/2000 - Loss: 1.764
Iter 6/2000 - Loss: 1.789
Iter 7/2000 - Loss: 1.729
Iter 8/2000 - Loss: 1.674
Iter 9/2000 - Loss: 1.674
Iter 10/2000 - Loss: 1.676
Iter 11/2000 - Loss: 1.639
Iter 12/2000 - Loss: 1.582
Iter 13/2000 - Loss: 1.533
Iter 14/2000 - Loss: 1.485
Iter 15/2000 - Loss: 1.417
Iter 16/2000 - Loss: 1.323
Iter 17/2000 - Loss: 1.213
Iter 18/2000 - Loss: 1.096
Iter 19/2000 - Loss: 0.967
Iter 20/2000 - Loss: 0.817
Iter 1981/2000 - Loss: -7.247
Iter 1982/2000 - Loss: -7.247
Iter 1983/2000 - Loss: -7.247
Iter 1984/2000 - Loss: -7.247
Iter 1985/2000 - Loss: -7.247
Iter 1986/2000 - Loss: -7.247
Iter 1987/2000 - Loss: -7.248
Iter 1988/2000 - Loss: -7.248
Iter 1989/2000 - Loss: -7.248
Iter 1990/2000 - Loss: -7.248
Iter 1991/2000 - Loss: -7.248
Iter 1992/2000 - Loss: -7.248
Iter 1993/2000 - Loss: -7.248
Iter 1994/2000 - Loss: -7.248
Iter 1995/2000 - Loss: -7.248
Iter 1996/2000 - Loss: -7.248
Iter 1997/2000 - Loss: -7.248
Iter 1998/2000 - Loss: -7.248
Iter 1999/2000 - Loss: -7.248
Iter 2000/2000 - Loss: -7.248
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.2256,  8.2037, 53.8375, 12.3439, 19.2837, 54.0779]],

        [[20.0390, 37.6997,  7.2718,  1.5253,  1.7817, 25.5336]],

        [[21.2626, 40.3115, 11.0637,  1.0419,  1.8844, 24.1346]],

        [[17.3225, 34.1521, 20.1943,  4.0034,  1.8083, 46.2985]]])
Signal Variance: tensor([ 0.1219,  1.9954, 17.6972,  0.8449])
Estimated target variance: tensor([0.0131, 0.2466, 2.8411, 0.0430])
N: 80
Signal to noise ratio: tensor([ 19.7974,  90.9088, 102.8801,  54.2916])
Bound on condition number: tensor([ 31356.0480, 661153.2572, 846746.7492, 235807.0958])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.0035575370605748623, policy loss: 10.22595927701563
Experience 8, Iter 1, disc loss: 0.00365053354212746, policy loss: 9.644070058220468
Experience 8, Iter 2, disc loss: 0.0036884810948700068, policy loss: 9.13523401479859
Experience 8, Iter 3, disc loss: 0.003572108309018735, policy loss: 9.378345810146286
Experience 8, Iter 4, disc loss: 0.003455396142343362, policy loss: 9.682246002217198
Experience 8, Iter 5, disc loss: 0.0033775504009302335, policy loss: 9.01257120522834
Experience 8, Iter 6, disc loss: 0.003231803092971818, policy loss: 8.815216874026639
Experience 8, Iter 7, disc loss: 0.0030536297386047602, policy loss: 8.831938243933521
Experience 8, Iter 8, disc loss: 0.00283797147766145, policy loss: 8.902787418330867
Experience 8, Iter 9, disc loss: 0.0027127207323436892, policy loss: 8.597107816090576
Experience 8, Iter 10, disc loss: 0.0025430555394504975, policy loss: 8.573627673231051
Experience 8, Iter 11, disc loss: 0.0023615486773147275, policy loss: 8.468968974451684
Experience 8, Iter 12, disc loss: 0.0021837743997010304, policy loss: 8.74659341096195
Experience 8, Iter 13, disc loss: 0.0020267019346060437, policy loss: 8.743115630582391
Experience 8, Iter 14, disc loss: 0.0018826239152776138, policy loss: 8.556405465985971
Experience 8, Iter 15, disc loss: 0.0018416303687152104, policy loss: 8.299972738303188
Experience 8, Iter 16, disc loss: 0.0017153644038883612, policy loss: 8.486415693187418
Experience 8, Iter 17, disc loss: 0.0015951407568696877, policy loss: 8.422558754786218
Experience 8, Iter 18, disc loss: 0.00150828296522461, policy loss: 8.473332872843809
Experience 8, Iter 19, disc loss: 0.0015327930714276398, policy loss: 8.213995967421777
Experience 8, Iter 20, disc loss: 0.0014699942268077072, policy loss: 8.093561289097348
Experience 8, Iter 21, disc loss: 0.0013539417939699597, policy loss: 8.353123721257564
Experience 8, Iter 22, disc loss: 0.0012414104189489485, policy loss: 8.382334262396856
Experience 8, Iter 23, disc loss: 0.0013517122583042158, policy loss: 7.996634936582063
Experience 8, Iter 24, disc loss: 0.0012741262162916803, policy loss: 8.050414790879405
Experience 8, Iter 25, disc loss: 0.0012466237259281462, policy loss: 8.022782572922472
Experience 8, Iter 26, disc loss: 0.0012495690252552196, policy loss: 7.914992292326708
Experience 8, Iter 27, disc loss: 0.001279440459474859, policy loss: 7.906668467592271
Experience 8, Iter 28, disc loss: 0.001324851775997628, policy loss: 7.813657560284376
Experience 8, Iter 29, disc loss: 0.001267456763768264, policy loss: 7.822444363078168
Experience 8, Iter 30, disc loss: 0.001179978302319591, policy loss: 8.021296106598438
Experience 8, Iter 31, disc loss: 0.0012372304069867765, policy loss: 7.748027416211359
Experience 8, Iter 32, disc loss: 0.0012505091402330404, policy loss: 7.718656182819661
Experience 8, Iter 33, disc loss: 0.0012528455749942994, policy loss: 7.787738109102281
Experience 8, Iter 34, disc loss: 0.0013062501912330608, policy loss: 7.606551321387894
Experience 8, Iter 35, disc loss: 0.0013614768225686206, policy loss: 7.584422299566539
Experience 8, Iter 36, disc loss: 0.0015411798138718613, policy loss: 7.378724011854391
Experience 8, Iter 37, disc loss: 0.0012026929451826348, policy loss: 7.625561830445582
Experience 8, Iter 38, disc loss: 0.0012348340820204197, policy loss: 7.566037252495509
Experience 8, Iter 39, disc loss: 0.001221430494419988, policy loss: 8.09770780251897
Experience 8, Iter 40, disc loss: 0.0011369293579809154, policy loss: 7.762555451043401
Experience 8, Iter 41, disc loss: 0.0012866340073246843, policy loss: 7.548497846841156
Experience 8, Iter 42, disc loss: 0.001277487779202864, policy loss: 7.541436993697131
Experience 8, Iter 43, disc loss: 0.0011221645349465542, policy loss: 7.818462111810165
Experience 8, Iter 44, disc loss: 0.0012581310131764586, policy loss: 7.492267807505491
Experience 8, Iter 45, disc loss: 0.001121810886235574, policy loss: 7.7412732442637715
Experience 8, Iter 46, disc loss: 0.001177235425841086, policy loss: 7.7359044894952405
Experience 8, Iter 47, disc loss: 0.0012214838043583774, policy loss: 7.583886853524547
Experience 8, Iter 48, disc loss: 0.0011390303652812107, policy loss: 7.662921915419756
Experience 8, Iter 49, disc loss: 0.0009593351622761675, policy loss: 8.030378210609909
Experience 8, Iter 50, disc loss: 0.0010382947792493278, policy loss: 7.854557888759628
Experience 8, Iter 51, disc loss: 0.0012325597872795118, policy loss: 7.60210063614489
Experience 8, Iter 52, disc loss: 0.0012513514930773755, policy loss: 7.522029614060365
Experience 8, Iter 53, disc loss: 0.0013178216019586785, policy loss: 7.432898981098198
Experience 8, Iter 54, disc loss: 0.0012078563055587187, policy loss: 7.5567292619232145
Experience 8, Iter 55, disc loss: 0.0010798359818611207, policy loss: 7.768935061082546
Experience 8, Iter 56, disc loss: 0.001217207555124723, policy loss: 7.515540383130402
Experience 8, Iter 57, disc loss: 0.0011189121758278912, policy loss: 7.62941729516908
Experience 8, Iter 58, disc loss: 0.001047964841095221, policy loss: 7.623552995867582
Experience 8, Iter 59, disc loss: 0.0012396722171688845, policy loss: 7.592968365216634
Experience 8, Iter 60, disc loss: 0.00108468394515103, policy loss: 7.643180923103647
Experience 8, Iter 61, disc loss: 0.0011646281115362278, policy loss: 7.771068183333021
Experience 8, Iter 62, disc loss: 0.0011536774317119497, policy loss: 7.488575220865387
Experience 8, Iter 63, disc loss: 0.0011966982955213396, policy loss: 7.7212271654336
Experience 8, Iter 64, disc loss: 0.0012491075831175826, policy loss: 7.616096368457618
Experience 8, Iter 65, disc loss: 0.0011217738985540727, policy loss: 7.685858128065187
Experience 8, Iter 66, disc loss: 0.0012103134463150305, policy loss: 7.607598183391033
Experience 8, Iter 67, disc loss: 0.0011820695874590588, policy loss: 7.695543141516852
Experience 8, Iter 68, disc loss: 0.0011739428322897314, policy loss: 7.529733506185545
Experience 8, Iter 69, disc loss: 0.0011794526708869967, policy loss: 7.69608168270621
Experience 8, Iter 70, disc loss: 0.0010540553229204472, policy loss: 7.693486088109005
Experience 8, Iter 71, disc loss: 0.0011011095945915614, policy loss: 7.714777254473518
Experience 8, Iter 72, disc loss: 0.001022680880208778, policy loss: 7.900282730348676
Experience 8, Iter 73, disc loss: 0.0010893037445154337, policy loss: 7.725428646865676
Experience 8, Iter 74, disc loss: 0.0010810146875226813, policy loss: 7.841525074551131
Experience 8, Iter 75, disc loss: 0.0009947277082839974, policy loss: 8.007227845877487
Experience 8, Iter 76, disc loss: 0.0012039390490848245, policy loss: 7.7145273001458
Experience 8, Iter 77, disc loss: 0.0009858523742154587, policy loss: 7.94548770424223
Experience 8, Iter 78, disc loss: 0.001074866800024938, policy loss: 7.73056923610184
Experience 8, Iter 79, disc loss: 0.001070706590023717, policy loss: 7.65891093035628
Experience 8, Iter 80, disc loss: 0.0010932512822167807, policy loss: 7.626951825320165
Experience 8, Iter 81, disc loss: 0.0010224940106826042, policy loss: 7.936134388245991
Experience 8, Iter 82, disc loss: 0.0011254841118081668, policy loss: 7.7045356911507845
Experience 8, Iter 83, disc loss: 0.001009198756400202, policy loss: 7.771448696960216
Experience 8, Iter 84, disc loss: 0.0009920266824997202, policy loss: 7.809933681639654
Experience 8, Iter 85, disc loss: 0.0010432491839891694, policy loss: 7.781899920120754
Experience 8, Iter 86, disc loss: 0.0010416927938339399, policy loss: 7.790304323991519
Experience 8, Iter 87, disc loss: 0.0009654776317245301, policy loss: 7.906515967718237
Experience 8, Iter 88, disc loss: 0.0010820951634570555, policy loss: 7.640085271726639
Experience 8, Iter 89, disc loss: 0.0011956128783803162, policy loss: 7.53938553371038
Experience 8, Iter 90, disc loss: 0.001110851572036851, policy loss: 7.59624817375878
Experience 8, Iter 91, disc loss: 0.0010587016500790737, policy loss: 7.842433067081617
Experience 8, Iter 92, disc loss: 0.001165007581982015, policy loss: 7.620278081533016
Experience 8, Iter 93, disc loss: 0.0011498353869962906, policy loss: 7.658596836070272
Experience 8, Iter 94, disc loss: 0.0011603430885887436, policy loss: 7.96425953834199
Experience 8, Iter 95, disc loss: 0.000935686826221155, policy loss: 8.034936222947689
Experience 8, Iter 96, disc loss: 0.0008101136373465493, policy loss: 8.218892567259475
Experience 8, Iter 97, disc loss: 0.0007690787663640858, policy loss: 8.405126122801223
Experience 8, Iter 98, disc loss: 0.0007675737858432227, policy loss: 8.500199201047092
Experience 8, Iter 99, disc loss: 0.0009801431576159964, policy loss: 7.8543220959610345
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0811],
        [0.9177],
        [0.0120]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0156, 0.1262, 0.5988, 0.0158, 0.0036, 1.9192]],

        [[0.0156, 0.1262, 0.5988, 0.0158, 0.0036, 1.9192]],

        [[0.0156, 0.1262, 0.5988, 0.0158, 0.0036, 1.9192]],

        [[0.0156, 0.1262, 0.5988, 0.0158, 0.0036, 1.9192]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0128, 0.3246, 3.6708, 0.0480], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0128, 0.3246, 3.6708, 0.0480])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.226
Iter 2/2000 - Loss: 2.168
Iter 3/2000 - Loss: 2.116
Iter 4/2000 - Loss: 2.044
Iter 5/2000 - Loss: 2.085
Iter 6/2000 - Loss: 2.088
Iter 7/2000 - Loss: 2.022
Iter 8/2000 - Loss: 1.990
Iter 9/2000 - Loss: 1.998
Iter 10/2000 - Loss: 1.983
Iter 11/2000 - Loss: 1.938
Iter 12/2000 - Loss: 1.892
Iter 13/2000 - Loss: 1.853
Iter 14/2000 - Loss: 1.802
Iter 15/2000 - Loss: 1.725
Iter 16/2000 - Loss: 1.628
Iter 17/2000 - Loss: 1.520
Iter 18/2000 - Loss: 1.402
Iter 19/2000 - Loss: 1.264
Iter 20/2000 - Loss: 1.101
Iter 1981/2000 - Loss: -7.392
Iter 1982/2000 - Loss: -7.392
Iter 1983/2000 - Loss: -7.392
Iter 1984/2000 - Loss: -7.392
Iter 1985/2000 - Loss: -7.392
Iter 1986/2000 - Loss: -7.392
Iter 1987/2000 - Loss: -7.392
Iter 1988/2000 - Loss: -7.392
Iter 1989/2000 - Loss: -7.392
Iter 1990/2000 - Loss: -7.392
Iter 1991/2000 - Loss: -7.392
Iter 1992/2000 - Loss: -7.392
Iter 1993/2000 - Loss: -7.392
Iter 1994/2000 - Loss: -7.393
Iter 1995/2000 - Loss: -7.393
Iter 1996/2000 - Loss: -7.393
Iter 1997/2000 - Loss: -7.393
Iter 1998/2000 - Loss: -7.393
Iter 1999/2000 - Loss: -7.393
Iter 2000/2000 - Loss: -7.393
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[14.8823,  7.7455, 50.4828,  9.8763, 17.7779, 58.2622]],

        [[20.2129, 38.0887,  8.3512,  1.4994,  1.5107, 25.5080]],

        [[21.5517, 42.0819,  8.8470,  1.0883,  2.2433, 23.5505]],

        [[16.6340, 35.9422, 20.7172,  4.7005,  1.9478, 46.4287]]])
Signal Variance: tensor([ 0.1101,  1.9114, 16.4543,  0.8713])
Estimated target variance: tensor([0.0128, 0.3246, 3.6708, 0.0480])
N: 90
Signal to noise ratio: tensor([ 19.0705,  90.6220, 101.7462,  52.9105])
Bound on condition number: tensor([ 32732.4899, 739111.9462, 931707.1837, 251957.9669])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0013864152950132176, policy loss: 7.407169494663144
Experience 9, Iter 1, disc loss: 0.0012126290351646349, policy loss: 7.652160614012139
Experience 9, Iter 2, disc loss: 0.0012881741850712062, policy loss: 7.370404085291083
Experience 9, Iter 3, disc loss: 0.0012846638320873967, policy loss: 7.422232445381161
Experience 9, Iter 4, disc loss: 0.001435876279355204, policy loss: 7.379067295577254
Experience 9, Iter 5, disc loss: 0.0011904377463312732, policy loss: 7.6769600286661195
Experience 9, Iter 6, disc loss: 0.0013644233576481756, policy loss: 7.1839131982408695
Experience 9, Iter 7, disc loss: 0.001296273371120051, policy loss: 7.370813800885619
Experience 9, Iter 8, disc loss: 0.0013309547057891733, policy loss: 7.446936006864189
Experience 9, Iter 9, disc loss: 0.0012733250815758525, policy loss: 7.277116171921624
Experience 9, Iter 10, disc loss: 0.0012312805779932186, policy loss: 7.3688518998685275
Experience 9, Iter 11, disc loss: 0.0012262225963352994, policy loss: 7.574194223136814
Experience 9, Iter 12, disc loss: 0.0011250578022681478, policy loss: 7.688674766184421
Experience 9, Iter 13, disc loss: 0.0010674250198627973, policy loss: 7.781423366820384
Experience 9, Iter 14, disc loss: 0.0011005879435852007, policy loss: 7.599875655101084
Experience 9, Iter 15, disc loss: 0.000937247652705426, policy loss: 7.925376754089121
Experience 9, Iter 16, disc loss: 0.001071228226785804, policy loss: 7.67317887004306
Experience 9, Iter 17, disc loss: 0.0008026709930339423, policy loss: 8.336328332609996
Experience 9, Iter 18, disc loss: 0.0007672888290413724, policy loss: 8.28027165564426
Experience 9, Iter 19, disc loss: 0.0008284109848931348, policy loss: 8.063355524752772
Experience 9, Iter 20, disc loss: 0.0009412684736510024, policy loss: 7.7998889359670835
Experience 9, Iter 21, disc loss: 0.00088382278767085, policy loss: 8.124511859068793
Experience 9, Iter 22, disc loss: 0.0008718684979947837, policy loss: 8.155284517757778
Experience 9, Iter 23, disc loss: 0.0009721258664240651, policy loss: 7.944768622153223
Experience 9, Iter 24, disc loss: 0.0009619875044023106, policy loss: 7.7851082892291545
Experience 9, Iter 25, disc loss: 0.0010222033740025912, policy loss: 7.6470683794546845
Experience 9, Iter 26, disc loss: 0.0010066741760601453, policy loss: 7.68578099749031
Experience 9, Iter 27, disc loss: 0.0010380077478409676, policy loss: 7.540428004999322
Experience 9, Iter 28, disc loss: 0.000992174560500104, policy loss: 7.756659690368762
Experience 9, Iter 29, disc loss: 0.0010572193444063332, policy loss: 7.581405026729426
Experience 9, Iter 30, disc loss: 0.0011588100452906017, policy loss: 7.405844751768885
Experience 9, Iter 31, disc loss: 0.0009342855407652988, policy loss: 7.884363387650669
Experience 9, Iter 32, disc loss: 0.0010252465333580852, policy loss: 7.661452435515663
Experience 9, Iter 33, disc loss: 0.0011586447457737872, policy loss: 7.490512482265749
Experience 9, Iter 34, disc loss: 0.001153525523770204, policy loss: 7.44844481045679
Experience 9, Iter 35, disc loss: 0.0011135081202891352, policy loss: 7.627862539379668
Experience 9, Iter 36, disc loss: 0.0012019728936242071, policy loss: 7.421604030982789
Experience 9, Iter 37, disc loss: 0.0012813879054714228, policy loss: 7.27521747359855
Experience 9, Iter 38, disc loss: 0.0010708350644137628, policy loss: 7.578271669716119
Experience 9, Iter 39, disc loss: 0.0011982097337078087, policy loss: 7.468820109836534
Experience 9, Iter 40, disc loss: 0.0012328465425484455, policy loss: 7.515776163126699
Experience 9, Iter 41, disc loss: 0.0011446456427529836, policy loss: 7.525756485245704
Experience 9, Iter 42, disc loss: 0.0011894799627973954, policy loss: 7.743208079622064
Experience 9, Iter 43, disc loss: 0.0012056561648393326, policy loss: 7.562258112035051
Experience 9, Iter 44, disc loss: 0.0012247022448587034, policy loss: 7.4044012523409775
Experience 9, Iter 45, disc loss: 0.0009903843833409122, policy loss: 7.81605335518557
Experience 9, Iter 46, disc loss: 0.0009707710714443908, policy loss: 7.909805977084389
Experience 9, Iter 47, disc loss: 0.0009872983048836467, policy loss: 7.959169789720618
Experience 9, Iter 48, disc loss: 0.0012144237065682885, policy loss: 7.765900061283514
Experience 9, Iter 49, disc loss: 0.0009307072241175145, policy loss: 8.389643107358076
Experience 9, Iter 50, disc loss: 0.0010107581671522734, policy loss: 7.937957814920776
Experience 9, Iter 51, disc loss: 0.0008668574086927674, policy loss: 8.221352606412802
Experience 9, Iter 52, disc loss: 0.0007650440569781259, policy loss: 8.528933883889628
Experience 9, Iter 53, disc loss: 0.0006328085713366513, policy loss: 8.955657765749805
Experience 9, Iter 54, disc loss: 0.0007389398457563152, policy loss: 8.706498517920036
Experience 9, Iter 55, disc loss: 0.0007000036039879485, policy loss: 8.464686090290956
Experience 9, Iter 56, disc loss: 0.000787294686090743, policy loss: 8.361716034285479
Experience 9, Iter 57, disc loss: 0.0008024100912716271, policy loss: 8.192511820238954
Experience 9, Iter 58, disc loss: 0.0006915353822593978, policy loss: 8.367964471397592
Experience 9, Iter 59, disc loss: 0.0006508111324922027, policy loss: 8.648059394836704
Experience 9, Iter 60, disc loss: 0.0006416151123962844, policy loss: 8.799505962500294
Experience 9, Iter 61, disc loss: 0.0007697550285920892, policy loss: 8.293706648392327
Experience 9, Iter 62, disc loss: 0.0009820396052169459, policy loss: 7.930572478585399
Experience 9, Iter 63, disc loss: 0.0008786766346415243, policy loss: 8.105042876157961
Experience 9, Iter 64, disc loss: 0.0008392903068149232, policy loss: 8.160816993932617
Experience 9, Iter 65, disc loss: 0.0009376003280745636, policy loss: 7.8271628417005426
Experience 9, Iter 66, disc loss: 0.0011116334883073593, policy loss: 7.481170993770425
Experience 9, Iter 67, disc loss: 0.0008695380692508772, policy loss: 8.082844451317383
Experience 9, Iter 68, disc loss: 0.0008485769278547641, policy loss: 8.324485365453263
Experience 9, Iter 69, disc loss: 0.0010603233380416296, policy loss: 7.601247734914042
Experience 9, Iter 70, disc loss: 0.0008249461870866873, policy loss: 8.037668499474124
Experience 9, Iter 71, disc loss: 0.0008285556649995779, policy loss: 7.9502346843319085
Experience 9, Iter 72, disc loss: 0.0007805482622991937, policy loss: 8.200114076237757
Experience 9, Iter 73, disc loss: 0.0009267849566581041, policy loss: 7.843387045750002
Experience 9, Iter 74, disc loss: 0.0010282516217408385, policy loss: 7.546155176763024
Experience 9, Iter 75, disc loss: 0.001015388558520345, policy loss: 7.702806561026875
Experience 9, Iter 76, disc loss: 0.0009497132629890838, policy loss: 7.858509838021083
Experience 9, Iter 77, disc loss: 0.000968034285651152, policy loss: 7.957964915540726
Experience 9, Iter 78, disc loss: 0.0009682913659538213, policy loss: 7.883610411860492
Experience 9, Iter 79, disc loss: 0.0011228379480003985, policy loss: 7.477942309790876
Experience 9, Iter 80, disc loss: 0.0009382419138826128, policy loss: 7.939827629743045
Experience 9, Iter 81, disc loss: 0.0008546472217419786, policy loss: 8.047572673143291
Experience 9, Iter 82, disc loss: 0.0009350547581305352, policy loss: 7.980465483143151
Experience 9, Iter 83, disc loss: 0.0009087372428089358, policy loss: 7.821722374847937
Experience 9, Iter 84, disc loss: 0.0011356455977252328, policy loss: 7.640505613713032
Experience 9, Iter 85, disc loss: 0.0009280208354116116, policy loss: 8.03204473252763
Experience 9, Iter 86, disc loss: 0.0011443710035637347, policy loss: 7.511227900731358
Experience 9, Iter 87, disc loss: 0.0011903399116545636, policy loss: 7.509450188637037
Experience 9, Iter 88, disc loss: 0.0010770445062555122, policy loss: 7.562555568387812
Experience 9, Iter 89, disc loss: 0.001011666764550758, policy loss: 7.609990526695238
Experience 9, Iter 90, disc loss: 0.0010297723522149852, policy loss: 7.728976395436064
Experience 9, Iter 91, disc loss: 0.0009810987241430963, policy loss: 7.864605139560594
Experience 9, Iter 92, disc loss: 0.0010732056241602003, policy loss: 7.668425849727066
Experience 9, Iter 93, disc loss: 0.001187347593464492, policy loss: 7.587126833040511
Experience 9, Iter 94, disc loss: 0.0011231427826823642, policy loss: 7.630851055443207
Experience 9, Iter 95, disc loss: 0.0009829441458489995, policy loss: 7.767100108451758
Experience 9, Iter 96, disc loss: 0.0009460602812064946, policy loss: 7.9177787489306475
Experience 9, Iter 97, disc loss: 0.0009757996091218697, policy loss: 7.634159417711221
Experience 9, Iter 98, disc loss: 0.001104954088106815, policy loss: 7.594427904710658
Experience 9, Iter 99, disc loss: 0.001096694146235534, policy loss: 7.777538420528748
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.1001],
        [1.0986],
        [0.0130]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0150, 0.1235, 0.6631, 0.0169, 0.0034, 2.3035]],

        [[0.0150, 0.1235, 0.6631, 0.0169, 0.0034, 2.3035]],

        [[0.0150, 0.1235, 0.6631, 0.0169, 0.0034, 2.3035]],

        [[0.0150, 0.1235, 0.6631, 0.0169, 0.0034, 2.3035]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0123, 0.4005, 4.3945, 0.0520], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0123, 0.4005, 4.3945, 0.0520])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.426
Iter 2/2000 - Loss: 2.406
Iter 3/2000 - Loss: 2.325
Iter 4/2000 - Loss: 2.265
Iter 5/2000 - Loss: 2.305
Iter 6/2000 - Loss: 2.289
Iter 7/2000 - Loss: 2.218
Iter 8/2000 - Loss: 2.183
Iter 9/2000 - Loss: 2.178
Iter 10/2000 - Loss: 2.143
Iter 11/2000 - Loss: 2.081
Iter 12/2000 - Loss: 2.021
Iter 13/2000 - Loss: 1.964
Iter 14/2000 - Loss: 1.886
Iter 15/2000 - Loss: 1.781
Iter 16/2000 - Loss: 1.657
Iter 17/2000 - Loss: 1.525
Iter 18/2000 - Loss: 1.381
Iter 19/2000 - Loss: 1.219
Iter 20/2000 - Loss: 1.035
Iter 1981/2000 - Loss: -7.468
Iter 1982/2000 - Loss: -7.468
Iter 1983/2000 - Loss: -7.468
Iter 1984/2000 - Loss: -7.468
Iter 1985/2000 - Loss: -7.468
Iter 1986/2000 - Loss: -7.468
Iter 1987/2000 - Loss: -7.468
Iter 1988/2000 - Loss: -7.468
Iter 1989/2000 - Loss: -7.468
Iter 1990/2000 - Loss: -7.468
Iter 1991/2000 - Loss: -7.468
Iter 1992/2000 - Loss: -7.468
Iter 1993/2000 - Loss: -7.468
Iter 1994/2000 - Loss: -7.468
Iter 1995/2000 - Loss: -7.468
Iter 1996/2000 - Loss: -7.468
Iter 1997/2000 - Loss: -7.468
Iter 1998/2000 - Loss: -7.468
Iter 1999/2000 - Loss: -7.468
Iter 2000/2000 - Loss: -7.469
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[15.3533,  7.9686, 51.9658, 10.2225, 16.9102, 58.1033]],

        [[19.2117, 37.4502,  9.1493,  1.4745,  1.5180, 28.8940]],

        [[21.3672, 40.5621,  8.3235,  1.1194,  2.6590, 26.1046]],

        [[16.5608, 32.6917, 20.3898,  4.7583,  1.8193, 47.7654]]])
Signal Variance: tensor([ 0.1149,  2.2351, 20.0235,  0.7513])
Estimated target variance: tensor([0.0123, 0.4005, 4.3945, 0.0520])
N: 100
Signal to noise ratio: tensor([ 19.5670,  94.7118, 107.6191,  47.0268])
Bound on condition number: tensor([  38287.8056,  897032.8966, 1158188.2357,  221153.3878])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0011027061601773386, policy loss: 7.830500906923786
Experience 10, Iter 1, disc loss: 0.0009821545800666625, policy loss: 7.984043156522993
Experience 10, Iter 2, disc loss: 0.0009358532844644245, policy loss: 7.952915574564546
Experience 10, Iter 3, disc loss: 0.0009432701348384507, policy loss: 7.860891140288782
Experience 10, Iter 4, disc loss: 0.0011590327590057289, policy loss: 7.724858796734386
Experience 10, Iter 5, disc loss: 0.0008903342459501549, policy loss: 8.098929940881561
Experience 10, Iter 6, disc loss: 0.0011878780160337857, policy loss: 7.673352859507983
Experience 10, Iter 7, disc loss: 0.0009831253368890271, policy loss: 7.785095986801064
Experience 10, Iter 8, disc loss: 0.0008875554867831383, policy loss: 8.126529637525763
Experience 10, Iter 9, disc loss: 0.0010179720019126565, policy loss: 7.7610125717311425
Experience 10, Iter 10, disc loss: 0.0010743490590149768, policy loss: 8.044527883217592
Experience 10, Iter 11, disc loss: 0.0009610844635575048, policy loss: 8.103239280800281
Experience 10, Iter 12, disc loss: 0.0008992608978594757, policy loss: 7.927045685706322
Experience 10, Iter 13, disc loss: 0.0006304064754269999, policy loss: 8.72217569323966
Experience 10, Iter 14, disc loss: 0.000626987800803903, policy loss: 8.657103371356598
Experience 10, Iter 15, disc loss: 0.0005803393203514784, policy loss: 8.89147001035117
Experience 10, Iter 16, disc loss: 0.0006425401945548943, policy loss: 8.519943646138296
Experience 10, Iter 17, disc loss: 0.0006332588391570158, policy loss: 8.711219369824526
Experience 10, Iter 18, disc loss: 0.000739159758986173, policy loss: 8.166531856997885
Experience 10, Iter 19, disc loss: 0.0007834105204683491, policy loss: 8.00835036309173
Experience 10, Iter 20, disc loss: 0.0008690384829523373, policy loss: 7.950271872119954
Experience 10, Iter 21, disc loss: 0.0007377099383227652, policy loss: 8.400690533218969
Experience 10, Iter 22, disc loss: 0.0007671777850300065, policy loss: 8.14333875527786
Experience 10, Iter 23, disc loss: 0.0008728192463720824, policy loss: 7.850276706387741
Experience 10, Iter 24, disc loss: 0.0008767429337269515, policy loss: 7.8323074505584245
Experience 10, Iter 25, disc loss: 0.0007592158964839842, policy loss: 8.075138432524811
Experience 10, Iter 26, disc loss: 0.0008445241586867863, policy loss: 7.873018906539933
Experience 10, Iter 27, disc loss: 0.0008490188868553883, policy loss: 7.902061404805018
Experience 10, Iter 28, disc loss: 0.000884041270941889, policy loss: 8.05969989482927
Experience 10, Iter 29, disc loss: 0.0008662585697053515, policy loss: 8.009810106551823
Experience 10, Iter 30, disc loss: 0.0009216499825481829, policy loss: 7.927249895717037
Experience 10, Iter 31, disc loss: 0.0009445486519569218, policy loss: 7.8391546810188295
Experience 10, Iter 32, disc loss: 0.0008575364734433034, policy loss: 7.914907629900661
Experience 10, Iter 33, disc loss: 0.0009230709118529454, policy loss: 7.933731923501338
Experience 10, Iter 34, disc loss: 0.001094002120719138, policy loss: 7.5821603533856985
Experience 10, Iter 35, disc loss: 0.0011157086608093742, policy loss: 7.907616396764909
Experience 10, Iter 36, disc loss: 0.0010689891170812507, policy loss: 7.755579371424892
Experience 10, Iter 37, disc loss: 0.001202591230829466, policy loss: 7.489039092668707
Experience 10, Iter 38, disc loss: 0.001038049919875573, policy loss: 7.7263594286159645
Experience 10, Iter 39, disc loss: 0.0009435224397023559, policy loss: 7.870305610778108
Experience 10, Iter 40, disc loss: 0.0009814152774673345, policy loss: 7.807254723129553
Experience 10, Iter 41, disc loss: 0.0010538299663274764, policy loss: 7.72621069435241
Experience 10, Iter 42, disc loss: 0.0010508310589930864, policy loss: 7.662275876049268
Experience 10, Iter 43, disc loss: 0.0011221057833052956, policy loss: 7.834611878651141
Experience 10, Iter 44, disc loss: 0.0010332351568817862, policy loss: 7.664028687349074
Experience 10, Iter 45, disc loss: 0.000893458370217021, policy loss: 8.070550877528959
Experience 10, Iter 46, disc loss: 0.0010194944689209653, policy loss: 7.9195970558970705
Experience 10, Iter 47, disc loss: 0.0010498531152194487, policy loss: 7.589190460315123
Experience 10, Iter 48, disc loss: 0.0009796901088032638, policy loss: 7.674181497806884
Experience 10, Iter 49, disc loss: 0.0009353765530584093, policy loss: 7.910981948433985
Experience 10, Iter 50, disc loss: 0.0010498118701230694, policy loss: 7.831519669106424
Experience 10, Iter 51, disc loss: 0.0010570714153964051, policy loss: 7.598887746686795
Experience 10, Iter 52, disc loss: 0.00095427675172149, policy loss: 7.821138645156738
Experience 10, Iter 53, disc loss: 0.0010130761296542228, policy loss: 7.831211133046196
Experience 10, Iter 54, disc loss: 0.0010273655580089703, policy loss: 7.692936245307548
Experience 10, Iter 55, disc loss: 0.000874870206094884, policy loss: 8.02705473645656
Experience 10, Iter 56, disc loss: 0.0009726099889989697, policy loss: 7.89754319499843
Experience 10, Iter 57, disc loss: 0.0009819821511683519, policy loss: 7.842698971320989
Experience 10, Iter 58, disc loss: 0.0009677267341009264, policy loss: 7.678341239551456
Experience 10, Iter 59, disc loss: 0.0008628022430421759, policy loss: 8.041810945381744
Experience 10, Iter 60, disc loss: 0.0009713021093807508, policy loss: 8.029097353186337
Experience 10, Iter 61, disc loss: 0.0009598894360488223, policy loss: 7.887983990479573
Experience 10, Iter 62, disc loss: 0.0008994199761872559, policy loss: 7.936734291559503
Experience 10, Iter 63, disc loss: 0.0008376446061994856, policy loss: 7.918597932861108
Experience 10, Iter 64, disc loss: 0.0009419483036270683, policy loss: 7.870241606358175
Experience 10, Iter 65, disc loss: 0.0009421768450308287, policy loss: 7.8408101762983655
Experience 10, Iter 66, disc loss: 0.0008653927386005336, policy loss: 8.046330528388324
Experience 10, Iter 67, disc loss: 0.0009549434676435027, policy loss: 7.957462547524242
Experience 10, Iter 68, disc loss: 0.0007919578255804926, policy loss: 8.029607842576482
Experience 10, Iter 69, disc loss: 0.0007849869741157642, policy loss: 8.053289507121185
Experience 10, Iter 70, disc loss: 0.0008322727551288084, policy loss: 7.969146571636781
Experience 10, Iter 71, disc loss: 0.0008798028725489066, policy loss: 7.982129778849317
Experience 10, Iter 72, disc loss: 0.0008937676170923212, policy loss: 8.352888380929944
Experience 10, Iter 73, disc loss: 0.0009573681565317336, policy loss: 7.665639041533857
Experience 10, Iter 74, disc loss: 0.0008189927288278399, policy loss: 8.032581504809109
Experience 10, Iter 75, disc loss: 0.0007970650264846022, policy loss: 8.116409018691195
Experience 10, Iter 76, disc loss: 0.0008520233928621759, policy loss: 8.014360537409758
Experience 10, Iter 77, disc loss: 0.0008505488155162132, policy loss: 8.056350031316263
Experience 10, Iter 78, disc loss: 0.0008846786433221453, policy loss: 7.96673508145534
Experience 10, Iter 79, disc loss: 0.0009452093204314071, policy loss: 7.904312340167593
Experience 10, Iter 80, disc loss: 0.0009273919638711218, policy loss: 7.722269072644407
Experience 10, Iter 81, disc loss: 0.0009008948087904585, policy loss: 7.902641732421983
Experience 10, Iter 82, disc loss: 0.0009303233792585277, policy loss: 7.852526476110689
Experience 10, Iter 83, disc loss: 0.0009191832277927782, policy loss: 7.886654831489361
Experience 10, Iter 84, disc loss: 0.0009520057277295611, policy loss: 7.955714560030895
Experience 10, Iter 85, disc loss: 0.0009085320807821319, policy loss: 7.884979399640311
Experience 10, Iter 86, disc loss: 0.0009050677437679894, policy loss: 7.833510539368551
Experience 10, Iter 87, disc loss: 0.0008497422139364984, policy loss: 7.96471490629286
Experience 10, Iter 88, disc loss: 0.0008951088280677923, policy loss: 7.86003930681048
Experience 10, Iter 89, disc loss: 0.0008960960205226973, policy loss: 7.897203808433124
Experience 10, Iter 90, disc loss: 0.0008992987166795272, policy loss: 7.970788646115256
Experience 10, Iter 91, disc loss: 0.0008323430123073669, policy loss: 8.023803465702741
Experience 10, Iter 92, disc loss: 0.0009093192468266736, policy loss: 7.833505986339429
Experience 10, Iter 93, disc loss: 0.0008884089638046916, policy loss: 7.81689262337893
Experience 10, Iter 94, disc loss: 0.0009838137340719474, policy loss: 7.683613243039671
Experience 10, Iter 95, disc loss: 0.0009475103431308893, policy loss: 7.816621983641444
Experience 10, Iter 96, disc loss: 0.0009393719078204762, policy loss: 7.893591136552773
Experience 10, Iter 97, disc loss: 0.0008119128137605533, policy loss: 8.217164884134391
Experience 10, Iter 98, disc loss: 0.0008536656352423561, policy loss: 7.996695875691314
Experience 10, Iter 99, disc loss: 0.0007780681325017828, policy loss: 8.215706897876306
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1100],
        [1.1829],
        [0.0135]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0147, 0.1220, 0.6938, 0.0174, 0.0032, 2.5377]],

        [[0.0147, 0.1220, 0.6938, 0.0174, 0.0032, 2.5377]],

        [[0.0147, 0.1220, 0.6938, 0.0174, 0.0032, 2.5377]],

        [[0.0147, 0.1220, 0.6938, 0.0174, 0.0032, 2.5377]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0120, 0.4402, 4.7315, 0.0541], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0120, 0.4402, 4.7315, 0.0541])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.496
Iter 2/2000 - Loss: 2.516
Iter 3/2000 - Loss: 2.395
Iter 4/2000 - Loss: 2.349
Iter 5/2000 - Loss: 2.385
Iter 6/2000 - Loss: 2.347
Iter 7/2000 - Loss: 2.265
Iter 8/2000 - Loss: 2.219
Iter 9/2000 - Loss: 2.197
Iter 10/2000 - Loss: 2.142
Iter 11/2000 - Loss: 2.058
Iter 12/2000 - Loss: 1.977
Iter 13/2000 - Loss: 1.899
Iter 14/2000 - Loss: 1.798
Iter 15/2000 - Loss: 1.668
Iter 16/2000 - Loss: 1.521
Iter 17/2000 - Loss: 1.367
Iter 18/2000 - Loss: 1.203
Iter 19/2000 - Loss: 1.020
Iter 20/2000 - Loss: 0.817
Iter 1981/2000 - Loss: -7.633
Iter 1982/2000 - Loss: -7.633
Iter 1983/2000 - Loss: -7.633
Iter 1984/2000 - Loss: -7.633
Iter 1985/2000 - Loss: -7.633
Iter 1986/2000 - Loss: -7.633
Iter 1987/2000 - Loss: -7.633
Iter 1988/2000 - Loss: -7.634
Iter 1989/2000 - Loss: -7.634
Iter 1990/2000 - Loss: -7.634
Iter 1991/2000 - Loss: -7.634
Iter 1992/2000 - Loss: -7.634
Iter 1993/2000 - Loss: -7.634
Iter 1994/2000 - Loss: -7.634
Iter 1995/2000 - Loss: -7.634
Iter 1996/2000 - Loss: -7.634
Iter 1997/2000 - Loss: -7.634
Iter 1998/2000 - Loss: -7.634
Iter 1999/2000 - Loss: -7.634
Iter 2000/2000 - Loss: -7.634
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.3730,  7.7242, 51.1137, 10.1398, 15.8713, 56.4340]],

        [[17.8118, 35.0387,  9.1008,  1.4228,  1.6781, 30.8534]],

        [[19.3699, 38.7887,  8.3768,  1.1219,  2.7888, 28.3988]],

        [[15.7006, 31.4082, 22.0123,  4.7975,  1.7113, 48.0263]]])
Signal Variance: tensor([ 0.1046,  2.4980, 21.9778,  0.8035])
Estimated target variance: tensor([0.0120, 0.4402, 4.7315, 0.0541])
N: 110
Signal to noise ratio: tensor([ 18.3704, 101.9411, 113.3889,  50.2839])
Bound on condition number: tensor([  37122.7094, 1143119.6992, 1414274.9490,  278133.0635])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0007798417864807779, policy loss: 8.105494041931129
Experience 11, Iter 1, disc loss: 0.0008653207960642647, policy loss: 7.930050112431374
Experience 11, Iter 2, disc loss: 0.000893094220995046, policy loss: 7.900422499014543
Experience 11, Iter 3, disc loss: 0.0008970962714357533, policy loss: 7.8624735738600195
Experience 11, Iter 4, disc loss: 0.0008512595214092166, policy loss: 8.093648764033661
Experience 11, Iter 5, disc loss: 0.0006952034973238529, policy loss: 8.39768100670063
Experience 11, Iter 6, disc loss: 0.000733353018924435, policy loss: 8.148548565397908
Experience 11, Iter 7, disc loss: 0.0008386386070476583, policy loss: 8.003589648883057
Experience 11, Iter 8, disc loss: 0.0008267503499000813, policy loss: 7.965668681984348
Experience 11, Iter 9, disc loss: 0.0008689631716404819, policy loss: 7.857064474111137
Experience 11, Iter 10, disc loss: 0.000888751358228096, policy loss: 7.847139630436299
Experience 11, Iter 11, disc loss: 0.0008195540936261148, policy loss: 7.9736058432722
Experience 11, Iter 12, disc loss: 0.0008129602252421955, policy loss: 8.029520640978886
Experience 11, Iter 13, disc loss: 0.0007590076231679364, policy loss: 8.180137514665642
Experience 11, Iter 14, disc loss: 0.0008075220356817223, policy loss: 8.003227646767437
Experience 11, Iter 15, disc loss: 0.0008290591432818905, policy loss: 8.023525316059471
Experience 11, Iter 16, disc loss: 0.0006786566303366531, policy loss: 8.343387851396214
Experience 11, Iter 17, disc loss: 0.0007886928183182577, policy loss: 8.058920588213127
Experience 11, Iter 18, disc loss: 0.0008785855669965419, policy loss: 7.981968584240017
Experience 11, Iter 19, disc loss: 0.0007963355752682562, policy loss: 8.067647930750185
Experience 11, Iter 20, disc loss: 0.0008543148970741025, policy loss: 7.9865482200008024
Experience 11, Iter 21, disc loss: 0.0008247757083325871, policy loss: 7.8903495437308155
Experience 11, Iter 22, disc loss: 0.0007663696794319617, policy loss: 8.100690992676157
Experience 11, Iter 23, disc loss: 0.0008409443091250799, policy loss: 7.915479174124424
Experience 11, Iter 24, disc loss: 0.0007141203942475685, policy loss: 8.241610988129336
Experience 11, Iter 25, disc loss: 0.000722103862542655, policy loss: 8.335248557701226
Experience 11, Iter 26, disc loss: 0.0008179776385364144, policy loss: 7.94166789710408
Experience 11, Iter 27, disc loss: 0.0008006127234651578, policy loss: 8.145025785331182
Experience 11, Iter 28, disc loss: 0.0008470470486778427, policy loss: 8.10432918165342
Experience 11, Iter 29, disc loss: 0.0007744989716850306, policy loss: 8.148172994817022
Experience 11, Iter 30, disc loss: 0.0008174320794292699, policy loss: 7.824860766257375
Experience 11, Iter 31, disc loss: 0.0007057144310214112, policy loss: 8.251711501743847
Experience 11, Iter 32, disc loss: 0.0008471047772404909, policy loss: 7.912974894526992
Experience 11, Iter 33, disc loss: 0.0007586467479294559, policy loss: 8.11502130769554
Experience 11, Iter 34, disc loss: 0.0006957588204861548, policy loss: 8.34410998234793
Experience 11, Iter 35, disc loss: 0.0008348304007606189, policy loss: 7.948020217562225
Experience 11, Iter 36, disc loss: 0.0007932975071835797, policy loss: 8.111764088352384
Experience 11, Iter 37, disc loss: 0.0007108375246000177, policy loss: 8.102364070202562
Experience 11, Iter 38, disc loss: 0.0008321167536747769, policy loss: 7.951835127454019
Experience 11, Iter 39, disc loss: 0.0008153439180428638, policy loss: 7.855197943386506
Experience 11, Iter 40, disc loss: 0.0008189881278416846, policy loss: 7.907489857005827
Experience 11, Iter 41, disc loss: 0.0008022788781277451, policy loss: 7.934057048948828
Experience 11, Iter 42, disc loss: 0.0007058259532593747, policy loss: 8.109785775705001
Experience 11, Iter 43, disc loss: 0.0007736082968693514, policy loss: 8.054265022139019
Experience 11, Iter 44, disc loss: 0.0007068128866396792, policy loss: 8.46585794732904
Experience 11, Iter 45, disc loss: 0.0006618283051227176, policy loss: 8.204387924137441
Experience 11, Iter 46, disc loss: 0.0007359705696184566, policy loss: 8.050596351999953
Experience 11, Iter 47, disc loss: 0.00076505045057556, policy loss: 8.054371776341362
Experience 11, Iter 48, disc loss: 0.0006985675548489693, policy loss: 8.20244353642397
Experience 11, Iter 49, disc loss: 0.0007154725692293067, policy loss: 8.011885710600836
Experience 11, Iter 50, disc loss: 0.0007834420145225846, policy loss: 8.165961460562537
Experience 11, Iter 51, disc loss: 0.0006795999225400844, policy loss: 8.20797318922979
Experience 11, Iter 52, disc loss: 0.0007736095356272964, policy loss: 8.069943649429758
Experience 11, Iter 53, disc loss: 0.0007779133856079031, policy loss: 8.161829438231294
Experience 11, Iter 54, disc loss: 0.0007042342376506001, policy loss: 8.060886654592833
Experience 11, Iter 55, disc loss: 0.0007050828695192531, policy loss: 8.103035563749438
Experience 11, Iter 56, disc loss: 0.0006840221836449669, policy loss: 8.106991275688738
Experience 11, Iter 57, disc loss: 0.000740408340807212, policy loss: 8.03256584170984
Experience 11, Iter 58, disc loss: 0.0007376934573201735, policy loss: 8.218641527133231
Experience 11, Iter 59, disc loss: 0.0007363516384807443, policy loss: 8.06558169124655
Experience 11, Iter 60, disc loss: 0.0007459853681756464, policy loss: 8.200136965374218
Experience 11, Iter 61, disc loss: 0.0006874853682372348, policy loss: 8.076469475926046
Experience 11, Iter 62, disc loss: 0.0007690699508627651, policy loss: 7.933246376243863
Experience 11, Iter 63, disc loss: 0.0007461500777319625, policy loss: 7.935736900528861
Experience 11, Iter 64, disc loss: 0.0007085371593463323, policy loss: 8.118906058291726
Experience 11, Iter 65, disc loss: 0.0006626049428329581, policy loss: 8.1221731427723
Experience 11, Iter 66, disc loss: 0.0007092141329906246, policy loss: 7.932943894058219
Experience 11, Iter 67, disc loss: 0.0006494901012155727, policy loss: 8.14705027650431
Experience 11, Iter 68, disc loss: 0.0007354362028045864, policy loss: 7.958234163149024
Experience 11, Iter 69, disc loss: 0.000666810472315239, policy loss: 8.335598791434332
Experience 11, Iter 70, disc loss: 0.0006214094189347273, policy loss: 8.306464966791927
Experience 11, Iter 71, disc loss: 0.0007418612987482747, policy loss: 7.974366130630628
Experience 11, Iter 72, disc loss: 0.0006519875806793528, policy loss: 8.253656451785094
Experience 11, Iter 73, disc loss: 0.0007500991181581093, policy loss: 7.906823901351885
Experience 11, Iter 74, disc loss: 0.0006322149941233676, policy loss: 8.194976546896864
Experience 11, Iter 75, disc loss: 0.0007870708017905154, policy loss: 7.953307911182095
Experience 11, Iter 76, disc loss: 0.0008052458493344215, policy loss: 7.870475261912682
Experience 11, Iter 77, disc loss: 0.0007784694857378065, policy loss: 8.021750360724226
Experience 11, Iter 78, disc loss: 0.0006488010409949653, policy loss: 8.46568833916071
Experience 11, Iter 79, disc loss: 0.0007183269090680106, policy loss: 8.105521957275203
Experience 11, Iter 80, disc loss: 0.0007313599267470623, policy loss: 8.06034951097356
Experience 11, Iter 81, disc loss: 0.0006644602774894604, policy loss: 8.637494235810877
Experience 11, Iter 82, disc loss: 0.0007179085159970981, policy loss: 7.984857366006234
Experience 11, Iter 83, disc loss: 0.0006755580279796824, policy loss: 8.11225638168029
Experience 11, Iter 84, disc loss: 0.0006800969298026835, policy loss: 8.185634461561651
Experience 11, Iter 85, disc loss: 0.0007145054074790894, policy loss: 8.06686430149595
Experience 11, Iter 86, disc loss: 0.0008119229138704613, policy loss: 8.0127511700292
Experience 11, Iter 87, disc loss: 0.0006774781860474884, policy loss: 8.178824333781362
Experience 11, Iter 88, disc loss: 0.0006953276561084711, policy loss: 8.287934567640393
Experience 11, Iter 89, disc loss: 0.0006969473370876153, policy loss: 8.036254555218962
Experience 11, Iter 90, disc loss: 0.0007344762355918929, policy loss: 8.163107571345932
Experience 11, Iter 91, disc loss: 0.0006330453909615097, policy loss: 8.280282938248606
Experience 11, Iter 92, disc loss: 0.0007370165238990569, policy loss: 8.121378220409056
Experience 11, Iter 93, disc loss: 0.0007181671261515342, policy loss: 8.162473300776618
Experience 11, Iter 94, disc loss: 0.0007123173964816034, policy loss: 7.995934107275422
Experience 11, Iter 95, disc loss: 0.0007069412814983097, policy loss: 8.182146379459654
Experience 11, Iter 96, disc loss: 0.0007409459919288005, policy loss: 7.994049587194868
Experience 11, Iter 97, disc loss: 0.0006683529106411117, policy loss: 8.202523851573707
Experience 11, Iter 98, disc loss: 0.0007034073209875643, policy loss: 8.040955474169003
Experience 11, Iter 99, disc loss: 0.000835726732245239, policy loss: 7.860179561473565
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1088],
        [1.1584],
        [0.0137]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0147, 0.1197, 0.6969, 0.0178, 0.0030, 2.6053]],

        [[0.0147, 0.1197, 0.6969, 0.0178, 0.0030, 2.6053]],

        [[0.0147, 0.1197, 0.6969, 0.0178, 0.0030, 2.6053]],

        [[0.0147, 0.1197, 0.6969, 0.0178, 0.0030, 2.6053]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0117, 0.4350, 4.6334, 0.0546], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0117, 0.4350, 4.6334, 0.0546])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.465
Iter 2/2000 - Loss: 2.523
Iter 3/2000 - Loss: 2.371
Iter 4/2000 - Loss: 2.346
Iter 5/2000 - Loss: 2.382
Iter 6/2000 - Loss: 2.333
Iter 7/2000 - Loss: 2.251
Iter 8/2000 - Loss: 2.205
Iter 9/2000 - Loss: 2.185
Iter 10/2000 - Loss: 2.131
Iter 11/2000 - Loss: 2.044
Iter 12/2000 - Loss: 1.958
Iter 13/2000 - Loss: 1.880
Iter 14/2000 - Loss: 1.784
Iter 15/2000 - Loss: 1.655
Iter 16/2000 - Loss: 1.504
Iter 17/2000 - Loss: 1.345
Iter 18/2000 - Loss: 1.179
Iter 19/2000 - Loss: 0.996
Iter 20/2000 - Loss: 0.790
Iter 1981/2000 - Loss: -7.733
Iter 1982/2000 - Loss: -7.733
Iter 1983/2000 - Loss: -7.733
Iter 1984/2000 - Loss: -7.734
Iter 1985/2000 - Loss: -7.734
Iter 1986/2000 - Loss: -7.734
Iter 1987/2000 - Loss: -7.734
Iter 1988/2000 - Loss: -7.734
Iter 1989/2000 - Loss: -7.734
Iter 1990/2000 - Loss: -7.734
Iter 1991/2000 - Loss: -7.734
Iter 1992/2000 - Loss: -7.734
Iter 1993/2000 - Loss: -7.734
Iter 1994/2000 - Loss: -7.734
Iter 1995/2000 - Loss: -7.734
Iter 1996/2000 - Loss: -7.734
Iter 1997/2000 - Loss: -7.734
Iter 1998/2000 - Loss: -7.734
Iter 1999/2000 - Loss: -7.734
Iter 2000/2000 - Loss: -7.734
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[13.9681,  7.4057, 52.9634, 10.3338, 14.6644, 56.2865]],

        [[16.3968, 33.3757,  8.9899,  1.4143,  1.8733, 32.2579]],

        [[20.1753, 38.2411,  7.7755,  1.1641,  2.7417, 26.0644]],

        [[15.5036, 30.8620, 21.9591,  3.8587,  1.8543, 44.7865]]])
Signal Variance: tensor([ 0.0953,  2.7826, 21.4108,  0.7740])
Estimated target variance: tensor([0.0117, 0.4350, 4.6334, 0.0546])
N: 120
Signal to noise ratio: tensor([ 17.1164, 109.3868, 114.6552,  49.5574])
Bound on condition number: tensor([  35157.4081, 1435857.1853, 1577499.7464,  294713.1420])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0008076970416284877, policy loss: 7.87880287558535
Experience 12, Iter 1, disc loss: 0.0007648741526137124, policy loss: 8.013170813942867
Experience 12, Iter 2, disc loss: 0.0009645170288330574, policy loss: 7.7027887759964
Experience 12, Iter 3, disc loss: 0.000720284925416505, policy loss: 8.129282153754078
Experience 12, Iter 4, disc loss: 0.0008989087867969335, policy loss: 7.736275587060833
Experience 12, Iter 5, disc loss: 0.0009935882640524734, policy loss: 7.718254268616328
Experience 12, Iter 6, disc loss: 0.0008676960499733996, policy loss: 7.820692289776092
Experience 12, Iter 7, disc loss: 0.0007985167045532849, policy loss: 8.053083312685073
Experience 12, Iter 8, disc loss: 0.000882968336962282, policy loss: 7.720136498327946
Experience 12, Iter 9, disc loss: 0.0008088533775717523, policy loss: 7.946242372760576
Experience 12, Iter 10, disc loss: 0.0011025297537815837, policy loss: 7.528767801033964
Experience 12, Iter 11, disc loss: 0.0010913942361928006, policy loss: 7.459217674069747
Experience 12, Iter 12, disc loss: 0.0010823961671731714, policy loss: 7.454233136570466
Experience 12, Iter 13, disc loss: 0.0009385479401178411, policy loss: 7.688171591952713
Experience 12, Iter 14, disc loss: 0.0010063599070081633, policy loss: 7.699595813297307
Experience 12, Iter 15, disc loss: 0.0007488651074598245, policy loss: 8.093198236845
Experience 12, Iter 16, disc loss: 0.0010291310658592239, policy loss: 7.535946175540619
Experience 12, Iter 17, disc loss: 0.0010457277126094122, policy loss: 7.500620004967379
Experience 12, Iter 18, disc loss: 0.0008436316347486138, policy loss: 8.16880332200314
Experience 12, Iter 19, disc loss: 0.0008953742878998401, policy loss: 8.00135609965228
Experience 12, Iter 20, disc loss: 0.0009319707010429338, policy loss: 7.682198566862656
Experience 12, Iter 21, disc loss: 0.0008678790502425458, policy loss: 7.683945298946337
Experience 12, Iter 22, disc loss: 0.000844980609130808, policy loss: 7.860022131962387
Experience 12, Iter 23, disc loss: 0.0009185024012649187, policy loss: 7.8638624150112575
Experience 12, Iter 24, disc loss: 0.0009458347606192234, policy loss: 7.678047788897745
Experience 12, Iter 25, disc loss: 0.001090909108804334, policy loss: 7.436882251478429
Experience 12, Iter 26, disc loss: 0.0009099348036962565, policy loss: 7.69822569776952
Experience 12, Iter 27, disc loss: 0.0011128842279882862, policy loss: 7.455167813209467
Experience 12, Iter 28, disc loss: 0.0009393944877790598, policy loss: 7.814656006001386
Experience 12, Iter 29, disc loss: 0.0008875898695517186, policy loss: 7.983851500548867
Experience 12, Iter 30, disc loss: 0.0007952901948484766, policy loss: 8.027469817951904
Experience 12, Iter 31, disc loss: 0.0009360442681133853, policy loss: 7.860780650011044
Experience 12, Iter 32, disc loss: 0.0008902603482577731, policy loss: 7.75430963796184
Experience 12, Iter 33, disc loss: 0.0010381371244004662, policy loss: 7.745311093064325
Experience 12, Iter 34, disc loss: 0.000845226625743926, policy loss: 7.910791137318405
Experience 12, Iter 35, disc loss: 0.0008239183792586981, policy loss: 7.9892808477080175
Experience 12, Iter 36, disc loss: 0.0008647510938986103, policy loss: 7.758446693725485
Experience 12, Iter 37, disc loss: 0.0008295926243919761, policy loss: 7.957920261198407
Experience 12, Iter 38, disc loss: 0.0008109288945885935, policy loss: 7.837469883086428
Experience 12, Iter 39, disc loss: 0.0008970184711227858, policy loss: 7.986339788894657
Experience 12, Iter 40, disc loss: 0.0008736391695947942, policy loss: 7.875646572585839
Experience 12, Iter 41, disc loss: 0.0008885876968945816, policy loss: 7.837917137936698
Experience 12, Iter 42, disc loss: 0.0008270568130478656, policy loss: 7.857898509523534
Experience 12, Iter 43, disc loss: 0.0009499744087650935, policy loss: 7.778733485690221
Experience 12, Iter 44, disc loss: 0.0007346762539990503, policy loss: 8.011552678632778
Experience 12, Iter 45, disc loss: 0.0009230547449190952, policy loss: 7.7465367616553
Experience 12, Iter 46, disc loss: 0.0008490273704480236, policy loss: 7.899392285324158
Experience 12, Iter 47, disc loss: 0.0008110574878152871, policy loss: 8.008614969080162
Experience 12, Iter 48, disc loss: 0.0008566768540626787, policy loss: 7.967211917118426
Experience 12, Iter 49, disc loss: 0.0008026625396327249, policy loss: 7.944983204998675
Experience 12, Iter 50, disc loss: 0.0008042205187227574, policy loss: 7.966519090139034
Experience 12, Iter 51, disc loss: 0.0008051437336399695, policy loss: 8.132748476962187
Experience 12, Iter 52, disc loss: 0.0007921356373423674, policy loss: 8.013501655528382
Experience 12, Iter 53, disc loss: 0.0007512871310853891, policy loss: 8.233283531847244
Experience 12, Iter 54, disc loss: 0.0007144792181856531, policy loss: 8.188706231249542
Experience 12, Iter 55, disc loss: 0.0007312991293256544, policy loss: 8.224579757450671
Experience 12, Iter 56, disc loss: 0.0008492528387033835, policy loss: 8.006551479930074
Experience 12, Iter 57, disc loss: 0.0006856760665704792, policy loss: 8.356315464172459
Experience 12, Iter 58, disc loss: 0.0006603990031950177, policy loss: 8.349155492735413
Experience 12, Iter 59, disc loss: 0.0006703112914279919, policy loss: 8.188458090739127
Experience 12, Iter 60, disc loss: 0.0006909689487900115, policy loss: 8.159895634807677
Experience 12, Iter 61, disc loss: 0.000720261684989278, policy loss: 8.367847690519309
Experience 12, Iter 62, disc loss: 0.0007717786021008002, policy loss: 8.00944181141535
Experience 12, Iter 63, disc loss: 0.0007605020367510277, policy loss: 8.012943800972803
Experience 12, Iter 64, disc loss: 0.0007604228157457691, policy loss: 7.991304571136402
Experience 12, Iter 65, disc loss: 0.0008248311404081528, policy loss: 8.051513676392794
Experience 12, Iter 66, disc loss: 0.0006229586478634301, policy loss: 8.372175037803531
Experience 12, Iter 67, disc loss: 0.0006934026010029738, policy loss: 8.25416604246172
Experience 12, Iter 68, disc loss: 0.0007056696036987098, policy loss: 8.008207885142596
Experience 12, Iter 69, disc loss: 0.0007576010007239347, policy loss: 8.159108112192774
Experience 12, Iter 70, disc loss: 0.0007367960734216317, policy loss: 8.25095870479674
Experience 12, Iter 71, disc loss: 0.0007337308588464836, policy loss: 8.081526402305975
Experience 12, Iter 72, disc loss: 0.0006642663922569739, policy loss: 8.331604854274516
Experience 12, Iter 73, disc loss: 0.0007069049094464768, policy loss: 8.05578868636373
Experience 12, Iter 74, disc loss: 0.0006214847343419831, policy loss: 8.369297483445845
Experience 12, Iter 75, disc loss: 0.0006831276893289689, policy loss: 8.064662633743453
Experience 12, Iter 76, disc loss: 0.0006514702790069435, policy loss: 8.373839605472115
Experience 12, Iter 77, disc loss: 0.0006295466116826994, policy loss: 8.359031476054241
Experience 12, Iter 78, disc loss: 0.0006138554656907659, policy loss: 8.348175759742611
Experience 12, Iter 79, disc loss: 0.0005588788849068052, policy loss: 8.523944375253832
Experience 12, Iter 80, disc loss: 0.000598906100555259, policy loss: 8.400187195075825
Experience 12, Iter 81, disc loss: 0.000661620141901125, policy loss: 8.217856637427705
Experience 12, Iter 82, disc loss: 0.0007424955415208671, policy loss: 8.072539860139724
Experience 12, Iter 83, disc loss: 0.0006610611875383385, policy loss: 8.166591840650065
Experience 12, Iter 84, disc loss: 0.0006357398310339398, policy loss: 8.171913629091495
Experience 12, Iter 85, disc loss: 0.0006182088640574764, policy loss: 8.409216824865348
Experience 12, Iter 86, disc loss: 0.000643429519451682, policy loss: 8.39570484787053
Experience 12, Iter 87, disc loss: 0.0006564422568560087, policy loss: 8.335094195774365
Experience 12, Iter 88, disc loss: 0.0006198212217626203, policy loss: 8.326583433714696
Experience 12, Iter 89, disc loss: 0.0006173898996196548, policy loss: 8.23575264688511
Experience 12, Iter 90, disc loss: 0.0005945477560482968, policy loss: 8.272231531146328
Experience 12, Iter 91, disc loss: 0.000768845633709175, policy loss: 8.044528431979309
Experience 12, Iter 92, disc loss: 0.000652628606101095, policy loss: 8.222887413027198
Experience 12, Iter 93, disc loss: 0.0006091870129781487, policy loss: 8.367956636288106
Experience 12, Iter 94, disc loss: 0.0005808083568176653, policy loss: 8.414744233604626
Experience 12, Iter 95, disc loss: 0.0005230028991926244, policy loss: 8.461103690867546
Experience 12, Iter 96, disc loss: 0.0005722399420657134, policy loss: 8.389733841070584
Experience 12, Iter 97, disc loss: 0.0005421975327020161, policy loss: 8.461497690019279
Experience 12, Iter 98, disc loss: 0.0007351536589622894, policy loss: 7.964401841557119
Experience 12, Iter 99, disc loss: 0.0006442457995404912, policy loss: 8.185748706258721
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1086],
        [1.1403],
        [0.0137]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0148, 0.1170, 0.6973, 0.0183, 0.0030, 2.6981]],

        [[0.0148, 0.1170, 0.6973, 0.0183, 0.0030, 2.6981]],

        [[0.0148, 0.1170, 0.6973, 0.0183, 0.0030, 2.6981]],

        [[0.0148, 0.1170, 0.6973, 0.0183, 0.0030, 2.6981]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0115, 0.4343, 4.5610, 0.0549], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0115, 0.4343, 4.5610, 0.0549])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.453
Iter 2/2000 - Loss: 2.530
Iter 3/2000 - Loss: 2.359
Iter 4/2000 - Loss: 2.346
Iter 5/2000 - Loss: 2.380
Iter 6/2000 - Loss: 2.323
Iter 7/2000 - Loss: 2.240
Iter 8/2000 - Loss: 2.194
Iter 9/2000 - Loss: 2.171
Iter 10/2000 - Loss: 2.118
Iter 11/2000 - Loss: 2.029
Iter 12/2000 - Loss: 1.936
Iter 13/2000 - Loss: 1.852
Iter 14/2000 - Loss: 1.755
Iter 15/2000 - Loss: 1.623
Iter 16/2000 - Loss: 1.462
Iter 17/2000 - Loss: 1.292
Iter 18/2000 - Loss: 1.117
Iter 19/2000 - Loss: 0.928
Iter 20/2000 - Loss: 0.718
Iter 1981/2000 - Loss: -7.881
Iter 1982/2000 - Loss: -7.881
Iter 1983/2000 - Loss: -7.881
Iter 1984/2000 - Loss: -7.881
Iter 1985/2000 - Loss: -7.881
Iter 1986/2000 - Loss: -7.881
Iter 1987/2000 - Loss: -7.881
Iter 1988/2000 - Loss: -7.881
Iter 1989/2000 - Loss: -7.881
Iter 1990/2000 - Loss: -7.881
Iter 1991/2000 - Loss: -7.881
Iter 1992/2000 - Loss: -7.881
Iter 1993/2000 - Loss: -7.881
Iter 1994/2000 - Loss: -7.881
Iter 1995/2000 - Loss: -7.881
Iter 1996/2000 - Loss: -7.881
Iter 1997/2000 - Loss: -7.881
Iter 1998/2000 - Loss: -7.881
Iter 1999/2000 - Loss: -7.882
Iter 2000/2000 - Loss: -7.882
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[14.0333,  7.4223, 49.1930, 10.1474, 14.1192, 56.4624]],

        [[14.7830, 31.7695,  9.8059,  1.4667,  1.7857, 33.6411]],

        [[20.3145, 37.0299,  7.8104,  1.1843,  2.7503, 23.6109]],

        [[15.2255, 30.0432, 22.6098,  3.7628,  1.7875, 44.5430]]])
Signal Variance: tensor([ 0.0947,  3.1594, 20.3893,  0.7895])
Estimated target variance: tensor([0.0115, 0.4343, 4.5610, 0.0549])
N: 130
Signal to noise ratio: tensor([ 17.4703, 115.4191, 112.7479,  51.1965])
Bound on condition number: tensor([  39678.5058, 1731805.3904, 1652573.6588,  340741.4603])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0005913972130462874, policy loss: 8.341438224273814
Experience 13, Iter 1, disc loss: 0.0005139163173014916, policy loss: 8.45348978298657
Experience 13, Iter 2, disc loss: 0.0005666957032588297, policy loss: 8.26106036509448
Experience 13, Iter 3, disc loss: 0.0005726635940700732, policy loss: 8.590169367368969
Experience 13, Iter 4, disc loss: 0.0005525357935508584, policy loss: 8.442113757567606
Experience 13, Iter 5, disc loss: 0.0005275964436757779, policy loss: 8.516804495642194
Experience 13, Iter 6, disc loss: 0.000516639621868351, policy loss: 8.55187845538332
Experience 13, Iter 7, disc loss: 0.0005305582684185194, policy loss: 8.546560724539825
Experience 13, Iter 8, disc loss: 0.0006714998671069501, policy loss: 8.130672954058786
Experience 13, Iter 9, disc loss: 0.000548010166264238, policy loss: 8.567139339741974
Experience 13, Iter 10, disc loss: 0.0006063058286198892, policy loss: 8.356422379129098
Experience 13, Iter 11, disc loss: 0.0005334035913572936, policy loss: 8.599960002661357
Experience 13, Iter 12, disc loss: 0.000615911323415083, policy loss: 8.096724834955722
Experience 13, Iter 13, disc loss: 0.0005089837737574271, policy loss: 8.559114447618521
Experience 13, Iter 14, disc loss: 0.0005229725626295509, policy loss: 8.743078933686501
Experience 13, Iter 15, disc loss: 0.0005473660895044421, policy loss: 8.468517639146157
Experience 13, Iter 16, disc loss: 0.0004897186844603519, policy loss: 8.711706357569758
Experience 13, Iter 17, disc loss: 0.0005420921525220674, policy loss: 8.524578419879973
Experience 13, Iter 18, disc loss: 0.0005014909390065516, policy loss: 8.579465826484569
Experience 13, Iter 19, disc loss: 0.00046249179087872784, policy loss: 8.528498044915562
Experience 13, Iter 20, disc loss: 0.0005132774208091162, policy loss: 8.505649587649605
Experience 13, Iter 21, disc loss: 0.0005460527314151223, policy loss: 8.427083621185556
Experience 13, Iter 22, disc loss: 0.0005327671375011532, policy loss: 8.416970679776462
Experience 13, Iter 23, disc loss: 0.0005197967450037999, policy loss: 8.426240242754691
Experience 13, Iter 24, disc loss: 0.0005312628557723277, policy loss: 8.56584845310714
Experience 13, Iter 25, disc loss: 0.0005057459688442949, policy loss: 8.561288944412558
Experience 13, Iter 26, disc loss: 0.0005596873872573581, policy loss: 8.335414957318052
Experience 13, Iter 27, disc loss: 0.0004585757962894577, policy loss: 8.650597238561343
Experience 13, Iter 28, disc loss: 0.0005936148244709703, policy loss: 8.254634320171856
Experience 13, Iter 29, disc loss: 0.0006074915575879946, policy loss: 8.392652402125087
Experience 13, Iter 30, disc loss: 0.000537615238802777, policy loss: 8.359348780718772
Experience 13, Iter 31, disc loss: 0.000580559829874736, policy loss: 8.406485603879691
Experience 13, Iter 32, disc loss: 0.0004909878347780366, policy loss: 8.556830201539363
Experience 13, Iter 33, disc loss: 0.00042015175339831336, policy loss: 8.837477111643018
Experience 13, Iter 34, disc loss: 0.0005114447953420098, policy loss: 8.441870638573072
Experience 13, Iter 35, disc loss: 0.0004734699475647202, policy loss: 8.558975559348966
Experience 13, Iter 36, disc loss: 0.0004701394640943468, policy loss: 8.631830150256231
Experience 13, Iter 37, disc loss: 0.0005299011964424801, policy loss: 8.621687751525249
Experience 13, Iter 38, disc loss: 0.0004908022228151405, policy loss: 8.474738135479887
Experience 13, Iter 39, disc loss: 0.000490976196705115, policy loss: 8.430901395083076
Experience 13, Iter 40, disc loss: 0.0004619975294239596, policy loss: 8.618163371031148
Experience 13, Iter 41, disc loss: 0.0005095855575613516, policy loss: 8.569409616961757
Experience 13, Iter 42, disc loss: 0.00043651889180000656, policy loss: 8.7251881700582
Experience 13, Iter 43, disc loss: 0.0005283710006344857, policy loss: 8.360779662076396
Experience 13, Iter 44, disc loss: 0.0005530279407032118, policy loss: 8.505724603084827
Experience 13, Iter 45, disc loss: 0.00043367984183871486, policy loss: 8.741672021152255
Experience 13, Iter 46, disc loss: 0.0004864324847819686, policy loss: 8.532311391358974
Experience 13, Iter 47, disc loss: 0.0005133597183455055, policy loss: 8.494037854966829
Experience 13, Iter 48, disc loss: 0.0004610599696598668, policy loss: 8.648461682612256
Experience 13, Iter 49, disc loss: 0.0004633312880708741, policy loss: 8.501941463688024
Experience 13, Iter 50, disc loss: 0.00047178093520835513, policy loss: 8.469741160342279
Experience 13, Iter 51, disc loss: 0.00045340622908444776, policy loss: 8.574836426525234
Experience 13, Iter 52, disc loss: 0.000488959187700872, policy loss: 8.425960365074983
Experience 13, Iter 53, disc loss: 0.0004922597521224282, policy loss: 8.476288807843195
Experience 13, Iter 54, disc loss: 0.0005491052625113569, policy loss: 8.398085669161574
Experience 13, Iter 55, disc loss: 0.0005431298284730659, policy loss: 8.463484065355994
Experience 13, Iter 56, disc loss: 0.0004475320261152782, policy loss: 8.628125499759467
Experience 13, Iter 57, disc loss: 0.0004331255774125158, policy loss: 8.636422145461948
Experience 13, Iter 58, disc loss: 0.00046156391241296664, policy loss: 8.595308550740024
Experience 13, Iter 59, disc loss: 0.0004839610718001063, policy loss: 8.64516036730388
Experience 13, Iter 60, disc loss: 0.00043755496638329803, policy loss: 8.808447282981255
Experience 13, Iter 61, disc loss: 0.0005264209848691822, policy loss: 8.351173108859882
Experience 13, Iter 62, disc loss: 0.0004494234405485818, policy loss: 8.617490486357495
Experience 13, Iter 63, disc loss: 0.00047640525711568546, policy loss: 8.609037399948617
Experience 13, Iter 64, disc loss: 0.0004132966468807775, policy loss: 8.825054837288
Experience 13, Iter 65, disc loss: 0.0004638396811647838, policy loss: 8.668465141790827
Experience 13, Iter 66, disc loss: 0.0003679040828176763, policy loss: 8.996343031521196
Experience 13, Iter 67, disc loss: 0.00043720608486814554, policy loss: 8.70717465375002
Experience 13, Iter 68, disc loss: 0.0004059190204956655, policy loss: 8.740355181930385
Experience 13, Iter 69, disc loss: 0.0004301344759483277, policy loss: 8.643286424131718
Experience 13, Iter 70, disc loss: 0.0004314614568186543, policy loss: 8.577873305123084
Experience 13, Iter 71, disc loss: 0.0004173760070961195, policy loss: 8.789166299272875
Experience 13, Iter 72, disc loss: 0.0005084368777702479, policy loss: 8.319953753168111
Experience 13, Iter 73, disc loss: 0.0004815001003627542, policy loss: 8.627871271164754
Experience 13, Iter 74, disc loss: 0.00039089273223191876, policy loss: 8.81677736004795
Experience 13, Iter 75, disc loss: 0.00036862661875422976, policy loss: 8.952099363066917
Experience 13, Iter 76, disc loss: 0.00035383381782686874, policy loss: 9.0375428380917
Experience 13, Iter 77, disc loss: 0.0004739312727297712, policy loss: 8.57736308137892
Experience 13, Iter 78, disc loss: 0.00044932729157447465, policy loss: 8.496254192568253
Experience 13, Iter 79, disc loss: 0.0003543491843172046, policy loss: 8.878677567825441
Experience 13, Iter 80, disc loss: 0.0004310360591834486, policy loss: 8.563590390841153
Experience 13, Iter 81, disc loss: 0.00043262396793784997, policy loss: 8.7037380742069
Experience 13, Iter 82, disc loss: 0.0004350165975496072, policy loss: 8.620470134224648
Experience 13, Iter 83, disc loss: 0.00031453814285207647, policy loss: 9.23736399292202
Experience 13, Iter 84, disc loss: 0.00036079026804378497, policy loss: 8.790278806022153
Experience 13, Iter 85, disc loss: 0.00038482931332520286, policy loss: 8.82236299437751
Experience 13, Iter 86, disc loss: 0.00043137459985686925, policy loss: 8.524167681556268
Experience 13, Iter 87, disc loss: 0.0003601952481694511, policy loss: 8.950602174284993
Experience 13, Iter 88, disc loss: 0.00041388926524588946, policy loss: 8.79678691889967
Experience 13, Iter 89, disc loss: 0.00036964594381110787, policy loss: 8.820774537386919
Experience 13, Iter 90, disc loss: 0.00038817438399663324, policy loss: 8.781403945100735
Experience 13, Iter 91, disc loss: 0.00044147265792197687, policy loss: 8.649579256180973
Experience 13, Iter 92, disc loss: 0.00036196142362632293, policy loss: 8.893774466881121
Experience 13, Iter 93, disc loss: 0.0004167179595347556, policy loss: 8.584161737783798
Experience 13, Iter 94, disc loss: 0.0003595976744520287, policy loss: 8.831289311292196
Experience 13, Iter 95, disc loss: 0.000364835971665175, policy loss: 9.03689955101539
Experience 13, Iter 96, disc loss: 0.00035539575622702143, policy loss: 8.886925767788513
Experience 13, Iter 97, disc loss: 0.00036171701850431386, policy loss: 8.94492998261182
Experience 13, Iter 98, disc loss: 0.0003421988151556993, policy loss: 9.006249055672816
Experience 13, Iter 99, disc loss: 0.00029776894289306675, policy loss: 9.127439765467912
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1073],
        [1.1123],
        [0.0136]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0141, 0.1151, 0.6914, 0.0190, 0.0032, 2.7738]],

        [[0.0141, 0.1151, 0.6914, 0.0190, 0.0032, 2.7738]],

        [[0.0141, 0.1151, 0.6914, 0.0190, 0.0032, 2.7738]],

        [[0.0141, 0.1151, 0.6914, 0.0190, 0.0032, 2.7738]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.4291, 4.4490, 0.0545], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.4291, 4.4490, 0.0545])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.431
Iter 2/2000 - Loss: 2.523
Iter 3/2000 - Loss: 2.335
Iter 4/2000 - Loss: 2.335
Iter 5/2000 - Loss: 2.371
Iter 6/2000 - Loss: 2.308
Iter 7/2000 - Loss: 2.225
Iter 8/2000 - Loss: 2.181
Iter 9/2000 - Loss: 2.160
Iter 10/2000 - Loss: 2.110
Iter 11/2000 - Loss: 2.021
Iter 12/2000 - Loss: 1.925
Iter 13/2000 - Loss: 1.839
Iter 14/2000 - Loss: 1.748
Iter 15/2000 - Loss: 1.623
Iter 16/2000 - Loss: 1.464
Iter 17/2000 - Loss: 1.290
Iter 18/2000 - Loss: 1.112
Iter 19/2000 - Loss: 0.924
Iter 20/2000 - Loss: 0.714
Iter 1981/2000 - Loss: -7.946
Iter 1982/2000 - Loss: -7.946
Iter 1983/2000 - Loss: -7.946
Iter 1984/2000 - Loss: -7.946
Iter 1985/2000 - Loss: -7.946
Iter 1986/2000 - Loss: -7.946
Iter 1987/2000 - Loss: -7.946
Iter 1988/2000 - Loss: -7.946
Iter 1989/2000 - Loss: -7.946
Iter 1990/2000 - Loss: -7.946
Iter 1991/2000 - Loss: -7.946
Iter 1992/2000 - Loss: -7.946
Iter 1993/2000 - Loss: -7.946
Iter 1994/2000 - Loss: -7.946
Iter 1995/2000 - Loss: -7.946
Iter 1996/2000 - Loss: -7.946
Iter 1997/2000 - Loss: -7.946
Iter 1998/2000 - Loss: -7.946
Iter 1999/2000 - Loss: -7.946
Iter 2000/2000 - Loss: -7.946
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[13.6455,  7.2456, 45.8059,  9.7899, 13.7479, 56.6961]],

        [[14.4036, 31.3584,  9.8965,  1.4668,  1.8241, 32.8019]],

        [[19.3264, 35.3054,  7.8549,  1.2093,  2.7056, 21.3753]],

        [[15.4178, 28.8046, 24.2244,  3.8228,  1.7690, 45.9552]]])
Signal Variance: tensor([ 0.0930,  2.9525, 19.5691,  0.8613])
Estimated target variance: tensor([0.0114, 0.4291, 4.4490, 0.0545])
N: 140
Signal to noise ratio: tensor([ 16.7276, 109.4862, 112.1223,  53.6800])
Bound on condition number: tensor([  39174.9416, 1678214.0572, 1759997.1337,  403416.2240])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.00035643622603852304, policy loss: 8.804132477356218
Experience 14, Iter 1, disc loss: 0.000367996433212776, policy loss: 8.83063512128908
Experience 14, Iter 2, disc loss: 0.0003204868517452435, policy loss: 9.037945415647565
Experience 14, Iter 3, disc loss: 0.00038522181600169475, policy loss: 8.7808632149137
Experience 14, Iter 4, disc loss: 0.0003422304998350961, policy loss: 8.938047700198013
Experience 14, Iter 5, disc loss: 0.0003495074236031276, policy loss: 9.073097602758564
Experience 14, Iter 6, disc loss: 0.000375706414751195, policy loss: 8.946974236481736
Experience 14, Iter 7, disc loss: 0.0003552872289390134, policy loss: 8.954323848299017
Experience 14, Iter 8, disc loss: 0.00034317351343920294, policy loss: 8.832267685568114
Experience 14, Iter 9, disc loss: 0.00033448638419313276, policy loss: 8.773656370777493
Experience 14, Iter 10, disc loss: 0.00032023830079969107, policy loss: 8.919477149294368
Experience 14, Iter 11, disc loss: 0.00039594309409352095, policy loss: 8.816238089750142
Experience 14, Iter 12, disc loss: 0.00038506810620865517, policy loss: 8.714212776151808
Experience 14, Iter 13, disc loss: 0.0003619769039801729, policy loss: 8.585827949653847
Experience 14, Iter 14, disc loss: 0.00040311815488731196, policy loss: 8.529447898132513
Experience 14, Iter 15, disc loss: 0.0004170679651708353, policy loss: 8.533085634165909
Experience 14, Iter 16, disc loss: 0.0003578799117550875, policy loss: 8.838320207810643
Experience 14, Iter 17, disc loss: 0.00038735578876954835, policy loss: 8.804368096463891
Experience 14, Iter 18, disc loss: 0.0003801810783087277, policy loss: 8.671454954951228
Experience 14, Iter 19, disc loss: 0.0003554212356187038, policy loss: 8.677930765834969
Experience 14, Iter 20, disc loss: 0.00038799476081744324, policy loss: 8.711600802926498
Experience 14, Iter 21, disc loss: 0.0003481758118557264, policy loss: 8.777973170840337
Experience 14, Iter 22, disc loss: 0.0004157769208674995, policy loss: 8.530968748474423
Experience 14, Iter 23, disc loss: 0.00038748047584676287, policy loss: 8.76835442095903
Experience 14, Iter 24, disc loss: 0.0003328839111627877, policy loss: 8.85505607948233
Experience 14, Iter 25, disc loss: 0.00040607440056606987, policy loss: 8.58453066641761
Experience 14, Iter 26, disc loss: 0.0003306921451464593, policy loss: 9.020692930069513
Experience 14, Iter 27, disc loss: 0.0003594783817932635, policy loss: 8.866690167952488
Experience 14, Iter 28, disc loss: 0.00030898034702882445, policy loss: 9.083816850710544
Experience 14, Iter 29, disc loss: 0.00034572385420250727, policy loss: 8.832611934705453
Experience 14, Iter 30, disc loss: 0.0003190535676841136, policy loss: 8.981186421256648
Experience 14, Iter 31, disc loss: 0.0003467619647635567, policy loss: 8.792945800976469
Experience 14, Iter 32, disc loss: 0.00035155596756762084, policy loss: 8.710024525596635
Experience 14, Iter 33, disc loss: 0.0003594080154162245, policy loss: 8.806514009920217
Experience 14, Iter 34, disc loss: 0.0003475949992996746, policy loss: 8.817013200156234
Experience 14, Iter 35, disc loss: 0.00033108640978923306, policy loss: 8.863285146307287
Experience 14, Iter 36, disc loss: 0.00037871247969411734, policy loss: 8.579329381815029
Experience 14, Iter 37, disc loss: 0.000361145909407979, policy loss: 8.881711117633575
Experience 14, Iter 38, disc loss: 0.00035053745013895817, policy loss: 8.767515872146372
Experience 14, Iter 39, disc loss: 0.00030681368539772265, policy loss: 9.044032560384307
Experience 14, Iter 40, disc loss: 0.0003039278998699465, policy loss: 9.007589655874222
Experience 14, Iter 41, disc loss: 0.0003223413328885934, policy loss: 9.03179612633739
Experience 14, Iter 42, disc loss: 0.00032282933869374493, policy loss: 8.957395900192523
Experience 14, Iter 43, disc loss: 0.000295360048979965, policy loss: 8.973789903012317
Experience 14, Iter 44, disc loss: 0.0003041464801451688, policy loss: 9.09122724479724
Experience 14, Iter 45, disc loss: 0.00032265339586091805, policy loss: 8.912008663622936
Experience 14, Iter 46, disc loss: 0.0003523148227146727, policy loss: 8.729561036810313
Experience 14, Iter 47, disc loss: 0.0003190249311934636, policy loss: 8.996190498825772
Experience 14, Iter 48, disc loss: 0.0003470909481930152, policy loss: 9.01754143059184
Experience 14, Iter 49, disc loss: 0.0002946143560937666, policy loss: 9.015252218817157
Experience 14, Iter 50, disc loss: 0.00030676879962813716, policy loss: 8.853919874685275
Experience 14, Iter 51, disc loss: 0.0003685483087639689, policy loss: 8.652287114310163
Experience 14, Iter 52, disc loss: 0.00034837786651599153, policy loss: 8.850787614770358
Experience 14, Iter 53, disc loss: 0.00033807987923625794, policy loss: 8.749197302179349
Experience 14, Iter 54, disc loss: 0.00033106781231348497, policy loss: 8.891603509801373
Experience 14, Iter 55, disc loss: 0.0003310492775505814, policy loss: 8.755754235588686
Experience 14, Iter 56, disc loss: 0.0003907272941148718, policy loss: 8.599172937413165
Experience 14, Iter 57, disc loss: 0.0003097925781890583, policy loss: 8.866386756591243
Experience 14, Iter 58, disc loss: 0.0004120914098680772, policy loss: 8.541628907477996
Experience 14, Iter 59, disc loss: 0.0002925817422793819, policy loss: 8.95206655804671
Experience 14, Iter 60, disc loss: 0.00032703467164011226, policy loss: 8.844841502604798
Experience 14, Iter 61, disc loss: 0.00034799299230044644, policy loss: 8.82133383606304
Experience 14, Iter 62, disc loss: 0.00035652956857140644, policy loss: 8.673089402030634
Experience 14, Iter 63, disc loss: 0.0003755789555041448, policy loss: 8.805603072848294
Experience 14, Iter 64, disc loss: 0.00032750571233651255, policy loss: 8.952431135701044
Experience 14, Iter 65, disc loss: 0.0003160360463613418, policy loss: 8.885650993783639
Experience 14, Iter 66, disc loss: 0.0003783692063573643, policy loss: 8.602958880856265
Experience 14, Iter 67, disc loss: 0.0003270023751037076, policy loss: 8.92344806694184
Experience 14, Iter 68, disc loss: 0.00033631936426780376, policy loss: 8.844853959141366
Experience 14, Iter 69, disc loss: 0.0003388309746266337, policy loss: 8.830040424410607
Experience 14, Iter 70, disc loss: 0.0003318357066331833, policy loss: 8.708677392061498
Experience 14, Iter 71, disc loss: 0.0003484414587826983, policy loss: 8.836793973058551
Experience 14, Iter 72, disc loss: 0.0003416682713868287, policy loss: 8.724711834346927
Experience 14, Iter 73, disc loss: 0.000307120142289359, policy loss: 9.000469281135599
Experience 14, Iter 74, disc loss: 0.00033816737636374996, policy loss: 8.787200747809896
Experience 14, Iter 75, disc loss: 0.0002825445173356255, policy loss: 9.165035045912315
Experience 14, Iter 76, disc loss: 0.0002997459618311608, policy loss: 9.027993251013521
Experience 14, Iter 77, disc loss: 0.00031798820960486483, policy loss: 8.88308111784071
Experience 14, Iter 78, disc loss: 0.0003026646652750304, policy loss: 9.092076905590309
Experience 14, Iter 79, disc loss: 0.00029601544588600396, policy loss: 9.009569406756958
Experience 14, Iter 80, disc loss: 0.00035797870476624374, policy loss: 8.682644039344853
Experience 14, Iter 81, disc loss: 0.00029178550938728124, policy loss: 9.033749729047877
Experience 14, Iter 82, disc loss: 0.00028093935846556925, policy loss: 9.05897169885117
Experience 14, Iter 83, disc loss: 0.00029936537268061494, policy loss: 8.94546041985435
Experience 14, Iter 84, disc loss: 0.00036053309118000095, policy loss: 8.770765761686668
Experience 14, Iter 85, disc loss: 0.0003246335617691735, policy loss: 9.03905703663608
Experience 14, Iter 86, disc loss: 0.0003814665403618343, policy loss: 8.790020386705475
Experience 14, Iter 87, disc loss: 0.000316575745023856, policy loss: 8.991415368249871
Experience 14, Iter 88, disc loss: 0.0002910696825597284, policy loss: 9.011352919087978
Experience 14, Iter 89, disc loss: 0.0002651313665070525, policy loss: 9.28280702489688
Experience 14, Iter 90, disc loss: 0.0003165010060516112, policy loss: 8.861044572302594
Experience 14, Iter 91, disc loss: 0.0003507724339302633, policy loss: 8.732946189112894
Experience 14, Iter 92, disc loss: 0.0003234675704166041, policy loss: 8.85006527068202
Experience 14, Iter 93, disc loss: 0.00030031265878992025, policy loss: 9.02646552758227
Experience 14, Iter 94, disc loss: 0.0003756485139913382, policy loss: 8.68585335495295
Experience 14, Iter 95, disc loss: 0.000287923890114767, policy loss: 8.936886707541635
Experience 14, Iter 96, disc loss: 0.00032581544659052733, policy loss: 8.91431700178847
Experience 14, Iter 97, disc loss: 0.00036425908269336336, policy loss: 8.63377340633088
Experience 14, Iter 98, disc loss: 0.0002936691629634936, policy loss: 9.112419339257237
Experience 14, Iter 99, disc loss: 0.00028336322230492004, policy loss: 9.14016632812645
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1070],
        [1.1013],
        [0.0136]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0139, 0.1138, 0.6867, 0.0194, 0.0033, 2.8344]],

        [[0.0139, 0.1138, 0.6867, 0.0194, 0.0033, 2.8344]],

        [[0.0139, 0.1138, 0.6867, 0.0194, 0.0033, 2.8344]],

        [[0.0139, 0.1138, 0.6867, 0.0194, 0.0033, 2.8344]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.4280, 4.4054, 0.0542], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.4280, 4.4054, 0.0542])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.425
Iter 2/2000 - Loss: 2.514
Iter 3/2000 - Loss: 2.319
Iter 4/2000 - Loss: 2.324
Iter 5/2000 - Loss: 2.358
Iter 6/2000 - Loss: 2.288
Iter 7/2000 - Loss: 2.204
Iter 8/2000 - Loss: 2.158
Iter 9/2000 - Loss: 2.134
Iter 10/2000 - Loss: 2.077
Iter 11/2000 - Loss: 1.981
Iter 12/2000 - Loss: 1.877
Iter 13/2000 - Loss: 1.785
Iter 14/2000 - Loss: 1.687
Iter 15/2000 - Loss: 1.554
Iter 16/2000 - Loss: 1.383
Iter 17/2000 - Loss: 1.196
Iter 18/2000 - Loss: 1.005
Iter 19/2000 - Loss: 0.805
Iter 20/2000 - Loss: 0.586
Iter 1981/2000 - Loss: -8.119
Iter 1982/2000 - Loss: -8.119
Iter 1983/2000 - Loss: -8.119
Iter 1984/2000 - Loss: -8.119
Iter 1985/2000 - Loss: -8.119
Iter 1986/2000 - Loss: -8.119
Iter 1987/2000 - Loss: -8.119
Iter 1988/2000 - Loss: -8.119
Iter 1989/2000 - Loss: -8.119
Iter 1990/2000 - Loss: -8.119
Iter 1991/2000 - Loss: -8.119
Iter 1992/2000 - Loss: -8.119
Iter 1993/2000 - Loss: -8.119
Iter 1994/2000 - Loss: -8.119
Iter 1995/2000 - Loss: -8.119
Iter 1996/2000 - Loss: -8.119
Iter 1997/2000 - Loss: -8.119
Iter 1998/2000 - Loss: -8.119
Iter 1999/2000 - Loss: -8.119
Iter 2000/2000 - Loss: -8.120
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[13.4761,  7.3800, 45.4152,  9.6447, 13.0382, 56.4949]],

        [[14.6562, 29.9314,  9.8392,  1.4971,  1.7958, 33.3268]],

        [[19.8709, 34.3358,  7.8854,  1.1987,  2.4631, 17.9901]],

        [[14.7229, 27.6562, 23.7118,  3.6415,  1.7724, 45.9493]]])
Signal Variance: tensor([ 0.0928,  3.0154, 16.6480,  0.8269])
Estimated target variance: tensor([0.0113, 0.4280, 4.4054, 0.0542])
N: 150
Signal to noise ratio: tensor([ 17.0598, 114.5302, 107.5662,  53.5786])
Bound on condition number: tensor([  43656.7059, 1967574.8780, 1735574.4906,  430601.0397])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.00036385287603281385, policy loss: 8.756205975860968
Experience 15, Iter 1, disc loss: 0.00031307699483535626, policy loss: 8.909886424326814
Experience 15, Iter 2, disc loss: 0.00031423960265754346, policy loss: 8.814830810939918
Experience 15, Iter 3, disc loss: 0.00028720180009968725, policy loss: 9.11329777012711
Experience 15, Iter 4, disc loss: 0.0003433487841804522, policy loss: 8.7818353667331
Experience 15, Iter 5, disc loss: 0.0002931077393019885, policy loss: 9.06620891930464
Experience 15, Iter 6, disc loss: 0.00029793797048991203, policy loss: 9.1070036542541
Experience 15, Iter 7, disc loss: 0.00029224602364777533, policy loss: 9.024109889937668
Experience 15, Iter 8, disc loss: 0.00031424343512305474, policy loss: 8.94443284409244
Experience 15, Iter 9, disc loss: 0.0002991343391440392, policy loss: 8.892137316255255
Experience 15, Iter 10, disc loss: 0.00028893973173489386, policy loss: 8.876485145482249
Experience 15, Iter 11, disc loss: 0.00034389167472400364, policy loss: 8.756019331397953
Experience 15, Iter 12, disc loss: 0.0003145745866168109, policy loss: 8.96804328699548
Experience 15, Iter 13, disc loss: 0.00035930290803022245, policy loss: 8.674375628000314
Experience 15, Iter 14, disc loss: 0.0003381566000885859, policy loss: 9.122998866319652
Experience 15, Iter 15, disc loss: 0.0003098254932003968, policy loss: 8.93057564672359
Experience 15, Iter 16, disc loss: 0.0002535796035095845, policy loss: 9.129526579520325
Experience 15, Iter 17, disc loss: 0.00031338952633657433, policy loss: 8.932003239492383
Experience 15, Iter 18, disc loss: 0.0003109753439732697, policy loss: 8.944866729175239
Experience 15, Iter 19, disc loss: 0.0003122213046756693, policy loss: 8.916892540107533
Experience 15, Iter 20, disc loss: 0.00036634994937626085, policy loss: 8.944795546064187
Experience 15, Iter 21, disc loss: 0.00028987971305062163, policy loss: 9.168831944991712
Experience 15, Iter 22, disc loss: 0.0003103649548423737, policy loss: 8.938486575329886
Experience 15, Iter 23, disc loss: 0.0002660040983626546, policy loss: 9.14014479065511
Experience 15, Iter 24, disc loss: 0.0003154253596373632, policy loss: 8.960162710751117
Experience 15, Iter 25, disc loss: 0.0002821283570648649, policy loss: 8.934825060127231
Experience 15, Iter 26, disc loss: 0.0003057951226402433, policy loss: 8.91129761196837
Experience 15, Iter 27, disc loss: 0.0002713415629532243, policy loss: 9.217049434957756
Experience 15, Iter 28, disc loss: 0.00029176186339463614, policy loss: 9.167049163714555
Experience 15, Iter 29, disc loss: 0.00033242203495441845, policy loss: 8.829884233953479
Experience 15, Iter 30, disc loss: 0.00027296220675143386, policy loss: 9.105875565949141
Experience 15, Iter 31, disc loss: 0.0002915144819633027, policy loss: 8.90092809834653
Experience 15, Iter 32, disc loss: 0.0003044400979125152, policy loss: 8.88221966865926
Experience 15, Iter 33, disc loss: 0.00029986107300562933, policy loss: 9.129568787097657
Experience 15, Iter 34, disc loss: 0.00028583110540550286, policy loss: 8.961226373448575
Experience 15, Iter 35, disc loss: 0.00028748344577918364, policy loss: 9.056421987001844
Experience 15, Iter 36, disc loss: 0.00031407741638296306, policy loss: 8.903348974490537
Experience 15, Iter 37, disc loss: 0.0003015984627691006, policy loss: 9.164772057138165
Experience 15, Iter 38, disc loss: 0.00030893019231900134, policy loss: 8.902507057574075
Experience 15, Iter 39, disc loss: 0.0002963522527729092, policy loss: 8.831730067912977
Experience 15, Iter 40, disc loss: 0.0003011808803090189, policy loss: 9.009217644324359
Experience 15, Iter 41, disc loss: 0.0002742408299598004, policy loss: 9.048272312301851
Experience 15, Iter 42, disc loss: 0.00029527773746584326, policy loss: 8.883027762028757
Experience 15, Iter 43, disc loss: 0.00032167915238751554, policy loss: 8.958056042156032
Experience 15, Iter 44, disc loss: 0.0002780143420021199, policy loss: 8.952897691952375
Experience 15, Iter 45, disc loss: 0.00028658058103732506, policy loss: 9.141450048570778
Experience 15, Iter 46, disc loss: 0.0002827359429605061, policy loss: 9.029078142103799
Experience 15, Iter 47, disc loss: 0.0003381899063493273, policy loss: 8.816119996314093
Experience 15, Iter 48, disc loss: 0.0003022302249469024, policy loss: 8.883930187730847
Experience 15, Iter 49, disc loss: 0.0003317393545715341, policy loss: 8.825872162849555
Experience 15, Iter 50, disc loss: 0.00029749659995539804, policy loss: 9.135758582699667
Experience 15, Iter 51, disc loss: 0.00028397756630371587, policy loss: 9.023124492825872
Experience 15, Iter 52, disc loss: 0.00027899992372581095, policy loss: 9.061179613041904
Experience 15, Iter 53, disc loss: 0.00028322603315427476, policy loss: 9.013455592415326
Experience 15, Iter 54, disc loss: 0.00028629835227529806, policy loss: 9.058901858794332
Experience 15, Iter 55, disc loss: 0.0003140959629618288, policy loss: 9.016197955008524
Experience 15, Iter 56, disc loss: 0.00028228598951878035, policy loss: 8.907227486330763
Experience 15, Iter 57, disc loss: 0.00032671999232200056, policy loss: 9.03810646461115
Experience 15, Iter 58, disc loss: 0.0002979984073128829, policy loss: 9.008694328929998
Experience 15, Iter 59, disc loss: 0.00028849427137563245, policy loss: 8.932057289849386
Experience 15, Iter 60, disc loss: 0.00027192131031799575, policy loss: 9.26663489803568
Experience 15, Iter 61, disc loss: 0.00028270879294373325, policy loss: 8.904710498353964
Experience 15, Iter 62, disc loss: 0.0002884577417643724, policy loss: 9.122285019318564
Experience 15, Iter 63, disc loss: 0.0002812274720753105, policy loss: 9.037749410719679
Experience 15, Iter 64, disc loss: 0.00028344878528536506, policy loss: 9.052782800239095
Experience 15, Iter 65, disc loss: 0.0002555841623741898, policy loss: 9.280864912682818
Experience 15, Iter 66, disc loss: 0.00024285203802282198, policy loss: 9.127327099830726
Experience 15, Iter 67, disc loss: 0.0002448574598141153, policy loss: 9.15741790304983
Experience 15, Iter 68, disc loss: 0.0002780690502727825, policy loss: 8.998176000322214
Experience 15, Iter 69, disc loss: 0.0002643690027418977, policy loss: 9.065418927164867
Experience 15, Iter 70, disc loss: 0.0002648830921993013, policy loss: 9.191618383317888
Experience 15, Iter 71, disc loss: 0.00024174016839325426, policy loss: 9.4080270487986
Experience 15, Iter 72, disc loss: 0.0002669552877335928, policy loss: 9.089067362069478
Experience 15, Iter 73, disc loss: 0.00026422047021832415, policy loss: 9.132170134549668
Experience 15, Iter 74, disc loss: 0.0002616640523478048, policy loss: 9.353452314242064
Experience 15, Iter 75, disc loss: 0.00026710870510953, policy loss: 9.017733210164252
Experience 15, Iter 76, disc loss: 0.0002486984330371889, policy loss: 9.194055901101198
Experience 15, Iter 77, disc loss: 0.0002581169748783761, policy loss: 9.241105904212196
Experience 15, Iter 78, disc loss: 0.00027730503130861845, policy loss: 9.01816153695794
Experience 15, Iter 79, disc loss: 0.0002615892464847757, policy loss: 9.120482052163798
Experience 15, Iter 80, disc loss: 0.00031613607940198527, policy loss: 8.987621639473275
Experience 15, Iter 81, disc loss: 0.00026179601176453326, policy loss: 8.991513101679102
Experience 15, Iter 82, disc loss: 0.00028622410559521927, policy loss: 9.229005852991254
Experience 15, Iter 83, disc loss: 0.000246494071888187, policy loss: 9.389518762191052
Experience 15, Iter 84, disc loss: 0.0002442333401752442, policy loss: 9.30224521154038
Experience 15, Iter 85, disc loss: 0.00026882230425091997, policy loss: 9.060562399270655
Experience 15, Iter 86, disc loss: 0.0002326773717796314, policy loss: 9.479273696731974
Experience 15, Iter 87, disc loss: 0.0002466876860392863, policy loss: 9.270407333951887
Experience 15, Iter 88, disc loss: 0.00023639445807131264, policy loss: 9.238996833071724
Experience 15, Iter 89, disc loss: 0.0002480674624766078, policy loss: 9.173420050355121
Experience 15, Iter 90, disc loss: 0.0002892912136594849, policy loss: 9.001694374644858
Experience 15, Iter 91, disc loss: 0.0002599135199280113, policy loss: 9.082980991506215
Experience 15, Iter 92, disc loss: 0.0002299922107604793, policy loss: 9.343126994004205
Experience 15, Iter 93, disc loss: 0.00028041969442505724, policy loss: 9.255576814468661
Experience 15, Iter 94, disc loss: 0.00034532564224691586, policy loss: 8.817984283089409
Experience 15, Iter 95, disc loss: 0.00024757064284059647, policy loss: 9.226176453071442
Experience 15, Iter 96, disc loss: 0.00024380259089268666, policy loss: 9.462293193804351
Experience 15, Iter 97, disc loss: 0.00025579894127422036, policy loss: 9.117216085603546
Experience 15, Iter 98, disc loss: 0.00027833418855107095, policy loss: 9.065611894926668
Experience 15, Iter 99, disc loss: 0.0002410618691698268, policy loss: 9.144552609360666
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1082],
        [1.1244],
        [0.0140]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0134, 0.1101, 0.7076, 0.0194, 0.0032, 2.8584]],

        [[0.0134, 0.1101, 0.7076, 0.0194, 0.0032, 2.8584]],

        [[0.0134, 0.1101, 0.7076, 0.0194, 0.0032, 2.8584]],

        [[0.0134, 0.1101, 0.7076, 0.0194, 0.0032, 2.8584]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0110, 0.4329, 4.4975, 0.0559], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0110, 0.4329, 4.4975, 0.0559])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.440
Iter 2/2000 - Loss: 2.552
Iter 3/2000 - Loss: 2.339
Iter 4/2000 - Loss: 2.351
Iter 5/2000 - Loss: 2.393
Iter 6/2000 - Loss: 2.324
Iter 7/2000 - Loss: 2.237
Iter 8/2000 - Loss: 2.193
Iter 9/2000 - Loss: 2.175
Iter 10/2000 - Loss: 2.130
Iter 11/2000 - Loss: 2.041
Iter 12/2000 - Loss: 1.941
Iter 13/2000 - Loss: 1.851
Iter 14/2000 - Loss: 1.763
Iter 15/2000 - Loss: 1.643
Iter 16/2000 - Loss: 1.483
Iter 17/2000 - Loss: 1.300
Iter 18/2000 - Loss: 1.110
Iter 19/2000 - Loss: 0.912
Iter 20/2000 - Loss: 0.694
Iter 1981/2000 - Loss: -8.218
Iter 1982/2000 - Loss: -8.218
Iter 1983/2000 - Loss: -8.219
Iter 1984/2000 - Loss: -8.219
Iter 1985/2000 - Loss: -8.219
Iter 1986/2000 - Loss: -8.219
Iter 1987/2000 - Loss: -8.219
Iter 1988/2000 - Loss: -8.219
Iter 1989/2000 - Loss: -8.219
Iter 1990/2000 - Loss: -8.219
Iter 1991/2000 - Loss: -8.219
Iter 1992/2000 - Loss: -8.219
Iter 1993/2000 - Loss: -8.219
Iter 1994/2000 - Loss: -8.219
Iter 1995/2000 - Loss: -8.219
Iter 1996/2000 - Loss: -8.219
Iter 1997/2000 - Loss: -8.219
Iter 1998/2000 - Loss: -8.219
Iter 1999/2000 - Loss: -8.219
Iter 2000/2000 - Loss: -8.219
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[12.5002,  7.2582, 43.1784, 10.2225, 12.5069, 57.1692]],

        [[14.8976, 28.9792,  9.7563,  1.4919,  1.9195, 32.7821]],

        [[18.8469, 33.4695,  7.9115,  1.2031,  2.3915, 17.2622]],

        [[15.4211, 27.5158, 25.7044,  3.9084,  1.8477, 49.0373]]])
Signal Variance: tensor([ 0.0887,  2.9980, 16.2483,  0.9510])
Estimated target variance: tensor([0.0110, 0.4329, 4.4975, 0.0559])
N: 160
Signal to noise ratio: tensor([ 16.8672, 113.5052, 108.6143,  57.8494])
Bound on condition number: tensor([  45521.1354, 2061348.4334, 1887530.0279,  535449.2786])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.00022642385226291802, policy loss: 9.302853439256682
Experience 16, Iter 1, disc loss: 0.0002605814546242205, policy loss: 9.022649096784873
Experience 16, Iter 2, disc loss: 0.0002575971586789916, policy loss: 9.012014958701695
Experience 16, Iter 3, disc loss: 0.0002671427612762758, policy loss: 9.115894232391152
Experience 16, Iter 4, disc loss: 0.00025652945514003213, policy loss: 9.307588969249375
Experience 16, Iter 5, disc loss: 0.00027118372201562945, policy loss: 9.18281166827866
Experience 16, Iter 6, disc loss: 0.00023701411786314114, policy loss: 9.160442292279905
Experience 16, Iter 7, disc loss: 0.00026927617849060136, policy loss: 9.020509632153876
Experience 16, Iter 8, disc loss: 0.00027950809248764036, policy loss: 9.024176733690025
Experience 16, Iter 9, disc loss: 0.0003059226951296511, policy loss: 8.864629034293426
Experience 16, Iter 10, disc loss: 0.00025823065901817453, policy loss: 9.32623794429318
Experience 16, Iter 11, disc loss: 0.00025265721024204, policy loss: 9.368889719293723
Experience 16, Iter 12, disc loss: 0.00023729442552174483, policy loss: 9.283135285330383
Experience 16, Iter 13, disc loss: 0.00026966378465477015, policy loss: 9.16894868676227
Experience 16, Iter 14, disc loss: 0.00022541385233740225, policy loss: 9.325027067391936
Experience 16, Iter 15, disc loss: 0.0002064289656523162, policy loss: 9.390152678366334
Experience 16, Iter 16, disc loss: 0.00021101506469310826, policy loss: 9.458855540621814
Experience 16, Iter 17, disc loss: 0.000237898058251668, policy loss: 9.249658881173325
Experience 16, Iter 18, disc loss: 0.0002485152882481167, policy loss: 9.242974582928047
Experience 16, Iter 19, disc loss: 0.00027500947709679674, policy loss: 9.016109907270748
Experience 16, Iter 20, disc loss: 0.00022598763067345964, policy loss: 9.320104083059412
Experience 16, Iter 21, disc loss: 0.00023955283591223873, policy loss: 9.242346464510137
Experience 16, Iter 22, disc loss: 0.00027257550073594256, policy loss: 9.165774577081617
Experience 16, Iter 23, disc loss: 0.00021815743184516926, policy loss: 9.379711320697727
Experience 16, Iter 24, disc loss: 0.00029602828190226796, policy loss: 9.085804344930018
Experience 16, Iter 25, disc loss: 0.0002586071575100846, policy loss: 9.141119777849422
Experience 16, Iter 26, disc loss: 0.00024135519773928988, policy loss: 9.244434994804562
Experience 16, Iter 27, disc loss: 0.0002481196633992732, policy loss: 9.223676607585132
Experience 16, Iter 28, disc loss: 0.0002761918743936989, policy loss: 8.982043571701675
Experience 16, Iter 29, disc loss: 0.0002393070641715412, policy loss: 9.085937336462987
Experience 16, Iter 30, disc loss: 0.00024013687996880803, policy loss: 9.13970316813517
Experience 16, Iter 31, disc loss: 0.00022793882220211597, policy loss: 9.295374768329602
Experience 16, Iter 32, disc loss: 0.00024327418064862914, policy loss: 9.145911168540854
Experience 16, Iter 33, disc loss: 0.00022276191379067868, policy loss: 9.401879122695199
Experience 16, Iter 34, disc loss: 0.00026266925649440204, policy loss: 9.107482160614992
Experience 16, Iter 35, disc loss: 0.00025150606710046253, policy loss: 9.135996245925636
Experience 16, Iter 36, disc loss: 0.0002672236971228388, policy loss: 9.14144635316476
Experience 16, Iter 37, disc loss: 0.0002164693174689984, policy loss: 9.34725920331044
Experience 16, Iter 38, disc loss: 0.00024128785929682106, policy loss: 9.319215535244922
Experience 16, Iter 39, disc loss: 0.0002604573071079689, policy loss: 9.109799571029349
Experience 16, Iter 40, disc loss: 0.00026793871522732366, policy loss: 9.19730694881008
Experience 16, Iter 41, disc loss: 0.00023995920110982295, policy loss: 9.098898733237592
Experience 16, Iter 42, disc loss: 0.0002403128950467891, policy loss: 9.289815048433985
Experience 16, Iter 43, disc loss: 0.0002437944887055774, policy loss: 9.065798015735412
Experience 16, Iter 44, disc loss: 0.000270083974690551, policy loss: 9.099401854780515
Experience 16, Iter 45, disc loss: 0.00022882409952955645, policy loss: 9.07232869535535
Experience 16, Iter 46, disc loss: 0.00025506308416323837, policy loss: 9.181991240560704
Experience 16, Iter 47, disc loss: 0.00023255872887889512, policy loss: 9.422159768578998
Experience 16, Iter 48, disc loss: 0.00023414682692743206, policy loss: 9.371918671777237
Experience 16, Iter 49, disc loss: 0.0002644719396229553, policy loss: 9.12722348616246
Experience 16, Iter 50, disc loss: 0.0002275539555039027, policy loss: 9.189064256526276
Experience 16, Iter 51, disc loss: 0.00024341572935725382, policy loss: 9.252600775971263
Experience 16, Iter 52, disc loss: 0.0002624409883584218, policy loss: 9.121355352215154
Experience 16, Iter 53, disc loss: 0.00024903086523966515, policy loss: 9.154342583149448
Experience 16, Iter 54, disc loss: 0.00021744694403667635, policy loss: 9.603595794880928
Experience 16, Iter 55, disc loss: 0.00023333189970858062, policy loss: 9.271830883451525
Experience 16, Iter 56, disc loss: 0.00025305072502857174, policy loss: 9.063121787315353
Experience 16, Iter 57, disc loss: 0.0002644425050942837, policy loss: 8.961674587780685
Experience 16, Iter 58, disc loss: 0.0002562808634386915, policy loss: 9.197873032048177
Experience 16, Iter 59, disc loss: 0.000267797114104935, policy loss: 9.170374399108109
Experience 16, Iter 60, disc loss: 0.0002713117390255122, policy loss: 9.047526342993061
Experience 16, Iter 61, disc loss: 0.0002240011440028874, policy loss: 9.327639661029266
Experience 16, Iter 62, disc loss: 0.0002465992843473737, policy loss: 9.343310864547146
Experience 16, Iter 63, disc loss: 0.00020884866693511356, policy loss: 9.366284791404777
Experience 16, Iter 64, disc loss: 0.00022872669351690217, policy loss: 9.185604429348626
Experience 16, Iter 65, disc loss: 0.000225314142122848, policy loss: 9.382318280453614
Experience 16, Iter 66, disc loss: 0.00023145077288676388, policy loss: 9.246129795712578
Experience 16, Iter 67, disc loss: 0.00025381230514807767, policy loss: 9.073510111405803
Experience 16, Iter 68, disc loss: 0.0002460298432808316, policy loss: 9.17830432683232
Experience 16, Iter 69, disc loss: 0.00020574284324495956, policy loss: 9.499195906534265
Experience 16, Iter 70, disc loss: 0.00021744475868958816, policy loss: 9.522533456343638
Experience 16, Iter 71, disc loss: 0.00021064675758025698, policy loss: 9.298428340133896
Experience 16, Iter 72, disc loss: 0.0002354162202308958, policy loss: 9.243448250103953
Experience 16, Iter 73, disc loss: 0.0002488490554976628, policy loss: 9.236332877369755
Experience 16, Iter 74, disc loss: 0.00021203588929087468, policy loss: 9.320653217021416
Experience 16, Iter 75, disc loss: 0.0002645886864245083, policy loss: 9.050549250871587
Experience 16, Iter 76, disc loss: 0.00023516662362043703, policy loss: 9.302278625135358
Experience 16, Iter 77, disc loss: 0.00022649171133663076, policy loss: 9.35490727744914
Experience 16, Iter 78, disc loss: 0.000243038351149362, policy loss: 9.297605203091445
Experience 16, Iter 79, disc loss: 0.00023378636346233904, policy loss: 9.278379610162894
Experience 16, Iter 80, disc loss: 0.0002142910689246972, policy loss: 9.484334631102687
Experience 16, Iter 81, disc loss: 0.00020409979164884033, policy loss: 9.568655499158865
Experience 16, Iter 82, disc loss: 0.0002177741668627733, policy loss: 9.207722045331604
Experience 16, Iter 83, disc loss: 0.00020579903414907417, policy loss: 9.328426483752034
Experience 16, Iter 84, disc loss: 0.00024440363688740514, policy loss: 9.193426265730913
Experience 16, Iter 85, disc loss: 0.0002473804002579217, policy loss: 9.22057907408755
Experience 16, Iter 86, disc loss: 0.00025102614009373095, policy loss: 9.406935587346673
Experience 16, Iter 87, disc loss: 0.00023000787318218144, policy loss: 9.287511871697294
Experience 16, Iter 88, disc loss: 0.0002485549684913286, policy loss: 9.00562877345844
Experience 16, Iter 89, disc loss: 0.0002217151656838785, policy loss: 9.413065791391638
Experience 16, Iter 90, disc loss: 0.00026333470763401323, policy loss: 9.066373766153681
Experience 16, Iter 91, disc loss: 0.0002198952484306028, policy loss: 9.289008829709099
Experience 16, Iter 92, disc loss: 0.00025741531233756747, policy loss: 9.144015349795982
Experience 16, Iter 93, disc loss: 0.00024620125794350286, policy loss: 9.11092418969038
Experience 16, Iter 94, disc loss: 0.00021863444330122268, policy loss: 9.5172507007065
Experience 16, Iter 95, disc loss: 0.00022659419127423825, policy loss: 9.225253061127953
Experience 16, Iter 96, disc loss: 0.00021300644173858366, policy loss: 9.336153007651799
Experience 16, Iter 97, disc loss: 0.00023243096033991457, policy loss: 9.173502616969657
Experience 16, Iter 98, disc loss: 0.0002207759339599172, policy loss: 9.287751946957377
Experience 16, Iter 99, disc loss: 0.00025459693451477833, policy loss: 9.03243337975481
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1080],
        [1.1271],
        [0.0142]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.1075, 0.7171, 0.0197, 0.0031, 2.8801]],

        [[0.0128, 0.1075, 0.7171, 0.0197, 0.0031, 2.8801]],

        [[0.0128, 0.1075, 0.7171, 0.0197, 0.0031, 2.8801]],

        [[0.0128, 0.1075, 0.7171, 0.0197, 0.0031, 2.8801]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.4319, 4.5085, 0.0569], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.4319, 4.5085, 0.0569])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.443
Iter 2/2000 - Loss: 2.552
Iter 3/2000 - Loss: 2.339
Iter 4/2000 - Loss: 2.352
Iter 5/2000 - Loss: 2.395
Iter 6/2000 - Loss: 2.325
Iter 7/2000 - Loss: 2.240
Iter 8/2000 - Loss: 2.199
Iter 9/2000 - Loss: 2.183
Iter 10/2000 - Loss: 2.136
Iter 11/2000 - Loss: 2.046
Iter 12/2000 - Loss: 1.945
Iter 13/2000 - Loss: 1.858
Iter 14/2000 - Loss: 1.770
Iter 15/2000 - Loss: 1.650
Iter 16/2000 - Loss: 1.487
Iter 17/2000 - Loss: 1.300
Iter 18/2000 - Loss: 1.108
Iter 19/2000 - Loss: 0.907
Iter 20/2000 - Loss: 0.686
Iter 1981/2000 - Loss: -8.276
Iter 1982/2000 - Loss: -8.276
Iter 1983/2000 - Loss: -8.276
Iter 1984/2000 - Loss: -8.276
Iter 1985/2000 - Loss: -8.276
Iter 1986/2000 - Loss: -8.276
Iter 1987/2000 - Loss: -8.276
Iter 1988/2000 - Loss: -8.276
Iter 1989/2000 - Loss: -8.276
Iter 1990/2000 - Loss: -8.276
Iter 1991/2000 - Loss: -8.276
Iter 1992/2000 - Loss: -8.276
Iter 1993/2000 - Loss: -8.276
Iter 1994/2000 - Loss: -8.276
Iter 1995/2000 - Loss: -8.276
Iter 1996/2000 - Loss: -8.276
Iter 1997/2000 - Loss: -8.276
Iter 1998/2000 - Loss: -8.277
Iter 1999/2000 - Loss: -8.277
Iter 2000/2000 - Loss: -8.277
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[12.5584,  7.2849, 42.2034,  9.8300, 12.1826, 56.8278]],

        [[17.3771, 30.6316,  6.0495,  1.5241,  2.5649, 29.2856]],

        [[20.4094, 34.2549,  7.9399,  1.2143,  2.3265, 17.1287]],

        [[15.9837, 26.4131, 24.6309,  3.6629,  1.8479, 47.8548]]])
Signal Variance: tensor([ 0.0893,  2.3079, 15.1533,  0.9064])
Estimated target variance: tensor([0.0108, 0.4319, 4.5085, 0.0569])
N: 170
Signal to noise ratio: tensor([ 16.8443, 105.3344, 103.4057,  56.3117])
Bound on condition number: tensor([  48235.4472, 1886208.6942, 1817765.9248,  539072.7206])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.00025649852974707094, policy loss: 9.360107193593173
Experience 17, Iter 1, disc loss: 0.0002499305891890204, policy loss: 9.086019154307225
Experience 17, Iter 2, disc loss: 0.0002098309717248431, policy loss: 9.36093636529416
Experience 17, Iter 3, disc loss: 0.0002339123523505763, policy loss: 9.398952386060202
Experience 17, Iter 4, disc loss: 0.0002319359742500676, policy loss: 9.32373690286729
Experience 17, Iter 5, disc loss: 0.0002591152646098258, policy loss: 9.198045853347185
Experience 17, Iter 6, disc loss: 0.00024695578866835505, policy loss: 9.3650240994658
Experience 17, Iter 7, disc loss: 0.00022082683923525304, policy loss: 9.385679891210902
Experience 17, Iter 8, disc loss: 0.00019479662923935426, policy loss: 9.535501972516753
Experience 17, Iter 9, disc loss: 0.00021716580363875237, policy loss: 9.308888323369583
Experience 17, Iter 10, disc loss: 0.0002382148017908584, policy loss: 9.120349116946956
Experience 17, Iter 11, disc loss: 0.00026223411046201743, policy loss: 8.976050986903607
Experience 17, Iter 12, disc loss: 0.000241806106030334, policy loss: 9.114731936138684
Experience 17, Iter 13, disc loss: 0.0002540623963169247, policy loss: 9.149569664479259
Experience 17, Iter 14, disc loss: 0.00023102618461739936, policy loss: 9.207973148954725
Experience 17, Iter 15, disc loss: 0.00025350232882484177, policy loss: 9.228656310456909
Experience 17, Iter 16, disc loss: 0.00023796894468826842, policy loss: 9.268333334165833
Experience 17, Iter 17, disc loss: 0.00024861112197242406, policy loss: 9.076683515139187
Experience 17, Iter 18, disc loss: 0.00021491830652302436, policy loss: 9.216165468340506
Experience 17, Iter 19, disc loss: 0.00020402738055167076, policy loss: 9.548057123290516
Experience 17, Iter 20, disc loss: 0.00023324298410187596, policy loss: 9.270532785542317
Experience 17, Iter 21, disc loss: 0.00026757416629439045, policy loss: 9.091188750469984
Experience 17, Iter 22, disc loss: 0.0002716235077824539, policy loss: 9.239172752442089
Experience 17, Iter 23, disc loss: 0.00021253345444801854, policy loss: 9.396375665658224
Experience 17, Iter 24, disc loss: 0.00020633350632454775, policy loss: 9.433092036834331
Experience 17, Iter 25, disc loss: 0.00021555536150440172, policy loss: 9.246477779461447
Experience 17, Iter 26, disc loss: 0.00021350100111056956, policy loss: 9.338069663193853
Experience 17, Iter 27, disc loss: 0.00022455471698668976, policy loss: 9.236154201520801
Experience 17, Iter 28, disc loss: 0.0002227536431548768, policy loss: 9.425163059762895
Experience 17, Iter 29, disc loss: 0.00028386863147336, policy loss: 9.152441286032708
Experience 17, Iter 30, disc loss: 0.00023014710229557838, policy loss: 9.396142692703688
Experience 17, Iter 31, disc loss: 0.00023735580243894696, policy loss: 9.217721393118259
Experience 17, Iter 32, disc loss: 0.0002184152198747243, policy loss: 9.20858612902231
Experience 17, Iter 33, disc loss: 0.0002830946048989298, policy loss: 8.946322779231462
Experience 17, Iter 34, disc loss: 0.0002177767884483018, policy loss: 9.813031101643364
Experience 17, Iter 35, disc loss: 0.00025081774975528723, policy loss: 8.989607936070964
Experience 17, Iter 36, disc loss: 0.00018712512398706986, policy loss: 9.546531319897522
Experience 17, Iter 37, disc loss: 0.0002421082545751988, policy loss: 9.254892025071058
Experience 17, Iter 38, disc loss: 0.0002013177031432067, policy loss: 9.658648543915952
Experience 17, Iter 39, disc loss: 0.000221046394138654, policy loss: 9.402385650657106
Experience 17, Iter 40, disc loss: 0.00021089150640000586, policy loss: 9.40350006506937
Experience 17, Iter 41, disc loss: 0.00021802000677831936, policy loss: 9.474095981892754
Experience 17, Iter 42, disc loss: 0.00023291203664197718, policy loss: 9.29944890326085
Experience 17, Iter 43, disc loss: 0.00022454167394947124, policy loss: 9.469989373463086
Experience 17, Iter 44, disc loss: 0.00023445458350166706, policy loss: 9.23508051239285
Experience 17, Iter 45, disc loss: 0.0002339735475198328, policy loss: 9.193614562378437
Experience 17, Iter 46, disc loss: 0.00021029218410259574, policy loss: 9.452327152050675
Experience 17, Iter 47, disc loss: 0.0002347622229049002, policy loss: 9.304682784008715
Experience 17, Iter 48, disc loss: 0.00020090428085964306, policy loss: 9.480135381522029
Experience 17, Iter 49, disc loss: 0.00023578715231111207, policy loss: 9.135587150815526
Experience 17, Iter 50, disc loss: 0.00021053608645955115, policy loss: 9.366286171905951
Experience 17, Iter 51, disc loss: 0.00020217661159864492, policy loss: 9.413958589581275
Experience 17, Iter 52, disc loss: 0.00024830661811584427, policy loss: 9.134056115530875
Experience 17, Iter 53, disc loss: 0.00023639118103048077, policy loss: 8.998484624578904
Experience 17, Iter 54, disc loss: 0.00022343481419661286, policy loss: 9.369744054300917
Experience 17, Iter 55, disc loss: 0.00022752863363556842, policy loss: 9.318660489986293
Experience 17, Iter 56, disc loss: 0.00022951559378486005, policy loss: 9.192259974406404
Experience 17, Iter 57, disc loss: 0.00020886067890819068, policy loss: 9.403548101924807
Experience 17, Iter 58, disc loss: 0.0002404400400479685, policy loss: 9.088509244726296
Experience 17, Iter 59, disc loss: 0.00017041406064164543, policy loss: 9.744627314337901
Experience 17, Iter 60, disc loss: 0.0001860679752457998, policy loss: 9.503493410960212
Experience 17, Iter 61, disc loss: 0.0002118339341517389, policy loss: 9.517541277552496
Experience 17, Iter 62, disc loss: 0.00026610021857062223, policy loss: 9.025943535023924
Experience 17, Iter 63, disc loss: 0.00018196990035422822, policy loss: 9.476729399633737
Experience 17, Iter 64, disc loss: 0.00022423173311661698, policy loss: 9.208013758775762
Experience 17, Iter 65, disc loss: 0.00022615895663012658, policy loss: 9.277409967232568
Experience 17, Iter 66, disc loss: 0.0001860257403830529, policy loss: 9.626912277573975
Experience 17, Iter 67, disc loss: 0.00021457581043959063, policy loss: 9.380827965360002
Experience 17, Iter 68, disc loss: 0.00023621254449685542, policy loss: 9.257686648945771
Experience 17, Iter 69, disc loss: 0.000196434099404374, policy loss: 9.415120633608478
Experience 17, Iter 70, disc loss: 0.00018761175269354786, policy loss: 9.495074130898928
Experience 17, Iter 71, disc loss: 0.00020430694605877236, policy loss: 9.414626414041075
Experience 17, Iter 72, disc loss: 0.00022146841873231147, policy loss: 9.319986848103468
Experience 17, Iter 73, disc loss: 0.00020731482130789322, policy loss: 9.43579430400877
Experience 17, Iter 74, disc loss: 0.00021656987319396508, policy loss: 9.318376920586015
Experience 17, Iter 75, disc loss: 0.00019691228057112107, policy loss: 9.537847733969993
Experience 17, Iter 76, disc loss: 0.00023282577273704087, policy loss: 9.15617629349524
Experience 17, Iter 77, disc loss: 0.000176680951149296, policy loss: 9.58488152621939
Experience 17, Iter 78, disc loss: 0.00020965554740069546, policy loss: 9.501391561400876
Experience 17, Iter 79, disc loss: 0.0002103607299616733, policy loss: 9.362939050834479
Experience 17, Iter 80, disc loss: 0.00022735849818066866, policy loss: 9.288203590219108
Experience 17, Iter 81, disc loss: 0.00023125809947995176, policy loss: 9.34170622005064
Experience 17, Iter 82, disc loss: 0.0002579334136327757, policy loss: 9.178048326379427
Experience 17, Iter 83, disc loss: 0.0002022419707870659, policy loss: 9.370071280110336
Experience 17, Iter 84, disc loss: 0.000213408902689488, policy loss: 9.24951021452227
Experience 17, Iter 85, disc loss: 0.00022860335340761922, policy loss: 9.446815712615965
Experience 17, Iter 86, disc loss: 0.00021006778087423686, policy loss: 9.23719209945937
Experience 17, Iter 87, disc loss: 0.0002063483404442439, policy loss: 9.511517285534168
Experience 17, Iter 88, disc loss: 0.00024357656078277643, policy loss: 9.276153557112444
Experience 17, Iter 89, disc loss: 0.00018917719992496087, policy loss: 9.490012131420084
Experience 17, Iter 90, disc loss: 0.0002152509164253538, policy loss: 9.399454396371796
Experience 17, Iter 91, disc loss: 0.0002625012963278635, policy loss: 9.124915293621017
Experience 17, Iter 92, disc loss: 0.0002196945518417672, policy loss: 9.210675870673485
Experience 17, Iter 93, disc loss: 0.00019367354012794178, policy loss: 9.406423945644846
Experience 17, Iter 94, disc loss: 0.00021877111158778248, policy loss: 9.160110857752784
Experience 17, Iter 95, disc loss: 0.00022460463525956042, policy loss: 9.462623209341242
Experience 17, Iter 96, disc loss: 0.00025097016208843717, policy loss: 9.176535860783432
Experience 17, Iter 97, disc loss: 0.00021249006969775697, policy loss: 9.243322205942773
Experience 17, Iter 98, disc loss: 0.00021929193571833402, policy loss: 9.281491352858543
Experience 17, Iter 99, disc loss: 0.00021872192620684533, policy loss: 9.459610473259835
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1114],
        [1.1787],
        [0.0149]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1081, 0.7499, 0.0203, 0.0033, 2.9507]],

        [[0.0122, 0.1081, 0.7499, 0.0203, 0.0033, 2.9507]],

        [[0.0122, 0.1081, 0.7499, 0.0203, 0.0033, 2.9507]],

        [[0.0122, 0.1081, 0.7499, 0.0203, 0.0033, 2.9507]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.4457, 4.7150, 0.0596], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.4457, 4.7150, 0.0596])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.508
Iter 2/2000 - Loss: 2.603
Iter 3/2000 - Loss: 2.394
Iter 4/2000 - Loss: 2.403
Iter 5/2000 - Loss: 2.439
Iter 6/2000 - Loss: 2.363
Iter 7/2000 - Loss: 2.274
Iter 8/2000 - Loss: 2.228
Iter 9/2000 - Loss: 2.204
Iter 10/2000 - Loss: 2.145
Iter 11/2000 - Loss: 2.044
Iter 12/2000 - Loss: 1.937
Iter 13/2000 - Loss: 1.844
Iter 14/2000 - Loss: 1.747
Iter 15/2000 - Loss: 1.612
Iter 16/2000 - Loss: 1.434
Iter 17/2000 - Loss: 1.236
Iter 18/2000 - Loss: 1.034
Iter 19/2000 - Loss: 0.825
Iter 20/2000 - Loss: 0.594
Iter 1981/2000 - Loss: -8.279
Iter 1982/2000 - Loss: -8.279
Iter 1983/2000 - Loss: -8.279
Iter 1984/2000 - Loss: -8.279
Iter 1985/2000 - Loss: -8.279
Iter 1986/2000 - Loss: -8.279
Iter 1987/2000 - Loss: -8.279
Iter 1988/2000 - Loss: -8.279
Iter 1989/2000 - Loss: -8.279
Iter 1990/2000 - Loss: -8.279
Iter 1991/2000 - Loss: -8.279
Iter 1992/2000 - Loss: -8.279
Iter 1993/2000 - Loss: -8.279
Iter 1994/2000 - Loss: -8.279
Iter 1995/2000 - Loss: -8.279
Iter 1996/2000 - Loss: -8.279
Iter 1997/2000 - Loss: -8.279
Iter 1998/2000 - Loss: -8.279
Iter 1999/2000 - Loss: -8.279
Iter 2000/2000 - Loss: -8.280
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[11.9782,  7.4205, 40.8669, 10.2315, 12.3234, 55.8743]],

        [[16.1474, 29.5061,  7.4154,  1.5448,  2.1296, 28.8184]],

        [[19.9100, 33.7550,  8.1300,  1.2324,  2.3011, 16.9797]],

        [[15.4985, 25.3951, 21.3629,  3.2765,  1.6385, 45.9694]]])
Signal Variance: tensor([ 0.0904,  2.5684, 14.9891,  0.7176])
Estimated target variance: tensor([0.0108, 0.4457, 4.7150, 0.0596])
N: 180
Signal to noise ratio: tensor([ 16.8040, 104.5267, 102.8957,  48.7622])
Bound on condition number: tensor([  50828.6528, 1966652.2196, 1905756.0269,  427995.8669])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.00020588183577291067, policy loss: 9.30758211738381
Experience 18, Iter 1, disc loss: 0.00025670797089797433, policy loss: 9.031101941513125
Experience 18, Iter 2, disc loss: 0.00022766651146885423, policy loss: 9.166633574988156
Experience 18, Iter 3, disc loss: 0.0002192379991680116, policy loss: 9.201980643388668
Experience 18, Iter 4, disc loss: 0.00023205808976915098, policy loss: 9.237211312368611
Experience 18, Iter 5, disc loss: 0.0002246350740729667, policy loss: 9.28170566866852
Experience 18, Iter 6, disc loss: 0.0002193563581846847, policy loss: 9.178539899257453
Experience 18, Iter 7, disc loss: 0.0002677115420735859, policy loss: 9.193726456650197
Experience 18, Iter 8, disc loss: 0.0002387575027233394, policy loss: 9.153927663455866
Experience 18, Iter 9, disc loss: 0.0002497533101875357, policy loss: 9.210240808208452
Experience 18, Iter 10, disc loss: 0.00026291901935327145, policy loss: 8.916900685898021
Experience 18, Iter 11, disc loss: 0.0002497173645173272, policy loss: 9.252915881547727
Experience 18, Iter 12, disc loss: 0.00022667758792572893, policy loss: 9.05839933776075
Experience 18, Iter 13, disc loss: 0.00024856219183801246, policy loss: 9.246201616721105
Experience 18, Iter 14, disc loss: 0.0002163405193303234, policy loss: 9.349283485893704
Experience 18, Iter 15, disc loss: 0.0002360278859400707, policy loss: 9.176150941490338
Experience 18, Iter 16, disc loss: 0.00023868473323524657, policy loss: 9.012135744458597
Experience 18, Iter 17, disc loss: 0.000237051906739239, policy loss: 9.186911067771494
Experience 18, Iter 18, disc loss: 0.00022294733423268458, policy loss: 9.348393743372513
Experience 18, Iter 19, disc loss: 0.0002432809174489235, policy loss: 9.158071944329473
Experience 18, Iter 20, disc loss: 0.00023516772375281857, policy loss: 9.086701443141866
Experience 18, Iter 21, disc loss: 0.0002624412921558359, policy loss: 9.151635598175895
Experience 18, Iter 22, disc loss: 0.0002549374640598945, policy loss: 8.941960696337745
Experience 18, Iter 23, disc loss: 0.0002818818726186087, policy loss: 9.123158862389557
Experience 18, Iter 24, disc loss: 0.0002483866923398933, policy loss: 8.982846054944556
Experience 18, Iter 25, disc loss: 0.00023916886930588574, policy loss: 9.0600871905554
Experience 18, Iter 26, disc loss: 0.00024146161771967898, policy loss: 9.273262526048487
Experience 18, Iter 27, disc loss: 0.00025367685589196265, policy loss: 9.079205875555605
Experience 18, Iter 28, disc loss: 0.0002775478286713587, policy loss: 9.012181734994096
Experience 18, Iter 29, disc loss: 0.00022490851240183115, policy loss: 9.544113175304727
Experience 18, Iter 30, disc loss: 0.00026921650601761677, policy loss: 9.149613860133861
Experience 18, Iter 31, disc loss: 0.00022776568824236062, policy loss: 9.455396936137525
Experience 18, Iter 32, disc loss: 0.000247925425826784, policy loss: 9.5110254723119
Experience 18, Iter 33, disc loss: 0.00029704714306074054, policy loss: 9.050978023355606
Experience 18, Iter 34, disc loss: 0.00023815725342976244, policy loss: 9.282777733759426
Experience 18, Iter 35, disc loss: 0.00029377994225286827, policy loss: 9.120460277134542
Experience 18, Iter 36, disc loss: 0.00025605516228373846, policy loss: 9.053726992466897
Experience 18, Iter 37, disc loss: 0.0003425373263691158, policy loss: 8.796544023435796
Experience 18, Iter 38, disc loss: 0.00035192720968473836, policy loss: 8.718040352279537
Experience 18, Iter 39, disc loss: 0.0003172221262043687, policy loss: 9.151315593551065
Experience 18, Iter 40, disc loss: 0.0003014827922400816, policy loss: 8.95338616159923
Experience 18, Iter 41, disc loss: 0.0003282252066723757, policy loss: 8.958275988953524
Experience 18, Iter 42, disc loss: 0.00029203332843163055, policy loss: 9.113478074521751
Experience 18, Iter 43, disc loss: 0.000350754357260363, policy loss: 9.052664907459574
Experience 18, Iter 44, disc loss: 0.00032382389058447964, policy loss: 8.904264050022316
Experience 18, Iter 45, disc loss: 0.0003598086992112292, policy loss: 8.781669879264655
Experience 18, Iter 46, disc loss: 0.00040726716847444425, policy loss: 8.713375037872991
Experience 18, Iter 47, disc loss: 0.0003371101587014174, policy loss: 8.684244156286427
Experience 18, Iter 48, disc loss: 0.0003254785333423246, policy loss: 8.82246054447069
Experience 18, Iter 49, disc loss: 0.0003558028387511355, policy loss: 8.790065970212122
Experience 18, Iter 50, disc loss: 0.00037953407050199774, policy loss: 8.879786527237453
Experience 18, Iter 51, disc loss: 0.0061737556267416685, policy loss: 8.452037557899057
Experience 18, Iter 52, disc loss: 0.0005308768776963514, policy loss: 8.624455248414494
Experience 18, Iter 53, disc loss: 0.0007209523692709932, policy loss: 8.806280931435364
Experience 18, Iter 54, disc loss: 0.0006285380531556606, policy loss: 8.850349826451527
Experience 18, Iter 55, disc loss: 0.0007418379693948862, policy loss: 8.863746522405911
Experience 18, Iter 56, disc loss: 0.0032326972995504774, policy loss: 8.524419107244329
Experience 18, Iter 57, disc loss: 0.0011010304662688508, policy loss: 8.319292163005134
Experience 18, Iter 58, disc loss: 0.001553129500790308, policy loss: 8.531786560580777
Experience 18, Iter 59, disc loss: 0.006118248643205146, policy loss: 8.3167390847343
Experience 18, Iter 60, disc loss: 0.0022141379959716083, policy loss: 8.419461545843877
Experience 18, Iter 61, disc loss: 0.0018964377906324932, policy loss: 8.482092841833868
Experience 18, Iter 62, disc loss: 0.008141465581037764, policy loss: 8.470192314274238
Experience 18, Iter 63, disc loss: 0.002305371754531331, policy loss: 8.732242048003943
Experience 18, Iter 64, disc loss: 0.005152039246865971, policy loss: 8.947022840422509
Experience 18, Iter 65, disc loss: 0.007959282831546718, policy loss: 8.548626256510612
Experience 18, Iter 66, disc loss: 0.0016739839635804624, policy loss: 9.436139800576617
Experience 18, Iter 67, disc loss: 0.004533731849566993, policy loss: 9.941305454671674
Experience 18, Iter 68, disc loss: 0.0025462713916600066, policy loss: 9.774595761428731
Experience 18, Iter 69, disc loss: 0.0019135794735209155, policy loss: 10.287142143178169
Experience 18, Iter 70, disc loss: 0.002486322059328735, policy loss: 10.097817780174577
Experience 18, Iter 71, disc loss: 0.0026192311977303396, policy loss: 10.572783425149574
Experience 18, Iter 72, disc loss: 0.003704422710908117, policy loss: 9.92560532349001
Experience 18, Iter 73, disc loss: 0.0036076827890132853, policy loss: 10.241721139166266
Experience 18, Iter 74, disc loss: 0.03638373969533249, policy loss: 10.258766526446554
Experience 18, Iter 75, disc loss: 0.004747266754200825, policy loss: 10.909342738728155
Experience 18, Iter 76, disc loss: 0.0044969436031031635, policy loss: 11.595834084372274
Experience 18, Iter 77, disc loss: 0.005126876937107776, policy loss: 10.153685492671794
Experience 18, Iter 78, disc loss: 0.005180777964667659, policy loss: 10.863197916908062
Experience 18, Iter 79, disc loss: 0.005460876294092142, policy loss: 10.13982197042702
Experience 18, Iter 80, disc loss: 0.004480931912127124, policy loss: 10.414884694468121
Experience 18, Iter 81, disc loss: 0.004704361552790959, policy loss: 10.421915996619088
Experience 18, Iter 82, disc loss: 0.004087580111779223, policy loss: 9.750922334700995
Experience 18, Iter 83, disc loss: 0.003358924819470863, policy loss: 9.809238292274797
Experience 18, Iter 84, disc loss: 0.002911321108588686, policy loss: 9.90711128436767
Experience 18, Iter 85, disc loss: 0.0022795423549532867, policy loss: 10.386987946820003
Experience 18, Iter 86, disc loss: 0.0019165291288932066, policy loss: 9.900328447728487
Experience 18, Iter 87, disc loss: 0.002403192893717819, policy loss: 9.35492746747528
Experience 18, Iter 88, disc loss: 0.0030330562553484426, policy loss: 9.217195918763181
Experience 18, Iter 89, disc loss: 0.002201602669621161, policy loss: 9.337529235180746
Experience 18, Iter 90, disc loss: 0.002590315146840711, policy loss: 8.367330546002515
Experience 18, Iter 91, disc loss: 0.002006999133025576, policy loss: 8.947901621980417
Experience 18, Iter 92, disc loss: 0.0017958715717429807, policy loss: 9.912454139556676
Experience 18, Iter 93, disc loss: 0.01684176879089888, policy loss: 8.6304810102682
Experience 18, Iter 94, disc loss: 0.0020001411906832483, policy loss: 9.395853511093463
Experience 18, Iter 95, disc loss: 0.002711604190848576, policy loss: 9.895933897484976
Experience 18, Iter 96, disc loss: 0.00259760186187825, policy loss: 8.836035584451638
Experience 18, Iter 97, disc loss: 0.0021751800515130305, policy loss: 9.74228887051404
Experience 18, Iter 98, disc loss: 0.003539314938967233, policy loss: 9.53539335723604
Experience 18, Iter 99, disc loss: 0.0014690836888343558, policy loss: 10.33870465978137
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1178],
        [1.2439],
        [0.0173]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0117, 0.1128, 0.8581, 0.0213, 0.0062, 3.0770]],

        [[0.0117, 0.1128, 0.8581, 0.0213, 0.0062, 3.0770]],

        [[0.0117, 0.1128, 0.8581, 0.0213, 0.0062, 3.0770]],

        [[0.0117, 0.1128, 0.8581, 0.0213, 0.0062, 3.0770]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0111, 0.4710, 4.9758, 0.0691], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0111, 0.4710, 4.9758, 0.0691])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.647
Iter 2/2000 - Loss: 2.730
Iter 3/2000 - Loss: 2.520
Iter 4/2000 - Loss: 2.537
Iter 5/2000 - Loss: 2.569
Iter 6/2000 - Loss: 2.479
Iter 7/2000 - Loss: 2.379
Iter 8/2000 - Loss: 2.331
Iter 9/2000 - Loss: 2.301
Iter 10/2000 - Loss: 2.226
Iter 11/2000 - Loss: 2.103
Iter 12/2000 - Loss: 1.970
Iter 13/2000 - Loss: 1.855
Iter 14/2000 - Loss: 1.738
Iter 15/2000 - Loss: 1.584
Iter 16/2000 - Loss: 1.384
Iter 17/2000 - Loss: 1.160
Iter 18/2000 - Loss: 0.930
Iter 19/2000 - Loss: 0.695
Iter 20/2000 - Loss: 0.442
Iter 1981/2000 - Loss: -8.168
Iter 1982/2000 - Loss: -8.168
Iter 1983/2000 - Loss: -8.168
Iter 1984/2000 - Loss: -8.168
Iter 1985/2000 - Loss: -8.168
Iter 1986/2000 - Loss: -8.168
Iter 1987/2000 - Loss: -8.168
Iter 1988/2000 - Loss: -8.168
Iter 1989/2000 - Loss: -8.168
Iter 1990/2000 - Loss: -8.168
Iter 1991/2000 - Loss: -8.168
Iter 1992/2000 - Loss: -8.168
Iter 1993/2000 - Loss: -8.168
Iter 1994/2000 - Loss: -8.168
Iter 1995/2000 - Loss: -8.168
Iter 1996/2000 - Loss: -8.168
Iter 1997/2000 - Loss: -8.168
Iter 1998/2000 - Loss: -8.168
Iter 1999/2000 - Loss: -8.168
Iter 2000/2000 - Loss: -8.168
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[12.2404,  7.2274, 36.0773,  8.6024, 13.9108, 54.7303]],

        [[14.8806, 28.8120,  9.3274,  1.5391,  2.2043, 32.7060]],

        [[18.7652, 27.6878,  9.0490,  1.1765,  1.4020, 15.4330]],

        [[14.9442, 23.4050, 15.5053,  2.2335,  2.2006, 41.1091]]])
Signal Variance: tensor([ 0.0895,  2.7227, 12.0901,  0.5483])
Estimated target variance: tensor([0.0111, 0.4710, 4.9758, 0.0691])
N: 190
Signal to noise ratio: tensor([ 16.7685, 101.0624,  90.3289,  43.3998])
Bound on condition number: tensor([  53426.0034, 1940587.2967, 1550269.2948,  357874.5048])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.021449673861764613, policy loss: 4.875183872837262
Experience 19, Iter 1, disc loss: 0.023419338421113985, policy loss: 5.23802722968491
Experience 19, Iter 2, disc loss: 0.01613303220675978, policy loss: 6.077422440936013
Experience 19, Iter 3, disc loss: 0.012543012044870024, policy loss: 6.347517666899459
Experience 19, Iter 4, disc loss: 0.011809291485646628, policy loss: 6.857790737397462
Experience 19, Iter 5, disc loss: 0.014225746629031902, policy loss: 7.14746469916977
Experience 19, Iter 6, disc loss: 0.019113657340846656, policy loss: 6.594322265760141
Experience 19, Iter 7, disc loss: 0.01946171918174998, policy loss: 6.833212971237189
Experience 19, Iter 8, disc loss: 0.01991542575344057, policy loss: 6.615196495253922
Experience 19, Iter 9, disc loss: 0.01658283900802492, policy loss: 6.165863105195646
Experience 19, Iter 10, disc loss: 0.013223205620896038, policy loss: 5.95374588489466
Experience 19, Iter 11, disc loss: 0.010533437746986299, policy loss: 5.959230043097529
Experience 19, Iter 12, disc loss: 0.013213697270411573, policy loss: 5.590722022767024
Experience 19, Iter 13, disc loss: 0.014340779186302274, policy loss: 5.284575881171543
Experience 19, Iter 14, disc loss: 0.032347368461221326, policy loss: 4.743546919842522
Experience 19, Iter 15, disc loss: 0.017092000092009473, policy loss: 5.854027981738722
Experience 19, Iter 16, disc loss: 0.02084501291405548, policy loss: 5.464409017758212
Experience 19, Iter 17, disc loss: 0.011933296040108891, policy loss: 6.544924773157271
Experience 19, Iter 18, disc loss: 0.014359173302721738, policy loss: 6.297401590382047
Experience 19, Iter 19, disc loss: 0.01638383522252896, policy loss: 6.860922854486541
Experience 19, Iter 20, disc loss: 0.015995676556555535, policy loss: 6.793304978913529
Experience 19, Iter 21, disc loss: 0.017827462049096767, policy loss: 6.785995110465052
Experience 19, Iter 22, disc loss: 0.015322247280324604, policy loss: 7.328943983529555
Experience 19, Iter 23, disc loss: 0.013747844918895602, policy loss: 6.718711701265965
Experience 19, Iter 24, disc loss: 0.01345272178393982, policy loss: 6.1144693631646545
Experience 19, Iter 25, disc loss: 0.013551812384335735, policy loss: 6.193157826248215
Experience 19, Iter 26, disc loss: 0.020649603996100538, policy loss: 5.524989489951441
Experience 19, Iter 27, disc loss: 0.01356593803439403, policy loss: 6.439428855352871
Experience 19, Iter 28, disc loss: 0.0123011458484431, policy loss: 6.399383504648572
Experience 19, Iter 29, disc loss: 0.009648380555847675, policy loss: 6.3995154089265
Experience 19, Iter 30, disc loss: 0.010734998456315863, policy loss: 7.08753679679176
Experience 19, Iter 31, disc loss: 0.010117630882051643, policy loss: 6.4768629775294215
Experience 19, Iter 32, disc loss: 0.013726499329962146, policy loss: 6.777956976012113
Experience 19, Iter 33, disc loss: 0.008572228338817735, policy loss: 7.3335521957916505
Experience 19, Iter 34, disc loss: 0.011835881028021071, policy loss: 6.706687577066583
Experience 19, Iter 35, disc loss: 0.011239883336202044, policy loss: 6.551599180149731
Experience 19, Iter 36, disc loss: 0.009665713707009367, policy loss: 7.020818824425828
Experience 19, Iter 37, disc loss: 0.008887264862460446, policy loss: 6.849453909636603
Experience 19, Iter 38, disc loss: 0.00925338138791602, policy loss: 7.361767604883893
Experience 19, Iter 39, disc loss: 0.009193753634203607, policy loss: 7.012557629308669
Experience 19, Iter 40, disc loss: 0.00843475857718678, policy loss: 7.260205864240096
Experience 19, Iter 41, disc loss: 0.010497223655029229, policy loss: 6.529612829092613
Experience 19, Iter 42, disc loss: 0.007905215901102637, policy loss: 6.6815192201194025
Experience 19, Iter 43, disc loss: 0.00904401951132595, policy loss: 6.819517877348656
Experience 19, Iter 44, disc loss: 0.0061605256670494, policy loss: 7.306354232316577
Experience 19, Iter 45, disc loss: 0.008850328606876883, policy loss: 7.329173725587638
Experience 19, Iter 46, disc loss: 0.008447878860838903, policy loss: 6.690804709199356
Experience 19, Iter 47, disc loss: 0.00899880247428144, policy loss: 6.8321459202012225
Experience 19, Iter 48, disc loss: 0.008123456330079572, policy loss: 7.397246347106498
Experience 19, Iter 49, disc loss: 0.006839095512518007, policy loss: 7.199784650394064
Experience 19, Iter 50, disc loss: 0.007556326253164258, policy loss: 7.1231791646225275
Experience 19, Iter 51, disc loss: 0.00837543112290871, policy loss: 7.275666278258985
Experience 19, Iter 52, disc loss: 0.007905496713553445, policy loss: 7.404287790668514
Experience 19, Iter 53, disc loss: 0.008350086210941353, policy loss: 7.301051644386422
Experience 19, Iter 54, disc loss: 0.007782425425435705, policy loss: 7.682845709542274
Experience 19, Iter 55, disc loss: 0.006412099855014461, policy loss: 7.6987650626014315
Experience 19, Iter 56, disc loss: 0.008156189853534007, policy loss: 7.109989166189327
Experience 19, Iter 57, disc loss: 0.009250417919569688, policy loss: 6.849289311303954
Experience 19, Iter 58, disc loss: 0.006319619476557967, policy loss: 7.678655406137528
Experience 19, Iter 59, disc loss: 0.006924891491479113, policy loss: 7.197867003195428
Experience 19, Iter 60, disc loss: 0.00705097292733587, policy loss: 7.451266172888314
Experience 19, Iter 61, disc loss: 0.0067371177067679, policy loss: 7.220925536740513
Experience 19, Iter 62, disc loss: 0.006837957272161604, policy loss: 7.614130759934353
Experience 19, Iter 63, disc loss: 0.006534044381335002, policy loss: 7.11771892307584
Experience 19, Iter 64, disc loss: 0.006254337536755748, policy loss: 7.170883601954243
Experience 19, Iter 65, disc loss: 0.006484110073269439, policy loss: 7.221411086359259
Experience 19, Iter 66, disc loss: 0.006583092577648555, policy loss: 7.224536853378804
Experience 19, Iter 67, disc loss: 0.0058977202469746425, policy loss: 7.455299771890202
Experience 19, Iter 68, disc loss: 0.0052458529992527834, policy loss: 7.565243523644179
Experience 19, Iter 69, disc loss: 0.00504795328833013, policy loss: 7.6509152098280016
Experience 19, Iter 70, disc loss: 0.006604403065177471, policy loss: 7.966879788478538
Experience 19, Iter 71, disc loss: 0.005713170447822805, policy loss: 7.808718716990142
Experience 19, Iter 72, disc loss: 0.0073684181312127575, policy loss: 7.202044342278195
Experience 19, Iter 73, disc loss: 0.006898450046929709, policy loss: 7.803161360312096
Experience 19, Iter 74, disc loss: 0.006066095147986639, policy loss: 7.300298641605839
Experience 19, Iter 75, disc loss: 0.005180142516332553, policy loss: 7.937967281573422
Experience 19, Iter 76, disc loss: 0.0052587850327739695, policy loss: 7.808905967204598
Experience 19, Iter 77, disc loss: 0.006745171149496058, policy loss: 8.449077283377921
Experience 19, Iter 78, disc loss: 0.005985872289595156, policy loss: 7.984244397144359
Experience 19, Iter 79, disc loss: 0.007161777998720814, policy loss: 7.556778759593843
Experience 19, Iter 80, disc loss: 0.005351184193759555, policy loss: 8.388068358343428
Experience 19, Iter 81, disc loss: 0.00673907934733683, policy loss: 7.901488260612341
Experience 19, Iter 82, disc loss: 0.005254469009161314, policy loss: 8.452422408876368
Experience 19, Iter 83, disc loss: 0.004943145704381088, policy loss: 8.421014581173221
Experience 19, Iter 84, disc loss: 0.00637110564602678, policy loss: 8.534263310240837
Experience 19, Iter 85, disc loss: 0.0053540950449663, policy loss: 7.703968267478827
Experience 19, Iter 86, disc loss: 0.004334094318329983, policy loss: 7.916157206177427
Experience 19, Iter 87, disc loss: 0.003716114382396797, policy loss: 8.197235731297633
Experience 19, Iter 88, disc loss: 0.00433496602399139, policy loss: 7.827908247147264
Experience 19, Iter 89, disc loss: 0.005638385459814887, policy loss: 8.492846141855441
Experience 19, Iter 90, disc loss: 0.004514842217775205, policy loss: 8.746512105856748
Experience 19, Iter 91, disc loss: 0.004841249998581615, policy loss: 7.877143923427806
Experience 19, Iter 92, disc loss: 0.00417514632656843, policy loss: 8.056921072760597
Experience 19, Iter 93, disc loss: 0.004291488498416393, policy loss: 7.693377758820862
Experience 19, Iter 94, disc loss: 0.00465460633386458, policy loss: 8.033560488628664
Experience 19, Iter 95, disc loss: 0.009126259022183465, policy loss: 7.660283809245942
Experience 19, Iter 96, disc loss: 0.00544874209396006, policy loss: 8.63367123186222
Experience 19, Iter 97, disc loss: 0.004753316874556397, policy loss: 8.282859734502539
Experience 19, Iter 98, disc loss: 0.003740971481545216, policy loss: 8.663558072421775
Experience 19, Iter 99, disc loss: 0.003931857789257615, policy loss: 8.424954625671932
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1269],
        [1.3097],
        [0.0202]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0114, 0.1186, 0.9843, 0.0220, 0.0091, 3.2614]],

        [[0.0114, 0.1186, 0.9843, 0.0220, 0.0091, 3.2614]],

        [[0.0114, 0.1186, 0.9843, 0.0220, 0.0091, 3.2614]],

        [[0.0114, 0.1186, 0.9843, 0.0220, 0.0091, 3.2614]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.5075, 5.2388, 0.0807], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.5075, 5.2388, 0.0807])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.796
Iter 2/2000 - Loss: 2.862
Iter 3/2000 - Loss: 2.653
Iter 4/2000 - Loss: 2.676
Iter 5/2000 - Loss: 2.705
Iter 6/2000 - Loss: 2.606
Iter 7/2000 - Loss: 2.496
Iter 8/2000 - Loss: 2.445
Iter 9/2000 - Loss: 2.414
Iter 10/2000 - Loss: 2.336
Iter 11/2000 - Loss: 2.204
Iter 12/2000 - Loss: 2.063
Iter 13/2000 - Loss: 1.936
Iter 14/2000 - Loss: 1.807
Iter 15/2000 - Loss: 1.641
Iter 16/2000 - Loss: 1.429
Iter 17/2000 - Loss: 1.187
Iter 18/2000 - Loss: 0.932
Iter 19/2000 - Loss: 0.667
Iter 20/2000 - Loss: 0.385
Iter 1981/2000 - Loss: -8.107
Iter 1982/2000 - Loss: -8.107
Iter 1983/2000 - Loss: -8.107
Iter 1984/2000 - Loss: -8.107
Iter 1985/2000 - Loss: -8.107
Iter 1986/2000 - Loss: -8.107
Iter 1987/2000 - Loss: -8.107
Iter 1988/2000 - Loss: -8.107
Iter 1989/2000 - Loss: -8.107
Iter 1990/2000 - Loss: -8.107
Iter 1991/2000 - Loss: -8.107
Iter 1992/2000 - Loss: -8.107
Iter 1993/2000 - Loss: -8.107
Iter 1994/2000 - Loss: -8.107
Iter 1995/2000 - Loss: -8.107
Iter 1996/2000 - Loss: -8.107
Iter 1997/2000 - Loss: -8.108
Iter 1998/2000 - Loss: -8.108
Iter 1999/2000 - Loss: -8.108
Iter 2000/2000 - Loss: -8.108
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[11.5523,  6.1525, 32.8785,  7.8533, 13.8282, 50.4963]],

        [[14.5339, 29.4027,  9.5547,  1.7135,  1.8370, 24.7401]],

        [[17.4781, 27.1221,  9.5944,  1.0891,  0.9317, 14.6469]],

        [[13.8183, 23.6151, 18.3378,  2.6019,  2.3034, 39.0402]]])
Signal Variance: tensor([ 0.0719,  2.3821, 10.1118,  0.6704])
Estimated target variance: tensor([0.0114, 0.5075, 5.2388, 0.0807])
N: 200
Signal to noise ratio: tensor([14.8199, 96.5152, 83.9606, 47.0903])
Bound on condition number: tensor([  43926.8752, 1863036.8402, 1409878.1105,  443499.8909])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.005124964616837362, policy loss: 7.091300715756003
Experience 20, Iter 1, disc loss: 0.006804785018870704, policy loss: 6.410638684597494
Experience 20, Iter 2, disc loss: 0.0208687109238451, policy loss: 5.678749986470802
Experience 20, Iter 3, disc loss: 0.06878768819972045, policy loss: 5.138018175086938
Experience 20, Iter 4, disc loss: 0.06758269262692981, policy loss: 5.947465923212162
Experience 20, Iter 5, disc loss: 0.058154735966283, policy loss: 5.8218307494766846
Experience 20, Iter 6, disc loss: 0.09702529361120824, policy loss: 6.080849575934954
Experience 20, Iter 7, disc loss: 0.09402930912820437, policy loss: 5.674262116008791
Experience 20, Iter 8, disc loss: 0.05897165596954895, policy loss: 4.601326077420329
Experience 20, Iter 9, disc loss: 0.05179862817311094, policy loss: 4.051135643889103
Experience 20, Iter 10, disc loss: 0.0767007817588095, policy loss: 3.6824418295718466
Experience 20, Iter 11, disc loss: 0.11548708590099223, policy loss: 3.8268936438389143
Experience 20, Iter 12, disc loss: 0.07460563756374622, policy loss: 4.816691999623579
Experience 20, Iter 13, disc loss: 0.060653604333567875, policy loss: 5.537913693499785
Experience 20, Iter 14, disc loss: 0.061782123395701605, policy loss: 5.517131363024469
Experience 20, Iter 15, disc loss: 0.07813598756569809, policy loss: 5.426207641767289
Experience 20, Iter 16, disc loss: 0.07257464044997153, policy loss: 6.928111936766786
Experience 20, Iter 17, disc loss: 0.054549796731122716, policy loss: 4.968389047447789
Experience 20, Iter 18, disc loss: 0.03675254059983893, policy loss: 5.252971060666601
Experience 20, Iter 19, disc loss: 0.02621855646390637, policy loss: 4.689873698465359
Experience 20, Iter 20, disc loss: 0.026436113391128516, policy loss: 4.89562325006965
Experience 20, Iter 21, disc loss: 0.041662653885829584, policy loss: 4.379481443136699
Experience 20, Iter 22, disc loss: 0.04823195567823858, policy loss: 4.480975417843744
Experience 20, Iter 23, disc loss: 0.04983041478276727, policy loss: 4.441267204049149
Experience 20, Iter 24, disc loss: 0.025893507840981614, policy loss: 5.4069617614979695
Experience 20, Iter 25, disc loss: 0.0200007396524933, policy loss: 5.946937850089061
Experience 20, Iter 26, disc loss: 0.020150964634354486, policy loss: 6.381616965029309
Experience 20, Iter 27, disc loss: 0.019818444334356904, policy loss: 6.567050852931154
Experience 20, Iter 28, disc loss: 0.02103171454294174, policy loss: 6.66612887893894
Experience 20, Iter 29, disc loss: 0.021701213801022065, policy loss: 6.416740636196845
Experience 20, Iter 30, disc loss: 0.02040726806940415, policy loss: 7.976210473312341
Experience 20, Iter 31, disc loss: 0.019320036771561716, policy loss: 6.73627397014647
Experience 20, Iter 32, disc loss: 0.01591469824200473, policy loss: 7.418279110149657
Experience 20, Iter 33, disc loss: 0.012449409861711187, policy loss: 8.722990979528607
Experience 20, Iter 34, disc loss: 0.010545367410649074, policy loss: 7.5450594965683955
Experience 20, Iter 35, disc loss: 0.009766224808829344, policy loss: 6.905846476848868
Experience 20, Iter 36, disc loss: 0.012414528441777633, policy loss: 5.880650170352471
Experience 20, Iter 37, disc loss: 0.014525218112219225, policy loss: 5.6394044779437795
Experience 20, Iter 38, disc loss: 0.011376127369459955, policy loss: 6.225134582127157
Experience 20, Iter 39, disc loss: 0.013569520198219828, policy loss: 6.3647849274816615
Experience 20, Iter 40, disc loss: 0.010059397479976695, policy loss: 6.014753060718276
Experience 20, Iter 41, disc loss: 0.005890964669760535, policy loss: 6.826174449223756
Experience 20, Iter 42, disc loss: 0.005392670389443461, policy loss: 7.093368235987927
Experience 20, Iter 43, disc loss: 0.006230802022753495, policy loss: 6.891353243581706
Experience 20, Iter 44, disc loss: 0.003494408810241491, policy loss: 10.456908941811552
Experience 20, Iter 45, disc loss: 0.004856882522226847, policy loss: 7.303578146444525
Experience 20, Iter 46, disc loss: 0.0033878787908345753, policy loss: 8.24477348344518
Experience 20, Iter 47, disc loss: 0.0033280621930007878, policy loss: 8.350069853102013
Experience 20, Iter 48, disc loss: 0.0032605088887076457, policy loss: 8.090476874934932
Experience 20, Iter 49, disc loss: 0.00322652723525833, policy loss: 8.07731287267064
Experience 20, Iter 50, disc loss: 0.0032043988824655124, policy loss: 7.495098682277709
Experience 20, Iter 51, disc loss: 0.0031084028527029193, policy loss: 7.742346577040943
Experience 20, Iter 52, disc loss: 0.0034081988571113345, policy loss: 7.305047363700847
Experience 20, Iter 53, disc loss: 0.0040147438190861395, policy loss: 7.112883560333316
Experience 20, Iter 54, disc loss: 0.004707925739258901, policy loss: 7.173118200286266
Experience 20, Iter 55, disc loss: 0.009494890021904515, policy loss: 6.5459269155794315
Experience 20, Iter 56, disc loss: 0.01562532437250423, policy loss: 6.391879786334064
Experience 20, Iter 57, disc loss: 0.012794226681582095, policy loss: 6.417735487461259
Experience 20, Iter 58, disc loss: 0.010252538069888475, policy loss: 8.062415163347163
Experience 20, Iter 59, disc loss: 0.012373017639100668, policy loss: 6.995653840310359
Experience 20, Iter 60, disc loss: 0.010678307487847902, policy loss: 7.2927151031549995
Experience 20, Iter 61, disc loss: 0.011592304500017137, policy loss: 7.292881876288297
Experience 20, Iter 62, disc loss: 0.008984690399742628, policy loss: 6.580528606381494
Experience 20, Iter 63, disc loss: 0.00699674689352197, policy loss: 8.596329418740787
Experience 20, Iter 64, disc loss: 0.009429326139143338, policy loss: 7.242789283945861
Experience 20, Iter 65, disc loss: 0.00832855855618437, policy loss: 7.884955386014517
Experience 20, Iter 66, disc loss: 0.008221594460596355, policy loss: 8.547994636080073
Experience 20, Iter 67, disc loss: 0.007885760521823722, policy loss: 8.543220507254501
Experience 20, Iter 68, disc loss: 0.008171651687912257, policy loss: 8.000959986989585
Experience 20, Iter 69, disc loss: 0.00966460473904761, policy loss: 6.949867721327374
Experience 20, Iter 70, disc loss: 0.008455381256495456, policy loss: 8.390969900598371
Experience 20, Iter 71, disc loss: 0.008010735070600409, policy loss: 8.320413627692874
Experience 20, Iter 72, disc loss: 0.0074080116225903984, policy loss: 7.298144386276833
Experience 20, Iter 73, disc loss: 0.005062380357958914, policy loss: 8.371878983416725
Experience 20, Iter 74, disc loss: 0.004111898430658812, policy loss: 10.0400156990084
Experience 20, Iter 75, disc loss: 0.0035478017301841967, policy loss: 9.878793170163814
Experience 20, Iter 76, disc loss: 0.0033080292040953987, policy loss: 9.301306362754023
Experience 20, Iter 77, disc loss: 0.0031496243634452532, policy loss: 8.791166943142718
Experience 20, Iter 78, disc loss: 0.003028440479230018, policy loss: 8.396028310295819
Experience 20, Iter 79, disc loss: 0.002816022057521971, policy loss: 7.869820098482059
Experience 20, Iter 80, disc loss: 0.0024456417769810664, policy loss: 8.229041998442398
Experience 20, Iter 81, disc loss: 0.0024331354432019477, policy loss: 8.0248104163589
Experience 20, Iter 82, disc loss: 0.00338981737346616, policy loss: 7.027058257804535
Experience 20, Iter 83, disc loss: 0.0031841221462726135, policy loss: 6.947198150381766
Experience 20, Iter 84, disc loss: 0.0040868842130873505, policy loss: 7.32376803023362
Experience 20, Iter 85, disc loss: 0.0033224901434620287, policy loss: 7.459334176147889
Experience 20, Iter 86, disc loss: 0.0037505995536989624, policy loss: 7.691250120038493
Experience 20, Iter 87, disc loss: 0.004419515818692644, policy loss: 6.4536321354634
Experience 20, Iter 88, disc loss: 0.004881365444284348, policy loss: 6.288360435267103
Experience 20, Iter 89, disc loss: 0.006544831600611548, policy loss: 6.222673878768998
Experience 20, Iter 90, disc loss: 0.00721714537147308, policy loss: 6.3659001766310315
Experience 20, Iter 91, disc loss: 0.008447737150644523, policy loss: 5.932162914629721
Experience 20, Iter 92, disc loss: 0.009912980426466766, policy loss: 5.609368863464428
Experience 20, Iter 93, disc loss: 0.009905299093873407, policy loss: 6.201614287831363
Experience 20, Iter 94, disc loss: 0.014290321875517153, policy loss: 5.793415227892561
Experience 20, Iter 95, disc loss: 0.006975855978839133, policy loss: 6.883077730813977
Experience 20, Iter 96, disc loss: 0.008471950923737218, policy loss: 6.533159425097027
Experience 20, Iter 97, disc loss: 0.006834894442603514, policy loss: 6.708016848498055
Experience 20, Iter 98, disc loss: 0.006378745816663878, policy loss: 7.23823647682252
Experience 20, Iter 99, disc loss: 0.006408455136225745, policy loss: 7.353187676487557
