Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0010],
        [0.0787],
        [0.0018]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0014, 0.0042, 0.3148, 0.0073], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0014, 0.0042, 0.3148, 0.0073])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.281
Iter 2/2000 - Loss: -2.486
Iter 3/2000 - Loss: -2.302
Iter 4/2000 - Loss: -2.248
Iter 5/2000 - Loss: -2.348
Iter 6/2000 - Loss: -2.531
Iter 7/2000 - Loss: -2.705
Iter 8/2000 - Loss: -2.897
Iter 9/2000 - Loss: -3.027
Iter 10/2000 - Loss: -3.023
Iter 11/2000 - Loss: -2.928
Iter 12/2000 - Loss: -2.836
Iter 13/2000 - Loss: -2.826
Iter 14/2000 - Loss: -2.917
Iter 15/2000 - Loss: -3.055
Iter 16/2000 - Loss: -3.170
Iter 17/2000 - Loss: -3.219
Iter 18/2000 - Loss: -3.203
Iter 19/2000 - Loss: -3.158
Iter 20/2000 - Loss: -3.132
Iter 1981/2000 - Loss: -3.605
Iter 1982/2000 - Loss: -3.605
Iter 1983/2000 - Loss: -3.605
Iter 1984/2000 - Loss: -3.605
Iter 1985/2000 - Loss: -3.605
Iter 1986/2000 - Loss: -3.605
Iter 1987/2000 - Loss: -3.605
Iter 1988/2000 - Loss: -3.605
Iter 1989/2000 - Loss: -3.605
Iter 1990/2000 - Loss: -3.605
Iter 1991/2000 - Loss: -3.605
Iter 1992/2000 - Loss: -3.605
Iter 1993/2000 - Loss: -3.605
Iter 1994/2000 - Loss: -3.605
Iter 1995/2000 - Loss: -3.605
Iter 1996/2000 - Loss: -3.605
Iter 1997/2000 - Loss: -3.605
Iter 1998/2000 - Loss: -3.605
Iter 1999/2000 - Loss: -3.605
Iter 2000/2000 - Loss: -3.605
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0008],
        [0.0535],
        [0.0013]])
Lengthscale: tensor([[[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]],

        [[3.9009e-03, 1.5828e-02, 7.9702e-02, 1.9801e-03, 4.5586e-05,
          9.8723e-03]]])
Signal Variance: tensor([0.0010, 0.0030, 0.2298, 0.0053])
Estimated target variance: tensor([0.0014, 0.0042, 0.3148, 0.0073])
N: 10
Signal to noise ratio: tensor([1.9995, 2.0013, 2.0724, 2.0017])
Bound on condition number: tensor([40.9800, 41.0536, 43.9492, 41.0687])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4392797705597693, policy loss: 0.8187744514380724
Experience 1, Iter 1, disc loss: 1.4256466206849236, policy loss: 0.8265902172826083
Experience 1, Iter 2, disc loss: 1.4193922580848497, policy loss: 0.825064058950817
Experience 1, Iter 3, disc loss: 1.4133408160644558, policy loss: 0.8235952576384591
Experience 1, Iter 4, disc loss: 1.4060970119776748, policy loss: 0.8239712338845897
Experience 1, Iter 5, disc loss: 1.395300869406796, policy loss: 0.8293597181904615
Experience 1, Iter 6, disc loss: 1.3864483982290476, policy loss: 0.8328692492087422
Experience 1, Iter 7, disc loss: 1.3821549777087552, policy loss: 0.8312478408877859
Experience 1, Iter 8, disc loss: 1.3764168041164595, policy loss: 0.8316276237696381
Experience 1, Iter 9, disc loss: 1.3702979082408024, policy loss: 0.8329159247565328
Experience 1, Iter 10, disc loss: 1.362395188681466, policy loss: 0.8367007574691065
Experience 1, Iter 11, disc loss: 1.3529989042754198, policy loss: 0.8423352251194
Experience 1, Iter 12, disc loss: 1.34750379862411, policy loss: 0.8423736462717795
Experience 1, Iter 13, disc loss: 1.3379249349916214, policy loss: 0.84882122564916
Experience 1, Iter 14, disc loss: 1.3291313707049255, policy loss: 0.8539347815644374
Experience 1, Iter 15, disc loss: 1.3200114085084658, policy loss: 0.8596427917503812
Experience 1, Iter 16, disc loss: 1.3181506574593151, policy loss: 0.8552972105754788
Experience 1, Iter 17, disc loss: 1.308893127891607, policy loss: 0.8615257809712257
Experience 1, Iter 18, disc loss: 1.3018060184713405, policy loss: 0.8645072885031442
Experience 1, Iter 19, disc loss: 1.2963009233225562, policy loss: 0.8655290309917798
Experience 1, Iter 20, disc loss: 1.2876209028276113, policy loss: 0.8708537432013537
Experience 1, Iter 21, disc loss: 1.2792509554360834, policy loss: 0.8763254005279513
Experience 1, Iter 22, disc loss: 1.2705455014109495, policy loss: 0.881557657623186
Experience 1, Iter 23, disc loss: 1.2604857918272274, policy loss: 0.8896031785977687
Experience 1, Iter 24, disc loss: 1.2473151892612169, policy loss: 0.9017282759591674
Experience 1, Iter 25, disc loss: 1.2354799238252148, policy loss: 0.9120206001666415
Experience 1, Iter 26, disc loss: 1.2268196494542942, policy loss: 0.9183073195945037
Experience 1, Iter 27, disc loss: 1.227452447526042, policy loss: 0.9084414641980499
Experience 1, Iter 28, disc loss: 1.2102304906874437, policy loss: 0.92724014812069
Experience 1, Iter 29, disc loss: 1.2112518312094775, policy loss: 0.9164675235772426
Experience 1, Iter 30, disc loss: 1.1885089149345685, policy loss: 0.9456803194362063
Experience 1, Iter 31, disc loss: 1.1778175632852599, policy loss: 0.9531468313921527
Experience 1, Iter 32, disc loss: 1.1666191449920267, policy loss: 0.9629019237746804
Experience 1, Iter 33, disc loss: 1.148820425411949, policy loss: 0.9833079911976297
Experience 1, Iter 34, disc loss: 1.1544634789090626, policy loss: 0.9638254117632128
Experience 1, Iter 35, disc loss: 1.1402594921433624, policy loss: 0.9781404626886194
Experience 1, Iter 36, disc loss: 1.1190283857128165, policy loss: 1.0061064029313653
Experience 1, Iter 37, disc loss: 1.1081637939757985, policy loss: 1.0168235383774498
Experience 1, Iter 38, disc loss: 1.0858676415589783, policy loss: 1.0500045013876766
Experience 1, Iter 39, disc loss: 1.1034537474732826, policy loss: 1.0088137807192696
Experience 1, Iter 40, disc loss: 1.0763971934624124, policy loss: 1.0550825712182594
Experience 1, Iter 41, disc loss: 1.0709496128780878, policy loss: 1.0532445622147542
Experience 1, Iter 42, disc loss: 1.0619165022989963, policy loss: 1.0638172225906766
Experience 1, Iter 43, disc loss: 1.0423933665757343, policy loss: 1.0945525877545217
Experience 1, Iter 44, disc loss: 1.041629498068999, policy loss: 1.0853242624263657
Experience 1, Iter 45, disc loss: 1.0148067800229772, policy loss: 1.1356433328817013
Experience 1, Iter 46, disc loss: 1.0141608973978087, policy loss: 1.1265558093270522
Experience 1, Iter 47, disc loss: 0.9964993744474586, policy loss: 1.1552243249133052
Experience 1, Iter 48, disc loss: 0.9766319548061465, policy loss: 1.184767212463013
Experience 1, Iter 49, disc loss: 0.9617332314346365, policy loss: 1.2051745218907621
Experience 1, Iter 50, disc loss: 0.952136311833244, policy loss: 1.219791249041251
Experience 1, Iter 51, disc loss: 0.9407753989827012, policy loss: 1.2322237133696976
Experience 1, Iter 52, disc loss: 0.9212051152948197, policy loss: 1.2722645178740217
Experience 1, Iter 53, disc loss: 0.9215366879429219, policy loss: 1.2575745940597132
Experience 1, Iter 54, disc loss: 0.9140654907709647, policy loss: 1.2664670794432404
Experience 1, Iter 55, disc loss: 0.9024403944033959, policy loss: 1.2735631116156108
Experience 1, Iter 56, disc loss: 0.8829590523204098, policy loss: 1.311947282140221
Experience 1, Iter 57, disc loss: 0.8729510572817369, policy loss: 1.3332679390330613
Experience 1, Iter 58, disc loss: 0.8456624580711432, policy loss: 1.385407513530144
Experience 1, Iter 59, disc loss: 0.845012885970594, policy loss: 1.379639880218207
Experience 1, Iter 60, disc loss: 0.8341685858285767, policy loss: 1.3824008259777247
Experience 1, Iter 61, disc loss: 0.8314174887910326, policy loss: 1.3784470125834867
Experience 1, Iter 62, disc loss: 0.7959481486211848, policy loss: 1.4662124328464419
Experience 1, Iter 63, disc loss: 0.7792030020829283, policy loss: 1.5078511760074527
Experience 1, Iter 64, disc loss: 0.7696388007136146, policy loss: 1.5213042006208939
Experience 1, Iter 65, disc loss: 0.77305680898735, policy loss: 1.4868639939605321
Experience 1, Iter 66, disc loss: 0.7328638871528825, policy loss: 1.6059616525053713
Experience 1, Iter 67, disc loss: 0.7322966507799922, policy loss: 1.5986789542469544
Experience 1, Iter 68, disc loss: 0.7277477427586423, policy loss: 1.5891858319136436
Experience 1, Iter 69, disc loss: 0.6909099390896642, policy loss: 1.7129846987953634
Experience 1, Iter 70, disc loss: 0.6882497438414765, policy loss: 1.6858105984847427
Experience 1, Iter 71, disc loss: 0.6802757820201241, policy loss: 1.6962075681626745
Experience 1, Iter 72, disc loss: 0.6516876185772845, policy loss: 1.8002192958320098
Experience 1, Iter 73, disc loss: 0.6560946297345245, policy loss: 1.751743699870293
Experience 1, Iter 74, disc loss: 0.6223794570333906, policy loss: 1.8695378568113084
Experience 1, Iter 75, disc loss: 0.6032562786300498, policy loss: 1.975483005596235
Experience 1, Iter 76, disc loss: 0.6072368122440512, policy loss: 1.8838433579817613
Experience 1, Iter 77, disc loss: 0.5860272086963456, policy loss: 1.9779315801313793
Experience 1, Iter 78, disc loss: 0.5883860180401705, policy loss: 1.9352926629237253
Experience 1, Iter 79, disc loss: 0.5634051336662441, policy loss: 2.0512644024556965
Experience 1, Iter 80, disc loss: 0.5445345472068075, policy loss: 2.08634386769041
Experience 1, Iter 81, disc loss: 0.5299637638890842, policy loss: 2.1352113089287257
Experience 1, Iter 82, disc loss: 0.5292394771329911, policy loss: 2.112876212993483
Experience 1, Iter 83, disc loss: 0.498996153386154, policy loss: 2.2741577412209617
Experience 1, Iter 84, disc loss: 0.4936339915808128, policy loss: 2.263110930354679
Experience 1, Iter 85, disc loss: 0.48069161969689583, policy loss: 2.335530799390371
Experience 1, Iter 86, disc loss: 0.48065421049464563, policy loss: 2.2989031738755306
Experience 1, Iter 87, disc loss: 0.4471094942669676, policy loss: 2.4817262021726503
Experience 1, Iter 88, disc loss: 0.44669919991920887, policy loss: 2.4579323692351505
Experience 1, Iter 89, disc loss: 0.4445369775034067, policy loss: 2.4191324721260448
Experience 1, Iter 90, disc loss: 0.4298794529584292, policy loss: 2.486737418279291
Experience 1, Iter 91, disc loss: 0.4179311126235433, policy loss: 2.572522141682593
Experience 1, Iter 92, disc loss: 0.3987180068511447, policy loss: 2.6950754398242704
Experience 1, Iter 93, disc loss: 0.39471314224910775, policy loss: 2.679620830109098
Experience 1, Iter 94, disc loss: 0.38456596133298604, policy loss: 2.7230125289681357
Experience 1, Iter 95, disc loss: 0.38524131540444717, policy loss: 2.6390309398450325
Experience 1, Iter 96, disc loss: 0.3713784458868757, policy loss: 2.688740507672115
Experience 1, Iter 97, disc loss: 0.36330602078399465, policy loss: 2.67638375764733
Experience 1, Iter 98, disc loss: 0.3549609094190056, policy loss: 2.7356876074566925
Experience 1, Iter 99, disc loss: 0.34089001039343964, policy loss: 2.9312423779223344
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0028],
        [0.0447],
        [0.0011]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0114, 0.1786, 0.0042], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0114, 0.1786, 0.0042])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.921
Iter 2/2000 - Loss: -1.043
Iter 3/2000 - Loss: -2.898
Iter 4/2000 - Loss: -2.711
Iter 5/2000 - Loss: -2.024
Iter 6/2000 - Loss: -2.239
Iter 7/2000 - Loss: -2.775
Iter 8/2000 - Loss: -2.985
Iter 9/2000 - Loss: -2.828
Iter 10/2000 - Loss: -2.622
Iter 11/2000 - Loss: -2.579
Iter 12/2000 - Loss: -2.683
Iter 13/2000 - Loss: -2.822
Iter 14/2000 - Loss: -2.908
Iter 15/2000 - Loss: -2.908
Iter 16/2000 - Loss: -2.854
Iter 17/2000 - Loss: -2.815
Iter 18/2000 - Loss: -2.841
Iter 19/2000 - Loss: -2.909
Iter 20/2000 - Loss: -2.959
Iter 1981/2000 - Loss: -3.118
Iter 1982/2000 - Loss: -3.118
Iter 1983/2000 - Loss: -3.118
Iter 1984/2000 - Loss: -3.118
Iter 1985/2000 - Loss: -3.118
Iter 1986/2000 - Loss: -3.118
Iter 1987/2000 - Loss: -3.118
Iter 1988/2000 - Loss: -3.118
Iter 1989/2000 - Loss: -3.118
Iter 1990/2000 - Loss: -3.118
Iter 1991/2000 - Loss: -3.118
Iter 1992/2000 - Loss: -3.118
Iter 1993/2000 - Loss: -3.118
Iter 1994/2000 - Loss: -3.118
Iter 1995/2000 - Loss: -3.118
Iter 1996/2000 - Loss: -3.118
Iter 1997/2000 - Loss: -3.118
Iter 1998/2000 - Loss: -3.118
Iter 1999/2000 - Loss: -3.118
Iter 2000/2000 - Loss: -3.118
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0021],
        [0.0330],
        [0.0008]])
Lengthscale: tensor([[[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]],

        [[3.3867e-03, 2.7133e-02, 4.5527e-02, 2.1097e-03, 2.9973e-05,
          1.2296e-01]]])
Signal Variance: tensor([0.0025, 0.0087, 0.1367, 0.0032])
Estimated target variance: tensor([0.0033, 0.0114, 0.1786, 0.0042])
N: 20
Signal to noise ratio: tensor([2.0012, 2.0092, 2.0368, 2.0013])
Bound on condition number: tensor([81.0991, 81.7416, 83.9679, 81.1060])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.5087058509256289, policy loss: 1.6487238647030735
Experience 2, Iter 1, disc loss: 0.47723303231158637, policy loss: 1.7920269909027466
Experience 2, Iter 2, disc loss: 0.47333989027030043, policy loss: 1.754670445825673
Experience 2, Iter 3, disc loss: 0.4552183223521388, policy loss: 1.8179739757177886
Experience 2, Iter 4, disc loss: 0.45674334634269376, policy loss: 1.7893199114940896
Experience 2, Iter 5, disc loss: 0.4261014636839675, policy loss: 1.8884771916771301
Experience 2, Iter 6, disc loss: 0.4227225231697708, policy loss: 1.900352166302433
Experience 2, Iter 7, disc loss: 0.44530039963724677, policy loss: 1.7664732782459396
Experience 2, Iter 8, disc loss: 0.39176518505673974, policy loss: 1.9520868178915487
Experience 2, Iter 9, disc loss: 0.39120740351671535, policy loss: 1.9519397618443661
Experience 2, Iter 10, disc loss: 0.40269753244922146, policy loss: 1.8850286604828208
Experience 2, Iter 11, disc loss: 0.3815310450147004, policy loss: 1.9794438407520347
Experience 2, Iter 12, disc loss: 0.4001836369984706, policy loss: 1.8989153736825903
Experience 2, Iter 13, disc loss: 0.37976346134107486, policy loss: 1.9259426453901138
Experience 2, Iter 14, disc loss: 0.3598110487339297, policy loss: 2.0254480317203387
Experience 2, Iter 15, disc loss: 0.3438883570721539, policy loss: 2.1214847126270255
Experience 2, Iter 16, disc loss: 0.3474248281597727, policy loss: 2.100528520521989
Experience 2, Iter 17, disc loss: 0.341850813902013, policy loss: 1.9941649123878165
Experience 2, Iter 18, disc loss: 0.34460363291627205, policy loss: 1.99885312488763
Experience 2, Iter 19, disc loss: 0.3023614404287259, policy loss: 2.268704441370442
Experience 2, Iter 20, disc loss: 0.3163883060442211, policy loss: 2.1423411828977077
Experience 2, Iter 21, disc loss: 0.29858210780717076, policy loss: 2.259730802786398
Experience 2, Iter 22, disc loss: 0.28543201463810386, policy loss: 2.3109147872062143
Experience 2, Iter 23, disc loss: 0.28752893421462444, policy loss: 2.270443614332825
Experience 2, Iter 24, disc loss: 0.26885125505157004, policy loss: 2.3389369953734693
Experience 2, Iter 25, disc loss: 0.2969979991449625, policy loss: 2.129532834896799
Experience 2, Iter 26, disc loss: 0.2669735068607293, policy loss: 2.2932451266221823
Experience 2, Iter 27, disc loss: 0.24634377003658395, policy loss: 2.469751229144353
Experience 2, Iter 28, disc loss: 0.23656969135491546, policy loss: 2.484988738890984
Experience 2, Iter 29, disc loss: 0.2558687854482415, policy loss: 2.3965512969981897
Experience 2, Iter 30, disc loss: 0.25180679411579865, policy loss: 2.4012234619180273
Experience 2, Iter 31, disc loss: 0.2185761340441877, policy loss: 2.602093150404024
Experience 2, Iter 32, disc loss: 0.22160775905932661, policy loss: 2.528401666845239
Experience 2, Iter 33, disc loss: 0.22454839507055482, policy loss: 2.5370288419911042
Experience 2, Iter 34, disc loss: 0.20785766126290797, policy loss: 2.7084980201478017
Experience 2, Iter 35, disc loss: 0.20661513252445257, policy loss: 2.5418524395334656
Experience 2, Iter 36, disc loss: 0.21648166763690518, policy loss: 2.41065094439926
Experience 2, Iter 37, disc loss: 0.20403613827622288, policy loss: 2.6236111170440015
Experience 2, Iter 38, disc loss: 0.20001745515049707, policy loss: 2.6809903347345685
Experience 2, Iter 39, disc loss: 0.18944890306867618, policy loss: 2.7017420416303897
Experience 2, Iter 40, disc loss: 0.19645644303859952, policy loss: 2.7843846867974174
Experience 2, Iter 41, disc loss: 0.19827847743432847, policy loss: 2.7486584899715574
Experience 2, Iter 42, disc loss: 0.1771322660517027, policy loss: 2.717033489300544
Experience 2, Iter 43, disc loss: 0.17911279477155578, policy loss: 3.0358450102927734
Experience 2, Iter 44, disc loss: 0.18611548851021492, policy loss: 2.712674399322574
Experience 2, Iter 45, disc loss: 0.15930205497936956, policy loss: 2.81083011097858
Experience 2, Iter 46, disc loss: 0.15220853198955991, policy loss: 3.0151466916967813
Experience 2, Iter 47, disc loss: 0.1514296674142475, policy loss: 2.9920319052251108
Experience 2, Iter 48, disc loss: 0.1423512810345844, policy loss: 3.0565134747039693
Experience 2, Iter 49, disc loss: 0.15491258257964197, policy loss: 2.8560245136028657
Experience 2, Iter 50, disc loss: 0.1382687302990093, policy loss: 3.0728963636360636
Experience 2, Iter 51, disc loss: 0.14352136296316395, policy loss: 3.0009682561593074
Experience 2, Iter 52, disc loss: 0.1258033577512092, policy loss: 3.1261800821604253
Experience 2, Iter 53, disc loss: 0.15260458437913188, policy loss: 2.9392729872418784
Experience 2, Iter 54, disc loss: 0.1462777285418433, policy loss: 2.949369286172404
Experience 2, Iter 55, disc loss: 0.12836889059252415, policy loss: 2.9549139887216205
Experience 2, Iter 56, disc loss: 0.1268239892119392, policy loss: 3.1207965561369964
Experience 2, Iter 57, disc loss: 0.12969997512057957, policy loss: 3.0130413687919404
Experience 2, Iter 58, disc loss: 0.12037906324146533, policy loss: 3.150224777631715
Experience 2, Iter 59, disc loss: 0.12776963372402206, policy loss: 3.0442349357948215
Experience 2, Iter 60, disc loss: 0.12092637006162382, policy loss: 3.267524131755701
Experience 2, Iter 61, disc loss: 0.11643466084242272, policy loss: 3.2682377041172193
Experience 2, Iter 62, disc loss: 0.1165920048772146, policy loss: 3.20423266736586
Experience 2, Iter 63, disc loss: 0.10489825697512066, policy loss: 3.5103199186945786
Experience 2, Iter 64, disc loss: 0.09939204503923274, policy loss: 3.5996817226341786
Experience 2, Iter 65, disc loss: 0.09630462788291602, policy loss: 3.5034338231157784
Experience 2, Iter 66, disc loss: 0.08300326730721569, policy loss: 3.739202503261012
Experience 2, Iter 67, disc loss: 0.10300382177340967, policy loss: 3.472766410146796
Experience 2, Iter 68, disc loss: 0.09773225889033554, policy loss: 3.445750706950607
Experience 2, Iter 69, disc loss: 0.09866329843151476, policy loss: 3.539072326916868
Experience 2, Iter 70, disc loss: 0.08926402966447347, policy loss: 3.6680996567417
Experience 2, Iter 71, disc loss: 0.09197736338847465, policy loss: 3.408802916502931
Experience 2, Iter 72, disc loss: 0.09891340674354242, policy loss: 3.418805017631242
Experience 2, Iter 73, disc loss: 0.09498314055797011, policy loss: 3.452163185465941
Experience 2, Iter 74, disc loss: 0.08476189278760027, policy loss: 3.7797812680274756
Experience 2, Iter 75, disc loss: 0.07699837118741645, policy loss: 3.7007618658707484
Experience 2, Iter 76, disc loss: 0.08223123660149485, policy loss: 3.5565733835234647
Experience 2, Iter 77, disc loss: 0.09144165583301009, policy loss: 3.64198699452801
Experience 2, Iter 78, disc loss: 0.07454307919177536, policy loss: 3.654150098284134
Experience 2, Iter 79, disc loss: 0.07787801644188429, policy loss: 3.8893174490202176
Experience 2, Iter 80, disc loss: 0.07504225087456268, policy loss: 3.7922841272157948
Experience 2, Iter 81, disc loss: 0.07234445401482259, policy loss: 3.8628933893103268
Experience 2, Iter 82, disc loss: 0.07175051663696935, policy loss: 3.865848140981856
Experience 2, Iter 83, disc loss: 0.07855338450855903, policy loss: 3.6659620718642785
Experience 2, Iter 84, disc loss: 0.0721036525248851, policy loss: 3.7908113027604737
Experience 2, Iter 85, disc loss: 0.06754931359040006, policy loss: 3.8344835393880428
Experience 2, Iter 86, disc loss: 0.06945839548733959, policy loss: 3.745666594968604
Experience 2, Iter 87, disc loss: 0.061130534627145516, policy loss: 4.123539328948721
Experience 2, Iter 88, disc loss: 0.06653730339192815, policy loss: 3.9688911404676497
Experience 2, Iter 89, disc loss: 0.06845779411657182, policy loss: 3.8568865193017534
Experience 2, Iter 90, disc loss: 0.0730526711407619, policy loss: 3.959512860629045
Experience 2, Iter 91, disc loss: 0.06663610710973744, policy loss: 3.831369648634353
Experience 2, Iter 92, disc loss: 0.06025027529464748, policy loss: 4.071113296792534
Experience 2, Iter 93, disc loss: 0.05934011043749459, policy loss: 4.160624975663666
Experience 2, Iter 94, disc loss: 0.05822195759568728, policy loss: 4.1049584259855765
Experience 2, Iter 95, disc loss: 0.06886190558859227, policy loss: 3.8278897373962333
Experience 2, Iter 96, disc loss: 0.06552289259632672, policy loss: 4.034732095935144
Experience 2, Iter 97, disc loss: 0.05434806492962797, policy loss: 4.039372076193051
Experience 2, Iter 98, disc loss: 0.05197325975190341, policy loss: 4.1346923728215925
Experience 2, Iter 99, disc loss: 0.06278349898532298, policy loss: 3.982202967436634
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0082],
        [0.0320],
        [0.5289],
        [0.0199]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0736, 0.3500, 0.8577, 0.0184, 0.0074, 1.3332]],

        [[0.0736, 0.3500, 0.8577, 0.0184, 0.0074, 1.3332]],

        [[0.0736, 0.3500, 0.8577, 0.0184, 0.0074, 1.3332]],

        [[0.0736, 0.3500, 0.8577, 0.0184, 0.0074, 1.3332]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0327, 0.1280, 2.1158, 0.0797], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0327, 0.1280, 2.1158, 0.0797])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.137
Iter 2/2000 - Loss: 1.922
Iter 3/2000 - Loss: 1.909
Iter 4/2000 - Loss: 1.878
Iter 5/2000 - Loss: 1.786
Iter 6/2000 - Loss: 1.690
Iter 7/2000 - Loss: 1.624
Iter 8/2000 - Loss: 1.564
Iter 9/2000 - Loss: 1.472
Iter 10/2000 - Loss: 1.345
Iter 11/2000 - Loss: 1.204
Iter 12/2000 - Loss: 1.064
Iter 13/2000 - Loss: 0.923
Iter 14/2000 - Loss: 0.774
Iter 15/2000 - Loss: 0.608
Iter 16/2000 - Loss: 0.425
Iter 17/2000 - Loss: 0.229
Iter 18/2000 - Loss: 0.026
Iter 19/2000 - Loss: -0.178
Iter 20/2000 - Loss: -0.384
Iter 1981/2000 - Loss: -5.686
Iter 1982/2000 - Loss: -5.686
Iter 1983/2000 - Loss: -5.686
Iter 1984/2000 - Loss: -5.686
Iter 1985/2000 - Loss: -5.686
Iter 1986/2000 - Loss: -5.686
Iter 1987/2000 - Loss: -5.686
Iter 1988/2000 - Loss: -5.686
Iter 1989/2000 - Loss: -5.686
Iter 1990/2000 - Loss: -5.686
Iter 1991/2000 - Loss: -5.686
Iter 1992/2000 - Loss: -5.686
Iter 1993/2000 - Loss: -5.687
Iter 1994/2000 - Loss: -5.687
Iter 1995/2000 - Loss: -5.687
Iter 1996/2000 - Loss: -5.687
Iter 1997/2000 - Loss: -5.687
Iter 1998/2000 - Loss: -5.687
Iter 1999/2000 - Loss: -5.687
Iter 2000/2000 - Loss: -5.687
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[19.3951,  5.0878, 43.8273,  9.5450,  4.8667, 42.9466]],

        [[30.3115, 52.9446,  5.1591,  2.6101, 15.3460, 14.7461]],

        [[29.9649, 50.4235, 16.7035,  0.7141,  2.7445, 12.3824]],

        [[31.8690, 52.9723, 13.5053,  3.7188,  1.3233, 39.0136]]])
Signal Variance: tensor([0.0812, 0.9689, 7.3558, 0.3704])
Estimated target variance: tensor([0.0327, 0.1280, 2.1158, 0.0797])
N: 30
Signal to noise ratio: tensor([17.4702, 53.1896, 56.9896, 40.6735])
Bound on condition number: tensor([ 9157.2483, 84874.9161, 97435.4925, 49631.0636])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.4248795184301537, policy loss: 1.3881451825543625
Experience 3, Iter 1, disc loss: 0.45023110838076175, policy loss: 1.4790558079865046
Experience 3, Iter 2, disc loss: 0.4309402573425628, policy loss: 1.448492934741327
Experience 3, Iter 3, disc loss: 0.44039587872187486, policy loss: 1.5324518583336895
Experience 3, Iter 4, disc loss: 0.4456348383409345, policy loss: 1.6332322081298765
Experience 3, Iter 5, disc loss: 0.5359380003357814, policy loss: 1.3699347133391058
Experience 3, Iter 6, disc loss: 0.6003529682985949, policy loss: 1.1313173359669562
Experience 3, Iter 7, disc loss: 0.4800291468519746, policy loss: 1.4936832668329256
Experience 3, Iter 8, disc loss: 0.4400974102870997, policy loss: 1.4189312040469566
Experience 3, Iter 9, disc loss: 0.3805520095045966, policy loss: 1.409499612752169
Experience 3, Iter 10, disc loss: 0.3432747418682817, policy loss: 1.5217580191843534
Experience 3, Iter 11, disc loss: 0.31614349326270474, policy loss: 1.6052021398161973
Experience 3, Iter 12, disc loss: 0.30062698939739985, policy loss: 1.6379896794316848
Experience 3, Iter 13, disc loss: 0.2739399327468997, policy loss: 1.7383454438653716
Experience 3, Iter 14, disc loss: 0.2713722877788579, policy loss: 1.712796858852752
Experience 3, Iter 15, disc loss: 0.2825750814226838, policy loss: 1.661964140523204
Experience 3, Iter 16, disc loss: 0.2669415429292281, policy loss: 1.7166880357104026
Experience 3, Iter 17, disc loss: 0.2739795294452072, policy loss: 1.7063793743598539
Experience 3, Iter 18, disc loss: 0.3064804895346822, policy loss: 1.637907161701486
Experience 3, Iter 19, disc loss: 0.29193042973813355, policy loss: 1.775777194566507
Experience 3, Iter 20, disc loss: 0.33922338678922015, policy loss: 1.6643986850245542
Experience 3, Iter 21, disc loss: 0.3649114412522313, policy loss: 1.5801269105197095
Experience 3, Iter 22, disc loss: 0.2525634601598041, policy loss: 2.152210833580252
Experience 3, Iter 23, disc loss: 0.29390211709607145, policy loss: 2.0520861711010037
Experience 3, Iter 24, disc loss: 0.2764056938599144, policy loss: 1.970460226489371
Experience 3, Iter 25, disc loss: 0.29779238221020743, policy loss: 1.9549728257844017
Experience 3, Iter 26, disc loss: 0.2543075153638386, policy loss: 2.1177628606579675
Experience 3, Iter 27, disc loss: 0.24414165018137335, policy loss: 2.2417501467365106
Experience 3, Iter 28, disc loss: 0.23349211801563524, policy loss: 2.265887935820894
Experience 3, Iter 29, disc loss: 0.21189272249652896, policy loss: 2.265943433120438
Experience 3, Iter 30, disc loss: 0.2115267155130671, policy loss: 2.371885728268219
Experience 3, Iter 31, disc loss: 0.215511416684708, policy loss: 2.250471272750631
Experience 3, Iter 32, disc loss: 0.19146229941467474, policy loss: 2.5091010646676697
Experience 3, Iter 33, disc loss: 0.24039987208476338, policy loss: 2.119692946672564
Experience 3, Iter 34, disc loss: 0.2533185063456929, policy loss: 2.1920442186780047
Experience 3, Iter 35, disc loss: 0.2145778673987272, policy loss: 2.4003460440768203
Experience 3, Iter 36, disc loss: 0.2209664640067699, policy loss: 2.426975530865491
Experience 3, Iter 37, disc loss: 0.23521954086821714, policy loss: 2.4789377491589146
Experience 3, Iter 38, disc loss: 0.19710452427357245, policy loss: 2.5532055678457373
Experience 3, Iter 39, disc loss: 0.17771279902942647, policy loss: 2.9103438156942194
Experience 3, Iter 40, disc loss: 0.22468748262893026, policy loss: 2.5264354981983983
Experience 3, Iter 41, disc loss: 0.20277951584436765, policy loss: 2.743694798061976
Experience 3, Iter 42, disc loss: 0.19314095533755313, policy loss: 2.7016547958891977
Experience 3, Iter 43, disc loss: 0.18120262937493928, policy loss: 2.596525433727202
Experience 3, Iter 44, disc loss: 0.21474478448693923, policy loss: 2.8011012064895318
Experience 3, Iter 45, disc loss: 0.17541497534043526, policy loss: 2.919831671418139
Experience 3, Iter 46, disc loss: 0.17632859902743198, policy loss: 2.945683763139144
Experience 3, Iter 47, disc loss: 0.2009546255059348, policy loss: 2.5329273361922993
Experience 3, Iter 48, disc loss: 0.16144080237397568, policy loss: 2.8432646277228395
Experience 3, Iter 49, disc loss: 0.16324065494166085, policy loss: 2.9465217043707272
Experience 3, Iter 50, disc loss: 0.16597622096537837, policy loss: 3.0133636328210205
Experience 3, Iter 51, disc loss: 0.14712735550235548, policy loss: 2.9903521258130032
Experience 3, Iter 52, disc loss: 0.1388543814489147, policy loss: 2.9850219973635603
Experience 3, Iter 53, disc loss: 0.14887711825154537, policy loss: 3.102100458336579
Experience 3, Iter 54, disc loss: 0.13153287965544522, policy loss: 3.209475031486501
Experience 3, Iter 55, disc loss: 0.1700970226445263, policy loss: 3.1052993944458125
Experience 3, Iter 56, disc loss: 0.12540284431108395, policy loss: 3.40342629893326
Experience 3, Iter 57, disc loss: 0.11450460474524549, policy loss: 3.6061945517978726
Experience 3, Iter 58, disc loss: 0.1371638120369603, policy loss: 3.155802946741937
Experience 3, Iter 59, disc loss: 0.1166652708393993, policy loss: 3.2971128221447334
Experience 3, Iter 60, disc loss: 0.1082493426736592, policy loss: 3.442358715797757
Experience 3, Iter 61, disc loss: 0.11275428847981955, policy loss: 3.60767683124286
Experience 3, Iter 62, disc loss: 0.12156953786765254, policy loss: 3.421551224256535
Experience 3, Iter 63, disc loss: 0.11440041689031577, policy loss: 3.3490672884724257
Experience 3, Iter 64, disc loss: 0.1061608275503177, policy loss: 3.5416290826963004
Experience 3, Iter 65, disc loss: 0.09336844328193587, policy loss: 3.6547706607772548
Experience 3, Iter 66, disc loss: 0.10408162488221177, policy loss: 3.5026996348622665
Experience 3, Iter 67, disc loss: 0.11026493168244832, policy loss: 3.350217134665206
Experience 3, Iter 68, disc loss: 0.11381341790330451, policy loss: 3.2968424180941724
Experience 3, Iter 69, disc loss: 0.08859883110221192, policy loss: 3.7095645755260183
Experience 3, Iter 70, disc loss: 0.10497547409595465, policy loss: 3.5277998140749602
Experience 3, Iter 71, disc loss: 0.0842616023459905, policy loss: 3.99847226009471
Experience 3, Iter 72, disc loss: 0.08211158696595962, policy loss: 3.820378267516646
Experience 3, Iter 73, disc loss: 0.0887731331189209, policy loss: 3.7576373217609076
Experience 3, Iter 74, disc loss: 0.08019593524546095, policy loss: 3.9519434324599314
Experience 3, Iter 75, disc loss: 0.08466895901800511, policy loss: 3.831641325764201
Experience 3, Iter 76, disc loss: 0.10247557225431736, policy loss: 3.591473277824407
Experience 3, Iter 77, disc loss: 0.0887987620638084, policy loss: 3.7119338130970045
Experience 3, Iter 78, disc loss: 0.08031480886892886, policy loss: 3.7742476640072415
Experience 3, Iter 79, disc loss: 0.07854108607885266, policy loss: 3.911294958983691
Experience 3, Iter 80, disc loss: 0.0786360688164573, policy loss: 3.9255866011670446
Experience 3, Iter 81, disc loss: 0.08880437992908397, policy loss: 3.908077743709088
Experience 3, Iter 82, disc loss: 0.08857560068699664, policy loss: 3.603282279442355
Experience 3, Iter 83, disc loss: 0.08965600650982694, policy loss: 3.8751844259619106
Experience 3, Iter 84, disc loss: 0.08160446733146645, policy loss: 3.9247358354850763
Experience 3, Iter 85, disc loss: 0.09199340802269093, policy loss: 3.685097301293265
Experience 3, Iter 86, disc loss: 0.07276030819305851, policy loss: 4.08451517095555
Experience 3, Iter 87, disc loss: 0.06777879632181395, policy loss: 4.264578044321686
Experience 3, Iter 88, disc loss: 0.08127765303941771, policy loss: 3.974906435626563
Experience 3, Iter 89, disc loss: 0.06934361872598405, policy loss: 4.122859616202284
Experience 3, Iter 90, disc loss: 0.06636155458667692, policy loss: 4.1169949630090095
Experience 3, Iter 91, disc loss: 0.07211676302249992, policy loss: 3.83729504787922
Experience 3, Iter 92, disc loss: 0.054916149774103126, policy loss: 4.518338916360872
Experience 3, Iter 93, disc loss: 0.061581159687024385, policy loss: 4.29650508413061
Experience 3, Iter 94, disc loss: 0.06494945520503317, policy loss: 4.15271252319766
Experience 3, Iter 95, disc loss: 0.07362218674123866, policy loss: 4.081656131609961
Experience 3, Iter 96, disc loss: 0.06312068470752953, policy loss: 4.361263655520208
Experience 3, Iter 97, disc loss: 0.06999907434924275, policy loss: 4.347882698692036
Experience 3, Iter 98, disc loss: 0.06205160018207122, policy loss: 4.109456545734432
Experience 3, Iter 99, disc loss: 0.07138882037669685, policy loss: 4.096079032299638
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0070],
        [0.0420],
        [0.7383],
        [0.0201]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0572, 0.2982, 0.8850, 0.0233, 0.0088, 1.4979]],

        [[0.0572, 0.2982, 0.8850, 0.0233, 0.0088, 1.4979]],

        [[0.0572, 0.2982, 0.8850, 0.0233, 0.0088, 1.4979]],

        [[0.0572, 0.2982, 0.8850, 0.0233, 0.0088, 1.4979]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0281, 0.1681, 2.9533, 0.0803], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0281, 0.1681, 2.9533, 0.0803])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.312
Iter 2/2000 - Loss: 2.169
Iter 3/2000 - Loss: 2.184
Iter 4/2000 - Loss: 2.092
Iter 5/2000 - Loss: 2.019
Iter 6/2000 - Loss: 1.997
Iter 7/2000 - Loss: 1.946
Iter 8/2000 - Loss: 1.850
Iter 9/2000 - Loss: 1.747
Iter 10/2000 - Loss: 1.656
Iter 11/2000 - Loss: 1.565
Iter 12/2000 - Loss: 1.457
Iter 13/2000 - Loss: 1.330
Iter 14/2000 - Loss: 1.190
Iter 15/2000 - Loss: 1.043
Iter 16/2000 - Loss: 0.893
Iter 17/2000 - Loss: 0.734
Iter 18/2000 - Loss: 0.564
Iter 19/2000 - Loss: 0.384
Iter 20/2000 - Loss: 0.197
Iter 1981/2000 - Loss: -5.733
Iter 1982/2000 - Loss: -5.733
Iter 1983/2000 - Loss: -5.733
Iter 1984/2000 - Loss: -5.733
Iter 1985/2000 - Loss: -5.733
Iter 1986/2000 - Loss: -5.733
Iter 1987/2000 - Loss: -5.733
Iter 1988/2000 - Loss: -5.733
Iter 1989/2000 - Loss: -5.733
Iter 1990/2000 - Loss: -5.733
Iter 1991/2000 - Loss: -5.733
Iter 1992/2000 - Loss: -5.733
Iter 1993/2000 - Loss: -5.733
Iter 1994/2000 - Loss: -5.733
Iter 1995/2000 - Loss: -5.733
Iter 1996/2000 - Loss: -5.733
Iter 1997/2000 - Loss: -5.733
Iter 1998/2000 - Loss: -5.733
Iter 1999/2000 - Loss: -5.734
Iter 2000/2000 - Loss: -5.734
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[15.5135,  6.2846, 41.9549, 14.6277, 15.2932, 44.9438]],

        [[34.7116, 52.9796,  7.9056,  1.5548, 11.8141, 27.4911]],

        [[25.4836, 47.8163,  9.1101,  0.9684,  8.4093, 22.9762]],

        [[32.3145, 52.3653, 12.6933,  4.2000,  1.3225, 45.3551]]])
Signal Variance: tensor([ 0.1011,  2.6671, 14.4691,  0.3090])
Estimated target variance: tensor([0.0281, 0.1681, 2.9533, 0.0803])
N: 40
Signal to noise ratio: tensor([18.6667, 91.5988, 86.4891, 36.2745])
Bound on condition number: tensor([ 13938.8226, 335614.9638, 299215.6291,  52634.5075])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.040153093713890746, policy loss: 4.701937361054456
Experience 4, Iter 1, disc loss: 0.04263371863342351, policy loss: 4.52874585749311
Experience 4, Iter 2, disc loss: 0.05286379961589123, policy loss: 3.918084073432613
Experience 4, Iter 3, disc loss: 0.055011428051686656, policy loss: 4.126719747087439
Experience 4, Iter 4, disc loss: 0.055154883452790224, policy loss: 4.216782218189046
Experience 4, Iter 5, disc loss: 0.06828716999280901, policy loss: 3.8838349446498173
Experience 4, Iter 6, disc loss: 0.06310387838072043, policy loss: 3.934908803755274
Experience 4, Iter 7, disc loss: 0.06409123540472192, policy loss: 4.3153039118702825
Experience 4, Iter 8, disc loss: 0.061989010735490804, policy loss: 4.262266605914622
Experience 4, Iter 9, disc loss: 0.056384992031592265, policy loss: 4.212931497962797
Experience 4, Iter 10, disc loss: 0.05497045692896654, policy loss: 4.308343932453843
Experience 4, Iter 11, disc loss: 0.055611899078862714, policy loss: 4.153696123054953
Experience 4, Iter 12, disc loss: 0.057475822695460754, policy loss: 4.146193576079219
Experience 4, Iter 13, disc loss: 0.059698605444207124, policy loss: 4.024473021190631
Experience 4, Iter 14, disc loss: 0.0629301003692633, policy loss: 4.070326470851821
Experience 4, Iter 15, disc loss: 0.06437667669438055, policy loss: 4.072531093325947
Experience 4, Iter 16, disc loss: 0.05829871827257575, policy loss: 4.260880201000873
Experience 4, Iter 17, disc loss: 0.06360950468156142, policy loss: 4.191627338941153
Experience 4, Iter 18, disc loss: 0.06206896174716829, policy loss: 4.044284751946792
Experience 4, Iter 19, disc loss: 0.06076136012227701, policy loss: 4.029740591288602
Experience 4, Iter 20, disc loss: 0.060109258619112295, policy loss: 4.6296494288526
Experience 4, Iter 21, disc loss: 0.05825415316864825, policy loss: 4.1098728782898855
Experience 4, Iter 22, disc loss: 0.059822925358646806, policy loss: 4.072434096325199
Experience 4, Iter 23, disc loss: 0.06026612109938239, policy loss: 4.131921727105723
Experience 4, Iter 24, disc loss: 0.0526937213906904, policy loss: 4.315284712964429
Experience 4, Iter 25, disc loss: 0.05848321692690828, policy loss: 4.109100343505249
Experience 4, Iter 26, disc loss: 0.0644931826551305, policy loss: 3.9674468624927783
Experience 4, Iter 27, disc loss: 0.053209292988836907, policy loss: 4.516608546575412
Experience 4, Iter 28, disc loss: 0.06000170195632522, policy loss: 4.108255453033424
Experience 4, Iter 29, disc loss: 0.06525675312193387, policy loss: 3.9806499613278628
Experience 4, Iter 30, disc loss: 0.05999585013768649, policy loss: 4.310677368932729
Experience 4, Iter 31, disc loss: 0.059746002589968, policy loss: 4.211973323664917
Experience 4, Iter 32, disc loss: 0.05721508890284881, policy loss: 4.336652951567088
Experience 4, Iter 33, disc loss: 0.06230185451178775, policy loss: 3.950249715859171
Experience 4, Iter 34, disc loss: 0.057578011878828474, policy loss: 4.244952072582363
Experience 4, Iter 35, disc loss: 0.061120918146066086, policy loss: 4.320091658049163
Experience 4, Iter 36, disc loss: 0.0572893826496238, policy loss: 4.292599527466949
Experience 4, Iter 37, disc loss: 0.0522295730812381, policy loss: 4.497376035120378
Experience 4, Iter 38, disc loss: 0.0620455173564727, policy loss: 4.124607076132005
Experience 4, Iter 39, disc loss: 0.06734417871736981, policy loss: 3.942376338104597
Experience 4, Iter 40, disc loss: 0.059473156978781985, policy loss: 4.491115949833933
Experience 4, Iter 41, disc loss: 0.05927517886592948, policy loss: 4.43702879319697
Experience 4, Iter 42, disc loss: 0.06957521886408424, policy loss: 3.965464822065469
Experience 4, Iter 43, disc loss: 0.05864430353439687, policy loss: 4.263022470749226
Experience 4, Iter 44, disc loss: 0.06091837093632825, policy loss: 4.230786863995574
Experience 4, Iter 45, disc loss: 0.057373947063576826, policy loss: 4.390794358165425
Experience 4, Iter 46, disc loss: 0.04366916219955387, policy loss: 4.807180655888153
Experience 4, Iter 47, disc loss: 0.05862456658583773, policy loss: 4.272149731973356
Experience 4, Iter 48, disc loss: 0.059452111412631264, policy loss: 4.177313319232372
Experience 4, Iter 49, disc loss: 0.0648090306013093, policy loss: 4.049763297992811
Experience 4, Iter 50, disc loss: 0.056496584381373616, policy loss: 4.24880479248276
Experience 4, Iter 51, disc loss: 0.0632915853506236, policy loss: 4.509986100373405
Experience 4, Iter 52, disc loss: 0.04704623451986475, policy loss: 4.5395981946416
Experience 4, Iter 53, disc loss: 0.07596757721764139, policy loss: 3.7945837070252644
Experience 4, Iter 54, disc loss: 0.06625948372407427, policy loss: 4.2339181144515265
Experience 4, Iter 55, disc loss: 0.04857438437221727, policy loss: 4.41656293034502
Experience 4, Iter 56, disc loss: 0.0605581275538917, policy loss: 4.410955937399762
Experience 4, Iter 57, disc loss: 0.05425899793992672, policy loss: 4.783622198642899
Experience 4, Iter 58, disc loss: 0.05925750005002084, policy loss: 4.508557290639853
Experience 4, Iter 59, disc loss: 0.061008413993689094, policy loss: 4.45034718124243
Experience 4, Iter 60, disc loss: 0.05489242104071318, policy loss: 4.626710061566065
Experience 4, Iter 61, disc loss: 0.06427781414107853, policy loss: 4.2638087214187275
Experience 4, Iter 62, disc loss: 0.05698743383765367, policy loss: 4.180742609461468
Experience 4, Iter 63, disc loss: 0.05532079610017458, policy loss: 4.433369350927579
Experience 4, Iter 64, disc loss: 0.04994162927652039, policy loss: 4.76045085254995
Experience 4, Iter 65, disc loss: 0.050082310398575, policy loss: 4.481174665437179
Experience 4, Iter 66, disc loss: 0.05145204603604764, policy loss: 4.6971072568780805
Experience 4, Iter 67, disc loss: 0.04859003031908771, policy loss: 4.712626441925953
Experience 4, Iter 68, disc loss: 0.05118289169090838, policy loss: 4.356361011065248
Experience 4, Iter 69, disc loss: 0.0460209457188552, policy loss: 4.523853743011141
Experience 4, Iter 70, disc loss: 0.04906689051041105, policy loss: 4.42497828391326
Experience 4, Iter 71, disc loss: 0.04691690399157198, policy loss: 4.768738148224307
Experience 4, Iter 72, disc loss: 0.04330063183262396, policy loss: 5.1390350152734205
Experience 4, Iter 73, disc loss: 0.04225059271125297, policy loss: 5.096057631156203
Experience 4, Iter 74, disc loss: 0.04293803853361263, policy loss: 5.047948447460588
Experience 4, Iter 75, disc loss: 0.03872554646986811, policy loss: 5.145338582142749
Experience 4, Iter 76, disc loss: 0.03389685324880755, policy loss: 5.220390861625661
Experience 4, Iter 77, disc loss: 0.03576629691069521, policy loss: 5.162039564778563
Experience 4, Iter 78, disc loss: 0.036983945944846594, policy loss: 5.1519684575603835
Experience 4, Iter 79, disc loss: 0.03228716691747669, policy loss: 5.161266743545328
Experience 4, Iter 80, disc loss: 0.03476612289902262, policy loss: 5.020590055234002
Experience 4, Iter 81, disc loss: 0.03305984050824498, policy loss: 4.760527614198928
Experience 4, Iter 82, disc loss: 0.028659401299190035, policy loss: 5.228309034808218
Experience 4, Iter 83, disc loss: 0.03680395566389463, policy loss: 4.725446364362911
Experience 4, Iter 84, disc loss: 0.04064123403382307, policy loss: 4.450285432646165
Experience 4, Iter 85, disc loss: 0.03656014870802245, policy loss: 4.4764279999664565
Experience 4, Iter 86, disc loss: 0.0405790811150219, policy loss: 4.293872039389405
Experience 4, Iter 87, disc loss: 0.04508038772318269, policy loss: 4.0221103471445625
Experience 4, Iter 88, disc loss: 0.04170932836905682, policy loss: 4.493308463528343
Experience 4, Iter 89, disc loss: 0.04321875656272582, policy loss: 4.311539806195144
Experience 4, Iter 90, disc loss: 0.04020027375052768, policy loss: 4.476916210456993
Experience 4, Iter 91, disc loss: 0.04570221807700945, policy loss: 4.477512669374587
Experience 4, Iter 92, disc loss: 0.04169264018889769, policy loss: 4.39296286015319
Experience 4, Iter 93, disc loss: 0.043468559174546895, policy loss: 4.604790549933471
Experience 4, Iter 94, disc loss: 0.040039428672836205, policy loss: 4.58885471282904
Experience 4, Iter 95, disc loss: 0.03875308914982846, policy loss: 4.770690371873175
Experience 4, Iter 96, disc loss: 0.031086108498622228, policy loss: 4.810069931146277
Experience 4, Iter 97, disc loss: 0.03722972152368635, policy loss: 4.575659113676287
Experience 4, Iter 98, disc loss: 0.03424553769788348, policy loss: 4.505016199728649
Experience 4, Iter 99, disc loss: 0.03545140177477562, policy loss: 4.70398769559892
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0058],
        [0.0666],
        [0.9380],
        [0.0192]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0468, 0.2469, 0.8763, 0.0219, 0.0072, 1.8617]],

        [[0.0468, 0.2469, 0.8763, 0.0219, 0.0072, 1.8617]],

        [[0.0468, 0.2469, 0.8763, 0.0219, 0.0072, 1.8617]],

        [[0.0468, 0.2469, 0.8763, 0.0219, 0.0072, 1.8617]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0231, 0.2665, 3.7522, 0.0767], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0231, 0.2665, 3.7522, 0.0767])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.572
Iter 2/2000 - Loss: 2.442
Iter 3/2000 - Loss: 2.461
Iter 4/2000 - Loss: 2.369
Iter 5/2000 - Loss: 2.314
Iter 6/2000 - Loss: 2.306
Iter 7/2000 - Loss: 2.259
Iter 8/2000 - Loss: 2.172
Iter 9/2000 - Loss: 2.087
Iter 10/2000 - Loss: 2.014
Iter 11/2000 - Loss: 1.936
Iter 12/2000 - Loss: 1.838
Iter 13/2000 - Loss: 1.723
Iter 14/2000 - Loss: 1.600
Iter 15/2000 - Loss: 1.473
Iter 16/2000 - Loss: 1.341
Iter 17/2000 - Loss: 1.196
Iter 18/2000 - Loss: 1.037
Iter 19/2000 - Loss: 0.868
Iter 20/2000 - Loss: 0.694
Iter 1981/2000 - Loss: -5.965
Iter 1982/2000 - Loss: -5.965
Iter 1983/2000 - Loss: -5.965
Iter 1984/2000 - Loss: -5.965
Iter 1985/2000 - Loss: -5.965
Iter 1986/2000 - Loss: -5.965
Iter 1987/2000 - Loss: -5.965
Iter 1988/2000 - Loss: -5.965
Iter 1989/2000 - Loss: -5.965
Iter 1990/2000 - Loss: -5.965
Iter 1991/2000 - Loss: -5.965
Iter 1992/2000 - Loss: -5.966
Iter 1993/2000 - Loss: -5.966
Iter 1994/2000 - Loss: -5.966
Iter 1995/2000 - Loss: -5.966
Iter 1996/2000 - Loss: -5.966
Iter 1997/2000 - Loss: -5.966
Iter 1998/2000 - Loss: -5.966
Iter 1999/2000 - Loss: -5.966
Iter 2000/2000 - Loss: -5.966
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[23.4555,  6.4539, 47.1637, 13.0912, 12.2727, 44.6541]],

        [[33.0848, 46.5739, 10.2133,  3.3775,  1.3784, 35.0887]],

        [[30.3695, 53.7110,  9.3423,  0.9160,  5.2549, 23.9010]],

        [[29.9879, 46.7894, 13.5069,  3.7717,  1.1628, 37.9185]]])
Signal Variance: tensor([ 0.0995,  3.5806, 12.2485,  0.3191])
Estimated target variance: tensor([0.0231, 0.2665, 3.7522, 0.0767])
N: 50
Signal to noise ratio: tensor([19.1856, 91.8531, 77.1745, 35.9214])
Bound on condition number: tensor([ 18405.4166, 421851.0098, 297796.3675,  64518.3189])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.04797455910301646, policy loss: 4.098505896816464
Experience 5, Iter 1, disc loss: 0.045601182385022795, policy loss: 4.097669117247698
Experience 5, Iter 2, disc loss: 0.04216363578587828, policy loss: 4.116454333789992
Experience 5, Iter 3, disc loss: 0.0421437142893189, policy loss: 4.061859082000497
Experience 5, Iter 4, disc loss: 0.04249619470775684, policy loss: 4.032359303241783
Experience 5, Iter 5, disc loss: 0.03633055274275615, policy loss: 4.432480051899178
Experience 5, Iter 6, disc loss: 0.027942174128531446, policy loss: 4.653658428545794
Experience 5, Iter 7, disc loss: 0.03726196763581931, policy loss: 4.392863890822771
Experience 5, Iter 8, disc loss: 0.0358885108282065, policy loss: 4.331108540545099
Experience 5, Iter 9, disc loss: 0.036551544538826225, policy loss: 4.41977001335118
Experience 5, Iter 10, disc loss: 0.037096210015021974, policy loss: 4.273634010970714
Experience 5, Iter 11, disc loss: 0.0378940883812621, policy loss: 4.186179545130873
Experience 5, Iter 12, disc loss: 0.036807094999021434, policy loss: 4.30969046567027
Experience 5, Iter 13, disc loss: 0.03960344784831565, policy loss: 4.133114544967896
Experience 5, Iter 14, disc loss: 0.03969914441381934, policy loss: 4.190719811959076
Experience 5, Iter 15, disc loss: 0.032318496041390116, policy loss: 4.445579002285704
Experience 5, Iter 16, disc loss: 0.041401067753669356, policy loss: 4.1479957910743686
Experience 5, Iter 17, disc loss: 0.03685560070060178, policy loss: 4.163007541640276
Experience 5, Iter 18, disc loss: 0.033805408612896236, policy loss: 4.475821042924631
Experience 5, Iter 19, disc loss: 0.04082310200500831, policy loss: 4.103561056497268
Experience 5, Iter 20, disc loss: 0.04984137912368039, policy loss: 3.9036275284683013
Experience 5, Iter 21, disc loss: 0.04371089596849623, policy loss: 3.9258002582538096
Experience 5, Iter 22, disc loss: 0.040659184864535125, policy loss: 4.085253049828778
Experience 5, Iter 23, disc loss: 0.036600142594382074, policy loss: 4.380828619838756
Experience 5, Iter 24, disc loss: 0.03791191804619068, policy loss: 4.177616447959231
Experience 5, Iter 25, disc loss: 0.04068533153096837, policy loss: 4.280082254893203
Experience 5, Iter 26, disc loss: 0.035434584492157126, policy loss: 4.185038719888352
Experience 5, Iter 27, disc loss: 0.04521638138541365, policy loss: 4.009249502756553
Experience 5, Iter 28, disc loss: 0.04288970829204487, policy loss: 3.9565451268857283
Experience 5, Iter 29, disc loss: 0.04197443536439238, policy loss: 3.9763975746847446
Experience 5, Iter 30, disc loss: 0.04133303841084385, policy loss: 4.234741305986555
Experience 5, Iter 31, disc loss: 0.03919485995736208, policy loss: 4.247540205271996
Experience 5, Iter 32, disc loss: 0.041918532883932914, policy loss: 4.162423783800861
Experience 5, Iter 33, disc loss: 0.04151646540842092, policy loss: 4.120474414062717
Experience 5, Iter 34, disc loss: 0.037415575288691, policy loss: 4.1683023619308255
Experience 5, Iter 35, disc loss: 0.040408542001079004, policy loss: 4.139571064995117
Experience 5, Iter 36, disc loss: 0.04022077687483617, policy loss: 4.156406306533076
Experience 5, Iter 37, disc loss: 0.039699895528240686, policy loss: 4.075202756941195
Experience 5, Iter 38, disc loss: 0.03649971918035333, policy loss: 4.381578942330074
Experience 5, Iter 39, disc loss: 0.03181026815106594, policy loss: 4.538586671109164
Experience 5, Iter 40, disc loss: 0.030197460607980196, policy loss: 4.829372606165087
Experience 5, Iter 41, disc loss: 0.03180013965514453, policy loss: 4.828453030271549
Experience 5, Iter 42, disc loss: 0.03546885736401834, policy loss: 4.429115847131451
Experience 5, Iter 43, disc loss: 0.035549129928084205, policy loss: 4.257161072029756
Experience 5, Iter 44, disc loss: 0.03317763768911914, policy loss: 4.316967031766151
Experience 5, Iter 45, disc loss: 0.03415552731494735, policy loss: 4.279082333713925
Experience 5, Iter 46, disc loss: 0.028416359199424993, policy loss: 4.763699405742527
Experience 5, Iter 47, disc loss: 0.027437693174933345, policy loss: 4.663386311374043
Experience 5, Iter 48, disc loss: 0.02758508702098972, policy loss: 4.669991044937435
Experience 5, Iter 49, disc loss: 0.029943639971472858, policy loss: 4.544415336485981
Experience 5, Iter 50, disc loss: 0.030854782703027005, policy loss: 4.632823498785244
Experience 5, Iter 51, disc loss: 0.03097848527942867, policy loss: 4.408254460540379
Experience 5, Iter 52, disc loss: 0.034358023179615546, policy loss: 4.376292163697692
Experience 5, Iter 53, disc loss: 0.03278493970709834, policy loss: 4.216146185516096
Experience 5, Iter 54, disc loss: 0.028603904090106713, policy loss: 4.5655985138282045
Experience 5, Iter 55, disc loss: 0.026246429081325212, policy loss: 5.28603533111112
Experience 5, Iter 56, disc loss: 0.019447607453804555, policy loss: 6.879364410757853
Experience 5, Iter 57, disc loss: 0.023092327795539658, policy loss: 5.259014871364575
Experience 5, Iter 58, disc loss: 0.02995821614791886, policy loss: 4.366039303009995
Experience 5, Iter 59, disc loss: 0.025401090331252506, policy loss: 4.650531623546895
Experience 5, Iter 60, disc loss: 0.021908782980723678, policy loss: 5.015562729434139
Experience 5, Iter 61, disc loss: 0.016858517702089112, policy loss: 5.560609119628262
Experience 5, Iter 62, disc loss: 0.016041500949425247, policy loss: 5.543097428070001
Experience 5, Iter 63, disc loss: 0.013972761948739518, policy loss: 5.786987315046836
Experience 5, Iter 64, disc loss: 0.01604797899080469, policy loss: 5.529724516433121
Experience 5, Iter 65, disc loss: 0.016671309210041056, policy loss: 5.3988982851301195
Experience 5, Iter 66, disc loss: 0.019194909142896288, policy loss: 5.0461932132304375
Experience 5, Iter 67, disc loss: 0.024222893198175856, policy loss: 4.71486987167892
Experience 5, Iter 68, disc loss: 0.025751559809434048, policy loss: 4.644556173625456
Experience 5, Iter 69, disc loss: 0.027259717106293414, policy loss: 4.279540593023938
Experience 5, Iter 70, disc loss: 0.02999490104218625, policy loss: 4.302156653039708
Experience 5, Iter 71, disc loss: 0.022420354156432575, policy loss: 5.0527746995615175
Experience 5, Iter 72, disc loss: 0.02018783403821433, policy loss: 5.6130817088788705
Experience 5, Iter 73, disc loss: 0.02591466689256562, policy loss: 4.723705907106307
Experience 5, Iter 74, disc loss: 0.027170475127308857, policy loss: 4.4985323023645165
Experience 5, Iter 75, disc loss: 0.022394739667183337, policy loss: 4.7138080450489515
Experience 5, Iter 76, disc loss: 0.025267992786649523, policy loss: 4.512972143836918
Experience 5, Iter 77, disc loss: 0.028189741780018755, policy loss: 4.506635858518635
Experience 5, Iter 78, disc loss: 0.02586709726082205, policy loss: 4.612013098271333
Experience 5, Iter 79, disc loss: 0.02469921224368627, policy loss: 4.673910021765165
Experience 5, Iter 80, disc loss: 0.020154720383915678, policy loss: 4.843318563272118
Experience 5, Iter 81, disc loss: 0.02599790885692929, policy loss: 4.636898537835226
Experience 5, Iter 82, disc loss: 0.022123890499532402, policy loss: 4.695310595850529
Experience 5, Iter 83, disc loss: 0.024005758158987728, policy loss: 4.700963400590817
Experience 5, Iter 84, disc loss: 0.02586256354526431, policy loss: 5.002850055412301
Experience 5, Iter 85, disc loss: 0.01441734930093826, policy loss: 8.465859081616806
Experience 5, Iter 86, disc loss: 0.008657198343776396, policy loss: 14.296305586587431
Experience 5, Iter 87, disc loss: 0.008290560774578687, policy loss: 14.51590672416014
Experience 5, Iter 88, disc loss: 0.008405751632007902, policy loss: 15.189851548358563
Experience 5, Iter 89, disc loss: 0.008027030968542927, policy loss: 14.020522437273824
Experience 5, Iter 90, disc loss: 0.009463783852424842, policy loss: 9.488137405413868
Experience 5, Iter 91, disc loss: 0.01603458429374809, policy loss: 6.058180183591785
Experience 5, Iter 92, disc loss: 0.023474848896352266, policy loss: 4.700471560854976
Experience 5, Iter 93, disc loss: 0.01895932132656237, policy loss: 5.114354311901399
Experience 5, Iter 94, disc loss: 0.015589305403962011, policy loss: 5.515032345157289
Experience 5, Iter 95, disc loss: 0.010416005765818892, policy loss: 6.395981951459517
Experience 5, Iter 96, disc loss: 0.00930364612873404, policy loss: 6.68403192142615
Experience 5, Iter 97, disc loss: 0.008417930929821273, policy loss: 7.206586567368383
Experience 5, Iter 98, disc loss: 0.008111062332625762, policy loss: 6.919380471953923
Experience 5, Iter 99, disc loss: 0.0076043750689761555, policy loss: 7.252089057521578
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.0810],
        [1.0637],
        [0.0181]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0398, 0.2103, 0.8496, 0.0205, 0.0062, 2.0241]],

        [[0.0398, 0.2103, 0.8496, 0.0205, 0.0062, 2.0241]],

        [[0.0398, 0.2103, 0.8496, 0.0205, 0.0062, 2.0241]],

        [[0.0398, 0.2103, 0.8496, 0.0205, 0.0062, 2.0241]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0193, 0.3241, 4.2547, 0.0723], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0193, 0.3241, 4.2547, 0.0723])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.649
Iter 2/2000 - Loss: 2.523
Iter 3/2000 - Loss: 2.554
Iter 4/2000 - Loss: 2.457
Iter 5/2000 - Loss: 2.416
Iter 6/2000 - Loss: 2.425
Iter 7/2000 - Loss: 2.385
Iter 8/2000 - Loss: 2.306
Iter 9/2000 - Loss: 2.238
Iter 10/2000 - Loss: 2.186
Iter 11/2000 - Loss: 2.125
Iter 12/2000 - Loss: 2.039
Iter 13/2000 - Loss: 1.938
Iter 14/2000 - Loss: 1.833
Iter 15/2000 - Loss: 1.726
Iter 16/2000 - Loss: 1.608
Iter 17/2000 - Loss: 1.470
Iter 18/2000 - Loss: 1.311
Iter 19/2000 - Loss: 1.138
Iter 20/2000 - Loss: 0.958
Iter 1981/2000 - Loss: -6.158
Iter 1982/2000 - Loss: -6.158
Iter 1983/2000 - Loss: -6.158
Iter 1984/2000 - Loss: -6.158
Iter 1985/2000 - Loss: -6.158
Iter 1986/2000 - Loss: -6.158
Iter 1987/2000 - Loss: -6.158
Iter 1988/2000 - Loss: -6.159
Iter 1989/2000 - Loss: -6.159
Iter 1990/2000 - Loss: -6.159
Iter 1991/2000 - Loss: -6.159
Iter 1992/2000 - Loss: -6.159
Iter 1993/2000 - Loss: -6.159
Iter 1994/2000 - Loss: -6.159
Iter 1995/2000 - Loss: -6.159
Iter 1996/2000 - Loss: -6.159
Iter 1997/2000 - Loss: -6.159
Iter 1998/2000 - Loss: -6.159
Iter 1999/2000 - Loss: -6.159
Iter 2000/2000 - Loss: -6.159
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[20.4934,  6.5923, 49.1047, 14.9939, 10.3766, 52.2270]],

        [[32.1221, 47.8519, 12.0663,  1.0510,  4.2589, 28.5809]],

        [[27.4551, 44.6513, 11.4220,  0.8796,  1.5039, 20.6454]],

        [[28.1697, 42.9858, 13.9719,  4.0968,  1.1532, 35.8313]]])
Signal Variance: tensor([ 0.0919,  2.0285, 12.8741,  0.3054])
Estimated target variance: tensor([0.0193, 0.3241, 4.2547, 0.0723])
N: 60
Signal to noise ratio: tensor([16.9894, 64.6702, 77.4726, 34.1099])
Bound on condition number: tensor([ 17319.4575, 250935.0309, 360121.6045,  69810.1864])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.007069749824235152, policy loss: 7.641163287251632
Experience 6, Iter 1, disc loss: 0.007148575985965395, policy loss: 7.447150875361298
Experience 6, Iter 2, disc loss: 0.007272085996585385, policy loss: 7.216632128795577
Experience 6, Iter 3, disc loss: 0.0070618419323366715, policy loss: 7.213845874960794
Experience 6, Iter 4, disc loss: 0.007104311043114247, policy loss: 7.091731297214929
Experience 6, Iter 5, disc loss: 0.007220031301011549, policy loss: 6.937452556950383
Experience 6, Iter 6, disc loss: 0.007794489646279415, policy loss: 6.572514095661952
Experience 6, Iter 7, disc loss: 0.008174848248378667, policy loss: 6.577150600530407
Experience 6, Iter 8, disc loss: 0.00739202275416665, policy loss: 6.737328792027447
Experience 6, Iter 9, disc loss: 0.008408675069123068, policy loss: 6.37465793989819
Experience 6, Iter 10, disc loss: 0.009458564506998758, policy loss: 6.408580024779843
Experience 6, Iter 11, disc loss: 0.008236469454362511, policy loss: 6.292011307904868
Experience 6, Iter 12, disc loss: 0.00887017263340759, policy loss: 6.113994558179816
Experience 6, Iter 13, disc loss: 0.009796678485139322, policy loss: 6.022369260000919
Experience 6, Iter 14, disc loss: 0.009283368975168755, policy loss: 6.037019222462852
Experience 6, Iter 15, disc loss: 0.011525742911899646, policy loss: 5.553463062654102
Experience 6, Iter 16, disc loss: 0.01202744312262835, policy loss: 5.471411593585012
Experience 6, Iter 17, disc loss: 0.014875037325576532, policy loss: 5.130949119002218
Experience 6, Iter 18, disc loss: 0.014380088132281036, policy loss: 5.311212885168258
Experience 6, Iter 19, disc loss: 0.01687836597227917, policy loss: 5.09395736803922
Experience 6, Iter 20, disc loss: 0.01643764637533613, policy loss: 5.0998537484510145
Experience 6, Iter 21, disc loss: 0.02390188416383025, policy loss: 4.907123223454084
Experience 6, Iter 22, disc loss: 0.021609932719869662, policy loss: 4.801273488175052
Experience 6, Iter 23, disc loss: 0.02400961450358044, policy loss: 4.942128547816374
Experience 6, Iter 24, disc loss: 0.025509075016817373, policy loss: 4.5986832349515865
Experience 6, Iter 25, disc loss: 0.021598305883321678, policy loss: 5.1126351355090645
Experience 6, Iter 26, disc loss: 0.021780658178895462, policy loss: 5.301374182638154
Experience 6, Iter 27, disc loss: 0.019117979644058154, policy loss: 5.481387163728439
Experience 6, Iter 28, disc loss: 0.018447514285765777, policy loss: 5.215152339288193
Experience 6, Iter 29, disc loss: 0.02247044263156667, policy loss: 4.8626341598542595
Experience 6, Iter 30, disc loss: 0.024664416675849895, policy loss: 4.914406818666348
Experience 6, Iter 31, disc loss: 0.02054725212031565, policy loss: 4.966413708500896
Experience 6, Iter 32, disc loss: 0.024491226466593265, policy loss: 4.77577570989406
Experience 6, Iter 33, disc loss: 0.01701656894565721, policy loss: 5.063335192830664
Experience 6, Iter 34, disc loss: 0.018208838083045183, policy loss: 4.950077345254062
Experience 6, Iter 35, disc loss: 0.017909827871855903, policy loss: 5.259426506060082
Experience 6, Iter 36, disc loss: 0.01593672125453768, policy loss: 5.275467066358765
Experience 6, Iter 37, disc loss: 0.0162550958262529, policy loss: 5.228332075251535
Experience 6, Iter 38, disc loss: 0.018215066459217978, policy loss: 4.984430547741978
Experience 6, Iter 39, disc loss: 0.015751900548668353, policy loss: 5.178099589833195
Experience 6, Iter 40, disc loss: 0.01852447394226867, policy loss: 5.015520344374415
Experience 6, Iter 41, disc loss: 0.019609101140089853, policy loss: 4.887643914492713
Experience 6, Iter 42, disc loss: 0.020828738628970653, policy loss: 4.757717097166376
Experience 6, Iter 43, disc loss: 0.018246343344190223, policy loss: 5.034024064307501
Experience 6, Iter 44, disc loss: 0.018264790375949337, policy loss: 5.114881169990229
Experience 6, Iter 45, disc loss: 0.016563671477084606, policy loss: 5.10954676957772
Experience 6, Iter 46, disc loss: 0.018187505574668385, policy loss: 4.98156875260889
Experience 6, Iter 47, disc loss: 0.018443594163480768, policy loss: 5.137617215832767
Experience 6, Iter 48, disc loss: 0.01690967593391811, policy loss: 5.046388985379306
Experience 6, Iter 49, disc loss: 0.01767289158360385, policy loss: 4.844316279931114
Experience 6, Iter 50, disc loss: 0.016210735014694774, policy loss: 5.057676268033453
Experience 6, Iter 51, disc loss: 0.01623036272608759, policy loss: 5.137748029427168
Experience 6, Iter 52, disc loss: 0.018677521854720946, policy loss: 4.8844609884536885
Experience 6, Iter 53, disc loss: 0.01788653414939114, policy loss: 5.427134712155165
Experience 6, Iter 54, disc loss: 0.01564755812584648, policy loss: 5.157701842507268
Experience 6, Iter 55, disc loss: 0.01676578462110557, policy loss: 5.368978472989868
Experience 6, Iter 56, disc loss: 0.01621354805536917, policy loss: 4.970621989763352
Experience 6, Iter 57, disc loss: 0.01533278979062304, policy loss: 5.186764286417311
Experience 6, Iter 58, disc loss: 0.01492240401521635, policy loss: 5.349132536555679
Experience 6, Iter 59, disc loss: 0.018299748510504095, policy loss: 5.038766897160553
Experience 6, Iter 60, disc loss: 0.01713679855663265, policy loss: 5.21621565950729
Experience 6, Iter 61, disc loss: 0.01378315715892837, policy loss: 5.375307980121952
Experience 6, Iter 62, disc loss: 0.01402761429171827, policy loss: 5.181204268773471
Experience 6, Iter 63, disc loss: 0.014039015130526019, policy loss: 5.508267129144697
Experience 6, Iter 64, disc loss: 0.015997040361645863, policy loss: 5.10375051067098
Experience 6, Iter 65, disc loss: 0.016923679567737826, policy loss: 5.1351508656978035
Experience 6, Iter 66, disc loss: 0.015668301705492868, policy loss: 5.181080990160387
Experience 6, Iter 67, disc loss: 0.013631558527244576, policy loss: 5.321216806551743
Experience 6, Iter 68, disc loss: 0.01522275758009433, policy loss: 5.330382666565751
Experience 6, Iter 69, disc loss: 0.01600496069495895, policy loss: 5.248390874221234
Experience 6, Iter 70, disc loss: 0.016621841095653504, policy loss: 5.10455342723534
Experience 6, Iter 71, disc loss: 0.012430140266634563, policy loss: 5.429968250807578
Experience 6, Iter 72, disc loss: 0.015163199425407274, policy loss: 5.278573034036908
Experience 6, Iter 73, disc loss: 0.013804848941413594, policy loss: 5.4497517574993815
Experience 6, Iter 74, disc loss: 0.014018199435345166, policy loss: 5.26094961745261
Experience 6, Iter 75, disc loss: 0.01466627891804573, policy loss: 5.114102386716219
Experience 6, Iter 76, disc loss: 0.014135045994204204, policy loss: 5.432607117554814
Experience 6, Iter 77, disc loss: 0.014967845338357277, policy loss: 5.309536932481783
Experience 6, Iter 78, disc loss: 0.012880653883111739, policy loss: 5.633882920724338
Experience 6, Iter 79, disc loss: 0.013921112747266989, policy loss: 5.5284772228167816
Experience 6, Iter 80, disc loss: 0.01487777659576382, policy loss: 5.520380426967791
Experience 6, Iter 81, disc loss: 0.014971857549799444, policy loss: 5.346479416418857
Experience 6, Iter 82, disc loss: 0.011751236238712064, policy loss: 5.593734406415532
Experience 6, Iter 83, disc loss: 0.011013242734754522, policy loss: 5.623634518501314
Experience 6, Iter 84, disc loss: 0.012364568903167871, policy loss: 5.514036508840887
Experience 6, Iter 85, disc loss: 0.013843113348864636, policy loss: 5.464085338391984
Experience 6, Iter 86, disc loss: 0.013417793770220602, policy loss: 5.557645483196893
Experience 6, Iter 87, disc loss: 0.010587983945907067, policy loss: 5.619790236864675
Experience 6, Iter 88, disc loss: 0.01057682561601692, policy loss: 5.770893967348878
Experience 6, Iter 89, disc loss: 0.013182876244405433, policy loss: 5.331530409019702
Experience 6, Iter 90, disc loss: 0.011618130344507926, policy loss: 5.575078441123978
Experience 6, Iter 91, disc loss: 0.012672447427948387, policy loss: 5.486949752185826
Experience 6, Iter 92, disc loss: 0.010140170615471988, policy loss: 5.755587795877851
Experience 6, Iter 93, disc loss: 0.012483149016179876, policy loss: 5.505512823227351
Experience 6, Iter 94, disc loss: 0.01155908166145574, policy loss: 5.622375128889482
Experience 6, Iter 95, disc loss: 0.012339839363884246, policy loss: 5.561379846654398
Experience 6, Iter 96, disc loss: 0.011379352591670335, policy loss: 5.682147602953769
Experience 6, Iter 97, disc loss: 0.012648085971483754, policy loss: 5.583851156853724
Experience 6, Iter 98, disc loss: 0.010038763486875778, policy loss: 5.716841569776097
Experience 6, Iter 99, disc loss: 0.010782144323293576, policy loss: 5.791520518706958
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.0885],
        [1.1179],
        [0.0170]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0344, 0.1848, 0.8146, 0.0195, 0.0054, 2.1366]],

        [[0.0344, 0.1848, 0.8146, 0.0195, 0.0054, 2.1366]],

        [[0.0344, 0.1848, 0.8146, 0.0195, 0.0054, 2.1366]],

        [[0.0344, 0.1848, 0.8146, 0.0195, 0.0054, 2.1366]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0169, 0.3539, 4.4717, 0.0681], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0169, 0.3539, 4.4717, 0.0681])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.632
Iter 2/2000 - Loss: 2.529
Iter 3/2000 - Loss: 2.544
Iter 4/2000 - Loss: 2.446
Iter 5/2000 - Loss: 2.426
Iter 6/2000 - Loss: 2.439
Iter 7/2000 - Loss: 2.387
Iter 8/2000 - Loss: 2.312
Iter 9/2000 - Loss: 2.257
Iter 10/2000 - Loss: 2.211
Iter 11/2000 - Loss: 2.145
Iter 12/2000 - Loss: 2.053
Iter 13/2000 - Loss: 1.951
Iter 14/2000 - Loss: 1.848
Iter 15/2000 - Loss: 1.739
Iter 16/2000 - Loss: 1.610
Iter 17/2000 - Loss: 1.456
Iter 18/2000 - Loss: 1.282
Iter 19/2000 - Loss: 1.097
Iter 20/2000 - Loss: 0.903
Iter 1981/2000 - Loss: -6.423
Iter 1982/2000 - Loss: -6.423
Iter 1983/2000 - Loss: -6.423
Iter 1984/2000 - Loss: -6.423
Iter 1985/2000 - Loss: -6.423
Iter 1986/2000 - Loss: -6.423
Iter 1987/2000 - Loss: -6.423
Iter 1988/2000 - Loss: -6.423
Iter 1989/2000 - Loss: -6.423
Iter 1990/2000 - Loss: -6.423
Iter 1991/2000 - Loss: -6.423
Iter 1992/2000 - Loss: -6.423
Iter 1993/2000 - Loss: -6.423
Iter 1994/2000 - Loss: -6.423
Iter 1995/2000 - Loss: -6.423
Iter 1996/2000 - Loss: -6.423
Iter 1997/2000 - Loss: -6.424
Iter 1998/2000 - Loss: -6.424
Iter 1999/2000 - Loss: -6.424
Iter 2000/2000 - Loss: -6.424
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[20.3238,  6.0186, 50.8869,  9.8138,  4.5966, 51.7890]],

        [[27.8775, 45.2493, 12.6112,  1.0110,  3.2343, 29.7406]],

        [[27.6865, 47.0488, 11.1139,  0.8452,  1.9047, 19.0841]],

        [[25.3052, 39.4594, 13.1841,  3.9147,  1.1851, 35.0787]]])
Signal Variance: tensor([ 0.0863,  2.3220, 12.0381,  0.2717])
Estimated target variance: tensor([0.0169, 0.3539, 4.4717, 0.0681])
N: 70
Signal to noise ratio: tensor([16.4116, 70.5078, 72.6570, 30.8768])
Bound on condition number: tensor([ 18854.7700, 347995.2598, 369533.6181,  66737.4996])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.011819133596132923, policy loss: 5.615343481395669
Experience 7, Iter 1, disc loss: 0.010241145513289775, policy loss: 5.833270976305521
Experience 7, Iter 2, disc loss: 0.013291719112467905, policy loss: 5.668637664862961
Experience 7, Iter 3, disc loss: 0.011329589781518887, policy loss: 5.729489473995436
Experience 7, Iter 4, disc loss: 0.009701488016083983, policy loss: 5.895281437024245
Experience 7, Iter 5, disc loss: 0.009429766086307452, policy loss: 5.787327543155559
Experience 7, Iter 6, disc loss: 0.010914213722586389, policy loss: 5.675133340157599
Experience 7, Iter 7, disc loss: 0.011712205203286422, policy loss: 5.4920874638398995
Experience 7, Iter 8, disc loss: 0.012636291082390493, policy loss: 5.394658318841542
Experience 7, Iter 9, disc loss: 0.011494582894080708, policy loss: 5.770775614234497
Experience 7, Iter 10, disc loss: 0.011817626832284957, policy loss: 5.473502246155364
Experience 7, Iter 11, disc loss: 0.01137266058201661, policy loss: 5.627213943598567
Experience 7, Iter 12, disc loss: 0.010267805488157675, policy loss: 5.774008502838522
Experience 7, Iter 13, disc loss: 0.01127281910781578, policy loss: 5.7504316204463315
Experience 7, Iter 14, disc loss: 0.010752196828845397, policy loss: 5.7642979685216105
Experience 7, Iter 15, disc loss: 0.009595480703288358, policy loss: 5.976709309748125
Experience 7, Iter 16, disc loss: 0.011958291260922682, policy loss: 5.44383265325046
Experience 7, Iter 17, disc loss: 0.010788842803077615, policy loss: 5.7498161118985545
Experience 7, Iter 18, disc loss: 0.00844279373950833, policy loss: 5.871464915658404
Experience 7, Iter 19, disc loss: 0.012450110998212392, policy loss: 5.545057001003045
Experience 7, Iter 20, disc loss: 0.010620353279463673, policy loss: 5.635686171905709
Experience 7, Iter 21, disc loss: 0.012491099100050968, policy loss: 5.505306009582556
Experience 7, Iter 22, disc loss: 0.008320944954889374, policy loss: 6.078058910730125
Experience 7, Iter 23, disc loss: 0.009075793310646922, policy loss: 5.970493760448597
Experience 7, Iter 24, disc loss: 0.011674867076249507, policy loss: 5.782184741578268
Experience 7, Iter 25, disc loss: 0.009817052314998513, policy loss: 5.8842906235928805
Experience 7, Iter 26, disc loss: 0.012037321364233486, policy loss: 5.73979302294242
Experience 7, Iter 27, disc loss: 0.008570241021320538, policy loss: 5.8961054890388365
Experience 7, Iter 28, disc loss: 0.010174815552789651, policy loss: 5.722524818432115
Experience 7, Iter 29, disc loss: 0.008351370887352483, policy loss: 5.870758270643296
Experience 7, Iter 30, disc loss: 0.008542536475407377, policy loss: 6.1447720124105105
Experience 7, Iter 31, disc loss: 0.00916024339064345, policy loss: 5.830653838492042
Experience 7, Iter 32, disc loss: 0.008253201930290766, policy loss: 6.134723552304295
Experience 7, Iter 33, disc loss: 0.009453954295956697, policy loss: 5.931998500428556
Experience 7, Iter 34, disc loss: 0.008734154142090923, policy loss: 5.919603921263029
Experience 7, Iter 35, disc loss: 0.01022813044915364, policy loss: 5.776059321351781
Experience 7, Iter 36, disc loss: 0.010087319319838379, policy loss: 5.798648220509495
Experience 7, Iter 37, disc loss: 0.007853040303912626, policy loss: 5.998267174504174
Experience 7, Iter 38, disc loss: 0.010286204434968013, policy loss: 5.744095943177724
Experience 7, Iter 39, disc loss: 0.007862700675040128, policy loss: 6.05928438736905
Experience 7, Iter 40, disc loss: 0.009338973231059939, policy loss: 5.874685564120986
Experience 7, Iter 41, disc loss: 0.007703982604451399, policy loss: 5.989585039817758
Experience 7, Iter 42, disc loss: 0.01070153217008644, policy loss: 5.76041611219535
Experience 7, Iter 43, disc loss: 0.01173050602181134, policy loss: 5.474460030254893
Experience 7, Iter 44, disc loss: 0.010823175963465121, policy loss: 5.918632273748047
Experience 7, Iter 45, disc loss: 0.00917010476605226, policy loss: 5.875913428535693
Experience 7, Iter 46, disc loss: 0.009260368056661578, policy loss: 6.184869649705914
Experience 7, Iter 47, disc loss: 0.010486032385516594, policy loss: 5.712295084436688
Experience 7, Iter 48, disc loss: 0.009659642183048378, policy loss: 6.05232587595086
Experience 7, Iter 49, disc loss: 0.011519840154498976, policy loss: 5.717726619664722
Experience 7, Iter 50, disc loss: 0.006907269927143099, policy loss: 6.225439947274641
Experience 7, Iter 51, disc loss: 0.008554883133202731, policy loss: 5.803475688784882
Experience 7, Iter 52, disc loss: 0.009956801739971438, policy loss: 5.79250667985786
Experience 7, Iter 53, disc loss: 0.009355973716432802, policy loss: 5.841943167360682
Experience 7, Iter 54, disc loss: 0.006881106946269552, policy loss: 6.1006060920898735
Experience 7, Iter 55, disc loss: 0.008094186862889544, policy loss: 6.071438595703237
Experience 7, Iter 56, disc loss: 0.008866749830118294, policy loss: 6.092947301660446
Experience 7, Iter 57, disc loss: 0.007392534877561334, policy loss: 6.170810744581023
Experience 7, Iter 58, disc loss: 0.008402051943312686, policy loss: 6.013722332935922
Experience 7, Iter 59, disc loss: 0.007210260836075287, policy loss: 6.301624395888371
Experience 7, Iter 60, disc loss: 0.006963921447294127, policy loss: 6.21536000634708
Experience 7, Iter 61, disc loss: 0.006635064550899203, policy loss: 6.309230418532993
Experience 7, Iter 62, disc loss: 0.007018006241183428, policy loss: 6.321866854491564
Experience 7, Iter 63, disc loss: 0.007869359826808236, policy loss: 6.126386480028851
Experience 7, Iter 64, disc loss: 0.007106090417425496, policy loss: 6.367004687328187
Experience 7, Iter 65, disc loss: 0.007337262854123352, policy loss: 6.0606118917762615
Experience 7, Iter 66, disc loss: 0.008195219528138703, policy loss: 6.221144540315053
Experience 7, Iter 67, disc loss: 0.007194999650369671, policy loss: 6.09266801383275
Experience 7, Iter 68, disc loss: 0.00883670378412869, policy loss: 6.124406099350643
Experience 7, Iter 69, disc loss: 0.00771883853147983, policy loss: 6.09775379502044
Experience 7, Iter 70, disc loss: 0.008667497942673639, policy loss: 5.804072444368321
Experience 7, Iter 71, disc loss: 0.010488715068372803, policy loss: 6.07225071110817
Experience 7, Iter 72, disc loss: 0.00823950006359608, policy loss: 6.370711209747013
Experience 7, Iter 73, disc loss: 0.00972864358019572, policy loss: 6.227783665316844
Experience 7, Iter 74, disc loss: 0.0076631409611372416, policy loss: 6.359298475467399
Experience 7, Iter 75, disc loss: 0.006697100674671786, policy loss: 6.446503725226167
Experience 7, Iter 76, disc loss: 0.007513150102467598, policy loss: 6.069646817685406
Experience 7, Iter 77, disc loss: 0.009064282358486083, policy loss: 6.5184035066545984
Experience 7, Iter 78, disc loss: 0.007294350781410945, policy loss: 6.277309375371961
Experience 7, Iter 79, disc loss: 0.011335959405400293, policy loss: 5.762980395456998
Experience 7, Iter 80, disc loss: 0.012172182432860217, policy loss: 5.916691418834803
Experience 7, Iter 81, disc loss: 0.00853698009291021, policy loss: 6.266419838328654
Experience 7, Iter 82, disc loss: 0.006068085329884334, policy loss: 6.619569203485602
Experience 7, Iter 83, disc loss: 0.009426907323460555, policy loss: 6.086489355916848
Experience 7, Iter 84, disc loss: 0.009184703190533952, policy loss: 5.974155716201662
Experience 7, Iter 85, disc loss: 0.009810873419821581, policy loss: 5.888189474911032
Experience 7, Iter 86, disc loss: 0.008593317995828584, policy loss: 6.030200069449681
Experience 7, Iter 87, disc loss: 0.006007561217613992, policy loss: 6.4856539804314695
Experience 7, Iter 88, disc loss: 0.007755959849549986, policy loss: 6.217885706698423
Experience 7, Iter 89, disc loss: 0.008673963870475913, policy loss: 6.264455128574666
Experience 7, Iter 90, disc loss: 0.005948721907287407, policy loss: 6.323452988720545
Experience 7, Iter 91, disc loss: 0.007441994721485998, policy loss: 6.296053393016815
Experience 7, Iter 92, disc loss: 0.007902488484986106, policy loss: 6.173564867469969
Experience 7, Iter 93, disc loss: 0.00787134269762378, policy loss: 6.0115701557924455
Experience 7, Iter 94, disc loss: 0.0090775591930595, policy loss: 6.402824538279187
Experience 7, Iter 95, disc loss: 0.00894461239805485, policy loss: 6.010318931591785
Experience 7, Iter 96, disc loss: 0.005977056835748573, policy loss: 6.57619012968085
Experience 7, Iter 97, disc loss: 0.00782861771153891, policy loss: 6.362255416123444
Experience 7, Iter 98, disc loss: 0.007957689197774415, policy loss: 6.075106523603658
Experience 7, Iter 99, disc loss: 0.006408781820951743, policy loss: 6.535691516095355
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.0865],
        [1.0739],
        [0.0159]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0302, 0.1639, 0.7626, 0.0182, 0.0048, 2.0564]],

        [[0.0302, 0.1639, 0.7626, 0.0182, 0.0048, 2.0564]],

        [[0.0302, 0.1639, 0.7626, 0.0182, 0.0048, 2.0564]],

        [[0.0302, 0.1639, 0.7626, 0.0182, 0.0048, 2.0564]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0150, 0.3461, 4.2956, 0.0634], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0150, 0.3461, 4.2956, 0.0634])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.514
Iter 2/2000 - Loss: 2.435
Iter 3/2000 - Loss: 2.433
Iter 4/2000 - Loss: 2.336
Iter 5/2000 - Loss: 2.339
Iter 6/2000 - Loss: 2.351
Iter 7/2000 - Loss: 2.293
Iter 8/2000 - Loss: 2.227
Iter 9/2000 - Loss: 2.190
Iter 10/2000 - Loss: 2.154
Iter 11/2000 - Loss: 2.087
Iter 12/2000 - Loss: 1.997
Iter 13/2000 - Loss: 1.905
Iter 14/2000 - Loss: 1.813
Iter 15/2000 - Loss: 1.707
Iter 16/2000 - Loss: 1.575
Iter 17/2000 - Loss: 1.419
Iter 18/2000 - Loss: 1.246
Iter 19/2000 - Loss: 1.063
Iter 20/2000 - Loss: 0.867
Iter 1981/2000 - Loss: -6.771
Iter 1982/2000 - Loss: -6.771
Iter 1983/2000 - Loss: -6.771
Iter 1984/2000 - Loss: -6.771
Iter 1985/2000 - Loss: -6.772
Iter 1986/2000 - Loss: -6.772
Iter 1987/2000 - Loss: -6.772
Iter 1988/2000 - Loss: -6.772
Iter 1989/2000 - Loss: -6.772
Iter 1990/2000 - Loss: -6.772
Iter 1991/2000 - Loss: -6.772
Iter 1992/2000 - Loss: -6.772
Iter 1993/2000 - Loss: -6.772
Iter 1994/2000 - Loss: -6.772
Iter 1995/2000 - Loss: -6.772
Iter 1996/2000 - Loss: -6.772
Iter 1997/2000 - Loss: -6.772
Iter 1998/2000 - Loss: -6.772
Iter 1999/2000 - Loss: -6.772
Iter 2000/2000 - Loss: -6.772
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[17.6919,  6.1287, 46.6590, 10.4522,  4.0113, 50.9696]],

        [[24.2565, 41.4964, 11.9915,  1.0984,  2.9152, 29.4754]],

        [[25.9296, 44.3972, 11.1596,  0.8888,  2.0322, 16.3633]],

        [[23.4901, 38.7650, 14.6992,  4.3927,  1.1009, 37.6040]]])
Signal Variance: tensor([ 0.0869,  2.3169, 12.7094,  0.3255])
Estimated target variance: tensor([0.0150, 0.3461, 4.2956, 0.0634])
N: 80
Signal to noise ratio: tensor([16.4948, 72.3671, 80.7911, 35.0753])
Bound on condition number: tensor([ 21767.2822, 418960.6751, 522177.0660,  98422.9434])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.009344393161389044, policy loss: 6.214151012507256
Experience 8, Iter 1, disc loss: 0.007440794330546634, policy loss: 6.319096417764655
Experience 8, Iter 2, disc loss: 0.00895484011321748, policy loss: 6.07106063864835
Experience 8, Iter 3, disc loss: 0.008690349798172054, policy loss: 6.183162567297757
Experience 8, Iter 4, disc loss: 0.006900446718576984, policy loss: 6.390524989563269
Experience 8, Iter 5, disc loss: 0.010394056508606388, policy loss: 5.830837147435777
Experience 8, Iter 6, disc loss: 0.00701532606657927, policy loss: 6.458351767313568
Experience 8, Iter 7, disc loss: 0.008951655648860602, policy loss: 6.055503832827283
Experience 8, Iter 8, disc loss: 0.008799945359497354, policy loss: 5.820437207152443
Experience 8, Iter 9, disc loss: 0.010047013763573795, policy loss: 5.987278270794916
Experience 8, Iter 10, disc loss: 0.00607008169298606, policy loss: 6.374456938887748
Experience 8, Iter 11, disc loss: 0.008689283038389313, policy loss: 6.054130208561482
Experience 8, Iter 12, disc loss: 0.008005829754036173, policy loss: 5.979917612736385
Experience 8, Iter 13, disc loss: 0.00856079574043866, policy loss: 6.034438190519926
Experience 8, Iter 14, disc loss: 0.010602199239694271, policy loss: 5.92623760763162
Experience 8, Iter 15, disc loss: 0.0064397067096693, policy loss: 6.711906669907538
Experience 8, Iter 16, disc loss: 0.00825140651159946, policy loss: 6.024561807101307
Experience 8, Iter 17, disc loss: 0.007093545158552879, policy loss: 6.192370595711996
Experience 8, Iter 18, disc loss: 0.008373197580014546, policy loss: 6.115263786950575
Experience 8, Iter 19, disc loss: 0.007469530855244558, policy loss: 6.076680465894405
Experience 8, Iter 20, disc loss: 0.006537805724381698, policy loss: 6.582264812436586
Experience 8, Iter 21, disc loss: 0.005961991174864997, policy loss: 6.357935980204255
Experience 8, Iter 22, disc loss: 0.007492648140037856, policy loss: 6.416968717618616
Experience 8, Iter 23, disc loss: 0.006946075062306323, policy loss: 6.253367759716559
Experience 8, Iter 24, disc loss: 0.006729389813128727, policy loss: 6.2755055976568705
Experience 8, Iter 25, disc loss: 0.008690587189730124, policy loss: 6.032030930871924
Experience 8, Iter 26, disc loss: 0.006878088401095252, policy loss: 6.4092646470268875
Experience 8, Iter 27, disc loss: 0.00831044375076105, policy loss: 6.194646239429311
Experience 8, Iter 28, disc loss: 0.007507623319441665, policy loss: 5.91851646770845
Experience 8, Iter 29, disc loss: 0.006098549136169817, policy loss: 6.404502432444197
Experience 8, Iter 30, disc loss: 0.007693697471129742, policy loss: 6.122262366070317
Experience 8, Iter 31, disc loss: 0.007479819843064488, policy loss: 6.213218762024011
Experience 8, Iter 32, disc loss: 0.007608741906187422, policy loss: 6.148847635885103
Experience 8, Iter 33, disc loss: 0.0069481304574255075, policy loss: 6.1873321982649845
Experience 8, Iter 34, disc loss: 0.007665577325644957, policy loss: 6.1624577700855605
Experience 8, Iter 35, disc loss: 0.006163079908957556, policy loss: 6.454403403108248
Experience 8, Iter 36, disc loss: 0.008074969707416627, policy loss: 6.191626000564142
Experience 8, Iter 37, disc loss: 0.007494817374138301, policy loss: 6.224675568366935
Experience 8, Iter 38, disc loss: 0.00675913123249863, policy loss: 6.241414678250772
Experience 8, Iter 39, disc loss: 0.007529658928917461, policy loss: 6.135309796480946
Experience 8, Iter 40, disc loss: 0.005833393970073111, policy loss: 6.39163797330073
Experience 8, Iter 41, disc loss: 0.00646876449384738, policy loss: 6.172690792260369
Experience 8, Iter 42, disc loss: 0.007382130055642905, policy loss: 5.993148698271069
Experience 8, Iter 43, disc loss: 0.006414943310776532, policy loss: 6.485636628560437
Experience 8, Iter 44, disc loss: 0.006796038103721361, policy loss: 6.338888648365852
Experience 8, Iter 45, disc loss: 0.006439530468023326, policy loss: 6.339327246275563
Experience 8, Iter 46, disc loss: 0.004899241048957597, policy loss: 6.78826102131255
Experience 8, Iter 47, disc loss: 0.005070670948462979, policy loss: 6.9136207146313815
Experience 8, Iter 48, disc loss: 0.004567837607535553, policy loss: 7.041276077108111
Experience 8, Iter 49, disc loss: 0.004320794685948207, policy loss: 6.831958719619728
Experience 8, Iter 50, disc loss: 0.0036983286475875494, policy loss: 7.09441165922976
Experience 8, Iter 51, disc loss: 0.004222546382207497, policy loss: 7.012996424384018
Experience 8, Iter 52, disc loss: 0.0039003998591162253, policy loss: 7.099685254092091
Experience 8, Iter 53, disc loss: 0.003886935541503357, policy loss: 6.8540531516906835
Experience 8, Iter 54, disc loss: 0.005083405876451948, policy loss: 7.10612782292818
Experience 8, Iter 55, disc loss: 0.0046787212339435905, policy loss: 6.718157554972047
Experience 8, Iter 56, disc loss: 0.005463129074089652, policy loss: 6.455675054311029
Experience 8, Iter 57, disc loss: 0.0074104752924372685, policy loss: 6.0532854537293135
Experience 8, Iter 58, disc loss: 0.00565472171300973, policy loss: 6.538202896725641
Experience 8, Iter 59, disc loss: 0.008077448870462283, policy loss: 6.225277002258275
Experience 8, Iter 60, disc loss: 0.006421013604080721, policy loss: 6.472115378594387
Experience 8, Iter 61, disc loss: 0.006434019410461548, policy loss: 6.280592881735202
Experience 8, Iter 62, disc loss: 0.00643287438824795, policy loss: 6.228009107927511
Experience 8, Iter 63, disc loss: 0.005165364613256768, policy loss: 6.802864421765198
Experience 8, Iter 64, disc loss: 0.005150177442720944, policy loss: 6.4143921218451645
Experience 8, Iter 65, disc loss: 0.006824287592798408, policy loss: 6.007325561864207
Experience 8, Iter 66, disc loss: 0.005777409903336115, policy loss: 6.435440656312723
Experience 8, Iter 67, disc loss: 0.005237156915678161, policy loss: 6.316557377291263
Experience 8, Iter 68, disc loss: 0.006185805742152346, policy loss: 6.4688399150465905
Experience 8, Iter 69, disc loss: 0.004892092785015139, policy loss: 6.511075365092802
Experience 8, Iter 70, disc loss: 0.007183220431402151, policy loss: 6.020672227384529
Experience 8, Iter 71, disc loss: 0.005448707392400511, policy loss: 6.149216127048273
Experience 8, Iter 72, disc loss: 0.0055455314707678754, policy loss: 6.516119151721687
Experience 8, Iter 73, disc loss: 0.006529343940309411, policy loss: 6.262335680907325
Experience 8, Iter 74, disc loss: 0.00533232756955755, policy loss: 6.529867116645844
Experience 8, Iter 75, disc loss: 0.005556402655742692, policy loss: 6.496141798196131
Experience 8, Iter 76, disc loss: 0.005771150126049156, policy loss: 6.651893848309647
Experience 8, Iter 77, disc loss: 0.005805647401452532, policy loss: 6.374908239926777
Experience 8, Iter 78, disc loss: 0.004832062874273471, policy loss: 6.490211193915279
Experience 8, Iter 79, disc loss: 0.005378947825976529, policy loss: 6.612509165269414
Experience 8, Iter 80, disc loss: 0.005938749326619211, policy loss: 6.263136025818242
Experience 8, Iter 81, disc loss: 0.004922000062462806, policy loss: 6.405514286998622
Experience 8, Iter 82, disc loss: 0.0058389468505333775, policy loss: 6.33322246492243
Experience 8, Iter 83, disc loss: 0.006001663259203644, policy loss: 6.328894733640324
Experience 8, Iter 84, disc loss: 0.004896536993351709, policy loss: 6.475313587386427
Experience 8, Iter 85, disc loss: 0.004828892457435287, policy loss: 6.462391466521356
Experience 8, Iter 86, disc loss: 0.005606770625185272, policy loss: 6.2917178776029
Experience 8, Iter 87, disc loss: 0.006914874972688586, policy loss: 6.282504752794392
Experience 8, Iter 88, disc loss: 0.006489466711565692, policy loss: 6.32693767156189
Experience 8, Iter 89, disc loss: 0.005078800037402927, policy loss: 6.580151627083713
Experience 8, Iter 90, disc loss: 0.005028771163792906, policy loss: 6.339158078423187
Experience 8, Iter 91, disc loss: 0.00613473539393139, policy loss: 6.227772768344714
Experience 8, Iter 92, disc loss: 0.005834399848159521, policy loss: 6.299965605362487
Experience 8, Iter 93, disc loss: 0.006180205381618899, policy loss: 6.058854057040709
Experience 8, Iter 94, disc loss: 0.004473455251412024, policy loss: 6.748605933590839
Experience 8, Iter 95, disc loss: 0.00438404416432443, policy loss: 6.676073915619642
Experience 8, Iter 96, disc loss: 0.004925998123811358, policy loss: 6.6205451023489355
Experience 8, Iter 97, disc loss: 0.004327536596835055, policy loss: 6.827393516306033
Experience 8, Iter 98, disc loss: 0.004556453576719795, policy loss: 6.690313287321467
Experience 8, Iter 99, disc loss: 0.0043436236716733, policy loss: 6.432335546522518
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0829],
        [1.0238],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0274, 0.1475, 0.7043, 0.0167, 0.0043, 1.9351]],

        [[0.0274, 0.1475, 0.7043, 0.0167, 0.0043, 1.9351]],

        [[0.0274, 0.1475, 0.7043, 0.0167, 0.0043, 1.9351]],

        [[0.0274, 0.1475, 0.7043, 0.0167, 0.0043, 1.9351]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0134, 0.3315, 4.0952, 0.0582], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0134, 0.3315, 4.0952, 0.0582])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.383
Iter 2/2000 - Loss: 2.305
Iter 3/2000 - Loss: 2.299
Iter 4/2000 - Loss: 2.196
Iter 5/2000 - Loss: 2.203
Iter 6/2000 - Loss: 2.216
Iter 7/2000 - Loss: 2.154
Iter 8/2000 - Loss: 2.086
Iter 9/2000 - Loss: 2.050
Iter 10/2000 - Loss: 2.015
Iter 11/2000 - Loss: 1.945
Iter 12/2000 - Loss: 1.852
Iter 13/2000 - Loss: 1.758
Iter 14/2000 - Loss: 1.664
Iter 15/2000 - Loss: 1.555
Iter 16/2000 - Loss: 1.418
Iter 17/2000 - Loss: 1.258
Iter 18/2000 - Loss: 1.081
Iter 19/2000 - Loss: 0.893
Iter 20/2000 - Loss: 0.692
Iter 1981/2000 - Loss: -7.069
Iter 1982/2000 - Loss: -7.069
Iter 1983/2000 - Loss: -7.069
Iter 1984/2000 - Loss: -7.069
Iter 1985/2000 - Loss: -7.069
Iter 1986/2000 - Loss: -7.069
Iter 1987/2000 - Loss: -7.069
Iter 1988/2000 - Loss: -7.069
Iter 1989/2000 - Loss: -7.069
Iter 1990/2000 - Loss: -7.070
Iter 1991/2000 - Loss: -7.070
Iter 1992/2000 - Loss: -7.070
Iter 1993/2000 - Loss: -7.070
Iter 1994/2000 - Loss: -7.070
Iter 1995/2000 - Loss: -7.070
Iter 1996/2000 - Loss: -7.070
Iter 1997/2000 - Loss: -7.070
Iter 1998/2000 - Loss: -7.070
Iter 1999/2000 - Loss: -7.070
Iter 2000/2000 - Loss: -7.070
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[18.1323,  5.9398, 45.4420,  8.6681,  3.5299, 47.7159]],

        [[22.2916, 38.6463, 11.9887,  1.1111,  2.7910, 29.4770]],

        [[24.5479, 43.8983, 10.1509,  0.9362,  2.0205, 17.5748]],

        [[22.2950, 36.5310, 14.8466,  4.2683,  1.0759, 37.1085]]])
Signal Variance: tensor([ 0.0841,  2.3037, 12.8987,  0.3194])
Estimated target variance: tensor([0.0134, 0.3315, 4.0952, 0.0582])
N: 90
Signal to noise ratio: tensor([16.2492, 74.0456, 80.6844, 35.9008])
Bound on condition number: tensor([ 23764.3765, 493448.3446, 585898.2035, 115999.0163])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0053780091281720455, policy loss: 6.227628055858132
Experience 9, Iter 1, disc loss: 0.005662487207310789, policy loss: 6.142296287060903
Experience 9, Iter 2, disc loss: 0.004625084506138183, policy loss: 6.474086589092287
Experience 9, Iter 3, disc loss: 0.004803919230932574, policy loss: 6.503929997085137
Experience 9, Iter 4, disc loss: 0.0055840697098664405, policy loss: 6.22043876793988
Experience 9, Iter 5, disc loss: 0.00492633001737031, policy loss: 6.326822114032075
Experience 9, Iter 6, disc loss: 0.005253877469383586, policy loss: 6.306412667310615
Experience 9, Iter 7, disc loss: 0.00677907575101185, policy loss: 6.074763862912732
Experience 9, Iter 8, disc loss: 0.0048810583708008655, policy loss: 6.333453229190702
Experience 9, Iter 9, disc loss: 0.006103964328102002, policy loss: 6.14755837271756
Experience 9, Iter 10, disc loss: 0.005870583524162252, policy loss: 5.9877113389534316
Experience 9, Iter 11, disc loss: 0.005564512140695327, policy loss: 6.148588641048193
Experience 9, Iter 12, disc loss: 0.005391290284859518, policy loss: 6.405994419884046
Experience 9, Iter 13, disc loss: 0.004620145398291498, policy loss: 6.429650212184846
Experience 9, Iter 14, disc loss: 0.005570126884091201, policy loss: 6.188560546474139
Experience 9, Iter 15, disc loss: 0.005116153505976556, policy loss: 6.366005181281774
Experience 9, Iter 16, disc loss: 0.005123420410327484, policy loss: 6.398461370615747
Experience 9, Iter 17, disc loss: 0.004542150591389675, policy loss: 6.3563373283920885
Experience 9, Iter 18, disc loss: 0.005072716954526171, policy loss: 6.15660752253497
Experience 9, Iter 19, disc loss: 0.006049181230689699, policy loss: 6.205083454225856
Experience 9, Iter 20, disc loss: 0.0056643977412006805, policy loss: 6.3171547054804265
Experience 9, Iter 21, disc loss: 0.005132946407653507, policy loss: 6.260544728792518
Experience 9, Iter 22, disc loss: 0.006252637530824757, policy loss: 6.081839369949975
Experience 9, Iter 23, disc loss: 0.004722064476386393, policy loss: 6.41756767577486
Experience 9, Iter 24, disc loss: 0.0055554428170012694, policy loss: 6.294732981152039
Experience 9, Iter 25, disc loss: 0.005562173711580883, policy loss: 6.4107827782335285
Experience 9, Iter 26, disc loss: 0.00480982934947064, policy loss: 6.613503044320147
Experience 9, Iter 27, disc loss: 0.005245745607716666, policy loss: 6.239157224411853
Experience 9, Iter 28, disc loss: 0.005302560933050709, policy loss: 6.399392346369056
Experience 9, Iter 29, disc loss: 0.004392572433362324, policy loss: 6.549943687766596
Experience 9, Iter 30, disc loss: 0.004976204019256001, policy loss: 6.601136549732385
Experience 9, Iter 31, disc loss: 0.004583109601694774, policy loss: 6.428192310342286
Experience 9, Iter 32, disc loss: 0.004638676513321074, policy loss: 6.362139014724031
Experience 9, Iter 33, disc loss: 0.005146579894319702, policy loss: 6.261075012289835
Experience 9, Iter 34, disc loss: 0.006827689811719444, policy loss: 6.064271239494018
Experience 9, Iter 35, disc loss: 0.004147645700060295, policy loss: 6.845692504151362
Experience 9, Iter 36, disc loss: 0.00467924292665186, policy loss: 6.549257990909352
Experience 9, Iter 37, disc loss: 0.006029179902888414, policy loss: 6.010386494932778
Experience 9, Iter 38, disc loss: 0.004882355362533321, policy loss: 6.421785367900452
Experience 9, Iter 39, disc loss: 0.006617300262707951, policy loss: 6.383432615848502
Experience 9, Iter 40, disc loss: 0.004104216036215142, policy loss: 6.486050771477105
Experience 9, Iter 41, disc loss: 0.005610389623698008, policy loss: 6.122306932196372
Experience 9, Iter 42, disc loss: 0.005209896061359927, policy loss: 6.270321911545152
Experience 9, Iter 43, disc loss: 0.004676932816840754, policy loss: 6.475038048340833
Experience 9, Iter 44, disc loss: 0.0043828432614108565, policy loss: 6.411202905607902
Experience 9, Iter 45, disc loss: 0.005091237140908439, policy loss: 6.474920030958739
Experience 9, Iter 46, disc loss: 0.0047813343238207835, policy loss: 6.323250690916218
Experience 9, Iter 47, disc loss: 0.0058389786505465896, policy loss: 6.2065363950640915
Experience 9, Iter 48, disc loss: 0.005972164943797379, policy loss: 6.2957136906851545
Experience 9, Iter 49, disc loss: 0.006165061288724158, policy loss: 6.391891877603072
Experience 9, Iter 50, disc loss: 0.004453121189772018, policy loss: 6.521368292084452
Experience 9, Iter 51, disc loss: 0.004744976006667357, policy loss: 6.563761488230648
Experience 9, Iter 52, disc loss: 0.004541340180910783, policy loss: 6.466083224940268
Experience 9, Iter 53, disc loss: 0.004644941129836373, policy loss: 6.537270387588026
Experience 9, Iter 54, disc loss: 0.0043202891612698175, policy loss: 6.578981137133173
Experience 9, Iter 55, disc loss: 0.0056699820707517765, policy loss: 6.357867603643015
Experience 9, Iter 56, disc loss: 0.004480699468288289, policy loss: 6.444247088055775
Experience 9, Iter 57, disc loss: 0.005311172521725634, policy loss: 6.362638414616883
Experience 9, Iter 58, disc loss: 0.0050305105283787715, policy loss: 6.339556774357092
Experience 9, Iter 59, disc loss: 0.0043063385510460715, policy loss: 6.415424923007914
Experience 9, Iter 60, disc loss: 0.004798506095017577, policy loss: 6.579481785732019
Experience 9, Iter 61, disc loss: 0.0057929433832536745, policy loss: 6.293664491618447
Experience 9, Iter 62, disc loss: 0.004988379972824182, policy loss: 6.502697631121119
Experience 9, Iter 63, disc loss: 0.004435476251593162, policy loss: 6.539163827316621
Experience 9, Iter 64, disc loss: 0.0051318429903596365, policy loss: 6.338206959886378
Experience 9, Iter 65, disc loss: 0.0035963325871211675, policy loss: 7.227209219239505
Experience 9, Iter 66, disc loss: 0.004003614197377011, policy loss: 7.351337875252641
Experience 9, Iter 67, disc loss: 0.005697413345355867, policy loss: 6.023268475459191
Experience 9, Iter 68, disc loss: 0.004562691093531506, policy loss: 6.465018075491635
Experience 9, Iter 69, disc loss: 0.0037558313003169526, policy loss: 6.600088583802715
Experience 9, Iter 70, disc loss: 0.0038956265940998944, policy loss: 6.658443772193081
Experience 9, Iter 71, disc loss: 0.0033914490104784757, policy loss: 6.843883318416687
Experience 9, Iter 72, disc loss: 0.00366071133737058, policy loss: 6.8053627204866345
Experience 9, Iter 73, disc loss: 0.003915321290429373, policy loss: 6.7409636081172835
Experience 9, Iter 74, disc loss: 0.0038061106389180584, policy loss: 6.61342652569323
Experience 9, Iter 75, disc loss: 0.004057079236675426, policy loss: 6.70567355338533
Experience 9, Iter 76, disc loss: 0.00402069025116175, policy loss: 6.592667511173502
Experience 9, Iter 77, disc loss: 0.004209090919246978, policy loss: 6.510488043828473
Experience 9, Iter 78, disc loss: 0.005172722359622672, policy loss: 6.388280676182307
Experience 9, Iter 79, disc loss: 0.00458396999412942, policy loss: 6.678048521848452
Experience 9, Iter 80, disc loss: 0.004057033746342253, policy loss: 6.778353133588886
Experience 9, Iter 81, disc loss: 0.0047476212112801675, policy loss: 6.489528684407473
Experience 9, Iter 82, disc loss: 0.004749899149682345, policy loss: 6.3133916548768685
Experience 9, Iter 83, disc loss: 0.0038911213310290887, policy loss: 6.722390793600314
Experience 9, Iter 84, disc loss: 0.0038901736174034715, policy loss: 6.705203990273199
Experience 9, Iter 85, disc loss: 0.003433716736462347, policy loss: 6.888671452577865
Experience 9, Iter 86, disc loss: 0.0029699192340378027, policy loss: 7.155707048724651
Experience 9, Iter 87, disc loss: 0.0029465190383432885, policy loss: 7.090906723254902
Experience 9, Iter 88, disc loss: 0.0029400018354484424, policy loss: 7.1455602771450755
Experience 9, Iter 89, disc loss: 0.0028588300506659212, policy loss: 7.082821930247169
Experience 9, Iter 90, disc loss: 0.0032157348791290257, policy loss: 6.888799705461713
Experience 9, Iter 91, disc loss: 0.0031093258191312017, policy loss: 6.876957117194806
Experience 9, Iter 92, disc loss: 0.0037549270312852144, policy loss: 6.859666490276583
Experience 9, Iter 93, disc loss: 0.00408491413090625, policy loss: 6.592000723060927
Experience 9, Iter 94, disc loss: 0.005164505610073591, policy loss: 6.22472468139402
Experience 9, Iter 95, disc loss: 0.004199963153507808, policy loss: 6.429128193319265
Experience 9, Iter 96, disc loss: 0.003831149763832225, policy loss: 6.9060841469290475
Experience 9, Iter 97, disc loss: 0.0037841967948593516, policy loss: 6.708282640247444
Experience 9, Iter 98, disc loss: 0.004689815907933509, policy loss: 6.1695332088846415
Experience 9, Iter 99, disc loss: 0.004701431703121099, policy loss: 6.362537984320214
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.0874],
        [1.0406],
        [0.0137]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0250, 0.1356, 0.6737, 0.0160, 0.0039, 2.0143]],

        [[0.0250, 0.1356, 0.6737, 0.0160, 0.0039, 2.0143]],

        [[0.0250, 0.1356, 0.6737, 0.0160, 0.0039, 2.0143]],

        [[0.0250, 0.1356, 0.6737, 0.0160, 0.0039, 2.0143]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0122, 0.3496, 4.1626, 0.0550], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0122, 0.3496, 4.1626, 0.0550])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.344
Iter 2/2000 - Loss: 2.277
Iter 3/2000 - Loss: 2.251
Iter 4/2000 - Loss: 2.145
Iter 5/2000 - Loss: 2.147
Iter 6/2000 - Loss: 2.149
Iter 7/2000 - Loss: 2.072
Iter 8/2000 - Loss: 1.995
Iter 9/2000 - Loss: 1.954
Iter 10/2000 - Loss: 1.905
Iter 11/2000 - Loss: 1.818
Iter 12/2000 - Loss: 1.712
Iter 13/2000 - Loss: 1.610
Iter 14/2000 - Loss: 1.505
Iter 15/2000 - Loss: 1.380
Iter 16/2000 - Loss: 1.228
Iter 17/2000 - Loss: 1.054
Iter 18/2000 - Loss: 0.865
Iter 19/2000 - Loss: 0.665
Iter 20/2000 - Loss: 0.452
Iter 1981/2000 - Loss: -7.271
Iter 1982/2000 - Loss: -7.271
Iter 1983/2000 - Loss: -7.271
Iter 1984/2000 - Loss: -7.271
Iter 1985/2000 - Loss: -7.271
Iter 1986/2000 - Loss: -7.271
Iter 1987/2000 - Loss: -7.271
Iter 1988/2000 - Loss: -7.271
Iter 1989/2000 - Loss: -7.271
Iter 1990/2000 - Loss: -7.271
Iter 1991/2000 - Loss: -7.271
Iter 1992/2000 - Loss: -7.271
Iter 1993/2000 - Loss: -7.271
Iter 1994/2000 - Loss: -7.271
Iter 1995/2000 - Loss: -7.271
Iter 1996/2000 - Loss: -7.271
Iter 1997/2000 - Loss: -7.271
Iter 1998/2000 - Loss: -7.272
Iter 1999/2000 - Loss: -7.272
Iter 2000/2000 - Loss: -7.272
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[16.4048,  2.9569, 44.5367,  8.6474, 10.1220, 43.2524]],

        [[22.1921, 36.2992, 12.0774,  1.1120,  2.6168, 29.8104]],

        [[24.2189, 41.4470, 10.2689,  0.9331,  1.9268, 18.0319]],

        [[21.4817, 34.3106, 15.3990,  4.3468,  1.0234, 36.8234]]])
Signal Variance: tensor([ 0.0472,  2.3242, 13.0081,  0.3457])
Estimated target variance: tensor([0.0122, 0.3496, 4.1626, 0.0550])
N: 100
Signal to noise ratio: tensor([11.4794, 75.3089, 83.8369, 38.3626])
Bound on condition number: tensor([ 13178.5829, 567143.6333, 702862.9130, 147170.2667])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.004666575808254994, policy loss: 6.7897690106762365
Experience 10, Iter 1, disc loss: 0.004038905098347956, policy loss: 6.495483292337248
Experience 10, Iter 2, disc loss: 0.0030084621523903564, policy loss: 6.783208475619012
Experience 10, Iter 3, disc loss: 0.0037493841120264386, policy loss: 6.757802869116585
Experience 10, Iter 4, disc loss: 0.00377303844596634, policy loss: 6.645516572044314
Experience 10, Iter 5, disc loss: 0.0037478513914236244, policy loss: 6.580870277543129
Experience 10, Iter 6, disc loss: 0.0038523508016732235, policy loss: 6.614384417714039
Experience 10, Iter 7, disc loss: 0.0042078635736782725, policy loss: 6.342870416748087
Experience 10, Iter 8, disc loss: 0.0045059966432690075, policy loss: 6.280266986561244
Experience 10, Iter 9, disc loss: 0.0032302909071004267, policy loss: 6.80402495776078
Experience 10, Iter 10, disc loss: 0.004484125382092578, policy loss: 6.719757500415621
Experience 10, Iter 11, disc loss: 0.003907994480221586, policy loss: 6.816492428907612
Experience 10, Iter 12, disc loss: 0.004129453028308274, policy loss: 6.564890015986827
Experience 10, Iter 13, disc loss: 0.004166467687677594, policy loss: 6.5249532619170285
Experience 10, Iter 14, disc loss: 0.004068405837955668, policy loss: 6.555646625216774
Experience 10, Iter 15, disc loss: 0.0032152609154849294, policy loss: 6.765568814856179
Experience 10, Iter 16, disc loss: 0.004063682761366226, policy loss: 6.6574305795325355
Experience 10, Iter 17, disc loss: 0.0026173790305121757, policy loss: 7.211610137554094
Experience 10, Iter 18, disc loss: 0.002351368919426402, policy loss: 7.451090757601052
Experience 10, Iter 19, disc loss: 0.0029214281286038734, policy loss: 7.119229653003078
Experience 10, Iter 20, disc loss: 0.003358319219319019, policy loss: 6.9351752710307135
Experience 10, Iter 21, disc loss: 0.0028467474631744096, policy loss: 7.038102647306889
Experience 10, Iter 22, disc loss: 0.003593907088189576, policy loss: 7.009435682772409
Experience 10, Iter 23, disc loss: 0.0030172507444602616, policy loss: 7.1091108837733
Experience 10, Iter 24, disc loss: 0.0037984974300990363, policy loss: 6.637359037114253
Experience 10, Iter 25, disc loss: 0.004590409014484976, policy loss: 6.689450421101996
Experience 10, Iter 26, disc loss: 0.003674185620977435, policy loss: 6.625228936296565
Experience 10, Iter 27, disc loss: 0.0038916477939264695, policy loss: 6.432175683189742
Experience 10, Iter 28, disc loss: 0.004021308454245139, policy loss: 6.685175926195417
Experience 10, Iter 29, disc loss: 0.0038228074151900257, policy loss: 6.738489941389152
Experience 10, Iter 30, disc loss: 0.003596522788972191, policy loss: 7.41814165608657
Experience 10, Iter 31, disc loss: 0.00342971832988298, policy loss: 6.882821542984596
Experience 10, Iter 32, disc loss: 0.003967383408068966, policy loss: 6.545903221364828
Experience 10, Iter 33, disc loss: 0.003019899016198331, policy loss: 6.79754222994867
Experience 10, Iter 34, disc loss: 0.0024567238284440634, policy loss: 7.272037071101089
Experience 10, Iter 35, disc loss: 0.0034684969642960013, policy loss: 6.934706795479979
Experience 10, Iter 36, disc loss: 0.003755341399298881, policy loss: 6.760636168553122
Experience 10, Iter 37, disc loss: 0.003201555875355231, policy loss: 6.827458363043676
Experience 10, Iter 38, disc loss: 0.0032712225225927545, policy loss: 6.921794733413802
Experience 10, Iter 39, disc loss: 0.0029635330654225146, policy loss: 7.077202154098789
Experience 10, Iter 40, disc loss: 0.0028049798823360584, policy loss: 7.26918346242439
Experience 10, Iter 41, disc loss: 0.00341523802966237, policy loss: 6.787422506097894
Experience 10, Iter 42, disc loss: 0.0029334234674931606, policy loss: 7.096429379847038
Experience 10, Iter 43, disc loss: 0.0037300528697055, policy loss: 6.80086802537638
Experience 10, Iter 44, disc loss: 0.0026507302308397688, policy loss: 7.158516516321507
Experience 10, Iter 45, disc loss: 0.0030233438144673394, policy loss: 7.0519604246135845
Experience 10, Iter 46, disc loss: 0.002942917654688879, policy loss: 6.9726679827561675
Experience 10, Iter 47, disc loss: 0.0025963594644312178, policy loss: 7.111163447487269
Experience 10, Iter 48, disc loss: 0.0032070401800649573, policy loss: 6.861673428823088
Experience 10, Iter 49, disc loss: 0.0033793130182016833, policy loss: 6.614286993705836
Experience 10, Iter 50, disc loss: 0.0036643622028664858, policy loss: 6.966894723484459
Experience 10, Iter 51, disc loss: 0.003096247323291928, policy loss: 7.768080429449348
Experience 10, Iter 52, disc loss: 0.003358848870115684, policy loss: 7.8677486746662835
Experience 10, Iter 53, disc loss: 0.003397241041075724, policy loss: 6.901726084031749
Experience 10, Iter 54, disc loss: 0.0028960252919579023, policy loss: 6.807979770101632
Experience 10, Iter 55, disc loss: 0.0034132146929140407, policy loss: 6.77134082670954
Experience 10, Iter 56, disc loss: 0.0029292884881146116, policy loss: 6.930786713679115
Experience 10, Iter 57, disc loss: 0.002588565510587684, policy loss: 7.093657709984926
Experience 10, Iter 58, disc loss: 0.0022831857775561888, policy loss: 7.281551256752506
Experience 10, Iter 59, disc loss: 0.0026556737545974235, policy loss: 7.212125723986073
Experience 10, Iter 60, disc loss: 0.003182543518342237, policy loss: 7.162154599066303
Experience 10, Iter 61, disc loss: 0.003216869045482007, policy loss: 7.053866544837204
Experience 10, Iter 62, disc loss: 0.0029088497789804277, policy loss: 7.139721848587566
Experience 10, Iter 63, disc loss: 0.002479079247345132, policy loss: 7.272145993350111
Experience 10, Iter 64, disc loss: 0.0033447165624170775, policy loss: 6.8696471608192216
Experience 10, Iter 65, disc loss: 0.0030446336625227543, policy loss: 6.80392600964033
Experience 10, Iter 66, disc loss: 0.003579795857385814, policy loss: 6.621387307650848
Experience 10, Iter 67, disc loss: 0.0031880201774683484, policy loss: 6.979041005706147
Experience 10, Iter 68, disc loss: 0.0027791717679183643, policy loss: 7.473856103942693
Experience 10, Iter 69, disc loss: 0.0032358113857086446, policy loss: 7.218065628770174
Experience 10, Iter 70, disc loss: 0.0037856108587886465, policy loss: 6.7220211275041315
Experience 10, Iter 71, disc loss: 0.0031364129714743626, policy loss: 6.810619274422419
Experience 10, Iter 72, disc loss: 0.0034004888877909078, policy loss: 6.689490796089416
Experience 10, Iter 73, disc loss: 0.003271146516288796, policy loss: 6.710887051307364
Experience 10, Iter 74, disc loss: 0.003921970268701783, policy loss: 6.591789263516535
Experience 10, Iter 75, disc loss: 0.0033075307496844633, policy loss: 6.67767084950726
Experience 10, Iter 76, disc loss: 0.0037090753429762325, policy loss: 6.8046844144983964
Experience 10, Iter 77, disc loss: 0.003728168000016887, policy loss: 6.792536913653102
Experience 10, Iter 78, disc loss: 0.003385949710645522, policy loss: 6.728949946057623
Experience 10, Iter 79, disc loss: 0.003927321767409778, policy loss: 6.56478661345316
Experience 10, Iter 80, disc loss: 0.0028588707228808514, policy loss: 6.736667372379178
Experience 10, Iter 81, disc loss: 0.0035727077707485995, policy loss: 6.671350916758222
Experience 10, Iter 82, disc loss: 0.003453835272604105, policy loss: 6.782640045878417
Experience 10, Iter 83, disc loss: 0.0038547878908070177, policy loss: 6.476130674712252
Experience 10, Iter 84, disc loss: 0.003470950344565799, policy loss: 7.02611996854395
Experience 10, Iter 85, disc loss: 0.0043652533597314, policy loss: 6.782488247875337
Experience 10, Iter 86, disc loss: 0.003624594634155158, policy loss: 6.658326019860148
Experience 10, Iter 87, disc loss: 0.003568139869065321, policy loss: 6.574078048425922
Experience 10, Iter 88, disc loss: 0.00472740366795984, policy loss: 6.425521323712898
Experience 10, Iter 89, disc loss: 0.0038439884496291312, policy loss: 6.753699746411281
Experience 10, Iter 90, disc loss: 0.003032512413045658, policy loss: 6.994178587788927
Experience 10, Iter 91, disc loss: 0.0035618423740920915, policy loss: 6.611608163422001
Experience 10, Iter 92, disc loss: 0.002946943303098982, policy loss: 7.0784209721610445
Experience 10, Iter 93, disc loss: 0.004181619228529427, policy loss: 6.451098470610548
Experience 10, Iter 94, disc loss: 0.00406552159557987, policy loss: 6.381330556635232
Experience 10, Iter 95, disc loss: 0.0037930970552271055, policy loss: 6.550931940617219
Experience 10, Iter 96, disc loss: 0.003721829209453734, policy loss: 6.791678826626637
Experience 10, Iter 97, disc loss: 0.003766105709540974, policy loss: 6.7231646638398805
Experience 10, Iter 98, disc loss: 0.0032159365760665816, policy loss: 6.944137317844744
Experience 10, Iter 99, disc loss: 0.004241153769242566, policy loss: 6.410006616198973
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.0920],
        [1.0793],
        [0.0134]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0234, 0.1262, 0.6672, 0.0155, 0.0036, 2.0713]],

        [[0.0234, 0.1262, 0.6672, 0.0155, 0.0036, 2.0713]],

        [[0.0234, 0.1262, 0.6672, 0.0155, 0.0036, 2.0713]],

        [[0.0234, 0.1262, 0.6672, 0.0155, 0.0036, 2.0713]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.3681, 4.3170, 0.0536], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.3681, 4.3170, 0.0536])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.341
Iter 2/2000 - Loss: 2.296
Iter 3/2000 - Loss: 2.253
Iter 4/2000 - Loss: 2.155
Iter 5/2000 - Loss: 2.169
Iter 6/2000 - Loss: 2.170
Iter 7/2000 - Loss: 2.092
Iter 8/2000 - Loss: 2.026
Iter 9/2000 - Loss: 1.998
Iter 10/2000 - Loss: 1.950
Iter 11/2000 - Loss: 1.861
Iter 12/2000 - Loss: 1.763
Iter 13/2000 - Loss: 1.672
Iter 14/2000 - Loss: 1.573
Iter 15/2000 - Loss: 1.447
Iter 16/2000 - Loss: 1.293
Iter 17/2000 - Loss: 1.119
Iter 18/2000 - Loss: 0.932
Iter 19/2000 - Loss: 0.733
Iter 20/2000 - Loss: 0.517
Iter 1981/2000 - Loss: -7.383
Iter 1982/2000 - Loss: -7.383
Iter 1983/2000 - Loss: -7.383
Iter 1984/2000 - Loss: -7.383
Iter 1985/2000 - Loss: -7.383
Iter 1986/2000 - Loss: -7.383
Iter 1987/2000 - Loss: -7.383
Iter 1988/2000 - Loss: -7.383
Iter 1989/2000 - Loss: -7.383
Iter 1990/2000 - Loss: -7.383
Iter 1991/2000 - Loss: -7.383
Iter 1992/2000 - Loss: -7.383
Iter 1993/2000 - Loss: -7.383
Iter 1994/2000 - Loss: -7.383
Iter 1995/2000 - Loss: -7.383
Iter 1996/2000 - Loss: -7.383
Iter 1997/2000 - Loss: -7.383
Iter 1998/2000 - Loss: -7.384
Iter 1999/2000 - Loss: -7.384
Iter 2000/2000 - Loss: -7.384
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[16.1133,  2.9853, 44.3130,  8.5607, 10.0934, 45.1284]],

        [[22.0079, 37.1480, 12.0481,  1.1280,  2.7313, 30.8125]],

        [[22.2793, 40.0537,  9.8969,  0.9708,  2.0441, 19.4916]],

        [[19.1553, 32.5149, 14.6895,  3.7904,  1.0431, 33.2711]]])
Signal Variance: tensor([ 0.0474,  2.3686, 13.7504,  0.3191])
Estimated target variance: tensor([0.0113, 0.3681, 4.3170, 0.0536])
N: 110
Signal to noise ratio: tensor([11.6276, 78.0893, 80.8869, 34.7781])
Bound on condition number: tensor([ 14873.1798, 670774.9103, 719696.8938, 133047.9191])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.002730670326696191, policy loss: 6.841893548310492
Experience 11, Iter 1, disc loss: 0.0029730763467631753, policy loss: 6.6643224857721695
Experience 11, Iter 2, disc loss: 0.0036537950812583785, policy loss: 6.770856552127517
Experience 11, Iter 3, disc loss: 0.004395382974633655, policy loss: 6.375580386730898
Experience 11, Iter 4, disc loss: 0.00300238352096278, policy loss: 6.911750563507172
Experience 11, Iter 5, disc loss: 0.0034086909333924205, policy loss: 6.645804239665747
Experience 11, Iter 6, disc loss: 0.0034189088523423174, policy loss: 6.593000928246268
Experience 11, Iter 7, disc loss: 0.0035739636030811188, policy loss: 6.723254102428831
Experience 11, Iter 8, disc loss: 0.0030688586976355295, policy loss: 6.755287819205664
Experience 11, Iter 9, disc loss: 0.0038945908490865093, policy loss: 6.538407437662774
Experience 11, Iter 10, disc loss: 0.004118644135995162, policy loss: 6.382762288003436
Experience 11, Iter 11, disc loss: 0.003595948404282884, policy loss: 6.582860226762265
Experience 11, Iter 12, disc loss: 0.0034919762551284743, policy loss: 6.782617271891428
Experience 11, Iter 13, disc loss: 0.0035172240733526284, policy loss: 6.771255459583017
Experience 11, Iter 14, disc loss: 0.0034651089872165426, policy loss: 6.75987424886341
Experience 11, Iter 15, disc loss: 0.0030244147659868473, policy loss: 7.131846747120148
Experience 11, Iter 16, disc loss: 0.00318291626604249, policy loss: 6.824047783278189
Experience 11, Iter 17, disc loss: 0.003218905115952851, policy loss: 7.026068530116982
Experience 11, Iter 18, disc loss: 0.0033516204013815925, policy loss: 6.670778121945386
Experience 11, Iter 19, disc loss: 0.0033274146046246753, policy loss: 6.615541229718197
Experience 11, Iter 20, disc loss: 0.003510628710152824, policy loss: 6.812353629449987
Experience 11, Iter 21, disc loss: 0.004045071218088641, policy loss: 6.705175386713131
Experience 11, Iter 22, disc loss: 0.0038614976764566613, policy loss: 6.767135781146111
Experience 11, Iter 23, disc loss: 0.0031518079021758986, policy loss: 6.585611367703471
Experience 11, Iter 24, disc loss: 0.002967061187110831, policy loss: 7.14865130703721
Experience 11, Iter 25, disc loss: 0.003229071009643269, policy loss: 6.620848215484375
Experience 11, Iter 26, disc loss: 0.0028196405906672595, policy loss: 6.908136796098705
Experience 11, Iter 27, disc loss: 0.0030537127824259306, policy loss: 6.901567145520332
Experience 11, Iter 28, disc loss: 0.0032172289265853782, policy loss: 6.868890750833217
Experience 11, Iter 29, disc loss: 0.003462219849914282, policy loss: 6.703078242961681
Experience 11, Iter 30, disc loss: 0.003451117169220051, policy loss: 6.967513274246091
Experience 11, Iter 31, disc loss: 0.0028880088928025438, policy loss: 6.843781418575461
Experience 11, Iter 32, disc loss: 0.0031783077851710163, policy loss: 6.626881339176559
Experience 11, Iter 33, disc loss: 0.0025734581516540485, policy loss: 7.0035153325292985
Experience 11, Iter 34, disc loss: 0.002315679580302586, policy loss: 7.18786715621675
Experience 11, Iter 35, disc loss: 0.002524527356693741, policy loss: 7.221408482528256
Experience 11, Iter 36, disc loss: 0.0022654670015314635, policy loss: 7.117400182706952
Experience 11, Iter 37, disc loss: 0.0032544821497166613, policy loss: 6.608112936017638
Experience 11, Iter 38, disc loss: 0.0024976875886845358, policy loss: 7.38333280530677
Experience 11, Iter 39, disc loss: 0.003058710247781049, policy loss: 7.63555090019282
Experience 11, Iter 40, disc loss: 0.0038264949602493737, policy loss: 6.69291550502367
Experience 11, Iter 41, disc loss: 0.0026295834806187666, policy loss: 6.955226841185227
Experience 11, Iter 42, disc loss: 0.002501478203370302, policy loss: 7.225567491607569
Experience 11, Iter 43, disc loss: 0.0024623094915069968, policy loss: 7.21788169418658
Experience 11, Iter 44, disc loss: 0.0026176501163640287, policy loss: 7.093437178105361
Experience 11, Iter 45, disc loss: 0.0028176264602135666, policy loss: 6.822332761533526
Experience 11, Iter 46, disc loss: 0.003132003500942188, policy loss: 6.857798227948711
Experience 11, Iter 47, disc loss: 0.002705874223608058, policy loss: 7.065666133095709
Experience 11, Iter 48, disc loss: 0.0033748010860748445, policy loss: 7.300884143926268
Experience 11, Iter 49, disc loss: 0.0029329731234785704, policy loss: 6.946301236928359
Experience 11, Iter 50, disc loss: 0.002607699097254979, policy loss: 7.061928925140729
Experience 11, Iter 51, disc loss: 0.002686848898129848, policy loss: 6.972890733034052
Experience 11, Iter 52, disc loss: 0.0021711234927638988, policy loss: 7.641148732553144
Experience 11, Iter 53, disc loss: 0.0017382928358583075, policy loss: 7.757150740282258
Experience 11, Iter 54, disc loss: 0.0016129265303290972, policy loss: 7.811347329925101
Experience 11, Iter 55, disc loss: 0.001782641848599659, policy loss: 7.719995327211631
Experience 11, Iter 56, disc loss: 0.0018098048902163216, policy loss: 8.154347721643223
Experience 11, Iter 57, disc loss: 0.0015381851032166991, policy loss: 7.942558878049607
Experience 11, Iter 58, disc loss: 0.001638959591378389, policy loss: 7.861719132951555
Experience 11, Iter 59, disc loss: 0.0015750733296980189, policy loss: 7.754245612453108
Experience 11, Iter 60, disc loss: 0.001783341109940747, policy loss: 7.613631882862645
Experience 11, Iter 61, disc loss: 0.0021075084884430067, policy loss: 7.381337865204943
Experience 11, Iter 62, disc loss: 0.002958753557541301, policy loss: 7.048245429241992
Experience 11, Iter 63, disc loss: 0.00246326966156713, policy loss: 7.231239258320404
Experience 11, Iter 64, disc loss: 0.0026006872634214057, policy loss: 7.097618696577222
Experience 11, Iter 65, disc loss: 0.002561838749112765, policy loss: 7.226058241758485
Experience 11, Iter 66, disc loss: 0.0024002267128500447, policy loss: 7.110792481002391
Experience 11, Iter 67, disc loss: 0.0022172599132026204, policy loss: 7.638131053912608
Experience 11, Iter 68, disc loss: 0.0024808230575155527, policy loss: 7.125200629613632
Experience 11, Iter 69, disc loss: 0.0020235980285131057, policy loss: 7.468341472398448
Experience 11, Iter 70, disc loss: 0.0031285478998214476, policy loss: 7.212774781921851
Experience 11, Iter 71, disc loss: 0.001991639681104623, policy loss: 7.558359825448129
Experience 11, Iter 72, disc loss: 0.0020681285593544974, policy loss: 7.673608066178124
Experience 11, Iter 73, disc loss: 0.0017494503981059385, policy loss: 7.830410489405229
Experience 11, Iter 74, disc loss: 0.00184273159667037, policy loss: 7.839063296581754
Experience 11, Iter 75, disc loss: 0.002063407766437691, policy loss: 7.941398470082607
Experience 11, Iter 76, disc loss: 0.0014358276182903397, policy loss: 8.249254335546336
Experience 11, Iter 77, disc loss: 0.001911254425425286, policy loss: 7.688692739693004
Experience 11, Iter 78, disc loss: 0.001761029256599095, policy loss: 7.88532082836586
Experience 11, Iter 79, disc loss: 0.0016027601807485144, policy loss: 8.129062708515708
Experience 11, Iter 80, disc loss: 0.002164172178557484, policy loss: 7.563759802641414
Experience 11, Iter 81, disc loss: 0.001876773001445097, policy loss: 7.719710468640765
Experience 11, Iter 82, disc loss: 0.0019001794589273008, policy loss: 7.683750422245102
Experience 11, Iter 83, disc loss: 0.0017981982585794655, policy loss: 7.841891839223658
Experience 11, Iter 84, disc loss: 0.0021147564505766964, policy loss: 7.661275394636634
Experience 11, Iter 85, disc loss: 0.0022665470488154424, policy loss: 7.482159534071304
Experience 11, Iter 86, disc loss: 0.001533840301457356, policy loss: 8.013019674551344
Experience 11, Iter 87, disc loss: 0.0024201783302832455, policy loss: 7.345094548143826
Experience 11, Iter 88, disc loss: 0.0022007246450336253, policy loss: 7.394573675439136
Experience 11, Iter 89, disc loss: 0.0019739474979700274, policy loss: 7.68638774611216
Experience 11, Iter 90, disc loss: 0.0019125413691924533, policy loss: 7.4130984780371145
Experience 11, Iter 91, disc loss: 0.0019648510144998953, policy loss: 7.506657490734638
Experience 11, Iter 92, disc loss: 0.0018470786667875063, policy loss: 7.386596045552712
Experience 11, Iter 93, disc loss: 0.0021795688431240296, policy loss: 7.130236221301119
Experience 11, Iter 94, disc loss: 0.002842428424189121, policy loss: 6.687167085600466
Experience 11, Iter 95, disc loss: 0.0026051987989155227, policy loss: 7.3156823018933865
Experience 11, Iter 96, disc loss: 0.0027603239403766336, policy loss: 7.020728940425627
Experience 11, Iter 97, disc loss: 0.002289599728912791, policy loss: 7.194385025677792
Experience 11, Iter 98, disc loss: 0.0021472790704183797, policy loss: 7.3341999320790885
Experience 11, Iter 99, disc loss: 0.0018525625067371749, policy loss: 7.38913685454769
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0913],
        [1.0346],
        [0.0126]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.1187, 0.6283, 0.0153, 0.0034, 2.1126]],

        [[0.0217, 0.1187, 0.6283, 0.0153, 0.0034, 2.1126]],

        [[0.0217, 0.1187, 0.6283, 0.0153, 0.0034, 2.1126]],

        [[0.0217, 0.1187, 0.6283, 0.0153, 0.0034, 2.1126]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.3653, 4.1385, 0.0503], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.3653, 4.1385, 0.0503])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.257
Iter 2/2000 - Loss: 2.239
Iter 3/2000 - Loss: 2.173
Iter 4/2000 - Loss: 2.083
Iter 5/2000 - Loss: 2.109
Iter 6/2000 - Loss: 2.105
Iter 7/2000 - Loss: 2.025
Iter 8/2000 - Loss: 1.967
Iter 9/2000 - Loss: 1.948
Iter 10/2000 - Loss: 1.903
Iter 11/2000 - Loss: 1.818
Iter 12/2000 - Loss: 1.727
Iter 13/2000 - Loss: 1.644
Iter 14/2000 - Loss: 1.550
Iter 15/2000 - Loss: 1.429
Iter 16/2000 - Loss: 1.282
Iter 17/2000 - Loss: 1.117
Iter 18/2000 - Loss: 0.940
Iter 19/2000 - Loss: 0.747
Iter 20/2000 - Loss: 0.535
Iter 1981/2000 - Loss: -7.526
Iter 1982/2000 - Loss: -7.527
Iter 1983/2000 - Loss: -7.527
Iter 1984/2000 - Loss: -7.527
Iter 1985/2000 - Loss: -7.527
Iter 1986/2000 - Loss: -7.527
Iter 1987/2000 - Loss: -7.527
Iter 1988/2000 - Loss: -7.527
Iter 1989/2000 - Loss: -7.527
Iter 1990/2000 - Loss: -7.527
Iter 1991/2000 - Loss: -7.527
Iter 1992/2000 - Loss: -7.527
Iter 1993/2000 - Loss: -7.527
Iter 1994/2000 - Loss: -7.527
Iter 1995/2000 - Loss: -7.527
Iter 1996/2000 - Loss: -7.527
Iter 1997/2000 - Loss: -7.527
Iter 1998/2000 - Loss: -7.527
Iter 1999/2000 - Loss: -7.527
Iter 2000/2000 - Loss: -7.527
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[16.0109,  2.9973, 43.3959,  8.1889,  9.8298, 44.9250]],

        [[20.9727, 35.2843, 11.7366,  1.1275,  2.7307, 30.8252]],

        [[21.5011, 37.7860,  9.4700,  0.9694,  2.1040, 19.1269]],

        [[17.9026, 31.5217, 15.5668,  3.9879,  1.0142, 31.3168]]])
Signal Variance: tensor([ 0.0458,  2.3195, 13.6609,  0.3450])
Estimated target variance: tensor([0.0108, 0.3653, 4.1385, 0.0503])
N: 120
Signal to noise ratio: tensor([11.6198, 79.0905, 81.9214, 35.8885])
Bound on condition number: tensor([ 16203.5034, 750638.3157, 805335.4711, 154559.0421])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.001694354075186474, policy loss: 7.459419554323926
Experience 12, Iter 1, disc loss: 0.0020143035079299007, policy loss: 7.146657243348209
Experience 12, Iter 2, disc loss: 0.0021774264737387107, policy loss: 7.590458887161875
Experience 12, Iter 3, disc loss: 0.002820732397601874, policy loss: 7.214665019375563
Experience 12, Iter 4, disc loss: 0.00259792071468687, policy loss: 6.967340513568123
Experience 12, Iter 5, disc loss: 0.002683068418054916, policy loss: 7.431932259698954
Experience 12, Iter 6, disc loss: 0.0026237924366195897, policy loss: 7.052142697152947
Experience 12, Iter 7, disc loss: 0.0018210361082900133, policy loss: 7.532071844359134
Experience 12, Iter 8, disc loss: 0.0024654327784206743, policy loss: 7.047200968509802
Experience 12, Iter 9, disc loss: 0.0022033897409435444, policy loss: 7.153702428494798
Experience 12, Iter 10, disc loss: 0.0021696866494930822, policy loss: 7.162777091898384
Experience 12, Iter 11, disc loss: 0.002568773501511838, policy loss: 7.053841635279323
Experience 12, Iter 12, disc loss: 0.0018809756489277531, policy loss: 7.298289229127056
Experience 12, Iter 13, disc loss: 0.002354159666520179, policy loss: 7.254999615724781
Experience 12, Iter 14, disc loss: 0.002226721585978536, policy loss: 7.0077811628181506
Experience 12, Iter 15, disc loss: 0.001824886677593648, policy loss: 7.48710421820147
Experience 12, Iter 16, disc loss: 0.002175928578407303, policy loss: 7.152832910240534
Experience 12, Iter 17, disc loss: 0.0018845258305099765, policy loss: 7.433579227282995
Experience 12, Iter 18, disc loss: 0.0020192681414501518, policy loss: 7.296768870911622
Experience 12, Iter 19, disc loss: 0.0020880824592335832, policy loss: 7.173249237323713
Experience 12, Iter 20, disc loss: 0.002492739601935503, policy loss: 7.234997595612957
Experience 12, Iter 21, disc loss: 0.0026127890813013033, policy loss: 6.979013620914789
Experience 12, Iter 22, disc loss: 0.0024880177671655936, policy loss: 7.149105091521136
Experience 12, Iter 23, disc loss: 0.0024498986502187402, policy loss: 7.419274518347457
Experience 12, Iter 24, disc loss: 0.00324905415375015, policy loss: 6.935455647465992
Experience 12, Iter 25, disc loss: 0.002728849710157937, policy loss: 7.0649634383703805
Experience 12, Iter 26, disc loss: 0.0020252479073879183, policy loss: 7.28559009474711
Experience 12, Iter 27, disc loss: 0.002507968277769202, policy loss: 6.991095825579144
Experience 12, Iter 28, disc loss: 0.002757356956600248, policy loss: 6.760513616930799
Experience 12, Iter 29, disc loss: 0.0024789895398189604, policy loss: 7.093042144225835
Experience 12, Iter 30, disc loss: 0.0024809759428430726, policy loss: 6.982577618671491
Experience 12, Iter 31, disc loss: 0.0024467813483391686, policy loss: 7.311273535233068
Experience 12, Iter 32, disc loss: 0.002313962338723242, policy loss: 7.066853485469899
Experience 12, Iter 33, disc loss: 0.002074277641327135, policy loss: 7.260228461609227
Experience 12, Iter 34, disc loss: 0.0024720747948687406, policy loss: 7.053958958301781
Experience 12, Iter 35, disc loss: 0.002413947813226929, policy loss: 6.975535557232991
Experience 12, Iter 36, disc loss: 0.0022020765961129726, policy loss: 7.231169497518881
Experience 12, Iter 37, disc loss: 0.002044694925402945, policy loss: 7.16674279390337
Experience 12, Iter 38, disc loss: 0.002299175636131464, policy loss: 7.038983301531732
Experience 12, Iter 39, disc loss: 0.0023902133411934164, policy loss: 7.233692612556947
Experience 12, Iter 40, disc loss: 0.00251384921507218, policy loss: 6.9922461724267535
Experience 12, Iter 41, disc loss: 0.002371204129978686, policy loss: 7.177826477409816
Experience 12, Iter 42, disc loss: 0.0024524452905220454, policy loss: 7.173493331375441
Experience 12, Iter 43, disc loss: 0.002546002698364193, policy loss: 6.977386846651534
Experience 12, Iter 44, disc loss: 0.002350005408988867, policy loss: 7.456844537770877
Experience 12, Iter 45, disc loss: 0.002430041826397209, policy loss: 7.355897800082266
Experience 12, Iter 46, disc loss: 0.0024479394405672586, policy loss: 7.0891422596043245
Experience 12, Iter 47, disc loss: 0.0022640251507498807, policy loss: 6.949834969376186
Experience 12, Iter 48, disc loss: 0.0024021219086545636, policy loss: 7.107412040519028
Experience 12, Iter 49, disc loss: 0.0021305313618834717, policy loss: 7.27298329911961
Experience 12, Iter 50, disc loss: 0.0020184764689620514, policy loss: 7.410374077468974
Experience 12, Iter 51, disc loss: 0.0023329314983899386, policy loss: 7.081048478793472
Experience 12, Iter 52, disc loss: 0.002058286446035833, policy loss: 7.383501853475071
Experience 12, Iter 53, disc loss: 0.002605155909416362, policy loss: 6.959479948987795
Experience 12, Iter 54, disc loss: 0.0021272182262515155, policy loss: 7.472820144661231
Experience 12, Iter 55, disc loss: 0.00242472005461229, policy loss: 7.329560596520465
Experience 12, Iter 56, disc loss: 0.0018913994641737264, policy loss: 7.3662161060664495
Experience 12, Iter 57, disc loss: 0.0022384290497435787, policy loss: 7.120284751589785
Experience 12, Iter 58, disc loss: 0.0018891298492082482, policy loss: 7.482998363905553
Experience 12, Iter 59, disc loss: 0.0022100193314858406, policy loss: 7.113751983828812
Experience 12, Iter 60, disc loss: 0.002244101924337784, policy loss: 7.416100748129471
Experience 12, Iter 61, disc loss: 0.0024646192575785734, policy loss: 6.968805335033471
Experience 12, Iter 62, disc loss: 0.001939778720687937, policy loss: 7.520693499249407
Experience 12, Iter 63, disc loss: 0.0018189593152165024, policy loss: 8.131625081888686
Experience 12, Iter 64, disc loss: 0.0023311831180297324, policy loss: 7.282878657449643
Experience 12, Iter 65, disc loss: 0.001827736195224553, policy loss: 7.364922644545482
Experience 12, Iter 66, disc loss: 0.0017295930334956624, policy loss: 7.475628510190477
Experience 12, Iter 67, disc loss: 0.0017056965840103505, policy loss: 7.446409301797976
Experience 12, Iter 68, disc loss: 0.0018451519894777738, policy loss: 7.434098426292389
Experience 12, Iter 69, disc loss: 0.001800041346233557, policy loss: 7.555825468383361
Experience 12, Iter 70, disc loss: 0.0023744643899564037, policy loss: 7.144925155481658
Experience 12, Iter 71, disc loss: 0.0023653150509566675, policy loss: 7.051070504748587
Experience 12, Iter 72, disc loss: 0.0019273360048781007, policy loss: 7.671292068552817
Experience 12, Iter 73, disc loss: 0.0022206208852909325, policy loss: 7.193109168334047
Experience 12, Iter 74, disc loss: 0.002017206015616512, policy loss: 7.230897394978101
Experience 12, Iter 75, disc loss: 0.002102143282198819, policy loss: 7.37794784961117
Experience 12, Iter 76, disc loss: 0.0021538051704323635, policy loss: 7.00232986721778
Experience 12, Iter 77, disc loss: 0.003047975116521784, policy loss: 7.14359920762409
Experience 12, Iter 78, disc loss: 0.0018157639773844205, policy loss: 7.3928240464820565
Experience 12, Iter 79, disc loss: 0.0019688032917933975, policy loss: 7.140763447663212
Experience 12, Iter 80, disc loss: 0.002175385857052365, policy loss: 7.351233937063261
Experience 12, Iter 81, disc loss: 0.002049476559332952, policy loss: 7.227981565612529
Experience 12, Iter 82, disc loss: 0.0019648252505348966, policy loss: 7.279444132004441
Experience 12, Iter 83, disc loss: 0.002493128479512147, policy loss: 6.925230830810999
Experience 12, Iter 84, disc loss: 0.001987383773660904, policy loss: 7.3304441113374414
Experience 12, Iter 85, disc loss: 0.002322508655071344, policy loss: 7.175135995657994
Experience 12, Iter 86, disc loss: 0.0021960327119427705, policy loss: 7.238524593222403
Experience 12, Iter 87, disc loss: 0.002571127744037907, policy loss: 7.0589810633058265
Experience 12, Iter 88, disc loss: 0.002010918344517338, policy loss: 7.429619929346698
Experience 12, Iter 89, disc loss: 0.002068688675695791, policy loss: 7.227468482720288
Experience 12, Iter 90, disc loss: 0.0017450639404201556, policy loss: 7.503035380114876
Experience 12, Iter 91, disc loss: 0.0022966610938236053, policy loss: 7.187092032383772
Experience 12, Iter 92, disc loss: 0.0019505050787175795, policy loss: 7.506307502452432
Experience 12, Iter 93, disc loss: 0.0021566293803089182, policy loss: 7.322680264718476
Experience 12, Iter 94, disc loss: 0.0024985071302889906, policy loss: 7.119047998538262
Experience 12, Iter 95, disc loss: 0.002170123057682495, policy loss: 7.528020125906978
Experience 12, Iter 96, disc loss: 0.0020115046196450157, policy loss: 7.332779403064854
Experience 12, Iter 97, disc loss: 0.0023374863588331097, policy loss: 6.886348341362313
Experience 12, Iter 98, disc loss: 0.0016969736325474867, policy loss: 7.465536820274764
Experience 12, Iter 99, disc loss: 0.0014699306102557946, policy loss: 7.701615995152869
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0897],
        [1.0072],
        [0.0121]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0202, 0.1109, 0.6061, 0.0149, 0.0032, 2.0882]],

        [[0.0202, 0.1109, 0.6061, 0.0149, 0.0032, 2.0882]],

        [[0.0202, 0.1109, 0.6061, 0.0149, 0.0032, 2.0882]],

        [[0.0202, 0.1109, 0.6061, 0.0149, 0.0032, 2.0882]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0101, 0.3587, 4.0288, 0.0483], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0101, 0.3587, 4.0288, 0.0483])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.174
Iter 2/2000 - Loss: 2.172
Iter 3/2000 - Loss: 2.091
Iter 4/2000 - Loss: 2.005
Iter 5/2000 - Loss: 2.037
Iter 6/2000 - Loss: 2.029
Iter 7/2000 - Loss: 1.946
Iter 8/2000 - Loss: 1.887
Iter 9/2000 - Loss: 1.869
Iter 10/2000 - Loss: 1.824
Iter 11/2000 - Loss: 1.737
Iter 12/2000 - Loss: 1.645
Iter 13/2000 - Loss: 1.561
Iter 14/2000 - Loss: 1.466
Iter 15/2000 - Loss: 1.342
Iter 16/2000 - Loss: 1.192
Iter 17/2000 - Loss: 1.025
Iter 18/2000 - Loss: 0.845
Iter 19/2000 - Loss: 0.649
Iter 20/2000 - Loss: 0.434
Iter 1981/2000 - Loss: -7.653
Iter 1982/2000 - Loss: -7.653
Iter 1983/2000 - Loss: -7.653
Iter 1984/2000 - Loss: -7.653
Iter 1985/2000 - Loss: -7.654
Iter 1986/2000 - Loss: -7.654
Iter 1987/2000 - Loss: -7.654
Iter 1988/2000 - Loss: -7.654
Iter 1989/2000 - Loss: -7.654
Iter 1990/2000 - Loss: -7.654
Iter 1991/2000 - Loss: -7.654
Iter 1992/2000 - Loss: -7.654
Iter 1993/2000 - Loss: -7.654
Iter 1994/2000 - Loss: -7.654
Iter 1995/2000 - Loss: -7.654
Iter 1996/2000 - Loss: -7.654
Iter 1997/2000 - Loss: -7.654
Iter 1998/2000 - Loss: -7.654
Iter 1999/2000 - Loss: -7.654
Iter 2000/2000 - Loss: -7.654
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[15.4112,  2.9667, 41.9388,  8.7015, 10.6700, 44.5052]],

        [[20.3754, 34.5424, 11.4535,  1.1468,  2.7264, 31.4689]],

        [[20.9070, 36.9473,  9.3861,  0.9774,  2.0600, 19.2584]],

        [[17.2604, 30.4578, 15.7549,  4.0322,  1.0132, 33.0528]]])
Signal Variance: tensor([ 0.0439,  2.3184, 13.3663,  0.3475])
Estimated target variance: tensor([0.0101, 0.3587, 4.0288, 0.0483])
N: 130
Signal to noise ratio: tensor([11.3543, 79.1547, 81.5426, 35.7677])
Bound on condition number: tensor([ 16760.5742, 814511.2084, 864397.2316, 166313.9570])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0018578898775924368, policy loss: 7.543241515732589
Experience 13, Iter 1, disc loss: 0.001882948375201008, policy loss: 7.565922948269911
Experience 13, Iter 2, disc loss: 0.002161483793743324, policy loss: 7.350972778928385
Experience 13, Iter 3, disc loss: 0.0020130463731431614, policy loss: 7.231104905334711
Experience 13, Iter 4, disc loss: 0.0021146473109320972, policy loss: 7.205280858558818
Experience 13, Iter 5, disc loss: 0.0019599676060806067, policy loss: 7.467689201609063
Experience 13, Iter 6, disc loss: 0.0012556556184880466, policy loss: 7.893983354605067
Experience 13, Iter 7, disc loss: 0.002013059204305359, policy loss: 7.471166905158418
Experience 13, Iter 8, disc loss: 0.0014719728533582415, policy loss: 7.8693975889326575
Experience 13, Iter 9, disc loss: 0.0016959460934084273, policy loss: 7.423296255372929
Experience 13, Iter 10, disc loss: 0.0018283865897057048, policy loss: 7.4759445038084955
Experience 13, Iter 11, disc loss: 0.0019528645646719974, policy loss: 7.132608462197952
Experience 13, Iter 12, disc loss: 0.0016372087618699534, policy loss: 7.821064372419849
Experience 13, Iter 13, disc loss: 0.001620653012315979, policy loss: 7.676379236470106
Experience 13, Iter 14, disc loss: 0.0016818384575095651, policy loss: 7.846889556973899
Experience 13, Iter 15, disc loss: 0.0019121866913340457, policy loss: 7.6757230944814845
Experience 13, Iter 16, disc loss: 0.0014658344641875526, policy loss: 7.856393780257239
Experience 13, Iter 17, disc loss: 0.0011883060089415546, policy loss: 8.177147207059527
Experience 13, Iter 18, disc loss: 0.0015980720521596332, policy loss: 8.05748302227956
Experience 13, Iter 19, disc loss: 0.001213258046984692, policy loss: 8.112021808025274
Experience 13, Iter 20, disc loss: 0.0012406490425030665, policy loss: 8.200278662871593
Experience 13, Iter 21, disc loss: 0.0011060852248970091, policy loss: 8.387757460775626
Experience 13, Iter 22, disc loss: 0.0014264292514331036, policy loss: 7.8059665151213125
Experience 13, Iter 23, disc loss: 0.001106636678285887, policy loss: 8.440381302448632
Experience 13, Iter 24, disc loss: 0.0016172086555392292, policy loss: 7.659668585766443
Experience 13, Iter 25, disc loss: 0.0014523788982072363, policy loss: 7.807488187627181
Experience 13, Iter 26, disc loss: 0.0015483533748449864, policy loss: 7.679429684933978
Experience 13, Iter 27, disc loss: 0.0014287591554627294, policy loss: 7.77999934719489
Experience 13, Iter 28, disc loss: 0.0015304699053262363, policy loss: 7.653360824698364
Experience 13, Iter 29, disc loss: 0.0015157732762618916, policy loss: 7.610555976341811
Experience 13, Iter 30, disc loss: 0.00182767347127408, policy loss: 7.679655292963394
Experience 13, Iter 31, disc loss: 0.001351781612414098, policy loss: 7.751700335912798
Experience 13, Iter 32, disc loss: 0.0015893066782461918, policy loss: 7.583292209272896
Experience 13, Iter 33, disc loss: 0.0014746902910090264, policy loss: 7.923994137769693
Experience 13, Iter 34, disc loss: 0.0013239846081431065, policy loss: 7.833931689377042
Experience 13, Iter 35, disc loss: 0.0014177765341967157, policy loss: 7.781127736786889
Experience 13, Iter 36, disc loss: 0.0016736701750145837, policy loss: 7.5958394291793985
Experience 13, Iter 37, disc loss: 0.001288641834199332, policy loss: 7.950754551872564
Experience 13, Iter 38, disc loss: 0.0019282079859962082, policy loss: 7.709465964995167
Experience 13, Iter 39, disc loss: 0.0011925947759576908, policy loss: 8.104147707939315
Experience 13, Iter 40, disc loss: 0.0013405873500394148, policy loss: 7.879665772282483
Experience 13, Iter 41, disc loss: 0.0012564584963754735, policy loss: 7.997574824741783
Experience 13, Iter 42, disc loss: 0.0014272325812365453, policy loss: 7.889172080789714
Experience 13, Iter 43, disc loss: 0.0012023412511511103, policy loss: 8.134611149482394
Experience 13, Iter 44, disc loss: 0.0016215263340848556, policy loss: 7.63373138794798
Experience 13, Iter 45, disc loss: 0.0016395500406541947, policy loss: 7.839328429681007
Experience 13, Iter 46, disc loss: 0.001229970591140746, policy loss: 8.149491851162413
Experience 13, Iter 47, disc loss: 0.0017353213434268672, policy loss: 7.6541277696728
Experience 13, Iter 48, disc loss: 0.0014029723143475343, policy loss: 7.7360070447809415
Experience 13, Iter 49, disc loss: 0.0015124062618936487, policy loss: 7.750414955085704
Experience 13, Iter 50, disc loss: 0.0017187663523196169, policy loss: 7.511855656749182
Experience 13, Iter 51, disc loss: 0.0013339021690324812, policy loss: 7.701858752002003
Experience 13, Iter 52, disc loss: 0.0014440538719711657, policy loss: 7.682212497867118
Experience 13, Iter 53, disc loss: 0.0017850902592206788, policy loss: 7.746023692897527
Experience 13, Iter 54, disc loss: 0.0016445658726432408, policy loss: 7.664084874107372
Experience 13, Iter 55, disc loss: 0.0013175792039843153, policy loss: 7.888223354009691
Experience 13, Iter 56, disc loss: 0.0014907833585331204, policy loss: 7.964066284783954
Experience 13, Iter 57, disc loss: 0.001554955730254727, policy loss: 7.626360755891359
Experience 13, Iter 58, disc loss: 0.0012816580086656777, policy loss: 7.902625833254347
Experience 13, Iter 59, disc loss: 0.0014181889870701226, policy loss: 7.621863254255718
Experience 13, Iter 60, disc loss: 0.0015619347585349224, policy loss: 7.904949503870934
Experience 13, Iter 61, disc loss: 0.001198426608544858, policy loss: 8.102408401276186
Experience 13, Iter 62, disc loss: 0.0014334615202952633, policy loss: 7.645315991971964
Experience 13, Iter 63, disc loss: 0.0015223178618686056, policy loss: 7.653539645556969
Experience 13, Iter 64, disc loss: 0.001893724553853807, policy loss: 7.666298977793723
Experience 13, Iter 65, disc loss: 0.0015050091484666718, policy loss: 7.6820161808766665
Experience 13, Iter 66, disc loss: 0.0013309929170978914, policy loss: 7.920445426108425
Experience 13, Iter 67, disc loss: 0.0017827450596901483, policy loss: 7.451457038554618
Experience 13, Iter 68, disc loss: 0.0012852593408277393, policy loss: 7.87441684667776
Experience 13, Iter 69, disc loss: 0.001467279264222176, policy loss: 7.986098781783515
Experience 13, Iter 70, disc loss: 0.0013791324226442474, policy loss: 7.773045759244294
Experience 13, Iter 71, disc loss: 0.0018630974344304154, policy loss: 7.551052712420558
Experience 13, Iter 72, disc loss: 0.0020558778736229242, policy loss: 7.464851804866557
Experience 13, Iter 73, disc loss: 0.0014100660906390867, policy loss: 8.035826926274547
Experience 13, Iter 74, disc loss: 0.0015434558262445918, policy loss: 7.755026146909695
Experience 13, Iter 75, disc loss: 0.0013006262443651688, policy loss: 7.99073207734858
Experience 13, Iter 76, disc loss: 0.0013796561511435009, policy loss: 7.794178672892526
Experience 13, Iter 77, disc loss: 0.0013809517937202343, policy loss: 7.957074790121828
Experience 13, Iter 78, disc loss: 0.0012715025541566135, policy loss: 7.761467717065438
Experience 13, Iter 79, disc loss: 0.0015327407646796702, policy loss: 7.512824467402071
Experience 13, Iter 80, disc loss: 0.0013740543838113438, policy loss: 7.829987712846054
Experience 13, Iter 81, disc loss: 0.0014728087707469105, policy loss: 7.91784099128658
Experience 13, Iter 82, disc loss: 0.0018004417132698508, policy loss: 7.603950414538479
Experience 13, Iter 83, disc loss: 0.0015190704137028584, policy loss: 7.61068742436424
Experience 13, Iter 84, disc loss: 0.0016174564498925477, policy loss: 7.642876765808239
Experience 13, Iter 85, disc loss: 0.0014313929803502663, policy loss: 7.757561246474189
Experience 13, Iter 86, disc loss: 0.0013619348831835183, policy loss: 7.816898993601492
Experience 13, Iter 87, disc loss: 0.0016756324271136331, policy loss: 7.660585235054224
Experience 13, Iter 88, disc loss: 0.001569767989301832, policy loss: 7.687916196753674
Experience 13, Iter 89, disc loss: 0.0016432208180120634, policy loss: 7.706437348192636
Experience 13, Iter 90, disc loss: 0.0017177781422821109, policy loss: 7.497056434236378
Experience 13, Iter 91, disc loss: 0.0019104152519766203, policy loss: 7.503532388474671
Experience 13, Iter 92, disc loss: 0.0014499757588276976, policy loss: 7.818878799968786
Experience 13, Iter 93, disc loss: 0.0015379386032858043, policy loss: 7.595509499569685
Experience 13, Iter 94, disc loss: 0.0012571750146476166, policy loss: 8.046460104649
Experience 13, Iter 95, disc loss: 0.0016109602179647294, policy loss: 7.490576416809453
Experience 13, Iter 96, disc loss: 0.0014010963188920305, policy loss: 7.618115671002799
Experience 13, Iter 97, disc loss: 0.0016293929870840986, policy loss: 7.521769203318952
Experience 13, Iter 98, disc loss: 0.00196245913975012, policy loss: 7.357126514494575
Experience 13, Iter 99, disc loss: 0.002028711771550685, policy loss: 7.4591242781120455
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.0934],
        [1.0273],
        [0.0117]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0188, 0.1055, 0.5915, 0.0145, 0.0030, 2.1517]],

        [[0.0188, 0.1055, 0.5915, 0.0145, 0.0030, 2.1517]],

        [[0.0188, 0.1055, 0.5915, 0.0145, 0.0030, 2.1517]],

        [[0.0188, 0.1055, 0.5915, 0.0145, 0.0030, 2.1517]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0096, 0.3735, 4.1093, 0.0467], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0096, 0.3735, 4.1093, 0.0467])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.163
Iter 2/2000 - Loss: 2.180
Iter 3/2000 - Loss: 2.082
Iter 4/2000 - Loss: 2.002
Iter 5/2000 - Loss: 2.041
Iter 6/2000 - Loss: 2.029
Iter 7/2000 - Loss: 1.945
Iter 8/2000 - Loss: 1.890
Iter 9/2000 - Loss: 1.874
Iter 10/2000 - Loss: 1.830
Iter 11/2000 - Loss: 1.742
Iter 12/2000 - Loss: 1.650
Iter 13/2000 - Loss: 1.566
Iter 14/2000 - Loss: 1.470
Iter 15/2000 - Loss: 1.341
Iter 16/2000 - Loss: 1.185
Iter 17/2000 - Loss: 1.011
Iter 18/2000 - Loss: 0.823
Iter 19/2000 - Loss: 0.621
Iter 20/2000 - Loss: 0.401
Iter 1981/2000 - Loss: -7.735
Iter 1982/2000 - Loss: -7.735
Iter 1983/2000 - Loss: -7.735
Iter 1984/2000 - Loss: -7.735
Iter 1985/2000 - Loss: -7.735
Iter 1986/2000 - Loss: -7.735
Iter 1987/2000 - Loss: -7.735
Iter 1988/2000 - Loss: -7.735
Iter 1989/2000 - Loss: -7.735
Iter 1990/2000 - Loss: -7.735
Iter 1991/2000 - Loss: -7.735
Iter 1992/2000 - Loss: -7.735
Iter 1993/2000 - Loss: -7.735
Iter 1994/2000 - Loss: -7.735
Iter 1995/2000 - Loss: -7.735
Iter 1996/2000 - Loss: -7.736
Iter 1997/2000 - Loss: -7.736
Iter 1998/2000 - Loss: -7.736
Iter 1999/2000 - Loss: -7.736
Iter 2000/2000 - Loss: -7.736
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.1462,  2.7929, 40.4638,  8.4648, 10.3887, 43.9644]],

        [[20.1622, 34.3747, 11.5177,  1.1585,  2.4913, 30.9256]],

        [[19.4972, 34.9794,  9.6740,  0.9825,  1.7781, 18.4007]],

        [[16.8261, 29.4810, 15.8934,  3.8290,  1.0000, 32.3100]]])
Signal Variance: tensor([ 0.0409,  2.3035, 12.7829,  0.3509])
Estimated target variance: tensor([0.0096, 0.3735, 4.1093, 0.0467])
N: 140
Signal to noise ratio: tensor([11.0208, 79.2115, 76.3593, 36.1569])
Bound on condition number: tensor([ 17005.1355, 878425.0753, 816304.4896, 183025.5734])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0014618075313704284, policy loss: 7.5946718577872545
Experience 14, Iter 1, disc loss: 0.001166391935355767, policy loss: 7.961621327398269
Experience 14, Iter 2, disc loss: 0.001198855138518349, policy loss: 8.10874808374629
Experience 14, Iter 3, disc loss: 0.0015439674066204628, policy loss: 7.923197391448518
Experience 14, Iter 4, disc loss: 0.0014073449617911854, policy loss: 7.841062529329984
Experience 14, Iter 5, disc loss: 0.0016828855493281885, policy loss: 7.83857355535374
Experience 14, Iter 6, disc loss: 0.0015728057558664801, policy loss: 7.782524644091379
Experience 14, Iter 7, disc loss: 0.0018365162644012167, policy loss: 7.769748410449239
Experience 14, Iter 8, disc loss: 0.0017248141423685673, policy loss: 7.755743961282164
Experience 14, Iter 9, disc loss: 0.0013790316033643899, policy loss: 7.660015859479049
Experience 14, Iter 10, disc loss: 0.0015199604964551729, policy loss: 7.760996801421152
Experience 14, Iter 11, disc loss: 0.001353182159130498, policy loss: 7.889126553966906
Experience 14, Iter 12, disc loss: 0.0012872800824436, policy loss: 8.030778321567052
Experience 14, Iter 13, disc loss: 0.0013334286882900207, policy loss: 8.145421233439805
Experience 14, Iter 14, disc loss: 0.0016752986895923914, policy loss: 7.833278038789936
Experience 14, Iter 15, disc loss: 0.001623962610824584, policy loss: 7.502220636899946
Experience 14, Iter 16, disc loss: 0.0013226284812017117, policy loss: 7.915532201943385
Experience 14, Iter 17, disc loss: 0.0013263289295602, policy loss: 7.937307778620994
Experience 14, Iter 18, disc loss: 0.0017512275342414247, policy loss: 7.642888959115762
Experience 14, Iter 19, disc loss: 0.0014502134457413745, policy loss: 7.873238540270715
Experience 14, Iter 20, disc loss: 0.0014627711106297641, policy loss: 7.862495849667995
Experience 14, Iter 21, disc loss: 0.0015251277295193861, policy loss: 7.7650625653108705
Experience 14, Iter 22, disc loss: 0.001534660361557812, policy loss: 7.6931061290695135
Experience 14, Iter 23, disc loss: 0.001449130731295093, policy loss: 7.887879449320289
Experience 14, Iter 24, disc loss: 0.0012987129428770846, policy loss: 8.0002657725414
Experience 14, Iter 25, disc loss: 0.001451378253078212, policy loss: 7.748864136580201
Experience 14, Iter 26, disc loss: 0.0013824061203996985, policy loss: 7.847000941798628
Experience 14, Iter 27, disc loss: 0.0012726206971754301, policy loss: 8.273196587977313
Experience 14, Iter 28, disc loss: 0.0013889470412449078, policy loss: 7.668130948021084
Experience 14, Iter 29, disc loss: 0.00148149963162022, policy loss: 7.69594835518627
Experience 14, Iter 30, disc loss: 0.0011397934077578276, policy loss: 8.216689795877539
Experience 14, Iter 31, disc loss: 0.002065153193429416, policy loss: 7.5517954400058915
Experience 14, Iter 32, disc loss: 0.0014365858222189416, policy loss: 7.896992026488514
Experience 14, Iter 33, disc loss: 0.0015294019348413932, policy loss: 7.930860966098333
Experience 14, Iter 34, disc loss: 0.0013744717247609842, policy loss: 8.050689249145577
Experience 14, Iter 35, disc loss: 0.0014314933997992799, policy loss: 7.791804590016903
Experience 14, Iter 36, disc loss: 0.001533114780453745, policy loss: 7.807708942998557
Experience 14, Iter 37, disc loss: 0.0018119552227263283, policy loss: 7.596610705564311
Experience 14, Iter 38, disc loss: 0.0009996579556423497, policy loss: 8.253231716425693
Experience 14, Iter 39, disc loss: 0.0008666813696362323, policy loss: 8.719435992668704
Experience 14, Iter 40, disc loss: 0.0013808181414840812, policy loss: 7.7370881281033
Experience 14, Iter 41, disc loss: 0.0011178779155680482, policy loss: 8.096324980362517
Experience 14, Iter 42, disc loss: 0.00149613195478901, policy loss: 7.953215303965267
Experience 14, Iter 43, disc loss: 0.0013786650309543205, policy loss: 7.772555178474222
Experience 14, Iter 44, disc loss: 0.0016320301780782967, policy loss: 7.710649348544181
Experience 14, Iter 45, disc loss: 0.0015891580409908082, policy loss: 7.877337177660442
Experience 14, Iter 46, disc loss: 0.0015375023300407795, policy loss: 7.995592314708237
Experience 14, Iter 47, disc loss: 0.001308787095470354, policy loss: 8.0520407208532
Experience 14, Iter 48, disc loss: 0.0014660461377416804, policy loss: 7.8596174304306246
Experience 14, Iter 49, disc loss: 0.0014929831710631392, policy loss: 7.790206130408618
Experience 14, Iter 50, disc loss: 0.0013237993831437958, policy loss: 7.88965526197615
Experience 14, Iter 51, disc loss: 0.0013341966920793684, policy loss: 7.792297351648834
Experience 14, Iter 52, disc loss: 0.001364075496824465, policy loss: 7.79301702441084
Experience 14, Iter 53, disc loss: 0.0015653148544514267, policy loss: 7.675476099983814
Experience 14, Iter 54, disc loss: 0.0013722210677901514, policy loss: 7.920052399918896
Experience 14, Iter 55, disc loss: 0.0019977848695884536, policy loss: 7.681776988404701
Experience 14, Iter 56, disc loss: 0.0016509033482235921, policy loss: 8.034831179822095
Experience 14, Iter 57, disc loss: 0.0011505735376106353, policy loss: 8.200244158214234
Experience 14, Iter 58, disc loss: 0.0009578016070246984, policy loss: 8.478402524002037
Experience 14, Iter 59, disc loss: 0.0008321202445899451, policy loss: 8.398031349125645
Experience 14, Iter 60, disc loss: 0.001308491541525771, policy loss: 8.001549177431757
Experience 14, Iter 61, disc loss: 0.0011120410835175633, policy loss: 7.989218541928845
Experience 14, Iter 62, disc loss: 0.0011213893746969553, policy loss: 7.999622694538457
Experience 14, Iter 63, disc loss: 0.0012337675281060092, policy loss: 7.589116178931112
Experience 14, Iter 64, disc loss: 0.0014381439903874073, policy loss: 7.990853186202081
Experience 14, Iter 65, disc loss: 0.0014698162958890727, policy loss: 7.745889364524025
Experience 14, Iter 66, disc loss: 0.0011086107006625249, policy loss: 7.949961989830193
Experience 14, Iter 67, disc loss: 0.001286458482096644, policy loss: 7.8776397838221826
Experience 14, Iter 68, disc loss: 0.0013916338368482025, policy loss: 7.575188918371
Experience 14, Iter 69, disc loss: 0.001337449653829704, policy loss: 7.67387606072096
Experience 14, Iter 70, disc loss: 0.0010847932684869672, policy loss: 8.223400953390195
Experience 14, Iter 71, disc loss: 0.0012972723436815554, policy loss: 7.901230923284146
Experience 14, Iter 72, disc loss: 0.0013187433195421508, policy loss: 7.98482495092054
Experience 14, Iter 73, disc loss: 0.0012678596447115412, policy loss: 7.872648551444046
Experience 14, Iter 74, disc loss: 0.0013072808506803773, policy loss: 7.672890981970169
Experience 14, Iter 75, disc loss: 0.0012620913890066309, policy loss: 7.837752139571755
Experience 14, Iter 76, disc loss: 0.0016159775125697351, policy loss: 7.576538063218672
Experience 14, Iter 77, disc loss: 0.0012353604276617099, policy loss: 7.6077720497690695
Experience 14, Iter 78, disc loss: 0.0013213911390518621, policy loss: 7.919556113130072
Experience 14, Iter 79, disc loss: 0.0013663938612037978, policy loss: 7.557100236145699
Experience 14, Iter 80, disc loss: 0.0011533104182223286, policy loss: 7.720397010457554
Experience 14, Iter 81, disc loss: 0.0015661279297553199, policy loss: 7.626222621742241
Experience 14, Iter 82, disc loss: 0.0015376956214754273, policy loss: 7.776505765645609
Experience 14, Iter 83, disc loss: 0.0013362432319083632, policy loss: 7.552643974744877
Experience 14, Iter 84, disc loss: 0.0012053373795671406, policy loss: 7.858838122307471
Experience 14, Iter 85, disc loss: 0.0012002317004514262, policy loss: 7.841937150690994
Experience 14, Iter 86, disc loss: 0.0013160571974314892, policy loss: 7.973778507303781
Experience 14, Iter 87, disc loss: 0.0014425938645393353, policy loss: 7.587324943699111
Experience 14, Iter 88, disc loss: 0.001781694588116095, policy loss: 7.827984090836297
Experience 14, Iter 89, disc loss: 0.0013454320068386255, policy loss: 7.902845823450587
Experience 14, Iter 90, disc loss: 0.0021796454571639525, policy loss: 7.691099368446675
Experience 14, Iter 91, disc loss: 0.0011658290175667698, policy loss: 8.172362648819487
Experience 14, Iter 92, disc loss: 0.0011739888830813662, policy loss: 7.954285888220086
Experience 14, Iter 93, disc loss: 0.0011528363404279813, policy loss: 7.7170495854434265
Experience 14, Iter 94, disc loss: 0.0010835210641205648, policy loss: 7.961450006745741
Experience 14, Iter 95, disc loss: 0.0011352545045568482, policy loss: 7.907356011604405
Experience 14, Iter 96, disc loss: 0.0016284259697590035, policy loss: 7.607042174859391
Experience 14, Iter 97, disc loss: 0.0018435305857518757, policy loss: 7.12675261753188
Experience 14, Iter 98, disc loss: 0.0012176406288642162, policy loss: 7.841800591754475
Experience 14, Iter 99, disc loss: 0.0013360713970019945, policy loss: 7.874268622440326
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0887],
        [0.9796],
        [0.0110]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0177, 0.0993, 0.5586, 0.0137, 0.0028, 2.0397]],

        [[0.0177, 0.0993, 0.5586, 0.0137, 0.0028, 2.0397]],

        [[0.0177, 0.0993, 0.5586, 0.0137, 0.0028, 2.0397]],

        [[0.0177, 0.0993, 0.5586, 0.0137, 0.0028, 2.0397]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0090, 0.3549, 3.9184, 0.0440], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0090, 0.3549, 3.9184, 0.0440])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.061
Iter 2/2000 - Loss: 2.091
Iter 3/2000 - Loss: 1.981
Iter 4/2000 - Loss: 1.902
Iter 5/2000 - Loss: 1.948
Iter 6/2000 - Loss: 1.937
Iter 7/2000 - Loss: 1.853
Iter 8/2000 - Loss: 1.799
Iter 9/2000 - Loss: 1.790
Iter 10/2000 - Loss: 1.753
Iter 11/2000 - Loss: 1.669
Iter 12/2000 - Loss: 1.580
Iter 13/2000 - Loss: 1.504
Iter 14/2000 - Loss: 1.416
Iter 15/2000 - Loss: 1.294
Iter 16/2000 - Loss: 1.143
Iter 17/2000 - Loss: 0.975
Iter 18/2000 - Loss: 0.793
Iter 19/2000 - Loss: 0.594
Iter 20/2000 - Loss: 0.375
Iter 1981/2000 - Loss: -7.847
Iter 1982/2000 - Loss: -7.847
Iter 1983/2000 - Loss: -7.847
Iter 1984/2000 - Loss: -7.847
Iter 1985/2000 - Loss: -7.847
Iter 1986/2000 - Loss: -7.847
Iter 1987/2000 - Loss: -7.847
Iter 1988/2000 - Loss: -7.847
Iter 1989/2000 - Loss: -7.847
Iter 1990/2000 - Loss: -7.847
Iter 1991/2000 - Loss: -7.847
Iter 1992/2000 - Loss: -7.847
Iter 1993/2000 - Loss: -7.847
Iter 1994/2000 - Loss: -7.847
Iter 1995/2000 - Loss: -7.847
Iter 1996/2000 - Loss: -7.847
Iter 1997/2000 - Loss: -7.847
Iter 1998/2000 - Loss: -7.847
Iter 1999/2000 - Loss: -7.847
Iter 2000/2000 - Loss: -7.847
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.1805,  2.8235, 39.9886,  8.3407, 10.0626, 43.9584]],

        [[19.6017, 33.2501, 11.3163,  1.1863,  2.3859, 31.6092]],

        [[18.3758, 33.7742,  9.7734,  0.9821,  1.7358, 18.1842]],

        [[16.2239, 28.8847, 16.2043,  3.8173,  0.9962, 30.5143]]])
Signal Variance: tensor([ 0.0413,  2.3960, 12.8095,  0.3648])
Estimated target variance: tensor([0.0090, 0.3549, 3.9184, 0.0440])
N: 150
Signal to noise ratio: tensor([11.0741, 80.7687, 77.0192, 37.5604])
Bound on condition number: tensor([ 18396.3175, 978537.4338, 889793.8449, 211618.3342])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0014610532333639483, policy loss: 7.666715750021144
Experience 15, Iter 1, disc loss: 0.0017216950104977317, policy loss: 7.712003629016812
Experience 15, Iter 2, disc loss: 0.0011147214250916205, policy loss: 8.198438541888141
Experience 15, Iter 3, disc loss: 0.0011562168854643323, policy loss: 7.877995825447431
Experience 15, Iter 4, disc loss: 0.001709804630966454, policy loss: 7.947483701829626
Experience 15, Iter 5, disc loss: 0.0019907300073823734, policy loss: 7.695712938225409
Experience 15, Iter 6, disc loss: 0.0011297058177978875, policy loss: 7.963157892353332
Experience 15, Iter 7, disc loss: 0.0014356345793736774, policy loss: 7.679160446021783
Experience 15, Iter 8, disc loss: 0.0019739472419106146, policy loss: 7.35934890842899
Experience 15, Iter 9, disc loss: 0.001482443946155605, policy loss: 7.719075384625425
Experience 15, Iter 10, disc loss: 0.0019170630097175446, policy loss: 7.482783322883856
Experience 15, Iter 11, disc loss: 0.0014755213622364822, policy loss: 7.795859097392392
Experience 15, Iter 12, disc loss: 0.0011264850408452681, policy loss: 8.092499148280028
Experience 15, Iter 13, disc loss: 0.0010034772871646198, policy loss: 8.659075484871899
Experience 15, Iter 14, disc loss: 0.0010266041565100041, policy loss: 8.490021669666316
Experience 15, Iter 15, disc loss: 0.0010520289141396062, policy loss: 8.351022716058488
Experience 15, Iter 16, disc loss: 0.0010927413879973772, policy loss: 7.99534766024161
Experience 15, Iter 17, disc loss: 0.0012372244600195833, policy loss: 7.6620545689513815
Experience 15, Iter 18, disc loss: 0.0011975889237078067, policy loss: 8.175487937089827
Experience 15, Iter 19, disc loss: 0.0009288801490033559, policy loss: 8.590097675307556
Experience 15, Iter 20, disc loss: 0.000963095391824692, policy loss: 8.38892698463364
Experience 15, Iter 21, disc loss: 0.0011316892785596324, policy loss: 8.322367262579908
Experience 15, Iter 22, disc loss: 0.001501410821537515, policy loss: 8.221229185190829
Experience 15, Iter 23, disc loss: 0.0012842617445844849, policy loss: 8.103906343948442
Experience 15, Iter 24, disc loss: 0.0011214381468803794, policy loss: 7.822307173715697
Experience 15, Iter 25, disc loss: 0.0015373329477692326, policy loss: 7.805251184433329
Experience 15, Iter 26, disc loss: 0.001113141953930388, policy loss: 7.877153486437554
Experience 15, Iter 27, disc loss: 0.001327379803274598, policy loss: 8.001106992943166
Experience 15, Iter 28, disc loss: 0.0013088906927028593, policy loss: 7.902549775755335
Experience 15, Iter 29, disc loss: 0.001330518834703409, policy loss: 8.098552088330809
Experience 15, Iter 30, disc loss: 0.0009424183054236187, policy loss: 8.002337858479642
Experience 15, Iter 31, disc loss: 0.001098163696102628, policy loss: 8.775667988350142
Experience 15, Iter 32, disc loss: 0.0010433631940124921, policy loss: 8.002846836270512
Experience 15, Iter 33, disc loss: 0.0010150166020936808, policy loss: 8.424245923770712
Experience 15, Iter 34, disc loss: 0.0009816177528760676, policy loss: 8.3727831039898
Experience 15, Iter 35, disc loss: 0.0008897797361013053, policy loss: 8.255769140919037
Experience 15, Iter 36, disc loss: 0.00116968915130812, policy loss: 8.203156305532435
Experience 15, Iter 37, disc loss: 0.0011451255634888722, policy loss: 7.911823857265853
Experience 15, Iter 38, disc loss: 0.0010773646819603883, policy loss: 8.314746826848893
Experience 15, Iter 39, disc loss: 0.0011763138535442917, policy loss: 8.21412879511408
Experience 15, Iter 40, disc loss: 0.0009535717674010193, policy loss: 8.215873683264633
Experience 15, Iter 41, disc loss: 0.000743524436124091, policy loss: 8.669345765759754
Experience 15, Iter 42, disc loss: 0.0007456157630222291, policy loss: 9.088136892470345
Experience 15, Iter 43, disc loss: 0.0008688763072203017, policy loss: 8.455492756688379
Experience 15, Iter 44, disc loss: 0.0010076361938215932, policy loss: 8.21255147025833
Experience 15, Iter 45, disc loss: 0.0012736323562721808, policy loss: 7.827695417183352
Experience 15, Iter 46, disc loss: 0.0013331744641163525, policy loss: 8.068059601979652
Experience 15, Iter 47, disc loss: 0.0008548225537312146, policy loss: 8.617168641791313
Experience 15, Iter 48, disc loss: 0.0006904472938899973, policy loss: 9.082946344834252
Experience 15, Iter 49, disc loss: 0.0007086581601684368, policy loss: 9.161559584793721
Experience 15, Iter 50, disc loss: 0.0005898214338367525, policy loss: 9.33606567772322
Experience 15, Iter 51, disc loss: 0.0009847101445924463, policy loss: 8.579019660375343
Experience 15, Iter 52, disc loss: 0.0006604220160709032, policy loss: 8.920028202133324
Experience 15, Iter 53, disc loss: 0.0008074233449804549, policy loss: 8.906802656203947
Experience 15, Iter 54, disc loss: 0.0007444812195601983, policy loss: 9.002444640861587
Experience 15, Iter 55, disc loss: 0.0006571045665059712, policy loss: 8.792533713294182
Experience 15, Iter 56, disc loss: 0.0007514657272844891, policy loss: 8.582863232119237
Experience 15, Iter 57, disc loss: 0.0008908714765307616, policy loss: 8.48577473929405
Experience 15, Iter 58, disc loss: 0.0009458301092958631, policy loss: 8.139304354939268
Experience 15, Iter 59, disc loss: 0.0010619722802258224, policy loss: 7.843963069091245
Experience 15, Iter 60, disc loss: 0.0012754753975409584, policy loss: 7.9056411284440475
Experience 15, Iter 61, disc loss: 0.0011916393411842245, policy loss: 8.071619627052979
Experience 15, Iter 62, disc loss: 0.000970711004690722, policy loss: 8.383963112066724
Experience 15, Iter 63, disc loss: 0.000929241718935005, policy loss: 8.225016468015914
Experience 15, Iter 64, disc loss: 0.0009284195037505504, policy loss: 8.479852591436625
Experience 15, Iter 65, disc loss: 0.0009291435130638415, policy loss: 8.321479046794703
Experience 15, Iter 66, disc loss: 0.0008239418438652686, policy loss: 8.35422926884849
Experience 15, Iter 67, disc loss: 0.0008451504598578339, policy loss: 8.463022398964561
Experience 15, Iter 68, disc loss: 0.0005981424296173174, policy loss: 9.852393284155347
Experience 15, Iter 69, disc loss: 0.0008561829753717092, policy loss: 8.39633932430786
Experience 15, Iter 70, disc loss: 0.0013179209122051948, policy loss: 7.83208030205989
Experience 15, Iter 71, disc loss: 0.0012570365494200663, policy loss: 7.649446940824567
Experience 15, Iter 72, disc loss: 0.0015643919386513089, policy loss: 7.8121394625527065
Experience 15, Iter 73, disc loss: 0.0010388423162618159, policy loss: 7.924721592326528
Experience 15, Iter 74, disc loss: 0.0007708992351132628, policy loss: 8.511177719531645
Experience 15, Iter 75, disc loss: 0.0007383099437713034, policy loss: 8.789041042706454
Experience 15, Iter 76, disc loss: 0.0007633208139794616, policy loss: 8.980006064189322
Experience 15, Iter 77, disc loss: 0.0005768400657551799, policy loss: 8.891044560315859
Experience 15, Iter 78, disc loss: 0.0008014834001687586, policy loss: 8.782646405055964
Experience 15, Iter 79, disc loss: 0.0006069934450837703, policy loss: 9.311056802292333
Experience 15, Iter 80, disc loss: 0.00075033638375894, policy loss: 9.030768368391886
Experience 15, Iter 81, disc loss: 0.0005422719449989118, policy loss: 9.742216657525944
Experience 15, Iter 82, disc loss: 0.0012242701234979696, policy loss: 8.753820450365064
Experience 15, Iter 83, disc loss: 0.0008004420844937495, policy loss: 8.798182332786126
Experience 15, Iter 84, disc loss: 0.0007201022602681136, policy loss: 8.817102866832842
Experience 15, Iter 85, disc loss: 0.0007529231310498174, policy loss: 9.064911104169388
Experience 15, Iter 86, disc loss: 0.0009118143698485045, policy loss: 8.794248538626151
Experience 15, Iter 87, disc loss: 0.00080219702398426, policy loss: 8.284782305428529
Experience 15, Iter 88, disc loss: 0.0008824343128850982, policy loss: 8.236954152013972
Experience 15, Iter 89, disc loss: 0.0014901088598140604, policy loss: 8.074862663866185
Experience 15, Iter 90, disc loss: 0.000905784324974736, policy loss: 8.013678860768271
Experience 15, Iter 91, disc loss: 0.0011272163264879028, policy loss: 7.948518183266262
Experience 15, Iter 92, disc loss: 0.0021819009799454595, policy loss: 8.011510229320383
Experience 15, Iter 93, disc loss: 0.0011369386037821897, policy loss: 8.31014367099407
Experience 15, Iter 94, disc loss: 0.0009879606128818785, policy loss: 8.181042228910021
Experience 15, Iter 95, disc loss: 0.0012519194063218626, policy loss: 7.658890598021811
Experience 15, Iter 96, disc loss: 0.0016316232148486584, policy loss: 7.5297711937504745
Experience 15, Iter 97, disc loss: 0.001025056124210011, policy loss: 8.092551249734353
Experience 15, Iter 98, disc loss: 0.0009091793655063139, policy loss: 8.283734760394488
Experience 15, Iter 99, disc loss: 0.0009755798707109544, policy loss: 8.134662601316462
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0936],
        [1.0056],
        [0.0107]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0167, 0.0950, 0.5492, 0.0136, 0.0026, 2.1408]],

        [[0.0167, 0.0950, 0.5492, 0.0136, 0.0026, 2.1408]],

        [[0.0167, 0.0950, 0.5492, 0.0136, 0.0026, 2.1408]],

        [[0.0167, 0.0950, 0.5492, 0.0136, 0.0026, 2.1408]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0086, 0.3744, 4.0224, 0.0429], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0086, 0.3744, 4.0224, 0.0429])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.052
Iter 2/2000 - Loss: 2.096
Iter 3/2000 - Loss: 1.966
Iter 4/2000 - Loss: 1.888
Iter 5/2000 - Loss: 1.937
Iter 6/2000 - Loss: 1.920
Iter 7/2000 - Loss: 1.831
Iter 8/2000 - Loss: 1.777
Iter 9/2000 - Loss: 1.767
Iter 10/2000 - Loss: 1.725
Iter 11/2000 - Loss: 1.635
Iter 12/2000 - Loss: 1.541
Iter 13/2000 - Loss: 1.463
Iter 14/2000 - Loss: 1.372
Iter 15/2000 - Loss: 1.246
Iter 16/2000 - Loss: 1.089
Iter 17/2000 - Loss: 0.915
Iter 18/2000 - Loss: 0.728
Iter 19/2000 - Loss: 0.525
Iter 20/2000 - Loss: 0.300
Iter 1981/2000 - Loss: -7.946
Iter 1982/2000 - Loss: -7.946
Iter 1983/2000 - Loss: -7.946
Iter 1984/2000 - Loss: -7.946
Iter 1985/2000 - Loss: -7.947
Iter 1986/2000 - Loss: -7.947
Iter 1987/2000 - Loss: -7.947
Iter 1988/2000 - Loss: -7.947
Iter 1989/2000 - Loss: -7.947
Iter 1990/2000 - Loss: -7.947
Iter 1991/2000 - Loss: -7.947
Iter 1992/2000 - Loss: -7.947
Iter 1993/2000 - Loss: -7.947
Iter 1994/2000 - Loss: -7.947
Iter 1995/2000 - Loss: -7.947
Iter 1996/2000 - Loss: -7.947
Iter 1997/2000 - Loss: -7.947
Iter 1998/2000 - Loss: -7.947
Iter 1999/2000 - Loss: -7.947
Iter 2000/2000 - Loss: -7.947
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.6854,  2.9477, 39.8910,  8.7501,  9.3248, 44.9620]],

        [[18.7013, 32.0258, 11.3125,  1.1825,  2.3115, 30.4179]],

        [[17.8216, 32.4211,  9.7275,  0.9759,  1.8074, 18.2229]],

        [[14.8902, 25.7864, 15.1033,  3.6831,  1.0289, 25.4472]]])
Signal Variance: tensor([ 0.0437,  2.4142, 12.7224,  0.3257])
Estimated target variance: tensor([0.0086, 0.3744, 4.0224, 0.0429])
N: 160
Signal to noise ratio: tensor([11.5589, 82.1801, 78.8522, 34.9911])
Bound on condition number: tensor([  21378.1916, 1080570.9602,  994827.4934,  195901.6136])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.00097679746481098, policy loss: 8.390989593517336
Experience 16, Iter 1, disc loss: 0.0010142731632529082, policy loss: 8.086178965513769
Experience 16, Iter 2, disc loss: 0.0011920024379437593, policy loss: 7.8135626484927005
Experience 16, Iter 3, disc loss: 0.0013473073637717472, policy loss: 7.8639889043544855
Experience 16, Iter 4, disc loss: 0.0012945522091341588, policy loss: 7.884644920976189
Experience 16, Iter 5, disc loss: 0.0012191720090134182, policy loss: 7.8666882190491805
Experience 16, Iter 6, disc loss: 0.0010647089513015698, policy loss: 8.099035520645721
Experience 16, Iter 7, disc loss: 0.0012963918603347007, policy loss: 7.714515647033522
Experience 16, Iter 8, disc loss: 0.0010424422883548025, policy loss: 7.621671411085856
Experience 16, Iter 9, disc loss: 0.0013132461980485052, policy loss: 7.636459040638328
Experience 16, Iter 10, disc loss: 0.001112335032029569, policy loss: 7.659969241419737
Experience 16, Iter 11, disc loss: 0.0010524830967548818, policy loss: 8.534912639302267
Experience 16, Iter 12, disc loss: 0.0008534383464370165, policy loss: 8.483166520161653
Experience 16, Iter 13, disc loss: 0.0010470944491129692, policy loss: 8.20084143484521
Experience 16, Iter 14, disc loss: 0.0007427455088670355, policy loss: 8.921970283938894
Experience 16, Iter 15, disc loss: 0.0008910233366523348, policy loss: 9.063455440868395
Experience 16, Iter 16, disc loss: 0.0005481694544346147, policy loss: 9.756672015012029
Experience 16, Iter 17, disc loss: 0.0005303791772669328, policy loss: 10.672301866040725
Experience 16, Iter 18, disc loss: 0.0006837077801187876, policy loss: 8.892480698852607
Experience 16, Iter 19, disc loss: 0.0006750516990988481, policy loss: 9.466182432230955
Experience 16, Iter 20, disc loss: 0.0011585863557779768, policy loss: 8.038933569098203
Experience 16, Iter 21, disc loss: 0.0012700953401229946, policy loss: 7.789879246090553
Experience 16, Iter 22, disc loss: 0.0015944999715147765, policy loss: 7.9450119399243775
Experience 16, Iter 23, disc loss: 0.001054120479000214, policy loss: 8.335026623416185
Experience 16, Iter 24, disc loss: 0.0015061822368481333, policy loss: 8.1493217232243
Experience 16, Iter 25, disc loss: 0.0008107814777159499, policy loss: 8.430321117060387
Experience 16, Iter 26, disc loss: 0.0009263723992602078, policy loss: 8.712717793784108
Experience 16, Iter 27, disc loss: 0.0009225104570825214, policy loss: 8.349680162434323
Experience 16, Iter 28, disc loss: 0.0008510679206008823, policy loss: 8.484886518284975
Experience 16, Iter 29, disc loss: 0.0010816317954910175, policy loss: 8.111684434677462
Experience 16, Iter 30, disc loss: 0.0009590025529038907, policy loss: 8.527622171065191
Experience 16, Iter 31, disc loss: 0.0008871688947833579, policy loss: 8.64587810126304
Experience 16, Iter 32, disc loss: 0.0009639649346261402, policy loss: 8.317670752099406
Experience 16, Iter 33, disc loss: 0.0010962341956820014, policy loss: 7.719012696458146
Experience 16, Iter 34, disc loss: 0.0009606287253353457, policy loss: 8.05035593050336
Experience 16, Iter 35, disc loss: 0.0011193321297886631, policy loss: 7.98591876723224
Experience 16, Iter 36, disc loss: 0.0008529753228208333, policy loss: 8.52584434420613
Experience 16, Iter 37, disc loss: 0.0011348063430357466, policy loss: 7.876929048487668
Experience 16, Iter 38, disc loss: 0.0009803314451138248, policy loss: 8.21527646156716
Experience 16, Iter 39, disc loss: 0.0007420771487924517, policy loss: 8.765010722982925
Experience 16, Iter 40, disc loss: 0.0008370273604426745, policy loss: 8.661530006188631
Experience 16, Iter 41, disc loss: 0.0006486554298518205, policy loss: 9.074623320452538
Experience 16, Iter 42, disc loss: 0.0006975292086191815, policy loss: 8.727512430148877
Experience 16, Iter 43, disc loss: 0.0008771056318771539, policy loss: 8.245660866331194
Experience 16, Iter 44, disc loss: 0.0008296927649718115, policy loss: 8.726397951859665
Experience 16, Iter 45, disc loss: 0.0008852782862350541, policy loss: 8.408457986810909
Experience 16, Iter 46, disc loss: 0.0007881136005905676, policy loss: 8.559054990465519
Experience 16, Iter 47, disc loss: 0.000916762898380276, policy loss: 8.135505351980688
Experience 16, Iter 48, disc loss: 0.0008982107014528231, policy loss: 8.458942865950327
Experience 16, Iter 49, disc loss: 0.0008332777674338108, policy loss: 8.420708807663473
Experience 16, Iter 50, disc loss: 0.00099962368430632, policy loss: 7.817590034320952
Experience 16, Iter 51, disc loss: 0.0011063456711957657, policy loss: 8.0240792006289
Experience 16, Iter 52, disc loss: 0.0011347896756844999, policy loss: 8.037435464446688
Experience 16, Iter 53, disc loss: 0.0012513415019425296, policy loss: 8.220595929843435
Experience 16, Iter 54, disc loss: 0.001104768952629419, policy loss: 8.033152145162939
Experience 16, Iter 55, disc loss: 0.0011069749345664257, policy loss: 7.933663783022306
Experience 16, Iter 56, disc loss: 0.0007996854837905742, policy loss: 8.678614220571339
Experience 16, Iter 57, disc loss: 0.0009242875953643727, policy loss: 8.623967218544763
Experience 16, Iter 58, disc loss: 0.0007836703696869965, policy loss: 8.600711344844278
Experience 16, Iter 59, disc loss: 0.00108096263236432, policy loss: 8.472055194727206
Experience 16, Iter 60, disc loss: 0.0010493116713218187, policy loss: 8.31901302688155
Experience 16, Iter 61, disc loss: 0.001012404411141866, policy loss: 8.434963119173704
Experience 16, Iter 62, disc loss: 0.0010064569320794793, policy loss: 8.24288584737634
Experience 16, Iter 63, disc loss: 0.0009543181978216751, policy loss: 8.422672003567946
Experience 16, Iter 64, disc loss: 0.0008984967620940128, policy loss: 8.162103192919975
Experience 16, Iter 65, disc loss: 0.0009135146633413519, policy loss: 8.099600918233687
Experience 16, Iter 66, disc loss: 0.0007684874675925888, policy loss: 8.27854472656988
Experience 16, Iter 67, disc loss: 0.0007722191983606425, policy loss: 8.615257030650806
Experience 16, Iter 68, disc loss: 0.0008434388713721259, policy loss: 8.41276754070934
Experience 16, Iter 69, disc loss: 0.001132377989777786, policy loss: 7.854807762412712
Experience 16, Iter 70, disc loss: 0.001137448283038842, policy loss: 8.069500159387456
Experience 16, Iter 71, disc loss: 0.0011399093161764279, policy loss: 8.288258709597471
Experience 16, Iter 72, disc loss: 0.0008533998021015742, policy loss: 8.239549910925469
Experience 16, Iter 73, disc loss: 0.0015953016898399945, policy loss: 8.888603833900616
Experience 16, Iter 74, disc loss: 0.0007015192125474732, policy loss: 8.553712722906472
Experience 16, Iter 75, disc loss: 0.0008559013176075461, policy loss: 8.545511773843629
Experience 16, Iter 76, disc loss: 0.0008007997548530009, policy loss: 8.425292117326196
Experience 16, Iter 77, disc loss: 0.0011336349720309216, policy loss: 8.108019762307835
Experience 16, Iter 78, disc loss: 0.001116790921989256, policy loss: 8.225658791249096
Experience 16, Iter 79, disc loss: 0.0009668034980382888, policy loss: 8.192737740931696
Experience 16, Iter 80, disc loss: 0.0010976716174899703, policy loss: 7.879479650064349
Experience 16, Iter 81, disc loss: 0.001089515430299069, policy loss: 7.8857476736007035
Experience 16, Iter 82, disc loss: 0.001093312581805407, policy loss: 8.05717427941907
Experience 16, Iter 83, disc loss: 0.0011026976673941253, policy loss: 7.936550047445522
Experience 16, Iter 84, disc loss: 0.0010120177472284038, policy loss: 7.975347002477918
Experience 16, Iter 85, disc loss: 0.0009949471117427404, policy loss: 8.400947003048453
Experience 16, Iter 86, disc loss: 0.0008101091364388361, policy loss: 8.256537380913445
Experience 16, Iter 87, disc loss: 0.0009624814862141695, policy loss: 8.257733001727047
Experience 16, Iter 88, disc loss: 0.0007807473952079286, policy loss: 8.294726245047809
Experience 16, Iter 89, disc loss: 0.0007461272181822809, policy loss: 8.497409181535124
Experience 16, Iter 90, disc loss: 0.0010548158998449602, policy loss: 8.067322406064177
Experience 16, Iter 91, disc loss: 0.0010541365345049724, policy loss: 8.037176375220811
Experience 16, Iter 92, disc loss: 0.000874907355286673, policy loss: 8.44893666196985
Experience 16, Iter 93, disc loss: 0.0010635402127541528, policy loss: 8.204470380935504
Experience 16, Iter 94, disc loss: 0.0010548153064746555, policy loss: 8.08617417852659
Experience 16, Iter 95, disc loss: 0.0014047852406908297, policy loss: 7.622630165885133
Experience 16, Iter 96, disc loss: 0.0008451876590158944, policy loss: 8.227757589563378
Experience 16, Iter 97, disc loss: 0.001576794967545039, policy loss: 7.857818207880224
Experience 16, Iter 98, disc loss: 0.0010362666348566266, policy loss: 8.09478193230117
Experience 16, Iter 99, disc loss: 0.0014237302367671452, policy loss: 7.739643171141632
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0020],
        [0.0958],
        [1.0068],
        [0.0103]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0158, 0.0907, 0.5319, 0.0134, 0.0025, 2.1969]],

        [[0.0158, 0.0907, 0.5319, 0.0134, 0.0025, 2.1969]],

        [[0.0158, 0.0907, 0.5319, 0.0134, 0.0025, 2.1969]],

        [[0.0158, 0.0907, 0.5319, 0.0134, 0.0025, 2.1969]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.3834, 4.0273, 0.0412], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.3834, 4.0273, 0.0412])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.027
Iter 2/2000 - Loss: 2.079
Iter 3/2000 - Loss: 1.941
Iter 4/2000 - Loss: 1.862
Iter 5/2000 - Loss: 1.916
Iter 6/2000 - Loss: 1.900
Iter 7/2000 - Loss: 1.810
Iter 8/2000 - Loss: 1.757
Iter 9/2000 - Loss: 1.749
Iter 10/2000 - Loss: 1.710
Iter 11/2000 - Loss: 1.621
Iter 12/2000 - Loss: 1.527
Iter 13/2000 - Loss: 1.449
Iter 14/2000 - Loss: 1.361
Iter 15/2000 - Loss: 1.234
Iter 16/2000 - Loss: 1.074
Iter 17/2000 - Loss: 0.897
Iter 18/2000 - Loss: 0.707
Iter 19/2000 - Loss: 0.499
Iter 20/2000 - Loss: 0.268
Iter 1981/2000 - Loss: -8.058
Iter 1982/2000 - Loss: -8.058
Iter 1983/2000 - Loss: -8.058
Iter 1984/2000 - Loss: -8.058
Iter 1985/2000 - Loss: -8.058
Iter 1986/2000 - Loss: -8.058
Iter 1987/2000 - Loss: -8.058
Iter 1988/2000 - Loss: -8.058
Iter 1989/2000 - Loss: -8.058
Iter 1990/2000 - Loss: -8.058
Iter 1991/2000 - Loss: -8.058
Iter 1992/2000 - Loss: -8.058
Iter 1993/2000 - Loss: -8.058
Iter 1994/2000 - Loss: -8.058
Iter 1995/2000 - Loss: -8.058
Iter 1996/2000 - Loss: -8.058
Iter 1997/2000 - Loss: -8.058
Iter 1998/2000 - Loss: -8.058
Iter 1999/2000 - Loss: -8.058
Iter 2000/2000 - Loss: -8.058
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.5133,  2.9590, 39.4076,  8.2192,  9.6926, 45.0764]],

        [[18.1103, 30.6146, 11.6044,  1.1481,  2.3771, 28.1852]],

        [[17.3828, 30.9052, 10.3106,  0.9481,  1.7504, 17.4652]],

        [[14.2546, 25.4364, 15.2783,  3.7501,  1.0264, 26.2949]]])
Signal Variance: tensor([ 0.0428,  2.3646, 12.3737,  0.3357])
Estimated target variance: tensor([0.0082, 0.3834, 4.0273, 0.0412])
N: 170
Signal to noise ratio: tensor([11.6486, 82.8507, 77.3776, 36.1813])
Bound on condition number: tensor([  23068.1666, 1166922.6389, 1017840.1714,  222545.3235])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0011797330752000975, policy loss: 7.882225405057298
Experience 17, Iter 1, disc loss: 0.0011517148513025535, policy loss: 8.246078383484189
Experience 17, Iter 2, disc loss: 0.0011720707341999247, policy loss: 7.84517838467165
Experience 17, Iter 3, disc loss: 0.0009589386799072795, policy loss: 7.976133439451087
Experience 17, Iter 4, disc loss: 0.0010797620881940403, policy loss: 8.024149054165107
Experience 17, Iter 5, disc loss: 0.001102822406140471, policy loss: 7.882221431901453
Experience 17, Iter 6, disc loss: 0.0009882024781051831, policy loss: 8.094873681501214
Experience 17, Iter 7, disc loss: 0.0010898552059303082, policy loss: 7.72042343864065
Experience 17, Iter 8, disc loss: 0.0010093634963343227, policy loss: 7.957874347477317
Experience 17, Iter 9, disc loss: 0.0006916945106064795, policy loss: 8.70236843733738
Experience 17, Iter 10, disc loss: 0.0007754121862408873, policy loss: 8.327010839356316
Experience 17, Iter 11, disc loss: 0.0007963204639176302, policy loss: 8.583756848902645
Experience 17, Iter 12, disc loss: 0.0005014183390418356, policy loss: 10.802668995809437
Experience 17, Iter 13, disc loss: 0.0005099521181009482, policy loss: 11.15945997304943
Experience 17, Iter 14, disc loss: 0.0004945236519182925, policy loss: 10.498670248166217
Experience 17, Iter 15, disc loss: 0.0003928337416093077, policy loss: 11.189099002285001
Experience 17, Iter 16, disc loss: 0.0003917038641227014, policy loss: 11.618135584678381
Experience 17, Iter 17, disc loss: 0.000360755749194272, policy loss: 11.505834156649044
Experience 17, Iter 18, disc loss: 0.000349363507369934, policy loss: 11.645893036023214
Experience 17, Iter 19, disc loss: 0.00033286102787117867, policy loss: 12.102440499097016
Experience 17, Iter 20, disc loss: 0.00032638641296739115, policy loss: 12.929541508276728
Experience 17, Iter 21, disc loss: 0.0003261032102571692, policy loss: 12.058674083480955
Experience 17, Iter 22, disc loss: 0.0003468214067094612, policy loss: 11.10876609419979
Experience 17, Iter 23, disc loss: 0.00040239011631573143, policy loss: 11.045145803194782
Experience 17, Iter 24, disc loss: 0.0004885727468190498, policy loss: 10.92713754884517
Experience 17, Iter 25, disc loss: 0.0005944663735168274, policy loss: 9.811797741596965
Experience 17, Iter 26, disc loss: 0.0007271705967670032, policy loss: 8.943812902860241
Experience 17, Iter 27, disc loss: 0.0007389084571308379, policy loss: 8.532786761167502
Experience 17, Iter 28, disc loss: 0.0012225850986143385, policy loss: 7.967161358312345
Experience 17, Iter 29, disc loss: 0.0008638060876008868, policy loss: 8.198250486364408
Experience 17, Iter 30, disc loss: 0.001105286106461922, policy loss: 8.505595713478415
Experience 17, Iter 31, disc loss: 0.001530038625185272, policy loss: 8.667728440856314
Experience 17, Iter 32, disc loss: 0.0006491099102463032, policy loss: 9.172183602566161
Experience 17, Iter 33, disc loss: 0.0004977880011659964, policy loss: 9.162773354070492
Experience 17, Iter 34, disc loss: 0.0006262460285121254, policy loss: 9.087336821440726
Experience 17, Iter 35, disc loss: 0.00044974730216107124, policy loss: 9.262821421468576
Experience 17, Iter 36, disc loss: 0.0006595997446356104, policy loss: 9.171974723817648
Experience 17, Iter 37, disc loss: 0.0005769065133025728, policy loss: 9.381727209498077
Experience 17, Iter 38, disc loss: 0.000564453395338807, policy loss: 9.318059268535112
Experience 17, Iter 39, disc loss: 0.0005114228475389981, policy loss: 9.261307088559974
Experience 17, Iter 40, disc loss: 0.00053363863642013, policy loss: 9.166577720383113
Experience 17, Iter 41, disc loss: 0.0005497481561794507, policy loss: 9.040174435721752
Experience 17, Iter 42, disc loss: 0.0005783953766070999, policy loss: 8.81062696229069
Experience 17, Iter 43, disc loss: 0.0008374038268022027, policy loss: 8.445241622352821
Experience 17, Iter 44, disc loss: 0.0007554400566663617, policy loss: 8.57679971810928
Experience 17, Iter 45, disc loss: 0.0007176325329879163, policy loss: 8.717218504875262
Experience 17, Iter 46, disc loss: 0.000855848057998673, policy loss: 8.500462089600948
Experience 17, Iter 47, disc loss: 0.0008617429213852283, policy loss: 8.573919892103309
Experience 17, Iter 48, disc loss: 0.0008185433076020054, policy loss: 8.415615165954524
Experience 17, Iter 49, disc loss: 0.001334212257917905, policy loss: 8.009901098235341
Experience 17, Iter 50, disc loss: 0.0008478409004411039, policy loss: 8.071325061070691
Experience 17, Iter 51, disc loss: 0.0008188969249130404, policy loss: 8.279170786055563
Experience 17, Iter 52, disc loss: 0.0010367395830552738, policy loss: 7.9438438818145585
Experience 17, Iter 53, disc loss: 0.0008278712871289903, policy loss: 8.32631303479716
Experience 17, Iter 54, disc loss: 0.0008601272836247404, policy loss: 8.366111617432995
Experience 17, Iter 55, disc loss: 0.001188178592980418, policy loss: 8.18983540714995
Experience 17, Iter 56, disc loss: 0.0010545924649245455, policy loss: 8.029967837179129
Experience 17, Iter 57, disc loss: 0.0009173045149500222, policy loss: 8.025120755192559
Experience 17, Iter 58, disc loss: 0.001237767006161602, policy loss: 8.104969129459194
Experience 17, Iter 59, disc loss: 0.0008958404559891923, policy loss: 8.044267090594625
Experience 17, Iter 60, disc loss: 0.0009629395811648347, policy loss: 8.131147855257456
Experience 17, Iter 61, disc loss: 0.0008274764494483871, policy loss: 8.283043991732702
Experience 17, Iter 62, disc loss: 0.0008226106757747422, policy loss: 8.213199022875502
Experience 17, Iter 63, disc loss: 0.0010273970143216333, policy loss: 8.111915396995677
Experience 17, Iter 64, disc loss: 0.0008927214507728507, policy loss: 8.248895420019995
Experience 17, Iter 65, disc loss: 0.0006846885698580639, policy loss: 8.29586821096433
Experience 17, Iter 66, disc loss: 0.0007792447003321914, policy loss: 8.35332880771534
Experience 17, Iter 67, disc loss: 0.0005983279083352699, policy loss: 8.78422517407896
Experience 17, Iter 68, disc loss: 0.0006974753482719144, policy loss: 8.880280194587048
Experience 17, Iter 69, disc loss: 0.0006951369783191853, policy loss: 8.363990764668483
Experience 17, Iter 70, disc loss: 0.0007916063966930012, policy loss: 8.297321572537646
Experience 17, Iter 71, disc loss: 0.000820012256027503, policy loss: 8.402486093906738
Experience 17, Iter 72, disc loss: 0.0009555862998968453, policy loss: 8.41278569835219
Experience 17, Iter 73, disc loss: 0.0006551058250066049, policy loss: 8.565009410773952
Experience 17, Iter 74, disc loss: 0.000781167691546112, policy loss: 8.53534954156299
Experience 17, Iter 75, disc loss: 0.0007763086978975267, policy loss: 8.399530610763044
Experience 17, Iter 76, disc loss: 0.0007312285561784893, policy loss: 8.557774206526421
Experience 17, Iter 77, disc loss: 0.0008864542726761952, policy loss: 8.23892234712062
Experience 17, Iter 78, disc loss: 0.0007942322002881201, policy loss: 8.27264697299779
Experience 17, Iter 79, disc loss: 0.0009173202056304358, policy loss: 8.291913397628873
Experience 17, Iter 80, disc loss: 0.0009824975832252903, policy loss: 8.169546600347541
Experience 17, Iter 81, disc loss: 0.0009370179728800617, policy loss: 8.45537752296567
Experience 17, Iter 82, disc loss: 0.0011079660185447063, policy loss: 8.309799104338689
Experience 17, Iter 83, disc loss: 0.0011600976953505045, policy loss: 8.168261097271488
Experience 17, Iter 84, disc loss: 0.001379497347561407, policy loss: 7.783690437695954
Experience 17, Iter 85, disc loss: 0.0010137762811004876, policy loss: 7.66587089482581
Experience 17, Iter 86, disc loss: 0.0008079043431067028, policy loss: 8.262392572178817
Experience 17, Iter 87, disc loss: 0.0009339752441967825, policy loss: 8.012364830374297
Experience 17, Iter 88, disc loss: 0.0008912211961307323, policy loss: 7.916529680213795
Experience 17, Iter 89, disc loss: 0.000772154617666082, policy loss: 8.467845312944954
Experience 17, Iter 90, disc loss: 0.0007613332912070034, policy loss: 8.157130727333234
Experience 17, Iter 91, disc loss: 0.00048283572688615737, policy loss: 9.152841339300881
Experience 17, Iter 92, disc loss: 0.0004604316468137066, policy loss: 9.966428724429978
Experience 17, Iter 93, disc loss: 0.0004394821691031855, policy loss: 10.288548461946258
Experience 17, Iter 94, disc loss: 0.0004634465930736416, policy loss: 10.441196411978593
Experience 17, Iter 95, disc loss: 0.0004757372901380421, policy loss: 10.56619014055244
Experience 17, Iter 96, disc loss: 0.0003872636734611321, policy loss: 10.66456266624252
Experience 17, Iter 97, disc loss: 0.00034496710439962177, policy loss: 11.13520621786599
Experience 17, Iter 98, disc loss: 0.00030332129528043243, policy loss: 11.567843030574751
Experience 17, Iter 99, disc loss: 0.00030987472086336175, policy loss: 11.01973866933583
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0955],
        [0.9655],
        [0.0098]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0174, 0.1033, 0.5055, 0.0136, 0.0024, 2.2691]],

        [[0.0174, 0.1033, 0.5055, 0.0136, 0.0024, 2.2691]],

        [[0.0174, 0.1033, 0.5055, 0.0136, 0.0024, 2.2691]],

        [[0.0174, 0.1033, 0.5055, 0.0136, 0.0024, 2.2691]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0098, 0.3820, 3.8621, 0.0391], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0098, 0.3820, 3.8621, 0.0391])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.036
Iter 2/2000 - Loss: 2.133
Iter 3/2000 - Loss: 1.963
Iter 4/2000 - Loss: 1.915
Iter 5/2000 - Loss: 1.965
Iter 6/2000 - Loss: 1.928
Iter 7/2000 - Loss: 1.840
Iter 8/2000 - Loss: 1.792
Iter 9/2000 - Loss: 1.779
Iter 10/2000 - Loss: 1.733
Iter 11/2000 - Loss: 1.641
Iter 12/2000 - Loss: 1.538
Iter 13/2000 - Loss: 1.448
Iter 14/2000 - Loss: 1.352
Iter 15/2000 - Loss: 1.225
Iter 16/2000 - Loss: 1.064
Iter 17/2000 - Loss: 0.880
Iter 18/2000 - Loss: 0.681
Iter 19/2000 - Loss: 0.469
Iter 20/2000 - Loss: 0.240
Iter 1981/2000 - Loss: -8.102
Iter 1982/2000 - Loss: -8.102
Iter 1983/2000 - Loss: -8.102
Iter 1984/2000 - Loss: -8.102
Iter 1985/2000 - Loss: -8.102
Iter 1986/2000 - Loss: -8.102
Iter 1987/2000 - Loss: -8.102
Iter 1988/2000 - Loss: -8.102
Iter 1989/2000 - Loss: -8.102
Iter 1990/2000 - Loss: -8.102
Iter 1991/2000 - Loss: -8.102
Iter 1992/2000 - Loss: -8.102
Iter 1993/2000 - Loss: -8.103
Iter 1994/2000 - Loss: -8.103
Iter 1995/2000 - Loss: -8.103
Iter 1996/2000 - Loss: -8.103
Iter 1997/2000 - Loss: -8.103
Iter 1998/2000 - Loss: -8.103
Iter 1999/2000 - Loss: -8.103
Iter 2000/2000 - Loss: -8.103
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[15.8774,  7.7479, 41.5456,  9.4503,  4.1051, 50.1893]],

        [[18.4731, 32.2248, 11.4480,  1.1703,  2.7211, 29.5992]],

        [[19.7574, 32.6276, 10.2347,  0.9243,  1.6414, 16.9603]],

        [[15.8052, 27.4216, 14.7614,  3.7295,  1.0214, 25.2049]]])
Signal Variance: tensor([ 0.1044,  2.4477, 11.2759,  0.3135])
Estimated target variance: tensor([0.0098, 0.3820, 3.8621, 0.0391])
N: 180
Signal to noise ratio: tensor([17.9430, 83.5545, 72.7553, 35.6657])
Bound on condition number: tensor([  57952.2227, 1256646.1730,  952800.7064,  228968.3980])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.00031234943869977503, policy loss: 10.819935068467707
Experience 18, Iter 1, disc loss: 0.0003064326488325312, policy loss: 11.006151867705839
Experience 18, Iter 2, disc loss: 0.00032995957628740257, policy loss: 10.64082398025419
Experience 18, Iter 3, disc loss: 0.00032773003556617125, policy loss: 10.552161706053843
Experience 18, Iter 4, disc loss: 0.00033520826533636197, policy loss: 10.391253135306034
Experience 18, Iter 5, disc loss: 0.00030855119297377136, policy loss: 10.80706279593478
Experience 18, Iter 6, disc loss: 0.00041383911433604686, policy loss: 10.329327879135086
Experience 18, Iter 7, disc loss: 0.00039695289658063954, policy loss: 10.154574828783561
Experience 18, Iter 8, disc loss: 0.000527443675227494, policy loss: 10.493213765100794
Experience 18, Iter 9, disc loss: 0.0004534390191910261, policy loss: 9.92143070601509
Experience 18, Iter 10, disc loss: 0.00046164691368449126, policy loss: 9.972249421493586
Experience 18, Iter 11, disc loss: 0.0003929895545919052, policy loss: 10.282547605814017
Experience 18, Iter 12, disc loss: 0.0005474647662203737, policy loss: 10.205903945916031
Experience 18, Iter 13, disc loss: 0.00042806292385299437, policy loss: 10.133015834779552
Experience 18, Iter 14, disc loss: 0.00047993810396588924, policy loss: 10.281102894508368
Experience 18, Iter 15, disc loss: 0.00046760038436493624, policy loss: 9.825798795307335
Experience 18, Iter 16, disc loss: 0.0005199559842746638, policy loss: 9.702380389868477
Experience 18, Iter 17, disc loss: 0.0004546648717207026, policy loss: 9.80586238090634
Experience 18, Iter 18, disc loss: 0.0004428663103996788, policy loss: 10.019146116945016
Experience 18, Iter 19, disc loss: 0.00038587493250494525, policy loss: 10.538319709153013
Experience 18, Iter 20, disc loss: 0.0006217542901648184, policy loss: 9.681385145712255
Experience 18, Iter 21, disc loss: 0.0005828961626653411, policy loss: 9.207197119269031
Experience 18, Iter 22, disc loss: 0.0005425957066802431, policy loss: 9.07109462019336
Experience 18, Iter 23, disc loss: 0.0011777794210792644, policy loss: 8.691274363337358
Experience 18, Iter 24, disc loss: 0.0005906112444331896, policy loss: 8.749777707434852
Experience 18, Iter 25, disc loss: 0.0006309141891540395, policy loss: 9.083412804649189
Experience 18, Iter 26, disc loss: 0.0006890108442901158, policy loss: 8.504904265435087
Experience 18, Iter 27, disc loss: 0.000738911856279775, policy loss: 8.444627492334394
Experience 18, Iter 28, disc loss: 0.0007115323639682424, policy loss: 8.413092525020728
Experience 18, Iter 29, disc loss: 0.0006945116307839226, policy loss: 8.425388788900692
Experience 18, Iter 30, disc loss: 0.0007052140793111733, policy loss: 8.587507504643003
Experience 18, Iter 31, disc loss: 0.0010362559479897109, policy loss: 8.387941108743854
Experience 18, Iter 32, disc loss: 0.0006035784950914734, policy loss: 8.52730817908446
Experience 18, Iter 33, disc loss: 0.0007310872466983929, policy loss: 8.407652792077343
Experience 18, Iter 34, disc loss: 0.0009111673165636523, policy loss: 8.29666003747068
Experience 18, Iter 35, disc loss: 0.0008896946488414184, policy loss: 8.680164486967396
Experience 18, Iter 36, disc loss: 0.0006922829075584802, policy loss: 8.419831446550937
Experience 18, Iter 37, disc loss: 0.001169992455462373, policy loss: 8.057035953321627
Experience 18, Iter 38, disc loss: 0.0007094427687056517, policy loss: 8.331274541879738
Experience 18, Iter 39, disc loss: 0.000867373578229557, policy loss: 8.2432209944618
Experience 18, Iter 40, disc loss: 0.0008816366962731209, policy loss: 8.224491153366863
Experience 18, Iter 41, disc loss: 0.0009096824505008938, policy loss: 8.35667949324009
Experience 18, Iter 42, disc loss: 0.0007738359375475036, policy loss: 8.523265927357528
Experience 18, Iter 43, disc loss: 0.0008880349670392356, policy loss: 8.031056721298807
Experience 18, Iter 44, disc loss: 0.0007150195171212489, policy loss: 8.081550874574061
Experience 18, Iter 45, disc loss: 0.0008271663967311697, policy loss: 8.232007104688835
Experience 18, Iter 46, disc loss: 0.0007635874345129194, policy loss: 8.390866432041786
Experience 18, Iter 47, disc loss: 0.0008637368945279345, policy loss: 8.075015196099214
Experience 18, Iter 48, disc loss: 0.0008955210609466397, policy loss: 8.269057902786598
Experience 18, Iter 49, disc loss: 0.0008270732384011009, policy loss: 8.580303440029535
Experience 18, Iter 50, disc loss: 0.0008893711957895973, policy loss: 8.640695777994107
Experience 18, Iter 51, disc loss: 0.000638003493543419, policy loss: 8.64042983689858
Experience 18, Iter 52, disc loss: 0.0008196258449252884, policy loss: 8.202938575711814
Experience 18, Iter 53, disc loss: 0.0008436020247976994, policy loss: 8.084872397187826
Experience 18, Iter 54, disc loss: 0.0009759531986675788, policy loss: 8.231838628698043
Experience 18, Iter 55, disc loss: 0.0008145481277457821, policy loss: 8.273301793793776
Experience 18, Iter 56, disc loss: 0.0012623288828178785, policy loss: 7.85625599014911
Experience 18, Iter 57, disc loss: 0.0007913123994789642, policy loss: 8.33801448368405
Experience 18, Iter 58, disc loss: 0.0009780113483671506, policy loss: 8.382021570796601
Experience 18, Iter 59, disc loss: 0.0008048098417070067, policy loss: 8.277895421868095
Experience 18, Iter 60, disc loss: 0.0006525235154386103, policy loss: 8.306794607917166
Experience 18, Iter 61, disc loss: 0.0006380103180966239, policy loss: 8.586624414290782
Experience 18, Iter 62, disc loss: 0.0007028163583334395, policy loss: 9.021692861371779
Experience 18, Iter 63, disc loss: 0.0005584042448254437, policy loss: 9.70222998385129
Experience 18, Iter 64, disc loss: 0.0004210076971873988, policy loss: 10.29668484841768
Experience 18, Iter 65, disc loss: 0.0004156533238988887, policy loss: 10.487569664874613
Experience 18, Iter 66, disc loss: 0.00032223194098533796, policy loss: 10.437483241279473
Experience 18, Iter 67, disc loss: 0.0003089738916948183, policy loss: 10.811936277419107
Experience 18, Iter 68, disc loss: 0.00032975789036577196, policy loss: 10.17286507398061
Experience 18, Iter 69, disc loss: 0.0003050325828618977, policy loss: 10.627436662905783
Experience 18, Iter 70, disc loss: 0.00027599615900727136, policy loss: 11.276924142273433
Experience 18, Iter 71, disc loss: 0.00025872249690374045, policy loss: 12.153954998159609
Experience 18, Iter 72, disc loss: 0.00026088203719299986, policy loss: 12.443313902908631
Experience 18, Iter 73, disc loss: 0.00026754893713815375, policy loss: 11.220497942998403
Experience 18, Iter 74, disc loss: 0.0003034967101449151, policy loss: 10.443094677151189
Experience 18, Iter 75, disc loss: 0.0002820747661710728, policy loss: 10.751490983416806
Experience 18, Iter 76, disc loss: 0.0002997221248476976, policy loss: 10.273983241284796
Experience 18, Iter 77, disc loss: 0.0003014488663737635, policy loss: 10.424954029690745
Experience 18, Iter 78, disc loss: 0.00035613787403075746, policy loss: 10.533420180128587
Experience 18, Iter 79, disc loss: 0.00042312481593121364, policy loss: 10.531997635789873
Experience 18, Iter 80, disc loss: 0.00034556602189993006, policy loss: 10.763984579079587
Experience 18, Iter 81, disc loss: 0.0004973490270683046, policy loss: 10.371821872281291
Experience 18, Iter 82, disc loss: 0.0005260797088776944, policy loss: 10.705202212435655
Experience 18, Iter 83, disc loss: 0.0005001439514054976, policy loss: 10.152227633834693
Experience 18, Iter 84, disc loss: 0.00043649748649417155, policy loss: 10.55885338298284
Experience 18, Iter 85, disc loss: 0.0003909169225451961, policy loss: 9.966804506726143
Experience 18, Iter 86, disc loss: 0.0004998158497999063, policy loss: 10.47252296397361
Experience 18, Iter 87, disc loss: 0.00041307609868483153, policy loss: 9.442023446420276
Experience 18, Iter 88, disc loss: 0.0005406145052558409, policy loss: 8.598733129926398
Experience 18, Iter 89, disc loss: 0.0009727485486074868, policy loss: 8.56659909701456
Experience 18, Iter 90, disc loss: 0.0007889038381600909, policy loss: 9.022129660910817
Experience 18, Iter 91, disc loss: 0.0005933160435035112, policy loss: 9.326383825305145
Experience 18, Iter 92, disc loss: 0.0004123833320229871, policy loss: 9.603196673334683
Experience 18, Iter 93, disc loss: 0.0003549298487620281, policy loss: 9.622159665406508
Experience 18, Iter 94, disc loss: 0.0003057750740132473, policy loss: 9.905851863441617
Experience 18, Iter 95, disc loss: 0.0015078261204869246, policy loss: 9.56233649081987
Experience 18, Iter 96, disc loss: 0.00043540952593544753, policy loss: 9.813621872558233
Experience 18, Iter 97, disc loss: 0.00031471661987248726, policy loss: 10.03184529832916
Experience 18, Iter 98, disc loss: 0.0003924511451779477, policy loss: 9.906291628084016
Experience 18, Iter 99, disc loss: 0.0003699563195916522, policy loss: 9.980369081884412
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0933],
        [0.9404],
        [0.0095]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0167, 0.0988, 0.4902, 0.0134, 0.0023, 2.2291]],

        [[0.0167, 0.0988, 0.4902, 0.0134, 0.0023, 2.2291]],

        [[0.0167, 0.0988, 0.4902, 0.0134, 0.0023, 2.2291]],

        [[0.0167, 0.0988, 0.4902, 0.0134, 0.0023, 2.2291]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0094, 0.3732, 3.7618, 0.0379], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0094, 0.3732, 3.7618, 0.0379])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.979
Iter 2/2000 - Loss: 2.087
Iter 3/2000 - Loss: 1.907
Iter 4/2000 - Loss: 1.860
Iter 5/2000 - Loss: 1.915
Iter 6/2000 - Loss: 1.878
Iter 7/2000 - Loss: 1.787
Iter 8/2000 - Loss: 1.739
Iter 9/2000 - Loss: 1.730
Iter 10/2000 - Loss: 1.689
Iter 11/2000 - Loss: 1.598
Iter 12/2000 - Loss: 1.495
Iter 13/2000 - Loss: 1.407
Iter 14/2000 - Loss: 1.315
Iter 15/2000 - Loss: 1.194
Iter 16/2000 - Loss: 1.035
Iter 17/2000 - Loss: 0.852
Iter 18/2000 - Loss: 0.655
Iter 19/2000 - Loss: 0.445
Iter 20/2000 - Loss: 0.217
Iter 1981/2000 - Loss: -8.167
Iter 1982/2000 - Loss: -8.167
Iter 1983/2000 - Loss: -8.167
Iter 1984/2000 - Loss: -8.167
Iter 1985/2000 - Loss: -8.167
Iter 1986/2000 - Loss: -8.167
Iter 1987/2000 - Loss: -8.167
Iter 1988/2000 - Loss: -8.168
Iter 1989/2000 - Loss: -8.168
Iter 1990/2000 - Loss: -8.168
Iter 1991/2000 - Loss: -8.168
Iter 1992/2000 - Loss: -8.168
Iter 1993/2000 - Loss: -8.168
Iter 1994/2000 - Loss: -8.168
Iter 1995/2000 - Loss: -8.168
Iter 1996/2000 - Loss: -8.168
Iter 1997/2000 - Loss: -8.168
Iter 1998/2000 - Loss: -8.168
Iter 1999/2000 - Loss: -8.168
Iter 2000/2000 - Loss: -8.168
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[15.4236,  7.6553, 40.6579,  8.8855,  4.1803, 49.1014]],

        [[17.7976, 31.3408, 11.6050,  1.1816,  2.5948, 29.3159]],

        [[18.7274, 31.8253, 10.3823,  0.9362,  1.6330, 16.8226]],

        [[14.9332, 26.6933, 14.6974,  3.7029,  1.0277, 23.0095]]])
Signal Variance: tensor([ 0.1026,  2.4768, 11.4263,  0.3152])
Estimated target variance: tensor([0.0094, 0.3732, 3.7618, 0.0379])
N: 190
Signal to noise ratio: tensor([17.9515, 83.1273, 73.3391, 36.1376])
Bound on condition number: tensor([  61229.9833, 1312927.5917, 1021938.3947,  248127.5860])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0003190275026149492, policy loss: 9.992609195943956
Experience 19, Iter 1, disc loss: 0.00039025556599292684, policy loss: 10.00967532250446
Experience 19, Iter 2, disc loss: 0.0002949250733709865, policy loss: 10.021201148288801
Experience 19, Iter 3, disc loss: 0.00032509076929348535, policy loss: 9.77091045537107
Experience 19, Iter 4, disc loss: 0.0003503751305663778, policy loss: 9.853644050612974
Experience 19, Iter 5, disc loss: 0.0002897245095469785, policy loss: 9.971140005519745
Experience 19, Iter 6, disc loss: 0.0003166219714363323, policy loss: 9.934774481079778
Experience 19, Iter 7, disc loss: 0.00035652168828138013, policy loss: 9.425178948870169
Experience 19, Iter 8, disc loss: 0.00034856872481615366, policy loss: 9.57268705157727
Experience 19, Iter 9, disc loss: 0.00048124561556763673, policy loss: 9.054158955269891
Experience 19, Iter 10, disc loss: 0.0003746318933512638, policy loss: 9.005223965792577
Experience 19, Iter 11, disc loss: 0.00044320829635551093, policy loss: 9.177620276532153
Experience 19, Iter 12, disc loss: 0.0006330044610785016, policy loss: 8.949156674328446
Experience 19, Iter 13, disc loss: 0.0008484131182493849, policy loss: 8.681998460391817
Experience 19, Iter 14, disc loss: 0.0006735206444815445, policy loss: 8.807046444976724
Experience 19, Iter 15, disc loss: 0.0009559002840452643, policy loss: 8.325746270543398
Experience 19, Iter 16, disc loss: 0.0013693836840433158, policy loss: 8.61713050516296
Experience 19, Iter 17, disc loss: 0.000748503354784836, policy loss: 8.396385916655161
Experience 19, Iter 18, disc loss: 0.0006848436501500548, policy loss: 8.716882595453704
Experience 19, Iter 19, disc loss: 0.0006342738125838113, policy loss: 8.505906667695559
Experience 19, Iter 20, disc loss: 0.0009673124983440844, policy loss: 8.385978876446622
Experience 19, Iter 21, disc loss: 0.0005717501110679796, policy loss: 8.757832648693
Experience 19, Iter 22, disc loss: 0.0007959848070388539, policy loss: 8.41576302480814
Experience 19, Iter 23, disc loss: 0.0005838558066947034, policy loss: 8.906881973318265
Experience 19, Iter 24, disc loss: 0.000683595487114473, policy loss: 8.473203755008559
Experience 19, Iter 25, disc loss: 0.0006003866877991595, policy loss: 9.32289260650645
Experience 19, Iter 26, disc loss: 0.0005499587228568094, policy loss: 9.0044302205736
Experience 19, Iter 27, disc loss: 0.0005958016102486862, policy loss: 8.668475130912952
Experience 19, Iter 28, disc loss: 0.0005336060017860679, policy loss: 8.94891776236468
Experience 19, Iter 29, disc loss: 0.0005830546246728942, policy loss: 8.93005886008287
Experience 19, Iter 30, disc loss: 0.00044608481506159143, policy loss: 9.411121446578207
Experience 19, Iter 31, disc loss: 0.0004072376754182035, policy loss: 9.24030845090125
Experience 19, Iter 32, disc loss: 0.0005263628479866212, policy loss: 9.34583923186042
Experience 19, Iter 33, disc loss: 0.0005469612607164055, policy loss: 8.70989487739322
Experience 19, Iter 34, disc loss: 0.0005661950468761638, policy loss: 8.873328930724654
Experience 19, Iter 35, disc loss: 0.0006378596367537178, policy loss: 8.504232961897124
Experience 19, Iter 36, disc loss: 0.0006500145340620925, policy loss: 8.555690353277935
Experience 19, Iter 37, disc loss: 0.0008667938059884427, policy loss: 8.261319810991687
Experience 19, Iter 38, disc loss: 0.0007090840662896023, policy loss: 8.525411038696678
Experience 19, Iter 39, disc loss: 0.0005390177378786809, policy loss: 8.621758957075548
Experience 19, Iter 40, disc loss: 0.0006952930898279154, policy loss: 8.610947695919432
Experience 19, Iter 41, disc loss: 0.0006659868889633783, policy loss: 8.641960842526748
Experience 19, Iter 42, disc loss: 0.0006391752154254666, policy loss: 8.41233013597893
Experience 19, Iter 43, disc loss: 0.0006735458461202014, policy loss: 8.431768499973572
Experience 19, Iter 44, disc loss: 0.0006843689265202256, policy loss: 8.552381270055996
Experience 19, Iter 45, disc loss: 0.0007559858648935213, policy loss: 8.297787737860979
Experience 19, Iter 46, disc loss: 0.0006357612638862449, policy loss: 8.430628840584049
Experience 19, Iter 47, disc loss: 0.0010021437925517558, policy loss: 8.247360173533167
Experience 19, Iter 48, disc loss: 0.0007808482386562877, policy loss: 8.604347997216834
Experience 19, Iter 49, disc loss: 0.0007271647081184229, policy loss: 8.33578441752146
Experience 19, Iter 50, disc loss: 0.0009754454555655081, policy loss: 7.887554658224294
Experience 19, Iter 51, disc loss: 0.0006918033525965502, policy loss: 8.876195708110837
Experience 19, Iter 52, disc loss: 0.0007026471562090379, policy loss: 8.380840423704662
Experience 19, Iter 53, disc loss: 0.000835144925254492, policy loss: 8.24092973718054
Experience 19, Iter 54, disc loss: 0.0008319951269763855, policy loss: 8.585028701778363
Experience 19, Iter 55, disc loss: 0.0009097656772292209, policy loss: 8.432215769402475
Experience 19, Iter 56, disc loss: 0.0007858451972708235, policy loss: 8.175906161040203
Experience 19, Iter 57, disc loss: 0.0010215912210078735, policy loss: 7.880653992700433
Experience 19, Iter 58, disc loss: 0.0008011587383636509, policy loss: 8.595860645276916
Experience 19, Iter 59, disc loss: 0.0007890788862817046, policy loss: 8.310890399102568
Experience 19, Iter 60, disc loss: 0.0007488883911882279, policy loss: 8.197389929883366
Experience 19, Iter 61, disc loss: 0.0005931523909018065, policy loss: 8.51724395383129
Experience 19, Iter 62, disc loss: 0.0007802299862535131, policy loss: 8.49070304415687
Experience 19, Iter 63, disc loss: 0.0008441215026072887, policy loss: 8.325070462531821
Experience 19, Iter 64, disc loss: 0.0006318669316965565, policy loss: 8.466123170129293
Experience 19, Iter 65, disc loss: 0.0009202876440399469, policy loss: 8.268143375383147
Experience 19, Iter 66, disc loss: 0.0007916599638163047, policy loss: 8.563275784064963
Experience 19, Iter 67, disc loss: 0.0007097580987854891, policy loss: 8.615163621629865
Experience 19, Iter 68, disc loss: 0.0008989353084148818, policy loss: 8.413277763831358
Experience 19, Iter 69, disc loss: 0.0005979447390651482, policy loss: 8.6569317353411
Experience 19, Iter 70, disc loss: 0.0010805045299790942, policy loss: 8.430350677173957
Experience 19, Iter 71, disc loss: 0.0006504343856935419, policy loss: 8.826675531615553
Experience 19, Iter 72, disc loss: 0.000647101621160252, policy loss: 8.53030060066725
Experience 19, Iter 73, disc loss: 0.0005755805032435066, policy loss: 9.11873048945136
Experience 19, Iter 74, disc loss: 0.000723880189734357, policy loss: 8.624875889455115
Experience 19, Iter 75, disc loss: 0.0006968096518204699, policy loss: 8.581492541495214
Experience 19, Iter 76, disc loss: 0.00075660540280648, policy loss: 8.400635397847154
Experience 19, Iter 77, disc loss: 0.0006825830694978051, policy loss: 8.48570899794656
Experience 19, Iter 78, disc loss: 0.0007860380266357576, policy loss: 8.975331448010701
Experience 19, Iter 79, disc loss: 0.0011396725260496537, policy loss: 8.371744133125027
Experience 19, Iter 80, disc loss: 0.0006371947770291426, policy loss: 8.291036317286348
Experience 19, Iter 81, disc loss: 0.0007112865800623493, policy loss: 8.745901524811224
Experience 19, Iter 82, disc loss: 0.0009047201637866116, policy loss: 8.665512163648346
Experience 19, Iter 83, disc loss: 0.0006539442858622239, policy loss: 8.429373124563917
Experience 19, Iter 84, disc loss: 0.0006221879149065598, policy loss: 8.552468482150854
Experience 19, Iter 85, disc loss: 0.0005540067846647686, policy loss: 8.778091072335965
Experience 19, Iter 86, disc loss: 0.0011377260753101385, policy loss: 8.348117486763254
Experience 19, Iter 87, disc loss: 0.0008557848157549279, policy loss: 8.61061000753504
Experience 19, Iter 88, disc loss: 0.0005771481047969226, policy loss: 8.803080207187438
Experience 19, Iter 89, disc loss: 0.0006939126189126172, policy loss: 8.439433719078394
Experience 19, Iter 90, disc loss: 0.0006241133464070809, policy loss: 8.551067323516687
Experience 19, Iter 91, disc loss: 0.0006223946978135157, policy loss: 8.457975022400333
Experience 19, Iter 92, disc loss: 0.0006490622865712631, policy loss: 8.37109049836681
Experience 19, Iter 93, disc loss: 0.000587071039299732, policy loss: 8.696747706394486
Experience 19, Iter 94, disc loss: 0.0007186931932591752, policy loss: 8.269056370592539
Experience 19, Iter 95, disc loss: 0.0007657902633483153, policy loss: 8.507439284533193
Experience 19, Iter 96, disc loss: 0.0005547310413318221, policy loss: 8.618156548684329
Experience 19, Iter 97, disc loss: 0.0006142676379288914, policy loss: 8.772191464729513
Experience 19, Iter 98, disc loss: 0.00043914979942648725, policy loss: 9.11604210005376
Experience 19, Iter 99, disc loss: 0.0005257201266830262, policy loss: 9.454151739890477
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0975],
        [0.9837],
        [0.0096]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.5970e-02, 9.6120e-02, 5.0370e-01, 1.3587e-02, 2.2186e-03,
          2.3078e+00]],

        [[1.5970e-02, 9.6120e-02, 5.0370e-01, 1.3587e-02, 2.2186e-03,
          2.3078e+00]],

        [[1.5970e-02, 9.6120e-02, 5.0370e-01, 1.3587e-02, 2.2186e-03,
          2.3078e+00]],

        [[1.5970e-02, 9.6120e-02, 5.0370e-01, 1.3587e-02, 2.2186e-03,
          2.3078e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0092, 0.3898, 3.9348, 0.0384], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0092, 0.3898, 3.9348, 0.0384])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.017
Iter 2/2000 - Loss: 2.142
Iter 3/2000 - Loss: 1.948
Iter 4/2000 - Loss: 1.908
Iter 5/2000 - Loss: 1.968
Iter 6/2000 - Loss: 1.930
Iter 7/2000 - Loss: 1.837
Iter 8/2000 - Loss: 1.790
Iter 9/2000 - Loss: 1.785
Iter 10/2000 - Loss: 1.748
Iter 11/2000 - Loss: 1.658
Iter 12/2000 - Loss: 1.553
Iter 13/2000 - Loss: 1.462
Iter 14/2000 - Loss: 1.372
Iter 15/2000 - Loss: 1.252
Iter 16/2000 - Loss: 1.092
Iter 17/2000 - Loss: 0.906
Iter 18/2000 - Loss: 0.708
Iter 19/2000 - Loss: 0.500
Iter 20/2000 - Loss: 0.274
Iter 1981/2000 - Loss: -8.206
Iter 1982/2000 - Loss: -8.206
Iter 1983/2000 - Loss: -8.206
Iter 1984/2000 - Loss: -8.206
Iter 1985/2000 - Loss: -8.206
Iter 1986/2000 - Loss: -8.206
Iter 1987/2000 - Loss: -8.206
Iter 1988/2000 - Loss: -8.206
Iter 1989/2000 - Loss: -8.206
Iter 1990/2000 - Loss: -8.206
Iter 1991/2000 - Loss: -8.206
Iter 1992/2000 - Loss: -8.206
Iter 1993/2000 - Loss: -8.206
Iter 1994/2000 - Loss: -8.206
Iter 1995/2000 - Loss: -8.206
Iter 1996/2000 - Loss: -8.206
Iter 1997/2000 - Loss: -8.206
Iter 1998/2000 - Loss: -8.207
Iter 1999/2000 - Loss: -8.207
Iter 2000/2000 - Loss: -8.207
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.8104,  4.5584, 40.0569,  9.1775,  7.1984, 48.7092]],

        [[17.9307, 31.3579, 11.4929,  1.1934,  2.9322, 30.3594]],

        [[18.4172, 31.3642, 10.0394,  0.9290,  1.6747, 17.2628]],

        [[14.5684, 26.3102, 15.9268,  3.5093,  1.0370, 26.2968]]])
Signal Variance: tensor([ 0.0732,  2.5029, 11.4177,  0.3310])
Estimated target variance: tensor([0.0092, 0.3898, 3.9348, 0.0384])
N: 200
Signal to noise ratio: tensor([15.0048, 84.2136, 73.3892, 36.3053])
Bound on condition number: tensor([  45029.5397, 1418387.9652, 1077196.1897,  263615.7446])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.00030701112442851615, policy loss: 9.97738892886557
Experience 20, Iter 1, disc loss: 0.0004250423278292717, policy loss: 9.866782736807174
Experience 20, Iter 2, disc loss: 0.00028217901678263634, policy loss: 10.16427601640062
Experience 20, Iter 3, disc loss: 0.0003092038072483095, policy loss: 9.856496657552855
Experience 20, Iter 4, disc loss: 0.000336022243641636, policy loss: 9.700684441455966
Experience 20, Iter 5, disc loss: 0.00033603158236590234, policy loss: 9.440604857574044
Experience 20, Iter 6, disc loss: 0.0004656548828491119, policy loss: 9.089705334681273
Experience 20, Iter 7, disc loss: 0.0004223433016062338, policy loss: 9.474893439229266
Experience 20, Iter 8, disc loss: 0.00041159939998772145, policy loss: 9.774805615040696
Experience 20, Iter 9, disc loss: 0.0003924965634417747, policy loss: 9.30240525507575
Experience 20, Iter 10, disc loss: 0.0003695213513774688, policy loss: 9.440234049510249
Experience 20, Iter 11, disc loss: 0.0015256966220711815, policy loss: 9.088299295232629
Experience 20, Iter 12, disc loss: 0.0004445613177565509, policy loss: 9.319225992791898
Experience 20, Iter 13, disc loss: 0.0006246749752181319, policy loss: 9.015574887071702
Experience 20, Iter 14, disc loss: 0.0006733039008614314, policy loss: 8.485512191865391
Experience 20, Iter 15, disc loss: 0.0005651400508224542, policy loss: 8.733344300102829
Experience 20, Iter 16, disc loss: 0.0005439887215485056, policy loss: 8.638425151086086
Experience 20, Iter 17, disc loss: 0.0008939657876627355, policy loss: 8.673610416608884
Experience 20, Iter 18, disc loss: 0.0007224316986720209, policy loss: 8.399473543281527
Experience 20, Iter 19, disc loss: 0.0006683848728084223, policy loss: 8.607725117225351
Experience 20, Iter 20, disc loss: 0.0005899200333829165, policy loss: 8.64660557660906
Experience 20, Iter 21, disc loss: 0.0006348073630915828, policy loss: 8.33747713878104
Experience 20, Iter 22, disc loss: 0.0005167053573013748, policy loss: 9.190246715505063
Experience 20, Iter 23, disc loss: 0.0005537489183901731, policy loss: 9.01388384691665
Experience 20, Iter 24, disc loss: 0.0005490248976930058, policy loss: 9.233052871984992
Experience 20, Iter 25, disc loss: 0.0004900482449497721, policy loss: 9.232272521850474
Experience 20, Iter 26, disc loss: 0.0005201393183043307, policy loss: 8.76860422994108
Experience 20, Iter 27, disc loss: 0.0008133712461629997, policy loss: 8.742690927638424
Experience 20, Iter 28, disc loss: 0.00049989734768974, policy loss: 8.725626091991387
Experience 20, Iter 29, disc loss: 0.0006874266183405835, policy loss: 8.678126716177989
Experience 20, Iter 30, disc loss: 0.0005391000835989347, policy loss: 8.579654595364591
Experience 20, Iter 31, disc loss: 0.0005863308060210026, policy loss: 8.510455233297346
Experience 20, Iter 32, disc loss: 0.0005655270514574823, policy loss: 8.73105989174658
Experience 20, Iter 33, disc loss: 0.0005912573376183597, policy loss: 8.753622492547052
Experience 20, Iter 34, disc loss: 0.0005388177792793017, policy loss: 8.778301961052474
Experience 20, Iter 35, disc loss: 0.00045938948646148675, policy loss: 8.907013828777885
Experience 20, Iter 36, disc loss: 0.0006831036538814956, policy loss: 8.513321652578185
Experience 20, Iter 37, disc loss: 0.0007543876265471053, policy loss: 8.65494771656806
Experience 20, Iter 38, disc loss: 0.0004942211278550267, policy loss: 8.892449385506596
Experience 20, Iter 39, disc loss: 0.0006825536256269136, policy loss: 8.633656546905069
Experience 20, Iter 40, disc loss: 0.0006888687979722025, policy loss: 8.521348560904347
Experience 20, Iter 41, disc loss: 0.0005414416958712768, policy loss: 9.021595895004564
Experience 20, Iter 42, disc loss: 0.000499259231690677, policy loss: 8.768900773753794
Experience 20, Iter 43, disc loss: 0.000593319996575417, policy loss: 8.607555055211442
Experience 20, Iter 44, disc loss: 0.0006437442495681128, policy loss: 8.741966728450935
Experience 20, Iter 45, disc loss: 0.0007241897523978342, policy loss: 8.45605357196232
Experience 20, Iter 46, disc loss: 0.000832337549003522, policy loss: 8.285338825615968
Experience 20, Iter 47, disc loss: 0.000509333268208097, policy loss: 8.643265928016612
Experience 20, Iter 48, disc loss: 0.0005445412287865504, policy loss: 8.62575135370611
Experience 20, Iter 49, disc loss: 0.0006075852113856995, policy loss: 8.8017920608179
Experience 20, Iter 50, disc loss: 0.0005551106897534674, policy loss: 8.665773523483042
Experience 20, Iter 51, disc loss: 0.000782129471748209, policy loss: 8.420852010570036
Experience 20, Iter 52, disc loss: 0.0007204737758082616, policy loss: 8.361932271552842
Experience 20, Iter 53, disc loss: 0.0009042273186766195, policy loss: 8.070522969667497
Experience 20, Iter 54, disc loss: 0.000572519826045992, policy loss: 9.017269719946475
Experience 20, Iter 55, disc loss: 0.0006186550986528989, policy loss: 8.736204742858536
Experience 20, Iter 56, disc loss: 0.0007094829507570848, policy loss: 8.7128245046476
Experience 20, Iter 57, disc loss: 0.000715912317385629, policy loss: 8.718930374894802
Experience 20, Iter 58, disc loss: 0.0006854217246820279, policy loss: 8.43529539578066
Experience 20, Iter 59, disc loss: 0.0005114504247977125, policy loss: 8.883930105336976
Experience 20, Iter 60, disc loss: 0.0005037970192552991, policy loss: 9.072658321486529
Experience 20, Iter 61, disc loss: 0.0006433099667540375, policy loss: 8.618570124934427
Experience 20, Iter 62, disc loss: 0.0005913954886993012, policy loss: 8.592623676244639
Experience 20, Iter 63, disc loss: 0.0007973045100006107, policy loss: 8.323891557964687
Experience 20, Iter 64, disc loss: 0.0006671366661278698, policy loss: 8.710540850983133
Experience 20, Iter 65, disc loss: 0.00063494521254453, policy loss: 8.575804938473487
Experience 20, Iter 66, disc loss: 0.0006028337938139005, policy loss: 8.580053997642509
Experience 20, Iter 67, disc loss: 0.000596937357260377, policy loss: 8.283668192305543
Experience 20, Iter 68, disc loss: 0.0005443875159296659, policy loss: 8.471683739399005
Experience 20, Iter 69, disc loss: 0.000497840167900626, policy loss: 8.756710056741415
Experience 20, Iter 70, disc loss: 0.000544411712107966, policy loss: 8.73852455230126
Experience 20, Iter 71, disc loss: 0.0008128194167911802, policy loss: 8.395838584380698
Experience 20, Iter 72, disc loss: 0.0007038299166349061, policy loss: 8.534299613460554
Experience 20, Iter 73, disc loss: 0.0006111419709117305, policy loss: 8.96038238751159
Experience 20, Iter 74, disc loss: 0.0007138579854912759, policy loss: 8.79982936511683
Experience 20, Iter 75, disc loss: 0.0006626425565525342, policy loss: 8.720338976526472
Experience 20, Iter 76, disc loss: 0.0006659176806117969, policy loss: 8.42721371800976
Experience 20, Iter 77, disc loss: 0.0005934044132731327, policy loss: 8.325620000449693
Experience 20, Iter 78, disc loss: 0.0007299186803774191, policy loss: 8.987984243961058
Experience 20, Iter 79, disc loss: 0.0005282491361255321, policy loss: 8.543824791072176
Experience 20, Iter 80, disc loss: 0.0005310748542781336, policy loss: 8.637861423670364
Experience 20, Iter 81, disc loss: 0.0005159740055939471, policy loss: 9.035326452928162
Experience 20, Iter 82, disc loss: 0.0005673889407831167, policy loss: 8.45852814393699
Experience 20, Iter 83, disc loss: 0.0006007948969275635, policy loss: 8.33260692884698
Experience 20, Iter 84, disc loss: 0.0008224967593548828, policy loss: 8.42915233446726
Experience 20, Iter 85, disc loss: 0.0005696978373866091, policy loss: 8.987005013642477
Experience 20, Iter 86, disc loss: 0.0007112153952569381, policy loss: 8.927243425736718
Experience 20, Iter 87, disc loss: 0.0007137253013846546, policy loss: 8.36982071446039
Experience 20, Iter 88, disc loss: 0.0006751665604529025, policy loss: 8.442409243818295
Experience 20, Iter 89, disc loss: 0.0005182073262748161, policy loss: 8.647945746763883
Experience 20, Iter 90, disc loss: 0.0004486969661911544, policy loss: 8.909625116031512
Experience 20, Iter 91, disc loss: 0.0006487931331780511, policy loss: 8.296943250764572
Experience 20, Iter 92, disc loss: 0.000568719177519417, policy loss: 8.805391582491584
Experience 20, Iter 93, disc loss: 0.0006760120685770136, policy loss: 8.47626780372778
Experience 20, Iter 94, disc loss: 0.0009528897932146885, policy loss: 8.4148914145743
Experience 20, Iter 95, disc loss: 0.0008419259353618378, policy loss: 8.280420353301057
Experience 20, Iter 96, disc loss: 0.0007994565713009172, policy loss: 8.468374335053468
Experience 20, Iter 97, disc loss: 0.0006070037178510108, policy loss: 8.486738452828707
Experience 20, Iter 98, disc loss: 0.0007116242751313731, policy loss: 8.137174873578997
Experience 20, Iter 99, disc loss: 0.0007540467303756519, policy loss: 8.191253222419888
