Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0008],
        [0.0842],
        [0.0022]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0034, 0.3368, 0.0087], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0034, 0.3368, 0.0087])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.621
Iter 2/2000 - Loss: 1.687
Iter 3/2000 - Loss: -3.459
Iter 4/2000 - Loss: -3.084
Iter 5/2000 - Loss: -1.276
Iter 6/2000 - Loss: -1.905
Iter 7/2000 - Loss: -3.319
Iter 8/2000 - Loss: -3.841
Iter 9/2000 - Loss: -3.370
Iter 10/2000 - Loss: -2.777
Iter 11/2000 - Loss: -2.729
Iter 12/2000 - Loss: -3.146
Iter 13/2000 - Loss: -3.580
Iter 14/2000 - Loss: -3.725
Iter 15/2000 - Loss: -3.612
Iter 16/2000 - Loss: -3.447
Iter 17/2000 - Loss: -3.376
Iter 18/2000 - Loss: -3.407
Iter 19/2000 - Loss: -3.491
Iter 20/2000 - Loss: -3.606
Iter 1981/2000 - Loss: -4.061
Iter 1982/2000 - Loss: -4.061
Iter 1983/2000 - Loss: -4.060
Iter 1984/2000 - Loss: -4.059
Iter 1985/2000 - Loss: -4.058
Iter 1986/2000 - Loss: -4.056
Iter 1987/2000 - Loss: -4.054
Iter 1988/2000 - Loss: -4.052
Iter 1989/2000 - Loss: -4.050
Iter 1990/2000 - Loss: -4.050
Iter 1991/2000 - Loss: -4.051
Iter 1992/2000 - Loss: -4.054
Iter 1993/2000 - Loss: -4.059
Iter 1994/2000 - Loss: -4.062
Iter 1995/2000 - Loss: -4.065
Iter 1996/2000 - Loss: -4.065
Iter 1997/2000 - Loss: -4.063
Iter 1998/2000 - Loss: -4.061
Iter 1999/2000 - Loss: -4.059
Iter 2000/2000 - Loss: -4.057
***AFTER OPTIMATION***
Noise Variance: tensor([[9.6743e-05],
        [6.0827e-04],
        [5.7011e-02],
        [1.5605e-03]])
Lengthscale: tensor([[[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]],

        [[7.0078e-04, 8.2832e-04, 9.3366e-02, 2.1451e-03, 8.5238e-06,
          1.1950e-02]]])
Signal Variance: tensor([0.0004, 0.0024, 0.2461, 0.0063])
Estimated target variance: tensor([0.0005, 0.0034, 0.3368, 0.0087])
N: 10
Signal to noise ratio: tensor([1.9974, 2.0014, 2.0775, 2.0021])
Bound on condition number: tensor([40.8965, 41.0550, 44.1619, 41.0843])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.6115482812671031, policy loss: 0.8994293659250068
Experience 1, Iter 1, disc loss: 1.5970475017248378, policy loss: 0.8976967645275113
Experience 1, Iter 2, disc loss: 1.5849113765656795, policy loss: 0.8929000742193485
Experience 1, Iter 3, disc loss: 1.572131478386014, policy loss: 0.8897884563759968
Experience 1, Iter 4, disc loss: 1.5517632515115922, policy loss: 0.8996427879729707
Experience 1, Iter 5, disc loss: 1.5375774545393068, policy loss: 0.9004465465171083
Experience 1, Iter 6, disc loss: 1.5273858644773495, policy loss: 0.8965100995416986
Experience 1, Iter 7, disc loss: 1.5139846495628004, policy loss: 0.8987120436398348
Experience 1, Iter 8, disc loss: 1.500073933589276, policy loss: 0.9039164603040001
Experience 1, Iter 9, disc loss: 1.4923629200731514, policy loss: 0.899913391540212
Experience 1, Iter 10, disc loss: 1.4867079014193607, policy loss: 0.8928928298200995
Experience 1, Iter 11, disc loss: 1.4687562746217775, policy loss: 0.9067899505708437
Experience 1, Iter 12, disc loss: 1.4627235650445543, policy loss: 0.9017779836815278
Experience 1, Iter 13, disc loss: 1.4547353324247485, policy loss: 0.9004967449667918
Experience 1, Iter 14, disc loss: 1.451133014947149, policy loss: 0.8929978640784599
Experience 1, Iter 15, disc loss: 1.4392101353628204, policy loss: 0.8994910702789133
Experience 1, Iter 16, disc loss: 1.4304620731803042, policy loss: 0.9018574861491853
Experience 1, Iter 17, disc loss: 1.421562787937212, policy loss: 0.9062181127960401
Experience 1, Iter 18, disc loss: 1.4223009479794002, policy loss: 0.8955085801695964
Experience 1, Iter 19, disc loss: 1.4129899557921664, policy loss: 0.9031703940945579
Experience 1, Iter 20, disc loss: 1.4118627680452553, policy loss: 0.8985186369647087
Experience 1, Iter 21, disc loss: 1.4023097170681709, policy loss: 0.9079508761754168
Experience 1, Iter 22, disc loss: 1.3991289514642344, policy loss: 0.9072580242429337
Experience 1, Iter 23, disc loss: 1.3925422544181858, policy loss: 0.9118669538048731
Experience 1, Iter 24, disc loss: 1.3913421698275397, policy loss: 0.9088992266948805
Experience 1, Iter 25, disc loss: 1.386089684605389, policy loss: 0.9119406586077591
Experience 1, Iter 26, disc loss: 1.3800026032644432, policy loss: 0.9176251518736729
Experience 1, Iter 27, disc loss: 1.382402681183722, policy loss: 0.9086015242693815
Experience 1, Iter 28, disc loss: 1.3747594951580184, policy loss: 0.9150135556486496
Experience 1, Iter 29, disc loss: 1.3756381244697011, policy loss: 0.9081960577366808
Experience 1, Iter 30, disc loss: 1.363612425414963, policy loss: 0.92248015365649
Experience 1, Iter 31, disc loss: 1.3602045559888496, policy loss: 0.9241994413386161
Experience 1, Iter 32, disc loss: 1.3660161804091575, policy loss: 0.9087383979128981
Experience 1, Iter 33, disc loss: 1.355992134125831, policy loss: 0.9208992488836582
Experience 1, Iter 34, disc loss: 1.3491270648754856, policy loss: 0.9288910463485334
Experience 1, Iter 35, disc loss: 1.3470414541309412, policy loss: 0.9265824443437611
Experience 1, Iter 36, disc loss: 1.3533238222806332, policy loss: 0.9107508221532403
Experience 1, Iter 37, disc loss: 1.3502456775213092, policy loss: 0.9105923940911669
Experience 1, Iter 38, disc loss: 1.3343478901964105, policy loss: 0.9330467614170709
Experience 1, Iter 39, disc loss: 1.3341573363729786, policy loss: 0.9285570704219853
Experience 1, Iter 40, disc loss: 1.3388294299763883, policy loss: 0.9151235469693058
Experience 1, Iter 41, disc loss: 1.315080333301156, policy loss: 0.9522066707088543
Experience 1, Iter 42, disc loss: 1.3253117264008039, policy loss: 0.9274169281864655
Experience 1, Iter 43, disc loss: 1.3147962490033251, policy loss: 0.9409823141426866
Experience 1, Iter 44, disc loss: 1.3183677645339749, policy loss: 0.9287854864517381
Experience 1, Iter 45, disc loss: 1.3152677027840207, policy loss: 0.9307074109371616
Experience 1, Iter 46, disc loss: 1.300808942508364, policy loss: 0.9509018627546553
Experience 1, Iter 47, disc loss: 1.2994457877297274, policy loss: 0.9451456938329316
Experience 1, Iter 48, disc loss: 1.2994217592051305, policy loss: 0.9392834272845934
Experience 1, Iter 49, disc loss: 1.3000890645808938, policy loss: 0.9329893121668416
Experience 1, Iter 50, disc loss: 1.2942694795497927, policy loss: 0.9388576614948247
Experience 1, Iter 51, disc loss: 1.277019537042601, policy loss: 0.9611669974583368
Experience 1, Iter 52, disc loss: 1.2600525160422624, policy loss: 0.9859289340484373
Experience 1, Iter 53, disc loss: 1.2549105905843099, policy loss: 0.992697632201492
Experience 1, Iter 54, disc loss: 1.2521122311427126, policy loss: 0.9936664890658196
Experience 1, Iter 55, disc loss: 1.2674682300992055, policy loss: 0.9574463934802321
Experience 1, Iter 56, disc loss: 1.2512171260081757, policy loss: 0.9898921107389709
Experience 1, Iter 57, disc loss: 1.2657350328139536, policy loss: 0.9546160989740439
Experience 1, Iter 58, disc loss: 1.2520339963406701, policy loss: 0.9800536925854247
Experience 1, Iter 59, disc loss: 1.2318836354345275, policy loss: 1.0190479087086324
Experience 1, Iter 60, disc loss: 1.2388378361101744, policy loss: 0.9991676692424177
Experience 1, Iter 61, disc loss: 1.2108643056570996, policy loss: 1.0495352045372495
Experience 1, Iter 62, disc loss: 1.2361698922053237, policy loss: 0.9971963301856555
Experience 1, Iter 63, disc loss: 1.224238379139249, policy loss: 1.0182909185446614
Experience 1, Iter 64, disc loss: 1.2031528754613374, policy loss: 1.0536199037030394
Experience 1, Iter 65, disc loss: 1.2187585378662664, policy loss: 1.017454281025031
Experience 1, Iter 66, disc loss: 1.2115937334306783, policy loss: 1.0297691845181334
Experience 1, Iter 67, disc loss: 1.2112150550519565, policy loss: 1.0230599878084583
Experience 1, Iter 68, disc loss: 1.1933579644246979, policy loss: 1.065594081220784
Experience 1, Iter 69, disc loss: 1.2130901180678935, policy loss: 1.0165385413805776
Experience 1, Iter 70, disc loss: 1.1953683837957476, policy loss: 1.0461708776948642
Experience 1, Iter 71, disc loss: 1.1898513591436852, policy loss: 1.047150714141003
Experience 1, Iter 72, disc loss: 1.1679545229818977, policy loss: 1.1070628439683579
Experience 1, Iter 73, disc loss: 1.1776250433075777, policy loss: 1.067524462130744
Experience 1, Iter 74, disc loss: 1.161052629936544, policy loss: 1.1041449710478366
Experience 1, Iter 75, disc loss: 1.1557565303349746, policy loss: 1.1055984938872854
Experience 1, Iter 76, disc loss: 1.1822844643321455, policy loss: 1.0327608421698633
Experience 1, Iter 77, disc loss: 1.1350611863265603, policy loss: 1.1358436410170114
Experience 1, Iter 78, disc loss: 1.129577478017969, policy loss: 1.131112939862122
Experience 1, Iter 79, disc loss: 1.1368774991981982, policy loss: 1.0852103178767862
Experience 1, Iter 80, disc loss: 1.106221674446866, policy loss: 1.1405573122722046
Experience 1, Iter 81, disc loss: 1.0909589827564528, policy loss: 1.1455630084597523
Experience 1, Iter 82, disc loss: 1.080979688457428, policy loss: 1.1722170534314158
Experience 1, Iter 83, disc loss: 1.0875509651105724, policy loss: 1.1144966844295263
Experience 1, Iter 84, disc loss: 1.05634453313333, policy loss: 1.1608699828420257
Experience 1, Iter 85, disc loss: 1.047706052420857, policy loss: 1.1692441373128042
Experience 1, Iter 86, disc loss: 1.0247758705111476, policy loss: 1.1928592294658322
Experience 1, Iter 87, disc loss: 0.9940160277328276, policy loss: 1.249506335959589
Experience 1, Iter 88, disc loss: 1.0053204532043727, policy loss: 1.1734770679615838
Experience 1, Iter 89, disc loss: 1.0155492702141253, policy loss: 1.1216861501910766
Experience 1, Iter 90, disc loss: 0.9607146858553873, policy loss: 1.2526089423211264
Experience 1, Iter 91, disc loss: 0.954370313644372, policy loss: 1.2328755375863016
Experience 1, Iter 92, disc loss: 0.9249940226536544, policy loss: 1.2558550175857928
Experience 1, Iter 93, disc loss: 0.9061337408583828, policy loss: 1.2914218860163842
Experience 1, Iter 94, disc loss: 0.9093581637216689, policy loss: 1.2442416041798192
Experience 1, Iter 95, disc loss: 0.9100445350075396, policy loss: 1.1983498458748065
Experience 1, Iter 96, disc loss: 0.9077005511067509, policy loss: 1.167528614889107
Experience 1, Iter 97, disc loss: 0.8649271432624259, policy loss: 1.2688302374112845
Experience 1, Iter 98, disc loss: 0.8291820859086083, policy loss: 1.3203401286785819
Experience 1, Iter 99, disc loss: 0.818348108415869, policy loss: 1.3266868554467899
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[8.8196e-05],
        [8.1809e-04],
        [8.5553e-02],
        [2.2960e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0004, 0.0033, 0.3422, 0.0092], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0004, 0.0033, 0.3422, 0.0092])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.620
Iter 2/2000 - Loss: 4.036
Iter 3/2000 - Loss: -3.288
Iter 4/2000 - Loss: -2.792
Iter 5/2000 - Loss: -0.231
Iter 6/2000 - Loss: -1.032
Iter 7/2000 - Loss: -3.058
Iter 8/2000 - Loss: -3.942
Iter 9/2000 - Loss: -3.389
Iter 10/2000 - Loss: -2.520
Iter 11/2000 - Loss: -2.340
Iter 12/2000 - Loss: -2.851
Iter 13/2000 - Loss: -3.478
Iter 14/2000 - Loss: -3.792
Iter 15/2000 - Loss: -3.763
Iter 16/2000 - Loss: -3.548
Iter 17/2000 - Loss: -3.317
Iter 18/2000 - Loss: -3.229
Iter 19/2000 - Loss: -3.370
Iter 20/2000 - Loss: -3.614
Iter 1981/2000 - Loss: -4.137
Iter 1982/2000 - Loss: -4.137
Iter 1983/2000 - Loss: -4.138
Iter 1984/2000 - Loss: -4.140
Iter 1985/2000 - Loss: -4.142
Iter 1986/2000 - Loss: -4.144
Iter 1987/2000 - Loss: -4.144
Iter 1988/2000 - Loss: -4.144
Iter 1989/2000 - Loss: -4.143
Iter 1990/2000 - Loss: -4.142
Iter 1991/2000 - Loss: -4.141
Iter 1992/2000 - Loss: -4.139
Iter 1993/2000 - Loss: -4.137
Iter 1994/2000 - Loss: -4.134
Iter 1995/2000 - Loss: -4.131
Iter 1996/2000 - Loss: -4.127
Iter 1997/2000 - Loss: -4.124
Iter 1998/2000 - Loss: -4.123
Iter 1999/2000 - Loss: -4.125
Iter 2000/2000 - Loss: -4.130
***AFTER OPTIMATION***
Noise Variance: tensor([[6.9395e-05],
        [6.2078e-04],
        [6.1511e-02],
        [1.7418e-03]])
Lengthscale: tensor([[[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]],

        [[7.5888e-04, 1.1483e-03, 9.4166e-02, 2.2672e-03, 1.2089e-05,
          1.0207e-02]]])
Signal Variance: tensor([0.0003, 0.0025, 0.2636, 0.0070])
Estimated target variance: tensor([0.0004, 0.0033, 0.3422, 0.0092])
N: 20
Signal to noise ratio: tensor([1.9971, 2.0019, 2.0701, 2.0023])
Bound on condition number: tensor([80.7652, 81.1548, 86.7052, 81.1844])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.8123036388151854, policy loss: 1.298415481901117
Experience 2, Iter 1, disc loss: 0.7723206853282426, policy loss: 1.3784484284662457
Experience 2, Iter 2, disc loss: 0.7779969910336326, policy loss: 1.3309400275621825
Experience 2, Iter 3, disc loss: 0.7616639180072383, policy loss: 1.353898626488743
Experience 2, Iter 4, disc loss: 0.7633954790453501, policy loss: 1.3213945070660256
Experience 2, Iter 5, disc loss: 0.699456588554629, policy loss: 1.473850051133844
Experience 2, Iter 6, disc loss: 0.7125689855023044, policy loss: 1.369390736583765
Experience 2, Iter 7, disc loss: 0.6633291335324831, policy loss: 1.5120643222702517
Experience 2, Iter 8, disc loss: 0.6703005475272187, policy loss: 1.4137559752140407
Experience 2, Iter 9, disc loss: 0.6527698574344092, policy loss: 1.4524114357168747
Experience 2, Iter 10, disc loss: 0.6593826216979617, policy loss: 1.3703430068912663
Experience 2, Iter 11, disc loss: 0.6306554610859284, policy loss: 1.451438558693708
Experience 2, Iter 12, disc loss: 0.6129181405296434, policy loss: 1.4576005137092296
Experience 2, Iter 13, disc loss: 0.5825234028376788, policy loss: 1.5775319679225657
Experience 2, Iter 14, disc loss: 0.5564113306940106, policy loss: 1.5717660012665697
Experience 2, Iter 15, disc loss: 0.5550843881782775, policy loss: 1.6019342783263892
Experience 2, Iter 16, disc loss: 0.5612980590189784, policy loss: 1.5077466864036142
Experience 2, Iter 17, disc loss: 0.5253120486746818, policy loss: 1.6395960329311774
Experience 2, Iter 18, disc loss: 0.5348844945672648, policy loss: 1.5253524061965646
Experience 2, Iter 19, disc loss: 0.5149855792219318, policy loss: 1.5420139306307845
Experience 2, Iter 20, disc loss: 0.5276236769224756, policy loss: 1.4939612837516598
Experience 2, Iter 21, disc loss: 0.45464711474584774, policy loss: 1.762053960344037
Experience 2, Iter 22, disc loss: 0.44924723111578013, policy loss: 1.7654803567300026
Experience 2, Iter 23, disc loss: 0.4678444675861869, policy loss: 1.6251115903954445
Experience 2, Iter 24, disc loss: 0.4484575888414425, policy loss: 1.659057293433583
Experience 2, Iter 25, disc loss: 0.43329436269719823, policy loss: 1.685024310141467
Experience 2, Iter 26, disc loss: 0.4145383189601287, policy loss: 1.724797249557164
Experience 2, Iter 27, disc loss: 0.39245243296253773, policy loss: 1.8588690020226992
Experience 2, Iter 28, disc loss: 0.43017852116377925, policy loss: 1.6208133100082731
Experience 2, Iter 29, disc loss: 0.3774034922480569, policy loss: 1.8018689346475905
Experience 2, Iter 30, disc loss: 0.3694652657233187, policy loss: 1.8490693955196065
Experience 2, Iter 31, disc loss: 0.3604402379021005, policy loss: 1.8540890383130681
Experience 2, Iter 32, disc loss: 0.3476379914225991, policy loss: 1.9267276364078911
Experience 2, Iter 33, disc loss: 0.37658843314320173, policy loss: 1.754334240942285
Experience 2, Iter 34, disc loss: 0.34364180871822814, policy loss: 1.8539658052317922
Experience 2, Iter 35, disc loss: 0.3179603476807823, policy loss: 2.014499351395023
Experience 2, Iter 36, disc loss: 0.31692271995314225, policy loss: 1.914491202943798
Experience 2, Iter 37, disc loss: 0.328375441225662, policy loss: 1.8219838576912264
Experience 2, Iter 38, disc loss: 0.2952826524620051, policy loss: 2.071286115354713
Experience 2, Iter 39, disc loss: 0.26823178244677054, policy loss: 2.0958724218051863
Experience 2, Iter 40, disc loss: 0.279514905747068, policy loss: 2.122523997329129
Experience 2, Iter 41, disc loss: 0.29071653889465726, policy loss: 1.9342889525437572
Experience 2, Iter 42, disc loss: 0.2665299087980086, policy loss: 2.1595343270131284
Experience 2, Iter 43, disc loss: 0.26666828388667185, policy loss: 2.0825776000567497
Experience 2, Iter 44, disc loss: 0.24018321495826006, policy loss: 2.2610615283693605
Experience 2, Iter 45, disc loss: 0.2516800154532297, policy loss: 2.1833990705358426
Experience 2, Iter 46, disc loss: 0.21989639957847107, policy loss: 2.3731457956187807
Experience 2, Iter 47, disc loss: 0.24329752103374994, policy loss: 2.137038984426348
Experience 2, Iter 48, disc loss: 0.21905855070377175, policy loss: 2.2723454749109697
Experience 2, Iter 49, disc loss: 0.22540903540064142, policy loss: 2.174331134992821
Experience 2, Iter 50, disc loss: 0.20882446138876265, policy loss: 2.3979838405059466
Experience 2, Iter 51, disc loss: 0.21517770889655463, policy loss: 2.3400464542079176
Experience 2, Iter 52, disc loss: 0.20236356646367312, policy loss: 2.4276031505582987
Experience 2, Iter 53, disc loss: 0.2029348045807722, policy loss: 2.401211462861325
Experience 2, Iter 54, disc loss: 0.19914576169282522, policy loss: 2.3726158909836377
Experience 2, Iter 55, disc loss: 0.19618061145686694, policy loss: 2.3834757191613716
Experience 2, Iter 56, disc loss: 0.19442033846317883, policy loss: 2.3157636369908667
Experience 2, Iter 57, disc loss: 0.19555128195978555, policy loss: 2.317003117149693
Experience 2, Iter 58, disc loss: 0.18281192330240958, policy loss: 2.4595622525282184
Experience 2, Iter 59, disc loss: 0.1612608045578012, policy loss: 2.7799860217278036
Experience 2, Iter 60, disc loss: 0.17754192965805296, policy loss: 2.538696701255957
Experience 2, Iter 61, disc loss: 0.16048907852099062, policy loss: 2.565104376861738
Experience 2, Iter 62, disc loss: 0.16578481260522734, policy loss: 2.4847018565364505
Experience 2, Iter 63, disc loss: 0.15337335097942667, policy loss: 2.6592839160856854
Experience 2, Iter 64, disc loss: 0.15559897737016473, policy loss: 2.7168161399017867
Experience 2, Iter 65, disc loss: 0.15820191860102176, policy loss: 2.586789335970838
Experience 2, Iter 66, disc loss: 0.1560756308679421, policy loss: 2.6191463204421916
Experience 2, Iter 67, disc loss: 0.15204303370295727, policy loss: 2.61044511862037
Experience 2, Iter 68, disc loss: 0.13532752791926878, policy loss: 2.8498597184074015
Experience 2, Iter 69, disc loss: 0.12523734838525208, policy loss: 2.9585012075282138
Experience 2, Iter 70, disc loss: 0.1536917138751348, policy loss: 2.497435556078838
Experience 2, Iter 71, disc loss: 0.1418900116711601, policy loss: 2.656158908118003
Experience 2, Iter 72, disc loss: 0.12757361474344625, policy loss: 2.8201031660624594
Experience 2, Iter 73, disc loss: 0.15106705309305912, policy loss: 2.4787790118417656
Experience 2, Iter 74, disc loss: 0.1287658714363456, policy loss: 2.7938164091459785
Experience 2, Iter 75, disc loss: 0.1474127568799337, policy loss: 2.44652002499981
Experience 2, Iter 76, disc loss: 0.12496068970909886, policy loss: 2.943119592437859
Experience 2, Iter 77, disc loss: 0.13223514836647937, policy loss: 2.7359466585433947
Experience 2, Iter 78, disc loss: 0.0938208358298737, policy loss: 3.38707242897027
Experience 2, Iter 79, disc loss: 0.128485794432048, policy loss: 2.792235714903342
Experience 2, Iter 80, disc loss: 0.12661478925200464, policy loss: 2.8406992894573504
Experience 2, Iter 81, disc loss: 0.1102029529375917, policy loss: 2.8623189974186305
Experience 2, Iter 82, disc loss: 0.10727853374849637, policy loss: 3.030723438255304
Experience 2, Iter 83, disc loss: 0.10648175591779718, policy loss: 2.947737190764358
Experience 2, Iter 84, disc loss: 0.10766659851777967, policy loss: 3.013537260473039
Experience 2, Iter 85, disc loss: 0.11569546503188444, policy loss: 2.802080100395777
Experience 2, Iter 86, disc loss: 0.0963398818624153, policy loss: 3.1353946005213036
Experience 2, Iter 87, disc loss: 0.10661481799023341, policy loss: 3.1577551131100026
Experience 2, Iter 88, disc loss: 0.10466238861330905, policy loss: 2.9727528367560354
Experience 2, Iter 89, disc loss: 0.10075131990626979, policy loss: 3.1387950884785907
Experience 2, Iter 90, disc loss: 0.10210667320007188, policy loss: 2.997944339822607
Experience 2, Iter 91, disc loss: 0.09618793182661642, policy loss: 3.2847777168606744
Experience 2, Iter 92, disc loss: 0.07899526016528077, policy loss: 3.5784664780214865
Experience 2, Iter 93, disc loss: 0.08765679508020172, policy loss: 3.4549739755146494
Experience 2, Iter 94, disc loss: 0.09993575170062471, policy loss: 2.9522401991447134
Experience 2, Iter 95, disc loss: 0.08184305786624081, policy loss: 3.351344137779521
Experience 2, Iter 96, disc loss: 0.07994730078917969, policy loss: 3.239765960324327
Experience 2, Iter 97, disc loss: 0.07455656176180718, policy loss: 3.517295460175891
Experience 2, Iter 98, disc loss: 0.08200326437185296, policy loss: 3.4383057977280327
Experience 2, Iter 99, disc loss: 0.08067564574103105, policy loss: 3.2951925965925537
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0012],
        [0.0605],
        [0.0016]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0011, 0.0049, 0.2421, 0.0066], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0011, 0.0049, 0.2421, 0.0066])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.627
Iter 2/2000 - Loss: 1.781
Iter 3/2000 - Loss: -3.328
Iter 4/2000 - Loss: -2.629
Iter 5/2000 - Loss: -0.818
Iter 6/2000 - Loss: -1.288
Iter 7/2000 - Loss: -2.663
Iter 8/2000 - Loss: -3.472
Iter 9/2000 - Loss: -3.361
Iter 10/2000 - Loss: -2.788
Iter 11/2000 - Loss: -2.396
Iter 12/2000 - Loss: -2.461
Iter 13/2000 - Loss: -2.837
Iter 14/2000 - Loss: -3.222
Iter 15/2000 - Loss: -3.400
Iter 16/2000 - Loss: -3.345
Iter 17/2000 - Loss: -3.183
Iter 18/2000 - Loss: -3.066
Iter 19/2000 - Loss: -3.068
Iter 20/2000 - Loss: -3.154
Iter 1981/2000 - Loss: -3.691
Iter 1982/2000 - Loss: -3.691
Iter 1983/2000 - Loss: -3.691
Iter 1984/2000 - Loss: -3.691
Iter 1985/2000 - Loss: -3.691
Iter 1986/2000 - Loss: -3.691
Iter 1987/2000 - Loss: -3.691
Iter 1988/2000 - Loss: -3.691
Iter 1989/2000 - Loss: -3.691
Iter 1990/2000 - Loss: -3.691
Iter 1991/2000 - Loss: -3.691
Iter 1992/2000 - Loss: -3.691
Iter 1993/2000 - Loss: -3.691
Iter 1994/2000 - Loss: -3.691
Iter 1995/2000 - Loss: -3.691
Iter 1996/2000 - Loss: -3.691
Iter 1997/2000 - Loss: -3.691
Iter 1998/2000 - Loss: -3.691
Iter 1999/2000 - Loss: -3.691
Iter 2000/2000 - Loss: -3.691
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0009],
        [0.0451],
        [0.0013]])
Lengthscale: tensor([[[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]],

        [[2.8184e-03, 9.4725e-03, 6.6479e-02, 1.8719e-03, 9.6839e-06,
          3.9586e-02]]])
Signal Variance: tensor([0.0008, 0.0038, 0.1889, 0.0051])
Estimated target variance: tensor([0.0011, 0.0049, 0.2421, 0.0066])
N: 30
Signal to noise ratio: tensor([1.9995, 2.0015, 2.0477, 2.0018])
Bound on condition number: tensor([120.9361, 121.1854, 126.7970, 121.2169])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.11988473410041736, policy loss: 2.667377541328265
Experience 3, Iter 1, disc loss: 0.09967963524091318, policy loss: 2.9013419038962667
Experience 3, Iter 2, disc loss: 0.10248006477461943, policy loss: 2.9190610859783277
Experience 3, Iter 3, disc loss: 0.10066874507095344, policy loss: 2.9820312974955305
Experience 3, Iter 4, disc loss: 0.08911409749674647, policy loss: 3.10855500303158
Experience 3, Iter 5, disc loss: 0.09579038019254664, policy loss: 2.9986797600465698
Experience 3, Iter 6, disc loss: 0.09467201032513493, policy loss: 3.1708644626924283
Experience 3, Iter 7, disc loss: 0.07764487691615238, policy loss: 3.3012154958942617
Experience 3, Iter 8, disc loss: 0.07997892072863842, policy loss: 3.4799531532681756
Experience 3, Iter 9, disc loss: 0.09384816483980596, policy loss: 2.9777355984362757
Experience 3, Iter 10, disc loss: 0.09059786010557987, policy loss: 2.9905268699886554
Experience 3, Iter 11, disc loss: 0.09120749044370571, policy loss: 2.9791547535900396
Experience 3, Iter 12, disc loss: 0.08518748058758523, policy loss: 3.206593547996284
Experience 3, Iter 13, disc loss: 0.0787254045727383, policy loss: 3.2870212374446
Experience 3, Iter 14, disc loss: 0.08684187139317479, policy loss: 3.1336271789679646
Experience 3, Iter 15, disc loss: 0.09294133012442098, policy loss: 2.9603566695814356
Experience 3, Iter 16, disc loss: 0.07764693168969261, policy loss: 3.3949615461007974
Experience 3, Iter 17, disc loss: 0.07062062888002447, policy loss: 3.4171690215747113
Experience 3, Iter 18, disc loss: 0.07883086046398838, policy loss: 3.210728597897167
Experience 3, Iter 19, disc loss: 0.08121967118944204, policy loss: 3.3691792469601056
Experience 3, Iter 20, disc loss: 0.06922709196387655, policy loss: 3.4412903371106176
Experience 3, Iter 21, disc loss: 0.09020385626060105, policy loss: 2.9249056532602165
Experience 3, Iter 22, disc loss: 0.08254873159296401, policy loss: 3.1037396267462514
Experience 3, Iter 23, disc loss: 0.06967320072451524, policy loss: 3.4890360613079316
Experience 3, Iter 24, disc loss: 0.07861100520494671, policy loss: 3.34518950548901
Experience 3, Iter 25, disc loss: 0.06487426188521471, policy loss: 3.624874631180351
Experience 3, Iter 26, disc loss: 0.06607101833862304, policy loss: 3.5118789621876503
Experience 3, Iter 27, disc loss: 0.06813535676228809, policy loss: 3.2692954008193054
Experience 3, Iter 28, disc loss: 0.06734370930210966, policy loss: 3.4554960169441227
Experience 3, Iter 29, disc loss: 0.068746841341666, policy loss: 3.461144029420705
Experience 3, Iter 30, disc loss: 0.06462479070153629, policy loss: 3.516518171229097
Experience 3, Iter 31, disc loss: 0.06316960549004692, policy loss: 3.5373204175408013
Experience 3, Iter 32, disc loss: 0.04982638046365204, policy loss: 3.9049572403263406
Experience 3, Iter 33, disc loss: 0.06180027771222263, policy loss: 3.3777889219774457
Experience 3, Iter 34, disc loss: 0.06568155859587757, policy loss: 3.494490970010744
Experience 3, Iter 35, disc loss: 0.0514316993351753, policy loss: 3.667777178592398
Experience 3, Iter 36, disc loss: 0.05390323121302048, policy loss: 3.8492815365822155
Experience 3, Iter 37, disc loss: 0.054833468429435706, policy loss: 3.6870477536207233
Experience 3, Iter 38, disc loss: 0.06706423703866635, policy loss: 3.4525553543437097
Experience 3, Iter 39, disc loss: 0.054198308310262, policy loss: 3.6586175361133044
Experience 3, Iter 40, disc loss: 0.05716803410459673, policy loss: 3.6269186942711773
Experience 3, Iter 41, disc loss: 0.05343816542730648, policy loss: 3.546709449931676
Experience 3, Iter 42, disc loss: 0.06534560184878248, policy loss: 3.469706533487283
Experience 3, Iter 43, disc loss: 0.055543473590086684, policy loss: 3.664276749301455
Experience 3, Iter 44, disc loss: 0.04761498990546584, policy loss: 3.8860062838398726
Experience 3, Iter 45, disc loss: 0.051703036180157924, policy loss: 4.0002679992366215
Experience 3, Iter 46, disc loss: 0.05414853536619624, policy loss: 3.801906458199489
Experience 3, Iter 47, disc loss: 0.05918017072164931, policy loss: 3.5439434071361635
Experience 3, Iter 48, disc loss: 0.04866022879180633, policy loss: 3.9447433383071804
Experience 3, Iter 49, disc loss: 0.05177757885048618, policy loss: 3.7739237436807693
Experience 3, Iter 50, disc loss: 0.056356088208158836, policy loss: 3.604948311185053
Experience 3, Iter 51, disc loss: 0.04603960639808096, policy loss: 3.904733593591412
Experience 3, Iter 52, disc loss: 0.04881157438991024, policy loss: 3.97086602724982
Experience 3, Iter 53, disc loss: 0.0480690357870322, policy loss: 4.010283056338068
Experience 3, Iter 54, disc loss: 0.04671786538448027, policy loss: 3.901410920063631
Experience 3, Iter 55, disc loss: 0.03838294742425439, policy loss: 4.177296337035807
Experience 3, Iter 56, disc loss: 0.042307675678088845, policy loss: 4.007764134398484
Experience 3, Iter 57, disc loss: 0.0445374328876922, policy loss: 3.9956122480475997
Experience 3, Iter 58, disc loss: 0.04398282409462469, policy loss: 4.185523282165807
Experience 3, Iter 59, disc loss: 0.04533008406517551, policy loss: 4.055464238605835
Experience 3, Iter 60, disc loss: 0.032298587048115536, policy loss: 4.377497773275563
Experience 3, Iter 61, disc loss: 0.03896532271159476, policy loss: 4.1749267933656995
Experience 3, Iter 62, disc loss: 0.03853845880801526, policy loss: 4.043033624725023
Experience 3, Iter 63, disc loss: 0.040061808452195716, policy loss: 4.027192605720553
Experience 3, Iter 64, disc loss: 0.04712214629270865, policy loss: 3.832569516702149
Experience 3, Iter 65, disc loss: 0.03587192140636901, policy loss: 4.378155761664832
Experience 3, Iter 66, disc loss: 0.03842516026522713, policy loss: 4.181699830281385
Experience 3, Iter 67, disc loss: 0.052609326042054885, policy loss: 3.928088853689495
Experience 3, Iter 68, disc loss: 0.04665329306358121, policy loss: 4.081361461920672
Experience 3, Iter 69, disc loss: 0.047656718904626055, policy loss: 3.7780019119004113
Experience 3, Iter 70, disc loss: 0.03565473579413658, policy loss: 4.388248384479635
Experience 3, Iter 71, disc loss: 0.04553138035148781, policy loss: 3.905411972367485
Experience 3, Iter 72, disc loss: 0.045426896450255785, policy loss: 3.8892708155715763
Experience 3, Iter 73, disc loss: 0.03265415221461838, policy loss: 4.181625207412999
Experience 3, Iter 74, disc loss: 0.03948371392947481, policy loss: 4.173148718831841
Experience 3, Iter 75, disc loss: 0.03593751139305172, policy loss: 4.0774377723735835
Experience 3, Iter 76, disc loss: 0.03979339217371808, policy loss: 4.127056963995105
Experience 3, Iter 77, disc loss: 0.040691763076454876, policy loss: 4.0633685671792605
Experience 3, Iter 78, disc loss: 0.037037661257176384, policy loss: 4.175586282214091
Experience 3, Iter 79, disc loss: 0.03745836248292062, policy loss: 4.268368232834039
Experience 3, Iter 80, disc loss: 0.031180979644386434, policy loss: 4.620040036107912
Experience 3, Iter 81, disc loss: 0.03376128449984707, policy loss: 4.177016021741416
Experience 3, Iter 82, disc loss: 0.03126876332843079, policy loss: 4.654326208622181
Experience 3, Iter 83, disc loss: 0.02996212385656357, policy loss: 4.432895287635073
Experience 3, Iter 84, disc loss: 0.03056643457143547, policy loss: 4.174776257255982
Experience 3, Iter 85, disc loss: 0.0354762953077195, policy loss: 4.368526390510544
Experience 3, Iter 86, disc loss: 0.03319551976137193, policy loss: 4.117536355669958
Experience 3, Iter 87, disc loss: 0.03393392175708058, policy loss: 4.296515699722433
Experience 3, Iter 88, disc loss: 0.02536324125443426, policy loss: 4.616717692342472
Experience 3, Iter 89, disc loss: 0.03146300416868682, policy loss: 4.317196095268512
Experience 3, Iter 90, disc loss: 0.029501871524740972, policy loss: 4.377446922919791
Experience 3, Iter 91, disc loss: 0.028421091084680064, policy loss: 4.795137856358808
Experience 3, Iter 92, disc loss: 0.031232013247109263, policy loss: 4.587944781181312
Experience 3, Iter 93, disc loss: 0.028809739742912732, policy loss: 4.396883199730087
Experience 3, Iter 94, disc loss: 0.03215859120182325, policy loss: 4.31489831164871
Experience 3, Iter 95, disc loss: 0.028807552400591463, policy loss: 4.590713031176323
Experience 3, Iter 96, disc loss: 0.03016600130148372, policy loss: 4.486944876979443
Experience 3, Iter 97, disc loss: 0.022221383246283775, policy loss: 4.812126799929338
Experience 3, Iter 98, disc loss: 0.032456147070317035, policy loss: 4.402476093995759
Experience 3, Iter 99, disc loss: 0.02626081482605943, policy loss: 4.430915241118447
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0041],
        [0.0545],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0044, 0.0166, 0.2179, 0.0052], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0044, 0.0166, 0.2179, 0.0052])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.311
Iter 2/2000 - Loss: -1.187
Iter 3/2000 - Loss: -2.338
Iter 4/2000 - Loss: -2.273
Iter 5/2000 - Loss: -1.830
Iter 6/2000 - Loss: -1.960
Iter 7/2000 - Loss: -2.312
Iter 8/2000 - Loss: -2.438
Iter 9/2000 - Loss: -2.304
Iter 10/2000 - Loss: -2.162
Iter 11/2000 - Loss: -2.182
Iter 12/2000 - Loss: -2.301
Iter 13/2000 - Loss: -2.385
Iter 14/2000 - Loss: -2.387
Iter 15/2000 - Loss: -2.368
Iter 16/2000 - Loss: -2.373
Iter 17/2000 - Loss: -2.388
Iter 18/2000 - Loss: -2.393
Iter 19/2000 - Loss: -2.403
Iter 20/2000 - Loss: -2.445
Iter 1981/2000 - Loss: -2.530
Iter 1982/2000 - Loss: -2.530
Iter 1983/2000 - Loss: -2.530
Iter 1984/2000 - Loss: -2.530
Iter 1985/2000 - Loss: -2.530
Iter 1986/2000 - Loss: -2.530
Iter 1987/2000 - Loss: -2.530
Iter 1988/2000 - Loss: -2.530
Iter 1989/2000 - Loss: -2.530
Iter 1990/2000 - Loss: -2.530
Iter 1991/2000 - Loss: -2.530
Iter 1992/2000 - Loss: -2.530
Iter 1993/2000 - Loss: -2.530
Iter 1994/2000 - Loss: -2.530
Iter 1995/2000 - Loss: -2.530
Iter 1996/2000 - Loss: -2.530
Iter 1997/2000 - Loss: -2.530
Iter 1998/2000 - Loss: -2.530
Iter 1999/2000 - Loss: -2.530
Iter 2000/2000 - Loss: -2.530
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0032],
        [0.0411],
        [0.0010]])
Lengthscale: tensor([[[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]],

        [[6.6096e-03, 4.4402e-02, 5.5693e-02, 2.8044e-03, 2.6032e-05,
          1.7939e-01]]])
Signal Variance: tensor([0.0034, 0.0130, 0.1713, 0.0041])
Estimated target variance: tensor([0.0044, 0.0166, 0.2179, 0.0052])
N: 40
Signal to noise ratio: tensor([2.0020, 2.0112, 2.0421, 2.0017])
Bound on condition number: tensor([161.3236, 162.8029, 167.8108, 161.2746])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.03512277619402158, policy loss: 4.164519533802144
Experience 4, Iter 1, disc loss: 0.028898771555456976, policy loss: 4.574133844362882
Experience 4, Iter 2, disc loss: 0.03816102969329344, policy loss: 4.0194311422767255
Experience 4, Iter 3, disc loss: 0.03339139737964272, policy loss: 4.177001608321493
Experience 4, Iter 4, disc loss: 0.03399926716109997, policy loss: 4.3179289381955765
Experience 4, Iter 5, disc loss: 0.036733017528105, policy loss: 4.204892162155321
Experience 4, Iter 6, disc loss: 0.0275069417289529, policy loss: 4.507390347910242
Experience 4, Iter 7, disc loss: 0.03194864658950229, policy loss: 4.352732595669037
Experience 4, Iter 8, disc loss: 0.024487721431616077, policy loss: 4.803438282983752
Experience 4, Iter 9, disc loss: 0.031164336533464343, policy loss: 4.373492531739247
Experience 4, Iter 10, disc loss: 0.037937387982013016, policy loss: 4.0636714287632065
Experience 4, Iter 11, disc loss: 0.035110614091144535, policy loss: 4.212714526327625
Experience 4, Iter 12, disc loss: 0.029301018752161304, policy loss: 4.5946276580306815
Experience 4, Iter 13, disc loss: 0.03457312927893867, policy loss: 4.304596080898488
Experience 4, Iter 14, disc loss: 0.025878383243497526, policy loss: 4.903385382566587
Experience 4, Iter 15, disc loss: 0.033148026672331174, policy loss: 4.258555936444319
Experience 4, Iter 16, disc loss: 0.02567009822457217, policy loss: 4.546349423685949
Experience 4, Iter 17, disc loss: 0.026127794686611264, policy loss: 4.535597790826021
Experience 4, Iter 18, disc loss: 0.019043980428535986, policy loss: 5.145023855701476
Experience 4, Iter 19, disc loss: 0.02998834343921668, policy loss: 4.711786502383211
Experience 4, Iter 20, disc loss: 0.02255576652074651, policy loss: 4.904274028715776
Experience 4, Iter 21, disc loss: 0.027882919491378857, policy loss: 4.672238858119377
Experience 4, Iter 22, disc loss: 0.029583662866982115, policy loss: 4.275027716566106
Experience 4, Iter 23, disc loss: 0.02551997674968254, policy loss: 4.58343346959911
Experience 4, Iter 24, disc loss: 0.024464666140106346, policy loss: 4.739475096103185
Experience 4, Iter 25, disc loss: 0.024747064563346268, policy loss: 4.695323069855785
Experience 4, Iter 26, disc loss: 0.028582457920511783, policy loss: 4.540117460809808
Experience 4, Iter 27, disc loss: 0.0290557174677858, policy loss: 4.658470849160006
Experience 4, Iter 28, disc loss: 0.019485445003448853, policy loss: 4.928679345127435
Experience 4, Iter 29, disc loss: 0.02712636970852107, policy loss: 4.630929982827274
Experience 4, Iter 30, disc loss: 0.021077311080924033, policy loss: 4.885860427148196
Experience 4, Iter 31, disc loss: 0.02475933217663566, policy loss: 4.756531527332596
Experience 4, Iter 32, disc loss: 0.028486793380240967, policy loss: 4.763300690196239
Experience 4, Iter 33, disc loss: 0.02187352031116068, policy loss: 4.866403721676913
Experience 4, Iter 34, disc loss: 0.020055124377842362, policy loss: 5.1378126726515285
Experience 4, Iter 35, disc loss: 0.021509913985484207, policy loss: 4.81360953660907
Experience 4, Iter 36, disc loss: 0.0245941796265797, policy loss: 4.805218937230448
Experience 4, Iter 37, disc loss: 0.02080334745152158, policy loss: 5.180566690924211
Experience 4, Iter 38, disc loss: 0.025722428442654884, policy loss: 4.855879307621526
Experience 4, Iter 39, disc loss: 0.02589164975667125, policy loss: 4.657197672328245
Experience 4, Iter 40, disc loss: 0.021380550610935238, policy loss: 4.932721676381783
Experience 4, Iter 41, disc loss: 0.01574171972576904, policy loss: 5.1340849825100605
Experience 4, Iter 42, disc loss: 0.02075593057335219, policy loss: 5.061868770739956
Experience 4, Iter 43, disc loss: 0.019615368574346773, policy loss: 5.171198073843498
Experience 4, Iter 44, disc loss: 0.02064293983147225, policy loss: 4.862517598371763
Experience 4, Iter 45, disc loss: 0.01413492546945635, policy loss: 5.225375232613734
Experience 4, Iter 46, disc loss: 0.017380660920907714, policy loss: 5.056851449028072
Experience 4, Iter 47, disc loss: 0.017046174751419615, policy loss: 5.026576685562621
Experience 4, Iter 48, disc loss: 0.017720171452063026, policy loss: 5.189126684086258
Experience 4, Iter 49, disc loss: 0.02122459556477252, policy loss: 5.25064725744396
Experience 4, Iter 50, disc loss: 0.017915106776693092, policy loss: 4.916495726820389
Experience 4, Iter 51, disc loss: 0.016660944915392727, policy loss: 5.168298016054101
Experience 4, Iter 52, disc loss: 0.01449977106583343, policy loss: 5.274051715078346
Experience 4, Iter 53, disc loss: 0.01534646036690621, policy loss: 5.382839162116344
Experience 4, Iter 54, disc loss: 0.017308161965956678, policy loss: 5.277030027087923
Experience 4, Iter 55, disc loss: 0.02073889485942866, policy loss: 5.032102269546289
Experience 4, Iter 56, disc loss: 0.014764950471223271, policy loss: 5.511954068902801
Experience 4, Iter 57, disc loss: 0.017173954718929706, policy loss: 5.254364214972955
Experience 4, Iter 58, disc loss: 0.016716477989564313, policy loss: 5.350527690668244
Experience 4, Iter 59, disc loss: 0.014907035633507695, policy loss: 5.402565211323267
Experience 4, Iter 60, disc loss: 0.011562507233105666, policy loss: 5.5809275251441735
Experience 4, Iter 61, disc loss: 0.014088804721611947, policy loss: 5.300581711312704
Experience 4, Iter 62, disc loss: 0.013386503805373637, policy loss: 5.664775405695001
Experience 4, Iter 63, disc loss: 0.015968224136330218, policy loss: 5.370614985132412
Experience 4, Iter 64, disc loss: 0.014632750714558922, policy loss: 5.314005337791287
Experience 4, Iter 65, disc loss: 0.014257556639557951, policy loss: 5.282139206894658
Experience 4, Iter 66, disc loss: 0.009315137183308363, policy loss: 6.0469302100043
Experience 4, Iter 67, disc loss: 0.014493058780919162, policy loss: 5.361599442468224
Experience 4, Iter 68, disc loss: 0.015129268487463023, policy loss: 5.424711822725605
Experience 4, Iter 69, disc loss: 0.01400394344579668, policy loss: 5.381619781234681
Experience 4, Iter 70, disc loss: 0.013465415722205318, policy loss: 5.545146563785768
Experience 4, Iter 71, disc loss: 0.011461490710531999, policy loss: 5.648669023759136
Experience 4, Iter 72, disc loss: 0.012869527708347199, policy loss: 5.607758717114905
Experience 4, Iter 73, disc loss: 0.015124220220569385, policy loss: 5.32808436004639
Experience 4, Iter 74, disc loss: 0.011707410237482938, policy loss: 5.68214362018247
Experience 4, Iter 75, disc loss: 0.01549971847072151, policy loss: 5.591110350450348
Experience 4, Iter 76, disc loss: 0.010253102788384531, policy loss: 5.735316113656435
Experience 4, Iter 77, disc loss: 0.013073010749118283, policy loss: 5.473964135601953
Experience 4, Iter 78, disc loss: 0.013950935069919476, policy loss: 5.799288950173532
Experience 4, Iter 79, disc loss: 0.011537231296993567, policy loss: 5.683462030242743
Experience 4, Iter 80, disc loss: 0.011058745838604618, policy loss: 5.5002650290827
Experience 4, Iter 81, disc loss: 0.012593650188243469, policy loss: 5.876333568393375
Experience 4, Iter 82, disc loss: 0.009424684863804596, policy loss: 5.998649704931048
Experience 4, Iter 83, disc loss: 0.013310870685191097, policy loss: 5.534846842295034
Experience 4, Iter 84, disc loss: 0.010871704214438203, policy loss: 5.704564315265658
Experience 4, Iter 85, disc loss: 0.012603437431982176, policy loss: 5.380754035354341
Experience 4, Iter 86, disc loss: 0.011916969586679715, policy loss: 5.773980268561637
Experience 4, Iter 87, disc loss: 0.012193461386688831, policy loss: 5.278434523274431
Experience 4, Iter 88, disc loss: 0.012810856329137258, policy loss: 5.4723713454980825
Experience 4, Iter 89, disc loss: 0.01222761896141801, policy loss: 5.731676074561057
Experience 4, Iter 90, disc loss: 0.010439440234196582, policy loss: 5.899324768248649
Experience 4, Iter 91, disc loss: 0.01141064794050643, policy loss: 5.662726335712034
Experience 4, Iter 92, disc loss: 0.010748135101197341, policy loss: 5.784452976724179
Experience 4, Iter 93, disc loss: 0.011521147624664888, policy loss: 5.63212516323987
Experience 4, Iter 94, disc loss: 0.011805445073151268, policy loss: 5.807796122004718
Experience 4, Iter 95, disc loss: 0.009045074409118357, policy loss: 6.0835251885518975
Experience 4, Iter 96, disc loss: 0.009414989003545949, policy loss: 5.972783947356682
Experience 4, Iter 97, disc loss: 0.008118403283483741, policy loss: 5.968109183799919
Experience 4, Iter 98, disc loss: 0.010148714248856452, policy loss: 5.85586106838398
Experience 4, Iter 99, disc loss: 0.013279846227705732, policy loss: 5.6253095660521115
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0050],
        [0.0583],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0200, 0.2332, 0.0048], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0200, 0.2332, 0.0048])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.046
Iter 2/2000 - Loss: -1.297
Iter 3/2000 - Loss: -2.073
Iter 4/2000 - Loss: -2.087
Iter 5/2000 - Loss: -1.827
Iter 6/2000 - Loss: -1.907
Iter 7/2000 - Loss: -2.151
Iter 8/2000 - Loss: -2.276
Iter 9/2000 - Loss: -2.208
Iter 10/2000 - Loss: -2.105
Iter 11/2000 - Loss: -2.119
Iter 12/2000 - Loss: -2.199
Iter 13/2000 - Loss: -2.246
Iter 14/2000 - Loss: -2.271
Iter 15/2000 - Loss: -2.302
Iter 16/2000 - Loss: -2.284
Iter 17/2000 - Loss: -2.225
Iter 18/2000 - Loss: -2.232
Iter 19/2000 - Loss: -2.318
Iter 20/2000 - Loss: -2.373
Iter 1981/2000 - Loss: -2.402
Iter 1982/2000 - Loss: -2.402
Iter 1983/2000 - Loss: -2.402
Iter 1984/2000 - Loss: -2.402
Iter 1985/2000 - Loss: -2.402
Iter 1986/2000 - Loss: -2.402
Iter 1987/2000 - Loss: -2.402
Iter 1988/2000 - Loss: -2.402
Iter 1989/2000 - Loss: -2.402
Iter 1990/2000 - Loss: -2.402
Iter 1991/2000 - Loss: -2.402
Iter 1992/2000 - Loss: -2.402
Iter 1993/2000 - Loss: -2.402
Iter 1994/2000 - Loss: -2.402
Iter 1995/2000 - Loss: -2.402
Iter 1996/2000 - Loss: -2.402
Iter 1997/2000 - Loss: -2.402
Iter 1998/2000 - Loss: -2.402
Iter 1999/2000 - Loss: -2.402
Iter 2000/2000 - Loss: -2.402
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0039],
        [0.0441],
        [0.0009]])
Lengthscale: tensor([[[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]],

        [[7.2584e-03, 4.6448e-02, 5.3542e-02, 2.8843e-03, 2.8729e-05,
          1.9616e-01]]])
Signal Variance: tensor([0.0036, 0.0157, 0.1844, 0.0038])
Estimated target variance: tensor([0.0046, 0.0200, 0.2332, 0.0048])
N: 50
Signal to noise ratio: tensor([2.0028, 2.0070, 2.0444, 2.0016])
Bound on condition number: tensor([201.5695, 202.3944, 209.9857, 201.3131])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.010663689933547701, policy loss: 5.870911330518124
Experience 5, Iter 1, disc loss: 0.00973785277664611, policy loss: 6.080867083710796
Experience 5, Iter 2, disc loss: 0.007876821025580943, policy loss: 6.036720413869579
Experience 5, Iter 3, disc loss: 0.007500091870396734, policy loss: 6.317125827428957
Experience 5, Iter 4, disc loss: 0.008206468853540658, policy loss: 6.531291228633213
Experience 5, Iter 5, disc loss: 0.007409092815866081, policy loss: 6.284576088459159
Experience 5, Iter 6, disc loss: 0.008367491889834933, policy loss: 6.008806958259491
Experience 5, Iter 7, disc loss: 0.00939778785061975, policy loss: 6.080505744857936
Experience 5, Iter 8, disc loss: 0.008894406503871004, policy loss: 6.165233689249283
Experience 5, Iter 9, disc loss: 0.006787572980111581, policy loss: 6.5555900171646195
Experience 5, Iter 10, disc loss: 0.006874535228207364, policy loss: 6.573578597488961
Experience 5, Iter 11, disc loss: 0.0074484704344130355, policy loss: 6.262717133767421
Experience 5, Iter 12, disc loss: 0.006661908499851483, policy loss: 6.355346538872649
Experience 5, Iter 13, disc loss: 0.007469048475705983, policy loss: 6.333577834631875
Experience 5, Iter 14, disc loss: 0.007788364439687987, policy loss: 6.450489436440996
Experience 5, Iter 15, disc loss: 0.006126247943371913, policy loss: 6.467950406229337
Experience 5, Iter 16, disc loss: 0.007827673979104686, policy loss: 6.397552752710277
Experience 5, Iter 17, disc loss: 0.007881769928028104, policy loss: 6.288638916373761
Experience 5, Iter 18, disc loss: 0.005926828413690603, policy loss: 6.602250769588172
Experience 5, Iter 19, disc loss: 0.007220125466981603, policy loss: 6.294236400917333
Experience 5, Iter 20, disc loss: 0.008367251777499756, policy loss: 6.320796672878685
Experience 5, Iter 21, disc loss: 0.007173995547685029, policy loss: 6.4267151362263375
Experience 5, Iter 22, disc loss: 0.007233787220582858, policy loss: 6.247675967102721
Experience 5, Iter 23, disc loss: 0.009138941787261242, policy loss: 6.063603970763962
Experience 5, Iter 24, disc loss: 0.005949370850401854, policy loss: 6.737154938223343
Experience 5, Iter 25, disc loss: 0.007489635119037596, policy loss: 6.7752666706380875
Experience 5, Iter 26, disc loss: 0.004984243430271684, policy loss: 6.619729424516167
Experience 5, Iter 27, disc loss: 0.006338972853465701, policy loss: 6.388153980572131
Experience 5, Iter 28, disc loss: 0.006022968398604764, policy loss: 6.543983411566391
Experience 5, Iter 29, disc loss: 0.009929897101396025, policy loss: 6.038951513684693
Experience 5, Iter 30, disc loss: 0.007838541380169489, policy loss: 6.661355674582918
Experience 5, Iter 31, disc loss: 0.006170058597170943, policy loss: 6.433662579161265
Experience 5, Iter 32, disc loss: 0.006107447102049208, policy loss: 6.3717736210277645
Experience 5, Iter 33, disc loss: 0.006467640051033198, policy loss: 6.653957882519679
Experience 5, Iter 34, disc loss: 0.008600525093798817, policy loss: 6.2242189974303495
Experience 5, Iter 35, disc loss: 0.007449278035117989, policy loss: 6.58334992838391
Experience 5, Iter 36, disc loss: 0.00818345747199195, policy loss: 6.289878806056961
Experience 5, Iter 37, disc loss: 0.006213751007839089, policy loss: 7.0856164072746814
Experience 5, Iter 38, disc loss: 0.006366013289241919, policy loss: 6.498219353451096
Experience 5, Iter 39, disc loss: 0.005938367037956626, policy loss: 6.804541891442885
Experience 5, Iter 40, disc loss: 0.006079014693085513, policy loss: 6.358159501897529
Experience 5, Iter 41, disc loss: 0.0063381573301834, policy loss: 6.53949238546794
Experience 5, Iter 42, disc loss: 0.00540254392866121, policy loss: 6.921510284397976
Experience 5, Iter 43, disc loss: 0.006012144164486326, policy loss: 6.697280762307214
Experience 5, Iter 44, disc loss: 0.005210882169164653, policy loss: 6.723217236546682
Experience 5, Iter 45, disc loss: 0.007485199935675913, policy loss: 6.343994250911175
Experience 5, Iter 46, disc loss: 0.005538075762384361, policy loss: 6.654253101692175
Experience 5, Iter 47, disc loss: 0.00515803695172812, policy loss: 6.770316962654304
Experience 5, Iter 48, disc loss: 0.004747747640316337, policy loss: 6.941638610708901
Experience 5, Iter 49, disc loss: 0.005471481634631363, policy loss: 6.7701335228915545
Experience 5, Iter 50, disc loss: 0.006703196705291463, policy loss: 6.636963595775464
Experience 5, Iter 51, disc loss: 0.005930491853039748, policy loss: 6.699898851256693
Experience 5, Iter 52, disc loss: 0.004975070029014471, policy loss: 6.677155450687233
Experience 5, Iter 53, disc loss: 0.004819582092745287, policy loss: 6.830295828714661
Experience 5, Iter 54, disc loss: 0.003923855786109336, policy loss: 6.901453180693662
Experience 5, Iter 55, disc loss: 0.005381412354432845, policy loss: 6.96907252888194
Experience 5, Iter 56, disc loss: 0.006749637612948908, policy loss: 6.665139853055639
Experience 5, Iter 57, disc loss: 0.0048765626823786025, policy loss: 6.807927164217259
Experience 5, Iter 58, disc loss: 0.00627276746457826, policy loss: 6.701247817627621
Experience 5, Iter 59, disc loss: 0.004500363654996819, policy loss: 6.943473289427852
Experience 5, Iter 60, disc loss: 0.005391039057345021, policy loss: 6.696000010354398
Experience 5, Iter 61, disc loss: 0.003971074988775177, policy loss: 7.140122303868597
Experience 5, Iter 62, disc loss: 0.005382975348791403, policy loss: 6.638908286573118
Experience 5, Iter 63, disc loss: 0.005639771768288029, policy loss: 7.023438455496423
Experience 5, Iter 64, disc loss: 0.00565209794004916, policy loss: 6.720341754983499
Experience 5, Iter 65, disc loss: 0.006030997007175398, policy loss: 6.571227593911539
Experience 5, Iter 66, disc loss: 0.006262477667308947, policy loss: 6.809483162120155
Experience 5, Iter 67, disc loss: 0.004302027386304507, policy loss: 6.965683043971268
Experience 5, Iter 68, disc loss: 0.004126233943677009, policy loss: 7.194262406554995
Experience 5, Iter 69, disc loss: 0.005029212992342779, policy loss: 6.679923589287609
Experience 5, Iter 70, disc loss: 0.005135195186093287, policy loss: 6.7190848676556625
Experience 5, Iter 71, disc loss: 0.0042911043832008135, policy loss: 6.856997805422395
Experience 5, Iter 72, disc loss: 0.00507602778886364, policy loss: 6.728493609719781
Experience 5, Iter 73, disc loss: 0.005811190255484295, policy loss: 6.926648343360558
Experience 5, Iter 74, disc loss: 0.004863926666293123, policy loss: 7.081853629865575
Experience 5, Iter 75, disc loss: 0.005102293673069441, policy loss: 6.809885564472404
Experience 5, Iter 76, disc loss: 0.004993009557195393, policy loss: 6.984835869003587
Experience 5, Iter 77, disc loss: 0.004634480665785217, policy loss: 7.2821292925976815
Experience 5, Iter 78, disc loss: 0.00451290453330219, policy loss: 6.750675302779941
Experience 5, Iter 79, disc loss: 0.005050851547639892, policy loss: 6.895476057998281
Experience 5, Iter 80, disc loss: 0.0050936639310581195, policy loss: 6.9226696764197015
Experience 5, Iter 81, disc loss: 0.0038192524315557915, policy loss: 7.133773656955986
Experience 5, Iter 82, disc loss: 0.004941415539650615, policy loss: 6.668896430840996
Experience 5, Iter 83, disc loss: 0.007011531249979988, policy loss: 6.3211504718714036
Experience 5, Iter 84, disc loss: 0.004731187374660334, policy loss: 6.8714207434775965
Experience 5, Iter 85, disc loss: 0.006470412187243469, policy loss: 6.518728381917547
Experience 5, Iter 86, disc loss: 0.004944820765219727, policy loss: 7.023201870084792
Experience 5, Iter 87, disc loss: 0.005195902176791384, policy loss: 6.639768400873651
Experience 5, Iter 88, disc loss: 0.005012179729648952, policy loss: 6.807774871046044
Experience 5, Iter 89, disc loss: 0.004189849876214815, policy loss: 7.162513945041553
Experience 5, Iter 90, disc loss: 0.004587827492897697, policy loss: 6.972545244289692
Experience 5, Iter 91, disc loss: 0.0044045558788023175, policy loss: 7.014310860384011
Experience 5, Iter 92, disc loss: 0.003829227634648091, policy loss: 7.172817505881467
Experience 5, Iter 93, disc loss: 0.003702321175939327, policy loss: 7.209247100386939
Experience 5, Iter 94, disc loss: 0.004738010336415837, policy loss: 6.745506493316177
Experience 5, Iter 95, disc loss: 0.003279247057778904, policy loss: 7.355914262879823
Experience 5, Iter 96, disc loss: 0.005345976229721961, policy loss: 6.586661459601437
Experience 5, Iter 97, disc loss: 0.004398636956525634, policy loss: 6.934426736900632
Experience 5, Iter 98, disc loss: 0.003259231679029666, policy loss: 7.508683601245769
Experience 5, Iter 99, disc loss: 0.0050105991787969244, policy loss: 6.906803812006888
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0076],
        [0.0261],
        [0.1615],
        [0.0030]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0440, 0.2845, 0.1379, 0.0078, 0.0019, 1.1326]],

        [[0.0440, 0.2845, 0.1379, 0.0078, 0.0019, 1.1326]],

        [[0.0440, 0.2845, 0.1379, 0.0078, 0.0019, 1.1326]],

        [[0.0440, 0.2845, 0.1379, 0.0078, 0.0019, 1.1326]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0305, 0.1046, 0.6459, 0.0121], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0305, 0.1046, 0.6459, 0.0121])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.702
Iter 2/2000 - Loss: 0.702
Iter 3/2000 - Loss: 0.446
Iter 4/2000 - Loss: 0.495
Iter 5/2000 - Loss: 0.549
Iter 6/2000 - Loss: 0.461
Iter 7/2000 - Loss: 0.365
Iter 8/2000 - Loss: 0.334
Iter 9/2000 - Loss: 0.326
Iter 10/2000 - Loss: 0.286
Iter 11/2000 - Loss: 0.210
Iter 12/2000 - Loss: 0.129
Iter 13/2000 - Loss: 0.059
Iter 14/2000 - Loss: -0.010
Iter 15/2000 - Loss: -0.103
Iter 16/2000 - Loss: -0.234
Iter 17/2000 - Loss: -0.398
Iter 18/2000 - Loss: -0.579
Iter 19/2000 - Loss: -0.767
Iter 20/2000 - Loss: -0.963
Iter 1981/2000 - Loss: -7.772
Iter 1982/2000 - Loss: -7.772
Iter 1983/2000 - Loss: -7.772
Iter 1984/2000 - Loss: -7.772
Iter 1985/2000 - Loss: -7.772
Iter 1986/2000 - Loss: -7.772
Iter 1987/2000 - Loss: -7.772
Iter 1988/2000 - Loss: -7.772
Iter 1989/2000 - Loss: -7.772
Iter 1990/2000 - Loss: -7.772
Iter 1991/2000 - Loss: -7.772
Iter 1992/2000 - Loss: -7.772
Iter 1993/2000 - Loss: -7.772
Iter 1994/2000 - Loss: -7.772
Iter 1995/2000 - Loss: -7.772
Iter 1996/2000 - Loss: -7.772
Iter 1997/2000 - Loss: -7.772
Iter 1998/2000 - Loss: -7.772
Iter 1999/2000 - Loss: -7.772
Iter 2000/2000 - Loss: -7.772
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0018],
        [0.0004]])
Lengthscale: tensor([[[18.2222,  8.9231, 35.9230, 13.7307, 12.3921, 51.4199]],

        [[22.1275, 39.3632, 10.2730,  1.4075,  0.8914, 10.0281]],

        [[ 8.3340, 44.1455, 14.6299,  0.8347,  3.4187, 15.2087]],

        [[24.4440, 46.6932, 13.6841,  3.7520, 11.9341, 32.4163]]])
Signal Variance: tensor([0.1652, 0.3993, 9.3571, 0.4476])
Estimated target variance: tensor([0.0305, 0.1046, 0.6459, 0.0121])
N: 60
Signal to noise ratio: tensor([24.2642, 40.7023, 71.7932, 34.7895])
Bound on condition number: tensor([ 35325.9953,  99401.4818, 309256.5044,  72619.6440])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.001587616903705567, policy loss: 28.740653031425893
Experience 6, Iter 1, disc loss: 0.001579644536020269, policy loss: 27.742347546809775
Experience 6, Iter 2, disc loss: 0.0015711422856800458, policy loss: 25.197532146623843
Experience 6, Iter 3, disc loss: 0.0015621728802758458, policy loss: 21.829453128772442
Experience 6, Iter 4, disc loss: 0.0015529569715601913, policy loss: 18.18331127319425
Experience 6, Iter 5, disc loss: 0.0015459961123982693, policy loss: 13.842880065649755
Experience 6, Iter 6, disc loss: 0.001566977223376301, policy loss: 10.839079437303978
Experience 6, Iter 7, disc loss: 0.0017957228592312114, policy loss: 8.50334929653552
Experience 6, Iter 8, disc loss: 0.0024093741270381636, policy loss: 7.154255706618452
Experience 6, Iter 9, disc loss: 0.0034354345055092354, policy loss: 6.38554088797475
Experience 6, Iter 10, disc loss: 0.005866714359132078, policy loss: 5.670483050062694
Experience 6, Iter 11, disc loss: 0.010034052455546935, policy loss: 4.998383999808011
Experience 6, Iter 12, disc loss: 0.010378536702087715, policy loss: 5.022729092131184
Experience 6, Iter 13, disc loss: 0.012231847994199402, policy loss: 4.782116420887534
Experience 6, Iter 14, disc loss: 0.01085580023783619, policy loss: 4.867905615380003
Experience 6, Iter 15, disc loss: 0.011947790011642452, policy loss: 4.823401941758648
Experience 6, Iter 16, disc loss: 0.010979872671275134, policy loss: 4.836136636187155
Experience 6, Iter 17, disc loss: 0.011555404696731532, policy loss: 4.888646839790408
Experience 6, Iter 18, disc loss: 0.011920362792494806, policy loss: 4.796045735972598
Experience 6, Iter 19, disc loss: 0.011328455080825773, policy loss: 4.868825188563093
Experience 6, Iter 20, disc loss: 0.012665422717988869, policy loss: 4.695977105929458
Experience 6, Iter 21, disc loss: 0.01373496524009571, policy loss: 4.632234206667651
Experience 6, Iter 22, disc loss: 0.012010515212764573, policy loss: 4.740400442918098
Experience 6, Iter 23, disc loss: 0.01350798441148059, policy loss: 4.623319067890132
Experience 6, Iter 24, disc loss: 0.01438024529506322, policy loss: 4.583732809952672
Experience 6, Iter 25, disc loss: 0.012211411653720536, policy loss: 4.727720056111236
Experience 6, Iter 26, disc loss: 0.012429603783990916, policy loss: 4.729000143358903
Experience 6, Iter 27, disc loss: 0.01168501918705921, policy loss: 4.76745252677503
Experience 6, Iter 28, disc loss: 0.014004336798066896, policy loss: 4.6380217873129
Experience 6, Iter 29, disc loss: 0.011379587629114923, policy loss: 4.797652380370899
Experience 6, Iter 30, disc loss: 0.00959841429117229, policy loss: 5.005164861350876
Experience 6, Iter 31, disc loss: 0.011212113423105723, policy loss: 4.78848289917185
Experience 6, Iter 32, disc loss: 0.009330968079485759, policy loss: 5.007620125811263
Experience 6, Iter 33, disc loss: 0.010365414256343068, policy loss: 4.90429848072428
Experience 6, Iter 34, disc loss: 0.010158638768162582, policy loss: 4.920270186762378
Experience 6, Iter 35, disc loss: 0.009543389354591737, policy loss: 5.042336932415707
Experience 6, Iter 36, disc loss: 0.008296163473147374, policy loss: 5.111817685610682
Experience 6, Iter 37, disc loss: 0.009214742625788598, policy loss: 5.007993500074835
Experience 6, Iter 38, disc loss: 0.010283597731071077, policy loss: 4.967661864890072
Experience 6, Iter 39, disc loss: 0.00966344881452443, policy loss: 4.975251687893564
Experience 6, Iter 40, disc loss: 0.010035629402538162, policy loss: 4.899581755858561
Experience 6, Iter 41, disc loss: 0.009507786683478436, policy loss: 4.970944131943183
Experience 6, Iter 42, disc loss: 0.01026182493054926, policy loss: 4.904896905747524
Experience 6, Iter 43, disc loss: 0.010549597765457646, policy loss: 4.8198203749106
Experience 6, Iter 44, disc loss: 0.010107682805683648, policy loss: 4.890207409729866
Experience 6, Iter 45, disc loss: 0.01302841243444454, policy loss: 4.608144149579662
Experience 6, Iter 46, disc loss: 0.012806739561414466, policy loss: 4.668446995563013
Experience 6, Iter 47, disc loss: 0.01232157738132511, policy loss: 4.65857496869831
Experience 6, Iter 48, disc loss: 0.01442570962864586, policy loss: 4.576847069058295
Experience 6, Iter 49, disc loss: 0.014872842412835133, policy loss: 4.582015439349284
Experience 6, Iter 50, disc loss: 0.013546855505938859, policy loss: 4.7697347826001275
Experience 6, Iter 51, disc loss: 0.013439873833727462, policy loss: 4.705588760277142
Experience 6, Iter 52, disc loss: 0.014142136754279905, policy loss: 4.716849319231657
Experience 6, Iter 53, disc loss: 0.015044441708436655, policy loss: 4.69920127688696
Experience 6, Iter 54, disc loss: 0.013362567510861336, policy loss: 4.740635628620906
Experience 6, Iter 55, disc loss: 0.013594403069372048, policy loss: 4.741184198308449
Experience 6, Iter 56, disc loss: 0.016223344061694317, policy loss: 4.600127493926518
Experience 6, Iter 57, disc loss: 0.013365024627816366, policy loss: 4.7856826201334
Experience 6, Iter 58, disc loss: 0.011935997295290364, policy loss: 4.825882108860141
Experience 6, Iter 59, disc loss: 0.014320287674012867, policy loss: 4.551781219473871
Experience 6, Iter 60, disc loss: 0.01353365730631145, policy loss: 4.6237178771229885
Experience 6, Iter 61, disc loss: 0.011686039143329798, policy loss: 4.834160661713128
Experience 6, Iter 62, disc loss: 0.01489383120675176, policy loss: 4.556432031477807
Experience 6, Iter 63, disc loss: 0.013105659384121693, policy loss: 4.710371822181996
Experience 6, Iter 64, disc loss: 0.013623813351064586, policy loss: 4.667405427299935
Experience 6, Iter 65, disc loss: 0.012879563491782285, policy loss: 4.850134253222848
Experience 6, Iter 66, disc loss: 0.014282157141157135, policy loss: 4.770260051192403
Experience 6, Iter 67, disc loss: 0.014048139177060406, policy loss: 4.709494552586542
Experience 6, Iter 68, disc loss: 0.012704983993914183, policy loss: 4.846689332962147
Experience 6, Iter 69, disc loss: 0.01249664998726423, policy loss: 4.812733982053956
Experience 6, Iter 70, disc loss: 0.012409274043892105, policy loss: 4.858551649628753
Experience 6, Iter 71, disc loss: 0.012476204993784691, policy loss: 4.859445333513261
Experience 6, Iter 72, disc loss: 0.0114146017043389, policy loss: 4.9981785298166495
Experience 6, Iter 73, disc loss: 0.011133016832147086, policy loss: 4.967178506375561
Experience 6, Iter 74, disc loss: 0.01138339726988746, policy loss: 4.9416623476298005
Experience 6, Iter 75, disc loss: 0.012343793630655218, policy loss: 4.807096209810475
Experience 6, Iter 76, disc loss: 0.011665587193379078, policy loss: 4.905332135545998
Experience 6, Iter 77, disc loss: 0.010871584759341621, policy loss: 5.106287392125136
Experience 6, Iter 78, disc loss: 0.01060632264966786, policy loss: 5.132616770538641
Experience 6, Iter 79, disc loss: 0.012167850515477292, policy loss: 5.119303208629003
Experience 6, Iter 80, disc loss: 0.012063233437365848, policy loss: 4.901732265277172
Experience 6, Iter 81, disc loss: 0.010834135549932216, policy loss: 4.958619791775617
Experience 6, Iter 82, disc loss: 0.009397975204458252, policy loss: 5.186632720272556
Experience 6, Iter 83, disc loss: 0.01184715305001273, policy loss: 4.885220541187204
Experience 6, Iter 84, disc loss: 0.009864474482811792, policy loss: 5.182257543149847
Experience 6, Iter 85, disc loss: 0.009214483886285518, policy loss: 5.184526316447807
Experience 6, Iter 86, disc loss: 0.010045469300756765, policy loss: 5.131031133443433
Experience 6, Iter 87, disc loss: 0.01036726813307988, policy loss: 5.05418873230366
Experience 6, Iter 88, disc loss: 0.011192438098344686, policy loss: 4.930610629116956
Experience 6, Iter 89, disc loss: 0.009927601164089039, policy loss: 5.146197328416231
Experience 6, Iter 90, disc loss: 0.010001903955508238, policy loss: 5.189671965866076
Experience 6, Iter 91, disc loss: 0.011077048470124619, policy loss: 4.980516162989132
Experience 6, Iter 92, disc loss: 0.010763103997801237, policy loss: 4.984149707791913
Experience 6, Iter 93, disc loss: 0.01209979202076299, policy loss: 4.9409464721798795
Experience 6, Iter 94, disc loss: 0.01068387251585017, policy loss: 5.084800193487363
Experience 6, Iter 95, disc loss: 0.011116620671845417, policy loss: 4.999183413989934
Experience 6, Iter 96, disc loss: 0.010386794314078868, policy loss: 5.106617986922673
Experience 6, Iter 97, disc loss: 0.011574761736113418, policy loss: 4.930596346821311
Experience 6, Iter 98, disc loss: 0.01127293639142883, policy loss: 5.0553358282703735
Experience 6, Iter 99, disc loss: 0.010488048980033126, policy loss: 5.284518693375507
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0071],
        [0.0242],
        [0.1398],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0397, 0.2609, 0.1188, 0.0072, 0.0017, 1.0516]],

        [[0.0397, 0.2609, 0.1188, 0.0072, 0.0017, 1.0516]],

        [[0.0397, 0.2609, 0.1188, 0.0072, 0.0017, 1.0516]],

        [[0.0397, 0.2609, 0.1188, 0.0072, 0.0017, 1.0516]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0282, 0.0969, 0.5594, 0.0105], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0282, 0.0969, 0.5594, 0.0105])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.405
Iter 2/2000 - Loss: 0.479
Iter 3/2000 - Loss: 0.215
Iter 4/2000 - Loss: 0.237
Iter 5/2000 - Loss: 0.300
Iter 6/2000 - Loss: 0.236
Iter 7/2000 - Loss: 0.135
Iter 8/2000 - Loss: 0.066
Iter 9/2000 - Loss: 0.045
Iter 10/2000 - Loss: 0.033
Iter 11/2000 - Loss: -0.016
Iter 12/2000 - Loss: -0.105
Iter 13/2000 - Loss: -0.210
Iter 14/2000 - Loss: -0.313
Iter 15/2000 - Loss: -0.418
Iter 16/2000 - Loss: -0.544
Iter 17/2000 - Loss: -0.702
Iter 18/2000 - Loss: -0.889
Iter 19/2000 - Loss: -1.096
Iter 20/2000 - Loss: -1.318
Iter 1981/2000 - Loss: -7.973
Iter 1982/2000 - Loss: -7.973
Iter 1983/2000 - Loss: -7.973
Iter 1984/2000 - Loss: -7.973
Iter 1985/2000 - Loss: -7.973
Iter 1986/2000 - Loss: -7.973
Iter 1987/2000 - Loss: -7.973
Iter 1988/2000 - Loss: -7.974
Iter 1989/2000 - Loss: -7.974
Iter 1990/2000 - Loss: -7.974
Iter 1991/2000 - Loss: -7.974
Iter 1992/2000 - Loss: -7.974
Iter 1993/2000 - Loss: -7.974
Iter 1994/2000 - Loss: -7.974
Iter 1995/2000 - Loss: -7.974
Iter 1996/2000 - Loss: -7.974
Iter 1997/2000 - Loss: -7.974
Iter 1998/2000 - Loss: -7.974
Iter 1999/2000 - Loss: -7.974
Iter 2000/2000 - Loss: -7.974
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0022],
        [0.0004]])
Lengthscale: tensor([[[17.1329,  9.2410, 31.8154, 12.0799, 12.1980, 48.1858]],

        [[21.4369, 36.9482, 10.2244,  1.4110,  0.9929, 11.7585]],

        [[18.1409, 42.7902, 10.6099,  0.8870,  3.3428, 13.1899]],

        [[23.6345, 45.2779, 13.5469,  3.7412, 11.4145, 34.5418]]])
Signal Variance: tensor([0.1752, 0.4990, 7.5847, 0.4387])
Estimated target variance: tensor([0.0282, 0.0969, 0.5594, 0.0105])
N: 70
Signal to noise ratio: tensor([24.5159, 46.9239, 58.9289, 34.8337])
Bound on condition number: tensor([ 42073.0861, 154130.7921, 243084.0854,  84937.8643])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.008383097353562146, policy loss: 5.5661342506060985
Experience 7, Iter 1, disc loss: 0.009879156625747666, policy loss: 5.215191826431773
Experience 7, Iter 2, disc loss: 0.009675235785648478, policy loss: 5.200990862700771
Experience 7, Iter 3, disc loss: 0.007900076247380289, policy loss: 5.51876055904021
Experience 7, Iter 4, disc loss: 0.00777640004674885, policy loss: 5.545368732201004
Experience 7, Iter 5, disc loss: 0.007595615599005894, policy loss: 5.622509320956757
Experience 7, Iter 6, disc loss: 0.007382362240566318, policy loss: 5.549108486910577
Experience 7, Iter 7, disc loss: 0.00782924201107672, policy loss: 5.442284365546078
Experience 7, Iter 8, disc loss: 0.006599712827374994, policy loss: 5.66749100417238
Experience 7, Iter 9, disc loss: 0.007771710275746259, policy loss: 5.489432613466986
Experience 7, Iter 10, disc loss: 0.008047091109407137, policy loss: 5.394883512120897
Experience 7, Iter 11, disc loss: 0.007411904943419594, policy loss: 5.560772996332728
Experience 7, Iter 12, disc loss: 0.006474932513541309, policy loss: 5.659386150393396
Experience 7, Iter 13, disc loss: 0.007951149324845065, policy loss: 5.348793288195356
Experience 7, Iter 14, disc loss: 0.007341816499626208, policy loss: 5.630257106705535
Experience 7, Iter 15, disc loss: 0.0075044213760113755, policy loss: 5.501810779460246
Experience 7, Iter 16, disc loss: 0.006401479055622087, policy loss: 5.777593334151872
Experience 7, Iter 17, disc loss: 0.007574262023437722, policy loss: 5.5798885323665655
Experience 7, Iter 18, disc loss: 0.006456142150165912, policy loss: 5.840992955027591
Experience 7, Iter 19, disc loss: 0.008211151903793795, policy loss: 5.386935911988865
Experience 7, Iter 20, disc loss: 0.007774983037000477, policy loss: 5.422995297388617
Experience 7, Iter 21, disc loss: 0.00752243140541348, policy loss: 5.433815428930337
Experience 7, Iter 22, disc loss: 0.007900925583688975, policy loss: 5.555574386804111
Experience 7, Iter 23, disc loss: 0.008457836400231747, policy loss: 5.372913653463299
Experience 7, Iter 24, disc loss: 0.007331142619771059, policy loss: 5.536219860664874
Experience 7, Iter 25, disc loss: 0.006651262584611534, policy loss: 5.964595701860832
Experience 7, Iter 26, disc loss: 0.00905205923106878, policy loss: 5.405257655873239
Experience 7, Iter 27, disc loss: 0.008893301398632654, policy loss: 5.564301262658336
Experience 7, Iter 28, disc loss: 0.006076607532420802, policy loss: 6.038488326209855
Experience 7, Iter 29, disc loss: 0.00862863685973819, policy loss: 5.326460888473928
Experience 7, Iter 30, disc loss: 0.007003013857410063, policy loss: 5.863556034354838
Experience 7, Iter 31, disc loss: 0.007568063051259997, policy loss: 5.567186626731172
Experience 7, Iter 32, disc loss: 0.0070011345005203465, policy loss: 5.811996450984039
Experience 7, Iter 33, disc loss: 0.006720384762771337, policy loss: 5.7698015064046
Experience 7, Iter 34, disc loss: 0.007097176286553593, policy loss: 5.7847035283218045
Experience 7, Iter 35, disc loss: 0.008807231271338579, policy loss: 5.481369930390884
Experience 7, Iter 36, disc loss: 0.00716038739699294, policy loss: 5.613095456291954
Experience 7, Iter 37, disc loss: 0.00632836159725016, policy loss: 5.795970135341792
Experience 7, Iter 38, disc loss: 0.0074214437434966425, policy loss: 5.653025554491882
Experience 7, Iter 39, disc loss: 0.006695750751751321, policy loss: 5.704927158092631
Experience 7, Iter 40, disc loss: 0.006087446062069014, policy loss: 5.902357789432097
Experience 7, Iter 41, disc loss: 0.007076056152584883, policy loss: 5.718441449144255
Experience 7, Iter 42, disc loss: 0.006415231080899466, policy loss: 5.89127867410886
Experience 7, Iter 43, disc loss: 0.006481364082419104, policy loss: 5.814382145715223
Experience 7, Iter 44, disc loss: 0.007649478176581518, policy loss: 5.872170575364355
Experience 7, Iter 45, disc loss: 0.007109476517899539, policy loss: 5.7561319715542
Experience 7, Iter 46, disc loss: 0.005909630311094051, policy loss: 5.97127730013754
Experience 7, Iter 47, disc loss: 0.006229383212327515, policy loss: 5.838038580317356
Experience 7, Iter 48, disc loss: 0.006234278362739626, policy loss: 5.944367550079864
Experience 7, Iter 49, disc loss: 0.006764938726986248, policy loss: 5.769374315035551
Experience 7, Iter 50, disc loss: 0.006275062158330195, policy loss: 5.93296247918517
Experience 7, Iter 51, disc loss: 0.007787248666766692, policy loss: 5.955599680115203
Experience 7, Iter 52, disc loss: 0.005503009674798882, policy loss: 6.017334203072897
Experience 7, Iter 53, disc loss: 0.006017049362323872, policy loss: 5.996656937037815
Experience 7, Iter 54, disc loss: 0.005113385955238728, policy loss: 6.318373140995137
Experience 7, Iter 55, disc loss: 0.005182796416961465, policy loss: 6.118512650299168
Experience 7, Iter 56, disc loss: 0.0053941021756022875, policy loss: 6.018657998225753
Experience 7, Iter 57, disc loss: 0.005392105644477006, policy loss: 6.233730174692667
Experience 7, Iter 58, disc loss: 0.005646250567043504, policy loss: 6.074527631834953
Experience 7, Iter 59, disc loss: 0.005994675972118823, policy loss: 5.921069026013937
Experience 7, Iter 60, disc loss: 0.0056681413954042525, policy loss: 5.988362259760457
Experience 7, Iter 61, disc loss: 0.005635867518424044, policy loss: 6.19619062987116
Experience 7, Iter 62, disc loss: 0.005839171029768704, policy loss: 6.089054045857079
Experience 7, Iter 63, disc loss: 0.005388844518221265, policy loss: 6.274147771238617
Experience 7, Iter 64, disc loss: 0.004907796936845215, policy loss: 6.077552411169371
Experience 7, Iter 65, disc loss: 0.004365800189110177, policy loss: 6.394344276580304
Experience 7, Iter 66, disc loss: 0.004499631759768408, policy loss: 6.316152538529565
Experience 7, Iter 67, disc loss: 0.0035981277801730447, policy loss: 6.683149581766552
Experience 7, Iter 68, disc loss: 0.004966980384776172, policy loss: 6.198729107654751
Experience 7, Iter 69, disc loss: 0.004721420245173748, policy loss: 6.250283868159974
Experience 7, Iter 70, disc loss: 0.00509040366754444, policy loss: 6.113728605680532
Experience 7, Iter 71, disc loss: 0.005067975697353719, policy loss: 6.147529826063314
Experience 7, Iter 72, disc loss: 0.005210838720829514, policy loss: 6.09292884966979
Experience 7, Iter 73, disc loss: 0.005118937575610881, policy loss: 6.210278781074257
Experience 7, Iter 74, disc loss: 0.0046298156264738466, policy loss: 6.750258752636585
Experience 7, Iter 75, disc loss: 0.005483279661797985, policy loss: 6.195237111129048
Experience 7, Iter 76, disc loss: 0.004458734445943427, policy loss: 6.338468229577528
Experience 7, Iter 77, disc loss: 0.005803184927968102, policy loss: 5.951352095026889
Experience 7, Iter 78, disc loss: 0.004521094903525938, policy loss: 6.290712930039908
Experience 7, Iter 79, disc loss: 0.00469751015657346, policy loss: 6.2235429001818385
Experience 7, Iter 80, disc loss: 0.004698327470199465, policy loss: 6.388140024688771
Experience 7, Iter 81, disc loss: 0.00471361611308041, policy loss: 6.2134516251534615
Experience 7, Iter 82, disc loss: 0.0049558571569967794, policy loss: 6.180911689644829
Experience 7, Iter 83, disc loss: 0.004309464652745164, policy loss: 6.454469223362701
Experience 7, Iter 84, disc loss: 0.0044715702905335905, policy loss: 6.414911135289063
Experience 7, Iter 85, disc loss: 0.00470222565269208, policy loss: 6.263082431034903
Experience 7, Iter 86, disc loss: 0.004809197713644523, policy loss: 6.170822862882382
Experience 7, Iter 87, disc loss: 0.005115217972129831, policy loss: 6.1197777307353025
Experience 7, Iter 88, disc loss: 0.004479103603418445, policy loss: 6.202259518646096
Experience 7, Iter 89, disc loss: 0.004482883163793422, policy loss: 6.324682088190262
Experience 7, Iter 90, disc loss: 0.004061299029652553, policy loss: 6.656756357742353
Experience 7, Iter 91, disc loss: 0.004770319964096296, policy loss: 6.299012018368112
Experience 7, Iter 92, disc loss: 0.004518834172760098, policy loss: 6.63338152435108
Experience 7, Iter 93, disc loss: 0.004723979737488051, policy loss: 6.302175457787768
Experience 7, Iter 94, disc loss: 0.004687260180153238, policy loss: 6.225841396165153
Experience 7, Iter 95, disc loss: 0.004078686738443936, policy loss: 6.384415838896486
Experience 7, Iter 96, disc loss: 0.0047772236818327506, policy loss: 6.345248676410284
Experience 7, Iter 97, disc loss: 0.004532581568352147, policy loss: 6.25655758370227
Experience 7, Iter 98, disc loss: 0.004415903141841826, policy loss: 6.390098354486401
Experience 7, Iter 99, disc loss: 0.004580599692993377, policy loss: 6.395952661058003
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0066],
        [0.0258],
        [0.1737],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0369, 0.2438, 0.1236, 0.0070, 0.0015, 1.0345]],

        [[0.0369, 0.2438, 0.1236, 0.0070, 0.0015, 1.0345]],

        [[0.0369, 0.2438, 0.1236, 0.0070, 0.0015, 1.0345]],

        [[0.0369, 0.2438, 0.1236, 0.0070, 0.0015, 1.0345]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0264, 0.1032, 0.6947, 0.0105], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0264, 0.1032, 0.6947, 0.0105])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.452
Iter 2/2000 - Loss: 0.568
Iter 3/2000 - Loss: 0.305
Iter 4/2000 - Loss: 0.326
Iter 5/2000 - Loss: 0.407
Iter 6/2000 - Loss: 0.349
Iter 7/2000 - Loss: 0.230
Iter 8/2000 - Loss: 0.166
Iter 9/2000 - Loss: 0.176
Iter 10/2000 - Loss: 0.185
Iter 11/2000 - Loss: 0.132
Iter 12/2000 - Loss: 0.028
Iter 13/2000 - Loss: -0.078
Iter 14/2000 - Loss: -0.161
Iter 15/2000 - Loss: -0.241
Iter 16/2000 - Loss: -0.351
Iter 17/2000 - Loss: -0.504
Iter 18/2000 - Loss: -0.689
Iter 19/2000 - Loss: -0.887
Iter 20/2000 - Loss: -1.093
Iter 1981/2000 - Loss: -8.011
Iter 1982/2000 - Loss: -8.011
Iter 1983/2000 - Loss: -8.011
Iter 1984/2000 - Loss: -8.011
Iter 1985/2000 - Loss: -8.011
Iter 1986/2000 - Loss: -8.011
Iter 1987/2000 - Loss: -8.011
Iter 1988/2000 - Loss: -8.011
Iter 1989/2000 - Loss: -8.011
Iter 1990/2000 - Loss: -8.011
Iter 1991/2000 - Loss: -8.011
Iter 1992/2000 - Loss: -8.011
Iter 1993/2000 - Loss: -8.011
Iter 1994/2000 - Loss: -8.011
Iter 1995/2000 - Loss: -8.011
Iter 1996/2000 - Loss: -8.011
Iter 1997/2000 - Loss: -8.011
Iter 1998/2000 - Loss: -8.011
Iter 1999/2000 - Loss: -8.011
Iter 2000/2000 - Loss: -8.011
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[17.7502,  9.7904, 31.4057, 12.7103, 11.6420, 44.9224]],

        [[19.3664, 34.7364, 19.5207,  1.0658,  1.3729, 19.6186]],

        [[20.5590, 35.7925, 21.9913,  0.7738,  5.1680, 23.3975]],

        [[22.9113, 40.4437, 15.2163,  4.0456,  9.7808, 44.3061]]])
Signal Variance: tensor([ 0.1905,  1.2932, 16.2673,  0.5157])
Estimated target variance: tensor([0.0264, 0.1032, 0.6947, 0.0105])
N: 80
Signal to noise ratio: tensor([25.4080, 74.4937, 81.1690, 38.7194])
Bound on condition number: tensor([ 51646.1774, 443946.4430, 527073.7644, 119936.2762])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.0031724713088958907, policy loss: 6.768197294529585
Experience 8, Iter 1, disc loss: 0.0031364524481716375, policy loss: 6.84574792411924
Experience 8, Iter 2, disc loss: 0.003308802461616544, policy loss: 6.781259208424828
Experience 8, Iter 3, disc loss: 0.003111114806780944, policy loss: 6.917973607387356
Experience 8, Iter 4, disc loss: 0.0033233218331206946, policy loss: 6.822050704792218
Experience 8, Iter 5, disc loss: 0.002912739252663081, policy loss: 6.9191906495731015
Experience 8, Iter 6, disc loss: 0.0031427570308820794, policy loss: 6.8897472697081215
Experience 8, Iter 7, disc loss: 0.0031883260679628905, policy loss: 6.827152988846062
Experience 8, Iter 8, disc loss: 0.0030086272331523193, policy loss: 6.763531626913139
Experience 8, Iter 9, disc loss: 0.002765913173257132, policy loss: 7.07538882187844
Experience 8, Iter 10, disc loss: 0.0033569285957261307, policy loss: 6.583238639684811
Experience 8, Iter 11, disc loss: 0.0031717499795806065, policy loss: 6.558956891174194
Experience 8, Iter 12, disc loss: 0.0030563362382988544, policy loss: 6.75169612129911
Experience 8, Iter 13, disc loss: 0.0026570904544670907, policy loss: 7.129897377425253
Experience 8, Iter 14, disc loss: 0.003002556112622328, policy loss: 6.8507389569708455
Experience 8, Iter 15, disc loss: 0.00283525434893004, policy loss: 6.984418479936609
Experience 8, Iter 16, disc loss: 0.002968604403742504, policy loss: 6.825754830785022
Experience 8, Iter 17, disc loss: 0.0031613218267781892, policy loss: 6.570721730800412
Experience 8, Iter 18, disc loss: 0.0029083671898742346, policy loss: 6.803412872302182
Experience 8, Iter 19, disc loss: 0.0028507240150712873, policy loss: 6.763613369038376
Experience 8, Iter 20, disc loss: 0.003034670195461878, policy loss: 6.633604378981395
Experience 8, Iter 21, disc loss: 0.0029136726569337105, policy loss: 6.811891177248718
Experience 8, Iter 22, disc loss: 0.002993043722719803, policy loss: 6.674847441530947
Experience 8, Iter 23, disc loss: 0.0029993397905264656, policy loss: 6.717479752172501
Experience 8, Iter 24, disc loss: 0.0028921195719826785, policy loss: 6.819769170738927
Experience 8, Iter 25, disc loss: 0.003194443216417069, policy loss: 6.566017309162749
Experience 8, Iter 26, disc loss: 0.0026047483534112604, policy loss: 7.08040412512608
Experience 8, Iter 27, disc loss: 0.0030735702821092017, policy loss: 6.593363812843306
Experience 8, Iter 28, disc loss: 0.0025591505412389344, policy loss: 7.01787975303264
Experience 8, Iter 29, disc loss: 0.0029908625919804525, policy loss: 6.795342520159171
Experience 8, Iter 30, disc loss: 0.0030323623744681416, policy loss: 6.574131383357247
Experience 8, Iter 31, disc loss: 0.0031224785602574218, policy loss: 6.554405300359948
Experience 8, Iter 32, disc loss: 0.0025686108280104127, policy loss: 6.970894477758444
Experience 8, Iter 33, disc loss: 0.0030244777956925113, policy loss: 6.70317824087264
Experience 8, Iter 34, disc loss: 0.0025573787102657975, policy loss: 7.017120413114142
Experience 8, Iter 35, disc loss: 0.0030151524475799346, policy loss: 6.698917381846991
Experience 8, Iter 36, disc loss: 0.0030496973391755107, policy loss: 6.764516995011109
Experience 8, Iter 37, disc loss: 0.0026909767899563, policy loss: 6.999995581102134
Experience 8, Iter 38, disc loss: 0.0028130745055308503, policy loss: 6.970282966964198
Experience 8, Iter 39, disc loss: 0.0027154027967586656, policy loss: 6.901525471750545
Experience 8, Iter 40, disc loss: 0.002868148129205575, policy loss: 6.781021139137799
Experience 8, Iter 41, disc loss: 0.0028931998896169764, policy loss: 6.7230637341285
Experience 8, Iter 42, disc loss: 0.002996199779025796, policy loss: 6.7612142150684615
Experience 8, Iter 43, disc loss: 0.0026689499154998156, policy loss: 6.965477362434303
Experience 8, Iter 44, disc loss: 0.002900916704396893, policy loss: 6.7970767807249795
Experience 8, Iter 45, disc loss: 0.00257772058568065, policy loss: 6.963414993400978
Experience 8, Iter 46, disc loss: 0.0029262064450096756, policy loss: 6.808978703204984
Experience 8, Iter 47, disc loss: 0.0029827784072387247, policy loss: 6.720319212863263
Experience 8, Iter 48, disc loss: 0.0025545832948608194, policy loss: 7.253244448411565
Experience 8, Iter 49, disc loss: 0.0026931123941717587, policy loss: 6.9560726605652645
Experience 8, Iter 50, disc loss: 0.002672052097668928, policy loss: 6.909497005009966
Experience 8, Iter 51, disc loss: 0.0029059088823553984, policy loss: 6.817372033478298
Experience 8, Iter 52, disc loss: 0.0028612979373179408, policy loss: 6.811648050617433
Experience 8, Iter 53, disc loss: 0.0027185581195635026, policy loss: 6.864126757078242
Experience 8, Iter 54, disc loss: 0.002723553737695241, policy loss: 6.847408800433589
Experience 8, Iter 55, disc loss: 0.0028731703050679635, policy loss: 6.714309090800974
Experience 8, Iter 56, disc loss: 0.002654319875112828, policy loss: 6.767615921257633
Experience 8, Iter 57, disc loss: 0.0026756582691643367, policy loss: 6.759815594128151
Experience 8, Iter 58, disc loss: 0.0022672105197126874, policy loss: 7.063370297005533
Experience 8, Iter 59, disc loss: 0.0024235257561070105, policy loss: 6.963201190797401
Experience 8, Iter 60, disc loss: 0.0025539706553511232, policy loss: 6.849814159833449
Experience 8, Iter 61, disc loss: 0.002485141620388551, policy loss: 7.1445936344697705
Experience 8, Iter 62, disc loss: 0.0027268770323811836, policy loss: 6.884982741307079
Experience 8, Iter 63, disc loss: 0.00227967522017057, policy loss: 7.225729735564291
Experience 8, Iter 64, disc loss: 0.002507199239264045, policy loss: 6.951604713613129
Experience 8, Iter 65, disc loss: 0.0027469705266333546, policy loss: 6.855002879616959
Experience 8, Iter 66, disc loss: 0.002700858003122205, policy loss: 6.822620761844878
Experience 8, Iter 67, disc loss: 0.0028656822993911755, policy loss: 6.6813472126275
Experience 8, Iter 68, disc loss: 0.0025956903549200798, policy loss: 6.89041315703214
Experience 8, Iter 69, disc loss: 0.0026446745955186427, policy loss: 6.829965368529727
Experience 8, Iter 70, disc loss: 0.0025219485361666913, policy loss: 6.975721209280765
Experience 8, Iter 71, disc loss: 0.002413080023051019, policy loss: 6.983556319786615
Experience 8, Iter 72, disc loss: 0.002711323187369574, policy loss: 6.800724095760229
Experience 8, Iter 73, disc loss: 0.002378768928282226, policy loss: 6.958156134717861
Experience 8, Iter 74, disc loss: 0.0027616193942413877, policy loss: 6.748085685234842
Experience 8, Iter 75, disc loss: 0.00255694474909748, policy loss: 6.887046345327459
Experience 8, Iter 76, disc loss: 0.002423938300721091, policy loss: 6.999440777505413
Experience 8, Iter 77, disc loss: 0.002446099806254781, policy loss: 6.974706172993608
Experience 8, Iter 78, disc loss: 0.0029375291461426743, policy loss: 6.48428763767053
Experience 8, Iter 79, disc loss: 0.002524077707406724, policy loss: 6.905610343295951
Experience 8, Iter 80, disc loss: 0.002432384838679985, policy loss: 7.087823928234322
Experience 8, Iter 81, disc loss: 0.002691752320480462, policy loss: 6.897209364611181
Experience 8, Iter 82, disc loss: 0.002481868367141096, policy loss: 7.0783694650086835
Experience 8, Iter 83, disc loss: 0.0024185088609258547, policy loss: 7.194815853363908
Experience 8, Iter 84, disc loss: 0.0025958076367584627, policy loss: 7.041683219321639
Experience 8, Iter 85, disc loss: 0.002461166770440059, policy loss: 6.989766904275193
Experience 8, Iter 86, disc loss: 0.002350447291369944, policy loss: 7.077416572810379
Experience 8, Iter 87, disc loss: 0.0022627732194169343, policy loss: 7.212693032714537
Experience 8, Iter 88, disc loss: 0.0025523471902322292, policy loss: 6.952301451768399
Experience 8, Iter 89, disc loss: 0.002418499010759018, policy loss: 7.027754387396527
Experience 8, Iter 90, disc loss: 0.0024669039122866988, policy loss: 7.047772852361756
Experience 8, Iter 91, disc loss: 0.002333176661653564, policy loss: 7.09889054488674
Experience 8, Iter 92, disc loss: 0.0024354689403740222, policy loss: 7.017080083899293
Experience 8, Iter 93, disc loss: 0.002565683928252728, policy loss: 7.0005839774763015
Experience 8, Iter 94, disc loss: 0.00234832733952683, policy loss: 6.956091871094989
Experience 8, Iter 95, disc loss: 0.0024271259998933873, policy loss: 6.860870954386903
Experience 8, Iter 96, disc loss: 0.0022882853997075493, policy loss: 6.933558781640049
Experience 8, Iter 97, disc loss: 0.0023684753885076294, policy loss: 7.101478774089427
Experience 8, Iter 98, disc loss: 0.002175947058278686, policy loss: 7.188430013489895
Experience 8, Iter 99, disc loss: 0.0022466946788234753, policy loss: 7.190231101837298
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.0281],
        [0.2221],
        [0.0030]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0348, 0.2263, 0.1443, 0.0071, 0.0013, 1.0271]],

        [[0.0348, 0.2263, 0.1443, 0.0071, 0.0013, 1.0271]],

        [[0.0348, 0.2263, 0.1443, 0.0071, 0.0013, 1.0271]],

        [[0.0348, 0.2263, 0.1443, 0.0071, 0.0013, 1.0271]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0244, 0.1124, 0.8886, 0.0120], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0244, 0.1124, 0.8886, 0.0120])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.606
Iter 2/2000 - Loss: 0.727
Iter 3/2000 - Loss: 0.489
Iter 4/2000 - Loss: 0.499
Iter 5/2000 - Loss: 0.578
Iter 6/2000 - Loss: 0.515
Iter 7/2000 - Loss: 0.399
Iter 8/2000 - Loss: 0.358
Iter 9/2000 - Loss: 0.382
Iter 10/2000 - Loss: 0.378
Iter 11/2000 - Loss: 0.303
Iter 12/2000 - Loss: 0.197
Iter 13/2000 - Loss: 0.105
Iter 14/2000 - Loss: 0.033
Iter 15/2000 - Loss: -0.052
Iter 16/2000 - Loss: -0.178
Iter 17/2000 - Loss: -0.340
Iter 18/2000 - Loss: -0.522
Iter 19/2000 - Loss: -0.713
Iter 20/2000 - Loss: -0.914
Iter 1981/2000 - Loss: -8.122
Iter 1982/2000 - Loss: -8.122
Iter 1983/2000 - Loss: -8.122
Iter 1984/2000 - Loss: -8.122
Iter 1985/2000 - Loss: -8.122
Iter 1986/2000 - Loss: -8.122
Iter 1987/2000 - Loss: -8.122
Iter 1988/2000 - Loss: -8.123
Iter 1989/2000 - Loss: -8.123
Iter 1990/2000 - Loss: -8.123
Iter 1991/2000 - Loss: -8.123
Iter 1992/2000 - Loss: -8.123
Iter 1993/2000 - Loss: -8.123
Iter 1994/2000 - Loss: -8.123
Iter 1995/2000 - Loss: -8.123
Iter 1996/2000 - Loss: -8.123
Iter 1997/2000 - Loss: -8.123
Iter 1998/2000 - Loss: -8.123
Iter 1999/2000 - Loss: -8.123
Iter 2000/2000 - Loss: -8.123
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[16.8187,  9.5068, 32.4776, 13.0308, 10.7457, 46.2153]],

        [[19.4559, 31.1241, 18.8394,  1.3596,  0.9884, 16.5565]],

        [[19.2895, 33.4630, 17.0577,  0.8418,  2.4391, 16.6136]],

        [[23.3089, 42.5454, 15.2948,  4.1892,  8.6403, 43.9995]]])
Signal Variance: tensor([ 0.1751,  0.9682, 10.7241,  0.4906])
Estimated target variance: tensor([0.0244, 0.1124, 0.8886, 0.0120])
N: 90
Signal to noise ratio: tensor([25.5206, 59.1749, 67.4373, 38.9536])
Bound on condition number: tensor([ 58618.2668, 315151.4597, 409302.6436, 136565.1968])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0025471160862800234, policy loss: 6.852463530719703
Experience 9, Iter 1, disc loss: 0.002395429670740918, policy loss: 6.950160148061336
Experience 9, Iter 2, disc loss: 0.0023398086104464127, policy loss: 7.0385807219784615
Experience 9, Iter 3, disc loss: 0.0026791635311833677, policy loss: 6.871738650933816
Experience 9, Iter 4, disc loss: 0.002451669009658433, policy loss: 6.846936413256562
Experience 9, Iter 5, disc loss: 0.0024145367756665762, policy loss: 6.91680508852017
Experience 9, Iter 6, disc loss: 0.0027586880527178496, policy loss: 6.776346078126303
Experience 9, Iter 7, disc loss: 0.002430414762312688, policy loss: 7.060693906849458
Experience 9, Iter 8, disc loss: 0.0027481052796588983, policy loss: 6.761271410411251
Experience 9, Iter 9, disc loss: 0.0027311983152638556, policy loss: 6.690745559574882
Experience 9, Iter 10, disc loss: 0.002605429647057961, policy loss: 6.77004256236031
Experience 9, Iter 11, disc loss: 0.0028326436740175546, policy loss: 6.617079502058285
Experience 9, Iter 12, disc loss: 0.0026085194656248634, policy loss: 6.872394360752998
Experience 9, Iter 13, disc loss: 0.0029130804441624397, policy loss: 6.778025180969552
Experience 9, Iter 14, disc loss: 0.002748168208030708, policy loss: 6.9998541382773345
Experience 9, Iter 15, disc loss: 0.0028448260861280495, policy loss: 6.914981549002507
Experience 9, Iter 16, disc loss: 0.0031555025366210624, policy loss: 6.737010968164771
Experience 9, Iter 17, disc loss: 0.003143928276925552, policy loss: 6.526763416942289
Experience 9, Iter 18, disc loss: 0.002829600356608406, policy loss: 6.81866847275159
Experience 9, Iter 19, disc loss: 0.0033467435781939347, policy loss: 6.607938949040813
Experience 9, Iter 20, disc loss: 0.002926317075960535, policy loss: 6.8714080038122205
Experience 9, Iter 21, disc loss: 0.0030085832200516076, policy loss: 6.777301422670469
Experience 9, Iter 22, disc loss: 0.0029927572939734215, policy loss: 7.035786206910168
Experience 9, Iter 23, disc loss: 0.002816904885082056, policy loss: 6.997382486443322
Experience 9, Iter 24, disc loss: 0.0031211235203383557, policy loss: 6.64908961085128
Experience 9, Iter 25, disc loss: 0.0029796549659303127, policy loss: 6.9472995776471524
Experience 9, Iter 26, disc loss: 0.003052001785514398, policy loss: 6.836343437217471
Experience 9, Iter 27, disc loss: 0.0030558139188675264, policy loss: 6.819841726148153
Experience 9, Iter 28, disc loss: 0.0034212805763799738, policy loss: 6.747025253037738
Experience 9, Iter 29, disc loss: 0.002834123600632375, policy loss: 6.809338468683128
Experience 9, Iter 30, disc loss: 0.003421880753333407, policy loss: 6.6498567602934315
Experience 9, Iter 31, disc loss: 0.0033048343888476064, policy loss: 6.902594014416707
Experience 9, Iter 32, disc loss: 0.0031014468369902704, policy loss: 6.7525667560795615
Experience 9, Iter 33, disc loss: 0.0030872662444492932, policy loss: 6.590836895347171
Experience 9, Iter 34, disc loss: 0.0029979378272363853, policy loss: 6.793468140823983
Experience 9, Iter 35, disc loss: 0.0029966613373488644, policy loss: 6.941276868315382
Experience 9, Iter 36, disc loss: 0.0028449037408535723, policy loss: 7.197489425129454
Experience 9, Iter 37, disc loss: 0.0032303001389102242, policy loss: 6.628042048899247
Experience 9, Iter 38, disc loss: 0.0033159018046420912, policy loss: 6.622935053555794
Experience 9, Iter 39, disc loss: 0.003209942022469028, policy loss: 6.62247609585854
Experience 9, Iter 40, disc loss: 0.003218608096840679, policy loss: 6.6853562764066785
Experience 9, Iter 41, disc loss: 0.0031559815483367913, policy loss: 6.858275967334089
Experience 9, Iter 42, disc loss: 0.0035026336180740943, policy loss: 6.661373553158185
Experience 9, Iter 43, disc loss: 0.003244790451257982, policy loss: 6.688497151063579
Experience 9, Iter 44, disc loss: 0.0034458288192616396, policy loss: 6.6985993908282575
Experience 9, Iter 45, disc loss: 0.0035724046375936657, policy loss: 6.624896639366875
Experience 9, Iter 46, disc loss: 0.0030709821317257153, policy loss: 6.6961714051121835
Experience 9, Iter 47, disc loss: 0.0035562289624789262, policy loss: 6.41956173813025
Experience 9, Iter 48, disc loss: 0.003489614584853319, policy loss: 6.44240695562678
Experience 9, Iter 49, disc loss: 0.00322524755656771, policy loss: 6.680280523007683
Experience 9, Iter 50, disc loss: 0.003569947835341769, policy loss: 6.9379462717702705
Experience 9, Iter 51, disc loss: 0.0032228511457545977, policy loss: 6.774087839015566
Experience 9, Iter 52, disc loss: 0.0032740588054304695, policy loss: 6.797196895546229
Experience 9, Iter 53, disc loss: 0.0029644898862303697, policy loss: 6.710625383736039
Experience 9, Iter 54, disc loss: 0.0036242910046073097, policy loss: 6.49082208747353
Experience 9, Iter 55, disc loss: 0.0036792526572853486, policy loss: 6.2760653374114685
Experience 9, Iter 56, disc loss: 0.0035404902583178583, policy loss: 6.524108559164733
Experience 9, Iter 57, disc loss: 0.0037748334318734177, policy loss: 6.493681973937615
Experience 9, Iter 58, disc loss: 0.0030609072177559377, policy loss: 6.754424175836993
Experience 9, Iter 59, disc loss: 0.00290701830293532, policy loss: 7.146229759284818
Experience 9, Iter 60, disc loss: 0.002917321206067783, policy loss: 7.005708320109112
Experience 9, Iter 61, disc loss: 0.0033097191690499104, policy loss: 6.765299268784792
Experience 9, Iter 62, disc loss: 0.0035569078867402327, policy loss: 6.370279200398103
Experience 9, Iter 63, disc loss: 0.0032904752467555554, policy loss: 6.666914018849935
Experience 9, Iter 64, disc loss: 0.0034356157789656745, policy loss: 6.683868740652583
Experience 9, Iter 65, disc loss: 0.003954064020226671, policy loss: 6.341674586614763
Experience 9, Iter 66, disc loss: 0.0037762596572792273, policy loss: 6.367207208681774
Experience 9, Iter 67, disc loss: 0.0034407970244163233, policy loss: 6.6704091443171265
Experience 9, Iter 68, disc loss: 0.0035317688453970334, policy loss: 6.647353198313293
Experience 9, Iter 69, disc loss: 0.004032612987087979, policy loss: 6.416182407609637
Experience 9, Iter 70, disc loss: 0.0033756282459354676, policy loss: 7.084646467976593
Experience 9, Iter 71, disc loss: 0.003681046100018473, policy loss: 6.744150779284899
Experience 9, Iter 72, disc loss: 0.003943551051247173, policy loss: 6.462866372783188
Experience 9, Iter 73, disc loss: 0.003666291737827671, policy loss: 6.514962393992593
Experience 9, Iter 74, disc loss: 0.0039304709926395905, policy loss: 6.364738202208071
Experience 9, Iter 75, disc loss: 0.0042038737492394096, policy loss: 6.620815807933706
Experience 9, Iter 76, disc loss: 0.004610399636545587, policy loss: 6.120637540528983
Experience 9, Iter 77, disc loss: 0.004693063640155738, policy loss: 6.293296891716961
Experience 9, Iter 78, disc loss: 0.004621624095754995, policy loss: 6.380015567501926
Experience 9, Iter 79, disc loss: 0.0049887707617861324, policy loss: 5.996821151010633
Experience 9, Iter 80, disc loss: 0.006817056371541921, policy loss: 5.7668743840410155
Experience 9, Iter 81, disc loss: 0.008345679679475896, policy loss: 6.019023813629875
Experience 9, Iter 82, disc loss: 0.009642285155160388, policy loss: 5.531089323502309
Experience 9, Iter 83, disc loss: 0.009611650746761938, policy loss: 5.77137379283565
Experience 9, Iter 84, disc loss: 0.011532735171289575, policy loss: 5.671457595495567
Experience 9, Iter 85, disc loss: 0.00906924955957553, policy loss: 5.665917240591272
Experience 9, Iter 86, disc loss: 0.012511308750713455, policy loss: 5.554924738571376
Experience 9, Iter 87, disc loss: 0.008114487954838632, policy loss: 5.826049844665871
Experience 9, Iter 88, disc loss: 0.008150228607838742, policy loss: 6.0525637615244765
Experience 9, Iter 89, disc loss: 0.01019456553260948, policy loss: 5.565181515806726
Experience 9, Iter 90, disc loss: 0.009476499377222574, policy loss: 6.322416741462982
Experience 9, Iter 91, disc loss: 0.012192421364699162, policy loss: 5.535534542091172
Experience 9, Iter 92, disc loss: 0.01283898290674212, policy loss: 5.745526067530996
Experience 9, Iter 93, disc loss: 0.014347195477981581, policy loss: 5.765229550257215
Experience 9, Iter 94, disc loss: 0.013909202686849004, policy loss: 5.7773194984501846
Experience 9, Iter 95, disc loss: 0.015186065909543515, policy loss: 6.133549457503794
Experience 9, Iter 96, disc loss: 0.015444324503102552, policy loss: 6.380691963185453
Experience 9, Iter 97, disc loss: 0.01544796190302653, policy loss: 6.520766561596811
Experience 9, Iter 98, disc loss: 0.01560668320874636, policy loss: 6.142650262033211
Experience 9, Iter 99, disc loss: 0.014448676102860773, policy loss: 6.081950733313485
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.0308],
        [0.3150],
        [0.0047]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0322, 0.2139, 0.2172, 0.0092, 0.0026, 1.0831]],

        [[0.0322, 0.2139, 0.2172, 0.0092, 0.0026, 1.0831]],

        [[0.0322, 0.2139, 0.2172, 0.0092, 0.0026, 1.0831]],

        [[0.0322, 0.2139, 0.2172, 0.0092, 0.0026, 1.0831]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0230, 0.1232, 1.2601, 0.0187], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0230, 0.1232, 1.2601, 0.0187])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.020
Iter 2/2000 - Loss: 0.987
Iter 3/2000 - Loss: 0.875
Iter 4/2000 - Loss: 0.840
Iter 5/2000 - Loss: 0.872
Iter 6/2000 - Loss: 0.819
Iter 7/2000 - Loss: 0.728
Iter 8/2000 - Loss: 0.684
Iter 9/2000 - Loss: 0.663
Iter 10/2000 - Loss: 0.605
Iter 11/2000 - Loss: 0.507
Iter 12/2000 - Loss: 0.395
Iter 13/2000 - Loss: 0.281
Iter 14/2000 - Loss: 0.155
Iter 15/2000 - Loss: 0.004
Iter 16/2000 - Loss: -0.177
Iter 17/2000 - Loss: -0.379
Iter 18/2000 - Loss: -0.596
Iter 19/2000 - Loss: -0.824
Iter 20/2000 - Loss: -1.067
Iter 1981/2000 - Loss: -7.982
Iter 1982/2000 - Loss: -7.982
Iter 1983/2000 - Loss: -7.982
Iter 1984/2000 - Loss: -7.982
Iter 1985/2000 - Loss: -7.982
Iter 1986/2000 - Loss: -7.982
Iter 1987/2000 - Loss: -7.982
Iter 1988/2000 - Loss: -7.982
Iter 1989/2000 - Loss: -7.982
Iter 1990/2000 - Loss: -7.982
Iter 1991/2000 - Loss: -7.982
Iter 1992/2000 - Loss: -7.982
Iter 1993/2000 - Loss: -7.982
Iter 1994/2000 - Loss: -7.982
Iter 1995/2000 - Loss: -7.982
Iter 1996/2000 - Loss: -7.982
Iter 1997/2000 - Loss: -7.982
Iter 1998/2000 - Loss: -7.982
Iter 1999/2000 - Loss: -7.982
Iter 2000/2000 - Loss: -7.983
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[13.4034,  9.5387, 29.6890, 11.6606, 15.6243, 47.4344]],

        [[18.8817, 30.7177, 12.2177,  1.1541,  8.9058, 26.7277]],

        [[17.9942, 37.1331, 18.5662,  0.8294,  6.1584, 22.7902]],

        [[18.7955, 36.4860, 12.1204,  2.3273,  8.0866, 35.6967]]])
Signal Variance: tensor([ 0.1863,  1.8083, 15.7963,  0.3078])
Estimated target variance: tensor([0.0230, 0.1232, 1.2601, 0.0187])
N: 100
Signal to noise ratio: tensor([25.0537, 79.8759, 81.9856, 30.9289])
Bound on condition number: tensor([ 62769.8317, 638017.4400, 672164.3422,  95660.8869])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.011917963808558727, policy loss: 6.418264902202983
Experience 10, Iter 1, disc loss: 0.015521565311282185, policy loss: 5.873046927795943
Experience 10, Iter 2, disc loss: 0.016408982238693644, policy loss: 5.718017161560175
Experience 10, Iter 3, disc loss: 0.016983300901424622, policy loss: 5.380037769365568
Experience 10, Iter 4, disc loss: 0.01644834105082467, policy loss: 5.461168165401711
Experience 10, Iter 5, disc loss: 0.01551213942869282, policy loss: 5.406299430054796
Experience 10, Iter 6, disc loss: 0.012913249217291888, policy loss: 5.9920370521253545
Experience 10, Iter 7, disc loss: 0.010963848306888053, policy loss: 6.146272639614614
Experience 10, Iter 8, disc loss: 0.00923445648891238, policy loss: 6.820040630250129
Experience 10, Iter 9, disc loss: 0.010163445921455978, policy loss: 6.635543830108475
Experience 10, Iter 10, disc loss: 0.008726156590386966, policy loss: 6.414608952778387
Experience 10, Iter 11, disc loss: 0.009611137278635802, policy loss: 6.624807460815701
Experience 10, Iter 12, disc loss: 0.007995440729810921, policy loss: 6.597864368900101
Experience 10, Iter 13, disc loss: 0.008498008433027027, policy loss: 7.484625856523817
Experience 10, Iter 14, disc loss: 0.009621900387001066, policy loss: 6.201764629286123
Experience 10, Iter 15, disc loss: 0.010481610365295085, policy loss: 6.261838160558114
Experience 10, Iter 16, disc loss: 0.009107031560233677, policy loss: 6.585997685234577
Experience 10, Iter 17, disc loss: 0.007563254647780465, policy loss: 6.754508895909667
Experience 10, Iter 18, disc loss: 0.007019261532479951, policy loss: 7.225940422205467
Experience 10, Iter 19, disc loss: 0.009080988514786281, policy loss: 7.371577014697488
Experience 10, Iter 20, disc loss: 0.0073445199073380275, policy loss: 7.066715166571983
Experience 10, Iter 21, disc loss: 0.005795581028338093, policy loss: 7.911706438162054
Experience 10, Iter 22, disc loss: 0.004182529423640532, policy loss: 8.13359655313041
Experience 10, Iter 23, disc loss: 0.0038600328183095887, policy loss: 8.94521071280294
Experience 10, Iter 24, disc loss: 0.004114631234677745, policy loss: 8.13860206667158
Experience 10, Iter 25, disc loss: 0.00472587948254716, policy loss: 7.450537016796042
Experience 10, Iter 26, disc loss: 0.004520514982214884, policy loss: 7.208132634881728
Experience 10, Iter 27, disc loss: 0.003722349647424003, policy loss: 8.401089068064186
Experience 10, Iter 28, disc loss: 0.003088401011159813, policy loss: 7.679094942367681
Experience 10, Iter 29, disc loss: 0.002962442352251285, policy loss: 8.048296822269968
Experience 10, Iter 30, disc loss: 0.0028137249639407155, policy loss: 8.148051148774641
Experience 10, Iter 31, disc loss: 0.003132298788667852, policy loss: 7.284106389541007
Experience 10, Iter 32, disc loss: 0.0030932608066560033, policy loss: 7.44568213273264
Experience 10, Iter 33, disc loss: 0.002535441807233369, policy loss: 8.646576849981853
Experience 10, Iter 34, disc loss: 0.0026754375181104206, policy loss: 8.545238793448249
Experience 10, Iter 35, disc loss: 0.0027227162131814365, policy loss: 7.615851964508078
Experience 10, Iter 36, disc loss: 0.0028558886013370964, policy loss: 7.387221354068599
Experience 10, Iter 37, disc loss: 0.00279768718626004, policy loss: 7.7076652713422416
Experience 10, Iter 38, disc loss: 0.00426241476529341, policy loss: 7.323143410724702
Experience 10, Iter 39, disc loss: 0.0037153085203706386, policy loss: 7.360371096223437
Experience 10, Iter 40, disc loss: 0.0039894055381888595, policy loss: 7.0313371976209105
Experience 10, Iter 41, disc loss: 0.0035535946592035877, policy loss: 6.930948680370163
Experience 10, Iter 42, disc loss: 0.0030627430477463426, policy loss: 7.869318955459019
Experience 10, Iter 43, disc loss: 0.0028896236084727245, policy loss: 7.816261689912176
Experience 10, Iter 44, disc loss: 0.0031858374935364543, policy loss: 8.094485235677613
Experience 10, Iter 45, disc loss: 0.003571434498250969, policy loss: 7.22771738790549
Experience 10, Iter 46, disc loss: 0.003062463396947182, policy loss: 7.75622135178472
Experience 10, Iter 47, disc loss: 0.0031187698270565755, policy loss: 8.17646867607457
Experience 10, Iter 48, disc loss: 0.0033756727949762963, policy loss: 7.984101135242865
Experience 10, Iter 49, disc loss: 0.002977393967636355, policy loss: 7.626081911263601
Experience 10, Iter 50, disc loss: 0.00296506624743503, policy loss: 7.972884130600217
Experience 10, Iter 51, disc loss: 0.0029919980147404025, policy loss: 7.828151847504193
Experience 10, Iter 52, disc loss: 0.00317676366606587, policy loss: 7.570856412075754
Experience 10, Iter 53, disc loss: 0.0030851245098019605, policy loss: 8.051737430301747
Experience 10, Iter 54, disc loss: 0.0030170259305749205, policy loss: 8.02833281732702
Experience 10, Iter 55, disc loss: 0.0032357742049397304, policy loss: 7.838590876260474
Experience 10, Iter 56, disc loss: 0.0032329906794126805, policy loss: 7.744887374189735
Experience 10, Iter 57, disc loss: 0.002998179585336393, policy loss: 8.1955218265059
Experience 10, Iter 58, disc loss: 0.0027579165287833635, policy loss: 8.513740490873275
Experience 10, Iter 59, disc loss: 0.002471660693010718, policy loss: 8.733525458495755
Experience 10, Iter 60, disc loss: 0.002921122336041343, policy loss: 7.974840690845351
Experience 10, Iter 61, disc loss: 0.002793070844343544, policy loss: 8.600424727273408
Experience 10, Iter 62, disc loss: 0.0025822469616652117, policy loss: 8.517469644250362
Experience 10, Iter 63, disc loss: 0.002401562105460148, policy loss: 8.048852031717255
Experience 10, Iter 64, disc loss: 0.0029757435411585332, policy loss: 8.209053285216926
Experience 10, Iter 65, disc loss: 0.002239691434880549, policy loss: 8.390514641357225
Experience 10, Iter 66, disc loss: 0.002470077711233575, policy loss: 8.507743948932497
Experience 10, Iter 67, disc loss: 0.0026560444026278787, policy loss: 7.534435414848102
Experience 10, Iter 68, disc loss: 0.0023959308088684645, policy loss: 7.999643790738349
Experience 10, Iter 69, disc loss: 0.0022777289332599837, policy loss: 7.989867865638237
Experience 10, Iter 70, disc loss: 0.002549277849586815, policy loss: 7.853923734276195
Experience 10, Iter 71, disc loss: 0.0021747852147428054, policy loss: 8.119194602604123
Experience 10, Iter 72, disc loss: 0.0026241772331975946, policy loss: 8.05348329619867
Experience 10, Iter 73, disc loss: 0.002480640404744951, policy loss: 8.001155161988606
Experience 10, Iter 74, disc loss: 0.0020257736423254747, policy loss: 7.928202870191996
Experience 10, Iter 75, disc loss: 0.0018976337868437734, policy loss: 9.377842053666276
Experience 10, Iter 76, disc loss: 0.0022948272824812052, policy loss: 8.45605722460851
Experience 10, Iter 77, disc loss: 0.0022504819954186836, policy loss: 7.619760029791905
Experience 10, Iter 78, disc loss: 0.0023667577561118187, policy loss: 8.178114737313901
Experience 10, Iter 79, disc loss: 0.002665141418031597, policy loss: 7.723712727669234
Experience 10, Iter 80, disc loss: 0.0021950569659124255, policy loss: 7.792829277839149
Experience 10, Iter 81, disc loss: 0.0021157298367136023, policy loss: 8.76880741021806
Experience 10, Iter 82, disc loss: 0.0022783641248139324, policy loss: 7.670430260590292
Experience 10, Iter 83, disc loss: 0.002274584318282536, policy loss: 7.910051733735031
Experience 10, Iter 84, disc loss: 0.002055732137665127, policy loss: 8.188257688418952
Experience 10, Iter 85, disc loss: 0.002223066040398201, policy loss: 8.34845118390665
Experience 10, Iter 86, disc loss: 0.002193521365841801, policy loss: 8.785256080825475
Experience 10, Iter 87, disc loss: 0.0017709286740508886, policy loss: 8.127396112419627
Experience 10, Iter 88, disc loss: 0.0019145628427182037, policy loss: 8.187762453240918
Experience 10, Iter 89, disc loss: 0.002060361510056257, policy loss: 8.449171237200762
Experience 10, Iter 90, disc loss: 0.0020093395726902812, policy loss: 8.522923725927505
Experience 10, Iter 91, disc loss: 0.0018023671386015604, policy loss: 9.148044849327077
Experience 10, Iter 92, disc loss: 0.001987416162156852, policy loss: 8.101706647413355
Experience 10, Iter 93, disc loss: 0.00203223837029551, policy loss: 7.870929947435384
Experience 10, Iter 94, disc loss: 0.0017768471759488536, policy loss: 8.545956818423257
Experience 10, Iter 95, disc loss: 0.002036677148823853, policy loss: 8.589910713354591
Experience 10, Iter 96, disc loss: 0.002046345545237854, policy loss: 8.418354476896582
Experience 10, Iter 97, disc loss: 0.002184593785651934, policy loss: 8.03820893070625
Experience 10, Iter 98, disc loss: 0.0021910098805638127, policy loss: 8.214907859946964
Experience 10, Iter 99, disc loss: 0.002241202528939174, policy loss: 7.896519054565627
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.0384],
        [0.4320],
        [0.0071]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0301, 0.2064, 0.3351, 0.0112, 0.0051, 1.2857]],

        [[0.0301, 0.2064, 0.3351, 0.0112, 0.0051, 1.2857]],

        [[0.0301, 0.2064, 0.3351, 0.0112, 0.0051, 1.2857]],

        [[0.0301, 0.2064, 0.3351, 0.0112, 0.0051, 1.2857]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0218, 0.1537, 1.7281, 0.0285], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0218, 0.1537, 1.7281, 0.0285])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.435
Iter 2/2000 - Loss: 1.314
Iter 3/2000 - Loss: 1.274
Iter 4/2000 - Loss: 1.193
Iter 5/2000 - Loss: 1.193
Iter 6/2000 - Loss: 1.160
Iter 7/2000 - Loss: 1.065
Iter 8/2000 - Loss: 0.983
Iter 9/2000 - Loss: 0.923
Iter 10/2000 - Loss: 0.847
Iter 11/2000 - Loss: 0.734
Iter 12/2000 - Loss: 0.597
Iter 13/2000 - Loss: 0.446
Iter 14/2000 - Loss: 0.283
Iter 15/2000 - Loss: 0.101
Iter 16/2000 - Loss: -0.105
Iter 17/2000 - Loss: -0.335
Iter 18/2000 - Loss: -0.583
Iter 19/2000 - Loss: -0.843
Iter 20/2000 - Loss: -1.115
Iter 1981/2000 - Loss: -7.851
Iter 1982/2000 - Loss: -7.851
Iter 1983/2000 - Loss: -7.851
Iter 1984/2000 - Loss: -7.851
Iter 1985/2000 - Loss: -7.851
Iter 1986/2000 - Loss: -7.851
Iter 1987/2000 - Loss: -7.851
Iter 1988/2000 - Loss: -7.852
Iter 1989/2000 - Loss: -7.852
Iter 1990/2000 - Loss: -7.852
Iter 1991/2000 - Loss: -7.852
Iter 1992/2000 - Loss: -7.852
Iter 1993/2000 - Loss: -7.852
Iter 1994/2000 - Loss: -7.852
Iter 1995/2000 - Loss: -7.852
Iter 1996/2000 - Loss: -7.852
Iter 1997/2000 - Loss: -7.852
Iter 1998/2000 - Loss: -7.852
Iter 1999/2000 - Loss: -7.852
Iter 2000/2000 - Loss: -7.852
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.4742,  8.8450, 26.3665, 10.0879,  4.2254, 47.5920]],

        [[21.0943, 34.8510, 11.5571,  1.5195,  4.4720, 35.2115]],

        [[19.8939, 38.0359, 16.3475,  0.8005,  2.7781, 20.4636]],

        [[19.8754, 38.9538, 16.6490,  4.1022,  2.0886, 39.5382]]])
Signal Variance: tensor([ 0.1713,  3.1126, 16.9255,  0.5291])
Estimated target variance: tensor([0.0218, 0.1537, 1.7281, 0.0285])
N: 110
Signal to noise ratio: tensor([ 23.1454, 104.7888,  85.5757,  42.4706])
Bound on condition number: tensor([  58928.8844, 1207878.1464,  805552.5029,  198413.9736])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.004722250944758223, policy loss: 6.669519792609965
Experience 11, Iter 1, disc loss: 0.006056989585407441, policy loss: 6.457234311029545
Experience 11, Iter 2, disc loss: 0.006254334231732001, policy loss: 7.320360712014504
Experience 11, Iter 3, disc loss: 0.0045715567938084765, policy loss: 7.054466933334147
Experience 11, Iter 4, disc loss: 0.0042392316759070695, policy loss: 7.11460781066377
Experience 11, Iter 5, disc loss: 0.005481888942997992, policy loss: 6.9653454771916845
Experience 11, Iter 6, disc loss: 0.004696308724194742, policy loss: 7.132404202635057
Experience 11, Iter 7, disc loss: 0.005613988501595595, policy loss: 6.58182879711435
Experience 11, Iter 8, disc loss: 0.006164049615362247, policy loss: 6.727038195637461
Experience 11, Iter 9, disc loss: 0.005753208310390458, policy loss: 6.625987852099858
Experience 11, Iter 10, disc loss: 0.005784789125330688, policy loss: 6.600897540623974
Experience 11, Iter 11, disc loss: 0.006071699550803233, policy loss: 6.819838884029619
Experience 11, Iter 12, disc loss: 0.006472271141158496, policy loss: 7.2178226912529615
Experience 11, Iter 13, disc loss: 0.006413839719452775, policy loss: 7.327421832850278
Experience 11, Iter 14, disc loss: 0.006905718249248875, policy loss: 7.731303654091419
Experience 11, Iter 15, disc loss: 0.006609575572279342, policy loss: 7.056659408106245
Experience 11, Iter 16, disc loss: 0.006533948537532161, policy loss: 7.250139855778054
Experience 11, Iter 17, disc loss: 0.007150837512555323, policy loss: 6.549839695603703
Experience 11, Iter 18, disc loss: 0.006423800682797893, policy loss: 7.2871505596235515
Experience 11, Iter 19, disc loss: 0.006947439080695556, policy loss: 7.001415398428099
Experience 11, Iter 20, disc loss: 0.0072344857925563675, policy loss: 6.21343289483231
Experience 11, Iter 21, disc loss: 0.007829758097426039, policy loss: 6.370338243100553
Experience 11, Iter 22, disc loss: 0.006086208153173924, policy loss: 7.20954541917533
Experience 11, Iter 23, disc loss: 0.008006373105883783, policy loss: 6.768811780894669
Experience 11, Iter 24, disc loss: 0.007236784333776304, policy loss: 6.3144975623383495
Experience 11, Iter 25, disc loss: 0.007370193470801069, policy loss: 6.762149228320029
Experience 11, Iter 26, disc loss: 0.008083694036002814, policy loss: 5.979233418563096
Experience 11, Iter 27, disc loss: 0.00808921764783805, policy loss: 6.241511482401251
Experience 11, Iter 28, disc loss: 0.0064658465209922, policy loss: 7.008059733084615
Experience 11, Iter 29, disc loss: 0.007357448810597478, policy loss: 6.527672116498991
Experience 11, Iter 30, disc loss: 0.00796109923725656, policy loss: 6.884635981060949
Experience 11, Iter 31, disc loss: 0.007778367694668824, policy loss: 6.746995515482094
Experience 11, Iter 32, disc loss: 0.00729849981599117, policy loss: 7.2866530524150015
Experience 11, Iter 33, disc loss: 0.009163653609848828, policy loss: 6.341543747835269
Experience 11, Iter 34, disc loss: 0.007492589741748353, policy loss: 7.023972057665036
Experience 11, Iter 35, disc loss: 0.008945559500157787, policy loss: 6.580950668006761
Experience 11, Iter 36, disc loss: 0.01027306709720512, policy loss: 6.3721526622672
Experience 11, Iter 37, disc loss: 0.00907648450161187, policy loss: 6.78146087426358
Experience 11, Iter 38, disc loss: 0.009179818941091503, policy loss: 6.280795037690803
Experience 11, Iter 39, disc loss: 0.010443248561585357, policy loss: 6.276072783203347
Experience 11, Iter 40, disc loss: 0.007900997383012717, policy loss: 6.699874558226858
Experience 11, Iter 41, disc loss: 0.00938252505598038, policy loss: 6.661459722098861
Experience 11, Iter 42, disc loss: 0.007338520999541184, policy loss: 7.5768227065461495
Experience 11, Iter 43, disc loss: 0.007115023959945574, policy loss: 7.362279346398611
Experience 11, Iter 44, disc loss: 0.0075590484327039955, policy loss: 6.8443926491752
Experience 11, Iter 45, disc loss: 0.007274288138518268, policy loss: 7.3931162908086
Experience 11, Iter 46, disc loss: 0.007844312652297923, policy loss: 6.797309627119464
Experience 11, Iter 47, disc loss: 0.010184104312026536, policy loss: 7.21153021032816
Experience 11, Iter 48, disc loss: 0.006399904842540236, policy loss: 6.858759439815215
Experience 11, Iter 49, disc loss: 0.007826973070865901, policy loss: 6.916279967111466
Experience 11, Iter 50, disc loss: 0.007374708804827722, policy loss: 7.246538518722987
Experience 11, Iter 51, disc loss: 0.00888466543705738, policy loss: 6.694104187124764
Experience 11, Iter 52, disc loss: 0.0066206960439025245, policy loss: 6.90647627291948
Experience 11, Iter 53, disc loss: 0.005315847776715614, policy loss: 7.0872225057733145
Experience 11, Iter 54, disc loss: 0.007350469263094227, policy loss: 6.903221167309015
Experience 11, Iter 55, disc loss: 0.006290750818276088, policy loss: 7.097379574853117
Experience 11, Iter 56, disc loss: 0.006136056207802685, policy loss: 8.161282781238228
Experience 11, Iter 57, disc loss: 0.007242629484224132, policy loss: 6.621602603065643
Experience 11, Iter 58, disc loss: 0.005982916448485725, policy loss: 7.287074633248985
Experience 11, Iter 59, disc loss: 0.00891137743787682, policy loss: 6.794389859092492
Experience 11, Iter 60, disc loss: 0.005483694401062763, policy loss: 7.21364112647999
Experience 11, Iter 61, disc loss: 0.005909103430258871, policy loss: 6.811270355921961
Experience 11, Iter 62, disc loss: 0.00519748176348174, policy loss: 7.189157500106386
Experience 11, Iter 63, disc loss: 0.005203188944686546, policy loss: 6.850018279152302
Experience 11, Iter 64, disc loss: 0.007381810863051576, policy loss: 7.3407297351909
Experience 11, Iter 65, disc loss: 0.00574321253323689, policy loss: 6.646339113667803
Experience 11, Iter 66, disc loss: 0.005615491153679864, policy loss: 6.853910684362675
Experience 11, Iter 67, disc loss: 0.004315734038425215, policy loss: 7.660597409235379
Experience 11, Iter 68, disc loss: 0.006226914622511312, policy loss: 6.760253122660516
Experience 11, Iter 69, disc loss: 0.004454061807240238, policy loss: 8.282084332442187
Experience 11, Iter 70, disc loss: 0.005870861922452213, policy loss: 6.614223010820426
Experience 11, Iter 71, disc loss: 0.0053613059532, policy loss: 7.028519484332257
Experience 11, Iter 72, disc loss: 0.0042765839234287, policy loss: 8.03475378817631
Experience 11, Iter 73, disc loss: 0.005451708245359821, policy loss: 7.568728129256922
Experience 11, Iter 74, disc loss: 0.004661844196642029, policy loss: 7.907078573862296
Experience 11, Iter 75, disc loss: 0.0039484524536207415, policy loss: 7.2920874560834745
Experience 11, Iter 76, disc loss: 0.004000281743149865, policy loss: 7.627513720808892
Experience 11, Iter 77, disc loss: 0.00494455888334212, policy loss: 7.760041991397025
Experience 11, Iter 78, disc loss: 0.006196525252806083, policy loss: 6.63116262743258
Experience 11, Iter 79, disc loss: 0.0043028176258892515, policy loss: 7.739379836487589
Experience 11, Iter 80, disc loss: 0.004959671693829307, policy loss: 7.12335902897168
Experience 11, Iter 81, disc loss: 0.004198393020514986, policy loss: 7.344619018439178
Experience 11, Iter 82, disc loss: 0.004076927242623532, policy loss: 7.674944716224375
Experience 11, Iter 83, disc loss: 0.004458176170507593, policy loss: 7.5936178134013845
Experience 11, Iter 84, disc loss: 0.004655043331449773, policy loss: 7.609035587401101
Experience 11, Iter 85, disc loss: 0.00405312200316617, policy loss: 7.749594676279502
Experience 11, Iter 86, disc loss: 0.004226909010734012, policy loss: 7.324006520414865
Experience 11, Iter 87, disc loss: 0.004586243268196665, policy loss: 7.179855850852604
Experience 11, Iter 88, disc loss: 0.0033805128600665384, policy loss: 7.936853965267373
Experience 11, Iter 89, disc loss: 0.003948143294497296, policy loss: 7.738870694869328
Experience 11, Iter 90, disc loss: 0.004563616113616125, policy loss: 6.902451274210724
Experience 11, Iter 91, disc loss: 0.004035512617512199, policy loss: 7.6420249158781886
Experience 11, Iter 92, disc loss: 0.00455344989964873, policy loss: 7.7014201198819645
Experience 11, Iter 93, disc loss: 0.0045208636719412144, policy loss: 7.528675350238902
Experience 11, Iter 94, disc loss: 0.004226583018430124, policy loss: 7.35738354024225
Experience 11, Iter 95, disc loss: 0.004330574621740581, policy loss: 8.133445439546268
Experience 11, Iter 96, disc loss: 0.0039196227142309715, policy loss: 7.407357038077015
Experience 11, Iter 97, disc loss: 0.00440646709678231, policy loss: 8.387942620863543
Experience 11, Iter 98, disc loss: 0.004201987792667899, policy loss: 7.6303392010714095
Experience 11, Iter 99, disc loss: 0.004660732203192162, policy loss: 7.368559254579556
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.0647],
        [0.5933],
        [0.0116]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0280, 0.2034, 0.5285, 0.0132, 0.0088, 1.8667]],

        [[0.0280, 0.2034, 0.5285, 0.0132, 0.0088, 1.8667]],

        [[0.0280, 0.2034, 0.5285, 0.0132, 0.0088, 1.8667]],

        [[0.0280, 0.2034, 0.5285, 0.0132, 0.0088, 1.8667]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0214, 0.2587, 2.3733, 0.0462], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0214, 0.2587, 2.3733, 0.0462])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.053
Iter 2/2000 - Loss: 1.922
Iter 3/2000 - Loss: 1.923
Iter 4/2000 - Loss: 1.833
Iter 5/2000 - Loss: 1.792
Iter 6/2000 - Loss: 1.769
Iter 7/2000 - Loss: 1.698
Iter 8/2000 - Loss: 1.605
Iter 9/2000 - Loss: 1.516
Iter 10/2000 - Loss: 1.420
Iter 11/2000 - Loss: 1.301
Iter 12/2000 - Loss: 1.158
Iter 13/2000 - Loss: 0.997
Iter 14/2000 - Loss: 0.820
Iter 15/2000 - Loss: 0.627
Iter 16/2000 - Loss: 0.414
Iter 17/2000 - Loss: 0.182
Iter 18/2000 - Loss: -0.068
Iter 19/2000 - Loss: -0.329
Iter 20/2000 - Loss: -0.599
Iter 1981/2000 - Loss: -7.591
Iter 1982/2000 - Loss: -7.591
Iter 1983/2000 - Loss: -7.591
Iter 1984/2000 - Loss: -7.591
Iter 1985/2000 - Loss: -7.591
Iter 1986/2000 - Loss: -7.591
Iter 1987/2000 - Loss: -7.591
Iter 1988/2000 - Loss: -7.592
Iter 1989/2000 - Loss: -7.592
Iter 1990/2000 - Loss: -7.592
Iter 1991/2000 - Loss: -7.592
Iter 1992/2000 - Loss: -7.592
Iter 1993/2000 - Loss: -7.592
Iter 1994/2000 - Loss: -7.592
Iter 1995/2000 - Loss: -7.592
Iter 1996/2000 - Loss: -7.592
Iter 1997/2000 - Loss: -7.592
Iter 1998/2000 - Loss: -7.592
Iter 1999/2000 - Loss: -7.592
Iter 2000/2000 - Loss: -7.592
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.1037,  8.8818, 35.3793, 10.6794, 21.3574, 52.4622]],

        [[17.4855, 33.0999, 12.2268,  0.9663,  8.2644, 16.9522]],

        [[18.6857, 34.9410, 12.3778,  0.8937,  1.1073, 23.9066]],

        [[19.2402, 37.2621, 16.3246,  2.8745,  1.8404, 41.5253]]])
Signal Variance: tensor([ 0.1795,  1.5221, 17.3125,  0.6628])
Estimated target variance: tensor([0.0214, 0.2587, 2.3733, 0.0462])
N: 120
Signal to noise ratio: tensor([23.2513, 73.9998, 87.0624, 47.8487])
Bound on condition number: tensor([ 64875.5504, 657117.9108, 909584.9520, 274740.5502])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.11640282315231551, policy loss: 3.5908210319061515
Experience 12, Iter 1, disc loss: 0.0525337327224374, policy loss: 5.1896698427685255
Experience 12, Iter 2, disc loss: 0.03703111978029381, policy loss: 5.353716692063854
Experience 12, Iter 3, disc loss: 0.034158021599482585, policy loss: 6.1154999456632595
Experience 12, Iter 4, disc loss: 0.05102065979343324, policy loss: 6.443186852973467
Experience 12, Iter 5, disc loss: 0.062398523104006814, policy loss: 6.184925014797634
Experience 12, Iter 6, disc loss: 0.05369782102029295, policy loss: 5.727392236421956
Experience 12, Iter 7, disc loss: 0.03523266938389276, policy loss: 6.333131899335414
Experience 12, Iter 8, disc loss: 0.021474351534104638, policy loss: 6.0715548992860136
Experience 12, Iter 9, disc loss: 0.01766428148317785, policy loss: 4.999346477013335
Experience 12, Iter 10, disc loss: 0.026199537777251477, policy loss: 4.180212919890256
Experience 12, Iter 11, disc loss: 0.01661121949353112, policy loss: 5.530556380102073
Experience 12, Iter 12, disc loss: 0.01770598305861268, policy loss: 4.827416308563391
Experience 12, Iter 13, disc loss: 0.01172279273855249, policy loss: 5.528315840048989
Experience 12, Iter 14, disc loss: 0.010416288546383969, policy loss: 5.51522128766632
Experience 12, Iter 15, disc loss: 0.01104029083833342, policy loss: 5.814418527609671
Experience 12, Iter 16, disc loss: 0.0107360279531497, policy loss: 5.680306964049311
Experience 12, Iter 17, disc loss: 0.013999597291555185, policy loss: 5.087923209863392
Experience 12, Iter 18, disc loss: 0.013088138975027808, policy loss: 5.908187626931038
Experience 12, Iter 19, disc loss: 0.02779206054421139, policy loss: 4.927677824431523
Experience 12, Iter 20, disc loss: 0.013043628446375386, policy loss: 6.274376959633847
Experience 12, Iter 21, disc loss: 0.017158404158259546, policy loss: 5.0024624822118735
Experience 12, Iter 22, disc loss: 0.007806984176192855, policy loss: 6.516949996555608
Experience 12, Iter 23, disc loss: 0.006132004501150725, policy loss: 7.266245413459802
Experience 12, Iter 24, disc loss: 0.005945180401651094, policy loss: 7.791981464097524
Experience 12, Iter 25, disc loss: 0.006093966406688292, policy loss: 8.313030794808162
Experience 12, Iter 26, disc loss: 0.00661899287651139, policy loss: 8.664583703276422
Experience 12, Iter 27, disc loss: 0.007060058118473805, policy loss: 8.283501598951526
Experience 12, Iter 28, disc loss: 0.007594600496762863, policy loss: 7.8642843585254365
Experience 12, Iter 29, disc loss: 0.007744117656048878, policy loss: 7.4500877491693265
Experience 12, Iter 30, disc loss: 0.0077067624149614305, policy loss: 7.649368623490689
Experience 12, Iter 31, disc loss: 0.007991189962080595, policy loss: 7.177197249398056
Experience 12, Iter 32, disc loss: 0.008295590400884475, policy loss: 6.334119095603119
Experience 12, Iter 33, disc loss: 0.006743409524505286, policy loss: 7.294074036477255
Experience 12, Iter 34, disc loss: 0.007663411221322473, policy loss: 6.62792448530133
Experience 12, Iter 35, disc loss: 0.007610821415557698, policy loss: 6.185228982377154
Experience 12, Iter 36, disc loss: 0.006485512371767052, policy loss: 6.788205649592719
Experience 12, Iter 37, disc loss: 0.005824543960904201, policy loss: 6.837629290579805
Experience 12, Iter 38, disc loss: 0.006312372855169142, policy loss: 7.615042710419515
Experience 12, Iter 39, disc loss: 0.006778697506515755, policy loss: 6.40059860232831
Experience 12, Iter 40, disc loss: 0.006601947199219224, policy loss: 6.15075772741332
Experience 12, Iter 41, disc loss: 0.008367453396496275, policy loss: 5.622268041320174
Experience 12, Iter 42, disc loss: 0.008437709777727157, policy loss: 5.8939740494105175
Experience 12, Iter 43, disc loss: 0.012001226775258625, policy loss: 5.315177820106378
Experience 12, Iter 44, disc loss: 0.009650323243455682, policy loss: 6.363406416605059
Experience 12, Iter 45, disc loss: 0.008462820545245562, policy loss: 6.045937394924389
Experience 12, Iter 46, disc loss: 0.008065499097441102, policy loss: 5.689314844112804
Experience 12, Iter 47, disc loss: 0.008792136446133806, policy loss: 5.876628925990124
Experience 12, Iter 48, disc loss: 0.007930353565314387, policy loss: 5.998093036274136
Experience 12, Iter 49, disc loss: 0.007086725014635097, policy loss: 6.298590511216304
Experience 12, Iter 50, disc loss: 0.007537872985563925, policy loss: 6.602722190298733
Experience 12, Iter 51, disc loss: 0.00830213943961387, policy loss: 6.306412623079538
Experience 12, Iter 52, disc loss: 0.00845320062153515, policy loss: 6.493472005203765
Experience 12, Iter 53, disc loss: 0.007372335167703879, policy loss: 6.496969541419633
Experience 12, Iter 54, disc loss: 0.00713479661389212, policy loss: 6.922599757270562
Experience 12, Iter 55, disc loss: 0.007743888315157316, policy loss: 6.336974848669039
Experience 12, Iter 56, disc loss: 0.007736522101964817, policy loss: 6.4154373949161405
Experience 12, Iter 57, disc loss: 0.007700831200932309, policy loss: 6.651634844506807
Experience 12, Iter 58, disc loss: 0.008184535354601218, policy loss: 6.505904040905234
Experience 12, Iter 59, disc loss: 0.0075691454836000925, policy loss: 6.182440142609409
Experience 12, Iter 60, disc loss: 0.007478495099865615, policy loss: 6.50823627781549
Experience 12, Iter 61, disc loss: 0.007505987624936292, policy loss: 6.597055884577767
Experience 12, Iter 62, disc loss: 0.006701831548040035, policy loss: 6.668474390576897
Experience 12, Iter 63, disc loss: 0.007071387127965649, policy loss: 6.630127632102283
Experience 12, Iter 64, disc loss: 0.006253400977055148, policy loss: 7.0358286867916515
Experience 12, Iter 65, disc loss: 0.006836228106668023, policy loss: 6.55060705975054
Experience 12, Iter 66, disc loss: 0.006849795025389422, policy loss: 6.64253079276722
Experience 12, Iter 67, disc loss: 0.006778391330838873, policy loss: 6.359292789065453
Experience 12, Iter 68, disc loss: 0.0070924988065392295, policy loss: 6.438083868978869
Experience 12, Iter 69, disc loss: 0.006990848865972682, policy loss: 6.2293536672784455
Experience 12, Iter 70, disc loss: 0.006583946157509502, policy loss: 6.42525970282287
Experience 12, Iter 71, disc loss: 0.006700388154973459, policy loss: 6.506995319406769
Experience 12, Iter 72, disc loss: 0.006516085653116047, policy loss: 6.810485119989947
Experience 12, Iter 73, disc loss: 0.006996698656015892, policy loss: 6.089209295195018
Experience 12, Iter 74, disc loss: 0.0060418649629201664, policy loss: 6.576578220641479
Experience 12, Iter 75, disc loss: 0.00599059714353693, policy loss: 7.415998001464665
Experience 12, Iter 76, disc loss: 0.006594258978654222, policy loss: 6.024170656390347
Experience 12, Iter 77, disc loss: 0.006101209227672743, policy loss: 6.50703509409365
Experience 12, Iter 78, disc loss: 0.006266989473523466, policy loss: 6.883618833463443
Experience 12, Iter 79, disc loss: 0.0057870022575594, policy loss: 6.642440613303707
Experience 12, Iter 80, disc loss: 0.005334006582569795, policy loss: 6.885967095212015
Experience 12, Iter 81, disc loss: 0.005269672947056402, policy loss: 6.947459479288402
Experience 12, Iter 82, disc loss: 0.005137880043248653, policy loss: 7.110329364788283
Experience 12, Iter 83, disc loss: 0.005852735794360129, policy loss: 6.995288061633953
Experience 12, Iter 84, disc loss: 0.005291462959518019, policy loss: 8.348911211488517
Experience 12, Iter 85, disc loss: 0.005526294615876096, policy loss: 7.085795137428108
Experience 12, Iter 86, disc loss: 0.0038491177737849536, policy loss: 7.331126558822666
Experience 12, Iter 87, disc loss: 0.0034642717759904925, policy loss: 8.27078813669751
Experience 12, Iter 88, disc loss: 0.0031229450691734415, policy loss: 8.48811620177954
Experience 12, Iter 89, disc loss: 0.003033702434482973, policy loss: 8.12870936616641
Experience 12, Iter 90, disc loss: 0.0029182816547646115, policy loss: 8.325031000025223
Experience 12, Iter 91, disc loss: 0.0028698833152353926, policy loss: 8.480698690471636
Experience 12, Iter 92, disc loss: 0.0028116454674931173, policy loss: 8.035476401500281
Experience 12, Iter 93, disc loss: 0.003183879468206274, policy loss: 7.150761615140776
Experience 12, Iter 94, disc loss: 0.0031174232478396453, policy loss: 7.7275873985956824
Experience 12, Iter 95, disc loss: 0.004192127447001098, policy loss: 6.980030739369516
Experience 12, Iter 96, disc loss: 0.004957527402136239, policy loss: 6.57599300621827
Experience 12, Iter 97, disc loss: 0.002719873977743443, policy loss: 7.995795998076584
Experience 12, Iter 98, disc loss: 0.00473989552286623, policy loss: 7.100907749050309
Experience 12, Iter 99, disc loss: 0.00452238504864115, policy loss: 6.747323892514233
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.0842],
        [0.7206],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0268, 0.2011, 0.6671, 0.0148, 0.0119, 2.3267]],

        [[0.0268, 0.2011, 0.6671, 0.0148, 0.0119, 2.3267]],

        [[0.0268, 0.2011, 0.6671, 0.0148, 0.0119, 2.3267]],

        [[0.0268, 0.2011, 0.6671, 0.0148, 0.0119, 2.3267]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0209, 0.3369, 2.8823, 0.0585], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0209, 0.3369, 2.8823, 0.0585])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.380
Iter 2/2000 - Loss: 2.251
Iter 3/2000 - Loss: 2.243
Iter 4/2000 - Loss: 2.166
Iter 5/2000 - Loss: 2.117
Iter 6/2000 - Loss: 2.071
Iter 7/2000 - Loss: 2.003
Iter 8/2000 - Loss: 1.921
Iter 9/2000 - Loss: 1.827
Iter 10/2000 - Loss: 1.714
Iter 11/2000 - Loss: 1.577
Iter 12/2000 - Loss: 1.423
Iter 13/2000 - Loss: 1.256
Iter 14/2000 - Loss: 1.070
Iter 15/2000 - Loss: 0.862
Iter 16/2000 - Loss: 0.627
Iter 17/2000 - Loss: 0.368
Iter 18/2000 - Loss: 0.093
Iter 19/2000 - Loss: -0.192
Iter 20/2000 - Loss: -0.483
Iter 1981/2000 - Loss: -7.640
Iter 1982/2000 - Loss: -7.640
Iter 1983/2000 - Loss: -7.640
Iter 1984/2000 - Loss: -7.640
Iter 1985/2000 - Loss: -7.640
Iter 1986/2000 - Loss: -7.640
Iter 1987/2000 - Loss: -7.640
Iter 1988/2000 - Loss: -7.640
Iter 1989/2000 - Loss: -7.640
Iter 1990/2000 - Loss: -7.640
Iter 1991/2000 - Loss: -7.640
Iter 1992/2000 - Loss: -7.640
Iter 1993/2000 - Loss: -7.640
Iter 1994/2000 - Loss: -7.641
Iter 1995/2000 - Loss: -7.641
Iter 1996/2000 - Loss: -7.641
Iter 1997/2000 - Loss: -7.641
Iter 1998/2000 - Loss: -7.641
Iter 1999/2000 - Loss: -7.641
Iter 2000/2000 - Loss: -7.641
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 7.6883,  8.4414, 32.0717, 11.1201, 23.1432, 53.4629]],

        [[17.8905, 30.7916, 10.3109,  1.2865,  3.6443, 28.4293]],

        [[18.8982, 30.4736,  9.0518,  1.0324,  1.1866, 24.6185]],

        [[18.4464, 35.1178, 18.1525,  2.4087,  1.3939, 45.3343]]])
Signal Variance: tensor([ 0.1716,  2.3969, 16.0493,  0.6069])
Estimated target variance: tensor([0.0209, 0.3369, 2.8823, 0.0585])
N: 130
Signal to noise ratio: tensor([23.3311, 91.5766, 86.1195, 45.1277])
Bound on condition number: tensor([  70765.0149, 1090217.5083,  964154.1987,  264747.1510])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0042352616380231765, policy loss: 7.183016904063339
Experience 13, Iter 1, disc loss: 0.004312015272790737, policy loss: 6.501427218717605
Experience 13, Iter 2, disc loss: 0.004045426955069243, policy loss: 6.749224874904942
Experience 13, Iter 3, disc loss: 0.0037432214046345764, policy loss: 7.258994295662829
Experience 13, Iter 4, disc loss: 0.004576881046276952, policy loss: 6.747671109064286
Experience 13, Iter 5, disc loss: 0.0032560502142033746, policy loss: 7.623799977312228
Experience 13, Iter 6, disc loss: 0.004331495382213313, policy loss: 6.3793884881645715
Experience 13, Iter 7, disc loss: 0.0037357095372694436, policy loss: 7.171180773809101
Experience 13, Iter 8, disc loss: 0.004142577613718812, policy loss: 6.448668036530118
Experience 13, Iter 9, disc loss: 0.004372411194013196, policy loss: 6.733760005913185
Experience 13, Iter 10, disc loss: 0.004489597540378651, policy loss: 7.574669253651498
Experience 13, Iter 11, disc loss: 0.005276447634294148, policy loss: 6.4214419640705875
Experience 13, Iter 12, disc loss: 0.004273572747241457, policy loss: 6.979971706976355
Experience 13, Iter 13, disc loss: 0.004316305822691809, policy loss: 7.423107404481623
Experience 13, Iter 14, disc loss: 0.004012792211893073, policy loss: 7.366519545022503
Experience 13, Iter 15, disc loss: 0.003993303657450204, policy loss: 7.013560117404342
Experience 13, Iter 16, disc loss: 0.0038699614484104155, policy loss: 7.257788343783739
Experience 13, Iter 17, disc loss: 0.00410130958509851, policy loss: 6.986351384024372
Experience 13, Iter 18, disc loss: 0.003682090340982322, policy loss: 7.090433032532196
Experience 13, Iter 19, disc loss: 0.0037662617411294884, policy loss: 7.628783674474807
Experience 13, Iter 20, disc loss: 0.0038563536676557903, policy loss: 6.7401883205723525
Experience 13, Iter 21, disc loss: 0.003992506012795487, policy loss: 7.124415091375234
Experience 13, Iter 22, disc loss: 0.0037839596245490877, policy loss: 7.5952920616598085
Experience 13, Iter 23, disc loss: 0.0037836260884928008, policy loss: 7.352347826615303
Experience 13, Iter 24, disc loss: 0.003574625477347903, policy loss: 7.340933753304539
Experience 13, Iter 25, disc loss: 0.0037826029200326, policy loss: 6.978731457743043
Experience 13, Iter 26, disc loss: 0.0039715347894915655, policy loss: 7.688236661716016
Experience 13, Iter 27, disc loss: 0.0036131124170900664, policy loss: 7.150223027828793
Experience 13, Iter 28, disc loss: 0.0036393151122037387, policy loss: 6.936094988111559
Experience 13, Iter 29, disc loss: 0.003227573644478883, policy loss: 7.468603039042884
Experience 13, Iter 30, disc loss: 0.003828901329451069, policy loss: 6.979289614728622
Experience 13, Iter 31, disc loss: 0.004095400252833722, policy loss: 7.327300770709077
Experience 13, Iter 32, disc loss: 0.0038307949657972007, policy loss: 6.861144787260432
Experience 13, Iter 33, disc loss: 0.0035212600263098046, policy loss: 7.711802416215854
Experience 13, Iter 34, disc loss: 0.003239731337122961, policy loss: 7.160602237662625
Experience 13, Iter 35, disc loss: 0.003458579818966896, policy loss: 7.05786626453925
Experience 13, Iter 36, disc loss: 0.0037650547147962225, policy loss: 6.780706319314076
Experience 13, Iter 37, disc loss: 0.0038837730186077634, policy loss: 6.946066811898393
Experience 13, Iter 38, disc loss: 0.0038312042978766612, policy loss: 6.653095038258394
Experience 13, Iter 39, disc loss: 0.0037256012236830784, policy loss: 7.620356834635817
Experience 13, Iter 40, disc loss: 0.0033682710674457837, policy loss: 7.4766994867137555
Experience 13, Iter 41, disc loss: 0.003990976220578435, policy loss: 7.199749096251673
Experience 13, Iter 42, disc loss: 0.00363088152737941, policy loss: 6.981191236289573
Experience 13, Iter 43, disc loss: 0.003437671395743988, policy loss: 8.01724118977807
Experience 13, Iter 44, disc loss: 0.003337264653895582, policy loss: 6.897500300731334
Experience 13, Iter 45, disc loss: 0.003590997748083701, policy loss: 6.923206022049591
Experience 13, Iter 46, disc loss: 0.003165833228159701, policy loss: 8.34870749173151
Experience 13, Iter 47, disc loss: 0.0034176843259920834, policy loss: 7.924102348631596
Experience 13, Iter 48, disc loss: 0.0037132803509503094, policy loss: 6.637365502030935
Experience 13, Iter 49, disc loss: 0.0035728154295863075, policy loss: 7.194194460624827
Experience 13, Iter 50, disc loss: 0.003037193316099477, policy loss: 7.678904635232872
Experience 13, Iter 51, disc loss: 0.00337665109010253, policy loss: 7.485284712537053
Experience 13, Iter 52, disc loss: 0.003338603438552552, policy loss: 7.410185786455047
Experience 13, Iter 53, disc loss: 0.002897114150481268, policy loss: 7.576328799529517
Experience 13, Iter 54, disc loss: 0.003470227865343649, policy loss: 7.205103155768391
Experience 13, Iter 55, disc loss: 0.0028552252197486076, policy loss: 7.610126437720687
Experience 13, Iter 56, disc loss: 0.002710822539441008, policy loss: 8.071051689281722
Experience 13, Iter 57, disc loss: 0.0023710391076126134, policy loss: 7.787011512393056
Experience 13, Iter 58, disc loss: 0.002344454587380246, policy loss: 8.050428942494948
Experience 13, Iter 59, disc loss: 0.0025141825521072555, policy loss: 7.925957923418382
Experience 13, Iter 60, disc loss: 0.0025772331994966856, policy loss: 7.550263140307337
Experience 13, Iter 61, disc loss: 0.002889063050305507, policy loss: 7.655621071832351
Experience 13, Iter 62, disc loss: 0.002577164542318683, policy loss: 7.619453243884675
Experience 13, Iter 63, disc loss: 0.002783065973888662, policy loss: 7.22103386418874
Experience 13, Iter 64, disc loss: 0.0019971786061502165, policy loss: 8.543937936321818
Experience 13, Iter 65, disc loss: 0.0028173598679374187, policy loss: 7.411615602885064
Experience 13, Iter 66, disc loss: 0.00291795833843366, policy loss: 7.870174657536117
Experience 13, Iter 67, disc loss: 0.0026204740582000292, policy loss: 7.396456729553773
Experience 13, Iter 68, disc loss: 0.002934346866702755, policy loss: 7.312817834348496
Experience 13, Iter 69, disc loss: 0.002823419460539763, policy loss: 7.451035542294831
Experience 13, Iter 70, disc loss: 0.0025230566298453504, policy loss: 7.525505216455841
Experience 13, Iter 71, disc loss: 0.0031988322516892705, policy loss: 7.440731628736835
Experience 13, Iter 72, disc loss: 0.0028947611676673245, policy loss: 7.45232630328953
Experience 13, Iter 73, disc loss: 0.0025900147305416084, policy loss: 7.339514712050809
Experience 13, Iter 74, disc loss: 0.002765825708267158, policy loss: 7.124718826115333
Experience 13, Iter 75, disc loss: 0.0023413671880991925, policy loss: 7.992608870828213
Experience 13, Iter 76, disc loss: 0.0029169817417466467, policy loss: 7.683189088069898
Experience 13, Iter 77, disc loss: 0.0025454769030837003, policy loss: 7.793590729843698
Experience 13, Iter 78, disc loss: 0.002842683950631619, policy loss: 7.886628711045749
Experience 13, Iter 79, disc loss: 0.002750352088158846, policy loss: 7.230008566888895
Experience 13, Iter 80, disc loss: 0.0030133539858195097, policy loss: 8.299377982512333
Experience 13, Iter 81, disc loss: 0.002864042176346249, policy loss: 7.153934329456242
Experience 13, Iter 82, disc loss: 0.0027905036017111615, policy loss: 7.215900616205552
Experience 13, Iter 83, disc loss: 0.0025442211724055483, policy loss: 7.357429676247854
Experience 13, Iter 84, disc loss: 0.0024593362078347775, policy loss: 7.320019779398472
Experience 13, Iter 85, disc loss: 0.0024988943633495724, policy loss: 7.5911108521108375
Experience 13, Iter 86, disc loss: 0.002810385869068149, policy loss: 7.712372562220098
Experience 13, Iter 87, disc loss: 0.00270053855229837, policy loss: 8.06004977340109
Experience 13, Iter 88, disc loss: 0.003140924277076731, policy loss: 6.913540556023925
Experience 13, Iter 89, disc loss: 0.0023202576240133384, policy loss: 7.486718237120369
Experience 13, Iter 90, disc loss: 0.0023967714605839475, policy loss: 7.533128869000545
Experience 13, Iter 91, disc loss: 0.002219155615669549, policy loss: 7.706750502493468
Experience 13, Iter 92, disc loss: 0.002067147042898228, policy loss: 7.688498488182681
Experience 13, Iter 93, disc loss: 0.0023436542464552084, policy loss: 7.7179311220088636
Experience 13, Iter 94, disc loss: 0.0027784849845208844, policy loss: 7.415996833485176
Experience 13, Iter 95, disc loss: 0.0025770251023072175, policy loss: 7.969404329504021
Experience 13, Iter 96, disc loss: 0.0026395329080907562, policy loss: 7.696094461235895
Experience 13, Iter 97, disc loss: 0.002391186890208146, policy loss: 7.652390709556282
Experience 13, Iter 98, disc loss: 0.0027771740438628785, policy loss: 7.109291039566282
Experience 13, Iter 99, disc loss: 0.0027201381296286828, policy loss: 7.113412061735448
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1033],
        [0.8426],
        [0.0174]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0256, 0.2000, 0.7960, 0.0161, 0.0145, 2.7428]],

        [[0.0256, 0.2000, 0.7960, 0.0161, 0.0145, 2.7428]],

        [[0.0256, 0.2000, 0.7960, 0.0161, 0.0145, 2.7428]],

        [[0.0256, 0.2000, 0.7960, 0.0161, 0.0145, 2.7428]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.4131, 3.3702, 0.0697], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.4131, 3.3702, 0.0697])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.593
Iter 2/2000 - Loss: 2.455
Iter 3/2000 - Loss: 2.412
Iter 4/2000 - Loss: 2.331
Iter 5/2000 - Loss: 2.267
Iter 6/2000 - Loss: 2.184
Iter 7/2000 - Loss: 2.080
Iter 8/2000 - Loss: 1.975
Iter 9/2000 - Loss: 1.861
Iter 10/2000 - Loss: 1.720
Iter 11/2000 - Loss: 1.551
Iter 12/2000 - Loss: 1.362
Iter 13/2000 - Loss: 1.161
Iter 14/2000 - Loss: 0.944
Iter 15/2000 - Loss: 0.707
Iter 16/2000 - Loss: 0.448
Iter 17/2000 - Loss: 0.170
Iter 18/2000 - Loss: -0.122
Iter 19/2000 - Loss: -0.419
Iter 20/2000 - Loss: -0.718
Iter 1981/2000 - Loss: -7.766
Iter 1982/2000 - Loss: -7.766
Iter 1983/2000 - Loss: -7.766
Iter 1984/2000 - Loss: -7.766
Iter 1985/2000 - Loss: -7.766
Iter 1986/2000 - Loss: -7.766
Iter 1987/2000 - Loss: -7.766
Iter 1988/2000 - Loss: -7.766
Iter 1989/2000 - Loss: -7.766
Iter 1990/2000 - Loss: -7.766
Iter 1991/2000 - Loss: -7.766
Iter 1992/2000 - Loss: -7.766
Iter 1993/2000 - Loss: -7.766
Iter 1994/2000 - Loss: -7.766
Iter 1995/2000 - Loss: -7.766
Iter 1996/2000 - Loss: -7.766
Iter 1997/2000 - Loss: -7.766
Iter 1998/2000 - Loss: -7.766
Iter 1999/2000 - Loss: -7.766
Iter 2000/2000 - Loss: -7.766
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 3.8197,  5.8471, 32.5863, 11.3725, 22.5834, 55.1571]],

        [[19.8334, 33.9847, 10.3585,  1.3177,  3.8980, 31.4035]],

        [[19.1511, 28.1587,  9.0860,  1.0441,  1.1213, 25.4650]],

        [[17.7731, 34.1822, 18.6316,  2.6250,  1.3532, 45.5238]]])
Signal Variance: tensor([ 0.1089,  2.8525, 15.7287,  0.5922])
Estimated target variance: tensor([0.0205, 0.4131, 3.3702, 0.0697])
N: 140
Signal to noise ratio: tensor([ 18.4973, 102.6308,  86.4494,  45.3650])
Bound on condition number: tensor([  47902.2179, 1474633.5228, 1046291.8270,  288118.7317])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.002929381608809493, policy loss: 6.95701710860564
Experience 14, Iter 1, disc loss: 0.0024204656732603747, policy loss: 7.557663942546299
Experience 14, Iter 2, disc loss: 0.003020322878603301, policy loss: 6.730286412988228
Experience 14, Iter 3, disc loss: 0.0030201608742989844, policy loss: 7.158980106972303
Experience 14, Iter 4, disc loss: 0.0027431538615244866, policy loss: 7.836856724180519
Experience 14, Iter 5, disc loss: 0.0026428906529912585, policy loss: 7.347310833308891
Experience 14, Iter 6, disc loss: 0.0027146967828012604, policy loss: 7.887911946277318
Experience 14, Iter 7, disc loss: 0.002559329262961191, policy loss: 7.692376339646172
Experience 14, Iter 8, disc loss: 0.0025361737846299496, policy loss: 7.536347608383218
Experience 14, Iter 9, disc loss: 0.0028553144708190004, policy loss: 7.202702886262226
Experience 14, Iter 10, disc loss: 0.0029182918115201054, policy loss: 7.836304033760263
Experience 14, Iter 11, disc loss: 0.0026300792353255043, policy loss: 7.529115706220944
Experience 14, Iter 12, disc loss: 0.0023378276578844065, policy loss: 7.789133398674338
Experience 14, Iter 13, disc loss: 0.0024048009615925325, policy loss: 7.542098282513526
Experience 14, Iter 14, disc loss: 0.0023774823463963675, policy loss: 7.689124073223689
Experience 14, Iter 15, disc loss: 0.0025471761752029836, policy loss: 7.926128158028794
Experience 14, Iter 16, disc loss: 0.0028969570473005527, policy loss: 7.634208888245066
Experience 14, Iter 17, disc loss: 0.0026141059431281375, policy loss: 8.052452221447219
Experience 14, Iter 18, disc loss: 0.0025202296704284425, policy loss: 8.055525178633948
Experience 14, Iter 19, disc loss: 0.0025244214799167624, policy loss: 7.7313879711419755
Experience 14, Iter 20, disc loss: 0.002778577698241618, policy loss: 7.659538260994051
Experience 14, Iter 21, disc loss: 0.002750529490343414, policy loss: 7.370917666769319
Experience 14, Iter 22, disc loss: 0.0025513169632594397, policy loss: 7.654022753977514
Experience 14, Iter 23, disc loss: 0.0027192286587809965, policy loss: 7.5289745042297165
Experience 14, Iter 24, disc loss: 0.0027278637156568602, policy loss: 8.512967029621409
Experience 14, Iter 25, disc loss: 0.002473969099523223, policy loss: 7.460358359061464
Experience 14, Iter 26, disc loss: 0.00224812161115909, policy loss: 7.7220038629953995
Experience 14, Iter 27, disc loss: 0.0020409201297908485, policy loss: 8.400316927594066
Experience 14, Iter 28, disc loss: 0.002302786510168699, policy loss: 7.829901416359995
Experience 14, Iter 29, disc loss: 0.002476013958228801, policy loss: 7.960385374296463
Experience 14, Iter 30, disc loss: 0.0022402694119781244, policy loss: 8.154642118128162
Experience 14, Iter 31, disc loss: 0.0022881638113927327, policy loss: 7.607867202386048
Experience 14, Iter 32, disc loss: 0.0026110224951996254, policy loss: 7.693975720364339
Experience 14, Iter 33, disc loss: 0.0022935800943624943, policy loss: 8.282646993714051
Experience 14, Iter 34, disc loss: 0.0025884590547971004, policy loss: 7.564866594091992
Experience 14, Iter 35, disc loss: 0.002158133092461046, policy loss: 7.670447475610024
Experience 14, Iter 36, disc loss: 0.0024569295459753817, policy loss: 8.038656519531223
Experience 14, Iter 37, disc loss: 0.0024328581966201602, policy loss: 7.433353744907269
Experience 14, Iter 38, disc loss: 0.0024137034385165895, policy loss: 7.166208414788812
Experience 14, Iter 39, disc loss: 0.0020421676136234404, policy loss: 8.607766178206868
Experience 14, Iter 40, disc loss: 0.002360152686264828, policy loss: 7.509001563495042
Experience 14, Iter 41, disc loss: 0.002368501482333741, policy loss: 7.993307257878666
Experience 14, Iter 42, disc loss: 0.0025596679957340307, policy loss: 7.638547630907124
Experience 14, Iter 43, disc loss: 0.002444659798646616, policy loss: 7.894545509222279
Experience 14, Iter 44, disc loss: 0.0023306841111045903, policy loss: 7.357757271763315
Experience 14, Iter 45, disc loss: 0.0024180555618750176, policy loss: 7.235620838353428
Experience 14, Iter 46, disc loss: 0.002337546055486567, policy loss: 7.82112222174414
Experience 14, Iter 47, disc loss: 0.002764374073872421, policy loss: 7.25197818235633
Experience 14, Iter 48, disc loss: 0.0027538863294721376, policy loss: 7.414762194632553
Experience 14, Iter 49, disc loss: 0.0022628614416083594, policy loss: 7.570632065746049
Experience 14, Iter 50, disc loss: 0.0026118363986817683, policy loss: 7.473250822222703
Experience 14, Iter 51, disc loss: 0.001965590951473358, policy loss: 8.203242612369346
Experience 14, Iter 52, disc loss: 0.002609775537659125, policy loss: 8.15122197966061
Experience 14, Iter 53, disc loss: 0.002496305075863412, policy loss: 7.587574739324321
Experience 14, Iter 54, disc loss: 0.002580096448990577, policy loss: 7.21036974466316
Experience 14, Iter 55, disc loss: 0.002538131029926372, policy loss: 7.671503262580461
Experience 14, Iter 56, disc loss: 0.002263333331177146, policy loss: 7.675475031032196
Experience 14, Iter 57, disc loss: 0.0021500251482742764, policy loss: 7.633131308992019
Experience 14, Iter 58, disc loss: 0.0027290697241425764, policy loss: 7.253556833219364
Experience 14, Iter 59, disc loss: 0.002576776330237504, policy loss: 7.58357621488954
Experience 14, Iter 60, disc loss: 0.002360809435095008, policy loss: 7.590849791182988
Experience 14, Iter 61, disc loss: 0.002185207791105438, policy loss: 7.500890956869848
Experience 14, Iter 62, disc loss: 0.002015629775420512, policy loss: 8.084681628243818
Experience 14, Iter 63, disc loss: 0.0020563958035701045, policy loss: 8.000236806687255
Experience 14, Iter 64, disc loss: 0.0020967306408451424, policy loss: 7.990049794547965
Experience 14, Iter 65, disc loss: 0.002020222031835335, policy loss: 7.836285613022065
Experience 14, Iter 66, disc loss: 0.002269509951196267, policy loss: 8.042421000397868
Experience 14, Iter 67, disc loss: 0.0023359011095505713, policy loss: 7.956868708305797
Experience 14, Iter 68, disc loss: 0.001919114010283224, policy loss: 8.236952153811005
Experience 14, Iter 69, disc loss: 0.0018826419678700096, policy loss: 8.00812147692894
Experience 14, Iter 70, disc loss: 0.0019431248675035085, policy loss: 7.7806661641433
Experience 14, Iter 71, disc loss: 0.001797283383888548, policy loss: 8.209437889328852
Experience 14, Iter 72, disc loss: 0.0016133162637664538, policy loss: 9.339432738071444
Experience 14, Iter 73, disc loss: 0.0016636517521226015, policy loss: 8.357609673659665
Experience 14, Iter 74, disc loss: 0.0017639094861651885, policy loss: 8.203320432801618
Experience 14, Iter 75, disc loss: 0.0020637924025248434, policy loss: 7.716951252927386
Experience 14, Iter 76, disc loss: 0.0018506901572637171, policy loss: 8.537667576780953
Experience 14, Iter 77, disc loss: 0.0023553932929465185, policy loss: 7.641168814930037
Experience 14, Iter 78, disc loss: 0.001731585323004247, policy loss: 8.078984546394771
Experience 14, Iter 79, disc loss: 0.0016529804968055096, policy loss: 8.769836650215522
Experience 14, Iter 80, disc loss: 0.0016642453236426677, policy loss: 8.087327489155665
Experience 14, Iter 81, disc loss: 0.001751399243631558, policy loss: 8.165488928317306
Experience 14, Iter 82, disc loss: 0.001897876476853549, policy loss: 8.397991562751008
Experience 14, Iter 83, disc loss: 0.0018773980684885596, policy loss: 7.5719086959271795
Experience 14, Iter 84, disc loss: 0.0020019102722637597, policy loss: 7.64665192771537
Experience 14, Iter 85, disc loss: 0.0019130577529401563, policy loss: 7.850950536805303
Experience 14, Iter 86, disc loss: 0.0018012847273347018, policy loss: 7.71483718344367
Experience 14, Iter 87, disc loss: 0.0017479609619266933, policy loss: 8.074019411530632
Experience 14, Iter 88, disc loss: 0.0018073677754683985, policy loss: 8.222886024552992
Experience 14, Iter 89, disc loss: 0.0017283251468835587, policy loss: 8.258207059177371
Experience 14, Iter 90, disc loss: 0.001794981815372493, policy loss: 7.76888845528506
Experience 14, Iter 91, disc loss: 0.0017808900467693178, policy loss: 8.047674647630021
Experience 14, Iter 92, disc loss: 0.0015937684485271637, policy loss: 8.484732450604414
Experience 14, Iter 93, disc loss: 0.0022344515783937952, policy loss: 7.682170504072964
Experience 14, Iter 94, disc loss: 0.0016780886390060626, policy loss: 7.977166321902369
Experience 14, Iter 95, disc loss: 0.0013436489933406036, policy loss: 8.813653470971758
Experience 14, Iter 96, disc loss: 0.0012274970416004363, policy loss: 9.362468269302472
Experience 14, Iter 97, disc loss: 0.001209407517684122, policy loss: 8.806007609559579
Experience 14, Iter 98, disc loss: 0.0012184091664418596, policy loss: 9.363665654232253
Experience 14, Iter 99, disc loss: 0.0012186218714287868, policy loss: 8.808223487769888
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1204],
        [0.9573],
        [0.0203]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0249, 0.1999, 0.9197, 0.0174, 0.0171, 3.1048]],

        [[0.0249, 0.1999, 0.9197, 0.0174, 0.0171, 3.1048]],

        [[0.0249, 0.1999, 0.9197, 0.0174, 0.0171, 3.1048]],

        [[0.0249, 0.1999, 0.9197, 0.0174, 0.0171, 3.1048]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.4816, 3.8293, 0.0810], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.4816, 3.8293, 0.0810])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.804
Iter 2/2000 - Loss: 2.674
Iter 3/2000 - Loss: 2.600
Iter 4/2000 - Loss: 2.531
Iter 5/2000 - Loss: 2.479
Iter 6/2000 - Loss: 2.380
Iter 7/2000 - Loss: 2.258
Iter 8/2000 - Loss: 2.146
Iter 9/2000 - Loss: 2.029
Iter 10/2000 - Loss: 1.881
Iter 11/2000 - Loss: 1.699
Iter 12/2000 - Loss: 1.494
Iter 13/2000 - Loss: 1.276
Iter 14/2000 - Loss: 1.044
Iter 15/2000 - Loss: 0.796
Iter 16/2000 - Loss: 0.531
Iter 17/2000 - Loss: 0.251
Iter 18/2000 - Loss: -0.038
Iter 19/2000 - Loss: -0.332
Iter 20/2000 - Loss: -0.628
Iter 1981/2000 - Loss: -7.769
Iter 1982/2000 - Loss: -7.769
Iter 1983/2000 - Loss: -7.769
Iter 1984/2000 - Loss: -7.769
Iter 1985/2000 - Loss: -7.769
Iter 1986/2000 - Loss: -7.769
Iter 1987/2000 - Loss: -7.769
Iter 1988/2000 - Loss: -7.769
Iter 1989/2000 - Loss: -7.769
Iter 1990/2000 - Loss: -7.769
Iter 1991/2000 - Loss: -7.769
Iter 1992/2000 - Loss: -7.769
Iter 1993/2000 - Loss: -7.769
Iter 1994/2000 - Loss: -7.769
Iter 1995/2000 - Loss: -7.769
Iter 1996/2000 - Loss: -7.769
Iter 1997/2000 - Loss: -7.769
Iter 1998/2000 - Loss: -7.769
Iter 1999/2000 - Loss: -7.769
Iter 2000/2000 - Loss: -7.769
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[ 5.2307,  5.7590, 37.0449, 12.2371, 20.0714, 54.7112]],

        [[15.6749, 30.7320, 12.1974,  1.0785,  7.4209, 21.1119]],

        [[19.1235, 31.1190,  9.1432,  1.0432,  0.8744, 24.8296]],

        [[16.8722, 31.4270, 13.9634,  1.8041,  1.9648, 36.8274]]])
Signal Variance: tensor([ 0.1196,  1.9639, 13.1225,  0.4208])
Estimated target variance: tensor([0.0205, 0.4816, 3.8293, 0.0810])
N: 150
Signal to noise ratio: tensor([18.9418, 80.6569, 80.5342, 38.6375])
Bound on condition number: tensor([ 53819.9894, 975832.0560, 972865.5663, 223929.3731])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0012713963396354944, policy loss: 9.087827689594961
Experience 15, Iter 1, disc loss: 0.0014010094494614658, policy loss: 8.439284850958494
Experience 15, Iter 2, disc loss: 0.0013838546778385521, policy loss: 7.9348656916572775
Experience 15, Iter 3, disc loss: 0.001219907781011218, policy loss: 8.416563447860419
Experience 15, Iter 4, disc loss: 0.0013490916520537775, policy loss: 8.107087504225461
Experience 15, Iter 5, disc loss: 0.0011810320671683144, policy loss: 8.478784090418468
Experience 15, Iter 6, disc loss: 0.0013787263672734324, policy loss: 8.798962733364192
Experience 15, Iter 7, disc loss: 0.0013792501724337299, policy loss: 8.762817474214067
Experience 15, Iter 8, disc loss: 0.0015048745037092013, policy loss: 7.958466027486838
Experience 15, Iter 9, disc loss: 0.001642141878399505, policy loss: 7.858612051419549
Experience 15, Iter 10, disc loss: 0.0015399601055566402, policy loss: 8.023349121706124
Experience 15, Iter 11, disc loss: 0.001674037511903268, policy loss: 7.9007933058104864
Experience 15, Iter 12, disc loss: 0.0014539055600792253, policy loss: 7.970095875611275
Experience 15, Iter 13, disc loss: 0.0017412361324476017, policy loss: 8.129295439987578
Experience 15, Iter 14, disc loss: 0.0019628436456112387, policy loss: 7.553836786698817
Experience 15, Iter 15, disc loss: 0.001608361172405258, policy loss: 8.612847912939088
Experience 15, Iter 16, disc loss: 0.0015712694792351352, policy loss: 7.7965393992290615
Experience 15, Iter 17, disc loss: 0.001506301253341317, policy loss: 8.217383427777637
Experience 15, Iter 18, disc loss: 0.001765576409533748, policy loss: 7.512163143785676
Experience 15, Iter 19, disc loss: 0.0017502102843480253, policy loss: 8.06342580801908
Experience 15, Iter 20, disc loss: 0.001704505040974902, policy loss: 7.873530593552887
Experience 15, Iter 21, disc loss: 0.0016425157251138748, policy loss: 8.093814867466643
Experience 15, Iter 22, disc loss: 0.0014839093675933306, policy loss: 8.339849963316812
Experience 15, Iter 23, disc loss: 0.001436222272837587, policy loss: 8.131302595145367
Experience 15, Iter 24, disc loss: 0.0013436591558723, policy loss: 8.3103019410514
Experience 15, Iter 25, disc loss: 0.0012950595281751053, policy loss: 8.411072327341625
Experience 15, Iter 26, disc loss: 0.0012961244170706032, policy loss: 8.662448091577545
Experience 15, Iter 27, disc loss: 0.0014130743635134987, policy loss: 8.41796258219482
Experience 15, Iter 28, disc loss: 0.0016441677657577226, policy loss: 8.490292310860141
Experience 15, Iter 29, disc loss: 0.001531266800160888, policy loss: 8.561719123919168
Experience 15, Iter 30, disc loss: 0.0017075915235644887, policy loss: 7.7756005781256246
Experience 15, Iter 31, disc loss: 0.001485108036318455, policy loss: 8.297013378946728
Experience 15, Iter 32, disc loss: 0.0015228978233222253, policy loss: 8.482544009071546
Experience 15, Iter 33, disc loss: 0.001481376059666695, policy loss: 8.223913916894725
Experience 15, Iter 34, disc loss: 0.0013948227648086322, policy loss: 8.078567472387261
Experience 15, Iter 35, disc loss: 0.0015758476204489119, policy loss: 7.729328384256222
Experience 15, Iter 36, disc loss: 0.0015736661777606812, policy loss: 7.836194398597973
Experience 15, Iter 37, disc loss: 0.001396808502707304, policy loss: 8.191273807301375
Experience 15, Iter 38, disc loss: 0.0013292838190524696, policy loss: 8.079182488336734
Experience 15, Iter 39, disc loss: 0.0014072098211721103, policy loss: 9.016989444889663
Experience 15, Iter 40, disc loss: 0.001407767709538971, policy loss: 8.275276444765398
Experience 15, Iter 41, disc loss: 0.001558005218648002, policy loss: 7.688008240521105
Experience 15, Iter 42, disc loss: 0.0013559288785772207, policy loss: 7.865369608363888
Experience 15, Iter 43, disc loss: 0.0013855055071899107, policy loss: 8.570059305317034
Experience 15, Iter 44, disc loss: 0.0014314607019117387, policy loss: 7.939502617670049
Experience 15, Iter 45, disc loss: 0.00158177338839915, policy loss: 8.015956462468413
Experience 15, Iter 46, disc loss: 0.0015850480526476852, policy loss: 7.980557290537228
Experience 15, Iter 47, disc loss: 0.0015027635487964582, policy loss: 8.35950692586702
Experience 15, Iter 48, disc loss: 0.0016143095355808745, policy loss: 8.204059612134381
Experience 15, Iter 49, disc loss: 0.0015847834788736575, policy loss: 8.064824578221247
Experience 15, Iter 50, disc loss: 0.0015249381078062257, policy loss: 7.6604994584394825
Experience 15, Iter 51, disc loss: 0.0015987623575331907, policy loss: 8.013109546098843
Experience 15, Iter 52, disc loss: 0.0016967868446873457, policy loss: 7.535029436933875
Experience 15, Iter 53, disc loss: 0.001785048574263586, policy loss: 7.450019620333474
Experience 15, Iter 54, disc loss: 0.001830413544032271, policy loss: 8.071130822009593
Experience 15, Iter 55, disc loss: 0.0015605254245496893, policy loss: 7.862497365051407
Experience 15, Iter 56, disc loss: 0.0014160372480471724, policy loss: 7.954978212643134
Experience 15, Iter 57, disc loss: 0.0014830164970278936, policy loss: 8.040811206818496
Experience 15, Iter 58, disc loss: 0.0015601943840266915, policy loss: 8.421176322523538
Experience 15, Iter 59, disc loss: 0.0014708498958724918, policy loss: 8.29291077673728
Experience 15, Iter 60, disc loss: 0.0015462921088352758, policy loss: 8.174941707316638
Experience 15, Iter 61, disc loss: 0.0016059973948285098, policy loss: 7.915525195577844
Experience 15, Iter 62, disc loss: 0.0014900821647045271, policy loss: 8.13328578827393
Experience 15, Iter 63, disc loss: 0.0017119710444622788, policy loss: 8.46542034397653
Experience 15, Iter 64, disc loss: 0.001707072429428697, policy loss: 8.659287032377469
Experience 15, Iter 65, disc loss: 0.0014148308313356912, policy loss: 7.971603888488306
Experience 15, Iter 66, disc loss: 0.001262112773983953, policy loss: 8.357153654669807
Experience 15, Iter 67, disc loss: 0.0011191779418611152, policy loss: 8.90700728288442
Experience 15, Iter 68, disc loss: 0.0011710663327989448, policy loss: 8.971516614870083
Experience 15, Iter 69, disc loss: 0.0013152303685691493, policy loss: 8.934240934467471
Experience 15, Iter 70, disc loss: 0.0011651492201519569, policy loss: 8.788358660576094
Experience 15, Iter 71, disc loss: 0.001342660803202117, policy loss: 8.575997157683481
Experience 15, Iter 72, disc loss: 0.001561678094837539, policy loss: 8.638645589684533
Experience 15, Iter 73, disc loss: 0.0009928422356971047, policy loss: 9.622442185131252
Experience 15, Iter 74, disc loss: 0.0013850235994967708, policy loss: 8.866788996207974
Experience 15, Iter 75, disc loss: 0.0011590585399173861, policy loss: 8.621435540162274
Experience 15, Iter 76, disc loss: 0.0010839917848461211, policy loss: 9.042714258871257
Experience 15, Iter 77, disc loss: 0.0010113216567581224, policy loss: 9.06389463136602
Experience 15, Iter 78, disc loss: 0.0009151487880133392, policy loss: 9.222130722776427
Experience 15, Iter 79, disc loss: 0.0010245334863706308, policy loss: 9.074838911820848
Experience 15, Iter 80, disc loss: 0.0009639169238936722, policy loss: 8.780912629913724
Experience 15, Iter 81, disc loss: 0.0010765862903609758, policy loss: 8.73161332707473
Experience 15, Iter 82, disc loss: 0.0013263006183870808, policy loss: 8.216535343565889
Experience 15, Iter 83, disc loss: 0.001608719038608406, policy loss: 7.632355506376746
Experience 15, Iter 84, disc loss: 0.0011906756313385905, policy loss: 8.337461202561363
Experience 15, Iter 85, disc loss: 0.0014522592886389988, policy loss: 8.5561771565161
Experience 15, Iter 86, disc loss: 0.0012250536045111866, policy loss: 8.8527990330822
Experience 15, Iter 87, disc loss: 0.0014799567632845768, policy loss: 8.472641358938764
Experience 15, Iter 88, disc loss: 0.0012040466420180787, policy loss: 8.397853626000067
Experience 15, Iter 89, disc loss: 0.0011365166967077394, policy loss: 8.186433821246315
Experience 15, Iter 90, disc loss: 0.0013189907206696703, policy loss: 8.106315845446638
Experience 15, Iter 91, disc loss: 0.0015609243764222553, policy loss: 7.889175421445385
Experience 15, Iter 92, disc loss: 0.0015453784260812675, policy loss: 8.208192668662498
Experience 15, Iter 93, disc loss: 0.0012620773976041385, policy loss: 8.666832033948227
Experience 15, Iter 94, disc loss: 0.0015574328025916044, policy loss: 8.10528157975444
Experience 15, Iter 95, disc loss: 0.0014633722470877404, policy loss: 8.547222693053214
Experience 15, Iter 96, disc loss: 0.0013269188966372918, policy loss: 8.406964935896879
Experience 15, Iter 97, disc loss: 0.0014092673977480388, policy loss: 8.65858748869972
Experience 15, Iter 98, disc loss: 0.0012845796425594658, policy loss: 8.787613172909033
Experience 15, Iter 99, disc loss: 0.0012341634720645417, policy loss: 8.210970996885003
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.1355],
        [1.0677],
        [0.0223]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0237, 0.1998, 1.0187, 0.0185, 0.0187, 3.4226]],

        [[0.0237, 0.1998, 1.0187, 0.0185, 0.0187, 3.4226]],

        [[0.0237, 0.1998, 1.0187, 0.0185, 0.0187, 3.4226]],

        [[0.0237, 0.1998, 1.0187, 0.0185, 0.0187, 3.4226]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0202, 0.5419, 4.2707, 0.0890], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0202, 0.5419, 4.2707, 0.0890])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.925
Iter 2/2000 - Loss: 2.795
Iter 3/2000 - Loss: 2.703
Iter 4/2000 - Loss: 2.628
Iter 5/2000 - Loss: 2.571
Iter 6/2000 - Loss: 2.460
Iter 7/2000 - Loss: 2.322
Iter 8/2000 - Loss: 2.195
Iter 9/2000 - Loss: 2.063
Iter 10/2000 - Loss: 1.899
Iter 11/2000 - Loss: 1.702
Iter 12/2000 - Loss: 1.482
Iter 13/2000 - Loss: 1.248
Iter 14/2000 - Loss: 1.003
Iter 15/2000 - Loss: 0.744
Iter 16/2000 - Loss: 0.472
Iter 17/2000 - Loss: 0.190
Iter 18/2000 - Loss: -0.099
Iter 19/2000 - Loss: -0.392
Iter 20/2000 - Loss: -0.685
Iter 1981/2000 - Loss: -7.847
Iter 1982/2000 - Loss: -7.847
Iter 1983/2000 - Loss: -7.847
Iter 1984/2000 - Loss: -7.847
Iter 1985/2000 - Loss: -7.848
Iter 1986/2000 - Loss: -7.848
Iter 1987/2000 - Loss: -7.848
Iter 1988/2000 - Loss: -7.848
Iter 1989/2000 - Loss: -7.848
Iter 1990/2000 - Loss: -7.848
Iter 1991/2000 - Loss: -7.848
Iter 1992/2000 - Loss: -7.848
Iter 1993/2000 - Loss: -7.848
Iter 1994/2000 - Loss: -7.848
Iter 1995/2000 - Loss: -7.848
Iter 1996/2000 - Loss: -7.848
Iter 1997/2000 - Loss: -7.848
Iter 1998/2000 - Loss: -7.848
Iter 1999/2000 - Loss: -7.848
Iter 2000/2000 - Loss: -7.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[ 5.0149,  5.4034, 36.1366, 11.8739, 16.8569, 54.7506]],

        [[16.3028, 31.8441, 11.7543,  1.0898,  5.5254, 27.5011]],

        [[18.5981, 31.9125,  9.1291,  0.9850,  0.9167, 24.2478]],

        [[16.6787, 30.6671, 14.9438,  1.6117,  2.1144, 40.2830]]])
Signal Variance: tensor([ 0.1146,  2.4117, 12.6805,  0.4496])
Estimated target variance: tensor([0.0202, 0.5419, 4.2707, 0.0890])
N: 160
Signal to noise ratio: tensor([18.9469, 86.6290, 81.2740, 40.2620])
Bound on condition number: tensor([  57438.7076, 1200733.3327, 1056874.1275,  259365.1112])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0018664607560468197, policy loss: 8.490565462339307
Experience 16, Iter 1, disc loss: 0.0015254131829904325, policy loss: 7.736406757160445
Experience 16, Iter 2, disc loss: 0.0015287240477780567, policy loss: 8.44156677839033
Experience 16, Iter 3, disc loss: 0.0014535373035163403, policy loss: 8.842922440267387
Experience 16, Iter 4, disc loss: 0.0013601174913170267, policy loss: 8.977319003715621
Experience 16, Iter 5, disc loss: 0.0014841750326707514, policy loss: 8.155887163934256
Experience 16, Iter 6, disc loss: 0.001437066542360118, policy loss: 8.03692948143078
Experience 16, Iter 7, disc loss: 0.0015919558189453687, policy loss: 8.410075476632286
Experience 16, Iter 8, disc loss: 0.0012264347046578117, policy loss: 8.103394441333275
Experience 16, Iter 9, disc loss: 0.0013552200884508912, policy loss: 8.399054641007076
Experience 16, Iter 10, disc loss: 0.0013431787774494438, policy loss: 8.236348230266774
Experience 16, Iter 11, disc loss: 0.0013418567748147295, policy loss: 8.121228733932563
Experience 16, Iter 12, disc loss: 0.0014622725094754613, policy loss: 8.090183013954974
Experience 16, Iter 13, disc loss: 0.0012942562025839931, policy loss: 8.283091360145605
Experience 16, Iter 14, disc loss: 0.0013774740721242927, policy loss: 8.608297121244133
Experience 16, Iter 15, disc loss: 0.0014373631173525221, policy loss: 8.1015160343537
Experience 16, Iter 16, disc loss: 0.0015940668656667292, policy loss: 8.092990042128985
Experience 16, Iter 17, disc loss: 0.0013212746690113345, policy loss: 8.032073049236027
Experience 16, Iter 18, disc loss: 0.001298641688635591, policy loss: 8.425303453184839
Experience 16, Iter 19, disc loss: 0.0015058257783779365, policy loss: 8.348224126734713
Experience 16, Iter 20, disc loss: 0.0016247442815533887, policy loss: 7.631568788645179
Experience 16, Iter 21, disc loss: 0.001240436434654442, policy loss: 9.133715476879845
Experience 16, Iter 22, disc loss: 0.0013077218608427534, policy loss: 8.102505865451569
Experience 16, Iter 23, disc loss: 0.0013507350317534259, policy loss: 8.94457271540735
Experience 16, Iter 24, disc loss: 0.0011625033233926658, policy loss: 9.094216990601211
Experience 16, Iter 25, disc loss: 0.0012158237915226105, policy loss: 8.623108776255062
Experience 16, Iter 26, disc loss: 0.0012043944557728592, policy loss: 8.419913260122806
Experience 16, Iter 27, disc loss: 0.0014193835200145143, policy loss: 8.178985196838143
Experience 16, Iter 28, disc loss: 0.0012462133134482777, policy loss: 9.483802548454374
Experience 16, Iter 29, disc loss: 0.001207453113349315, policy loss: 8.708513690233412
Experience 16, Iter 30, disc loss: 0.0010082816179418517, policy loss: 9.795332960499142
Experience 16, Iter 31, disc loss: 0.0009431802116839468, policy loss: 9.490462802171525
Experience 16, Iter 32, disc loss: 0.0009412591827977893, policy loss: 9.40881509219157
Experience 16, Iter 33, disc loss: 0.0009661094147481749, policy loss: 9.407132704630566
Experience 16, Iter 34, disc loss: 0.0009894672898224151, policy loss: 9.530392718518895
Experience 16, Iter 35, disc loss: 0.0010027097708681177, policy loss: 9.467279899387957
Experience 16, Iter 36, disc loss: 0.001003082686174381, policy loss: 8.779764354226488
Experience 16, Iter 37, disc loss: 0.00120640661823801, policy loss: 8.140447280973973
Experience 16, Iter 38, disc loss: 0.0012493832921098512, policy loss: 8.371619631678298
Experience 16, Iter 39, disc loss: 0.0009773017105433176, policy loss: 8.869401116109245
Experience 16, Iter 40, disc loss: 0.0010388882829661446, policy loss: 8.825144112587012
Experience 16, Iter 41, disc loss: 0.0011421216626869121, policy loss: 8.66574235401671
Experience 16, Iter 42, disc loss: 0.0009942001677729069, policy loss: 9.298717808264659
Experience 16, Iter 43, disc loss: 0.0011780027475282642, policy loss: 8.419059820241475
Experience 16, Iter 44, disc loss: 0.0013405015454830513, policy loss: 8.531034092478187
Experience 16, Iter 45, disc loss: 0.0011862735101825501, policy loss: 8.633544462621593
Experience 16, Iter 46, disc loss: 0.0014859401708119282, policy loss: 8.18056841608943
Experience 16, Iter 47, disc loss: 0.00136158652964323, policy loss: 8.794060164223684
Experience 16, Iter 48, disc loss: 0.0013058955033540612, policy loss: 8.109650749653548
Experience 16, Iter 49, disc loss: 0.0013265279199204576, policy loss: 7.898241522471075
Experience 16, Iter 50, disc loss: 0.001396964418631293, policy loss: 8.921270644330164
Experience 16, Iter 51, disc loss: 0.0012095413410919912, policy loss: 8.377870640913056
Experience 16, Iter 52, disc loss: 0.0013507067864657043, policy loss: 7.966881572134621
Experience 16, Iter 53, disc loss: 0.0012274771948573996, policy loss: 8.782521104759851
Experience 16, Iter 54, disc loss: 0.0012199581572519243, policy loss: 8.209451393639668
Experience 16, Iter 55, disc loss: 0.0011235280196927088, policy loss: 8.412283663020252
Experience 16, Iter 56, disc loss: 0.0011432692228317825, policy loss: 8.813117691603235
Experience 16, Iter 57, disc loss: 0.0012148087468488102, policy loss: 8.209858696386858
Experience 16, Iter 58, disc loss: 0.001219636280652009, policy loss: 8.112629931835718
Experience 16, Iter 59, disc loss: 0.0015142765953545636, policy loss: 8.072478936317601
Experience 16, Iter 60, disc loss: 0.0012854631248800917, policy loss: 8.888362248987642
Experience 16, Iter 61, disc loss: 0.0012153236004768127, policy loss: 8.5704287941486
Experience 16, Iter 62, disc loss: 0.0010759910980678506, policy loss: 8.48253504237119
Experience 16, Iter 63, disc loss: 0.0011822296152909394, policy loss: 8.622968014286087
Experience 16, Iter 64, disc loss: 0.0015651439193770564, policy loss: 7.737675816732423
Experience 16, Iter 65, disc loss: 0.0012556917715264128, policy loss: 8.168815586077583
Experience 16, Iter 66, disc loss: 0.0015436535423503741, policy loss: 7.6811905833763525
Experience 16, Iter 67, disc loss: 0.0012080161776449319, policy loss: 8.165275424686739
Experience 16, Iter 68, disc loss: 0.001466925828506359, policy loss: 7.953675770233481
Experience 16, Iter 69, disc loss: 0.001076769947938218, policy loss: 8.186339331765502
Experience 16, Iter 70, disc loss: 0.0011471373777249673, policy loss: 8.700811682092256
Experience 16, Iter 71, disc loss: 0.001214849318820597, policy loss: 8.508233614628526
Experience 16, Iter 72, disc loss: 0.0011792034898283105, policy loss: 8.383924696847318
Experience 16, Iter 73, disc loss: 0.001347326345308582, policy loss: 8.692852220934494
Experience 16, Iter 74, disc loss: 0.0013527758249168081, policy loss: 8.016199232878947
Experience 16, Iter 75, disc loss: 0.001266236786321267, policy loss: 8.628859219893982
Experience 16, Iter 76, disc loss: 0.0013921845052209622, policy loss: 8.089393013747447
Experience 16, Iter 77, disc loss: 0.0012598025391815824, policy loss: 8.317425843919427
Experience 16, Iter 78, disc loss: 0.001299470664346491, policy loss: 8.248326131807916
Experience 16, Iter 79, disc loss: 0.0014110615539866876, policy loss: 8.498853750537345
Experience 16, Iter 80, disc loss: 0.0011468531056911305, policy loss: 9.22029445617392
Experience 16, Iter 81, disc loss: 0.0012837284409261285, policy loss: 8.671597202777928
Experience 16, Iter 82, disc loss: 0.0012129737440250087, policy loss: 8.26224200070812
Experience 16, Iter 83, disc loss: 0.0013331506083315249, policy loss: 8.388283462419844
Experience 16, Iter 84, disc loss: 0.0014017258107415134, policy loss: 9.538274036117235
Experience 16, Iter 85, disc loss: 0.0013992952091837953, policy loss: 8.971997618461817
Experience 16, Iter 86, disc loss: 0.0012798687214159712, policy loss: 8.489271290340465
Experience 16, Iter 87, disc loss: 0.0012316298829087038, policy loss: 8.312996993332572
Experience 16, Iter 88, disc loss: 0.0009899698557810548, policy loss: 9.685227192717342
Experience 16, Iter 89, disc loss: 0.0011240763497848982, policy loss: 8.722069796332903
Experience 16, Iter 90, disc loss: 0.0012851722818698328, policy loss: 8.58258318140064
Experience 16, Iter 91, disc loss: 0.0012109269626501048, policy loss: 8.212689790948643
Experience 16, Iter 92, disc loss: 0.001178312423231419, policy loss: 8.318666300446402
Experience 16, Iter 93, disc loss: 0.0012051530023555706, policy loss: 8.708457989297917
Experience 16, Iter 94, disc loss: 0.0011649982147521418, policy loss: 8.693998188932348
Experience 16, Iter 95, disc loss: 0.0011627030328799728, policy loss: 9.117937071431047
Experience 16, Iter 96, disc loss: 0.00124378358537887, policy loss: 8.260372661662469
Experience 16, Iter 97, disc loss: 0.001004240725882987, policy loss: 9.3548145942217
Experience 16, Iter 98, disc loss: 0.0013354202043602845, policy loss: 8.181252762548763
Experience 16, Iter 99, disc loss: 0.0010198550713843213, policy loss: 8.565745331562061
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.1465],
        [1.1399],
        [0.0237]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0227, 0.1980, 1.0884, 0.0193, 0.0201, 3.6832]],

        [[0.0227, 0.1980, 1.0884, 0.0193, 0.0201, 3.6832]],

        [[0.0227, 0.1980, 1.0884, 0.0193, 0.0201, 3.6832]],

        [[0.0227, 0.1980, 1.0884, 0.0193, 0.0201, 3.6832]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0199, 0.5862, 4.5597, 0.0946], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0199, 0.5862, 4.5597, 0.0946])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.974
Iter 2/2000 - Loss: 2.839
Iter 3/2000 - Loss: 2.726
Iter 4/2000 - Loss: 2.641
Iter 5/2000 - Loss: 2.577
Iter 6/2000 - Loss: 2.452
Iter 7/2000 - Loss: 2.297
Iter 8/2000 - Loss: 2.155
Iter 9/2000 - Loss: 2.010
Iter 10/2000 - Loss: 1.836
Iter 11/2000 - Loss: 1.628
Iter 12/2000 - Loss: 1.398
Iter 13/2000 - Loss: 1.154
Iter 14/2000 - Loss: 0.900
Iter 15/2000 - Loss: 0.633
Iter 16/2000 - Loss: 0.356
Iter 17/2000 - Loss: 0.070
Iter 18/2000 - Loss: -0.220
Iter 19/2000 - Loss: -0.513
Iter 20/2000 - Loss: -0.805
Iter 1981/2000 - Loss: -7.876
Iter 1982/2000 - Loss: -7.876
Iter 1983/2000 - Loss: -7.876
Iter 1984/2000 - Loss: -7.876
Iter 1985/2000 - Loss: -7.876
Iter 1986/2000 - Loss: -7.876
Iter 1987/2000 - Loss: -7.876
Iter 1988/2000 - Loss: -7.876
Iter 1989/2000 - Loss: -7.876
Iter 1990/2000 - Loss: -7.876
Iter 1991/2000 - Loss: -7.876
Iter 1992/2000 - Loss: -7.876
Iter 1993/2000 - Loss: -7.876
Iter 1994/2000 - Loss: -7.876
Iter 1995/2000 - Loss: -7.876
Iter 1996/2000 - Loss: -7.877
Iter 1997/2000 - Loss: -7.877
Iter 1998/2000 - Loss: -7.877
Iter 1999/2000 - Loss: -7.877
Iter 2000/2000 - Loss: -7.877
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[ 5.9841,  4.7544, 38.8679, 12.1593, 14.5959, 45.9438]],

        [[16.5481, 32.1375, 10.9864,  1.1793,  5.3982, 30.5191]],

        [[18.4871, 29.2437,  9.1272,  0.9847,  0.9956, 25.3341]],

        [[16.4460, 29.9840, 15.3147,  1.5214,  2.2407, 41.5215]]])
Signal Variance: tensor([ 0.1061,  2.6094, 13.5379,  0.4573])
Estimated target variance: tensor([0.0199, 0.5862, 4.5597, 0.0946])
N: 170
Signal to noise ratio: tensor([17.4647, 87.1854, 84.0693, 41.2524])
Bound on condition number: tensor([  51853.6826, 1292221.1316, 1201500.9037,  289299.9730])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.001309468687760659, policy loss: 8.379947547025068
Experience 17, Iter 1, disc loss: 0.0011802892283059806, policy loss: 8.594301562455986
Experience 17, Iter 2, disc loss: 0.0010995450915022274, policy loss: 9.239044611177846
Experience 17, Iter 3, disc loss: 0.0011780418029618453, policy loss: 8.333069939396768
Experience 17, Iter 4, disc loss: 0.0011927240473535885, policy loss: 8.247296265707789
Experience 17, Iter 5, disc loss: 0.001308852741911017, policy loss: 8.683855900782198
Experience 17, Iter 6, disc loss: 0.0011135396114865647, policy loss: 8.44745829025662
Experience 17, Iter 7, disc loss: 0.0012329787718273972, policy loss: 8.421766946730472
Experience 17, Iter 8, disc loss: 0.0010734841546373353, policy loss: 8.301666787756865
Experience 17, Iter 9, disc loss: 0.0010586847321430522, policy loss: 8.582161020641873
Experience 17, Iter 10, disc loss: 0.0010326337156939514, policy loss: 9.177405306459699
Experience 17, Iter 11, disc loss: 0.001104143494363643, policy loss: 9.073168107049558
Experience 17, Iter 12, disc loss: 0.001120731784831156, policy loss: 8.579710868824943
Experience 17, Iter 13, disc loss: 0.0009608205442698286, policy loss: 8.845888049160866
Experience 17, Iter 14, disc loss: 0.0009148433762987403, policy loss: 8.83546866260759
Experience 17, Iter 15, disc loss: 0.0009512287064454419, policy loss: 8.915183314921855
Experience 17, Iter 16, disc loss: 0.0010654458711320694, policy loss: 8.937061648280073
Experience 17, Iter 17, disc loss: 0.0009360314338735788, policy loss: 9.03527607384514
Experience 17, Iter 18, disc loss: 0.000968115569091234, policy loss: 9.309798989284598
Experience 17, Iter 19, disc loss: 0.001126613239045849, policy loss: 8.692218065552051
Experience 17, Iter 20, disc loss: 0.0011349432590915943, policy loss: 8.7755631361312
Experience 17, Iter 21, disc loss: 0.0009612738642504599, policy loss: 9.290970584783997
Experience 17, Iter 22, disc loss: 0.0008570104381456308, policy loss: 9.226516922696028
Experience 17, Iter 23, disc loss: 0.000996067427050064, policy loss: 8.720609384984819
Experience 17, Iter 24, disc loss: 0.0010841425984196543, policy loss: 8.829042921656024
Experience 17, Iter 25, disc loss: 0.0009884407079358074, policy loss: 8.702334730922585
Experience 17, Iter 26, disc loss: 0.0009266663653016084, policy loss: 8.967845698724805
Experience 17, Iter 27, disc loss: 0.000976818247944283, policy loss: 8.971971892127488
Experience 17, Iter 28, disc loss: 0.0009685234282641567, policy loss: 9.250592405665193
Experience 17, Iter 29, disc loss: 0.0009073300561141246, policy loss: 8.85133203973007
Experience 17, Iter 30, disc loss: 0.000957852163085633, policy loss: 8.462060985811956
Experience 17, Iter 31, disc loss: 0.0009648523789945306, policy loss: 9.284435194796187
Experience 17, Iter 32, disc loss: 0.0008926312690718723, policy loss: 8.691678250647115
Experience 17, Iter 33, disc loss: 0.0009817697942323426, policy loss: 8.343519862955588
Experience 17, Iter 34, disc loss: 0.0009816875912338876, policy loss: 8.45439938244194
Experience 17, Iter 35, disc loss: 0.0011270362688276898, policy loss: 9.0563552910607
Experience 17, Iter 36, disc loss: 0.001470898749200286, policy loss: 7.9598100084945544
Experience 17, Iter 37, disc loss: 0.0008152215989861464, policy loss: 9.317069669417885
Experience 17, Iter 38, disc loss: 0.0009775665885237116, policy loss: 9.160697953383202
Experience 17, Iter 39, disc loss: 0.001170175587322848, policy loss: 8.733287579066342
Experience 17, Iter 40, disc loss: 0.001031320140768839, policy loss: 8.972576248091226
Experience 17, Iter 41, disc loss: 0.0009850041847406631, policy loss: 8.92209688363697
Experience 17, Iter 42, disc loss: 0.0009900927133562865, policy loss: 8.845111990254091
Experience 17, Iter 43, disc loss: 0.0010882989340196575, policy loss: 8.687402086704513
Experience 17, Iter 44, disc loss: 0.0009623699205954933, policy loss: 9.007637597612938
Experience 17, Iter 45, disc loss: 0.0012496194708927827, policy loss: 8.492093673757818
Experience 17, Iter 46, disc loss: 0.0009599859867080905, policy loss: 8.74154068985939
Experience 17, Iter 47, disc loss: 0.0010192247094872827, policy loss: 9.561812881876685
Experience 17, Iter 48, disc loss: 0.0008654971467373757, policy loss: 9.329050485544359
Experience 17, Iter 49, disc loss: 0.0009265324045818561, policy loss: 8.704368286025893
Experience 17, Iter 50, disc loss: 0.0011845025498614446, policy loss: 8.805855779902915
Experience 17, Iter 51, disc loss: 0.001030805515017674, policy loss: 9.263777479752441
Experience 17, Iter 52, disc loss: 0.0010883596101760938, policy loss: 8.562236712276329
Experience 17, Iter 53, disc loss: 0.000913876915788531, policy loss: 9.521974984244146
Experience 17, Iter 54, disc loss: 0.0009180389641640849, policy loss: 8.55930169362038
Experience 17, Iter 55, disc loss: 0.0009638686776069984, policy loss: 8.755632553440766
Experience 17, Iter 56, disc loss: 0.0008810753054894151, policy loss: 9.264863414316274
Experience 17, Iter 57, disc loss: 0.0008941188552069783, policy loss: 9.305939766816158
Experience 17, Iter 58, disc loss: 0.0009536560734419937, policy loss: 9.364666499307651
Experience 17, Iter 59, disc loss: 0.0009972796592392218, policy loss: 8.34393965285274
Experience 17, Iter 60, disc loss: 0.0009947405656491612, policy loss: 9.31454254094168
Experience 17, Iter 61, disc loss: 0.0010111708317235113, policy loss: 9.201505327013155
Experience 17, Iter 62, disc loss: 0.0008768396514923945, policy loss: 8.74344762598038
Experience 17, Iter 63, disc loss: 0.0009005705536499415, policy loss: 8.481954049344399
Experience 17, Iter 64, disc loss: 0.0008569169728114182, policy loss: 9.156994934459911
Experience 17, Iter 65, disc loss: 0.0009655818626988887, policy loss: 8.677392287252141
Experience 17, Iter 66, disc loss: 0.0010519206796012189, policy loss: 8.071509997098978
Experience 17, Iter 67, disc loss: 0.0008960988142445406, policy loss: 8.784345388512167
Experience 17, Iter 68, disc loss: 0.0011385109601034587, policy loss: 8.59999411393909
Experience 17, Iter 69, disc loss: 0.0010142209105498047, policy loss: 8.609450743637495
Experience 17, Iter 70, disc loss: 0.0010360919982869486, policy loss: 8.450564294379792
Experience 17, Iter 71, disc loss: 0.0009579207212930738, policy loss: 10.016498291225748
Experience 17, Iter 72, disc loss: 0.0010600039935879323, policy loss: 8.505407168715344
Experience 17, Iter 73, disc loss: 0.0009266228039001448, policy loss: 8.742276809710852
Experience 17, Iter 74, disc loss: 0.0009423585015389045, policy loss: 8.8100875774682
Experience 17, Iter 75, disc loss: 0.0009874603102291704, policy loss: 9.15617094515935
Experience 17, Iter 76, disc loss: 0.0009278635748197653, policy loss: 9.282054994527563
Experience 17, Iter 77, disc loss: 0.0014965668447203863, policy loss: 8.11909922842004
Experience 17, Iter 78, disc loss: 0.0009544076976788846, policy loss: 8.378371914683827
Experience 17, Iter 79, disc loss: 0.0009333268118709091, policy loss: 8.93762815934938
Experience 17, Iter 80, disc loss: 0.0009383338219830087, policy loss: 9.04713877748039
Experience 17, Iter 81, disc loss: 0.00107125949220867, policy loss: 8.712714697555304
Experience 17, Iter 82, disc loss: 0.0010297316429203228, policy loss: 8.830099128069854
Experience 17, Iter 83, disc loss: 0.000831022018459776, policy loss: 9.425106155741085
Experience 17, Iter 84, disc loss: 0.00088652894197745, policy loss: 9.112410614535307
Experience 17, Iter 85, disc loss: 0.0012984187962243095, policy loss: 8.24703278060575
Experience 17, Iter 86, disc loss: 0.0010210277547284138, policy loss: 8.973516774206315
Experience 17, Iter 87, disc loss: 0.0009658688463823594, policy loss: 8.558343843293379
Experience 17, Iter 88, disc loss: 0.0009011958426354557, policy loss: 8.780425146698896
Experience 17, Iter 89, disc loss: 0.0008270611824951078, policy loss: 8.893989274626017
Experience 17, Iter 90, disc loss: 0.0008414390038288326, policy loss: 9.404017339799672
Experience 17, Iter 91, disc loss: 0.0008658323119457766, policy loss: 9.530259574930938
Experience 17, Iter 92, disc loss: 0.0008290153798129073, policy loss: 8.938632167825466
Experience 17, Iter 93, disc loss: 0.0009525361887755012, policy loss: 8.60863204615238
Experience 17, Iter 94, disc loss: 0.0007967612932095079, policy loss: 8.730580186049977
Experience 17, Iter 95, disc loss: 0.0007604984269597148, policy loss: 9.779377987717526
Experience 17, Iter 96, disc loss: 0.001056418377225218, policy loss: 8.714977678074405
Experience 17, Iter 97, disc loss: 0.0008747705861092529, policy loss: 8.498466500086055
Experience 17, Iter 98, disc loss: 0.0008414162229226018, policy loss: 8.770945603964469
Experience 17, Iter 99, disc loss: 0.000921421326517394, policy loss: 8.792587527519839
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1584],
        [1.2183],
        [0.0254]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0218, 0.1972, 1.1685, 0.0202, 0.0214, 3.9296]],

        [[0.0218, 0.1972, 1.1685, 0.0202, 0.0214, 3.9296]],

        [[0.0218, 0.1972, 1.1685, 0.0202, 0.0214, 3.9296]],

        [[0.0218, 0.1972, 1.1685, 0.0202, 0.0214, 3.9296]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0196, 0.6337, 4.8733, 0.1017], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0196, 0.6337, 4.8733, 0.1017])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.082
Iter 2/2000 - Loss: 2.948
Iter 3/2000 - Loss: 2.820
Iter 4/2000 - Loss: 2.732
Iter 5/2000 - Loss: 2.670
Iter 6/2000 - Loss: 2.538
Iter 7/2000 - Loss: 2.370
Iter 8/2000 - Loss: 2.211
Iter 9/2000 - Loss: 2.047
Iter 10/2000 - Loss: 1.855
Iter 11/2000 - Loss: 1.630
Iter 12/2000 - Loss: 1.385
Iter 13/2000 - Loss: 1.127
Iter 14/2000 - Loss: 0.858
Iter 15/2000 - Loss: 0.580
Iter 16/2000 - Loss: 0.293
Iter 17/2000 - Loss: 0.000
Iter 18/2000 - Loss: -0.295
Iter 19/2000 - Loss: -0.589
Iter 20/2000 - Loss: -0.882
Iter 1981/2000 - Loss: -7.930
Iter 1982/2000 - Loss: -7.930
Iter 1983/2000 - Loss: -7.930
Iter 1984/2000 - Loss: -7.930
Iter 1985/2000 - Loss: -7.930
Iter 1986/2000 - Loss: -7.930
Iter 1987/2000 - Loss: -7.930
Iter 1988/2000 - Loss: -7.930
Iter 1989/2000 - Loss: -7.930
Iter 1990/2000 - Loss: -7.930
Iter 1991/2000 - Loss: -7.930
Iter 1992/2000 - Loss: -7.930
Iter 1993/2000 - Loss: -7.930
Iter 1994/2000 - Loss: -7.930
Iter 1995/2000 - Loss: -7.930
Iter 1996/2000 - Loss: -7.930
Iter 1997/2000 - Loss: -7.930
Iter 1998/2000 - Loss: -7.930
Iter 1999/2000 - Loss: -7.930
Iter 2000/2000 - Loss: -7.930
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[ 6.4745,  4.2224, 37.8373, 11.9201, 16.0637, 29.2413]],

        [[15.6841, 31.5793, 10.6519,  1.1945,  5.0335, 30.1111]],

        [[17.9476, 29.1399,  9.1023,  0.9919,  0.9387, 24.9997]],

        [[15.5263, 29.3949, 15.0814,  1.5527,  2.1898, 40.5335]]])
Signal Variance: tensor([ 0.1041,  2.4760, 13.1606,  0.4470])
Estimated target variance: tensor([0.0196, 0.6337, 4.8733, 0.1017])
N: 180
Signal to noise ratio: tensor([17.3236, 86.1241, 84.2791, 40.1929])
Bound on condition number: tensor([  54020.2312, 1335126.9906, 1278535.2977,  290785.2560])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0010470490925528892, policy loss: 9.069509064348836
Experience 18, Iter 1, disc loss: 0.0008571286706629029, policy loss: 8.417657573896166
Experience 18, Iter 2, disc loss: 0.0009637885354490487, policy loss: 8.71199941524412
Experience 18, Iter 3, disc loss: 0.0008512223944901616, policy loss: 8.652292107172947
Experience 18, Iter 4, disc loss: 0.0011754824316482894, policy loss: 8.283206465827632
Experience 18, Iter 5, disc loss: 0.0008219682069953674, policy loss: 10.01863264673263
Experience 18, Iter 6, disc loss: 0.0008585978123935327, policy loss: 8.621615460780818
Experience 18, Iter 7, disc loss: 0.0009831999709090647, policy loss: 9.742334738885116
Experience 18, Iter 8, disc loss: 0.0007635864189266735, policy loss: 8.9310948960959
Experience 18, Iter 9, disc loss: 0.000803855937731295, policy loss: 9.761651215515787
Experience 18, Iter 10, disc loss: 0.0006249713609748649, policy loss: 9.623841188982212
Experience 18, Iter 11, disc loss: 0.0006278637474000765, policy loss: 10.061799183449914
Experience 18, Iter 12, disc loss: 0.000609470125596305, policy loss: 9.829728635295242
Experience 18, Iter 13, disc loss: 0.0005487198768699963, policy loss: 10.870436313673673
Experience 18, Iter 14, disc loss: 0.0005754298765764115, policy loss: 10.617500384934509
Experience 18, Iter 15, disc loss: 0.0005661856330469502, policy loss: 10.550696095204142
Experience 18, Iter 16, disc loss: 0.00061846831745772, policy loss: 9.932456168663556
Experience 18, Iter 17, disc loss: 0.0006782712266026434, policy loss: 9.232888408009202
Experience 18, Iter 18, disc loss: 0.0010393896532276022, policy loss: 8.686436901704655
Experience 18, Iter 19, disc loss: 0.0006045500381273568, policy loss: 9.427659192294907
Experience 18, Iter 20, disc loss: 0.0005741053931837021, policy loss: 10.142389468546675
Experience 18, Iter 21, disc loss: 0.0006935745488113665, policy loss: 9.165753855985397
Experience 18, Iter 22, disc loss: 0.0006643615463476036, policy loss: 9.135603285095055
Experience 18, Iter 23, disc loss: 0.0007011860797543998, policy loss: 9.06761055215333
Experience 18, Iter 24, disc loss: 0.0006136085706843146, policy loss: 9.549179138493752
Experience 18, Iter 25, disc loss: 0.000665640406512665, policy loss: 9.435021411149858
Experience 18, Iter 26, disc loss: 0.0006088422948303611, policy loss: 9.474540547128719
Experience 18, Iter 27, disc loss: 0.0007065251631241339, policy loss: 9.183463536634523
Experience 18, Iter 28, disc loss: 0.0006175786884777878, policy loss: 9.846593810307429
Experience 18, Iter 29, disc loss: 0.0009578395053740575, policy loss: 8.199881176975484
Experience 18, Iter 30, disc loss: 0.0008350443446386947, policy loss: 9.142433373016631
Experience 18, Iter 31, disc loss: 0.0007976671121931918, policy loss: 9.200944508731617
Experience 18, Iter 32, disc loss: 0.0007413804997049696, policy loss: 8.666019601511753
Experience 18, Iter 33, disc loss: 0.000818181588778031, policy loss: 8.738781943196344
Experience 18, Iter 34, disc loss: 0.00101176034547203, policy loss: 8.62274996239347
Experience 18, Iter 35, disc loss: 0.0008427480250941637, policy loss: 8.753263613937742
Experience 18, Iter 36, disc loss: 0.0006934788514446597, policy loss: 8.700314328080939
Experience 18, Iter 37, disc loss: 0.0006660188474083726, policy loss: 9.155919703635629
Experience 18, Iter 38, disc loss: 0.0007914514062012708, policy loss: 9.543587217780885
Experience 18, Iter 39, disc loss: 0.0008106433289274713, policy loss: 9.092759902894514
Experience 18, Iter 40, disc loss: 0.0007554830374495614, policy loss: 9.468522671588417
Experience 18, Iter 41, disc loss: 0.000966588558610728, policy loss: 8.63534133787542
Experience 18, Iter 42, disc loss: 0.0010718009367128437, policy loss: 8.315987420727682
Experience 18, Iter 43, disc loss: 0.0007886516454694345, policy loss: 8.858346858713702
Experience 18, Iter 44, disc loss: 0.0007765415162961985, policy loss: 9.012393063641717
Experience 18, Iter 45, disc loss: 0.0007641344003023421, policy loss: 9.14852200221058
Experience 18, Iter 46, disc loss: 0.0006632370822358718, policy loss: 10.201567965591586
Experience 18, Iter 47, disc loss: 0.0009930604687561997, policy loss: 8.831991227548034
Experience 18, Iter 48, disc loss: 0.00071925746819637, policy loss: 9.906864181191848
Experience 18, Iter 49, disc loss: 0.0009839643652343475, policy loss: 9.23972886313367
Experience 18, Iter 50, disc loss: 0.0009597176667170632, policy loss: 9.279780715353391
Experience 18, Iter 51, disc loss: 0.0011707571414448933, policy loss: 8.659229661982911
Experience 18, Iter 52, disc loss: 0.0008175810564059359, policy loss: 9.143886815997561
Experience 18, Iter 53, disc loss: 0.0009442876033113902, policy loss: 8.78809267972589
Experience 18, Iter 54, disc loss: 0.0006733647797068842, policy loss: 9.754083217245086
Experience 18, Iter 55, disc loss: 0.000726027729011094, policy loss: 9.130919696512326
Experience 18, Iter 56, disc loss: 0.0008448576615621185, policy loss: 8.621027037768343
Experience 18, Iter 57, disc loss: 0.0007469024301558617, policy loss: 9.101746212952248
Experience 18, Iter 58, disc loss: 0.0006675184125960224, policy loss: 9.096163753161107
Experience 18, Iter 59, disc loss: 0.0007707701433076416, policy loss: 9.442766184833523
Experience 18, Iter 60, disc loss: 0.0008076987221080374, policy loss: 9.47193458835591
Experience 18, Iter 61, disc loss: 0.0007133628504692137, policy loss: 9.191715326442257
Experience 18, Iter 62, disc loss: 0.0008722314781321127, policy loss: 9.569242054193204
Experience 18, Iter 63, disc loss: 0.0007750181296907303, policy loss: 9.487912580144265
Experience 18, Iter 64, disc loss: 0.0010236870127940075, policy loss: 9.276054482295276
Experience 18, Iter 65, disc loss: 0.0011397787556498395, policy loss: 8.517979882556226
Experience 18, Iter 66, disc loss: 0.0007930687964505768, policy loss: 9.075899241753735
Experience 18, Iter 67, disc loss: 0.0008994013151633131, policy loss: 8.406947853208022
Experience 18, Iter 68, disc loss: 0.0009274738425928645, policy loss: 9.258759339002957
Experience 18, Iter 69, disc loss: 0.0008134454383252527, policy loss: 8.769619171714716
Experience 18, Iter 70, disc loss: 0.0008698452188082996, policy loss: 8.81859560018966
Experience 18, Iter 71, disc loss: 0.0008530885946341363, policy loss: 8.974989569325865
Experience 18, Iter 72, disc loss: 0.0008480857436781839, policy loss: 9.310831049007904
Experience 18, Iter 73, disc loss: 0.0008961090176849917, policy loss: 8.828391563385809
Experience 18, Iter 74, disc loss: 0.0007300592485960665, policy loss: 9.002774526488913
Experience 18, Iter 75, disc loss: 0.0008203950462269006, policy loss: 8.618004790590788
Experience 18, Iter 76, disc loss: 0.0007495419784171085, policy loss: 9.78438462210459
Experience 18, Iter 77, disc loss: 0.0009464062275703279, policy loss: 8.625097099791917
Experience 18, Iter 78, disc loss: 0.0007380871372987311, policy loss: 9.199086165521553
Experience 18, Iter 79, disc loss: 0.0007505251408836109, policy loss: 8.982145034532525
Experience 18, Iter 80, disc loss: 0.0007795107967541392, policy loss: 9.266807168441556
Experience 18, Iter 81, disc loss: 0.0007341730118459833, policy loss: 9.81017577593022
Experience 18, Iter 82, disc loss: 0.0007263917664659887, policy loss: 9.017094498700004
Experience 18, Iter 83, disc loss: 0.0008539046792094214, policy loss: 9.073582819586646
Experience 18, Iter 84, disc loss: 0.0007908725905180275, policy loss: 8.848611571962136
Experience 18, Iter 85, disc loss: 0.0009595022682570222, policy loss: 8.738459239479628
Experience 18, Iter 86, disc loss: 0.0007800164938869018, policy loss: 9.660662005957022
Experience 18, Iter 87, disc loss: 0.0008814433650111545, policy loss: 8.68128050542614
Experience 18, Iter 88, disc loss: 0.0006993319137939828, policy loss: 9.538702081626312
Experience 18, Iter 89, disc loss: 0.000847716086574454, policy loss: 8.797207263079486
Experience 18, Iter 90, disc loss: 0.0007710687296196272, policy loss: 8.654565474569967
Experience 18, Iter 91, disc loss: 0.0007480613623637377, policy loss: 9.58236135717802
Experience 18, Iter 92, disc loss: 0.0008181717814059941, policy loss: 9.565068758267326
Experience 18, Iter 93, disc loss: 0.0008719292537619787, policy loss: 8.99051887045075
Experience 18, Iter 94, disc loss: 0.0007931062154358212, policy loss: 9.415978586399255
Experience 18, Iter 95, disc loss: 0.0008691785432648901, policy loss: 9.650032253763445
Experience 18, Iter 96, disc loss: 0.0008960763176539539, policy loss: 8.737049141463023
Experience 18, Iter 97, disc loss: 0.0008220492901742892, policy loss: 8.832258910532051
Experience 18, Iter 98, disc loss: 0.0007527539974569226, policy loss: 9.253217965376475
Experience 18, Iter 99, disc loss: 0.000719378472639506, policy loss: 9.046396310169325
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1658],
        [1.2624],
        [0.0265]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0209, 0.1965, 1.2172, 0.0210, 0.0227, 4.1208]],

        [[0.0209, 0.1965, 1.2172, 0.0210, 0.0227, 4.1208]],

        [[0.0209, 0.1965, 1.2172, 0.0210, 0.0227, 4.1208]],

        [[0.0209, 0.1965, 1.2172, 0.0210, 0.0227, 4.1208]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0195, 0.6634, 5.0496, 0.1059], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0195, 0.6634, 5.0496, 0.1059])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.134
Iter 2/2000 - Loss: 3.005
Iter 3/2000 - Loss: 2.854
Iter 4/2000 - Loss: 2.764
Iter 5/2000 - Loss: 2.700
Iter 6/2000 - Loss: 2.558
Iter 7/2000 - Loss: 2.381
Iter 8/2000 - Loss: 2.212
Iter 9/2000 - Loss: 2.039
Iter 10/2000 - Loss: 1.839
Iter 11/2000 - Loss: 1.610
Iter 12/2000 - Loss: 1.362
Iter 13/2000 - Loss: 1.100
Iter 14/2000 - Loss: 0.827
Iter 15/2000 - Loss: 0.543
Iter 16/2000 - Loss: 0.251
Iter 17/2000 - Loss: -0.045
Iter 18/2000 - Loss: -0.342
Iter 19/2000 - Loss: -0.636
Iter 20/2000 - Loss: -0.928
Iter 1981/2000 - Loss: -7.987
Iter 1982/2000 - Loss: -7.987
Iter 1983/2000 - Loss: -7.987
Iter 1984/2000 - Loss: -7.987
Iter 1985/2000 - Loss: -7.987
Iter 1986/2000 - Loss: -7.987
Iter 1987/2000 - Loss: -7.987
Iter 1988/2000 - Loss: -7.987
Iter 1989/2000 - Loss: -7.987
Iter 1990/2000 - Loss: -7.987
Iter 1991/2000 - Loss: -7.987
Iter 1992/2000 - Loss: -7.987
Iter 1993/2000 - Loss: -7.987
Iter 1994/2000 - Loss: -7.987
Iter 1995/2000 - Loss: -7.987
Iter 1996/2000 - Loss: -7.987
Iter 1997/2000 - Loss: -7.988
Iter 1998/2000 - Loss: -7.988
Iter 1999/2000 - Loss: -7.988
Iter 2000/2000 - Loss: -7.988
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[ 7.7301,  4.6310, 31.6442,  9.9274, 14.8631, 37.3860]],

        [[14.9087, 29.3822, 10.3252,  1.1930,  5.0531, 28.3570]],

        [[17.7930, 31.9023,  9.1079,  1.0082,  0.9049, 24.1122]],

        [[15.3672, 28.5995, 14.7804,  1.5750,  2.1454, 40.1734]]])
Signal Variance: tensor([ 0.1093,  2.2652, 12.9139,  0.4245])
Estimated target variance: tensor([0.0195, 0.6634, 5.0496, 0.1059])
N: 190
Signal to noise ratio: tensor([17.5390, 82.2233, 83.5722, 39.8739])
Bound on condition number: tensor([  58448.2314, 1284529.9762, 1327019.5010,  302086.7350])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0006684816745175123, policy loss: 10.086405283518783
Experience 19, Iter 1, disc loss: 0.0006881635674882043, policy loss: 9.581402334329404
Experience 19, Iter 2, disc loss: 0.0008254656650740153, policy loss: 8.616646229048042
Experience 19, Iter 3, disc loss: 0.0009025436220400978, policy loss: 8.778009917336917
Experience 19, Iter 4, disc loss: 0.0008864386992879159, policy loss: 9.102067561138554
Experience 19, Iter 5, disc loss: 0.0007214904709393353, policy loss: 9.58744427326854
Experience 19, Iter 6, disc loss: 0.0007963788129707019, policy loss: 9.012177262564482
Experience 19, Iter 7, disc loss: 0.0007254665680077025, policy loss: 9.628229686220148
Experience 19, Iter 8, disc loss: 0.0007493603943967709, policy loss: 8.909899920232654
Experience 19, Iter 9, disc loss: 0.0008205553589727083, policy loss: 8.675167301998991
Experience 19, Iter 10, disc loss: 0.0007472513044704428, policy loss: 9.108544630611117
Experience 19, Iter 11, disc loss: 0.0007508690163112355, policy loss: 9.106669266922061
Experience 19, Iter 12, disc loss: 0.0009271859157142076, policy loss: 9.322211510679047
Experience 19, Iter 13, disc loss: 0.0007862463310780562, policy loss: 9.375452822016921
Experience 19, Iter 14, disc loss: 0.0006871517631338134, policy loss: 9.545125567373583
Experience 19, Iter 15, disc loss: 0.0007043383997640965, policy loss: 8.846305250258474
Experience 19, Iter 16, disc loss: 0.0007495021083589536, policy loss: 9.060484732647875
Experience 19, Iter 17, disc loss: 0.0007871436980389336, policy loss: 8.58689301240864
Experience 19, Iter 18, disc loss: 0.0007480357703259134, policy loss: 9.547382400878405
Experience 19, Iter 19, disc loss: 0.000868956258359748, policy loss: 9.414157111168805
Experience 19, Iter 20, disc loss: 0.0007914274269325907, policy loss: 9.166573198940764
Experience 19, Iter 21, disc loss: 0.0007458728067017274, policy loss: 9.655297842799333
Experience 19, Iter 22, disc loss: 0.0008678620451751277, policy loss: 9.340671269436537
Experience 19, Iter 23, disc loss: 0.0006702963386882083, policy loss: 9.696423868621872
Experience 19, Iter 24, disc loss: 0.0007926295846133143, policy loss: 9.00154900149816
Experience 19, Iter 25, disc loss: 0.0006608002725882021, policy loss: 9.266064651311172
Experience 19, Iter 26, disc loss: 0.0008272579175070821, policy loss: 9.297236909733176
Experience 19, Iter 27, disc loss: 0.0008021331868562296, policy loss: 9.284931693847536
Experience 19, Iter 28, disc loss: 0.0006796597420834551, policy loss: 10.113676519861954
Experience 19, Iter 29, disc loss: 0.0007567544803435615, policy loss: 8.740100036230642
Experience 19, Iter 30, disc loss: 0.0006977962144763646, policy loss: 9.352944751302076
Experience 19, Iter 31, disc loss: 0.0008029578688117313, policy loss: 8.803726464667424
Experience 19, Iter 32, disc loss: 0.000636053886813586, policy loss: 9.542019798488608
Experience 19, Iter 33, disc loss: 0.0006921178184375501, policy loss: 9.627418618316051
Experience 19, Iter 34, disc loss: 0.000778268853801617, policy loss: 9.227002540429432
Experience 19, Iter 35, disc loss: 0.000791310973012292, policy loss: 9.203898834313332
Experience 19, Iter 36, disc loss: 0.0007364934623943635, policy loss: 9.318545642878687
Experience 19, Iter 37, disc loss: 0.0006803667119275487, policy loss: 9.737045575085164
Experience 19, Iter 38, disc loss: 0.0007133064999185998, policy loss: 9.470206414093767
Experience 19, Iter 39, disc loss: 0.0007858337220371653, policy loss: 8.963193837799889
Experience 19, Iter 40, disc loss: 0.00077914508517907, policy loss: 9.009454004904342
Experience 19, Iter 41, disc loss: 0.000709974300294379, policy loss: 8.914678750434298
Experience 19, Iter 42, disc loss: 0.0007037545955422439, policy loss: 9.024871213707588
Experience 19, Iter 43, disc loss: 0.0008286589059715449, policy loss: 9.180581664913614
Experience 19, Iter 44, disc loss: 0.0007646310705441129, policy loss: 9.073941116372003
Experience 19, Iter 45, disc loss: 0.0008019664991962855, policy loss: 8.789794174089817
Experience 19, Iter 46, disc loss: 0.000770609633097206, policy loss: 9.10814162952475
Experience 19, Iter 47, disc loss: 0.0007470828792079747, policy loss: 9.18570840432556
Experience 19, Iter 48, disc loss: 0.0008604823429721843, policy loss: 9.481915701248663
Experience 19, Iter 49, disc loss: 0.0009484918078903774, policy loss: 9.302559564245687
Experience 19, Iter 50, disc loss: 0.0008884438600368762, policy loss: 8.713943755180665
Experience 19, Iter 51, disc loss: 0.000713875304679144, policy loss: 8.9488647093502
Experience 19, Iter 52, disc loss: 0.000812387393101035, policy loss: 9.145794521592002
Experience 19, Iter 53, disc loss: 0.0006486536147720596, policy loss: 9.827852209563503
Experience 19, Iter 54, disc loss: 0.0006613924824923942, policy loss: 9.076152350630872
Experience 19, Iter 55, disc loss: 0.0008857985516912297, policy loss: 9.620950065432796
Experience 19, Iter 56, disc loss: 0.0006137594827297141, policy loss: 9.212886088663195
Experience 19, Iter 57, disc loss: 0.0008667290048509239, policy loss: 9.146739024695519
Experience 19, Iter 58, disc loss: 0.0006436823960149823, policy loss: 8.983985325648913
Experience 19, Iter 59, disc loss: 0.0008602528884575197, policy loss: 9.430411581653551
Experience 19, Iter 60, disc loss: 0.0007613132844806822, policy loss: 9.18351806332951
Experience 19, Iter 61, disc loss: 0.0007000274622023578, policy loss: 10.246293275785526
Experience 19, Iter 62, disc loss: 0.0006714479879480655, policy loss: 9.451778120893067
Experience 19, Iter 63, disc loss: 0.000743028000136661, policy loss: 9.262884731717367
Experience 19, Iter 64, disc loss: 0.0006997501065684949, policy loss: 9.31024723122901
Experience 19, Iter 65, disc loss: 0.0005798180880724217, policy loss: 9.569615743266485
Experience 19, Iter 66, disc loss: 0.0006491037381766332, policy loss: 9.295347438438242
Experience 19, Iter 67, disc loss: 0.0007316609410231793, policy loss: 8.905813622306445
Experience 19, Iter 68, disc loss: 0.0005517885640479545, policy loss: 9.73628252516928
Experience 19, Iter 69, disc loss: 0.0006246541075599451, policy loss: 9.116934963367566
Experience 19, Iter 70, disc loss: 0.0006887775675501028, policy loss: 9.28486214023427
Experience 19, Iter 71, disc loss: 0.0006368571650617672, policy loss: 9.040734641851888
Experience 19, Iter 72, disc loss: 0.0006731980600579413, policy loss: 8.966511525038722
Experience 19, Iter 73, disc loss: 0.0006934441516027516, policy loss: 9.27220082601453
Experience 19, Iter 74, disc loss: 0.0007251394312337732, policy loss: 9.042084010883801
Experience 19, Iter 75, disc loss: 0.0005931274768869387, policy loss: 9.843730818247959
Experience 19, Iter 76, disc loss: 0.0007964274511201694, policy loss: 8.64780989382745
Experience 19, Iter 77, disc loss: 0.0006146034829449095, policy loss: 9.902933046430796
Experience 19, Iter 78, disc loss: 0.0006902157345095212, policy loss: 9.244150188726612
Experience 19, Iter 79, disc loss: 0.0007211762098071109, policy loss: 9.037027465581891
Experience 19, Iter 80, disc loss: 0.0007644527541031137, policy loss: 9.457786154607152
Experience 19, Iter 81, disc loss: 0.000819874420883726, policy loss: 8.47565266704322
Experience 19, Iter 82, disc loss: 0.0010348462638455544, policy loss: 9.54417605167075
Experience 19, Iter 83, disc loss: 0.0007662430363471095, policy loss: 9.419107851639282
Experience 19, Iter 84, disc loss: 0.0006357365065157031, policy loss: 9.05923590732503
Experience 19, Iter 85, disc loss: 0.0006349258647732063, policy loss: 9.627610434612553
Experience 19, Iter 86, disc loss: 0.0008231578046645172, policy loss: 9.133493635041583
Experience 19, Iter 87, disc loss: 0.0007704792534530675, policy loss: 9.33670589648634
Experience 19, Iter 88, disc loss: 0.0007507925680479285, policy loss: 9.32878071788536
Experience 19, Iter 89, disc loss: 0.0007726431416357976, policy loss: 9.328897114932472
Experience 19, Iter 90, disc loss: 0.0006464993563341033, policy loss: 9.093241522803279
Experience 19, Iter 91, disc loss: 0.0008340782763711399, policy loss: 9.2359567529775
Experience 19, Iter 92, disc loss: 0.0006277658076974, policy loss: 9.551146712700014
Experience 19, Iter 93, disc loss: 0.0006983901332553734, policy loss: 8.909023305879723
Experience 19, Iter 94, disc loss: 0.0005987057091734888, policy loss: 9.173508169064887
Experience 19, Iter 95, disc loss: 0.0006309978079095177, policy loss: 9.543108640265395
Experience 19, Iter 96, disc loss: 0.000644922010430719, policy loss: 9.307071712960969
Experience 19, Iter 97, disc loss: 0.0006312234846984128, policy loss: 9.623896609031743
Experience 19, Iter 98, disc loss: 0.0006639244061669783, policy loss: 8.960768600498376
Experience 19, Iter 99, disc loss: 0.0006189353927697496, policy loss: 9.460243124075065
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.1738],
        [1.3152],
        [0.0274]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0202, 0.1955, 1.2650, 0.0216, 0.0237, 4.3074]],

        [[0.0202, 0.1955, 1.2650, 0.0216, 0.0237, 4.3074]],

        [[0.0202, 0.1955, 1.2650, 0.0216, 0.0237, 4.3074]],

        [[0.0202, 0.1955, 1.2650, 0.0216, 0.0237, 4.3074]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0194, 0.6950, 5.2610, 0.1097], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0194, 0.6950, 5.2610, 0.1097])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.118
Iter 2/2000 - Loss: 2.979
Iter 3/2000 - Loss: 2.805
Iter 4/2000 - Loss: 2.699
Iter 5/2000 - Loss: 2.618
Iter 6/2000 - Loss: 2.462
Iter 7/2000 - Loss: 2.273
Iter 8/2000 - Loss: 2.091
Iter 9/2000 - Loss: 1.908
Iter 10/2000 - Loss: 1.700
Iter 11/2000 - Loss: 1.465
Iter 12/2000 - Loss: 1.212
Iter 13/2000 - Loss: 0.948
Iter 14/2000 - Loss: 0.674
Iter 15/2000 - Loss: 0.391
Iter 16/2000 - Loss: 0.102
Iter 17/2000 - Loss: -0.190
Iter 18/2000 - Loss: -0.481
Iter 19/2000 - Loss: -0.768
Iter 20/2000 - Loss: -1.052
Iter 1981/2000 - Loss: -8.045
Iter 1982/2000 - Loss: -8.045
Iter 1983/2000 - Loss: -8.045
Iter 1984/2000 - Loss: -8.045
Iter 1985/2000 - Loss: -8.045
Iter 1986/2000 - Loss: -8.045
Iter 1987/2000 - Loss: -8.045
Iter 1988/2000 - Loss: -8.045
Iter 1989/2000 - Loss: -8.045
Iter 1990/2000 - Loss: -8.045
Iter 1991/2000 - Loss: -8.045
Iter 1992/2000 - Loss: -8.045
Iter 1993/2000 - Loss: -8.045
Iter 1994/2000 - Loss: -8.045
Iter 1995/2000 - Loss: -8.045
Iter 1996/2000 - Loss: -8.046
Iter 1997/2000 - Loss: -8.046
Iter 1998/2000 - Loss: -8.046
Iter 1999/2000 - Loss: -8.046
Iter 2000/2000 - Loss: -8.046
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[ 7.3586,  5.3873, 29.8044,  5.0189, 11.0154, 46.6831]],

        [[14.6215, 28.5374, 10.2460,  1.1992,  5.0340, 28.3958]],

        [[18.1112, 32.0887,  9.0294,  1.0036,  0.8935, 23.9277]],

        [[15.1577, 28.2846, 14.7162,  1.4611,  2.2368, 40.4431]]])
Signal Variance: tensor([ 0.1213,  2.2619, 12.8300,  0.4122])
Estimated target variance: tensor([0.0194, 0.6950, 5.2610, 0.1097])
N: 200
Signal to noise ratio: tensor([17.9015, 83.0957, 84.3259, 39.7338])
Bound on condition number: tensor([  64093.9872, 1380978.9812, 1422170.8873,  315755.9209])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0007831679196858836, policy loss: 9.416561115708042
Experience 20, Iter 1, disc loss: 0.0006367076310534393, policy loss: 9.320296752952476
Experience 20, Iter 2, disc loss: 0.0005945159093943859, policy loss: 9.985668676588185
Experience 20, Iter 3, disc loss: 0.0007248984755500684, policy loss: 9.359507482912436
Experience 20, Iter 4, disc loss: 0.0006499812266037969, policy loss: 9.613162321037755
Experience 20, Iter 5, disc loss: 0.0007628973765174123, policy loss: 8.900519015522503
Experience 20, Iter 6, disc loss: 0.0005944014983547861, policy loss: 9.319747578548224
Experience 20, Iter 7, disc loss: 0.0005830083715067045, policy loss: 9.458187693623056
Experience 20, Iter 8, disc loss: 0.0007633827211767567, policy loss: 9.118693660224704
Experience 20, Iter 9, disc loss: 0.0008124131947327478, policy loss: 8.630968899027673
Experience 20, Iter 10, disc loss: 0.0006690657991328641, policy loss: 9.388698704321646
Experience 20, Iter 11, disc loss: 0.0006725526759653983, policy loss: 9.604450996712774
Experience 20, Iter 12, disc loss: 0.000755361217163911, policy loss: 10.039492856937613
Experience 20, Iter 13, disc loss: 0.0006317314919750156, policy loss: 9.50791969986588
Experience 20, Iter 14, disc loss: 0.0005521744967801476, policy loss: 9.50953746636251
Experience 20, Iter 15, disc loss: 0.0006342446136049704, policy loss: 9.324486133893206
Experience 20, Iter 16, disc loss: 0.0005729080733468189, policy loss: 10.004437802724544
Experience 20, Iter 17, disc loss: 0.0005517423895064657, policy loss: 9.935919946020775
Experience 20, Iter 18, disc loss: 0.0005765020805125775, policy loss: 9.687932960995699
Experience 20, Iter 19, disc loss: 0.0006070706207371645, policy loss: 9.61143750719578
Experience 20, Iter 20, disc loss: 0.0006873042037228393, policy loss: 10.076197010023009
Experience 20, Iter 21, disc loss: 0.0005063022584794348, policy loss: 10.63670400911614
Experience 20, Iter 22, disc loss: 0.0005029438154392095, policy loss: 10.354369375531915
Experience 20, Iter 23, disc loss: 0.0004819252094859685, policy loss: 10.133462666147206
Experience 20, Iter 24, disc loss: 0.0004149134859528319, policy loss: 10.857911606301805
Experience 20, Iter 25, disc loss: 0.0004362580564192407, policy loss: 10.471820452780577
Experience 20, Iter 26, disc loss: 0.000407314643258658, policy loss: 10.719653934520148
Experience 20, Iter 27, disc loss: 0.0004032223448879711, policy loss: 10.798625532320987
Experience 20, Iter 28, disc loss: 0.0004180927238012511, policy loss: 10.605588310131917
Experience 20, Iter 29, disc loss: 0.0003884364739493403, policy loss: 11.764749061253179
Experience 20, Iter 30, disc loss: 0.0004262250203673438, policy loss: 10.309264170779258
Experience 20, Iter 31, disc loss: 0.0004447836820038355, policy loss: 9.606220309868672
Experience 20, Iter 32, disc loss: 0.00047025125658787457, policy loss: 10.133900472549758
Experience 20, Iter 33, disc loss: 0.0004628185228527337, policy loss: 9.303047056996457
Experience 20, Iter 34, disc loss: 0.00048486039870363677, policy loss: 10.489235420223444
Experience 20, Iter 35, disc loss: 0.00046639765129274284, policy loss: 9.75197537828232
Experience 20, Iter 36, disc loss: 0.000471816873306126, policy loss: 9.825716536425215
Experience 20, Iter 37, disc loss: 0.0004895331671594871, policy loss: 9.240914919982675
Experience 20, Iter 38, disc loss: 0.0005560910307960254, policy loss: 9.472213010451352
Experience 20, Iter 39, disc loss: 0.000541893678175938, policy loss: 9.417351425595806
Experience 20, Iter 40, disc loss: 0.0005789055200441546, policy loss: 8.916481604902252
Experience 20, Iter 41, disc loss: 0.0005969905599137734, policy loss: 9.537212470952072
Experience 20, Iter 42, disc loss: 0.0006699599087352037, policy loss: 8.751440171165587
Experience 20, Iter 43, disc loss: 0.0005567853906216666, policy loss: 8.964096717685905
Experience 20, Iter 44, disc loss: 0.0006298850951566343, policy loss: 9.090100214400593
Experience 20, Iter 45, disc loss: 0.0004858921107192208, policy loss: 9.28742837035822
Experience 20, Iter 46, disc loss: 0.0006234326103306644, policy loss: 9.835673221506095
Experience 20, Iter 47, disc loss: 0.0005125958692752065, policy loss: 9.872542572101253
Experience 20, Iter 48, disc loss: 0.0004953527115320113, policy loss: 9.054068769855158
Experience 20, Iter 49, disc loss: 0.0006696934507476298, policy loss: 8.959295314650795
Experience 20, Iter 50, disc loss: 0.0005962511676664373, policy loss: 9.301382230509027
Experience 20, Iter 51, disc loss: 0.0006549229386868902, policy loss: 9.062659512600796
Experience 20, Iter 52, disc loss: 0.0005525133834698271, policy loss: 9.166952945263802
Experience 20, Iter 53, disc loss: 0.0005430283651437092, policy loss: 9.041104623214625
Experience 20, Iter 54, disc loss: 0.0005964783066973198, policy loss: 9.17236752014619
Experience 20, Iter 55, disc loss: 0.0005923452872310844, policy loss: 9.142068920597044
Experience 20, Iter 56, disc loss: 0.000741612697422624, policy loss: 8.911375169742344
Experience 20, Iter 57, disc loss: 0.0006269699456135076, policy loss: 9.24697347192826
Experience 20, Iter 58, disc loss: 0.0006077811957071783, policy loss: 8.98734554131982
Experience 20, Iter 59, disc loss: 0.00048468568401850877, policy loss: 9.251541307890506
Experience 20, Iter 60, disc loss: 0.000594949002232891, policy loss: 9.284179322474788
Experience 20, Iter 61, disc loss: 0.000526197129323316, policy loss: 9.526101784848667
Experience 20, Iter 62, disc loss: 0.0006103604702041507, policy loss: 8.869111209059207
Experience 20, Iter 63, disc loss: 0.0005733515460712548, policy loss: 9.735372158426577
Experience 20, Iter 64, disc loss: 0.0008670196986516959, policy loss: 9.27473831986023
Experience 20, Iter 65, disc loss: 0.0005856378221629852, policy loss: 9.886837714454419
Experience 20, Iter 66, disc loss: 0.0006101737380154783, policy loss: 9.71245148367203
Experience 20, Iter 67, disc loss: 0.0005375695297282344, policy loss: 9.183247024101934
Experience 20, Iter 68, disc loss: 0.0007196076714069493, policy loss: 9.067074647357181
Experience 20, Iter 69, disc loss: 0.0007036954323004453, policy loss: 9.163959985122435
Experience 20, Iter 70, disc loss: 0.0007627125283260962, policy loss: 8.926597855135308
Experience 20, Iter 71, disc loss: 0.000630832569186655, policy loss: 9.238065561571911
Experience 20, Iter 72, disc loss: 0.000615076800318838, policy loss: 9.214321770793086
Experience 20, Iter 73, disc loss: 0.0006620129251535508, policy loss: 9.28782086005234
Experience 20, Iter 74, disc loss: 0.0006551885110805669, policy loss: 9.567326703261479
Experience 20, Iter 75, disc loss: 0.0005262983136229154, policy loss: 9.44962819928394
Experience 20, Iter 76, disc loss: 0.0007158448999783377, policy loss: 9.451333382390832
Experience 20, Iter 77, disc loss: 0.0005369501593647542, policy loss: 10.031628000791383
Experience 20, Iter 78, disc loss: 0.0006579922127999721, policy loss: 10.059959564128164
Experience 20, Iter 79, disc loss: 0.0005585579513562376, policy loss: 9.938407702541943
Experience 20, Iter 80, disc loss: 0.0005717544973909419, policy loss: 9.389719123660196
Experience 20, Iter 81, disc loss: 0.0005319483143124825, policy loss: 9.326021621129016
Experience 20, Iter 82, disc loss: 0.0005982571004993925, policy loss: 9.61120754552903
Experience 20, Iter 83, disc loss: 0.000598460887464165, policy loss: 9.053290680310495
Experience 20, Iter 84, disc loss: 0.0006422484008216148, policy loss: 9.725954995192634
Experience 20, Iter 85, disc loss: 0.0006790643968814579, policy loss: 9.405878755291639
Experience 20, Iter 86, disc loss: 0.0007399967208903672, policy loss: 9.130215541861936
Experience 20, Iter 87, disc loss: 0.0005545568819982602, policy loss: 9.675837124913379
Experience 20, Iter 88, disc loss: 0.0005613189602076322, policy loss: 10.022982858002793
Experience 20, Iter 89, disc loss: 0.0006574656081362762, policy loss: 9.11670265053125
Experience 20, Iter 90, disc loss: 0.0006912887466816887, policy loss: 9.333238211767178
Experience 20, Iter 91, disc loss: 0.0006001898872113201, policy loss: 9.74427710806815
Experience 20, Iter 92, disc loss: 0.0005837458821357347, policy loss: 9.188990482384877
Experience 20, Iter 93, disc loss: 0.0006270111146701652, policy loss: 9.720867732845118
Experience 20, Iter 94, disc loss: 0.0006446869795576346, policy loss: 9.12089830295734
Experience 20, Iter 95, disc loss: 0.0007968503225749624, policy loss: 9.496378035352183
Experience 20, Iter 96, disc loss: 0.0006049051953248893, policy loss: 9.290592679427599
Experience 20, Iter 97, disc loss: 0.0007243854119027258, policy loss: 9.277080571049655
Experience 20, Iter 98, disc loss: 0.0006248184644400672, policy loss: 9.468576874918465
Experience 20, Iter 99, disc loss: 0.0007896888885423561, policy loss: 9.397030783798552
