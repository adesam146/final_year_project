Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0091],
        [0.2391],
        [0.0035]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0048, 0.0363, 0.9563, 0.0139], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0048, 0.0363, 0.9563, 0.0139])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.905
Iter 2/2000 - Loss: -0.604
Iter 3/2000 - Loss: -0.568
Iter 4/2000 - Loss: -0.369
Iter 5/2000 - Loss: -0.375
Iter 6/2000 - Loss: -0.544
Iter 7/2000 - Loss: -0.679
Iter 8/2000 - Loss: -0.666
Iter 9/2000 - Loss: -0.598
Iter 10/2000 - Loss: -0.594
Iter 11/2000 - Loss: -0.658
Iter 12/2000 - Loss: -0.742
Iter 13/2000 - Loss: -0.811
Iter 14/2000 - Loss: -0.847
Iter 15/2000 - Loss: -0.836
Iter 16/2000 - Loss: -0.791
Iter 17/2000 - Loss: -0.763
Iter 18/2000 - Loss: -0.797
Iter 19/2000 - Loss: -0.880
Iter 20/2000 - Loss: -0.951
Iter 1981/2000 - Loss: -1.024
Iter 1982/2000 - Loss: -1.024
Iter 1983/2000 - Loss: -1.024
Iter 1984/2000 - Loss: -1.024
Iter 1985/2000 - Loss: -1.024
Iter 1986/2000 - Loss: -1.024
Iter 1987/2000 - Loss: -1.024
Iter 1988/2000 - Loss: -1.024
Iter 1989/2000 - Loss: -1.024
Iter 1990/2000 - Loss: -1.024
Iter 1991/2000 - Loss: -1.024
Iter 1992/2000 - Loss: -1.024
Iter 1993/2000 - Loss: -1.024
Iter 1994/2000 - Loss: -1.024
Iter 1995/2000 - Loss: -1.024
Iter 1996/2000 - Loss: -1.024
Iter 1997/2000 - Loss: -1.024
Iter 1998/2000 - Loss: -1.024
Iter 1999/2000 - Loss: -1.024
Iter 2000/2000 - Loss: -1.024
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0065],
        [0.1463],
        [0.0025]])
Lengthscale: tensor([[[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]],

        [[9.7792e-03, 4.3292e-02, 1.6932e-01, 5.2580e-03, 1.3069e-04,
          1.2643e-01]]])
Signal Variance: tensor([0.0034, 0.0262, 0.7144, 0.0101])
Estimated target variance: tensor([0.0048, 0.0363, 0.9563, 0.0139])
N: 10
Signal to noise ratio: tensor([2.0014, 2.0096, 2.2101, 2.0089])
Bound on condition number: tensor([41.0575, 41.3849, 49.8443, 41.3551])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2099766055940597, policy loss: 0.7639258098746945
Experience 1, Iter 1, disc loss: 1.1894635666335636, policy loss: 0.7755910236707374
Experience 1, Iter 2, disc loss: 1.168297783329816, policy loss: 0.7871793955145518
Experience 1, Iter 3, disc loss: 1.17036060366596, policy loss: 0.7691997006787404
Experience 1, Iter 4, disc loss: 1.1498535635006206, policy loss: 0.7797579636863228
Experience 1, Iter 5, disc loss: 1.13249355898506, policy loss: 0.7885052843865943
Experience 1, Iter 6, disc loss: 1.111585614957617, policy loss: 0.8011750472996938
Experience 1, Iter 7, disc loss: 1.09897764204961, policy loss: 0.8035093069216918
Experience 1, Iter 8, disc loss: 1.0785893220255407, policy loss: 0.8158373106135386
Experience 1, Iter 9, disc loss: 1.0805934031883406, policy loss: 0.7954744195167961
Experience 1, Iter 10, disc loss: 1.0548569965128496, policy loss: 0.8173991069740476
Experience 1, Iter 11, disc loss: 1.0425650421191395, policy loss: 0.8192700223748501
Experience 1, Iter 12, disc loss: 1.0255544543997603, policy loss: 0.8287319756351332
Experience 1, Iter 13, disc loss: 1.007578655260201, policy loss: 0.8371353072722538
Experience 1, Iter 14, disc loss: 0.9891153500722478, policy loss: 0.8480341702385119
Experience 1, Iter 15, disc loss: 0.9862008184275848, policy loss: 0.8386835816373186
Experience 1, Iter 16, disc loss: 0.9620037530696561, policy loss: 0.8650589866203497
Experience 1, Iter 17, disc loss: 0.9497740000508352, policy loss: 0.8688893382510487
Experience 1, Iter 18, disc loss: 0.9428487304341338, policy loss: 0.8716066591999793
Experience 1, Iter 19, disc loss: 0.9174954596690024, policy loss: 0.8905324294155748
Experience 1, Iter 20, disc loss: 0.9254222705039198, policy loss: 0.8675554629092688
Experience 1, Iter 21, disc loss: 0.8972448402746686, policy loss: 0.8934913662754328
Experience 1, Iter 22, disc loss: 0.8956471324760684, policy loss: 0.8881604105646845
Experience 1, Iter 23, disc loss: 0.8575848642104031, policy loss: 0.9323227176317694
Experience 1, Iter 24, disc loss: 0.8783771808910283, policy loss: 0.8837779971233952
Experience 1, Iter 25, disc loss: 0.8445005297000568, policy loss: 0.9237335929076071
Experience 1, Iter 26, disc loss: 0.8227460866132728, policy loss: 0.9449819023683608
Experience 1, Iter 27, disc loss: 0.8237002928122484, policy loss: 0.9273612302009064
Experience 1, Iter 28, disc loss: 0.7844613026945628, policy loss: 0.9876026747714164
Experience 1, Iter 29, disc loss: 0.8045840866711536, policy loss: 0.931106731582654
Experience 1, Iter 30, disc loss: 0.7883172124137439, policy loss: 0.9442668936047104
Experience 1, Iter 31, disc loss: 0.7475879289975855, policy loss: 0.9981620717974005
Experience 1, Iter 32, disc loss: 0.7552430728644317, policy loss: 0.9814805625507715
Experience 1, Iter 33, disc loss: 0.7212017552205096, policy loss: 1.0218838642127253
Experience 1, Iter 34, disc loss: 0.7314531715603614, policy loss: 0.9873296454112684
Experience 1, Iter 35, disc loss: 0.695989435823899, policy loss: 1.0385263061257886
Experience 1, Iter 36, disc loss: 0.6793567549448026, policy loss: 1.0675556850589438
Experience 1, Iter 37, disc loss: 0.6662939859528401, policy loss: 1.0743806447110047
Experience 1, Iter 38, disc loss: 0.6766612985212561, policy loss: 1.0360487434716195
Experience 1, Iter 39, disc loss: 0.6527555894315591, policy loss: 1.0629729489435724
Experience 1, Iter 40, disc loss: 0.6205250957230006, policy loss: 1.1422035865854518
Experience 1, Iter 41, disc loss: 0.6259465465890843, policy loss: 1.09677155502171
Experience 1, Iter 42, disc loss: 0.6156108504447924, policy loss: 1.1044916641345286
Experience 1, Iter 43, disc loss: 0.5967818391230871, policy loss: 1.1420768997005348
Experience 1, Iter 44, disc loss: 0.5751147555073435, policy loss: 1.1704211038620378
Experience 1, Iter 45, disc loss: 0.5901788233124303, policy loss: 1.1095274188655102
Experience 1, Iter 46, disc loss: 0.5357449127216785, policy loss: 1.2379148896575518
Experience 1, Iter 47, disc loss: 0.5180531518681462, policy loss: 1.2702473030156902
Experience 1, Iter 48, disc loss: 0.49788627451504797, policy loss: 1.313150010288112
Experience 1, Iter 49, disc loss: 0.5029577095926667, policy loss: 1.2890987727758259
Experience 1, Iter 50, disc loss: 0.4810804450254113, policy loss: 1.3480166196605885
Experience 1, Iter 51, disc loss: 0.4661767145524534, policy loss: 1.3604375272345848
Experience 1, Iter 52, disc loss: 0.5093385858491852, policy loss: 1.2487829109184605
Experience 1, Iter 53, disc loss: 0.4695235511902636, policy loss: 1.3077638395818125
Experience 1, Iter 54, disc loss: 0.4499324160908582, policy loss: 1.3876529144045862
Experience 1, Iter 55, disc loss: 0.4215609594346872, policy loss: 1.4469510732063167
Experience 1, Iter 56, disc loss: 0.41489742474020375, policy loss: 1.4874697176849858
Experience 1, Iter 57, disc loss: 0.41993138335457736, policy loss: 1.4230040525906484
Experience 1, Iter 58, disc loss: 0.3892466709852971, policy loss: 1.5447854870434365
Experience 1, Iter 59, disc loss: 0.38857850564069996, policy loss: 1.4992482885785874
Experience 1, Iter 60, disc loss: 0.4001399096686556, policy loss: 1.4406253919967593
Experience 1, Iter 61, disc loss: 0.38873341956288804, policy loss: 1.4979079434488916
Experience 1, Iter 62, disc loss: 0.3990199182685087, policy loss: 1.3983219306716346
Experience 1, Iter 63, disc loss: 0.3466281113655587, policy loss: 1.6081068981324411
Experience 1, Iter 64, disc loss: 0.3744446349944165, policy loss: 1.4952775831423106
Experience 1, Iter 65, disc loss: 0.3412653224885014, policy loss: 1.6295001247879988
Experience 1, Iter 66, disc loss: 0.3310107254745418, policy loss: 1.6008683268898267
Experience 1, Iter 67, disc loss: 0.3008183171671978, policy loss: 1.747449289628317
Experience 1, Iter 68, disc loss: 0.28379296386815855, policy loss: 1.8794219849299443
Experience 1, Iter 69, disc loss: 0.3029877502350241, policy loss: 1.7123627187623374
Experience 1, Iter 70, disc loss: 0.31311234156881373, policy loss: 1.7409831253071975
Experience 1, Iter 71, disc loss: 0.2727344527607467, policy loss: 1.8904556775802346
Experience 1, Iter 72, disc loss: 0.2977318885239323, policy loss: 1.7182522515689618
Experience 1, Iter 73, disc loss: 0.26101325503633077, policy loss: 1.8807476803834353
Experience 1, Iter 74, disc loss: 0.2808188928775254, policy loss: 1.7766525953966723
Experience 1, Iter 75, disc loss: 0.2503952986803318, policy loss: 1.9137774636525169
Experience 1, Iter 76, disc loss: 0.2365617591439162, policy loss: 2.013155044400696
Experience 1, Iter 77, disc loss: 0.23812233782187398, policy loss: 1.951276553111495
Experience 1, Iter 78, disc loss: 0.23871426511434682, policy loss: 1.911633830181903
Experience 1, Iter 79, disc loss: 0.24496548956406097, policy loss: 2.022266019001739
Experience 1, Iter 80, disc loss: 0.23959832806105408, policy loss: 1.9547896082339018
Experience 1, Iter 81, disc loss: 0.21366677138384815, policy loss: 2.074666150388834
Experience 1, Iter 82, disc loss: 0.19706205806628152, policy loss: 2.2415185356024283
Experience 1, Iter 83, disc loss: 0.18606509727799486, policy loss: 2.299708492885224
Experience 1, Iter 84, disc loss: 0.2026451176844679, policy loss: 2.135516596581911
Experience 1, Iter 85, disc loss: 0.17203978906166595, policy loss: 2.378819849094302
Experience 1, Iter 86, disc loss: 0.18666955671436425, policy loss: 2.264837832711379
Experience 1, Iter 87, disc loss: 0.17332571650351242, policy loss: 2.293003578099067
Experience 1, Iter 88, disc loss: 0.1680890703508181, policy loss: 2.3409962070547325
Experience 1, Iter 89, disc loss: 0.15658145795389994, policy loss: 2.4747002714769426
Experience 1, Iter 90, disc loss: 0.15847447256725367, policy loss: 2.4751339190502293
Experience 1, Iter 91, disc loss: 0.15947135888581368, policy loss: 2.44672619818105
Experience 1, Iter 92, disc loss: 0.14562217402477548, policy loss: 2.5353991726279452
Experience 1, Iter 93, disc loss: 0.13890687468897164, policy loss: 2.60273908702392
Experience 1, Iter 94, disc loss: 0.15020650648960765, policy loss: 2.4206638549686943
Experience 1, Iter 95, disc loss: 0.14750562000238632, policy loss: 2.4600014360105336
Experience 1, Iter 96, disc loss: 0.12926238838557713, policy loss: 2.662583335700004
Experience 1, Iter 97, disc loss: 0.12390747490597832, policy loss: 2.753020633612029
Experience 1, Iter 98, disc loss: 0.14661480686643794, policy loss: 2.429940848059422
Experience 1, Iter 99, disc loss: 0.1255387717079133, policy loss: 2.6167893345179847
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.0159],
        [0.3945],
        [0.0066]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0245, 0.1089, 0.3075, 0.0090, 0.0014, 0.3554]],

        [[0.0245, 0.1089, 0.3075, 0.0090, 0.0014, 0.3554]],

        [[0.0245, 0.1089, 0.3075, 0.0090, 0.0014, 0.3554]],

        [[0.0245, 0.1089, 0.3075, 0.0090, 0.0014, 0.3554]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0120, 0.0636, 1.5780, 0.0265], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0120, 0.0636, 1.5780, 0.0265])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.148
Iter 2/2000 - Loss: 1.039
Iter 3/2000 - Loss: 0.701
Iter 4/2000 - Loss: 0.754
Iter 5/2000 - Loss: 0.843
Iter 6/2000 - Loss: 0.796
Iter 7/2000 - Loss: 0.717
Iter 8/2000 - Loss: 0.692
Iter 9/2000 - Loss: 0.704
Iter 10/2000 - Loss: 0.727
Iter 11/2000 - Loss: 0.751
Iter 12/2000 - Loss: 0.753
Iter 13/2000 - Loss: 0.709
Iter 14/2000 - Loss: 0.622
Iter 15/2000 - Loss: 0.529
Iter 16/2000 - Loss: 0.473
Iter 17/2000 - Loss: 0.467
Iter 18/2000 - Loss: 0.492
Iter 19/2000 - Loss: 0.518
Iter 20/2000 - Loss: 0.524
Iter 1981/2000 - Loss: -5.525
Iter 1982/2000 - Loss: -5.525
Iter 1983/2000 - Loss: -5.525
Iter 1984/2000 - Loss: -5.525
Iter 1985/2000 - Loss: -5.525
Iter 1986/2000 - Loss: -5.525
Iter 1987/2000 - Loss: -5.525
Iter 1988/2000 - Loss: -5.525
Iter 1989/2000 - Loss: -5.525
Iter 1990/2000 - Loss: -5.525
Iter 1991/2000 - Loss: -5.525
Iter 1992/2000 - Loss: -5.525
Iter 1993/2000 - Loss: -5.525
Iter 1994/2000 - Loss: -5.525
Iter 1995/2000 - Loss: -5.525
Iter 1996/2000 - Loss: -5.525
Iter 1997/2000 - Loss: -5.525
Iter 1998/2000 - Loss: -5.525
Iter 1999/2000 - Loss: -5.525
Iter 2000/2000 - Loss: -5.526
***AFTER OPTIMATION***
Noise Variance: tensor([[5.7211e-04],
        [7.4195e-05],
        [9.1745e-04],
        [4.8877e-04]])
Lengthscale: tensor([[[20.4479,  3.3617, 56.7528, 18.9106, 13.9921, 52.6550]],

        [[18.5363, 17.7231, 14.4823,  1.2301,  0.5585,  9.7564]],

        [[28.4527, 37.6310, 19.8598,  0.9081,  0.8290, 15.4598]],

        [[40.4479, 59.6885, 10.4297,  3.6833, 11.7235, 33.9253]]])
Signal Variance: tensor([ 0.0414,  0.4643, 11.9354,  0.3698])
Estimated target variance: tensor([0.0120, 0.0636, 1.5780, 0.0265])
N: 20
Signal to noise ratio: tensor([  8.5109,  79.1097, 114.0583,  27.5048])
Bound on condition number: tensor([  1449.6954, 125167.8202, 260186.9075,  15131.3013])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.11564782780797939, policy loss: 2.4823330152734675
Experience 2, Iter 1, disc loss: 0.1204330962047461, policy loss: 2.4300725475225105
Experience 2, Iter 2, disc loss: 0.12459342979992355, policy loss: 2.388699262363731
Experience 2, Iter 3, disc loss: 0.13068258086760767, policy loss: 2.3273156492134124
Experience 2, Iter 4, disc loss: 0.14411384901227545, policy loss: 2.2127974637098395
Experience 2, Iter 5, disc loss: 0.15631768518070807, policy loss: 2.115437965217461
Experience 2, Iter 6, disc loss: 0.16729753658874447, policy loss: 2.0393638246862555
Experience 2, Iter 7, disc loss: 0.18420147326955602, policy loss: 1.9375291492047801
Experience 2, Iter 8, disc loss: 0.19529577612001053, policy loss: 1.870376651481654
Experience 2, Iter 9, disc loss: 0.21356453111647217, policy loss: 1.7797913041012179
Experience 2, Iter 10, disc loss: 0.22716328017660625, policy loss: 1.7248123588469435
Experience 2, Iter 11, disc loss: 0.26512163103538594, policy loss: 1.5897843966943928
Experience 2, Iter 12, disc loss: 0.2752268534174909, policy loss: 1.5565253880886223
Experience 2, Iter 13, disc loss: 0.27467100816700535, policy loss: 1.5722567833713361
Experience 2, Iter 14, disc loss: 0.2744943092134952, policy loss: 1.587192691254203
Experience 2, Iter 15, disc loss: 0.30768751570342784, policy loss: 1.4800032802165326
Experience 2, Iter 16, disc loss: 0.3148756348889078, policy loss: 1.5320167470889525
Experience 2, Iter 17, disc loss: 0.33971978666541425, policy loss: 1.5072479647862178
Experience 2, Iter 18, disc loss: 0.37351954318780023, policy loss: 1.371411887105495
Experience 2, Iter 19, disc loss: 0.368540358904116, policy loss: 1.4193467900176708
Experience 2, Iter 20, disc loss: 0.3985757848971717, policy loss: 1.444507664434388
Experience 2, Iter 21, disc loss: 0.3833212192325003, policy loss: 1.482565708138202
Experience 2, Iter 22, disc loss: 0.38596757625726724, policy loss: 1.6257514321533018
Experience 2, Iter 23, disc loss: 0.37199540364964306, policy loss: 1.4470568316843235
Experience 2, Iter 24, disc loss: 0.3226640508192954, policy loss: 1.6652628827598468
Experience 2, Iter 25, disc loss: 0.4368362060548605, policy loss: 1.433721259682442
Experience 2, Iter 26, disc loss: 0.36276737051172003, policy loss: 1.6046849007792066
Experience 2, Iter 27, disc loss: 0.352565415637849, policy loss: 1.719003241633005
Experience 2, Iter 28, disc loss: 0.3410399342130458, policy loss: 1.6345162774123625
Experience 2, Iter 29, disc loss: 0.3204850233097307, policy loss: 1.763592143479573
Experience 2, Iter 30, disc loss: 0.31612647725232335, policy loss: 1.8159031202478109
Experience 2, Iter 31, disc loss: 0.29749666530636376, policy loss: 1.801279873764816
Experience 2, Iter 32, disc loss: 0.288011410850543, policy loss: 1.7751467857719532
Experience 2, Iter 33, disc loss: 0.3145000019223428, policy loss: 1.7537807719942764
Experience 2, Iter 34, disc loss: 0.26301674504538586, policy loss: 1.920959877355545
Experience 2, Iter 35, disc loss: 0.26855810693772475, policy loss: 1.9402298198386514
Experience 2, Iter 36, disc loss: 0.2691162429573677, policy loss: 1.8889707522531096
Experience 2, Iter 37, disc loss: 0.24988173085692494, policy loss: 1.9897373101035056
Experience 2, Iter 38, disc loss: 0.2828450029026598, policy loss: 1.8493238079672882
Experience 2, Iter 39, disc loss: 0.24243371998964147, policy loss: 2.0718887692607444
Experience 2, Iter 40, disc loss: 0.26388548983510884, policy loss: 1.8638142142901757
Experience 2, Iter 41, disc loss: 0.229968397041665, policy loss: 2.0178879627546
Experience 2, Iter 42, disc loss: 0.25332167840008735, policy loss: 1.8837523916306014
Experience 2, Iter 43, disc loss: 0.23857086820850287, policy loss: 1.9125266381170087
Experience 2, Iter 44, disc loss: 0.23234795877591796, policy loss: 1.9296191435465357
Experience 2, Iter 45, disc loss: 0.2078552669513638, policy loss: 2.12530200866616
Experience 2, Iter 46, disc loss: 0.2474011940448213, policy loss: 2.0252984497151014
Experience 2, Iter 47, disc loss: 0.21143426461004622, policy loss: 2.0716900933854494
Experience 2, Iter 48, disc loss: 0.2195696489062151, policy loss: 1.9766179660390932
Experience 2, Iter 49, disc loss: 0.19679322203384422, policy loss: 2.1291854674002435
Experience 2, Iter 50, disc loss: 0.18684871916760903, policy loss: 2.1064218834802206
Experience 2, Iter 51, disc loss: 0.18600239097852742, policy loss: 2.140195011448795
Experience 2, Iter 52, disc loss: 0.18414976278050263, policy loss: 2.1685837654067037
Experience 2, Iter 53, disc loss: 0.20254224889472772, policy loss: 2.041685804975301
Experience 2, Iter 54, disc loss: 0.18954050588691151, policy loss: 2.139690955073825
Experience 2, Iter 55, disc loss: 0.21598310214168237, policy loss: 1.9513111547980497
Experience 2, Iter 56, disc loss: 0.20802671644098433, policy loss: 1.993138636914264
Experience 2, Iter 57, disc loss: 0.15353484144595977, policy loss: 2.2846846612303935
Experience 2, Iter 58, disc loss: 0.16537446490353944, policy loss: 2.1800628290765625
Experience 2, Iter 59, disc loss: 0.1898210293817196, policy loss: 2.1250780467829085
Experience 2, Iter 60, disc loss: 0.17426438856612156, policy loss: 2.185133203426328
Experience 2, Iter 61, disc loss: 0.16122959338988518, policy loss: 2.306917067722628
Experience 2, Iter 62, disc loss: 0.17159758719709553, policy loss: 2.343882700259808
Experience 2, Iter 63, disc loss: 0.17932005877427862, policy loss: 2.261020075244528
Experience 2, Iter 64, disc loss: 0.19244726081460276, policy loss: 2.312744014581689
Experience 2, Iter 65, disc loss: 0.17901550611454373, policy loss: 2.1988470330302357
Experience 2, Iter 66, disc loss: 0.1714172266137322, policy loss: 2.344958886151072
Experience 2, Iter 67, disc loss: 0.17779401393730324, policy loss: 2.1449833323215097
Experience 2, Iter 68, disc loss: 0.14403233639134203, policy loss: 2.561960266417562
Experience 2, Iter 69, disc loss: 0.15684116475368237, policy loss: 2.408618609227307
Experience 2, Iter 70, disc loss: 0.1658905409006672, policy loss: 2.422414512905701
Experience 2, Iter 71, disc loss: 0.14726763706433393, policy loss: 2.4090800928959637
Experience 2, Iter 72, disc loss: 0.14070423417758568, policy loss: 2.456178741688394
Experience 2, Iter 73, disc loss: 0.17570951314314218, policy loss: 2.395609509983421
Experience 2, Iter 74, disc loss: 0.12660129750022803, policy loss: 2.6810897640370706
Experience 2, Iter 75, disc loss: 0.13694733411848659, policy loss: 2.52230868853813
Experience 2, Iter 76, disc loss: 0.15585661859312477, policy loss: 2.5182964992134123
Experience 2, Iter 77, disc loss: 0.14147556884599924, policy loss: 2.592647134972129
Experience 2, Iter 78, disc loss: 0.13185852853500007, policy loss: 2.5140484461048898
Experience 2, Iter 79, disc loss: 0.1358188832026258, policy loss: 2.5678105501596153
Experience 2, Iter 80, disc loss: 0.1323495360698202, policy loss: 2.613294928467877
Experience 2, Iter 81, disc loss: 0.1437166119539237, policy loss: 2.4655065580078395
Experience 2, Iter 82, disc loss: 0.1387113078544159, policy loss: 2.447460328214361
Experience 2, Iter 83, disc loss: 0.12516639845611682, policy loss: 2.6414344786265627
Experience 2, Iter 84, disc loss: 0.11894548921803512, policy loss: 2.643353515390422
Experience 2, Iter 85, disc loss: 0.11300426054572205, policy loss: 2.6904165500544286
Experience 2, Iter 86, disc loss: 0.10949425215838374, policy loss: 2.7711562206227343
Experience 2, Iter 87, disc loss: 0.11681524873153987, policy loss: 2.676625752354357
Experience 2, Iter 88, disc loss: 0.11186315605401131, policy loss: 2.675009397559213
Experience 2, Iter 89, disc loss: 0.09701304286394766, policy loss: 2.7947114009642475
Experience 2, Iter 90, disc loss: 0.11667006307438504, policy loss: 2.6470586539368046
Experience 2, Iter 91, disc loss: 0.11240397171926268, policy loss: 2.638811458579673
Experience 2, Iter 92, disc loss: 0.1028039900208712, policy loss: 2.80960382411546
Experience 2, Iter 93, disc loss: 0.10244654851618776, policy loss: 2.6849090509306732
Experience 2, Iter 94, disc loss: 0.11209626852707658, policy loss: 2.7286836083780295
Experience 2, Iter 95, disc loss: 0.10861274357029327, policy loss: 2.663321802817822
Experience 2, Iter 96, disc loss: 0.12923809110545192, policy loss: 2.71369373430531
Experience 2, Iter 97, disc loss: 0.10124793757711059, policy loss: 2.7779468868123374
Experience 2, Iter 98, disc loss: 0.11215461746720365, policy loss: 2.7254839120412635
Experience 2, Iter 99, disc loss: 0.10595073494492982, policy loss: 2.776099657231562
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0043],
        [0.0243],
        [0.2752],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0193, 0.1540, 0.2133, 0.0111, 0.0010, 0.8776]],

        [[0.0193, 0.1540, 0.2133, 0.0111, 0.0010, 0.8776]],

        [[0.0193, 0.1540, 0.2133, 0.0111, 0.0010, 0.8776]],

        [[0.0193, 0.1540, 0.2133, 0.0111, 0.0010, 0.8776]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0171, 0.0972, 1.1007, 0.0191], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0171, 0.0972, 1.1007, 0.0191])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.928
Iter 2/2000 - Loss: 0.704
Iter 3/2000 - Loss: 0.614
Iter 4/2000 - Loss: 0.612
Iter 5/2000 - Loss: 0.585
Iter 6/2000 - Loss: 0.505
Iter 7/2000 - Loss: 0.471
Iter 8/2000 - Loss: 0.476
Iter 9/2000 - Loss: 0.438
Iter 10/2000 - Loss: 0.351
Iter 11/2000 - Loss: 0.291
Iter 12/2000 - Loss: 0.292
Iter 13/2000 - Loss: 0.298
Iter 14/2000 - Loss: 0.255
Iter 15/2000 - Loss: 0.170
Iter 16/2000 - Loss: 0.091
Iter 17/2000 - Loss: 0.037
Iter 18/2000 - Loss: -0.008
Iter 19/2000 - Loss: -0.067
Iter 20/2000 - Loss: -0.149
Iter 1981/2000 - Loss: -6.303
Iter 1982/2000 - Loss: -6.303
Iter 1983/2000 - Loss: -6.303
Iter 1984/2000 - Loss: -6.303
Iter 1985/2000 - Loss: -6.303
Iter 1986/2000 - Loss: -6.303
Iter 1987/2000 - Loss: -6.303
Iter 1988/2000 - Loss: -6.303
Iter 1989/2000 - Loss: -6.303
Iter 1990/2000 - Loss: -6.303
Iter 1991/2000 - Loss: -6.303
Iter 1992/2000 - Loss: -6.303
Iter 1993/2000 - Loss: -6.304
Iter 1994/2000 - Loss: -6.304
Iter 1995/2000 - Loss: -6.304
Iter 1996/2000 - Loss: -6.304
Iter 1997/2000 - Loss: -6.304
Iter 1998/2000 - Loss: -6.304
Iter 1999/2000 - Loss: -6.304
Iter 2000/2000 - Loss: -6.304
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0003],
        [0.0018],
        [0.0004]])
Lengthscale: tensor([[[20.4789,  5.7283, 50.5594, 23.4190, 10.3956, 55.0901]],

        [[32.2936, 36.3831, 13.2113,  2.1158,  0.7409, 13.5691]],

        [[31.5198, 47.5115, 13.2778,  0.9329,  3.8309, 14.1384]],

        [[32.5634, 52.6350, 10.0633,  3.7547, 11.7375, 35.9389]]])
Signal Variance: tensor([0.0899, 0.8695, 8.7332, 0.3369])
Estimated target variance: tensor([0.0171, 0.0972, 1.1007, 0.0191])
N: 30
Signal to noise ratio: tensor([13.2855, 54.3167, 69.1549, 29.9491])
Bound on condition number: tensor([  5296.1654,  88510.1611, 143472.8414,  26909.5292])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.364262211063456, policy loss: 1.9037083797470244
Experience 3, Iter 1, disc loss: 0.4683556247920346, policy loss: 1.4458629924109447
Experience 3, Iter 2, disc loss: 0.42985388310980144, policy loss: 1.5512083675646586
Experience 3, Iter 3, disc loss: 0.37606683617921877, policy loss: 1.7382881973726407
Experience 3, Iter 4, disc loss: 0.3915956061592516, policy loss: 1.7179692940045965
Experience 3, Iter 5, disc loss: 0.296667035529919, policy loss: 1.9131011772786393
Experience 3, Iter 6, disc loss: 0.30281172312972776, policy loss: 1.862015298051805
Experience 3, Iter 7, disc loss: 0.3195941364774795, policy loss: 2.154865000171938
Experience 3, Iter 8, disc loss: 0.3058142736194329, policy loss: 2.0230804833266824
Experience 3, Iter 9, disc loss: 0.312016169341764, policy loss: 2.141275123009903
Experience 3, Iter 10, disc loss: 0.2833796809092498, policy loss: 2.3257987365455937
Experience 3, Iter 11, disc loss: 0.2820661916759777, policy loss: 2.4707936567879685
Experience 3, Iter 12, disc loss: 0.30843039427856467, policy loss: 2.291300736519406
Experience 3, Iter 13, disc loss: 0.30416433966781964, policy loss: 2.5493942518319863
Experience 3, Iter 14, disc loss: 0.2776862561774809, policy loss: 2.6756124509164514
Experience 3, Iter 15, disc loss: 0.32015151388040075, policy loss: 2.244269023121511
Experience 3, Iter 16, disc loss: 0.30658469009445505, policy loss: 2.3820041044359623
Experience 3, Iter 17, disc loss: 0.33210751110325254, policy loss: 2.0908739887809227
Experience 3, Iter 18, disc loss: 0.3021381673377537, policy loss: 2.156893306870315
Experience 3, Iter 19, disc loss: 0.27740099147684716, policy loss: 2.504046073961384
Experience 3, Iter 20, disc loss: 0.299775797388809, policy loss: 1.9893460787235975
Experience 3, Iter 21, disc loss: 0.3173980279490313, policy loss: 2.09325986294058
Experience 3, Iter 22, disc loss: 0.343400066290363, policy loss: 2.057381191212913
Experience 3, Iter 23, disc loss: 0.31721824624352374, policy loss: 2.182445347730641
Experience 3, Iter 24, disc loss: 0.27921951479826196, policy loss: 2.1038752167141297
Experience 3, Iter 25, disc loss: 0.27916623037371324, policy loss: 2.1976228640649245
Experience 3, Iter 26, disc loss: 0.3027448476398655, policy loss: 2.0703680270901814
Experience 3, Iter 27, disc loss: 0.24655333272539537, policy loss: 2.4897931702775487
Experience 3, Iter 28, disc loss: 0.2461030112588414, policy loss: 2.386181387720938
Experience 3, Iter 29, disc loss: 0.24796877925443228, policy loss: 2.4685333615486487
Experience 3, Iter 30, disc loss: 0.22199074835800464, policy loss: 2.704522393448357
Experience 3, Iter 31, disc loss: 0.21008297439242582, policy loss: 2.924236953715294
Experience 3, Iter 32, disc loss: 0.22675645177306075, policy loss: 2.8349380197971596
Experience 3, Iter 33, disc loss: 0.19391698301962, policy loss: 2.7679106628378896
Experience 3, Iter 34, disc loss: 0.19057759718592507, policy loss: 2.6877105963141545
Experience 3, Iter 35, disc loss: 0.18888872930888484, policy loss: 2.812253580297898
Experience 3, Iter 36, disc loss: 0.17201396077875322, policy loss: 2.823462182713244
Experience 3, Iter 37, disc loss: 0.1746211028984165, policy loss: 2.753742816828001
Experience 3, Iter 38, disc loss: 0.15628716336239964, policy loss: 2.6672002639836454
Experience 3, Iter 39, disc loss: 0.14955737622669027, policy loss: 2.71273200873288
Experience 3, Iter 40, disc loss: 0.1505018600633431, policy loss: 2.7882723954536868
Experience 3, Iter 41, disc loss: 0.1498822580840217, policy loss: 2.771949871326055
Experience 3, Iter 42, disc loss: 0.14619126920139952, policy loss: 2.6639793841345423
Experience 3, Iter 43, disc loss: 0.14772409812377324, policy loss: 2.6585604968725303
Experience 3, Iter 44, disc loss: 0.13978385831104828, policy loss: 2.7283806482413704
Experience 3, Iter 45, disc loss: 0.14781364933213825, policy loss: 2.718254740264741
Experience 3, Iter 46, disc loss: 0.1327014106973419, policy loss: 2.7227082238165066
Experience 3, Iter 47, disc loss: 0.1319144572510102, policy loss: 2.6405845945877893
Experience 3, Iter 48, disc loss: 0.12019807277245616, policy loss: 3.0718205934659446
Experience 3, Iter 49, disc loss: 0.131787992381676, policy loss: 2.6262925476888737
Experience 3, Iter 50, disc loss: 0.13299389972516928, policy loss: 2.894911678009329
Experience 3, Iter 51, disc loss: 0.1294992572463965, policy loss: 2.7442701743850395
Experience 3, Iter 52, disc loss: 0.12063607640939539, policy loss: 2.759983491384576
Experience 3, Iter 53, disc loss: 0.11886370004704959, policy loss: 2.9811120675246987
Experience 3, Iter 54, disc loss: 0.11121942234711923, policy loss: 3.043638862299385
Experience 3, Iter 55, disc loss: 0.10431353825482523, policy loss: 3.1951847180754473
Experience 3, Iter 56, disc loss: 0.10464000835936402, policy loss: 3.240360926995553
Experience 3, Iter 57, disc loss: 0.1081228625900877, policy loss: 3.127860209667407
Experience 3, Iter 58, disc loss: 0.1097803781234456, policy loss: 3.1344806266309218
Experience 3, Iter 59, disc loss: 0.10070507796736014, policy loss: 3.2303824319553556
Experience 3, Iter 60, disc loss: 0.1072683082647557, policy loss: 2.921374137686728
Experience 3, Iter 61, disc loss: 0.10200368581961566, policy loss: 3.3136687618275307
Experience 3, Iter 62, disc loss: 0.10269323613175392, policy loss: 3.1109786198452296
Experience 3, Iter 63, disc loss: 0.10393672802384499, policy loss: 3.0582917408308052
Experience 3, Iter 64, disc loss: 0.08900932464032363, policy loss: 3.269045975874863
Experience 3, Iter 65, disc loss: 0.09456319411315771, policy loss: 3.430546452316227
Experience 3, Iter 66, disc loss: 0.09046592535410476, policy loss: 3.2251360274458234
Experience 3, Iter 67, disc loss: 0.08831227442181636, policy loss: 3.4366092269634847
Experience 3, Iter 68, disc loss: 0.08815244735481256, policy loss: 3.4238595955844184
Experience 3, Iter 69, disc loss: 0.08909174112417027, policy loss: 3.3115090410359436
Experience 3, Iter 70, disc loss: 0.10286122209795867, policy loss: 3.194164895342219
Experience 3, Iter 71, disc loss: 0.09078738887372584, policy loss: 3.1067499549142066
Experience 3, Iter 72, disc loss: 0.0842550291711399, policy loss: 3.343574884957028
Experience 3, Iter 73, disc loss: 0.08065794871612411, policy loss: 3.3129683166162653
Experience 3, Iter 74, disc loss: 0.08247558645281111, policy loss: 3.3784483367701803
Experience 3, Iter 75, disc loss: 0.07976914207498068, policy loss: 3.4170956096766787
Experience 3, Iter 76, disc loss: 0.0779306075542501, policy loss: 3.394840488934623
Experience 3, Iter 77, disc loss: 0.08712215878284821, policy loss: 3.1470187143436386
Experience 3, Iter 78, disc loss: 0.08616565769522319, policy loss: 3.1194954064319633
Experience 3, Iter 79, disc loss: 0.0838043825088379, policy loss: 3.333983097612878
Experience 3, Iter 80, disc loss: 0.08211499459884747, policy loss: 3.3667592730674585
Experience 3, Iter 81, disc loss: 0.08428185166907513, policy loss: 3.185585891440865
Experience 3, Iter 82, disc loss: 0.09181410407821512, policy loss: 3.17924265932102
Experience 3, Iter 83, disc loss: 0.08111337677013773, policy loss: 3.3831583867163344
Experience 3, Iter 84, disc loss: 0.07873484466943442, policy loss: 3.438976847291287
Experience 3, Iter 85, disc loss: 0.0738238304819741, policy loss: 3.574046677665799
Experience 3, Iter 86, disc loss: 0.07272815469616367, policy loss: 4.0351545999770995
Experience 3, Iter 87, disc loss: 0.0808039708808146, policy loss: 3.2592127079484725
Experience 3, Iter 88, disc loss: 0.07469130449019196, policy loss: 3.6308849956257987
Experience 3, Iter 89, disc loss: 0.07963797397265278, policy loss: 3.2757503033113364
Experience 3, Iter 90, disc loss: 0.0766495449858984, policy loss: 3.4030472748773195
Experience 3, Iter 91, disc loss: 0.06953524272355079, policy loss: 3.795480363067056
Experience 3, Iter 92, disc loss: 0.07591699886981917, policy loss: 3.532089702810404
Experience 3, Iter 93, disc loss: 0.08161513682683766, policy loss: 3.4974659287449485
Experience 3, Iter 94, disc loss: 0.07392428693005365, policy loss: 3.7065651715921817
Experience 3, Iter 95, disc loss: 0.07262728077811287, policy loss: 3.4208448346513585
Experience 3, Iter 96, disc loss: 0.06257545163026881, policy loss: 3.761862371208459
Experience 3, Iter 97, disc loss: 0.07006688976481619, policy loss: 3.7173340765066145
Experience 3, Iter 98, disc loss: 0.0588590476116362, policy loss: 3.818588313284995
Experience 3, Iter 99, disc loss: 0.0613400001786889, policy loss: 3.7063739293739077
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.0790],
        [0.6724],
        [0.0082]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.5216e-02, 1.5877e-01, 3.9801e-01, 1.6378e-02, 1.8200e-03,
          2.3944e+00]],

        [[1.5216e-02, 1.5877e-01, 3.9801e-01, 1.6378e-02, 1.8200e-03,
          2.3944e+00]],

        [[1.5216e-02, 1.5877e-01, 3.9801e-01, 1.6378e-02, 1.8200e-03,
          2.3944e+00]],

        [[1.5216e-02, 1.5877e-01, 3.9801e-01, 1.6378e-02, 1.8200e-03,
          2.3944e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0179, 0.3159, 2.6894, 0.0327], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0179, 0.3159, 2.6894, 0.0327])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.958
Iter 2/2000 - Loss: 1.987
Iter 3/2000 - Loss: 1.884
Iter 4/2000 - Loss: 1.830
Iter 5/2000 - Loss: 1.864
Iter 6/2000 - Loss: 1.852
Iter 7/2000 - Loss: 1.778
Iter 8/2000 - Loss: 1.729
Iter 9/2000 - Loss: 1.731
Iter 10/2000 - Loss: 1.732
Iter 11/2000 - Loss: 1.692
Iter 12/2000 - Loss: 1.629
Iter 13/2000 - Loss: 1.578
Iter 14/2000 - Loss: 1.547
Iter 15/2000 - Loss: 1.514
Iter 16/2000 - Loss: 1.460
Iter 17/2000 - Loss: 1.387
Iter 18/2000 - Loss: 1.309
Iter 19/2000 - Loss: 1.230
Iter 20/2000 - Loss: 1.146
Iter 1981/2000 - Loss: -6.213
Iter 1982/2000 - Loss: -6.213
Iter 1983/2000 - Loss: -6.214
Iter 1984/2000 - Loss: -6.214
Iter 1985/2000 - Loss: -6.214
Iter 1986/2000 - Loss: -6.214
Iter 1987/2000 - Loss: -6.214
Iter 1988/2000 - Loss: -6.214
Iter 1989/2000 - Loss: -6.214
Iter 1990/2000 - Loss: -6.214
Iter 1991/2000 - Loss: -6.214
Iter 1992/2000 - Loss: -6.214
Iter 1993/2000 - Loss: -6.214
Iter 1994/2000 - Loss: -6.214
Iter 1995/2000 - Loss: -6.214
Iter 1996/2000 - Loss: -6.214
Iter 1997/2000 - Loss: -6.214
Iter 1998/2000 - Loss: -6.214
Iter 1999/2000 - Loss: -6.214
Iter 2000/2000 - Loss: -6.214
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0004],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[16.8360,  7.2371, 54.2646, 10.4929, 15.1735, 52.4797]],

        [[26.1125, 41.1137, 15.3239,  1.4379,  1.3629, 18.1791]],

        [[26.4503, 41.8132, 12.5725,  1.0140,  6.0114, 17.1627]],

        [[25.7541, 49.1183, 12.2020,  3.9688,  1.4869, 48.3884]]])
Signal Variance: tensor([ 0.1278,  1.2759, 10.7841,  0.4143])
Estimated target variance: tensor([0.0179, 0.3159, 2.6894, 0.0327])
N: 40
Signal to noise ratio: tensor([15.7575, 60.1422, 82.2634, 38.2111])
Bound on condition number: tensor([  9932.9741, 144684.1485, 270691.9227,  58404.6557])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.11034269887455428, policy loss: 2.9901768403025617
Experience 4, Iter 1, disc loss: 0.13142745616520585, policy loss: 2.696563857839818
Experience 4, Iter 2, disc loss: 0.1289724823442095, policy loss: 2.6228072002097815
Experience 4, Iter 3, disc loss: 0.13375306397194703, policy loss: 2.7244944004011664
Experience 4, Iter 4, disc loss: 0.1213989640917402, policy loss: 2.9697846902359837
Experience 4, Iter 5, disc loss: 0.11035796966277281, policy loss: 2.966939006764235
Experience 4, Iter 6, disc loss: 0.1103299143819319, policy loss: 2.8125285516363094
Experience 4, Iter 7, disc loss: 0.1159811230276862, policy loss: 2.9114831855911536
Experience 4, Iter 8, disc loss: 0.11263887437143107, policy loss: 2.9720748159083
Experience 4, Iter 9, disc loss: 0.1110244862239, policy loss: 2.9415822393306863
Experience 4, Iter 10, disc loss: 0.09370469157806452, policy loss: 3.3164505724829207
Experience 4, Iter 11, disc loss: 0.09221513082773608, policy loss: 3.381986007018014
Experience 4, Iter 12, disc loss: 0.09219106891066126, policy loss: 3.1899678734450534
Experience 4, Iter 13, disc loss: 0.10010888314519374, policy loss: 3.155645834539361
Experience 4, Iter 14, disc loss: 0.09424125730675947, policy loss: 3.1834369710386543
Experience 4, Iter 15, disc loss: 0.08478051638670892, policy loss: 3.4709645256764152
Experience 4, Iter 16, disc loss: 0.08190551951372839, policy loss: 3.430452605777355
Experience 4, Iter 17, disc loss: 0.08169043937762442, policy loss: 3.445454065165882
Experience 4, Iter 18, disc loss: 0.08348296080702702, policy loss: 3.3315316365580903
Experience 4, Iter 19, disc loss: 0.07785775346330492, policy loss: 3.294639971795788
Experience 4, Iter 20, disc loss: 0.07607301809231182, policy loss: 3.4348026119057984
Experience 4, Iter 21, disc loss: 0.07681809622343254, policy loss: 3.2208459343748403
Experience 4, Iter 22, disc loss: 0.07901679892283384, policy loss: 3.1210742248086465
Experience 4, Iter 23, disc loss: 0.07034535925234414, policy loss: 3.4033697124463176
Experience 4, Iter 24, disc loss: 0.07074481426840867, policy loss: 3.4336821459654967
Experience 4, Iter 25, disc loss: 0.07813722186011078, policy loss: 3.1715937232056266
Experience 4, Iter 26, disc loss: 0.07732364816390527, policy loss: 3.4372356909863924
Experience 4, Iter 27, disc loss: 0.06928318609508316, policy loss: 3.54982341054519
Experience 4, Iter 28, disc loss: 0.07144582034472366, policy loss: 3.3492376730999105
Experience 4, Iter 29, disc loss: 0.07848629260146665, policy loss: 3.062962749078408
Experience 4, Iter 30, disc loss: 0.07082453992298757, policy loss: 3.4017459390724314
Experience 4, Iter 31, disc loss: 0.06879476964773129, policy loss: 3.355764879368216
Experience 4, Iter 32, disc loss: 0.06641669210226966, policy loss: 3.5890552915557254
Experience 4, Iter 33, disc loss: 0.06928042669134564, policy loss: 3.3331503196551955
Experience 4, Iter 34, disc loss: 0.06776347570831538, policy loss: 3.6549854733216494
Experience 4, Iter 35, disc loss: 0.06722057575968637, policy loss: 3.47396509466076
Experience 4, Iter 36, disc loss: 0.07637574020834825, policy loss: 3.1712562208491466
Experience 4, Iter 37, disc loss: 0.06682949675093537, policy loss: 3.5868737064817804
Experience 4, Iter 38, disc loss: 0.0686890260530417, policy loss: 3.476904104050722
Experience 4, Iter 39, disc loss: 0.06893155153244591, policy loss: 3.470216896250304
Experience 4, Iter 40, disc loss: 0.06493948137477218, policy loss: 3.5288300208954295
Experience 4, Iter 41, disc loss: 0.06432036467331896, policy loss: 3.829534680003083
Experience 4, Iter 42, disc loss: 0.07033696971525773, policy loss: 3.5582271197606055
Experience 4, Iter 43, disc loss: 0.06642278598070454, policy loss: 3.591780122539003
Experience 4, Iter 44, disc loss: 0.06063198649552165, policy loss: 3.8163067692843717
Experience 4, Iter 45, disc loss: 0.06580889278779256, policy loss: 3.4560608369861305
Experience 4, Iter 46, disc loss: 0.055788480298765084, policy loss: 4.057861995287823
Experience 4, Iter 47, disc loss: 0.06648481865032929, policy loss: 3.7170650089265225
Experience 4, Iter 48, disc loss: 0.07000829507947992, policy loss: 3.4150705411991362
Experience 4, Iter 49, disc loss: 0.06174004979197266, policy loss: 3.868050917047321
Experience 4, Iter 50, disc loss: 0.060786304587587166, policy loss: 4.118870208740837
Experience 4, Iter 51, disc loss: 0.062040800365069884, policy loss: 3.6872130034549753
Experience 4, Iter 52, disc loss: 0.06477026451766946, policy loss: 3.7887153062708867
Experience 4, Iter 53, disc loss: 0.06640652473585672, policy loss: 3.4656200673345507
Experience 4, Iter 54, disc loss: 0.0624073186617026, policy loss: 3.770165883270916
Experience 4, Iter 55, disc loss: 0.0641747381372429, policy loss: 3.5363809351043884
Experience 4, Iter 56, disc loss: 0.06282892807079539, policy loss: 3.5643242651420124
Experience 4, Iter 57, disc loss: 0.05119772829086574, policy loss: 3.9353457855759904
Experience 4, Iter 58, disc loss: 0.05861975445104819, policy loss: 3.7720019674740923
Experience 4, Iter 59, disc loss: 0.06214922703490214, policy loss: 3.5025782196441666
Experience 4, Iter 60, disc loss: 0.060366741844266755, policy loss: 3.598365680139216
Experience 4, Iter 61, disc loss: 0.06050665493795945, policy loss: 3.610203532097901
Experience 4, Iter 62, disc loss: 0.05837546408536752, policy loss: 3.6124227660133506
Experience 4, Iter 63, disc loss: 0.05665538747215651, policy loss: 3.683316847543539
Experience 4, Iter 64, disc loss: 0.0498850089669716, policy loss: 4.019463583563119
Experience 4, Iter 65, disc loss: 0.046864240889065586, policy loss: 4.166392700053508
Experience 4, Iter 66, disc loss: 0.05459071284296525, policy loss: 3.700524747556773
Experience 4, Iter 67, disc loss: 0.05361786360650364, policy loss: 3.7725004760073775
Experience 4, Iter 68, disc loss: 0.04792067811093998, policy loss: 4.118152664632464
Experience 4, Iter 69, disc loss: 0.0523244070360504, policy loss: 3.884402550872219
Experience 4, Iter 70, disc loss: 0.05466154980435421, policy loss: 3.771235631640085
Experience 4, Iter 71, disc loss: 0.04725543848127539, policy loss: 4.0470590188065065
Experience 4, Iter 72, disc loss: 0.05203383460665778, policy loss: 3.857196346940916
Experience 4, Iter 73, disc loss: 0.04981243169901085, policy loss: 3.943052505438377
Experience 4, Iter 74, disc loss: 0.051032358118840014, policy loss: 3.89482368244025
Experience 4, Iter 75, disc loss: 0.05398237318640971, policy loss: 3.7217805610682873
Experience 4, Iter 76, disc loss: 0.05318822664554907, policy loss: 3.8439720453617516
Experience 4, Iter 77, disc loss: 0.054109916005699144, policy loss: 3.8292587044009263
Experience 4, Iter 78, disc loss: 0.052779571895840274, policy loss: 4.099466889466487
Experience 4, Iter 79, disc loss: 0.04625277916325918, policy loss: 4.241268212509121
Experience 4, Iter 80, disc loss: 0.050157062542755516, policy loss: 4.07464498599518
Experience 4, Iter 81, disc loss: 0.05058596729018208, policy loss: 3.816811016793537
Experience 4, Iter 82, disc loss: 0.04703648410503947, policy loss: 3.9901356513108377
Experience 4, Iter 83, disc loss: 0.04915881419672219, policy loss: 3.872926313369835
Experience 4, Iter 84, disc loss: 0.04372774731194369, policy loss: 4.091494066616987
Experience 4, Iter 85, disc loss: 0.04728696699827163, policy loss: 4.076340409012224
Experience 4, Iter 86, disc loss: 0.04583082787499847, policy loss: 4.02383031522041
Experience 4, Iter 87, disc loss: 0.0470542832758256, policy loss: 4.029330327067325
Experience 4, Iter 88, disc loss: 0.04658398377723744, policy loss: 4.0161200740423
Experience 4, Iter 89, disc loss: 0.04488171082056353, policy loss: 4.250085547009416
Experience 4, Iter 90, disc loss: 0.04470301722593323, policy loss: 4.053128452875508
Experience 4, Iter 91, disc loss: 0.04592657313619079, policy loss: 4.043910142915018
Experience 4, Iter 92, disc loss: 0.04551899393536338, policy loss: 4.151640492351583
Experience 4, Iter 93, disc loss: 0.044264470425829205, policy loss: 4.144990635862226
Experience 4, Iter 94, disc loss: 0.04611261003582598, policy loss: 3.9217696801368334
Experience 4, Iter 95, disc loss: 0.04573807964861758, policy loss: 4.010983738390067
Experience 4, Iter 96, disc loss: 0.04322965899453284, policy loss: 4.078150408144974
Experience 4, Iter 97, disc loss: 0.04173608452012181, policy loss: 4.163979223140417
Experience 4, Iter 98, disc loss: 0.038570880839170915, policy loss: 4.240751850909689
Experience 4, Iter 99, disc loss: 0.04313374460218157, policy loss: 4.004232169968093
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.1033],
        [0.9401],
        [0.0121]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0137, 0.1673, 0.5795, 0.0208, 0.0039, 3.0428]],

        [[0.0137, 0.1673, 0.5795, 0.0208, 0.0039, 3.0428]],

        [[0.0137, 0.1673, 0.5795, 0.0208, 0.0039, 3.0428]],

        [[0.0137, 0.1673, 0.5795, 0.0208, 0.0039, 3.0428]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0191, 0.4133, 3.7605, 0.0484], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0191, 0.4133, 3.7605, 0.0484])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.438
Iter 2/2000 - Loss: 2.502
Iter 3/2000 - Loss: 2.368
Iter 4/2000 - Loss: 2.348
Iter 5/2000 - Loss: 2.391
Iter 6/2000 - Loss: 2.340
Iter 7/2000 - Loss: 2.287
Iter 8/2000 - Loss: 2.289
Iter 9/2000 - Loss: 2.286
Iter 10/2000 - Loss: 2.243
Iter 11/2000 - Loss: 2.193
Iter 12/2000 - Loss: 2.160
Iter 13/2000 - Loss: 2.133
Iter 14/2000 - Loss: 2.088
Iter 15/2000 - Loss: 2.018
Iter 16/2000 - Loss: 1.936
Iter 17/2000 - Loss: 1.850
Iter 18/2000 - Loss: 1.756
Iter 19/2000 - Loss: 1.642
Iter 20/2000 - Loss: 1.504
Iter 1981/2000 - Loss: -6.454
Iter 1982/2000 - Loss: -6.454
Iter 1983/2000 - Loss: -6.454
Iter 1984/2000 - Loss: -6.454
Iter 1985/2000 - Loss: -6.454
Iter 1986/2000 - Loss: -6.454
Iter 1987/2000 - Loss: -6.454
Iter 1988/2000 - Loss: -6.454
Iter 1989/2000 - Loss: -6.454
Iter 1990/2000 - Loss: -6.454
Iter 1991/2000 - Loss: -6.454
Iter 1992/2000 - Loss: -6.454
Iter 1993/2000 - Loss: -6.454
Iter 1994/2000 - Loss: -6.454
Iter 1995/2000 - Loss: -6.454
Iter 1996/2000 - Loss: -6.454
Iter 1997/2000 - Loss: -6.454
Iter 1998/2000 - Loss: -6.454
Iter 1999/2000 - Loss: -6.454
Iter 2000/2000 - Loss: -6.454
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0003],
        [0.0014],
        [0.0003]])
Lengthscale: tensor([[[16.1487,  8.4456, 51.2463, 11.4674, 17.6212, 59.3711]],

        [[23.5362, 36.9279, 10.1625,  1.8088,  2.1208, 22.6412]],

        [[21.8050, 40.2051, 11.1823,  1.0422,  1.5178, 18.9146]],

        [[19.8489, 39.9660, 12.7285,  3.7489, 10.3883, 56.0091]]])
Signal Variance: tensor([ 0.1589,  1.5184, 10.7727,  0.4492])
Estimated target variance: tensor([0.0191, 0.4133, 3.7605, 0.0484])
N: 50
Signal to noise ratio: tensor([18.7387, 68.7916, 88.5338, 37.8835])
Bound on condition number: tensor([ 17558.0291, 236615.0832, 391912.9839,  71758.9432])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.02647025406715978, policy loss: 4.979623904131396
Experience 5, Iter 1, disc loss: 0.024310964994823082, policy loss: 5.242638831679113
Experience 5, Iter 2, disc loss: 0.025830607002811277, policy loss: 5.218984289117506
Experience 5, Iter 3, disc loss: 0.025266585887442117, policy loss: 5.025885792438533
Experience 5, Iter 4, disc loss: 0.02767915543573985, policy loss: 4.62294751932703
Experience 5, Iter 5, disc loss: 0.026058400008211142, policy loss: 5.052769232234328
Experience 5, Iter 6, disc loss: 0.02383549676036553, policy loss: 4.996904934686988
Experience 5, Iter 7, disc loss: 0.026950211842615802, policy loss: 4.882015220310324
Experience 5, Iter 8, disc loss: 0.02858421499355326, policy loss: 4.519728587002865
Experience 5, Iter 9, disc loss: 0.027271152902803984, policy loss: 4.47623518734604
Experience 5, Iter 10, disc loss: 0.031983734447759, policy loss: 4.3497209767394605
Experience 5, Iter 11, disc loss: 0.025367715601186676, policy loss: 4.75102546109652
Experience 5, Iter 12, disc loss: 0.02539531052072683, policy loss: 4.9123764296852475
Experience 5, Iter 13, disc loss: 0.035356635653936874, policy loss: 4.090085296571425
Experience 5, Iter 14, disc loss: 0.02992918585161982, policy loss: 4.312245952665155
Experience 5, Iter 15, disc loss: 0.029008281359972146, policy loss: 4.438982969585691
Experience 5, Iter 16, disc loss: 0.028245752094258465, policy loss: 4.474524117733859
Experience 5, Iter 17, disc loss: 0.02738756319545816, policy loss: 4.563699600631947
Experience 5, Iter 18, disc loss: 0.022622288955911646, policy loss: 4.840725728455896
Experience 5, Iter 19, disc loss: 0.029786013842529435, policy loss: 4.128856928932059
Experience 5, Iter 20, disc loss: 0.022593379369433127, policy loss: 4.826785930455811
Experience 5, Iter 21, disc loss: 0.022548786771129276, policy loss: 5.243458471420949
Experience 5, Iter 22, disc loss: 0.019782774399785055, policy loss: 5.262116321492054
Experience 5, Iter 23, disc loss: 0.023230305107912564, policy loss: 4.705670490277726
Experience 5, Iter 24, disc loss: 0.025844723207079475, policy loss: 4.467551540077156
Experience 5, Iter 25, disc loss: 0.02873340301625911, policy loss: 4.147992193970583
Experience 5, Iter 26, disc loss: 0.024674030904892265, policy loss: 4.4939159435222145
Experience 5, Iter 27, disc loss: 0.022781664315819572, policy loss: 4.4473140479917515
Experience 5, Iter 28, disc loss: 0.0202972570715075, policy loss: 4.747294454440138
Experience 5, Iter 29, disc loss: 0.020120807319571415, policy loss: 4.728926622971665
Experience 5, Iter 30, disc loss: 0.021162464321908814, policy loss: 4.535076549928917
Experience 5, Iter 31, disc loss: 0.025194627311592784, policy loss: 4.382599939289289
Experience 5, Iter 32, disc loss: 0.024861587772577926, policy loss: 4.332509422403727
Experience 5, Iter 33, disc loss: 0.021900020399079942, policy loss: 4.64090590308055
Experience 5, Iter 34, disc loss: 0.020320462247458357, policy loss: 4.70878029428787
Experience 5, Iter 35, disc loss: 0.019245138496427804, policy loss: 5.043050953382158
Experience 5, Iter 36, disc loss: 0.021899671275956503, policy loss: 4.533914028079556
Experience 5, Iter 37, disc loss: 0.02251834679152321, policy loss: 4.382900309770609
Experience 5, Iter 38, disc loss: 0.0239389191456549, policy loss: 4.2879254953310415
Experience 5, Iter 39, disc loss: 0.023343425236992482, policy loss: 4.361010850606707
Experience 5, Iter 40, disc loss: 0.022069521759186618, policy loss: 4.408810924206544
Experience 5, Iter 41, disc loss: 0.020339659238929927, policy loss: 4.629811683578497
Experience 5, Iter 42, disc loss: 0.019637768094006497, policy loss: 4.646833469565738
Experience 5, Iter 43, disc loss: 0.020615645903509006, policy loss: 4.554414263039486
Experience 5, Iter 44, disc loss: 0.02167024352599057, policy loss: 4.484705741104144
Experience 5, Iter 45, disc loss: 0.022695222316341683, policy loss: 4.35317531923027
Experience 5, Iter 46, disc loss: 0.022011820505239595, policy loss: 4.431526698823364
Experience 5, Iter 47, disc loss: 0.021670281609752148, policy loss: 4.479253619129688
Experience 5, Iter 48, disc loss: 0.02238895930181274, policy loss: 4.483769975557134
Experience 5, Iter 49, disc loss: 0.022934869179065728, policy loss: 4.39562241577016
Experience 5, Iter 50, disc loss: 0.023940130606941244, policy loss: 4.367385293862901
Experience 5, Iter 51, disc loss: 0.02507710634372434, policy loss: 4.154586893808142
Experience 5, Iter 52, disc loss: 0.021593591276783217, policy loss: 4.551939188956893
Experience 5, Iter 53, disc loss: 0.02467738504944717, policy loss: 4.275411792710699
Experience 5, Iter 54, disc loss: 0.019254666686065007, policy loss: 4.835922640332395
Experience 5, Iter 55, disc loss: 0.021613129027297798, policy loss: 4.5073647965761845
Experience 5, Iter 56, disc loss: 0.022624903793046082, policy loss: 4.3564346652543975
Experience 5, Iter 57, disc loss: 0.023498463913026683, policy loss: 4.317314459621182
Experience 5, Iter 58, disc loss: 0.021119149942700647, policy loss: 4.604656924812032
Experience 5, Iter 59, disc loss: 0.01991234819409847, policy loss: 4.65652166732686
Experience 5, Iter 60, disc loss: 0.021135569363763852, policy loss: 4.500378636551897
Experience 5, Iter 61, disc loss: 0.021710432237245718, policy loss: 4.615985421978067
Experience 5, Iter 62, disc loss: 0.020916565896130772, policy loss: 4.5581497611252715
Experience 5, Iter 63, disc loss: 0.020206821305755768, policy loss: 4.64840890536371
Experience 5, Iter 64, disc loss: 0.017815120224060087, policy loss: 4.871459221329733
Experience 5, Iter 65, disc loss: 0.01922425092840835, policy loss: 4.702933359851844
Experience 5, Iter 66, disc loss: 0.019701381839806048, policy loss: 4.599296322393284
Experience 5, Iter 67, disc loss: 0.019481907231102543, policy loss: 4.733505694844415
Experience 5, Iter 68, disc loss: 0.021489364363213935, policy loss: 4.517823468508142
Experience 5, Iter 69, disc loss: 0.021187149445172724, policy loss: 4.54320584018857
Experience 5, Iter 70, disc loss: 0.01971697175023532, policy loss: 4.634026383170618
Experience 5, Iter 71, disc loss: 0.01828796356085177, policy loss: 4.7660561944987005
Experience 5, Iter 72, disc loss: 0.01934940389429315, policy loss: 4.618810602954073
Experience 5, Iter 73, disc loss: 0.020449174180393784, policy loss: 4.520815904078454
Experience 5, Iter 74, disc loss: 0.01902168298398227, policy loss: 4.745085965494102
Experience 5, Iter 75, disc loss: 0.01944591514581939, policy loss: 4.715448713647623
Experience 5, Iter 76, disc loss: 0.018491468377787854, policy loss: 4.837325511037823
Experience 5, Iter 77, disc loss: 0.019406427362963385, policy loss: 4.665422717537804
Experience 5, Iter 78, disc loss: 0.016209256582535473, policy loss: 5.130400970135633
Experience 5, Iter 79, disc loss: 0.01877991394619241, policy loss: 4.602781016479918
Experience 5, Iter 80, disc loss: 0.016971616889045468, policy loss: 4.831874453006669
Experience 5, Iter 81, disc loss: 0.018280615306522273, policy loss: 4.649518550835951
Experience 5, Iter 82, disc loss: 0.0196339401166664, policy loss: 4.5555105075720075
Experience 5, Iter 83, disc loss: 0.016197721329756644, policy loss: 4.8927051553217265
Experience 5, Iter 84, disc loss: 0.017896431736973292, policy loss: 4.629380542431706
Experience 5, Iter 85, disc loss: 0.016062010411797188, policy loss: 4.852762304158908
Experience 5, Iter 86, disc loss: 0.016765518195467097, policy loss: 4.741782850195669
Experience 5, Iter 87, disc loss: 0.017404059967556874, policy loss: 4.692389317434706
Experience 5, Iter 88, disc loss: 0.016551356190994695, policy loss: 4.808328132364334
Experience 5, Iter 89, disc loss: 0.016640478635664226, policy loss: 4.954663484192547
Experience 5, Iter 90, disc loss: 0.01797032295685226, policy loss: 4.6141158573740935
Experience 5, Iter 91, disc loss: 0.01850528130708695, policy loss: 4.550911177800124
Experience 5, Iter 92, disc loss: 0.017193827648023748, policy loss: 4.755614406549046
Experience 5, Iter 93, disc loss: 0.017494571826314677, policy loss: 4.627438036619411
Experience 5, Iter 94, disc loss: 0.016235209374308314, policy loss: 4.9217955656986545
Experience 5, Iter 95, disc loss: 0.015638782989824684, policy loss: 4.989623735764194
Experience 5, Iter 96, disc loss: 0.01583115549073838, policy loss: 4.761613359998627
Experience 5, Iter 97, disc loss: 0.01584533719638022, policy loss: 4.80219060951239
Experience 5, Iter 98, disc loss: 0.016009062661872166, policy loss: 4.927753865643986
Experience 5, Iter 99, disc loss: 0.01596077272841114, policy loss: 4.700387514189789
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.1104],
        [0.9669],
        [0.0119]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0116, 0.1494, 0.5760, 0.0209, 0.0038, 3.2665]],

        [[0.0116, 0.1494, 0.5760, 0.0209, 0.0038, 3.2665]],

        [[0.0116, 0.1494, 0.5760, 0.0209, 0.0038, 3.2665]],

        [[0.0116, 0.1494, 0.5760, 0.0209, 0.0038, 3.2665]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0169, 0.4415, 3.8676, 0.0475], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0169, 0.4415, 3.8676, 0.0475])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.424
Iter 2/2000 - Loss: 2.515
Iter 3/2000 - Loss: 2.362
Iter 4/2000 - Loss: 2.348
Iter 5/2000 - Loss: 2.399
Iter 6/2000 - Loss: 2.346
Iter 7/2000 - Loss: 2.287
Iter 8/2000 - Loss: 2.289
Iter 9/2000 - Loss: 2.296
Iter 10/2000 - Loss: 2.257
Iter 11/2000 - Loss: 2.203
Iter 12/2000 - Loss: 2.169
Iter 13/2000 - Loss: 2.145
Iter 14/2000 - Loss: 2.102
Iter 15/2000 - Loss: 2.032
Iter 16/2000 - Loss: 1.946
Iter 17/2000 - Loss: 1.856
Iter 18/2000 - Loss: 1.758
Iter 19/2000 - Loss: 1.640
Iter 20/2000 - Loss: 1.496
Iter 1981/2000 - Loss: -6.680
Iter 1982/2000 - Loss: -6.680
Iter 1983/2000 - Loss: -6.680
Iter 1984/2000 - Loss: -6.680
Iter 1985/2000 - Loss: -6.680
Iter 1986/2000 - Loss: -6.680
Iter 1987/2000 - Loss: -6.680
Iter 1988/2000 - Loss: -6.680
Iter 1989/2000 - Loss: -6.680
Iter 1990/2000 - Loss: -6.680
Iter 1991/2000 - Loss: -6.680
Iter 1992/2000 - Loss: -6.680
Iter 1993/2000 - Loss: -6.680
Iter 1994/2000 - Loss: -6.680
Iter 1995/2000 - Loss: -6.680
Iter 1996/2000 - Loss: -6.681
Iter 1997/2000 - Loss: -6.681
Iter 1998/2000 - Loss: -6.681
Iter 1999/2000 - Loss: -6.681
Iter 2000/2000 - Loss: -6.681
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[15.5223,  8.6761, 48.3453, 13.2341, 17.2022, 60.4681]],

        [[24.0966, 45.2086, 10.4415,  1.2704, 10.0801, 26.7247]],

        [[23.9189, 49.3597, 10.6147,  0.9118,  1.8841, 21.4759]],

        [[17.3279, 35.9333, 10.9714,  3.7018, 11.3052, 46.9559]]])
Signal Variance: tensor([ 0.1514,  1.9792, 14.1536,  0.3438])
Estimated target variance: tensor([0.0169, 0.4415, 3.8676, 0.0475])
N: 60
Signal to noise ratio: tensor([19.2817, 83.9757, 89.6825, 31.6900])
Bound on condition number: tensor([ 22308.1150, 423116.5103, 482578.2394,  60256.4155])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.015404355870693207, policy loss: 4.756390715127309
Experience 6, Iter 1, disc loss: 0.014589439734441389, policy loss: 4.952190227846949
Experience 6, Iter 2, disc loss: 0.015683437212683865, policy loss: 4.712075415863062
Experience 6, Iter 3, disc loss: 0.01634684269138721, policy loss: 4.635261282280505
Experience 6, Iter 4, disc loss: 0.016542450588273666, policy loss: 4.687679070921197
Experience 6, Iter 5, disc loss: 0.015953079064819804, policy loss: 4.7492957330131595
Experience 6, Iter 6, disc loss: 0.015749862244165108, policy loss: 4.851098190702734
Experience 6, Iter 7, disc loss: 0.015627651988388308, policy loss: 4.81845800644974
Experience 6, Iter 8, disc loss: 0.013187088397379173, policy loss: 5.247172191813187
Experience 6, Iter 9, disc loss: 0.014049305824705753, policy loss: 5.032355981552321
Experience 6, Iter 10, disc loss: 0.015106721623208846, policy loss: 4.892664617010775
Experience 6, Iter 11, disc loss: 0.015051949867712511, policy loss: 4.844415880552646
Experience 6, Iter 12, disc loss: 0.014305494175860665, policy loss: 4.926824642652524
Experience 6, Iter 13, disc loss: 0.013361239031005556, policy loss: 5.090693589344519
Experience 6, Iter 14, disc loss: 0.01340672117145789, policy loss: 4.951405370780716
Experience 6, Iter 15, disc loss: 0.01388288124701156, policy loss: 4.8075358506283195
Experience 6, Iter 16, disc loss: 0.01370976076067905, policy loss: 4.8771182247923495
Experience 6, Iter 17, disc loss: 0.014435280284465741, policy loss: 4.744124005434179
Experience 6, Iter 18, disc loss: 0.013305734108174927, policy loss: 4.905690852997882
Experience 6, Iter 19, disc loss: 0.012676776346621298, policy loss: 5.204680868217109
Experience 6, Iter 20, disc loss: 0.01320223046800274, policy loss: 4.974546247616616
Experience 6, Iter 21, disc loss: 0.014099502213834419, policy loss: 4.762145819532677
Experience 6, Iter 22, disc loss: 0.014001636191609757, policy loss: 4.827379041526432
Experience 6, Iter 23, disc loss: 0.013483838877823144, policy loss: 4.888904874529934
Experience 6, Iter 24, disc loss: 0.01399429422821479, policy loss: 4.7908151084347335
Experience 6, Iter 25, disc loss: 0.013931268411644632, policy loss: 4.762661435669146
Experience 6, Iter 26, disc loss: 0.013479676576636866, policy loss: 4.7809829869934655
Experience 6, Iter 27, disc loss: 0.013846798606256906, policy loss: 4.754837942571227
Experience 6, Iter 28, disc loss: 0.013395154252172616, policy loss: 4.881814180536077
Experience 6, Iter 29, disc loss: 0.012573944448022371, policy loss: 5.042257695948461
Experience 6, Iter 30, disc loss: 0.012938081648412918, policy loss: 4.857831205298225
Experience 6, Iter 31, disc loss: 0.012691058492157355, policy loss: 4.95289781905727
Experience 6, Iter 32, disc loss: 0.013745295432086548, policy loss: 4.8434414723872194
Experience 6, Iter 33, disc loss: 0.012793545975883473, policy loss: 4.997817568479066
Experience 6, Iter 34, disc loss: 0.01267505544199237, policy loss: 5.1257093336021216
Experience 6, Iter 35, disc loss: 0.01272465359919017, policy loss: 5.033046487832314
Experience 6, Iter 36, disc loss: 0.012296328618068689, policy loss: 5.034816099419579
Experience 6, Iter 37, disc loss: 0.013658667447720449, policy loss: 4.7868959241803655
Experience 6, Iter 38, disc loss: 0.013470118014351048, policy loss: 4.857045406895486
Experience 6, Iter 39, disc loss: 0.012559603552233999, policy loss: 4.939555279604518
Experience 6, Iter 40, disc loss: 0.013328537493519067, policy loss: 4.826710704904154
Experience 6, Iter 41, disc loss: 0.012072829344141143, policy loss: 5.115645819689583
Experience 6, Iter 42, disc loss: 0.012498003003652555, policy loss: 4.953859844976703
Experience 6, Iter 43, disc loss: 0.011514245100142893, policy loss: 5.238987917890193
Experience 6, Iter 44, disc loss: 0.012002855235161026, policy loss: 5.007815373186833
Experience 6, Iter 45, disc loss: 0.012390730918542847, policy loss: 5.00085128835731
Experience 6, Iter 46, disc loss: 0.01156909220458932, policy loss: 5.042443063948751
Experience 6, Iter 47, disc loss: 0.011614391346223676, policy loss: 5.182698713005926
Experience 6, Iter 48, disc loss: 0.011730502851942267, policy loss: 5.113738334904613
Experience 6, Iter 49, disc loss: 0.011628274822592797, policy loss: 5.170231866239616
Experience 6, Iter 50, disc loss: 0.011253520177515722, policy loss: 5.02465038847072
Experience 6, Iter 51, disc loss: 0.01081472716875161, policy loss: 5.200044601913548
Experience 6, Iter 52, disc loss: 0.011511459985255771, policy loss: 4.990652743234013
Experience 6, Iter 53, disc loss: 0.012038575245846981, policy loss: 5.031611883092044
Experience 6, Iter 54, disc loss: 0.011007638066725615, policy loss: 5.291184957512904
Experience 6, Iter 55, disc loss: 0.010914235997249276, policy loss: 5.144813354226447
Experience 6, Iter 56, disc loss: 0.011191901243626155, policy loss: 5.108834343826038
Experience 6, Iter 57, disc loss: 0.011831953853972047, policy loss: 4.953325249027303
Experience 6, Iter 58, disc loss: 0.010272806160459124, policy loss: 5.222816241353122
Experience 6, Iter 59, disc loss: 0.01008549310676168, policy loss: 5.356347320982273
Experience 6, Iter 60, disc loss: 0.01025493525780621, policy loss: 5.29430700925491
Experience 6, Iter 61, disc loss: 0.010785848567624247, policy loss: 5.143210746995733
Experience 6, Iter 62, disc loss: 0.010652867519929145, policy loss: 5.191744483714093
Experience 6, Iter 63, disc loss: 0.010514205987729036, policy loss: 5.021817505764684
Experience 6, Iter 64, disc loss: 0.010145359878200507, policy loss: 5.182915879635631
Experience 6, Iter 65, disc loss: 0.009966439403922504, policy loss: 5.293778836172079
Experience 6, Iter 66, disc loss: 0.010429026859101364, policy loss: 5.251915066358171
Experience 6, Iter 67, disc loss: 0.010826815448466158, policy loss: 4.987434138193033
Experience 6, Iter 68, disc loss: 0.010527211075418983, policy loss: 5.15004829069035
Experience 6, Iter 69, disc loss: 0.010391510524395658, policy loss: 5.120230541606452
Experience 6, Iter 70, disc loss: 0.010196886210888443, policy loss: 5.197182164596796
Experience 6, Iter 71, disc loss: 0.010669784909111666, policy loss: 5.151604980016253
Experience 6, Iter 72, disc loss: 0.010779789348485388, policy loss: 5.177085905582697
Experience 6, Iter 73, disc loss: 0.010421754194741995, policy loss: 5.063830306160222
Experience 6, Iter 74, disc loss: 0.010283870838696307, policy loss: 5.0651168852918165
Experience 6, Iter 75, disc loss: 0.009049381851512685, policy loss: 5.418228291620267
Experience 6, Iter 76, disc loss: 0.009487613544714057, policy loss: 5.166305960867291
Experience 6, Iter 77, disc loss: 0.008981880217096592, policy loss: 5.250154318832286
Experience 6, Iter 78, disc loss: 0.00908974283929737, policy loss: 5.209938974901611
Experience 6, Iter 79, disc loss: 0.009569790617287056, policy loss: 5.179238580576952
Experience 6, Iter 80, disc loss: 0.009182730466532044, policy loss: 5.190797111125942
Experience 6, Iter 81, disc loss: 0.009231660418538006, policy loss: 5.31438466888706
Experience 6, Iter 82, disc loss: 0.009379682985907311, policy loss: 5.320325757964197
Experience 6, Iter 83, disc loss: 0.009418859912898382, policy loss: 5.23537914038366
Experience 6, Iter 84, disc loss: 0.010239584214597789, policy loss: 5.2115369732836045
Experience 6, Iter 85, disc loss: 0.009728173715674126, policy loss: 5.183485297903676
Experience 6, Iter 86, disc loss: 0.009903336093287127, policy loss: 5.119290491919009
Experience 6, Iter 87, disc loss: 0.009205476433793177, policy loss: 5.425738830390726
Experience 6, Iter 88, disc loss: 0.00857751617986375, policy loss: 5.537159899391854
Experience 6, Iter 89, disc loss: 0.00903544751445382, policy loss: 5.358287100925362
Experience 6, Iter 90, disc loss: 0.00993772763436701, policy loss: 5.106353704329052
Experience 6, Iter 91, disc loss: 0.00916349435085527, policy loss: 5.340884142664391
Experience 6, Iter 92, disc loss: 0.008963003945441524, policy loss: 5.35187249157583
Experience 6, Iter 93, disc loss: 0.009514696956214957, policy loss: 5.151361994138299
Experience 6, Iter 94, disc loss: 0.009335788845563324, policy loss: 5.188282478269399
Experience 6, Iter 95, disc loss: 0.009215535540911205, policy loss: 5.232886395635877
Experience 6, Iter 96, disc loss: 0.00873394216882849, policy loss: 5.304433769841706
Experience 6, Iter 97, disc loss: 0.009552897082769497, policy loss: 5.283452517891546
Experience 6, Iter 98, disc loss: 0.009323445088991586, policy loss: 5.1457998669739915
Experience 6, Iter 99, disc loss: 0.009224103601987763, policy loss: 5.244938256084163
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1089],
        [0.9512],
        [0.0118]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0105, 0.1362, 0.5693, 0.0208, 0.0036, 3.2460]],

        [[0.0105, 0.1362, 0.5693, 0.0208, 0.0036, 3.2460]],

        [[0.0105, 0.1362, 0.5693, 0.0208, 0.0036, 3.2460]],

        [[0.0105, 0.1362, 0.5693, 0.0208, 0.0036, 3.2460]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.4357, 3.8046, 0.0474], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.4357, 3.8046, 0.0474])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.372
Iter 2/2000 - Loss: 2.493
Iter 3/2000 - Loss: 2.317
Iter 4/2000 - Loss: 2.308
Iter 5/2000 - Loss: 2.366
Iter 6/2000 - Loss: 2.316
Iter 7/2000 - Loss: 2.241
Iter 8/2000 - Loss: 2.228
Iter 9/2000 - Loss: 2.243
Iter 10/2000 - Loss: 2.218
Iter 11/2000 - Loss: 2.156
Iter 12/2000 - Loss: 2.101
Iter 13/2000 - Loss: 2.067
Iter 14/2000 - Loss: 2.030
Iter 15/2000 - Loss: 1.964
Iter 16/2000 - Loss: 1.870
Iter 17/2000 - Loss: 1.763
Iter 18/2000 - Loss: 1.650
Iter 19/2000 - Loss: 1.524
Iter 20/2000 - Loss: 1.371
Iter 1981/2000 - Loss: -7.028
Iter 1982/2000 - Loss: -7.028
Iter 1983/2000 - Loss: -7.028
Iter 1984/2000 - Loss: -7.028
Iter 1985/2000 - Loss: -7.028
Iter 1986/2000 - Loss: -7.028
Iter 1987/2000 - Loss: -7.028
Iter 1988/2000 - Loss: -7.028
Iter 1989/2000 - Loss: -7.028
Iter 1990/2000 - Loss: -7.028
Iter 1991/2000 - Loss: -7.028
Iter 1992/2000 - Loss: -7.028
Iter 1993/2000 - Loss: -7.028
Iter 1994/2000 - Loss: -7.028
Iter 1995/2000 - Loss: -7.028
Iter 1996/2000 - Loss: -7.028
Iter 1997/2000 - Loss: -7.028
Iter 1998/2000 - Loss: -7.028
Iter 1999/2000 - Loss: -7.028
Iter 2000/2000 - Loss: -7.029
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[14.4756,  8.5681, 47.0353, 12.5195, 16.5444, 57.0300]],

        [[21.5568, 43.1097, 10.4067,  1.2878, 10.6151, 27.2060]],

        [[22.6612, 46.6743, 10.6437,  0.9210,  1.9068, 22.2285]],

        [[15.7572, 34.9850, 10.8693,  3.4118,  6.8882, 46.3291]]])
Signal Variance: tensor([ 0.1399,  2.0936, 14.5298,  0.3177])
Estimated target variance: tensor([0.0155, 0.4357, 3.8046, 0.0474])
N: 70
Signal to noise ratio: tensor([19.5582, 78.2108, 97.6586, 31.5996])
Bound on condition number: tensor([ 26777.5349, 428186.4591, 667604.8193,  69898.2505])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.01005091828594285, policy loss: 5.233792397751509
Experience 7, Iter 1, disc loss: 0.00990096435751956, policy loss: 5.099249994796906
Experience 7, Iter 2, disc loss: 0.009056771302031318, policy loss: 5.309354594050596
Experience 7, Iter 3, disc loss: 0.009832995453813883, policy loss: 5.106108350433828
Experience 7, Iter 4, disc loss: 0.008614877005798122, policy loss: 5.439325259312656
Experience 7, Iter 5, disc loss: 0.009371256450465104, policy loss: 5.260208985447878
Experience 7, Iter 6, disc loss: 0.009224979853219817, policy loss: 5.237814765103796
Experience 7, Iter 7, disc loss: 0.007201141086775125, policy loss: 5.8242977188865
Experience 7, Iter 8, disc loss: 0.008388107111281632, policy loss: 5.543176997472846
Experience 7, Iter 9, disc loss: 0.009360379088747206, policy loss: 5.222995819122635
Experience 7, Iter 10, disc loss: 0.009187101556724336, policy loss: 5.501282827391048
Experience 7, Iter 11, disc loss: 0.007812492183487201, policy loss: 5.611826153875653
Experience 7, Iter 12, disc loss: 0.008899820146522828, policy loss: 5.341168147190911
Experience 7, Iter 13, disc loss: 0.008913224167775895, policy loss: 5.224227137834723
Experience 7, Iter 14, disc loss: 0.0094599439188504, policy loss: 5.115306323737094
Experience 7, Iter 15, disc loss: 0.008775663571032497, policy loss: 5.360103031539049
Experience 7, Iter 16, disc loss: 0.008171809695669913, policy loss: 5.510883083649204
Experience 7, Iter 17, disc loss: 0.008674671965096723, policy loss: 5.296646301523285
Experience 7, Iter 18, disc loss: 0.007677939696690198, policy loss: 5.642567855361703
Experience 7, Iter 19, disc loss: 0.008270210936246278, policy loss: 5.331779232092881
Experience 7, Iter 20, disc loss: 0.00875069720980775, policy loss: 5.215152803396466
Experience 7, Iter 21, disc loss: 0.007563987297561445, policy loss: 5.605897807215991
Experience 7, Iter 22, disc loss: 0.008396847096994262, policy loss: 5.421438834170403
Experience 7, Iter 23, disc loss: 0.008802725039171875, policy loss: 5.258671157421584
Experience 7, Iter 24, disc loss: 0.007762153401080828, policy loss: 5.486467940617489
Experience 7, Iter 25, disc loss: 0.007721758317497489, policy loss: 5.45815396305853
Experience 7, Iter 26, disc loss: 0.007381530828470033, policy loss: 5.558011217429561
Experience 7, Iter 27, disc loss: 0.007646710083798585, policy loss: 5.508792555834231
Experience 7, Iter 28, disc loss: 0.007436955902670416, policy loss: 5.668484754831211
Experience 7, Iter 29, disc loss: 0.007387187152418177, policy loss: 5.514864724348461
Experience 7, Iter 30, disc loss: 0.0076046897094401295, policy loss: 5.527185999300747
Experience 7, Iter 31, disc loss: 0.007902279293141544, policy loss: 5.449450622252766
Experience 7, Iter 32, disc loss: 0.0071971806377126215, policy loss: 5.648517815438593
Experience 7, Iter 33, disc loss: 0.007712923091614232, policy loss: 5.549498575701638
Experience 7, Iter 34, disc loss: 0.006916874462267917, policy loss: 5.6698767513215245
Experience 7, Iter 35, disc loss: 0.007035483982527648, policy loss: 5.55132475577069
Experience 7, Iter 36, disc loss: 0.007575789755213249, policy loss: 5.500150455409283
Experience 7, Iter 37, disc loss: 0.007864128176723406, policy loss: 5.335674834187442
Experience 7, Iter 38, disc loss: 0.007230652165273346, policy loss: 5.580904612770953
Experience 7, Iter 39, disc loss: 0.007365758954577296, policy loss: 5.5664506650322805
Experience 7, Iter 40, disc loss: 0.0066951546686415874, policy loss: 5.850605829594148
Experience 7, Iter 41, disc loss: 0.007854002769720599, policy loss: 5.4390307562392115
Experience 7, Iter 42, disc loss: 0.007761908282489597, policy loss: 5.391488045884446
Experience 7, Iter 43, disc loss: 0.006761663460049694, policy loss: 5.564608583271348
Experience 7, Iter 44, disc loss: 0.006753074978648238, policy loss: 5.53466621398035
Experience 7, Iter 45, disc loss: 0.006727453483010126, policy loss: 5.568713545312555
Experience 7, Iter 46, disc loss: 0.00701263328671722, policy loss: 5.5513388521715665
Experience 7, Iter 47, disc loss: 0.00691109977682388, policy loss: 5.558820401875904
Experience 7, Iter 48, disc loss: 0.007142756286989136, policy loss: 5.48698162314278
Experience 7, Iter 49, disc loss: 0.00757086973636804, policy loss: 5.48827353211218
Experience 7, Iter 50, disc loss: 0.00796828231460139, policy loss: 5.375559490860356
Experience 7, Iter 51, disc loss: 0.006966387356487169, policy loss: 5.592264553037336
Experience 7, Iter 52, disc loss: 0.007400472782549725, policy loss: 5.563656279696131
Experience 7, Iter 53, disc loss: 0.006983623982949826, policy loss: 5.562740191109038
Experience 7, Iter 54, disc loss: 0.007158885650839683, policy loss: 5.663636143627988
Experience 7, Iter 55, disc loss: 0.007267349740273996, policy loss: 5.360696210253934
Experience 7, Iter 56, disc loss: 0.006485838001755888, policy loss: 5.719484289094719
Experience 7, Iter 57, disc loss: 0.006869614119775101, policy loss: 5.590194708954725
Experience 7, Iter 58, disc loss: 0.006775371538823529, policy loss: 5.633185521772255
Experience 7, Iter 59, disc loss: 0.007258535720958225, policy loss: 5.405106337343787
Experience 7, Iter 60, disc loss: 0.0072429854788060525, policy loss: 5.535817116018652
Experience 7, Iter 61, disc loss: 0.007419667063098975, policy loss: 5.361693941887744
Experience 7, Iter 62, disc loss: 0.007427667891868659, policy loss: 5.316384811822443
Experience 7, Iter 63, disc loss: 0.006751379888101438, policy loss: 5.725321821007668
Experience 7, Iter 64, disc loss: 0.007171167100601707, policy loss: 5.461277143186544
Experience 7, Iter 65, disc loss: 0.0068426419857717715, policy loss: 5.617325034299684
Experience 7, Iter 66, disc loss: 0.006771554975598683, policy loss: 5.623738598766468
Experience 7, Iter 67, disc loss: 0.006534971993952669, policy loss: 5.799121495194406
Experience 7, Iter 68, disc loss: 0.005999442927094438, policy loss: 5.868998827852641
Experience 7, Iter 69, disc loss: 0.006347579251266579, policy loss: 5.5618480448748775
Experience 7, Iter 70, disc loss: 0.006632883701078807, policy loss: 5.6235688307757705
Experience 7, Iter 71, disc loss: 0.0060479819441322654, policy loss: 5.901520572682253
Experience 7, Iter 72, disc loss: 0.0066330233520378815, policy loss: 5.490285030699951
Experience 7, Iter 73, disc loss: 0.006262302683293019, policy loss: 5.637853799652337
Experience 7, Iter 74, disc loss: 0.006325179946423101, policy loss: 5.644973042422103
Experience 7, Iter 75, disc loss: 0.006720777518971538, policy loss: 5.506970446982434
Experience 7, Iter 76, disc loss: 0.006518938179601733, policy loss: 5.585511146133245
Experience 7, Iter 77, disc loss: 0.006637364427721528, policy loss: 5.5605218398543705
Experience 7, Iter 78, disc loss: 0.006563344569843857, policy loss: 5.715255844234214
Experience 7, Iter 79, disc loss: 0.006849695861539828, policy loss: 5.447082806279398
Experience 7, Iter 80, disc loss: 0.005842920951571809, policy loss: 5.78793097189515
Experience 7, Iter 81, disc loss: 0.006121572521686847, policy loss: 5.825375077162324
Experience 7, Iter 82, disc loss: 0.006531247610306849, policy loss: 5.583668451570873
Experience 7, Iter 83, disc loss: 0.006858405771588292, policy loss: 5.505222334077883
Experience 7, Iter 84, disc loss: 0.0060824258236300294, policy loss: 5.650504760330261
Experience 7, Iter 85, disc loss: 0.0063135117339188015, policy loss: 5.707983700313336
Experience 7, Iter 86, disc loss: 0.006539331336207337, policy loss: 5.612732313592333
Experience 7, Iter 87, disc loss: 0.006382351222550384, policy loss: 5.574154516759608
Experience 7, Iter 88, disc loss: 0.006769025116559023, policy loss: 5.673966846959467
Experience 7, Iter 89, disc loss: 0.006101542798382089, policy loss: 5.740996923525868
Experience 7, Iter 90, disc loss: 0.0057146019305676695, policy loss: 5.754104011897228
Experience 7, Iter 91, disc loss: 0.00573040707394016, policy loss: 5.704716546288054
Experience 7, Iter 92, disc loss: 0.005600102271114099, policy loss: 5.7706952516438825
Experience 7, Iter 93, disc loss: 0.0060658940312925845, policy loss: 5.6817472984673705
Experience 7, Iter 94, disc loss: 0.005691477443469699, policy loss: 5.754654717738977
Experience 7, Iter 95, disc loss: 0.00564329715226834, policy loss: 5.747488672565056
Experience 7, Iter 96, disc loss: 0.0058448757586023765, policy loss: 5.734995273807923
Experience 7, Iter 97, disc loss: 0.005727751527633941, policy loss: 5.729142789748438
Experience 7, Iter 98, disc loss: 0.005504425424447307, policy loss: 5.843645309155377
Experience 7, Iter 99, disc loss: 0.005808144397496348, policy loss: 5.649815967424306
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.1122],
        [0.9560],
        [0.0113]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0093, 0.1317, 0.5544, 0.0204, 0.0034, 3.3560]],

        [[0.0093, 0.1317, 0.5544, 0.0204, 0.0034, 3.3560]],

        [[0.0093, 0.1317, 0.5544, 0.0204, 0.0034, 3.3560]],

        [[0.0093, 0.1317, 0.5544, 0.0204, 0.0034, 3.3560]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0152, 0.4488, 3.8240, 0.0453], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0152, 0.4488, 3.8240, 0.0453])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.365
Iter 2/2000 - Loss: 2.520
Iter 3/2000 - Loss: 2.321
Iter 4/2000 - Loss: 2.319
Iter 5/2000 - Loss: 2.384
Iter 6/2000 - Loss: 2.344
Iter 7/2000 - Loss: 2.260
Iter 8/2000 - Loss: 2.225
Iter 9/2000 - Loss: 2.241
Iter 10/2000 - Loss: 2.240
Iter 11/2000 - Loss: 2.190
Iter 12/2000 - Loss: 2.118
Iter 13/2000 - Loss: 2.062
Iter 14/2000 - Loss: 2.024
Iter 15/2000 - Loss: 1.974
Iter 16/2000 - Loss: 1.893
Iter 17/2000 - Loss: 1.782
Iter 18/2000 - Loss: 1.661
Iter 19/2000 - Loss: 1.532
Iter 20/2000 - Loss: 1.390
Iter 1981/2000 - Loss: -7.235
Iter 1982/2000 - Loss: -7.235
Iter 1983/2000 - Loss: -7.235
Iter 1984/2000 - Loss: -7.235
Iter 1985/2000 - Loss: -7.236
Iter 1986/2000 - Loss: -7.236
Iter 1987/2000 - Loss: -7.236
Iter 1988/2000 - Loss: -7.236
Iter 1989/2000 - Loss: -7.236
Iter 1990/2000 - Loss: -7.236
Iter 1991/2000 - Loss: -7.236
Iter 1992/2000 - Loss: -7.236
Iter 1993/2000 - Loss: -7.236
Iter 1994/2000 - Loss: -7.236
Iter 1995/2000 - Loss: -7.236
Iter 1996/2000 - Loss: -7.236
Iter 1997/2000 - Loss: -7.236
Iter 1998/2000 - Loss: -7.236
Iter 1999/2000 - Loss: -7.236
Iter 2000/2000 - Loss: -7.236
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[14.8077,  8.5569, 46.1869, 11.2751, 15.3596, 57.0648]],

        [[21.6355, 42.2260, 10.2792,  1.3293,  9.4837, 28.4972]],

        [[21.9884, 45.0209, 10.3349,  0.9644,  2.1558, 21.5176]],

        [[15.7345, 39.3901, 14.6015,  3.1832,  0.8719, 47.5828]]])
Signal Variance: tensor([ 0.1386,  2.4561, 15.5687,  0.3520])
Estimated target variance: tensor([0.0152, 0.4488, 3.8240, 0.0453])
N: 80
Signal to noise ratio: tensor([ 19.4109,  87.1860, 100.3438,  36.5763])
Bound on condition number: tensor([ 30143.7380, 608113.2144, 805511.1906, 107027.1996])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.00546118626506766, policy loss: 5.70329392175725
Experience 8, Iter 1, disc loss: 0.00612745469893255, policy loss: 5.599107622259193
Experience 8, Iter 2, disc loss: 0.005949395511088635, policy loss: 5.746393787469578
Experience 8, Iter 3, disc loss: 0.005576098291066586, policy loss: 5.785972204945699
Experience 8, Iter 4, disc loss: 0.005949067990311973, policy loss: 5.754664961820153
Experience 8, Iter 5, disc loss: 0.005627443386498176, policy loss: 5.740868514626638
Experience 8, Iter 6, disc loss: 0.005612598917998358, policy loss: 5.693796319497427
Experience 8, Iter 7, disc loss: 0.005800238087858713, policy loss: 5.676597351010475
Experience 8, Iter 8, disc loss: 0.005582965238853637, policy loss: 5.744689303284771
Experience 8, Iter 9, disc loss: 0.005404954005223048, policy loss: 5.912082484037178
Experience 8, Iter 10, disc loss: 0.005624039735801339, policy loss: 5.7165227765778495
Experience 8, Iter 11, disc loss: 0.00585412287183691, policy loss: 5.6659175190919555
Experience 8, Iter 12, disc loss: 0.006118416476244865, policy loss: 5.574975049953869
Experience 8, Iter 13, disc loss: 0.005895655301496608, policy loss: 5.7373567326318575
Experience 8, Iter 14, disc loss: 0.005419554848078896, policy loss: 5.692816764670399
Experience 8, Iter 15, disc loss: 0.005182307041379147, policy loss: 5.888545951601851
Experience 8, Iter 16, disc loss: 0.005436850182120073, policy loss: 5.717615591859991
Experience 8, Iter 17, disc loss: 0.006275447113986206, policy loss: 5.534728617235777
Experience 8, Iter 18, disc loss: 0.006186721581760149, policy loss: 5.648160540205231
Experience 8, Iter 19, disc loss: 0.005919571917732686, policy loss: 5.731431127909357
Experience 8, Iter 20, disc loss: 0.0058019952640354, policy loss: 5.600603510874464
Experience 8, Iter 21, disc loss: 0.005882820468379117, policy loss: 5.595504883013293
Experience 8, Iter 22, disc loss: 0.005715015065492823, policy loss: 5.644350884852113
Experience 8, Iter 23, disc loss: 0.005504834312424271, policy loss: 5.925885263635957
Experience 8, Iter 24, disc loss: 0.005712093990341427, policy loss: 5.978206151011955
Experience 8, Iter 25, disc loss: 0.005160819434713595, policy loss: 5.8808772428431055
Experience 8, Iter 26, disc loss: 0.0054286364540334, policy loss: 5.812152799298039
Experience 8, Iter 27, disc loss: 0.005163979148312625, policy loss: 5.8796275951831465
Experience 8, Iter 28, disc loss: 0.005011515089184591, policy loss: 5.903991966612065
Experience 8, Iter 29, disc loss: 0.0050741721660267775, policy loss: 5.896899051917417
Experience 8, Iter 30, disc loss: 0.005671936894683134, policy loss: 5.761817536144989
Experience 8, Iter 31, disc loss: 0.005523306660506076, policy loss: 5.691128663670285
Experience 8, Iter 32, disc loss: 0.005649768002970951, policy loss: 5.667709324889807
Experience 8, Iter 33, disc loss: 0.0056068048941065925, policy loss: 5.730843481929549
Experience 8, Iter 34, disc loss: 0.0054282262739882205, policy loss: 5.89106982845176
Experience 8, Iter 35, disc loss: 0.005874812884418385, policy loss: 5.792902868046271
Experience 8, Iter 36, disc loss: 0.005282237746085126, policy loss: 6.036956377240184
Experience 8, Iter 37, disc loss: 0.005376920645163366, policy loss: 5.9165470463879855
Experience 8, Iter 38, disc loss: 0.0050992012126755285, policy loss: 5.886609749158774
Experience 8, Iter 39, disc loss: 0.004942277922350177, policy loss: 6.028396924981235
Experience 8, Iter 40, disc loss: 0.005002141602937954, policy loss: 6.004755501037234
Experience 8, Iter 41, disc loss: 0.00536073430959123, policy loss: 5.843229958975634
Experience 8, Iter 42, disc loss: 0.005072010999904584, policy loss: 5.780260438494935
Experience 8, Iter 43, disc loss: 0.005075597418202855, policy loss: 5.815966820380402
Experience 8, Iter 44, disc loss: 0.005365398053823691, policy loss: 5.768573687487732
Experience 8, Iter 45, disc loss: 0.005765197205220589, policy loss: 5.66564190922102
Experience 8, Iter 46, disc loss: 0.005036094937380633, policy loss: 5.938535838909776
Experience 8, Iter 47, disc loss: 0.005379561535987612, policy loss: 5.8299447332946315
Experience 8, Iter 48, disc loss: 0.005617028298443954, policy loss: 5.643426218339162
Experience 8, Iter 49, disc loss: 0.0051685207711553716, policy loss: 6.057325532452231
Experience 8, Iter 50, disc loss: 0.0056074390087814275, policy loss: 5.698043907432274
Experience 8, Iter 51, disc loss: 0.0051804211433349725, policy loss: 5.889629244632847
Experience 8, Iter 52, disc loss: 0.005493136483718674, policy loss: 5.743956333810388
Experience 8, Iter 53, disc loss: 0.005412683447144794, policy loss: 5.809290456830621
Experience 8, Iter 54, disc loss: 0.004988405731536319, policy loss: 5.790849560564506
Experience 8, Iter 55, disc loss: 0.005372221007596433, policy loss: 5.736460157072002
Experience 8, Iter 56, disc loss: 0.0049323870772407215, policy loss: 5.919126334921524
Experience 8, Iter 57, disc loss: 0.005001539104389659, policy loss: 5.93260565853562
Experience 8, Iter 58, disc loss: 0.0062292709775528705, policy loss: 5.564300663083777
Experience 8, Iter 59, disc loss: 0.005185600290635708, policy loss: 5.843657515265378
Experience 8, Iter 60, disc loss: 0.005071557512063186, policy loss: 5.798194174531167
Experience 8, Iter 61, disc loss: 0.005604504283960057, policy loss: 5.772764041317987
Experience 8, Iter 62, disc loss: 0.004676727705220342, policy loss: 6.137971645455223
Experience 8, Iter 63, disc loss: 0.0055260077462567815, policy loss: 5.658349181882229
Experience 8, Iter 64, disc loss: 0.005204164288089766, policy loss: 5.740769386010253
Experience 8, Iter 65, disc loss: 0.005162901823171695, policy loss: 5.808617564538956
Experience 8, Iter 66, disc loss: 0.004893418092449453, policy loss: 6.037231986773298
Experience 8, Iter 67, disc loss: 0.0049291885634533605, policy loss: 5.962860181397175
Experience 8, Iter 68, disc loss: 0.005467790793581015, policy loss: 5.802956520304146
Experience 8, Iter 69, disc loss: 0.005241549265041391, policy loss: 5.966367416294018
Experience 8, Iter 70, disc loss: 0.0047193687285078485, policy loss: 6.007832146375968
Experience 8, Iter 71, disc loss: 0.004984965252518411, policy loss: 5.957732992513417
Experience 8, Iter 72, disc loss: 0.004117613604715531, policy loss: 6.238998349743087
Experience 8, Iter 73, disc loss: 0.004274094848944186, policy loss: 6.115040983886649
Experience 8, Iter 74, disc loss: 0.004218093208525028, policy loss: 6.094396313617443
Experience 8, Iter 75, disc loss: 0.004100303838034542, policy loss: 6.208582955788369
Experience 8, Iter 76, disc loss: 0.004389187191794764, policy loss: 6.169819499799852
Experience 8, Iter 77, disc loss: 0.0044562663251349775, policy loss: 6.119041455347449
Experience 8, Iter 78, disc loss: 0.004737774840421259, policy loss: 6.06727920217811
Experience 8, Iter 79, disc loss: 0.005280344478874124, policy loss: 5.819156525123677
Experience 8, Iter 80, disc loss: 0.00532825870273688, policy loss: 5.742736737138303
Experience 8, Iter 81, disc loss: 0.00473622886459822, policy loss: 5.900503394345664
Experience 8, Iter 82, disc loss: 0.003822146319006888, policy loss: 6.614295391422775
Experience 8, Iter 83, disc loss: 0.004496325104635561, policy loss: 6.031626382344447
Experience 8, Iter 84, disc loss: 0.004374483780284996, policy loss: 5.97659649863918
Experience 8, Iter 85, disc loss: 0.0040025164219094425, policy loss: 6.3649852521245425
Experience 8, Iter 86, disc loss: 0.004519106249487311, policy loss: 5.940798957380522
Experience 8, Iter 87, disc loss: 0.005021209582441305, policy loss: 5.850574266425456
Experience 8, Iter 88, disc loss: 0.0036515863118575303, policy loss: 6.378681975842726
Experience 8, Iter 89, disc loss: 0.004271890856709065, policy loss: 6.131129757616312
Experience 8, Iter 90, disc loss: 0.0039210871471399, policy loss: 6.195422267380572
Experience 8, Iter 91, disc loss: 0.004770252234561972, policy loss: 5.907709519192544
Experience 8, Iter 92, disc loss: 0.004549431936513613, policy loss: 5.947899653452099
Experience 8, Iter 93, disc loss: 0.004663595375567254, policy loss: 5.887335409179329
Experience 8, Iter 94, disc loss: 0.0049539696949096705, policy loss: 5.889872074587416
Experience 8, Iter 95, disc loss: 0.005641602416494859, policy loss: 5.692777550288339
Experience 8, Iter 96, disc loss: 0.005162252640950399, policy loss: 5.7740566337531245
Experience 8, Iter 97, disc loss: 0.004618394184977174, policy loss: 6.021597138410305
Experience 8, Iter 98, disc loss: 0.004730937279346042, policy loss: 5.900988211069278
Experience 8, Iter 99, disc loss: 0.005393508471251217, policy loss: 5.667321646335182
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0035],
        [0.1212],
        [1.0385],
        [0.0114]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.6141e-03, 1.2239e-01, 5.6741e-01, 1.9892e-02, 3.2226e-03,
          3.4882e+00]],

        [[8.6141e-03, 1.2239e-01, 5.6741e-01, 1.9892e-02, 3.2226e-03,
          3.4882e+00]],

        [[8.6141e-03, 1.2239e-01, 5.6741e-01, 1.9892e-02, 3.2226e-03,
          3.4882e+00]],

        [[8.6141e-03, 1.2239e-01, 5.6741e-01, 1.9892e-02, 3.2226e-03,
          3.4882e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0140, 0.4847, 4.1540, 0.0456], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0140, 0.4847, 4.1540, 0.0456])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.412
Iter 2/2000 - Loss: 2.603
Iter 3/2000 - Loss: 2.374
Iter 4/2000 - Loss: 2.379
Iter 5/2000 - Loss: 2.453
Iter 6/2000 - Loss: 2.416
Iter 7/2000 - Loss: 2.325
Iter 8/2000 - Loss: 2.278
Iter 9/2000 - Loss: 2.291
Iter 10/2000 - Loss: 2.302
Iter 11/2000 - Loss: 2.262
Iter 12/2000 - Loss: 2.189
Iter 13/2000 - Loss: 2.122
Iter 14/2000 - Loss: 2.078
Iter 15/2000 - Loss: 2.036
Iter 16/2000 - Loss: 1.967
Iter 17/2000 - Loss: 1.863
Iter 18/2000 - Loss: 1.739
Iter 19/2000 - Loss: 1.608
Iter 20/2000 - Loss: 1.466
Iter 1981/2000 - Loss: -7.383
Iter 1982/2000 - Loss: -7.383
Iter 1983/2000 - Loss: -7.383
Iter 1984/2000 - Loss: -7.383
Iter 1985/2000 - Loss: -7.383
Iter 1986/2000 - Loss: -7.383
Iter 1987/2000 - Loss: -7.383
Iter 1988/2000 - Loss: -7.383
Iter 1989/2000 - Loss: -7.383
Iter 1990/2000 - Loss: -7.384
Iter 1991/2000 - Loss: -7.384
Iter 1992/2000 - Loss: -7.384
Iter 1993/2000 - Loss: -7.384
Iter 1994/2000 - Loss: -7.384
Iter 1995/2000 - Loss: -7.384
Iter 1996/2000 - Loss: -7.384
Iter 1997/2000 - Loss: -7.384
Iter 1998/2000 - Loss: -7.384
Iter 1999/2000 - Loss: -7.384
Iter 2000/2000 - Loss: -7.384
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[14.4129,  8.6691, 45.8012, 11.3536, 14.7998, 55.2107]],

        [[20.3153, 41.1475, 10.0740,  1.3908,  8.0701, 29.6684]],

        [[21.3842, 44.4343, 11.0205,  0.9591,  2.1136, 21.4392]],

        [[15.3673, 38.9016, 15.2531,  3.8143,  1.0963, 42.6688]]])
Signal Variance: tensor([ 0.1381,  2.5759, 15.6671,  0.3867])
Estimated target variance: tensor([0.0140, 0.4847, 4.1540, 0.0456])
N: 90
Signal to noise ratio: tensor([19.6296, 91.4551, 98.1535, 34.6132])
Bound on condition number: tensor([ 34679.7952, 752764.4567, 867070.6989, 107827.8054])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.005321483776902818, policy loss: 5.787550642611161
Experience 9, Iter 1, disc loss: 0.005062357293582877, policy loss: 5.748239822987549
Experience 9, Iter 2, disc loss: 0.004549401651187438, policy loss: 5.897740369429695
Experience 9, Iter 3, disc loss: 0.0046788339008574506, policy loss: 5.901619522970413
Experience 9, Iter 4, disc loss: 0.004639057311294333, policy loss: 5.96752721072322
Experience 9, Iter 5, disc loss: 0.004819335183530895, policy loss: 5.826708389883716
Experience 9, Iter 6, disc loss: 0.005351896437193143, policy loss: 5.714751343474406
Experience 9, Iter 7, disc loss: 0.005484517778929324, policy loss: 5.624900114452448
Experience 9, Iter 8, disc loss: 0.005141265787842927, policy loss: 5.742909724559079
Experience 9, Iter 9, disc loss: 0.005004651958877991, policy loss: 5.796140952885628
Experience 9, Iter 10, disc loss: 0.004893265682280504, policy loss: 5.9072142910203835
Experience 9, Iter 11, disc loss: 0.005252954779728476, policy loss: 5.6436878199303795
Experience 9, Iter 12, disc loss: 0.004975340584846432, policy loss: 5.98769095898715
Experience 9, Iter 13, disc loss: 0.005195417030729637, policy loss: 5.702010201134925
Experience 9, Iter 14, disc loss: 0.0052092139217600805, policy loss: 5.740580589751405
Experience 9, Iter 15, disc loss: 0.0054569702854130745, policy loss: 5.681210952163819
Experience 9, Iter 16, disc loss: 0.0048419513746233616, policy loss: 5.805060757956662
Experience 9, Iter 17, disc loss: 0.005115266192505719, policy loss: 5.730123673356442
Experience 9, Iter 18, disc loss: 0.005067142529382537, policy loss: 5.775610078397484
Experience 9, Iter 19, disc loss: 0.004660480805620248, policy loss: 5.910459642518118
Experience 9, Iter 20, disc loss: 0.004424088172906493, policy loss: 5.9432502388444775
Experience 9, Iter 21, disc loss: 0.00490073261793603, policy loss: 5.73688885702015
Experience 9, Iter 22, disc loss: 0.004746585482794792, policy loss: 5.8623099287254545
Experience 9, Iter 23, disc loss: 0.004754621043286133, policy loss: 5.823961553917682
Experience 9, Iter 24, disc loss: 0.004716271443010065, policy loss: 5.880776726869469
Experience 9, Iter 25, disc loss: 0.004724170271747149, policy loss: 5.959735621108974
Experience 9, Iter 26, disc loss: 0.005072619289258047, policy loss: 5.74240960381516
Experience 9, Iter 27, disc loss: 0.00447351177518044, policy loss: 5.883585354222212
Experience 9, Iter 28, disc loss: 0.004438236562414366, policy loss: 5.924145984138869
Experience 9, Iter 29, disc loss: 0.0044800399920073355, policy loss: 6.110256234410841
Experience 9, Iter 30, disc loss: 0.004825505492106887, policy loss: 5.857660327904767
Experience 9, Iter 31, disc loss: 0.005165456227782134, policy loss: 5.670606707470932
Experience 9, Iter 32, disc loss: 0.00490618333102321, policy loss: 5.775326711184927
Experience 9, Iter 33, disc loss: 0.004387394005663195, policy loss: 5.995034406217199
Experience 9, Iter 34, disc loss: 0.0048992984816614545, policy loss: 5.83398943981602
Experience 9, Iter 35, disc loss: 0.00452006415446787, policy loss: 5.9115883601917805
Experience 9, Iter 36, disc loss: 0.004372886500915022, policy loss: 5.997841437389734
Experience 9, Iter 37, disc loss: 0.004541393578553911, policy loss: 5.913838302738492
Experience 9, Iter 38, disc loss: 0.004860588104613839, policy loss: 5.759026682630348
Experience 9, Iter 39, disc loss: 0.004337468099493565, policy loss: 5.941850232835014
Experience 9, Iter 40, disc loss: 0.0046245210035726885, policy loss: 5.821506933471094
Experience 9, Iter 41, disc loss: 0.004472581952529588, policy loss: 5.924632243865456
Experience 9, Iter 42, disc loss: 0.004306752043833448, policy loss: 5.933625746014404
Experience 9, Iter 43, disc loss: 0.0044547303381822865, policy loss: 5.948130268616717
Experience 9, Iter 44, disc loss: 0.00508696439918388, policy loss: 5.664340386349596
Experience 9, Iter 45, disc loss: 0.004382381601200882, policy loss: 5.994251723900141
Experience 9, Iter 46, disc loss: 0.0038845530040774396, policy loss: 6.086576674312113
Experience 9, Iter 47, disc loss: 0.004537080737949006, policy loss: 5.8258890007404975
Experience 9, Iter 48, disc loss: 0.004734616611856068, policy loss: 5.88363616349342
Experience 9, Iter 49, disc loss: 0.004566609521073356, policy loss: 5.842343691607097
Experience 9, Iter 50, disc loss: 0.004312621700887129, policy loss: 6.052316824612868
Experience 9, Iter 51, disc loss: 0.004387802699063309, policy loss: 5.911361531366271
Experience 9, Iter 52, disc loss: 0.004449797699709952, policy loss: 5.871058210774477
Experience 9, Iter 53, disc loss: 0.004271960982663964, policy loss: 6.053608064278201
Experience 9, Iter 54, disc loss: 0.004603077789768802, policy loss: 5.8090048502793765
Experience 9, Iter 55, disc loss: 0.004702822878633207, policy loss: 5.739121489331753
Experience 9, Iter 56, disc loss: 0.004306896722163774, policy loss: 5.975926058864638
Experience 9, Iter 57, disc loss: 0.003920268913825601, policy loss: 6.176141911477664
Experience 9, Iter 58, disc loss: 0.00400986127757757, policy loss: 6.148883715341429
Experience 9, Iter 59, disc loss: 0.004221940514395118, policy loss: 5.996182225715033
Experience 9, Iter 60, disc loss: 0.004042410375201019, policy loss: 6.111583785972133
Experience 9, Iter 61, disc loss: 0.004046477306012157, policy loss: 5.977945572053676
Experience 9, Iter 62, disc loss: 0.004530379729223333, policy loss: 5.778992232761836
Experience 9, Iter 63, disc loss: 0.004164440966674598, policy loss: 5.979453187082603
Experience 9, Iter 64, disc loss: 0.0038857862741697455, policy loss: 6.147857104892287
Experience 9, Iter 65, disc loss: 0.004167484851192337, policy loss: 5.949101605066613
Experience 9, Iter 66, disc loss: 0.004548279363672183, policy loss: 5.823556888440975
Experience 9, Iter 67, disc loss: 0.004156668703195443, policy loss: 5.961932840700154
Experience 9, Iter 68, disc loss: 0.004240497785124321, policy loss: 5.911107572387851
Experience 9, Iter 69, disc loss: 0.004103452447628258, policy loss: 5.9895473352088615
Experience 9, Iter 70, disc loss: 0.004350318034965281, policy loss: 5.846604052110305
Experience 9, Iter 71, disc loss: 0.0038739790874586537, policy loss: 6.104418461384775
Experience 9, Iter 72, disc loss: 0.003958700613487447, policy loss: 6.0751988170430575
Experience 9, Iter 73, disc loss: 0.004276237975865929, policy loss: 5.934954187981607
Experience 9, Iter 74, disc loss: 0.004045073529317612, policy loss: 6.019862411343256
Experience 9, Iter 75, disc loss: 0.00408630525005391, policy loss: 5.973148990546324
Experience 9, Iter 76, disc loss: 0.004120733594284368, policy loss: 5.983438643648194
Experience 9, Iter 77, disc loss: 0.004145893095624795, policy loss: 5.940796701336604
Experience 9, Iter 78, disc loss: 0.004173502756866201, policy loss: 5.9169444530173045
Experience 9, Iter 79, disc loss: 0.004266740284324999, policy loss: 5.888707998808988
Experience 9, Iter 80, disc loss: 0.004242094362950419, policy loss: 5.83092362084909
Experience 9, Iter 81, disc loss: 0.003977358060030014, policy loss: 6.081364679294798
Experience 9, Iter 82, disc loss: 0.004030992644606153, policy loss: 5.944681638152858
Experience 9, Iter 83, disc loss: 0.003971861361505083, policy loss: 5.999128447051049
Experience 9, Iter 84, disc loss: 0.0038653802992234237, policy loss: 6.030977033209842
Experience 9, Iter 85, disc loss: 0.003900746661271156, policy loss: 5.967564833747536
Experience 9, Iter 86, disc loss: 0.0038536983309105533, policy loss: 6.025988353873726
Experience 9, Iter 87, disc loss: 0.0037988459456215293, policy loss: 6.161306713361696
Experience 9, Iter 88, disc loss: 0.003504280273758067, policy loss: 6.227719368692867
Experience 9, Iter 89, disc loss: 0.0037729886881458434, policy loss: 6.077852396665816
Experience 9, Iter 90, disc loss: 0.0033275523666211307, policy loss: 6.2457027093333775
Experience 9, Iter 91, disc loss: 0.003072635088520075, policy loss: 6.29887237051701
Experience 9, Iter 92, disc loss: 0.0031617408670394486, policy loss: 6.271447062697235
Experience 9, Iter 93, disc loss: 0.0035630406337502562, policy loss: 6.135938376750447
Experience 9, Iter 94, disc loss: 0.003855322237147981, policy loss: 5.985504555018811
Experience 9, Iter 95, disc loss: 0.0033918570459561307, policy loss: 6.220931754652665
Experience 9, Iter 96, disc loss: 0.003354746153740087, policy loss: 6.255659506844319
Experience 9, Iter 97, disc loss: 0.003609330967266763, policy loss: 6.11626716840518
Experience 9, Iter 98, disc loss: 0.0032392863273719846, policy loss: 6.291939667738987
Experience 9, Iter 99, disc loss: 0.0032855190833159407, policy loss: 6.227276040429929
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1261],
        [1.0799],
        [0.0110]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.8324e-03, 1.1317e-01, 5.5876e-01, 1.9060e-02, 2.9849e-03,
          3.5471e+00]],

        [[7.8324e-03, 1.1317e-01, 5.5876e-01, 1.9060e-02, 2.9849e-03,
          3.5471e+00]],

        [[7.8324e-03, 1.1317e-01, 5.5876e-01, 1.9060e-02, 2.9849e-03,
          3.5471e+00]],

        [[7.8324e-03, 1.1317e-01, 5.5876e-01, 1.9060e-02, 2.9849e-03,
          3.5471e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0129, 0.5046, 4.3195, 0.0442], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0129, 0.5046, 4.3195, 0.0442])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.400
Iter 2/2000 - Loss: 2.618
Iter 3/2000 - Loss: 2.365
Iter 4/2000 - Loss: 2.374
Iter 5/2000 - Loss: 2.458
Iter 6/2000 - Loss: 2.421
Iter 7/2000 - Loss: 2.323
Iter 8/2000 - Loss: 2.269
Iter 9/2000 - Loss: 2.281
Iter 10/2000 - Loss: 2.297
Iter 11/2000 - Loss: 2.260
Iter 12/2000 - Loss: 2.183
Iter 13/2000 - Loss: 2.108
Iter 14/2000 - Loss: 2.056
Iter 15/2000 - Loss: 2.012
Iter 16/2000 - Loss: 1.946
Iter 17/2000 - Loss: 1.842
Iter 18/2000 - Loss: 1.712
Iter 19/2000 - Loss: 1.572
Iter 20/2000 - Loss: 1.423
Iter 1981/2000 - Loss: -7.576
Iter 1982/2000 - Loss: -7.576
Iter 1983/2000 - Loss: -7.576
Iter 1984/2000 - Loss: -7.576
Iter 1985/2000 - Loss: -7.576
Iter 1986/2000 - Loss: -7.576
Iter 1987/2000 - Loss: -7.576
Iter 1988/2000 - Loss: -7.576
Iter 1989/2000 - Loss: -7.576
Iter 1990/2000 - Loss: -7.576
Iter 1991/2000 - Loss: -7.576
Iter 1992/2000 - Loss: -7.576
Iter 1993/2000 - Loss: -7.576
Iter 1994/2000 - Loss: -7.576
Iter 1995/2000 - Loss: -7.576
Iter 1996/2000 - Loss: -7.576
Iter 1997/2000 - Loss: -7.577
Iter 1998/2000 - Loss: -7.577
Iter 1999/2000 - Loss: -7.577
Iter 2000/2000 - Loss: -7.577
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[14.4018,  8.2734, 44.9973, 10.6533, 13.9574, 51.0661]],

        [[20.4263, 39.1043,  9.6453,  1.5868,  4.1591, 30.5288]],

        [[20.8044, 42.2644, 10.9448,  0.9618,  2.1563, 21.5062]],

        [[14.3321, 36.5942, 13.7492,  3.4355,  1.2728, 42.5250]]])
Signal Variance: tensor([ 0.1286,  2.6142, 15.9313,  0.3253])
Estimated target variance: tensor([0.0129, 0.5046, 4.3195, 0.0442])
N: 100
Signal to noise ratio: tensor([ 19.4368,  90.9680, 100.2548,  31.1276])
Bound on condition number: tensor([  37779.9901,  827518.2557, 1005104.3974,   96893.8778])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0031214688342579336, policy loss: 6.291003304829253
Experience 10, Iter 1, disc loss: 0.0036406906990588033, policy loss: 6.105497168123307
Experience 10, Iter 2, disc loss: 0.003633455322768836, policy loss: 6.111180082011238
Experience 10, Iter 3, disc loss: 0.0035854651623009994, policy loss: 6.054028587753748
Experience 10, Iter 4, disc loss: 0.0035512110877052062, policy loss: 6.089584486635581
Experience 10, Iter 5, disc loss: 0.0034268714514376405, policy loss: 6.2511466100464546
Experience 10, Iter 6, disc loss: 0.003462540782577952, policy loss: 6.138352319648871
Experience 10, Iter 7, disc loss: 0.0034999146377918703, policy loss: 6.109666335935257
Experience 10, Iter 8, disc loss: 0.0033321419646158007, policy loss: 6.267959336072354
Experience 10, Iter 9, disc loss: 0.0036427974797020256, policy loss: 6.0553030755861075
Experience 10, Iter 10, disc loss: 0.0035580902093367032, policy loss: 6.066269186744069
Experience 10, Iter 11, disc loss: 0.0034781834795042767, policy loss: 6.147529723081972
Experience 10, Iter 12, disc loss: 0.00357881632403056, policy loss: 6.043942246944297
Experience 10, Iter 13, disc loss: 0.003347574475649121, policy loss: 6.176919210452603
Experience 10, Iter 14, disc loss: 0.0034896443561171337, policy loss: 6.077797063857352
Experience 10, Iter 15, disc loss: 0.003125581236603737, policy loss: 6.511866738462489
Experience 10, Iter 16, disc loss: 0.0034174365235261584, policy loss: 6.203168363635719
Experience 10, Iter 17, disc loss: 0.0031894594126795238, policy loss: 6.2405206852899076
Experience 10, Iter 18, disc loss: 0.0033243056468306113, policy loss: 6.263302200811338
Experience 10, Iter 19, disc loss: 0.0034552510614261327, policy loss: 6.253778325035715
Experience 10, Iter 20, disc loss: 0.003325770716132106, policy loss: 6.2331145887513495
Experience 10, Iter 21, disc loss: 0.0034963601663261104, policy loss: 6.048839017980377
Experience 10, Iter 22, disc loss: 0.0034587721418632214, policy loss: 6.063406721215979
Experience 10, Iter 23, disc loss: 0.0033351509171639527, policy loss: 6.171325386249412
Experience 10, Iter 24, disc loss: 0.0030488947789818043, policy loss: 6.330611195800906
Experience 10, Iter 25, disc loss: 0.003068054976225003, policy loss: 6.3197768500287905
Experience 10, Iter 26, disc loss: 0.0034517927626836207, policy loss: 6.079886181669145
Experience 10, Iter 27, disc loss: 0.0035103842708978034, policy loss: 6.005714244687576
Experience 10, Iter 28, disc loss: 0.003097447887464623, policy loss: 6.274945703077938
Experience 10, Iter 29, disc loss: 0.003302951674921945, policy loss: 6.116072517779886
Experience 10, Iter 30, disc loss: 0.003321118742694565, policy loss: 6.128437380745433
Experience 10, Iter 31, disc loss: 0.003049138159965851, policy loss: 6.310378910102598
Experience 10, Iter 32, disc loss: 0.0030700226525590623, policy loss: 6.323878391945171
Experience 10, Iter 33, disc loss: 0.0030991278858775342, policy loss: 6.276658037483699
Experience 10, Iter 34, disc loss: 0.003173246941813916, policy loss: 6.158417681647737
Experience 10, Iter 35, disc loss: 0.003000003396712628, policy loss: 6.241047356873269
Experience 10, Iter 36, disc loss: 0.003108430625132935, policy loss: 6.214357245867722
Experience 10, Iter 37, disc loss: 0.003156130497048965, policy loss: 6.1429088857663
Experience 10, Iter 38, disc loss: 0.0031810181851370197, policy loss: 6.19265676057644
Experience 10, Iter 39, disc loss: 0.0031114049221559028, policy loss: 6.22520749407263
Experience 10, Iter 40, disc loss: 0.0029184776175965195, policy loss: 6.326954701816223
Experience 10, Iter 41, disc loss: 0.0031094494224569447, policy loss: 6.178133748776043
Experience 10, Iter 42, disc loss: 0.0029718812918259224, policy loss: 6.243371121962222
Experience 10, Iter 43, disc loss: 0.0028928967992448225, policy loss: 6.299979679799021
Experience 10, Iter 44, disc loss: 0.002881891066097756, policy loss: 6.377872956523232
Experience 10, Iter 45, disc loss: 0.002971962801798997, policy loss: 6.257707526930811
Experience 10, Iter 46, disc loss: 0.0029193228920746947, policy loss: 6.315408786714565
Experience 10, Iter 47, disc loss: 0.0029724189263251205, policy loss: 6.235375827438048
Experience 10, Iter 48, disc loss: 0.002916258175983988, policy loss: 6.254566761340683
Experience 10, Iter 49, disc loss: 0.0030051417649655295, policy loss: 6.232523882872819
Experience 10, Iter 50, disc loss: 0.0028994144161942344, policy loss: 6.280941614969843
Experience 10, Iter 51, disc loss: 0.0027750900856960935, policy loss: 6.3545135415877345
Experience 10, Iter 52, disc loss: 0.002873259600082475, policy loss: 6.267308556761014
Experience 10, Iter 53, disc loss: 0.003049708537685773, policy loss: 6.149918931832345
Experience 10, Iter 54, disc loss: 0.002600657560882958, policy loss: 6.496576658201883
Experience 10, Iter 55, disc loss: 0.0027298313465286953, policy loss: 6.376848875197442
Experience 10, Iter 56, disc loss: 0.0028095000030944764, policy loss: 6.3852636629173976
Experience 10, Iter 57, disc loss: 0.002913524292563688, policy loss: 6.2234428942349265
Experience 10, Iter 58, disc loss: 0.0027199527504057058, policy loss: 6.390347988743201
Experience 10, Iter 59, disc loss: 0.002946206458632433, policy loss: 6.243849439592847
Experience 10, Iter 60, disc loss: 0.0028566448337585424, policy loss: 6.29533607282573
Experience 10, Iter 61, disc loss: 0.0027385323675663273, policy loss: 6.37122690951774
Experience 10, Iter 62, disc loss: 0.002917850693902792, policy loss: 6.316378549544802
Experience 10, Iter 63, disc loss: 0.0026360622899960855, policy loss: 6.480166093653836
Experience 10, Iter 64, disc loss: 0.002518877089873388, policy loss: 6.548404498615766
Experience 10, Iter 65, disc loss: 0.002885043696385571, policy loss: 6.269425614527141
Experience 10, Iter 66, disc loss: 0.002598288546203699, policy loss: 6.5159702479966235
Experience 10, Iter 67, disc loss: 0.0026878450201665927, policy loss: 6.415149256192882
Experience 10, Iter 68, disc loss: 0.00272221245400447, policy loss: 6.345475979664434
Experience 10, Iter 69, disc loss: 0.0027333844916069333, policy loss: 6.314147650606926
Experience 10, Iter 70, disc loss: 0.0026076854119776414, policy loss: 6.501499644088952
Experience 10, Iter 71, disc loss: 0.002734824020561641, policy loss: 6.307194020818411
Experience 10, Iter 72, disc loss: 0.0026216875848773286, policy loss: 6.36411226866168
Experience 10, Iter 73, disc loss: 0.0025160214337115573, policy loss: 6.492175296142183
Experience 10, Iter 74, disc loss: 0.0028028572788969764, policy loss: 6.23767854685052
Experience 10, Iter 75, disc loss: 0.0027645182470877145, policy loss: 6.254767068882267
Experience 10, Iter 76, disc loss: 0.0026720305912738404, policy loss: 6.349341754048311
Experience 10, Iter 77, disc loss: 0.002516240188447182, policy loss: 6.50903344388017
Experience 10, Iter 78, disc loss: 0.002678761990340108, policy loss: 6.333917256000101
Experience 10, Iter 79, disc loss: 0.002521266308580739, policy loss: 6.480463658949308
Experience 10, Iter 80, disc loss: 0.002448113557668987, policy loss: 6.5190878441633515
Experience 10, Iter 81, disc loss: 0.0026375919703098046, policy loss: 6.3636383443664934
Experience 10, Iter 82, disc loss: 0.0026769776812074412, policy loss: 6.330770293942509
Experience 10, Iter 83, disc loss: 0.0025432143769581507, policy loss: 6.410104365217961
Experience 10, Iter 84, disc loss: 0.0024991994877807027, policy loss: 6.489071948411281
Experience 10, Iter 85, disc loss: 0.002412658828694999, policy loss: 6.431174944790969
Experience 10, Iter 86, disc loss: 0.002404120852830603, policy loss: 6.469959525719633
Experience 10, Iter 87, disc loss: 0.002377647074404313, policy loss: 6.415634485172024
Experience 10, Iter 88, disc loss: 0.00233476852566565, policy loss: 6.569541800214186
Experience 10, Iter 89, disc loss: 0.0023345501450935105, policy loss: 6.63659024596546
Experience 10, Iter 90, disc loss: 0.0023516279771494835, policy loss: 6.53339030133043
Experience 10, Iter 91, disc loss: 0.002545250435867536, policy loss: 6.456266556726728
Experience 10, Iter 92, disc loss: 0.002261213855738973, policy loss: 6.606446121529489
Experience 10, Iter 93, disc loss: 0.002319282838912063, policy loss: 6.532180558158766
Experience 10, Iter 94, disc loss: 0.002338895842843393, policy loss: 6.491308348880979
Experience 10, Iter 95, disc loss: 0.00218189526330877, policy loss: 6.631334449201814
Experience 10, Iter 96, disc loss: 0.0023257722452191803, policy loss: 6.446281404362166
Experience 10, Iter 97, disc loss: 0.002197074723060824, policy loss: 6.584640159358342
Experience 10, Iter 98, disc loss: 0.0024333766072037084, policy loss: 6.465998077372107
Experience 10, Iter 99, disc loss: 0.0021682661395984173, policy loss: 6.7332815762423905
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.1249],
        [1.0605],
        [0.0108]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.1908e-03, 1.0599e-01, 5.4783e-01, 1.8933e-02, 2.8717e-03,
          3.5311e+00]],

        [[7.1908e-03, 1.0599e-01, 5.4783e-01, 1.8933e-02, 2.8717e-03,
          3.5311e+00]],

        [[7.1908e-03, 1.0599e-01, 5.4783e-01, 1.8933e-02, 2.8717e-03,
          3.5311e+00]],

        [[7.1908e-03, 1.0599e-01, 5.4783e-01, 1.8933e-02, 2.8717e-03,
          3.5311e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0120, 0.4996, 4.2420, 0.0434], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0120, 0.4996, 4.2420, 0.0434])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.351
Iter 2/2000 - Loss: 2.590
Iter 3/2000 - Loss: 2.317
Iter 4/2000 - Loss: 2.331
Iter 5/2000 - Loss: 2.421
Iter 6/2000 - Loss: 2.385
Iter 7/2000 - Loss: 2.284
Iter 8/2000 - Loss: 2.225
Iter 9/2000 - Loss: 2.235
Iter 10/2000 - Loss: 2.256
Iter 11/2000 - Loss: 2.228
Iter 12/2000 - Loss: 2.151
Iter 13/2000 - Loss: 2.068
Iter 14/2000 - Loss: 2.007
Iter 15/2000 - Loss: 1.961
Iter 16/2000 - Loss: 1.898
Iter 17/2000 - Loss: 1.798
Iter 18/2000 - Loss: 1.666
Iter 19/2000 - Loss: 1.521
Iter 20/2000 - Loss: 1.368
Iter 1981/2000 - Loss: -7.697
Iter 1982/2000 - Loss: -7.697
Iter 1983/2000 - Loss: -7.697
Iter 1984/2000 - Loss: -7.698
Iter 1985/2000 - Loss: -7.698
Iter 1986/2000 - Loss: -7.698
Iter 1987/2000 - Loss: -7.698
Iter 1988/2000 - Loss: -7.698
Iter 1989/2000 - Loss: -7.698
Iter 1990/2000 - Loss: -7.698
Iter 1991/2000 - Loss: -7.698
Iter 1992/2000 - Loss: -7.698
Iter 1993/2000 - Loss: -7.698
Iter 1994/2000 - Loss: -7.698
Iter 1995/2000 - Loss: -7.698
Iter 1996/2000 - Loss: -7.698
Iter 1997/2000 - Loss: -7.698
Iter 1998/2000 - Loss: -7.698
Iter 1999/2000 - Loss: -7.698
Iter 2000/2000 - Loss: -7.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.9040,  8.3052, 43.7444, 10.4062, 13.6271, 50.9743]],

        [[19.7434, 37.9626,  9.4912,  1.5014,  5.7820, 30.4971]],

        [[19.5278, 41.2101, 11.0435,  0.9384,  2.2116, 19.5311]],

        [[13.5148, 35.0448, 13.7084,  3.4572,  1.2830, 42.1547]]])
Signal Variance: tensor([ 0.1277,  2.6449, 14.4766,  0.3187])
Estimated target variance: tensor([0.0120, 0.4996, 4.2420, 0.0434])
N: 110
Signal to noise ratio: tensor([20.2135, 89.6348, 89.8993, 30.7198])
Bound on condition number: tensor([ 44945.2749, 883784.8362, 889008.6088, 103808.8258])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.002218730089692976, policy loss: 6.569823036004571
Experience 11, Iter 1, disc loss: 0.0023089333156160793, policy loss: 6.456697565525272
Experience 11, Iter 2, disc loss: 0.0022056971246644896, policy loss: 6.614402492615042
Experience 11, Iter 3, disc loss: 0.002211153783500198, policy loss: 6.520972929415018
Experience 11, Iter 4, disc loss: 0.0020377559981732706, policy loss: 6.72633457755632
Experience 11, Iter 5, disc loss: 0.0022493875039598446, policy loss: 6.5211938683754305
Experience 11, Iter 6, disc loss: 0.0023186784421264936, policy loss: 6.433791642402703
Experience 11, Iter 7, disc loss: 0.0021823880749852554, policy loss: 6.583631617038057
Experience 11, Iter 8, disc loss: 0.0019369883066645828, policy loss: 6.805991852405039
Experience 11, Iter 9, disc loss: 0.002114555574504795, policy loss: 6.5811518907741755
Experience 11, Iter 10, disc loss: 0.0021833910177298963, policy loss: 6.576384895101977
Experience 11, Iter 11, disc loss: 0.002246959994306008, policy loss: 6.480964569827329
Experience 11, Iter 12, disc loss: 0.002123076920631001, policy loss: 6.52507087846565
Experience 11, Iter 13, disc loss: 0.002167582494823506, policy loss: 6.546435810225786
Experience 11, Iter 14, disc loss: 0.0019642769509837727, policy loss: 6.699954772442443
Experience 11, Iter 15, disc loss: 0.002196528303756552, policy loss: 6.515600846783764
Experience 11, Iter 16, disc loss: 0.0021442360319730933, policy loss: 6.599974733464834
Experience 11, Iter 17, disc loss: 0.0020665346971801253, policy loss: 6.64259519268569
Experience 11, Iter 18, disc loss: 0.0020911641439327823, policy loss: 6.583585843008172
Experience 11, Iter 19, disc loss: 0.0021817131591469617, policy loss: 6.583751665911121
Experience 11, Iter 20, disc loss: 0.002253049902352358, policy loss: 6.501559744821742
Experience 11, Iter 21, disc loss: 0.0020833728706510313, policy loss: 6.640897141336479
Experience 11, Iter 22, disc loss: 0.002116665412645527, policy loss: 6.59404479941113
Experience 11, Iter 23, disc loss: 0.002159767945413873, policy loss: 6.595626349208655
Experience 11, Iter 24, disc loss: 0.002309356457462813, policy loss: 6.407065774790013
Experience 11, Iter 25, disc loss: 0.0021998260854258337, policy loss: 6.511347589324113
Experience 11, Iter 26, disc loss: 0.0022258513727252643, policy loss: 6.531154651278076
Experience 11, Iter 27, disc loss: 0.00221474357608826, policy loss: 6.501358804926193
Experience 11, Iter 28, disc loss: 0.0021195902359820227, policy loss: 6.602863915755023
Experience 11, Iter 29, disc loss: 0.002064812526984904, policy loss: 6.664355888279388
Experience 11, Iter 30, disc loss: 0.0020519286062084196, policy loss: 6.6051930157721594
Experience 11, Iter 31, disc loss: 0.0021274737525115595, policy loss: 6.540888082266687
Experience 11, Iter 32, disc loss: 0.002226426001326135, policy loss: 6.473247813756171
Experience 11, Iter 33, disc loss: 0.002251137765025886, policy loss: 6.456139152247504
Experience 11, Iter 34, disc loss: 0.0019880092799642837, policy loss: 6.662504676576723
Experience 11, Iter 35, disc loss: 0.0020035119287593506, policy loss: 6.7833760594676615
Experience 11, Iter 36, disc loss: 0.0021402296008368503, policy loss: 6.523085238598515
Experience 11, Iter 37, disc loss: 0.0019806361965391513, policy loss: 6.712266960107334
Experience 11, Iter 38, disc loss: 0.0019031304296247555, policy loss: 6.807070245766936
Experience 11, Iter 39, disc loss: 0.0020712068476048846, policy loss: 6.654617679744359
Experience 11, Iter 40, disc loss: 0.0021377438509117717, policy loss: 6.527458956352952
Experience 11, Iter 41, disc loss: 0.001992328793196756, policy loss: 6.6469713338958005
Experience 11, Iter 42, disc loss: 0.0022339621188691753, policy loss: 6.478558045389059
Experience 11, Iter 43, disc loss: 0.0021686147615656913, policy loss: 6.520951621054582
Experience 11, Iter 44, disc loss: 0.0020855039241842134, policy loss: 6.573194706346892
Experience 11, Iter 45, disc loss: 0.0020340932618242223, policy loss: 6.6450456599211565
Experience 11, Iter 46, disc loss: 0.0020890484039713417, policy loss: 6.5526306011187705
Experience 11, Iter 47, disc loss: 0.0019848592923367697, policy loss: 6.680770367928496
Experience 11, Iter 48, disc loss: 0.002110556126130366, policy loss: 6.5457839123157004
Experience 11, Iter 49, disc loss: 0.0019933498205638184, policy loss: 6.657999417145599
Experience 11, Iter 50, disc loss: 0.0019360112777946762, policy loss: 6.738610016344112
Experience 11, Iter 51, disc loss: 0.0018191845363183193, policy loss: 6.762588861982048
Experience 11, Iter 52, disc loss: 0.0015625802093420145, policy loss: 7.023799637083449
Experience 11, Iter 53, disc loss: 0.0015515631797245542, policy loss: 7.043096204027404
Experience 11, Iter 54, disc loss: 0.0015932147380819973, policy loss: 6.930908825591056
Experience 11, Iter 55, disc loss: 0.0015823894582273784, policy loss: 7.031929675629846
Experience 11, Iter 56, disc loss: 0.0016649253449846652, policy loss: 6.948029947727376
Experience 11, Iter 57, disc loss: 0.0020936848281628245, policy loss: 6.735393703029698
Experience 11, Iter 58, disc loss: 0.0020288277842462595, policy loss: 6.851451756887525
Experience 11, Iter 59, disc loss: 0.0033657488413001233, policy loss: 6.399190724882695
Experience 11, Iter 60, disc loss: 0.0030035294566108546, policy loss: 6.409966951823238
Experience 11, Iter 61, disc loss: 0.0038345683499375308, policy loss: 6.030026988257655
Experience 11, Iter 62, disc loss: 0.0037340994889253996, policy loss: 6.11248344444693
Experience 11, Iter 63, disc loss: 0.0038655875161850116, policy loss: 6.10868412949
Experience 11, Iter 64, disc loss: 0.0034641635688360374, policy loss: 6.339038534449525
Experience 11, Iter 65, disc loss: 0.0033290127530664878, policy loss: 6.141992842747785
Experience 11, Iter 66, disc loss: 0.0031858750292064753, policy loss: 6.402204664685318
Experience 11, Iter 67, disc loss: 0.003287415057419354, policy loss: 6.060894649603817
Experience 11, Iter 68, disc loss: 0.003119223075395116, policy loss: 6.2999721048754225
Experience 11, Iter 69, disc loss: 0.0030522971340168374, policy loss: 6.240296150213694
Experience 11, Iter 70, disc loss: 0.003194919354775903, policy loss: 6.296039437579356
Experience 11, Iter 71, disc loss: 0.003155037274158002, policy loss: 6.462744819175252
Experience 11, Iter 72, disc loss: 0.003278814742789054, policy loss: 6.167959510065259
Experience 11, Iter 73, disc loss: 0.003467088102554277, policy loss: 6.376321654846651
Experience 11, Iter 74, disc loss: 0.003619230178029757, policy loss: 6.077908398469894
Experience 11, Iter 75, disc loss: 0.003257731837466806, policy loss: 6.369881783107343
Experience 11, Iter 76, disc loss: 0.0034565746073851053, policy loss: 6.288739488667323
Experience 11, Iter 77, disc loss: 0.0037051058173607266, policy loss: 6.211687867280409
Experience 11, Iter 78, disc loss: 0.003788642824289444, policy loss: 6.063642462565758
Experience 11, Iter 79, disc loss: 0.0036307325012933786, policy loss: 6.24398065433848
Experience 11, Iter 80, disc loss: 0.003513575577192554, policy loss: 6.218468681623563
Experience 11, Iter 81, disc loss: 0.0034856188061607356, policy loss: 6.224108898317848
Experience 11, Iter 82, disc loss: 0.0037868171481551306, policy loss: 6.175127975698343
Experience 11, Iter 83, disc loss: 0.003770179790776826, policy loss: 6.169172649742594
Experience 11, Iter 84, disc loss: 0.003953208587548418, policy loss: 6.108311419058907
Experience 11, Iter 85, disc loss: 0.0037063393248062664, policy loss: 6.133976548418368
Experience 11, Iter 86, disc loss: 0.003614058193609075, policy loss: 6.1695584390250255
Experience 11, Iter 87, disc loss: 0.003874359901001425, policy loss: 6.1471600416280925
Experience 11, Iter 88, disc loss: 0.00341652694362515, policy loss: 6.31791209900774
Experience 11, Iter 89, disc loss: 0.003526798188920028, policy loss: 6.1390528749633795
Experience 11, Iter 90, disc loss: 0.003673705932814514, policy loss: 6.302472683534715
Experience 11, Iter 91, disc loss: 0.0035213622746687683, policy loss: 6.276832166379108
Experience 11, Iter 92, disc loss: 0.0030640818967781216, policy loss: 6.4100157636725665
Experience 11, Iter 93, disc loss: 0.0033611230262496255, policy loss: 6.557275619155951
Experience 11, Iter 94, disc loss: 0.0035914112501365864, policy loss: 6.192951614273004
Experience 11, Iter 95, disc loss: 0.0030920736928805267, policy loss: 6.601711948751938
Experience 11, Iter 96, disc loss: 0.0033257218026628912, policy loss: 6.479472785350909
Experience 11, Iter 97, disc loss: 0.003852288902012788, policy loss: 6.244374075627746
Experience 11, Iter 98, disc loss: 0.0034492499936536773, policy loss: 6.387238924381657
Experience 11, Iter 99, disc loss: 0.0032588212168815885, policy loss: 6.342044389246882
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1212],
        [1.0330],
        [0.0107]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.6993e-03, 9.9887e-02, 5.4321e-01, 1.8769e-02, 2.7370e-03,
          3.4515e+00]],

        [[6.6993e-03, 9.9887e-02, 5.4321e-01, 1.8769e-02, 2.7370e-03,
          3.4515e+00]],

        [[6.6993e-03, 9.9887e-02, 5.4321e-01, 1.8769e-02, 2.7370e-03,
          3.4515e+00]],

        [[6.6993e-03, 9.9887e-02, 5.4321e-01, 1.8769e-02, 2.7370e-03,
          3.4515e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.4849, 4.1322, 0.0430], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.4849, 4.1322, 0.0430])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.296
Iter 2/2000 - Loss: 2.553
Iter 3/2000 - Loss: 2.260
Iter 4/2000 - Loss: 2.276
Iter 5/2000 - Loss: 2.370
Iter 6/2000 - Loss: 2.332
Iter 7/2000 - Loss: 2.227
Iter 8/2000 - Loss: 2.161
Iter 9/2000 - Loss: 2.166
Iter 10/2000 - Loss: 2.188
Iter 11/2000 - Loss: 2.163
Iter 12/2000 - Loss: 2.085
Iter 13/2000 - Loss: 1.993
Iter 14/2000 - Loss: 1.921
Iter 15/2000 - Loss: 1.869
Iter 16/2000 - Loss: 1.805
Iter 17/2000 - Loss: 1.705
Iter 18/2000 - Loss: 1.570
Iter 19/2000 - Loss: 1.417
Iter 20/2000 - Loss: 1.259
Iter 1981/2000 - Loss: -7.869
Iter 1982/2000 - Loss: -7.869
Iter 1983/2000 - Loss: -7.869
Iter 1984/2000 - Loss: -7.869
Iter 1985/2000 - Loss: -7.869
Iter 1986/2000 - Loss: -7.869
Iter 1987/2000 - Loss: -7.869
Iter 1988/2000 - Loss: -7.869
Iter 1989/2000 - Loss: -7.869
Iter 1990/2000 - Loss: -7.869
Iter 1991/2000 - Loss: -7.869
Iter 1992/2000 - Loss: -7.869
Iter 1993/2000 - Loss: -7.869
Iter 1994/2000 - Loss: -7.869
Iter 1995/2000 - Loss: -7.869
Iter 1996/2000 - Loss: -7.869
Iter 1997/2000 - Loss: -7.869
Iter 1998/2000 - Loss: -7.869
Iter 1999/2000 - Loss: -7.869
Iter 2000/2000 - Loss: -7.869
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[13.4750,  8.3614, 43.1976, 10.4983, 13.2832, 52.0814]],

        [[19.1645, 36.7608,  9.5115,  1.5303,  5.6169, 30.8582]],

        [[19.2284, 40.1426, 10.9534,  0.9446,  2.3001, 20.5885]],

        [[13.3471, 33.8698, 14.0545,  3.5787,  1.1459, 44.1086]]])
Signal Variance: tensor([ 0.1241,  2.6403, 14.9619,  0.3323])
Estimated target variance: tensor([0.0114, 0.4849, 4.1322, 0.0430])
N: 120
Signal to noise ratio: tensor([20.0456, 91.6662, 92.9905, 31.7104])
Bound on condition number: tensor([  48219.9064, 1008324.1555, 1037669.1631,  120666.5753])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.003987963287736996, policy loss: 6.049887259357886
Experience 12, Iter 1, disc loss: 0.004007394822437845, policy loss: 6.069460422008797
Experience 12, Iter 2, disc loss: 0.0035164600559692057, policy loss: 6.336390798566523
Experience 12, Iter 3, disc loss: 0.0038939902644053717, policy loss: 6.249688195473369
Experience 12, Iter 4, disc loss: 0.0038558439459779083, policy loss: 6.213723808898207
Experience 12, Iter 5, disc loss: 0.004060658989561553, policy loss: 6.1088517930833595
Experience 12, Iter 6, disc loss: 0.004046681620235937, policy loss: 6.126478756372527
Experience 12, Iter 7, disc loss: 0.004365878198843463, policy loss: 5.9487252463561955
Experience 12, Iter 8, disc loss: 0.003884215074618219, policy loss: 6.259552924873106
Experience 12, Iter 9, disc loss: 0.004256085280381908, policy loss: 6.223868437230753
Experience 12, Iter 10, disc loss: 0.004171835035527469, policy loss: 6.129841977604959
Experience 12, Iter 11, disc loss: 0.004004606944976851, policy loss: 6.133994944198429
Experience 12, Iter 12, disc loss: 0.004431006985336544, policy loss: 5.9716663975214175
Experience 12, Iter 13, disc loss: 0.003815173320588103, policy loss: 6.512019361850225
Experience 12, Iter 14, disc loss: 0.003908790730799016, policy loss: 6.228181166676775
Experience 12, Iter 15, disc loss: 0.0036387944944882506, policy loss: 6.333336090259392
Experience 12, Iter 16, disc loss: 0.00401703829640928, policy loss: 6.235662280506832
Experience 12, Iter 17, disc loss: 0.004413632663745375, policy loss: 6.097516292056552
Experience 12, Iter 18, disc loss: 0.003889160609615089, policy loss: 6.163649827084991
Experience 12, Iter 19, disc loss: 0.004276972811749382, policy loss: 6.113255690182485
Experience 12, Iter 20, disc loss: 0.003945034874121239, policy loss: 6.245887434659654
Experience 12, Iter 21, disc loss: 0.003954317660314236, policy loss: 6.377932214501383
Experience 12, Iter 22, disc loss: 0.003678497805336068, policy loss: 6.3664488645052995
Experience 12, Iter 23, disc loss: 0.0036932995260005137, policy loss: 6.399332681181049
Experience 12, Iter 24, disc loss: 0.0039480508197838665, policy loss: 6.262826400162303
Experience 12, Iter 25, disc loss: 0.003910695792602846, policy loss: 6.227051599875219
Experience 12, Iter 26, disc loss: 0.0036808441401499133, policy loss: 6.422703678235376
Experience 12, Iter 27, disc loss: 0.004427791724128791, policy loss: 6.050994394410504
Experience 12, Iter 28, disc loss: 0.004161078599245297, policy loss: 6.192417790971552
Experience 12, Iter 29, disc loss: 0.00439278399283468, policy loss: 6.048259192416846
Experience 12, Iter 30, disc loss: 0.003962392405663116, policy loss: 6.20467960288541
Experience 12, Iter 31, disc loss: 0.004042823194564149, policy loss: 6.346160100028154
Experience 12, Iter 32, disc loss: 0.004174155979086622, policy loss: 6.709461614229912
Experience 12, Iter 33, disc loss: 0.003417133425272886, policy loss: 6.45788100220755
Experience 12, Iter 34, disc loss: 0.004384961632267951, policy loss: 6.10782407327057
Experience 12, Iter 35, disc loss: 0.003740710567874186, policy loss: 6.311121827756571
Experience 12, Iter 36, disc loss: 0.0035071098917941234, policy loss: 6.332687874278477
Experience 12, Iter 37, disc loss: 0.004354058725993202, policy loss: 6.132101525404457
Experience 12, Iter 38, disc loss: 0.003530502577877775, policy loss: 6.413616404475455
Experience 12, Iter 39, disc loss: 0.003947546337295083, policy loss: 6.161701856814207
Experience 12, Iter 40, disc loss: 0.004147715269473396, policy loss: 6.379081715286995
Experience 12, Iter 41, disc loss: 0.004175258396951079, policy loss: 6.0497249079104005
Experience 12, Iter 42, disc loss: 0.004478593975223687, policy loss: 6.052735358441073
Experience 12, Iter 43, disc loss: 0.0035081435297057454, policy loss: 6.422304983932287
Experience 12, Iter 44, disc loss: 0.0038073678738934163, policy loss: 6.175517043914572
Experience 12, Iter 45, disc loss: 0.00402202699335769, policy loss: 6.13029780527681
Experience 12, Iter 46, disc loss: 0.0039481529186202735, policy loss: 6.355568198424198
Experience 12, Iter 47, disc loss: 0.004187263168881008, policy loss: 6.076610505647224
Experience 12, Iter 48, disc loss: 0.004443420503418508, policy loss: 6.0215323083643275
Experience 12, Iter 49, disc loss: 0.003809759071425011, policy loss: 6.246501727635404
Experience 12, Iter 50, disc loss: 0.00431851080389041, policy loss: 6.214416001498833
Experience 12, Iter 51, disc loss: 0.00407818350222929, policy loss: 6.262362699819949
Experience 12, Iter 52, disc loss: 0.0036750954916886635, policy loss: 6.265006271996772
Experience 12, Iter 53, disc loss: 0.003993116075178852, policy loss: 6.41997940557115
Experience 12, Iter 54, disc loss: 0.003436266057504123, policy loss: 6.384724804020957
Experience 12, Iter 55, disc loss: 0.0033908868349327714, policy loss: 6.339681122302791
Experience 12, Iter 56, disc loss: 0.004694351984409363, policy loss: 6.131453592121414
Experience 12, Iter 57, disc loss: 0.0036566957555925275, policy loss: 6.3932817214562245
Experience 12, Iter 58, disc loss: 0.004093182900045336, policy loss: 6.150298540449091
Experience 12, Iter 59, disc loss: 0.004463020702643126, policy loss: 6.170617355327709
Experience 12, Iter 60, disc loss: 0.003996553090813366, policy loss: 6.253101133872473
Experience 12, Iter 61, disc loss: 0.004295912106395511, policy loss: 6.36979398549752
Experience 12, Iter 62, disc loss: 0.004047417292610775, policy loss: 6.24431069582318
Experience 12, Iter 63, disc loss: 0.004217619502718483, policy loss: 6.2170339365592096
Experience 12, Iter 64, disc loss: 0.004260171982640382, policy loss: 6.385438170630712
Experience 12, Iter 65, disc loss: 0.00330210851217251, policy loss: 6.560770465174356
Experience 12, Iter 66, disc loss: 0.004279644620314856, policy loss: 6.316875305914476
Experience 12, Iter 67, disc loss: 0.004293154560375836, policy loss: 6.00341960812767
Experience 12, Iter 68, disc loss: 0.0041272278027766705, policy loss: 6.323636648383896
Experience 12, Iter 69, disc loss: 0.004373446256298654, policy loss: 6.206067478343184
Experience 12, Iter 70, disc loss: 0.004166491960290467, policy loss: 6.126501644250838
Experience 12, Iter 71, disc loss: 0.004068850307949436, policy loss: 6.1886235975063055
Experience 12, Iter 72, disc loss: 0.004805356009918124, policy loss: 5.8309280014072105
Experience 12, Iter 73, disc loss: 0.003838393395839695, policy loss: 6.41647251363897
Experience 12, Iter 74, disc loss: 0.0041003146389756735, policy loss: 6.235694388743557
Experience 12, Iter 75, disc loss: 0.003952310655484509, policy loss: 6.39087022536851
Experience 12, Iter 76, disc loss: 0.004483825353890627, policy loss: 6.025937096990855
Experience 12, Iter 77, disc loss: 0.004059729658209141, policy loss: 6.443597334847852
Experience 12, Iter 78, disc loss: 0.003883390627588355, policy loss: 6.663193298631864
Experience 12, Iter 79, disc loss: 0.004131137014515238, policy loss: 6.149039046143353
Experience 12, Iter 80, disc loss: 0.004246822180734354, policy loss: 6.038175432911607
Experience 12, Iter 81, disc loss: 0.003969007673559116, policy loss: 6.236448923115093
Experience 12, Iter 82, disc loss: 0.0043966504551202126, policy loss: 5.98275172226233
Experience 12, Iter 83, disc loss: 0.003676150378049288, policy loss: 6.288292593656065
Experience 12, Iter 84, disc loss: 0.0038684896423962075, policy loss: 6.284845590715186
Experience 12, Iter 85, disc loss: 0.0042929757461183105, policy loss: 6.085969235835568
Experience 12, Iter 86, disc loss: 0.004069672447655122, policy loss: 6.266675372600596
Experience 12, Iter 87, disc loss: 0.003913710660008103, policy loss: 6.172191055137962
Experience 12, Iter 88, disc loss: 0.004085978834086192, policy loss: 6.167692565625116
Experience 12, Iter 89, disc loss: 0.004151205173589972, policy loss: 6.084782693377989
Experience 12, Iter 90, disc loss: 0.0035350263970634842, policy loss: 6.549240931376618
Experience 12, Iter 91, disc loss: 0.003525587777032232, policy loss: 6.5916321685491575
Experience 12, Iter 92, disc loss: 0.003719886645484182, policy loss: 6.387457028026898
Experience 12, Iter 93, disc loss: 0.004093152239115273, policy loss: 6.104352523225746
Experience 12, Iter 94, disc loss: 0.00415553943373523, policy loss: 6.111540049354039
Experience 12, Iter 95, disc loss: 0.0035166161746026433, policy loss: 6.4718660875875775
Experience 12, Iter 96, disc loss: 0.0038292679915082656, policy loss: 6.203862791417956
Experience 12, Iter 97, disc loss: 0.0038813803183909805, policy loss: 6.140603876708803
Experience 12, Iter 98, disc loss: 0.00400002233763556, policy loss: 6.161425923298463
Experience 12, Iter 99, disc loss: 0.0036848707567420265, policy loss: 6.267284092246488
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1190],
        [1.0141],
        [0.0106]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.3894e-03, 9.4797e-02, 5.3576e-01, 1.8427e-02, 2.5950e-03,
          3.3863e+00]],

        [[6.3894e-03, 9.4797e-02, 5.3576e-01, 1.8427e-02, 2.5950e-03,
          3.3863e+00]],

        [[6.3894e-03, 9.4797e-02, 5.3576e-01, 1.8427e-02, 2.5950e-03,
          3.3863e+00]],

        [[6.3894e-03, 9.4797e-02, 5.3576e-01, 1.8427e-02, 2.5950e-03,
          3.3863e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.4759, 4.0563, 0.0424], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.4759, 4.0563, 0.0424])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.252
Iter 2/2000 - Loss: 2.531
Iter 3/2000 - Loss: 2.216
Iter 4/2000 - Loss: 2.235
Iter 5/2000 - Loss: 2.337
Iter 6/2000 - Loss: 2.298
Iter 7/2000 - Loss: 2.189
Iter 8/2000 - Loss: 2.117
Iter 9/2000 - Loss: 2.119
Iter 10/2000 - Loss: 2.145
Iter 11/2000 - Loss: 2.125
Iter 12/2000 - Loss: 2.048
Iter 13/2000 - Loss: 1.951
Iter 14/2000 - Loss: 1.871
Iter 15/2000 - Loss: 1.812
Iter 16/2000 - Loss: 1.748
Iter 17/2000 - Loss: 1.651
Iter 18/2000 - Loss: 1.516
Iter 19/2000 - Loss: 1.358
Iter 20/2000 - Loss: 1.194
Iter 1981/2000 - Loss: -7.987
Iter 1982/2000 - Loss: -7.987
Iter 1983/2000 - Loss: -7.987
Iter 1984/2000 - Loss: -7.987
Iter 1985/2000 - Loss: -7.987
Iter 1986/2000 - Loss: -7.987
Iter 1987/2000 - Loss: -7.987
Iter 1988/2000 - Loss: -7.987
Iter 1989/2000 - Loss: -7.987
Iter 1990/2000 - Loss: -7.987
Iter 1991/2000 - Loss: -7.987
Iter 1992/2000 - Loss: -7.988
Iter 1993/2000 - Loss: -7.988
Iter 1994/2000 - Loss: -7.988
Iter 1995/2000 - Loss: -7.988
Iter 1996/2000 - Loss: -7.988
Iter 1997/2000 - Loss: -7.988
Iter 1998/2000 - Loss: -7.988
Iter 1999/2000 - Loss: -7.988
Iter 2000/2000 - Loss: -7.988
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.3206,  8.3272, 44.3082,  9.6732, 12.7757, 51.9656]],

        [[18.8703, 35.1625,  9.4964,  1.5179,  5.9810, 30.9422]],

        [[18.7417, 38.5963, 10.7514,  0.9626,  2.3327, 20.1841]],

        [[13.0699, 32.4088, 14.4518,  3.5449,  1.1746, 42.8718]]])
Signal Variance: tensor([ 0.1207,  2.5313, 15.0494,  0.3420])
Estimated target variance: tensor([0.0109, 0.4759, 4.0563, 0.0424])
N: 130
Signal to noise ratio: tensor([19.9070, 90.5077, 92.3058, 32.3815])
Bound on condition number: tensor([  51518.3907, 1064914.1798, 1107647.4772,  136314.2284])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0039797489136078786, policy loss: 6.17068451626106
Experience 13, Iter 1, disc loss: 0.004495913093488974, policy loss: 5.8345795337002055
Experience 13, Iter 2, disc loss: 0.003570546693646378, policy loss: 6.566898326939283
Experience 13, Iter 3, disc loss: 0.003757712033786185, policy loss: 6.223480634455788
Experience 13, Iter 4, disc loss: 0.00368385064675047, policy loss: 6.496595014012515
Experience 13, Iter 5, disc loss: 0.0039430900544069935, policy loss: 6.336905700738662
Experience 13, Iter 6, disc loss: 0.0038928050164135615, policy loss: 6.36877373341093
Experience 13, Iter 7, disc loss: 0.003975128129081537, policy loss: 6.156165342611423
Experience 13, Iter 8, disc loss: 0.0037551150161511276, policy loss: 6.286988138469346
Experience 13, Iter 9, disc loss: 0.0037070839306661075, policy loss: 6.2683814786771705
Experience 13, Iter 10, disc loss: 0.004067885114585779, policy loss: 6.053064843147237
Experience 13, Iter 11, disc loss: 0.003636336962729844, policy loss: 6.316435836203242
Experience 13, Iter 12, disc loss: 0.004029500040375838, policy loss: 6.196177615543003
Experience 13, Iter 13, disc loss: 0.003835717282015161, policy loss: 6.1728619439539685
Experience 13, Iter 14, disc loss: 0.003612428463325326, policy loss: 6.241606013868672
Experience 13, Iter 15, disc loss: 0.004122707822268101, policy loss: 6.014660164658939
Experience 13, Iter 16, disc loss: 0.0035711614904616745, policy loss: 6.427775860076957
Experience 13, Iter 17, disc loss: 0.004030204414586898, policy loss: 6.124159123836816
Experience 13, Iter 18, disc loss: 0.0036341463856297447, policy loss: 6.42116811716338
Experience 13, Iter 19, disc loss: 0.003866611299944234, policy loss: 6.178668114910279
Experience 13, Iter 20, disc loss: 0.003681524260935468, policy loss: 6.304515171604634
Experience 13, Iter 21, disc loss: 0.003464318171792519, policy loss: 6.519240943827446
Experience 13, Iter 22, disc loss: 0.0034689544034808353, policy loss: 6.364863318657017
Experience 13, Iter 23, disc loss: 0.003869786107918078, policy loss: 6.077748450593486
Experience 13, Iter 24, disc loss: 0.0035484597858878005, policy loss: 6.330799440630636
Experience 13, Iter 25, disc loss: 0.0040793930456964255, policy loss: 6.1626236103434255
Experience 13, Iter 26, disc loss: 0.003743867830372319, policy loss: 6.204779928187043
Experience 13, Iter 27, disc loss: 0.003471518164933774, policy loss: 6.346742980437876
Experience 13, Iter 28, disc loss: 0.0035603871892214583, policy loss: 6.3525743506526755
Experience 13, Iter 29, disc loss: 0.003170982407022297, policy loss: 6.50973186322753
Experience 13, Iter 30, disc loss: 0.003750166573158799, policy loss: 6.172758614285813
Experience 13, Iter 31, disc loss: 0.00380716806466326, policy loss: 6.3461378133047415
Experience 13, Iter 32, disc loss: 0.003250138159494676, policy loss: 6.630338619239803
Experience 13, Iter 33, disc loss: 0.003644336416955484, policy loss: 6.290248352385046
Experience 13, Iter 34, disc loss: 0.0036006675265115506, policy loss: 6.292172545135276
Experience 13, Iter 35, disc loss: 0.0034158832860291177, policy loss: 6.436799359146709
Experience 13, Iter 36, disc loss: 0.0035931974597502234, policy loss: 6.280932286299768
Experience 13, Iter 37, disc loss: 0.003554963332086623, policy loss: 6.442347383452938
Experience 13, Iter 38, disc loss: 0.0033541836163135364, policy loss: 6.3136863059103785
Experience 13, Iter 39, disc loss: 0.002813125818603419, policy loss: 6.673830124153738
Experience 13, Iter 40, disc loss: 0.003402096380946146, policy loss: 6.401099013332872
Experience 13, Iter 41, disc loss: 0.002761695938484558, policy loss: 6.755406442827206
Experience 13, Iter 42, disc loss: 0.003403150105929631, policy loss: 6.410814752639436
Experience 13, Iter 43, disc loss: 0.0025359201232840025, policy loss: 6.837043162536807
Experience 13, Iter 44, disc loss: 0.0023913340533623564, policy loss: 6.855682823783266
Experience 13, Iter 45, disc loss: 0.0034773548890037554, policy loss: 6.257071550908832
Experience 13, Iter 46, disc loss: 0.0029375482525180047, policy loss: 6.620036455687828
Experience 13, Iter 47, disc loss: 0.003397476951182893, policy loss: 6.34014510231097
Experience 13, Iter 48, disc loss: 0.0030364179401496582, policy loss: 6.496727167238919
Experience 13, Iter 49, disc loss: 0.0026552916950833534, policy loss: 6.845422707229912
Experience 13, Iter 50, disc loss: 0.0029425240522665858, policy loss: 6.549763937793672
Experience 13, Iter 51, disc loss: 0.003252591213815931, policy loss: 6.501143975839681
Experience 13, Iter 52, disc loss: 0.0027724760424460535, policy loss: 6.701967146758971
Experience 13, Iter 53, disc loss: 0.0036102084034964565, policy loss: 6.267320035533704
Experience 13, Iter 54, disc loss: 0.0029626306763379257, policy loss: 6.425650962368595
Experience 13, Iter 55, disc loss: 0.0029719687797157266, policy loss: 6.504734333300341
Experience 13, Iter 56, disc loss: 0.0034527123208680397, policy loss: 6.335821429791615
Experience 13, Iter 57, disc loss: 0.0032421882295729353, policy loss: 6.511129700713747
Experience 13, Iter 58, disc loss: 0.002953517233484592, policy loss: 6.551906204992081
Experience 13, Iter 59, disc loss: 0.003457552637652677, policy loss: 6.321703501656973
Experience 13, Iter 60, disc loss: 0.0027971471469962267, policy loss: 6.563386290942665
Experience 13, Iter 61, disc loss: 0.0028348022386729546, policy loss: 6.531338506162875
Experience 13, Iter 62, disc loss: 0.0034021908573127957, policy loss: 6.4903012329942396
Experience 13, Iter 63, disc loss: 0.0028045311150660668, policy loss: 6.871204671330869
Experience 13, Iter 64, disc loss: 0.0031775065432948493, policy loss: 6.423701108968803
Experience 13, Iter 65, disc loss: 0.002851234937344808, policy loss: 6.671510479494259
Experience 13, Iter 66, disc loss: 0.002206217667740931, policy loss: 7.18112076253766
Experience 13, Iter 67, disc loss: 0.002327402679260973, policy loss: 7.0696295728720155
Experience 13, Iter 68, disc loss: 0.0027690471782887676, policy loss: 6.710326019582198
Experience 13, Iter 69, disc loss: 0.003074919600796503, policy loss: 6.548994711210055
Experience 13, Iter 70, disc loss: 0.0029522274380937193, policy loss: 6.573974912066736
Experience 13, Iter 71, disc loss: 0.0032454952063216665, policy loss: 6.678975786720893
Experience 13, Iter 72, disc loss: 0.0031941115542901835, policy loss: 6.343483624593567
Experience 13, Iter 73, disc loss: 0.0027699367813704655, policy loss: 6.617466331707859
Experience 13, Iter 74, disc loss: 0.0032812050171690103, policy loss: 6.341711772987232
Experience 13, Iter 75, disc loss: 0.0031951109555641687, policy loss: 6.342457239863279
Experience 13, Iter 76, disc loss: 0.0032050046049009206, policy loss: 6.339746500428864
Experience 13, Iter 77, disc loss: 0.003013937394568744, policy loss: 6.490568265692859
Experience 13, Iter 78, disc loss: 0.002813327390079425, policy loss: 6.60579589808557
Experience 13, Iter 79, disc loss: 0.002987537813839008, policy loss: 6.406350795264985
Experience 13, Iter 80, disc loss: 0.0029908101987838227, policy loss: 6.69430112175295
Experience 13, Iter 81, disc loss: 0.0028234955089592737, policy loss: 6.60836867153076
Experience 13, Iter 82, disc loss: 0.003271453909615694, policy loss: 6.374588114462772
Experience 13, Iter 83, disc loss: 0.0028898861354498193, policy loss: 6.5404308775233675
Experience 13, Iter 84, disc loss: 0.0027632069926743596, policy loss: 6.68760875805884
Experience 13, Iter 85, disc loss: 0.003210190586382926, policy loss: 6.300597813557139
Experience 13, Iter 86, disc loss: 0.0030748903478981008, policy loss: 6.375985195065331
Experience 13, Iter 87, disc loss: 0.0033137635902141097, policy loss: 6.270543514176509
Experience 13, Iter 88, disc loss: 0.0029862482239882485, policy loss: 6.496610422824142
Experience 13, Iter 89, disc loss: 0.0028306027551598953, policy loss: 6.643877445254393
Experience 13, Iter 90, disc loss: 0.0028433371494897765, policy loss: 6.6255764995270034
Experience 13, Iter 91, disc loss: 0.003200921141459985, policy loss: 6.377720232992542
Experience 13, Iter 92, disc loss: 0.00308946838235182, policy loss: 6.50156700104798
Experience 13, Iter 93, disc loss: 0.0028875385577917024, policy loss: 6.529479282645534
Experience 13, Iter 94, disc loss: 0.003090611623876614, policy loss: 6.549383254274504
Experience 13, Iter 95, disc loss: 0.0030724823843539407, policy loss: 6.3320112518476845
Experience 13, Iter 96, disc loss: 0.0028485378548239815, policy loss: 6.499554003618302
Experience 13, Iter 97, disc loss: 0.003032619012165858, policy loss: 6.516639442160381
Experience 13, Iter 98, disc loss: 0.0031251071628197995, policy loss: 6.348632989131121
Experience 13, Iter 99, disc loss: 0.0028133344224875226, policy loss: 6.727758155158048
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1147],
        [0.9674],
        [0.0102]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.0977e-03, 9.0929e-02, 5.1471e-01, 1.7972e-02, 2.4934e-03,
          3.3009e+00]],

        [[6.0977e-03, 9.0929e-02, 5.1471e-01, 1.7972e-02, 2.4934e-03,
          3.3009e+00]],

        [[6.0977e-03, 9.0929e-02, 5.1471e-01, 1.7972e-02, 2.4934e-03,
          3.3009e+00]],

        [[6.0977e-03, 9.0929e-02, 5.1471e-01, 1.7972e-02, 2.4934e-03,
          3.3009e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0105, 0.4590, 3.8697, 0.0407], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0105, 0.4590, 3.8697, 0.0407])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.179
Iter 2/2000 - Loss: 2.479
Iter 3/2000 - Loss: 2.144
Iter 4/2000 - Loss: 2.167
Iter 5/2000 - Loss: 2.276
Iter 6/2000 - Loss: 2.239
Iter 7/2000 - Loss: 2.126
Iter 8/2000 - Loss: 2.050
Iter 9/2000 - Loss: 2.050
Iter 10/2000 - Loss: 2.080
Iter 11/2000 - Loss: 2.069
Iter 12/2000 - Loss: 1.999
Iter 13/2000 - Loss: 1.900
Iter 14/2000 - Loss: 1.814
Iter 15/2000 - Loss: 1.753
Iter 16/2000 - Loss: 1.693
Iter 17/2000 - Loss: 1.603
Iter 18/2000 - Loss: 1.472
Iter 19/2000 - Loss: 1.314
Iter 20/2000 - Loss: 1.147
Iter 1981/2000 - Loss: -8.113
Iter 1982/2000 - Loss: -8.113
Iter 1983/2000 - Loss: -8.113
Iter 1984/2000 - Loss: -8.113
Iter 1985/2000 - Loss: -8.113
Iter 1986/2000 - Loss: -8.113
Iter 1987/2000 - Loss: -8.113
Iter 1988/2000 - Loss: -8.113
Iter 1989/2000 - Loss: -8.113
Iter 1990/2000 - Loss: -8.113
Iter 1991/2000 - Loss: -8.113
Iter 1992/2000 - Loss: -8.113
Iter 1993/2000 - Loss: -8.113
Iter 1994/2000 - Loss: -8.113
Iter 1995/2000 - Loss: -8.113
Iter 1996/2000 - Loss: -8.113
Iter 1997/2000 - Loss: -8.114
Iter 1998/2000 - Loss: -8.114
Iter 1999/2000 - Loss: -8.114
Iter 2000/2000 - Loss: -8.114
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[13.0784,  8.3801, 43.7922,  9.6937, 12.7859, 49.8122]],

        [[18.1555, 34.9260,  9.5004,  1.4620,  6.6834, 30.1860]],

        [[18.2041, 38.3843, 10.8802,  0.9624,  2.3026, 20.3099]],

        [[11.7469, 32.4946, 15.2210,  3.6545,  1.1719, 44.8002]]])
Signal Variance: tensor([ 0.1188,  2.3802, 15.1328,  0.3756])
Estimated target variance: tensor([0.0105, 0.4590, 3.8697, 0.0407])
N: 140
Signal to noise ratio: tensor([20.1699, 89.4219, 93.5262, 33.4174])
Bound on condition number: tensor([  56956.4630, 1119480.4440, 1224602.9411,  156342.3285])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0029617147414758172, policy loss: 6.672056399052039
Experience 14, Iter 1, disc loss: 0.0031107829149623085, policy loss: 6.265160401469007
Experience 14, Iter 2, disc loss: 0.0029621487328448355, policy loss: 6.509853016765269
Experience 14, Iter 3, disc loss: 0.0029850814077377772, policy loss: 6.499308220784749
Experience 14, Iter 4, disc loss: 0.002949551974278373, policy loss: 6.406280249558951
Experience 14, Iter 5, disc loss: 0.0031788721405677824, policy loss: 6.2798751380125655
Experience 14, Iter 6, disc loss: 0.002952571460659106, policy loss: 6.499009686273273
Experience 14, Iter 7, disc loss: 0.0028632147982738114, policy loss: 6.552150023058982
Experience 14, Iter 8, disc loss: 0.003087177634648899, policy loss: 6.410120480870068
Experience 14, Iter 9, disc loss: 0.0029596868501372236, policy loss: 6.392725137739694
Experience 14, Iter 10, disc loss: 0.002821698814999456, policy loss: 6.593516209239342
Experience 14, Iter 11, disc loss: 0.003029735775915331, policy loss: 6.484503219609896
Experience 14, Iter 12, disc loss: 0.0028560855636521787, policy loss: 6.518969996443483
Experience 14, Iter 13, disc loss: 0.0028073317853321575, policy loss: 6.525155093851026
Experience 14, Iter 14, disc loss: 0.0027839618256573504, policy loss: 6.623920986274092
Experience 14, Iter 15, disc loss: 0.00296773392163185, policy loss: 6.514057952193026
Experience 14, Iter 16, disc loss: 0.0029625576524997437, policy loss: 6.3939972462215025
Experience 14, Iter 17, disc loss: 0.002733734813245124, policy loss: 6.675505776302439
Experience 14, Iter 18, disc loss: 0.0027649575176211783, policy loss: 6.636332856035093
Experience 14, Iter 19, disc loss: 0.003029023429794374, policy loss: 6.323748739573831
Experience 14, Iter 20, disc loss: 0.002801617763640649, policy loss: 6.543343698324312
Experience 14, Iter 21, disc loss: 0.0030439490858001784, policy loss: 6.454514897994947
Experience 14, Iter 22, disc loss: 0.002889372336553746, policy loss: 6.606334339886149
Experience 14, Iter 23, disc loss: 0.0026615066607397465, policy loss: 6.669358600507655
Experience 14, Iter 24, disc loss: 0.0028711200188152796, policy loss: 6.460701562732503
Experience 14, Iter 25, disc loss: 0.002824116199160701, policy loss: 6.582769354491164
Experience 14, Iter 26, disc loss: 0.002869432351145799, policy loss: 6.565858063355526
Experience 14, Iter 27, disc loss: 0.0029866644362200735, policy loss: 6.345225799757592
Experience 14, Iter 28, disc loss: 0.00272555975037815, policy loss: 6.827104945276212
Experience 14, Iter 29, disc loss: 0.0027589603724052438, policy loss: 6.520467338306419
Experience 14, Iter 30, disc loss: 0.0028562316791149317, policy loss: 6.613618689940205
Experience 14, Iter 31, disc loss: 0.0026688603894888013, policy loss: 6.7263515598627865
Experience 14, Iter 32, disc loss: 0.002802570976050099, policy loss: 6.571717928053369
Experience 14, Iter 33, disc loss: 0.0027808042528021165, policy loss: 6.5489003875567295
Experience 14, Iter 34, disc loss: 0.00269248422777206, policy loss: 6.712199014114329
Experience 14, Iter 35, disc loss: 0.002736463491257445, policy loss: 6.712241252492329
Experience 14, Iter 36, disc loss: 0.0024311062256803817, policy loss: 6.898969380976217
Experience 14, Iter 37, disc loss: 0.0027356624987638543, policy loss: 6.803758614722657
Experience 14, Iter 38, disc loss: 0.0025775229967778027, policy loss: 6.595720822943946
Experience 14, Iter 39, disc loss: 0.002410953670592482, policy loss: 6.751029227412617
Experience 14, Iter 40, disc loss: 0.0026909658025933776, policy loss: 6.560390197998682
Experience 14, Iter 41, disc loss: 0.0024611639905088674, policy loss: 6.867244735508824
Experience 14, Iter 42, disc loss: 0.0024986767548589115, policy loss: 6.976095138212617
Experience 14, Iter 43, disc loss: 0.0027392393766485744, policy loss: 6.49869134889
Experience 14, Iter 44, disc loss: 0.0026596865294593614, policy loss: 6.556819211631918
Experience 14, Iter 45, disc loss: 0.002729764623234331, policy loss: 6.458103825098963
Experience 14, Iter 46, disc loss: 0.0028115038800036823, policy loss: 6.407598276933132
Experience 14, Iter 47, disc loss: 0.0025734793948484283, policy loss: 6.641766381730223
Experience 14, Iter 48, disc loss: 0.002855458068401115, policy loss: 6.5665303874406655
Experience 14, Iter 49, disc loss: 0.0025982088830320497, policy loss: 6.623042419959353
Experience 14, Iter 50, disc loss: 0.002529558351282149, policy loss: 6.585691467059224
Experience 14, Iter 51, disc loss: 0.0024462865102547176, policy loss: 6.786580278472387
Experience 14, Iter 52, disc loss: 0.0026803363239820653, policy loss: 6.5466673099421255
Experience 14, Iter 53, disc loss: 0.0027834019917151636, policy loss: 6.4283240095994145
Experience 14, Iter 54, disc loss: 0.0025975281243681707, policy loss: 6.69550140883842
Experience 14, Iter 55, disc loss: 0.002566310151370218, policy loss: 6.620104941820775
Experience 14, Iter 56, disc loss: 0.0024813800987901168, policy loss: 6.705430638984051
Experience 14, Iter 57, disc loss: 0.0027518035182635577, policy loss: 6.4551956521658544
Experience 14, Iter 58, disc loss: 0.002268997344559161, policy loss: 7.050331514445932
Experience 14, Iter 59, disc loss: 0.002800790574273888, policy loss: 6.39438905430966
Experience 14, Iter 60, disc loss: 0.002438200936409101, policy loss: 6.77498412940448
Experience 14, Iter 61, disc loss: 0.002368326338759298, policy loss: 6.840646333026326
Experience 14, Iter 62, disc loss: 0.0025417703247660193, policy loss: 6.681187643118068
Experience 14, Iter 63, disc loss: 0.002558049812659776, policy loss: 6.6101937383164415
Experience 14, Iter 64, disc loss: 0.0024881580134965687, policy loss: 6.678233031604885
Experience 14, Iter 65, disc loss: 0.0025132366504005714, policy loss: 6.585870396447339
Experience 14, Iter 66, disc loss: 0.002703452835845839, policy loss: 6.460101250135553
Experience 14, Iter 67, disc loss: 0.002519552986543594, policy loss: 6.747684518747418
Experience 14, Iter 68, disc loss: 0.002340995525804865, policy loss: 6.913171163936578
Experience 14, Iter 69, disc loss: 0.0023862816080697444, policy loss: 6.772045854543745
Experience 14, Iter 70, disc loss: 0.002289618997835899, policy loss: 6.79390678370041
Experience 14, Iter 71, disc loss: 0.0025057059193594983, policy loss: 6.77560469454505
Experience 14, Iter 72, disc loss: 0.0020180334541417353, policy loss: 7.017229690168621
Experience 14, Iter 73, disc loss: 0.0024306690522144887, policy loss: 6.699150924793273
Experience 14, Iter 74, disc loss: 0.00220291433541087, policy loss: 6.830850815439675
Experience 14, Iter 75, disc loss: 0.0021546107369351144, policy loss: 6.845297016507773
Experience 14, Iter 76, disc loss: 0.002332824115498275, policy loss: 6.823061490950082
Experience 14, Iter 77, disc loss: 0.002168549909205077, policy loss: 6.955920774098614
Experience 14, Iter 78, disc loss: 0.0020098366407605523, policy loss: 7.104473015796408
Experience 14, Iter 79, disc loss: 0.001911712272887794, policy loss: 7.124450381715579
Experience 14, Iter 80, disc loss: 0.001961708929209246, policy loss: 6.949076950199737
Experience 14, Iter 81, disc loss: 0.0019627523186411377, policy loss: 7.112786920008984
Experience 14, Iter 82, disc loss: 0.0025626594663522234, policy loss: 6.666481406553633
Experience 14, Iter 83, disc loss: 0.0023594985083028558, policy loss: 6.6443538695528295
Experience 14, Iter 84, disc loss: 0.0021545748683250722, policy loss: 6.812105276316906
Experience 14, Iter 85, disc loss: 0.0020266841817604647, policy loss: 7.1108548967113645
Experience 14, Iter 86, disc loss: 0.002079227376753824, policy loss: 7.106227998023547
Experience 14, Iter 87, disc loss: 0.002241837656233561, policy loss: 6.800617585403948
Experience 14, Iter 88, disc loss: 0.0021463462537951467, policy loss: 6.938976117617621
Experience 14, Iter 89, disc loss: 0.002264024875374455, policy loss: 6.656261167593403
Experience 14, Iter 90, disc loss: 0.002504463916502857, policy loss: 6.525158533447678
Experience 14, Iter 91, disc loss: 0.002502138634134997, policy loss: 6.475196160399914
Experience 14, Iter 92, disc loss: 0.0023272084147637428, policy loss: 6.7124035443420915
Experience 14, Iter 93, disc loss: 0.0024342530385063356, policy loss: 6.707552581411371
Experience 14, Iter 94, disc loss: 0.0020111954031385303, policy loss: 6.800050647018097
Experience 14, Iter 95, disc loss: 0.0021063921601440086, policy loss: 6.908262418330954
Experience 14, Iter 96, disc loss: 0.0023059147780951435, policy loss: 6.8028745047765
Experience 14, Iter 97, disc loss: 0.002338427964209534, policy loss: 6.674166173918618
Experience 14, Iter 98, disc loss: 0.002292456794799765, policy loss: 6.728663390455861
Experience 14, Iter 99, disc loss: 0.0023686438249665093, policy loss: 6.658280431440921
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1136],
        [0.9423],
        [0.0097]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.7554e-03, 8.6782e-02, 4.9194e-01, 1.7388e-02, 2.3801e-03,
          3.2721e+00]],

        [[5.7554e-03, 8.6782e-02, 4.9194e-01, 1.7388e-02, 2.3801e-03,
          3.2721e+00]],

        [[5.7554e-03, 8.6782e-02, 4.9194e-01, 1.7388e-02, 2.3801e-03,
          3.2721e+00]],

        [[5.7554e-03, 8.6782e-02, 4.9194e-01, 1.7388e-02, 2.3801e-03,
          3.2721e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0101, 0.4544, 3.7694, 0.0387], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0101, 0.4544, 3.7694, 0.0387])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.124
Iter 2/2000 - Loss: 2.435
Iter 3/2000 - Loss: 2.088
Iter 4/2000 - Loss: 2.114
Iter 5/2000 - Loss: 2.227
Iter 6/2000 - Loss: 2.189
Iter 7/2000 - Loss: 2.077
Iter 8/2000 - Loss: 2.000
Iter 9/2000 - Loss: 2.001
Iter 10/2000 - Loss: 2.035
Iter 11/2000 - Loss: 2.031
Iter 12/2000 - Loss: 1.966
Iter 13/2000 - Loss: 1.870
Iter 14/2000 - Loss: 1.785
Iter 15/2000 - Loss: 1.727
Iter 16/2000 - Loss: 1.674
Iter 17/2000 - Loss: 1.593
Iter 18/2000 - Loss: 1.468
Iter 19/2000 - Loss: 1.313
Iter 20/2000 - Loss: 1.146
Iter 1981/2000 - Loss: -8.190
Iter 1982/2000 - Loss: -8.190
Iter 1983/2000 - Loss: -8.190
Iter 1984/2000 - Loss: -8.190
Iter 1985/2000 - Loss: -8.190
Iter 1986/2000 - Loss: -8.190
Iter 1987/2000 - Loss: -8.190
Iter 1988/2000 - Loss: -8.190
Iter 1989/2000 - Loss: -8.190
Iter 1990/2000 - Loss: -8.190
Iter 1991/2000 - Loss: -8.190
Iter 1992/2000 - Loss: -8.190
Iter 1993/2000 - Loss: -8.190
Iter 1994/2000 - Loss: -8.190
Iter 1995/2000 - Loss: -8.191
Iter 1996/2000 - Loss: -8.191
Iter 1997/2000 - Loss: -8.191
Iter 1998/2000 - Loss: -8.191
Iter 1999/2000 - Loss: -8.191
Iter 2000/2000 - Loss: -8.191
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.7648,  8.3882, 42.7877,  9.5621, 12.5381, 49.7984]],

        [[17.7885, 34.2232,  9.3808,  1.4839,  5.6139, 29.9442]],

        [[17.5784, 36.9864, 10.8587,  0.9705,  2.3915, 20.1166]],

        [[11.5693, 30.7283, 14.8229,  3.5376,  1.3204, 43.4939]]])
Signal Variance: tensor([ 0.1173,  2.3786, 15.3302,  0.3595])
Estimated target variance: tensor([0.0101, 0.4544, 3.7694, 0.0387])
N: 150
Signal to noise ratio: tensor([20.4081, 91.0168, 90.7610, 32.2759])
Bound on condition number: tensor([  62474.5205, 1242608.6432, 1235635.9257,  156260.7487])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0020889556182084497, policy loss: 6.812602940008787
Experience 15, Iter 1, disc loss: 0.00204116262003378, policy loss: 6.9932157384224
Experience 15, Iter 2, disc loss: 0.0023055178002098892, policy loss: 6.771774694000785
Experience 15, Iter 3, disc loss: 0.002221460082403233, policy loss: 6.7440870449345
Experience 15, Iter 4, disc loss: 0.00206729406529387, policy loss: 6.873581396998193
Experience 15, Iter 5, disc loss: 0.002350289327025403, policy loss: 6.5913007064202604
Experience 15, Iter 6, disc loss: 0.0021089132542477214, policy loss: 6.904699747299885
Experience 15, Iter 7, disc loss: 0.002366550583084486, policy loss: 6.516922778319218
Experience 15, Iter 8, disc loss: 0.002346759541176647, policy loss: 6.720762270189806
Experience 15, Iter 9, disc loss: 0.0023006741102563426, policy loss: 6.585476201580704
Experience 15, Iter 10, disc loss: 0.0022706903274267013, policy loss: 6.605144076792773
Experience 15, Iter 11, disc loss: 0.00229026814232886, policy loss: 6.741837018482379
Experience 15, Iter 12, disc loss: 0.0022068168795456606, policy loss: 6.830184200677328
Experience 15, Iter 13, disc loss: 0.0023504569930177573, policy loss: 6.685143529269728
Experience 15, Iter 14, disc loss: 0.002017831881405927, policy loss: 7.020545499288489
Experience 15, Iter 15, disc loss: 0.0020390592994699074, policy loss: 6.856807253072022
Experience 15, Iter 16, disc loss: 0.0023236686392882765, policy loss: 6.693253067020331
Experience 15, Iter 17, disc loss: 0.002070612099282301, policy loss: 7.024328002891664
Experience 15, Iter 18, disc loss: 0.0023202875834645318, policy loss: 6.664652659149619
Experience 15, Iter 19, disc loss: 0.0021912056785856862, policy loss: 6.7670529377649435
Experience 15, Iter 20, disc loss: 0.0023175588279316975, policy loss: 6.629789514561773
Experience 15, Iter 21, disc loss: 0.002323561512474334, policy loss: 6.60088553982294
Experience 15, Iter 22, disc loss: 0.0021986912684571155, policy loss: 6.785555284548824
Experience 15, Iter 23, disc loss: 0.0021230638436182782, policy loss: 6.813299911514147
Experience 15, Iter 24, disc loss: 0.0021232530482448962, policy loss: 6.8990416822081055
Experience 15, Iter 25, disc loss: 0.002233687199305445, policy loss: 6.665035639109481
Experience 15, Iter 26, disc loss: 0.0022355937295413754, policy loss: 6.654638424923689
Experience 15, Iter 27, disc loss: 0.0023562563124266992, policy loss: 6.768140961384311
Experience 15, Iter 28, disc loss: 0.0020171608913387906, policy loss: 6.99830310854829
Experience 15, Iter 29, disc loss: 0.002273278085651056, policy loss: 6.655606659389495
Experience 15, Iter 30, disc loss: 0.0021208687540045313, policy loss: 6.794933607822321
Experience 15, Iter 31, disc loss: 0.0020418195614349695, policy loss: 6.967384146938889
Experience 15, Iter 32, disc loss: 0.0021252635757387753, policy loss: 6.893872142793912
Experience 15, Iter 33, disc loss: 0.002033857555049648, policy loss: 6.796587329565668
Experience 15, Iter 34, disc loss: 0.0020853032083700935, policy loss: 7.0092452436770305
Experience 15, Iter 35, disc loss: 0.0020881059946030404, policy loss: 6.93456288167746
Experience 15, Iter 36, disc loss: 0.00213874309843449, policy loss: 6.829315307495391
Experience 15, Iter 37, disc loss: 0.0020799460435958267, policy loss: 6.749816662593496
Experience 15, Iter 38, disc loss: 0.0021709970628122225, policy loss: 6.717283430544945
Experience 15, Iter 39, disc loss: 0.00195941132463237, policy loss: 7.107577780816131
Experience 15, Iter 40, disc loss: 0.002130751020370253, policy loss: 6.772479767896623
Experience 15, Iter 41, disc loss: 0.001602936356516511, policy loss: 7.318719847171133
Experience 15, Iter 42, disc loss: 0.0017833228461986126, policy loss: 7.12050378721301
Experience 15, Iter 43, disc loss: 0.001781623009825263, policy loss: 7.274577030416662
Experience 15, Iter 44, disc loss: 0.002042465126259568, policy loss: 7.016253374569338
Experience 15, Iter 45, disc loss: 0.0019826932043590095, policy loss: 7.100846867795698
Experience 15, Iter 46, disc loss: 0.001621584048246583, policy loss: 7.0467925465250065
Experience 15, Iter 47, disc loss: 0.0015010637830407316, policy loss: 7.323841987760704
Experience 15, Iter 48, disc loss: 0.001735900162131944, policy loss: 7.128182254601615
Experience 15, Iter 49, disc loss: 0.0018576854066621024, policy loss: 6.961583433889117
Experience 15, Iter 50, disc loss: 0.0018703062495710797, policy loss: 6.9908978066889365
Experience 15, Iter 51, disc loss: 0.0020509549177824215, policy loss: 6.720107103717525
Experience 15, Iter 52, disc loss: 0.0016742070332417232, policy loss: 7.099073212935291
Experience 15, Iter 53, disc loss: 0.0019176827012271926, policy loss: 6.772058104349631
Experience 15, Iter 54, disc loss: 0.0019052073333703954, policy loss: 7.003882243936737
Experience 15, Iter 55, disc loss: 0.0019852571706547428, policy loss: 6.8382081075307735
Experience 15, Iter 56, disc loss: 0.0018361933299318373, policy loss: 6.8483282549576465
Experience 15, Iter 57, disc loss: 0.0017138678358942622, policy loss: 6.97528581089545
Experience 15, Iter 58, disc loss: 0.0018834879385831059, policy loss: 6.921625187472431
Experience 15, Iter 59, disc loss: 0.0019576466987165806, policy loss: 6.832735824802933
Experience 15, Iter 60, disc loss: 0.0019351857066130843, policy loss: 6.96159386499207
Experience 15, Iter 61, disc loss: 0.0017579785557748791, policy loss: 7.128163536889067
Experience 15, Iter 62, disc loss: 0.0018351609053816522, policy loss: 7.013345925444198
Experience 15, Iter 63, disc loss: 0.0018997885015121403, policy loss: 6.868832194980758
Experience 15, Iter 64, disc loss: 0.0018788741164992013, policy loss: 6.884240685705505
Experience 15, Iter 65, disc loss: 0.001849726740839015, policy loss: 6.889780431053465
Experience 15, Iter 66, disc loss: 0.0017908735381955646, policy loss: 7.126193675191107
Experience 15, Iter 67, disc loss: 0.00161950975657946, policy loss: 7.069525978078841
Experience 15, Iter 68, disc loss: 0.0019529144605679308, policy loss: 6.805627704522889
Experience 15, Iter 69, disc loss: 0.0018817910404391506, policy loss: 6.943533840294085
Experience 15, Iter 70, disc loss: 0.001933829720406525, policy loss: 6.717626349594912
Experience 15, Iter 71, disc loss: 0.0018368584451825631, policy loss: 7.012082219455134
Experience 15, Iter 72, disc loss: 0.0018758032132585043, policy loss: 6.847142326476591
Experience 15, Iter 73, disc loss: 0.0020024807618797135, policy loss: 6.753669501825276
Experience 15, Iter 74, disc loss: 0.0019975445798273094, policy loss: 6.86366079151719
Experience 15, Iter 75, disc loss: 0.0019290925601841203, policy loss: 6.8417068085553385
Experience 15, Iter 76, disc loss: 0.0018563355712258043, policy loss: 7.022399968409066
Experience 15, Iter 77, disc loss: 0.001835740321597397, policy loss: 6.989974435308647
Experience 15, Iter 78, disc loss: 0.001891861805387101, policy loss: 6.9664394356252615
Experience 15, Iter 79, disc loss: 0.001650763280619061, policy loss: 7.075995824602206
Experience 15, Iter 80, disc loss: 0.0018071039400054572, policy loss: 6.984699724552627
Experience 15, Iter 81, disc loss: 0.001839299804356904, policy loss: 7.14996461906051
Experience 15, Iter 82, disc loss: 0.001893810107620289, policy loss: 6.937690881903395
Experience 15, Iter 83, disc loss: 0.0019141083224375934, policy loss: 6.8955808748614755
Experience 15, Iter 84, disc loss: 0.0018359992204745731, policy loss: 6.849059488140432
Experience 15, Iter 85, disc loss: 0.0016712732774992831, policy loss: 7.10922694718124
Experience 15, Iter 86, disc loss: 0.0018439451797119281, policy loss: 6.981650723420014
Experience 15, Iter 87, disc loss: 0.0018754168193346214, policy loss: 6.88611325600653
Experience 15, Iter 88, disc loss: 0.001812264884750973, policy loss: 6.943465963032364
Experience 15, Iter 89, disc loss: 0.0019149636401693182, policy loss: 6.839479752425083
Experience 15, Iter 90, disc loss: 0.0018653629750907934, policy loss: 6.878746627790956
Experience 15, Iter 91, disc loss: 0.0016158263974545544, policy loss: 7.189582222930467
Experience 15, Iter 92, disc loss: 0.001823947160928357, policy loss: 6.957643916101011
Experience 15, Iter 93, disc loss: 0.0016817449135133663, policy loss: 7.194719097647163
Experience 15, Iter 94, disc loss: 0.001636456389041365, policy loss: 6.984522996455096
Experience 15, Iter 95, disc loss: 0.0018343351994299054, policy loss: 6.908504229630944
Experience 15, Iter 96, disc loss: 0.0017834680548276945, policy loss: 6.943498703189096
Experience 15, Iter 97, disc loss: 0.0018384349286149483, policy loss: 6.874491187308251
Experience 15, Iter 98, disc loss: 0.0019217511228927287, policy loss: 6.799038135432406
Experience 15, Iter 99, disc loss: 0.002017650172849953, policy loss: 6.6273838386911565
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1115],
        [0.9114],
        [0.0094]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.4379e-03, 8.6024e-02, 4.7747e-01, 1.7310e-02, 2.3129e-03,
          3.2576e+00]],

        [[5.4379e-03, 8.6024e-02, 4.7747e-01, 1.7310e-02, 2.3129e-03,
          3.2576e+00]],

        [[5.4379e-03, 8.6024e-02, 4.7747e-01, 1.7310e-02, 2.3129e-03,
          3.2576e+00]],

        [[5.4379e-03, 8.6024e-02, 4.7747e-01, 1.7310e-02, 2.3129e-03,
          3.2576e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0101, 0.4461, 3.6455, 0.0376], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0101, 0.4461, 3.6455, 0.0376])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.094
Iter 2/2000 - Loss: 2.414
Iter 3/2000 - Loss: 2.054
Iter 4/2000 - Loss: 2.081
Iter 5/2000 - Loss: 2.197
Iter 6/2000 - Loss: 2.162
Iter 7/2000 - Loss: 2.048
Iter 8/2000 - Loss: 1.963
Iter 9/2000 - Loss: 1.955
Iter 10/2000 - Loss: 1.987
Iter 11/2000 - Loss: 1.990
Iter 12/2000 - Loss: 1.931
Iter 13/2000 - Loss: 1.832
Iter 14/2000 - Loss: 1.736
Iter 15/2000 - Loss: 1.666
Iter 16/2000 - Loss: 1.609
Iter 17/2000 - Loss: 1.531
Iter 18/2000 - Loss: 1.410
Iter 19/2000 - Loss: 1.252
Iter 20/2000 - Loss: 1.076
Iter 1981/2000 - Loss: -8.300
Iter 1982/2000 - Loss: -8.300
Iter 1983/2000 - Loss: -8.301
Iter 1984/2000 - Loss: -8.301
Iter 1985/2000 - Loss: -8.301
Iter 1986/2000 - Loss: -8.301
Iter 1987/2000 - Loss: -8.301
Iter 1988/2000 - Loss: -8.301
Iter 1989/2000 - Loss: -8.301
Iter 1990/2000 - Loss: -8.301
Iter 1991/2000 - Loss: -8.301
Iter 1992/2000 - Loss: -8.301
Iter 1993/2000 - Loss: -8.301
Iter 1994/2000 - Loss: -8.301
Iter 1995/2000 - Loss: -8.301
Iter 1996/2000 - Loss: -8.301
Iter 1997/2000 - Loss: -8.301
Iter 1998/2000 - Loss: -8.301
Iter 1999/2000 - Loss: -8.301
Iter 2000/2000 - Loss: -8.301
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.3686,  8.6276, 41.3477,  9.6285, 12.4151, 50.7745]],

        [[17.3759, 34.4561,  9.4346,  1.5198,  4.7276, 30.2760]],

        [[17.2146, 36.5025, 10.6926,  0.9771,  2.4534, 19.7545]],

        [[10.9182, 30.8285, 14.9076,  3.6108,  1.2796, 43.8881]]])
Signal Variance: tensor([ 0.1225,  2.4393, 15.1129,  0.3592])
Estimated target variance: tensor([0.0101, 0.4461, 3.6455, 0.0376])
N: 160
Signal to noise ratio: tensor([20.7845, 92.7199, 92.3038, 32.8613])
Bound on condition number: tensor([  69120.3962, 1375517.4106, 1363198.9887,  172779.3073])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0017788946006502223, policy loss: 7.00950552555372
Experience 16, Iter 1, disc loss: 0.001678623685660241, policy loss: 7.017915719886187
Experience 16, Iter 2, disc loss: 0.001694226856408183, policy loss: 7.254365656625446
Experience 16, Iter 3, disc loss: 0.0016115587802739528, policy loss: 7.255734083297145
Experience 16, Iter 4, disc loss: 0.0016632228662257843, policy loss: 7.174114213996399
Experience 16, Iter 5, disc loss: 0.0018187727698433763, policy loss: 6.947763281341893
Experience 16, Iter 6, disc loss: 0.0019764667048110434, policy loss: 6.85789504758796
Experience 16, Iter 7, disc loss: 0.0018536683706185664, policy loss: 6.773487097645003
Experience 16, Iter 8, disc loss: 0.00174255740451351, policy loss: 7.108865363142282
Experience 16, Iter 9, disc loss: 0.001668628576060185, policy loss: 7.265691519784988
Experience 16, Iter 10, disc loss: 0.0016167635624349773, policy loss: 7.0192955720672146
Experience 16, Iter 11, disc loss: 0.001731828485158625, policy loss: 7.039105095067173
Experience 16, Iter 12, disc loss: 0.0016721629665521205, policy loss: 7.436216634428614
Experience 16, Iter 13, disc loss: 0.0018033053400438306, policy loss: 6.96277176320695
Experience 16, Iter 14, disc loss: 0.0017376386180957644, policy loss: 7.033202889850046
Experience 16, Iter 15, disc loss: 0.0016978040182972912, policy loss: 7.031022940134589
Experience 16, Iter 16, disc loss: 0.001699022316041742, policy loss: 6.951096435840234
Experience 16, Iter 17, disc loss: 0.0018625188178038815, policy loss: 6.735392906196178
Experience 16, Iter 18, disc loss: 0.0017044986102222096, policy loss: 7.321604316979583
Experience 16, Iter 19, disc loss: 0.0016552494279631791, policy loss: 7.027512453401675
Experience 16, Iter 20, disc loss: 0.0018217061067639212, policy loss: 6.952166493783129
Experience 16, Iter 21, disc loss: 0.0017984267671459098, policy loss: 6.8581480476330565
Experience 16, Iter 22, disc loss: 0.0017606757566744216, policy loss: 7.095286980466345
Experience 16, Iter 23, disc loss: 0.0015835391979583035, policy loss: 7.356169455638561
Experience 16, Iter 24, disc loss: 0.0016686005706057134, policy loss: 7.121327839812496
Experience 16, Iter 25, disc loss: 0.0018200850169448669, policy loss: 7.072057823983449
Experience 16, Iter 26, disc loss: 0.001752659645956991, policy loss: 6.886429116688971
Experience 16, Iter 27, disc loss: 0.0016546744029497101, policy loss: 7.017255618150989
Experience 16, Iter 28, disc loss: 0.0017124485174393692, policy loss: 7.0155098786704695
Experience 16, Iter 29, disc loss: 0.0016193749550278507, policy loss: 7.144302076054496
Experience 16, Iter 30, disc loss: 0.0016363504600559059, policy loss: 7.074093433299964
Experience 16, Iter 31, disc loss: 0.0017082233157955885, policy loss: 6.972415249167001
Experience 16, Iter 32, disc loss: 0.0019140122015412321, policy loss: 6.652310785533821
Experience 16, Iter 33, disc loss: 0.0018359600093494622, policy loss: 6.828326865933538
Experience 16, Iter 34, disc loss: 0.0016503324353723259, policy loss: 7.058431472421487
Experience 16, Iter 35, disc loss: 0.0016457792911025882, policy loss: 6.999554876501323
Experience 16, Iter 36, disc loss: 0.001641098732619345, policy loss: 7.135135496185751
Experience 16, Iter 37, disc loss: 0.00155430576415708, policy loss: 7.189446555429283
Experience 16, Iter 38, disc loss: 0.0016904527694019595, policy loss: 7.0274310180025195
Experience 16, Iter 39, disc loss: 0.0015277642000408257, policy loss: 7.248787390655621
Experience 16, Iter 40, disc loss: 0.0017129983713097953, policy loss: 7.064271956838118
Experience 16, Iter 41, disc loss: 0.0016829279720141155, policy loss: 7.003905537082774
Experience 16, Iter 42, disc loss: 0.0016213190873408025, policy loss: 6.991438469396253
Experience 16, Iter 43, disc loss: 0.0017288819467856703, policy loss: 7.06950699922951
Experience 16, Iter 44, disc loss: 0.0017064093330046146, policy loss: 7.0168335946815095
Experience 16, Iter 45, disc loss: 0.0014786459540413672, policy loss: 7.2223571046669175
Experience 16, Iter 46, disc loss: 0.0017302374631519258, policy loss: 6.982930671190365
Experience 16, Iter 47, disc loss: 0.0014648218201475942, policy loss: 7.399865225716011
Experience 16, Iter 48, disc loss: 0.0017381141643251338, policy loss: 6.936906594391733
Experience 16, Iter 49, disc loss: 0.0016548806732842166, policy loss: 7.05256203800962
Experience 16, Iter 50, disc loss: 0.0017674590151889422, policy loss: 6.828216976696872
Experience 16, Iter 51, disc loss: 0.001665606311066332, policy loss: 7.022942156473903
Experience 16, Iter 52, disc loss: 0.0017140694307412416, policy loss: 7.047533161513968
Experience 16, Iter 53, disc loss: 0.0015098438223449515, policy loss: 7.4919543006937594
Experience 16, Iter 54, disc loss: 0.0015118960500256798, policy loss: 7.319044617592253
Experience 16, Iter 55, disc loss: 0.0016054715774335808, policy loss: 7.129111560138551
Experience 16, Iter 56, disc loss: 0.0017561344318622048, policy loss: 6.937849586986721
Experience 16, Iter 57, disc loss: 0.001764008129351583, policy loss: 6.850803928595424
Experience 16, Iter 58, disc loss: 0.0015034077884819017, policy loss: 7.130150924897477
Experience 16, Iter 59, disc loss: 0.0017655066587687577, policy loss: 6.939571856469516
Experience 16, Iter 60, disc loss: 0.0016699432243936975, policy loss: 7.085338040582792
Experience 16, Iter 61, disc loss: 0.0016609730097137175, policy loss: 7.052764289488794
Experience 16, Iter 62, disc loss: 0.001607268167076757, policy loss: 7.0863283932850205
Experience 16, Iter 63, disc loss: 0.001539775819907611, policy loss: 7.4253634574057195
Experience 16, Iter 64, disc loss: 0.0015472418311754145, policy loss: 7.233343113421557
Experience 16, Iter 65, disc loss: 0.0017570798688245318, policy loss: 6.9108479582648314
Experience 16, Iter 66, disc loss: 0.0015546597472721306, policy loss: 7.247098301300584
Experience 16, Iter 67, disc loss: 0.001587401153050047, policy loss: 7.114989692335328
Experience 16, Iter 68, disc loss: 0.0015965763253471448, policy loss: 7.091664441194065
Experience 16, Iter 69, disc loss: 0.0016890287319312708, policy loss: 7.2194506592356475
Experience 16, Iter 70, disc loss: 0.001716982395884387, policy loss: 6.936346689941563
Experience 16, Iter 71, disc loss: 0.0015543011457346948, policy loss: 7.1808728363081515
Experience 16, Iter 72, disc loss: 0.0015811750404074137, policy loss: 7.136951645332525
Experience 16, Iter 73, disc loss: 0.00161821081154999, policy loss: 7.088745321056126
Experience 16, Iter 74, disc loss: 0.0016000167703517252, policy loss: 7.280942842054722
Experience 16, Iter 75, disc loss: 0.0015054186571759629, policy loss: 7.288142346007592
Experience 16, Iter 76, disc loss: 0.0017414669680357472, policy loss: 6.95127819224749
Experience 16, Iter 77, disc loss: 0.0013270390130854064, policy loss: 7.824713878425428
Experience 16, Iter 78, disc loss: 0.0015067468748365966, policy loss: 7.269108474834929
Experience 16, Iter 79, disc loss: 0.0015029944684401132, policy loss: 7.255933889466401
Experience 16, Iter 80, disc loss: 0.0015875474170817744, policy loss: 7.22962071749367
Experience 16, Iter 81, disc loss: 0.0014713628140652847, policy loss: 7.181422762038132
Experience 16, Iter 82, disc loss: 0.0013838555719272348, policy loss: 7.476263390266423
Experience 16, Iter 83, disc loss: 0.0015302633335637498, policy loss: 7.1941754124163975
Experience 16, Iter 84, disc loss: 0.0014580913341532445, policy loss: 7.394933049626323
Experience 16, Iter 85, disc loss: 0.001547076348236619, policy loss: 7.163436219398139
Experience 16, Iter 86, disc loss: 0.0014288555602086604, policy loss: 7.297663250205318
Experience 16, Iter 87, disc loss: 0.0014063785060253698, policy loss: 7.213412277457497
Experience 16, Iter 88, disc loss: 0.0014664959567841102, policy loss: 7.225347158928351
Experience 16, Iter 89, disc loss: 0.0014605567563731798, policy loss: 7.239407141200559
Experience 16, Iter 90, disc loss: 0.001637197048366328, policy loss: 7.112642139935685
Experience 16, Iter 91, disc loss: 0.0015141068046255906, policy loss: 7.219437518878628
Experience 16, Iter 92, disc loss: 0.0016475975590533763, policy loss: 7.0247360012122755
Experience 16, Iter 93, disc loss: 0.0016286084130307833, policy loss: 6.998703387051773
Experience 16, Iter 94, disc loss: 0.0015293510806741375, policy loss: 7.228222398823611
Experience 16, Iter 95, disc loss: 0.0015750552148400061, policy loss: 7.217704719742207
Experience 16, Iter 96, disc loss: 0.0014672584604972193, policy loss: 7.399681438551649
Experience 16, Iter 97, disc loss: 0.0014915304519285064, policy loss: 7.203851473663571
Experience 16, Iter 98, disc loss: 0.0015077630879760488, policy loss: 7.046260780482097
Experience 16, Iter 99, disc loss: 0.0014140741533682458, policy loss: 7.6076441857908605
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1125],
        [0.9069],
        [0.0090]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.2267e-03, 8.2913e-02, 4.6210e-01, 1.6827e-02, 2.2226e-03,
          3.2613e+00]],

        [[5.2267e-03, 8.2913e-02, 4.6210e-01, 1.6827e-02, 2.2226e-03,
          3.2613e+00]],

        [[5.2267e-03, 8.2913e-02, 4.6210e-01, 1.6827e-02, 2.2226e-03,
          3.2613e+00]],

        [[5.2267e-03, 8.2913e-02, 4.6210e-01, 1.6827e-02, 2.2226e-03,
          3.2613e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0097, 0.4498, 3.6277, 0.0361], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0097, 0.4498, 3.6277, 0.0361])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.062
Iter 2/2000 - Loss: 2.390
Iter 3/2000 - Loss: 2.021
Iter 4/2000 - Loss: 2.049
Iter 5/2000 - Loss: 2.167
Iter 6/2000 - Loss: 2.129
Iter 7/2000 - Loss: 2.015
Iter 8/2000 - Loss: 1.929
Iter 9/2000 - Loss: 1.919
Iter 10/2000 - Loss: 1.951
Iter 11/2000 - Loss: 1.954
Iter 12/2000 - Loss: 1.896
Iter 13/2000 - Loss: 1.796
Iter 14/2000 - Loss: 1.697
Iter 15/2000 - Loss: 1.623
Iter 16/2000 - Loss: 1.563
Iter 17/2000 - Loss: 1.483
Iter 18/2000 - Loss: 1.362
Iter 19/2000 - Loss: 1.200
Iter 20/2000 - Loss: 1.018
Iter 1981/2000 - Loss: -8.318
Iter 1982/2000 - Loss: -8.318
Iter 1983/2000 - Loss: -8.318
Iter 1984/2000 - Loss: -8.318
Iter 1985/2000 - Loss: -8.318
Iter 1986/2000 - Loss: -8.318
Iter 1987/2000 - Loss: -8.318
Iter 1988/2000 - Loss: -8.318
Iter 1989/2000 - Loss: -8.318
Iter 1990/2000 - Loss: -8.318
Iter 1991/2000 - Loss: -8.318
Iter 1992/2000 - Loss: -8.318
Iter 1993/2000 - Loss: -8.318
Iter 1994/2000 - Loss: -8.318
Iter 1995/2000 - Loss: -8.318
Iter 1996/2000 - Loss: -8.318
Iter 1997/2000 - Loss: -8.318
Iter 1998/2000 - Loss: -8.318
Iter 1999/2000 - Loss: -8.318
Iter 2000/2000 - Loss: -8.318
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.2168,  8.4160, 39.9220,  9.2794, 11.9358, 49.5939]],

        [[16.9937, 33.4946,  9.5379,  1.4684,  5.2776, 30.2030]],

        [[16.7801, 35.1376, 10.8149,  0.9504,  2.3707, 19.3262]],

        [[ 9.7946, 29.8414, 15.9425,  3.9810,  1.1996, 43.2834]]])
Signal Variance: tensor([ 0.1184,  2.3995, 14.2133,  0.4075])
Estimated target variance: tensor([0.0097, 0.4498, 3.6277, 0.0361])
N: 170
Signal to noise ratio: tensor([19.8551, 91.7056, 88.4119, 34.8254])
Bound on condition number: tensor([  67019.1926, 1429686.1841, 1328832.7328,  206178.2586])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0014353857799152955, policy loss: 7.366327796403803
Experience 17, Iter 1, disc loss: 0.0015433381401817577, policy loss: 7.139160672888384
Experience 17, Iter 2, disc loss: 0.0014420276005063883, policy loss: 7.166968850044954
Experience 17, Iter 3, disc loss: 0.0013461866426779142, policy loss: 7.451085937222585
Experience 17, Iter 4, disc loss: 0.0014883890215631225, policy loss: 7.1543771581276685
Experience 17, Iter 5, disc loss: 0.001365397824441953, policy loss: 7.530511004643824
Experience 17, Iter 6, disc loss: 0.0012888500305078907, policy loss: 7.53443397931044
Experience 17, Iter 7, disc loss: 0.0013651676738113932, policy loss: 7.280885829598163
Experience 17, Iter 8, disc loss: 0.0013504033873237057, policy loss: 7.256346562403898
Experience 17, Iter 9, disc loss: 0.0014815495461294403, policy loss: 7.231391556114229
Experience 17, Iter 10, disc loss: 0.001597290683274037, policy loss: 6.991402882354761
Experience 17, Iter 11, disc loss: 0.001349282539854614, policy loss: 7.28657184057981
Experience 17, Iter 12, disc loss: 0.0013755920589887717, policy loss: 7.2574323607723725
Experience 17, Iter 13, disc loss: 0.0013994893235608136, policy loss: 7.360359550134062
Experience 17, Iter 14, disc loss: 0.0016413772985004946, policy loss: 6.943786269450367
Experience 17, Iter 15, disc loss: 0.001464564108325215, policy loss: 7.078835826421825
Experience 17, Iter 16, disc loss: 0.0014106189161501655, policy loss: 7.228660250644269
Experience 17, Iter 17, disc loss: 0.0013930507247846536, policy loss: 7.2868137749434005
Experience 17, Iter 18, disc loss: 0.0012856126297088812, policy loss: 7.492035116738403
Experience 17, Iter 19, disc loss: 0.0013976663240781412, policy loss: 7.216551212575798
Experience 17, Iter 20, disc loss: 0.001319259598692318, policy loss: 7.313389835063379
Experience 17, Iter 21, disc loss: 0.0013208940442547825, policy loss: 7.396769747957329
Experience 17, Iter 22, disc loss: 0.0014260985426476588, policy loss: 7.0898766692180955
Experience 17, Iter 23, disc loss: 0.0013503886817246712, policy loss: 7.414065149070396
Experience 17, Iter 24, disc loss: 0.0011584793917019348, policy loss: 7.618755931792026
Experience 17, Iter 25, disc loss: 0.0013911249497820112, policy loss: 7.3420405310184975
Experience 17, Iter 26, disc loss: 0.0011623007621372554, policy loss: 7.566014859261101
Experience 17, Iter 27, disc loss: 0.0012176451791854255, policy loss: 7.460918913964096
Experience 17, Iter 28, disc loss: 0.0013935309715113256, policy loss: 7.232348053282377
Experience 17, Iter 29, disc loss: 0.001371216751226533, policy loss: 7.185894623994122
Experience 17, Iter 30, disc loss: 0.001414933670627933, policy loss: 7.1814703976795915
Experience 17, Iter 31, disc loss: 0.0013502203305430151, policy loss: 7.302395390968655
Experience 17, Iter 32, disc loss: 0.0013643763470829822, policy loss: 7.085146308296287
Experience 17, Iter 33, disc loss: 0.0014623791001293985, policy loss: 7.015792651705881
Experience 17, Iter 34, disc loss: 0.0013339352436024035, policy loss: 7.175702710402993
Experience 17, Iter 35, disc loss: 0.0012759058004839187, policy loss: 7.452514667083246
Experience 17, Iter 36, disc loss: 0.0012963317983991985, policy loss: 7.3672871695183
Experience 17, Iter 37, disc loss: 0.001148604970227064, policy loss: 7.546968624050337
Experience 17, Iter 38, disc loss: 0.001273205820443122, policy loss: 7.393823209286422
Experience 17, Iter 39, disc loss: 0.0013603644561505663, policy loss: 7.282756728814158
Experience 17, Iter 40, disc loss: 0.001396667116381167, policy loss: 7.123937858599183
Experience 17, Iter 41, disc loss: 0.00118811386185789, policy loss: 7.4274159011381675
Experience 17, Iter 42, disc loss: 0.0011737149857627294, policy loss: 7.575838378071687
Experience 17, Iter 43, disc loss: 0.0013318849551082561, policy loss: 7.265925372438403
Experience 17, Iter 44, disc loss: 0.0013272711791133362, policy loss: 7.305446634477115
Experience 17, Iter 45, disc loss: 0.0013283967824322654, policy loss: 7.460789361000914
Experience 17, Iter 46, disc loss: 0.0012229068976031302, policy loss: 7.425857429898404
Experience 17, Iter 47, disc loss: 0.0013099133588578972, policy loss: 7.302906020500409
Experience 17, Iter 48, disc loss: 0.0013051812230151123, policy loss: 7.2206502957435195
Experience 17, Iter 49, disc loss: 0.0014046847389715955, policy loss: 7.030757068015303
Experience 17, Iter 50, disc loss: 0.0011727980524265961, policy loss: 7.551357774230483
Experience 17, Iter 51, disc loss: 0.0013504419785863504, policy loss: 7.261432686886452
Experience 17, Iter 52, disc loss: 0.001244927147438865, policy loss: 7.442993331457018
Experience 17, Iter 53, disc loss: 0.0012517967753578798, policy loss: 7.2442837522575125
Experience 17, Iter 54, disc loss: 0.001359520571898385, policy loss: 7.180231335149452
Experience 17, Iter 55, disc loss: 0.0013293230426262796, policy loss: 7.309725582283061
Experience 17, Iter 56, disc loss: 0.0012562560592729397, policy loss: 7.296230288882879
Experience 17, Iter 57, disc loss: 0.001212928228894355, policy loss: 7.5455356530906785
Experience 17, Iter 58, disc loss: 0.0014243806366521668, policy loss: 7.02985452629375
Experience 17, Iter 59, disc loss: 0.0011783892420922587, policy loss: 7.537455360229383
Experience 17, Iter 60, disc loss: 0.0013136617923273073, policy loss: 7.370443408702831
Experience 17, Iter 61, disc loss: 0.001288367762663117, policy loss: 7.137523362653725
Experience 17, Iter 62, disc loss: 0.0012874874948594806, policy loss: 7.346153879541711
Experience 17, Iter 63, disc loss: 0.0013073148925397248, policy loss: 7.226338411463691
Experience 17, Iter 64, disc loss: 0.001290754922294191, policy loss: 7.297595994969097
Experience 17, Iter 65, disc loss: 0.0012846833249164937, policy loss: 7.3079944862080835
Experience 17, Iter 66, disc loss: 0.001312241851745451, policy loss: 7.340035383548948
Experience 17, Iter 67, disc loss: 0.0013677848194025776, policy loss: 7.079736184272983
Experience 17, Iter 68, disc loss: 0.0012115763648630972, policy loss: 7.3342077863723905
Experience 17, Iter 69, disc loss: 0.0011829317309644755, policy loss: 7.531142596550422
Experience 17, Iter 70, disc loss: 0.0012201468255371726, policy loss: 7.287043664082265
Experience 17, Iter 71, disc loss: 0.0011843033199970391, policy loss: 7.455371786452337
Experience 17, Iter 72, disc loss: 0.001305621116276283, policy loss: 7.288984112682121
Experience 17, Iter 73, disc loss: 0.0013023842296701785, policy loss: 7.304810264736833
Experience 17, Iter 74, disc loss: 0.0013664256418349627, policy loss: 7.241360731720594
Experience 17, Iter 75, disc loss: 0.00129076067565161, policy loss: 7.178824221729828
Experience 17, Iter 76, disc loss: 0.0012952212751456043, policy loss: 7.245499174788376
Experience 17, Iter 77, disc loss: 0.001238661633947508, policy loss: 7.530244349400715
Experience 17, Iter 78, disc loss: 0.0013255607739337185, policy loss: 7.177045385862436
Experience 17, Iter 79, disc loss: 0.0012528479315039546, policy loss: 7.283153221091092
Experience 17, Iter 80, disc loss: 0.0011257865300457778, policy loss: 7.571369491501781
Experience 17, Iter 81, disc loss: 0.0011072876362943913, policy loss: 7.745178311508521
Experience 17, Iter 82, disc loss: 0.0011680344472494342, policy loss: 7.459548345939513
Experience 17, Iter 83, disc loss: 0.0012794259067411434, policy loss: 7.1829967986641305
Experience 17, Iter 84, disc loss: 0.001140721203475035, policy loss: 7.3849880683579805
Experience 17, Iter 85, disc loss: 0.0011584105046241757, policy loss: 7.367171755836528
Experience 17, Iter 86, disc loss: 0.0011577967510241105, policy loss: 7.439132966768331
Experience 17, Iter 87, disc loss: 0.0011222055258647496, policy loss: 7.52582225648791
Experience 17, Iter 88, disc loss: 0.001271600125997559, policy loss: 7.377868778419412
Experience 17, Iter 89, disc loss: 0.0011410853241904297, policy loss: 7.511350730140196
Experience 17, Iter 90, disc loss: 0.0012972979247942427, policy loss: 7.169296034137555
Experience 17, Iter 91, disc loss: 0.001209380096177875, policy loss: 7.307435022743617
Experience 17, Iter 92, disc loss: 0.0013003921560963694, policy loss: 7.148076104628288
Experience 17, Iter 93, disc loss: 0.0012232530235007275, policy loss: 7.313114179117086
Experience 17, Iter 94, disc loss: 0.001256415242054839, policy loss: 7.412451427106285
Experience 17, Iter 95, disc loss: 0.0011209828719734127, policy loss: 7.699251676320308
Experience 17, Iter 96, disc loss: 0.0011744475328813577, policy loss: 7.621172712323091
Experience 17, Iter 97, disc loss: 0.0012737786007216809, policy loss: 7.196196236789989
Experience 17, Iter 98, disc loss: 0.0012410055075494435, policy loss: 7.2324633410710195
Experience 17, Iter 99, disc loss: 0.0011214736775908298, policy loss: 7.656966057073122
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1108],
        [0.8940],
        [0.0090]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.0133e-03, 8.0505e-02, 4.5893e-01, 1.6601e-02, 2.1468e-03,
          3.2119e+00]],

        [[5.0133e-03, 8.0505e-02, 4.5893e-01, 1.6601e-02, 2.1468e-03,
          3.2119e+00]],

        [[5.0133e-03, 8.0505e-02, 4.5893e-01, 1.6601e-02, 2.1468e-03,
          3.2119e+00]],

        [[5.0133e-03, 8.0505e-02, 4.5893e-01, 1.6601e-02, 2.1468e-03,
          3.2119e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0095, 0.4430, 3.5759, 0.0358], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0095, 0.4430, 3.5759, 0.0358])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.036
Iter 2/2000 - Loss: 2.383
Iter 3/2000 - Loss: 1.994
Iter 4/2000 - Loss: 2.022
Iter 5/2000 - Loss: 2.146
Iter 6/2000 - Loss: 2.108
Iter 7/2000 - Loss: 1.987
Iter 8/2000 - Loss: 1.892
Iter 9/2000 - Loss: 1.875
Iter 10/2000 - Loss: 1.903
Iter 11/2000 - Loss: 1.908
Iter 12/2000 - Loss: 1.850
Iter 13/2000 - Loss: 1.745
Iter 14/2000 - Loss: 1.634
Iter 15/2000 - Loss: 1.546
Iter 16/2000 - Loss: 1.473
Iter 17/2000 - Loss: 1.387
Iter 18/2000 - Loss: 1.262
Iter 19/2000 - Loss: 1.094
Iter 20/2000 - Loss: 0.901
Iter 1981/2000 - Loss: -8.403
Iter 1982/2000 - Loss: -8.403
Iter 1983/2000 - Loss: -8.403
Iter 1984/2000 - Loss: -8.403
Iter 1985/2000 - Loss: -8.403
Iter 1986/2000 - Loss: -8.403
Iter 1987/2000 - Loss: -8.403
Iter 1988/2000 - Loss: -8.403
Iter 1989/2000 - Loss: -8.403
Iter 1990/2000 - Loss: -8.403
Iter 1991/2000 - Loss: -8.403
Iter 1992/2000 - Loss: -8.403
Iter 1993/2000 - Loss: -8.403
Iter 1994/2000 - Loss: -8.403
Iter 1995/2000 - Loss: -8.403
Iter 1996/2000 - Loss: -8.403
Iter 1997/2000 - Loss: -8.403
Iter 1998/2000 - Loss: -8.403
Iter 1999/2000 - Loss: -8.403
Iter 2000/2000 - Loss: -8.403
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[12.1930,  8.3990, 37.8678,  9.4153, 11.9491, 50.9056]],

        [[16.5303, 33.1730,  9.4569,  1.5206,  4.0189, 29.9275]],

        [[16.3181, 34.4041, 10.7819,  0.9429,  2.3422, 19.1900]],

        [[ 9.1948, 29.0651, 16.3540,  3.9753,  1.1568, 43.0464]]])
Signal Variance: tensor([ 0.1176,  2.3480, 13.9145,  0.4199])
Estimated target variance: tensor([0.0095, 0.4430, 3.5759, 0.0358])
N: 180
Signal to noise ratio: tensor([19.6261, 90.1082, 90.0813, 36.0330])
Bound on condition number: tensor([  69334.2678, 1461509.1637, 1460636.0823,  233708.4730])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.001315431199307902, policy loss: 7.259827655544791
Experience 18, Iter 1, disc loss: 0.0010133326168478849, policy loss: 7.796997191722232
Experience 18, Iter 2, disc loss: 0.0011519896416224919, policy loss: 7.551293572659972
Experience 18, Iter 3, disc loss: 0.0011735513334151407, policy loss: 7.387161848845517
Experience 18, Iter 4, disc loss: 0.0012364549198399096, policy loss: 7.362568563588946
Experience 18, Iter 5, disc loss: 0.0011729286702320298, policy loss: 7.4181445760117555
Experience 18, Iter 6, disc loss: 0.0010805654324780969, policy loss: 7.606998448679179
Experience 18, Iter 7, disc loss: 0.0011336964547404385, policy loss: 7.407670437873584
Experience 18, Iter 8, disc loss: 0.0011062190080952982, policy loss: 7.50899403213516
Experience 18, Iter 9, disc loss: 0.0011712066381200756, policy loss: 7.587639504832959
Experience 18, Iter 10, disc loss: 0.0012667391293080905, policy loss: 7.278947862946611
Experience 18, Iter 11, disc loss: 0.001119997018694228, policy loss: 7.543075324609639
Experience 18, Iter 12, disc loss: 0.0012344547403951535, policy loss: 7.185404747756323
Experience 18, Iter 13, disc loss: 0.0013115208285807932, policy loss: 7.308884998073526
Experience 18, Iter 14, disc loss: 0.0012238640827106795, policy loss: 7.256572633119385
Experience 18, Iter 15, disc loss: 0.0011850690539865196, policy loss: 7.386600353464183
Experience 18, Iter 16, disc loss: 0.001145209512536035, policy loss: 7.605566975581594
Experience 18, Iter 17, disc loss: 0.0011590508033621893, policy loss: 7.358388809892975
Experience 18, Iter 18, disc loss: 0.0011054318189995262, policy loss: 7.504101287522671
Experience 18, Iter 19, disc loss: 0.0011284072287236332, policy loss: 7.469729580924994
Experience 18, Iter 20, disc loss: 0.0012142292741201804, policy loss: 7.28223232140747
Experience 18, Iter 21, disc loss: 0.0011424565915711105, policy loss: 7.403560016332336
Experience 18, Iter 22, disc loss: 0.001088672939110509, policy loss: 7.530713054016298
Experience 18, Iter 23, disc loss: 0.0012685682673682001, policy loss: 7.152659988228714
Experience 18, Iter 24, disc loss: 0.0010918491377418594, policy loss: 7.4657144100349875
Experience 18, Iter 25, disc loss: 0.0010362559037030094, policy loss: 7.6734496865520025
Experience 18, Iter 26, disc loss: 0.0012090426154549225, policy loss: 7.249923476790266
Experience 18, Iter 27, disc loss: 0.001157641951125452, policy loss: 7.349007716791019
Experience 18, Iter 28, disc loss: 0.0011779493488804737, policy loss: 7.611609845961118
Experience 18, Iter 29, disc loss: 0.001199791135021095, policy loss: 7.3858611291342395
Experience 18, Iter 30, disc loss: 0.001137688460055415, policy loss: 7.428822085240172
Experience 18, Iter 31, disc loss: 0.0011107839661253789, policy loss: 7.586184732591672
Experience 18, Iter 32, disc loss: 0.0010830248658731632, policy loss: 7.650983699713347
Experience 18, Iter 33, disc loss: 0.0010816901345228867, policy loss: 7.752447367739946
Experience 18, Iter 34, disc loss: 0.0012262704024967532, policy loss: 7.240333902758262
Experience 18, Iter 35, disc loss: 0.00113125428687706, policy loss: 7.435763716655018
Experience 18, Iter 36, disc loss: 0.0010898210755409035, policy loss: 7.549867214221687
Experience 18, Iter 37, disc loss: 0.0011703120630729932, policy loss: 7.307390131422699
Experience 18, Iter 38, disc loss: 0.001141293292070932, policy loss: 7.469965616778551
Experience 18, Iter 39, disc loss: 0.001208421821365761, policy loss: 7.2572704412719355
Experience 18, Iter 40, disc loss: 0.0009694431798574518, policy loss: 7.651376315211337
Experience 18, Iter 41, disc loss: 0.0011499082301444407, policy loss: 7.499501147930713
Experience 18, Iter 42, disc loss: 0.0010503828905063751, policy loss: 7.552307983792469
Experience 18, Iter 43, disc loss: 0.0010803200777558851, policy loss: 7.528041071700928
Experience 18, Iter 44, disc loss: 0.0012202086338121867, policy loss: 7.177295549358218
Experience 18, Iter 45, disc loss: 0.0011445975734501069, policy loss: 7.455577081240907
Experience 18, Iter 46, disc loss: 0.0010566242466393543, policy loss: 7.712307872152366
Experience 18, Iter 47, disc loss: 0.0011586146961645703, policy loss: 7.364668776803999
Experience 18, Iter 48, disc loss: 0.0009732135904604607, policy loss: 7.699352598801754
Experience 18, Iter 49, disc loss: 0.0009807636842405457, policy loss: 7.724760046908507
Experience 18, Iter 50, disc loss: 0.0010873550339919199, policy loss: 7.490689890998681
Experience 18, Iter 51, disc loss: 0.0011247671968119188, policy loss: 7.330909290412282
Experience 18, Iter 52, disc loss: 0.0009352793640460898, policy loss: 7.630728528799726
Experience 18, Iter 53, disc loss: 0.0008702072317534617, policy loss: 7.75365885983846
Experience 18, Iter 54, disc loss: 0.001111863618611859, policy loss: 7.357586447729738
Experience 18, Iter 55, disc loss: 0.0011046150290914688, policy loss: 7.552434271718031
Experience 18, Iter 56, disc loss: 0.0010066339272717289, policy loss: 7.566476117950876
Experience 18, Iter 57, disc loss: 0.00087821855861486, policy loss: 7.844293288529634
Experience 18, Iter 58, disc loss: 0.0009957952957341957, policy loss: 7.528214540432115
Experience 18, Iter 59, disc loss: 0.0009689331691667395, policy loss: 7.700348953209265
Experience 18, Iter 60, disc loss: 0.0011242751543212549, policy loss: 7.524103545762634
Experience 18, Iter 61, disc loss: 0.0007624986803864483, policy loss: 8.164011989492101
Experience 18, Iter 62, disc loss: 0.0009229281272915626, policy loss: 7.665048446307228
Experience 18, Iter 63, disc loss: 0.0011507161382029903, policy loss: 7.4776965690743795
Experience 18, Iter 64, disc loss: 0.000961637031991808, policy loss: 7.79882312938897
Experience 18, Iter 65, disc loss: 0.00084166620874019, policy loss: 7.790008534003654
Experience 18, Iter 66, disc loss: 0.0007871673067511662, policy loss: 8.12983937057183
Experience 18, Iter 67, disc loss: 0.0008502931909962954, policy loss: 7.852115790512309
Experience 18, Iter 68, disc loss: 0.000989798098154985, policy loss: 7.576977278396079
Experience 18, Iter 69, disc loss: 0.001002675847087116, policy loss: 7.642248001546694
Experience 18, Iter 70, disc loss: 0.0010499887333103586, policy loss: 7.477353561102927
Experience 18, Iter 71, disc loss: 0.000954020040084123, policy loss: 7.579023237640584
Experience 18, Iter 72, disc loss: 0.0009211948214490297, policy loss: 7.899643925931581
Experience 18, Iter 73, disc loss: 0.0008826615762333927, policy loss: 7.725261792691917
Experience 18, Iter 74, disc loss: 0.0011187263656707342, policy loss: 7.340195766262633
Experience 18, Iter 75, disc loss: 0.0008580892536289978, policy loss: 7.721068495703669
Experience 18, Iter 76, disc loss: 0.0008723615380416687, policy loss: 7.5550122353873554
Experience 18, Iter 77, disc loss: 0.0008435193025809524, policy loss: 7.857927842481296
Experience 18, Iter 78, disc loss: 0.0009680447665932697, policy loss: 7.687440082565779
Experience 18, Iter 79, disc loss: 0.0010292393496703182, policy loss: 7.459214985686488
Experience 18, Iter 80, disc loss: 0.0010342722184109063, policy loss: 7.4466919639013955
Experience 18, Iter 81, disc loss: 0.0009560425158953289, policy loss: 7.588296709546557
Experience 18, Iter 82, disc loss: 0.0008898190554257121, policy loss: 7.796386364528935
Experience 18, Iter 83, disc loss: 0.0010460766446464041, policy loss: 7.415019551662202
Experience 18, Iter 84, disc loss: 0.0010533233758462303, policy loss: 7.441872794585746
Experience 18, Iter 85, disc loss: 0.0009778267846224317, policy loss: 7.5722523218478
Experience 18, Iter 86, disc loss: 0.001030294569294236, policy loss: 7.393990424528581
Experience 18, Iter 87, disc loss: 0.0009851425161850245, policy loss: 7.4995039000217005
Experience 18, Iter 88, disc loss: 0.000930153961819989, policy loss: 7.60818426072615
Experience 18, Iter 89, disc loss: 0.0010578408570918276, policy loss: 7.500661466580577
Experience 18, Iter 90, disc loss: 0.0009511597057946094, policy loss: 7.650794429505332
Experience 18, Iter 91, disc loss: 0.0009302697726772298, policy loss: 7.6495359443157716
Experience 18, Iter 92, disc loss: 0.0009776046180166842, policy loss: 7.577656228214209
Experience 18, Iter 93, disc loss: 0.0009400399624943196, policy loss: 7.749268489652691
Experience 18, Iter 94, disc loss: 0.0009095004396653496, policy loss: 7.653671444922477
Experience 18, Iter 95, disc loss: 0.0009615331922842515, policy loss: 7.622126946070891
Experience 18, Iter 96, disc loss: 0.0009825840408517962, policy loss: 7.612653650101675
Experience 18, Iter 97, disc loss: 0.0009934885795133191, policy loss: 7.569288423538104
Experience 18, Iter 98, disc loss: 0.0009494797840440821, policy loss: 7.740660869484544
Experience 18, Iter 99, disc loss: 0.0010212912689907827, policy loss: 7.528753140373931
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1081],
        [0.8635],
        [0.0087]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.7801e-03, 7.7595e-02, 4.4512e-01, 1.6227e-02, 2.0680e-03,
          3.1554e+00]],

        [[4.7801e-03, 7.7595e-02, 4.4512e-01, 1.6227e-02, 2.0680e-03,
          3.1554e+00]],

        [[4.7801e-03, 7.7595e-02, 4.4512e-01, 1.6227e-02, 2.0680e-03,
          3.1554e+00]],

        [[4.7801e-03, 7.7595e-02, 4.4512e-01, 1.6227e-02, 2.0680e-03,
          3.1554e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0093, 0.4326, 3.4541, 0.0348], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0093, 0.4326, 3.4541, 0.0348])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.984
Iter 2/2000 - Loss: 2.340
Iter 3/2000 - Loss: 1.940
Iter 4/2000 - Loss: 1.968
Iter 5/2000 - Loss: 2.096
Iter 6/2000 - Loss: 2.057
Iter 7/2000 - Loss: 1.932
Iter 8/2000 - Loss: 1.832
Iter 9/2000 - Loss: 1.810
Iter 10/2000 - Loss: 1.836
Iter 11/2000 - Loss: 1.840
Iter 12/2000 - Loss: 1.781
Iter 13/2000 - Loss: 1.673
Iter 14/2000 - Loss: 1.556
Iter 15/2000 - Loss: 1.460
Iter 16/2000 - Loss: 1.381
Iter 17/2000 - Loss: 1.291
Iter 18/2000 - Loss: 1.164
Iter 19/2000 - Loss: 0.994
Iter 20/2000 - Loss: 0.796
Iter 1981/2000 - Loss: -8.468
Iter 1982/2000 - Loss: -8.468
Iter 1983/2000 - Loss: -8.468
Iter 1984/2000 - Loss: -8.468
Iter 1985/2000 - Loss: -8.468
Iter 1986/2000 - Loss: -8.468
Iter 1987/2000 - Loss: -8.468
Iter 1988/2000 - Loss: -8.469
Iter 1989/2000 - Loss: -8.469
Iter 1990/2000 - Loss: -8.469
Iter 1991/2000 - Loss: -8.469
Iter 1992/2000 - Loss: -8.469
Iter 1993/2000 - Loss: -8.469
Iter 1994/2000 - Loss: -8.469
Iter 1995/2000 - Loss: -8.469
Iter 1996/2000 - Loss: -8.469
Iter 1997/2000 - Loss: -8.469
Iter 1998/2000 - Loss: -8.469
Iter 1999/2000 - Loss: -8.469
Iter 2000/2000 - Loss: -8.469
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[12.0191,  8.3752, 37.0424,  9.3489, 12.0207, 49.4449]],

        [[16.3376, 32.9185,  9.4991,  1.6640,  3.2852, 30.8261]],

        [[16.0974, 34.0500, 10.7561,  0.9353,  2.2907, 18.6707]],

        [[ 9.0267, 28.9010, 16.2845,  3.9576,  1.1397, 42.0376]]])
Signal Variance: tensor([ 0.1152,  2.5810, 13.5899,  0.4181])
Estimated target variance: tensor([0.0093, 0.4326, 3.4541, 0.0348])
N: 190
Signal to noise ratio: tensor([19.7345, 92.2698, 89.3448, 36.5145])
Bound on condition number: tensor([  73996.9627, 1617605.3776, 1516673.4402,  253329.8454])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0009672434887111961, policy loss: 7.5895288367165
Experience 19, Iter 1, disc loss: 0.0009212807518135984, policy loss: 7.658345786542345
Experience 19, Iter 2, disc loss: 0.0009689119048420253, policy loss: 7.845590783298963
Experience 19, Iter 3, disc loss: 0.0009951518919666064, policy loss: 7.508826604355354
Experience 19, Iter 4, disc loss: 0.0009901894293903673, policy loss: 7.50788816101228
Experience 19, Iter 5, disc loss: 0.0009800055680706874, policy loss: 7.578289590286996
Experience 19, Iter 6, disc loss: 0.000923194599098214, policy loss: 7.770248345259069
Experience 19, Iter 7, disc loss: 0.0009351951544485635, policy loss: 7.7521089681505355
Experience 19, Iter 8, disc loss: 0.0009799280692570412, policy loss: 7.509415322401757
Experience 19, Iter 9, disc loss: 0.001001201749503879, policy loss: 7.477682616407405
Experience 19, Iter 10, disc loss: 0.0009989540187788752, policy loss: 7.591645248585571
Experience 19, Iter 11, disc loss: 0.0009609097051456442, policy loss: 7.543880449093255
Experience 19, Iter 12, disc loss: 0.0010218358743959537, policy loss: 7.397422438530993
Experience 19, Iter 13, disc loss: 0.0010006633072873776, policy loss: 7.522555598549649
Experience 19, Iter 14, disc loss: 0.0010180148510067191, policy loss: 7.495032766324407
Experience 19, Iter 15, disc loss: 0.000989639130260326, policy loss: 7.546327908740855
Experience 19, Iter 16, disc loss: 0.0009048141328831278, policy loss: 7.855583676967473
Experience 19, Iter 17, disc loss: 0.0010302949608391385, policy loss: 7.348974812309908
Experience 19, Iter 18, disc loss: 0.0008975307405177154, policy loss: 7.820776043635931
Experience 19, Iter 19, disc loss: 0.0010539825371378912, policy loss: 7.394762117326886
Experience 19, Iter 20, disc loss: 0.000980547338728533, policy loss: 7.700020799004241
Experience 19, Iter 21, disc loss: 0.0008615702593620754, policy loss: 7.727619816185365
Experience 19, Iter 22, disc loss: 0.0009624167067566966, policy loss: 7.5983346803231955
Experience 19, Iter 23, disc loss: 0.0009145765934938738, policy loss: 7.647739418389218
Experience 19, Iter 24, disc loss: 0.0008977380265245149, policy loss: 7.826243198198315
Experience 19, Iter 25, disc loss: 0.0009657473459885143, policy loss: 7.6503446392841195
Experience 19, Iter 26, disc loss: 0.0010070263410279744, policy loss: 7.4437430975167915
Experience 19, Iter 27, disc loss: 0.0009006224392389976, policy loss: 7.71310656658971
Experience 19, Iter 28, disc loss: 0.0009804503070415973, policy loss: 7.6303729756774334
Experience 19, Iter 29, disc loss: 0.0009397991768974323, policy loss: 7.669753786502966
Experience 19, Iter 30, disc loss: 0.0009546167181913193, policy loss: 7.786972375989196
Experience 19, Iter 31, disc loss: 0.00098209053383677, policy loss: 7.750984733003284
Experience 19, Iter 32, disc loss: 0.0009387302685974679, policy loss: 7.535555929854592
Experience 19, Iter 33, disc loss: 0.0010952119466917849, policy loss: 7.352177210834283
Experience 19, Iter 34, disc loss: 0.0009417681608684926, policy loss: 7.587014052422122
Experience 19, Iter 35, disc loss: 0.0009304539835526121, policy loss: 7.699129412583401
Experience 19, Iter 36, disc loss: 0.0009914026943117012, policy loss: 7.441261835720727
Experience 19, Iter 37, disc loss: 0.0009533578380394815, policy loss: 7.599771053500062
Experience 19, Iter 38, disc loss: 0.0009692403296782121, policy loss: 7.506157842122847
Experience 19, Iter 39, disc loss: 0.000926703641822946, policy loss: 7.788233092015261
Experience 19, Iter 40, disc loss: 0.0010386687954880842, policy loss: 7.3773600204173375
Experience 19, Iter 41, disc loss: 0.0009689315186758805, policy loss: 7.565060136224735
Experience 19, Iter 42, disc loss: 0.0009357856566076395, policy loss: 7.557120759124962
Experience 19, Iter 43, disc loss: 0.0009889156079110801, policy loss: 7.4817977588381765
Experience 19, Iter 44, disc loss: 0.0009035152834120573, policy loss: 7.591627394456216
Experience 19, Iter 45, disc loss: 0.0008071721144030579, policy loss: 7.8853549423786555
Experience 19, Iter 46, disc loss: 0.0008329645916864541, policy loss: 7.89487457163307
Experience 19, Iter 47, disc loss: 0.0007918630366404191, policy loss: 7.874007812888411
Experience 19, Iter 48, disc loss: 0.0009290900653549398, policy loss: 7.523863855024757
Experience 19, Iter 49, disc loss: 0.0009292407391357322, policy loss: 7.707688739670595
Experience 19, Iter 50, disc loss: 0.0008698094565556893, policy loss: 7.69709779300676
Experience 19, Iter 51, disc loss: 0.000879595803577412, policy loss: 7.809586706436724
Experience 19, Iter 52, disc loss: 0.0008500304228685684, policy loss: 7.68078479209977
Experience 19, Iter 53, disc loss: 0.0009374094197767073, policy loss: 7.650516716842388
Experience 19, Iter 54, disc loss: 0.0009388011766214714, policy loss: 7.552800616962318
Experience 19, Iter 55, disc loss: 0.0008753605023473041, policy loss: 7.634092520544714
Experience 19, Iter 56, disc loss: 0.0007327805686944369, policy loss: 8.019256509064512
Experience 19, Iter 57, disc loss: 0.0008641026066950455, policy loss: 7.869342061648371
Experience 19, Iter 58, disc loss: 0.0008563726788052162, policy loss: 7.69429646327533
Experience 19, Iter 59, disc loss: 0.0009261262822945989, policy loss: 7.620666493867892
Experience 19, Iter 60, disc loss: 0.0008693934021862008, policy loss: 8.074337140247033
Experience 19, Iter 61, disc loss: 0.0008500531838257559, policy loss: 7.565898674555708
Experience 19, Iter 62, disc loss: 0.0008112906304736237, policy loss: 7.790338263623694
Experience 19, Iter 63, disc loss: 0.0009413696692391683, policy loss: 7.610764684483056
Experience 19, Iter 64, disc loss: 0.0008275435101934782, policy loss: 7.852305689285185
Experience 19, Iter 65, disc loss: 0.000923359156872045, policy loss: 7.652302845168475
Experience 19, Iter 66, disc loss: 0.000860463991673208, policy loss: 7.997667843892547
Experience 19, Iter 67, disc loss: 0.0008263009283760419, policy loss: 7.829595976811493
Experience 19, Iter 68, disc loss: 0.00072130475200117, policy loss: 7.925373060256441
Experience 19, Iter 69, disc loss: 0.0008455682031483179, policy loss: 7.696161128159049
Experience 19, Iter 70, disc loss: 0.0009199085765438721, policy loss: 7.667369245367017
Experience 19, Iter 71, disc loss: 0.0008580856660221443, policy loss: 7.739537419718804
Experience 19, Iter 72, disc loss: 0.0009319380143773639, policy loss: 7.696354781661428
Experience 19, Iter 73, disc loss: 0.0008355522164058428, policy loss: 7.750153174328247
Experience 19, Iter 74, disc loss: 0.0008896895623539864, policy loss: 7.566750979772188
Experience 19, Iter 75, disc loss: 0.0007831260075688044, policy loss: 7.893037869259387
Experience 19, Iter 76, disc loss: 0.0008443131250680622, policy loss: 7.7017369380172465
Experience 19, Iter 77, disc loss: 0.0007908355398587661, policy loss: 8.034400300194845
Experience 19, Iter 78, disc loss: 0.0008584084224173163, policy loss: 7.633892737635112
Experience 19, Iter 79, disc loss: 0.0007793394798165747, policy loss: 7.866335486945759
Experience 19, Iter 80, disc loss: 0.0008369174513960242, policy loss: 7.781836036858548
Experience 19, Iter 81, disc loss: 0.0008023093778315652, policy loss: 7.768850745263791
Experience 19, Iter 82, disc loss: 0.000807304672067063, policy loss: 7.7139221212022715
Experience 19, Iter 83, disc loss: 0.0008512183690746664, policy loss: 7.622778129643134
Experience 19, Iter 84, disc loss: 0.0007869109496080806, policy loss: 7.976637910415339
Experience 19, Iter 85, disc loss: 0.0008305435069224133, policy loss: 7.627103480191098
Experience 19, Iter 86, disc loss: 0.0008006982527515017, policy loss: 7.780655172975351
Experience 19, Iter 87, disc loss: 0.0007820784494279143, policy loss: 7.88923010490007
Experience 19, Iter 88, disc loss: 0.0008442544755894774, policy loss: 7.658503004934385
Experience 19, Iter 89, disc loss: 0.0007925885896194027, policy loss: 7.820708861183739
Experience 19, Iter 90, disc loss: 0.0008803922002860072, policy loss: 7.789876367507975
Experience 19, Iter 91, disc loss: 0.0008528857091856465, policy loss: 7.774712742171688
Experience 19, Iter 92, disc loss: 0.0007681437921585162, policy loss: 7.875037429763026
Experience 19, Iter 93, disc loss: 0.0008410820410257163, policy loss: 7.676462423319933
Experience 19, Iter 94, disc loss: 0.0008118954142539867, policy loss: 7.737619900632053
Experience 19, Iter 95, disc loss: 0.0009510305740892669, policy loss: 7.411659821436046
Experience 19, Iter 96, disc loss: 0.0008436465683813964, policy loss: 7.647242829375475
Experience 19, Iter 97, disc loss: 0.0007702788767190974, policy loss: 7.8852273165252225
Experience 19, Iter 98, disc loss: 0.0008211297256284511, policy loss: 7.781551589764339
Experience 19, Iter 99, disc loss: 0.000890577885572293, policy loss: 7.645688704850953
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1067],
        [0.8530],
        [0.0087]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5942e-03, 7.5714e-02, 4.4260e-01, 1.6031e-02, 2.0080e-03,
          3.1131e+00]],

        [[4.5942e-03, 7.5714e-02, 4.4260e-01, 1.6031e-02, 2.0080e-03,
          3.1131e+00]],

        [[4.5942e-03, 7.5714e-02, 4.4260e-01, 1.6031e-02, 2.0080e-03,
          3.1131e+00]],

        [[4.5942e-03, 7.5714e-02, 4.4260e-01, 1.6031e-02, 2.0080e-03,
          3.1131e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0091, 0.4268, 3.4121, 0.0346], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0091, 0.4268, 3.4121, 0.0346])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.957
Iter 2/2000 - Loss: 2.318
Iter 3/2000 - Loss: 1.906
Iter 4/2000 - Loss: 1.936
Iter 5/2000 - Loss: 2.065
Iter 6/2000 - Loss: 2.023
Iter 7/2000 - Loss: 1.891
Iter 8/2000 - Loss: 1.783
Iter 9/2000 - Loss: 1.753
Iter 10/2000 - Loss: 1.772
Iter 11/2000 - Loss: 1.771
Iter 12/2000 - Loss: 1.706
Iter 13/2000 - Loss: 1.590
Iter 14/2000 - Loss: 1.465
Iter 15/2000 - Loss: 1.361
Iter 16/2000 - Loss: 1.275
Iter 17/2000 - Loss: 1.177
Iter 18/2000 - Loss: 1.041
Iter 19/2000 - Loss: 0.861
Iter 20/2000 - Loss: 0.650
Iter 1981/2000 - Loss: -8.503
Iter 1982/2000 - Loss: -8.504
Iter 1983/2000 - Loss: -8.504
Iter 1984/2000 - Loss: -8.504
Iter 1985/2000 - Loss: -8.504
Iter 1986/2000 - Loss: -8.504
Iter 1987/2000 - Loss: -8.504
Iter 1988/2000 - Loss: -8.504
Iter 1989/2000 - Loss: -8.504
Iter 1990/2000 - Loss: -8.504
Iter 1991/2000 - Loss: -8.504
Iter 1992/2000 - Loss: -8.504
Iter 1993/2000 - Loss: -8.504
Iter 1994/2000 - Loss: -8.504
Iter 1995/2000 - Loss: -8.504
Iter 1996/2000 - Loss: -8.504
Iter 1997/2000 - Loss: -8.504
Iter 1998/2000 - Loss: -8.504
Iter 1999/2000 - Loss: -8.504
Iter 2000/2000 - Loss: -8.504
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[11.8639,  8.4031, 36.9169,  9.3379, 11.9098, 47.3592]],

        [[15.6206, 31.7061,  9.4269,  1.6566,  3.3185, 30.5234]],

        [[15.6416, 33.2895, 10.6832,  0.9315,  2.2989, 18.7082]],

        [[ 8.7008, 28.3393, 16.3319,  3.8837,  1.1133, 42.3303]]])
Signal Variance: tensor([ 0.1143,  2.5228, 13.5366,  0.4164])
Estimated target variance: tensor([0.0091, 0.4268, 3.4121, 0.0346])
N: 200
Signal to noise ratio: tensor([19.8969, 86.0917, 90.3120, 37.0395])
Bound on condition number: tensor([  79178.3602, 1482356.4054, 1631253.1498,  274386.5522])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0008253628822202784, policy loss: 7.85128030594233
Experience 20, Iter 1, disc loss: 0.000877798739917517, policy loss: 7.579437954178655
Experience 20, Iter 2, disc loss: 0.0008071114880861664, policy loss: 7.695812234961723
Experience 20, Iter 3, disc loss: 0.0008076371397256563, policy loss: 7.806032446941648
Experience 20, Iter 4, disc loss: 0.0009022821560321805, policy loss: 7.563339689076072
Experience 20, Iter 5, disc loss: 0.0008089757714258786, policy loss: 7.712182339435891
Experience 20, Iter 6, disc loss: 0.0008387859842136969, policy loss: 7.712322653616762
Experience 20, Iter 7, disc loss: 0.0008523786564171793, policy loss: 7.65313855848205
Experience 20, Iter 8, disc loss: 0.000824563455592464, policy loss: 7.8088338400104504
Experience 20, Iter 9, disc loss: 0.0008708869410824396, policy loss: 7.754511033485381
Experience 20, Iter 10, disc loss: 0.0008194684056238889, policy loss: 7.756633345721674
Experience 20, Iter 11, disc loss: 0.0008544179989461925, policy loss: 7.542159483815265
Experience 20, Iter 12, disc loss: 0.0008681619113394658, policy loss: 7.638442254855665
Experience 20, Iter 13, disc loss: 0.0007795566546727784, policy loss: 7.8262466161069
Experience 20, Iter 14, disc loss: 0.0008633902941657256, policy loss: 7.633337294803913
Experience 20, Iter 15, disc loss: 0.0007804472308719089, policy loss: 7.8233327593471085
Experience 20, Iter 16, disc loss: 0.0008665456610321804, policy loss: 7.5583528536539495
Experience 20, Iter 17, disc loss: 0.0008536322947857855, policy loss: 7.66856285223353
Experience 20, Iter 18, disc loss: 0.0008315434553088925, policy loss: 7.866350336569582
Experience 20, Iter 19, disc loss: 0.0008554061138721599, policy loss: 7.633411375329437
Experience 20, Iter 20, disc loss: 0.0008308071001189141, policy loss: 7.678066592599414
Experience 20, Iter 21, disc loss: 0.000834141518003639, policy loss: 7.824865813533148
Experience 20, Iter 22, disc loss: 0.0008074426410397632, policy loss: 7.818846308832443
Experience 20, Iter 23, disc loss: 0.0008248965071606051, policy loss: 7.8579628331055495
Experience 20, Iter 24, disc loss: 0.0007486590792559404, policy loss: 7.877830408413443
Experience 20, Iter 25, disc loss: 0.0008337473348077747, policy loss: 7.7953257238775056
Experience 20, Iter 26, disc loss: 0.0007480846933260278, policy loss: 7.893492138763725
Experience 20, Iter 27, disc loss: 0.0008208003338152915, policy loss: 7.751612842587418
Experience 20, Iter 28, disc loss: 0.0007750063978463617, policy loss: 7.874344758475912
Experience 20, Iter 29, disc loss: 0.0006948999369651339, policy loss: 8.067463633823627
Experience 20, Iter 30, disc loss: 0.0008146522178485215, policy loss: 7.86618379472757
Experience 20, Iter 31, disc loss: 0.0007653391431852426, policy loss: 8.031976750267024
Experience 20, Iter 32, disc loss: 0.0008123613766972172, policy loss: 7.644835339651629
Experience 20, Iter 33, disc loss: 0.0008754166962710902, policy loss: 7.4980650328209855
Experience 20, Iter 34, disc loss: 0.0007565231565556755, policy loss: 7.653498438110342
Experience 20, Iter 35, disc loss: 0.0007871778645076213, policy loss: 7.700057667391763
Experience 20, Iter 36, disc loss: 0.0008075389435402766, policy loss: 7.722168213854929
Experience 20, Iter 37, disc loss: 0.0008051514007244692, policy loss: 7.7424014316086645
Experience 20, Iter 38, disc loss: 0.0008596699818822761, policy loss: 7.505588333529435
Experience 20, Iter 39, disc loss: 0.0008543512828880795, policy loss: 7.527016251336029
Experience 20, Iter 40, disc loss: 0.0008604876716117221, policy loss: 7.623908378751765
Experience 20, Iter 41, disc loss: 0.0008044430449630936, policy loss: 7.727378981854644
Experience 20, Iter 42, disc loss: 0.0007726275570735478, policy loss: 7.939715672142118
Experience 20, Iter 43, disc loss: 0.0007513349176426975, policy loss: 7.941170390746386
Experience 20, Iter 44, disc loss: 0.0007516219180908222, policy loss: 7.861338937204494
Experience 20, Iter 45, disc loss: 0.000771592688940352, policy loss: 7.739448574313883
Experience 20, Iter 46, disc loss: 0.0007028217801764226, policy loss: 8.206760159624256
Experience 20, Iter 47, disc loss: 0.0008237023042619671, policy loss: 7.737383272242113
Experience 20, Iter 48, disc loss: 0.0007558972787236799, policy loss: 7.876910567372826
Experience 20, Iter 49, disc loss: 0.0007633513020315643, policy loss: 7.829314262868785
Experience 20, Iter 50, disc loss: 0.0007464162804215662, policy loss: 7.935570748442938
Experience 20, Iter 51, disc loss: 0.0008207867459017182, policy loss: 7.689907036607595
Experience 20, Iter 52, disc loss: 0.0007477743076352141, policy loss: 7.905080535910075
Experience 20, Iter 53, disc loss: 0.0008577662078344811, policy loss: 7.6345780328928114
Experience 20, Iter 54, disc loss: 0.0007858389871655358, policy loss: 7.72935694580373
Experience 20, Iter 55, disc loss: 0.0007516571792435119, policy loss: 8.07774709785336
Experience 20, Iter 56, disc loss: 0.0008017740459272374, policy loss: 8.020770350692299
Experience 20, Iter 57, disc loss: 0.0007717380696515285, policy loss: 7.888375829732572
Experience 20, Iter 58, disc loss: 0.0007800442078856973, policy loss: 7.914588577977689
Experience 20, Iter 59, disc loss: 0.0007878168824010546, policy loss: 7.637156259746474
Experience 20, Iter 60, disc loss: 0.0007523041778493683, policy loss: 7.8242290178957425
Experience 20, Iter 61, disc loss: 0.0007799982031663909, policy loss: 7.8381640581162895
Experience 20, Iter 62, disc loss: 0.000683601869805922, policy loss: 8.096960574495538
Experience 20, Iter 63, disc loss: 0.0007748923998773106, policy loss: 8.043407444020202
Experience 20, Iter 64, disc loss: 0.0007441169920862605, policy loss: 7.7580121310006405
Experience 20, Iter 65, disc loss: 0.0007222772754883226, policy loss: 7.866428056274612
Experience 20, Iter 66, disc loss: 0.0007451606724101477, policy loss: 7.939049792505935
Experience 20, Iter 67, disc loss: 0.0007958570546853242, policy loss: 7.736563260432828
Experience 20, Iter 68, disc loss: 0.0006986193243303648, policy loss: 7.847053913799856
Experience 20, Iter 69, disc loss: 0.0007924904820265529, policy loss: 8.140729583513926
Experience 20, Iter 70, disc loss: 0.0006734446802846992, policy loss: 7.9505977771909455
Experience 20, Iter 71, disc loss: 0.0006444280749194214, policy loss: 8.028779055920909
Experience 20, Iter 72, disc loss: 0.0006861680077751234, policy loss: 7.961590075121302
Experience 20, Iter 73, disc loss: 0.0007924082919633692, policy loss: 7.807173614900859
Experience 20, Iter 74, disc loss: 0.000719231046015408, policy loss: 7.8612207837742005
Experience 20, Iter 75, disc loss: 0.0008125376304628844, policy loss: 7.794409096359328
Experience 20, Iter 76, disc loss: 0.0007673108923498414, policy loss: 7.662200561466982
Experience 20, Iter 77, disc loss: 0.0007251145299051994, policy loss: 7.974235707198047
Experience 20, Iter 78, disc loss: 0.0008261904093692307, policy loss: 7.7681423862215215
Experience 20, Iter 79, disc loss: 0.0006794101005425353, policy loss: 8.112885965226043
Experience 20, Iter 80, disc loss: 0.0007274175662911066, policy loss: 8.071152473922634
Experience 20, Iter 81, disc loss: 0.0008052428060977559, policy loss: 7.633846314612289
Experience 20, Iter 82, disc loss: 0.0007352190891437025, policy loss: 7.934482651932793
Experience 20, Iter 83, disc loss: 0.0007888997195843453, policy loss: 7.879482275412563
Experience 20, Iter 84, disc loss: 0.0007820032805061149, policy loss: 7.92056935543737
Experience 20, Iter 85, disc loss: 0.0008118700865343228, policy loss: 7.736192312360837
Experience 20, Iter 86, disc loss: 0.0008093395386525801, policy loss: 7.628420939116483
Experience 20, Iter 87, disc loss: 0.0006874734811595422, policy loss: 8.162778716801396
Experience 20, Iter 88, disc loss: 0.0006672436154730173, policy loss: 8.073957268327018
Experience 20, Iter 89, disc loss: 0.0007791587362581062, policy loss: 7.742325528448645
Experience 20, Iter 90, disc loss: 0.0007161422714348301, policy loss: 7.9193685054888014
Experience 20, Iter 91, disc loss: 0.0007338338336120662, policy loss: 7.906651503450645
Experience 20, Iter 92, disc loss: 0.0006925191158721505, policy loss: 8.037002773889178
Experience 20, Iter 93, disc loss: 0.0007238362628682429, policy loss: 8.048873299863432
Experience 20, Iter 94, disc loss: 0.0007847867891465149, policy loss: 7.813858576302525
Experience 20, Iter 95, disc loss: 0.0007865773228693271, policy loss: 7.843481821922948
Experience 20, Iter 96, disc loss: 0.0008004535958357008, policy loss: 7.6589799398427125
Experience 20, Iter 97, disc loss: 0.0007101570115837394, policy loss: 7.924946145464691
Experience 20, Iter 98, disc loss: 0.0008106476404107518, policy loss: 7.749566324432924
Experience 20, Iter 99, disc loss: 0.0007022953430128145, policy loss: 8.078931373247881
