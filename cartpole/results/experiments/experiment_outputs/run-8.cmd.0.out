Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0023],
        [0.2489],
        [0.0063]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0094, 0.9956, 0.0251], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0094, 0.9956, 0.0251])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.475
Iter 2/2000 - Loss: -0.308
Iter 3/2000 - Loss: -0.909
Iter 4/2000 - Loss: -0.663
Iter 5/2000 - Loss: -0.490
Iter 6/2000 - Loss: -0.513
Iter 7/2000 - Loss: -0.576
Iter 8/2000 - Loss: -0.583
Iter 9/2000 - Loss: -0.582
Iter 10/2000 - Loss: -0.659
Iter 11/2000 - Loss: -0.811
Iter 12/2000 - Loss: -0.971
Iter 13/2000 - Loss: -1.073
Iter 14/2000 - Loss: -1.099
Iter 15/2000 - Loss: -1.061
Iter 16/2000 - Loss: -0.986
Iter 17/2000 - Loss: -0.915
Iter 18/2000 - Loss: -0.890
Iter 19/2000 - Loss: -0.931
Iter 20/2000 - Loss: -1.018
Iter 1981/2000 - Loss: -1.410
Iter 1982/2000 - Loss: -1.410
Iter 1983/2000 - Loss: -1.410
Iter 1984/2000 - Loss: -1.410
Iter 1985/2000 - Loss: -1.410
Iter 1986/2000 - Loss: -1.410
Iter 1987/2000 - Loss: -1.410
Iter 1988/2000 - Loss: -1.410
Iter 1989/2000 - Loss: -1.410
Iter 1990/2000 - Loss: -1.410
Iter 1991/2000 - Loss: -1.410
Iter 1992/2000 - Loss: -1.410
Iter 1993/2000 - Loss: -1.410
Iter 1994/2000 - Loss: -1.410
Iter 1995/2000 - Loss: -1.410
Iter 1996/2000 - Loss: -1.410
Iter 1997/2000 - Loss: -1.410
Iter 1998/2000 - Loss: -1.410
Iter 1999/2000 - Loss: -1.410
Iter 2000/2000 - Loss: -1.410
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0017],
        [0.1514],
        [0.0045]])
Lengthscale: tensor([[[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]],

        [[0.0104, 0.0499, 0.2774, 0.0060, 0.0005, 0.0209]]])
Signal Variance: tensor([0.0033, 0.0067, 0.7446, 0.0181])
Estimated target variance: tensor([0.0046, 0.0094, 0.9956, 0.0251])
N: 10
Signal to noise ratio: tensor([2.0014, 2.0037, 2.2173, 2.0083])
Bound on condition number: tensor([41.0556, 41.1485, 50.1646, 41.3309])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4634849009895865, policy loss: 0.6374648621955981
Experience 1, Iter 1, disc loss: 1.4516714879206611, policy loss: 0.6398329538639855
Experience 1, Iter 2, disc loss: 1.4371888169467621, policy loss: 0.6454689796031641
Experience 1, Iter 3, disc loss: 1.4243836136619066, policy loss: 0.6498598262437164
Experience 1, Iter 4, disc loss: 1.414803872807447, policy loss: 0.651669658510899
Experience 1, Iter 5, disc loss: 1.3992527350650406, policy loss: 0.6596307291421165
Experience 1, Iter 6, disc loss: 1.388979219807516, policy loss: 0.6624785682585644
Experience 1, Iter 7, disc loss: 1.3749354928803355, policy loss: 0.6692727268893788
Experience 1, Iter 8, disc loss: 1.3662555405911583, policy loss: 0.6700275463975183
Experience 1, Iter 9, disc loss: 1.3507160599805674, policy loss: 0.6796925822245284
Experience 1, Iter 10, disc loss: 1.3383706230222168, policy loss: 0.6855325009574182
Experience 1, Iter 11, disc loss: 1.3279106501906854, policy loss: 0.6896046473820817
Experience 1, Iter 12, disc loss: 1.3171264479810536, policy loss: 0.6941444890560322
Experience 1, Iter 13, disc loss: 1.3018088875423677, policy loss: 0.7041236574716192
Experience 1, Iter 14, disc loss: 1.2898317884445187, policy loss: 0.7106691871478605
Experience 1, Iter 15, disc loss: 1.2769990445469634, policy loss: 0.7180038280736125
Experience 1, Iter 16, disc loss: 1.2737458316539043, policy loss: 0.7149645952898387
Experience 1, Iter 17, disc loss: 1.2627678310790276, policy loss: 0.7211936720037837
Experience 1, Iter 18, disc loss: 1.241774969779487, policy loss: 0.7385555819268632
Experience 1, Iter 19, disc loss: 1.2391992958304452, policy loss: 0.7361011676227629
Experience 1, Iter 20, disc loss: 1.2070237888261768, policy loss: 0.7684348976218236
Experience 1, Iter 21, disc loss: 1.2228072764495597, policy loss: 0.7453397550339464
Experience 1, Iter 22, disc loss: 1.202162756461175, policy loss: 0.7653392135446565
Experience 1, Iter 23, disc loss: 1.2034545724432948, policy loss: 0.7576630753803516
Experience 1, Iter 24, disc loss: 1.1803856307287748, policy loss: 0.7808023014450161
Experience 1, Iter 25, disc loss: 1.151898666819195, policy loss: 0.8122946847545399
Experience 1, Iter 26, disc loss: 1.1457428735302653, policy loss: 0.8148165960538352
Experience 1, Iter 27, disc loss: 1.1426493508409468, policy loss: 0.8118947986366138
Experience 1, Iter 28, disc loss: 1.1192110332571197, policy loss: 0.830894077324154
Experience 1, Iter 29, disc loss: 1.1344351850337344, policy loss: 0.7993495827033607
Experience 1, Iter 30, disc loss: 1.102349706665811, policy loss: 0.8367543857115323
Experience 1, Iter 31, disc loss: 1.096541836536689, policy loss: 0.8300521827932583
Experience 1, Iter 32, disc loss: 1.0862035514526283, policy loss: 0.830553982540578
Experience 1, Iter 33, disc loss: 1.0471546069258422, policy loss: 0.8748924756756034
Experience 1, Iter 34, disc loss: 1.0352105206917408, policy loss: 0.8807349154879592
Experience 1, Iter 35, disc loss: 1.0099177809457407, policy loss: 0.9058029056145027
Experience 1, Iter 36, disc loss: 0.9817826484942973, policy loss: 0.937388202891164
Experience 1, Iter 37, disc loss: 0.9748030843090463, policy loss: 0.9335932120913677
Experience 1, Iter 38, disc loss: 0.9592237158522052, policy loss: 0.9466937730650117
Experience 1, Iter 39, disc loss: 0.9192700408781632, policy loss: 0.9926383476900025
Experience 1, Iter 40, disc loss: 0.9394035662867768, policy loss: 0.9469309964316148
Experience 1, Iter 41, disc loss: 0.9249095266520369, policy loss: 0.9599359752852135
Experience 1, Iter 42, disc loss: 0.9109519687766787, policy loss: 0.9705861557072912
Experience 1, Iter 43, disc loss: 0.8782158542360176, policy loss: 0.9987243707101732
Experience 1, Iter 44, disc loss: 0.8913362347015176, policy loss: 0.9660862189768763
Experience 1, Iter 45, disc loss: 0.848506263679096, policy loss: 1.0232661166507766
Experience 1, Iter 46, disc loss: 0.821947554334703, policy loss: 1.0472906824423736
Experience 1, Iter 47, disc loss: 0.7999117874463163, policy loss: 1.0855130864616978
Experience 1, Iter 48, disc loss: 0.7912485009711587, policy loss: 1.0816749580917704
Experience 1, Iter 49, disc loss: 0.7837704479239476, policy loss: 1.0703808387093467
Experience 1, Iter 50, disc loss: 0.7388186109501468, policy loss: 1.150055455454328
Experience 1, Iter 51, disc loss: 0.7163319850643459, policy loss: 1.1726492024984765
Experience 1, Iter 52, disc loss: 0.7426179899085608, policy loss: 1.1025311249555712
Experience 1, Iter 53, disc loss: 0.6778407587183792, policy loss: 1.2251614764772776
Experience 1, Iter 54, disc loss: 0.6614509996367102, policy loss: 1.2302532616763882
Experience 1, Iter 55, disc loss: 0.6368350380909669, policy loss: 1.2685268354198698
Experience 1, Iter 56, disc loss: 0.6596061804577409, policy loss: 1.1784521335225222
Experience 1, Iter 57, disc loss: 0.6118406691249161, policy loss: 1.2890104555924304
Experience 1, Iter 58, disc loss: 0.5813506729347829, policy loss: 1.3191540355508526
Experience 1, Iter 59, disc loss: 0.5930362679590855, policy loss: 1.2639140852319548
Experience 1, Iter 60, disc loss: 0.5771636310522728, policy loss: 1.30765891533208
Experience 1, Iter 61, disc loss: 0.49135487058422944, policy loss: 1.5188155395297613
Experience 1, Iter 62, disc loss: 0.5190509665904195, policy loss: 1.3823295375991118
Experience 1, Iter 63, disc loss: 0.5125309239781299, policy loss: 1.3771557538759438
Experience 1, Iter 64, disc loss: 0.46234237118616206, policy loss: 1.5064349041615903
Experience 1, Iter 65, disc loss: 0.45876960236519954, policy loss: 1.4738833438765393
Experience 1, Iter 66, disc loss: 0.4537914220667961, policy loss: 1.507341705231469
Experience 1, Iter 67, disc loss: 0.4300895939682197, policy loss: 1.5353417582786135
Experience 1, Iter 68, disc loss: 0.39883530210668594, policy loss: 1.625416932919551
Experience 1, Iter 69, disc loss: 0.40287210077239477, policy loss: 1.5941884584802772
Experience 1, Iter 70, disc loss: 0.36661326135485545, policy loss: 1.6792997980834312
Experience 1, Iter 71, disc loss: 0.3641969627521421, policy loss: 1.7165675058477539
Experience 1, Iter 72, disc loss: 0.34276005722691977, policy loss: 1.7634846556028818
Experience 1, Iter 73, disc loss: 0.33603663764957925, policy loss: 1.761700688044369
Experience 1, Iter 74, disc loss: 0.321130936308368, policy loss: 1.8130457890727432
Experience 1, Iter 75, disc loss: 0.31755059107642913, policy loss: 1.8028220655938847
Experience 1, Iter 76, disc loss: 0.29086779028408716, policy loss: 1.890804197383273
Experience 1, Iter 77, disc loss: 0.28293191494636827, policy loss: 1.918908286230391
Experience 1, Iter 78, disc loss: 0.2885427377344826, policy loss: 1.8907392739821853
Experience 1, Iter 79, disc loss: 0.28649493799205467, policy loss: 1.850632490503386
Experience 1, Iter 80, disc loss: 0.2481301663066643, policy loss: 2.013651693885983
Experience 1, Iter 81, disc loss: 0.26974722488206304, policy loss: 1.9301149175418735
Experience 1, Iter 82, disc loss: 0.24867408522095202, policy loss: 2.005949676811947
Experience 1, Iter 83, disc loss: 0.2141202492686124, policy loss: 2.149237418901154
Experience 1, Iter 84, disc loss: 0.21633387813670346, policy loss: 2.193720001850179
Experience 1, Iter 85, disc loss: 0.19861351775294284, policy loss: 2.227909230247911
Experience 1, Iter 86, disc loss: 0.21944864692991886, policy loss: 2.0807748622745903
Experience 1, Iter 87, disc loss: 0.1982482416791601, policy loss: 2.2216893266677147
Experience 1, Iter 88, disc loss: 0.205018797515728, policy loss: 2.1620211966866454
Experience 1, Iter 89, disc loss: 0.19443689261316965, policy loss: 2.1827495287950716
Experience 1, Iter 90, disc loss: 0.16881093525903731, policy loss: 2.3989719655161648
Experience 1, Iter 91, disc loss: 0.16403305110202254, policy loss: 2.4502256948147845
Experience 1, Iter 92, disc loss: 0.15623096968362987, policy loss: 2.460972913254408
Experience 1, Iter 93, disc loss: 0.17080958789839762, policy loss: 2.4025034154090035
Experience 1, Iter 94, disc loss: 0.15375380588987897, policy loss: 2.460840451651188
Experience 1, Iter 95, disc loss: 0.13754325125329295, policy loss: 2.558098104114776
Experience 1, Iter 96, disc loss: 0.137489240970592, policy loss: 2.5137292363797643
Experience 1, Iter 97, disc loss: 0.14382650894074564, policy loss: 2.4956466165589952
Experience 1, Iter 98, disc loss: 0.13326660821809239, policy loss: 2.657198410311536
Experience 1, Iter 99, disc loss: 0.12880637830151875, policy loss: 2.6475405673375128
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0031],
        [0.1557],
        [0.0039]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0123, 0.6226, 0.0157], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0123, 0.6226, 0.0157])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.172
Iter 2/2000 - Loss: -1.324
Iter 3/2000 - Loss: -1.182
Iter 4/2000 - Loss: -0.990
Iter 5/2000 - Loss: -1.061
Iter 6/2000 - Loss: -1.258
Iter 7/2000 - Loss: -1.354
Iter 8/2000 - Loss: -1.332
Iter 9/2000 - Loss: -1.314
Iter 10/2000 - Loss: -1.383
Iter 11/2000 - Loss: -1.488
Iter 12/2000 - Loss: -1.533
Iter 13/2000 - Loss: -1.488
Iter 14/2000 - Loss: -1.412
Iter 15/2000 - Loss: -1.405
Iter 16/2000 - Loss: -1.495
Iter 17/2000 - Loss: -1.619
Iter 18/2000 - Loss: -1.688
Iter 19/2000 - Loss: -1.670
Iter 20/2000 - Loss: -1.611
Iter 1981/2000 - Loss: -1.791
Iter 1982/2000 - Loss: -1.791
Iter 1983/2000 - Loss: -1.791
Iter 1984/2000 - Loss: -1.791
Iter 1985/2000 - Loss: -1.791
Iter 1986/2000 - Loss: -1.791
Iter 1987/2000 - Loss: -1.790
Iter 1988/2000 - Loss: -1.789
Iter 1989/2000 - Loss: -1.788
Iter 1990/2000 - Loss: -1.786
Iter 1991/2000 - Loss: -1.782
Iter 1992/2000 - Loss: -1.777
Iter 1993/2000 - Loss: -1.772
Iter 1994/2000 - Loss: -1.770
Iter 1995/2000 - Loss: -1.773
Iter 1996/2000 - Loss: -1.782
Iter 1997/2000 - Loss: -1.790
Iter 1998/2000 - Loss: -1.790
Iter 1999/2000 - Loss: -1.784
Iter 2000/2000 - Loss: -1.775
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0023],
        [0.1073],
        [0.0030]])
Lengthscale: tensor([[[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]],

        [[0.0066, 0.0308, 0.1675, 0.0046, 0.0003, 0.0937]]])
Signal Variance: tensor([0.0025, 0.0093, 0.4842, 0.0120])
Estimated target variance: tensor([0.0033, 0.0123, 0.6226, 0.0157])
N: 20
Signal to noise ratio: tensor([2.0009, 2.0038, 2.1243, 2.0058])
Bound on condition number: tensor([81.0732, 81.3043, 91.2499, 81.4650])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.19873883591090996, policy loss: 2.0462702125984062
Experience 2, Iter 1, disc loss: 0.19039365126538269, policy loss: 2.0563269100543438
Experience 2, Iter 2, disc loss: 0.1875883519768232, policy loss: 2.0880503350603337
Experience 2, Iter 3, disc loss: 0.18903524365595178, policy loss: 2.1009587687435793
Experience 2, Iter 4, disc loss: 0.17371418465796418, policy loss: 2.1606744774039206
Experience 2, Iter 5, disc loss: 0.14816058515304933, policy loss: 2.3591258660595877
Experience 2, Iter 6, disc loss: 0.16321481409423375, policy loss: 2.1767269301058207
Experience 2, Iter 7, disc loss: 0.17259993923484782, policy loss: 2.1173925203536963
Experience 2, Iter 8, disc loss: 0.15263329513882035, policy loss: 2.319475330769685
Experience 2, Iter 9, disc loss: 0.1479233187844624, policy loss: 2.3168888742277036
Experience 2, Iter 10, disc loss: 0.15113364031472587, policy loss: 2.2764299307906923
Experience 2, Iter 11, disc loss: 0.1386602061102133, policy loss: 2.404458978002556
Experience 2, Iter 12, disc loss: 0.13673783213582355, policy loss: 2.42436822254356
Experience 2, Iter 13, disc loss: 0.12097662434113468, policy loss: 2.5722544152006837
Experience 2, Iter 14, disc loss: 0.12666008118686636, policy loss: 2.5058896614185397
Experience 2, Iter 15, disc loss: 0.13477216770669137, policy loss: 2.407335675615666
Experience 2, Iter 16, disc loss: 0.11502553993247394, policy loss: 2.5927581641079027
Experience 2, Iter 17, disc loss: 0.12385832697751796, policy loss: 2.535772543449306
Experience 2, Iter 18, disc loss: 0.11999918675149591, policy loss: 2.5911812557790648
Experience 2, Iter 19, disc loss: 0.1119424155657924, policy loss: 2.639432032640932
Experience 2, Iter 20, disc loss: 0.10355884748197507, policy loss: 2.6663194635911807
Experience 2, Iter 21, disc loss: 0.10971469029596216, policy loss: 2.6834187288937796
Experience 2, Iter 22, disc loss: 0.09702893239016314, policy loss: 2.763064664626641
Experience 2, Iter 23, disc loss: 0.1094977699927773, policy loss: 2.6252310448993574
Experience 2, Iter 24, disc loss: 0.10296179014942852, policy loss: 2.672770347050017
Experience 2, Iter 25, disc loss: 0.08778877087960943, policy loss: 2.8577447251523815
Experience 2, Iter 26, disc loss: 0.09421246910097472, policy loss: 2.7738402033887057
Experience 2, Iter 27, disc loss: 0.0919937919980092, policy loss: 2.824087938326391
Experience 2, Iter 28, disc loss: 0.08862713113916076, policy loss: 2.8614058417521004
Experience 2, Iter 29, disc loss: 0.08516200171261645, policy loss: 2.901374959605471
Experience 2, Iter 30, disc loss: 0.08767469422577032, policy loss: 2.9442353255234486
Experience 2, Iter 31, disc loss: 0.08622685803089059, policy loss: 2.914705506517749
Experience 2, Iter 32, disc loss: 0.07288974473355202, policy loss: 3.153736420580297
Experience 2, Iter 33, disc loss: 0.08543233478888762, policy loss: 2.9441465541914926
Experience 2, Iter 34, disc loss: 0.07973647669512195, policy loss: 3.0348267623545495
Experience 2, Iter 35, disc loss: 0.07687851891734195, policy loss: 3.049039012842118
Experience 2, Iter 36, disc loss: 0.07000783442921067, policy loss: 3.09039912234796
Experience 2, Iter 37, disc loss: 0.06711395553530632, policy loss: 3.1714061099480437
Experience 2, Iter 38, disc loss: 0.06569179783853611, policy loss: 3.2191139034908556
Experience 2, Iter 39, disc loss: 0.07112536233295944, policy loss: 3.0653598907178075
Experience 2, Iter 40, disc loss: 0.07497120716413092, policy loss: 3.0561978453134584
Experience 2, Iter 41, disc loss: 0.06588037408462269, policy loss: 3.1872630300046247
Experience 2, Iter 42, disc loss: 0.05628011646574275, policy loss: 3.3223069013553994
Experience 2, Iter 43, disc loss: 0.05795738682949926, policy loss: 3.3075817074528104
Experience 2, Iter 44, disc loss: 0.06264963950858841, policy loss: 3.1715054973587886
Experience 2, Iter 45, disc loss: 0.057341116633278066, policy loss: 3.3222442620855075
Experience 2, Iter 46, disc loss: 0.05691633618188956, policy loss: 3.343686006988957
Experience 2, Iter 47, disc loss: 0.050117391242465635, policy loss: 3.484200915636055
Experience 2, Iter 48, disc loss: 0.062109822878822094, policy loss: 3.2514468548857263
Experience 2, Iter 49, disc loss: 0.0556892684618987, policy loss: 3.3611094608812557
Experience 2, Iter 50, disc loss: 0.051016895994072956, policy loss: 3.492437015209835
Experience 2, Iter 51, disc loss: 0.04548547692977446, policy loss: 3.5491964827714426
Experience 2, Iter 52, disc loss: 0.04915649628759024, policy loss: 3.4886999606553775
Experience 2, Iter 53, disc loss: 0.05305843907910107, policy loss: 3.469517657856539
Experience 2, Iter 54, disc loss: 0.0477110703464094, policy loss: 3.521525904759618
Experience 2, Iter 55, disc loss: 0.047372040563765705, policy loss: 3.490928294824166
Experience 2, Iter 56, disc loss: 0.0425524493676178, policy loss: 3.719728623519061
Experience 2, Iter 57, disc loss: 0.04479366560995887, policy loss: 3.6686543638482494
Experience 2, Iter 58, disc loss: 0.042817609266656356, policy loss: 3.6211750641584635
Experience 2, Iter 59, disc loss: 0.044900182461124746, policy loss: 3.7035989302277645
Experience 2, Iter 60, disc loss: 0.0473352675728338, policy loss: 3.5506846260598866
Experience 2, Iter 61, disc loss: 0.04230374032460405, policy loss: 3.644197686035872
Experience 2, Iter 62, disc loss: 0.039966513481394336, policy loss: 3.776615951650737
Experience 2, Iter 63, disc loss: 0.04256144980146217, policy loss: 3.669901599298054
Experience 2, Iter 64, disc loss: 0.04314468137486107, policy loss: 3.6428198708870667
Experience 2, Iter 65, disc loss: 0.04324657552075101, policy loss: 3.723484233545056
Experience 2, Iter 66, disc loss: 0.039229328531028854, policy loss: 3.775757500811598
Experience 2, Iter 67, disc loss: 0.04129205166293838, policy loss: 3.6932460885198646
Experience 2, Iter 68, disc loss: 0.03762917991629172, policy loss: 3.826553958470823
Experience 2, Iter 69, disc loss: 0.03703362147957675, policy loss: 3.8072209177150964
Experience 2, Iter 70, disc loss: 0.034117768433969534, policy loss: 3.8539410588879086
Experience 2, Iter 71, disc loss: 0.032643974862019476, policy loss: 3.93313938971092
Experience 2, Iter 72, disc loss: 0.03996824619944183, policy loss: 3.6734833056347926
Experience 2, Iter 73, disc loss: 0.02842900237577612, policy loss: 4.165546590863235
Experience 2, Iter 74, disc loss: 0.03372271622439357, policy loss: 3.9386231695078835
Experience 2, Iter 75, disc loss: 0.030275958291200014, policy loss: 4.03140360775459
Experience 2, Iter 76, disc loss: 0.028917581718838493, policy loss: 4.029794554255432
Experience 2, Iter 77, disc loss: 0.032955678198960774, policy loss: 3.940298530373547
Experience 2, Iter 78, disc loss: 0.03182310551195162, policy loss: 4.004229972388218
Experience 2, Iter 79, disc loss: 0.03649096399538483, policy loss: 3.9410043427792276
Experience 2, Iter 80, disc loss: 0.030468958845089225, policy loss: 3.96658433142466
Experience 2, Iter 81, disc loss: 0.02425341815296874, policy loss: 4.3143941192687025
Experience 2, Iter 82, disc loss: 0.03004962549311027, policy loss: 4.011322014652777
Experience 2, Iter 83, disc loss: 0.026312146002470022, policy loss: 4.138274890067351
Experience 2, Iter 84, disc loss: 0.033120695527703234, policy loss: 3.9410017692953017
Experience 2, Iter 85, disc loss: 0.02773023669410348, policy loss: 4.071771760269989
Experience 2, Iter 86, disc loss: 0.02714718647254651, policy loss: 4.150492906373671
Experience 2, Iter 87, disc loss: 0.028791629603715917, policy loss: 4.105498611855404
Experience 2, Iter 88, disc loss: 0.027099915175527338, policy loss: 4.156383981122888
Experience 2, Iter 89, disc loss: 0.025108493127818817, policy loss: 4.249020172966647
Experience 2, Iter 90, disc loss: 0.02748647499849494, policy loss: 4.13434743891286
Experience 2, Iter 91, disc loss: 0.023377257769748826, policy loss: 4.3635990103836395
Experience 2, Iter 92, disc loss: 0.02293559613991888, policy loss: 4.363089957235065
Experience 2, Iter 93, disc loss: 0.024694199669108633, policy loss: 4.262703129334754
Experience 2, Iter 94, disc loss: 0.02224493662670747, policy loss: 4.380383894320373
Experience 2, Iter 95, disc loss: 0.024534407873503146, policy loss: 4.314724356539763
Experience 2, Iter 96, disc loss: 0.021644561133565415, policy loss: 4.395791850627048
Experience 2, Iter 97, disc loss: 0.01775789041657492, policy loss: 4.644271155593986
Experience 2, Iter 98, disc loss: 0.02306970164607315, policy loss: 4.275584996166466
Experience 2, Iter 99, disc loss: 0.02023552034961165, policy loss: 4.462761083219094
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0061],
        [0.1103],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1525e-02, 5.3278e-02, 1.1674e-01, 4.6875e-03, 2.2337e-04,
          2.3167e-01]],

        [[1.1525e-02, 5.3278e-02, 1.1674e-01, 4.6875e-03, 2.2337e-04,
          2.3167e-01]],

        [[1.1525e-02, 5.3278e-02, 1.1674e-01, 4.6875e-03, 2.2337e-04,
          2.3167e-01]],

        [[1.1525e-02, 5.3278e-02, 1.1674e-01, 4.6875e-03, 2.2337e-04,
          2.3167e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0062, 0.0244, 0.4411, 0.0109], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0062, 0.0244, 0.4411, 0.0109])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.150
Iter 2/2000 - Loss: -0.853
Iter 3/2000 - Loss: -1.187
Iter 4/2000 - Loss: -1.254
Iter 5/2000 - Loss: -1.152
Iter 6/2000 - Loss: -1.184
Iter 7/2000 - Loss: -1.316
Iter 8/2000 - Loss: -1.385
Iter 9/2000 - Loss: -1.333
Iter 10/2000 - Loss: -1.288
Iter 11/2000 - Loss: -1.336
Iter 12/2000 - Loss: -1.387
Iter 13/2000 - Loss: -1.394
Iter 14/2000 - Loss: -1.408
Iter 15/2000 - Loss: -1.426
Iter 16/2000 - Loss: -1.413
Iter 17/2000 - Loss: -1.407
Iter 18/2000 - Loss: -1.447
Iter 19/2000 - Loss: -1.492
Iter 20/2000 - Loss: -1.494
Iter 1981/2000 - Loss: -6.183
Iter 1982/2000 - Loss: -6.183
Iter 1983/2000 - Loss: -6.183
Iter 1984/2000 - Loss: -6.183
Iter 1985/2000 - Loss: -6.183
Iter 1986/2000 - Loss: -6.183
Iter 1987/2000 - Loss: -6.183
Iter 1988/2000 - Loss: -6.183
Iter 1989/2000 - Loss: -6.183
Iter 1990/2000 - Loss: -6.183
Iter 1991/2000 - Loss: -6.183
Iter 1992/2000 - Loss: -6.183
Iter 1993/2000 - Loss: -6.183
Iter 1994/2000 - Loss: -6.183
Iter 1995/2000 - Loss: -6.183
Iter 1996/2000 - Loss: -6.183
Iter 1997/2000 - Loss: -6.183
Iter 1998/2000 - Loss: -6.183
Iter 1999/2000 - Loss: -6.183
Iter 2000/2000 - Loss: -6.183
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0009],
        [0.0021]])
Lengthscale: tensor([[[1.3849e+01, 3.2609e+00, 4.5573e+01, 1.3629e+01, 8.0285e+00,
          1.9364e+01]],

        [[2.7486e+01, 4.3931e+01, 4.1182e+01, 1.9781e+00, 7.8626e+00,
          8.3158e+00]],

        [[2.9355e+01, 3.1455e+01, 1.8062e+01, 7.5374e-01, 4.2566e+00,
          1.0604e+01]],

        [[8.5035e-03, 4.3910e-02, 9.7041e-02, 3.5384e-03, 1.6508e-04,
          1.8861e-01]]])
Signal Variance: tensor([0.0378, 0.5098, 7.0109, 0.0084])
Estimated target variance: tensor([0.0062, 0.0244, 0.4411, 0.0109])
N: 30
Signal to noise ratio: tensor([10.4790, 37.8391, 86.4408,  2.0038])
Bound on condition number: tensor([3.2953e+03, 4.2955e+04, 2.2416e+05, 1.2145e+02])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 1.073097180836115, policy loss: 0.9096935155670941
Experience 3, Iter 1, disc loss: 1.4371186364540192, policy loss: 0.8053429382830375
Experience 3, Iter 2, disc loss: 1.9413894456756624, policy loss: 0.5164950587871945
Experience 3, Iter 3, disc loss: 2.561918752727871, policy loss: 0.39435600016773287
Experience 3, Iter 4, disc loss: 2.729889504318423, policy loss: 0.37435553407163447
Experience 3, Iter 5, disc loss: 2.656210368940502, policy loss: 0.26234044509739884
Experience 3, Iter 6, disc loss: 2.287384229937879, policy loss: 0.32533444568280095
Experience 3, Iter 7, disc loss: 1.7833143156041833, policy loss: 0.513425932798368
Experience 3, Iter 8, disc loss: 1.3962178797522369, policy loss: 0.7747868820672834
Experience 3, Iter 9, disc loss: 0.8389195530496645, policy loss: 1.430776266888813
Experience 3, Iter 10, disc loss: 0.7824334844436832, policy loss: 1.351368465402499
Experience 3, Iter 11, disc loss: 0.47042562119364933, policy loss: 2.393663027738632
Experience 3, Iter 12, disc loss: 0.39623324865368825, policy loss: 2.5494801978960067
Experience 3, Iter 13, disc loss: 0.34698250190272695, policy loss: 3.23583488897428
Experience 3, Iter 14, disc loss: 0.3046226883472638, policy loss: 3.280402885965379
Experience 3, Iter 15, disc loss: 0.3363674285857202, policy loss: 3.2154474377300843
Experience 3, Iter 16, disc loss: 0.3994636856962546, policy loss: 2.8554434136514004
Experience 3, Iter 17, disc loss: 0.3760685678106608, policy loss: 3.0802767423824635
Experience 3, Iter 18, disc loss: 0.43147663208550424, policy loss: 2.895408431241104
Experience 3, Iter 19, disc loss: 0.4112263093730428, policy loss: 2.660220709998741
Experience 3, Iter 20, disc loss: 0.4442863752962174, policy loss: 2.5895578499191356
Experience 3, Iter 21, disc loss: 0.4264448128771342, policy loss: 2.400425658039982
Experience 3, Iter 22, disc loss: 0.4097169113718527, policy loss: 2.350411597599259
Experience 3, Iter 23, disc loss: 0.39873266253080597, policy loss: 2.333596718473451
Experience 3, Iter 24, disc loss: 0.37172005330987823, policy loss: 2.3352672775024637
Experience 3, Iter 25, disc loss: 0.3605975005532408, policy loss: 2.311103358681326
Experience 3, Iter 26, disc loss: 0.33393507612805484, policy loss: 2.3262937405271624
Experience 3, Iter 27, disc loss: 0.30466129543323983, policy loss: 2.412912480965331
Experience 3, Iter 28, disc loss: 0.2983454396290722, policy loss: 2.382435875375705
Experience 3, Iter 29, disc loss: 0.2884393088550778, policy loss: 2.2863316525705697
Experience 3, Iter 30, disc loss: 0.2845445583286971, policy loss: 2.2380844091352934
Experience 3, Iter 31, disc loss: 0.2581171726594091, policy loss: 2.301605714855267
Experience 3, Iter 32, disc loss: 0.25875165793988053, policy loss: 2.177279812311236
Experience 3, Iter 33, disc loss: 0.2553749601447739, policy loss: 2.1663845191442785
Experience 3, Iter 34, disc loss: 0.24276416121971267, policy loss: 2.1990756837741
Experience 3, Iter 35, disc loss: 0.2745954093788664, policy loss: 2.028487464146248
Experience 3, Iter 36, disc loss: 0.22767066005999526, policy loss: 2.206484026057295
Experience 3, Iter 37, disc loss: 0.24862318152051438, policy loss: 2.0337594523064957
Experience 3, Iter 38, disc loss: 0.24846558221816964, policy loss: 2.1112585515766913
Experience 3, Iter 39, disc loss: 0.22671772042930766, policy loss: 2.1473667129294416
Experience 3, Iter 40, disc loss: 0.259451543122205, policy loss: 1.9584434070790584
Experience 3, Iter 41, disc loss: 0.23146863647876756, policy loss: 2.0677791187639976
Experience 3, Iter 42, disc loss: 0.2555258266379202, policy loss: 1.9238841311558788
Experience 3, Iter 43, disc loss: 0.27435871952408764, policy loss: 1.8772069965434002
Experience 3, Iter 44, disc loss: 0.24232228332886205, policy loss: 1.9312562509130924
Experience 3, Iter 45, disc loss: 0.26094146104552246, policy loss: 1.9090232298782681
Experience 3, Iter 46, disc loss: 0.24931833909840107, policy loss: 1.9303024094224646
Experience 3, Iter 47, disc loss: 0.22427596093991697, policy loss: 2.0312375792156976
Experience 3, Iter 48, disc loss: 0.28371043432247234, policy loss: 1.7600357808885367
Experience 3, Iter 49, disc loss: 0.24746809992476362, policy loss: 1.9535567632263149
Experience 3, Iter 50, disc loss: 0.24943048348588182, policy loss: 1.9897949615861799
Experience 3, Iter 51, disc loss: 0.27052289560961706, policy loss: 1.8434045095206537
Experience 3, Iter 52, disc loss: 0.25031655596734853, policy loss: 1.9136913299123752
Experience 3, Iter 53, disc loss: 0.2049174218378016, policy loss: 2.183637973108468
Experience 3, Iter 54, disc loss: 0.2320238229011227, policy loss: 1.9537858505563996
Experience 3, Iter 55, disc loss: 0.22842215513044073, policy loss: 2.0700466572615204
Experience 3, Iter 56, disc loss: 0.26564285652024516, policy loss: 1.9125138958024868
Experience 3, Iter 57, disc loss: 0.232132605608438, policy loss: 1.922709330931855
Experience 3, Iter 58, disc loss: 0.21687440276402192, policy loss: 1.9982044821636347
Experience 3, Iter 59, disc loss: 0.23796521699990633, policy loss: 1.9663610759091803
Experience 3, Iter 60, disc loss: 0.19370641591740717, policy loss: 2.288286833543597
Experience 3, Iter 61, disc loss: 0.20947851245953492, policy loss: 2.189041560101287
Experience 3, Iter 62, disc loss: 0.15390964202572174, policy loss: 2.405755702481227
Experience 3, Iter 63, disc loss: 0.22339175586163126, policy loss: 2.091890266178066
Experience 3, Iter 64, disc loss: 0.17587334762969492, policy loss: 2.2411796629733027
Experience 3, Iter 65, disc loss: 0.1946813417980186, policy loss: 2.2154713531922168
Experience 3, Iter 66, disc loss: 0.20957164035700337, policy loss: 2.2046082909663847
Experience 3, Iter 67, disc loss: 0.1717770258802932, policy loss: 2.311146207593561
Experience 3, Iter 68, disc loss: 0.17205931249108347, policy loss: 2.3247538785826256
Experience 3, Iter 69, disc loss: 0.17283160228126504, policy loss: 2.198083273437907
Experience 3, Iter 70, disc loss: 0.19321778785931668, policy loss: 2.149987704537974
Experience 3, Iter 71, disc loss: 0.2156115530179806, policy loss: 2.0405229646830745
Experience 3, Iter 72, disc loss: 0.178069739582827, policy loss: 2.3385578082495275
Experience 3, Iter 73, disc loss: 0.15568224671087227, policy loss: 2.3991949722388
Experience 3, Iter 74, disc loss: 0.17459941028251152, policy loss: 2.285608245307766
Experience 3, Iter 75, disc loss: 0.202231774316887, policy loss: 2.1663531992981073
Experience 3, Iter 76, disc loss: 0.15507609554052199, policy loss: 2.433023451029786
Experience 3, Iter 77, disc loss: 0.16643498702686102, policy loss: 2.397320512287153
Experience 3, Iter 78, disc loss: 0.15699250333828513, policy loss: 2.4051247626221515
Experience 3, Iter 79, disc loss: 0.16213446632684905, policy loss: 2.386976122488726
Experience 3, Iter 80, disc loss: 0.15376873274127908, policy loss: 2.3899114596342854
Experience 3, Iter 81, disc loss: 0.15490428768598516, policy loss: 2.388165221849987
Experience 3, Iter 82, disc loss: 0.15813976185462097, policy loss: 2.354861777349978
Experience 3, Iter 83, disc loss: 0.16113042139490927, policy loss: 2.463652630464397
Experience 3, Iter 84, disc loss: 0.12684171679827363, policy loss: 2.6884895535814812
Experience 3, Iter 85, disc loss: 0.14176389985620957, policy loss: 2.5535647206027305
Experience 3, Iter 86, disc loss: 0.16853891054316072, policy loss: 2.454040958335532
Experience 3, Iter 87, disc loss: 0.16078136920007252, policy loss: 2.473755325315683
Experience 3, Iter 88, disc loss: 0.1529366114011393, policy loss: 2.549935127632828
Experience 3, Iter 89, disc loss: 0.14465232894562707, policy loss: 2.469478222910877
Experience 3, Iter 90, disc loss: 0.1247428380449339, policy loss: 2.6377335230928987
Experience 3, Iter 91, disc loss: 0.13426483724184618, policy loss: 2.536877541803004
Experience 3, Iter 92, disc loss: 0.13003766181572213, policy loss: 2.5935355840127494
Experience 3, Iter 93, disc loss: 0.13911542268666177, policy loss: 2.5185675977547177
Experience 3, Iter 94, disc loss: 0.1294532089721153, policy loss: 2.638600723851445
Experience 3, Iter 95, disc loss: 0.13768173339855252, policy loss: 2.5578125936092193
Experience 3, Iter 96, disc loss: 0.13660204030892423, policy loss: 2.5975622219219408
Experience 3, Iter 97, disc loss: 0.13217805247923073, policy loss: 2.6061759274085836
Experience 3, Iter 98, disc loss: 0.127271644751543, policy loss: 2.6308637253172327
Experience 3, Iter 99, disc loss: 0.1132470465821103, policy loss: 2.8191719306117804
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0068],
        [0.0879],
        [0.0020]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0213e-02, 5.2845e-02, 8.7780e-02, 4.0736e-03, 1.7710e-04,
          2.5957e-01]],

        [[1.0213e-02, 5.2845e-02, 8.7780e-02, 4.0736e-03, 1.7710e-04,
          2.5957e-01]],

        [[1.0213e-02, 5.2845e-02, 8.7780e-02, 4.0736e-03, 1.7710e-04,
          2.5957e-01]],

        [[1.0213e-02, 5.2845e-02, 8.7780e-02, 4.0736e-03, 1.7710e-04,
          2.5957e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0063, 0.0270, 0.3514, 0.0082], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0063, 0.0270, 0.3514, 0.0082])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.520
Iter 2/2000 - Loss: -0.704
Iter 3/2000 - Loss: -1.540
Iter 4/2000 - Loss: -1.441
Iter 5/2000 - Loss: -1.131
Iter 6/2000 - Loss: -1.280
Iter 7/2000 - Loss: -1.527
Iter 8/2000 - Loss: -1.567
Iter 9/2000 - Loss: -1.447
Iter 10/2000 - Loss: -1.369
Iter 11/2000 - Loss: -1.424
Iter 12/2000 - Loss: -1.536
Iter 13/2000 - Loss: -1.594
Iter 14/2000 - Loss: -1.566
Iter 15/2000 - Loss: -1.517
Iter 16/2000 - Loss: -1.524
Iter 17/2000 - Loss: -1.587
Iter 18/2000 - Loss: -1.645
Iter 19/2000 - Loss: -1.651
Iter 20/2000 - Loss: -1.631
Iter 1981/2000 - Loss: -6.176
Iter 1982/2000 - Loss: -6.176
Iter 1983/2000 - Loss: -6.176
Iter 1984/2000 - Loss: -6.176
Iter 1985/2000 - Loss: -6.176
Iter 1986/2000 - Loss: -6.176
Iter 1987/2000 - Loss: -6.176
Iter 1988/2000 - Loss: -6.176
Iter 1989/2000 - Loss: -6.176
Iter 1990/2000 - Loss: -6.177
Iter 1991/2000 - Loss: -6.177
Iter 1992/2000 - Loss: -6.177
Iter 1993/2000 - Loss: -6.177
Iter 1994/2000 - Loss: -6.177
Iter 1995/2000 - Loss: -6.177
Iter 1996/2000 - Loss: -6.177
Iter 1997/2000 - Loss: -6.177
Iter 1998/2000 - Loss: -6.177
Iter 1999/2000 - Loss: -6.177
Iter 2000/2000 - Loss: -6.177
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0650],
        [0.0001]])
Lengthscale: tensor([[[1.4023e+01, 3.3786e+00, 4.0532e+01, 1.2373e+01, 7.4625e+00,
          1.9408e+01]],

        [[2.2733e+01, 3.2250e+01, 3.8327e+01, 2.1049e+00, 7.5035e+00,
          8.9815e+00]],

        [[7.3086e-03, 3.9912e-02, 6.0037e-02, 4.0308e-03, 1.7566e-04,
          1.6707e-01]],

        [[2.4857e+01, 3.7543e+01, 4.1123e+00, 1.8101e+00, 6.9209e+00,
          1.7157e+01]]])
Signal Variance: tensor([0.0380, 0.5351, 0.2777, 0.0686])
Estimated target variance: tensor([0.0063, 0.0270, 0.3514, 0.0082])
N: 40
Signal to noise ratio: tensor([10.5138, 41.9573,  2.0671, 21.5152])
Bound on condition number: tensor([ 4422.5607, 70417.7369,   171.9082, 18517.0964])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.226777288760308, policy loss: 2.593807884382546
Experience 4, Iter 1, disc loss: 0.18670234931288088, policy loss: 2.6936801055012296
Experience 4, Iter 2, disc loss: 0.2582427813096224, policy loss: 2.340452887982943
Experience 4, Iter 3, disc loss: 0.20025213452888255, policy loss: 2.627077059846976
Experience 4, Iter 4, disc loss: 0.24995381133884198, policy loss: 2.074314543534795
Experience 4, Iter 5, disc loss: 0.2098327098279306, policy loss: 2.404024291374711
Experience 4, Iter 6, disc loss: 0.25286269019276814, policy loss: 2.1794499156143843
Experience 4, Iter 7, disc loss: 0.24365756255816914, policy loss: 2.221164740656467
Experience 4, Iter 8, disc loss: 0.2512146412126142, policy loss: 2.079088261404177
Experience 4, Iter 9, disc loss: 0.25576785736072927, policy loss: 2.084101219052698
Experience 4, Iter 10, disc loss: 0.23485400323990235, policy loss: 2.1023688175474446
Experience 4, Iter 11, disc loss: 0.2241118930587811, policy loss: 2.1185630704020206
Experience 4, Iter 12, disc loss: 0.23033478683377512, policy loss: 2.154995535987995
Experience 4, Iter 13, disc loss: 0.2326420969168217, policy loss: 2.11760599755311
Experience 4, Iter 14, disc loss: 0.21238975701821947, policy loss: 2.1027585785556937
Experience 4, Iter 15, disc loss: 0.1818336603479189, policy loss: 2.295101147746127
Experience 4, Iter 16, disc loss: 0.21440841047323048, policy loss: 1.9883600615861217
Experience 4, Iter 17, disc loss: 0.17366128407483025, policy loss: 2.3275146066288785
Experience 4, Iter 18, disc loss: 0.16493390631830726, policy loss: 2.329206182311749
Experience 4, Iter 19, disc loss: 0.18867666663154006, policy loss: 2.2099164133860807
Experience 4, Iter 20, disc loss: 0.17323418893286585, policy loss: 2.3096992923953996
Experience 4, Iter 21, disc loss: 0.15872307899299504, policy loss: 2.3057696738710494
Experience 4, Iter 22, disc loss: 0.15140661468242256, policy loss: 2.3576024067562873
Experience 4, Iter 23, disc loss: 0.14629683886431943, policy loss: 2.3432484859917087
Experience 4, Iter 24, disc loss: 0.1314947765444797, policy loss: 2.5652662615080253
Experience 4, Iter 25, disc loss: 0.13151395929533433, policy loss: 2.521071296854938
Experience 4, Iter 26, disc loss: 0.13447089372481394, policy loss: 2.4716372332234586
Experience 4, Iter 27, disc loss: 0.13207384381199885, policy loss: 2.5402633289150973
Experience 4, Iter 28, disc loss: 0.13509931130166947, policy loss: 2.5619313922878146
Experience 4, Iter 29, disc loss: 0.13431909132225167, policy loss: 2.461708525831747
Experience 4, Iter 30, disc loss: 0.1129802482084403, policy loss: 2.7299037858782467
Experience 4, Iter 31, disc loss: 0.1282327690899696, policy loss: 2.4990766802904756
Experience 4, Iter 32, disc loss: 0.11127281738166794, policy loss: 2.76304155533787
Experience 4, Iter 33, disc loss: 0.11969395922829595, policy loss: 2.584348868383855
Experience 4, Iter 34, disc loss: 0.10768147173447078, policy loss: 2.6686180897069387
Experience 4, Iter 35, disc loss: 0.0941576724230998, policy loss: 3.0112770930310244
Experience 4, Iter 36, disc loss: 0.09775344312795711, policy loss: 2.8837775597913122
Experience 4, Iter 37, disc loss: 0.09604382376914726, policy loss: 2.879434357502637
Experience 4, Iter 38, disc loss: 0.09144039905564443, policy loss: 2.9519195262606095
Experience 4, Iter 39, disc loss: 0.09581372312872542, policy loss: 2.9296739058403
Experience 4, Iter 40, disc loss: 0.08922297921172784, policy loss: 2.9921440956393814
Experience 4, Iter 41, disc loss: 0.09054862700813776, policy loss: 2.9543050887998104
Experience 4, Iter 42, disc loss: 0.09781025039644935, policy loss: 2.887044835345558
Experience 4, Iter 43, disc loss: 0.08999592413830133, policy loss: 2.9271717599003244
Experience 4, Iter 44, disc loss: 0.08088321861511373, policy loss: 3.080779439711573
Experience 4, Iter 45, disc loss: 0.07583492841118725, policy loss: 3.2736712635914853
Experience 4, Iter 46, disc loss: 0.07835388538959553, policy loss: 3.15626414393284
Experience 4, Iter 47, disc loss: 0.0755552846127934, policy loss: 3.1876635170507375
Experience 4, Iter 48, disc loss: 0.0842639030668627, policy loss: 3.018447938413042
Experience 4, Iter 49, disc loss: 0.07691014452955672, policy loss: 3.1438327472407437
Experience 4, Iter 50, disc loss: 0.08010614391786587, policy loss: 3.043289370743908
Experience 4, Iter 51, disc loss: 0.06844493362455535, policy loss: 3.3425033802385613
Experience 4, Iter 52, disc loss: 0.06967734123873706, policy loss: 3.1983744208561116
Experience 4, Iter 53, disc loss: 0.06539686466629244, policy loss: 3.2829058595733036
Experience 4, Iter 54, disc loss: 0.06912185938965745, policy loss: 3.370277551510257
Experience 4, Iter 55, disc loss: 0.06855079670526512, policy loss: 3.2232667256630094
Experience 4, Iter 56, disc loss: 0.07612198757453466, policy loss: 3.121967858746035
Experience 4, Iter 57, disc loss: 0.06997964224271185, policy loss: 3.3361103025183274
Experience 4, Iter 58, disc loss: 0.06169845677326383, policy loss: 3.295556785086249
Experience 4, Iter 59, disc loss: 0.05764896170765174, policy loss: 3.5013906756574062
Experience 4, Iter 60, disc loss: 0.05829848125064141, policy loss: 3.482989776922287
Experience 4, Iter 61, disc loss: 0.05860606485236851, policy loss: 3.357087993088147
Experience 4, Iter 62, disc loss: 0.06024130680550989, policy loss: 3.42674523902555
Experience 4, Iter 63, disc loss: 0.06450453774455488, policy loss: 3.3085399577301855
Experience 4, Iter 64, disc loss: 0.05806153350253836, policy loss: 3.530713465694182
Experience 4, Iter 65, disc loss: 0.05297931275380722, policy loss: 3.5729668125171417
Experience 4, Iter 66, disc loss: 0.06078850403623834, policy loss: 3.3987879223537156
Experience 4, Iter 67, disc loss: 0.06661260884447241, policy loss: 3.3830377331285044
Experience 4, Iter 68, disc loss: 0.06388855294351922, policy loss: 3.3671466963287044
Experience 4, Iter 69, disc loss: 0.06002247243812882, policy loss: 3.3914638568032665
Experience 4, Iter 70, disc loss: 0.055692578052183764, policy loss: 3.549622837093385
Experience 4, Iter 71, disc loss: 0.055761930019385245, policy loss: 3.4206095482013605
Experience 4, Iter 72, disc loss: 0.05471040866136402, policy loss: 3.5273027255454137
Experience 4, Iter 73, disc loss: 0.05270005259194022, policy loss: 3.470823587643497
Experience 4, Iter 74, disc loss: 0.06389341715144643, policy loss: 3.361903032801366
Experience 4, Iter 75, disc loss: 0.05065414080597079, policy loss: 3.6067334194716976
Experience 4, Iter 76, disc loss: 0.056712044314829346, policy loss: 3.501547453636002
Experience 4, Iter 77, disc loss: 0.05879243737128073, policy loss: 3.376610481083909
Experience 4, Iter 78, disc loss: 0.05599093730577642, policy loss: 3.4560946714452907
Experience 4, Iter 79, disc loss: 0.05556568885056387, policy loss: 3.3840634949225237
Experience 4, Iter 80, disc loss: 0.04903460490325509, policy loss: 3.7055060026428617
Experience 4, Iter 81, disc loss: 0.056367687014185174, policy loss: 3.6291496327406394
Experience 4, Iter 82, disc loss: 0.06193158683645293, policy loss: 3.4241184063044803
Experience 4, Iter 83, disc loss: 0.051883840382877085, policy loss: 3.6522805707584056
Experience 4, Iter 84, disc loss: 0.049425958365887944, policy loss: 3.5986572126114087
Experience 4, Iter 85, disc loss: 0.05369182386510962, policy loss: 3.6052147764628057
Experience 4, Iter 86, disc loss: 0.05752258576372301, policy loss: 3.5354534689733175
Experience 4, Iter 87, disc loss: 0.05474140603713893, policy loss: 3.665520190322626
Experience 4, Iter 88, disc loss: 0.053236688454111086, policy loss: 3.6108117821266625
Experience 4, Iter 89, disc loss: 0.05468652712029956, policy loss: 3.701577491259571
Experience 4, Iter 90, disc loss: 0.053800383666090856, policy loss: 3.6304493221378187
Experience 4, Iter 91, disc loss: 0.06489784521680395, policy loss: 3.3806659482979917
Experience 4, Iter 92, disc loss: 0.052912955798663605, policy loss: 3.702538317891019
Experience 4, Iter 93, disc loss: 0.051437896988446474, policy loss: 3.581788797551283
Experience 4, Iter 94, disc loss: 0.052203957363131895, policy loss: 3.6843780240271986
Experience 4, Iter 95, disc loss: 0.05445513529665122, policy loss: 3.6685443095774173
Experience 4, Iter 96, disc loss: 0.04629587989555965, policy loss: 3.68876815233215
Experience 4, Iter 97, disc loss: 0.050925359411095855, policy loss: 3.701056282502915
Experience 4, Iter 98, disc loss: 0.04845047183048248, policy loss: 3.8359473330508855
Experience 4, Iter 99, disc loss: 0.04840146493953581, policy loss: 3.658124545765994
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0208],
        [0.0849],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4526e-02, 1.1022e-01, 9.8826e-02, 7.3439e-03, 7.3633e-04,
          9.9377e-01]],

        [[1.4526e-02, 1.1022e-01, 9.8826e-02, 7.3439e-03, 7.3633e-04,
          9.9377e-01]],

        [[1.4526e-02, 1.1022e-01, 9.8826e-02, 7.3439e-03, 7.3633e-04,
          9.9377e-01]],

        [[1.4526e-02, 1.1022e-01, 9.8826e-02, 7.3439e-03, 7.3633e-04,
          9.9377e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0135, 0.0834, 0.3394, 0.0094], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0135, 0.0834, 0.3394, 0.0094])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.522
Iter 2/2000 - Loss: -0.093
Iter 3/2000 - Loss: -0.557
Iter 4/2000 - Loss: -0.517
Iter 5/2000 - Loss: -0.343
Iter 6/2000 - Loss: -0.428
Iter 7/2000 - Loss: -0.579
Iter 8/2000 - Loss: -0.603
Iter 9/2000 - Loss: -0.528
Iter 10/2000 - Loss: -0.501
Iter 11/2000 - Loss: -0.564
Iter 12/2000 - Loss: -0.640
Iter 13/2000 - Loss: -0.664
Iter 14/2000 - Loss: -0.654
Iter 15/2000 - Loss: -0.664
Iter 16/2000 - Loss: -0.712
Iter 17/2000 - Loss: -0.777
Iter 18/2000 - Loss: -0.837
Iter 19/2000 - Loss: -0.891
Iter 20/2000 - Loss: -0.952
Iter 1981/2000 - Loss: -7.868
Iter 1982/2000 - Loss: -7.868
Iter 1983/2000 - Loss: -7.868
Iter 1984/2000 - Loss: -7.868
Iter 1985/2000 - Loss: -7.868
Iter 1986/2000 - Loss: -7.868
Iter 1987/2000 - Loss: -7.868
Iter 1988/2000 - Loss: -7.868
Iter 1989/2000 - Loss: -7.868
Iter 1990/2000 - Loss: -7.869
Iter 1991/2000 - Loss: -7.869
Iter 1992/2000 - Loss: -7.869
Iter 1993/2000 - Loss: -7.869
Iter 1994/2000 - Loss: -7.869
Iter 1995/2000 - Loss: -7.869
Iter 1996/2000 - Loss: -7.869
Iter 1997/2000 - Loss: -7.869
Iter 1998/2000 - Loss: -7.869
Iter 1999/2000 - Loss: -7.869
Iter 2000/2000 - Loss: -7.869
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[17.1277,  2.9755, 32.7662, 15.0765,  8.3600, 38.0842]],

        [[22.1157, 35.6182, 29.6751,  2.7193,  8.6486, 12.1604]],

        [[22.1351, 30.5891, 22.3643,  1.0641, 11.8634, 14.6333]],

        [[21.0731, 33.0226,  5.9492,  2.2196,  8.1761, 25.3710]]])
Signal Variance: tensor([ 0.0920,  0.7485, 10.8812,  0.1325])
Estimated target variance: tensor([0.0135, 0.0834, 0.3394, 0.0094])
N: 50
Signal to noise ratio: tensor([16.4629, 47.8592, 78.3795, 23.0818])
Bound on condition number: tensor([ 13552.3066, 114526.3259, 307168.0414,  26639.5145])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.02365203832285402, policy loss: 4.3607989330194386
Experience 5, Iter 1, disc loss: 0.023512408713667457, policy loss: 4.3423614446766114
Experience 5, Iter 2, disc loss: 0.02285575517581223, policy loss: 4.453012794826068
Experience 5, Iter 3, disc loss: 0.024527554471732764, policy loss: 4.282561334790265
Experience 5, Iter 4, disc loss: 0.02308764864615977, policy loss: 4.4337285510883095
Experience 5, Iter 5, disc loss: 0.02434032685639523, policy loss: 4.299314977337126
Experience 5, Iter 6, disc loss: 0.02092660376250948, policy loss: 4.570424233060946
Experience 5, Iter 7, disc loss: 0.022478704115349482, policy loss: 4.447471613362255
Experience 5, Iter 8, disc loss: 0.02359178883273247, policy loss: 4.348742876945317
Experience 5, Iter 9, disc loss: 0.020444484526565544, policy loss: 4.606061098605592
Experience 5, Iter 10, disc loss: 0.021101544141938267, policy loss: 4.535830927533681
Experience 5, Iter 11, disc loss: 0.022736494542911254, policy loss: 4.40723270416328
Experience 5, Iter 12, disc loss: 0.022487359957026214, policy loss: 4.429069395747032
Experience 5, Iter 13, disc loss: 0.023816859407348123, policy loss: 4.333629859980954
Experience 5, Iter 14, disc loss: 0.02917037706541561, policy loss: 4.086862479459798
Experience 5, Iter 15, disc loss: 0.025509511411715355, policy loss: 4.335399557289661
Experience 5, Iter 16, disc loss: 0.022858957805924077, policy loss: 4.5602307099482715
Experience 5, Iter 17, disc loss: 0.02538056761104309, policy loss: 4.26798606549463
Experience 5, Iter 18, disc loss: 0.025560550137262116, policy loss: 4.2363750838525025
Experience 5, Iter 19, disc loss: 0.026086223496220237, policy loss: 4.296387334012422
Experience 5, Iter 20, disc loss: 0.024698858807167337, policy loss: 4.3000454446475
Experience 5, Iter 21, disc loss: 0.025208693187883987, policy loss: 4.344819558023364
Experience 5, Iter 22, disc loss: 0.025867140485920575, policy loss: 4.3816270364253755
Experience 5, Iter 23, disc loss: 0.02265525572857101, policy loss: 4.46813083529476
Experience 5, Iter 24, disc loss: 0.02141788391360578, policy loss: 4.563310273340065
Experience 5, Iter 25, disc loss: 0.022816670793518505, policy loss: 4.373720754847329
Experience 5, Iter 26, disc loss: 0.02869836063072675, policy loss: 4.0223549063177675
Experience 5, Iter 27, disc loss: 0.025865007987316807, policy loss: 4.229556204821371
Experience 5, Iter 28, disc loss: 0.02789203258886442, policy loss: 4.067954235120404
Experience 5, Iter 29, disc loss: 0.024683746396633743, policy loss: 4.275671158225807
Experience 5, Iter 30, disc loss: 0.023358815827306395, policy loss: 4.331213742407814
Experience 5, Iter 31, disc loss: 0.025816273823167958, policy loss: 4.166627310826884
Experience 5, Iter 32, disc loss: 0.028233605070482868, policy loss: 4.031793262815793
Experience 5, Iter 33, disc loss: 0.02534372461603138, policy loss: 4.207337658841958
Experience 5, Iter 34, disc loss: 0.031026047487340676, policy loss: 3.901013007025563
Experience 5, Iter 35, disc loss: 0.024879844263888663, policy loss: 4.218485473778262
Experience 5, Iter 36, disc loss: 0.029172175086387548, policy loss: 3.9723664978371893
Experience 5, Iter 37, disc loss: 0.029592038563915988, policy loss: 3.9832646594819834
Experience 5, Iter 38, disc loss: 0.02799897267557359, policy loss: 4.093062017870438
Experience 5, Iter 39, disc loss: 0.028811763070743505, policy loss: 4.049867233892368
Experience 5, Iter 40, disc loss: 0.035826448799032226, policy loss: 3.6875629750610983
Experience 5, Iter 41, disc loss: 0.0321009644833879, policy loss: 3.9838140338335277
Experience 5, Iter 42, disc loss: 0.029911625186091653, policy loss: 4.072972727414063
Experience 5, Iter 43, disc loss: 0.03247251514392314, policy loss: 3.9259871428985065
Experience 5, Iter 44, disc loss: 0.029264807840351726, policy loss: 4.047805315213658
Experience 5, Iter 45, disc loss: 0.03432260321008453, policy loss: 3.8734845800841766
Experience 5, Iter 46, disc loss: 0.028332414111804587, policy loss: 4.158777117088253
Experience 5, Iter 47, disc loss: 0.03379200952761383, policy loss: 3.8371422557452637
Experience 5, Iter 48, disc loss: 0.0332696042387906, policy loss: 3.891496578736599
Experience 5, Iter 49, disc loss: 0.03156408655594329, policy loss: 3.8895331114457097
Experience 5, Iter 50, disc loss: 0.03020006061771624, policy loss: 4.023817629924091
Experience 5, Iter 51, disc loss: 0.03315281058200901, policy loss: 3.817929353086318
Experience 5, Iter 52, disc loss: 0.03583460858063914, policy loss: 3.805474160987229
Experience 5, Iter 53, disc loss: 0.032974445148281586, policy loss: 3.9368002273549427
Experience 5, Iter 54, disc loss: 0.03279625306008723, policy loss: 3.8434176324819775
Experience 5, Iter 55, disc loss: 0.03638845688873863, policy loss: 3.667965275470471
Experience 5, Iter 56, disc loss: 0.039388846264004, policy loss: 3.521037055088961
Experience 5, Iter 57, disc loss: 0.036975035402998846, policy loss: 3.6429964762812994
Experience 5, Iter 58, disc loss: 0.037466737541026904, policy loss: 3.72216430817974
Experience 5, Iter 59, disc loss: 0.033170432820841866, policy loss: 3.9046393059687325
Experience 5, Iter 60, disc loss: 0.037691352694296465, policy loss: 3.6025557902536254
Experience 5, Iter 61, disc loss: 0.03509102293288704, policy loss: 3.821982373175741
Experience 5, Iter 62, disc loss: 0.03562106760328412, policy loss: 3.7653284887339202
Experience 5, Iter 63, disc loss: 0.033487490198958504, policy loss: 3.9211037963641573
Experience 5, Iter 64, disc loss: 0.035417735141712994, policy loss: 3.7764167021813853
Experience 5, Iter 65, disc loss: 0.03346383842486175, policy loss: 3.8286736676456803
Experience 5, Iter 66, disc loss: 0.033344983914403964, policy loss: 3.8050315971271655
Experience 5, Iter 67, disc loss: 0.03306913966065527, policy loss: 3.722882808339523
Experience 5, Iter 68, disc loss: 0.03593160320646457, policy loss: 3.6714084928692534
Experience 5, Iter 69, disc loss: 0.03550755663172235, policy loss: 3.707689060507927
Experience 5, Iter 70, disc loss: 0.03320464324853712, policy loss: 3.856297250900468
Experience 5, Iter 71, disc loss: 0.03374548510679463, policy loss: 3.8075308948870097
Experience 5, Iter 72, disc loss: 0.036534292643170665, policy loss: 3.669757528456691
Experience 5, Iter 73, disc loss: 0.03745371867798902, policy loss: 3.691043323666083
Experience 5, Iter 74, disc loss: 0.03425357321822623, policy loss: 3.772143902342542
Experience 5, Iter 75, disc loss: 0.02974511809880153, policy loss: 4.045955815814478
Experience 5, Iter 76, disc loss: 0.031986870547978365, policy loss: 3.9124563688024483
Experience 5, Iter 77, disc loss: 0.03491361887126176, policy loss: 3.721254517085825
Experience 5, Iter 78, disc loss: 0.033292566618292746, policy loss: 3.8438451130310467
Experience 5, Iter 79, disc loss: 0.031429942127105676, policy loss: 3.9185102241365826
Experience 5, Iter 80, disc loss: 0.03644732241098689, policy loss: 3.693514600974681
Experience 5, Iter 81, disc loss: 0.03252934064781254, policy loss: 3.8538920879889047
Experience 5, Iter 82, disc loss: 0.032736426463053844, policy loss: 3.8908155058023226
Experience 5, Iter 83, disc loss: 0.033376116316557514, policy loss: 3.9258234726402623
Experience 5, Iter 84, disc loss: 0.030027365599800675, policy loss: 4.1079772414471964
Experience 5, Iter 85, disc loss: 0.03585058724246191, policy loss: 3.76668000835568
Experience 5, Iter 86, disc loss: 0.03156204141727145, policy loss: 3.9854272938912696
Experience 5, Iter 87, disc loss: 0.029494828001192505, policy loss: 4.094477627146874
Experience 5, Iter 88, disc loss: 0.03406716048333361, policy loss: 3.7528141165584885
Experience 5, Iter 89, disc loss: 0.031197801980351506, policy loss: 3.9656667510118733
Experience 5, Iter 90, disc loss: 0.029941686239241334, policy loss: 4.0566583892739025
Experience 5, Iter 91, disc loss: 0.03202133218894468, policy loss: 3.9756607368612324
Experience 5, Iter 92, disc loss: 0.028283059266561685, policy loss: 4.140343877384111
Experience 5, Iter 93, disc loss: 0.03226011665989897, policy loss: 3.8652554373723556
Experience 5, Iter 94, disc loss: 0.030877009731150176, policy loss: 3.9421566965755908
Experience 5, Iter 95, disc loss: 0.029788386218553665, policy loss: 3.929330425956162
Experience 5, Iter 96, disc loss: 0.027431368256620174, policy loss: 4.119222920798357
Experience 5, Iter 97, disc loss: 0.03386829524779976, policy loss: 3.8110268404762184
Experience 5, Iter 98, disc loss: 0.0321160123599373, policy loss: 3.949983294888411
Experience 5, Iter 99, disc loss: 0.03154551864780522, policy loss: 3.95223843223415
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.0359],
        [0.1942],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.5839e-02, 1.6129e-01, 1.1661e-01, 9.3480e-03, 1.3297e-03,
          1.4734e+00]],

        [[1.5839e-02, 1.6129e-01, 1.1661e-01, 9.3480e-03, 1.3297e-03,
          1.4734e+00]],

        [[1.5839e-02, 1.6129e-01, 1.1661e-01, 9.3480e-03, 1.3297e-03,
          1.4734e+00]],

        [[1.5839e-02, 1.6129e-01, 1.1661e-01, 9.3480e-03, 1.3297e-03,
          1.4734e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0180, 0.1435, 0.7766, 0.0109], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0180, 0.1435, 0.7766, 0.0109])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.451
Iter 2/2000 - Loss: 0.528
Iter 3/2000 - Loss: 0.361
Iter 4/2000 - Loss: 0.311
Iter 5/2000 - Loss: 0.390
Iter 6/2000 - Loss: 0.347
Iter 7/2000 - Loss: 0.273
Iter 8/2000 - Loss: 0.280
Iter 9/2000 - Loss: 0.306
Iter 10/2000 - Loss: 0.266
Iter 11/2000 - Loss: 0.200
Iter 12/2000 - Loss: 0.168
Iter 13/2000 - Loss: 0.159
Iter 14/2000 - Loss: 0.123
Iter 15/2000 - Loss: 0.048
Iter 16/2000 - Loss: -0.041
Iter 17/2000 - Loss: -0.125
Iter 18/2000 - Loss: -0.210
Iter 19/2000 - Loss: -0.315
Iter 20/2000 - Loss: -0.447
Iter 1981/2000 - Loss: -7.807
Iter 1982/2000 - Loss: -7.807
Iter 1983/2000 - Loss: -7.808
Iter 1984/2000 - Loss: -7.808
Iter 1985/2000 - Loss: -7.808
Iter 1986/2000 - Loss: -7.808
Iter 1987/2000 - Loss: -7.808
Iter 1988/2000 - Loss: -7.808
Iter 1989/2000 - Loss: -7.808
Iter 1990/2000 - Loss: -7.808
Iter 1991/2000 - Loss: -7.808
Iter 1992/2000 - Loss: -7.808
Iter 1993/2000 - Loss: -7.808
Iter 1994/2000 - Loss: -7.808
Iter 1995/2000 - Loss: -7.808
Iter 1996/2000 - Loss: -7.808
Iter 1997/2000 - Loss: -7.808
Iter 1998/2000 - Loss: -7.808
Iter 1999/2000 - Loss: -7.808
Iter 2000/2000 - Loss: -7.808
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[18.0634,  3.2946, 28.8544, 15.3709,  8.9890, 49.7800]],

        [[22.0378, 36.2909, 23.5328,  2.7281,  5.4619, 14.0468]],

        [[25.1933, 33.8291, 23.8881,  1.0133,  8.2875, 16.9751]],

        [[19.2850, 34.7176,  6.5159,  2.0012,  8.4896, 18.7358]]])
Signal Variance: tensor([ 0.0920,  0.8643, 10.5375,  0.1074])
Estimated target variance: tensor([0.0180, 0.1435, 0.7766, 0.0109])
N: 60
Signal to noise ratio: tensor([16.6033, 47.6683, 75.1892, 21.6985])
Bound on condition number: tensor([ 16541.2225, 136337.0988, 339205.9062,  28250.4322])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.03602193933936115, policy loss: 3.747825509378423
Experience 6, Iter 1, disc loss: 0.033038279504190186, policy loss: 3.9574219067359886
Experience 6, Iter 2, disc loss: 0.03082877285899709, policy loss: 4.018145048546972
Experience 6, Iter 3, disc loss: 0.03212908504962217, policy loss: 3.9658002164206407
Experience 6, Iter 4, disc loss: 0.03535432230309459, policy loss: 3.93253325125563
Experience 6, Iter 5, disc loss: 0.03252749846408472, policy loss: 3.9078053067501015
Experience 6, Iter 6, disc loss: 0.03265510376749374, policy loss: 3.98142456561605
Experience 6, Iter 7, disc loss: 0.03233644138955027, policy loss: 4.078463457415218
Experience 6, Iter 8, disc loss: 0.03539453782302244, policy loss: 3.922683609508111
Experience 6, Iter 9, disc loss: 0.031738340285295534, policy loss: 4.0194081268075585
Experience 6, Iter 10, disc loss: 0.03449295825722803, policy loss: 3.9194198035417074
Experience 6, Iter 11, disc loss: 0.034533987618923796, policy loss: 3.948255196097258
Experience 6, Iter 12, disc loss: 0.03336105285350996, policy loss: 3.954763930278965
Experience 6, Iter 13, disc loss: 0.03502058649340739, policy loss: 3.8991686996437878
Experience 6, Iter 14, disc loss: 0.03653714881086831, policy loss: 3.8745604349069422
Experience 6, Iter 15, disc loss: 0.03322852524220331, policy loss: 4.001433778158661
Experience 6, Iter 16, disc loss: 0.033498100050528674, policy loss: 3.997371579619403
Experience 6, Iter 17, disc loss: 0.036854321310242806, policy loss: 3.8425392940787804
Experience 6, Iter 18, disc loss: 0.03340987267995115, policy loss: 3.978225527380784
Experience 6, Iter 19, disc loss: 0.02890832609259657, policy loss: 4.227604445479816
Experience 6, Iter 20, disc loss: 0.03197483435526559, policy loss: 4.07075770846954
Experience 6, Iter 21, disc loss: 0.03383118052270326, policy loss: 3.8882991214986538
Experience 6, Iter 22, disc loss: 0.03258982586829262, policy loss: 4.000289420877756
Experience 6, Iter 23, disc loss: 0.031186532885398886, policy loss: 4.218004317763202
Experience 6, Iter 24, disc loss: 0.031640925988578185, policy loss: 4.149679385477725
Experience 6, Iter 25, disc loss: 0.035671317601095466, policy loss: 4.018225160850333
Experience 6, Iter 26, disc loss: 0.03180509746921096, policy loss: 4.057532054504245
Experience 6, Iter 27, disc loss: 0.033590484014502146, policy loss: 3.945512639601138
Experience 6, Iter 28, disc loss: 0.03232963259565438, policy loss: 4.051179976056505
Experience 6, Iter 29, disc loss: 0.032520395740053054, policy loss: 4.032148438666234
Experience 6, Iter 30, disc loss: 0.03618281841593292, policy loss: 3.9019599203899444
Experience 6, Iter 31, disc loss: 0.0366655550403545, policy loss: 3.920949408301663
Experience 6, Iter 32, disc loss: 0.040078130072168386, policy loss: 3.7491018875505415
Experience 6, Iter 33, disc loss: 0.04140857430629216, policy loss: 3.815365356107758
Experience 6, Iter 34, disc loss: 0.03499662907548976, policy loss: 3.952327133753926
Experience 6, Iter 35, disc loss: 0.03611322785797584, policy loss: 3.851516046261139
Experience 6, Iter 36, disc loss: 0.0374987529170683, policy loss: 3.7672332849381345
Experience 6, Iter 37, disc loss: 0.035017942210035034, policy loss: 3.9329238228597463
Experience 6, Iter 38, disc loss: 0.03145965278290503, policy loss: 4.181748411529893
Experience 6, Iter 39, disc loss: 0.037175758692893315, policy loss: 3.9452200398105393
Experience 6, Iter 40, disc loss: 0.0371964391586031, policy loss: 3.9145017136195945
Experience 6, Iter 41, disc loss: 0.04536583007157189, policy loss: 3.606564123833273
Experience 6, Iter 42, disc loss: 0.03565450853585565, policy loss: 3.9662806109211406
Experience 6, Iter 43, disc loss: 0.037108579788499116, policy loss: 3.8547529939690883
Experience 6, Iter 44, disc loss: 0.037348958665502245, policy loss: 3.886324678719685
Experience 6, Iter 45, disc loss: 0.037582497289873604, policy loss: 3.8608414088976644
Experience 6, Iter 46, disc loss: 0.03957906783232572, policy loss: 3.889792044839364
Experience 6, Iter 47, disc loss: 0.038801374578791493, policy loss: 3.8985587924901006
Experience 6, Iter 48, disc loss: 0.04100645188304867, policy loss: 3.6770911768254653
Experience 6, Iter 49, disc loss: 0.039033479660451495, policy loss: 3.9169716957631686
Experience 6, Iter 50, disc loss: 0.04051570546014642, policy loss: 3.8394365431128037
Experience 6, Iter 51, disc loss: 0.04994018463779301, policy loss: 3.446046651749957
Experience 6, Iter 52, disc loss: 0.04815138131938411, policy loss: 3.7229826088806126
Experience 6, Iter 53, disc loss: 0.037757587725153696, policy loss: 3.945315405940465
Experience 6, Iter 54, disc loss: 0.05073240107977535, policy loss: 3.5657549082968787
Experience 6, Iter 55, disc loss: 0.04815620787596655, policy loss: 3.6232384761298686
Experience 6, Iter 56, disc loss: 0.04498099333136262, policy loss: 3.6949981525819853
Experience 6, Iter 57, disc loss: 0.04698315072119111, policy loss: 3.8200355769950893
Experience 6, Iter 58, disc loss: 0.04877663131310479, policy loss: 3.6793167308645205
Experience 6, Iter 59, disc loss: 0.0478478907317581, policy loss: 3.786181425463492
Experience 6, Iter 60, disc loss: 0.05346808315892469, policy loss: 3.417702884566855
Experience 6, Iter 61, disc loss: 0.057987666949505745, policy loss: 3.356188431567857
Experience 6, Iter 62, disc loss: 0.04896420831274426, policy loss: 3.6932004507374634
Experience 6, Iter 63, disc loss: 0.05332139777630361, policy loss: 3.508728893887393
Experience 6, Iter 64, disc loss: 0.04695284220490392, policy loss: 3.880903452703339
Experience 6, Iter 65, disc loss: 0.05208965659168595, policy loss: 3.6320009555293726
Experience 6, Iter 66, disc loss: 0.04611013161982335, policy loss: 3.7801469538659185
Experience 6, Iter 67, disc loss: 0.04909444330907107, policy loss: 3.7198921171298753
Experience 6, Iter 68, disc loss: 0.0504534117640109, policy loss: 3.814698864324445
Experience 6, Iter 69, disc loss: 0.04850887856852949, policy loss: 3.790566044898944
Experience 6, Iter 70, disc loss: 0.05038553067621499, policy loss: 3.7510575364704066
Experience 6, Iter 71, disc loss: 0.0521497210367804, policy loss: 3.655043606069272
Experience 6, Iter 72, disc loss: 0.054182086706181146, policy loss: 3.5496358059883106
Experience 6, Iter 73, disc loss: 0.05242650534511409, policy loss: 3.67405711023272
Experience 6, Iter 74, disc loss: 0.058100648076246125, policy loss: 3.531596198134962
Experience 6, Iter 75, disc loss: 0.055325075601794144, policy loss: 3.937414421451454
Experience 6, Iter 76, disc loss: 0.0540865604468667, policy loss: 3.8936627017368126
Experience 6, Iter 77, disc loss: 0.05661875768759532, policy loss: 3.755394522619339
Experience 6, Iter 78, disc loss: 0.05361559138970639, policy loss: 3.7584507638410045
Experience 6, Iter 79, disc loss: 0.053534512384531024, policy loss: 3.814551071814425
Experience 6, Iter 80, disc loss: 0.05591993963062461, policy loss: 3.857758783913892
Experience 6, Iter 81, disc loss: 0.057658309227250515, policy loss: 3.934609848516219
Experience 6, Iter 82, disc loss: 0.05256302080527185, policy loss: 4.050607240646727
Experience 6, Iter 83, disc loss: 0.052075634558034774, policy loss: 3.999383228924574
Experience 6, Iter 84, disc loss: 0.058773136819012695, policy loss: 3.7779836113838905
Experience 6, Iter 85, disc loss: 0.047861328439278275, policy loss: 4.0909338709109875
Experience 6, Iter 86, disc loss: 0.04732765441925518, policy loss: 4.470036120448931
Experience 6, Iter 87, disc loss: 0.04549087498344384, policy loss: 4.311085959899687
Experience 6, Iter 88, disc loss: 0.055956159654168475, policy loss: 3.809239862152741
Experience 6, Iter 89, disc loss: 0.05127461713712268, policy loss: 4.38383398189764
Experience 6, Iter 90, disc loss: 0.05799868018527475, policy loss: 3.9752896836722265
Experience 6, Iter 91, disc loss: 0.05798254070979436, policy loss: 3.9523077388101293
Experience 6, Iter 92, disc loss: 0.047363606373036, policy loss: 4.347887391363415
Experience 6, Iter 93, disc loss: 0.04692454633205861, policy loss: 4.363588930757455
Experience 6, Iter 94, disc loss: 0.04279617669016175, policy loss: 4.601032072982003
Experience 6, Iter 95, disc loss: 0.05189908715748967, policy loss: 4.0146479299586915
Experience 6, Iter 96, disc loss: 0.053553447450057516, policy loss: 4.221267393815295
Experience 6, Iter 97, disc loss: 0.057477498711605235, policy loss: 3.924405186688669
Experience 6, Iter 98, disc loss: 0.053343142360796084, policy loss: 4.215981708335922
Experience 6, Iter 99, disc loss: 0.05094686532342914, policy loss: 4.323275622974698
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.0506],
        [0.3374],
        [0.0038]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4885e-02, 1.6380e-01, 1.7401e-01, 9.2610e-03, 1.3051e-03,
          1.7845e+00]],

        [[1.4885e-02, 1.6380e-01, 1.7401e-01, 9.2610e-03, 1.3051e-03,
          1.7845e+00]],

        [[1.4885e-02, 1.6380e-01, 1.7401e-01, 9.2610e-03, 1.3051e-03,
          1.7845e+00]],

        [[1.4885e-02, 1.6380e-01, 1.7401e-01, 9.2610e-03, 1.3051e-03,
          1.7845e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0184, 0.2025, 1.3496, 0.0151], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0184, 0.2025, 1.3496, 0.0151])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.120
Iter 2/2000 - Loss: 1.056
Iter 3/2000 - Loss: 1.002
Iter 4/2000 - Loss: 0.917
Iter 5/2000 - Loss: 0.976
Iter 6/2000 - Loss: 0.971
Iter 7/2000 - Loss: 0.895
Iter 8/2000 - Loss: 0.872
Iter 9/2000 - Loss: 0.892
Iter 10/2000 - Loss: 0.880
Iter 11/2000 - Loss: 0.827
Iter 12/2000 - Loss: 0.773
Iter 13/2000 - Loss: 0.740
Iter 14/2000 - Loss: 0.702
Iter 15/2000 - Loss: 0.638
Iter 16/2000 - Loss: 0.551
Iter 17/2000 - Loss: 0.461
Iter 18/2000 - Loss: 0.369
Iter 19/2000 - Loss: 0.263
Iter 20/2000 - Loss: 0.134
Iter 1981/2000 - Loss: -7.750
Iter 1982/2000 - Loss: -7.750
Iter 1983/2000 - Loss: -7.750
Iter 1984/2000 - Loss: -7.751
Iter 1985/2000 - Loss: -7.751
Iter 1986/2000 - Loss: -7.751
Iter 1987/2000 - Loss: -7.751
Iter 1988/2000 - Loss: -7.751
Iter 1989/2000 - Loss: -7.751
Iter 1990/2000 - Loss: -7.751
Iter 1991/2000 - Loss: -7.751
Iter 1992/2000 - Loss: -7.751
Iter 1993/2000 - Loss: -7.751
Iter 1994/2000 - Loss: -7.751
Iter 1995/2000 - Loss: -7.751
Iter 1996/2000 - Loss: -7.751
Iter 1997/2000 - Loss: -7.751
Iter 1998/2000 - Loss: -7.751
Iter 1999/2000 - Loss: -7.751
Iter 2000/2000 - Loss: -7.751
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[19.3369,  8.2778, 34.8066, 17.6323, 10.2712, 63.9416]],

        [[22.0810,  9.8672, 27.8435,  3.1923,  4.8905, 25.5510]],

        [[24.7951, 37.6237, 22.5782,  1.0236,  7.4676, 22.3280]],

        [[18.9288, 28.5006,  9.1488,  2.4904,  9.2230, 22.1464]]])
Signal Variance: tensor([ 0.1417,  2.2154, 17.1419,  0.1702])
Estimated target variance: tensor([0.0184, 0.2025, 1.3496, 0.0151])
N: 70
Signal to noise ratio: tensor([19.6100, 76.5473, 91.4382, 27.9556])
Bound on condition number: tensor([ 26919.6585, 410165.0955, 585266.8063,  54707.0305])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.022353446741361895, policy loss: 6.147430600634821
Experience 7, Iter 1, disc loss: 0.023083902373105, policy loss: 5.8047368680559766
Experience 7, Iter 2, disc loss: 0.02146587355583088, policy loss: 5.821725323587525
Experience 7, Iter 3, disc loss: 0.028706506660029964, policy loss: 5.233455829636405
Experience 7, Iter 4, disc loss: 0.03379426133342616, policy loss: 5.067121808211087
Experience 7, Iter 5, disc loss: 0.03714927372812171, policy loss: 4.467382239259698
Experience 7, Iter 6, disc loss: 0.032696578742272345, policy loss: 4.7735947643232866
Experience 7, Iter 7, disc loss: 0.030050356601781198, policy loss: 4.9005477707832386
Experience 7, Iter 8, disc loss: 0.03483795842736222, policy loss: 4.48232513706919
Experience 7, Iter 9, disc loss: 0.02928252480088522, policy loss: 4.76455501081641
Experience 7, Iter 10, disc loss: 0.03734645082142043, policy loss: 4.253092976511099
Experience 7, Iter 11, disc loss: 0.03522878769992094, policy loss: 4.4297136482964765
Experience 7, Iter 12, disc loss: 0.03771942471596384, policy loss: 4.2376906542523995
Experience 7, Iter 13, disc loss: 0.03453616151305158, policy loss: 4.459762436488495
Experience 7, Iter 14, disc loss: 0.03596292019217917, policy loss: 4.550822463192331
Experience 7, Iter 15, disc loss: 0.043278314079360176, policy loss: 4.135026410223321
Experience 7, Iter 16, disc loss: 0.03764593828783692, policy loss: 4.452035147767203
Experience 7, Iter 17, disc loss: 0.038485769123981745, policy loss: 4.139661052331226
Experience 7, Iter 18, disc loss: 0.029841160086798706, policy loss: 4.78045040021337
Experience 7, Iter 19, disc loss: 0.036970631752701905, policy loss: 4.372266431392468
Experience 7, Iter 20, disc loss: 0.039614050267431325, policy loss: 4.153034822576371
Experience 7, Iter 21, disc loss: 0.042989083278920426, policy loss: 4.183770581948107
Experience 7, Iter 22, disc loss: 0.03675543426233823, policy loss: 4.4242206697253685
Experience 7, Iter 23, disc loss: 0.04513861679526213, policy loss: 4.02830257336008
Experience 7, Iter 24, disc loss: 0.039760716071666174, policy loss: 4.246436637499998
Experience 7, Iter 25, disc loss: 0.0402264456181004, policy loss: 4.494304315558649
Experience 7, Iter 26, disc loss: 0.03771804399248765, policy loss: 4.607469283126925
Experience 7, Iter 27, disc loss: 0.035650791305138256, policy loss: 4.533772757535203
Experience 7, Iter 28, disc loss: 0.037329926344827374, policy loss: 4.477691541370367
Experience 7, Iter 29, disc loss: 0.03629441593806917, policy loss: 4.257538790446547
Experience 7, Iter 30, disc loss: 0.037550324683528924, policy loss: 4.185356741763112
Experience 7, Iter 31, disc loss: 0.04200288537171913, policy loss: 4.304850302928965
Experience 7, Iter 32, disc loss: 0.03736059983585685, policy loss: 4.1820856924633
Experience 7, Iter 33, disc loss: 0.04352203276292291, policy loss: 4.272529488968479
Experience 7, Iter 34, disc loss: 0.04767041544197198, policy loss: 4.257601614588402
Experience 7, Iter 35, disc loss: 0.04030795365068151, policy loss: 4.5176254809894605
Experience 7, Iter 36, disc loss: 0.04146791747210139, policy loss: 4.5171982451510795
Experience 7, Iter 37, disc loss: 0.04456471129962232, policy loss: 4.677068400625719
Experience 7, Iter 38, disc loss: 0.03615382621351367, policy loss: 4.813597584483679
Experience 7, Iter 39, disc loss: 0.038893106652508204, policy loss: 4.395673750892326
Experience 7, Iter 40, disc loss: 0.03475662789706382, policy loss: 4.664415926949495
Experience 7, Iter 41, disc loss: 0.03882462818906752, policy loss: 4.5550565611971265
Experience 7, Iter 42, disc loss: 0.04092366260792336, policy loss: 4.508451873158708
Experience 7, Iter 43, disc loss: 0.04032673056543815, policy loss: 4.569956161219982
Experience 7, Iter 44, disc loss: 0.030996220360534364, policy loss: 5.20385378222368
Experience 7, Iter 45, disc loss: 0.038451682536916804, policy loss: 4.821733654353232
Experience 7, Iter 46, disc loss: 0.03742982293060443, policy loss: 4.511172737381141
Experience 7, Iter 47, disc loss: 0.039367028819203995, policy loss: 4.792392430920792
Experience 7, Iter 48, disc loss: 0.03881140959501947, policy loss: 4.580509781358186
Experience 7, Iter 49, disc loss: 0.03212842613843988, policy loss: 4.6766297618343105
Experience 7, Iter 50, disc loss: 0.03636808414647736, policy loss: 4.782533009003332
Experience 7, Iter 51, disc loss: 0.031015377807038735, policy loss: 4.95178087196048
Experience 7, Iter 52, disc loss: 0.02757765009819771, policy loss: 5.190408947114994
Experience 7, Iter 53, disc loss: 0.039748126753119645, policy loss: 4.4769200443881205
Experience 7, Iter 54, disc loss: 0.0313096487218232, policy loss: 4.7404888007844015
Experience 7, Iter 55, disc loss: 0.03563190759423598, policy loss: 4.735531136096771
Experience 7, Iter 56, disc loss: 0.03544955751646721, policy loss: 4.676879096926362
Experience 7, Iter 57, disc loss: 0.03546652339558021, policy loss: 4.6290597765152866
Experience 7, Iter 58, disc loss: 0.032310400550305034, policy loss: 4.944027251797551
Experience 7, Iter 59, disc loss: 0.033571948688000904, policy loss: 4.958191072755916
Experience 7, Iter 60, disc loss: 0.0357177195177088, policy loss: 4.900356438834918
Experience 7, Iter 61, disc loss: 0.03420373590370132, policy loss: 4.86339290206459
Experience 7, Iter 62, disc loss: 0.03717225283313236, policy loss: 4.592116155295468
Experience 7, Iter 63, disc loss: 0.03621043745887241, policy loss: 4.575300231675587
Experience 7, Iter 64, disc loss: 0.03588107238677901, policy loss: 4.971727254309583
Experience 7, Iter 65, disc loss: 0.03363877885339181, policy loss: 4.944132348958279
Experience 7, Iter 66, disc loss: 0.03653767582598015, policy loss: 5.193899416151588
Experience 7, Iter 67, disc loss: 0.03873113674182113, policy loss: 4.596854531363393
Experience 7, Iter 68, disc loss: 0.03070073930048464, policy loss: 4.948749720993432
Experience 7, Iter 69, disc loss: 0.030809235296103493, policy loss: 4.6858269480204235
Experience 7, Iter 70, disc loss: 0.030588176236072323, policy loss: 4.953584991273061
Experience 7, Iter 71, disc loss: 0.03255910437928011, policy loss: 4.653057829300463
Experience 7, Iter 72, disc loss: 0.030084679218223644, policy loss: 4.963440182984302
Experience 7, Iter 73, disc loss: 0.03416660224776618, policy loss: 5.077493740787206
Experience 7, Iter 74, disc loss: 0.037398509466934786, policy loss: 4.539438422229328
Experience 7, Iter 75, disc loss: 0.031509244065623485, policy loss: 4.6961752639804875
Experience 7, Iter 76, disc loss: 0.04765415589191038, policy loss: 4.592225637056743
Experience 7, Iter 77, disc loss: 0.03751733644367709, policy loss: 4.9145518532025
Experience 7, Iter 78, disc loss: 0.03882454185197628, policy loss: 4.6161731821015035
Experience 7, Iter 79, disc loss: 0.030100074127838362, policy loss: 5.244188152353133
Experience 7, Iter 80, disc loss: 0.041943625577463825, policy loss: 4.84180357485435
Experience 7, Iter 81, disc loss: 0.041721954850188106, policy loss: 4.752470238692115
Experience 7, Iter 82, disc loss: 0.03435836329834761, policy loss: 4.706298459575921
Experience 7, Iter 83, disc loss: 0.035608310566471964, policy loss: 4.674724156641549
Experience 7, Iter 84, disc loss: 0.043849313909436, policy loss: 4.649430321854049
Experience 7, Iter 85, disc loss: 0.04038967415766227, policy loss: 4.993065357271446
Experience 7, Iter 86, disc loss: 0.039413797214788264, policy loss: 4.895014353576222
Experience 7, Iter 87, disc loss: 0.03737082957314397, policy loss: 4.934196691242665
Experience 7, Iter 88, disc loss: 0.03278963180140537, policy loss: 4.882318867340719
Experience 7, Iter 89, disc loss: 0.03428094766680955, policy loss: 4.849828588908744
Experience 7, Iter 90, disc loss: 0.038288119812101645, policy loss: 5.038335263419479
Experience 7, Iter 91, disc loss: 0.042316630187876667, policy loss: 4.85941780679291
Experience 7, Iter 92, disc loss: 0.03686675842390553, policy loss: 5.0974483566974405
Experience 7, Iter 93, disc loss: 0.03230480572491613, policy loss: 4.7907295347024
Experience 7, Iter 94, disc loss: 0.041483736791803894, policy loss: 4.588169515879053
Experience 7, Iter 95, disc loss: 0.0478526280832373, policy loss: 4.527797826714475
Experience 7, Iter 96, disc loss: 0.05404594982287588, policy loss: 4.581420496469199
Experience 7, Iter 97, disc loss: 0.0448498000048209, policy loss: 4.529204069057979
Experience 7, Iter 98, disc loss: 0.06475455599323966, policy loss: 4.067222592411247
Experience 7, Iter 99, disc loss: 0.05278477299990342, policy loss: 4.482825762089702
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0716],
        [0.5856],
        [0.0042]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3105e-02, 1.4883e-01, 2.1740e-01, 8.4154e-03, 1.1663e-03,
          2.0077e+00]],

        [[1.3105e-02, 1.4883e-01, 2.1740e-01, 8.4154e-03, 1.1663e-03,
          2.0077e+00]],

        [[1.3105e-02, 1.4883e-01, 2.1740e-01, 8.4154e-03, 1.1663e-03,
          2.0077e+00]],

        [[1.3105e-02, 1.4883e-01, 2.1740e-01, 8.4154e-03, 1.1663e-03,
          2.0077e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0163, 0.2864, 2.3422, 0.0166], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0163, 0.2864, 2.3422, 0.0166])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.521
Iter 2/2000 - Loss: 1.521
Iter 3/2000 - Loss: 1.425
Iter 4/2000 - Loss: 1.356
Iter 5/2000 - Loss: 1.425
Iter 6/2000 - Loss: 1.398
Iter 7/2000 - Loss: 1.326
Iter 8/2000 - Loss: 1.323
Iter 9/2000 - Loss: 1.344
Iter 10/2000 - Loss: 1.318
Iter 11/2000 - Loss: 1.262
Iter 12/2000 - Loss: 1.222
Iter 13/2000 - Loss: 1.199
Iter 14/2000 - Loss: 1.161
Iter 15/2000 - Loss: 1.091
Iter 16/2000 - Loss: 1.006
Iter 17/2000 - Loss: 0.920
Iter 18/2000 - Loss: 0.829
Iter 19/2000 - Loss: 0.719
Iter 20/2000 - Loss: 0.584
Iter 1981/2000 - Loss: -7.749
Iter 1982/2000 - Loss: -7.749
Iter 1983/2000 - Loss: -7.749
Iter 1984/2000 - Loss: -7.749
Iter 1985/2000 - Loss: -7.749
Iter 1986/2000 - Loss: -7.749
Iter 1987/2000 - Loss: -7.749
Iter 1988/2000 - Loss: -7.749
Iter 1989/2000 - Loss: -7.749
Iter 1990/2000 - Loss: -7.749
Iter 1991/2000 - Loss: -7.749
Iter 1992/2000 - Loss: -7.749
Iter 1993/2000 - Loss: -7.749
Iter 1994/2000 - Loss: -7.749
Iter 1995/2000 - Loss: -7.749
Iter 1996/2000 - Loss: -7.749
Iter 1997/2000 - Loss: -7.749
Iter 1998/2000 - Loss: -7.749
Iter 1999/2000 - Loss: -7.749
Iter 2000/2000 - Loss: -7.749
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[18.4121,  8.5469, 37.4228, 15.3254,  9.6279, 56.1335]],

        [[21.0746, 21.1951, 30.7244,  1.5572,  3.4134, 31.1990]],

        [[21.5955, 36.0445, 15.5162,  0.9940,  7.7715, 26.2252]],

        [[16.4176, 10.6724, 11.1774,  3.5869,  9.6143, 32.6325]]])
Signal Variance: tensor([ 0.1546,  2.5859, 19.1677,  0.2943])
Estimated target variance: tensor([0.0163, 0.2864, 2.3422, 0.0166])
N: 80
Signal to noise ratio: tensor([20.4050, 81.9413, 99.4493, 35.8197])
Bound on condition number: tensor([ 33310.1461, 537151.7712, 791214.8559, 102645.3510])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.059545064081133324, policy loss: 4.499215130744541
Experience 8, Iter 1, disc loss: 0.044898047628496765, policy loss: 4.206726156765671
Experience 8, Iter 2, disc loss: 0.05396471324495371, policy loss: 4.06725162230482
Experience 8, Iter 3, disc loss: 0.042611500800450085, policy loss: 4.77511642572323
Experience 8, Iter 4, disc loss: 0.03879409440792419, policy loss: 4.369205872895858
Experience 8, Iter 5, disc loss: 0.0341269420263781, policy loss: 4.787849090008297
Experience 8, Iter 6, disc loss: 0.038803646752781557, policy loss: 4.775741334863132
Experience 8, Iter 7, disc loss: 0.04401172451166781, policy loss: 4.381726364332798
Experience 8, Iter 8, disc loss: 0.03657623128539722, policy loss: 4.7659952223123545
Experience 8, Iter 9, disc loss: 0.043813864718852236, policy loss: 4.349294100323052
Experience 8, Iter 10, disc loss: 0.04111228024037965, policy loss: 4.743518292645176
Experience 8, Iter 11, disc loss: 0.03802609976976619, policy loss: 4.693175033050066
Experience 8, Iter 12, disc loss: 0.034820591313276285, policy loss: 4.591619912207376
Experience 8, Iter 13, disc loss: 0.02951904159686842, policy loss: 5.067895959034644
Experience 8, Iter 14, disc loss: 0.03978236987273341, policy loss: 4.8612861565983945
Experience 8, Iter 15, disc loss: 0.04345559961834619, policy loss: 4.4712889165238
Experience 8, Iter 16, disc loss: 0.03462637254894643, policy loss: 4.718064713949621
Experience 8, Iter 17, disc loss: 0.05515024500742198, policy loss: 4.475342782495109
Experience 8, Iter 18, disc loss: 0.033698747211485934, policy loss: 4.838942414318911
Experience 8, Iter 19, disc loss: 0.02991529461055285, policy loss: 4.889298715338764
Experience 8, Iter 20, disc loss: 0.03701604275081247, policy loss: 4.618829927715083
Experience 8, Iter 21, disc loss: 0.0329223905408237, policy loss: 4.753365369379499
Experience 8, Iter 22, disc loss: 0.04241331212554129, policy loss: 4.668617342072524
Experience 8, Iter 23, disc loss: 0.04520005360180905, policy loss: 4.508237222606008
Experience 8, Iter 24, disc loss: 0.03288544419144722, policy loss: 4.774897481777543
Experience 8, Iter 25, disc loss: 0.0328884486918207, policy loss: 4.773902620612647
Experience 8, Iter 26, disc loss: 0.030536263194551494, policy loss: 4.872931235549635
Experience 8, Iter 27, disc loss: 0.029848185444710396, policy loss: 4.836419274351563
Experience 8, Iter 28, disc loss: 0.030827954011087656, policy loss: 4.899652672407125
Experience 8, Iter 29, disc loss: 0.03285958403127875, policy loss: 4.898050154771743
Experience 8, Iter 30, disc loss: 0.029125926834192913, policy loss: 4.944592071205406
Experience 8, Iter 31, disc loss: 0.02463494311585091, policy loss: 5.341444628904953
Experience 8, Iter 32, disc loss: 0.027963225397012306, policy loss: 4.951856987488823
Experience 8, Iter 33, disc loss: 0.02817437157745903, policy loss: 4.904987541672178
Experience 8, Iter 34, disc loss: 0.024787384758425155, policy loss: 5.231709354913453
Experience 8, Iter 35, disc loss: 0.02678113560433567, policy loss: 4.791659937957093
Experience 8, Iter 36, disc loss: 0.02731841286220244, policy loss: 5.146073501014631
Experience 8, Iter 37, disc loss: 0.023177155564743018, policy loss: 5.228443601539966
Experience 8, Iter 38, disc loss: 0.02368685056219701, policy loss: 5.174706388528676
Experience 8, Iter 39, disc loss: 0.023511466868121826, policy loss: 5.174911363904058
Experience 8, Iter 40, disc loss: 0.024514681036124072, policy loss: 4.92587518864522
Experience 8, Iter 41, disc loss: 0.025428567298446315, policy loss: 5.253416309288827
Experience 8, Iter 42, disc loss: 0.02358374396426447, policy loss: 5.140998019268013
Experience 8, Iter 43, disc loss: 0.028286152336551355, policy loss: 5.178910447755773
Experience 8, Iter 44, disc loss: 0.023552246484402472, policy loss: 4.975771775468495
Experience 8, Iter 45, disc loss: 0.024529223114626485, policy loss: 5.1354634711289275
Experience 8, Iter 46, disc loss: 0.026465085307131743, policy loss: 4.98541980379904
Experience 8, Iter 47, disc loss: 0.023691721100699198, policy loss: 4.833910950036584
Experience 8, Iter 48, disc loss: 0.021454898145480777, policy loss: 5.406690154576941
Experience 8, Iter 49, disc loss: 0.026998932314672458, policy loss: 4.97372315089908
Experience 8, Iter 50, disc loss: 0.027189142554176215, policy loss: 5.346221115728423
Experience 8, Iter 51, disc loss: 0.02860860727204571, policy loss: 4.854728762587168
Experience 8, Iter 52, disc loss: 0.021487886494385405, policy loss: 5.355038991657224
Experience 8, Iter 53, disc loss: 0.027319244872286035, policy loss: 5.0567227113009565
Experience 8, Iter 54, disc loss: 0.022681425730634176, policy loss: 5.619217696200505
Experience 8, Iter 55, disc loss: 0.022327672544361344, policy loss: 4.981973969921345
Experience 8, Iter 56, disc loss: 0.0278243493450648, policy loss: 5.262685869959494
Experience 8, Iter 57, disc loss: 0.026057347628439892, policy loss: 5.690041335559359
Experience 8, Iter 58, disc loss: 0.025032481169195535, policy loss: 5.207221961045068
Experience 8, Iter 59, disc loss: 0.020450375880932542, policy loss: 5.371882275514112
Experience 8, Iter 60, disc loss: 0.021155411029237698, policy loss: 5.260782745125068
Experience 8, Iter 61, disc loss: 0.020486489320495745, policy loss: 5.530485206662451
Experience 8, Iter 62, disc loss: 0.019096644043529776, policy loss: 5.71646994870031
Experience 8, Iter 63, disc loss: 0.0213148022834166, policy loss: 5.219620822122181
Experience 8, Iter 64, disc loss: 0.01966640529378788, policy loss: 5.218447894563617
Experience 8, Iter 65, disc loss: 0.016952065124022978, policy loss: 5.670082534481134
Experience 8, Iter 66, disc loss: 0.020712307429944437, policy loss: 5.4037032035350885
Experience 8, Iter 67, disc loss: 0.02058798797979555, policy loss: 5.393669454483944
Experience 8, Iter 68, disc loss: 0.02996200727023749, policy loss: 4.919328625515196
Experience 8, Iter 69, disc loss: 0.02108353107046932, policy loss: 5.329981432835654
Experience 8, Iter 70, disc loss: 0.02508518993570029, policy loss: 5.247743840227882
Experience 8, Iter 71, disc loss: 0.014313404414148165, policy loss: 5.87099467098425
Experience 8, Iter 72, disc loss: 0.0164670928094083, policy loss: 5.755341372953908
Experience 8, Iter 73, disc loss: 0.023574036685215845, policy loss: 5.288912591342163
Experience 8, Iter 74, disc loss: 0.022822872191104265, policy loss: 5.237214394024033
Experience 8, Iter 75, disc loss: 0.013475840084146332, policy loss: 6.146107684661371
Experience 8, Iter 76, disc loss: 0.016909162521941058, policy loss: 5.454746892855598
Experience 8, Iter 77, disc loss: 0.017152788053707554, policy loss: 5.320801920717269
Experience 8, Iter 78, disc loss: 0.015611435563492062, policy loss: 5.5100665419595956
Experience 8, Iter 79, disc loss: 0.015793290542434738, policy loss: 5.667303956244609
Experience 8, Iter 80, disc loss: 0.014958422230413954, policy loss: 5.422001040107744
Experience 8, Iter 81, disc loss: 0.018791624313119617, policy loss: 5.339787466717741
Experience 8, Iter 82, disc loss: 0.018053467400769952, policy loss: 5.6598804090788155
Experience 8, Iter 83, disc loss: 0.01852235602318777, policy loss: 5.39236721408175
Experience 8, Iter 84, disc loss: 0.018310056584439433, policy loss: 5.663961630476266
Experience 8, Iter 85, disc loss: 0.01877953355978315, policy loss: 5.579146542504596
Experience 8, Iter 86, disc loss: 0.01630210172589941, policy loss: 5.62058987286397
Experience 8, Iter 87, disc loss: 0.014523530548848524, policy loss: 5.927299217772814
Experience 8, Iter 88, disc loss: 0.01683989971922842, policy loss: 5.734666431764557
Experience 8, Iter 89, disc loss: 0.01656170817206184, policy loss: 5.420014638051795
Experience 8, Iter 90, disc loss: 0.016300228044559377, policy loss: 5.63021376939405
Experience 8, Iter 91, disc loss: 0.014790803867225725, policy loss: 5.548380279400266
Experience 8, Iter 92, disc loss: 0.01684934973721395, policy loss: 5.713603612545587
Experience 8, Iter 93, disc loss: 0.018173747955267515, policy loss: 5.718652344620201
Experience 8, Iter 94, disc loss: 0.01314369676857387, policy loss: 5.898788041980873
Experience 8, Iter 95, disc loss: 0.0117792469368232, policy loss: 5.814206141662041
Experience 8, Iter 96, disc loss: 0.011833530636381775, policy loss: 6.096624052944604
Experience 8, Iter 97, disc loss: 0.012630288703699123, policy loss: 5.777943079513514
Experience 8, Iter 98, disc loss: 0.015231881353903947, policy loss: 5.491113246992121
Experience 8, Iter 99, disc loss: 0.013464556211745063, policy loss: 5.813022348946473
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.0960],
        [0.8533],
        [0.0047]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1681e-02, 1.3935e-01, 2.6822e-01, 7.8794e-03, 1.0498e-03,
          2.3108e+00]],

        [[1.1681e-02, 1.3935e-01, 2.6822e-01, 7.8794e-03, 1.0498e-03,
          2.3108e+00]],

        [[1.1681e-02, 1.3935e-01, 2.6822e-01, 7.8794e-03, 1.0498e-03,
          2.3108e+00]],

        [[1.1681e-02, 1.3935e-01, 2.6822e-01, 7.8794e-03, 1.0498e-03,
          2.3108e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0148, 0.3840, 3.4133, 0.0186], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0148, 0.3840, 3.4133, 0.0186])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.834
Iter 2/2000 - Loss: 1.904
Iter 3/2000 - Loss: 1.756
Iter 4/2000 - Loss: 1.716
Iter 5/2000 - Loss: 1.790
Iter 6/2000 - Loss: 1.745
Iter 7/2000 - Loss: 1.677
Iter 8/2000 - Loss: 1.689
Iter 9/2000 - Loss: 1.713
Iter 10/2000 - Loss: 1.678
Iter 11/2000 - Loss: 1.625
Iter 12/2000 - Loss: 1.599
Iter 13/2000 - Loss: 1.588
Iter 14/2000 - Loss: 1.554
Iter 15/2000 - Loss: 1.487
Iter 16/2000 - Loss: 1.407
Iter 17/2000 - Loss: 1.329
Iter 18/2000 - Loss: 1.241
Iter 19/2000 - Loss: 1.128
Iter 20/2000 - Loss: 0.986
Iter 1981/2000 - Loss: -7.798
Iter 1982/2000 - Loss: -7.799
Iter 1983/2000 - Loss: -7.799
Iter 1984/2000 - Loss: -7.799
Iter 1985/2000 - Loss: -7.799
Iter 1986/2000 - Loss: -7.799
Iter 1987/2000 - Loss: -7.799
Iter 1988/2000 - Loss: -7.799
Iter 1989/2000 - Loss: -7.799
Iter 1990/2000 - Loss: -7.799
Iter 1991/2000 - Loss: -7.799
Iter 1992/2000 - Loss: -7.799
Iter 1993/2000 - Loss: -7.799
Iter 1994/2000 - Loss: -7.799
Iter 1995/2000 - Loss: -7.799
Iter 1996/2000 - Loss: -7.799
Iter 1997/2000 - Loss: -7.799
Iter 1998/2000 - Loss: -7.799
Iter 1999/2000 - Loss: -7.799
Iter 2000/2000 - Loss: -7.799
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[17.4045,  7.9704, 35.3600, 17.7215,  9.4908, 58.3978]],

        [[19.0356, 14.2876, 34.3161,  2.1192,  5.2075, 34.0958]],

        [[21.1211, 30.0828, 16.4469,  1.0965,  6.9007, 24.8259]],

        [[17.0318,  9.9281, 13.0297,  3.7121,  8.9583, 32.3792]]])
Signal Variance: tensor([ 0.1265,  3.5621, 24.2795,  0.3314])
Estimated target variance: tensor([0.0148, 0.3840, 3.4133, 0.0186])
N: 90
Signal to noise ratio: tensor([ 17.9508,  95.3064, 109.7714,  38.3727])
Bound on condition number: tensor([  29001.8284,  817498.4148, 1084480.1934,  132522.8314])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.01793257089574853, policy loss: 5.329359415933255
Experience 9, Iter 1, disc loss: 0.015716602484625464, policy loss: 5.884637570681416
Experience 9, Iter 2, disc loss: 0.0171584171770338, policy loss: 5.769992763206314
Experience 9, Iter 3, disc loss: 0.013939134700689453, policy loss: 6.00756262869571
Experience 9, Iter 4, disc loss: 0.01616893381120189, policy loss: 5.575862137868597
Experience 9, Iter 5, disc loss: 0.017153305855856164, policy loss: 5.747396241699316
Experience 9, Iter 6, disc loss: 0.014944722019267023, policy loss: 5.688215702426087
Experience 9, Iter 7, disc loss: 0.017884851279940935, policy loss: 5.295585524321123
Experience 9, Iter 8, disc loss: 0.015152207699119716, policy loss: 5.554563991396387
Experience 9, Iter 9, disc loss: 0.015807480130234204, policy loss: 5.8258292789803825
Experience 9, Iter 10, disc loss: 0.01490947271070015, policy loss: 5.584952961997603
Experience 9, Iter 11, disc loss: 0.01396125870672876, policy loss: 5.73913620873605
Experience 9, Iter 12, disc loss: 0.01488711907651338, policy loss: 5.713604158316359
Experience 9, Iter 13, disc loss: 0.02253156938947637, policy loss: 5.161518715836433
Experience 9, Iter 14, disc loss: 0.016203988810012213, policy loss: 5.871260836941954
Experience 9, Iter 15, disc loss: 0.013354899765248653, policy loss: 6.021397762214487
Experience 9, Iter 16, disc loss: 0.01707467884727074, policy loss: 5.9739847823966645
Experience 9, Iter 17, disc loss: 0.014640642566737956, policy loss: 5.7882085101504686
Experience 9, Iter 18, disc loss: 0.015357261655194798, policy loss: 5.652604139525051
Experience 9, Iter 19, disc loss: 0.017604960127033685, policy loss: 5.3175271348638615
Experience 9, Iter 20, disc loss: 0.019443565675985135, policy loss: 5.739098533167634
Experience 9, Iter 21, disc loss: 0.01611494152522728, policy loss: 5.535089090526659
Experience 9, Iter 22, disc loss: 0.013661813962382452, policy loss: 6.362057929804379
Experience 9, Iter 23, disc loss: 0.01387001408761603, policy loss: 5.779211047605416
Experience 9, Iter 24, disc loss: 0.01576225279830948, policy loss: 5.917029972377126
Experience 9, Iter 25, disc loss: 0.013557327142857352, policy loss: 6.045379859280868
Experience 9, Iter 26, disc loss: 0.012702928858667152, policy loss: 5.816212916508617
Experience 9, Iter 27, disc loss: 0.014285709876454571, policy loss: 6.197609184501124
Experience 9, Iter 28, disc loss: 0.015641538002047033, policy loss: 5.50284394652864
Experience 9, Iter 29, disc loss: 0.010001661631648062, policy loss: 6.493117112563505
Experience 9, Iter 30, disc loss: 0.013208068889874924, policy loss: 6.3793429994592525
Experience 9, Iter 31, disc loss: 0.011871071670590693, policy loss: 5.796812657234364
Experience 9, Iter 32, disc loss: 0.013016411403765539, policy loss: 6.1122939536963745
Experience 9, Iter 33, disc loss: 0.010115196911520038, policy loss: 6.0661245110742446
Experience 9, Iter 34, disc loss: 0.012391162921796288, policy loss: 6.11399328495607
Experience 9, Iter 35, disc loss: 0.012479319606706897, policy loss: 5.746430178095418
Experience 9, Iter 36, disc loss: 0.01252619407114497, policy loss: 5.8671516939029775
Experience 9, Iter 37, disc loss: 0.012319411454479474, policy loss: 6.179135215454911
Experience 9, Iter 38, disc loss: 0.014963392291041946, policy loss: 6.009320483429446
Experience 9, Iter 39, disc loss: 0.012554914621579594, policy loss: 6.161375764651536
Experience 9, Iter 40, disc loss: 0.011795120551935049, policy loss: 6.255072374148506
Experience 9, Iter 41, disc loss: 0.013348063836519614, policy loss: 6.4020484974828715
Experience 9, Iter 42, disc loss: 0.013521266129491501, policy loss: 6.039917114018072
Experience 9, Iter 43, disc loss: 0.010681360514676618, policy loss: 6.186235204764968
Experience 9, Iter 44, disc loss: 0.011860831397864133, policy loss: 6.556298331788626
Experience 9, Iter 45, disc loss: 0.012970841638612271, policy loss: 6.529228668015726
Experience 9, Iter 46, disc loss: 0.013486024935354122, policy loss: 6.176513564815294
Experience 9, Iter 47, disc loss: 0.01105924309619297, policy loss: 6.622869683085199
Experience 9, Iter 48, disc loss: 0.011474718881400375, policy loss: 6.449544246395226
Experience 9, Iter 49, disc loss: 0.010375801804180938, policy loss: 6.0288587748781985
Experience 9, Iter 50, disc loss: 0.010579115977872939, policy loss: 5.778031079585979
Experience 9, Iter 51, disc loss: 0.010278506481762918, policy loss: 5.9864565496289925
Experience 9, Iter 52, disc loss: 0.011960523271531485, policy loss: 5.774481517177697
Experience 9, Iter 53, disc loss: 0.010988322621361776, policy loss: 5.989417470891974
Experience 9, Iter 54, disc loss: 0.00912825551994848, policy loss: 6.542837211522153
Experience 9, Iter 55, disc loss: 0.009151618579426337, policy loss: 6.043922616713876
Experience 9, Iter 56, disc loss: 0.012001858837884363, policy loss: 5.984684803026196
Experience 9, Iter 57, disc loss: 0.009994715846659027, policy loss: 6.469920514496081
Experience 9, Iter 58, disc loss: 0.009711488962427703, policy loss: 6.226588551318061
Experience 9, Iter 59, disc loss: 0.012037653050986415, policy loss: 5.807666374201463
Experience 9, Iter 60, disc loss: 0.011011588850939402, policy loss: 6.484828344591298
Experience 9, Iter 61, disc loss: 0.009861548969825976, policy loss: 6.570158561716893
Experience 9, Iter 62, disc loss: 0.011058101086921792, policy loss: 6.441757173556926
Experience 9, Iter 63, disc loss: 0.008817316080606307, policy loss: 6.497457491922209
Experience 9, Iter 64, disc loss: 0.012080760262378764, policy loss: 6.159363158643709
Experience 9, Iter 65, disc loss: 0.009211745554336888, policy loss: 7.036798488031559
Experience 9, Iter 66, disc loss: 0.009650702838715443, policy loss: 6.35404274593702
Experience 9, Iter 67, disc loss: 0.010995158202720531, policy loss: 6.203875948024427
Experience 9, Iter 68, disc loss: 0.01039346656274634, policy loss: 6.177544996061164
Experience 9, Iter 69, disc loss: 0.010328233041601147, policy loss: 6.2735558143672066
Experience 9, Iter 70, disc loss: 0.012670433654258345, policy loss: 5.856996223797769
Experience 9, Iter 71, disc loss: 0.01111445926778934, policy loss: 6.477618436116248
Experience 9, Iter 72, disc loss: 0.01160069543261403, policy loss: 6.0857241570907785
Experience 9, Iter 73, disc loss: 0.007655067709245434, policy loss: 6.989425563364589
Experience 9, Iter 74, disc loss: 0.008824623173135458, policy loss: 6.4211996228448935
Experience 9, Iter 75, disc loss: 0.009565428058993516, policy loss: 6.217220451059245
Experience 9, Iter 76, disc loss: 0.010988726398766212, policy loss: 6.220647017910784
Experience 9, Iter 77, disc loss: 0.009797757928997865, policy loss: 6.002483078357736
Experience 9, Iter 78, disc loss: 0.008555232510512094, policy loss: 6.383040564661726
Experience 9, Iter 79, disc loss: 0.009510867201512166, policy loss: 6.1424759558048825
Experience 9, Iter 80, disc loss: 0.00940406992953053, policy loss: 6.176209571574171
Experience 9, Iter 81, disc loss: 0.008775587073853477, policy loss: 6.3496289879503625
Experience 9, Iter 82, disc loss: 0.011661473940227056, policy loss: 5.9263823159405415
Experience 9, Iter 83, disc loss: 0.009096815118880604, policy loss: 6.5662128575789005
Experience 9, Iter 84, disc loss: 0.011643527382441121, policy loss: 6.389787290785586
Experience 9, Iter 85, disc loss: 0.011794357258868678, policy loss: 5.603739223442833
Experience 9, Iter 86, disc loss: 0.008784956076805829, policy loss: 7.031731323628704
Experience 9, Iter 87, disc loss: 0.009642718584881877, policy loss: 6.441889295282476
Experience 9, Iter 88, disc loss: 0.007551242798268686, policy loss: 7.243832714542237
Experience 9, Iter 89, disc loss: 0.008794414756092274, policy loss: 6.001332752207704
Experience 9, Iter 90, disc loss: 0.006720868056971805, policy loss: 6.66333452859079
Experience 9, Iter 91, disc loss: 0.008121517773985978, policy loss: 6.4559602104591685
Experience 9, Iter 92, disc loss: 0.006335628433656255, policy loss: 6.994223116090299
Experience 9, Iter 93, disc loss: 0.006970532625178671, policy loss: 6.432294700718304
Experience 9, Iter 94, disc loss: 0.006672254042145125, policy loss: 6.486403483211135
Experience 9, Iter 95, disc loss: 0.007728091704377504, policy loss: 6.429867734380435
Experience 9, Iter 96, disc loss: 0.006708801967624958, policy loss: 6.5217777616984725
Experience 9, Iter 97, disc loss: 0.00659528300001903, policy loss: 6.330548464828004
Experience 9, Iter 98, disc loss: 0.006649186549673705, policy loss: 6.751731984495701
Experience 9, Iter 99, disc loss: 0.005957453059403623, policy loss: 7.804511025178618
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.1186],
        [1.0992],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0538e-02, 1.3122e-01, 3.0180e-01, 7.3498e-03, 9.5825e-04,
          2.6137e+00]],

        [[1.0538e-02, 1.3122e-01, 3.0180e-01, 7.3498e-03, 9.5825e-04,
          2.6137e+00]],

        [[1.0538e-02, 1.3122e-01, 3.0180e-01, 7.3498e-03, 9.5825e-04,
          2.6137e+00]],

        [[1.0538e-02, 1.3122e-01, 3.0180e-01, 7.3498e-03, 9.5825e-04,
          2.6137e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0136, 0.4745, 4.3966, 0.0192], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0136, 0.4745, 4.3966, 0.0192])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.029
Iter 2/2000 - Loss: 2.129
Iter 3/2000 - Loss: 1.956
Iter 4/2000 - Loss: 1.921
Iter 5/2000 - Loss: 1.995
Iter 6/2000 - Loss: 1.939
Iter 7/2000 - Loss: 1.865
Iter 8/2000 - Loss: 1.872
Iter 9/2000 - Loss: 1.891
Iter 10/2000 - Loss: 1.846
Iter 11/2000 - Loss: 1.780
Iter 12/2000 - Loss: 1.746
Iter 13/2000 - Loss: 1.727
Iter 14/2000 - Loss: 1.679
Iter 15/2000 - Loss: 1.594
Iter 16/2000 - Loss: 1.495
Iter 17/2000 - Loss: 1.396
Iter 18/2000 - Loss: 1.286
Iter 19/2000 - Loss: 1.148
Iter 20/2000 - Loss: 0.980
Iter 1981/2000 - Loss: -8.044
Iter 1982/2000 - Loss: -8.044
Iter 1983/2000 - Loss: -8.044
Iter 1984/2000 - Loss: -8.044
Iter 1985/2000 - Loss: -8.044
Iter 1986/2000 - Loss: -8.044
Iter 1987/2000 - Loss: -8.044
Iter 1988/2000 - Loss: -8.044
Iter 1989/2000 - Loss: -8.045
Iter 1990/2000 - Loss: -8.045
Iter 1991/2000 - Loss: -8.045
Iter 1992/2000 - Loss: -8.045
Iter 1993/2000 - Loss: -8.045
Iter 1994/2000 - Loss: -8.045
Iter 1995/2000 - Loss: -8.045
Iter 1996/2000 - Loss: -8.045
Iter 1997/2000 - Loss: -8.045
Iter 1998/2000 - Loss: -8.045
Iter 1999/2000 - Loss: -8.045
Iter 2000/2000 - Loss: -8.045
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[16.8642,  8.1448, 33.7032, 16.3101,  8.8556, 58.3620]],

        [[19.5493, 17.0186, 33.1140,  1.8330,  4.5921, 34.7230]],

        [[21.4524, 35.4100, 16.9487,  1.0251,  6.3360, 22.0972]],

        [[15.8924,  9.1999, 11.8179,  3.4891,  8.1082, 33.0325]]])
Signal Variance: tensor([ 0.1317,  3.5697, 21.8683,  0.2799])
Estimated target variance: tensor([0.0136, 0.4745, 4.3966, 0.0192])
N: 100
Signal to noise ratio: tensor([ 19.0447, 100.4528, 108.2820,  36.4286])
Bound on condition number: tensor([  36271.1440, 1009077.6228, 1172499.3465,  132705.5756])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.006950938463619281, policy loss: 6.632964144895284
Experience 10, Iter 1, disc loss: 0.007158371792939641, policy loss: 6.926840771030541
Experience 10, Iter 2, disc loss: 0.008730455262042244, policy loss: 6.771028738448684
Experience 10, Iter 3, disc loss: 0.007338290156380562, policy loss: 6.610459324266344
Experience 10, Iter 4, disc loss: 0.0065411440068815115, policy loss: 7.019027864153611
Experience 10, Iter 5, disc loss: 0.008398243631759197, policy loss: 6.016617715282743
Experience 10, Iter 6, disc loss: 0.008020751447492312, policy loss: 6.21512005477771
Experience 10, Iter 7, disc loss: 0.007462109026500605, policy loss: 6.840371797878134
Experience 10, Iter 8, disc loss: 0.00534758996315627, policy loss: 6.800326262211432
Experience 10, Iter 9, disc loss: 0.005062828257823879, policy loss: 6.919027671943663
Experience 10, Iter 10, disc loss: 0.00535466703123519, policy loss: 6.848036001111215
Experience 10, Iter 11, disc loss: 0.005597643266017851, policy loss: 6.763487255153473
Experience 10, Iter 12, disc loss: 0.007051200812452325, policy loss: 6.354778367406077
Experience 10, Iter 13, disc loss: 0.006415705281512682, policy loss: 6.54517316309228
Experience 10, Iter 14, disc loss: 0.0074552901374755134, policy loss: 6.3069222346401315
Experience 10, Iter 15, disc loss: 0.006868149571372488, policy loss: 6.390825568848209
Experience 10, Iter 16, disc loss: 0.007332960804991825, policy loss: 6.238986668264623
Experience 10, Iter 17, disc loss: 0.0071093674691221865, policy loss: 6.389003131043699
Experience 10, Iter 18, disc loss: 0.005107971414420679, policy loss: 7.115589331100314
Experience 10, Iter 19, disc loss: 0.00563902724696212, policy loss: 7.03093534086614
Experience 10, Iter 20, disc loss: 0.005272069925143701, policy loss: 7.22523215938174
Experience 10, Iter 21, disc loss: 0.007020238325403893, policy loss: 6.815584705411922
Experience 10, Iter 22, disc loss: 0.008322820560534048, policy loss: 6.417129131272148
Experience 10, Iter 23, disc loss: 0.006558720148148366, policy loss: 6.583503733923252
Experience 10, Iter 24, disc loss: 0.00600824916157605, policy loss: 6.522214420559666
Experience 10, Iter 25, disc loss: 0.00655805401159501, policy loss: 7.006869279353607
Experience 10, Iter 26, disc loss: 0.007002340228690222, policy loss: 6.457956020105963
Experience 10, Iter 27, disc loss: 0.007063961214126125, policy loss: 6.351051571472344
Experience 10, Iter 28, disc loss: 0.0068094787234123265, policy loss: 6.083134932302826
Experience 10, Iter 29, disc loss: 0.005689102249156433, policy loss: 8.3601452684882
Experience 10, Iter 30, disc loss: 0.006179862209884986, policy loss: 6.1382594200141884
Experience 10, Iter 31, disc loss: 0.00618832588003319, policy loss: 6.3528693204352304
Experience 10, Iter 32, disc loss: 0.0067700261472999415, policy loss: 6.224685892021096
Experience 10, Iter 33, disc loss: 0.006680694916643581, policy loss: 6.123725658717552
Experience 10, Iter 34, disc loss: 0.0060812004102580395, policy loss: 6.662557536063422
Experience 10, Iter 35, disc loss: 0.00603734164933996, policy loss: 6.986486903537612
Experience 10, Iter 36, disc loss: 0.005792949950798886, policy loss: 6.628309217263516
Experience 10, Iter 37, disc loss: 0.006666672635660436, policy loss: 6.577029155720167
Experience 10, Iter 38, disc loss: 0.006193382179164723, policy loss: 6.533734658228134
Experience 10, Iter 39, disc loss: 0.007699035160275996, policy loss: 6.224117463259777
Experience 10, Iter 40, disc loss: 0.005250969686135629, policy loss: 6.718166880268116
Experience 10, Iter 41, disc loss: 0.006833997698673466, policy loss: 6.183606620440786
Experience 10, Iter 42, disc loss: 0.00678553904278561, policy loss: 6.513167631652207
Experience 10, Iter 43, disc loss: 0.008361132021547401, policy loss: 6.0035456370238744
Experience 10, Iter 44, disc loss: 0.008067917716443874, policy loss: 6.011318838007119
Experience 10, Iter 45, disc loss: 0.00592778240991195, policy loss: 6.863933080141442
Experience 10, Iter 46, disc loss: 0.005526334870487273, policy loss: 7.057047473579065
Experience 10, Iter 47, disc loss: 0.007754870249531038, policy loss: 6.303847811989278
Experience 10, Iter 48, disc loss: 0.007133001446477551, policy loss: 6.3198843916752985
Experience 10, Iter 49, disc loss: 0.005839781576075469, policy loss: 6.57401394220821
Experience 10, Iter 50, disc loss: 0.007712626164105466, policy loss: 6.676749773981536
Experience 10, Iter 51, disc loss: 0.00670012657628857, policy loss: 6.21664193938593
Experience 10, Iter 52, disc loss: 0.006134385372555115, policy loss: 7.162326010287578
Experience 10, Iter 53, disc loss: 0.0076172724493815056, policy loss: 6.201546560991247
Experience 10, Iter 54, disc loss: 0.006668798697035833, policy loss: 6.450952992286723
Experience 10, Iter 55, disc loss: 0.006137054505729826, policy loss: 6.543300206466805
Experience 10, Iter 56, disc loss: 0.007039322831135415, policy loss: 6.359037264081868
Experience 10, Iter 57, disc loss: 0.007114964527774452, policy loss: 6.138227646437803
Experience 10, Iter 58, disc loss: 0.00701273003653788, policy loss: 5.749505359089964
Experience 10, Iter 59, disc loss: 0.006475498864696044, policy loss: 6.535036693818375
Experience 10, Iter 60, disc loss: 0.005871750292396098, policy loss: 6.465314161442054
Experience 10, Iter 61, disc loss: 0.005564339857067979, policy loss: 7.010767167167868
Experience 10, Iter 62, disc loss: 0.007456671484034842, policy loss: 6.8568892424698085
Experience 10, Iter 63, disc loss: 0.005930095903326104, policy loss: 6.632511595976423
Experience 10, Iter 64, disc loss: 0.005814624347442662, policy loss: 6.416313097971103
Experience 10, Iter 65, disc loss: 0.007042165580947888, policy loss: 6.047534579169456
Experience 10, Iter 66, disc loss: 0.006349400414376207, policy loss: 6.436514865139057
Experience 10, Iter 67, disc loss: 0.006620627070381451, policy loss: 6.633221400808441
Experience 10, Iter 68, disc loss: 0.005057039200530407, policy loss: 6.9285739914233115
Experience 10, Iter 69, disc loss: 0.006210206327192771, policy loss: 6.395745499852928
Experience 10, Iter 70, disc loss: 0.005626093306691952, policy loss: 6.427645205394533
Experience 10, Iter 71, disc loss: 0.006242488811155255, policy loss: 6.2808551836367865
Experience 10, Iter 72, disc loss: 0.005951143990425244, policy loss: 6.420148883598014
Experience 10, Iter 73, disc loss: 0.006733441995512589, policy loss: 6.529339251749006
Experience 10, Iter 74, disc loss: 0.0066926973073831025, policy loss: 6.486210043595561
Experience 10, Iter 75, disc loss: 0.006109969239070481, policy loss: 6.57223278670268
Experience 10, Iter 76, disc loss: 0.006289390781960669, policy loss: 6.343927439681023
Experience 10, Iter 77, disc loss: 0.006809963058539103, policy loss: 6.600896949108145
Experience 10, Iter 78, disc loss: 0.007454977047483842, policy loss: 6.358063440693844
Experience 10, Iter 79, disc loss: 0.0067625242407024135, policy loss: 6.025483308953458
Experience 10, Iter 80, disc loss: 0.007012670525644067, policy loss: 6.3197422390953175
Experience 10, Iter 81, disc loss: 0.007046785716085929, policy loss: 6.418838386970559
Experience 10, Iter 82, disc loss: 0.005480900306585651, policy loss: 6.265483494849913
Experience 10, Iter 83, disc loss: 0.007176570584774451, policy loss: 6.145511365264592
Experience 10, Iter 84, disc loss: 0.0069438170330775855, policy loss: 6.491680209091189
Experience 10, Iter 85, disc loss: 0.007407441020221362, policy loss: 6.068421333257051
Experience 10, Iter 86, disc loss: 0.007832702000259272, policy loss: 5.879235799939103
Experience 10, Iter 87, disc loss: 0.006338368230246593, policy loss: 6.526828844522095
Experience 10, Iter 88, disc loss: 0.0077751483202351, policy loss: 6.1035066178347
Experience 10, Iter 89, disc loss: 0.0066269025690328665, policy loss: 6.305833754338317
Experience 10, Iter 90, disc loss: 0.006021156242264492, policy loss: 6.689317663393521
Experience 10, Iter 91, disc loss: 0.008296957678328736, policy loss: 5.84185693609256
Experience 10, Iter 92, disc loss: 0.006853936611177263, policy loss: 6.097560609171033
Experience 10, Iter 93, disc loss: 0.007843695946637974, policy loss: 5.844140160408334
Experience 10, Iter 94, disc loss: 0.008285587901901278, policy loss: 5.736818787806049
Experience 10, Iter 95, disc loss: 0.008791651821760527, policy loss: 5.795985442744339
Experience 10, Iter 96, disc loss: 0.009258807671336959, policy loss: 6.21646560933807
Experience 10, Iter 97, disc loss: 0.008983597329230866, policy loss: 5.842146540243947
Experience 10, Iter 98, disc loss: 0.009567845605580827, policy loss: 5.905568674079517
Experience 10, Iter 99, disc loss: 0.008396511012667118, policy loss: 6.178472165536382
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1265],
        [1.1852],
        [0.0047]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.5788e-03, 1.2337e-01, 3.0760e-01, 6.9722e-03, 8.8153e-04,
          2.6926e+00]],

        [[9.5788e-03, 1.2337e-01, 3.0760e-01, 6.9722e-03, 8.8153e-04,
          2.6926e+00]],

        [[9.5788e-03, 1.2337e-01, 3.0760e-01, 6.9722e-03, 8.8153e-04,
          2.6926e+00]],

        [[9.5788e-03, 1.2337e-01, 3.0760e-01, 6.9722e-03, 8.8153e-04,
          2.6926e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0126, 0.5061, 4.7407, 0.0188], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0126, 0.5061, 4.7407, 0.0188])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.041
Iter 2/2000 - Loss: 2.195
Iter 3/2000 - Loss: 1.979
Iter 4/2000 - Loss: 1.963
Iter 5/2000 - Loss: 2.046
Iter 6/2000 - Loss: 1.980
Iter 7/2000 - Loss: 1.900
Iter 8/2000 - Loss: 1.910
Iter 9/2000 - Loss: 1.935
Iter 10/2000 - Loss: 1.889
Iter 11/2000 - Loss: 1.815
Iter 12/2000 - Loss: 1.775
Iter 13/2000 - Loss: 1.758
Iter 14/2000 - Loss: 1.713
Iter 15/2000 - Loss: 1.625
Iter 16/2000 - Loss: 1.521
Iter 17/2000 - Loss: 1.419
Iter 18/2000 - Loss: 1.312
Iter 19/2000 - Loss: 1.179
Iter 20/2000 - Loss: 1.013
Iter 1981/2000 - Loss: -8.194
Iter 1982/2000 - Loss: -8.194
Iter 1983/2000 - Loss: -8.194
Iter 1984/2000 - Loss: -8.194
Iter 1985/2000 - Loss: -8.194
Iter 1986/2000 - Loss: -8.194
Iter 1987/2000 - Loss: -8.194
Iter 1988/2000 - Loss: -8.194
Iter 1989/2000 - Loss: -8.194
Iter 1990/2000 - Loss: -8.194
Iter 1991/2000 - Loss: -8.194
Iter 1992/2000 - Loss: -8.194
Iter 1993/2000 - Loss: -8.194
Iter 1994/2000 - Loss: -8.194
Iter 1995/2000 - Loss: -8.194
Iter 1996/2000 - Loss: -8.194
Iter 1997/2000 - Loss: -8.194
Iter 1998/2000 - Loss: -8.194
Iter 1999/2000 - Loss: -8.194
Iter 2000/2000 - Loss: -8.194
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[16.5287,  7.8495, 36.1748, 15.9218,  8.8648, 53.2590]],

        [[19.4803, 15.5597, 34.8582,  2.0267,  5.3675, 37.2393]],

        [[20.7769, 36.2748, 17.0853,  0.9586,  5.6118, 19.3979]],

        [[14.6713,  8.8033, 11.2517,  3.2617,  7.3068, 34.1209]]])
Signal Variance: tensor([ 0.1180,  4.1562, 19.2223,  0.2543])
Estimated target variance: tensor([0.0126, 0.5061, 4.7407, 0.0188])
N: 110
Signal to noise ratio: tensor([ 17.8733, 108.1574, 106.1028,  35.6407])
Bound on condition number: tensor([  35140.8935, 1286784.0340, 1238360.5518,  139729.2134])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.011391224811615433, policy loss: 5.933785134987036
Experience 11, Iter 1, disc loss: 0.010984085653036785, policy loss: 5.682131271338827
Experience 11, Iter 2, disc loss: 0.0089264838034789, policy loss: 6.315686452887304
Experience 11, Iter 3, disc loss: 0.011232496401908118, policy loss: 5.373568962477762
Experience 11, Iter 4, disc loss: 0.009858687962402432, policy loss: 5.5599413621398615
Experience 11, Iter 5, disc loss: 0.011628616720404997, policy loss: 5.757533297806508
Experience 11, Iter 6, disc loss: 0.010962088326212763, policy loss: 5.595457027265031
Experience 11, Iter 7, disc loss: 0.011980886781077688, policy loss: 5.477342374772503
Experience 11, Iter 8, disc loss: 0.012717756166113524, policy loss: 5.672693028472308
Experience 11, Iter 9, disc loss: 0.010246479787236057, policy loss: 5.960214633132782
Experience 11, Iter 10, disc loss: 0.010094178039380106, policy loss: 6.171380835274787
Experience 11, Iter 11, disc loss: 0.011524619074586573, policy loss: 5.9788343137441
Experience 11, Iter 12, disc loss: 0.011980258191720607, policy loss: 5.575995627660388
Experience 11, Iter 13, disc loss: 0.011996223454813337, policy loss: 5.3806409067321095
Experience 11, Iter 14, disc loss: 0.01034634669389679, policy loss: 5.916782990319508
Experience 11, Iter 15, disc loss: 0.010107345094198788, policy loss: 5.857081331381191
Experience 11, Iter 16, disc loss: 0.013046648768844767, policy loss: 5.833391494303987
Experience 11, Iter 17, disc loss: 0.01518143164694621, policy loss: 5.485256098636551
Experience 11, Iter 18, disc loss: 0.0116825179178973, policy loss: 5.809668157357999
Experience 11, Iter 19, disc loss: 0.014205618672850773, policy loss: 5.557362091857942
Experience 11, Iter 20, disc loss: 0.012929419900725074, policy loss: 5.381146703364822
Experience 11, Iter 21, disc loss: 0.012281316578011531, policy loss: 5.567467257433043
Experience 11, Iter 22, disc loss: 0.01486885941905338, policy loss: 5.303678370017775
Experience 11, Iter 23, disc loss: 0.012215106133707875, policy loss: 5.757583439325407
Experience 11, Iter 24, disc loss: 0.013075971412995279, policy loss: 5.504993902388542
Experience 11, Iter 25, disc loss: 0.013819613087612205, policy loss: 5.774334869020493
Experience 11, Iter 26, disc loss: 0.011925210128959049, policy loss: 5.44553380591996
Experience 11, Iter 27, disc loss: 0.011828230604602568, policy loss: 5.692184509470938
Experience 11, Iter 28, disc loss: 0.011871267352257625, policy loss: 5.668086252491866
Experience 11, Iter 29, disc loss: 0.014139792132946506, policy loss: 5.1937654470476495
Experience 11, Iter 30, disc loss: 0.012443779387157537, policy loss: 5.526224091961996
Experience 11, Iter 31, disc loss: 0.012494958289488167, policy loss: 5.536410249974503
Experience 11, Iter 32, disc loss: 0.010334584409588338, policy loss: 5.942257848019793
Experience 11, Iter 33, disc loss: 0.010666013375773541, policy loss: 5.798158198947386
Experience 11, Iter 34, disc loss: 0.010573872170267284, policy loss: 5.747070282713818
Experience 11, Iter 35, disc loss: 0.010523191253643151, policy loss: 6.145016161729042
Experience 11, Iter 36, disc loss: 0.010309768027119292, policy loss: 5.842563486326144
Experience 11, Iter 37, disc loss: 0.01295581458976399, policy loss: 5.630314449743446
Experience 11, Iter 38, disc loss: 0.011152349132810433, policy loss: 5.493290307655979
Experience 11, Iter 39, disc loss: 0.011133437112796232, policy loss: 5.553039997372039
Experience 11, Iter 40, disc loss: 0.012210126294420174, policy loss: 5.554101782080182
Experience 11, Iter 41, disc loss: 0.010154261703894954, policy loss: 5.998180740674448
Experience 11, Iter 42, disc loss: 0.013003191479882924, policy loss: 5.21178113795371
Experience 11, Iter 43, disc loss: 0.013631034871498623, policy loss: 5.301729321649205
Experience 11, Iter 44, disc loss: 0.013149803330740686, policy loss: 5.252515685427824
Experience 11, Iter 45, disc loss: 0.011288976351532976, policy loss: 6.0022686072097855
Experience 11, Iter 46, disc loss: 0.011327833140351545, policy loss: 5.599296161165258
Experience 11, Iter 47, disc loss: 0.01148409473276242, policy loss: 5.506139064968523
Experience 11, Iter 48, disc loss: 0.01529174735233431, policy loss: 5.302027091201845
Experience 11, Iter 49, disc loss: 0.010781543343567254, policy loss: 5.719422755057003
Experience 11, Iter 50, disc loss: 0.011966694741217282, policy loss: 5.542091844928449
Experience 11, Iter 51, disc loss: 0.01242706330877762, policy loss: 5.573450264530002
Experience 11, Iter 52, disc loss: 0.013334428972715393, policy loss: 5.455403625508133
Experience 11, Iter 53, disc loss: 0.012453847541807925, policy loss: 5.134228044180485
Experience 11, Iter 54, disc loss: 0.01274721362059959, policy loss: 5.499314592216165
Experience 11, Iter 55, disc loss: 0.013058357606172459, policy loss: 5.324516690044124
Experience 11, Iter 56, disc loss: 0.013449902940490263, policy loss: 5.5133871292996695
Experience 11, Iter 57, disc loss: 0.013097807794610129, policy loss: 5.47445367836171
Experience 11, Iter 58, disc loss: 0.013419484348671936, policy loss: 5.104679943342156
Experience 11, Iter 59, disc loss: 0.013902611255045431, policy loss: 5.121090855494799
Experience 11, Iter 60, disc loss: 0.011062115193322057, policy loss: 5.530377240911772
Experience 11, Iter 61, disc loss: 0.0153837074178746, policy loss: 5.004908967345971
Experience 11, Iter 62, disc loss: 0.012301516877048667, policy loss: 5.616359578858827
Experience 11, Iter 63, disc loss: 0.013719851679334923, policy loss: 5.451508751017386
Experience 11, Iter 64, disc loss: 0.012986229818035253, policy loss: 5.367430286675734
Experience 11, Iter 65, disc loss: 0.013188960886175164, policy loss: 5.112811610271401
Experience 11, Iter 66, disc loss: 0.013008795700435137, policy loss: 5.264087601137907
Experience 11, Iter 67, disc loss: 0.012145990136929031, policy loss: 5.370322668404112
Experience 11, Iter 68, disc loss: 0.014374608919953165, policy loss: 5.258145177183319
Experience 11, Iter 69, disc loss: 0.012186843703586941, policy loss: 5.494035607751802
Experience 11, Iter 70, disc loss: 0.012122205163051717, policy loss: 5.41985299566892
Experience 11, Iter 71, disc loss: 0.011880697916041245, policy loss: 5.339800240361492
Experience 11, Iter 72, disc loss: 0.011701696099692264, policy loss: 5.48363880998326
Experience 11, Iter 73, disc loss: 0.011858064512611916, policy loss: 5.2593291862609455
Experience 11, Iter 74, disc loss: 0.011874319494672011, policy loss: 5.385067665863764
Experience 11, Iter 75, disc loss: 0.01283752534700842, policy loss: 5.19788500841988
Experience 11, Iter 76, disc loss: 0.012091634340847795, policy loss: 5.499834226550577
Experience 11, Iter 77, disc loss: 0.010967890816616473, policy loss: 5.6422389409235265
Experience 11, Iter 78, disc loss: 0.012957660972381383, policy loss: 5.280840700950103
Experience 11, Iter 79, disc loss: 0.012755400315482744, policy loss: 5.216723268438914
Experience 11, Iter 80, disc loss: 0.011204559825872894, policy loss: 5.668052256101229
Experience 11, Iter 81, disc loss: 0.011819638234409843, policy loss: 5.40099335481362
Experience 11, Iter 82, disc loss: 0.01108409604456562, policy loss: 5.706020793922474
Experience 11, Iter 83, disc loss: 0.012487417486556506, policy loss: 5.414516426804299
Experience 11, Iter 84, disc loss: 0.012450330108449429, policy loss: 5.387591326082934
Experience 11, Iter 85, disc loss: 0.011908706188972912, policy loss: 5.708728043440052
Experience 11, Iter 86, disc loss: 0.010393350753804837, policy loss: 5.898503921977605
Experience 11, Iter 87, disc loss: 0.01043990092949659, policy loss: 5.844629148099161
Experience 11, Iter 88, disc loss: 0.01043028718747132, policy loss: 5.735182940860117
Experience 11, Iter 89, disc loss: 0.008895557904651732, policy loss: 5.9432643114760735
Experience 11, Iter 90, disc loss: 0.010378216184537749, policy loss: 6.009651568049234
Experience 11, Iter 91, disc loss: 0.00967151671671895, policy loss: 6.09537349941977
Experience 11, Iter 92, disc loss: 0.007704235201275932, policy loss: 7.136284049054835
Experience 11, Iter 93, disc loss: 0.007122065705884147, policy loss: 6.745104548840059
Experience 11, Iter 94, disc loss: 0.007265570224713195, policy loss: 7.340628279169703
Experience 11, Iter 95, disc loss: 0.006931462600304025, policy loss: 7.823686402466425
Experience 11, Iter 96, disc loss: 0.005988481020508819, policy loss: 7.604608134614769
Experience 11, Iter 97, disc loss: 0.006916807566257053, policy loss: 8.113198343668824
Experience 11, Iter 98, disc loss: 0.006862954402270303, policy loss: 8.029170202596378
Experience 11, Iter 99, disc loss: 0.006420676867919973, policy loss: 8.621208775399518
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.1235],
        [1.1559],
        [0.0045]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.3444e-03, 1.2018e-01, 2.9686e-01, 6.7736e-03, 8.1469e-04,
          2.6104e+00]],

        [[9.3444e-03, 1.2018e-01, 2.9686e-01, 6.7736e-03, 8.1469e-04,
          2.6104e+00]],

        [[9.3444e-03, 1.2018e-01, 2.9686e-01, 6.7736e-03, 8.1469e-04,
          2.6104e+00]],

        [[9.3444e-03, 1.2018e-01, 2.9686e-01, 6.7736e-03, 8.1469e-04,
          2.6104e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0123, 0.4941, 4.6237, 0.0179], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0123, 0.4941, 4.6237, 0.0179])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.966
Iter 2/2000 - Loss: 2.180
Iter 3/2000 - Loss: 1.916
Iter 4/2000 - Loss: 1.920
Iter 5/2000 - Loss: 2.014
Iter 6/2000 - Loss: 1.939
Iter 7/2000 - Loss: 1.851
Iter 8/2000 - Loss: 1.861
Iter 9/2000 - Loss: 1.896
Iter 10/2000 - Loss: 1.859
Iter 11/2000 - Loss: 1.784
Iter 12/2000 - Loss: 1.739
Iter 13/2000 - Loss: 1.729
Iter 14/2000 - Loss: 1.699
Iter 15/2000 - Loss: 1.621
Iter 16/2000 - Loss: 1.521
Iter 17/2000 - Loss: 1.425
Iter 18/2000 - Loss: 1.331
Iter 19/2000 - Loss: 1.214
Iter 20/2000 - Loss: 1.062
Iter 1981/2000 - Loss: -8.321
Iter 1982/2000 - Loss: -8.321
Iter 1983/2000 - Loss: -8.321
Iter 1984/2000 - Loss: -8.321
Iter 1985/2000 - Loss: -8.321
Iter 1986/2000 - Loss: -8.321
Iter 1987/2000 - Loss: -8.321
Iter 1988/2000 - Loss: -8.321
Iter 1989/2000 - Loss: -8.321
Iter 1990/2000 - Loss: -8.321
Iter 1991/2000 - Loss: -8.321
Iter 1992/2000 - Loss: -8.321
Iter 1993/2000 - Loss: -8.321
Iter 1994/2000 - Loss: -8.321
Iter 1995/2000 - Loss: -8.321
Iter 1996/2000 - Loss: -8.321
Iter 1997/2000 - Loss: -8.321
Iter 1998/2000 - Loss: -8.321
Iter 1999/2000 - Loss: -8.321
Iter 2000/2000 - Loss: -8.322
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[15.1226,  8.1357, 33.4785, 15.8289,  7.9481, 54.6015]],

        [[18.2792, 18.4238, 32.5114,  1.8349,  5.3051, 40.0882]],

        [[20.0483, 33.8332, 15.8192,  0.9575,  5.1390, 17.9324]],

        [[15.5226, 23.8658, 10.4073,  3.5401,  1.0615, 36.5957]]])
Signal Variance: tensor([ 0.1201,  4.6090, 17.7386,  0.2360])
Estimated target variance: tensor([0.0123, 0.4941, 4.6237, 0.0179])
N: 120
Signal to noise ratio: tensor([ 17.7462, 117.5433, 104.8805,  34.8590])
Bound on condition number: tensor([  37792.4113, 1657971.1598, 1319991.3621,  145819.3802])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.007788000108996277, policy loss: 7.901547379656779
Experience 12, Iter 1, disc loss: 0.008523216679458788, policy loss: 8.04134066280917
Experience 12, Iter 2, disc loss: 0.007987515949565887, policy loss: 7.687312231695158
Experience 12, Iter 3, disc loss: 0.011166640294307546, policy loss: 6.70541229804531
Experience 12, Iter 4, disc loss: 0.009407261653199588, policy loss: 6.255862417967987
Experience 12, Iter 5, disc loss: 0.008391399477448994, policy loss: 6.218595804733178
Experience 12, Iter 6, disc loss: 0.00606095808739175, policy loss: 6.233289819325254
Experience 12, Iter 7, disc loss: 0.005032276597319581, policy loss: 6.920550650095846
Experience 12, Iter 8, disc loss: 0.005185714852746073, policy loss: 6.436658100751586
Experience 12, Iter 9, disc loss: 0.004626459307967027, policy loss: 6.5391452922846725
Experience 12, Iter 10, disc loss: 0.004213589846136082, policy loss: 6.751759185417098
Experience 12, Iter 11, disc loss: 0.003748421535905755, policy loss: 6.881634440685014
Experience 12, Iter 12, disc loss: 0.0038760453988065033, policy loss: 6.712600013802393
Experience 12, Iter 13, disc loss: 0.0038314905340938706, policy loss: 6.744641632812236
Experience 12, Iter 14, disc loss: 0.004054106416403009, policy loss: 6.551261045029328
Experience 12, Iter 15, disc loss: 0.004586888292902358, policy loss: 6.435771606511353
Experience 12, Iter 16, disc loss: 0.005765245740331459, policy loss: 5.985308505594501
Experience 12, Iter 17, disc loss: 0.007037431922209576, policy loss: 5.754830288370616
Experience 12, Iter 18, disc loss: 0.010756234525988635, policy loss: 5.334715971984104
Experience 12, Iter 19, disc loss: 0.01572368530645733, policy loss: 4.816871585442721
Experience 12, Iter 20, disc loss: 0.02002430121443862, policy loss: 4.592517452027444
Experience 12, Iter 21, disc loss: 0.03658755433721493, policy loss: 3.810583573330314
Experience 12, Iter 22, disc loss: 0.058439063785116596, policy loss: 3.4783141977420544
Experience 12, Iter 23, disc loss: 0.0691614072849089, policy loss: 3.4503084388389507
Experience 12, Iter 24, disc loss: 0.05893422929058026, policy loss: 3.5061161711314104
Experience 12, Iter 25, disc loss: 0.05441140230489925, policy loss: 3.7364268251387527
Experience 12, Iter 26, disc loss: 0.03745203711403392, policy loss: 4.459211545721443
Experience 12, Iter 27, disc loss: 0.032460859704685915, policy loss: 4.427208265673111
Experience 12, Iter 28, disc loss: 0.03254217352244737, policy loss: 4.439047780200012
Experience 12, Iter 29, disc loss: 0.030962639170410356, policy loss: 5.065293153970941
Experience 12, Iter 30, disc loss: 0.04248627009426712, policy loss: 4.535999627863585
Experience 12, Iter 31, disc loss: 0.04066363996266349, policy loss: 4.6357138997965075
Experience 12, Iter 32, disc loss: 0.04312609404034992, policy loss: 4.40047426663325
Experience 12, Iter 33, disc loss: 0.044466866386826426, policy loss: 4.535857457270261
Experience 12, Iter 34, disc loss: 0.05345246630299069, policy loss: 4.5914755069182505
Experience 12, Iter 35, disc loss: 0.05850435887102127, policy loss: 4.937842245745589
Experience 12, Iter 36, disc loss: 0.0748793005870246, policy loss: 4.096283946328544
Experience 12, Iter 37, disc loss: 0.0825364502678928, policy loss: 4.078342930267653
Experience 12, Iter 38, disc loss: 0.09867716424352369, policy loss: 4.66046328982404
Experience 12, Iter 39, disc loss: 0.11864849733826441, policy loss: 4.242502058082324
Experience 12, Iter 40, disc loss: 0.12726520788630513, policy loss: 4.2301342915959514
Experience 12, Iter 41, disc loss: 0.14698467031038637, policy loss: 4.66838707651169
Experience 12, Iter 42, disc loss: 0.1858293770758721, policy loss: 3.776172042018036
Experience 12, Iter 43, disc loss: 0.18815634111037421, policy loss: 4.41242220027164
Experience 12, Iter 44, disc loss: 0.17110107310332068, policy loss: 5.092358703176142
Experience 12, Iter 45, disc loss: 0.23987565647124517, policy loss: 3.632031171147113
Experience 12, Iter 46, disc loss: 0.19670834457959913, policy loss: 4.271512267079787
Experience 12, Iter 47, disc loss: 0.22334539011932483, policy loss: 4.310925347562405
Experience 12, Iter 48, disc loss: 0.20148493340662338, policy loss: 4.007360814140455
Experience 12, Iter 49, disc loss: 0.17601657312442082, policy loss: 4.109051307546399
Experience 12, Iter 50, disc loss: 0.19280157868391087, policy loss: 4.292172673310546
Experience 12, Iter 51, disc loss: 0.1633491482553317, policy loss: 4.6383604043314275
Experience 12, Iter 52, disc loss: 0.15764935058323848, policy loss: 3.8531689790086245
Experience 12, Iter 53, disc loss: 0.1514874643820015, policy loss: 3.790946763340874
Experience 12, Iter 54, disc loss: 0.1424090514780704, policy loss: 5.150212413455108
Experience 12, Iter 55, disc loss: 0.1295511996089226, policy loss: 4.357200449168701
Experience 12, Iter 56, disc loss: 0.10496596895109495, policy loss: 4.621090412244356
Experience 12, Iter 57, disc loss: 0.10507785746943406, policy loss: 5.009170114577111
Experience 12, Iter 58, disc loss: 0.09759999611560743, policy loss: 4.85522882807043
Experience 12, Iter 59, disc loss: 0.09969539075937557, policy loss: 4.499389327866014
Experience 12, Iter 60, disc loss: 0.0902474195128081, policy loss: 4.496803249282226
Experience 12, Iter 61, disc loss: 0.0640340515832929, policy loss: 5.419450400156876
Experience 12, Iter 62, disc loss: 0.08132035445284375, policy loss: 4.33908991815245
Experience 12, Iter 63, disc loss: 0.06383344572108704, policy loss: 5.127823693704197
Experience 12, Iter 64, disc loss: 0.06677414745322618, policy loss: 5.728531966304617
Experience 12, Iter 65, disc loss: 0.05974726794927258, policy loss: 5.665048370612714
Experience 12, Iter 66, disc loss: 0.06084442481025523, policy loss: 4.893832845939688
Experience 12, Iter 67, disc loss: 0.04276950629427463, policy loss: 5.717678485345833
Experience 12, Iter 68, disc loss: 0.0480408923967002, policy loss: 5.4196803106119855
Experience 12, Iter 69, disc loss: 0.041724231827405, policy loss: 5.6058607086942285
Experience 12, Iter 70, disc loss: 0.047563519027937665, policy loss: 5.426862562750583
Experience 12, Iter 71, disc loss: 0.04743913035132715, policy loss: 5.3060840740718
Experience 12, Iter 72, disc loss: 0.04736177314587316, policy loss: 5.1353959708060755
Experience 12, Iter 73, disc loss: 0.03528054251580433, policy loss: 6.463411544548892
Experience 12, Iter 74, disc loss: 0.04763381328488356, policy loss: 4.888998355359829
Experience 12, Iter 75, disc loss: 0.03917188680811389, policy loss: 5.57295032959925
Experience 12, Iter 76, disc loss: 0.04539637435225244, policy loss: 5.874991364343526
Experience 12, Iter 77, disc loss: 0.0314824804462828, policy loss: 6.206033563358857
Experience 12, Iter 78, disc loss: 0.036988660430597724, policy loss: 6.267319538095355
Experience 12, Iter 79, disc loss: 0.03673726062659234, policy loss: 5.669163353768768
Experience 12, Iter 80, disc loss: 0.02932642323001533, policy loss: 5.932053327288747
Experience 12, Iter 81, disc loss: 0.03138302878535044, policy loss: 6.360226679687187
Experience 12, Iter 82, disc loss: 0.03315289126055029, policy loss: 6.466552675634965
Experience 12, Iter 83, disc loss: 0.02977301061028354, policy loss: 6.579408145810491
Experience 12, Iter 84, disc loss: 0.027855561944668063, policy loss: 6.621542737856459
Experience 12, Iter 85, disc loss: 0.036822637117754436, policy loss: 5.506457490206456
Experience 12, Iter 86, disc loss: 0.032607379675787494, policy loss: 6.182664700873264
Experience 12, Iter 87, disc loss: 0.03141488195128357, policy loss: 6.837759475469982
Experience 12, Iter 88, disc loss: 0.027525997333586384, policy loss: 6.659573295255269
Experience 12, Iter 89, disc loss: 0.027097738709802775, policy loss: 6.524977039277838
Experience 12, Iter 90, disc loss: 0.027417300226071334, policy loss: 6.365858916449014
Experience 12, Iter 91, disc loss: 0.02928744652549491, policy loss: 6.385280277902607
Experience 12, Iter 92, disc loss: 0.026962790636195486, policy loss: 6.418900566668901
Experience 12, Iter 93, disc loss: 0.022176558599375812, policy loss: 6.886183794150144
Experience 12, Iter 94, disc loss: 0.029244549069969164, policy loss: 5.8564687564936335
Experience 12, Iter 95, disc loss: 0.026449495493555637, policy loss: 5.748049670437311
Experience 12, Iter 96, disc loss: 0.028423477431516284, policy loss: 5.92054419662165
Experience 12, Iter 97, disc loss: 0.02237840464829024, policy loss: 6.3077949385514644
Experience 12, Iter 98, disc loss: 0.02564098728257732, policy loss: 6.354244565284819
Experience 12, Iter 99, disc loss: 0.023998040663007657, policy loss: 6.09326873107143
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1253],
        [1.2442],
        [0.0082]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0092, 0.1243, 0.4706, 0.0092, 0.0052, 2.6889]],

        [[0.0092, 0.1243, 0.4706, 0.0092, 0.0052, 2.6889]],

        [[0.0092, 0.1243, 0.4706, 0.0092, 0.0052, 2.6889]],

        [[0.0092, 0.1243, 0.4706, 0.0092, 0.0052, 2.6889]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0126, 0.5012, 4.9768, 0.0328], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0126, 0.5012, 4.9768, 0.0328])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.331
Iter 2/2000 - Loss: 2.447
Iter 3/2000 - Loss: 2.270
Iter 4/2000 - Loss: 2.247
Iter 5/2000 - Loss: 2.313
Iter 6/2000 - Loss: 2.260
Iter 7/2000 - Loss: 2.180
Iter 8/2000 - Loss: 2.167
Iter 9/2000 - Loss: 2.176
Iter 10/2000 - Loss: 2.135
Iter 11/2000 - Loss: 2.055
Iter 12/2000 - Loss: 1.988
Iter 13/2000 - Loss: 1.939
Iter 14/2000 - Loss: 1.873
Iter 15/2000 - Loss: 1.766
Iter 16/2000 - Loss: 1.628
Iter 17/2000 - Loss: 1.480
Iter 18/2000 - Loss: 1.323
Iter 19/2000 - Loss: 1.149
Iter 20/2000 - Loss: 0.946
Iter 1981/2000 - Loss: -8.023
Iter 1982/2000 - Loss: -8.023
Iter 1983/2000 - Loss: -8.023
Iter 1984/2000 - Loss: -8.023
Iter 1985/2000 - Loss: -8.023
Iter 1986/2000 - Loss: -8.023
Iter 1987/2000 - Loss: -8.023
Iter 1988/2000 - Loss: -8.023
Iter 1989/2000 - Loss: -8.024
Iter 1990/2000 - Loss: -8.024
Iter 1991/2000 - Loss: -8.024
Iter 1992/2000 - Loss: -8.024
Iter 1993/2000 - Loss: -8.024
Iter 1994/2000 - Loss: -8.024
Iter 1995/2000 - Loss: -8.024
Iter 1996/2000 - Loss: -8.024
Iter 1997/2000 - Loss: -8.024
Iter 1998/2000 - Loss: -8.024
Iter 1999/2000 - Loss: -8.024
Iter 2000/2000 - Loss: -8.024
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.9766,  3.3412, 12.1593, 13.4793, 19.1944, 47.4608]],

        [[17.3990, 32.0863, 14.5136,  1.5653,  6.7040, 41.2387]],

        [[18.7031, 35.9594, 15.7205,  0.9954,  1.9095, 18.2811]],

        [[14.3438, 16.1746, 13.8260,  2.5528,  1.5380, 33.5626]]])
Signal Variance: tensor([ 0.0548,  4.4904, 16.7979,  0.3367])
Estimated target variance: tensor([0.0126, 0.5012, 4.9768, 0.0328])
N: 130
Signal to noise ratio: tensor([ 12.3313, 116.4913, 101.0070,  41.5282])
Bound on condition number: tensor([  19768.7763, 1764130.7021, 1326313.5670,  224197.5671])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.013754068844839321, policy loss: 7.600627472574745
Experience 13, Iter 1, disc loss: 0.013253230790546664, policy loss: 7.924288616464768
Experience 13, Iter 2, disc loss: 0.013446130732267057, policy loss: 7.3989324253038395
Experience 13, Iter 3, disc loss: 0.013797324832656038, policy loss: 7.213248553920243
Experience 13, Iter 4, disc loss: 0.014770075059268854, policy loss: 7.694106428496766
Experience 13, Iter 5, disc loss: 0.014913686864777209, policy loss: 7.049798803600539
Experience 13, Iter 6, disc loss: 0.016226597349223063, policy loss: 7.548605257789649
Experience 13, Iter 7, disc loss: 0.014716436645162, policy loss: 6.663888294632152
Experience 13, Iter 8, disc loss: 0.01726656233261348, policy loss: 6.7976255667000665
Experience 13, Iter 9, disc loss: 0.015456402573563914, policy loss: 6.209829595538169
Experience 13, Iter 10, disc loss: 0.020687400832174425, policy loss: 6.675132709971509
Experience 13, Iter 11, disc loss: 0.019345320346387678, policy loss: 5.726122916263741
Experience 13, Iter 12, disc loss: 0.018947158954928506, policy loss: 6.9052328430102925
Experience 13, Iter 13, disc loss: 0.018270468571425006, policy loss: 6.283965877986457
Experience 13, Iter 14, disc loss: 0.018769627487801033, policy loss: 6.984169131756325
Experience 13, Iter 15, disc loss: 0.01915579304233265, policy loss: 6.552449096565379
Experience 13, Iter 16, disc loss: 0.016211902712355238, policy loss: 7.125126462545335
Experience 13, Iter 17, disc loss: 0.01904971692441752, policy loss: 5.638057545245861
Experience 13, Iter 18, disc loss: 0.016370848760899986, policy loss: 6.871754142318736
Experience 13, Iter 19, disc loss: 0.01812210269703284, policy loss: 6.69861794524136
Experience 13, Iter 20, disc loss: 0.015648226287570524, policy loss: 6.793965802096339
Experience 13, Iter 21, disc loss: 0.01755424974461294, policy loss: 7.449941402915797
Experience 13, Iter 22, disc loss: 0.0158174150246151, policy loss: 6.63201629388657
Experience 13, Iter 23, disc loss: 0.014423603883942255, policy loss: 6.541421915112327
Experience 13, Iter 24, disc loss: 0.017807854213558467, policy loss: 6.395791521131086
Experience 13, Iter 25, disc loss: 0.014323987134714646, policy loss: 6.960142497319094
Experience 13, Iter 26, disc loss: 0.015167069030612303, policy loss: 6.745701866701966
Experience 13, Iter 27, disc loss: 0.01436403359882505, policy loss: 7.802519476317479
Experience 13, Iter 28, disc loss: 0.012869179767794503, policy loss: 7.570743757974231
Experience 13, Iter 29, disc loss: 0.01791081348343009, policy loss: 6.583004016794593
Experience 13, Iter 30, disc loss: 0.017034954078254234, policy loss: 6.216176190272294
Experience 13, Iter 31, disc loss: 0.016185306616937026, policy loss: 6.181533124852565
Experience 13, Iter 32, disc loss: 0.013189751520568786, policy loss: 6.840930834032735
Experience 13, Iter 33, disc loss: 0.01544410530840695, policy loss: 6.912629846224186
Experience 13, Iter 34, disc loss: 0.015802750868397275, policy loss: 6.272462088554253
Experience 13, Iter 35, disc loss: 0.014048316452466503, policy loss: 6.161060274661285
Experience 13, Iter 36, disc loss: 0.017724603547515046, policy loss: 6.361137610525628
Experience 13, Iter 37, disc loss: 0.013373106429761951, policy loss: 7.377396544297156
Experience 13, Iter 38, disc loss: 0.01577023873898914, policy loss: 6.9314164809985686
Experience 13, Iter 39, disc loss: 0.01777575511967156, policy loss: 6.334732287549234
Experience 13, Iter 40, disc loss: 0.014408614031401715, policy loss: 6.823659245591394
Experience 13, Iter 41, disc loss: 0.01640884156450686, policy loss: 6.668538324359055
Experience 13, Iter 42, disc loss: 0.017792755963324143, policy loss: 6.0210996477172625
Experience 13, Iter 43, disc loss: 0.014448082806586935, policy loss: 6.891708972249837
Experience 13, Iter 44, disc loss: 0.015667402888601367, policy loss: 6.3058218607867635
Experience 13, Iter 45, disc loss: 0.02065569825019091, policy loss: 5.864158565839839
Experience 13, Iter 46, disc loss: 0.014917542664665231, policy loss: 8.069034886364188
Experience 13, Iter 47, disc loss: 0.017494442061705764, policy loss: 6.152030197050976
Experience 13, Iter 48, disc loss: 0.01637155257938851, policy loss: 6.421218828626714
Experience 13, Iter 49, disc loss: 0.01733634115350266, policy loss: 6.072424498018576
Experience 13, Iter 50, disc loss: 0.020602712574380684, policy loss: 5.801495702690831
Experience 13, Iter 51, disc loss: 0.020015386860181783, policy loss: 6.8800724552426855
Experience 13, Iter 52, disc loss: 0.020954466444873532, policy loss: 6.642574489495264
Experience 13, Iter 53, disc loss: 0.02196415026602897, policy loss: 6.463570632906402
Experience 13, Iter 54, disc loss: 0.01950699026017308, policy loss: 6.615230875482371
Experience 13, Iter 55, disc loss: 0.018231813348406505, policy loss: 7.5304404985266
Experience 13, Iter 56, disc loss: 0.017124451444404276, policy loss: 7.8742051068764685
Experience 13, Iter 57, disc loss: 0.02114033933314703, policy loss: 6.435858972079712
Experience 13, Iter 58, disc loss: 0.017850697265509773, policy loss: 6.198855264739805
Experience 13, Iter 59, disc loss: 0.02021183455898198, policy loss: 5.873673317100032
Experience 13, Iter 60, disc loss: 0.017671730646392894, policy loss: 7.858347593501303
Experience 13, Iter 61, disc loss: 0.017160321161401385, policy loss: 6.148210626279875
Experience 13, Iter 62, disc loss: 0.019127314939761407, policy loss: 7.119528514899573
Experience 13, Iter 63, disc loss: 0.01923580109575275, policy loss: 7.193433538614278
Experience 13, Iter 64, disc loss: 0.019196225177137396, policy loss: 8.270363542302903
Experience 13, Iter 65, disc loss: 0.01801299133254771, policy loss: 7.273486668006267
Experience 13, Iter 66, disc loss: 0.018209742156796237, policy loss: 6.7984368083438484
Experience 13, Iter 67, disc loss: 0.018148696499598063, policy loss: 7.528835614120633
Experience 13, Iter 68, disc loss: 0.019418660096628834, policy loss: 7.359915094157596
Experience 13, Iter 69, disc loss: 0.016504763017594907, policy loss: 8.316722785997122
Experience 13, Iter 70, disc loss: 0.017067906699529574, policy loss: 7.114218650573023
Experience 13, Iter 71, disc loss: 0.01681913045964475, policy loss: 8.111353539183119
Experience 13, Iter 72, disc loss: 0.018693595377570815, policy loss: 7.147628590142992
Experience 13, Iter 73, disc loss: 0.017148070614148206, policy loss: 7.078867070573573
Experience 13, Iter 74, disc loss: 0.017441750024369276, policy loss: 7.68550767597805
Experience 13, Iter 75, disc loss: 0.018356109367120302, policy loss: 5.985302807364077
Experience 13, Iter 76, disc loss: 0.01959993037304634, policy loss: 7.477605314450903
Experience 13, Iter 77, disc loss: 0.017946384504790983, policy loss: 7.089777562223385
Experience 13, Iter 78, disc loss: 0.01660763729037955, policy loss: 7.641613734139667
Experience 13, Iter 79, disc loss: 0.0171133653773555, policy loss: 6.878003405482589
Experience 13, Iter 80, disc loss: 0.013697766628162607, policy loss: 7.22269465339844
Experience 13, Iter 81, disc loss: 0.016230808282338943, policy loss: 5.946900182268272
Experience 13, Iter 82, disc loss: 0.01673107842224393, policy loss: 6.567337743038319
Experience 13, Iter 83, disc loss: 0.012641804143620764, policy loss: 7.185751182215877
Experience 13, Iter 84, disc loss: 0.015646074635270728, policy loss: 6.829593614203265
Experience 13, Iter 85, disc loss: 0.014163720780589585, policy loss: 6.583091665984931
Experience 13, Iter 86, disc loss: 0.016263943866202963, policy loss: 6.788807312996455
Experience 13, Iter 87, disc loss: 0.01441965559419961, policy loss: 7.1388114432310825
Experience 13, Iter 88, disc loss: 0.016339471146548316, policy loss: 6.464418410988136
Experience 13, Iter 89, disc loss: 0.016057000723256123, policy loss: 6.856546163157295
Experience 13, Iter 90, disc loss: 0.014102345399802329, policy loss: 6.373569635257148
Experience 13, Iter 91, disc loss: 0.01463896778188389, policy loss: 6.594313255353402
Experience 13, Iter 92, disc loss: 0.014963210896318045, policy loss: 6.880655841826666
Experience 13, Iter 93, disc loss: 0.016171197499230162, policy loss: 6.865141851255884
Experience 13, Iter 94, disc loss: 0.014505223792877798, policy loss: 7.82285700085948
Experience 13, Iter 95, disc loss: 0.017426321297616863, policy loss: 7.127362628822514
Experience 13, Iter 96, disc loss: 0.015452517699094864, policy loss: 6.090681556017719
Experience 13, Iter 97, disc loss: 0.01518523993642586, policy loss: 6.997132079716987
Experience 13, Iter 98, disc loss: 0.01698696594610284, policy loss: 7.119749432355584
Experience 13, Iter 99, disc loss: 0.019500455661340215, policy loss: 6.35751487871875
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.1297],
        [1.3093],
        [0.0119]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0090, 0.1275, 0.6353, 0.0112, 0.0083, 2.7967]],

        [[0.0090, 0.1275, 0.6353, 0.0112, 0.0083, 2.7967]],

        [[0.0090, 0.1275, 0.6353, 0.0112, 0.0083, 2.7967]],

        [[0.0090, 0.1275, 0.6353, 0.0112, 0.0083, 2.7967]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0127, 0.5186, 5.2372, 0.0474], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0127, 0.5186, 5.2372, 0.0474])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.564
Iter 2/2000 - Loss: 2.656
Iter 3/2000 - Loss: 2.504
Iter 4/2000 - Loss: 2.480
Iter 5/2000 - Loss: 2.522
Iter 6/2000 - Loss: 2.482
Iter 7/2000 - Loss: 2.409
Iter 8/2000 - Loss: 2.376
Iter 9/2000 - Loss: 2.371
Iter 10/2000 - Loss: 2.332
Iter 11/2000 - Loss: 2.254
Iter 12/2000 - Loss: 2.171
Iter 13/2000 - Loss: 2.101
Iter 14/2000 - Loss: 2.025
Iter 15/2000 - Loss: 1.916
Iter 16/2000 - Loss: 1.772
Iter 17/2000 - Loss: 1.608
Iter 18/2000 - Loss: 1.430
Iter 19/2000 - Loss: 1.234
Iter 20/2000 - Loss: 1.014
Iter 1981/2000 - Loss: -7.879
Iter 1982/2000 - Loss: -7.879
Iter 1983/2000 - Loss: -7.879
Iter 1984/2000 - Loss: -7.879
Iter 1985/2000 - Loss: -7.880
Iter 1986/2000 - Loss: -7.880
Iter 1987/2000 - Loss: -7.880
Iter 1988/2000 - Loss: -7.880
Iter 1989/2000 - Loss: -7.880
Iter 1990/2000 - Loss: -7.880
Iter 1991/2000 - Loss: -7.880
Iter 1992/2000 - Loss: -7.880
Iter 1993/2000 - Loss: -7.880
Iter 1994/2000 - Loss: -7.880
Iter 1995/2000 - Loss: -7.880
Iter 1996/2000 - Loss: -7.880
Iter 1997/2000 - Loss: -7.880
Iter 1998/2000 - Loss: -7.880
Iter 1999/2000 - Loss: -7.880
Iter 2000/2000 - Loss: -7.880
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.4993,  3.4584, 13.3481, 13.7922, 22.0931, 49.6995]],

        [[17.0677, 27.1485, 10.0055,  1.3180,  4.0215, 30.3640]],

        [[18.2795, 33.0796,  9.6038,  1.1359,  1.4732, 20.3263]],

        [[14.7726, 25.7305, 14.9362,  2.3039,  2.1257, 28.4334]]])
Signal Variance: tensor([ 0.0551,  2.8378, 19.3803,  0.4032])
Estimated target variance: tensor([0.0127, 0.5186, 5.2372, 0.0474])
N: 140
Signal to noise ratio: tensor([ 12.6020,  91.3929, 111.5870,  45.3551])
Bound on condition number: tensor([  22234.5942, 1169373.8947, 1743232.2853,  287993.0169])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.01902537958045876, policy loss: 6.760247567307237
Experience 14, Iter 1, disc loss: 0.018371501805372795, policy loss: 6.868539349348909
Experience 14, Iter 2, disc loss: 0.023108054677122018, policy loss: 5.249672152641523
Experience 14, Iter 3, disc loss: 0.021184026954232285, policy loss: 5.9597208581686205
Experience 14, Iter 4, disc loss: 0.01920611173743675, policy loss: 6.789137871247131
Experience 14, Iter 5, disc loss: 0.01781052493608099, policy loss: 6.48699303220105
Experience 14, Iter 6, disc loss: 0.019185285982442124, policy loss: 6.0979183918236775
Experience 14, Iter 7, disc loss: 0.02192144160813484, policy loss: 5.642720951024826
Experience 14, Iter 8, disc loss: 0.01785413691743468, policy loss: 6.389997837161426
Experience 14, Iter 9, disc loss: 0.019639254326692605, policy loss: 5.947404364833478
Experience 14, Iter 10, disc loss: 0.015202870890365264, policy loss: 6.975536639034359
Experience 14, Iter 11, disc loss: 0.019491734205418436, policy loss: 5.96476791926418
Experience 14, Iter 12, disc loss: 0.016926162555062664, policy loss: 7.845567167379859
Experience 14, Iter 13, disc loss: 0.0176558887384608, policy loss: 7.3230397489825165
Experience 14, Iter 14, disc loss: 0.019100950813020223, policy loss: 6.343552814782269
Experience 14, Iter 15, disc loss: 0.020381462486821957, policy loss: 6.450178609145815
Experience 14, Iter 16, disc loss: 0.01804352177078942, policy loss: 7.512743029003935
Experience 14, Iter 17, disc loss: 0.016927876715952507, policy loss: 6.731211292087048
Experience 14, Iter 18, disc loss: 0.019873923951537656, policy loss: 6.498425880032999
Experience 14, Iter 19, disc loss: 0.020828708605030933, policy loss: 5.45467684763452
Experience 14, Iter 20, disc loss: 0.02098325189382896, policy loss: 6.421568720352242
Experience 14, Iter 21, disc loss: 0.02316792117385381, policy loss: 5.542045991800524
Experience 14, Iter 22, disc loss: 0.021922434541026166, policy loss: 5.8305650976403385
Experience 14, Iter 23, disc loss: 0.026324065224825405, policy loss: 5.3104389662243
Experience 14, Iter 24, disc loss: 0.021757995606650413, policy loss: 6.298217399777127
Experience 14, Iter 25, disc loss: 0.02381812839480816, policy loss: 6.443169728509416
Experience 14, Iter 26, disc loss: 0.020775096785080972, policy loss: 6.313905894327899
Experience 14, Iter 27, disc loss: 0.022352574157877687, policy loss: 7.061273995480109
Experience 14, Iter 28, disc loss: 0.023316087570230907, policy loss: 6.3600633270776905
Experience 14, Iter 29, disc loss: 0.026753230071483863, policy loss: 6.0858275869538625
Experience 14, Iter 30, disc loss: 0.025803688022513625, policy loss: 6.460832250894136
Experience 14, Iter 31, disc loss: 0.024943254693483956, policy loss: 6.808190198897991
Experience 14, Iter 32, disc loss: 0.022369960817043823, policy loss: 6.961787038818046
Experience 14, Iter 33, disc loss: 0.022365477594348083, policy loss: 6.03714209028148
Experience 14, Iter 34, disc loss: 0.024042137559407158, policy loss: 6.017706084254361
Experience 14, Iter 35, disc loss: 0.020417635366849936, policy loss: 6.373280051568781
Experience 14, Iter 36, disc loss: 0.025853411152684386, policy loss: 5.395591102223548
Experience 14, Iter 37, disc loss: 0.020027832560047573, policy loss: 7.262124582190086
Experience 14, Iter 38, disc loss: 0.025359979513288597, policy loss: 5.9168577055438
Experience 14, Iter 39, disc loss: 0.022980654753124115, policy loss: 6.200473658751159
Experience 14, Iter 40, disc loss: 0.021458199002059225, policy loss: 5.867935863384102
Experience 14, Iter 41, disc loss: 0.021365191574941312, policy loss: 6.828754888725182
Experience 14, Iter 42, disc loss: 0.023323717374316624, policy loss: 6.095844557822617
Experience 14, Iter 43, disc loss: 0.023831990490244627, policy loss: 5.81894289856403
Experience 14, Iter 44, disc loss: 0.02525062798786182, policy loss: 6.670073645156374
Experience 14, Iter 45, disc loss: 0.022723745478102738, policy loss: 6.6883338597084165
Experience 14, Iter 46, disc loss: 0.024696281344351194, policy loss: 5.8707850981064125
Experience 14, Iter 47, disc loss: 0.025469548385376763, policy loss: 6.019638447934781
Experience 14, Iter 48, disc loss: 0.021626110193219597, policy loss: 6.523101579002667
Experience 14, Iter 49, disc loss: 0.022651914695511526, policy loss: 7.031402538947539
Experience 14, Iter 50, disc loss: 0.02046827422732452, policy loss: 6.126227341295635
Experience 14, Iter 51, disc loss: 0.024395751916702194, policy loss: 6.245906108788683
Experience 14, Iter 52, disc loss: 0.021247922692121883, policy loss: 5.900178599929232
Experience 14, Iter 53, disc loss: 0.020406173275893284, policy loss: 6.6061728302385125
Experience 14, Iter 54, disc loss: 0.022866462516857443, policy loss: 5.789048443970845
Experience 14, Iter 55, disc loss: 0.022265858002340988, policy loss: 6.01543865392996
Experience 14, Iter 56, disc loss: 0.020103758227512345, policy loss: 6.425476380850142
Experience 14, Iter 57, disc loss: 0.020423867497215958, policy loss: 6.031419803502928
Experience 14, Iter 58, disc loss: 0.02193745361971567, policy loss: 5.900117168885365
Experience 14, Iter 59, disc loss: 0.022038634968498304, policy loss: 5.954497652741239
Experience 14, Iter 60, disc loss: 0.018919366834963883, policy loss: 6.751203079551198
Experience 14, Iter 61, disc loss: 0.020293101935944025, policy loss: 6.120815056534578
Experience 14, Iter 62, disc loss: 0.022832406347781505, policy loss: 5.6944390885984575
Experience 14, Iter 63, disc loss: 0.020414733061582993, policy loss: 6.46605263879138
Experience 14, Iter 64, disc loss: 0.020835829129859488, policy loss: 5.992442577385462
Experience 14, Iter 65, disc loss: 0.020717867792919217, policy loss: 6.2346651844692715
Experience 14, Iter 66, disc loss: 0.022433200266670835, policy loss: 5.876614937218841
Experience 14, Iter 67, disc loss: 0.02439276133464795, policy loss: 5.953271050250571
Experience 14, Iter 68, disc loss: 0.023491257232179742, policy loss: 5.834035957283537
Experience 14, Iter 69, disc loss: 0.02438266209340257, policy loss: 5.588823802320201
Experience 14, Iter 70, disc loss: 0.022369591871115122, policy loss: 6.294435920784498
Experience 14, Iter 71, disc loss: 0.022588745050077193, policy loss: 5.947962345280441
Experience 14, Iter 72, disc loss: 0.02220724721490383, policy loss: 5.69972691223136
Experience 14, Iter 73, disc loss: 0.027882810967417147, policy loss: 5.265493695225419
Experience 14, Iter 74, disc loss: 0.025633748948403548, policy loss: 6.342173084528991
Experience 14, Iter 75, disc loss: 0.02398876529176703, policy loss: 5.844507394920098
Experience 14, Iter 76, disc loss: 0.023877150510618974, policy loss: 5.639994739054228
Experience 14, Iter 77, disc loss: 0.022020283939358228, policy loss: 6.381312761589477
Experience 14, Iter 78, disc loss: 0.021841503888541954, policy loss: 6.17326832195754
Experience 14, Iter 79, disc loss: 0.022590605848981975, policy loss: 5.749378671687499
Experience 14, Iter 80, disc loss: 0.024391334868319365, policy loss: 5.152384355546559
Experience 14, Iter 81, disc loss: 0.020536980741998435, policy loss: 6.6053229895207615
Experience 14, Iter 82, disc loss: 0.023339031298247617, policy loss: 6.4944705399097575
Experience 14, Iter 83, disc loss: 0.02315986627830955, policy loss: 5.568060683597302
Experience 14, Iter 84, disc loss: 0.02394741484068639, policy loss: 5.823240432618577
Experience 14, Iter 85, disc loss: 0.02182621437974492, policy loss: 5.721805941268496
Experience 14, Iter 86, disc loss: 0.022913547064770185, policy loss: 5.5643805039567305
Experience 14, Iter 87, disc loss: 0.021919185395407957, policy loss: 5.9844064344341685
Experience 14, Iter 88, disc loss: 0.020659152074670853, policy loss: 5.8619938751091984
Experience 14, Iter 89, disc loss: 0.022667305992974264, policy loss: 5.996964853343346
Experience 14, Iter 90, disc loss: 0.02160837105190997, policy loss: 6.2716794895224615
Experience 14, Iter 91, disc loss: 0.02595995249947534, policy loss: 5.65169814778851
Experience 14, Iter 92, disc loss: 0.02226312736311237, policy loss: 5.21076104277636
Experience 14, Iter 93, disc loss: 0.021357480126104098, policy loss: 5.9812561813851275
Experience 14, Iter 94, disc loss: 0.023287846926598008, policy loss: 5.895778927170561
Experience 14, Iter 95, disc loss: 0.025264075019909064, policy loss: 5.7731004838521045
Experience 14, Iter 96, disc loss: 0.022857230105223177, policy loss: 5.542472035392747
Experience 14, Iter 97, disc loss: 0.023918881987729998, policy loss: 5.616036820505375
Experience 14, Iter 98, disc loss: 0.018748928479208733, policy loss: 6.576706332256346
Experience 14, Iter 99, disc loss: 0.022371791020152826, policy loss: 6.107500264648152
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1384],
        [1.3985],
        [0.0162]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0089, 0.1338, 0.8309, 0.0133, 0.0116, 2.9435]],

        [[0.0089, 0.1338, 0.8309, 0.0133, 0.0116, 2.9435]],

        [[0.0089, 0.1338, 0.8309, 0.0133, 0.0116, 2.9435]],

        [[0.0089, 0.1338, 0.8309, 0.0133, 0.0116, 2.9435]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0131, 0.5536, 5.5941, 0.0648], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0131, 0.5536, 5.5941, 0.0648])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.805
Iter 2/2000 - Loss: 2.875
Iter 3/2000 - Loss: 2.739
Iter 4/2000 - Loss: 2.723
Iter 5/2000 - Loss: 2.746
Iter 6/2000 - Loss: 2.699
Iter 7/2000 - Loss: 2.641
Iter 8/2000 - Loss: 2.612
Iter 9/2000 - Loss: 2.592
Iter 10/2000 - Loss: 2.540
Iter 11/2000 - Loss: 2.460
Iter 12/2000 - Loss: 2.379
Iter 13/2000 - Loss: 2.305
Iter 14/2000 - Loss: 2.215
Iter 15/2000 - Loss: 2.093
Iter 16/2000 - Loss: 1.940
Iter 17/2000 - Loss: 1.770
Iter 18/2000 - Loss: 1.584
Iter 19/2000 - Loss: 1.377
Iter 20/2000 - Loss: 1.143
Iter 1981/2000 - Loss: -7.794
Iter 1982/2000 - Loss: -7.794
Iter 1983/2000 - Loss: -7.794
Iter 1984/2000 - Loss: -7.794
Iter 1985/2000 - Loss: -7.794
Iter 1986/2000 - Loss: -7.794
Iter 1987/2000 - Loss: -7.794
Iter 1988/2000 - Loss: -7.794
Iter 1989/2000 - Loss: -7.794
Iter 1990/2000 - Loss: -7.794
Iter 1991/2000 - Loss: -7.794
Iter 1992/2000 - Loss: -7.794
Iter 1993/2000 - Loss: -7.794
Iter 1994/2000 - Loss: -7.794
Iter 1995/2000 - Loss: -7.795
Iter 1996/2000 - Loss: -7.795
Iter 1997/2000 - Loss: -7.795
Iter 1998/2000 - Loss: -7.795
Iter 1999/2000 - Loss: -7.795
Iter 2000/2000 - Loss: -7.795
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.3329,  3.8355, 35.0961, 11.6482, 18.7169, 51.3105]],

        [[16.9154, 32.4609, 10.8346,  1.3096,  1.8370, 29.4434]],

        [[17.3283, 34.0136,  9.2239,  1.2566,  1.2129, 21.6068]],

        [[14.7230, 30.2952, 15.3599,  2.3553,  2.3291, 30.2807]]])
Signal Variance: tensor([ 0.0639,  2.7004, 18.6722,  0.5193])
Estimated target variance: tensor([0.0131, 0.5536, 5.5941, 0.0648])
N: 150
Signal to noise ratio: tensor([ 13.2927,  89.8862, 108.5218,  50.9344])
Bound on condition number: tensor([  26505.3620, 1211931.1725, 1766547.4288,  389148.5589])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.020448672741741298, policy loss: 6.567457680228054
Experience 15, Iter 1, disc loss: 0.023670715961790545, policy loss: 5.405199873678978
Experience 15, Iter 2, disc loss: 0.0241851913202554, policy loss: 5.892524324409861
Experience 15, Iter 3, disc loss: 0.021639261880731692, policy loss: 6.108952287549953
Experience 15, Iter 4, disc loss: 0.0240976795805894, policy loss: 5.867362669810199
Experience 15, Iter 5, disc loss: 0.018624471424227546, policy loss: 7.465178777020797
Experience 15, Iter 6, disc loss: 0.024058791857202923, policy loss: 5.424641829035508
Experience 15, Iter 7, disc loss: 0.020340783668170215, policy loss: 5.994907857340535
Experience 15, Iter 8, disc loss: 0.02468569381930709, policy loss: 5.5685246659793215
Experience 15, Iter 9, disc loss: 0.024607910477927902, policy loss: 5.020927055151997
Experience 15, Iter 10, disc loss: 0.023781284342730913, policy loss: 5.368521098457178
Experience 15, Iter 11, disc loss: 0.02336501527085192, policy loss: 6.074936277551405
Experience 15, Iter 12, disc loss: 0.023034606906549772, policy loss: 6.2163053780984
Experience 15, Iter 13, disc loss: 0.022539788477902944, policy loss: 6.049379576166256
Experience 15, Iter 14, disc loss: 0.023924400470561973, policy loss: 5.720320303647303
Experience 15, Iter 15, disc loss: 0.023252237052669662, policy loss: 5.7217120874671075
Experience 15, Iter 16, disc loss: 0.01949153951650407, policy loss: 6.151925540455242
Experience 15, Iter 17, disc loss: 0.02299404005092653, policy loss: 6.1785852446716305
Experience 15, Iter 18, disc loss: 0.023325482716586677, policy loss: 5.562893386772753
Experience 15, Iter 19, disc loss: 0.02156118776188029, policy loss: 6.096347223102464
Experience 15, Iter 20, disc loss: 0.021961519876803212, policy loss: 5.638476915886248
Experience 15, Iter 21, disc loss: 0.022072429825883925, policy loss: 5.609539235976893
Experience 15, Iter 22, disc loss: 0.022433915076493893, policy loss: 5.358239919974023
Experience 15, Iter 23, disc loss: 0.020979339494190837, policy loss: 5.820323638453063
Experience 15, Iter 24, disc loss: 0.020200565124791084, policy loss: 5.938259135559171
Experience 15, Iter 25, disc loss: 0.019811081236126408, policy loss: 5.872650583690546
Experience 15, Iter 26, disc loss: 0.021222253053344395, policy loss: 6.233071937284366
Experience 15, Iter 27, disc loss: 0.02161084093478321, policy loss: 5.520419754891111
Experience 15, Iter 28, disc loss: 0.02211372724647803, policy loss: 5.7329565857205145
Experience 15, Iter 29, disc loss: 0.021483999293617925, policy loss: 5.517844725457399
Experience 15, Iter 30, disc loss: 0.019549987720132056, policy loss: 5.904698359718816
Experience 15, Iter 31, disc loss: 0.019336116650172368, policy loss: 6.006559621931389
Experience 15, Iter 32, disc loss: 0.019845901719515013, policy loss: 5.607328631197829
Experience 15, Iter 33, disc loss: 0.020096199876514585, policy loss: 5.716535504833249
Experience 15, Iter 34, disc loss: 0.020121773976981804, policy loss: 5.54760886710264
Experience 15, Iter 35, disc loss: 0.02163907888972557, policy loss: 5.376158062063548
Experience 15, Iter 36, disc loss: 0.019782162956254547, policy loss: 5.885373485410921
Experience 15, Iter 37, disc loss: 0.02222954950004346, policy loss: 5.772198486204159
Experience 15, Iter 38, disc loss: 0.021025132565430062, policy loss: 5.720671698453
Experience 15, Iter 39, disc loss: 0.019558966641709537, policy loss: 5.815660629495895
Experience 15, Iter 40, disc loss: 0.01996801291337709, policy loss: 6.495878719369605
Experience 15, Iter 41, disc loss: 0.02128407568235935, policy loss: 5.642321401072266
Experience 15, Iter 42, disc loss: 0.019080892565787794, policy loss: 5.764577611246886
Experience 15, Iter 43, disc loss: 0.018949121523699715, policy loss: 5.927883725069924
Experience 15, Iter 44, disc loss: 0.017113578964205782, policy loss: 6.395365705168014
Experience 15, Iter 45, disc loss: 0.018746863560765645, policy loss: 5.952410809805402
Experience 15, Iter 46, disc loss: 0.018667656817366485, policy loss: 5.212555508571182
Experience 15, Iter 47, disc loss: 0.019569209483759363, policy loss: 5.585068227385746
Experience 15, Iter 48, disc loss: 0.02049747899812645, policy loss: 5.544588640039192
Experience 15, Iter 49, disc loss: 0.021820453777734138, policy loss: 4.947842774604492
Experience 15, Iter 50, disc loss: 0.021181018804974736, policy loss: 5.8911456229005115
Experience 15, Iter 51, disc loss: 0.02057619891992264, policy loss: 5.890129245579949
Experience 15, Iter 52, disc loss: 0.020392820810456375, policy loss: 5.421420063478225
Experience 15, Iter 53, disc loss: 0.021512069644344906, policy loss: 5.600369657654671
Experience 15, Iter 54, disc loss: 0.019974914290530797, policy loss: 5.530328421566551
Experience 15, Iter 55, disc loss: 0.0192139960595636, policy loss: 5.674260940199593
Experience 15, Iter 56, disc loss: 0.017395854350469114, policy loss: 5.813616865764605
Experience 15, Iter 57, disc loss: 0.018103917063911693, policy loss: 5.707074315102478
Experience 15, Iter 58, disc loss: 0.020075536303670836, policy loss: 5.163687122729003
Experience 15, Iter 59, disc loss: 0.022965540398856628, policy loss: 5.08440619012422
Experience 15, Iter 60, disc loss: 0.0217648941192543, policy loss: 5.8942637163853915
Experience 15, Iter 61, disc loss: 0.02279908199223537, policy loss: 5.391507692461417
Experience 15, Iter 62, disc loss: 0.020086743060794048, policy loss: 6.091209643307671
Experience 15, Iter 63, disc loss: 0.019494153991236038, policy loss: 5.698036487725554
Experience 15, Iter 64, disc loss: 0.02251490207861154, policy loss: 4.836819490883042
Experience 15, Iter 65, disc loss: 0.0191655625284896, policy loss: 5.29031688161662
Experience 15, Iter 66, disc loss: 0.020060209616217354, policy loss: 5.343104795678142
Experience 15, Iter 67, disc loss: 0.02037778863725794, policy loss: 5.211736809998194
Experience 15, Iter 68, disc loss: 0.02088150573041265, policy loss: 5.3015519418064585
Experience 15, Iter 69, disc loss: 0.017954886736053305, policy loss: 5.403810259792074
Experience 15, Iter 70, disc loss: 0.021180439794470353, policy loss: 4.942657571986036
Experience 15, Iter 71, disc loss: 0.019357172014933186, policy loss: 5.297118535538473
Experience 15, Iter 72, disc loss: 0.020866331569054118, policy loss: 5.058100508216229
Experience 15, Iter 73, disc loss: 0.02008504624683398, policy loss: 5.18264637254974
Experience 15, Iter 74, disc loss: 0.019496630506728042, policy loss: 5.771747319533829
Experience 15, Iter 75, disc loss: 0.020017322373410678, policy loss: 5.348752623795205
Experience 15, Iter 76, disc loss: 0.020251271641543826, policy loss: 5.022398031140538
Experience 15, Iter 77, disc loss: 0.019032474218870075, policy loss: 5.360979586204241
Experience 15, Iter 78, disc loss: 0.01841933046011903, policy loss: 5.2290627104407825
Experience 15, Iter 79, disc loss: 0.01975677835834106, policy loss: 5.080991449924783
Experience 15, Iter 80, disc loss: 0.020776381346529753, policy loss: 4.936660805671381
Experience 15, Iter 81, disc loss: 0.02301746379197225, policy loss: 4.7144177600630215
Experience 15, Iter 82, disc loss: 0.02114847082457639, policy loss: 5.26373342798837
Experience 15, Iter 83, disc loss: 0.02272326660126613, policy loss: 4.759825735497284
Experience 15, Iter 84, disc loss: 0.021996779413437313, policy loss: 5.266805729533894
Experience 15, Iter 85, disc loss: 0.021490584361919564, policy loss: 5.583119087297405
Experience 15, Iter 86, disc loss: 0.020920971851506355, policy loss: 5.793392399497106
Experience 15, Iter 87, disc loss: 0.01974081166620156, policy loss: 5.424312700088926
Experience 15, Iter 88, disc loss: 0.018959934799637902, policy loss: 5.999670648113364
Experience 15, Iter 89, disc loss: 0.018504575646777717, policy loss: 5.734342889262358
Experience 15, Iter 90, disc loss: 0.020827913016316497, policy loss: 4.9838001705177515
Experience 15, Iter 91, disc loss: 0.019579453326660202, policy loss: 5.188045312665638
Experience 15, Iter 92, disc loss: 0.019531039194455077, policy loss: 5.747728584919443
Experience 15, Iter 93, disc loss: 0.020891569182325317, policy loss: 5.595050219005114
Experience 15, Iter 94, disc loss: 0.02079965800144274, policy loss: 5.328893760909816
Experience 15, Iter 95, disc loss: 0.020364642159733284, policy loss: 5.736776235257181
Experience 15, Iter 96, disc loss: 0.022095935152389713, policy loss: 5.336742405462854
Experience 15, Iter 97, disc loss: 0.022234161665287307, policy loss: 4.752195422294784
Experience 15, Iter 98, disc loss: 0.01862712296803873, policy loss: 6.037708315975136
Experience 15, Iter 99, disc loss: 0.01898659678497428, policy loss: 5.498211127517204
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1475],
        [1.4635],
        [0.0188]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0086, 0.1362, 0.9495, 0.0150, 0.0139, 3.1441]],

        [[0.0086, 0.1362, 0.9495, 0.0150, 0.0139, 3.1441]],

        [[0.0086, 0.1362, 0.9495, 0.0150, 0.0139, 3.1441]],

        [[0.0086, 0.1362, 0.9495, 0.0150, 0.0139, 3.1441]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0132, 0.5900, 5.8539, 0.0753], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0132, 0.5900, 5.8539, 0.0753])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.941
Iter 2/2000 - Loss: 3.004
Iter 3/2000 - Loss: 2.867
Iter 4/2000 - Loss: 2.859
Iter 5/2000 - Loss: 2.879
Iter 6/2000 - Loss: 2.822
Iter 7/2000 - Loss: 2.765
Iter 8/2000 - Loss: 2.743
Iter 9/2000 - Loss: 2.726
Iter 10/2000 - Loss: 2.668
Iter 11/2000 - Loss: 2.582
Iter 12/2000 - Loss: 2.500
Iter 13/2000 - Loss: 2.427
Iter 14/2000 - Loss: 2.339
Iter 15/2000 - Loss: 2.216
Iter 16/2000 - Loss: 2.060
Iter 17/2000 - Loss: 1.887
Iter 18/2000 - Loss: 1.700
Iter 19/2000 - Loss: 1.495
Iter 20/2000 - Loss: 1.264
Iter 1981/2000 - Loss: -7.793
Iter 1982/2000 - Loss: -7.793
Iter 1983/2000 - Loss: -7.793
Iter 1984/2000 - Loss: -7.793
Iter 1985/2000 - Loss: -7.793
Iter 1986/2000 - Loss: -7.793
Iter 1987/2000 - Loss: -7.793
Iter 1988/2000 - Loss: -7.793
Iter 1989/2000 - Loss: -7.793
Iter 1990/2000 - Loss: -7.793
Iter 1991/2000 - Loss: -7.793
Iter 1992/2000 - Loss: -7.793
Iter 1993/2000 - Loss: -7.793
Iter 1994/2000 - Loss: -7.793
Iter 1995/2000 - Loss: -7.793
Iter 1996/2000 - Loss: -7.793
Iter 1997/2000 - Loss: -7.793
Iter 1998/2000 - Loss: -7.793
Iter 1999/2000 - Loss: -7.793
Iter 2000/2000 - Loss: -7.794
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.6305,  4.1637, 36.5583,  9.6664, 18.5625, 51.2134]],

        [[16.4853, 32.9682, 10.2310,  1.2871,  2.0560, 27.7500]],

        [[15.9571, 32.4579,  9.4515,  1.1545,  1.1058, 20.7628]],

        [[12.7192, 32.1069, 15.8946,  2.4383,  1.9531, 29.2374]]])
Signal Variance: tensor([ 0.0678,  2.3826, 16.8137,  0.4834])
Estimated target variance: tensor([0.0132, 0.5900, 5.8539, 0.0753])
N: 160
Signal to noise ratio: tensor([ 13.7226,  83.8620, 102.3167,  48.7242])
Bound on condition number: tensor([  30130.5712, 1125255.4734, 1674992.5154,  379848.0119])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.021351478774979865, policy loss: 5.098234975621614
Experience 16, Iter 1, disc loss: 0.022111205052613186, policy loss: 5.062767487507471
Experience 16, Iter 2, disc loss: 0.02200781589661055, policy loss: 5.118582225262987
Experience 16, Iter 3, disc loss: 0.022911599728823492, policy loss: 4.859092816333846
Experience 16, Iter 4, disc loss: 0.01974771567384378, policy loss: 5.704914962532562
Experience 16, Iter 5, disc loss: 0.021296335217869052, policy loss: 5.7821061512820355
Experience 16, Iter 6, disc loss: 0.022228151832312734, policy loss: 4.894769583662111
Experience 16, Iter 7, disc loss: 0.019725205394398596, policy loss: 5.58668617321989
Experience 16, Iter 8, disc loss: 0.02095173622146023, policy loss: 5.015100936880028
Experience 16, Iter 9, disc loss: 0.02120304182325094, policy loss: 5.4950936974706766
Experience 16, Iter 10, disc loss: 0.02127448832021868, policy loss: 5.558523775409492
Experience 16, Iter 11, disc loss: 0.022333723582105416, policy loss: 4.902209328530734
Experience 16, Iter 12, disc loss: 0.022379138999675363, policy loss: 5.282935718564406
Experience 16, Iter 13, disc loss: 0.020376009129555755, policy loss: 5.3633935271169655
Experience 16, Iter 14, disc loss: 0.02347637802734128, policy loss: 4.661038877171242
Experience 16, Iter 15, disc loss: 0.021062060642829776, policy loss: 5.270819168864505
Experience 16, Iter 16, disc loss: 0.022820168167776655, policy loss: 4.9361269949931
Experience 16, Iter 17, disc loss: 0.021875767344841507, policy loss: 5.514835297778261
Experience 16, Iter 18, disc loss: 0.0218121609171453, policy loss: 5.337745626984645
Experience 16, Iter 19, disc loss: 0.018510797249025497, policy loss: 6.056336665384
Experience 16, Iter 20, disc loss: 0.020565692475120287, policy loss: 5.395932352438807
Experience 16, Iter 21, disc loss: 0.021292746586465496, policy loss: 5.1193279747002265
Experience 16, Iter 22, disc loss: 0.019913502314783785, policy loss: 5.24354902165348
Experience 16, Iter 23, disc loss: 0.01999177520440199, policy loss: 5.32044712042943
Experience 16, Iter 24, disc loss: 0.022274392966796452, policy loss: 4.931639152034316
Experience 16, Iter 25, disc loss: 0.022188953319701107, policy loss: 4.8562912236043445
Experience 16, Iter 26, disc loss: 0.021628856837813588, policy loss: 4.930123173687167
Experience 16, Iter 27, disc loss: 0.02046286227174935, policy loss: 5.38996148770653
Experience 16, Iter 28, disc loss: 0.020872496741474022, policy loss: 5.754878483741084
Experience 16, Iter 29, disc loss: 0.021074590704438827, policy loss: 5.0389289529017525
Experience 16, Iter 30, disc loss: 0.02043325321151304, policy loss: 5.3963902712180305
Experience 16, Iter 31, disc loss: 0.01984720926022489, policy loss: 5.1379701003034794
Experience 16, Iter 32, disc loss: 0.01867086834877478, policy loss: 5.386463148263241
Experience 16, Iter 33, disc loss: 0.019519187445717116, policy loss: 5.250557819534293
Experience 16, Iter 34, disc loss: 0.02130225957494873, policy loss: 5.020187841049895
Experience 16, Iter 35, disc loss: 0.01853866560887754, policy loss: 5.435893044217472
Experience 16, Iter 36, disc loss: 0.020581235318635836, policy loss: 5.275045018523393
Experience 16, Iter 37, disc loss: 0.021592591246700467, policy loss: 4.780901762681408
Experience 16, Iter 38, disc loss: 0.02088077213458154, policy loss: 5.02259345782514
Experience 16, Iter 39, disc loss: 0.02155279068834664, policy loss: 5.019177071625748
Experience 16, Iter 40, disc loss: 0.019399869477511017, policy loss: 5.366759722398678
Experience 16, Iter 41, disc loss: 0.019833265840524777, policy loss: 5.491921164760905
Experience 16, Iter 42, disc loss: 0.020640583739671238, policy loss: 5.414870286818664
Experience 16, Iter 43, disc loss: 0.020378440490900304, policy loss: 5.2224732904774696
Experience 16, Iter 44, disc loss: 0.018500583686336336, policy loss: 5.5399828995591305
Experience 16, Iter 45, disc loss: 0.02260809306632043, policy loss: 4.62806796235692
Experience 16, Iter 46, disc loss: 0.018657711419136934, policy loss: 5.752985401540733
Experience 16, Iter 47, disc loss: 0.021661563941107825, policy loss: 5.004718860848239
Experience 16, Iter 48, disc loss: 0.020731859959149435, policy loss: 5.061408229319653
Experience 16, Iter 49, disc loss: 0.019293543954463924, policy loss: 5.117720538842505
Experience 16, Iter 50, disc loss: 0.018995543309559222, policy loss: 5.157930692917145
Experience 16, Iter 51, disc loss: 0.019323384749395947, policy loss: 5.012205279517686
Experience 16, Iter 52, disc loss: 0.017842460751522717, policy loss: 5.514182001216307
Experience 16, Iter 53, disc loss: 0.018854499984465214, policy loss: 5.292902260228051
Experience 16, Iter 54, disc loss: 0.02045201197866823, policy loss: 5.18937490038425
Experience 16, Iter 55, disc loss: 0.020975062998767606, policy loss: 5.206075706127428
Experience 16, Iter 56, disc loss: 0.018796637370826614, policy loss: 5.561723530664632
Experience 16, Iter 57, disc loss: 0.019411611629188332, policy loss: 5.726620665414899
Experience 16, Iter 58, disc loss: 0.017560658412643457, policy loss: 5.434536824460879
Experience 16, Iter 59, disc loss: 0.016799251519769572, policy loss: 5.3360451154213315
Experience 16, Iter 60, disc loss: 0.017674540307190595, policy loss: 5.568497461743355
Experience 16, Iter 61, disc loss: 0.019332484919803467, policy loss: 5.180317636516234
Experience 16, Iter 62, disc loss: 0.019266114304472275, policy loss: 5.362090929382973
Experience 16, Iter 63, disc loss: 0.018960766942165603, policy loss: 4.94822121399101
Experience 16, Iter 64, disc loss: 0.017834072734870234, policy loss: 5.5805065141745605
Experience 16, Iter 65, disc loss: 0.01829377651857895, policy loss: 4.916144330622282
Experience 16, Iter 66, disc loss: 0.01798976653469154, policy loss: 5.077555841543286
Experience 16, Iter 67, disc loss: 0.017294457431334585, policy loss: 5.2340711982933374
Experience 16, Iter 68, disc loss: 0.017516043924213742, policy loss: 5.008442304019587
Experience 16, Iter 69, disc loss: 0.01901197537043607, policy loss: 5.245309225112779
Experience 16, Iter 70, disc loss: 0.018115063209866687, policy loss: 5.369240729219872
Experience 16, Iter 71, disc loss: 0.017646008233401164, policy loss: 5.373730592270029
Experience 16, Iter 72, disc loss: 0.019117054766831818, policy loss: 5.0279639539973875
Experience 16, Iter 73, disc loss: 0.018585798480947303, policy loss: 5.287855009190169
Experience 16, Iter 74, disc loss: 0.016677968750645186, policy loss: 5.684388267632053
Experience 16, Iter 75, disc loss: 0.01859480557749156, policy loss: 5.236869723832485
Experience 16, Iter 76, disc loss: 0.017901585447590747, policy loss: 5.402939595540286
Experience 16, Iter 77, disc loss: 0.017530309780278152, policy loss: 5.6946549906484
Experience 16, Iter 78, disc loss: 0.018852821294834857, policy loss: 4.967324268186452
Experience 16, Iter 79, disc loss: 0.01874046136075448, policy loss: 5.16020287026841
Experience 16, Iter 80, disc loss: 0.018229947144734873, policy loss: 5.585148825013255
Experience 16, Iter 81, disc loss: 0.01715572906633265, policy loss: 5.64302773344309
Experience 16, Iter 82, disc loss: 0.0161783498874989, policy loss: 5.737604367739855
Experience 16, Iter 83, disc loss: 0.018563584313725554, policy loss: 4.945008948146009
Experience 16, Iter 84, disc loss: 0.01602534903389563, policy loss: 5.474143976318373
Experience 16, Iter 85, disc loss: 0.01631382587371032, policy loss: 5.253488733579924
Experience 16, Iter 86, disc loss: 0.016483090151135353, policy loss: 5.563539241392661
Experience 16, Iter 87, disc loss: 0.015557874762474076, policy loss: 5.433904041543784
Experience 16, Iter 88, disc loss: 0.015578126532995862, policy loss: 5.295255400632453
Experience 16, Iter 89, disc loss: 0.015033874851857653, policy loss: 5.506350687722916
Experience 16, Iter 90, disc loss: 0.01689829383289267, policy loss: 5.027150716639223
Experience 16, Iter 91, disc loss: 0.01500954737646509, policy loss: 5.841673944874493
Experience 16, Iter 92, disc loss: 0.01711906484050984, policy loss: 4.917410215335797
Experience 16, Iter 93, disc loss: 0.015803983724736157, policy loss: 5.3446362003372805
Experience 16, Iter 94, disc loss: 0.015383834878824512, policy loss: 5.636309626995274
Experience 16, Iter 95, disc loss: 0.015854848135628187, policy loss: 5.612715969362611
Experience 16, Iter 96, disc loss: 0.016694525974679287, policy loss: 5.392109032693222
Experience 16, Iter 97, disc loss: 0.015791370617210226, policy loss: 5.65572366052819
Experience 16, Iter 98, disc loss: 0.01713016630993938, policy loss: 4.940172590268584
Experience 16, Iter 99, disc loss: 0.015389400816154346, policy loss: 5.355627563739915
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1552],
        [1.4888],
        [0.0202]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0082, 0.1366, 1.0071, 0.0163, 0.0166, 3.4000]],

        [[0.0082, 0.1366, 1.0071, 0.0163, 0.0166, 3.4000]],

        [[0.0082, 0.1366, 1.0071, 0.0163, 0.0166, 3.4000]],

        [[0.0082, 0.1366, 1.0071, 0.0163, 0.0166, 3.4000]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0132, 0.6206, 5.9553, 0.0807], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0132, 0.6206, 5.9553, 0.0807])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.015
Iter 2/2000 - Loss: 3.082
Iter 3/2000 - Loss: 2.929
Iter 4/2000 - Loss: 2.937
Iter 5/2000 - Loss: 2.961
Iter 6/2000 - Loss: 2.894
Iter 7/2000 - Loss: 2.828
Iter 8/2000 - Loss: 2.811
Iter 9/2000 - Loss: 2.802
Iter 10/2000 - Loss: 2.749
Iter 11/2000 - Loss: 2.657
Iter 12/2000 - Loss: 2.566
Iter 13/2000 - Loss: 2.490
Iter 14/2000 - Loss: 2.405
Iter 15/2000 - Loss: 2.287
Iter 16/2000 - Loss: 2.131
Iter 17/2000 - Loss: 1.953
Iter 18/2000 - Loss: 1.761
Iter 19/2000 - Loss: 1.554
Iter 20/2000 - Loss: 1.324
Iter 1981/2000 - Loss: -7.808
Iter 1982/2000 - Loss: -7.808
Iter 1983/2000 - Loss: -7.808
Iter 1984/2000 - Loss: -7.808
Iter 1985/2000 - Loss: -7.808
Iter 1986/2000 - Loss: -7.808
Iter 1987/2000 - Loss: -7.808
Iter 1988/2000 - Loss: -7.808
Iter 1989/2000 - Loss: -7.808
Iter 1990/2000 - Loss: -7.808
Iter 1991/2000 - Loss: -7.808
Iter 1992/2000 - Loss: -7.808
Iter 1993/2000 - Loss: -7.808
Iter 1994/2000 - Loss: -7.808
Iter 1995/2000 - Loss: -7.808
Iter 1996/2000 - Loss: -7.808
Iter 1997/2000 - Loss: -7.808
Iter 1998/2000 - Loss: -7.809
Iter 1999/2000 - Loss: -7.809
Iter 2000/2000 - Loss: -7.809
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[11.5703,  8.3747, 34.2163,  1.5996, 16.3296, 59.3775]],

        [[16.6699, 31.8299,  9.4970,  1.3683,  2.1578, 29.0818]],

        [[17.4127, 31.3248,  9.0643,  1.0598,  1.0870, 20.3599]],

        [[13.3621, 31.6876, 15.8284,  2.5454,  1.9050, 25.5105]]])
Signal Variance: tensor([ 0.1164,  2.3610, 15.4744,  0.4682])
Estimated target variance: tensor([0.0132, 0.6206, 5.9553, 0.0807])
N: 170
Signal to noise ratio: tensor([ 18.7131,  83.2669, 100.4207,  47.3707])
Bound on condition number: tensor([  59531.9019, 1178674.1689, 1714335.9697,  381477.8682])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.012887791626264364, policy loss: 5.581118186192964
Experience 17, Iter 1, disc loss: 0.01517802197339921, policy loss: 5.071194905993921
Experience 17, Iter 2, disc loss: 0.012862626787223335, policy loss: 5.7038766433721815
Experience 17, Iter 3, disc loss: 0.012985056428557289, policy loss: 5.653891948177272
Experience 17, Iter 4, disc loss: 0.015149681499086297, policy loss: 5.3432994016380535
Experience 17, Iter 5, disc loss: 0.013220822619350928, policy loss: 5.334814957937643
Experience 17, Iter 6, disc loss: 0.014271618990948491, policy loss: 5.368403606598011
Experience 17, Iter 7, disc loss: 0.014334843403276908, policy loss: 5.501411633410489
Experience 17, Iter 8, disc loss: 0.015667212131000943, policy loss: 5.729303475561673
Experience 17, Iter 9, disc loss: 0.013731486411299454, policy loss: 5.54641365239998
Experience 17, Iter 10, disc loss: 0.01505998137371817, policy loss: 5.323646401269833
Experience 17, Iter 11, disc loss: 0.014696551025141984, policy loss: 5.522175192020994
Experience 17, Iter 12, disc loss: 0.014554186686821766, policy loss: 5.244572158446959
Experience 17, Iter 13, disc loss: 0.014778363785926028, policy loss: 5.543405835896241
Experience 17, Iter 14, disc loss: 0.014772278070913518, policy loss: 5.334583069365365
Experience 17, Iter 15, disc loss: 0.014719186272366945, policy loss: 5.505530229116173
Experience 17, Iter 16, disc loss: 0.014620666066296962, policy loss: 5.34168811253167
Experience 17, Iter 17, disc loss: 0.013420240492387873, policy loss: 6.31114947596303
Experience 17, Iter 18, disc loss: 0.01435828333810843, policy loss: 5.706029082924256
Experience 17, Iter 19, disc loss: 0.013231401882907577, policy loss: 5.994689511126264
Experience 17, Iter 20, disc loss: 0.014621176308975196, policy loss: 5.298704059737803
Experience 17, Iter 21, disc loss: 0.014442228779581588, policy loss: 5.749017150984438
Experience 17, Iter 22, disc loss: 0.01291397982461339, policy loss: 6.071318444200569
Experience 17, Iter 23, disc loss: 0.01307749482704394, policy loss: 5.581162815879418
Experience 17, Iter 24, disc loss: 0.013266598680267635, policy loss: 5.700050029639158
Experience 17, Iter 25, disc loss: 0.013561448095325519, policy loss: 5.432268862462131
Experience 17, Iter 26, disc loss: 0.01348808306206923, policy loss: 5.750791008232012
Experience 17, Iter 27, disc loss: 0.012842984258483002, policy loss: 5.97556257856287
Experience 17, Iter 28, disc loss: 0.012769832806859904, policy loss: 5.825376257367391
Experience 17, Iter 29, disc loss: 0.013643717647919022, policy loss: 6.148830611507828
Experience 17, Iter 30, disc loss: 0.012794735669103881, policy loss: 5.382158933449373
Experience 17, Iter 31, disc loss: 0.01370953932285205, policy loss: 5.50178940424743
Experience 17, Iter 32, disc loss: 0.013510357257102028, policy loss: 5.615515028098338
Experience 17, Iter 33, disc loss: 0.013071499376748064, policy loss: 5.32416725479014
Experience 17, Iter 34, disc loss: 0.01191391482500348, policy loss: 5.697933090706846
Experience 17, Iter 35, disc loss: 0.012474195574944173, policy loss: 5.406047354983027
Experience 17, Iter 36, disc loss: 0.012383133388118776, policy loss: 5.371174318811509
Experience 17, Iter 37, disc loss: 0.01297026693255129, policy loss: 5.45572787322914
Experience 17, Iter 38, disc loss: 0.012267111043918796, policy loss: 5.5254380533
Experience 17, Iter 39, disc loss: 0.012472631955926045, policy loss: 5.85219788015609
Experience 17, Iter 40, disc loss: 0.011805978303717095, policy loss: 5.7294900469385475
Experience 17, Iter 41, disc loss: 0.01255280831305743, policy loss: 5.448597596137457
Experience 17, Iter 42, disc loss: 0.010917964471027497, policy loss: 5.792300934896771
Experience 17, Iter 43, disc loss: 0.012124356638154046, policy loss: 5.638257595448398
Experience 17, Iter 44, disc loss: 0.011347671117802275, policy loss: 5.778128351681968
Experience 17, Iter 45, disc loss: 0.011585904450660996, policy loss: 5.88442423343633
Experience 17, Iter 46, disc loss: 0.011882407836951998, policy loss: 5.376352909870004
Experience 17, Iter 47, disc loss: 0.01185301534708294, policy loss: 5.607431674675863
Experience 17, Iter 48, disc loss: 0.011676786003830137, policy loss: 5.565104454272444
Experience 17, Iter 49, disc loss: 0.011526294951288912, policy loss: 5.575274255800162
Experience 17, Iter 50, disc loss: 0.011369572286084763, policy loss: 5.505469585698421
Experience 17, Iter 51, disc loss: 0.01099856786630792, policy loss: 5.868109324761823
Experience 17, Iter 52, disc loss: 0.01161708757430541, policy loss: 5.632321238325554
Experience 17, Iter 53, disc loss: 0.01075309855830489, policy loss: 6.626542075589825
Experience 17, Iter 54, disc loss: 0.011252117140625992, policy loss: 5.463008220561839
Experience 17, Iter 55, disc loss: 0.011778322549978086, policy loss: 5.620969254508282
Experience 17, Iter 56, disc loss: 0.011166351395259816, policy loss: 5.7797449545821955
Experience 17, Iter 57, disc loss: 0.011005061119430271, policy loss: 5.56633797970008
Experience 17, Iter 58, disc loss: 0.010225094180668306, policy loss: 5.873863282292098
Experience 17, Iter 59, disc loss: 0.010139457531626966, policy loss: 5.650225369273303
Experience 17, Iter 60, disc loss: 0.010913303242539304, policy loss: 5.952786429335521
Experience 17, Iter 61, disc loss: 0.010604032236155152, policy loss: 5.893652690967279
Experience 17, Iter 62, disc loss: 0.011250478592779155, policy loss: 5.957553271651496
Experience 17, Iter 63, disc loss: 0.011195864087900492, policy loss: 5.429247788157263
Experience 17, Iter 64, disc loss: 0.01030982577242323, policy loss: 6.12141046042332
Experience 17, Iter 65, disc loss: 0.009659512370289015, policy loss: 5.780034059206719
Experience 17, Iter 66, disc loss: 0.010626678383089049, policy loss: 5.94949860351553
Experience 17, Iter 67, disc loss: 0.010392043404996502, policy loss: 6.0564086422957795
Experience 17, Iter 68, disc loss: 0.009381485962480172, policy loss: 6.390077388682924
Experience 17, Iter 69, disc loss: 0.009868240248974161, policy loss: 6.298523938522187
Experience 17, Iter 70, disc loss: 0.00974798545679919, policy loss: 6.010432612249858
Experience 17, Iter 71, disc loss: 0.008084593053704079, policy loss: 6.002857644064967
Experience 17, Iter 72, disc loss: 0.008164858758736026, policy loss: 6.325411677557761
Experience 17, Iter 73, disc loss: 0.008020415612990616, policy loss: 6.990901494196498
Experience 17, Iter 74, disc loss: 0.005938926282022822, policy loss: 7.834494293697489
Experience 17, Iter 75, disc loss: 0.0062966657719473445, policy loss: 6.96755732339139
Experience 17, Iter 76, disc loss: 0.009023825311965739, policy loss: 6.342314312858542
Experience 17, Iter 77, disc loss: 0.00936242386024381, policy loss: 5.62739197156032
Experience 17, Iter 78, disc loss: 0.004570793554288601, policy loss: 10.354658965084564
Experience 17, Iter 79, disc loss: 0.007283586986985101, policy loss: 6.399908396833903
Experience 17, Iter 80, disc loss: 0.004399705306086084, policy loss: 9.283459060092818
Experience 17, Iter 81, disc loss: 0.003691481239881344, policy loss: 11.23001292109248
Experience 17, Iter 82, disc loss: 0.0034173264720387107, policy loss: 12.952707303340596
Experience 17, Iter 83, disc loss: 0.0032120560025234828, policy loss: 12.769357712721458
Experience 17, Iter 84, disc loss: 0.003020879611750396, policy loss: 12.381474724886097
Experience 17, Iter 85, disc loss: 0.002855185091317078, policy loss: 10.942784004058637
Experience 17, Iter 86, disc loss: 0.0027168899026375946, policy loss: 10.692369809212483
Experience 17, Iter 87, disc loss: 0.0027033933661825445, policy loss: 9.883707023672505
Experience 17, Iter 88, disc loss: 0.003073788842545071, policy loss: 7.990707307995545
Experience 17, Iter 89, disc loss: 0.00424302400317598, policy loss: 6.934207225461405
Experience 17, Iter 90, disc loss: 0.004997925306366313, policy loss: 6.164055311875081
Experience 17, Iter 91, disc loss: 0.0035957761868952286, policy loss: 7.097696572145341
Experience 17, Iter 92, disc loss: 0.004688253812406989, policy loss: 6.647068554335385
Experience 17, Iter 93, disc loss: 0.005037944163292779, policy loss: 6.850503132928832
Experience 17, Iter 94, disc loss: 0.006798014729664637, policy loss: 5.791918758528297
Experience 17, Iter 95, disc loss: 0.010258952843367817, policy loss: 5.0616110313771845
Experience 17, Iter 96, disc loss: 0.009175545798449591, policy loss: 5.330355357448685
Experience 17, Iter 97, disc loss: 0.010022484157182815, policy loss: 5.298413850545636
Experience 17, Iter 98, disc loss: 0.010675165412435381, policy loss: 5.268074952370151
Experience 17, Iter 99, disc loss: 0.008341009872055244, policy loss: 5.970981281205599
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1633],
        [1.5307],
        [0.0220]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0081, 0.1390, 1.0805, 0.0173, 0.0188, 3.6294]],

        [[0.0081, 0.1390, 1.0805, 0.0173, 0.0188, 3.6294]],

        [[0.0081, 0.1390, 1.0805, 0.0173, 0.0188, 3.6294]],

        [[0.0081, 0.1390, 1.0805, 0.0173, 0.0188, 3.6294]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0132, 0.6534, 6.1228, 0.0878], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0132, 0.6534, 6.1228, 0.0878])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.103
Iter 2/2000 - Loss: 3.172
Iter 3/2000 - Loss: 3.004
Iter 4/2000 - Loss: 3.023
Iter 5/2000 - Loss: 3.053
Iter 6/2000 - Loss: 2.979
Iter 7/2000 - Loss: 2.904
Iter 8/2000 - Loss: 2.885
Iter 9/2000 - Loss: 2.882
Iter 10/2000 - Loss: 2.831
Iter 11/2000 - Loss: 2.736
Iter 12/2000 - Loss: 2.637
Iter 13/2000 - Loss: 2.551
Iter 14/2000 - Loss: 2.461
Iter 15/2000 - Loss: 2.341
Iter 16/2000 - Loss: 2.182
Iter 17/2000 - Loss: 1.997
Iter 18/2000 - Loss: 1.797
Iter 19/2000 - Loss: 1.580
Iter 20/2000 - Loss: 1.343
Iter 1981/2000 - Loss: -7.779
Iter 1982/2000 - Loss: -7.779
Iter 1983/2000 - Loss: -7.779
Iter 1984/2000 - Loss: -7.779
Iter 1985/2000 - Loss: -7.779
Iter 1986/2000 - Loss: -7.779
Iter 1987/2000 - Loss: -7.779
Iter 1988/2000 - Loss: -7.779
Iter 1989/2000 - Loss: -7.780
Iter 1990/2000 - Loss: -7.780
Iter 1991/2000 - Loss: -7.780
Iter 1992/2000 - Loss: -7.780
Iter 1993/2000 - Loss: -7.780
Iter 1994/2000 - Loss: -7.780
Iter 1995/2000 - Loss: -7.780
Iter 1996/2000 - Loss: -7.780
Iter 1997/2000 - Loss: -7.780
Iter 1998/2000 - Loss: -7.780
Iter 1999/2000 - Loss: -7.780
Iter 2000/2000 - Loss: -7.780
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.8082,  8.5654, 33.5322,  1.6841, 16.9609, 59.4526]],

        [[16.6804, 30.2440,  9.2004,  1.5060,  1.4137, 28.3820]],

        [[19.2165, 23.6463,  8.3971,  1.0077,  1.0256, 20.9929]],

        [[14.0657, 30.2603, 15.9631,  2.5501,  1.9089, 25.9622]]])
Signal Variance: tensor([ 0.1212,  2.2534, 15.0231,  0.4504])
Estimated target variance: tensor([0.0132, 0.6534, 6.1228, 0.0878])
N: 180
Signal to noise ratio: tensor([19.3313, 81.2898, 98.1348, 44.3501])
Bound on condition number: tensor([  67266.6314, 1189446.5843, 1733480.3646,  354048.9786])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.008842507732336951, policy loss: 5.742918689362907
Experience 18, Iter 1, disc loss: 0.008411540897485463, policy loss: 5.691053793111035
Experience 18, Iter 2, disc loss: 0.009091053275329056, policy loss: 5.941295769000967
Experience 18, Iter 3, disc loss: 0.009419820210366874, policy loss: 5.509224746922717
Experience 18, Iter 4, disc loss: 0.007958816753879853, policy loss: 6.344534253824612
Experience 18, Iter 5, disc loss: 0.007373485654513316, policy loss: 6.296937685073112
Experience 18, Iter 6, disc loss: 0.008114829399959252, policy loss: 6.48043152309393
Experience 18, Iter 7, disc loss: 0.00749130387809082, policy loss: 6.317500701462778
Experience 18, Iter 8, disc loss: 0.00751349638495679, policy loss: 6.421483018681924
Experience 18, Iter 9, disc loss: 0.00749478220507262, policy loss: 6.484331948815537
Experience 18, Iter 10, disc loss: 0.007806501403074328, policy loss: 6.553371188622124
Experience 18, Iter 11, disc loss: 0.007477092116290714, policy loss: 6.636580402146897
Experience 18, Iter 12, disc loss: 0.008101573785611348, policy loss: 6.237001221016198
Experience 18, Iter 13, disc loss: 0.007758296211021932, policy loss: 6.674675880699599
Experience 18, Iter 14, disc loss: 0.007990135688773273, policy loss: 6.119196240063925
Experience 18, Iter 15, disc loss: 0.0078038535256940616, policy loss: 6.354370432150878
Experience 18, Iter 16, disc loss: 0.00813848899801165, policy loss: 6.391859033097016
Experience 18, Iter 17, disc loss: 0.008188915170911745, policy loss: 6.610760803796581
Experience 18, Iter 18, disc loss: 0.007829682648879737, policy loss: 6.18949950813
Experience 18, Iter 19, disc loss: 0.007826252069008558, policy loss: 6.863398401905858
Experience 18, Iter 20, disc loss: 0.008269517891448183, policy loss: 5.949147863711703
Experience 18, Iter 21, disc loss: 0.007937637239298169, policy loss: 6.036520943802634
Experience 18, Iter 22, disc loss: 0.008301057252144986, policy loss: 5.983897679973052
Experience 18, Iter 23, disc loss: 0.008507761826786569, policy loss: 5.936284851521511
Experience 18, Iter 24, disc loss: 0.00825154628821468, policy loss: 5.9952607482076115
Experience 18, Iter 25, disc loss: 0.007843831073161708, policy loss: 6.28002028562827
Experience 18, Iter 26, disc loss: 0.00840383007755904, policy loss: 5.94384266907409
Experience 18, Iter 27, disc loss: 0.0074232964499866124, policy loss: 6.259595419602778
Experience 18, Iter 28, disc loss: 0.0069445488544414204, policy loss: 6.470994003112476
Experience 18, Iter 29, disc loss: 0.006857864054402901, policy loss: 6.218912100758324
Experience 18, Iter 30, disc loss: 0.008024908322573684, policy loss: 5.8091078260117595
Experience 18, Iter 31, disc loss: 0.0065050817541328874, policy loss: 6.720375825617001
Experience 18, Iter 32, disc loss: 0.006860355921731735, policy loss: 6.161615173240955
Experience 18, Iter 33, disc loss: 0.0077714530713214785, policy loss: 5.870570813163392
Experience 18, Iter 34, disc loss: 0.007898564193131851, policy loss: 5.862820635385708
Experience 18, Iter 35, disc loss: 0.008281653949427498, policy loss: 6.053348767596889
Experience 18, Iter 36, disc loss: 0.006583954757092253, policy loss: 6.6709655102295935
Experience 18, Iter 37, disc loss: 0.007644830359032705, policy loss: 6.048242493831264
Experience 18, Iter 38, disc loss: 0.007594229921052688, policy loss: 5.960332585046421
Experience 18, Iter 39, disc loss: 0.0069720001556610265, policy loss: 6.447227441137303
Experience 18, Iter 40, disc loss: 0.007390712473282456, policy loss: 6.62665342283066
Experience 18, Iter 41, disc loss: 0.007568844428167854, policy loss: 5.905165228829971
Experience 18, Iter 42, disc loss: 0.008522170272570207, policy loss: 5.87259445532125
Experience 18, Iter 43, disc loss: 0.007340790143151161, policy loss: 6.612046700212044
Experience 18, Iter 44, disc loss: 0.006961013899740593, policy loss: 6.443386585485495
Experience 18, Iter 45, disc loss: 0.007138946058933546, policy loss: 6.365757136072732
Experience 18, Iter 46, disc loss: 0.006762104202796078, policy loss: 6.5828369494211225
Experience 18, Iter 47, disc loss: 0.007943016673371729, policy loss: 5.947030900970668
Experience 18, Iter 48, disc loss: 0.007186753738309834, policy loss: 6.197598039783532
Experience 18, Iter 49, disc loss: 0.006968193458031867, policy loss: 6.075631944506247
Experience 18, Iter 50, disc loss: 0.006870691148128363, policy loss: 6.56936999414427
Experience 18, Iter 51, disc loss: 0.007206463503805939, policy loss: 6.10847045493269
Experience 18, Iter 52, disc loss: 0.007029357206345472, policy loss: 5.935281764573739
Experience 18, Iter 53, disc loss: 0.006910345489022867, policy loss: 6.678689900572496
Experience 18, Iter 54, disc loss: 0.007636997522551056, policy loss: 5.935214347752915
Experience 18, Iter 55, disc loss: 0.007682423447109838, policy loss: 7.17111703963932
Experience 18, Iter 56, disc loss: 0.006629359002845536, policy loss: 6.382860255939853
Experience 18, Iter 57, disc loss: 0.0072600798382596345, policy loss: 6.075924702709292
Experience 18, Iter 58, disc loss: 0.0069340628915222175, policy loss: 6.47061124087913
Experience 18, Iter 59, disc loss: 0.006876997110630694, policy loss: 6.098603631753613
Experience 18, Iter 60, disc loss: 0.006821734011822541, policy loss: 6.254827738913397
Experience 18, Iter 61, disc loss: 0.006689433865746471, policy loss: 6.345286740143679
Experience 18, Iter 62, disc loss: 0.006560249895695115, policy loss: 6.164356612821383
Experience 18, Iter 63, disc loss: 0.006232642868809378, policy loss: 6.361442564529384
Experience 18, Iter 64, disc loss: 0.005973110086425187, policy loss: 6.601476137689733
Experience 18, Iter 65, disc loss: 0.005894894183865031, policy loss: 7.533628946917286
Experience 18, Iter 66, disc loss: 0.006098217506160122, policy loss: 6.324959366801901
Experience 18, Iter 67, disc loss: 0.005830053774084031, policy loss: 6.154146246720133
Experience 18, Iter 68, disc loss: 0.00672319180833991, policy loss: 6.008627522493084
Experience 18, Iter 69, disc loss: 0.006319488753872015, policy loss: 6.48540119551477
Experience 18, Iter 70, disc loss: 0.00695336330214118, policy loss: 6.263197549028766
Experience 18, Iter 71, disc loss: 0.007645946431391809, policy loss: 6.069725849856422
Experience 18, Iter 72, disc loss: 0.006203325602318101, policy loss: 6.49847365021219
Experience 18, Iter 73, disc loss: 0.006683570608330781, policy loss: 6.323101142167511
Experience 18, Iter 74, disc loss: 0.006104803446929977, policy loss: 6.548291615964999
Experience 18, Iter 75, disc loss: 0.006297441726772917, policy loss: 6.062942808137729
Experience 18, Iter 76, disc loss: 0.005623269356794267, policy loss: 6.437347210554179
Experience 18, Iter 77, disc loss: 0.0057536581651536035, policy loss: 6.629056167959313
Experience 18, Iter 78, disc loss: 0.006139241744094591, policy loss: 6.521757157088097
Experience 18, Iter 79, disc loss: 0.0061107910126631795, policy loss: 6.345935427106219
Experience 18, Iter 80, disc loss: 0.006305600525763913, policy loss: 6.477138127022571
Experience 18, Iter 81, disc loss: 0.006724533312286622, policy loss: 6.224206914955874
Experience 18, Iter 82, disc loss: 0.005906309420904532, policy loss: 6.5628830824225854
Experience 18, Iter 83, disc loss: 0.005822172372394447, policy loss: 6.3106963436522605
Experience 18, Iter 84, disc loss: 0.0055367224376029565, policy loss: 6.374092334497707
Experience 18, Iter 85, disc loss: 0.005586960140652917, policy loss: 6.226128326768858
Experience 18, Iter 86, disc loss: 0.00606052619671932, policy loss: 6.168302134333733
Experience 18, Iter 87, disc loss: 0.005805859222025023, policy loss: 6.730851646272397
Experience 18, Iter 88, disc loss: 0.006180510641532967, policy loss: 6.202739088615393
Experience 18, Iter 89, disc loss: 0.006054797701828695, policy loss: 6.781236562458687
Experience 18, Iter 90, disc loss: 0.005902720506202767, policy loss: 6.40945375921265
Experience 18, Iter 91, disc loss: 0.006312907499445774, policy loss: 6.192509673231083
Experience 18, Iter 92, disc loss: 0.005922312038884795, policy loss: 6.468025619839716
Experience 18, Iter 93, disc loss: 0.00682225648223944, policy loss: 6.0695875032468525
Experience 18, Iter 94, disc loss: 0.006269282249674558, policy loss: 7.1257067758837245
Experience 18, Iter 95, disc loss: 0.006809865377914732, policy loss: 6.04225833557568
Experience 18, Iter 96, disc loss: 0.005973257037093189, policy loss: 6.318746273186457
Experience 18, Iter 97, disc loss: 0.005756368397474503, policy loss: 6.30138915766895
Experience 18, Iter 98, disc loss: 0.00617638944373983, policy loss: 5.990703278676919
Experience 18, Iter 99, disc loss: 0.00555959200291966, policy loss: 6.512867263405056
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1692],
        [1.5586],
        [0.0231]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0082, 0.1399, 1.1304, 0.0183, 0.0208, 3.8178]],

        [[0.0082, 0.1399, 1.1304, 0.0183, 0.0208, 3.8178]],

        [[0.0082, 0.1399, 1.1304, 0.0183, 0.0208, 3.8178]],

        [[0.0082, 0.1399, 1.1304, 0.0183, 0.0208, 3.8178]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0132, 0.6768, 6.2343, 0.0923], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0132, 0.6768, 6.2343, 0.0923])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.162
Iter 2/2000 - Loss: 3.229
Iter 3/2000 - Loss: 3.050
Iter 4/2000 - Loss: 3.076
Iter 5/2000 - Loss: 3.111
Iter 6/2000 - Loss: 3.033
Iter 7/2000 - Loss: 2.949
Iter 8/2000 - Loss: 2.927
Iter 9/2000 - Loss: 2.926
Iter 10/2000 - Loss: 2.879
Iter 11/2000 - Loss: 2.784
Iter 12/2000 - Loss: 2.680
Iter 13/2000 - Loss: 2.588
Iter 14/2000 - Loss: 2.493
Iter 15/2000 - Loss: 2.368
Iter 16/2000 - Loss: 2.207
Iter 17/2000 - Loss: 2.016
Iter 18/2000 - Loss: 1.807
Iter 19/2000 - Loss: 1.580
Iter 20/2000 - Loss: 1.330
Iter 1981/2000 - Loss: -7.874
Iter 1982/2000 - Loss: -7.874
Iter 1983/2000 - Loss: -7.874
Iter 1984/2000 - Loss: -7.874
Iter 1985/2000 - Loss: -7.874
Iter 1986/2000 - Loss: -7.874
Iter 1987/2000 - Loss: -7.874
Iter 1988/2000 - Loss: -7.874
Iter 1989/2000 - Loss: -7.874
Iter 1990/2000 - Loss: -7.875
Iter 1991/2000 - Loss: -7.875
Iter 1992/2000 - Loss: -7.875
Iter 1993/2000 - Loss: -7.875
Iter 1994/2000 - Loss: -7.875
Iter 1995/2000 - Loss: -7.875
Iter 1996/2000 - Loss: -7.875
Iter 1997/2000 - Loss: -7.875
Iter 1998/2000 - Loss: -7.875
Iter 1999/2000 - Loss: -7.875
Iter 2000/2000 - Loss: -7.875
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.2663,  8.5494, 32.2628,  1.6383, 17.1619, 60.1275]],

        [[17.0110, 30.8979,  9.1360,  1.4202,  1.5977, 28.7539]],

        [[18.0195, 25.5713,  8.3009,  0.9863,  0.9804, 19.7268]],

        [[13.8281, 29.0641, 15.7692,  2.3620,  1.8911, 27.6007]]])
Signal Variance: tensor([ 0.1195,  2.2432, 13.0768,  0.4266])
Estimated target variance: tensor([0.0132, 0.6768, 6.2343, 0.0923])
N: 190
Signal to noise ratio: tensor([19.6658, 81.6032, 90.7237, 43.3271])
Bound on condition number: tensor([  73482.2300, 1265225.6471, 1563852.2581,  356675.5915])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.006150325537936074, policy loss: 6.796878204268894
Experience 19, Iter 1, disc loss: 0.005937313481645532, policy loss: 6.482645160471915
Experience 19, Iter 2, disc loss: 0.006070742668520096, policy loss: 6.249552650004
Experience 19, Iter 3, disc loss: 0.005672752751544954, policy loss: 6.841951500628931
Experience 19, Iter 4, disc loss: 0.005652603731531257, policy loss: 6.582499015258415
Experience 19, Iter 5, disc loss: 0.005866268207531394, policy loss: 6.591127934287748
Experience 19, Iter 6, disc loss: 0.005475396800948028, policy loss: 6.498558030046852
Experience 19, Iter 7, disc loss: 0.005928047648579566, policy loss: 6.136396963791001
Experience 19, Iter 8, disc loss: 0.005904760690586071, policy loss: 6.1925525736007145
Experience 19, Iter 9, disc loss: 0.005854362243509315, policy loss: 6.282802581729785
Experience 19, Iter 10, disc loss: 0.0059490373491472084, policy loss: 6.284970459221726
Experience 19, Iter 11, disc loss: 0.00562626415670768, policy loss: 6.943345126904687
Experience 19, Iter 12, disc loss: 0.00601002472280318, policy loss: 6.096377207780649
Experience 19, Iter 13, disc loss: 0.005804252237693064, policy loss: 6.164954621596567
Experience 19, Iter 14, disc loss: 0.006131442703633784, policy loss: 6.527832375889496
Experience 19, Iter 15, disc loss: 0.005663005724388957, policy loss: 6.486435087947026
Experience 19, Iter 16, disc loss: 0.005652119038406564, policy loss: 6.6778707753460465
Experience 19, Iter 17, disc loss: 0.006025367353036271, policy loss: 6.2196972760510025
Experience 19, Iter 18, disc loss: 0.00542896847054497, policy loss: 6.694407122283613
Experience 19, Iter 19, disc loss: 0.005229936727336582, policy loss: 6.335822469339727
Experience 19, Iter 20, disc loss: 0.0052646377167094305, policy loss: 6.242728355675387
Experience 19, Iter 21, disc loss: 0.005394360150105842, policy loss: 6.242566280212843
Experience 19, Iter 22, disc loss: 0.00571710512051429, policy loss: 6.53261455842892
Experience 19, Iter 23, disc loss: 0.005605828352067119, policy loss: 6.083123923749579
Experience 19, Iter 24, disc loss: 0.005695777759441149, policy loss: 6.4153496239914585
Experience 19, Iter 25, disc loss: 0.006022929936223366, policy loss: 6.202910703873884
Experience 19, Iter 26, disc loss: 0.0058220866828664185, policy loss: 6.027069487133213
Experience 19, Iter 27, disc loss: 0.004952420338406865, policy loss: 6.576791896219524
Experience 19, Iter 28, disc loss: 0.0049028699573811455, policy loss: 6.4868610028327565
Experience 19, Iter 29, disc loss: 0.00499589173702745, policy loss: 6.439567515813778
Experience 19, Iter 30, disc loss: 0.00581494527825766, policy loss: 6.533840645655945
Experience 19, Iter 31, disc loss: 0.005580263562014036, policy loss: 6.156210015252254
Experience 19, Iter 32, disc loss: 0.0053801740508331725, policy loss: 6.515277236971425
Experience 19, Iter 33, disc loss: 0.005540078019524369, policy loss: 6.459329906403155
Experience 19, Iter 34, disc loss: 0.005930921863195223, policy loss: 6.234458358982684
Experience 19, Iter 35, disc loss: 0.0051618901661609545, policy loss: 6.608878644239799
Experience 19, Iter 36, disc loss: 0.005479163459777738, policy loss: 6.151676902039437
Experience 19, Iter 37, disc loss: 0.005830121028548999, policy loss: 6.429669117737949
Experience 19, Iter 38, disc loss: 0.0053385921994368755, policy loss: 6.684448825047623
Experience 19, Iter 39, disc loss: 0.005229364317197581, policy loss: 6.319418712735613
Experience 19, Iter 40, disc loss: 0.004782657704966007, policy loss: 6.3898292157460865
Experience 19, Iter 41, disc loss: 0.004660412419332228, policy loss: 6.531383277041499
Experience 19, Iter 42, disc loss: 0.004994032884497738, policy loss: 6.46582028326501
Experience 19, Iter 43, disc loss: 0.005082793384317678, policy loss: 6.602303645821875
Experience 19, Iter 44, disc loss: 0.005031988361923027, policy loss: 6.505004254444536
Experience 19, Iter 45, disc loss: 0.004951428638593392, policy loss: 6.3917812689264055
Experience 19, Iter 46, disc loss: 0.00493674654569695, policy loss: 6.922447448806838
Experience 19, Iter 47, disc loss: 0.005196307114813335, policy loss: 6.688092954441119
Experience 19, Iter 48, disc loss: 0.0050225709115592244, policy loss: 6.408548036282248
Experience 19, Iter 49, disc loss: 0.005013124900851455, policy loss: 6.328127797635373
Experience 19, Iter 50, disc loss: 0.0044222276759284885, policy loss: 7.232597556386182
Experience 19, Iter 51, disc loss: 0.004940134834508139, policy loss: 6.697306739769308
Experience 19, Iter 52, disc loss: 0.005157537414352439, policy loss: 6.597090837442847
Experience 19, Iter 53, disc loss: 0.004866960844452713, policy loss: 6.486808874445147
Experience 19, Iter 54, disc loss: 0.005174315879451937, policy loss: 6.06640139852533
Experience 19, Iter 55, disc loss: 0.005039895907681563, policy loss: 6.65466899562918
Experience 19, Iter 56, disc loss: 0.0049333263532684475, policy loss: 6.310402614687616
Experience 19, Iter 57, disc loss: 0.004958576863809334, policy loss: 6.501436077894562
Experience 19, Iter 58, disc loss: 0.005035137079500838, policy loss: 6.606303801377699
Experience 19, Iter 59, disc loss: 0.004907652017926433, policy loss: 6.60560224246092
Experience 19, Iter 60, disc loss: 0.005203312345406239, policy loss: 6.530667017803415
Experience 19, Iter 61, disc loss: 0.004982390099721922, policy loss: 6.51934791883481
Experience 19, Iter 62, disc loss: 0.004635004351563852, policy loss: 6.869883822681952
Experience 19, Iter 63, disc loss: 0.004688589125125478, policy loss: 7.059283647922251
Experience 19, Iter 64, disc loss: 0.004250775468619519, policy loss: 6.969648492562545
Experience 19, Iter 65, disc loss: 0.004681007787261094, policy loss: 6.557545671436801
Experience 19, Iter 66, disc loss: 0.00476556799885464, policy loss: 6.541858353023683
Experience 19, Iter 67, disc loss: 0.0049120948773298785, policy loss: 6.21692473827548
Experience 19, Iter 68, disc loss: 0.004913073409551701, policy loss: 6.314674186400797
Experience 19, Iter 69, disc loss: 0.0046992310593714, policy loss: 6.777116786304333
Experience 19, Iter 70, disc loss: 0.005202496898994471, policy loss: 6.140227254970776
Experience 19, Iter 71, disc loss: 0.005086210361250833, policy loss: 6.356787726732629
Experience 19, Iter 72, disc loss: 0.0044797909858819256, policy loss: 6.862986459063993
Experience 19, Iter 73, disc loss: 0.004765133365079082, policy loss: 6.2993075157124725
Experience 19, Iter 74, disc loss: 0.004450217220194502, policy loss: 6.876094470577776
Experience 19, Iter 75, disc loss: 0.004354985824086799, policy loss: 6.609899241650334
Experience 19, Iter 76, disc loss: 0.004272903199131699, policy loss: 6.651113469092726
Experience 19, Iter 77, disc loss: 0.0046648372528652685, policy loss: 6.303223333688134
Experience 19, Iter 78, disc loss: 0.004624045970161463, policy loss: 6.853380956634107
Experience 19, Iter 79, disc loss: 0.00462136165049137, policy loss: 6.537819575709895
Experience 19, Iter 80, disc loss: 0.004713251678164646, policy loss: 6.311353044413669
Experience 19, Iter 81, disc loss: 0.004604803267279222, policy loss: 6.585582052747984
Experience 19, Iter 82, disc loss: 0.004779359627720829, policy loss: 6.363321811134287
Experience 19, Iter 83, disc loss: 0.004731390782065713, policy loss: 6.409383249195264
Experience 19, Iter 84, disc loss: 0.004467141483447359, policy loss: 6.584114439708982
Experience 19, Iter 85, disc loss: 0.004412975298156227, policy loss: 6.474163508147507
Experience 19, Iter 86, disc loss: 0.004533299152574622, policy loss: 6.494495754637818
Experience 19, Iter 87, disc loss: 0.004260599783399089, policy loss: 6.973662273620437
Experience 19, Iter 88, disc loss: 0.004549001123045168, policy loss: 6.351906222856432
Experience 19, Iter 89, disc loss: 0.004437185441799481, policy loss: 6.579217754184202
Experience 19, Iter 90, disc loss: 0.004332035950724558, policy loss: 6.661547188762846
Experience 19, Iter 91, disc loss: 0.0045543318138284465, policy loss: 6.700884777249934
Experience 19, Iter 92, disc loss: 0.004254856269391906, policy loss: 6.624376573001003
Experience 19, Iter 93, disc loss: 0.004146461311564659, policy loss: 7.62140354861098
Experience 19, Iter 94, disc loss: 0.0042137078471818626, policy loss: 7.149973047404946
Experience 19, Iter 95, disc loss: 0.004710510445018733, policy loss: 6.153426825236567
Experience 19, Iter 96, disc loss: 0.004111128086267235, policy loss: 6.6738144739915075
Experience 19, Iter 97, disc loss: 0.0039002296332166296, policy loss: 7.557018430934384
Experience 19, Iter 98, disc loss: 0.0039532642280165455, policy loss: 7.08871799739803
Experience 19, Iter 99, disc loss: 0.004061676600250308, policy loss: 6.637100274157195
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.1782],
        [1.6110],
        [0.0247]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0080, 0.1422, 1.2073, 0.0192, 0.0217, 4.0097]],

        [[0.0080, 0.1422, 1.2073, 0.0192, 0.0217, 4.0097]],

        [[0.0080, 0.1422, 1.2073, 0.0192, 0.0217, 4.0097]],

        [[0.0080, 0.1422, 1.2073, 0.0192, 0.0217, 4.0097]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0134, 0.7129, 6.4442, 0.0989], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0134, 0.7129, 6.4442, 0.0989])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.245
Iter 2/2000 - Loss: 3.311
Iter 3/2000 - Loss: 3.125
Iter 4/2000 - Loss: 3.153
Iter 5/2000 - Loss: 3.191
Iter 6/2000 - Loss: 3.111
Iter 7/2000 - Loss: 3.023
Iter 8/2000 - Loss: 2.996
Iter 9/2000 - Loss: 2.993
Iter 10/2000 - Loss: 2.948
Iter 11/2000 - Loss: 2.853
Iter 12/2000 - Loss: 2.746
Iter 13/2000 - Loss: 2.649
Iter 14/2000 - Loss: 2.546
Iter 15/2000 - Loss: 2.414
Iter 16/2000 - Loss: 2.243
Iter 17/2000 - Loss: 2.044
Iter 18/2000 - Loss: 1.823
Iter 19/2000 - Loss: 1.583
Iter 20/2000 - Loss: 1.317
Iter 1981/2000 - Loss: -7.847
Iter 1982/2000 - Loss: -7.847
Iter 1983/2000 - Loss: -7.847
Iter 1984/2000 - Loss: -7.847
Iter 1985/2000 - Loss: -7.847
Iter 1986/2000 - Loss: -7.847
Iter 1987/2000 - Loss: -7.847
Iter 1988/2000 - Loss: -7.848
Iter 1989/2000 - Loss: -7.848
Iter 1990/2000 - Loss: -7.848
Iter 1991/2000 - Loss: -7.848
Iter 1992/2000 - Loss: -7.848
Iter 1993/2000 - Loss: -7.848
Iter 1994/2000 - Loss: -7.848
Iter 1995/2000 - Loss: -7.848
Iter 1996/2000 - Loss: -7.848
Iter 1997/2000 - Loss: -7.848
Iter 1998/2000 - Loss: -7.848
Iter 1999/2000 - Loss: -7.848
Iter 2000/2000 - Loss: -7.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[10.8043,  9.1902, 31.3389,  3.0358,  5.5731, 59.1595]],

        [[16.0380, 31.3676,  8.8013,  1.4947,  1.5701, 27.5522]],

        [[17.9278, 29.2963,  7.7899,  0.9608,  0.9665, 19.6073]],

        [[13.4415, 28.8668, 15.9700,  2.2308,  1.9074, 29.3026]]])
Signal Variance: tensor([ 0.1377,  2.1320, 11.9868,  0.4453])
Estimated target variance: tensor([0.0134, 0.7129, 6.4442, 0.0989])
N: 200
Signal to noise ratio: tensor([20.7109, 78.3577, 82.1592, 43.9495])
Bound on condition number: tensor([  85789.3960, 1227987.5897, 1350026.2788,  386312.1320])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0044962766044255595, policy loss: 7.028412617868021
Experience 20, Iter 1, disc loss: 0.004166601604801752, policy loss: 6.634411770631505
Experience 20, Iter 2, disc loss: 0.004045483162849179, policy loss: 6.844571107002349
Experience 20, Iter 3, disc loss: 0.003965582988448407, policy loss: 6.909127932798917
Experience 20, Iter 4, disc loss: 0.0034547418861662097, policy loss: 7.303949718412933
Experience 20, Iter 5, disc loss: 0.0035756057027152583, policy loss: 6.848708044596432
Experience 20, Iter 6, disc loss: 0.003865336933730987, policy loss: 6.638653592572064
Experience 20, Iter 7, disc loss: 0.0045205774498444234, policy loss: 6.33583605444352
Experience 20, Iter 8, disc loss: 0.004586539621351117, policy loss: 6.413466818819233
Experience 20, Iter 9, disc loss: 0.004073619593673269, policy loss: 6.672790018456636
Experience 20, Iter 10, disc loss: 0.004239176721154084, policy loss: 6.877785586315526
Experience 20, Iter 11, disc loss: 0.003932531021216711, policy loss: 6.614206067427644
Experience 20, Iter 12, disc loss: 0.004218754280562665, policy loss: 6.788380534579111
Experience 20, Iter 13, disc loss: 0.00411926194275274, policy loss: 6.93447057427077
Experience 20, Iter 14, disc loss: 0.004469314266104791, policy loss: 6.483101291559814
Experience 20, Iter 15, disc loss: 0.003973818783717808, policy loss: 6.623950055582145
Experience 20, Iter 16, disc loss: 0.00438783479184981, policy loss: 6.322558565152825
Experience 20, Iter 17, disc loss: 0.0042173165785034565, policy loss: 6.5016706131230775
Experience 20, Iter 18, disc loss: 0.004153063052673, policy loss: 6.698910976327493
Experience 20, Iter 19, disc loss: 0.004275761085722737, policy loss: 6.765848681044523
Experience 20, Iter 20, disc loss: 0.004463336261207659, policy loss: 6.56337807350145
Experience 20, Iter 21, disc loss: 0.0042362673894173145, policy loss: 6.603009523910993
Experience 20, Iter 22, disc loss: 0.004069372146255594, policy loss: 6.5414217393319
Experience 20, Iter 23, disc loss: 0.004300279747717579, policy loss: 6.41411178024429
Experience 20, Iter 24, disc loss: 0.004234792312792738, policy loss: 6.596765364367866
Experience 20, Iter 25, disc loss: 0.004818483778023184, policy loss: 6.183236646980955
Experience 20, Iter 26, disc loss: 0.004535701000331975, policy loss: 6.702364600624767
Experience 20, Iter 27, disc loss: 0.003995415112869686, policy loss: 6.472725153295479
Experience 20, Iter 28, disc loss: 0.004468841208116526, policy loss: 6.638148035424093
Experience 20, Iter 29, disc loss: 0.004245191225958495, policy loss: 6.6374905442070204
Experience 20, Iter 30, disc loss: 0.004272324612620842, policy loss: 6.977798795253119
Experience 20, Iter 31, disc loss: 0.004049728848897047, policy loss: 6.820944171419167
Experience 20, Iter 32, disc loss: 0.004143471691732651, policy loss: 6.47541895306499
Experience 20, Iter 33, disc loss: 0.003940057515051718, policy loss: 6.746446913691415
Experience 20, Iter 34, disc loss: 0.003928710293838158, policy loss: 7.081872877167991
Experience 20, Iter 35, disc loss: 0.0042500879813162674, policy loss: 6.577415710250196
Experience 20, Iter 36, disc loss: 0.004014737206787246, policy loss: 6.772565632565588
Experience 20, Iter 37, disc loss: 0.0037983133099141328, policy loss: 7.771361466362057
Experience 20, Iter 38, disc loss: 0.003907765456106915, policy loss: 6.831746808188069
Experience 20, Iter 39, disc loss: 0.0037537375562473703, policy loss: 6.769699430222458
Experience 20, Iter 40, disc loss: 0.004216236024387671, policy loss: 6.705347675046606
Experience 20, Iter 41, disc loss: 0.004250350353923617, policy loss: 6.507279595310262
Experience 20, Iter 42, disc loss: 0.003989322265300139, policy loss: 7.729428325172507
Experience 20, Iter 43, disc loss: 0.004214238699605638, policy loss: 6.783753378199682
Experience 20, Iter 44, disc loss: 0.0038262268382511884, policy loss: 7.156140690876867
Experience 20, Iter 45, disc loss: 0.0038107336318085184, policy loss: 6.778779611777301
Experience 20, Iter 46, disc loss: 0.0040452291141800795, policy loss: 6.99664284616553
Experience 20, Iter 47, disc loss: 0.003675821540055332, policy loss: 7.091622949059026
Experience 20, Iter 48, disc loss: 0.0034864169878486904, policy loss: 7.040027071045111
Experience 20, Iter 49, disc loss: 0.003668768045794585, policy loss: 7.027051889427511
Experience 20, Iter 50, disc loss: 0.0037940241791228797, policy loss: 6.714950566971415
Experience 20, Iter 51, disc loss: 0.0037097904736269243, policy loss: 7.666639386158924
Experience 20, Iter 52, disc loss: 0.003826447287766886, policy loss: 7.12692098901295
Experience 20, Iter 53, disc loss: 0.003617797443138848, policy loss: 7.025016438966471
Experience 20, Iter 54, disc loss: 0.0042311748800796965, policy loss: 6.624605826997241
Experience 20, Iter 55, disc loss: 0.0038396297820510834, policy loss: 6.632147563256144
Experience 20, Iter 56, disc loss: 0.0039056691486920926, policy loss: 6.667506175758704
Experience 20, Iter 57, disc loss: 0.003837506859381079, policy loss: 6.570661598506255
Experience 20, Iter 58, disc loss: 0.0038219893456904177, policy loss: 7.097365867522689
Experience 20, Iter 59, disc loss: 0.0036246885793471033, policy loss: 6.925339851827154
Experience 20, Iter 60, disc loss: 0.00418690348523618, policy loss: 6.457214681661162
Experience 20, Iter 61, disc loss: 0.003905104848720738, policy loss: 6.35603917886634
Experience 20, Iter 62, disc loss: 0.0036423106590425776, policy loss: 6.85556120662705
Experience 20, Iter 63, disc loss: 0.0036560089631834103, policy loss: 6.602827380896401
Experience 20, Iter 64, disc loss: 0.004004749900257346, policy loss: 6.931142074985167
Experience 20, Iter 65, disc loss: 0.0035010324393727294, policy loss: 6.993138227707179
Experience 20, Iter 66, disc loss: 0.0036151254515090553, policy loss: 6.9922309477962115
Experience 20, Iter 67, disc loss: 0.0038415839249774522, policy loss: 6.978428148570536
Experience 20, Iter 68, disc loss: 0.0035660212439429777, policy loss: 7.156829980148933
Experience 20, Iter 69, disc loss: 0.00396695919717315, policy loss: 6.7786602284898905
Experience 20, Iter 70, disc loss: 0.003453128964976975, policy loss: 7.186070200630972
Experience 20, Iter 71, disc loss: 0.003778504996372213, policy loss: 6.919184848566666
Experience 20, Iter 72, disc loss: 0.0035509217877561826, policy loss: 7.284430345588509
Experience 20, Iter 73, disc loss: 0.0038007907857184157, policy loss: 6.955833101437027
Experience 20, Iter 74, disc loss: 0.0037884073811590437, policy loss: 7.378946521061584
Experience 20, Iter 75, disc loss: 0.003455018001210226, policy loss: 7.013703258488205
Experience 20, Iter 76, disc loss: 0.003623778632403223, policy loss: 7.0269740450844385
Experience 20, Iter 77, disc loss: 0.0033957308559352875, policy loss: 7.724119893526671
Experience 20, Iter 78, disc loss: 0.0035242757853108826, policy loss: 6.993255409842394
Experience 20, Iter 79, disc loss: 0.003857827203292931, policy loss: 6.659489403683661
Experience 20, Iter 80, disc loss: 0.003499063309837969, policy loss: 6.921710327251323
Experience 20, Iter 81, disc loss: 0.00329169288050096, policy loss: 7.276262866914133
Experience 20, Iter 82, disc loss: 0.003909351989533838, policy loss: 6.401049504503924
Experience 20, Iter 83, disc loss: 0.0034981868882478775, policy loss: 6.850759684833427
Experience 20, Iter 84, disc loss: 0.0033104365567230287, policy loss: 7.217446886966888
Experience 20, Iter 85, disc loss: 0.00317069457337353, policy loss: 6.969072329889707
Experience 20, Iter 86, disc loss: 0.003461896158671282, policy loss: 6.800460123346574
Experience 20, Iter 87, disc loss: 0.0032944541612302185, policy loss: 7.226639678798304
Experience 20, Iter 88, disc loss: 0.0037406926268274757, policy loss: 6.674861732731392
Experience 20, Iter 89, disc loss: 0.003398484356651383, policy loss: 6.817415030774212
Experience 20, Iter 90, disc loss: 0.0034565795832294365, policy loss: 6.9220895034561005
Experience 20, Iter 91, disc loss: 0.003555537685181048, policy loss: 7.173576100828806
Experience 20, Iter 92, disc loss: 0.003277260375252019, policy loss: 6.722387829589138
Experience 20, Iter 93, disc loss: 0.0034090623404272563, policy loss: 6.871220680453996
Experience 20, Iter 94, disc loss: 0.003010957018393226, policy loss: 7.318265677219882
Experience 20, Iter 95, disc loss: 0.0031104017061814996, policy loss: 7.079902798616759
Experience 20, Iter 96, disc loss: 0.003427422704764341, policy loss: 7.089121871488634
Experience 20, Iter 97, disc loss: 0.003321222906025155, policy loss: 7.284207288387333
Experience 20, Iter 98, disc loss: 0.003623715988240435, policy loss: 7.004899953268072
Experience 20, Iter 99, disc loss: 0.003246090345365912, policy loss: 6.796440689269635
