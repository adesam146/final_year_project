Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0024],
        [0.1095],
        [0.0018]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0026, 0.0095, 0.4379, 0.0071], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0026, 0.0095, 0.4379, 0.0071])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.826
Iter 2/2000 - Loss: -1.708
Iter 3/2000 - Loss: -1.989
Iter 4/2000 - Loss: -1.524
Iter 5/2000 - Loss: -1.432
Iter 6/2000 - Loss: -1.719
Iter 7/2000 - Loss: -1.978
Iter 8/2000 - Loss: -2.021
Iter 9/2000 - Loss: -1.941
Iter 10/2000 - Loss: -1.933
Iter 11/2000 - Loss: -2.070
Iter 12/2000 - Loss: -2.250
Iter 13/2000 - Loss: -2.341
Iter 14/2000 - Loss: -2.292
Iter 15/2000 - Loss: -2.173
Iter 16/2000 - Loss: -2.102
Iter 17/2000 - Loss: -2.134
Iter 18/2000 - Loss: -2.233
Iter 19/2000 - Loss: -2.334
Iter 20/2000 - Loss: -2.407
Iter 1981/2000 - Loss: -2.717
Iter 1982/2000 - Loss: -2.717
Iter 1983/2000 - Loss: -2.717
Iter 1984/2000 - Loss: -2.717
Iter 1985/2000 - Loss: -2.717
Iter 1986/2000 - Loss: -2.717
Iter 1987/2000 - Loss: -2.717
Iter 1988/2000 - Loss: -2.717
Iter 1989/2000 - Loss: -2.717
Iter 1990/2000 - Loss: -2.717
Iter 1991/2000 - Loss: -2.717
Iter 1992/2000 - Loss: -2.717
Iter 1993/2000 - Loss: -2.717
Iter 1994/2000 - Loss: -2.717
Iter 1995/2000 - Loss: -2.717
Iter 1996/2000 - Loss: -2.717
Iter 1997/2000 - Loss: -2.717
Iter 1998/2000 - Loss: -2.717
Iter 1999/2000 - Loss: -2.717
Iter 2000/2000 - Loss: -2.717
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0017],
        [0.0728],
        [0.0013]])
Lengthscale: tensor([[[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]],

        [[9.0082e-03, 2.7594e-02, 8.3547e-02, 2.0854e-03, 6.5495e-05,
          1.3424e-02]]])
Signal Variance: tensor([0.0019, 0.0068, 0.3213, 0.0051])
Estimated target variance: tensor([0.0026, 0.0095, 0.4379, 0.0071])
N: 10
Signal to noise ratio: tensor([2.0005, 2.0035, 2.1008, 2.0027])
Bound on condition number: tensor([41.0210, 41.1414, 45.1320, 41.1064])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.275872628341367, policy loss: 0.8042354586044123
Experience 1, Iter 1, disc loss: 1.2583271690909752, policy loss: 0.8045314135911995
Experience 1, Iter 2, disc loss: 1.2404465229032229, policy loss: 0.8060028606232394
Experience 1, Iter 3, disc loss: 1.220161584942908, policy loss: 0.8112993843851561
Experience 1, Iter 4, disc loss: 1.2034286093627085, policy loss: 0.8117273602250822
Experience 1, Iter 5, disc loss: 1.1880732306547097, policy loss: 0.8110911303834482
Experience 1, Iter 6, disc loss: 1.1701403587908046, policy loss: 0.815941102698967
Experience 1, Iter 7, disc loss: 1.1604469460674374, policy loss: 0.8138027287631158
Experience 1, Iter 8, disc loss: 1.1486284236044977, policy loss: 0.816674203646232
Experience 1, Iter 9, disc loss: 1.1311527104813721, policy loss: 0.8251182900199385
Experience 1, Iter 10, disc loss: 1.1218361047610261, policy loss: 0.8215254347010595
Experience 1, Iter 11, disc loss: 1.1080465011211995, policy loss: 0.8229758862843654
Experience 1, Iter 12, disc loss: 1.0891153926534516, policy loss: 0.8310213465553498
Experience 1, Iter 13, disc loss: 1.0746047462525055, policy loss: 0.8323043077232937
Experience 1, Iter 14, disc loss: 1.0542395012617263, policy loss: 0.8424648732457101
Experience 1, Iter 15, disc loss: 1.0436040312486994, policy loss: 0.8381979697653602
Experience 1, Iter 16, disc loss: 1.0208081358196919, policy loss: 0.8503324309316073
Experience 1, Iter 17, disc loss: 1.0164901131523079, policy loss: 0.8368343980087362
Experience 1, Iter 18, disc loss: 0.995346943207186, policy loss: 0.847361138834224
Experience 1, Iter 19, disc loss: 0.9717260953146132, policy loss: 0.8623716240006805
Experience 1, Iter 20, disc loss: 0.9515483932993156, policy loss: 0.8726171861815293
Experience 1, Iter 21, disc loss: 0.9467387803933486, policy loss: 0.8616940559714331
Experience 1, Iter 22, disc loss: 0.9348198670822457, policy loss: 0.8589753181496489
Experience 1, Iter 23, disc loss: 0.9082166940562052, policy loss: 0.8802675830296078
Experience 1, Iter 24, disc loss: 0.8959394361776736, policy loss: 0.879717079244669
Experience 1, Iter 25, disc loss: 0.8817905303499144, policy loss: 0.8838030497847056
Experience 1, Iter 26, disc loss: 0.8712232397393198, policy loss: 0.8778752663088343
Experience 1, Iter 27, disc loss: 0.8570794654538478, policy loss: 0.8821592916989007
Experience 1, Iter 28, disc loss: 0.8308117069063226, policy loss: 0.9066956026390514
Experience 1, Iter 29, disc loss: 0.8124001660562554, policy loss: 0.915533440633182
Experience 1, Iter 30, disc loss: 0.7866709171604631, policy loss: 0.9401098391297421
Experience 1, Iter 31, disc loss: 0.7779835968112012, policy loss: 0.9392692619691476
Experience 1, Iter 32, disc loss: 0.7658059759288629, policy loss: 0.9395410130100086
Experience 1, Iter 33, disc loss: 0.7542943546125157, policy loss: 0.9432727316070053
Experience 1, Iter 34, disc loss: 0.747797251693238, policy loss: 0.9374960313191855
Experience 1, Iter 35, disc loss: 0.7164293809275784, policy loss: 0.9716772805564662
Experience 1, Iter 36, disc loss: 0.7199325804598334, policy loss: 0.9511630499892475
Experience 1, Iter 37, disc loss: 0.7065170020483664, policy loss: 0.9629618284152277
Experience 1, Iter 38, disc loss: 0.682578873915177, policy loss: 0.9875774585893988
Experience 1, Iter 39, disc loss: 0.6915590179152455, policy loss: 0.9556275866197701
Experience 1, Iter 40, disc loss: 0.6656716485725435, policy loss: 0.9920835472590452
Experience 1, Iter 41, disc loss: 0.6426515394746988, policy loss: 1.0238729899608439
Experience 1, Iter 42, disc loss: 0.6346895047197154, policy loss: 1.0180349985929125
Experience 1, Iter 43, disc loss: 0.6166667801469764, policy loss: 1.037243688443232
Experience 1, Iter 44, disc loss: 0.6300150788233139, policy loss: 1.0081592035027245
Experience 1, Iter 45, disc loss: 0.6198604461903888, policy loss: 1.0199135254622163
Experience 1, Iter 46, disc loss: 0.5776066752017961, policy loss: 1.0900901252292634
Experience 1, Iter 47, disc loss: 0.5746883243770772, policy loss: 1.0814456845150435
Experience 1, Iter 48, disc loss: 0.5679332385919598, policy loss: 1.0899639550046323
Experience 1, Iter 49, disc loss: 0.5680750965814969, policy loss: 1.0666581343152106
Experience 1, Iter 50, disc loss: 0.5543588087457414, policy loss: 1.0896831171981294
Experience 1, Iter 51, disc loss: 0.5291568454652702, policy loss: 1.1423838115235398
Experience 1, Iter 52, disc loss: 0.5258123796729853, policy loss: 1.1432933007887414
Experience 1, Iter 53, disc loss: 0.514386578813536, policy loss: 1.1571075477240282
Experience 1, Iter 54, disc loss: 0.5255287976662784, policy loss: 1.126620093134683
Experience 1, Iter 55, disc loss: 0.48945898163796425, policy loss: 1.1955443956532648
Experience 1, Iter 56, disc loss: 0.4957035996809172, policy loss: 1.1549822529370255
Experience 1, Iter 57, disc loss: 0.473067394070373, policy loss: 1.22312473019857
Experience 1, Iter 58, disc loss: 0.4640417084699303, policy loss: 1.2132639125794742
Experience 1, Iter 59, disc loss: 0.4406641125505293, policy loss: 1.2830505090739712
Experience 1, Iter 60, disc loss: 0.41533319387906575, policy loss: 1.3326301246000727
Experience 1, Iter 61, disc loss: 0.435450254048006, policy loss: 1.2805615801065904
Experience 1, Iter 62, disc loss: 0.3911335738324594, policy loss: 1.3913360141973738
Experience 1, Iter 63, disc loss: 0.43660074385909076, policy loss: 1.2763831235745544
Experience 1, Iter 64, disc loss: 0.40734101530835987, policy loss: 1.346008998060797
Experience 1, Iter 65, disc loss: 0.37517127035482845, policy loss: 1.4370826238796632
Experience 1, Iter 66, disc loss: 0.37415760207667376, policy loss: 1.4166075354454275
Experience 1, Iter 67, disc loss: 0.37147207494517337, policy loss: 1.4443770244098144
Experience 1, Iter 68, disc loss: 0.35032277197482387, policy loss: 1.5075674381407267
Experience 1, Iter 69, disc loss: 0.3267098875295329, policy loss: 1.561865828992151
Experience 1, Iter 70, disc loss: 0.35238316979288026, policy loss: 1.4842492314715456
Experience 1, Iter 71, disc loss: 0.3684315794974796, policy loss: 1.4273067693782937
Experience 1, Iter 72, disc loss: 0.359534555944494, policy loss: 1.4754005254579914
Experience 1, Iter 73, disc loss: 0.3222608812060126, policy loss: 1.5857651127849959
Experience 1, Iter 74, disc loss: 0.32268568012658366, policy loss: 1.6086858850505186
Experience 1, Iter 75, disc loss: 0.3371393328418377, policy loss: 1.5407200781780466
Experience 1, Iter 76, disc loss: 0.3414470975024594, policy loss: 1.5466365099270543
Experience 1, Iter 77, disc loss: 0.27644523736393734, policy loss: 1.7611033265001161
Experience 1, Iter 78, disc loss: 0.29626961633520676, policy loss: 1.6799200373015566
Experience 1, Iter 79, disc loss: 0.3010623196619618, policy loss: 1.6694962026157176
Experience 1, Iter 80, disc loss: 0.27439437164151176, policy loss: 1.7773891591844033
Experience 1, Iter 81, disc loss: 0.303186604149546, policy loss: 1.642964081832259
Experience 1, Iter 82, disc loss: 0.2853821978951889, policy loss: 1.729940529695202
Experience 1, Iter 83, disc loss: 0.30121683322623205, policy loss: 1.6850849194990765
Experience 1, Iter 84, disc loss: 0.2592675511087057, policy loss: 1.878239688935533
Experience 1, Iter 85, disc loss: 0.2612631436694933, policy loss: 1.8352742216611708
Experience 1, Iter 86, disc loss: 0.24787976541132176, policy loss: 1.9228370371555679
Experience 1, Iter 87, disc loss: 0.27952501368716454, policy loss: 1.8042855248190013
Experience 1, Iter 88, disc loss: 0.25861240211998554, policy loss: 1.9106982298935766
Experience 1, Iter 89, disc loss: 0.2385208735050767, policy loss: 1.9692037849736903
Experience 1, Iter 90, disc loss: 0.21502353094284335, policy loss: 2.07748674564722
Experience 1, Iter 91, disc loss: 0.21216808091423167, policy loss: 2.031364324644544
Experience 1, Iter 92, disc loss: 0.19875748234844284, policy loss: 2.092449078957248
Experience 1, Iter 93, disc loss: 0.1910862601136534, policy loss: 2.1918453575553047
Experience 1, Iter 94, disc loss: 0.19283587224434295, policy loss: 2.209166952382845
Experience 1, Iter 95, disc loss: 0.22019873800549042, policy loss: 2.0519016884051124
Experience 1, Iter 96, disc loss: 0.16398769697471918, policy loss: 2.3096598540940194
Experience 1, Iter 97, disc loss: 0.2069283783281805, policy loss: 2.2317127337682097
Experience 1, Iter 98, disc loss: 0.16209623473117069, policy loss: 2.4568363609475226
Experience 1, Iter 99, disc loss: 0.19224442752854504, policy loss: 2.1594436570895663
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0021],
        [0.0555],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.0085, 0.2221, 0.0036], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.0085, 0.2221, 0.0036])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.431
Iter 2/2000 - Loss: -1.980
Iter 3/2000 - Loss: -2.577
Iter 4/2000 - Loss: -2.392
Iter 5/2000 - Loss: -2.182
Iter 6/2000 - Loss: -2.368
Iter 7/2000 - Loss: -2.639
Iter 8/2000 - Loss: -2.813
Iter 9/2000 - Loss: -2.854
Iter 10/2000 - Loss: -2.801
Iter 11/2000 - Loss: -2.732
Iter 12/2000 - Loss: -2.702
Iter 13/2000 - Loss: -2.742
Iter 14/2000 - Loss: -2.850
Iter 15/2000 - Loss: -2.966
Iter 16/2000 - Loss: -3.021
Iter 17/2000 - Loss: -3.001
Iter 18/2000 - Loss: -2.962
Iter 19/2000 - Loss: -2.980
Iter 20/2000 - Loss: -3.061
Iter 1981/2000 - Loss: -3.290
Iter 1982/2000 - Loss: -3.295
Iter 1983/2000 - Loss: -3.286
Iter 1984/2000 - Loss: -3.273
Iter 1985/2000 - Loss: -3.267
Iter 1986/2000 - Loss: -3.279
Iter 1987/2000 - Loss: -3.293
Iter 1988/2000 - Loss: -3.288
Iter 1989/2000 - Loss: -3.280
Iter 1990/2000 - Loss: -3.288
Iter 1991/2000 - Loss: -3.295
Iter 1992/2000 - Loss: -3.289
Iter 1993/2000 - Loss: -3.281
Iter 1994/2000 - Loss: -3.282
Iter 1995/2000 - Loss: -3.290
Iter 1996/2000 - Loss: -3.295
Iter 1997/2000 - Loss: -3.293
Iter 1998/2000 - Loss: -3.289
Iter 1999/2000 - Loss: -3.292
Iter 2000/2000 - Loss: -3.295
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0016],
        [0.0407],
        [0.0007]])
Lengthscale: tensor([[[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]],

        [[5.9890e-03, 2.7196e-02, 4.2376e-02, 1.2428e-03, 4.2590e-05,
          3.8567e-02]]])
Signal Variance: tensor([0.0022, 0.0065, 0.1703, 0.0028])
Estimated target variance: tensor([0.0029, 0.0085, 0.2221, 0.0036])
N: 20
Signal to noise ratio: tensor([2.0009, 2.0027, 2.0469, 2.0012])
Bound on condition number: tensor([81.0687, 81.2196, 84.7951, 81.0996])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.22995677117251895, policy loss: 1.8901605048785277
Experience 2, Iter 1, disc loss: 0.21720893635306096, policy loss: 1.9626863873364089
Experience 2, Iter 2, disc loss: 0.20226861201367266, policy loss: 2.006378047754874
Experience 2, Iter 3, disc loss: 0.1997687467666445, policy loss: 2.0042400447230855
Experience 2, Iter 4, disc loss: 0.21358366074093382, policy loss: 1.9367801955172292
Experience 2, Iter 5, disc loss: 0.22039549736126923, policy loss: 1.9058727570292096
Experience 2, Iter 6, disc loss: 0.19621595723924498, policy loss: 2.1084491671655456
Experience 2, Iter 7, disc loss: 0.20283248490283506, policy loss: 2.059436153886847
Experience 2, Iter 8, disc loss: 0.16929902049347392, policy loss: 2.2650950804896746
Experience 2, Iter 9, disc loss: 0.17174567997508017, policy loss: 2.209331102378687
Experience 2, Iter 10, disc loss: 0.18250096793664422, policy loss: 2.1279138930924875
Experience 2, Iter 11, disc loss: 0.180038886858799, policy loss: 2.0942270649786257
Experience 2, Iter 12, disc loss: 0.17976362582244743, policy loss: 2.1863393137191913
Experience 2, Iter 13, disc loss: 0.19248355100216993, policy loss: 2.138367509742176
Experience 2, Iter 14, disc loss: 0.1542290117847558, policy loss: 2.3620438174248695
Experience 2, Iter 15, disc loss: 0.14775318885379246, policy loss: 2.361960164337144
Experience 2, Iter 16, disc loss: 0.16160508101543405, policy loss: 2.282890807554475
Experience 2, Iter 17, disc loss: 0.16213757188149058, policy loss: 2.331852837787298
Experience 2, Iter 18, disc loss: 0.1753572780963018, policy loss: 2.234064882230637
Experience 2, Iter 19, disc loss: 0.1600004859194701, policy loss: 2.3184076311969934
Experience 2, Iter 20, disc loss: 0.12107794898487252, policy loss: 2.49842505501839
Experience 2, Iter 21, disc loss: 0.1321332790560521, policy loss: 2.4569450705175973
Experience 2, Iter 22, disc loss: 0.16194810875450794, policy loss: 2.305211510104353
Experience 2, Iter 23, disc loss: 0.14000429309812265, policy loss: 2.3904571612227397
Experience 2, Iter 24, disc loss: 0.12483040937306959, policy loss: 2.5417740454538977
Experience 2, Iter 25, disc loss: 0.12104187448329676, policy loss: 2.634374040921911
Experience 2, Iter 26, disc loss: 0.13247056176030997, policy loss: 2.5612488293599434
Experience 2, Iter 27, disc loss: 0.11963246874740392, policy loss: 2.673530165978421
Experience 2, Iter 28, disc loss: 0.12478630023867116, policy loss: 2.7076992066098198
Experience 2, Iter 29, disc loss: 0.10605432537400518, policy loss: 2.718356558846286
Experience 2, Iter 30, disc loss: 0.10734412361430506, policy loss: 2.663364472139533
Experience 2, Iter 31, disc loss: 0.09603609328982805, policy loss: 2.816532301600878
Experience 2, Iter 32, disc loss: 0.10400023468379219, policy loss: 2.682026877617719
Experience 2, Iter 33, disc loss: 0.126289743331194, policy loss: 2.632837659508397
Experience 2, Iter 34, disc loss: 0.12779567093159896, policy loss: 2.660350028839926
Experience 2, Iter 35, disc loss: 0.1133845738325601, policy loss: 2.741430486607193
Experience 2, Iter 36, disc loss: 0.110008853670849, policy loss: 2.718866382954262
Experience 2, Iter 37, disc loss: 0.09677772414747039, policy loss: 2.9347337636202058
Experience 2, Iter 38, disc loss: 0.12301745700275024, policy loss: 2.619884753139609
Experience 2, Iter 39, disc loss: 0.08381966927376389, policy loss: 2.9620360776695875
Experience 2, Iter 40, disc loss: 0.11310917333279061, policy loss: 2.73570097066049
Experience 2, Iter 41, disc loss: 0.08708985803975586, policy loss: 3.009641468951706
Experience 2, Iter 42, disc loss: 0.0984583159885867, policy loss: 2.8615941281620536
Experience 2, Iter 43, disc loss: 0.09429212454412389, policy loss: 2.9117217043382913
Experience 2, Iter 44, disc loss: 0.09380276195698381, policy loss: 2.961917419830865
Experience 2, Iter 45, disc loss: 0.09384888151714234, policy loss: 2.9829947439579936
Experience 2, Iter 46, disc loss: 0.067143901238028, policy loss: 3.285949218152751
Experience 2, Iter 47, disc loss: 0.08056566831786356, policy loss: 2.982723786742132
Experience 2, Iter 48, disc loss: 0.09347626320299969, policy loss: 3.000516241605644
Experience 2, Iter 49, disc loss: 0.06753748504649089, policy loss: 3.213698822023888
Experience 2, Iter 50, disc loss: 0.07695572367860386, policy loss: 3.2432236125378484
Experience 2, Iter 51, disc loss: 0.09204024674109017, policy loss: 2.971592118561709
Experience 2, Iter 52, disc loss: 0.07975152122438384, policy loss: 3.0814381053692474
Experience 2, Iter 53, disc loss: 0.06617591041936402, policy loss: 3.32130662532194
Experience 2, Iter 54, disc loss: 0.05182839298098235, policy loss: 3.6398035787098983
Experience 2, Iter 55, disc loss: 0.07274575891538072, policy loss: 3.289092475946241
Experience 2, Iter 56, disc loss: 0.0752260768076306, policy loss: 3.2612163273032158
Experience 2, Iter 57, disc loss: 0.08203355632517345, policy loss: 3.2524578898563368
Experience 2, Iter 58, disc loss: 0.061649945722357984, policy loss: 3.4344152391809395
Experience 2, Iter 59, disc loss: 0.058842249773433546, policy loss: 3.3876966942308426
Experience 2, Iter 60, disc loss: 0.06529859073077851, policy loss: 3.2908313965855367
Experience 2, Iter 61, disc loss: 0.06785702365094355, policy loss: 3.4327765186966945
Experience 2, Iter 62, disc loss: 0.05832809523764461, policy loss: 3.4799928985291104
Experience 2, Iter 63, disc loss: 0.0651945960808417, policy loss: 3.3413367048479623
Experience 2, Iter 64, disc loss: 0.05861992331989687, policy loss: 3.597560719262087
Experience 2, Iter 65, disc loss: 0.0551781196253952, policy loss: 3.483011351046385
Experience 2, Iter 66, disc loss: 0.07243156371531995, policy loss: 3.339801464585047
Experience 2, Iter 67, disc loss: 0.06070077769481944, policy loss: 3.539414291596189
Experience 2, Iter 68, disc loss: 0.042091399641785515, policy loss: 3.715301986845433
Experience 2, Iter 69, disc loss: 0.07267933934946946, policy loss: 3.4156710764505447
Experience 2, Iter 70, disc loss: 0.06256175612377733, policy loss: 3.6309933613594776
Experience 2, Iter 71, disc loss: 0.05149700375425258, policy loss: 3.7198484602597652
Experience 2, Iter 72, disc loss: 0.05359637456414926, policy loss: 3.673077765231257
Experience 2, Iter 73, disc loss: 0.07057135283237112, policy loss: 3.2897084013138267
Experience 2, Iter 74, disc loss: 0.06555881601082235, policy loss: 3.546918510270374
Experience 2, Iter 75, disc loss: 0.05173382499470122, policy loss: 3.7759411983269606
Experience 2, Iter 76, disc loss: 0.03178559767919806, policy loss: 4.1034878619259985
Experience 2, Iter 77, disc loss: 0.058706774170569485, policy loss: 3.5652506781981996
Experience 2, Iter 78, disc loss: 0.05003907906302033, policy loss: 3.6653177096504743
Experience 2, Iter 79, disc loss: 0.04601950884208168, policy loss: 3.814469454577223
Experience 2, Iter 80, disc loss: 0.050940240354676446, policy loss: 3.6022076665515512
Experience 2, Iter 81, disc loss: 0.03839690065515526, policy loss: 4.072913536015541
Experience 2, Iter 82, disc loss: 0.045238559083522326, policy loss: 3.9955702340825487
Experience 2, Iter 83, disc loss: 0.037889746705251685, policy loss: 4.105564743214588
Experience 2, Iter 84, disc loss: 0.043175511560553784, policy loss: 3.9778144814308343
Experience 2, Iter 85, disc loss: 0.05366618759001701, policy loss: 3.623367524665876
Experience 2, Iter 86, disc loss: 0.033420925843719335, policy loss: 4.169876996010941
Experience 2, Iter 87, disc loss: 0.02961174361890745, policy loss: 4.268128298801916
Experience 2, Iter 88, disc loss: 0.039306809902336294, policy loss: 3.9382918844281867
Experience 2, Iter 89, disc loss: 0.031302384748186994, policy loss: 4.101199174965757
Experience 2, Iter 90, disc loss: 0.035925691546058326, policy loss: 4.284286287815773
Experience 2, Iter 91, disc loss: 0.03600649970931945, policy loss: 4.009151906757545
Experience 2, Iter 92, disc loss: 0.027682946023916754, policy loss: 4.229257053283161
Experience 2, Iter 93, disc loss: 0.03236193081996419, policy loss: 4.173600922587257
Experience 2, Iter 94, disc loss: 0.040148250716305366, policy loss: 3.9988226088448275
Experience 2, Iter 95, disc loss: 0.03232427013174684, policy loss: 4.057777278107506
Experience 2, Iter 96, disc loss: 0.03641375808966454, policy loss: 4.265312576210212
Experience 2, Iter 97, disc loss: 0.03571904017082013, policy loss: 4.034389875879387
Experience 2, Iter 98, disc loss: 0.03391793368084398, policy loss: 4.475827209979373
Experience 2, Iter 99, disc loss: 0.04205360475938601, policy loss: 4.034020500704747
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0026],
        [0.0366],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.2627e-03, 3.0826e-02, 2.7855e-02, 1.1981e-03, 3.2894e-05,
          7.6862e-02]],

        [[6.2627e-03, 3.0826e-02, 2.7855e-02, 1.1981e-03, 3.2894e-05,
          7.6862e-02]],

        [[6.2627e-03, 3.0826e-02, 2.7855e-02, 1.1981e-03, 3.2894e-05,
          7.6862e-02]],

        [[6.2627e-03, 3.0826e-02, 2.7855e-02, 1.1981e-03, 3.2894e-05,
          7.6862e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0104, 0.1463, 0.0025], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0104, 0.1463, 0.0025])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.751
Iter 2/2000 - Loss: -1.552
Iter 3/2000 - Loss: -2.986
Iter 4/2000 - Loss: -2.862
Iter 5/2000 - Loss: -2.380
Iter 6/2000 - Loss: -2.557
Iter 7/2000 - Loss: -3.049
Iter 8/2000 - Loss: -3.336
Iter 9/2000 - Loss: -3.261
Iter 10/2000 - Loss: -3.044
Iter 11/2000 - Loss: -2.995
Iter 12/2000 - Loss: -3.152
Iter 13/2000 - Loss: -3.321
Iter 14/2000 - Loss: -3.383
Iter 15/2000 - Loss: -3.420
Iter 16/2000 - Loss: -3.485
Iter 17/2000 - Loss: -3.512
Iter 18/2000 - Loss: -3.491
Iter 19/2000 - Loss: -3.524
Iter 20/2000 - Loss: -3.637
Iter 1981/2000 - Loss: -7.371
Iter 1982/2000 - Loss: -7.371
Iter 1983/2000 - Loss: -7.371
Iter 1984/2000 - Loss: -7.371
Iter 1985/2000 - Loss: -7.371
Iter 1986/2000 - Loss: -7.371
Iter 1987/2000 - Loss: -7.371
Iter 1988/2000 - Loss: -7.371
Iter 1989/2000 - Loss: -7.371
Iter 1990/2000 - Loss: -7.371
Iter 1991/2000 - Loss: -7.371
Iter 1992/2000 - Loss: -7.371
Iter 1993/2000 - Loss: -7.371
Iter 1994/2000 - Loss: -7.371
Iter 1995/2000 - Loss: -7.371
Iter 1996/2000 - Loss: -7.371
Iter 1997/2000 - Loss: -7.371
Iter 1998/2000 - Loss: -7.371
Iter 1999/2000 - Loss: -7.371
Iter 2000/2000 - Loss: -7.371
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0002],
        [0.0002]])
Lengthscale: tensor([[[1.1996e+01, 2.6411e+00, 1.2363e+01, 9.7735e+00, 5.6453e+00,
          1.2789e+01]],

        [[8.7667e+00, 5.5473e+00, 8.6731e+00, 3.1121e+00, 1.4047e-03,
          9.0672e-01]],

        [[9.7234e-01, 2.4792e+01, 2.6553e+00, 6.8678e+00, 7.6278e-03,
          1.4309e+00]],

        [[2.0860e+01, 3.0668e+01, 2.3172e+00, 7.7902e-01, 3.5645e+00,
          4.7440e+00]]])
Signal Variance: tensor([0.0241, 0.0102, 0.3019, 0.0192])
Estimated target variance: tensor([0.0035, 0.0104, 0.1463, 0.0025])
N: 30
Signal to noise ratio: tensor([ 8.5294,  5.8319, 37.7623,  9.4730])
Bound on condition number: tensor([ 2183.5352,  1021.3476, 42780.6346,  2693.1219])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.20970757484313668, policy loss: 1.793486537781674
Experience 3, Iter 1, disc loss: 0.20568562721982978, policy loss: 1.8848886992465839
Experience 3, Iter 2, disc loss: 0.2048281642497874, policy loss: 1.9476126885548692
Experience 3, Iter 3, disc loss: 0.164174701861607, policy loss: 2.1109161069447406
Experience 3, Iter 4, disc loss: 0.18736598319715447, policy loss: 1.9591248657220257
Experience 3, Iter 5, disc loss: 0.19362396231754583, policy loss: 1.939832892054822
Experience 3, Iter 6, disc loss: 0.15007614467726893, policy loss: 2.2088429319265104
Experience 3, Iter 7, disc loss: 0.143947405808002, policy loss: 2.1608598971639363
Experience 3, Iter 8, disc loss: 0.1371710009321577, policy loss: 2.235376711058486
Experience 3, Iter 9, disc loss: 0.14470856808725052, policy loss: 2.1796948506265372
Experience 3, Iter 10, disc loss: 0.15519465759459922, policy loss: 2.1081925771874936
Experience 3, Iter 11, disc loss: 0.1354931780682136, policy loss: 2.2812009918647687
Experience 3, Iter 12, disc loss: 0.1575870350863206, policy loss: 2.1356620395880137
Experience 3, Iter 13, disc loss: 0.12931453838758086, policy loss: 2.371387980062428
Experience 3, Iter 14, disc loss: 0.12381426108262727, policy loss: 2.3923156940168955
Experience 3, Iter 15, disc loss: 0.12324624406557859, policy loss: 2.415374273053522
Experience 3, Iter 16, disc loss: 0.11623647726180594, policy loss: 2.509619421165481
Experience 3, Iter 17, disc loss: 0.11185573734480751, policy loss: 2.5063810600314174
Experience 3, Iter 18, disc loss: 0.10884880974804247, policy loss: 2.533962608248927
Experience 3, Iter 19, disc loss: 0.10759301154200451, policy loss: 2.5902160345825385
Experience 3, Iter 20, disc loss: 0.10091700320770079, policy loss: 2.5795531891802312
Experience 3, Iter 21, disc loss: 0.10300470916093797, policy loss: 2.613344629082485
Experience 3, Iter 22, disc loss: 0.0960700104229877, policy loss: 2.669166326744623
Experience 3, Iter 23, disc loss: 0.10390856242230663, policy loss: 2.5594040892555814
Experience 3, Iter 24, disc loss: 0.09620351361195892, policy loss: 2.6459080359550162
Experience 3, Iter 25, disc loss: 0.09016716605637036, policy loss: 2.7803297802587834
Experience 3, Iter 26, disc loss: 0.09268124264778337, policy loss: 2.663264564249274
Experience 3, Iter 27, disc loss: 0.08527250319197993, policy loss: 2.8166807969824994
Experience 3, Iter 28, disc loss: 0.08658426461601264, policy loss: 2.7488001941981763
Experience 3, Iter 29, disc loss: 0.08797383494742192, policy loss: 2.7383373017820194
Experience 3, Iter 30, disc loss: 0.08084446388251054, policy loss: 2.7993502782022346
Experience 3, Iter 31, disc loss: 0.08111334231903058, policy loss: 2.7577123277883615
Experience 3, Iter 32, disc loss: 0.07475177238155772, policy loss: 2.843470684520538
Experience 3, Iter 33, disc loss: 0.07342092781859354, policy loss: 2.864413331122239
Experience 3, Iter 34, disc loss: 0.06971326507707792, policy loss: 2.9055752864259925
Experience 3, Iter 35, disc loss: 0.06840873276477073, policy loss: 2.944779764314755
Experience 3, Iter 36, disc loss: 0.07113432767550822, policy loss: 2.9022066534852913
Experience 3, Iter 37, disc loss: 0.06383564448343562, policy loss: 2.9908991906919877
Experience 3, Iter 38, disc loss: 0.06404164630791773, policy loss: 3.00380742861305
Experience 3, Iter 39, disc loss: 0.055242469762791874, policy loss: 3.2167746488997198
Experience 3, Iter 40, disc loss: 0.06308264032821925, policy loss: 2.98957081208464
Experience 3, Iter 41, disc loss: 0.061214169237144705, policy loss: 3.015543844622961
Experience 3, Iter 42, disc loss: 0.057002564505020045, policy loss: 3.0733859679040405
Experience 3, Iter 43, disc loss: 0.05325777047846944, policy loss: 3.212142466233272
Experience 3, Iter 44, disc loss: 0.052751174035349825, policy loss: 3.2098348397322987
Experience 3, Iter 45, disc loss: 0.05677723493086345, policy loss: 3.152653591867551
Experience 3, Iter 46, disc loss: 0.05604403153857979, policy loss: 3.1195388191598
Experience 3, Iter 47, disc loss: 0.05338025550076841, policy loss: 3.189139466183724
Experience 3, Iter 48, disc loss: 0.053298925957399775, policy loss: 3.1616428270851324
Experience 3, Iter 49, disc loss: 0.04636160293255026, policy loss: 3.324823850211692
Experience 3, Iter 50, disc loss: 0.0471440960490486, policy loss: 3.294052640845135
Experience 3, Iter 51, disc loss: 0.04666168413509687, policy loss: 3.3309686988966845
Experience 3, Iter 52, disc loss: 0.04177513277590979, policy loss: 3.436260084919092
Experience 3, Iter 53, disc loss: 0.043206071357203404, policy loss: 3.361346916164947
Experience 3, Iter 54, disc loss: 0.03846185731821535, policy loss: 3.4952003429785052
Experience 3, Iter 55, disc loss: 0.04027461105873687, policy loss: 3.4588813766359614
Experience 3, Iter 56, disc loss: 0.0444039502343129, policy loss: 3.353509357185606
Experience 3, Iter 57, disc loss: 0.03908668205673353, policy loss: 3.50631150096598
Experience 3, Iter 58, disc loss: 0.04153901690251422, policy loss: 3.4551737690536473
Experience 3, Iter 59, disc loss: 0.038166980461465556, policy loss: 3.528275820714815
Experience 3, Iter 60, disc loss: 0.03766980217625765, policy loss: 3.585421542899853
Experience 3, Iter 61, disc loss: 0.03870549090316471, policy loss: 3.5148509289184307
Experience 3, Iter 62, disc loss: 0.03769765422895256, policy loss: 3.5155352425174944
Experience 3, Iter 63, disc loss: 0.03363987090931113, policy loss: 3.6789558637668076
Experience 3, Iter 64, disc loss: 0.034938235615495215, policy loss: 3.65391744762567
Experience 3, Iter 65, disc loss: 0.03623848773086909, policy loss: 3.545267441704884
Experience 3, Iter 66, disc loss: 0.033721415400110935, policy loss: 3.6290484254304918
Experience 3, Iter 67, disc loss: 0.03634836663172165, policy loss: 3.52216521795118
Experience 3, Iter 68, disc loss: 0.03230151698317341, policy loss: 3.6842873441950736
Experience 3, Iter 69, disc loss: 0.03411876740984454, policy loss: 3.5795546626423667
Experience 3, Iter 70, disc loss: 0.03302689501328081, policy loss: 3.7865032171330433
Experience 3, Iter 71, disc loss: 0.031198411239539453, policy loss: 3.7519496753327077
Experience 3, Iter 72, disc loss: 0.030157048869553893, policy loss: 3.749120136577732
Experience 3, Iter 73, disc loss: 0.027484411029696978, policy loss: 3.901608223383172
Experience 3, Iter 74, disc loss: 0.02888253922025393, policy loss: 3.867980086336315
Experience 3, Iter 75, disc loss: 0.026134830295328017, policy loss: 3.9434504503138244
Experience 3, Iter 76, disc loss: 0.026696278896054592, policy loss: 3.8793541573252686
Experience 3, Iter 77, disc loss: 0.025391136867379465, policy loss: 3.975650991003517
Experience 3, Iter 78, disc loss: 0.02718490403322455, policy loss: 3.909521029788476
Experience 3, Iter 79, disc loss: 0.028694018611852813, policy loss: 3.7874964647049385
Experience 3, Iter 80, disc loss: 0.0260133631642764, policy loss: 3.96603781161923
Experience 3, Iter 81, disc loss: 0.02828434033924871, policy loss: 3.8487620344341575
Experience 3, Iter 82, disc loss: 0.027766577769126867, policy loss: 3.8749031839804533
Experience 3, Iter 83, disc loss: 0.02872839347458127, policy loss: 3.7732869328341403
Experience 3, Iter 84, disc loss: 0.02600770945814227, policy loss: 3.911681644838459
Experience 3, Iter 85, disc loss: 0.02545996527091536, policy loss: 3.945472005944105
Experience 3, Iter 86, disc loss: 0.021262236155924673, policy loss: 4.171375446517451
Experience 3, Iter 87, disc loss: 0.02585372116697475, policy loss: 3.886541867320714
Experience 3, Iter 88, disc loss: 0.024867346382181913, policy loss: 3.980121202957915
Experience 3, Iter 89, disc loss: 0.022229599363185946, policy loss: 4.109695875477749
Experience 3, Iter 90, disc loss: 0.023704775488024986, policy loss: 4.028230894920071
Experience 3, Iter 91, disc loss: 0.021336639557170314, policy loss: 4.124157744809921
Experience 3, Iter 92, disc loss: 0.02471665304804658, policy loss: 3.920656047917552
Experience 3, Iter 93, disc loss: 0.02268754442217317, policy loss: 4.035217994951744
Experience 3, Iter 94, disc loss: 0.020559505019250027, policy loss: 4.166724371338823
Experience 3, Iter 95, disc loss: 0.020641268640397746, policy loss: 4.151065562835632
Experience 3, Iter 96, disc loss: 0.02349718301550958, policy loss: 4.126339486028172
Experience 3, Iter 97, disc loss: 0.021980250685043997, policy loss: 4.082189312444279
Experience 3, Iter 98, disc loss: 0.023333251134960282, policy loss: 4.028441146362195
Experience 3, Iter 99, disc loss: 0.019575665175338146, policy loss: 4.235613767924104
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0025],
        [0.0274],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.8547e-03, 2.4730e-02, 2.0919e-02, 1.0371e-03, 2.6262e-05,
          7.8205e-02]],

        [[4.8547e-03, 2.4730e-02, 2.0919e-02, 1.0371e-03, 2.6262e-05,
          7.8205e-02]],

        [[4.8547e-03, 2.4730e-02, 2.0919e-02, 1.0371e-03, 2.6262e-05,
          7.8205e-02]],

        [[4.8547e-03, 2.4730e-02, 2.0919e-02, 1.0371e-03, 2.6262e-05,
          7.8205e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.0099, 0.1097, 0.0020], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.0099, 0.1097, 0.0020])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.344
Iter 2/2000 - Loss: -1.099
Iter 3/2000 - Loss: -3.313
Iter 4/2000 - Loss: -3.214
Iter 5/2000 - Loss: -2.444
Iter 6/2000 - Loss: -2.621
Iter 7/2000 - Loss: -3.235
Iter 8/2000 - Loss: -3.614
Iter 9/2000 - Loss: -3.586
Iter 10/2000 - Loss: -3.369
Iter 11/2000 - Loss: -3.248
Iter 12/2000 - Loss: -3.305
Iter 13/2000 - Loss: -3.460
Iter 14/2000 - Loss: -3.607
Iter 15/2000 - Loss: -3.714
Iter 16/2000 - Loss: -3.770
Iter 17/2000 - Loss: -3.753
Iter 18/2000 - Loss: -3.693
Iter 19/2000 - Loss: -3.688
Iter 20/2000 - Loss: -3.793
Iter 1981/2000 - Loss: -8.170
Iter 1982/2000 - Loss: -8.170
Iter 1983/2000 - Loss: -8.170
Iter 1984/2000 - Loss: -8.170
Iter 1985/2000 - Loss: -8.170
Iter 1986/2000 - Loss: -8.170
Iter 1987/2000 - Loss: -8.170
Iter 1988/2000 - Loss: -8.170
Iter 1989/2000 - Loss: -8.170
Iter 1990/2000 - Loss: -8.170
Iter 1991/2000 - Loss: -8.170
Iter 1992/2000 - Loss: -8.170
Iter 1993/2000 - Loss: -8.170
Iter 1994/2000 - Loss: -8.170
Iter 1995/2000 - Loss: -8.170
Iter 1996/2000 - Loss: -8.170
Iter 1997/2000 - Loss: -8.170
Iter 1998/2000 - Loss: -8.170
Iter 1999/2000 - Loss: -8.170
Iter 2000/2000 - Loss: -8.170
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0007],
        [0.0002]])
Lengthscale: tensor([[[8.9142e+00, 2.7468e+00, 1.3268e+01, 1.0262e+01, 5.5089e+00,
          1.5646e+01]],

        [[1.4894e+01, 2.0171e+01, 1.2144e+01, 6.6946e-01, 1.4281e+00,
          2.5148e+00]],

        [[3.8806e+00, 3.2883e+00, 3.2094e+00, 6.8432e-01, 1.0012e-02,
          1.5707e+00]],

        [[1.7741e+01, 2.7920e+01, 2.3039e+00, 7.1740e-01, 3.3037e+00,
          5.0984e+00]]])
Signal Variance: tensor([0.0273, 0.0660, 0.3349, 0.0186])
Estimated target variance: tensor([0.0029, 0.0099, 0.1097, 0.0020])
N: 40
Signal to noise ratio: tensor([ 9.5346, 11.3304, 22.0622,  9.9059])
Bound on condition number: tensor([ 3637.3374,  5136.0818, 19470.6141,  3926.0680])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.020500076565556115, policy loss: 4.156938802942764
Experience 4, Iter 1, disc loss: 0.022072461365819855, policy loss: 4.140429167169271
Experience 4, Iter 2, disc loss: 0.026246815197547194, policy loss: 3.8603644588244563
Experience 4, Iter 3, disc loss: 0.025624841313576268, policy loss: 3.9592021220106632
Experience 4, Iter 4, disc loss: 0.02774301011652181, policy loss: 3.8429172339523543
Experience 4, Iter 5, disc loss: 0.026399133289317506, policy loss: 3.875205361638823
Experience 4, Iter 6, disc loss: 0.023892822847590752, policy loss: 4.026661726307919
Experience 4, Iter 7, disc loss: 0.03183703023274221, policy loss: 3.7242896053912764
Experience 4, Iter 8, disc loss: 0.026449647835580268, policy loss: 3.9652162274349605
Experience 4, Iter 9, disc loss: 0.027937931733488162, policy loss: 3.9039027964877673
Experience 4, Iter 10, disc loss: 0.024301447471786475, policy loss: 4.109332466885517
Experience 4, Iter 11, disc loss: 0.026073838831524067, policy loss: 3.971026395833591
Experience 4, Iter 12, disc loss: 0.02352518610415245, policy loss: 4.149679946294073
Experience 4, Iter 13, disc loss: 0.022888454754004722, policy loss: 4.2264806598409335
Experience 4, Iter 14, disc loss: 0.025233829490080607, policy loss: 4.090444898767387
Experience 4, Iter 15, disc loss: 0.027560115810398394, policy loss: 4.074351703568171
Experience 4, Iter 16, disc loss: 0.023083723913405702, policy loss: 4.172226265302193
Experience 4, Iter 17, disc loss: 0.020809634718939662, policy loss: 4.312562560617246
Experience 4, Iter 18, disc loss: 0.020494476386418572, policy loss: 4.374719846320019
Experience 4, Iter 19, disc loss: 0.021420226564402785, policy loss: 4.2111869347001765
Experience 4, Iter 20, disc loss: 0.02156963555748338, policy loss: 4.227598375757846
Experience 4, Iter 21, disc loss: 0.020795580209534203, policy loss: 4.326998607477083
Experience 4, Iter 22, disc loss: 0.020205660207204903, policy loss: 4.270988227267905
Experience 4, Iter 23, disc loss: 0.019155296344586886, policy loss: 4.421921592936812
Experience 4, Iter 24, disc loss: 0.02156843565123387, policy loss: 4.125787011038524
Experience 4, Iter 25, disc loss: 0.020972320378323108, policy loss: 4.215083976066404
Experience 4, Iter 26, disc loss: 0.018653148204025145, policy loss: 4.403732823114567
Experience 4, Iter 27, disc loss: 0.01934057077632301, policy loss: 4.288906814024905
Experience 4, Iter 28, disc loss: 0.0183935228555253, policy loss: 4.363816831270027
Experience 4, Iter 29, disc loss: 0.017901693684983747, policy loss: 4.467393750335367
Experience 4, Iter 30, disc loss: 0.017626637411931198, policy loss: 4.4805260950227925
Experience 4, Iter 31, disc loss: 0.017158695160883634, policy loss: 4.388353330360591
Experience 4, Iter 32, disc loss: 0.01826833182454265, policy loss: 4.333286065333546
Experience 4, Iter 33, disc loss: 0.015669284194641926, policy loss: 4.607528105826791
Experience 4, Iter 34, disc loss: 0.015836087802464108, policy loss: 4.5136996053948915
Experience 4, Iter 35, disc loss: 0.01571527071876627, policy loss: 4.532813181098236
Experience 4, Iter 36, disc loss: 0.01723393195689096, policy loss: 4.422688759060386
Experience 4, Iter 37, disc loss: 0.016487867453048283, policy loss: 4.529520749910755
Experience 4, Iter 38, disc loss: 0.01452994963696794, policy loss: 4.628987737895424
Experience 4, Iter 39, disc loss: 0.015611601896895179, policy loss: 4.606488043328528
Experience 4, Iter 40, disc loss: 0.016985088059148015, policy loss: 4.357278775267435
Experience 4, Iter 41, disc loss: 0.01592922912126565, policy loss: 4.496864052207801
Experience 4, Iter 42, disc loss: 0.014286873022731785, policy loss: 4.594122033138359
Experience 4, Iter 43, disc loss: 0.015309890827474087, policy loss: 4.496802523422711
Experience 4, Iter 44, disc loss: 0.01771052898748097, policy loss: 4.424498853677229
Experience 4, Iter 45, disc loss: 0.013296734667720482, policy loss: 4.621351008643719
Experience 4, Iter 46, disc loss: 0.013422719064510836, policy loss: 4.825354456020982
Experience 4, Iter 47, disc loss: 0.013584556470210387, policy loss: 4.670884452221264
Experience 4, Iter 48, disc loss: 0.014724321601864515, policy loss: 4.610578350807723
Experience 4, Iter 49, disc loss: 0.014648903828680502, policy loss: 4.550665945358097
Experience 4, Iter 50, disc loss: 0.012742082570728754, policy loss: 4.744390771927671
Experience 4, Iter 51, disc loss: 0.014321851799529339, policy loss: 4.645456684188036
Experience 4, Iter 52, disc loss: 0.014493693931022376, policy loss: 4.637180943197478
Experience 4, Iter 53, disc loss: 0.015033963835604125, policy loss: 4.507535953993925
Experience 4, Iter 54, disc loss: 0.01295035016653498, policy loss: 4.709382451646872
Experience 4, Iter 55, disc loss: 0.013141666787879714, policy loss: 4.657814511830356
Experience 4, Iter 56, disc loss: 0.013238407626750008, policy loss: 4.695065285393029
Experience 4, Iter 57, disc loss: 0.012801654499294608, policy loss: 4.799775386185402
Experience 4, Iter 58, disc loss: 0.012758739403311022, policy loss: 4.724395897702967
Experience 4, Iter 59, disc loss: 0.013605289931359673, policy loss: 4.620043468694687
Experience 4, Iter 60, disc loss: 0.012388291295858181, policy loss: 4.71339005315851
Experience 4, Iter 61, disc loss: 0.012126138016084467, policy loss: 4.716711991308351
Experience 4, Iter 62, disc loss: 0.012572317665027864, policy loss: 4.671748111895788
Experience 4, Iter 63, disc loss: 0.011607435682894837, policy loss: 4.825759954934405
Experience 4, Iter 64, disc loss: 0.012171852411182427, policy loss: 4.794605963152531
Experience 4, Iter 65, disc loss: 0.010873283355409331, policy loss: 4.940240873790109
Experience 4, Iter 66, disc loss: 0.011004930169001967, policy loss: 4.9008537969651265
Experience 4, Iter 67, disc loss: 0.010761142335953593, policy loss: 4.848020077865369
Experience 4, Iter 68, disc loss: 0.010369265766578522, policy loss: 5.086408650127116
Experience 4, Iter 69, disc loss: 0.01114375541261201, policy loss: 4.901527743907874
Experience 4, Iter 70, disc loss: 0.011399480841436804, policy loss: 4.854046823886907
Experience 4, Iter 71, disc loss: 0.011308803593175711, policy loss: 4.883295259186538
Experience 4, Iter 72, disc loss: 0.009742105317732484, policy loss: 5.071265913990521
Experience 4, Iter 73, disc loss: 0.010456579599877527, policy loss: 5.202182202234007
Experience 4, Iter 74, disc loss: 0.009986616534581327, policy loss: 5.006752557861471
Experience 4, Iter 75, disc loss: 0.010374121096467924, policy loss: 4.864824871380267
Experience 4, Iter 76, disc loss: 0.010366973040330709, policy loss: 4.882897106419164
Experience 4, Iter 77, disc loss: 0.010648765248793423, policy loss: 4.8249504103937015
Experience 4, Iter 78, disc loss: 0.0106987930607503, policy loss: 4.784618561607653
Experience 4, Iter 79, disc loss: 0.009689262800055502, policy loss: 4.903080852802102
Experience 4, Iter 80, disc loss: 0.008883267741331823, policy loss: 5.092532139106566
Experience 4, Iter 81, disc loss: 0.009069351026095636, policy loss: 5.111442454403588
Experience 4, Iter 82, disc loss: 0.00988823411887551, policy loss: 4.9941411849249375
Experience 4, Iter 83, disc loss: 0.009529557008379536, policy loss: 4.907451134405642
Experience 4, Iter 84, disc loss: 0.00990920909783563, policy loss: 5.0100541916675345
Experience 4, Iter 85, disc loss: 0.009410889409100485, policy loss: 5.097425718143777
Experience 4, Iter 86, disc loss: 0.0098264223875172, policy loss: 4.963979995661143
Experience 4, Iter 87, disc loss: 0.009674325963296836, policy loss: 4.949185165857639
Experience 4, Iter 88, disc loss: 0.008522092703272933, policy loss: 5.116037554977523
Experience 4, Iter 89, disc loss: 0.008865078160340888, policy loss: 5.185627663585637
Experience 4, Iter 90, disc loss: 0.00790472344743573, policy loss: 5.301099314148634
Experience 4, Iter 91, disc loss: 0.009536057367473077, policy loss: 5.016307632566481
Experience 4, Iter 92, disc loss: 0.009068343645552283, policy loss: 5.124975829942724
Experience 4, Iter 93, disc loss: 0.009729173407869543, policy loss: 5.030387986746206
Experience 4, Iter 94, disc loss: 0.0086764963518317, policy loss: 5.182454621298151
Experience 4, Iter 95, disc loss: 0.008368221642406793, policy loss: 5.168637637056554
Experience 4, Iter 96, disc loss: 0.009353596706291123, policy loss: 5.113710280582682
Experience 4, Iter 97, disc loss: 0.008129604554597799, policy loss: 5.318364626410192
Experience 4, Iter 98, disc loss: 0.009233475230682913, policy loss: 5.191318083415239
Experience 4, Iter 99, disc loss: 0.009117279717784503, policy loss: 5.029133834231164
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0028],
        [0.0221],
        [0.0004]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.2628e-03, 2.4119e-02, 1.6797e-02, 1.0906e-03, 2.1516e-05,
          9.2833e-02]],

        [[4.2628e-03, 2.4119e-02, 1.6797e-02, 1.0906e-03, 2.1516e-05,
          9.2833e-02]],

        [[4.2628e-03, 2.4119e-02, 1.6797e-02, 1.0906e-03, 2.1516e-05,
          9.2833e-02]],

        [[4.2628e-03, 2.4119e-02, 1.6797e-02, 1.0906e-03, 2.1516e-05,
          9.2833e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.0111, 0.0883, 0.0017], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.0111, 0.0883, 0.0017])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.720
Iter 2/2000 - Loss: -0.401
Iter 3/2000 - Loss: -3.633
Iter 4/2000 - Loss: -3.293
Iter 5/2000 - Loss: -2.146
Iter 6/2000 - Loss: -2.477
Iter 7/2000 - Loss: -3.375
Iter 8/2000 - Loss: -3.814
Iter 9/2000 - Loss: -3.645
Iter 10/2000 - Loss: -3.309
Iter 11/2000 - Loss: -3.176
Iter 12/2000 - Loss: -3.282
Iter 13/2000 - Loss: -3.484
Iter 14/2000 - Loss: -3.648
Iter 15/2000 - Loss: -3.721
Iter 16/2000 - Loss: -3.718
Iter 17/2000 - Loss: -3.687
Iter 18/2000 - Loss: -3.668
Iter 19/2000 - Loss: -3.678
Iter 20/2000 - Loss: -3.715
Iter 1981/2000 - Loss: -8.352
Iter 1982/2000 - Loss: -8.352
Iter 1983/2000 - Loss: -8.352
Iter 1984/2000 - Loss: -8.352
Iter 1985/2000 - Loss: -8.352
Iter 1986/2000 - Loss: -8.352
Iter 1987/2000 - Loss: -8.352
Iter 1988/2000 - Loss: -8.352
Iter 1989/2000 - Loss: -8.352
Iter 1990/2000 - Loss: -8.352
Iter 1991/2000 - Loss: -8.352
Iter 1992/2000 - Loss: -8.352
Iter 1993/2000 - Loss: -8.352
Iter 1994/2000 - Loss: -8.352
Iter 1995/2000 - Loss: -8.352
Iter 1996/2000 - Loss: -8.352
Iter 1997/2000 - Loss: -8.352
Iter 1998/2000 - Loss: -8.352
Iter 1999/2000 - Loss: -8.352
Iter 2000/2000 - Loss: -8.352
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0006],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[ 7.3528,  3.1917, 14.0532, 11.6052,  4.5377, 15.4646]],

        [[16.1646, 23.6268, 20.4238,  1.1123,  1.9262,  5.1989]],

        [[12.4783, 16.1532,  4.6992,  0.3015,  1.0891,  1.5717]],

        [[18.3252, 27.4596,  2.6528,  0.7551,  3.1348,  7.3599]]])
Signal Variance: tensor([0.0357, 0.2337, 0.4724, 0.0252])
Estimated target variance: tensor([0.0029, 0.0111, 0.0883, 0.0017])
N: 50
Signal to noise ratio: tensor([11.6195, 20.3061, 16.8510, 10.7390])
Bound on condition number: tensor([ 6751.6432, 20617.8183, 14198.7791,  5767.3298])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.00999511278199963, policy loss: 4.862629440016965
Experience 5, Iter 1, disc loss: 0.010812648451945612, policy loss: 4.789235133490612
Experience 5, Iter 2, disc loss: 0.01122833509675796, policy loss: 4.711887655646024
Experience 5, Iter 3, disc loss: 0.011790915518427796, policy loss: 4.6258459970026085
Experience 5, Iter 4, disc loss: 0.011253795858562753, policy loss: 4.705754116300369
Experience 5, Iter 5, disc loss: 0.011481493710496823, policy loss: 4.6432127207650575
Experience 5, Iter 6, disc loss: 0.010746201736217448, policy loss: 4.8260984650333585
Experience 5, Iter 7, disc loss: 0.01156816838524289, policy loss: 4.717634870953721
Experience 5, Iter 8, disc loss: 0.011168194982101828, policy loss: 4.678435859592943
Experience 5, Iter 9, disc loss: 0.010740195182282426, policy loss: 4.715540480238743
Experience 5, Iter 10, disc loss: 0.010999121093534153, policy loss: 4.734916938815817
Experience 5, Iter 11, disc loss: 0.010110017729831935, policy loss: 4.778651055469741
Experience 5, Iter 12, disc loss: 0.010150062015436948, policy loss: 4.803018964185462
Experience 5, Iter 13, disc loss: 0.011163451661601034, policy loss: 4.689800341637977
Experience 5, Iter 14, disc loss: 0.010563763703283834, policy loss: 4.729815963713728
Experience 5, Iter 15, disc loss: 0.010411426684193915, policy loss: 4.768220676847889
Experience 5, Iter 16, disc loss: 0.009932953839857933, policy loss: 4.812282883545268
Experience 5, Iter 17, disc loss: 0.010354059279041532, policy loss: 4.769522127141464
Experience 5, Iter 18, disc loss: 0.011153343007026115, policy loss: 4.709075571648724
Experience 5, Iter 19, disc loss: 0.009462780113442744, policy loss: 4.839936608725266
Experience 5, Iter 20, disc loss: 0.010252477869573444, policy loss: 4.747564034436464
Experience 5, Iter 21, disc loss: 0.010032027266396928, policy loss: 4.803779884450005
Experience 5, Iter 22, disc loss: 0.008863013761648147, policy loss: 4.954224216554484
Experience 5, Iter 23, disc loss: 0.009176571138561875, policy loss: 4.896268318520307
Experience 5, Iter 24, disc loss: 0.009457470363041471, policy loss: 4.8580588704912655
Experience 5, Iter 25, disc loss: 0.009513287543927735, policy loss: 4.847965396842014
Experience 5, Iter 26, disc loss: 0.009011634431386072, policy loss: 4.894309069892406
Experience 5, Iter 27, disc loss: 0.009507219331983447, policy loss: 4.909725090136127
Experience 5, Iter 28, disc loss: 0.00985263451132632, policy loss: 4.854757368088269
Experience 5, Iter 29, disc loss: 0.009440776837352798, policy loss: 4.905966006688133
Experience 5, Iter 30, disc loss: 0.008758827585006937, policy loss: 4.982560314709957
Experience 5, Iter 31, disc loss: 0.009533059239010233, policy loss: 4.8555895166581
Experience 5, Iter 32, disc loss: 0.009171793107937936, policy loss: 4.872867964278958
Experience 5, Iter 33, disc loss: 0.008343797383582457, policy loss: 5.014846618558423
Experience 5, Iter 34, disc loss: 0.009785601272072541, policy loss: 4.826167197551355
Experience 5, Iter 35, disc loss: 0.008831441855436502, policy loss: 4.940517356195267
Experience 5, Iter 36, disc loss: 0.008743701892544246, policy loss: 4.940352997021072
Experience 5, Iter 37, disc loss: 0.008870065024153892, policy loss: 4.985395703097465
Experience 5, Iter 38, disc loss: 0.008311417468261284, policy loss: 4.982319258685417
Experience 5, Iter 39, disc loss: 0.008220936788523191, policy loss: 5.003665753094674
Experience 5, Iter 40, disc loss: 0.007536827902179199, policy loss: 5.10949794453231
Experience 5, Iter 41, disc loss: 0.00814039647214862, policy loss: 4.989481276970128
Experience 5, Iter 42, disc loss: 0.007980902618112563, policy loss: 5.084664518251996
Experience 5, Iter 43, disc loss: 0.008575140952911722, policy loss: 4.9490698900447665
Experience 5, Iter 44, disc loss: 0.007223110077461711, policy loss: 5.1764449768795355
Experience 5, Iter 45, disc loss: 0.007779391432170621, policy loss: 5.061936988333439
Experience 5, Iter 46, disc loss: 0.008514407025704477, policy loss: 4.956685811031757
Experience 5, Iter 47, disc loss: 0.007256661435014421, policy loss: 5.103532549787988
Experience 5, Iter 48, disc loss: 0.008585162799951574, policy loss: 4.987206047556579
Experience 5, Iter 49, disc loss: 0.007810013222147167, policy loss: 5.07792085648936
Experience 5, Iter 50, disc loss: 0.007445939142568269, policy loss: 5.080094286074768
Experience 5, Iter 51, disc loss: 0.007536271620900906, policy loss: 5.112835629283383
Experience 5, Iter 52, disc loss: 0.0076955970777471995, policy loss: 5.098864950879305
Experience 5, Iter 53, disc loss: 0.00802808664275717, policy loss: 5.018232685648292
Experience 5, Iter 54, disc loss: 0.007607670309060622, policy loss: 5.078435807682134
Experience 5, Iter 55, disc loss: 0.008161753571093998, policy loss: 5.018278101312479
Experience 5, Iter 56, disc loss: 0.007056022900282738, policy loss: 5.15807517979812
Experience 5, Iter 57, disc loss: 0.007041328048274117, policy loss: 5.139570917305743
Experience 5, Iter 58, disc loss: 0.007960103360582026, policy loss: 5.096683464583781
Experience 5, Iter 59, disc loss: 0.007764741058161974, policy loss: 5.103313151167972
Experience 5, Iter 60, disc loss: 0.0074952262869815624, policy loss: 5.079660487317288
Experience 5, Iter 61, disc loss: 0.007023392591828999, policy loss: 5.175398156453047
Experience 5, Iter 62, disc loss: 0.00767669686278645, policy loss: 5.083537220126924
Experience 5, Iter 63, disc loss: 0.006551932524821769, policy loss: 5.27574646258544
Experience 5, Iter 64, disc loss: 0.0066570157876048065, policy loss: 5.2818179146588955
Experience 5, Iter 65, disc loss: 0.006574183423928017, policy loss: 5.2517979478779555
Experience 5, Iter 66, disc loss: 0.006107478191653033, policy loss: 5.327072989382731
Experience 5, Iter 67, disc loss: 0.007094566373164032, policy loss: 5.184194102603849
Experience 5, Iter 68, disc loss: 0.007945816930783453, policy loss: 5.050659949230483
Experience 5, Iter 69, disc loss: 0.0066452108204842885, policy loss: 5.27541871570284
Experience 5, Iter 70, disc loss: 0.007027382506180749, policy loss: 5.225492771784863
Experience 5, Iter 71, disc loss: 0.007155698460662337, policy loss: 5.174647931649951
Experience 5, Iter 72, disc loss: 0.00714867388328902, policy loss: 5.175247383645264
Experience 5, Iter 73, disc loss: 0.006330161532824758, policy loss: 5.31056148297999
Experience 5, Iter 74, disc loss: 0.007520342866982028, policy loss: 5.2038281950509315
Experience 5, Iter 75, disc loss: 0.007507324707691019, policy loss: 5.129798279989356
Experience 5, Iter 76, disc loss: 0.00712448226464613, policy loss: 5.2292359498342105
Experience 5, Iter 77, disc loss: 0.007494197233408341, policy loss: 5.177657353193082
Experience 5, Iter 78, disc loss: 0.006156801835447934, policy loss: 5.3707112089403335
Experience 5, Iter 79, disc loss: 0.006667326937195669, policy loss: 5.291945179197761
Experience 5, Iter 80, disc loss: 0.007057849053147813, policy loss: 5.228218176078839
Experience 5, Iter 81, disc loss: 0.0063133755142918855, policy loss: 5.318942289067757
Experience 5, Iter 82, disc loss: 0.0066335596587330375, policy loss: 5.319873956159656
Experience 5, Iter 83, disc loss: 0.008297872400980807, policy loss: 5.219658308261122
Experience 5, Iter 84, disc loss: 0.0062609851339353075, policy loss: 5.344200500505742
Experience 5, Iter 85, disc loss: 0.005821289608266349, policy loss: 5.382109963475896
Experience 5, Iter 86, disc loss: 0.006094313814244545, policy loss: 5.352681043339954
Experience 5, Iter 87, disc loss: 0.006306655134351637, policy loss: 5.37009707043308
Experience 5, Iter 88, disc loss: 0.006980678747918267, policy loss: 5.189769019520693
Experience 5, Iter 89, disc loss: 0.00649292322426362, policy loss: 5.293565766998289
Experience 5, Iter 90, disc loss: 0.007236680839780809, policy loss: 5.2249816669989855
Experience 5, Iter 91, disc loss: 0.0055828824809183745, policy loss: 5.446153553687567
Experience 5, Iter 92, disc loss: 0.006619514695427873, policy loss: 5.337508766440085
Experience 5, Iter 93, disc loss: 0.007115413617648318, policy loss: 5.219592262179898
Experience 5, Iter 94, disc loss: 0.007656388861136067, policy loss: 5.152483748749683
Experience 5, Iter 95, disc loss: 0.006232830210968144, policy loss: 5.346084464001773
Experience 5, Iter 96, disc loss: 0.007484859100308363, policy loss: 5.352276268473091
Experience 5, Iter 97, disc loss: 0.006365507257296043, policy loss: 5.418551183240684
Experience 5, Iter 98, disc loss: 0.006621624933869293, policy loss: 5.346224102845501
Experience 5, Iter 99, disc loss: 0.0063435591987283544, policy loss: 5.452850787458065
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0035],
        [0.0231],
        [0.0004]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5294e-03, 3.0328e-02, 1.5591e-02, 1.2077e-03, 1.8068e-05,
          1.1532e-01]],

        [[4.5294e-03, 3.0328e-02, 1.5591e-02, 1.2077e-03, 1.8068e-05,
          1.1532e-01]],

        [[4.5294e-03, 3.0328e-02, 1.5591e-02, 1.2077e-03, 1.8068e-05,
          1.1532e-01]],

        [[4.5294e-03, 3.0328e-02, 1.5591e-02, 1.2077e-03, 1.8068e-05,
          1.1532e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0139, 0.0924, 0.0015], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0139, 0.0924, 0.0015])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.663
Iter 2/2000 - Loss: 0.150
Iter 3/2000 - Loss: -3.493
Iter 4/2000 - Loss: -2.988
Iter 5/2000 - Loss: -1.721
Iter 6/2000 - Loss: -2.134
Iter 7/2000 - Loss: -3.121
Iter 8/2000 - Loss: -3.610
Iter 9/2000 - Loss: -3.456
Iter 10/2000 - Loss: -3.074
Iter 11/2000 - Loss: -2.871
Iter 12/2000 - Loss: -2.955
Iter 13/2000 - Loss: -3.201
Iter 14/2000 - Loss: -3.440
Iter 15/2000 - Loss: -3.556
Iter 16/2000 - Loss: -3.522
Iter 17/2000 - Loss: -3.411
Iter 18/2000 - Loss: -3.332
Iter 19/2000 - Loss: -3.350
Iter 20/2000 - Loss: -3.450
Iter 1981/2000 - Loss: -8.378
Iter 1982/2000 - Loss: -8.378
Iter 1983/2000 - Loss: -8.378
Iter 1984/2000 - Loss: -8.378
Iter 1985/2000 - Loss: -8.378
Iter 1986/2000 - Loss: -8.378
Iter 1987/2000 - Loss: -8.378
Iter 1988/2000 - Loss: -8.378
Iter 1989/2000 - Loss: -8.378
Iter 1990/2000 - Loss: -8.378
Iter 1991/2000 - Loss: -8.378
Iter 1992/2000 - Loss: -8.378
Iter 1993/2000 - Loss: -8.378
Iter 1994/2000 - Loss: -8.378
Iter 1995/2000 - Loss: -8.378
Iter 1996/2000 - Loss: -8.378
Iter 1997/2000 - Loss: -8.378
Iter 1998/2000 - Loss: -8.378
Iter 1999/2000 - Loss: -8.378
Iter 2000/2000 - Loss: -8.378
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0026],
        [0.0002]])
Lengthscale: tensor([[[14.5036,  3.2210, 15.7910, 12.9024,  4.4788, 19.3061]],

        [[13.5616, 25.8660, 32.4406,  2.0714,  4.8474, 10.0951]],

        [[23.4099, 43.8376, 18.2746,  1.2707,  4.2878, 13.2148]],

        [[22.6303, 28.4613,  3.3251,  1.3767,  4.5584, 13.0839]]])
Signal Variance: tensor([ 0.0321,  0.7851, 11.1389,  0.0459])
Estimated target variance: tensor([0.0035, 0.0139, 0.0924, 0.0015])
N: 60
Signal to noise ratio: tensor([10.8441, 38.0001, 65.7390, 14.2040])
Bound on condition number: tensor([  7056.7122,  86641.6490, 259298.1346,  12106.2962])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.003759484225907268, policy loss: 5.839688037635666
Experience 6, Iter 1, disc loss: 0.0037499266533667836, policy loss: 5.968177013936669
Experience 6, Iter 2, disc loss: 0.003995084009835946, policy loss: 5.764953469121524
Experience 6, Iter 3, disc loss: 0.003980284914158135, policy loss: 5.721556684173283
Experience 6, Iter 4, disc loss: 0.003867912132123825, policy loss: 5.868707014865029
Experience 6, Iter 5, disc loss: 0.004163144789672771, policy loss: 5.718624117483377
Experience 6, Iter 6, disc loss: 0.003956658798706615, policy loss: 5.756033163449141
Experience 6, Iter 7, disc loss: 0.0041797512972089005, policy loss: 5.698761994503337
Experience 6, Iter 8, disc loss: 0.003908996157901189, policy loss: 5.7787094373043715
Experience 6, Iter 9, disc loss: 0.003986363767348048, policy loss: 5.755606517408168
Experience 6, Iter 10, disc loss: 0.003880262314259447, policy loss: 5.81074765932242
Experience 6, Iter 11, disc loss: 0.0038154281819530764, policy loss: 5.795375943675464
Experience 6, Iter 12, disc loss: 0.0038738028572459977, policy loss: 5.780504473901677
Experience 6, Iter 13, disc loss: 0.00382589596956749, policy loss: 5.7705867918687455
Experience 6, Iter 14, disc loss: 0.0040024146244888315, policy loss: 5.716622196169735
Experience 6, Iter 15, disc loss: 0.0038107105828585506, policy loss: 5.77931043209374
Experience 6, Iter 16, disc loss: 0.0034558592187127314, policy loss: 5.915833180129144
Experience 6, Iter 17, disc loss: 0.003919055225411959, policy loss: 5.764824653039259
Experience 6, Iter 18, disc loss: 0.003934152226834888, policy loss: 5.712711961985296
Experience 6, Iter 19, disc loss: 0.00402739378440823, policy loss: 5.729704698033338
Experience 6, Iter 20, disc loss: 0.003895866763419633, policy loss: 5.755062838929744
Experience 6, Iter 21, disc loss: 0.0037673800792942572, policy loss: 5.7656145663015606
Experience 6, Iter 22, disc loss: 0.004155796599609919, policy loss: 5.644922559981676
Experience 6, Iter 23, disc loss: 0.0037950102433753708, policy loss: 5.79514831878974
Experience 6, Iter 24, disc loss: 0.004006608577741141, policy loss: 5.714977714471721
Experience 6, Iter 25, disc loss: 0.004069088819116103, policy loss: 5.680158352450222
Experience 6, Iter 26, disc loss: 0.004472293593705129, policy loss: 5.595253131754147
Experience 6, Iter 27, disc loss: 0.003917922528249516, policy loss: 5.743937919023846
Experience 6, Iter 28, disc loss: 0.004756138019914682, policy loss: 5.520185253950926
Experience 6, Iter 29, disc loss: 0.0047245773267669735, policy loss: 5.561131600930692
Experience 6, Iter 30, disc loss: 0.004836892068836238, policy loss: 5.526645902938517
Experience 6, Iter 31, disc loss: 0.004744707425218186, policy loss: 5.561613180674401
Experience 6, Iter 32, disc loss: 0.004393529043113141, policy loss: 5.665745321700069
Experience 6, Iter 33, disc loss: 0.004759600001575489, policy loss: 5.596455962934474
Experience 6, Iter 34, disc loss: 0.005225818647225103, policy loss: 5.443701439185339
Experience 6, Iter 35, disc loss: 0.005067900359386084, policy loss: 5.512165037869462
Experience 6, Iter 36, disc loss: 0.005185573989961477, policy loss: 5.494107212179138
Experience 6, Iter 37, disc loss: 0.005078746144652126, policy loss: 5.563967186709058
Experience 6, Iter 38, disc loss: 0.005487047644190746, policy loss: 5.4390711462213055
Experience 6, Iter 39, disc loss: 0.004595255430861839, policy loss: 5.635813279354397
Experience 6, Iter 40, disc loss: 0.00496510575921236, policy loss: 5.54240189151731
Experience 6, Iter 41, disc loss: 0.004801624376860373, policy loss: 5.606898885524014
Experience 6, Iter 42, disc loss: 0.00567512661868313, policy loss: 5.376211088367441
Experience 6, Iter 43, disc loss: 0.0058456110965836045, policy loss: 5.411029591068204
Experience 6, Iter 44, disc loss: 0.004772393929865407, policy loss: 5.644094317482028
Experience 6, Iter 45, disc loss: 0.005243155641071695, policy loss: 5.537557282570788
Experience 6, Iter 46, disc loss: 0.005057860000953209, policy loss: 5.564031526575462
Experience 6, Iter 47, disc loss: 0.00547130438448427, policy loss: 5.487554272513714
Experience 6, Iter 48, disc loss: 0.004805533247503173, policy loss: 5.6153880178749045
Experience 6, Iter 49, disc loss: 0.0054167666733629305, policy loss: 5.486328630132697
Experience 6, Iter 50, disc loss: 0.00546077099581769, policy loss: 5.432707504626701
Experience 6, Iter 51, disc loss: 0.006028305687879156, policy loss: 5.390017424506882
Experience 6, Iter 52, disc loss: 0.005744987993173851, policy loss: 5.469356911764117
Experience 6, Iter 53, disc loss: 0.004784176479587813, policy loss: 5.618156899995522
Experience 6, Iter 54, disc loss: 0.0058535940796512875, policy loss: 5.448536462071322
Experience 6, Iter 55, disc loss: 0.005917489503379806, policy loss: 5.36083009412812
Experience 6, Iter 56, disc loss: 0.005581197236274514, policy loss: 5.460015742630488
Experience 6, Iter 57, disc loss: 0.0051460084582166, policy loss: 5.576481011674479
Experience 6, Iter 58, disc loss: 0.005064296827995715, policy loss: 5.566873872264779
Experience 6, Iter 59, disc loss: 0.005797922045219803, policy loss: 5.52155713084754
Experience 6, Iter 60, disc loss: 0.005400236453200714, policy loss: 5.618129241240791
Experience 6, Iter 61, disc loss: 0.00644077258913836, policy loss: 5.464751340296069
Experience 6, Iter 62, disc loss: 0.0050329100423348325, policy loss: 5.6734476428653995
Experience 6, Iter 63, disc loss: 0.006283740136307715, policy loss: 5.435759300774259
Experience 6, Iter 64, disc loss: 0.007144344253128439, policy loss: 5.2879950312930175
Experience 6, Iter 65, disc loss: 0.005598888441344138, policy loss: 5.560982457666811
Experience 6, Iter 66, disc loss: 0.006180934307084111, policy loss: 5.5443816135311605
Experience 6, Iter 67, disc loss: 0.006375221275622428, policy loss: 5.500876401631995
Experience 6, Iter 68, disc loss: 0.006218548944158201, policy loss: 5.549627728273549
Experience 6, Iter 69, disc loss: 0.00918309302547252, policy loss: 5.413637385019643
Experience 6, Iter 70, disc loss: 0.008134594192093565, policy loss: 5.538712799921537
Experience 6, Iter 71, disc loss: 0.006764262955060334, policy loss: 5.599929794711194
Experience 6, Iter 72, disc loss: 0.006634986840390513, policy loss: 5.5742216798773905
Experience 6, Iter 73, disc loss: 0.008170123671512776, policy loss: 5.541878525999522
Experience 6, Iter 74, disc loss: 0.006641116317168873, policy loss: 5.833135067181937
Experience 6, Iter 75, disc loss: 0.007902369049172285, policy loss: 5.742716525595572
Experience 6, Iter 76, disc loss: 0.007670990808041868, policy loss: 5.645270395604197
Experience 6, Iter 77, disc loss: 0.00814297986200318, policy loss: 5.469428852251306
Experience 6, Iter 78, disc loss: 0.006833046718695573, policy loss: 5.63883785879961
Experience 6, Iter 79, disc loss: 0.00958548063910203, policy loss: 5.607283846356751
Experience 6, Iter 80, disc loss: 0.007702281064501937, policy loss: 5.568972236247703
Experience 6, Iter 81, disc loss: 0.007004670057210867, policy loss: 5.877576003098509
Experience 6, Iter 82, disc loss: 0.010380652714218796, policy loss: 5.498363915234616
Experience 6, Iter 83, disc loss: 0.009807743375357364, policy loss: 5.327392311858908
Experience 6, Iter 84, disc loss: 0.008174878925267838, policy loss: 5.769309670897028
Experience 6, Iter 85, disc loss: 0.008974887941699644, policy loss: 5.470236264955709
Experience 6, Iter 86, disc loss: 0.013588624523711984, policy loss: 5.520129721056412
Experience 6, Iter 87, disc loss: 0.018308566073893466, policy loss: 5.219960897192478
Experience 6, Iter 88, disc loss: 0.007624855975450292, policy loss: 5.974897392403605
Experience 6, Iter 89, disc loss: 0.01753246654726444, policy loss: 5.328624180288948
Experience 6, Iter 90, disc loss: 0.01956685660049413, policy loss: 5.2185175034095295
Experience 6, Iter 91, disc loss: 0.016021390403405958, policy loss: 5.443794653854865
Experience 6, Iter 92, disc loss: 0.015205688870009866, policy loss: 5.441859009327097
Experience 6, Iter 93, disc loss: 0.015587294461310136, policy loss: 5.61267806962485
Experience 6, Iter 94, disc loss: 0.01815999340543418, policy loss: 5.670529178362483
Experience 6, Iter 95, disc loss: 0.01824952725170856, policy loss: 5.770665594759622
Experience 6, Iter 96, disc loss: 0.018185357417427637, policy loss: 6.950710328684934
Experience 6, Iter 97, disc loss: 0.011966791069873275, policy loss: 6.078203090273852
Experience 6, Iter 98, disc loss: 0.015960768934410417, policy loss: 6.310950209134879
Experience 6, Iter 99, disc loss: 0.009169475590666018, policy loss: 6.622170746690807
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0424],
        [0.4115],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.9511e-03, 3.1515e-02, 6.8255e-02, 1.1099e-03, 1.5695e-05,
          7.4477e-01]],

        [[3.9511e-03, 3.1515e-02, 6.8255e-02, 1.1099e-03, 1.5695e-05,
          7.4477e-01]],

        [[3.9511e-03, 3.1515e-02, 6.8255e-02, 1.1099e-03, 1.5695e-05,
          7.4477e-01]],

        [[3.9511e-03, 3.1515e-02, 6.8255e-02, 1.1099e-03, 1.5695e-05,
          7.4477e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0032, 0.1695, 1.6461, 0.0032], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0032, 0.1695, 1.6461, 0.0032])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.686
Iter 2/2000 - Loss: 1.738
Iter 3/2000 - Loss: -0.585
Iter 4/2000 - Loss: -0.256
Iter 5/2000 - Loss: 0.553
Iter 6/2000 - Loss: 0.301
Iter 7/2000 - Loss: -0.325
Iter 8/2000 - Loss: -0.646
Iter 9/2000 - Loss: -0.543
Iter 10/2000 - Loss: -0.271
Iter 11/2000 - Loss: -0.126
Iter 12/2000 - Loss: -0.200
Iter 13/2000 - Loss: -0.394
Iter 14/2000 - Loss: -0.560
Iter 15/2000 - Loss: -0.612
Iter 16/2000 - Loss: -0.560
Iter 17/2000 - Loss: -0.480
Iter 18/2000 - Loss: -0.451
Iter 19/2000 - Loss: -0.504
Iter 20/2000 - Loss: -0.609
Iter 1981/2000 - Loss: -8.241
Iter 1982/2000 - Loss: -8.241
Iter 1983/2000 - Loss: -8.241
Iter 1984/2000 - Loss: -8.241
Iter 1985/2000 - Loss: -8.241
Iter 1986/2000 - Loss: -8.241
Iter 1987/2000 - Loss: -8.241
Iter 1988/2000 - Loss: -8.241
Iter 1989/2000 - Loss: -8.241
Iter 1990/2000 - Loss: -8.241
Iter 1991/2000 - Loss: -8.241
Iter 1992/2000 - Loss: -8.241
Iter 1993/2000 - Loss: -8.241
Iter 1994/2000 - Loss: -8.241
Iter 1995/2000 - Loss: -8.241
Iter 1996/2000 - Loss: -8.241
Iter 1997/2000 - Loss: -8.241
Iter 1998/2000 - Loss: -8.241
Iter 1999/2000 - Loss: -8.241
Iter 2000/2000 - Loss: -8.241
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[14.9051,  3.4389, 42.0635,  9.0095,  4.4773, 34.8012]],

        [[18.5390, 35.9370, 42.0851,  4.1531,  5.4295, 27.5675]],

        [[21.8495, 43.3727, 32.2141,  2.1877,  5.7246, 31.8487]],

        [[20.1014, 37.3542,  6.8226,  2.0721,  5.2495, 26.5675]]])
Signal Variance: tensor([ 0.0399,  4.0961, 34.2195,  0.1411])
Estimated target variance: tensor([0.0032, 0.1695, 1.6461, 0.0032])
N: 70
Signal to noise ratio: tensor([ 12.4149,  91.0660, 115.4791,  22.9115])
Bound on condition number: tensor([ 10790.0827, 580512.7335, 933481.0119,  36746.4878])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.005684260106485942, policy loss: 6.5626707700437255
Experience 7, Iter 1, disc loss: 0.006354615293798175, policy loss: 6.641207213598138
Experience 7, Iter 2, disc loss: 0.006561224936759096, policy loss: 6.605094856140598
Experience 7, Iter 3, disc loss: 0.005113132785217323, policy loss: 6.964277922588751
Experience 7, Iter 4, disc loss: 0.005747347235250977, policy loss: 6.643895287496228
Experience 7, Iter 5, disc loss: 0.004319694414675491, policy loss: 6.874397417177516
Experience 7, Iter 6, disc loss: 0.004233290884045681, policy loss: 6.89148853539531
Experience 7, Iter 7, disc loss: 0.0036863296011896574, policy loss: 7.054765950507349
Experience 7, Iter 8, disc loss: 0.0034454228969986156, policy loss: 6.9354698224494
Experience 7, Iter 9, disc loss: 0.0032499867488964564, policy loss: 7.014694197418926
Experience 7, Iter 10, disc loss: 0.003307284492079555, policy loss: 6.82858281044418
Experience 7, Iter 11, disc loss: 0.002902284976862936, policy loss: 6.8556514816546725
Experience 7, Iter 12, disc loss: 0.0026622867835798883, policy loss: 6.897654157777072
Experience 7, Iter 13, disc loss: 0.0028929744441635354, policy loss: 6.665739123219573
Experience 7, Iter 14, disc loss: 0.0027688361379682603, policy loss: 6.672478345791294
Experience 7, Iter 15, disc loss: 0.002484651682668435, policy loss: 6.73457299433048
Experience 7, Iter 16, disc loss: 0.0026028562594090756, policy loss: 6.651504524789471
Experience 7, Iter 17, disc loss: 0.0026020616181078016, policy loss: 6.5801365828294
Experience 7, Iter 18, disc loss: 0.002573700987332365, policy loss: 6.551846925499561
Experience 7, Iter 19, disc loss: 0.0021110789444618323, policy loss: 6.802163567286794
Experience 7, Iter 20, disc loss: 0.002337722621023284, policy loss: 6.595031544481034
Experience 7, Iter 21, disc loss: 0.0021392392408265274, policy loss: 6.661597846323314
Experience 7, Iter 22, disc loss: 0.0022690780859938933, policy loss: 6.584283447780804
Experience 7, Iter 23, disc loss: 0.0019120276494514964, policy loss: 6.793779034168279
Experience 7, Iter 24, disc loss: 0.0023694098375338394, policy loss: 6.508015223145662
Experience 7, Iter 25, disc loss: 0.002161416337540829, policy loss: 6.623141710714286
Experience 7, Iter 26, disc loss: 0.0021238303717293215, policy loss: 6.616324757250302
Experience 7, Iter 27, disc loss: 0.002058517864026111, policy loss: 6.609231101299753
Experience 7, Iter 28, disc loss: 0.0020443641737481833, policy loss: 6.628132054402949
Experience 7, Iter 29, disc loss: 0.002073808089100187, policy loss: 6.587552920284264
Experience 7, Iter 30, disc loss: 0.0020665861708009992, policy loss: 6.644930423342656
Experience 7, Iter 31, disc loss: 0.0017712509723348824, policy loss: 6.7523755414521505
Experience 7, Iter 32, disc loss: 0.00206236157516607, policy loss: 6.614091747378106
Experience 7, Iter 33, disc loss: 0.0019752673881547143, policy loss: 6.628862783287177
Experience 7, Iter 34, disc loss: 0.001901462204500239, policy loss: 6.674075373576376
Experience 7, Iter 35, disc loss: 0.0022567157530530217, policy loss: 6.6191149425501585
Experience 7, Iter 36, disc loss: 0.002303367827083195, policy loss: 6.490730405342392
Experience 7, Iter 37, disc loss: 0.0021667529556041944, policy loss: 6.543412969824082
Experience 7, Iter 38, disc loss: 0.0017481918715741363, policy loss: 6.754650529706927
Experience 7, Iter 39, disc loss: 0.0020577677592631544, policy loss: 6.5888598595826915
Experience 7, Iter 40, disc loss: 0.0021679680413101946, policy loss: 6.584675613197039
Experience 7, Iter 41, disc loss: 0.0023798675913929788, policy loss: 6.485772960290663
Experience 7, Iter 42, disc loss: 0.001995820914836688, policy loss: 6.646988356854695
Experience 7, Iter 43, disc loss: 0.0021917638934480727, policy loss: 6.611137554099273
Experience 7, Iter 44, disc loss: 0.002275780708576757, policy loss: 6.625814256791602
Experience 7, Iter 45, disc loss: 0.0021846669085128514, policy loss: 6.5800260718473105
Experience 7, Iter 46, disc loss: 0.0021380024181404263, policy loss: 6.620956549584572
Experience 7, Iter 47, disc loss: 0.0022476595291854654, policy loss: 6.632518057715084
Experience 7, Iter 48, disc loss: 0.002372619100933836, policy loss: 6.5720489534769495
Experience 7, Iter 49, disc loss: 0.002170840977404124, policy loss: 6.696390879669099
Experience 7, Iter 50, disc loss: 0.0023579233089522297, policy loss: 6.5581321182944485
Experience 7, Iter 51, disc loss: 0.002940866000418183, policy loss: 6.30370814636448
Experience 7, Iter 52, disc loss: 0.0019638655400132177, policy loss: 6.778059083879759
Experience 7, Iter 53, disc loss: 0.0026884299209237947, policy loss: 6.51424982821505
Experience 7, Iter 54, disc loss: 0.002416519383877064, policy loss: 6.629511398668361
Experience 7, Iter 55, disc loss: 0.0028345933112273076, policy loss: 6.487805052129801
Experience 7, Iter 56, disc loss: 0.0026065253996428515, policy loss: 6.492272764405659
Experience 7, Iter 57, disc loss: 0.0028085973729628376, policy loss: 6.540482896663047
Experience 7, Iter 58, disc loss: 0.002531609407688422, policy loss: 6.7241948200070025
Experience 7, Iter 59, disc loss: 0.0032890185173153944, policy loss: 6.416365360398531
Experience 7, Iter 60, disc loss: 0.003159641149458615, policy loss: 6.378946464427685
Experience 7, Iter 61, disc loss: 0.002658777325877753, policy loss: 6.559649495933986
Experience 7, Iter 62, disc loss: 0.0030288610916705157, policy loss: 6.422847047913109
Experience 7, Iter 63, disc loss: 0.0025033803785845794, policy loss: 6.729335005184627
Experience 7, Iter 64, disc loss: 0.0021809562216493685, policy loss: 6.67883211192262
Experience 7, Iter 65, disc loss: 0.002635881378677754, policy loss: 6.4615483232231625
Experience 7, Iter 66, disc loss: 0.0035012815615670825, policy loss: 6.387014115175026
Experience 7, Iter 67, disc loss: 0.003256624349787094, policy loss: 6.482745334565566
Experience 7, Iter 68, disc loss: 0.0026574649368157894, policy loss: 6.50975494842051
Experience 7, Iter 69, disc loss: 0.0043585604593250095, policy loss: 6.224954619126127
Experience 7, Iter 70, disc loss: 0.0033015434323290594, policy loss: 6.444577280802303
Experience 7, Iter 71, disc loss: 0.0045217539884391455, policy loss: 6.186269453343549
Experience 7, Iter 72, disc loss: 0.0048591371346109555, policy loss: 6.081644436559016
Experience 7, Iter 73, disc loss: 0.0037482526314454156, policy loss: 6.38566896487921
Experience 7, Iter 74, disc loss: 0.003644706875223043, policy loss: 6.5023512012126155
Experience 7, Iter 75, disc loss: 0.003392706619568412, policy loss: 6.567879907074847
Experience 7, Iter 76, disc loss: 0.005739861652002689, policy loss: 5.984168183951925
Experience 7, Iter 77, disc loss: 0.005285278215538428, policy loss: 6.257408062010837
Experience 7, Iter 78, disc loss: 0.004921567942396419, policy loss: 6.235475287619567
Experience 7, Iter 79, disc loss: 0.006215886758811134, policy loss: 6.045799844577915
Experience 7, Iter 80, disc loss: 0.005003324492506798, policy loss: 6.412440325376313
Experience 7, Iter 81, disc loss: 0.00473859426284054, policy loss: 6.320718762515173
Experience 7, Iter 82, disc loss: 0.006464133140008912, policy loss: 6.092599551156995
Experience 7, Iter 83, disc loss: 0.00525794675594415, policy loss: 6.55201583244371
Experience 7, Iter 84, disc loss: 0.007234147676446959, policy loss: 5.987283602777936
Experience 7, Iter 85, disc loss: 0.005539389759512688, policy loss: 6.3458687535764255
Experience 7, Iter 86, disc loss: 0.0071771309960197835, policy loss: 6.251373421191955
Experience 7, Iter 87, disc loss: 0.00611130464265402, policy loss: 6.096784452766748
Experience 7, Iter 88, disc loss: 0.00522796553806349, policy loss: 6.318058845514528
Experience 7, Iter 89, disc loss: 0.006702879252207678, policy loss: 6.291065399943966
Experience 7, Iter 90, disc loss: 0.005078632182140158, policy loss: 6.461114057411549
Experience 7, Iter 91, disc loss: 0.0052082852345347675, policy loss: 6.47253078170635
Experience 7, Iter 92, disc loss: 0.004285705367457709, policy loss: 6.597141025556167
Experience 7, Iter 93, disc loss: 0.006230170483721954, policy loss: 6.326774643820551
Experience 7, Iter 94, disc loss: 0.005962597594469463, policy loss: 6.196686644981584
Experience 7, Iter 95, disc loss: 0.005273916174916463, policy loss: 6.336456146003604
Experience 7, Iter 96, disc loss: 0.006295310661931333, policy loss: 6.116645339590198
Experience 7, Iter 97, disc loss: 0.006200976986810047, policy loss: 6.419737545516396
Experience 7, Iter 98, disc loss: 0.006440336921371145, policy loss: 6.05371466060795
Experience 7, Iter 99, disc loss: 0.00551442678003638, policy loss: 6.3752163292008115
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0499],
        [0.4893],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.4975e-03, 3.0550e-02, 8.2669e-02, 1.0539e-03, 1.3878e-05,
          8.5551e-01]],

        [[3.4975e-03, 3.0550e-02, 8.2669e-02, 1.0539e-03, 1.3878e-05,
          8.5551e-01]],

        [[3.4975e-03, 3.0550e-02, 8.2669e-02, 1.0539e-03, 1.3878e-05,
          8.5551e-01]],

        [[3.4975e-03, 3.0550e-02, 8.2669e-02, 1.0539e-03, 1.3878e-05,
          8.5551e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0030, 0.1995, 1.9571, 0.0036], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0030, 0.1995, 1.9571, 0.0036])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.495
Iter 2/2000 - Loss: 1.976
Iter 3/2000 - Loss: -0.383
Iter 4/2000 - Loss: -0.051
Iter 5/2000 - Loss: 0.778
Iter 6/2000 - Loss: 0.539
Iter 7/2000 - Loss: -0.096
Iter 8/2000 - Loss: -0.441
Iter 9/2000 - Loss: -0.355
Iter 10/2000 - Loss: -0.077
Iter 11/2000 - Loss: 0.091
Iter 12/2000 - Loss: 0.037
Iter 13/2000 - Loss: -0.156
Iter 14/2000 - Loss: -0.337
Iter 15/2000 - Loss: -0.408
Iter 16/2000 - Loss: -0.363
Iter 17/2000 - Loss: -0.269
Iter 18/2000 - Loss: -0.214
Iter 19/2000 - Loss: -0.248
Iter 20/2000 - Loss: -0.352
Iter 1981/2000 - Loss: -8.453
Iter 1982/2000 - Loss: -8.454
Iter 1983/2000 - Loss: -8.454
Iter 1984/2000 - Loss: -8.454
Iter 1985/2000 - Loss: -8.454
Iter 1986/2000 - Loss: -8.454
Iter 1987/2000 - Loss: -8.454
Iter 1988/2000 - Loss: -8.454
Iter 1989/2000 - Loss: -8.454
Iter 1990/2000 - Loss: -8.454
Iter 1991/2000 - Loss: -8.454
Iter 1992/2000 - Loss: -8.454
Iter 1993/2000 - Loss: -8.454
Iter 1994/2000 - Loss: -8.454
Iter 1995/2000 - Loss: -8.454
Iter 1996/2000 - Loss: -8.454
Iter 1997/2000 - Loss: -8.454
Iter 1998/2000 - Loss: -8.454
Iter 1999/2000 - Loss: -8.454
Iter 2000/2000 - Loss: -8.454
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0024],
        [0.0002]])
Lengthscale: tensor([[[ 8.9451,  3.3192, 34.6599,  3.9727,  4.0770, 35.7740]],

        [[19.8297, 40.0298, 44.9818,  5.2372,  5.6771, 34.9112]],

        [[20.0195, 41.0099, 35.1563,  2.1726,  5.2058, 36.7834]],

        [[17.2219, 36.2147,  9.7897,  2.6969,  5.9818, 37.0485]]])
Signal Variance: tensor([ 0.0442,  5.2735, 36.3967,  0.2724])
Estimated target variance: tensor([0.0030, 0.1995, 1.9571, 0.0036])
N: 80
Signal to noise ratio: tensor([ 13.3316, 107.7613, 122.4503,  33.6663])
Bound on condition number: tensor([  14219.5319,  929000.9524, 1199526.2804,   90674.6869])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.004642436704295377, policy loss: 6.585221933594792
Experience 8, Iter 1, disc loss: 0.0069812017300338695, policy loss: 5.87088114650304
Experience 8, Iter 2, disc loss: 0.007052083292334558, policy loss: 6.095447933970151
Experience 8, Iter 3, disc loss: 0.005188898965616081, policy loss: 6.6173869424235106
Experience 8, Iter 4, disc loss: 0.007361356066982268, policy loss: 5.9498900022850725
Experience 8, Iter 5, disc loss: 0.005715298298703297, policy loss: 6.115998576378377
Experience 8, Iter 6, disc loss: 0.0053737008349789535, policy loss: 6.578947679146216
Experience 8, Iter 7, disc loss: 0.005037838448397482, policy loss: 6.561354061085481
Experience 8, Iter 8, disc loss: 0.005146548022789045, policy loss: 6.525048057192857
Experience 8, Iter 9, disc loss: 0.0049670419891315225, policy loss: 6.738680083925924
Experience 8, Iter 10, disc loss: 0.005459755690182925, policy loss: 6.642261196896669
Experience 8, Iter 11, disc loss: 0.005153390332804419, policy loss: 6.385224589287956
Experience 8, Iter 12, disc loss: 0.005215285285106263, policy loss: 6.487012820591811
Experience 8, Iter 13, disc loss: 0.00529304388156359, policy loss: 6.449809618450794
Experience 8, Iter 14, disc loss: 0.005032321306121508, policy loss: 6.4893282406160715
Experience 8, Iter 15, disc loss: 0.004617177298671265, policy loss: 6.567809271178794
Experience 8, Iter 16, disc loss: 0.004586342943978995, policy loss: 6.346552068350986
Experience 8, Iter 17, disc loss: 0.004320104205577139, policy loss: 6.5008485476109215
Experience 8, Iter 18, disc loss: 0.004238509159779141, policy loss: 6.5440615123639425
Experience 8, Iter 19, disc loss: 0.0037401224512520316, policy loss: 6.649520744958995
Experience 8, Iter 20, disc loss: 0.0039461188426992465, policy loss: 6.708353735332913
Experience 8, Iter 21, disc loss: 0.0041598517158830456, policy loss: 6.598262469280069
Experience 8, Iter 22, disc loss: 0.005498386242629887, policy loss: 6.099120306911962
Experience 8, Iter 23, disc loss: 0.006327540839431413, policy loss: 6.055765790201903
Experience 8, Iter 24, disc loss: 0.0068254983771240405, policy loss: 5.943572115153485
Experience 8, Iter 25, disc loss: 0.0056751406133627854, policy loss: 6.242200970491013
Experience 8, Iter 26, disc loss: 0.007800587438988428, policy loss: 5.778829204649
Experience 8, Iter 27, disc loss: 0.0115361153777739, policy loss: 5.259227642216953
Experience 8, Iter 28, disc loss: 0.012698528349835219, policy loss: 5.268158221539126
Experience 8, Iter 29, disc loss: 0.014001694692242618, policy loss: 5.398931010104464
Experience 8, Iter 30, disc loss: 0.012513906889364504, policy loss: 5.43295845800799
Experience 8, Iter 31, disc loss: 0.025022928410767642, policy loss: 4.810576112816437
Experience 8, Iter 32, disc loss: 0.01594924774769102, policy loss: 5.2941939194733125
Experience 8, Iter 33, disc loss: 0.022162984433812535, policy loss: 5.0599904711643
Experience 8, Iter 34, disc loss: 0.017665695833852336, policy loss: 5.562034260841557
Experience 8, Iter 35, disc loss: 0.022078716231997576, policy loss: 4.937885711553312
Experience 8, Iter 36, disc loss: 0.024128893500213276, policy loss: 5.037609221166992
Experience 8, Iter 37, disc loss: 0.01842980896170566, policy loss: 5.289749091798862
Experience 8, Iter 38, disc loss: 0.024138743843334233, policy loss: 4.886159507861671
Experience 8, Iter 39, disc loss: 0.021291944668566164, policy loss: 5.334635774222778
Experience 8, Iter 40, disc loss: 0.02241835783697061, policy loss: 5.415740519545077
Experience 8, Iter 41, disc loss: 0.024740314507991117, policy loss: 5.034666533484444
Experience 8, Iter 42, disc loss: 0.02701213659584067, policy loss: 5.013991342713849
Experience 8, Iter 43, disc loss: 0.022482773476872442, policy loss: 5.535667099204305
Experience 8, Iter 44, disc loss: 0.024609370086189946, policy loss: 5.271377702014729
Experience 8, Iter 45, disc loss: 0.021405420592814625, policy loss: 5.602360290497346
Experience 8, Iter 46, disc loss: 0.02310973494991936, policy loss: 5.325569494347805
Experience 8, Iter 47, disc loss: 0.02503389555334544, policy loss: 5.21644213088698
Experience 8, Iter 48, disc loss: 0.023687860898270956, policy loss: 5.176324205268051
Experience 8, Iter 49, disc loss: 0.027176110603940887, policy loss: 4.736095468260771
Experience 8, Iter 50, disc loss: 0.025218963576642196, policy loss: 5.205505149092341
Experience 8, Iter 51, disc loss: 0.027404671755477283, policy loss: 5.0278071791014405
Experience 8, Iter 52, disc loss: 0.04046936067770261, policy loss: 4.488536652387533
Experience 8, Iter 53, disc loss: 0.029310951841546227, policy loss: 4.6351188866268505
Experience 8, Iter 54, disc loss: 0.03519047239719244, policy loss: 4.9884847029443655
Experience 8, Iter 55, disc loss: 0.04479001930598875, policy loss: 4.674026086196947
Experience 8, Iter 56, disc loss: 0.04054328283357901, policy loss: 4.695686681977273
Experience 8, Iter 57, disc loss: 0.047794893380104356, policy loss: 4.568292603077813
Experience 8, Iter 58, disc loss: 0.06168847942118633, policy loss: 4.657709927424653
Experience 8, Iter 59, disc loss: 0.12199274323264146, policy loss: 3.746107288223251
Experience 8, Iter 60, disc loss: 0.10223153847786305, policy loss: 4.785672442578425
Experience 8, Iter 61, disc loss: 0.1568388018896968, policy loss: 4.7925861789003354
Experience 8, Iter 62, disc loss: 0.14734304237854887, policy loss: 4.907589109322949
Experience 8, Iter 63, disc loss: 0.18011014905982942, policy loss: 4.039412168542184
Experience 8, Iter 64, disc loss: 0.15039461332277188, policy loss: 4.870568138558923
Experience 8, Iter 65, disc loss: 0.17671534970597186, policy loss: 4.126953729963754
Experience 8, Iter 66, disc loss: 0.24266860497968584, policy loss: 3.9687267574279677
Experience 8, Iter 67, disc loss: 0.26495906319526275, policy loss: 4.108360024708487
Experience 8, Iter 68, disc loss: 0.22635575839676442, policy loss: 4.733412461006925
Experience 8, Iter 69, disc loss: 0.23459671967695722, policy loss: 5.227197324947289
Experience 8, Iter 70, disc loss: 0.3664126057706773, policy loss: 5.166966111896198
Experience 8, Iter 71, disc loss: 0.3490271519027113, policy loss: 5.812666087715248
Experience 8, Iter 72, disc loss: 0.46280186602466566, policy loss: 5.464704522681092
Experience 8, Iter 73, disc loss: 0.5136047446317273, policy loss: 4.569169995897126
Experience 8, Iter 74, disc loss: 0.47333566648083475, policy loss: 4.631217047235249
Experience 8, Iter 75, disc loss: 0.34654982786631794, policy loss: 6.1976746740173025
Experience 8, Iter 76, disc loss: 0.3518286949112174, policy loss: 5.455835246019117
Experience 8, Iter 77, disc loss: 0.34575508992908344, policy loss: 6.553570264039269
Experience 8, Iter 78, disc loss: 0.3392441743089195, policy loss: 4.775546565506348
Experience 8, Iter 79, disc loss: 0.2182554338690058, policy loss: 6.8125766402422965
Experience 8, Iter 80, disc loss: 0.16221392173078747, policy loss: 7.617464371880327
Experience 8, Iter 81, disc loss: 0.1961979867994562, policy loss: 6.7030173561970585
Experience 8, Iter 82, disc loss: 0.2481879880909158, policy loss: 5.441804571148948
Experience 8, Iter 83, disc loss: 0.32209152789935164, policy loss: 5.404389650388991
Experience 8, Iter 84, disc loss: 0.2690688330761755, policy loss: 6.484086567581031
Experience 8, Iter 85, disc loss: 0.22713755982866127, policy loss: 8.548677043741124
Experience 8, Iter 86, disc loss: 0.22987020554627163, policy loss: 8.574387953901681
Experience 8, Iter 87, disc loss: 0.18473045623641332, policy loss: 9.655336578128427
Experience 8, Iter 88, disc loss: 0.17807015497804185, policy loss: 8.635887826394066
Experience 8, Iter 89, disc loss: 0.1363647033622438, policy loss: 8.932536682940142
Experience 8, Iter 90, disc loss: 0.0785884416135152, policy loss: 10.493560871627679
Experience 8, Iter 91, disc loss: 0.10623930420909371, policy loss: 8.73549207769589
Experience 8, Iter 92, disc loss: 0.14706683208561921, policy loss: 8.456586967642176
Experience 8, Iter 93, disc loss: 0.10170088273562335, policy loss: 8.738206428162012
Experience 8, Iter 94, disc loss: 0.1010718284401387, policy loss: 9.238290190455986
Experience 8, Iter 95, disc loss: 0.0755801569432837, policy loss: 10.525240054074729
Experience 8, Iter 96, disc loss: 0.07846167809189013, policy loss: 9.891255965138294
Experience 8, Iter 97, disc loss: 0.07869520346677569, policy loss: 9.725186060627639
Experience 8, Iter 98, disc loss: 0.04201783539274104, policy loss: 10.802328217032528
Experience 8, Iter 99, disc loss: 0.06789628186625656, policy loss: 9.360267351352348
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0590],
        [0.5888],
        [0.0036]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.4371e-03, 3.7679e-02, 1.9848e-01, 2.2416e-03, 2.5996e-04,
          1.1827e+00]],

        [[3.4371e-03, 3.7679e-02, 1.9848e-01, 2.2416e-03, 2.5996e-04,
          1.1827e+00]],

        [[3.4371e-03, 3.7679e-02, 1.9848e-01, 2.2416e-03, 2.5996e-04,
          1.1827e+00]],

        [[3.4371e-03, 3.7679e-02, 1.9848e-01, 2.2416e-03, 2.5996e-04,
          1.1827e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.2361, 2.3553, 0.0142], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.2361, 2.3553, 0.0142])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.484
Iter 2/2000 - Loss: 1.742
Iter 3/2000 - Loss: 0.517
Iter 4/2000 - Loss: 0.692
Iter 5/2000 - Loss: 1.119
Iter 6/2000 - Loss: 0.963
Iter 7/2000 - Loss: 0.622
Iter 8/2000 - Loss: 0.473
Iter 9/2000 - Loss: 0.552
Iter 10/2000 - Loss: 0.682
Iter 11/2000 - Loss: 0.719
Iter 12/2000 - Loss: 0.653
Iter 13/2000 - Loss: 0.558
Iter 14/2000 - Loss: 0.489
Iter 15/2000 - Loss: 0.452
Iter 16/2000 - Loss: 0.438
Iter 17/2000 - Loss: 0.431
Iter 18/2000 - Loss: 0.411
Iter 19/2000 - Loss: 0.353
Iter 20/2000 - Loss: 0.253
Iter 1981/2000 - Loss: -8.253
Iter 1982/2000 - Loss: -8.253
Iter 1983/2000 - Loss: -8.253
Iter 1984/2000 - Loss: -8.253
Iter 1985/2000 - Loss: -8.253
Iter 1986/2000 - Loss: -8.253
Iter 1987/2000 - Loss: -8.253
Iter 1988/2000 - Loss: -8.253
Iter 1989/2000 - Loss: -8.253
Iter 1990/2000 - Loss: -8.253
Iter 1991/2000 - Loss: -8.253
Iter 1992/2000 - Loss: -8.253
Iter 1993/2000 - Loss: -8.253
Iter 1994/2000 - Loss: -8.254
Iter 1995/2000 - Loss: -8.254
Iter 1996/2000 - Loss: -8.254
Iter 1997/2000 - Loss: -8.254
Iter 1998/2000 - Loss: -8.254
Iter 1999/2000 - Loss: -8.254
Iter 2000/2000 - Loss: -8.254
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0002]])
Lengthscale: tensor([[[ 6.6203,  3.0452, 24.2311,  4.0013,  1.4801, 34.2334]],

        [[14.5896, 28.1376, 29.1199,  2.9871,  4.2754, 10.9827]],

        [[13.3723, 34.1083, 21.3204,  0.9912,  6.4409, 30.0564]],

        [[13.7798, 28.0068,  9.7178,  3.1739, 10.6343, 33.7427]]])
Signal Variance: tensor([ 0.0379,  1.7503, 24.8694,  0.2226])
Estimated target variance: tensor([0.0037, 0.2361, 2.3553, 0.0142])
N: 90
Signal to noise ratio: tensor([ 12.1910,  64.8176, 101.4388,  30.9033])
Bound on condition number: tensor([ 13376.8664, 378120.1832, 926086.0216,  85952.2693])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.07483682481649595, policy loss: 9.174850643089268
Experience 9, Iter 1, disc loss: 0.10107664633845417, policy loss: 8.159685785331405
Experience 9, Iter 2, disc loss: 0.09025183386206365, policy loss: 9.450368340056746
Experience 9, Iter 3, disc loss: 0.11216798172013773, policy loss: 8.280675098140122
Experience 9, Iter 4, disc loss: 0.10509056539028312, policy loss: 8.039845627752186
Experience 9, Iter 5, disc loss: 0.07130039238751996, policy loss: 8.895608072067049
Experience 9, Iter 6, disc loss: 0.07190961856121117, policy loss: 8.771548677920475
Experience 9, Iter 7, disc loss: 0.10117129840027875, policy loss: 8.47727550527022
Experience 9, Iter 8, disc loss: 0.0663226432588782, policy loss: 9.656067166483375
Experience 9, Iter 9, disc loss: 0.056351240675371256, policy loss: 8.083689781173357
Experience 9, Iter 10, disc loss: 0.06505685763600502, policy loss: 8.745137849105884
Experience 9, Iter 11, disc loss: 0.07416982177591411, policy loss: 8.897881866421953
Experience 9, Iter 12, disc loss: 0.06420502274952236, policy loss: 8.255330079331763
Experience 9, Iter 13, disc loss: 0.05359664968011155, policy loss: 8.612638425222304
Experience 9, Iter 14, disc loss: 0.10595208553388026, policy loss: 7.597884398513349
Experience 9, Iter 15, disc loss: 0.08305997504859321, policy loss: 7.209834672742241
Experience 9, Iter 16, disc loss: 0.041213057292550065, policy loss: 8.302805473006496
Experience 9, Iter 17, disc loss: 0.05004411870616046, policy loss: 7.537372317073441
Experience 9, Iter 18, disc loss: 0.04358740490412068, policy loss: 8.30486689678807
Experience 9, Iter 19, disc loss: 0.041315290011392156, policy loss: 7.877816847326839
Experience 9, Iter 20, disc loss: 0.04036842975880349, policy loss: 8.625077751129696
Experience 9, Iter 21, disc loss: 0.05525730713571771, policy loss: 7.249461580945982
Experience 9, Iter 22, disc loss: 0.05256797101720806, policy loss: 7.244697756654093
Experience 9, Iter 23, disc loss: 0.041954270126663834, policy loss: 6.769789750257923
Experience 9, Iter 24, disc loss: 0.036766763009198206, policy loss: 7.928724195794405
Experience 9, Iter 25, disc loss: 0.03284499628306278, policy loss: 8.181759775965478
Experience 9, Iter 26, disc loss: 0.04014056577265021, policy loss: 6.584948511551094
Experience 9, Iter 27, disc loss: 0.04543390771998228, policy loss: 6.652537149845413
Experience 9, Iter 28, disc loss: 0.04906794498811236, policy loss: 7.143990460319289
Experience 9, Iter 29, disc loss: 0.042440961086898085, policy loss: 6.854519879512699
Experience 9, Iter 30, disc loss: 0.04127008688574454, policy loss: 6.6440737413414706
Experience 9, Iter 31, disc loss: 0.05789758190925888, policy loss: 5.430120292391607
Experience 9, Iter 32, disc loss: 0.03902600103669497, policy loss: 6.91626090423689
Experience 9, Iter 33, disc loss: 0.044766247961514835, policy loss: 6.6286991499664385
Experience 9, Iter 34, disc loss: 0.03460111173612171, policy loss: 7.394504374725635
Experience 9, Iter 35, disc loss: 0.04126927457761525, policy loss: 5.631764085403841
Experience 9, Iter 36, disc loss: 0.03221587657386758, policy loss: 6.298416146573099
Experience 9, Iter 37, disc loss: 0.029565317476352455, policy loss: 5.892904874658221
Experience 9, Iter 38, disc loss: 0.029339652859722176, policy loss: 6.020357010029404
Experience 9, Iter 39, disc loss: 0.029616985151476427, policy loss: 5.607982003875877
Experience 9, Iter 40, disc loss: 0.030719117557881825, policy loss: 5.479005392752205
Experience 9, Iter 41, disc loss: 0.026256763754098286, policy loss: 6.586076613712862
Experience 9, Iter 42, disc loss: 0.023009083904673128, policy loss: 6.14810345651469
Experience 9, Iter 43, disc loss: 0.02424569865546141, policy loss: 6.018165705194854
Experience 9, Iter 44, disc loss: 0.0229384248785875, policy loss: 6.240010218812531
Experience 9, Iter 45, disc loss: 0.017744943374029223, policy loss: 6.261941096718257
Experience 9, Iter 46, disc loss: 0.02047190206073201, policy loss: 6.203513829175549
Experience 9, Iter 47, disc loss: 0.013908264676961955, policy loss: 6.666808397579795
Experience 9, Iter 48, disc loss: 0.015086722974074193, policy loss: 6.73744867202236
Experience 9, Iter 49, disc loss: 0.019231877237285602, policy loss: 6.0522583794222164
Experience 9, Iter 50, disc loss: 0.011652676360144679, policy loss: 6.699235735663349
Experience 9, Iter 51, disc loss: 0.0143012486414519, policy loss: 6.392438289840256
Experience 9, Iter 52, disc loss: 0.01108623659214529, policy loss: 6.637262622564853
Experience 9, Iter 53, disc loss: 0.01497476019098781, policy loss: 5.971352004958103
Experience 9, Iter 54, disc loss: 0.012007585977020773, policy loss: 6.301379917244244
Experience 9, Iter 55, disc loss: 0.011837861936885444, policy loss: 6.382503323022824
Experience 9, Iter 56, disc loss: 0.011036754143610827, policy loss: 6.849810490790817
Experience 9, Iter 57, disc loss: 0.011381729542902275, policy loss: 6.6245951634871485
Experience 9, Iter 58, disc loss: 0.008903457140889054, policy loss: 7.1310807854252385
Experience 9, Iter 59, disc loss: 0.009902346218957806, policy loss: 6.553484062143473
Experience 9, Iter 60, disc loss: 0.008615179476116674, policy loss: 6.842255350682856
Experience 9, Iter 61, disc loss: 0.00978114034086916, policy loss: 6.780076990899214
Experience 9, Iter 62, disc loss: 0.013218020338488257, policy loss: 6.114455925209728
Experience 9, Iter 63, disc loss: 0.01233833060113449, policy loss: 6.224451236427719
Experience 9, Iter 64, disc loss: 0.010044808260397567, policy loss: 6.854435140066196
Experience 9, Iter 65, disc loss: 0.010210882406068019, policy loss: 7.188572361097301
Experience 9, Iter 66, disc loss: 0.010421148834658198, policy loss: 6.6636885540164155
Experience 9, Iter 67, disc loss: 0.011652503507488001, policy loss: 6.655850293954547
Experience 9, Iter 68, disc loss: 0.0123961149741568, policy loss: 6.352390341422154
Experience 9, Iter 69, disc loss: 0.01451495518804629, policy loss: 6.4620187770392095
Experience 9, Iter 70, disc loss: 0.010912516156777862, policy loss: 7.014042704905971
Experience 9, Iter 71, disc loss: 0.01118060023674266, policy loss: 6.807101027215545
Experience 9, Iter 72, disc loss: 0.011795323350744163, policy loss: 6.6512880448927945
Experience 9, Iter 73, disc loss: 0.012691872560300682, policy loss: 6.855775654934874
Experience 9, Iter 74, disc loss: 0.01601348984273117, policy loss: 6.381521753405961
Experience 9, Iter 75, disc loss: 0.020918925035610145, policy loss: 6.167666740938594
Experience 9, Iter 76, disc loss: 0.01733680069080721, policy loss: 6.830593818293849
Experience 9, Iter 77, disc loss: 0.012322442246364263, policy loss: 7.2419281864189715
Experience 9, Iter 78, disc loss: 0.011144393801630487, policy loss: 6.983657023425293
Experience 9, Iter 79, disc loss: 0.011379123965497154, policy loss: 7.000955527562947
Experience 9, Iter 80, disc loss: 0.014591423213046826, policy loss: 6.628730389008363
Experience 9, Iter 81, disc loss: 0.009057535253315907, policy loss: 7.314714448083057
Experience 9, Iter 82, disc loss: 0.011958248061340007, policy loss: 6.418591122619123
Experience 9, Iter 83, disc loss: 0.009904837733424987, policy loss: 7.187222509733487
Experience 9, Iter 84, disc loss: 0.011843773407009858, policy loss: 6.644343279710446
Experience 9, Iter 85, disc loss: 0.010372639099066806, policy loss: 6.570975883066062
Experience 9, Iter 86, disc loss: 0.012797188618790183, policy loss: 6.2931366623192115
Experience 9, Iter 87, disc loss: 0.014695490679350498, policy loss: 6.7446666950962495
Experience 9, Iter 88, disc loss: 0.0132678424720692, policy loss: 6.742909414945681
Experience 9, Iter 89, disc loss: 0.010442686674527957, policy loss: 6.915845042738026
Experience 9, Iter 90, disc loss: 0.009269636400003688, policy loss: 7.016511772891471
Experience 9, Iter 91, disc loss: 0.01126018096076686, policy loss: 7.10528768907516
Experience 9, Iter 92, disc loss: 0.012510988742032547, policy loss: 6.86522627780287
Experience 9, Iter 93, disc loss: 0.012081785372237221, policy loss: 6.974340787281661
Experience 9, Iter 94, disc loss: 0.011295703901987586, policy loss: 6.939190414385754
Experience 9, Iter 95, disc loss: 0.010933680714492489, policy loss: 7.14887920299113
Experience 9, Iter 96, disc loss: 0.008283738642133586, policy loss: 7.543806244089845
Experience 9, Iter 97, disc loss: 0.025539454960753483, policy loss: 7.277279005122388
Experience 9, Iter 98, disc loss: 0.013414673710085798, policy loss: 7.693354155647894
Experience 9, Iter 99, disc loss: 0.025641860816629718, policy loss: 7.936049789368552
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0698],
        [0.7117],
        [0.0057]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.4894e-03, 4.2549e-02, 3.0376e-01, 3.6519e-03, 6.7900e-04,
          1.4417e+00]],

        [[3.4894e-03, 4.2549e-02, 3.0376e-01, 3.6519e-03, 6.7900e-04,
          1.4417e+00]],

        [[3.4894e-03, 4.2549e-02, 3.0376e-01, 3.6519e-03, 6.7900e-04,
          1.4417e+00]],

        [[3.4894e-03, 4.2549e-02, 3.0376e-01, 3.6519e-03, 6.7900e-04,
          1.4417e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.2792, 2.8467, 0.0226], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.2792, 2.8467, 0.0226])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.961
Iter 2/2000 - Loss: 1.997
Iter 3/2000 - Loss: 0.977
Iter 4/2000 - Loss: 1.104
Iter 5/2000 - Loss: 1.467
Iter 6/2000 - Loss: 1.348
Iter 7/2000 - Loss: 1.070
Iter 8/2000 - Loss: 0.939
Iter 9/2000 - Loss: 0.977
Iter 10/2000 - Loss: 1.069
Iter 11/2000 - Loss: 1.116
Iter 12/2000 - Loss: 1.086
Iter 13/2000 - Loss: 1.003
Iter 14/2000 - Loss: 0.909
Iter 15/2000 - Loss: 0.839
Iter 16/2000 - Loss: 0.806
Iter 17/2000 - Loss: 0.789
Iter 18/2000 - Loss: 0.754
Iter 19/2000 - Loss: 0.671
Iter 20/2000 - Loss: 0.541
Iter 1981/2000 - Loss: -8.187
Iter 1982/2000 - Loss: -8.187
Iter 1983/2000 - Loss: -8.187
Iter 1984/2000 - Loss: -8.187
Iter 1985/2000 - Loss: -8.187
Iter 1986/2000 - Loss: -8.187
Iter 1987/2000 - Loss: -8.187
Iter 1988/2000 - Loss: -8.187
Iter 1989/2000 - Loss: -8.187
Iter 1990/2000 - Loss: -8.187
Iter 1991/2000 - Loss: -8.187
Iter 1992/2000 - Loss: -8.187
Iter 1993/2000 - Loss: -8.187
Iter 1994/2000 - Loss: -8.187
Iter 1995/2000 - Loss: -8.187
Iter 1996/2000 - Loss: -8.187
Iter 1997/2000 - Loss: -8.187
Iter 1998/2000 - Loss: -8.187
Iter 1999/2000 - Loss: -8.187
Iter 2000/2000 - Loss: -8.187
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[ 6.6245,  2.9595, 29.7809,  3.1756,  1.3177, 38.9073]],

        [[13.8932, 29.8747, 16.6783,  1.7296,  2.3742, 24.4593]],

        [[12.6286, 33.0182, 20.6343,  0.9658,  7.2749, 24.2826]],

        [[12.7572, 26.7410, 10.2537,  3.0566, 12.4890, 31.9984]]])
Signal Variance: tensor([ 0.0405,  2.2074, 23.2601,  0.2627])
Estimated target variance: tensor([0.0041, 0.2792, 2.8467, 0.0226])
N: 100
Signal to noise ratio: tensor([ 12.8380,  71.3879, 101.7649,  32.7743])
Bound on condition number: tensor([  16482.3262,  509624.0764, 1035610.2974,  107416.3601])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.01395469363885475, policy loss: 7.885090288394202
Experience 10, Iter 1, disc loss: 0.014335661344942175, policy loss: 7.961885517111243
Experience 10, Iter 2, disc loss: 0.030726558544015188, policy loss: 8.1421711107344
Experience 10, Iter 3, disc loss: 0.007774262425289833, policy loss: 9.269184973802753
Experience 10, Iter 4, disc loss: 0.02340777745601668, policy loss: 8.589964252620419
Experience 10, Iter 5, disc loss: 0.06251739191479268, policy loss: 8.75802962602185
Experience 10, Iter 6, disc loss: 0.06146991059610307, policy loss: 8.830228006879464
Experience 10, Iter 7, disc loss: 0.048813755544061505, policy loss: 8.869643599843627
Experience 10, Iter 8, disc loss: 0.04413662376057724, policy loss: 9.894064402428242
Experience 10, Iter 9, disc loss: 0.0659983900227754, policy loss: 7.991418667383501
Experience 10, Iter 10, disc loss: 0.03008678588982923, policy loss: 8.569614956106417
Experience 10, Iter 11, disc loss: 0.04165719974904189, policy loss: 9.130944637606067
Experience 10, Iter 12, disc loss: 0.05197801469753132, policy loss: 8.108871776459601
Experience 10, Iter 13, disc loss: 0.057587313071436726, policy loss: 9.099151009104778
Experience 10, Iter 14, disc loss: 0.03622779286557164, policy loss: 9.097284747685599
Experience 10, Iter 15, disc loss: 0.04405342612048964, policy loss: 8.177118282298606
Experience 10, Iter 16, disc loss: 0.04008307329843412, policy loss: 9.747928108946697
Experience 10, Iter 17, disc loss: 0.03928998568637648, policy loss: 9.357051547599875
Experience 10, Iter 18, disc loss: 0.03973331403069147, policy loss: 8.648475517422936
Experience 10, Iter 19, disc loss: 0.03432112659786823, policy loss: 10.815180832993756
Experience 10, Iter 20, disc loss: 0.03441070000425326, policy loss: 10.24332146284378
Experience 10, Iter 21, disc loss: 0.03628847462456925, policy loss: 8.603342426064073
Experience 10, Iter 22, disc loss: 0.04280144547177925, policy loss: 7.6822138752576015
Experience 10, Iter 23, disc loss: 0.03246700286174337, policy loss: 8.841477127448162
Experience 10, Iter 24, disc loss: 0.043891745372104676, policy loss: 8.231375811569006
Experience 10, Iter 25, disc loss: 0.0553828981762718, policy loss: 6.290674326221284
Experience 10, Iter 26, disc loss: 0.03896433721761217, policy loss: 7.626760716054424
Experience 10, Iter 27, disc loss: 0.04814027466476401, policy loss: 7.479024161402734
Experience 10, Iter 28, disc loss: 0.05319485057438732, policy loss: 7.4467723745553265
Experience 10, Iter 29, disc loss: 0.04008616103886307, policy loss: 7.9101158943376975
Experience 10, Iter 30, disc loss: 0.03624686528979326, policy loss: 8.754259421034519
Experience 10, Iter 31, disc loss: 0.027754579154953708, policy loss: 8.880914658803244
Experience 10, Iter 32, disc loss: 0.03024567572876435, policy loss: 8.811620553699697
Experience 10, Iter 33, disc loss: 0.028441324025058678, policy loss: 8.613811819175272
Experience 10, Iter 34, disc loss: 0.029846107650003124, policy loss: 8.413932187400261
Experience 10, Iter 35, disc loss: 0.023323047386032435, policy loss: 8.877188985886058
Experience 10, Iter 36, disc loss: 0.026526484606946208, policy loss: 7.642491131890992
Experience 10, Iter 37, disc loss: 0.02456270197398885, policy loss: 8.39574604309553
Experience 10, Iter 38, disc loss: 0.021163388110139982, policy loss: 8.52287031015902
Experience 10, Iter 39, disc loss: 0.018444970890483213, policy loss: 9.429668114568216
Experience 10, Iter 40, disc loss: 0.016898854483574186, policy loss: 9.362775344183621
Experience 10, Iter 41, disc loss: 0.014988926855534303, policy loss: 8.637459059376088
Experience 10, Iter 42, disc loss: 0.015147801451587846, policy loss: 9.217147389071561
Experience 10, Iter 43, disc loss: 0.012672847524055782, policy loss: 9.367769032346073
Experience 10, Iter 44, disc loss: 0.011600662743334954, policy loss: 8.93457186302054
Experience 10, Iter 45, disc loss: 0.011670276963623858, policy loss: 8.238426153358178
Experience 10, Iter 46, disc loss: 0.012442818931303036, policy loss: 8.49104227939359
Experience 10, Iter 47, disc loss: 0.015782004748144764, policy loss: 7.980368855249309
Experience 10, Iter 48, disc loss: 0.020197188937456166, policy loss: 7.942792787010575
Experience 10, Iter 49, disc loss: 0.018477046171177035, policy loss: 8.666338003141282
Experience 10, Iter 50, disc loss: 0.010360385110742642, policy loss: 9.456653086997978
Experience 10, Iter 51, disc loss: 0.019233303554831847, policy loss: 9.384530650995337
Experience 10, Iter 52, disc loss: 0.01584163009626774, policy loss: 9.12372803986878
Experience 10, Iter 53, disc loss: 0.016595818538258186, policy loss: 9.640122374059331
Experience 10, Iter 54, disc loss: 0.022642089412496923, policy loss: 8.925037715287024
Experience 10, Iter 55, disc loss: 0.0104722521522632, policy loss: 9.834028972481475
Experience 10, Iter 56, disc loss: 0.012010313582107943, policy loss: 9.49459219472518
Experience 10, Iter 57, disc loss: 0.007896999736151896, policy loss: 11.071616683626363
Experience 10, Iter 58, disc loss: 0.01512211455757826, policy loss: 9.68303286053811
Experience 10, Iter 59, disc loss: 0.006153478933294966, policy loss: 10.50965423758922
Experience 10, Iter 60, disc loss: 0.007426960226549442, policy loss: 9.683445638548207
Experience 10, Iter 61, disc loss: 0.0062834272992151035, policy loss: 10.108777529601713
Experience 10, Iter 62, disc loss: 0.008721045934807773, policy loss: 10.082973581162943
Experience 10, Iter 63, disc loss: 0.005739117717266469, policy loss: 11.095311566196871
Experience 10, Iter 64, disc loss: 0.007763010526986705, policy loss: 9.841561415726558
Experience 10, Iter 65, disc loss: 0.004803711623464641, policy loss: 11.132104520910694
Experience 10, Iter 66, disc loss: 0.0058588612155923816, policy loss: 11.244055118800034
Experience 10, Iter 67, disc loss: 0.011371340022632627, policy loss: 10.737347377569684
Experience 10, Iter 68, disc loss: 0.005177703040842812, policy loss: 11.292828276209159
Experience 10, Iter 69, disc loss: 0.004843066545957644, policy loss: 10.252664483299725
Experience 10, Iter 70, disc loss: 0.005039627466250859, policy loss: 10.062702358192702
Experience 10, Iter 71, disc loss: 0.004297403929986454, policy loss: 11.102864879198975
Experience 10, Iter 72, disc loss: 0.0034198799810421236, policy loss: 10.771771983314121
Experience 10, Iter 73, disc loss: 0.0035109353948269417, policy loss: 10.901481741551073
Experience 10, Iter 74, disc loss: 0.004462160401261994, policy loss: 11.18381639092992
Experience 10, Iter 75, disc loss: 0.004657315448221145, policy loss: 10.471751296259693
Experience 10, Iter 76, disc loss: 0.003949232435109661, policy loss: 11.212065036428541
Experience 10, Iter 77, disc loss: 0.005605323527451831, policy loss: 10.960991779889167
Experience 10, Iter 78, disc loss: 0.003566669513570064, policy loss: 10.200772764550381
Experience 10, Iter 79, disc loss: 0.0025821053611431288, policy loss: 10.70507627541143
Experience 10, Iter 80, disc loss: 0.0026159327817802977, policy loss: 10.416952440501525
Experience 10, Iter 81, disc loss: 0.004978113631808494, policy loss: 10.648078910102708
Experience 10, Iter 82, disc loss: 0.007481940218019951, policy loss: 10.473418620960372
Experience 10, Iter 83, disc loss: 0.002260504566302716, policy loss: 9.840195280345453
Experience 10, Iter 84, disc loss: 0.0022752127641572613, policy loss: 10.86390054673062
Experience 10, Iter 85, disc loss: 0.0035957091297318952, policy loss: 10.456461364716468
Experience 10, Iter 86, disc loss: 0.009775963572945242, policy loss: 9.990538558406325
Experience 10, Iter 87, disc loss: 0.0019522071008357764, policy loss: 10.218989789085384
Experience 10, Iter 88, disc loss: 0.0020673386686768033, policy loss: 9.697958660715443
Experience 10, Iter 89, disc loss: 0.0018851110879044542, policy loss: 10.384107714314855
Experience 10, Iter 90, disc loss: 0.004192476205854031, policy loss: 10.52984742291235
Experience 10, Iter 91, disc loss: 0.0022120493934758498, policy loss: 9.601443333934753
Experience 10, Iter 92, disc loss: 0.002117636040507599, policy loss: 8.939555682818357
Experience 10, Iter 93, disc loss: 0.0020404924029081887, policy loss: 10.046897995198107
Experience 10, Iter 94, disc loss: 0.0024075147717537142, policy loss: 8.810487017322519
Experience 10, Iter 95, disc loss: 0.003172251145847839, policy loss: 9.299002256185656
Experience 10, Iter 96, disc loss: 0.0019055074776215479, policy loss: 9.33434994938225
Experience 10, Iter 97, disc loss: 0.0018054371371648006, policy loss: 9.447786360693545
Experience 10, Iter 98, disc loss: 0.0022866860753539215, policy loss: 9.200841590376005
Experience 10, Iter 99, disc loss: 0.0017979320118950133, policy loss: 10.074132370464799
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0804],
        [0.8804],
        [0.0097]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0036, 0.0649, 0.4800, 0.0073, 0.0050, 1.9002]],

        [[0.0036, 0.0649, 0.4800, 0.0073, 0.0050, 1.9002]],

        [[0.0036, 0.0649, 0.4800, 0.0073, 0.0050, 1.9002]],

        [[0.0036, 0.0649, 0.4800, 0.0073, 0.0050, 1.9002]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0068, 0.3218, 3.5217, 0.0389], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0068, 0.3218, 3.5217, 0.0389])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.661
Iter 2/2000 - Loss: 2.229
Iter 3/2000 - Loss: 1.648
Iter 4/2000 - Loss: 1.702
Iter 5/2000 - Loss: 1.916
Iter 6/2000 - Loss: 1.854
Iter 7/2000 - Loss: 1.678
Iter 8/2000 - Loss: 1.582
Iter 9/2000 - Loss: 1.593
Iter 10/2000 - Loss: 1.640
Iter 11/2000 - Loss: 1.649
Iter 12/2000 - Loss: 1.597
Iter 13/2000 - Loss: 1.506
Iter 14/2000 - Loss: 1.407
Iter 15/2000 - Loss: 1.321
Iter 16/2000 - Loss: 1.250
Iter 17/2000 - Loss: 1.175
Iter 18/2000 - Loss: 1.073
Iter 19/2000 - Loss: 0.931
Iter 20/2000 - Loss: 0.755
Iter 1981/2000 - Loss: -7.987
Iter 1982/2000 - Loss: -7.987
Iter 1983/2000 - Loss: -7.987
Iter 1984/2000 - Loss: -7.987
Iter 1985/2000 - Loss: -7.987
Iter 1986/2000 - Loss: -7.987
Iter 1987/2000 - Loss: -7.987
Iter 1988/2000 - Loss: -7.987
Iter 1989/2000 - Loss: -7.987
Iter 1990/2000 - Loss: -7.987
Iter 1991/2000 - Loss: -7.987
Iter 1992/2000 - Loss: -7.987
Iter 1993/2000 - Loss: -7.987
Iter 1994/2000 - Loss: -7.987
Iter 1995/2000 - Loss: -7.987
Iter 1996/2000 - Loss: -7.987
Iter 1997/2000 - Loss: -7.987
Iter 1998/2000 - Loss: -7.987
Iter 1999/2000 - Loss: -7.987
Iter 2000/2000 - Loss: -7.988
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[ 7.4870,  6.5819, 37.8293,  8.6976, 14.8744, 51.2372]],

        [[13.9025, 29.7354, 15.4205,  1.6577,  1.9336, 20.0225]],

        [[13.7230, 30.0245, 16.9105,  0.9388,  2.1410, 17.7880]],

        [[11.8387, 23.5756, 13.2577,  3.5041,  2.2792, 36.7563]]])
Signal Variance: tensor([ 0.1138,  1.7555, 17.0487,  0.3814])
Estimated target variance: tensor([0.0068, 0.3218, 3.5217, 0.0389])
N: 110
Signal to noise ratio: tensor([21.4482, 65.8823, 88.9488, 39.4003])
Bound on condition number: tensor([ 50603.7133, 477453.9260, 870309.5203, 170763.1805])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.001285077264654161, policy loss: 10.949194216603
Experience 11, Iter 1, disc loss: 0.0012747812099712429, policy loss: 10.691627694871844
Experience 11, Iter 2, disc loss: 0.001244460903620039, policy loss: 10.708456761684982
Experience 11, Iter 3, disc loss: 0.0012110984286239827, policy loss: 10.735345078685697
Experience 11, Iter 4, disc loss: 0.0011813190829374307, policy loss: 10.710607237544302
Experience 11, Iter 5, disc loss: 0.0011518985858522347, policy loss: 10.485297952281613
Experience 11, Iter 6, disc loss: 0.0011543777641108896, policy loss: 10.247314941063738
Experience 11, Iter 7, disc loss: 0.001125539415740827, policy loss: 10.382646322171258
Experience 11, Iter 8, disc loss: 0.001096663938959913, policy loss: 10.344061974874794
Experience 11, Iter 9, disc loss: 0.0010951036380276351, policy loss: 10.35182193290554
Experience 11, Iter 10, disc loss: 0.001118115178631817, policy loss: 10.14727318138143
Experience 11, Iter 11, disc loss: 0.0010551737582815482, policy loss: 10.713894967977216
Experience 11, Iter 12, disc loss: 0.0010850345824200305, policy loss: 10.359426409160369
Experience 11, Iter 13, disc loss: 0.001058039535172124, policy loss: 9.859649124592801
Experience 11, Iter 14, disc loss: 0.0009721195331472981, policy loss: 10.309819515208712
Experience 11, Iter 15, disc loss: 0.000998160287133001, policy loss: 10.36036714877513
Experience 11, Iter 16, disc loss: 0.0009570633780183118, policy loss: 10.701262078326389
Experience 11, Iter 17, disc loss: 0.0010024470017991952, policy loss: 10.369623697128743
Experience 11, Iter 18, disc loss: 0.0009642103226646887, policy loss: 10.627346255731627
Experience 11, Iter 19, disc loss: 0.0009709926752640743, policy loss: 10.9833250994398
Experience 11, Iter 20, disc loss: 0.0008660745136534699, policy loss: 10.284923674930596
Experience 11, Iter 21, disc loss: 0.0009193353257692514, policy loss: 9.7101679495204
Experience 11, Iter 22, disc loss: 0.0008350884052490261, policy loss: 10.222820226421351
Experience 11, Iter 23, disc loss: 0.0008634455949177945, policy loss: 10.023830692747302
Experience 11, Iter 24, disc loss: 0.0008439950003571407, policy loss: 10.317449890205257
Experience 11, Iter 25, disc loss: 0.0009355225275813476, policy loss: 9.601632446539451
Experience 11, Iter 26, disc loss: 0.0007801635457922973, policy loss: 9.830231053965829
Experience 11, Iter 27, disc loss: 0.0008520568647380634, policy loss: 9.824940008147363
Experience 11, Iter 28, disc loss: 0.0008906443517491568, policy loss: 9.721277026119534
Experience 11, Iter 29, disc loss: 0.0009500671870129433, policy loss: 9.517315185641749
Experience 11, Iter 30, disc loss: 0.0009653666348606059, policy loss: 9.409385203724431
Experience 11, Iter 31, disc loss: 0.0011911567617645534, policy loss: 9.580354054804731
Experience 11, Iter 32, disc loss: 0.0009004971981491092, policy loss: 10.209301540477039
Experience 11, Iter 33, disc loss: 0.0008545949732606878, policy loss: 9.901463235938179
Experience 11, Iter 34, disc loss: 0.0010854166796585405, policy loss: 9.245827681573108
Experience 11, Iter 35, disc loss: 0.0011394530134446544, policy loss: 9.255694465812454
Experience 11, Iter 36, disc loss: 0.0008653118927091586, policy loss: 9.946579837533013
Experience 11, Iter 37, disc loss: 0.00287894560497493, policy loss: 9.178725901284178
Experience 11, Iter 38, disc loss: 0.001099574805950321, policy loss: 8.820904685091302
Experience 11, Iter 39, disc loss: 0.0014250599839262862, policy loss: 9.098977556060657
Experience 11, Iter 40, disc loss: 0.0009779609023204436, policy loss: 9.035571610407038
Experience 11, Iter 41, disc loss: 0.0012457779096209458, policy loss: 9.044970574563475
Experience 11, Iter 42, disc loss: 0.0010072181189003239, policy loss: 9.263401735923157
Experience 11, Iter 43, disc loss: 0.0009421990155899711, policy loss: 9.368784893009611
Experience 11, Iter 44, disc loss: 0.0008709722982392799, policy loss: 9.336256641158032
Experience 11, Iter 45, disc loss: 0.0010691135914008203, policy loss: 8.81016715646441
Experience 11, Iter 46, disc loss: 0.0011169371728694494, policy loss: 9.086836964146567
Experience 11, Iter 47, disc loss: 0.000869116879251423, policy loss: 8.83234416897783
Experience 11, Iter 48, disc loss: 0.0008637488150594093, policy loss: 9.221577792497344
Experience 11, Iter 49, disc loss: 0.0009825762709551678, policy loss: 9.046224234074973
Experience 11, Iter 50, disc loss: 0.0008344353388141804, policy loss: 9.10995703948178
Experience 11, Iter 51, disc loss: 0.0008471205403549243, policy loss: 9.021644189116351
Experience 11, Iter 52, disc loss: 0.0010007243270245484, policy loss: 9.639996834949827
Experience 11, Iter 53, disc loss: 0.0009843657597526684, policy loss: 9.090882813533234
Experience 11, Iter 54, disc loss: 0.001130899392687553, policy loss: 8.982955029258639
Experience 11, Iter 55, disc loss: 0.0008629600822714931, policy loss: 8.922294572344903
Experience 11, Iter 56, disc loss: 0.0010371621083004722, policy loss: 8.790916934120943
Experience 11, Iter 57, disc loss: 0.0011640487241227934, policy loss: 8.974144624092975
Experience 11, Iter 58, disc loss: 0.0009380017202166307, policy loss: 9.140050782154326
Experience 11, Iter 59, disc loss: 0.000979480849007457, policy loss: 9.169335428886043
Experience 11, Iter 60, disc loss: 0.0015810022520332022, policy loss: 9.176704369058099
Experience 11, Iter 61, disc loss: 0.0010420959231440142, policy loss: 8.723234290372718
Experience 11, Iter 62, disc loss: 0.0011450746247792022, policy loss: 8.853465458992552
Experience 11, Iter 63, disc loss: 0.0010481698045025217, policy loss: 8.85217629082956
Experience 11, Iter 64, disc loss: 0.0009168394959424596, policy loss: 8.778173085957203
Experience 11, Iter 65, disc loss: 0.002280193630109494, policy loss: 8.382912358824548
Experience 11, Iter 66, disc loss: 0.0010338278633354012, policy loss: 8.84589065268364
Experience 11, Iter 67, disc loss: 0.0014447102404786514, policy loss: 8.606164779476691
Experience 11, Iter 68, disc loss: 0.0012090989216736897, policy loss: 8.53563847416648
Experience 11, Iter 69, disc loss: 0.0013720070242364008, policy loss: 8.242881112015285
Experience 11, Iter 70, disc loss: 0.0008962931219934974, policy loss: 9.186335139590142
Experience 11, Iter 71, disc loss: 0.0013285876805769429, policy loss: 8.602350452496928
Experience 11, Iter 72, disc loss: 0.0010948639682227354, policy loss: 9.100706315989182
Experience 11, Iter 73, disc loss: 0.002030004512973807, policy loss: 8.894545745122654
Experience 11, Iter 74, disc loss: 0.0015425626355395404, policy loss: 8.625441636646139
Experience 11, Iter 75, disc loss: 0.0012570432978459855, policy loss: 9.083033870042438
Experience 11, Iter 76, disc loss: 0.0010985373834357798, policy loss: 9.292032245469787
Experience 11, Iter 77, disc loss: 0.0011361894938124398, policy loss: 8.688402539089964
Experience 11, Iter 78, disc loss: 0.0011597496265753817, policy loss: 8.66840094959834
Experience 11, Iter 79, disc loss: 0.0009778875441213543, policy loss: 8.882045697658654
Experience 11, Iter 80, disc loss: 0.0031911841826038584, policy loss: 8.498658995912646
Experience 11, Iter 81, disc loss: 0.001164371971703861, policy loss: 8.625240426736346
Experience 11, Iter 82, disc loss: 0.0013739642456896421, policy loss: 8.775669413221777
Experience 11, Iter 83, disc loss: 0.0019351380698384893, policy loss: 9.329385954009435
Experience 11, Iter 84, disc loss: 0.002510727437500428, policy loss: 9.04658790522258
Experience 11, Iter 85, disc loss: 0.0009663843408658808, policy loss: 9.275857609715272
Experience 11, Iter 86, disc loss: 0.0015387189263772916, policy loss: 9.014309564942353
Experience 11, Iter 87, disc loss: 0.01971387153389523, policy loss: 8.882589873198683
Experience 11, Iter 88, disc loss: 0.0010684782866666606, policy loss: 8.723685759224027
Experience 11, Iter 89, disc loss: 0.0011053189217087134, policy loss: 9.236620820613847
Experience 11, Iter 90, disc loss: 0.0009982264882233516, policy loss: 9.311766748309385
Experience 11, Iter 91, disc loss: 0.0010664786173004528, policy loss: 9.017078326325544
Experience 11, Iter 92, disc loss: 0.0010619145244754216, policy loss: 9.39150494719173
Experience 11, Iter 93, disc loss: 0.001194347397653223, policy loss: 9.245929227846345
Experience 11, Iter 94, disc loss: 0.0010680703352265596, policy loss: 9.11465632803707
Experience 11, Iter 95, disc loss: 0.0012433764346547402, policy loss: 9.035923468942514
Experience 11, Iter 96, disc loss: 0.0010063815687183183, policy loss: 9.458100203544523
Experience 11, Iter 97, disc loss: 0.002365145235789632, policy loss: 8.778419393830273
Experience 11, Iter 98, disc loss: 0.000991519921131164, policy loss: 9.848938956701097
Experience 11, Iter 99, disc loss: 0.0012884951633832587, policy loss: 9.013260677246622
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0949],
        [1.0516],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0045, 0.0732, 0.7099, 0.0106, 0.0083, 2.1276]],

        [[0.0045, 0.0732, 0.7099, 0.0106, 0.0083, 2.1276]],

        [[0.0045, 0.0732, 0.7099, 0.0106, 0.0083, 2.1276]],

        [[0.0045, 0.0732, 0.7099, 0.0106, 0.0083, 2.1276]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0074, 0.3798, 4.2064, 0.0584], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0074, 0.3798, 4.2064, 0.0584])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.073
Iter 2/2000 - Loss: 2.580
Iter 3/2000 - Loss: 2.058
Iter 4/2000 - Loss: 2.100
Iter 5/2000 - Loss: 2.281
Iter 6/2000 - Loss: 2.237
Iter 7/2000 - Loss: 2.075
Iter 8/2000 - Loss: 1.960
Iter 9/2000 - Loss: 1.943
Iter 10/2000 - Loss: 1.973
Iter 11/2000 - Loss: 1.972
Iter 12/2000 - Loss: 1.907
Iter 13/2000 - Loss: 1.795
Iter 14/2000 - Loss: 1.672
Iter 15/2000 - Loss: 1.560
Iter 16/2000 - Loss: 1.460
Iter 17/2000 - Loss: 1.356
Iter 18/2000 - Loss: 1.228
Iter 19/2000 - Loss: 1.065
Iter 20/2000 - Loss: 0.867
Iter 1981/2000 - Loss: -7.591
Iter 1982/2000 - Loss: -7.591
Iter 1983/2000 - Loss: -7.591
Iter 1984/2000 - Loss: -7.591
Iter 1985/2000 - Loss: -7.591
Iter 1986/2000 - Loss: -7.591
Iter 1987/2000 - Loss: -7.591
Iter 1988/2000 - Loss: -7.591
Iter 1989/2000 - Loss: -7.591
Iter 1990/2000 - Loss: -7.591
Iter 1991/2000 - Loss: -7.591
Iter 1992/2000 - Loss: -7.591
Iter 1993/2000 - Loss: -7.591
Iter 1994/2000 - Loss: -7.591
Iter 1995/2000 - Loss: -7.591
Iter 1996/2000 - Loss: -7.591
Iter 1997/2000 - Loss: -7.591
Iter 1998/2000 - Loss: -7.591
Iter 1999/2000 - Loss: -7.592
Iter 2000/2000 - Loss: -7.592
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 7.6226,  5.1869, 45.1735,  4.7327,  5.2214, 48.4223]],

        [[14.3441, 29.3220, 10.3502,  1.5219,  1.3329, 26.1154]],

        [[14.7196, 29.8436, 10.5843,  1.2086,  1.2880, 20.1072]],

        [[11.9443, 24.3755, 15.3271,  3.5179,  1.5683, 44.9254]]])
Signal Variance: tensor([ 0.0769,  2.4357, 20.9902,  0.5568])
Estimated target variance: tensor([0.0074, 0.3798, 4.2064, 0.0584])
N: 120
Signal to noise ratio: tensor([18.1005, 76.5392, 99.7298, 46.5505])
Bound on condition number: tensor([  39316.4921,  702991.0506, 1193524.6227,  260034.5364])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.003133258772396368, policy loss: 7.521707546878957
Experience 12, Iter 1, disc loss: 0.014394931082445616, policy loss: 7.2952292687484315
Experience 12, Iter 2, disc loss: 0.018773571596795838, policy loss: 5.870502172785033
Experience 12, Iter 3, disc loss: 0.01966017572390119, policy loss: 5.697695535893685
Experience 12, Iter 4, disc loss: 0.016537103440548217, policy loss: 6.087706795093061
Experience 12, Iter 5, disc loss: 0.03600307538217869, policy loss: 5.536289478718356
Experience 12, Iter 6, disc loss: 0.03461151786117494, policy loss: 6.1533231885666675
Experience 12, Iter 7, disc loss: 0.024322841796769645, policy loss: 6.925252920582976
Experience 12, Iter 8, disc loss: 0.027398262489622446, policy loss: 6.505592865986663
Experience 12, Iter 9, disc loss: 0.02769595479768834, policy loss: 6.765043053133247
Experience 12, Iter 10, disc loss: 0.012547582294820675, policy loss: 7.387263930441737
Experience 12, Iter 11, disc loss: 0.011996355955724876, policy loss: 8.369508707386723
Experience 12, Iter 12, disc loss: 0.011690658183039425, policy loss: 8.376524294274326
Experience 12, Iter 13, disc loss: 0.014270445965181483, policy loss: 8.404491566386195
Experience 12, Iter 14, disc loss: 0.01546206758230239, policy loss: 8.724912865040576
Experience 12, Iter 15, disc loss: 0.01736920669078897, policy loss: 8.578435739788818
Experience 12, Iter 16, disc loss: 0.01762117374646213, policy loss: 8.183397304047196
Experience 12, Iter 17, disc loss: 0.017229519470295968, policy loss: 9.149230772179545
Experience 12, Iter 18, disc loss: 0.01711409774717409, policy loss: 9.422941295690727
Experience 12, Iter 19, disc loss: 0.016621005372901423, policy loss: 9.600229838332005
Experience 12, Iter 20, disc loss: 0.015785417706567377, policy loss: 8.837588632370084
Experience 12, Iter 21, disc loss: 0.015493142709238406, policy loss: 8.287217915734075
Experience 12, Iter 22, disc loss: 0.013496295328999472, policy loss: 8.625460069743191
Experience 12, Iter 23, disc loss: 0.01240020843246418, policy loss: 8.431685301743272
Experience 12, Iter 24, disc loss: 0.01378232473801814, policy loss: 8.092964624494162
Experience 12, Iter 25, disc loss: 0.011405990019058313, policy loss: 7.691210991175362
Experience 12, Iter 26, disc loss: 0.01170063766978632, policy loss: 7.872655837779192
Experience 12, Iter 27, disc loss: 0.008517175687329752, policy loss: 8.186859095988448
Experience 12, Iter 28, disc loss: 0.015785625585901984, policy loss: 8.556063411543683
Experience 12, Iter 29, disc loss: 0.012308833812490648, policy loss: 8.53258056654889
Experience 12, Iter 30, disc loss: 0.011078922010128207, policy loss: 7.350555476786051
Experience 12, Iter 31, disc loss: 0.013967279031942963, policy loss: 7.474015575864565
Experience 12, Iter 32, disc loss: 0.0074211443395432435, policy loss: 8.050498554065129
Experience 12, Iter 33, disc loss: 0.01148868118631697, policy loss: 7.266840535525999
Experience 12, Iter 34, disc loss: 0.008707806640008385, policy loss: 7.869349684892605
Experience 12, Iter 35, disc loss: 0.0064452205801734126, policy loss: 7.7461075605805405
Experience 12, Iter 36, disc loss: 0.007883474008153864, policy loss: 8.04207542826847
Experience 12, Iter 37, disc loss: 0.007343399874199612, policy loss: 7.836126848328208
Experience 12, Iter 38, disc loss: 0.009683643774806482, policy loss: 7.330844015704453
Experience 12, Iter 39, disc loss: 0.009409292934183735, policy loss: 7.6896427804796526
Experience 12, Iter 40, disc loss: 0.01508419202499161, policy loss: 8.014913420208657
Experience 12, Iter 41, disc loss: 0.011045919622385717, policy loss: 7.291320609479857
Experience 12, Iter 42, disc loss: 0.0055412705474720825, policy loss: 8.435667604925086
Experience 12, Iter 43, disc loss: 0.008216534195617312, policy loss: 8.056990677798428
Experience 12, Iter 44, disc loss: 0.012445951353123385, policy loss: 8.567754069949885
Experience 12, Iter 45, disc loss: 0.0055656162956340405, policy loss: 8.787501255478322
Experience 12, Iter 46, disc loss: 0.013023361549384169, policy loss: 7.467332932687077
Experience 12, Iter 47, disc loss: 0.00793364365859608, policy loss: 8.671429291643246
Experience 12, Iter 48, disc loss: 0.00707503131908521, policy loss: 8.462882544419443
Experience 12, Iter 49, disc loss: 0.01203165386122419, policy loss: 7.852822409499087
Experience 12, Iter 50, disc loss: 0.007016915151652606, policy loss: 8.109163623778073
Experience 12, Iter 51, disc loss: 0.009174860250813762, policy loss: 8.454491007208379
Experience 12, Iter 52, disc loss: 0.008925914588346757, policy loss: 8.23565200168591
Experience 12, Iter 53, disc loss: 0.006000965082356124, policy loss: 9.068992001993696
Experience 12, Iter 54, disc loss: 0.007261982858472264, policy loss: 8.536580725186855
Experience 12, Iter 55, disc loss: 0.006003971872058736, policy loss: 8.591718795261162
Experience 12, Iter 56, disc loss: 0.008086654707475256, policy loss: 8.742055024451693
Experience 12, Iter 57, disc loss: 0.00719097690173348, policy loss: 8.938440964040689
Experience 12, Iter 58, disc loss: 0.006082531185314653, policy loss: 8.801606477827725
Experience 12, Iter 59, disc loss: 0.0062890327083640965, policy loss: 9.03133607690092
Experience 12, Iter 60, disc loss: 0.005431071424996756, policy loss: 8.996605980714168
Experience 12, Iter 61, disc loss: 0.005727268845899372, policy loss: 9.77236449548783
Experience 12, Iter 62, disc loss: 0.0059045260130683366, policy loss: 8.478969389940088
Experience 12, Iter 63, disc loss: 0.012896891395739036, policy loss: 8.630336249141779
Experience 12, Iter 64, disc loss: 0.0084058630179153, policy loss: 8.647770028893374
Experience 12, Iter 65, disc loss: 0.006589691613695766, policy loss: 8.395963766142476
Experience 12, Iter 66, disc loss: 0.005550285152745125, policy loss: 9.014453603782526
Experience 12, Iter 67, disc loss: 0.0055020366638920855, policy loss: 8.927859168607007
Experience 12, Iter 68, disc loss: 0.008037476692187906, policy loss: 9.728242278202899
Experience 12, Iter 69, disc loss: 0.01009923971598033, policy loss: 8.716670156327805
Experience 12, Iter 70, disc loss: 0.007062759673992988, policy loss: 8.903892616599034
Experience 12, Iter 71, disc loss: 0.01196867493043672, policy loss: 8.221754551229697
Experience 12, Iter 72, disc loss: 0.0046534090628206695, policy loss: 9.234595424871408
Experience 12, Iter 73, disc loss: 0.00754046278254592, policy loss: 8.432729646838126
Experience 12, Iter 74, disc loss: 0.005704861456057657, policy loss: 8.914622458472302
Experience 12, Iter 75, disc loss: 0.0050914408337692845, policy loss: 8.481322739848395
Experience 12, Iter 76, disc loss: 0.005816156282658793, policy loss: 8.382317347735293
Experience 12, Iter 77, disc loss: 0.004811460306267818, policy loss: 9.093811108417126
Experience 12, Iter 78, disc loss: 0.0054713662646498715, policy loss: 9.953711175908683
Experience 12, Iter 79, disc loss: 0.0061847235561421346, policy loss: 8.709360756350234
Experience 12, Iter 80, disc loss: 0.004604559671401055, policy loss: 9.005724319863067
Experience 12, Iter 81, disc loss: 0.0037317389998128073, policy loss: 9.7549511845864
Experience 12, Iter 82, disc loss: 0.004867797785763093, policy loss: 8.936030440945126
Experience 12, Iter 83, disc loss: 0.0069879973619802905, policy loss: 8.2801982079681
Experience 12, Iter 84, disc loss: 0.003906553043244163, policy loss: 9.090586268723587
Experience 12, Iter 85, disc loss: 0.0070903888976359825, policy loss: 8.794071833107346
Experience 12, Iter 86, disc loss: 0.004708810462040431, policy loss: 8.418563434621536
Experience 12, Iter 87, disc loss: 0.006752144637647649, policy loss: 8.625962974873833
Experience 12, Iter 88, disc loss: 0.004565531287890228, policy loss: 9.013759862938226
Experience 12, Iter 89, disc loss: 0.006686935339312694, policy loss: 8.305390650426304
Experience 12, Iter 90, disc loss: 0.004561369643514772, policy loss: 9.078507081772399
Experience 12, Iter 91, disc loss: 0.007177367410855007, policy loss: 8.831667814732
Experience 12, Iter 92, disc loss: 0.005031416903610021, policy loss: 8.526426696961614
Experience 12, Iter 93, disc loss: 0.004306956312379958, policy loss: 9.446544952798153
Experience 12, Iter 94, disc loss: 0.004220558688986305, policy loss: 8.813100466596875
Experience 12, Iter 95, disc loss: 0.007268558789737671, policy loss: 8.698967092276082
Experience 12, Iter 96, disc loss: 0.004135602179898255, policy loss: 9.010763466331325
Experience 12, Iter 97, disc loss: 0.003788097138365216, policy loss: 9.465743022567953
Experience 12, Iter 98, disc loss: 0.0068533216762140334, policy loss: 8.564620220884176
Experience 12, Iter 99, disc loss: 0.005092608911868666, policy loss: 9.306183899479741
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.1130],
        [1.1561],
        [0.0206]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0046, 0.0840, 0.9631, 0.0131, 0.0124, 2.4760]],

        [[0.0046, 0.0840, 0.9631, 0.0131, 0.0124, 2.4760]],

        [[0.0046, 0.0840, 0.9631, 0.0131, 0.0124, 2.4760]],

        [[0.0046, 0.0840, 0.9631, 0.0131, 0.0124, 2.4760]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0084, 0.4522, 4.6243, 0.0823], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0084, 0.4522, 4.6243, 0.0823])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.444
Iter 2/2000 - Loss: 2.845
Iter 3/2000 - Loss: 2.414
Iter 4/2000 - Loss: 2.458
Iter 5/2000 - Loss: 2.590
Iter 6/2000 - Loss: 2.532
Iter 7/2000 - Loss: 2.400
Iter 8/2000 - Loss: 2.307
Iter 9/2000 - Loss: 2.282
Iter 10/2000 - Loss: 2.288
Iter 11/2000 - Loss: 2.266
Iter 12/2000 - Loss: 2.191
Iter 13/2000 - Loss: 2.076
Iter 14/2000 - Loss: 1.953
Iter 15/2000 - Loss: 1.842
Iter 16/2000 - Loss: 1.741
Iter 17/2000 - Loss: 1.632
Iter 18/2000 - Loss: 1.495
Iter 19/2000 - Loss: 1.323
Iter 20/2000 - Loss: 1.122
Iter 1981/2000 - Loss: -7.494
Iter 1982/2000 - Loss: -7.494
Iter 1983/2000 - Loss: -7.494
Iter 1984/2000 - Loss: -7.494
Iter 1985/2000 - Loss: -7.494
Iter 1986/2000 - Loss: -7.494
Iter 1987/2000 - Loss: -7.494
Iter 1988/2000 - Loss: -7.494
Iter 1989/2000 - Loss: -7.494
Iter 1990/2000 - Loss: -7.494
Iter 1991/2000 - Loss: -7.495
Iter 1992/2000 - Loss: -7.495
Iter 1993/2000 - Loss: -7.495
Iter 1994/2000 - Loss: -7.495
Iter 1995/2000 - Loss: -7.495
Iter 1996/2000 - Loss: -7.495
Iter 1997/2000 - Loss: -7.495
Iter 1998/2000 - Loss: -7.495
Iter 1999/2000 - Loss: -7.495
Iter 2000/2000 - Loss: -7.495
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.4312,  6.0311, 51.4600,  4.8492,  5.5890, 50.0571]],

        [[12.9205, 30.8048, 10.3481,  1.5521,  1.5124, 26.7852]],

        [[14.8316, 32.5416, 10.1347,  1.1145,  1.0523, 18.9900]],

        [[13.3194, 24.7608, 12.9654,  3.4402,  2.0499, 43.7594]]])
Signal Variance: tensor([ 0.0854,  2.7447, 17.3993,  0.5428])
Estimated target variance: tensor([0.0084, 0.4522, 4.6243, 0.0823])
N: 130
Signal to noise ratio: tensor([19.4085, 80.6378, 89.0614, 45.2303])
Bound on condition number: tensor([  48970.6132,  845319.2247, 1031151.9247,  265952.8604])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.009871343776043854, policy loss: 7.1483221732244875
Experience 13, Iter 1, disc loss: 0.013657254400675445, policy loss: 7.049934442072066
Experience 13, Iter 2, disc loss: 0.008214565658745294, policy loss: 7.277115472159034
Experience 13, Iter 3, disc loss: 0.006209019770665064, policy loss: 7.907033723592089
Experience 13, Iter 4, disc loss: 0.01342510340263575, policy loss: 7.317911026814333
Experience 13, Iter 5, disc loss: 0.011140344367629229, policy loss: 7.462349842318278
Experience 13, Iter 6, disc loss: 0.0073806706950775335, policy loss: 7.703783239433252
Experience 13, Iter 7, disc loss: 0.008343612802430803, policy loss: 7.615515307494173
Experience 13, Iter 8, disc loss: 0.00881138569679669, policy loss: 8.366503990432957
Experience 13, Iter 9, disc loss: 0.006059087308897836, policy loss: 7.705971927431036
Experience 13, Iter 10, disc loss: 0.010012112589394849, policy loss: 8.010508891695155
Experience 13, Iter 11, disc loss: 0.008199752016926775, policy loss: 8.07844209710455
Experience 13, Iter 12, disc loss: 0.005823507671457243, policy loss: 8.659185793953569
Experience 13, Iter 13, disc loss: 0.011018220123696222, policy loss: 7.427042345151491
Experience 13, Iter 14, disc loss: 0.006390821235276926, policy loss: 8.560506564623255
Experience 13, Iter 15, disc loss: 0.00969850987368692, policy loss: 7.377401253548475
Experience 13, Iter 16, disc loss: 0.008181893884727108, policy loss: 8.966425630789914
Experience 13, Iter 17, disc loss: 0.007531349920382693, policy loss: 8.42994283859209
Experience 13, Iter 18, disc loss: 0.009181357058631798, policy loss: 8.925340449476293
Experience 13, Iter 19, disc loss: 0.006845878363747445, policy loss: 8.684555829922227
Experience 13, Iter 20, disc loss: 0.006762368039916382, policy loss: 8.210314719211109
Experience 13, Iter 21, disc loss: 0.0065856007090195415, policy loss: 8.378678809752042
Experience 13, Iter 22, disc loss: 0.006105702708429004, policy loss: 9.022905925200526
Experience 13, Iter 23, disc loss: 0.008298088882131887, policy loss: 9.233314527996352
Experience 13, Iter 24, disc loss: 0.005680596180119371, policy loss: 8.445686572202606
Experience 13, Iter 25, disc loss: 0.008050391543976127, policy loss: 8.779024119939784
Experience 13, Iter 26, disc loss: 0.0065882402418475, policy loss: 8.658312148712437
Experience 13, Iter 27, disc loss: 0.006618224133817403, policy loss: 7.851114170335624
Experience 13, Iter 28, disc loss: 0.005204364579625396, policy loss: 8.079020188543224
Experience 13, Iter 29, disc loss: 0.00866284782565642, policy loss: 8.397793789005016
Experience 13, Iter 30, disc loss: 0.0053027897436070445, policy loss: 8.519589962303563
Experience 13, Iter 31, disc loss: 0.007133490632123136, policy loss: 9.07468325744897
Experience 13, Iter 32, disc loss: 0.0069061918266404045, policy loss: 8.200634982506129
Experience 13, Iter 33, disc loss: 0.007695172737355118, policy loss: 8.332596172265209
Experience 13, Iter 34, disc loss: 0.008131213832734605, policy loss: 8.982491009239636
Experience 13, Iter 35, disc loss: 0.006508778123905178, policy loss: 8.24779034832903
Experience 13, Iter 36, disc loss: 0.004450489700024324, policy loss: 9.179452430936678
Experience 13, Iter 37, disc loss: 0.006747024281920242, policy loss: 8.714064664598407
Experience 13, Iter 38, disc loss: 0.005359632272517179, policy loss: 8.802212636090378
Experience 13, Iter 39, disc loss: 0.003916253881987147, policy loss: 8.94895783879579
Experience 13, Iter 40, disc loss: 0.004278072603756147, policy loss: 9.546281443708054
Experience 13, Iter 41, disc loss: 0.005843623435300933, policy loss: 8.624078967194544
Experience 13, Iter 42, disc loss: 0.005959231056696782, policy loss: 9.062958660685581
Experience 13, Iter 43, disc loss: 0.004560068004260849, policy loss: 9.129162314556133
Experience 13, Iter 44, disc loss: 0.006550755311861906, policy loss: 8.429524783835205
Experience 13, Iter 45, disc loss: 0.005712250725297345, policy loss: 8.96148187635884
Experience 13, Iter 46, disc loss: 0.005357824199940202, policy loss: 8.660282747611959
Experience 13, Iter 47, disc loss: 0.004422358666970446, policy loss: 8.914649046505396
Experience 13, Iter 48, disc loss: 0.00397236000081058, policy loss: 8.77985083512347
Experience 13, Iter 49, disc loss: 0.0040603446100524205, policy loss: 8.332927363421788
Experience 13, Iter 50, disc loss: 0.007436495905148901, policy loss: 8.659933817043953
Experience 13, Iter 51, disc loss: 0.0037825597040377443, policy loss: 9.143796827660974
Experience 13, Iter 52, disc loss: 0.0037811825527326827, policy loss: 8.841299416181307
Experience 13, Iter 53, disc loss: 0.0037157024345598677, policy loss: 9.01231228008367
Experience 13, Iter 54, disc loss: 0.0037923306742996352, policy loss: 8.650587578865512
Experience 13, Iter 55, disc loss: 0.005363133961810621, policy loss: 9.395050201009473
Experience 13, Iter 56, disc loss: 0.007545371866992726, policy loss: 8.420936693401636
Experience 13, Iter 57, disc loss: 0.004283712416043337, policy loss: 8.240795750179098
Experience 13, Iter 58, disc loss: 0.004873063773951867, policy loss: 8.600661753398532
Experience 13, Iter 59, disc loss: 0.004147784001612249, policy loss: 8.732624776723085
Experience 13, Iter 60, disc loss: 0.003978436153052256, policy loss: 8.684390154333613
Experience 13, Iter 61, disc loss: 0.009790075396879936, policy loss: 8.60023304221081
Experience 13, Iter 62, disc loss: 0.0043459317329705375, policy loss: 9.595205674820301
Experience 13, Iter 63, disc loss: 0.00544057181647423, policy loss: 8.173033195922997
Experience 13, Iter 64, disc loss: 0.004815356214584825, policy loss: 9.060921329226531
Experience 13, Iter 65, disc loss: 0.004018469285448165, policy loss: 9.308682414131553
Experience 13, Iter 66, disc loss: 0.004866439935396091, policy loss: 8.7340685325828
Experience 13, Iter 67, disc loss: 0.0037206750673199436, policy loss: 8.938988297265842
Experience 13, Iter 68, disc loss: 0.004522298260660814, policy loss: 8.573312934124724
Experience 13, Iter 69, disc loss: 0.0039324167387454634, policy loss: 8.475555207119385
Experience 13, Iter 70, disc loss: 0.004348544633011141, policy loss: 9.155765894900023
Experience 13, Iter 71, disc loss: 0.0061405885891054664, policy loss: 8.68513284624159
Experience 13, Iter 72, disc loss: 0.004350481197199687, policy loss: 8.602181752690944
Experience 13, Iter 73, disc loss: 0.0037838922153536126, policy loss: 8.58675516254799
Experience 13, Iter 74, disc loss: 0.004049206958974309, policy loss: 9.61372323870889
Experience 13, Iter 75, disc loss: 0.003265776098945997, policy loss: 9.708159205268306
Experience 13, Iter 76, disc loss: 0.0051889976258984475, policy loss: 8.890131798064779
Experience 13, Iter 77, disc loss: 0.0039074671072891936, policy loss: 9.58011709754909
Experience 13, Iter 78, disc loss: 0.006128987157537243, policy loss: 8.91582183880389
Experience 13, Iter 79, disc loss: 0.0054832820421337105, policy loss: 8.886159347602042
Experience 13, Iter 80, disc loss: 0.004418312106943497, policy loss: 9.715348564416018
Experience 13, Iter 81, disc loss: 0.0036427282994640803, policy loss: 9.536572551877988
Experience 13, Iter 82, disc loss: 0.003869737854922512, policy loss: 8.598097781808155
Experience 13, Iter 83, disc loss: 0.003033451688659825, policy loss: 9.156157827165451
Experience 13, Iter 84, disc loss: 0.0037428311800314696, policy loss: 9.086604257098475
Experience 13, Iter 85, disc loss: 0.0040566624785494465, policy loss: 9.339138929943946
Experience 13, Iter 86, disc loss: 0.0036275722319469247, policy loss: 8.99828150226224
Experience 13, Iter 87, disc loss: 0.0036742489546178645, policy loss: 9.321083279210129
Experience 13, Iter 88, disc loss: 0.003183169785488317, policy loss: 8.851171020155007
Experience 13, Iter 89, disc loss: 0.004750408859196478, policy loss: 8.838806680235653
Experience 13, Iter 90, disc loss: 0.0054316376581695575, policy loss: 9.2406183345634
Experience 13, Iter 91, disc loss: 0.0031202954300959523, policy loss: 9.869759135298855
Experience 13, Iter 92, disc loss: 0.003353574383028555, policy loss: 9.057654123925325
Experience 13, Iter 93, disc loss: 0.0034582338406738, policy loss: 9.401591436626756
Experience 13, Iter 94, disc loss: 0.0034796283356401427, policy loss: 9.324365470565226
Experience 13, Iter 95, disc loss: 0.005144479195091896, policy loss: 9.025989384880829
Experience 13, Iter 96, disc loss: 0.003569456806593099, policy loss: 8.82485510200756
Experience 13, Iter 97, disc loss: 0.003339803608859388, policy loss: 9.629150722309644
Experience 13, Iter 98, disc loss: 0.004480963455508756, policy loss: 9.40355032761071
Experience 13, Iter 99, disc loss: 0.004851265203444721, policy loss: 9.16606586210972
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.1271],
        [1.2466],
        [0.0233]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0049, 0.0897, 1.0903, 0.0154, 0.0149, 2.7822]],

        [[0.0049, 0.0897, 1.0903, 0.0154, 0.0149, 2.7822]],

        [[0.0049, 0.0897, 1.0903, 0.0154, 0.0149, 2.7822]],

        [[0.0049, 0.0897, 1.0903, 0.0154, 0.0149, 2.7822]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0089, 0.5082, 4.9864, 0.0932], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0089, 0.5082, 4.9864, 0.0932])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.641
Iter 2/2000 - Loss: 2.968
Iter 3/2000 - Loss: 2.590
Iter 4/2000 - Loss: 2.639
Iter 5/2000 - Loss: 2.743
Iter 6/2000 - Loss: 2.668
Iter 7/2000 - Loss: 2.543
Iter 8/2000 - Loss: 2.468
Iter 9/2000 - Loss: 2.450
Iter 10/2000 - Loss: 2.442
Iter 11/2000 - Loss: 2.396
Iter 12/2000 - Loss: 2.302
Iter 13/2000 - Loss: 2.183
Iter 14/2000 - Loss: 2.068
Iter 15/2000 - Loss: 1.964
Iter 16/2000 - Loss: 1.861
Iter 17/2000 - Loss: 1.739
Iter 18/2000 - Loss: 1.583
Iter 19/2000 - Loss: 1.398
Iter 20/2000 - Loss: 1.194
Iter 1981/2000 - Loss: -7.463
Iter 1982/2000 - Loss: -7.463
Iter 1983/2000 - Loss: -7.464
Iter 1984/2000 - Loss: -7.464
Iter 1985/2000 - Loss: -7.464
Iter 1986/2000 - Loss: -7.464
Iter 1987/2000 - Loss: -7.464
Iter 1988/2000 - Loss: -7.464
Iter 1989/2000 - Loss: -7.464
Iter 1990/2000 - Loss: -7.464
Iter 1991/2000 - Loss: -7.464
Iter 1992/2000 - Loss: -7.464
Iter 1993/2000 - Loss: -7.464
Iter 1994/2000 - Loss: -7.464
Iter 1995/2000 - Loss: -7.464
Iter 1996/2000 - Loss: -7.464
Iter 1997/2000 - Loss: -7.464
Iter 1998/2000 - Loss: -7.464
Iter 1999/2000 - Loss: -7.464
Iter 2000/2000 - Loss: -7.464
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.5659,  6.1052, 50.4870,  5.2050,  5.9185, 50.7104]],

        [[13.5562, 30.5243, 11.3213,  1.3225,  1.8475, 28.1635]],

        [[12.5205, 31.1621, 10.8482,  1.0996,  0.9905, 21.7441]],

        [[12.8918, 25.3635, 15.9603,  2.7625,  1.8148, 42.9280]]])
Signal Variance: tensor([ 0.0850,  3.3824, 21.8430,  0.5387])
Estimated target variance: tensor([0.0089, 0.5082, 4.9864, 0.0932])
N: 140
Signal to noise ratio: tensor([19.8591, 85.5126, 97.7601, 45.7106])
Bound on condition number: tensor([  55214.5998, 1023736.6131, 1337986.0578,  292525.4583])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0024830454261671976, policy loss: 9.631419153496285
Experience 14, Iter 1, disc loss: 0.002395427127974712, policy loss: 9.427011796924631
Experience 14, Iter 2, disc loss: 0.0030737340018474955, policy loss: 9.129520653406688
Experience 14, Iter 3, disc loss: 0.0022493128021372757, policy loss: 10.001650871672114
Experience 14, Iter 4, disc loss: 0.0025330354058733635, policy loss: 8.792778439954967
Experience 14, Iter 5, disc loss: 0.0028710142718521795, policy loss: 9.386273687727485
Experience 14, Iter 6, disc loss: 0.0027099145622771453, policy loss: 9.570461545572451
Experience 14, Iter 7, disc loss: 0.0019666791317819155, policy loss: 10.132209154245437
Experience 14, Iter 8, disc loss: 0.002235834049658271, policy loss: 10.42410255950709
Experience 14, Iter 9, disc loss: 0.0026389228094511356, policy loss: 9.435222482737197
Experience 14, Iter 10, disc loss: 0.005216130557301274, policy loss: 8.13464405534405
Experience 14, Iter 11, disc loss: 0.0037333439715953732, policy loss: 9.475568280131885
Experience 14, Iter 12, disc loss: 0.002877853256934613, policy loss: 9.282545117894035
Experience 14, Iter 13, disc loss: 0.0021429401989613308, policy loss: 8.780158252009539
Experience 14, Iter 14, disc loss: 0.003212962659335108, policy loss: 9.621879607004658
Experience 14, Iter 15, disc loss: 0.004161620496902644, policy loss: 9.561276318037239
Experience 14, Iter 16, disc loss: 0.0028860941365498214, policy loss: 9.373817606638479
Experience 14, Iter 17, disc loss: 0.003827064714792071, policy loss: 9.91994653039825
Experience 14, Iter 18, disc loss: 0.002407787274139349, policy loss: 9.510457974082582
Experience 14, Iter 19, disc loss: 0.002626439877243019, policy loss: 8.896286321061162
Experience 14, Iter 20, disc loss: 0.002713897621237433, policy loss: 9.337313367324938
Experience 14, Iter 21, disc loss: 0.0020567234167054847, policy loss: 9.397829063629871
Experience 14, Iter 22, disc loss: 0.0032219679970783505, policy loss: 8.967110582480382
Experience 14, Iter 23, disc loss: 0.004038793500543113, policy loss: 9.070916192464146
Experience 14, Iter 24, disc loss: 0.0032170201902996746, policy loss: 9.295309731164146
Experience 14, Iter 25, disc loss: 0.0022742186358723905, policy loss: 9.218859146725835
Experience 14, Iter 26, disc loss: 0.0027147171829344065, policy loss: 8.816090376471017
Experience 14, Iter 27, disc loss: 0.002465454601018118, policy loss: 8.873170615126305
Experience 14, Iter 28, disc loss: 0.003206492887397764, policy loss: 9.90449873081829
Experience 14, Iter 29, disc loss: 0.002579856891346373, policy loss: 9.549817335938743
Experience 14, Iter 30, disc loss: 0.0023824039214343767, policy loss: 9.210756919984123
Experience 14, Iter 31, disc loss: 0.002111857731716245, policy loss: 9.523645523342957
Experience 14, Iter 32, disc loss: 0.0019045451381532528, policy loss: 9.694447886886792
Experience 14, Iter 33, disc loss: 0.0030552502175153814, policy loss: 8.837267183425332
Experience 14, Iter 34, disc loss: 0.0025076425192868336, policy loss: 9.975675682774249
Experience 14, Iter 35, disc loss: 0.0023348113992043, policy loss: 9.533751430258736
Experience 14, Iter 36, disc loss: 0.003719638518915663, policy loss: 8.750395293831367
Experience 14, Iter 37, disc loss: 0.0028094093914166817, policy loss: 8.368376430275099
Experience 14, Iter 38, disc loss: 0.0021564415558489604, policy loss: 10.212032163885034
Experience 14, Iter 39, disc loss: 0.0035568228636394406, policy loss: 8.663043921057676
Experience 14, Iter 40, disc loss: 0.002290763597036013, policy loss: 9.075864535071425
Experience 14, Iter 41, disc loss: 0.0019108024777375536, policy loss: 10.223844166192697
Experience 14, Iter 42, disc loss: 0.0025717349599336586, policy loss: 9.01342807916908
Experience 14, Iter 43, disc loss: 0.0024248519030142424, policy loss: 10.215556212924941
Experience 14, Iter 44, disc loss: 0.0032612992591501285, policy loss: 8.962344584787544
Experience 14, Iter 45, disc loss: 0.001930119324324277, policy loss: 9.958728487783436
Experience 14, Iter 46, disc loss: 0.002272780434802551, policy loss: 9.04171808447532
Experience 14, Iter 47, disc loss: 0.0021020955083358933, policy loss: 10.005085295826552
Experience 14, Iter 48, disc loss: 0.00216242163295461, policy loss: 9.306568651820157
Experience 14, Iter 49, disc loss: 0.001977673952670921, policy loss: 9.72571050479389
Experience 14, Iter 50, disc loss: 0.0015925086399261506, policy loss: 9.588241710326844
Experience 14, Iter 51, disc loss: 0.0022193833354497376, policy loss: 9.564667578514749
Experience 14, Iter 52, disc loss: 0.0020478465943047264, policy loss: 9.596988653799908
Experience 14, Iter 53, disc loss: 0.0018859248700002255, policy loss: 10.131906389350725
Experience 14, Iter 54, disc loss: 0.001752979279365842, policy loss: 9.641979509466827
Experience 14, Iter 55, disc loss: 0.0016100584449042965, policy loss: 9.803966905677004
Experience 14, Iter 56, disc loss: 0.002389257298437761, policy loss: 9.306464849299177
Experience 14, Iter 57, disc loss: 0.001981045308779161, policy loss: 9.615836526484468
Experience 14, Iter 58, disc loss: 0.0018530395223149653, policy loss: 9.31105398212095
Experience 14, Iter 59, disc loss: 0.0015727771195396076, policy loss: 9.710734117415235
Experience 14, Iter 60, disc loss: 0.0017380760975525666, policy loss: 9.129445572196131
Experience 14, Iter 61, disc loss: 0.0022467440297316366, policy loss: 9.35521660808299
Experience 14, Iter 62, disc loss: 0.0020350758345416262, policy loss: 9.406923358317218
Experience 14, Iter 63, disc loss: 0.00325140028303111, policy loss: 8.668710959566974
Experience 14, Iter 64, disc loss: 0.0034895163547160974, policy loss: 8.858386363943957
Experience 14, Iter 65, disc loss: 0.0018942391138897642, policy loss: 8.924082120301081
Experience 14, Iter 66, disc loss: 0.002138994522540059, policy loss: 9.572428790190882
Experience 14, Iter 67, disc loss: 0.00355657985811296, policy loss: 9.333386197325058
Experience 14, Iter 68, disc loss: 0.002447717260714601, policy loss: 9.028107620457119
Experience 14, Iter 69, disc loss: 0.0020547631418835442, policy loss: 9.319865369851197
Experience 14, Iter 70, disc loss: 0.004433138981711456, policy loss: 8.771686658249585
Experience 14, Iter 71, disc loss: 0.001701843412475491, policy loss: 9.807782897320582
Experience 14, Iter 72, disc loss: 0.0022719540161875865, policy loss: 9.290661754142038
Experience 14, Iter 73, disc loss: 0.0018322646259615737, policy loss: 9.139182512491445
Experience 14, Iter 74, disc loss: 0.003068909078935184, policy loss: 9.049824637298597
Experience 14, Iter 75, disc loss: 0.0026008913292968547, policy loss: 9.477147625852837
Experience 14, Iter 76, disc loss: 0.0023547705026036795, policy loss: 9.092548756521914
Experience 14, Iter 77, disc loss: 0.002378334204544155, policy loss: 9.234697190364137
Experience 14, Iter 78, disc loss: 0.002406208466621401, policy loss: 9.069520492461814
Experience 14, Iter 79, disc loss: 0.0018245882437283628, policy loss: 9.325550985774099
Experience 14, Iter 80, disc loss: 0.002964152566909805, policy loss: 8.502029976063389
Experience 14, Iter 81, disc loss: 0.00240915675869859, policy loss: 9.163994946688842
Experience 14, Iter 82, disc loss: 0.0023804323620176494, policy loss: 9.186177510224063
Experience 14, Iter 83, disc loss: 0.003034911704985714, policy loss: 9.182333376122497
Experience 14, Iter 84, disc loss: 0.0028502800169589743, policy loss: 8.590301783847108
Experience 14, Iter 85, disc loss: 0.002335348302905241, policy loss: 9.002633687606199
Experience 14, Iter 86, disc loss: 0.0022143272162676733, policy loss: 9.883681348927597
Experience 14, Iter 87, disc loss: 0.0022606784788900894, policy loss: 9.283092280779229
Experience 14, Iter 88, disc loss: 0.0031387816132455292, policy loss: 9.511791943196359
Experience 14, Iter 89, disc loss: 0.003168606527265027, policy loss: 9.337550305949087
Experience 14, Iter 90, disc loss: 0.002801095207280946, policy loss: 9.734623683750089
Experience 14, Iter 91, disc loss: 0.0032097093841741515, policy loss: 9.666945656278873
Experience 14, Iter 92, disc loss: 0.0035167782420781122, policy loss: 9.471345814839555
Experience 14, Iter 93, disc loss: 0.0022106906739471155, policy loss: 9.503267194509231
Experience 14, Iter 94, disc loss: 0.0023531344206789103, policy loss: 9.19577112489216
Experience 14, Iter 95, disc loss: 0.002087532125686119, policy loss: 10.497666181032711
Experience 14, Iter 96, disc loss: 0.0019709907873635725, policy loss: 9.368294922091632
Experience 14, Iter 97, disc loss: 0.002172129200184873, policy loss: 9.314637986379234
Experience 14, Iter 98, disc loss: 0.002661656666344594, policy loss: 9.46646147972331
Experience 14, Iter 99, disc loss: 0.0025750117716754595, policy loss: 8.99684994802144
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1414],
        [1.3303],
        [0.0261]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0049, 0.0952, 1.2111, 0.0172, 0.0173, 3.0741]],

        [[0.0049, 0.0952, 1.2111, 0.0172, 0.0173, 3.0741]],

        [[0.0049, 0.0952, 1.2111, 0.0172, 0.0173, 3.0741]],

        [[0.0049, 0.0952, 1.2111, 0.0172, 0.0173, 3.0741]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0094, 0.5656, 5.3210, 0.1044], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0094, 0.5656, 5.3210, 0.1044])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.815
Iter 2/2000 - Loss: 3.090
Iter 3/2000 - Loss: 2.742
Iter 4/2000 - Loss: 2.793
Iter 5/2000 - Loss: 2.882
Iter 6/2000 - Loss: 2.796
Iter 7/2000 - Loss: 2.669
Iter 8/2000 - Loss: 2.601
Iter 9/2000 - Loss: 2.589
Iter 10/2000 - Loss: 2.574
Iter 11/2000 - Loss: 2.511
Iter 12/2000 - Loss: 2.401
Iter 13/2000 - Loss: 2.276
Iter 14/2000 - Loss: 2.162
Iter 15/2000 - Loss: 2.059
Iter 16/2000 - Loss: 1.948
Iter 17/2000 - Loss: 1.805
Iter 18/2000 - Loss: 1.625
Iter 19/2000 - Loss: 1.414
Iter 20/2000 - Loss: 1.186
Iter 1981/2000 - Loss: -7.472
Iter 1982/2000 - Loss: -7.472
Iter 1983/2000 - Loss: -7.472
Iter 1984/2000 - Loss: -7.472
Iter 1985/2000 - Loss: -7.472
Iter 1986/2000 - Loss: -7.472
Iter 1987/2000 - Loss: -7.472
Iter 1988/2000 - Loss: -7.472
Iter 1989/2000 - Loss: -7.472
Iter 1990/2000 - Loss: -7.472
Iter 1991/2000 - Loss: -7.472
Iter 1992/2000 - Loss: -7.472
Iter 1993/2000 - Loss: -7.472
Iter 1994/2000 - Loss: -7.472
Iter 1995/2000 - Loss: -7.472
Iter 1996/2000 - Loss: -7.472
Iter 1997/2000 - Loss: -7.472
Iter 1998/2000 - Loss: -7.472
Iter 1999/2000 - Loss: -7.472
Iter 2000/2000 - Loss: -7.472
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 7.9262,  5.5503, 44.8651,  5.6524,  6.1321, 53.8803]],

        [[13.1623, 30.3202, 11.3235,  1.3829,  1.8893, 27.4589]],

        [[13.5843, 30.1773, 11.8238,  1.1116,  1.0222, 22.3001]],

        [[12.0241, 25.0755, 16.5280,  2.8225,  1.7558, 43.7529]]])
Signal Variance: tensor([ 0.0786,  3.5301, 25.7102,  0.5534])
Estimated target variance: tensor([0.0094, 0.5656, 5.3210, 0.1044])
N: 150
Signal to noise ratio: tensor([ 18.5713,  88.3555, 101.2141,  45.1538])
Bound on condition number: tensor([  51735.1054, 1171004.6955, 1536645.6601,  305831.1321])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0021947472007976592, policy loss: 9.536370927475057
Experience 15, Iter 1, disc loss: 0.0023805380728674276, policy loss: 9.563354901870172
Experience 15, Iter 2, disc loss: 0.0018450751211658866, policy loss: 9.592567738354095
Experience 15, Iter 3, disc loss: 0.0024033958735913807, policy loss: 9.743153185846728
Experience 15, Iter 4, disc loss: 0.0021134029323226014, policy loss: 9.658256928680924
Experience 15, Iter 5, disc loss: 0.0035878296845367036, policy loss: 9.433235375774457
Experience 15, Iter 6, disc loss: 0.0023436222290377697, policy loss: 9.955188465515976
Experience 15, Iter 7, disc loss: 0.0023092276762413625, policy loss: 10.110570650829048
Experience 15, Iter 8, disc loss: 0.002121526850013069, policy loss: 9.304595725789426
Experience 15, Iter 9, disc loss: 0.002062050304710749, policy loss: 9.804457060144262
Experience 15, Iter 10, disc loss: 0.0017570747730327745, policy loss: 9.620739619002391
Experience 15, Iter 11, disc loss: 0.002503731828631154, policy loss: 9.474208596627905
Experience 15, Iter 12, disc loss: 0.002353292249211992, policy loss: 10.302010121954671
Experience 15, Iter 13, disc loss: 0.0022519598477992934, policy loss: 9.48289897628569
Experience 15, Iter 14, disc loss: 0.003546847515160639, policy loss: 9.549614046850365
Experience 15, Iter 15, disc loss: 0.0018469066465987456, policy loss: 9.846801943587007
Experience 15, Iter 16, disc loss: 0.0019757906219167224, policy loss: 10.104388007210206
Experience 15, Iter 17, disc loss: 0.0023742864843886618, policy loss: 9.475592137529244
Experience 15, Iter 18, disc loss: 0.001995560145527652, policy loss: 10.11967723304906
Experience 15, Iter 19, disc loss: 0.002118672137488062, policy loss: 9.953961972979194
Experience 15, Iter 20, disc loss: 0.0020560239668834957, policy loss: 10.403091444818227
Experience 15, Iter 21, disc loss: 0.0018094977504157532, policy loss: 10.377139559871692
Experience 15, Iter 22, disc loss: 0.0015208441803717763, policy loss: 10.246999506223887
Experience 15, Iter 23, disc loss: 0.0017758558471089934, policy loss: 10.266011248647683
Experience 15, Iter 24, disc loss: 0.0018154542676349706, policy loss: 10.623742509001955
Experience 15, Iter 25, disc loss: 0.0016438245140863112, policy loss: 10.396112704938206
Experience 15, Iter 26, disc loss: 0.0015523219381005928, policy loss: 10.411444352579874
Experience 15, Iter 27, disc loss: 0.0021371217591449818, policy loss: 9.915077477654998
Experience 15, Iter 28, disc loss: 0.0021141783097599546, policy loss: 9.618488853974139
Experience 15, Iter 29, disc loss: 0.0014626664943769032, policy loss: 10.092222889744203
Experience 15, Iter 30, disc loss: 0.0017387934576092395, policy loss: 9.912392346390732
Experience 15, Iter 31, disc loss: 0.0014543039998811506, policy loss: 10.251667179080545
Experience 15, Iter 32, disc loss: 0.001537030821221431, policy loss: 11.04670237414285
Experience 15, Iter 33, disc loss: 0.0016438634732235525, policy loss: 9.865551457831783
Experience 15, Iter 34, disc loss: 0.001881037298398497, policy loss: 9.649398907672673
Experience 15, Iter 35, disc loss: 0.001682707876981495, policy loss: 9.687826109681517
Experience 15, Iter 36, disc loss: 0.0017429424575683295, policy loss: 9.820415445902235
Experience 15, Iter 37, disc loss: 0.0014314973407766232, policy loss: 10.18137024869596
Experience 15, Iter 38, disc loss: 0.0012387493114783398, policy loss: 10.110966579261344
Experience 15, Iter 39, disc loss: 0.0018096265829469417, policy loss: 9.561947714403633
Experience 15, Iter 40, disc loss: 0.0020683693094577208, policy loss: 9.466589090160758
Experience 15, Iter 41, disc loss: 0.001778949322621467, policy loss: 9.520936226774742
Experience 15, Iter 42, disc loss: 0.0025727415762969957, policy loss: 10.470155590179456
Experience 15, Iter 43, disc loss: 0.0015541355854622628, policy loss: 10.16087172209155
Experience 15, Iter 44, disc loss: 0.0027381180647678477, policy loss: 9.52184899998209
Experience 15, Iter 45, disc loss: 0.00224567704048751, policy loss: 9.676559931784368
Experience 15, Iter 46, disc loss: 0.002167225403757891, policy loss: 9.531321625926926
Experience 15, Iter 47, disc loss: 0.0017796636924957972, policy loss: 9.594028672766624
Experience 15, Iter 48, disc loss: 0.0014919427290780206, policy loss: 9.738361447428758
Experience 15, Iter 49, disc loss: 0.0025860328033269057, policy loss: 9.55362763812067
Experience 15, Iter 50, disc loss: 0.0015606911809078838, policy loss: 9.236571790891311
Experience 15, Iter 51, disc loss: 0.002457869000698741, policy loss: 9.354257157688302
Experience 15, Iter 52, disc loss: 0.0023004427907153285, policy loss: 9.330708988559536
Experience 15, Iter 53, disc loss: 0.001624133654333509, policy loss: 9.629141290523327
Experience 15, Iter 54, disc loss: 0.0017057041822654353, policy loss: 10.5497420263649
Experience 15, Iter 55, disc loss: 0.002915233407069202, policy loss: 9.557960527410787
Experience 15, Iter 56, disc loss: 0.001186033340834018, policy loss: 10.900306413932178
Experience 15, Iter 57, disc loss: 0.002015188436100708, policy loss: 10.187421675981618
Experience 15, Iter 58, disc loss: 0.0013958628247041699, policy loss: 10.415015093255032
Experience 15, Iter 59, disc loss: 0.0016070225680000472, policy loss: 10.521268367138784
Experience 15, Iter 60, disc loss: 0.001263610161846858, policy loss: 10.414530154546753
Experience 15, Iter 61, disc loss: 0.0013212633198604706, policy loss: 10.288637280461625
Experience 15, Iter 62, disc loss: 0.0013214022947943957, policy loss: 11.156328884475347
Experience 15, Iter 63, disc loss: 0.0012586823220899642, policy loss: 10.859394182456091
Experience 15, Iter 64, disc loss: 0.002008221297201848, policy loss: 10.241602102963927
Experience 15, Iter 65, disc loss: 0.0009216358910592878, policy loss: 10.95568036642647
Experience 15, Iter 66, disc loss: 0.0019487285665372419, policy loss: 10.60825063362747
Experience 15, Iter 67, disc loss: 0.0009559097522901396, policy loss: 11.17509913495127
Experience 15, Iter 68, disc loss: 0.00111621070382243, policy loss: 11.189380129664835
Experience 15, Iter 69, disc loss: 0.000921101766692551, policy loss: 11.274529603732915
Experience 15, Iter 70, disc loss: 0.001058830000105527, policy loss: 11.175657634436904
Experience 15, Iter 71, disc loss: 0.0008744245579720596, policy loss: 11.740606346304551
Experience 15, Iter 72, disc loss: 0.0013236627639243263, policy loss: 11.345526274131661
Experience 15, Iter 73, disc loss: 0.0010897719815587217, policy loss: 11.232968451100508
Experience 15, Iter 74, disc loss: 0.0010520372192142765, policy loss: 10.64106205688866
Experience 15, Iter 75, disc loss: 0.0018007469145582978, policy loss: 10.674950183297051
Experience 15, Iter 76, disc loss: 0.000896799318328054, policy loss: 11.014844328377421
Experience 15, Iter 77, disc loss: 0.0011190082150899117, policy loss: 10.658153765198207
Experience 15, Iter 78, disc loss: 0.0022319722677794866, policy loss: 11.102704371576547
Experience 15, Iter 79, disc loss: 0.0009201998093950549, policy loss: 11.223781204164073
Experience 15, Iter 80, disc loss: 0.001331472001568647, policy loss: 9.867537331478434
Experience 15, Iter 81, disc loss: 0.0011407093608531468, policy loss: 11.875451040506256
Experience 15, Iter 82, disc loss: 0.000990108360659977, policy loss: 10.630781478086048
Experience 15, Iter 83, disc loss: 0.0009070331223836041, policy loss: 10.530743903549736
Experience 15, Iter 84, disc loss: 0.0010516930901712067, policy loss: 10.432762454126838
Experience 15, Iter 85, disc loss: 0.0008695935529824576, policy loss: 11.256861613125547
Experience 15, Iter 86, disc loss: 0.0007599201110305146, policy loss: 10.702945767776423
Experience 15, Iter 87, disc loss: 0.001002324128043914, policy loss: 10.66021030913612
Experience 15, Iter 88, disc loss: 0.0010002300265459754, policy loss: 10.69790974856745
Experience 15, Iter 89, disc loss: 0.0017107106207217152, policy loss: 10.681654756647283
Experience 15, Iter 90, disc loss: 0.0016777317278602335, policy loss: 10.662244450792581
Experience 15, Iter 91, disc loss: 0.0010496196590974835, policy loss: 10.806263398318114
Experience 15, Iter 92, disc loss: 0.002287600227884479, policy loss: 11.057938426337786
Experience 15, Iter 93, disc loss: 0.001275829014791839, policy loss: 10.081138354584112
Experience 15, Iter 94, disc loss: 0.0012684325012252873, policy loss: 10.952083508313079
Experience 15, Iter 95, disc loss: 0.0008079769871203893, policy loss: 10.270320737516474
Experience 15, Iter 96, disc loss: 0.0012898955939941942, policy loss: 10.24626370110105
Experience 15, Iter 97, disc loss: 0.0008618693052638936, policy loss: 10.79608621004077
Experience 15, Iter 98, disc loss: 0.0012341554497976004, policy loss: 10.795712626787006
Experience 15, Iter 99, disc loss: 0.0012736904580635134, policy loss: 10.19561301355482
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1553],
        [1.4155],
        [0.0297]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0049, 0.1012, 1.3546, 0.0188, 0.0192, 3.3352]],

        [[0.0049, 0.1012, 1.3546, 0.0188, 0.0192, 3.3352]],

        [[0.0049, 0.1012, 1.3546, 0.0188, 0.0192, 3.3352]],

        [[0.0049, 0.1012, 1.3546, 0.0188, 0.0192, 3.3352]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.6213, 5.6621, 0.1188], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.6213, 5.6621, 0.1188])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.989
Iter 2/2000 - Loss: 3.224
Iter 3/2000 - Loss: 2.896
Iter 4/2000 - Loss: 2.943
Iter 5/2000 - Loss: 3.022
Iter 6/2000 - Loss: 2.931
Iter 7/2000 - Loss: 2.800
Iter 8/2000 - Loss: 2.729
Iter 9/2000 - Loss: 2.713
Iter 10/2000 - Loss: 2.687
Iter 11/2000 - Loss: 2.608
Iter 12/2000 - Loss: 2.482
Iter 13/2000 - Loss: 2.343
Iter 14/2000 - Loss: 2.216
Iter 15/2000 - Loss: 2.097
Iter 16/2000 - Loss: 1.964
Iter 17/2000 - Loss: 1.799
Iter 18/2000 - Loss: 1.598
Iter 19/2000 - Loss: 1.371
Iter 20/2000 - Loss: 1.130
Iter 1981/2000 - Loss: -7.506
Iter 1982/2000 - Loss: -7.506
Iter 1983/2000 - Loss: -7.506
Iter 1984/2000 - Loss: -7.506
Iter 1985/2000 - Loss: -7.506
Iter 1986/2000 - Loss: -7.506
Iter 1987/2000 - Loss: -7.506
Iter 1988/2000 - Loss: -7.506
Iter 1989/2000 - Loss: -7.506
Iter 1990/2000 - Loss: -7.506
Iter 1991/2000 - Loss: -7.506
Iter 1992/2000 - Loss: -7.506
Iter 1993/2000 - Loss: -7.506
Iter 1994/2000 - Loss: -7.506
Iter 1995/2000 - Loss: -7.507
Iter 1996/2000 - Loss: -7.507
Iter 1997/2000 - Loss: -7.507
Iter 1998/2000 - Loss: -7.507
Iter 1999/2000 - Loss: -7.507
Iter 2000/2000 - Loss: -7.507
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[ 8.7651,  6.4413, 49.7954,  6.2013,  6.9827, 55.9474]],

        [[13.0249, 27.9581, 10.5956,  1.3279,  1.5000, 28.3828]],

        [[13.3936, 31.1947, 10.1778,  1.0964,  1.0254, 22.3794]],

        [[11.7699, 26.1313, 17.6414,  2.7404,  1.8545, 44.2505]]])
Signal Variance: tensor([ 0.0906,  2.8672, 21.1773,  0.6373])
Estimated target variance: tensor([0.0099, 0.6213, 5.6621, 0.1188])
N: 160
Signal to noise ratio: tensor([19.8847, 78.1042, 89.9126, 48.1818])
Bound on condition number: tensor([  63265.0339,  976044.4517, 1293483.9357,  371438.4587])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0009194089171070292, policy loss: 10.712118389830149
Experience 16, Iter 1, disc loss: 0.0013429491593517774, policy loss: 10.355226591142399
Experience 16, Iter 2, disc loss: 0.0009910639808707082, policy loss: 10.493565395585446
Experience 16, Iter 3, disc loss: 0.0007561193901088444, policy loss: 11.333344633123708
Experience 16, Iter 4, disc loss: 0.0009861817921949668, policy loss: 10.609395678737517
Experience 16, Iter 5, disc loss: 0.002582790639820921, policy loss: 9.958972302531542
Experience 16, Iter 6, disc loss: 0.000910402694127286, policy loss: 10.662442384274438
Experience 16, Iter 7, disc loss: 0.0010774942382443089, policy loss: 10.464169751263231
Experience 16, Iter 8, disc loss: 0.0016223896131223698, policy loss: 9.882389668315358
Experience 16, Iter 9, disc loss: 0.0013558748062522355, policy loss: 10.810580133427607
Experience 16, Iter 10, disc loss: 0.0009682223136437021, policy loss: 10.522104338246372
Experience 16, Iter 11, disc loss: 0.000956928213893966, policy loss: 10.432956390541875
Experience 16, Iter 12, disc loss: 0.0017859014728598777, policy loss: 9.845277940401505
Experience 16, Iter 13, disc loss: 0.001130510380301962, policy loss: 10.052882519574565
Experience 16, Iter 14, disc loss: 0.0015224714015314332, policy loss: 10.258288792937938
Experience 16, Iter 15, disc loss: 0.001355355527145325, policy loss: 10.810950139038981
Experience 16, Iter 16, disc loss: 0.0021787114574387087, policy loss: 9.833745863742383
Experience 16, Iter 17, disc loss: 0.0010960295565168702, policy loss: 10.717974905170829
Experience 16, Iter 18, disc loss: 0.002737282703283916, policy loss: 10.655056836096634
Experience 16, Iter 19, disc loss: 0.000849416606377084, policy loss: 10.649294499383215
Experience 16, Iter 20, disc loss: 0.0014803487979158604, policy loss: 9.983298464464891
Experience 16, Iter 21, disc loss: 0.0012450161213873024, policy loss: 10.12562246616948
Experience 16, Iter 22, disc loss: 0.0009799720041207983, policy loss: 10.668842688222082
Experience 16, Iter 23, disc loss: 0.0008712704953908932, policy loss: 10.685881258051465
Experience 16, Iter 24, disc loss: 0.001023235431837161, policy loss: 10.048533510178125
Experience 16, Iter 25, disc loss: 0.0007902892132882242, policy loss: 10.766132760805158
Experience 16, Iter 26, disc loss: 0.0012009839037259635, policy loss: 10.461699336422809
Experience 16, Iter 27, disc loss: 0.0011267061740542574, policy loss: 10.679271004736012
Experience 16, Iter 28, disc loss: 0.0015902240249897077, policy loss: 9.945768249466639
Experience 16, Iter 29, disc loss: 0.00135133619475023, policy loss: 9.810695261331375
Experience 16, Iter 30, disc loss: 0.0007568781282426276, policy loss: 11.219371626695127
Experience 16, Iter 31, disc loss: 0.001071561481185895, policy loss: 10.846183362798664
Experience 16, Iter 32, disc loss: 0.0011636782267861962, policy loss: 9.673300428477352
Experience 16, Iter 33, disc loss: 0.0022750564523363022, policy loss: 9.691327537369292
Experience 16, Iter 34, disc loss: 0.0014510765880086868, policy loss: 10.059928516500335
Experience 16, Iter 35, disc loss: 0.0010253960751498545, policy loss: 10.751232643044347
Experience 16, Iter 36, disc loss: 0.0008241600154899302, policy loss: 10.57941411922005
Experience 16, Iter 37, disc loss: 0.0023858524085214627, policy loss: 9.877831821576674
Experience 16, Iter 38, disc loss: 0.001125547499972145, policy loss: 10.782859078154594
Experience 16, Iter 39, disc loss: 0.0017166519255739118, policy loss: 9.46828495991171
Experience 16, Iter 40, disc loss: 0.0019295408235184548, policy loss: 9.395966741336963
Experience 16, Iter 41, disc loss: 0.0022443245320326725, policy loss: 10.492374957304717
Experience 16, Iter 42, disc loss: 0.0017218475709234434, policy loss: 10.111528660375235
Experience 16, Iter 43, disc loss: 0.001282856189285344, policy loss: 9.93357009367832
Experience 16, Iter 44, disc loss: 0.0019839042014633537, policy loss: 9.98468491814549
Experience 16, Iter 45, disc loss: 0.001857099229358537, policy loss: 9.954362708256888
Experience 16, Iter 46, disc loss: 0.0014158571608651068, policy loss: 9.583065581417689
Experience 16, Iter 47, disc loss: 0.002095416956167838, policy loss: 9.995824369705648
Experience 16, Iter 48, disc loss: 0.0013664155161930308, policy loss: 10.10426499929461
Experience 16, Iter 49, disc loss: 0.002907925633199036, policy loss: 9.580773917734533
Experience 16, Iter 50, disc loss: 0.0011305223983638195, policy loss: 9.729679076929365
Experience 16, Iter 51, disc loss: 0.0011793011862046025, policy loss: 10.5193607705981
Experience 16, Iter 52, disc loss: 0.003134876309722819, policy loss: 9.490491932844076
Experience 16, Iter 53, disc loss: 0.0017113471169641378, policy loss: 10.13612809497172
Experience 16, Iter 54, disc loss: 0.0020792478899152433, policy loss: 9.262211282078038
Experience 16, Iter 55, disc loss: 0.001126840479775302, policy loss: 10.220921483887743
Experience 16, Iter 56, disc loss: 0.002357199429350056, policy loss: 9.880441019586842
Experience 16, Iter 57, disc loss: 0.0015268237025975108, policy loss: 10.10562167208998
Experience 16, Iter 58, disc loss: 0.0019350897962894735, policy loss: 9.878209238092822
Experience 16, Iter 59, disc loss: 0.0013072331872836613, policy loss: 10.110921273074766
Experience 16, Iter 60, disc loss: 0.002046092099259697, policy loss: 10.119953226140858
Experience 16, Iter 61, disc loss: 0.0015896241860263783, policy loss: 9.592481389764245
Experience 16, Iter 62, disc loss: 0.0016578891597172915, policy loss: 9.480805756302585
Experience 16, Iter 63, disc loss: 0.0016054981906473546, policy loss: 9.726621999374585
Experience 16, Iter 64, disc loss: 0.0015256573795510162, policy loss: 10.034082433789592
Experience 16, Iter 65, disc loss: 0.0015811112230453884, policy loss: 9.831278606944213
Experience 16, Iter 66, disc loss: 0.002508693520467113, policy loss: 9.626582334645247
Experience 16, Iter 67, disc loss: 0.0017243759051804783, policy loss: 9.947697497202554
Experience 16, Iter 68, disc loss: 0.0012849624314063071, policy loss: 10.062269173704255
Experience 16, Iter 69, disc loss: 0.0022397718566109174, policy loss: 8.924727111687476
Experience 16, Iter 70, disc loss: 0.0012946974747978265, policy loss: 10.308995693051502
Experience 16, Iter 71, disc loss: 0.0022256051209078396, policy loss: 9.604859498496186
Experience 16, Iter 72, disc loss: 0.0019690368686599373, policy loss: 9.645349881190606
Experience 16, Iter 73, disc loss: 0.0018373682875512224, policy loss: 9.836039133928033
Experience 16, Iter 74, disc loss: 0.0015315051697435472, policy loss: 9.865823246192036
Experience 16, Iter 75, disc loss: 0.0021080627962237024, policy loss: 9.261587372080584
Experience 16, Iter 76, disc loss: 0.0022493408966637474, policy loss: 9.252723100900955
Experience 16, Iter 77, disc loss: 0.0018707572204241232, policy loss: 9.790435527659296
Experience 16, Iter 78, disc loss: 0.0014435969886438856, policy loss: 9.714428629742967
Experience 16, Iter 79, disc loss: 0.0018959005192769143, policy loss: 9.278654247267918
Experience 16, Iter 80, disc loss: 0.0024624512657836863, policy loss: 8.934952496823282
Experience 16, Iter 81, disc loss: 0.0019325662410943512, policy loss: 9.36263023795837
Experience 16, Iter 82, disc loss: 0.001737349712098041, policy loss: 9.803166794631384
Experience 16, Iter 83, disc loss: 0.0023317879784149915, policy loss: 9.930505063944814
Experience 16, Iter 84, disc loss: 0.0023418200800334184, policy loss: 9.198155101787279
Experience 16, Iter 85, disc loss: 0.002364099758100315, policy loss: 9.414990902755637
Experience 16, Iter 86, disc loss: 0.0019059022911577519, policy loss: 10.41663659444457
Experience 16, Iter 87, disc loss: 0.0027210267596557047, policy loss: 9.640130564223425
Experience 16, Iter 88, disc loss: 0.0020459841697626924, policy loss: 9.582278222536752
Experience 16, Iter 89, disc loss: 0.0015088221488265448, policy loss: 9.529388130990105
Experience 16, Iter 90, disc loss: 0.0020345485159333197, policy loss: 9.25876915611159
Experience 16, Iter 91, disc loss: 0.002384988023784959, policy loss: 9.217079488159715
Experience 16, Iter 92, disc loss: 0.0022017612009425924, policy loss: 9.4786101154496
Experience 16, Iter 93, disc loss: 0.0021092708594526075, policy loss: 9.55250470543936
Experience 16, Iter 94, disc loss: 0.0018759424098288191, policy loss: 10.419700573001766
Experience 16, Iter 95, disc loss: 0.0019155672572845534, policy loss: 9.458977592186733
Experience 16, Iter 96, disc loss: 0.0019431415154621213, policy loss: 9.026734027572594
Experience 16, Iter 97, disc loss: 0.00132852617630573, policy loss: 10.083364489299969
Experience 16, Iter 98, disc loss: 0.002084552418374272, policy loss: 9.350702951305234
Experience 16, Iter 99, disc loss: 0.0017207437397936285, policy loss: 9.839706502116647
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1649],
        [1.4726],
        [0.0315]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0050, 0.1050, 1.4328, 0.0201, 0.0209, 3.5483]],

        [[0.0050, 0.1050, 1.4328, 0.0201, 0.0209, 3.5483]],

        [[0.0050, 0.1050, 1.4328, 0.0201, 0.0209, 3.5483]],

        [[0.0050, 0.1050, 1.4328, 0.0201, 0.0209, 3.5483]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0102, 0.6595, 5.8903, 0.1259], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0102, 0.6595, 5.8903, 0.1259])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.091
Iter 2/2000 - Loss: 3.304
Iter 3/2000 - Loss: 2.985
Iter 4/2000 - Loss: 3.029
Iter 5/2000 - Loss: 3.105
Iter 6/2000 - Loss: 3.013
Iter 7/2000 - Loss: 2.879
Iter 8/2000 - Loss: 2.801
Iter 9/2000 - Loss: 2.776
Iter 10/2000 - Loss: 2.742
Iter 11/2000 - Loss: 2.651
Iter 12/2000 - Loss: 2.513
Iter 13/2000 - Loss: 2.360
Iter 14/2000 - Loss: 2.215
Iter 15/2000 - Loss: 2.077
Iter 16/2000 - Loss: 1.924
Iter 17/2000 - Loss: 1.740
Iter 18/2000 - Loss: 1.521
Iter 19/2000 - Loss: 1.279
Iter 20/2000 - Loss: 1.022
Iter 1981/2000 - Loss: -7.610
Iter 1982/2000 - Loss: -7.610
Iter 1983/2000 - Loss: -7.610
Iter 1984/2000 - Loss: -7.610
Iter 1985/2000 - Loss: -7.610
Iter 1986/2000 - Loss: -7.610
Iter 1987/2000 - Loss: -7.610
Iter 1988/2000 - Loss: -7.610
Iter 1989/2000 - Loss: -7.610
Iter 1990/2000 - Loss: -7.610
Iter 1991/2000 - Loss: -7.610
Iter 1992/2000 - Loss: -7.610
Iter 1993/2000 - Loss: -7.610
Iter 1994/2000 - Loss: -7.610
Iter 1995/2000 - Loss: -7.610
Iter 1996/2000 - Loss: -7.610
Iter 1997/2000 - Loss: -7.610
Iter 1998/2000 - Loss: -7.610
Iter 1999/2000 - Loss: -7.610
Iter 2000/2000 - Loss: -7.610
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 8.6781,  6.1002, 48.9618,  5.4970,  5.2871, 54.2530]],

        [[12.6723, 27.0291, 10.2426,  1.4204,  1.3533, 27.6743]],

        [[13.6918, 30.8994, 10.3124,  0.9710,  0.8567, 21.6874]],

        [[11.5947, 24.4919, 17.4634,  2.6317,  1.8109, 44.1995]]])
Signal Variance: tensor([ 0.0809,  2.7167, 17.8007,  0.6238])
Estimated target variance: tensor([0.0102, 0.6595, 5.8903, 0.1259])
N: 170
Signal to noise ratio: tensor([19.0230, 77.5926, 84.0106, 48.7675])
Bound on condition number: tensor([  61519.7466, 1023505.4415, 1199824.8630,  404306.2096])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.002034044318126412, policy loss: 9.58772488531151
Experience 17, Iter 1, disc loss: 0.0022410713427379074, policy loss: 9.52816558754752
Experience 17, Iter 2, disc loss: 0.0019555575085654377, policy loss: 9.703920029470957
Experience 17, Iter 3, disc loss: 0.0023654438431150635, policy loss: 9.805571676388482
Experience 17, Iter 4, disc loss: 0.001524740411128107, policy loss: 9.845300360714367
Experience 17, Iter 5, disc loss: 0.001838192893692987, policy loss: 9.140488180819345
Experience 17, Iter 6, disc loss: 0.00179674600918806, policy loss: 10.032670621860085
Experience 17, Iter 7, disc loss: 0.0016401726658648425, policy loss: 10.12442444888328
Experience 17, Iter 8, disc loss: 0.002076930891525029, policy loss: 9.552774726763534
Experience 17, Iter 9, disc loss: 0.0018244217063637922, policy loss: 9.916133573873239
Experience 17, Iter 10, disc loss: 0.002432479832993566, policy loss: 9.69857477628673
Experience 17, Iter 11, disc loss: 0.001840059320231091, policy loss: 10.368884650075959
Experience 17, Iter 12, disc loss: 0.0021389523773671275, policy loss: 9.434256492916532
Experience 17, Iter 13, disc loss: 0.002478717953831224, policy loss: 8.986455374834874
Experience 17, Iter 14, disc loss: 0.002243180596917515, policy loss: 9.641565609875355
Experience 17, Iter 15, disc loss: 0.0018027021538747946, policy loss: 10.052615509414006
Experience 17, Iter 16, disc loss: 0.002233544601911534, policy loss: 9.712357382737279
Experience 17, Iter 17, disc loss: 0.001636268494434517, policy loss: 9.599878354779072
Experience 17, Iter 18, disc loss: 0.0020504949446890656, policy loss: 9.254687959503718
Experience 17, Iter 19, disc loss: 0.0018295898084742372, policy loss: 9.590171754872994
Experience 17, Iter 20, disc loss: 0.0016410554991175288, policy loss: 10.105940887963524
Experience 17, Iter 21, disc loss: 0.0021455125310540493, policy loss: 9.32084820190044
Experience 17, Iter 22, disc loss: 0.0015791518290048194, policy loss: 10.070696923669342
Experience 17, Iter 23, disc loss: 0.0015310627575731571, policy loss: 10.32964062242365
Experience 17, Iter 24, disc loss: 0.0019021408793406308, policy loss: 10.47659370175114
Experience 17, Iter 25, disc loss: 0.0024388135792835974, policy loss: 9.703622768359294
Experience 17, Iter 26, disc loss: 0.0020324061237838053, policy loss: 9.994763926330553
Experience 17, Iter 27, disc loss: 0.0010729970033895764, policy loss: 10.407235965827965
Experience 17, Iter 28, disc loss: 0.0015875947702383452, policy loss: 10.769043507116422
Experience 17, Iter 29, disc loss: 0.0014741586057794877, policy loss: 9.517446701885955
Experience 17, Iter 30, disc loss: 0.002098730993734965, policy loss: 9.665628704581259
Experience 17, Iter 31, disc loss: 0.0015940909001475902, policy loss: 10.183079304469882
Experience 17, Iter 32, disc loss: 0.00161286672169782, policy loss: 9.914591675073272
Experience 17, Iter 33, disc loss: 0.0014414089001412791, policy loss: 10.028776995158996
Experience 17, Iter 34, disc loss: 0.0019583062740638744, policy loss: 9.598995370298233
Experience 17, Iter 35, disc loss: 0.0013221470760233834, policy loss: 10.101498222206969
Experience 17, Iter 36, disc loss: 0.0011766255795092786, policy loss: 10.433168731720336
Experience 17, Iter 37, disc loss: 0.0017948847630741204, policy loss: 9.961845131221772
Experience 17, Iter 38, disc loss: 0.003157967149636492, policy loss: 10.268352482252396
Experience 17, Iter 39, disc loss: 0.0016833240420175179, policy loss: 9.64337463751815
Experience 17, Iter 40, disc loss: 0.00172488302843039, policy loss: 9.52325279156405
Experience 17, Iter 41, disc loss: 0.0016976947544116846, policy loss: 9.154078423307816
Experience 17, Iter 42, disc loss: 0.0018969279747321664, policy loss: 9.918157422246589
Experience 17, Iter 43, disc loss: 0.0018312555003102498, policy loss: 10.164862341968858
Experience 17, Iter 44, disc loss: 0.0017815923117797889, policy loss: 9.556633382851453
Experience 17, Iter 45, disc loss: 0.0015947543685827595, policy loss: 9.090209509683435
Experience 17, Iter 46, disc loss: 0.0016353145615191514, policy loss: 10.400313212334165
Experience 17, Iter 47, disc loss: 0.001485673263901761, policy loss: 10.11205622335454
Experience 17, Iter 48, disc loss: 0.0019353224307581502, policy loss: 10.014299192717273
Experience 17, Iter 49, disc loss: 0.0018932467834987046, policy loss: 9.791667531907208
Experience 17, Iter 50, disc loss: 0.0023559743152185043, policy loss: 9.928540161481191
Experience 17, Iter 51, disc loss: 0.0017670560637038685, policy loss: 11.049208450763661
Experience 17, Iter 52, disc loss: 0.001470030984516865, policy loss: 10.175755684099434
Experience 17, Iter 53, disc loss: 0.0017808312286511805, policy loss: 10.018388671797602
Experience 17, Iter 54, disc loss: 0.001593998118898322, policy loss: 9.148471179760609
Experience 17, Iter 55, disc loss: 0.0020947832108560906, policy loss: 9.938428002752929
Experience 17, Iter 56, disc loss: 0.001746222781010429, policy loss: 10.103155094835337
Experience 17, Iter 57, disc loss: 0.0017430465619112647, policy loss: 9.689085755398551
Experience 17, Iter 58, disc loss: 0.0015770234144883329, policy loss: 9.290330709103795
Experience 17, Iter 59, disc loss: 0.0016217731713757092, policy loss: 9.818259493455272
Experience 17, Iter 60, disc loss: 0.0015096694363585994, policy loss: 9.366673867723252
Experience 17, Iter 61, disc loss: 0.002138777577280376, policy loss: 8.843884034943429
Experience 17, Iter 62, disc loss: 0.0017748771928480674, policy loss: 10.001748035247276
Experience 17, Iter 63, disc loss: 0.0013691911794399127, policy loss: 9.9122473031698
Experience 17, Iter 64, disc loss: 0.0016126556659567183, policy loss: 9.693509246607725
Experience 17, Iter 65, disc loss: 0.0018095874723528232, policy loss: 9.590283907130063
Experience 17, Iter 66, disc loss: 0.001457868791023062, policy loss: 9.792415388495094
Experience 17, Iter 67, disc loss: 0.0017413694498403832, policy loss: 9.111536272527044
Experience 17, Iter 68, disc loss: 0.0013571547481922002, policy loss: 11.007085077657374
Experience 17, Iter 69, disc loss: 0.0014844787886148085, policy loss: 10.385886484415224
Experience 17, Iter 70, disc loss: 0.0019190890407114107, policy loss: 9.659461208614601
Experience 17, Iter 71, disc loss: 0.0019461286189341143, policy loss: 10.642810220005344
Experience 17, Iter 72, disc loss: 0.0016775620975929078, policy loss: 9.488474791309972
Experience 17, Iter 73, disc loss: 0.001548135875948774, policy loss: 9.433940421865685
Experience 17, Iter 74, disc loss: 0.0021166986153569253, policy loss: 9.22319934090354
Experience 17, Iter 75, disc loss: 0.0020252517520948504, policy loss: 9.510172201032574
Experience 17, Iter 76, disc loss: 0.0014686436389917978, policy loss: 9.947684421467201
Experience 17, Iter 77, disc loss: 0.001931419245015837, policy loss: 9.21531311329167
Experience 17, Iter 78, disc loss: 0.0018234390355736632, policy loss: 9.75027012446383
Experience 17, Iter 79, disc loss: 0.0015576336197725203, policy loss: 9.750413693636125
Experience 17, Iter 80, disc loss: 0.0017489645672363614, policy loss: 10.28323769347342
Experience 17, Iter 81, disc loss: 0.0015138029262441492, policy loss: 10.024188971415894
Experience 17, Iter 82, disc loss: 0.002313529443190594, policy loss: 8.863892759395025
Experience 17, Iter 83, disc loss: 0.001741531660359845, policy loss: 9.28182518728258
Experience 17, Iter 84, disc loss: 0.0018026926825988466, policy loss: 9.298359710460751
Experience 17, Iter 85, disc loss: 0.001117798939231966, policy loss: 10.83860341063487
Experience 17, Iter 86, disc loss: 0.0016705946801630366, policy loss: 9.607569023441197
Experience 17, Iter 87, disc loss: 0.0015243774437004982, policy loss: 9.428232836862465
Experience 17, Iter 88, disc loss: 0.001969002008922548, policy loss: 9.02608681390207
Experience 17, Iter 89, disc loss: 0.001409121926095058, policy loss: 9.558909334933897
Experience 17, Iter 90, disc loss: 0.0016869170414159874, policy loss: 8.854283862251664
Experience 17, Iter 91, disc loss: 0.0018130394886670942, policy loss: 9.428680333115327
Experience 17, Iter 92, disc loss: 0.00180748774762217, policy loss: 9.626782877003187
Experience 17, Iter 93, disc loss: 0.0026908500256528087, policy loss: 8.908735464455399
Experience 17, Iter 94, disc loss: 0.0021904734482012787, policy loss: 9.772171133848898
Experience 17, Iter 95, disc loss: 0.0024057883277467133, policy loss: 9.193836470835217
Experience 17, Iter 96, disc loss: 0.0015770060731972817, policy loss: 9.21397901500214
Experience 17, Iter 97, disc loss: 0.0018350973087987315, policy loss: 9.576128700098458
Experience 17, Iter 98, disc loss: 0.0014822915681478508, policy loss: 9.678085123978324
Experience 17, Iter 99, disc loss: 0.0013472416806137961, policy loss: 9.683700769662149
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1749],
        [1.5327],
        [0.0335]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0049, 0.1093, 1.5165, 0.0212, 0.0225, 3.7478]],

        [[0.0049, 0.1093, 1.5165, 0.0212, 0.0225, 3.7478]],

        [[0.0049, 0.1093, 1.5165, 0.0212, 0.0225, 3.7478]],

        [[0.0049, 0.1093, 1.5165, 0.0212, 0.0225, 3.7478]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0106, 0.6997, 6.1309, 0.1339], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0106, 0.6997, 6.1309, 0.1339])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.195
Iter 2/2000 - Loss: 3.388
Iter 3/2000 - Loss: 3.076
Iter 4/2000 - Loss: 3.115
Iter 5/2000 - Loss: 3.188
Iter 6/2000 - Loss: 3.094
Iter 7/2000 - Loss: 2.956
Iter 8/2000 - Loss: 2.872
Iter 9/2000 - Loss: 2.840
Iter 10/2000 - Loss: 2.799
Iter 11/2000 - Loss: 2.703
Iter 12/2000 - Loss: 2.559
Iter 13/2000 - Loss: 2.401
Iter 14/2000 - Loss: 2.249
Iter 15/2000 - Loss: 2.101
Iter 16/2000 - Loss: 1.938
Iter 17/2000 - Loss: 1.744
Iter 18/2000 - Loss: 1.518
Iter 19/2000 - Loss: 1.268
Iter 20/2000 - Loss: 1.003
Iter 1981/2000 - Loss: -7.608
Iter 1982/2000 - Loss: -7.608
Iter 1983/2000 - Loss: -7.608
Iter 1984/2000 - Loss: -7.608
Iter 1985/2000 - Loss: -7.608
Iter 1986/2000 - Loss: -7.608
Iter 1987/2000 - Loss: -7.608
Iter 1988/2000 - Loss: -7.608
Iter 1989/2000 - Loss: -7.608
Iter 1990/2000 - Loss: -7.608
Iter 1991/2000 - Loss: -7.608
Iter 1992/2000 - Loss: -7.608
Iter 1993/2000 - Loss: -7.608
Iter 1994/2000 - Loss: -7.608
Iter 1995/2000 - Loss: -7.608
Iter 1996/2000 - Loss: -7.608
Iter 1997/2000 - Loss: -7.608
Iter 1998/2000 - Loss: -7.608
Iter 1999/2000 - Loss: -7.608
Iter 2000/2000 - Loss: -7.608
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[ 7.2500,  4.8716, 42.7094,  4.6202,  4.3864, 54.1693]],

        [[12.6572, 26.1864, 10.2121,  1.7326,  1.1615, 26.0767]],

        [[13.7658, 28.1048,  9.9321,  1.0378,  0.9329, 20.9141]],

        [[11.3682, 24.4999, 16.7731,  2.6956,  1.8378, 43.4856]]])
Signal Variance: tensor([ 0.0646,  2.7046, 17.3837,  0.5802])
Estimated target variance: tensor([0.0106, 0.6997, 6.1309, 0.1339])
N: 180
Signal to noise ratio: tensor([16.2372, 79.5902, 80.7097, 45.5058])
Bound on condition number: tensor([  47457.4540, 1140229.2034, 1172530.4525,  372741.0230])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0024271114344027516, policy loss: 9.248391527669755
Experience 18, Iter 1, disc loss: 0.001828195463261507, policy loss: 9.412735959889652
Experience 18, Iter 2, disc loss: 0.0015941613425172608, policy loss: 9.679432018172559
Experience 18, Iter 3, disc loss: 0.0021411641313937727, policy loss: 9.35485802423959
Experience 18, Iter 4, disc loss: 0.0018558753457310767, policy loss: 9.422521638168433
Experience 18, Iter 5, disc loss: 0.0023913696401407806, policy loss: 8.915318241490041
Experience 18, Iter 6, disc loss: 0.0024567375124002686, policy loss: 9.842121366687014
Experience 18, Iter 7, disc loss: 0.0017533583456836761, policy loss: 10.245453386400985
Experience 18, Iter 8, disc loss: 0.0023458768073151568, policy loss: 9.854499410907989
Experience 18, Iter 9, disc loss: 0.002004925912503717, policy loss: 9.136639241565318
Experience 18, Iter 10, disc loss: 0.002290679811787692, policy loss: 10.091130092677343
Experience 18, Iter 11, disc loss: 0.0022380493904125822, policy loss: 9.724397800343665
Experience 18, Iter 12, disc loss: 0.0034945248883810577, policy loss: 8.85085460286043
Experience 18, Iter 13, disc loss: 0.002063379495029839, policy loss: 9.614286443839998
Experience 18, Iter 14, disc loss: 0.0016193198986404855, policy loss: 10.154734181469406
Experience 18, Iter 15, disc loss: 0.0015172570490269688, policy loss: 10.815541955644385
Experience 18, Iter 16, disc loss: 0.0021022351406606472, policy loss: 9.112096429709158
Experience 18, Iter 17, disc loss: 0.0017696890264473006, policy loss: 9.550505027770194
Experience 18, Iter 18, disc loss: 0.0019162300514466075, policy loss: 10.266268936560284
Experience 18, Iter 19, disc loss: 0.0018848660267261664, policy loss: 9.557222541351026
Experience 18, Iter 20, disc loss: 0.0015907098795149774, policy loss: 11.082116459966764
Experience 18, Iter 21, disc loss: 0.0018868346517922406, policy loss: 9.997783514274829
Experience 18, Iter 22, disc loss: 0.0017497535200388307, policy loss: 9.970694557950413
Experience 18, Iter 23, disc loss: 0.0016011062727720187, policy loss: 9.598969478970808
Experience 18, Iter 24, disc loss: 0.0017897463297531953, policy loss: 8.981744548623253
Experience 18, Iter 25, disc loss: 0.0017894235969379416, policy loss: 9.567125609938358
Experience 18, Iter 26, disc loss: 0.001976000614334013, policy loss: 9.711891699408795
Experience 18, Iter 27, disc loss: 0.0015933678177884243, policy loss: 10.056580716855159
Experience 18, Iter 28, disc loss: 0.0016290189911963371, policy loss: 10.351317548252378
Experience 18, Iter 29, disc loss: 0.0018912207634579868, policy loss: 9.890479763382013
Experience 18, Iter 30, disc loss: 0.0016895119034915156, policy loss: 9.823759579179123
Experience 18, Iter 31, disc loss: 0.00146692337992126, policy loss: 10.248553051585521
Experience 18, Iter 32, disc loss: 0.0019187842949251847, policy loss: 9.298112373860553
Experience 18, Iter 33, disc loss: 0.0019875859116953573, policy loss: 8.974493181046636
Experience 18, Iter 34, disc loss: 0.0017117315467227337, policy loss: 9.758261104509119
Experience 18, Iter 35, disc loss: 0.0019721466892475333, policy loss: 9.651814569308746
Experience 18, Iter 36, disc loss: 0.001964916809447191, policy loss: 9.266189904604193
Experience 18, Iter 37, disc loss: 0.0014400922902493966, policy loss: 10.00809546490411
Experience 18, Iter 38, disc loss: 0.002261722098329749, policy loss: 8.662252839247044
Experience 18, Iter 39, disc loss: 0.0019874986772549378, policy loss: 9.324240151094436
Experience 18, Iter 40, disc loss: 0.0019153340183144997, policy loss: 9.632585934743652
Experience 18, Iter 41, disc loss: 0.002069932883679594, policy loss: 9.664103525937435
Experience 18, Iter 42, disc loss: 0.0018360537136293874, policy loss: 9.319783553710648
Experience 18, Iter 43, disc loss: 0.0021625211727373346, policy loss: 8.743340396086195
Experience 18, Iter 44, disc loss: 0.0019189823034518644, policy loss: 9.691514745134302
Experience 18, Iter 45, disc loss: 0.0016853266263562015, policy loss: 10.063876208840025
Experience 18, Iter 46, disc loss: 0.0013371692429613126, policy loss: 10.438192726078737
Experience 18, Iter 47, disc loss: 0.0020536389759883374, policy loss: 9.46481689824203
Experience 18, Iter 48, disc loss: 0.002289434897637939, policy loss: 9.58830201377875
Experience 18, Iter 49, disc loss: 0.001718163121672257, policy loss: 9.43149610304934
Experience 18, Iter 50, disc loss: 0.001790830587474495, policy loss: 9.373777559123397
Experience 18, Iter 51, disc loss: 0.0019913497877268875, policy loss: 9.214516318174152
Experience 18, Iter 52, disc loss: 0.0020052142658113347, policy loss: 9.618755392320974
Experience 18, Iter 53, disc loss: 0.0019307415433747978, policy loss: 9.402070997469592
Experience 18, Iter 54, disc loss: 0.001673638196024515, policy loss: 9.818762474311255
Experience 18, Iter 55, disc loss: 0.0016313238689700201, policy loss: 9.854399077206011
Experience 18, Iter 56, disc loss: 0.001363944411146132, policy loss: 9.843418092046448
Experience 18, Iter 57, disc loss: 0.002407705771831424, policy loss: 10.111126040800544
Experience 18, Iter 58, disc loss: 0.0021243753796267406, policy loss: 8.603056706173803
Experience 18, Iter 59, disc loss: 0.002061129589212737, policy loss: 10.09679112998166
Experience 18, Iter 60, disc loss: 0.0018928789642031758, policy loss: 9.414224649311112
Experience 18, Iter 61, disc loss: 0.0016231586252342001, policy loss: 10.01131991203502
Experience 18, Iter 62, disc loss: 0.0018328423645556837, policy loss: 10.2308197868745
Experience 18, Iter 63, disc loss: 0.001654665468005538, policy loss: 10.560905186633317
Experience 18, Iter 64, disc loss: 0.0023296459944436333, policy loss: 10.703710477036498
Experience 18, Iter 65, disc loss: 0.002027329116622921, policy loss: 10.165534490821017
Experience 18, Iter 66, disc loss: 0.0014001024745299174, policy loss: 10.632784802326501
Experience 18, Iter 67, disc loss: 0.0021391332228289294, policy loss: 10.346794697450473
Experience 18, Iter 68, disc loss: 0.001408501745883071, policy loss: 10.82372182256497
Experience 18, Iter 69, disc loss: 0.0015460319542351013, policy loss: 10.419764989404973
Experience 18, Iter 70, disc loss: 0.0014872964813706147, policy loss: 10.142156589016208
Experience 18, Iter 71, disc loss: 0.0015245949134966308, policy loss: 11.022120982836073
Experience 18, Iter 72, disc loss: 0.0018139129915308166, policy loss: 11.01981926842458
Experience 18, Iter 73, disc loss: 0.0015149456150972966, policy loss: 10.202295898930435
Experience 18, Iter 74, disc loss: 0.0015813023692488387, policy loss: 10.666236291939196
Experience 18, Iter 75, disc loss: 0.002278434418667507, policy loss: 10.69594043274005
Experience 18, Iter 76, disc loss: 0.0016365713422351884, policy loss: 12.22089618154843
Experience 18, Iter 77, disc loss: 0.0016483575867041365, policy loss: 11.248641598817986
Experience 18, Iter 78, disc loss: 0.002445026030818946, policy loss: 10.231703230625813
Experience 18, Iter 79, disc loss: 0.0019518415560662488, policy loss: 11.368621169784578
Experience 18, Iter 80, disc loss: 0.0011125198080131262, policy loss: 12.22095592432293
Experience 18, Iter 81, disc loss: 0.0011508185107268506, policy loss: 11.761855974905382
Experience 18, Iter 82, disc loss: 0.0009573803828981667, policy loss: 12.27679845473749
Experience 18, Iter 83, disc loss: 0.0009667384926097219, policy loss: 12.550227812698093
Experience 18, Iter 84, disc loss: 0.001395411603838513, policy loss: 12.531372179129622
Experience 18, Iter 85, disc loss: 0.0013595947707343353, policy loss: 11.642381492451833
Experience 18, Iter 86, disc loss: 0.0008629824779638753, policy loss: 12.62410624558117
Experience 18, Iter 87, disc loss: 0.0013755341353540907, policy loss: 11.82975321337443
Experience 18, Iter 88, disc loss: 0.0009897319487199017, policy loss: 12.025615590137372
Experience 18, Iter 89, disc loss: 0.0012853109107002713, policy loss: 13.098109232586223
Experience 18, Iter 90, disc loss: 0.0010168778361805473, policy loss: 12.000384944662901
Experience 18, Iter 91, disc loss: 0.0012128504956407109, policy loss: 12.57249806665914
Experience 18, Iter 92, disc loss: 0.0013396406387952884, policy loss: 11.258554618068496
Experience 18, Iter 93, disc loss: 0.000851252258606347, policy loss: 12.221093336004225
Experience 18, Iter 94, disc loss: 0.001806922748942149, policy loss: 11.74376877444817
Experience 18, Iter 95, disc loss: 0.0010675938421019042, policy loss: 11.878419006649997
Experience 18, Iter 96, disc loss: 0.0008068067950997445, policy loss: 11.95952265937599
Experience 18, Iter 97, disc loss: 0.0017150014711347162, policy loss: 11.474497266693005
Experience 18, Iter 98, disc loss: 0.0012880008019060065, policy loss: 12.12728516816198
Experience 18, Iter 99, disc loss: 0.0007633872866279203, policy loss: 12.03982426900773
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1850],
        [1.5945],
        [0.0359]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0049, 0.1143, 1.6204, 0.0224, 0.0237, 3.9295]],

        [[0.0049, 0.1143, 1.6204, 0.0224, 0.0237, 3.9295]],

        [[0.0049, 0.1143, 1.6204, 0.0224, 0.0237, 3.9295]],

        [[0.0049, 0.1143, 1.6204, 0.0224, 0.0237, 3.9295]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0110, 0.7400, 6.3781, 0.1435], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0110, 0.7400, 6.3781, 0.1435])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.297
Iter 2/2000 - Loss: 3.476
Iter 3/2000 - Loss: 3.169
Iter 4/2000 - Loss: 3.202
Iter 5/2000 - Loss: 3.272
Iter 6/2000 - Loss: 3.181
Iter 7/2000 - Loss: 3.043
Iter 8/2000 - Loss: 2.955
Iter 9/2000 - Loss: 2.919
Iter 10/2000 - Loss: 2.876
Iter 11/2000 - Loss: 2.779
Iter 12/2000 - Loss: 2.635
Iter 13/2000 - Loss: 2.475
Iter 14/2000 - Loss: 2.320
Iter 15/2000 - Loss: 2.168
Iter 16/2000 - Loss: 1.999
Iter 17/2000 - Loss: 1.799
Iter 18/2000 - Loss: 1.568
Iter 19/2000 - Loss: 1.316
Iter 20/2000 - Loss: 1.047
Iter 1981/2000 - Loss: -7.619
Iter 1982/2000 - Loss: -7.619
Iter 1983/2000 - Loss: -7.619
Iter 1984/2000 - Loss: -7.619
Iter 1985/2000 - Loss: -7.619
Iter 1986/2000 - Loss: -7.619
Iter 1987/2000 - Loss: -7.619
Iter 1988/2000 - Loss: -7.619
Iter 1989/2000 - Loss: -7.620
Iter 1990/2000 - Loss: -7.620
Iter 1991/2000 - Loss: -7.620
Iter 1992/2000 - Loss: -7.620
Iter 1993/2000 - Loss: -7.620
Iter 1994/2000 - Loss: -7.620
Iter 1995/2000 - Loss: -7.620
Iter 1996/2000 - Loss: -7.620
Iter 1997/2000 - Loss: -7.620
Iter 1998/2000 - Loss: -7.620
Iter 1999/2000 - Loss: -7.620
Iter 2000/2000 - Loss: -7.620
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[ 6.9785,  4.1305, 42.3899,  4.8936,  3.9745, 49.0284]],

        [[12.9790, 26.3162,  8.1171,  1.5249,  1.4472, 29.4785]],

        [[14.9789, 28.5009,  8.3672,  1.1522,  1.0189, 21.2999]],

        [[11.6144, 25.4060, 17.1483,  2.5118,  1.8744, 43.2388]]])
Signal Variance: tensor([ 0.0553,  2.6440, 16.9516,  0.5865])
Estimated target variance: tensor([0.0110, 0.7400, 6.3781, 0.1435])
N: 190
Signal to noise ratio: tensor([14.8102, 80.0927, 79.6848, 44.8897])
Bound on condition number: tensor([  41676.2442, 1218820.4855, 1206438.4006,  382867.3580])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0009748290596043952, policy loss: 11.86626733213399
Experience 19, Iter 1, disc loss: 0.0008723695942112872, policy loss: 11.069875759014153
Experience 19, Iter 2, disc loss: 0.001552533992040109, policy loss: 10.769346897973492
Experience 19, Iter 3, disc loss: 0.001075219133281996, policy loss: 11.543251381940813
Experience 19, Iter 4, disc loss: 0.001529623724778962, policy loss: 11.00621721662654
Experience 19, Iter 5, disc loss: 0.0010431352012406493, policy loss: 10.879502508412902
Experience 19, Iter 6, disc loss: 0.0010720451523287813, policy loss: 12.383821907114939
Experience 19, Iter 7, disc loss: 0.0010637137474597458, policy loss: 11.403075884889336
Experience 19, Iter 8, disc loss: 0.0011368008169267568, policy loss: 11.606598667334298
Experience 19, Iter 9, disc loss: 0.001004386549707376, policy loss: 11.895805222008168
Experience 19, Iter 10, disc loss: 0.0007289504476446189, policy loss: 11.369687916546305
Experience 19, Iter 11, disc loss: 0.0010734469351917392, policy loss: 11.549744395761008
Experience 19, Iter 12, disc loss: 0.0009706109721052686, policy loss: 11.378546747385176
Experience 19, Iter 13, disc loss: 0.0014634795974575905, policy loss: 10.621989104969554
Experience 19, Iter 14, disc loss: 0.001403496317425183, policy loss: 10.718498889679374
Experience 19, Iter 15, disc loss: 0.0009310801106541947, policy loss: 10.839769685810658
Experience 19, Iter 16, disc loss: 0.0009976708892294535, policy loss: 10.964754193179026
Experience 19, Iter 17, disc loss: 0.0017204129909847222, policy loss: 10.370329326333714
Experience 19, Iter 18, disc loss: 0.0015213220070654258, policy loss: 11.12554476503654
Experience 19, Iter 19, disc loss: 0.0011407868385029279, policy loss: 10.502338819247402
Experience 19, Iter 20, disc loss: 0.001647608904327268, policy loss: 10.012304137538225
Experience 19, Iter 21, disc loss: 0.0012888657143890546, policy loss: 10.809384356011487
Experience 19, Iter 22, disc loss: 0.0007779572814746954, policy loss: 10.917287979709307
Experience 19, Iter 23, disc loss: 0.000820760185851097, policy loss: 12.44537218542236
Experience 19, Iter 24, disc loss: 0.0014744049210272162, policy loss: 10.319698402945466
Experience 19, Iter 25, disc loss: 0.0018452681108587412, policy loss: 10.52250043781023
Experience 19, Iter 26, disc loss: 0.0014223241763823965, policy loss: 10.141839450350544
Experience 19, Iter 27, disc loss: 0.0015671377556177083, policy loss: 10.904547285634077
Experience 19, Iter 28, disc loss: 0.001037626491179984, policy loss: 10.5909683523736
Experience 19, Iter 29, disc loss: 0.0015270925019201884, policy loss: 10.546156141981415
Experience 19, Iter 30, disc loss: 0.0014575364582194079, policy loss: 10.23236468304559
Experience 19, Iter 31, disc loss: 0.0017846048666279877, policy loss: 10.367275038349646
Experience 19, Iter 32, disc loss: 0.0013782290306005483, policy loss: 10.589371878105837
Experience 19, Iter 33, disc loss: 0.0014928768006744995, policy loss: 10.747845624239783
Experience 19, Iter 34, disc loss: 0.001427030438446942, policy loss: 10.80310699607538
Experience 19, Iter 35, disc loss: 0.0012641621271649067, policy loss: 10.431817962959006
Experience 19, Iter 36, disc loss: 0.0012688788225517361, policy loss: 10.55615543196014
Experience 19, Iter 37, disc loss: 0.0012355966415090011, policy loss: 10.446592314255753
Experience 19, Iter 38, disc loss: 0.0017224901159727803, policy loss: 9.708961902477085
Experience 19, Iter 39, disc loss: 0.0010560885728591548, policy loss: 11.028319537590324
Experience 19, Iter 40, disc loss: 0.002180412696462596, policy loss: 10.456816275681971
Experience 19, Iter 41, disc loss: 0.0017091600149060924, policy loss: 9.946213892813677
Experience 19, Iter 42, disc loss: 0.0013709972420961351, policy loss: 10.487219602654811
Experience 19, Iter 43, disc loss: 0.0012389687946935688, policy loss: 11.113634636590412
Experience 19, Iter 44, disc loss: 0.0015296755759200793, policy loss: 10.330851923459623
Experience 19, Iter 45, disc loss: 0.0015779273248851902, policy loss: 10.081448890756985
Experience 19, Iter 46, disc loss: 0.0011088770462081778, policy loss: 10.389549652086835
Experience 19, Iter 47, disc loss: 0.0012074400015057352, policy loss: 10.631450901736299
Experience 19, Iter 48, disc loss: 0.0010819846435978064, policy loss: 10.752404225997964
Experience 19, Iter 49, disc loss: 0.0012744296885270094, policy loss: 10.460110862183868
Experience 19, Iter 50, disc loss: 0.0009461539272583177, policy loss: 11.655128886743466
Experience 19, Iter 51, disc loss: 0.0013478094111436525, policy loss: 10.051216194113001
Experience 19, Iter 52, disc loss: 0.001406328774991889, policy loss: 10.010129612172381
Experience 19, Iter 53, disc loss: 0.001303168315131826, policy loss: 10.3347659934836
Experience 19, Iter 54, disc loss: 0.0016418428105211915, policy loss: 10.440830579126136
Experience 19, Iter 55, disc loss: 0.001316072567567951, policy loss: 10.03160389893548
Experience 19, Iter 56, disc loss: 0.001305845771734575, policy loss: 10.652959352545782
Experience 19, Iter 57, disc loss: 0.001548271429946057, policy loss: 9.964688138206348
Experience 19, Iter 58, disc loss: 0.0016150519627793603, policy loss: 10.253141452024485
Experience 19, Iter 59, disc loss: 0.001313537213149069, policy loss: 10.220977427842772
Experience 19, Iter 60, disc loss: 0.0012471262532378767, policy loss: 10.66665149018087
Experience 19, Iter 61, disc loss: 0.00129735457363678, policy loss: 9.791377051051205
Experience 19, Iter 62, disc loss: 0.0015469999586399036, policy loss: 9.669068028946672
Experience 19, Iter 63, disc loss: 0.0008778894540983333, policy loss: 10.885694021133077
Experience 19, Iter 64, disc loss: 0.0012926313164406578, policy loss: 10.235530000821688
Experience 19, Iter 65, disc loss: 0.001628185712956903, policy loss: 9.429418724360447
Experience 19, Iter 66, disc loss: 0.0014005087734159749, policy loss: 10.070282234948339
Experience 19, Iter 67, disc loss: 0.00142498717214724, policy loss: 9.333684383794917
Experience 19, Iter 68, disc loss: 0.0011549349847423881, policy loss: 10.312336947513867
Experience 19, Iter 69, disc loss: 0.0011738075401907118, policy loss: 9.895954908721901
Experience 19, Iter 70, disc loss: 0.001370925360881375, policy loss: 9.658211402065566
Experience 19, Iter 71, disc loss: 0.0013471987217628528, policy loss: 10.035582353098198
Experience 19, Iter 72, disc loss: 0.0015840042978774284, policy loss: 9.935749504013636
Experience 19, Iter 73, disc loss: 0.001220769549042097, policy loss: 10.581451009277085
Experience 19, Iter 74, disc loss: 0.0016492344987358047, policy loss: 9.173628911887382
Experience 19, Iter 75, disc loss: 0.001856830227961335, policy loss: 9.424700468177322
Experience 19, Iter 76, disc loss: 0.001540289788442444, policy loss: 9.772472185347784
Experience 19, Iter 77, disc loss: 0.0012638238124891214, policy loss: 10.539982221686927
Experience 19, Iter 78, disc loss: 0.001733251143314822, policy loss: 10.039379537783965
Experience 19, Iter 79, disc loss: 0.0018523010344095553, policy loss: 9.575472744314151
Experience 19, Iter 80, disc loss: 0.001174877781240853, policy loss: 10.072288815887166
Experience 19, Iter 81, disc loss: 0.0021488446948289717, policy loss: 9.49262513330874
Experience 19, Iter 82, disc loss: 0.0015393581172514002, policy loss: 9.678216597128326
Experience 19, Iter 83, disc loss: 0.0014920873613668635, policy loss: 9.794624033834166
Experience 19, Iter 84, disc loss: 0.00145014797430404, policy loss: 9.541697223844272
Experience 19, Iter 85, disc loss: 0.0016885344503640462, policy loss: 9.69822905700185
Experience 19, Iter 86, disc loss: 0.0014913957539523838, policy loss: 10.464169269787876
Experience 19, Iter 87, disc loss: 0.00191650562059702, policy loss: 10.272340642637428
Experience 19, Iter 88, disc loss: 0.0015247191955774725, policy loss: 9.973095562239237
Experience 19, Iter 89, disc loss: 0.0013316095048158125, policy loss: 10.305233143810757
Experience 19, Iter 90, disc loss: 0.001529144596516671, policy loss: 10.679932291525448
Experience 19, Iter 91, disc loss: 0.0014030466526228959, policy loss: 10.085855770062363
Experience 19, Iter 92, disc loss: 0.0015086242987445283, policy loss: 10.392178453584911
Experience 19, Iter 93, disc loss: 0.0012895397168039284, policy loss: 10.209046495978644
Experience 19, Iter 94, disc loss: 0.0014087214048332897, policy loss: 11.101637138640978
Experience 19, Iter 95, disc loss: 0.0017683895476633888, policy loss: 9.789814561619057
Experience 19, Iter 96, disc loss: 0.001733023706245576, policy loss: 9.850198535078718
Experience 19, Iter 97, disc loss: 0.0014412876455750837, policy loss: 10.33372879203925
Experience 19, Iter 98, disc loss: 0.0014719905210710596, policy loss: 10.469456809770346
Experience 19, Iter 99, disc loss: 0.0015634031483865206, policy loss: 9.97305593236699
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1941],
        [1.6503],
        [0.0380]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0048, 0.1194, 1.7097, 0.0234, 0.0248, 4.1068]],

        [[0.0048, 0.1194, 1.7097, 0.0234, 0.0248, 4.1068]],

        [[0.0048, 0.1194, 1.7097, 0.0234, 0.0248, 4.1068]],

        [[0.0048, 0.1194, 1.7097, 0.0234, 0.0248, 4.1068]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.7763, 6.6012, 0.1521], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.7763, 6.6012, 0.1521])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.380
Iter 2/2000 - Loss: 3.553
Iter 3/2000 - Loss: 3.245
Iter 4/2000 - Loss: 3.273
Iter 5/2000 - Loss: 3.344
Iter 6/2000 - Loss: 3.255
Iter 7/2000 - Loss: 3.116
Iter 8/2000 - Loss: 3.023
Iter 9/2000 - Loss: 2.981
Iter 10/2000 - Loss: 2.933
Iter 11/2000 - Loss: 2.833
Iter 12/2000 - Loss: 2.684
Iter 13/2000 - Loss: 2.516
Iter 14/2000 - Loss: 2.350
Iter 15/2000 - Loss: 2.185
Iter 16/2000 - Loss: 2.004
Iter 17/2000 - Loss: 1.792
Iter 18/2000 - Loss: 1.550
Iter 19/2000 - Loss: 1.285
Iter 20/2000 - Loss: 1.005
Iter 1981/2000 - Loss: -7.691
Iter 1982/2000 - Loss: -7.691
Iter 1983/2000 - Loss: -7.691
Iter 1984/2000 - Loss: -7.691
Iter 1985/2000 - Loss: -7.691
Iter 1986/2000 - Loss: -7.691
Iter 1987/2000 - Loss: -7.691
Iter 1988/2000 - Loss: -7.691
Iter 1989/2000 - Loss: -7.691
Iter 1990/2000 - Loss: -7.691
Iter 1991/2000 - Loss: -7.691
Iter 1992/2000 - Loss: -7.691
Iter 1993/2000 - Loss: -7.691
Iter 1994/2000 - Loss: -7.691
Iter 1995/2000 - Loss: -7.691
Iter 1996/2000 - Loss: -7.691
Iter 1997/2000 - Loss: -7.691
Iter 1998/2000 - Loss: -7.691
Iter 1999/2000 - Loss: -7.691
Iter 2000/2000 - Loss: -7.691
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[ 7.9419,  4.2184, 41.2132,  5.2521,  4.0093, 47.0915]],

        [[13.0059, 27.3218,  8.0635,  1.5838,  1.3154, 29.6483]],

        [[14.6044, 30.3337,  8.2953,  1.0973,  1.0128, 20.4140]],

        [[11.8062, 26.7668, 17.0456,  2.3337,  1.8545, 42.7671]]])
Signal Variance: tensor([ 0.0560,  2.5900, 15.1660,  0.5501])
Estimated target variance: tensor([0.0113, 0.7763, 6.6012, 0.1521])
N: 200
Signal to noise ratio: tensor([15.1033, 79.7170, 75.9843, 43.0030])
Bound on condition number: tensor([  45622.7703, 1270961.5486, 1154724.8656,  369853.2631])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0017785543213848748, policy loss: 9.345605473436752
Experience 20, Iter 1, disc loss: 0.0021411295365862107, policy loss: 9.32772961556337
Experience 20, Iter 2, disc loss: 0.0020042528457590303, policy loss: 9.316093176923472
Experience 20, Iter 3, disc loss: 0.0017967671939713818, policy loss: 8.994725970402515
Experience 20, Iter 4, disc loss: 0.0015970443365102267, policy loss: 9.783503695397641
Experience 20, Iter 5, disc loss: 0.001787489734314493, policy loss: 9.274422585983254
Experience 20, Iter 6, disc loss: 0.001928964326336216, policy loss: 8.95523213645209
Experience 20, Iter 7, disc loss: 0.0018642590734275289, policy loss: 8.984024195579483
Experience 20, Iter 8, disc loss: 0.0017959735491433974, policy loss: 9.48314464942112
Experience 20, Iter 9, disc loss: 0.0013220681423278602, policy loss: 10.612558980245765
Experience 20, Iter 10, disc loss: 0.001422836341679658, policy loss: 9.653635266023112
Experience 20, Iter 11, disc loss: 0.0015296023217596236, policy loss: 10.73908695387075
Experience 20, Iter 12, disc loss: 0.0014745702384253866, policy loss: 10.260576250252708
Experience 20, Iter 13, disc loss: 0.001693524165442032, policy loss: 10.084360440636093
Experience 20, Iter 14, disc loss: 0.0016497935124850204, policy loss: 10.373282912536638
Experience 20, Iter 15, disc loss: 0.0018101267769579134, policy loss: 9.355710549376422
Experience 20, Iter 16, disc loss: 0.0018191927214400597, policy loss: 9.533448754294074
Experience 20, Iter 17, disc loss: 0.0014660377202031292, policy loss: 10.620550030434632
Experience 20, Iter 18, disc loss: 0.0017848761132211641, policy loss: 9.473189921046242
Experience 20, Iter 19, disc loss: 0.0023416791250023172, policy loss: 8.788761534390211
Experience 20, Iter 20, disc loss: 0.0017512216886997544, policy loss: 10.218455667714128
Experience 20, Iter 21, disc loss: 0.0018924807031602323, policy loss: 9.91378808508772
Experience 20, Iter 22, disc loss: 0.00173958451867463, policy loss: 10.160480981115398
Experience 20, Iter 23, disc loss: 0.001992223092112862, policy loss: 10.164806552822823
Experience 20, Iter 24, disc loss: 0.0020106453683597847, policy loss: 9.878366117702493
Experience 20, Iter 25, disc loss: 0.002431192694518954, policy loss: 9.407177655293046
Experience 20, Iter 26, disc loss: 0.0019191050626416507, policy loss: 9.890805972362369
Experience 20, Iter 27, disc loss: 0.0017374418732088656, policy loss: 10.11357032759585
Experience 20, Iter 28, disc loss: 0.0021270486366168637, policy loss: 9.837081981456164
Experience 20, Iter 29, disc loss: 0.0025982173737859517, policy loss: 9.46659688157907
Experience 20, Iter 30, disc loss: 0.0024704546167933833, policy loss: 10.267497127039984
Experience 20, Iter 31, disc loss: 0.0021070714215178023, policy loss: 9.86041720274043
Experience 20, Iter 32, disc loss: 0.0020395058748786605, policy loss: 10.327882929849721
Experience 20, Iter 33, disc loss: 0.0022088315533602227, policy loss: 10.527403500446825
Experience 20, Iter 34, disc loss: 0.0022979070229410645, policy loss: 9.438885075574888
Experience 20, Iter 35, disc loss: 0.0017063824608042305, policy loss: 9.872607941909639
Experience 20, Iter 36, disc loss: 0.002486963795107331, policy loss: 9.77326556775266
Experience 20, Iter 37, disc loss: 0.0029153048243267727, policy loss: 9.881454600713283
Experience 20, Iter 38, disc loss: 0.00292974291953135, policy loss: 9.623757838679948
Experience 20, Iter 39, disc loss: 0.0023348871207107646, policy loss: 9.89305749319742
Experience 20, Iter 40, disc loss: 0.001911880368183736, policy loss: 10.172296539227654
Experience 20, Iter 41, disc loss: 0.0022878911588133646, policy loss: 10.506404522553133
Experience 20, Iter 42, disc loss: 0.0023162076449775503, policy loss: 10.131770023685371
Experience 20, Iter 43, disc loss: 0.002676256212682795, policy loss: 8.992447642830367
Experience 20, Iter 44, disc loss: 0.0028552130409173074, policy loss: 9.732187827323086
Experience 20, Iter 45, disc loss: 0.001875377307394364, policy loss: 9.805289066365468
Experience 20, Iter 46, disc loss: 0.0026954496532349925, policy loss: 9.381545539665208
Experience 20, Iter 47, disc loss: 0.002619588056522541, policy loss: 9.52117050848945
Experience 20, Iter 48, disc loss: 0.0023243579589462557, policy loss: 10.276809442652993
Experience 20, Iter 49, disc loss: 0.0028642378033056203, policy loss: 10.71716725065243
Experience 20, Iter 50, disc loss: 0.0023073535823949203, policy loss: 9.596828648753158
Experience 20, Iter 51, disc loss: 0.0029923585865907114, policy loss: 9.811228593125918
Experience 20, Iter 52, disc loss: 0.00266019161176233, policy loss: 9.128404715425633
Experience 20, Iter 53, disc loss: 0.0035514461591549164, policy loss: 8.783560958466325
Experience 20, Iter 54, disc loss: 0.0034245291531202243, policy loss: 9.153379158903906
Experience 20, Iter 55, disc loss: 0.002742786204163796, policy loss: 9.498696317352078
Experience 20, Iter 56, disc loss: 0.003074763817245367, policy loss: 8.887466078366725
Experience 20, Iter 57, disc loss: 0.003169127226787945, policy loss: 9.610863722383844
Experience 20, Iter 58, disc loss: 0.002545319741494372, policy loss: 10.443330254371674
Experience 20, Iter 59, disc loss: 0.0028446985371232154, policy loss: 9.99448333827766
Experience 20, Iter 60, disc loss: 0.0023328352769263565, policy loss: 10.002984423979488
Experience 20, Iter 61, disc loss: 0.003043300823378227, policy loss: 9.697813851505952
Experience 20, Iter 62, disc loss: 0.003120491829333362, policy loss: 9.592992565725584
Experience 20, Iter 63, disc loss: 0.003833675598501188, policy loss: 10.391728244095905
Experience 20, Iter 64, disc loss: 0.0026762467176096203, policy loss: 10.266942110381567
Experience 20, Iter 65, disc loss: 0.0036774144364216024, policy loss: 9.271048993508282
Experience 20, Iter 66, disc loss: 0.0033119279476633108, policy loss: 10.245576004411141
Experience 20, Iter 67, disc loss: 0.0032421137874948736, policy loss: 10.341256952936558
Experience 20, Iter 68, disc loss: 0.00281272866106895, policy loss: 9.932569127830028
Experience 20, Iter 69, disc loss: 0.003194167958592283, policy loss: 10.137438426748888
Experience 20, Iter 70, disc loss: 0.0033657226319703123, policy loss: 9.921887123728892
Experience 20, Iter 71, disc loss: 0.0027639540461767663, policy loss: 10.07874966132547
Experience 20, Iter 72, disc loss: 0.0033137711213108256, policy loss: 9.394545837297983
Experience 20, Iter 73, disc loss: 0.002715632376059091, policy loss: 10.050145530806923
Experience 20, Iter 74, disc loss: 0.0022349693395181873, policy loss: 11.20741332646566
Experience 20, Iter 75, disc loss: 0.0028682854718717317, policy loss: 10.25918297644984
Experience 20, Iter 76, disc loss: 0.0027761932094874735, policy loss: 10.534180092432806
Experience 20, Iter 77, disc loss: 0.0028899031459392794, policy loss: 9.980979515666041
Experience 20, Iter 78, disc loss: 0.002735281662807113, policy loss: 9.609988640021259
Experience 20, Iter 79, disc loss: 0.004073120462724431, policy loss: 8.918393024592966
Experience 20, Iter 80, disc loss: 0.003582960321135759, policy loss: 9.267809414843045
Experience 20, Iter 81, disc loss: 0.003027004405177831, policy loss: 10.010518505568431
Experience 20, Iter 82, disc loss: 0.002565393642118787, policy loss: 11.281442555365057
Experience 20, Iter 83, disc loss: 0.0027524118743220034, policy loss: 10.872911476921217
Experience 20, Iter 84, disc loss: 0.002758200471613214, policy loss: 10.568561932934687
Experience 20, Iter 85, disc loss: 0.002970128197493142, policy loss: 10.293888306437005
Experience 20, Iter 86, disc loss: 0.003097565310425392, policy loss: 9.774736506866692
Experience 20, Iter 87, disc loss: 0.004083403683098862, policy loss: 9.008843882106875
Experience 20, Iter 88, disc loss: 0.0027683885114939697, policy loss: 10.879121207785621
Experience 20, Iter 89, disc loss: 0.0035901193082822063, policy loss: 9.408714700569362
Experience 20, Iter 90, disc loss: 0.0027417840340518587, policy loss: 9.868495974227626
Experience 20, Iter 91, disc loss: 0.0031850899040539735, policy loss: 9.42449929179444
Experience 20, Iter 92, disc loss: 0.003905074569300434, policy loss: 8.862747406720727
Experience 20, Iter 93, disc loss: 0.003387057663644352, policy loss: 10.368567492072088
Experience 20, Iter 94, disc loss: 0.0032928761673914914, policy loss: 9.62021650173413
Experience 20, Iter 95, disc loss: 0.0030845352462117778, policy loss: 10.490306493083724
Experience 20, Iter 96, disc loss: 0.0031013072398488367, policy loss: 9.932178542555631
Experience 20, Iter 97, disc loss: 0.0034859418151329804, policy loss: 9.93821685471575
Experience 20, Iter 98, disc loss: 0.0031776714651938925, policy loss: 10.071729139754213
Experience 20, Iter 99, disc loss: 0.00372087548738354, policy loss: 9.478735248044389
