Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0027],
        [0.2620],
        [0.0061]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0034, 0.0109, 1.0480, 0.0242], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0034, 0.0109, 1.0480, 0.0242])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.821
Iter 2/2000 - Loss: -0.714
Iter 3/2000 - Loss: -0.927
Iter 4/2000 - Loss: -0.510
Iter 5/2000 - Loss: -0.411
Iter 6/2000 - Loss: -0.603
Iter 7/2000 - Loss: -0.805
Iter 8/2000 - Loss: -0.865
Iter 9/2000 - Loss: -0.824
Iter 10/2000 - Loss: -0.821
Iter 11/2000 - Loss: -0.915
Iter 12/2000 - Loss: -1.047
Iter 13/2000 - Loss: -1.129
Iter 14/2000 - Loss: -1.122
Iter 15/2000 - Loss: -1.056
Iter 16/2000 - Loss: -0.989
Iter 17/2000 - Loss: -0.970
Iter 18/2000 - Loss: -1.013
Iter 19/2000 - Loss: -1.100
Iter 20/2000 - Loss: -1.196
Iter 1981/2000 - Loss: -1.478
Iter 1982/2000 - Loss: -1.478
Iter 1983/2000 - Loss: -1.478
Iter 1984/2000 - Loss: -1.478
Iter 1985/2000 - Loss: -1.478
Iter 1986/2000 - Loss: -1.478
Iter 1987/2000 - Loss: -1.478
Iter 1988/2000 - Loss: -1.478
Iter 1989/2000 - Loss: -1.478
Iter 1990/2000 - Loss: -1.478
Iter 1991/2000 - Loss: -1.478
Iter 1992/2000 - Loss: -1.478
Iter 1993/2000 - Loss: -1.478
Iter 1994/2000 - Loss: -1.478
Iter 1995/2000 - Loss: -1.478
Iter 1996/2000 - Loss: -1.478
Iter 1997/2000 - Loss: -1.478
Iter 1998/2000 - Loss: -1.478
Iter 1999/2000 - Loss: -1.478
Iter 2000/2000 - Loss: -1.478
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0020],
        [0.1582],
        [0.0043]])
Lengthscale: tensor([[[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]],

        [[0.0109, 0.0376, 0.2922, 0.0065, 0.0004, 0.0156]]])
Signal Variance: tensor([0.0024, 0.0078, 0.7850, 0.0175])
Estimated target variance: tensor([0.0034, 0.0109, 1.0480, 0.0242])
N: 10
Signal to noise ratio: tensor([2.0009, 2.0041, 2.2278, 2.0082])
Bound on condition number: tensor([41.0364, 41.1623, 50.6313, 41.3271])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4243374633868289, policy loss: 0.8293133404292796
Experience 1, Iter 1, disc loss: 1.417038853355853, policy loss: 0.8244547035327677
Experience 1, Iter 2, disc loss: 1.3946502022042604, policy loss: 0.8335497869651969
Experience 1, Iter 3, disc loss: 1.3816246372831027, policy loss: 0.8339003656255551
Experience 1, Iter 4, disc loss: 1.3700595160003841, policy loss: 0.8324889865065316
Experience 1, Iter 5, disc loss: 1.359876142705156, policy loss: 0.829305015656335
Experience 1, Iter 6, disc loss: 1.349341833018113, policy loss: 0.8257599014488912
Experience 1, Iter 7, disc loss: 1.3288264014129305, policy loss: 0.8341360491455718
Experience 1, Iter 8, disc loss: 1.3131710076216847, policy loss: 0.8379559745579828
Experience 1, Iter 9, disc loss: 1.3091812219578616, policy loss: 0.8259600216310314
Experience 1, Iter 10, disc loss: 1.2918989519393327, policy loss: 0.8317299965602485
Experience 1, Iter 11, disc loss: 1.2759836661188029, policy loss: 0.8353131525045714
Experience 1, Iter 12, disc loss: 1.2708639391401975, policy loss: 0.826205008429358
Experience 1, Iter 13, disc loss: 1.2487843465838941, policy loss: 0.8382059044245534
Experience 1, Iter 14, disc loss: 1.2410756692501828, policy loss: 0.832364902774481
Experience 1, Iter 15, disc loss: 1.2237579424784628, policy loss: 0.8385288655529167
Experience 1, Iter 16, disc loss: 1.2177492598179536, policy loss: 0.829769610541998
Experience 1, Iter 17, disc loss: 1.205656342189434, policy loss: 0.8295290284412782
Experience 1, Iter 18, disc loss: 1.1952125796582747, policy loss: 0.8268104510401235
Experience 1, Iter 19, disc loss: 1.1785382505471476, policy loss: 0.8324105048051975
Experience 1, Iter 20, disc loss: 1.158758891563418, policy loss: 0.8423640566938292
Experience 1, Iter 21, disc loss: 1.1576142202279476, policy loss: 0.8267423716670151
Experience 1, Iter 22, disc loss: 1.1386924191430947, policy loss: 0.8355263012901512
Experience 1, Iter 23, disc loss: 1.1296473586713962, policy loss: 0.8308476546538032
Experience 1, Iter 24, disc loss: 1.1210720324992196, policy loss: 0.8253624188939498
Experience 1, Iter 25, disc loss: 1.1030171486364266, policy loss: 0.8332632508746203
Experience 1, Iter 26, disc loss: 1.096599362766941, policy loss: 0.8250859918099933
Experience 1, Iter 27, disc loss: 1.078761496320244, policy loss: 0.8324824217321614
Experience 1, Iter 28, disc loss: 1.063662162768499, policy loss: 0.8364833486432234
Experience 1, Iter 29, disc loss: 1.0514543861022543, policy loss: 0.8367807742527977
Experience 1, Iter 30, disc loss: 1.0460322251671164, policy loss: 0.8276040153443757
Experience 1, Iter 31, disc loss: 1.0366298697252052, policy loss: 0.8239383071872206
Experience 1, Iter 32, disc loss: 1.0148344465523738, policy loss: 0.8377880718967213
Experience 1, Iter 33, disc loss: 1.0139044030809015, policy loss: 0.8234935912539186
Experience 1, Iter 34, disc loss: 1.0059820905585486, policy loss: 0.8186047772899738
Experience 1, Iter 35, disc loss: 0.9823853641266889, policy loss: 0.8350475660406962
Experience 1, Iter 36, disc loss: 0.9701298974593304, policy loss: 0.8360973792380395
Experience 1, Iter 37, disc loss: 0.949681613870674, policy loss: 0.8478626264884349
Experience 1, Iter 38, disc loss: 0.9514692540166665, policy loss: 0.8310384809735334
Experience 1, Iter 39, disc loss: 0.931077663291655, policy loss: 0.8435391193474162
Experience 1, Iter 40, disc loss: 0.9196407303580946, policy loss: 0.8442191778675815
Experience 1, Iter 41, disc loss: 0.9161851089401692, policy loss: 0.8353657311477097
Experience 1, Iter 42, disc loss: 0.911231884394561, policy loss: 0.8279763903210594
Experience 1, Iter 43, disc loss: 0.8995696414457696, policy loss: 0.8312380877238612
Experience 1, Iter 44, disc loss: 0.8715173958268896, policy loss: 0.8545178117373239
Experience 1, Iter 45, disc loss: 0.8714136349309413, policy loss: 0.8421724464141077
Experience 1, Iter 46, disc loss: 0.8540470137668659, policy loss: 0.8540972832712169
Experience 1, Iter 47, disc loss: 0.8410871151075963, policy loss: 0.8577501652996131
Experience 1, Iter 48, disc loss: 0.829765470153755, policy loss: 0.8616507241190653
Experience 1, Iter 49, disc loss: 0.8303117228844417, policy loss: 0.8488684743425275
Experience 1, Iter 50, disc loss: 0.8328668184172177, policy loss: 0.832103789129845
Experience 1, Iter 51, disc loss: 0.797828342588971, policy loss: 0.8707300966777454
Experience 1, Iter 52, disc loss: 0.7917305484288326, policy loss: 0.8674803417265388
Experience 1, Iter 53, disc loss: 0.7825419671181603, policy loss: 0.8684021711584179
Experience 1, Iter 54, disc loss: 0.7897547053977435, policy loss: 0.8494963038933881
Experience 1, Iter 55, disc loss: 0.7719644881780896, policy loss: 0.8624276836199939
Experience 1, Iter 56, disc loss: 0.7733770147932614, policy loss: 0.8526348103727222
Experience 1, Iter 57, disc loss: 0.7587164864626441, policy loss: 0.8638845882631667
Experience 1, Iter 58, disc loss: 0.7482802664201742, policy loss: 0.8675372575517489
Experience 1, Iter 59, disc loss: 0.7388307788730695, policy loss: 0.8707032516933962
Experience 1, Iter 60, disc loss: 0.7375009994496873, policy loss: 0.8654902604727823
Experience 1, Iter 61, disc loss: 0.7237814310792181, policy loss: 0.878301397568584
Experience 1, Iter 62, disc loss: 0.712701502582788, policy loss: 0.8849720322740189
Experience 1, Iter 63, disc loss: 0.7188582555595509, policy loss: 0.8681566066781327
Experience 1, Iter 64, disc loss: 0.7133127812549387, policy loss: 0.8683635502821783
Experience 1, Iter 65, disc loss: 0.6937565002621413, policy loss: 0.8848442908152406
Experience 1, Iter 66, disc loss: 0.6986371680613959, policy loss: 0.8733191965759031
Experience 1, Iter 67, disc loss: 0.6908318221168814, policy loss: 0.8768803878489091
Experience 1, Iter 68, disc loss: 0.6790576642267994, policy loss: 0.8840161291493551
Experience 1, Iter 69, disc loss: 0.6788268736752658, policy loss: 0.8773553707039166
Experience 1, Iter 70, disc loss: 0.6566448000783496, policy loss: 0.9028758355148503
Experience 1, Iter 71, disc loss: 0.6425653400098685, policy loss: 0.915973651838158
Experience 1, Iter 72, disc loss: 0.630131940549246, policy loss: 0.9273857023801328
Experience 1, Iter 73, disc loss: 0.624754379340272, policy loss: 0.9273554653493281
Experience 1, Iter 74, disc loss: 0.614648362092127, policy loss: 0.9422133357811893
Experience 1, Iter 75, disc loss: 0.6346630004234995, policy loss: 0.9017817223010294
Experience 1, Iter 76, disc loss: 0.6225764880258309, policy loss: 0.9146082286380907
Experience 1, Iter 77, disc loss: 0.607122281253565, policy loss: 0.9345244057134002
Experience 1, Iter 78, disc loss: 0.5951816396307581, policy loss: 0.948002494903702
Experience 1, Iter 79, disc loss: 0.5955967674817777, policy loss: 0.9448899823608778
Experience 1, Iter 80, disc loss: 0.5875310078367703, policy loss: 0.9479583853327982
Experience 1, Iter 81, disc loss: 0.5859174343802308, policy loss: 0.9490208512508751
Experience 1, Iter 82, disc loss: 0.5890110514640897, policy loss: 0.9383617758473598
Experience 1, Iter 83, disc loss: 0.5868709265600456, policy loss: 0.9443188404731544
Experience 1, Iter 84, disc loss: 0.5691061194902868, policy loss: 0.9652047629235926
Experience 1, Iter 85, disc loss: 0.5765466238186397, policy loss: 0.9457406744994202
Experience 1, Iter 86, disc loss: 0.5792247325291656, policy loss: 0.9491918889531459
Experience 1, Iter 87, disc loss: 0.5651832200269054, policy loss: 0.9634501349877294
Experience 1, Iter 88, disc loss: 0.5271208123638081, policy loss: 1.0199137232844668
Experience 1, Iter 89, disc loss: 0.5284203802938863, policy loss: 1.0145412492188635
Experience 1, Iter 90, disc loss: 0.5389267381544889, policy loss: 0.9885287352553054
Experience 1, Iter 91, disc loss: 0.5668192876262086, policy loss: 0.9469252796424558
Experience 1, Iter 92, disc loss: 0.5178807291599412, policy loss: 1.0250047552736075
Experience 1, Iter 93, disc loss: 0.4946549016775428, policy loss: 1.0644069372339504
Experience 1, Iter 94, disc loss: 0.4995422200080839, policy loss: 1.0490447430490029
Experience 1, Iter 95, disc loss: 0.48712552812176785, policy loss: 1.0708701735323551
Experience 1, Iter 96, disc loss: 0.5104988384867879, policy loss: 1.0359946250179688
Experience 1, Iter 97, disc loss: 0.5158328160084276, policy loss: 1.0212176239883233
Experience 1, Iter 98, disc loss: 0.49269939582485417, policy loss: 1.060866000899385
Experience 1, Iter 99, disc loss: 0.4893941874090649, policy loss: 1.0808232126933925
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0100],
        [0.1275],
        [0.0030]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.2639e-02, 1.1240e-01, 1.4227e-01, 7.1212e-03, 2.5182e-04,
          4.4626e-01]],

        [[3.2639e-02, 1.1240e-01, 1.4227e-01, 7.1212e-03, 2.5182e-04,
          4.4626e-01]],

        [[3.2639e-02, 1.1240e-01, 1.4227e-01, 7.1212e-03, 2.5182e-04,
          4.4626e-01]],

        [[3.2639e-02, 1.1240e-01, 1.4227e-01, 7.1212e-03, 2.5182e-04,
          4.4626e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0127, 0.0402, 0.5098, 0.0120], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0127, 0.0402, 0.5098, 0.0120])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.678
Iter 2/2000 - Loss: -0.081
Iter 3/2000 - Loss: -0.694
Iter 4/2000 - Loss: -0.609
Iter 5/2000 - Loss: -0.380
Iter 6/2000 - Loss: -0.492
Iter 7/2000 - Loss: -0.683
Iter 8/2000 - Loss: -0.732
Iter 9/2000 - Loss: -0.660
Iter 10/2000 - Loss: -0.610
Iter 11/2000 - Loss: -0.652
Iter 12/2000 - Loss: -0.744
Iter 13/2000 - Loss: -0.807
Iter 14/2000 - Loss: -0.818
Iter 15/2000 - Loss: -0.811
Iter 16/2000 - Loss: -0.838
Iter 17/2000 - Loss: -0.906
Iter 18/2000 - Loss: -0.985
Iter 19/2000 - Loss: -1.047
Iter 20/2000 - Loss: -1.095
Iter 1981/2000 - Loss: -6.318
Iter 1982/2000 - Loss: -6.318
Iter 1983/2000 - Loss: -6.319
Iter 1984/2000 - Loss: -6.319
Iter 1985/2000 - Loss: -6.319
Iter 1986/2000 - Loss: -6.319
Iter 1987/2000 - Loss: -6.319
Iter 1988/2000 - Loss: -6.319
Iter 1989/2000 - Loss: -6.319
Iter 1990/2000 - Loss: -6.319
Iter 1991/2000 - Loss: -6.319
Iter 1992/2000 - Loss: -6.319
Iter 1993/2000 - Loss: -6.319
Iter 1994/2000 - Loss: -6.319
Iter 1995/2000 - Loss: -6.319
Iter 1996/2000 - Loss: -6.319
Iter 1997/2000 - Loss: -6.319
Iter 1998/2000 - Loss: -6.319
Iter 1999/2000 - Loss: -6.319
Iter 2000/2000 - Loss: -6.319
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 2.7900,  4.6424, 51.9715, 20.7381,  7.7163, 57.6411]],

        [[12.3287,  8.5490,  4.2719,  0.7770,  0.1124,  1.6379]],

        [[35.7418, 49.6144, 20.6527,  1.1173,  6.7105, 11.6463]],

        [[34.4718, 54.0800,  6.1768,  2.1259,  7.2197, 19.4997]]])
Signal Variance: tensor([ 0.0415,  0.0270, 10.2207,  0.1322])
Estimated target variance: tensor([0.0127, 0.0402, 0.5098, 0.0120])
N: 20
Signal to noise ratio: tensor([10.8976,  8.1055, 69.2729, 22.8851])
Bound on condition number: tensor([ 2376.1542,  1314.9714, 95975.7585, 10475.5832])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.5654292199439163, policy loss: 0.9227021721927322
Experience 2, Iter 1, disc loss: 0.6346773716098382, policy loss: 0.8510560733134329
Experience 2, Iter 2, disc loss: 0.696177322162955, policy loss: 0.7800523155199406
Experience 2, Iter 3, disc loss: 0.6754063605924234, policy loss: 0.8097265544247376
Experience 2, Iter 4, disc loss: 0.7834252297407885, policy loss: 0.7003348943716903
Experience 2, Iter 5, disc loss: 0.8404270179195699, policy loss: 0.6486733692802575
Experience 2, Iter 6, disc loss: 0.898681814478664, policy loss: 0.6114710104053516
Experience 2, Iter 7, disc loss: 0.8691206355195082, policy loss: 0.6313125805826512
Experience 2, Iter 8, disc loss: 0.9021713380414508, policy loss: 0.6130841117864078
Experience 2, Iter 9, disc loss: 0.9016463634480205, policy loss: 0.6136079825030873
Experience 2, Iter 10, disc loss: 0.9212799995752331, policy loss: 0.6102609249701522
Experience 2, Iter 11, disc loss: 0.8881184327575715, policy loss: 0.6167438519356044
Experience 2, Iter 12, disc loss: 0.9385615630599733, policy loss: 0.5943875510851915
Experience 2, Iter 13, disc loss: 0.9008743842870602, policy loss: 0.6133567436742702
Experience 2, Iter 14, disc loss: 0.9238469121819727, policy loss: 0.598360845698486
Experience 2, Iter 15, disc loss: 0.9228400487613242, policy loss: 0.5947773093778375
Experience 2, Iter 16, disc loss: 0.932899716808189, policy loss: 0.5861615073894062
Experience 2, Iter 17, disc loss: 0.9199049567975587, policy loss: 0.5929802406798781
Experience 2, Iter 18, disc loss: 0.9021955835994051, policy loss: 0.615131214223035
Experience 2, Iter 19, disc loss: 0.9096010731939592, policy loss: 0.6120323556728128
Experience 2, Iter 20, disc loss: 0.9181289976324255, policy loss: 0.6039742809970835
Experience 2, Iter 21, disc loss: 0.921949307867306, policy loss: 0.6034435722156124
Experience 2, Iter 22, disc loss: 0.919107553903141, policy loss: 0.6084215540319013
Experience 2, Iter 23, disc loss: 0.9017293633046006, policy loss: 0.6247068193672239
Experience 2, Iter 24, disc loss: 0.9011582239042786, policy loss: 0.6140377347031414
Experience 2, Iter 25, disc loss: 0.9348945202378341, policy loss: 0.5930572482137773
Experience 2, Iter 26, disc loss: 0.8927604982299874, policy loss: 0.6324442682199851
Experience 2, Iter 27, disc loss: 0.8938630028364276, policy loss: 0.6339487873938542
Experience 2, Iter 28, disc loss: 0.8704211242012023, policy loss: 0.6579383702871744
Experience 2, Iter 29, disc loss: 0.8645828762136426, policy loss: 0.6653727310995157
Experience 2, Iter 30, disc loss: 0.86348657600521, policy loss: 0.6684672328004461
Experience 2, Iter 31, disc loss: 0.8682011180088152, policy loss: 0.6707037323720992
Experience 2, Iter 32, disc loss: 0.84819833817196, policy loss: 0.6888767359588622
Experience 2, Iter 33, disc loss: 0.8515745480433312, policy loss: 0.6921117721072294
Experience 2, Iter 34, disc loss: 0.829620649789648, policy loss: 0.7166280109175511
Experience 2, Iter 35, disc loss: 0.8211303182068362, policy loss: 0.7243589878172538
Experience 2, Iter 36, disc loss: 0.8134676912566368, policy loss: 0.7373784647274475
Experience 2, Iter 37, disc loss: 0.810035598765391, policy loss: 0.7464578285369847
Experience 2, Iter 38, disc loss: 0.7988365294552635, policy loss: 0.7605187560879778
Experience 2, Iter 39, disc loss: 0.7821093853798318, policy loss: 0.7828405565923288
Experience 2, Iter 40, disc loss: 0.7892503386145164, policy loss: 0.7781212455304143
Experience 2, Iter 41, disc loss: 0.7926101683335333, policy loss: 0.772343782614757
Experience 2, Iter 42, disc loss: 0.7450665771625826, policy loss: 0.8379039500922162
Experience 2, Iter 43, disc loss: 0.77549705012941, policy loss: 0.8009749942219813
Experience 2, Iter 44, disc loss: 0.7451737813265163, policy loss: 0.8374700522805554
Experience 2, Iter 45, disc loss: 0.7451531759139618, policy loss: 0.8390032829579779
Experience 2, Iter 46, disc loss: 0.719117931117305, policy loss: 0.8742350618673709
Experience 2, Iter 47, disc loss: 0.7147331484491932, policy loss: 0.8807469935192243
Experience 2, Iter 48, disc loss: 0.7299333008854958, policy loss: 0.8577600954639917
Experience 2, Iter 49, disc loss: 0.709542390753205, policy loss: 0.89084508691902
Experience 2, Iter 50, disc loss: 0.7079431823934789, policy loss: 0.8901062425145471
Experience 2, Iter 51, disc loss: 0.6922261164483072, policy loss: 0.9124903032152549
Experience 2, Iter 52, disc loss: 0.7112051294799562, policy loss: 0.8800658269924273
Experience 2, Iter 53, disc loss: 0.6953523228626041, policy loss: 0.9156007608104043
Experience 2, Iter 54, disc loss: 0.6705889951765408, policy loss: 0.9364998257502528
Experience 2, Iter 55, disc loss: 0.6717461109019203, policy loss: 0.9346285827852936
Experience 2, Iter 56, disc loss: 0.6657493360515707, policy loss: 0.9445920440893241
Experience 2, Iter 57, disc loss: 0.6590559086122483, policy loss: 0.9475204576588079
Experience 2, Iter 58, disc loss: 0.6591052601640734, policy loss: 0.9462715101512632
Experience 2, Iter 59, disc loss: 0.6658346523122632, policy loss: 0.9289210064086202
Experience 2, Iter 60, disc loss: 0.6392734544376149, policy loss: 0.9692870448986468
Experience 2, Iter 61, disc loss: 0.637737107530706, policy loss: 0.9692272596760522
Experience 2, Iter 62, disc loss: 0.6320772809972894, policy loss: 0.979459304290628
Experience 2, Iter 63, disc loss: 0.6431643721363914, policy loss: 0.9452204467086376
Experience 2, Iter 64, disc loss: 0.6238251533979084, policy loss: 0.9756258022755979
Experience 2, Iter 65, disc loss: 0.6273877713122735, policy loss: 0.9641564886041167
Experience 2, Iter 66, disc loss: 0.6015557585037312, policy loss: 1.0007241923216523
Experience 2, Iter 67, disc loss: 0.6228956407580899, policy loss: 0.9657078236557172
Experience 2, Iter 68, disc loss: 0.6049167427609459, policy loss: 0.9862808753984365
Experience 2, Iter 69, disc loss: 0.5892894276051497, policy loss: 1.0124770428098067
Experience 2, Iter 70, disc loss: 0.5762697686389733, policy loss: 1.0262757273927527
Experience 2, Iter 71, disc loss: 0.5574482183615236, policy loss: 1.0612902760406229
Experience 2, Iter 72, disc loss: 0.5749830036928009, policy loss: 1.016572816567201
Experience 2, Iter 73, disc loss: 0.5626875382620702, policy loss: 1.0462196458541828
Experience 2, Iter 74, disc loss: 0.570945417792158, policy loss: 1.008787470689992
Experience 2, Iter 75, disc loss: 0.5299315479644044, policy loss: 1.092568592312101
Experience 2, Iter 76, disc loss: 0.5554999969784442, policy loss: 1.029849411516119
Experience 2, Iter 77, disc loss: 0.5500336351828498, policy loss: 1.04583421561682
Experience 2, Iter 78, disc loss: 0.5481494721929049, policy loss: 1.035508486074347
Experience 2, Iter 79, disc loss: 0.5418604466929613, policy loss: 1.0481669905367714
Experience 2, Iter 80, disc loss: 0.545900632078436, policy loss: 1.0408228808792823
Experience 2, Iter 81, disc loss: 0.5417558350011236, policy loss: 1.039873835349238
Experience 2, Iter 82, disc loss: 0.5214579890813027, policy loss: 1.0800387489880037
Experience 2, Iter 83, disc loss: 0.5392935636774968, policy loss: 1.0371959961408665
Experience 2, Iter 84, disc loss: 0.5163853545919688, policy loss: 1.0796472848218284
Experience 2, Iter 85, disc loss: 0.53011878621584, policy loss: 1.060656003205594
Experience 2, Iter 86, disc loss: 0.5242913002496292, policy loss: 1.067560789854714
Experience 2, Iter 87, disc loss: 0.5098772636406035, policy loss: 1.1118291853258535
Experience 2, Iter 88, disc loss: 0.5096460750302763, policy loss: 1.089847297737
Experience 2, Iter 89, disc loss: 0.49567459971113637, policy loss: 1.11179985878199
Experience 2, Iter 90, disc loss: 0.5004456776390023, policy loss: 1.1090479741381847
Experience 2, Iter 91, disc loss: 0.5109815457942812, policy loss: 1.0809341259780152
Experience 2, Iter 92, disc loss: 0.492480572484472, policy loss: 1.11770548053863
Experience 2, Iter 93, disc loss: 0.47450216780023213, policy loss: 1.156303517842281
Experience 2, Iter 94, disc loss: 0.4888061766725004, policy loss: 1.1257249648889536
Experience 2, Iter 95, disc loss: 0.4821997624344426, policy loss: 1.1390840094481527
Experience 2, Iter 96, disc loss: 0.4791463512768143, policy loss: 1.1482348199144061
Experience 2, Iter 97, disc loss: 0.46726936871945124, policy loss: 1.1786185468987576
Experience 2, Iter 98, disc loss: 0.4615158202735592, policy loss: 1.1742958302812718
Experience 2, Iter 99, disc loss: 0.4688521947195883, policy loss: 1.168494243547682
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.0432],
        [0.4504],
        [0.0083]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0286, 0.2002, 0.3248, 0.0160, 0.0069, 1.7000]],

        [[0.0286, 0.2002, 0.3248, 0.0160, 0.0069, 1.7000]],

        [[0.0286, 0.2002, 0.3248, 0.0160, 0.0069, 1.7000]],

        [[0.0286, 0.2002, 0.3248, 0.0160, 0.0069, 1.7000]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0203, 0.1727, 1.8015, 0.0333], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0203, 0.1727, 1.8015, 0.0333])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.531
Iter 2/2000 - Loss: 1.460
Iter 3/2000 - Loss: 1.411
Iter 4/2000 - Loss: 1.347
Iter 5/2000 - Loss: 1.354
Iter 6/2000 - Loss: 1.330
Iter 7/2000 - Loss: 1.266
Iter 8/2000 - Loss: 1.224
Iter 9/2000 - Loss: 1.193
Iter 10/2000 - Loss: 1.138
Iter 11/2000 - Loss: 1.059
Iter 12/2000 - Loss: 0.975
Iter 13/2000 - Loss: 0.895
Iter 14/2000 - Loss: 0.808
Iter 15/2000 - Loss: 0.699
Iter 16/2000 - Loss: 0.568
Iter 17/2000 - Loss: 0.430
Iter 18/2000 - Loss: 0.294
Iter 19/2000 - Loss: 0.155
Iter 20/2000 - Loss: 0.004
Iter 1981/2000 - Loss: -5.609
Iter 1982/2000 - Loss: -5.609
Iter 1983/2000 - Loss: -5.609
Iter 1984/2000 - Loss: -5.609
Iter 1985/2000 - Loss: -5.609
Iter 1986/2000 - Loss: -5.609
Iter 1987/2000 - Loss: -5.609
Iter 1988/2000 - Loss: -5.609
Iter 1989/2000 - Loss: -5.609
Iter 1990/2000 - Loss: -5.609
Iter 1991/2000 - Loss: -5.609
Iter 1992/2000 - Loss: -5.609
Iter 1993/2000 - Loss: -5.609
Iter 1994/2000 - Loss: -5.609
Iter 1995/2000 - Loss: -5.610
Iter 1996/2000 - Loss: -5.610
Iter 1997/2000 - Loss: -5.610
Iter 1998/2000 - Loss: -5.610
Iter 1999/2000 - Loss: -5.610
Iter 2000/2000 - Loss: -5.610
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[15.3343,  6.3575, 51.0441, 18.6225, 21.0599, 47.0305]],

        [[23.2098, 34.9432, 18.8835,  1.9531,  1.6186, 16.5296]],

        [[21.6365, 41.4920, 14.2385,  0.8017,  2.7803, 12.9996]],

        [[17.6088, 33.8894, 16.1848,  3.2616,  2.1992, 32.3911]]])
Signal Variance: tensor([0.1016, 1.5655, 8.5506, 0.6057])
Estimated target variance: tensor([0.0203, 0.1727, 1.8015, 0.0333])
N: 30
Signal to noise ratio: tensor([16.3012, 58.1821, 57.8648, 44.5806])
Bound on condition number: tensor([  7972.8513, 101555.7975, 100451.1929,  59623.8757])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.32251925121987024, policy loss: 1.6271076836097436
Experience 3, Iter 1, disc loss: 0.31122390633826874, policy loss: 1.6602633968103178
Experience 3, Iter 2, disc loss: 0.3299149595362231, policy loss: 1.5815355358517023
Experience 3, Iter 3, disc loss: 0.32563861296129226, policy loss: 1.6025839109758409
Experience 3, Iter 4, disc loss: 0.32902615308370753, policy loss: 1.5527187212600506
Experience 3, Iter 5, disc loss: 0.33337860009200815, policy loss: 1.5231599744776445
Experience 3, Iter 6, disc loss: 0.34485888332100445, policy loss: 1.4965618048283378
Experience 3, Iter 7, disc loss: 0.3493676304793132, policy loss: 1.452221854252799
Experience 3, Iter 8, disc loss: 0.3482357801429756, policy loss: 1.449986974719868
Experience 3, Iter 9, disc loss: 0.35066349779934913, policy loss: 1.4412174856458866
Experience 3, Iter 10, disc loss: 0.35531977883333443, policy loss: 1.431785126806883
Experience 3, Iter 11, disc loss: 0.3673503484358087, policy loss: 1.3929276845636638
Experience 3, Iter 12, disc loss: 0.3662684504475487, policy loss: 1.395604345294793
Experience 3, Iter 13, disc loss: 0.3513314743821829, policy loss: 1.42307742998634
Experience 3, Iter 14, disc loss: 0.357424438314309, policy loss: 1.3983574807299797
Experience 3, Iter 15, disc loss: 0.3489773576026303, policy loss: 1.4241806872889429
Experience 3, Iter 16, disc loss: 0.3591171301911027, policy loss: 1.3859183956782004
Experience 3, Iter 17, disc loss: 0.36176112841778535, policy loss: 1.3744983700947926
Experience 3, Iter 18, disc loss: 0.34387636773988084, policy loss: 1.4293120111545377
Experience 3, Iter 19, disc loss: 0.3308333099657063, policy loss: 1.46647653016191
Experience 3, Iter 20, disc loss: 0.32304452998239397, policy loss: 1.5182517829263888
Experience 3, Iter 21, disc loss: 0.330428142208033, policy loss: 1.457755355796174
Experience 3, Iter 22, disc loss: 0.30813380864489354, policy loss: 1.547577773809333
Experience 3, Iter 23, disc loss: 0.318843633857421, policy loss: 1.4899310399033485
Experience 3, Iter 24, disc loss: 0.3117777558829849, policy loss: 1.5178311221295049
Experience 3, Iter 25, disc loss: 0.30128960953803613, policy loss: 1.5415731436584652
Experience 3, Iter 26, disc loss: 0.30249861758395363, policy loss: 1.5443725510918138
Experience 3, Iter 27, disc loss: 0.30381940702287125, policy loss: 1.5297131438428861
Experience 3, Iter 28, disc loss: 0.2920382733661191, policy loss: 1.5789635330414022
Experience 3, Iter 29, disc loss: 0.29007408287379166, policy loss: 1.579751295489766
Experience 3, Iter 30, disc loss: 0.3037653645025439, policy loss: 1.5178224003431495
Experience 3, Iter 31, disc loss: 0.28915371734883016, policy loss: 1.573784195533125
Experience 3, Iter 32, disc loss: 0.2877439974104173, policy loss: 1.5798729882690934
Experience 3, Iter 33, disc loss: 0.2671181888871096, policy loss: 1.6637125133792008
Experience 3, Iter 34, disc loss: 0.29053697837083414, policy loss: 1.545751644308445
Experience 3, Iter 35, disc loss: 0.28662784128731317, policy loss: 1.5529152476926238
Experience 3, Iter 36, disc loss: 0.2768489039165546, policy loss: 1.5890016358730608
Experience 3, Iter 37, disc loss: 0.28659412389704225, policy loss: 1.53923855667609
Experience 3, Iter 38, disc loss: 0.2881317841792259, policy loss: 1.5300185115461415
Experience 3, Iter 39, disc loss: 0.2814870801329156, policy loss: 1.543461384408379
Experience 3, Iter 40, disc loss: 0.284861004053209, policy loss: 1.5249729055467274
Experience 3, Iter 41, disc loss: 0.28588695052219903, policy loss: 1.5166456959099834
Experience 3, Iter 42, disc loss: 0.27757850666197215, policy loss: 1.5419940245142734
Experience 3, Iter 43, disc loss: 0.2807378613751512, policy loss: 1.5226526218408527
Experience 3, Iter 44, disc loss: 0.2845335072827666, policy loss: 1.5052891624152371
Experience 3, Iter 45, disc loss: 0.280474241266574, policy loss: 1.513332764252164
Experience 3, Iter 46, disc loss: 0.27677245144787493, policy loss: 1.5213589400532506
Experience 3, Iter 47, disc loss: 0.2739289833702273, policy loss: 1.5282575528464104
Experience 3, Iter 48, disc loss: 0.2714931736504431, policy loss: 1.5321218454085674
Experience 3, Iter 49, disc loss: 0.2670063280477737, policy loss: 1.5445368909120476
Experience 3, Iter 50, disc loss: 0.2666677595028872, policy loss: 1.5424201483116782
Experience 3, Iter 51, disc loss: 0.26361667998562793, policy loss: 1.55027909442224
Experience 3, Iter 52, disc loss: 0.26319408070292244, policy loss: 1.5481073021999443
Experience 3, Iter 53, disc loss: 0.2550129783076419, policy loss: 1.5820977985245368
Experience 3, Iter 54, disc loss: 0.25189207918489814, policy loss: 1.5865468599882724
Experience 3, Iter 55, disc loss: 0.2575289960563772, policy loss: 1.5599405156419226
Experience 3, Iter 56, disc loss: 0.2539073278060954, policy loss: 1.5744269956569732
Experience 3, Iter 57, disc loss: 0.2521218999160263, policy loss: 1.5764719657321158
Experience 3, Iter 58, disc loss: 0.24845635115275028, policy loss: 1.5912647148342518
Experience 3, Iter 59, disc loss: 0.24714888566151813, policy loss: 1.593185490347589
Experience 3, Iter 60, disc loss: 0.24881874916514138, policy loss: 1.583745665627968
Experience 3, Iter 61, disc loss: 0.24177264988540542, policy loss: 1.6318084586810384
Experience 3, Iter 62, disc loss: 0.2435514301263345, policy loss: 1.6062363229378245
Experience 3, Iter 63, disc loss: 0.24005556252504492, policy loss: 1.6149038688420083
Experience 3, Iter 64, disc loss: 0.24076196597138172, policy loss: 1.6098392823421839
Experience 3, Iter 65, disc loss: 0.23803992514739636, policy loss: 1.6173370067602333
Experience 3, Iter 66, disc loss: 0.23934968515602187, policy loss: 1.6106776374700298
Experience 3, Iter 67, disc loss: 0.22739893055985408, policy loss: 1.6594909713052555
Experience 3, Iter 68, disc loss: 0.23378165066751033, policy loss: 1.6425305983355847
Experience 3, Iter 69, disc loss: 0.22875322353801297, policy loss: 1.6505540734105806
Experience 3, Iter 70, disc loss: 0.2277168246921922, policy loss: 1.655031671923659
Experience 3, Iter 71, disc loss: 0.22654731084611132, policy loss: 1.6593552725810252
Experience 3, Iter 72, disc loss: 0.22032781408692195, policy loss: 1.6842435363815118
Experience 3, Iter 73, disc loss: 0.2231118185692189, policy loss: 1.6702476016297083
Experience 3, Iter 74, disc loss: 0.2203474738558435, policy loss: 1.6825724816071705
Experience 3, Iter 75, disc loss: 0.22278851788155113, policy loss: 1.6677925350927982
Experience 3, Iter 76, disc loss: 0.21405320332383088, policy loss: 1.7088609626269586
Experience 3, Iter 77, disc loss: 0.21535445231530623, policy loss: 1.7072650730898555
Experience 3, Iter 78, disc loss: 0.21144637660466556, policy loss: 1.716637735140137
Experience 3, Iter 79, disc loss: 0.21062731460489806, policy loss: 1.7205255214562314
Experience 3, Iter 80, disc loss: 0.2134384974196209, policy loss: 1.7065707517568436
Experience 3, Iter 81, disc loss: 0.20368063316226356, policy loss: 1.7516001709819895
Experience 3, Iter 82, disc loss: 0.20553364979408412, policy loss: 1.744768491382214
Experience 3, Iter 83, disc loss: 0.20854618627842791, policy loss: 1.7256007082929665
Experience 3, Iter 84, disc loss: 0.20079017214759165, policy loss: 1.7639090768018952
Experience 3, Iter 85, disc loss: 0.2054158568646508, policy loss: 1.7442986379384067
Experience 3, Iter 86, disc loss: 0.20371028958672538, policy loss: 1.7467335409008606
Experience 3, Iter 87, disc loss: 0.195661801050007, policy loss: 1.791864880681985
Experience 3, Iter 88, disc loss: 0.20036752378944295, policy loss: 1.7671862579350806
Experience 3, Iter 89, disc loss: 0.1909195832153946, policy loss: 1.8110736205161877
Experience 3, Iter 90, disc loss: 0.19695363112884803, policy loss: 1.7822498088445982
Experience 3, Iter 91, disc loss: 0.18805583937577505, policy loss: 1.8378984193817254
Experience 3, Iter 92, disc loss: 0.1905482049783395, policy loss: 1.8107604051979171
Experience 3, Iter 93, disc loss: 0.19232773577577916, policy loss: 1.8061679751009443
Experience 3, Iter 94, disc loss: 0.18863572242789778, policy loss: 1.8383052466026637
Experience 3, Iter 95, disc loss: 0.1860974011156597, policy loss: 1.8361079933905764
Experience 3, Iter 96, disc loss: 0.1880181219813919, policy loss: 1.8259141736224351
Experience 3, Iter 97, disc loss: 0.18561899496680478, policy loss: 1.840143687644054
Experience 3, Iter 98, disc loss: 0.1864212051795517, policy loss: 1.8283057221346932
Experience 3, Iter 99, disc loss: 0.1920121752651088, policy loss: 1.7971263111842646
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.0339],
        [0.3463],
        [0.0067]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0224, 0.1609, 0.2581, 0.0127, 0.0054, 1.3467]],

        [[0.0224, 0.1609, 0.2581, 0.0127, 0.0054, 1.3467]],

        [[0.0224, 0.1609, 0.2581, 0.0127, 0.0054, 1.3467]],

        [[0.0224, 0.1609, 0.2581, 0.0127, 0.0054, 1.3467]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0162, 0.1355, 1.3852, 0.0267], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0162, 0.1355, 1.3852, 0.0267])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.057
Iter 2/2000 - Loss: 1.051
Iter 3/2000 - Loss: 0.938
Iter 4/2000 - Loss: 0.891
Iter 5/2000 - Loss: 0.927
Iter 6/2000 - Loss: 0.864
Iter 7/2000 - Loss: 0.793
Iter 8/2000 - Loss: 0.777
Iter 9/2000 - Loss: 0.752
Iter 10/2000 - Loss: 0.674
Iter 11/2000 - Loss: 0.579
Iter 12/2000 - Loss: 0.499
Iter 13/2000 - Loss: 0.423
Iter 14/2000 - Loss: 0.324
Iter 15/2000 - Loss: 0.197
Iter 16/2000 - Loss: 0.053
Iter 17/2000 - Loss: -0.093
Iter 18/2000 - Loss: -0.243
Iter 19/2000 - Loss: -0.404
Iter 20/2000 - Loss: -0.579
Iter 1981/2000 - Loss: -6.423
Iter 1982/2000 - Loss: -6.423
Iter 1983/2000 - Loss: -6.423
Iter 1984/2000 - Loss: -6.423
Iter 1985/2000 - Loss: -6.423
Iter 1986/2000 - Loss: -6.423
Iter 1987/2000 - Loss: -6.423
Iter 1988/2000 - Loss: -6.423
Iter 1989/2000 - Loss: -6.423
Iter 1990/2000 - Loss: -6.423
Iter 1991/2000 - Loss: -6.424
Iter 1992/2000 - Loss: -6.424
Iter 1993/2000 - Loss: -6.424
Iter 1994/2000 - Loss: -6.424
Iter 1995/2000 - Loss: -6.424
Iter 1996/2000 - Loss: -6.424
Iter 1997/2000 - Loss: -6.424
Iter 1998/2000 - Loss: -6.424
Iter 1999/2000 - Loss: -6.424
Iter 2000/2000 - Loss: -6.424
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[13.7906,  6.4677, 42.6338, 15.8653, 19.4636, 51.2818]],

        [[22.9422, 34.4355, 17.3791,  2.7355,  1.5150, 19.2952]],

        [[21.7791, 40.2285, 15.5302,  0.7821,  2.5711, 15.6679]],

        [[19.3424, 40.1404, 13.1122,  2.3801,  2.0238, 27.8322]]])
Signal Variance: tensor([0.1011, 1.9548, 9.9766, 0.3721])
Estimated target variance: tensor([0.0162, 0.1355, 1.3852, 0.0267])
N: 40
Signal to noise ratio: tensor([15.2477, 70.9377, 68.1811, 32.6175])
Bound on condition number: tensor([  9300.6974, 201287.3993, 185947.4701,  42557.0957])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.17701358658041524, policy loss: 1.8785899296587867
Experience 4, Iter 1, disc loss: 0.17804583485960723, policy loss: 1.876174569949792
Experience 4, Iter 2, disc loss: 0.16454788158528752, policy loss: 2.0004778200809517
Experience 4, Iter 3, disc loss: 0.17794103532484015, policy loss: 1.8735387743682688
Experience 4, Iter 4, disc loss: 0.17371155930982862, policy loss: 1.899806389610711
Experience 4, Iter 5, disc loss: 0.17248721883176052, policy loss: 1.9034649875593772
Experience 4, Iter 6, disc loss: 0.15765376862751493, policy loss: 1.995250961146112
Experience 4, Iter 7, disc loss: 0.1598770813241967, policy loss: 1.9833079491888361
Experience 4, Iter 8, disc loss: 0.16034089884953223, policy loss: 1.9778171132747766
Experience 4, Iter 9, disc loss: 0.16283202252286363, policy loss: 1.9663432545704114
Experience 4, Iter 10, disc loss: 0.16561303585585288, policy loss: 1.9411508662185657
Experience 4, Iter 11, disc loss: 0.1634842332794783, policy loss: 1.95511870145274
Experience 4, Iter 12, disc loss: 0.1575168131164408, policy loss: 2.013126712389656
Experience 4, Iter 13, disc loss: 0.16227999903193355, policy loss: 1.9820521324219693
Experience 4, Iter 14, disc loss: 0.15650603600366644, policy loss: 2.0054761274902035
Experience 4, Iter 15, disc loss: 0.15426784876804797, policy loss: 2.008418789537829
Experience 4, Iter 16, disc loss: 0.15669940946603234, policy loss: 1.9896336926179772
Experience 4, Iter 17, disc loss: 0.15225619389328532, policy loss: 2.0588307526202962
Experience 4, Iter 18, disc loss: 0.15299613482625488, policy loss: 2.01949399592481
Experience 4, Iter 19, disc loss: 0.15226890751522737, policy loss: 2.024891389520533
Experience 4, Iter 20, disc loss: 0.15135282947579834, policy loss: 2.033093920409198
Experience 4, Iter 21, disc loss: 0.15106833900631966, policy loss: 2.033203717078818
Experience 4, Iter 22, disc loss: 0.14307406043859883, policy loss: 2.0975295137611987
Experience 4, Iter 23, disc loss: 0.15039896697281313, policy loss: 2.0379299978189684
Experience 4, Iter 24, disc loss: 0.1400099236830649, policy loss: 2.1268887595246926
Experience 4, Iter 25, disc loss: 0.1436184126122441, policy loss: 2.0903009113539284
Experience 4, Iter 26, disc loss: 0.1462897918889632, policy loss: 2.0572365418739924
Experience 4, Iter 27, disc loss: 0.14341259275172738, policy loss: 2.0775589562337817
Experience 4, Iter 28, disc loss: 0.13470960515725067, policy loss: 2.1742270422559864
Experience 4, Iter 29, disc loss: 0.1412354952450563, policy loss: 2.1175956533342903
Experience 4, Iter 30, disc loss: 0.13644323957676505, policy loss: 2.1411840795109294
Experience 4, Iter 31, disc loss: 0.14272183991490445, policy loss: 2.079986667805791
Experience 4, Iter 32, disc loss: 0.1404325959142518, policy loss: 2.0957557310243398
Experience 4, Iter 33, disc loss: 0.1323701115481987, policy loss: 2.198100341282279
Experience 4, Iter 34, disc loss: 0.1389524191692932, policy loss: 2.112933655209546
Experience 4, Iter 35, disc loss: 0.1326700197724346, policy loss: 2.152187841425232
Experience 4, Iter 36, disc loss: 0.14034833059242635, policy loss: 2.1012583284034223
Experience 4, Iter 37, disc loss: 0.13473716430636515, policy loss: 2.1497905222463123
Experience 4, Iter 38, disc loss: 0.13202681217666512, policy loss: 2.1652963422165534
Experience 4, Iter 39, disc loss: 0.13030571046571643, policy loss: 2.189407818770351
Experience 4, Iter 40, disc loss: 0.12711435753880454, policy loss: 2.2206430515684676
Experience 4, Iter 41, disc loss: 0.12199264737669721, policy loss: 2.269235970673284
Experience 4, Iter 42, disc loss: 0.1280085481096128, policy loss: 2.191850741438342
Experience 4, Iter 43, disc loss: 0.13055182268833007, policy loss: 2.1712181349517947
Experience 4, Iter 44, disc loss: 0.12727338460569648, policy loss: 2.19043442507122
Experience 4, Iter 45, disc loss: 0.12157575580315677, policy loss: 2.243356704163901
Experience 4, Iter 46, disc loss: 0.12198196591322724, policy loss: 2.2416488401893666
Experience 4, Iter 47, disc loss: 0.12133515771930223, policy loss: 2.241408352791115
Experience 4, Iter 48, disc loss: 0.12296610544897103, policy loss: 2.2270918583065478
Experience 4, Iter 49, disc loss: 0.12230627080252818, policy loss: 2.2345495926080896
Experience 4, Iter 50, disc loss: 0.11804179619767871, policy loss: 2.2648568906543605
Experience 4, Iter 51, disc loss: 0.11747051477592697, policy loss: 2.3069866386324396
Experience 4, Iter 52, disc loss: 0.11778456081704172, policy loss: 2.283435682337729
Experience 4, Iter 53, disc loss: 0.11662160256041247, policy loss: 2.2858338832593392
Experience 4, Iter 54, disc loss: 0.1139329434429533, policy loss: 2.3110246681961635
Experience 4, Iter 55, disc loss: 0.11536581261592138, policy loss: 2.3014147689232844
Experience 4, Iter 56, disc loss: 0.11015655973671828, policy loss: 2.353688498688256
Experience 4, Iter 57, disc loss: 0.11156801120382215, policy loss: 2.343144947214289
Experience 4, Iter 58, disc loss: 0.11272536150917244, policy loss: 2.32434073504818
Experience 4, Iter 59, disc loss: 0.108093026669535, policy loss: 2.377544263910821
Experience 4, Iter 60, disc loss: 0.11508912105636768, policy loss: 2.3021509296618063
Experience 4, Iter 61, disc loss: 0.11110201813816818, policy loss: 2.349830896282362
Experience 4, Iter 62, disc loss: 0.10947238601595903, policy loss: 2.353603055937503
Experience 4, Iter 63, disc loss: 0.10999075388588736, policy loss: 2.3492999049604326
Experience 4, Iter 64, disc loss: 0.11177238989722947, policy loss: 2.336716666696508
Experience 4, Iter 65, disc loss: 0.11254044297013967, policy loss: 2.319133194330611
Experience 4, Iter 66, disc loss: 0.10672009462025708, policy loss: 2.3785171188140373
Experience 4, Iter 67, disc loss: 0.10738692274805985, policy loss: 2.375105544499829
Experience 4, Iter 68, disc loss: 0.10300538464362269, policy loss: 2.423241578467243
Experience 4, Iter 69, disc loss: 0.10519372180067035, policy loss: 2.4030888271285615
Experience 4, Iter 70, disc loss: 0.10548994575835575, policy loss: 2.4031042497018635
Experience 4, Iter 71, disc loss: 0.1052838019861026, policy loss: 2.3942200503202624
Experience 4, Iter 72, disc loss: 0.10248538230754806, policy loss: 2.434134209942146
Experience 4, Iter 73, disc loss: 0.10748504464165703, policy loss: 2.378153396557466
Experience 4, Iter 74, disc loss: 0.10379665859530689, policy loss: 2.414307013921581
Experience 4, Iter 75, disc loss: 0.09751820545412015, policy loss: 2.4852614656732817
Experience 4, Iter 76, disc loss: 0.10071460065246898, policy loss: 2.4470396690233907
Experience 4, Iter 77, disc loss: 0.1011257946403804, policy loss: 2.435773556167652
Experience 4, Iter 78, disc loss: 0.09438723476558819, policy loss: 2.509551182545578
Experience 4, Iter 79, disc loss: 0.10317227450154329, policy loss: 2.419180839572961
Experience 4, Iter 80, disc loss: 0.10085551256672512, policy loss: 2.4394243943916925
Experience 4, Iter 81, disc loss: 0.0975268362849908, policy loss: 2.477154676753802
Experience 4, Iter 82, disc loss: 0.09882300567927688, policy loss: 2.4658810974071934
Experience 4, Iter 83, disc loss: 0.09587806071821839, policy loss: 2.5031979816584844
Experience 4, Iter 84, disc loss: 0.1013636676910388, policy loss: 2.438681838167378
Experience 4, Iter 85, disc loss: 0.09801627179875634, policy loss: 2.479953922146871
Experience 4, Iter 86, disc loss: 0.09552681988179436, policy loss: 2.503050717345508
Experience 4, Iter 87, disc loss: 0.0950496335814259, policy loss: 2.513694444536127
Experience 4, Iter 88, disc loss: 0.10033152704515767, policy loss: 2.445278061323787
Experience 4, Iter 89, disc loss: 0.10019686145519605, policy loss: 2.4485181916263556
Experience 4, Iter 90, disc loss: 0.09643669238125609, policy loss: 2.5005618404114225
Experience 4, Iter 91, disc loss: 0.09731989048417913, policy loss: 2.484328755862318
Experience 4, Iter 92, disc loss: 0.09305129485393819, policy loss: 2.5403501461693807
Experience 4, Iter 93, disc loss: 0.09551947208559487, policy loss: 2.510600800201037
Experience 4, Iter 94, disc loss: 0.09169419736436503, policy loss: 2.568636716360262
Experience 4, Iter 95, disc loss: 0.09234567090126253, policy loss: 2.551263777964776
Experience 4, Iter 96, disc loss: 0.08714907918953421, policy loss: 2.624089393485922
Experience 4, Iter 97, disc loss: 0.08758723908667886, policy loss: 2.6245009557769468
Experience 4, Iter 98, disc loss: 0.08825932984669055, policy loss: 2.6269327004017677
Experience 4, Iter 99, disc loss: 0.08903710260886154, policy loss: 2.601423218350016
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0290],
        [0.3112],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0182, 0.1338, 0.2452, 0.0113, 0.0044, 1.1442]],

        [[0.0182, 0.1338, 0.2452, 0.0113, 0.0044, 1.1442]],

        [[0.0182, 0.1338, 0.2452, 0.0113, 0.0044, 1.1442]],

        [[0.0182, 0.1338, 0.2452, 0.0113, 0.0044, 1.1442]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0135, 0.1158, 1.2446, 0.0248], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0135, 0.1158, 1.2446, 0.0248])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.804
Iter 2/2000 - Loss: 0.881
Iter 3/2000 - Loss: 0.710
Iter 4/2000 - Loss: 0.689
Iter 5/2000 - Loss: 0.744
Iter 6/2000 - Loss: 0.675
Iter 7/2000 - Loss: 0.606
Iter 8/2000 - Loss: 0.610
Iter 9/2000 - Loss: 0.613
Iter 10/2000 - Loss: 0.555
Iter 11/2000 - Loss: 0.476
Iter 12/2000 - Loss: 0.424
Iter 13/2000 - Loss: 0.388
Iter 14/2000 - Loss: 0.326
Iter 15/2000 - Loss: 0.228
Iter 16/2000 - Loss: 0.112
Iter 17/2000 - Loss: -0.001
Iter 18/2000 - Loss: -0.115
Iter 19/2000 - Loss: -0.244
Iter 20/2000 - Loss: -0.397
Iter 1981/2000 - Loss: -6.834
Iter 1982/2000 - Loss: -6.834
Iter 1983/2000 - Loss: -6.834
Iter 1984/2000 - Loss: -6.834
Iter 1985/2000 - Loss: -6.834
Iter 1986/2000 - Loss: -6.834
Iter 1987/2000 - Loss: -6.834
Iter 1988/2000 - Loss: -6.834
Iter 1989/2000 - Loss: -6.834
Iter 1990/2000 - Loss: -6.834
Iter 1991/2000 - Loss: -6.834
Iter 1992/2000 - Loss: -6.834
Iter 1993/2000 - Loss: -6.834
Iter 1994/2000 - Loss: -6.834
Iter 1995/2000 - Loss: -6.834
Iter 1996/2000 - Loss: -6.834
Iter 1997/2000 - Loss: -6.834
Iter 1998/2000 - Loss: -6.834
Iter 1999/2000 - Loss: -6.834
Iter 2000/2000 - Loss: -6.834
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.6026,  6.4077, 39.8161, 13.5812, 15.7540, 46.1859]],

        [[23.4467, 37.7663, 19.4375,  3.1195,  1.5532, 19.7974]],

        [[22.2847, 36.1102, 15.4488,  1.1979,  1.4399, 17.6094]],

        [[19.8146, 37.0196, 15.8941,  3.1930,  2.2807, 32.0067]]])
Signal Variance: tensor([ 0.0985,  2.0980, 14.3636,  0.5140])
Estimated target variance: tensor([0.0135, 0.1158, 1.2446, 0.0248])
N: 50
Signal to noise ratio: tensor([14.7969, 68.0433, 82.5387, 40.9087])
Bound on condition number: tensor([ 10948.4724, 231495.7025, 340632.7602,  83676.9770])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.09398486800461177, policy loss: 2.5396186143841453
Experience 5, Iter 1, disc loss: 0.09590509940567332, policy loss: 2.5132494975040336
Experience 5, Iter 2, disc loss: 0.09365684203750221, policy loss: 2.544756119063842
Experience 5, Iter 3, disc loss: 0.09713528463362481, policy loss: 2.4955649500754564
Experience 5, Iter 4, disc loss: 0.09461077528710769, policy loss: 2.5277510590255594
Experience 5, Iter 5, disc loss: 0.09374843866434741, policy loss: 2.551642871340074
Experience 5, Iter 6, disc loss: 0.09431706268528052, policy loss: 2.5418051037151166
Experience 5, Iter 7, disc loss: 0.08870377038408989, policy loss: 2.6255782040538445
Experience 5, Iter 8, disc loss: 0.08842624744544228, policy loss: 2.642284465231068
Experience 5, Iter 9, disc loss: 0.09067942075624466, policy loss: 2.6067690050867234
Experience 5, Iter 10, disc loss: 0.08655597121672752, policy loss: 2.652849794770705
Experience 5, Iter 11, disc loss: 0.09059619758664599, policy loss: 2.6131016832295035
Experience 5, Iter 12, disc loss: 0.09013991919604555, policy loss: 2.622554628124354
Experience 5, Iter 13, disc loss: 0.08928766438108665, policy loss: 2.6185215465730485
Experience 5, Iter 14, disc loss: 0.08726184727564089, policy loss: 2.6711485639027766
Experience 5, Iter 15, disc loss: 0.08893633370337191, policy loss: 2.640621917881733
Experience 5, Iter 16, disc loss: 0.08753567974060482, policy loss: 2.6655799002472644
Experience 5, Iter 17, disc loss: 0.08711117613724947, policy loss: 2.6662966088547524
Experience 5, Iter 18, disc loss: 0.08910962017709717, policy loss: 2.649244566109225
Experience 5, Iter 19, disc loss: 0.08788060252227266, policy loss: 2.6704261944172645
Experience 5, Iter 20, disc loss: 0.08427963975911357, policy loss: 2.744963961545216
Experience 5, Iter 21, disc loss: 0.08373569801896372, policy loss: 2.7273639176586597
Experience 5, Iter 22, disc loss: 0.08181618420710855, policy loss: 2.7777799104748775
Experience 5, Iter 23, disc loss: 0.08251384489912138, policy loss: 2.7589271723086153
Experience 5, Iter 24, disc loss: 0.08324303552660252, policy loss: 2.7427684041189773
Experience 5, Iter 25, disc loss: 0.0835991023253827, policy loss: 2.7648888525534825
Experience 5, Iter 26, disc loss: 0.08447235110636941, policy loss: 2.746976011225314
Experience 5, Iter 27, disc loss: 0.08463572337952047, policy loss: 2.7224380014365415
Experience 5, Iter 28, disc loss: 0.08046897508729468, policy loss: 2.8186854595386577
Experience 5, Iter 29, disc loss: 0.08386548498735619, policy loss: 2.739315513160328
Experience 5, Iter 30, disc loss: 0.08043326561855371, policy loss: 2.802169793271176
Experience 5, Iter 31, disc loss: 0.07884662525798754, policy loss: 2.8572269407693596
Experience 5, Iter 32, disc loss: 0.07905849032526947, policy loss: 2.8538435906571658
Experience 5, Iter 33, disc loss: 0.0797493128100143, policy loss: 2.832728790918436
Experience 5, Iter 34, disc loss: 0.07668801567000709, policy loss: 2.892467358406348
Experience 5, Iter 35, disc loss: 0.07352193618009234, policy loss: 2.9451254582132993
Experience 5, Iter 36, disc loss: 0.08207642989410244, policy loss: 2.7834577296911513
Experience 5, Iter 37, disc loss: 0.07264613404992953, policy loss: 2.979766004532993
Experience 5, Iter 38, disc loss: 0.07907080564800231, policy loss: 2.8431463344483436
Experience 5, Iter 39, disc loss: 0.07615183900295375, policy loss: 2.8853638508153434
Experience 5, Iter 40, disc loss: 0.07397821303083646, policy loss: 2.950294099844868
Experience 5, Iter 41, disc loss: 0.07598238880679259, policy loss: 2.9116762198825636
Experience 5, Iter 42, disc loss: 0.07508769130581136, policy loss: 2.9118970557487707
Experience 5, Iter 43, disc loss: 0.07510148297690065, policy loss: 2.9430096304568454
Experience 5, Iter 44, disc loss: 0.07438209403693338, policy loss: 2.921047036559842
Experience 5, Iter 45, disc loss: 0.0761754180160464, policy loss: 2.9011417220793336
Experience 5, Iter 46, disc loss: 0.07543601725493725, policy loss: 2.88719387984725
Experience 5, Iter 47, disc loss: 0.07481823340532143, policy loss: 2.9197116491696566
Experience 5, Iter 48, disc loss: 0.07639246410280535, policy loss: 2.8842506103096577
Experience 5, Iter 49, disc loss: 0.07532666425008173, policy loss: 2.898169727326418
Experience 5, Iter 50, disc loss: 0.07354520378289799, policy loss: 2.918862658166953
Experience 5, Iter 51, disc loss: 0.07082945422343286, policy loss: 3.009617133582946
Experience 5, Iter 52, disc loss: 0.06770938603334013, policy loss: 3.043978009268142
Experience 5, Iter 53, disc loss: 0.06875840131179174, policy loss: 3.0143304862275375
Experience 5, Iter 54, disc loss: 0.06864962330794594, policy loss: 3.0210649846098816
Experience 5, Iter 55, disc loss: 0.07340155575992799, policy loss: 2.9555884167395488
Experience 5, Iter 56, disc loss: 0.0730750978252915, policy loss: 2.948468826476458
Experience 5, Iter 57, disc loss: 0.06714593554929976, policy loss: 3.077791743624899
Experience 5, Iter 58, disc loss: 0.06881708719938323, policy loss: 3.0144925929335256
Experience 5, Iter 59, disc loss: 0.06881448221285397, policy loss: 3.0706050751054534
Experience 5, Iter 60, disc loss: 0.07016509714331914, policy loss: 3.0057757943169325
Experience 5, Iter 61, disc loss: 0.07048179051901708, policy loss: 3.005324479103032
Experience 5, Iter 62, disc loss: 0.06970478506618566, policy loss: 3.007894960526682
Experience 5, Iter 63, disc loss: 0.06649914832836133, policy loss: 3.0878089842599725
Experience 5, Iter 64, disc loss: 0.06712622858145875, policy loss: 3.108304133365648
Experience 5, Iter 65, disc loss: 0.06346885352012956, policy loss: 3.1874760615088773
Experience 5, Iter 66, disc loss: 0.06791879300935652, policy loss: 3.0477395026982936
Experience 5, Iter 67, disc loss: 0.06580103095988969, policy loss: 3.1126175303696026
Experience 5, Iter 68, disc loss: 0.0680373341763954, policy loss: 3.0841536840776804
Experience 5, Iter 69, disc loss: 0.06431422603003364, policy loss: 3.1343937718067494
Experience 5, Iter 70, disc loss: 0.06904940051803694, policy loss: 3.0459245970530566
Experience 5, Iter 71, disc loss: 0.06305229541367152, policy loss: 3.1954777063734627
Experience 5, Iter 72, disc loss: 0.06407939816853474, policy loss: 3.1251108044677958
Experience 5, Iter 73, disc loss: 0.062246098998154184, policy loss: 3.163575974908895
Experience 5, Iter 74, disc loss: 0.06249179777568927, policy loss: 3.177499256314757
Experience 5, Iter 75, disc loss: 0.06176137608013002, policy loss: 3.2238582335833526
Experience 5, Iter 76, disc loss: 0.06294787513717726, policy loss: 3.151029646591502
Experience 5, Iter 77, disc loss: 0.059456615420017005, policy loss: 3.262114409152526
Experience 5, Iter 78, disc loss: 0.05938749795820186, policy loss: 3.2766864368222937
Experience 5, Iter 79, disc loss: 0.05674879329719036, policy loss: 3.3152132222029635
Experience 5, Iter 80, disc loss: 0.061009773057196674, policy loss: 3.2314095865654364
Experience 5, Iter 81, disc loss: 0.058809176250499154, policy loss: 3.2690446426474447
Experience 5, Iter 82, disc loss: 0.059072163594891966, policy loss: 3.252981282669993
Experience 5, Iter 83, disc loss: 0.05916947274741959, policy loss: 3.2214800017831537
Experience 5, Iter 84, disc loss: 0.06136662771633882, policy loss: 3.194258814955906
Experience 5, Iter 85, disc loss: 0.06104816313079253, policy loss: 3.2112519803478192
Experience 5, Iter 86, disc loss: 0.05787915823080852, policy loss: 3.2533183378262205
Experience 5, Iter 87, disc loss: 0.0577301492532992, policy loss: 3.290542906749676
Experience 5, Iter 88, disc loss: 0.057278627639827495, policy loss: 3.2845225180592448
Experience 5, Iter 89, disc loss: 0.05601255260457176, policy loss: 3.3059180581881797
Experience 5, Iter 90, disc loss: 0.05694381851630399, policy loss: 3.3037681811363657
Experience 5, Iter 91, disc loss: 0.05695880462232501, policy loss: 3.313928912845726
Experience 5, Iter 92, disc loss: 0.060609665236013875, policy loss: 3.2020656282100424
Experience 5, Iter 93, disc loss: 0.057782080016461186, policy loss: 3.2733661451458067
Experience 5, Iter 94, disc loss: 0.055685847839025845, policy loss: 3.3849485183418073
Experience 5, Iter 95, disc loss: 0.056684487796548284, policy loss: 3.3010620723545747
Experience 5, Iter 96, disc loss: 0.054368690909374455, policy loss: 3.4001287031368577
Experience 5, Iter 97, disc loss: 0.061230409023280585, policy loss: 3.2014064447101864
Experience 5, Iter 98, disc loss: 0.054537030132241524, policy loss: 3.367207271223027
Experience 5, Iter 99, disc loss: 0.05491419788063448, policy loss: 3.330477510140695
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.0363],
        [0.3601],
        [0.0065]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0157, 0.1203, 0.2698, 0.0114, 0.0038, 1.2866]],

        [[0.0157, 0.1203, 0.2698, 0.0114, 0.0038, 1.2866]],

        [[0.0157, 0.1203, 0.2698, 0.0114, 0.0038, 1.2866]],

        [[0.0157, 0.1203, 0.2698, 0.0114, 0.0038, 1.2866]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0121, 0.1452, 1.4403, 0.0260], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0121, 0.1452, 1.4403, 0.0260])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.959
Iter 2/2000 - Loss: 1.100
Iter 3/2000 - Loss: 0.876
Iter 4/2000 - Loss: 0.876
Iter 5/2000 - Loss: 0.952
Iter 6/2000 - Loss: 0.882
Iter 7/2000 - Loss: 0.791
Iter 8/2000 - Loss: 0.782
Iter 9/2000 - Loss: 0.807
Iter 10/2000 - Loss: 0.779
Iter 11/2000 - Loss: 0.701
Iter 12/2000 - Loss: 0.631
Iter 13/2000 - Loss: 0.594
Iter 14/2000 - Loss: 0.560
Iter 15/2000 - Loss: 0.493
Iter 16/2000 - Loss: 0.389
Iter 17/2000 - Loss: 0.275
Iter 18/2000 - Loss: 0.164
Iter 19/2000 - Loss: 0.051
Iter 20/2000 - Loss: -0.080
Iter 1981/2000 - Loss: -6.974
Iter 1982/2000 - Loss: -6.974
Iter 1983/2000 - Loss: -6.974
Iter 1984/2000 - Loss: -6.974
Iter 1985/2000 - Loss: -6.974
Iter 1986/2000 - Loss: -6.974
Iter 1987/2000 - Loss: -6.975
Iter 1988/2000 - Loss: -6.975
Iter 1989/2000 - Loss: -6.975
Iter 1990/2000 - Loss: -6.975
Iter 1991/2000 - Loss: -6.975
Iter 1992/2000 - Loss: -6.975
Iter 1993/2000 - Loss: -6.975
Iter 1994/2000 - Loss: -6.975
Iter 1995/2000 - Loss: -6.975
Iter 1996/2000 - Loss: -6.975
Iter 1997/2000 - Loss: -6.975
Iter 1998/2000 - Loss: -6.975
Iter 1999/2000 - Loss: -6.975
Iter 2000/2000 - Loss: -6.975
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.6095,  6.4774, 42.3145, 12.9125, 15.6621, 40.2834]],

        [[24.3302, 41.8344, 20.8174,  5.5147,  1.7626, 34.4478]],

        [[22.4556, 42.7649, 18.2382,  0.9795,  1.7858, 23.9018]],

        [[20.5027, 33.1636, 16.4304,  3.9395,  1.9886, 32.1157]]])
Signal Variance: tensor([ 0.1007,  5.0262, 19.1461,  0.4897])
Estimated target variance: tensor([0.0121, 0.1452, 1.4403, 0.0260])
N: 60
Signal to noise ratio: tensor([ 15.3810, 105.0013,  93.2605,  39.6549])
Bound on condition number: tensor([ 14195.5659, 661517.0003, 521852.0560,  94351.8572])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.057950047444510086, policy loss: 3.291541627822514
Experience 6, Iter 1, disc loss: 0.06131283173504848, policy loss: 3.2489963843550322
Experience 6, Iter 2, disc loss: 0.05894271496889823, policy loss: 3.312079037408994
Experience 6, Iter 3, disc loss: 0.059496063928806006, policy loss: 3.3353714099213194
Experience 6, Iter 4, disc loss: 0.05492359599544665, policy loss: 3.3667273901852264
Experience 6, Iter 5, disc loss: 0.060487108670490224, policy loss: 3.273087335697388
Experience 6, Iter 6, disc loss: 0.05612456247302837, policy loss: 3.4124625286456767
Experience 6, Iter 7, disc loss: 0.062217219976592325, policy loss: 3.246707781778326
Experience 6, Iter 8, disc loss: 0.06246016912518986, policy loss: 3.2177602944633947
Experience 6, Iter 9, disc loss: 0.06141572187278917, policy loss: 3.309479858094256
Experience 6, Iter 10, disc loss: 0.059741925390300044, policy loss: 3.305209921572804
Experience 6, Iter 11, disc loss: 0.05797432322289939, policy loss: 3.3527129414261254
Experience 6, Iter 12, disc loss: 0.05768569304816448, policy loss: 3.4125029472980497
Experience 6, Iter 13, disc loss: 0.05754961630427347, policy loss: 3.3590811917579333
Experience 6, Iter 14, disc loss: 0.05954721694537831, policy loss: 3.3677594108504336
Experience 6, Iter 15, disc loss: 0.056057911606562635, policy loss: 3.4196789427831202
Experience 6, Iter 16, disc loss: 0.058762975051299376, policy loss: 3.373052917104382
Experience 6, Iter 17, disc loss: 0.05681159108816809, policy loss: 3.4659496140189856
Experience 6, Iter 18, disc loss: 0.05428556236925342, policy loss: 3.492648979153743
Experience 6, Iter 19, disc loss: 0.05693074283555821, policy loss: 3.4173432148150367
Experience 6, Iter 20, disc loss: 0.05870174863495713, policy loss: 3.384896469821194
Experience 6, Iter 21, disc loss: 0.05667374794970846, policy loss: 3.4685770251126216
Experience 6, Iter 22, disc loss: 0.05664945209192977, policy loss: 3.4383170060896293
Experience 6, Iter 23, disc loss: 0.059762751869979286, policy loss: 3.3341987811541074
Experience 6, Iter 24, disc loss: 0.055163502029938, policy loss: 3.533128717587623
Experience 6, Iter 25, disc loss: 0.05141026209510442, policy loss: 3.6361993328966498
Experience 6, Iter 26, disc loss: 0.05671233022132324, policy loss: 3.4105597976532245
Experience 6, Iter 27, disc loss: 0.053677315685570576, policy loss: 3.540954607306193
Experience 6, Iter 28, disc loss: 0.0552163777578879, policy loss: 3.454704288046748
Experience 6, Iter 29, disc loss: 0.05559497700711319, policy loss: 3.484902601969195
Experience 6, Iter 30, disc loss: 0.055648142579713636, policy loss: 3.4537483463228327
Experience 6, Iter 31, disc loss: 0.05302299911071199, policy loss: 3.5289607399888974
Experience 6, Iter 32, disc loss: 0.05259844693463016, policy loss: 3.6004118801068783
Experience 6, Iter 33, disc loss: 0.05158750823730161, policy loss: 3.647434948650674
Experience 6, Iter 34, disc loss: 0.05162276229474631, policy loss: 3.6140418930194906
Experience 6, Iter 35, disc loss: 0.05114716953061146, policy loss: 3.6377635471333662
Experience 6, Iter 36, disc loss: 0.055146751258605325, policy loss: 3.4535467736155483
Experience 6, Iter 37, disc loss: 0.052379438064669576, policy loss: 3.536063758706577
Experience 6, Iter 38, disc loss: 0.049438202889257515, policy loss: 3.607280207570033
Experience 6, Iter 39, disc loss: 0.05188654873162013, policy loss: 3.5394029406001435
Experience 6, Iter 40, disc loss: 0.05194048909583317, policy loss: 3.5638665547910944
Experience 6, Iter 41, disc loss: 0.05175445062940304, policy loss: 3.540325312781474
Experience 6, Iter 42, disc loss: 0.05001797212162627, policy loss: 3.59438636469597
Experience 6, Iter 43, disc loss: 0.05005531083587334, policy loss: 3.6111892167525372
Experience 6, Iter 44, disc loss: 0.048328456695500344, policy loss: 3.720691847415147
Experience 6, Iter 45, disc loss: 0.04679636328359683, policy loss: 3.7260131165554986
Experience 6, Iter 46, disc loss: 0.045524976791012736, policy loss: 3.8612183643780673
Experience 6, Iter 47, disc loss: 0.04442866934248262, policy loss: 3.8048748644693875
Experience 6, Iter 48, disc loss: 0.05073056196939576, policy loss: 3.5667827475664016
Experience 6, Iter 49, disc loss: 0.040508320675233925, policy loss: 4.001041770383825
Experience 6, Iter 50, disc loss: 0.04557915926374223, policy loss: 3.70505620042369
Experience 6, Iter 51, disc loss: 0.035370513926032514, policy loss: 4.185634364900929
Experience 6, Iter 52, disc loss: 0.03632695428679057, policy loss: 4.099931087529662
Experience 6, Iter 53, disc loss: 0.04037591201490365, policy loss: 3.94608903316987
Experience 6, Iter 54, disc loss: 0.04488785668690288, policy loss: 3.7654189584142177
Experience 6, Iter 55, disc loss: 0.03674769537516138, policy loss: 4.214010291642115
Experience 6, Iter 56, disc loss: 0.045423920891977276, policy loss: 3.7944459798828563
Experience 6, Iter 57, disc loss: 0.04405900313672, policy loss: 3.7807641260426625
Experience 6, Iter 58, disc loss: 0.041316977504667736, policy loss: 3.8833177332473268
Experience 6, Iter 59, disc loss: 0.045602477438761446, policy loss: 3.6900971593641305
Experience 6, Iter 60, disc loss: 0.04721279257419571, policy loss: 3.65266039897447
Experience 6, Iter 61, disc loss: 0.04257420583649796, policy loss: 3.833578730540221
Experience 6, Iter 62, disc loss: 0.04617286917052241, policy loss: 3.7093089011762332
Experience 6, Iter 63, disc loss: 0.04486095267622435, policy loss: 3.7698032629295635
Experience 6, Iter 64, disc loss: 0.04486051050173387, policy loss: 3.7289068059093866
Experience 6, Iter 65, disc loss: 0.04690585608466482, policy loss: 3.687498732300682
Experience 6, Iter 66, disc loss: 0.04872180853061421, policy loss: 3.6063766505282535
Experience 6, Iter 67, disc loss: 0.047592186999512544, policy loss: 3.677596906385558
Experience 6, Iter 68, disc loss: 0.04183306993637316, policy loss: 4.00785301664639
Experience 6, Iter 69, disc loss: 0.04288008953029954, policy loss: 3.8589090397429255
Experience 6, Iter 70, disc loss: 0.043534335878161606, policy loss: 3.853743838697765
Experience 6, Iter 71, disc loss: 0.04314498069424738, policy loss: 3.945103253523969
Experience 6, Iter 72, disc loss: 0.042502359045312035, policy loss: 3.885064929361283
Experience 6, Iter 73, disc loss: 0.04495405109647956, policy loss: 3.8394860396708435
Experience 6, Iter 74, disc loss: 0.04479692667095932, policy loss: 3.7756083338239086
Experience 6, Iter 75, disc loss: 0.04140785324956902, policy loss: 3.943597477185303
Experience 6, Iter 76, disc loss: 0.043681481472934575, policy loss: 3.899395914596398
Experience 6, Iter 77, disc loss: 0.04118115652127628, policy loss: 3.9990593806749266
Experience 6, Iter 78, disc loss: 0.03824333305296378, policy loss: 4.148793315240693
Experience 6, Iter 79, disc loss: 0.039721587964744495, policy loss: 4.024122113914521
Experience 6, Iter 80, disc loss: 0.03913420240624602, policy loss: 4.060753723044174
Experience 6, Iter 81, disc loss: 0.03883386838715029, policy loss: 4.04971058420008
Experience 6, Iter 82, disc loss: 0.04103663517393358, policy loss: 3.9898053748240203
Experience 6, Iter 83, disc loss: 0.03891081742740486, policy loss: 4.169519563703526
Experience 6, Iter 84, disc loss: 0.04069853711337101, policy loss: 3.949012071791934
Experience 6, Iter 85, disc loss: 0.03669433595365288, policy loss: 4.148425938448935
Experience 6, Iter 86, disc loss: 0.03590619277000211, policy loss: 4.121722900934714
Experience 6, Iter 87, disc loss: 0.034569103473672674, policy loss: 4.196011849455559
Experience 6, Iter 88, disc loss: 0.036207821110554296, policy loss: 4.272433782982197
Experience 6, Iter 89, disc loss: 0.034601522958066906, policy loss: 4.2750909660449885
Experience 6, Iter 90, disc loss: 0.03336127380697791, policy loss: 4.3801924678799455
Experience 6, Iter 91, disc loss: 0.036037789956907776, policy loss: 4.096728572140016
Experience 6, Iter 92, disc loss: 0.033615349581191306, policy loss: 4.166248912926487
Experience 6, Iter 93, disc loss: 0.031689399434326, policy loss: 4.253572810563572
Experience 6, Iter 94, disc loss: 0.034610541417982955, policy loss: 4.05644228326493
Experience 6, Iter 95, disc loss: 0.032027765339588676, policy loss: 4.2871859332580184
Experience 6, Iter 96, disc loss: 0.03439122156485436, policy loss: 4.1007730409428245
Experience 6, Iter 97, disc loss: 0.034844377817495276, policy loss: 4.113888825540463
Experience 6, Iter 98, disc loss: 0.03246644468649194, policy loss: 4.167642158250986
Experience 6, Iter 99, disc loss: 0.03266922778648072, policy loss: 4.125409719002543
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0719],
        [0.7140],
        [0.0088]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.1164, 0.4164, 0.0138, 0.0037, 1.9472]],

        [[0.0153, 0.1164, 0.4164, 0.0138, 0.0037, 1.9472]],

        [[0.0153, 0.1164, 0.4164, 0.0138, 0.0037, 1.9472]],

        [[0.0153, 0.1164, 0.4164, 0.0138, 0.0037, 1.9472]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.2876, 2.8559, 0.0352], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.2876, 2.8559, 0.0352])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.752
Iter 2/2000 - Loss: 1.907
Iter 3/2000 - Loss: 1.679
Iter 4/2000 - Loss: 1.680
Iter 5/2000 - Loss: 1.754
Iter 6/2000 - Loss: 1.694
Iter 7/2000 - Loss: 1.595
Iter 8/2000 - Loss: 1.567
Iter 9/2000 - Loss: 1.589
Iter 10/2000 - Loss: 1.573
Iter 11/2000 - Loss: 1.499
Iter 12/2000 - Loss: 1.418
Iter 13/2000 - Loss: 1.365
Iter 14/2000 - Loss: 1.328
Iter 15/2000 - Loss: 1.267
Iter 16/2000 - Loss: 1.167
Iter 17/2000 - Loss: 1.046
Iter 18/2000 - Loss: 0.924
Iter 19/2000 - Loss: 0.805
Iter 20/2000 - Loss: 0.675
Iter 1981/2000 - Loss: -6.733
Iter 1982/2000 - Loss: -6.733
Iter 1983/2000 - Loss: -6.733
Iter 1984/2000 - Loss: -6.733
Iter 1985/2000 - Loss: -6.733
Iter 1986/2000 - Loss: -6.733
Iter 1987/2000 - Loss: -6.733
Iter 1988/2000 - Loss: -6.733
Iter 1989/2000 - Loss: -6.733
Iter 1990/2000 - Loss: -6.734
Iter 1991/2000 - Loss: -6.734
Iter 1992/2000 - Loss: -6.734
Iter 1993/2000 - Loss: -6.734
Iter 1994/2000 - Loss: -6.734
Iter 1995/2000 - Loss: -6.734
Iter 1996/2000 - Loss: -6.734
Iter 1997/2000 - Loss: -6.734
Iter 1998/2000 - Loss: -6.734
Iter 1999/2000 - Loss: -6.734
Iter 2000/2000 - Loss: -6.734
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[17.5789,  7.5202, 46.4262, 10.5090,  8.1132, 47.9904]],

        [[22.4105, 40.0218, 20.6372,  1.7216,  1.1896, 25.5559]],

        [[23.0508, 45.0763, 15.6221,  0.9844,  1.8280, 17.5968]],

        [[22.1307, 39.1900, 16.4436,  3.9672,  0.8429, 53.8513]]])
Signal Variance: tensor([ 0.1189,  2.8167, 17.7944,  0.5048])
Estimated target variance: tensor([0.0114, 0.2876, 2.8559, 0.0352])
N: 70
Signal to noise ratio: tensor([17.4053, 76.9821, 88.4857, 40.0172])
Bound on condition number: tensor([ 21207.1336, 414838.3813, 548080.8645, 112097.4512])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.02563050573358636, policy loss: 4.580428474829454
Experience 7, Iter 1, disc loss: 0.0236175477944019, policy loss: 4.717488890061007
Experience 7, Iter 2, disc loss: 0.025015320061652566, policy loss: 4.501835798858353
Experience 7, Iter 3, disc loss: 0.024909912012163825, policy loss: 4.4995134620353605
Experience 7, Iter 4, disc loss: 0.024182933754610236, policy loss: 4.604819260443566
Experience 7, Iter 5, disc loss: 0.02379294708942842, policy loss: 4.535442240561758
Experience 7, Iter 6, disc loss: 0.023543577895201034, policy loss: 4.541192336763283
Experience 7, Iter 7, disc loss: 0.023284388879000806, policy loss: 4.566812449002617
Experience 7, Iter 8, disc loss: 0.022008257978154165, policy loss: 4.710067961726176
Experience 7, Iter 9, disc loss: 0.02276670226167591, policy loss: 4.541821927353374
Experience 7, Iter 10, disc loss: 0.02144257664652177, policy loss: 4.627895893043079
Experience 7, Iter 11, disc loss: 0.022238413869126696, policy loss: 4.569399610001723
Experience 7, Iter 12, disc loss: 0.021951189310728578, policy loss: 4.6516317837109735
Experience 7, Iter 13, disc loss: 0.020483317296959947, policy loss: 4.899275692583541
Experience 7, Iter 14, disc loss: 0.021670366489380356, policy loss: 4.610127575072968
Experience 7, Iter 15, disc loss: 0.021852450893664464, policy loss: 4.613231832413935
Experience 7, Iter 16, disc loss: 0.02136718061275441, policy loss: 4.664648482284258
Experience 7, Iter 17, disc loss: 0.021529372929629444, policy loss: 4.534636827814932
Experience 7, Iter 18, disc loss: 0.019957758801986684, policy loss: 4.714681351959501
Experience 7, Iter 19, disc loss: 0.02215722165134562, policy loss: 4.499878322731469
Experience 7, Iter 20, disc loss: 0.01900801926610744, policy loss: 4.82167452940119
Experience 7, Iter 21, disc loss: 0.01996456634897306, policy loss: 4.747888485451182
Experience 7, Iter 22, disc loss: 0.01950887838318431, policy loss: 4.767449072413269
Experience 7, Iter 23, disc loss: 0.01938766105794086, policy loss: 4.7496783955913955
Experience 7, Iter 24, disc loss: 0.019442786834417067, policy loss: 4.696625460414573
Experience 7, Iter 25, disc loss: 0.01833821468190721, policy loss: 4.736530232491222
Experience 7, Iter 26, disc loss: 0.018511039413126845, policy loss: 4.731460474604838
Experience 7, Iter 27, disc loss: 0.01681370632692194, policy loss: 4.909029681363556
Experience 7, Iter 28, disc loss: 0.016181722776023824, policy loss: 5.0189323088123
Experience 7, Iter 29, disc loss: 0.016807600521931657, policy loss: 4.978125048530963
Experience 7, Iter 30, disc loss: 0.01577874491930304, policy loss: 5.028089254935464
Experience 7, Iter 31, disc loss: 0.015957837728026638, policy loss: 4.983556162705947
Experience 7, Iter 32, disc loss: 0.016921068206971916, policy loss: 4.838471275998172
Experience 7, Iter 33, disc loss: 0.01611562482875254, policy loss: 4.926191017990234
Experience 7, Iter 34, disc loss: 0.016271876058542408, policy loss: 4.843030627447451
Experience 7, Iter 35, disc loss: 0.015168811521102055, policy loss: 5.166528605940536
Experience 7, Iter 36, disc loss: 0.016221308720976935, policy loss: 4.883031259222902
Experience 7, Iter 37, disc loss: 0.01602090509185901, policy loss: 4.890724420486055
Experience 7, Iter 38, disc loss: 0.014565086063134036, policy loss: 5.103108471578248
Experience 7, Iter 39, disc loss: 0.01483640361362563, policy loss: 4.944701907659461
Experience 7, Iter 40, disc loss: 0.014429230812686907, policy loss: 5.020033094637208
Experience 7, Iter 41, disc loss: 0.014739754987897051, policy loss: 4.944069543929397
Experience 7, Iter 42, disc loss: 0.014460472989693059, policy loss: 4.959640417098296
Experience 7, Iter 43, disc loss: 0.013995377924632096, policy loss: 5.07291202924045
Experience 7, Iter 44, disc loss: 0.014597228149317024, policy loss: 4.914811671191354
Experience 7, Iter 45, disc loss: 0.01362564028558487, policy loss: 5.148652265595209
Experience 7, Iter 46, disc loss: 0.014176740337864464, policy loss: 4.959528131964511
Experience 7, Iter 47, disc loss: 0.013366857088586478, policy loss: 5.109728452520914
Experience 7, Iter 48, disc loss: 0.01356527640737798, policy loss: 5.010490357411163
Experience 7, Iter 49, disc loss: 0.01318600535059292, policy loss: 5.141323387586555
Experience 7, Iter 50, disc loss: 0.013284803950247132, policy loss: 5.091801704124821
Experience 7, Iter 51, disc loss: 0.013518182597778813, policy loss: 4.991295273690193
Experience 7, Iter 52, disc loss: 0.01291773977013061, policy loss: 5.070492511872661
Experience 7, Iter 53, disc loss: 0.013705220664461181, policy loss: 4.944273964252303
Experience 7, Iter 54, disc loss: 0.013207107711326129, policy loss: 4.9959948862635475
Experience 7, Iter 55, disc loss: 0.011873158262952491, policy loss: 5.266019172045631
Experience 7, Iter 56, disc loss: 0.011921206241103745, policy loss: 5.251785117673544
Experience 7, Iter 57, disc loss: 0.012387125061668738, policy loss: 5.058022697901955
Experience 7, Iter 58, disc loss: 0.01202586381850972, policy loss: 5.1456063260039215
Experience 7, Iter 59, disc loss: 0.012134820691717611, policy loss: 5.139173929619021
Experience 7, Iter 60, disc loss: 0.012062355451734204, policy loss: 5.1539699100991525
Experience 7, Iter 61, disc loss: 0.011892623935084777, policy loss: 5.114195627642773
Experience 7, Iter 62, disc loss: 0.011267729327242695, policy loss: 5.275628498479041
Experience 7, Iter 63, disc loss: 0.011881653540725321, policy loss: 5.124483253077893
Experience 7, Iter 64, disc loss: 0.011372587731062294, policy loss: 5.254135025885859
Experience 7, Iter 65, disc loss: 0.01184801805376063, policy loss: 5.119087323258677
Experience 7, Iter 66, disc loss: 0.011236761347662765, policy loss: 5.243225227704785
Experience 7, Iter 67, disc loss: 0.010955806392784056, policy loss: 5.22254055702189
Experience 7, Iter 68, disc loss: 0.011137726574165199, policy loss: 5.21197471862521
Experience 7, Iter 69, disc loss: 0.010927807882232361, policy loss: 5.274474711931019
Experience 7, Iter 70, disc loss: 0.010942346070167773, policy loss: 5.231131801103475
Experience 7, Iter 71, disc loss: 0.010863496613395902, policy loss: 5.246724051693186
Experience 7, Iter 72, disc loss: 0.011003073546574487, policy loss: 5.198381530034909
Experience 7, Iter 73, disc loss: 0.010444203689584874, policy loss: 5.331288652542802
Experience 7, Iter 74, disc loss: 0.010488203985527229, policy loss: 5.249110278280215
Experience 7, Iter 75, disc loss: 0.010240701960327874, policy loss: 5.286632873584835
Experience 7, Iter 76, disc loss: 0.010534631553798881, policy loss: 5.303253819425004
Experience 7, Iter 77, disc loss: 0.01052529289354974, policy loss: 5.1899126487143254
Experience 7, Iter 78, disc loss: 0.010085745198635268, policy loss: 5.3115997436356865
Experience 7, Iter 79, disc loss: 0.009626788309637686, policy loss: 5.401540226833273
Experience 7, Iter 80, disc loss: 0.010581869793751454, policy loss: 5.175140939065242
Experience 7, Iter 81, disc loss: 0.009628855729617986, policy loss: 5.342033629548286
Experience 7, Iter 82, disc loss: 0.009845523856604015, policy loss: 5.316029674743991
Experience 7, Iter 83, disc loss: 0.009487544185019075, policy loss: 5.456145717206964
Experience 7, Iter 84, disc loss: 0.009840632906546184, policy loss: 5.3017883013750104
Experience 7, Iter 85, disc loss: 0.009662319099389406, policy loss: 5.391710805758245
Experience 7, Iter 86, disc loss: 0.009178769568521135, policy loss: 5.413036403778685
Experience 7, Iter 87, disc loss: 0.009893202574802914, policy loss: 5.271332361868826
Experience 7, Iter 88, disc loss: 0.00953247095309928, policy loss: 5.329998951395301
Experience 7, Iter 89, disc loss: 0.009354352093278964, policy loss: 5.419281550378217
Experience 7, Iter 90, disc loss: 0.008729213454065073, policy loss: 5.501335295628829
Experience 7, Iter 91, disc loss: 0.009063971174680002, policy loss: 5.414831708139204
Experience 7, Iter 92, disc loss: 0.00879069151691073, policy loss: 5.483440590415099
Experience 7, Iter 93, disc loss: 0.009304379183896441, policy loss: 5.360292068637392
Experience 7, Iter 94, disc loss: 0.008789828099556498, policy loss: 5.443633750787255
Experience 7, Iter 95, disc loss: 0.00880561041010115, policy loss: 5.430433589585542
Experience 7, Iter 96, disc loss: 0.009493207614678121, policy loss: 5.275153608224464
Experience 7, Iter 97, disc loss: 0.009659492153934173, policy loss: 5.22478194375466
Experience 7, Iter 98, disc loss: 0.00932543850650962, policy loss: 5.30475090453445
Experience 7, Iter 99, disc loss: 0.009335070366999827, policy loss: 5.284043971964975
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0935],
        [0.9024],
        [0.0098]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0138, 0.1124, 0.4827, 0.0152, 0.0034, 2.4169]],

        [[0.0138, 0.1124, 0.4827, 0.0152, 0.0034, 2.4169]],

        [[0.0138, 0.1124, 0.4827, 0.0152, 0.0034, 2.4169]],

        [[0.0138, 0.1124, 0.4827, 0.0152, 0.0034, 2.4169]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.3741, 3.6095, 0.0392], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.3741, 3.6095, 0.0392])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.023
Iter 2/2000 - Loss: 2.218
Iter 3/2000 - Loss: 1.959
Iter 4/2000 - Loss: 1.969
Iter 5/2000 - Loss: 2.049
Iter 6/2000 - Loss: 1.993
Iter 7/2000 - Loss: 1.885
Iter 8/2000 - Loss: 1.839
Iter 9/2000 - Loss: 1.856
Iter 10/2000 - Loss: 1.855
Iter 11/2000 - Loss: 1.792
Iter 12/2000 - Loss: 1.701
Iter 13/2000 - Loss: 1.629
Iter 14/2000 - Loss: 1.585
Iter 15/2000 - Loss: 1.537
Iter 16/2000 - Loss: 1.454
Iter 17/2000 - Loss: 1.334
Iter 18/2000 - Loss: 1.201
Iter 19/2000 - Loss: 1.069
Iter 20/2000 - Loss: 0.931
Iter 1981/2000 - Loss: -6.827
Iter 1982/2000 - Loss: -6.827
Iter 1983/2000 - Loss: -6.827
Iter 1984/2000 - Loss: -6.827
Iter 1985/2000 - Loss: -6.827
Iter 1986/2000 - Loss: -6.827
Iter 1987/2000 - Loss: -6.827
Iter 1988/2000 - Loss: -6.827
Iter 1989/2000 - Loss: -6.827
Iter 1990/2000 - Loss: -6.827
Iter 1991/2000 - Loss: -6.827
Iter 1992/2000 - Loss: -6.827
Iter 1993/2000 - Loss: -6.827
Iter 1994/2000 - Loss: -6.828
Iter 1995/2000 - Loss: -6.828
Iter 1996/2000 - Loss: -6.828
Iter 1997/2000 - Loss: -6.828
Iter 1998/2000 - Loss: -6.828
Iter 1999/2000 - Loss: -6.828
Iter 2000/2000 - Loss: -6.828
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[17.1783,  7.6487, 45.7138, 11.4178,  8.2558, 48.4551]],

        [[21.3317, 36.7156, 13.1292,  1.5835,  2.0626, 27.5995]],

        [[22.1632, 38.5498, 15.8499,  0.9427,  1.4982, 13.5636]],

        [[20.2971, 37.9399, 15.5222,  3.8163,  1.0866, 47.8396]]])
Signal Variance: tensor([ 0.1171,  2.5633, 13.4722,  0.4438])
Estimated target variance: tensor([0.0108, 0.3741, 3.6095, 0.0392])
N: 80
Signal to noise ratio: tensor([18.1470, 67.0958, 78.3632, 36.1177])
Bound on condition number: tensor([ 26346.0591, 360148.5594, 491264.6257, 104360.2767])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.010016623278438358, policy loss: 5.230638592826218
Experience 8, Iter 1, disc loss: 0.009623583488450481, policy loss: 5.1905905639110985
Experience 8, Iter 2, disc loss: 0.008794757480020997, policy loss: 5.435500288844938
Experience 8, Iter 3, disc loss: 0.009396112820293057, policy loss: 5.2265360346351635
Experience 8, Iter 4, disc loss: 0.009763648028493223, policy loss: 5.18458758661734
Experience 8, Iter 5, disc loss: 0.009606422794585491, policy loss: 5.2054056337682955
Experience 8, Iter 6, disc loss: 0.009976824971751964, policy loss: 5.11398027857039
Experience 8, Iter 7, disc loss: 0.009221634447473804, policy loss: 5.254358832188858
Experience 8, Iter 8, disc loss: 0.00909172060550769, policy loss: 5.301784297726666
Experience 8, Iter 9, disc loss: 0.009762166448594022, policy loss: 5.140364318142909
Experience 8, Iter 10, disc loss: 0.009121971182956469, policy loss: 5.303686447669731
Experience 8, Iter 11, disc loss: 0.008929843456979227, policy loss: 5.27075583884894
Experience 8, Iter 12, disc loss: 0.00943181685123171, policy loss: 5.1554719306157795
Experience 8, Iter 13, disc loss: 0.009496565435743043, policy loss: 5.16835059385806
Experience 8, Iter 14, disc loss: 0.00948703171771214, policy loss: 5.212761277519016
Experience 8, Iter 15, disc loss: 0.008928495336407582, policy loss: 5.310053475397764
Experience 8, Iter 16, disc loss: 0.009726383212645025, policy loss: 5.201704041961517
Experience 8, Iter 17, disc loss: 0.009732597553284195, policy loss: 5.138137529727129
Experience 8, Iter 18, disc loss: 0.009382140589134664, policy loss: 5.204866763864024
Experience 8, Iter 19, disc loss: 0.008868800556784741, policy loss: 5.298713747414935
Experience 8, Iter 20, disc loss: 0.009116835104438843, policy loss: 5.282467726012898
Experience 8, Iter 21, disc loss: 0.009014115361091216, policy loss: 5.22727929334191
Experience 8, Iter 22, disc loss: 0.00900009209589079, policy loss: 5.311798131374832
Experience 8, Iter 23, disc loss: 0.00922026029124164, policy loss: 5.228387515758235
Experience 8, Iter 24, disc loss: 0.009024462368988826, policy loss: 5.307622948901511
Experience 8, Iter 25, disc loss: 0.008858583537478076, policy loss: 5.2977242046855775
Experience 8, Iter 26, disc loss: 0.008846940010748606, policy loss: 5.339186597689132
Experience 8, Iter 27, disc loss: 0.007813620876085626, policy loss: 5.617425726789882
Experience 8, Iter 28, disc loss: 0.008109534775108087, policy loss: 5.4330573636472685
Experience 8, Iter 29, disc loss: 0.00841922459361262, policy loss: 5.427028195894485
Experience 8, Iter 30, disc loss: 0.008676678212940853, policy loss: 5.305429100276459
Experience 8, Iter 31, disc loss: 0.008361827572951229, policy loss: 5.425763881201698
Experience 8, Iter 32, disc loss: 0.007714777417867408, policy loss: 5.595302459571736
Experience 8, Iter 33, disc loss: 0.00842132461087151, policy loss: 5.371763416836544
Experience 8, Iter 34, disc loss: 0.008290125914667587, policy loss: 5.444607381162827
Experience 8, Iter 35, disc loss: 0.0085445748603397, policy loss: 5.3689404412649395
Experience 8, Iter 36, disc loss: 0.007863792804369284, policy loss: 5.547192309679013
Experience 8, Iter 37, disc loss: 0.008491012409482097, policy loss: 5.387594594897465
Experience 8, Iter 38, disc loss: 0.00773745353621079, policy loss: 5.55270914575404
Experience 8, Iter 39, disc loss: 0.007774319898098083, policy loss: 5.559478591941353
Experience 8, Iter 40, disc loss: 0.0076565934262201, policy loss: 5.608161196636544
Experience 8, Iter 41, disc loss: 0.007887904561020575, policy loss: 5.514741166775333
Experience 8, Iter 42, disc loss: 0.008535626072593153, policy loss: 5.311637808063061
Experience 8, Iter 43, disc loss: 0.007441136291220229, policy loss: 5.63206638376601
Experience 8, Iter 44, disc loss: 0.007238327249689733, policy loss: 5.675800904840159
Experience 8, Iter 45, disc loss: 0.00799886493869985, policy loss: 5.4467164322949415
Experience 8, Iter 46, disc loss: 0.007492688376062615, policy loss: 5.58416750458193
Experience 8, Iter 47, disc loss: 0.007345220576395037, policy loss: 5.571435888934723
Experience 8, Iter 48, disc loss: 0.007563545917181761, policy loss: 5.564009764857856
Experience 8, Iter 49, disc loss: 0.007956449752038372, policy loss: 5.3714705262625415
Experience 8, Iter 50, disc loss: 0.007726608289968014, policy loss: 5.4524930570547365
Experience 8, Iter 51, disc loss: 0.0076773453043883395, policy loss: 5.5396138455649
Experience 8, Iter 52, disc loss: 0.007696725830084445, policy loss: 5.452842897016598
Experience 8, Iter 53, disc loss: 0.0074166338309801, policy loss: 5.525491080422765
Experience 8, Iter 54, disc loss: 0.007683920485673963, policy loss: 5.435321466968265
Experience 8, Iter 55, disc loss: 0.007638819901235706, policy loss: 5.496314331402296
Experience 8, Iter 56, disc loss: 0.007233668180017527, policy loss: 5.628414672076872
Experience 8, Iter 57, disc loss: 0.007388639794062073, policy loss: 5.623524487716187
Experience 8, Iter 58, disc loss: 0.00762958158089008, policy loss: 5.509045512317817
Experience 8, Iter 59, disc loss: 0.007346601768698529, policy loss: 5.518191545042058
Experience 8, Iter 60, disc loss: 0.0063104350907321315, policy loss: 5.855943636581123
Experience 8, Iter 61, disc loss: 0.007055375535225113, policy loss: 5.687835204064278
Experience 8, Iter 62, disc loss: 0.006057668347895527, policy loss: 5.955472162161879
Experience 8, Iter 63, disc loss: 0.005950731627555251, policy loss: 5.89686696577208
Experience 8, Iter 64, disc loss: 0.007151490290159669, policy loss: 5.567869772142836
Experience 8, Iter 65, disc loss: 0.006612521041171687, policy loss: 5.676667683491305
Experience 8, Iter 66, disc loss: 0.006509533036814027, policy loss: 5.674554813104734
Experience 8, Iter 67, disc loss: 0.006836082460727305, policy loss: 5.652229599175898
Experience 8, Iter 68, disc loss: 0.006921166221752688, policy loss: 5.612188536222103
Experience 8, Iter 69, disc loss: 0.00649158873005813, policy loss: 5.764751100873988
Experience 8, Iter 70, disc loss: 0.0068378671217267095, policy loss: 5.592034892834436
Experience 8, Iter 71, disc loss: 0.006445675185747122, policy loss: 5.794593502762153
Experience 8, Iter 72, disc loss: 0.006643749158435536, policy loss: 5.651356897658841
Experience 8, Iter 73, disc loss: 0.00650999039610375, policy loss: 5.659086248512441
Experience 8, Iter 74, disc loss: 0.006429469663380128, policy loss: 5.710687896336735
Experience 8, Iter 75, disc loss: 0.007059463956182055, policy loss: 5.479787402740733
Experience 8, Iter 76, disc loss: 0.006980095226333695, policy loss: 5.528222654352293
Experience 8, Iter 77, disc loss: 0.006384490012200549, policy loss: 5.757665648996216
Experience 8, Iter 78, disc loss: 0.006604511932005829, policy loss: 5.627827456954137
Experience 8, Iter 79, disc loss: 0.006596366761082571, policy loss: 5.598194964402577
Experience 8, Iter 80, disc loss: 0.0063547383944335175, policy loss: 5.668372580584993
Experience 8, Iter 81, disc loss: 0.0066760335856597495, policy loss: 5.654845557461207
Experience 8, Iter 82, disc loss: 0.006486779904121813, policy loss: 5.666325037521324
Experience 8, Iter 83, disc loss: 0.006661342955927487, policy loss: 5.601742301889034
Experience 8, Iter 84, disc loss: 0.00662013040695969, policy loss: 5.609273432189802
Experience 8, Iter 85, disc loss: 0.0063046728972654615, policy loss: 5.7281635762411565
Experience 8, Iter 86, disc loss: 0.006701249010510613, policy loss: 5.528284964525854
Experience 8, Iter 87, disc loss: 0.006339430887008696, policy loss: 5.682695555430126
Experience 8, Iter 88, disc loss: 0.0063006909367222036, policy loss: 5.6741974029127915
Experience 8, Iter 89, disc loss: 0.006511999989512795, policy loss: 5.611527548806099
Experience 8, Iter 90, disc loss: 0.0061045734951297856, policy loss: 5.7505271372592475
Experience 8, Iter 91, disc loss: 0.0058611472174530665, policy loss: 5.8234801616152545
Experience 8, Iter 92, disc loss: 0.006032098705726377, policy loss: 5.777519901991019
Experience 8, Iter 93, disc loss: 0.006411238885835887, policy loss: 5.655774486463223
Experience 8, Iter 94, disc loss: 0.005788862332202091, policy loss: 5.840865347372683
Experience 8, Iter 95, disc loss: 0.005780582458066911, policy loss: 5.76171894234184
Experience 8, Iter 96, disc loss: 0.0059438744794315535, policy loss: 5.743601415681534
Experience 8, Iter 97, disc loss: 0.005645156691786071, policy loss: 5.848256722732792
Experience 8, Iter 98, disc loss: 0.005732673189839714, policy loss: 5.787905395750435
Experience 8, Iter 99, disc loss: 0.005830312281730888, policy loss: 5.8500043537914745
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1115],
        [1.0684],
        [0.0109]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.1101, 0.5518, 0.0162, 0.0032, 2.8135]],

        [[0.0128, 0.1101, 0.5518, 0.0162, 0.0032, 2.8135]],

        [[0.0128, 0.1101, 0.5518, 0.0162, 0.0032, 2.8135]],

        [[0.0128, 0.1101, 0.5518, 0.0162, 0.0032, 2.8135]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0106, 0.4462, 4.2735, 0.0435], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0106, 0.4462, 4.2735, 0.0435])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.234
Iter 2/2000 - Loss: 2.468
Iter 3/2000 - Loss: 2.178
Iter 4/2000 - Loss: 2.197
Iter 5/2000 - Loss: 2.287
Iter 6/2000 - Loss: 2.234
Iter 7/2000 - Loss: 2.120
Iter 8/2000 - Loss: 2.058
Iter 9/2000 - Loss: 2.065
Iter 10/2000 - Loss: 2.072
Iter 11/2000 - Loss: 2.021
Iter 12/2000 - Loss: 1.925
Iter 13/2000 - Loss: 1.830
Iter 14/2000 - Loss: 1.761
Iter 15/2000 - Loss: 1.703
Iter 16/2000 - Loss: 1.620
Iter 17/2000 - Loss: 1.493
Iter 18/2000 - Loss: 1.333
Iter 19/2000 - Loss: 1.163
Iter 20/2000 - Loss: 0.991
Iter 1981/2000 - Loss: -6.963
Iter 1982/2000 - Loss: -6.963
Iter 1983/2000 - Loss: -6.964
Iter 1984/2000 - Loss: -6.964
Iter 1985/2000 - Loss: -6.964
Iter 1986/2000 - Loss: -6.964
Iter 1987/2000 - Loss: -6.964
Iter 1988/2000 - Loss: -6.964
Iter 1989/2000 - Loss: -6.964
Iter 1990/2000 - Loss: -6.964
Iter 1991/2000 - Loss: -6.964
Iter 1992/2000 - Loss: -6.964
Iter 1993/2000 - Loss: -6.964
Iter 1994/2000 - Loss: -6.964
Iter 1995/2000 - Loss: -6.964
Iter 1996/2000 - Loss: -6.964
Iter 1997/2000 - Loss: -6.964
Iter 1998/2000 - Loss: -6.964
Iter 1999/2000 - Loss: -6.964
Iter 2000/2000 - Loss: -6.964
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[16.2492,  7.2354, 42.6629, 11.8364, 12.5214, 46.9216]],

        [[20.4573, 36.1034, 10.8704,  1.4737,  4.4066, 31.8407]],

        [[22.1713, 39.3789, 11.3335,  1.1116,  2.0906, 16.1429]],

        [[19.0144, 34.7359, 12.3460,  3.3339,  1.0132, 42.1896]]])
Signal Variance: tensor([ 0.1036,  2.8740, 16.2243,  0.3372])
Estimated target variance: tensor([0.0106, 0.4462, 4.2735, 0.0435])
N: 90
Signal to noise ratio: tensor([16.4382, 74.0610, 86.3688, 32.4915])
Bound on condition number: tensor([ 24320.2557, 493654.4382, 671361.5774,  95013.9775])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.005792911206093599, policy loss: 5.786305785984391
Experience 9, Iter 1, disc loss: 0.0062655768701347005, policy loss: 5.694616889190229
Experience 9, Iter 2, disc loss: 0.006420106506940225, policy loss: 5.628585123729742
Experience 9, Iter 3, disc loss: 0.005957526286732523, policy loss: 5.824747390897101
Experience 9, Iter 4, disc loss: 0.006466496843189416, policy loss: 5.599788233159309
Experience 9, Iter 5, disc loss: 0.006347596363168315, policy loss: 5.688453634939094
Experience 9, Iter 6, disc loss: 0.00585242607529249, policy loss: 5.796306903515401
Experience 9, Iter 7, disc loss: 0.005822238949815061, policy loss: 5.738880457137559
Experience 9, Iter 8, disc loss: 0.006573331846843886, policy loss: 5.663003233074831
Experience 9, Iter 9, disc loss: 0.00589459850627593, policy loss: 5.870363365277056
Experience 9, Iter 10, disc loss: 0.0048858372823420634, policy loss: 6.269490460644339
Experience 9, Iter 11, disc loss: 0.006133125357947741, policy loss: 5.730253340610194
Experience 9, Iter 12, disc loss: 0.0049524411934050916, policy loss: 6.061216282667726
Experience 9, Iter 13, disc loss: 0.004302331024097254, policy loss: 6.285710614208507
Experience 9, Iter 14, disc loss: 0.005615558357934142, policy loss: 5.853127682349349
Experience 9, Iter 15, disc loss: 0.005948568798454768, policy loss: 5.732222622965361
Experience 9, Iter 16, disc loss: 0.004726654984070397, policy loss: 6.311600542525718
Experience 9, Iter 17, disc loss: 0.004147627969791787, policy loss: 6.676589151475632
Experience 9, Iter 18, disc loss: 0.005863069620364425, policy loss: 5.800654152491025
Experience 9, Iter 19, disc loss: 0.00694121746635153, policy loss: 5.496965579749929
Experience 9, Iter 20, disc loss: 0.009343015343633463, policy loss: 4.976122717955843
Experience 9, Iter 21, disc loss: 0.010738188351091492, policy loss: 4.994745958989469
Experience 9, Iter 22, disc loss: 0.011590281809255441, policy loss: 5.12424741223038
Experience 9, Iter 23, disc loss: 0.010247275908285223, policy loss: 5.292388820117646
Experience 9, Iter 24, disc loss: 0.01122345996839999, policy loss: 4.9792084812353465
Experience 9, Iter 25, disc loss: 0.010186871668186577, policy loss: 4.932998776783464
Experience 9, Iter 26, disc loss: 0.007323359305182553, policy loss: 5.506552607451807
Experience 9, Iter 27, disc loss: 0.0069326509086316945, policy loss: 5.511423837793877
Experience 9, Iter 28, disc loss: 0.0049619684753851746, policy loss: 5.944105041137668
Experience 9, Iter 29, disc loss: 0.004027218472138865, policy loss: 6.314568115737515
Experience 9, Iter 30, disc loss: 0.004484796441849029, policy loss: 6.057373423260986
Experience 9, Iter 31, disc loss: 0.005981823154657509, policy loss: 5.614124075117824
Experience 9, Iter 32, disc loss: 0.006890151868426555, policy loss: 5.553563029727096
Experience 9, Iter 33, disc loss: 0.008504947169394012, policy loss: 5.370012375446361
Experience 9, Iter 34, disc loss: 0.008722243416982581, policy loss: 5.392637104163958
Experience 9, Iter 35, disc loss: 0.0064130291583017005, policy loss: 6.225739364836805
Experience 9, Iter 36, disc loss: 0.004696070420669861, policy loss: 6.6271226188726375
Experience 9, Iter 37, disc loss: 0.005007611383458397, policy loss: 6.634150757142943
Experience 9, Iter 38, disc loss: 0.007334890528203024, policy loss: 6.042599864892371
Experience 9, Iter 39, disc loss: 0.012163547597021658, policy loss: 4.869481846271523
Experience 9, Iter 40, disc loss: 0.01265333833422642, policy loss: 4.656827336106874
Experience 9, Iter 41, disc loss: 0.011012004481115771, policy loss: 4.88994652862483
Experience 9, Iter 42, disc loss: 0.009060533761459117, policy loss: 5.27457480188907
Experience 9, Iter 43, disc loss: 0.008631204894463392, policy loss: 5.374369908384745
Experience 9, Iter 44, disc loss: 0.007030389458775971, policy loss: 5.7104052781928125
Experience 9, Iter 45, disc loss: 0.00458472131051944, policy loss: 6.351029345342392
Experience 9, Iter 46, disc loss: 0.004844804212067481, policy loss: 6.196196167372103
Experience 9, Iter 47, disc loss: 0.006066723926591673, policy loss: 5.732508706082849
Experience 9, Iter 48, disc loss: 0.006032197694392637, policy loss: 5.6478808084254695
Experience 9, Iter 49, disc loss: 0.006668005942461735, policy loss: 5.364930940108093
Experience 9, Iter 50, disc loss: 0.005995693932592966, policy loss: 5.561573421260665
Experience 9, Iter 51, disc loss: 0.007716661365023122, policy loss: 5.167489829517543
Experience 9, Iter 52, disc loss: 0.009982258235848104, policy loss: 4.786104058648918
Experience 9, Iter 53, disc loss: 0.010922534906395567, policy loss: 4.722181402870966
Experience 9, Iter 54, disc loss: 0.01243502794646819, policy loss: 4.614053172347594
Experience 9, Iter 55, disc loss: 0.009843843081599041, policy loss: 5.056585281847943
Experience 9, Iter 56, disc loss: 0.008610760944501122, policy loss: 5.333505241313074
Experience 9, Iter 57, disc loss: 0.0081198819830023, policy loss: 5.566482276980105
Experience 9, Iter 58, disc loss: 0.0070212819043594926, policy loss: 5.75889193959039
Experience 9, Iter 59, disc loss: 0.007297607245797107, policy loss: 5.524683413582021
Experience 9, Iter 60, disc loss: 0.005775019278522027, policy loss: 5.9146818957575675
Experience 9, Iter 61, disc loss: 0.007119001607866554, policy loss: 5.620367512726599
Experience 9, Iter 62, disc loss: 0.005603269531450844, policy loss: 5.917266711792894
Experience 9, Iter 63, disc loss: 0.006729465837081692, policy loss: 5.642226530554381
Experience 9, Iter 64, disc loss: 0.007247622918057172, policy loss: 5.582346503099157
Experience 9, Iter 65, disc loss: 0.008313983283163514, policy loss: 5.274780392717346
Experience 9, Iter 66, disc loss: 0.00796819548979232, policy loss: 5.275622581478371
Experience 9, Iter 67, disc loss: 0.009413888108811755, policy loss: 4.969765670107806
Experience 9, Iter 68, disc loss: 0.008217962304211276, policy loss: 5.124515149989701
Experience 9, Iter 69, disc loss: 0.010188400153714318, policy loss: 4.842373108838428
Experience 9, Iter 70, disc loss: 0.009247001191563648, policy loss: 4.956033744909383
Experience 9, Iter 71, disc loss: 0.009657603814422563, policy loss: 4.848034331810776
Experience 9, Iter 72, disc loss: 0.00989218480703288, policy loss: 4.823508330940343
Experience 9, Iter 73, disc loss: 0.008866734017554915, policy loss: 4.990965132356572
Experience 9, Iter 74, disc loss: 0.008850087109891155, policy loss: 4.992393665692379
Experience 9, Iter 75, disc loss: 0.009622383720480139, policy loss: 4.85637072232025
Experience 9, Iter 76, disc loss: 0.008836002911848571, policy loss: 4.985238016046376
Experience 9, Iter 77, disc loss: 0.009161725830156855, policy loss: 4.888904852993616
Experience 9, Iter 78, disc loss: 0.008402002753492883, policy loss: 5.034524272000631
Experience 9, Iter 79, disc loss: 0.008832283343734504, policy loss: 4.95191371074174
Experience 9, Iter 80, disc loss: 0.008166713434196386, policy loss: 5.015596526347585
Experience 9, Iter 81, disc loss: 0.008709742488772617, policy loss: 4.956442944485794
Experience 9, Iter 82, disc loss: 0.008562472074052615, policy loss: 4.957389080050444
Experience 9, Iter 83, disc loss: 0.007944218754427466, policy loss: 5.0396377049946395
Experience 9, Iter 84, disc loss: 0.008027863150762617, policy loss: 5.079829570704041
Experience 9, Iter 85, disc loss: 0.008859118777622276, policy loss: 4.904948995802406
Experience 9, Iter 86, disc loss: 0.008062976543975819, policy loss: 5.021650182301674
Experience 9, Iter 87, disc loss: 0.007515582841062767, policy loss: 5.10161660482218
Experience 9, Iter 88, disc loss: 0.008657853778352115, policy loss: 4.9112376445691295
Experience 9, Iter 89, disc loss: 0.00787632921044612, policy loss: 5.027508546960933
Experience 9, Iter 90, disc loss: 0.007073175450764447, policy loss: 5.232547867376297
Experience 9, Iter 91, disc loss: 0.007765973487590354, policy loss: 5.080528142555922
Experience 9, Iter 92, disc loss: 0.00733259305384002, policy loss: 5.144418289665733
Experience 9, Iter 93, disc loss: 0.0069492664423763065, policy loss: 5.202002624620065
Experience 9, Iter 94, disc loss: 0.007501029530236132, policy loss: 5.097632981807097
Experience 9, Iter 95, disc loss: 0.007400237195117759, policy loss: 5.095570590401611
Experience 9, Iter 96, disc loss: 0.0067052318762607765, policy loss: 5.207774850174795
Experience 9, Iter 97, disc loss: 0.007111132212180665, policy loss: 5.154730866855015
Experience 9, Iter 98, disc loss: 0.007595171090530641, policy loss: 5.071948494625116
Experience 9, Iter 99, disc loss: 0.007214991989692612, policy loss: 5.155648596776169
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1006],
        [0.9641],
        [0.0098]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0119, 0.1016, 0.4985, 0.0146, 0.0029, 2.5375]],

        [[0.0119, 0.1016, 0.4985, 0.0146, 0.0029, 2.5375]],

        [[0.0119, 0.1016, 0.4985, 0.0146, 0.0029, 2.5375]],

        [[0.0119, 0.1016, 0.4985, 0.0146, 0.0029, 2.5375]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0098, 0.4025, 3.8565, 0.0393], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0098, 0.4025, 3.8565, 0.0393])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.050
Iter 2/2000 - Loss: 2.290
Iter 3/2000 - Loss: 1.992
Iter 4/2000 - Loss: 2.011
Iter 5/2000 - Loss: 2.109
Iter 6/2000 - Loss: 2.052
Iter 7/2000 - Loss: 1.936
Iter 8/2000 - Loss: 1.883
Iter 9/2000 - Loss: 1.902
Iter 10/2000 - Loss: 1.909
Iter 11/2000 - Loss: 1.852
Iter 12/2000 - Loss: 1.755
Iter 13/2000 - Loss: 1.667
Iter 14/2000 - Loss: 1.606
Iter 15/2000 - Loss: 1.550
Iter 16/2000 - Loss: 1.463
Iter 17/2000 - Loss: 1.333
Iter 18/2000 - Loss: 1.176
Iter 19/2000 - Loss: 1.011
Iter 20/2000 - Loss: 0.844
Iter 1981/2000 - Loss: -7.128
Iter 1982/2000 - Loss: -7.128
Iter 1983/2000 - Loss: -7.128
Iter 1984/2000 - Loss: -7.128
Iter 1985/2000 - Loss: -7.128
Iter 1986/2000 - Loss: -7.128
Iter 1987/2000 - Loss: -7.128
Iter 1988/2000 - Loss: -7.128
Iter 1989/2000 - Loss: -7.128
Iter 1990/2000 - Loss: -7.128
Iter 1991/2000 - Loss: -7.128
Iter 1992/2000 - Loss: -7.128
Iter 1993/2000 - Loss: -7.128
Iter 1994/2000 - Loss: -7.128
Iter 1995/2000 - Loss: -7.128
Iter 1996/2000 - Loss: -7.128
Iter 1997/2000 - Loss: -7.128
Iter 1998/2000 - Loss: -7.128
Iter 1999/2000 - Loss: -7.128
Iter 2000/2000 - Loss: -7.128
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[15.7081,  7.1409, 40.7605, 11.6991, 12.1551, 48.2631]],

        [[19.7834, 34.6029, 10.9188,  1.4877,  4.6273, 32.8689]],

        [[21.1942, 36.9942, 11.6104,  1.1202,  2.0611, 16.5313]],

        [[18.3031, 33.0003, 11.9527,  3.2447,  1.0844, 39.8116]]])
Signal Variance: tensor([ 0.1016,  3.0034, 16.4815,  0.3236])
Estimated target variance: tensor([0.0098, 0.4025, 3.8565, 0.0393])
N: 100
Signal to noise ratio: tensor([15.7932, 76.6748, 84.1493, 31.7551])
Bound on condition number: tensor([ 24943.6694, 587903.8500, 708112.2215, 100839.6547])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0071262023776514595, policy loss: 5.174000549868099
Experience 10, Iter 1, disc loss: 0.006470013635773274, policy loss: 5.281288739146316
Experience 10, Iter 2, disc loss: 0.006676295454346522, policy loss: 5.229292258919048
Experience 10, Iter 3, disc loss: 0.00629526669818262, policy loss: 5.306130601137703
Experience 10, Iter 4, disc loss: 0.006965905767253995, policy loss: 5.191174701371146
Experience 10, Iter 5, disc loss: 0.0068314329651508795, policy loss: 5.198943973752671
Experience 10, Iter 6, disc loss: 0.006516007366048048, policy loss: 5.300754264697728
Experience 10, Iter 7, disc loss: 0.0066137223095158685, policy loss: 5.2658159300271175
Experience 10, Iter 8, disc loss: 0.006301805062932957, policy loss: 5.279605196982387
Experience 10, Iter 9, disc loss: 0.006560071085469182, policy loss: 5.270812232278301
Experience 10, Iter 10, disc loss: 0.006213893648398897, policy loss: 5.330715509474514
Experience 10, Iter 11, disc loss: 0.006568255274986995, policy loss: 5.255377117967237
Experience 10, Iter 12, disc loss: 0.006656011540482773, policy loss: 5.222804265647891
Experience 10, Iter 13, disc loss: 0.006375835316887956, policy loss: 5.3035251250481
Experience 10, Iter 14, disc loss: 0.006149255422697255, policy loss: 5.354319832781167
Experience 10, Iter 15, disc loss: 0.006436222147530602, policy loss: 5.265373411480124
Experience 10, Iter 16, disc loss: 0.0061860423716472174, policy loss: 5.347521068619718
Experience 10, Iter 17, disc loss: 0.0062092668226523965, policy loss: 5.350033244842811
Experience 10, Iter 18, disc loss: 0.005787134999894872, policy loss: 5.432677994691857
Experience 10, Iter 19, disc loss: 0.006508726400831929, policy loss: 5.258788162695223
Experience 10, Iter 20, disc loss: 0.006535397786370447, policy loss: 5.2487701109487865
Experience 10, Iter 21, disc loss: 0.006205335805465399, policy loss: 5.295337442162802
Experience 10, Iter 22, disc loss: 0.005928046672821862, policy loss: 5.409521641645138
Experience 10, Iter 23, disc loss: 0.00598844218479512, policy loss: 5.377383562293637
Experience 10, Iter 24, disc loss: 0.005744554156047428, policy loss: 5.40616979053744
Experience 10, Iter 25, disc loss: 0.00579779992917919, policy loss: 5.361179602595671
Experience 10, Iter 26, disc loss: 0.00580942977695997, policy loss: 5.392686108191432
Experience 10, Iter 27, disc loss: 0.005322689553645812, policy loss: 5.482250371888824
Experience 10, Iter 28, disc loss: 0.005555299760885623, policy loss: 5.441492898602571
Experience 10, Iter 29, disc loss: 0.005714567104636755, policy loss: 5.415256486550486
Experience 10, Iter 30, disc loss: 0.005468361142474444, policy loss: 5.453240398350447
Experience 10, Iter 31, disc loss: 0.005440795232148937, policy loss: 5.496455754969865
Experience 10, Iter 32, disc loss: 0.0052984662860133015, policy loss: 5.535804738929573
Experience 10, Iter 33, disc loss: 0.0055627274832601645, policy loss: 5.4132181884205055
Experience 10, Iter 34, disc loss: 0.0050451524051530585, policy loss: 5.568092070018151
Experience 10, Iter 35, disc loss: 0.005023051899989733, policy loss: 5.557953092290011
Experience 10, Iter 36, disc loss: 0.004764664751937299, policy loss: 5.613193578396748
Experience 10, Iter 37, disc loss: 0.004996220823847359, policy loss: 5.558534740737919
Experience 10, Iter 38, disc loss: 0.0049947855005979275, policy loss: 5.56649136923968
Experience 10, Iter 39, disc loss: 0.005129813296024449, policy loss: 5.516378137569938
Experience 10, Iter 40, disc loss: 0.005078718517911516, policy loss: 5.532593799426666
Experience 10, Iter 41, disc loss: 0.005057473238032542, policy loss: 5.567867412740625
Experience 10, Iter 42, disc loss: 0.004647365099463754, policy loss: 5.649640592590562
Experience 10, Iter 43, disc loss: 0.004935912868909548, policy loss: 5.5488578415574175
Experience 10, Iter 44, disc loss: 0.005076149843800841, policy loss: 5.503455339613591
Experience 10, Iter 45, disc loss: 0.004957543357075768, policy loss: 5.562174516224889
Experience 10, Iter 46, disc loss: 0.004641542444149032, policy loss: 5.647268621864235
Experience 10, Iter 47, disc loss: 0.005181921373962303, policy loss: 5.503039210875089
Experience 10, Iter 48, disc loss: 0.004660952458912707, policy loss: 5.598696246920459
Experience 10, Iter 49, disc loss: 0.005046798559064167, policy loss: 5.538043961895882
Experience 10, Iter 50, disc loss: 0.004143540888232683, policy loss: 5.800457664796967
Experience 10, Iter 51, disc loss: 0.004865969544746307, policy loss: 5.600284968854527
Experience 10, Iter 52, disc loss: 0.004822420196323456, policy loss: 5.5902272246865925
Experience 10, Iter 53, disc loss: 0.004984101514748816, policy loss: 5.5151233386903025
Experience 10, Iter 54, disc loss: 0.004782071796589267, policy loss: 5.570279320733601
Experience 10, Iter 55, disc loss: 0.004297852718545355, policy loss: 5.730439303067046
Experience 10, Iter 56, disc loss: 0.004927755856519472, policy loss: 5.563731140330214
Experience 10, Iter 57, disc loss: 0.004452374579699433, policy loss: 5.700259208342622
Experience 10, Iter 58, disc loss: 0.004567836534872645, policy loss: 5.658183078121727
Experience 10, Iter 59, disc loss: 0.004593270230655828, policy loss: 5.629165414743745
Experience 10, Iter 60, disc loss: 0.0044275378830270105, policy loss: 5.780618245072123
Experience 10, Iter 61, disc loss: 0.0042315439605024405, policy loss: 5.7304856103897155
Experience 10, Iter 62, disc loss: 0.0039937105480380725, policy loss: 5.809681752935933
Experience 10, Iter 63, disc loss: 0.004242318908992945, policy loss: 5.766297861350093
Experience 10, Iter 64, disc loss: 0.004613326702941326, policy loss: 5.6753093829700365
Experience 10, Iter 65, disc loss: 0.004024395376745795, policy loss: 5.789916340059845
Experience 10, Iter 66, disc loss: 0.0038612230260501156, policy loss: 5.827458122698989
Experience 10, Iter 67, disc loss: 0.003724240675738743, policy loss: 5.929746128218218
Experience 10, Iter 68, disc loss: 0.004360823446498411, policy loss: 5.700497498022075
Experience 10, Iter 69, disc loss: 0.003910478851953051, policy loss: 5.8313719620917945
Experience 10, Iter 70, disc loss: 0.004084405990673563, policy loss: 5.74657010442651
Experience 10, Iter 71, disc loss: 0.004023933660841153, policy loss: 5.786070634868843
Experience 10, Iter 72, disc loss: 0.004397486493589248, policy loss: 5.659838607342502
Experience 10, Iter 73, disc loss: 0.0040826349449385605, policy loss: 5.728650132613449
Experience 10, Iter 74, disc loss: 0.0038659801807672868, policy loss: 5.834967157802357
Experience 10, Iter 75, disc loss: 0.003972076152766711, policy loss: 5.7681324200162045
Experience 10, Iter 76, disc loss: 0.004145248748860844, policy loss: 5.774314175409918
Experience 10, Iter 77, disc loss: 0.0039676117143295315, policy loss: 5.811809720691644
Experience 10, Iter 78, disc loss: 0.003949811163854363, policy loss: 5.839962284490962
Experience 10, Iter 79, disc loss: 0.004141745704542901, policy loss: 5.714022441553761
Experience 10, Iter 80, disc loss: 0.003912658766126183, policy loss: 5.784535203341866
Experience 10, Iter 81, disc loss: 0.003907682222137338, policy loss: 5.815275991106473
Experience 10, Iter 82, disc loss: 0.003995878896671929, policy loss: 5.753056183591966
Experience 10, Iter 83, disc loss: 0.0036511665543148973, policy loss: 5.877843116780971
Experience 10, Iter 84, disc loss: 0.0034763437513635914, policy loss: 5.971877224192794
Experience 10, Iter 85, disc loss: 0.004111152166960794, policy loss: 5.750746022553054
Experience 10, Iter 86, disc loss: 0.003620543712992987, policy loss: 5.900217590429234
Experience 10, Iter 87, disc loss: 0.003546206937255592, policy loss: 5.905519067576755
Experience 10, Iter 88, disc loss: 0.0038563318134940253, policy loss: 5.829222989676249
Experience 10, Iter 89, disc loss: 0.003974505611294688, policy loss: 5.7655194150428954
Experience 10, Iter 90, disc loss: 0.0037540219758793983, policy loss: 5.844418182686144
Experience 10, Iter 91, disc loss: 0.0037825502703474788, policy loss: 5.861073670676916
Experience 10, Iter 92, disc loss: 0.0034678475063011647, policy loss: 5.941847217300262
Experience 10, Iter 93, disc loss: 0.0036632554542008556, policy loss: 5.856326432679895
Experience 10, Iter 94, disc loss: 0.003554113732898512, policy loss: 5.915800484687931
Experience 10, Iter 95, disc loss: 0.0035454363594544995, policy loss: 5.8870125566220874
Experience 10, Iter 96, disc loss: 0.003585568821455409, policy loss: 5.907526077692249
Experience 10, Iter 97, disc loss: 0.003720761799856422, policy loss: 5.858975704958567
Experience 10, Iter 98, disc loss: 0.003462055899927619, policy loss: 5.95328224775463
Experience 10, Iter 99, disc loss: 0.0034825639862609533, policy loss: 5.947780566689481
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0916],
        [0.8774],
        [0.0090]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0115, 0.0969, 0.4552, 0.0134, 0.0027, 2.3091]],

        [[0.0115, 0.0969, 0.4552, 0.0134, 0.0027, 2.3091]],

        [[0.0115, 0.0969, 0.4552, 0.0134, 0.0027, 2.3091]],

        [[0.0115, 0.0969, 0.4552, 0.0134, 0.0027, 2.3091]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0094, 0.3663, 3.5097, 0.0359], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0094, 0.3663, 3.5097, 0.0359])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.895
Iter 2/2000 - Loss: 2.111
Iter 3/2000 - Loss: 1.827
Iter 4/2000 - Loss: 1.835
Iter 5/2000 - Loss: 1.927
Iter 6/2000 - Loss: 1.863
Iter 7/2000 - Loss: 1.746
Iter 8/2000 - Loss: 1.699
Iter 9/2000 - Loss: 1.714
Iter 10/2000 - Loss: 1.704
Iter 11/2000 - Loss: 1.628
Iter 12/2000 - Loss: 1.523
Iter 13/2000 - Loss: 1.433
Iter 14/2000 - Loss: 1.367
Iter 15/2000 - Loss: 1.294
Iter 16/2000 - Loss: 1.183
Iter 17/2000 - Loss: 1.035
Iter 18/2000 - Loss: 0.869
Iter 19/2000 - Loss: 0.698
Iter 20/2000 - Loss: 0.521
Iter 1981/2000 - Loss: -7.337
Iter 1982/2000 - Loss: -7.337
Iter 1983/2000 - Loss: -7.337
Iter 1984/2000 - Loss: -7.337
Iter 1985/2000 - Loss: -7.337
Iter 1986/2000 - Loss: -7.337
Iter 1987/2000 - Loss: -7.337
Iter 1988/2000 - Loss: -7.337
Iter 1989/2000 - Loss: -7.337
Iter 1990/2000 - Loss: -7.337
Iter 1991/2000 - Loss: -7.337
Iter 1992/2000 - Loss: -7.337
Iter 1993/2000 - Loss: -7.337
Iter 1994/2000 - Loss: -7.337
Iter 1995/2000 - Loss: -7.337
Iter 1996/2000 - Loss: -7.337
Iter 1997/2000 - Loss: -7.337
Iter 1998/2000 - Loss: -7.337
Iter 1999/2000 - Loss: -7.337
Iter 2000/2000 - Loss: -7.337
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.4753,  7.0970, 39.5182, 11.4962, 12.2133, 46.4771]],

        [[18.9348, 33.1579, 10.9655,  1.4853,  4.5910, 32.7745]],

        [[20.2847, 36.1107, 11.4424,  1.1154,  2.1040, 16.0700]],

        [[17.4903, 32.0191, 12.0106,  3.2314,  1.1396, 38.9944]]])
Signal Variance: tensor([ 0.0985,  2.9672, 15.7336,  0.3216])
Estimated target variance: tensor([0.0094, 0.3663, 3.5097, 0.0359])
N: 110
Signal to noise ratio: tensor([15.6300, 77.1881, 83.6851, 32.3836])
Bound on condition number: tensor([ 26873.7399, 655381.5281, 770352.8493, 115357.5723])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0033921772398441356, policy loss: 5.946556110818891
Experience 11, Iter 1, disc loss: 0.0034642045969191827, policy loss: 5.990657471959816
Experience 11, Iter 2, disc loss: 0.0034076953315585775, policy loss: 5.992476310472902
Experience 11, Iter 3, disc loss: 0.003315443483617904, policy loss: 6.028890866877071
Experience 11, Iter 4, disc loss: 0.003673629733304628, policy loss: 5.828515916719322
Experience 11, Iter 5, disc loss: 0.003591241804081306, policy loss: 5.890405872250539
Experience 11, Iter 6, disc loss: 0.0033795268008720595, policy loss: 5.9669839874268575
Experience 11, Iter 7, disc loss: 0.0031225039985413915, policy loss: 6.10918283415217
Experience 11, Iter 8, disc loss: 0.003552736270894718, policy loss: 5.915555854542498
Experience 11, Iter 9, disc loss: 0.003524115236998628, policy loss: 5.932919359779973
Experience 11, Iter 10, disc loss: 0.0032729038560696303, policy loss: 6.015176784594882
Experience 11, Iter 11, disc loss: 0.003612009130484837, policy loss: 5.875196352550697
Experience 11, Iter 12, disc loss: 0.003655346099637048, policy loss: 5.913471024717497
Experience 11, Iter 13, disc loss: 0.003300238501667822, policy loss: 6.0098242241023865
Experience 11, Iter 14, disc loss: 0.0034168929029494673, policy loss: 5.951084221652129
Experience 11, Iter 15, disc loss: 0.003108080552461913, policy loss: 6.07821918525134
Experience 11, Iter 16, disc loss: 0.0030519658977030623, policy loss: 6.0885838160326955
Experience 11, Iter 17, disc loss: 0.003371506347519038, policy loss: 5.955724807786651
Experience 11, Iter 18, disc loss: 0.0034505388930465794, policy loss: 5.957841908735208
Experience 11, Iter 19, disc loss: 0.0032716387489809128, policy loss: 5.988343371090155
Experience 11, Iter 20, disc loss: 0.003419609370788144, policy loss: 5.945931031846984
Experience 11, Iter 21, disc loss: 0.003203162747637224, policy loss: 6.031501780851323
Experience 11, Iter 22, disc loss: 0.003180601575535425, policy loss: 6.051979961586937
Experience 11, Iter 23, disc loss: 0.003275320923287282, policy loss: 5.986941230239543
Experience 11, Iter 24, disc loss: 0.0031882931691736413, policy loss: 6.043690508891369
Experience 11, Iter 25, disc loss: 0.0030804951226013713, policy loss: 6.081966838372032
Experience 11, Iter 26, disc loss: 0.00321090517058963, policy loss: 6.003454298071681
Experience 11, Iter 27, disc loss: 0.0033241094690477035, policy loss: 5.9417130318753175
Experience 11, Iter 28, disc loss: 0.00317912083907863, policy loss: 6.0061382427514705
Experience 11, Iter 29, disc loss: 0.0030557924613895102, policy loss: 6.138807120318737
Experience 11, Iter 30, disc loss: 0.003173232543637102, policy loss: 6.015398713880934
Experience 11, Iter 31, disc loss: 0.003088109636411496, policy loss: 6.040853938952776
Experience 11, Iter 32, disc loss: 0.0031781359437656, policy loss: 5.973022799114074
Experience 11, Iter 33, disc loss: 0.0028692628398385976, policy loss: 6.231382788085475
Experience 11, Iter 34, disc loss: 0.003312884404836996, policy loss: 5.960884718378071
Experience 11, Iter 35, disc loss: 0.002917879790039392, policy loss: 6.120313388539573
Experience 11, Iter 36, disc loss: 0.0030924964588772237, policy loss: 6.0342427780171946
Experience 11, Iter 37, disc loss: 0.0027878450574527138, policy loss: 6.18784707048122
Experience 11, Iter 38, disc loss: 0.0031283773524397873, policy loss: 6.04701384860735
Experience 11, Iter 39, disc loss: 0.00280185456807824, policy loss: 6.151634315956409
Experience 11, Iter 40, disc loss: 0.0030167427909592198, policy loss: 6.074555221559744
Experience 11, Iter 41, disc loss: 0.002849494621636588, policy loss: 6.162981104192676
Experience 11, Iter 42, disc loss: 0.0027121673911824494, policy loss: 6.243161554501402
Experience 11, Iter 43, disc loss: 0.00268917742496983, policy loss: 6.2617215822323065
Experience 11, Iter 44, disc loss: 0.0029789916732533145, policy loss: 6.1224105926633525
Experience 11, Iter 45, disc loss: 0.0030243619419819375, policy loss: 6.102266756561788
Experience 11, Iter 46, disc loss: 0.002845778931000064, policy loss: 6.211705808998295
Experience 11, Iter 47, disc loss: 0.0027344481045724544, policy loss: 6.183308799254865
Experience 11, Iter 48, disc loss: 0.0029171200127171286, policy loss: 6.1053418910770745
Experience 11, Iter 49, disc loss: 0.002681311914327575, policy loss: 6.219926733152537
Experience 11, Iter 50, disc loss: 0.0025792206021063237, policy loss: 6.260034690593686
Experience 11, Iter 51, disc loss: 0.0026691786806688164, policy loss: 6.231424675186848
Experience 11, Iter 52, disc loss: 0.0027892059888868508, policy loss: 6.183947003066647
Experience 11, Iter 53, disc loss: 0.0028202818810646865, policy loss: 6.120894613631966
Experience 11, Iter 54, disc loss: 0.0025081885695002596, policy loss: 6.3416523946313195
Experience 11, Iter 55, disc loss: 0.0027168955505893727, policy loss: 6.205227491592876
Experience 11, Iter 56, disc loss: 0.0027006294147062103, policy loss: 6.206903369857721
Experience 11, Iter 57, disc loss: 0.002859386916824494, policy loss: 6.15574185895214
Experience 11, Iter 58, disc loss: 0.00242507632759099, policy loss: 6.378194740453546
Experience 11, Iter 59, disc loss: 0.0025310479766702536, policy loss: 6.291833955310033
Experience 11, Iter 60, disc loss: 0.002726112463273064, policy loss: 6.147818827308688
Experience 11, Iter 61, disc loss: 0.002804861676883509, policy loss: 6.137184469405527
Experience 11, Iter 62, disc loss: 0.002582009307354066, policy loss: 6.286571961721255
Experience 11, Iter 63, disc loss: 0.0024060669753110547, policy loss: 6.324800996357093
Experience 11, Iter 64, disc loss: 0.0024960784213548456, policy loss: 6.301968717045463
Experience 11, Iter 65, disc loss: 0.002548804911123545, policy loss: 6.269117840015076
Experience 11, Iter 66, disc loss: 0.00243269907986092, policy loss: 6.303209543366235
Experience 11, Iter 67, disc loss: 0.002452262099163543, policy loss: 6.297838889719692
Experience 11, Iter 68, disc loss: 0.002662870546847531, policy loss: 6.2053349791493915
Experience 11, Iter 69, disc loss: 0.002315093276879766, policy loss: 6.439733776633606
Experience 11, Iter 70, disc loss: 0.0026479186311253833, policy loss: 6.172480293908279
Experience 11, Iter 71, disc loss: 0.002426665835454658, policy loss: 6.315799091939667
Experience 11, Iter 72, disc loss: 0.002398650766835413, policy loss: 6.333988583203201
Experience 11, Iter 73, disc loss: 0.0026064519096901466, policy loss: 6.2361194658090096
Experience 11, Iter 74, disc loss: 0.002649404542607847, policy loss: 6.207199446254311
Experience 11, Iter 75, disc loss: 0.0025023107720484787, policy loss: 6.289283684196249
Experience 11, Iter 76, disc loss: 0.002386801060257367, policy loss: 6.380884709095834
Experience 11, Iter 77, disc loss: 0.0025504149007328876, policy loss: 6.218932895889671
Experience 11, Iter 78, disc loss: 0.0024671388483032356, policy loss: 6.2893507810880225
Experience 11, Iter 79, disc loss: 0.0025708154222864998, policy loss: 6.256056831300834
Experience 11, Iter 80, disc loss: 0.002658991749758904, policy loss: 6.18839316044759
Experience 11, Iter 81, disc loss: 0.002287867011475491, policy loss: 6.389973189831223
Experience 11, Iter 82, disc loss: 0.0025311681619257357, policy loss: 6.3229812985204905
Experience 11, Iter 83, disc loss: 0.002569713644569405, policy loss: 6.226926420501272
Experience 11, Iter 84, disc loss: 0.0026023759904748446, policy loss: 6.256769230244272
Experience 11, Iter 85, disc loss: 0.002296478467657393, policy loss: 6.355019951744045
Experience 11, Iter 86, disc loss: 0.0024841138464064363, policy loss: 6.320440975814651
Experience 11, Iter 87, disc loss: 0.002613280803163363, policy loss: 6.236481355915505
Experience 11, Iter 88, disc loss: 0.0024047971533907456, policy loss: 6.34473272639439
Experience 11, Iter 89, disc loss: 0.0022973559305998967, policy loss: 6.352816424666756
Experience 11, Iter 90, disc loss: 0.0026433422342671746, policy loss: 6.178061986522744
Experience 11, Iter 91, disc loss: 0.002492278771682285, policy loss: 6.256662217992716
Experience 11, Iter 92, disc loss: 0.002372726432444059, policy loss: 6.304846873036243
Experience 11, Iter 93, disc loss: 0.0024182428000457373, policy loss: 6.307348852088011
Experience 11, Iter 94, disc loss: 0.002483767467875068, policy loss: 6.2707548334068814
Experience 11, Iter 95, disc loss: 0.002523839091490109, policy loss: 6.244895423201874
Experience 11, Iter 96, disc loss: 0.002588012027454275, policy loss: 6.22527837283236
Experience 11, Iter 97, disc loss: 0.002129831847235598, policy loss: 6.493145397246914
Experience 11, Iter 98, disc loss: 0.0022887271225129876, policy loss: 6.407776272747997
Experience 11, Iter 99, disc loss: 0.0022647883548204883, policy loss: 6.410022910381345
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0841],
        [0.8059],
        [0.0083]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0124, 0.0903, 0.4184, 0.0123, 0.0025, 2.1203]],

        [[0.0124, 0.0903, 0.4184, 0.0123, 0.0025, 2.1203]],

        [[0.0124, 0.0903, 0.4184, 0.0123, 0.0025, 2.1203]],

        [[0.0124, 0.0903, 0.4184, 0.0123, 0.0025, 2.1203]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0088, 0.3366, 3.2237, 0.0330], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0088, 0.3366, 3.2237, 0.0330])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.742
Iter 2/2000 - Loss: 1.960
Iter 3/2000 - Loss: 1.669
Iter 4/2000 - Loss: 1.675
Iter 5/2000 - Loss: 1.772
Iter 6/2000 - Loss: 1.704
Iter 7/2000 - Loss: 1.586
Iter 8/2000 - Loss: 1.547
Iter 9/2000 - Loss: 1.567
Iter 10/2000 - Loss: 1.553
Iter 11/2000 - Loss: 1.473
Iter 12/2000 - Loss: 1.370
Iter 13/2000 - Loss: 1.288
Iter 14/2000 - Loss: 1.226
Iter 15/2000 - Loss: 1.150
Iter 16/2000 - Loss: 1.034
Iter 17/2000 - Loss: 0.882
Iter 18/2000 - Loss: 0.718
Iter 19/2000 - Loss: 0.550
Iter 20/2000 - Loss: 0.369
Iter 1981/2000 - Loss: -7.524
Iter 1982/2000 - Loss: -7.524
Iter 1983/2000 - Loss: -7.524
Iter 1984/2000 - Loss: -7.524
Iter 1985/2000 - Loss: -7.524
Iter 1986/2000 - Loss: -7.524
Iter 1987/2000 - Loss: -7.524
Iter 1988/2000 - Loss: -7.524
Iter 1989/2000 - Loss: -7.525
Iter 1990/2000 - Loss: -7.525
Iter 1991/2000 - Loss: -7.525
Iter 1992/2000 - Loss: -7.525
Iter 1993/2000 - Loss: -7.525
Iter 1994/2000 - Loss: -7.525
Iter 1995/2000 - Loss: -7.525
Iter 1996/2000 - Loss: -7.525
Iter 1997/2000 - Loss: -7.525
Iter 1998/2000 - Loss: -7.525
Iter 1999/2000 - Loss: -7.525
Iter 2000/2000 - Loss: -7.525
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.4759,  7.0485, 39.1120, 11.2542, 11.8139, 45.7607]],

        [[19.0290, 30.9320, 10.9267,  1.4837,  4.6019, 32.5824]],

        [[19.8448, 34.0841, 11.3781,  1.1151,  2.0317, 16.0266]],

        [[17.2940, 29.7984, 11.8155,  3.1989,  1.1068, 38.7232]]])
Signal Variance: tensor([ 0.0953,  2.9212, 15.4905,  0.3097])
Estimated target variance: tensor([0.0088, 0.3366, 3.2237, 0.0330])
N: 120
Signal to noise ratio: tensor([15.4369, 77.8667, 85.1426, 32.5440])
Bound on condition number: tensor([ 28596.6338, 727587.1392, 869913.0738, 127094.4059])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0023415489143353545, policy loss: 6.39471611602788
Experience 12, Iter 1, disc loss: 0.002365141146968247, policy loss: 6.33777389492969
Experience 12, Iter 2, disc loss: 0.0020800794146879076, policy loss: 6.498649848722097
Experience 12, Iter 3, disc loss: 0.0022086455517838625, policy loss: 6.437674829647788
Experience 12, Iter 4, disc loss: 0.002256690130722377, policy loss: 6.359557751779475
Experience 12, Iter 5, disc loss: 0.002101480848001054, policy loss: 6.532772338422777
Experience 12, Iter 6, disc loss: 0.0022421981920947307, policy loss: 6.375106940678277
Experience 12, Iter 7, disc loss: 0.002147351606438154, policy loss: 6.499187114756925
Experience 12, Iter 8, disc loss: 0.002304120236417644, policy loss: 6.437814673444324
Experience 12, Iter 9, disc loss: 0.002115667495848084, policy loss: 6.529079701433651
Experience 12, Iter 10, disc loss: 0.0022221824146379477, policy loss: 6.394623847260223
Experience 12, Iter 11, disc loss: 0.0022773969107079163, policy loss: 6.430818936380941
Experience 12, Iter 12, disc loss: 0.001987989741291096, policy loss: 6.539372499335488
Experience 12, Iter 13, disc loss: 0.0020607474495286378, policy loss: 6.490674245141173
Experience 12, Iter 14, disc loss: 0.0020093862098106477, policy loss: 6.536870648456726
Experience 12, Iter 15, disc loss: 0.0021513726801212526, policy loss: 6.437787734926935
Experience 12, Iter 16, disc loss: 0.0019383278658830563, policy loss: 6.608694291623667
Experience 12, Iter 17, disc loss: 0.0020551510051926344, policy loss: 6.4808421064005
Experience 12, Iter 18, disc loss: 0.0020986722274382048, policy loss: 6.4841774023679175
Experience 12, Iter 19, disc loss: 0.0020197235228870103, policy loss: 6.482051294344707
Experience 12, Iter 20, disc loss: 0.002225564500603081, policy loss: 6.388039578548726
Experience 12, Iter 21, disc loss: 0.0020873219080320676, policy loss: 6.5137126113711785
Experience 12, Iter 22, disc loss: 0.001978664013480068, policy loss: 6.515769000054006
Experience 12, Iter 23, disc loss: 0.0020349517136183924, policy loss: 6.576308372372707
Experience 12, Iter 24, disc loss: 0.0022722347781268543, policy loss: 6.340935506003165
Experience 12, Iter 25, disc loss: 0.0018419704192356389, policy loss: 6.619709468810058
Experience 12, Iter 26, disc loss: 0.0020459269405948837, policy loss: 6.488452286997907
Experience 12, Iter 27, disc loss: 0.0019533157546341782, policy loss: 6.557101178999288
Experience 12, Iter 28, disc loss: 0.0021919875462722807, policy loss: 6.3910714955013335
Experience 12, Iter 29, disc loss: 0.0020334904690882502, policy loss: 6.50349252862555
Experience 12, Iter 30, disc loss: 0.0018882028761059832, policy loss: 6.547049911383334
Experience 12, Iter 31, disc loss: 0.0021959437339911453, policy loss: 6.408000257173378
Experience 12, Iter 32, disc loss: 0.0020193266858849115, policy loss: 6.474302942337298
Experience 12, Iter 33, disc loss: 0.0021220128159461114, policy loss: 6.445718777728931
Experience 12, Iter 34, disc loss: 0.0020388520399193815, policy loss: 6.465401321260677
Experience 12, Iter 35, disc loss: 0.002041111555744052, policy loss: 6.476660806579125
Experience 12, Iter 36, disc loss: 0.002074500094237832, policy loss: 6.48089516505824
Experience 12, Iter 37, disc loss: 0.0020174752835609667, policy loss: 6.534210705358182
Experience 12, Iter 38, disc loss: 0.0019527405989323238, policy loss: 6.572710966971908
Experience 12, Iter 39, disc loss: 0.002039321283926823, policy loss: 6.4441403581322385
Experience 12, Iter 40, disc loss: 0.0018271221509139858, policy loss: 6.680103879581626
Experience 12, Iter 41, disc loss: 0.002037197325759228, policy loss: 6.488784786364624
Experience 12, Iter 42, disc loss: 0.0019624884615028044, policy loss: 6.5127174754925345
Experience 12, Iter 43, disc loss: 0.0019635618057889416, policy loss: 6.54709964961094
Experience 12, Iter 44, disc loss: 0.001987863272967332, policy loss: 6.471902769739363
Experience 12, Iter 45, disc loss: 0.002172008133658696, policy loss: 6.4023314335608426
Experience 12, Iter 46, disc loss: 0.0019436864413726659, policy loss: 6.530192194047555
Experience 12, Iter 47, disc loss: 0.0019716901116376907, policy loss: 6.537543830745072
Experience 12, Iter 48, disc loss: 0.0019881123044720186, policy loss: 6.520412223611746
Experience 12, Iter 49, disc loss: 0.002019464034872464, policy loss: 6.497410041002528
Experience 12, Iter 50, disc loss: 0.0017339640963384086, policy loss: 6.650149631741315
Experience 12, Iter 51, disc loss: 0.0018157504037175677, policy loss: 6.604678726369658
Experience 12, Iter 52, disc loss: 0.0018932469659222973, policy loss: 6.606818344907752
Experience 12, Iter 53, disc loss: 0.0019137528538184948, policy loss: 6.557909321800257
Experience 12, Iter 54, disc loss: 0.0019486418915202956, policy loss: 6.5513169170132795
Experience 12, Iter 55, disc loss: 0.002049018190858148, policy loss: 6.412593135403002
Experience 12, Iter 56, disc loss: 0.002074380889839239, policy loss: 6.41374943956203
Experience 12, Iter 57, disc loss: 0.0018239567224119788, policy loss: 6.5975735922677865
Experience 12, Iter 58, disc loss: 0.00203600076014544, policy loss: 6.453495991232145
Experience 12, Iter 59, disc loss: 0.0020056920526473347, policy loss: 6.492014695394981
Experience 12, Iter 60, disc loss: 0.0017272385480125475, policy loss: 6.6732025799709005
Experience 12, Iter 61, disc loss: 0.001936521006364307, policy loss: 6.530822238339228
Experience 12, Iter 62, disc loss: 0.0018190557814039292, policy loss: 6.609798737264656
Experience 12, Iter 63, disc loss: 0.0019757395845993063, policy loss: 6.4559318937598835
Experience 12, Iter 64, disc loss: 0.0016325615670680165, policy loss: 6.748713478881669
Experience 12, Iter 65, disc loss: 0.0016786428518612484, policy loss: 6.7173509173284565
Experience 12, Iter 66, disc loss: 0.001748779206896271, policy loss: 6.6590934590438255
Experience 12, Iter 67, disc loss: 0.0018488966425808035, policy loss: 6.677586925540238
Experience 12, Iter 68, disc loss: 0.0019637841920489758, policy loss: 6.558948432848075
Experience 12, Iter 69, disc loss: 0.0018732217728929623, policy loss: 6.565896750932179
Experience 12, Iter 70, disc loss: 0.00174485920792105, policy loss: 6.606615255479845
Experience 12, Iter 71, disc loss: 0.0017364916514465997, policy loss: 6.661968850062034
Experience 12, Iter 72, disc loss: 0.001821726069985233, policy loss: 6.595252268427511
Experience 12, Iter 73, disc loss: 0.0017859373957861188, policy loss: 6.601932582315609
Experience 12, Iter 74, disc loss: 0.002005946979283594, policy loss: 6.511569189406336
Experience 12, Iter 75, disc loss: 0.0017027843650184054, policy loss: 6.785830669358978
Experience 12, Iter 76, disc loss: 0.0017395277906017887, policy loss: 6.624474820225797
Experience 12, Iter 77, disc loss: 0.001718635505044251, policy loss: 6.668829258428174
Experience 12, Iter 78, disc loss: 0.0016682069616659, policy loss: 6.698717262979225
Experience 12, Iter 79, disc loss: 0.0017533785048933672, policy loss: 6.670297682956235
Experience 12, Iter 80, disc loss: 0.0016230878684323196, policy loss: 6.692926419115117
Experience 12, Iter 81, disc loss: 0.0016867210304137403, policy loss: 6.677959465708027
Experience 12, Iter 82, disc loss: 0.0016626861459908611, policy loss: 6.673304606811227
Experience 12, Iter 83, disc loss: 0.0017253076148331919, policy loss: 6.732801879953589
Experience 12, Iter 84, disc loss: 0.002026071368346897, policy loss: 6.452962048790873
Experience 12, Iter 85, disc loss: 0.0016495045660309088, policy loss: 6.766163935146118
Experience 12, Iter 86, disc loss: 0.0018709259294117007, policy loss: 6.535424479154121
Experience 12, Iter 87, disc loss: 0.001569458925230252, policy loss: 6.7624917572541765
Experience 12, Iter 88, disc loss: 0.0017373529967536032, policy loss: 6.663667495792804
Experience 12, Iter 89, disc loss: 0.0017833337812101593, policy loss: 6.615796312941164
Experience 12, Iter 90, disc loss: 0.0017579249182973536, policy loss: 6.641949276677337
Experience 12, Iter 91, disc loss: 0.0018635584820350399, policy loss: 6.546907807125349
Experience 12, Iter 92, disc loss: 0.001529211526396224, policy loss: 6.794314647891873
Experience 12, Iter 93, disc loss: 0.001633987693334803, policy loss: 6.734203384538842
Experience 12, Iter 94, disc loss: 0.0017828912733924966, policy loss: 6.669058891973789
Experience 12, Iter 95, disc loss: 0.001758053706607208, policy loss: 6.651008798029116
Experience 12, Iter 96, disc loss: 0.001839840934767394, policy loss: 6.59772507649371
Experience 12, Iter 97, disc loss: 0.0016834023860424463, policy loss: 6.682441362220954
Experience 12, Iter 98, disc loss: 0.00172474537032092, policy loss: 6.652854163618993
Experience 12, Iter 99, disc loss: 0.001568668629831719, policy loss: 6.755098210573731
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.0778],
        [0.7449],
        [0.0076]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0115, 0.0844, 0.3872, 0.0114, 0.0023, 1.9599]],

        [[0.0115, 0.0844, 0.3872, 0.0114, 0.0023, 1.9599]],

        [[0.0115, 0.0844, 0.3872, 0.0114, 0.0023, 1.9599]],

        [[0.0115, 0.0844, 0.3872, 0.0114, 0.0023, 1.9599]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.3112, 2.9798, 0.0306], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.3112, 2.9798, 0.0306])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.597
Iter 2/2000 - Loss: 1.832
Iter 3/2000 - Loss: 1.523
Iter 4/2000 - Loss: 1.529
Iter 5/2000 - Loss: 1.635
Iter 6/2000 - Loss: 1.562
Iter 7/2000 - Loss: 1.441
Iter 8/2000 - Loss: 1.404
Iter 9/2000 - Loss: 1.427
Iter 10/2000 - Loss: 1.414
Iter 11/2000 - Loss: 1.334
Iter 12/2000 - Loss: 1.229
Iter 13/2000 - Loss: 1.144
Iter 14/2000 - Loss: 1.077
Iter 15/2000 - Loss: 0.997
Iter 16/2000 - Loss: 0.877
Iter 17/2000 - Loss: 0.722
Iter 18/2000 - Loss: 0.551
Iter 19/2000 - Loss: 0.375
Iter 20/2000 - Loss: 0.187
Iter 1981/2000 - Loss: -7.692
Iter 1982/2000 - Loss: -7.692
Iter 1983/2000 - Loss: -7.692
Iter 1984/2000 - Loss: -7.692
Iter 1985/2000 - Loss: -7.692
Iter 1986/2000 - Loss: -7.692
Iter 1987/2000 - Loss: -7.692
Iter 1988/2000 - Loss: -7.692
Iter 1989/2000 - Loss: -7.692
Iter 1990/2000 - Loss: -7.692
Iter 1991/2000 - Loss: -7.692
Iter 1992/2000 - Loss: -7.692
Iter 1993/2000 - Loss: -7.692
Iter 1994/2000 - Loss: -7.692
Iter 1995/2000 - Loss: -7.692
Iter 1996/2000 - Loss: -7.692
Iter 1997/2000 - Loss: -7.692
Iter 1998/2000 - Loss: -7.692
Iter 1999/2000 - Loss: -7.692
Iter 2000/2000 - Loss: -7.692
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.0171,  7.0571, 39.5107, 11.0136, 11.4705, 44.8486]],

        [[18.4009, 30.1087, 10.8187,  1.4808,  4.6483, 32.5111]],

        [[18.6927, 32.7778, 11.2923,  1.1169,  2.0693, 15.9654]],

        [[16.6337, 28.7200, 11.8289,  3.2225,  1.1497, 38.0113]]])
Signal Variance: tensor([ 0.0945,  2.8800, 15.3108,  0.3083])
Estimated target variance: tensor([0.0082, 0.3112, 2.9798, 0.0306])
N: 130
Signal to noise ratio: tensor([15.6630, 79.8446, 84.4316, 32.6774])
Bound on condition number: tensor([ 31893.7443, 828771.7765, 926730.9144, 138816.4084])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0016072892977723077, policy loss: 6.7553839180538935
Experience 13, Iter 1, disc loss: 0.001679404512316483, policy loss: 6.726955370715497
Experience 13, Iter 2, disc loss: 0.0016442244612761483, policy loss: 6.763355006741235
Experience 13, Iter 3, disc loss: 0.0016200339328522897, policy loss: 6.73777886583563
Experience 13, Iter 4, disc loss: 0.0015325013330129552, policy loss: 6.799064460530308
Experience 13, Iter 5, disc loss: 0.001658558872849644, policy loss: 6.728092623107942
Experience 13, Iter 6, disc loss: 0.0015679342588925081, policy loss: 6.817257562589883
Experience 13, Iter 7, disc loss: 0.0016171719119190621, policy loss: 6.73175141473809
Experience 13, Iter 8, disc loss: 0.001463021534016981, policy loss: 6.846228965176981
Experience 13, Iter 9, disc loss: 0.0015586545845290133, policy loss: 6.843406502125553
Experience 13, Iter 10, disc loss: 0.0015145598681350018, policy loss: 6.806904479910577
Experience 13, Iter 11, disc loss: 0.0015157301120156525, policy loss: 6.863405422797886
Experience 13, Iter 12, disc loss: 0.0016073526740109966, policy loss: 6.771182743262698
Experience 13, Iter 13, disc loss: 0.0016322904568615132, policy loss: 6.715434420670915
Experience 13, Iter 14, disc loss: 0.0016039358316450438, policy loss: 6.738209860322223
Experience 13, Iter 15, disc loss: 0.001543565568112892, policy loss: 6.778681565240504
Experience 13, Iter 16, disc loss: 0.0014683349871864588, policy loss: 6.832468879237511
Experience 13, Iter 17, disc loss: 0.0016293186314614483, policy loss: 6.7070245355476406
Experience 13, Iter 18, disc loss: 0.001398492646280482, policy loss: 6.915495149775714
Experience 13, Iter 19, disc loss: 0.001670504687194254, policy loss: 6.64912245614795
Experience 13, Iter 20, disc loss: 0.0015247403180343446, policy loss: 6.724644869030172
Experience 13, Iter 21, disc loss: 0.001548494891951059, policy loss: 6.75143673031291
Experience 13, Iter 22, disc loss: 0.0014556713925904694, policy loss: 6.850063624195391
Experience 13, Iter 23, disc loss: 0.0015643113001724766, policy loss: 6.775356730097869
Experience 13, Iter 24, disc loss: 0.0015246767661441366, policy loss: 6.8024858135215105
Experience 13, Iter 25, disc loss: 0.0014386943592252057, policy loss: 6.8189340366544755
Experience 13, Iter 26, disc loss: 0.0014314942264877044, policy loss: 6.825848198773845
Experience 13, Iter 27, disc loss: 0.0015522101299430655, policy loss: 6.752675304276451
Experience 13, Iter 28, disc loss: 0.0013091687255489994, policy loss: 6.965638059849972
Experience 13, Iter 29, disc loss: 0.0015933596750304247, policy loss: 6.728384401472143
Experience 13, Iter 30, disc loss: 0.001502515236424317, policy loss: 6.788278508173116
Experience 13, Iter 31, disc loss: 0.0015156733603468935, policy loss: 6.790754736294042
Experience 13, Iter 32, disc loss: 0.0014457101106579078, policy loss: 6.855313280408632
Experience 13, Iter 33, disc loss: 0.0014320933506066713, policy loss: 6.920191801466521
Experience 13, Iter 34, disc loss: 0.001448809596046085, policy loss: 6.832691242228716
Experience 13, Iter 35, disc loss: 0.0014046819036198875, policy loss: 6.902864548604574
Experience 13, Iter 36, disc loss: 0.0015074295198604659, policy loss: 6.793451221669072
Experience 13, Iter 37, disc loss: 0.0014491435188924862, policy loss: 6.917077727370712
Experience 13, Iter 38, disc loss: 0.0013507157390868993, policy loss: 6.947166239386522
Experience 13, Iter 39, disc loss: 0.0012762806996576461, policy loss: 7.068055069439826
Experience 13, Iter 40, disc loss: 0.001462035559544383, policy loss: 6.834148416681456
Experience 13, Iter 41, disc loss: 0.0013265151184389872, policy loss: 7.006139760856472
Experience 13, Iter 42, disc loss: 0.0016658552682331754, policy loss: 6.670439941199106
Experience 13, Iter 43, disc loss: 0.0016262787044817636, policy loss: 6.7601808711201725
Experience 13, Iter 44, disc loss: 0.0014958824962644738, policy loss: 6.87048378592687
Experience 13, Iter 45, disc loss: 0.001553981791964292, policy loss: 6.772048026625775
Experience 13, Iter 46, disc loss: 0.0015015907862537304, policy loss: 6.8743231292165925
Experience 13, Iter 47, disc loss: 0.0015450912989536092, policy loss: 6.770799624035625
Experience 13, Iter 48, disc loss: 0.00135732644073971, policy loss: 6.9494643188656395
Experience 13, Iter 49, disc loss: 0.0015082651177391587, policy loss: 6.808063530425794
Experience 13, Iter 50, disc loss: 0.0014348572359421288, policy loss: 6.859624059710486
Experience 13, Iter 51, disc loss: 0.001545545085837055, policy loss: 6.76701345105355
Experience 13, Iter 52, disc loss: 0.0014035811898790646, policy loss: 6.859051419066768
Experience 13, Iter 53, disc loss: 0.0015482440799992113, policy loss: 6.774062407520004
Experience 13, Iter 54, disc loss: 0.001495144749156479, policy loss: 6.840800031662288
Experience 13, Iter 55, disc loss: 0.0013274053111050335, policy loss: 7.011126890916835
Experience 13, Iter 56, disc loss: 0.0012733038662735168, policy loss: 7.0154408856125166
Experience 13, Iter 57, disc loss: 0.0013842330958327626, policy loss: 6.9306583104368915
Experience 13, Iter 58, disc loss: 0.0015220624089604155, policy loss: 6.796053778775248
Experience 13, Iter 59, disc loss: 0.0014482222130037387, policy loss: 6.815543909741535
Experience 13, Iter 60, disc loss: 0.0014421390692449486, policy loss: 6.832397877161116
Experience 13, Iter 61, disc loss: 0.001390051838354131, policy loss: 6.870460420835734
Experience 13, Iter 62, disc loss: 0.001374961848425613, policy loss: 6.916545710933331
Experience 13, Iter 63, disc loss: 0.0013644417221487439, policy loss: 6.88914594400368
Experience 13, Iter 64, disc loss: 0.0014118887057116404, policy loss: 6.8681806390733895
Experience 13, Iter 65, disc loss: 0.0012347047133543134, policy loss: 7.0535243480985965
Experience 13, Iter 66, disc loss: 0.0013210022775293757, policy loss: 6.908776012769103
Experience 13, Iter 67, disc loss: 0.0013894567855739066, policy loss: 6.840034890333808
Experience 13, Iter 68, disc loss: 0.001373413825069815, policy loss: 6.907248428594518
Experience 13, Iter 69, disc loss: 0.0013846106249594914, policy loss: 6.921653509758768
Experience 13, Iter 70, disc loss: 0.0013834718390499713, policy loss: 6.872650550322373
Experience 13, Iter 71, disc loss: 0.001493134425243253, policy loss: 6.770589759786498
Experience 13, Iter 72, disc loss: 0.0014732107147830504, policy loss: 6.761555998728057
Experience 13, Iter 73, disc loss: 0.0014241362695599812, policy loss: 6.798419689245545
Experience 13, Iter 74, disc loss: 0.0014917960440466635, policy loss: 6.750199648637684
Experience 13, Iter 75, disc loss: 0.001204915695315892, policy loss: 7.055551718552567
Experience 13, Iter 76, disc loss: 0.0014097614666868888, policy loss: 6.878016560250392
Experience 13, Iter 77, disc loss: 0.0015080446186400112, policy loss: 6.822008939770342
Experience 13, Iter 78, disc loss: 0.0012591578544454995, policy loss: 7.010143815065616
Experience 13, Iter 79, disc loss: 0.0013387435821204038, policy loss: 6.934514623123382
Experience 13, Iter 80, disc loss: 0.0013564128148979484, policy loss: 6.992160872870427
Experience 13, Iter 81, disc loss: 0.0012682441452339433, policy loss: 6.997301475056426
Experience 13, Iter 82, disc loss: 0.0011300985576939235, policy loss: 7.161618981151951
Experience 13, Iter 83, disc loss: 0.0013720798213543192, policy loss: 6.910197798807872
Experience 13, Iter 84, disc loss: 0.001254181191816141, policy loss: 7.07709618343438
Experience 13, Iter 85, disc loss: 0.0013763035529846018, policy loss: 6.907959391080832
Experience 13, Iter 86, disc loss: 0.0012510262835950397, policy loss: 6.983260057166063
Experience 13, Iter 87, disc loss: 0.0013397048845491838, policy loss: 6.911513260298184
Experience 13, Iter 88, disc loss: 0.0011961920451349132, policy loss: 7.062579045521738
Experience 13, Iter 89, disc loss: 0.0012509557451343533, policy loss: 7.063382267775673
Experience 13, Iter 90, disc loss: 0.001297936400690383, policy loss: 6.916571058166239
Experience 13, Iter 91, disc loss: 0.0012280603404619338, policy loss: 7.015526621946049
Experience 13, Iter 92, disc loss: 0.0013341993730583736, policy loss: 6.9439017676162775
Experience 13, Iter 93, disc loss: 0.0012791204944050845, policy loss: 6.96738143315206
Experience 13, Iter 94, disc loss: 0.0013144276584868134, policy loss: 6.909484174734748
Experience 13, Iter 95, disc loss: 0.001246627592218697, policy loss: 7.000492638213735
Experience 13, Iter 96, disc loss: 0.0012969401584166887, policy loss: 7.018901040006009
Experience 13, Iter 97, disc loss: 0.0013140211628588096, policy loss: 6.974944995897694
Experience 13, Iter 98, disc loss: 0.0012278521909160726, policy loss: 7.035520191597605
Experience 13, Iter 99, disc loss: 0.001222378887636123, policy loss: 6.996468720931593
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0725],
        [0.6947],
        [0.0071]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0107, 0.0788, 0.3606, 0.0106, 0.0022, 1.8265]],

        [[0.0107, 0.0788, 0.3606, 0.0106, 0.0022, 1.8265]],

        [[0.0107, 0.0788, 0.3606, 0.0106, 0.0022, 1.8265]],

        [[0.0107, 0.0788, 0.3606, 0.0106, 0.0022, 1.8265]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0077, 0.2902, 2.7788, 0.0285], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0077, 0.2902, 2.7788, 0.0285])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.464
Iter 2/2000 - Loss: 1.725
Iter 3/2000 - Loss: 1.392
Iter 4/2000 - Loss: 1.402
Iter 5/2000 - Loss: 1.518
Iter 6/2000 - Loss: 1.441
Iter 7/2000 - Loss: 1.316
Iter 8/2000 - Loss: 1.279
Iter 9/2000 - Loss: 1.303
Iter 10/2000 - Loss: 1.292
Iter 11/2000 - Loss: 1.212
Iter 12/2000 - Loss: 1.105
Iter 13/2000 - Loss: 1.011
Iter 14/2000 - Loss: 0.936
Iter 15/2000 - Loss: 0.851
Iter 16/2000 - Loss: 0.730
Iter 17/2000 - Loss: 0.571
Iter 18/2000 - Loss: 0.393
Iter 19/2000 - Loss: 0.210
Iter 20/2000 - Loss: 0.018
Iter 1981/2000 - Loss: -7.862
Iter 1982/2000 - Loss: -7.862
Iter 1983/2000 - Loss: -7.862
Iter 1984/2000 - Loss: -7.862
Iter 1985/2000 - Loss: -7.862
Iter 1986/2000 - Loss: -7.862
Iter 1987/2000 - Loss: -7.862
Iter 1988/2000 - Loss: -7.862
Iter 1989/2000 - Loss: -7.862
Iter 1990/2000 - Loss: -7.862
Iter 1991/2000 - Loss: -7.862
Iter 1992/2000 - Loss: -7.862
Iter 1993/2000 - Loss: -7.862
Iter 1994/2000 - Loss: -7.862
Iter 1995/2000 - Loss: -7.862
Iter 1996/2000 - Loss: -7.862
Iter 1997/2000 - Loss: -7.862
Iter 1998/2000 - Loss: -7.862
Iter 1999/2000 - Loss: -7.862
Iter 2000/2000 - Loss: -7.862
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.7359,  6.9947, 39.0965, 10.7464, 11.5585, 44.2454]],

        [[17.9809, 29.5066, 10.6710,  1.4882,  4.6570, 32.5850]],

        [[18.1904, 31.7875, 11.5773,  1.1058,  2.0207, 15.9950]],

        [[16.1878, 27.4333, 12.0467,  3.1961,  1.1300, 38.7528]]])
Signal Variance: tensor([ 0.0920,  2.8648, 15.1752,  0.3091])
Estimated target variance: tensor([0.0077, 0.2902, 2.7788, 0.0285])
N: 140
Signal to noise ratio: tensor([15.7951, 81.0062, 85.0639, 33.4342])
Bound on condition number: tensor([  34928.8980,  918682.0377, 1013023.5152,  156499.3006])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0012027660979724525, policy loss: 7.0551711358290525
Experience 14, Iter 1, disc loss: 0.0011605970089228436, policy loss: 7.051025555854948
Experience 14, Iter 2, disc loss: 0.0012521159224362336, policy loss: 6.983221479407586
Experience 14, Iter 3, disc loss: 0.0011288816819974075, policy loss: 7.164219400558378
Experience 14, Iter 4, disc loss: 0.0011851356949927607, policy loss: 7.06918615542145
Experience 14, Iter 5, disc loss: 0.0012759128323522524, policy loss: 6.968612195784222
Experience 14, Iter 6, disc loss: 0.0012738464615114206, policy loss: 6.935602527753787
Experience 14, Iter 7, disc loss: 0.0010719473544225744, policy loss: 7.167852411808891
Experience 14, Iter 8, disc loss: 0.0013794590602920762, policy loss: 6.883712247609806
Experience 14, Iter 9, disc loss: 0.0011176831630387825, policy loss: 7.136703926321874
Experience 14, Iter 10, disc loss: 0.0011643257611543023, policy loss: 7.09485188520083
Experience 14, Iter 11, disc loss: 0.0012366992964431345, policy loss: 7.007422534250389
Experience 14, Iter 12, disc loss: 0.0013777304925662434, policy loss: 6.865693780719726
Experience 14, Iter 13, disc loss: 0.0013075700478389692, policy loss: 6.913685835148755
Experience 14, Iter 14, disc loss: 0.001234619049339324, policy loss: 7.0015772496586575
Experience 14, Iter 15, disc loss: 0.0012085168778069781, policy loss: 6.995132234332675
Experience 14, Iter 16, disc loss: 0.0011296884291186787, policy loss: 7.0882856567729915
Experience 14, Iter 17, disc loss: 0.0011288699371444582, policy loss: 7.159112794714041
Experience 14, Iter 18, disc loss: 0.0011591250764794014, policy loss: 7.064635818599285
Experience 14, Iter 19, disc loss: 0.0011449715300695254, policy loss: 7.1005508312585865
Experience 14, Iter 20, disc loss: 0.0011719835670189656, policy loss: 7.04121147119163
Experience 14, Iter 21, disc loss: 0.001153406954123118, policy loss: 7.083877349415921
Experience 14, Iter 22, disc loss: 0.0011376940098691555, policy loss: 7.076831826283181
Experience 14, Iter 23, disc loss: 0.0013307857372543046, policy loss: 6.880411558897734
Experience 14, Iter 24, disc loss: 0.0012355955550995056, policy loss: 7.045567145896265
Experience 14, Iter 25, disc loss: 0.001079449425363329, policy loss: 7.176339128648243
Experience 14, Iter 26, disc loss: 0.001174173311628075, policy loss: 7.09072436693949
Experience 14, Iter 27, disc loss: 0.0011837139074539036, policy loss: 7.129639635066953
Experience 14, Iter 28, disc loss: 0.0011525818555288881, policy loss: 7.077311385405592
Experience 14, Iter 29, disc loss: 0.00119811645661326, policy loss: 7.000245615467505
Experience 14, Iter 30, disc loss: 0.0011169265824251123, policy loss: 7.174780134061288
Experience 14, Iter 31, disc loss: 0.0010712598297016823, policy loss: 7.161826988213857
Experience 14, Iter 32, disc loss: 0.0012111457284855083, policy loss: 7.054308551647467
Experience 14, Iter 33, disc loss: 0.0009953363293438637, policy loss: 7.299848767543312
Experience 14, Iter 34, disc loss: 0.001134692619267835, policy loss: 7.154368870620379
Experience 14, Iter 35, disc loss: 0.0010404294651609521, policy loss: 7.209295074301306
Experience 14, Iter 36, disc loss: 0.0010873791001419502, policy loss: 7.11962265215755
Experience 14, Iter 37, disc loss: 0.0011554752410211813, policy loss: 7.018959775111898
Experience 14, Iter 38, disc loss: 0.001059268700086849, policy loss: 7.206346877475803
Experience 14, Iter 39, disc loss: 0.0010304485543169897, policy loss: 7.161119109707238
Experience 14, Iter 40, disc loss: 0.00098830720567132, policy loss: 7.235270563357826
Experience 14, Iter 41, disc loss: 0.00106178295394808, policy loss: 7.138835421705442
Experience 14, Iter 42, disc loss: 0.0011030852769532893, policy loss: 7.14198107719179
Experience 14, Iter 43, disc loss: 0.0010591882070730193, policy loss: 7.156015642457407
Experience 14, Iter 44, disc loss: 0.0010844230325261915, policy loss: 7.122822239087775
Experience 14, Iter 45, disc loss: 0.001115871616101112, policy loss: 7.077442234912958
Experience 14, Iter 46, disc loss: 0.0011758219267938319, policy loss: 7.025954388458011
Experience 14, Iter 47, disc loss: 0.0009429235006341189, policy loss: 7.278067027527843
Experience 14, Iter 48, disc loss: 0.0010141884889511487, policy loss: 7.177634977657126
Experience 14, Iter 49, disc loss: 0.0010832004505510655, policy loss: 7.094429455167763
Experience 14, Iter 50, disc loss: 0.0009768838500459462, policy loss: 7.2247356019348405
Experience 14, Iter 51, disc loss: 0.0011317127986956096, policy loss: 7.060791357413669
Experience 14, Iter 52, disc loss: 0.0010791018727587455, policy loss: 7.170156548791996
Experience 14, Iter 53, disc loss: 0.0011079069239121055, policy loss: 7.066858904455902
Experience 14, Iter 54, disc loss: 0.0011558407348469342, policy loss: 7.069556244793907
Experience 14, Iter 55, disc loss: 0.0011388995941446149, policy loss: 7.091124027995038
Experience 14, Iter 56, disc loss: 0.0011507887551356116, policy loss: 7.1066413973513205
Experience 14, Iter 57, disc loss: 0.0011124561777947745, policy loss: 7.135223748262577
Experience 14, Iter 58, disc loss: 0.0011276608211015933, policy loss: 7.109358001738744
Experience 14, Iter 59, disc loss: 0.001095107152149857, policy loss: 7.139398487635583
Experience 14, Iter 60, disc loss: 0.0011112237209542656, policy loss: 7.102455324015917
Experience 14, Iter 61, disc loss: 0.001099402958886274, policy loss: 7.160138126033571
Experience 14, Iter 62, disc loss: 0.0011190506701492704, policy loss: 7.111138562933018
Experience 14, Iter 63, disc loss: 0.0012179472976237501, policy loss: 6.992005722010656
Experience 14, Iter 64, disc loss: 0.0011344638324838638, policy loss: 7.032677250551437
Experience 14, Iter 65, disc loss: 0.0010388223107539727, policy loss: 7.20864603413057
Experience 14, Iter 66, disc loss: 0.001123921224847907, policy loss: 7.136089643413927
Experience 14, Iter 67, disc loss: 0.0010815358097131477, policy loss: 7.21093023421665
Experience 14, Iter 68, disc loss: 0.0011878453097168241, policy loss: 7.0553210253153384
Experience 14, Iter 69, disc loss: 0.000926056632254991, policy loss: 7.360682062456369
Experience 14, Iter 70, disc loss: 0.0010567649916539486, policy loss: 7.14923930359468
Experience 14, Iter 71, disc loss: 0.0010393903414796005, policy loss: 7.220312130853755
Experience 14, Iter 72, disc loss: 0.0010530950724981023, policy loss: 7.149450964745776
Experience 14, Iter 73, disc loss: 0.0009147283242456148, policy loss: 7.307566234991212
Experience 14, Iter 74, disc loss: 0.0010219289407263845, policy loss: 7.189456168678268
Experience 14, Iter 75, disc loss: 0.0010084152018681953, policy loss: 7.1912977756444
Experience 14, Iter 76, disc loss: 0.0010428283496343874, policy loss: 7.177983688392615
Experience 14, Iter 77, disc loss: 0.0010715764942253252, policy loss: 7.141446309734471
Experience 14, Iter 78, disc loss: 0.0009762482683090746, policy loss: 7.212174508807452
Experience 14, Iter 79, disc loss: 0.0009260122934842698, policy loss: 7.321440562657311
Experience 14, Iter 80, disc loss: 0.001045146504164087, policy loss: 7.184396616169397
Experience 14, Iter 81, disc loss: 0.0010356876932186708, policy loss: 7.172612998730684
Experience 14, Iter 82, disc loss: 0.0010215986080477622, policy loss: 7.204075738894416
Experience 14, Iter 83, disc loss: 0.0011206860924975902, policy loss: 7.136837689177583
Experience 14, Iter 84, disc loss: 0.0011950500597889264, policy loss: 7.101020703333513
Experience 14, Iter 85, disc loss: 0.0011091512072333059, policy loss: 7.174939033532372
Experience 14, Iter 86, disc loss: 0.0011021783156582579, policy loss: 7.2481803649894365
Experience 14, Iter 87, disc loss: 0.001103958503417339, policy loss: 7.1896801327623
Experience 14, Iter 88, disc loss: 0.0011441515386735714, policy loss: 7.096296828079039
Experience 14, Iter 89, disc loss: 0.0009431476726387907, policy loss: 7.415473213170397
Experience 14, Iter 90, disc loss: 0.0008885945410780265, policy loss: 7.6250536414676535
Experience 14, Iter 91, disc loss: 0.0009326481330896859, policy loss: 7.4535183713012625
Experience 14, Iter 92, disc loss: 0.0010019153135759939, policy loss: 7.367242295671092
Experience 14, Iter 93, disc loss: 0.001144062542531487, policy loss: 7.1335980114534845
Experience 14, Iter 94, disc loss: 0.0009231244509990454, policy loss: 7.408777193389922
Experience 14, Iter 95, disc loss: 0.0010132513663124614, policy loss: 7.454635233581396
Experience 14, Iter 96, disc loss: 0.0009298107917525001, policy loss: 7.45128692209552
Experience 14, Iter 97, disc loss: 0.0010932823850470098, policy loss: 7.132326515630579
Experience 14, Iter 98, disc loss: 0.0011003854458340322, policy loss: 7.18652121919361
Experience 14, Iter 99, disc loss: 0.000976701381454395, policy loss: 7.327357971603334
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0679],
        [0.6504],
        [0.0067]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0102, 0.0752, 0.3373, 0.0099, 0.0021, 1.7090]],

        [[0.0102, 0.0752, 0.3373, 0.0099, 0.0021, 1.7090]],

        [[0.0102, 0.0752, 0.3373, 0.0099, 0.0021, 1.7090]],

        [[0.0102, 0.0752, 0.3373, 0.0099, 0.0021, 1.7090]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0073, 0.2718, 2.6015, 0.0267], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0073, 0.2718, 2.6015, 0.0267])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.352
Iter 2/2000 - Loss: 1.612
Iter 3/2000 - Loss: 1.276
Iter 4/2000 - Loss: 1.284
Iter 5/2000 - Loss: 1.404
Iter 6/2000 - Loss: 1.326
Iter 7/2000 - Loss: 1.204
Iter 8/2000 - Loss: 1.176
Iter 9/2000 - Loss: 1.205
Iter 10/2000 - Loss: 1.192
Iter 11/2000 - Loss: 1.113
Iter 12/2000 - Loss: 1.011
Iter 13/2000 - Loss: 0.926
Iter 14/2000 - Loss: 0.857
Iter 15/2000 - Loss: 0.773
Iter 16/2000 - Loss: 0.649
Iter 17/2000 - Loss: 0.490
Iter 18/2000 - Loss: 0.315
Iter 19/2000 - Loss: 0.136
Iter 20/2000 - Loss: -0.054
Iter 1981/2000 - Loss: -8.000
Iter 1982/2000 - Loss: -8.000
Iter 1983/2000 - Loss: -8.000
Iter 1984/2000 - Loss: -8.000
Iter 1985/2000 - Loss: -8.000
Iter 1986/2000 - Loss: -8.000
Iter 1987/2000 - Loss: -8.000
Iter 1988/2000 - Loss: -8.001
Iter 1989/2000 - Loss: -8.001
Iter 1990/2000 - Loss: -8.001
Iter 1991/2000 - Loss: -8.001
Iter 1992/2000 - Loss: -8.001
Iter 1993/2000 - Loss: -8.001
Iter 1994/2000 - Loss: -8.001
Iter 1995/2000 - Loss: -8.001
Iter 1996/2000 - Loss: -8.001
Iter 1997/2000 - Loss: -8.001
Iter 1998/2000 - Loss: -8.001
Iter 1999/2000 - Loss: -8.001
Iter 2000/2000 - Loss: -8.001
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.4852,  6.9920, 38.5300, 10.5583, 11.0832, 43.9055]],

        [[17.4905, 27.8724, 10.6127,  1.4870,  4.7585, 32.7036]],

        [[17.5675, 30.2362, 11.4576,  1.1058,  2.0452, 15.8842]],

        [[15.6444, 26.2521, 11.9001,  3.1968,  1.0997, 39.4646]]])
Signal Variance: tensor([ 0.0905,  2.8721, 14.9279,  0.3069])
Estimated target variance: tensor([0.0073, 0.2718, 2.6015, 0.0267])
N: 150
Signal to noise ratio: tensor([16.0029, 84.2439, 85.7198, 32.9071])
Bound on condition number: tensor([  38415.1001, 1064555.7543, 1102183.5223,  162432.4639])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0011373767619095832, policy loss: 7.107833241864223
Experience 15, Iter 1, disc loss: 0.001066016681271292, policy loss: 7.163513102657802
Experience 15, Iter 2, disc loss: 0.0010987260691871935, policy loss: 7.10749191467592
Experience 15, Iter 3, disc loss: 0.0010880056055214559, policy loss: 7.129196282616765
Experience 15, Iter 4, disc loss: 0.0009844018543827106, policy loss: 7.210176762161234
Experience 15, Iter 5, disc loss: 0.001006800717084376, policy loss: 7.161873718897869
Experience 15, Iter 6, disc loss: 0.0011814676339621506, policy loss: 7.061029159209015
Experience 15, Iter 7, disc loss: 0.0010775907437490149, policy loss: 7.082373359705866
Experience 15, Iter 8, disc loss: 0.0010041808192894268, policy loss: 7.2366594170872
Experience 15, Iter 9, disc loss: 0.0009613165809523947, policy loss: 7.237528784951705
Experience 15, Iter 10, disc loss: 0.0010623978043336627, policy loss: 7.101414641869612
Experience 15, Iter 11, disc loss: 0.0009578201568640572, policy loss: 7.261309594291841
Experience 15, Iter 12, disc loss: 0.0010511590567378198, policy loss: 7.135142314484326
Experience 15, Iter 13, disc loss: 0.0009214794367584018, policy loss: 7.248725811031241
Experience 15, Iter 14, disc loss: 0.0008799959037093532, policy loss: 7.4316504378432064
Experience 15, Iter 15, disc loss: 0.0010189125736906372, policy loss: 7.210122155127234
Experience 15, Iter 16, disc loss: 0.0010186696936781813, policy loss: 7.227605009383041
Experience 15, Iter 17, disc loss: 0.001044795587262266, policy loss: 7.187299493493067
Experience 15, Iter 18, disc loss: 0.000934304460392429, policy loss: 7.286231516005618
Experience 15, Iter 19, disc loss: 0.0010769528012167955, policy loss: 7.0879328115620766
Experience 15, Iter 20, disc loss: 0.0009191092386044485, policy loss: 7.271850660129577
Experience 15, Iter 21, disc loss: 0.0010106024959170212, policy loss: 7.262979410538108
Experience 15, Iter 22, disc loss: 0.0009987821050708692, policy loss: 7.182035998465968
Experience 15, Iter 23, disc loss: 0.0011040781728536828, policy loss: 7.096414341183818
Experience 15, Iter 24, disc loss: 0.0009582895601367217, policy loss: 7.257696443502163
Experience 15, Iter 25, disc loss: 0.001171843003616936, policy loss: 7.025470908976311
Experience 15, Iter 26, disc loss: 0.0009720020884456254, policy loss: 7.30342282246505
Experience 15, Iter 27, disc loss: 0.0009980348051590756, policy loss: 7.227480962757389
Experience 15, Iter 28, disc loss: 0.0009583735728598644, policy loss: 7.295945166809892
Experience 15, Iter 29, disc loss: 0.0009067189828000744, policy loss: 7.356302868151673
Experience 15, Iter 30, disc loss: 0.000978906033715934, policy loss: 7.300264149762415
Experience 15, Iter 31, disc loss: 0.0010157270052558612, policy loss: 7.23706858275921
Experience 15, Iter 32, disc loss: 0.000990377854834156, policy loss: 7.2135292518475165
Experience 15, Iter 33, disc loss: 0.0009277924985430937, policy loss: 7.315189422924701
Experience 15, Iter 34, disc loss: 0.0009221880483574468, policy loss: 7.3791473082733505
Experience 15, Iter 35, disc loss: 0.0010112794116377063, policy loss: 7.184196280691111
Experience 15, Iter 36, disc loss: 0.0008834795336401427, policy loss: 7.429370264220875
Experience 15, Iter 37, disc loss: 0.0009185444831295604, policy loss: 7.325671352662118
Experience 15, Iter 38, disc loss: 0.0010234707216042103, policy loss: 7.193773370054097
Experience 15, Iter 39, disc loss: 0.0009819911857042172, policy loss: 7.251643501706356
Experience 15, Iter 40, disc loss: 0.0009871990167333328, policy loss: 7.264902180655392
Experience 15, Iter 41, disc loss: 0.0009510934263950356, policy loss: 7.298594672233831
Experience 15, Iter 42, disc loss: 0.0009763237798626913, policy loss: 7.290793573748467
Experience 15, Iter 43, disc loss: 0.0009477591578188608, policy loss: 7.324216128572424
Experience 15, Iter 44, disc loss: 0.0010025932567793677, policy loss: 7.188763998726774
Experience 15, Iter 45, disc loss: 0.0009676954411068548, policy loss: 7.27753161815887
Experience 15, Iter 46, disc loss: 0.0009655235244498178, policy loss: 7.275646484373463
Experience 15, Iter 47, disc loss: 0.0010222839006717196, policy loss: 7.193848960369563
Experience 15, Iter 48, disc loss: 0.0008951714032174577, policy loss: 7.337720733217129
Experience 15, Iter 49, disc loss: 0.0008486650059692902, policy loss: 7.385164646392667
Experience 15, Iter 50, disc loss: 0.0009831818305198416, policy loss: 7.179629359062083
Experience 15, Iter 51, disc loss: 0.0009396628525419344, policy loss: 7.271655069381506
Experience 15, Iter 52, disc loss: 0.0008505868259949299, policy loss: 7.374088474317418
Experience 15, Iter 53, disc loss: 0.0008930128797256213, policy loss: 7.378421768150364
Experience 15, Iter 54, disc loss: 0.0010171419511756843, policy loss: 7.227082091628788
Experience 15, Iter 55, disc loss: 0.0009374665140135736, policy loss: 7.361881755958693
Experience 15, Iter 56, disc loss: 0.0009104344575620109, policy loss: 7.290290106639237
Experience 15, Iter 57, disc loss: 0.0009293673232238787, policy loss: 7.323608835685253
Experience 15, Iter 58, disc loss: 0.0009651320808978967, policy loss: 7.275261071588638
Experience 15, Iter 59, disc loss: 0.000931428709316225, policy loss: 7.276539453534522
Experience 15, Iter 60, disc loss: 0.0009359862867221189, policy loss: 7.296342382917278
Experience 15, Iter 61, disc loss: 0.0009707768721511329, policy loss: 7.253601672182773
Experience 15, Iter 62, disc loss: 0.0009232969672303303, policy loss: 7.299085124041568
Experience 15, Iter 63, disc loss: 0.0008463958580948084, policy loss: 7.359973584074056
Experience 15, Iter 64, disc loss: 0.0008713316579065789, policy loss: 7.374645072693163
Experience 15, Iter 65, disc loss: 0.0009790494641405151, policy loss: 7.216389744394231
Experience 15, Iter 66, disc loss: 0.0009040062857807571, policy loss: 7.328833700333503
Experience 15, Iter 67, disc loss: 0.0008519084867343614, policy loss: 7.378492596077477
Experience 15, Iter 68, disc loss: 0.0009428525090818192, policy loss: 7.309952175085164
Experience 15, Iter 69, disc loss: 0.000771716465133816, policy loss: 7.463043846638914
Experience 15, Iter 70, disc loss: 0.0008230199371763932, policy loss: 7.409880464899398
Experience 15, Iter 71, disc loss: 0.0007351549858784853, policy loss: 7.592333925622319
Experience 15, Iter 72, disc loss: 0.0007101016223074175, policy loss: 7.652811678863244
Experience 15, Iter 73, disc loss: 0.000645714071400391, policy loss: 7.703975063660545
Experience 15, Iter 74, disc loss: 0.0006366836324195135, policy loss: 7.758426894978969
Experience 15, Iter 75, disc loss: 0.0006776096036601779, policy loss: 7.623884287513471
Experience 15, Iter 76, disc loss: 0.0006834225293937668, policy loss: 7.606479117615592
Experience 15, Iter 77, disc loss: 0.0007194101115611839, policy loss: 7.548038060451171
Experience 15, Iter 78, disc loss: 0.0007992528581251626, policy loss: 7.437704888990028
Experience 15, Iter 79, disc loss: 0.0008077563848233025, policy loss: 7.411914184504403
Experience 15, Iter 80, disc loss: 0.0008781256670437281, policy loss: 7.327846831048856
Experience 15, Iter 81, disc loss: 0.0008194579198239379, policy loss: 7.411931637083717
Experience 15, Iter 82, disc loss: 0.0009545555864998378, policy loss: 7.305882845207446
Experience 15, Iter 83, disc loss: 0.0009428663491200704, policy loss: 7.306591281744836
Experience 15, Iter 84, disc loss: 0.0008984937502263433, policy loss: 7.373224046506253
Experience 15, Iter 85, disc loss: 0.0007142690483550256, policy loss: 7.720167246401506
Experience 15, Iter 86, disc loss: 0.0009352160356682763, policy loss: 7.3358399499362745
Experience 15, Iter 87, disc loss: 0.0008884446295998902, policy loss: 7.474688027552464
Experience 15, Iter 88, disc loss: 0.0009280813167841186, policy loss: 7.403272959350537
Experience 15, Iter 89, disc loss: 0.0009666218344742727, policy loss: 7.300219678201098
Experience 15, Iter 90, disc loss: 0.0008590739236314272, policy loss: 7.555995201770987
Experience 15, Iter 91, disc loss: 0.0010250423975388648, policy loss: 7.22982056306072
Experience 15, Iter 92, disc loss: 0.0007747088046412678, policy loss: 7.616087857137531
Experience 15, Iter 93, disc loss: 0.0009441225795564567, policy loss: 7.40751079481076
Experience 15, Iter 94, disc loss: 0.0009848510411202033, policy loss: 7.237384851028598
Experience 15, Iter 95, disc loss: 0.0009298752172701407, policy loss: 7.40313557080216
Experience 15, Iter 96, disc loss: 0.0009810645763657661, policy loss: 7.2895069313231495
Experience 15, Iter 97, disc loss: 0.0008692151668513627, policy loss: 7.347624535766732
Experience 15, Iter 98, disc loss: 0.0008266262932895015, policy loss: 7.462403226299732
Experience 15, Iter 99, disc loss: 0.0008980112504383578, policy loss: 7.407658505337916
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0641],
        [0.6127],
        [0.0063]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0096, 0.0712, 0.3169, 0.0093, 0.0020, 1.6094]],

        [[0.0096, 0.0712, 0.3169, 0.0093, 0.0020, 1.6094]],

        [[0.0096, 0.0712, 0.3169, 0.0093, 0.0020, 1.6094]],

        [[0.0096, 0.0712, 0.3169, 0.0093, 0.0020, 1.6094]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0069, 0.2562, 2.4508, 0.0251], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0069, 0.2562, 2.4508, 0.0251])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.240
Iter 2/2000 - Loss: 1.511
Iter 3/2000 - Loss: 1.161
Iter 4/2000 - Loss: 1.167
Iter 5/2000 - Loss: 1.291
Iter 6/2000 - Loss: 1.210
Iter 7/2000 - Loss: 1.085
Iter 8/2000 - Loss: 1.056
Iter 9/2000 - Loss: 1.085
Iter 10/2000 - Loss: 1.071
Iter 11/2000 - Loss: 0.990
Iter 12/2000 - Loss: 0.886
Iter 13/2000 - Loss: 0.798
Iter 14/2000 - Loss: 0.727
Iter 15/2000 - Loss: 0.640
Iter 16/2000 - Loss: 0.512
Iter 17/2000 - Loss: 0.348
Iter 18/2000 - Loss: 0.167
Iter 19/2000 - Loss: -0.019
Iter 20/2000 - Loss: -0.214
Iter 1981/2000 - Loss: -8.101
Iter 1982/2000 - Loss: -8.101
Iter 1983/2000 - Loss: -8.101
Iter 1984/2000 - Loss: -8.101
Iter 1985/2000 - Loss: -8.101
Iter 1986/2000 - Loss: -8.101
Iter 1987/2000 - Loss: -8.101
Iter 1988/2000 - Loss: -8.101
Iter 1989/2000 - Loss: -8.101
Iter 1990/2000 - Loss: -8.101
Iter 1991/2000 - Loss: -8.101
Iter 1992/2000 - Loss: -8.101
Iter 1993/2000 - Loss: -8.102
Iter 1994/2000 - Loss: -8.102
Iter 1995/2000 - Loss: -8.102
Iter 1996/2000 - Loss: -8.102
Iter 1997/2000 - Loss: -8.102
Iter 1998/2000 - Loss: -8.102
Iter 1999/2000 - Loss: -8.102
Iter 2000/2000 - Loss: -8.102
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[14.1643,  6.9404, 38.3564, 10.3088, 10.7867, 44.0585]],

        [[17.0033, 26.6600, 10.5810,  1.4860,  4.7280, 32.5970]],

        [[17.3359, 29.4994, 11.0990,  1.1085,  2.1241, 15.6659]],

        [[15.1803, 25.1997, 11.6798,  3.1373,  1.0763, 38.9412]]])
Signal Variance: tensor([ 0.0882,  2.8375, 14.5708,  0.2983])
Estimated target variance: tensor([0.0069, 0.2562, 2.4508, 0.0251])
N: 160
Signal to noise ratio: tensor([15.9621, 84.9001, 85.3026, 32.3190])
Bound on condition number: tensor([  40766.9910, 1153285.3408, 1164245.2244,  167123.9168])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0009204702020961684, policy loss: 7.330648514530469
Experience 16, Iter 1, disc loss: 0.0008143126351104424, policy loss: 7.505439045815283
Experience 16, Iter 2, disc loss: 0.0009799852957075976, policy loss: 7.268368147443309
Experience 16, Iter 3, disc loss: 0.0007584556651259815, policy loss: 7.500202176673019
Experience 16, Iter 4, disc loss: 0.0008989591486670773, policy loss: 7.331747044870164
Experience 16, Iter 5, disc loss: 0.000928074636844614, policy loss: 7.24004858344813
Experience 16, Iter 6, disc loss: 0.0008083030387384149, policy loss: 7.46452588819312
Experience 16, Iter 7, disc loss: 0.0007959282408881378, policy loss: 7.489810151748739
Experience 16, Iter 8, disc loss: 0.0008654040961940032, policy loss: 7.432445849739797
Experience 16, Iter 9, disc loss: 0.0008327960753147761, policy loss: 7.433596502781492
Experience 16, Iter 10, disc loss: 0.0007737323183941139, policy loss: 7.51325715100306
Experience 16, Iter 11, disc loss: 0.0008176623366401543, policy loss: 7.430595382780809
Experience 16, Iter 12, disc loss: 0.0008456714576445887, policy loss: 7.3328172156647575
Experience 16, Iter 13, disc loss: 0.0008500838287420777, policy loss: 7.417711829555445
Experience 16, Iter 14, disc loss: 0.0007478813097998667, policy loss: 7.566151765209346
Experience 16, Iter 15, disc loss: 0.0009349368950463193, policy loss: 7.279226551751331
Experience 16, Iter 16, disc loss: 0.0008793915627774625, policy loss: 7.3273826477321
Experience 16, Iter 17, disc loss: 0.0008941699455910114, policy loss: 7.364911440157014
Experience 16, Iter 18, disc loss: 0.0008128400496662851, policy loss: 7.439659821394065
Experience 16, Iter 19, disc loss: 0.0009309294450332153, policy loss: 7.306109035572987
Experience 16, Iter 20, disc loss: 0.0009237324844290675, policy loss: 7.294036169617348
Experience 16, Iter 21, disc loss: 0.0008497379841375157, policy loss: 7.500635301027745
Experience 16, Iter 22, disc loss: 0.0007770575618285548, policy loss: 7.522305456596509
Experience 16, Iter 23, disc loss: 0.0008186172374810346, policy loss: 7.541663387118156
Experience 16, Iter 24, disc loss: 0.0008580984359903192, policy loss: 7.46495395376741
Experience 16, Iter 25, disc loss: 0.0008926497481000788, policy loss: 7.359641232349678
Experience 16, Iter 26, disc loss: 0.0008041410432236492, policy loss: 7.621050610690636
Experience 16, Iter 27, disc loss: 0.0007067104780410253, policy loss: 7.683645760816023
Experience 16, Iter 28, disc loss: 0.0008610394221148816, policy loss: 7.513945741839274
Experience 16, Iter 29, disc loss: 0.0008394636778953498, policy loss: 7.469679451713043
Experience 16, Iter 30, disc loss: 0.0007519007308159476, policy loss: 7.6873720502368785
Experience 16, Iter 31, disc loss: 0.0009186676729758649, policy loss: 7.431253443358051
Experience 16, Iter 32, disc loss: 0.0007904658870467613, policy loss: 7.535831444766405
Experience 16, Iter 33, disc loss: 0.0007835395031519722, policy loss: 7.550631763224663
Experience 16, Iter 34, disc loss: 0.0008064143126269773, policy loss: 7.486457856452402
Experience 16, Iter 35, disc loss: 0.0008819678004504413, policy loss: 7.367315974710593
Experience 16, Iter 36, disc loss: 0.0007805104326329398, policy loss: 7.512236400679882
Experience 16, Iter 37, disc loss: 0.0007716572294075232, policy loss: 7.528713923101891
Experience 16, Iter 38, disc loss: 0.0007935322208113763, policy loss: 7.494494740191222
Experience 16, Iter 39, disc loss: 0.0007667614747505294, policy loss: 7.548603685529015
Experience 16, Iter 40, disc loss: 0.0008104531688733383, policy loss: 7.462905595706974
Experience 16, Iter 41, disc loss: 0.0007397586784013732, policy loss: 7.556275630443805
Experience 16, Iter 42, disc loss: 0.0007561342166451085, policy loss: 7.5292036515109295
Experience 16, Iter 43, disc loss: 0.0007793402162353451, policy loss: 7.512339230220086
Experience 16, Iter 44, disc loss: 0.0008632549325757421, policy loss: 7.382440709365422
Experience 16, Iter 45, disc loss: 0.0007525205496977256, policy loss: 7.4909203338730626
Experience 16, Iter 46, disc loss: 0.0007407641420522663, policy loss: 7.571838388541744
Experience 16, Iter 47, disc loss: 0.0006844720903942307, policy loss: 7.5794017216980905
Experience 16, Iter 48, disc loss: 0.0007312297114840159, policy loss: 7.568423906488254
Experience 16, Iter 49, disc loss: 0.000746229104468814, policy loss: 7.5290475767346505
Experience 16, Iter 50, disc loss: 0.0007917721669896147, policy loss: 7.424826729103296
Experience 16, Iter 51, disc loss: 0.0007119338976307796, policy loss: 7.537649615985799
Experience 16, Iter 52, disc loss: 0.0006635984816398478, policy loss: 7.684454086219246
Experience 16, Iter 53, disc loss: 0.000768219227877292, policy loss: 7.419816394057851
Experience 16, Iter 54, disc loss: 0.0007953841168456584, policy loss: 7.439772174297536
Experience 16, Iter 55, disc loss: 0.0007492429384525862, policy loss: 7.4904451907708
Experience 16, Iter 56, disc loss: 0.0008446859973284455, policy loss: 7.34517684222279
Experience 16, Iter 57, disc loss: 0.0008586093810441731, policy loss: 7.329403979323683
Experience 16, Iter 58, disc loss: 0.0007484023294346165, policy loss: 7.4937839610311245
Experience 16, Iter 59, disc loss: 0.0006724601045278704, policy loss: 7.592357993685639
Experience 16, Iter 60, disc loss: 0.0007288137529707371, policy loss: 7.517159720752825
Experience 16, Iter 61, disc loss: 0.0007384214043631678, policy loss: 7.473850022732222
Experience 16, Iter 62, disc loss: 0.0007325396255844059, policy loss: 7.54423977348883
Experience 16, Iter 63, disc loss: 0.0007498455632269244, policy loss: 7.543578960731036
Experience 16, Iter 64, disc loss: 0.0007725645116640633, policy loss: 7.431889940978939
Experience 16, Iter 65, disc loss: 0.0007591596143254915, policy loss: 7.4874369144275645
Experience 16, Iter 66, disc loss: 0.000729128129207777, policy loss: 7.574239805259154
Experience 16, Iter 67, disc loss: 0.0008053691364773562, policy loss: 7.398472779280642
Experience 16, Iter 68, disc loss: 0.0007342488209769234, policy loss: 7.530947991195973
Experience 16, Iter 69, disc loss: 0.000756401160432109, policy loss: 7.581670530289363
Experience 16, Iter 70, disc loss: 0.0007248159439883606, policy loss: 7.529117895401956
Experience 16, Iter 71, disc loss: 0.0007465167703337042, policy loss: 7.525049326317681
Experience 16, Iter 72, disc loss: 0.0007198720182982949, policy loss: 7.527937410463458
Experience 16, Iter 73, disc loss: 0.0007242730573683899, policy loss: 7.553281891121163
Experience 16, Iter 74, disc loss: 0.0007419862021212226, policy loss: 7.509138609273562
Experience 16, Iter 75, disc loss: 0.000746668737265936, policy loss: 7.5721087259589455
Experience 16, Iter 76, disc loss: 0.0007695860194975631, policy loss: 7.478129499542558
Experience 16, Iter 77, disc loss: 0.0008158609485578079, policy loss: 7.386196599267134
Experience 16, Iter 78, disc loss: 0.0007155467587993227, policy loss: 7.556953213301695
Experience 16, Iter 79, disc loss: 0.0007680305814237391, policy loss: 7.449511134289379
Experience 16, Iter 80, disc loss: 0.0007805545663578063, policy loss: 7.469533794378664
Experience 16, Iter 81, disc loss: 0.0007792711496466371, policy loss: 7.489658700753873
Experience 16, Iter 82, disc loss: 0.0007285575956983426, policy loss: 7.578294268303345
Experience 16, Iter 83, disc loss: 0.0007426305905836432, policy loss: 7.5549463034904445
Experience 16, Iter 84, disc loss: 0.0007780241828252752, policy loss: 7.52902845653364
Experience 16, Iter 85, disc loss: 0.0008609673037728251, policy loss: 7.347589810253295
Experience 16, Iter 86, disc loss: 0.0007013833911887137, policy loss: 7.690302540801387
Experience 16, Iter 87, disc loss: 0.000674975862615511, policy loss: 7.671001129309168
Experience 16, Iter 88, disc loss: 0.0006943172336134347, policy loss: 7.6300775253902104
Experience 16, Iter 89, disc loss: 0.0007770332282357946, policy loss: 7.451793360027473
Experience 16, Iter 90, disc loss: 0.0007036466204157892, policy loss: 7.518315164152439
Experience 16, Iter 91, disc loss: 0.0006367121392614321, policy loss: 7.663263864231917
Experience 16, Iter 92, disc loss: 0.0006343160717358899, policy loss: 7.660202383275205
Experience 16, Iter 93, disc loss: 0.0005777969920953824, policy loss: 7.7646854680984205
Experience 16, Iter 94, disc loss: 0.0005936716308192175, policy loss: 7.762483238595225
Experience 16, Iter 95, disc loss: 0.0005597287125246274, policy loss: 7.823198126391484
Experience 16, Iter 96, disc loss: 0.00047177503174981015, policy loss: 8.102230936109873
Experience 16, Iter 97, disc loss: 0.0003406789025893993, policy loss: 8.62580612027321
Experience 16, Iter 98, disc loss: 0.0003085437927733467, policy loss: 8.895031636581958
Experience 16, Iter 99, disc loss: 0.00027291728178326487, policy loss: 9.095428950680045
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0610],
        [0.5769],
        [0.0059]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0093, 0.0714, 0.2984, 0.0091, 0.0019, 1.5509]],

        [[0.0093, 0.0714, 0.2984, 0.0091, 0.0019, 1.5509]],

        [[0.0093, 0.0714, 0.2984, 0.0091, 0.0019, 1.5509]],

        [[0.0093, 0.0714, 0.2984, 0.0091, 0.0019, 1.5509]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0070, 0.2441, 2.3075, 0.0236], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0070, 0.2441, 2.3075, 0.0236])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.147
Iter 2/2000 - Loss: 1.486
Iter 3/2000 - Loss: 1.086
Iter 4/2000 - Loss: 1.109
Iter 5/2000 - Loss: 1.247
Iter 6/2000 - Loss: 1.160
Iter 7/2000 - Loss: 1.027
Iter 8/2000 - Loss: 0.996
Iter 9/2000 - Loss: 1.029
Iter 10/2000 - Loss: 1.028
Iter 11/2000 - Loss: 0.959
Iter 12/2000 - Loss: 0.859
Iter 13/2000 - Loss: 0.769
Iter 14/2000 - Loss: 0.697
Iter 15/2000 - Loss: 0.620
Iter 16/2000 - Loss: 0.513
Iter 17/2000 - Loss: 0.367
Iter 18/2000 - Loss: 0.192
Iter 19/2000 - Loss: 0.005
Iter 20/2000 - Loss: -0.189
Iter 1981/2000 - Loss: -8.206
Iter 1982/2000 - Loss: -8.206
Iter 1983/2000 - Loss: -8.206
Iter 1984/2000 - Loss: -8.206
Iter 1985/2000 - Loss: -8.206
Iter 1986/2000 - Loss: -8.206
Iter 1987/2000 - Loss: -8.206
Iter 1988/2000 - Loss: -8.206
Iter 1989/2000 - Loss: -8.206
Iter 1990/2000 - Loss: -8.206
Iter 1991/2000 - Loss: -8.207
Iter 1992/2000 - Loss: -8.207
Iter 1993/2000 - Loss: -8.207
Iter 1994/2000 - Loss: -8.207
Iter 1995/2000 - Loss: -8.207
Iter 1996/2000 - Loss: -8.207
Iter 1997/2000 - Loss: -8.207
Iter 1998/2000 - Loss: -8.207
Iter 1999/2000 - Loss: -8.207
Iter 2000/2000 - Loss: -8.207
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.9867,  6.8665, 37.5551, 10.4387, 10.7860, 43.6253]],

        [[16.5456, 27.6173, 10.7437,  1.4844,  4.5016, 33.5329]],

        [[17.0048, 28.7218, 10.9738,  1.1168,  2.1095, 15.4305]],

        [[15.0508, 24.8818, 11.5106,  3.0802,  1.0262, 39.1786]]])
Signal Variance: tensor([ 0.0856,  2.9525, 14.3909,  0.2894])
Estimated target variance: tensor([0.0070, 0.2441, 2.3075, 0.0236])
N: 170
Signal to noise ratio: tensor([15.5906, 87.3459, 86.9746, 32.6706])
Bound on condition number: tensor([  41322.5113, 1296984.3875, 1285980.5265,  181453.0354])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.00020907696875408936, policy loss: 9.526799905454006
Experience 17, Iter 1, disc loss: 0.00017415457706842428, policy loss: 10.113043337069929
Experience 17, Iter 2, disc loss: 0.00016256799785163565, policy loss: 10.033264308007386
Experience 17, Iter 3, disc loss: 0.00016607031211043213, policy loss: 10.121099100656643
Experience 17, Iter 4, disc loss: 0.000145550118701018, policy loss: 10.175112314479025
Experience 17, Iter 5, disc loss: 0.00015637036707144515, policy loss: 10.065310284029447
Experience 17, Iter 6, disc loss: 0.0001448901835878351, policy loss: 10.011060961191493
Experience 17, Iter 7, disc loss: 0.00013402278193446488, policy loss: 10.246228421649834
Experience 17, Iter 8, disc loss: 0.0001447030352036069, policy loss: 9.997820636477297
Experience 17, Iter 9, disc loss: 0.00014215839256128082, policy loss: 10.092960892404498
Experience 17, Iter 10, disc loss: 0.00015591047863362808, policy loss: 9.891247495416216
Experience 17, Iter 11, disc loss: 0.00016799866684991135, policy loss: 9.750112475794822
Experience 17, Iter 12, disc loss: 0.00016779444740235515, policy loss: 9.737360122025347
Experience 17, Iter 13, disc loss: 0.00017933445758799664, policy loss: 9.482402849653324
Experience 17, Iter 14, disc loss: 0.0002066265127571872, policy loss: 9.350686907377417
Experience 17, Iter 15, disc loss: 0.0002206500491501849, policy loss: 9.187842717241486
Experience 17, Iter 16, disc loss: 0.000264311638867682, policy loss: 8.870184962026574
Experience 17, Iter 17, disc loss: 0.0002370100430475007, policy loss: 8.97666693634536
Experience 17, Iter 18, disc loss: 0.0002670057574358206, policy loss: 8.855661232349021
Experience 17, Iter 19, disc loss: 0.00027521905906190494, policy loss: 8.709039264568112
Experience 17, Iter 20, disc loss: 0.0002826843412716523, policy loss: 8.6676878060303
Experience 17, Iter 21, disc loss: 0.00030620357489092714, policy loss: 8.630999804840574
Experience 17, Iter 22, disc loss: 0.00029721482521008686, policy loss: 8.604944616424179
Experience 17, Iter 23, disc loss: 0.0003019563873924456, policy loss: 8.640022823436102
Experience 17, Iter 24, disc loss: 0.00032385829332779977, policy loss: 8.500157633849135
Experience 17, Iter 25, disc loss: 0.00035544126808038635, policy loss: 8.399131088358125
Experience 17, Iter 26, disc loss: 0.0003855418387436531, policy loss: 8.265992672710656
Experience 17, Iter 27, disc loss: 0.0004191193941724877, policy loss: 8.183122576243015
Experience 17, Iter 28, disc loss: 0.00047687617382035177, policy loss: 8.001803895376977
Experience 17, Iter 29, disc loss: 0.000526154013642103, policy loss: 7.867750023883534
Experience 17, Iter 30, disc loss: 0.0006734357979354196, policy loss: 7.629488373286231
Experience 17, Iter 31, disc loss: 0.0006128756002563428, policy loss: 7.6288873306598175
Experience 17, Iter 32, disc loss: 0.0006905500898692177, policy loss: 7.5261614183561445
Experience 17, Iter 33, disc loss: 0.0006223631805446239, policy loss: 7.65707342877705
Experience 17, Iter 34, disc loss: 0.0005772016844084938, policy loss: 7.706065797696325
Experience 17, Iter 35, disc loss: 0.0006730723106074468, policy loss: 7.618275097766668
Experience 17, Iter 36, disc loss: 0.0006121272006990293, policy loss: 7.65434434863344
Experience 17, Iter 37, disc loss: 0.0006308196073643348, policy loss: 7.61679808450956
Experience 17, Iter 38, disc loss: 0.0006125704190579447, policy loss: 7.6714488463110015
Experience 17, Iter 39, disc loss: 0.000714885302182459, policy loss: 7.524308080138351
Experience 17, Iter 40, disc loss: 0.0006206634675555881, policy loss: 7.65209740262313
Experience 17, Iter 41, disc loss: 0.0006563766112032877, policy loss: 7.653770921384569
Experience 17, Iter 42, disc loss: 0.000626018075834557, policy loss: 7.656350122027363
Experience 17, Iter 43, disc loss: 0.0006154147707294767, policy loss: 7.697128649771533
Experience 17, Iter 44, disc loss: 0.0006913517238386789, policy loss: 7.596288699647294
Experience 17, Iter 45, disc loss: 0.0007012295289307607, policy loss: 7.559399498035338
Experience 17, Iter 46, disc loss: 0.0006645510787126861, policy loss: 7.61676193396141
Experience 17, Iter 47, disc loss: 0.0006681551756667008, policy loss: 7.602038416616544
Experience 17, Iter 48, disc loss: 0.0006185446907127659, policy loss: 7.672320234308451
Experience 17, Iter 49, disc loss: 0.0006422395390085658, policy loss: 7.646587471614046
Experience 17, Iter 50, disc loss: 0.0006938230706715012, policy loss: 7.558185055626547
Experience 17, Iter 51, disc loss: 0.0006951192595553726, policy loss: 7.607109654602969
Experience 17, Iter 52, disc loss: 0.0006596574670007747, policy loss: 7.593241601490535
Experience 17, Iter 53, disc loss: 0.0006673212506419693, policy loss: 7.576604318504562
Experience 17, Iter 54, disc loss: 0.0006843550057311497, policy loss: 7.590394636936479
Experience 17, Iter 55, disc loss: 0.0006808920950274588, policy loss: 7.603717990565339
Experience 17, Iter 56, disc loss: 0.0005358313710086752, policy loss: 7.950868642669452
Experience 17, Iter 57, disc loss: 0.0005999224146659664, policy loss: 7.862815225246847
Experience 17, Iter 58, disc loss: 0.0005418377107356554, policy loss: 7.992094236328608
Experience 17, Iter 59, disc loss: 0.0004876266932925762, policy loss: 8.102375328678075
Experience 17, Iter 60, disc loss: 0.0005523337099582936, policy loss: 7.985887339350457
Experience 17, Iter 61, disc loss: 0.0005303469321003935, policy loss: 7.925045394227139
Experience 17, Iter 62, disc loss: 0.0005506602787427401, policy loss: 7.974591016104357
Experience 17, Iter 63, disc loss: 0.0005956376421430596, policy loss: 7.858765434236549
Experience 17, Iter 64, disc loss: 0.0006249085822022115, policy loss: 7.850406410820895
Experience 17, Iter 65, disc loss: 0.0006631987262210678, policy loss: 7.742923596559562
Experience 17, Iter 66, disc loss: 0.0006554934709734967, policy loss: 7.80549389363829
Experience 17, Iter 67, disc loss: 0.0007074691619749553, policy loss: 7.630012540637502
Experience 17, Iter 68, disc loss: 0.0006607145078656334, policy loss: 7.707226075195816
Experience 17, Iter 69, disc loss: 0.0007559128911031282, policy loss: 7.602262953961928
Experience 17, Iter 70, disc loss: 0.0007549803253462318, policy loss: 7.609201765003784
Experience 17, Iter 71, disc loss: 0.000543579445973288, policy loss: 7.930676337452089
Experience 17, Iter 72, disc loss: 0.0005006747822524911, policy loss: 8.071905339520825
Experience 17, Iter 73, disc loss: 0.0004235376755923427, policy loss: 8.24997436949019
Experience 17, Iter 74, disc loss: 0.0004358713630540126, policy loss: 8.204907271092527
Experience 17, Iter 75, disc loss: 0.0004191022896677021, policy loss: 8.283665413558175
Experience 17, Iter 76, disc loss: 0.0004671012460312386, policy loss: 8.214209078494015
Experience 17, Iter 77, disc loss: 0.0003849002019310586, policy loss: 8.336470373790956
Experience 17, Iter 78, disc loss: 0.0004061468714247939, policy loss: 8.285227720422048
Experience 17, Iter 79, disc loss: 0.00041109936405949273, policy loss: 8.270726028677483
Experience 17, Iter 80, disc loss: 0.0003873999994813979, policy loss: 8.313851164291414
Experience 17, Iter 81, disc loss: 0.0004747796400375071, policy loss: 8.13887891251011
Experience 17, Iter 82, disc loss: 0.000453697971254091, policy loss: 8.160407826013909
Experience 17, Iter 83, disc loss: 0.0004704329498045422, policy loss: 8.128132110083234
Experience 17, Iter 84, disc loss: 0.000489522784035343, policy loss: 8.080878266486835
Experience 17, Iter 85, disc loss: 0.00046673026133707736, policy loss: 8.148038869350597
Experience 17, Iter 86, disc loss: 0.0005576838673279084, policy loss: 7.983400848620725
Experience 17, Iter 87, disc loss: 0.0005419340432731858, policy loss: 7.955254655218631
Experience 17, Iter 88, disc loss: 0.0005743780879710593, policy loss: 7.873469781911929
Experience 17, Iter 89, disc loss: 0.0007141798863562767, policy loss: 7.6237544849125545
Experience 17, Iter 90, disc loss: 0.0006178496980816148, policy loss: 7.777845032854176
Experience 17, Iter 91, disc loss: 0.000814801953937011, policy loss: 7.447129010692831
Experience 17, Iter 92, disc loss: 0.0008397912003485539, policy loss: 7.453997347217589
Experience 17, Iter 93, disc loss: 0.0009574518471593728, policy loss: 7.289209900815225
Experience 17, Iter 94, disc loss: 0.0008165595442957852, policy loss: 7.469737636234811
Experience 17, Iter 95, disc loss: 0.0008311129070328819, policy loss: 7.432872034015214
Experience 17, Iter 96, disc loss: 0.0008405247926519081, policy loss: 7.441204372420589
Experience 17, Iter 97, disc loss: 0.0008444632490065212, policy loss: 7.458197991999361
Experience 17, Iter 98, disc loss: 0.0008786727805374845, policy loss: 7.434718449586495
Experience 17, Iter 99, disc loss: 0.0009092342469601708, policy loss: 7.420174119856045
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0615],
        [0.5858],
        [0.0063]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0091, 0.0701, 0.3164, 0.0096, 0.0018, 1.5893]],

        [[0.0091, 0.0701, 0.3164, 0.0096, 0.0018, 1.5893]],

        [[0.0091, 0.0701, 0.3164, 0.0096, 0.0018, 1.5893]],

        [[0.0091, 0.0701, 0.3164, 0.0096, 0.0018, 1.5893]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0069, 0.2459, 2.3433, 0.0253], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0069, 0.2459, 2.3433, 0.0253])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.177
Iter 2/2000 - Loss: 1.526
Iter 3/2000 - Loss: 1.112
Iter 4/2000 - Loss: 1.136
Iter 5/2000 - Loss: 1.277
Iter 6/2000 - Loss: 1.189
Iter 7/2000 - Loss: 1.049
Iter 8/2000 - Loss: 1.008
Iter 9/2000 - Loss: 1.040
Iter 10/2000 - Loss: 1.042
Iter 11/2000 - Loss: 0.976
Iter 12/2000 - Loss: 0.871
Iter 13/2000 - Loss: 0.771
Iter 14/2000 - Loss: 0.689
Iter 15/2000 - Loss: 0.608
Iter 16/2000 - Loss: 0.501
Iter 17/2000 - Loss: 0.355
Iter 18/2000 - Loss: 0.177
Iter 19/2000 - Loss: -0.019
Iter 20/2000 - Loss: -0.222
Iter 1981/2000 - Loss: -8.257
Iter 1982/2000 - Loss: -8.257
Iter 1983/2000 - Loss: -8.257
Iter 1984/2000 - Loss: -8.257
Iter 1985/2000 - Loss: -8.257
Iter 1986/2000 - Loss: -8.257
Iter 1987/2000 - Loss: -8.257
Iter 1988/2000 - Loss: -8.257
Iter 1989/2000 - Loss: -8.257
Iter 1990/2000 - Loss: -8.257
Iter 1991/2000 - Loss: -8.257
Iter 1992/2000 - Loss: -8.257
Iter 1993/2000 - Loss: -8.258
Iter 1994/2000 - Loss: -8.258
Iter 1995/2000 - Loss: -8.258
Iter 1996/2000 - Loss: -8.258
Iter 1997/2000 - Loss: -8.258
Iter 1998/2000 - Loss: -8.258
Iter 1999/2000 - Loss: -8.258
Iter 2000/2000 - Loss: -8.258
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.8073,  6.7950, 37.1313, 10.4929,  9.5426, 40.8916]],

        [[16.6090, 27.0507, 11.2707,  1.5094,  5.0430, 33.6897]],

        [[17.0056, 28.4404, 11.0810,  1.1311,  1.9799, 17.0664]],

        [[14.6750, 24.5881, 12.3290,  3.1129,  1.1828, 38.2931]]])
Signal Variance: tensor([ 0.0811,  3.1372, 15.1074,  0.3109])
Estimated target variance: tensor([0.0069, 0.2459, 2.3433, 0.0253])
N: 180
Signal to noise ratio: tensor([15.0474, 89.2704, 88.8171, 34.2698])
Bound on condition number: tensor([  40757.5028, 1434456.5194, 1419926.5184,  211396.3437])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0009979667299374647, policy loss: 7.293504942562871
Experience 18, Iter 1, disc loss: 0.0008960104944729064, policy loss: 7.498916425023095
Experience 18, Iter 2, disc loss: 0.000897994637020315, policy loss: 7.381223270721845
Experience 18, Iter 3, disc loss: 0.0009243154461536855, policy loss: 7.403468393660381
Experience 18, Iter 4, disc loss: 0.0010245863227150529, policy loss: 7.270977381836131
Experience 18, Iter 5, disc loss: 0.0006903668788782912, policy loss: 7.708181427459052
Experience 18, Iter 6, disc loss: 0.0007672971816865801, policy loss: 7.668310936148477
Experience 18, Iter 7, disc loss: 0.0008824975208175721, policy loss: 7.49633353925844
Experience 18, Iter 8, disc loss: 0.001014790553881317, policy loss: 7.3058311815670045
Experience 18, Iter 9, disc loss: 0.0009185514297406351, policy loss: 7.496498810947268
Experience 18, Iter 10, disc loss: 0.0008894499856127067, policy loss: 7.42181176993983
Experience 18, Iter 11, disc loss: 0.0011090732372138108, policy loss: 7.163401520894225
Experience 18, Iter 12, disc loss: 0.0010035943002657595, policy loss: 7.406469340039698
Experience 18, Iter 13, disc loss: 0.0010381435674731276, policy loss: 7.266364562189587
Experience 18, Iter 14, disc loss: 0.0010496173098163276, policy loss: 7.322657565774136
Experience 18, Iter 15, disc loss: 0.001006599923468133, policy loss: 7.34927879413986
Experience 18, Iter 16, disc loss: 0.0008381428794193598, policy loss: 7.6323585419746856
Experience 18, Iter 17, disc loss: 0.0009234928678265774, policy loss: 7.536752568204503
Experience 18, Iter 18, disc loss: 0.0007835973971623516, policy loss: 7.670683639736687
Experience 18, Iter 19, disc loss: 0.0006896760873789783, policy loss: 7.871894467095343
Experience 18, Iter 20, disc loss: 0.000765976088954447, policy loss: 7.762034849686939
Experience 18, Iter 21, disc loss: 0.001012909887276178, policy loss: 7.470630701824648
Experience 18, Iter 22, disc loss: 0.001036508775611303, policy loss: 7.414087345136107
Experience 18, Iter 23, disc loss: 0.0014179897332100184, policy loss: 6.945406010952727
Experience 18, Iter 24, disc loss: 0.0013215565323132289, policy loss: 6.9933410158173865
Experience 18, Iter 25, disc loss: 0.0013704100712714747, policy loss: 6.933359990550982
Experience 18, Iter 26, disc loss: 0.0013716734297981414, policy loss: 7.034915967729643
Experience 18, Iter 27, disc loss: 0.0014333364274661566, policy loss: 7.001591365127846
Experience 18, Iter 28, disc loss: 0.0016411076478883443, policy loss: 6.723197467889943
Experience 18, Iter 29, disc loss: 0.001592343178968875, policy loss: 6.760558830600307
Experience 18, Iter 30, disc loss: 0.0017171841743626767, policy loss: 6.768835241090154
Experience 18, Iter 31, disc loss: 0.00155148122175671, policy loss: 6.870800213161633
Experience 18, Iter 32, disc loss: 0.001061402559996739, policy loss: 7.253021496317942
Experience 18, Iter 33, disc loss: 0.0011681073450729982, policy loss: 7.238877552444282
Experience 18, Iter 34, disc loss: 0.001130213758647601, policy loss: 7.201212586939116
Experience 18, Iter 35, disc loss: 0.000964510830883214, policy loss: 7.389200298016488
Experience 18, Iter 36, disc loss: 0.001194702540823738, policy loss: 7.12240406532388
Experience 18, Iter 37, disc loss: 0.0011809940760336862, policy loss: 7.128179524549147
Experience 18, Iter 38, disc loss: 0.0014098187003716027, policy loss: 7.090318393891995
Experience 18, Iter 39, disc loss: 0.0022238115815287856, policy loss: 6.555974379040208
Experience 18, Iter 40, disc loss: 0.002276538057702516, policy loss: 6.525630753045145
Experience 18, Iter 41, disc loss: 0.0015527478024938333, policy loss: 7.034314110466941
Experience 18, Iter 42, disc loss: 0.0018367976153754611, policy loss: 6.941386291725884
Experience 18, Iter 43, disc loss: 0.0020458902291429423, policy loss: 6.653656290582473
Experience 18, Iter 44, disc loss: 0.002193648824366459, policy loss: 6.664299973488126
Experience 18, Iter 45, disc loss: 0.0025606198734224018, policy loss: 6.274875017358195
Experience 18, Iter 46, disc loss: 0.0026984971346959894, policy loss: 6.275360709075204
Experience 18, Iter 47, disc loss: 0.0020673471649627004, policy loss: 6.631612752208866
Experience 18, Iter 48, disc loss: 0.0023471150273257036, policy loss: 6.488225866047987
Experience 18, Iter 49, disc loss: 0.0023525006398969067, policy loss: 6.450635374644577
Experience 18, Iter 50, disc loss: 0.0023655880510967983, policy loss: 6.580370623409521
Experience 18, Iter 51, disc loss: 0.0028097456446094763, policy loss: 6.28421153521214
Experience 18, Iter 52, disc loss: 0.0022428996300923225, policy loss: 6.809040236998838
Experience 18, Iter 53, disc loss: 0.0019841602028615145, policy loss: 6.815047284225678
Experience 18, Iter 54, disc loss: 0.0019429644857876865, policy loss: 6.87382338695153
Experience 18, Iter 55, disc loss: 0.0024723964643717966, policy loss: 6.405541591128379
Experience 18, Iter 56, disc loss: 0.0020626787030001864, policy loss: 6.540109775430451
Experience 18, Iter 57, disc loss: 0.0016962539716455123, policy loss: 6.870284534440188
Experience 18, Iter 58, disc loss: 0.0015155629766341282, policy loss: 6.9591456632061295
Experience 18, Iter 59, disc loss: 0.0016280388781094238, policy loss: 6.923042357780853
Experience 18, Iter 60, disc loss: 0.0017193932730739775, policy loss: 6.82985267707187
Experience 18, Iter 61, disc loss: 0.0018191134831302513, policy loss: 6.705527036387805
Experience 18, Iter 62, disc loss: 0.0017973420685080772, policy loss: 6.838421585680879
Experience 18, Iter 63, disc loss: 0.0017235020950368942, policy loss: 6.902755944349738
Experience 18, Iter 64, disc loss: 0.0017238507267009137, policy loss: 6.918744457354066
Experience 18, Iter 65, disc loss: 0.001501018505728723, policy loss: 7.138467183718375
Experience 18, Iter 66, disc loss: 0.001852932786366855, policy loss: 6.8408102954596455
Experience 18, Iter 67, disc loss: 0.0015956895365230443, policy loss: 7.040102676291571
Experience 18, Iter 68, disc loss: 0.0017008949262617271, policy loss: 6.913320800581521
Experience 18, Iter 69, disc loss: 0.0017688215579594953, policy loss: 6.925105573257494
Experience 18, Iter 70, disc loss: 0.0017715787884544937, policy loss: 6.958036785802553
Experience 18, Iter 71, disc loss: 0.0016043443122439104, policy loss: 7.12463467503505
Experience 18, Iter 72, disc loss: 0.001734920937172054, policy loss: 6.889284870871885
Experience 18, Iter 73, disc loss: 0.0016175344874913063, policy loss: 7.228687833488416
Experience 18, Iter 74, disc loss: 0.0017932277094686144, policy loss: 6.907864243141988
Experience 18, Iter 75, disc loss: 0.0017768765236938966, policy loss: 7.072262823770885
Experience 18, Iter 76, disc loss: 0.0015385379049975515, policy loss: 7.3606952932607195
Experience 18, Iter 77, disc loss: 0.0016593296956787342, policy loss: 7.04436843470126
Experience 18, Iter 78, disc loss: 0.001652905052113897, policy loss: 7.071883250733036
Experience 18, Iter 79, disc loss: 0.0017108866919699655, policy loss: 6.993856066089993
Experience 18, Iter 80, disc loss: 0.0015984523437966756, policy loss: 7.118898541277259
Experience 18, Iter 81, disc loss: 0.0017144886245462375, policy loss: 7.025864992262784
Experience 18, Iter 82, disc loss: 0.0017209401597934704, policy loss: 7.133617163127899
Experience 18, Iter 83, disc loss: 0.0017557632391204127, policy loss: 7.036164263854513
Experience 18, Iter 84, disc loss: 0.0015836411045531606, policy loss: 7.256503540990007
Experience 18, Iter 85, disc loss: 0.0014985987222458207, policy loss: 7.293782787761485
Experience 18, Iter 86, disc loss: 0.0015603109009346315, policy loss: 7.257095855713759
Experience 18, Iter 87, disc loss: 0.0015894264348617162, policy loss: 7.220646469032507
Experience 18, Iter 88, disc loss: 0.0015786046234151827, policy loss: 7.302232211659012
Experience 18, Iter 89, disc loss: 0.0013882704252956267, policy loss: 7.571034625561582
Experience 18, Iter 90, disc loss: 0.0015479658953994895, policy loss: 7.252305946964056
Experience 18, Iter 91, disc loss: 0.0015226692179237012, policy loss: 7.306562384352633
Experience 18, Iter 92, disc loss: 0.0017732105133169067, policy loss: 6.952512313533674
Experience 18, Iter 93, disc loss: 0.0015633770612221939, policy loss: 7.318851732184277
Experience 18, Iter 94, disc loss: 0.0015637793900850662, policy loss: 7.321596035795949
Experience 18, Iter 95, disc loss: 0.001492249086644292, policy loss: 7.396450882887487
Experience 18, Iter 96, disc loss: 0.0015073322105270985, policy loss: 7.403823001360908
Experience 18, Iter 97, disc loss: 0.0014423328914288133, policy loss: 7.4481694093148185
Experience 18, Iter 98, disc loss: 0.0015664332553709977, policy loss: 7.244403679407183
Experience 18, Iter 99, disc loss: 0.0016059481225567324, policy loss: 7.1835385710964985
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0715],
        [0.6795],
        [0.0072]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0092, 0.0724, 0.3664, 0.0107, 0.0019, 1.8181]],

        [[0.0092, 0.0724, 0.3664, 0.0107, 0.0019, 1.8181]],

        [[0.0092, 0.0724, 0.3664, 0.0107, 0.0019, 1.8181]],

        [[0.0092, 0.0724, 0.3664, 0.0107, 0.0019, 1.8181]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0071, 0.2860, 2.7179, 0.0288], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0071, 0.2860, 2.7179, 0.0288])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.392
Iter 2/2000 - Loss: 1.751
Iter 3/2000 - Loss: 1.332
Iter 4/2000 - Loss: 1.358
Iter 5/2000 - Loss: 1.498
Iter 6/2000 - Loss: 1.415
Iter 7/2000 - Loss: 1.268
Iter 8/2000 - Loss: 1.212
Iter 9/2000 - Loss: 1.233
Iter 10/2000 - Loss: 1.238
Iter 11/2000 - Loss: 1.175
Iter 12/2000 - Loss: 1.063
Iter 13/2000 - Loss: 0.947
Iter 14/2000 - Loss: 0.845
Iter 15/2000 - Loss: 0.749
Iter 16/2000 - Loss: 0.635
Iter 17/2000 - Loss: 0.484
Iter 18/2000 - Loss: 0.295
Iter 19/2000 - Loss: 0.081
Iter 20/2000 - Loss: -0.144
Iter 1981/2000 - Loss: -8.255
Iter 1982/2000 - Loss: -8.255
Iter 1983/2000 - Loss: -8.255
Iter 1984/2000 - Loss: -8.255
Iter 1985/2000 - Loss: -8.255
Iter 1986/2000 - Loss: -8.255
Iter 1987/2000 - Loss: -8.255
Iter 1988/2000 - Loss: -8.255
Iter 1989/2000 - Loss: -8.255
Iter 1990/2000 - Loss: -8.255
Iter 1991/2000 - Loss: -8.255
Iter 1992/2000 - Loss: -8.255
Iter 1993/2000 - Loss: -8.255
Iter 1994/2000 - Loss: -8.255
Iter 1995/2000 - Loss: -8.255
Iter 1996/2000 - Loss: -8.255
Iter 1997/2000 - Loss: -8.255
Iter 1998/2000 - Loss: -8.255
Iter 1999/2000 - Loss: -8.255
Iter 2000/2000 - Loss: -8.255
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.4710,  6.9359, 37.0080, 12.0862, 11.1157, 38.2136]],

        [[16.5258, 26.7993, 10.2394,  1.8413,  1.4421, 30.5375]],

        [[16.8011, 30.0308, 10.4553,  1.2043,  1.5038, 18.6514]],

        [[14.4710, 24.8147, 11.9932,  3.4664,  1.0387, 43.0124]]])
Signal Variance: tensor([ 0.0840,  2.5311, 16.8166,  0.3269])
Estimated target variance: tensor([0.0071, 0.2860, 2.7179, 0.0288])
N: 190
Signal to noise ratio: tensor([15.3694, 80.2792, 94.0221, 35.1163])
Bound on condition number: tensor([  44882.6758, 1224503.2738, 1679631.6609,  234300.9913])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0015200840530596388, policy loss: 7.17573995829133
Experience 19, Iter 1, disc loss: 0.0014229453767635354, policy loss: 7.389996508470842
Experience 19, Iter 2, disc loss: 0.0014840182735012832, policy loss: 7.32390370309029
Experience 19, Iter 3, disc loss: 0.0014254506983602826, policy loss: 7.342067397122188
Experience 19, Iter 4, disc loss: 0.0014705627124784095, policy loss: 7.332527690069275
Experience 19, Iter 5, disc loss: 0.0014940835879862805, policy loss: 7.232213216789408
Experience 19, Iter 6, disc loss: 0.0015477086365939191, policy loss: 7.1777056897501215
Experience 19, Iter 7, disc loss: 0.0014317542783998118, policy loss: 7.4202416916691
Experience 19, Iter 8, disc loss: 0.0014858898546072155, policy loss: 7.2003905290156185
Experience 19, Iter 9, disc loss: 0.0014474224582380602, policy loss: 7.3347427338622175
Experience 19, Iter 10, disc loss: 0.001500060006363399, policy loss: 7.308524573608125
Experience 19, Iter 11, disc loss: 0.0014614468508069418, policy loss: 7.304449780386373
Experience 19, Iter 12, disc loss: 0.0013495422377102454, policy loss: 7.526742995824087
Experience 19, Iter 13, disc loss: 0.001542768820979565, policy loss: 7.207786061817414
Experience 19, Iter 14, disc loss: 0.0014182643148096534, policy loss: 7.390283520815137
Experience 19, Iter 15, disc loss: 0.001496261279076385, policy loss: 7.413736749983965
Experience 19, Iter 16, disc loss: 0.0015313429738445595, policy loss: 7.223916864288315
Experience 19, Iter 17, disc loss: 0.0013986040711283048, policy loss: 7.436731749906423
Experience 19, Iter 18, disc loss: 0.001485636320448144, policy loss: 7.2516703480562
Experience 19, Iter 19, disc loss: 0.0015225364795996832, policy loss: 7.263877509001297
Experience 19, Iter 20, disc loss: 0.0014445235161659234, policy loss: 7.257386417059536
Experience 19, Iter 21, disc loss: 0.0013677053748080404, policy loss: 7.386749069089306
Experience 19, Iter 22, disc loss: 0.0014445772282219854, policy loss: 7.397517738381494
Experience 19, Iter 23, disc loss: 0.001449848995993202, policy loss: 7.251795261813423
Experience 19, Iter 24, disc loss: 0.0013784240449514484, policy loss: 7.419838333143333
Experience 19, Iter 25, disc loss: 0.001437381331209485, policy loss: 7.321851496552839
Experience 19, Iter 26, disc loss: 0.0015385791784120785, policy loss: 7.106217971913594
Experience 19, Iter 27, disc loss: 0.0013749521628473873, policy loss: 7.543331522341276
Experience 19, Iter 28, disc loss: 0.0013125912179180019, policy loss: 7.4892218144732645
Experience 19, Iter 29, disc loss: 0.0014271456720427343, policy loss: 7.377661523327061
Experience 19, Iter 30, disc loss: 0.0013621177927447556, policy loss: 7.349845006336732
Experience 19, Iter 31, disc loss: 0.001337612956049401, policy loss: 7.453079499581216
Experience 19, Iter 32, disc loss: 0.0014623433030203981, policy loss: 7.3886473936790775
Experience 19, Iter 33, disc loss: 0.0013909022324331225, policy loss: 7.42999482142377
Experience 19, Iter 34, disc loss: 0.0013777402693096394, policy loss: 7.3649499250306
Experience 19, Iter 35, disc loss: 0.0014713479247386537, policy loss: 7.203949658477947
Experience 19, Iter 36, disc loss: 0.0014266173898779247, policy loss: 7.386124265899657
Experience 19, Iter 37, disc loss: 0.0014605049583011635, policy loss: 7.280021165949429
Experience 19, Iter 38, disc loss: 0.0014557082469892216, policy loss: 7.191420444421275
Experience 19, Iter 39, disc loss: 0.0014244298029757855, policy loss: 7.254804844848783
Experience 19, Iter 40, disc loss: 0.0013633568912123162, policy loss: 7.387886399527803
Experience 19, Iter 41, disc loss: 0.001262409102753389, policy loss: 7.536938890629145
Experience 19, Iter 42, disc loss: 0.0012737826886713105, policy loss: 7.574060832082421
Experience 19, Iter 43, disc loss: 0.001396412707587914, policy loss: 7.417011394847195
Experience 19, Iter 44, disc loss: 0.0013300005687317128, policy loss: 7.414135257244151
Experience 19, Iter 45, disc loss: 0.0013082747199388532, policy loss: 7.474157924799326
Experience 19, Iter 46, disc loss: 0.001319261603511178, policy loss: 7.537218066958564
Experience 19, Iter 47, disc loss: 0.0012872181053391833, policy loss: 7.498095570312481
Experience 19, Iter 48, disc loss: 0.0013822617612775589, policy loss: 7.342909058352837
Experience 19, Iter 49, disc loss: 0.0013039462679639295, policy loss: 7.445229832322214
Experience 19, Iter 50, disc loss: 0.0012653587246249582, policy loss: 7.5260271364602795
Experience 19, Iter 51, disc loss: 0.0012464055902299518, policy loss: 7.624376238100187
Experience 19, Iter 52, disc loss: 0.0012508578638716133, policy loss: 7.477729318781892
Experience 19, Iter 53, disc loss: 0.0013209956495276206, policy loss: 7.417823831168683
Experience 19, Iter 54, disc loss: 0.0013249166958307097, policy loss: 7.531484088810172
Experience 19, Iter 55, disc loss: 0.0014123155179309914, policy loss: 7.245916914462589
Experience 19, Iter 56, disc loss: 0.0012950129450513582, policy loss: 7.516315083962955
Experience 19, Iter 57, disc loss: 0.001324793772825404, policy loss: 7.361363667181707
Experience 19, Iter 58, disc loss: 0.001359566211191249, policy loss: 7.404650641773505
Experience 19, Iter 59, disc loss: 0.0012366349016806708, policy loss: 7.494367419189539
Experience 19, Iter 60, disc loss: 0.0012325774116793116, policy loss: 7.479053122132159
Experience 19, Iter 61, disc loss: 0.001313077770855278, policy loss: 7.420842664551232
Experience 19, Iter 62, disc loss: 0.001245069326418543, policy loss: 7.48362439431666
Experience 19, Iter 63, disc loss: 0.0013147236211690793, policy loss: 7.454584385682422
Experience 19, Iter 64, disc loss: 0.0013437221266292569, policy loss: 7.368901209530247
Experience 19, Iter 65, disc loss: 0.0012354289395025758, policy loss: 7.521034793273474
Experience 19, Iter 66, disc loss: 0.0012837171828049313, policy loss: 7.422378296563291
Experience 19, Iter 67, disc loss: 0.0012682086642494313, policy loss: 7.517549753551851
Experience 19, Iter 68, disc loss: 0.0012486678686445757, policy loss: 7.658654576705839
Experience 19, Iter 69, disc loss: 0.0013532789074195478, policy loss: 7.379257242102756
Experience 19, Iter 70, disc loss: 0.0012822309641767878, policy loss: 7.457595888680268
Experience 19, Iter 71, disc loss: 0.001218905114639541, policy loss: 7.588475141675936
Experience 19, Iter 72, disc loss: 0.0012139386539610172, policy loss: 7.540716077639194
Experience 19, Iter 73, disc loss: 0.0012643938334987988, policy loss: 7.467171661905361
Experience 19, Iter 74, disc loss: 0.0012647320048426451, policy loss: 7.489990532647259
Experience 19, Iter 75, disc loss: 0.0013909343909086149, policy loss: 7.303467535332774
Experience 19, Iter 76, disc loss: 0.0012130872360914496, policy loss: 7.602095053307186
Experience 19, Iter 77, disc loss: 0.0012699823243890107, policy loss: 7.400871891474331
Experience 19, Iter 78, disc loss: 0.001228488379460632, policy loss: 7.451981904874581
Experience 19, Iter 79, disc loss: 0.00125894598411835, policy loss: 7.585912723136271
Experience 19, Iter 80, disc loss: 0.0013505088233389884, policy loss: 7.428002555332103
Experience 19, Iter 81, disc loss: 0.0011773163135460703, policy loss: 7.698615336004794
Experience 19, Iter 82, disc loss: 0.0011663822126973889, policy loss: 7.664587485956512
Experience 19, Iter 83, disc loss: 0.001127856964157762, policy loss: 7.70574311533124
Experience 19, Iter 84, disc loss: 0.0012360732023487012, policy loss: 7.611409147940401
Experience 19, Iter 85, disc loss: 0.001121380119361796, policy loss: 7.703498175872447
Experience 19, Iter 86, disc loss: 0.0011855380709973697, policy loss: 7.5535138986785615
Experience 19, Iter 87, disc loss: 0.0011015510965027607, policy loss: 7.7602455569805695
Experience 19, Iter 88, disc loss: 0.001280539964081109, policy loss: 7.407143927575497
Experience 19, Iter 89, disc loss: 0.0013079962641868522, policy loss: 7.3648329476134204
Experience 19, Iter 90, disc loss: 0.0011885213553091036, policy loss: 7.601489677486777
Experience 19, Iter 91, disc loss: 0.0010267909809285286, policy loss: 8.047909208506823
Experience 19, Iter 92, disc loss: 0.001156460323022723, policy loss: 7.67674348983344
Experience 19, Iter 93, disc loss: 0.001191368736870453, policy loss: 7.5478200451928075
Experience 19, Iter 94, disc loss: 0.0011476102915078798, policy loss: 7.70480683851042
Experience 19, Iter 95, disc loss: 0.0012291675795809386, policy loss: 7.396106294727829
Experience 19, Iter 96, disc loss: 0.0011099592643366162, policy loss: 7.701145169259387
Experience 19, Iter 97, disc loss: 0.0010481005823654056, policy loss: 7.81304645149708
Experience 19, Iter 98, disc loss: 0.0011091610308486427, policy loss: 7.624808991863418
Experience 19, Iter 99, disc loss: 0.0011017150146028724, policy loss: 7.676801178258242
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0792],
        [0.7589],
        [0.0084]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.0238e-03, 7.3958e-02, 4.2226e-01, 1.1568e-02, 1.9614e-03,
          1.9905e+00]],

        [[9.0238e-03, 7.3958e-02, 4.2226e-01, 1.1568e-02, 1.9614e-03,
          1.9905e+00]],

        [[9.0238e-03, 7.3958e-02, 4.2226e-01, 1.1568e-02, 1.9614e-03,
          1.9905e+00]],

        [[9.0238e-03, 7.3958e-02, 4.2226e-01, 1.1568e-02, 1.9614e-03,
          1.9905e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0072, 0.3169, 3.0355, 0.0338], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0072, 0.3169, 3.0355, 0.0338])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.575
Iter 2/2000 - Loss: 1.953
Iter 3/2000 - Loss: 1.521
Iter 4/2000 - Loss: 1.554
Iter 5/2000 - Loss: 1.697
Iter 6/2000 - Loss: 1.616
Iter 7/2000 - Loss: 1.467
Iter 8/2000 - Loss: 1.404
Iter 9/2000 - Loss: 1.421
Iter 10/2000 - Loss: 1.429
Iter 11/2000 - Loss: 1.373
Iter 12/2000 - Loss: 1.265
Iter 13/2000 - Loss: 1.144
Iter 14/2000 - Loss: 1.035
Iter 15/2000 - Loss: 0.934
Iter 16/2000 - Loss: 0.821
Iter 17/2000 - Loss: 0.675
Iter 18/2000 - Loss: 0.488
Iter 19/2000 - Loss: 0.273
Iter 20/2000 - Loss: 0.044
Iter 1981/2000 - Loss: -8.234
Iter 1982/2000 - Loss: -8.234
Iter 1983/2000 - Loss: -8.234
Iter 1984/2000 - Loss: -8.234
Iter 1985/2000 - Loss: -8.234
Iter 1986/2000 - Loss: -8.234
Iter 1987/2000 - Loss: -8.234
Iter 1988/2000 - Loss: -8.234
Iter 1989/2000 - Loss: -8.234
Iter 1990/2000 - Loss: -8.234
Iter 1991/2000 - Loss: -8.234
Iter 1992/2000 - Loss: -8.234
Iter 1993/2000 - Loss: -8.234
Iter 1994/2000 - Loss: -8.234
Iter 1995/2000 - Loss: -8.234
Iter 1996/2000 - Loss: -8.234
Iter 1997/2000 - Loss: -8.234
Iter 1998/2000 - Loss: -8.234
Iter 1999/2000 - Loss: -8.234
Iter 2000/2000 - Loss: -8.234
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.4383,  7.0076, 36.7171, 12.8028, 11.1198, 39.8459]],

        [[17.0957, 29.1952, 11.2318,  1.4489,  2.0700, 34.0179]],

        [[16.7563, 31.2640, 11.1204,  1.1835,  1.4814, 19.2224]],

        [[14.7625, 22.8210, 17.0694,  4.5790,  1.4357, 44.8414]]])
Signal Variance: tensor([ 0.0873,  2.9597, 17.1242,  0.5672])
Estimated target variance: tensor([0.0072, 0.3169, 3.0355, 0.0338])
N: 200
Signal to noise ratio: tensor([15.9382, 83.1810, 95.0309, 46.0543])
Bound on condition number: tensor([  50806.3681, 1383816.6021, 1806174.1888,  424201.2120])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0010451427771921435, policy loss: 7.715353760985011
Experience 20, Iter 1, disc loss: 0.000977198930969014, policy loss: 7.9901567985326984
Experience 20, Iter 2, disc loss: 0.00106587018502169, policy loss: 7.775993241414101
Experience 20, Iter 3, disc loss: 0.0009761811021256251, policy loss: 7.8953750264792895
Experience 20, Iter 4, disc loss: 0.0010030305764105636, policy loss: 7.781378696776309
Experience 20, Iter 5, disc loss: 0.001074298324374367, policy loss: 7.7224570887526784
Experience 20, Iter 6, disc loss: 0.001090467763544872, policy loss: 7.582140578837729
Experience 20, Iter 7, disc loss: 0.0010092126954376578, policy loss: 7.786746006029282
Experience 20, Iter 8, disc loss: 0.000929736949529288, policy loss: 8.019801199842881
Experience 20, Iter 9, disc loss: 0.0009496685530961944, policy loss: 7.970265767554041
Experience 20, Iter 10, disc loss: 0.0010654075664666195, policy loss: 7.6556878368035886
Experience 20, Iter 11, disc loss: 0.0010633763305867487, policy loss: 7.532288353915556
Experience 20, Iter 12, disc loss: 0.0010381088789148613, policy loss: 7.737188365248532
Experience 20, Iter 13, disc loss: 0.0010486547424529953, policy loss: 7.759028172694653
Experience 20, Iter 14, disc loss: 0.0010188890529172895, policy loss: 7.740953049397309
Experience 20, Iter 15, disc loss: 0.0010131668671423295, policy loss: 7.730399193893964
Experience 20, Iter 16, disc loss: 0.0010657377482250665, policy loss: 7.698785165008198
Experience 20, Iter 17, disc loss: 0.0010935837135636777, policy loss: 7.540510331565239
Experience 20, Iter 18, disc loss: 0.0009927578797125384, policy loss: 7.830986994345732
Experience 20, Iter 19, disc loss: 0.0009968846059936153, policy loss: 7.803579640975551
Experience 20, Iter 20, disc loss: 0.001018935086799842, policy loss: 7.7550211104493085
Experience 20, Iter 21, disc loss: 0.0010543084935931268, policy loss: 7.640060627605518
Experience 20, Iter 22, disc loss: 0.001007631377218389, policy loss: 7.769785491750426
Experience 20, Iter 23, disc loss: 0.0010200448839108897, policy loss: 7.645590994215469
Experience 20, Iter 24, disc loss: 0.0009629978499223026, policy loss: 7.975467137151785
Experience 20, Iter 25, disc loss: 0.0010398534805608028, policy loss: 7.674226280167193
Experience 20, Iter 26, disc loss: 0.0009893329385739225, policy loss: 7.801561315423534
Experience 20, Iter 27, disc loss: 0.000995666124007435, policy loss: 7.6394472874688315
Experience 20, Iter 28, disc loss: 0.0009701464844885848, policy loss: 7.7228239104745855
Experience 20, Iter 29, disc loss: 0.0009570222407070584, policy loss: 7.897364287006218
Experience 20, Iter 30, disc loss: 0.0010192791543251758, policy loss: 7.63225694191066
Experience 20, Iter 31, disc loss: 0.0010894423977495927, policy loss: 7.579616583229888
Experience 20, Iter 32, disc loss: 0.0009997503614487336, policy loss: 7.7859010093937195
Experience 20, Iter 33, disc loss: 0.0010203478554596493, policy loss: 7.657602157734013
Experience 20, Iter 34, disc loss: 0.0009793415220812, policy loss: 7.771144159358804
Experience 20, Iter 35, disc loss: 0.0009253534981870374, policy loss: 7.983790805018128
Experience 20, Iter 36, disc loss: 0.0009367649235116329, policy loss: 7.764440095313939
Experience 20, Iter 37, disc loss: 0.0010379550021512556, policy loss: 7.5773149346796815
Experience 20, Iter 38, disc loss: 0.0009307017328051756, policy loss: 7.78374401274136
Experience 20, Iter 39, disc loss: 0.0009758933559614241, policy loss: 7.7111071837115714
Experience 20, Iter 40, disc loss: 0.0009858047422996695, policy loss: 7.608809420368387
Experience 20, Iter 41, disc loss: 0.0009217007263852047, policy loss: 7.873096720625032
Experience 20, Iter 42, disc loss: 0.0008644340045551901, policy loss: 7.915605709771283
Experience 20, Iter 43, disc loss: 0.0008939243511227273, policy loss: 7.948365697535711
Experience 20, Iter 44, disc loss: 0.0009421659486387665, policy loss: 7.793252197038285
Experience 20, Iter 45, disc loss: 0.0008733490224863442, policy loss: 7.922171737120117
Experience 20, Iter 46, disc loss: 0.0009514970657552951, policy loss: 7.814603604614844
Experience 20, Iter 47, disc loss: 0.0008689494717306829, policy loss: 7.915430808939899
Experience 20, Iter 48, disc loss: 0.0010296711878561358, policy loss: 7.5527348829603875
Experience 20, Iter 49, disc loss: 0.0009506708056536523, policy loss: 7.764953926091998
Experience 20, Iter 50, disc loss: 0.0009656864024114242, policy loss: 7.822069148246245
Experience 20, Iter 51, disc loss: 0.000920426288719071, policy loss: 7.883177264503682
Experience 20, Iter 52, disc loss: 0.0009777011447166255, policy loss: 7.736543115409481
Experience 20, Iter 53, disc loss: 0.0009559657018678311, policy loss: 7.827563708847287
Experience 20, Iter 54, disc loss: 0.0009265370989096191, policy loss: 7.879126091210371
Experience 20, Iter 55, disc loss: 0.0008833612396653864, policy loss: 7.965538873905242
Experience 20, Iter 56, disc loss: 0.0009129087966008222, policy loss: 7.873851587782224
Experience 20, Iter 57, disc loss: 0.0009007759025705222, policy loss: 8.015852471112598
Experience 20, Iter 58, disc loss: 0.0009059407838483705, policy loss: 7.871653092952368
Experience 20, Iter 59, disc loss: 0.0010525913121726144, policy loss: 7.45832588429421
Experience 20, Iter 60, disc loss: 0.0009231627546746363, policy loss: 7.847628467428094
Experience 20, Iter 61, disc loss: 0.0009137633623189809, policy loss: 7.863226515719377
Experience 20, Iter 62, disc loss: 0.0009329435967090436, policy loss: 7.837178166991963
Experience 20, Iter 63, disc loss: 0.0008494093211749486, policy loss: 7.935845749421133
Experience 20, Iter 64, disc loss: 0.0008777301364487073, policy loss: 7.857641886349254
Experience 20, Iter 65, disc loss: 0.0008828823566530211, policy loss: 7.948488786453374
Experience 20, Iter 66, disc loss: 0.0008925083273091963, policy loss: 7.826716395489912
Experience 20, Iter 67, disc loss: 0.0009098377802096759, policy loss: 7.857272114794128
Experience 20, Iter 68, disc loss: 0.0008560276338026944, policy loss: 7.957018225942147
Experience 20, Iter 69, disc loss: 0.0008483815322175293, policy loss: 7.873397730056076
Experience 20, Iter 70, disc loss: 0.0009738051569745243, policy loss: 7.652059644327313
Experience 20, Iter 71, disc loss: 0.0008756839751108637, policy loss: 7.92045951210136
Experience 20, Iter 72, disc loss: 0.0008523186593145402, policy loss: 7.938049479967085
Experience 20, Iter 73, disc loss: 0.00048020533439425283, policy loss: 10.33826238325354
Experience 20, Iter 74, disc loss: 0.0006846539718299068, policy loss: 8.444597510446155
Experience 20, Iter 75, disc loss: 0.0007655969422242894, policy loss: 8.2443844608259
Experience 20, Iter 76, disc loss: 0.0008565178496534537, policy loss: 7.911197437453814
Experience 20, Iter 77, disc loss: 0.0008744703522031094, policy loss: 7.941652104961251
Experience 20, Iter 78, disc loss: 0.0007198923184119255, policy loss: 8.477646465713484
Experience 20, Iter 79, disc loss: 0.0006529946067292751, policy loss: 8.67899971791135
Experience 20, Iter 80, disc loss: 0.0007455311314814363, policy loss: 8.438802804238701
Experience 20, Iter 81, disc loss: 0.0008053974682180639, policy loss: 8.162079887191261
Experience 20, Iter 82, disc loss: 0.000836574401848337, policy loss: 8.229653620472241
Experience 20, Iter 83, disc loss: 0.0007501553653246357, policy loss: 8.240495133794857
Experience 20, Iter 84, disc loss: 0.0006587039054404508, policy loss: 8.25000467971476
Experience 20, Iter 85, disc loss: 0.0005421993663788878, policy loss: 8.814173617958424
Experience 20, Iter 86, disc loss: 0.0005582393497905466, policy loss: 8.863403874625757
Experience 20, Iter 87, disc loss: 0.00048270558959788564, policy loss: 9.16346813439315
Experience 20, Iter 88, disc loss: 0.000511793460615972, policy loss: 9.0717411836842
Experience 20, Iter 89, disc loss: 0.00047964547428657497, policy loss: 9.264492054367356
Experience 20, Iter 90, disc loss: 0.0005698479921816676, policy loss: 8.804990957331633
Experience 20, Iter 91, disc loss: 0.0005738695309623328, policy loss: 8.636009750909768
Experience 20, Iter 92, disc loss: 0.0006789855813985184, policy loss: 8.1878864582225
Experience 20, Iter 93, disc loss: 0.0006869810814600697, policy loss: 8.271224694375569
Experience 20, Iter 94, disc loss: 0.000699419520901751, policy loss: 8.184218904380392
Experience 20, Iter 95, disc loss: 0.000728669556159805, policy loss: 8.159398758506857
Experience 20, Iter 96, disc loss: 0.0007274322301057004, policy loss: 8.06791817439114
Experience 20, Iter 97, disc loss: 0.0007807089086817787, policy loss: 8.080874094220164
Experience 20, Iter 98, disc loss: 0.000763483459024483, policy loss: 8.092869170468873
Experience 20, Iter 99, disc loss: 0.0007089153466327791, policy loss: 8.182763373917432
