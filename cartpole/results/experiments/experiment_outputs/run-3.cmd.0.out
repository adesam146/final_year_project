Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0005],
        [0.0119],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0019, 0.0477, 0.0018], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0019, 0.0477, 0.0018])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.935
Iter 2/2000 - Loss: 1.596
Iter 3/2000 - Loss: -4.774
Iter 4/2000 - Loss: -4.586
Iter 5/2000 - Loss: -2.452
Iter 6/2000 - Loss: -3.086
Iter 7/2000 - Loss: -4.757
Iter 8/2000 - Loss: -5.589
Iter 9/2000 - Loss: -5.280
Iter 10/2000 - Loss: -4.591
Iter 11/2000 - Loss: -4.301
Iter 12/2000 - Loss: -4.577
Iter 13/2000 - Loss: -5.120
Iter 14/2000 - Loss: -5.544
Iter 15/2000 - Loss: -5.631
Iter 16/2000 - Loss: -5.416
Iter 17/2000 - Loss: -5.127
Iter 18/2000 - Loss: -5.010
Iter 19/2000 - Loss: -5.157
Iter 20/2000 - Loss: -5.443
Iter 1981/2000 - Loss: -6.181
Iter 1982/2000 - Loss: -6.176
Iter 1983/2000 - Loss: -6.173
Iter 1984/2000 - Loss: -6.177
Iter 1985/2000 - Loss: -6.182
Iter 1986/2000 - Loss: -6.181
Iter 1987/2000 - Loss: -6.177
Iter 1988/2000 - Loss: -6.172
Iter 1989/2000 - Loss: -6.171
Iter 1990/2000 - Loss: -6.174
Iter 1991/2000 - Loss: -6.180
Iter 1992/2000 - Loss: -6.182
Iter 1993/2000 - Loss: -6.179
Iter 1994/2000 - Loss: -6.177
Iter 1995/2000 - Loss: -6.180
Iter 1996/2000 - Loss: -6.182
Iter 1997/2000 - Loss: -6.182
Iter 1998/2000 - Loss: -6.179
Iter 1999/2000 - Loss: -6.178
Iter 2000/2000 - Loss: -6.178
***AFTER OPTIMATION***
Noise Variance: tensor([[8.3672e-05],
        [3.4060e-04],
        [8.4953e-03],
        [3.2470e-04]])
Lengthscale: tensor([[[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]],

        [[4.2831e-04, 3.2573e-03, 8.9454e-03, 3.2202e-04, 7.7266e-07,
          5.4921e-03]]])
Signal Variance: tensor([0.0003, 0.0014, 0.0344, 0.0013])
Estimated target variance: tensor([0.0005, 0.0019, 0.0477, 0.0018])
N: 10
Signal to noise ratio: tensor([1.9968, 2.0002, 2.0129, 2.0000])
Bound on condition number: tensor([40.8733, 41.0072, 41.5195, 40.9991])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2214895339879908, policy loss: 0.7139015593695931
Experience 1, Iter 1, disc loss: 1.2090223021740143, policy loss: 0.7141427437508273
Experience 1, Iter 2, disc loss: 1.1941644597407775, policy loss: 0.7167637948153296
Experience 1, Iter 3, disc loss: 1.177489452255702, policy loss: 0.7208949203304534
Experience 1, Iter 4, disc loss: 1.1669298104530335, policy loss: 0.718445549694448
Experience 1, Iter 5, disc loss: 1.1532116567539772, policy loss: 0.7192793760288805
Experience 1, Iter 6, disc loss: 1.1394037610598873, policy loss: 0.7203937690168828
Experience 1, Iter 7, disc loss: 1.1255868035941226, policy loss: 0.72179909042107
Experience 1, Iter 8, disc loss: 1.1098467249122816, policy loss: 0.7253976638032652
Experience 1, Iter 9, disc loss: 1.0979763169294072, policy loss: 0.7248659642940598
Experience 1, Iter 10, disc loss: 1.0912550386368163, policy loss: 0.7193669842254907
Experience 1, Iter 11, disc loss: 1.0739280194906802, policy loss: 0.7254259014164814
Experience 1, Iter 12, disc loss: 1.0561290004708297, policy loss: 0.7318187240233797
Experience 1, Iter 13, disc loss: 1.051576154688941, policy loss: 0.7249600695821543
Experience 1, Iter 14, disc loss: 1.0390111756212712, policy loss: 0.7263909347657576
Experience 1, Iter 15, disc loss: 1.027648427430098, policy loss: 0.7271717124100556
Experience 1, Iter 16, disc loss: 1.0051718449472413, policy loss: 0.7396700258396709
Experience 1, Iter 17, disc loss: 1.0005243760768716, policy loss: 0.7334392695287079
Experience 1, Iter 18, disc loss: 0.9834959057524542, policy loss: 0.74068281052928
Experience 1, Iter 19, disc loss: 0.9730267517537345, policy loss: 0.7412732776652172
Experience 1, Iter 20, disc loss: 0.9625451654116147, policy loss: 0.7419861038123562
Experience 1, Iter 21, disc loss: 0.9598220040559785, policy loss: 0.7346210689385633
Experience 1, Iter 22, disc loss: 0.9441211202485081, policy loss: 0.7415432044363786
Experience 1, Iter 23, disc loss: 0.9298641871672029, policy loss: 0.747184383469963
Experience 1, Iter 24, disc loss: 0.9226105263560318, policy loss: 0.7456604491385905
Experience 1, Iter 25, disc loss: 0.9151538279570939, policy loss: 0.7440097048381854
Experience 1, Iter 26, disc loss: 0.899195275687581, policy loss: 0.752279836186771
Experience 1, Iter 27, disc loss: 0.8910835795734514, policy loss: 0.7524915318186828
Experience 1, Iter 28, disc loss: 0.8947942093286191, policy loss: 0.7400993030116961
Experience 1, Iter 29, disc loss: 0.8694886227937642, policy loss: 0.7600081243906625
Experience 1, Iter 30, disc loss: 0.8619270497646141, policy loss: 0.7600942343520688
Experience 1, Iter 31, disc loss: 0.8559583858921206, policy loss: 0.7589292724309373
Experience 1, Iter 32, disc loss: 0.8526748689323189, policy loss: 0.7551170521492871
Experience 1, Iter 33, disc loss: 0.8415074166305715, policy loss: 0.7599826837052016
Experience 1, Iter 34, disc loss: 0.8333810840263693, policy loss: 0.7620527190302048
Experience 1, Iter 35, disc loss: 0.8222092147543099, policy loss: 0.7679292795848479
Experience 1, Iter 36, disc loss: 0.8198076174554304, policy loss: 0.7638877133846355
Experience 1, Iter 37, disc loss: 0.8174992507556598, policy loss: 0.7609156547687858
Experience 1, Iter 38, disc loss: 0.8078931848215436, policy loss: 0.7642347176255553
Experience 1, Iter 39, disc loss: 0.793853149687145, policy loss: 0.7738014271926199
Experience 1, Iter 40, disc loss: 0.7880500489556013, policy loss: 0.7742409027021755
Experience 1, Iter 41, disc loss: 0.7829599890034116, policy loss: 0.7750998754513341
Experience 1, Iter 42, disc loss: 0.7816488071893957, policy loss: 0.7708940189876997
Experience 1, Iter 43, disc loss: 0.7652931655046471, policy loss: 0.7834712641809434
Experience 1, Iter 44, disc loss: 0.7610543363277007, policy loss: 0.7826162383386643
Experience 1, Iter 45, disc loss: 0.7525649020846481, policy loss: 0.7868594662284689
Experience 1, Iter 46, disc loss: 0.739488496663611, policy loss: 0.796685028414381
Experience 1, Iter 47, disc loss: 0.7456227532009532, policy loss: 0.7853260478845331
Experience 1, Iter 48, disc loss: 0.7502607311922181, policy loss: 0.7747736776821315
Experience 1, Iter 49, disc loss: 0.7442178278945422, policy loss: 0.7779512634825846
Experience 1, Iter 50, disc loss: 0.7307719479681383, policy loss: 0.7876359788590301
Experience 1, Iter 51, disc loss: 0.7346278610640489, policy loss: 0.7801955801265482
Experience 1, Iter 52, disc loss: 0.7189887508370687, policy loss: 0.7930665536506375
Experience 1, Iter 53, disc loss: 0.7234402158374439, policy loss: 0.7843483324524785
Experience 1, Iter 54, disc loss: 0.7006426427504987, policy loss: 0.8067697960791111
Experience 1, Iter 55, disc loss: 0.6997192260158644, policy loss: 0.8045066092137829
Experience 1, Iter 56, disc loss: 0.692950332697914, policy loss: 0.8084333310512302
Experience 1, Iter 57, disc loss: 0.6960625851031534, policy loss: 0.8024627977983685
Experience 1, Iter 58, disc loss: 0.6895302913800175, policy loss: 0.8059989285296035
Experience 1, Iter 59, disc loss: 0.6813138624177376, policy loss: 0.812168637807513
Experience 1, Iter 60, disc loss: 0.6769943660091399, policy loss: 0.8151884174071866
Experience 1, Iter 61, disc loss: 0.6746050751979811, policy loss: 0.8138594180970964
Experience 1, Iter 62, disc loss: 0.6632031451316254, policy loss: 0.824557550460879
Experience 1, Iter 63, disc loss: 0.6654540550025386, policy loss: 0.8192468713120303
Experience 1, Iter 64, disc loss: 0.6631673651340364, policy loss: 0.8198058109146465
Experience 1, Iter 65, disc loss: 0.645800966732237, policy loss: 0.8384737453635929
Experience 1, Iter 66, disc loss: 0.645136678326913, policy loss: 0.8355771993976833
Experience 1, Iter 67, disc loss: 0.648368089704607, policy loss: 0.8298859534275678
Experience 1, Iter 68, disc loss: 0.6379833279166937, policy loss: 0.84009084311923
Experience 1, Iter 69, disc loss: 0.6353975870797175, policy loss: 0.8403301878068621
Experience 1, Iter 70, disc loss: 0.6392621750142577, policy loss: 0.8341495038721952
Experience 1, Iter 71, disc loss: 0.6386049204740337, policy loss: 0.8334556351096196
Experience 1, Iter 72, disc loss: 0.6365169828270336, policy loss: 0.8335383274943462
Experience 1, Iter 73, disc loss: 0.6278568630699812, policy loss: 0.8418603455745595
Experience 1, Iter 74, disc loss: 0.6205236544308352, policy loss: 0.8501327755565893
Experience 1, Iter 75, disc loss: 0.6030236417077367, policy loss: 0.8692524720229999
Experience 1, Iter 76, disc loss: 0.6191912389976453, policy loss: 0.8472013687795811
Experience 1, Iter 77, disc loss: 0.5953806397642771, policy loss: 0.8757848966852633
Experience 1, Iter 78, disc loss: 0.6134341158513661, policy loss: 0.8518737685357239
Experience 1, Iter 79, disc loss: 0.585926765774253, policy loss: 0.8851268941650458
Experience 1, Iter 80, disc loss: 0.5929015069117569, policy loss: 0.8742997710369997
Experience 1, Iter 81, disc loss: 0.5970913020032153, policy loss: 0.8681149361943479
Experience 1, Iter 82, disc loss: 0.5814564177427605, policy loss: 0.8869106798935136
Experience 1, Iter 83, disc loss: 0.5827007466365707, policy loss: 0.8838157874403385
Experience 1, Iter 84, disc loss: 0.5681258778269186, policy loss: 0.9013786573411406
Experience 1, Iter 85, disc loss: 0.5659833764137224, policy loss: 0.9035121701280489
Experience 1, Iter 86, disc loss: 0.5807778064965436, policy loss: 0.8824536125749306
Experience 1, Iter 87, disc loss: 0.5522218411081499, policy loss: 0.9213818242069747
Experience 1, Iter 88, disc loss: 0.5622981800837332, policy loss: 0.906809828553029
Experience 1, Iter 89, disc loss: 0.5545853324902417, policy loss: 0.9139647387342039
Experience 1, Iter 90, disc loss: 0.549954573515602, policy loss: 0.9188206356334025
Experience 1, Iter 91, disc loss: 0.5524634780069668, policy loss: 0.9161847473958256
Experience 1, Iter 92, disc loss: 0.5262440073150588, policy loss: 0.9537626219614979
Experience 1, Iter 93, disc loss: 0.5387152952567753, policy loss: 0.9350361391604438
Experience 1, Iter 94, disc loss: 0.5265861172305362, policy loss: 0.9507474399702159
Experience 1, Iter 95, disc loss: 0.5234087099537237, policy loss: 0.9526387337263631
Experience 1, Iter 96, disc loss: 0.5133590155889766, policy loss: 0.9686922298397407
Experience 1, Iter 97, disc loss: 0.5277484147560929, policy loss: 0.952912911113399
Experience 1, Iter 98, disc loss: 0.5019931315358636, policy loss: 0.9846073408707772
Experience 1, Iter 99, disc loss: 0.5070930651802462, policy loss: 0.9763732964793168
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0003],
        [0.0059],
        [0.0003]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0014, 0.0238, 0.0010], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0014, 0.0238, 0.0010])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.492
Iter 2/2000 - Loss: 1.820
Iter 3/2000 - Loss: -5.326
Iter 4/2000 - Loss: -5.420
Iter 5/2000 - Loss: -2.851
Iter 6/2000 - Loss: -3.260
Iter 7/2000 - Loss: -5.177
Iter 8/2000 - Loss: -6.297
Iter 9/2000 - Loss: -6.040
Iter 10/2000 - Loss: -5.219
Iter 11/2000 - Loss: -4.795
Iter 12/2000 - Loss: -5.047
Iter 13/2000 - Loss: -5.642
Iter 14/2000 - Loss: -6.110
Iter 15/2000 - Loss: -6.206
Iter 16/2000 - Loss: -6.001
Iter 17/2000 - Loss: -5.760
Iter 18/2000 - Loss: -5.699
Iter 19/2000 - Loss: -5.828
Iter 20/2000 - Loss: -6.004
Iter 1981/2000 - Loss: -6.794
Iter 1982/2000 - Loss: -6.795
Iter 1983/2000 - Loss: -6.795
Iter 1984/2000 - Loss: -6.795
Iter 1985/2000 - Loss: -6.795
Iter 1986/2000 - Loss: -6.795
Iter 1987/2000 - Loss: -6.795
Iter 1988/2000 - Loss: -6.795
Iter 1989/2000 - Loss: -6.795
Iter 1990/2000 - Loss: -6.795
Iter 1991/2000 - Loss: -6.795
Iter 1992/2000 - Loss: -6.795
Iter 1993/2000 - Loss: -6.795
Iter 1994/2000 - Loss: -6.795
Iter 1995/2000 - Loss: -6.795
Iter 1996/2000 - Loss: -6.795
Iter 1997/2000 - Loss: -6.795
Iter 1998/2000 - Loss: -6.795
Iter 1999/2000 - Loss: -6.795
Iter 2000/2000 - Loss: -6.795
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0003],
        [0.0045],
        [0.0002]])
Lengthscale: tensor([[[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]],

        [[1.3169e-03, 2.9785e-03, 4.4186e-03, 1.7793e-04, 5.2868e-07,
          5.1957e-03]]])
Signal Variance: tensor([0.0004, 0.0010, 0.0181, 0.0008])
Estimated target variance: tensor([0.0005, 0.0014, 0.0238, 0.0010])
N: 20
Signal to noise ratio: tensor([1.9980, 1.9999, 2.0084, 1.9993])
Bound on condition number: tensor([80.8367, 80.9928, 81.6760, 80.9414])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.5184600646172425, policy loss: 0.9564226113065228
Experience 2, Iter 1, disc loss: 0.5094053834841793, policy loss: 0.970339445087496
Experience 2, Iter 2, disc loss: 0.5043672372802794, policy loss: 0.9773722381431372
Experience 2, Iter 3, disc loss: 0.5053736877018615, policy loss: 0.9743322048790612
Experience 2, Iter 4, disc loss: 0.4976423735447686, policy loss: 0.9875635125384463
Experience 2, Iter 5, disc loss: 0.48758084521354406, policy loss: 1.003639767997739
Experience 2, Iter 6, disc loss: 0.4885233024718638, policy loss: 0.9994235969721263
Experience 2, Iter 7, disc loss: 0.47416290305186704, policy loss: 1.022950983514607
Experience 2, Iter 8, disc loss: 0.4769512717385392, policy loss: 1.0167887950243546
Experience 2, Iter 9, disc loss: 0.4757138391597988, policy loss: 1.0178533467621864
Experience 2, Iter 10, disc loss: 0.4712044721075171, policy loss: 1.0280136586911812
Experience 2, Iter 11, disc loss: 0.47738954923867144, policy loss: 1.0171152072232479
Experience 2, Iter 12, disc loss: 0.45344710257739756, policy loss: 1.0586651231971018
Experience 2, Iter 13, disc loss: 0.45438553826580586, policy loss: 1.057833050564696
Experience 2, Iter 14, disc loss: 0.4553094780387113, policy loss: 1.0530915509304446
Experience 2, Iter 15, disc loss: 0.4404333856714239, policy loss: 1.0821259784727313
Experience 2, Iter 16, disc loss: 0.4328975755219828, policy loss: 1.097464393902957
Experience 2, Iter 17, disc loss: 0.43726020889106976, policy loss: 1.0856997234496775
Experience 2, Iter 18, disc loss: 0.4265448211605271, policy loss: 1.1043295508822968
Experience 2, Iter 19, disc loss: 0.42278239647894394, policy loss: 1.115775012130452
Experience 2, Iter 20, disc loss: 0.40740953500153765, policy loss: 1.1442808909503746
Experience 2, Iter 21, disc loss: 0.39966621487935267, policy loss: 1.1646422346286205
Experience 2, Iter 22, disc loss: 0.41345968586287396, policy loss: 1.131627053962307
Experience 2, Iter 23, disc loss: 0.3902743785337964, policy loss: 1.178364388688412
Experience 2, Iter 24, disc loss: 0.4103810531788531, policy loss: 1.1355584989146883
Experience 2, Iter 25, disc loss: 0.37606309225628043, policy loss: 1.2102714932413376
Experience 2, Iter 26, disc loss: 0.37791303422160916, policy loss: 1.2084239193093924
Experience 2, Iter 27, disc loss: 0.3923437128054567, policy loss: 1.177312132892642
Experience 2, Iter 28, disc loss: 0.3592866626305201, policy loss: 1.2551934736273762
Experience 2, Iter 29, disc loss: 0.38537747409464335, policy loss: 1.1900520796130585
Experience 2, Iter 30, disc loss: 0.3691804990379641, policy loss: 1.2282326502483676
Experience 2, Iter 31, disc loss: 0.3595547174937607, policy loss: 1.2508518299177633
Experience 2, Iter 32, disc loss: 0.35390413089971184, policy loss: 1.2624347241783975
Experience 2, Iter 33, disc loss: 0.35346567368513265, policy loss: 1.266577215831856
Experience 2, Iter 34, disc loss: 0.34274277649225743, policy loss: 1.2911711389270768
Experience 2, Iter 35, disc loss: 0.34967540107433664, policy loss: 1.2859422292911917
Experience 2, Iter 36, disc loss: 0.3110327540726283, policy loss: 1.3829586684650594
Experience 2, Iter 37, disc loss: 0.3175880436415844, policy loss: 1.3669599538871104
Experience 2, Iter 38, disc loss: 0.3043478409366916, policy loss: 1.3976574971623434
Experience 2, Iter 39, disc loss: 0.3127421577365368, policy loss: 1.3895068429571862
Experience 2, Iter 40, disc loss: 0.30182941419699205, policy loss: 1.4276035706389854
Experience 2, Iter 41, disc loss: 0.29634845647532315, policy loss: 1.4300087590673287
Experience 2, Iter 42, disc loss: 0.2957094025069641, policy loss: 1.428425349442977
Experience 2, Iter 43, disc loss: 0.28454361043979587, policy loss: 1.464034754633764
Experience 2, Iter 44, disc loss: 0.3028000890206116, policy loss: 1.4195382570530028
Experience 2, Iter 45, disc loss: 0.2658074928920706, policy loss: 1.5327890624138019
Experience 2, Iter 46, disc loss: 0.2714708581564596, policy loss: 1.5182722268828954
Experience 2, Iter 47, disc loss: 0.262902089625543, policy loss: 1.5491197420506282
Experience 2, Iter 48, disc loss: 0.26014525791352194, policy loss: 1.555592738802798
Experience 2, Iter 49, disc loss: 0.2661758586582735, policy loss: 1.5358068738295898
Experience 2, Iter 50, disc loss: 0.23979495063494327, policy loss: 1.6357942385568922
Experience 2, Iter 51, disc loss: 0.24534644659432794, policy loss: 1.6372508543877442
Experience 2, Iter 52, disc loss: 0.2543585018385914, policy loss: 1.582442923351311
Experience 2, Iter 53, disc loss: 0.2384308149828748, policy loss: 1.634018809631491
Experience 2, Iter 54, disc loss: 0.2291411178799922, policy loss: 1.69236936022389
Experience 2, Iter 55, disc loss: 0.21807236152422096, policy loss: 1.764964886033158
Experience 2, Iter 56, disc loss: 0.2240384371380779, policy loss: 1.7101107448669857
Experience 2, Iter 57, disc loss: 0.2051063667107744, policy loss: 1.8105511565060302
Experience 2, Iter 58, disc loss: 0.1901784187653585, policy loss: 1.867796779732129
Experience 2, Iter 59, disc loss: 0.21772080267857513, policy loss: 1.7591359958537494
Experience 2, Iter 60, disc loss: 0.21292991156336513, policy loss: 1.7892207855420914
Experience 2, Iter 61, disc loss: 0.20435150417389786, policy loss: 1.826615477851467
Experience 2, Iter 62, disc loss: 0.20462602914904168, policy loss: 1.820049708281265
Experience 2, Iter 63, disc loss: 0.19791314640299035, policy loss: 1.857939556610257
Experience 2, Iter 64, disc loss: 0.1826057848716769, policy loss: 1.9346963288354373
Experience 2, Iter 65, disc loss: 0.1798272462016234, policy loss: 1.9381536761819338
Experience 2, Iter 66, disc loss: 0.16483535023781928, policy loss: 2.036057069571495
Experience 2, Iter 67, disc loss: 0.16773476279682092, policy loss: 2.006271658411693
Experience 2, Iter 68, disc loss: 0.14827706901919724, policy loss: 2.161475221093892
Experience 2, Iter 69, disc loss: 0.1523044346633143, policy loss: 2.1215082671661647
Experience 2, Iter 70, disc loss: 0.15113316299626922, policy loss: 2.1362539793342434
Experience 2, Iter 71, disc loss: 0.14969516139059524, policy loss: 2.1280029811598116
Experience 2, Iter 72, disc loss: 0.1491587409447324, policy loss: 2.132405807965937
Experience 2, Iter 73, disc loss: 0.1421161356620078, policy loss: 2.222751930494117
Experience 2, Iter 74, disc loss: 0.14012537066117306, policy loss: 2.2083171922153246
Experience 2, Iter 75, disc loss: 0.14646310577595656, policy loss: 2.184694651562351
Experience 2, Iter 76, disc loss: 0.13154820996946404, policy loss: 2.309790981940251
Experience 2, Iter 77, disc loss: 0.12406665645377528, policy loss: 2.3027084232101163
Experience 2, Iter 78, disc loss: 0.12484999781835307, policy loss: 2.319436050496204
Experience 2, Iter 79, disc loss: 0.13497346479271202, policy loss: 2.2744669546498812
Experience 2, Iter 80, disc loss: 0.12323283218752702, policy loss: 2.3702586986071603
Experience 2, Iter 81, disc loss: 0.11373559518340187, policy loss: 2.4047591601681084
Experience 2, Iter 82, disc loss: 0.10366607628322737, policy loss: 2.500608951242539
Experience 2, Iter 83, disc loss: 0.11141840946212136, policy loss: 2.442476568628393
Experience 2, Iter 84, disc loss: 0.13104024385624902, policy loss: 2.333182804514461
Experience 2, Iter 85, disc loss: 0.10900476989403589, policy loss: 2.4785635215543356
Experience 2, Iter 86, disc loss: 0.11449905429829531, policy loss: 2.4814168844477216
Experience 2, Iter 87, disc loss: 0.10725819606704652, policy loss: 2.481953873563943
Experience 2, Iter 88, disc loss: 0.09560539951620567, policy loss: 2.6542117880188183
Experience 2, Iter 89, disc loss: 0.08725772131078294, policy loss: 2.6923481110297054
Experience 2, Iter 90, disc loss: 0.10190295160189419, policy loss: 2.625954426141683
Experience 2, Iter 91, disc loss: 0.08142028670161637, policy loss: 2.788380177830351
Experience 2, Iter 92, disc loss: 0.08755607091721584, policy loss: 2.7582273218143225
Experience 2, Iter 93, disc loss: 0.10038191428250114, policy loss: 2.6136807097218013
Experience 2, Iter 94, disc loss: 0.07678484124331578, policy loss: 2.866034297037468
Experience 2, Iter 95, disc loss: 0.08226092485051767, policy loss: 2.8760217037698705
Experience 2, Iter 96, disc loss: 0.07136652559151863, policy loss: 2.9070626215249495
Experience 2, Iter 97, disc loss: 0.09046290530278779, policy loss: 2.7682220558544364
Experience 2, Iter 98, disc loss: 0.08330703383921674, policy loss: 2.7644740422242124
Experience 2, Iter 99, disc loss: 0.08354523416938255, policy loss: 2.7530128968709278
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0016],
        [0.0693],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0065, 0.2770, 0.0047], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0065, 0.2770, 0.0047])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.354
Iter 2/2000 - Loss: 1.624
Iter 3/2000 - Loss: -3.215
Iter 4/2000 - Loss: -3.182
Iter 5/2000 - Loss: -1.459
Iter 6/2000 - Loss: -1.877
Iter 7/2000 - Loss: -3.206
Iter 8/2000 - Loss: -3.846
Iter 9/2000 - Loss: -3.503
Iter 10/2000 - Loss: -2.878
Iter 11/2000 - Loss: -2.716
Iter 12/2000 - Loss: -3.092
Iter 13/2000 - Loss: -3.563
Iter 14/2000 - Loss: -3.756
Iter 15/2000 - Loss: -3.657
Iter 16/2000 - Loss: -3.453
Iter 17/2000 - Loss: -3.309
Iter 18/2000 - Loss: -3.316
Iter 19/2000 - Loss: -3.475
Iter 20/2000 - Loss: -3.679
Iter 1981/2000 - Loss: -4.052
Iter 1982/2000 - Loss: -4.049
Iter 1983/2000 - Loss: -4.047
Iter 1984/2000 - Loss: -4.044
Iter 1985/2000 - Loss: -4.041
Iter 1986/2000 - Loss: -4.040
Iter 1987/2000 - Loss: -4.040
Iter 1988/2000 - Loss: -4.043
Iter 1989/2000 - Loss: -4.049
Iter 1990/2000 - Loss: -4.055
Iter 1991/2000 - Loss: -4.058
Iter 1992/2000 - Loss: -4.057
Iter 1993/2000 - Loss: -4.054
Iter 1994/2000 - Loss: -4.052
Iter 1995/2000 - Loss: -4.052
Iter 1996/2000 - Loss: -4.055
Iter 1997/2000 - Loss: -4.057
Iter 1998/2000 - Loss: -4.059
Iter 1999/2000 - Loss: -4.059
Iter 2000/2000 - Loss: -4.057
***AFTER OPTIMATION***
Noise Variance: tensor([[9.0190e-05],
        [1.2577e-03],
        [5.1240e-02],
        [9.1552e-04]])
Lengthscale: tensor([[[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]],

        [[1.0495e-03, 3.3290e-03, 5.1844e-02, 1.3396e-03, 1.2460e-05,
          7.0834e-03]]])
Signal Variance: tensor([0.0004, 0.0051, 0.2165, 0.0037])
Estimated target variance: tensor([0.0005, 0.0065, 0.2770, 0.0047])
N: 30
Signal to noise ratio: tensor([1.9977, 2.0045, 2.0557, 2.0015])
Bound on condition number: tensor([120.7216, 121.5409, 127.7809, 121.1819])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.13163650948558592, policy loss: 3.2618234796934398
Experience 3, Iter 1, disc loss: 0.09037891257245316, policy loss: 3.6475884503766265
Experience 3, Iter 2, disc loss: 0.12018281796201483, policy loss: 3.1517949863543717
Experience 3, Iter 3, disc loss: 0.10835030867042957, policy loss: 3.2644038605767003
Experience 3, Iter 4, disc loss: 0.06550914222696116, policy loss: 3.8387373731699377
Experience 3, Iter 5, disc loss: 0.08588020377863655, policy loss: 3.7977625334362957
Experience 3, Iter 6, disc loss: 0.06730621449251419, policy loss: 3.7972145648381703
Experience 3, Iter 7, disc loss: 0.09128131260231788, policy loss: 3.793824958723798
Experience 3, Iter 8, disc loss: 0.06333913307645492, policy loss: 3.726753283774289
Experience 3, Iter 9, disc loss: 0.11718055433595391, policy loss: 3.4012546124295353
Experience 3, Iter 10, disc loss: 0.06518591600308257, policy loss: 3.8484205478105635
Experience 3, Iter 11, disc loss: 0.06718394673273538, policy loss: 3.626606902008771
Experience 3, Iter 12, disc loss: 0.08290391443643283, policy loss: 3.5789721110269657
Experience 3, Iter 13, disc loss: 0.06519528464530437, policy loss: 3.898308144521719
Experience 3, Iter 14, disc loss: 0.09152056582012938, policy loss: 3.615551084725222
Experience 3, Iter 15, disc loss: 0.05787286712185746, policy loss: 4.007289550599626
Experience 3, Iter 16, disc loss: 0.056327086464159154, policy loss: 3.7738974954982902
Experience 3, Iter 17, disc loss: 0.05695210038049736, policy loss: 3.9223576782542637
Experience 3, Iter 18, disc loss: 0.06330009406839014, policy loss: 4.176047467575092
Experience 3, Iter 19, disc loss: 0.060457733529537594, policy loss: 3.9396815407519465
Experience 3, Iter 20, disc loss: 0.09352329835433813, policy loss: 3.657638218549027
Experience 3, Iter 21, disc loss: 0.05309255908434922, policy loss: 3.945545779995917
Experience 3, Iter 22, disc loss: 0.05473522929743632, policy loss: 3.8703767254174095
Experience 3, Iter 23, disc loss: 0.07200574336184322, policy loss: 3.737747663600973
Experience 3, Iter 24, disc loss: 0.04754397895570711, policy loss: 4.226633601069313
Experience 3, Iter 25, disc loss: 0.07331139678924332, policy loss: 3.8626121311069532
Experience 3, Iter 26, disc loss: 0.03687837706118319, policy loss: 4.295409444334211
Experience 3, Iter 27, disc loss: 0.05424541428035562, policy loss: 4.17848728016674
Experience 3, Iter 28, disc loss: 0.06931823321502979, policy loss: 3.8821635691174965
Experience 3, Iter 29, disc loss: 0.045935644753884626, policy loss: 4.152620576189589
Experience 3, Iter 30, disc loss: 0.051223033531531965, policy loss: 4.07508566875056
Experience 3, Iter 31, disc loss: 0.043499506218826825, policy loss: 4.199897778167429
Experience 3, Iter 32, disc loss: 0.045667774681383305, policy loss: 4.290432320291152
Experience 3, Iter 33, disc loss: 0.045340345569525244, policy loss: 4.112903820850074
Experience 3, Iter 34, disc loss: 0.046711471399827245, policy loss: 3.829956716241413
Experience 3, Iter 35, disc loss: 0.05148943820592283, policy loss: 4.148454908392618
Experience 3, Iter 36, disc loss: 0.03600469157920669, policy loss: 4.408264329573184
Experience 3, Iter 37, disc loss: 0.036640940723154686, policy loss: 4.262733097355746
Experience 3, Iter 38, disc loss: 0.06201143802326227, policy loss: 3.8108966004525655
Experience 3, Iter 39, disc loss: 0.0399596492751971, policy loss: 4.759039142769843
Experience 3, Iter 40, disc loss: 0.04245006211122655, policy loss: 4.322808303537156
Experience 3, Iter 41, disc loss: 0.02877873454296202, policy loss: 4.713745644747171
Experience 3, Iter 42, disc loss: 0.03254185986250608, policy loss: 4.5347603941157555
Experience 3, Iter 43, disc loss: 0.040595662760481835, policy loss: 4.359560240923816
Experience 3, Iter 44, disc loss: 0.035996391059663485, policy loss: 4.429822305762983
Experience 3, Iter 45, disc loss: 0.03968823719090123, policy loss: 4.574193374343994
Experience 3, Iter 46, disc loss: 0.030188780423906027, policy loss: 4.55266730647
Experience 3, Iter 47, disc loss: 0.03496345914322469, policy loss: 4.450076971918962
Experience 3, Iter 48, disc loss: 0.044294313682136724, policy loss: 4.38488207743543
Experience 3, Iter 49, disc loss: 0.03947855531680523, policy loss: 4.566602592291867
Experience 3, Iter 50, disc loss: 0.03578582314939667, policy loss: 4.4165882772493195
Experience 3, Iter 51, disc loss: 0.02844889242998093, policy loss: 4.68993602730487
Experience 3, Iter 52, disc loss: 0.03462399002812263, policy loss: 4.593354469228544
Experience 3, Iter 53, disc loss: 0.03087942211899874, policy loss: 4.334981254111341
Experience 3, Iter 54, disc loss: 0.02158314128272089, policy loss: 4.978932092953874
Experience 3, Iter 55, disc loss: 0.02589275595292623, policy loss: 4.667019832994651
Experience 3, Iter 56, disc loss: 0.029926759015870125, policy loss: 4.449489302593629
Experience 3, Iter 57, disc loss: 0.0243813827485301, policy loss: 4.745340830083344
Experience 3, Iter 58, disc loss: 0.027087264464652504, policy loss: 4.65229560884023
Experience 3, Iter 59, disc loss: 0.027818263500111656, policy loss: 4.619000522485209
Experience 3, Iter 60, disc loss: 0.025298916862945813, policy loss: 4.961151138031339
Experience 3, Iter 61, disc loss: 0.029220332958574548, policy loss: 4.496628834885431
Experience 3, Iter 62, disc loss: 0.026691583163367026, policy loss: 4.723261335492583
Experience 3, Iter 63, disc loss: 0.03198321723338264, policy loss: 4.467465438980668
Experience 3, Iter 64, disc loss: 0.028224387400792226, policy loss: 4.681301526014698
Experience 3, Iter 65, disc loss: 0.021516960385619198, policy loss: 4.542254458903209
Experience 3, Iter 66, disc loss: 0.02258177964892106, policy loss: 5.035018349017783
Experience 3, Iter 67, disc loss: 0.019781591660362363, policy loss: 4.900140733190971
Experience 3, Iter 68, disc loss: 0.03925780001901913, policy loss: 4.7484894550884444
Experience 3, Iter 69, disc loss: 0.029073858736751765, policy loss: 4.5747362209071705
Experience 3, Iter 70, disc loss: 0.02129214049147634, policy loss: 5.081641793714631
Experience 3, Iter 71, disc loss: 0.02221904580937164, policy loss: 5.1982845448909085
Experience 3, Iter 72, disc loss: 0.022133759252619564, policy loss: 5.032485781680152
Experience 3, Iter 73, disc loss: 0.01742648414245189, policy loss: 5.0273071905078295
Experience 3, Iter 74, disc loss: 0.018601602879762376, policy loss: 4.9750632336438105
Experience 3, Iter 75, disc loss: 0.025206981787408787, policy loss: 4.627135003032689
Experience 3, Iter 76, disc loss: 0.020754198849839894, policy loss: 4.832823202554868
Experience 3, Iter 77, disc loss: 0.023714537347460634, policy loss: 4.817776667559383
Experience 3, Iter 78, disc loss: 0.028874906080978063, policy loss: 4.817076025100656
Experience 3, Iter 79, disc loss: 0.022783461207305865, policy loss: 4.832083646860781
Experience 3, Iter 80, disc loss: 0.014657210115266718, policy loss: 5.259431684085257
Experience 3, Iter 81, disc loss: 0.031587519547856206, policy loss: 4.980933704816578
Experience 3, Iter 82, disc loss: 0.025358356252419466, policy loss: 4.728609525537413
Experience 3, Iter 83, disc loss: 0.019972994428106704, policy loss: 5.188043100972568
Experience 3, Iter 84, disc loss: 0.024136399392634456, policy loss: 4.84776504336749
Experience 3, Iter 85, disc loss: 0.020935282728763674, policy loss: 4.838329205783582
Experience 3, Iter 86, disc loss: 0.016960302469410302, policy loss: 5.22483136607206
Experience 3, Iter 87, disc loss: 0.029403629912251168, policy loss: 4.698945401808075
Experience 3, Iter 88, disc loss: 0.019132806471822823, policy loss: 5.048447471538788
Experience 3, Iter 89, disc loss: 0.02290260289155398, policy loss: 4.873652911938548
Experience 3, Iter 90, disc loss: 0.02219865737755851, policy loss: 4.854351072870289
Experience 3, Iter 91, disc loss: 0.02977824845841439, policy loss: 4.751536069292301
Experience 3, Iter 92, disc loss: 0.017128541450144082, policy loss: 4.907970076789715
Experience 3, Iter 93, disc loss: 0.012140691581883639, policy loss: 5.200641533685922
Experience 3, Iter 94, disc loss: 0.01569699627240909, policy loss: 5.1486140349711995
Experience 3, Iter 95, disc loss: 0.021605717966059524, policy loss: 4.612342584910467
Experience 3, Iter 96, disc loss: 0.014851293207797872, policy loss: 5.28869604160521
Experience 3, Iter 97, disc loss: 0.01874233061578508, policy loss: 5.409428213047014
Experience 3, Iter 98, disc loss: 0.015159336089504935, policy loss: 5.260858176868567
Experience 3, Iter 99, disc loss: 0.01524408392806912, policy loss: 5.243998934074705
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0028],
        [0.0779],
        [0.0014]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0011, 0.0113, 0.3114, 0.0055], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0011, 0.0113, 0.3114, 0.0055])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.099
Iter 2/2000 - Loss: 1.103
Iter 3/2000 - Loss: -2.918
Iter 4/2000 - Loss: -2.329
Iter 5/2000 - Loss: -0.957
Iter 6/2000 - Loss: -1.473
Iter 7/2000 - Loss: -2.577
Iter 8/2000 - Loss: -3.071
Iter 9/2000 - Loss: -2.819
Iter 10/2000 - Loss: -2.356
Iter 11/2000 - Loss: -2.179
Iter 12/2000 - Loss: -2.360
Iter 13/2000 - Loss: -2.679
Iter 14/2000 - Loss: -2.914
Iter 15/2000 - Loss: -2.976
Iter 16/2000 - Loss: -2.883
Iter 17/2000 - Loss: -2.730
Iter 18/2000 - Loss: -2.637
Iter 19/2000 - Loss: -2.675
Iter 20/2000 - Loss: -2.814
Iter 1981/2000 - Loss: -3.208
Iter 1982/2000 - Loss: -3.208
Iter 1983/2000 - Loss: -3.207
Iter 1984/2000 - Loss: -3.207
Iter 1985/2000 - Loss: -3.207
Iter 1986/2000 - Loss: -3.207
Iter 1987/2000 - Loss: -3.208
Iter 1988/2000 - Loss: -3.208
Iter 1989/2000 - Loss: -3.207
Iter 1990/2000 - Loss: -3.207
Iter 1991/2000 - Loss: -3.207
Iter 1992/2000 - Loss: -3.207
Iter 1993/2000 - Loss: -3.208
Iter 1994/2000 - Loss: -3.208
Iter 1995/2000 - Loss: -3.208
Iter 1996/2000 - Loss: -3.208
Iter 1997/2000 - Loss: -3.207
Iter 1998/2000 - Loss: -3.207
Iter 1999/2000 - Loss: -3.208
Iter 2000/2000 - Loss: -3.208
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0022],
        [0.0579],
        [0.0011]])
Lengthscale: tensor([[[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]],

        [[2.5859e-03, 9.0471e-03, 6.0172e-02, 1.8543e-03, 1.5236e-05,
          5.1057e-02]]])
Signal Variance: tensor([0.0009, 0.0088, 0.2457, 0.0043])
Estimated target variance: tensor([0.0011, 0.0113, 0.3114, 0.0055])
N: 40
Signal to noise ratio: tensor([1.9995, 2.0054, 2.0600, 2.0018])
Bound on condition number: tensor([160.9263, 161.8633, 170.7492, 161.2936])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.05979835705077005, policy loss: 3.705230056347793
Experience 4, Iter 1, disc loss: 0.0779601673526633, policy loss: 3.525284374319642
Experience 4, Iter 2, disc loss: 0.08462021011988059, policy loss: 3.338935654308224
Experience 4, Iter 3, disc loss: 0.0645673010312733, policy loss: 3.8697185914662273
Experience 4, Iter 4, disc loss: 0.06940745497152102, policy loss: 3.614456628398765
Experience 4, Iter 5, disc loss: 0.06098769480645674, policy loss: 3.6376792170596257
Experience 4, Iter 6, disc loss: 0.07026751133033904, policy loss: 3.5978381481091857
Experience 4, Iter 7, disc loss: 0.04581106740848486, policy loss: 3.9729505040155026
Experience 4, Iter 8, disc loss: 0.05203697157732484, policy loss: 4.068850179796545
Experience 4, Iter 9, disc loss: 0.07458940869020939, policy loss: 3.640173794644237
Experience 4, Iter 10, disc loss: 0.05431366061462974, policy loss: 3.7677573092871683
Experience 4, Iter 11, disc loss: 0.07148846267118725, policy loss: 3.6551398266089086
Experience 4, Iter 12, disc loss: 0.0685085602836991, policy loss: 3.737457640062197
Experience 4, Iter 13, disc loss: 0.07560254734407658, policy loss: 3.6178934299188237
Experience 4, Iter 14, disc loss: 0.08137333130322984, policy loss: 3.6912238558248065
Experience 4, Iter 15, disc loss: 0.07104733482937503, policy loss: 3.778179664768307
Experience 4, Iter 16, disc loss: 0.055517882479739, policy loss: 3.9735788692349523
Experience 4, Iter 17, disc loss: 0.05042308452026787, policy loss: 3.560611768891741
Experience 4, Iter 18, disc loss: 0.05435250525617953, policy loss: 3.8898935174746634
Experience 4, Iter 19, disc loss: 0.06057854939091627, policy loss: 3.763566011321989
Experience 4, Iter 20, disc loss: 0.050248927840321665, policy loss: 4.1156849962032735
Experience 4, Iter 21, disc loss: 0.05259793599302024, policy loss: 3.959697067303833
Experience 4, Iter 22, disc loss: 0.05627677072484451, policy loss: 4.0155257511787354
Experience 4, Iter 23, disc loss: 0.04319807062315563, policy loss: 4.058406999960056
Experience 4, Iter 24, disc loss: 0.028416840684946442, policy loss: 4.570442814813795
Experience 4, Iter 25, disc loss: 0.059519935598855864, policy loss: 4.009038224160724
Experience 4, Iter 26, disc loss: 0.04848512331818092, policy loss: 4.208209345433418
Experience 4, Iter 27, disc loss: 0.05164175935541161, policy loss: 3.9979338165895566
Experience 4, Iter 28, disc loss: 0.0499380011818664, policy loss: 4.084838921734402
Experience 4, Iter 29, disc loss: 0.045264797376512206, policy loss: 4.0710396280873535
Experience 4, Iter 30, disc loss: 0.046696967024429575, policy loss: 3.901690829239808
Experience 4, Iter 31, disc loss: 0.048826136631846345, policy loss: 4.277357683433748
Experience 4, Iter 32, disc loss: 0.05023910659615006, policy loss: 4.218010334312982
Experience 4, Iter 33, disc loss: 0.03133427054032066, policy loss: 4.713340659204556
Experience 4, Iter 34, disc loss: 0.04535879555538577, policy loss: 4.225199942505148
Experience 4, Iter 35, disc loss: 0.035116654350957226, policy loss: 4.472094941517248
Experience 4, Iter 36, disc loss: 0.04468874300741704, policy loss: 4.365539162460937
Experience 4, Iter 37, disc loss: 0.03329544037667992, policy loss: 4.56780773010869
Experience 4, Iter 38, disc loss: 0.035687786691814, policy loss: 4.478099731214579
Experience 4, Iter 39, disc loss: 0.033073647193977394, policy loss: 4.474147063278981
Experience 4, Iter 40, disc loss: 0.0434563077255098, policy loss: 4.028497276531535
Experience 4, Iter 41, disc loss: 0.023740099351197367, policy loss: 4.73863897578707
Experience 4, Iter 42, disc loss: 0.037605417165675434, policy loss: 4.506942550924614
Experience 4, Iter 43, disc loss: 0.036783951148991724, policy loss: 4.4134361621655085
Experience 4, Iter 44, disc loss: 0.04331089671366911, policy loss: 4.499678362054352
Experience 4, Iter 45, disc loss: 0.04768320055266789, policy loss: 4.342781086874952
Experience 4, Iter 46, disc loss: 0.0278354341956503, policy loss: 4.829467369244824
Experience 4, Iter 47, disc loss: 0.0363289550246231, policy loss: 4.474654429054571
Experience 4, Iter 48, disc loss: 0.029494472606162267, policy loss: 4.489679573173945
Experience 4, Iter 49, disc loss: 0.03901999164989385, policy loss: 4.34493798326702
Experience 4, Iter 50, disc loss: 0.025649455823564527, policy loss: 4.730261687897963
Experience 4, Iter 51, disc loss: 0.039300421017350956, policy loss: 4.586898216656939
Experience 4, Iter 52, disc loss: 0.02742609259511794, policy loss: 4.743962011837166
Experience 4, Iter 53, disc loss: 0.02665458270299606, policy loss: 4.785114151056332
Experience 4, Iter 54, disc loss: 0.029129100547155465, policy loss: 4.963774273681068
Experience 4, Iter 55, disc loss: 0.033465999370995696, policy loss: 4.7128846700761375
Experience 4, Iter 56, disc loss: 0.028645067516775112, policy loss: 4.8937711501383205
Experience 4, Iter 57, disc loss: 0.024568804526998007, policy loss: 4.974245391099852
Experience 4, Iter 58, disc loss: 0.03305079837643934, policy loss: 4.825889028772253
Experience 4, Iter 59, disc loss: 0.02110993496407254, policy loss: 4.763371671078242
Experience 4, Iter 60, disc loss: 0.0336972837739544, policy loss: 4.617216914313178
Experience 4, Iter 61, disc loss: 0.02553938581133408, policy loss: 4.950185263157957
Experience 4, Iter 62, disc loss: 0.025297932002182454, policy loss: 4.96658529611394
Experience 4, Iter 63, disc loss: 0.03181878822044154, policy loss: 4.6929301529870004
Experience 4, Iter 64, disc loss: 0.03171635500988236, policy loss: 4.715545091565416
Experience 4, Iter 65, disc loss: 0.020835503304814713, policy loss: 4.983374900000932
Experience 4, Iter 66, disc loss: 0.039959555594244675, policy loss: 4.61286664949406
Experience 4, Iter 67, disc loss: 0.03233757489227251, policy loss: 4.5541280837552796
Experience 4, Iter 68, disc loss: 0.018483292693296027, policy loss: 5.250792422523141
Experience 4, Iter 69, disc loss: 0.02632177457615923, policy loss: 5.058499360513249
Experience 4, Iter 70, disc loss: 0.03174942723224174, policy loss: 5.206558131620964
Experience 4, Iter 71, disc loss: 0.02982238935002436, policy loss: 5.152855536048191
Experience 4, Iter 72, disc loss: 0.025080368424960996, policy loss: 4.9388000660572615
Experience 4, Iter 73, disc loss: 0.022378523744429876, policy loss: 5.2212505834871585
Experience 4, Iter 74, disc loss: 0.03809162376540812, policy loss: 4.413933718751541
Experience 4, Iter 75, disc loss: 0.01736770937114186, policy loss: 5.241631185331835
Experience 4, Iter 76, disc loss: 0.022455216757491618, policy loss: 5.203072803198301
Experience 4, Iter 77, disc loss: 0.023368378678577658, policy loss: 5.232663109350504
Experience 4, Iter 78, disc loss: 0.01587815097659432, policy loss: 5.4020376104713055
Experience 4, Iter 79, disc loss: 0.0276542493644396, policy loss: 5.087774433614221
Experience 4, Iter 80, disc loss: 0.025119627637557575, policy loss: 5.177854698893887
Experience 4, Iter 81, disc loss: 0.017119112838768512, policy loss: 5.333267250503973
Experience 4, Iter 82, disc loss: 0.02514917282889958, policy loss: 5.001828918276095
Experience 4, Iter 83, disc loss: 0.014634992879377933, policy loss: 5.147788870738252
Experience 4, Iter 84, disc loss: 0.01959561638174435, policy loss: 5.281582773197865
Experience 4, Iter 85, disc loss: 0.028139300573989803, policy loss: 4.8618537483138144
Experience 4, Iter 86, disc loss: 0.025960292175166835, policy loss: 4.928553567567781
Experience 4, Iter 87, disc loss: 0.023901339636664038, policy loss: 5.217622158247544
Experience 4, Iter 88, disc loss: 0.02117656478057095, policy loss: 5.4473421551121195
Experience 4, Iter 89, disc loss: 0.020752170924924453, policy loss: 5.162245760063379
Experience 4, Iter 90, disc loss: 0.013098021091140014, policy loss: 5.735068713226611
Experience 4, Iter 91, disc loss: 0.020406333582986463, policy loss: 5.050057706386121
Experience 4, Iter 92, disc loss: 0.024360074985846688, policy loss: 5.062072576163319
Experience 4, Iter 93, disc loss: 0.016418198336204864, policy loss: 5.830988099738123
Experience 4, Iter 94, disc loss: 0.02431334740436157, policy loss: 5.2671064409758275
Experience 4, Iter 95, disc loss: 0.023385487904532765, policy loss: 5.30203917010615
Experience 4, Iter 96, disc loss: 0.023557155106016137, policy loss: 5.514179801816011
Experience 4, Iter 97, disc loss: 0.02415137076173953, policy loss: 5.155823042566962
Experience 4, Iter 98, disc loss: 0.01722900667709071, policy loss: 5.566725768412718
Experience 4, Iter 99, disc loss: 0.01840656031451533, policy loss: 5.59082737645799
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0027],
        [0.0636],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0014, 0.0109, 0.2544, 0.0046], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0014, 0.0109, 0.2544, 0.0046])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.223
Iter 2/2000 - Loss: 1.231
Iter 3/2000 - Loss: -2.974
Iter 4/2000 - Loss: -2.417
Iter 5/2000 - Loss: -0.906
Iter 6/2000 - Loss: -1.253
Iter 7/2000 - Loss: -2.378
Iter 8/2000 - Loss: -3.074
Iter 9/2000 - Loss: -3.017
Iter 10/2000 - Loss: -2.553
Iter 11/2000 - Loss: -2.206
Iter 12/2000 - Loss: -2.230
Iter 13/2000 - Loss: -2.526
Iter 14/2000 - Loss: -2.849
Iter 15/2000 - Loss: -3.013
Iter 16/2000 - Loss: -2.983
Iter 17/2000 - Loss: -2.855
Iter 18/2000 - Loss: -2.747
Iter 19/2000 - Loss: -2.726
Iter 20/2000 - Loss: -2.784
Iter 1981/2000 - Loss: -3.277
Iter 1982/2000 - Loss: -3.276
Iter 1983/2000 - Loss: -3.277
Iter 1984/2000 - Loss: -3.278
Iter 1985/2000 - Loss: -3.278
Iter 1986/2000 - Loss: -3.277
Iter 1987/2000 - Loss: -3.277
Iter 1988/2000 - Loss: -3.278
Iter 1989/2000 - Loss: -3.278
Iter 1990/2000 - Loss: -3.278
Iter 1991/2000 - Loss: -3.278
Iter 1992/2000 - Loss: -3.277
Iter 1993/2000 - Loss: -3.278
Iter 1994/2000 - Loss: -3.278
Iter 1995/2000 - Loss: -3.278
Iter 1996/2000 - Loss: -3.278
Iter 1997/2000 - Loss: -3.278
Iter 1998/2000 - Loss: -3.278
Iter 1999/2000 - Loss: -3.278
Iter 2000/2000 - Loss: -3.279
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0021],
        [0.0480],
        [0.0009]])
Lengthscale: tensor([[[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]],

        [[2.6614e-03, 1.2014e-02, 5.0096e-02, 1.7496e-03, 1.2747e-05,
          6.3219e-02]]])
Signal Variance: tensor([0.0011, 0.0086, 0.2014, 0.0036])
Estimated target variance: tensor([0.0014, 0.0109, 0.2544, 0.0046])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0034, 2.0485, 2.0015])
Bound on condition number: tensor([200.9914, 201.6757, 210.8107, 201.3050])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.05038119448289184, policy loss: 4.275532852532464
Experience 5, Iter 1, disc loss: 0.03833621380626838, policy loss: 4.266758296613745
Experience 5, Iter 2, disc loss: 0.03606904018825475, policy loss: 4.478719699088456
Experience 5, Iter 3, disc loss: 0.04229530613582298, policy loss: 3.980049170767884
Experience 5, Iter 4, disc loss: 0.03313718493572674, policy loss: 4.649724507623494
Experience 5, Iter 5, disc loss: 0.037270465307789, policy loss: 4.262391694481827
Experience 5, Iter 6, disc loss: 0.036922300597112295, policy loss: 4.318218499658117
Experience 5, Iter 7, disc loss: 0.04526971425730095, policy loss: 4.306197679105274
Experience 5, Iter 8, disc loss: 0.04415239582434473, policy loss: 4.195285590541111
Experience 5, Iter 9, disc loss: 0.035189527829016236, policy loss: 4.371427620649451
Experience 5, Iter 10, disc loss: 0.03959331966266589, policy loss: 4.379763620219108
Experience 5, Iter 11, disc loss: 0.033170704016532784, policy loss: 4.443633673489266
Experience 5, Iter 12, disc loss: 0.03279163822035816, policy loss: 4.454238638798255
Experience 5, Iter 13, disc loss: 0.028401411246897584, policy loss: 4.6469804416803075
Experience 5, Iter 14, disc loss: 0.028472344407846996, policy loss: 4.483206443521442
Experience 5, Iter 15, disc loss: 0.03391485735256886, policy loss: 4.629962514982096
Experience 5, Iter 16, disc loss: 0.0253123055973977, policy loss: 4.807851294883167
Experience 5, Iter 17, disc loss: 0.04376529186997098, policy loss: 4.444457433948997
Experience 5, Iter 18, disc loss: 0.030472682647502834, policy loss: 4.600707780525873
Experience 5, Iter 19, disc loss: 0.03004712782617839, policy loss: 4.670540103022059
Experience 5, Iter 20, disc loss: 0.023861597673745197, policy loss: 4.7241219012229045
Experience 5, Iter 21, disc loss: 0.026932616533266795, policy loss: 4.8204426771578905
Experience 5, Iter 22, disc loss: 0.033302935831035343, policy loss: 4.678622679075236
Experience 5, Iter 23, disc loss: 0.026409609756517304, policy loss: 4.918405523285264
Experience 5, Iter 24, disc loss: 0.03629228685510677, policy loss: 4.573761425976287
Experience 5, Iter 25, disc loss: 0.029649016046775712, policy loss: 4.667062180804193
Experience 5, Iter 26, disc loss: 0.0290991127560783, policy loss: 4.788887964180734
Experience 5, Iter 27, disc loss: 0.027104058965533112, policy loss: 5.03601737151979
Experience 5, Iter 28, disc loss: 0.02340330103219469, policy loss: 5.181062156310506
Experience 5, Iter 29, disc loss: 0.026329769162067026, policy loss: 4.665212450902075
Experience 5, Iter 30, disc loss: 0.02139590319022803, policy loss: 5.311227706080232
Experience 5, Iter 31, disc loss: 0.034703722613463016, policy loss: 4.713309555651023
Experience 5, Iter 32, disc loss: 0.019081088160132872, policy loss: 4.925112907288494
Experience 5, Iter 33, disc loss: 0.03021653682644263, policy loss: 4.885080255247047
Experience 5, Iter 34, disc loss: 0.02866750125798046, policy loss: 4.789296729113577
Experience 5, Iter 35, disc loss: 0.029086380851419524, policy loss: 4.6148983805196035
Experience 5, Iter 36, disc loss: 0.029358600839829697, policy loss: 4.6694384680908945
Experience 5, Iter 37, disc loss: 0.0148402146403748, policy loss: 5.368380246019111
Experience 5, Iter 38, disc loss: 0.019838537426381624, policy loss: 4.65300819669015
Experience 5, Iter 39, disc loss: 0.021148415639291863, policy loss: 5.027051190284462
Experience 5, Iter 40, disc loss: 0.024579709192507307, policy loss: 4.99307432641452
Experience 5, Iter 41, disc loss: 0.020190891270410502, policy loss: 5.101968800395111
Experience 5, Iter 42, disc loss: 0.0214002346861434, policy loss: 4.905734509657537
Experience 5, Iter 43, disc loss: 0.018277654375757228, policy loss: 5.168075309303994
Experience 5, Iter 44, disc loss: 0.019344307645065516, policy loss: 5.520653067683478
Experience 5, Iter 45, disc loss: 0.014721938274658751, policy loss: 5.4235798129885024
Experience 5, Iter 46, disc loss: 0.021620380270430214, policy loss: 5.136906990267569
Experience 5, Iter 47, disc loss: 0.020038131491367024, policy loss: 5.301358555854633
Experience 5, Iter 48, disc loss: 0.0177529346893215, policy loss: 5.107322445681483
Experience 5, Iter 49, disc loss: 0.0241872187447438, policy loss: 4.900318081036353
Experience 5, Iter 50, disc loss: 0.016989445107024062, policy loss: 5.28161785104254
Experience 5, Iter 51, disc loss: 0.021977211948264113, policy loss: 5.161471654599428
Experience 5, Iter 52, disc loss: 0.02100797615342285, policy loss: 5.3192627964162105
Experience 5, Iter 53, disc loss: 0.00992070914056159, policy loss: 5.905245402959526
Experience 5, Iter 54, disc loss: 0.019308137720507535, policy loss: 5.170426846771471
Experience 5, Iter 55, disc loss: 0.02372794191218819, policy loss: 5.631061314236116
Experience 5, Iter 56, disc loss: 0.01731468746182382, policy loss: 5.588993762379102
Experience 5, Iter 57, disc loss: 0.021211063894701707, policy loss: 5.3618717610979365
Experience 5, Iter 58, disc loss: 0.015332074675357703, policy loss: 5.764567247306394
Experience 5, Iter 59, disc loss: 0.012521378118354338, policy loss: 5.473504682540666
Experience 5, Iter 60, disc loss: 0.01718050312429548, policy loss: 5.51296866783174
Experience 5, Iter 61, disc loss: 0.01451714241503117, policy loss: 5.569347732552709
Experience 5, Iter 62, disc loss: 0.01630640158571831, policy loss: 5.741867303867832
Experience 5, Iter 63, disc loss: 0.01593407774813821, policy loss: 5.521044944604428
Experience 5, Iter 64, disc loss: 0.01726574617670904, policy loss: 5.590309326817747
Experience 5, Iter 65, disc loss: 0.015940709388411387, policy loss: 5.498626447671722
Experience 5, Iter 66, disc loss: 0.0168288781355019, policy loss: 5.642243769991501
Experience 5, Iter 67, disc loss: 0.014162903653887723, policy loss: 5.797569634086027
Experience 5, Iter 68, disc loss: 0.018055957139151476, policy loss: 5.334314038053046
Experience 5, Iter 69, disc loss: 0.015512817687843728, policy loss: 5.391621314934108
Experience 5, Iter 70, disc loss: 0.01664107245729121, policy loss: 5.108090375531388
Experience 5, Iter 71, disc loss: 0.014759522056186004, policy loss: 5.91877038445198
Experience 5, Iter 72, disc loss: 0.00898057875164765, policy loss: 5.922513315845154
Experience 5, Iter 73, disc loss: 0.017955194880544065, policy loss: 5.531327026289773
Experience 5, Iter 74, disc loss: 0.016935886853457413, policy loss: 5.491715191789931
Experience 5, Iter 75, disc loss: 0.011434420500762337, policy loss: 6.1904635117519256
Experience 5, Iter 76, disc loss: 0.01139824478196884, policy loss: 5.651387041658144
Experience 5, Iter 77, disc loss: 0.01635546062623138, policy loss: 5.593489188854393
Experience 5, Iter 78, disc loss: 0.013258066158613163, policy loss: 5.883912000038052
Experience 5, Iter 79, disc loss: 0.012018326516327435, policy loss: 5.506687772569681
Experience 5, Iter 80, disc loss: 0.017315332474996257, policy loss: 5.7359860871583095
Experience 5, Iter 81, disc loss: 0.01086299485441506, policy loss: 5.774187070823434
Experience 5, Iter 82, disc loss: 0.01273221377037605, policy loss: 5.625323801712916
Experience 5, Iter 83, disc loss: 0.01130855727336097, policy loss: 5.505847774763555
Experience 5, Iter 84, disc loss: 0.011305992701723888, policy loss: 6.0022962848019255
Experience 5, Iter 85, disc loss: 0.010886580055492695, policy loss: 6.021049093813369
Experience 5, Iter 86, disc loss: 0.011549086277454945, policy loss: 5.598431144820555
Experience 5, Iter 87, disc loss: 0.012494295516688575, policy loss: 5.431197341984593
Experience 5, Iter 88, disc loss: 0.011234382097423288, policy loss: 5.627575312689863
Experience 5, Iter 89, disc loss: 0.013270947537684043, policy loss: 5.941337515903744
Experience 5, Iter 90, disc loss: 0.013599657479435574, policy loss: 5.645763793503077
Experience 5, Iter 91, disc loss: 0.01198339305196389, policy loss: 5.830172145740641
Experience 5, Iter 92, disc loss: 0.01298630409281165, policy loss: 6.127540019941558
Experience 5, Iter 93, disc loss: 0.015521071393846756, policy loss: 5.571914651650106
Experience 5, Iter 94, disc loss: 0.010406724727711633, policy loss: 5.88140914119904
Experience 5, Iter 95, disc loss: 0.010692668460730253, policy loss: 5.96985714808526
Experience 5, Iter 96, disc loss: 0.016427408959945173, policy loss: 5.660024776276833
Experience 5, Iter 97, disc loss: 0.014022614633396706, policy loss: 6.075355984038713
Experience 5, Iter 98, disc loss: 0.0081153806899734, policy loss: 6.041730482878142
Experience 5, Iter 99, disc loss: 0.011684194427498212, policy loss: 5.8374454879205695
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0028],
        [0.0564],
        [0.0011]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0015, 0.0111, 0.2258, 0.0042], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0015, 0.0111, 0.2258, 0.0042])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.272
Iter 2/2000 - Loss: 0.662
Iter 3/2000 - Loss: -3.074
Iter 4/2000 - Loss: -2.535
Iter 5/2000 - Loss: -1.227
Iter 6/2000 - Loss: -1.634
Iter 7/2000 - Loss: -2.654
Iter 8/2000 - Loss: -3.185
Iter 9/2000 - Loss: -3.024
Iter 10/2000 - Loss: -2.589
Iter 11/2000 - Loss: -2.358
Iter 12/2000 - Loss: -2.469
Iter 13/2000 - Loss: -2.756
Iter 14/2000 - Loss: -2.996
Iter 15/2000 - Loss: -3.082
Iter 16/2000 - Loss: -3.030
Iter 17/2000 - Loss: -2.915
Iter 18/2000 - Loss: -2.819
Iter 19/2000 - Loss: -2.804
Iter 20/2000 - Loss: -2.885
Iter 1981/2000 - Loss: -3.340
Iter 1982/2000 - Loss: -3.340
Iter 1983/2000 - Loss: -3.340
Iter 1984/2000 - Loss: -3.340
Iter 1985/2000 - Loss: -3.340
Iter 1986/2000 - Loss: -3.340
Iter 1987/2000 - Loss: -3.340
Iter 1988/2000 - Loss: -3.340
Iter 1989/2000 - Loss: -3.340
Iter 1990/2000 - Loss: -3.340
Iter 1991/2000 - Loss: -3.340
Iter 1992/2000 - Loss: -3.340
Iter 1993/2000 - Loss: -3.340
Iter 1994/2000 - Loss: -3.340
Iter 1995/2000 - Loss: -3.340
Iter 1996/2000 - Loss: -3.340
Iter 1997/2000 - Loss: -3.340
Iter 1998/2000 - Loss: -3.340
Iter 1999/2000 - Loss: -3.340
Iter 2000/2000 - Loss: -3.340
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0022],
        [0.0429],
        [0.0008]])
Lengthscale: tensor([[[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]],

        [[2.4827e-03, 1.2489e-02, 4.4614e-02, 1.6816e-03, 1.1445e-05,
          7.0488e-02]]])
Signal Variance: tensor([0.0012, 0.0088, 0.1791, 0.0033])
Estimated target variance: tensor([0.0015, 0.0111, 0.2258, 0.0042])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0047, 2.0426, 2.0014])
Bound on condition number: tensor([241.0077, 242.1178, 251.3436, 241.3333])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.021597927240688975, policy loss: 5.1355713363914175
Experience 6, Iter 1, disc loss: 0.017429065528884194, policy loss: 5.304903056791741
Experience 6, Iter 2, disc loss: 0.012908004609075652, policy loss: 5.305117809017207
Experience 6, Iter 3, disc loss: 0.01396251781406479, policy loss: 5.438067939794968
Experience 6, Iter 4, disc loss: 0.023839711691958757, policy loss: 5.013306851819504
Experience 6, Iter 5, disc loss: 0.010617567073391065, policy loss: 5.926513932308711
Experience 6, Iter 6, disc loss: 0.011794288904503535, policy loss: 5.492028395972355
Experience 6, Iter 7, disc loss: 0.017579373730694643, policy loss: 5.494770907433052
Experience 6, Iter 8, disc loss: 0.01449005496487678, policy loss: 5.145128981728079
Experience 6, Iter 9, disc loss: 0.013825339887629587, policy loss: 5.401038126002327
Experience 6, Iter 10, disc loss: 0.017237643823597007, policy loss: 5.177617913346709
Experience 6, Iter 11, disc loss: 0.015692403057861157, policy loss: 4.957412422310261
Experience 6, Iter 12, disc loss: 0.012942744235473326, policy loss: 5.353019252994788
Experience 6, Iter 13, disc loss: 0.015252693320580143, policy loss: 5.277854377909222
Experience 6, Iter 14, disc loss: 0.016272461714786312, policy loss: 5.352428967744871
Experience 6, Iter 15, disc loss: 0.013857064724916974, policy loss: 5.22857423962671
Experience 6, Iter 16, disc loss: 0.01477806615704102, policy loss: 5.639344467089256
Experience 6, Iter 17, disc loss: 0.011866966376460399, policy loss: 5.698880761214797
Experience 6, Iter 18, disc loss: 0.017018967317766358, policy loss: 5.264539968253473
Experience 6, Iter 19, disc loss: 0.014063291579069766, policy loss: 5.654050755625621
Experience 6, Iter 20, disc loss: 0.01598094812590184, policy loss: 5.505090251934688
Experience 6, Iter 21, disc loss: 0.020682625699474703, policy loss: 5.469278566592431
Experience 6, Iter 22, disc loss: 0.015404347782544564, policy loss: 5.117576029261814
Experience 6, Iter 23, disc loss: 0.01001909018631955, policy loss: 5.7330226209433786
Experience 6, Iter 24, disc loss: 0.017913392753432403, policy loss: 5.300721661510126
Experience 6, Iter 25, disc loss: 0.01399814929585227, policy loss: 5.623604655722421
Experience 6, Iter 26, disc loss: 0.012090133531940701, policy loss: 5.720975033106592
Experience 6, Iter 27, disc loss: 0.014698064847307027, policy loss: 5.576640564623998
Experience 6, Iter 28, disc loss: 0.015319381271123707, policy loss: 5.659201259175816
Experience 6, Iter 29, disc loss: 0.01431725289323795, policy loss: 5.4070048413690674
Experience 6, Iter 30, disc loss: 0.0068368001403798576, policy loss: 6.213755027980902
Experience 6, Iter 31, disc loss: 0.013305541997036567, policy loss: 5.357026174201357
Experience 6, Iter 32, disc loss: 0.00830387532587691, policy loss: 5.887369956427365
Experience 6, Iter 33, disc loss: 0.012165347236627383, policy loss: 5.860045485358256
Experience 6, Iter 34, disc loss: 0.012357454515001434, policy loss: 5.511131300402877
Experience 6, Iter 35, disc loss: 0.0082541667236599, policy loss: 6.287461871984242
Experience 6, Iter 36, disc loss: 0.011709068457321201, policy loss: 5.593611719757228
Experience 6, Iter 37, disc loss: 0.012479310663525436, policy loss: 5.8220269508988345
Experience 6, Iter 38, disc loss: 0.019912013526806323, policy loss: 5.813022896951802
Experience 6, Iter 39, disc loss: 0.011536088827867217, policy loss: 5.589431836894635
Experience 6, Iter 40, disc loss: 0.010055524049526478, policy loss: 6.1321368144441895
Experience 6, Iter 41, disc loss: 0.011306307729639097, policy loss: 5.744874975369143
Experience 6, Iter 42, disc loss: 0.010058105466181048, policy loss: 6.047575251293704
Experience 6, Iter 43, disc loss: 0.00808823652085954, policy loss: 5.959348331652881
Experience 6, Iter 44, disc loss: 0.008994493335908826, policy loss: 6.3137752661577995
Experience 6, Iter 45, disc loss: 0.008932140329162699, policy loss: 5.8282300728660275
Experience 6, Iter 46, disc loss: 0.014068281002352292, policy loss: 5.817558019675737
Experience 6, Iter 47, disc loss: 0.010945625662657821, policy loss: 5.448942258803655
Experience 6, Iter 48, disc loss: 0.011002258593776666, policy loss: 5.798462167712401
Experience 6, Iter 49, disc loss: 0.008890011431887787, policy loss: 5.907964144431984
Experience 6, Iter 50, disc loss: 0.009650717376096882, policy loss: 5.800331554863289
Experience 6, Iter 51, disc loss: 0.00867483640359791, policy loss: 6.252154546908381
Experience 6, Iter 52, disc loss: 0.008892032509124272, policy loss: 6.218332376975313
Experience 6, Iter 53, disc loss: 0.01019329223134064, policy loss: 6.012940743378713
Experience 6, Iter 54, disc loss: 0.01191748161293742, policy loss: 5.708600269018482
Experience 6, Iter 55, disc loss: 0.008127595751186666, policy loss: 6.21133477862907
Experience 6, Iter 56, disc loss: 0.007310950647184365, policy loss: 6.376096501303975
Experience 6, Iter 57, disc loss: 0.010377773662199264, policy loss: 5.639794863112307
Experience 6, Iter 58, disc loss: 0.012041142507423113, policy loss: 5.74776868073476
Experience 6, Iter 59, disc loss: 0.008498760654141093, policy loss: 5.923267058596833
Experience 6, Iter 60, disc loss: 0.012270755948834556, policy loss: 5.658170781689004
Experience 6, Iter 61, disc loss: 0.0100847275212489, policy loss: 5.58132866053723
Experience 6, Iter 62, disc loss: 0.009711844778675666, policy loss: 6.293906779527406
Experience 6, Iter 63, disc loss: 0.007648496537641369, policy loss: 6.210392638274229
Experience 6, Iter 64, disc loss: 0.00989933755714343, policy loss: 6.304565250571945
Experience 6, Iter 65, disc loss: 0.009273409883437996, policy loss: 6.0372318842842265
Experience 6, Iter 66, disc loss: 0.006850094095744669, policy loss: 6.268420876073206
Experience 6, Iter 67, disc loss: 0.010151072947457507, policy loss: 5.846511851834668
Experience 6, Iter 68, disc loss: 0.00844010664588505, policy loss: 6.056138477660011
Experience 6, Iter 69, disc loss: 0.010756041813781281, policy loss: 6.0655734556549215
Experience 6, Iter 70, disc loss: 0.008073755229702365, policy loss: 6.401938906121793
Experience 6, Iter 71, disc loss: 0.0110507700822871, policy loss: 5.980388917092723
Experience 6, Iter 72, disc loss: 0.006927389089709458, policy loss: 6.154557762201527
Experience 6, Iter 73, disc loss: 0.009529671822808674, policy loss: 5.817410536950543
Experience 6, Iter 74, disc loss: 0.005094896425547205, policy loss: 6.462747738418567
Experience 6, Iter 75, disc loss: 0.005971656427543669, policy loss: 6.429493956485162
Experience 6, Iter 76, disc loss: 0.007047167063204171, policy loss: 6.153433272962171
Experience 6, Iter 77, disc loss: 0.010524993110559292, policy loss: 5.956808569721544
Experience 6, Iter 78, disc loss: 0.011006477982885433, policy loss: 6.643033215753318
Experience 6, Iter 79, disc loss: 0.005731204089353152, policy loss: 6.600594545847889
Experience 6, Iter 80, disc loss: 0.008985422989208342, policy loss: 6.5974662713268115
Experience 6, Iter 81, disc loss: 0.00748927510922273, policy loss: 6.445341142435396
Experience 6, Iter 82, disc loss: 0.009723177307705383, policy loss: 6.403382780411683
Experience 6, Iter 83, disc loss: 0.007868364262254974, policy loss: 6.33862949533548
Experience 6, Iter 84, disc loss: 0.006794247807685299, policy loss: 6.4174985945695315
Experience 6, Iter 85, disc loss: 0.005965497845601289, policy loss: 6.881876871984115
Experience 6, Iter 86, disc loss: 0.005774869690269536, policy loss: 6.25386263756023
Experience 6, Iter 87, disc loss: 0.006858754301622166, policy loss: 6.336846965908397
Experience 6, Iter 88, disc loss: 0.0063883760145394975, policy loss: 6.18392367548992
Experience 6, Iter 89, disc loss: 0.006770838428956398, policy loss: 6.109639579260555
Experience 6, Iter 90, disc loss: 0.0066613624586062785, policy loss: 6.309833128095905
Experience 6, Iter 91, disc loss: 0.007153113818252376, policy loss: 6.18400022208965
Experience 6, Iter 92, disc loss: 0.007565282662611593, policy loss: 6.169901554516819
Experience 6, Iter 93, disc loss: 0.01126642266642636, policy loss: 6.421500949584534
Experience 6, Iter 94, disc loss: 0.008468254656075562, policy loss: 6.062194584250637
Experience 6, Iter 95, disc loss: 0.00900698522144424, policy loss: 6.298839916158165
Experience 6, Iter 96, disc loss: 0.006308818406646558, policy loss: 6.858987202666091
Experience 6, Iter 97, disc loss: 0.007606376386923085, policy loss: 6.181915112227727
Experience 6, Iter 98, disc loss: 0.007230553564065597, policy loss: 6.344651035211826
Experience 6, Iter 99, disc loss: 0.006529052204009744, policy loss: 6.394621110741852
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0027],
        [0.0519],
        [0.0010]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0018, 0.0110, 0.2077, 0.0040], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0018, 0.0110, 0.2077, 0.0040])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.197
Iter 2/2000 - Loss: -0.233
Iter 3/2000 - Loss: -3.093
Iter 4/2000 - Loss: -2.675
Iter 5/2000 - Loss: -1.697
Iter 6/2000 - Loss: -2.093
Iter 7/2000 - Loss: -2.881
Iter 8/2000 - Loss: -3.186
Iter 9/2000 - Loss: -2.954
Iter 10/2000 - Loss: -2.622
Iter 11/2000 - Loss: -2.539
Iter 12/2000 - Loss: -2.706
Iter 13/2000 - Loss: -2.932
Iter 14/2000 - Loss: -3.070
Iter 15/2000 - Loss: -3.079
Iter 16/2000 - Loss: -2.987
Iter 17/2000 - Loss: -2.876
Iter 18/2000 - Loss: -2.842
Iter 19/2000 - Loss: -2.922
Iter 20/2000 - Loss: -3.060
Iter 1981/2000 - Loss: -3.313
Iter 1982/2000 - Loss: -3.313
Iter 1983/2000 - Loss: -3.313
Iter 1984/2000 - Loss: -3.313
Iter 1985/2000 - Loss: -3.313
Iter 1986/2000 - Loss: -3.313
Iter 1987/2000 - Loss: -3.313
Iter 1988/2000 - Loss: -3.313
Iter 1989/2000 - Loss: -3.313
Iter 1990/2000 - Loss: -3.313
Iter 1991/2000 - Loss: -3.313
Iter 1992/2000 - Loss: -3.313
Iter 1993/2000 - Loss: -3.313
Iter 1994/2000 - Loss: -3.313
Iter 1995/2000 - Loss: -3.313
Iter 1996/2000 - Loss: -3.313
Iter 1997/2000 - Loss: -3.313
Iter 1998/2000 - Loss: -3.313
Iter 1999/2000 - Loss: -3.313
Iter 2000/2000 - Loss: -3.313
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0022],
        [0.0397],
        [0.0008]])
Lengthscale: tensor([[[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]],

        [[2.8246e-03, 1.5111e-02, 4.2794e-02, 1.7596e-03, 1.6398e-05,
          7.5107e-02]]])
Signal Variance: tensor([0.0014, 0.0087, 0.1650, 0.0031])
Estimated target variance: tensor([0.0018, 0.0110, 0.2077, 0.0040])
N: 70
Signal to noise ratio: tensor([2.0003, 2.0055, 2.0391, 2.0011])
Bound on condition number: tensor([281.0878, 282.5304, 292.0505, 281.3035])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.007744053548453182, policy loss: 6.404293337260388
Experience 7, Iter 1, disc loss: 0.011942323466242692, policy loss: 5.630242925704779
Experience 7, Iter 2, disc loss: 0.012582598850214793, policy loss: 5.547792617862239
Experience 7, Iter 3, disc loss: 0.010729086918734067, policy loss: 5.8805448966775895
Experience 7, Iter 4, disc loss: 0.009012190523806729, policy loss: 5.805070544854885
Experience 7, Iter 5, disc loss: 0.008594839223354916, policy loss: 5.723407794048255
Experience 7, Iter 6, disc loss: 0.006886854736592898, policy loss: 6.00558880370607
Experience 7, Iter 7, disc loss: 0.006068876878734292, policy loss: 6.50361767470764
Experience 7, Iter 8, disc loss: 0.010751803199710287, policy loss: 5.850766347252201
Experience 7, Iter 9, disc loss: 0.0046594694311709465, policy loss: 6.864636099080167
Experience 7, Iter 10, disc loss: 0.008608387748382547, policy loss: 5.799636595103689
Experience 7, Iter 11, disc loss: 0.007690178266337873, policy loss: 6.388118570930644
Experience 7, Iter 12, disc loss: 0.00963028176887679, policy loss: 5.802455920833056
Experience 7, Iter 13, disc loss: 0.01071714312020821, policy loss: 5.682161923355487
Experience 7, Iter 14, disc loss: 0.0074303542166784045, policy loss: 6.193655681158778
Experience 7, Iter 15, disc loss: 0.006540104535626042, policy loss: 6.231155149764053
Experience 7, Iter 16, disc loss: 0.00538984642343787, policy loss: 6.08809450083462
Experience 7, Iter 17, disc loss: 0.006629923040441645, policy loss: 6.337167617201095
Experience 7, Iter 18, disc loss: 0.01076065403272663, policy loss: 6.1231843284013525
Experience 7, Iter 19, disc loss: 0.006665310387293065, policy loss: 6.142955640350755
Experience 7, Iter 20, disc loss: 0.008323652781293125, policy loss: 5.817063494258733
Experience 7, Iter 21, disc loss: 0.0057001539016842614, policy loss: 6.132378316947345
Experience 7, Iter 22, disc loss: 0.007605548912491969, policy loss: 6.194712121946202
Experience 7, Iter 23, disc loss: 0.007937977245655915, policy loss: 6.027056393140786
Experience 7, Iter 24, disc loss: 0.006936614595760218, policy loss: 6.317647287592518
Experience 7, Iter 25, disc loss: 0.0068268296713094095, policy loss: 6.190429630393368
Experience 7, Iter 26, disc loss: 0.0096647992756546, policy loss: 6.085062610423494
Experience 7, Iter 27, disc loss: 0.0056179628829819495, policy loss: 6.402034107903765
Experience 7, Iter 28, disc loss: 0.008356314464240924, policy loss: 6.249065299598023
Experience 7, Iter 29, disc loss: 0.009112538506415625, policy loss: 6.277680827781048
Experience 7, Iter 30, disc loss: 0.008547531653103922, policy loss: 6.290826231746104
Experience 7, Iter 31, disc loss: 0.009557719310330308, policy loss: 6.061229370249828
Experience 7, Iter 32, disc loss: 0.008431066134196119, policy loss: 6.0128702814030435
Experience 7, Iter 33, disc loss: 0.008142020576554672, policy loss: 6.19755836439332
Experience 7, Iter 34, disc loss: 0.011829099490236332, policy loss: 5.705694934264597
Experience 7, Iter 35, disc loss: 0.00593862821137431, policy loss: 6.126006716063636
Experience 7, Iter 36, disc loss: 0.0043876471987996405, policy loss: 6.472086046492693
Experience 7, Iter 37, disc loss: 0.005258752708991891, policy loss: 6.811104532156878
Experience 7, Iter 38, disc loss: 0.006152777613322915, policy loss: 6.2716486354246666
Experience 7, Iter 39, disc loss: 0.006452711771645166, policy loss: 6.363636540879133
Experience 7, Iter 40, disc loss: 0.005150005901361963, policy loss: 6.335585442242861
Experience 7, Iter 41, disc loss: 0.006076533098454617, policy loss: 6.449996821071995
Experience 7, Iter 42, disc loss: 0.007981261596804267, policy loss: 6.199703997683224
Experience 7, Iter 43, disc loss: 0.005908007201545956, policy loss: 6.195028566033935
Experience 7, Iter 44, disc loss: 0.005820720913662124, policy loss: 6.476587389268849
Experience 7, Iter 45, disc loss: 0.0073614076224185416, policy loss: 6.2827801706194295
Experience 7, Iter 46, disc loss: 0.0076163708599829155, policy loss: 6.137552595431936
Experience 7, Iter 47, disc loss: 0.005926121154848911, policy loss: 6.423666035351793
Experience 7, Iter 48, disc loss: 0.00655257858258433, policy loss: 6.389716779058693
Experience 7, Iter 49, disc loss: 0.004472831929965299, policy loss: 6.802009127052112
Experience 7, Iter 50, disc loss: 0.006072371620567449, policy loss: 6.286302752917937
Experience 7, Iter 51, disc loss: 0.004729966908690725, policy loss: 6.43929599620414
Experience 7, Iter 52, disc loss: 0.005451333712366255, policy loss: 6.157422937002025
Experience 7, Iter 53, disc loss: 0.005814260338658152, policy loss: 6.592797752140926
Experience 7, Iter 54, disc loss: 0.0049516761658788, policy loss: 6.391926117275636
Experience 7, Iter 55, disc loss: 0.004927823557996353, policy loss: 6.600402128789396
Experience 7, Iter 56, disc loss: 0.0042839887698003926, policy loss: 6.681553585236334
Experience 7, Iter 57, disc loss: 0.005495576455550012, policy loss: 6.470473655262149
Experience 7, Iter 58, disc loss: 0.004755485979962304, policy loss: 6.548446462309872
Experience 7, Iter 59, disc loss: 0.0052963406513163135, policy loss: 6.457393073922426
Experience 7, Iter 60, disc loss: 0.006107892304765709, policy loss: 6.354536435103483
Experience 7, Iter 61, disc loss: 0.004618302559525802, policy loss: 6.762039697728561
Experience 7, Iter 62, disc loss: 0.003981590905664054, policy loss: 7.075125594815531
Experience 7, Iter 63, disc loss: 0.005713423840200608, policy loss: 6.669308088966542
Experience 7, Iter 64, disc loss: 0.0032389157660228473, policy loss: 6.780050198084039
Experience 7, Iter 65, disc loss: 0.003963933361968493, policy loss: 6.696954135984242
Experience 7, Iter 66, disc loss: 0.004373744140470799, policy loss: 6.548975203426105
Experience 7, Iter 67, disc loss: 0.005425500631120984, policy loss: 6.581252453672047
Experience 7, Iter 68, disc loss: 0.005069517299238927, policy loss: 6.485343251609899
Experience 7, Iter 69, disc loss: 0.0040518661550967795, policy loss: 6.5289406939338175
Experience 7, Iter 70, disc loss: 0.003672862440880901, policy loss: 7.063990390257787
Experience 7, Iter 71, disc loss: 0.00552960316359449, policy loss: 6.429743557004159
Experience 7, Iter 72, disc loss: 0.004713339494870879, policy loss: 6.504765574577197
Experience 7, Iter 73, disc loss: 0.0026911224125366895, policy loss: 7.0444594382684995
Experience 7, Iter 74, disc loss: 0.004788032897849649, policy loss: 6.689912629409012
Experience 7, Iter 75, disc loss: 0.0040952659168044196, policy loss: 6.838833915544255
Experience 7, Iter 76, disc loss: 0.004881157737402955, policy loss: 6.40475925947277
Experience 7, Iter 77, disc loss: 0.004085266726767887, policy loss: 6.291755946577895
Experience 7, Iter 78, disc loss: 0.00463204209309984, policy loss: 6.8473727870754475
Experience 7, Iter 79, disc loss: 0.004858694505761888, policy loss: 6.770274633272818
Experience 7, Iter 80, disc loss: 0.006032675710234344, policy loss: 6.688959469570064
Experience 7, Iter 81, disc loss: 0.004408512392509546, policy loss: 6.968381868724571
Experience 7, Iter 82, disc loss: 0.00433009422073702, policy loss: 6.972415511928744
Experience 7, Iter 83, disc loss: 0.004940499561940482, policy loss: 6.547589552139404
Experience 7, Iter 84, disc loss: 0.005042080824047596, policy loss: 7.187322150248272
Experience 7, Iter 85, disc loss: 0.00607936530962385, policy loss: 6.501965980399797
Experience 7, Iter 86, disc loss: 0.00372131970899275, policy loss: 7.203343544299532
Experience 7, Iter 87, disc loss: 0.0032786441751953914, policy loss: 6.806673812881263
Experience 7, Iter 88, disc loss: 0.003265444639272118, policy loss: 7.251817934997227
Experience 7, Iter 89, disc loss: 0.0045613588136468285, policy loss: 6.635550186455196
Experience 7, Iter 90, disc loss: 0.003216358714057465, policy loss: 7.203470270316712
Experience 7, Iter 91, disc loss: 0.00380479121014298, policy loss: 6.943964289644637
Experience 7, Iter 92, disc loss: 0.003887134982183722, policy loss: 7.089123368058293
Experience 7, Iter 93, disc loss: 0.004790123050278634, policy loss: 6.529342524068559
Experience 7, Iter 94, disc loss: 0.004174777617651203, policy loss: 6.722213212792242
Experience 7, Iter 95, disc loss: 0.0031080065674359003, policy loss: 7.138368977467665
Experience 7, Iter 96, disc loss: 0.00289945576120203, policy loss: 7.253506734316256
Experience 7, Iter 97, disc loss: 0.005207897432260948, policy loss: 6.213975642267419
Experience 7, Iter 98, disc loss: 0.004811616144613784, policy loss: 7.036610588435421
Experience 7, Iter 99, disc loss: 0.0028040239062704342, policy loss: 7.2694415618994945
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0027],
        [0.0472],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0020, 0.0107, 0.1888, 0.0037], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0020, 0.0107, 0.1888, 0.0037])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.199
Iter 2/2000 - Loss: -0.625
Iter 3/2000 - Loss: -3.151
Iter 4/2000 - Loss: -2.826
Iter 5/2000 - Loss: -1.954
Iter 6/2000 - Loss: -2.321
Iter 7/2000 - Loss: -3.022
Iter 8/2000 - Loss: -3.251
Iter 9/2000 - Loss: -3.006
Iter 10/2000 - Loss: -2.725
Iter 11/2000 - Loss: -2.693
Iter 12/2000 - Loss: -2.862
Iter 13/2000 - Loss: -3.058
Iter 14/2000 - Loss: -3.163
Iter 15/2000 - Loss: -3.138
Iter 16/2000 - Loss: -3.027
Iter 17/2000 - Loss: -2.941
Iter 18/2000 - Loss: -2.966
Iter 19/2000 - Loss: -3.084
Iter 20/2000 - Loss: -3.198
Iter 1981/2000 - Loss: -3.374
Iter 1982/2000 - Loss: -3.373
Iter 1983/2000 - Loss: -3.373
Iter 1984/2000 - Loss: -3.374
Iter 1985/2000 - Loss: -3.373
Iter 1986/2000 - Loss: -3.372
Iter 1987/2000 - Loss: -3.372
Iter 1988/2000 - Loss: -3.369
Iter 1989/2000 - Loss: -3.366
Iter 1990/2000 - Loss: -3.360
Iter 1991/2000 - Loss: -3.350
Iter 1992/2000 - Loss: -3.339
Iter 1993/2000 - Loss: -3.334
Iter 1994/2000 - Loss: -3.342
Iter 1995/2000 - Loss: -3.362
Iter 1996/2000 - Loss: -3.372
Iter 1997/2000 - Loss: -3.363
Iter 1998/2000 - Loss: -3.351
Iter 1999/2000 - Loss: -3.350
Iter 2000/2000 - Loss: -3.358
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0021],
        [0.0361],
        [0.0007]])
Lengthscale: tensor([[[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4701e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]],

        [[3.2195e-03, 1.5908e-02, 3.8704e-02, 1.6477e-03, 1.4810e-05,
          7.4700e-02]]])
Signal Variance: tensor([0.0015, 0.0084, 0.1497, 0.0029])
Estimated target variance: tensor([0.0020, 0.0107, 0.1888, 0.0037])
N: 80
Signal to noise ratio: tensor([2.0004, 2.0084, 2.0351, 2.0010])
Bound on condition number: tensor([321.1288, 323.6891, 332.3289, 321.3187])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.005363269422864933, policy loss: 6.3130892473345455
Experience 8, Iter 1, disc loss: 0.005020487695372689, policy loss: 6.526203220856041
Experience 8, Iter 2, disc loss: 0.004475958462559164, policy loss: 6.430462791554573
Experience 8, Iter 3, disc loss: 0.0047413569946124405, policy loss: 6.378087998980202
Experience 8, Iter 4, disc loss: 0.0061036542945012125, policy loss: 6.2298808599639495
Experience 8, Iter 5, disc loss: 0.0047947656099743725, policy loss: 6.614899114720645
Experience 8, Iter 6, disc loss: 0.005514546845881942, policy loss: 6.482687077361443
Experience 8, Iter 7, disc loss: 0.005615423358678203, policy loss: 6.21241473391154
Experience 8, Iter 8, disc loss: 0.006012446778477551, policy loss: 6.079559584687354
Experience 8, Iter 9, disc loss: 0.004888900565239468, policy loss: 6.658794390937733
Experience 8, Iter 10, disc loss: 0.0043207143483702986, policy loss: 6.4267614430431035
Experience 8, Iter 11, disc loss: 0.004797463104743605, policy loss: 6.43803082885977
Experience 8, Iter 12, disc loss: 0.004005242223876584, policy loss: 6.680754464246372
Experience 8, Iter 13, disc loss: 0.005267014835323753, policy loss: 7.188306883229121
Experience 8, Iter 14, disc loss: 0.004923198500030641, policy loss: 6.936797290394871
Experience 8, Iter 15, disc loss: 0.0031145143485289827, policy loss: 6.878584903494531
Experience 8, Iter 16, disc loss: 0.003531347435139478, policy loss: 6.926003110181898
Experience 8, Iter 17, disc loss: 0.0034349727205019824, policy loss: 6.6540179440711515
Experience 8, Iter 18, disc loss: 0.004113362749614264, policy loss: 6.46692004588717
Experience 8, Iter 19, disc loss: 0.003212868092395574, policy loss: 7.160663831645973
Experience 8, Iter 20, disc loss: 0.003168015840970299, policy loss: 6.879995423124248
Experience 8, Iter 21, disc loss: 0.004135249424294384, policy loss: 6.9887724139518035
Experience 8, Iter 22, disc loss: 0.006164314843329519, policy loss: 6.539800948329509
Experience 8, Iter 23, disc loss: 0.004568492834678146, policy loss: 6.541008726246245
Experience 8, Iter 24, disc loss: 0.004283004371863006, policy loss: 6.924308142237596
Experience 8, Iter 25, disc loss: 0.0038923472533887895, policy loss: 6.641440092427908
Experience 8, Iter 26, disc loss: 0.0037267669537307145, policy loss: 7.010332461598315
Experience 8, Iter 27, disc loss: 0.0023948046405900037, policy loss: 7.113939350873167
Experience 8, Iter 28, disc loss: 0.003000425711171078, policy loss: 7.24448436879912
Experience 8, Iter 29, disc loss: 0.002950731030454376, policy loss: 7.157508595610589
Experience 8, Iter 30, disc loss: 0.004564477864629827, policy loss: 6.9151404122354165
Experience 8, Iter 31, disc loss: 0.003288555092777896, policy loss: 6.710278648836321
Experience 8, Iter 32, disc loss: 0.00485165142013857, policy loss: 6.9981373467061365
Experience 8, Iter 33, disc loss: 0.004372037171108532, policy loss: 6.880969162181227
Experience 8, Iter 34, disc loss: 0.004610351058565067, policy loss: 6.768929128915692
Experience 8, Iter 35, disc loss: 0.003423486058885452, policy loss: 6.53927849194595
Experience 8, Iter 36, disc loss: 0.003682555409808668, policy loss: 6.92575576279344
Experience 8, Iter 37, disc loss: 0.004925708375055405, policy loss: 6.266616841005831
Experience 8, Iter 38, disc loss: 0.005835179544623217, policy loss: 6.4992740847664665
Experience 8, Iter 39, disc loss: 0.0038965853442786665, policy loss: 6.918868401256168
Experience 8, Iter 40, disc loss: 0.0028514924209246516, policy loss: 6.9296470485475545
Experience 8, Iter 41, disc loss: 0.004148087812135375, policy loss: 6.678306952902627
Experience 8, Iter 42, disc loss: 0.0043387090777658325, policy loss: 7.064334202037741
Experience 8, Iter 43, disc loss: 0.003038343575635842, policy loss: 7.044434548801569
Experience 8, Iter 44, disc loss: 0.0029388402859244008, policy loss: 6.957201684690502
Experience 8, Iter 45, disc loss: 0.0029781955312042146, policy loss: 6.884451420248109
Experience 8, Iter 46, disc loss: 0.0026892730085339494, policy loss: 7.177441022707283
Experience 8, Iter 47, disc loss: 0.004421582432282322, policy loss: 6.857662478785291
Experience 8, Iter 48, disc loss: 0.002607788575206306, policy loss: 7.167604242477615
Experience 8, Iter 49, disc loss: 0.0039551264953810415, policy loss: 7.0406483399136235
Experience 8, Iter 50, disc loss: 0.0034309525531613792, policy loss: 6.870780562460436
Experience 8, Iter 51, disc loss: 0.0031625893260880315, policy loss: 6.848914848195607
Experience 8, Iter 52, disc loss: 0.003115478291679364, policy loss: 7.115206797711844
Experience 8, Iter 53, disc loss: 0.0028088768350600325, policy loss: 7.002462942215342
Experience 8, Iter 54, disc loss: 0.0032539305278397076, policy loss: 6.983107515148554
Experience 8, Iter 55, disc loss: 0.00225672443579214, policy loss: 7.4571147401001685
Experience 8, Iter 56, disc loss: 0.0035769211145450057, policy loss: 7.032440170164315
Experience 8, Iter 57, disc loss: 0.004199375997362748, policy loss: 6.679976898205762
Experience 8, Iter 58, disc loss: 0.0032152163174454934, policy loss: 6.6464310617544395
Experience 8, Iter 59, disc loss: 0.003311007573640659, policy loss: 6.846938155319069
Experience 8, Iter 60, disc loss: 0.003524594742224827, policy loss: 6.646378975532268
Experience 8, Iter 61, disc loss: 0.0022995701817875034, policy loss: 7.485853328487586
Experience 8, Iter 62, disc loss: 0.0028395621248040134, policy loss: 6.889316610333243
Experience 8, Iter 63, disc loss: 0.0023467584685140552, policy loss: 7.269182979276956
Experience 8, Iter 64, disc loss: 0.0025838584492761073, policy loss: 7.311272147622834
Experience 8, Iter 65, disc loss: 0.0034393405248175036, policy loss: 7.102505728569048
Experience 8, Iter 66, disc loss: 0.0029760863527292336, policy loss: 6.795356474853158
Experience 8, Iter 67, disc loss: 0.003491456629094075, policy loss: 6.9583713135832905
Experience 8, Iter 68, disc loss: 0.003061768256684121, policy loss: 6.993725329833146
Experience 8, Iter 69, disc loss: 0.0021644740438711684, policy loss: 7.242462621983837
Experience 8, Iter 70, disc loss: 0.004316912087340333, policy loss: 6.590085207733596
Experience 8, Iter 71, disc loss: 0.0026722432775886395, policy loss: 6.9831361066065725
Experience 8, Iter 72, disc loss: 0.0026681436961502906, policy loss: 7.3775046145020085
Experience 8, Iter 73, disc loss: 0.0034650731929286548, policy loss: 6.846954939570374
Experience 8, Iter 74, disc loss: 0.0031765874524702447, policy loss: 7.059834822770949
Experience 8, Iter 75, disc loss: 0.0027171648560122006, policy loss: 7.152316328335784
Experience 8, Iter 76, disc loss: 0.0023586234038534197, policy loss: 7.127787277113427
Experience 8, Iter 77, disc loss: 0.0030246368567551493, policy loss: 6.980919197193893
Experience 8, Iter 78, disc loss: 0.0027595176527901466, policy loss: 7.3043481538505795
Experience 8, Iter 79, disc loss: 0.0027332183708387555, policy loss: 7.2823868829780665
Experience 8, Iter 80, disc loss: 0.002366524395920888, policy loss: 7.316166810649408
Experience 8, Iter 81, disc loss: 0.002562760380521682, policy loss: 7.262370578982411
Experience 8, Iter 82, disc loss: 0.001709661359256633, policy loss: 7.617129602273534
Experience 8, Iter 83, disc loss: 0.003098178714877764, policy loss: 6.802296888274347
Experience 8, Iter 84, disc loss: 0.004641129411185999, policy loss: 7.6386228583143
Experience 8, Iter 85, disc loss: 0.002928737626927636, policy loss: 6.8715340314047335
Experience 8, Iter 86, disc loss: 0.002050954834572838, policy loss: 7.445747406524614
Experience 8, Iter 87, disc loss: 0.002289365259889469, policy loss: 7.161425269317646
Experience 8, Iter 88, disc loss: 0.0015021624456546799, policy loss: 7.514887410845927
Experience 8, Iter 89, disc loss: 0.003362752752029162, policy loss: 7.000831013566639
Experience 8, Iter 90, disc loss: 0.002413944148790555, policy loss: 7.00805457060265
Experience 8, Iter 91, disc loss: 0.002410790331211452, policy loss: 7.052382210507247
Experience 8, Iter 92, disc loss: 0.002246123367903535, policy loss: 7.363064331919526
Experience 8, Iter 93, disc loss: 0.0025428357875608644, policy loss: 7.438752020859092
Experience 8, Iter 94, disc loss: 0.0019144250429990975, policy loss: 7.489760602522898
Experience 8, Iter 95, disc loss: 0.003481944996009428, policy loss: 7.222664787104253
Experience 8, Iter 96, disc loss: 0.0019087636693662345, policy loss: 7.682442490258461
Experience 8, Iter 97, disc loss: 0.002996094490719449, policy loss: 7.459322014708886
Experience 8, Iter 98, disc loss: 0.0023035477810420136, policy loss: 7.351106217722753
Experience 8, Iter 99, disc loss: 0.0027314579594461784, policy loss: 7.742611087218002
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0031],
        [0.0647],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.9679e-03, 2.0523e-02, 5.4457e-02, 2.0104e-03, 2.8668e-05,
          8.2730e-02]],

        [[3.9679e-03, 2.0523e-02, 5.4457e-02, 2.0104e-03, 2.8668e-05,
          8.2730e-02]],

        [[3.9679e-03, 2.0523e-02, 5.4457e-02, 2.0104e-03, 2.8668e-05,
          8.2730e-02]],

        [[3.9679e-03, 2.0523e-02, 5.4457e-02, 2.0104e-03, 2.8668e-05,
          8.2730e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0023, 0.0125, 0.2588, 0.0051], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0023, 0.0125, 0.2588, 0.0051])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.642
Iter 2/2000 - Loss: -1.122
Iter 3/2000 - Loss: -2.617
Iter 4/2000 - Loss: -2.584
Iter 5/2000 - Loss: -1.994
Iter 6/2000 - Loss: -2.137
Iter 7/2000 - Loss: -2.586
Iter 8/2000 - Loss: -2.753
Iter 9/2000 - Loss: -2.589
Iter 10/2000 - Loss: -2.408
Iter 11/2000 - Loss: -2.415
Iter 12/2000 - Loss: -2.556
Iter 13/2000 - Loss: -2.680
Iter 14/2000 - Loss: -2.708
Iter 15/2000 - Loss: -2.663
Iter 16/2000 - Loss: -2.627
Iter 17/2000 - Loss: -2.650
Iter 18/2000 - Loss: -2.710
Iter 19/2000 - Loss: -2.753
Iter 20/2000 - Loss: -2.761
Iter 1981/2000 - Loss: -5.350
Iter 1982/2000 - Loss: -5.350
Iter 1983/2000 - Loss: -5.350
Iter 1984/2000 - Loss: -5.350
Iter 1985/2000 - Loss: -5.350
Iter 1986/2000 - Loss: -5.350
Iter 1987/2000 - Loss: -5.350
Iter 1988/2000 - Loss: -5.350
Iter 1989/2000 - Loss: -5.350
Iter 1990/2000 - Loss: -5.350
Iter 1991/2000 - Loss: -5.350
Iter 1992/2000 - Loss: -5.350
Iter 1993/2000 - Loss: -5.350
Iter 1994/2000 - Loss: -5.350
Iter 1995/2000 - Loss: -5.350
Iter 1996/2000 - Loss: -5.350
Iter 1997/2000 - Loss: -5.350
Iter 1998/2000 - Loss: -5.350
Iter 1999/2000 - Loss: -5.350
Iter 2000/2000 - Loss: -5.350
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0493],
        [0.0010]])
Lengthscale: tensor([[[1.2496e+01, 1.7088e+00, 2.4189e+01, 1.4737e+01, 4.6953e+00,
          3.2437e+01]],

        [[1.9314e+01, 2.3452e+01, 2.1004e+01, 1.8824e+00, 4.8912e+00,
          8.4005e+00]],

        [[3.9636e-03, 1.9836e-02, 5.4457e-02, 2.0102e-03, 2.8616e-05,
          8.2558e-02]],

        [[3.9559e-03, 1.8579e-02, 5.4457e-02, 2.0097e-03, 2.8524e-05,
          8.2253e-02]]])
Signal Variance: tensor([0.0091, 0.4556, 0.2066, 0.0040])
Estimated target variance: tensor([0.0023, 0.0125, 0.2588, 0.0051])
N: 90
Signal to noise ratio: tensor([ 5.1219, 38.4289,  2.0478,  2.0017])
Bound on condition number: tensor([  2362.0262, 132910.9295,    378.4192,    361.6064])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.01260750110488412, policy loss: 5.31297319178659
Experience 9, Iter 1, disc loss: 0.00949952817007948, policy loss: 5.509131699503383
Experience 9, Iter 2, disc loss: 0.013770973913404866, policy loss: 5.097680288753626
Experience 9, Iter 3, disc loss: 0.011487330817879041, policy loss: 5.686532262799037
Experience 9, Iter 4, disc loss: 0.01855976364256892, policy loss: 4.929068262897096
Experience 9, Iter 5, disc loss: 0.01230522961129886, policy loss: 5.322159834777567
Experience 9, Iter 6, disc loss: 0.010725922613502498, policy loss: 5.349014158280853
Experience 9, Iter 7, disc loss: 0.014132061801827777, policy loss: 5.34919004310957
Experience 9, Iter 8, disc loss: 0.011271649052642087, policy loss: 5.289728121328489
Experience 9, Iter 9, disc loss: 0.01589606385947611, policy loss: 4.8953169842929025
Experience 9, Iter 10, disc loss: 0.012534354864992176, policy loss: 5.062793182119188
Experience 9, Iter 11, disc loss: 0.011939832621009587, policy loss: 5.101978976392942
Experience 9, Iter 12, disc loss: 0.01215373560592165, policy loss: 5.074283430733279
Experience 9, Iter 13, disc loss: 0.014188033370884595, policy loss: 4.963802124154016
Experience 9, Iter 14, disc loss: 0.010099400010646742, policy loss: 5.447331515258858
Experience 9, Iter 15, disc loss: 0.01024191661493852, policy loss: 5.353961160329925
Experience 9, Iter 16, disc loss: 0.0107331150053123, policy loss: 5.284421735898258
Experience 9, Iter 17, disc loss: 0.011583629321600493, policy loss: 5.14973646666742
Experience 9, Iter 18, disc loss: 0.009095842125324381, policy loss: 5.505425864326572
Experience 9, Iter 19, disc loss: 0.009583286527106459, policy loss: 5.494052774752991
Experience 9, Iter 20, disc loss: 0.009547289410937443, policy loss: 5.617986567148153
Experience 9, Iter 21, disc loss: 0.013640443271425828, policy loss: 5.183186728702987
Experience 9, Iter 22, disc loss: 0.011715694115154917, policy loss: 5.2918975465391735
Experience 9, Iter 23, disc loss: 0.007526606523133325, policy loss: 5.740701075059882
Experience 9, Iter 24, disc loss: 0.012658288390856132, policy loss: 5.6341423059115545
Experience 9, Iter 25, disc loss: 0.00931466352302333, policy loss: 5.493576033013211
Experience 9, Iter 26, disc loss: 0.010459090756604058, policy loss: 5.451811413209956
Experience 9, Iter 27, disc loss: 0.007018352945641802, policy loss: 5.80550096988001
Experience 9, Iter 28, disc loss: 0.0077078342469947825, policy loss: 5.7036346789156065
Experience 9, Iter 29, disc loss: 0.007085879618893576, policy loss: 5.874227308549905
Experience 9, Iter 30, disc loss: 0.007838577787078272, policy loss: 5.90312660136005
Experience 9, Iter 31, disc loss: 0.009818652680330355, policy loss: 5.748695922623229
Experience 9, Iter 32, disc loss: 0.006788124752765639, policy loss: 5.732259627461233
Experience 9, Iter 33, disc loss: 0.006092960484075603, policy loss: 5.677416232637961
Experience 9, Iter 34, disc loss: 0.005470199898086374, policy loss: 6.05097092266662
Experience 9, Iter 35, disc loss: 0.008016615383624809, policy loss: 5.649890860443722
Experience 9, Iter 36, disc loss: 0.006493877466686256, policy loss: 5.77066469596555
Experience 9, Iter 37, disc loss: 0.005844956324092304, policy loss: 5.931227533025455
Experience 9, Iter 38, disc loss: 0.00528581177974093, policy loss: 5.855348481907921
Experience 9, Iter 39, disc loss: 0.006461120720789021, policy loss: 5.921524956131824
Experience 9, Iter 40, disc loss: 0.004559027039757856, policy loss: 5.9801659468331465
Experience 9, Iter 41, disc loss: 0.004687879175424864, policy loss: 6.237666238501718
Experience 9, Iter 42, disc loss: 0.006648949256093029, policy loss: 5.6720942484475465
Experience 9, Iter 43, disc loss: 0.005491354439595364, policy loss: 5.884431991180396
Experience 9, Iter 44, disc loss: 0.0048010968705512365, policy loss: 6.031177698805015
Experience 9, Iter 45, disc loss: 0.004088009437793587, policy loss: 6.360680680213506
Experience 9, Iter 46, disc loss: 0.003307011822065625, policy loss: 6.615462817912068
Experience 9, Iter 47, disc loss: 0.003981981953343717, policy loss: 6.170164602104417
Experience 9, Iter 48, disc loss: 0.0038576675434418657, policy loss: 6.18677614878707
Experience 9, Iter 49, disc loss: 0.004492507573914894, policy loss: 6.153818211452179
Experience 9, Iter 50, disc loss: 0.003865019049085991, policy loss: 6.181084553136194
Experience 9, Iter 51, disc loss: 0.0031975788130217996, policy loss: 6.4853598629268845
Experience 9, Iter 52, disc loss: 0.0037494736653546873, policy loss: 6.32759615598225
Experience 9, Iter 53, disc loss: 0.0037233382445898177, policy loss: 6.337262244891388
Experience 9, Iter 54, disc loss: 0.002914426115591174, policy loss: 6.382622405604559
Experience 9, Iter 55, disc loss: 0.004120763524996343, policy loss: 6.237402829404761
Experience 9, Iter 56, disc loss: 0.0040913196300761834, policy loss: 6.151430378607753
Experience 9, Iter 57, disc loss: 0.0031724733126845396, policy loss: 6.620728417473588
Experience 9, Iter 58, disc loss: 0.003370419820705436, policy loss: 6.319331228541638
Experience 9, Iter 59, disc loss: 0.0036577828280246034, policy loss: 6.363951959100027
Experience 9, Iter 60, disc loss: 0.0038141581648985542, policy loss: 6.609240135252666
Experience 9, Iter 61, disc loss: 0.0031111656483071527, policy loss: 6.489025927185477
Experience 9, Iter 62, disc loss: 0.0028494271312899263, policy loss: 6.62876424160406
Experience 9, Iter 63, disc loss: 0.0033725682861571065, policy loss: 6.569374112741667
Experience 9, Iter 64, disc loss: 0.003340723332699305, policy loss: 6.396815537408559
Experience 9, Iter 65, disc loss: 0.00257619218635949, policy loss: 6.598313885947494
Experience 9, Iter 66, disc loss: 0.0034325131589329856, policy loss: 6.463335142416225
Experience 9, Iter 67, disc loss: 0.004117139895916929, policy loss: 6.220303033577997
Experience 9, Iter 68, disc loss: 0.0033764816703033217, policy loss: 6.479170290025956
Experience 9, Iter 69, disc loss: 0.002278195589073443, policy loss: 6.986773536313261
Experience 9, Iter 70, disc loss: 0.0025724823170694, policy loss: 6.730627076641264
Experience 9, Iter 71, disc loss: 0.0036039657986986933, policy loss: 6.318578741783776
Experience 9, Iter 72, disc loss: 0.0028080584618585007, policy loss: 6.604460047706179
Experience 9, Iter 73, disc loss: 0.0023250336905371195, policy loss: 6.7935249016657036
Experience 9, Iter 74, disc loss: 0.0034988166092125073, policy loss: 6.616132021125988
Experience 9, Iter 75, disc loss: 0.0031394672727723793, policy loss: 6.813986783463877
Experience 9, Iter 76, disc loss: 0.002620628555801954, policy loss: 6.84121151068285
Experience 9, Iter 77, disc loss: 0.0025390079277457967, policy loss: 6.731433828990387
Experience 9, Iter 78, disc loss: 0.0025482256695809023, policy loss: 6.686877975713873
Experience 9, Iter 79, disc loss: 0.002427609359050731, policy loss: 6.616240155343195
Experience 9, Iter 80, disc loss: 0.0022406577377339166, policy loss: 6.71235824332259
Experience 9, Iter 81, disc loss: 0.002945868329142723, policy loss: 6.41007985257324
Experience 9, Iter 82, disc loss: 0.0020157351467081364, policy loss: 6.9833358659817275
Experience 9, Iter 83, disc loss: 0.002846830964220319, policy loss: 6.483928561752983
Experience 9, Iter 84, disc loss: 0.002167151069435293, policy loss: 6.910446013699675
Experience 9, Iter 85, disc loss: 0.0022107410069771975, policy loss: 6.8565182955081205
Experience 9, Iter 86, disc loss: 0.002051234348592512, policy loss: 6.875082928054512
Experience 9, Iter 87, disc loss: 0.002568747148706028, policy loss: 6.761622713607641
Experience 9, Iter 88, disc loss: 0.0020200721169922726, policy loss: 6.85096188076113
Experience 9, Iter 89, disc loss: 0.001658009132560187, policy loss: 7.340736856373855
Experience 9, Iter 90, disc loss: 0.0025398701347941113, policy loss: 6.818723592471171
Experience 9, Iter 91, disc loss: 0.002238757996517771, policy loss: 7.040508935618777
Experience 9, Iter 92, disc loss: 0.0023028669466725136, policy loss: 6.851016882161023
Experience 9, Iter 93, disc loss: 0.00195751954716856, policy loss: 6.897212607110779
Experience 9, Iter 94, disc loss: 0.0023105340034296627, policy loss: 6.735787127361771
Experience 9, Iter 95, disc loss: 0.0023892604442070347, policy loss: 6.4911147679702665
Experience 9, Iter 96, disc loss: 0.001886132511190364, policy loss: 6.990959925211477
Experience 9, Iter 97, disc loss: 0.0021410278168935777, policy loss: 6.738170413334152
Experience 9, Iter 98, disc loss: 0.0021314549355029217, policy loss: 6.87667775498709
Experience 9, Iter 99, disc loss: 0.0019548383475046665, policy loss: 6.840027276815651
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0251],
        [0.2655],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.7024e-03, 2.9677e-02, 7.2356e-02, 2.0803e-03, 2.9684e-05,
          4.8129e-01]],

        [[5.7024e-03, 2.9677e-02, 7.2356e-02, 2.0803e-03, 2.9684e-05,
          4.8129e-01]],

        [[5.7024e-03, 2.9677e-02, 7.2356e-02, 2.0803e-03, 2.9684e-05,
          4.8129e-01]],

        [[5.7024e-03, 2.9677e-02, 7.2356e-02, 2.0803e-03, 2.9684e-05,
          4.8129e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.1003, 1.0620, 0.0048], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.1003, 1.0620, 0.0048])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.829
Iter 2/2000 - Loss: 0.339
Iter 3/2000 - Loss: -0.812
Iter 4/2000 - Loss: -0.816
Iter 5/2000 - Loss: -0.360
Iter 6/2000 - Loss: -0.432
Iter 7/2000 - Loss: -0.779
Iter 8/2000 - Loss: -0.947
Iter 9/2000 - Loss: -0.839
Iter 10/2000 - Loss: -0.676
Iter 11/2000 - Loss: -0.673
Iter 12/2000 - Loss: -0.801
Iter 13/2000 - Loss: -0.905
Iter 14/2000 - Loss: -0.912
Iter 15/2000 - Loss: -0.887
Iter 16/2000 - Loss: -0.896
Iter 17/2000 - Loss: -0.914
Iter 18/2000 - Loss: -0.902
Iter 19/2000 - Loss: -0.911
Iter 20/2000 - Loss: -0.980
Iter 1981/2000 - Loss: -8.475
Iter 1982/2000 - Loss: -8.475
Iter 1983/2000 - Loss: -8.475
Iter 1984/2000 - Loss: -8.475
Iter 1985/2000 - Loss: -8.475
Iter 1986/2000 - Loss: -8.475
Iter 1987/2000 - Loss: -8.475
Iter 1988/2000 - Loss: -8.475
Iter 1989/2000 - Loss: -8.475
Iter 1990/2000 - Loss: -8.475
Iter 1991/2000 - Loss: -8.475
Iter 1992/2000 - Loss: -8.475
Iter 1993/2000 - Loss: -8.475
Iter 1994/2000 - Loss: -8.475
Iter 1995/2000 - Loss: -8.475
Iter 1996/2000 - Loss: -8.475
Iter 1997/2000 - Loss: -8.475
Iter 1998/2000 - Loss: -8.475
Iter 1999/2000 - Loss: -8.475
Iter 2000/2000 - Loss: -8.475
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[13.5427,  2.6682, 28.7076,  9.9085,  0.0998, 23.5837]],

        [[22.0838, 40.8881, 32.3345,  3.3397,  6.6843, 30.4122]],

        [[18.8472, 31.9352, 27.8028,  2.2141,  6.0638, 29.6893]],

        [[19.9149, 45.2350,  9.3260,  2.7053,  6.7822, 31.2588]]])
Signal Variance: tensor([1.6949e-02, 3.4467e+00, 3.1155e+01, 2.0138e-01])
Estimated target variance: tensor([0.0029, 0.1003, 1.0620, 0.0048])
N: 100
Signal to noise ratio: tensor([  7.1803, 103.6224, 110.8788,  25.7385])
Bound on condition number: tensor([   5156.6627, 1073760.1895, 1229412.0348,   66247.8136])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0005266666726693753, policy loss: 8.90110334489078
Experience 10, Iter 1, disc loss: 0.0004637020432571669, policy loss: 8.819631469672602
Experience 10, Iter 2, disc loss: 0.0005739700970652378, policy loss: 8.294786740811158
Experience 10, Iter 3, disc loss: 0.0005975020021183166, policy loss: 8.20935742387008
Experience 10, Iter 4, disc loss: 0.0006315382872590051, policy loss: 8.0171496642212
Experience 10, Iter 5, disc loss: 0.0005620663021148881, policy loss: 8.246779062383323
Experience 10, Iter 6, disc loss: 0.0005350558377293375, policy loss: 8.325299749761873
Experience 10, Iter 7, disc loss: 0.0005156026746717114, policy loss: 8.335592579203652
Experience 10, Iter 8, disc loss: 0.00048002448810110707, policy loss: 8.480548267886753
Experience 10, Iter 9, disc loss: 0.0005055192692843477, policy loss: 8.468544967580154
Experience 10, Iter 10, disc loss: 0.0004423970726643261, policy loss: 8.598785642356678
Experience 10, Iter 11, disc loss: 0.0004856957887083655, policy loss: 8.461296477936333
Experience 10, Iter 12, disc loss: 0.0005191319296615758, policy loss: 8.292213058337836
Experience 10, Iter 13, disc loss: 0.0006209459791906633, policy loss: 8.089464537466046
Experience 10, Iter 14, disc loss: 0.0007818410476107641, policy loss: 7.734096562969397
Experience 10, Iter 15, disc loss: 0.0008659591027999848, policy loss: 7.534860286694392
Experience 10, Iter 16, disc loss: 0.000829907188169735, policy loss: 7.707290009308644
Experience 10, Iter 17, disc loss: 0.0008571150686246328, policy loss: 7.542486152684372
Experience 10, Iter 18, disc loss: 0.0012730082972100295, policy loss: 7.216487683680398
Experience 10, Iter 19, disc loss: 0.0012546226821737593, policy loss: 7.0706618898730405
Experience 10, Iter 20, disc loss: 0.00136124119555503, policy loss: 7.128191870838967
Experience 10, Iter 21, disc loss: 0.0013097357954693293, policy loss: 7.118842191989389
Experience 10, Iter 22, disc loss: 0.0011155473507550124, policy loss: 7.284441142431342
Experience 10, Iter 23, disc loss: 0.0010343556858339342, policy loss: 7.340942644516565
Experience 10, Iter 24, disc loss: 0.001040844357634689, policy loss: 7.386038449593769
Experience 10, Iter 25, disc loss: 0.0015048606370039544, policy loss: 6.883767396143648
Experience 10, Iter 26, disc loss: 0.0012463149100073174, policy loss: 7.159606301982018
Experience 10, Iter 27, disc loss: 0.0015544617243431608, policy loss: 7.001023110692563
Experience 10, Iter 28, disc loss: 0.0012479894045731078, policy loss: 7.177576344048134
Experience 10, Iter 29, disc loss: 0.0017878729065780993, policy loss: 6.796736055380409
Experience 10, Iter 30, disc loss: 0.0018114288759893673, policy loss: 6.699364561548782
Experience 10, Iter 31, disc loss: 0.0016472353751384903, policy loss: 6.812012889263331
Experience 10, Iter 32, disc loss: 0.0013270850015346407, policy loss: 7.166963873324609
Experience 10, Iter 33, disc loss: 0.0016011379227796193, policy loss: 6.890048526640793
Experience 10, Iter 34, disc loss: 0.001708907212447774, policy loss: 6.805894562600831
Experience 10, Iter 35, disc loss: 0.0015165069096403844, policy loss: 6.83200550251222
Experience 10, Iter 36, disc loss: 0.001668107904106324, policy loss: 6.763674286157647
Experience 10, Iter 37, disc loss: 0.001473442312100322, policy loss: 6.890262323547615
Experience 10, Iter 38, disc loss: 0.0014357378941930047, policy loss: 6.8287177796354674
Experience 10, Iter 39, disc loss: 0.0015322267673204956, policy loss: 6.775188359655432
Experience 10, Iter 40, disc loss: 0.0013450846374950794, policy loss: 6.9292174211178885
Experience 10, Iter 41, disc loss: 0.0016321642641743406, policy loss: 6.69707051030297
Experience 10, Iter 42, disc loss: 0.0015820769375751431, policy loss: 6.716476577693214
Experience 10, Iter 43, disc loss: 0.0014217283512618938, policy loss: 6.843875671366339
Experience 10, Iter 44, disc loss: 0.0015650769986046854, policy loss: 6.682846186130324
Experience 10, Iter 45, disc loss: 0.0015698651346115556, policy loss: 6.6967358421108925
Experience 10, Iter 46, disc loss: 0.0014271366495521298, policy loss: 6.854066587019331
Experience 10, Iter 47, disc loss: 0.0016419876259945773, policy loss: 6.675134557228375
Experience 10, Iter 48, disc loss: 0.001508340650239479, policy loss: 6.8810482012034155
Experience 10, Iter 49, disc loss: 0.001594843354188865, policy loss: 6.706065272911156
Experience 10, Iter 50, disc loss: 0.0015921184912125307, policy loss: 6.69474879974762
Experience 10, Iter 51, disc loss: 0.0015123545697150373, policy loss: 6.7555286481960275
Experience 10, Iter 52, disc loss: 0.0014319274539295384, policy loss: 6.824986504196491
Experience 10, Iter 53, disc loss: 0.001500545537613748, policy loss: 6.793929942441221
Experience 10, Iter 54, disc loss: 0.0015148397766086568, policy loss: 6.757629117409092
Experience 10, Iter 55, disc loss: 0.0014916529567009875, policy loss: 6.759375285772929
Experience 10, Iter 56, disc loss: 0.0013550043567543421, policy loss: 6.876527768704956
Experience 10, Iter 57, disc loss: 0.0014121905103050083, policy loss: 6.875783556769526
Experience 10, Iter 58, disc loss: 0.001514570391496245, policy loss: 6.724812289343436
Experience 10, Iter 59, disc loss: 0.0015148220710779533, policy loss: 6.710676532250199
Experience 10, Iter 60, disc loss: 0.0013201122046928387, policy loss: 6.942568765334409
Experience 10, Iter 61, disc loss: 0.001613312949921684, policy loss: 6.696750876324287
Experience 10, Iter 62, disc loss: 0.001424363550116784, policy loss: 6.831793196887876
Experience 10, Iter 63, disc loss: 0.0016168397623521427, policy loss: 6.673241596805966
Experience 10, Iter 64, disc loss: 0.001430820861873597, policy loss: 6.808045373650904
Experience 10, Iter 65, disc loss: 0.0015188921116919417, policy loss: 6.754857114760263
Experience 10, Iter 66, disc loss: 0.0012595897081485096, policy loss: 7.046917377223445
Experience 10, Iter 67, disc loss: 0.0014220925641682943, policy loss: 6.908945306642631
Experience 10, Iter 68, disc loss: 0.001594967386418651, policy loss: 6.651749623876427
Experience 10, Iter 69, disc loss: 0.0016534207404809399, policy loss: 6.589032877243052
Experience 10, Iter 70, disc loss: 0.0014908001155641655, policy loss: 6.778594517066744
Experience 10, Iter 71, disc loss: 0.0016084254510137052, policy loss: 6.670500241412968
Experience 10, Iter 72, disc loss: 0.0015375694213224098, policy loss: 6.708805015809221
Experience 10, Iter 73, disc loss: 0.0015140033011375085, policy loss: 6.774752352537207
Experience 10, Iter 74, disc loss: 0.0014629437637091855, policy loss: 6.753803003799043
Experience 10, Iter 75, disc loss: 0.0014008521367166776, policy loss: 6.867592442849465
Experience 10, Iter 76, disc loss: 0.0014221301268444655, policy loss: 6.8675710804583705
Experience 10, Iter 77, disc loss: 0.0014083665116125898, policy loss: 6.772299614530474
Experience 10, Iter 78, disc loss: 0.0014052736706365002, policy loss: 6.778176286750005
Experience 10, Iter 79, disc loss: 0.0014938115346018902, policy loss: 6.7835813566035545
Experience 10, Iter 80, disc loss: 0.0014352098782547651, policy loss: 6.896186560314604
Experience 10, Iter 81, disc loss: 0.0014412388315005026, policy loss: 6.805880451401878
Experience 10, Iter 82, disc loss: 0.0013244255065216007, policy loss: 6.940705513595787
Experience 10, Iter 83, disc loss: 0.0014515174472925349, policy loss: 6.8124498891876355
Experience 10, Iter 84, disc loss: 0.0014327791234899966, policy loss: 6.928088751485655
Experience 10, Iter 85, disc loss: 0.0013953916595838371, policy loss: 6.839325492471872
Experience 10, Iter 86, disc loss: 0.0015648691387938221, policy loss: 6.662660271193955
Experience 10, Iter 87, disc loss: 0.0014669318763348965, policy loss: 6.751487733076688
Experience 10, Iter 88, disc loss: 0.0013370011006650947, policy loss: 6.856849904493559
Experience 10, Iter 89, disc loss: 0.001258067625244333, policy loss: 7.001766513979318
Experience 10, Iter 90, disc loss: 0.0013271648005887353, policy loss: 6.886382267645283
Experience 10, Iter 91, disc loss: 0.001304775504247463, policy loss: 6.950149108532914
Experience 10, Iter 92, disc loss: 0.0013489625858358798, policy loss: 6.841904822783832
Experience 10, Iter 93, disc loss: 0.0011985025964513442, policy loss: 7.02713537629157
Experience 10, Iter 94, disc loss: 0.0013198392471141388, policy loss: 6.974442081480026
Experience 10, Iter 95, disc loss: 0.0013553382272579613, policy loss: 6.8971692462242755
Experience 10, Iter 96, disc loss: 0.0012735967512301014, policy loss: 6.947643275112794
Experience 10, Iter 97, disc loss: 0.001334328617047803, policy loss: 6.88890880215864
Experience 10, Iter 98, disc loss: 0.0014067124931638423, policy loss: 6.781610166101803
Experience 10, Iter 99, disc loss: 0.0013045638213658606, policy loss: 6.937695400438141
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0231],
        [0.2449],
        [0.0011]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.3138e-03, 2.7315e-02, 6.6554e-02, 1.9038e-03, 2.7701e-05,
          4.4354e-01]],

        [[5.3138e-03, 2.7315e-02, 6.6554e-02, 1.9038e-03, 2.7701e-05,
          4.4354e-01]],

        [[5.3138e-03, 2.7315e-02, 6.6554e-02, 1.9038e-03, 2.7701e-05,
          4.4354e-01]],

        [[5.3138e-03, 2.7315e-02, 6.6554e-02, 1.9038e-03, 2.7701e-05,
          4.4354e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0027, 0.0926, 0.9794, 0.0044], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0027, 0.0926, 0.9794, 0.0044])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.966
Iter 2/2000 - Loss: 0.283
Iter 3/2000 - Loss: -0.948
Iter 4/2000 - Loss: -0.957
Iter 5/2000 - Loss: -0.471
Iter 6/2000 - Loss: -0.543
Iter 7/2000 - Loss: -0.912
Iter 8/2000 - Loss: -1.095
Iter 9/2000 - Loss: -0.984
Iter 10/2000 - Loss: -0.808
Iter 11/2000 - Loss: -0.794
Iter 12/2000 - Loss: -0.925
Iter 13/2000 - Loss: -1.043
Iter 14/2000 - Loss: -1.058
Iter 15/2000 - Loss: -1.026
Iter 16/2000 - Loss: -1.027
Iter 17/2000 - Loss: -1.049
Iter 18/2000 - Loss: -1.047
Iter 19/2000 - Loss: -1.049
Iter 20/2000 - Loss: -1.109
Iter 1981/2000 - Loss: -5.890
Iter 1982/2000 - Loss: -5.890
Iter 1983/2000 - Loss: -5.890
Iter 1984/2000 - Loss: -5.890
Iter 1985/2000 - Loss: -5.890
Iter 1986/2000 - Loss: -5.890
Iter 1987/2000 - Loss: -5.890
Iter 1988/2000 - Loss: -5.890
Iter 1989/2000 - Loss: -5.890
Iter 1990/2000 - Loss: -5.890
Iter 1991/2000 - Loss: -5.890
Iter 1992/2000 - Loss: -5.890
Iter 1993/2000 - Loss: -5.890
Iter 1994/2000 - Loss: -5.890
Iter 1995/2000 - Loss: -5.890
Iter 1996/2000 - Loss: -5.890
Iter 1997/2000 - Loss: -5.890
Iter 1998/2000 - Loss: -5.890
Iter 1999/2000 - Loss: -5.890
Iter 2000/2000 - Loss: -5.890
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.1706],
        [0.0003]])
Lengthscale: tensor([[[1.3989e+01, 2.6764e+00, 2.1572e+01, 1.1459e+01, 4.9473e+00,
          2.5680e+01]],

        [[2.0515e+01, 3.9539e+01, 3.0133e+01, 3.1466e+00, 6.4168e+00,
          3.0767e+01]],

        [[5.2711e-03, 2.0036e-02, 6.6557e-02, 1.9002e-03, 2.6950e-05,
          4.4322e-01]],

        [[1.8731e+01, 4.6077e+01, 9.3925e+00, 2.8058e+00, 6.8136e+00,
          3.2849e+01]]])
Signal Variance: tensor([0.0200, 3.5027, 0.7999, 0.2104])
Estimated target variance: tensor([0.0027, 0.0926, 0.9794, 0.0044])
N: 110
Signal to noise ratio: tensor([  7.4437, 108.3274,   2.1650,  26.5892])
Bound on condition number: tensor([6.0959e+03, 1.2908e+06, 5.1661e+02, 7.7769e+04])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.004191097976810598, policy loss: 9.397816060721164
Experience 11, Iter 1, disc loss: 0.0019318441209061872, policy loss: 9.558107979246369
Experience 11, Iter 2, disc loss: 0.002341466135529101, policy loss: 8.585524938886822
Experience 11, Iter 3, disc loss: 0.0009729434687676719, policy loss: 9.982742114934268
Experience 11, Iter 4, disc loss: 0.0024765171394713925, policy loss: 9.460820027366697
Experience 11, Iter 5, disc loss: 0.006599049189047368, policy loss: 9.039781945669889
Experience 11, Iter 6, disc loss: 0.005609018149994849, policy loss: 8.91366921595384
Experience 11, Iter 7, disc loss: 0.002260256919846913, policy loss: 8.942890663379758
Experience 11, Iter 8, disc loss: 0.0021259798917138873, policy loss: 9.011899678984438
Experience 11, Iter 9, disc loss: 0.0010711259663797133, policy loss: 9.131738966624303
Experience 11, Iter 10, disc loss: 0.0011983916977967091, policy loss: 9.490227235653727
Experience 11, Iter 11, disc loss: 0.00547321569312966, policy loss: 8.625030002946446
Experience 11, Iter 12, disc loss: 0.0010437542962068596, policy loss: 9.430742126868648
Experience 11, Iter 13, disc loss: 0.0013103733593581668, policy loss: 9.177167176091817
Experience 11, Iter 14, disc loss: 0.0016158733536111637, policy loss: 9.201973317522668
Experience 11, Iter 15, disc loss: 0.0015070749861370747, policy loss: 8.9049228074996
Experience 11, Iter 16, disc loss: 0.002577838299991576, policy loss: 8.509800711181452
Experience 11, Iter 17, disc loss: 0.000981661702202644, policy loss: 9.141101150808787
Experience 11, Iter 18, disc loss: 0.0011338874006449296, policy loss: 8.937490807149075
Experience 11, Iter 19, disc loss: 0.0018888833729172896, policy loss: 8.902869226292577
Experience 11, Iter 20, disc loss: 0.0014091663271243238, policy loss: 8.899342788956428
Experience 11, Iter 21, disc loss: 0.00099779235699602, policy loss: 9.627134550228803
Experience 11, Iter 22, disc loss: 0.0009052761172939215, policy loss: 9.166774400236307
Experience 11, Iter 23, disc loss: 0.0020379463964437165, policy loss: 8.606947744265497
Experience 11, Iter 24, disc loss: 0.0008380003399710422, policy loss: 9.037343450063041
Experience 11, Iter 25, disc loss: 0.002578395116723194, policy loss: 9.05559725650984
Experience 11, Iter 26, disc loss: 0.0007762913005895586, policy loss: 8.883773821635865
Experience 11, Iter 27, disc loss: 0.0016046020599811191, policy loss: 8.799147617614217
Experience 11, Iter 28, disc loss: 0.0009734294033784511, policy loss: 9.42202491292619
Experience 11, Iter 29, disc loss: 0.001356640752762859, policy loss: 8.848103024645221
Experience 11, Iter 30, disc loss: 0.000897041337272918, policy loss: 8.987222088796251
Experience 11, Iter 31, disc loss: 0.002028724316068308, policy loss: 9.216119528477073
Experience 11, Iter 32, disc loss: 0.0007317709605533908, policy loss: 9.262704820814886
Experience 11, Iter 33, disc loss: 0.0009913253185866726, policy loss: 8.922502951959633
Experience 11, Iter 34, disc loss: 0.0009555609551406644, policy loss: 9.597862652569532
Experience 11, Iter 35, disc loss: 0.0010696931446582917, policy loss: 9.456854553599435
Experience 11, Iter 36, disc loss: 0.0014722193660148095, policy loss: 9.457333038561561
Experience 11, Iter 37, disc loss: 0.0012535739832222302, policy loss: 9.26351679185138
Experience 11, Iter 38, disc loss: 0.0006431766036830515, policy loss: 9.258087038055951
Experience 11, Iter 39, disc loss: 0.001833098516695057, policy loss: 9.106825563497534
Experience 11, Iter 40, disc loss: 0.0012408761089952606, policy loss: 9.787934769887116
Experience 11, Iter 41, disc loss: 0.000692197707132115, policy loss: 9.820918136522764
Experience 11, Iter 42, disc loss: 0.0006565860024124075, policy loss: 9.823707872318833
Experience 11, Iter 43, disc loss: 0.002253472031717009, policy loss: 9.315763158423849
Experience 11, Iter 44, disc loss: 0.0008420756545681323, policy loss: 9.76559060300809
Experience 11, Iter 45, disc loss: 0.0016634311903979948, policy loss: 9.35599473461119
Experience 11, Iter 46, disc loss: 0.001600825911904696, policy loss: 8.97250986190378
Experience 11, Iter 47, disc loss: 0.003263339447890559, policy loss: 9.080992298133808
Experience 11, Iter 48, disc loss: 0.0018640378905725858, policy loss: 9.126580125328948
Experience 11, Iter 49, disc loss: 0.0011458134283061725, policy loss: 9.225472245264843
Experience 11, Iter 50, disc loss: 0.0025779010142462483, policy loss: 9.315892690564121
Experience 11, Iter 51, disc loss: 0.0007206476334369709, policy loss: 10.338337056285427
Experience 11, Iter 52, disc loss: 0.0011573180617932507, policy loss: 9.122459599284547
Experience 11, Iter 53, disc loss: 0.0008293614519970917, policy loss: 9.704231298780249
Experience 11, Iter 54, disc loss: 0.0009708208718146431, policy loss: 9.15471325262699
Experience 11, Iter 55, disc loss: 0.0007259838210082325, policy loss: 9.173457835322143
Experience 11, Iter 56, disc loss: 0.0009802445298649715, policy loss: 8.876677888280135
Experience 11, Iter 57, disc loss: 0.0007923973658733763, policy loss: 9.50642481238026
Experience 11, Iter 58, disc loss: 0.0005674269892027003, policy loss: 9.760829211923612
Experience 11, Iter 59, disc loss: 0.0007600174552986243, policy loss: 9.386627508640382
Experience 11, Iter 60, disc loss: 0.0007128724270844925, policy loss: 9.715834566137888
Experience 11, Iter 61, disc loss: 0.0010215201358344003, policy loss: 9.830408938715859
Experience 11, Iter 62, disc loss: 0.001369940912775743, policy loss: 9.117568193312765
Experience 11, Iter 63, disc loss: 0.0005922039598761453, policy loss: 9.44524258341758
Experience 11, Iter 64, disc loss: 0.0012422822784377667, policy loss: 9.473743714797664
Experience 11, Iter 65, disc loss: 0.0010416635261609743, policy loss: 9.124637859853834
Experience 11, Iter 66, disc loss: 0.000844323944518654, policy loss: 9.450564339014244
Experience 11, Iter 67, disc loss: 0.0004264719002294649, policy loss: 10.311630713293804
Experience 11, Iter 68, disc loss: 0.0007598650816117223, policy loss: 9.016656203052012
Experience 11, Iter 69, disc loss: 0.0007398191732572873, policy loss: 9.782585169942127
Experience 11, Iter 70, disc loss: 0.0011450586947946689, policy loss: 9.748999159436565
Experience 11, Iter 71, disc loss: 0.0008390601067005249, policy loss: 9.64935730166958
Experience 11, Iter 72, disc loss: 0.0013669417022310875, policy loss: 8.948877016141253
Experience 11, Iter 73, disc loss: 0.0007095798159891267, policy loss: 9.742463118746713
Experience 11, Iter 74, disc loss: 0.0008723382249721135, policy loss: 9.264768100310313
Experience 11, Iter 75, disc loss: 0.0007011342864936309, policy loss: 9.42767528694255
Experience 11, Iter 76, disc loss: 0.0004881046011719225, policy loss: 9.64039948969521
Experience 11, Iter 77, disc loss: 0.0008769983448906398, policy loss: 9.610390110848574
Experience 11, Iter 78, disc loss: 0.0011646652457447703, policy loss: 9.632433616532147
Experience 11, Iter 79, disc loss: 0.0007939398674023648, policy loss: 9.299184217566058
Experience 11, Iter 80, disc loss: 0.0006750523462385805, policy loss: 9.55627843678924
Experience 11, Iter 81, disc loss: 0.0011721567013944793, policy loss: 9.496489618380833
Experience 11, Iter 82, disc loss: 0.0008913668857608126, policy loss: 8.669243751459135
Experience 11, Iter 83, disc loss: 0.000496515696268016, policy loss: 9.828704474422066
Experience 11, Iter 84, disc loss: 0.0006151716114805715, policy loss: 9.807475217615883
Experience 11, Iter 85, disc loss: 0.0009190768426575619, policy loss: 9.338806114262344
Experience 11, Iter 86, disc loss: 0.0006871983308849609, policy loss: 9.389041085437949
Experience 11, Iter 87, disc loss: 0.0007882780198219803, policy loss: 9.755941724958664
Experience 11, Iter 88, disc loss: 0.0008759360836089453, policy loss: 9.430325826784262
Experience 11, Iter 89, disc loss: 0.0010956113490317649, policy loss: 10.025162157025331
Experience 11, Iter 90, disc loss: 0.0005501751846601178, policy loss: 9.874182062999964
Experience 11, Iter 91, disc loss: 0.0005336726373628062, policy loss: 9.553707241854104
Experience 11, Iter 92, disc loss: 0.0005295852669156471, policy loss: 9.617344730119267
Experience 11, Iter 93, disc loss: 0.0007077555768707395, policy loss: 9.6570100165664
Experience 11, Iter 94, disc loss: 0.0005446293325864729, policy loss: 9.517037361213054
Experience 11, Iter 95, disc loss: 0.0005252535781836139, policy loss: 9.720489436308204
Experience 11, Iter 96, disc loss: 0.0007012866157806387, policy loss: 9.563665912724147
Experience 11, Iter 97, disc loss: 0.0011098217131587835, policy loss: 9.42554998078781
Experience 11, Iter 98, disc loss: 0.0005096686656751688, policy loss: 9.893704804041658
Experience 11, Iter 99, disc loss: 0.0011742864333237333, policy loss: 9.327697678735547
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0339],
        [0.3530],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.4504e-03, 2.8182e-02, 8.2317e-02, 1.9294e-03, 2.7254e-05,
          6.1768e-01]],

        [[5.4504e-03, 2.8182e-02, 8.2317e-02, 1.9294e-03, 2.7254e-05,
          6.1768e-01]],

        [[5.4504e-03, 2.8182e-02, 8.2317e-02, 1.9294e-03, 2.7254e-05,
          6.1768e-01]],

        [[5.4504e-03, 2.8182e-02, 8.2317e-02, 1.9294e-03, 2.7254e-05,
          6.1768e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0029, 0.1358, 1.4119, 0.0052], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0029, 0.1358, 1.4119, 0.0052])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.449
Iter 2/2000 - Loss: 0.559
Iter 3/2000 - Loss: -0.444
Iter 4/2000 - Loss: -0.501
Iter 5/2000 - Loss: -0.100
Iter 6/2000 - Loss: -0.118
Iter 7/2000 - Loss: -0.424
Iter 8/2000 - Loss: -0.613
Iter 9/2000 - Loss: -0.540
Iter 10/2000 - Loss: -0.377
Iter 11/2000 - Loss: -0.350
Iter 12/2000 - Loss: -0.470
Iter 13/2000 - Loss: -0.582
Iter 14/2000 - Loss: -0.595
Iter 15/2000 - Loss: -0.569
Iter 16/2000 - Loss: -0.579
Iter 17/2000 - Loss: -0.592
Iter 18/2000 - Loss: -0.575
Iter 19/2000 - Loss: -0.584
Iter 20/2000 - Loss: -0.657
Iter 1981/2000 - Loss: -5.673
Iter 1982/2000 - Loss: -5.673
Iter 1983/2000 - Loss: -5.673
Iter 1984/2000 - Loss: -5.674
Iter 1985/2000 - Loss: -5.674
Iter 1986/2000 - Loss: -5.674
Iter 1987/2000 - Loss: -5.674
Iter 1988/2000 - Loss: -5.674
Iter 1989/2000 - Loss: -5.674
Iter 1990/2000 - Loss: -5.674
Iter 1991/2000 - Loss: -5.674
Iter 1992/2000 - Loss: -5.674
Iter 1993/2000 - Loss: -5.674
Iter 1994/2000 - Loss: -5.674
Iter 1995/2000 - Loss: -5.674
Iter 1996/2000 - Loss: -5.674
Iter 1997/2000 - Loss: -5.674
Iter 1998/2000 - Loss: -5.674
Iter 1999/2000 - Loss: -5.674
Iter 2000/2000 - Loss: -5.674
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.2355],
        [0.0003]])
Lengthscale: tensor([[[1.3207e+01, 2.3630e+00, 2.3444e+01, 1.3678e+01, 9.4313e-02,
          2.1137e+01]],

        [[2.0700e+01, 4.0797e+01, 3.1597e+01, 4.9788e+00, 5.2670e-01,
          3.0878e+01]],

        [[5.3996e-03, 2.0834e-02, 8.2326e-02, 1.9250e-03, 2.6327e-05,
          6.1743e-01]],

        [[1.8877e+01, 4.5414e+01, 1.1757e+01, 2.8780e+00, 7.0526e+00,
          3.6512e+01]]])
Signal Variance: tensor([0.0141, 3.4543, 1.1646, 0.3150])
Estimated target variance: tensor([0.0029, 0.1358, 1.4119, 0.0052])
N: 120
Signal to noise ratio: tensor([  6.2100, 111.3832,   2.2236,  31.9012])
Bound on condition number: tensor([4.6287e+03, 1.4887e+06, 5.9434e+02, 1.2212e+05])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0005667306655872807, policy loss: 10.686596753001552
Experience 12, Iter 1, disc loss: 0.0009096609404337865, policy loss: 10.86891167097398
Experience 12, Iter 2, disc loss: 0.0004927598303809148, policy loss: 10.61922175122371
Experience 12, Iter 3, disc loss: 0.00067408949438656, policy loss: 10.93801906641182
Experience 12, Iter 4, disc loss: 0.0005732834986364292, policy loss: 10.920447753113434
Experience 12, Iter 5, disc loss: 0.0006537911941454473, policy loss: 10.56230779087208
Experience 12, Iter 6, disc loss: 0.0009862304236066411, policy loss: 10.65533466302811
Experience 12, Iter 7, disc loss: 0.0015254467636842382, policy loss: 10.00619442708302
Experience 12, Iter 8, disc loss: 0.0009233030042818365, policy loss: 10.832396006226816
Experience 12, Iter 9, disc loss: 0.001275360012806906, policy loss: 10.254457058039328
Experience 12, Iter 10, disc loss: 0.0007242739942034535, policy loss: 10.81335700924645
Experience 12, Iter 11, disc loss: 0.000327142804808836, policy loss: 10.853965925895796
Experience 12, Iter 12, disc loss: 0.0013897935195475832, policy loss: 11.015440687075472
Experience 12, Iter 13, disc loss: 0.0006359009933918421, policy loss: 10.931056429892926
Experience 12, Iter 14, disc loss: 0.0012466863071519036, policy loss: 10.32899587234525
Experience 12, Iter 15, disc loss: 0.0003027382667611141, policy loss: 11.11322472268758
Experience 12, Iter 16, disc loss: 0.0005688593692923905, policy loss: 10.756221699818726
Experience 12, Iter 17, disc loss: 0.0003570571388321122, policy loss: 10.896068979641704
Experience 12, Iter 18, disc loss: 0.000824598996722353, policy loss: 10.321356992612612
Experience 12, Iter 19, disc loss: 0.00039661885323356605, policy loss: 11.254466180111983
Experience 12, Iter 20, disc loss: 0.0004646542389050833, policy loss: 10.739017078473832
Experience 12, Iter 21, disc loss: 0.0015572998480158247, policy loss: 10.368249488675206
Experience 12, Iter 22, disc loss: 0.0003599875988401901, policy loss: 10.998270395810662
Experience 12, Iter 23, disc loss: 0.00034141621823836974, policy loss: 10.704851921841918
Experience 12, Iter 24, disc loss: 0.001298848051019142, policy loss: 10.317797736945902
Experience 12, Iter 25, disc loss: 0.0003792689788912416, policy loss: 11.243329469520528
Experience 12, Iter 26, disc loss: 0.0007974275650730196, policy loss: 11.07192593674905
Experience 12, Iter 27, disc loss: 0.0009141830106156035, policy loss: 10.492789696606808
Experience 12, Iter 28, disc loss: 0.0009765869166946943, policy loss: 10.700846356164586
Experience 12, Iter 29, disc loss: 0.001066645632496358, policy loss: 11.096248433669889
Experience 12, Iter 30, disc loss: 0.00029925828208801923, policy loss: 11.08281952423379
Experience 12, Iter 31, disc loss: 0.00043274315130086395, policy loss: 10.860228416092458
Experience 12, Iter 32, disc loss: 0.0004522138885037667, policy loss: 10.367418023191817
Experience 12, Iter 33, disc loss: 0.0012055603866290329, policy loss: 11.197118912433101
Experience 12, Iter 34, disc loss: 0.0004773923073393565, policy loss: 10.750117078181592
Experience 12, Iter 35, disc loss: 0.000819969614909714, policy loss: 10.720685076053543
Experience 12, Iter 36, disc loss: 0.0005525388351514032, policy loss: 10.248124564106375
Experience 12, Iter 37, disc loss: 0.000549892209677592, policy loss: 11.030232489069647
Experience 12, Iter 38, disc loss: 0.0010323995291779515, policy loss: 10.796007880991883
Experience 12, Iter 39, disc loss: 0.00043655297273553935, policy loss: 11.089447549303362
Experience 12, Iter 40, disc loss: 0.000724646324058893, policy loss: 9.975690602291976
Experience 12, Iter 41, disc loss: 0.00035585658912948206, policy loss: 11.316717000314862
Experience 12, Iter 42, disc loss: 0.001095215841395436, policy loss: 10.918671460230179
Experience 12, Iter 43, disc loss: 0.0003988366874841119, policy loss: 11.022848000350109
Experience 12, Iter 44, disc loss: 0.0004173992964918381, policy loss: 10.883053127102947
Experience 12, Iter 45, disc loss: 0.0024028621716435487, policy loss: 10.685140522483724
Experience 12, Iter 46, disc loss: 0.000298745962163287, policy loss: 10.62124843073586
Experience 12, Iter 47, disc loss: 0.0018059457023061312, policy loss: 10.800461952433158
Experience 12, Iter 48, disc loss: 0.0002859750878158477, policy loss: 11.442640540167233
Experience 12, Iter 49, disc loss: 0.001305647854732959, policy loss: 10.237651836509453
Experience 12, Iter 50, disc loss: 0.0002857137615240348, policy loss: 10.973709006689909
Experience 12, Iter 51, disc loss: 0.0005065963655999978, policy loss: 10.563699167257045
Experience 12, Iter 52, disc loss: 0.0005673791468345994, policy loss: 10.789155709366046
Experience 12, Iter 53, disc loss: 0.0005306492811982781, policy loss: 10.457827516006919
Experience 12, Iter 54, disc loss: 0.0003301736683231011, policy loss: 10.86401466141968
Experience 12, Iter 55, disc loss: 0.00044163908906886303, policy loss: 10.784709868086068
Experience 12, Iter 56, disc loss: 0.00032595762792462005, policy loss: 11.189966062294415
Experience 12, Iter 57, disc loss: 0.0003604101795615429, policy loss: 10.190150735207173
Experience 12, Iter 58, disc loss: 0.00047803586208724965, policy loss: 10.900883541877516
Experience 12, Iter 59, disc loss: 0.0004998228047264281, policy loss: 11.007352876857839
Experience 12, Iter 60, disc loss: 0.0011325521475888049, policy loss: 10.409788761769384
Experience 12, Iter 61, disc loss: 0.0008178733820975212, policy loss: 10.625954847929027
Experience 12, Iter 62, disc loss: 0.00040285692318063566, policy loss: 10.847630256564658
Experience 12, Iter 63, disc loss: 0.0003751226653426319, policy loss: 10.993171848928814
Experience 12, Iter 64, disc loss: 0.00045826058557150187, policy loss: 11.53814676268586
Experience 12, Iter 65, disc loss: 0.0004058376633751623, policy loss: 10.782342744416658
Experience 12, Iter 66, disc loss: 0.0003624218356882273, policy loss: 11.531368344387475
Experience 12, Iter 67, disc loss: 0.0006606854270056212, policy loss: 9.859411120218002
Experience 12, Iter 68, disc loss: 0.0005911596756674033, policy loss: 10.82053896254765
Experience 12, Iter 69, disc loss: 0.0009852078142354376, policy loss: 10.784712241263655
Experience 12, Iter 70, disc loss: 0.000383749897902864, policy loss: 11.585788458987714
Experience 12, Iter 71, disc loss: 0.00027140062176753154, policy loss: 11.407755966779925
Experience 12, Iter 72, disc loss: 0.0007240349794112134, policy loss: 10.69974298841502
Experience 12, Iter 73, disc loss: 0.00035674232961009503, policy loss: 10.598880813760168
Experience 12, Iter 74, disc loss: 0.0003102862682309249, policy loss: 10.811508753566168
Experience 12, Iter 75, disc loss: 0.0003449502123258441, policy loss: 11.020144579254003
Experience 12, Iter 76, disc loss: 0.001433870224577555, policy loss: 10.850335777607704
Experience 12, Iter 77, disc loss: 0.0007662731207579139, policy loss: 11.182203731691303
Experience 12, Iter 78, disc loss: 0.0009210092101671701, policy loss: 11.370809295684847
Experience 12, Iter 79, disc loss: 0.00032496475211119555, policy loss: 10.944985609690491
Experience 12, Iter 80, disc loss: 0.00037535752009574944, policy loss: 11.04747410493093
Experience 12, Iter 81, disc loss: 0.000520137809608624, policy loss: 11.103032381090035
Experience 12, Iter 82, disc loss: 0.0005736110812527706, policy loss: 10.995965000518368
Experience 12, Iter 83, disc loss: 0.000641164159383392, policy loss: 11.115721673951544
Experience 12, Iter 84, disc loss: 0.0003670268680287449, policy loss: 11.035243534251547
Experience 12, Iter 85, disc loss: 0.00034537144443356636, policy loss: 10.747594296422072
Experience 12, Iter 86, disc loss: 0.0008787926359221942, policy loss: 11.20345706416379
Experience 12, Iter 87, disc loss: 0.0005827696299354832, policy loss: 10.576647420549705
Experience 12, Iter 88, disc loss: 0.0011289394888104208, policy loss: 11.031638608649764
Experience 12, Iter 89, disc loss: 0.00035162347443929223, policy loss: 11.162168503026818
Experience 12, Iter 90, disc loss: 0.004084446346054689, policy loss: 10.860241281374869
Experience 12, Iter 91, disc loss: 0.0003723637895905041, policy loss: 10.697594203331821
Experience 12, Iter 92, disc loss: 0.0007108883977120661, policy loss: 11.673167556742623
Experience 12, Iter 93, disc loss: 0.0009038359412314344, policy loss: 10.940162030564323
Experience 12, Iter 94, disc loss: 0.0012647461111664634, policy loss: 10.45996525758769
Experience 12, Iter 95, disc loss: 0.0004086879767444991, policy loss: 10.940836785065446
Experience 12, Iter 96, disc loss: 0.0003188599476966093, policy loss: 11.514288501846766
Experience 12, Iter 97, disc loss: 0.0005254956239249287, policy loss: 10.517554666541729
Experience 12, Iter 98, disc loss: 0.000532295818137457, policy loss: 10.734586467212822
Experience 12, Iter 99, disc loss: 0.0003988674935297169, policy loss: 10.96634600283998
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0522],
        [0.5275],
        [0.0015]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.6395e-03, 3.0256e-02, 1.0894e-01, 2.0030e-03, 2.8887e-05,
          9.2519e-01]],

        [[5.6395e-03, 3.0256e-02, 1.0894e-01, 2.0030e-03, 2.8887e-05,
          9.2519e-01]],

        [[5.6395e-03, 3.0256e-02, 1.0894e-01, 2.0030e-03, 2.8887e-05,
          9.2519e-01]],

        [[5.6395e-03, 3.0256e-02, 1.0894e-01, 2.0030e-03, 2.8887e-05,
          9.2519e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0028, 0.2086, 2.1099, 0.0058], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0028, 0.2086, 2.1099, 0.0058])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.041
Iter 2/2000 - Loss: 0.943
Iter 3/2000 - Loss: 0.041
Iter 4/2000 - Loss: -0.048
Iter 5/2000 - Loss: 0.316
Iter 6/2000 - Loss: 0.331
Iter 7/2000 - Loss: 0.056
Iter 8/2000 - Loss: -0.142
Iter 9/2000 - Loss: -0.097
Iter 10/2000 - Loss: 0.060
Iter 11/2000 - Loss: 0.105
Iter 12/2000 - Loss: -0.000
Iter 13/2000 - Loss: -0.117
Iter 14/2000 - Loss: -0.139
Iter 15/2000 - Loss: -0.115
Iter 16/2000 - Loss: -0.123
Iter 17/2000 - Loss: -0.140
Iter 18/2000 - Loss: -0.122
Iter 19/2000 - Loss: -0.124
Iter 20/2000 - Loss: -0.194
Iter 1981/2000 - Loss: -8.526
Iter 1982/2000 - Loss: -8.526
Iter 1983/2000 - Loss: -8.526
Iter 1984/2000 - Loss: -8.526
Iter 1985/2000 - Loss: -8.526
Iter 1986/2000 - Loss: -8.526
Iter 1987/2000 - Loss: -8.526
Iter 1988/2000 - Loss: -8.526
Iter 1989/2000 - Loss: -8.526
Iter 1990/2000 - Loss: -8.526
Iter 1991/2000 - Loss: -8.527
Iter 1992/2000 - Loss: -8.527
Iter 1993/2000 - Loss: -8.527
Iter 1994/2000 - Loss: -8.527
Iter 1995/2000 - Loss: -8.527
Iter 1996/2000 - Loss: -8.527
Iter 1997/2000 - Loss: -8.527
Iter 1998/2000 - Loss: -8.527
Iter 1999/2000 - Loss: -8.527
Iter 2000/2000 - Loss: -8.527
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[13.4072,  2.4290, 33.2703, 13.5113,  0.1039, 23.3572]],

        [[20.3341, 39.2625, 34.0886,  5.6040,  0.5756, 34.8043]],

        [[20.5525, 22.3253, 32.4605,  1.9995,  5.2314, 33.5842]],

        [[18.5036, 41.0986, 10.3483,  3.0297,  6.6649, 33.3513]]])
Signal Variance: tensor([1.5949e-02, 4.6751e+00, 3.3664e+01, 2.2731e-01])
Estimated target variance: tensor([0.0028, 0.2086, 2.1099, 0.0058])
N: 130
Signal to noise ratio: tensor([  6.6884, 133.8569, 105.1735,  27.3349])
Bound on condition number: tensor([   5816.5506, 2329299.4416, 1437991.9070,   97136.2906])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0001847430900631052, policy loss: 11.619999740011455
Experience 13, Iter 1, disc loss: 0.00018437472266398397, policy loss: 11.439477811943963
Experience 13, Iter 2, disc loss: 0.0001927023033479121, policy loss: 10.927836508725553
Experience 13, Iter 3, disc loss: 0.0002128592551469198, policy loss: 10.521182145411238
Experience 13, Iter 4, disc loss: 0.00022102513301647367, policy loss: 10.010999269760948
Experience 13, Iter 5, disc loss: 0.00021706419231830272, policy loss: 10.219715715819229
Experience 13, Iter 6, disc loss: 0.0002160004364318898, policy loss: 10.618748953769863
Experience 13, Iter 7, disc loss: 0.00027843515279291596, policy loss: 10.074148154237609
Experience 13, Iter 8, disc loss: 0.0003257817271833214, policy loss: 10.01399090374526
Experience 13, Iter 9, disc loss: 0.000291090905737737, policy loss: 9.984602699551635
Experience 13, Iter 10, disc loss: 0.00028614614611749525, policy loss: 9.757399098320045
Experience 13, Iter 11, disc loss: 0.00028204157807502157, policy loss: 10.301509409146972
Experience 13, Iter 12, disc loss: 0.0003642364002524186, policy loss: 9.78945294915378
Experience 13, Iter 13, disc loss: 0.00044480916264120774, policy loss: 9.675612029365627
Experience 13, Iter 14, disc loss: 0.0004668528753661049, policy loss: 9.676585695044954
Experience 13, Iter 15, disc loss: 0.0004178078148692459, policy loss: 9.872437423619676
Experience 13, Iter 16, disc loss: 0.0003978382189789279, policy loss: 9.473400254221394
Experience 13, Iter 17, disc loss: 0.00043355602958238595, policy loss: 9.514965525176592
Experience 13, Iter 18, disc loss: 0.0004101377767164202, policy loss: 9.260114129695797
Experience 13, Iter 19, disc loss: 0.0004526957509635414, policy loss: 9.04938841461781
Experience 13, Iter 20, disc loss: 0.0004260111436986514, policy loss: 8.84074430223789
Experience 13, Iter 21, disc loss: 0.00042081616310440495, policy loss: 8.952868621755893
Experience 13, Iter 22, disc loss: 0.00040888618625203573, policy loss: 8.888409551623234
Experience 13, Iter 23, disc loss: 0.00040836172353603446, policy loss: 8.870953028727207
Experience 13, Iter 24, disc loss: 0.0003512361042417047, policy loss: 8.970390946934312
Experience 13, Iter 25, disc loss: 0.0004293146133081863, policy loss: 8.69400395963335
Experience 13, Iter 26, disc loss: 0.00039664874136229896, policy loss: 8.748957356690553
Experience 13, Iter 27, disc loss: 0.0004493207692104083, policy loss: 8.466139209840897
Experience 13, Iter 28, disc loss: 0.000376495405031464, policy loss: 9.029799649948089
Experience 13, Iter 29, disc loss: 0.00044624959450163744, policy loss: 8.43119541707231
Experience 13, Iter 30, disc loss: 0.0004950050963258125, policy loss: 8.345169572633921
Experience 13, Iter 31, disc loss: 0.0004895375468723135, policy loss: 8.270882881632824
Experience 13, Iter 32, disc loss: 0.0005258375241939447, policy loss: 8.137475447230841
Experience 13, Iter 33, disc loss: 0.000638855059188061, policy loss: 7.90424797637168
Experience 13, Iter 34, disc loss: 0.0006418349370351613, policy loss: 7.980796557757062
Experience 13, Iter 35, disc loss: 0.0006349000643790926, policy loss: 8.00008788180631
Experience 13, Iter 36, disc loss: 0.000901321928461508, policy loss: 7.72182041833831
Experience 13, Iter 37, disc loss: 0.000995506682640205, policy loss: 7.630429714073504
Experience 13, Iter 38, disc loss: 0.0010196555941849577, policy loss: 7.603023215844747
Experience 13, Iter 39, disc loss: 0.0008868397905123109, policy loss: 7.855792763401222
Experience 13, Iter 40, disc loss: 0.0009861546811416183, policy loss: 7.879733936468714
Experience 13, Iter 41, disc loss: 0.0011628441769262884, policy loss: 7.561717905551877
Experience 13, Iter 42, disc loss: 0.001707475120310066, policy loss: 7.301297041273237
Experience 13, Iter 43, disc loss: 0.0016233828904339948, policy loss: 7.347880244406746
Experience 13, Iter 44, disc loss: 0.0016661858463896152, policy loss: 7.355610816281776
Experience 13, Iter 45, disc loss: 0.0028806156892084303, policy loss: 7.6345047230745084
Experience 13, Iter 46, disc loss: 0.001863506127925432, policy loss: 7.424237714429497
Experience 13, Iter 47, disc loss: 0.0023187154681617284, policy loss: 7.297484117271167
Experience 13, Iter 48, disc loss: 0.0023412232978484216, policy loss: 7.2317913797155065
Experience 13, Iter 49, disc loss: 0.004782369070081281, policy loss: 7.021563536803132
Experience 13, Iter 50, disc loss: 0.005655914466585566, policy loss: 7.277691029429845
Experience 13, Iter 51, disc loss: 0.004710297327475209, policy loss: 7.243768694933983
Experience 13, Iter 52, disc loss: 0.007566062794691084, policy loss: 6.794193351621606
Experience 13, Iter 53, disc loss: 0.007892971529426555, policy loss: 6.581559575823665
Experience 13, Iter 54, disc loss: 0.013958359080842022, policy loss: 6.3186164393810635
Experience 13, Iter 55, disc loss: 0.009107570393153828, policy loss: 6.447221147926899
Experience 13, Iter 56, disc loss: 0.0059026387077158885, policy loss: 6.884699517668352
Experience 13, Iter 57, disc loss: 0.010825487646903456, policy loss: 6.2379155624302545
Experience 13, Iter 58, disc loss: 0.012317929553119744, policy loss: 6.491137213635595
Experience 13, Iter 59, disc loss: 0.013135603607204217, policy loss: 6.54506114164008
Experience 13, Iter 60, disc loss: 0.020340642218336317, policy loss: 5.468101232154519
Experience 13, Iter 61, disc loss: 0.013335770370132597, policy loss: 5.8958476593591955
Experience 13, Iter 62, disc loss: 0.011454601562265127, policy loss: 6.683543758924111
Experience 13, Iter 63, disc loss: 0.008386880996994797, policy loss: 6.261263347419585
Experience 13, Iter 64, disc loss: 0.007811172807019824, policy loss: 6.659434158871961
Experience 13, Iter 65, disc loss: 0.008818167308784071, policy loss: 6.677273528367018
Experience 13, Iter 66, disc loss: 0.012545324902064506, policy loss: 6.003933148465694
Experience 13, Iter 67, disc loss: 0.009124450540523367, policy loss: 6.260861728799599
Experience 13, Iter 68, disc loss: 0.011956907871501493, policy loss: 6.158880688541911
Experience 13, Iter 69, disc loss: 0.008044837573888144, policy loss: 6.490856686182088
Experience 13, Iter 70, disc loss: 0.010797354856162305, policy loss: 5.926025713278088
Experience 13, Iter 71, disc loss: 0.014607696814625237, policy loss: 5.883921073432794
Experience 13, Iter 72, disc loss: 0.01038797835422722, policy loss: 5.905271422949648
Experience 13, Iter 73, disc loss: 0.010855417370509558, policy loss: 6.128708242700439
Experience 13, Iter 74, disc loss: 0.011063368809407371, policy loss: 6.152247468085087
Experience 13, Iter 75, disc loss: 0.010094533211581875, policy loss: 6.405012678517091
Experience 13, Iter 76, disc loss: 0.012591597011265993, policy loss: 5.977425221527502
Experience 13, Iter 77, disc loss: 0.011391717773033296, policy loss: 6.27640918851505
Experience 13, Iter 78, disc loss: 0.009777343358864514, policy loss: 6.183049282524641
Experience 13, Iter 79, disc loss: 0.007577233329989755, policy loss: 6.7007446990574175
Experience 13, Iter 80, disc loss: 0.010082681253172986, policy loss: 6.076889916022517
Experience 13, Iter 81, disc loss: 0.009996961168471178, policy loss: 6.331473480354746
Experience 13, Iter 82, disc loss: 0.006156144571848286, policy loss: 6.263156069854665
Experience 13, Iter 83, disc loss: 0.007249889255816339, policy loss: 6.198142029825963
Experience 13, Iter 84, disc loss: 0.011275011432788428, policy loss: 6.024770147253121
Experience 13, Iter 85, disc loss: 0.009016744803098775, policy loss: 5.96368178879689
Experience 13, Iter 86, disc loss: 0.009910900692999446, policy loss: 6.275252850427422
Experience 13, Iter 87, disc loss: 0.012696036554713595, policy loss: 6.1182689715752225
Experience 13, Iter 88, disc loss: 0.007137683528983437, policy loss: 6.513745525053334
Experience 13, Iter 89, disc loss: 0.016200522822028197, policy loss: 6.476524769468951
Experience 13, Iter 90, disc loss: 0.008191715126809064, policy loss: 6.231231822501548
Experience 13, Iter 91, disc loss: 0.01483369516737653, policy loss: 6.4098513677048015
Experience 13, Iter 92, disc loss: 0.009113218421397909, policy loss: 6.2421123622722
Experience 13, Iter 93, disc loss: 0.006654640391882297, policy loss: 6.838257533470033
Experience 13, Iter 94, disc loss: 0.006262588404895415, policy loss: 7.140719374727809
Experience 13, Iter 95, disc loss: 0.006856837714964944, policy loss: 7.404792039545123
Experience 13, Iter 96, disc loss: 0.005223448022792732, policy loss: 7.190039744161122
Experience 13, Iter 97, disc loss: 0.009733020170395384, policy loss: 6.785835481837311
Experience 13, Iter 98, disc loss: 0.009469516694074049, policy loss: 7.080616122910154
Experience 13, Iter 99, disc loss: 0.006658711075829009, policy loss: 7.046378312631354
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0502],
        [0.5092],
        [0.0014]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.3376e-03, 2.8548e-02, 1.0645e-01, 1.9135e-03, 2.7474e-05,
          8.8502e-01]],

        [[5.3376e-03, 2.8548e-02, 1.0645e-01, 1.9135e-03, 2.7474e-05,
          8.8502e-01]],

        [[5.3376e-03, 2.8548e-02, 1.0645e-01, 1.9135e-03, 2.7474e-05,
          8.8502e-01]],

        [[5.3376e-03, 2.8548e-02, 1.0645e-01, 1.9135e-03, 2.7474e-05,
          8.8502e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0027, 0.2008, 2.0366, 0.0057], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0027, 0.2008, 2.0366, 0.0057])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.021
Iter 2/2000 - Loss: 0.899
Iter 3/2000 - Loss: -0.024
Iter 4/2000 - Loss: -0.121
Iter 5/2000 - Loss: 0.253
Iter 6/2000 - Loss: 0.267
Iter 7/2000 - Loss: -0.014
Iter 8/2000 - Loss: -0.211
Iter 9/2000 - Loss: -0.161
Iter 10/2000 - Loss: 0.000
Iter 11/2000 - Loss: 0.045
Iter 12/2000 - Loss: -0.066
Iter 13/2000 - Loss: -0.189
Iter 14/2000 - Loss: -0.214
Iter 15/2000 - Loss: -0.180
Iter 16/2000 - Loss: -0.179
Iter 17/2000 - Loss: -0.204
Iter 18/2000 - Loss: -0.201
Iter 19/2000 - Loss: -0.200
Iter 20/2000 - Loss: -0.259
Iter 1981/2000 - Loss: -5.606
Iter 1982/2000 - Loss: -5.606
Iter 1983/2000 - Loss: -5.606
Iter 1984/2000 - Loss: -5.606
Iter 1985/2000 - Loss: -5.606
Iter 1986/2000 - Loss: -5.606
Iter 1987/2000 - Loss: -5.606
Iter 1988/2000 - Loss: -5.606
Iter 1989/2000 - Loss: -5.606
Iter 1990/2000 - Loss: -5.606
Iter 1991/2000 - Loss: -5.606
Iter 1992/2000 - Loss: -5.606
Iter 1993/2000 - Loss: -5.607
Iter 1994/2000 - Loss: -5.607
Iter 1995/2000 - Loss: -5.607
Iter 1996/2000 - Loss: -5.607
Iter 1997/2000 - Loss: -5.607
Iter 1998/2000 - Loss: -5.607
Iter 1999/2000 - Loss: -5.607
Iter 2000/2000 - Loss: -5.607
***AFTER OPTIMATION***
Noise Variance: tensor([[3.3969e-04],
        [2.6633e-04],
        [3.2264e-01],
        [2.9650e-04]])
Lengthscale: tensor([[[1.3377e+01, 2.3951e+00, 3.0984e+01, 1.3159e+01, 1.0118e-01,
          2.3061e+01]],

        [[1.8946e+01, 3.5659e+01, 3.3427e+01, 5.0355e+00, 5.5344e-01,
          3.3157e+01]],

        [[5.2906e-03, 2.0755e-02, 1.0646e-01, 1.9095e-03, 2.6638e-05,
          8.8489e-01]],

        [[1.8345e+01, 4.0801e+01, 1.0331e+01, 3.0069e+00, 6.5563e+00,
          3.5712e+01]]])
Signal Variance: tensor([0.0155, 4.3202, 1.6994, 0.2306])
Estimated target variance: tensor([0.0027, 0.2008, 2.0366, 0.0057])
N: 140
Signal to noise ratio: tensor([  6.7649, 127.3614,   2.2950,  27.8878])
Bound on condition number: tensor([6.4080e+03, 2.2709e+06, 7.3840e+02, 1.0888e+05])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0025905669103079712, policy loss: 16.335600613739736
Experience 14, Iter 1, disc loss: 0.002450906518821465, policy loss: 14.506454996617315
Experience 14, Iter 2, disc loss: 0.002358146288450168, policy loss: 15.206606086683909
Experience 14, Iter 3, disc loss: 0.0021006117682791548, policy loss: 15.284209842933034
Experience 14, Iter 4, disc loss: 0.0018630835789267015, policy loss: 15.586235769005196
Experience 14, Iter 5, disc loss: 0.0016774580799608903, policy loss: 14.256228877421321
Experience 14, Iter 6, disc loss: 0.0015609513302330803, policy loss: 15.32492283687263
Experience 14, Iter 7, disc loss: 0.0013174707033166244, policy loss: 14.006363331556871
Experience 14, Iter 8, disc loss: 0.0012047097503564649, policy loss: 14.433787065460756
Experience 14, Iter 9, disc loss: 0.0010907437612352865, policy loss: 14.460247983722304
Experience 14, Iter 10, disc loss: 0.0009715234824493326, policy loss: 13.951083808232395
Experience 14, Iter 11, disc loss: 0.0008216916595271357, policy loss: 13.594766700569602
Experience 14, Iter 12, disc loss: 0.0007272457155110809, policy loss: 15.021518786207798
Experience 14, Iter 13, disc loss: 0.0011562107925309653, policy loss: 14.476487835900173
Experience 14, Iter 14, disc loss: 0.0009016750652086912, policy loss: 15.039481488330882
Experience 14, Iter 15, disc loss: 0.0004937874568592403, policy loss: 14.161903306267634
Experience 14, Iter 16, disc loss: 0.001061109955910254, policy loss: 13.95376595516089
Experience 14, Iter 17, disc loss: 0.0006748999259534576, policy loss: 14.280800279438532
Experience 14, Iter 18, disc loss: 0.0005367116270001452, policy loss: 13.927567602962835
Experience 14, Iter 19, disc loss: 0.0009900838741346923, policy loss: 13.92922767285636
Experience 14, Iter 20, disc loss: 0.0003886815926199014, policy loss: 14.542339873047862
Experience 14, Iter 21, disc loss: 0.0005657152475842197, policy loss: 13.344367052725909
Experience 14, Iter 22, disc loss: 0.0003817522286662724, policy loss: 14.164219987073444
Experience 14, Iter 23, disc loss: 0.0002961803365839451, policy loss: 14.613605268484903
Experience 14, Iter 24, disc loss: 0.0005866178255086129, policy loss: 14.062808584103651
Experience 14, Iter 25, disc loss: 0.00023583366962092966, policy loss: 14.023105458804885
Experience 14, Iter 26, disc loss: 0.000999978355475816, policy loss: 14.939766129697222
Experience 14, Iter 27, disc loss: 0.00027118398992384476, policy loss: 13.448406414251805
Experience 14, Iter 28, disc loss: 0.0002968283140206098, policy loss: 14.009980398715529
Experience 14, Iter 29, disc loss: 0.00022373501828815953, policy loss: 13.764566901629966
Experience 14, Iter 30, disc loss: 0.0007256818896156213, policy loss: 13.594439364078312
Experience 14, Iter 31, disc loss: 0.00027557649148387934, policy loss: 14.003481254041679
Experience 14, Iter 32, disc loss: 0.00024675014142514173, policy loss: 14.886786469365973
Experience 14, Iter 33, disc loss: 0.00018437394858421404, policy loss: 14.249706704920882
Experience 14, Iter 34, disc loss: 0.0004179585044072056, policy loss: 13.320039356659453
Experience 14, Iter 35, disc loss: 0.00016723988720667547, policy loss: 14.141093548915926
Experience 14, Iter 36, disc loss: 0.00042424243307830187, policy loss: 14.959282202120871
Experience 14, Iter 37, disc loss: 0.00022113228546941622, policy loss: 13.78977178852416
Experience 14, Iter 38, disc loss: 0.00014681594354166706, policy loss: 14.45576216600324
Experience 14, Iter 39, disc loss: 0.0001997732696256305, policy loss: 13.879522024966352
Experience 14, Iter 40, disc loss: 0.0002863899801151256, policy loss: 14.447643657663042
Experience 14, Iter 41, disc loss: 0.0002905322968758868, policy loss: 15.083350568615979
Experience 14, Iter 42, disc loss: 0.00018912025858369366, policy loss: 13.78340506215778
Experience 14, Iter 43, disc loss: 0.00017705745178257283, policy loss: 14.194875763659095
Experience 14, Iter 44, disc loss: 0.00030765868579970233, policy loss: 14.0131973309477
Experience 14, Iter 45, disc loss: 0.000138697154072237, policy loss: 13.781840746822324
Experience 14, Iter 46, disc loss: 0.00021970492372225454, policy loss: 14.233356936110162
Experience 14, Iter 47, disc loss: 0.00034324175745592344, policy loss: 14.514508358201718
Experience 14, Iter 48, disc loss: 0.00031897307661531263, policy loss: 14.561116882965074
Experience 14, Iter 49, disc loss: 0.00013044714651337503, policy loss: 14.453809554044796
Experience 14, Iter 50, disc loss: 0.0002608587879009515, policy loss: 13.996279270342988
Experience 14, Iter 51, disc loss: 0.00016507974301140424, policy loss: 14.046141148779345
Experience 14, Iter 52, disc loss: 0.0009355903589978919, policy loss: 13.102880854227454
Experience 14, Iter 53, disc loss: 0.0001495023772165606, policy loss: 14.238748251596174
Experience 14, Iter 54, disc loss: 0.00021640054207455544, policy loss: 14.88562564526807
Experience 14, Iter 55, disc loss: 0.0003916730689473803, policy loss: 13.33477420756924
Experience 14, Iter 56, disc loss: 0.00024801047502712517, policy loss: 13.926461973144312
Experience 14, Iter 57, disc loss: 0.0001425925361610936, policy loss: 13.573154485525727
Experience 14, Iter 58, disc loss: 0.00015402436894527947, policy loss: 14.346721494560573
Experience 14, Iter 59, disc loss: 0.00028605503040480195, policy loss: 14.292040052052991
Experience 14, Iter 60, disc loss: 0.0003156097949875943, policy loss: 13.958919341560343
Experience 14, Iter 61, disc loss: 0.0005211071780302828, policy loss: 14.87734451366106
Experience 14, Iter 62, disc loss: 0.00014583233916734472, policy loss: 14.013460530378069
Experience 14, Iter 63, disc loss: 0.0001264406384114704, policy loss: 14.010291030479934
Experience 14, Iter 64, disc loss: 0.00012954064667037663, policy loss: 14.661911408326166
Experience 14, Iter 65, disc loss: 0.00040391007482763297, policy loss: 13.793298831278324
Experience 14, Iter 66, disc loss: 0.00012894297969861558, policy loss: 15.100740026827204
Experience 14, Iter 67, disc loss: 0.0004655168670175503, policy loss: 14.600687357778623
Experience 14, Iter 68, disc loss: 0.0002466873782825005, policy loss: 14.037028636266125
Experience 14, Iter 69, disc loss: 0.0009355444200304546, policy loss: 14.174298270155706
Experience 14, Iter 70, disc loss: 0.00010590802287268228, policy loss: 14.420121394785829
Experience 14, Iter 71, disc loss: 0.0017945793163356527, policy loss: 13.111653122865896
Experience 14, Iter 72, disc loss: 0.00010435482564429985, policy loss: 14.49001706641155
Experience 14, Iter 73, disc loss: 0.00015161573291293004, policy loss: 14.321620239984682
Experience 14, Iter 74, disc loss: 0.000180301508781534, policy loss: 14.99146059943084
Experience 14, Iter 75, disc loss: 0.00034471882810784417, policy loss: 13.544423568762603
Experience 14, Iter 76, disc loss: 0.00021176760075293537, policy loss: 15.164704753082194
Experience 14, Iter 77, disc loss: 0.00023107490718574444, policy loss: 14.045313796474312
Experience 14, Iter 78, disc loss: 0.0003294369291669087, policy loss: 13.776513748013466
Experience 14, Iter 79, disc loss: 0.00013099090546151186, policy loss: 14.493240108774959
Experience 14, Iter 80, disc loss: 0.0002856168312365758, policy loss: 14.335115282572469
Experience 14, Iter 81, disc loss: 0.0001369066642222879, policy loss: 14.662992143353188
Experience 14, Iter 82, disc loss: 0.0002285426439796037, policy loss: 13.507163300662471
Experience 14, Iter 83, disc loss: 0.000136829127742229, policy loss: 14.87331917753232
Experience 14, Iter 84, disc loss: 0.0001598173776999197, policy loss: 14.468669141342051
Experience 14, Iter 85, disc loss: 9.877125336935008e-05, policy loss: 14.649784503045765
Experience 14, Iter 86, disc loss: 0.0009743113399272971, policy loss: 12.691982972690605
Experience 14, Iter 87, disc loss: 0.000142775162225492, policy loss: 14.036005873899775
Experience 14, Iter 88, disc loss: 0.00010535197988911674, policy loss: 14.6951780200764
Experience 14, Iter 89, disc loss: 0.00012332032828342757, policy loss: 14.090520822854312
Experience 14, Iter 90, disc loss: 0.0003225270830794374, policy loss: 13.654021900991221
Experience 14, Iter 91, disc loss: 0.0007100959927613276, policy loss: 14.772793346937362
Experience 14, Iter 92, disc loss: 0.0004014620373467123, policy loss: 14.892267251127254
Experience 14, Iter 93, disc loss: 0.00011472392459116763, policy loss: 14.607795772147504
Experience 14, Iter 94, disc loss: 0.0005808649631461318, policy loss: 13.84498770144576
Experience 14, Iter 95, disc loss: 0.00012118504562109384, policy loss: 14.942632387846642
Experience 14, Iter 96, disc loss: 0.00011373850389368993, policy loss: 14.337574670387799
Experience 14, Iter 97, disc loss: 0.00021322141513209324, policy loss: 15.105476086263794
Experience 14, Iter 98, disc loss: 0.000133391102012167, policy loss: 13.796619298970795
Experience 14, Iter 99, disc loss: 0.00013261167373174328, policy loss: 14.737665515142147
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[6.4823e-04],
        [7.4445e-02],
        [7.3374e-01],
        [1.5919e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.1264e-03, 3.0439e-02, 1.3635e-01, 1.9244e-03, 2.9617e-05,
          1.2848e+00]],

        [[5.1264e-03, 3.0439e-02, 1.3635e-01, 1.9244e-03, 2.9617e-05,
          1.2848e+00]],

        [[5.1264e-03, 3.0439e-02, 1.3635e-01, 1.9244e-03, 2.9617e-05,
          1.2848e+00]],

        [[5.1264e-03, 3.0439e-02, 1.3635e-01, 1.9244e-03, 2.9617e-05,
          1.2848e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.5929e-03, 2.9778e-01, 2.9350e+00, 6.3677e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.5929e-03, 2.9778e-01, 2.9350e+00, 6.3677e-03])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.414
Iter 2/2000 - Loss: 1.247
Iter 3/2000 - Loss: 0.405
Iter 4/2000 - Loss: 0.285
Iter 5/2000 - Loss: 0.630
Iter 6/2000 - Loss: 0.660
Iter 7/2000 - Loss: 0.403
Iter 8/2000 - Loss: 0.210
Iter 9/2000 - Loss: 0.249
Iter 10/2000 - Loss: 0.402
Iter 11/2000 - Loss: 0.450
Iter 12/2000 - Loss: 0.344
Iter 13/2000 - Loss: 0.219
Iter 14/2000 - Loss: 0.192
Iter 15/2000 - Loss: 0.233
Iter 16/2000 - Loss: 0.240
Iter 17/2000 - Loss: 0.208
Iter 18/2000 - Loss: 0.197
Iter 19/2000 - Loss: 0.198
Iter 20/2000 - Loss: 0.152
Iter 1981/2000 - Loss: -8.592
Iter 1982/2000 - Loss: -8.592
Iter 1983/2000 - Loss: -8.592
Iter 1984/2000 - Loss: -8.592
Iter 1985/2000 - Loss: -8.592
Iter 1986/2000 - Loss: -8.592
Iter 1987/2000 - Loss: -8.592
Iter 1988/2000 - Loss: -8.592
Iter 1989/2000 - Loss: -8.592
Iter 1990/2000 - Loss: -8.592
Iter 1991/2000 - Loss: -8.592
Iter 1992/2000 - Loss: -8.592
Iter 1993/2000 - Loss: -8.592
Iter 1994/2000 - Loss: -8.592
Iter 1995/2000 - Loss: -8.592
Iter 1996/2000 - Loss: -8.592
Iter 1997/2000 - Loss: -8.592
Iter 1998/2000 - Loss: -8.592
Iter 1999/2000 - Loss: -8.593
Iter 2000/2000 - Loss: -8.593
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[13.3796,  2.4665, 30.7728,  7.9186,  4.1658, 27.2608]],

        [[14.3377, 32.5096, 33.3435,  4.3785,  0.6093, 32.8715]],

        [[19.7335, 38.5273, 30.1696,  1.5256,  4.7530, 37.1360]],

        [[18.6232, 41.5720, 12.4099,  3.5208,  6.7887, 42.6555]]])
Signal Variance: tensor([1.9626e-02, 5.3820e+00, 3.4388e+01, 3.1187e-01])
Estimated target variance: tensor([2.5929e-03, 2.9778e-01, 2.9350e+00, 6.3677e-03])
N: 150
Signal to noise ratio: tensor([  7.3751, 139.9136, 107.1736,  32.9660])
Bound on condition number: tensor([   8159.8385, 2936373.2871, 1722926.9645,  163014.1013])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 9.527295357272109e-05, policy loss: 12.386102093421457
Experience 15, Iter 1, disc loss: 9.4578480105254e-05, policy loss: 12.587165312297
Experience 15, Iter 2, disc loss: 9.92688499248391e-05, policy loss: 12.091239171127981
Experience 15, Iter 3, disc loss: 0.0001146802155567621, policy loss: 11.503086804877446
Experience 15, Iter 4, disc loss: 0.00011008381658966405, policy loss: 11.633351560506616
Experience 15, Iter 5, disc loss: 0.0001592464714345164, policy loss: 10.760671496705406
Experience 15, Iter 6, disc loss: 0.00019483346466867203, policy loss: 10.330619672155596
Experience 15, Iter 7, disc loss: 0.0014437588053936698, policy loss: 8.614206774422156
Experience 15, Iter 8, disc loss: 0.005574989085001006, policy loss: 7.031864479018494
Experience 15, Iter 9, disc loss: 0.013242779786321381, policy loss: 6.219606568152708
Experience 15, Iter 10, disc loss: 0.009800318340104076, policy loss: 6.909363962360875
Experience 15, Iter 11, disc loss: 0.004416352631915, policy loss: 8.482598548716961
Experience 15, Iter 12, disc loss: 0.005234709588876652, policy loss: 7.895518526238943
Experience 15, Iter 13, disc loss: 0.006255242000724502, policy loss: 7.440533403649203
Experience 15, Iter 14, disc loss: 0.00896926618629104, policy loss: 7.261746916249464
Experience 15, Iter 15, disc loss: 0.005372460333656702, policy loss: 7.60312999321083
Experience 15, Iter 16, disc loss: 0.0053617842711189065, policy loss: 7.530424497036667
Experience 15, Iter 17, disc loss: 0.003228672351405645, policy loss: 7.71456038407764
Experience 15, Iter 18, disc loss: 0.002188204693430402, policy loss: 7.962698800413833
Experience 15, Iter 19, disc loss: 0.002151229769344073, policy loss: 7.865259059959337
Experience 15, Iter 20, disc loss: 0.0024147010833880365, policy loss: 7.562699237700991
Experience 15, Iter 21, disc loss: 0.0026639880177637895, policy loss: 7.915833021439672
Experience 15, Iter 22, disc loss: 0.0019999809112688357, policy loss: 8.539351764600159
Experience 15, Iter 23, disc loss: 0.0020889394943689114, policy loss: 8.243170179376731
Experience 15, Iter 24, disc loss: 0.002154858856289295, policy loss: 7.9497742282879695
Experience 15, Iter 25, disc loss: 0.0024481063128940135, policy loss: 7.992023775282874
Experience 15, Iter 26, disc loss: 0.0017495622749013132, policy loss: 8.408937038192892
Experience 15, Iter 27, disc loss: 0.002299114077166164, policy loss: 8.57175316701155
Experience 15, Iter 28, disc loss: 0.0019602681037865803, policy loss: 8.111720278370989
Experience 15, Iter 29, disc loss: 0.001824574253796707, policy loss: 8.737488114265025
Experience 15, Iter 30, disc loss: 0.003118148994605381, policy loss: 8.090577558183327
Experience 15, Iter 31, disc loss: 0.0022097108744798697, policy loss: 8.005641923571272
Experience 15, Iter 32, disc loss: 0.0022150239580101, policy loss: 8.20617954848714
Experience 15, Iter 33, disc loss: 0.0026653917998345103, policy loss: 7.8419899470343895
Experience 15, Iter 34, disc loss: 0.0028055428658814436, policy loss: 7.621803860026608
Experience 15, Iter 35, disc loss: 0.0021040579436466134, policy loss: 8.325794847946279
Experience 15, Iter 36, disc loss: 0.0026542878645139855, policy loss: 7.5407508687760725
Experience 15, Iter 37, disc loss: 0.0028260422778455686, policy loss: 7.680950135527899
Experience 15, Iter 38, disc loss: 0.0020248346814043692, policy loss: 8.215016420499623
Experience 15, Iter 39, disc loss: 0.0027260833951705025, policy loss: 7.880192794823183
Experience 15, Iter 40, disc loss: 0.0028234921897377354, policy loss: 7.982982371687359
Experience 15, Iter 41, disc loss: 0.002354449029877624, policy loss: 7.8505006368810495
Experience 15, Iter 42, disc loss: 0.002397520137614623, policy loss: 8.093006247666244
Experience 15, Iter 43, disc loss: 0.00218249575242531, policy loss: 7.968853351804963
Experience 15, Iter 44, disc loss: 0.004269200429704353, policy loss: 7.569567077199437
Experience 15, Iter 45, disc loss: 0.0027316179467424843, policy loss: 7.90395028420769
Experience 15, Iter 46, disc loss: 0.002777243685885712, policy loss: 8.25177193621942
Experience 15, Iter 47, disc loss: 0.0036846785038216726, policy loss: 8.089520461449029
Experience 15, Iter 48, disc loss: 0.004658137033348242, policy loss: 7.293561443844199
Experience 15, Iter 49, disc loss: 0.003508291340370698, policy loss: 8.204546412153277
Experience 15, Iter 50, disc loss: 0.002943546989696791, policy loss: 7.795422254654385
Experience 15, Iter 51, disc loss: 0.003893164196972499, policy loss: 8.076722701273916
Experience 15, Iter 52, disc loss: 0.0030257019564035505, policy loss: 7.464394154579716
Experience 15, Iter 53, disc loss: 0.0038604424636698802, policy loss: 7.548135354706014
Experience 15, Iter 54, disc loss: 0.0026627677733696946, policy loss: 7.87245984620839
Experience 15, Iter 55, disc loss: 0.0032634442889019656, policy loss: 7.682560527789429
Experience 15, Iter 56, disc loss: 0.0029810380858142195, policy loss: 8.387273937818081
Experience 15, Iter 57, disc loss: 0.004654632830488295, policy loss: 7.554359700442645
Experience 15, Iter 58, disc loss: 0.004183300156352206, policy loss: 7.484013527190369
Experience 15, Iter 59, disc loss: 0.002745507517082619, policy loss: 7.85150549345467
Experience 15, Iter 60, disc loss: 0.0037255369450725903, policy loss: 7.778478311469426
Experience 15, Iter 61, disc loss: 0.002407943959105252, policy loss: 7.659074055962674
Experience 15, Iter 62, disc loss: 0.004449136118260132, policy loss: 7.850274724209702
Experience 15, Iter 63, disc loss: 0.003810801815895314, policy loss: 7.742692900365453
Experience 15, Iter 64, disc loss: 0.0034510810746816766, policy loss: 8.21488709756795
Experience 15, Iter 65, disc loss: 0.003470659096338971, policy loss: 8.049954183964909
Experience 15, Iter 66, disc loss: 0.0030058390144727084, policy loss: 7.874088560821702
Experience 15, Iter 67, disc loss: 0.0034007095302916986, policy loss: 8.10185177013102
Experience 15, Iter 68, disc loss: 0.0029249922228114698, policy loss: 8.208059082983432
Experience 15, Iter 69, disc loss: 0.003860081353516214, policy loss: 7.856962948932452
Experience 15, Iter 70, disc loss: 0.0019885860240211268, policy loss: 8.381750377551807
Experience 15, Iter 71, disc loss: 0.002837418261243142, policy loss: 7.714416642917866
Experience 15, Iter 72, disc loss: 0.0028830667324449234, policy loss: 7.722606418919083
Experience 15, Iter 73, disc loss: 0.003194512078629446, policy loss: 7.438666487415488
Experience 15, Iter 74, disc loss: 0.0036235709456321733, policy loss: 8.165059337379706
Experience 15, Iter 75, disc loss: 0.0029885807357114876, policy loss: 8.137202924302123
Experience 15, Iter 76, disc loss: 0.0030865616529890958, policy loss: 7.987153265499168
Experience 15, Iter 77, disc loss: 0.002783930780704407, policy loss: 8.091531210325561
Experience 15, Iter 78, disc loss: 0.002591435334354367, policy loss: 8.018770922073946
Experience 15, Iter 79, disc loss: 0.0037756109664590304, policy loss: 7.318536207937829
Experience 15, Iter 80, disc loss: 0.003493842566565139, policy loss: 8.187724891463958
Experience 15, Iter 81, disc loss: 0.0028639324739237223, policy loss: 8.133650820858524
Experience 15, Iter 82, disc loss: 0.002276118987347281, policy loss: 8.409445895399095
Experience 15, Iter 83, disc loss: 0.0032929481661671954, policy loss: 7.930414574588358
Experience 15, Iter 84, disc loss: 0.006371578980297163, policy loss: 7.15089628853533
Experience 15, Iter 85, disc loss: 0.004371731390645601, policy loss: 7.954603612429256
Experience 15, Iter 86, disc loss: 0.00332670321670898, policy loss: 8.014911434980718
Experience 15, Iter 87, disc loss: 0.002351020889929863, policy loss: 7.98868902011617
Experience 15, Iter 88, disc loss: 0.0022839029803093927, policy loss: 8.438060393332846
Experience 15, Iter 89, disc loss: 0.002921903695152633, policy loss: 8.13080725821294
Experience 15, Iter 90, disc loss: 0.0030971943170831837, policy loss: 8.282129716001732
Experience 15, Iter 91, disc loss: 0.0019010193338768063, policy loss: 8.439770865511386
Experience 15, Iter 92, disc loss: 0.0035214386237047176, policy loss: 7.877952872985492
Experience 15, Iter 93, disc loss: 0.0031997279284380594, policy loss: 8.339000483354333
Experience 15, Iter 94, disc loss: 0.0035182979635296503, policy loss: 8.209256553167165
Experience 15, Iter 95, disc loss: 0.0027887835387685133, policy loss: 8.363479129760107
Experience 15, Iter 96, disc loss: 0.0029450338120749985, policy loss: 7.98045537413106
Experience 15, Iter 97, disc loss: 0.0029996180444182285, policy loss: 8.226611355658967
Experience 15, Iter 98, disc loss: 0.0024840378252070604, policy loss: 8.447350423195319
Experience 15, Iter 99, disc loss: 0.0035572430944318546, policy loss: 8.24112854047936
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[6.3403e-04],
        [7.8422e-02],
        [7.8129e-01],
        [1.8091e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.8743e-03, 3.0543e-02, 1.5063e-01, 1.9220e-03, 2.8679e-05,
          1.3353e+00]],

        [[4.8743e-03, 3.0543e-02, 1.5063e-01, 1.9220e-03, 2.8679e-05,
          1.3353e+00]],

        [[4.8743e-03, 3.0543e-02, 1.5063e-01, 1.9220e-03, 2.8679e-05,
          1.3353e+00]],

        [[4.8743e-03, 3.0543e-02, 1.5063e-01, 1.9220e-03, 2.8679e-05,
          1.3353e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.5361e-03, 3.1369e-01, 3.1252e+00, 7.2363e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.5361e-03, 3.1369e-01, 3.1252e+00, 7.2363e-03])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.517
Iter 2/2000 - Loss: 1.327
Iter 3/2000 - Loss: 0.507
Iter 4/2000 - Loss: 0.382
Iter 5/2000 - Loss: 0.724
Iter 6/2000 - Loss: 0.757
Iter 7/2000 - Loss: 0.505
Iter 8/2000 - Loss: 0.318
Iter 9/2000 - Loss: 0.357
Iter 10/2000 - Loss: 0.506
Iter 11/2000 - Loss: 0.549
Iter 12/2000 - Loss: 0.445
Iter 13/2000 - Loss: 0.325
Iter 14/2000 - Loss: 0.301
Iter 15/2000 - Loss: 0.341
Iter 16/2000 - Loss: 0.345
Iter 17/2000 - Loss: 0.310
Iter 18/2000 - Loss: 0.298
Iter 19/2000 - Loss: 0.301
Iter 20/2000 - Loss: 0.261
Iter 1981/2000 - Loss: -5.417
Iter 1982/2000 - Loss: -5.417
Iter 1983/2000 - Loss: -5.417
Iter 1984/2000 - Loss: -5.417
Iter 1985/2000 - Loss: -5.417
Iter 1986/2000 - Loss: -5.417
Iter 1987/2000 - Loss: -5.417
Iter 1988/2000 - Loss: -5.417
Iter 1989/2000 - Loss: -5.417
Iter 1990/2000 - Loss: -5.417
Iter 1991/2000 - Loss: -5.417
Iter 1992/2000 - Loss: -5.417
Iter 1993/2000 - Loss: -5.417
Iter 1994/2000 - Loss: -5.417
Iter 1995/2000 - Loss: -5.417
Iter 1996/2000 - Loss: -5.417
Iter 1997/2000 - Loss: -5.417
Iter 1998/2000 - Loss: -5.417
Iter 1999/2000 - Loss: -5.417
Iter 2000/2000 - Loss: -5.417
***AFTER OPTIMATION***
Noise Variance: tensor([[3.4894e-04],
        [2.8023e-04],
        [4.6244e-01],
        [2.8151e-04]])
Lengthscale: tensor([[[1.3512e+01, 2.5332e+00, 3.2835e+01, 7.4103e+00, 4.0645e+00,
          2.8137e+01]],

        [[1.5419e+01, 2.7070e+01, 3.4602e+01, 5.4692e+00, 5.3901e-01,
          3.9781e+01]],

        [[4.8046e-03, 2.1116e-02, 1.5064e-01, 1.9168e-03, 2.7635e-05,
          1.3352e+00]],

        [[1.8260e+01, 4.1962e+01, 1.4985e+01, 3.9343e+00, 6.9850e+00,
          4.5505e+01]]])
Signal Variance: tensor([0.0201, 6.5564, 2.6432, 0.4038])
Estimated target variance: tensor([2.5361e-03, 3.1369e-01, 3.1252e+00, 7.2363e-03])
N: 160
Signal to noise ratio: tensor([  7.5952, 152.9589,   2.3908,  37.8739])
Bound on condition number: tensor([9.2308e+03, 3.7434e+06, 9.1553e+02, 2.2951e+05])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.000799592155528856, policy loss: 18.372315907838946
Experience 16, Iter 1, disc loss: 0.0008020924741944607, policy loss: 18.956593537727287
Experience 16, Iter 2, disc loss: 0.0007693517126977426, policy loss: 19.937057535776496
Experience 16, Iter 3, disc loss: 0.0007504318706753975, policy loss: 18.977709083346333
Experience 16, Iter 4, disc loss: 0.0011248852416187592, policy loss: 21.12192818129186
Experience 16, Iter 5, disc loss: 0.0006899393222114503, policy loss: 19.61577690232481
Experience 16, Iter 6, disc loss: 0.0008677454077092666, policy loss: 17.3316250869031
Experience 16, Iter 7, disc loss: 0.0005976232141094498, policy loss: 18.469314466279947
Experience 16, Iter 8, disc loss: 0.0005666121953432, policy loss: 18.642705578785645
Experience 16, Iter 9, disc loss: 0.000660549169053628, policy loss: 18.333865328032477
Experience 16, Iter 10, disc loss: 0.0005075271642902267, policy loss: 19.97616907553212
Experience 16, Iter 11, disc loss: 0.0004734613171486756, policy loss: 20.00214588158115
Experience 16, Iter 12, disc loss: 0.0004400102108980126, policy loss: 20.089012318536007
Experience 16, Iter 13, disc loss: 0.0004324787175072729, policy loss: 18.612130137882318
Experience 16, Iter 14, disc loss: 0.0003907533537592346, policy loss: 20.454482096365787
Experience 16, Iter 15, disc loss: 0.0007533353867128553, policy loss: 17.09787158637757
Experience 16, Iter 16, disc loss: 0.00037111726958038294, policy loss: 17.560184786394945
Experience 16, Iter 17, disc loss: 0.00039305763494725633, policy loss: 18.36270601621211
Experience 16, Iter 18, disc loss: 0.0004424647585229483, policy loss: 19.78908789961232
Experience 16, Iter 19, disc loss: 0.0003007904899262203, policy loss: 19.87157458336413
Experience 16, Iter 20, disc loss: 0.0002833814000620289, policy loss: 18.543086925707623
Experience 16, Iter 21, disc loss: 0.00032317618861075147, policy loss: 18.61083689720724
Experience 16, Iter 22, disc loss: 0.00034682178541219717, policy loss: 19.31294679576434
Experience 16, Iter 23, disc loss: 0.00025727266527879474, policy loss: 19.237242692073806
Experience 16, Iter 24, disc loss: 0.00025414005161866055, policy loss: 17.942611779948344
Experience 16, Iter 25, disc loss: 0.0002210775981095332, policy loss: 17.504968193227384
Experience 16, Iter 26, disc loss: 0.0002140768289267219, policy loss: 19.050085892462846
Experience 16, Iter 27, disc loss: 0.00019921108166611918, policy loss: 18.0265105673935
Experience 16, Iter 28, disc loss: 0.0005044969458170123, policy loss: 17.6214727299051
Experience 16, Iter 29, disc loss: 0.00021611648892527993, policy loss: 18.8514741386181
Experience 16, Iter 30, disc loss: 0.0002258921667726416, policy loss: 18.74040336129169
Experience 16, Iter 31, disc loss: 0.00029349073255609064, policy loss: 19.856517470136332
Experience 16, Iter 32, disc loss: 0.0001943222571685121, policy loss: 18.528793674086636
Experience 16, Iter 33, disc loss: 0.00017214355900048886, policy loss: 20.525192362075643
Experience 16, Iter 34, disc loss: 0.00024190876036085874, policy loss: 18.783167834877357
Experience 16, Iter 35, disc loss: 0.0002036814543029229, policy loss: 20.882707379398344
Experience 16, Iter 36, disc loss: 0.00031429737648114883, policy loss: 18.297250047393025
Experience 16, Iter 37, disc loss: 0.0004496630953346229, policy loss: 18.71123614027661
Experience 16, Iter 38, disc loss: 0.0002275502786786824, policy loss: 19.299031635388896
Experience 16, Iter 39, disc loss: 0.00018488616713980644, policy loss: 18.62868093781266
Experience 16, Iter 40, disc loss: 0.00018096478629765054, policy loss: 18.169032188166575
Experience 16, Iter 41, disc loss: 0.00014736137508541107, policy loss: 18.737429833263764
Experience 16, Iter 42, disc loss: 0.00012847399971194397, policy loss: 18.315778891979996
Experience 16, Iter 43, disc loss: 0.0001241409873883095, policy loss: 19.937327591554734
Experience 16, Iter 44, disc loss: 0.0001493690538008101, policy loss: 19.22936069494989
Experience 16, Iter 45, disc loss: 0.00047915811601042147, policy loss: 18.531218618292776
Experience 16, Iter 46, disc loss: 0.0003151335753312322, policy loss: 17.4288807259238
Experience 16, Iter 47, disc loss: 0.00015092260987306357, policy loss: 18.848517211155126
Experience 16, Iter 48, disc loss: 0.00015858606791586033, policy loss: 19.039582002353676
Experience 16, Iter 49, disc loss: 0.00024059230622053914, policy loss: 18.042345700189266
Experience 16, Iter 50, disc loss: 0.00041380116603089746, policy loss: 18.573956064970332
Experience 16, Iter 51, disc loss: 0.00012757058918391628, policy loss: 17.187220785672977
Experience 16, Iter 52, disc loss: 0.0004827322951101343, policy loss: 17.194777252230264
Experience 16, Iter 53, disc loss: 0.0001152259613499584, policy loss: 17.39568415875657
Experience 16, Iter 54, disc loss: 0.00012459317228732706, policy loss: 17.32825787386846
Experience 16, Iter 55, disc loss: 0.00013585550551518613, policy loss: 18.75474638623071
Experience 16, Iter 56, disc loss: 0.00017430983231856746, policy loss: 18.455453822359804
Experience 16, Iter 57, disc loss: 0.0013493168706405943, policy loss: 17.578743725573624
Experience 16, Iter 58, disc loss: 0.0002862952469155169, policy loss: 16.965058293777304
Experience 16, Iter 59, disc loss: 9.933745975177428e-05, policy loss: 18.937424367529264
Experience 16, Iter 60, disc loss: 0.0003057421346620901, policy loss: 17.6053810695226
Experience 16, Iter 61, disc loss: 0.0001981081967894191, policy loss: 18.1510005120378
Experience 16, Iter 62, disc loss: 0.00011197930143447812, policy loss: 17.317805154760578
Experience 16, Iter 63, disc loss: 0.00016940129172001253, policy loss: 18.850242076234412
Experience 16, Iter 64, disc loss: 9.653641779462691e-05, policy loss: 18.16618943747354
Experience 16, Iter 65, disc loss: 0.0003204652556450532, policy loss: 18.81470621001021
Experience 16, Iter 66, disc loss: 0.0005878993907501795, policy loss: 18.351838196024683
Experience 16, Iter 67, disc loss: 0.00012250347003745337, policy loss: 18.4226994414968
Experience 16, Iter 68, disc loss: 0.0001237968043696177, policy loss: 18.091683461907543
Experience 16, Iter 69, disc loss: 0.00010506416059890292, policy loss: 17.915797789659532
Experience 16, Iter 70, disc loss: 0.0001840462168503909, policy loss: 19.967124045738828
Experience 16, Iter 71, disc loss: 0.007409088883720666, policy loss: 17.443489334018178
Experience 16, Iter 72, disc loss: 0.00011586922885020308, policy loss: 17.399028635041514
Experience 16, Iter 73, disc loss: 0.00022262491039409782, policy loss: 16.238167475916004
Experience 16, Iter 74, disc loss: 0.00028555938695219333, policy loss: 17.577117510221647
Experience 16, Iter 75, disc loss: 0.0001447805148238519, policy loss: 17.701388655772963
Experience 16, Iter 76, disc loss: 0.00013355199354156243, policy loss: 18.37388847774941
Experience 16, Iter 77, disc loss: 0.00017686288757320185, policy loss: 19.523952786552197
Experience 16, Iter 78, disc loss: 0.00034360074317576464, policy loss: 17.938232414251694
Experience 16, Iter 79, disc loss: 0.0001397374421171865, policy loss: 18.237409863202004
Experience 16, Iter 80, disc loss: 0.0001447533768244403, policy loss: 17.89539549136863
Experience 16, Iter 81, disc loss: 0.0002804229990766371, policy loss: 19.23883597823353
Experience 16, Iter 82, disc loss: 0.00016525311730979054, policy loss: 17.924790346501673
Experience 16, Iter 83, disc loss: 0.0001525173231816568, policy loss: 18.920969372112076
Experience 16, Iter 84, disc loss: 0.0001402897191287696, policy loss: 17.95902604217288
Experience 16, Iter 85, disc loss: 0.00016662065165173304, policy loss: 19.70833123245219
Experience 16, Iter 86, disc loss: 0.00022708039185135402, policy loss: 16.724885935910443
Experience 16, Iter 87, disc loss: 0.00013194056851740737, policy loss: 17.70360607614556
Experience 16, Iter 88, disc loss: 0.0001553509632160859, policy loss: 19.937106614314594
Experience 16, Iter 89, disc loss: 0.00019790747039909618, policy loss: 18.25800018887592
Experience 16, Iter 90, disc loss: 0.00013534215102305042, policy loss: 18.36740841671859
Experience 16, Iter 91, disc loss: 0.00017946745191258732, policy loss: 16.61636181655959
Experience 16, Iter 92, disc loss: 0.0001298659430136252, policy loss: 17.982240910725636
Experience 16, Iter 93, disc loss: 0.00012313542942009575, policy loss: 18.797710545768815
Experience 16, Iter 94, disc loss: 0.00013611857356125224, policy loss: 18.178702640800392
Experience 16, Iter 95, disc loss: 0.0007348597255451799, policy loss: 18.152090084976557
Experience 16, Iter 96, disc loss: 0.00011671372238807497, policy loss: 18.013960165761453
Experience 16, Iter 97, disc loss: 0.00018769045673049626, policy loss: 18.29576655765976
Experience 16, Iter 98, disc loss: 0.00011104709302545848, policy loss: 20.18787153747546
Experience 16, Iter 99, disc loss: 0.0001463737078229215, policy loss: 18.630014082330636
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[6.1158e-04],
        [8.0414e-02],
        [8.0135e-01],
        [1.8273e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.7510e-03, 2.9827e-02, 1.5470e-01, 1.8663e-03, 2.7757e-05,
          1.3628e+00]],

        [[4.7510e-03, 2.9827e-02, 1.5470e-01, 1.8663e-03, 2.7757e-05,
          1.3628e+00]],

        [[4.7510e-03, 2.9827e-02, 1.5470e-01, 1.8663e-03, 2.7757e-05,
          1.3628e+00]],

        [[4.7510e-03, 2.9827e-02, 1.5470e-01, 1.8663e-03, 2.7757e-05,
          1.3628e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.4463e-03, 3.2166e-01, 3.2054e+00, 7.3091e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.4463e-03, 3.2166e-01, 3.2054e+00, 7.3091e-03])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.538
Iter 2/2000 - Loss: 1.365
Iter 3/2000 - Loss: 0.527
Iter 4/2000 - Loss: 0.395
Iter 5/2000 - Loss: 0.746
Iter 6/2000 - Loss: 0.784
Iter 7/2000 - Loss: 0.528
Iter 8/2000 - Loss: 0.335
Iter 9/2000 - Loss: 0.371
Iter 10/2000 - Loss: 0.521
Iter 11/2000 - Loss: 0.571
Iter 12/2000 - Loss: 0.470
Iter 13/2000 - Loss: 0.348
Iter 14/2000 - Loss: 0.318
Iter 15/2000 - Loss: 0.356
Iter 16/2000 - Loss: 0.364
Iter 17/2000 - Loss: 0.329
Iter 18/2000 - Loss: 0.313
Iter 19/2000 - Loss: 0.318
Iter 20/2000 - Loss: 0.283
Iter 1981/2000 - Loss: -5.455
Iter 1982/2000 - Loss: -5.455
Iter 1983/2000 - Loss: -5.455
Iter 1984/2000 - Loss: -5.455
Iter 1985/2000 - Loss: -5.455
Iter 1986/2000 - Loss: -5.455
Iter 1987/2000 - Loss: -5.455
Iter 1988/2000 - Loss: -5.455
Iter 1989/2000 - Loss: -5.455
Iter 1990/2000 - Loss: -5.455
Iter 1991/2000 - Loss: -5.455
Iter 1992/2000 - Loss: -5.456
Iter 1993/2000 - Loss: -5.456
Iter 1994/2000 - Loss: -5.456
Iter 1995/2000 - Loss: -5.456
Iter 1996/2000 - Loss: -5.456
Iter 1997/2000 - Loss: -5.456
Iter 1998/2000 - Loss: -5.456
Iter 1999/2000 - Loss: -5.456
Iter 2000/2000 - Loss: -5.456
***AFTER OPTIMATION***
Noise Variance: tensor([[3.4864e-04],
        [2.7030e-04],
        [4.7273e-01],
        [2.7485e-04]])
Lengthscale: tensor([[[1.3458e+01, 2.4566e+00, 3.3307e+01, 7.1020e+00, 3.8960e+00,
          2.7113e+01]],

        [[1.5912e+01, 2.5460e+01, 3.4638e+01, 5.5107e+00, 5.3088e-01,
          3.9829e+01]],

        [[4.6730e-03, 2.1220e-02, 1.5426e-01, 1.8485e-03, 2.6823e-05,
          1.3627e+00]],

        [[1.8088e+01, 4.0222e+01, 1.5097e+01, 3.9352e+00, 6.8043e+00,
          4.6461e+01]]])
Signal Variance: tensor([0.0189, 6.4930, 2.7138, 0.4029])
Estimated target variance: tensor([2.4463e-03, 3.2166e-01, 3.2054e+00, 7.3091e-03])
N: 170
Signal to noise ratio: tensor([  7.3570, 154.9880,   2.3960,  38.2891])
Bound on condition number: tensor([9.2024e+03, 4.0836e+06, 9.7692e+02, 2.4923e+05])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.00015427241846769764, policy loss: 19.388882049264215
Experience 17, Iter 1, disc loss: 0.00017431579890057035, policy loss: 18.440925911816645
Experience 17, Iter 2, disc loss: 0.0001368524327899176, policy loss: 17.123026241573115
Experience 17, Iter 3, disc loss: 0.0001122986149513829, policy loss: 18.971599635381537
Experience 17, Iter 4, disc loss: 0.0003422066376005778, policy loss: 18.162905967877027
Experience 17, Iter 5, disc loss: 0.00013118868415792337, policy loss: 18.175543788028758
Experience 17, Iter 6, disc loss: 0.00023119370393499627, policy loss: 17.493676133985552
Experience 17, Iter 7, disc loss: 0.00011403448564637595, policy loss: 17.85516106265131
Experience 17, Iter 8, disc loss: 0.0007429820551626164, policy loss: 17.998679042759342
Experience 17, Iter 9, disc loss: 0.0004749089443537581, policy loss: 18.127485311095956
Experience 17, Iter 10, disc loss: 0.0008381674511069772, policy loss: 19.852219610954634
Experience 17, Iter 11, disc loss: 0.0006707081377564741, policy loss: 16.65651100178022
Experience 17, Iter 12, disc loss: 0.00011337340391475535, policy loss: 16.81299152464871
Experience 17, Iter 13, disc loss: 0.0001026781077352271, policy loss: 18.57765618108827
Experience 17, Iter 14, disc loss: 0.0001056908966400246, policy loss: 19.120148447133886
Experience 17, Iter 15, disc loss: 0.00010251938220715647, policy loss: 19.802187336128195
Experience 17, Iter 16, disc loss: 0.0001076482238959169, policy loss: 18.457278172550474
Experience 17, Iter 17, disc loss: 0.0001500967574961555, policy loss: 18.857515953718398
Experience 17, Iter 18, disc loss: 0.00023097624760457937, policy loss: 21.364978692745353
Experience 17, Iter 19, disc loss: 0.0001017739926446652, policy loss: 19.294896222963263
Experience 17, Iter 20, disc loss: 0.0001282645292962236, policy loss: 18.981069076514856
Experience 17, Iter 21, disc loss: 0.00011497769871285269, policy loss: 18.541525389975877
Experience 17, Iter 22, disc loss: 0.00013189645082456382, policy loss: 18.81071688126977
Experience 17, Iter 23, disc loss: 9.535086033101112e-05, policy loss: 19.508981278545917
Experience 17, Iter 24, disc loss: 0.0002605957300143781, policy loss: 17.586626613560725
Experience 17, Iter 25, disc loss: 0.00012374715102192715, policy loss: 19.971679218573858
Experience 17, Iter 26, disc loss: 9.94260906953124e-05, policy loss: 20.46523496571575
Experience 17, Iter 27, disc loss: 0.0001746987857670331, policy loss: 17.666362001738065
Experience 17, Iter 28, disc loss: 0.00010122911677618321, policy loss: 19.33745209207541
Experience 17, Iter 29, disc loss: 0.00010162592836390946, policy loss: 19.654354557870867
Experience 17, Iter 30, disc loss: 0.00025586705350875007, policy loss: 19.017928631091515
Experience 17, Iter 31, disc loss: 0.00021705143872727625, policy loss: 18.14574954679025
Experience 17, Iter 32, disc loss: 0.0013269243717829442, policy loss: 18.365148904782025
Experience 17, Iter 33, disc loss: 9.143245726034782e-05, policy loss: 19.222542881533816
Experience 17, Iter 34, disc loss: 0.00010403578598798598, policy loss: 18.866747983572623
Experience 17, Iter 35, disc loss: 0.0001468212470049239, policy loss: 18.80646297762018
Experience 17, Iter 36, disc loss: 0.0002903687934430779, policy loss: 19.197103260230037
Experience 17, Iter 37, disc loss: 0.0001723313788647872, policy loss: 18.60476781867912
Experience 17, Iter 38, disc loss: 0.00011413010061391778, policy loss: 18.48721712762581
Experience 17, Iter 39, disc loss: 9.728761529566187e-05, policy loss: 17.977045203752436
Experience 17, Iter 40, disc loss: 8.575202707606294e-05, policy loss: 19.878338116715376
Experience 17, Iter 41, disc loss: 0.00030004941958223304, policy loss: 18.152205369377384
Experience 17, Iter 42, disc loss: 9.914631032337151e-05, policy loss: 19.702813704388912
Experience 17, Iter 43, disc loss: 0.00014728642594652566, policy loss: 18.554305618848634
Experience 17, Iter 44, disc loss: 0.0001322594748812518, policy loss: 18.929593427571028
Experience 17, Iter 45, disc loss: 9.020485742209755e-05, policy loss: 18.8911210478064
Experience 17, Iter 46, disc loss: 0.0002651812203517122, policy loss: 18.13481852241314
Experience 17, Iter 47, disc loss: 9.72754401277858e-05, policy loss: 19.07190566460671
Experience 17, Iter 48, disc loss: 0.00010069492129115678, policy loss: 18.420191374523856
Experience 17, Iter 49, disc loss: 0.00010612706521207947, policy loss: 18.775596961375882
Experience 17, Iter 50, disc loss: 9.928670020956581e-05, policy loss: 18.869644235650924
Experience 17, Iter 51, disc loss: 0.00010389589450120124, policy loss: 19.942903516975864
Experience 17, Iter 52, disc loss: 8.764748079490536e-05, policy loss: 19.069281382900257
Experience 17, Iter 53, disc loss: 8.608096610391395e-05, policy loss: 19.02225590519818
Experience 17, Iter 54, disc loss: 0.00037072454981252603, policy loss: 17.232252326665126
Experience 17, Iter 55, disc loss: 9.033913208684751e-05, policy loss: 18.46967219088365
Experience 17, Iter 56, disc loss: 0.00010418244410483841, policy loss: 20.460905676868297
Experience 17, Iter 57, disc loss: 8.418074029833453e-05, policy loss: 18.686346562129238
Experience 17, Iter 58, disc loss: 0.00015310310757120368, policy loss: 17.444378866878335
Experience 17, Iter 59, disc loss: 8.304149349687125e-05, policy loss: 18.622749213012263
Experience 17, Iter 60, disc loss: 0.00013512323724853588, policy loss: 17.674761045394348
Experience 17, Iter 61, disc loss: 0.0013319408564979195, policy loss: 18.924070612388554
Experience 17, Iter 62, disc loss: 7.818889472490052e-05, policy loss: 18.22664272378715
Experience 17, Iter 63, disc loss: 9.675839702182924e-05, policy loss: 17.3869913126574
Experience 17, Iter 64, disc loss: 8.985190142799811e-05, policy loss: 17.53764219187056
Experience 17, Iter 65, disc loss: 0.00024811269337384867, policy loss: 18.28463425420862
Experience 17, Iter 66, disc loss: 7.575010128037849e-05, policy loss: 18.82616500379757
Experience 17, Iter 67, disc loss: 0.0002420164932516624, policy loss: 19.217750208907194
Experience 17, Iter 68, disc loss: 7.446226574893992e-05, policy loss: 20.002329114808383
Experience 17, Iter 69, disc loss: 7.930406383884453e-05, policy loss: 18.783519996848444
Experience 17, Iter 70, disc loss: 9.435276654295844e-05, policy loss: 19.38556808403196
Experience 17, Iter 71, disc loss: 7.423742384366953e-05, policy loss: 17.682602523223103
Experience 17, Iter 72, disc loss: 0.00014858498960743715, policy loss: 18.42196612212851
Experience 17, Iter 73, disc loss: 8.945184118740204e-05, policy loss: 19.930660908213824
Experience 17, Iter 74, disc loss: 7.943506966078653e-05, policy loss: 19.942670478740215
Experience 17, Iter 75, disc loss: 9.288113207180387e-05, policy loss: 18.42912591781923
Experience 17, Iter 76, disc loss: 0.00025145389699902693, policy loss: 19.009428810995438
Experience 17, Iter 77, disc loss: 7.196369206398246e-05, policy loss: 18.909238682205277
Experience 17, Iter 78, disc loss: 0.0004956296105656125, policy loss: 17.77141238816847
Experience 17, Iter 79, disc loss: 7.866137743490862e-05, policy loss: 19.79343777013817
Experience 17, Iter 80, disc loss: 0.0001392422663620002, policy loss: 18.936007207150915
Experience 17, Iter 81, disc loss: 0.00019607850922614685, policy loss: 20.02760024873231
Experience 17, Iter 82, disc loss: 0.0001138730039752552, policy loss: 19.244117127834645
Experience 17, Iter 83, disc loss: 6.837647817474012e-05, policy loss: 19.618129606226724
Experience 17, Iter 84, disc loss: 7.99145328907996e-05, policy loss: 18.434691461052388
Experience 17, Iter 85, disc loss: 0.00011689400765787705, policy loss: 19.589716506791625
Experience 17, Iter 86, disc loss: 6.939760228623643e-05, policy loss: 18.00394661953021
Experience 17, Iter 87, disc loss: 0.0003782520086506445, policy loss: 17.671220696880358
Experience 17, Iter 88, disc loss: 7.367024368513893e-05, policy loss: 17.18169664175085
Experience 17, Iter 89, disc loss: 7.507412885713158e-05, policy loss: 19.319635765612933
Experience 17, Iter 90, disc loss: 0.0002400269356389211, policy loss: 18.206932153219036
Experience 17, Iter 91, disc loss: 0.0002662912756899207, policy loss: 18.56473185355916
Experience 17, Iter 92, disc loss: 0.00018299624809865248, policy loss: 16.550642455989028
Experience 17, Iter 93, disc loss: 8.736298858877925e-05, policy loss: 17.15006889965273
Experience 17, Iter 94, disc loss: 0.00010786240400215655, policy loss: 19.243135468049566
Experience 17, Iter 95, disc loss: 7.266657691381766e-05, policy loss: 18.449800655522374
Experience 17, Iter 96, disc loss: 0.0001432824808826537, policy loss: 19.461605144356405
Experience 17, Iter 97, disc loss: 0.00023887603460728186, policy loss: 18.682288552925968
Experience 17, Iter 98, disc loss: 0.00010415319240313675, policy loss: 18.844689445825693
Experience 17, Iter 99, disc loss: 7.903663376918926e-05, policy loss: 17.373623560502416
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[5.8648e-04],
        [9.4216e-02],
        [9.2399e-01],
        [1.7781e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.6066e-03, 3.0444e-02, 1.6587e-01, 1.8469e-03, 2.7165e-05,
          1.6014e+00]],

        [[4.6066e-03, 3.0444e-02, 1.6587e-01, 1.8469e-03, 2.7165e-05,
          1.6014e+00]],

        [[4.6066e-03, 3.0444e-02, 1.6587e-01, 1.8469e-03, 2.7165e-05,
          1.6014e+00]],

        [[4.6066e-03, 3.0444e-02, 1.6587e-01, 1.8469e-03, 2.7165e-05,
          1.6014e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.3459e-03, 3.7686e-01, 3.6960e+00, 7.1124e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.3459e-03, 3.7686e-01, 3.6960e+00, 7.1124e-03])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.667
Iter 2/2000 - Loss: 1.502
Iter 3/2000 - Loss: 0.654
Iter 4/2000 - Loss: 0.514
Iter 5/2000 - Loss: 0.868
Iter 6/2000 - Loss: 0.906
Iter 7/2000 - Loss: 0.648
Iter 8/2000 - Loss: 0.455
Iter 9/2000 - Loss: 0.493
Iter 10/2000 - Loss: 0.646
Iter 11/2000 - Loss: 0.696
Iter 12/2000 - Loss: 0.593
Iter 13/2000 - Loss: 0.465
Iter 14/2000 - Loss: 0.433
Iter 15/2000 - Loss: 0.477
Iter 16/2000 - Loss: 0.495
Iter 17/2000 - Loss: 0.457
Iter 18/2000 - Loss: 0.426
Iter 19/2000 - Loss: 0.427
Iter 20/2000 - Loss: 0.402
Iter 1981/2000 - Loss: -5.360
Iter 1982/2000 - Loss: -5.360
Iter 1983/2000 - Loss: -5.360
Iter 1984/2000 - Loss: -5.360
Iter 1985/2000 - Loss: -5.360
Iter 1986/2000 - Loss: -5.360
Iter 1987/2000 - Loss: -5.360
Iter 1988/2000 - Loss: -5.360
Iter 1989/2000 - Loss: -5.360
Iter 1990/2000 - Loss: -5.360
Iter 1991/2000 - Loss: -5.360
Iter 1992/2000 - Loss: -5.360
Iter 1993/2000 - Loss: -5.360
Iter 1994/2000 - Loss: -5.360
Iter 1995/2000 - Loss: -5.360
Iter 1996/2000 - Loss: -5.360
Iter 1997/2000 - Loss: -5.360
Iter 1998/2000 - Loss: -5.360
Iter 1999/2000 - Loss: -5.360
Iter 2000/2000 - Loss: -5.360
***AFTER OPTIMATION***
Noise Variance: tensor([[3.5086e-04],
        [2.7392e-04],
        [5.3289e-01],
        [2.9422e-04]])
Lengthscale: tensor([[[1.3121e+01, 2.3781e+00, 3.6484e+01, 6.8903e+00, 3.9196e+00,
          2.7951e+01]],

        [[1.6483e+01, 2.5735e+01, 3.4998e+01, 5.6166e+00, 5.2145e-01,
          4.3794e+01]],

        [[4.5214e-03, 2.1509e-02, 1.6545e-01, 1.8280e-03, 2.6156e-05,
          1.6013e+00]],

        [[1.6861e+01, 3.9733e+01, 1.4875e+01, 3.1621e+00, 6.3773e+00,
          4.0518e+01]]])
Signal Variance: tensor([0.0181, 6.6075, 3.1424, 0.3666])
Estimated target variance: tensor([2.3459e-03, 3.7686e-01, 3.6960e+00, 7.1124e-03])
N: 180
Signal to noise ratio: tensor([  7.1852, 155.3130,   2.4283,  35.3004])
Bound on condition number: tensor([9.2938e+03, 4.3420e+06, 1.0624e+03, 2.2430e+05])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 9.46834698660437e-05, policy loss: 21.794731123916115
Experience 18, Iter 1, disc loss: 7.662740012484882e-05, policy loss: 17.694555956933993
Experience 18, Iter 2, disc loss: 6.460229549800156e-05, policy loss: 20.341694966685665
Experience 18, Iter 3, disc loss: 6.803420301240351e-05, policy loss: 19.235012437288916
Experience 18, Iter 4, disc loss: 7.703825004451285e-05, policy loss: 20.072749305960162
Experience 18, Iter 5, disc loss: 6.132634274233285e-05, policy loss: 22.172698859353027
Experience 18, Iter 6, disc loss: 6.805308713679202e-05, policy loss: 19.37597798463805
Experience 18, Iter 7, disc loss: 6.925527765582414e-05, policy loss: 18.789032292844485
Experience 18, Iter 8, disc loss: 6.128720024929716e-05, policy loss: 19.953004669113376
Experience 18, Iter 9, disc loss: 8.092585519928604e-05, policy loss: 20.534934943454953
Experience 18, Iter 10, disc loss: 6.028332211720844e-05, policy loss: 17.95360415119398
Experience 18, Iter 11, disc loss: 0.00040269739064067344, policy loss: 19.78577510032752
Experience 18, Iter 12, disc loss: 6.05126836577226e-05, policy loss: 20.55784688096142
Experience 18, Iter 13, disc loss: 9.338172332195399e-05, policy loss: 18.343487445890336
Experience 18, Iter 14, disc loss: 0.00015186903405882274, policy loss: 17.883282334655224
Experience 18, Iter 15, disc loss: 7.026991531853248e-05, policy loss: 20.435170915505637
Experience 18, Iter 16, disc loss: 0.0007544478575818744, policy loss: 18.84779601622417
Experience 18, Iter 17, disc loss: 0.00013429409004834457, policy loss: 21.425013238098657
Experience 18, Iter 18, disc loss: 6.405165892116007e-05, policy loss: 19.701900952229504
Experience 18, Iter 19, disc loss: 0.0001345272977759473, policy loss: 20.387451271432866
Experience 18, Iter 20, disc loss: 5.940036014250406e-05, policy loss: 20.189795634764025
Experience 18, Iter 21, disc loss: 7.665246630743732e-05, policy loss: 18.988382479317007
Experience 18, Iter 22, disc loss: 6.971037308819896e-05, policy loss: 19.129202916654897
Experience 18, Iter 23, disc loss: 0.0009399111043754608, policy loss: 18.752577275356728
Experience 18, Iter 24, disc loss: 7.158622982021859e-05, policy loss: 19.516669277856735
Experience 18, Iter 25, disc loss: 0.00012102389177066736, policy loss: 19.789043672729186
Experience 18, Iter 26, disc loss: 5.6370559840302234e-05, policy loss: 20.206671170361282
Experience 18, Iter 27, disc loss: 6.0216296144222894e-05, policy loss: 18.846842978224814
Experience 18, Iter 28, disc loss: 0.00016983581485143737, policy loss: 18.806757709595825
Experience 18, Iter 29, disc loss: 0.0006096680390913814, policy loss: 20.44070913240299
Experience 18, Iter 30, disc loss: 0.0002277420824350749, policy loss: 21.898611774660257
Experience 18, Iter 31, disc loss: 5.9970108464183934e-05, policy loss: 19.14621300208787
Experience 18, Iter 32, disc loss: 6.492329831366395e-05, policy loss: 18.3557621156146
Experience 18, Iter 33, disc loss: 0.00018944444612785442, policy loss: 18.443336261292025
Experience 18, Iter 34, disc loss: 0.00018896593774614777, policy loss: 18.708832769213192
Experience 18, Iter 35, disc loss: 0.0001926526702257469, policy loss: 21.193256403307107
Experience 18, Iter 36, disc loss: 6.264327625584462e-05, policy loss: 19.016808372296495
Experience 18, Iter 37, disc loss: 0.00032273327013765564, policy loss: 18.48756507797269
Experience 18, Iter 38, disc loss: 5.680003673662915e-05, policy loss: 20.617575546429073
Experience 18, Iter 39, disc loss: 0.001135816275567776, policy loss: 18.856535145393302
Experience 18, Iter 40, disc loss: 0.0003796012737183926, policy loss: 18.782553811010864
Experience 18, Iter 41, disc loss: 5.805922110947643e-05, policy loss: 19.24614921491867
Experience 18, Iter 42, disc loss: 0.00021856082889439702, policy loss: 19.598035678171843
Experience 18, Iter 43, disc loss: 0.0001621279036110457, policy loss: 18.21981524815375
Experience 18, Iter 44, disc loss: 0.00012844745477621653, policy loss: 18.68443257255706
Experience 18, Iter 45, disc loss: 6.216981816284599e-05, policy loss: 19.28588333320073
Experience 18, Iter 46, disc loss: 7.898028332753782e-05, policy loss: 20.563535035799077
Experience 18, Iter 47, disc loss: 0.0003995833880182449, policy loss: 18.060833423259318
Experience 18, Iter 48, disc loss: 7.744781737532197e-05, policy loss: 19.259359081455774
Experience 18, Iter 49, disc loss: 6.453143767808049e-05, policy loss: 19.54585048205658
Experience 18, Iter 50, disc loss: 0.0006758022264169087, policy loss: 18.707501994622568
Experience 18, Iter 51, disc loss: 0.00026434865551178274, policy loss: 21.514312445749155
Experience 18, Iter 52, disc loss: 0.00020696035298765843, policy loss: 18.547613095317477
Experience 18, Iter 53, disc loss: 6.319412903450299e-05, policy loss: 19.86360608433096
Experience 18, Iter 54, disc loss: 0.00019925976251762822, policy loss: 17.60942594069018
Experience 18, Iter 55, disc loss: 7.427924293905699e-05, policy loss: 19.188885989907106
Experience 18, Iter 56, disc loss: 0.0002741930795990986, policy loss: 17.429867120801518
Experience 18, Iter 57, disc loss: 9.058099483702803e-05, policy loss: 21.28571921327427
Experience 18, Iter 58, disc loss: 0.00010239261383760499, policy loss: 18.650310059547547
Experience 18, Iter 59, disc loss: 7.593053182022451e-05, policy loss: 19.549304772195914
Experience 18, Iter 60, disc loss: 7.346958427154107e-05, policy loss: 19.1461619617819
Experience 18, Iter 61, disc loss: 6.895978246952865e-05, policy loss: 19.909884283619686
Experience 18, Iter 62, disc loss: 8.729284820369934e-05, policy loss: 18.458507741148424
Experience 18, Iter 63, disc loss: 0.00012697181847149726, policy loss: 21.663865409793235
Experience 18, Iter 64, disc loss: 0.0001087684269221072, policy loss: 19.21402069426643
Experience 18, Iter 65, disc loss: 7.349623572179208e-05, policy loss: 20.465033658908922
Experience 18, Iter 66, disc loss: 6.873469753900434e-05, policy loss: 21.81682377953391
Experience 18, Iter 67, disc loss: 6.823758157635242e-05, policy loss: 20.612515874770175
Experience 18, Iter 68, disc loss: 0.00010764664589311984, policy loss: 19.06622967848252
Experience 18, Iter 69, disc loss: 7.243240340203034e-05, policy loss: 19.582700286532734
Experience 18, Iter 70, disc loss: 6.473350932595701e-05, policy loss: 21.000604851040112
Experience 18, Iter 71, disc loss: 0.00013013954174199006, policy loss: 18.82309869291926
Experience 18, Iter 72, disc loss: 6.841104042782097e-05, policy loss: 18.96531476434424
Experience 18, Iter 73, disc loss: 7.012341279585204e-05, policy loss: 20.43677207862961
Experience 18, Iter 74, disc loss: 7.173265890533433e-05, policy loss: 21.466836004015583
Experience 18, Iter 75, disc loss: 6.145147917580914e-05, policy loss: 17.059985878121203
Experience 18, Iter 76, disc loss: 8.407646445344645e-05, policy loss: 20.734005975254085
Experience 18, Iter 77, disc loss: 5.883825042031275e-05, policy loss: 19.03145197570675
Experience 18, Iter 78, disc loss: 8.70979516733484e-05, policy loss: 19.638331068190833
Experience 18, Iter 79, disc loss: 8.173477174791347e-05, policy loss: 20.521184267165474
Experience 18, Iter 80, disc loss: 8.670013628324342e-05, policy loss: 17.922205881546407
Experience 18, Iter 81, disc loss: 0.0004886179797763966, policy loss: 19.649190620149895
Experience 18, Iter 82, disc loss: 7.14922153491564e-05, policy loss: 19.935361307602268
Experience 18, Iter 83, disc loss: 5.4957091370930864e-05, policy loss: 20.185005697025375
Experience 18, Iter 84, disc loss: 0.00010113847081088331, policy loss: 21.205079635476878
Experience 18, Iter 85, disc loss: 5.915784730284409e-05, policy loss: 21.9424437668916
Experience 18, Iter 86, disc loss: 5.9950742664430625e-05, policy loss: 20.769931028195224
Experience 18, Iter 87, disc loss: 5.722667424546176e-05, policy loss: 20.040931562424056
Experience 18, Iter 88, disc loss: 5.30900506569558e-05, policy loss: 21.202106003170915
Experience 18, Iter 89, disc loss: 6.98632697467805e-05, policy loss: 19.56651201412031
Experience 18, Iter 90, disc loss: 0.00027285889288203107, policy loss: 22.307843753500272
Experience 18, Iter 91, disc loss: 8.127143754497409e-05, policy loss: 18.129466018177503
Experience 18, Iter 92, disc loss: 5.5752653111811355e-05, policy loss: 18.69180574572865
Experience 18, Iter 93, disc loss: 0.0004831737957782224, policy loss: 19.99294788118341
Experience 18, Iter 94, disc loss: 5.3048265692759986e-05, policy loss: 20.09018941512922
Experience 18, Iter 95, disc loss: 7.087825656628014e-05, policy loss: 21.17432430451535
Experience 18, Iter 96, disc loss: 8.016031762782418e-05, policy loss: 19.401823705866754
Experience 18, Iter 97, disc loss: 5.380022952483052e-05, policy loss: 20.730865055949206
Experience 18, Iter 98, disc loss: 7.467640522096419e-05, policy loss: 19.488259687701873
Experience 18, Iter 99, disc loss: 5.3373237566660394e-05, policy loss: 20.547478557891566
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[5.6892e-04],
        [1.1263e-01],
        [1.0937e+00],
        [1.7760e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.4814e-03, 3.1669e-02, 1.8347e-01, 1.8142e-03, 2.6472e-05,
          1.9170e+00]],

        [[4.4814e-03, 3.1669e-02, 1.8347e-01, 1.8142e-03, 2.6472e-05,
          1.9170e+00]],

        [[4.4814e-03, 3.1669e-02, 1.8347e-01, 1.8142e-03, 2.6472e-05,
          1.9170e+00]],

        [[4.4814e-03, 3.1669e-02, 1.8347e-01, 1.8142e-03, 2.6472e-05,
          1.9170e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.2757e-03, 4.5053e-01, 4.3750e+00, 7.1039e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.2757e-03, 4.5053e-01, 4.3750e+00, 7.1039e-03])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.843
Iter 2/2000 - Loss: 1.667
Iter 3/2000 - Loss: 0.828
Iter 4/2000 - Loss: 0.673
Iter 5/2000 - Loss: 1.024
Iter 6/2000 - Loss: 1.072
Iter 7/2000 - Loss: 0.818
Iter 8/2000 - Loss: 0.619
Iter 9/2000 - Loss: 0.650
Iter 10/2000 - Loss: 0.802
Iter 11/2000 - Loss: 0.860
Iter 12/2000 - Loss: 0.763
Iter 13/2000 - Loss: 0.630
Iter 14/2000 - Loss: 0.590
Iter 15/2000 - Loss: 0.635
Iter 16/2000 - Loss: 0.659
Iter 17/2000 - Loss: 0.623
Iter 18/2000 - Loss: 0.585
Iter 19/2000 - Loss: 0.582
Iter 20/2000 - Loss: 0.563
Iter 1981/2000 - Loss: -8.674
Iter 1982/2000 - Loss: -8.674
Iter 1983/2000 - Loss: -8.674
Iter 1984/2000 - Loss: -8.674
Iter 1985/2000 - Loss: -8.674
Iter 1986/2000 - Loss: -8.674
Iter 1987/2000 - Loss: -8.674
Iter 1988/2000 - Loss: -8.674
Iter 1989/2000 - Loss: -8.674
Iter 1990/2000 - Loss: -8.674
Iter 1991/2000 - Loss: -8.674
Iter 1992/2000 - Loss: -8.674
Iter 1993/2000 - Loss: -8.674
Iter 1994/2000 - Loss: -8.674
Iter 1995/2000 - Loss: -8.674
Iter 1996/2000 - Loss: -8.674
Iter 1997/2000 - Loss: -8.674
Iter 1998/2000 - Loss: -8.674
Iter 1999/2000 - Loss: -8.674
Iter 2000/2000 - Loss: -8.674
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[13.3836,  2.6706, 37.0380,  7.4084,  4.1087, 30.8325]],

        [[16.2887, 25.8288, 37.3960,  6.5653,  0.5240, 47.4198]],

        [[18.7559, 42.8119, 14.9792,  1.1134,  3.2214, 22.4486]],

        [[16.8424, 39.9067, 14.8139,  2.7958,  5.9999, 39.8782]]])
Signal Variance: tensor([2.3419e-02, 7.5328e+00, 2.3870e+01, 3.5874e-01])
Estimated target variance: tensor([2.2757e-03, 4.5053e-01, 4.3750e+00, 7.1039e-03])
N: 190
Signal to noise ratio: tensor([  8.0405, 161.8010,  93.0963,  35.2300])
Bound on condition number: tensor([  12284.4975, 4974117.1627, 1646714.3540,  235820.2152])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 4.9637670700509196e-05, policy loss: 15.210629561638749
Experience 19, Iter 1, disc loss: 4.914937983165888e-05, policy loss: 15.469221578906412
Experience 19, Iter 2, disc loss: 4.9031806995782616e-05, policy loss: 15.035656417250589
Experience 19, Iter 3, disc loss: 4.887383520441642e-05, policy loss: 14.72883958774154
Experience 19, Iter 4, disc loss: 4.911673146153014e-05, policy loss: 14.518027309903506
Experience 19, Iter 5, disc loss: 4.917205067818901e-05, policy loss: 14.208703462494066
Experience 19, Iter 6, disc loss: 5.054476470550564e-05, policy loss: 13.439973561933247
Experience 19, Iter 7, disc loss: 5.5924159394633345e-05, policy loss: 12.990486091047076
Experience 19, Iter 8, disc loss: 7.497949757269464e-05, policy loss: 11.981820711749801
Experience 19, Iter 9, disc loss: 0.00010503859750973096, policy loss: 11.569801379661309
Experience 19, Iter 10, disc loss: 0.000162067948548722, policy loss: 10.519222025882932
Experience 19, Iter 11, disc loss: 0.0009624142665407127, policy loss: 9.306512825150438
Experience 19, Iter 12, disc loss: 0.0006470428451672697, policy loss: 10.256212737459967
Experience 19, Iter 13, disc loss: 0.0014344436265031082, policy loss: 9.898882613693623
Experience 19, Iter 14, disc loss: 0.0005020195298584226, policy loss: 11.104480407381152
Experience 19, Iter 15, disc loss: 0.0003628932214168811, policy loss: 9.95935663636562
Experience 19, Iter 16, disc loss: 0.002445088301949202, policy loss: 9.638737975275047
Experience 19, Iter 17, disc loss: 0.00156529096487316, policy loss: 9.241589815601637
Experience 19, Iter 18, disc loss: 0.003618156291445895, policy loss: 8.497446421916173
Experience 19, Iter 19, disc loss: 0.009155657920165388, policy loss: 8.908638942572559
Experience 19, Iter 20, disc loss: 0.002710444882233428, policy loss: 8.388163147778394
Experience 19, Iter 21, disc loss: 0.0024422168423020615, policy loss: 8.545664304388808
Experience 19, Iter 22, disc loss: 0.0014676133302942076, policy loss: 8.617048482318602
Experience 19, Iter 23, disc loss: 0.001344264149125703, policy loss: 8.503475213247189
Experience 19, Iter 24, disc loss: 0.002503669822124673, policy loss: 8.115411908503464
Experience 19, Iter 25, disc loss: 0.003371031846244082, policy loss: 8.114382700952305
Experience 19, Iter 26, disc loss: 0.0025390451072797066, policy loss: 8.425519651343418
Experience 19, Iter 27, disc loss: 0.0027983129243635755, policy loss: 8.271926974065217
Experience 19, Iter 28, disc loss: 0.0040774596758602075, policy loss: 7.944349413320051
Experience 19, Iter 29, disc loss: 0.003461664826130327, policy loss: 8.16593029755554
Experience 19, Iter 30, disc loss: 0.0023132400171402613, policy loss: 8.337877314551898
Experience 19, Iter 31, disc loss: 0.0017161790524309979, policy loss: 8.958544184803237
Experience 19, Iter 32, disc loss: 0.002524129599806334, policy loss: 8.393192352823299
Experience 19, Iter 33, disc loss: 0.0041094209976144476, policy loss: 7.936261491671168
Experience 19, Iter 34, disc loss: 0.001780372592666093, policy loss: 8.857610826705812
Experience 19, Iter 35, disc loss: 0.0027219256975487267, policy loss: 8.472005733700302
Experience 19, Iter 36, disc loss: 0.0019425900444377381, policy loss: 8.783633036906487
Experience 19, Iter 37, disc loss: 0.0014352555896195137, policy loss: 8.9712059529558
Experience 19, Iter 38, disc loss: 0.0030640241615475863, policy loss: 8.327014755282823
Experience 19, Iter 39, disc loss: 0.0016696700406449896, policy loss: 8.970366548003284
Experience 19, Iter 40, disc loss: 0.0017982817767932101, policy loss: 8.304558218716007
Experience 19, Iter 41, disc loss: 0.00234948341541754, policy loss: 8.927552555896355
Experience 19, Iter 42, disc loss: 0.0021615865356773446, policy loss: 8.3097161435039
Experience 19, Iter 43, disc loss: 0.0016795928234694408, policy loss: 8.82522894851869
Experience 19, Iter 44, disc loss: 0.002287462489043224, policy loss: 8.693753768216984
Experience 19, Iter 45, disc loss: 0.003211249112567546, policy loss: 8.478849895319737
Experience 19, Iter 46, disc loss: 0.0018032852129394898, policy loss: 8.80035609344619
Experience 19, Iter 47, disc loss: 0.002059939541254494, policy loss: 8.67119331274496
Experience 19, Iter 48, disc loss: 0.002313380457849879, policy loss: 8.8202031812394
Experience 19, Iter 49, disc loss: 0.0018157395263251684, policy loss: 8.802921323545062
Experience 19, Iter 50, disc loss: 0.002848973556253313, policy loss: 8.713423673843383
Experience 19, Iter 51, disc loss: 0.0016875379860490695, policy loss: 8.719472792674205
Experience 19, Iter 52, disc loss: 0.003486422586328794, policy loss: 8.791534267057415
Experience 19, Iter 53, disc loss: 0.0034353884113945063, policy loss: 8.503221692381496
Experience 19, Iter 54, disc loss: 0.0028578154933081353, policy loss: 8.72105416793083
Experience 19, Iter 55, disc loss: 0.002200111801270991, policy loss: 8.682368173113701
Experience 19, Iter 56, disc loss: 0.0024847171762456275, policy loss: 8.506321592577411
Experience 19, Iter 57, disc loss: 0.0019165553281838175, policy loss: 8.941042304100389
Experience 19, Iter 58, disc loss: 0.002012893901232042, policy loss: 8.915976643781708
Experience 19, Iter 59, disc loss: 0.0021851948453355747, policy loss: 8.897179857003522
Experience 19, Iter 60, disc loss: 0.002755437854021586, policy loss: 8.15519492457936
Experience 19, Iter 61, disc loss: 0.001752279696142574, policy loss: 9.004769056230451
Experience 19, Iter 62, disc loss: 0.001807407359598418, policy loss: 8.557604646605682
Experience 19, Iter 63, disc loss: 0.002304242022380464, policy loss: 8.93793476484988
Experience 19, Iter 64, disc loss: 0.0029508409437114446, policy loss: 8.205175819193252
Experience 19, Iter 65, disc loss: 0.002963508458794493, policy loss: 8.260481774264608
Experience 19, Iter 66, disc loss: 0.0025779385769803846, policy loss: 9.335764185231202
Experience 19, Iter 67, disc loss: 0.002019317780242188, policy loss: 8.907365732632082
Experience 19, Iter 68, disc loss: 0.0035046425466627416, policy loss: 8.094834072479241
Experience 19, Iter 69, disc loss: 0.0015739832021362502, policy loss: 8.551082564419179
Experience 19, Iter 70, disc loss: 0.0017279039009942812, policy loss: 8.437107395326503
Experience 19, Iter 71, disc loss: 0.001360969129207946, policy loss: 9.248867495590646
Experience 19, Iter 72, disc loss: 0.0017197279355742356, policy loss: 9.288421665897244
Experience 19, Iter 73, disc loss: 0.002502365587079583, policy loss: 8.626696635711944
Experience 19, Iter 74, disc loss: 0.0024777873499248182, policy loss: 7.951442804124891
Experience 19, Iter 75, disc loss: 0.0012461024550271269, policy loss: 8.693402755284426
Experience 19, Iter 76, disc loss: 0.001786911218324663, policy loss: 8.29483660438463
Experience 19, Iter 77, disc loss: 0.0020965397556184976, policy loss: 8.552806663978604
Experience 19, Iter 78, disc loss: 0.001609432683446075, policy loss: 8.870826306059731
Experience 19, Iter 79, disc loss: 0.0015055847715994678, policy loss: 8.58580045550861
Experience 19, Iter 80, disc loss: 0.00157413202292101, policy loss: 8.624320809733101
Experience 19, Iter 81, disc loss: 0.0015592669930500854, policy loss: 9.000038609115556
Experience 19, Iter 82, disc loss: 0.0015100465181749295, policy loss: 8.942718565342673
Experience 19, Iter 83, disc loss: 0.0015401493231016546, policy loss: 8.84708454615134
Experience 19, Iter 84, disc loss: 0.0013348029919633463, policy loss: 8.586965781969242
Experience 19, Iter 85, disc loss: 0.0019136620019924748, policy loss: 8.708150085413262
Experience 19, Iter 86, disc loss: 0.0014979901061589807, policy loss: 8.572254616229351
Experience 19, Iter 87, disc loss: 0.0013610858052449733, policy loss: 8.811459131209674
Experience 19, Iter 88, disc loss: 0.0015866274629125119, policy loss: 8.725245243047869
Experience 19, Iter 89, disc loss: 0.0010605319053695334, policy loss: 8.855844905965307
Experience 19, Iter 90, disc loss: 0.0017377078359973833, policy loss: 8.79118639426273
Experience 19, Iter 91, disc loss: 0.0016641203681040787, policy loss: 8.684255974567767
Experience 19, Iter 92, disc loss: 0.0013555332747143885, policy loss: 8.989007335883962
Experience 19, Iter 93, disc loss: 0.0020669476561513025, policy loss: 8.859635148627119
Experience 19, Iter 94, disc loss: 0.0015797783253869492, policy loss: 9.196599397344375
Experience 19, Iter 95, disc loss: 0.0023817350425691317, policy loss: 8.660615713488127
Experience 19, Iter 96, disc loss: 0.0019866170950327525, policy loss: 8.664982754846855
Experience 19, Iter 97, disc loss: 0.001992705068714663, policy loss: 8.355502922518212
Experience 19, Iter 98, disc loss: 0.0020553435103494136, policy loss: 8.491083838039534
Experience 19, Iter 99, disc loss: 0.0009516965467203324, policy loss: 8.931625578371673
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[5.5671e-04],
        [1.0873e-01],
        [1.0620e+00],
        [1.8320e-03]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3622e-03, 3.0767e-02, 1.8295e-01, 1.8276e-03, 2.5431e-05,
          1.8460e+00]],

        [[4.3622e-03, 3.0767e-02, 1.8295e-01, 1.8276e-03, 2.5431e-05,
          1.8460e+00]],

        [[4.3622e-03, 3.0767e-02, 1.8295e-01, 1.8276e-03, 2.5431e-05,
          1.8460e+00]],

        [[4.3622e-03, 3.0767e-02, 1.8295e-01, 1.8276e-03, 2.5431e-05,
          1.8460e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([2.2268e-03, 4.3494e-01, 4.2481e+00, 7.3281e-03],
       grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([2.2268e-03, 4.3494e-01, 4.2481e+00, 7.3281e-03])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.829
Iter 2/2000 - Loss: 1.627
Iter 3/2000 - Loss: 0.809
Iter 4/2000 - Loss: 0.643
Iter 5/2000 - Loss: 0.986
Iter 6/2000 - Loss: 1.042
Iter 7/2000 - Loss: 0.795
Iter 8/2000 - Loss: 0.595
Iter 9/2000 - Loss: 0.621
Iter 10/2000 - Loss: 0.772
Iter 11/2000 - Loss: 0.833
Iter 12/2000 - Loss: 0.738
Iter 13/2000 - Loss: 0.603
Iter 14/2000 - Loss: 0.560
Iter 15/2000 - Loss: 0.606
Iter 16/2000 - Loss: 0.636
Iter 17/2000 - Loss: 0.599
Iter 18/2000 - Loss: 0.555
Iter 19/2000 - Loss: 0.549
Iter 20/2000 - Loss: 0.537
Iter 1981/2000 - Loss: -5.316
Iter 1982/2000 - Loss: -5.316
Iter 1983/2000 - Loss: -5.316
Iter 1984/2000 - Loss: -5.316
Iter 1985/2000 - Loss: -5.316
Iter 1986/2000 - Loss: -5.316
Iter 1987/2000 - Loss: -5.316
Iter 1988/2000 - Loss: -5.316
Iter 1989/2000 - Loss: -5.316
Iter 1990/2000 - Loss: -5.316
Iter 1991/2000 - Loss: -5.316
Iter 1992/2000 - Loss: -5.316
Iter 1993/2000 - Loss: -5.316
Iter 1994/2000 - Loss: -5.316
Iter 1995/2000 - Loss: -5.316
Iter 1996/2000 - Loss: -5.316
Iter 1997/2000 - Loss: -5.316
Iter 1998/2000 - Loss: -5.316
Iter 1999/2000 - Loss: -5.316
Iter 2000/2000 - Loss: -5.316
***AFTER OPTIMATION***
Noise Variance: tensor([[3.5998e-04],
        [2.8231e-04],
        [6.0005e-01],
        [2.8212e-04]])
Lengthscale: tensor([[[1.2874e+01, 2.7227e+00, 3.6171e+01, 7.6795e+00, 4.2976e+00,
          3.1775e+01]],

        [[1.6646e+01, 2.7060e+01, 3.6966e+01, 7.4126e+00, 5.1488e-01,
          4.8124e+01]],

        [[4.2776e-03, 2.2128e-02, 1.8288e-01, 1.8123e-03, 2.4391e-05,
          1.8459e+00]],

        [[1.6308e+01, 3.9251e+01, 1.5061e+01, 2.7539e+00, 5.9063e+00,
          4.0185e+01]]])
Signal Variance: tensor([0.0249, 7.9993, 3.6268, 0.3625])
Estimated target variance: tensor([2.2268e-03, 4.3494e-01, 4.2481e+00, 7.3281e-03])
N: 200
Signal to noise ratio: tensor([  8.3157, 168.3303,   2.4585,  35.8454])
Bound on condition number: tensor([1.3831e+04, 5.6670e+06, 1.2098e+03, 2.5698e+05])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.00041286948535799734, policy loss: 22.488365564513302
Experience 20, Iter 1, disc loss: 0.00042895744295941935, policy loss: 23.222874467851653
Experience 20, Iter 2, disc loss: 0.00041157057876419, policy loss: 24.982197301207794
Experience 20, Iter 3, disc loss: 0.0003987450537117299, policy loss: 22.982182520571953
Experience 20, Iter 4, disc loss: 0.00038847506570538134, policy loss: 22.21537306995239
Experience 20, Iter 5, disc loss: 0.00042132890270335364, policy loss: 22.283734451262518
Experience 20, Iter 6, disc loss: 0.000565355781873776, policy loss: 23.49427918117086
Experience 20, Iter 7, disc loss: 0.0003530807385652476, policy loss: 23.761627727797524
Experience 20, Iter 8, disc loss: 0.0004181369338020599, policy loss: 24.44019071515799
Experience 20, Iter 9, disc loss: 0.0003246463355549856, policy loss: 24.09550765615876
Experience 20, Iter 10, disc loss: 0.00030761883020561927, policy loss: 23.540894327485468
Experience 20, Iter 11, disc loss: 0.00031120762993070106, policy loss: 21.196905122103225
Experience 20, Iter 12, disc loss: 0.0003291496682350563, policy loss: 24.271518373246074
Experience 20, Iter 13, disc loss: 0.0002715715097628558, policy loss: 22.270900086022955
Experience 20, Iter 14, disc loss: 0.00025514545858671865, policy loss: 23.231065842922142
Experience 20, Iter 15, disc loss: 0.00025443193939868265, policy loss: 22.57978102521905
Experience 20, Iter 16, disc loss: 0.00023986716209028943, policy loss: 23.034324377384447
Experience 20, Iter 17, disc loss: 0.0003131151935492069, policy loss: 23.587463557678333
Experience 20, Iter 18, disc loss: 0.0003575995751207057, policy loss: 21.533947618800287
Experience 20, Iter 19, disc loss: 0.0002099722286716843, policy loss: 22.530449388262785
Experience 20, Iter 20, disc loss: 0.00019659777219561806, policy loss: 21.112395830853714
Experience 20, Iter 21, disc loss: 0.00018527894294058148, policy loss: 22.186717060218847
Experience 20, Iter 22, disc loss: 0.0003720242340229951, policy loss: 22.98360681825297
Experience 20, Iter 23, disc loss: 0.000169913855523576, policy loss: 22.09610443485046
Experience 20, Iter 24, disc loss: 0.000163132012041969, policy loss: 24.352029172483242
Experience 20, Iter 25, disc loss: 0.00016432584193484172, policy loss: 23.941183581054414
Experience 20, Iter 26, disc loss: 0.0006653863347900314, policy loss: 21.50625911573122
Experience 20, Iter 27, disc loss: 0.00021426783370946662, policy loss: 21.884791453679593
Experience 20, Iter 28, disc loss: 0.00025950040790482674, policy loss: 23.31459753418434
Experience 20, Iter 29, disc loss: 0.0002268998949624603, policy loss: 21.24746346154693
Experience 20, Iter 30, disc loss: 0.0001393973789683724, policy loss: 22.82513765189486
Experience 20, Iter 31, disc loss: 0.00014149219856888164, policy loss: 22.292454185341086
Experience 20, Iter 32, disc loss: 0.00015272709604424752, policy loss: 23.058954597511033
Experience 20, Iter 33, disc loss: 0.00012765708391711535, policy loss: 22.095972679858686
Experience 20, Iter 34, disc loss: 0.00012172789028152105, policy loss: 23.607438799511662
Experience 20, Iter 35, disc loss: 0.00013953894790691348, policy loss: 22.13647917053985
Experience 20, Iter 36, disc loss: 0.00012377032477101714, policy loss: 21.750958507694136
Experience 20, Iter 37, disc loss: 0.00011112876308041096, policy loss: 22.30693974955623
Experience 20, Iter 38, disc loss: 0.00010933677699952315, policy loss: 22.5248682956301
Experience 20, Iter 39, disc loss: 0.00013126853161308927, policy loss: 20.680773246104433
Experience 20, Iter 40, disc loss: 0.00011524588444991428, policy loss: 22.554265307319156
Experience 20, Iter 41, disc loss: 0.00013677362234121787, policy loss: 24.372374076393697
Experience 20, Iter 42, disc loss: 0.00011264044104412388, policy loss: 23.3390777581139
Experience 20, Iter 43, disc loss: 0.0001762673462156141, policy loss: 24.313870376510664
Experience 20, Iter 44, disc loss: 0.00010215807831145006, policy loss: 22.14236261852198
Experience 20, Iter 45, disc loss: 0.00011151391640271425, policy loss: 23.13408824132291
Experience 20, Iter 46, disc loss: 0.00013182795393685222, policy loss: 23.165491662207653
Experience 20, Iter 47, disc loss: 0.00010970775851648925, policy loss: 23.7997236019449
Experience 20, Iter 48, disc loss: 0.0001127990138668615, policy loss: 24.096502051380824
Experience 20, Iter 49, disc loss: 9.962155681900337e-05, policy loss: 23.647737812181155
Experience 20, Iter 50, disc loss: 8.287702115978245e-05, policy loss: 22.553174214963526
Experience 20, Iter 51, disc loss: 0.00014249323438953292, policy loss: 23.78823835601601
Experience 20, Iter 52, disc loss: 0.00012876011227929524, policy loss: 23.53442734924009
Experience 20, Iter 53, disc loss: 8.786450785958183e-05, policy loss: 22.93343073800784
Experience 20, Iter 54, disc loss: 7.8502958494544e-05, policy loss: 25.199464210093627
Experience 20, Iter 55, disc loss: 7.865333686998741e-05, policy loss: 22.18506621006032
Experience 20, Iter 56, disc loss: 7.362164728647547e-05, policy loss: 25.12584775505195
Experience 20, Iter 57, disc loss: 7.416070253886821e-05, policy loss: 24.605930497952265
Experience 20, Iter 58, disc loss: 0.000215742919175873, policy loss: 22.60146467006433
Experience 20, Iter 59, disc loss: 0.00010520496089680233, policy loss: 19.633928797538708
Experience 20, Iter 60, disc loss: 7.53820114703342e-05, policy loss: 23.151359392342158
Experience 20, Iter 61, disc loss: 0.00015251459070736168, policy loss: 21.96788367399083
Experience 20, Iter 62, disc loss: 7.097237838796667e-05, policy loss: 22.119153492338828
Experience 20, Iter 63, disc loss: 0.00010039722569530235, policy loss: 22.744128679256484
Experience 20, Iter 64, disc loss: 0.00018478274900067125, policy loss: 21.352303757010255
Experience 20, Iter 65, disc loss: 7.977720789124348e-05, policy loss: 22.212299614617343
Experience 20, Iter 66, disc loss: 0.0001249720836083275, policy loss: 21.795161643251284
Experience 20, Iter 67, disc loss: 0.0002129406588089183, policy loss: 21.505277300936953
Experience 20, Iter 68, disc loss: 0.0001390614553879, policy loss: 21.957005160791926
Experience 20, Iter 69, disc loss: 6.268559850993225e-05, policy loss: 23.44067488648478
Experience 20, Iter 70, disc loss: 6.378192288630421e-05, policy loss: 24.9304397235648
Experience 20, Iter 71, disc loss: 0.0008310299515028361, policy loss: 22.851477484997773
Experience 20, Iter 72, disc loss: 0.00010015356385317974, policy loss: 20.03996404570931
Experience 20, Iter 73, disc loss: 6.614216300930609e-05, policy loss: 24.76334280501177
Experience 20, Iter 74, disc loss: 7.64822228672019e-05, policy loss: 21.27558664440596
Experience 20, Iter 75, disc loss: 6.489210075792166e-05, policy loss: 22.344534035846088
Experience 20, Iter 76, disc loss: 7.405700246735863e-05, policy loss: 23.214203412054545
Experience 20, Iter 77, disc loss: 8.586162310188935e-05, policy loss: 23.88991289791974
Experience 20, Iter 78, disc loss: 0.00026673076068124923, policy loss: 21.673562229235042
Experience 20, Iter 79, disc loss: 7.120923054779725e-05, policy loss: 21.658415813640154
Experience 20, Iter 80, disc loss: 6.003649251764422e-05, policy loss: 24.00306747950388
Experience 20, Iter 81, disc loss: 0.00029727943877830873, policy loss: 20.380004769897326
Experience 20, Iter 82, disc loss: 9.032776136539298e-05, policy loss: 20.327026656305776
Experience 20, Iter 83, disc loss: 6.764828687652343e-05, policy loss: 22.476507477044656
Experience 20, Iter 84, disc loss: 9.81831845734865e-05, policy loss: 22.339818457424762
Experience 20, Iter 85, disc loss: 6.108027320494625e-05, policy loss: 22.140686653757854
Experience 20, Iter 86, disc loss: 5.914685447473456e-05, policy loss: 22.425559454275895
Experience 20, Iter 87, disc loss: 0.00010757360452172415, policy loss: 24.289658376340444
Experience 20, Iter 88, disc loss: 9.476427568055906e-05, policy loss: 22.305125577322308
Experience 20, Iter 89, disc loss: 6.729797856492103e-05, policy loss: 22.54608645352834
Experience 20, Iter 90, disc loss: 7.469270100925272e-05, policy loss: 21.822175829928888
Experience 20, Iter 91, disc loss: 6.757868160943149e-05, policy loss: 22.397451849537802
Experience 20, Iter 92, disc loss: 7.89324549339828e-05, policy loss: 19.878918181557374
Experience 20, Iter 93, disc loss: 7.628599994947552e-05, policy loss: 22.859671870948063
Experience 20, Iter 94, disc loss: 0.00026068782370442043, policy loss: 23.330524350070164
Experience 20, Iter 95, disc loss: 0.0002483549523099705, policy loss: 21.43977693721827
Experience 20, Iter 96, disc loss: 0.0002069510670787325, policy loss: 21.060714090933743
Experience 20, Iter 97, disc loss: 5.852288198173219e-05, policy loss: 22.04689263266186
Experience 20, Iter 98, disc loss: 0.00171510345890228, policy loss: 22.788367275729218
Experience 20, Iter 99, disc loss: 8.599532156042157e-05, policy loss: 22.667646047198126
