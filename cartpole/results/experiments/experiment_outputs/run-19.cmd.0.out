Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0032],
        [0.2402],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0017, 0.0129, 0.9607, 0.0199], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0017, 0.0129, 0.9607, 0.0199])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.536
Iter 2/2000 - Loss: -1.435
Iter 3/2000 - Loss: -0.765
Iter 4/2000 - Loss: -0.704
Iter 5/2000 - Loss: -1.103
Iter 6/2000 - Loss: -1.418
Iter 7/2000 - Loss: -1.425
Iter 8/2000 - Loss: -1.284
Iter 9/2000 - Loss: -1.224
Iter 10/2000 - Loss: -1.315
Iter 11/2000 - Loss: -1.459
Iter 12/2000 - Loss: -1.537
Iter 13/2000 - Loss: -1.518
Iter 14/2000 - Loss: -1.459
Iter 15/2000 - Loss: -1.446
Iter 16/2000 - Loss: -1.502
Iter 17/2000 - Loss: -1.581
Iter 18/2000 - Loss: -1.629
Iter 19/2000 - Loss: -1.634
Iter 20/2000 - Loss: -1.619
Iter 1981/2000 - Loss: -1.861
Iter 1982/2000 - Loss: -1.858
Iter 1983/2000 - Loss: -1.855
Iter 1984/2000 - Loss: -1.849
Iter 1985/2000 - Loss: -1.843
Iter 1986/2000 - Loss: -1.837
Iter 1987/2000 - Loss: -1.837
Iter 1988/2000 - Loss: -1.846
Iter 1989/2000 - Loss: -1.858
Iter 1990/2000 - Loss: -1.861
Iter 1991/2000 - Loss: -1.855
Iter 1992/2000 - Loss: -1.849
Iter 1993/2000 - Loss: -1.850
Iter 1994/2000 - Loss: -1.855
Iter 1995/2000 - Loss: -1.860
Iter 1996/2000 - Loss: -1.863
Iter 1997/2000 - Loss: -1.863
Iter 1998/2000 - Loss: -1.860
Iter 1999/2000 - Loss: -1.857
Iter 2000/2000 - Loss: -1.853
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0023],
        [0.1468],
        [0.0036]])
Lengthscale: tensor([[[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]],

        [[6.0762e-03, 1.3693e-02, 2.2449e-01, 5.4515e-03, 1.3972e-04,
          8.1469e-03]]])
Signal Variance: tensor([0.0013, 0.0093, 0.7178, 0.0144])
Estimated target variance: tensor([0.0017, 0.0129, 0.9607, 0.0199])
N: 10
Signal to noise ratio: tensor([1.9999, 2.0035, 2.2112, 2.0072])
Bound on condition number: tensor([40.9961, 41.1396, 49.8934, 41.2893])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4009322546255631, policy loss: 0.6613368051598263
Experience 1, Iter 1, disc loss: 1.3812758784069659, policy loss: 0.6654862511601098
Experience 1, Iter 2, disc loss: 1.3732735156089462, policy loss: 0.6604794462867041
Experience 1, Iter 3, disc loss: 1.3493743357034154, policy loss: 0.6697983257683926
Experience 1, Iter 4, disc loss: 1.3388340380504096, policy loss: 0.6674962834774949
Experience 1, Iter 5, disc loss: 1.3218726308961153, policy loss: 0.6718666158104523
Experience 1, Iter 6, disc loss: 1.3074112244873626, policy loss: 0.672853824023871
Experience 1, Iter 7, disc loss: 1.2965401747067324, policy loss: 0.6709835646702484
Experience 1, Iter 8, disc loss: 1.2828668025565573, policy loss: 0.6723534026146566
Experience 1, Iter 9, disc loss: 1.267237521665681, policy loss: 0.6762139637530279
Experience 1, Iter 10, disc loss: 1.2466420944417556, policy loss: 0.6843134988924241
Experience 1, Iter 11, disc loss: 1.242979262027041, policy loss: 0.6757944699519658
Experience 1, Iter 12, disc loss: 1.2232652936706638, policy loss: 0.6836791012737037
Experience 1, Iter 13, disc loss: 1.1939400718268116, policy loss: 0.7029753182955227
Experience 1, Iter 14, disc loss: 1.20221303981529, policy loss: 0.6812313441499569
Experience 1, Iter 15, disc loss: 1.180067103694465, policy loss: 0.6937656388799931
Experience 1, Iter 16, disc loss: 1.1780601903241226, policy loss: 0.6834251827987148
Experience 1, Iter 17, disc loss: 1.1428294794283251, policy loss: 0.7084182725443111
Experience 1, Iter 18, disc loss: 1.1425332567121282, policy loss: 0.6953118084515623
Experience 1, Iter 19, disc loss: 1.1275468426746862, policy loss: 0.7008104403006727
Experience 1, Iter 20, disc loss: 1.1336983295300218, policy loss: 0.6853863126386406
Experience 1, Iter 21, disc loss: 1.1234389564783742, policy loss: 0.6867700099331135
Experience 1, Iter 22, disc loss: 1.0932089066316841, policy loss: 0.7092358190651609
Experience 1, Iter 23, disc loss: 1.1005441927432653, policy loss: 0.6935868708718167
Experience 1, Iter 24, disc loss: 1.0917218587115505, policy loss: 0.693515098640024
Experience 1, Iter 25, disc loss: 1.0642792826687346, policy loss: 0.7168569738660853
Experience 1, Iter 26, disc loss: 1.0464854035256244, policy loss: 0.7272911938038258
Experience 1, Iter 27, disc loss: 1.0479407956360374, policy loss: 0.713416268180069
Experience 1, Iter 28, disc loss: 1.029614691460731, policy loss: 0.7268496407505525
Experience 1, Iter 29, disc loss: 1.0117641979277363, policy loss: 0.7379020079791933
Experience 1, Iter 30, disc loss: 1.0088804721662865, policy loss: 0.7360654137017111
Experience 1, Iter 31, disc loss: 0.9976663815782999, policy loss: 0.7386732866837148
Experience 1, Iter 32, disc loss: 0.9860085112800909, policy loss: 0.7429691399095115
Experience 1, Iter 33, disc loss: 0.9684844385862182, policy loss: 0.7558372503897832
Experience 1, Iter 34, disc loss: 0.9711321152047132, policy loss: 0.7441957244268099
Experience 1, Iter 35, disc loss: 0.9532192215522258, policy loss: 0.7613852311786182
Experience 1, Iter 36, disc loss: 0.9514547385769545, policy loss: 0.7488125142843619
Experience 1, Iter 37, disc loss: 0.9419324347061084, policy loss: 0.7549204968370882
Experience 1, Iter 38, disc loss: 0.9304495706637482, policy loss: 0.7660181067288709
Experience 1, Iter 39, disc loss: 0.9200255108052822, policy loss: 0.7694527923972674
Experience 1, Iter 40, disc loss: 0.8960467703338094, policy loss: 0.7897480175821034
Experience 1, Iter 41, disc loss: 0.877098585976762, policy loss: 0.8071869898360298
Experience 1, Iter 42, disc loss: 0.8882173331271397, policy loss: 0.7836822154172597
Experience 1, Iter 43, disc loss: 0.8759157772682238, policy loss: 0.7988655548523969
Experience 1, Iter 44, disc loss: 0.8409837410511332, policy loss: 0.8436982496896543
Experience 1, Iter 45, disc loss: 0.8313275181314514, policy loss: 0.840929877632674
Experience 1, Iter 46, disc loss: 0.8354874312819831, policy loss: 0.8391947775433053
Experience 1, Iter 47, disc loss: 0.7879257646888568, policy loss: 0.904009579199753
Experience 1, Iter 48, disc loss: 0.7998094236016353, policy loss: 0.8675430641811044
Experience 1, Iter 49, disc loss: 0.806771679465097, policy loss: 0.857772777985236
Experience 1, Iter 50, disc loss: 0.7702267125443013, policy loss: 0.8942391762821218
Experience 1, Iter 51, disc loss: 0.766726114835818, policy loss: 0.8983265078632543
Experience 1, Iter 52, disc loss: 0.7549749372200376, policy loss: 0.8963193733210371
Experience 1, Iter 53, disc loss: 0.7594371501805265, policy loss: 0.8866118719090362
Experience 1, Iter 54, disc loss: 0.7247161843214411, policy loss: 0.9349100098613773
Experience 1, Iter 55, disc loss: 0.7211450065416769, policy loss: 0.9305680394332002
Experience 1, Iter 56, disc loss: 0.7097158272693356, policy loss: 0.9506608310953598
Experience 1, Iter 57, disc loss: 0.6991200142038426, policy loss: 0.9690274113284308
Experience 1, Iter 58, disc loss: 0.6982270961259082, policy loss: 0.9437049360169145
Experience 1, Iter 59, disc loss: 0.64854499193073, policy loss: 1.0363234084909636
Experience 1, Iter 60, disc loss: 0.6896528343934627, policy loss: 0.9576511642655823
Experience 1, Iter 61, disc loss: 0.6413267546856449, policy loss: 1.0242418313327908
Experience 1, Iter 62, disc loss: 0.6358037676793366, policy loss: 1.0217175314268139
Experience 1, Iter 63, disc loss: 0.6272333984424765, policy loss: 1.0538292232356532
Experience 1, Iter 64, disc loss: 0.5935344922138837, policy loss: 1.0860757390997777
Experience 1, Iter 65, disc loss: 0.5889747376419834, policy loss: 1.1134846408413193
Experience 1, Iter 66, disc loss: 0.6071538636175471, policy loss: 1.0761895069296363
Experience 1, Iter 67, disc loss: 0.5991457846363446, policy loss: 1.0593747207984614
Experience 1, Iter 68, disc loss: 0.5297144909815967, policy loss: 1.2080494953506984
Experience 1, Iter 69, disc loss: 0.5463508730646509, policy loss: 1.1725364031703056
Experience 1, Iter 70, disc loss: 0.5470616080736668, policy loss: 1.1525010847709005
Experience 1, Iter 71, disc loss: 0.533545958318623, policy loss: 1.1717293661785786
Experience 1, Iter 72, disc loss: 0.5045859327586534, policy loss: 1.256033769631508
Experience 1, Iter 73, disc loss: 0.5001182290173251, policy loss: 1.2370274102562218
Experience 1, Iter 74, disc loss: 0.493468951204803, policy loss: 1.2507398700652457
Experience 1, Iter 75, disc loss: 0.48394745058210364, policy loss: 1.263151497514937
Experience 1, Iter 76, disc loss: 0.46804913558420397, policy loss: 1.2863432396832208
Experience 1, Iter 77, disc loss: 0.45297977032168707, policy loss: 1.3162402724644156
Experience 1, Iter 78, disc loss: 0.45890506912952567, policy loss: 1.3145579178483442
Experience 1, Iter 79, disc loss: 0.460739044768251, policy loss: 1.2631555391309224
Experience 1, Iter 80, disc loss: 0.4137809578161068, policy loss: 1.4116996451824888
Experience 1, Iter 81, disc loss: 0.4037084046922272, policy loss: 1.454021155908804
Experience 1, Iter 82, disc loss: 0.37109222495135136, policy loss: 1.581092045045179
Experience 1, Iter 83, disc loss: 0.37497770980231, policy loss: 1.4983955972963623
Experience 1, Iter 84, disc loss: 0.3729234543486948, policy loss: 1.5156305273534671
Experience 1, Iter 85, disc loss: 0.36797924120562897, policy loss: 1.5342509465764416
Experience 1, Iter 86, disc loss: 0.3646324379299217, policy loss: 1.5075128775259086
Experience 1, Iter 87, disc loss: 0.3523289603154621, policy loss: 1.5430517947985911
Experience 1, Iter 88, disc loss: 0.326761530845387, policy loss: 1.6588921809160362
Experience 1, Iter 89, disc loss: 0.31630766715976355, policy loss: 1.6892778656127945
Experience 1, Iter 90, disc loss: 0.3071547226784459, policy loss: 1.6935883504014397
Experience 1, Iter 91, disc loss: 0.3071894341316421, policy loss: 1.720643820209501
Experience 1, Iter 92, disc loss: 0.32970057887861387, policy loss: 1.593140071187333
Experience 1, Iter 93, disc loss: 0.281771616339892, policy loss: 1.8100109695914766
Experience 1, Iter 94, disc loss: 0.28791043384257753, policy loss: 1.7666511969218865
Experience 1, Iter 95, disc loss: 0.2753717264365705, policy loss: 1.8686148928686777
Experience 1, Iter 96, disc loss: 0.2791067108035724, policy loss: 1.8310059825725085
Experience 1, Iter 97, disc loss: 0.2680521531079264, policy loss: 1.8194002245576777
Experience 1, Iter 98, disc loss: 0.2487112671975329, policy loss: 1.9202629122018244
Experience 1, Iter 99, disc loss: 0.24069595772963928, policy loss: 2.0123625467812922
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0017],
        [0.1271],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0016, 0.0069, 0.5086, 0.0105], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0016, 0.0069, 0.5086, 0.0105])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.416
Iter 2/2000 - Loss: -2.024
Iter 3/2000 - Loss: -1.611
Iter 4/2000 - Loss: -1.700
Iter 5/2000 - Loss: -1.957
Iter 6/2000 - Loss: -2.112
Iter 7/2000 - Loss: -2.133
Iter 8/2000 - Loss: -2.167
Iter 9/2000 - Loss: -2.229
Iter 10/2000 - Loss: -2.267
Iter 11/2000 - Loss: -2.269
Iter 12/2000 - Loss: -2.254
Iter 13/2000 - Loss: -2.256
Iter 14/2000 - Loss: -2.298
Iter 15/2000 - Loss: -2.372
Iter 16/2000 - Loss: -2.448
Iter 17/2000 - Loss: -2.491
Iter 18/2000 - Loss: -2.483
Iter 19/2000 - Loss: -2.444
Iter 20/2000 - Loss: -2.433
Iter 1981/2000 - Loss: -2.759
Iter 1982/2000 - Loss: -2.759
Iter 1983/2000 - Loss: -2.759
Iter 1984/2000 - Loss: -2.759
Iter 1985/2000 - Loss: -2.759
Iter 1986/2000 - Loss: -2.759
Iter 1987/2000 - Loss: -2.759
Iter 1988/2000 - Loss: -2.759
Iter 1989/2000 - Loss: -2.759
Iter 1990/2000 - Loss: -2.759
Iter 1991/2000 - Loss: -2.759
Iter 1992/2000 - Loss: -2.759
Iter 1993/2000 - Loss: -2.759
Iter 1994/2000 - Loss: -2.759
Iter 1995/2000 - Loss: -2.759
Iter 1996/2000 - Loss: -2.759
Iter 1997/2000 - Loss: -2.759
Iter 1998/2000 - Loss: -2.759
Iter 1999/2000 - Loss: -2.759
Iter 2000/2000 - Loss: -2.759
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0013],
        [0.0891],
        [0.0020]])
Lengthscale: tensor([[[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]],

        [[4.4177e-03, 1.3686e-02, 1.1776e-01, 2.8831e-03, 8.6639e-05,
          7.8564e-03]]])
Signal Variance: tensor([0.0012, 0.0053, 0.3941, 0.0080])
Estimated target variance: tensor([0.0016, 0.0069, 0.5086, 0.0105])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0024, 2.1034, 2.0033])
Bound on condition number: tensor([81.0002, 81.1914, 89.4873, 81.2647])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.28225620380505906, policy loss: 1.6910171290602332
Experience 2, Iter 1, disc loss: 0.27149989311459216, policy loss: 1.7473719289975227
Experience 2, Iter 2, disc loss: 0.2546054995033478, policy loss: 1.808360183606593
Experience 2, Iter 3, disc loss: 0.23425543421563091, policy loss: 1.9022162172816133
Experience 2, Iter 4, disc loss: 0.22916655534765373, policy loss: 1.9261786316211762
Experience 2, Iter 5, disc loss: 0.2443893110632121, policy loss: 1.8205236372362683
Experience 2, Iter 6, disc loss: 0.23579214625529107, policy loss: 1.8440426506828487
Experience 2, Iter 7, disc loss: 0.22483552118270228, policy loss: 1.9303468607134724
Experience 2, Iter 8, disc loss: 0.22219347609864387, policy loss: 1.9852241170643394
Experience 2, Iter 9, disc loss: 0.21731759665008804, policy loss: 1.9926277664832734
Experience 2, Iter 10, disc loss: 0.20538676584445892, policy loss: 2.0170546913273375
Experience 2, Iter 11, disc loss: 0.20220674245180856, policy loss: 2.0317388225410604
Experience 2, Iter 12, disc loss: 0.18759022705281858, policy loss: 2.1082918494675855
Experience 2, Iter 13, disc loss: 0.17978781084683404, policy loss: 2.1365218856705255
Experience 2, Iter 14, disc loss: 0.18092531714630367, policy loss: 2.1486069867107056
Experience 2, Iter 15, disc loss: 0.18332315922808057, policy loss: 2.111381597315602
Experience 2, Iter 16, disc loss: 0.17000008782979484, policy loss: 2.2056082999469604
Experience 2, Iter 17, disc loss: 0.16522104286025543, policy loss: 2.231925050160579
Experience 2, Iter 18, disc loss: 0.16209172582577608, policy loss: 2.245380496222155
Experience 2, Iter 19, disc loss: 0.15524570186137449, policy loss: 2.2989374000413316
Experience 2, Iter 20, disc loss: 0.15618531976542765, policy loss: 2.3211316065584606
Experience 2, Iter 21, disc loss: 0.14360036572831786, policy loss: 2.399082302521917
Experience 2, Iter 22, disc loss: 0.14272594892588686, policy loss: 2.3984890174963303
Experience 2, Iter 23, disc loss: 0.1413391633915467, policy loss: 2.394617280035765
Experience 2, Iter 24, disc loss: 0.1409924702106803, policy loss: 2.4153789315864036
Experience 2, Iter 25, disc loss: 0.14286205048007922, policy loss: 2.352833600306679
Experience 2, Iter 26, disc loss: 0.1367211943821178, policy loss: 2.406720421721543
Experience 2, Iter 27, disc loss: 0.12441175656150288, policy loss: 2.5655750866597975
Experience 2, Iter 28, disc loss: 0.11821682961989159, policy loss: 2.6053069071234676
Experience 2, Iter 29, disc loss: 0.12288567827442391, policy loss: 2.520088486340562
Experience 2, Iter 30, disc loss: 0.11789935871310207, policy loss: 2.589844474558987
Experience 2, Iter 31, disc loss: 0.11726767931327366, policy loss: 2.5873718725892667
Experience 2, Iter 32, disc loss: 0.10623358807578083, policy loss: 2.76784863388219
Experience 2, Iter 33, disc loss: 0.10997003870513052, policy loss: 2.628811157825749
Experience 2, Iter 34, disc loss: 0.10273467553153041, policy loss: 2.6998886647865383
Experience 2, Iter 35, disc loss: 0.1018360723212158, policy loss: 2.7567951678242566
Experience 2, Iter 36, disc loss: 0.10203069441595222, policy loss: 2.692968698460747
Experience 2, Iter 37, disc loss: 0.09924799875317442, policy loss: 2.7205949280753776
Experience 2, Iter 38, disc loss: 0.10269755704722054, policy loss: 2.77465759768356
Experience 2, Iter 39, disc loss: 0.09407125208220007, policy loss: 2.7803283321995655
Experience 2, Iter 40, disc loss: 0.0910649524869461, policy loss: 2.85942391091334
Experience 2, Iter 41, disc loss: 0.09284640231654878, policy loss: 2.8248006819334095
Experience 2, Iter 42, disc loss: 0.09338361475662102, policy loss: 2.8757694256130693
Experience 2, Iter 43, disc loss: 0.08860314329826072, policy loss: 2.8771654797115165
Experience 2, Iter 44, disc loss: 0.07560067031661835, policy loss: 3.068612096749283
Experience 2, Iter 45, disc loss: 0.08061083022097992, policy loss: 2.995484511090317
Experience 2, Iter 46, disc loss: 0.07477828611550769, policy loss: 3.066170008757223
Experience 2, Iter 47, disc loss: 0.08288217649654028, policy loss: 2.94664867485627
Experience 2, Iter 48, disc loss: 0.07553788508258918, policy loss: 3.083695179637102
Experience 2, Iter 49, disc loss: 0.07053930242891132, policy loss: 3.228693929759271
Experience 2, Iter 50, disc loss: 0.06736155494971051, policy loss: 3.2246647497169056
Experience 2, Iter 51, disc loss: 0.07221865532843555, policy loss: 3.188548935753917
Experience 2, Iter 52, disc loss: 0.0660156351520148, policy loss: 3.1955860095443147
Experience 2, Iter 53, disc loss: 0.0669682796488693, policy loss: 3.209229731826663
Experience 2, Iter 54, disc loss: 0.06793004656807924, policy loss: 3.2004518551404084
Experience 2, Iter 55, disc loss: 0.057920576951394076, policy loss: 3.372472829752814
Experience 2, Iter 56, disc loss: 0.0647929643651623, policy loss: 3.2029379017146598
Experience 2, Iter 57, disc loss: 0.05767066848636704, policy loss: 3.333437830979557
Experience 2, Iter 58, disc loss: 0.05869582033258902, policy loss: 3.2813308132286334
Experience 2, Iter 59, disc loss: 0.06153065032410238, policy loss: 3.328688702830785
Experience 2, Iter 60, disc loss: 0.054252501255285156, policy loss: 3.4003274168274844
Experience 2, Iter 61, disc loss: 0.054835149898692676, policy loss: 3.463044831509539
Experience 2, Iter 62, disc loss: 0.054890052819789634, policy loss: 3.384724830847455
Experience 2, Iter 63, disc loss: 0.05300768405381068, policy loss: 3.437158711708374
Experience 2, Iter 64, disc loss: 0.05213364043332608, policy loss: 3.555403528423599
Experience 2, Iter 65, disc loss: 0.05422538185145083, policy loss: 3.4826754733109553
Experience 2, Iter 66, disc loss: 0.04948175841042637, policy loss: 3.487421142673065
Experience 2, Iter 67, disc loss: 0.05450434793057755, policy loss: 3.4448895852479025
Experience 2, Iter 68, disc loss: 0.043240658688993336, policy loss: 3.8145818184732523
Experience 2, Iter 69, disc loss: 0.047559535951552635, policy loss: 3.597803703109488
Experience 2, Iter 70, disc loss: 0.045280780916413924, policy loss: 3.705225894552817
Experience 2, Iter 71, disc loss: 0.047743099900144054, policy loss: 3.66810078832315
Experience 2, Iter 72, disc loss: 0.0426569426592301, policy loss: 3.7035369775347284
Experience 2, Iter 73, disc loss: 0.04304619448610401, policy loss: 3.6655262389219962
Experience 2, Iter 74, disc loss: 0.042281170204875894, policy loss: 3.7598865908804924
Experience 2, Iter 75, disc loss: 0.041993095645483494, policy loss: 3.715611920651635
Experience 2, Iter 76, disc loss: 0.040953322584265275, policy loss: 3.8091058933176147
Experience 2, Iter 77, disc loss: 0.041458617554971655, policy loss: 3.7282620075441377
Experience 2, Iter 78, disc loss: 0.036018670359644775, policy loss: 3.931672321272785
Experience 2, Iter 79, disc loss: 0.03767025528787933, policy loss: 3.8654985168426044
Experience 2, Iter 80, disc loss: 0.03736077062120536, policy loss: 3.8869878763874937
Experience 2, Iter 81, disc loss: 0.03506669087911094, policy loss: 3.978132708533253
Experience 2, Iter 82, disc loss: 0.03648857692050319, policy loss: 3.8537590011104625
Experience 2, Iter 83, disc loss: 0.037127331992474734, policy loss: 3.840889600372554
Experience 2, Iter 84, disc loss: 0.03688100181599803, policy loss: 3.8637068503034757
Experience 2, Iter 85, disc loss: 0.03763600664968558, policy loss: 3.8996864251464856
Experience 2, Iter 86, disc loss: 0.03152083225688386, policy loss: 4.086891029450799
Experience 2, Iter 87, disc loss: 0.03397495919832045, policy loss: 3.9203521974752977
Experience 2, Iter 88, disc loss: 0.03143895848410998, policy loss: 4.117193447553989
Experience 2, Iter 89, disc loss: 0.03339955827526497, policy loss: 4.0394955745746906
Experience 2, Iter 90, disc loss: 0.030563401463270995, policy loss: 4.176705026699437
Experience 2, Iter 91, disc loss: 0.030550236950544936, policy loss: 4.139158274719104
Experience 2, Iter 92, disc loss: 0.03044219318483495, policy loss: 4.113398120059307
Experience 2, Iter 93, disc loss: 0.03670451456135314, policy loss: 3.9740012801610307
Experience 2, Iter 94, disc loss: 0.031856640619856574, policy loss: 4.036950597339886
Experience 2, Iter 95, disc loss: 0.028930674123710136, policy loss: 4.119917390473036
Experience 2, Iter 96, disc loss: 0.030175200260094265, policy loss: 4.024454914335118
Experience 2, Iter 97, disc loss: 0.03118736227807279, policy loss: 4.116273274719887
Experience 2, Iter 98, disc loss: 0.029724078868629607, policy loss: 4.114496592037539
Experience 2, Iter 99, disc loss: 0.03516534473114932, policy loss: 3.9060257837688845
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0019],
        [0.0898],
        [0.0018]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0020, 0.0075, 0.3593, 0.0074], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0020, 0.0075, 0.3593, 0.0074])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.823
Iter 2/2000 - Loss: -1.856
Iter 3/2000 - Loss: -2.179
Iter 4/2000 - Loss: -2.375
Iter 5/2000 - Loss: -2.290
Iter 6/2000 - Loss: -2.296
Iter 7/2000 - Loss: -2.455
Iter 8/2000 - Loss: -2.584
Iter 9/2000 - Loss: -2.570
Iter 10/2000 - Loss: -2.477
Iter 11/2000 - Loss: -2.452
Iter 12/2000 - Loss: -2.559
Iter 13/2000 - Loss: -2.709
Iter 14/2000 - Loss: -2.768
Iter 15/2000 - Loss: -2.706
Iter 16/2000 - Loss: -2.640
Iter 17/2000 - Loss: -2.690
Iter 18/2000 - Loss: -2.815
Iter 19/2000 - Loss: -2.870
Iter 20/2000 - Loss: -2.815
Iter 1981/2000 - Loss: -2.919
Iter 1982/2000 - Loss: -2.920
Iter 1983/2000 - Loss: -2.919
Iter 1984/2000 - Loss: -2.919
Iter 1985/2000 - Loss: -2.920
Iter 1986/2000 - Loss: -2.919
Iter 1987/2000 - Loss: -2.919
Iter 1988/2000 - Loss: -2.920
Iter 1989/2000 - Loss: -2.920
Iter 1990/2000 - Loss: -2.919
Iter 1991/2000 - Loss: -2.920
Iter 1992/2000 - Loss: -2.920
Iter 1993/2000 - Loss: -2.920
Iter 1994/2000 - Loss: -2.920
Iter 1995/2000 - Loss: -2.920
Iter 1996/2000 - Loss: -2.920
Iter 1997/2000 - Loss: -2.920
Iter 1998/2000 - Loss: -2.920
Iter 1999/2000 - Loss: -2.920
Iter 2000/2000 - Loss: -2.920
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0015],
        [0.0657],
        [0.0014]])
Lengthscale: tensor([[[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]],

        [[2.9594e-03, 1.7525e-02, 8.2843e-02, 2.2558e-03, 6.5303e-05,
          3.5593e-02]]])
Signal Variance: tensor([0.0015, 0.0058, 0.2816, 0.0057])
Estimated target variance: tensor([0.0020, 0.0075, 0.3593, 0.0074])
N: 30
Signal to noise ratio: tensor([2.0010, 2.0021, 2.0706, 2.0018])
Bound on condition number: tensor([121.1170, 121.2518, 129.6260, 121.2170])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.048123377887443045, policy loss: 3.536753543728011
Experience 3, Iter 1, disc loss: 0.056411004800113046, policy loss: 3.4005405477908788
Experience 3, Iter 2, disc loss: 0.04395814165999873, policy loss: 3.562096471535882
Experience 3, Iter 3, disc loss: 0.05281900983173858, policy loss: 3.4104859029076184
Experience 3, Iter 4, disc loss: 0.04913187386643019, policy loss: 3.5386208387610116
Experience 3, Iter 5, disc loss: 0.053627600782315175, policy loss: 3.301865403359238
Experience 3, Iter 6, disc loss: 0.05207710682625034, policy loss: 3.479925547643005
Experience 3, Iter 7, disc loss: 0.04784354666697932, policy loss: 3.524254683054842
Experience 3, Iter 8, disc loss: 0.04879755548652064, policy loss: 3.572913881867427
Experience 3, Iter 9, disc loss: 0.04466052260157664, policy loss: 3.661751323071549
Experience 3, Iter 10, disc loss: 0.04488869842863849, policy loss: 3.5047744022055136
Experience 3, Iter 11, disc loss: 0.04380513807499273, policy loss: 3.611689066466141
Experience 3, Iter 12, disc loss: 0.03900348190269282, policy loss: 3.766600354436829
Experience 3, Iter 13, disc loss: 0.046992196926155104, policy loss: 3.6129394301234155
Experience 3, Iter 14, disc loss: 0.03770177688873527, policy loss: 3.817034825436518
Experience 3, Iter 15, disc loss: 0.04125849895934364, policy loss: 3.7857636933541174
Experience 3, Iter 16, disc loss: 0.042284379687478835, policy loss: 3.584327692463256
Experience 3, Iter 17, disc loss: 0.03758649339363529, policy loss: 3.745399028081928
Experience 3, Iter 18, disc loss: 0.041541412242757036, policy loss: 3.6867701455574924
Experience 3, Iter 19, disc loss: 0.036452145630673526, policy loss: 3.740066243891209
Experience 3, Iter 20, disc loss: 0.03904973011335443, policy loss: 3.816285926778778
Experience 3, Iter 21, disc loss: 0.03528446581580735, policy loss: 3.859577990336471
Experience 3, Iter 22, disc loss: 0.037598992879489264, policy loss: 3.7366601636991703
Experience 3, Iter 23, disc loss: 0.037552303400176214, policy loss: 3.822828263886139
Experience 3, Iter 24, disc loss: 0.032012392558196906, policy loss: 4.00429112020158
Experience 3, Iter 25, disc loss: 0.03414395791636208, policy loss: 4.027014459710238
Experience 3, Iter 26, disc loss: 0.042567042674901814, policy loss: 3.717315609562946
Experience 3, Iter 27, disc loss: 0.030847726409209117, policy loss: 4.049882830383241
Experience 3, Iter 28, disc loss: 0.034625357301411384, policy loss: 3.995480413976876
Experience 3, Iter 29, disc loss: 0.031694921093006534, policy loss: 3.9210362067698834
Experience 3, Iter 30, disc loss: 0.03720858090038643, policy loss: 3.923919332767994
Experience 3, Iter 31, disc loss: 0.031841929942612424, policy loss: 4.0239320972629065
Experience 3, Iter 32, disc loss: 0.028390625084716972, policy loss: 4.14720713028633
Experience 3, Iter 33, disc loss: 0.02962215811428688, policy loss: 4.062858645312199
Experience 3, Iter 34, disc loss: 0.032247476434611955, policy loss: 3.9823062398400397
Experience 3, Iter 35, disc loss: 0.028106037114531145, policy loss: 4.070386822075257
Experience 3, Iter 36, disc loss: 0.028120228189899232, policy loss: 4.102676270510524
Experience 3, Iter 37, disc loss: 0.030040582886259257, policy loss: 4.065534574585718
Experience 3, Iter 38, disc loss: 0.026959112615409328, policy loss: 4.153525070678986
Experience 3, Iter 39, disc loss: 0.02851360115649143, policy loss: 4.2186428353046574
Experience 3, Iter 40, disc loss: 0.02881033069163527, policy loss: 4.184666079232534
Experience 3, Iter 41, disc loss: 0.029234172603881732, policy loss: 4.123072486760477
Experience 3, Iter 42, disc loss: 0.034997788209321846, policy loss: 3.981193457781858
Experience 3, Iter 43, disc loss: 0.034476940520564596, policy loss: 4.017215337156676
Experience 3, Iter 44, disc loss: 0.02404499503668116, policy loss: 4.368199110097821
Experience 3, Iter 45, disc loss: 0.02809707706469556, policy loss: 4.12898537936378
Experience 3, Iter 46, disc loss: 0.02905671918009606, policy loss: 4.177505413529129
Experience 3, Iter 47, disc loss: 0.02372296361726379, policy loss: 4.407980306965922
Experience 3, Iter 48, disc loss: 0.023176844104631014, policy loss: 4.424385411220543
Experience 3, Iter 49, disc loss: 0.024496662294400548, policy loss: 4.391640271988692
Experience 3, Iter 50, disc loss: 0.022054420077908966, policy loss: 4.521349557565776
Experience 3, Iter 51, disc loss: 0.023858282405232193, policy loss: 4.496044377982577
Experience 3, Iter 52, disc loss: 0.021476898340201125, policy loss: 4.497367144062349
Experience 3, Iter 53, disc loss: 0.023013592784064778, policy loss: 4.51253628497324
Experience 3, Iter 54, disc loss: 0.024141947792105177, policy loss: 4.381621637866797
Experience 3, Iter 55, disc loss: 0.02488218607401492, policy loss: 4.307647429088954
Experience 3, Iter 56, disc loss: 0.02137780826470243, policy loss: 4.551162261253693
Experience 3, Iter 57, disc loss: 0.023314974586416556, policy loss: 4.455440131491294
Experience 3, Iter 58, disc loss: 0.021961112122050805, policy loss: 4.483743465591091
Experience 3, Iter 59, disc loss: 0.02281478502432448, policy loss: 4.405706620247016
Experience 3, Iter 60, disc loss: 0.02551139624559203, policy loss: 4.368270822802094
Experience 3, Iter 61, disc loss: 0.019956778596630066, policy loss: 4.584588609817202
Experience 3, Iter 62, disc loss: 0.02729572727592543, policy loss: 4.502127653665845
Experience 3, Iter 63, disc loss: 0.017356014712723446, policy loss: 4.7673039769364305
Experience 3, Iter 64, disc loss: 0.022163068477407816, policy loss: 4.570345476936087
Experience 3, Iter 65, disc loss: 0.026295078473477226, policy loss: 4.381164947005906
Experience 3, Iter 66, disc loss: 0.019092264143433225, policy loss: 4.636652874245373
Experience 3, Iter 67, disc loss: 0.020060295971332492, policy loss: 4.680207308028237
Experience 3, Iter 68, disc loss: 0.02104467617188852, policy loss: 4.547718721549303
Experience 3, Iter 69, disc loss: 0.01978705634748478, policy loss: 4.539075699557283
Experience 3, Iter 70, disc loss: 0.018842989399334455, policy loss: 4.64520395347934
Experience 3, Iter 71, disc loss: 0.017255329321733976, policy loss: 4.757977109813577
Experience 3, Iter 72, disc loss: 0.016899594558813822, policy loss: 4.778938742351814
Experience 3, Iter 73, disc loss: 0.016618155668957565, policy loss: 4.9126973334335915
Experience 3, Iter 74, disc loss: 0.01937073761584751, policy loss: 4.714569519043495
Experience 3, Iter 75, disc loss: 0.017719739214315886, policy loss: 4.851444789928409
Experience 3, Iter 76, disc loss: 0.014857704765725304, policy loss: 4.998636604854365
Experience 3, Iter 77, disc loss: 0.018305864197848052, policy loss: 4.709719624153985
Experience 3, Iter 78, disc loss: 0.021495315154095654, policy loss: 4.502305090361466
Experience 3, Iter 79, disc loss: 0.013641477127220972, policy loss: 5.014737889687048
Experience 3, Iter 80, disc loss: 0.01877888736458328, policy loss: 4.618098610029696
Experience 3, Iter 81, disc loss: 0.020086515392529876, policy loss: 4.564819246947705
Experience 3, Iter 82, disc loss: 0.01708158014819125, policy loss: 4.866975249568892
Experience 3, Iter 83, disc loss: 0.015813222404599068, policy loss: 4.8099342465016255
Experience 3, Iter 84, disc loss: 0.017086792173008146, policy loss: 4.81749266421167
Experience 3, Iter 85, disc loss: 0.016753543317284594, policy loss: 4.852351848783972
Experience 3, Iter 86, disc loss: 0.014587038114388488, policy loss: 4.8932788118339605
Experience 3, Iter 87, disc loss: 0.015588882626154429, policy loss: 5.02923699506635
Experience 3, Iter 88, disc loss: 0.014850453258319858, policy loss: 4.896489952041997
Experience 3, Iter 89, disc loss: 0.015998887013860826, policy loss: 4.864038466923581
Experience 3, Iter 90, disc loss: 0.014336265326405643, policy loss: 5.001748653891248
Experience 3, Iter 91, disc loss: 0.01755056004710792, policy loss: 5.054782093164949
Experience 3, Iter 92, disc loss: 0.012442404341918366, policy loss: 5.069138180144907
Experience 3, Iter 93, disc loss: 0.01713074198966964, policy loss: 5.130779099285261
Experience 3, Iter 94, disc loss: 0.017902862902295015, policy loss: 4.853553750611367
Experience 3, Iter 95, disc loss: 0.016091795290862427, policy loss: 4.996281314432894
Experience 3, Iter 96, disc loss: 0.019014967986455245, policy loss: 4.98371007416894
Experience 3, Iter 97, disc loss: 0.014759231844531167, policy loss: 5.129734089973918
Experience 3, Iter 98, disc loss: 0.014847207526340504, policy loss: 4.968384864842303
Experience 3, Iter 99, disc loss: 0.01388234542274905, policy loss: 4.907570994954427
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0018],
        [0.0835],
        [0.0017]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0017, 0.0071, 0.3342, 0.0066], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0017, 0.0071, 0.3342, 0.0066])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.161
Iter 2/2000 - Loss: -1.772
Iter 3/2000 - Loss: -2.389
Iter 4/2000 - Loss: -2.628
Iter 5/2000 - Loss: -2.414
Iter 6/2000 - Loss: -2.418
Iter 7/2000 - Loss: -2.638
Iter 8/2000 - Loss: -2.773
Iter 9/2000 - Loss: -2.727
Iter 10/2000 - Loss: -2.625
Iter 11/2000 - Loss: -2.629
Iter 12/2000 - Loss: -2.754
Iter 13/2000 - Loss: -2.873
Iter 14/2000 - Loss: -2.894
Iter 15/2000 - Loss: -2.842
Iter 16/2000 - Loss: -2.820
Iter 17/2000 - Loss: -2.883
Iter 18/2000 - Loss: -2.966
Iter 19/2000 - Loss: -2.988
Iter 20/2000 - Loss: -2.961
Iter 1981/2000 - Loss: -3.085
Iter 1982/2000 - Loss: -3.085
Iter 1983/2000 - Loss: -3.088
Iter 1984/2000 - Loss: -3.088
Iter 1985/2000 - Loss: -3.086
Iter 1986/2000 - Loss: -3.088
Iter 1987/2000 - Loss: -3.089
Iter 1988/2000 - Loss: -3.087
Iter 1989/2000 - Loss: -3.087
Iter 1990/2000 - Loss: -3.089
Iter 1991/2000 - Loss: -3.089
Iter 1992/2000 - Loss: -3.088
Iter 1993/2000 - Loss: -3.089
Iter 1994/2000 - Loss: -3.089
Iter 1995/2000 - Loss: -3.088
Iter 1996/2000 - Loss: -3.088
Iter 1997/2000 - Loss: -3.089
Iter 1998/2000 - Loss: -3.089
Iter 1999/2000 - Loss: -3.088
Iter 2000/2000 - Loss: -3.089
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0014],
        [0.0619],
        [0.0013]])
Lengthscale: tensor([[[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]],

        [[2.5052e-03, 1.4241e-02, 7.3532e-02, 2.1104e-03, 5.0847e-05,
          3.7312e-02]]])
Signal Variance: tensor([0.0013, 0.0056, 0.2639, 0.0052])
Estimated target variance: tensor([0.0017, 0.0071, 0.3342, 0.0066])
N: 40
Signal to noise ratio: tensor([2.0003, 2.0027, 2.0649, 2.0018])
Bound on condition number: tensor([161.0501, 161.4316, 171.5568, 161.2887])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.0224790701307779, policy loss: 4.413541397082231
Experience 4, Iter 1, disc loss: 0.018980058134815507, policy loss: 4.70831786497947
Experience 4, Iter 2, disc loss: 0.01432181978622658, policy loss: 4.944243651308847
Experience 4, Iter 3, disc loss: 0.020034964191815514, policy loss: 4.542455579752386
Experience 4, Iter 4, disc loss: 0.01636038263510986, policy loss: 4.918939079187161
Experience 4, Iter 5, disc loss: 0.018422564900973437, policy loss: 4.604357711799616
Experience 4, Iter 6, disc loss: 0.013886984544929523, policy loss: 4.963049078630651
Experience 4, Iter 7, disc loss: 0.018253153481573216, policy loss: 4.624518209497245
Experience 4, Iter 8, disc loss: 0.016642010238020238, policy loss: 4.804630489814185
Experience 4, Iter 9, disc loss: 0.016313546953647003, policy loss: 4.779296106050529
Experience 4, Iter 10, disc loss: 0.01722638257020176, policy loss: 4.696428972944398
Experience 4, Iter 11, disc loss: 0.013459523281849569, policy loss: 5.0701741365937565
Experience 4, Iter 12, disc loss: 0.01911523651296267, policy loss: 4.565525791111354
Experience 4, Iter 13, disc loss: 0.013823102640160043, policy loss: 4.914638832639227
Experience 4, Iter 14, disc loss: 0.018332770118749266, policy loss: 5.0005439449115965
Experience 4, Iter 15, disc loss: 0.017388174347482705, policy loss: 4.782612782956646
Experience 4, Iter 16, disc loss: 0.018046810324807126, policy loss: 4.710396129260015
Experience 4, Iter 17, disc loss: 0.013618585182592697, policy loss: 4.93137923631747
Experience 4, Iter 18, disc loss: 0.018519720272829058, policy loss: 4.91913855633308
Experience 4, Iter 19, disc loss: 0.01576665205800266, policy loss: 5.043937132747615
Experience 4, Iter 20, disc loss: 0.017093828787995004, policy loss: 4.881099326147681
Experience 4, Iter 21, disc loss: 0.012451755240010314, policy loss: 5.053618237027996
Experience 4, Iter 22, disc loss: 0.014995500120076647, policy loss: 4.96563046679427
Experience 4, Iter 23, disc loss: 0.014363844109094746, policy loss: 4.998650783138892
Experience 4, Iter 24, disc loss: 0.011601501078387757, policy loss: 5.151876474112049
Experience 4, Iter 25, disc loss: 0.014153387326436263, policy loss: 4.904278056730108
Experience 4, Iter 26, disc loss: 0.011651568095120666, policy loss: 5.2645051678252255
Experience 4, Iter 27, disc loss: 0.01535630874142159, policy loss: 5.076140505071844
Experience 4, Iter 28, disc loss: 0.01534769755526859, policy loss: 5.110053117808064
Experience 4, Iter 29, disc loss: 0.014975783461340784, policy loss: 4.838899667102464
Experience 4, Iter 30, disc loss: 0.011657634319174576, policy loss: 5.287065742497183
Experience 4, Iter 31, disc loss: 0.017375971831994488, policy loss: 4.820929042766261
Experience 4, Iter 32, disc loss: 0.0127517252176623, policy loss: 5.058183107040215
Experience 4, Iter 33, disc loss: 0.015563147256936364, policy loss: 4.933578086616775
Experience 4, Iter 34, disc loss: 0.013479566373507852, policy loss: 5.13251468435863
Experience 4, Iter 35, disc loss: 0.015197297043151124, policy loss: 5.020902861054903
Experience 4, Iter 36, disc loss: 0.013547140904737713, policy loss: 5.035366580505658
Experience 4, Iter 37, disc loss: 0.011185213826372248, policy loss: 5.218441547259879
Experience 4, Iter 38, disc loss: 0.010450763916331512, policy loss: 5.43437197620849
Experience 4, Iter 39, disc loss: 0.01574223425515415, policy loss: 5.031176694100703
Experience 4, Iter 40, disc loss: 0.014178317381487069, policy loss: 5.120082059371353
Experience 4, Iter 41, disc loss: 0.011383670817395497, policy loss: 5.166270786294586
Experience 4, Iter 42, disc loss: 0.013639939033160022, policy loss: 5.081467795147315
Experience 4, Iter 43, disc loss: 0.014366482185414352, policy loss: 5.126647549678497
Experience 4, Iter 44, disc loss: 0.012440538139694923, policy loss: 5.194695406656129
Experience 4, Iter 45, disc loss: 0.01041165072080489, policy loss: 5.438323782454967
Experience 4, Iter 46, disc loss: 0.013376419141851701, policy loss: 5.183933182832387
Experience 4, Iter 47, disc loss: 0.011227583180421326, policy loss: 5.259820786936408
Experience 4, Iter 48, disc loss: 0.01172885787684635, policy loss: 5.295207265893589
Experience 4, Iter 49, disc loss: 0.011388525345660375, policy loss: 5.320834310793505
Experience 4, Iter 50, disc loss: 0.010007375352530382, policy loss: 5.424913811880446
Experience 4, Iter 51, disc loss: 0.009823485672454123, policy loss: 5.490623557556557
Experience 4, Iter 52, disc loss: 0.013201067676429519, policy loss: 5.2784071536305746
Experience 4, Iter 53, disc loss: 0.010581486273147114, policy loss: 5.417511486127761
Experience 4, Iter 54, disc loss: 0.015245221413065507, policy loss: 5.214980169271558
Experience 4, Iter 55, disc loss: 0.011112594971090212, policy loss: 5.2908968141854
Experience 4, Iter 56, disc loss: 0.01130619383260651, policy loss: 5.489078719413199
Experience 4, Iter 57, disc loss: 0.007829677041937507, policy loss: 5.586921784791588
Experience 4, Iter 58, disc loss: 0.009980057877961126, policy loss: 5.380786718633711
Experience 4, Iter 59, disc loss: 0.010681547355011346, policy loss: 5.370062662676023
Experience 4, Iter 60, disc loss: 0.011120263587016744, policy loss: 5.300658843576885
Experience 4, Iter 61, disc loss: 0.009630649444826644, policy loss: 5.415409319477666
Experience 4, Iter 62, disc loss: 0.011582843086094866, policy loss: 5.378882273998989
Experience 4, Iter 63, disc loss: 0.009780905823571759, policy loss: 5.357127230963976
Experience 4, Iter 64, disc loss: 0.010344963143578325, policy loss: 5.340288726528887
Experience 4, Iter 65, disc loss: 0.010571166840567244, policy loss: 5.338999068491436
Experience 4, Iter 66, disc loss: 0.012259467773661489, policy loss: 5.347737180604515
Experience 4, Iter 67, disc loss: 0.00916026125001886, policy loss: 5.647862901361525
Experience 4, Iter 68, disc loss: 0.011779054831077738, policy loss: 5.221822741134821
Experience 4, Iter 69, disc loss: 0.011896709502938394, policy loss: 5.354304367326411
Experience 4, Iter 70, disc loss: 0.008265354903282705, policy loss: 5.674581301389984
Experience 4, Iter 71, disc loss: 0.011113081115790933, policy loss: 5.512366463917843
Experience 4, Iter 72, disc loss: 0.012108137069646687, policy loss: 5.414632770000605
Experience 4, Iter 73, disc loss: 0.01043346645078081, policy loss: 5.342626692688396
Experience 4, Iter 74, disc loss: 0.011386806912475714, policy loss: 5.463907681246354
Experience 4, Iter 75, disc loss: 0.009396221683114703, policy loss: 5.727589573569466
Experience 4, Iter 76, disc loss: 0.010871713668416428, policy loss: 5.664321881069865
Experience 4, Iter 77, disc loss: 0.00894471544174804, policy loss: 5.632845723198079
Experience 4, Iter 78, disc loss: 0.009956672784071918, policy loss: 5.511657635519192
Experience 4, Iter 79, disc loss: 0.008873246764357362, policy loss: 5.795309006636322
Experience 4, Iter 80, disc loss: 0.009152899031267744, policy loss: 5.554060554617617
Experience 4, Iter 81, disc loss: 0.008571462418402506, policy loss: 5.4782559454062945
Experience 4, Iter 82, disc loss: 0.006668750615263164, policy loss: 5.81414373478495
Experience 4, Iter 83, disc loss: 0.009606171646715934, policy loss: 5.523065181478645
Experience 4, Iter 84, disc loss: 0.009287483639589584, policy loss: 5.50467754060943
Experience 4, Iter 85, disc loss: 0.007331101903340909, policy loss: 5.919783623297779
Experience 4, Iter 86, disc loss: 0.010194958592700398, policy loss: 5.642763995917121
Experience 4, Iter 87, disc loss: 0.008192890209688018, policy loss: 5.740716735303376
Experience 4, Iter 88, disc loss: 0.008524840287897217, policy loss: 5.680791962552814
Experience 4, Iter 89, disc loss: 0.008957372848475087, policy loss: 5.814099763704862
Experience 4, Iter 90, disc loss: 0.007664329134280203, policy loss: 5.608530430966959
Experience 4, Iter 91, disc loss: 0.009545565497498083, policy loss: 5.840041446485167
Experience 4, Iter 92, disc loss: 0.006936355741207512, policy loss: 5.793961299942488
Experience 4, Iter 93, disc loss: 0.008726005159656403, policy loss: 5.53300495372345
Experience 4, Iter 94, disc loss: 0.007078572938792511, policy loss: 5.854506708862935
Experience 4, Iter 95, disc loss: 0.008370011460786777, policy loss: 5.722027555802892
Experience 4, Iter 96, disc loss: 0.009594658135357273, policy loss: 5.538035012298641
Experience 4, Iter 97, disc loss: 0.008689538752008546, policy loss: 5.797755083724182
Experience 4, Iter 98, disc loss: 0.009302182924807391, policy loss: 5.709391341387247
Experience 4, Iter 99, disc loss: 0.006218483811854841, policy loss: 5.903803207386241
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0038],
        [0.0743],
        [0.0015]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0153, 0.2973, 0.0060], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0153, 0.2973, 0.0060])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.187
Iter 2/2000 - Loss: -0.843
Iter 3/2000 - Loss: -2.173
Iter 4/2000 - Loss: -2.006
Iter 5/2000 - Loss: -1.521
Iter 6/2000 - Loss: -1.713
Iter 7/2000 - Loss: -2.094
Iter 8/2000 - Loss: -2.211
Iter 9/2000 - Loss: -2.076
Iter 10/2000 - Loss: -1.927
Iter 11/2000 - Loss: -1.917
Iter 12/2000 - Loss: -2.024
Iter 13/2000 - Loss: -2.145
Iter 14/2000 - Loss: -2.201
Iter 15/2000 - Loss: -2.170
Iter 16/2000 - Loss: -2.102
Iter 17/2000 - Loss: -2.084
Iter 18/2000 - Loss: -2.146
Iter 19/2000 - Loss: -2.236
Iter 20/2000 - Loss: -2.270
Iter 1981/2000 - Loss: -2.310
Iter 1982/2000 - Loss: -2.310
Iter 1983/2000 - Loss: -2.310
Iter 1984/2000 - Loss: -2.310
Iter 1985/2000 - Loss: -2.310
Iter 1986/2000 - Loss: -2.310
Iter 1987/2000 - Loss: -2.310
Iter 1988/2000 - Loss: -2.310
Iter 1989/2000 - Loss: -2.310
Iter 1990/2000 - Loss: -2.310
Iter 1991/2000 - Loss: -2.310
Iter 1992/2000 - Loss: -2.310
Iter 1993/2000 - Loss: -2.310
Iter 1994/2000 - Loss: -2.310
Iter 1995/2000 - Loss: -2.310
Iter 1996/2000 - Loss: -2.310
Iter 1997/2000 - Loss: -2.310
Iter 1998/2000 - Loss: -2.310
Iter 1999/2000 - Loss: -2.310
Iter 2000/2000 - Loss: -2.310
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0030],
        [0.0557],
        [0.0012]])
Lengthscale: tensor([[[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]],

        [[7.8003e-03, 4.2066e-02, 6.5033e-02, 2.7287e-03, 4.3890e-05,
          1.3770e-01]]])
Signal Variance: tensor([0.0036, 0.0121, 0.2356, 0.0047])
Estimated target variance: tensor([0.0046, 0.0153, 0.2973, 0.0060])
N: 50
Signal to noise ratio: tensor([2.0024, 2.0107, 2.0565, 2.0018])
Bound on condition number: tensor([201.4807, 203.1411, 212.4564, 201.3637])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.04612623164913782, policy loss: 4.218322101056151
Experience 5, Iter 1, disc loss: 0.045560135572869996, policy loss: 4.3107235940129165
Experience 5, Iter 2, disc loss: 0.0401929067550238, policy loss: 4.572818986590697
Experience 5, Iter 3, disc loss: 0.04585779305568335, policy loss: 4.091282411177864
Experience 5, Iter 4, disc loss: 0.036150574275753064, policy loss: 4.449821852606557
Experience 5, Iter 5, disc loss: 0.03957452349500668, policy loss: 4.068756738109612
Experience 5, Iter 6, disc loss: 0.0343220873387679, policy loss: 4.400505817395841
Experience 5, Iter 7, disc loss: 0.031176254087767395, policy loss: 4.499447651313231
Experience 5, Iter 8, disc loss: 0.02835691162975351, policy loss: 4.423758066824225
Experience 5, Iter 9, disc loss: 0.02878347359207868, policy loss: 4.269180846705554
Experience 5, Iter 10, disc loss: 0.04216077862019473, policy loss: 4.080940729473724
Experience 5, Iter 11, disc loss: 0.034868101525995426, policy loss: 4.462206681005813
Experience 5, Iter 12, disc loss: 0.029346227437494245, policy loss: 4.500021802998663
Experience 5, Iter 13, disc loss: 0.027607015111538763, policy loss: 4.53619280007166
Experience 5, Iter 14, disc loss: 0.02900246096362533, policy loss: 4.532429893001783
Experience 5, Iter 15, disc loss: 0.0251734573506632, policy loss: 5.00381174294837
Experience 5, Iter 16, disc loss: 0.022727333931617216, policy loss: 4.927230200011192
Experience 5, Iter 17, disc loss: 0.028060572160426622, policy loss: 4.464901346807103
Experience 5, Iter 18, disc loss: 0.029293022606829453, policy loss: 4.686433648029015
Experience 5, Iter 19, disc loss: 0.030945619739854966, policy loss: 4.569643154918646
Experience 5, Iter 20, disc loss: 0.024296528980799176, policy loss: 4.9070367701482835
Experience 5, Iter 21, disc loss: 0.024730267166590502, policy loss: 4.694362243215982
Experience 5, Iter 22, disc loss: 0.04000105223045198, policy loss: 4.614355433386107
Experience 5, Iter 23, disc loss: 0.02335845122968764, policy loss: 4.875303956898147
Experience 5, Iter 24, disc loss: 0.024786480095016116, policy loss: 4.652124694398666
Experience 5, Iter 25, disc loss: 0.027157234024597337, policy loss: 4.760757628794451
Experience 5, Iter 26, disc loss: 0.027319727642634434, policy loss: 4.915196190433797
Experience 5, Iter 27, disc loss: 0.021353041198572714, policy loss: 4.950105996032948
Experience 5, Iter 28, disc loss: 0.02567959116815979, policy loss: 4.893576864615184
Experience 5, Iter 29, disc loss: 0.027273151044695364, policy loss: 4.695548029826373
Experience 5, Iter 30, disc loss: 0.03202582844918543, policy loss: 4.7402523172910955
Experience 5, Iter 31, disc loss: 0.02248845824565111, policy loss: 5.05105511504979
Experience 5, Iter 32, disc loss: 0.019122421165795555, policy loss: 4.994220879902933
Experience 5, Iter 33, disc loss: 0.02407079071513521, policy loss: 5.160289387973803
Experience 5, Iter 34, disc loss: 0.02334995267835157, policy loss: 4.851182546767275
Experience 5, Iter 35, disc loss: 0.021269913426690924, policy loss: 5.16265920438548
Experience 5, Iter 36, disc loss: 0.019465659872322464, policy loss: 4.993777117119599
Experience 5, Iter 37, disc loss: 0.02048899093657194, policy loss: 5.209966320446721
Experience 5, Iter 38, disc loss: 0.025377680177319836, policy loss: 5.197065589894963
Experience 5, Iter 39, disc loss: 0.0225783280460454, policy loss: 4.972126089465249
Experience 5, Iter 40, disc loss: 0.027239146827982282, policy loss: 4.934686740283295
Experience 5, Iter 41, disc loss: 0.021542571753080518, policy loss: 5.173222835550995
Experience 5, Iter 42, disc loss: 0.017614084780574774, policy loss: 5.189697589523172
Experience 5, Iter 43, disc loss: 0.030920875976075096, policy loss: 4.650125096354832
Experience 5, Iter 44, disc loss: 0.03144591734226631, policy loss: 4.825043421703068
Experience 5, Iter 45, disc loss: 0.024982338605487146, policy loss: 4.924930717038267
Experience 5, Iter 46, disc loss: 0.021212935804241034, policy loss: 5.320100915678745
Experience 5, Iter 47, disc loss: 0.02562584761526548, policy loss: 5.254435006812066
Experience 5, Iter 48, disc loss: 0.017017294418044553, policy loss: 5.248203165702067
Experience 5, Iter 49, disc loss: 0.017128978885022583, policy loss: 5.364292709179232
Experience 5, Iter 50, disc loss: 0.023888623729182032, policy loss: 5.102807563681679
Experience 5, Iter 51, disc loss: 0.021670331588763497, policy loss: 5.530224684577698
Experience 5, Iter 52, disc loss: 0.019409285776568287, policy loss: 5.326417493073946
Experience 5, Iter 53, disc loss: 0.01486631845731343, policy loss: 5.771256407462253
Experience 5, Iter 54, disc loss: 0.026074209368311178, policy loss: 5.302933118758703
Experience 5, Iter 55, disc loss: 0.017752629855147462, policy loss: 5.086167672649998
Experience 5, Iter 56, disc loss: 0.02234919901241892, policy loss: 4.9202077699359155
Experience 5, Iter 57, disc loss: 0.01824960112149147, policy loss: 5.444931706415577
Experience 5, Iter 58, disc loss: 0.018700971610066633, policy loss: 5.199011644207287
Experience 5, Iter 59, disc loss: 0.025493821030965706, policy loss: 5.342187825843388
Experience 5, Iter 60, disc loss: 0.021454271550289124, policy loss: 5.3477744477955405
Experience 5, Iter 61, disc loss: 0.016015002743674213, policy loss: 5.234161921506987
Experience 5, Iter 62, disc loss: 0.014114556185895497, policy loss: 5.714941788057294
Experience 5, Iter 63, disc loss: 0.013437224178642699, policy loss: 5.408740126114014
Experience 5, Iter 64, disc loss: 0.019428088704415664, policy loss: 5.443247114049569
Experience 5, Iter 65, disc loss: 0.015330669925533745, policy loss: 5.575787811960964
Experience 5, Iter 66, disc loss: 0.023486286546047164, policy loss: 5.421763352423094
Experience 5, Iter 67, disc loss: 0.014557443361542383, policy loss: 5.423570035770611
Experience 5, Iter 68, disc loss: 0.013076865994991659, policy loss: 5.727221944232474
Experience 5, Iter 69, disc loss: 0.01918358138878233, policy loss: 5.316032605678933
Experience 5, Iter 70, disc loss: 0.016896815800509023, policy loss: 5.850747616938241
Experience 5, Iter 71, disc loss: 0.017505812284959607, policy loss: 5.049466325760591
Experience 5, Iter 72, disc loss: 0.018502385174909517, policy loss: 5.36435108653099
Experience 5, Iter 73, disc loss: 0.014503447797110863, policy loss: 5.466335977206866
Experience 5, Iter 74, disc loss: 0.015335631552351639, policy loss: 5.994401936981104
Experience 5, Iter 75, disc loss: 0.01819335665676518, policy loss: 5.282699768271082
Experience 5, Iter 76, disc loss: 0.015930276062178783, policy loss: 6.018225326907087
Experience 5, Iter 77, disc loss: 0.014398011234395908, policy loss: 5.709155989389173
Experience 5, Iter 78, disc loss: 0.02240560323236511, policy loss: 5.238104106620135
Experience 5, Iter 79, disc loss: 0.019631717802247525, policy loss: 5.768782571736521
Experience 5, Iter 80, disc loss: 0.012337863188194626, policy loss: 5.725355283103578
Experience 5, Iter 81, disc loss: 0.015075911738283973, policy loss: 5.425163359793515
Experience 5, Iter 82, disc loss: 0.014681202468521842, policy loss: 5.744765211525509
Experience 5, Iter 83, disc loss: 0.017758041090019716, policy loss: 5.5210149045128105
Experience 5, Iter 84, disc loss: 0.02103082488974749, policy loss: 5.472421611026734
Experience 5, Iter 85, disc loss: 0.01276940093266627, policy loss: 5.753136290740144
Experience 5, Iter 86, disc loss: 0.011878500735620497, policy loss: 6.185420873365942
Experience 5, Iter 87, disc loss: 0.018586537715636404, policy loss: 5.631320589957847
Experience 5, Iter 88, disc loss: 0.013177373473906382, policy loss: 5.727357270444123
Experience 5, Iter 89, disc loss: 0.01010632471545072, policy loss: 5.941955027551389
Experience 5, Iter 90, disc loss: 0.015075884153251737, policy loss: 5.442236980651033
Experience 5, Iter 91, disc loss: 0.007562710393150937, policy loss: 6.261762363411168
Experience 5, Iter 92, disc loss: 0.00958487161753108, policy loss: 6.107337082312483
Experience 5, Iter 93, disc loss: 0.008899615479554035, policy loss: 6.032429227863378
Experience 5, Iter 94, disc loss: 0.01075236138858989, policy loss: 5.930957687933123
Experience 5, Iter 95, disc loss: 0.009311842091465208, policy loss: 6.129209666802101
Experience 5, Iter 96, disc loss: 0.015928387499497664, policy loss: 5.6762073737651795
Experience 5, Iter 97, disc loss: 0.011673075712960055, policy loss: 5.813335224947707
Experience 5, Iter 98, disc loss: 0.013832881766813531, policy loss: 5.486621414159963
Experience 5, Iter 99, disc loss: 0.010940152232398146, policy loss: 6.258187878909631
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0020],
        [0.0081],
        [0.1491],
        [0.0031]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0136, 0.0743, 0.1460, 0.0055, 0.0003, 0.2883]],

        [[0.0136, 0.0743, 0.1460, 0.0055, 0.0003, 0.2883]],

        [[0.0136, 0.0743, 0.1460, 0.0055, 0.0003, 0.2883]],

        [[0.0136, 0.0743, 0.1460, 0.0055, 0.0003, 0.2883]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0081, 0.0323, 0.5963, 0.0124], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0081, 0.0323, 0.5963, 0.0124])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.874
Iter 2/2000 - Loss: -0.024
Iter 3/2000 - Loss: -0.859
Iter 4/2000 - Loss: -0.725
Iter 5/2000 - Loss: -0.421
Iter 6/2000 - Loss: -0.550
Iter 7/2000 - Loss: -0.799
Iter 8/2000 - Loss: -0.884
Iter 9/2000 - Loss: -0.793
Iter 10/2000 - Loss: -0.689
Iter 11/2000 - Loss: -0.699
Iter 12/2000 - Loss: -0.803
Iter 13/2000 - Loss: -0.900
Iter 14/2000 - Loss: -0.920
Iter 15/2000 - Loss: -0.875
Iter 16/2000 - Loss: -0.843
Iter 17/2000 - Loss: -0.877
Iter 18/2000 - Loss: -0.958
Iter 19/2000 - Loss: -1.022
Iter 20/2000 - Loss: -1.041
Iter 1981/2000 - Loss: -8.156
Iter 1982/2000 - Loss: -8.156
Iter 1983/2000 - Loss: -8.156
Iter 1984/2000 - Loss: -8.156
Iter 1985/2000 - Loss: -8.156
Iter 1986/2000 - Loss: -8.156
Iter 1987/2000 - Loss: -8.156
Iter 1988/2000 - Loss: -8.156
Iter 1989/2000 - Loss: -8.156
Iter 1990/2000 - Loss: -8.156
Iter 1991/2000 - Loss: -8.156
Iter 1992/2000 - Loss: -8.156
Iter 1993/2000 - Loss: -8.156
Iter 1994/2000 - Loss: -8.156
Iter 1995/2000 - Loss: -8.156
Iter 1996/2000 - Loss: -8.156
Iter 1997/2000 - Loss: -8.156
Iter 1998/2000 - Loss: -8.156
Iter 1999/2000 - Loss: -8.156
Iter 2000/2000 - Loss: -8.156
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.1073,  4.9024, 38.8406, 14.5050,  8.2704, 37.3948]],

        [[21.2089, 35.5047, 17.5012,  0.7886,  4.6574,  8.0013]],

        [[27.5794, 39.0713, 15.7666,  0.8743,  4.5326, 12.1764]],

        [[22.1236, 33.0419,  4.9543,  1.6764,  4.7121, 20.5867]]])
Signal Variance: tensor([0.0581, 0.2434, 6.7110, 0.0972])
Estimated target variance: tensor([0.0081, 0.0323, 0.5963, 0.0124])
N: 60
Signal to noise ratio: tensor([13.7272, 29.6864, 62.4800, 19.4754])
Bound on condition number: tensor([ 11307.0858,  52877.8052, 234226.3927,  22758.3794])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.31221618293517583, policy loss: 1.7925819862464598
Experience 6, Iter 1, disc loss: 0.3503354386054865, policy loss: 1.773725854051491
Experience 6, Iter 2, disc loss: 0.4120685114835418, policy loss: 1.461880803473818
Experience 6, Iter 3, disc loss: 0.4575175028807573, policy loss: 1.4766438123329355
Experience 6, Iter 4, disc loss: 0.46691353859570295, policy loss: 1.3672656030361519
Experience 6, Iter 5, disc loss: 0.5577295237668901, policy loss: 1.2704932593107205
Experience 6, Iter 6, disc loss: 0.5149003216059225, policy loss: 1.2828817819854408
Experience 6, Iter 7, disc loss: 0.5253239217227216, policy loss: 1.256374861676146
Experience 6, Iter 8, disc loss: 0.5136653762873301, policy loss: 1.4230625717796261
Experience 6, Iter 9, disc loss: 0.5117847014918128, policy loss: 1.3495237965075404
Experience 6, Iter 10, disc loss: 0.5005889651493935, policy loss: 1.292238108289356
Experience 6, Iter 11, disc loss: 0.5163589892808419, policy loss: 1.2925670013633566
Experience 6, Iter 12, disc loss: 0.3983445069734673, policy loss: 1.594704479385542
Experience 6, Iter 13, disc loss: 0.38846580686262155, policy loss: 1.5495463403801482
Experience 6, Iter 14, disc loss: 0.29845305962908236, policy loss: 2.031489070938114
Experience 6, Iter 15, disc loss: 0.26462841009206195, policy loss: 2.192901211293419
Experience 6, Iter 16, disc loss: 0.23832532001067513, policy loss: 2.368044494175758
Experience 6, Iter 17, disc loss: 0.23228602688634553, policy loss: 2.3708129429038087
Experience 6, Iter 18, disc loss: 0.21494110344931983, policy loss: 2.500653406320465
Experience 6, Iter 19, disc loss: 0.20935121746577354, policy loss: 2.594266604256503
Experience 6, Iter 20, disc loss: 0.2007712858773411, policy loss: 2.9393535308220633
Experience 6, Iter 21, disc loss: 0.20467571232351361, policy loss: 2.850417290484052
Experience 6, Iter 22, disc loss: 0.1818183835373394, policy loss: 3.102029936896673
Experience 6, Iter 23, disc loss: 0.17452347460259743, policy loss: 2.9944642035656326
Experience 6, Iter 24, disc loss: 0.1622605958641024, policy loss: 3.0232348987137225
Experience 6, Iter 25, disc loss: 0.1494757552469341, policy loss: 2.9140667423874387
Experience 6, Iter 26, disc loss: 0.1283362493982401, policy loss: 3.0820536192065164
Experience 6, Iter 27, disc loss: 0.1249037126793075, policy loss: 3.0220677496963044
Experience 6, Iter 28, disc loss: 0.11693240395161314, policy loss: 3.014002866364475
Experience 6, Iter 29, disc loss: 0.10750922800154733, policy loss: 3.032152883355671
Experience 6, Iter 30, disc loss: 0.11045880742358682, policy loss: 2.88308197068843
Experience 6, Iter 31, disc loss: 0.10141278798046019, policy loss: 2.9479372116732243
Experience 6, Iter 32, disc loss: 0.09997228077670693, policy loss: 3.033677761016149
Experience 6, Iter 33, disc loss: 0.10318489624247176, policy loss: 2.9089835538046738
Experience 6, Iter 34, disc loss: 0.096182449050776, policy loss: 3.1430381826654146
Experience 6, Iter 35, disc loss: 0.09744829988444188, policy loss: 2.7718627032125784
Experience 6, Iter 36, disc loss: 0.0900049354962318, policy loss: 2.9240863991718005
Experience 6, Iter 37, disc loss: 0.10561208690374135, policy loss: 2.749770397099624
Experience 6, Iter 38, disc loss: 0.08971904193548935, policy loss: 3.1174714298481723
Experience 6, Iter 39, disc loss: 0.09709139281379799, policy loss: 2.9619249215170482
Experience 6, Iter 40, disc loss: 0.08640510665506249, policy loss: 3.1959161518080963
Experience 6, Iter 41, disc loss: 0.08615287339753838, policy loss: 3.0480598279804654
Experience 6, Iter 42, disc loss: 0.0776174409244766, policy loss: 3.101311396424935
Experience 6, Iter 43, disc loss: 0.08351646303249036, policy loss: 3.157644334009145
Experience 6, Iter 44, disc loss: 0.07431430445446657, policy loss: 3.2881615849340866
Experience 6, Iter 45, disc loss: 0.08740123083610679, policy loss: 3.2487634027256433
Experience 6, Iter 46, disc loss: 0.0756062053875402, policy loss: 3.4249136152328417
Experience 6, Iter 47, disc loss: 0.07760309787311721, policy loss: 3.507757165806942
Experience 6, Iter 48, disc loss: 0.07543473521884478, policy loss: 3.5178261922463117
Experience 6, Iter 49, disc loss: 0.0728109712709249, policy loss: 3.460351222359232
Experience 6, Iter 50, disc loss: 0.0804492581371518, policy loss: 3.273052070705423
Experience 6, Iter 51, disc loss: 0.07453121564896101, policy loss: 3.4903327188698308
Experience 6, Iter 52, disc loss: 0.08699661412014514, policy loss: 3.087558663853937
Experience 6, Iter 53, disc loss: 0.07015917234616689, policy loss: 3.746684150888524
Experience 6, Iter 54, disc loss: 0.07136345817994516, policy loss: 3.520335938284743
Experience 6, Iter 55, disc loss: 0.06327940405702256, policy loss: 3.8215679033832397
Experience 6, Iter 56, disc loss: 0.07473063266623148, policy loss: 3.36510273396215
Experience 6, Iter 57, disc loss: 0.06964171995921442, policy loss: 3.4922643497373835
Experience 6, Iter 58, disc loss: 0.06904612038555907, policy loss: 3.589738645420125
Experience 6, Iter 59, disc loss: 0.06960970383311309, policy loss: 3.484056630494663
Experience 6, Iter 60, disc loss: 0.06601483355753208, policy loss: 3.7609075139113166
Experience 6, Iter 61, disc loss: 0.06986911015721686, policy loss: 3.5825257310718346
Experience 6, Iter 62, disc loss: 0.07166674289997566, policy loss: 3.5975551770204324
Experience 6, Iter 63, disc loss: 0.06253144135048133, policy loss: 3.7340874554303207
Experience 6, Iter 64, disc loss: 0.06984667872854236, policy loss: 3.5191316033322946
Experience 6, Iter 65, disc loss: 0.07032662033150419, policy loss: 3.684903890754299
Experience 6, Iter 66, disc loss: 0.06538581367550243, policy loss: 3.6096603472188713
Experience 6, Iter 67, disc loss: 0.07320284442518073, policy loss: 3.596896838790758
Experience 6, Iter 68, disc loss: 0.05825215613110117, policy loss: 3.7387278830914754
Experience 6, Iter 69, disc loss: 0.0663435952345045, policy loss: 3.4136392361079246
Experience 6, Iter 70, disc loss: 0.059647933145616, policy loss: 3.7323942987781513
Experience 6, Iter 71, disc loss: 0.07409139635165342, policy loss: 3.599483253674612
Experience 6, Iter 72, disc loss: 0.06177635560298192, policy loss: 3.9364289287580223
Experience 6, Iter 73, disc loss: 0.05974182990755609, policy loss: 3.820677677938748
Experience 6, Iter 74, disc loss: 0.05265006663017959, policy loss: 3.7536973963173814
Experience 6, Iter 75, disc loss: 0.05838873997909944, policy loss: 3.680290604559213
Experience 6, Iter 76, disc loss: 0.06674750853451002, policy loss: 3.43953279496344
Experience 6, Iter 77, disc loss: 0.06433332029673332, policy loss: 3.6640327848985965
Experience 6, Iter 78, disc loss: 0.05681395037318146, policy loss: 3.8007842099440348
Experience 6, Iter 79, disc loss: 0.05538263004682528, policy loss: 3.6977015328691705
Experience 6, Iter 80, disc loss: 0.05523654026736736, policy loss: 3.9810243613961385
Experience 6, Iter 81, disc loss: 0.05622948154376328, policy loss: 3.783820266134049
Experience 6, Iter 82, disc loss: 0.05798625219458109, policy loss: 3.7678699944800718
Experience 6, Iter 83, disc loss: 0.05637822125429216, policy loss: 3.896892714793858
Experience 6, Iter 84, disc loss: 0.05803978551845032, policy loss: 3.6757870985834424
Experience 6, Iter 85, disc loss: 0.050181235756494746, policy loss: 3.8920642917256467
Experience 6, Iter 86, disc loss: 0.04928638916217048, policy loss: 3.9425944644176347
Experience 6, Iter 87, disc loss: 0.05998580576511702, policy loss: 3.739926261326895
Experience 6, Iter 88, disc loss: 0.04954986535526915, policy loss: 3.910769048195043
Experience 6, Iter 89, disc loss: 0.05692263194859517, policy loss: 3.8149664761629816
Experience 6, Iter 90, disc loss: 0.047490928320960385, policy loss: 3.9495661510673674
Experience 6, Iter 91, disc loss: 0.057123497009947996, policy loss: 3.6374234449372826
Experience 6, Iter 92, disc loss: 0.05746647934828078, policy loss: 3.7738964563301045
Experience 6, Iter 93, disc loss: 0.05210970995167633, policy loss: 3.8876519066482493
Experience 6, Iter 94, disc loss: 0.054020930961213603, policy loss: 3.9904232031786933
Experience 6, Iter 95, disc loss: 0.059028789959379144, policy loss: 3.570032978663856
Experience 6, Iter 96, disc loss: 0.05301963846599808, policy loss: 3.9155538240286125
Experience 6, Iter 97, disc loss: 0.05452281215082702, policy loss: 3.9674757742916498
Experience 6, Iter 98, disc loss: 0.052301636753004636, policy loss: 4.0306528043523935
Experience 6, Iter 99, disc loss: 0.04860220232339321, policy loss: 4.053719643884633
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0223],
        [0.2981],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0121, 0.0715, 0.2351, 0.0086, 0.0008, 0.6802]],

        [[0.0121, 0.0715, 0.2351, 0.0086, 0.0008, 0.6802]],

        [[0.0121, 0.0715, 0.2351, 0.0086, 0.0008, 0.6802]],

        [[0.0121, 0.0715, 0.2351, 0.0086, 0.0008, 0.6802]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0078, 0.0892, 1.1926, 0.0197], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0078, 0.0892, 1.1926, 0.0197])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.193
Iter 2/2000 - Loss: 0.840
Iter 3/2000 - Loss: 0.190
Iter 4/2000 - Loss: 0.292
Iter 5/2000 - Loss: 0.519
Iter 6/2000 - Loss: 0.408
Iter 7/2000 - Loss: 0.215
Iter 8/2000 - Loss: 0.156
Iter 9/2000 - Loss: 0.226
Iter 10/2000 - Loss: 0.291
Iter 11/2000 - Loss: 0.265
Iter 12/2000 - Loss: 0.179
Iter 13/2000 - Loss: 0.104
Iter 14/2000 - Loss: 0.079
Iter 15/2000 - Loss: 0.082
Iter 16/2000 - Loss: 0.069
Iter 17/2000 - Loss: 0.013
Iter 18/2000 - Loss: -0.078
Iter 19/2000 - Loss: -0.180
Iter 20/2000 - Loss: -0.276
Iter 1981/2000 - Loss: -7.817
Iter 1982/2000 - Loss: -7.817
Iter 1983/2000 - Loss: -7.818
Iter 1984/2000 - Loss: -7.818
Iter 1985/2000 - Loss: -7.818
Iter 1986/2000 - Loss: -7.818
Iter 1987/2000 - Loss: -7.818
Iter 1988/2000 - Loss: -7.818
Iter 1989/2000 - Loss: -7.818
Iter 1990/2000 - Loss: -7.818
Iter 1991/2000 - Loss: -7.818
Iter 1992/2000 - Loss: -7.818
Iter 1993/2000 - Loss: -7.818
Iter 1994/2000 - Loss: -7.818
Iter 1995/2000 - Loss: -7.818
Iter 1996/2000 - Loss: -7.818
Iter 1997/2000 - Loss: -7.818
Iter 1998/2000 - Loss: -7.818
Iter 1999/2000 - Loss: -7.818
Iter 2000/2000 - Loss: -7.818
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[13.5686,  4.8570, 42.2971,  6.9831,  8.5510, 29.1310]],

        [[22.6486, 35.4397, 12.0385,  0.8881,  3.1609,  7.9336]],

        [[25.2265, 43.5477, 12.6888,  0.9447,  6.5788, 15.3620]],

        [[20.1030, 33.2349,  4.2470,  2.7647,  5.2444, 33.1989]]])
Signal Variance: tensor([ 0.0505,  0.4908, 10.5095,  0.1706])
Estimated target variance: tensor([0.0078, 0.0892, 1.1926, 0.0197])
N: 70
Signal to noise ratio: tensor([13.0889, 44.4523, 78.9920, 25.5192])
Bound on condition number: tensor([ 11993.3022, 138321.5765, 436782.6341,  45586.8919])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.02136077294201058, policy loss: 5.994270796248077
Experience 7, Iter 1, disc loss: 0.020636663834571956, policy loss: 5.993091712238435
Experience 7, Iter 2, disc loss: 0.02220811608324392, policy loss: 5.65641426371417
Experience 7, Iter 3, disc loss: 0.02118485044865106, policy loss: 5.392897260551405
Experience 7, Iter 4, disc loss: 0.020176492461342767, policy loss: 5.465989623819715
Experience 7, Iter 5, disc loss: 0.02104673326801047, policy loss: 5.1512919971809055
Experience 7, Iter 6, disc loss: 0.020584824928838025, policy loss: 5.135271276662931
Experience 7, Iter 7, disc loss: 0.02123566000513244, policy loss: 4.803332915375987
Experience 7, Iter 8, disc loss: 0.02195372342201652, policy loss: 4.707661951690299
Experience 7, Iter 9, disc loss: 0.019607018427633037, policy loss: 4.791325656797927
Experience 7, Iter 10, disc loss: 0.01961710258688809, policy loss: 4.693378959017692
Experience 7, Iter 11, disc loss: 0.016839495153250204, policy loss: 4.944935757796824
Experience 7, Iter 12, disc loss: 0.01739214100800519, policy loss: 4.854635022900949
Experience 7, Iter 13, disc loss: 0.019217559693505176, policy loss: 4.650049543189935
Experience 7, Iter 14, disc loss: 0.019848272836234564, policy loss: 4.474486884345273
Experience 7, Iter 15, disc loss: 0.016539310601562697, policy loss: 4.788457611599902
Experience 7, Iter 16, disc loss: 0.01754150766245885, policy loss: 4.640244639895117
Experience 7, Iter 17, disc loss: 0.0186867754404121, policy loss: 4.509635838024817
Experience 7, Iter 18, disc loss: 0.019869611679622614, policy loss: 4.4235226685013895
Experience 7, Iter 19, disc loss: 0.0215619599456004, policy loss: 4.323050947191758
Experience 7, Iter 20, disc loss: 0.01869038472382762, policy loss: 4.438977709480909
Experience 7, Iter 21, disc loss: 0.021287764018641263, policy loss: 4.240240544114172
Experience 7, Iter 22, disc loss: 0.019501809821584802, policy loss: 4.382229820831784
Experience 7, Iter 23, disc loss: 0.023794143885118763, policy loss: 4.124760795090037
Experience 7, Iter 24, disc loss: 0.02343552850851497, policy loss: 4.205801824104734
Experience 7, Iter 25, disc loss: 0.0234592990332581, policy loss: 4.191698950535362
Experience 7, Iter 26, disc loss: 0.023774971145846917, policy loss: 4.245083114852321
Experience 7, Iter 27, disc loss: 0.018376231434707097, policy loss: 4.594585180594997
Experience 7, Iter 28, disc loss: 0.024917303779555578, policy loss: 4.13132828676812
Experience 7, Iter 29, disc loss: 0.024216022678046893, policy loss: 4.128212203810486
Experience 7, Iter 30, disc loss: 0.021999633288414235, policy loss: 4.2868729101821135
Experience 7, Iter 31, disc loss: 0.02353543818195373, policy loss: 4.159925966113226
Experience 7, Iter 32, disc loss: 0.02220593274240604, policy loss: 4.3600437786138375
Experience 7, Iter 33, disc loss: 0.023278651977687116, policy loss: 4.296729994120662
Experience 7, Iter 34, disc loss: 0.024565350745285163, policy loss: 4.092367736844039
Experience 7, Iter 35, disc loss: 0.021228416520695376, policy loss: 4.358607747037513
Experience 7, Iter 36, disc loss: 0.020581627336420834, policy loss: 4.359788895407329
Experience 7, Iter 37, disc loss: 0.02346146889802389, policy loss: 4.254301171455266
Experience 7, Iter 38, disc loss: 0.020795338948691806, policy loss: 4.412004335494205
Experience 7, Iter 39, disc loss: 0.019413993917727072, policy loss: 4.628225463253167
Experience 7, Iter 40, disc loss: 0.018257802817253366, policy loss: 4.596124756236817
Experience 7, Iter 41, disc loss: 0.019824538914613596, policy loss: 4.505804769216051
Experience 7, Iter 42, disc loss: 0.018621730147152175, policy loss: 4.655988781837475
Experience 7, Iter 43, disc loss: 0.01938623165428121, policy loss: 4.550731511877773
Experience 7, Iter 44, disc loss: 0.019938035405496587, policy loss: 4.5933747177503275
Experience 7, Iter 45, disc loss: 0.020596730435631674, policy loss: 4.517589449450959
Experience 7, Iter 46, disc loss: 0.01969252345406633, policy loss: 4.759873914677453
Experience 7, Iter 47, disc loss: 0.019938384717070323, policy loss: 4.493199819198113
Experience 7, Iter 48, disc loss: 0.01777744879640905, policy loss: 4.705674838061958
Experience 7, Iter 49, disc loss: 0.016511361723405234, policy loss: 4.954885219379369
Experience 7, Iter 50, disc loss: 0.017087485851845868, policy loss: 4.847238937755878
Experience 7, Iter 51, disc loss: 0.018138314780226827, policy loss: 4.714595886472225
Experience 7, Iter 52, disc loss: 0.018170606031760593, policy loss: 4.592684226243373
Experience 7, Iter 53, disc loss: 0.01846551445729179, policy loss: 4.60521542242604
Experience 7, Iter 54, disc loss: 0.01651223095443853, policy loss: 4.681705635136797
Experience 7, Iter 55, disc loss: 0.01813870496807647, policy loss: 4.593150623730454
Experience 7, Iter 56, disc loss: 0.01755864985879689, policy loss: 4.601473762352532
Experience 7, Iter 57, disc loss: 0.016706991947261104, policy loss: 4.691641208135844
Experience 7, Iter 58, disc loss: 0.016381439561197857, policy loss: 4.659526157288505
Experience 7, Iter 59, disc loss: 0.016256932222937625, policy loss: 4.721748377177282
Experience 7, Iter 60, disc loss: 0.0173654299697121, policy loss: 4.639419597339854
Experience 7, Iter 61, disc loss: 0.017356721714927997, policy loss: 4.693527400570483
Experience 7, Iter 62, disc loss: 0.016255771708558433, policy loss: 4.637429714179332
Experience 7, Iter 63, disc loss: 0.016091581854976077, policy loss: 4.7008799615039
Experience 7, Iter 64, disc loss: 0.015498707288832778, policy loss: 4.7337201522959305
Experience 7, Iter 65, disc loss: 0.016630209571594393, policy loss: 4.606893157291793
Experience 7, Iter 66, disc loss: 0.014189547632772533, policy loss: 4.869788442147071
Experience 7, Iter 67, disc loss: 0.01572633197041851, policy loss: 4.750951849327455
Experience 7, Iter 68, disc loss: 0.015401661326931548, policy loss: 4.681503479099774
Experience 7, Iter 69, disc loss: 0.016185838626873273, policy loss: 4.695136343276481
Experience 7, Iter 70, disc loss: 0.015994491290792043, policy loss: 4.642633436786715
Experience 7, Iter 71, disc loss: 0.015146271443244881, policy loss: 4.716094963896907
Experience 7, Iter 72, disc loss: 0.014385217838778708, policy loss: 4.788314456852175
Experience 7, Iter 73, disc loss: 0.01382283045718253, policy loss: 4.835437135669148
Experience 7, Iter 74, disc loss: 0.013442655616869793, policy loss: 4.919461096850259
Experience 7, Iter 75, disc loss: 0.014916953919471565, policy loss: 4.762291974865549
Experience 7, Iter 76, disc loss: 0.01332037461881208, policy loss: 5.121437213304178
Experience 7, Iter 77, disc loss: 0.013885787138221767, policy loss: 4.842627181532158
Experience 7, Iter 78, disc loss: 0.013641032941289407, policy loss: 4.867766121127137
Experience 7, Iter 79, disc loss: 0.01355284229524398, policy loss: 4.806014554571767
Experience 7, Iter 80, disc loss: 0.013205978948893988, policy loss: 4.91174958104727
Experience 7, Iter 81, disc loss: 0.013429504557019085, policy loss: 4.8485666592009
Experience 7, Iter 82, disc loss: 0.013374636506799114, policy loss: 4.824998391914722
Experience 7, Iter 83, disc loss: 0.014226997565212143, policy loss: 4.801409022216986
Experience 7, Iter 84, disc loss: 0.012338159141107145, policy loss: 5.0026505744107235
Experience 7, Iter 85, disc loss: 0.012862486470207107, policy loss: 5.030413416995791
Experience 7, Iter 86, disc loss: 0.013486517841646372, policy loss: 4.816536370934421
Experience 7, Iter 87, disc loss: 0.012419692066092183, policy loss: 4.9331588717561345
Experience 7, Iter 88, disc loss: 0.0130284963293048, policy loss: 4.927738223937182
Experience 7, Iter 89, disc loss: 0.012688796394212155, policy loss: 4.967750057397245
Experience 7, Iter 90, disc loss: 0.011185471823796589, policy loss: 5.181110036152665
Experience 7, Iter 91, disc loss: 0.01183434065486773, policy loss: 5.131159651296921
Experience 7, Iter 92, disc loss: 0.012251677967728747, policy loss: 4.9520688769947405
Experience 7, Iter 93, disc loss: 0.011476743707663176, policy loss: 5.103805983513663
Experience 7, Iter 94, disc loss: 0.011673522243022599, policy loss: 5.085553358088742
Experience 7, Iter 95, disc loss: 0.01177635733307101, policy loss: 4.973847509744051
Experience 7, Iter 96, disc loss: 0.011500944291313209, policy loss: 4.999196296283049
Experience 7, Iter 97, disc loss: 0.011890799934775242, policy loss: 4.996589659117594
Experience 7, Iter 98, disc loss: 0.010563190861535402, policy loss: 5.151843778676346
Experience 7, Iter 99, disc loss: 0.010472501960492058, policy loss: 5.087523097374385
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0261],
        [0.3164],
        [0.0051]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1031e-02, 6.5451e-02, 2.4477e-01, 8.9103e-03, 7.1943e-04,
          7.8853e-01]],

        [[1.1031e-02, 6.5451e-02, 2.4477e-01, 8.9103e-03, 7.1943e-04,
          7.8853e-01]],

        [[1.1031e-02, 6.5451e-02, 2.4477e-01, 8.9103e-03, 7.1943e-04,
          7.8853e-01]],

        [[1.1031e-02, 6.5451e-02, 2.4477e-01, 8.9103e-03, 7.1943e-04,
          7.8853e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0071, 0.1043, 1.2655, 0.0205], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0071, 0.1043, 1.2655, 0.0205])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.290
Iter 2/2000 - Loss: 0.890
Iter 3/2000 - Loss: 0.278
Iter 4/2000 - Loss: 0.371
Iter 5/2000 - Loss: 0.581
Iter 6/2000 - Loss: 0.467
Iter 7/2000 - Loss: 0.291
Iter 8/2000 - Loss: 0.254
Iter 9/2000 - Loss: 0.326
Iter 10/2000 - Loss: 0.374
Iter 11/2000 - Loss: 0.339
Iter 12/2000 - Loss: 0.263
Iter 13/2000 - Loss: 0.208
Iter 14/2000 - Loss: 0.191
Iter 15/2000 - Loss: 0.191
Iter 16/2000 - Loss: 0.173
Iter 17/2000 - Loss: 0.119
Iter 18/2000 - Loss: 0.037
Iter 19/2000 - Loss: -0.050
Iter 20/2000 - Loss: -0.131
Iter 1981/2000 - Loss: -7.928
Iter 1982/2000 - Loss: -7.928
Iter 1983/2000 - Loss: -7.928
Iter 1984/2000 - Loss: -7.928
Iter 1985/2000 - Loss: -7.928
Iter 1986/2000 - Loss: -7.928
Iter 1987/2000 - Loss: -7.928
Iter 1988/2000 - Loss: -7.928
Iter 1989/2000 - Loss: -7.928
Iter 1990/2000 - Loss: -7.929
Iter 1991/2000 - Loss: -7.929
Iter 1992/2000 - Loss: -7.929
Iter 1993/2000 - Loss: -7.929
Iter 1994/2000 - Loss: -7.929
Iter 1995/2000 - Loss: -7.929
Iter 1996/2000 - Loss: -7.929
Iter 1997/2000 - Loss: -7.929
Iter 1998/2000 - Loss: -7.929
Iter 1999/2000 - Loss: -7.929
Iter 2000/2000 - Loss: -7.929
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[15.6598,  4.8022, 41.4785,  7.0628,  9.6082, 37.6892]],

        [[24.7653, 40.1873,  8.9485,  1.2735,  6.3085, 23.7355]],

        [[25.2300, 42.8314,  8.9548,  1.0439,  6.9176, 20.2811]],

        [[19.0565, 32.9837, 12.1267,  3.3293,  0.9650, 26.4157]]])
Signal Variance: tensor([ 0.0566,  1.5853, 12.9207,  0.2494])
Estimated target variance: tensor([0.0071, 0.1043, 1.2655, 0.0205])
N: 80
Signal to noise ratio: tensor([13.6336, 75.5489, 87.9738, 30.5658])
Bound on condition number: tensor([ 14871.0231, 456612.3902, 619152.0663,  74742.6705])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.010251347679601047, policy loss: 5.12263883783525
Experience 8, Iter 1, disc loss: 0.01018785126605604, policy loss: 5.104457070013248
Experience 8, Iter 2, disc loss: 0.010835215751827582, policy loss: 5.002631859814926
Experience 8, Iter 3, disc loss: 0.010414019479602537, policy loss: 5.095911535595623
Experience 8, Iter 4, disc loss: 0.010067049126331096, policy loss: 5.168768115577531
Experience 8, Iter 5, disc loss: 0.01132500011984226, policy loss: 4.915819432764541
Experience 8, Iter 6, disc loss: 0.010596959265085193, policy loss: 5.030702151092036
Experience 8, Iter 7, disc loss: 0.009954754976609545, policy loss: 5.118684836616694
Experience 8, Iter 8, disc loss: 0.010258314322480165, policy loss: 5.097421282682238
Experience 8, Iter 9, disc loss: 0.010776659882819957, policy loss: 5.280545598329761
Experience 8, Iter 10, disc loss: 0.009842683883185072, policy loss: 5.1592423858293985
Experience 8, Iter 11, disc loss: 0.0103388683895233, policy loss: 5.075311552350247
Experience 8, Iter 12, disc loss: 0.009736002778732912, policy loss: 5.184457563569413
Experience 8, Iter 13, disc loss: 0.009442383894414494, policy loss: 5.1767039930175365
Experience 8, Iter 14, disc loss: 0.009727754303662576, policy loss: 5.183527217655505
Experience 8, Iter 15, disc loss: 0.010727332952133881, policy loss: 5.012401108226629
Experience 8, Iter 16, disc loss: 0.009798812943086398, policy loss: 5.1432278274522325
Experience 8, Iter 17, disc loss: 0.00985214287917158, policy loss: 5.137231722241409
Experience 8, Iter 18, disc loss: 0.009955882442437522, policy loss: 5.1584030739346325
Experience 8, Iter 19, disc loss: 0.009711532981476, policy loss: 5.163145233457218
Experience 8, Iter 20, disc loss: 0.009908969821135923, policy loss: 5.145388467907934
Experience 8, Iter 21, disc loss: 0.010705184517874833, policy loss: 5.0740801697646924
Experience 8, Iter 22, disc loss: 0.008727795527195641, policy loss: 5.311128296807207
Experience 8, Iter 23, disc loss: 0.0097167473219017, policy loss: 5.169961571841538
Experience 8, Iter 24, disc loss: 0.009302007295907602, policy loss: 5.248479624662721
Experience 8, Iter 25, disc loss: 0.00915921619526163, policy loss: 5.22211719080367
Experience 8, Iter 26, disc loss: 0.009169806453233257, policy loss: 5.191671981253734
Experience 8, Iter 27, disc loss: 0.009402933860518381, policy loss: 5.22044216799633
Experience 8, Iter 28, disc loss: 0.008718200037958462, policy loss: 5.357868638820474
Experience 8, Iter 29, disc loss: 0.00965932845276342, policy loss: 5.1601547427969425
Experience 8, Iter 30, disc loss: 0.008656424616531953, policy loss: 5.3726621544290944
Experience 8, Iter 31, disc loss: 0.009049796123919795, policy loss: 5.252865021253971
Experience 8, Iter 32, disc loss: 0.009057430149303694, policy loss: 5.240353959598169
Experience 8, Iter 33, disc loss: 0.009141579496131019, policy loss: 5.2216953662059815
Experience 8, Iter 34, disc loss: 0.00890015005217819, policy loss: 5.297178136260243
Experience 8, Iter 35, disc loss: 0.009158892074039051, policy loss: 5.17834388356551
Experience 8, Iter 36, disc loss: 0.008941333065237325, policy loss: 5.382632096419456
Experience 8, Iter 37, disc loss: 0.008582384761887364, policy loss: 5.390836669018969
Experience 8, Iter 38, disc loss: 0.009021497798305989, policy loss: 5.258650739290995
Experience 8, Iter 39, disc loss: 0.008851205786406306, policy loss: 5.234719868380868
Experience 8, Iter 40, disc loss: 0.00792245195568669, policy loss: 5.394305802174786
Experience 8, Iter 41, disc loss: 0.007740085848674046, policy loss: 5.4588908258231985
Experience 8, Iter 42, disc loss: 0.008492370530971318, policy loss: 5.318605550113073
Experience 8, Iter 43, disc loss: 0.008589614977705972, policy loss: 5.326676134112629
Experience 8, Iter 44, disc loss: 0.008753574331204455, policy loss: 5.287751234520929
Experience 8, Iter 45, disc loss: 0.008649636985985944, policy loss: 5.296284581331717
Experience 8, Iter 46, disc loss: 0.008345096692546466, policy loss: 5.422207495213661
Experience 8, Iter 47, disc loss: 0.007900113386570113, policy loss: 5.444154543806495
Experience 8, Iter 48, disc loss: 0.008182184611954092, policy loss: 5.354515090850796
Experience 8, Iter 49, disc loss: 0.0080632686462595, policy loss: 5.478629170839712
Experience 8, Iter 50, disc loss: 0.007669133941405618, policy loss: 5.4197994209423275
Experience 8, Iter 51, disc loss: 0.007730404922723989, policy loss: 5.491262257539864
Experience 8, Iter 52, disc loss: 0.008396781678632715, policy loss: 5.285595851238567
Experience 8, Iter 53, disc loss: 0.008023302662769745, policy loss: 5.434531250955155
Experience 8, Iter 54, disc loss: 0.008323576158535776, policy loss: 5.319557690360649
Experience 8, Iter 55, disc loss: 0.00848645804818247, policy loss: 5.319813093036169
Experience 8, Iter 56, disc loss: 0.008176570644339472, policy loss: 5.385089775365317
Experience 8, Iter 57, disc loss: 0.007681897676403643, policy loss: 5.432275657209514
Experience 8, Iter 58, disc loss: 0.007960916329980482, policy loss: 5.390018547478013
Experience 8, Iter 59, disc loss: 0.008624355789915425, policy loss: 5.293294065233427
Experience 8, Iter 60, disc loss: 0.0074587322751496505, policy loss: 5.6209809312789565
Experience 8, Iter 61, disc loss: 0.0077934069477831455, policy loss: 5.374039449702637
Experience 8, Iter 62, disc loss: 0.0070991571466050936, policy loss: 5.646663698703577
Experience 8, Iter 63, disc loss: 0.007103486827076556, policy loss: 5.620346609910408
Experience 8, Iter 64, disc loss: 0.007513919697143445, policy loss: 5.5356960674845
Experience 8, Iter 65, disc loss: 0.007807357809790461, policy loss: 5.408090852066341
Experience 8, Iter 66, disc loss: 0.007256416310398633, policy loss: 5.537560735376458
Experience 8, Iter 67, disc loss: 0.007818467141805095, policy loss: 5.449257189661175
Experience 8, Iter 68, disc loss: 0.00709852033602162, policy loss: 5.660921340262292
Experience 8, Iter 69, disc loss: 0.007235976773025008, policy loss: 5.5264042963996625
Experience 8, Iter 70, disc loss: 0.006433552932837117, policy loss: 5.68034457230567
Experience 8, Iter 71, disc loss: 0.00676143911569458, policy loss: 5.634158753787602
Experience 8, Iter 72, disc loss: 0.007499979634162438, policy loss: 5.413577162551688
Experience 8, Iter 73, disc loss: 0.0075002485364788, policy loss: 5.43785237083174
Experience 8, Iter 74, disc loss: 0.007375134178052513, policy loss: 5.549871565752685
Experience 8, Iter 75, disc loss: 0.006770276801920097, policy loss: 5.679235991378445
Experience 8, Iter 76, disc loss: 0.006544814889437077, policy loss: 5.690998444497677
Experience 8, Iter 77, disc loss: 0.0071520968013102535, policy loss: 5.515041327752786
Experience 8, Iter 78, disc loss: 0.007004840593905083, policy loss: 5.532888521260119
Experience 8, Iter 79, disc loss: 0.007803850297364599, policy loss: 5.337444247025008
Experience 8, Iter 80, disc loss: 0.006149531822263953, policy loss: 5.752529632541059
Experience 8, Iter 81, disc loss: 0.006653480795996557, policy loss: 5.6472769366416165
Experience 8, Iter 82, disc loss: 0.0065252703477273934, policy loss: 5.629348388647358
Experience 8, Iter 83, disc loss: 0.006645815910837721, policy loss: 5.635941476649718
Experience 8, Iter 84, disc loss: 0.005138151690264919, policy loss: 6.105467653236406
Experience 8, Iter 85, disc loss: 0.0067313761465437, policy loss: 5.588559553306292
Experience 8, Iter 86, disc loss: 0.006430597264808845, policy loss: 5.6174806178007275
Experience 8, Iter 87, disc loss: 0.00557982671336065, policy loss: 5.891948214007646
Experience 8, Iter 88, disc loss: 0.00597363158475957, policy loss: 5.7941088611572855
Experience 8, Iter 89, disc loss: 0.005780550700493301, policy loss: 5.819264867857026
Experience 8, Iter 90, disc loss: 0.006197105367876741, policy loss: 5.686163479849398
Experience 8, Iter 91, disc loss: 0.005728751870046717, policy loss: 5.787273125727372
Experience 8, Iter 92, disc loss: 0.005644330385617267, policy loss: 5.80938428961832
Experience 8, Iter 93, disc loss: 0.006427805682701842, policy loss: 5.62376933669841
Experience 8, Iter 94, disc loss: 0.005915810311077868, policy loss: 5.695808523298065
Experience 8, Iter 95, disc loss: 0.005890230509358132, policy loss: 5.743248327084671
Experience 8, Iter 96, disc loss: 0.006051441472149241, policy loss: 5.759091471847271
Experience 8, Iter 97, disc loss: 0.005657639879348124, policy loss: 5.94149464336884
Experience 8, Iter 98, disc loss: 0.005374985461994591, policy loss: 5.850478101151671
Experience 8, Iter 99, disc loss: 0.005927735261685743, policy loss: 5.718548969115751
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0374],
        [0.4060],
        [0.0056]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0614e-02, 6.3044e-02, 2.7599e-01, 9.4042e-03, 6.8667e-04,
          1.0389e+00]],

        [[1.0614e-02, 6.3044e-02, 2.7599e-01, 9.4042e-03, 6.8667e-04,
          1.0389e+00]],

        [[1.0614e-02, 6.3044e-02, 2.7599e-01, 9.4042e-03, 6.8667e-04,
          1.0389e+00]],

        [[1.0614e-02, 6.3044e-02, 2.7599e-01, 9.4042e-03, 6.8667e-04,
          1.0389e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0067, 0.1494, 1.6239, 0.0225], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0067, 0.1494, 1.6239, 0.0225])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.623
Iter 2/2000 - Loss: 1.159
Iter 3/2000 - Loss: 0.600
Iter 4/2000 - Loss: 0.675
Iter 5/2000 - Loss: 0.866
Iter 6/2000 - Loss: 0.758
Iter 7/2000 - Loss: 0.599
Iter 8/2000 - Loss: 0.575
Iter 9/2000 - Loss: 0.640
Iter 10/2000 - Loss: 0.674
Iter 11/2000 - Loss: 0.634
Iter 12/2000 - Loss: 0.563
Iter 13/2000 - Loss: 0.512
Iter 14/2000 - Loss: 0.495
Iter 15/2000 - Loss: 0.490
Iter 16/2000 - Loss: 0.464
Iter 17/2000 - Loss: 0.400
Iter 18/2000 - Loss: 0.312
Iter 19/2000 - Loss: 0.221
Iter 20/2000 - Loss: 0.136
Iter 1981/2000 - Loss: -8.025
Iter 1982/2000 - Loss: -8.025
Iter 1983/2000 - Loss: -8.025
Iter 1984/2000 - Loss: -8.025
Iter 1985/2000 - Loss: -8.025
Iter 1986/2000 - Loss: -8.025
Iter 1987/2000 - Loss: -8.025
Iter 1988/2000 - Loss: -8.025
Iter 1989/2000 - Loss: -8.026
Iter 1990/2000 - Loss: -8.026
Iter 1991/2000 - Loss: -8.026
Iter 1992/2000 - Loss: -8.026
Iter 1993/2000 - Loss: -8.026
Iter 1994/2000 - Loss: -8.026
Iter 1995/2000 - Loss: -8.026
Iter 1996/2000 - Loss: -8.026
Iter 1997/2000 - Loss: -8.026
Iter 1998/2000 - Loss: -8.026
Iter 1999/2000 - Loss: -8.026
Iter 2000/2000 - Loss: -8.026
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[14.2293,  5.1793, 42.2567,  8.0211,  8.9178, 40.4239]],

        [[22.8772, 35.7097, 10.0440,  1.3021,  7.0308, 28.8212]],

        [[22.5854, 38.9755,  8.7455,  1.0590,  6.8058, 25.6149]],

        [[18.2786, 30.2625, 13.9851,  3.8954,  0.8070, 35.8022]]])
Signal Variance: tensor([ 0.0606,  2.2934, 17.8740,  0.3302])
Estimated target variance: tensor([0.0067, 0.1494, 1.6239, 0.0225])
N: 90
Signal to noise ratio: tensor([ 14.8389,  83.8482, 106.3232,  37.4546])
Bound on condition number: tensor([  19818.3354,  632747.3965, 1017417.5214,  126257.0933])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.005843141978413522, policy loss: 5.690413793124651
Experience 9, Iter 1, disc loss: 0.005848046481418366, policy loss: 5.702287055026968
Experience 9, Iter 2, disc loss: 0.006150225300265785, policy loss: 5.596161878964087
Experience 9, Iter 3, disc loss: 0.0057147034306078565, policy loss: 5.744856521426016
Experience 9, Iter 4, disc loss: 0.006092843200065125, policy loss: 5.63377048740442
Experience 9, Iter 5, disc loss: 0.0061136388368131945, policy loss: 5.666729866153752
Experience 9, Iter 6, disc loss: 0.005093116713483456, policy loss: 5.93656252102989
Experience 9, Iter 7, disc loss: 0.0056989554605689375, policy loss: 5.725370698091567
Experience 9, Iter 8, disc loss: 0.005681543076986322, policy loss: 5.717882204803871
Experience 9, Iter 9, disc loss: 0.006264831738858584, policy loss: 5.53802776424725
Experience 9, Iter 10, disc loss: 0.005401678019068424, policy loss: 5.7480449177380954
Experience 9, Iter 11, disc loss: 0.0053354346827367265, policy loss: 5.810971537864503
Experience 9, Iter 12, disc loss: 0.005111071373451288, policy loss: 5.89673505924109
Experience 9, Iter 13, disc loss: 0.005574313817449375, policy loss: 5.775773119625818
Experience 9, Iter 14, disc loss: 0.0051658666814921855, policy loss: 5.9199228065147445
Experience 9, Iter 15, disc loss: 0.005282809506109016, policy loss: 5.875330441210078
Experience 9, Iter 16, disc loss: 0.00501230211626649, policy loss: 5.844881981419414
Experience 9, Iter 17, disc loss: 0.005047676094146209, policy loss: 5.90608937894379
Experience 9, Iter 18, disc loss: 0.004959681718560774, policy loss: 5.942096292612154
Experience 9, Iter 19, disc loss: 0.005578862095195173, policy loss: 5.822029203307107
Experience 9, Iter 20, disc loss: 0.005237292377380964, policy loss: 5.89208914857773
Experience 9, Iter 21, disc loss: 0.005422518004030472, policy loss: 5.746436905575459
Experience 9, Iter 22, disc loss: 0.004988995682038379, policy loss: 6.1028860103688825
Experience 9, Iter 23, disc loss: 0.005150611174794459, policy loss: 5.954895832254253
Experience 9, Iter 24, disc loss: 0.005095762287526045, policy loss: 5.879103808940762
Experience 9, Iter 25, disc loss: 0.005090023877421275, policy loss: 5.898580562947497
Experience 9, Iter 26, disc loss: 0.005103725523401516, policy loss: 5.789076658147552
Experience 9, Iter 27, disc loss: 0.0050834044145989556, policy loss: 5.797117112398793
Experience 9, Iter 28, disc loss: 0.0052787528785679496, policy loss: 5.751316283698366
Experience 9, Iter 29, disc loss: 0.005244651017080848, policy loss: 5.81343818543076
Experience 9, Iter 30, disc loss: 0.005110167297029199, policy loss: 5.79707197217961
Experience 9, Iter 31, disc loss: 0.005230109029445226, policy loss: 5.727099801326399
Experience 9, Iter 32, disc loss: 0.00532622363838849, policy loss: 5.76897020196634
Experience 9, Iter 33, disc loss: 0.0055695692950754145, policy loss: 5.722838022267199
Experience 9, Iter 34, disc loss: 0.005165149979268953, policy loss: 5.8639300436552775
Experience 9, Iter 35, disc loss: 0.0049162085035507844, policy loss: 6.016618257585801
Experience 9, Iter 36, disc loss: 0.004712305262949634, policy loss: 5.9488716385062865
Experience 9, Iter 37, disc loss: 0.004801980940217495, policy loss: 5.9829153420372165
Experience 9, Iter 38, disc loss: 0.0047788325551326345, policy loss: 5.865738198037155
Experience 9, Iter 39, disc loss: 0.004689612647648038, policy loss: 5.922295580009237
Experience 9, Iter 40, disc loss: 0.0051529052889948985, policy loss: 5.7854675200516334
Experience 9, Iter 41, disc loss: 0.004183365390759429, policy loss: 6.077409775054724
Experience 9, Iter 42, disc loss: 0.004601121214697748, policy loss: 6.037737084572653
Experience 9, Iter 43, disc loss: 0.004899243771138454, policy loss: 5.971769442249654
Experience 9, Iter 44, disc loss: 0.004754570453339054, policy loss: 5.90803924249233
Experience 9, Iter 45, disc loss: 0.004441616081579432, policy loss: 6.051052354469308
Experience 9, Iter 46, disc loss: 0.004489191641268672, policy loss: 5.934143238539898
Experience 9, Iter 47, disc loss: 0.004257578052494088, policy loss: 6.026391763156763
Experience 9, Iter 48, disc loss: 0.00472609595754571, policy loss: 5.901538982471369
Experience 9, Iter 49, disc loss: 0.00414326353932262, policy loss: 6.062278110680126
Experience 9, Iter 50, disc loss: 0.004179218094330591, policy loss: 6.177092822939436
Experience 9, Iter 51, disc loss: 0.0045563816475187, policy loss: 5.9729416453595
Experience 9, Iter 52, disc loss: 0.004322480639950974, policy loss: 6.014220320372212
Experience 9, Iter 53, disc loss: 0.004133070841058956, policy loss: 6.169213044480221
Experience 9, Iter 54, disc loss: 0.0044479631866280655, policy loss: 6.030330247723194
Experience 9, Iter 55, disc loss: 0.004143846204524074, policy loss: 6.120860896110613
Experience 9, Iter 56, disc loss: 0.004451447521080905, policy loss: 6.009847844325184
Experience 9, Iter 57, disc loss: 0.004318745709335741, policy loss: 6.097068640704682
Experience 9, Iter 58, disc loss: 0.0043461403863409255, policy loss: 5.9384665276399025
Experience 9, Iter 59, disc loss: 0.0046183340702188845, policy loss: 5.901138609507139
Experience 9, Iter 60, disc loss: 0.004608385948089294, policy loss: 5.84207240598941
Experience 9, Iter 61, disc loss: 0.0040767639730127985, policy loss: 6.112600242052983
Experience 9, Iter 62, disc loss: 0.004229557213142698, policy loss: 5.981225145573635
Experience 9, Iter 63, disc loss: 0.004079363875625445, policy loss: 6.086269037779962
Experience 9, Iter 64, disc loss: 0.003988129589510779, policy loss: 6.130915159281791
Experience 9, Iter 65, disc loss: 0.00413140257245551, policy loss: 6.1388643852150135
Experience 9, Iter 66, disc loss: 0.003928259302481417, policy loss: 6.236899280828761
Experience 9, Iter 67, disc loss: 0.004190130558360915, policy loss: 5.996586199293008
Experience 9, Iter 68, disc loss: 0.0041878093884569175, policy loss: 6.040207132719494
Experience 9, Iter 69, disc loss: 0.004057010694651421, policy loss: 6.020048983038658
Experience 9, Iter 70, disc loss: 0.003948999365536186, policy loss: 6.117236974821046
Experience 9, Iter 71, disc loss: 0.0038766182933398495, policy loss: 6.280866401517194
Experience 9, Iter 72, disc loss: 0.004238983134781038, policy loss: 5.951499573042718
Experience 9, Iter 73, disc loss: 0.0036539411875307344, policy loss: 6.21841838371256
Experience 9, Iter 74, disc loss: 0.0040721126222145005, policy loss: 6.083104834153257
Experience 9, Iter 75, disc loss: 0.004189141087099157, policy loss: 6.102734922850716
Experience 9, Iter 76, disc loss: 0.0034643598677342397, policy loss: 6.370817203415921
Experience 9, Iter 77, disc loss: 0.003832987337978427, policy loss: 6.167948711758779
Experience 9, Iter 78, disc loss: 0.0037994784261528728, policy loss: 6.15421990566556
Experience 9, Iter 79, disc loss: 0.003693976274124659, policy loss: 6.293950640234968
Experience 9, Iter 80, disc loss: 0.003924551235113271, policy loss: 6.1369592626046625
Experience 9, Iter 81, disc loss: 0.0036750361506440276, policy loss: 6.162885452373132
Experience 9, Iter 82, disc loss: 0.003786058297614299, policy loss: 6.253484425545137
Experience 9, Iter 83, disc loss: 0.0035900864899453813, policy loss: 6.227043940973539
Experience 9, Iter 84, disc loss: 0.003680377319053321, policy loss: 6.1049115792274495
Experience 9, Iter 85, disc loss: 0.0039022531009732884, policy loss: 6.062926043457264
Experience 9, Iter 86, disc loss: 0.0033414540497082174, policy loss: 6.3754418689569485
Experience 9, Iter 87, disc loss: 0.003635810718929235, policy loss: 6.1991217553347475
Experience 9, Iter 88, disc loss: 0.003686708840271099, policy loss: 6.171760651879252
Experience 9, Iter 89, disc loss: 0.0037818210439075654, policy loss: 6.098799727936386
Experience 9, Iter 90, disc loss: 0.003347163653431067, policy loss: 6.381543111376102
Experience 9, Iter 91, disc loss: 0.0034489415430256965, policy loss: 6.277692478172672
Experience 9, Iter 92, disc loss: 0.003984631809021288, policy loss: 6.009734498675735
Experience 9, Iter 93, disc loss: 0.0031659494081185038, policy loss: 6.447597530188593
Experience 9, Iter 94, disc loss: 0.00373538544354704, policy loss: 6.099606428365806
Experience 9, Iter 95, disc loss: 0.0036281617166434126, policy loss: 6.138294641389805
Experience 9, Iter 96, disc loss: 0.0035275919791994143, policy loss: 6.198285213524278
Experience 9, Iter 97, disc loss: 0.0034469966220061826, policy loss: 6.261649360109734
Experience 9, Iter 98, disc loss: 0.003374000024673687, policy loss: 6.248612490834634
Experience 9, Iter 99, disc loss: 0.0036718099210238577, policy loss: 6.27615102326009
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0452],
        [0.4731],
        [0.0063]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0228e-02, 6.1330e-02, 3.0970e-01, 9.9225e-03, 6.7926e-04,
          1.2374e+00]],

        [[1.0228e-02, 6.1330e-02, 3.0970e-01, 9.9225e-03, 6.7926e-04,
          1.2374e+00]],

        [[1.0228e-02, 6.1330e-02, 3.0970e-01, 9.9225e-03, 6.7926e-04,
          1.2374e+00]],

        [[1.0228e-02, 6.1330e-02, 3.0970e-01, 9.9225e-03, 6.7926e-04,
          1.2374e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0064, 0.1808, 1.8924, 0.0251], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0064, 0.1808, 1.8924, 0.0251])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.838
Iter 2/2000 - Loss: 1.325
Iter 3/2000 - Loss: 0.807
Iter 4/2000 - Loss: 0.865
Iter 5/2000 - Loss: 1.046
Iter 6/2000 - Loss: 0.944
Iter 7/2000 - Loss: 0.797
Iter 8/2000 - Loss: 0.780
Iter 9/2000 - Loss: 0.843
Iter 10/2000 - Loss: 0.872
Iter 11/2000 - Loss: 0.829
Iter 12/2000 - Loss: 0.759
Iter 13/2000 - Loss: 0.712
Iter 14/2000 - Loss: 0.699
Iter 15/2000 - Loss: 0.693
Iter 16/2000 - Loss: 0.658
Iter 17/2000 - Loss: 0.584
Iter 18/2000 - Loss: 0.491
Iter 19/2000 - Loss: 0.400
Iter 20/2000 - Loss: 0.315
Iter 1981/2000 - Loss: -8.132
Iter 1982/2000 - Loss: -8.132
Iter 1983/2000 - Loss: -8.132
Iter 1984/2000 - Loss: -8.132
Iter 1985/2000 - Loss: -8.132
Iter 1986/2000 - Loss: -8.132
Iter 1987/2000 - Loss: -8.132
Iter 1988/2000 - Loss: -8.132
Iter 1989/2000 - Loss: -8.132
Iter 1990/2000 - Loss: -8.132
Iter 1991/2000 - Loss: -8.132
Iter 1992/2000 - Loss: -8.132
Iter 1993/2000 - Loss: -8.132
Iter 1994/2000 - Loss: -8.133
Iter 1995/2000 - Loss: -8.133
Iter 1996/2000 - Loss: -8.133
Iter 1997/2000 - Loss: -8.133
Iter 1998/2000 - Loss: -8.133
Iter 1999/2000 - Loss: -8.133
Iter 2000/2000 - Loss: -8.133
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[13.1791,  5.4472, 42.0146,  9.1249,  7.6593, 43.0834]],

        [[23.2385, 36.8116, 10.3973,  1.2981,  7.1435, 32.9290]],

        [[21.8377, 35.6716,  9.2894,  1.0327,  6.2517, 27.7505]],

        [[18.7124, 33.6981, 16.3314,  4.1184,  0.6421, 38.7245]]])
Signal Variance: tensor([ 0.0636,  2.7945, 20.7523,  0.4136])
Estimated target variance: tensor([0.0064, 0.1808, 1.8924, 0.0251])
N: 100
Signal to noise ratio: tensor([ 15.4438,  94.1262, 114.4564,  42.0931])
Bound on condition number: tensor([  23852.1046,  885975.7371, 1310026.7147,  177183.7250])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.003613702393960012, policy loss: 6.179724731112709
Experience 10, Iter 1, disc loss: 0.0035719814823619443, policy loss: 6.1920702167394595
Experience 10, Iter 2, disc loss: 0.0039374735140693265, policy loss: 6.016635871909401
Experience 10, Iter 3, disc loss: 0.003873272655877433, policy loss: 6.019408796390858
Experience 10, Iter 4, disc loss: 0.003389907374401909, policy loss: 6.257205852393961
Experience 10, Iter 5, disc loss: 0.003746351852011511, policy loss: 6.0546718518857485
Experience 10, Iter 6, disc loss: 0.003649997862930553, policy loss: 6.145450659662906
Experience 10, Iter 7, disc loss: 0.0033827388743334313, policy loss: 6.241994792638895
Experience 10, Iter 8, disc loss: 0.0035581600991285503, policy loss: 6.13277404025078
Experience 10, Iter 9, disc loss: 0.003596415363670391, policy loss: 6.121820540091635
Experience 10, Iter 10, disc loss: 0.003830615187518258, policy loss: 5.995772903414226
Experience 10, Iter 11, disc loss: 0.0036021058111997822, policy loss: 6.217576404519012
Experience 10, Iter 12, disc loss: 0.003619482280982604, policy loss: 6.141559479629875
Experience 10, Iter 13, disc loss: 0.0036631775728777185, policy loss: 6.569560600644715
Experience 10, Iter 14, disc loss: 0.0035284647385186055, policy loss: 6.172678554700582
Experience 10, Iter 15, disc loss: 0.003745500430052213, policy loss: 6.043078610286756
Experience 10, Iter 16, disc loss: 0.0035034919343026215, policy loss: 6.32268442089607
Experience 10, Iter 17, disc loss: 0.003118845079266113, policy loss: 6.33705761944278
Experience 10, Iter 18, disc loss: 0.0034888911364965997, policy loss: 6.222830339572139
Experience 10, Iter 19, disc loss: 0.003429470676506735, policy loss: 6.17082279028531
Experience 10, Iter 20, disc loss: 0.0035838420932977886, policy loss: 6.119129050123714
Experience 10, Iter 21, disc loss: 0.0034514777980716223, policy loss: 6.255321577168777
Experience 10, Iter 22, disc loss: 0.0034601970587599196, policy loss: 6.203737755551998
Experience 10, Iter 23, disc loss: 0.003407342903175068, policy loss: 6.188397424927409
Experience 10, Iter 24, disc loss: 0.003211002365799816, policy loss: 6.370626978898356
Experience 10, Iter 25, disc loss: 0.0030351398827803784, policy loss: 6.3818943375940105
Experience 10, Iter 26, disc loss: 0.003704928958780687, policy loss: 6.097849534819188
Experience 10, Iter 27, disc loss: 0.0037515546870090493, policy loss: 6.069706865563486
Experience 10, Iter 28, disc loss: 0.0036129568871251672, policy loss: 6.113347760727477
Experience 10, Iter 29, disc loss: 0.0033505120521631964, policy loss: 6.334022330973369
Experience 10, Iter 30, disc loss: 0.0034383877617568774, policy loss: 6.183772700794413
Experience 10, Iter 31, disc loss: 0.00327648311710983, policy loss: 6.320520368286007
Experience 10, Iter 32, disc loss: 0.003270831642667891, policy loss: 6.369110154399262
Experience 10, Iter 33, disc loss: 0.0033204839974401903, policy loss: 6.373895273193696
Experience 10, Iter 34, disc loss: 0.003089484530489588, policy loss: 6.474474992784544
Experience 10, Iter 35, disc loss: 0.003343999053390635, policy loss: 6.283288859959638
Experience 10, Iter 36, disc loss: 0.0032221710595800334, policy loss: 6.382674530553393
Experience 10, Iter 37, disc loss: 0.003334561881965104, policy loss: 6.294918964069836
Experience 10, Iter 38, disc loss: 0.0035172746048801533, policy loss: 6.269038628885818
Experience 10, Iter 39, disc loss: 0.0031563552672420805, policy loss: 6.394525642322298
Experience 10, Iter 40, disc loss: 0.0030221735874854353, policy loss: 6.413142621154265
Experience 10, Iter 41, disc loss: 0.0031425340472889087, policy loss: 6.379864009450125
Experience 10, Iter 42, disc loss: 0.0032483426876314383, policy loss: 6.311653838123318
Experience 10, Iter 43, disc loss: 0.0030191349346659103, policy loss: 6.451147437516271
Experience 10, Iter 44, disc loss: 0.0031938518798687084, policy loss: 6.324825413032455
Experience 10, Iter 45, disc loss: 0.0030613201190061957, policy loss: 6.404740521221783
Experience 10, Iter 46, disc loss: 0.003137190878304401, policy loss: 6.338896066336977
Experience 10, Iter 47, disc loss: 0.002876705870720272, policy loss: 6.461662092378325
Experience 10, Iter 48, disc loss: 0.002569984871008373, policy loss: 6.637696411151664
Experience 10, Iter 49, disc loss: 0.0031352828054591023, policy loss: 6.359386078877516
Experience 10, Iter 50, disc loss: 0.002901688814589293, policy loss: 6.643163270966983
Experience 10, Iter 51, disc loss: 0.003133383366083176, policy loss: 6.373612116760405
Experience 10, Iter 52, disc loss: 0.003180113548094587, policy loss: 6.304313788091984
Experience 10, Iter 53, disc loss: 0.0029992269965508243, policy loss: 6.354854925375023
Experience 10, Iter 54, disc loss: 0.0028987443156058973, policy loss: 6.510138453277591
Experience 10, Iter 55, disc loss: 0.002987502328596148, policy loss: 6.399978888322076
Experience 10, Iter 56, disc loss: 0.0029118241056401385, policy loss: 6.372021496548311
Experience 10, Iter 57, disc loss: 0.0030215103361099943, policy loss: 6.275521100050336
Experience 10, Iter 58, disc loss: 0.0029958175278565246, policy loss: 6.336146778548813
Experience 10, Iter 59, disc loss: 0.0027787659145968903, policy loss: 6.558129163226887
Experience 10, Iter 60, disc loss: 0.002655286501393627, policy loss: 6.708459756042886
Experience 10, Iter 61, disc loss: 0.003117625112347135, policy loss: 6.277599022330104
Experience 10, Iter 62, disc loss: 0.00297235284140089, policy loss: 6.377154241633184
Experience 10, Iter 63, disc loss: 0.003108951625930058, policy loss: 6.291609313863229
Experience 10, Iter 64, disc loss: 0.002825920828091528, policy loss: 6.4618300388808265
Experience 10, Iter 65, disc loss: 0.002992485119509307, policy loss: 6.430576497182999
Experience 10, Iter 66, disc loss: 0.0028775461658012793, policy loss: 6.4300075155247445
Experience 10, Iter 67, disc loss: 0.0027953994458876953, policy loss: 6.558425954661407
Experience 10, Iter 68, disc loss: 0.0029230242330658537, policy loss: 6.407350897155501
Experience 10, Iter 69, disc loss: 0.0029949762724444592, policy loss: 6.3014945274371765
Experience 10, Iter 70, disc loss: 0.0029133919366834816, policy loss: 6.412161797197871
Experience 10, Iter 71, disc loss: 0.0028256425804353722, policy loss: 6.487838838712685
Experience 10, Iter 72, disc loss: 0.002717062943027292, policy loss: 6.484409543506795
Experience 10, Iter 73, disc loss: 0.0027615667844796046, policy loss: 6.463273689106202
Experience 10, Iter 74, disc loss: 0.0025622694973326046, policy loss: 6.586882961710834
Experience 10, Iter 75, disc loss: 0.0027413714067408973, policy loss: 6.586322496026552
Experience 10, Iter 76, disc loss: 0.0027502791615521154, policy loss: 6.4836265201368555
Experience 10, Iter 77, disc loss: 0.0024288021708135615, policy loss: 6.687308881480079
Experience 10, Iter 78, disc loss: 0.002923220806500906, policy loss: 6.411161467294991
Experience 10, Iter 79, disc loss: 0.002749613339682137, policy loss: 6.426721253674419
Experience 10, Iter 80, disc loss: 0.0026807890845960626, policy loss: 6.509172515375033
Experience 10, Iter 81, disc loss: 0.0027066171023291535, policy loss: 6.4950225613299555
Experience 10, Iter 82, disc loss: 0.002822501913279365, policy loss: 6.415956221508048
Experience 10, Iter 83, disc loss: 0.0025416329113966225, policy loss: 6.593426473305305
Experience 10, Iter 84, disc loss: 0.002870270472557544, policy loss: 6.543833949669919
Experience 10, Iter 85, disc loss: 0.002668668836660969, policy loss: 6.661540467471921
Experience 10, Iter 86, disc loss: 0.0026029850002510365, policy loss: 6.648754265244088
Experience 10, Iter 87, disc loss: 0.002687999402173969, policy loss: 6.536249449596275
Experience 10, Iter 88, disc loss: 0.0025900723084867397, policy loss: 6.6057220673362576
Experience 10, Iter 89, disc loss: 0.0026184305912941583, policy loss: 6.588627030859601
Experience 10, Iter 90, disc loss: 0.0025176395863290464, policy loss: 6.64310212542796
Experience 10, Iter 91, disc loss: 0.0022847268568706863, policy loss: 6.727363378790143
Experience 10, Iter 92, disc loss: 0.002583674036604526, policy loss: 6.52156422549485
Experience 10, Iter 93, disc loss: 0.0025999870936906765, policy loss: 6.565122749502645
Experience 10, Iter 94, disc loss: 0.0025848077981423957, policy loss: 6.5914468915061
Experience 10, Iter 95, disc loss: 0.0026031689145058884, policy loss: 6.590262917468412
Experience 10, Iter 96, disc loss: 0.0026903548100738438, policy loss: 6.454004723654695
Experience 10, Iter 97, disc loss: 0.002727237680101983, policy loss: 6.477768942330245
Experience 10, Iter 98, disc loss: 0.0026027117552663275, policy loss: 6.534763596581789
Experience 10, Iter 99, disc loss: 0.0023197649909352997, policy loss: 6.781628840397754
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0560],
        [0.5739],
        [0.0073]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.5384e-03, 6.0745e-02, 3.6259e-01, 1.0736e-02, 7.1783e-04,
          1.4783e+00]],

        [[9.5384e-03, 6.0745e-02, 3.6259e-01, 1.0736e-02, 7.1783e-04,
          1.4783e+00]],

        [[9.5384e-03, 6.0745e-02, 3.6259e-01, 1.0736e-02, 7.1783e-04,
          1.4783e+00]],

        [[9.5384e-03, 6.0745e-02, 3.6259e-01, 1.0736e-02, 7.1783e-04,
          1.4783e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0062, 0.2241, 2.2956, 0.0290], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0062, 0.2241, 2.2956, 0.0290])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.104
Iter 2/2000 - Loss: 1.566
Iter 3/2000 - Loss: 1.070
Iter 4/2000 - Loss: 1.120
Iter 5/2000 - Loss: 1.295
Iter 6/2000 - Loss: 1.200
Iter 7/2000 - Loss: 1.057
Iter 8/2000 - Loss: 1.038
Iter 9/2000 - Loss: 1.099
Iter 10/2000 - Loss: 1.128
Iter 11/2000 - Loss: 1.085
Iter 12/2000 - Loss: 1.014
Iter 13/2000 - Loss: 0.966
Iter 14/2000 - Loss: 0.951
Iter 15/2000 - Loss: 0.943
Iter 16/2000 - Loss: 0.905
Iter 17/2000 - Loss: 0.827
Iter 18/2000 - Loss: 0.729
Iter 19/2000 - Loss: 0.634
Iter 20/2000 - Loss: 0.544
Iter 1981/2000 - Loss: -8.119
Iter 1982/2000 - Loss: -8.119
Iter 1983/2000 - Loss: -8.119
Iter 1984/2000 - Loss: -8.119
Iter 1985/2000 - Loss: -8.119
Iter 1986/2000 - Loss: -8.119
Iter 1987/2000 - Loss: -8.119
Iter 1988/2000 - Loss: -8.119
Iter 1989/2000 - Loss: -8.119
Iter 1990/2000 - Loss: -8.119
Iter 1991/2000 - Loss: -8.119
Iter 1992/2000 - Loss: -8.119
Iter 1993/2000 - Loss: -8.119
Iter 1994/2000 - Loss: -8.119
Iter 1995/2000 - Loss: -8.119
Iter 1996/2000 - Loss: -8.119
Iter 1997/2000 - Loss: -8.119
Iter 1998/2000 - Loss: -8.119
Iter 1999/2000 - Loss: -8.119
Iter 2000/2000 - Loss: -8.119
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[13.0712,  4.4101, 39.9788,  6.9728,  7.7881, 32.8555]],

        [[22.6677, 37.9539, 10.9603,  1.2910,  6.9454, 34.4034]],

        [[23.1191, 38.1579,  9.5854,  1.0321,  6.0189, 26.9059]],

        [[17.7552, 32.7567, 18.8228,  4.8535,  0.8495, 43.6801]]])
Signal Variance: tensor([ 0.0396,  3.4754, 19.3799,  0.5427])
Estimated target variance: tensor([0.0062, 0.2241, 2.2956, 0.0290])
N: 110
Signal to noise ratio: tensor([ 11.5501, 106.7851, 108.0332,  46.3557])
Bound on condition number: tensor([  14675.5546, 1254336.7021, 1283828.7953,  236374.5671])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0025333165818144086, policy loss: 6.53085128301667
Experience 11, Iter 1, disc loss: 0.002425202211468739, policy loss: 6.726048467309502
Experience 11, Iter 2, disc loss: 0.0023959289573785157, policy loss: 6.832628040028463
Experience 11, Iter 3, disc loss: 0.0026153450912716134, policy loss: 6.53014209895993
Experience 11, Iter 4, disc loss: 0.0024757565952856974, policy loss: 6.735147925771836
Experience 11, Iter 5, disc loss: 0.0023499388387562413, policy loss: 6.652124144868132
Experience 11, Iter 6, disc loss: 0.002503966154058535, policy loss: 6.540221820141412
Experience 11, Iter 7, disc loss: 0.002433094124217043, policy loss: 6.5775509013185225
Experience 11, Iter 8, disc loss: 0.002497103355124567, policy loss: 6.578227403543443
Experience 11, Iter 9, disc loss: 0.0022553824737225267, policy loss: 6.733862028901817
Experience 11, Iter 10, disc loss: 0.0023273430218541816, policy loss: 6.591280253383475
Experience 11, Iter 11, disc loss: 0.00238574898369323, policy loss: 6.659679975552283
Experience 11, Iter 12, disc loss: 0.0023910963203200583, policy loss: 6.619416080956879
Experience 11, Iter 13, disc loss: 0.0024566523366559376, policy loss: 6.576213902556602
Experience 11, Iter 14, disc loss: 0.0023808136920386645, policy loss: 6.645779672089631
Experience 11, Iter 15, disc loss: 0.002361054753891606, policy loss: 6.600274926276274
Experience 11, Iter 16, disc loss: 0.002443588837604577, policy loss: 6.61722932333307
Experience 11, Iter 17, disc loss: 0.002351869894998338, policy loss: 6.5933537535791205
Experience 11, Iter 18, disc loss: 0.002336556229320017, policy loss: 6.714015126370566
Experience 11, Iter 19, disc loss: 0.0023159026244743072, policy loss: 6.680514197402459
Experience 11, Iter 20, disc loss: 0.0023193539701611044, policy loss: 6.626546340783393
Experience 11, Iter 21, disc loss: 0.0022997479384408654, policy loss: 6.87353212140909
Experience 11, Iter 22, disc loss: 0.0023061432153970696, policy loss: 6.721960282709478
Experience 11, Iter 23, disc loss: 0.002337490774970266, policy loss: 6.834494853874835
Experience 11, Iter 24, disc loss: 0.002451389189901286, policy loss: 6.635445653785263
Experience 11, Iter 25, disc loss: 0.0021511899776241687, policy loss: 6.797896610875352
Experience 11, Iter 26, disc loss: 0.0023667715668143335, policy loss: 6.625303196812354
Experience 11, Iter 27, disc loss: 0.0025174095902936165, policy loss: 6.509558650999226
Experience 11, Iter 28, disc loss: 0.0023402969969691175, policy loss: 6.822777800429938
Experience 11, Iter 29, disc loss: 0.0024622157892955077, policy loss: 6.668508775443891
Experience 11, Iter 30, disc loss: 0.0024769485975148064, policy loss: 6.492270769944727
Experience 11, Iter 31, disc loss: 0.0025191684674349935, policy loss: 6.607615933140876
Experience 11, Iter 32, disc loss: 0.002569903033245727, policy loss: 6.472992605812138
Experience 11, Iter 33, disc loss: 0.002342048938004844, policy loss: 6.678201506021667
Experience 11, Iter 34, disc loss: 0.0024240309098803913, policy loss: 6.731699860371365
Experience 11, Iter 35, disc loss: 0.002332407396548615, policy loss: 6.764827895267825
Experience 11, Iter 36, disc loss: 0.0024251067390509825, policy loss: 6.635977450389332
Experience 11, Iter 37, disc loss: 0.0023705922359866997, policy loss: 6.689083689288321
Experience 11, Iter 38, disc loss: 0.0024880318734483322, policy loss: 6.66543764290474
Experience 11, Iter 39, disc loss: 0.0024242339305381414, policy loss: 6.729364247412537
Experience 11, Iter 40, disc loss: 0.002629324985124785, policy loss: 6.509883434560001
Experience 11, Iter 41, disc loss: 0.002296072101033594, policy loss: 6.821659647186575
Experience 11, Iter 42, disc loss: 0.0024622179994553345, policy loss: 6.623481901933749
Experience 11, Iter 43, disc loss: 0.002663458904769406, policy loss: 6.467244116803951
Experience 11, Iter 44, disc loss: 0.002459660769404826, policy loss: 6.529895144634535
Experience 11, Iter 45, disc loss: 0.002506872254379865, policy loss: 6.6480541115376575
Experience 11, Iter 46, disc loss: 0.002706081979675143, policy loss: 6.544500857331071
Experience 11, Iter 47, disc loss: 0.0025508168223803912, policy loss: 6.539383529548569
Experience 11, Iter 48, disc loss: 0.002594329439239939, policy loss: 6.628290234306783
Experience 11, Iter 49, disc loss: 0.002656429982106391, policy loss: 6.439128912995339
Experience 11, Iter 50, disc loss: 0.002901356960839746, policy loss: 6.407863042680747
Experience 11, Iter 51, disc loss: 0.0027311013012587693, policy loss: 6.550475797882676
Experience 11, Iter 52, disc loss: 0.0028857553337784912, policy loss: 6.412631646668026
Experience 11, Iter 53, disc loss: 0.003870569821908648, policy loss: 5.983384246527054
Experience 11, Iter 54, disc loss: 0.007324400512215623, policy loss: 5.856016927576162
Experience 11, Iter 55, disc loss: 0.012123756826540988, policy loss: 5.38730632031027
Experience 11, Iter 56, disc loss: 0.03409099028560855, policy loss: 4.961336902036737
Experience 11, Iter 57, disc loss: 0.06495479472061755, policy loss: 5.06541079937131
Experience 11, Iter 58, disc loss: 0.02790665007594695, policy loss: 5.603482021189092
Experience 11, Iter 59, disc loss: 0.033026437356299024, policy loss: 6.26683156718993
Experience 11, Iter 60, disc loss: 0.029577111579368404, policy loss: 6.760279935739527
Experience 11, Iter 61, disc loss: 0.018445166081710672, policy loss: 7.6990978876488905
Experience 11, Iter 62, disc loss: 0.033482830458584224, policy loss: 7.797948392237212
Experience 11, Iter 63, disc loss: 0.026509436526460893, policy loss: 8.047446179782641
Experience 11, Iter 64, disc loss: 0.04182138439167638, policy loss: 8.034540156284702
Experience 11, Iter 65, disc loss: 0.04247205625495641, policy loss: 7.4736311620043185
Experience 11, Iter 66, disc loss: 0.03916775953681187, policy loss: 8.05405020795293
Experience 11, Iter 67, disc loss: 0.03714594143066111, policy loss: 7.000672064089974
Experience 11, Iter 68, disc loss: 0.032644628443974553, policy loss: 7.119700770281931
Experience 11, Iter 69, disc loss: 0.02561062529375929, policy loss: 7.095160405028023
Experience 11, Iter 70, disc loss: 0.01952042148594704, policy loss: 6.559716396397864
Experience 11, Iter 71, disc loss: 0.01618429595585201, policy loss: 6.396500892115506
Experience 11, Iter 72, disc loss: 0.016414875842365444, policy loss: 5.773096470901988
Experience 11, Iter 73, disc loss: 0.012708410463260163, policy loss: 6.355433078160045
Experience 11, Iter 74, disc loss: 0.02001747026462939, policy loss: 5.205660183801301
Experience 11, Iter 75, disc loss: 0.03382285414210298, policy loss: 5.30011472188452
Experience 11, Iter 76, disc loss: 0.027167988295212612, policy loss: 5.3770793514623145
Experience 11, Iter 77, disc loss: 0.026743262783468776, policy loss: 5.191725917849328
Experience 11, Iter 78, disc loss: 0.01788770653327093, policy loss: 6.186236674500387
Experience 11, Iter 79, disc loss: 0.02322444115139319, policy loss: 5.327510483047647
Experience 11, Iter 80, disc loss: 0.02458353936227793, policy loss: 5.885058598489188
Experience 11, Iter 81, disc loss: 0.01987090794133701, policy loss: 5.800283084918807
Experience 11, Iter 82, disc loss: 0.013912887328417068, policy loss: 6.671554132888537
Experience 11, Iter 83, disc loss: 0.012376797888844352, policy loss: 6.733428744609922
Experience 11, Iter 84, disc loss: 0.02901545711608901, policy loss: 6.194149755744768
Experience 11, Iter 85, disc loss: 0.02364909100585428, policy loss: 6.656517332139846
Experience 11, Iter 86, disc loss: 0.02354138745583923, policy loss: 6.743287212350193
Experience 11, Iter 87, disc loss: 0.018727372472639656, policy loss: 6.795786257593957
Experience 11, Iter 88, disc loss: 0.017535549005528877, policy loss: 7.016770093440204
Experience 11, Iter 89, disc loss: 0.0197024962653477, policy loss: 6.440951858205199
Experience 11, Iter 90, disc loss: 0.021653638793217612, policy loss: 7.0327102203037715
Experience 11, Iter 91, disc loss: 0.019339548397236145, policy loss: 6.776914970385686
Experience 11, Iter 92, disc loss: 0.025932298831459734, policy loss: 6.6102752203293225
Experience 11, Iter 93, disc loss: 0.01484764953028834, policy loss: 6.975009937685908
Experience 11, Iter 94, disc loss: 0.018006319900366233, policy loss: 6.464158026110317
Experience 11, Iter 95, disc loss: 0.018680966308286126, policy loss: 6.440117424223652
Experience 11, Iter 96, disc loss: 0.020618755391605886, policy loss: 6.013020079958971
Experience 11, Iter 97, disc loss: 0.012851231109939812, policy loss: 6.752764176080434
Experience 11, Iter 98, disc loss: 0.014137612926571928, policy loss: 6.593441208052493
Experience 11, Iter 99, disc loss: 0.01828048855290977, policy loss: 5.982991302942914
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0755],
        [0.7498],
        [0.0112]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0091, 0.0720, 0.5496, 0.0130, 0.0044, 1.8763]],

        [[0.0091, 0.0720, 0.5496, 0.0130, 0.0044, 1.8763]],

        [[0.0091, 0.0720, 0.5496, 0.0130, 0.0044, 1.8763]],

        [[0.0091, 0.0720, 0.5496, 0.0130, 0.0044, 1.8763]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0069, 0.3020, 2.9993, 0.0446], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0069, 0.3020, 2.9993, 0.0446])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.652
Iter 2/2000 - Loss: 2.050
Iter 3/2000 - Loss: 1.619
Iter 4/2000 - Loss: 1.655
Iter 5/2000 - Loss: 1.805
Iter 6/2000 - Loss: 1.733
Iter 7/2000 - Loss: 1.593
Iter 8/2000 - Loss: 1.546
Iter 9/2000 - Loss: 1.584
Iter 10/2000 - Loss: 1.610
Iter 11/2000 - Loss: 1.566
Iter 12/2000 - Loss: 1.477
Iter 13/2000 - Loss: 1.392
Iter 14/2000 - Loss: 1.331
Iter 15/2000 - Loss: 1.280
Iter 16/2000 - Loss: 1.204
Iter 17/2000 - Loss: 1.085
Iter 18/2000 - Loss: 0.934
Iter 19/2000 - Loss: 0.770
Iter 20/2000 - Loss: 0.606
Iter 1981/2000 - Loss: -7.773
Iter 1982/2000 - Loss: -7.773
Iter 1983/2000 - Loss: -7.773
Iter 1984/2000 - Loss: -7.773
Iter 1985/2000 - Loss: -7.773
Iter 1986/2000 - Loss: -7.773
Iter 1987/2000 - Loss: -7.773
Iter 1988/2000 - Loss: -7.773
Iter 1989/2000 - Loss: -7.773
Iter 1990/2000 - Loss: -7.773
Iter 1991/2000 - Loss: -7.773
Iter 1992/2000 - Loss: -7.773
Iter 1993/2000 - Loss: -7.773
Iter 1994/2000 - Loss: -7.773
Iter 1995/2000 - Loss: -7.773
Iter 1996/2000 - Loss: -7.773
Iter 1997/2000 - Loss: -7.774
Iter 1998/2000 - Loss: -7.774
Iter 1999/2000 - Loss: -7.774
Iter 2000/2000 - Loss: -7.774
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.2472,  4.2108, 39.8080,  6.7962,  2.1250, 41.2400]],

        [[16.7542, 33.0087, 10.2759,  1.1979,  6.9614, 15.5317]],

        [[18.0022, 32.8135,  9.9173,  1.0519,  1.8170, 20.8367]],

        [[16.5295, 27.6150, 15.7791,  1.0236, 10.7805, 38.9145]]])
Signal Variance: tensor([ 0.0403,  1.7580, 14.4780,  0.4808])
Estimated target variance: tensor([0.0069, 0.3020, 2.9993, 0.0446])
N: 120
Signal to noise ratio: tensor([11.6319, 78.7100, 87.6535, 44.0075])
Bound on condition number: tensor([ 16237.1525, 743433.5261, 921977.1965, 232399.7945])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.184184713021667, policy loss: 2.6326328273507498
Experience 12, Iter 1, disc loss: 0.1230055979070483, policy loss: 3.1189326915965716
Experience 12, Iter 2, disc loss: 0.09980257987107927, policy loss: 3.6309080543236507
Experience 12, Iter 3, disc loss: 0.07305797998129801, policy loss: 4.403055343027156
Experience 12, Iter 4, disc loss: 0.07914365247380893, policy loss: 4.609498098208277
Experience 12, Iter 5, disc loss: 0.10742397791317033, policy loss: 4.669920917905379
Experience 12, Iter 6, disc loss: 0.12278852296117103, policy loss: 4.784030669990015
Experience 12, Iter 7, disc loss: 0.1172088886624042, policy loss: 4.436843562921546
Experience 12, Iter 8, disc loss: 0.09781886071498225, policy loss: 3.851183595896897
Experience 12, Iter 9, disc loss: 0.08469668084121484, policy loss: 3.334921947556236
Experience 12, Iter 10, disc loss: 0.08321368271134448, policy loss: 3.140581163249146
Experience 12, Iter 11, disc loss: 0.09000600147377774, policy loss: 3.2128545589624626
Experience 12, Iter 12, disc loss: 0.10614703531220213, policy loss: 2.706585443292948
Experience 12, Iter 13, disc loss: 0.12388623736092316, policy loss: 2.7640566917191607
Experience 12, Iter 14, disc loss: 0.10979930218300044, policy loss: 3.026543717463084
Experience 12, Iter 15, disc loss: 0.08205806593817974, policy loss: 3.5831656633777325
Experience 12, Iter 16, disc loss: 0.08343963517985128, policy loss: 3.8749287408875226
Experience 12, Iter 17, disc loss: 0.08707986969393185, policy loss: 4.356174374784905
Experience 12, Iter 18, disc loss: 0.08520261135848148, policy loss: 4.165695847061869
Experience 12, Iter 19, disc loss: 0.09462068970712582, policy loss: 4.029027462000987
Experience 12, Iter 20, disc loss: 0.09059773079209163, policy loss: 4.43634188603622
Experience 12, Iter 21, disc loss: 0.09022482922881842, policy loss: 4.0450940176407
Experience 12, Iter 22, disc loss: 0.07764544385925709, policy loss: 4.502340988802718
Experience 12, Iter 23, disc loss: 0.07438234602458194, policy loss: 4.195582494987916
Experience 12, Iter 24, disc loss: 0.07421733821293766, policy loss: 3.749849978475418
Experience 12, Iter 25, disc loss: 0.07859667816276102, policy loss: 4.538735901162258
Experience 12, Iter 26, disc loss: 0.07729267337355136, policy loss: 4.576371044430894
Experience 12, Iter 27, disc loss: 0.08491934826515704, policy loss: 3.9266142595454285
Experience 12, Iter 28, disc loss: 0.073593257669449, policy loss: 3.524604917513425
Experience 12, Iter 29, disc loss: 0.06280492381302835, policy loss: 4.144185320828028
Experience 12, Iter 30, disc loss: 0.07106977275525726, policy loss: 3.8968253805692212
Experience 12, Iter 31, disc loss: 0.06621785184362297, policy loss: 3.93886042487011
Experience 12, Iter 32, disc loss: 0.06800823031958975, policy loss: 4.876332110924835
Experience 12, Iter 33, disc loss: 0.06816358225603605, policy loss: 4.453981488977133
Experience 12, Iter 34, disc loss: 0.06059552947326896, policy loss: 4.428093488953655
Experience 12, Iter 35, disc loss: 0.06099981349941068, policy loss: 4.189545946414137
Experience 12, Iter 36, disc loss: 0.05730973259815478, policy loss: 4.303509543700615
Experience 12, Iter 37, disc loss: 0.056172342843752915, policy loss: 4.127553858176045
Experience 12, Iter 38, disc loss: 0.05179042903693527, policy loss: 4.2316285561593805
Experience 12, Iter 39, disc loss: 0.05712807787827393, policy loss: 4.2160306508691745
Experience 12, Iter 40, disc loss: 0.06713135722521271, policy loss: 3.7729512270688357
Experience 12, Iter 41, disc loss: 0.05878059583453331, policy loss: 3.808403378802437
Experience 12, Iter 42, disc loss: 0.04852543064067663, policy loss: 4.124751110493082
Experience 12, Iter 43, disc loss: 0.049704606590458174, policy loss: 4.219163107614696
Experience 12, Iter 44, disc loss: 0.044589808628675856, policy loss: 4.285804128883678
Experience 12, Iter 45, disc loss: 0.04766512957413217, policy loss: 4.303740823963166
Experience 12, Iter 46, disc loss: 0.04363686963486091, policy loss: 4.391784435931208
Experience 12, Iter 47, disc loss: 0.03931935236913589, policy loss: 5.00444958287067
Experience 12, Iter 48, disc loss: 0.04392330694700575, policy loss: 4.613131469975697
Experience 12, Iter 49, disc loss: 0.04317535184046932, policy loss: 4.494731788171671
Experience 12, Iter 50, disc loss: 0.042775848916928275, policy loss: 4.475594722951234
Experience 12, Iter 51, disc loss: 0.04156904144143478, policy loss: 4.386315757219706
Experience 12, Iter 52, disc loss: 0.044599286943998555, policy loss: 4.471666337379067
Experience 12, Iter 53, disc loss: 0.03739438614038891, policy loss: 4.590978915420993
Experience 12, Iter 54, disc loss: 0.037305301664969655, policy loss: 4.408541262953015
Experience 12, Iter 55, disc loss: 0.03611968640765235, policy loss: 4.492879997526716
Experience 12, Iter 56, disc loss: 0.03694329884107159, policy loss: 4.609012144046147
Experience 12, Iter 57, disc loss: 0.03793869180408958, policy loss: 4.685666123875567
Experience 12, Iter 58, disc loss: 0.03865587689802576, policy loss: 4.567010142546334
Experience 12, Iter 59, disc loss: 0.03124081734465091, policy loss: 4.9304966739469
Experience 12, Iter 60, disc loss: 0.030664909157098623, policy loss: 4.772626069261286
Experience 12, Iter 61, disc loss: 0.0330882307992299, policy loss: 4.691994328197377
Experience 12, Iter 62, disc loss: 0.03175368774252371, policy loss: 4.736298128327105
Experience 12, Iter 63, disc loss: 0.0370405132755811, policy loss: 4.34423557247727
Experience 12, Iter 64, disc loss: 0.03340945386621605, policy loss: 4.812952233044422
Experience 12, Iter 65, disc loss: 0.03051486071936306, policy loss: 4.693667451036192
Experience 12, Iter 66, disc loss: 0.03338580888561157, policy loss: 5.051946173453102
Experience 12, Iter 67, disc loss: 0.029716544777098244, policy loss: 4.813055913273285
Experience 12, Iter 68, disc loss: 0.030322289707880996, policy loss: 4.771071742412223
Experience 12, Iter 69, disc loss: 0.03295160557060656, policy loss: 4.5879499174560125
Experience 12, Iter 70, disc loss: 0.03081975100121182, policy loss: 4.83150156770202
Experience 12, Iter 71, disc loss: 0.028109487784161974, policy loss: 5.2201201433386215
Experience 12, Iter 72, disc loss: 0.03031003625976039, policy loss: 4.980092992942194
Experience 12, Iter 73, disc loss: 0.028027257122519772, policy loss: 5.02620518444615
Experience 12, Iter 74, disc loss: 0.03049728976022492, policy loss: 4.625093839934914
Experience 12, Iter 75, disc loss: 0.03020498911316051, policy loss: 5.034862564176198
Experience 12, Iter 76, disc loss: 0.027178407597136245, policy loss: 5.135766687475089
Experience 12, Iter 77, disc loss: 0.023045898912766295, policy loss: 5.305777877752044
Experience 12, Iter 78, disc loss: 0.02579238814017146, policy loss: 4.970882203811391
Experience 12, Iter 79, disc loss: 0.025891759171976426, policy loss: 4.970819568315265
Experience 12, Iter 80, disc loss: 0.02420976295776245, policy loss: 4.890278203469129
Experience 12, Iter 81, disc loss: 0.02831922643576123, policy loss: 4.609561828090304
Experience 12, Iter 82, disc loss: 0.0302630864772113, policy loss: 4.8179695626721335
Experience 12, Iter 83, disc loss: 0.024929339670270716, policy loss: 4.837302982650282
Experience 12, Iter 84, disc loss: 0.026863893849536323, policy loss: 4.8079714363697645
Experience 12, Iter 85, disc loss: 0.023837752234298463, policy loss: 5.009277186650143
Experience 12, Iter 86, disc loss: 0.022892244593203097, policy loss: 5.018395567315425
Experience 12, Iter 87, disc loss: 0.02557535428397211, policy loss: 4.9304110208898955
Experience 12, Iter 88, disc loss: 0.02569864657733512, policy loss: 5.28614871292668
Experience 12, Iter 89, disc loss: 0.02309968049091412, policy loss: 5.408496183668277
Experience 12, Iter 90, disc loss: 0.021068059145199335, policy loss: 5.431151963002307
Experience 12, Iter 91, disc loss: 0.023236768359881595, policy loss: 5.187331261342276
Experience 12, Iter 92, disc loss: 0.025398921233640445, policy loss: 5.111042504936972
Experience 12, Iter 93, disc loss: 0.02346283974944153, policy loss: 5.303540234683528
Experience 12, Iter 94, disc loss: 0.020309468069076672, policy loss: 5.666748610696258
Experience 12, Iter 95, disc loss: 0.024874241017749238, policy loss: 4.992247011910173
Experience 12, Iter 96, disc loss: 0.023995553294598296, policy loss: 4.938166602853371
Experience 12, Iter 97, disc loss: 0.02255304824779694, policy loss: 5.425941277801403
Experience 12, Iter 98, disc loss: 0.02130358200295588, policy loss: 5.389934636371703
Experience 12, Iter 99, disc loss: 0.019148588930993773, policy loss: 5.515054419047432
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0968],
        [0.8855],
        [0.0149]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0088, 0.0818, 0.7101, 0.0146, 0.0079, 2.3520]],

        [[0.0088, 0.0818, 0.7101, 0.0146, 0.0079, 2.3520]],

        [[0.0088, 0.0818, 0.7101, 0.0146, 0.0079, 2.3520]],

        [[0.0088, 0.0818, 0.7101, 0.0146, 0.0079, 2.3520]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0076, 0.3872, 3.5419, 0.0594], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0076, 0.3872, 3.5419, 0.0594])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.037
Iter 2/2000 - Loss: 2.386
Iter 3/2000 - Loss: 1.993
Iter 4/2000 - Loss: 2.024
Iter 5/2000 - Loss: 2.137
Iter 6/2000 - Loss: 2.072
Iter 7/2000 - Loss: 1.945
Iter 8/2000 - Loss: 1.875
Iter 9/2000 - Loss: 1.879
Iter 10/2000 - Loss: 1.889
Iter 11/2000 - Loss: 1.841
Iter 12/2000 - Loss: 1.736
Iter 13/2000 - Loss: 1.615
Iter 14/2000 - Loss: 1.511
Iter 15/2000 - Loss: 1.424
Iter 16/2000 - Loss: 1.325
Iter 17/2000 - Loss: 1.190
Iter 18/2000 - Loss: 1.016
Iter 19/2000 - Loss: 0.819
Iter 20/2000 - Loss: 0.616
Iter 1981/2000 - Loss: -7.740
Iter 1982/2000 - Loss: -7.740
Iter 1983/2000 - Loss: -7.740
Iter 1984/2000 - Loss: -7.740
Iter 1985/2000 - Loss: -7.740
Iter 1986/2000 - Loss: -7.740
Iter 1987/2000 - Loss: -7.740
Iter 1988/2000 - Loss: -7.740
Iter 1989/2000 - Loss: -7.740
Iter 1990/2000 - Loss: -7.740
Iter 1991/2000 - Loss: -7.740
Iter 1992/2000 - Loss: -7.740
Iter 1993/2000 - Loss: -7.740
Iter 1994/2000 - Loss: -7.740
Iter 1995/2000 - Loss: -7.740
Iter 1996/2000 - Loss: -7.740
Iter 1997/2000 - Loss: -7.740
Iter 1998/2000 - Loss: -7.740
Iter 1999/2000 - Loss: -7.740
Iter 2000/2000 - Loss: -7.740
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.9754,  4.5288, 44.0603,  7.8367,  8.8686, 45.2518]],

        [[15.3798, 32.1871, 10.3263,  1.1995,  7.5645, 15.3367]],

        [[17.5943, 30.7185, 11.0797,  1.1049,  1.1201, 20.2382]],

        [[14.6123, 27.4159, 16.2543,  1.6909,  1.7666, 33.6940]]])
Signal Variance: tensor([ 0.0411,  1.6300, 17.6120,  0.5622])
Estimated target variance: tensor([0.0076, 0.3872, 3.5419, 0.0594])
N: 130
Signal to noise ratio: tensor([11.3697, 76.3159, 97.3817, 46.5056])
Bound on condition number: tensor([  16806.0648,  757135.4766, 1232817.4443,  281160.6718])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0833026493399617, policy loss: 3.0182662536468925
Experience 13, Iter 1, disc loss: 0.06810098345121435, policy loss: 3.545062014997641
Experience 13, Iter 2, disc loss: 0.06503459639914975, policy loss: 3.4656976380714033
Experience 13, Iter 3, disc loss: 0.05718359891449729, policy loss: 3.9911305066622016
Experience 13, Iter 4, disc loss: 0.05596971543492547, policy loss: 4.101712494209888
Experience 13, Iter 5, disc loss: 0.056352871622913164, policy loss: 4.506861693457086
Experience 13, Iter 6, disc loss: 0.06119550396110563, policy loss: 4.278560029120706
Experience 13, Iter 7, disc loss: 0.0590481298333794, policy loss: 5.176244740960457
Experience 13, Iter 8, disc loss: 0.05884643610681141, policy loss: 4.391378654594836
Experience 13, Iter 9, disc loss: 0.05007162199700278, policy loss: 4.687571636694078
Experience 13, Iter 10, disc loss: 0.04550343538799918, policy loss: 4.146929918914186
Experience 13, Iter 11, disc loss: 0.04410314720280489, policy loss: 3.870929659053477
Experience 13, Iter 12, disc loss: 0.03865105314858214, policy loss: 4.021966855023207
Experience 13, Iter 13, disc loss: 0.044773082501498966, policy loss: 4.054990235538724
Experience 13, Iter 14, disc loss: 0.045155946080803776, policy loss: 4.5645774404998445
Experience 13, Iter 15, disc loss: 0.04172002606339629, policy loss: 4.2675269133262415
Experience 13, Iter 16, disc loss: 0.04404087394032945, policy loss: 3.9743665988653842
Experience 13, Iter 17, disc loss: 0.02929507212637681, policy loss: 4.621523402518685
Experience 13, Iter 18, disc loss: 0.02416301553484694, policy loss: 4.855748600702686
Experience 13, Iter 19, disc loss: 0.022342065549264333, policy loss: 4.900260774954953
Experience 13, Iter 20, disc loss: 0.02029653784022567, policy loss: 4.958138961506637
Experience 13, Iter 21, disc loss: 0.020099679711512823, policy loss: 5.312348277177468
Experience 13, Iter 22, disc loss: 0.02228854989706746, policy loss: 4.906564069605128
Experience 13, Iter 23, disc loss: 0.02459620388665005, policy loss: 4.756765052259899
Experience 13, Iter 24, disc loss: 0.02823571660461563, policy loss: 4.472409614270093
Experience 13, Iter 25, disc loss: 0.0327911059707282, policy loss: 4.6770929484245
Experience 13, Iter 26, disc loss: 0.017293696888812826, policy loss: 10.008826672532377
Experience 13, Iter 27, disc loss: 0.025478663317904597, policy loss: 5.214807214099664
Experience 13, Iter 28, disc loss: 0.02050103776171679, policy loss: 5.126285880333311
Experience 13, Iter 29, disc loss: 0.015800561478064586, policy loss: 5.8672062342134
Experience 13, Iter 30, disc loss: 0.01420461756800504, policy loss: 6.061199409642485
Experience 13, Iter 31, disc loss: 0.012915591907531925, policy loss: 6.64514126705611
Experience 13, Iter 32, disc loss: 0.01172997118592568, policy loss: 6.168402321976907
Experience 13, Iter 33, disc loss: 0.010607077611461856, policy loss: 6.525136379802088
Experience 13, Iter 34, disc loss: 0.009986691318706753, policy loss: 6.278669191854522
Experience 13, Iter 35, disc loss: 0.00924217243749345, policy loss: 6.2617710634486565
Experience 13, Iter 36, disc loss: 0.008965403817749306, policy loss: 6.189836704051194
Experience 13, Iter 37, disc loss: 0.009423459623627322, policy loss: 5.679179860894766
Experience 13, Iter 38, disc loss: 0.010259788291608516, policy loss: 5.343117453370622
Experience 13, Iter 39, disc loss: 0.011191085359940699, policy loss: 5.256142020083773
Experience 13, Iter 40, disc loss: 0.018453832335410956, policy loss: 4.3309544282005135
Experience 13, Iter 41, disc loss: 0.02481630659961261, policy loss: 4.213227059947162
Experience 13, Iter 42, disc loss: 0.027596687967660613, policy loss: 4.424110823318234
Experience 13, Iter 43, disc loss: 0.02047981214158479, policy loss: 5.361022210525542
Experience 13, Iter 44, disc loss: 0.028665825337967892, policy loss: 4.137180388669908
Experience 13, Iter 45, disc loss: 0.019525694352303967, policy loss: 5.717322890411332
Experience 13, Iter 46, disc loss: 0.015476533326954714, policy loss: 5.276683895132705
Experience 13, Iter 47, disc loss: 0.012861974867372521, policy loss: 5.682259127963116
Experience 13, Iter 48, disc loss: 0.012947541050047931, policy loss: 5.609420233738877
Experience 13, Iter 49, disc loss: 0.01205589127816854, policy loss: 6.724632163017997
Experience 13, Iter 50, disc loss: 0.01271170612782507, policy loss: 5.919422855935592
Experience 13, Iter 51, disc loss: 0.01331632771144527, policy loss: 5.947506533550706
Experience 13, Iter 52, disc loss: 0.01400662276775155, policy loss: 6.0004032163074505
Experience 13, Iter 53, disc loss: 0.015931416694578957, policy loss: 5.595731857215622
Experience 13, Iter 54, disc loss: 0.017374216214339537, policy loss: 5.143111449264363
Experience 13, Iter 55, disc loss: 0.018628130059386815, policy loss: 5.202228356282945
Experience 13, Iter 56, disc loss: 0.01595046844308066, policy loss: 5.998363706267391
Experience 13, Iter 57, disc loss: 0.017414663907021857, policy loss: 5.403123024529362
Experience 13, Iter 58, disc loss: 0.01439662527285437, policy loss: 5.6867373585263685
Experience 13, Iter 59, disc loss: 0.012255114891114801, policy loss: 6.508490679752816
Experience 13, Iter 60, disc loss: 0.011002162486262991, policy loss: 6.145060381400219
Experience 13, Iter 61, disc loss: 0.010347637042452337, policy loss: 6.386601993245964
Experience 13, Iter 62, disc loss: 0.00948357925475614, policy loss: 6.603789252259732
Experience 13, Iter 63, disc loss: 0.009468579015744489, policy loss: 6.184028496295777
Experience 13, Iter 64, disc loss: 0.0085452695227014, policy loss: 6.876043166829291
Experience 13, Iter 65, disc loss: 0.008995400342517477, policy loss: 6.4702890115765275
Experience 13, Iter 66, disc loss: 0.009572005104691372, policy loss: 6.517262731713753
Experience 13, Iter 67, disc loss: 0.010662232709981376, policy loss: 5.4966524127202305
Experience 13, Iter 68, disc loss: 0.012962565479857992, policy loss: 5.212712824480784
Experience 13, Iter 69, disc loss: 0.01519630863939037, policy loss: 5.175686598936474
Experience 13, Iter 70, disc loss: 0.021222658427272445, policy loss: 4.390495108570782
Experience 13, Iter 71, disc loss: 0.011263172045658184, policy loss: 5.7110696124448985
Experience 13, Iter 72, disc loss: 0.017505171242578027, policy loss: 4.7886204758152875
Experience 13, Iter 73, disc loss: 0.01316571438930576, policy loss: 5.3823812527027695
Experience 13, Iter 74, disc loss: 0.01052977904478599, policy loss: 5.465031387816911
Experience 13, Iter 75, disc loss: 0.009607731390628703, policy loss: 5.6196466257497075
Experience 13, Iter 76, disc loss: 0.009765003362209108, policy loss: 5.888563333359243
Experience 13, Iter 77, disc loss: 0.009530242493735713, policy loss: 6.366914247114966
Experience 13, Iter 78, disc loss: 0.009512621936528985, policy loss: 6.327010866740482
Experience 13, Iter 79, disc loss: 0.010398679030128657, policy loss: 5.786818633572276
Experience 13, Iter 80, disc loss: 0.011336881710378085, policy loss: 5.985930311275258
Experience 13, Iter 81, disc loss: 0.012443217901188826, policy loss: 5.639992745511102
Experience 13, Iter 82, disc loss: 0.013856778115390874, policy loss: 5.3226287310907185
Experience 13, Iter 83, disc loss: 0.01599142347138813, policy loss: 5.048229063725163
Experience 13, Iter 84, disc loss: 0.014609282069822711, policy loss: 6.020501873945532
Experience 13, Iter 85, disc loss: 0.013875092211041436, policy loss: 5.788297759909254
Experience 13, Iter 86, disc loss: 0.01314148760699381, policy loss: 5.387421661639462
Experience 13, Iter 87, disc loss: 0.011611199109438683, policy loss: 5.987381970827204
Experience 13, Iter 88, disc loss: 0.011375973880466236, policy loss: 5.575825255637574
Experience 13, Iter 89, disc loss: 0.011344656331963018, policy loss: 5.744232710601776
Experience 13, Iter 90, disc loss: 0.01245892176925983, policy loss: 5.623312844349711
Experience 13, Iter 91, disc loss: 0.013895802478382149, policy loss: 5.519414845601978
Experience 13, Iter 92, disc loss: 0.01241797010369458, policy loss: 5.6472263260419435
Experience 13, Iter 93, disc loss: 0.014655590844719877, policy loss: 5.325648172731272
Experience 13, Iter 94, disc loss: 0.01484583891483804, policy loss: 5.7235737386217345
Experience 13, Iter 95, disc loss: 0.014941352514212223, policy loss: 5.359487818687238
Experience 13, Iter 96, disc loss: 0.016395602049807875, policy loss: 5.280605839157939
Experience 13, Iter 97, disc loss: 0.01289740690245749, policy loss: 7.222511709241111
Experience 13, Iter 98, disc loss: 0.010840461751423369, policy loss: 5.907360643889721
Experience 13, Iter 99, disc loss: 0.009472768905170091, policy loss: 6.080748520010244
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.1127],
        [0.9893],
        [0.0173]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0085, 0.0880, 0.8241, 0.0165, 0.0112, 2.7428]],

        [[0.0085, 0.0880, 0.8241, 0.0165, 0.0112, 2.7428]],

        [[0.0085, 0.0880, 0.8241, 0.0165, 0.0112, 2.7428]],

        [[0.0085, 0.0880, 0.8241, 0.0165, 0.0112, 2.7428]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.4509, 3.9574, 0.0691], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.4509, 3.9574, 0.0691])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.291
Iter 2/2000 - Loss: 2.606
Iter 3/2000 - Loss: 2.238
Iter 4/2000 - Loss: 2.277
Iter 5/2000 - Loss: 2.374
Iter 6/2000 - Loss: 2.299
Iter 7/2000 - Loss: 2.182
Iter 8/2000 - Loss: 2.117
Iter 9/2000 - Loss: 2.111
Iter 10/2000 - Loss: 2.106
Iter 11/2000 - Loss: 2.049
Iter 12/2000 - Loss: 1.943
Iter 13/2000 - Loss: 1.821
Iter 14/2000 - Loss: 1.709
Iter 15/2000 - Loss: 1.608
Iter 16/2000 - Loss: 1.498
Iter 17/2000 - Loss: 1.356
Iter 18/2000 - Loss: 1.178
Iter 19/2000 - Loss: 0.976
Iter 20/2000 - Loss: 0.763
Iter 1981/2000 - Loss: -7.719
Iter 1982/2000 - Loss: -7.719
Iter 1983/2000 - Loss: -7.719
Iter 1984/2000 - Loss: -7.719
Iter 1985/2000 - Loss: -7.719
Iter 1986/2000 - Loss: -7.719
Iter 1987/2000 - Loss: -7.719
Iter 1988/2000 - Loss: -7.719
Iter 1989/2000 - Loss: -7.719
Iter 1990/2000 - Loss: -7.719
Iter 1991/2000 - Loss: -7.719
Iter 1992/2000 - Loss: -7.719
Iter 1993/2000 - Loss: -7.719
Iter 1994/2000 - Loss: -7.719
Iter 1995/2000 - Loss: -7.719
Iter 1996/2000 - Loss: -7.719
Iter 1997/2000 - Loss: -7.719
Iter 1998/2000 - Loss: -7.720
Iter 1999/2000 - Loss: -7.720
Iter 2000/2000 - Loss: -7.720
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[11.4442,  4.3688, 42.5369,  7.4121,  4.2741, 45.9812]],

        [[15.9924, 30.4774,  8.6627,  1.2660,  3.6892, 17.9838]],

        [[18.0287, 29.9533,  9.1577,  1.1673,  1.2500, 22.6900]],

        [[13.1796, 23.1475, 11.3974,  1.3749,  1.6759, 36.4874]]])
Signal Variance: tensor([ 0.0427,  1.6637, 18.8384,  0.3326])
Estimated target variance: tensor([0.0082, 0.4509, 3.9574, 0.0691])
N: 140
Signal to noise ratio: tensor([ 11.7477,  77.8834, 103.1990,  35.2597])
Bound on condition number: tensor([  19322.3223,  849216.3144, 1491006.6278,  174055.2900])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.007758558087686126, policy loss: 7.47630100547248
Experience 14, Iter 1, disc loss: 0.0071739754349305985, policy loss: 7.786891772967124
Experience 14, Iter 2, disc loss: 0.006774002969647574, policy loss: 7.249054127534314
Experience 14, Iter 3, disc loss: 0.006396439862622662, policy loss: 7.169302083385264
Experience 14, Iter 4, disc loss: 0.0059600552035644445, policy loss: 7.591334554512733
Experience 14, Iter 5, disc loss: 0.005656311241811232, policy loss: 7.4856299170480405
Experience 14, Iter 6, disc loss: 0.0054670242902784055, policy loss: 7.263475073060198
Experience 14, Iter 7, disc loss: 0.005093164910220794, policy loss: 7.2324339942425695
Experience 14, Iter 8, disc loss: 0.005078953070844598, policy loss: 6.960749254539586
Experience 14, Iter 9, disc loss: 0.005240830812701448, policy loss: 6.45598815849982
Experience 14, Iter 10, disc loss: 0.006072865139595699, policy loss: 5.978909681040902
Experience 14, Iter 11, disc loss: 0.007684121404760569, policy loss: 5.599537133201554
Experience 14, Iter 12, disc loss: 0.009690749663780938, policy loss: 5.165643690410157
Experience 14, Iter 13, disc loss: 0.012048144403259676, policy loss: 4.811970408351694
Experience 14, Iter 14, disc loss: 0.014033434700047764, policy loss: 5.063368888120927
Experience 14, Iter 15, disc loss: 0.012336859220649657, policy loss: 5.256159598542392
Experience 14, Iter 16, disc loss: 0.013871520563058194, policy loss: 4.976987018522523
Experience 14, Iter 17, disc loss: 0.011400574579792325, policy loss: 5.2527240497453525
Experience 14, Iter 18, disc loss: 0.00867217338984944, policy loss: 6.434608092200019
Experience 14, Iter 19, disc loss: 0.010841686613434398, policy loss: 5.4125784002616175
Experience 14, Iter 20, disc loss: 0.009842972859471857, policy loss: 5.404397886901967
Experience 14, Iter 21, disc loss: 0.01157429333270138, policy loss: 5.533139644503209
Experience 14, Iter 22, disc loss: 0.01099333855753968, policy loss: 5.840070768975728
Experience 14, Iter 23, disc loss: 0.007836593516151768, policy loss: 6.575058915743648
Experience 14, Iter 24, disc loss: 0.009651649835466096, policy loss: 5.589868408018442
Experience 14, Iter 25, disc loss: 0.007523228145109023, policy loss: 6.3837507976740975
Experience 14, Iter 26, disc loss: 0.0071526580394977585, policy loss: 6.729495043484551
Experience 14, Iter 27, disc loss: 0.006853567033399219, policy loss: 6.642346761098938
Experience 14, Iter 28, disc loss: 0.006700240414424095, policy loss: 7.054688694754291
Experience 14, Iter 29, disc loss: 0.006886626451198745, policy loss: 6.565986030051853
Experience 14, Iter 30, disc loss: 0.006616018076368703, policy loss: 6.937575814012809
Experience 14, Iter 31, disc loss: 0.006819901837132811, policy loss: 6.4917207017671785
Experience 14, Iter 32, disc loss: 0.006811772109675649, policy loss: 6.6733052675047935
Experience 14, Iter 33, disc loss: 0.0074300399146485, policy loss: 6.0395076273163495
Experience 14, Iter 34, disc loss: 0.0076603309800554326, policy loss: 6.206956707501659
Experience 14, Iter 35, disc loss: 0.008110385076654784, policy loss: 5.9414263236061
Experience 14, Iter 36, disc loss: 0.008612554379019526, policy loss: 5.867922072949178
Experience 14, Iter 37, disc loss: 0.010381913993648951, policy loss: 5.831170308240263
Experience 14, Iter 38, disc loss: 0.00948107316317281, policy loss: 5.724064375027784
Experience 14, Iter 39, disc loss: 0.009865014386637071, policy loss: 5.323661763176037
Experience 14, Iter 40, disc loss: 0.009993640057057784, policy loss: 5.482554127719688
Experience 14, Iter 41, disc loss: 0.008800675699860712, policy loss: 6.355166761158825
Experience 14, Iter 42, disc loss: 0.008185904896086959, policy loss: 5.932880939039999
Experience 14, Iter 43, disc loss: 0.009095879465904726, policy loss: 5.696527047297582
Experience 14, Iter 44, disc loss: 0.008605274658969226, policy loss: 6.680291962201692
Experience 14, Iter 45, disc loss: 0.010216510695679845, policy loss: 5.46098359757707
Experience 14, Iter 46, disc loss: 0.01028873397000719, policy loss: 5.469367322630868
Experience 14, Iter 47, disc loss: 0.010325511808977094, policy loss: 5.493531668476599
Experience 14, Iter 48, disc loss: 0.009187239105005462, policy loss: 5.63883008883369
Experience 14, Iter 49, disc loss: 0.008662549753799136, policy loss: 5.881589336022969
Experience 14, Iter 50, disc loss: 0.008913230688134887, policy loss: 5.645192038256253
Experience 14, Iter 51, disc loss: 0.008368034279384023, policy loss: 5.895718933730869
Experience 14, Iter 52, disc loss: 0.009178637535772959, policy loss: 5.494248143236555
Experience 14, Iter 53, disc loss: 0.009496272204935746, policy loss: 5.756952241409847
Experience 14, Iter 54, disc loss: 0.010016932003907492, policy loss: 6.0947976208746475
Experience 14, Iter 55, disc loss: 0.009419545574495751, policy loss: 5.640589285119269
Experience 14, Iter 56, disc loss: 0.008874502856586336, policy loss: 6.295638902243518
Experience 14, Iter 57, disc loss: 0.009074732708287996, policy loss: 5.913680793331065
Experience 14, Iter 58, disc loss: 0.009816346572060212, policy loss: 5.434499199374415
Experience 14, Iter 59, disc loss: 0.008843814601300962, policy loss: 6.458948795799474
Experience 14, Iter 60, disc loss: 0.009052447102937733, policy loss: 5.7344747856513045
Experience 14, Iter 61, disc loss: 0.009573949342721795, policy loss: 5.85875778538435
Experience 14, Iter 62, disc loss: 0.00914590354972106, policy loss: 5.734853076780352
Experience 14, Iter 63, disc loss: 0.009150338438161622, policy loss: 5.620888583015996
Experience 14, Iter 64, disc loss: 0.009507474580720137, policy loss: 5.452303402460898
Experience 14, Iter 65, disc loss: 0.00863990252370616, policy loss: 5.975067100038809
Experience 14, Iter 66, disc loss: 0.009119537433617526, policy loss: 5.73086845583809
Experience 14, Iter 67, disc loss: 0.008966862074337955, policy loss: 5.7521990246786565
Experience 14, Iter 68, disc loss: 0.008601829588586327, policy loss: 5.832738593004933
Experience 14, Iter 69, disc loss: 0.008900703760801962, policy loss: 5.805621974583187
Experience 14, Iter 70, disc loss: 0.00873002392407495, policy loss: 5.945192781234068
Experience 14, Iter 71, disc loss: 0.009180489620386808, policy loss: 5.6927562163797605
Experience 14, Iter 72, disc loss: 0.009094214113965302, policy loss: 5.797896896083563
Experience 14, Iter 73, disc loss: 0.009287727517179135, policy loss: 5.5752151916792725
Experience 14, Iter 74, disc loss: 0.008963094162672765, policy loss: 5.933782182079746
Experience 14, Iter 75, disc loss: 0.008376396385376704, policy loss: 5.836388930448485
Experience 14, Iter 76, disc loss: 0.008399442617141789, policy loss: 6.514802766230114
Experience 14, Iter 77, disc loss: 0.008469160680391278, policy loss: 5.844916084199491
Experience 14, Iter 78, disc loss: 0.008435157294019675, policy loss: 5.655439496344888
Experience 14, Iter 79, disc loss: 0.00838331086023246, policy loss: 5.7336291767612035
Experience 14, Iter 80, disc loss: 0.007871691654962123, policy loss: 5.889253439088881
Experience 14, Iter 81, disc loss: 0.007428764927538126, policy loss: 6.05920395458478
Experience 14, Iter 82, disc loss: 0.007839849784151108, policy loss: 5.904319408288893
Experience 14, Iter 83, disc loss: 0.007705340425559929, policy loss: 5.937135692069485
Experience 14, Iter 84, disc loss: 0.008113166668354205, policy loss: 5.909389365275533
Experience 14, Iter 85, disc loss: 0.008635068524369457, policy loss: 5.596583464388473
Experience 14, Iter 86, disc loss: 0.00815752599151374, policy loss: 5.7283918840198496
Experience 14, Iter 87, disc loss: 0.00832380460452992, policy loss: 5.931302466359349
Experience 14, Iter 88, disc loss: 0.008165573072600765, policy loss: 5.780191857968279
Experience 14, Iter 89, disc loss: 0.008460824192536288, policy loss: 5.767830165129013
Experience 14, Iter 90, disc loss: 0.007761980992745572, policy loss: 6.225067804920915
Experience 14, Iter 91, disc loss: 0.008799343715573078, policy loss: 5.577536763079758
Experience 14, Iter 92, disc loss: 0.007765503734628458, policy loss: 5.869356213915406
Experience 14, Iter 93, disc loss: 0.007199987744147039, policy loss: 6.04181177910339
Experience 14, Iter 94, disc loss: 0.007644137882338705, policy loss: 5.754560784823353
Experience 14, Iter 95, disc loss: 0.00691120663543236, policy loss: 6.161855753575017
Experience 14, Iter 96, disc loss: 0.007297094599537266, policy loss: 6.034867388993026
Experience 14, Iter 97, disc loss: 0.006703373294233603, policy loss: 6.201117245624638
Experience 14, Iter 98, disc loss: 0.006182914591961237, policy loss: 6.860934867679028
Experience 14, Iter 99, disc loss: 0.006925705562308176, policy loss: 6.0311201291435435
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.1281],
        [1.0777],
        [0.0198]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0083, 0.0934, 0.9343, 0.0177, 0.0136, 3.0907]],

        [[0.0083, 0.0934, 0.9343, 0.0177, 0.0136, 3.0907]],

        [[0.0083, 0.0934, 0.9343, 0.0177, 0.0136, 3.0907]],

        [[0.0083, 0.0934, 0.9343, 0.0177, 0.0136, 3.0907]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0087, 0.5123, 4.3109, 0.0793], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0087, 0.5123, 4.3109, 0.0793])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.500
Iter 2/2000 - Loss: 2.792
Iter 3/2000 - Loss: 2.429
Iter 4/2000 - Loss: 2.473
Iter 5/2000 - Loss: 2.561
Iter 6/2000 - Loss: 2.473
Iter 7/2000 - Loss: 2.347
Iter 8/2000 - Loss: 2.280
Iter 9/2000 - Loss: 2.265
Iter 10/2000 - Loss: 2.246
Iter 11/2000 - Loss: 2.174
Iter 12/2000 - Loss: 2.054
Iter 13/2000 - Loss: 1.917
Iter 14/2000 - Loss: 1.790
Iter 15/2000 - Loss: 1.670
Iter 16/2000 - Loss: 1.537
Iter 17/2000 - Loss: 1.370
Iter 18/2000 - Loss: 1.166
Iter 19/2000 - Loss: 0.937
Iter 20/2000 - Loss: 0.695
Iter 1981/2000 - Loss: -7.786
Iter 1982/2000 - Loss: -7.786
Iter 1983/2000 - Loss: -7.786
Iter 1984/2000 - Loss: -7.786
Iter 1985/2000 - Loss: -7.786
Iter 1986/2000 - Loss: -7.786
Iter 1987/2000 - Loss: -7.786
Iter 1988/2000 - Loss: -7.786
Iter 1989/2000 - Loss: -7.786
Iter 1990/2000 - Loss: -7.786
Iter 1991/2000 - Loss: -7.786
Iter 1992/2000 - Loss: -7.787
Iter 1993/2000 - Loss: -7.787
Iter 1994/2000 - Loss: -7.787
Iter 1995/2000 - Loss: -7.787
Iter 1996/2000 - Loss: -7.787
Iter 1997/2000 - Loss: -7.787
Iter 1998/2000 - Loss: -7.787
Iter 1999/2000 - Loss: -7.787
Iter 2000/2000 - Loss: -7.787
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[11.1962,  3.7319, 41.8354,  6.7072,  6.0592, 42.8947]],

        [[16.3495, 29.1364,  8.3111,  1.2828,  3.3923, 18.8575]],

        [[17.5786, 20.7064,  9.5494,  1.2410,  1.3602, 23.5536]],

        [[12.6336, 19.9736,  8.8896,  1.2137,  1.7463, 38.2579]]])
Signal Variance: tensor([ 0.0353,  1.6602, 22.7459,  0.2891])
Estimated target variance: tensor([0.0087, 0.5123, 4.3109, 0.0793])
N: 150
Signal to noise ratio: tensor([ 10.5952,  75.8412, 113.9983,  34.6265])
Bound on condition number: tensor([  16839.6344,  862784.1599, 1949342.3832,  179849.8979])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.007284966922960485, policy loss: 6.0455866732722185
Experience 15, Iter 1, disc loss: 0.006397868466593158, policy loss: 6.351573126698697
Experience 15, Iter 2, disc loss: 0.007560325971775916, policy loss: 6.1403728762009555
Experience 15, Iter 3, disc loss: 0.006738882148955875, policy loss: 6.337694038842116
Experience 15, Iter 4, disc loss: 0.006917318253324469, policy loss: 6.2796546833927
Experience 15, Iter 5, disc loss: 0.006862034829410014, policy loss: 6.054566654862919
Experience 15, Iter 6, disc loss: 0.007199028318489863, policy loss: 6.031446186703112
Experience 15, Iter 7, disc loss: 0.007919508639894995, policy loss: 5.859705375837233
Experience 15, Iter 8, disc loss: 0.007627637070519539, policy loss: 5.8501481305374625
Experience 15, Iter 9, disc loss: 0.007721387399837883, policy loss: 5.849623024581506
Experience 15, Iter 10, disc loss: 0.007271718684256503, policy loss: 5.886219033555571
Experience 15, Iter 11, disc loss: 0.007362076552134363, policy loss: 6.2188505625344614
Experience 15, Iter 12, disc loss: 0.006879118013499006, policy loss: 6.285453343083374
Experience 15, Iter 13, disc loss: 0.007003101677554096, policy loss: 6.1469231053706554
Experience 15, Iter 14, disc loss: 0.00754385748133755, policy loss: 5.779253230729999
Experience 15, Iter 15, disc loss: 0.0074314363133404365, policy loss: 5.951176798513026
Experience 15, Iter 16, disc loss: 0.007193190915270127, policy loss: 6.0070712078316335
Experience 15, Iter 17, disc loss: 0.007123275399354213, policy loss: 5.928987301837716
Experience 15, Iter 18, disc loss: 0.006951550525268803, policy loss: 5.8604783207682605
Experience 15, Iter 19, disc loss: 0.006477987657884154, policy loss: 6.054405650424737
Experience 15, Iter 20, disc loss: 0.007449571263763413, policy loss: 6.012444605182871
Experience 15, Iter 21, disc loss: 0.007060175063884653, policy loss: 5.88421975476475
Experience 15, Iter 22, disc loss: 0.006686726067811354, policy loss: 6.084033649892115
Experience 15, Iter 23, disc loss: 0.006979011699624617, policy loss: 6.310871435862469
Experience 15, Iter 24, disc loss: 0.006839728483698435, policy loss: 6.326561632560208
Experience 15, Iter 25, disc loss: 0.006597231991747474, policy loss: 6.482131598711964
Experience 15, Iter 26, disc loss: 0.0069980128371979495, policy loss: 5.95664762095897
Experience 15, Iter 27, disc loss: 0.006133885103304546, policy loss: 6.149066888417905
Experience 15, Iter 28, disc loss: 0.006849271568717135, policy loss: 6.067521457169026
Experience 15, Iter 29, disc loss: 0.006814314117250957, policy loss: 6.292081081047431
Experience 15, Iter 30, disc loss: 0.006510023579142217, policy loss: 6.466396148842186
Experience 15, Iter 31, disc loss: 0.0068069147645739864, policy loss: 6.05758487063441
Experience 15, Iter 32, disc loss: 0.0065037685164692095, policy loss: 6.057266938038328
Experience 15, Iter 33, disc loss: 0.006204730271881833, policy loss: 6.3038506388726425
Experience 15, Iter 34, disc loss: 0.007172488132570971, policy loss: 5.972575522298495
Experience 15, Iter 35, disc loss: 0.005938344065076165, policy loss: 6.528752103333533
Experience 15, Iter 36, disc loss: 0.006306324413244042, policy loss: 6.88767721569904
Experience 15, Iter 37, disc loss: 0.005756931570499749, policy loss: 6.1172807818335855
Experience 15, Iter 38, disc loss: 0.005203715733500588, policy loss: 6.949411551510634
Experience 15, Iter 39, disc loss: 0.005322039969711071, policy loss: 6.51003533162029
Experience 15, Iter 40, disc loss: 0.005194636822983935, policy loss: 6.6887987611224435
Experience 15, Iter 41, disc loss: 0.005432095172148813, policy loss: 7.07017122433456
Experience 15, Iter 42, disc loss: 0.006474105699251498, policy loss: 6.106579955471349
Experience 15, Iter 43, disc loss: 0.004501688803170788, policy loss: 7.517625975556509
Experience 15, Iter 44, disc loss: 0.005235244140038398, policy loss: 6.708041934292828
Experience 15, Iter 45, disc loss: 0.004213123423725125, policy loss: 6.896540613877744
Experience 15, Iter 46, disc loss: 0.003981205673673894, policy loss: 6.7891184015413835
Experience 15, Iter 47, disc loss: 0.0035446654954023253, policy loss: 7.545125987404551
Experience 15, Iter 48, disc loss: 0.0032969706840275566, policy loss: 7.599822947900002
Experience 15, Iter 49, disc loss: 0.0032011409168663644, policy loss: 7.598375918981699
Experience 15, Iter 50, disc loss: 0.0031208292080337876, policy loss: 7.784039527118349
Experience 15, Iter 51, disc loss: 0.00307387682238583, policy loss: 7.401594763443793
Experience 15, Iter 52, disc loss: 0.0032962822986368034, policy loss: 7.161766798830914
Experience 15, Iter 53, disc loss: 0.0032406741894062153, policy loss: 7.3691170377619
Experience 15, Iter 54, disc loss: 0.003640047767361217, policy loss: 7.012162859489199
Experience 15, Iter 55, disc loss: 0.003765481572271164, policy loss: 6.591674348930555
Experience 15, Iter 56, disc loss: 0.004009078831620082, policy loss: 6.673446210487867
Experience 15, Iter 57, disc loss: 0.004435005636672864, policy loss: 6.309766334434341
Experience 15, Iter 58, disc loss: 0.005113028270801739, policy loss: 5.996704872836778
Experience 15, Iter 59, disc loss: 0.007082396027212471, policy loss: 5.6109743243505585
Experience 15, Iter 60, disc loss: 0.007250771726767482, policy loss: 5.413934097614122
Experience 15, Iter 61, disc loss: 0.007580246356862339, policy loss: 6.087724323617209
Experience 15, Iter 62, disc loss: 0.006814454590524477, policy loss: 5.718782709944842
Experience 15, Iter 63, disc loss: 0.0056854760400075685, policy loss: 6.690524458381205
Experience 15, Iter 64, disc loss: 0.005299405208611507, policy loss: 6.695053692242493
Experience 15, Iter 65, disc loss: 0.005938373968978085, policy loss: 6.206236945030756
Experience 15, Iter 66, disc loss: 0.006217797230707263, policy loss: 6.282798038897036
Experience 15, Iter 67, disc loss: 0.007224194021767837, policy loss: 5.746544585765515
Experience 15, Iter 68, disc loss: 0.006099219622455964, policy loss: 6.186344063370269
Experience 15, Iter 69, disc loss: 0.005481581663501463, policy loss: 6.560268626813883
Experience 15, Iter 70, disc loss: 0.005570441940753302, policy loss: 6.2067478428497065
Experience 15, Iter 71, disc loss: 0.005255431015072828, policy loss: 6.219474921558019
Experience 15, Iter 72, disc loss: 0.0052528549362296905, policy loss: 6.206514343717255
Experience 15, Iter 73, disc loss: 0.005362039169073931, policy loss: 6.4950359241905975
Experience 15, Iter 74, disc loss: 0.005145614461616876, policy loss: 6.3877482456682895
Experience 15, Iter 75, disc loss: 0.005053610582010603, policy loss: 6.6817272236073935
Experience 15, Iter 76, disc loss: 0.005128703727609933, policy loss: 6.429236903880183
Experience 15, Iter 77, disc loss: 0.005032336754686815, policy loss: 6.827089317080297
Experience 15, Iter 78, disc loss: 0.005237842767579028, policy loss: 6.567310918927211
Experience 15, Iter 79, disc loss: 0.005644024963032601, policy loss: 6.255663891034953
Experience 15, Iter 80, disc loss: 0.005187364062777424, policy loss: 6.730800609894286
Experience 15, Iter 81, disc loss: 0.00500133593339876, policy loss: 6.842870106412173
Experience 15, Iter 82, disc loss: 0.005630201929435253, policy loss: 6.145335446513805
Experience 15, Iter 83, disc loss: 0.004994269233224106, policy loss: 7.019292256347935
Experience 15, Iter 84, disc loss: 0.005310894779403538, policy loss: 6.510765326462498
Experience 15, Iter 85, disc loss: 0.004525490153914371, policy loss: 6.798635801440994
Experience 15, Iter 86, disc loss: 0.004250025634216721, policy loss: 7.219055765825029
Experience 15, Iter 87, disc loss: 0.004448732999045805, policy loss: 6.795675878435187
Experience 15, Iter 88, disc loss: 0.004312776986840025, policy loss: 6.784194414650274
Experience 15, Iter 89, disc loss: 0.004457150625016792, policy loss: 6.809565023377569
Experience 15, Iter 90, disc loss: 0.005085784589799381, policy loss: 6.558503714450848
Experience 15, Iter 91, disc loss: 0.005303548176915902, policy loss: 6.679503652369979
Experience 15, Iter 92, disc loss: 0.005025105086770322, policy loss: 6.726522368802771
Experience 15, Iter 93, disc loss: 0.004580594023300056, policy loss: 6.7597557454139485
Experience 15, Iter 94, disc loss: 0.0039755597725917675, policy loss: 6.662242688498413
Experience 15, Iter 95, disc loss: 0.003375681318796735, policy loss: 7.118159718704595
Experience 15, Iter 96, disc loss: 0.003201208892644368, policy loss: 7.739207043061001
Experience 15, Iter 97, disc loss: 0.003202854068874749, policy loss: 7.281897867076677
Experience 15, Iter 98, disc loss: 0.003451931128983224, policy loss: 6.891319086492221
Experience 15, Iter 99, disc loss: 0.0033036737134322946, policy loss: 7.4114304831773055
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1379],
        [1.1526],
        [0.0216]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0080, 0.0978, 1.0141, 0.0193, 0.0160, 3.3529]],

        [[0.0080, 0.0978, 1.0141, 0.0193, 0.0160, 3.3529]],

        [[0.0080, 0.0978, 1.0141, 0.0193, 0.0160, 3.3529]],

        [[0.0080, 0.0978, 1.0141, 0.0193, 0.0160, 3.3529]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0091, 0.5516, 4.6104, 0.0862], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0091, 0.5516, 4.6104, 0.0862])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.642
Iter 2/2000 - Loss: 2.900
Iter 3/2000 - Loss: 2.553
Iter 4/2000 - Loss: 2.599
Iter 5/2000 - Loss: 2.679
Iter 6/2000 - Loss: 2.584
Iter 7/2000 - Loss: 2.454
Iter 8/2000 - Loss: 2.388
Iter 9/2000 - Loss: 2.373
Iter 10/2000 - Loss: 2.348
Iter 11/2000 - Loss: 2.266
Iter 12/2000 - Loss: 2.137
Iter 13/2000 - Loss: 1.997
Iter 14/2000 - Loss: 1.869
Iter 15/2000 - Loss: 1.749
Iter 16/2000 - Loss: 1.613
Iter 17/2000 - Loss: 1.442
Iter 18/2000 - Loss: 1.232
Iter 19/2000 - Loss: 0.999
Iter 20/2000 - Loss: 0.756
Iter 1981/2000 - Loss: -7.776
Iter 1982/2000 - Loss: -7.776
Iter 1983/2000 - Loss: -7.776
Iter 1984/2000 - Loss: -7.777
Iter 1985/2000 - Loss: -7.777
Iter 1986/2000 - Loss: -7.777
Iter 1987/2000 - Loss: -7.777
Iter 1988/2000 - Loss: -7.777
Iter 1989/2000 - Loss: -7.777
Iter 1990/2000 - Loss: -7.777
Iter 1991/2000 - Loss: -7.777
Iter 1992/2000 - Loss: -7.777
Iter 1993/2000 - Loss: -7.777
Iter 1994/2000 - Loss: -7.777
Iter 1995/2000 - Loss: -7.777
Iter 1996/2000 - Loss: -7.777
Iter 1997/2000 - Loss: -7.777
Iter 1998/2000 - Loss: -7.777
Iter 1999/2000 - Loss: -7.777
Iter 2000/2000 - Loss: -7.777
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[10.7924,  4.1428, 42.5074,  7.9376,  4.8004, 44.7207]],

        [[16.6279, 28.4764,  8.4581,  1.3144,  2.8291, 22.9328]],

        [[18.1588, 21.0019,  9.2954,  1.2535,  1.3893, 23.7767]],

        [[14.0032, 24.0838, 15.9043,  1.9757,  1.8282, 43.7653]]])
Signal Variance: tensor([ 0.0378,  1.9870, 22.3461,  0.5264])
Estimated target variance: tensor([0.0091, 0.5516, 4.6104, 0.0862])
N: 160
Signal to noise ratio: tensor([ 10.8728,  80.4495, 111.3457,  42.3857])
Bound on condition number: tensor([  18915.7597, 1035539.3431, 1983657.7210,  287448.3462])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0032957777620145853, policy loss: 6.985147460291662
Experience 16, Iter 1, disc loss: 0.0032115307281761884, policy loss: 7.137944945993532
Experience 16, Iter 2, disc loss: 0.0032930442259106506, policy loss: 7.059107198120895
Experience 16, Iter 3, disc loss: 0.0037453552426859474, policy loss: 6.842642330333604
Experience 16, Iter 4, disc loss: 0.004056270018416916, policy loss: 6.511396715540956
Experience 16, Iter 5, disc loss: 0.004484449583350936, policy loss: 6.223760895763935
Experience 16, Iter 6, disc loss: 0.004752704424571048, policy loss: 6.437223516166694
Experience 16, Iter 7, disc loss: 0.0050920711911711936, policy loss: 6.618916632400264
Experience 16, Iter 8, disc loss: 0.004661767579297981, policy loss: 6.165907198577481
Experience 16, Iter 9, disc loss: 0.004153040787832417, policy loss: 6.443281568222812
Experience 16, Iter 10, disc loss: 0.0037177130864873347, policy loss: 6.771184520527744
Experience 16, Iter 11, disc loss: 0.003752066326317501, policy loss: 6.87098794836226
Experience 16, Iter 12, disc loss: 0.0038691671533068986, policy loss: 6.861756012685969
Experience 16, Iter 13, disc loss: 0.0037835674923231565, policy loss: 6.690556950296111
Experience 16, Iter 14, disc loss: 0.00395942476564499, policy loss: 7.005424191544672
Experience 16, Iter 15, disc loss: 0.004464062925506097, policy loss: 6.325383784235378
Experience 16, Iter 16, disc loss: 0.004635953378603196, policy loss: 6.593492332349687
Experience 16, Iter 17, disc loss: 0.004587249599026568, policy loss: 6.489130701045422
Experience 16, Iter 18, disc loss: 0.005149536320329504, policy loss: 5.946058766232217
Experience 16, Iter 19, disc loss: 0.004337754730344723, policy loss: 6.354140359383107
Experience 16, Iter 20, disc loss: 0.0047330785273567745, policy loss: 6.171518933834732
Experience 16, Iter 21, disc loss: 0.0050621849054371, policy loss: 6.010047961034201
Experience 16, Iter 22, disc loss: 0.004449317672099318, policy loss: 6.288440882320158
Experience 16, Iter 23, disc loss: 0.004337896085349937, policy loss: 6.654035146694124
Experience 16, Iter 24, disc loss: 0.004485618296819693, policy loss: 6.403611760142302
Experience 16, Iter 25, disc loss: 0.004396857107105497, policy loss: 6.500117766267259
Experience 16, Iter 26, disc loss: 0.004738618129726672, policy loss: 6.446069494311928
Experience 16, Iter 27, disc loss: 0.004463841907320439, policy loss: 6.392039309927618
Experience 16, Iter 28, disc loss: 0.004361481192588057, policy loss: 6.75883830284768
Experience 16, Iter 29, disc loss: 0.004714586211656536, policy loss: 6.520549923934695
Experience 16, Iter 30, disc loss: 0.005089343118328194, policy loss: 6.207162453773702
Experience 16, Iter 31, disc loss: 0.004422003305439664, policy loss: 6.812656123829081
Experience 16, Iter 32, disc loss: 0.004450122943572391, policy loss: 6.775859466774368
Experience 16, Iter 33, disc loss: 0.004759213411214033, policy loss: 6.610577391223342
Experience 16, Iter 34, disc loss: 0.004412631970299377, policy loss: 6.540764533075223
Experience 16, Iter 35, disc loss: 0.004366425842565697, policy loss: 6.914607689051737
Experience 16, Iter 36, disc loss: 0.00451096321340353, policy loss: 6.558637087032403
Experience 16, Iter 37, disc loss: 0.004172474034661589, policy loss: 7.052316911444099
Experience 16, Iter 38, disc loss: 0.004275517253357484, policy loss: 6.743215206154848
Experience 16, Iter 39, disc loss: 0.00443248600308164, policy loss: 6.464150210011752
Experience 16, Iter 40, disc loss: 0.004385113181292069, policy loss: 6.600883829741491
Experience 16, Iter 41, disc loss: 0.003853805420493522, policy loss: 6.760082820462973
Experience 16, Iter 42, disc loss: 0.004188105233622622, policy loss: 6.687942401967833
Experience 16, Iter 43, disc loss: 0.003989142409228204, policy loss: 6.927028318511975
Experience 16, Iter 44, disc loss: 0.003909345729542731, policy loss: 6.668255547552672
Experience 16, Iter 45, disc loss: 0.004019256866011594, policy loss: 6.6590974848444
Experience 16, Iter 46, disc loss: 0.0037665411640681644, policy loss: 6.926794820581716
Experience 16, Iter 47, disc loss: 0.004108007674722445, policy loss: 6.54213393973483
Experience 16, Iter 48, disc loss: 0.0038620812230840094, policy loss: 6.71169461380711
Experience 16, Iter 49, disc loss: 0.004353654377615481, policy loss: 6.548196330969979
Experience 16, Iter 50, disc loss: 0.004193735157438004, policy loss: 6.661969114301338
Experience 16, Iter 51, disc loss: 0.004537008535872317, policy loss: 6.288116401695623
Experience 16, Iter 52, disc loss: 0.004147495272563285, policy loss: 6.940888935135554
Experience 16, Iter 53, disc loss: 0.004452893819577621, policy loss: 6.424151617599363
Experience 16, Iter 54, disc loss: 0.004207189802469589, policy loss: 6.449640949453117
Experience 16, Iter 55, disc loss: 0.004164339157104926, policy loss: 6.595494883433687
Experience 16, Iter 56, disc loss: 0.004140207533990435, policy loss: 6.36691814072548
Experience 16, Iter 57, disc loss: 0.0038618297924775612, policy loss: 6.766600319370838
Experience 16, Iter 58, disc loss: 0.004314084548852373, policy loss: 6.455097985033079
Experience 16, Iter 59, disc loss: 0.004551663532308782, policy loss: 6.353496996864688
Experience 16, Iter 60, disc loss: 0.004186202654504638, policy loss: 6.342773752440506
Experience 16, Iter 61, disc loss: 0.004091974251770441, policy loss: 6.509434757163481
Experience 16, Iter 62, disc loss: 0.00426605974979774, policy loss: 6.246841094718977
Experience 16, Iter 63, disc loss: 0.004194484242680319, policy loss: 6.619940785412557
Experience 16, Iter 64, disc loss: 0.00348872240624931, policy loss: 6.9921726223478515
Experience 16, Iter 65, disc loss: 0.003913643698907628, policy loss: 6.525240935409185
Experience 16, Iter 66, disc loss: 0.0040754357208909455, policy loss: 6.46493953638222
Experience 16, Iter 67, disc loss: 0.00386536283909077, policy loss: 6.72715846829755
Experience 16, Iter 68, disc loss: 0.003931910074221111, policy loss: 6.750950976076117
Experience 16, Iter 69, disc loss: 0.0037036140691724066, policy loss: 6.806662118575856
Experience 16, Iter 70, disc loss: 0.004086054576391926, policy loss: 6.581983791571896
Experience 16, Iter 71, disc loss: 0.003948397566551798, policy loss: 6.590400230119224
Experience 16, Iter 72, disc loss: 0.0034752806441476635, policy loss: 6.974650382048198
Experience 16, Iter 73, disc loss: 0.0037655425728071465, policy loss: 6.6589475149428266
Experience 16, Iter 74, disc loss: 0.003408778285092985, policy loss: 7.612311057116677
Experience 16, Iter 75, disc loss: 0.0035138013426518648, policy loss: 6.846389686149946
Experience 16, Iter 76, disc loss: 0.003578823158062093, policy loss: 6.75272224310077
Experience 16, Iter 77, disc loss: 0.0034734493273650803, policy loss: 6.851929329445806
Experience 16, Iter 78, disc loss: 0.00335613876214407, policy loss: 7.057882512645246
Experience 16, Iter 79, disc loss: 0.0036798889717076537, policy loss: 6.995741178935512
Experience 16, Iter 80, disc loss: 0.003730759603455798, policy loss: 6.695627019000032
Experience 16, Iter 81, disc loss: 0.003472976880211955, policy loss: 7.26689697447841
Experience 16, Iter 82, disc loss: 0.0037002679700149105, policy loss: 6.562401910032644
Experience 16, Iter 83, disc loss: 0.0036557263818015667, policy loss: 6.6781270703481015
Experience 16, Iter 84, disc loss: 0.0038841114764234944, policy loss: 6.51473065238274
Experience 16, Iter 85, disc loss: 0.003479780740134982, policy loss: 6.880208075014234
Experience 16, Iter 86, disc loss: 0.0037485834687977677, policy loss: 6.560188521418199
Experience 16, Iter 87, disc loss: 0.003717653409255432, policy loss: 6.819436828666587
Experience 16, Iter 88, disc loss: 0.003831289871352987, policy loss: 7.139597023741045
Experience 16, Iter 89, disc loss: 0.0039731140952621435, policy loss: 6.648515353444887
Experience 16, Iter 90, disc loss: 0.0034957579089218123, policy loss: 6.860096265915011
Experience 16, Iter 91, disc loss: 0.0034827336839852686, policy loss: 7.206499389981673
Experience 16, Iter 92, disc loss: 0.003935429372528089, policy loss: 6.764934289092429
Experience 16, Iter 93, disc loss: 0.003333947532839013, policy loss: 6.9498172665591165
Experience 16, Iter 94, disc loss: 0.003607152316309972, policy loss: 6.545132526051865
Experience 16, Iter 95, disc loss: 0.003817530984319689, policy loss: 6.414221856549322
Experience 16, Iter 96, disc loss: 0.0036946115424207745, policy loss: 6.9765009735467265
Experience 16, Iter 97, disc loss: 0.0038101357318019147, policy loss: 6.566976902481025
Experience 16, Iter 98, disc loss: 0.003793457122158937, policy loss: 6.806456639686195
Experience 16, Iter 99, disc loss: 0.0034153658113803627, policy loss: 6.793224092916768
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1485],
        [1.2135],
        [0.0232]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0078, 0.1013, 1.0877, 0.0203, 0.0175, 3.6097]],

        [[0.0078, 0.1013, 1.0877, 0.0203, 0.0175, 3.6097]],

        [[0.0078, 0.1013, 1.0877, 0.0203, 0.0175, 3.6097]],

        [[0.0078, 0.1013, 1.0877, 0.0203, 0.0175, 3.6097]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0094, 0.5940, 4.8538, 0.0928], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0094, 0.5940, 4.8538, 0.0928])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.759
Iter 2/2000 - Loss: 3.005
Iter 3/2000 - Loss: 2.652
Iter 4/2000 - Loss: 2.696
Iter 5/2000 - Loss: 2.772
Iter 6/2000 - Loss: 2.669
Iter 7/2000 - Loss: 2.525
Iter 8/2000 - Loss: 2.442
Iter 9/2000 - Loss: 2.412
Iter 10/2000 - Loss: 2.370
Iter 11/2000 - Loss: 2.270
Iter 12/2000 - Loss: 2.119
Iter 13/2000 - Loss: 1.952
Iter 14/2000 - Loss: 1.794
Iter 15/2000 - Loss: 1.645
Iter 16/2000 - Loss: 1.486
Iter 17/2000 - Loss: 1.294
Iter 18/2000 - Loss: 1.066
Iter 19/2000 - Loss: 0.815
Iter 20/2000 - Loss: 0.555
Iter 1981/2000 - Loss: -7.893
Iter 1982/2000 - Loss: -7.893
Iter 1983/2000 - Loss: -7.894
Iter 1984/2000 - Loss: -7.894
Iter 1985/2000 - Loss: -7.894
Iter 1986/2000 - Loss: -7.894
Iter 1987/2000 - Loss: -7.894
Iter 1988/2000 - Loss: -7.894
Iter 1989/2000 - Loss: -7.894
Iter 1990/2000 - Loss: -7.894
Iter 1991/2000 - Loss: -7.894
Iter 1992/2000 - Loss: -7.894
Iter 1993/2000 - Loss: -7.894
Iter 1994/2000 - Loss: -7.894
Iter 1995/2000 - Loss: -7.894
Iter 1996/2000 - Loss: -7.894
Iter 1997/2000 - Loss: -7.894
Iter 1998/2000 - Loss: -7.894
Iter 1999/2000 - Loss: -7.894
Iter 2000/2000 - Loss: -7.894
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.6035,  4.1599, 41.3342,  6.9370,  2.8945, 43.6887]],

        [[16.2712, 28.7118,  8.6131,  1.3026,  2.7937, 23.8029]],

        [[17.7299, 27.4037,  9.2254,  1.1952,  1.2738, 23.4103]],

        [[13.5793, 24.0636, 16.3750,  1.8061,  1.8796, 44.1934]]])
Signal Variance: tensor([ 0.0366,  2.0511, 19.4782,  0.5401])
Estimated target variance: tensor([0.0094, 0.5940, 4.8538, 0.0928])
N: 170
Signal to noise ratio: tensor([ 10.9626,  82.9999, 101.9982,  43.5544])
Bound on condition number: tensor([  20431.3993, 1171128.2346, 1768618.2342,  322487.8854])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.003912524095081794, policy loss: 6.529805209285981
Experience 17, Iter 1, disc loss: 0.003895714430772877, policy loss: 6.422229927124484
Experience 17, Iter 2, disc loss: 0.0035552978352665007, policy loss: 6.634128945933036
Experience 17, Iter 3, disc loss: 0.003695446564265446, policy loss: 6.5563228109461
Experience 17, Iter 4, disc loss: 0.003768250310932207, policy loss: 7.002930187618112
Experience 17, Iter 5, disc loss: 0.003799236407129682, policy loss: 6.801687990930185
Experience 17, Iter 6, disc loss: 0.003753360894901506, policy loss: 6.555612761353749
Experience 17, Iter 7, disc loss: 0.003357613431822346, policy loss: 6.743564245980437
Experience 17, Iter 8, disc loss: 0.0035121747907287315, policy loss: 6.8571965952506
Experience 17, Iter 9, disc loss: 0.003177807073463917, policy loss: 6.896914798135009
Experience 17, Iter 10, disc loss: 0.0033947795321839945, policy loss: 6.8461923951326495
Experience 17, Iter 11, disc loss: 0.003558449834552728, policy loss: 6.687471100203705
Experience 17, Iter 12, disc loss: 0.003374146307736023, policy loss: 7.196405114791267
Experience 17, Iter 13, disc loss: 0.0035538592296490337, policy loss: 6.94837957543764
Experience 17, Iter 14, disc loss: 0.00349021245508282, policy loss: 6.784314754977771
Experience 17, Iter 15, disc loss: 0.003119091248337612, policy loss: 7.601811746722515
Experience 17, Iter 16, disc loss: 0.0035159421788588264, policy loss: 6.704605114726072
Experience 17, Iter 17, disc loss: 0.0032787397079169717, policy loss: 6.763585079257748
Experience 17, Iter 18, disc loss: 0.0033449379861542855, policy loss: 6.7751987380887915
Experience 17, Iter 19, disc loss: 0.0033072404370548185, policy loss: 6.982501307805796
Experience 17, Iter 20, disc loss: 0.00332138265359029, policy loss: 6.784074255235364
Experience 17, Iter 21, disc loss: 0.0035594343523679627, policy loss: 6.703767524320913
Experience 17, Iter 22, disc loss: 0.0035408201660943723, policy loss: 6.721799003107142
Experience 17, Iter 23, disc loss: 0.003375041779701956, policy loss: 6.923321501855467
Experience 17, Iter 24, disc loss: 0.0037377212755709075, policy loss: 6.576033619008927
Experience 17, Iter 25, disc loss: 0.0035193180150770564, policy loss: 7.025154239482356
Experience 17, Iter 26, disc loss: 0.0037107857303939566, policy loss: 6.552334799941541
Experience 17, Iter 27, disc loss: 0.003284290876451124, policy loss: 6.964966455231047
Experience 17, Iter 28, disc loss: 0.003412729882156825, policy loss: 6.890508330954806
Experience 17, Iter 29, disc loss: 0.0036352195843611414, policy loss: 6.532860149400309
Experience 17, Iter 30, disc loss: 0.0036149223457265125, policy loss: 6.7266715502583745
Experience 17, Iter 31, disc loss: 0.0034217565838734814, policy loss: 6.893655232134206
Experience 17, Iter 32, disc loss: 0.0034992933266586347, policy loss: 6.98325033596384
Experience 17, Iter 33, disc loss: 0.0034241157193782633, policy loss: 7.319161551928645
Experience 17, Iter 34, disc loss: 0.0035624671188269896, policy loss: 6.823785254650494
Experience 17, Iter 35, disc loss: 0.003353641963178991, policy loss: 6.645794216546639
Experience 17, Iter 36, disc loss: 0.003410108922326912, policy loss: 6.923774040346075
Experience 17, Iter 37, disc loss: 0.0034614396274884173, policy loss: 6.720805547927198
Experience 17, Iter 38, disc loss: 0.0033473313194800946, policy loss: 6.753752278297062
Experience 17, Iter 39, disc loss: 0.0031963171215906505, policy loss: 6.918535851987945
Experience 17, Iter 40, disc loss: 0.00330917111874691, policy loss: 6.846278090399791
Experience 17, Iter 41, disc loss: 0.0032336667805570665, policy loss: 7.169186629381686
Experience 17, Iter 42, disc loss: 0.002987760064045423, policy loss: 7.754904505917372
Experience 17, Iter 43, disc loss: 0.0030981089436731913, policy loss: 6.834148872945526
Experience 17, Iter 44, disc loss: 0.002933959312300077, policy loss: 7.378923193040583
Experience 17, Iter 45, disc loss: 0.0032172634923346858, policy loss: 6.844150224510573
Experience 17, Iter 46, disc loss: 0.0030902004083985034, policy loss: 7.2464038659737975
Experience 17, Iter 47, disc loss: 0.0032600952841489663, policy loss: 6.883722553211222
Experience 17, Iter 48, disc loss: 0.003148000450152178, policy loss: 6.940505458604444
Experience 17, Iter 49, disc loss: 0.003245556804078081, policy loss: 6.936712652963525
Experience 17, Iter 50, disc loss: 0.0032396851551402314, policy loss: 7.0934288582947405
Experience 17, Iter 51, disc loss: 0.0034968142431226028, policy loss: 7.010234107738425
Experience 17, Iter 52, disc loss: 0.00327326352977564, policy loss: 6.905896513091143
Experience 17, Iter 53, disc loss: 0.0036224127370845433, policy loss: 6.54166443398638
Experience 17, Iter 54, disc loss: 0.003382895118380881, policy loss: 6.613976247122286
Experience 17, Iter 55, disc loss: 0.003055468906435117, policy loss: 7.012687178300723
Experience 17, Iter 56, disc loss: 0.0031247993760794386, policy loss: 7.033292902801017
Experience 17, Iter 57, disc loss: 0.0031300891092392075, policy loss: 6.930678908766136
Experience 17, Iter 58, disc loss: 0.003213094497801397, policy loss: 6.685026456941237
Experience 17, Iter 59, disc loss: 0.002959859158668833, policy loss: 7.339631111486161
Experience 17, Iter 60, disc loss: 0.003205388428206119, policy loss: 6.724501276624011
Experience 17, Iter 61, disc loss: 0.0031524616901951645, policy loss: 6.691766333221944
Experience 17, Iter 62, disc loss: 0.003178144913312235, policy loss: 7.0123229242690055
Experience 17, Iter 63, disc loss: 0.003126464572393864, policy loss: 6.986761602762325
Experience 17, Iter 64, disc loss: 0.003057564891019305, policy loss: 6.867486243672495
Experience 17, Iter 65, disc loss: 0.0030094591498453427, policy loss: 6.870437462008871
Experience 17, Iter 66, disc loss: 0.0031797123439493927, policy loss: 6.7242080272624225
Experience 17, Iter 67, disc loss: 0.003143185453806279, policy loss: 7.097041572193099
Experience 17, Iter 68, disc loss: 0.0030172915283353556, policy loss: 6.878175690597728
Experience 17, Iter 69, disc loss: 0.0031679075891787164, policy loss: 6.944150547222822
Experience 17, Iter 70, disc loss: 0.002781466203762548, policy loss: 7.135735088679647
Experience 17, Iter 71, disc loss: 0.0032522167179153552, policy loss: 6.700487807245796
Experience 17, Iter 72, disc loss: 0.0033974039005474368, policy loss: 7.144438616249184
Experience 17, Iter 73, disc loss: 0.002841246256744448, policy loss: 7.203797352423829
Experience 17, Iter 74, disc loss: 0.0030046660924268162, policy loss: 6.960509040273286
Experience 17, Iter 75, disc loss: 0.0029569721745946512, policy loss: 7.379383451425349
Experience 17, Iter 76, disc loss: 0.002723330885241607, policy loss: 7.679576997872723
Experience 17, Iter 77, disc loss: 0.0029210981127963187, policy loss: 7.377957793785104
Experience 17, Iter 78, disc loss: 0.0028038261884798937, policy loss: 6.929763351195833
Experience 17, Iter 79, disc loss: 0.0028424664908612797, policy loss: 7.127195082061288
Experience 17, Iter 80, disc loss: 0.0032620090778098204, policy loss: 6.795733845895505
Experience 17, Iter 81, disc loss: 0.003147150099580523, policy loss: 6.799754935321166
Experience 17, Iter 82, disc loss: 0.0032330736262773424, policy loss: 6.622206694813913
Experience 17, Iter 83, disc loss: 0.002824609083362861, policy loss: 6.901303478522462
Experience 17, Iter 84, disc loss: 0.0032082377164442434, policy loss: 6.721599193348928
Experience 17, Iter 85, disc loss: 0.0025682437678669606, policy loss: 7.345194739323248
Experience 17, Iter 86, disc loss: 0.0029368542025673856, policy loss: 6.993069129078485
Experience 17, Iter 87, disc loss: 0.0027899173541309423, policy loss: 6.86972823967484
Experience 17, Iter 88, disc loss: 0.003089859297932195, policy loss: 6.802686627650249
Experience 17, Iter 89, disc loss: 0.0027737360770275325, policy loss: 7.65182847790103
Experience 17, Iter 90, disc loss: 0.0027631184513416604, policy loss: 6.972016735532751
Experience 17, Iter 91, disc loss: 0.002899840315461438, policy loss: 7.17144257399302
Experience 17, Iter 92, disc loss: 0.003063855428713085, policy loss: 6.974041316923191
Experience 17, Iter 93, disc loss: 0.0030366410466646865, policy loss: 6.821626990951301
Experience 17, Iter 94, disc loss: 0.003056010609421074, policy loss: 6.762992128788764
Experience 17, Iter 95, disc loss: 0.0025531817179936683, policy loss: 8.211527404090674
Experience 17, Iter 96, disc loss: 0.003030213277207744, policy loss: 6.9346224997321
Experience 17, Iter 97, disc loss: 0.0025544779155666506, policy loss: 7.378458074292307
Experience 17, Iter 98, disc loss: 0.002625958224838471, policy loss: 7.107726063020763
Experience 17, Iter 99, disc loss: 0.002665249049224461, policy loss: 7.264752395381063
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1595],
        [1.2924],
        [0.0248]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0077, 0.1076, 1.1638, 0.0211, 0.0186, 3.8463]],

        [[0.0077, 0.1076, 1.1638, 0.0211, 0.0186, 3.8463]],

        [[0.0077, 0.1076, 1.1638, 0.0211, 0.0186, 3.8463]],

        [[0.0077, 0.1076, 1.1638, 0.0211, 0.0186, 3.8463]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.6382, 5.1698, 0.0991], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.6382, 5.1698, 0.0991])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.873
Iter 2/2000 - Loss: 3.114
Iter 3/2000 - Loss: 2.753
Iter 4/2000 - Loss: 2.796
Iter 5/2000 - Loss: 2.876
Iter 6/2000 - Loss: 2.773
Iter 7/2000 - Loss: 2.620
Iter 8/2000 - Loss: 2.523
Iter 9/2000 - Loss: 2.482
Iter 10/2000 - Loss: 2.437
Iter 11/2000 - Loss: 2.337
Iter 12/2000 - Loss: 2.180
Iter 13/2000 - Loss: 1.999
Iter 14/2000 - Loss: 1.821
Iter 15/2000 - Loss: 1.655
Iter 16/2000 - Loss: 1.484
Iter 17/2000 - Loss: 1.287
Iter 18/2000 - Loss: 1.056
Iter 19/2000 - Loss: 0.796
Iter 20/2000 - Loss: 0.521
Iter 1981/2000 - Loss: -7.910
Iter 1982/2000 - Loss: -7.910
Iter 1983/2000 - Loss: -7.910
Iter 1984/2000 - Loss: -7.910
Iter 1985/2000 - Loss: -7.910
Iter 1986/2000 - Loss: -7.910
Iter 1987/2000 - Loss: -7.911
Iter 1988/2000 - Loss: -7.911
Iter 1989/2000 - Loss: -7.911
Iter 1990/2000 - Loss: -7.911
Iter 1991/2000 - Loss: -7.911
Iter 1992/2000 - Loss: -7.911
Iter 1993/2000 - Loss: -7.911
Iter 1994/2000 - Loss: -7.911
Iter 1995/2000 - Loss: -7.911
Iter 1996/2000 - Loss: -7.911
Iter 1997/2000 - Loss: -7.911
Iter 1998/2000 - Loss: -7.911
Iter 1999/2000 - Loss: -7.911
Iter 2000/2000 - Loss: -7.911
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[10.3861,  4.7833, 37.3850,  5.1276,  1.6897, 46.6318]],

        [[15.9930, 29.4034,  8.6588,  1.2723,  2.9280, 21.2168]],

        [[17.3736, 27.9015,  9.2853,  1.1959,  1.2798, 22.4127]],

        [[13.1025, 23.3279, 15.7569,  1.7149,  1.9695, 42.7925]]])
Signal Variance: tensor([ 0.0487,  1.7675, 17.8498,  0.5388])
Estimated target variance: tensor([0.0099, 0.6382, 5.1698, 0.0991])
N: 180
Signal to noise ratio: tensor([12.5978, 77.3002, 95.1406, 43.3431])
Bound on condition number: tensor([  28567.9819, 1075559.7179, 1629313.5794,  338153.3557])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.003102351377121865, policy loss: 6.806649670714043
Experience 18, Iter 1, disc loss: 0.002805741289456324, policy loss: 7.34610810459969
Experience 18, Iter 2, disc loss: 0.002840559035547338, policy loss: 7.376591636313238
Experience 18, Iter 3, disc loss: 0.0026321350990835607, policy loss: 7.546984453762766
Experience 18, Iter 4, disc loss: 0.0030168891835082082, policy loss: 6.638105255252558
Experience 18, Iter 5, disc loss: 0.002935368194296046, policy loss: 6.752123701177764
Experience 18, Iter 6, disc loss: 0.002847945820343897, policy loss: 6.963443462714254
Experience 18, Iter 7, disc loss: 0.0029311258178175755, policy loss: 6.725543017805169
Experience 18, Iter 8, disc loss: 0.0030203464737009952, policy loss: 7.042128793485801
Experience 18, Iter 9, disc loss: 0.0029510484680623315, policy loss: 6.844192683386883
Experience 18, Iter 10, disc loss: 0.0025446839356122785, policy loss: 7.5132743355069485
Experience 18, Iter 11, disc loss: 0.0027908869119386663, policy loss: 7.154308729420702
Experience 18, Iter 12, disc loss: 0.002750266389453732, policy loss: 7.124515982877214
Experience 18, Iter 13, disc loss: 0.002891880366377283, policy loss: 7.2014874175010775
Experience 18, Iter 14, disc loss: 0.0024472374420424856, policy loss: 7.361130572457281
Experience 18, Iter 15, disc loss: 0.0027953155702960726, policy loss: 6.821758713738777
Experience 18, Iter 16, disc loss: 0.00288324650646729, policy loss: 7.114511538804996
Experience 18, Iter 17, disc loss: 0.0027558803043676782, policy loss: 6.942596185408779
Experience 18, Iter 18, disc loss: 0.00242638827949966, policy loss: 7.1850095715327935
Experience 18, Iter 19, disc loss: 0.002735226617200134, policy loss: 7.290602134614696
Experience 18, Iter 20, disc loss: 0.002524994862828974, policy loss: 7.103577539961298
Experience 18, Iter 21, disc loss: 0.0026354261661216743, policy loss: 7.145385842561133
Experience 18, Iter 22, disc loss: 0.0027233741565837973, policy loss: 7.171018495588882
Experience 18, Iter 23, disc loss: 0.002801848462053786, policy loss: 6.974047059356867
Experience 18, Iter 24, disc loss: 0.002656595157660754, policy loss: 7.065576757673934
Experience 18, Iter 25, disc loss: 0.0027026753652772493, policy loss: 6.953769801180136
Experience 18, Iter 26, disc loss: 0.0027503466434819048, policy loss: 6.931467586461894
Experience 18, Iter 27, disc loss: 0.002580731642787118, policy loss: 7.187993524294654
Experience 18, Iter 28, disc loss: 0.002550637708488088, policy loss: 7.348014651379168
Experience 18, Iter 29, disc loss: 0.002813351786016549, policy loss: 7.144001568991075
Experience 18, Iter 30, disc loss: 0.002792520449376814, policy loss: 7.203972250267304
Experience 18, Iter 31, disc loss: 0.002721618690560984, policy loss: 6.975213690812849
Experience 18, Iter 32, disc loss: 0.0029688420403828847, policy loss: 6.729208514168432
Experience 18, Iter 33, disc loss: 0.0026962265218023087, policy loss: 7.2814391767598465
Experience 18, Iter 34, disc loss: 0.0031178280346871017, policy loss: 7.4326443531919555
Experience 18, Iter 35, disc loss: 0.002816683585566987, policy loss: 6.926104811936101
Experience 18, Iter 36, disc loss: 0.002828706916237843, policy loss: 7.113592246878291
Experience 18, Iter 37, disc loss: 0.0023859643447191386, policy loss: 7.308081596198319
Experience 18, Iter 38, disc loss: 0.002272082558489941, policy loss: 7.470785140059602
Experience 18, Iter 39, disc loss: 0.0026520078156303537, policy loss: 7.112473814637061
Experience 18, Iter 40, disc loss: 0.0028349366626564962, policy loss: 7.200261209563062
Experience 18, Iter 41, disc loss: 0.002882405866882899, policy loss: 7.132554264838172
Experience 18, Iter 42, disc loss: 0.0025699507578513764, policy loss: 7.235087341659423
Experience 18, Iter 43, disc loss: 0.0026423569880069386, policy loss: 7.3338760283676265
Experience 18, Iter 44, disc loss: 0.0026567369875081437, policy loss: 7.259486954676287
Experience 18, Iter 45, disc loss: 0.0029214847857491517, policy loss: 6.876904105369101
Experience 18, Iter 46, disc loss: 0.002467460473036183, policy loss: 7.441467088770772
Experience 18, Iter 47, disc loss: 0.0026999895204788777, policy loss: 6.840361299217328
Experience 18, Iter 48, disc loss: 0.002660782261198926, policy loss: 7.239282819506207
Experience 18, Iter 49, disc loss: 0.0023015478981648725, policy loss: 7.641193888404624
Experience 18, Iter 50, disc loss: 0.0022720811116916373, policy loss: 7.497558502304983
Experience 18, Iter 51, disc loss: 0.0022918952525287258, policy loss: 7.8408512181926655
Experience 18, Iter 52, disc loss: 0.0026664013888086234, policy loss: 7.27180811981025
Experience 18, Iter 53, disc loss: 0.0024476424548962074, policy loss: 7.362420686573057
Experience 18, Iter 54, disc loss: 0.00279598652105071, policy loss: 7.13507397385813
Experience 18, Iter 55, disc loss: 0.002813050799474714, policy loss: 7.1725192250992595
Experience 18, Iter 56, disc loss: 0.0023282389115272875, policy loss: 7.497982922681611
Experience 18, Iter 57, disc loss: 0.0022913347388724085, policy loss: 7.9757302319390995
Experience 18, Iter 58, disc loss: 0.002567315540792937, policy loss: 7.584579549706422
Experience 18, Iter 59, disc loss: 0.0026910463657617853, policy loss: 7.0781020571266575
Experience 18, Iter 60, disc loss: 0.0026407720893597214, policy loss: 6.875622216236513
Experience 18, Iter 61, disc loss: 0.002498064383623817, policy loss: 7.252247557382019
Experience 18, Iter 62, disc loss: 0.0022809666515621893, policy loss: 7.685961695062091
Experience 18, Iter 63, disc loss: 0.0025942619262792387, policy loss: 6.896830315250123
Experience 18, Iter 64, disc loss: 0.0029902902968513547, policy loss: 6.859628617608664
Experience 18, Iter 65, disc loss: 0.0024928122690366546, policy loss: 7.080964159415132
Experience 18, Iter 66, disc loss: 0.002470503113936145, policy loss: 7.07739674197705
Experience 18, Iter 67, disc loss: 0.0027314527473698436, policy loss: 7.057023733004016
Experience 18, Iter 68, disc loss: 0.0027406615581240593, policy loss: 7.004468344307968
Experience 18, Iter 69, disc loss: 0.0026131053795823375, policy loss: 7.213512061194331
Experience 18, Iter 70, disc loss: 0.0026680539577729322, policy loss: 6.921895203552868
Experience 18, Iter 71, disc loss: 0.0024425648688808843, policy loss: 6.975554476782722
Experience 18, Iter 72, disc loss: 0.002311721276395255, policy loss: 7.316937957592698
Experience 18, Iter 73, disc loss: 0.00268286128610802, policy loss: 7.023953804528599
Experience 18, Iter 74, disc loss: 0.0023249940013365927, policy loss: 7.07330708936187
Experience 18, Iter 75, disc loss: 0.0026028987889540264, policy loss: 6.954132620486948
Experience 18, Iter 76, disc loss: 0.002409244378756746, policy loss: 7.537990463693668
Experience 18, Iter 77, disc loss: 0.0025974854920250874, policy loss: 7.1321946539175265
Experience 18, Iter 78, disc loss: 0.002680273703943376, policy loss: 6.893463443657229
Experience 18, Iter 79, disc loss: 0.0024055059991340407, policy loss: 7.373612103345446
Experience 18, Iter 80, disc loss: 0.0026022406166789627, policy loss: 7.08273876736011
Experience 18, Iter 81, disc loss: 0.0025303180768442464, policy loss: 6.963057009973221
Experience 18, Iter 82, disc loss: 0.0025550805602972708, policy loss: 6.81894446614819
Experience 18, Iter 83, disc loss: 0.0024448765788511453, policy loss: 7.179231659984579
Experience 18, Iter 84, disc loss: 0.002470313706996452, policy loss: 7.768811083634349
Experience 18, Iter 85, disc loss: 0.002725438706102203, policy loss: 6.966291246493783
Experience 18, Iter 86, disc loss: 0.002587504116169353, policy loss: 7.716858101891122
Experience 18, Iter 87, disc loss: 0.002450392494015765, policy loss: 7.638282996266953
Experience 18, Iter 88, disc loss: 0.002257404908908356, policy loss: 7.917214273476322
Experience 18, Iter 89, disc loss: 0.0024775608747624692, policy loss: 7.517349368645772
Experience 18, Iter 90, disc loss: 0.0027227668102260097, policy loss: 7.017667198695248
Experience 18, Iter 91, disc loss: 0.0022767238008872944, policy loss: 7.523071190880291
Experience 18, Iter 92, disc loss: 0.002594164490259538, policy loss: 7.293961390135056
Experience 18, Iter 93, disc loss: 0.0024624422084660695, policy loss: 7.114969895408656
Experience 18, Iter 94, disc loss: 0.002545916251107991, policy loss: 7.030834254537364
Experience 18, Iter 95, disc loss: 0.0024455711107527573, policy loss: 7.109474655959699
Experience 18, Iter 96, disc loss: 0.0024698784947233383, policy loss: 7.190348615006435
Experience 18, Iter 97, disc loss: 0.0024540888838706174, policy loss: 7.399768819851247
Experience 18, Iter 98, disc loss: 0.0024751099357070336, policy loss: 7.642347323735271
Experience 18, Iter 99, disc loss: 0.0027999822704817138, policy loss: 6.820006180032761
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1663],
        [1.3319],
        [0.0256]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0075, 0.1097, 1.2097, 0.0219, 0.0196, 4.0439]],

        [[0.0075, 0.1097, 1.2097, 0.0219, 0.0196, 4.0439]],

        [[0.0075, 0.1097, 1.2097, 0.0219, 0.0196, 4.0439]],

        [[0.0075, 0.1097, 1.2097, 0.0219, 0.0196, 4.0439]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0100, 0.6652, 5.3278, 0.1024], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0100, 0.6652, 5.3278, 0.1024])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.932
Iter 2/2000 - Loss: 3.162
Iter 3/2000 - Loss: 2.799
Iter 4/2000 - Loss: 2.839
Iter 5/2000 - Loss: 2.917
Iter 6/2000 - Loss: 2.810
Iter 7/2000 - Loss: 2.649
Iter 8/2000 - Loss: 2.542
Iter 9/2000 - Loss: 2.494
Iter 10/2000 - Loss: 2.443
Iter 11/2000 - Loss: 2.337
Iter 12/2000 - Loss: 2.174
Iter 13/2000 - Loss: 1.984
Iter 14/2000 - Loss: 1.797
Iter 15/2000 - Loss: 1.621
Iter 16/2000 - Loss: 1.442
Iter 17/2000 - Loss: 1.239
Iter 18/2000 - Loss: 1.002
Iter 19/2000 - Loss: 0.736
Iter 20/2000 - Loss: 0.452
Iter 1981/2000 - Loss: -7.987
Iter 1982/2000 - Loss: -7.987
Iter 1983/2000 - Loss: -7.988
Iter 1984/2000 - Loss: -7.988
Iter 1985/2000 - Loss: -7.988
Iter 1986/2000 - Loss: -7.988
Iter 1987/2000 - Loss: -7.988
Iter 1988/2000 - Loss: -7.988
Iter 1989/2000 - Loss: -7.988
Iter 1990/2000 - Loss: -7.988
Iter 1991/2000 - Loss: -7.988
Iter 1992/2000 - Loss: -7.988
Iter 1993/2000 - Loss: -7.988
Iter 1994/2000 - Loss: -7.988
Iter 1995/2000 - Loss: -7.988
Iter 1996/2000 - Loss: -7.988
Iter 1997/2000 - Loss: -7.988
Iter 1998/2000 - Loss: -7.988
Iter 1999/2000 - Loss: -7.988
Iter 2000/2000 - Loss: -7.988
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.2606,  5.3717, 35.0122,  3.5109,  1.5208, 46.8317]],

        [[15.4651, 29.3600,  8.7916,  1.3078,  2.9442, 22.9373]],

        [[16.5487, 27.5185,  9.5008,  1.1967,  1.2323, 23.3132]],

        [[12.5963, 22.8968, 16.0979,  1.6353,  2.1052, 38.2729]]])
Signal Variance: tensor([ 0.0562,  1.9679, 18.3822,  0.5670])
Estimated target variance: tensor([0.0100, 0.6652, 5.3278, 0.1024])
N: 190
Signal to noise ratio: tensor([13.8252, 81.6135, 98.2096, 44.2313])
Bound on condition number: tensor([  36316.6590, 1265545.7110, 1832573.3944,  371718.2735])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.002585543935179824, policy loss: 7.044676804893331
Experience 19, Iter 1, disc loss: 0.002443110105399402, policy loss: 7.088847209714504
Experience 19, Iter 2, disc loss: 0.0023134400436245275, policy loss: 7.768049327076538
Experience 19, Iter 3, disc loss: 0.002523969992089469, policy loss: 7.175105292215935
Experience 19, Iter 4, disc loss: 0.0022678003363273656, policy loss: 7.236662914884173
Experience 19, Iter 5, disc loss: 0.0025405223953521515, policy loss: 7.146001698441779
Experience 19, Iter 6, disc loss: 0.0026785856782576105, policy loss: 6.922836015567021
Experience 19, Iter 7, disc loss: 0.0023404310051760454, policy loss: 7.142747006761202
Experience 19, Iter 8, disc loss: 0.002413239030110803, policy loss: 7.308450633964343
Experience 19, Iter 9, disc loss: 0.002275347832166054, policy loss: 7.215388070188979
Experience 19, Iter 10, disc loss: 0.002489374101699333, policy loss: 7.0372293215028225
Experience 19, Iter 11, disc loss: 0.002561164268094954, policy loss: 7.204801000690109
Experience 19, Iter 12, disc loss: 0.0027612087373247423, policy loss: 6.935578875781865
Experience 19, Iter 13, disc loss: 0.0024330448556449122, policy loss: 7.177795875176578
Experience 19, Iter 14, disc loss: 0.002663823644704569, policy loss: 7.223056406676879
Experience 19, Iter 15, disc loss: 0.00269459649843554, policy loss: 7.297093225274309
Experience 19, Iter 16, disc loss: 0.0021097595986287986, policy loss: 7.554346511528206
Experience 19, Iter 17, disc loss: 0.002455415913129034, policy loss: 7.250837138375789
Experience 19, Iter 18, disc loss: 0.00225111610300797, policy loss: 7.51474372135795
Experience 19, Iter 19, disc loss: 0.0025088328142290686, policy loss: 7.203400198063049
Experience 19, Iter 20, disc loss: 0.0023083859043191235, policy loss: 7.239581372453048
Experience 19, Iter 21, disc loss: 0.0023637534939906337, policy loss: 7.29946076627186
Experience 19, Iter 22, disc loss: 0.0024931286405274456, policy loss: 7.093709139494164
Experience 19, Iter 23, disc loss: 0.0022756412506205513, policy loss: 7.531385326511039
Experience 19, Iter 24, disc loss: 0.002141301031616526, policy loss: 7.330529765203396
Experience 19, Iter 25, disc loss: 0.002560436174733402, policy loss: 7.203397764682838
Experience 19, Iter 26, disc loss: 0.0022398991534448654, policy loss: 7.6110129716816015
Experience 19, Iter 27, disc loss: 0.0022656496356921194, policy loss: 7.795859849381465
Experience 19, Iter 28, disc loss: 0.0024920532227375443, policy loss: 7.518639224075631
Experience 19, Iter 29, disc loss: 0.002149420711007935, policy loss: 7.3689485933162615
Experience 19, Iter 30, disc loss: 0.0022643463338778472, policy loss: 7.319186543199832
Experience 19, Iter 31, disc loss: 0.0022615007704595187, policy loss: 7.1581546654446875
Experience 19, Iter 32, disc loss: 0.0022584763308692722, policy loss: 7.201265280740342
Experience 19, Iter 33, disc loss: 0.002189631050995707, policy loss: 7.226181950914599
Experience 19, Iter 34, disc loss: 0.0021546711841655916, policy loss: 7.570123926122813
Experience 19, Iter 35, disc loss: 0.002257590081684245, policy loss: 7.632366245148223
Experience 19, Iter 36, disc loss: 0.002136425054125329, policy loss: 7.30342163288633
Experience 19, Iter 37, disc loss: 0.002268382951225331, policy loss: 7.334581670525551
Experience 19, Iter 38, disc loss: 0.002058584232619238, policy loss: 7.675610668149788
Experience 19, Iter 39, disc loss: 0.0021112389329363984, policy loss: 7.553075700815411
Experience 19, Iter 40, disc loss: 0.002213564131047256, policy loss: 7.3117848496367035
Experience 19, Iter 41, disc loss: 0.002014859775011568, policy loss: 7.585128521540248
Experience 19, Iter 42, disc loss: 0.0024592292200449877, policy loss: 7.482842198190225
Experience 19, Iter 43, disc loss: 0.0020946253156387947, policy loss: 7.965279740550396
Experience 19, Iter 44, disc loss: 0.0022804767376821376, policy loss: 7.051614240481923
Experience 19, Iter 45, disc loss: 0.002173762732622696, policy loss: 7.447686265902744
Experience 19, Iter 46, disc loss: 0.002229615987839913, policy loss: 7.325291765193347
Experience 19, Iter 47, disc loss: 0.0021449098647318607, policy loss: 7.4432223199395295
Experience 19, Iter 48, disc loss: 0.0024331113037405127, policy loss: 7.036571653775789
Experience 19, Iter 49, disc loss: 0.0020880848243813926, policy loss: 7.363481832688257
Experience 19, Iter 50, disc loss: 0.0021080950800860094, policy loss: 7.505876561459317
Experience 19, Iter 51, disc loss: 0.002196762717572978, policy loss: 7.5518185229835
Experience 19, Iter 52, disc loss: 0.0019033662385639497, policy loss: 7.498683073344804
Experience 19, Iter 53, disc loss: 0.001986871448445714, policy loss: 7.654162874837202
Experience 19, Iter 54, disc loss: 0.002134809630036199, policy loss: 7.595996895854979
Experience 19, Iter 55, disc loss: 0.0022076875407223196, policy loss: 7.760689897909325
Experience 19, Iter 56, disc loss: 0.0021162811118433248, policy loss: 7.265128825993248
Experience 19, Iter 57, disc loss: 0.0020111428659788356, policy loss: 7.503162951946059
Experience 19, Iter 58, disc loss: 0.002232320757461377, policy loss: 6.991977874347646
Experience 19, Iter 59, disc loss: 0.002001290522500183, policy loss: 7.639893060067649
Experience 19, Iter 60, disc loss: 0.00216635527771066, policy loss: 7.160790132756626
Experience 19, Iter 61, disc loss: 0.002183608315319165, policy loss: 7.251093854321048
Experience 19, Iter 62, disc loss: 0.0021204184086834744, policy loss: 7.214585547820654
Experience 19, Iter 63, disc loss: 0.0019247440853458106, policy loss: 7.772975083132309
Experience 19, Iter 64, disc loss: 0.0021721614085712902, policy loss: 7.142716542198778
Experience 19, Iter 65, disc loss: 0.0023181253781568586, policy loss: 7.111130000149533
Experience 19, Iter 66, disc loss: 0.0022133358394076444, policy loss: 7.434728149564885
Experience 19, Iter 67, disc loss: 0.002224828175259737, policy loss: 7.1067318276157465
Experience 19, Iter 68, disc loss: 0.0021459095164926057, policy loss: 7.331090299585314
Experience 19, Iter 69, disc loss: 0.0020702871329666217, policy loss: 7.245811021585193
Experience 19, Iter 70, disc loss: 0.001955548136192915, policy loss: 7.524425771675448
Experience 19, Iter 71, disc loss: 0.002128111987922042, policy loss: 7.419542891878985
Experience 19, Iter 72, disc loss: 0.0020966949135763835, policy loss: 7.463179667556995
Experience 19, Iter 73, disc loss: 0.002202326794897428, policy loss: 6.975190436436543
Experience 19, Iter 74, disc loss: 0.0019050825048082657, policy loss: 7.440902526578868
Experience 19, Iter 75, disc loss: 0.001833014129754797, policy loss: 7.988353760761797
Experience 19, Iter 76, disc loss: 0.0021639212656120207, policy loss: 7.492075470841745
Experience 19, Iter 77, disc loss: 0.001920549583106523, policy loss: 7.745066083253759
Experience 19, Iter 78, disc loss: 0.0020191053823387104, policy loss: 7.244867188577727
Experience 19, Iter 79, disc loss: 0.001803085909711208, policy loss: 7.8662492013548535
Experience 19, Iter 80, disc loss: 0.0018260393225811098, policy loss: 7.467944801089452
Experience 19, Iter 81, disc loss: 0.001892106295654609, policy loss: 7.5372170520805755
Experience 19, Iter 82, disc loss: 0.001965379164088574, policy loss: 7.511063518000898
Experience 19, Iter 83, disc loss: 0.0019286014748864633, policy loss: 7.958088265190633
Experience 19, Iter 84, disc loss: 0.0017188305542906858, policy loss: 7.969140156498959
Experience 19, Iter 85, disc loss: 0.002096485844110995, policy loss: 7.429516532124938
Experience 19, Iter 86, disc loss: 0.001976706129970107, policy loss: 7.65875106920239
Experience 19, Iter 87, disc loss: 0.0022099676543956333, policy loss: 7.114900676740998
Experience 19, Iter 88, disc loss: 0.001901353284423429, policy loss: 7.578367938060639
Experience 19, Iter 89, disc loss: 0.001879571766680715, policy loss: 7.593757288178666
Experience 19, Iter 90, disc loss: 0.0017335299703515766, policy loss: 7.810373121942793
Experience 19, Iter 91, disc loss: 0.0017786152725280217, policy loss: 7.5575352629109736
Experience 19, Iter 92, disc loss: 0.001755100500420495, policy loss: 7.546826374858114
Experience 19, Iter 93, disc loss: 0.0018263336618909842, policy loss: 7.553994877965054
Experience 19, Iter 94, disc loss: 0.0019185883599310662, policy loss: 7.658097017265326
Experience 19, Iter 95, disc loss: 0.001695784631859572, policy loss: 7.682316741028045
Experience 19, Iter 96, disc loss: 0.0018467051212134157, policy loss: 7.909521719173631
Experience 19, Iter 97, disc loss: 0.0017466912622544882, policy loss: 8.038174441590819
Experience 19, Iter 98, disc loss: 0.001852567144629395, policy loss: 7.3816148606458585
Experience 19, Iter 99, disc loss: 0.0018625462122259088, policy loss: 7.67984825863723
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1726],
        [1.3771],
        [0.0263]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0072, 0.1121, 1.2502, 0.0226, 0.0204, 4.2232]],

        [[0.0072, 0.1121, 1.2502, 0.0226, 0.0204, 4.2232]],

        [[0.0072, 0.1121, 1.2502, 0.0226, 0.0204, 4.2232]],

        [[0.0072, 0.1121, 1.2502, 0.0226, 0.0204, 4.2232]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0102, 0.6903, 5.5083, 0.1054], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0102, 0.6903, 5.5083, 0.1054])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.995
Iter 2/2000 - Loss: 3.214
Iter 3/2000 - Loss: 2.853
Iter 4/2000 - Loss: 2.886
Iter 5/2000 - Loss: 2.960
Iter 6/2000 - Loss: 2.851
Iter 7/2000 - Loss: 2.684
Iter 8/2000 - Loss: 2.569
Iter 9/2000 - Loss: 2.512
Iter 10/2000 - Loss: 2.454
Iter 11/2000 - Loss: 2.341
Iter 12/2000 - Loss: 2.169
Iter 13/2000 - Loss: 1.970
Iter 14/2000 - Loss: 1.772
Iter 15/2000 - Loss: 1.585
Iter 16/2000 - Loss: 1.395
Iter 17/2000 - Loss: 1.182
Iter 18/2000 - Loss: 0.936
Iter 19/2000 - Loss: 0.660
Iter 20/2000 - Loss: 0.367
Iter 1981/2000 - Loss: -8.039
Iter 1982/2000 - Loss: -8.039
Iter 1983/2000 - Loss: -8.039
Iter 1984/2000 - Loss: -8.039
Iter 1985/2000 - Loss: -8.039
Iter 1986/2000 - Loss: -8.039
Iter 1987/2000 - Loss: -8.039
Iter 1988/2000 - Loss: -8.039
Iter 1989/2000 - Loss: -8.040
Iter 1990/2000 - Loss: -8.040
Iter 1991/2000 - Loss: -8.040
Iter 1992/2000 - Loss: -8.040
Iter 1993/2000 - Loss: -8.040
Iter 1994/2000 - Loss: -8.040
Iter 1995/2000 - Loss: -8.040
Iter 1996/2000 - Loss: -8.040
Iter 1997/2000 - Loss: -8.040
Iter 1998/2000 - Loss: -8.040
Iter 1999/2000 - Loss: -8.040
Iter 2000/2000 - Loss: -8.040
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.1828,  5.3198, 29.4999,  3.4673,  1.4942, 45.6538]],

        [[15.0361, 28.3127,  8.8709,  1.3193,  2.9234, 23.2284]],

        [[16.0454, 27.2373,  9.1165,  1.1823,  1.2903, 20.8678]],

        [[12.2340, 22.9331, 16.9196,  1.8949,  2.1911, 37.7258]]])
Signal Variance: tensor([ 0.0559,  2.0193, 15.7699,  0.6300])
Estimated target variance: tensor([0.0102, 0.6903, 5.5083, 0.1054])
N: 200
Signal to noise ratio: tensor([13.9989, 82.0345, 90.2322, 45.9125])
Bound on condition number: tensor([  39194.9206, 1345932.9281, 1628370.8262,  421592.3440])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0018159554707223718, policy loss: 7.702881346503615
Experience 20, Iter 1, disc loss: 0.0017642750189440898, policy loss: 7.602179435192703
Experience 20, Iter 2, disc loss: 0.0017610965152355317, policy loss: 7.897377054691081
Experience 20, Iter 3, disc loss: 0.0018831402441711253, policy loss: 7.360765477450531
Experience 20, Iter 4, disc loss: 0.0018328681378403104, policy loss: 7.756937869250192
Experience 20, Iter 5, disc loss: 0.0018897920171642153, policy loss: 7.554559600797832
Experience 20, Iter 6, disc loss: 0.0019069276635707373, policy loss: 7.3831963451519265
Experience 20, Iter 7, disc loss: 0.0018707570900172732, policy loss: 7.553179858925126
Experience 20, Iter 8, disc loss: 0.0021543265707906862, policy loss: 7.1706757064454845
Experience 20, Iter 9, disc loss: 0.002023644508364609, policy loss: 7.2703396338076995
Experience 20, Iter 10, disc loss: 0.001994616998125647, policy loss: 7.251128948363261
Experience 20, Iter 11, disc loss: 0.0020161394801703632, policy loss: 7.603485527751782
Experience 20, Iter 12, disc loss: 0.0021124403929904953, policy loss: 7.097222782494226
Experience 20, Iter 13, disc loss: 0.001871757118887925, policy loss: 7.453300540535126
Experience 20, Iter 14, disc loss: 0.0019560779570633857, policy loss: 7.323956979768151
Experience 20, Iter 15, disc loss: 0.001941904488759862, policy loss: 7.402489690518751
Experience 20, Iter 16, disc loss: 0.0018600079287095978, policy loss: 7.260374314875996
Experience 20, Iter 17, disc loss: 0.0019933460743355473, policy loss: 7.417838223182417
Experience 20, Iter 18, disc loss: 0.00197125165030362, policy loss: 7.377611725057346
Experience 20, Iter 19, disc loss: 0.0018307735971545603, policy loss: 8.030660096106812
Experience 20, Iter 20, disc loss: 0.0017580646700465653, policy loss: 7.5400009247035715
Experience 20, Iter 21, disc loss: 0.002039049211032176, policy loss: 7.402431338686732
Experience 20, Iter 22, disc loss: 0.002064018536408909, policy loss: 7.335046910288364
Experience 20, Iter 23, disc loss: 0.0020855070033263617, policy loss: 7.133231662848226
Experience 20, Iter 24, disc loss: 0.001872532312055367, policy loss: 7.862601531581859
Experience 20, Iter 25, disc loss: 0.0017354303581182365, policy loss: 7.496944336393547
Experience 20, Iter 26, disc loss: 0.0018538529039283116, policy loss: 7.93907882384896
Experience 20, Iter 27, disc loss: 0.0019171940619044502, policy loss: 7.428110437746536
Experience 20, Iter 28, disc loss: 0.001745588137943613, policy loss: 8.332006305641958
Experience 20, Iter 29, disc loss: 0.0016519873327862206, policy loss: 8.634647638592146
Experience 20, Iter 30, disc loss: 0.001841619828835914, policy loss: 7.491667648167162
Experience 20, Iter 31, disc loss: 0.0017840469523970484, policy loss: 7.454644403084674
Experience 20, Iter 32, disc loss: 0.0017200807554524028, policy loss: 7.444507879598513
Experience 20, Iter 33, disc loss: 0.001924924740373795, policy loss: 7.337706640376531
Experience 20, Iter 34, disc loss: 0.0017501320972812958, policy loss: 7.545238968456583
Experience 20, Iter 35, disc loss: 0.0019168466718509473, policy loss: 7.656739747860737
Experience 20, Iter 36, disc loss: 0.0017666758262974806, policy loss: 7.937199168727542
Experience 20, Iter 37, disc loss: 0.001814477124237914, policy loss: 7.607225961121278
Experience 20, Iter 38, disc loss: 0.0016028220832615383, policy loss: 7.8887845587092675
Experience 20, Iter 39, disc loss: 0.0018927353088662824, policy loss: 7.6148741500150035
Experience 20, Iter 40, disc loss: 0.001790878542403666, policy loss: 7.582265925164914
Experience 20, Iter 41, disc loss: 0.0018954595812567569, policy loss: 7.311799371496088
Experience 20, Iter 42, disc loss: 0.0018493906959064744, policy loss: 7.505753129061203
Experience 20, Iter 43, disc loss: 0.0018609529472433024, policy loss: 7.780656573444868
Experience 20, Iter 44, disc loss: 0.00180561812868803, policy loss: 7.314705130121903
Experience 20, Iter 45, disc loss: 0.0018308104352619719, policy loss: 7.8962998114285785
Experience 20, Iter 46, disc loss: 0.0016606076727207863, policy loss: 7.873851847517507
Experience 20, Iter 47, disc loss: 0.0019582710682414343, policy loss: 7.392863378631276
Experience 20, Iter 48, disc loss: 0.0016381769416559933, policy loss: 7.702161620207198
Experience 20, Iter 49, disc loss: 0.0016047282483901242, policy loss: 7.996076130600891
Experience 20, Iter 50, disc loss: 0.0019089156673686464, policy loss: 7.501161817208173
Experience 20, Iter 51, disc loss: 0.0017315413819422015, policy loss: 7.41293031344135
Experience 20, Iter 52, disc loss: 0.0018225308141961278, policy loss: 7.710817798927952
Experience 20, Iter 53, disc loss: 0.001734661235811652, policy loss: 7.818198394240445
Experience 20, Iter 54, disc loss: 0.0017746685673755657, policy loss: 7.494000403939802
Experience 20, Iter 55, disc loss: 0.0017510714906286051, policy loss: 7.580940214778195
Experience 20, Iter 56, disc loss: 0.0015308927246062301, policy loss: 8.172931906831646
Experience 20, Iter 57, disc loss: 0.001775150076287675, policy loss: 7.317946637236881
Experience 20, Iter 58, disc loss: 0.0015959318385652204, policy loss: 7.894486739432894
Experience 20, Iter 59, disc loss: 0.0018212786959159526, policy loss: 7.38119816175308
Experience 20, Iter 60, disc loss: 0.001748983911816326, policy loss: 7.592412717582018
Experience 20, Iter 61, disc loss: 0.0015784115441898174, policy loss: 7.646219984444538
Experience 20, Iter 62, disc loss: 0.0016548046123915537, policy loss: 7.6687256737251905
Experience 20, Iter 63, disc loss: 0.0017384794372561872, policy loss: 7.515249043075307
Experience 20, Iter 64, disc loss: 0.0015481689976953776, policy loss: 7.736790316561956
Experience 20, Iter 65, disc loss: 0.0017548332208179038, policy loss: 7.632514483189014
Experience 20, Iter 66, disc loss: 0.001517006469790855, policy loss: 8.471864315564591
Experience 20, Iter 67, disc loss: 0.0013751324097469343, policy loss: 8.02557130854559
Experience 20, Iter 68, disc loss: 0.0016529276336944383, policy loss: 7.603254504967813
Experience 20, Iter 69, disc loss: 0.001738965994813823, policy loss: 7.454197382547233
Experience 20, Iter 70, disc loss: 0.0017887452600450142, policy loss: 7.339535681630247
Experience 20, Iter 71, disc loss: 0.001581768888079143, policy loss: 7.796618297699203
Experience 20, Iter 72, disc loss: 0.0016689892700802412, policy loss: 7.402183414211359
Experience 20, Iter 73, disc loss: 0.001562361741088581, policy loss: 7.856483982483325
Experience 20, Iter 74, disc loss: 0.0016494641323338611, policy loss: 7.542910789883153
Experience 20, Iter 75, disc loss: 0.001460755376202712, policy loss: 8.020651804090628
Experience 20, Iter 76, disc loss: 0.0015716088761354506, policy loss: 8.034341484875263
Experience 20, Iter 77, disc loss: 0.0016126199660828726, policy loss: 7.643935575854147
Experience 20, Iter 78, disc loss: 0.0013420149228467568, policy loss: 8.243066369640143
Experience 20, Iter 79, disc loss: 0.0015930981960178973, policy loss: 7.673724742328359
Experience 20, Iter 80, disc loss: 0.0015923665964276685, policy loss: 7.635535629685885
Experience 20, Iter 81, disc loss: 0.00154980158352645, policy loss: 7.5487125966334
Experience 20, Iter 82, disc loss: 0.0016858417279001277, policy loss: 7.534624592431969
Experience 20, Iter 83, disc loss: 0.0017104675140282009, policy loss: 7.396744075317024
Experience 20, Iter 84, disc loss: 0.0015380137957212022, policy loss: 7.64340811228773
Experience 20, Iter 85, disc loss: 0.0015816167387246302, policy loss: 7.489327992839318
Experience 20, Iter 86, disc loss: 0.0016269680928189703, policy loss: 7.54869370372829
Experience 20, Iter 87, disc loss: 0.0018179247769459605, policy loss: 7.505500178070969
Experience 20, Iter 88, disc loss: 0.001853698362591465, policy loss: 7.3372234135701895
Experience 20, Iter 89, disc loss: 0.001625263520763377, policy loss: 7.758637299611049
Experience 20, Iter 90, disc loss: 0.0015978241801054992, policy loss: 7.80666558659636
Experience 20, Iter 91, disc loss: 0.0016984415890485003, policy loss: 7.887134184652
Experience 20, Iter 92, disc loss: 0.001691302448325528, policy loss: 7.513342617739039
Experience 20, Iter 93, disc loss: 0.0015028938481115664, policy loss: 7.785994242787172
Experience 20, Iter 94, disc loss: 0.0017210460890265528, policy loss: 7.373853105030976
Experience 20, Iter 95, disc loss: 0.0017079570137595744, policy loss: 7.906658021060947
Experience 20, Iter 96, disc loss: 0.0016711523515785774, policy loss: 7.346843426826238
Experience 20, Iter 97, disc loss: 0.0016806970841376236, policy loss: 7.7701505749125275
Experience 20, Iter 98, disc loss: 0.0014765139036263827, policy loss: 7.875532357960524
Experience 20, Iter 99, disc loss: 0.0015186120885120652, policy loss: 7.88876191493544
