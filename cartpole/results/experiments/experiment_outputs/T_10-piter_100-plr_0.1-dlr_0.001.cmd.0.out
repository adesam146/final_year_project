Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0130],
        [0.8980],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0518, 3.5922, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 1.196
Iter 3/2000 - Loss: 0.953
Iter 4/2000 - Loss: 1.383
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.493
Iter 7/2000 - Loss: 1.248
Iter 8/2000 - Loss: 1.053
Iter 9/2000 - Loss: 0.964
Iter 10/2000 - Loss: 0.972
Iter 11/2000 - Loss: 1.033
Iter 12/2000 - Loss: 1.091
Iter 13/2000 - Loss: 1.105
Iter 14/2000 - Loss: 1.071
Iter 15/2000 - Loss: 1.012
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.889
Iter 20/2000 - Loss: 0.912
Iter 1981/2000 - Loss: 0.698
Iter 1982/2000 - Loss: 0.698
Iter 1983/2000 - Loss: 0.698
Iter 1984/2000 - Loss: 0.698
Iter 1985/2000 - Loss: 0.698
Iter 1986/2000 - Loss: 0.698
Iter 1987/2000 - Loss: 0.698
Iter 1988/2000 - Loss: 0.698
Iter 1989/2000 - Loss: 0.698
Iter 1990/2000 - Loss: 0.698
Iter 1991/2000 - Loss: 0.698
Iter 1992/2000 - Loss: 0.698
Iter 1993/2000 - Loss: 0.698
Iter 1994/2000 - Loss: 0.698
Iter 1995/2000 - Loss: 0.698
Iter 1996/2000 - Loss: 0.698
Iter 1997/2000 - Loss: 0.698
Iter 1998/2000 - Loss: 0.698
Iter 1999/2000 - Loss: 0.698
Iter 2000/2000 - Loss: 0.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0093],
        [0.4134],
        [0.0149]])
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]])
Signal Variance: tensor([0.0033, 0.0374, 2.8196, 0.0608])
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([1.9681, 2.0097, 2.6117, 2.0186])
Bound on condition number: tensor([39.7326, 41.3892, 69.2095, 41.7460])
Policy Optimizer learning rate:
0.1
Experience 1, Iter 0, disc loss: 1.2474460746714535, policy loss: 0.7896661085587864
Experience 1, Iter 1, disc loss: 1.2385534174313004, policy loss: 0.7873818506933048
Experience 1, Iter 2, disc loss: 1.221287701433177, policy loss: 0.7971586923306344
Experience 1, Iter 3, disc loss: 1.2120400761679528, policy loss: 0.7965297759401154
Experience 1, Iter 4, disc loss: 1.1978804054077716, policy loss: 0.8019454908319006
Experience 1, Iter 5, disc loss: 1.1900256674011445, policy loss: 0.8005868053649651
Experience 1, Iter 6, disc loss: 1.1811051548116078, policy loss: 0.7995218106222612
Experience 1, Iter 7, disc loss: 1.1722305983717825, policy loss: 0.8010936074968416
Experience 1, Iter 8, disc loss: 1.1530991791718836, policy loss: 0.813266633799753
Experience 1, Iter 9, disc loss: 1.1224154675113953, policy loss: 0.8429442610524513
Experience 1, Iter 10, disc loss: 1.137940023549, policy loss: 0.8168081913335836
Experience 1, Iter 11, disc loss: 1.1172319238089976, policy loss: 0.8307872454493004
Experience 1, Iter 12, disc loss: 1.098983107404453, policy loss: 0.8478118590705999
Experience 1, Iter 13, disc loss: 1.1018775601337427, policy loss: 0.8298852632917657
Experience 1, Iter 14, disc loss: 1.0776469578454368, policy loss: 0.8528750422190298
Experience 1, Iter 15, disc loss: 1.066983977471272, policy loss: 0.8582052458934089
Experience 1, Iter 16, disc loss: 1.0539035435447968, policy loss: 0.8622547025279359
Experience 1, Iter 17, disc loss: 1.0581101219827023, policy loss: 0.8483690288433328
Experience 1, Iter 18, disc loss: 1.0190388534949555, policy loss: 0.8856889664332229
Experience 1, Iter 19, disc loss: 1.0282522219158672, policy loss: 0.8659167600619706
Experience 1, Iter 20, disc loss: 1.0055492448117576, policy loss: 0.8822799071249742
Experience 1, Iter 21, disc loss: 0.991123756141371, policy loss: 0.8927324216946204
Experience 1, Iter 22, disc loss: 0.975604619179942, policy loss: 0.9004524381949457
Experience 1, Iter 23, disc loss: 0.9525134027634273, policy loss: 0.9241171824435054
Experience 1, Iter 24, disc loss: 0.969899798343897, policy loss: 0.8907429526136229
Experience 1, Iter 25, disc loss: 0.9357404026567325, policy loss: 0.9279121777910799
Experience 1, Iter 26, disc loss: 0.9433129807100223, policy loss: 0.9104318302053871
Experience 1, Iter 27, disc loss: 0.894084697331267, policy loss: 0.9704382097947319
Experience 1, Iter 28, disc loss: 0.8965264134730262, policy loss: 0.9472825446499394
Experience 1, Iter 29, disc loss: 0.87480869643604, policy loss: 0.9639585719096777
Experience 1, Iter 30, disc loss: 0.8543100549709655, policy loss: 0.9879334004008167
Experience 1, Iter 31, disc loss: 0.8306405877361533, policy loss: 1.0119252158915553
Experience 1, Iter 32, disc loss: 0.8303192459924558, policy loss: 1.0040086537944237
Experience 1, Iter 33, disc loss: 0.8114109028394345, policy loss: 1.0324556258741386
Experience 1, Iter 34, disc loss: 0.7925579457482074, policy loss: 1.0438225775599603
Experience 1, Iter 35, disc loss: 0.765583400817435, policy loss: 1.0864921258604068
Experience 1, Iter 36, disc loss: 0.745028148290485, policy loss: 1.099258059904606
Experience 1, Iter 37, disc loss: 0.7451762616574403, policy loss: 1.0895094252407094
Experience 1, Iter 38, disc loss: 0.7391954547212884, policy loss: 1.086437955293336
Experience 1, Iter 39, disc loss: 0.7031286798750958, policy loss: 1.1408587100858498
Experience 1, Iter 40, disc loss: 0.7074782242829716, policy loss: 1.1217064390395648
Experience 1, Iter 41, disc loss: 0.6955391952503188, policy loss: 1.125500227995175
Experience 1, Iter 42, disc loss: 0.6716118386293704, policy loss: 1.1548474968871274
Experience 1, Iter 43, disc loss: 0.6463576310722516, policy loss: 1.198557543359421
Experience 1, Iter 44, disc loss: 0.6482399036655055, policy loss: 1.179948184425072
Experience 1, Iter 45, disc loss: 0.6435355654110662, policy loss: 1.1688867615825398
Experience 1, Iter 46, disc loss: 0.6185024467568553, policy loss: 1.2132924397020886
Experience 1, Iter 47, disc loss: 0.6347635953069236, policy loss: 1.1773094257771437
Experience 1, Iter 48, disc loss: 0.5892366858870546, policy loss: 1.2653826708504
Experience 1, Iter 49, disc loss: 0.5718250706426055, policy loss: 1.290396293824924
Experience 1, Iter 50, disc loss: 0.5457168491848015, policy loss: 1.3307761132762321
Experience 1, Iter 51, disc loss: 0.5465762520859853, policy loss: 1.3109643751694884
Experience 1, Iter 52, disc loss: 0.5410711372619583, policy loss: 1.3517537134370485
Experience 1, Iter 53, disc loss: 0.5376693384729834, policy loss: 1.322420029605074
Experience 1, Iter 54, disc loss: 0.49414520615879765, policy loss: 1.419812699223686
Experience 1, Iter 55, disc loss: 0.47135273002537204, policy loss: 1.4529450823720742
Experience 1, Iter 56, disc loss: 0.468311826052745, policy loss: 1.444285813567221
Experience 1, Iter 57, disc loss: 0.48122364327408296, policy loss: 1.4165388226252902
Experience 1, Iter 58, disc loss: 0.4412871257954926, policy loss: 1.5047575678994085
Experience 1, Iter 59, disc loss: 0.41501821192540417, policy loss: 1.5713903073154243
Experience 1, Iter 60, disc loss: 0.4407498129183174, policy loss: 1.4896728428061994
Experience 1, Iter 61, disc loss: 0.4459727301143649, policy loss: 1.5004045999045088
Experience 1, Iter 62, disc loss: 0.4056767397346523, policy loss: 1.5695316691026993
Experience 1, Iter 63, disc loss: 0.3896843292793018, policy loss: 1.634181182360591
Experience 1, Iter 64, disc loss: 0.40779127653842573, policy loss: 1.5841119501365801
Experience 1, Iter 65, disc loss: 0.4019642362822795, policy loss: 1.5955005892595735
Experience 1, Iter 66, disc loss: 0.3713572800859089, policy loss: 1.658326579552166
Experience 1, Iter 67, disc loss: 0.3826829391163854, policy loss: 1.5965098721314162
Experience 1, Iter 68, disc loss: 0.3484773335296942, policy loss: 1.7236334094447927
Experience 1, Iter 69, disc loss: 0.34141714093009334, policy loss: 1.7296994999603672
Experience 1, Iter 70, disc loss: 0.3264153496469248, policy loss: 1.7894945714734032
Experience 1, Iter 71, disc loss: 0.3106781922746784, policy loss: 1.8484601787423134
Experience 1, Iter 72, disc loss: 0.3078763708556841, policy loss: 1.8704288485023162
Experience 1, Iter 73, disc loss: 0.32102200618753396, policy loss: 1.8479767141682872
Experience 1, Iter 74, disc loss: 0.29204290926243687, policy loss: 1.9159827947404597
Experience 1, Iter 75, disc loss: 0.29701704522759165, policy loss: 1.8947795971732493
Experience 1, Iter 76, disc loss: 0.2633525150143007, policy loss: 1.9922984744172054
Experience 1, Iter 77, disc loss: 0.275369459739366, policy loss: 1.9632440728271319
Experience 1, Iter 78, disc loss: 0.2730119014096785, policy loss: 1.9728521857552135
Experience 1, Iter 79, disc loss: 0.2221715742276096, policy loss: 2.1977719840927303
Experience 1, Iter 80, disc loss: 0.264441726988359, policy loss: 2.0002116753775874
Experience 1, Iter 81, disc loss: 0.23908454311314217, policy loss: 2.090089130688219
Experience 1, Iter 82, disc loss: 0.22893403392881362, policy loss: 2.1300748579607323
Experience 1, Iter 83, disc loss: 0.24042158246434336, policy loss: 1.9983986576632964
Experience 1, Iter 84, disc loss: 0.19047596936560718, policy loss: 2.299941132063104
Experience 1, Iter 85, disc loss: 0.21093912974217577, policy loss: 2.2370234715187105
Experience 1, Iter 86, disc loss: 0.21061683507831427, policy loss: 2.171473390664091
Experience 1, Iter 87, disc loss: 0.21281934734756916, policy loss: 2.1308288018455226
Experience 1, Iter 88, disc loss: 0.1892558315146678, policy loss: 2.370423184059933
Experience 1, Iter 89, disc loss: 0.19847094984204544, policy loss: 2.2775141929277534
Experience 1, Iter 90, disc loss: 0.16999988619508397, policy loss: 2.3676991528521016
Experience 1, Iter 91, disc loss: 0.19267485950768537, policy loss: 2.4402930640938507
Experience 1, Iter 92, disc loss: 0.1693914189807277, policy loss: 2.495947315778783
Experience 1, Iter 93, disc loss: 0.16839139738380132, policy loss: 2.540915219004585
Experience 1, Iter 94, disc loss: 0.1785357435396639, policy loss: 2.4052016477165106
Experience 1, Iter 95, disc loss: 0.16760217679083905, policy loss: 2.488984575431509
Experience 1, Iter 96, disc loss: 0.1740335543428581, policy loss: 2.4628969319782694
Experience 1, Iter 97, disc loss: 0.1672871405484412, policy loss: 2.4987395931998155
Experience 1, Iter 98, disc loss: 0.13416985377633545, policy loss: 2.8241374649856956
Experience 1, Iter 99, disc loss: 0.1528688619810205, policy loss: 2.477458209300937
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0146],
        [0.0538],
        [1.6569],
        [0.0317]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1186, 0.5823, 1.4699, 0.0239, 0.0131, 1.4137]],

        [[0.1186, 0.5823, 1.4699, 0.0239, 0.0131, 1.4137]],

        [[0.1186, 0.5823, 1.4699, 0.0239, 0.0131, 1.4137]],

        [[0.1186, 0.5823, 1.4699, 0.0239, 0.0131, 1.4137]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0584, 0.2151, 6.6277, 0.1269], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0584, 0.2151, 6.6277, 0.1269])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.702
Iter 2/2000 - Loss: 4.105
Iter 3/2000 - Loss: 3.726
Iter 4/2000 - Loss: 3.556
Iter 5/2000 - Loss: 3.507
Iter 6/2000 - Loss: 3.504
Iter 7/2000 - Loss: 3.521
Iter 8/2000 - Loss: 3.539
Iter 9/2000 - Loss: 3.536
Iter 10/2000 - Loss: 3.507
Iter 11/2000 - Loss: 3.464
Iter 12/2000 - Loss: 3.422
Iter 13/2000 - Loss: 3.387
Iter 14/2000 - Loss: 3.358
Iter 15/2000 - Loss: 3.335
Iter 16/2000 - Loss: 3.316
Iter 17/2000 - Loss: 3.296
Iter 18/2000 - Loss: 3.268
Iter 19/2000 - Loss: 3.226
Iter 20/2000 - Loss: 3.168
Iter 1981/2000 - Loss: -2.846
Iter 1982/2000 - Loss: -2.847
Iter 1983/2000 - Loss: -2.847
Iter 1984/2000 - Loss: -2.847
Iter 1985/2000 - Loss: -2.847
Iter 1986/2000 - Loss: -2.847
Iter 1987/2000 - Loss: -2.847
Iter 1988/2000 - Loss: -2.847
Iter 1989/2000 - Loss: -2.847
Iter 1990/2000 - Loss: -2.847
Iter 1991/2000 - Loss: -2.847
Iter 1992/2000 - Loss: -2.847
Iter 1993/2000 - Loss: -2.847
Iter 1994/2000 - Loss: -2.847
Iter 1995/2000 - Loss: -2.847
Iter 1996/2000 - Loss: -2.847
Iter 1997/2000 - Loss: -2.847
Iter 1998/2000 - Loss: -2.847
Iter 1999/2000 - Loss: -2.848
Iter 2000/2000 - Loss: -2.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0002],
        [0.0012],
        [0.0006]])
Lengthscale: tensor([[[22.4270,  9.0906, 67.9791, 19.6542, 14.8335, 50.9473]],

        [[26.7993, 46.8139,  6.7224,  0.8915,  2.3291,  8.6624]],

        [[37.7146, 55.2806, 13.7787,  0.9017,  3.7294, 13.1092]],

        [[45.5404, 86.8762, 20.3963,  4.0507,  6.8656, 52.4671]]])
Signal Variance: tensor([ 0.2041,  0.4090, 12.6908,  0.9702])
Estimated target variance: tensor([0.0584, 0.2151, 6.6277, 0.1269])
N: 20
Signal to noise ratio: tensor([ 22.3826,  43.6875, 101.9567,  39.0224])
Bound on condition number: tensor([ 10020.6506,  38172.8983, 207904.3758,  30455.9174])
Policy Optimizer learning rate:
0.09989469496904545
Experience 2, Iter 0, disc loss: 0.038835175355375055, policy loss: 5.987430010240274
Experience 2, Iter 1, disc loss: 0.040978588161959685, policy loss: 5.517380818289854
Experience 2, Iter 2, disc loss: 0.037971913798006525, policy loss: 5.58715871759464
Experience 2, Iter 3, disc loss: 0.04042017897404525, policy loss: 4.926822463174888
Experience 2, Iter 4, disc loss: 0.05121328203933229, policy loss: 4.0212451825369735
Experience 2, Iter 5, disc loss: 0.1071780769379761, policy loss: 2.705829566805092
Experience 2, Iter 6, disc loss: 0.25693725083090657, policy loss: 1.6906828491885602
Experience 2, Iter 7, disc loss: 0.39444556892032584, policy loss: 1.754853054695953
Experience 2, Iter 8, disc loss: 0.37816265241238756, policy loss: 2.6388721873054166
Experience 2, Iter 9, disc loss: 0.3845453576718196, policy loss: 2.4420144391600673
Experience 2, Iter 10, disc loss: 0.3719964167013824, policy loss: 2.060091831639437
Experience 2, Iter 11, disc loss: 0.3930432511330678, policy loss: 1.5151081210647912
Experience 2, Iter 12, disc loss: 0.26891864162989093, policy loss: 1.6975818951615782
Experience 2, Iter 13, disc loss: 0.2401778086748776, policy loss: 1.8019087862904775
Experience 2, Iter 14, disc loss: 0.19993822983617232, policy loss: 1.975809780967246
Experience 2, Iter 15, disc loss: 0.14933178037086056, policy loss: 2.275642963961165
Experience 2, Iter 16, disc loss: 0.0989358258129909, policy loss: 2.824622477792705
Experience 2, Iter 17, disc loss: 0.07458327605220888, policy loss: 3.25483226126443
Experience 2, Iter 18, disc loss: 0.056215293822198406, policy loss: 3.7122089399148286
Experience 2, Iter 19, disc loss: 0.04971295151320486, policy loss: 3.979297726169806
Experience 2, Iter 20, disc loss: 0.04333569294353483, policy loss: 4.285142723036049
Experience 2, Iter 21, disc loss: 0.035974232067725334, policy loss: 4.710878458336611
Experience 2, Iter 22, disc loss: 0.03159644456515305, policy loss: 5.039886192274215
Experience 2, Iter 23, disc loss: 0.028158174136021608, policy loss: 5.4765279800457005
Experience 2, Iter 24, disc loss: 0.02702015775831714, policy loss: 5.434920280092829
Experience 2, Iter 25, disc loss: 0.025153110206141777, policy loss: 5.642227066823196
Experience 2, Iter 26, disc loss: 0.024255829416413396, policy loss: 5.716723465988236
Experience 2, Iter 27, disc loss: 0.022402133078158793, policy loss: 6.0125077287595134
Experience 2, Iter 28, disc loss: 0.021370828575660444, policy loss: 6.089424062845554
Experience 2, Iter 29, disc loss: 0.020791027653633855, policy loss: 6.19982404607883
Experience 2, Iter 30, disc loss: 0.020195423619053746, policy loss: 6.163649103879566
Experience 2, Iter 31, disc loss: 0.019362811159528717, policy loss: 6.33611273814674
Experience 2, Iter 32, disc loss: 0.018672979079253225, policy loss: 6.418468612914964
Experience 2, Iter 33, disc loss: 0.01803488256280466, policy loss: 6.53128656192259
Experience 2, Iter 34, disc loss: 0.01731418258079528, policy loss: 6.722615768266379
Experience 2, Iter 35, disc loss: 0.016947967514885733, policy loss: 6.699507669708739
Experience 2, Iter 36, disc loss: 0.016597586496162813, policy loss: 6.689641196666173
Experience 2, Iter 37, disc loss: 0.01625642729055772, policy loss: 6.6541652076471065
Experience 2, Iter 38, disc loss: 0.015639436875997982, policy loss: 6.802846825463451
Experience 2, Iter 39, disc loss: 0.015427385584202272, policy loss: 6.7032940040537445
Experience 2, Iter 40, disc loss: 0.015113519797608482, policy loss: 6.762261115793104
Experience 2, Iter 41, disc loss: 0.014535619490295977, policy loss: 6.8653335631504735
Experience 2, Iter 42, disc loss: 0.014218848840393926, policy loss: 6.923060953156763
Experience 2, Iter 43, disc loss: 0.013853374388893594, policy loss: 6.904500335138616
Experience 2, Iter 44, disc loss: 0.013837690665757742, policy loss: 6.737980390379151
Experience 2, Iter 45, disc loss: 0.013179960623260949, policy loss: 7.00682039210162
Experience 2, Iter 46, disc loss: 0.012937242346744215, policy loss: 6.9860425641685415
Experience 2, Iter 47, disc loss: 0.012653200023210841, policy loss: 7.001745880697449
Experience 2, Iter 48, disc loss: 0.012622899429330835, policy loss: 6.830170734335644
Experience 2, Iter 49, disc loss: 0.012142967687243622, policy loss: 7.031879864915249
Experience 2, Iter 50, disc loss: 0.011856175718917053, policy loss: 7.049730936524981
Experience 2, Iter 51, disc loss: 0.011527473239850386, policy loss: 7.161155776714741
Experience 2, Iter 52, disc loss: 0.011357830848279966, policy loss: 7.103174021076209
Experience 2, Iter 53, disc loss: 0.011213590003563883, policy loss: 7.026656714995447
Experience 2, Iter 54, disc loss: 0.011103793803953247, policy loss: 6.935328489095869
Experience 2, Iter 55, disc loss: 0.010993819833953658, policy loss: 6.867525804604626
Experience 2, Iter 56, disc loss: 0.010658778188775989, policy loss: 7.017455872734217
Experience 2, Iter 57, disc loss: 0.010356956078084987, policy loss: 7.05681985512841
Experience 2, Iter 58, disc loss: 0.010190317318766873, policy loss: 7.045713715899117
Experience 2, Iter 59, disc loss: 0.010120794946520698, policy loss: 6.945391805185918
Experience 2, Iter 60, disc loss: 0.009933782632362127, policy loss: 6.979676984052248
Experience 2, Iter 61, disc loss: 0.009780103835869719, policy loss: 6.949374598281709
Experience 2, Iter 62, disc loss: 0.00956229121472292, policy loss: 6.970948999115995
Experience 2, Iter 63, disc loss: 0.009453972963606944, policy loss: 6.965620966441145
Experience 2, Iter 64, disc loss: 0.009170619833343989, policy loss: 7.025254014044947
Experience 2, Iter 65, disc loss: 0.009162796584135923, policy loss: 6.9566675647456755
Experience 2, Iter 66, disc loss: 0.008949994043141637, policy loss: 6.984272580693737
Experience 2, Iter 67, disc loss: 0.008870431517118195, policy loss: 6.956825239548082
Experience 2, Iter 68, disc loss: 0.008648291528875527, policy loss: 6.956871363441728
Experience 2, Iter 69, disc loss: 0.008565297963256426, policy loss: 6.962379521589281
Experience 2, Iter 70, disc loss: 0.0084183945898617, policy loss: 6.9401096645779194
Experience 2, Iter 71, disc loss: 0.008387502936581958, policy loss: 6.979797586469057
Experience 2, Iter 72, disc loss: 0.008260910409137012, policy loss: 6.931384348148069
Experience 2, Iter 73, disc loss: 0.008070347354810641, policy loss: 6.96112182383508
Experience 2, Iter 74, disc loss: 0.008041493083555996, policy loss: 6.9238443036874795
Experience 2, Iter 75, disc loss: 0.007876206645752408, policy loss: 6.913127212093034
Experience 2, Iter 76, disc loss: 0.007861130588599217, policy loss: 6.856808414462176
Experience 2, Iter 77, disc loss: 0.007858224003853585, policy loss: 6.8422992991342415
Experience 2, Iter 78, disc loss: 0.007599920777609429, policy loss: 6.8689360273687665
Experience 2, Iter 79, disc loss: 0.007612646538241457, policy loss: 6.804476198448885
Experience 2, Iter 80, disc loss: 0.007494260072262962, policy loss: 6.859309012748054
Experience 2, Iter 81, disc loss: 0.007522943492388339, policy loss: 6.740639981980202
Experience 2, Iter 82, disc loss: 0.00747093277159785, policy loss: 6.739672569949839
Experience 2, Iter 83, disc loss: 0.007147435968434712, policy loss: 6.819984702188455
Experience 2, Iter 84, disc loss: 0.007280365947871353, policy loss: 6.739207570880721
Experience 2, Iter 85, disc loss: 0.007081736785882826, policy loss: 6.821803655328932
Experience 2, Iter 86, disc loss: 0.006869836848628561, policy loss: 6.8628260341074885
Experience 2, Iter 87, disc loss: 0.006859512580008617, policy loss: 6.818448880550165
Experience 2, Iter 88, disc loss: 0.0069308706925701544, policy loss: 6.692095153066572
Experience 2, Iter 89, disc loss: 0.0067255812926085455, policy loss: 6.746754509325239
Experience 2, Iter 90, disc loss: 0.006872662636371289, policy loss: 6.70230321975773
Experience 2, Iter 91, disc loss: 0.006772163655689538, policy loss: 6.640127513629192
Experience 2, Iter 92, disc loss: 0.0067314464920700656, policy loss: 6.6701319151864
Experience 2, Iter 93, disc loss: 0.006746793064632479, policy loss: 6.618455740720131
Experience 2, Iter 94, disc loss: 0.006346938426857603, policy loss: 6.757692597342402
Experience 2, Iter 95, disc loss: 0.006711506957978052, policy loss: 6.540279806924896
Experience 2, Iter 96, disc loss: 0.006546483474668885, policy loss: 6.561084169542621
Experience 2, Iter 97, disc loss: 0.006204671494570098, policy loss: 6.719391094469419
Experience 2, Iter 98, disc loss: 0.00657490366673766, policy loss: 6.464340110081592
Experience 2, Iter 99, disc loss: 0.006287493006783246, policy loss: 6.6055358734838405
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0120],
        [0.0405],
        [1.4818],
        [0.0285]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1071, 0.4813, 1.3162, 0.0215, 0.0111, 0.9337]],

        [[0.1071, 0.4813, 1.3162, 0.0215, 0.0111, 0.9337]],

        [[0.1071, 0.4813, 1.3162, 0.0215, 0.0111, 0.9337]],

        [[0.1071, 0.4813, 1.3162, 0.0215, 0.0111, 0.9337]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0480, 0.1619, 5.9272, 0.1139], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0480, 0.1619, 5.9272, 0.1139])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.811
Iter 2/2000 - Loss: 4.038
Iter 3/2000 - Loss: 3.540
Iter 4/2000 - Loss: 3.312
Iter 5/2000 - Loss: 3.241
Iter 6/2000 - Loss: 3.232
Iter 7/2000 - Loss: 3.253
Iter 8/2000 - Loss: 3.278
Iter 9/2000 - Loss: 3.285
Iter 10/2000 - Loss: 3.262
Iter 11/2000 - Loss: 3.221
Iter 12/2000 - Loss: 3.177
Iter 13/2000 - Loss: 3.138
Iter 14/2000 - Loss: 3.105
Iter 15/2000 - Loss: 3.075
Iter 16/2000 - Loss: 3.047
Iter 17/2000 - Loss: 3.015
Iter 18/2000 - Loss: 2.976
Iter 19/2000 - Loss: 2.922
Iter 20/2000 - Loss: 2.848
Iter 1981/2000 - Loss: -4.550
Iter 1982/2000 - Loss: -4.550
Iter 1983/2000 - Loss: -4.550
Iter 1984/2000 - Loss: -4.550
Iter 1985/2000 - Loss: -4.551
Iter 1986/2000 - Loss: -4.551
Iter 1987/2000 - Loss: -4.551
Iter 1988/2000 - Loss: -4.551
Iter 1989/2000 - Loss: -4.551
Iter 1990/2000 - Loss: -4.551
Iter 1991/2000 - Loss: -4.551
Iter 1992/2000 - Loss: -4.551
Iter 1993/2000 - Loss: -4.551
Iter 1994/2000 - Loss: -4.551
Iter 1995/2000 - Loss: -4.551
Iter 1996/2000 - Loss: -4.551
Iter 1997/2000 - Loss: -4.551
Iter 1998/2000 - Loss: -4.551
Iter 1999/2000 - Loss: -4.551
Iter 2000/2000 - Loss: -4.552
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0027],
        [0.0005]])
Lengthscale: tensor([[[13.0188, 10.4189, 62.7962, 24.5626, 21.7457, 43.4572]],

        [[36.3232, 55.2104, 11.9936,  1.7400,  1.1796, 15.3128]],

        [[43.8669, 55.2586, 17.5185,  1.0122,  1.8530, 15.3258]],

        [[53.0405, 65.7517, 19.8335,  4.0930,  6.5119, 47.0496]]])
Signal Variance: tensor([ 0.1846,  1.0317, 15.4931,  0.8011])
Estimated target variance: tensor([0.0480, 0.1619, 5.9272, 0.1139])
N: 30
Signal to noise ratio: tensor([22.4392, 43.0059, 76.3081, 40.7368])
Bound on condition number: tensor([ 15106.5901,  55486.1713, 174688.8856,  49785.5655])
Policy Optimizer learning rate:
0.09978950082958633
Experience 3, Iter 0, disc loss: 0.006954353511839423, policy loss: 6.169644995104851
Experience 3, Iter 1, disc loss: 0.007020911736300056, policy loss: 6.108969204486362
Experience 3, Iter 2, disc loss: 0.007512691085655709, policy loss: 5.945549199017735
Experience 3, Iter 3, disc loss: 0.007300062189183286, policy loss: 5.983915229235603
Experience 3, Iter 4, disc loss: 0.0070997060860758495, policy loss: 6.017496000781916
Experience 3, Iter 5, disc loss: 0.006990029174790612, policy loss: 6.0017333438047515
Experience 3, Iter 6, disc loss: 0.0072145826776769855, policy loss: 5.947973823908246
Experience 3, Iter 7, disc loss: 0.006888430933610608, policy loss: 6.024724534835222
Experience 3, Iter 8, disc loss: 0.007144779831747106, policy loss: 5.959151095904733
Experience 3, Iter 9, disc loss: 0.007031500265237585, policy loss: 5.939095635511628
Experience 3, Iter 10, disc loss: 0.007237902924658159, policy loss: 5.847397483548757
Experience 3, Iter 11, disc loss: 0.006970075076468413, policy loss: 5.9057417113575825
Experience 3, Iter 12, disc loss: 0.007581685625192044, policy loss: 5.724060241299887
Experience 3, Iter 13, disc loss: 0.007069849064346585, policy loss: 5.845663276227457
Experience 3, Iter 14, disc loss: 0.007450213981053203, policy loss: 5.740038456729918
Experience 3, Iter 15, disc loss: 0.00768540721705002, policy loss: 5.688313397081974
Experience 3, Iter 16, disc loss: 0.007297883382125659, policy loss: 5.753111590507122
Experience 3, Iter 17, disc loss: 0.007015357459716808, policy loss: 5.844180347618195
Experience 3, Iter 18, disc loss: 0.007602227384095684, policy loss: 5.682059051057934
Experience 3, Iter 19, disc loss: 0.0072144774598318345, policy loss: 5.778286142876768
Experience 3, Iter 20, disc loss: 0.007808119758813116, policy loss: 5.632502757673945
Experience 3, Iter 21, disc loss: 0.0076226536631324605, policy loss: 5.615456088018625
Experience 3, Iter 22, disc loss: 0.007535651337511282, policy loss: 5.622275710553327
Experience 3, Iter 23, disc loss: 0.007599347008770549, policy loss: 5.626834943967788
Experience 3, Iter 24, disc loss: 0.008160856292812822, policy loss: 5.4851782369102295
Experience 3, Iter 25, disc loss: 0.0075662609698782, policy loss: 5.58938948966432
Experience 3, Iter 26, disc loss: 0.007890434358324004, policy loss: 5.504651128418832
Experience 3, Iter 27, disc loss: 0.00796821074455786, policy loss: 5.476227306560931
Experience 3, Iter 28, disc loss: 0.007883876123687112, policy loss: 5.460904056708369
Experience 3, Iter 29, disc loss: 0.008135560385852773, policy loss: 5.432960291393002
Experience 3, Iter 30, disc loss: 0.008686394146565222, policy loss: 5.321359910485051
Experience 3, Iter 31, disc loss: 0.008714674353841075, policy loss: 5.316633890535442
Experience 3, Iter 32, disc loss: 0.008557348166890435, policy loss: 5.3369534674198205
Experience 3, Iter 33, disc loss: 0.008360022503956066, policy loss: 5.373831294919533
Experience 3, Iter 34, disc loss: 0.008337889159761736, policy loss: 5.325695245185306
Experience 3, Iter 35, disc loss: 0.008794251732663749, policy loss: 5.23470094569763
Experience 3, Iter 36, disc loss: 0.009830704103664524, policy loss: 5.080082404329063
Experience 3, Iter 37, disc loss: 0.009905664732175011, policy loss: 5.110503327889624
Experience 3, Iter 38, disc loss: 0.010384014578680038, policy loss: 5.046200422058963
Experience 3, Iter 39, disc loss: 0.010019118246846571, policy loss: 5.08610234727179
Experience 3, Iter 40, disc loss: 0.009938065009808109, policy loss: 5.107619250185169
Experience 3, Iter 41, disc loss: 0.01066275340410971, policy loss: 4.996342748277833
Experience 3, Iter 42, disc loss: 0.009609761050511314, policy loss: 5.0943312637434826
Experience 3, Iter 43, disc loss: 0.011189471234672385, policy loss: 4.915677204481715
Experience 3, Iter 44, disc loss: 0.011615867931275814, policy loss: 4.881340906170481
Experience 3, Iter 45, disc loss: 0.012048118576712147, policy loss: 4.791105611099123
Experience 3, Iter 46, disc loss: 0.011308995509987301, policy loss: 4.869090357778205
Experience 3, Iter 47, disc loss: 0.011822061448250126, policy loss: 4.807171208079577
Experience 3, Iter 48, disc loss: 0.012687410500108166, policy loss: 4.7338618558271985
Experience 3, Iter 49, disc loss: 0.011268213464232665, policy loss: 4.864737864909648
Experience 3, Iter 50, disc loss: 0.01394409222457904, policy loss: 4.632202831103795
Experience 3, Iter 51, disc loss: 0.012921059962936021, policy loss: 4.72468867258943
Experience 3, Iter 52, disc loss: 0.012298139355346367, policy loss: 4.764974659106834
Experience 3, Iter 53, disc loss: 0.015178389462765214, policy loss: 4.546835345440471
Experience 3, Iter 54, disc loss: 0.014379667062825972, policy loss: 4.6127497786286975
Experience 3, Iter 55, disc loss: 0.016288202154623593, policy loss: 4.462737810170151
Experience 3, Iter 56, disc loss: 0.017196271072402698, policy loss: 4.477897421786364
Experience 3, Iter 57, disc loss: 0.017121096408436145, policy loss: 4.3875928370764825
Experience 3, Iter 58, disc loss: 0.01986643166350378, policy loss: 4.2188737998941885
Experience 3, Iter 59, disc loss: 0.016712492770320997, policy loss: 4.397097685557785
Experience 3, Iter 60, disc loss: 0.01829748735857628, policy loss: 4.311346353637445
Experience 3, Iter 61, disc loss: 0.01805888610190306, policy loss: 4.3764077764136955
Experience 3, Iter 62, disc loss: 0.0214608795439529, policy loss: 4.1315445402372255
Experience 3, Iter 63, disc loss: 0.022692965834131405, policy loss: 4.10996737179623
Experience 3, Iter 64, disc loss: 0.023712545316183702, policy loss: 4.001436459062826
Experience 3, Iter 65, disc loss: 0.02413653061116486, policy loss: 3.9984628572326204
Experience 3, Iter 66, disc loss: 0.02298192327223432, policy loss: 4.074003608044704
Experience 3, Iter 67, disc loss: 0.029305190548941085, policy loss: 3.8625132544932397
Experience 3, Iter 68, disc loss: 0.02457709156963215, policy loss: 3.9731777397240267
Experience 3, Iter 69, disc loss: 0.02919113650256394, policy loss: 3.8751749631335453
Experience 3, Iter 70, disc loss: 0.02920238790077655, policy loss: 3.7943860249907786
Experience 3, Iter 71, disc loss: 0.030856947184727677, policy loss: 3.7844431863884243
Experience 3, Iter 72, disc loss: 0.03472922426110728, policy loss: 3.670156332251377
Experience 3, Iter 73, disc loss: 0.030762768974741044, policy loss: 3.7482274707403818
Experience 3, Iter 74, disc loss: 0.037544075292132245, policy loss: 3.581481049413928
Experience 3, Iter 75, disc loss: 0.04886833122870041, policy loss: 3.379371794809995
Experience 3, Iter 76, disc loss: 0.04134847467573368, policy loss: 3.5225102865115874
Experience 3, Iter 77, disc loss: 0.04258539359070585, policy loss: 3.4806791621141295
Experience 3, Iter 78, disc loss: 0.04848408227562512, policy loss: 3.4014390093512796
Experience 3, Iter 79, disc loss: 0.05021695994283475, policy loss: 3.229602587060035
Experience 3, Iter 80, disc loss: 0.052347555033583765, policy loss: 3.2036732761296296
Experience 3, Iter 81, disc loss: 0.055765900775285586, policy loss: 3.1431107667922973
Experience 3, Iter 82, disc loss: 0.06132743805887907, policy loss: 3.109426564514358
Experience 3, Iter 83, disc loss: 0.07647201497626935, policy loss: 2.9267657658334416
Experience 3, Iter 84, disc loss: 0.06299786517097404, policy loss: 3.1002215773921464
Experience 3, Iter 85, disc loss: 0.06418399624437482, policy loss: 3.001414084569123
Experience 3, Iter 86, disc loss: 0.08673246273819665, policy loss: 2.8270392235053867
Experience 3, Iter 87, disc loss: 0.08517068112916562, policy loss: 2.8059505177956012
Experience 3, Iter 88, disc loss: 0.08205433644624756, policy loss: 2.8110575466184744
Experience 3, Iter 89, disc loss: 0.08169455684190445, policy loss: 2.7662256270298773
Experience 3, Iter 90, disc loss: 0.10864631104100075, policy loss: 2.5826612659023915
Experience 3, Iter 91, disc loss: 0.11828655133747601, policy loss: 2.449857969652888
Experience 3, Iter 92, disc loss: 0.1232288660232406, policy loss: 2.55300102927206
Experience 3, Iter 93, disc loss: 0.13077068376182013, policy loss: 2.3695650332444496
Experience 3, Iter 94, disc loss: 0.13523347255457857, policy loss: 2.403686786348372
Experience 3, Iter 95, disc loss: 0.15882665972858165, policy loss: 2.2422169053591166
Experience 3, Iter 96, disc loss: 0.1973852810671026, policy loss: 2.000267971207254
Experience 3, Iter 97, disc loss: 0.15408503976581023, policy loss: 2.305522770420561
Experience 3, Iter 98, disc loss: 0.1817693596187622, policy loss: 2.212300820653008
Experience 3, Iter 99, disc loss: 0.18110283845993286, policy loss: 2.2009251599316695
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0142],
        [0.0492],
        [1.1111],
        [0.0213]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.1055, 0.5447, 0.9858, 0.0200, 0.0098, 1.5855]],

        [[0.1055, 0.5447, 0.9858, 0.0200, 0.0098, 1.5855]],

        [[0.1055, 0.5447, 0.9858, 0.0200, 0.0098, 1.5855]],

        [[0.1055, 0.5447, 0.9858, 0.0200, 0.0098, 1.5855]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0570, 0.1967, 4.4446, 0.0853], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0570, 0.1967, 4.4446, 0.0853])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.729
Iter 2/2000 - Loss: 3.298
Iter 3/2000 - Loss: 3.010
Iter 4/2000 - Loss: 2.893
Iter 5/2000 - Loss: 2.850
Iter 6/2000 - Loss: 2.803
Iter 7/2000 - Loss: 2.742
Iter 8/2000 - Loss: 2.668
Iter 9/2000 - Loss: 2.580
Iter 10/2000 - Loss: 2.480
Iter 11/2000 - Loss: 2.380
Iter 12/2000 - Loss: 2.292
Iter 13/2000 - Loss: 2.217
Iter 14/2000 - Loss: 2.144
Iter 15/2000 - Loss: 2.057
Iter 16/2000 - Loss: 1.945
Iter 17/2000 - Loss: 1.815
Iter 18/2000 - Loss: 1.678
Iter 19/2000 - Loss: 1.548
Iter 20/2000 - Loss: 1.427
Iter 1981/2000 - Loss: -5.399
Iter 1982/2000 - Loss: -5.399
Iter 1983/2000 - Loss: -5.399
Iter 1984/2000 - Loss: -5.399
Iter 1985/2000 - Loss: -5.399
Iter 1986/2000 - Loss: -5.399
Iter 1987/2000 - Loss: -5.399
Iter 1988/2000 - Loss: -5.399
Iter 1989/2000 - Loss: -5.399
Iter 1990/2000 - Loss: -5.399
Iter 1991/2000 - Loss: -5.400
Iter 1992/2000 - Loss: -5.400
Iter 1993/2000 - Loss: -5.400
Iter 1994/2000 - Loss: -5.400
Iter 1995/2000 - Loss: -5.400
Iter 1996/2000 - Loss: -5.400
Iter 1997/2000 - Loss: -5.400
Iter 1998/2000 - Loss: -5.400
Iter 1999/2000 - Loss: -5.400
Iter 2000/2000 - Loss: -5.400
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0040],
        [0.0004]])
Lengthscale: tensor([[[15.2878, 10.6366, 54.3850, 18.2607, 21.4131, 46.7667]],

        [[36.4901, 53.4384, 11.7719,  1.6089,  1.2485, 16.6399]],

        [[38.8715, 53.2795, 19.1587,  1.0576,  2.1938, 18.8891]],

        [[42.9230, 63.9679, 19.8926,  4.0198,  6.1442, 48.3812]]])
Signal Variance: tensor([ 0.1878,  1.0317, 19.7682,  0.7577])
Estimated target variance: tensor([0.0570, 0.1967, 4.4446, 0.0853])
N: 40
Signal to noise ratio: tensor([21.6184, 46.9226, 70.0391, 42.4107])
Bound on condition number: tensor([ 18695.2002,  88070.0941, 196219.9378,  71947.6331])
Policy Optimizer learning rate:
0.09968441746484834
Experience 4, Iter 0, disc loss: 0.3029688223770467, policy loss: 1.5235881591055755
Experience 4, Iter 1, disc loss: 0.33075563884615145, policy loss: 1.4695989832685599
Experience 4, Iter 2, disc loss: 0.32378448299717566, policy loss: 1.517991553204339
Experience 4, Iter 3, disc loss: 0.28955876012538156, policy loss: 1.5745162806493864
Experience 4, Iter 4, disc loss: 0.31098803710093026, policy loss: 1.540274946827261
Experience 4, Iter 5, disc loss: 0.30662326047533656, policy loss: 1.5573816082706196
Experience 4, Iter 6, disc loss: 0.32446307031324445, policy loss: 1.4543218757786591
Experience 4, Iter 7, disc loss: 0.33775695872025113, policy loss: 1.4002605118849998
Experience 4, Iter 8, disc loss: 0.2884311765632374, policy loss: 1.5740029363133587
Experience 4, Iter 9, disc loss: 0.3152615908176164, policy loss: 1.4881926012554034
Experience 4, Iter 10, disc loss: 0.27355941157768593, policy loss: 1.6365345927942214
Experience 4, Iter 11, disc loss: 0.30755755933303003, policy loss: 1.550169357526106
Experience 4, Iter 12, disc loss: 0.26892391883572614, policy loss: 1.6441431116444074
Experience 4, Iter 13, disc loss: 0.2771434789851464, policy loss: 1.6666886365845142
Experience 4, Iter 14, disc loss: 0.2508095386410468, policy loss: 1.743270311113685
Experience 4, Iter 15, disc loss: 0.2716232707166435, policy loss: 1.6406383455906721
Experience 4, Iter 16, disc loss: 0.2645783938670614, policy loss: 1.705713052818008
Experience 4, Iter 17, disc loss: 0.2125931713086966, policy loss: 1.8866135421017023
Experience 4, Iter 18, disc loss: 0.2237430557725725, policy loss: 1.868629809598474
Experience 4, Iter 19, disc loss: 0.19794246526786652, policy loss: 1.9827090109874446
Experience 4, Iter 20, disc loss: 0.20689101240700652, policy loss: 1.963477835614981
Experience 4, Iter 21, disc loss: 0.18652706084830628, policy loss: 2.0893360631424835
Experience 4, Iter 22, disc loss: 0.16942120789776016, policy loss: 2.182712041217883
Experience 4, Iter 23, disc loss: 0.16031306145296387, policy loss: 2.2435398696982216
Experience 4, Iter 24, disc loss: 0.16974786969309996, policy loss: 2.202802657035094
Experience 4, Iter 25, disc loss: 0.1532136614285836, policy loss: 2.309590123759774
Experience 4, Iter 26, disc loss: 0.14310975595089923, policy loss: 2.381627603487127
Experience 4, Iter 27, disc loss: 0.13884070691274297, policy loss: 2.377065968202464
Experience 4, Iter 28, disc loss: 0.12849274744742217, policy loss: 2.46320555510253
Experience 4, Iter 29, disc loss: 0.10842181530702592, policy loss: 2.634688896431747
Experience 4, Iter 30, disc loss: 0.1027828167616018, policy loss: 2.6640161640190345
Experience 4, Iter 31, disc loss: 0.11085945635746695, policy loss: 2.590845779971575
Experience 4, Iter 32, disc loss: 0.09591603225985051, policy loss: 2.702917826220601
Experience 4, Iter 33, disc loss: 0.09056263078190968, policy loss: 2.74760864107874
Experience 4, Iter 34, disc loss: 0.08792130611484623, policy loss: 2.80607950949543
Experience 4, Iter 35, disc loss: 0.07810212081139796, policy loss: 2.888145057351265
Experience 4, Iter 36, disc loss: 0.06869365230272263, policy loss: 2.9789649655250434
Experience 4, Iter 37, disc loss: 0.06655303305016987, policy loss: 3.0065500529041205
Experience 4, Iter 38, disc loss: 0.06057941939331535, policy loss: 3.0927336773238077
Experience 4, Iter 39, disc loss: 0.06065009706321287, policy loss: 3.0938185208096574
Experience 4, Iter 40, disc loss: 0.05380787226182055, policy loss: 3.2170326459330263
Experience 4, Iter 41, disc loss: 0.04958271657422321, policy loss: 3.274281918879013
Experience 4, Iter 42, disc loss: 0.045524216457044124, policy loss: 3.339497775326521
Experience 4, Iter 43, disc loss: 0.04733150463170353, policy loss: 3.3093268626870076
Experience 4, Iter 44, disc loss: 0.04624660172267528, policy loss: 3.337251852822452
Experience 4, Iter 45, disc loss: 0.04312978429153048, policy loss: 3.3656578543726132
Experience 4, Iter 46, disc loss: 0.04291056351505661, policy loss: 3.437159362059956
Experience 4, Iter 47, disc loss: 0.03907142872350442, policy loss: 3.4858671273850925
Experience 4, Iter 48, disc loss: 0.038288076246236084, policy loss: 3.5079916969116547
Experience 4, Iter 49, disc loss: 0.037647004614401264, policy loss: 3.552608998996894
Experience 4, Iter 50, disc loss: 0.03634441873809917, policy loss: 3.539416020568791
Experience 4, Iter 51, disc loss: 0.0313216286237293, policy loss: 3.687896491539022
Experience 4, Iter 52, disc loss: 0.03176723924821968, policy loss: 3.651169534971298
Experience 4, Iter 53, disc loss: 0.028962511722743164, policy loss: 3.754030542701237
Experience 4, Iter 54, disc loss: 0.031038344132087038, policy loss: 3.6975203340130927
Experience 4, Iter 55, disc loss: 0.030336816647557323, policy loss: 3.7185881708513815
Experience 4, Iter 56, disc loss: 0.0287729950878602, policy loss: 3.79281126723547
Experience 4, Iter 57, disc loss: 0.027669731501272925, policy loss: 3.7967831960011647
Experience 4, Iter 58, disc loss: 0.027186205001113805, policy loss: 3.827837893843833
Experience 4, Iter 59, disc loss: 0.025116374216039718, policy loss: 3.8939355257511963
Experience 4, Iter 60, disc loss: 0.02581437069752266, policy loss: 3.8672644681935004
Experience 4, Iter 61, disc loss: 0.024721864074859264, policy loss: 3.894807986561183
Experience 4, Iter 62, disc loss: 0.02530146806727576, policy loss: 3.8747599020770798
Experience 4, Iter 63, disc loss: 0.025166177706905702, policy loss: 3.878294050387858
Experience 4, Iter 64, disc loss: 0.025359753512309638, policy loss: 3.863863042819778
Experience 4, Iter 65, disc loss: 0.024104583671646616, policy loss: 3.9170384785942094
Experience 4, Iter 66, disc loss: 0.02245883806566817, policy loss: 3.9791994168736404
Experience 4, Iter 67, disc loss: 0.02335596445613052, policy loss: 3.9425789385736323
Experience 4, Iter 68, disc loss: 0.022851708954766174, policy loss: 3.9602838920256227
Experience 4, Iter 69, disc loss: 0.021854357507204887, policy loss: 4.014390368752816
Experience 4, Iter 70, disc loss: 0.02159055339095787, policy loss: 4.009730544346275
Experience 4, Iter 71, disc loss: 0.021874308405125734, policy loss: 3.983139446688586
Experience 4, Iter 72, disc loss: 0.02255382895591836, policy loss: 3.9663096882196287
Experience 4, Iter 73, disc loss: 0.022792330190562517, policy loss: 3.95573077324505
Experience 4, Iter 74, disc loss: 0.02233724937835587, policy loss: 3.9511166508922457
Experience 4, Iter 75, disc loss: 0.021747877488169615, policy loss: 3.983055786570044
Experience 4, Iter 76, disc loss: 0.021786121140231346, policy loss: 3.9845261349306296
Experience 4, Iter 77, disc loss: 0.02219992852826746, policy loss: 3.9660490702357887
Experience 4, Iter 78, disc loss: 0.02241829566767637, policy loss: 3.964073323812218
Experience 4, Iter 79, disc loss: 0.021568996214695175, policy loss: 3.9862667103205087
Experience 4, Iter 80, disc loss: 0.023298298497330373, policy loss: 3.920304968534634
Experience 4, Iter 81, disc loss: 0.022320440443852636, policy loss: 3.9575521233909847
Experience 4, Iter 82, disc loss: 0.02326651118035616, policy loss: 3.912165791633675
Experience 4, Iter 83, disc loss: 0.022715712037469884, policy loss: 3.9434138354283625
Experience 4, Iter 84, disc loss: 0.02304073571919731, policy loss: 3.917718497997769
Experience 4, Iter 85, disc loss: 0.02121957155161478, policy loss: 4.014609633335731
Experience 4, Iter 86, disc loss: 0.02431766306922214, policy loss: 3.856500691807417
Experience 4, Iter 87, disc loss: 0.02152476056701607, policy loss: 4.0262123378942904
Experience 4, Iter 88, disc loss: 0.023176898870294494, policy loss: 3.9022034964276946
Experience 4, Iter 89, disc loss: 0.026041724582746558, policy loss: 3.8045514184036415
Experience 4, Iter 90, disc loss: 0.024920598971653322, policy loss: 3.825884745405463
Experience 4, Iter 91, disc loss: 0.024367906985935495, policy loss: 3.8592929944164265
Experience 4, Iter 92, disc loss: 0.022636554311966012, policy loss: 3.925135191562417
Experience 4, Iter 93, disc loss: 0.022159572769395786, policy loss: 4.014126756664089
Experience 4, Iter 94, disc loss: 0.02316598485478987, policy loss: 3.924211199923812
Experience 4, Iter 95, disc loss: 0.02240485100181645, policy loss: 3.9491143613893303
Experience 4, Iter 96, disc loss: 0.02389895213856188, policy loss: 3.909587482775117
Experience 4, Iter 97, disc loss: 0.022542823795085508, policy loss: 3.9615367693334824
Experience 4, Iter 98, disc loss: 0.021466442390591707, policy loss: 4.0435402504224935
Experience 4, Iter 99, disc loss: 0.020422004603692343, policy loss: 4.1448105984466785
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0133],
        [0.0479],
        [0.9013],
        [0.0174]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0936, 0.5073, 0.7996, 0.0184, 0.0085, 1.6680]],

        [[0.0936, 0.5073, 0.7996, 0.0184, 0.0085, 1.6680]],

        [[0.0936, 0.5073, 0.7996, 0.0184, 0.0085, 1.6680]],

        [[0.0936, 0.5073, 0.7996, 0.0184, 0.0085, 1.6680]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0534, 0.1914, 3.6051, 0.0695], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0534, 0.1914, 3.6051, 0.0695])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.203
Iter 2/2000 - Loss: 2.871
Iter 3/2000 - Loss: 2.630
Iter 4/2000 - Loss: 2.540
Iter 5/2000 - Loss: 2.499
Iter 6/2000 - Loss: 2.421
Iter 7/2000 - Loss: 2.314
Iter 8/2000 - Loss: 2.197
Iter 9/2000 - Loss: 2.083
Iter 10/2000 - Loss: 1.970
Iter 11/2000 - Loss: 1.859
Iter 12/2000 - Loss: 1.748
Iter 13/2000 - Loss: 1.629
Iter 14/2000 - Loss: 1.494
Iter 15/2000 - Loss: 1.342
Iter 16/2000 - Loss: 1.184
Iter 17/2000 - Loss: 1.031
Iter 18/2000 - Loss: 0.887
Iter 19/2000 - Loss: 0.749
Iter 20/2000 - Loss: 0.608
Iter 1981/2000 - Loss: -5.930
Iter 1982/2000 - Loss: -5.930
Iter 1983/2000 - Loss: -5.930
Iter 1984/2000 - Loss: -5.930
Iter 1985/2000 - Loss: -5.930
Iter 1986/2000 - Loss: -5.930
Iter 1987/2000 - Loss: -5.930
Iter 1988/2000 - Loss: -5.930
Iter 1989/2000 - Loss: -5.930
Iter 1990/2000 - Loss: -5.930
Iter 1991/2000 - Loss: -5.930
Iter 1992/2000 - Loss: -5.930
Iter 1993/2000 - Loss: -5.930
Iter 1994/2000 - Loss: -5.930
Iter 1995/2000 - Loss: -5.930
Iter 1996/2000 - Loss: -5.930
Iter 1997/2000 - Loss: -5.930
Iter 1998/2000 - Loss: -5.931
Iter 1999/2000 - Loss: -5.931
Iter 2000/2000 - Loss: -5.931
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0041],
        [0.0005]])
Lengthscale: tensor([[[17.6258, 10.5111, 46.9554, 16.0714, 21.0821, 47.7280]],

        [[33.6749, 53.3724, 12.3509,  1.5646,  1.2675, 16.9351]],

        [[34.8111, 50.8385, 19.8940,  1.0835,  2.2391, 19.4537]],

        [[36.7510, 60.8618, 20.3619,  4.6451,  6.6434, 48.6205]]])
Signal Variance: tensor([ 0.1828,  1.0719, 20.6510,  0.7929])
Estimated target variance: tensor([0.0534, 0.1914, 3.6051, 0.0695])
N: 50
Signal to noise ratio: tensor([21.2465, 45.9624, 71.3574, 40.7365])
Bound on condition number: tensor([ 22571.7327, 105627.9813, 254595.1386,  82974.2380])
Policy Optimizer learning rate:
0.0995794447581801
Experience 5, Iter 0, disc loss: 0.022841133604711787, policy loss: 3.983050925815437
Experience 5, Iter 1, disc loss: 0.02017383402073744, policy loss: 4.1186228047969475
Experience 5, Iter 2, disc loss: 0.020809504776341223, policy loss: 4.032079309324234
Experience 5, Iter 3, disc loss: 0.021951598140426732, policy loss: 4.035391933942055
Experience 5, Iter 4, disc loss: 0.020029740625467418, policy loss: 4.1156733374201355
Experience 5, Iter 5, disc loss: 0.021700692412750466, policy loss: 4.018162863001726
Experience 5, Iter 6, disc loss: 0.019703367319550028, policy loss: 4.120116024851866
Experience 5, Iter 7, disc loss: 0.01972953438307804, policy loss: 4.19074394826799
Experience 5, Iter 8, disc loss: 0.019997582471633267, policy loss: 4.09484556849089
Experience 5, Iter 9, disc loss: 0.019622661038107996, policy loss: 4.133283062579425
Experience 5, Iter 10, disc loss: 0.019666197450526484, policy loss: 4.133948403085834
Experience 5, Iter 11, disc loss: 0.018247567578424925, policy loss: 4.183518961259097
Experience 5, Iter 12, disc loss: 0.019495198730889715, policy loss: 4.102618463068698
Experience 5, Iter 13, disc loss: 0.019209384726944133, policy loss: 4.152740514375088
Experience 5, Iter 14, disc loss: 0.01980892042799277, policy loss: 4.09373457083811
Experience 5, Iter 15, disc loss: 0.019270436401425665, policy loss: 4.131322203793983
Experience 5, Iter 16, disc loss: 0.01832401193494079, policy loss: 4.187102114464061
Experience 5, Iter 17, disc loss: 0.01828140677335051, policy loss: 4.174147475817302
Experience 5, Iter 18, disc loss: 0.018604570378599534, policy loss: 4.178596672944648
Experience 5, Iter 19, disc loss: 0.016638674948115422, policy loss: 4.312393418775693
Experience 5, Iter 20, disc loss: 0.017316939241460518, policy loss: 4.284294919137661
Experience 5, Iter 21, disc loss: 0.015850416092962304, policy loss: 4.375690829060536
Experience 5, Iter 22, disc loss: 0.017962593983594136, policy loss: 4.186429379966857
Experience 5, Iter 23, disc loss: 0.01653956711118185, policy loss: 4.317707335423963
Experience 5, Iter 24, disc loss: 0.016747779392500262, policy loss: 4.251167901976848
Experience 5, Iter 25, disc loss: 0.014779739934674812, policy loss: 4.396571727723018
Experience 5, Iter 26, disc loss: 0.01764719398450394, policy loss: 4.207845285539521
Experience 5, Iter 27, disc loss: 0.015693693109583358, policy loss: 4.390960084828998
Experience 5, Iter 28, disc loss: 0.01588416878851176, policy loss: 4.279188076663598
Experience 5, Iter 29, disc loss: 0.01593814270875967, policy loss: 4.323670469787277
Experience 5, Iter 30, disc loss: 0.015331628485941088, policy loss: 4.3422006336185515
Experience 5, Iter 31, disc loss: 0.014824933572962154, policy loss: 4.445990717270972
Experience 5, Iter 32, disc loss: 0.01581882253873138, policy loss: 4.323745420911029
Experience 5, Iter 33, disc loss: 0.014458391962042245, policy loss: 4.4212039434471535
Experience 5, Iter 34, disc loss: 0.014612401620705533, policy loss: 4.422579287190766
Experience 5, Iter 35, disc loss: 0.014126108437418693, policy loss: 4.439356401546613
Experience 5, Iter 36, disc loss: 0.013892065116237997, policy loss: 4.492828606529379
Experience 5, Iter 37, disc loss: 0.014109782217095015, policy loss: 4.442492967863213
Experience 5, Iter 38, disc loss: 0.013771492982769548, policy loss: 4.472105332047779
Experience 5, Iter 39, disc loss: 0.014536057450472862, policy loss: 4.3987720076261105
Experience 5, Iter 40, disc loss: 0.014611385795056527, policy loss: 4.3797036942625205
Experience 5, Iter 41, disc loss: 0.013157433405215283, policy loss: 4.555270644849699
Experience 5, Iter 42, disc loss: 0.014290481925057281, policy loss: 4.414604849803805
Experience 5, Iter 43, disc loss: 0.013115533529912246, policy loss: 4.573895425796336
Experience 5, Iter 44, disc loss: 0.013599713373345796, policy loss: 4.504164278922201
Experience 5, Iter 45, disc loss: 0.012666481291330205, policy loss: 4.565616654920279
Experience 5, Iter 46, disc loss: 0.012429254118327645, policy loss: 4.594634098696099
Experience 5, Iter 47, disc loss: 0.013034785971528146, policy loss: 4.563076923829936
Experience 5, Iter 48, disc loss: 0.011533821575493623, policy loss: 4.715492556446577
Experience 5, Iter 49, disc loss: 0.01249017728839408, policy loss: 4.607115245155809
Experience 5, Iter 50, disc loss: 0.01320725103316185, policy loss: 4.559264605175972
Experience 5, Iter 51, disc loss: 0.014046554087131905, policy loss: 4.445221872500525
Experience 5, Iter 52, disc loss: 0.012562818865961011, policy loss: 4.536546706600601
Experience 5, Iter 53, disc loss: 0.010996397535603058, policy loss: 4.7423953620447055
Experience 5, Iter 54, disc loss: 0.011221112013207938, policy loss: 4.683731582440558
Experience 5, Iter 55, disc loss: 0.012910762284229759, policy loss: 4.517550759248606
Experience 5, Iter 56, disc loss: 0.011535309184812178, policy loss: 4.709606830801343
Experience 5, Iter 57, disc loss: 0.012065382187052987, policy loss: 4.675274191454509
Experience 5, Iter 58, disc loss: 0.01324414122859988, policy loss: 4.482530159034405
Experience 5, Iter 59, disc loss: 0.011541475821965482, policy loss: 4.663608383394443
Experience 5, Iter 60, disc loss: 0.011018746145309325, policy loss: 4.736226424420898
Experience 5, Iter 61, disc loss: 0.011875172327832525, policy loss: 4.606132597615801
Experience 5, Iter 62, disc loss: 0.011236908338327982, policy loss: 4.654713869841071
Experience 5, Iter 63, disc loss: 0.011840600357086877, policy loss: 4.606923111698814
Experience 5, Iter 64, disc loss: 0.01091307078134182, policy loss: 4.762307038484732
Experience 5, Iter 65, disc loss: 0.011134830190482652, policy loss: 4.715126722581731
Experience 5, Iter 66, disc loss: 0.011594771289255894, policy loss: 4.63302085762248
Experience 5, Iter 67, disc loss: 0.010820981391181941, policy loss: 4.742117703587036
Experience 5, Iter 68, disc loss: 0.011871822588249457, policy loss: 4.627483114612884
Experience 5, Iter 69, disc loss: 0.010467924352816068, policy loss: 4.76392202313769
Experience 5, Iter 70, disc loss: 0.010724827081617398, policy loss: 4.720856352563787
Experience 5, Iter 71, disc loss: 0.010078256224385938, policy loss: 4.813437838942231
Experience 5, Iter 72, disc loss: 0.011232493673126797, policy loss: 4.679830047507702
Experience 5, Iter 73, disc loss: 0.01149420712658067, policy loss: 4.611794754346377
Experience 5, Iter 74, disc loss: 0.01045100562517853, policy loss: 4.767069480116163
Experience 5, Iter 75, disc loss: 0.009753620819854062, policy loss: 4.865200708435609
Experience 5, Iter 76, disc loss: 0.010041016626242166, policy loss: 4.8394201635939025
Experience 5, Iter 77, disc loss: 0.010736252311068253, policy loss: 4.7309798837190264
Experience 5, Iter 78, disc loss: 0.010088411242040499, policy loss: 4.870368772896532
Experience 5, Iter 79, disc loss: 0.010551076994937411, policy loss: 4.7383605586526745
Experience 5, Iter 80, disc loss: 0.00926749918430226, policy loss: 4.902535709781598
Experience 5, Iter 81, disc loss: 0.009612830737247955, policy loss: 4.8549033415400125
Experience 5, Iter 82, disc loss: 0.010136371017254299, policy loss: 4.824743220945205
Experience 5, Iter 83, disc loss: 0.009977073173369681, policy loss: 4.793684798537763
Experience 5, Iter 84, disc loss: 0.010062794339900752, policy loss: 4.817882737976654
Experience 5, Iter 85, disc loss: 0.009763206328102552, policy loss: 4.84010646381725
Experience 5, Iter 86, disc loss: 0.009900907223102976, policy loss: 4.8393463769927045
Experience 5, Iter 87, disc loss: 0.009028028962910929, policy loss: 4.9181121519625055
Experience 5, Iter 88, disc loss: 0.0094624528667178, policy loss: 4.892198523545254
Experience 5, Iter 89, disc loss: 0.008817820913366147, policy loss: 4.919776347064087
Experience 5, Iter 90, disc loss: 0.009062247250759367, policy loss: 4.920101883513309
Experience 5, Iter 91, disc loss: 0.00843225260435469, policy loss: 5.00643899828633
Experience 5, Iter 92, disc loss: 0.008332747191682272, policy loss: 5.004750443130455
Experience 5, Iter 93, disc loss: 0.009110151289708967, policy loss: 4.876829981484946
Experience 5, Iter 94, disc loss: 0.009559237787077245, policy loss: 4.818590635644982
Experience 5, Iter 95, disc loss: 0.008580999265820587, policy loss: 4.950656413981892
Experience 5, Iter 96, disc loss: 0.00918665014313558, policy loss: 4.882745789199391
Experience 5, Iter 97, disc loss: 0.00920922209823243, policy loss: 4.866591655091758
Experience 5, Iter 98, disc loss: 0.008839094319506014, policy loss: 4.921194635319829
Experience 5, Iter 99, disc loss: 0.007584573182110241, policy loss: 5.101297822698668
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0125],
        [0.0447],
        [0.7491],
        [0.0145]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0809, 0.4676, 0.6646, 0.0163, 0.0075, 1.6160]],

        [[0.0809, 0.4676, 0.6646, 0.0163, 0.0075, 1.6160]],

        [[0.0809, 0.4676, 0.6646, 0.0163, 0.0075, 1.6160]],

        [[0.0809, 0.4676, 0.6646, 0.0163, 0.0075, 1.6160]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0501, 0.1790, 2.9966, 0.0579], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0501, 0.1790, 2.9966, 0.0579])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.670
Iter 2/2000 - Loss: 2.374
Iter 3/2000 - Loss: 2.134
Iter 4/2000 - Loss: 2.041
Iter 5/2000 - Loss: 1.982
Iter 6/2000 - Loss: 1.873
Iter 7/2000 - Loss: 1.732
Iter 8/2000 - Loss: 1.589
Iter 9/2000 - Loss: 1.457
Iter 10/2000 - Loss: 1.332
Iter 11/2000 - Loss: 1.201
Iter 12/2000 - Loss: 1.054
Iter 13/2000 - Loss: 0.886
Iter 14/2000 - Loss: 0.703
Iter 15/2000 - Loss: 0.519
Iter 16/2000 - Loss: 0.344
Iter 17/2000 - Loss: 0.181
Iter 18/2000 - Loss: 0.025
Iter 19/2000 - Loss: -0.135
Iter 20/2000 - Loss: -0.303
Iter 1981/2000 - Loss: -6.403
Iter 1982/2000 - Loss: -6.403
Iter 1983/2000 - Loss: -6.403
Iter 1984/2000 - Loss: -6.403
Iter 1985/2000 - Loss: -6.403
Iter 1986/2000 - Loss: -6.403
Iter 1987/2000 - Loss: -6.403
Iter 1988/2000 - Loss: -6.403
Iter 1989/2000 - Loss: -6.403
Iter 1990/2000 - Loss: -6.403
Iter 1991/2000 - Loss: -6.404
Iter 1992/2000 - Loss: -6.404
Iter 1993/2000 - Loss: -6.404
Iter 1994/2000 - Loss: -6.404
Iter 1995/2000 - Loss: -6.404
Iter 1996/2000 - Loss: -6.404
Iter 1997/2000 - Loss: -6.404
Iter 1998/2000 - Loss: -6.404
Iter 1999/2000 - Loss: -6.404
Iter 2000/2000 - Loss: -6.404
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0039],
        [0.0005]])
Lengthscale: tensor([[[20.9387, 10.4942, 43.2596, 13.8588, 20.9288, 48.0999]],

        [[31.3192, 53.6535, 12.0874,  1.4928,  1.2664, 16.6771]],

        [[33.6024, 49.9218, 19.9018,  1.0868,  2.1171, 19.4725]],

        [[32.9507, 60.8114, 20.0869,  4.5574,  6.4973, 47.8117]]])
Signal Variance: tensor([ 0.1923,  0.9914, 20.2427,  0.7588])
Estimated target variance: tensor([0.0501, 0.1790, 2.9966, 0.0579])
N: 60
Signal to noise ratio: tensor([21.9403, 47.8412, 71.8336, 39.2934])
Bound on condition number: tensor([ 28883.7134, 137327.8950, 309604.7422,  92639.3239])
Policy Optimizer learning rate:
0.09947458259305313
Experience 6, Iter 0, disc loss: 0.008514602116471495, policy loss: 5.012799653076449
Experience 6, Iter 1, disc loss: 0.008614307178023061, policy loss: 4.94508513779898
Experience 6, Iter 2, disc loss: 0.00882240188208323, policy loss: 4.943047914188392
Experience 6, Iter 3, disc loss: 0.009609336155605613, policy loss: 4.818957986596829
Experience 6, Iter 4, disc loss: 0.008234029476974002, policy loss: 5.061359895114657
Experience 6, Iter 5, disc loss: 0.007566788943626126, policy loss: 5.1017661107448244
Experience 6, Iter 6, disc loss: 0.009025012884663038, policy loss: 4.884708805778919
Experience 6, Iter 7, disc loss: 0.007695692152481206, policy loss: 5.061032651974728
Experience 6, Iter 8, disc loss: 0.008053732143924424, policy loss: 5.062991413679034
Experience 6, Iter 9, disc loss: 0.007682877351082692, policy loss: 5.132658170221484
Experience 6, Iter 10, disc loss: 0.008187061566265237, policy loss: 5.115393678990308
Experience 6, Iter 11, disc loss: 0.007679285206199423, policy loss: 5.081095146279499
Experience 6, Iter 12, disc loss: 0.008073637013872436, policy loss: 5.027369590096091
Experience 6, Iter 13, disc loss: 0.007691049407725391, policy loss: 5.047383233664551
Experience 6, Iter 14, disc loss: 0.008613750878705624, policy loss: 4.944577167454664
Experience 6, Iter 15, disc loss: 0.0072709978548221865, policy loss: 5.1117948017187835
Experience 6, Iter 16, disc loss: 0.007121063374377816, policy loss: 5.210606456622593
Experience 6, Iter 17, disc loss: 0.008207440125409148, policy loss: 4.991285133042892
Experience 6, Iter 18, disc loss: 0.007336469454682121, policy loss: 5.142635172923189
Experience 6, Iter 19, disc loss: 0.00755990321887729, policy loss: 5.150738630447083
Experience 6, Iter 20, disc loss: 0.00749183820087255, policy loss: 5.115902455708745
Experience 6, Iter 21, disc loss: 0.007565862533373014, policy loss: 5.094284120591347
Experience 6, Iter 22, disc loss: 0.007667386143276886, policy loss: 5.045643673201104
Experience 6, Iter 23, disc loss: 0.007386722544017659, policy loss: 5.105815516128842
Experience 6, Iter 24, disc loss: 0.007645715799326648, policy loss: 5.130431336040932
Experience 6, Iter 25, disc loss: 0.00709761773769708, policy loss: 5.196544741315475
Experience 6, Iter 26, disc loss: 0.006650018244048416, policy loss: 5.290478868985831
Experience 6, Iter 27, disc loss: 0.007423483222848668, policy loss: 5.100982802471204
Experience 6, Iter 28, disc loss: 0.007968608395665728, policy loss: 5.076885974855552
Experience 6, Iter 29, disc loss: 0.006769813866370857, policy loss: 5.232593315142143
Experience 6, Iter 30, disc loss: 0.0070321374907851356, policy loss: 5.1992651311316544
Experience 6, Iter 31, disc loss: 0.007152154605476628, policy loss: 5.218990056535005
Experience 6, Iter 32, disc loss: 0.006570178672572018, policy loss: 5.280169256535828
Experience 6, Iter 33, disc loss: 0.007023770201625353, policy loss: 5.1564757431352835
Experience 6, Iter 34, disc loss: 0.006926738442494802, policy loss: 5.17215710740091
Experience 6, Iter 35, disc loss: 0.007069678691393834, policy loss: 5.1865088059457864
Experience 6, Iter 36, disc loss: 0.0063212742331379, policy loss: 5.283884985946147
Experience 6, Iter 37, disc loss: 0.006418263295342706, policy loss: 5.247790529010583
Experience 6, Iter 38, disc loss: 0.006048375530789342, policy loss: 5.358078452606915
Experience 6, Iter 39, disc loss: 0.0064064127782728925, policy loss: 5.2724689065597445
Experience 6, Iter 40, disc loss: 0.007303685075955112, policy loss: 5.144139000344919
Experience 6, Iter 41, disc loss: 0.006078902635684784, policy loss: 5.338069870865744
Experience 6, Iter 42, disc loss: 0.006821919743785365, policy loss: 5.2063278131093185
Experience 6, Iter 43, disc loss: 0.006358151825503229, policy loss: 5.3309352916702135
Experience 6, Iter 44, disc loss: 0.0062682750623368555, policy loss: 5.324838454500217
Experience 6, Iter 45, disc loss: 0.005755543090471485, policy loss: 5.355523791529398
Experience 6, Iter 46, disc loss: 0.0058118912793688965, policy loss: 5.328633789604621
Experience 6, Iter 47, disc loss: 0.006047349780040064, policy loss: 5.315812877626605
Experience 6, Iter 48, disc loss: 0.006459796311741042, policy loss: 5.212196896105218
Experience 6, Iter 49, disc loss: 0.005541226250859335, policy loss: 5.416521324631372
Experience 6, Iter 50, disc loss: 0.007012753330077454, policy loss: 5.130512863374236
Experience 6, Iter 51, disc loss: 0.0070638306053509925, policy loss: 5.122629803212798
Experience 6, Iter 52, disc loss: 0.005754758415991875, policy loss: 5.397798318840914
Experience 6, Iter 53, disc loss: 0.005778026034205969, policy loss: 5.424345036095621
Experience 6, Iter 54, disc loss: 0.005989727986320104, policy loss: 5.352533502837487
Experience 6, Iter 55, disc loss: 0.006073988368282188, policy loss: 5.317966908816406
Experience 6, Iter 56, disc loss: 0.006060708795985537, policy loss: 5.314361747841174
Experience 6, Iter 57, disc loss: 0.006333870812765974, policy loss: 5.241177422789976
Experience 6, Iter 58, disc loss: 0.005110055158085489, policy loss: 5.513838970837383
Experience 6, Iter 59, disc loss: 0.005636673112312777, policy loss: 5.454458751080569
Experience 6, Iter 60, disc loss: 0.005792226826067913, policy loss: 5.381223253739224
Experience 6, Iter 61, disc loss: 0.005878131150925995, policy loss: 5.372077688729943
Experience 6, Iter 62, disc loss: 0.006078352802309388, policy loss: 5.271441876945705
Experience 6, Iter 63, disc loss: 0.005719656082535684, policy loss: 5.358004335712472
Experience 6, Iter 64, disc loss: 0.005860182670306916, policy loss: 5.40104994075392
Experience 6, Iter 65, disc loss: 0.005423785793596304, policy loss: 5.450752583088574
Experience 6, Iter 66, disc loss: 0.0051355310141463825, policy loss: 5.563766979574564
Experience 6, Iter 67, disc loss: 0.005439406203207083, policy loss: 5.418285042158382
Experience 6, Iter 68, disc loss: 0.0056313346253132725, policy loss: 5.375245314991328
Experience 6, Iter 69, disc loss: 0.0052526513064490386, policy loss: 5.516958482036802
Experience 6, Iter 70, disc loss: 0.005373660637792228, policy loss: 5.46237469873661
Experience 6, Iter 71, disc loss: 0.005678056592838693, policy loss: 5.372673912953202
Experience 6, Iter 72, disc loss: 0.005603237818146781, policy loss: 5.409349006556299
Experience 6, Iter 73, disc loss: 0.005028315257253781, policy loss: 5.61370677515729
Experience 6, Iter 74, disc loss: 0.005276126675806528, policy loss: 5.472004712264042
Experience 6, Iter 75, disc loss: 0.004930941888510992, policy loss: 5.507231349763096
Experience 6, Iter 76, disc loss: 0.005554723758451002, policy loss: 5.40454955795645
Experience 6, Iter 77, disc loss: 0.005757688218430064, policy loss: 5.352092285053162
Experience 6, Iter 78, disc loss: 0.005189080278892229, policy loss: 5.4965043319551175
Experience 6, Iter 79, disc loss: 0.005077730257952299, policy loss: 5.468007262987639
Experience 6, Iter 80, disc loss: 0.004744708436503546, policy loss: 5.638033876262104
Experience 6, Iter 81, disc loss: 0.0047021966384341194, policy loss: 5.584040346072456
Experience 6, Iter 82, disc loss: 0.005272561179817455, policy loss: 5.463258984506937
Experience 6, Iter 83, disc loss: 0.00501647037387914, policy loss: 5.5327436467128415
Experience 6, Iter 84, disc loss: 0.005180539603186427, policy loss: 5.453983589063147
Experience 6, Iter 85, disc loss: 0.004988929171075488, policy loss: 5.531632422229534
Experience 6, Iter 86, disc loss: 0.005097989231545119, policy loss: 5.50016871753497
Experience 6, Iter 87, disc loss: 0.004771398358570683, policy loss: 5.632015828940443
Experience 6, Iter 88, disc loss: 0.005262264680946433, policy loss: 5.432491249038552
Experience 6, Iter 89, disc loss: 0.004788705699115126, policy loss: 5.570942481954868
Experience 6, Iter 90, disc loss: 0.0050446514892401925, policy loss: 5.465094706202476
Experience 6, Iter 91, disc loss: 0.004794846689477166, policy loss: 5.644170155334535
Experience 6, Iter 92, disc loss: 0.0048707139947867665, policy loss: 5.619101915528763
Experience 6, Iter 93, disc loss: 0.0043868938992352915, policy loss: 5.75774325306774
Experience 6, Iter 94, disc loss: 0.0049136141262368874, policy loss: 5.572196582567258
Experience 6, Iter 95, disc loss: 0.00514538036734637, policy loss: 5.488601484030123
Experience 6, Iter 96, disc loss: 0.005304415135868081, policy loss: 5.454267517596708
Experience 6, Iter 97, disc loss: 0.004322990172834125, policy loss: 5.7037548567461
Experience 6, Iter 98, disc loss: 0.005036849242162605, policy loss: 5.488164837338589
Experience 6, Iter 99, disc loss: 0.004509863855553144, policy loss: 5.64438976690609
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0112],
        [0.0414],
        [0.6421],
        [0.0124]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0709, 0.4176, 0.5694, 0.0146, 0.0066, 1.5196]],

        [[0.0709, 0.4176, 0.5694, 0.0146, 0.0066, 1.5196]],

        [[0.0709, 0.4176, 0.5694, 0.0146, 0.0066, 1.5196]],

        [[0.0709, 0.4176, 0.5694, 0.0146, 0.0066, 1.5196]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0449, 0.1657, 2.5684, 0.0496], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0449, 0.1657, 2.5684, 0.0496])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.326
Iter 2/2000 - Loss: 2.050
Iter 3/2000 - Loss: 1.813
Iter 4/2000 - Loss: 1.721
Iter 5/2000 - Loss: 1.650
Iter 6/2000 - Loss: 1.522
Iter 7/2000 - Loss: 1.363
Iter 8/2000 - Loss: 1.207
Iter 9/2000 - Loss: 1.066
Iter 10/2000 - Loss: 0.930
Iter 11/2000 - Loss: 0.780
Iter 12/2000 - Loss: 0.606
Iter 13/2000 - Loss: 0.410
Iter 14/2000 - Loss: 0.206
Iter 15/2000 - Loss: 0.008
Iter 16/2000 - Loss: -0.179
Iter 17/2000 - Loss: -0.358
Iter 18/2000 - Loss: -0.538
Iter 19/2000 - Loss: -0.722
Iter 20/2000 - Loss: -0.913
Iter 1981/2000 - Loss: -6.889
Iter 1982/2000 - Loss: -6.889
Iter 1983/2000 - Loss: -6.889
Iter 1984/2000 - Loss: -6.889
Iter 1985/2000 - Loss: -6.889
Iter 1986/2000 - Loss: -6.889
Iter 1987/2000 - Loss: -6.889
Iter 1988/2000 - Loss: -6.889
Iter 1989/2000 - Loss: -6.889
Iter 1990/2000 - Loss: -6.889
Iter 1991/2000 - Loss: -6.889
Iter 1992/2000 - Loss: -6.889
Iter 1993/2000 - Loss: -6.889
Iter 1994/2000 - Loss: -6.889
Iter 1995/2000 - Loss: -6.889
Iter 1996/2000 - Loss: -6.889
Iter 1997/2000 - Loss: -6.889
Iter 1998/2000 - Loss: -6.889
Iter 1999/2000 - Loss: -6.889
Iter 2000/2000 - Loss: -6.889
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0034],
        [0.0005]])
Lengthscale: tensor([[[20.2762, 10.7058, 41.2036, 13.3316, 21.7564, 48.6443]],

        [[28.1957, 42.4205, 11.6140,  1.4056,  1.2411, 15.8035]],

        [[29.7770, 42.4697, 19.7100,  1.0726,  2.0480, 18.6171]],

        [[29.7688, 50.7335, 19.8688,  4.3140,  6.0425, 46.5722]]])
Signal Variance: tensor([ 0.1928,  0.8419, 18.9023,  0.7066])
Estimated target variance: tensor([0.0449, 0.1657, 2.5684, 0.0496])
N: 70
Signal to noise ratio: tensor([23.4531, 46.9066, 75.0951, 39.4662])
Bound on condition number: tensor([ 38504.2468, 154017.0750, 394749.6753, 109031.9421])
Policy Optimizer learning rate:
0.09936983085306159
Experience 7, Iter 0, disc loss: 0.004797487725759738, policy loss: 5.566945493132403
Experience 7, Iter 1, disc loss: 0.004755052916286589, policy loss: 5.593326893677199
Experience 7, Iter 2, disc loss: 0.004517009686967727, policy loss: 5.654453430213885
Experience 7, Iter 3, disc loss: 0.004971804102221058, policy loss: 5.517091813389452
Experience 7, Iter 4, disc loss: 0.004563679050017731, policy loss: 5.623712215719051
Experience 7, Iter 5, disc loss: 0.004409287074871558, policy loss: 5.662633639514075
Experience 7, Iter 6, disc loss: 0.004536354179117811, policy loss: 5.627016568359848
Experience 7, Iter 7, disc loss: 0.004415444666372145, policy loss: 5.6610059611714005
Experience 7, Iter 8, disc loss: 0.004818109286004278, policy loss: 5.574978575414377
Experience 7, Iter 9, disc loss: 0.005045467983002018, policy loss: 5.532617424110066
Experience 7, Iter 10, disc loss: 0.004508334897064425, policy loss: 5.698110246764068
Experience 7, Iter 11, disc loss: 0.0038930713423595917, policy loss: 5.798539682948577
Experience 7, Iter 12, disc loss: 0.005181070756444715, policy loss: 5.481067725559801
Experience 7, Iter 13, disc loss: 0.004627700386879248, policy loss: 5.614574741632617
Experience 7, Iter 14, disc loss: 0.004782655641001645, policy loss: 5.633599065781706
Experience 7, Iter 15, disc loss: 0.003982776997154139, policy loss: 5.77330517211445
Experience 7, Iter 16, disc loss: 0.004379363438692244, policy loss: 5.637932552997086
Experience 7, Iter 17, disc loss: 0.0041798195500105835, policy loss: 5.685484905688828
Experience 7, Iter 18, disc loss: 0.004132895276159614, policy loss: 5.754818808948142
Experience 7, Iter 19, disc loss: 0.004193452998188469, policy loss: 5.7233206504730285
Experience 7, Iter 20, disc loss: 0.004230817715792316, policy loss: 5.685777839888178
Experience 7, Iter 21, disc loss: 0.004160877443003906, policy loss: 5.701105834288452
Experience 7, Iter 22, disc loss: 0.004123307050033472, policy loss: 5.689028388074191
Experience 7, Iter 23, disc loss: 0.003911382279930882, policy loss: 5.776070995233564
Experience 7, Iter 24, disc loss: 0.003756943626816369, policy loss: 5.85652805851035
Experience 7, Iter 25, disc loss: 0.003963399382686618, policy loss: 5.754924653716998
Experience 7, Iter 26, disc loss: 0.00447717024415346, policy loss: 5.62502962893733
Experience 7, Iter 27, disc loss: 0.004463982981381677, policy loss: 5.603527756716666
Experience 7, Iter 28, disc loss: 0.0038049240269790443, policy loss: 5.823327029941078
Experience 7, Iter 29, disc loss: 0.004008095750200169, policy loss: 5.783332349452349
Experience 7, Iter 30, disc loss: 0.004282128680693223, policy loss: 5.640888849973458
Experience 7, Iter 31, disc loss: 0.0039923112266106895, policy loss: 5.768904761593988
Experience 7, Iter 32, disc loss: 0.004066457438009634, policy loss: 5.74426547994215
Experience 7, Iter 33, disc loss: 0.004308626727766714, policy loss: 5.647016810482432
Experience 7, Iter 34, disc loss: 0.0036959453689539754, policy loss: 5.91378840901309
Experience 7, Iter 35, disc loss: 0.0037975427957550856, policy loss: 5.799558906256896
Experience 7, Iter 36, disc loss: 0.004144685138675051, policy loss: 5.810167290038735
Experience 7, Iter 37, disc loss: 0.003963360920635534, policy loss: 5.8267584337366864
Experience 7, Iter 38, disc loss: 0.004150723631709723, policy loss: 5.760549685504965
Experience 7, Iter 39, disc loss: 0.004038028874118079, policy loss: 5.7566172919202785
Experience 7, Iter 40, disc loss: 0.003920864354199274, policy loss: 5.799242410280171
Experience 7, Iter 41, disc loss: 0.0037128496284617616, policy loss: 5.865648851154034
Experience 7, Iter 42, disc loss: 0.0038280529396114383, policy loss: 5.784305063052849
Experience 7, Iter 43, disc loss: 0.004034393309892105, policy loss: 5.758265675893801
Experience 7, Iter 44, disc loss: 0.0037777474772386944, policy loss: 5.866124049313847
Experience 7, Iter 45, disc loss: 0.0035518938004520974, policy loss: 5.949610628718955
Experience 7, Iter 46, disc loss: 0.003936986886476999, policy loss: 5.771378019547706
Experience 7, Iter 47, disc loss: 0.0040266508450784244, policy loss: 5.7429147269993805
Experience 7, Iter 48, disc loss: 0.0036006149785536797, policy loss: 5.9131171191310195
Experience 7, Iter 49, disc loss: 0.0033607404365333235, policy loss: 5.927027681217168
Experience 7, Iter 50, disc loss: 0.003896173304368358, policy loss: 5.808099323664641
Experience 7, Iter 51, disc loss: 0.003667374139115244, policy loss: 5.855294367311529
Experience 7, Iter 52, disc loss: 0.003642409079344411, policy loss: 5.858108089489916
Experience 7, Iter 53, disc loss: 0.0037536144134892044, policy loss: 5.873542297501439
Experience 7, Iter 54, disc loss: 0.0036292727859821854, policy loss: 5.809231422924183
Experience 7, Iter 55, disc loss: 0.0035881459484442273, policy loss: 5.850740811471141
Experience 7, Iter 56, disc loss: 0.003989729037373208, policy loss: 5.805418741899108
Experience 7, Iter 57, disc loss: 0.0037345968957847985, policy loss: 5.847408662759507
Experience 7, Iter 58, disc loss: 0.0031612878343060848, policy loss: 5.976655753667058
Experience 7, Iter 59, disc loss: 0.003423994217146533, policy loss: 5.950188582648484
Experience 7, Iter 60, disc loss: 0.0034460550022470298, policy loss: 5.873624123938962
Experience 7, Iter 61, disc loss: 0.003451092869881731, policy loss: 5.918623297298268
Experience 7, Iter 62, disc loss: 0.0033602051575078763, policy loss: 5.957473014917936
Experience 7, Iter 63, disc loss: 0.003727696327114917, policy loss: 5.850333194346013
Experience 7, Iter 64, disc loss: 0.003421183930277667, policy loss: 5.929610762833563
Experience 7, Iter 65, disc loss: 0.0033106431634532165, policy loss: 5.997134787571299
Experience 7, Iter 66, disc loss: 0.003335107106928092, policy loss: 5.989082170510089
Experience 7, Iter 67, disc loss: 0.0036508605092919933, policy loss: 5.831620725342523
Experience 7, Iter 68, disc loss: 0.003356591781030125, policy loss: 5.964242639053243
Experience 7, Iter 69, disc loss: 0.003222345349057917, policy loss: 6.007546663500507
Experience 7, Iter 70, disc loss: 0.0035218134782776358, policy loss: 5.967092810796661
Experience 7, Iter 71, disc loss: 0.0033666820738315747, policy loss: 5.975561318427204
Experience 7, Iter 72, disc loss: 0.0035820453560316333, policy loss: 5.866103303343241
Experience 7, Iter 73, disc loss: 0.003448569251908105, policy loss: 5.884014511710778
Experience 7, Iter 74, disc loss: 0.0032798897905383802, policy loss: 6.040740897595451
Experience 7, Iter 75, disc loss: 0.0035809615935222835, policy loss: 5.8991849969155705
Experience 7, Iter 76, disc loss: 0.0028292392280052986, policy loss: 6.162110359119716
Experience 7, Iter 77, disc loss: 0.0034360738715408265, policy loss: 5.943548038568548
Experience 7, Iter 78, disc loss: 0.0030836172412009224, policy loss: 6.089793130913543
Experience 7, Iter 79, disc loss: 0.003587534896311961, policy loss: 5.803903397811817
Experience 7, Iter 80, disc loss: 0.0031589885993799894, policy loss: 6.076536966107815
Experience 7, Iter 81, disc loss: 0.0034876842401646083, policy loss: 5.8951329862618485
Experience 7, Iter 82, disc loss: 0.0036930839553258256, policy loss: 5.843062799319833
Experience 7, Iter 83, disc loss: 0.0033085526512672413, policy loss: 5.961186924402929
Experience 7, Iter 84, disc loss: 0.0036118805448135238, policy loss: 5.917921948895004
Experience 7, Iter 85, disc loss: 0.003161512572465252, policy loss: 5.994982104412158
Experience 7, Iter 86, disc loss: 0.003203951095137164, policy loss: 6.03263368795663
Experience 7, Iter 87, disc loss: 0.0032487019022625803, policy loss: 5.927989329409575
Experience 7, Iter 88, disc loss: 0.0034575958824898485, policy loss: 5.940626478694117
Experience 7, Iter 89, disc loss: 0.003491057501845486, policy loss: 5.876341786825632
Experience 7, Iter 90, disc loss: 0.003005387517970979, policy loss: 6.11692503416928
Experience 7, Iter 91, disc loss: 0.0030511681744068084, policy loss: 6.048544032852883
Experience 7, Iter 92, disc loss: 0.003095597547092538, policy loss: 5.95327841936119
Experience 7, Iter 93, disc loss: 0.0032501554947203413, policy loss: 5.982186351195939
Experience 7, Iter 94, disc loss: 0.00319241231503533, policy loss: 6.026337410019502
Experience 7, Iter 95, disc loss: 0.003269039928758763, policy loss: 6.000760615242685
Experience 7, Iter 96, disc loss: 0.0036347583930481614, policy loss: 5.800189262652027
Experience 7, Iter 97, disc loss: 0.003060239431870633, policy loss: 6.082963622062756
Experience 7, Iter 98, disc loss: 0.002944532513718155, policy loss: 6.169521575376445
Experience 7, Iter 99, disc loss: 0.0030678060623897745, policy loss: 6.099943868166608
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0104],
        [0.0383],
        [0.5798],
        [0.0111]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0644, 0.3853, 0.5141, 0.0137, 0.0059, 1.4158]],

        [[0.0644, 0.3853, 0.5141, 0.0137, 0.0059, 1.4158]],

        [[0.0644, 0.3853, 0.5141, 0.0137, 0.0059, 1.4158]],

        [[0.0644, 0.3853, 0.5141, 0.0137, 0.0059, 1.4158]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0415, 0.1530, 2.3194, 0.0446], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0415, 0.1530, 2.3194, 0.0446])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.126
Iter 2/2000 - Loss: 1.894
Iter 3/2000 - Loss: 1.689
Iter 4/2000 - Loss: 1.624
Iter 5/2000 - Loss: 1.556
Iter 6/2000 - Loss: 1.422
Iter 7/2000 - Loss: 1.269
Iter 8/2000 - Loss: 1.132
Iter 9/2000 - Loss: 1.007
Iter 10/2000 - Loss: 0.870
Iter 11/2000 - Loss: 0.700
Iter 12/2000 - Loss: 0.502
Iter 13/2000 - Loss: 0.293
Iter 14/2000 - Loss: 0.087
Iter 15/2000 - Loss: -0.114
Iter 16/2000 - Loss: -0.313
Iter 17/2000 - Loss: -0.515
Iter 18/2000 - Loss: -0.725
Iter 19/2000 - Loss: -0.940
Iter 20/2000 - Loss: -1.157
Iter 1981/2000 - Loss: -7.182
Iter 1982/2000 - Loss: -7.182
Iter 1983/2000 - Loss: -7.182
Iter 1984/2000 - Loss: -7.182
Iter 1985/2000 - Loss: -7.183
Iter 1986/2000 - Loss: -7.183
Iter 1987/2000 - Loss: -7.183
Iter 1988/2000 - Loss: -7.183
Iter 1989/2000 - Loss: -7.183
Iter 1990/2000 - Loss: -7.183
Iter 1991/2000 - Loss: -7.183
Iter 1992/2000 - Loss: -7.183
Iter 1993/2000 - Loss: -7.183
Iter 1994/2000 - Loss: -7.183
Iter 1995/2000 - Loss: -7.183
Iter 1996/2000 - Loss: -7.183
Iter 1997/2000 - Loss: -7.183
Iter 1998/2000 - Loss: -7.183
Iter 1999/2000 - Loss: -7.183
Iter 2000/2000 - Loss: -7.183
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0033],
        [0.0004]])
Lengthscale: tensor([[[19.7270, 10.5423, 36.0667, 11.2457, 19.4389, 46.2793]],

        [[28.2003, 43.5335, 12.0186,  1.5543,  1.1957, 16.4319]],

        [[29.2495, 42.8844, 19.5669,  1.0484,  2.0972, 18.1090]],

        [[28.8484, 49.5515, 19.7437,  4.3154,  5.5728, 46.2613]]])
Signal Variance: tensor([ 0.1859,  0.9165, 18.0456,  0.6777])
Estimated target variance: tensor([0.0415, 0.1530, 2.3194, 0.0446])
N: 80
Signal to noise ratio: tensor([23.7013, 49.2825, 74.3740, 40.2332])
Bound on condition number: tensor([ 44941.0073, 194302.1291, 442520.6755, 129497.8796])
Policy Optimizer learning rate:
0.09926518942192228
Experience 8, Iter 0, disc loss: 0.00311295346451811, policy loss: 6.054263130751885
Experience 8, Iter 1, disc loss: 0.003122619647276277, policy loss: 6.023580536634686
Experience 8, Iter 2, disc loss: 0.003229056258069261, policy loss: 6.0084084141602006
Experience 8, Iter 3, disc loss: 0.0030483114343172113, policy loss: 6.047834267402656
Experience 8, Iter 4, disc loss: 0.0028931447096518684, policy loss: 6.10236454438978
Experience 8, Iter 5, disc loss: 0.0031528062434850396, policy loss: 6.03185725688132
Experience 8, Iter 6, disc loss: 0.003086663238063939, policy loss: 6.011723204909451
Experience 8, Iter 7, disc loss: 0.0030958507685575824, policy loss: 6.096049179421536
Experience 8, Iter 8, disc loss: 0.003092401619983003, policy loss: 6.044542004190104
Experience 8, Iter 9, disc loss: 0.0028750345313773872, policy loss: 6.107026569886101
Experience 8, Iter 10, disc loss: 0.0032830425401391802, policy loss: 6.009471817646304
Experience 8, Iter 11, disc loss: 0.0029742812975853865, policy loss: 6.156230635829152
Experience 8, Iter 12, disc loss: 0.0031040181661386177, policy loss: 6.036118163343367
Experience 8, Iter 13, disc loss: 0.0029361216970423833, policy loss: 6.107663465343161
Experience 8, Iter 14, disc loss: 0.0033103590314117475, policy loss: 5.973500671399251
Experience 8, Iter 15, disc loss: 0.003006057357966947, policy loss: 6.05248330472985
Experience 8, Iter 16, disc loss: 0.002878515034624494, policy loss: 6.1273011773899775
Experience 8, Iter 17, disc loss: 0.002864688601757906, policy loss: 6.064425905693969
Experience 8, Iter 18, disc loss: 0.002754121280035656, policy loss: 6.1963045233930885
Experience 8, Iter 19, disc loss: 0.002931839604600561, policy loss: 6.177354329030656
Experience 8, Iter 20, disc loss: 0.0029845873871408566, policy loss: 6.07474037432643
Experience 8, Iter 21, disc loss: 0.0029722669558175023, policy loss: 6.124822908203674
Experience 8, Iter 22, disc loss: 0.0023808033720548183, policy loss: 6.306435999166945
Experience 8, Iter 23, disc loss: 0.002980271362553693, policy loss: 6.067395363108485
Experience 8, Iter 24, disc loss: 0.002599780054216457, policy loss: 6.296569064897123
Experience 8, Iter 25, disc loss: 0.0028158682400970074, policy loss: 6.190328387167684
Experience 8, Iter 26, disc loss: 0.0025664933874891926, policy loss: 6.247274795546079
Experience 8, Iter 27, disc loss: 0.002931638162768096, policy loss: 6.054195329939002
Experience 8, Iter 28, disc loss: 0.00270884295418532, policy loss: 6.269752201106105
Experience 8, Iter 29, disc loss: 0.002738757686405891, policy loss: 6.183643411722837
Experience 8, Iter 30, disc loss: 0.0026302552923737734, policy loss: 6.177382904829807
Experience 8, Iter 31, disc loss: 0.002684615742468434, policy loss: 6.2521670536589395
Experience 8, Iter 32, disc loss: 0.002727678545868908, policy loss: 6.212559700730532
Experience 8, Iter 33, disc loss: 0.0027419674548919085, policy loss: 6.205115091991569
Experience 8, Iter 34, disc loss: 0.0024323399071671764, policy loss: 6.365843101783582
Experience 8, Iter 35, disc loss: 0.002691838952089108, policy loss: 6.179940595003715
Experience 8, Iter 36, disc loss: 0.0027118106659090066, policy loss: 6.237913572232616
Experience 8, Iter 37, disc loss: 0.0026747175889390365, policy loss: 6.194474633703847
Experience 8, Iter 38, disc loss: 0.0024589267044154263, policy loss: 6.433476951290386
Experience 8, Iter 39, disc loss: 0.0025668694257451885, policy loss: 6.270396274461166
Experience 8, Iter 40, disc loss: 0.0026000286077752133, policy loss: 6.309996334049203
Experience 8, Iter 41, disc loss: 0.002525210374844286, policy loss: 6.257146253919328
Experience 8, Iter 42, disc loss: 0.0026146626316799425, policy loss: 6.142636224303142
Experience 8, Iter 43, disc loss: 0.0025348686042527256, policy loss: 6.276827128926186
Experience 8, Iter 44, disc loss: 0.0027271722268145423, policy loss: 6.1356658725116
Experience 8, Iter 45, disc loss: 0.002558295550903744, policy loss: 6.30268605678635
Experience 8, Iter 46, disc loss: 0.002827465074321053, policy loss: 6.2069078869755305
Experience 8, Iter 47, disc loss: 0.00263043351208, policy loss: 6.2136034414677415
Experience 8, Iter 48, disc loss: 0.002956124717822822, policy loss: 6.073250386334745
Experience 8, Iter 49, disc loss: 0.002933059474227594, policy loss: 6.20220438391412
Experience 8, Iter 50, disc loss: 0.0026504433787900195, policy loss: 6.183424242414988
Experience 8, Iter 51, disc loss: 0.002536556716223764, policy loss: 6.271658761294404
Experience 8, Iter 52, disc loss: 0.002682216519133176, policy loss: 6.323734632702076
Experience 8, Iter 53, disc loss: 0.0021660488536412686, policy loss: 6.507985209339543
Experience 8, Iter 54, disc loss: 0.0024886041523388765, policy loss: 6.340873254381841
Experience 8, Iter 55, disc loss: 0.002428483863148332, policy loss: 6.326667316123052
Experience 8, Iter 56, disc loss: 0.002501537252197079, policy loss: 6.348577923632227
Experience 8, Iter 57, disc loss: 0.0028678460223501875, policy loss: 6.141432163730084
Experience 8, Iter 58, disc loss: 0.0024980073603708495, policy loss: 6.353465838547523
Experience 8, Iter 59, disc loss: 0.0022677640134252585, policy loss: 6.456209708278121
Experience 8, Iter 60, disc loss: 0.0023810029101304373, policy loss: 6.354497252838238
Experience 8, Iter 61, disc loss: 0.0023856043116075682, policy loss: 6.386873379247003
Experience 8, Iter 62, disc loss: 0.0021723947923057495, policy loss: 6.426720921272196
Experience 8, Iter 63, disc loss: 0.002408943445489248, policy loss: 6.331714478069745
Experience 8, Iter 64, disc loss: 0.002608454426429145, policy loss: 6.209437067923224
Experience 8, Iter 65, disc loss: 0.0026619050459041464, policy loss: 6.213133917847346
Experience 8, Iter 66, disc loss: 0.002484993667725124, policy loss: 6.400408859232781
Experience 8, Iter 67, disc loss: 0.002462489834023591, policy loss: 6.226825924990592
Experience 8, Iter 68, disc loss: 0.0021307738047439562, policy loss: 6.483856135703008
Experience 8, Iter 69, disc loss: 0.0025551504981515187, policy loss: 6.294350135515088
Experience 8, Iter 70, disc loss: 0.0023937673498325316, policy loss: 6.280439945403878
Experience 8, Iter 71, disc loss: 0.002628508566559767, policy loss: 6.220344178266216
Experience 8, Iter 72, disc loss: 0.0023911802330265556, policy loss: 6.345131978982609
Experience 8, Iter 73, disc loss: 0.002417861454137249, policy loss: 6.282941913354101
Experience 8, Iter 74, disc loss: 0.002329319359070319, policy loss: 6.347423490624914
Experience 8, Iter 75, disc loss: 0.002065148877961663, policy loss: 6.5241554675372875
Experience 8, Iter 76, disc loss: 0.002057788468735949, policy loss: 6.659202826327435
Experience 8, Iter 77, disc loss: 0.0025379367095212865, policy loss: 6.3358490906787335
Experience 8, Iter 78, disc loss: 0.0022603605604072643, policy loss: 6.406698314431573
Experience 8, Iter 79, disc loss: 0.001973621119427159, policy loss: 6.588319542375573
Experience 8, Iter 80, disc loss: 0.0022122035083240693, policy loss: 6.453675188345004
Experience 8, Iter 81, disc loss: 0.00203184805805825, policy loss: 6.533396261067105
Experience 8, Iter 82, disc loss: 0.00250201004874688, policy loss: 6.251529216627041
Experience 8, Iter 83, disc loss: 0.0022544771397848646, policy loss: 6.433976711216425
Experience 8, Iter 84, disc loss: 0.0020147613917612093, policy loss: 6.545275447233344
Experience 8, Iter 85, disc loss: 0.0022946350096846504, policy loss: 6.347728200963912
Experience 8, Iter 86, disc loss: 0.002276182422879633, policy loss: 6.406820857680334
Experience 8, Iter 87, disc loss: 0.0022131633258049934, policy loss: 6.374016632359933
Experience 8, Iter 88, disc loss: 0.0021179903738442083, policy loss: 6.482612843703845
Experience 8, Iter 89, disc loss: 0.002400200077118469, policy loss: 6.265762324902395
Experience 8, Iter 90, disc loss: 0.0025235277608908462, policy loss: 6.254539831152284
Experience 8, Iter 91, disc loss: 0.0021121594473155246, policy loss: 6.408436400158486
Experience 8, Iter 92, disc loss: 0.0018992319154995465, policy loss: 6.651400703863764
Experience 8, Iter 93, disc loss: 0.0021657305851784164, policy loss: 6.452270601513915
Experience 8, Iter 94, disc loss: 0.0021735074275522604, policy loss: 6.458980560673589
Experience 8, Iter 95, disc loss: 0.0024402022504972846, policy loss: 6.387170513145498
Experience 8, Iter 96, disc loss: 0.0021259755505126736, policy loss: 6.4904723029021225
Experience 8, Iter 97, disc loss: 0.0021538834688177334, policy loss: 6.462574976000071
Experience 8, Iter 98, disc loss: 0.0022648812495051672, policy loss: 6.281970511091462
Experience 8, Iter 99, disc loss: 0.0021235943380517605, policy loss: 6.566032128589839
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0095],
        [0.0353],
        [0.5199],
        [0.0100]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0581, 0.3534, 0.4609, 0.0125, 0.0054, 1.3180]],

        [[0.0581, 0.3534, 0.4609, 0.0125, 0.0054, 1.3180]],

        [[0.0581, 0.3534, 0.4609, 0.0125, 0.0054, 1.3180]],

        [[0.0581, 0.3534, 0.4609, 0.0125, 0.0054, 1.3180]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0381, 0.1414, 2.0797, 0.0400], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0381, 0.1414, 2.0797, 0.0400])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.934
Iter 2/2000 - Loss: 1.738
Iter 3/2000 - Loss: 1.550
Iter 4/2000 - Loss: 1.495
Iter 5/2000 - Loss: 1.424
Iter 6/2000 - Loss: 1.287
Iter 7/2000 - Loss: 1.137
Iter 8/2000 - Loss: 1.004
Iter 9/2000 - Loss: 0.874
Iter 10/2000 - Loss: 0.719
Iter 11/2000 - Loss: 0.529
Iter 12/2000 - Loss: 0.318
Iter 13/2000 - Loss: 0.098
Iter 14/2000 - Loss: -0.124
Iter 15/2000 - Loss: -0.347
Iter 16/2000 - Loss: -0.575
Iter 17/2000 - Loss: -0.810
Iter 18/2000 - Loss: -1.050
Iter 19/2000 - Loss: -1.291
Iter 20/2000 - Loss: -1.528
Iter 1981/2000 - Loss: -7.443
Iter 1982/2000 - Loss: -7.443
Iter 1983/2000 - Loss: -7.443
Iter 1984/2000 - Loss: -7.443
Iter 1985/2000 - Loss: -7.443
Iter 1986/2000 - Loss: -7.443
Iter 1987/2000 - Loss: -7.443
Iter 1988/2000 - Loss: -7.443
Iter 1989/2000 - Loss: -7.443
Iter 1990/2000 - Loss: -7.443
Iter 1991/2000 - Loss: -7.444
Iter 1992/2000 - Loss: -7.444
Iter 1993/2000 - Loss: -7.444
Iter 1994/2000 - Loss: -7.444
Iter 1995/2000 - Loss: -7.444
Iter 1996/2000 - Loss: -7.444
Iter 1997/2000 - Loss: -7.444
Iter 1998/2000 - Loss: -7.444
Iter 1999/2000 - Loss: -7.444
Iter 2000/2000 - Loss: -7.444
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0031],
        [0.0004]])
Lengthscale: tensor([[[18.1393, 10.4254, 34.4262, 11.3805, 19.6070, 46.0906]],

        [[25.7592, 42.2870, 11.9488,  1.5045,  1.1925, 16.1815]],

        [[27.4757, 42.4470, 19.3637,  1.0370,  2.0531, 17.8663]],

        [[25.8188, 48.4471, 19.4491,  4.1457,  4.9264, 45.3629]]])
Signal Variance: tensor([ 0.1791,  0.8782, 17.2973,  0.6357])
Estimated target variance: tensor([0.0381, 0.1414, 2.0797, 0.0400])
N: 90
Signal to noise ratio: tensor([23.3725, 49.8176, 75.0269, 39.9156])
Bound on condition number: tensor([ 49165.6301, 223362.7254, 506614.1014, 143394.1515])
Policy Optimizer learning rate:
0.09916065818347441
Experience 9, Iter 0, disc loss: 0.00220782417692628, policy loss: 6.342753242355993
Experience 9, Iter 1, disc loss: 0.002219965810912195, policy loss: 6.3809997171675015
Experience 9, Iter 2, disc loss: 0.0020884452358422042, policy loss: 6.495072009192004
Experience 9, Iter 3, disc loss: 0.0021076330432671306, policy loss: 6.435510889368423
Experience 9, Iter 4, disc loss: 0.002307559759347187, policy loss: 6.444066942186462
Experience 9, Iter 5, disc loss: 0.00214456097896907, policy loss: 6.418768900174124
Experience 9, Iter 6, disc loss: 0.002485966621962591, policy loss: 6.307358584734024
Experience 9, Iter 7, disc loss: 0.00210443454656272, policy loss: 6.450995474142335
Experience 9, Iter 8, disc loss: 0.0021166979481119894, policy loss: 6.47510761146429
Experience 9, Iter 9, disc loss: 0.002539282349330769, policy loss: 6.24334018785964
Experience 9, Iter 10, disc loss: 0.001956019402217857, policy loss: 6.559690285826728
Experience 9, Iter 11, disc loss: 0.0020333653715911676, policy loss: 6.5106606734086245
Experience 9, Iter 12, disc loss: 0.002155868419334081, policy loss: 6.417446742006075
Experience 9, Iter 13, disc loss: 0.002325605261211939, policy loss: 6.393588064833235
Experience 9, Iter 14, disc loss: 0.002101912143956019, policy loss: 6.544105552266833
Experience 9, Iter 15, disc loss: 0.0022144104642217366, policy loss: 6.405112839822998
Experience 9, Iter 16, disc loss: 0.0021267026459883477, policy loss: 6.47138185137805
Experience 9, Iter 17, disc loss: 0.0019498052877022394, policy loss: 6.581163133987872
Experience 9, Iter 18, disc loss: 0.00192427550507689, policy loss: 6.533965777783899
Experience 9, Iter 19, disc loss: 0.0023803303935310692, policy loss: 6.409033234686559
Experience 9, Iter 20, disc loss: 0.0019635159649723385, policy loss: 6.468478497627382
Experience 9, Iter 21, disc loss: 0.0020046888178592726, policy loss: 6.476501852857604
Experience 9, Iter 22, disc loss: 0.0019579044109395115, policy loss: 6.548071209926331
Experience 9, Iter 23, disc loss: 0.0018170722283575226, policy loss: 6.583396720883944
Experience 9, Iter 24, disc loss: 0.0018422078422833362, policy loss: 6.6795078961959025
Experience 9, Iter 25, disc loss: 0.0023653794943137387, policy loss: 6.351520412550348
Experience 9, Iter 26, disc loss: 0.0020243998034027884, policy loss: 6.453247738979661
Experience 9, Iter 27, disc loss: 0.0020092393298373696, policy loss: 6.500862389256968
Experience 9, Iter 28, disc loss: 0.0018892956091437997, policy loss: 6.531204288796089
Experience 9, Iter 29, disc loss: 0.0023312606689733268, policy loss: 6.461534346047691
Experience 9, Iter 30, disc loss: 0.002012278270137925, policy loss: 6.558615067689069
Experience 9, Iter 31, disc loss: 0.0018892208262338045, policy loss: 6.601263721611571
Experience 9, Iter 32, disc loss: 0.0020829415732336863, policy loss: 6.491040376630097
Experience 9, Iter 33, disc loss: 0.001998552767459736, policy loss: 6.51392250166202
Experience 9, Iter 34, disc loss: 0.0020002268639586662, policy loss: 6.532427167053064
Experience 9, Iter 35, disc loss: 0.002131597991699134, policy loss: 6.456921449034069
Experience 9, Iter 36, disc loss: 0.001923865900888415, policy loss: 6.605081116404232
Experience 9, Iter 37, disc loss: 0.0018429849885802049, policy loss: 6.5810065456287905
Experience 9, Iter 38, disc loss: 0.0019233403109234605, policy loss: 6.610780702230986
Experience 9, Iter 39, disc loss: 0.0020319846542333894, policy loss: 6.515504930635407
Experience 9, Iter 40, disc loss: 0.0016967870013058738, policy loss: 6.726497023723537
Experience 9, Iter 41, disc loss: 0.001883002083043946, policy loss: 6.584671151676607
Experience 9, Iter 42, disc loss: 0.001922940647635181, policy loss: 6.564720358369472
Experience 9, Iter 43, disc loss: 0.001714120405129572, policy loss: 6.6453016717626685
Experience 9, Iter 44, disc loss: 0.0018682284880419293, policy loss: 6.638303093423365
Experience 9, Iter 45, disc loss: 0.0017170334019384259, policy loss: 6.745756865971668
Experience 9, Iter 46, disc loss: 0.0019287364002969168, policy loss: 6.519847379178132
Experience 9, Iter 47, disc loss: 0.002072670488530241, policy loss: 6.478469162975608
Experience 9, Iter 48, disc loss: 0.0017315547120169755, policy loss: 6.789060530215627
Experience 9, Iter 49, disc loss: 0.0018343277853953073, policy loss: 6.659852029100604
Experience 9, Iter 50, disc loss: 0.0018233690379631823, policy loss: 6.574699295586507
Experience 9, Iter 51, disc loss: 0.0020159276441444373, policy loss: 6.465530848339597
Experience 9, Iter 52, disc loss: 0.0018779859366381763, policy loss: 6.54682478302763
Experience 9, Iter 53, disc loss: 0.0019443251079884422, policy loss: 6.594206103086471
Experience 9, Iter 54, disc loss: 0.0021432285547153653, policy loss: 6.44338675682028
Experience 9, Iter 55, disc loss: 0.002067033283116215, policy loss: 6.493001790607366
Experience 9, Iter 56, disc loss: 0.001715155009180881, policy loss: 6.704278347441974
Experience 9, Iter 57, disc loss: 0.0018275845003819303, policy loss: 6.644830267019254
Experience 9, Iter 58, disc loss: 0.0019194433618701437, policy loss: 6.578864101698443
Experience 9, Iter 59, disc loss: 0.002098407718025744, policy loss: 6.53256843173145
Experience 9, Iter 60, disc loss: 0.0019358869265054843, policy loss: 6.566085731396041
Experience 9, Iter 61, disc loss: 0.0019021326900655988, policy loss: 6.5114324109062744
Experience 9, Iter 62, disc loss: 0.001899619987781082, policy loss: 6.533827168341672
Experience 9, Iter 63, disc loss: 0.001665826120654839, policy loss: 6.799542931074304
Experience 9, Iter 64, disc loss: 0.0019049201460093097, policy loss: 6.558817190788545
Experience 9, Iter 65, disc loss: 0.0018782315664325063, policy loss: 6.759851060417236
Experience 9, Iter 66, disc loss: 0.0018650183168824473, policy loss: 6.802056034683021
Experience 9, Iter 67, disc loss: 0.0019354214835126742, policy loss: 6.599369715740632
Experience 9, Iter 68, disc loss: 0.0017022681756984157, policy loss: 6.735346630841446
Experience 9, Iter 69, disc loss: 0.0017730728967397852, policy loss: 6.617355069693467
Experience 9, Iter 70, disc loss: 0.0017598757294007943, policy loss: 6.694351358050081
Experience 9, Iter 71, disc loss: 0.0017216942281526727, policy loss: 6.721685310346906
Experience 9, Iter 72, disc loss: 0.001789137485069711, policy loss: 6.644530510987047
Experience 9, Iter 73, disc loss: 0.0016533505464078053, policy loss: 6.686524718758637
Experience 9, Iter 74, disc loss: 0.0017907626193836821, policy loss: 6.622841217557971
Experience 9, Iter 75, disc loss: 0.0017092901710926518, policy loss: 6.717532547994078
Experience 9, Iter 76, disc loss: 0.0017687297918590216, policy loss: 6.696591564055715
Experience 9, Iter 77, disc loss: 0.001666907592401961, policy loss: 6.724949506801167
Experience 9, Iter 78, disc loss: 0.0015676417120060078, policy loss: 6.859733534326373
Experience 9, Iter 79, disc loss: 0.0019028110401273642, policy loss: 6.624827585661322
Experience 9, Iter 80, disc loss: 0.0018296297658077214, policy loss: 6.646672208414332
Experience 9, Iter 81, disc loss: 0.0016295096470156044, policy loss: 6.783458322553596
Experience 9, Iter 82, disc loss: 0.0015819365517523894, policy loss: 6.742820284836611
Experience 9, Iter 83, disc loss: 0.001486140741331061, policy loss: 6.903694451397208
Experience 9, Iter 84, disc loss: 0.0016906115965948517, policy loss: 6.776717190606422
Experience 9, Iter 85, disc loss: 0.001641448100879195, policy loss: 6.690073740879934
Experience 9, Iter 86, disc loss: 0.0018125550175974539, policy loss: 6.691982007193764
Experience 9, Iter 87, disc loss: 0.0014813506296831082, policy loss: 6.8292690380465775
Experience 9, Iter 88, disc loss: 0.001682470163763031, policy loss: 6.671755479709095
Experience 9, Iter 89, disc loss: 0.0020774573377972602, policy loss: 6.48007150228505
Experience 9, Iter 90, disc loss: 0.0016516603284085392, policy loss: 6.699930976866616
Experience 9, Iter 91, disc loss: 0.001678487879036773, policy loss: 6.677334024991693
Experience 9, Iter 92, disc loss: 0.0018137600684826377, policy loss: 6.674427077415906
Experience 9, Iter 93, disc loss: 0.001560896094283528, policy loss: 6.837984127787654
Experience 9, Iter 94, disc loss: 0.0014667403998725877, policy loss: 6.955724896016994
Experience 9, Iter 95, disc loss: 0.0017425393755121353, policy loss: 6.588746822694607
Experience 9, Iter 96, disc loss: 0.0015872810084489209, policy loss: 6.778766533479704
Experience 9, Iter 97, disc loss: 0.0017916569122552804, policy loss: 6.60186814504216
Experience 9, Iter 98, disc loss: 0.001847930149062059, policy loss: 6.659089833624349
Experience 9, Iter 99, disc loss: 0.0018048013422422037, policy loss: 6.673126157767784
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0089],
        [0.0327],
        [0.4689],
        [0.0090]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0532, 0.3271, 0.4155, 0.0115, 0.0049, 1.2278]],

        [[0.0532, 0.3271, 0.4155, 0.0115, 0.0049, 1.2278]],

        [[0.0532, 0.3271, 0.4155, 0.0115, 0.0049, 1.2278]],

        [[0.0532, 0.3271, 0.4155, 0.0115, 0.0049, 1.2278]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0354, 0.1308, 1.8756, 0.0360], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0354, 0.1308, 1.8756, 0.0360])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.738
Iter 2/2000 - Loss: 1.561
Iter 3/2000 - Loss: 1.373
Iter 4/2000 - Loss: 1.310
Iter 5/2000 - Loss: 1.227
Iter 6/2000 - Loss: 1.082
Iter 7/2000 - Loss: 0.925
Iter 8/2000 - Loss: 0.781
Iter 9/2000 - Loss: 0.633
Iter 10/2000 - Loss: 0.456
Iter 11/2000 - Loss: 0.250
Iter 12/2000 - Loss: 0.026
Iter 13/2000 - Loss: -0.204
Iter 14/2000 - Loss: -0.438
Iter 15/2000 - Loss: -0.676
Iter 16/2000 - Loss: -0.920
Iter 17/2000 - Loss: -1.170
Iter 18/2000 - Loss: -1.422
Iter 19/2000 - Loss: -1.673
Iter 20/2000 - Loss: -1.918
Iter 1981/2000 - Loss: -7.691
Iter 1982/2000 - Loss: -7.691
Iter 1983/2000 - Loss: -7.692
Iter 1984/2000 - Loss: -7.692
Iter 1985/2000 - Loss: -7.692
Iter 1986/2000 - Loss: -7.692
Iter 1987/2000 - Loss: -7.692
Iter 1988/2000 - Loss: -7.692
Iter 1989/2000 - Loss: -7.692
Iter 1990/2000 - Loss: -7.692
Iter 1991/2000 - Loss: -7.692
Iter 1992/2000 - Loss: -7.692
Iter 1993/2000 - Loss: -7.692
Iter 1994/2000 - Loss: -7.692
Iter 1995/2000 - Loss: -7.692
Iter 1996/2000 - Loss: -7.692
Iter 1997/2000 - Loss: -7.692
Iter 1998/2000 - Loss: -7.692
Iter 1999/2000 - Loss: -7.692
Iter 2000/2000 - Loss: -7.692
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[15.7726,  9.9722, 32.2191, 10.1712, 19.0118, 40.2823]],

        [[24.2760, 40.9811, 12.1503,  1.5897,  1.1804, 16.7656]],

        [[26.0900, 41.7800, 19.2784,  1.0323,  1.9870, 17.6952]],

        [[24.0194, 46.6643, 19.2913,  3.9607,  4.4706, 44.7040]]])
Signal Variance: tensor([ 0.1669,  0.9411, 16.8132,  0.6058])
Estimated target variance: tensor([0.0354, 0.1308, 1.8756, 0.0360])
N: 100
Signal to noise ratio: tensor([23.4801, 51.3403, 76.6510, 40.6052])
Bound on condition number: tensor([ 55132.4350, 263583.5360, 587538.3953, 164879.3411])
Policy Optimizer learning rate:
0.09905623702167957
Experience 10, Iter 0, disc loss: 0.0016837135419743871, policy loss: 6.744624518543118
Experience 10, Iter 1, disc loss: 0.001473198622384277, policy loss: 6.9629392598668485
Experience 10, Iter 2, disc loss: 0.0019003933064520124, policy loss: 6.58532356711688
Experience 10, Iter 3, disc loss: 0.0016944206563845098, policy loss: 6.7766438449208515
Experience 10, Iter 4, disc loss: 0.001858271099658979, policy loss: 6.696155204018609
Experience 10, Iter 5, disc loss: 0.0016956115289203388, policy loss: 6.750110452686955
Experience 10, Iter 6, disc loss: 0.0014984163863005042, policy loss: 6.865024789100695
Experience 10, Iter 7, disc loss: 0.0015246593758834136, policy loss: 6.820909158027165
Experience 10, Iter 8, disc loss: 0.0015190583232171661, policy loss: 6.806984721818013
Experience 10, Iter 9, disc loss: 0.0017671585504639457, policy loss: 6.642768900523508
Experience 10, Iter 10, disc loss: 0.0018193578149004047, policy loss: 6.687244492470289
Experience 10, Iter 11, disc loss: 0.0016630212719918427, policy loss: 6.728742899383083
Experience 10, Iter 12, disc loss: 0.001621112330354056, policy loss: 6.778006443117209
Experience 10, Iter 13, disc loss: 0.0016077805076725336, policy loss: 6.747274621546725
Experience 10, Iter 14, disc loss: 0.0016578102865076737, policy loss: 6.781040736318921
Experience 10, Iter 15, disc loss: 0.001643549574136292, policy loss: 6.692921662089032
Experience 10, Iter 16, disc loss: 0.001360074265092948, policy loss: 7.021759919749405
Experience 10, Iter 17, disc loss: 0.001468432089835763, policy loss: 6.81786145382657
Experience 10, Iter 18, disc loss: 0.001625990163765949, policy loss: 6.621967702961681
Experience 10, Iter 19, disc loss: 0.0014970138720233224, policy loss: 6.825202202546086
Experience 10, Iter 20, disc loss: 0.0015044961759030175, policy loss: 6.844106551829432
Experience 10, Iter 21, disc loss: 0.0016964345632534093, policy loss: 6.708232327576643
Experience 10, Iter 22, disc loss: 0.0014098923791149723, policy loss: 6.919050227670995
Experience 10, Iter 23, disc loss: 0.0015726152736813413, policy loss: 6.808495763218536
Experience 10, Iter 24, disc loss: 0.001503782405148846, policy loss: 6.925251758749933
Experience 10, Iter 25, disc loss: 0.0015351725070780183, policy loss: 6.799870635898067
Experience 10, Iter 26, disc loss: 0.0013977135313461369, policy loss: 6.958448001319555
Experience 10, Iter 27, disc loss: 0.0015888508145155337, policy loss: 6.713002813715988
Experience 10, Iter 28, disc loss: 0.0013821974324390981, policy loss: 6.968934435275678
Experience 10, Iter 29, disc loss: 0.0015505110250373846, policy loss: 6.811551263691872
Experience 10, Iter 30, disc loss: 0.0014539730618624903, policy loss: 6.850839409334302
Experience 10, Iter 31, disc loss: 0.001588172165368403, policy loss: 6.774717967129055
Experience 10, Iter 32, disc loss: 0.0015757378991904088, policy loss: 6.84238000373756
Experience 10, Iter 33, disc loss: 0.0014097657479842601, policy loss: 6.91955128772371
Experience 10, Iter 34, disc loss: 0.0012459165962259927, policy loss: 7.172672279271307
Experience 10, Iter 35, disc loss: 0.0013939186655100042, policy loss: 6.995416926932164
Experience 10, Iter 36, disc loss: 0.0016099655267420093, policy loss: 6.708110994210243
Experience 10, Iter 37, disc loss: 0.0014596710813090641, policy loss: 6.828831961336124
Experience 10, Iter 38, disc loss: 0.0012433373684458205, policy loss: 7.072465256613908
Experience 10, Iter 39, disc loss: 0.001574458588871086, policy loss: 6.721755273037908
Experience 10, Iter 40, disc loss: 0.0015210807362694076, policy loss: 6.764789967075114
Experience 10, Iter 41, disc loss: 0.0015284369634888516, policy loss: 6.832785085507259
Experience 10, Iter 42, disc loss: 0.0016479939713056105, policy loss: 6.7611942085295
Experience 10, Iter 43, disc loss: 0.0014441441312360746, policy loss: 6.874096237601722
Experience 10, Iter 44, disc loss: 0.0012902956296723844, policy loss: 6.995145486350895
Experience 10, Iter 45, disc loss: 0.0014590612829679147, policy loss: 6.837943703464894
Experience 10, Iter 46, disc loss: 0.0013895926564287692, policy loss: 6.9385338199002655
Experience 10, Iter 47, disc loss: 0.0014961928849258636, policy loss: 6.775958255528113
Experience 10, Iter 48, disc loss: 0.0014574337975296562, policy loss: 6.921378915353884
Experience 10, Iter 49, disc loss: 0.0014485321278332542, policy loss: 6.835273508671598
Experience 10, Iter 50, disc loss: 0.001566386275637655, policy loss: 6.80555208321077
Experience 10, Iter 51, disc loss: 0.001294343534074966, policy loss: 7.007246487911614
Experience 10, Iter 52, disc loss: 0.0014506908447281903, policy loss: 6.8103172577819615
Experience 10, Iter 53, disc loss: 0.0016322525667385084, policy loss: 6.743930480337822
Experience 10, Iter 54, disc loss: 0.001341650511238876, policy loss: 6.918000996305464
Experience 10, Iter 55, disc loss: 0.0013310177966011651, policy loss: 6.9184815245542115
Experience 10, Iter 56, disc loss: 0.001389951610081845, policy loss: 6.8363155288396475
Experience 10, Iter 57, disc loss: 0.0014046772267230902, policy loss: 6.951585630078462
Experience 10, Iter 58, disc loss: 0.0013983578021942254, policy loss: 6.957779488039227
Experience 10, Iter 59, disc loss: 0.0015489961955307023, policy loss: 6.865920810378576
Experience 10, Iter 60, disc loss: 0.0013003058662616057, policy loss: 7.044158551065593
Experience 10, Iter 61, disc loss: 0.0014697227682585426, policy loss: 6.847820079185527
Experience 10, Iter 62, disc loss: 0.0013880080358151085, policy loss: 6.934027396325015
Experience 10, Iter 63, disc loss: 0.0012698740815422556, policy loss: 7.025384484417064
Experience 10, Iter 64, disc loss: 0.001351032519116609, policy loss: 6.939543403044334
Experience 10, Iter 65, disc loss: 0.0011814758196634486, policy loss: 7.094593140400834
Experience 10, Iter 66, disc loss: 0.001317108061379921, policy loss: 6.915369771123261
Experience 10, Iter 67, disc loss: 0.0013561580276773234, policy loss: 6.8924308245081525
Experience 10, Iter 68, disc loss: 0.0014465671891217193, policy loss: 6.909365807639657
Experience 10, Iter 69, disc loss: 0.0014953040190946458, policy loss: 6.927633385222686
Experience 10, Iter 70, disc loss: 0.0012767032177933694, policy loss: 7.008526613239557
Experience 10, Iter 71, disc loss: 0.0014823796573563335, policy loss: 6.784639277829111
Experience 10, Iter 72, disc loss: 0.0015119604536832144, policy loss: 6.827174220990181
Experience 10, Iter 73, disc loss: 0.0012910489743078073, policy loss: 6.995865345080807
Experience 10, Iter 74, disc loss: 0.0015585775375663985, policy loss: 6.827584299778733
Experience 10, Iter 75, disc loss: 0.0012533250892664619, policy loss: 6.996094685299694
Experience 10, Iter 76, disc loss: 0.0013751395157013233, policy loss: 6.984985231773509
Experience 10, Iter 77, disc loss: 0.001401836848717405, policy loss: 6.880473555522275
Experience 10, Iter 78, disc loss: 0.0016012532201038947, policy loss: 6.757312404777891
Experience 10, Iter 79, disc loss: 0.0014097336391366744, policy loss: 6.923688199193579
Experience 10, Iter 80, disc loss: 0.0014679302954012595, policy loss: 6.877857006156142
Experience 10, Iter 81, disc loss: 0.0011783623853728528, policy loss: 7.160652000466534
Experience 10, Iter 82, disc loss: 0.0014736430281073196, policy loss: 6.9567776137947686
Experience 10, Iter 83, disc loss: 0.0012524013644924325, policy loss: 7.0580211774040205
Experience 10, Iter 84, disc loss: 0.0012939709655192132, policy loss: 7.092010858487013
Experience 10, Iter 85, disc loss: 0.0013662090256153535, policy loss: 6.993880236443575
Experience 10, Iter 86, disc loss: 0.0013624999273332708, policy loss: 6.978609081824561
Experience 10, Iter 87, disc loss: 0.0013569684611490207, policy loss: 6.983506043259016
Experience 10, Iter 88, disc loss: 0.0012567074260388072, policy loss: 6.967301469264503
Experience 10, Iter 89, disc loss: 0.0012958044632659094, policy loss: 7.078415662936669
Experience 10, Iter 90, disc loss: 0.0012759117199548221, policy loss: 6.938733547496704
Experience 10, Iter 91, disc loss: 0.0011386189858614737, policy loss: 7.145405468059544
Experience 10, Iter 92, disc loss: 0.0013745516185019926, policy loss: 6.920659704276409
Experience 10, Iter 93, disc loss: 0.0011852108555970764, policy loss: 7.131730730667849
Experience 10, Iter 94, disc loss: 0.0013975442142927228, policy loss: 6.854224672973778
Experience 10, Iter 95, disc loss: 0.0012224559841203717, policy loss: 7.07349398517587
Experience 10, Iter 96, disc loss: 0.0012366258821667716, policy loss: 7.042648075557741
Experience 10, Iter 97, disc loss: 0.0014984767937686989, policy loss: 6.856047885489624
Experience 10, Iter 98, disc loss: 0.0011406368670877141, policy loss: 7.076037204381683
Experience 10, Iter 99, disc loss: 0.001196712926325678, policy loss: 7.144347418120908
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0082],
        [0.0308],
        [0.4347],
        [0.0083]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0495, 0.3036, 0.3847, 0.0107, 0.0045, 1.1488]],

        [[0.0495, 0.3036, 0.3847, 0.0107, 0.0045, 1.1488]],

        [[0.0495, 0.3036, 0.3847, 0.0107, 0.0045, 1.1488]],

        [[0.0495, 0.3036, 0.3847, 0.0107, 0.0045, 1.1488]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0329, 0.1231, 1.7387, 0.0334], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0329, 0.1231, 1.7387, 0.0334])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.576
Iter 2/2000 - Loss: 1.412
Iter 3/2000 - Loss: 1.248
Iter 4/2000 - Loss: 1.185
Iter 5/2000 - Loss: 1.099
Iter 6/2000 - Loss: 0.965
Iter 7/2000 - Loss: 0.820
Iter 8/2000 - Loss: 0.672
Iter 9/2000 - Loss: 0.510
Iter 10/2000 - Loss: 0.325
Iter 11/2000 - Loss: 0.118
Iter 12/2000 - Loss: -0.108
Iter 13/2000 - Loss: -0.348
Iter 14/2000 - Loss: -0.597
Iter 15/2000 - Loss: -0.855
Iter 16/2000 - Loss: -1.119
Iter 17/2000 - Loss: -1.388
Iter 18/2000 - Loss: -1.658
Iter 19/2000 - Loss: -1.923
Iter 20/2000 - Loss: -2.182
Iter 1981/2000 - Loss: -7.809
Iter 1982/2000 - Loss: -7.810
Iter 1983/2000 - Loss: -7.810
Iter 1984/2000 - Loss: -7.810
Iter 1985/2000 - Loss: -7.810
Iter 1986/2000 - Loss: -7.810
Iter 1987/2000 - Loss: -7.810
Iter 1988/2000 - Loss: -7.810
Iter 1989/2000 - Loss: -7.810
Iter 1990/2000 - Loss: -7.810
Iter 1991/2000 - Loss: -7.810
Iter 1992/2000 - Loss: -7.810
Iter 1993/2000 - Loss: -7.810
Iter 1994/2000 - Loss: -7.810
Iter 1995/2000 - Loss: -7.810
Iter 1996/2000 - Loss: -7.810
Iter 1997/2000 - Loss: -7.810
Iter 1998/2000 - Loss: -7.810
Iter 1999/2000 - Loss: -7.810
Iter 2000/2000 - Loss: -7.810
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[16.0534, 10.1036, 31.4633, 10.6365, 19.0092, 43.1360]],

        [[23.1178, 37.6798, 11.7212,  1.3956,  1.2723, 16.5598]],

        [[25.8002, 42.1478, 18.8451,  1.0187,  1.9310, 17.7904]],

        [[23.2735, 45.5747, 18.9782,  3.8278,  4.0163, 44.5700]]])
Signal Variance: tensor([ 0.1664,  0.9082, 16.1745,  0.5743])
Estimated target variance: tensor([0.0329, 0.1231, 1.7387, 0.0334])
N: 110
Signal to noise ratio: tensor([23.6178, 47.2900, 76.8236, 40.6385])
Bound on condition number: tensor([ 61359.1590, 245999.3219, 649206.8746, 181664.2869])
Policy Optimizer learning rate:
0.09895192582062147
Experience 11, Iter 0, disc loss: 0.0012246673860208082, policy loss: 6.982102040119296
Experience 11, Iter 1, disc loss: 0.001293759689221611, policy loss: 6.926330705156033
Experience 11, Iter 2, disc loss: 0.0012795562533251847, policy loss: 7.016541680187265
Experience 11, Iter 3, disc loss: 0.0012464808193445806, policy loss: 7.02682415376843
Experience 11, Iter 4, disc loss: 0.0012567991736621533, policy loss: 7.090369007419145
Experience 11, Iter 5, disc loss: 0.0012692449382536459, policy loss: 6.999974703336969
Experience 11, Iter 6, disc loss: 0.0014615549565464131, policy loss: 6.836725346766189
Experience 11, Iter 7, disc loss: 0.001244368540598854, policy loss: 7.005531724537429
Experience 11, Iter 8, disc loss: 0.001202040895007601, policy loss: 7.067865533906162
Experience 11, Iter 9, disc loss: 0.0013455052585665244, policy loss: 6.93638083074454
Experience 11, Iter 10, disc loss: 0.0012040881805390753, policy loss: 7.083591219487806
Experience 11, Iter 11, disc loss: 0.0013689422219456532, policy loss: 6.839248168073672
Experience 11, Iter 12, disc loss: 0.0012251400386154865, policy loss: 7.047124043796897
Experience 11, Iter 13, disc loss: 0.0012344793210354247, policy loss: 7.047062108630993
Experience 11, Iter 14, disc loss: 0.0011851283126279293, policy loss: 7.140743229270193
Experience 11, Iter 15, disc loss: 0.0012881142168872125, policy loss: 7.057141814952641
Experience 11, Iter 16, disc loss: 0.0012331441766339902, policy loss: 6.998431018409805
Experience 11, Iter 17, disc loss: 0.001090351802035015, policy loss: 7.175486292763103
Experience 11, Iter 18, disc loss: 0.0010894784403558259, policy loss: 7.193028160945916
Experience 11, Iter 19, disc loss: 0.0011543387217954502, policy loss: 7.109795469678073
Experience 11, Iter 20, disc loss: 0.0011214947042810116, policy loss: 7.125288548298645
Experience 11, Iter 21, disc loss: 0.0011435166141304236, policy loss: 7.085761751995928
Experience 11, Iter 22, disc loss: 0.001269032915096601, policy loss: 7.016286184973183
Experience 11, Iter 23, disc loss: 0.0011342787348449601, policy loss: 7.092673341661052
Experience 11, Iter 24, disc loss: 0.00112865998577056, policy loss: 7.075303915503902
Experience 11, Iter 25, disc loss: 0.001292932268305789, policy loss: 7.0167168279228616
Experience 11, Iter 26, disc loss: 0.0012155032263949203, policy loss: 7.015683754820448
Experience 11, Iter 27, disc loss: 0.001127451704680977, policy loss: 7.195662269433603
Experience 11, Iter 28, disc loss: 0.0010590716636615933, policy loss: 7.2034515488567585
Experience 11, Iter 29, disc loss: 0.0011477877078453745, policy loss: 7.113403647444297
Experience 11, Iter 30, disc loss: 0.001061264471313099, policy loss: 7.203437681009589
Experience 11, Iter 31, disc loss: 0.001192658714905322, policy loss: 7.0415505807768115
Experience 11, Iter 32, disc loss: 0.0011158140442344363, policy loss: 7.0796599900989
Experience 11, Iter 33, disc loss: 0.0011514547701686707, policy loss: 7.065473135339363
Experience 11, Iter 34, disc loss: 0.0012079051209635424, policy loss: 7.1106868002793515
Experience 11, Iter 35, disc loss: 0.0010435001095707387, policy loss: 7.291025161914374
Experience 11, Iter 36, disc loss: 0.0011702023995041606, policy loss: 7.063154503696213
Experience 11, Iter 37, disc loss: 0.0011883428383089828, policy loss: 7.069819665286165
Experience 11, Iter 38, disc loss: 0.0012026954470753717, policy loss: 7.033589718417073
Experience 11, Iter 39, disc loss: 0.0010193005006003983, policy loss: 7.266399070481867
Experience 11, Iter 40, disc loss: 0.0012662256847754537, policy loss: 7.0118686029136565
Experience 11, Iter 41, disc loss: 0.0012658867201172672, policy loss: 6.999735652625314
Experience 11, Iter 42, disc loss: 0.0013662082910957524, policy loss: 6.983110319101462
Experience 11, Iter 43, disc loss: 0.0010600391968624188, policy loss: 7.2680890002222025
Experience 11, Iter 44, disc loss: 0.0010511071561948573, policy loss: 7.24713430283577
Experience 11, Iter 45, disc loss: 0.0010511255166006427, policy loss: 7.282382541494681
Experience 11, Iter 46, disc loss: 0.0010735537506289328, policy loss: 7.2143554239620755
Experience 11, Iter 47, disc loss: 0.0011275559623756947, policy loss: 7.198571700477329
Experience 11, Iter 48, disc loss: 0.001124498433830307, policy loss: 7.134441105497953
Experience 11, Iter 49, disc loss: 0.001054304158715873, policy loss: 7.155383562768162
Experience 11, Iter 50, disc loss: 0.0012800106680620434, policy loss: 6.9483161137190095
Experience 11, Iter 51, disc loss: 0.0011006575112182393, policy loss: 7.177821633920946
Experience 11, Iter 52, disc loss: 0.0011774273855528247, policy loss: 7.01953156528301
Experience 11, Iter 53, disc loss: 0.0009317847541079697, policy loss: 7.437757181354336
Experience 11, Iter 54, disc loss: 0.0011058865871113293, policy loss: 7.21648818274455
Experience 11, Iter 55, disc loss: 0.000991998039413695, policy loss: 7.234466825356993
Experience 11, Iter 56, disc loss: 0.001074377467228111, policy loss: 7.205279213074961
Experience 11, Iter 57, disc loss: 0.0011442068405910456, policy loss: 7.190569088470756
Experience 11, Iter 58, disc loss: 0.0011925185810720386, policy loss: 6.970211621646573
Experience 11, Iter 59, disc loss: 0.0009095437501370317, policy loss: 7.424768139292192
Experience 11, Iter 60, disc loss: 0.0010956227938374848, policy loss: 7.192572720791034
Experience 11, Iter 61, disc loss: 0.0008624564322884638, policy loss: 7.479881015936325
Experience 11, Iter 62, disc loss: 0.0011512922548602594, policy loss: 7.03979388942147
Experience 11, Iter 63, disc loss: 0.0011392965602452787, policy loss: 7.135275968499389
Experience 11, Iter 64, disc loss: 0.0010522227845067315, policy loss: 7.2370665463549795
Experience 11, Iter 65, disc loss: 0.0010793198278714318, policy loss: 7.232821305433333
Experience 11, Iter 66, disc loss: 0.0010205297992277101, policy loss: 7.29221847653219
Experience 11, Iter 67, disc loss: 0.0009696473581580708, policy loss: 7.392870219613407
Experience 11, Iter 68, disc loss: 0.0010330841343367128, policy loss: 7.309798614596662
Experience 11, Iter 69, disc loss: 0.001023259103929073, policy loss: 7.190001492087053
Experience 11, Iter 70, disc loss: 0.0010645008397456599, policy loss: 7.141880569448748
Experience 11, Iter 71, disc loss: 0.0010388420178579858, policy loss: 7.261176271167935
Experience 11, Iter 72, disc loss: 0.0009925789781360527, policy loss: 7.304492259768845
Experience 11, Iter 73, disc loss: 0.0011152791277670433, policy loss: 7.111037021035042
Experience 11, Iter 74, disc loss: 0.0010817594336810271, policy loss: 7.146413861607757
Experience 11, Iter 75, disc loss: 0.0010556770967335579, policy loss: 7.242887776857351
Experience 11, Iter 76, disc loss: 0.001085599543777475, policy loss: 7.182162005387197
Experience 11, Iter 77, disc loss: 0.0010557747891743723, policy loss: 7.290083696343851
Experience 11, Iter 78, disc loss: 0.0011252271410203346, policy loss: 7.142485453434997
Experience 11, Iter 79, disc loss: 0.0011921576290722092, policy loss: 7.023920319776941
Experience 11, Iter 80, disc loss: 0.0010626752473053872, policy loss: 7.181250292558185
Experience 11, Iter 81, disc loss: 0.0011490906953462673, policy loss: 7.267844319447743
Experience 11, Iter 82, disc loss: 0.0009098901123193453, policy loss: 7.396602515296526
Experience 11, Iter 83, disc loss: 0.0012126522614861715, policy loss: 7.084663305757675
Experience 11, Iter 84, disc loss: 0.0010724249830691458, policy loss: 7.182646723384947
Experience 11, Iter 85, disc loss: 0.0009724374028561801, policy loss: 7.370366249349211
Experience 11, Iter 86, disc loss: 0.001090206653717013, policy loss: 7.103846889875193
Experience 11, Iter 87, disc loss: 0.0011556111318955277, policy loss: 7.1579788777993025
Experience 11, Iter 88, disc loss: 0.0009163512025741597, policy loss: 7.456024205720798
Experience 11, Iter 89, disc loss: 0.0009562307296844125, policy loss: 7.295353939520094
Experience 11, Iter 90, disc loss: 0.0010096076661876264, policy loss: 7.243899215860915
Experience 11, Iter 91, disc loss: 0.0009203424458740595, policy loss: 7.527659139261159
Experience 11, Iter 92, disc loss: 0.0009617558958050582, policy loss: 7.349613878100641
Experience 11, Iter 93, disc loss: 0.000991083542597134, policy loss: 7.3441040747864745
Experience 11, Iter 94, disc loss: 0.0009942701167462902, policy loss: 7.294865587682807
Experience 11, Iter 95, disc loss: 0.0010249397683796762, policy loss: 7.376627080565571
Experience 11, Iter 96, disc loss: 0.001034980345776865, policy loss: 7.211305914684701
Experience 11, Iter 97, disc loss: 0.0010201878277718939, policy loss: 7.255743174629631
Experience 11, Iter 98, disc loss: 0.0010985200079709624, policy loss: 7.1285545786957485
Experience 11, Iter 99, disc loss: 0.0011098469249670253, policy loss: 7.104876446356768
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0077],
        [0.0288],
        [0.3989],
        [0.0076]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0462, 0.2835, 0.3531, 0.0099, 0.0042, 1.0782]],

        [[0.0462, 0.2835, 0.3531, 0.0099, 0.0042, 1.0782]],

        [[0.0462, 0.2835, 0.3531, 0.0099, 0.0042, 1.0782]],

        [[0.0462, 0.2835, 0.3531, 0.0099, 0.0042, 1.0782]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0308, 0.1151, 1.5958, 0.0306], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0308, 0.1151, 1.5958, 0.0306])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.405
Iter 2/2000 - Loss: 1.248
Iter 3/2000 - Loss: 1.084
Iter 4/2000 - Loss: 1.018
Iter 5/2000 - Loss: 0.936
Iter 6/2000 - Loss: 0.798
Iter 7/2000 - Loss: 0.642
Iter 8/2000 - Loss: 0.480
Iter 9/2000 - Loss: 0.305
Iter 10/2000 - Loss: 0.113
Iter 11/2000 - Loss: -0.100
Iter 12/2000 - Loss: -0.331
Iter 13/2000 - Loss: -0.577
Iter 14/2000 - Loss: -0.835
Iter 15/2000 - Loss: -1.103
Iter 16/2000 - Loss: -1.378
Iter 17/2000 - Loss: -1.657
Iter 18/2000 - Loss: -1.935
Iter 19/2000 - Loss: -2.207
Iter 20/2000 - Loss: -2.471
Iter 1981/2000 - Loss: -7.912
Iter 1982/2000 - Loss: -7.912
Iter 1983/2000 - Loss: -7.912
Iter 1984/2000 - Loss: -7.912
Iter 1985/2000 - Loss: -7.912
Iter 1986/2000 - Loss: -7.912
Iter 1987/2000 - Loss: -7.912
Iter 1988/2000 - Loss: -7.912
Iter 1989/2000 - Loss: -7.912
Iter 1990/2000 - Loss: -7.912
Iter 1991/2000 - Loss: -7.912
Iter 1992/2000 - Loss: -7.912
Iter 1993/2000 - Loss: -7.912
Iter 1994/2000 - Loss: -7.912
Iter 1995/2000 - Loss: -7.912
Iter 1996/2000 - Loss: -7.912
Iter 1997/2000 - Loss: -7.912
Iter 1998/2000 - Loss: -7.912
Iter 1999/2000 - Loss: -7.912
Iter 2000/2000 - Loss: -7.912
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[15.1802,  9.9952, 30.7719, 10.5464, 19.0611, 43.1438]],

        [[21.9414, 36.6010, 11.6615,  1.4067,  1.2583, 16.3662]],

        [[24.4159, 41.3685, 18.5587,  1.0223,  1.9481, 17.7420]],

        [[21.8907, 44.5875, 18.7855,  3.6902,  3.6877, 44.3986]]])
Signal Variance: tensor([ 0.1637,  0.8874, 15.8089,  0.5507])
Estimated target variance: tensor([0.0308, 0.1151, 1.5958, 0.0306])
N: 120
Signal to noise ratio: tensor([23.4082, 46.5131, 73.1661, 40.9660])
Bound on condition number: tensor([ 65754.0978, 259617.1547, 642394.2613, 201386.3275])
Policy Optimizer learning rate:
0.09884772446450593
Experience 12, Iter 0, disc loss: 0.0010459121573097073, policy loss: 7.208801459077359
Experience 12, Iter 1, disc loss: 0.0009565321543640598, policy loss: 7.313146099681085
Experience 12, Iter 2, disc loss: 0.0009058077587674298, policy loss: 7.418072877178363
Experience 12, Iter 3, disc loss: 0.001060023652139905, policy loss: 7.174149759005378
Experience 12, Iter 4, disc loss: 0.0010681947789675334, policy loss: 7.182038924397158
Experience 12, Iter 5, disc loss: 0.0009159801749543168, policy loss: 7.34334117489983
Experience 12, Iter 6, disc loss: 0.0010310423686282738, policy loss: 7.112527109124171
Experience 12, Iter 7, disc loss: 0.0009250386991426207, policy loss: 7.349001051967586
Experience 12, Iter 8, disc loss: 0.0009858633159756328, policy loss: 7.243483370987242
Experience 12, Iter 9, disc loss: 0.000928751706778495, policy loss: 7.389585810522942
Experience 12, Iter 10, disc loss: 0.0010277611473315368, policy loss: 7.260959916640345
Experience 12, Iter 11, disc loss: 0.0009500882208358036, policy loss: 7.296817451597359
Experience 12, Iter 12, disc loss: 0.0009627311674240035, policy loss: 7.461784713343423
Experience 12, Iter 13, disc loss: 0.0010295680602726072, policy loss: 7.159640718694723
Experience 12, Iter 14, disc loss: 0.0009960226114865212, policy loss: 7.279300644899785
Experience 12, Iter 15, disc loss: 0.001085965475101426, policy loss: 7.115215355590248
Experience 12, Iter 16, disc loss: 0.0010049559687527877, policy loss: 7.310156434960621
Experience 12, Iter 17, disc loss: 0.0009990805341489974, policy loss: 7.23515808572323
Experience 12, Iter 18, disc loss: 0.001005923522486997, policy loss: 7.1849700694420005
Experience 12, Iter 19, disc loss: 0.0008324760711765164, policy loss: 7.528971787019103
Experience 12, Iter 20, disc loss: 0.0011324150343253634, policy loss: 7.141686493668257
Experience 12, Iter 21, disc loss: 0.0009743309856510207, policy loss: 7.226845900539802
Experience 12, Iter 22, disc loss: 0.0010442622123931337, policy loss: 7.165710065108047
Experience 12, Iter 23, disc loss: 0.0008893100343665002, policy loss: 7.535589280962425
Experience 12, Iter 24, disc loss: 0.0010337965294000594, policy loss: 7.247804085750891
Experience 12, Iter 25, disc loss: 0.0009848586250409833, policy loss: 7.3092269661798195
Experience 12, Iter 26, disc loss: 0.0007691201075624392, policy loss: 7.687704304742223
Experience 12, Iter 27, disc loss: 0.0010139318429595889, policy loss: 7.24749827248619
Experience 12, Iter 28, disc loss: 0.000991155064464654, policy loss: 7.245932867230562
Experience 12, Iter 29, disc loss: 0.0009134506628362616, policy loss: 7.418688869951925
Experience 12, Iter 30, disc loss: 0.0009994563395724014, policy loss: 7.346100724206984
Experience 12, Iter 31, disc loss: 0.0009964357769501825, policy loss: 7.209356050678312
Experience 12, Iter 32, disc loss: 0.0009401978266169805, policy loss: 7.348286834175419
Experience 12, Iter 33, disc loss: 0.0009160113772223743, policy loss: 7.334258167420375
Experience 12, Iter 34, disc loss: 0.0009162447827936685, policy loss: 7.389475115320975
Experience 12, Iter 35, disc loss: 0.0009689117169149051, policy loss: 7.320982968700868
Experience 12, Iter 36, disc loss: 0.0008551550531689502, policy loss: 7.501176673287148
Experience 12, Iter 37, disc loss: 0.0008991680951175401, policy loss: 7.42079585519339
Experience 12, Iter 38, disc loss: 0.0009414711029464871, policy loss: 7.309961842325841
Experience 12, Iter 39, disc loss: 0.0008350791726419161, policy loss: 7.449034539585222
Experience 12, Iter 40, disc loss: 0.0010269542518576964, policy loss: 7.164721136587315
Experience 12, Iter 41, disc loss: 0.0008981737700615602, policy loss: 7.406166084395876
Experience 12, Iter 42, disc loss: 0.0009311697836453389, policy loss: 7.323304786395904
Experience 12, Iter 43, disc loss: 0.0008300263547228397, policy loss: 7.462862024420069
Experience 12, Iter 44, disc loss: 0.0008119372806878058, policy loss: 7.529750007365966
Experience 12, Iter 45, disc loss: 0.0008761296655497688, policy loss: 7.427798348792283
Experience 12, Iter 46, disc loss: 0.0009672705082155691, policy loss: 7.245930486707037
Experience 12, Iter 47, disc loss: 0.0008253639691534644, policy loss: 7.447990644294199
Experience 12, Iter 48, disc loss: 0.0009582995602753434, policy loss: 7.375401039094507
Experience 12, Iter 49, disc loss: 0.0008835478040667531, policy loss: 7.5113338686244
Experience 12, Iter 50, disc loss: 0.0009671190177453315, policy loss: 7.311822545459071
Experience 12, Iter 51, disc loss: 0.0008417617511365981, policy loss: 7.4587762458393225
Experience 12, Iter 52, disc loss: 0.0008811526505074725, policy loss: 7.485714397192873
Experience 12, Iter 53, disc loss: 0.0009074540275895301, policy loss: 7.317386644088035
Experience 12, Iter 54, disc loss: 0.0008314175546238652, policy loss: 7.424146600062604
Experience 12, Iter 55, disc loss: 0.0010686216631205648, policy loss: 7.207731217721018
Experience 12, Iter 56, disc loss: 0.0009271457731559284, policy loss: 7.310146991219906
Experience 12, Iter 57, disc loss: 0.0009351950447905518, policy loss: 7.365957969799867
Experience 12, Iter 58, disc loss: 0.0008561318045306454, policy loss: 7.546321942625268
Experience 12, Iter 59, disc loss: 0.0008072325228235017, policy loss: 7.4750520901489885
Experience 12, Iter 60, disc loss: 0.0008484039149170032, policy loss: 7.449987088245035
Experience 12, Iter 61, disc loss: 0.00081696230590582, policy loss: 7.552577853951912
Experience 12, Iter 62, disc loss: 0.0008613979955979522, policy loss: 7.411736790707993
Experience 12, Iter 63, disc loss: 0.0009188604861786564, policy loss: 7.331961894190033
Experience 12, Iter 64, disc loss: 0.0009182292757056227, policy loss: 7.350303157397085
Experience 12, Iter 65, disc loss: 0.0008809692183476645, policy loss: 7.357184721551448
Experience 12, Iter 66, disc loss: 0.0008392450911497246, policy loss: 7.474689975497203
Experience 12, Iter 67, disc loss: 0.0010724056785951796, policy loss: 7.16883484581436
Experience 12, Iter 68, disc loss: 0.0009176447418783576, policy loss: 7.366961936084406
Experience 12, Iter 69, disc loss: 0.0009648704170932287, policy loss: 7.275600953488768
Experience 12, Iter 70, disc loss: 0.0008808476897525222, policy loss: 7.409986716559363
Experience 12, Iter 71, disc loss: 0.0008219299251923152, policy loss: 7.460374424648235
Experience 12, Iter 72, disc loss: 0.0010098765101683334, policy loss: 7.322557935865865
Experience 12, Iter 73, disc loss: 0.0009874553646785046, policy loss: 7.257881065043023
Experience 12, Iter 74, disc loss: 0.0008889221997169386, policy loss: 7.428419839716883
Experience 12, Iter 75, disc loss: 0.0008403255930482576, policy loss: 7.443833923337042
Experience 12, Iter 76, disc loss: 0.000840179105518269, policy loss: 7.457274822547632
Experience 12, Iter 77, disc loss: 0.0009618343026355565, policy loss: 7.311369555602286
Experience 12, Iter 78, disc loss: 0.0008728792433800737, policy loss: 7.434246634185687
Experience 12, Iter 79, disc loss: 0.0009012111775518612, policy loss: 7.393313262220419
Experience 12, Iter 80, disc loss: 0.0008141714961470771, policy loss: 7.46015106530895
Experience 12, Iter 81, disc loss: 0.0007799571695876965, policy loss: 7.5013754417052585
Experience 12, Iter 82, disc loss: 0.0008267150847946144, policy loss: 7.486203782817926
Experience 12, Iter 83, disc loss: 0.000904246371676845, policy loss: 7.437612366558465
Experience 12, Iter 84, disc loss: 0.0008326317561452498, policy loss: 7.473803938904977
Experience 12, Iter 85, disc loss: 0.0008296408477797172, policy loss: 7.524571239243691
Experience 12, Iter 86, disc loss: 0.0008180608089839046, policy loss: 7.550991230441462
Experience 12, Iter 87, disc loss: 0.0008542955292079796, policy loss: 7.375947391893413
Experience 12, Iter 88, disc loss: 0.0007733877567361789, policy loss: 7.551862163239306
Experience 12, Iter 89, disc loss: 0.0008382387845900434, policy loss: 7.341130809348538
Experience 12, Iter 90, disc loss: 0.0008669471673659416, policy loss: 7.391541701110539
Experience 12, Iter 91, disc loss: 0.0008637082467637511, policy loss: 7.3993515303412245
Experience 12, Iter 92, disc loss: 0.0008727394182151723, policy loss: 7.339255542050292
Experience 12, Iter 93, disc loss: 0.0008231935183335358, policy loss: 7.525684815965787
Experience 12, Iter 94, disc loss: 0.0009022781547528053, policy loss: 7.396368054950511
Experience 12, Iter 95, disc loss: 0.0008239598051316317, policy loss: 7.467575866245724
Experience 12, Iter 96, disc loss: 0.0007990578111031044, policy loss: 7.49968901260948
Experience 12, Iter 97, disc loss: 0.0010387950092857836, policy loss: 7.301697038106352
Experience 12, Iter 98, disc loss: 0.0008155607065617364, policy loss: 7.515246315852243
Experience 12, Iter 99, disc loss: 0.0008750446584508958, policy loss: 7.394697683789528
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0072],
        [0.0270],
        [0.3702],
        [0.0071]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0432, 0.2657, 0.3277, 0.0093, 0.0039, 1.0136]],

        [[0.0432, 0.2657, 0.3277, 0.0093, 0.0039, 1.0136]],

        [[0.0432, 0.2657, 0.3277, 0.0093, 0.0039, 1.0136]],

        [[0.0432, 0.2657, 0.3277, 0.0093, 0.0039, 1.0136]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0288, 0.1078, 1.4808, 0.0284], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0288, 0.1078, 1.4808, 0.0284])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.266
Iter 2/2000 - Loss: 1.132
Iter 3/2000 - Loss: 0.969
Iter 4/2000 - Loss: 0.899
Iter 5/2000 - Loss: 0.820
Iter 6/2000 - Loss: 0.686
Iter 7/2000 - Loss: 0.527
Iter 8/2000 - Loss: 0.354
Iter 9/2000 - Loss: 0.170
Iter 10/2000 - Loss: -0.028
Iter 11/2000 - Loss: -0.245
Iter 12/2000 - Loss: -0.482
Iter 13/2000 - Loss: -0.738
Iter 14/2000 - Loss: -1.008
Iter 15/2000 - Loss: -1.289
Iter 16/2000 - Loss: -1.575
Iter 17/2000 - Loss: -1.862
Iter 18/2000 - Loss: -2.146
Iter 19/2000 - Loss: -2.423
Iter 20/2000 - Loss: -2.692
Iter 1981/2000 - Loss: -8.025
Iter 1982/2000 - Loss: -8.025
Iter 1983/2000 - Loss: -8.025
Iter 1984/2000 - Loss: -8.025
Iter 1985/2000 - Loss: -8.025
Iter 1986/2000 - Loss: -8.025
Iter 1987/2000 - Loss: -8.025
Iter 1988/2000 - Loss: -8.025
Iter 1989/2000 - Loss: -8.025
Iter 1990/2000 - Loss: -8.025
Iter 1991/2000 - Loss: -8.025
Iter 1992/2000 - Loss: -8.025
Iter 1993/2000 - Loss: -8.025
Iter 1994/2000 - Loss: -8.025
Iter 1995/2000 - Loss: -8.025
Iter 1996/2000 - Loss: -8.025
Iter 1997/2000 - Loss: -8.025
Iter 1998/2000 - Loss: -8.025
Iter 1999/2000 - Loss: -8.025
Iter 2000/2000 - Loss: -8.025
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0003]])
Lengthscale: tensor([[[14.3973,  9.8400, 29.6885, 10.5026, 19.0976, 44.2971]],

        [[21.0679, 35.3734, 11.6275,  1.4291,  1.2461, 16.4676]],

        [[23.4890, 40.4812, 18.6026,  1.0234,  1.9020, 17.6138]],

        [[21.2314, 43.9882, 18.7307,  3.7581,  3.9560, 44.5418]]])
Signal Variance: tensor([ 0.1601,  0.8888, 15.5532,  0.5523])
Estimated target variance: tensor([0.0288, 0.1078, 1.4808, 0.0284])
N: 130
Signal to noise ratio: tensor([23.0948, 47.6049, 73.3918, 40.6562])
Bound on condition number: tensor([ 69339.0234, 294610.7163, 700228.2319, 214881.1204])
Policy Optimizer learning rate:
0.0987436328376607
Experience 13, Iter 0, disc loss: 0.0008694842065673537, policy loss: 7.512561168089622
Experience 13, Iter 1, disc loss: 0.0007143333576317663, policy loss: 7.677609168480783
Experience 13, Iter 2, disc loss: 0.0007831619209782766, policy loss: 7.579531931628389
Experience 13, Iter 3, disc loss: 0.0008115746498807469, policy loss: 7.5371151649538
Experience 13, Iter 4, disc loss: 0.0008044532266767842, policy loss: 7.447214462196456
Experience 13, Iter 5, disc loss: 0.0008349276041082661, policy loss: 7.477662418676924
Experience 13, Iter 6, disc loss: 0.0008541299534173993, policy loss: 7.399062996371841
Experience 13, Iter 7, disc loss: 0.0007196805534783197, policy loss: 7.66649014512338
Experience 13, Iter 8, disc loss: 0.0007860671465887443, policy loss: 7.55540055176137
Experience 13, Iter 9, disc loss: 0.0008214758442593436, policy loss: 7.40122232383224
Experience 13, Iter 10, disc loss: 0.0008016218799278415, policy loss: 7.491252061066123
Experience 13, Iter 11, disc loss: 0.0007523883667167022, policy loss: 7.653856571450543
Experience 13, Iter 12, disc loss: 0.0008135578170107446, policy loss: 7.451523594119394
Experience 13, Iter 13, disc loss: 0.000824308504436029, policy loss: 7.480165291822896
Experience 13, Iter 14, disc loss: 0.000721717247316794, policy loss: 7.617774769731122
Experience 13, Iter 15, disc loss: 0.0007853545315360637, policy loss: 7.597454123500333
Experience 13, Iter 16, disc loss: 0.000783626715246473, policy loss: 7.599994263852182
Experience 13, Iter 17, disc loss: 0.0007939583480123618, policy loss: 7.536221364314712
Experience 13, Iter 18, disc loss: 0.0007188440754049203, policy loss: 7.645494909283993
Experience 13, Iter 19, disc loss: 0.0007361287543057059, policy loss: 7.553227227608495
Experience 13, Iter 20, disc loss: 0.000828767861562231, policy loss: 7.530573622435101
Experience 13, Iter 21, disc loss: 0.000775838826417309, policy loss: 7.530265302721747
Experience 13, Iter 22, disc loss: 0.0008226102433109193, policy loss: 7.446050979633745
Experience 13, Iter 23, disc loss: 0.0007254126505765411, policy loss: 7.663684174436325
Experience 13, Iter 24, disc loss: 0.0007667251842427064, policy loss: 7.451933216820345
Experience 13, Iter 25, disc loss: 0.0006599392145564491, policy loss: 7.707005718552628
Experience 13, Iter 26, disc loss: 0.000719462879365342, policy loss: 7.6500859518244475
Experience 13, Iter 27, disc loss: 0.0007366732676136532, policy loss: 7.555447028102932
Experience 13, Iter 28, disc loss: 0.0006778630948119044, policy loss: 7.741478170788838
Experience 13, Iter 29, disc loss: 0.0006421829193119805, policy loss: 7.854977647136725
Experience 13, Iter 30, disc loss: 0.0007862008421688358, policy loss: 7.51802302876103
Experience 13, Iter 31, disc loss: 0.0007217397471825683, policy loss: 7.591195535600014
Experience 13, Iter 32, disc loss: 0.0007550878218087221, policy loss: 7.518358095426885
Experience 13, Iter 33, disc loss: 0.0007277661796813572, policy loss: 7.576867930653314
Experience 13, Iter 34, disc loss: 0.0007447233561225391, policy loss: 7.478075163369468
Experience 13, Iter 35, disc loss: 0.0006754197033788821, policy loss: 7.664536358178275
Experience 13, Iter 36, disc loss: 0.0007916343700906975, policy loss: 7.527322593475349
Experience 13, Iter 37, disc loss: 0.0007656121941708694, policy loss: 7.575899056740434
Experience 13, Iter 38, disc loss: 0.0007633552826274261, policy loss: 7.486723578890489
Experience 13, Iter 39, disc loss: 0.0007520133171876923, policy loss: 7.545833461343396
Experience 13, Iter 40, disc loss: 0.0008187683688274781, policy loss: 7.476704912290505
Experience 13, Iter 41, disc loss: 0.0006770425990180961, policy loss: 7.732259651465995
Experience 13, Iter 42, disc loss: 0.0007329759385429813, policy loss: 7.532415764960572
Experience 13, Iter 43, disc loss: 0.0008743325293705686, policy loss: 7.395789641697676
Experience 13, Iter 44, disc loss: 0.0006904121101793495, policy loss: 7.674790463884275
Experience 13, Iter 45, disc loss: 0.0007758781002782354, policy loss: 7.540981175789412
Experience 13, Iter 46, disc loss: 0.0008065434128510573, policy loss: 7.555447341245296
Experience 13, Iter 47, disc loss: 0.0008448927773129851, policy loss: 7.475053016653543
Experience 13, Iter 48, disc loss: 0.0007755221911639898, policy loss: 7.620365953982175
Experience 13, Iter 49, disc loss: 0.0008078882367865338, policy loss: 7.489462137935849
Experience 13, Iter 50, disc loss: 0.0006659939078826442, policy loss: 7.7684685818203345
Experience 13, Iter 51, disc loss: 0.0007139551448416995, policy loss: 7.5638326685465795
Experience 13, Iter 52, disc loss: 0.0007188314481925047, policy loss: 7.6131393039502
Experience 13, Iter 53, disc loss: 0.0006554967702604657, policy loss: 7.705745875680593
Experience 13, Iter 54, disc loss: 0.0006908487607257968, policy loss: 7.697013550815143
Experience 13, Iter 55, disc loss: 0.0007031651097938195, policy loss: 7.634468065349456
Experience 13, Iter 56, disc loss: 0.0007034579980107444, policy loss: 7.754998820681453
Experience 13, Iter 57, disc loss: 0.0007814379567281231, policy loss: 7.542780904271321
Experience 13, Iter 58, disc loss: 0.0007740700963409608, policy loss: 7.471445812086699
Experience 13, Iter 59, disc loss: 0.0006881464786542926, policy loss: 7.7203357058685675
Experience 13, Iter 60, disc loss: 0.0007522269948366966, policy loss: 7.6310159774985316
Experience 13, Iter 61, disc loss: 0.0007770327766423546, policy loss: 7.458442711189394
Experience 13, Iter 62, disc loss: 0.0007008964992665141, policy loss: 7.681909972064942
Experience 13, Iter 63, disc loss: 0.0007017668779954658, policy loss: 7.684985426698214
Experience 13, Iter 64, disc loss: 0.000827434108150908, policy loss: 7.437510746145641
Experience 13, Iter 65, disc loss: 0.0007096081756498791, policy loss: 7.603535951215019
Experience 13, Iter 66, disc loss: 0.0006375743610576649, policy loss: 7.752320593986656
Experience 13, Iter 67, disc loss: 0.0007910710127104579, policy loss: 7.46553108715918
Experience 13, Iter 68, disc loss: 0.0007599148602342089, policy loss: 7.635607951101704
Experience 13, Iter 69, disc loss: 0.000685806712377258, policy loss: 7.789233671816607
Experience 13, Iter 70, disc loss: 0.0007521035578836923, policy loss: 7.473223913685311
Experience 13, Iter 71, disc loss: 0.0006898797044026328, policy loss: 7.674768748626837
Experience 13, Iter 72, disc loss: 0.0007018762699933863, policy loss: 7.6017128680371355
Experience 13, Iter 73, disc loss: 0.0006064900043040229, policy loss: 7.776442584866572
Experience 13, Iter 74, disc loss: 0.0006997725579412608, policy loss: 7.678433973602388
Experience 13, Iter 75, disc loss: 0.0007165882124719349, policy loss: 7.599216474282375
Experience 13, Iter 76, disc loss: 0.0007682356840701184, policy loss: 7.625275021126758
Experience 13, Iter 77, disc loss: 0.0006953262089450379, policy loss: 7.656557733350405
Experience 13, Iter 78, disc loss: 0.0006566367137430162, policy loss: 7.645275059113164
Experience 13, Iter 79, disc loss: 0.0006773724199243519, policy loss: 7.632694922428453
Experience 13, Iter 80, disc loss: 0.0006814203553003726, policy loss: 7.669862232349754
Experience 13, Iter 81, disc loss: 0.0007299039200993065, policy loss: 7.524293681878322
Experience 13, Iter 82, disc loss: 0.0007782896118835379, policy loss: 7.449016936318927
Experience 13, Iter 83, disc loss: 0.0006394727898442838, policy loss: 7.847134436933472
Experience 13, Iter 84, disc loss: 0.0006734244450924614, policy loss: 7.65755259262402
Experience 13, Iter 85, disc loss: 0.0007446567359996798, policy loss: 7.622480394591304
Experience 13, Iter 86, disc loss: 0.0007648002117412034, policy loss: 7.640245877186878
Experience 13, Iter 87, disc loss: 0.0006906586199814008, policy loss: 7.747662560340898
Experience 13, Iter 88, disc loss: 0.0006887166687009954, policy loss: 7.710553226155828
Experience 13, Iter 89, disc loss: 0.00082376940632873, policy loss: 7.505039437332535
Experience 13, Iter 90, disc loss: 0.0006763863602257632, policy loss: 7.773145571154882
Experience 13, Iter 91, disc loss: 0.0007057872862506938, policy loss: 7.589033179975321
Experience 13, Iter 92, disc loss: 0.0006418942630091101, policy loss: 7.740087340140873
Experience 13, Iter 93, disc loss: 0.000634298688496758, policy loss: 7.822279667985202
Experience 13, Iter 94, disc loss: 0.0006293800692237729, policy loss: 7.768835164043099
Experience 13, Iter 95, disc loss: 0.0005899116440336895, policy loss: 7.993063066767703
Experience 13, Iter 96, disc loss: 0.0006249663249614751, policy loss: 7.815953635616672
Experience 13, Iter 97, disc loss: 0.0007163815478430738, policy loss: 7.666306773938034
Experience 13, Iter 98, disc loss: 0.0006851620257776321, policy loss: 7.690429144642804
Experience 13, Iter 99, disc loss: 0.0006536108481570973, policy loss: 7.685264105377311
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.0254],
        [0.3456],
        [0.0066]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0402, 0.2490, 0.3059, 0.0087, 0.0037, 0.9564]],

        [[0.0402, 0.2490, 0.3059, 0.0087, 0.0037, 0.9564]],

        [[0.0402, 0.2490, 0.3059, 0.0087, 0.0037, 0.9564]],

        [[0.0402, 0.2490, 0.3059, 0.0087, 0.0037, 0.9564]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0270, 0.1017, 1.3826, 0.0265], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0270, 0.1017, 1.3826, 0.0265])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.141
Iter 2/2000 - Loss: 1.034
Iter 3/2000 - Loss: 0.875
Iter 4/2000 - Loss: 0.806
Iter 5/2000 - Loss: 0.735
Iter 6/2000 - Loss: 0.607
Iter 7/2000 - Loss: 0.448
Iter 8/2000 - Loss: 0.272
Iter 9/2000 - Loss: 0.085
Iter 10/2000 - Loss: -0.113
Iter 11/2000 - Loss: -0.332
Iter 12/2000 - Loss: -0.576
Iter 13/2000 - Loss: -0.842
Iter 14/2000 - Loss: -1.125
Iter 15/2000 - Loss: -1.419
Iter 16/2000 - Loss: -1.718
Iter 17/2000 - Loss: -2.015
Iter 18/2000 - Loss: -2.308
Iter 19/2000 - Loss: -2.593
Iter 20/2000 - Loss: -2.871
Iter 1981/2000 - Loss: -8.115
Iter 1982/2000 - Loss: -8.115
Iter 1983/2000 - Loss: -8.115
Iter 1984/2000 - Loss: -8.115
Iter 1985/2000 - Loss: -8.116
Iter 1986/2000 - Loss: -8.116
Iter 1987/2000 - Loss: -8.116
Iter 1988/2000 - Loss: -8.116
Iter 1989/2000 - Loss: -8.116
Iter 1990/2000 - Loss: -8.116
Iter 1991/2000 - Loss: -8.116
Iter 1992/2000 - Loss: -8.116
Iter 1993/2000 - Loss: -8.116
Iter 1994/2000 - Loss: -8.116
Iter 1995/2000 - Loss: -8.116
Iter 1996/2000 - Loss: -8.116
Iter 1997/2000 - Loss: -8.116
Iter 1998/2000 - Loss: -8.116
Iter 1999/2000 - Loss: -8.116
Iter 2000/2000 - Loss: -8.116
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[14.1732,  9.8048, 28.8003, 10.3068, 19.1851, 43.2156]],

        [[20.3349, 34.6678, 11.6392,  1.4322,  1.2318, 16.3939]],

        [[22.7761, 40.0898, 18.4524,  1.0164,  1.8649, 17.5702]],

        [[20.3008, 42.9733, 18.5465,  3.6194,  3.7756, 44.0671]]])
Signal Variance: tensor([ 0.1567,  0.8812, 15.1500,  0.5330])
Estimated target variance: tensor([0.0270, 0.1017, 1.3826, 0.0265])
N: 140
Signal to noise ratio: tensor([22.3637, 47.9664, 73.4103, 40.3997])
Bound on condition number: tensor([ 70019.9102, 322109.0191, 754471.6228, 228500.0584])
Policy Optimizer learning rate:
0.09863965082453535
Experience 14, Iter 0, disc loss: 0.0006712460391430284, policy loss: 7.703284385555387
Experience 14, Iter 1, disc loss: 0.0007713807003204422, policy loss: 7.617031342598262
Experience 14, Iter 2, disc loss: 0.0007526902631768029, policy loss: 7.628026827015697
Experience 14, Iter 3, disc loss: 0.00066657347464805, policy loss: 7.785994162943069
Experience 14, Iter 4, disc loss: 0.0007037782047476603, policy loss: 7.672007390172269
Experience 14, Iter 5, disc loss: 0.0007069346800321839, policy loss: 7.690800201119854
Experience 14, Iter 6, disc loss: 0.0006300058156903876, policy loss: 7.732544240999479
Experience 14, Iter 7, disc loss: 0.0006817993622182725, policy loss: 7.740485134021355
Experience 14, Iter 8, disc loss: 0.0005775433852604367, policy loss: 7.79097983603234
Experience 14, Iter 9, disc loss: 0.0006640268822915591, policy loss: 7.724064119435426
Experience 14, Iter 10, disc loss: 0.0006442635003874943, policy loss: 7.701075912265667
Experience 14, Iter 11, disc loss: 0.0006593615293587612, policy loss: 7.726816262448727
Experience 14, Iter 12, disc loss: 0.0006487382544750567, policy loss: 7.679430666828913
Experience 14, Iter 13, disc loss: 0.0006468410550108211, policy loss: 7.703543000465867
Experience 14, Iter 14, disc loss: 0.0006696474434164794, policy loss: 7.618667761681037
Experience 14, Iter 15, disc loss: 0.0006390988971920226, policy loss: 7.684335079707341
Experience 14, Iter 16, disc loss: 0.0006295128572479352, policy loss: 7.727215533454347
Experience 14, Iter 17, disc loss: 0.0006130795855375493, policy loss: 7.761361705404417
Experience 14, Iter 18, disc loss: 0.0006456483497135237, policy loss: 7.7118365461228
Experience 14, Iter 19, disc loss: 0.0006388241861759928, policy loss: 7.845188509590521
Experience 14, Iter 20, disc loss: 0.000688827680960888, policy loss: 7.548486115007105
Experience 14, Iter 21, disc loss: 0.0006298201061002344, policy loss: 7.736442458298412
Experience 14, Iter 22, disc loss: 0.0006372509474034671, policy loss: 7.7508524375197325
Experience 14, Iter 23, disc loss: 0.0006764912917338621, policy loss: 7.5921066131688
Experience 14, Iter 24, disc loss: 0.0005646873074744652, policy loss: 7.853623435076883
Experience 14, Iter 25, disc loss: 0.0005480856804715087, policy loss: 7.918323354904043
Experience 14, Iter 26, disc loss: 0.0007376256820318931, policy loss: 7.602845049444918
Experience 14, Iter 27, disc loss: 0.0006529580816133419, policy loss: 7.759902729971341
Experience 14, Iter 28, disc loss: 0.0006341758064829317, policy loss: 7.728941512532635
Experience 14, Iter 29, disc loss: 0.0005862734383590607, policy loss: 7.938962204383937
Experience 14, Iter 30, disc loss: 0.0005271628406755147, policy loss: 8.032617982589201
Experience 14, Iter 31, disc loss: 0.0006633660100345416, policy loss: 7.778805324937991
Experience 14, Iter 32, disc loss: 0.0006816057840753398, policy loss: 7.725503035954032
Experience 14, Iter 33, disc loss: 0.0006280403592845153, policy loss: 7.711125271598528
Experience 14, Iter 34, disc loss: 0.0005884028951712568, policy loss: 7.730738565568857
Experience 14, Iter 35, disc loss: 0.00063752079711545, policy loss: 7.867059580108405
Experience 14, Iter 36, disc loss: 0.0006426276785446337, policy loss: 7.661387335594222
Experience 14, Iter 37, disc loss: 0.0005555910255793636, policy loss: 7.943999824857528
Experience 14, Iter 38, disc loss: 0.0005808216306422455, policy loss: 7.812259701715031
Experience 14, Iter 39, disc loss: 0.0006113697409060009, policy loss: 7.788983803565124
Experience 14, Iter 40, disc loss: 0.0005508860335746623, policy loss: 7.9332536518875845
Experience 14, Iter 41, disc loss: 0.0006227219109067627, policy loss: 7.723533608012659
Experience 14, Iter 42, disc loss: 0.0006540836850322128, policy loss: 7.712635251347525
Experience 14, Iter 43, disc loss: 0.0006584816396059931, policy loss: 7.67323976023142
Experience 14, Iter 44, disc loss: 0.0006394310573839649, policy loss: 7.803602466877246
Experience 14, Iter 45, disc loss: 0.0006057839512750062, policy loss: 7.8792683699779404
Experience 14, Iter 46, disc loss: 0.0005767862742019399, policy loss: 7.822409475558693
Experience 14, Iter 47, disc loss: 0.0006275205102743219, policy loss: 7.676472672273476
Experience 14, Iter 48, disc loss: 0.0005723011997112633, policy loss: 7.911373763202796
Experience 14, Iter 49, disc loss: 0.0007398776632781537, policy loss: 7.68627452793715
Experience 14, Iter 50, disc loss: 0.000595806139047741, policy loss: 7.819558326187826
Experience 14, Iter 51, disc loss: 0.0006439263630978665, policy loss: 7.652359432556836
Experience 14, Iter 52, disc loss: 0.0005641026976086325, policy loss: 7.952477327162426
Experience 14, Iter 53, disc loss: 0.0006932703149721768, policy loss: 7.63437345452539
Experience 14, Iter 54, disc loss: 0.0005784837932985709, policy loss: 7.831872201736546
Experience 14, Iter 55, disc loss: 0.0006866094063289925, policy loss: 7.655766251268743
Experience 14, Iter 56, disc loss: 0.0006846475973076536, policy loss: 7.70293666467668
Experience 14, Iter 57, disc loss: 0.0006203148721883169, policy loss: 7.765559405535042
Experience 14, Iter 58, disc loss: 0.0005192276116876022, policy loss: 7.987886291939569
Experience 14, Iter 59, disc loss: 0.0005963418497162946, policy loss: 7.805130002411543
Experience 14, Iter 60, disc loss: 0.0006706945471886081, policy loss: 7.7252212619714795
Experience 14, Iter 61, disc loss: 0.000600017989214147, policy loss: 7.770235278028034
Experience 14, Iter 62, disc loss: 0.0006255122741567885, policy loss: 7.702195940066368
Experience 14, Iter 63, disc loss: 0.0005945242475784525, policy loss: 7.711478511600957
Experience 14, Iter 64, disc loss: 0.0005560742835099783, policy loss: 7.896400748968473
Experience 14, Iter 65, disc loss: 0.0005862423594492794, policy loss: 7.790704204578789
Experience 14, Iter 66, disc loss: 0.000593022515306967, policy loss: 7.752644498257048
Experience 14, Iter 67, disc loss: 0.0005863186358304937, policy loss: 7.802655001642493
Experience 14, Iter 68, disc loss: 0.0006006883751740919, policy loss: 7.754952973341758
Experience 14, Iter 69, disc loss: 0.0006210391252657029, policy loss: 7.777824129774242
Experience 14, Iter 70, disc loss: 0.0005926611337327899, policy loss: 7.823491847405068
Experience 14, Iter 71, disc loss: 0.0005941155314321347, policy loss: 7.845600475099406
Experience 14, Iter 72, disc loss: 0.000688320868897719, policy loss: 7.674347342750592
Experience 14, Iter 73, disc loss: 0.0005144336448427507, policy loss: 8.02427644312652
Experience 14, Iter 74, disc loss: 0.0005859286045942106, policy loss: 7.904144893608866
Experience 14, Iter 75, disc loss: 0.0006188804610122269, policy loss: 7.811475732281728
Experience 14, Iter 76, disc loss: 0.0005761552331497448, policy loss: 7.853599320389549
Experience 14, Iter 77, disc loss: 0.0005332838123075362, policy loss: 7.96442651414568
Experience 14, Iter 78, disc loss: 0.0005418667653464269, policy loss: 7.908262389830383
Experience 14, Iter 79, disc loss: 0.000527184937375866, policy loss: 7.957667407709823
Experience 14, Iter 80, disc loss: 0.000636165440872356, policy loss: 7.735568500510724
Experience 14, Iter 81, disc loss: 0.0005631729724175533, policy loss: 7.894094957349954
Experience 14, Iter 82, disc loss: 0.0005985095900790586, policy loss: 7.798340453605537
Experience 14, Iter 83, disc loss: 0.0006373701308978006, policy loss: 7.744296491832159
Experience 14, Iter 84, disc loss: 0.000588290845989661, policy loss: 7.789150624145032
Experience 14, Iter 85, disc loss: 0.0005898796038001497, policy loss: 7.852837643037221
Experience 14, Iter 86, disc loss: 0.0005951128109702834, policy loss: 7.812562099367468
Experience 14, Iter 87, disc loss: 0.0005847107876832337, policy loss: 7.853366268075606
Experience 14, Iter 88, disc loss: 0.0005977715185637662, policy loss: 7.800781213907719
Experience 14, Iter 89, disc loss: 0.0005205212705477118, policy loss: 7.9266708119006015
Experience 14, Iter 90, disc loss: 0.0006186915354808399, policy loss: 7.7630644119064485
Experience 14, Iter 91, disc loss: 0.0006474597125630509, policy loss: 7.811233697555524
Experience 14, Iter 92, disc loss: 0.000545333706216886, policy loss: 7.8873831928664035
Experience 14, Iter 93, disc loss: 0.000556794275788077, policy loss: 7.981712719859218
Experience 14, Iter 94, disc loss: 0.0005595935539811131, policy loss: 7.834039056595754
Experience 14, Iter 95, disc loss: 0.0005195825323035334, policy loss: 7.967247606297903
Experience 14, Iter 96, disc loss: 0.0005558889913579543, policy loss: 7.948803518659385
Experience 14, Iter 97, disc loss: 0.0005516262387245255, policy loss: 7.870223512568298
Experience 14, Iter 98, disc loss: 0.0006153295339644572, policy loss: 7.688469050689169
Experience 14, Iter 99, disc loss: 0.0005035736544507165, policy loss: 8.071340994737106
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0064],
        [0.0240],
        [0.3226],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0379, 0.2360, 0.2855, 0.0082, 0.0034, 0.9048]],

        [[0.0379, 0.2360, 0.2855, 0.0082, 0.0034, 0.9048]],

        [[0.0379, 0.2360, 0.2855, 0.0082, 0.0034, 0.9048]],

        [[0.0379, 0.2360, 0.2855, 0.0082, 0.0034, 0.9048]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0256, 0.0960, 1.2903, 0.0248], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0256, 0.0960, 1.2903, 0.0248])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.981
Iter 2/2000 - Loss: 0.887
Iter 3/2000 - Loss: 0.720
Iter 4/2000 - Loss: 0.645
Iter 5/2000 - Loss: 0.572
Iter 6/2000 - Loss: 0.443
Iter 7/2000 - Loss: 0.276
Iter 8/2000 - Loss: 0.091
Iter 9/2000 - Loss: -0.101
Iter 10/2000 - Loss: -0.302
Iter 11/2000 - Loss: -0.522
Iter 12/2000 - Loss: -0.768
Iter 13/2000 - Loss: -1.037
Iter 14/2000 - Loss: -1.325
Iter 15/2000 - Loss: -1.623
Iter 16/2000 - Loss: -1.926
Iter 17/2000 - Loss: -2.226
Iter 18/2000 - Loss: -2.521
Iter 19/2000 - Loss: -2.810
Iter 20/2000 - Loss: -3.090
Iter 1981/2000 - Loss: -8.197
Iter 1982/2000 - Loss: -8.197
Iter 1983/2000 - Loss: -8.197
Iter 1984/2000 - Loss: -8.197
Iter 1985/2000 - Loss: -8.197
Iter 1986/2000 - Loss: -8.197
Iter 1987/2000 - Loss: -8.197
Iter 1988/2000 - Loss: -8.197
Iter 1989/2000 - Loss: -8.197
Iter 1990/2000 - Loss: -8.197
Iter 1991/2000 - Loss: -8.198
Iter 1992/2000 - Loss: -8.198
Iter 1993/2000 - Loss: -8.198
Iter 1994/2000 - Loss: -8.198
Iter 1995/2000 - Loss: -8.198
Iter 1996/2000 - Loss: -8.198
Iter 1997/2000 - Loss: -8.198
Iter 1998/2000 - Loss: -8.198
Iter 1999/2000 - Loss: -8.198
Iter 2000/2000 - Loss: -8.198
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[13.2781,  6.9628, 27.6702, 10.0494, 18.8467, 38.6652]],

        [[20.1469, 35.3343, 11.6238,  1.4384,  1.2196, 16.2340]],

        [[22.5953, 41.9055, 17.9948,  0.9989,  1.7871, 17.5100]],

        [[19.7367, 41.4366, 18.3539,  3.5307,  3.5667, 43.9273]]])
Signal Variance: tensor([ 0.1247,  0.8571, 14.3032,  0.5154])
Estimated target variance: tensor([0.0256, 0.0960, 1.2903, 0.0248])
N: 150
Signal to noise ratio: tensor([20.2950, 46.2529, 71.7866, 40.3083])
Bound on condition number: tensor([ 61783.9000, 320900.2855, 772997.3282, 243714.7428])
Policy Optimizer learning rate:
0.0985357783097011
Experience 15, Iter 0, disc loss: 0.0005774534317063367, policy loss: 7.845171072110901
Experience 15, Iter 1, disc loss: 0.0005653134155970347, policy loss: 7.855852244851599
Experience 15, Iter 2, disc loss: 0.0005358609438458089, policy loss: 7.838260439320528
Experience 15, Iter 3, disc loss: 0.0005419883471643999, policy loss: 7.924523871383881
Experience 15, Iter 4, disc loss: 0.0005166357686418862, policy loss: 7.96885150108319
Experience 15, Iter 5, disc loss: 0.00057148774469957, policy loss: 7.8499227396847475
Experience 15, Iter 6, disc loss: 0.0005315843036819601, policy loss: 7.974639546641721
Experience 15, Iter 7, disc loss: 0.0005616539304722348, policy loss: 7.828337493946089
Experience 15, Iter 8, disc loss: 0.0005432827814531295, policy loss: 7.9710517235029466
Experience 15, Iter 9, disc loss: 0.0005244319067440093, policy loss: 8.048753371890228
Experience 15, Iter 10, disc loss: 0.0005678799067340888, policy loss: 7.862797588827896
Experience 15, Iter 11, disc loss: 0.0006140370516015577, policy loss: 7.871063793547666
Experience 15, Iter 12, disc loss: 0.0005421890363577792, policy loss: 7.943437246298496
Experience 15, Iter 13, disc loss: 0.0005549610031485493, policy loss: 7.888680037680692
Experience 15, Iter 14, disc loss: 0.0005697828046565272, policy loss: 7.868408715118127
Experience 15, Iter 15, disc loss: 0.00048583414636238677, policy loss: 7.992987599359212
Experience 15, Iter 16, disc loss: 0.0006465236475203575, policy loss: 7.679528767141421
Experience 15, Iter 17, disc loss: 0.0005682037785541076, policy loss: 7.8155780427766715
Experience 15, Iter 18, disc loss: 0.0005168371972204747, policy loss: 7.952570545948088
Experience 15, Iter 19, disc loss: 0.0005330150474111857, policy loss: 7.882308916073995
Experience 15, Iter 20, disc loss: 0.0005319950904097081, policy loss: 7.930332396845481
Experience 15, Iter 21, disc loss: 0.0005652458627225624, policy loss: 7.815323124688529
Experience 15, Iter 22, disc loss: 0.0005004430748406548, policy loss: 7.936685419063504
Experience 15, Iter 23, disc loss: 0.0005181384152023385, policy loss: 7.960271159860982
Experience 15, Iter 24, disc loss: 0.0005108683721546054, policy loss: 7.987076665521013
Experience 15, Iter 25, disc loss: 0.0005792042349251222, policy loss: 7.825547816661293
Experience 15, Iter 26, disc loss: 0.0004314685666936295, policy loss: 8.17747037695288
Experience 15, Iter 27, disc loss: 0.0004990747262147225, policy loss: 8.125797169240226
Experience 15, Iter 28, disc loss: 0.0005477808331032438, policy loss: 8.011988351590345
Experience 15, Iter 29, disc loss: 0.0004817591695961301, policy loss: 8.19505453187541
Experience 15, Iter 30, disc loss: 0.0005206763855626576, policy loss: 7.974189158643558
Experience 15, Iter 31, disc loss: 0.0004906428492939719, policy loss: 7.915885896342731
Experience 15, Iter 32, disc loss: 0.00048306826453259485, policy loss: 8.052090286141036
Experience 15, Iter 33, disc loss: 0.0005108349304915712, policy loss: 7.961233611737152
Experience 15, Iter 34, disc loss: 0.0004994611190911886, policy loss: 8.068548246508561
Experience 15, Iter 35, disc loss: 0.0004774010078932959, policy loss: 7.984957656057467
Experience 15, Iter 36, disc loss: 0.0005573555295381743, policy loss: 7.885057275863839
Experience 15, Iter 37, disc loss: 0.0005591307143831324, policy loss: 7.864914233240973
Experience 15, Iter 38, disc loss: 0.000515005717519152, policy loss: 7.888842287559054
Experience 15, Iter 39, disc loss: 0.0005258901039874825, policy loss: 7.887859870359259
Experience 15, Iter 40, disc loss: 0.000472774918885779, policy loss: 7.960263492868828
Experience 15, Iter 41, disc loss: 0.0005664725984136531, policy loss: 7.851368908634205
Experience 15, Iter 42, disc loss: 0.00048522511900349623, policy loss: 8.127331954038747
Experience 15, Iter 43, disc loss: 0.0005199816278939536, policy loss: 7.990017695772337
Experience 15, Iter 44, disc loss: 0.0005393814411539155, policy loss: 7.873117704238641
Experience 15, Iter 45, disc loss: 0.0004911768008464823, policy loss: 8.104071299483603
Experience 15, Iter 46, disc loss: 0.00048726809119058, policy loss: 8.043615109226197
Experience 15, Iter 47, disc loss: 0.0005120225365839419, policy loss: 8.022095983605782
Experience 15, Iter 48, disc loss: 0.0005161760060904573, policy loss: 8.096509302276473
Experience 15, Iter 49, disc loss: 0.00045270710225728126, policy loss: 8.197560035039855
Experience 15, Iter 50, disc loss: 0.0005211863696234015, policy loss: 7.887100639918182
Experience 15, Iter 51, disc loss: 0.0004806958443879876, policy loss: 8.114228661482363
Experience 15, Iter 52, disc loss: 0.00048283003511246476, policy loss: 7.997601905453216
Experience 15, Iter 53, disc loss: 0.0004993755116944156, policy loss: 8.012752521128682
Experience 15, Iter 54, disc loss: 0.0004923000392895588, policy loss: 7.987686946378684
Experience 15, Iter 55, disc loss: 0.0005057736466453348, policy loss: 7.917322741748071
Experience 15, Iter 56, disc loss: 0.000438078755940037, policy loss: 8.125028076326176
Experience 15, Iter 57, disc loss: 0.0004882826636780923, policy loss: 7.969047644817021
Experience 15, Iter 58, disc loss: 0.00048194241919421305, policy loss: 8.033149634845277
Experience 15, Iter 59, disc loss: 0.0005084569009366056, policy loss: 7.878817740358764
Experience 15, Iter 60, disc loss: 0.0005452939404968734, policy loss: 7.849701994786224
Experience 15, Iter 61, disc loss: 0.0005604329065800922, policy loss: 7.826514789191263
Experience 15, Iter 62, disc loss: 0.0006234514556522027, policy loss: 7.850416518802509
Experience 15, Iter 63, disc loss: 0.0005082503461520893, policy loss: 7.986840718138099
Experience 15, Iter 64, disc loss: 0.0004776562798002114, policy loss: 8.06164218969974
Experience 15, Iter 65, disc loss: 0.0004812659774190365, policy loss: 8.075928170933645
Experience 15, Iter 66, disc loss: 0.0005462141843629673, policy loss: 7.993383569816434
Experience 15, Iter 67, disc loss: 0.0005233500363963006, policy loss: 7.969894005460338
Experience 15, Iter 68, disc loss: 0.0005560326435316461, policy loss: 7.839369144592146
Experience 15, Iter 69, disc loss: 0.0004806149943003419, policy loss: 8.02691068638343
Experience 15, Iter 70, disc loss: 0.0004929417110781434, policy loss: 8.085227233868606
Experience 15, Iter 71, disc loss: 0.0005604519037622737, policy loss: 7.850764665063124
Experience 15, Iter 72, disc loss: 0.0005730946414003872, policy loss: 7.796571360067714
Experience 15, Iter 73, disc loss: 0.0005842664995791265, policy loss: 7.8544010245358304
Experience 15, Iter 74, disc loss: 0.0005252398164100927, policy loss: 8.034290495263537
Experience 15, Iter 75, disc loss: 0.00046741195368407164, policy loss: 8.150425337138635
Experience 15, Iter 76, disc loss: 0.0004644855504426109, policy loss: 8.171030422026515
Experience 15, Iter 77, disc loss: 0.0005560821309186271, policy loss: 7.8924307521690995
Experience 15, Iter 78, disc loss: 0.0006349701973102138, policy loss: 7.7509342946825415
Experience 15, Iter 79, disc loss: 0.00044461052205495755, policy loss: 8.190524172248523
Experience 15, Iter 80, disc loss: 0.000496592569502382, policy loss: 7.9925077990314195
Experience 15, Iter 81, disc loss: 0.0004920401562760426, policy loss: 8.022252145031969
Experience 15, Iter 82, disc loss: 0.00046519311368035693, policy loss: 8.090598182348584
Experience 15, Iter 83, disc loss: 0.0005111280414637928, policy loss: 8.018209974133484
Experience 15, Iter 84, disc loss: 0.00044229015106866156, policy loss: 8.194588895192759
Experience 15, Iter 85, disc loss: 0.0004893188088881226, policy loss: 8.082137751672896
Experience 15, Iter 86, disc loss: 0.0005034406005453463, policy loss: 8.052475017876928
Experience 15, Iter 87, disc loss: 0.0004948235855105057, policy loss: 8.058297154735703
Experience 15, Iter 88, disc loss: 0.0004995211572065359, policy loss: 7.940115871487812
Experience 15, Iter 89, disc loss: 0.00047322767984340634, policy loss: 8.088445599068942
Experience 15, Iter 90, disc loss: 0.00039928938535789804, policy loss: 8.431942597310876
Experience 15, Iter 91, disc loss: 0.0005368631969981588, policy loss: 7.907393289660761
Experience 15, Iter 92, disc loss: 0.00049814481857727, policy loss: 8.006644385161794
Experience 15, Iter 93, disc loss: 0.0005135310710185998, policy loss: 7.8830520636730155
Experience 15, Iter 94, disc loss: 0.0005217061250063194, policy loss: 7.964103589851115
Experience 15, Iter 95, disc loss: 0.00047665781497168954, policy loss: 8.02057784539819
Experience 15, Iter 96, disc loss: 0.000456477663214627, policy loss: 8.025744255205874
Experience 15, Iter 97, disc loss: 0.0004509628196883002, policy loss: 8.079936435476647
Experience 15, Iter 98, disc loss: 0.0004461128697796207, policy loss: 8.106363177663958
Experience 15, Iter 99, disc loss: 0.0004914159999851944, policy loss: 8.047409195042096
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0060],
        [0.0227],
        [0.3035],
        [0.0058]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0355, 0.2225, 0.2686, 0.0078, 0.0032, 0.8577]],

        [[0.0355, 0.2225, 0.2686, 0.0078, 0.0032, 0.8577]],

        [[0.0355, 0.2225, 0.2686, 0.0078, 0.0032, 0.8577]],

        [[0.0355, 0.2225, 0.2686, 0.0078, 0.0032, 0.8577]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0242, 0.0909, 1.2140, 0.0233], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0242, 0.0909, 1.2140, 0.0233])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.870
Iter 2/2000 - Loss: 0.799
Iter 3/2000 - Loss: 0.630
Iter 4/2000 - Loss: 0.556
Iter 5/2000 - Loss: 0.491
Iter 6/2000 - Loss: 0.369
Iter 7/2000 - Loss: 0.201
Iter 8/2000 - Loss: 0.015
Iter 9/2000 - Loss: -0.175
Iter 10/2000 - Loss: -0.374
Iter 11/2000 - Loss: -0.594
Iter 12/2000 - Loss: -0.843
Iter 13/2000 - Loss: -1.120
Iter 14/2000 - Loss: -1.416
Iter 15/2000 - Loss: -1.722
Iter 16/2000 - Loss: -2.031
Iter 17/2000 - Loss: -2.337
Iter 18/2000 - Loss: -2.639
Iter 19/2000 - Loss: -2.935
Iter 20/2000 - Loss: -3.224
Iter 1981/2000 - Loss: -8.283
Iter 1982/2000 - Loss: -8.283
Iter 1983/2000 - Loss: -8.283
Iter 1984/2000 - Loss: -8.283
Iter 1985/2000 - Loss: -8.283
Iter 1986/2000 - Loss: -8.283
Iter 1987/2000 - Loss: -8.283
Iter 1988/2000 - Loss: -8.283
Iter 1989/2000 - Loss: -8.283
Iter 1990/2000 - Loss: -8.283
Iter 1991/2000 - Loss: -8.283
Iter 1992/2000 - Loss: -8.283
Iter 1993/2000 - Loss: -8.283
Iter 1994/2000 - Loss: -8.283
Iter 1995/2000 - Loss: -8.283
Iter 1996/2000 - Loss: -8.283
Iter 1997/2000 - Loss: -8.283
Iter 1998/2000 - Loss: -8.283
Iter 1999/2000 - Loss: -8.283
Iter 2000/2000 - Loss: -8.283
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[13.1248,  5.3428, 26.7960,  9.6865, 18.7799, 39.2514]],

        [[19.4385, 34.7123, 11.6376,  1.4417,  1.2108, 16.2951]],

        [[21.4428, 41.2845, 18.1537,  0.9951,  1.7365, 17.3903]],

        [[18.8069, 40.3890, 18.2609,  3.4682,  3.4496, 43.5083]]])
Signal Variance: tensor([ 0.1026,  0.8610, 14.1614,  0.5021])
Estimated target variance: tensor([0.0242, 0.0909, 1.2140, 0.0233])
N: 160
Signal to noise ratio: tensor([18.3774, 47.1780, 72.3206, 40.0676])
Bound on condition number: tensor([ 54037.3610, 356122.8127, 836844.3230, 256867.2998])
Policy Optimizer learning rate:
0.09843201517785076
Experience 16, Iter 0, disc loss: 0.0005017805226995131, policy loss: 7.9578942450222865
Experience 16, Iter 1, disc loss: 0.0005258587538234733, policy loss: 7.969636177669399
Experience 16, Iter 2, disc loss: 0.00045115988117799415, policy loss: 8.151036242407185
Experience 16, Iter 3, disc loss: 0.0004869301034292559, policy loss: 8.043257859301391
Experience 16, Iter 4, disc loss: 0.00048104901573710855, policy loss: 7.974543520872038
Experience 16, Iter 5, disc loss: 0.0005481920006553395, policy loss: 7.94812820363275
Experience 16, Iter 6, disc loss: 0.0005469504377644394, policy loss: 7.8756949849180975
Experience 16, Iter 7, disc loss: 0.0005595602717921833, policy loss: 8.039039851217261
Experience 16, Iter 8, disc loss: 0.00048627535866741455, policy loss: 7.9699766720192855
Experience 16, Iter 9, disc loss: 0.0004672303045032255, policy loss: 8.06921839648604
Experience 16, Iter 10, disc loss: 0.0005059354118128033, policy loss: 8.07628092696685
Experience 16, Iter 11, disc loss: 0.0004039571529496047, policy loss: 8.183556856178614
Experience 16, Iter 12, disc loss: 0.0005214393674466, policy loss: 8.0065914460812
Experience 16, Iter 13, disc loss: 0.00039948703031285815, policy loss: 8.232691755594313
Experience 16, Iter 14, disc loss: 0.0004641028030650361, policy loss: 8.078654571475024
Experience 16, Iter 15, disc loss: 0.00046600733347953923, policy loss: 8.033816331795574
Experience 16, Iter 16, disc loss: 0.00048291287225682764, policy loss: 7.980905188364923
Experience 16, Iter 17, disc loss: 0.00046281660880349233, policy loss: 8.081113638939826
Experience 16, Iter 18, disc loss: 0.0004348004704538767, policy loss: 8.234828036183124
Experience 16, Iter 19, disc loss: 0.0004956034357437246, policy loss: 7.931156833974529
Experience 16, Iter 20, disc loss: 0.0004614270251278112, policy loss: 8.09239261531665
Experience 16, Iter 21, disc loss: 0.000518484404675068, policy loss: 7.919609414782595
Experience 16, Iter 22, disc loss: 0.0004452039345025827, policy loss: 8.064362949241456
Experience 16, Iter 23, disc loss: 0.000418607059106754, policy loss: 8.19900806152692
Experience 16, Iter 24, disc loss: 0.0004736165909610626, policy loss: 7.993601340949035
Experience 16, Iter 25, disc loss: 0.00041245204431697126, policy loss: 8.180612877468477
Experience 16, Iter 26, disc loss: 0.000418776550830664, policy loss: 8.201186989453564
Experience 16, Iter 27, disc loss: 0.0004688806425998547, policy loss: 8.008043046227975
Experience 16, Iter 28, disc loss: 0.000465704892489438, policy loss: 8.002573112947161
Experience 16, Iter 29, disc loss: 0.0005033858215174598, policy loss: 7.9660691291290835
Experience 16, Iter 30, disc loss: 0.0004550901642718367, policy loss: 8.063302423344286
Experience 16, Iter 31, disc loss: 0.0004917832661406053, policy loss: 8.016945320297033
Experience 16, Iter 32, disc loss: 0.0005284562444532591, policy loss: 7.9236608377942455
Experience 16, Iter 33, disc loss: 0.00046386618451954706, policy loss: 8.102857897582544
Experience 16, Iter 34, disc loss: 0.00043529435408447135, policy loss: 8.202254433046075
Experience 16, Iter 35, disc loss: 0.00047750785184064756, policy loss: 7.960640122056346
Experience 16, Iter 36, disc loss: 0.0004342036197682317, policy loss: 8.142242606361636
Experience 16, Iter 37, disc loss: 0.00047096440260669054, policy loss: 8.102210952189802
Experience 16, Iter 38, disc loss: 0.000507077840445596, policy loss: 7.945950146995241
Experience 16, Iter 39, disc loss: 0.0004759291766249081, policy loss: 8.069321196466746
Experience 16, Iter 40, disc loss: 0.00042885080080672, policy loss: 8.177535015242231
Experience 16, Iter 41, disc loss: 0.00041085780759370896, policy loss: 8.14359005239514
Experience 16, Iter 42, disc loss: 0.0004953460962420007, policy loss: 7.9519276881385
Experience 16, Iter 43, disc loss: 0.0004544436843662994, policy loss: 8.09878170820664
Experience 16, Iter 44, disc loss: 0.00046878133098198665, policy loss: 8.072677647135338
Experience 16, Iter 45, disc loss: 0.0004528985448141301, policy loss: 8.023549231053579
Experience 16, Iter 46, disc loss: 0.0004261561542568677, policy loss: 8.214746391883802
Experience 16, Iter 47, disc loss: 0.00042757635541344264, policy loss: 8.121966064763635
Experience 16, Iter 48, disc loss: 0.0005171793558930533, policy loss: 8.008971238352302
Experience 16, Iter 49, disc loss: 0.0004705788300358506, policy loss: 8.024499710270494
Experience 16, Iter 50, disc loss: 0.0004447684374182526, policy loss: 8.13832901416104
Experience 16, Iter 51, disc loss: 0.000489814871408517, policy loss: 7.921665793160542
Experience 16, Iter 52, disc loss: 0.00045262313827424403, policy loss: 8.151794496483149
Experience 16, Iter 53, disc loss: 0.00043739573348543426, policy loss: 8.224572359197005
Experience 16, Iter 54, disc loss: 0.00042970549577696316, policy loss: 8.131928626530502
Experience 16, Iter 55, disc loss: 0.0005029009075426132, policy loss: 8.00992072369013
Experience 16, Iter 56, disc loss: 0.0004402670634594552, policy loss: 8.129069513406296
Experience 16, Iter 57, disc loss: 0.0003653782941858619, policy loss: 8.328367702346828
Experience 16, Iter 58, disc loss: 0.00039180869242662954, policy loss: 8.381794998205477
Experience 16, Iter 59, disc loss: 0.00045381941468321545, policy loss: 8.13651045627217
Experience 16, Iter 60, disc loss: 0.00045956114327669903, policy loss: 8.126835182148717
Experience 16, Iter 61, disc loss: 0.000479258062870806, policy loss: 8.089742666184964
Experience 16, Iter 62, disc loss: 0.0003540688889083701, policy loss: 8.428700535818152
Experience 16, Iter 63, disc loss: 0.0004167938803334268, policy loss: 8.13191916279655
Experience 16, Iter 64, disc loss: 0.000402490307122598, policy loss: 8.157745313171613
Experience 16, Iter 65, disc loss: 0.00042695643078630483, policy loss: 8.07909821735326
Experience 16, Iter 66, disc loss: 0.0003854359805014357, policy loss: 8.257826612682543
Experience 16, Iter 67, disc loss: 0.0004744466730802657, policy loss: 8.01035327773638
Experience 16, Iter 68, disc loss: 0.0004385896614688367, policy loss: 8.060653266179266
Experience 16, Iter 69, disc loss: 0.00038410197565284413, policy loss: 8.344011659459085
Experience 16, Iter 70, disc loss: 0.0003906918086336973, policy loss: 8.280940961295142
Experience 16, Iter 71, disc loss: 0.000420619878927568, policy loss: 8.110993227062371
Experience 16, Iter 72, disc loss: 0.0003986232017459885, policy loss: 8.194038166031842
Experience 16, Iter 73, disc loss: 0.0004721587689731808, policy loss: 8.058287709659961
Experience 16, Iter 74, disc loss: 0.0004632946211828162, policy loss: 8.119890671486157
Experience 16, Iter 75, disc loss: 0.0004443513886175592, policy loss: 8.143057507018321
Experience 16, Iter 76, disc loss: 0.0004593572276198094, policy loss: 8.111253338237749
Experience 16, Iter 77, disc loss: 0.00045108977732802015, policy loss: 8.055514411357409
Experience 16, Iter 78, disc loss: 0.0004087015582938846, policy loss: 8.189483859092915
Experience 16, Iter 79, disc loss: 0.00043298686797940054, policy loss: 8.162951794908313
Experience 16, Iter 80, disc loss: 0.0004242009344695448, policy loss: 8.188075015556374
Experience 16, Iter 81, disc loss: 0.0004189340397700395, policy loss: 8.229695302345077
Experience 16, Iter 82, disc loss: 0.00047154960926426174, policy loss: 8.04786096640324
Experience 16, Iter 83, disc loss: 0.0004206864995596084, policy loss: 8.266031668342958
Experience 16, Iter 84, disc loss: 0.0004091416247684657, policy loss: 8.153163582285627
Experience 16, Iter 85, disc loss: 0.0004191676121150371, policy loss: 8.177677268511841
Experience 16, Iter 86, disc loss: 0.00045779480318825785, policy loss: 8.152063010956509
Experience 16, Iter 87, disc loss: 0.0004421313582417007, policy loss: 8.07217022484286
Experience 16, Iter 88, disc loss: 0.00039898111598556487, policy loss: 8.323573475519321
Experience 16, Iter 89, disc loss: 0.0004924753046988642, policy loss: 7.916229040375604
Experience 16, Iter 90, disc loss: 0.00042690731758110017, policy loss: 8.102597801656888
Experience 16, Iter 91, disc loss: 0.0003637738939510228, policy loss: 8.34476362006608
Experience 16, Iter 92, disc loss: 0.0004358516428672548, policy loss: 8.008577778295145
Experience 16, Iter 93, disc loss: 0.0003851038726660708, policy loss: 8.303852958015296
Experience 16, Iter 94, disc loss: 0.0004420414117866649, policy loss: 8.125786119094876
Experience 16, Iter 95, disc loss: 0.00040108105107654687, policy loss: 8.15131410150553
Experience 16, Iter 96, disc loss: 0.00040052609853738263, policy loss: 8.203133120616087
Experience 16, Iter 97, disc loss: 0.0004418934241656973, policy loss: 8.06325542268485
Experience 16, Iter 98, disc loss: 0.0004293349582878465, policy loss: 8.072679849035294
Experience 16, Iter 99, disc loss: 0.00040030330994196713, policy loss: 8.215833790523087
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0058],
        [0.0216],
        [0.2864],
        [0.0055]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0338, 0.2127, 0.2535, 0.0074, 0.0031, 0.8155]],

        [[0.0338, 0.2127, 0.2535, 0.0074, 0.0031, 0.8155]],

        [[0.0338, 0.2127, 0.2535, 0.0074, 0.0031, 0.8155]],

        [[0.0338, 0.2127, 0.2535, 0.0074, 0.0031, 0.8155]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0231, 0.0863, 1.1457, 0.0220], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0231, 0.0863, 1.1457, 0.0220])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.768
Iter 2/2000 - Loss: 0.728
Iter 3/2000 - Loss: 0.549
Iter 4/2000 - Loss: 0.476
Iter 5/2000 - Loss: 0.425
Iter 6/2000 - Loss: 0.309
Iter 7/2000 - Loss: 0.137
Iter 8/2000 - Loss: -0.051
Iter 9/2000 - Loss: -0.237
Iter 10/2000 - Loss: -0.432
Iter 11/2000 - Loss: -0.652
Iter 12/2000 - Loss: -0.907
Iter 13/2000 - Loss: -1.191
Iter 14/2000 - Loss: -1.495
Iter 15/2000 - Loss: -1.808
Iter 16/2000 - Loss: -2.123
Iter 17/2000 - Loss: -2.435
Iter 18/2000 - Loss: -2.742
Iter 19/2000 - Loss: -3.045
Iter 20/2000 - Loss: -3.341
Iter 1981/2000 - Loss: -8.334
Iter 1982/2000 - Loss: -8.334
Iter 1983/2000 - Loss: -8.334
Iter 1984/2000 - Loss: -8.334
Iter 1985/2000 - Loss: -8.334
Iter 1986/2000 - Loss: -8.334
Iter 1987/2000 - Loss: -8.334
Iter 1988/2000 - Loss: -8.334
Iter 1989/2000 - Loss: -8.334
Iter 1990/2000 - Loss: -8.334
Iter 1991/2000 - Loss: -8.334
Iter 1992/2000 - Loss: -8.334
Iter 1993/2000 - Loss: -8.334
Iter 1994/2000 - Loss: -8.334
Iter 1995/2000 - Loss: -8.334
Iter 1996/2000 - Loss: -8.334
Iter 1997/2000 - Loss: -8.334
Iter 1998/2000 - Loss: -8.334
Iter 1999/2000 - Loss: -8.335
Iter 2000/2000 - Loss: -8.335
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[12.5286,  5.7011, 26.0369,  9.5320, 18.7012, 40.5200]],

        [[19.2215, 34.0416, 11.5266,  1.4212,  1.2035, 16.0264]],

        [[20.9465, 39.0428, 17.9519,  0.9866,  1.7121, 17.3538]],

        [[18.4454, 37.7751, 18.0393,  3.4617,  3.3727, 43.7259]]])
Signal Variance: tensor([ 0.1074,  0.8248, 13.7720,  0.4885])
Estimated target variance: tensor([0.0231, 0.0863, 1.1457, 0.0220])
N: 170
Signal to noise ratio: tensor([18.4166, 46.5344, 71.4206, 39.8264])
Bound on condition number: tensor([ 57659.9417, 368128.0251, 867153.5120, 269644.6936])
Policy Optimizer learning rate:
0.09832836131379853
Experience 17, Iter 0, disc loss: 0.0004166413219048908, policy loss: 8.0984126153114
Experience 17, Iter 1, disc loss: 0.00043436018934961537, policy loss: 8.08519197204389
Experience 17, Iter 2, disc loss: 0.0004214596697358819, policy loss: 8.068575382383777
Experience 17, Iter 3, disc loss: 0.0003919797065902984, policy loss: 8.187359696198682
Experience 17, Iter 4, disc loss: 0.0003943261160620265, policy loss: 8.202952857923297
Experience 17, Iter 5, disc loss: 0.0003879680599627904, policy loss: 8.170030305889794
Experience 17, Iter 6, disc loss: 0.0004739472167768873, policy loss: 7.923557307414813
Experience 17, Iter 7, disc loss: 0.00039147452720881393, policy loss: 8.299734651257427
Experience 17, Iter 8, disc loss: 0.00036010445276745063, policy loss: 8.430719320377662
Experience 17, Iter 9, disc loss: 0.00039397847106077086, policy loss: 8.308644838742719
Experience 17, Iter 10, disc loss: 0.00045418523595485544, policy loss: 8.010865437212855
Experience 17, Iter 11, disc loss: 0.0004787116577770828, policy loss: 8.009746220723482
Experience 17, Iter 12, disc loss: 0.0004361530012479052, policy loss: 8.077633749394879
Experience 17, Iter 13, disc loss: 0.00045901699001269826, policy loss: 8.181352988369682
Experience 17, Iter 14, disc loss: 0.00040777418450030866, policy loss: 8.169885018900107
Experience 17, Iter 15, disc loss: 0.00042973391323837583, policy loss: 8.061211597785205
Experience 17, Iter 16, disc loss: 0.00036717933995516637, policy loss: 8.329869666160256
Experience 17, Iter 17, disc loss: 0.00035254665305030987, policy loss: 8.478256045643992
Experience 17, Iter 18, disc loss: 0.0004169558347519743, policy loss: 8.108976877818323
Experience 17, Iter 19, disc loss: 0.0003820599101213882, policy loss: 8.391681792439718
Experience 17, Iter 20, disc loss: 0.0004179981578184373, policy loss: 8.178787667895584
Experience 17, Iter 21, disc loss: 0.0004039494782236922, policy loss: 8.106370003514696
Experience 17, Iter 22, disc loss: 0.0003649059533022697, policy loss: 8.402918942561413
Experience 17, Iter 23, disc loss: 0.00042002932297600866, policy loss: 8.2587941778026
Experience 17, Iter 24, disc loss: 0.0003624470659504239, policy loss: 8.329578477707319
Experience 17, Iter 25, disc loss: 0.0004031107948290261, policy loss: 8.096477374457868
Experience 17, Iter 26, disc loss: 0.00040331504871662346, policy loss: 8.119980837506553
Experience 17, Iter 27, disc loss: 0.00040333380617955033, policy loss: 8.163654808886097
Experience 17, Iter 28, disc loss: 0.0003890309667758812, policy loss: 8.278741327433881
Experience 17, Iter 29, disc loss: 0.00039232589731758147, policy loss: 8.21667191717453
Experience 17, Iter 30, disc loss: 0.00041098820785993135, policy loss: 8.252078729991073
Experience 17, Iter 31, disc loss: 0.00038444526684587214, policy loss: 8.198849866273537
Experience 17, Iter 32, disc loss: 0.00037763772502846533, policy loss: 8.312382771461824
Experience 17, Iter 33, disc loss: 0.00035412003799202086, policy loss: 8.329868030836987
Experience 17, Iter 34, disc loss: 0.0003367673489767574, policy loss: 8.422539427780197
Experience 17, Iter 35, disc loss: 0.00037167212275385073, policy loss: 8.330771746698439
Experience 17, Iter 36, disc loss: 0.0003682007499940577, policy loss: 8.250059331745769
Experience 17, Iter 37, disc loss: 0.00039028303430612397, policy loss: 8.268016433769297
Experience 17, Iter 38, disc loss: 0.0003822699851402574, policy loss: 8.282204435875508
Experience 17, Iter 39, disc loss: 0.00032038655277715557, policy loss: 8.507944739707197
Experience 17, Iter 40, disc loss: 0.00036501397600536824, policy loss: 8.312441296408434
Experience 17, Iter 41, disc loss: 0.00041913766451422596, policy loss: 8.19603397876407
Experience 17, Iter 42, disc loss: 0.0003524199980061483, policy loss: 8.291194149859455
Experience 17, Iter 43, disc loss: 0.0003453825305197514, policy loss: 8.401930988224402
Experience 17, Iter 44, disc loss: 0.0004040987939961895, policy loss: 8.199108242040229
Experience 17, Iter 45, disc loss: 0.0004337728774411585, policy loss: 8.183951993173453
Experience 17, Iter 46, disc loss: 0.0003572831983344388, policy loss: 8.448121725052768
Experience 17, Iter 47, disc loss: 0.0003689575508329068, policy loss: 8.291959904681146
Experience 17, Iter 48, disc loss: 0.0004096267954016916, policy loss: 8.263964553087668
Experience 17, Iter 49, disc loss: 0.00038535495168097965, policy loss: 8.280531207337619
Experience 17, Iter 50, disc loss: 0.00037776356235026806, policy loss: 8.303076639237855
Experience 17, Iter 51, disc loss: 0.0003490740790971895, policy loss: 8.457821187872856
Experience 17, Iter 52, disc loss: 0.0003775628334773244, policy loss: 8.249454352184431
Experience 17, Iter 53, disc loss: 0.00035882234634210204, policy loss: 8.310986730549015
Experience 17, Iter 54, disc loss: 0.00038963845045392957, policy loss: 8.217007064183607
Experience 17, Iter 55, disc loss: 0.00035082932446740426, policy loss: 8.389577129789604
Experience 17, Iter 56, disc loss: 0.00042723621298080666, policy loss: 8.157242700573281
Experience 17, Iter 57, disc loss: 0.00043494621337494864, policy loss: 8.244911103140074
Experience 17, Iter 58, disc loss: 0.0003848908181185991, policy loss: 8.363910123698687
Experience 17, Iter 59, disc loss: 0.0003488800769056167, policy loss: 8.340603749318491
Experience 17, Iter 60, disc loss: 0.00038171757492374403, policy loss: 8.271206233457034
Experience 17, Iter 61, disc loss: 0.0003710186845262768, policy loss: 8.354171896921017
Experience 17, Iter 62, disc loss: 0.000359403358991811, policy loss: 8.313739242575014
Experience 17, Iter 63, disc loss: 0.0003878703330752627, policy loss: 8.343148103741466
Experience 17, Iter 64, disc loss: 0.0003710671460989167, policy loss: 8.284863647360812
Experience 17, Iter 65, disc loss: 0.00040540665739489744, policy loss: 8.210119207084878
Experience 17, Iter 66, disc loss: 0.00041523868771956826, policy loss: 8.16588042638494
Experience 17, Iter 67, disc loss: 0.00031420286623222015, policy loss: 8.422400661735324
Experience 17, Iter 68, disc loss: 0.0003607738781412972, policy loss: 8.38691463251315
Experience 17, Iter 69, disc loss: 0.0004094609626176007, policy loss: 8.047588553713991
Experience 17, Iter 70, disc loss: 0.00038750422436548967, policy loss: 8.202966412996169
Experience 17, Iter 71, disc loss: 0.00046073047105661853, policy loss: 8.313058207204055
Experience 17, Iter 72, disc loss: 0.0003602699569897155, policy loss: 8.316545384749903
Experience 17, Iter 73, disc loss: 0.0004060517426021825, policy loss: 8.205945579612935
Experience 17, Iter 74, disc loss: 0.0003329424630362899, policy loss: 8.422093794631326
Experience 17, Iter 75, disc loss: 0.0003545889122259028, policy loss: 8.33357706497417
Experience 17, Iter 76, disc loss: 0.0003591079417350915, policy loss: 8.381538702441143
Experience 17, Iter 77, disc loss: 0.00041100992053953045, policy loss: 8.201736661962864
Experience 17, Iter 78, disc loss: 0.00038739099373812785, policy loss: 8.171205084524017
Experience 17, Iter 79, disc loss: 0.0003541790073688837, policy loss: 8.332940278082079
Experience 17, Iter 80, disc loss: 0.0003553673062721774, policy loss: 8.396198069191442
Experience 17, Iter 81, disc loss: 0.0003833376929598993, policy loss: 8.370599215734563
Experience 17, Iter 82, disc loss: 0.00040910784487528304, policy loss: 8.207378265353178
Experience 17, Iter 83, disc loss: 0.00035667591412300156, policy loss: 8.26369439664557
Experience 17, Iter 84, disc loss: 0.00036415143893538354, policy loss: 8.47009472196505
Experience 17, Iter 85, disc loss: 0.00032912192104926093, policy loss: 8.547093048652457
Experience 17, Iter 86, disc loss: 0.00031443879325490816, policy loss: 8.611739801044461
Experience 17, Iter 87, disc loss: 0.00034028542734685366, policy loss: 8.360160817256407
Experience 17, Iter 88, disc loss: 0.00038722630866582896, policy loss: 8.2753814233583
Experience 17, Iter 89, disc loss: 0.00035902011533990125, policy loss: 8.332789267884891
Experience 17, Iter 90, disc loss: 0.00035687895325300846, policy loss: 8.315580751594053
Experience 17, Iter 91, disc loss: 0.00033899822658805135, policy loss: 8.3352206422902
Experience 17, Iter 92, disc loss: 0.00041504019845938877, policy loss: 8.06641235805122
Experience 17, Iter 93, disc loss: 0.00033806188435819476, policy loss: 8.295138693859817
Experience 17, Iter 94, disc loss: 0.0003586495058277239, policy loss: 8.255005471594671
Experience 17, Iter 95, disc loss: 0.0003721579631886228, policy loss: 8.315932059869212
Experience 17, Iter 96, disc loss: 0.00037683943682994186, policy loss: 8.256953289892625
Experience 17, Iter 97, disc loss: 0.00040699127568512967, policy loss: 8.296131696877769
Experience 17, Iter 98, disc loss: 0.00039259868155182694, policy loss: 8.302563941533665
Experience 17, Iter 99, disc loss: 0.0003741344196936436, policy loss: 8.247989346329948
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.0205],
        [0.2705],
        [0.0052]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0320, 0.2021, 0.2393, 0.0070, 0.0029, 0.7760]],

        [[0.0320, 0.2021, 0.2393, 0.0070, 0.0029, 0.7760]],

        [[0.0320, 0.2021, 0.2393, 0.0070, 0.0029, 0.7760]],

        [[0.0320, 0.2021, 0.2393, 0.0070, 0.0029, 0.7760]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0220, 0.0820, 1.0819, 0.0208], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0220, 0.0820, 1.0819, 0.0208])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.628
Iter 2/2000 - Loss: 0.595
Iter 3/2000 - Loss: 0.406
Iter 4/2000 - Loss: 0.328
Iter 5/2000 - Loss: 0.275
Iter 6/2000 - Loss: 0.154
Iter 7/2000 - Loss: -0.024
Iter 8/2000 - Loss: -0.214
Iter 9/2000 - Loss: -0.401
Iter 10/2000 - Loss: -0.594
Iter 11/2000 - Loss: -0.814
Iter 12/2000 - Loss: -1.067
Iter 13/2000 - Loss: -1.351
Iter 14/2000 - Loss: -1.654
Iter 15/2000 - Loss: -1.967
Iter 16/2000 - Loss: -2.281
Iter 17/2000 - Loss: -2.593
Iter 18/2000 - Loss: -2.901
Iter 19/2000 - Loss: -3.204
Iter 20/2000 - Loss: -3.502
Iter 1981/2000 - Loss: -8.400
Iter 1982/2000 - Loss: -8.400
Iter 1983/2000 - Loss: -8.400
Iter 1984/2000 - Loss: -8.400
Iter 1985/2000 - Loss: -8.400
Iter 1986/2000 - Loss: -8.400
Iter 1987/2000 - Loss: -8.400
Iter 1988/2000 - Loss: -8.400
Iter 1989/2000 - Loss: -8.400
Iter 1990/2000 - Loss: -8.401
Iter 1991/2000 - Loss: -8.401
Iter 1992/2000 - Loss: -8.401
Iter 1993/2000 - Loss: -8.401
Iter 1994/2000 - Loss: -8.401
Iter 1995/2000 - Loss: -8.401
Iter 1996/2000 - Loss: -8.401
Iter 1997/2000 - Loss: -8.401
Iter 1998/2000 - Loss: -8.401
Iter 1999/2000 - Loss: -8.401
Iter 2000/2000 - Loss: -8.401
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[12.1655,  5.4824, 25.6092,  9.3345, 18.5694, 39.6635]],

        [[18.9762, 33.3988, 11.5686,  1.4335,  1.2127, 16.5056]],

        [[20.6867, 38.4111, 18.0720,  0.9903,  1.6827, 17.8252]],

        [[18.0158, 36.0888, 17.8861,  3.4109,  3.2894, 43.6494]]])
Signal Variance: tensor([ 0.1040,  0.8598, 14.1256,  0.4773])
Estimated target variance: tensor([0.0220, 0.0820, 1.0819, 0.0208])
N: 180
Signal to noise ratio: tensor([17.9148, 48.3082, 72.7473, 39.7260])
Bound on condition number: tensor([ 57770.2101, 420063.7427, 952591.9583, 284068.3423])
Policy Optimizer learning rate:
0.09822481660247992
Experience 18, Iter 0, disc loss: 0.0003753071291779009, policy loss: 8.212013320480914
Experience 18, Iter 1, disc loss: 0.00036826797404414767, policy loss: 8.467685515371247
Experience 18, Iter 2, disc loss: 0.0003180493084715099, policy loss: 8.520705196813658
Experience 18, Iter 3, disc loss: 0.00034221530434345857, policy loss: 8.446630882404445
Experience 18, Iter 4, disc loss: 0.00037117933168864657, policy loss: 8.295525281295223
Experience 18, Iter 5, disc loss: 0.0003721049276241605, policy loss: 8.260340825859638
Experience 18, Iter 6, disc loss: 0.00033514030377311527, policy loss: 8.459256098909893
Experience 18, Iter 7, disc loss: 0.0003223503268820421, policy loss: 8.5324748999202
Experience 18, Iter 8, disc loss: 0.0003285649643900296, policy loss: 8.510318040911368
Experience 18, Iter 9, disc loss: 0.00033868155119783707, policy loss: 8.447543273939617
Experience 18, Iter 10, disc loss: 0.00036443109992503756, policy loss: 8.243161873919055
Experience 18, Iter 11, disc loss: 0.0003317585577615903, policy loss: 8.392434437375881
Experience 18, Iter 12, disc loss: 0.00035590908481800886, policy loss: 8.341670045944856
Experience 18, Iter 13, disc loss: 0.00033619119260541285, policy loss: 8.42007968027815
Experience 18, Iter 14, disc loss: 0.00035544612837522986, policy loss: 8.468335752242085
Experience 18, Iter 15, disc loss: 0.00037075900911503687, policy loss: 8.316132472784583
Experience 18, Iter 16, disc loss: 0.0002904742508656248, policy loss: 8.678319022294508
Experience 18, Iter 17, disc loss: 0.00037876990853028875, policy loss: 8.264986717572425
Experience 18, Iter 18, disc loss: 0.0003117089879967328, policy loss: 8.506245181441079
Experience 18, Iter 19, disc loss: 0.00035779548118614345, policy loss: 8.281699679774952
Experience 18, Iter 20, disc loss: 0.0003491382719591672, policy loss: 8.423127293914723
Experience 18, Iter 21, disc loss: 0.0003133419723647009, policy loss: 8.411490382630301
Experience 18, Iter 22, disc loss: 0.00032540472120123757, policy loss: 8.405509144114406
Experience 18, Iter 23, disc loss: 0.0003208999269507817, policy loss: 8.392553483153925
Experience 18, Iter 24, disc loss: 0.0003803960879795516, policy loss: 8.29828637832275
Experience 18, Iter 25, disc loss: 0.000314173663391268, policy loss: 8.497847294807155
Experience 18, Iter 26, disc loss: 0.000381449120613586, policy loss: 8.269317343702186
Experience 18, Iter 27, disc loss: 0.0003245268087819432, policy loss: 8.519100026215941
Experience 18, Iter 28, disc loss: 0.00034525210242389824, policy loss: 8.396353740083528
Experience 18, Iter 29, disc loss: 0.0003775393657689639, policy loss: 8.426017788049391
Experience 18, Iter 30, disc loss: 0.00032307470794507465, policy loss: 8.477960477325453
Experience 18, Iter 31, disc loss: 0.00036592839811102923, policy loss: 8.380769862346003
Experience 18, Iter 32, disc loss: 0.0003415089391148983, policy loss: 8.292612922436577
Experience 18, Iter 33, disc loss: 0.000452629677906826, policy loss: 8.111862257906921
Experience 18, Iter 34, disc loss: 0.0003354269179895233, policy loss: 8.38307536052914
Experience 18, Iter 35, disc loss: 0.0003336773394533708, policy loss: 8.37992029165652
Experience 18, Iter 36, disc loss: 0.0003447225769088562, policy loss: 8.361116398995295
Experience 18, Iter 37, disc loss: 0.0003646692582265384, policy loss: 8.44007290502398
Experience 18, Iter 38, disc loss: 0.0003673665611695128, policy loss: 8.337391997624401
Experience 18, Iter 39, disc loss: 0.0003684434190316535, policy loss: 8.46651962447608
Experience 18, Iter 40, disc loss: 0.0003911158407856431, policy loss: 8.355594706504371
Experience 18, Iter 41, disc loss: 0.0003585250331618878, policy loss: 8.474357790246712
Experience 18, Iter 42, disc loss: 0.0003576548623818178, policy loss: 8.443926430316518
Experience 18, Iter 43, disc loss: 0.0003557292338700692, policy loss: 8.384349580519341
Experience 18, Iter 44, disc loss: 0.00035220046484818696, policy loss: 8.266788357103305
Experience 18, Iter 45, disc loss: 0.00036237634203660274, policy loss: 8.219814396357041
Experience 18, Iter 46, disc loss: 0.00032466490893616186, policy loss: 8.415821669420591
Experience 18, Iter 47, disc loss: 0.00033056982692244725, policy loss: 8.425818540604258
Experience 18, Iter 48, disc loss: 0.0003029307759096748, policy loss: 8.583586387064784
Experience 18, Iter 49, disc loss: 0.0003577481959615876, policy loss: 8.210116560351473
Experience 18, Iter 50, disc loss: 0.00034702256886591195, policy loss: 8.38454676542999
Experience 18, Iter 51, disc loss: 0.0003625525017029203, policy loss: 8.322401491703967
Experience 18, Iter 52, disc loss: 0.0003500951755438634, policy loss: 8.286534983103301
Experience 18, Iter 53, disc loss: 0.0003109792140582001, policy loss: 8.531000028387098
Experience 18, Iter 54, disc loss: 0.0003553360552111241, policy loss: 8.416034164476166
Experience 18, Iter 55, disc loss: 0.00029938503713099456, policy loss: 8.519156226440955
Experience 18, Iter 56, disc loss: 0.00031865130096777206, policy loss: 8.545699453504465
Experience 18, Iter 57, disc loss: 0.0002948821811697853, policy loss: 8.563585565101565
Experience 18, Iter 58, disc loss: 0.0003336261535330501, policy loss: 8.50141765587568
Experience 18, Iter 59, disc loss: 0.00036474751445388846, policy loss: 8.526885319351997
Experience 18, Iter 60, disc loss: 0.0003095565159707262, policy loss: 8.586774360825341
Experience 18, Iter 61, disc loss: 0.0003759441853916982, policy loss: 8.25774735506351
Experience 18, Iter 62, disc loss: 0.0003716574583482571, policy loss: 8.183325224462077
Experience 18, Iter 63, disc loss: 0.00034038807026036595, policy loss: 8.374533881332884
Experience 18, Iter 64, disc loss: 0.00032615038349342255, policy loss: 8.453144941576372
Experience 18, Iter 65, disc loss: 0.0003641110786832086, policy loss: 8.380866714723656
Experience 18, Iter 66, disc loss: 0.00035864350083658956, policy loss: 8.366872165005699
Experience 18, Iter 67, disc loss: 0.00033171433171110443, policy loss: 8.475872535150344
Experience 18, Iter 68, disc loss: 0.000333047602543297, policy loss: 8.495169590039705
Experience 18, Iter 69, disc loss: 0.0003922496436001171, policy loss: 8.291772595716523
Experience 18, Iter 70, disc loss: 0.0002866042445871359, policy loss: 8.585005310633424
Experience 18, Iter 71, disc loss: 0.00033082620893043105, policy loss: 8.47006646004394
Experience 18, Iter 72, disc loss: 0.00037469621487440884, policy loss: 8.343164802278286
Experience 18, Iter 73, disc loss: 0.00031123133978238065, policy loss: 8.551037081524422
Experience 18, Iter 74, disc loss: 0.00033327638071541283, policy loss: 8.385309705170599
Experience 18, Iter 75, disc loss: 0.00035487114580307056, policy loss: 8.340709271537188
Experience 18, Iter 76, disc loss: 0.00030860905445067774, policy loss: 8.510780707938295
Experience 18, Iter 77, disc loss: 0.00031046545319488755, policy loss: 8.497212394470806
Experience 18, Iter 78, disc loss: 0.0003138660846707496, policy loss: 8.45178662022177
Experience 18, Iter 79, disc loss: 0.0003355052638879838, policy loss: 8.411340306352749
Experience 18, Iter 80, disc loss: 0.0003682496566414124, policy loss: 8.386785848082809
Experience 18, Iter 81, disc loss: 0.0003303377818384255, policy loss: 8.39322705218785
Experience 18, Iter 82, disc loss: 0.0003255242136176608, policy loss: 8.415116445327058
Experience 18, Iter 83, disc loss: 0.0002888351784391937, policy loss: 8.583251389096743
Experience 18, Iter 84, disc loss: 0.0002976702304939925, policy loss: 8.45921962235852
Experience 18, Iter 85, disc loss: 0.0002882670327747245, policy loss: 8.604506110982246
Experience 18, Iter 86, disc loss: 0.00028332633318878985, policy loss: 8.59303208804466
Experience 18, Iter 87, disc loss: 0.00031758961498959285, policy loss: 8.444599876670095
Experience 18, Iter 88, disc loss: 0.00032648935991060177, policy loss: 8.429601208703534
Experience 18, Iter 89, disc loss: 0.0003138758789452611, policy loss: 8.476890667284561
Experience 18, Iter 90, disc loss: 0.0003467054591219667, policy loss: 8.478446981357285
Experience 18, Iter 91, disc loss: 0.00031263334339097097, policy loss: 8.544623349200696
Experience 18, Iter 92, disc loss: 0.00028914376779314304, policy loss: 8.79485279337002
Experience 18, Iter 93, disc loss: 0.00029434344493998706, policy loss: 8.552690878738451
Experience 18, Iter 94, disc loss: 0.0002966046634319313, policy loss: 8.486295976488947
Experience 18, Iter 95, disc loss: 0.00031470590893990823, policy loss: 8.483210262049731
Experience 18, Iter 96, disc loss: 0.0003720832715448896, policy loss: 8.252339350855928
Experience 18, Iter 97, disc loss: 0.00029437913814811593, policy loss: 8.524513901764104
Experience 18, Iter 98, disc loss: 0.0002746192197981607, policy loss: 8.71626908937396
Experience 18, Iter 99, disc loss: 0.0003165862594897064, policy loss: 8.464210783765921
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.0196],
        [0.2566],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0304, 0.1920, 0.2271, 0.0067, 0.0028, 0.7407]],

        [[0.0304, 0.1920, 0.2271, 0.0067, 0.0028, 0.7407]],

        [[0.0304, 0.1920, 0.2271, 0.0067, 0.0028, 0.7407]],

        [[0.0304, 0.1920, 0.2271, 0.0067, 0.0028, 0.7407]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0209, 0.0783, 1.0266, 0.0198], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0209, 0.0783, 1.0266, 0.0198])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.523
Iter 2/2000 - Loss: 0.510
Iter 3/2000 - Loss: 0.312
Iter 4/2000 - Loss: 0.234
Iter 5/2000 - Loss: 0.187
Iter 6/2000 - Loss: 0.066
Iter 7/2000 - Loss: -0.116
Iter 8/2000 - Loss: -0.308
Iter 9/2000 - Loss: -0.494
Iter 10/2000 - Loss: -0.689
Iter 11/2000 - Loss: -0.911
Iter 12/2000 - Loss: -1.168
Iter 13/2000 - Loss: -1.455
Iter 14/2000 - Loss: -1.762
Iter 15/2000 - Loss: -2.078
Iter 16/2000 - Loss: -2.395
Iter 17/2000 - Loss: -2.710
Iter 18/2000 - Loss: -3.022
Iter 19/2000 - Loss: -3.329
Iter 20/2000 - Loss: -3.630
Iter 1981/2000 - Loss: -8.473
Iter 1982/2000 - Loss: -8.473
Iter 1983/2000 - Loss: -8.473
Iter 1984/2000 - Loss: -8.473
Iter 1985/2000 - Loss: -8.473
Iter 1986/2000 - Loss: -8.473
Iter 1987/2000 - Loss: -8.473
Iter 1988/2000 - Loss: -8.473
Iter 1989/2000 - Loss: -8.473
Iter 1990/2000 - Loss: -8.473
Iter 1991/2000 - Loss: -8.474
Iter 1992/2000 - Loss: -8.474
Iter 1993/2000 - Loss: -8.474
Iter 1994/2000 - Loss: -8.474
Iter 1995/2000 - Loss: -8.474
Iter 1996/2000 - Loss: -8.474
Iter 1997/2000 - Loss: -8.474
Iter 1998/2000 - Loss: -8.474
Iter 1999/2000 - Loss: -8.474
Iter 2000/2000 - Loss: -8.474
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[12.0866,  5.2947, 25.0802,  9.2204, 18.7142, 39.4015]],

        [[18.5109, 32.4573, 11.4955,  1.4092,  1.2195, 16.3875]],

        [[20.1811, 37.3592, 17.8911,  0.9894,  1.6935, 17.5827]],

        [[17.5261, 34.9363, 17.6878,  3.3410,  3.0752, 44.0484]]])
Signal Variance: tensor([ 0.1003,  0.8390, 13.7170,  0.4646])
Estimated target variance: tensor([0.0209, 0.0783, 1.0266, 0.0198])
N: 190
Signal to noise ratio: tensor([17.8797, 47.5692, 71.7310, 39.8869])
Bound on condition number: tensor([ 60741.0946, 429938.0859, 977614.4002, 302284.1421])
Policy Optimizer learning rate:
0.09812138092895162
Experience 19, Iter 0, disc loss: 0.0002745554936230942, policy loss: 8.590429366051001
Experience 19, Iter 1, disc loss: 0.00031515932240082443, policy loss: 8.45844638968039
Experience 19, Iter 2, disc loss: 0.00031250533887249916, policy loss: 8.535180237113064
Experience 19, Iter 3, disc loss: 0.00028483772789182025, policy loss: 8.678207017432985
Experience 19, Iter 4, disc loss: 0.00034018892921045126, policy loss: 8.402196415873956
Experience 19, Iter 5, disc loss: 0.00030204377843879606, policy loss: 8.553574721315448
Experience 19, Iter 6, disc loss: 0.0003040483656986117, policy loss: 8.508052784482995
Experience 19, Iter 7, disc loss: 0.00025448839538635856, policy loss: 8.67627715849909
Experience 19, Iter 8, disc loss: 0.00031353009346239176, policy loss: 8.398707580897211
Experience 19, Iter 9, disc loss: 0.00031515456506611196, policy loss: 8.494524432141173
Experience 19, Iter 10, disc loss: 0.0003103396435215625, policy loss: 8.385710046732465
Experience 19, Iter 11, disc loss: 0.000309120507569432, policy loss: 8.60139961924008
Experience 19, Iter 12, disc loss: 0.00029149496823170607, policy loss: 8.527764420670255
Experience 19, Iter 13, disc loss: 0.0002570205150964329, policy loss: 8.745181242645
Experience 19, Iter 14, disc loss: 0.0002891489876954584, policy loss: 8.58098740321337
Experience 19, Iter 15, disc loss: 0.0002955149367969081, policy loss: 8.564094425068454
Experience 19, Iter 16, disc loss: 0.0002743955357035157, policy loss: 8.573052808452438
Experience 19, Iter 17, disc loss: 0.00025974732601757226, policy loss: 8.764018851499735
Experience 19, Iter 18, disc loss: 0.00028049599177545014, policy loss: 8.627177915882564
Experience 19, Iter 19, disc loss: 0.00038215176629917674, policy loss: 8.341936192719132
Experience 19, Iter 20, disc loss: 0.00032966073949500295, policy loss: 8.471832072837655
Experience 19, Iter 21, disc loss: 0.0002774618051025335, policy loss: 8.606995157940824
Experience 19, Iter 22, disc loss: 0.0002931086982192506, policy loss: 8.535674828683593
Experience 19, Iter 23, disc loss: 0.00029712781242868737, policy loss: 8.543213180490808
Experience 19, Iter 24, disc loss: 0.00031159482848710786, policy loss: 8.360891604953059
Experience 19, Iter 25, disc loss: 0.00030136255675160164, policy loss: 8.530609756897515
Experience 19, Iter 26, disc loss: 0.00029221022332739916, policy loss: 8.590454035719283
Experience 19, Iter 27, disc loss: 0.000267025395344097, policy loss: 8.666652928386725
Experience 19, Iter 28, disc loss: 0.00028872233313367764, policy loss: 8.673800518034273
Experience 19, Iter 29, disc loss: 0.00031406923545164895, policy loss: 8.527824830496337
Experience 19, Iter 30, disc loss: 0.00027397468514257827, policy loss: 8.569192416902357
Experience 19, Iter 31, disc loss: 0.00027647089020606, policy loss: 8.60771597172225
Experience 19, Iter 32, disc loss: 0.0003009706847971225, policy loss: 8.479788375528369
Experience 19, Iter 33, disc loss: 0.0003477392747276002, policy loss: 8.432933527800424
Experience 19, Iter 34, disc loss: 0.0003018016002738189, policy loss: 8.609942708751326
Experience 19, Iter 35, disc loss: 0.00030819258353892903, policy loss: 8.520526248418394
Experience 19, Iter 36, disc loss: 0.00030054232989617407, policy loss: 8.629272904946262
Experience 19, Iter 37, disc loss: 0.0002916084990046617, policy loss: 8.599167296831805
Experience 19, Iter 38, disc loss: 0.0002683816221475116, policy loss: 8.709565212701758
Experience 19, Iter 39, disc loss: 0.0003101545381264443, policy loss: 8.430430903149414
Experience 19, Iter 40, disc loss: 0.00027547287352561216, policy loss: 8.536782171021112
Experience 19, Iter 41, disc loss: 0.00035737793965251674, policy loss: 8.405544652751448
Experience 19, Iter 42, disc loss: 0.0002708773539089984, policy loss: 8.653105097478553
Experience 19, Iter 43, disc loss: 0.0003092414310319269, policy loss: 8.441894438209626
Experience 19, Iter 44, disc loss: 0.0002837745697673896, policy loss: 8.53442017474709
Experience 19, Iter 45, disc loss: 0.00027189781550319547, policy loss: 8.644508110060968
Experience 19, Iter 46, disc loss: 0.0003060391449545715, policy loss: 8.493820615998827
Experience 19, Iter 47, disc loss: 0.0003352427371771792, policy loss: 8.53899162323702
Experience 19, Iter 48, disc loss: 0.0003190609243243195, policy loss: 8.516525746508307
Experience 19, Iter 49, disc loss: 0.00032772068223239795, policy loss: 8.476110555831273
Experience 19, Iter 50, disc loss: 0.0003008297315322435, policy loss: 8.49016704728248
Experience 19, Iter 51, disc loss: 0.0002568613002666028, policy loss: 8.793234225533551
Experience 19, Iter 52, disc loss: 0.00029436865059921294, policy loss: 8.528924275514475
Experience 19, Iter 53, disc loss: 0.0002850261427489056, policy loss: 8.649957060561775
Experience 19, Iter 54, disc loss: 0.00031769795593129607, policy loss: 8.507219350316374
Experience 19, Iter 55, disc loss: 0.0003056944817102998, policy loss: 8.467989682942626
Experience 19, Iter 56, disc loss: 0.00027720584535899, policy loss: 8.640474836329393
Experience 19, Iter 57, disc loss: 0.0002695142056515153, policy loss: 8.70017018276871
Experience 19, Iter 58, disc loss: 0.0003200490369491877, policy loss: 8.481671728983953
Experience 19, Iter 59, disc loss: 0.0003073018383688862, policy loss: 8.432543595912744
Experience 19, Iter 60, disc loss: 0.00026359283816172123, policy loss: 8.631825799862716
Experience 19, Iter 61, disc loss: 0.0002753400268026298, policy loss: 8.665974184879756
Experience 19, Iter 62, disc loss: 0.0003130621739979576, policy loss: 8.500279029288654
Experience 19, Iter 63, disc loss: 0.00029422646872307915, policy loss: 8.712686406081001
Experience 19, Iter 64, disc loss: 0.0002703470451401968, policy loss: 8.67240001273638
Experience 19, Iter 65, disc loss: 0.00028975124960947646, policy loss: 8.457483145940182
Experience 19, Iter 66, disc loss: 0.00026997792240488253, policy loss: 8.639611134337848
Experience 19, Iter 67, disc loss: 0.00025868377464566104, policy loss: 8.638461903402002
Experience 19, Iter 68, disc loss: 0.00025189113375369667, policy loss: 8.731727112227224
Experience 19, Iter 69, disc loss: 0.0002776543104366281, policy loss: 8.538196536110316
Experience 19, Iter 70, disc loss: 0.0002639518400530632, policy loss: 8.57782589148842
Experience 19, Iter 71, disc loss: 0.00027197190026771446, policy loss: 8.631701396873275
Experience 19, Iter 72, disc loss: 0.0002641687830252571, policy loss: 8.73721468236365
Experience 19, Iter 73, disc loss: 0.00027199805757665007, policy loss: 8.705107520265544
Experience 19, Iter 74, disc loss: 0.0002987106283538206, policy loss: 8.506744677914252
Experience 19, Iter 75, disc loss: 0.00029799385360661473, policy loss: 8.36705915290485
Experience 19, Iter 76, disc loss: 0.0002522366525120396, policy loss: 8.77772626031527
Experience 19, Iter 77, disc loss: 0.00031187835655819434, policy loss: 8.484810481036252
Experience 19, Iter 78, disc loss: 0.0002822544560228073, policy loss: 8.546134845166334
Experience 19, Iter 79, disc loss: 0.00026016553160589724, policy loss: 8.701373835144265
Experience 19, Iter 80, disc loss: 0.00023942600950646343, policy loss: 8.830299960345762
Experience 19, Iter 81, disc loss: 0.00027361531428195864, policy loss: 8.648480748200035
Experience 19, Iter 82, disc loss: 0.00026293782695547657, policy loss: 8.630696725203755
Experience 19, Iter 83, disc loss: 0.0002600530561617141, policy loss: 8.648061390433636
Experience 19, Iter 84, disc loss: 0.0003031167186611237, policy loss: 8.500052397632032
Experience 19, Iter 85, disc loss: 0.000310955235564408, policy loss: 8.42357084733726
Experience 19, Iter 86, disc loss: 0.00032662896339832247, policy loss: 8.410078101803139
Experience 19, Iter 87, disc loss: 0.0002506588098714096, policy loss: 8.679991119472557
Experience 19, Iter 88, disc loss: 0.00026194153805154355, policy loss: 8.7213636278552
Experience 19, Iter 89, disc loss: 0.0002862287600336798, policy loss: 8.621741161509778
Experience 19, Iter 90, disc loss: 0.0003426007340182222, policy loss: 8.324402770667893
Experience 19, Iter 91, disc loss: 0.0002977165737352068, policy loss: 8.50870836733094
Experience 19, Iter 92, disc loss: 0.0002886732565125715, policy loss: 8.568383224672619
Experience 19, Iter 93, disc loss: 0.0002752625938711335, policy loss: 8.525792145674519
Experience 19, Iter 94, disc loss: 0.0002892855397826626, policy loss: 8.600255004366897
Experience 19, Iter 95, disc loss: 0.0002726333497346494, policy loss: 8.622886894292652
Experience 19, Iter 96, disc loss: 0.0002921844491115365, policy loss: 8.518364250475733
Experience 19, Iter 97, disc loss: 0.0002542336542339111, policy loss: 8.760397469470902
Experience 19, Iter 98, disc loss: 0.000246841875868901, policy loss: 8.813245901321903
Experience 19, Iter 99, disc loss: 0.00025296261829428564, policy loss: 8.820363372164229
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.0188],
        [0.2556],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0289, 0.1835, 0.2263, 0.0066, 0.0026, 0.7082]],

        [[0.0289, 0.1835, 0.2263, 0.0066, 0.0026, 0.7082]],

        [[0.0289, 0.1835, 0.2263, 0.0066, 0.0026, 0.7082]],

        [[0.0289, 0.1835, 0.2263, 0.0066, 0.0026, 0.7082]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.0754, 1.0225, 0.0197], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.0754, 1.0225, 0.0197])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.486
Iter 2/2000 - Loss: 0.497
Iter 3/2000 - Loss: 0.295
Iter 4/2000 - Loss: 0.223
Iter 5/2000 - Loss: 0.191
Iter 6/2000 - Loss: 0.078
Iter 7/2000 - Loss: -0.097
Iter 8/2000 - Loss: -0.276
Iter 9/2000 - Loss: -0.444
Iter 10/2000 - Loss: -0.621
Iter 11/2000 - Loss: -0.828
Iter 12/2000 - Loss: -1.072
Iter 13/2000 - Loss: -1.347
Iter 14/2000 - Loss: -1.641
Iter 15/2000 - Loss: -1.942
Iter 16/2000 - Loss: -2.247
Iter 17/2000 - Loss: -2.553
Iter 18/2000 - Loss: -2.860
Iter 19/2000 - Loss: -3.167
Iter 20/2000 - Loss: -3.473
Iter 1981/2000 - Loss: -8.480
Iter 1982/2000 - Loss: -8.480
Iter 1983/2000 - Loss: -8.480
Iter 1984/2000 - Loss: -8.480
Iter 1985/2000 - Loss: -8.480
Iter 1986/2000 - Loss: -8.480
Iter 1987/2000 - Loss: -8.480
Iter 1988/2000 - Loss: -8.480
Iter 1989/2000 - Loss: -8.480
Iter 1990/2000 - Loss: -8.480
Iter 1991/2000 - Loss: -8.480
Iter 1992/2000 - Loss: -8.480
Iter 1993/2000 - Loss: -8.480
Iter 1994/2000 - Loss: -8.480
Iter 1995/2000 - Loss: -8.480
Iter 1996/2000 - Loss: -8.480
Iter 1997/2000 - Loss: -8.480
Iter 1998/2000 - Loss: -8.480
Iter 1999/2000 - Loss: -8.481
Iter 2000/2000 - Loss: -8.481
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[12.1675,  6.0600, 24.8318, 10.0036, 18.0643, 37.0800]],

        [[18.0656, 32.2189, 11.3827,  1.4609,  1.2648, 16.7735]],

        [[19.8308, 36.7240, 18.0705,  1.0161,  1.7723, 17.8448]],

        [[17.1849, 35.1483, 17.7882,  3.8117,  2.9710, 44.0364]]])
Signal Variance: tensor([ 0.1042,  0.8563, 14.0145,  0.4716])
Estimated target variance: tensor([0.0200, 0.0754, 1.0225, 0.0197])
N: 200
Signal to noise ratio: tensor([18.2493, 47.9050, 71.3610, 39.4068])
Bound on condition number: tensor([  66608.5400,  458979.4458, 1018480.7269,  310580.2398])
Policy Optimizer learning rate:
0.09801805417839134
Experience 20, Iter 0, disc loss: 0.00028347136436197753, policy loss: 8.663560288767023
Experience 20, Iter 1, disc loss: 0.00026541595502874784, policy loss: 8.685577102613724
Experience 20, Iter 2, disc loss: 0.00023229901884674368, policy loss: 8.82087438378679
Experience 20, Iter 3, disc loss: 0.0002849877712460916, policy loss: 8.609837365964907
Experience 20, Iter 4, disc loss: 0.00027527001175110767, policy loss: 8.662119445184608
Experience 20, Iter 5, disc loss: 0.0002523734583736529, policy loss: 8.694988655471548
Experience 20, Iter 6, disc loss: 0.00028431799233723633, policy loss: 8.515227678439128
Experience 20, Iter 7, disc loss: 0.00024749748365135205, policy loss: 8.691297147223796
Experience 20, Iter 8, disc loss: 0.00025714724784415584, policy loss: 8.722250222104222
Experience 20, Iter 9, disc loss: 0.0002786552614586613, policy loss: 8.550167115399312
Experience 20, Iter 10, disc loss: 0.0002714185413750649, policy loss: 8.57646848356352
Experience 20, Iter 11, disc loss: 0.00028098513384001, policy loss: 8.529736312218104
Experience 20, Iter 12, disc loss: 0.00024468508238478055, policy loss: 8.834269779255512
Experience 20, Iter 13, disc loss: 0.00025380534382629785, policy loss: 8.687583590969165
Experience 20, Iter 14, disc loss: 0.0002663841327716026, policy loss: 8.6408480677538
Experience 20, Iter 15, disc loss: 0.00027140260084845645, policy loss: 8.784904603363938
Experience 20, Iter 16, disc loss: 0.0003142313895996912, policy loss: 8.464348310554488
Experience 20, Iter 17, disc loss: 0.00028226376385667034, policy loss: 8.61121756868018
Experience 20, Iter 18, disc loss: 0.0002763534387980077, policy loss: 8.542690337247254
Experience 20, Iter 19, disc loss: 0.00030207516445172194, policy loss: 8.517943573302912
Experience 20, Iter 20, disc loss: 0.00031432803208864717, policy loss: 8.607419464084026
Experience 20, Iter 21, disc loss: 0.0002812989239416245, policy loss: 8.60682641671028
Experience 20, Iter 22, disc loss: 0.00029448530331301976, policy loss: 8.618306579340622
Experience 20, Iter 23, disc loss: 0.00028417360692690406, policy loss: 8.729062598219485
Experience 20, Iter 24, disc loss: 0.00025847613628301547, policy loss: 8.705119971713554
Experience 20, Iter 25, disc loss: 0.00027114488016943817, policy loss: 8.724274937359151
Experience 20, Iter 26, disc loss: 0.0002479839760924337, policy loss: 8.808942302240368
Experience 20, Iter 27, disc loss: 0.0002435845827152815, policy loss: 8.907118271738392
Experience 20, Iter 28, disc loss: 0.0002586464088987773, policy loss: 8.74549495045705
Experience 20, Iter 29, disc loss: 0.0002477642230995937, policy loss: 8.707485491905103
Experience 20, Iter 30, disc loss: 0.00023860614743032097, policy loss: 8.811405490148209
Experience 20, Iter 31, disc loss: 0.0002892736516361655, policy loss: 8.45302669738584
Experience 20, Iter 32, disc loss: 0.00023335224006128426, policy loss: 8.837862280616113
Experience 20, Iter 33, disc loss: 0.00026869544774247927, policy loss: 8.52209993220426
Experience 20, Iter 34, disc loss: 0.0002630832290776612, policy loss: 8.672347908704477
Experience 20, Iter 35, disc loss: 0.00024246337482974317, policy loss: 8.9368357049207
Experience 20, Iter 36, disc loss: 0.00023898029569338757, policy loss: 8.795319822505563
Experience 20, Iter 37, disc loss: 0.0002634912628380606, policy loss: 8.77754544260426
Experience 20, Iter 38, disc loss: 0.0003137152043324612, policy loss: 8.752448046074367
Experience 20, Iter 39, disc loss: 0.00027207238932088623, policy loss: 8.502429309827289
Experience 20, Iter 40, disc loss: 0.00033776251213056465, policy loss: 8.359000915926309
Experience 20, Iter 41, disc loss: 0.00023941062183821112, policy loss: 8.760470540911799
Experience 20, Iter 42, disc loss: 0.0002697718513605566, policy loss: 8.623720138691546
Experience 20, Iter 43, disc loss: 0.00026911082261811515, policy loss: 8.641123696086172
Experience 20, Iter 44, disc loss: 0.0002754923173968489, policy loss: 8.636000935068456
Experience 20, Iter 45, disc loss: 0.0002796243660598006, policy loss: 8.683953523245
Experience 20, Iter 46, disc loss: 0.0003063406558219832, policy loss: 8.525409733266347
Experience 20, Iter 47, disc loss: 0.0002995589835732707, policy loss: 8.611368447939505
Experience 20, Iter 48, disc loss: 0.00026304073122463583, policy loss: 8.700487391837955
Experience 20, Iter 49, disc loss: 0.0002764837421065944, policy loss: 8.5292075913516
Experience 20, Iter 50, disc loss: 0.00023521442535516635, policy loss: 8.903309142807544
Experience 20, Iter 51, disc loss: 0.00023598343491165917, policy loss: 8.839968579148719
Experience 20, Iter 52, disc loss: 0.00026433360378630687, policy loss: 8.70730627777657
Experience 20, Iter 53, disc loss: 0.00029206860366312603, policy loss: 8.546082029696766
Experience 20, Iter 54, disc loss: 0.00023464671325673954, policy loss: 8.85007464776692
Experience 20, Iter 55, disc loss: 0.00023027334897463093, policy loss: 8.801742140650752
Experience 20, Iter 56, disc loss: 0.00026673648584542574, policy loss: 8.66221132463993
Experience 20, Iter 57, disc loss: 0.00022548923077783794, policy loss: 8.844735569066913
Experience 20, Iter 58, disc loss: 0.0002482531519126336, policy loss: 8.725951847311098
Experience 20, Iter 59, disc loss: 0.0002327433161697672, policy loss: 8.80660066837093
Experience 20, Iter 60, disc loss: 0.00025314967329942694, policy loss: 8.683421195897047
Experience 20, Iter 61, disc loss: 0.00023449630651483006, policy loss: 8.744106494987303
Experience 20, Iter 62, disc loss: 0.0002738299629753275, policy loss: 8.66880296049009
Experience 20, Iter 63, disc loss: 0.00022715596675434066, policy loss: 8.851531116634012
Experience 20, Iter 64, disc loss: 0.00023342698298085693, policy loss: 8.767417158575098
Experience 20, Iter 65, disc loss: 0.0002397671109504034, policy loss: 8.840590581332773
Experience 20, Iter 66, disc loss: 0.0002807541811040601, policy loss: 8.632547467921405
Experience 20, Iter 67, disc loss: 0.00023369783948366959, policy loss: 8.759418654676205
Experience 20, Iter 68, disc loss: 0.0002334808704855442, policy loss: 8.832360018056153
Experience 20, Iter 69, disc loss: 0.0002467474281289937, policy loss: 8.779990636877086
Experience 20, Iter 70, disc loss: 0.0002607935073834364, policy loss: 8.65509398806164
Experience 20, Iter 71, disc loss: 0.00026000713763556666, policy loss: 8.657355106282756
Experience 20, Iter 72, disc loss: 0.00022803796148868899, policy loss: 8.807335496769134
Experience 20, Iter 73, disc loss: 0.00025900769254270814, policy loss: 8.701589511220828
Experience 20, Iter 74, disc loss: 0.00022047751619996164, policy loss: 8.817300900111492
Experience 20, Iter 75, disc loss: 0.00026847964029718074, policy loss: 8.71269282966443
Experience 20, Iter 76, disc loss: 0.0002559515608807094, policy loss: 8.702525000707146
Experience 20, Iter 77, disc loss: 0.00028110783017628194, policy loss: 8.753764916396012
Experience 20, Iter 78, disc loss: 0.0002725332972079869, policy loss: 8.675817497721896
Experience 20, Iter 79, disc loss: 0.00023278330384945032, policy loss: 8.871408841875002
Experience 20, Iter 80, disc loss: 0.0002416060343753435, policy loss: 8.877264419514928
Experience 20, Iter 81, disc loss: 0.0002554952973961292, policy loss: 8.682528605236136
Experience 20, Iter 82, disc loss: 0.00026680490041539257, policy loss: 8.686772403763271
Experience 20, Iter 83, disc loss: 0.0002402072150600911, policy loss: 8.829632310005358
Experience 20, Iter 84, disc loss: 0.00032181899761288274, policy loss: 8.4730411385415
Experience 20, Iter 85, disc loss: 0.0002516663133110421, policy loss: 8.696188639295546
Experience 20, Iter 86, disc loss: 0.0002430158723757261, policy loss: 8.731661780915491
Experience 20, Iter 87, disc loss: 0.000294595894592043, policy loss: 8.59914504337726
Experience 20, Iter 88, disc loss: 0.0002718685369972202, policy loss: 8.632966268550168
Experience 20, Iter 89, disc loss: 0.0002358105009612436, policy loss: 8.725303527584595
Experience 20, Iter 90, disc loss: 0.00027395320642856485, policy loss: 8.900004556538981
Experience 20, Iter 91, disc loss: 0.00024237634662590562, policy loss: 8.755161606248048
Experience 20, Iter 92, disc loss: 0.0002474064732768903, policy loss: 8.843261041124062
Experience 20, Iter 93, disc loss: 0.0002426964783440433, policy loss: 8.601197100101045
Experience 20, Iter 94, disc loss: 0.00025905715832967016, policy loss: 8.75313977406747
Experience 20, Iter 95, disc loss: 0.00026576825914441165, policy loss: 8.69150945504812
Experience 20, Iter 96, disc loss: 0.00024745118756774095, policy loss: 8.877190606128718
Experience 20, Iter 97, disc loss: 0.00021842420978164333, policy loss: 8.892560788815523
Experience 20, Iter 98, disc loss: 0.00027534777750698973, policy loss: 8.633315443768574
Experience 20, Iter 99, disc loss: 0.0002397622550770438, policy loss: 8.81839234755722
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.0180],
        [0.2446],
        [0.0047]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0281, 0.1764, 0.2165, 0.0063, 0.0025, 0.6786]],

        [[0.0281, 0.1764, 0.2165, 0.0063, 0.0025, 0.6786]],

        [[0.0281, 0.1764, 0.2165, 0.0063, 0.0025, 0.6786]],

        [[0.0281, 0.1764, 0.2165, 0.0063, 0.0025, 0.6786]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0192, 0.0722, 0.9784, 0.0189], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0192, 0.0722, 0.9784, 0.0189])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.411
Iter 2/2000 - Loss: 0.454
Iter 3/2000 - Loss: 0.236
Iter 4/2000 - Loss: 0.172
Iter 5/2000 - Loss: 0.157
Iter 6/2000 - Loss: 0.044
Iter 7/2000 - Loss: -0.129
Iter 8/2000 - Loss: -0.299
Iter 9/2000 - Loss: -0.457
Iter 10/2000 - Loss: -0.628
Iter 11/2000 - Loss: -0.832
Iter 12/2000 - Loss: -1.075
Iter 13/2000 - Loss: -1.349
Iter 14/2000 - Loss: -1.641
Iter 15/2000 - Loss: -1.943
Iter 16/2000 - Loss: -2.250
Iter 17/2000 - Loss: -2.562
Iter 18/2000 - Loss: -2.877
Iter 19/2000 - Loss: -3.192
Iter 20/2000 - Loss: -3.504
Iter 1981/2000 - Loss: -8.483
Iter 1982/2000 - Loss: -8.483
Iter 1983/2000 - Loss: -8.483
Iter 1984/2000 - Loss: -8.483
Iter 1985/2000 - Loss: -8.483
Iter 1986/2000 - Loss: -8.483
Iter 1987/2000 - Loss: -8.483
Iter 1988/2000 - Loss: -8.483
Iter 1989/2000 - Loss: -8.483
Iter 1990/2000 - Loss: -8.483
Iter 1991/2000 - Loss: -8.483
Iter 1992/2000 - Loss: -8.483
Iter 1993/2000 - Loss: -8.483
Iter 1994/2000 - Loss: -8.483
Iter 1995/2000 - Loss: -8.483
Iter 1996/2000 - Loss: -8.483
Iter 1997/2000 - Loss: -8.483
Iter 1998/2000 - Loss: -8.483
Iter 1999/2000 - Loss: -8.484
Iter 2000/2000 - Loss: -8.484
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[12.1814,  5.6167, 24.4249,  9.8829, 17.8020, 35.6738]],

        [[17.4213, 31.0051, 11.4879,  1.5086,  1.2253, 16.6766]],

        [[19.1865, 36.0725, 17.9472,  1.0120,  1.7455, 17.6624]],

        [[16.4997, 34.4761, 17.4494,  3.7036,  2.6884, 43.7030]]])
Signal Variance: tensor([ 0.0966,  0.8567, 13.6150,  0.4520])
Estimated target variance: tensor([0.0192, 0.0722, 0.9784, 0.0189])
N: 210
Signal to noise ratio: tensor([17.3344, 46.7462, 70.7737, 38.5703])
Bound on condition number: tensor([  63102.4593,  458894.1084, 1051872.7453,  312410.9263])
Policy Optimizer learning rate:
0.09791483623609773
Experience 21, Iter 0, disc loss: 0.0002136018870997383, policy loss: 8.936683753410962
Experience 21, Iter 1, disc loss: 0.0002334709526709079, policy loss: 8.76005609753393
Experience 21, Iter 2, disc loss: 0.00023626282241558398, policy loss: 8.78737963590692
Experience 21, Iter 3, disc loss: 0.0002483227555268764, policy loss: 8.7141990035971
Experience 21, Iter 4, disc loss: 0.00023541788644047338, policy loss: 8.78802642892949
Experience 21, Iter 5, disc loss: 0.00024576572610412816, policy loss: 8.807933792953486
Experience 21, Iter 6, disc loss: 0.0002845401279374935, policy loss: 8.673975597406228
Experience 21, Iter 7, disc loss: 0.00021839951725864977, policy loss: 8.888665590636439
Experience 21, Iter 8, disc loss: 0.00021647552228188388, policy loss: 8.84401013389721
Experience 21, Iter 9, disc loss: 0.000233313666343967, policy loss: 8.727073470445275
Experience 21, Iter 10, disc loss: 0.00025362488398429325, policy loss: 8.695632775103453
Experience 21, Iter 11, disc loss: 0.00022521166108594123, policy loss: 8.893221605269943
Experience 21, Iter 12, disc loss: 0.0002210787731825564, policy loss: 8.882415553781545
Experience 21, Iter 13, disc loss: 0.0002368567394516289, policy loss: 8.891180645368912
Experience 21, Iter 14, disc loss: 0.00025729794016647134, policy loss: 8.621893044759531
Experience 21, Iter 15, disc loss: 0.00025097870981453355, policy loss: 8.64632689629605
Experience 21, Iter 16, disc loss: 0.0002277906527833518, policy loss: 8.813223496538441
Experience 21, Iter 17, disc loss: 0.00023826874908539, policy loss: 8.801343850863745
Experience 21, Iter 18, disc loss: 0.00025327890933808217, policy loss: 8.68729584532986
Experience 21, Iter 19, disc loss: 0.00022550464777211057, policy loss: 8.83405799705923
Experience 21, Iter 20, disc loss: 0.0002908915606773388, policy loss: 8.613048280968751
Experience 21, Iter 21, disc loss: 0.0002861923117624297, policy loss: 8.591014700117313
Experience 21, Iter 22, disc loss: 0.0002230775406326904, policy loss: 8.93778257603047
Experience 21, Iter 23, disc loss: 0.00027259882846879225, policy loss: 8.5890937561215
Experience 21, Iter 24, disc loss: 0.00028050170906710885, policy loss: 8.666815356127987
Experience 21, Iter 25, disc loss: 0.00022812093007393606, policy loss: 8.849433456686157
Experience 21, Iter 26, disc loss: 0.00023558457633115673, policy loss: 8.868982282010968
Experience 21, Iter 27, disc loss: 0.00023776481477525904, policy loss: 8.955640548229823
Experience 21, Iter 28, disc loss: 0.0002345765315072762, policy loss: 8.827440428848867
Experience 21, Iter 29, disc loss: 0.00024810582236771883, policy loss: 8.74654636993459
Experience 21, Iter 30, disc loss: 0.00020279652727329896, policy loss: 8.939307930196279
Experience 21, Iter 31, disc loss: 0.00023752619864259099, policy loss: 8.75852701996606
Experience 21, Iter 32, disc loss: 0.00024125723049483384, policy loss: 8.844309540220975
Experience 21, Iter 33, disc loss: 0.0002559439213617171, policy loss: 8.61747322865402
Experience 21, Iter 34, disc loss: 0.00019296860632687945, policy loss: 9.116059563227292
Experience 21, Iter 35, disc loss: 0.0002097791450937132, policy loss: 8.99516800269226
Experience 21, Iter 36, disc loss: 0.00021478879052935255, policy loss: 8.890060877663984
Experience 21, Iter 37, disc loss: 0.00023575150359019648, policy loss: 8.821189073956027
Experience 21, Iter 38, disc loss: 0.00022235726059452278, policy loss: 8.817247145595005
Experience 21, Iter 39, disc loss: 0.00020979402002860293, policy loss: 8.934551291551827
Experience 21, Iter 40, disc loss: 0.0002543323209888235, policy loss: 8.697157789606738
Experience 21, Iter 41, disc loss: 0.0002653545187037597, policy loss: 8.644411704801698
Experience 21, Iter 42, disc loss: 0.00023749244013699495, policy loss: 8.843355236042433
Experience 21, Iter 43, disc loss: 0.0002271341068201997, policy loss: 8.78992770392359
Experience 21, Iter 44, disc loss: 0.00023333984609736702, policy loss: 8.868029114967642
Experience 21, Iter 45, disc loss: 0.00024498475659174825, policy loss: 8.72579395756932
Experience 21, Iter 46, disc loss: 0.0002176287132123088, policy loss: 8.992116894923287
Experience 21, Iter 47, disc loss: 0.0002526438931500681, policy loss: 8.762266756275759
Experience 21, Iter 48, disc loss: 0.00025018509005127844, policy loss: 8.622585899084907
Experience 21, Iter 49, disc loss: 0.00023507926226654747, policy loss: 8.867904353326432
Experience 21, Iter 50, disc loss: 0.000288186601345963, policy loss: 8.62554858254654
Experience 21, Iter 51, disc loss: 0.00025496849540301717, policy loss: 8.735662222736629
Experience 21, Iter 52, disc loss: 0.00021711347413037166, policy loss: 9.015568300867356
Experience 21, Iter 53, disc loss: 0.00024142190053346737, policy loss: 8.675893334987617
Experience 21, Iter 54, disc loss: 0.00019549772099052286, policy loss: 9.044831582725871
Experience 21, Iter 55, disc loss: 0.00024552306944181746, policy loss: 8.784217682440472
Experience 21, Iter 56, disc loss: 0.00024086994907947774, policy loss: 8.69373433600681
Experience 21, Iter 57, disc loss: 0.00021414981618788219, policy loss: 8.862244713449076
Experience 21, Iter 58, disc loss: 0.00025475106718510763, policy loss: 8.715637370333898
Experience 21, Iter 59, disc loss: 0.00025307702507697765, policy loss: 8.802621726782963
Experience 21, Iter 60, disc loss: 0.0002379399503100664, policy loss: 8.812935300343185
Experience 21, Iter 61, disc loss: 0.00028247564074253826, policy loss: 8.657017403749126
Experience 21, Iter 62, disc loss: 0.0002466968442047749, policy loss: 8.667420406524338
Experience 21, Iter 63, disc loss: 0.0002451403788325593, policy loss: 8.7398068112985
Experience 21, Iter 64, disc loss: 0.00022691015102246977, policy loss: 8.788493646632595
Experience 21, Iter 65, disc loss: 0.0003347259144199168, policy loss: 8.524216071836964
Experience 21, Iter 66, disc loss: 0.00020200485305234332, policy loss: 9.038048184984897
Experience 21, Iter 67, disc loss: 0.00023665453297987067, policy loss: 8.697239351474526
Experience 21, Iter 68, disc loss: 0.000232735118347442, policy loss: 8.823212567714254
Experience 21, Iter 69, disc loss: 0.00021414936178077871, policy loss: 8.897533027789082
Experience 21, Iter 70, disc loss: 0.00024658292193878797, policy loss: 8.694516787782119
Experience 21, Iter 71, disc loss: 0.00025488774284786544, policy loss: 8.722126962879692
Experience 21, Iter 72, disc loss: 0.0002186855470023317, policy loss: 8.76293963051414
Experience 21, Iter 73, disc loss: 0.000263487163368083, policy loss: 8.888817270440528
Experience 21, Iter 74, disc loss: 0.0002559951891714961, policy loss: 8.636040163345509
Experience 21, Iter 75, disc loss: 0.00023319054319043256, policy loss: 8.993751367872019
Experience 21, Iter 76, disc loss: 0.0002211605999233224, policy loss: 8.895457442153297
Experience 21, Iter 77, disc loss: 0.00023961667609944849, policy loss: 8.83604748798421
Experience 21, Iter 78, disc loss: 0.0002350978390751125, policy loss: 8.683368108754935
Experience 21, Iter 79, disc loss: 0.0002574316720524794, policy loss: 8.690729826576362
Experience 21, Iter 80, disc loss: 0.000218806598757066, policy loss: 8.842123257649384
Experience 21, Iter 81, disc loss: 0.0002355864690619101, policy loss: 8.790867776077922
Experience 21, Iter 82, disc loss: 0.00020866759508327712, policy loss: 8.910439462414141
Experience 21, Iter 83, disc loss: 0.0001876528505538107, policy loss: 9.070027270869524
Experience 21, Iter 84, disc loss: 0.00020394922058660831, policy loss: 9.044437955631041
Experience 21, Iter 85, disc loss: 0.0002174852882913389, policy loss: 8.942956891133422
Experience 21, Iter 86, disc loss: 0.0002411028576700038, policy loss: 8.898911715425186
Experience 21, Iter 87, disc loss: 0.00021062043958170834, policy loss: 8.967719188726846
Experience 21, Iter 88, disc loss: 0.00019568813608128447, policy loss: 8.984295199704071
Experience 21, Iter 89, disc loss: 0.00022769116327369204, policy loss: 8.76704723979689
Experience 21, Iter 90, disc loss: 0.0002183171642218867, policy loss: 8.916225283785199
Experience 21, Iter 91, disc loss: 0.000213654947552559, policy loss: 8.854501910422446
Experience 21, Iter 92, disc loss: 0.00020363999926118392, policy loss: 8.919707044494237
Experience 21, Iter 93, disc loss: 0.000192877321007006, policy loss: 9.06447992638233
Experience 21, Iter 94, disc loss: 0.00022025898568368688, policy loss: 8.923812450539828
Experience 21, Iter 95, disc loss: 0.00020114884358870972, policy loss: 9.055784697064324
Experience 21, Iter 96, disc loss: 0.00024788990864955463, policy loss: 8.769567220238761
Experience 21, Iter 97, disc loss: 0.0002520517580427537, policy loss: 8.645812668872374
Experience 21, Iter 98, disc loss: 0.00025449520405312594, policy loss: 8.770333576869266
Experience 21, Iter 99, disc loss: 0.00023599221399394374, policy loss: 8.951775870366557
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.0173],
        [0.2353],
        [0.0045]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0274, 0.1690, 0.2081, 0.0061, 0.0024, 0.6510]],

        [[0.0274, 0.1690, 0.2081, 0.0061, 0.0024, 0.6510]],

        [[0.0274, 0.1690, 0.2081, 0.0061, 0.0024, 0.6510]],

        [[0.0274, 0.1690, 0.2081, 0.0061, 0.0024, 0.6510]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0184, 0.0694, 0.9412, 0.0181], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0184, 0.0694, 0.9412, 0.0181])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.340
Iter 2/2000 - Loss: 0.413
Iter 3/2000 - Loss: 0.183
Iter 4/2000 - Loss: 0.126
Iter 5/2000 - Loss: 0.125
Iter 6/2000 - Loss: 0.017
Iter 7/2000 - Loss: -0.151
Iter 8/2000 - Loss: -0.310
Iter 9/2000 - Loss: -0.457
Iter 10/2000 - Loss: -0.619
Iter 11/2000 - Loss: -0.816
Iter 12/2000 - Loss: -1.052
Iter 13/2000 - Loss: -1.318
Iter 14/2000 - Loss: -1.604
Iter 15/2000 - Loss: -1.904
Iter 16/2000 - Loss: -2.213
Iter 17/2000 - Loss: -2.530
Iter 18/2000 - Loss: -2.852
Iter 19/2000 - Loss: -3.176
Iter 20/2000 - Loss: -3.498
Iter 1981/2000 - Loss: -8.524
Iter 1982/2000 - Loss: -8.524
Iter 1983/2000 - Loss: -8.524
Iter 1984/2000 - Loss: -8.524
Iter 1985/2000 - Loss: -8.524
Iter 1986/2000 - Loss: -8.524
Iter 1987/2000 - Loss: -8.524
Iter 1988/2000 - Loss: -8.525
Iter 1989/2000 - Loss: -8.525
Iter 1990/2000 - Loss: -8.525
Iter 1991/2000 - Loss: -8.525
Iter 1992/2000 - Loss: -8.525
Iter 1993/2000 - Loss: -8.525
Iter 1994/2000 - Loss: -8.525
Iter 1995/2000 - Loss: -8.525
Iter 1996/2000 - Loss: -8.525
Iter 1997/2000 - Loss: -8.525
Iter 1998/2000 - Loss: -8.525
Iter 1999/2000 - Loss: -8.525
Iter 2000/2000 - Loss: -8.525
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[12.0213,  5.2864, 24.0246,  9.7061, 17.5438, 35.4080]],

        [[17.1667, 31.1061, 11.4119,  1.4971,  1.2351, 16.8025]],

        [[18.8814, 36.1907, 17.8544,  1.0124,  1.7195, 18.0650]],

        [[16.2726, 33.3739, 17.3684,  3.6952,  2.7273, 43.3474]]])
Signal Variance: tensor([ 0.0912,  0.8455, 13.6226,  0.4440])
Estimated target variance: tensor([0.0184, 0.0694, 0.9412, 0.0181])
N: 220
Signal to noise ratio: tensor([17.0546, 46.6657, 70.6719, 38.0489])
Bound on condition number: tensor([  63990.0737,  479092.2508, 1098795.9079,  318498.6013])
Policy Optimizer learning rate:
0.0978117269874902
Experience 22, Iter 0, disc loss: 0.00021876027562668662, policy loss: 8.93548016218452
Experience 22, Iter 1, disc loss: 0.00022858870841948957, policy loss: 8.779279821929237
Experience 22, Iter 2, disc loss: 0.00024348606327551184, policy loss: 8.693048190515059
Experience 22, Iter 3, disc loss: 0.00020783564990987694, policy loss: 8.889644276507351
Experience 22, Iter 4, disc loss: 0.0001907909346755482, policy loss: 8.97745924570588
Experience 22, Iter 5, disc loss: 0.0001907018018609014, policy loss: 8.995349214788046
Experience 22, Iter 6, disc loss: 0.0002178710103486348, policy loss: 8.938871568767963
Experience 22, Iter 7, disc loss: 0.00021744775587993471, policy loss: 8.926224913108236
Experience 22, Iter 8, disc loss: 0.00022083950136422243, policy loss: 8.869739029659138
Experience 22, Iter 9, disc loss: 0.0002538936150065527, policy loss: 8.80070563996166
Experience 22, Iter 10, disc loss: 0.00023388626264465336, policy loss: 8.859441181324899
Experience 22, Iter 11, disc loss: 0.0001886455933635954, policy loss: 8.993329486479379
Experience 22, Iter 12, disc loss: 0.00021636130783698202, policy loss: 8.819634848284515
Experience 22, Iter 13, disc loss: 0.0001985001709895616, policy loss: 8.989469922443908
Experience 22, Iter 14, disc loss: 0.0002234366716727814, policy loss: 8.79338493227663
Experience 22, Iter 15, disc loss: 0.00018931812226367348, policy loss: 8.916886376372881
Experience 22, Iter 16, disc loss: 0.00019928301913362475, policy loss: 8.895594204511145
Experience 22, Iter 17, disc loss: 0.0001878108402706275, policy loss: 9.043939211277824
Experience 22, Iter 18, disc loss: 0.00021073657836757833, policy loss: 8.85448013875406
Experience 22, Iter 19, disc loss: 0.0001779294131563488, policy loss: 9.10362101997933
Experience 22, Iter 20, disc loss: 0.00019797093860942803, policy loss: 9.021179865980901
Experience 22, Iter 21, disc loss: 0.0002448094903895482, policy loss: 8.663389026488126
Experience 22, Iter 22, disc loss: 0.00019751487303818643, policy loss: 8.939823309350967
Experience 22, Iter 23, disc loss: 0.00021393262049282768, policy loss: 8.867688119647859
Experience 22, Iter 24, disc loss: 0.0002369241511193236, policy loss: 8.802789248139252
Experience 22, Iter 25, disc loss: 0.00018125736114994607, policy loss: 9.080174104199198
Experience 22, Iter 26, disc loss: 0.00019908478860045271, policy loss: 9.049112502163236
Experience 22, Iter 27, disc loss: 0.00022321186096194872, policy loss: 8.705823771505084
Experience 22, Iter 28, disc loss: 0.00019931529855414718, policy loss: 9.106299377106822
Experience 22, Iter 29, disc loss: 0.0002122439016810666, policy loss: 8.893028415520702
Experience 22, Iter 30, disc loss: 0.0002380785868276542, policy loss: 8.658520472146291
Experience 22, Iter 31, disc loss: 0.00021281175123913606, policy loss: 8.841035927018757
Experience 22, Iter 32, disc loss: 0.00021400974868191408, policy loss: 8.856289073578402
Experience 22, Iter 33, disc loss: 0.0002055458253555559, policy loss: 8.876164852880365
Experience 22, Iter 34, disc loss: 0.00020309959341081966, policy loss: 8.86254986846591
Experience 22, Iter 35, disc loss: 0.0001900162368521757, policy loss: 9.044602606399607
Experience 22, Iter 36, disc loss: 0.00020807731833348808, policy loss: 8.842814523396253
Experience 22, Iter 37, disc loss: 0.0002223386293504858, policy loss: 8.7782427965044
Experience 22, Iter 38, disc loss: 0.00022966953409244665, policy loss: 8.8595770044302
Experience 22, Iter 39, disc loss: 0.00022156685213055354, policy loss: 8.794992155556628
Experience 22, Iter 40, disc loss: 0.00020737492805121037, policy loss: 9.017997310798904
Experience 22, Iter 41, disc loss: 0.0002069918935509849, policy loss: 9.043426017235587
Experience 22, Iter 42, disc loss: 0.00019364648693753788, policy loss: 8.979368989360818
Experience 22, Iter 43, disc loss: 0.0002098868318758474, policy loss: 9.007783618924448
Experience 22, Iter 44, disc loss: 0.0002097213430564053, policy loss: 8.836087655216106
Experience 22, Iter 45, disc loss: 0.0001796749503653766, policy loss: 9.043650680254483
Experience 22, Iter 46, disc loss: 0.00024273343129772737, policy loss: 8.95643977558348
Experience 22, Iter 47, disc loss: 0.00021194680687463993, policy loss: 8.788655035648738
Experience 22, Iter 48, disc loss: 0.0001993533873844208, policy loss: 8.831186160368858
Experience 22, Iter 49, disc loss: 0.00018874322425947905, policy loss: 9.064600203221122
Experience 22, Iter 50, disc loss: 0.00018887813694244143, policy loss: 9.048487676936174
Experience 22, Iter 51, disc loss: 0.00024029522040247659, policy loss: 8.70617085354468
Experience 22, Iter 52, disc loss: 0.0001737516300513797, policy loss: 9.090456462507843
Experience 22, Iter 53, disc loss: 0.0002016386279369561, policy loss: 8.891334355575037
Experience 22, Iter 54, disc loss: 0.00022655496009169661, policy loss: 8.819463244311166
Experience 22, Iter 55, disc loss: 0.00019712827977315045, policy loss: 8.944795187506335
Experience 22, Iter 56, disc loss: 0.00019744670324431152, policy loss: 8.96540763152743
Experience 22, Iter 57, disc loss: 0.0001966216852764349, policy loss: 8.987093742021619
Experience 22, Iter 58, disc loss: 0.00017052149652666291, policy loss: 9.192671691985474
Experience 22, Iter 59, disc loss: 0.00018969264203546392, policy loss: 9.149870283038172
Experience 22, Iter 60, disc loss: 0.00024244951160830772, policy loss: 8.909997192302981
Experience 22, Iter 61, disc loss: 0.0001975296735751488, policy loss: 8.916999180102676
Experience 22, Iter 62, disc loss: 0.00019452267413296023, policy loss: 9.053828366844208
Experience 22, Iter 63, disc loss: 0.00018435898273839882, policy loss: 9.023736707442232
Experience 22, Iter 64, disc loss: 0.00020917148604774247, policy loss: 8.977123769839814
Experience 22, Iter 65, disc loss: 0.0001916182923823708, policy loss: 8.96291797599654
Experience 22, Iter 66, disc loss: 0.0002120150375657964, policy loss: 8.859456976218713
Experience 22, Iter 67, disc loss: 0.00021232153788201783, policy loss: 8.94585283076464
Experience 22, Iter 68, disc loss: 0.00017575855257593516, policy loss: 9.070476422989955
Experience 22, Iter 69, disc loss: 0.00018710886495489955, policy loss: 9.017281789014849
Experience 22, Iter 70, disc loss: 0.00019793773563901577, policy loss: 8.903245373300756
Experience 22, Iter 71, disc loss: 0.00019644562450378794, policy loss: 8.910060271292522
Experience 22, Iter 72, disc loss: 0.00017594121738552726, policy loss: 9.110493918908627
Experience 22, Iter 73, disc loss: 0.00020782831411668008, policy loss: 8.941709714192774
Experience 22, Iter 74, disc loss: 0.00021036035423492564, policy loss: 8.842799596919813
Experience 22, Iter 75, disc loss: 0.0002222434823445877, policy loss: 8.948083687701487
Experience 22, Iter 76, disc loss: 0.00017990555886957915, policy loss: 9.02235912393982
Experience 22, Iter 77, disc loss: 0.00021113414629948162, policy loss: 9.075385353573884
Experience 22, Iter 78, disc loss: 0.00021964447276374128, policy loss: 8.78580414583806
Experience 22, Iter 79, disc loss: 0.0001794089892984596, policy loss: 9.125720353048122
Experience 22, Iter 80, disc loss: 0.00022793057853408503, policy loss: 8.98403468912791
Experience 22, Iter 81, disc loss: 0.00017738530359245655, policy loss: 9.270415926837103
Experience 22, Iter 82, disc loss: 0.00022835733250947164, policy loss: 8.820947131653153
Experience 22, Iter 83, disc loss: 0.0002773515531785462, policy loss: 9.027978004860627
Experience 22, Iter 84, disc loss: 0.00017167961179336477, policy loss: 9.188796127919002
Experience 22, Iter 85, disc loss: 0.0001736767863453685, policy loss: 8.9998602924273
Experience 22, Iter 86, disc loss: 0.00018352147451613588, policy loss: 8.970735770969194
Experience 22, Iter 87, disc loss: 0.0001767166701693163, policy loss: 9.1698484212049
Experience 22, Iter 88, disc loss: 0.0001791663405342928, policy loss: 9.056713847101914
Experience 22, Iter 89, disc loss: 0.00019744608783681388, policy loss: 8.943946881913408
Experience 22, Iter 90, disc loss: 0.00018250605112416748, policy loss: 8.983690278101871
Experience 22, Iter 91, disc loss: 0.00018747873250607198, policy loss: 9.009083672717438
Experience 22, Iter 92, disc loss: 0.00017965760211100472, policy loss: 9.041646823387527
Experience 22, Iter 93, disc loss: 0.0001825480872554042, policy loss: 9.11083511833974
Experience 22, Iter 94, disc loss: 0.00015831103586302626, policy loss: 9.389362299081087
Experience 22, Iter 95, disc loss: 0.00016133851727829728, policy loss: 9.239587997178441
Experience 22, Iter 96, disc loss: 0.00016783356684228046, policy loss: 9.188782672461485
Experience 22, Iter 97, disc loss: 0.00022125239281011182, policy loss: 8.877636478903703
Experience 22, Iter 98, disc loss: 0.00021822608380646897, policy loss: 8.910113226169758
Experience 22, Iter 99, disc loss: 0.00017889832775884717, policy loss: 9.027157666204136
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0044],
        [0.0167],
        [0.2251],
        [0.0043]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0262, 0.1620, 0.1991, 0.0058, 0.0023, 0.6259]],

        [[0.0262, 0.1620, 0.1991, 0.0058, 0.0023, 0.6259]],

        [[0.0262, 0.1620, 0.1991, 0.0058, 0.0023, 0.6259]],

        [[0.0262, 0.1620, 0.1991, 0.0058, 0.0023, 0.6259]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0177, 0.0667, 0.9005, 0.0174], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0177, 0.0667, 0.9005, 0.0174])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.243
Iter 2/2000 - Loss: 0.326
Iter 3/2000 - Loss: 0.085
Iter 4/2000 - Loss: 0.024
Iter 5/2000 - Loss: 0.023
Iter 6/2000 - Loss: -0.090
Iter 7/2000 - Loss: -0.263
Iter 8/2000 - Loss: -0.427
Iter 9/2000 - Loss: -0.578
Iter 10/2000 - Loss: -0.742
Iter 11/2000 - Loss: -0.940
Iter 12/2000 - Loss: -1.175
Iter 13/2000 - Loss: -1.440
Iter 14/2000 - Loss: -1.725
Iter 15/2000 - Loss: -2.023
Iter 16/2000 - Loss: -2.332
Iter 17/2000 - Loss: -2.648
Iter 18/2000 - Loss: -2.970
Iter 19/2000 - Loss: -3.294
Iter 20/2000 - Loss: -3.615
Iter 1981/2000 - Loss: -8.577
Iter 1982/2000 - Loss: -8.577
Iter 1983/2000 - Loss: -8.577
Iter 1984/2000 - Loss: -8.577
Iter 1985/2000 - Loss: -8.577
Iter 1986/2000 - Loss: -8.577
Iter 1987/2000 - Loss: -8.577
Iter 1988/2000 - Loss: -8.577
Iter 1989/2000 - Loss: -8.577
Iter 1990/2000 - Loss: -8.577
Iter 1991/2000 - Loss: -8.577
Iter 1992/2000 - Loss: -8.577
Iter 1993/2000 - Loss: -8.577
Iter 1994/2000 - Loss: -8.577
Iter 1995/2000 - Loss: -8.578
Iter 1996/2000 - Loss: -8.578
Iter 1997/2000 - Loss: -8.578
Iter 1998/2000 - Loss: -8.578
Iter 1999/2000 - Loss: -8.578
Iter 2000/2000 - Loss: -8.578
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[11.8425,  5.2536, 23.6591,  9.5806, 17.5242, 35.3436]],

        [[16.8303, 30.1336, 11.3598,  1.4853,  1.2356, 16.7121]],

        [[18.5286, 35.2476, 17.8078,  1.0084,  1.6966, 17.9593]],

        [[16.0162, 32.2770, 17.2650,  3.6892,  2.7667, 43.0871]]])
Signal Variance: tensor([ 0.0904,  0.8310, 13.4335,  0.4385])
Estimated target variance: tensor([0.0177, 0.0667, 0.9005, 0.0174])
N: 230
Signal to noise ratio: tensor([17.1773, 46.5532, 71.2399, 37.4852])
Bound on condition number: tensor([  67864.5369,  498457.9919, 1167277.8681,  323183.4729])
Policy Optimizer learning rate:
0.09770872631810884
Experience 23, Iter 0, disc loss: 0.0002140553526087534, policy loss: 8.93657284414735
Experience 23, Iter 1, disc loss: 0.00020673048680426885, policy loss: 8.875708356312593
Experience 23, Iter 2, disc loss: 0.00019873775442849962, policy loss: 9.00207662718444
Experience 23, Iter 3, disc loss: 0.00019365686280214096, policy loss: 8.921864974584514
Experience 23, Iter 4, disc loss: 0.00019614902144526904, policy loss: 9.064266619626135
Experience 23, Iter 5, disc loss: 0.00017233772373333628, policy loss: 9.197342904614658
Experience 23, Iter 6, disc loss: 0.00017034276013713182, policy loss: 9.110450217059608
Experience 23, Iter 7, disc loss: 0.0001887681801427951, policy loss: 9.11199757824182
Experience 23, Iter 8, disc loss: 0.00019206802469938196, policy loss: 8.941480213935737
Experience 23, Iter 9, disc loss: 0.00018583400488180577, policy loss: 9.053914843732981
Experience 23, Iter 10, disc loss: 0.00022232361150347955, policy loss: 8.767778911246936
Experience 23, Iter 11, disc loss: 0.00020752614530620905, policy loss: 8.91534194065705
Experience 23, Iter 12, disc loss: 0.0001707113770948236, policy loss: 9.152905586602724
Experience 23, Iter 13, disc loss: 0.00016510372962535698, policy loss: 9.223993131893781
Experience 23, Iter 14, disc loss: 0.00019134320740399997, policy loss: 8.962001482079682
Experience 23, Iter 15, disc loss: 0.0001985046651159894, policy loss: 8.979841640703027
Experience 23, Iter 16, disc loss: 0.00018647665769110645, policy loss: 8.98267493808457
Experience 23, Iter 17, disc loss: 0.0001757586265659863, policy loss: 9.17150054527446
Experience 23, Iter 18, disc loss: 0.00016948790832243648, policy loss: 9.188081415702325
Experience 23, Iter 19, disc loss: 0.00022553381512844818, policy loss: 8.820169682240412
Experience 23, Iter 20, disc loss: 0.00019448548386353636, policy loss: 8.984799745827205
Experience 23, Iter 21, disc loss: 0.0001890717615883896, policy loss: 8.90702155635141
Experience 23, Iter 22, disc loss: 0.00019540983340271567, policy loss: 9.006502689318928
Experience 23, Iter 23, disc loss: 0.00019852334393368796, policy loss: 9.03031633873645
Experience 23, Iter 24, disc loss: 0.0002217271231989035, policy loss: 8.842114780660685
Experience 23, Iter 25, disc loss: 0.00018916863047190003, policy loss: 9.143229656900452
Experience 23, Iter 26, disc loss: 0.00022568884862167544, policy loss: 8.995867188990415
Experience 23, Iter 27, disc loss: 0.00017087175145691634, policy loss: 9.209058514401102
Experience 23, Iter 28, disc loss: 0.00018892556764436863, policy loss: 9.07791882506111
Experience 23, Iter 29, disc loss: 0.00017056168841403046, policy loss: 9.168245185555524
Experience 23, Iter 30, disc loss: 0.00017658068743991572, policy loss: 9.098871402972557
Experience 23, Iter 31, disc loss: 0.00018950282631871197, policy loss: 9.01530434443469
Experience 23, Iter 32, disc loss: 0.00019189867678321245, policy loss: 8.860794476173732
Experience 23, Iter 33, disc loss: 0.00019475624963408467, policy loss: 8.961447800439364
Experience 23, Iter 34, disc loss: 0.00018215979900098299, policy loss: 9.07112993683693
Experience 23, Iter 35, disc loss: 0.00017316312396602958, policy loss: 9.037937190813818
Experience 23, Iter 36, disc loss: 0.0001818270190114956, policy loss: 9.11048554245284
Experience 23, Iter 37, disc loss: 0.00021118516251582926, policy loss: 8.967484351275896
Experience 23, Iter 38, disc loss: 0.0001856707095099726, policy loss: 8.999639155123456
Experience 23, Iter 39, disc loss: 0.00021846499299683824, policy loss: 8.963012703605587
Experience 23, Iter 40, disc loss: 0.00016461826995400783, policy loss: 9.207353398567747
Experience 23, Iter 41, disc loss: 0.00017802803593944619, policy loss: 9.089378273809405
Experience 23, Iter 42, disc loss: 0.00017816987718378258, policy loss: 9.113435099799226
Experience 23, Iter 43, disc loss: 0.00020002044475540538, policy loss: 9.018600074467098
Experience 23, Iter 44, disc loss: 0.00019959283229808381, policy loss: 8.97166039894383
Experience 23, Iter 45, disc loss: 0.00022182910045718293, policy loss: 8.997369748387005
Experience 23, Iter 46, disc loss: 0.00018815116292999058, policy loss: 8.966423818581873
Experience 23, Iter 47, disc loss: 0.00017469690659892184, policy loss: 9.237514744591326
Experience 23, Iter 48, disc loss: 0.00016543236312916953, policy loss: 9.100421093262804
Experience 23, Iter 49, disc loss: 0.0001750443756330429, policy loss: 9.099769859945642
Experience 23, Iter 50, disc loss: 0.00018658276800996435, policy loss: 9.01052432864893
Experience 23, Iter 51, disc loss: 0.00019442645122864295, policy loss: 9.032382486894814
Experience 23, Iter 52, disc loss: 0.0001868847756545683, policy loss: 9.16139992325246
Experience 23, Iter 53, disc loss: 0.0001930307642347281, policy loss: 8.886924735113556
Experience 23, Iter 54, disc loss: 0.00018587128414704378, policy loss: 9.018853534196197
Experience 23, Iter 55, disc loss: 0.0001886705405245195, policy loss: 8.95559941578485
Experience 23, Iter 56, disc loss: 0.00016172225080943832, policy loss: 9.24556237477408
Experience 23, Iter 57, disc loss: 0.00015581997062422785, policy loss: 9.256373815944837
Experience 23, Iter 58, disc loss: 0.0001801085497079676, policy loss: 9.050047772560692
Experience 23, Iter 59, disc loss: 0.0001828002394971273, policy loss: 9.156565980747013
Experience 23, Iter 60, disc loss: 0.00016637069400658234, policy loss: 9.18842411074344
Experience 23, Iter 61, disc loss: 0.00019011824493342163, policy loss: 8.952920980115586
Experience 23, Iter 62, disc loss: 0.00016650750628459087, policy loss: 9.173450308612455
Experience 23, Iter 63, disc loss: 0.00016176275930057427, policy loss: 9.317078122255928
Experience 23, Iter 64, disc loss: 0.00017506023950217533, policy loss: 9.19552332461455
Experience 23, Iter 65, disc loss: 0.0002135041074383443, policy loss: 8.948511771635228
Experience 23, Iter 66, disc loss: 0.00017277606460714423, policy loss: 9.06622102250511
Experience 23, Iter 67, disc loss: 0.0002074956991944382, policy loss: 8.81495961113055
Experience 23, Iter 68, disc loss: 0.00018063438437796112, policy loss: 8.9737824978552
Experience 23, Iter 69, disc loss: 0.00016931705289062465, policy loss: 9.061604557119683
Experience 23, Iter 70, disc loss: 0.0001753307120923456, policy loss: 9.051261098122941
Experience 23, Iter 71, disc loss: 0.00018863080567975508, policy loss: 9.045601345171761
Experience 23, Iter 72, disc loss: 0.0001882663855082433, policy loss: 8.942511505908517
Experience 23, Iter 73, disc loss: 0.00018515342000153027, policy loss: 9.097071547150538
Experience 23, Iter 74, disc loss: 0.00017178134844358323, policy loss: 9.094113428926995
Experience 23, Iter 75, disc loss: 0.00018654273494290942, policy loss: 9.126689873200064
Experience 23, Iter 76, disc loss: 0.0001821086468779302, policy loss: 9.072392982418584
Experience 23, Iter 77, disc loss: 0.00017974885415596766, policy loss: 9.083608000993367
Experience 23, Iter 78, disc loss: 0.00017995114874526198, policy loss: 9.173677316877676
Experience 23, Iter 79, disc loss: 0.0001647285802195136, policy loss: 9.2324661708003
Experience 23, Iter 80, disc loss: 0.00017857494408089628, policy loss: 9.22032757859217
Experience 23, Iter 81, disc loss: 0.00016562189537209016, policy loss: 9.093954043689363
Experience 23, Iter 82, disc loss: 0.00020672762367096877, policy loss: 8.963365887454662
Experience 23, Iter 83, disc loss: 0.0001500972416641986, policy loss: 9.316627612607032
Experience 23, Iter 84, disc loss: 0.00017840564677621212, policy loss: 9.023676277246429
Experience 23, Iter 85, disc loss: 0.00016135967709757516, policy loss: 9.137654294163042
Experience 23, Iter 86, disc loss: 0.00014919757229070932, policy loss: 9.307044074293264
Experience 23, Iter 87, disc loss: 0.00015385478432741102, policy loss: 9.268622063231112
Experience 23, Iter 88, disc loss: 0.00017318490042452127, policy loss: 9.01263749438628
Experience 23, Iter 89, disc loss: 0.00016473131334829218, policy loss: 9.121208317859743
Experience 23, Iter 90, disc loss: 0.00019461650865019108, policy loss: 8.951678932699348
Experience 23, Iter 91, disc loss: 0.0001760804941230159, policy loss: 9.013009451057648
Experience 23, Iter 92, disc loss: 0.00017128328471815584, policy loss: 9.131869050286742
Experience 23, Iter 93, disc loss: 0.0001629492402269142, policy loss: 9.09751384496265
Experience 23, Iter 94, disc loss: 0.00018776894220689225, policy loss: 8.981479638887697
Experience 23, Iter 95, disc loss: 0.0001989325173306257, policy loss: 8.964047016881837
Experience 23, Iter 96, disc loss: 0.00017508714199225626, policy loss: 9.129054713918173
Experience 23, Iter 97, disc loss: 0.00018921448139374273, policy loss: 9.071122116450272
Experience 23, Iter 98, disc loss: 0.0001746392550166866, policy loss: 9.19865430405629
Experience 23, Iter 99, disc loss: 0.0001980810584807181, policy loss: 8.903211532943699
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.0160],
        [0.2162],
        [0.0042]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0252, 0.1556, 0.1912, 0.0056, 0.0022, 0.6017]],

        [[0.0252, 0.1556, 0.1912, 0.0056, 0.0022, 0.6017]],

        [[0.0252, 0.1556, 0.1912, 0.0056, 0.0022, 0.6017]],

        [[0.0252, 0.1556, 0.1912, 0.0056, 0.0022, 0.6017]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0170, 0.0641, 0.8649, 0.0167], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0170, 0.0641, 0.8649, 0.0167])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.169
Iter 2/2000 - Loss: 0.268
Iter 3/2000 - Loss: 0.015
Iter 4/2000 - Loss: -0.045
Iter 5/2000 - Loss: -0.041
Iter 6/2000 - Loss: -0.155
Iter 7/2000 - Loss: -0.330
Iter 8/2000 - Loss: -0.496
Iter 9/2000 - Loss: -0.648
Iter 10/2000 - Loss: -0.813
Iter 11/2000 - Loss: -1.011
Iter 12/2000 - Loss: -1.246
Iter 13/2000 - Loss: -1.510
Iter 14/2000 - Loss: -1.793
Iter 15/2000 - Loss: -2.090
Iter 16/2000 - Loss: -2.399
Iter 17/2000 - Loss: -2.716
Iter 18/2000 - Loss: -3.039
Iter 19/2000 - Loss: -3.364
Iter 20/2000 - Loss: -3.687
Iter 1981/2000 - Loss: -8.608
Iter 1982/2000 - Loss: -8.608
Iter 1983/2000 - Loss: -8.608
Iter 1984/2000 - Loss: -8.608
Iter 1985/2000 - Loss: -8.608
Iter 1986/2000 - Loss: -8.608
Iter 1987/2000 - Loss: -8.608
Iter 1988/2000 - Loss: -8.608
Iter 1989/2000 - Loss: -8.608
Iter 1990/2000 - Loss: -8.608
Iter 1991/2000 - Loss: -8.608
Iter 1992/2000 - Loss: -8.608
Iter 1993/2000 - Loss: -8.608
Iter 1994/2000 - Loss: -8.608
Iter 1995/2000 - Loss: -8.608
Iter 1996/2000 - Loss: -8.608
Iter 1997/2000 - Loss: -8.608
Iter 1998/2000 - Loss: -8.608
Iter 1999/2000 - Loss: -8.608
Iter 2000/2000 - Loss: -8.608
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[11.6783,  5.2599, 23.3827,  9.5318, 17.5790, 35.7943]],

        [[16.6194, 29.6079, 11.4306,  1.4743,  1.2488, 16.9796]],

        [[18.1974, 34.2184, 17.6634,  1.0035,  1.6808, 17.9526]],

        [[15.6899, 30.9965, 17.1277,  3.6016,  2.6261, 42.5746]]])
Signal Variance: tensor([ 0.0901,  0.8489, 13.2463,  0.4261])
Estimated target variance: tensor([0.0170, 0.0641, 0.8649, 0.0167])
N: 240
Signal to noise ratio: tensor([17.2272, 45.9249, 71.4731, 37.4094])
Bound on condition number: tensor([  71227.6056,  506183.6953, 1226018.3387,  335871.9813])
Policy Optimizer learning rate:
0.09760583411361425
Experience 24, Iter 0, disc loss: 0.00018827025261726943, policy loss: 9.094984686255039
Experience 24, Iter 1, disc loss: 0.00016677312913625103, policy loss: 9.318282590643701
Experience 24, Iter 2, disc loss: 0.00018212318965430798, policy loss: 9.106088610932343
Experience 24, Iter 3, disc loss: 0.00018526343772991245, policy loss: 9.080334798495203
Experience 24, Iter 4, disc loss: 0.00015747881815556802, policy loss: 9.19047046634098
Experience 24, Iter 5, disc loss: 0.00017276649187223956, policy loss: 9.12262331823052
Experience 24, Iter 6, disc loss: 0.00021458331256441154, policy loss: 8.971309421654277
Experience 24, Iter 7, disc loss: 0.00015862079562217792, policy loss: 9.182793606947312
Experience 24, Iter 8, disc loss: 0.00016360771788869182, policy loss: 9.12013342681619
Experience 24, Iter 9, disc loss: 0.0001849653610597416, policy loss: 9.01428157169969
Experience 24, Iter 10, disc loss: 0.00014343820387099638, policy loss: 9.356110132681218
Experience 24, Iter 11, disc loss: 0.0001687447208393803, policy loss: 9.02989381922386
Experience 24, Iter 12, disc loss: 0.00017479704396768743, policy loss: 8.979605763504011
Experience 24, Iter 13, disc loss: 0.00016881910997742646, policy loss: 9.200150604753055
Experience 24, Iter 14, disc loss: 0.00015373490566085137, policy loss: 9.181105525609743
Experience 24, Iter 15, disc loss: 0.00018164362117289832, policy loss: 9.032003554713299
Experience 24, Iter 16, disc loss: 0.00018323996973293563, policy loss: 9.134326503086236
Experience 24, Iter 17, disc loss: 0.00019246565193758812, policy loss: 8.97780931051746
Experience 24, Iter 18, disc loss: 0.00016978580565520378, policy loss: 9.281663517213707
Experience 24, Iter 19, disc loss: 0.00018730508179248958, policy loss: 9.037734468657852
Experience 24, Iter 20, disc loss: 0.0001587226769219092, policy loss: 9.271712027651786
Experience 24, Iter 21, disc loss: 0.00017577082671463092, policy loss: 9.225882534877607
Experience 24, Iter 22, disc loss: 0.00017885080060713008, policy loss: 9.058100987851999
Experience 24, Iter 23, disc loss: 0.0001561211091044547, policy loss: 9.382652019411001
Experience 24, Iter 24, disc loss: 0.00016276443819237984, policy loss: 9.220275308453925
Experience 24, Iter 25, disc loss: 0.00017226136738129736, policy loss: 9.192073925791858
Experience 24, Iter 26, disc loss: 0.00015896007054010634, policy loss: 9.287961365081536
Experience 24, Iter 27, disc loss: 0.0001644652540295983, policy loss: 9.179780857131746
Experience 24, Iter 28, disc loss: 0.0001663048752318804, policy loss: 9.133212908430073
Experience 24, Iter 29, disc loss: 0.00015510915057268492, policy loss: 9.374462232584952
Experience 24, Iter 30, disc loss: 0.00016575488287789782, policy loss: 9.10107713915692
Experience 24, Iter 31, disc loss: 0.00015074367130757933, policy loss: 9.279224149827218
Experience 24, Iter 32, disc loss: 0.00014982911163074794, policy loss: 9.185848943926876
Experience 24, Iter 33, disc loss: 0.00016714789605661167, policy loss: 9.152842048902011
Experience 24, Iter 34, disc loss: 0.00016025637289250497, policy loss: 9.19498350282633
Experience 24, Iter 35, disc loss: 0.00017728070865937967, policy loss: 9.059550856307117
Experience 24, Iter 36, disc loss: 0.0001408837120685129, policy loss: 9.389411888848054
Experience 24, Iter 37, disc loss: 0.0001716605620316033, policy loss: 9.109984668737164
Experience 24, Iter 38, disc loss: 0.0001689454229154649, policy loss: 9.148418470006678
Experience 24, Iter 39, disc loss: 0.00016408338541797069, policy loss: 9.102511486238308
Experience 24, Iter 40, disc loss: 0.00016131123470471004, policy loss: 9.19366273414315
Experience 24, Iter 41, disc loss: 0.00018410404563876638, policy loss: 9.04174680569615
Experience 24, Iter 42, disc loss: 0.00015599729710055358, policy loss: 9.205563535688572
Experience 24, Iter 43, disc loss: 0.00019128488588612046, policy loss: 9.014670572465485
Experience 24, Iter 44, disc loss: 0.00019178617724112377, policy loss: 8.95670392065697
Experience 24, Iter 45, disc loss: 0.00016630536184965425, policy loss: 9.321107695636424
Experience 24, Iter 46, disc loss: 0.0001613087321158533, policy loss: 9.109289143305178
Experience 24, Iter 47, disc loss: 0.00019782565980056938, policy loss: 9.294003990951799
Experience 24, Iter 48, disc loss: 0.00014337338756773154, policy loss: 9.30283814214264
Experience 24, Iter 49, disc loss: 0.0001749457294191923, policy loss: 9.288149643856418
Experience 24, Iter 50, disc loss: 0.00015493250638143807, policy loss: 9.351436551382548
Experience 24, Iter 51, disc loss: 0.00018269002966837385, policy loss: 9.192765428920978
Experience 24, Iter 52, disc loss: 0.0001568130924438286, policy loss: 9.23317200317694
Experience 24, Iter 53, disc loss: 0.00014988281375276276, policy loss: 9.35038530481748
Experience 24, Iter 54, disc loss: 0.00017973470996147283, policy loss: 9.060320537680962
Experience 24, Iter 55, disc loss: 0.00015824289970686807, policy loss: 9.176588311051205
Experience 24, Iter 56, disc loss: 0.00016518381606544475, policy loss: 9.341590568198407
Experience 24, Iter 57, disc loss: 0.00014877138185554793, policy loss: 9.153419666969986
Experience 24, Iter 58, disc loss: 0.00015308820435106288, policy loss: 9.184767458512416
Experience 24, Iter 59, disc loss: 0.00016487546277647115, policy loss: 9.122367544845702
Experience 24, Iter 60, disc loss: 0.00017055897962791526, policy loss: 9.15824859350413
Experience 24, Iter 61, disc loss: 0.00015332115147051215, policy loss: 9.283792749345604
Experience 24, Iter 62, disc loss: 0.00016170775730290753, policy loss: 9.185431826854186
Experience 24, Iter 63, disc loss: 0.00017366213636018095, policy loss: 9.110450757622703
Experience 24, Iter 64, disc loss: 0.00016469510495235193, policy loss: 9.169971471948557
Experience 24, Iter 65, disc loss: 0.00015984687543697587, policy loss: 9.18313123401919
Experience 24, Iter 66, disc loss: 0.000225527279604419, policy loss: 9.03311860844532
Experience 24, Iter 67, disc loss: 0.0001677823015549409, policy loss: 9.121695569058039
Experience 24, Iter 68, disc loss: 0.0001698433362156307, policy loss: 9.349123439176884
Experience 24, Iter 69, disc loss: 0.00017475688883313256, policy loss: 9.234690977272713
Experience 24, Iter 70, disc loss: 0.00018638849220298546, policy loss: 9.40776909188838
Experience 24, Iter 71, disc loss: 0.00016307323437930542, policy loss: 9.20511978875515
Experience 24, Iter 72, disc loss: 0.00015429805295441438, policy loss: 9.196294006543157
Experience 24, Iter 73, disc loss: 0.00015363059968207527, policy loss: 9.179884712980003
Experience 24, Iter 74, disc loss: 0.00016342255623955226, policy loss: 9.139728406529905
Experience 24, Iter 75, disc loss: 0.000155633771244397, policy loss: 9.205312211316764
Experience 24, Iter 76, disc loss: 0.00017363051942839578, policy loss: 9.145646271659121
Experience 24, Iter 77, disc loss: 0.00016805400988880467, policy loss: 9.178555960167527
Experience 24, Iter 78, disc loss: 0.00018187434446011356, policy loss: 9.201817680266885
Experience 24, Iter 79, disc loss: 0.00014887231907697668, policy loss: 9.21727302680209
Experience 24, Iter 80, disc loss: 0.00019032346967563213, policy loss: 9.186340306585583
Experience 24, Iter 81, disc loss: 0.00020591732399170671, policy loss: 8.959972608545119
Experience 24, Iter 82, disc loss: 0.00014775392936802254, policy loss: 9.239345605790241
Experience 24, Iter 83, disc loss: 0.0001577579836242578, policy loss: 9.229204991826194
Experience 24, Iter 84, disc loss: 0.00015129301348484577, policy loss: 9.344280757581025
Experience 24, Iter 85, disc loss: 0.00016254347246375154, policy loss: 9.11353443156403
Experience 24, Iter 86, disc loss: 0.00018099026328079312, policy loss: 8.945504007842008
Experience 24, Iter 87, disc loss: 0.0001545341321923915, policy loss: 9.316143705719956
Experience 24, Iter 88, disc loss: 0.00016432621732135122, policy loss: 9.248987199240982
Experience 24, Iter 89, disc loss: 0.0001932628097219989, policy loss: 9.134303291969543
Experience 24, Iter 90, disc loss: 0.00014651236569993128, policy loss: 9.58135805541626
Experience 24, Iter 91, disc loss: 0.0001635785208745258, policy loss: 9.25895766399846
Experience 24, Iter 92, disc loss: 0.00015334084388969263, policy loss: 9.336678822609457
Experience 24, Iter 93, disc loss: 0.00017657282696484334, policy loss: 9.122560559353085
Experience 24, Iter 94, disc loss: 0.00019895372960920045, policy loss: 9.150429711358013
Experience 24, Iter 95, disc loss: 0.0001563974670368099, policy loss: 9.21332670344725
Experience 24, Iter 96, disc loss: 0.00016593024892910303, policy loss: 9.10381789838494
Experience 24, Iter 97, disc loss: 0.00015068344892288816, policy loss: 9.255831304983644
Experience 24, Iter 98, disc loss: 0.00014298665748266342, policy loss: 9.247511639061528
Experience 24, Iter 99, disc loss: 0.00014987915764546068, policy loss: 9.302977001822445
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.0155],
        [0.2077],
        [0.0040]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0242, 0.1500, 0.1837, 0.0054, 0.0021, 0.5803]],

        [[0.0242, 0.1500, 0.1837, 0.0054, 0.0021, 0.5803]],

        [[0.0242, 0.1500, 0.1837, 0.0054, 0.0021, 0.5803]],

        [[0.0242, 0.1500, 0.1837, 0.0054, 0.0021, 0.5803]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0164, 0.0618, 0.8309, 0.0161], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0164, 0.0618, 0.8309, 0.0161])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.091
Iter 2/2000 - Loss: 0.205
Iter 3/2000 - Loss: -0.062
Iter 4/2000 - Loss: -0.123
Iter 5/2000 - Loss: -0.117
Iter 6/2000 - Loss: -0.235
Iter 7/2000 - Loss: -0.414
Iter 8/2000 - Loss: -0.585
Iter 9/2000 - Loss: -0.741
Iter 10/2000 - Loss: -0.910
Iter 11/2000 - Loss: -1.112
Iter 12/2000 - Loss: -1.348
Iter 13/2000 - Loss: -1.612
Iter 14/2000 - Loss: -1.895
Iter 15/2000 - Loss: -2.192
Iter 16/2000 - Loss: -2.501
Iter 17/2000 - Loss: -2.819
Iter 18/2000 - Loss: -3.143
Iter 19/2000 - Loss: -3.468
Iter 20/2000 - Loss: -3.791
Iter 1981/2000 - Loss: -8.674
Iter 1982/2000 - Loss: -8.674
Iter 1983/2000 - Loss: -8.674
Iter 1984/2000 - Loss: -8.674
Iter 1985/2000 - Loss: -8.674
Iter 1986/2000 - Loss: -8.674
Iter 1987/2000 - Loss: -8.674
Iter 1988/2000 - Loss: -8.674
Iter 1989/2000 - Loss: -8.674
Iter 1990/2000 - Loss: -8.674
Iter 1991/2000 - Loss: -8.674
Iter 1992/2000 - Loss: -8.674
Iter 1993/2000 - Loss: -8.674
Iter 1994/2000 - Loss: -8.674
Iter 1995/2000 - Loss: -8.674
Iter 1996/2000 - Loss: -8.674
Iter 1997/2000 - Loss: -8.674
Iter 1998/2000 - Loss: -8.674
Iter 1999/2000 - Loss: -8.674
Iter 2000/2000 - Loss: -8.674
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[11.5029,  5.2189, 22.9680,  9.4499, 17.6220, 35.8863]],

        [[16.3903, 29.2698, 11.3799,  1.4693,  1.2500, 16.9683]],

        [[17.9101, 33.6959, 17.6437,  0.9997,  1.6515, 17.8792]],

        [[15.3918, 30.2537, 16.9890,  3.5175,  2.5200, 42.3675]]])
Signal Variance: tensor([ 0.0892,  0.8413, 13.0942,  0.4161])
Estimated target variance: tensor([0.0164, 0.0618, 0.8309, 0.0161])
N: 250
Signal to noise ratio: tensor([17.2569, 46.2622, 72.0371, 37.4134])
Bound on condition number: tensor([  74451.5558,  535049.4848, 1297337.9307,  349941.0377])
Policy Optimizer learning rate:
0.09750305025978745
Experience 25, Iter 0, disc loss: 0.00015885443431786124, policy loss: 9.195082763069646
Experience 25, Iter 1, disc loss: 0.00016744230981135627, policy loss: 9.10145029606761
Experience 25, Iter 2, disc loss: 0.00015869242321941935, policy loss: 9.256185721253258
Experience 25, Iter 3, disc loss: 0.00015695121664719756, policy loss: 9.09701080295027
Experience 25, Iter 4, disc loss: 0.00015103542304926613, policy loss: 9.239014341391982
Experience 25, Iter 5, disc loss: 0.0001662262123903378, policy loss: 9.125107979972409
Experience 25, Iter 6, disc loss: 0.00016060641763956203, policy loss: 9.207442860316313
Experience 25, Iter 7, disc loss: 0.0001415777876645417, policy loss: 9.277126005476799
Experience 25, Iter 8, disc loss: 0.0001533122392140808, policy loss: 9.216462521274458
Experience 25, Iter 9, disc loss: 0.00015427834098562758, policy loss: 9.346239922715549
Experience 25, Iter 10, disc loss: 0.0001568580290112682, policy loss: 9.245601524538753
Experience 25, Iter 11, disc loss: 0.00015063651368133315, policy loss: 9.169419761163635
Experience 25, Iter 12, disc loss: 0.0001314723318469706, policy loss: 9.301036424199498
Experience 25, Iter 13, disc loss: 0.00018158581377859, policy loss: 9.276679546412797
Experience 25, Iter 14, disc loss: 0.00015013222903892677, policy loss: 9.224860572470018
Experience 25, Iter 15, disc loss: 0.00016823465523398847, policy loss: 9.123012965122603
Experience 25, Iter 16, disc loss: 0.00015463664524224458, policy loss: 9.226860322316547
Experience 25, Iter 17, disc loss: 0.0001446816465689933, policy loss: 9.247025647288472
Experience 25, Iter 18, disc loss: 0.0001473911252550787, policy loss: 9.307286717480881
Experience 25, Iter 19, disc loss: 0.00014993892682275042, policy loss: 9.176583125299452
Experience 25, Iter 20, disc loss: 0.0001667426030346844, policy loss: 9.15270225088232
Experience 25, Iter 21, disc loss: 0.0001531742847667897, policy loss: 9.227369265281462
Experience 25, Iter 22, disc loss: 0.00017245928677579354, policy loss: 9.237545272054856
Experience 25, Iter 23, disc loss: 0.00016584458281851682, policy loss: 9.08459645353625
Experience 25, Iter 24, disc loss: 0.00014428768597442532, policy loss: 9.253541301733197
Experience 25, Iter 25, disc loss: 0.00014668507245933678, policy loss: 9.244163959360327
Experience 25, Iter 26, disc loss: 0.00014113991603239396, policy loss: 9.330136535920628
Experience 25, Iter 27, disc loss: 0.00014073671945094617, policy loss: 9.334282886508912
Experience 25, Iter 28, disc loss: 0.00014680884347413007, policy loss: 9.384296496863278
Experience 25, Iter 29, disc loss: 0.00017393355065460896, policy loss: 9.2539523807801
Experience 25, Iter 30, disc loss: 0.00014962002049938522, policy loss: 9.269063541090087
Experience 25, Iter 31, disc loss: 0.00015779629449504663, policy loss: 9.189483977244558
Experience 25, Iter 32, disc loss: 0.0001433384663732703, policy loss: 9.314775353420954
Experience 25, Iter 33, disc loss: 0.00014738085474747182, policy loss: 9.23177522980401
Experience 25, Iter 34, disc loss: 0.0001658182890959832, policy loss: 9.165261962682266
Experience 25, Iter 35, disc loss: 0.00014597151638170216, policy loss: 9.291130176413361
Experience 25, Iter 36, disc loss: 0.0001379440084042066, policy loss: 9.39858407070326
Experience 25, Iter 37, disc loss: 0.00016755494746982746, policy loss: 9.025216212130317
Experience 25, Iter 38, disc loss: 0.00014403464821248051, policy loss: 9.325554365896762
Experience 25, Iter 39, disc loss: 0.00011449927057132002, policy loss: 9.656718918167126
Experience 25, Iter 40, disc loss: 0.0001860862579062791, policy loss: 9.083258692298392
Experience 25, Iter 41, disc loss: 0.000152413322796159, policy loss: 9.211962597016319
Experience 25, Iter 42, disc loss: 0.0001373999446256207, policy loss: 9.464231511056624
Experience 25, Iter 43, disc loss: 0.00014158739334070824, policy loss: 9.352204165087997
Experience 25, Iter 44, disc loss: 0.00014926352284915575, policy loss: 9.354265822605926
Experience 25, Iter 45, disc loss: 0.00019216695056762867, policy loss: 8.945017259471792
Experience 25, Iter 46, disc loss: 0.00017333615767680142, policy loss: 9.054506266737338
Experience 25, Iter 47, disc loss: 0.00016028364243174715, policy loss: 9.259458287894514
Experience 25, Iter 48, disc loss: 0.00014154911502363556, policy loss: 9.33256514963674
Experience 25, Iter 49, disc loss: 0.00013932892452707548, policy loss: 9.291613069212994
Experience 25, Iter 50, disc loss: 0.00014785874300158276, policy loss: 9.228754750918059
Experience 25, Iter 51, disc loss: 0.00012415142444453903, policy loss: 9.474587119852085
Experience 25, Iter 52, disc loss: 0.00015034298902290758, policy loss: 9.268231224605824
Experience 25, Iter 53, disc loss: 0.00015123347515516355, policy loss: 9.252642801437643
Experience 25, Iter 54, disc loss: 0.00017506433518011338, policy loss: 9.085823056802633
Experience 25, Iter 55, disc loss: 0.00017013565329222066, policy loss: 9.202513294576086
Experience 25, Iter 56, disc loss: 0.00014221851621868677, policy loss: 9.3251516184647
Experience 25, Iter 57, disc loss: 0.00013638754692232716, policy loss: 9.422085609450523
Experience 25, Iter 58, disc loss: 0.00015649779491507627, policy loss: 9.34757263471707
Experience 25, Iter 59, disc loss: 0.00014662756882159478, policy loss: 9.244875122938922
Experience 25, Iter 60, disc loss: 0.00014153412438503078, policy loss: 9.311474083560732
Experience 25, Iter 61, disc loss: 0.00014499218662292042, policy loss: 9.369474178645866
Experience 25, Iter 62, disc loss: 0.00014030391823486904, policy loss: 9.383777902861393
Experience 25, Iter 63, disc loss: 0.00017893009475474647, policy loss: 9.180482154521957
Experience 25, Iter 64, disc loss: 0.0001581492445901242, policy loss: 9.222572796432324
Experience 25, Iter 65, disc loss: 0.0001500147233586532, policy loss: 9.233745357389159
Experience 25, Iter 66, disc loss: 0.00012885524329945956, policy loss: 9.496982973533827
Experience 25, Iter 67, disc loss: 0.0001616228514007381, policy loss: 9.108021535753716
Experience 25, Iter 68, disc loss: 0.00012972254351570394, policy loss: 9.414269729352533
Experience 25, Iter 69, disc loss: 0.00014853976065756124, policy loss: 9.408840565121373
Experience 25, Iter 70, disc loss: 0.00012893005334082522, policy loss: 9.486973937600652
Experience 25, Iter 71, disc loss: 0.00013676424503105688, policy loss: 9.404811934481483
Experience 25, Iter 72, disc loss: 0.00014070952451705545, policy loss: 9.310786339940728
Experience 25, Iter 73, disc loss: 0.000152732105836358, policy loss: 9.19783150085993
Experience 25, Iter 74, disc loss: 0.00014160644001578876, policy loss: 9.419142776457148
Experience 25, Iter 75, disc loss: 0.00016120657307114542, policy loss: 9.39293915095449
Experience 25, Iter 76, disc loss: 0.0001524764511726945, policy loss: 9.295801261015292
Experience 25, Iter 77, disc loss: 0.00015425777431742014, policy loss: 9.316749022403807
Experience 25, Iter 78, disc loss: 0.00014780774839406652, policy loss: 9.312564288254782
Experience 25, Iter 79, disc loss: 0.0001499074616578549, policy loss: 9.238814299359735
Experience 25, Iter 80, disc loss: 0.00014186144918115312, policy loss: 9.289495370198871
Experience 25, Iter 81, disc loss: 0.00017686954206624266, policy loss: 9.050361848767281
Experience 25, Iter 82, disc loss: 0.0001791848150432463, policy loss: 9.179926984811484
Experience 25, Iter 83, disc loss: 0.00016989224649269224, policy loss: 9.155780798553463
Experience 25, Iter 84, disc loss: 0.000177281482993671, policy loss: 9.073505022144559
Experience 25, Iter 85, disc loss: 0.0001344796174435404, policy loss: 9.360313787857795
Experience 25, Iter 86, disc loss: 0.00016870283827670265, policy loss: 9.196957822765764
Experience 25, Iter 87, disc loss: 0.0001460060491889342, policy loss: 9.323945052780607
Experience 25, Iter 88, disc loss: 0.00015253204514428947, policy loss: 9.387182109101419
Experience 25, Iter 89, disc loss: 0.00016046275744478083, policy loss: 9.21414141757293
Experience 25, Iter 90, disc loss: 0.00015971822783956846, policy loss: 9.077711039349401
Experience 25, Iter 91, disc loss: 0.0001546842764497322, policy loss: 9.213657306389159
Experience 25, Iter 92, disc loss: 0.0001422992268901096, policy loss: 9.295479601426155
Experience 25, Iter 93, disc loss: 0.000131044743116314, policy loss: 9.387828844575347
Experience 25, Iter 94, disc loss: 0.00015321989438889466, policy loss: 9.212487913376505
Experience 25, Iter 95, disc loss: 0.00012749187581356574, policy loss: 9.442708251853364
Experience 25, Iter 96, disc loss: 0.0001382421261991999, policy loss: 9.340264380521287
Experience 25, Iter 97, disc loss: 0.00014300258794862258, policy loss: 9.303674450696537
Experience 25, Iter 98, disc loss: 0.00013858756166624004, policy loss: 9.341235377242882
Experience 25, Iter 99, disc loss: 0.00012827306762554203, policy loss: 9.431553700046514
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.0149],
        [0.2009],
        [0.0039]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0235, 0.1447, 0.1776, 0.0052, 0.0021, 0.5603]],

        [[0.0235, 0.1447, 0.1776, 0.0052, 0.0021, 0.5603]],

        [[0.0235, 0.1447, 0.1776, 0.0052, 0.0021, 0.5603]],

        [[0.0235, 0.1447, 0.1776, 0.0052, 0.0021, 0.5603]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.0597, 0.8035, 0.0155], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.0597, 0.8035, 0.0155])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.028
Iter 2/2000 - Loss: 0.171
Iter 3/2000 - Loss: -0.113
Iter 4/2000 - Loss: -0.169
Iter 5/2000 - Loss: -0.152
Iter 6/2000 - Loss: -0.266
Iter 7/2000 - Loss: -0.445
Iter 8/2000 - Loss: -0.614
Iter 9/2000 - Loss: -0.769
Iter 10/2000 - Loss: -0.937
Iter 11/2000 - Loss: -1.136
Iter 12/2000 - Loss: -1.371
Iter 13/2000 - Loss: -1.635
Iter 14/2000 - Loss: -1.920
Iter 15/2000 - Loss: -2.222
Iter 16/2000 - Loss: -2.536
Iter 17/2000 - Loss: -2.859
Iter 18/2000 - Loss: -3.187
Iter 19/2000 - Loss: -3.517
Iter 20/2000 - Loss: -3.844
Iter 1981/2000 - Loss: -8.722
Iter 1982/2000 - Loss: -8.722
Iter 1983/2000 - Loss: -8.722
Iter 1984/2000 - Loss: -8.722
Iter 1985/2000 - Loss: -8.722
Iter 1986/2000 - Loss: -8.723
Iter 1987/2000 - Loss: -8.723
Iter 1988/2000 - Loss: -8.723
Iter 1989/2000 - Loss: -8.723
Iter 1990/2000 - Loss: -8.723
Iter 1991/2000 - Loss: -8.723
Iter 1992/2000 - Loss: -8.723
Iter 1993/2000 - Loss: -8.723
Iter 1994/2000 - Loss: -8.723
Iter 1995/2000 - Loss: -8.723
Iter 1996/2000 - Loss: -8.723
Iter 1997/2000 - Loss: -8.723
Iter 1998/2000 - Loss: -8.723
Iter 1999/2000 - Loss: -8.723
Iter 2000/2000 - Loss: -8.723
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[11.1979,  5.1073, 22.4058,  9.2196, 17.5097, 36.2331]],

        [[15.9258, 28.5938, 11.2697,  1.4561,  1.2743, 17.1061]],

        [[17.4924, 32.8981, 17.6269,  1.0053,  1.6678, 17.8472]],

        [[15.0253, 29.2236, 16.8514,  3.3862,  2.3937, 42.1067]]])
Signal Variance: tensor([ 0.0882,  0.8413, 12.9891,  0.4049])
Estimated target variance: tensor([0.0158, 0.0597, 0.8035, 0.0155])
N: 260
Signal to noise ratio: tensor([17.3629, 46.4213, 71.5753, 37.4316])
Bound on condition number: tensor([  78383.6952,  560285.5524, 1331987.7438,  364293.6138])
Policy Optimizer learning rate:
0.09740037464252974
Experience 26, Iter 0, disc loss: 0.00015098442271731542, policy loss: 9.22262146940487
Experience 26, Iter 1, disc loss: 0.00015292317250130915, policy loss: 9.209422002273543
Experience 26, Iter 2, disc loss: 0.00012276597997746769, policy loss: 9.501602521412064
Experience 26, Iter 3, disc loss: 0.00015811021833000817, policy loss: 9.165318936481674
Experience 26, Iter 4, disc loss: 0.0001361172178092841, policy loss: 9.380210629843415
Experience 26, Iter 5, disc loss: 0.00013474830110097273, policy loss: 9.368264773528596
Experience 26, Iter 6, disc loss: 0.00012751032380503859, policy loss: 9.418802448412519
Experience 26, Iter 7, disc loss: 0.00017107443877190798, policy loss: 9.1817083723605
Experience 26, Iter 8, disc loss: 0.00016468778433114217, policy loss: 9.096937991856763
Experience 26, Iter 9, disc loss: 0.0001336291665100144, policy loss: 9.337945316142404
Experience 26, Iter 10, disc loss: 0.00013385437822322345, policy loss: 9.45545004449314
Experience 26, Iter 11, disc loss: 0.00013484498647518993, policy loss: 9.356635969933386
Experience 26, Iter 12, disc loss: 0.0001451891620679148, policy loss: 9.37112312791558
Experience 26, Iter 13, disc loss: 0.0001314729644867579, policy loss: 9.526612087192255
Experience 26, Iter 14, disc loss: 0.0001407146520530058, policy loss: 9.469973369910209
Experience 26, Iter 15, disc loss: 0.00016069789999154476, policy loss: 9.103640695429352
Experience 26, Iter 16, disc loss: 0.00013136252630425977, policy loss: 9.461100051835546
Experience 26, Iter 17, disc loss: 0.0001464800061464051, policy loss: 9.2886660353815
Experience 26, Iter 18, disc loss: 0.00013527786166944077, policy loss: 9.33872333473521
Experience 26, Iter 19, disc loss: 0.0001160363591598811, policy loss: 9.616049598780089
Experience 26, Iter 20, disc loss: 0.00013748383610419788, policy loss: 9.373419999819996
Experience 26, Iter 21, disc loss: 0.0001561744243345381, policy loss: 9.157410835681622
Experience 26, Iter 22, disc loss: 0.00014064241635370803, policy loss: 9.421358880494315
Experience 26, Iter 23, disc loss: 0.00013863468084093848, policy loss: 9.248262414189416
Experience 26, Iter 24, disc loss: 0.00014482819443525536, policy loss: 9.14841090954781
Experience 26, Iter 25, disc loss: 0.0001426901239232944, policy loss: 9.327973281789788
Experience 26, Iter 26, disc loss: 0.00012664724206578972, policy loss: 9.497034448595773
Experience 26, Iter 27, disc loss: 0.00014879091989934153, policy loss: 9.321179972302163
Experience 26, Iter 28, disc loss: 0.00012540670159484803, policy loss: 9.586888445982037
Experience 26, Iter 29, disc loss: 0.00013239167336668727, policy loss: 9.516058616858363
Experience 26, Iter 30, disc loss: 0.00013327538890241114, policy loss: 9.35298841010127
Experience 26, Iter 31, disc loss: 0.00011944474287054294, policy loss: 9.494704956652715
Experience 26, Iter 32, disc loss: 0.00015244904338275052, policy loss: 9.315005188491295
Experience 26, Iter 33, disc loss: 0.0001328033077542996, policy loss: 9.455759297416675
Experience 26, Iter 34, disc loss: 0.00012628466814642358, policy loss: 9.430056284636613
Experience 26, Iter 35, disc loss: 0.00014971891720605922, policy loss: 9.23661074885658
Experience 26, Iter 36, disc loss: 0.00014566874484098506, policy loss: 9.283092683208036
Experience 26, Iter 37, disc loss: 0.00013961484339741016, policy loss: 9.394346295600808
Experience 26, Iter 38, disc loss: 0.0001330467388975632, policy loss: 9.436366195784398
Experience 26, Iter 39, disc loss: 0.00014261086870519149, policy loss: 9.28038094867632
Experience 26, Iter 40, disc loss: 0.0001465171420761024, policy loss: 9.328244757246955
Experience 26, Iter 41, disc loss: 0.00012012590673890952, policy loss: 9.480253510291242
Experience 26, Iter 42, disc loss: 0.0001187771160294649, policy loss: 9.593470186278191
Experience 26, Iter 43, disc loss: 0.0001324647994133923, policy loss: 9.383733077914162
Experience 26, Iter 44, disc loss: 0.00013666608580335796, policy loss: 9.256906819151652
Experience 26, Iter 45, disc loss: 0.0001356024051758211, policy loss: 9.403961046005227
Experience 26, Iter 46, disc loss: 0.00012798895048293758, policy loss: 9.721343174119756
Experience 26, Iter 47, disc loss: 0.000142246417440539, policy loss: 9.402138220611953
Experience 26, Iter 48, disc loss: 0.0001154247814551309, policy loss: 9.630729138738825
Experience 26, Iter 49, disc loss: 0.00012445151221421303, policy loss: 9.54052635790088
Experience 26, Iter 50, disc loss: 0.00013594641936032068, policy loss: 9.345000667237906
Experience 26, Iter 51, disc loss: 0.00012362607088843515, policy loss: 9.467632068660404
Experience 26, Iter 52, disc loss: 0.00013502742663811573, policy loss: 9.475828367987756
Experience 26, Iter 53, disc loss: 0.00013920969599238696, policy loss: 9.31618957318818
Experience 26, Iter 54, disc loss: 0.00014939851412880493, policy loss: 9.258207019778045
Experience 26, Iter 55, disc loss: 0.00014833526869503747, policy loss: 9.230563113470515
Experience 26, Iter 56, disc loss: 0.00012482687282579526, policy loss: 9.508711568364356
Experience 26, Iter 57, disc loss: 0.00014511290074874429, policy loss: 9.370634954638637
Experience 26, Iter 58, disc loss: 0.00012192337475641551, policy loss: 9.456519143384398
Experience 26, Iter 59, disc loss: 0.00015422398455238368, policy loss: 9.290274850999625
Experience 26, Iter 60, disc loss: 0.00013036851909953734, policy loss: 9.399614813925233
Experience 26, Iter 61, disc loss: 0.0001407582885876056, policy loss: 9.352309200992547
Experience 26, Iter 62, disc loss: 0.00012673642911262587, policy loss: 9.4177968571848
Experience 26, Iter 63, disc loss: 0.00012625996119940502, policy loss: 9.48141557302533
Experience 26, Iter 64, disc loss: 0.00012737928329009096, policy loss: 9.45295610563496
Experience 26, Iter 65, disc loss: 0.00013651157629056892, policy loss: 9.443288103013659
Experience 26, Iter 66, disc loss: 0.00012547501435336832, policy loss: 9.45164973324153
Experience 26, Iter 67, disc loss: 0.00013681542749600245, policy loss: 9.41789168654163
Experience 26, Iter 68, disc loss: 0.00013376029738310398, policy loss: 9.464534465141758
Experience 26, Iter 69, disc loss: 0.00011712742516125889, policy loss: 9.631179455594165
Experience 26, Iter 70, disc loss: 0.00015709219467766928, policy loss: 9.202398748338567
Experience 26, Iter 71, disc loss: 0.00014343134475709935, policy loss: 9.262851732479067
Experience 26, Iter 72, disc loss: 0.00013056117320248587, policy loss: 9.27417050662373
Experience 26, Iter 73, disc loss: 0.00013501499103655944, policy loss: 9.39436331256235
Experience 26, Iter 74, disc loss: 0.00012042546800753342, policy loss: 9.563700667509055
Experience 26, Iter 75, disc loss: 0.00013566793619557632, policy loss: 9.351632991348577
Experience 26, Iter 76, disc loss: 0.00011338196110776842, policy loss: 9.56153325680746
Experience 26, Iter 77, disc loss: 0.0001418632516705446, policy loss: 9.58949137928308
Experience 26, Iter 78, disc loss: 0.00011942299149306983, policy loss: 9.63641644480354
Experience 26, Iter 79, disc loss: 0.00012974995149691334, policy loss: 9.504319326055803
Experience 26, Iter 80, disc loss: 0.00014494475095278285, policy loss: 9.402547740798312
Experience 26, Iter 81, disc loss: 0.0001610571738146648, policy loss: 9.36272435445332
Experience 26, Iter 82, disc loss: 0.00013401397567544706, policy loss: 9.332253829553135
Experience 26, Iter 83, disc loss: 0.00011171202746943453, policy loss: 9.60922546188903
Experience 26, Iter 84, disc loss: 0.00014905601033188425, policy loss: 9.23675535393667
Experience 26, Iter 85, disc loss: 0.0001334195895951108, policy loss: 9.38199238567985
Experience 26, Iter 86, disc loss: 0.00013651430158982079, policy loss: 9.387905256859414
Experience 26, Iter 87, disc loss: 0.00011497328509323535, policy loss: 9.681372057658848
Experience 26, Iter 88, disc loss: 0.0001359421292044796, policy loss: 9.392777627953354
Experience 26, Iter 89, disc loss: 0.000120941381939013, policy loss: 9.474560584153998
Experience 26, Iter 90, disc loss: 0.0001331459611406337, policy loss: 9.306456715317005
Experience 26, Iter 91, disc loss: 0.00013165833286057276, policy loss: 9.336122984174825
Experience 26, Iter 92, disc loss: 0.00010608800456778326, policy loss: 9.66980823160612
Experience 26, Iter 93, disc loss: 0.00014647374393137223, policy loss: 9.168573021907118
Experience 26, Iter 94, disc loss: 0.00012639617260341114, policy loss: 9.421865130224804
Experience 26, Iter 95, disc loss: 0.00013630287098159298, policy loss: 9.29648239331891
Experience 26, Iter 96, disc loss: 0.00012113286427885656, policy loss: 9.522508944385732
Experience 26, Iter 97, disc loss: 0.00016940856983694238, policy loss: 9.243090278379256
Experience 26, Iter 98, disc loss: 0.00012976579006377778, policy loss: 9.465062606502133
Experience 26, Iter 99, disc loss: 0.00014429625257291506, policy loss: 9.267475620102402
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0144],
        [0.1937],
        [0.0037]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0226, 0.1396, 0.1713, 0.0051, 0.0020, 0.5409]],

        [[0.0226, 0.1396, 0.1713, 0.0051, 0.0020, 0.5409]],

        [[0.0226, 0.1396, 0.1713, 0.0051, 0.0020, 0.5409]],

        [[0.0226, 0.1396, 0.1713, 0.0051, 0.0020, 0.5409]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0153, 0.0576, 0.7750, 0.0150], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0153, 0.0576, 0.7750, 0.0150])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.034
Iter 2/2000 - Loss: 0.132
Iter 3/2000 - Loss: -0.166
Iter 4/2000 - Loss: -0.217
Iter 5/2000 - Loss: -0.192
Iter 6/2000 - Loss: -0.304
Iter 7/2000 - Loss: -0.483
Iter 8/2000 - Loss: -0.653
Iter 9/2000 - Loss: -0.807
Iter 10/2000 - Loss: -0.973
Iter 11/2000 - Loss: -1.172
Iter 12/2000 - Loss: -1.406
Iter 13/2000 - Loss: -1.670
Iter 14/2000 - Loss: -1.955
Iter 15/2000 - Loss: -2.257
Iter 16/2000 - Loss: -2.571
Iter 17/2000 - Loss: -2.894
Iter 18/2000 - Loss: -3.223
Iter 19/2000 - Loss: -3.554
Iter 20/2000 - Loss: -3.883
Iter 1981/2000 - Loss: -8.734
Iter 1982/2000 - Loss: -8.734
Iter 1983/2000 - Loss: -8.734
Iter 1984/2000 - Loss: -8.734
Iter 1985/2000 - Loss: -8.735
Iter 1986/2000 - Loss: -8.735
Iter 1987/2000 - Loss: -8.735
Iter 1988/2000 - Loss: -8.735
Iter 1989/2000 - Loss: -8.735
Iter 1990/2000 - Loss: -8.735
Iter 1991/2000 - Loss: -8.735
Iter 1992/2000 - Loss: -8.735
Iter 1993/2000 - Loss: -8.735
Iter 1994/2000 - Loss: -8.735
Iter 1995/2000 - Loss: -8.735
Iter 1996/2000 - Loss: -8.735
Iter 1997/2000 - Loss: -8.735
Iter 1998/2000 - Loss: -8.735
Iter 1999/2000 - Loss: -8.735
Iter 2000/2000 - Loss: -8.735
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[10.9085,  5.1298, 22.0397,  9.1566, 17.5136, 35.6426]],

        [[15.6008, 28.3100, 11.3261,  1.5005,  1.2839, 17.6418]],

        [[17.1110, 32.4626, 17.6089,  1.0001,  1.6372, 17.7569]],

        [[14.6892, 28.7439, 16.6916,  3.2878,  2.2923, 41.5497]]])
Signal Variance: tensor([ 0.0876,  0.8896, 12.8287,  0.3940])
Estimated target variance: tensor([0.0153, 0.0576, 0.7750, 0.0150])
N: 270
Signal to noise ratio: tensor([17.2457, 47.2422, 71.4727, 36.8886])
Bound on condition number: tensor([  80303.1496,  602594.4517, 1379255.0910,  367408.4076])
Policy Optimizer learning rate:
0.09729780714786256
Experience 27, Iter 0, disc loss: 0.00012163552010121712, policy loss: 9.46967178861505
Experience 27, Iter 1, disc loss: 0.00014185556068882588, policy loss: 9.377798312510912
Experience 27, Iter 2, disc loss: 0.0001232244521532591, policy loss: 9.509234651506702
Experience 27, Iter 3, disc loss: 0.0001614468988746583, policy loss: 9.272405739965018
Experience 27, Iter 4, disc loss: 0.00012417758120185205, policy loss: 9.609934933784954
Experience 27, Iter 5, disc loss: 0.00013051306897117696, policy loss: 9.389962393173885
Experience 27, Iter 6, disc loss: 0.0001417008210764901, policy loss: 9.330329457644716
Experience 27, Iter 7, disc loss: 0.00013187391866160085, policy loss: 9.423773806792866
Experience 27, Iter 8, disc loss: 0.00013036334975684557, policy loss: 9.402301275810945
Experience 27, Iter 9, disc loss: 0.00011193841261376874, policy loss: 9.665078413013404
Experience 27, Iter 10, disc loss: 0.00012185326390367426, policy loss: 9.542208080050095
Experience 27, Iter 11, disc loss: 0.0001261396357118881, policy loss: 9.588342195140694
Experience 27, Iter 12, disc loss: 0.0001280541684730316, policy loss: 9.422143182703799
Experience 27, Iter 13, disc loss: 0.00011617515801538416, policy loss: 9.525598054815582
Experience 27, Iter 14, disc loss: 0.00013599954891662739, policy loss: 9.480925581653482
Experience 27, Iter 15, disc loss: 0.00011196276366338201, policy loss: 9.635962856310332
Experience 27, Iter 16, disc loss: 0.00012720067530551108, policy loss: 9.445470698658564
Experience 27, Iter 17, disc loss: 0.0001207985682429534, policy loss: 9.420152211991292
Experience 27, Iter 18, disc loss: 0.00013793250311847807, policy loss: 9.266168615836616
Experience 27, Iter 19, disc loss: 0.00016134568184727094, policy loss: 9.410612924610279
Experience 27, Iter 20, disc loss: 0.00012110406257083161, policy loss: 9.525588287196511
Experience 27, Iter 21, disc loss: 0.00012689618969732405, policy loss: 9.41904745715458
Experience 27, Iter 22, disc loss: 0.00011267607161798963, policy loss: 9.585235328122577
Experience 27, Iter 23, disc loss: 0.00013108684734731995, policy loss: 9.417122690006757
Experience 27, Iter 24, disc loss: 0.00014024052896268913, policy loss: 9.420356366949234
Experience 27, Iter 25, disc loss: 0.00011564079493932182, policy loss: 9.471313099239381
Experience 27, Iter 26, disc loss: 0.00014564663023666714, policy loss: 9.224841817189974
Experience 27, Iter 27, disc loss: 0.00011543304772429437, policy loss: 9.545857499141867
Experience 27, Iter 28, disc loss: 0.00012859202372408012, policy loss: 9.484875763490825
Experience 27, Iter 29, disc loss: 0.00011442455303364805, policy loss: 9.642696666973457
Experience 27, Iter 30, disc loss: 0.00013135051236427662, policy loss: 9.41926699531516
Experience 27, Iter 31, disc loss: 0.00011014796570912612, policy loss: 9.663115953071465
Experience 27, Iter 32, disc loss: 0.00012111854458346733, policy loss: 9.59008207083961
Experience 27, Iter 33, disc loss: 0.00011374793194390511, policy loss: 9.540686910419033
Experience 27, Iter 34, disc loss: 0.00011481689015841746, policy loss: 9.5939485358843
Experience 27, Iter 35, disc loss: 0.0001283649437572612, policy loss: 9.278722778646728
Experience 27, Iter 36, disc loss: 0.0001163415754859874, policy loss: 9.52274237572337
Experience 27, Iter 37, disc loss: 0.0001091464904822366, policy loss: 9.55769396519167
Experience 27, Iter 38, disc loss: 0.00011361034942662172, policy loss: 9.588815639062346
Experience 27, Iter 39, disc loss: 0.00010242089103316512, policy loss: 9.59615558721749
Experience 27, Iter 40, disc loss: 0.00011540240321779847, policy loss: 9.576628668853225
Experience 27, Iter 41, disc loss: 0.00013354321377052105, policy loss: 9.412973782050617
Experience 27, Iter 42, disc loss: 0.00011577153988574363, policy loss: 9.508338233240874
Experience 27, Iter 43, disc loss: 0.00013290179139748478, policy loss: 9.400676148019091
Experience 27, Iter 44, disc loss: 0.00011352075177324217, policy loss: 9.585011729167611
Experience 27, Iter 45, disc loss: 0.00013496001485478168, policy loss: 9.562927008285827
Experience 27, Iter 46, disc loss: 0.00011270855359394757, policy loss: 9.50677998255118
Experience 27, Iter 47, disc loss: 0.00013498168145042024, policy loss: 9.422742513914194
Experience 27, Iter 48, disc loss: 0.00011613447589555592, policy loss: 9.558519557084548
Experience 27, Iter 49, disc loss: 0.00011056382799184516, policy loss: 9.58795360148672
Experience 27, Iter 50, disc loss: 0.00012150993828407124, policy loss: 9.51477156442118
Experience 27, Iter 51, disc loss: 0.00010747108108877297, policy loss: 9.561775840118964
Experience 27, Iter 52, disc loss: 0.0001419812813154289, policy loss: 9.49014427842205
Experience 27, Iter 53, disc loss: 0.0001080171350608435, policy loss: 9.529519971021234
Experience 27, Iter 54, disc loss: 0.00011973308053571926, policy loss: 9.540236311087664
Experience 27, Iter 55, disc loss: 0.00012381013366194617, policy loss: 9.565857859449633
Experience 27, Iter 56, disc loss: 0.00012674867484723299, policy loss: 9.489949607616918
Experience 27, Iter 57, disc loss: 0.00012158873764744392, policy loss: 9.577070649799488
Experience 27, Iter 58, disc loss: 0.0001237547785901925, policy loss: 9.377243688686296
Experience 27, Iter 59, disc loss: 0.0001291571293040089, policy loss: 9.347302463516803
Experience 27, Iter 60, disc loss: 0.00011090737727575762, policy loss: 9.706554559407314
Experience 27, Iter 61, disc loss: 0.00011857604426639205, policy loss: 9.552736479672177
Experience 27, Iter 62, disc loss: 0.00012858241816510093, policy loss: 9.394952027286147
Experience 27, Iter 63, disc loss: 0.00012355801956552757, policy loss: 9.446830974908899
Experience 27, Iter 64, disc loss: 0.00011561413263747275, policy loss: 9.636609656077121
Experience 27, Iter 65, disc loss: 0.00013116309280975286, policy loss: 9.353194911441678
Experience 27, Iter 66, disc loss: 0.00011250940271353772, policy loss: 9.638968565652952
Experience 27, Iter 67, disc loss: 0.00011714111182201997, policy loss: 9.539812667614452
Experience 27, Iter 68, disc loss: 0.00014151596886324095, policy loss: 9.336998568048113
Experience 27, Iter 69, disc loss: 0.00011029427316588218, policy loss: 9.614453306036125
Experience 27, Iter 70, disc loss: 0.000136815025376403, policy loss: 9.342641348758525
Experience 27, Iter 71, disc loss: 0.00011807547591413643, policy loss: 9.592707233760798
Experience 27, Iter 72, disc loss: 0.00011837677391206881, policy loss: 9.539514593288555
Experience 27, Iter 73, disc loss: 0.00011228589780222085, policy loss: 9.530438081536804
Experience 27, Iter 74, disc loss: 0.00012132997345132835, policy loss: 9.581945336161859
Experience 27, Iter 75, disc loss: 0.00010562318945823724, policy loss: 9.569763922497149
Experience 27, Iter 76, disc loss: 0.00014701776736088238, policy loss: 9.429458905925994
Experience 27, Iter 77, disc loss: 0.00011530873252652562, policy loss: 9.50674971605709
Experience 27, Iter 78, disc loss: 0.00013719664478935225, policy loss: 9.333455982260796
Experience 27, Iter 79, disc loss: 0.00012552115089673342, policy loss: 9.58965937641449
Experience 27, Iter 80, disc loss: 0.00011287176845894121, policy loss: 9.653147868371265
Experience 27, Iter 81, disc loss: 0.00011092604279700779, policy loss: 9.632379709197984
Experience 27, Iter 82, disc loss: 0.00012090563285958594, policy loss: 9.538703600059932
Experience 27, Iter 83, disc loss: 0.00012251447279737655, policy loss: 9.48731708904019
Experience 27, Iter 84, disc loss: 0.0001233129259356244, policy loss: 9.52203006763628
Experience 27, Iter 85, disc loss: 0.00012393260325454836, policy loss: 9.392435481963195
Experience 27, Iter 86, disc loss: 0.00012690113804560545, policy loss: 9.42901606697029
Experience 27, Iter 87, disc loss: 0.00015615760170507281, policy loss: 9.445878489203837
Experience 27, Iter 88, disc loss: 0.00012738084047358896, policy loss: 9.568847750977307
Experience 27, Iter 89, disc loss: 0.00012354422916371462, policy loss: 9.551216099721916
Experience 27, Iter 90, disc loss: 0.00010711619961282853, policy loss: 9.630083239330371
Experience 27, Iter 91, disc loss: 0.00012087623221284466, policy loss: 9.416643773841773
Experience 27, Iter 92, disc loss: 0.00013843720599361698, policy loss: 9.21634318537835
Experience 27, Iter 93, disc loss: 0.00012166361134760052, policy loss: 9.574124124983634
Experience 27, Iter 94, disc loss: 0.00011163806838610863, policy loss: 9.594948845442088
Experience 27, Iter 95, disc loss: 0.00013210107839272612, policy loss: 9.282946691445956
Experience 27, Iter 96, disc loss: 0.00010882965333048369, policy loss: 9.626234366499379
Experience 27, Iter 97, disc loss: 0.00012467435200120313, policy loss: 9.492851013082252
Experience 27, Iter 98, disc loss: 0.0001208342429864986, policy loss: 9.507308765666076
Experience 27, Iter 99, disc loss: 0.00012929799646434607, policy loss: 9.503311884119885
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.0139],
        [0.1871],
        [0.0036]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0219, 0.1347, 0.1654, 0.0049, 0.0019, 0.5231]],

        [[0.0219, 0.1347, 0.1654, 0.0049, 0.0019, 0.5231]],

        [[0.0219, 0.1347, 0.1654, 0.0049, 0.0019, 0.5231]],

        [[0.0219, 0.1347, 0.1654, 0.0049, 0.0019, 0.5231]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0148, 0.0557, 0.7483, 0.0145], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0148, 0.0557, 0.7483, 0.0145])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.097
Iter 2/2000 - Loss: 0.086
Iter 3/2000 - Loss: -0.224
Iter 4/2000 - Loss: -0.273
Iter 5/2000 - Loss: -0.242
Iter 6/2000 - Loss: -0.352
Iter 7/2000 - Loss: -0.533
Iter 8/2000 - Loss: -0.704
Iter 9/2000 - Loss: -0.858
Iter 10/2000 - Loss: -1.023
Iter 11/2000 - Loss: -1.221
Iter 12/2000 - Loss: -1.453
Iter 13/2000 - Loss: -1.716
Iter 14/2000 - Loss: -1.999
Iter 15/2000 - Loss: -2.300
Iter 16/2000 - Loss: -2.613
Iter 17/2000 - Loss: -2.936
Iter 18/2000 - Loss: -3.265
Iter 19/2000 - Loss: -3.596
Iter 20/2000 - Loss: -3.925
Iter 1981/2000 - Loss: -8.732
Iter 1982/2000 - Loss: -8.732
Iter 1983/2000 - Loss: -8.732
Iter 1984/2000 - Loss: -8.732
Iter 1985/2000 - Loss: -8.732
Iter 1986/2000 - Loss: -8.732
Iter 1987/2000 - Loss: -8.732
Iter 1988/2000 - Loss: -8.732
Iter 1989/2000 - Loss: -8.732
Iter 1990/2000 - Loss: -8.732
Iter 1991/2000 - Loss: -8.732
Iter 1992/2000 - Loss: -8.732
Iter 1993/2000 - Loss: -8.732
Iter 1994/2000 - Loss: -8.732
Iter 1995/2000 - Loss: -8.732
Iter 1996/2000 - Loss: -8.732
Iter 1997/2000 - Loss: -8.732
Iter 1998/2000 - Loss: -8.732
Iter 1999/2000 - Loss: -8.732
Iter 2000/2000 - Loss: -8.732
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.7005,  4.9889, 21.7915,  9.1231, 17.6079, 35.8334]],

        [[15.4430, 27.9479, 11.3625,  1.5379,  1.2713, 17.9436]],

        [[16.8059, 31.3128, 17.7678,  1.0041,  1.6247, 18.0367]],

        [[14.4169, 27.5281, 16.5119,  3.1876,  2.2203, 41.3102]]])
Signal Variance: tensor([ 0.0874,  0.9111, 13.1379,  0.3843])
Estimated target variance: tensor([0.0148, 0.0557, 0.7483, 0.0145])
N: 280
Signal to noise ratio: tensor([17.0387, 48.0593, 71.0743, 36.4409])
Bound on condition number: tensor([  81290.0657,  646715.2719, 1414436.0656,  371822.9773])
Policy Optimizer learning rate:
0.09719534766192739
Experience 28, Iter 0, disc loss: 0.00011994089495052628, policy loss: 9.586588425985497
Experience 28, Iter 1, disc loss: 0.00011435579257752488, policy loss: 9.49534479710041
Experience 28, Iter 2, disc loss: 0.00012809572576714593, policy loss: 9.47635875302831
Experience 28, Iter 3, disc loss: 0.0001351482805846726, policy loss: 9.363971441763528
Experience 28, Iter 4, disc loss: 0.00011237534696266336, policy loss: 9.55489416967338
Experience 28, Iter 5, disc loss: 0.00012862382266672957, policy loss: 9.409747103918125
Experience 28, Iter 6, disc loss: 0.00011003698206804336, policy loss: 9.651265128211287
Experience 28, Iter 7, disc loss: 0.00011941382082614612, policy loss: 9.513799305341188
Experience 28, Iter 8, disc loss: 0.00012673651530487235, policy loss: 9.464457841214758
Experience 28, Iter 9, disc loss: 0.00011721164397529507, policy loss: 9.530881103792836
Experience 28, Iter 10, disc loss: 0.0001124798424289912, policy loss: 9.63357881384082
Experience 28, Iter 11, disc loss: 0.00010542064432181645, policy loss: 9.567021114430379
Experience 28, Iter 12, disc loss: 0.0001267056577648269, policy loss: 9.482897295982546
Experience 28, Iter 13, disc loss: 0.00013222906256170873, policy loss: 9.383683814776274
Experience 28, Iter 14, disc loss: 9.841627894164321e-05, policy loss: 9.738061295036157
Experience 28, Iter 15, disc loss: 0.0001037466940789978, policy loss: 9.79254224784612
Experience 28, Iter 16, disc loss: 0.00011461282187241768, policy loss: 9.56631264009518
Experience 28, Iter 17, disc loss: 9.751777521152934e-05, policy loss: 9.797544751845155
Experience 28, Iter 18, disc loss: 0.00011951083788300184, policy loss: 9.444152237018356
Experience 28, Iter 19, disc loss: 0.00011360275672715631, policy loss: 9.617293825951474
Experience 28, Iter 20, disc loss: 0.000126196175546834, policy loss: 9.373285385671075
Experience 28, Iter 21, disc loss: 0.00011759065693409358, policy loss: 9.512229788173322
Experience 28, Iter 22, disc loss: 0.0001231785092906184, policy loss: 9.703722008053203
Experience 28, Iter 23, disc loss: 9.760912415316547e-05, policy loss: 9.697498601341998
Experience 28, Iter 24, disc loss: 0.00011018259695719958, policy loss: 9.541778366710023
Experience 28, Iter 25, disc loss: 0.00012140136624207955, policy loss: 9.53018873173156
Experience 28, Iter 26, disc loss: 0.00010183051078780414, policy loss: 9.680485578135713
Experience 28, Iter 27, disc loss: 0.00012801013402315582, policy loss: 9.376933631400597
Experience 28, Iter 28, disc loss: 9.505941366981565e-05, policy loss: 9.815891199558262
Experience 28, Iter 29, disc loss: 0.0001337900837654114, policy loss: 9.426367517369751
Experience 28, Iter 30, disc loss: 0.00011892009773345908, policy loss: 9.502288337899314
Experience 28, Iter 31, disc loss: 0.00012073595956033519, policy loss: 9.532524756721717
Experience 28, Iter 32, disc loss: 0.00012480962887284012, policy loss: 9.572695478112472
Experience 28, Iter 33, disc loss: 0.00010818093142055204, policy loss: 9.505726119462548
Experience 28, Iter 34, disc loss: 0.00012481317001339557, policy loss: 9.418370549427578
Experience 28, Iter 35, disc loss: 0.00010512602595384028, policy loss: 9.535762021093294
Experience 28, Iter 36, disc loss: 0.00010634756225363387, policy loss: 9.61671200865375
Experience 28, Iter 37, disc loss: 0.00013510631770483067, policy loss: 9.614184364617817
Experience 28, Iter 38, disc loss: 0.00010504295980718123, policy loss: 9.577112318600465
Experience 28, Iter 39, disc loss: 0.00014535786126581673, policy loss: 9.349797174658594
Experience 28, Iter 40, disc loss: 0.00010340554286494296, policy loss: 9.711798651564447
Experience 28, Iter 41, disc loss: 0.00010754282528958971, policy loss: 9.618439119257756
Experience 28, Iter 42, disc loss: 0.0001356181582702662, policy loss: 9.417677471247954
Experience 28, Iter 43, disc loss: 0.00011464564668086988, policy loss: 9.568953338424553
Experience 28, Iter 44, disc loss: 0.00011139533713617648, policy loss: 9.678011281314095
Experience 28, Iter 45, disc loss: 0.00013217167022724105, policy loss: 9.536806228493514
Experience 28, Iter 46, disc loss: 0.00011027187929662388, policy loss: 9.804587981353972
Experience 28, Iter 47, disc loss: 0.00011250388818604223, policy loss: 9.710990187845066
Experience 28, Iter 48, disc loss: 9.332067931715669e-05, policy loss: 9.86599244031329
Experience 28, Iter 49, disc loss: 0.000108656956665617, policy loss: 9.691321232688807
Experience 28, Iter 50, disc loss: 9.809914453690245e-05, policy loss: 9.786695566868733
Experience 28, Iter 51, disc loss: 0.00011088813997101895, policy loss: 9.50714242301892
Experience 28, Iter 52, disc loss: 0.000110974410069669, policy loss: 9.552927350806222
Experience 28, Iter 53, disc loss: 0.00010391653615522147, policy loss: 9.547689518231252
Experience 28, Iter 54, disc loss: 0.00011780747553626502, policy loss: 9.475643044822665
Experience 28, Iter 55, disc loss: 0.0001153802679192828, policy loss: 9.416413813698332
Experience 28, Iter 56, disc loss: 0.00012697084199568768, policy loss: 9.448227896770492
Experience 28, Iter 57, disc loss: 0.00011160294683536969, policy loss: 9.567322857895814
Experience 28, Iter 58, disc loss: 0.00010311252122113925, policy loss: 9.873503285329516
Experience 28, Iter 59, disc loss: 0.00011119173550095098, policy loss: 9.582308390998575
Experience 28, Iter 60, disc loss: 0.00012167809031310432, policy loss: 9.515403474341458
Experience 28, Iter 61, disc loss: 0.00012153212271366978, policy loss: 9.591006950465815
Experience 28, Iter 62, disc loss: 0.00013957716684768332, policy loss: 9.339559967936566
Experience 28, Iter 63, disc loss: 0.0001380168049874888, policy loss: 9.539450279216645
Experience 28, Iter 64, disc loss: 0.00011272039641276617, policy loss: 9.554251631179744
Experience 28, Iter 65, disc loss: 0.00012146418687950788, policy loss: 9.546406904892779
Experience 28, Iter 66, disc loss: 0.00010522147576153907, policy loss: 9.663381552239876
Experience 28, Iter 67, disc loss: 0.00013006249724366068, policy loss: 9.430526687011941
Experience 28, Iter 68, disc loss: 0.00012218810516939797, policy loss: 9.56511804670323
Experience 28, Iter 69, disc loss: 0.00013228508095799034, policy loss: 9.42852958456401
Experience 28, Iter 70, disc loss: 9.51790643656645e-05, policy loss: 9.737311120886133
Experience 28, Iter 71, disc loss: 9.720978669014174e-05, policy loss: 9.812571307705886
Experience 28, Iter 72, disc loss: 0.00013219712798856118, policy loss: 9.551289892405405
Experience 28, Iter 73, disc loss: 9.752031022928662e-05, policy loss: 9.807176802993043
Experience 28, Iter 74, disc loss: 0.0001189440461482783, policy loss: 9.409661468731253
Experience 28, Iter 75, disc loss: 9.701383065538829e-05, policy loss: 9.745642366593785
Experience 28, Iter 76, disc loss: 0.00011994036420023097, policy loss: 9.426448971486874
Experience 28, Iter 77, disc loss: 9.897890533372967e-05, policy loss: 9.691779783832004
Experience 28, Iter 78, disc loss: 0.00010482746209389017, policy loss: 9.685600004211782
Experience 28, Iter 79, disc loss: 0.00010970909689923518, policy loss: 9.58214019874838
Experience 28, Iter 80, disc loss: 9.610424011612069e-05, policy loss: 9.674075607990556
Experience 28, Iter 81, disc loss: 0.0001088818672586199, policy loss: 9.651758034713373
Experience 28, Iter 82, disc loss: 9.625391672871857e-05, policy loss: 9.758258887535437
Experience 28, Iter 83, disc loss: 0.00011637773431791916, policy loss: 9.56369660888933
Experience 28, Iter 84, disc loss: 0.00010733106556549465, policy loss: 9.777446950393745
Experience 28, Iter 85, disc loss: 0.000106510483835176, policy loss: 9.591286619292323
Experience 28, Iter 86, disc loss: 0.0001145961544631861, policy loss: 9.565972089126127
Experience 28, Iter 87, disc loss: 0.00010031911162356765, policy loss: 9.678258279750453
Experience 28, Iter 88, disc loss: 9.777954548899406e-05, policy loss: 9.708963678351704
Experience 28, Iter 89, disc loss: 0.00012122088917760227, policy loss: 9.415710042175002
Experience 28, Iter 90, disc loss: 0.00010609114200654977, policy loss: 9.628431632460666
Experience 28, Iter 91, disc loss: 0.0001310956732837146, policy loss: 9.545582737974327
Experience 28, Iter 92, disc loss: 0.00011138633059409358, policy loss: 9.538621609968834
Experience 28, Iter 93, disc loss: 0.00010881708366968469, policy loss: 9.639121744698071
Experience 28, Iter 94, disc loss: 0.00012175413496098045, policy loss: 9.640200522186042
Experience 28, Iter 95, disc loss: 0.00010322538492080458, policy loss: 9.726294383864655
Experience 28, Iter 96, disc loss: 9.7791834344775e-05, policy loss: 9.76237035774476
Experience 28, Iter 97, disc loss: 0.00012093150552806807, policy loss: 9.61930641066661
Experience 28, Iter 98, disc loss: 9.110655633743007e-05, policy loss: 9.793360919844453
Experience 28, Iter 99, disc loss: 0.00011083818644314457, policy loss: 9.760614639907267
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.0135],
        [0.1806],
        [0.0035]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0211, 0.1304, 0.1597, 0.0047, 0.0019, 0.5065]],

        [[0.0211, 0.1304, 0.1597, 0.0047, 0.0019, 0.5065]],

        [[0.0211, 0.1304, 0.1597, 0.0047, 0.0019, 0.5065]],

        [[0.0211, 0.1304, 0.1597, 0.0047, 0.0019, 0.5065]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0143, 0.0539, 0.7225, 0.0140], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0143, 0.0539, 0.7225, 0.0140])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.188
Iter 2/2000 - Loss: 0.004
Iter 3/2000 - Loss: -0.318
Iter 4/2000 - Loss: -0.370
Iter 5/2000 - Loss: -0.340
Iter 6/2000 - Loss: -0.455
Iter 7/2000 - Loss: -0.640
Iter 8/2000 - Loss: -0.816
Iter 9/2000 - Loss: -0.972
Iter 10/2000 - Loss: -1.139
Iter 11/2000 - Loss: -1.336
Iter 12/2000 - Loss: -1.568
Iter 13/2000 - Loss: -1.829
Iter 14/2000 - Loss: -2.112
Iter 15/2000 - Loss: -2.411
Iter 16/2000 - Loss: -2.722
Iter 17/2000 - Loss: -3.043
Iter 18/2000 - Loss: -3.370
Iter 19/2000 - Loss: -3.698
Iter 20/2000 - Loss: -4.025
Iter 1981/2000 - Loss: -8.758
Iter 1982/2000 - Loss: -8.758
Iter 1983/2000 - Loss: -8.758
Iter 1984/2000 - Loss: -8.758
Iter 1985/2000 - Loss: -8.758
Iter 1986/2000 - Loss: -8.758
Iter 1987/2000 - Loss: -8.758
Iter 1988/2000 - Loss: -8.758
Iter 1989/2000 - Loss: -8.758
Iter 1990/2000 - Loss: -8.758
Iter 1991/2000 - Loss: -8.758
Iter 1992/2000 - Loss: -8.759
Iter 1993/2000 - Loss: -8.759
Iter 1994/2000 - Loss: -8.759
Iter 1995/2000 - Loss: -8.759
Iter 1996/2000 - Loss: -8.759
Iter 1997/2000 - Loss: -8.759
Iter 1998/2000 - Loss: -8.759
Iter 1999/2000 - Loss: -8.759
Iter 2000/2000 - Loss: -8.759
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.6215,  4.9856, 21.6124,  9.0758, 17.5509, 35.8671]],

        [[15.2972, 27.6753, 11.3547,  1.5487,  1.2619, 17.9036]],

        [[16.5913, 30.8378, 17.7089,  1.0025,  1.6212, 17.9767]],

        [[14.2364, 27.0234, 16.4057,  3.1455,  2.2000, 41.0785]]])
Signal Variance: tensor([ 0.0872,  0.9072, 13.0501,  0.3784])
Estimated target variance: tensor([0.0143, 0.0539, 0.7225, 0.0140])
N: 290
Signal to noise ratio: tensor([17.0466, 48.0667, 70.9818, 36.1876])
Bound on condition number: tensor([  84271.0379,  670019.9867, 1461141.3754,  379769.0708])
Policy Optimizer learning rate:
0.09709299607098561
Experience 29, Iter 0, disc loss: 0.00010157537989327287, policy loss: 9.691536938871742
Experience 29, Iter 1, disc loss: 0.00010072876460698787, policy loss: 9.624182460360519
Experience 29, Iter 2, disc loss: 9.960505752147306e-05, policy loss: 9.652537484363393
Experience 29, Iter 3, disc loss: 0.00011093602787240818, policy loss: 9.443452601740706
Experience 29, Iter 4, disc loss: 9.71137632139453e-05, policy loss: 9.710110866847263
Experience 29, Iter 5, disc loss: 0.00010414197070366161, policy loss: 9.604737198393284
Experience 29, Iter 6, disc loss: 0.00013202434025701598, policy loss: 9.51806694854556
Experience 29, Iter 7, disc loss: 0.00011661018500461624, policy loss: 9.48789892324688
Experience 29, Iter 8, disc loss: 0.00011151615332749407, policy loss: 9.560288799823912
Experience 29, Iter 9, disc loss: 0.00013890055238229004, policy loss: 9.36927248498139
Experience 29, Iter 10, disc loss: 8.126714107437e-05, policy loss: 10.042045374688922
Experience 29, Iter 11, disc loss: 9.849074221664149e-05, policy loss: 9.716818842896824
Experience 29, Iter 12, disc loss: 0.00010851535370266344, policy loss: 9.537908173318883
Experience 29, Iter 13, disc loss: 0.00011151509588911938, policy loss: 9.534216708287659
Experience 29, Iter 14, disc loss: 0.00010041224418971021, policy loss: 9.67318121937291
Experience 29, Iter 15, disc loss: 0.00011554934890465574, policy loss: 9.613924642490975
Experience 29, Iter 16, disc loss: 9.264752765998405e-05, policy loss: 9.7881867947834
Experience 29, Iter 17, disc loss: 0.0001366089470008164, policy loss: 9.41834932915529
Experience 29, Iter 18, disc loss: 9.919450250406192e-05, policy loss: 9.732646390017592
Experience 29, Iter 19, disc loss: 0.00010941694011730978, policy loss: 9.615807483875972
Experience 29, Iter 20, disc loss: 0.00011748779875979722, policy loss: 9.630628083107384
Experience 29, Iter 21, disc loss: 0.00012671120274253018, policy loss: 9.538280247692875
Experience 29, Iter 22, disc loss: 9.616558249088586e-05, policy loss: 9.771804238878637
Experience 29, Iter 23, disc loss: 0.00010846282936868826, policy loss: 9.748054802724091
Experience 29, Iter 24, disc loss: 0.00010140201467297264, policy loss: 9.821335918206291
Experience 29, Iter 25, disc loss: 0.00010062032157459385, policy loss: 9.69284927805898
Experience 29, Iter 26, disc loss: 0.00011009630299357306, policy loss: 9.58336744595352
Experience 29, Iter 27, disc loss: 0.00011437432441901319, policy loss: 9.56034026186617
Experience 29, Iter 28, disc loss: 8.632280700551519e-05, policy loss: 9.975126882227887
Experience 29, Iter 29, disc loss: 0.00010966638835769202, policy loss: 9.572676004533859
Experience 29, Iter 30, disc loss: 0.00010417184642415326, policy loss: 9.588723688221883
Experience 29, Iter 31, disc loss: 0.00010607766656431295, policy loss: 9.520409746215536
Experience 29, Iter 32, disc loss: 0.00010188154431938836, policy loss: 9.690290292089161
Experience 29, Iter 33, disc loss: 0.00010952435416503435, policy loss: 9.66384278955406
Experience 29, Iter 34, disc loss: 9.629708382397626e-05, policy loss: 9.746001204505317
Experience 29, Iter 35, disc loss: 9.1884444460163e-05, policy loss: 9.67841881803907
Experience 29, Iter 36, disc loss: 8.85917942294601e-05, policy loss: 10.052120939373562
Experience 29, Iter 37, disc loss: 8.929729553520888e-05, policy loss: 9.825417032293487
Experience 29, Iter 38, disc loss: 0.00010512455563797173, policy loss: 9.588825415881853
Experience 29, Iter 39, disc loss: 8.136100389672249e-05, policy loss: 9.998030933709442
Experience 29, Iter 40, disc loss: 9.983727861235006e-05, policy loss: 9.636166733317243
Experience 29, Iter 41, disc loss: 9.311759180655223e-05, policy loss: 9.82242550209977
Experience 29, Iter 42, disc loss: 8.711464733284128e-05, policy loss: 9.754254786712355
Experience 29, Iter 43, disc loss: 9.575452014492616e-05, policy loss: 9.70909758260123
Experience 29, Iter 44, disc loss: 8.948711957676888e-05, policy loss: 9.777789023554018
Experience 29, Iter 45, disc loss: 0.00010923723118651444, policy loss: 9.547979237268503
Experience 29, Iter 46, disc loss: 8.322851616568288e-05, policy loss: 9.932865720873462
Experience 29, Iter 47, disc loss: 0.00010511962379037277, policy loss: 9.581001771462013
Experience 29, Iter 48, disc loss: 9.15892022492472e-05, policy loss: 9.846452143589634
Experience 29, Iter 49, disc loss: 0.00010062360045915764, policy loss: 9.737980693913535
Experience 29, Iter 50, disc loss: 0.00011052123455563592, policy loss: 9.466268062939989
Experience 29, Iter 51, disc loss: 0.00010136396190536158, policy loss: 9.693676867057444
Experience 29, Iter 52, disc loss: 0.00011260556559829653, policy loss: 9.775921018247805
Experience 29, Iter 53, disc loss: 0.00010810207872724863, policy loss: 9.694743362126237
Experience 29, Iter 54, disc loss: 0.00011412368877341392, policy loss: 9.728530413537214
Experience 29, Iter 55, disc loss: 0.00010960919141948237, policy loss: 9.674964986903122
Experience 29, Iter 56, disc loss: 0.00011259341396054692, policy loss: 9.608953413401192
Experience 29, Iter 57, disc loss: 0.00010505348819615012, policy loss: 9.743458754761022
Experience 29, Iter 58, disc loss: 0.00010956161826612629, policy loss: 9.674142269606271
Experience 29, Iter 59, disc loss: 0.00010221801983716284, policy loss: 9.732687934754523
Experience 29, Iter 60, disc loss: 9.738589477356372e-05, policy loss: 9.701376591520821
Experience 29, Iter 61, disc loss: 9.703010499190244e-05, policy loss: 9.755810222377916
Experience 29, Iter 62, disc loss: 9.062616614796501e-05, policy loss: 9.799305662955614
Experience 29, Iter 63, disc loss: 9.748341506010464e-05, policy loss: 9.609590842235747
Experience 29, Iter 64, disc loss: 9.693908504360986e-05, policy loss: 9.728197384274946
Experience 29, Iter 65, disc loss: 8.818554595419853e-05, policy loss: 9.87990425582255
Experience 29, Iter 66, disc loss: 0.00010109286627323019, policy loss: 9.75651568164525
Experience 29, Iter 67, disc loss: 9.132207997685583e-05, policy loss: 9.754889501135693
Experience 29, Iter 68, disc loss: 8.8442880159233e-05, policy loss: 9.777401310375545
Experience 29, Iter 69, disc loss: 8.841237351821449e-05, policy loss: 9.787582325913966
Experience 29, Iter 70, disc loss: 9.965834863503713e-05, policy loss: 9.705766762812537
Experience 29, Iter 71, disc loss: 0.00010759437201547665, policy loss: 9.584468687421406
Experience 29, Iter 72, disc loss: 0.00010162830042167295, policy loss: 9.655830035315171
Experience 29, Iter 73, disc loss: 9.472438562775198e-05, policy loss: 9.790030742407842
Experience 29, Iter 74, disc loss: 0.00012035907196983895, policy loss: 9.566274699135638
Experience 29, Iter 75, disc loss: 9.574440915943529e-05, policy loss: 9.870804071234357
Experience 29, Iter 76, disc loss: 8.534802971767734e-05, policy loss: 9.901530187400464
Experience 29, Iter 77, disc loss: 0.00010666248451123944, policy loss: 9.572252725000702
Experience 29, Iter 78, disc loss: 9.852307335658313e-05, policy loss: 9.740971279245606
Experience 29, Iter 79, disc loss: 9.623126813196339e-05, policy loss: 9.832219653779443
Experience 29, Iter 80, disc loss: 0.00011824890022501218, policy loss: 9.4825467464869
Experience 29, Iter 81, disc loss: 0.00010849113304741205, policy loss: 9.578984480900939
Experience 29, Iter 82, disc loss: 9.51896597958885e-05, policy loss: 9.716598990576529
Experience 29, Iter 83, disc loss: 9.645371198338155e-05, policy loss: 9.723771646871656
Experience 29, Iter 84, disc loss: 0.0001023779383334304, policy loss: 9.766214247638846
Experience 29, Iter 85, disc loss: 0.00010236971803943039, policy loss: 9.681764760690871
Experience 29, Iter 86, disc loss: 0.00010709407582269479, policy loss: 9.797784142015152
Experience 29, Iter 87, disc loss: 9.570601608125014e-05, policy loss: 9.812037645075112
Experience 29, Iter 88, disc loss: 8.725645445036954e-05, policy loss: 10.00932655301348
Experience 29, Iter 89, disc loss: 7.916654050895932e-05, policy loss: 10.035184624358259
Experience 29, Iter 90, disc loss: 0.00010675898276266779, policy loss: 9.596381424353712
Experience 29, Iter 91, disc loss: 0.00010035983192814668, policy loss: 9.678665118816607
Experience 29, Iter 92, disc loss: 9.283702031470527e-05, policy loss: 9.820166987539796
Experience 29, Iter 93, disc loss: 9.910854115559851e-05, policy loss: 9.558608678779587
Experience 29, Iter 94, disc loss: 9.430898260243717e-05, policy loss: 9.821026977986062
Experience 29, Iter 95, disc loss: 8.749610719248834e-05, policy loss: 9.830204297847638
Experience 29, Iter 96, disc loss: 9.357626939923812e-05, policy loss: 9.694130355504155
Experience 29, Iter 97, disc loss: 9.280472874166582e-05, policy loss: 9.642761078841982
Experience 29, Iter 98, disc loss: 9.34918319138355e-05, policy loss: 9.767692051699221
Experience 29, Iter 99, disc loss: 0.00010213757888210908, policy loss: 9.703588064828288
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0035],
        [0.0131],
        [0.1752],
        [0.0034]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0204, 0.1262, 0.1548, 0.0046, 0.0018, 0.4907]],

        [[0.0204, 0.1262, 0.1548, 0.0046, 0.0018, 0.4907]],

        [[0.0204, 0.1262, 0.1548, 0.0046, 0.0018, 0.4907]],

        [[0.0204, 0.1262, 0.1548, 0.0046, 0.0018, 0.4907]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0138, 0.0523, 0.7007, 0.0136], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0138, 0.0523, 0.7007, 0.0136])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.246
Iter 2/2000 - Loss: -0.033
Iter 3/2000 - Loss: -0.370
Iter 4/2000 - Loss: -0.418
Iter 5/2000 - Loss: -0.381
Iter 6/2000 - Loss: -0.492
Iter 7/2000 - Loss: -0.680
Iter 8/2000 - Loss: -0.856
Iter 9/2000 - Loss: -1.011
Iter 10/2000 - Loss: -1.175
Iter 11/2000 - Loss: -1.371
Iter 12/2000 - Loss: -1.602
Iter 13/2000 - Loss: -1.862
Iter 14/2000 - Loss: -2.144
Iter 15/2000 - Loss: -2.442
Iter 16/2000 - Loss: -2.754
Iter 17/2000 - Loss: -3.075
Iter 18/2000 - Loss: -3.402
Iter 19/2000 - Loss: -3.731
Iter 20/2000 - Loss: -4.059
Iter 1981/2000 - Loss: -8.755
Iter 1982/2000 - Loss: -8.755
Iter 1983/2000 - Loss: -8.755
Iter 1984/2000 - Loss: -8.755
Iter 1985/2000 - Loss: -8.755
Iter 1986/2000 - Loss: -8.755
Iter 1987/2000 - Loss: -8.755
Iter 1988/2000 - Loss: -8.755
Iter 1989/2000 - Loss: -8.755
Iter 1990/2000 - Loss: -8.755
Iter 1991/2000 - Loss: -8.755
Iter 1992/2000 - Loss: -8.755
Iter 1993/2000 - Loss: -8.755
Iter 1994/2000 - Loss: -8.755
Iter 1995/2000 - Loss: -8.755
Iter 1996/2000 - Loss: -8.755
Iter 1997/2000 - Loss: -8.755
Iter 1998/2000 - Loss: -8.755
Iter 1999/2000 - Loss: -8.755
Iter 2000/2000 - Loss: -8.755
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[10.4959,  5.0058, 21.3762,  8.9918, 17.6281, 35.4775]],

        [[15.0639, 27.1824, 11.4075,  1.6254,  1.2203, 17.9535]],

        [[16.2967, 29.9213, 17.6716,  0.9902,  1.5786, 18.1050]],

        [[14.1310, 26.5724, 16.3487,  3.2021,  2.2806, 41.0898]]])
Signal Variance: tensor([ 0.0866,  0.9220, 13.1415,  0.3774])
Estimated target variance: tensor([0.0138, 0.0523, 0.7007, 0.0136])
N: 300
Signal to noise ratio: tensor([17.0193, 48.0239, 70.2800, 36.0520])
Bound on condition number: tensor([  86898.0563,  691890.7428, 1481784.9909,  389924.9028])
Policy Optimizer learning rate:
0.09699075226141834
Experience 30, Iter 0, disc loss: 9.853954584804873e-05, policy loss: 9.705053237706087
Experience 30, Iter 1, disc loss: 9.765317848037369e-05, policy loss: 9.865719289309569
Experience 30, Iter 2, disc loss: 8.728322156249346e-05, policy loss: 9.915251152383641
Experience 30, Iter 3, disc loss: 0.00010395151304852692, policy loss: 9.651273429893394
Experience 30, Iter 4, disc loss: 9.692732171328735e-05, policy loss: 9.745592883866319
Experience 30, Iter 5, disc loss: 9.232856327064656e-05, policy loss: 9.873143631105805
Experience 30, Iter 6, disc loss: 9.895589585044591e-05, policy loss: 9.676419543115866
Experience 30, Iter 7, disc loss: 9.01777647431459e-05, policy loss: 9.714521066088572
Experience 30, Iter 8, disc loss: 9.643750099444294e-05, policy loss: 9.675547642991306
Experience 30, Iter 9, disc loss: 9.238115699218959e-05, policy loss: 9.720355118699239
Experience 30, Iter 10, disc loss: 9.334853759548775e-05, policy loss: 9.768444707249163
Experience 30, Iter 11, disc loss: 9.887383302743515e-05, policy loss: 9.683766700106265
Experience 30, Iter 12, disc loss: 9.756493178491755e-05, policy loss: 9.746398220090818
Experience 30, Iter 13, disc loss: 9.492784124310856e-05, policy loss: 9.803845413271848
Experience 30, Iter 14, disc loss: 0.00010334706604978306, policy loss: 9.666726639901619
Experience 30, Iter 15, disc loss: 0.00010900261986892466, policy loss: 9.58968215659586
Experience 30, Iter 16, disc loss: 0.00011364335300878142, policy loss: 9.58877401078763
Experience 30, Iter 17, disc loss: 0.00010286177517828692, policy loss: 9.577862157660588
Experience 30, Iter 18, disc loss: 0.00011820541150914808, policy loss: 9.617815919775932
Experience 30, Iter 19, disc loss: 8.577702251044758e-05, policy loss: 9.902150622869467
Experience 30, Iter 20, disc loss: 9.778799323242039e-05, policy loss: 9.770579123700639
Experience 30, Iter 21, disc loss: 9.953344665909996e-05, policy loss: 9.645597813818274
Experience 30, Iter 22, disc loss: 0.00010849794243534662, policy loss: 9.634613050038668
Experience 30, Iter 23, disc loss: 9.411974484917034e-05, policy loss: 9.78276068054425
Experience 30, Iter 24, disc loss: 9.111679332756464e-05, policy loss: 9.795946631584867
Experience 30, Iter 25, disc loss: 8.840848573052573e-05, policy loss: 9.861636762544654
Experience 30, Iter 26, disc loss: 9.777209857162719e-05, policy loss: 9.752996211875557
Experience 30, Iter 27, disc loss: 9.252709677587555e-05, policy loss: 9.786108400937714
Experience 30, Iter 28, disc loss: 8.717532181446102e-05, policy loss: 9.815029060728987
Experience 30, Iter 29, disc loss: 9.561738407897231e-05, policy loss: 9.748346188323076
Experience 30, Iter 30, disc loss: 8.801943217822242e-05, policy loss: 9.762568180494114
Experience 30, Iter 31, disc loss: 9.019866963429588e-05, policy loss: 9.810754901938115
Experience 30, Iter 32, disc loss: 8.25608407870757e-05, policy loss: 9.944996478001736
Experience 30, Iter 33, disc loss: 8.366269253382942e-05, policy loss: 9.802917863372112
Experience 30, Iter 34, disc loss: 0.0001110809772764547, policy loss: 9.615109595956099
Experience 30, Iter 35, disc loss: 8.309131683179933e-05, policy loss: 9.797715712781132
Experience 30, Iter 36, disc loss: 8.791538520949039e-05, policy loss: 9.828966802215804
Experience 30, Iter 37, disc loss: 9.17715500711678e-05, policy loss: 9.794799146950949
Experience 30, Iter 38, disc loss: 9.60150961776128e-05, policy loss: 9.710802620825278
Experience 30, Iter 39, disc loss: 0.0001063109621215968, policy loss: 9.713792361580586
Experience 30, Iter 40, disc loss: 0.00011591882521653222, policy loss: 9.616120902625628
Experience 30, Iter 41, disc loss: 9.962721914773892e-05, policy loss: 9.760012892455254
Experience 30, Iter 42, disc loss: 0.00011418507588292572, policy loss: 9.442038147151376
Experience 30, Iter 43, disc loss: 9.265309906645295e-05, policy loss: 9.898186270782698
Experience 30, Iter 44, disc loss: 9.925205937019647e-05, policy loss: 9.77990318504533
Experience 30, Iter 45, disc loss: 8.108406687988857e-05, policy loss: 9.986854837648512
Experience 30, Iter 46, disc loss: 8.918818505704045e-05, policy loss: 9.893450163367852
Experience 30, Iter 47, disc loss: 0.00010555916853163735, policy loss: 9.622542474477761
Experience 30, Iter 48, disc loss: 8.281526618727226e-05, policy loss: 9.89208689331034
Experience 30, Iter 49, disc loss: 8.826165558070806e-05, policy loss: 9.985662161566976
Experience 30, Iter 50, disc loss: 9.193806483914199e-05, policy loss: 9.83431057618969
Experience 30, Iter 51, disc loss: 8.421327317289238e-05, policy loss: 9.912033457320916
Experience 30, Iter 52, disc loss: 9.424563540260267e-05, policy loss: 9.74101586866556
Experience 30, Iter 53, disc loss: 8.271974385747616e-05, policy loss: 9.797918334771712
Experience 30, Iter 54, disc loss: 0.0001064095754782772, policy loss: 9.64401602929986
Experience 30, Iter 55, disc loss: 8.594220044221025e-05, policy loss: 9.807359206893503
Experience 30, Iter 56, disc loss: 9.848046808593169e-05, policy loss: 9.655992170964455
Experience 30, Iter 57, disc loss: 7.947747198839437e-05, policy loss: 9.858761579850862
Experience 30, Iter 58, disc loss: 9.57553079990074e-05, policy loss: 9.737934108893107
Experience 30, Iter 59, disc loss: 8.588158059345496e-05, policy loss: 9.921140723141384
Experience 30, Iter 60, disc loss: 0.00010359315179414512, policy loss: 9.64237991551964
Experience 30, Iter 61, disc loss: 8.457775560504212e-05, policy loss: 9.893058368412465
Experience 30, Iter 62, disc loss: 9.914167608382401e-05, policy loss: 9.624823131793796
Experience 30, Iter 63, disc loss: 0.00010190654164827157, policy loss: 9.746271513938272
Experience 30, Iter 64, disc loss: 8.364531831649827e-05, policy loss: 9.905775429455236
Experience 30, Iter 65, disc loss: 8.911275836119327e-05, policy loss: 9.931211736207324
Experience 30, Iter 66, disc loss: 0.00010246352618456059, policy loss: 9.683757028818327
Experience 30, Iter 67, disc loss: 8.222969991521364e-05, policy loss: 9.995581170269968
Experience 30, Iter 68, disc loss: 8.0245493115776e-05, policy loss: 9.95621018801451
Experience 30, Iter 69, disc loss: 9.534227449407924e-05, policy loss: 9.670353525107782
Experience 30, Iter 70, disc loss: 8.781429520647147e-05, policy loss: 9.879907436831768
Experience 30, Iter 71, disc loss: 0.00010924274623270766, policy loss: 9.664333762751262
Experience 30, Iter 72, disc loss: 8.296088311727885e-05, policy loss: 9.810769754231524
Experience 30, Iter 73, disc loss: 8.885747739891357e-05, policy loss: 9.87835007110045
Experience 30, Iter 74, disc loss: 7.910159333493774e-05, policy loss: 9.900804785235483
Experience 30, Iter 75, disc loss: 8.739737690938133e-05, policy loss: 9.842274158633755
Experience 30, Iter 76, disc loss: 9.870620210144006e-05, policy loss: 9.745545036647155
Experience 30, Iter 77, disc loss: 7.915120902721101e-05, policy loss: 10.057885817190986
Experience 30, Iter 78, disc loss: 8.024162867875251e-05, policy loss: 10.219635749085818
Experience 30, Iter 79, disc loss: 9.882851254981826e-05, policy loss: 9.611612353975818
Experience 30, Iter 80, disc loss: 8.980206075419375e-05, policy loss: 9.839119015556186
Experience 30, Iter 81, disc loss: 8.264658373066035e-05, policy loss: 9.981230279433728
Experience 30, Iter 82, disc loss: 8.152705948353566e-05, policy loss: 9.820742268527761
Experience 30, Iter 83, disc loss: 0.000109119536223696, policy loss: 9.57389160428826
Experience 30, Iter 84, disc loss: 8.040448155700626e-05, policy loss: 9.943840273689675
Experience 30, Iter 85, disc loss: 9.32594632837149e-05, policy loss: 9.671166291324734
Experience 30, Iter 86, disc loss: 9.304870905429608e-05, policy loss: 9.79130799069539
Experience 30, Iter 87, disc loss: 7.930023418575491e-05, policy loss: 9.941997627084376
Experience 30, Iter 88, disc loss: 8.021737992478726e-05, policy loss: 10.009091490931272
Experience 30, Iter 89, disc loss: 8.436522735630774e-05, policy loss: 9.934922946842597
Experience 30, Iter 90, disc loss: 9.443447335164386e-05, policy loss: 9.763318338406405
Experience 30, Iter 91, disc loss: 8.658869968969215e-05, policy loss: 9.964600952375097
Experience 30, Iter 92, disc loss: 8.207235947697412e-05, policy loss: 9.951853802684223
Experience 30, Iter 93, disc loss: 0.00010342106286816994, policy loss: 9.582202709203063
Experience 30, Iter 94, disc loss: 8.428606198893571e-05, policy loss: 9.962120992733993
Experience 30, Iter 95, disc loss: 8.320562993426922e-05, policy loss: 9.922159033072653
Experience 30, Iter 96, disc loss: 0.0001050099926830761, policy loss: 9.749323968346905
Experience 30, Iter 97, disc loss: 9.67904684155033e-05, policy loss: 9.685979595088911
Experience 30, Iter 98, disc loss: 8.577911566051469e-05, policy loss: 9.787038979642613
Experience 30, Iter 99, disc loss: 0.00010105336394153913, policy loss: 9.762124731635229
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0127],
        [0.1697],
        [0.0033]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0198, 0.1223, 0.1500, 0.0044, 0.0017, 0.4759]],

        [[0.0198, 0.1223, 0.1500, 0.0044, 0.0017, 0.4759]],

        [[0.0198, 0.1223, 0.1500, 0.0044, 0.0017, 0.4759]],

        [[0.0198, 0.1223, 0.1500, 0.0044, 0.0017, 0.4759]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0134, 0.0507, 0.6789, 0.0132], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0134, 0.0507, 0.6789, 0.0132])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.308
Iter 2/2000 - Loss: -0.078
Iter 3/2000 - Loss: -0.429
Iter 4/2000 - Loss: -0.475
Iter 5/2000 - Loss: -0.431
Iter 6/2000 - Loss: -0.543
Iter 7/2000 - Loss: -0.734
Iter 8/2000 - Loss: -0.913
Iter 9/2000 - Loss: -1.068
Iter 10/2000 - Loss: -1.231
Iter 11/2000 - Loss: -1.426
Iter 12/2000 - Loss: -1.656
Iter 13/2000 - Loss: -1.916
Iter 14/2000 - Loss: -2.198
Iter 15/2000 - Loss: -2.496
Iter 16/2000 - Loss: -2.806
Iter 17/2000 - Loss: -3.125
Iter 18/2000 - Loss: -3.450
Iter 19/2000 - Loss: -3.777
Iter 20/2000 - Loss: -4.103
Iter 1981/2000 - Loss: -8.749
Iter 1982/2000 - Loss: -8.749
Iter 1983/2000 - Loss: -8.749
Iter 1984/2000 - Loss: -8.749
Iter 1985/2000 - Loss: -8.750
Iter 1986/2000 - Loss: -8.750
Iter 1987/2000 - Loss: -8.750
Iter 1988/2000 - Loss: -8.750
Iter 1989/2000 - Loss: -8.750
Iter 1990/2000 - Loss: -8.750
Iter 1991/2000 - Loss: -8.750
Iter 1992/2000 - Loss: -8.750
Iter 1993/2000 - Loss: -8.750
Iter 1994/2000 - Loss: -8.750
Iter 1995/2000 - Loss: -8.750
Iter 1996/2000 - Loss: -8.750
Iter 1997/2000 - Loss: -8.750
Iter 1998/2000 - Loss: -8.750
Iter 1999/2000 - Loss: -8.750
Iter 2000/2000 - Loss: -8.750
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[10.3602,  5.0041, 21.2103,  8.8508, 17.5507, 35.1893]],

        [[14.8164, 26.7040, 11.5230,  1.7565,  1.1763, 18.1903]],

        [[16.0577, 29.4848, 17.5636,  0.9871,  1.5760, 17.9966]],

        [[13.9683, 26.1830, 16.2654,  3.1754,  2.2639, 40.6986]]])
Signal Variance: tensor([ 0.0863,  0.9625, 12.9387,  0.3716])
Estimated target variance: tensor([0.0134, 0.0507, 0.6789, 0.0132])
N: 310
Signal to noise ratio: tensor([16.8460, 48.6926, 69.5101, 35.6293])
Bound on condition number: tensor([  87974.7422,  735001.1767, 1497812.7223,  393530.4467])
Policy Optimizer learning rate:
0.09688861611972639
Experience 31, Iter 0, disc loss: 8.477217697724901e-05, policy loss: 9.854719800514488
Experience 31, Iter 1, disc loss: 0.00010256220768165002, policy loss: 9.64039059388011
Experience 31, Iter 2, disc loss: 9.766610688145435e-05, policy loss: 9.850833386094113
Experience 31, Iter 3, disc loss: 9.070674475779514e-05, policy loss: 9.838864682923736
Experience 31, Iter 4, disc loss: 0.00011087189537400658, policy loss: 9.554073792486843
Experience 31, Iter 5, disc loss: 8.357942693752076e-05, policy loss: 9.910753234784277
Experience 31, Iter 6, disc loss: 8.458660839640376e-05, policy loss: 9.866270330348797
Experience 31, Iter 7, disc loss: 8.883723175964391e-05, policy loss: 9.710980568027935
Experience 31, Iter 8, disc loss: 9.873489999526266e-05, policy loss: 9.745295556848774
Experience 31, Iter 9, disc loss: 7.620723694731435e-05, policy loss: 9.968421909382847
Experience 31, Iter 10, disc loss: 8.58799749545259e-05, policy loss: 9.833209250863181
Experience 31, Iter 11, disc loss: 9.84996033828537e-05, policy loss: 9.757227362916082
Experience 31, Iter 12, disc loss: 8.515957043100876e-05, policy loss: 9.907427894814827
Experience 31, Iter 13, disc loss: 9.598254646009288e-05, policy loss: 9.658610013061235
Experience 31, Iter 14, disc loss: 9.752731776023387e-05, policy loss: 9.813845176562111
Experience 31, Iter 15, disc loss: 9.237938296117059e-05, policy loss: 9.679042747186525
Experience 31, Iter 16, disc loss: 8.293762099509254e-05, policy loss: 10.01736893167088
Experience 31, Iter 17, disc loss: 8.534368549127477e-05, policy loss: 9.832030109743094
Experience 31, Iter 18, disc loss: 9.092831462134996e-05, policy loss: 9.743130565984547
Experience 31, Iter 19, disc loss: 7.848279971010822e-05, policy loss: 9.967962077975345
Experience 31, Iter 20, disc loss: 8.50084381175074e-05, policy loss: 9.902672947259
Experience 31, Iter 21, disc loss: 8.591072125041822e-05, policy loss: 9.859931365428299
Experience 31, Iter 22, disc loss: 9.066663789068536e-05, policy loss: 9.856170629073894
Experience 31, Iter 23, disc loss: 7.786506390630081e-05, policy loss: 9.939167888406047
Experience 31, Iter 24, disc loss: 8.780515147090645e-05, policy loss: 9.953163136252268
Experience 31, Iter 25, disc loss: 8.34665233725833e-05, policy loss: 9.811541155567326
Experience 31, Iter 26, disc loss: 7.657922892637887e-05, policy loss: 9.931238608963746
Experience 31, Iter 27, disc loss: 8.27435388312152e-05, policy loss: 9.875634813624096
Experience 31, Iter 28, disc loss: 8.777244644409781e-05, policy loss: 9.731357868784805
Experience 31, Iter 29, disc loss: 7.714911425590998e-05, policy loss: 9.967410859718978
Experience 31, Iter 30, disc loss: 8.467339420078792e-05, policy loss: 9.80519894415504
Experience 31, Iter 31, disc loss: 6.713357124223007e-05, policy loss: 10.178663004875936
Experience 31, Iter 32, disc loss: 8.862955837481766e-05, policy loss: 9.830444647084985
Experience 31, Iter 33, disc loss: 0.00010933270111141165, policy loss: 9.54420148086703
Experience 31, Iter 34, disc loss: 8.278841615696685e-05, policy loss: 9.847128130041177
Experience 31, Iter 35, disc loss: 9.547004668225753e-05, policy loss: 9.660799503227093
Experience 31, Iter 36, disc loss: 8.374391140720093e-05, policy loss: 9.914322272264338
Experience 31, Iter 37, disc loss: 8.348956681260572e-05, policy loss: 9.923286241907366
Experience 31, Iter 38, disc loss: 8.975566898060494e-05, policy loss: 9.858343002391258
Experience 31, Iter 39, disc loss: 7.971096951588504e-05, policy loss: 9.978996403611266
Experience 31, Iter 40, disc loss: 8.19350908557398e-05, policy loss: 9.918554894212743
Experience 31, Iter 41, disc loss: 8.552003395009086e-05, policy loss: 9.938651369595712
Experience 31, Iter 42, disc loss: 9.025960782082428e-05, policy loss: 9.818064076270097
Experience 31, Iter 43, disc loss: 7.7428804671814e-05, policy loss: 9.979350195030781
Experience 31, Iter 44, disc loss: 9.840882739514683e-05, policy loss: 9.798631715484246
Experience 31, Iter 45, disc loss: 8.163221034297163e-05, policy loss: 9.89007533097957
Experience 31, Iter 46, disc loss: 9.30158492040802e-05, policy loss: 9.684068101127096
Experience 31, Iter 47, disc loss: 0.00010842240421898916, policy loss: 9.606487459146075
Experience 31, Iter 48, disc loss: 0.00010411570228726303, policy loss: 9.750346650612133
Experience 31, Iter 49, disc loss: 9.402761948863915e-05, policy loss: 9.838958339114896
Experience 31, Iter 50, disc loss: 8.515734259201386e-05, policy loss: 9.870310685986745
Experience 31, Iter 51, disc loss: 9.675087325397696e-05, policy loss: 9.865225635802656
Experience 31, Iter 52, disc loss: 0.00010191928872420207, policy loss: 9.59536570023878
Experience 31, Iter 53, disc loss: 9.395741391233534e-05, policy loss: 9.765065636149332
Experience 31, Iter 54, disc loss: 9.639047680562657e-05, policy loss: 9.76434616537597
Experience 31, Iter 55, disc loss: 0.00010135100060604629, policy loss: 9.703301645401998
Experience 31, Iter 56, disc loss: 8.6879726600536e-05, policy loss: 9.907002039515916
Experience 31, Iter 57, disc loss: 9.330997744515312e-05, policy loss: 9.74057541785131
Experience 31, Iter 58, disc loss: 7.84660114520793e-05, policy loss: 10.089779539849687
Experience 31, Iter 59, disc loss: 7.33666151877206e-05, policy loss: 10.1052995671695
Experience 31, Iter 60, disc loss: 8.021814741138305e-05, policy loss: 9.976909832955126
Experience 31, Iter 61, disc loss: 7.054167001725704e-05, policy loss: 10.10719127650215
Experience 31, Iter 62, disc loss: 8.897357812561452e-05, policy loss: 9.6445438369799
Experience 31, Iter 63, disc loss: 8.408084454545441e-05, policy loss: 9.738438465086832
Experience 31, Iter 64, disc loss: 8.462330506916256e-05, policy loss: 9.873864240233985
Experience 31, Iter 65, disc loss: 9.44203569060505e-05, policy loss: 9.783851034490263
Experience 31, Iter 66, disc loss: 8.498724961088845e-05, policy loss: 9.849395000225513
Experience 31, Iter 67, disc loss: 7.881140365273196e-05, policy loss: 10.015234606597685
Experience 31, Iter 68, disc loss: 0.0001000740214678967, policy loss: 9.672864599388841
Experience 31, Iter 69, disc loss: 8.425394760706483e-05, policy loss: 9.902854021839524
Experience 31, Iter 70, disc loss: 8.400389798554396e-05, policy loss: 9.872040819349687
Experience 31, Iter 71, disc loss: 8.600065662063775e-05, policy loss: 9.92643207436388
Experience 31, Iter 72, disc loss: 7.946903022996094e-05, policy loss: 9.997958214246966
Experience 31, Iter 73, disc loss: 8.679693032627126e-05, policy loss: 10.084151880903114
Experience 31, Iter 74, disc loss: 8.15716490225195e-05, policy loss: 9.974679798836458
Experience 31, Iter 75, disc loss: 8.108939720731218e-05, policy loss: 9.97728782880563
Experience 31, Iter 76, disc loss: 8.910473511623758e-05, policy loss: 9.705765033499294
Experience 31, Iter 77, disc loss: 8.684651475076823e-05, policy loss: 9.750261406815117
Experience 31, Iter 78, disc loss: 8.176136515071655e-05, policy loss: 9.811597412626792
Experience 31, Iter 79, disc loss: 7.816527912958636e-05, policy loss: 9.965813831260048
Experience 31, Iter 80, disc loss: 0.00010418185892986104, policy loss: 9.836580046744132
Experience 31, Iter 81, disc loss: 8.69755167673026e-05, policy loss: 9.785725317869574
Experience 31, Iter 82, disc loss: 7.641505494497855e-05, policy loss: 9.97732633738248
Experience 31, Iter 83, disc loss: 9.842005415197587e-05, policy loss: 9.764480369356752
Experience 31, Iter 84, disc loss: 6.139562472339601e-05, policy loss: 10.319061264222553
Experience 31, Iter 85, disc loss: 7.219712434167845e-05, policy loss: 10.064692235663989
Experience 31, Iter 86, disc loss: 8.165719849104987e-05, policy loss: 9.88251183241366
Experience 31, Iter 87, disc loss: 8.832837607704945e-05, policy loss: 9.95399571733811
Experience 31, Iter 88, disc loss: 9.456575492058613e-05, policy loss: 9.863706932072578
Experience 31, Iter 89, disc loss: 8.768621893285404e-05, policy loss: 9.831658526469004
Experience 31, Iter 90, disc loss: 7.120092003651214e-05, policy loss: 10.11021007579657
Experience 31, Iter 91, disc loss: 8.378613160293615e-05, policy loss: 9.762143844030241
Experience 31, Iter 92, disc loss: 7.477208874583306e-05, policy loss: 9.860723690845505
Experience 31, Iter 93, disc loss: 7.430348892978736e-05, policy loss: 9.983379749083934
Experience 31, Iter 94, disc loss: 7.589123985011812e-05, policy loss: 9.883124128736478
Experience 31, Iter 95, disc loss: 8.271539870969546e-05, policy loss: 9.848962429701327
Experience 31, Iter 96, disc loss: 0.00010028076533935273, policy loss: 9.845924742795837
Experience 31, Iter 97, disc loss: 8.51502120068317e-05, policy loss: 9.857075802231503
Experience 31, Iter 98, disc loss: 7.257472818404819e-05, policy loss: 10.142403713114167
Experience 31, Iter 99, disc loss: 8.13480059727073e-05, policy loss: 9.962597798002552
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0033],
        [0.0124],
        [0.1685],
        [0.0033]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0192, 0.1188, 0.1488, 0.0044, 0.0017, 0.4618]],

        [[0.0192, 0.1188, 0.1488, 0.0044, 0.0017, 0.4618]],

        [[0.0192, 0.1188, 0.1488, 0.0044, 0.0017, 0.4618]],

        [[0.0192, 0.1188, 0.1488, 0.0044, 0.0017, 0.4618]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0130, 0.0494, 0.6741, 0.0131], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0130, 0.0494, 0.6741, 0.0131])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.337
Iter 2/2000 - Loss: -0.087
Iter 3/2000 - Loss: -0.450
Iter 4/2000 - Loss: -0.491
Iter 5/2000 - Loss: -0.438
Iter 6/2000 - Loss: -0.547
Iter 7/2000 - Loss: -0.737
Iter 8/2000 - Loss: -0.913
Iter 9/2000 - Loss: -1.061
Iter 10/2000 - Loss: -1.215
Iter 11/2000 - Loss: -1.404
Iter 12/2000 - Loss: -1.629
Iter 13/2000 - Loss: -1.885
Iter 14/2000 - Loss: -2.162
Iter 15/2000 - Loss: -2.455
Iter 16/2000 - Loss: -2.762
Iter 17/2000 - Loss: -3.079
Iter 18/2000 - Loss: -3.404
Iter 19/2000 - Loss: -3.733
Iter 20/2000 - Loss: -4.062
Iter 1981/2000 - Loss: -8.776
Iter 1982/2000 - Loss: -8.776
Iter 1983/2000 - Loss: -8.776
Iter 1984/2000 - Loss: -8.776
Iter 1985/2000 - Loss: -8.776
Iter 1986/2000 - Loss: -8.776
Iter 1987/2000 - Loss: -8.776
Iter 1988/2000 - Loss: -8.776
Iter 1989/2000 - Loss: -8.776
Iter 1990/2000 - Loss: -8.776
Iter 1991/2000 - Loss: -8.776
Iter 1992/2000 - Loss: -8.776
Iter 1993/2000 - Loss: -8.776
Iter 1994/2000 - Loss: -8.776
Iter 1995/2000 - Loss: -8.776
Iter 1996/2000 - Loss: -8.776
Iter 1997/2000 - Loss: -8.776
Iter 1998/2000 - Loss: -8.776
Iter 1999/2000 - Loss: -8.776
Iter 2000/2000 - Loss: -8.776
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[10.4166,  5.0232, 21.2413,  8.8460, 16.7568, 34.1378]],

        [[14.7060, 26.3530, 11.6784,  1.9899,  1.1017, 18.1017]],

        [[15.9338, 28.8859, 17.7760,  0.9753,  1.5760, 17.4807]],

        [[14.0811, 26.3148, 16.0924,  3.2959,  2.2961, 40.8520]]])
Signal Variance: tensor([ 0.0847,  0.9895, 12.6237,  0.3684])
Estimated target variance: tensor([0.0130, 0.0494, 0.6741, 0.0131])
N: 320
Signal to noise ratio: tensor([16.7873, 49.6705, 68.7397, 35.4034])
Bound on condition number: tensor([  90180.9753,  789491.0809, 1512049.2901,  401089.8391])
Policy Optimizer learning rate:
0.09678658753253007
Experience 32, Iter 0, disc loss: 9.850252996270228e-05, policy loss: 9.76911993131507
Experience 32, Iter 1, disc loss: 0.00010431383047250473, policy loss: 9.968999467240689
Experience 32, Iter 2, disc loss: 9.618293996394391e-05, policy loss: 9.702806414021431
Experience 32, Iter 3, disc loss: 8.865805497721629e-05, policy loss: 9.765712161486077
Experience 32, Iter 4, disc loss: 9.299607091266494e-05, policy loss: 9.704310643529874
Experience 32, Iter 5, disc loss: 8.533699895599606e-05, policy loss: 9.915141544593855
Experience 32, Iter 6, disc loss: 8.098500544996651e-05, policy loss: 10.048778749251085
Experience 32, Iter 7, disc loss: 9.137774185070398e-05, policy loss: 9.791181960481964
Experience 32, Iter 8, disc loss: 7.329382976872315e-05, policy loss: 10.028271202399726
Experience 32, Iter 9, disc loss: 0.00010955193921829733, policy loss: 9.6097739827225
Experience 32, Iter 10, disc loss: 7.84558814125036e-05, policy loss: 9.952502087395528
Experience 32, Iter 11, disc loss: 8.475658861503542e-05, policy loss: 9.821023700068835
Experience 32, Iter 12, disc loss: 8.22943202095651e-05, policy loss: 9.862736291553581
Experience 32, Iter 13, disc loss: 7.8562677174796e-05, policy loss: 10.003403811151387
Experience 32, Iter 14, disc loss: 9.849448153018412e-05, policy loss: 9.688585032852048
Experience 32, Iter 15, disc loss: 8.868901710385581e-05, policy loss: 9.934015627454713
Experience 32, Iter 16, disc loss: 7.27018964781507e-05, policy loss: 10.011968340871725
Experience 32, Iter 17, disc loss: 9.814802917959615e-05, policy loss: 9.744003067579554
Experience 32, Iter 18, disc loss: 0.00010001689951601763, policy loss: 9.68392287303655
Experience 32, Iter 19, disc loss: 7.034215119334788e-05, policy loss: 10.091612775873699
Experience 32, Iter 20, disc loss: 8.061562866641257e-05, policy loss: 9.975276819907597
Experience 32, Iter 21, disc loss: 8.511812595480909e-05, policy loss: 9.792154580057252
Experience 32, Iter 22, disc loss: 6.83366741766205e-05, policy loss: 10.221524858281496
Experience 32, Iter 23, disc loss: 8.29614965796554e-05, policy loss: 9.942596813648077
Experience 32, Iter 24, disc loss: 7.918944128536153e-05, policy loss: 9.888132517832297
Experience 32, Iter 25, disc loss: 8.050324904300751e-05, policy loss: 9.86932870043898
Experience 32, Iter 26, disc loss: 8.374243114645332e-05, policy loss: 9.720879501017897
Experience 32, Iter 27, disc loss: 7.852146223712209e-05, policy loss: 10.049734053826988
Experience 32, Iter 28, disc loss: 9.555855570108153e-05, policy loss: 9.733712081336044
Experience 32, Iter 29, disc loss: 8.355556673639617e-05, policy loss: 9.818743231351082
Experience 32, Iter 30, disc loss: 0.000146464795334771, policy loss: 9.710157609020511
Experience 32, Iter 31, disc loss: 8.722350910994722e-05, policy loss: 9.972108279082036
Experience 32, Iter 32, disc loss: 8.961812476092649e-05, policy loss: 9.885650334430057
Experience 32, Iter 33, disc loss: 9.09321079502152e-05, policy loss: 9.794824659302487
Experience 32, Iter 34, disc loss: 6.158836046276799e-05, policy loss: 10.258487896293861
Experience 32, Iter 35, disc loss: 7.604823254299013e-05, policy loss: 9.938431387495092
Experience 32, Iter 36, disc loss: 9.06193992791329e-05, policy loss: 9.971053178843157
Experience 32, Iter 37, disc loss: 8.159018635359527e-05, policy loss: 10.032529866546454
Experience 32, Iter 38, disc loss: 7.847718646992327e-05, policy loss: 9.857758480659172
Experience 32, Iter 39, disc loss: 7.290146189477663e-05, policy loss: 9.94499867464981
Experience 32, Iter 40, disc loss: 7.822525921422726e-05, policy loss: 9.851591486918842
Experience 32, Iter 41, disc loss: 8.022934347917751e-05, policy loss: 9.959618074311908
Experience 32, Iter 42, disc loss: 8.561041151505959e-05, policy loss: 9.738465355206497
Experience 32, Iter 43, disc loss: 7.066929710921955e-05, policy loss: 10.16036804288465
Experience 32, Iter 44, disc loss: 8.129667725869474e-05, policy loss: 9.885319483923325
Experience 32, Iter 45, disc loss: 9.425212234597323e-05, policy loss: 9.938530133364868
Experience 32, Iter 46, disc loss: 9.032710562202207e-05, policy loss: 9.796582483559266
Experience 32, Iter 47, disc loss: 8.229802344607901e-05, policy loss: 9.972999198828791
Experience 32, Iter 48, disc loss: 8.96002810826706e-05, policy loss: 9.982406478774632
Experience 32, Iter 49, disc loss: 8.530349708340502e-05, policy loss: 9.757278506568454
Experience 32, Iter 50, disc loss: 7.5940666725757e-05, policy loss: 10.10508093159299
Experience 32, Iter 51, disc loss: 9.07392670707708e-05, policy loss: 9.838293774199535
Experience 32, Iter 52, disc loss: 6.904937748970948e-05, policy loss: 10.067503529105407
Experience 32, Iter 53, disc loss: 9.116872801173227e-05, policy loss: 9.753041032639342
Experience 32, Iter 54, disc loss: 7.382959313379974e-05, policy loss: 10.013111355810674
Experience 32, Iter 55, disc loss: 7.490305890180514e-05, policy loss: 9.933175868046364
Experience 32, Iter 56, disc loss: 7.471618241991818e-05, policy loss: 10.072080778236682
Experience 32, Iter 57, disc loss: 6.658179724724044e-05, policy loss: 10.142394681009105
Experience 32, Iter 58, disc loss: 7.29233415428081e-05, policy loss: 9.99595019151923
Experience 32, Iter 59, disc loss: 7.096875491144575e-05, policy loss: 9.967160679306225
Experience 32, Iter 60, disc loss: 7.157103141343661e-05, policy loss: 10.10508816483645
Experience 32, Iter 61, disc loss: 7.149245070791309e-05, policy loss: 9.944449243505627
Experience 32, Iter 62, disc loss: 6.371068297986281e-05, policy loss: 10.308980463350615
Experience 32, Iter 63, disc loss: 7.379943691402437e-05, policy loss: 10.017671025084905
Experience 32, Iter 64, disc loss: 8.934899421216816e-05, policy loss: 9.806603811024221
Experience 32, Iter 65, disc loss: 7.161082484359676e-05, policy loss: 10.068151417589622
Experience 32, Iter 66, disc loss: 7.424885546360035e-05, policy loss: 9.990822843733547
Experience 32, Iter 67, disc loss: 0.00010275818513219231, policy loss: 9.915041961845777
Experience 32, Iter 68, disc loss: 7.851105603143396e-05, policy loss: 10.047833249825402
Experience 32, Iter 69, disc loss: 8.600924852132503e-05, policy loss: 9.944357768663039
Experience 32, Iter 70, disc loss: 7.79906236911722e-05, policy loss: 9.97732845915663
Experience 32, Iter 71, disc loss: 9.00281286567982e-05, policy loss: 9.814981629465091
Experience 32, Iter 72, disc loss: 7.53783277426479e-05, policy loss: 9.960062573596707
Experience 32, Iter 73, disc loss: 9.044791735485936e-05, policy loss: 10.025332602135963
Experience 32, Iter 74, disc loss: 6.764710785337553e-05, policy loss: 10.152162511589694
Experience 32, Iter 75, disc loss: 7.846224892603158e-05, policy loss: 9.850873757558546
Experience 32, Iter 76, disc loss: 7.228942170985632e-05, policy loss: 10.030837956218932
Experience 32, Iter 77, disc loss: 6.671642544782446e-05, policy loss: 10.09293203078261
Experience 32, Iter 78, disc loss: 8.247411362666607e-05, policy loss: 9.883615894373692
Experience 32, Iter 79, disc loss: 7.383413415093503e-05, policy loss: 9.93192865159257
Experience 32, Iter 80, disc loss: 8.018118097069299e-05, policy loss: 9.822882540439785
Experience 32, Iter 81, disc loss: 6.785237667868091e-05, policy loss: 10.082998560824247
Experience 32, Iter 82, disc loss: 7.784820461286695e-05, policy loss: 10.111373915993894
Experience 32, Iter 83, disc loss: 6.370666203808584e-05, policy loss: 10.18755226498358
Experience 32, Iter 84, disc loss: 7.506071382810099e-05, policy loss: 10.084182106289795
Experience 32, Iter 85, disc loss: 9.093721112414033e-05, policy loss: 9.790759958835363
Experience 32, Iter 86, disc loss: 8.261297878610176e-05, policy loss: 9.877968461034948
Experience 32, Iter 87, disc loss: 7.330391519289782e-05, policy loss: 9.929444378605705
Experience 32, Iter 88, disc loss: 8.145215582882115e-05, policy loss: 9.948813591176151
Experience 32, Iter 89, disc loss: 7.465751197766081e-05, policy loss: 10.040865618596522
Experience 32, Iter 90, disc loss: 9.270424477098519e-05, policy loss: 9.798710337002566
Experience 32, Iter 91, disc loss: 8.091861515399915e-05, policy loss: 9.912397381853335
Experience 32, Iter 92, disc loss: 6.758840936404698e-05, policy loss: 10.127093873925388
Experience 32, Iter 93, disc loss: 7.439349081243597e-05, policy loss: 10.074761342268971
Experience 32, Iter 94, disc loss: 6.673704719412324e-05, policy loss: 10.081576957182342
Experience 32, Iter 95, disc loss: 6.696307455591052e-05, policy loss: 10.107531695035892
Experience 32, Iter 96, disc loss: 8.232804091631531e-05, policy loss: 9.89895384672197
Experience 32, Iter 97, disc loss: 7.753488554361637e-05, policy loss: 9.927168661382758
Experience 32, Iter 98, disc loss: 6.583922660753446e-05, policy loss: 10.135162220865134
Experience 32, Iter 99, disc loss: 5.998075921180107e-05, policy loss: 10.262356668285099
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0120],
        [0.1636],
        [0.0032]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0186, 0.1154, 0.1445, 0.0043, 0.0016, 0.4488]],

        [[0.0186, 0.1154, 0.1445, 0.0043, 0.0016, 0.4488]],

        [[0.0186, 0.1154, 0.1445, 0.0043, 0.0016, 0.4488]],

        [[0.0186, 0.1154, 0.1445, 0.0043, 0.0016, 0.4488]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0127, 0.0480, 0.6544, 0.0127], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0127, 0.0480, 0.6544, 0.0127])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.395
Iter 2/2000 - Loss: -0.132
Iter 3/2000 - Loss: -0.508
Iter 4/2000 - Loss: -0.548
Iter 5/2000 - Loss: -0.491
Iter 6/2000 - Loss: -0.602
Iter 7/2000 - Loss: -0.797
Iter 8/2000 - Loss: -0.977
Iter 9/2000 - Loss: -1.127
Iter 10/2000 - Loss: -1.282
Iter 11/2000 - Loss: -1.471
Iter 12/2000 - Loss: -1.697
Iter 13/2000 - Loss: -1.954
Iter 14/2000 - Loss: -2.231
Iter 15/2000 - Loss: -2.523
Iter 16/2000 - Loss: -2.828
Iter 17/2000 - Loss: -3.143
Iter 18/2000 - Loss: -3.466
Iter 19/2000 - Loss: -3.793
Iter 20/2000 - Loss: -4.120
Iter 1981/2000 - Loss: -8.800
Iter 1982/2000 - Loss: -8.800
Iter 1983/2000 - Loss: -8.800
Iter 1984/2000 - Loss: -8.800
Iter 1985/2000 - Loss: -8.800
Iter 1986/2000 - Loss: -8.800
Iter 1987/2000 - Loss: -8.800
Iter 1988/2000 - Loss: -8.800
Iter 1989/2000 - Loss: -8.800
Iter 1990/2000 - Loss: -8.800
Iter 1991/2000 - Loss: -8.800
Iter 1992/2000 - Loss: -8.800
Iter 1993/2000 - Loss: -8.800
Iter 1994/2000 - Loss: -8.800
Iter 1995/2000 - Loss: -8.800
Iter 1996/2000 - Loss: -8.800
Iter 1997/2000 - Loss: -8.800
Iter 1998/2000 - Loss: -8.800
Iter 1999/2000 - Loss: -8.800
Iter 2000/2000 - Loss: -8.800
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.4291,  5.0240, 21.2016,  8.8228, 16.8515, 34.5807]],

        [[14.5524, 26.1664, 11.7843,  2.2438,  1.0700, 18.5536]],

        [[15.6917, 28.4715, 17.8188,  0.9681,  1.5450, 17.3850]],

        [[13.9211, 25.9140, 15.9520,  3.2158,  2.2288, 40.5672]]])
Signal Variance: tensor([ 0.0847,  1.0618, 12.5305,  0.3606])
Estimated target variance: tensor([0.0127, 0.0480, 0.6544, 0.0127])
N: 330
Signal to noise ratio: tensor([16.5719, 51.8438, 69.2771, 35.1996])
Bound on condition number: tensor([  90628.4179,  886967.8056, 1583776.8577,  408875.6994])
Policy Optimizer learning rate:
0.09668466638656907
Experience 33, Iter 0, disc loss: 0.00010469359175143412, policy loss: 9.645553202075508
Experience 33, Iter 1, disc loss: 7.578918055486174e-05, policy loss: 9.976436916559544
Experience 33, Iter 2, disc loss: 9.443924807305519e-05, policy loss: 9.866262043686602
Experience 33, Iter 3, disc loss: 9.292465483692152e-05, policy loss: 9.724654862368109
Experience 33, Iter 4, disc loss: 8.610305805009029e-05, policy loss: 9.873766735670493
Experience 33, Iter 5, disc loss: 8.425829330163796e-05, policy loss: 10.09483857614484
Experience 33, Iter 6, disc loss: 7.752272909313791e-05, policy loss: 9.960236488548786
Experience 33, Iter 7, disc loss: 7.643154012008488e-05, policy loss: 9.943682661982276
Experience 33, Iter 8, disc loss: 9.001804814620091e-05, policy loss: 9.91221383597427
Experience 33, Iter 9, disc loss: 8.376904952198103e-05, policy loss: 9.989638885907054
Experience 33, Iter 10, disc loss: 6.392278783170354e-05, policy loss: 10.23229427476585
Experience 33, Iter 11, disc loss: 6.990452363493188e-05, policy loss: 10.180755985987663
Experience 33, Iter 12, disc loss: 7.111648866461622e-05, policy loss: 10.011306709173015
Experience 33, Iter 13, disc loss: 8.354211281116385e-05, policy loss: 10.023764839164611
Experience 33, Iter 14, disc loss: 7.430030294682235e-05, policy loss: 9.926495885043474
Experience 33, Iter 15, disc loss: 8.35979698459638e-05, policy loss: 9.989912117745334
Experience 33, Iter 16, disc loss: 7.169181684372503e-05, policy loss: 9.984060563380996
Experience 33, Iter 17, disc loss: 6.79712581113047e-05, policy loss: 10.053381449374536
Experience 33, Iter 18, disc loss: 7.393974765961634e-05, policy loss: 10.027441462420937
Experience 33, Iter 19, disc loss: 7.951027222329054e-05, policy loss: 9.97219092155352
Experience 33, Iter 20, disc loss: 7.371382330602315e-05, policy loss: 10.089550984229444
Experience 33, Iter 21, disc loss: 0.00010113168298973675, policy loss: 9.620281221620928
Experience 33, Iter 22, disc loss: 8.366421406240045e-05, policy loss: 10.055166236349084
Experience 33, Iter 23, disc loss: 7.853923275881297e-05, policy loss: 9.94078227926374
Experience 33, Iter 24, disc loss: 7.270971235400885e-05, policy loss: 10.07739155552262
Experience 33, Iter 25, disc loss: 8.081360733045047e-05, policy loss: 10.062787689864635
Experience 33, Iter 26, disc loss: 7.210420123576771e-05, policy loss: 10.051874345309425
Experience 33, Iter 27, disc loss: 6.785315534528716e-05, policy loss: 10.194178519301925
Experience 33, Iter 28, disc loss: 7.99855815643501e-05, policy loss: 9.955825175654336
Experience 33, Iter 29, disc loss: 7.005043261493202e-05, policy loss: 10.089611376179844
Experience 33, Iter 30, disc loss: 7.510310826808856e-05, policy loss: 9.842595978470056
Experience 33, Iter 31, disc loss: 6.545559810224976e-05, policy loss: 10.271962488442444
Experience 33, Iter 32, disc loss: 7.247391263069913e-05, policy loss: 10.027341948743896
Experience 33, Iter 33, disc loss: 7.783751734283798e-05, policy loss: 9.827542424565564
Experience 33, Iter 34, disc loss: 7.916203471190325e-05, policy loss: 10.004706574473142
Experience 33, Iter 35, disc loss: 7.520359029459762e-05, policy loss: 9.98505174866986
Experience 33, Iter 36, disc loss: 8.837629994539855e-05, policy loss: 9.818607705393447
Experience 33, Iter 37, disc loss: 7.849598264714464e-05, policy loss: 9.987611506943908
Experience 33, Iter 38, disc loss: 7.645135687843856e-05, policy loss: 10.14984925892721
Experience 33, Iter 39, disc loss: 9.701907850939201e-05, policy loss: 9.928969053458074
Experience 33, Iter 40, disc loss: 7.695615643226506e-05, policy loss: 9.992559015855047
Experience 33, Iter 41, disc loss: 7.679840083863181e-05, policy loss: 9.964015360734331
Experience 33, Iter 42, disc loss: 7.427961166749534e-05, policy loss: 9.995783461162524
Experience 33, Iter 43, disc loss: 8.705785838166817e-05, policy loss: 9.720801542592772
Experience 33, Iter 44, disc loss: 7.511325688833325e-05, policy loss: 9.885219056700354
Experience 33, Iter 45, disc loss: 7.186187544066027e-05, policy loss: 10.046964301904318
Experience 33, Iter 46, disc loss: 5.934107288749564e-05, policy loss: 10.206764249199043
Experience 33, Iter 47, disc loss: 9.255057646989648e-05, policy loss: 9.69327752728014
Experience 33, Iter 48, disc loss: 7.415874137965269e-05, policy loss: 10.02541834511409
Experience 33, Iter 49, disc loss: 5.967908631253945e-05, policy loss: 10.257673198052075
Experience 33, Iter 50, disc loss: 6.213953567851387e-05, policy loss: 10.217577738337324
Experience 33, Iter 51, disc loss: 7.415897413192298e-05, policy loss: 10.065209641256546
Experience 33, Iter 52, disc loss: 6.529823443236466e-05, policy loss: 10.01876180456265
Experience 33, Iter 53, disc loss: 6.541193510632644e-05, policy loss: 10.087592666189165
Experience 33, Iter 54, disc loss: 6.538509716081e-05, policy loss: 10.208776954883128
Experience 33, Iter 55, disc loss: 6.50927475220599e-05, policy loss: 10.053559026506363
Experience 33, Iter 56, disc loss: 6.629690104473591e-05, policy loss: 10.24878376981573
Experience 33, Iter 57, disc loss: 7.30673562002327e-05, policy loss: 10.018065904311449
Experience 33, Iter 58, disc loss: 7.465984986571028e-05, policy loss: 9.987285356018635
Experience 33, Iter 59, disc loss: 0.00010186571184785428, policy loss: 9.77192134472527
Experience 33, Iter 60, disc loss: 7.368411668979934e-05, policy loss: 10.005147809943548
Experience 33, Iter 61, disc loss: 7.601698593755291e-05, policy loss: 9.921008892632731
Experience 33, Iter 62, disc loss: 7.495553297663643e-05, policy loss: 10.165678141305728
Experience 33, Iter 63, disc loss: 7.536589072885937e-05, policy loss: 10.042351514119684
Experience 33, Iter 64, disc loss: 5.964764540321666e-05, policy loss: 10.25519320898534
Experience 33, Iter 65, disc loss: 8.007109288420788e-05, policy loss: 9.984075843066528
Experience 33, Iter 66, disc loss: 6.90174826769382e-05, policy loss: 10.130855096610205
Experience 33, Iter 67, disc loss: 8.198157302225299e-05, policy loss: 10.040535277899261
Experience 33, Iter 68, disc loss: 7.846224283788439e-05, policy loss: 10.108844287142619
Experience 33, Iter 69, disc loss: 6.570505545781505e-05, policy loss: 10.319107977968962
Experience 33, Iter 70, disc loss: 7.096958445193783e-05, policy loss: 10.105133929242333
Experience 33, Iter 71, disc loss: 6.707426340262262e-05, policy loss: 10.0989634778359
Experience 33, Iter 72, disc loss: 7.01060740966511e-05, policy loss: 10.097231294800071
Experience 33, Iter 73, disc loss: 6.627253498674115e-05, policy loss: 9.99450455164018
Experience 33, Iter 74, disc loss: 8.253136386948135e-05, policy loss: 9.90852046261517
Experience 33, Iter 75, disc loss: 6.933749580629868e-05, policy loss: 10.034895798439141
Experience 33, Iter 76, disc loss: 6.820865551041814e-05, policy loss: 10.141196671761014
Experience 33, Iter 77, disc loss: 6.214306655046275e-05, policy loss: 10.288315451773935
Experience 33, Iter 78, disc loss: 7.655750184696822e-05, policy loss: 9.887171689941152
Experience 33, Iter 79, disc loss: 6.140018806716034e-05, policy loss: 10.262937075356984
Experience 33, Iter 80, disc loss: 6.678483502387482e-05, policy loss: 10.10462675966469
Experience 33, Iter 81, disc loss: 7.424337676702589e-05, policy loss: 9.876134803674761
Experience 33, Iter 82, disc loss: 7.28784286279987e-05, policy loss: 10.016598603201249
Experience 33, Iter 83, disc loss: 7.252200409517122e-05, policy loss: 10.081785337084039
Experience 33, Iter 84, disc loss: 6.949846523965751e-05, policy loss: 10.059461718169562
Experience 33, Iter 85, disc loss: 7.08946648814804e-05, policy loss: 10.061270948019292
Experience 33, Iter 86, disc loss: 7.80849687683738e-05, policy loss: 10.017473645162383
Experience 33, Iter 87, disc loss: 6.64145877305958e-05, policy loss: 10.183627582441488
Experience 33, Iter 88, disc loss: 7.633577863793029e-05, policy loss: 9.907112085197205
Experience 33, Iter 89, disc loss: 7.766555523855888e-05, policy loss: 9.999742818183906
Experience 33, Iter 90, disc loss: 6.0984243979695995e-05, policy loss: 10.254023861859649
Experience 33, Iter 91, disc loss: 6.366064001634467e-05, policy loss: 10.186686366236254
Experience 33, Iter 92, disc loss: 6.54265359291016e-05, policy loss: 10.083390469713342
Experience 33, Iter 93, disc loss: 8.40722633600259e-05, policy loss: 9.890359250408558
Experience 33, Iter 94, disc loss: 6.820125978913226e-05, policy loss: 10.120355363354212
Experience 33, Iter 95, disc loss: 7.258919887632045e-05, policy loss: 10.007639125828954
Experience 33, Iter 96, disc loss: 7.036857224706919e-05, policy loss: 9.979741952874521
Experience 33, Iter 97, disc loss: 6.399855090654875e-05, policy loss: 10.175037384436447
Experience 33, Iter 98, disc loss: 6.445444306949676e-05, policy loss: 10.246184268853078
Experience 33, Iter 99, disc loss: 6.44020470884101e-05, policy loss: 10.22230423240337
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0031],
        [0.0117],
        [0.1591],
        [0.0031]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0181, 0.1120, 0.1405, 0.0042, 0.0016, 0.4364]],

        [[0.0181, 0.1120, 0.1405, 0.0042, 0.0016, 0.4364]],

        [[0.0181, 0.1120, 0.1405, 0.0042, 0.0016, 0.4364]],

        [[0.0181, 0.1120, 0.1405, 0.0042, 0.0016, 0.4364]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0123, 0.0467, 0.6365, 0.0124], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0123, 0.0467, 0.6365, 0.0124])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.447
Iter 2/2000 - Loss: -0.163
Iter 3/2000 - Loss: -0.554
Iter 4/2000 - Loss: -0.591
Iter 5/2000 - Loss: -0.526
Iter 6/2000 - Loss: -0.635
Iter 7/2000 - Loss: -0.833
Iter 8/2000 - Loss: -1.013
Iter 9/2000 - Loss: -1.160
Iter 10/2000 - Loss: -1.312
Iter 11/2000 - Loss: -1.499
Iter 12/2000 - Loss: -1.725
Iter 13/2000 - Loss: -1.982
Iter 14/2000 - Loss: -2.260
Iter 15/2000 - Loss: -2.553
Iter 16/2000 - Loss: -2.859
Iter 17/2000 - Loss: -3.175
Iter 18/2000 - Loss: -3.498
Iter 19/2000 - Loss: -3.826
Iter 20/2000 - Loss: -4.154
Iter 1981/2000 - Loss: -8.824
Iter 1982/2000 - Loss: -8.824
Iter 1983/2000 - Loss: -8.825
Iter 1984/2000 - Loss: -8.825
Iter 1985/2000 - Loss: -8.825
Iter 1986/2000 - Loss: -8.825
Iter 1987/2000 - Loss: -8.825
Iter 1988/2000 - Loss: -8.825
Iter 1989/2000 - Loss: -8.825
Iter 1990/2000 - Loss: -8.825
Iter 1991/2000 - Loss: -8.825
Iter 1992/2000 - Loss: -8.825
Iter 1993/2000 - Loss: -8.825
Iter 1994/2000 - Loss: -8.825
Iter 1995/2000 - Loss: -8.825
Iter 1996/2000 - Loss: -8.825
Iter 1997/2000 - Loss: -8.825
Iter 1998/2000 - Loss: -8.825
Iter 1999/2000 - Loss: -8.825
Iter 2000/2000 - Loss: -8.825
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.2545,  4.9490, 20.9813,  8.7573, 16.9462, 34.5209]],

        [[14.4394, 25.9648, 11.7555,  2.1957,  1.0798, 18.5955]],

        [[15.5064, 27.8989, 17.7189,  0.9655,  1.5281, 17.2407]],

        [[13.7777, 25.3282, 15.8807,  3.1825,  2.1930, 40.4099]]])
Signal Variance: tensor([ 0.0848,  1.0506, 12.2292,  0.3563])
Estimated target variance: tensor([0.0123, 0.0467, 0.6365, 0.0124])
N: 340
Signal to noise ratio: tensor([16.6258, 51.3895, 69.0820, 35.0904])
Bound on condition number: tensor([  93983.2260,  897901.7155, 1622591.8334,  418655.2223])
Policy Optimizer learning rate:
0.09658285256870237
Experience 34, Iter 0, disc loss: 6.44383466062746e-05, policy loss: 10.15055039521592
Experience 34, Iter 1, disc loss: 9.868692898767345e-05, policy loss: 9.859590838348712
Experience 34, Iter 2, disc loss: 6.317339157381858e-05, policy loss: 10.1517003585824
Experience 34, Iter 3, disc loss: 7.624952992116033e-05, policy loss: 10.052826367579296
Experience 34, Iter 4, disc loss: 6.242551212846224e-05, policy loss: 10.094161417293362
Experience 34, Iter 5, disc loss: 7.096474421208765e-05, policy loss: 10.17514231911898
Experience 34, Iter 6, disc loss: 6.539669383332926e-05, policy loss: 10.102193293687264
Experience 34, Iter 7, disc loss: 7.256863564839277e-05, policy loss: 10.054799745440627
Experience 34, Iter 8, disc loss: 6.0364863192890253e-05, policy loss: 10.198260340559532
Experience 34, Iter 9, disc loss: 5.9738991277394665e-05, policy loss: 10.28308204456437
Experience 34, Iter 10, disc loss: 6.469805948660376e-05, policy loss: 10.131159573434207
Experience 34, Iter 11, disc loss: 6.432452389276926e-05, policy loss: 10.132135074668012
Experience 34, Iter 12, disc loss: 6.492596068965159e-05, policy loss: 10.209198699122656
Experience 34, Iter 13, disc loss: 7.898120973155417e-05, policy loss: 9.989406737373306
Experience 34, Iter 14, disc loss: 6.765590400907855e-05, policy loss: 10.247704523156063
Experience 34, Iter 15, disc loss: 7.136447634805063e-05, policy loss: 10.016335405362506
Experience 34, Iter 16, disc loss: 6.300167199554023e-05, policy loss: 10.155083328617142
Experience 34, Iter 17, disc loss: 8.20251112002458e-05, policy loss: 9.896179421394834
Experience 34, Iter 18, disc loss: 6.0022976586739504e-05, policy loss: 10.20740721996126
Experience 34, Iter 19, disc loss: 7.687719328087057e-05, policy loss: 9.975822284626087
Experience 34, Iter 20, disc loss: 6.147696127493982e-05, policy loss: 10.18315415687904
Experience 34, Iter 21, disc loss: 6.379513985156247e-05, policy loss: 10.227857505817285
Experience 34, Iter 22, disc loss: 7.186431302813614e-05, policy loss: 10.111743271432308
Experience 34, Iter 23, disc loss: 5.3850911384534886e-05, policy loss: 10.333270691038111
Experience 34, Iter 24, disc loss: 7.010327346337449e-05, policy loss: 10.050048795371087
Experience 34, Iter 25, disc loss: 6.19767966619648e-05, policy loss: 10.154368640626087
Experience 34, Iter 26, disc loss: 6.415305999820983e-05, policy loss: 10.242767607601472
Experience 34, Iter 27, disc loss: 8.28709369861864e-05, policy loss: 9.905802924029661
Experience 34, Iter 28, disc loss: 6.58354677509486e-05, policy loss: 10.020245578548433
Experience 34, Iter 29, disc loss: 6.987958429588431e-05, policy loss: 9.982445531943757
Experience 34, Iter 30, disc loss: 7.190927251970395e-05, policy loss: 9.981353820860665
Experience 34, Iter 31, disc loss: 6.515810646009014e-05, policy loss: 10.169090293586624
Experience 34, Iter 32, disc loss: 6.707986339928367e-05, policy loss: 10.155144975250192
Experience 34, Iter 33, disc loss: 8.441720664756911e-05, policy loss: 10.129447203649285
Experience 34, Iter 34, disc loss: 7.514107262691822e-05, policy loss: 10.010872769961166
Experience 34, Iter 35, disc loss: 6.325307253018291e-05, policy loss: 10.29007272660978
Experience 34, Iter 36, disc loss: 0.0001024270201331156, policy loss: 9.775935349472633
Experience 34, Iter 37, disc loss: 6.621249084023769e-05, policy loss: 10.285728467502564
Experience 34, Iter 38, disc loss: 8.749686820461338e-05, policy loss: 9.916231841085
Experience 34, Iter 39, disc loss: 6.43509756591081e-05, policy loss: 10.248903166282306
Experience 34, Iter 40, disc loss: 6.381960870291973e-05, policy loss: 10.209003277464067
Experience 34, Iter 41, disc loss: 7.480328811019778e-05, policy loss: 9.995046430456151
Experience 34, Iter 42, disc loss: 7.247471951672834e-05, policy loss: 9.883229473360975
Experience 34, Iter 43, disc loss: 5.646610356412415e-05, policy loss: 10.447259965329726
Experience 34, Iter 44, disc loss: 6.82235871578106e-05, policy loss: 10.33350087683495
Experience 34, Iter 45, disc loss: 7.07944755157498e-05, policy loss: 10.048066023219604
Experience 34, Iter 46, disc loss: 6.369980021662144e-05, policy loss: 10.100381246987059
Experience 34, Iter 47, disc loss: 7.112858639508372e-05, policy loss: 9.948098937763291
Experience 34, Iter 48, disc loss: 7.716660785132459e-05, policy loss: 10.027836332339671
Experience 34, Iter 49, disc loss: 6.874725548613343e-05, policy loss: 10.15417596477662
Experience 34, Iter 50, disc loss: 5.937112155034297e-05, policy loss: 10.271794714928934
Experience 34, Iter 51, disc loss: 7.029225481285847e-05, policy loss: 10.016052945764944
Experience 34, Iter 52, disc loss: 6.198264191937718e-05, policy loss: 10.219492800741257
Experience 34, Iter 53, disc loss: 6.743118882553195e-05, policy loss: 10.077846661082246
Experience 34, Iter 54, disc loss: 6.949027495042008e-05, policy loss: 10.118613071697228
Experience 34, Iter 55, disc loss: 7.369857464900473e-05, policy loss: 10.117354242583133
Experience 34, Iter 56, disc loss: 6.876663880518682e-05, policy loss: 10.25102665631444
Experience 34, Iter 57, disc loss: 6.77091234562587e-05, policy loss: 10.193983501326812
Experience 34, Iter 58, disc loss: 6.175931568228289e-05, policy loss: 10.2144095723868
Experience 34, Iter 59, disc loss: 5.7063096600799875e-05, policy loss: 10.341358018988736
Experience 34, Iter 60, disc loss: 6.163065689842521e-05, policy loss: 10.094797316008002
Experience 34, Iter 61, disc loss: 5.6177448943088774e-05, policy loss: 10.40756857718768
Experience 34, Iter 62, disc loss: 6.966325129165829e-05, policy loss: 9.965246023607108
Experience 34, Iter 63, disc loss: 5.640165652337008e-05, policy loss: 10.297914945829032
Experience 34, Iter 64, disc loss: 5.790583169881603e-05, policy loss: 10.223252944706513
Experience 34, Iter 65, disc loss: 6.562908770216394e-05, policy loss: 10.036135735471067
Experience 34, Iter 66, disc loss: 5.8018758073244836e-05, policy loss: 10.260528613428791
Experience 34, Iter 67, disc loss: 5.7942271947295136e-05, policy loss: 10.30559592018247
Experience 34, Iter 68, disc loss: 6.285398255373563e-05, policy loss: 10.191734996555526
Experience 34, Iter 69, disc loss: 5.998638507432712e-05, policy loss: 10.295457137618037
Experience 34, Iter 70, disc loss: 6.26846776862363e-05, policy loss: 10.116549260281225
Experience 34, Iter 71, disc loss: 7.095031237050768e-05, policy loss: 10.096999903944372
Experience 34, Iter 72, disc loss: 6.125600325008486e-05, policy loss: 10.23800335808996
Experience 34, Iter 73, disc loss: 6.476504882150971e-05, policy loss: 9.99921724780581
Experience 34, Iter 74, disc loss: 6.931968715832582e-05, policy loss: 10.156490866293977
Experience 34, Iter 75, disc loss: 6.410185756906955e-05, policy loss: 10.21439555312332
Experience 34, Iter 76, disc loss: 6.578031697093525e-05, policy loss: 10.017473802744453
Experience 34, Iter 77, disc loss: 5.842233791945242e-05, policy loss: 10.27863924564198
Experience 34, Iter 78, disc loss: 7.239515535547221e-05, policy loss: 10.010302875078164
Experience 34, Iter 79, disc loss: 7.091733573953818e-05, policy loss: 10.028245108975415
Experience 34, Iter 80, disc loss: 6.801681904567917e-05, policy loss: 10.103421574552561
Experience 34, Iter 81, disc loss: 6.625762930693371e-05, policy loss: 10.257211113508063
Experience 34, Iter 82, disc loss: 6.346946993467009e-05, policy loss: 10.293432687242241
Experience 34, Iter 83, disc loss: 5.5088662120754226e-05, policy loss: 10.231263001473147
Experience 34, Iter 84, disc loss: 5.829898701128373e-05, policy loss: 10.170052166105204
Experience 34, Iter 85, disc loss: 7.566320015656081e-05, policy loss: 9.877977910524333
Experience 34, Iter 86, disc loss: 6.702476127576194e-05, policy loss: 10.106368091058496
Experience 34, Iter 87, disc loss: 6.31054860215268e-05, policy loss: 10.222891755540829
Experience 34, Iter 88, disc loss: 6.69115378360742e-05, policy loss: 10.127899265869807
Experience 34, Iter 89, disc loss: 6.896031047433525e-05, policy loss: 10.132264662162365
Experience 34, Iter 90, disc loss: 6.386305465207889e-05, policy loss: 10.100746523037127
Experience 34, Iter 91, disc loss: 5.975009237341136e-05, policy loss: 10.227735608372058
Experience 34, Iter 92, disc loss: 5.79751201869416e-05, policy loss: 10.162102046354802
Experience 34, Iter 93, disc loss: 6.406991185504739e-05, policy loss: 10.177328338987802
Experience 34, Iter 94, disc loss: 6.055102119854011e-05, policy loss: 10.258481468939198
Experience 34, Iter 95, disc loss: 6.85315907588831e-05, policy loss: 10.012410342711899
Experience 34, Iter 96, disc loss: 5.93242052173829e-05, policy loss: 10.263528379367873
Experience 34, Iter 97, disc loss: 5.7641501465890127e-05, policy loss: 10.271167088400754
Experience 34, Iter 98, disc loss: 6.198658789978036e-05, policy loss: 10.179809920648424
Experience 34, Iter 99, disc loss: 7.101743306199043e-05, policy loss: 9.986531628942155
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0030],
        [0.0114],
        [0.1550],
        [0.0030]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0177, 0.1090, 0.1369, 0.0040, 0.0016, 0.4246]],

        [[0.0177, 0.1090, 0.1369, 0.0040, 0.0016, 0.4246]],

        [[0.0177, 0.1090, 0.1369, 0.0040, 0.0016, 0.4246]],

        [[0.0177, 0.1090, 0.1369, 0.0040, 0.0016, 0.4246]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0120, 0.0455, 0.6200, 0.0121], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0120, 0.0455, 0.6200, 0.0121])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.494
Iter 2/2000 - Loss: -0.182
Iter 3/2000 - Loss: -0.594
Iter 4/2000 - Loss: -0.623
Iter 5/2000 - Loss: -0.545
Iter 6/2000 - Loss: -0.650
Iter 7/2000 - Loss: -0.849
Iter 8/2000 - Loss: -1.026
Iter 9/2000 - Loss: -1.164
Iter 10/2000 - Loss: -1.306
Iter 11/2000 - Loss: -1.485
Iter 12/2000 - Loss: -1.706
Iter 13/2000 - Loss: -1.959
Iter 14/2000 - Loss: -2.234
Iter 15/2000 - Loss: -2.524
Iter 16/2000 - Loss: -2.828
Iter 17/2000 - Loss: -3.144
Iter 18/2000 - Loss: -3.469
Iter 19/2000 - Loss: -3.801
Iter 20/2000 - Loss: -4.134
Iter 1981/2000 - Loss: -8.835
Iter 1982/2000 - Loss: -8.835
Iter 1983/2000 - Loss: -8.835
Iter 1984/2000 - Loss: -8.835
Iter 1985/2000 - Loss: -8.835
Iter 1986/2000 - Loss: -8.835
Iter 1987/2000 - Loss: -8.835
Iter 1988/2000 - Loss: -8.835
Iter 1989/2000 - Loss: -8.835
Iter 1990/2000 - Loss: -8.835
Iter 1991/2000 - Loss: -8.835
Iter 1992/2000 - Loss: -8.835
Iter 1993/2000 - Loss: -8.835
Iter 1994/2000 - Loss: -8.835
Iter 1995/2000 - Loss: -8.835
Iter 1996/2000 - Loss: -8.835
Iter 1997/2000 - Loss: -8.835
Iter 1998/2000 - Loss: -8.835
Iter 1999/2000 - Loss: -8.835
Iter 2000/2000 - Loss: -8.835
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.1928,  4.9551, 20.8342,  8.6601, 16.8839, 34.7027]],

        [[14.2247, 25.4966, 11.5919,  1.9011,  1.1161, 17.9418]],

        [[15.1824, 27.0984, 17.6750,  0.9649,  1.5365, 17.0133]],

        [[13.6737, 24.7535, 15.8296,  3.1698,  2.1249, 40.1608]]])
Signal Variance: tensor([ 0.0842,  0.9483, 12.0333,  0.3510])
Estimated target variance: tensor([0.0120, 0.0455, 0.6200, 0.0121])
N: 350
Signal to noise ratio: tensor([16.6856, 48.6498, 68.3201, 34.6920])
Bound on condition number: tensor([  97444.5159,  828381.7464, 1633671.3469,  421238.2223])
Policy Optimizer learning rate:
0.09648114596590811
Experience 35, Iter 0, disc loss: 6.827750338829418e-05, policy loss: 10.066219176762925
Experience 35, Iter 1, disc loss: 6.115779398803363e-05, policy loss: 10.279775254738382
Experience 35, Iter 2, disc loss: 6.737525906025353e-05, policy loss: 10.14437795781985
Experience 35, Iter 3, disc loss: 6.056006845197972e-05, policy loss: 10.328491742401685
Experience 35, Iter 4, disc loss: 6.960122926399123e-05, policy loss: 10.120793530654387
Experience 35, Iter 5, disc loss: 6.1429374582399e-05, policy loss: 10.181588113552746
Experience 35, Iter 6, disc loss: 5.952758911350799e-05, policy loss: 10.340013943279283
Experience 35, Iter 7, disc loss: 6.191206139063975e-05, policy loss: 10.19405277741107
Experience 35, Iter 8, disc loss: 5.997348394779307e-05, policy loss: 10.215695538436734
Experience 35, Iter 9, disc loss: 6.167652391370593e-05, policy loss: 10.09641518386273
Experience 35, Iter 10, disc loss: 5.985904548063442e-05, policy loss: 10.35506108672811
Experience 35, Iter 11, disc loss: 6.622608748949098e-05, policy loss: 10.097109840333397
Experience 35, Iter 12, disc loss: 7.01568924851829e-05, policy loss: 10.028363038617336
Experience 35, Iter 13, disc loss: 6.476304603110021e-05, policy loss: 10.075531163944367
Experience 35, Iter 14, disc loss: 7.578793731725822e-05, policy loss: 10.063528604655989
Experience 35, Iter 15, disc loss: 6.987446339094393e-05, policy loss: 10.275157627653744
Experience 35, Iter 16, disc loss: 6.991796500872586e-05, policy loss: 10.049229486761696
Experience 35, Iter 17, disc loss: 6.329626309225088e-05, policy loss: 10.311459887586143
Experience 35, Iter 18, disc loss: 6.090029659480724e-05, policy loss: 10.146193543531867
Experience 35, Iter 19, disc loss: 6.404433199899435e-05, policy loss: 10.245789013683792
Experience 35, Iter 20, disc loss: 7.275424425323304e-05, policy loss: 10.273333696854522
Experience 35, Iter 21, disc loss: 5.967208889330703e-05, policy loss: 10.315661149710133
Experience 35, Iter 22, disc loss: 6.305127189905224e-05, policy loss: 10.118453519757395
Experience 35, Iter 23, disc loss: 5.6809472960372924e-05, policy loss: 10.305734495220367
Experience 35, Iter 24, disc loss: 4.918747320870353e-05, policy loss: 10.435126013776445
Experience 35, Iter 25, disc loss: 5.55229408427535e-05, policy loss: 10.202532344690587
Experience 35, Iter 26, disc loss: 5.451612836441435e-05, policy loss: 10.284137428702829
Experience 35, Iter 27, disc loss: 6.955702474960814e-05, policy loss: 9.989734426928347
Experience 35, Iter 28, disc loss: 6.677412940790093e-05, policy loss: 10.275054298193771
Experience 35, Iter 29, disc loss: 6.572877750993026e-05, policy loss: 10.133238158118143
Experience 35, Iter 30, disc loss: 6.201896694049638e-05, policy loss: 10.347433649516265
Experience 35, Iter 31, disc loss: 6.889530187893158e-05, policy loss: 10.115179688657399
Experience 35, Iter 32, disc loss: 6.763090207521056e-05, policy loss: 10.359841688457088
Experience 35, Iter 33, disc loss: 6.607907465197432e-05, policy loss: 10.104233596739338
Experience 35, Iter 34, disc loss: 7.355607898768403e-05, policy loss: 10.185621347886274
Experience 35, Iter 35, disc loss: 6.361134261530796e-05, policy loss: 10.282443107079548
Experience 35, Iter 36, disc loss: 7.933059101494427e-05, policy loss: 9.819558654031795
Experience 35, Iter 37, disc loss: 5.951520583499524e-05, policy loss: 10.288298384000587
Experience 35, Iter 38, disc loss: 7.717761449982708e-05, policy loss: 9.968680082364013
Experience 35, Iter 39, disc loss: 5.633945094671633e-05, policy loss: 10.374418414224316
Experience 35, Iter 40, disc loss: 6.0739185392739086e-05, policy loss: 10.109821009104648
Experience 35, Iter 41, disc loss: 6.413501560185959e-05, policy loss: 10.19720560908603
Experience 35, Iter 42, disc loss: 5.716234118853331e-05, policy loss: 10.288279783681157
Experience 35, Iter 43, disc loss: 6.389491407405724e-05, policy loss: 10.130659239616362
Experience 35, Iter 44, disc loss: 6.527466819293752e-05, policy loss: 10.391612202474146
Experience 35, Iter 45, disc loss: 5.7788651887751275e-05, policy loss: 10.418957626726906
Experience 35, Iter 46, disc loss: 5.489215950295666e-05, policy loss: 10.254646069762238
Experience 35, Iter 47, disc loss: 6.728146225677852e-05, policy loss: 10.14061788505434
Experience 35, Iter 48, disc loss: 5.799366263314391e-05, policy loss: 10.244238530473751
Experience 35, Iter 49, disc loss: 5.180686032145228e-05, policy loss: 10.374322525994824
Experience 35, Iter 50, disc loss: 6.191263965568825e-05, policy loss: 10.232669920053056
Experience 35, Iter 51, disc loss: 7.292013294027084e-05, policy loss: 9.919455924327128
Experience 35, Iter 52, disc loss: 7.955697479871966e-05, policy loss: 10.090307412650297
Experience 35, Iter 53, disc loss: 7.025168689529162e-05, policy loss: 10.274388258798064
Experience 35, Iter 54, disc loss: 8.45441851590816e-05, policy loss: 10.150302978608723
Experience 35, Iter 55, disc loss: 7.088522048308769e-05, policy loss: 10.120195662547427
Experience 35, Iter 56, disc loss: 7.113145193167872e-05, policy loss: 10.146479198170512
Experience 35, Iter 57, disc loss: 6.328284763064449e-05, policy loss: 10.259321877329395
Experience 35, Iter 58, disc loss: 7.150176151113566e-05, policy loss: 10.113924767870465
Experience 35, Iter 59, disc loss: 6.149596175974825e-05, policy loss: 10.452483085399512
Experience 35, Iter 60, disc loss: 5.2233293022747017e-05, policy loss: 10.524589641096153
Experience 35, Iter 61, disc loss: 6.380049867228693e-05, policy loss: 10.233729268001385
Experience 35, Iter 62, disc loss: 6.151861217895995e-05, policy loss: 10.278394229429656
Experience 35, Iter 63, disc loss: 6.116358612503633e-05, policy loss: 10.087036520983045
Experience 35, Iter 64, disc loss: 5.3921465095448836e-05, policy loss: 10.211503865217868
Experience 35, Iter 65, disc loss: 5.9051250930875526e-05, policy loss: 10.131193855076955
Experience 35, Iter 66, disc loss: 5.525546453415392e-05, policy loss: 10.35168713607164
Experience 35, Iter 67, disc loss: 5.705100245843197e-05, policy loss: 10.24350661038997
Experience 35, Iter 68, disc loss: 5.525484407822825e-05, policy loss: 10.265204645933002
Experience 35, Iter 69, disc loss: 6.116949488028424e-05, policy loss: 10.111725524271169
Experience 35, Iter 70, disc loss: 5.736993597551413e-05, policy loss: 10.303990862195178
Experience 35, Iter 71, disc loss: 6.732500795383975e-05, policy loss: 10.040980026948448
Experience 35, Iter 72, disc loss: 7.815104303351416e-05, policy loss: 10.004450270543142
Experience 35, Iter 73, disc loss: 7.02054126735879e-05, policy loss: 10.026485541143085
Experience 35, Iter 74, disc loss: 7.88233969345589e-05, policy loss: 9.952152488732711
Experience 35, Iter 75, disc loss: 7.471809302778395e-05, policy loss: 9.969796525803408
Experience 35, Iter 76, disc loss: 5.9649367597315784e-05, policy loss: 10.480448322635663
Experience 35, Iter 77, disc loss: 7.13228402396752e-05, policy loss: 10.220400894062106
Experience 35, Iter 78, disc loss: 5.571820634199588e-05, policy loss: 10.4541133741761
Experience 35, Iter 79, disc loss: 6.192437247568418e-05, policy loss: 10.1651584666375
Experience 35, Iter 80, disc loss: 6.22315253687787e-05, policy loss: 10.127820538102318
Experience 35, Iter 81, disc loss: 5.44319909468605e-05, policy loss: 10.335067850144476
Experience 35, Iter 82, disc loss: 4.855231742506153e-05, policy loss: 10.373924744502562
Experience 35, Iter 83, disc loss: 5.980848400271927e-05, policy loss: 10.203984936548462
Experience 35, Iter 84, disc loss: 5.714691426840375e-05, policy loss: 10.239653704594172
Experience 35, Iter 85, disc loss: 5.4285694463276124e-05, policy loss: 10.220504935606563
Experience 35, Iter 86, disc loss: 5.740391328434753e-05, policy loss: 10.202462757737706
Experience 35, Iter 87, disc loss: 6.523247794431153e-05, policy loss: 10.038350164107168
Experience 35, Iter 88, disc loss: 4.9144011918959576e-05, policy loss: 10.534666479358975
Experience 35, Iter 89, disc loss: 6.12602649855692e-05, policy loss: 10.364655794992345
Experience 35, Iter 90, disc loss: 6.274489322738101e-05, policy loss: 10.208638679192955
Experience 35, Iter 91, disc loss: 7.061248487778694e-05, policy loss: 10.232408533758104
Experience 35, Iter 92, disc loss: 5.525076471272089e-05, policy loss: 10.532547755505156
Experience 35, Iter 93, disc loss: 5.605383465479758e-05, policy loss: 10.537955085607226
Experience 35, Iter 94, disc loss: 4.6662256788872726e-05, policy loss: 10.49993933367964
Experience 35, Iter 95, disc loss: 5.98637147675559e-05, policy loss: 10.278748882677025
Experience 35, Iter 96, disc loss: 7.065795432476828e-05, policy loss: 10.247527117631243
Experience 35, Iter 97, disc loss: 5.404221329889165e-05, policy loss: 10.2052044740963
Experience 35, Iter 98, disc loss: 5.7746800862234574e-05, policy loss: 10.227932120467262
Experience 35, Iter 99, disc loss: 4.6095878157747405e-05, policy loss: 10.575277983805368
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0111],
        [0.1512],
        [0.0029]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0172, 0.1064, 0.1335, 0.0039, 0.0015, 0.4138]],

        [[0.0172, 0.1064, 0.1335, 0.0039, 0.0015, 0.4138]],

        [[0.0172, 0.1064, 0.1335, 0.0039, 0.0015, 0.4138]],

        [[0.0172, 0.1064, 0.1335, 0.0039, 0.0015, 0.4138]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0117, 0.0443, 0.6049, 0.0118], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0117, 0.0443, 0.6049, 0.0118])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.541
Iter 2/2000 - Loss: -0.202
Iter 3/2000 - Loss: -0.635
Iter 4/2000 - Loss: -0.658
Iter 5/2000 - Loss: -0.569
Iter 6/2000 - Loss: -0.673
Iter 7/2000 - Loss: -0.874
Iter 8/2000 - Loss: -1.050
Iter 9/2000 - Loss: -1.184
Iter 10/2000 - Loss: -1.320
Iter 11/2000 - Loss: -1.495
Iter 12/2000 - Loss: -1.714
Iter 13/2000 - Loss: -1.967
Iter 14/2000 - Loss: -2.242
Iter 15/2000 - Loss: -2.533
Iter 16/2000 - Loss: -2.837
Iter 17/2000 - Loss: -3.154
Iter 18/2000 - Loss: -3.480
Iter 19/2000 - Loss: -3.814
Iter 20/2000 - Loss: -4.150
Iter 1981/2000 - Loss: -8.848
Iter 1982/2000 - Loss: -8.848
Iter 1983/2000 - Loss: -8.848
Iter 1984/2000 - Loss: -8.848
Iter 1985/2000 - Loss: -8.848
Iter 1986/2000 - Loss: -8.848
Iter 1987/2000 - Loss: -8.848
Iter 1988/2000 - Loss: -8.848
Iter 1989/2000 - Loss: -8.848
Iter 1990/2000 - Loss: -8.848
Iter 1991/2000 - Loss: -8.848
Iter 1992/2000 - Loss: -8.848
Iter 1993/2000 - Loss: -8.848
Iter 1994/2000 - Loss: -8.848
Iter 1995/2000 - Loss: -8.848
Iter 1996/2000 - Loss: -8.848
Iter 1997/2000 - Loss: -8.848
Iter 1998/2000 - Loss: -8.848
Iter 1999/2000 - Loss: -8.848
Iter 2000/2000 - Loss: -8.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.0808,  5.1110, 20.7864,  8.4863, 16.8265, 35.2926]],

        [[14.0229, 25.0236, 11.6890,  2.1071,  1.0774, 18.1116]],

        [[14.9546, 26.4341, 17.5807,  0.9632,  1.5333, 16.9702]],

        [[13.5722, 24.3963, 15.8014,  3.1365,  2.1376, 40.1463]]])
Signal Variance: tensor([ 0.0850,  0.9883, 11.9544,  0.3488])
Estimated target variance: tensor([0.0117, 0.0443, 0.6049, 0.0118])
N: 360
Signal to noise ratio: tensor([16.5922, 49.7955, 68.2780, 34.7726])
Bound on condition number: tensor([  99109.9468,  892653.1391, 1678278.4058,  435289.9114])
Policy Optimizer learning rate:
0.09637954646528339
Experience 36, Iter 0, disc loss: 5.99109902210289e-05, policy loss: 10.321955125165225
Experience 36, Iter 1, disc loss: 5.713450089693714e-05, policy loss: 10.2150954226311
Experience 36, Iter 2, disc loss: 5.420999747049845e-05, policy loss: 10.333764163726874
Experience 36, Iter 3, disc loss: 5.448168161744886e-05, policy loss: 10.173824617817703
Experience 36, Iter 4, disc loss: 6.279223365313465e-05, policy loss: 10.328660225684155
Experience 36, Iter 5, disc loss: 5.464999700221432e-05, policy loss: 10.392645338664625
Experience 36, Iter 6, disc loss: 5.4430645108510006e-05, policy loss: 10.375363938471523
Experience 36, Iter 7, disc loss: 7.309366152884875e-05, policy loss: 10.166333532721103
Experience 36, Iter 8, disc loss: 5.835100824066526e-05, policy loss: 10.188666760196416
Experience 36, Iter 9, disc loss: 6.147795621022325e-05, policy loss: 10.227068124449026
Experience 36, Iter 10, disc loss: 6.259077049307549e-05, policy loss: 10.254471922757766
Experience 36, Iter 11, disc loss: 6.990266995636168e-05, policy loss: 10.006143139540855
Experience 36, Iter 12, disc loss: 4.884930513576786e-05, policy loss: 10.493846431783776
Experience 36, Iter 13, disc loss: 6.580991972334393e-05, policy loss: 10.225900172932644
Experience 36, Iter 14, disc loss: 6.395916285023433e-05, policy loss: 10.14186148614063
Experience 36, Iter 15, disc loss: 6.115577519736296e-05, policy loss: 10.23054278520551
Experience 36, Iter 16, disc loss: 7.08593824509247e-05, policy loss: 10.095897678752443
Experience 36, Iter 17, disc loss: 6.087377077349347e-05, policy loss: 10.249369005233639
Experience 36, Iter 18, disc loss: 5.9742134759733325e-05, policy loss: 10.223487815684035
Experience 36, Iter 19, disc loss: 5.434334649783967e-05, policy loss: 10.340228759395007
Experience 36, Iter 20, disc loss: 5.924347750941248e-05, policy loss: 10.259754660855965
Experience 36, Iter 21, disc loss: 6.786334066846537e-05, policy loss: 10.042771645744553
Experience 36, Iter 22, disc loss: 5.831100129600802e-05, policy loss: 10.32797686293154
Experience 36, Iter 23, disc loss: 5.7958145379499356e-05, policy loss: 10.45452890598889
Experience 36, Iter 24, disc loss: 5.7795608728591986e-05, policy loss: 10.355298403991563
Experience 36, Iter 25, disc loss: 6.104370260224862e-05, policy loss: 10.368301366597056
Experience 36, Iter 26, disc loss: 5.686459542342508e-05, policy loss: 10.29267314730621
Experience 36, Iter 27, disc loss: 5.389613715236524e-05, policy loss: 10.28903854029983
Experience 36, Iter 28, disc loss: 5.336941838072726e-05, policy loss: 10.409386749026629
Experience 36, Iter 29, disc loss: 5.6077249477586754e-05, policy loss: 10.272848452908953
Experience 36, Iter 30, disc loss: 6.75958961138266e-05, policy loss: 10.236242010864121
Experience 36, Iter 31, disc loss: 6.161178726485689e-05, policy loss: 10.311163748302288
Experience 36, Iter 32, disc loss: 5.393501249650704e-05, policy loss: 10.282347852382525
Experience 36, Iter 33, disc loss: 4.7483948827003436e-05, policy loss: 10.444256425760667
Experience 36, Iter 34, disc loss: 5.319801557695453e-05, policy loss: 10.3656902267682
Experience 36, Iter 35, disc loss: 5.5776277059705266e-05, policy loss: 10.26936175482329
Experience 36, Iter 36, disc loss: 6.111404517019042e-05, policy loss: 10.196824055209921
Experience 36, Iter 37, disc loss: 6.275062342591497e-05, policy loss: 10.183275538850417
Experience 36, Iter 38, disc loss: 5.4576835457991004e-05, policy loss: 10.491815967080006
Experience 36, Iter 39, disc loss: 5.287460437579763e-05, policy loss: 10.53060652370861
Experience 36, Iter 40, disc loss: 5.2789213736460665e-05, policy loss: 10.407953509613973
Experience 36, Iter 41, disc loss: 6.187223202248394e-05, policy loss: 10.281236574990206
Experience 36, Iter 42, disc loss: 6.322076669269433e-05, policy loss: 10.240741998597843
Experience 36, Iter 43, disc loss: 4.697614419992984e-05, policy loss: 10.453497745245038
Experience 36, Iter 44, disc loss: 6.14976344289797e-05, policy loss: 10.135344985022599
Experience 36, Iter 45, disc loss: 5.3325205310366454e-05, policy loss: 10.381768395951472
Experience 36, Iter 46, disc loss: 6.036348144280369e-05, policy loss: 10.104699639032953
Experience 36, Iter 47, disc loss: 4.5082913515105086e-05, policy loss: 10.507188644147636
Experience 36, Iter 48, disc loss: 4.736077758515586e-05, policy loss: 10.452861674169089
Experience 36, Iter 49, disc loss: 5.0485654996711846e-05, policy loss: 10.441348082071656
Experience 36, Iter 50, disc loss: 5.833121202160933e-05, policy loss: 10.16633039524318
Experience 36, Iter 51, disc loss: 5.799716382747844e-05, policy loss: 10.266048472983135
Experience 36, Iter 52, disc loss: 5.7713544400862885e-05, policy loss: 10.467802842409696
Experience 36, Iter 53, disc loss: 7.45766149951708e-05, policy loss: 10.029503850746115
Experience 36, Iter 54, disc loss: 5.63318846383184e-05, policy loss: 10.308708343986755
Experience 36, Iter 55, disc loss: 7.493204842365791e-05, policy loss: 9.951393460947546
Experience 36, Iter 56, disc loss: 5.742654636839615e-05, policy loss: 10.320529695678527
Experience 36, Iter 57, disc loss: 6.513537213100619e-05, policy loss: 10.290365622002593
Experience 36, Iter 58, disc loss: 4.6037043724775414e-05, policy loss: 10.497091929281257
Experience 36, Iter 59, disc loss: 6.098832959792246e-05, policy loss: 10.357474374373215
Experience 36, Iter 60, disc loss: 5.591209013094993e-05, policy loss: 10.31171365122264
Experience 36, Iter 61, disc loss: 5.104398507008038e-05, policy loss: 10.440503372813218
Experience 36, Iter 62, disc loss: 5.453975440191271e-05, policy loss: 10.229575868761346
Experience 36, Iter 63, disc loss: 5.0552197105874475e-05, policy loss: 10.426433926174525
Experience 36, Iter 64, disc loss: 5.060874325702579e-05, policy loss: 10.454545780845404
Experience 36, Iter 65, disc loss: 5.245373346685391e-05, policy loss: 10.441742966835978
Experience 36, Iter 66, disc loss: 5.447496642630578e-05, policy loss: 10.249352042038769
Experience 36, Iter 67, disc loss: 5.423928841266576e-05, policy loss: 10.372814742987822
Experience 36, Iter 68, disc loss: 5.554944853247893e-05, policy loss: 10.290575071683541
Experience 36, Iter 69, disc loss: 5.869996158802633e-05, policy loss: 10.417280411591443
Experience 36, Iter 70, disc loss: 7.293657416951434e-05, policy loss: 10.399682540936752
Experience 36, Iter 71, disc loss: 5.834233779318829e-05, policy loss: 10.240836792109638
Experience 36, Iter 72, disc loss: 5.982557289088051e-05, policy loss: 10.321504794695501
Experience 36, Iter 73, disc loss: 5.463730476180555e-05, policy loss: 10.237513652052094
Experience 36, Iter 74, disc loss: 6.849461304977494e-05, policy loss: 10.243770124600783
Experience 36, Iter 75, disc loss: 5.344458126307948e-05, policy loss: 10.278944916348296
Experience 36, Iter 76, disc loss: 5.42972511243726e-05, policy loss: 10.250427949423674
Experience 36, Iter 77, disc loss: 5.5213232398789206e-05, policy loss: 10.329728937433439
Experience 36, Iter 78, disc loss: 5.6407047831059985e-05, policy loss: 10.224942210360986
Experience 36, Iter 79, disc loss: 5.2371939891318194e-05, policy loss: 10.327212720249952
Experience 36, Iter 80, disc loss: 6.125590337802978e-05, policy loss: 10.243605946474275
Experience 36, Iter 81, disc loss: 5.6950474152178886e-05, policy loss: 10.342693515305909
Experience 36, Iter 82, disc loss: 6.0509949968205105e-05, policy loss: 10.198662824932718
Experience 36, Iter 83, disc loss: 4.856966123441208e-05, policy loss: 10.599644742014972
Experience 36, Iter 84, disc loss: 5.1536618674231605e-05, policy loss: 10.323071054206917
Experience 36, Iter 85, disc loss: 4.5842239992102294e-05, policy loss: 10.489875227976933
Experience 36, Iter 86, disc loss: 5.992190700279592e-05, policy loss: 10.237537067414197
Experience 36, Iter 87, disc loss: 5.837444894658666e-05, policy loss: 10.25488167330177
Experience 36, Iter 88, disc loss: 5.296480263703966e-05, policy loss: 10.41730311254138
Experience 36, Iter 89, disc loss: 5.245282611650706e-05, policy loss: 10.32382617918553
Experience 36, Iter 90, disc loss: 4.69981981945128e-05, policy loss: 10.475406166903912
Experience 36, Iter 91, disc loss: 5.406081245600519e-05, policy loss: 10.565765384698643
Experience 36, Iter 92, disc loss: 5.563788779262363e-05, policy loss: 10.378717920483442
Experience 36, Iter 93, disc loss: 5.08318114197393e-05, policy loss: 10.529129547077869
Experience 36, Iter 94, disc loss: 5.1605378153562e-05, policy loss: 10.36206337045384
Experience 36, Iter 95, disc loss: 4.8025960741319225e-05, policy loss: 10.574284462044352
Experience 36, Iter 96, disc loss: 4.2941964682520505e-05, policy loss: 10.646042679113553
Experience 36, Iter 97, disc loss: 5.211550695252072e-05, policy loss: 10.338549915330786
Experience 36, Iter 98, disc loss: 4.9722313312209004e-05, policy loss: 10.419753702153997
Experience 36, Iter 99, disc loss: 6.270988039643784e-05, policy loss: 10.195507102343967
Experience: 37
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0108],
        [0.1475],
        [0.0029]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0168, 0.1038, 0.1303, 0.0039, 0.0015, 0.4034]],

        [[0.0168, 0.1038, 0.1303, 0.0039, 0.0015, 0.4034]],

        [[0.0168, 0.1038, 0.1303, 0.0039, 0.0015, 0.4034]],

        [[0.0168, 0.1038, 0.1303, 0.0039, 0.0015, 0.4034]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.0432, 0.5901, 0.0115], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.0432, 0.5901, 0.0115])
N: 370
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1481.0000, 1481.0000, 1481.0000, 1481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.592
Iter 2/2000 - Loss: -0.232
Iter 3/2000 - Loss: -0.682
Iter 4/2000 - Loss: -0.701
Iter 5/2000 - Loss: -0.604
Iter 6/2000 - Loss: -0.708
Iter 7/2000 - Loss: -0.913
Iter 8/2000 - Loss: -1.090
Iter 9/2000 - Loss: -1.222
Iter 10/2000 - Loss: -1.355
Iter 11/2000 - Loss: -1.528
Iter 12/2000 - Loss: -1.748
Iter 13/2000 - Loss: -2.002
Iter 14/2000 - Loss: -2.277
Iter 15/2000 - Loss: -2.569
Iter 16/2000 - Loss: -2.873
Iter 17/2000 - Loss: -3.190
Iter 18/2000 - Loss: -3.516
Iter 19/2000 - Loss: -3.850
Iter 20/2000 - Loss: -4.187
Iter 1981/2000 - Loss: -8.877
Iter 1982/2000 - Loss: -8.877
Iter 1983/2000 - Loss: -8.877
Iter 1984/2000 - Loss: -8.877
Iter 1985/2000 - Loss: -8.877
Iter 1986/2000 - Loss: -8.877
Iter 1987/2000 - Loss: -8.877
Iter 1988/2000 - Loss: -8.877
Iter 1989/2000 - Loss: -8.877
Iter 1990/2000 - Loss: -8.877
Iter 1991/2000 - Loss: -8.877
Iter 1992/2000 - Loss: -8.877
Iter 1993/2000 - Loss: -8.877
Iter 1994/2000 - Loss: -8.877
Iter 1995/2000 - Loss: -8.877
Iter 1996/2000 - Loss: -8.877
Iter 1997/2000 - Loss: -8.877
Iter 1998/2000 - Loss: -8.877
Iter 1999/2000 - Loss: -8.877
Iter 2000/2000 - Loss: -8.878
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 9.9931,  5.1676, 20.5939,  8.3226, 16.8299, 35.9412]],

        [[13.8668, 24.5921, 11.6371,  2.0970,  1.0848, 18.1716]],

        [[14.8044, 26.0931, 17.5799,  0.9648,  1.5181, 16.9214]],

        [[13.4544, 23.9995, 15.7314,  3.0814,  2.0976, 39.9970]]])
Signal Variance: tensor([ 0.0856,  0.9862, 11.8484,  0.3441])
Estimated target variance: tensor([0.0114, 0.0432, 0.5901, 0.0115])
N: 370
Signal to noise ratio: tensor([16.6204, 49.9069, 68.4143, 34.8878])
Bound on condition number: tensor([ 102208.8151,  921558.9918, 1731791.3887,  450350.4479])
Policy Optimizer learning rate:
0.09627805395404426
Experience 37, Iter 0, disc loss: 6.283374633321584e-05, policy loss: 10.23877047678238
Experience 37, Iter 1, disc loss: 6.707573735907723e-05, policy loss: 10.366509038911506
Experience 37, Iter 2, disc loss: 4.9680058288754356e-05, policy loss: 10.427492140387216
Experience 37, Iter 3, disc loss: 5.6460771661160485e-05, policy loss: 10.384510735080346
Experience 37, Iter 4, disc loss: 5.506165060551208e-05, policy loss: 10.46584051672723
Experience 37, Iter 5, disc loss: 5.957879188875419e-05, policy loss: 10.402731860362708
Experience 37, Iter 6, disc loss: 5.281009684151013e-05, policy loss: 10.28295010772522
Experience 37, Iter 7, disc loss: 5.141922821736672e-05, policy loss: 10.43818418680494
Experience 37, Iter 8, disc loss: 5.0416741967905816e-05, policy loss: 10.557815788428666
Experience 37, Iter 9, disc loss: 5.2411994683182334e-05, policy loss: 10.384360934330752
Experience 37, Iter 10, disc loss: 4.862669714318225e-05, policy loss: 10.341758104678682
Experience 37, Iter 11, disc loss: 5.868687546342589e-05, policy loss: 10.15915626131402
Experience 37, Iter 12, disc loss: 6.118991684186079e-05, policy loss: 10.102595465995936
Experience 37, Iter 13, disc loss: 5.010912270855295e-05, policy loss: 10.430226286860435
Experience 37, Iter 14, disc loss: 5.701040978035149e-05, policy loss: 10.312508891368582
Experience 37, Iter 15, disc loss: 5.735594741995891e-05, policy loss: 10.260298664430724
Experience 37, Iter 16, disc loss: 5.855625841779878e-05, policy loss: 10.256056744173392
Experience 37, Iter 17, disc loss: 5.730850370743987e-05, policy loss: 10.406902958701778
Experience 37, Iter 18, disc loss: 6.239682722321181e-05, policy loss: 10.253174771522371
Experience 37, Iter 19, disc loss: 5.664587548352476e-05, policy loss: 10.430493767569894
Experience 37, Iter 20, disc loss: 5.6065215806596456e-05, policy loss: 10.432960582195443
Experience 37, Iter 21, disc loss: 4.567112629573324e-05, policy loss: 10.55276665083326
Experience 37, Iter 22, disc loss: 4.4598886574809764e-05, policy loss: 10.549744071218718
Experience 37, Iter 23, disc loss: 5.5509469451280326e-05, policy loss: 10.218137540626142
Experience 37, Iter 24, disc loss: 4.4943783897487286e-05, policy loss: 10.43742474609736
Experience 37, Iter 25, disc loss: 4.873353797057039e-05, policy loss: 10.470231922812633
Experience 37, Iter 26, disc loss: 4.7853189933916635e-05, policy loss: 10.380265747850583
Experience 37, Iter 27, disc loss: 5.251973135977656e-05, policy loss: 10.383641234688486
Experience 37, Iter 28, disc loss: 5.861521290790804e-05, policy loss: 10.35418300406813
Experience 37, Iter 29, disc loss: 4.6438901996716554e-05, policy loss: 10.533243189652772
Experience 37, Iter 30, disc loss: 6.752953752368184e-05, policy loss: 10.233237294468514
Experience 37, Iter 31, disc loss: 5.8944364266500654e-05, policy loss: 10.317348229336805
Experience 37, Iter 32, disc loss: 4.958219546785977e-05, policy loss: 10.45604757341771
Experience 37, Iter 33, disc loss: 6.081381393693603e-05, policy loss: 10.191026743397519
Experience 37, Iter 34, disc loss: 5.608477933269348e-05, policy loss: 10.37792136430783
Experience 37, Iter 35, disc loss: 5.4658164298516826e-05, policy loss: 10.383195916714426
Experience 37, Iter 36, disc loss: 5.5337660455376955e-05, policy loss: 10.404792654314868
Experience 37, Iter 37, disc loss: 5.748090080774662e-05, policy loss: 10.400925615768612
Experience 37, Iter 38, disc loss: 5.3331061800583146e-05, policy loss: 10.318242443388524
Experience 37, Iter 39, disc loss: 4.20586714626243e-05, policy loss: 10.667401234729208
Experience 37, Iter 40, disc loss: 5.124951544027614e-05, policy loss: 10.477595045282264
Experience 37, Iter 41, disc loss: 5.146736020864581e-05, policy loss: 10.402062344360036
Experience 37, Iter 42, disc loss: 4.403814521063342e-05, policy loss: 10.517993790173133
Experience 37, Iter 43, disc loss: 4.6250051053106146e-05, policy loss: 10.43976308147152
Experience 37, Iter 44, disc loss: 4.6326794456508626e-05, policy loss: 10.46612024135462
Experience 37, Iter 45, disc loss: 4.8740360734726023e-05, policy loss: 10.35220703224171
Experience 37, Iter 46, disc loss: 6.419932940391151e-05, policy loss: 10.064928264817993
Experience 37, Iter 47, disc loss: 4.0974081480309165e-05, policy loss: 10.791076731807813
Experience 37, Iter 48, disc loss: 5.140241955983894e-05, policy loss: 10.605072784159068
Experience 37, Iter 49, disc loss: 4.634546866373814e-05, policy loss: 10.546325032035517
Experience 37, Iter 50, disc loss: 6.464667779682167e-05, policy loss: 10.251828723147682
Experience 37, Iter 51, disc loss: 6.41781797038687e-05, policy loss: 10.14651625085332
Experience 37, Iter 52, disc loss: 5.330950621784358e-05, policy loss: 10.47095616728324
Experience 37, Iter 53, disc loss: 4.916431863287567e-05, policy loss: 10.49163645742818
Experience 37, Iter 54, disc loss: 5.5425306382647035e-05, policy loss: 10.346422948484495
Experience 37, Iter 55, disc loss: 4.710668110231463e-05, policy loss: 10.624025259334134
Experience 37, Iter 56, disc loss: 5.316079727604953e-05, policy loss: 10.41714129801374
Experience 37, Iter 57, disc loss: 6.098673084987326e-05, policy loss: 10.1542754896341
Experience 37, Iter 58, disc loss: 6.733824993715039e-05, policy loss: 10.09608703924616
Experience 37, Iter 59, disc loss: 6.729105248911553e-05, policy loss: 10.370201727676218
Experience 37, Iter 60, disc loss: 5.420087588925189e-05, policy loss: 10.254851330656482
Experience 37, Iter 61, disc loss: 6.042907119461239e-05, policy loss: 10.177933527076622
Experience 37, Iter 62, disc loss: 5.86768476393358e-05, policy loss: 10.240956025379646
Experience 37, Iter 63, disc loss: 6.052371608752215e-05, policy loss: 10.226086561608248
Experience 37, Iter 64, disc loss: 5.139422670721379e-05, policy loss: 10.321698799876618
Experience 37, Iter 65, disc loss: 4.7710391482234084e-05, policy loss: 10.434464944809804
Experience 37, Iter 66, disc loss: 5.6949368332788376e-05, policy loss: 10.249702335774723
Experience 37, Iter 67, disc loss: 5.047753304046874e-05, policy loss: 10.414250254090872
Experience 37, Iter 68, disc loss: 5.143272529434803e-05, policy loss: 10.340811012358547
Experience 37, Iter 69, disc loss: 5.627230616320743e-05, policy loss: 10.43452376925742
Experience 37, Iter 70, disc loss: 7.008541717125891e-05, policy loss: 10.185377505129615
Experience 37, Iter 71, disc loss: 5.027163515695231e-05, policy loss: 10.41237884720795
Experience 37, Iter 72, disc loss: 5.481936351000235e-05, policy loss: 10.133323255347147
Experience 37, Iter 73, disc loss: 5.190658768110694e-05, policy loss: 10.388983430336943
Experience 37, Iter 74, disc loss: 4.828299643619839e-05, policy loss: 10.383589781263312
Experience 37, Iter 75, disc loss: 5.880004460081856e-05, policy loss: 10.308436541530138
Experience 37, Iter 76, disc loss: 4.7403508657975854e-05, policy loss: 10.472164968244748
Experience 37, Iter 77, disc loss: 5.4973204270400505e-05, policy loss: 10.34008606211897
Experience 37, Iter 78, disc loss: 5.9808427438478466e-05, policy loss: 10.388450730206294
Experience 37, Iter 79, disc loss: 4.8654852157672624e-05, policy loss: 10.492768742006621
Experience 37, Iter 80, disc loss: 5.710607504020354e-05, policy loss: 10.194688092383101
Experience 37, Iter 81, disc loss: 4.8181780498008575e-05, policy loss: 10.512655778509533
Experience 37, Iter 82, disc loss: 5.510420627378242e-05, policy loss: 10.29536644727811
Experience 37, Iter 83, disc loss: 6.448778521010865e-05, policy loss: 10.12695306169532
Experience 37, Iter 84, disc loss: 4.893773629397232e-05, policy loss: 10.521856416146933
Experience 37, Iter 85, disc loss: 5.2472740100755185e-05, policy loss: 10.501487162871715
Experience 37, Iter 86, disc loss: 4.849485775740603e-05, policy loss: 10.43125312740247
Experience 37, Iter 87, disc loss: 5.357776868157059e-05, policy loss: 10.454112763379143
Experience 37, Iter 88, disc loss: 5.691766325781524e-05, policy loss: 10.327811032352503
Experience 37, Iter 89, disc loss: 5.668890963832136e-05, policy loss: 10.246063491300585
Experience 37, Iter 90, disc loss: 4.571134011858281e-05, policy loss: 10.529021032868666
Experience 37, Iter 91, disc loss: 5.661271699133388e-05, policy loss: 10.321174170571494
Experience 37, Iter 92, disc loss: 4.460696622861931e-05, policy loss: 10.456443300949747
Experience 37, Iter 93, disc loss: 4.417504731246847e-05, policy loss: 10.540513582644992
Experience 37, Iter 94, disc loss: 5.6719079689655614e-05, policy loss: 10.598128833154027
Experience 37, Iter 95, disc loss: 5.352898323708621e-05, policy loss: 10.414513989123048
Experience 37, Iter 96, disc loss: 5.9434884039905584e-05, policy loss: 10.262589562604415
Experience 37, Iter 97, disc loss: 5.7490912400642107e-05, policy loss: 10.221778345763191
Experience 37, Iter 98, disc loss: 5.083804895154534e-05, policy loss: 10.396383789434331
Experience 37, Iter 99, disc loss: 6.0196220139282045e-05, policy loss: 10.184789816830135
Experience: 38
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.0105],
        [0.1437],
        [0.0028]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0164, 0.1011, 0.1268, 0.0038, 0.0014, 0.3931]],

        [[0.0164, 0.1011, 0.1268, 0.0038, 0.0014, 0.3931]],

        [[0.0164, 0.1011, 0.1268, 0.0038, 0.0014, 0.3931]],

        [[0.0164, 0.1011, 0.1268, 0.0038, 0.0014, 0.3931]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0111, 0.0421, 0.5746, 0.0112], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0111, 0.0421, 0.5746, 0.0112])
N: 380
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1521.0000, 1521.0000, 1521.0000, 1521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.639
Iter 2/2000 - Loss: -0.259
Iter 3/2000 - Loss: -0.725
Iter 4/2000 - Loss: -0.741
Iter 5/2000 - Loss: -0.634
Iter 6/2000 - Loss: -0.737
Iter 7/2000 - Loss: -0.944
Iter 8/2000 - Loss: -1.121
Iter 9/2000 - Loss: -1.249
Iter 10/2000 - Loss: -1.378
Iter 11/2000 - Loss: -1.549
Iter 12/2000 - Loss: -1.767
Iter 13/2000 - Loss: -2.020
Iter 14/2000 - Loss: -2.294
Iter 15/2000 - Loss: -2.584
Iter 16/2000 - Loss: -2.888
Iter 17/2000 - Loss: -3.203
Iter 18/2000 - Loss: -3.529
Iter 19/2000 - Loss: -3.863
Iter 20/2000 - Loss: -4.200
Iter 1981/2000 - Loss: -8.913
Iter 1982/2000 - Loss: -8.913
Iter 1983/2000 - Loss: -8.913
Iter 1984/2000 - Loss: -8.913
Iter 1985/2000 - Loss: -8.913
Iter 1986/2000 - Loss: -8.913
Iter 1987/2000 - Loss: -8.913
Iter 1988/2000 - Loss: -8.913
Iter 1989/2000 - Loss: -8.913
Iter 1990/2000 - Loss: -8.913
Iter 1991/2000 - Loss: -8.913
Iter 1992/2000 - Loss: -8.913
Iter 1993/2000 - Loss: -8.913
Iter 1994/2000 - Loss: -8.913
Iter 1995/2000 - Loss: -8.913
Iter 1996/2000 - Loss: -8.913
Iter 1997/2000 - Loss: -8.913
Iter 1998/2000 - Loss: -8.913
Iter 1999/2000 - Loss: -8.913
Iter 2000/2000 - Loss: -8.913
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 9.9359,  5.1276, 20.4942,  8.2940, 16.9107, 35.4451]],

        [[13.7633, 24.1836, 11.6301,  2.0830,  1.0866, 18.2361]],

        [[14.5551, 25.2243, 17.5814,  0.9583,  1.5000, 16.8789]],

        [[13.3484, 23.4418, 15.6712,  3.0598,  2.1049, 39.8479]]])
Signal Variance: tensor([ 0.0849,  0.9834, 11.8419,  0.3404])
Estimated target variance: tensor([0.0111, 0.0421, 0.5746, 0.0112])
N: 380
Signal to noise ratio: tensor([16.6624, 49.9751, 69.1281, 34.9264])
Bound on condition number: tensor([ 105501.9251,  949055.3766, 1815904.7823,  463545.3611])
Policy Optimizer learning rate:
0.09617666831952551
Experience 38, Iter 0, disc loss: 4.349123544121321e-05, policy loss: 10.712406759299265
Experience 38, Iter 1, disc loss: 4.968903280732182e-05, policy loss: 10.454427148668525
Experience 38, Iter 2, disc loss: 5.25386453349225e-05, policy loss: 10.34757709068132
Experience 38, Iter 3, disc loss: 6.181399593393915e-05, policy loss: 10.29083108591452
Experience 38, Iter 4, disc loss: 4.94429836594565e-05, policy loss: 10.436120461812378
Experience 38, Iter 5, disc loss: 4.744027817871485e-05, policy loss: 10.497667010482806
Experience 38, Iter 6, disc loss: 4.97085667511236e-05, policy loss: 10.465624310401857
Experience 38, Iter 7, disc loss: 4.571382411949647e-05, policy loss: 10.417831498935657
Experience 38, Iter 8, disc loss: 4.4300668153031234e-05, policy loss: 10.468126018389981
Experience 38, Iter 9, disc loss: 4.743024532426012e-05, policy loss: 10.380832578573676
Experience 38, Iter 10, disc loss: 3.919724552951506e-05, policy loss: 10.65938558684684
Experience 38, Iter 11, disc loss: 4.5399171681896804e-05, policy loss: 10.449685986271454
Experience 38, Iter 12, disc loss: 4.42246866544881e-05, policy loss: 10.63234442042775
Experience 38, Iter 13, disc loss: 4.7524858778453455e-05, policy loss: 10.461295118771176
Experience 38, Iter 14, disc loss: 4.5850669840804856e-05, policy loss: 10.49766451180951
Experience 38, Iter 15, disc loss: 4.408535880767367e-05, policy loss: 10.537075904069939
Experience 38, Iter 16, disc loss: 4.501786822158085e-05, policy loss: 10.543010584797415
Experience 38, Iter 17, disc loss: 4.298495338442114e-05, policy loss: 10.602165031206855
Experience 38, Iter 18, disc loss: 4.898618634362859e-05, policy loss: 10.29518287674711
Experience 38, Iter 19, disc loss: 4.43205267441843e-05, policy loss: 10.502568292035747
Experience 38, Iter 20, disc loss: 4.901222598396816e-05, policy loss: 10.489885112835681
Experience 38, Iter 21, disc loss: 4.714058410208459e-05, policy loss: 10.426380128421155
Experience 38, Iter 22, disc loss: 5.1950687650698653e-05, policy loss: 10.396155688170872
Experience 38, Iter 23, disc loss: 4.8877739915711756e-05, policy loss: 10.375821581763999
Experience 38, Iter 24, disc loss: 5.36426735869837e-05, policy loss: 10.325291647976004
Experience 38, Iter 25, disc loss: 4.591721380187292e-05, policy loss: 10.564522465749109
Experience 38, Iter 26, disc loss: 5.9448140179055616e-05, policy loss: 10.273988143754233
Experience 38, Iter 27, disc loss: 4.740904127277258e-05, policy loss: 10.615273215592309
Experience 38, Iter 28, disc loss: 5.324936053522674e-05, policy loss: 10.320742761774838
Experience 38, Iter 29, disc loss: 5.2835564522994734e-05, policy loss: 10.380957878160906
Experience 38, Iter 30, disc loss: 5.0713536087463375e-05, policy loss: 10.531103392475972
Experience 38, Iter 31, disc loss: 4.536269600951343e-05, policy loss: 10.560936314585634
Experience 38, Iter 32, disc loss: 5.0065676979430965e-05, policy loss: 10.554610753415027
Experience 38, Iter 33, disc loss: 4.749892362021014e-05, policy loss: 10.485964759447658
Experience 38, Iter 34, disc loss: 4.459686048840942e-05, policy loss: 10.566947374071535
Experience 38, Iter 35, disc loss: 5.5193176996568516e-05, policy loss: 10.170851118587986
Experience 38, Iter 36, disc loss: 4.7222052762257165e-05, policy loss: 10.47472557107638
Experience 38, Iter 37, disc loss: 4.3184373757670374e-05, policy loss: 10.518779602154105
Experience 38, Iter 38, disc loss: 6.330731834847366e-05, policy loss: 10.23522351795653
Experience 38, Iter 39, disc loss: 4.4274329635061736e-05, policy loss: 10.577727584182451
Experience 38, Iter 40, disc loss: 5.69200417538279e-05, policy loss: 10.432832544021185
Experience 38, Iter 41, disc loss: 5.9923506906391125e-05, policy loss: 10.150643084160409
Experience 38, Iter 42, disc loss: 6.218152115574529e-05, policy loss: 10.303162252534122
Experience 38, Iter 43, disc loss: 4.262556059867161e-05, policy loss: 10.728179329754656
Experience 38, Iter 44, disc loss: 5.293658087664653e-05, policy loss: 10.360277938442298
Experience 38, Iter 45, disc loss: 4.5942933394250036e-05, policy loss: 10.52264104595685
Experience 38, Iter 46, disc loss: 3.9624273631198346e-05, policy loss: 10.691225498470668
Experience 38, Iter 47, disc loss: 5.426414794384868e-05, policy loss: 10.447475726062107
Experience 38, Iter 48, disc loss: 4.60772058499791e-05, policy loss: 10.427951041591028
Experience 38, Iter 49, disc loss: 4.2112887805083734e-05, policy loss: 10.6123068112257
Experience 38, Iter 50, disc loss: 4.6116101921254826e-05, policy loss: 10.327731242379858
Experience 38, Iter 51, disc loss: 4.467956658696303e-05, policy loss: 10.647347260175401
Experience 38, Iter 52, disc loss: 5.417599371629443e-05, policy loss: 10.23838208511741
Experience 38, Iter 53, disc loss: 3.4690002289468536e-05, policy loss: 10.92350447055939
Experience 38, Iter 54, disc loss: 4.670442957450547e-05, policy loss: 10.440331135266094
Experience 38, Iter 55, disc loss: 4.964656757411544e-05, policy loss: 10.649625686597652
Experience 38, Iter 56, disc loss: 4.552042071734444e-05, policy loss: 10.611689746922421
Experience 38, Iter 57, disc loss: 6.01648877647991e-05, policy loss: 10.30561295300838
Experience 38, Iter 58, disc loss: 5.181660021168751e-05, policy loss: 10.42312357596535
Experience 38, Iter 59, disc loss: 4.48130964479728e-05, policy loss: 10.604162807092937
Experience 38, Iter 60, disc loss: 4.6686336948684374e-05, policy loss: 10.43261528708524
Experience 38, Iter 61, disc loss: 4.76285354208612e-05, policy loss: 10.437955809902961
Experience 38, Iter 62, disc loss: 4.3912132477316954e-05, policy loss: 10.683434941496893
Experience 38, Iter 63, disc loss: 4.402325321134236e-05, policy loss: 10.654196457967945
Experience 38, Iter 64, disc loss: 5.0374558046786247e-05, policy loss: 10.365898342713905
Experience 38, Iter 65, disc loss: 4.3436933681993356e-05, policy loss: 10.52437615970668
Experience 38, Iter 66, disc loss: 5.155106970847946e-05, policy loss: 10.342050670560281
Experience 38, Iter 67, disc loss: 5.51268763203701e-05, policy loss: 10.484445993484346
Experience 38, Iter 68, disc loss: 5.127298647611506e-05, policy loss: 10.46219331125123
Experience 38, Iter 69, disc loss: 5.0618666819099824e-05, policy loss: 10.29195297841586
Experience 38, Iter 70, disc loss: 4.5107696009961356e-05, policy loss: 10.552525381718947
Experience 38, Iter 71, disc loss: 4.309618106960259e-05, policy loss: 10.668349174032183
Experience 38, Iter 72, disc loss: 4.934573911427367e-05, policy loss: 10.366800058170856
Experience 38, Iter 73, disc loss: 4.83503090416982e-05, policy loss: 10.43848498120259
Experience 38, Iter 74, disc loss: 4.517383806717208e-05, policy loss: 10.56561066255481
Experience 38, Iter 75, disc loss: 4.264601675329687e-05, policy loss: 10.71461182001725
Experience 38, Iter 76, disc loss: 4.619774665820843e-05, policy loss: 10.39986416399794
Experience 38, Iter 77, disc loss: 4.318713053750521e-05, policy loss: 10.56669574626382
Experience 38, Iter 78, disc loss: 4.5659244087740984e-05, policy loss: 10.428147579646481
Experience 38, Iter 79, disc loss: 5.358061040754597e-05, policy loss: 10.275715820214526
Experience 38, Iter 80, disc loss: 5.1571597965843234e-05, policy loss: 10.563416704439998
Experience 38, Iter 81, disc loss: 5.5770025754705446e-05, policy loss: 10.256254621478671
Experience 38, Iter 82, disc loss: 4.829922300030463e-05, policy loss: 10.4496054117803
Experience 38, Iter 83, disc loss: 4.542444303223556e-05, policy loss: 10.440097789790176
Experience 38, Iter 84, disc loss: 4.409762540460264e-05, policy loss: 10.568289664084915
Experience 38, Iter 85, disc loss: 6.03834267192622e-05, policy loss: 10.378261415347279
Experience 38, Iter 86, disc loss: 5.389656107899863e-05, policy loss: 10.350974836716823
Experience 38, Iter 87, disc loss: 5.1398234419009105e-05, policy loss: 10.44287272388495
Experience 38, Iter 88, disc loss: 4.671988926038104e-05, policy loss: 10.501917096425982
Experience 38, Iter 89, disc loss: 5.102855328418734e-05, policy loss: 10.566900373353212
Experience 38, Iter 90, disc loss: 5.32353600767446e-05, policy loss: 10.358254313803869
Experience 38, Iter 91, disc loss: 4.498141888266874e-05, policy loss: 10.620369128612928
Experience 38, Iter 92, disc loss: 5.519129935686004e-05, policy loss: 10.365768727630801
Experience 38, Iter 93, disc loss: 4.2905458997453364e-05, policy loss: 10.637468128558966
Experience 38, Iter 94, disc loss: 4.390875979012399e-05, policy loss: 10.718925467448543
Experience 38, Iter 95, disc loss: 4.760138525659665e-05, policy loss: 10.492414516612316
Experience 38, Iter 96, disc loss: 4.551125074598015e-05, policy loss: 10.426663501489676
Experience 38, Iter 97, disc loss: 4.299607679409593e-05, policy loss: 10.617039799916443
Experience 38, Iter 98, disc loss: 4.4067724447845405e-05, policy loss: 10.442717053048494
Experience 38, Iter 99, disc loss: 4.6123196979283584e-05, policy loss: 10.478628696964766
Experience: 39
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0103],
        [0.1411],
        [0.0028]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0159, 0.0987, 0.1246, 0.0037, 0.0014, 0.3836]],

        [[0.0159, 0.0987, 0.1246, 0.0037, 0.0014, 0.3836]],

        [[0.0159, 0.0987, 0.1246, 0.0037, 0.0014, 0.3836]],

        [[0.0159, 0.0987, 0.1246, 0.0037, 0.0014, 0.3836]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0108, 0.0412, 0.5645, 0.0110], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0108, 0.0412, 0.5645, 0.0110])
N: 390
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1561.0000, 1561.0000, 1561.0000, 1561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.676
Iter 2/2000 - Loss: -0.268
Iter 3/2000 - Loss: -0.755
Iter 4/2000 - Loss: -0.764
Iter 5/2000 - Loss: -0.644
Iter 6/2000 - Loss: -0.746
Iter 7/2000 - Loss: -0.955
Iter 8/2000 - Loss: -1.128
Iter 9/2000 - Loss: -1.247
Iter 10/2000 - Loss: -1.367
Iter 11/2000 - Loss: -1.531
Iter 12/2000 - Loss: -1.745
Iter 13/2000 - Loss: -1.994
Iter 14/2000 - Loss: -2.265
Iter 15/2000 - Loss: -2.552
Iter 16/2000 - Loss: -2.854
Iter 17/2000 - Loss: -3.171
Iter 18/2000 - Loss: -3.500
Iter 19/2000 - Loss: -3.837
Iter 20/2000 - Loss: -4.180
Iter 1981/2000 - Loss: -8.937
Iter 1982/2000 - Loss: -8.937
Iter 1983/2000 - Loss: -8.937
Iter 1984/2000 - Loss: -8.937
Iter 1985/2000 - Loss: -8.937
Iter 1986/2000 - Loss: -8.937
Iter 1987/2000 - Loss: -8.937
Iter 1988/2000 - Loss: -8.937
Iter 1989/2000 - Loss: -8.937
Iter 1990/2000 - Loss: -8.937
Iter 1991/2000 - Loss: -8.937
Iter 1992/2000 - Loss: -8.937
Iter 1993/2000 - Loss: -8.937
Iter 1994/2000 - Loss: -8.937
Iter 1995/2000 - Loss: -8.937
Iter 1996/2000 - Loss: -8.937
Iter 1997/2000 - Loss: -8.937
Iter 1998/2000 - Loss: -8.937
Iter 1999/2000 - Loss: -8.937
Iter 2000/2000 - Loss: -8.937
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 9.8405,  5.1899, 20.3571,  8.1959, 16.7209, 35.5438]],

        [[13.6007, 24.0788, 11.5632,  2.0303,  1.0999, 18.2607]],

        [[14.4150, 25.1045, 17.4210,  0.9570,  1.4985, 16.9386]],

        [[13.1977, 23.4200, 15.5374,  3.0509,  2.1117, 39.7217]]])
Signal Variance: tensor([ 0.0850,  0.9738, 11.7498,  0.3361])
Estimated target variance: tensor([0.0108, 0.0412, 0.5645, 0.0110])
N: 390
Signal to noise ratio: tensor([16.6935, 50.2038, 68.9326, 34.7709])
Bound on condition number: tensor([ 108683.0740,  982964.1237, 1853167.8561,  471517.1352])
Policy Optimizer learning rate:
0.09607538944918056
Experience 39, Iter 0, disc loss: 4.062490746179122e-05, policy loss: 10.734531529380883
Experience 39, Iter 1, disc loss: 4.504348758402567e-05, policy loss: 10.52866788148333
Experience 39, Iter 2, disc loss: 4.305139224389047e-05, policy loss: 10.638009298153051
Experience 39, Iter 3, disc loss: 5.303197734438347e-05, policy loss: 10.443454073597568
Experience 39, Iter 4, disc loss: 4.462111004882048e-05, policy loss: 10.540930129784105
Experience 39, Iter 5, disc loss: 4.575286640560416e-05, policy loss: 10.672695845970043
Experience 39, Iter 6, disc loss: 4.7683484981643506e-05, policy loss: 10.472392214274379
Experience 39, Iter 7, disc loss: 5.079095150434199e-05, policy loss: 10.24330584298454
Experience 39, Iter 8, disc loss: 4.498095289691246e-05, policy loss: 10.55670656263517
Experience 39, Iter 9, disc loss: 4.3110156602230084e-05, policy loss: 10.549573483580062
Experience 39, Iter 10, disc loss: 4.243276935215171e-05, policy loss: 10.521899779696263
Experience 39, Iter 11, disc loss: 5.5824497149631764e-05, policy loss: 10.325282872032199
Experience 39, Iter 12, disc loss: 4.793116102691187e-05, policy loss: 10.520651247749507
Experience 39, Iter 13, disc loss: 4.334339240405199e-05, policy loss: 10.564206654245092
Experience 39, Iter 14, disc loss: 5.212122003403106e-05, policy loss: 10.415866925970569
Experience 39, Iter 15, disc loss: 5.009877230990825e-05, policy loss: 10.405881763894147
Experience 39, Iter 16, disc loss: 4.1389922520131866e-05, policy loss: 10.61787681107936
Experience 39, Iter 17, disc loss: 4.2606631470247096e-05, policy loss: 10.576974537054122
Experience 39, Iter 18, disc loss: 4.3396290423862343e-05, policy loss: 10.530810054905531
Experience 39, Iter 19, disc loss: 4.899577100413385e-05, policy loss: 10.539852684300637
Experience 39, Iter 20, disc loss: 3.7572912907115664e-05, policy loss: 10.808675481642045
Experience 39, Iter 21, disc loss: 5.4275912541743644e-05, policy loss: 10.390263622164717
Experience 39, Iter 22, disc loss: 4.260945504346378e-05, policy loss: 10.550331058846128
Experience 39, Iter 23, disc loss: 4.963264162241677e-05, policy loss: 10.451555361654853
Experience 39, Iter 24, disc loss: 4.3292904418782845e-05, policy loss: 10.55244994020387
Experience 39, Iter 25, disc loss: 4.972414280703799e-05, policy loss: 10.448553534696817
Experience 39, Iter 26, disc loss: 5.553964336606429e-05, policy loss: 10.351818117080903
Experience 39, Iter 27, disc loss: 4.482549271498889e-05, policy loss: 10.655748520253164
Experience 39, Iter 28, disc loss: 4.573609200195076e-05, policy loss: 10.532647768482803
Experience 39, Iter 29, disc loss: 4.913990618355557e-05, policy loss: 10.535047746773039
Experience 39, Iter 30, disc loss: 5.0980491112466636e-05, policy loss: 10.398192002039004
Experience 39, Iter 31, disc loss: 5.0847320625153654e-05, policy loss: 10.455770683379601
Experience 39, Iter 32, disc loss: 5.0274321185112564e-05, policy loss: 10.365407364589561
Experience 39, Iter 33, disc loss: 3.922305695536667e-05, policy loss: 10.719936176485843
Experience 39, Iter 34, disc loss: 5.891758961324121e-05, policy loss: 10.322673046100839
Experience 39, Iter 35, disc loss: 4.0847566959873496e-05, policy loss: 10.728456147117829
Experience 39, Iter 36, disc loss: 4.302598311592783e-05, policy loss: 10.564282589908395
Experience 39, Iter 37, disc loss: 4.733362185342895e-05, policy loss: 10.461009328353885
Experience 39, Iter 38, disc loss: 4.3910367538737676e-05, policy loss: 10.605251732121989
Experience 39, Iter 39, disc loss: 4.161935381051123e-05, policy loss: 10.494145412249292
Experience 39, Iter 40, disc loss: 4.1120998586574154e-05, policy loss: 10.545914672318505
Experience 39, Iter 41, disc loss: 3.8712351605692964e-05, policy loss: 10.66312723088995
Experience 39, Iter 42, disc loss: 4.16186992766492e-05, policy loss: 10.572303066152111
Experience 39, Iter 43, disc loss: 4.2876859060282675e-05, policy loss: 10.538986968458953
Experience 39, Iter 44, disc loss: 5.385999651286504e-05, policy loss: 10.334386156875187
Experience 39, Iter 45, disc loss: 4.401921404915719e-05, policy loss: 10.6578766524973
Experience 39, Iter 46, disc loss: 4.4258237373365784e-05, policy loss: 10.68413435800226
Experience 39, Iter 47, disc loss: 4.7905703455691765e-05, policy loss: 10.42457017638114
Experience 39, Iter 48, disc loss: 5.0247160128253866e-05, policy loss: 10.653388266389177
Experience 39, Iter 49, disc loss: 5.330630937375931e-05, policy loss: 10.283875944997442
Experience 39, Iter 50, disc loss: 3.860276085960163e-05, policy loss: 10.81157217221783
Experience 39, Iter 51, disc loss: 4.5884286076930384e-05, policy loss: 10.564563842820112
Experience 39, Iter 52, disc loss: 4.3735377336170245e-05, policy loss: 10.65399596059651
Experience 39, Iter 53, disc loss: 3.8116606175017454e-05, policy loss: 10.721817890979136
Experience 39, Iter 54, disc loss: 3.6908371292936385e-05, policy loss: 10.671404459456127
Experience 39, Iter 55, disc loss: 3.6045117073102905e-05, policy loss: 10.888092562369176
Experience 39, Iter 56, disc loss: 4.432124726832858e-05, policy loss: 10.565563004949215
Experience 39, Iter 57, disc loss: 3.9007485152260936e-05, policy loss: 10.621879158168426
Experience 39, Iter 58, disc loss: 4.3410080679941724e-05, policy loss: 10.503763471861735
Experience 39, Iter 59, disc loss: 5.170437881222628e-05, policy loss: 10.454720313391569
Experience 39, Iter 60, disc loss: 4.823421479985805e-05, policy loss: 10.469365907603347
Experience 39, Iter 61, disc loss: 5.392297770657583e-05, policy loss: 10.469542805846142
Experience 39, Iter 62, disc loss: 4.0667438895925966e-05, policy loss: 10.561577097864134
Experience 39, Iter 63, disc loss: 4.337935421471971e-05, policy loss: 10.462270123699318
Experience 39, Iter 64, disc loss: 4.6143358534554986e-05, policy loss: 10.67741625331313
Experience 39, Iter 65, disc loss: 4.223191024890623e-05, policy loss: 10.603800111696804
Experience 39, Iter 66, disc loss: 3.9835560992565594e-05, policy loss: 10.676062171153077
Experience 39, Iter 67, disc loss: 4.3251316330472975e-05, policy loss: 10.68778936060151
Experience 39, Iter 68, disc loss: 4.299835830630945e-05, policy loss: 10.513683534895723
Experience 39, Iter 69, disc loss: 3.980051369136167e-05, policy loss: 10.654554826487859
Experience 39, Iter 70, disc loss: 4.0901895821856657e-05, policy loss: 10.59994924326907
Experience 39, Iter 71, disc loss: 4.73264554710383e-05, policy loss: 10.440701838926872
Experience 39, Iter 72, disc loss: 4.739948425867298e-05, policy loss: 10.35099990581662
Experience 39, Iter 73, disc loss: 4.7423137449898485e-05, policy loss: 10.425402229113402
Experience 39, Iter 74, disc loss: 4.081753304514633e-05, policy loss: 10.915393419697944
Experience 39, Iter 75, disc loss: 4.208400942538116e-05, policy loss: 10.580246470673973
Experience 39, Iter 76, disc loss: 4.853684438291438e-05, policy loss: 10.529914954824154
Experience 39, Iter 77, disc loss: 4.507604895173618e-05, policy loss: 10.619873596942956
Experience 39, Iter 78, disc loss: 5.074725550950431e-05, policy loss: 10.293420820181414
Experience 39, Iter 79, disc loss: 4.268048925930795e-05, policy loss: 10.509022193311095
Experience 39, Iter 80, disc loss: 4.5853197725163536e-05, policy loss: 10.608722274897053
Experience 39, Iter 81, disc loss: 3.870581995358331e-05, policy loss: 10.619151104841192
Experience 39, Iter 82, disc loss: 4.848944225979174e-05, policy loss: 10.38434518304631
Experience 39, Iter 83, disc loss: 4.2116632876118626e-05, policy loss: 10.688745033988898
Experience 39, Iter 84, disc loss: 4.036488775840932e-05, policy loss: 10.705025655942352
Experience 39, Iter 85, disc loss: 4.0856340848690814e-05, policy loss: 10.536900923860186
Experience 39, Iter 86, disc loss: 4.100520979200418e-05, policy loss: 10.560866595457977
Experience 39, Iter 87, disc loss: 4.802212270403904e-05, policy loss: 10.427665822859026
Experience 39, Iter 88, disc loss: 4.759311609913656e-05, policy loss: 10.433024358129323
Experience 39, Iter 89, disc loss: 4.885019723871103e-05, policy loss: 10.423515314187208
Experience 39, Iter 90, disc loss: 5.009620655029984e-05, policy loss: 10.38286211862847
Experience 39, Iter 91, disc loss: 5.119332388753415e-05, policy loss: 10.651042604291455
Experience 39, Iter 92, disc loss: 4.682542532343115e-05, policy loss: 10.603570008777094
Experience 39, Iter 93, disc loss: 4.9241255270439794e-05, policy loss: 10.600369585236496
Experience 39, Iter 94, disc loss: 4.6473108584955155e-05, policy loss: 10.679815227474522
Experience 39, Iter 95, disc loss: 4.553989380156674e-05, policy loss: 10.568722336981025
Experience 39, Iter 96, disc loss: 4.857799306907961e-05, policy loss: 10.51121834098279
Experience 39, Iter 97, disc loss: 3.646394815104226e-05, policy loss: 11.034926460324698
Experience 39, Iter 98, disc loss: 4.016735145333774e-05, policy loss: 10.545957300833347
Experience 39, Iter 99, disc loss: 4.064545183016878e-05, policy loss: 10.655835566959832
Experience: 40
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0101],
        [0.1378],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0156, 0.0964, 0.1216, 0.0036, 0.0014, 0.3748]],

        [[0.0156, 0.0964, 0.1216, 0.0036, 0.0014, 0.3748]],

        [[0.0156, 0.0964, 0.1216, 0.0036, 0.0014, 0.3748]],

        [[0.0156, 0.0964, 0.1216, 0.0036, 0.0014, 0.3748]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0106, 0.0402, 0.5512, 0.0108], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0106, 0.0402, 0.5512, 0.0108])
N: 400
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1601.0000, 1601.0000, 1601.0000, 1601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.725
Iter 2/2000 - Loss: -0.301
Iter 3/2000 - Loss: -0.804
Iter 4/2000 - Loss: -0.810
Iter 5/2000 - Loss: -0.685
Iter 6/2000 - Loss: -0.788
Iter 7/2000 - Loss: -1.001
Iter 8/2000 - Loss: -1.176
Iter 9/2000 - Loss: -1.295
Iter 10/2000 - Loss: -1.413
Iter 11/2000 - Loss: -1.577
Iter 12/2000 - Loss: -1.791
Iter 13/2000 - Loss: -2.041
Iter 14/2000 - Loss: -2.312
Iter 15/2000 - Loss: -2.598
Iter 16/2000 - Loss: -2.899
Iter 17/2000 - Loss: -3.213
Iter 18/2000 - Loss: -3.540
Iter 19/2000 - Loss: -3.876
Iter 20/2000 - Loss: -4.216
Iter 1981/2000 - Loss: -8.939
Iter 1982/2000 - Loss: -8.939
Iter 1983/2000 - Loss: -8.939
Iter 1984/2000 - Loss: -8.939
Iter 1985/2000 - Loss: -8.939
Iter 1986/2000 - Loss: -8.939
Iter 1987/2000 - Loss: -8.939
Iter 1988/2000 - Loss: -8.939
Iter 1989/2000 - Loss: -8.939
Iter 1990/2000 - Loss: -8.939
Iter 1991/2000 - Loss: -8.939
Iter 1992/2000 - Loss: -8.939
Iter 1993/2000 - Loss: -8.939
Iter 1994/2000 - Loss: -8.939
Iter 1995/2000 - Loss: -8.939
Iter 1996/2000 - Loss: -8.939
Iter 1997/2000 - Loss: -8.939
Iter 1998/2000 - Loss: -8.939
Iter 1999/2000 - Loss: -8.939
Iter 2000/2000 - Loss: -8.939
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[ 9.7705,  5.1670, 20.0735,  8.1529, 16.7396, 35.6155]],

        [[13.4848, 23.8525, 11.5895,  2.0672,  1.0919, 18.2910]],

        [[14.2526, 24.8069, 17.3549,  0.9547,  1.4802, 17.0092]],

        [[13.0564, 23.1617, 15.4274,  3.0395,  2.1085, 39.4480]]])
Signal Variance: tensor([ 0.0846,  0.9796, 11.7691,  0.3317])
Estimated target variance: tensor([0.0106, 0.0402, 0.5512, 0.0108])
N: 400
Signal to noise ratio: tensor([16.5645, 50.1638, 69.2624, 34.4160])
Bound on condition number: tensor([ 109753.7633, 1006563.5560, 1918913.4718,  473785.9510])
Policy Optimizer learning rate:
0.09597421723058139
Experience 40, Iter 0, disc loss: 4.980061738399925e-05, policy loss: 10.547496590942373
Experience 40, Iter 1, disc loss: 4.437190200337801e-05, policy loss: 10.414440707368705
Experience 40, Iter 2, disc loss: 4.30669944910981e-05, policy loss: 10.555678857444981
Experience 40, Iter 3, disc loss: 4.406572621002361e-05, policy loss: 10.553996007498641
Experience 40, Iter 4, disc loss: 3.9003830218902976e-05, policy loss: 10.862811355368299
Experience 40, Iter 5, disc loss: 4.5185654235582005e-05, policy loss: 10.45277261565105
Experience 40, Iter 6, disc loss: 4.772565466117269e-05, policy loss: 10.483315255830892
Experience 40, Iter 7, disc loss: 4.925880775939317e-05, policy loss: 10.494988519994738
Experience 40, Iter 8, disc loss: 4.410046311257829e-05, policy loss: 10.593915226886203
Experience 40, Iter 9, disc loss: 4.922721628439932e-05, policy loss: 10.3839560509393
Experience 40, Iter 10, disc loss: 4.893529748321839e-05, policy loss: 10.84793167975861
Experience 40, Iter 11, disc loss: 4.269148148913905e-05, policy loss: 10.556749780175158
Experience 40, Iter 12, disc loss: 4.5549384157814765e-05, policy loss: 10.473738832593812
Experience 40, Iter 13, disc loss: 3.8432521954094916e-05, policy loss: 10.830097732985186
Experience 40, Iter 14, disc loss: 3.661322873569067e-05, policy loss: 10.76894441178764
Experience 40, Iter 15, disc loss: 4.155084380852919e-05, policy loss: 10.52266406958875
Experience 40, Iter 16, disc loss: 3.7131622771724355e-05, policy loss: 10.665970543516599
Experience 40, Iter 17, disc loss: 4.3834732609350986e-05, policy loss: 10.616456476571436
Experience 40, Iter 18, disc loss: 3.8772706913971846e-05, policy loss: 10.674016571535933
Experience 40, Iter 19, disc loss: 4.268772123694758e-05, policy loss: 10.522699323910809
Experience 40, Iter 20, disc loss: 4.518296260589516e-05, policy loss: 10.507699715126508
Experience 40, Iter 21, disc loss: 3.9335620650959245e-05, policy loss: 10.735660340414546
Experience 40, Iter 22, disc loss: 4.3406576141811184e-05, policy loss: 10.614277465637148
Experience 40, Iter 23, disc loss: 5.004306599695788e-05, policy loss: 10.658456641448455
Experience 40, Iter 24, disc loss: 4.060031688617768e-05, policy loss: 10.831428675951525
Experience 40, Iter 25, disc loss: 4.144988509197534e-05, policy loss: 10.839296981833108
Experience 40, Iter 26, disc loss: 3.88971245607947e-05, policy loss: 10.839348743473433
Experience 40, Iter 27, disc loss: 5.188837591972866e-05, policy loss: 10.490685122237235
Experience 40, Iter 28, disc loss: 3.807764048726097e-05, policy loss: 10.786996667095018
Experience 40, Iter 29, disc loss: 3.878993335915114e-05, policy loss: 10.679811253519059
Experience 40, Iter 30, disc loss: 4.331155559839857e-05, policy loss: 10.614736839452004
Experience 40, Iter 31, disc loss: 4.448230294015776e-05, policy loss: 10.507497473256366
Experience 40, Iter 32, disc loss: 5.461747555991076e-05, policy loss: 10.535855835232613
Experience 40, Iter 33, disc loss: 3.980378530940805e-05, policy loss: 10.688112972359189
Experience 40, Iter 34, disc loss: 5.242220643276306e-05, policy loss: 10.502784960282643
Experience 40, Iter 35, disc loss: 4.9967458553031176e-05, policy loss: 10.532347953808614
Experience 40, Iter 36, disc loss: 4.2432236455952023e-05, policy loss: 10.540981775592154
Experience 40, Iter 37, disc loss: 3.8906276115741826e-05, policy loss: 10.698187788177433
Experience 40, Iter 38, disc loss: 3.8530231706119995e-05, policy loss: 10.827280042158586
Experience 40, Iter 39, disc loss: 4.649564185041517e-05, policy loss: 10.375115973212992
Experience 40, Iter 40, disc loss: 4.122700473168171e-05, policy loss: 10.682794657168735
Experience 40, Iter 41, disc loss: 4.6091415240341145e-05, policy loss: 10.597154177443489
Experience 40, Iter 42, disc loss: 4.101067888006386e-05, policy loss: 10.513555610456475
Experience 40, Iter 43, disc loss: 3.851627164895223e-05, policy loss: 10.687803727216753
Experience 40, Iter 44, disc loss: 4.029630811950507e-05, policy loss: 10.705873283413464
Experience 40, Iter 45, disc loss: 4.5612963391346946e-05, policy loss: 10.577490595635632
Experience 40, Iter 46, disc loss: 4.792709535430427e-05, policy loss: 10.52422919981721
Experience 40, Iter 47, disc loss: 4.117640031138735e-05, policy loss: 10.84457359228114
Experience 40, Iter 48, disc loss: 4.6547906111566984e-05, policy loss: 10.452673048373942
Experience 40, Iter 49, disc loss: 4.202563638682765e-05, policy loss: 10.813219907141779
Experience 40, Iter 50, disc loss: 4.19303745115593e-05, policy loss: 10.63105334384321
Experience 40, Iter 51, disc loss: 5.1584240530668556e-05, policy loss: 10.67730910164331
Experience 40, Iter 52, disc loss: 4.028684304247035e-05, policy loss: 10.651885620881364
Experience 40, Iter 53, disc loss: 4.4360665423100677e-05, policy loss: 10.48161415468942
Experience 40, Iter 54, disc loss: 3.8878906543838576e-05, policy loss: 10.797360164918913
Experience 40, Iter 55, disc loss: 4.461532226918797e-05, policy loss: 10.554850142073574
Experience 40, Iter 56, disc loss: 3.8803481101650456e-05, policy loss: 10.575820874558033
Experience 40, Iter 57, disc loss: 4.4568560553269544e-05, policy loss: 10.395154851060527
Experience 40, Iter 58, disc loss: 3.857372633367363e-05, policy loss: 10.76674946611369
Experience 40, Iter 59, disc loss: 4.97416517587028e-05, policy loss: 10.41850262704515
Experience 40, Iter 60, disc loss: 4.743147938903914e-05, policy loss: 10.488387230675345
Experience 40, Iter 61, disc loss: 4.57913751438676e-05, policy loss: 10.612115378915409
Experience 40, Iter 62, disc loss: 4.194875786572627e-05, policy loss: 10.664608320607758
Experience 40, Iter 63, disc loss: 3.772441128877789e-05, policy loss: 10.84599505444224
Experience 40, Iter 64, disc loss: 4.026305117694299e-05, policy loss: 10.799485113721042
Experience 40, Iter 65, disc loss: 4.294840647297421e-05, policy loss: 10.632976316572627
Experience 40, Iter 66, disc loss: 3.856636210084776e-05, policy loss: 10.682057788350498
Experience 40, Iter 67, disc loss: 3.4329262846029626e-05, policy loss: 10.67476027175779
Experience 40, Iter 68, disc loss: 3.80857769110652e-05, policy loss: 10.61516645605604
Experience 40, Iter 69, disc loss: 3.628278472078927e-05, policy loss: 10.696777837588161
Experience 40, Iter 70, disc loss: 4.385779806095517e-05, policy loss: 10.51981876448836
Experience 40, Iter 71, disc loss: 3.904821849869684e-05, policy loss: 10.617890884970581
Experience 40, Iter 72, disc loss: 3.638046996893087e-05, policy loss: 10.64682541922137
Experience 40, Iter 73, disc loss: 3.6720625072881436e-05, policy loss: 10.818366407141148
Experience 40, Iter 74, disc loss: 3.6797452792650135e-05, policy loss: 10.726028512252014
Experience 40, Iter 75, disc loss: 4.1417967786157074e-05, policy loss: 10.725362655076768
Experience 40, Iter 76, disc loss: 3.5592267737947224e-05, policy loss: 10.797949174575336
Experience 40, Iter 77, disc loss: 4.356709796307073e-05, policy loss: 10.669856133651365
Experience 40, Iter 78, disc loss: 4.4010953901230514e-05, policy loss: 10.498475390875203
Experience 40, Iter 79, disc loss: 5.003201717000606e-05, policy loss: 10.627900991385618
Experience 40, Iter 80, disc loss: 4.101024513492608e-05, policy loss: 10.558112680126003
Experience 40, Iter 81, disc loss: 4.328597734362802e-05, policy loss: 10.593907632040082
Experience 40, Iter 82, disc loss: 4.158589646094079e-05, policy loss: 10.562320255084618
Experience 40, Iter 83, disc loss: 4.0913156812109094e-05, policy loss: 10.757299896226428
Experience 40, Iter 84, disc loss: 4.353202507359186e-05, policy loss: 10.493643398621028
Experience 40, Iter 85, disc loss: 4.1735290053158425e-05, policy loss: 10.711054648485725
Experience 40, Iter 86, disc loss: 4.64671328636622e-05, policy loss: 10.50738048924562
Experience 40, Iter 87, disc loss: 3.43157296631729e-05, policy loss: 10.859135685315568
Experience 40, Iter 88, disc loss: 4.783008625987871e-05, policy loss: 10.56652665708427
Experience 40, Iter 89, disc loss: 4.1582834549416724e-05, policy loss: 10.671743825141103
Experience 40, Iter 90, disc loss: 3.8516194960889584e-05, policy loss: 10.771689525301056
Experience 40, Iter 91, disc loss: 3.429503891560171e-05, policy loss: 10.942520451066468
Experience 40, Iter 92, disc loss: 3.4599958014530836e-05, policy loss: 11.004637932026768
Experience 40, Iter 93, disc loss: 4.251191708261746e-05, policy loss: 10.517237231067167
Experience 40, Iter 94, disc loss: 3.988232735952098e-05, policy loss: 10.654265507039696
Experience 40, Iter 95, disc loss: 3.86010719481696e-05, policy loss: 10.603716975277893
Experience 40, Iter 96, disc loss: 4.532679470286467e-05, policy loss: 10.67127338258515
Experience 40, Iter 97, disc loss: 4.080655822316856e-05, policy loss: 10.704338327567019
Experience 40, Iter 98, disc loss: 3.7727252037768836e-05, policy loss: 10.721386922120631
Experience 40, Iter 99, disc loss: 4.7046981549743e-05, policy loss: 10.443327908523026
Experience: 41
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.0098],
        [0.1354],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.0942, 0.1195, 0.0035, 0.0013, 0.3660]],

        [[0.0153, 0.0942, 0.1195, 0.0035, 0.0013, 0.3660]],

        [[0.0153, 0.0942, 0.1195, 0.0035, 0.0013, 0.3660]],

        [[0.0153, 0.0942, 0.1195, 0.0035, 0.0013, 0.3660]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0104, 0.0393, 0.5418, 0.0106], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0104, 0.0393, 0.5418, 0.0106])
N: 410
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1641.0000, 1641.0000, 1641.0000, 1641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.763
Iter 2/2000 - Loss: -0.312
Iter 3/2000 - Loss: -0.836
Iter 4/2000 - Loss: -0.836
Iter 5/2000 - Loss: -0.698
Iter 6/2000 - Loss: -0.801
Iter 7/2000 - Loss: -1.017
Iter 8/2000 - Loss: -1.189
Iter 9/2000 - Loss: -1.301
Iter 10/2000 - Loss: -1.411
Iter 11/2000 - Loss: -1.568
Iter 12/2000 - Loss: -1.778
Iter 13/2000 - Loss: -2.024
Iter 14/2000 - Loss: -2.291
Iter 15/2000 - Loss: -2.574
Iter 16/2000 - Loss: -2.872
Iter 17/2000 - Loss: -3.185
Iter 18/2000 - Loss: -3.513
Iter 19/2000 - Loss: -3.851
Iter 20/2000 - Loss: -4.196
Iter 1981/2000 - Loss: -8.942
Iter 1982/2000 - Loss: -8.942
Iter 1983/2000 - Loss: -8.942
Iter 1984/2000 - Loss: -8.942
Iter 1985/2000 - Loss: -8.942
Iter 1986/2000 - Loss: -8.942
Iter 1987/2000 - Loss: -8.942
Iter 1988/2000 - Loss: -8.942
Iter 1989/2000 - Loss: -8.942
Iter 1990/2000 - Loss: -8.942
Iter 1991/2000 - Loss: -8.942
Iter 1992/2000 - Loss: -8.942
Iter 1993/2000 - Loss: -8.942
Iter 1994/2000 - Loss: -8.942
Iter 1995/2000 - Loss: -8.942
Iter 1996/2000 - Loss: -8.942
Iter 1997/2000 - Loss: -8.942
Iter 1998/2000 - Loss: -8.942
Iter 1999/2000 - Loss: -8.942
Iter 2000/2000 - Loss: -8.942
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.6601,  5.1688, 19.9768,  8.0700, 16.5229, 35.0892]],

        [[13.3645, 23.7016, 11.6344,  2.2251,  1.0771, 18.6276]],

        [[14.1260, 24.5183, 17.2761,  0.9632,  1.4699, 17.2752]],

        [[13.0357, 23.0346, 15.3671,  3.0194,  2.1182, 39.1669]]])
Signal Variance: tensor([ 0.0840,  1.0224, 11.9664,  0.3277])
Estimated target variance: tensor([0.0104, 0.0393, 0.5418, 0.0106])
N: 410
Signal to noise ratio: tensor([16.3830, 51.0192, 70.2060, 34.2108])
Bound on condition number: tensor([ 110045.4761, 1067212.8577, 2020845.4010,  479857.3205])
Policy Optimizer learning rate:
0.09587315155141833
Experience 41, Iter 0, disc loss: 3.9653368429270715e-05, policy loss: 10.738485326112768
Experience 41, Iter 1, disc loss: 4.203518853335306e-05, policy loss: 10.810475336251955
Experience 41, Iter 2, disc loss: 3.808066309451839e-05, policy loss: 10.72118919520852
Experience 41, Iter 3, disc loss: 4.179333301972981e-05, policy loss: 10.670672498236843
Experience 41, Iter 4, disc loss: 3.649286461787586e-05, policy loss: 10.782443930784666
Experience 41, Iter 5, disc loss: 3.89524038609789e-05, policy loss: 10.733666077631987
Experience 41, Iter 6, disc loss: 4.214046088614673e-05, policy loss: 10.521537158640614
Experience 41, Iter 7, disc loss: 3.571513878317067e-05, policy loss: 10.707679300109234
Experience 41, Iter 8, disc loss: 3.793694912315933e-05, policy loss: 10.565055635191184
Experience 41, Iter 9, disc loss: 4.210742625500648e-05, policy loss: 10.536620209371435
Experience 41, Iter 10, disc loss: 3.874993677778562e-05, policy loss: 10.635875953466154
Experience 41, Iter 11, disc loss: 4.20916885776992e-05, policy loss: 10.600869970930287
Experience 41, Iter 12, disc loss: 4.171369970277692e-05, policy loss: 10.816014434803934
Experience 41, Iter 13, disc loss: 3.012597897889286e-05, policy loss: 11.049775416735777
Experience 41, Iter 14, disc loss: 3.816883007533258e-05, policy loss: 10.75742196918772
Experience 41, Iter 15, disc loss: 4.680591535950534e-05, policy loss: 10.604850630134177
Experience 41, Iter 16, disc loss: 4.070791858329983e-05, policy loss: 10.733455843067635
Experience 41, Iter 17, disc loss: 3.750236548501587e-05, policy loss: 10.657376955515447
Experience 41, Iter 18, disc loss: 3.921148865404041e-05, policy loss: 10.597952730904009
Experience 41, Iter 19, disc loss: 3.883521378872415e-05, policy loss: 10.648370602520387
Experience 41, Iter 20, disc loss: 3.580277598862142e-05, policy loss: 10.70484464914792
Experience 41, Iter 21, disc loss: 3.599921277180483e-05, policy loss: 10.796607973536712
Experience 41, Iter 22, disc loss: 3.8776152901263126e-05, policy loss: 10.701222080207753
Experience 41, Iter 23, disc loss: 4.267287507332558e-05, policy loss: 10.567983835211422
Experience 41, Iter 24, disc loss: 3.3678438270297615e-05, policy loss: 10.931842587343745
Experience 41, Iter 25, disc loss: 4.113328376504732e-05, policy loss: 10.565074689615136
Experience 41, Iter 26, disc loss: 4.903919758260738e-05, policy loss: 10.60224792589655
Experience 41, Iter 27, disc loss: 4.096083060318299e-05, policy loss: 10.556089493618588
Experience 41, Iter 28, disc loss: 3.587161228100158e-05, policy loss: 10.647788453997315
Experience 41, Iter 29, disc loss: 3.8595449017676785e-05, policy loss: 10.720936242576606
Experience 41, Iter 30, disc loss: 3.853973474373084e-05, policy loss: 10.718263031848394
Experience 41, Iter 31, disc loss: 4.542996930539979e-05, policy loss: 10.512337006504902
Experience 41, Iter 32, disc loss: 3.917228224773441e-05, policy loss: 11.008871855135315
Experience 41, Iter 33, disc loss: 4.1512454388303945e-05, policy loss: 10.58928244227224
Experience 41, Iter 34, disc loss: 3.906273040636977e-05, policy loss: 10.52616453788374
Experience 41, Iter 35, disc loss: 4.4686161167225845e-05, policy loss: 10.42508970965895
Experience 41, Iter 36, disc loss: 3.809329413855305e-05, policy loss: 10.66271642391446
Experience 41, Iter 37, disc loss: 4.1663769774089425e-05, policy loss: 10.580241160549697
Experience 41, Iter 38, disc loss: 4.578014803462754e-05, policy loss: 10.695373170344178
Experience 41, Iter 39, disc loss: 4.065696908702994e-05, policy loss: 10.59836552998523
Experience 41, Iter 40, disc loss: 4.4988050957740296e-05, policy loss: 10.738041502534948
Experience 41, Iter 41, disc loss: 4.4392169026840794e-05, policy loss: 10.557050272406556
Experience 41, Iter 42, disc loss: 3.504957831046015e-05, policy loss: 10.873581800600196
Experience 41, Iter 43, disc loss: 3.972695867008867e-05, policy loss: 10.74708538423713
Experience 41, Iter 44, disc loss: 3.654540651836732e-05, policy loss: 10.604879907992583
Experience 41, Iter 45, disc loss: 3.638856091785386e-05, policy loss: 10.823699581449265
Experience 41, Iter 46, disc loss: 3.071734203685312e-05, policy loss: 10.857003528683258
Experience 41, Iter 47, disc loss: 3.7848147908552804e-05, policy loss: 10.609005866891291
Experience 41, Iter 48, disc loss: 3.6100402220214895e-05, policy loss: 10.665741101249347
Experience 41, Iter 49, disc loss: 3.538485270009896e-05, policy loss: 10.77396734815302
Experience 41, Iter 50, disc loss: 4.217495604304834e-05, policy loss: 10.61232956628367
Experience 41, Iter 51, disc loss: 3.933707127854602e-05, policy loss: 10.668480021170975
Experience 41, Iter 52, disc loss: 4.4422824785309985e-05, policy loss: 10.608338266578638
Experience 41, Iter 53, disc loss: 4.473270087000825e-05, policy loss: 10.657200943242763
Experience 41, Iter 54, disc loss: 3.6302254395615046e-05, policy loss: 10.778563121841955
Experience 41, Iter 55, disc loss: 4.577365073733562e-05, policy loss: 10.594567318795447
Experience 41, Iter 56, disc loss: 3.4915875332366825e-05, policy loss: 10.840913862015746
Experience 41, Iter 57, disc loss: 4.341715253370615e-05, policy loss: 10.468969667379525
Experience 41, Iter 58, disc loss: 3.340169705213478e-05, policy loss: 10.872895175839655
Experience 41, Iter 59, disc loss: 3.551151320124716e-05, policy loss: 10.711067085268223
Experience 41, Iter 60, disc loss: 3.175744658403376e-05, policy loss: 10.866070852253717
Experience 41, Iter 61, disc loss: 3.9049132476136315e-05, policy loss: 10.645293667221962
Experience 41, Iter 62, disc loss: 3.9632167959115626e-05, policy loss: 10.584402815456306
Experience 41, Iter 63, disc loss: 3.269509680004378e-05, policy loss: 10.773762221371536
Experience 41, Iter 64, disc loss: 3.839137410094694e-05, policy loss: 10.867739126872358
Experience 41, Iter 65, disc loss: 3.833102323088554e-05, policy loss: 10.671410966827153
Experience 41, Iter 66, disc loss: 3.648520449945191e-05, policy loss: 10.777937803504958
Experience 41, Iter 67, disc loss: 4.0847596558177556e-05, policy loss: 10.601413873000622
Experience 41, Iter 68, disc loss: 3.768054033743101e-05, policy loss: 10.765059717489988
Experience 41, Iter 69, disc loss: 5.091322672423436e-05, policy loss: 10.62881707991685
Experience 41, Iter 70, disc loss: 4.060333454184887e-05, policy loss: 10.794570779237851
Experience 41, Iter 71, disc loss: 3.8704869447992234e-05, policy loss: 10.670230435485868
Experience 41, Iter 72, disc loss: 3.8598843017135644e-05, policy loss: 10.607720274282837
Experience 41, Iter 73, disc loss: 4.357998887220173e-05, policy loss: 10.480262460555661
Experience 41, Iter 74, disc loss: 4.0305995830196144e-05, policy loss: 10.59779553455736
Experience 41, Iter 75, disc loss: 3.6681176111771274e-05, policy loss: 10.820875054251001
Experience 41, Iter 76, disc loss: 3.883645068109374e-05, policy loss: 10.6044890512246
Experience 41, Iter 77, disc loss: 3.72956713361429e-05, policy loss: 10.816435470157456
Experience 41, Iter 78, disc loss: 3.808209483536926e-05, policy loss: 10.627807576989188
Experience 41, Iter 79, disc loss: 3.7227198283994145e-05, policy loss: 10.752515249811353
Experience 41, Iter 80, disc loss: 3.64220734265046e-05, policy loss: 10.803295741579264
Experience 41, Iter 81, disc loss: 3.169516037019408e-05, policy loss: 10.877772811102227
Experience 41, Iter 82, disc loss: 3.943167253420116e-05, policy loss: 10.872507355396936
Experience 41, Iter 83, disc loss: 3.2439762085194035e-05, policy loss: 10.7808595932709
Experience 41, Iter 84, disc loss: 3.79422417106249e-05, policy loss: 10.562313055388824
Experience 41, Iter 85, disc loss: 3.706155819482755e-05, policy loss: 10.670336979941982
Experience 41, Iter 86, disc loss: 4.1006136835110876e-05, policy loss: 10.538144372584107
Experience 41, Iter 87, disc loss: 4.897326925046233e-05, policy loss: 10.645340577220978
Experience 41, Iter 88, disc loss: 3.722398928155588e-05, policy loss: 10.732531538836968
Experience 41, Iter 89, disc loss: 5.1249272109770626e-05, policy loss: 10.62190391945498
Experience 41, Iter 90, disc loss: 3.9609667485256e-05, policy loss: 10.903451601410328
Experience 41, Iter 91, disc loss: 3.700078743607414e-05, policy loss: 10.871704452433683
Experience 41, Iter 92, disc loss: 4.4448363938882e-05, policy loss: 10.61988488568386
Experience 41, Iter 93, disc loss: 4.106922637420095e-05, policy loss: 10.673520084605942
Experience 41, Iter 94, disc loss: 3.9865272014121357e-05, policy loss: 10.760028408136012
Experience 41, Iter 95, disc loss: 3.3784303502661155e-05, policy loss: 10.703603506027616
Experience 41, Iter 96, disc loss: 3.5277471037136706e-05, policy loss: 10.803316254788239
Experience 41, Iter 97, disc loss: 4.1421222072540534e-05, policy loss: 10.930756891290521
Experience 41, Iter 98, disc loss: 3.529566311474636e-05, policy loss: 10.698886847669458
Experience 41, Iter 99, disc loss: 4.0435509413269944e-05, policy loss: 10.836219777463919
Experience: 42
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0096],
        [0.1322],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0149, 0.0921, 0.1167, 0.0035, 0.0013, 0.3579]],

        [[0.0149, 0.0921, 0.1167, 0.0035, 0.0013, 0.3579]],

        [[0.0149, 0.0921, 0.1167, 0.0035, 0.0013, 0.3579]],

        [[0.0149, 0.0921, 0.1167, 0.0035, 0.0013, 0.3579]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0101, 0.0385, 0.5289, 0.0104], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0101, 0.0385, 0.5289, 0.0104])
N: 420
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1681.0000, 1681.0000, 1681.0000, 1681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.810
Iter 2/2000 - Loss: -0.338
Iter 3/2000 - Loss: -0.881
Iter 4/2000 - Loss: -0.877
Iter 5/2000 - Loss: -0.732
Iter 6/2000 - Loss: -0.836
Iter 7/2000 - Loss: -1.054
Iter 8/2000 - Loss: -1.227
Iter 9/2000 - Loss: -1.335
Iter 10/2000 - Loss: -1.441
Iter 11/2000 - Loss: -1.596
Iter 12/2000 - Loss: -1.804
Iter 13/2000 - Loss: -2.049
Iter 14/2000 - Loss: -2.315
Iter 15/2000 - Loss: -2.597
Iter 16/2000 - Loss: -2.894
Iter 17/2000 - Loss: -3.207
Iter 18/2000 - Loss: -3.535
Iter 19/2000 - Loss: -3.874
Iter 20/2000 - Loss: -4.219
Iter 1981/2000 - Loss: -8.950
Iter 1982/2000 - Loss: -8.950
Iter 1983/2000 - Loss: -8.950
Iter 1984/2000 - Loss: -8.950
Iter 1985/2000 - Loss: -8.950
Iter 1986/2000 - Loss: -8.950
Iter 1987/2000 - Loss: -8.950
Iter 1988/2000 - Loss: -8.950
Iter 1989/2000 - Loss: -8.950
Iter 1990/2000 - Loss: -8.950
Iter 1991/2000 - Loss: -8.950
Iter 1992/2000 - Loss: -8.950
Iter 1993/2000 - Loss: -8.950
Iter 1994/2000 - Loss: -8.950
Iter 1995/2000 - Loss: -8.950
Iter 1996/2000 - Loss: -8.950
Iter 1997/2000 - Loss: -8.950
Iter 1998/2000 - Loss: -8.950
Iter 1999/2000 - Loss: -8.950
Iter 2000/2000 - Loss: -8.950
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.6065,  5.1611, 19.8623,  8.0514, 16.5410, 34.9192]],

        [[13.2589, 23.5919, 11.6168,  2.1708,  1.0860, 18.6444]],

        [[13.9116, 24.2434, 17.2541,  0.9601,  1.4547, 17.2071]],

        [[12.9338, 22.8227, 15.3008,  3.0107,  2.0957, 39.0629]]])
Signal Variance: tensor([ 0.0836,  1.0113, 11.8976,  0.3247])
Estimated target variance: tensor([0.0101, 0.0385, 0.5289, 0.0104])
N: 420
Signal to noise ratio: tensor([16.4045, 50.7553, 70.0439, 33.8370])
Bound on condition number: tensor([ 113025.9948, 1081965.3073, 2060582.7748,  480876.4451])
Policy Optimizer learning rate:
0.0957721922995
Experience 42, Iter 0, disc loss: 3.607417036665939e-05, policy loss: 10.621591559314066
Experience 42, Iter 1, disc loss: 3.587182655236732e-05, policy loss: 10.783337292222853
Experience 42, Iter 2, disc loss: 3.732167313666523e-05, policy loss: 10.738654316416937
Experience 42, Iter 3, disc loss: 3.6222864681681935e-05, policy loss: 10.709707970864137
Experience 42, Iter 4, disc loss: 3.351755878244157e-05, policy loss: 10.959461063343202
Experience 42, Iter 5, disc loss: 4.3620449071399843e-05, policy loss: 10.500038888831417
Experience 42, Iter 6, disc loss: 3.8237158341217206e-05, policy loss: 10.84966561711084
Experience 42, Iter 7, disc loss: 3.345224028012652e-05, policy loss: 10.809107126440363
Experience 42, Iter 8, disc loss: 3.750232235022635e-05, policy loss: 10.798985059647535
Experience 42, Iter 9, disc loss: 4.43736757788565e-05, policy loss: 10.459573421350239
Experience 42, Iter 10, disc loss: 3.942887169476997e-05, policy loss: 10.930021303451127
Experience 42, Iter 11, disc loss: 3.967640465655591e-05, policy loss: 10.736990530179774
Experience 42, Iter 12, disc loss: 4.098407097537609e-05, policy loss: 10.616772072524201
Experience 42, Iter 13, disc loss: 3.68018638569906e-05, policy loss: 10.797961716883398
Experience 42, Iter 14, disc loss: 4.0165565582521795e-05, policy loss: 10.740982349625018
Experience 42, Iter 15, disc loss: 3.5979358918613535e-05, policy loss: 10.750932522857866
Experience 42, Iter 16, disc loss: 3.772954029850032e-05, policy loss: 10.871975253787106
Experience 42, Iter 17, disc loss: 3.305763383098953e-05, policy loss: 10.847380287760505
Experience 42, Iter 18, disc loss: 3.7844943297612424e-05, policy loss: 10.66564284723953
Experience 42, Iter 19, disc loss: 3.78207807752489e-05, policy loss: 10.84265548476818
Experience 42, Iter 20, disc loss: 3.2564672359378665e-05, policy loss: 10.927294173796419
Experience 42, Iter 21, disc loss: 3.3063891845719154e-05, policy loss: 10.861962072177118
Experience 42, Iter 22, disc loss: 3.3892022008981545e-05, policy loss: 10.741557526731416
Experience 42, Iter 23, disc loss: 3.533351423946859e-05, policy loss: 10.861447092282187
Experience 42, Iter 24, disc loss: 4.1296409598520695e-05, policy loss: 10.713665283716427
Experience 42, Iter 25, disc loss: 3.49412033820492e-05, policy loss: 10.834185457926683
Experience 42, Iter 26, disc loss: 3.8883356579598775e-05, policy loss: 10.853152710284954
Experience 42, Iter 27, disc loss: 3.817545287346e-05, policy loss: 10.583291281590416
Experience 42, Iter 28, disc loss: 3.4620743912466354e-05, policy loss: 10.792222191920782
Experience 42, Iter 29, disc loss: 3.321703105944145e-05, policy loss: 10.821803495146408
Experience 42, Iter 30, disc loss: 4.370780851294427e-05, policy loss: 10.562253977340202
Experience 42, Iter 31, disc loss: 3.87792852496582e-05, policy loss: 10.70403145399057
Experience 42, Iter 32, disc loss: 3.659532051537043e-05, policy loss: 10.728173136745605
Experience 42, Iter 33, disc loss: 4.835326649620092e-05, policy loss: 10.433987738346016
Experience 42, Iter 34, disc loss: 4.35193180454989e-05, policy loss: 10.612042693581765
Experience 42, Iter 35, disc loss: 3.904600589202983e-05, policy loss: 10.693920792192953
Experience 42, Iter 36, disc loss: 4.2760861511462216e-05, policy loss: 10.70139741007469
Experience 42, Iter 37, disc loss: 4.139803479100816e-05, policy loss: 10.63864945967467
Experience 42, Iter 38, disc loss: 4.407524156824802e-05, policy loss: 10.723222423151027
Experience 42, Iter 39, disc loss: 4.345523568622716e-05, policy loss: 10.521947115267078
Experience 42, Iter 40, disc loss: 3.6019426030926244e-05, policy loss: 10.776136538834432
Experience 42, Iter 41, disc loss: 3.477616107354174e-05, policy loss: 10.731650957501536
Experience 42, Iter 42, disc loss: 3.4886089924653394e-05, policy loss: 10.800167906542018
Experience 42, Iter 43, disc loss: 3.504674813089912e-05, policy loss: 10.870958569858262
Experience 42, Iter 44, disc loss: 2.775979459860005e-05, policy loss: 11.045634080609398
Experience 42, Iter 45, disc loss: 3.450359646627194e-05, policy loss: 10.713296445916715
Experience 42, Iter 46, disc loss: 3.3633675718749056e-05, policy loss: 10.7582800066929
Experience 42, Iter 47, disc loss: 3.72990719360724e-05, policy loss: 10.76230533742432
Experience 42, Iter 48, disc loss: 3.466518400873794e-05, policy loss: 10.848065742572631
Experience 42, Iter 49, disc loss: 3.0993143747116064e-05, policy loss: 10.93964958895139
Experience 42, Iter 50, disc loss: 3.675296471515171e-05, policy loss: 10.789050413441078
Experience 42, Iter 51, disc loss: 3.924463692836235e-05, policy loss: 10.729478719935877
Experience 42, Iter 52, disc loss: 3.614925108729155e-05, policy loss: 10.801968782003474
Experience 42, Iter 53, disc loss: 3.605208838969548e-05, policy loss: 10.749290471045489
Experience 42, Iter 54, disc loss: 3.106120343651173e-05, policy loss: 10.9694393083598
Experience 42, Iter 55, disc loss: 3.980747684672936e-05, policy loss: 10.578669815323272
Experience 42, Iter 56, disc loss: 3.727717737893684e-05, policy loss: 10.599869509605558
Experience 42, Iter 57, disc loss: 4.5646068328730006e-05, policy loss: 10.658358960118859
Experience 42, Iter 58, disc loss: 5.049379888861831e-05, policy loss: 10.471367802915015
Experience 42, Iter 59, disc loss: 3.6953706177329297e-05, policy loss: 10.825392759426453
Experience 42, Iter 60, disc loss: 3.693185045031303e-05, policy loss: 10.818242302150468
Experience 42, Iter 61, disc loss: 3.6247500856588985e-05, policy loss: 10.706729874673997
Experience 42, Iter 62, disc loss: 3.746364077594217e-05, policy loss: 10.675685373439755
Experience 42, Iter 63, disc loss: 3.405440594170426e-05, policy loss: 10.840215993699918
Experience 42, Iter 64, disc loss: 3.7994089924151624e-05, policy loss: 10.611415240587743
Experience 42, Iter 65, disc loss: 3.678189560973331e-05, policy loss: 10.860757404126977
Experience 42, Iter 66, disc loss: 3.267814126093592e-05, policy loss: 10.85939188778288
Experience 42, Iter 67, disc loss: 3.633912973429677e-05, policy loss: 10.65303050983822
Experience 42, Iter 68, disc loss: 3.2788601507495165e-05, policy loss: 10.905373800827379
Experience 42, Iter 69, disc loss: 3.307140201950856e-05, policy loss: 10.845687917184117
Experience 42, Iter 70, disc loss: 2.8804278572171055e-05, policy loss: 10.984950220287734
Experience 42, Iter 71, disc loss: 3.321589214304618e-05, policy loss: 10.812125612047485
Experience 42, Iter 72, disc loss: 3.662684625991696e-05, policy loss: 10.758802553166785
Experience 42, Iter 73, disc loss: 3.347650466290359e-05, policy loss: 10.867143558190616
Experience 42, Iter 74, disc loss: 3.738879996100719e-05, policy loss: 10.63300498952999
Experience 42, Iter 75, disc loss: 2.920740581365478e-05, policy loss: 11.169962564508317
Experience 42, Iter 76, disc loss: 3.8836845583856426e-05, policy loss: 10.762422604940145
Experience 42, Iter 77, disc loss: 4.061498913217136e-05, policy loss: 10.614532445676986
Experience 42, Iter 78, disc loss: 4.1693635118078486e-05, policy loss: 10.613367769865201
Experience 42, Iter 79, disc loss: 3.3615094285639486e-05, policy loss: 10.93981374754677
Experience 42, Iter 80, disc loss: 3.5789528375400676e-05, policy loss: 10.8134031665703
Experience 42, Iter 81, disc loss: 3.097761020009932e-05, policy loss: 11.00885974971935
Experience 42, Iter 82, disc loss: 3.799366870688833e-05, policy loss: 10.776596572810245
Experience 42, Iter 83, disc loss: 3.122883752112754e-05, policy loss: 10.883290228732587
Experience 42, Iter 84, disc loss: 3.535182301995851e-05, policy loss: 10.814097401942025
Experience 42, Iter 85, disc loss: 3.2231211879079484e-05, policy loss: 10.892349406789132
Experience 42, Iter 86, disc loss: 3.6643424715512666e-05, policy loss: 10.740201178685474
Experience 42, Iter 87, disc loss: 4.012271119793418e-05, policy loss: 10.619959547802083
Experience 42, Iter 88, disc loss: 3.250550926244824e-05, policy loss: 10.83910607873894
Experience 42, Iter 89, disc loss: 3.707083958095187e-05, policy loss: 10.79201340901378
Experience 42, Iter 90, disc loss: 4.9226750163150734e-05, policy loss: 10.456726341464872
Experience 42, Iter 91, disc loss: 3.540275155871076e-05, policy loss: 10.797403719067878
Experience 42, Iter 92, disc loss: 3.69742192634117e-05, policy loss: 10.757192353084285
Experience 42, Iter 93, disc loss: 4.0696685874022804e-05, policy loss: 10.82773206328901
Experience 42, Iter 94, disc loss: 3.288468036513171e-05, policy loss: 10.982233298254258
Experience 42, Iter 95, disc loss: 3.5944726887212104e-05, policy loss: 10.940531766443328
Experience 42, Iter 96, disc loss: 3.6331539922859726e-05, policy loss: 10.785593104756998
Experience 42, Iter 97, disc loss: 3.783260232222441e-05, policy loss: 10.848894207144173
Experience 42, Iter 98, disc loss: 3.573852921176031e-05, policy loss: 10.698943825201644
Experience 42, Iter 99, disc loss: 3.075478152539811e-05, policy loss: 10.891325626144166
Experience: 43
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0094],
        [0.1293],
        [0.0025]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0146, 0.0901, 0.1141, 0.0034, 0.0013, 0.3500]],

        [[0.0146, 0.0901, 0.1141, 0.0034, 0.0013, 0.3500]],

        [[0.0146, 0.0901, 0.1141, 0.0034, 0.0013, 0.3500]],

        [[0.0146, 0.0901, 0.1141, 0.0034, 0.0013, 0.3500]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.0376, 0.5171, 0.0101], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.0376, 0.5171, 0.0101])
N: 430
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1721.0000, 1721.0000, 1721.0000, 1721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.855
Iter 2/2000 - Loss: -0.361
Iter 3/2000 - Loss: -0.923
Iter 4/2000 - Loss: -0.914
Iter 5/2000 - Loss: -0.760
Iter 6/2000 - Loss: -0.865
Iter 7/2000 - Loss: -1.087
Iter 8/2000 - Loss: -1.260
Iter 9/2000 - Loss: -1.365
Iter 10/2000 - Loss: -1.468
Iter 11/2000 - Loss: -1.620
Iter 12/2000 - Loss: -1.827
Iter 13/2000 - Loss: -2.072
Iter 14/2000 - Loss: -2.339
Iter 15/2000 - Loss: -2.619
Iter 16/2000 - Loss: -2.915
Iter 17/2000 - Loss: -3.228
Iter 18/2000 - Loss: -3.555
Iter 19/2000 - Loss: -3.894
Iter 20/2000 - Loss: -4.239
Iter 1981/2000 - Loss: -8.968
Iter 1982/2000 - Loss: -8.968
Iter 1983/2000 - Loss: -8.968
Iter 1984/2000 - Loss: -8.968
Iter 1985/2000 - Loss: -8.968
Iter 1986/2000 - Loss: -8.968
Iter 1987/2000 - Loss: -8.968
Iter 1988/2000 - Loss: -8.968
Iter 1989/2000 - Loss: -8.968
Iter 1990/2000 - Loss: -8.968
Iter 1991/2000 - Loss: -8.968
Iter 1992/2000 - Loss: -8.968
Iter 1993/2000 - Loss: -8.968
Iter 1994/2000 - Loss: -8.968
Iter 1995/2000 - Loss: -8.968
Iter 1996/2000 - Loss: -8.968
Iter 1997/2000 - Loss: -8.968
Iter 1998/2000 - Loss: -8.968
Iter 1999/2000 - Loss: -8.968
Iter 2000/2000 - Loss: -8.968
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.5755,  5.3281, 19.8195,  8.0608, 16.5675, 35.3413]],

        [[13.1156, 23.2707, 11.6249,  2.2058,  1.0776, 18.6126]],

        [[13.6592, 23.7281, 17.2390,  0.9529,  1.4447, 17.0021]],

        [[12.8331, 22.3705, 15.2673,  2.9793,  2.0699, 38.9243]]])
Signal Variance: tensor([ 0.0843,  1.0090, 11.7708,  0.3217])
Estimated target variance: tensor([0.0099, 0.0376, 0.5171, 0.0101])
N: 430
Signal to noise ratio: tensor([16.5539, 50.8313, 69.7442, 33.7194])
Bound on condition number: tensor([ 117833.9443, 1111042.2398, 2091631.2804,  488910.5484])
Policy Optimizer learning rate:
0.09567133936275315
Experience 43, Iter 0, disc loss: 3.243062100881816e-05, policy loss: 10.900534431910408
Experience 43, Iter 1, disc loss: 3.6057954199479546e-05, policy loss: 10.73621788243699
Experience 43, Iter 2, disc loss: 3.0498170535114657e-05, policy loss: 11.140103284983773
Experience 43, Iter 3, disc loss: 3.287105480390221e-05, policy loss: 10.897042034807765
Experience 43, Iter 4, disc loss: 3.61095383164093e-05, policy loss: 10.79711319861772
Experience 43, Iter 5, disc loss: 3.0563514350344744e-05, policy loss: 11.004127673874683
Experience 43, Iter 6, disc loss: 4.1171597152394694e-05, policy loss: 10.723032378253802
Experience 43, Iter 7, disc loss: 3.532968905364319e-05, policy loss: 10.830407353419867
Experience 43, Iter 8, disc loss: 3.732336587559524e-05, policy loss: 10.734279218806524
Experience 43, Iter 9, disc loss: 3.548483378046621e-05, policy loss: 10.96263454967838
Experience 43, Iter 10, disc loss: 3.150132876952887e-05, policy loss: 10.816533828314064
Experience 43, Iter 11, disc loss: 3.737441882686722e-05, policy loss: 10.718062468345305
Experience 43, Iter 12, disc loss: 3.385783618409545e-05, policy loss: 10.86398606713409
Experience 43, Iter 13, disc loss: 3.913341987297034e-05, policy loss: 10.775675713990164
Experience 43, Iter 14, disc loss: 3.506490398584155e-05, policy loss: 10.740443569868743
Experience 43, Iter 15, disc loss: 3.656733028336878e-05, policy loss: 10.955264864756105
Experience 43, Iter 16, disc loss: 4.388958398852041e-05, policy loss: 10.823119905097823
Experience 43, Iter 17, disc loss: 3.617636262583258e-05, policy loss: 10.749450283392889
Experience 43, Iter 18, disc loss: 3.875353959102461e-05, policy loss: 10.898098335063182
Experience 43, Iter 19, disc loss: 3.889041013879262e-05, policy loss: 10.843278403175928
Experience 43, Iter 20, disc loss: 3.818619522136679e-05, policy loss: 10.597544163063308
Experience 43, Iter 21, disc loss: 3.322806844480725e-05, policy loss: 10.854251767973514
Experience 43, Iter 22, disc loss: 3.0349116569259044e-05, policy loss: 11.075232037397118
Experience 43, Iter 23, disc loss: 3.403862975075156e-05, policy loss: 10.693966058017203
Experience 43, Iter 24, disc loss: 3.035061775835608e-05, policy loss: 11.068103472462676
Experience 43, Iter 25, disc loss: 3.0970954364585755e-05, policy loss: 10.842445257639852
Experience 43, Iter 26, disc loss: 3.3054388539403266e-05, policy loss: 10.81592331777193
Experience 43, Iter 27, disc loss: 3.0944400556858726e-05, policy loss: 10.799142245860036
Experience 43, Iter 28, disc loss: 2.951363616143889e-05, policy loss: 11.095612411333843
Experience 43, Iter 29, disc loss: 3.646715994528551e-05, policy loss: 10.76687123078353
Experience 43, Iter 30, disc loss: 3.802808152367885e-05, policy loss: 10.700674121586587
Experience 43, Iter 31, disc loss: 4.049616125468061e-05, policy loss: 10.717966225017577
Experience 43, Iter 32, disc loss: 4.1023141821166305e-05, policy loss: 10.575264376447304
Experience 43, Iter 33, disc loss: 3.2797297829153054e-05, policy loss: 11.10739229611469
Experience 43, Iter 34, disc loss: 3.569357923604221e-05, policy loss: 10.898591343494044
Experience 43, Iter 35, disc loss: 3.348152552185286e-05, policy loss: 11.011208100510359
Experience 43, Iter 36, disc loss: 3.217847199105152e-05, policy loss: 10.976168571888227
Experience 43, Iter 37, disc loss: 3.1575640325820126e-05, policy loss: 10.821046845679351
Experience 43, Iter 38, disc loss: 2.7998774468628316e-05, policy loss: 11.024411680572708
Experience 43, Iter 39, disc loss: 3.239500493160492e-05, policy loss: 10.743588718462586
Experience 43, Iter 40, disc loss: 3.5527005139102584e-05, policy loss: 10.70292783940323
Experience 43, Iter 41, disc loss: 3.6404476204701756e-05, policy loss: 10.712817741642926
Experience 43, Iter 42, disc loss: 3.665059024234026e-05, policy loss: 10.725489702954771
Experience 43, Iter 43, disc loss: 3.442681960933727e-05, policy loss: 10.899298848921443
Experience 43, Iter 44, disc loss: 3.6990035333536996e-05, policy loss: 10.622686376267515
Experience 43, Iter 45, disc loss: 3.48941585083658e-05, policy loss: 10.896008580874533
Experience 43, Iter 46, disc loss: 3.358879647455573e-05, policy loss: 10.905731904676507
Experience 43, Iter 47, disc loss: 3.910812542026577e-05, policy loss: 10.780878126643252
Experience 43, Iter 48, disc loss: 3.0423664523098505e-05, policy loss: 11.139352866118468
Experience 43, Iter 49, disc loss: 3.6348789705010516e-05, policy loss: 10.589339713746813
Experience 43, Iter 50, disc loss: 3.0876689354324085e-05, policy loss: 10.884681066353224
Experience 43, Iter 51, disc loss: 2.9192966461066627e-05, policy loss: 10.944635558759753
Experience 43, Iter 52, disc loss: 2.7792104292469152e-05, policy loss: 11.115715473578632
Experience 43, Iter 53, disc loss: 2.7300011781808293e-05, policy loss: 11.014438283371472
Experience 43, Iter 54, disc loss: 3.25379250027424e-05, policy loss: 10.689481874725647
Experience 43, Iter 55, disc loss: 2.8680526055236362e-05, policy loss: 10.89757147805892
Experience 43, Iter 56, disc loss: 3.405441880209456e-05, policy loss: 10.713714280869112
Experience 43, Iter 57, disc loss: 3.323788026249531e-05, policy loss: 10.970682132871234
Experience 43, Iter 58, disc loss: 4.1324164429808534e-05, policy loss: 10.563197754896567
Experience 43, Iter 59, disc loss: 3.65592207913018e-05, policy loss: 10.88500626009952
Experience 43, Iter 60, disc loss: 2.7975840872940826e-05, policy loss: 11.12983836374202
Experience 43, Iter 61, disc loss: 2.8104804288933292e-05, policy loss: 11.132912878863374
Experience 43, Iter 62, disc loss: 3.481946123455192e-05, policy loss: 10.785690199530704
Experience 43, Iter 63, disc loss: 2.7878046528667312e-05, policy loss: 11.090145779312396
Experience 43, Iter 64, disc loss: 2.7937427853169276e-05, policy loss: 11.024557463204578
Experience 43, Iter 65, disc loss: 2.926084637960422e-05, policy loss: 10.861164407534025
Experience 43, Iter 66, disc loss: 2.9034786481310677e-05, policy loss: 10.886587301347483
Experience 43, Iter 67, disc loss: 3.66518454199987e-05, policy loss: 10.813720640521275
Experience 43, Iter 68, disc loss: 3.252855365922512e-05, policy loss: 10.841905473067003
Experience 43, Iter 69, disc loss: 3.0297658847181766e-05, policy loss: 11.002603001967913
Experience 43, Iter 70, disc loss: 3.203200394151601e-05, policy loss: 10.872606336964482
Experience 43, Iter 71, disc loss: 3.5364386520535876e-05, policy loss: 11.231845648984091
Experience 43, Iter 72, disc loss: 3.324281679041816e-05, policy loss: 10.835531533585144
Experience 43, Iter 73, disc loss: 3.1379410438292496e-05, policy loss: 10.93682171358297
Experience 43, Iter 74, disc loss: 3.3018920423290314e-05, policy loss: 10.845950781118173
Experience 43, Iter 75, disc loss: 3.0279217816190912e-05, policy loss: 11.075807095739485
Experience 43, Iter 76, disc loss: 2.9026390677006733e-05, policy loss: 11.023647752839944
Experience 43, Iter 77, disc loss: 3.228040356251214e-05, policy loss: 10.82316816080835
Experience 43, Iter 78, disc loss: 3.138644984224556e-05, policy loss: 10.869810827153849
Experience 43, Iter 79, disc loss: 2.9237156486877586e-05, policy loss: 10.895659808257665
Experience 43, Iter 80, disc loss: 3.9818795955294544e-05, policy loss: 10.632354677878205
Experience 43, Iter 81, disc loss: 3.0846564661931195e-05, policy loss: 10.956992596975493
Experience 43, Iter 82, disc loss: 3.229055919423406e-05, policy loss: 10.9593688707665
Experience 43, Iter 83, disc loss: 2.9554404083138554e-05, policy loss: 11.017298943536344
Experience 43, Iter 84, disc loss: 3.6521473447236776e-05, policy loss: 10.704866354149654
Experience 43, Iter 85, disc loss: 3.353423262946958e-05, policy loss: 10.922132175465181
Experience 43, Iter 86, disc loss: 3.373328823339503e-05, policy loss: 10.884190443782373
Experience 43, Iter 87, disc loss: 3.90136888051634e-05, policy loss: 10.743996888667207
Experience 43, Iter 88, disc loss: 3.2695364228848983e-05, policy loss: 10.970761915874416
Experience 43, Iter 89, disc loss: 3.113778982383159e-05, policy loss: 10.990292720423815
Experience 43, Iter 90, disc loss: 3.2024050752706166e-05, policy loss: 10.809963588539647
Experience 43, Iter 91, disc loss: 3.718627575135348e-05, policy loss: 10.702425012661784
Experience 43, Iter 92, disc loss: 2.5972958710437025e-05, policy loss: 11.216687231372074
Experience 43, Iter 93, disc loss: 3.5760337063982604e-05, policy loss: 10.81085819850125
Experience 43, Iter 94, disc loss: 2.6128721109470772e-05, policy loss: 11.19159296127032
Experience 43, Iter 95, disc loss: 3.09835051660184e-05, policy loss: 10.878855898717509
Experience 43, Iter 96, disc loss: 3.601128697500155e-05, policy loss: 10.886265348400617
Experience 43, Iter 97, disc loss: 3.2789153565524284e-05, policy loss: 10.825643834355676
Experience 43, Iter 98, disc loss: 3.361141182450595e-05, policy loss: 10.800345348413611
Experience 43, Iter 99, disc loss: 3.3452487638315996e-05, policy loss: 10.82504463063453
Experience: 44
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.0092],
        [0.1264],
        [0.0025]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0142, 0.0881, 0.1116, 0.0033, 0.0012, 0.3424]],

        [[0.0142, 0.0881, 0.1116, 0.0033, 0.0012, 0.3424]],

        [[0.0142, 0.0881, 0.1116, 0.0033, 0.0012, 0.3424]],

        [[0.0142, 0.0881, 0.1116, 0.0033, 0.0012, 0.3424]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0097, 0.0368, 0.5057, 0.0099], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0097, 0.0368, 0.5057, 0.0099])
N: 440
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1761.0000, 1761.0000, 1761.0000, 1761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.898
Iter 2/2000 - Loss: -0.388
Iter 3/2000 - Loss: -0.965
Iter 4/2000 - Loss: -0.954
Iter 5/2000 - Loss: -0.795
Iter 6/2000 - Loss: -0.902
Iter 7/2000 - Loss: -1.128
Iter 8/2000 - Loss: -1.304
Iter 9/2000 - Loss: -1.409
Iter 10/2000 - Loss: -1.512
Iter 11/2000 - Loss: -1.663
Iter 12/2000 - Loss: -1.872
Iter 13/2000 - Loss: -2.118
Iter 14/2000 - Loss: -2.384
Iter 15/2000 - Loss: -2.665
Iter 16/2000 - Loss: -2.959
Iter 17/2000 - Loss: -3.270
Iter 18/2000 - Loss: -3.596
Iter 19/2000 - Loss: -3.933
Iter 20/2000 - Loss: -4.277
Iter 1981/2000 - Loss: -8.995
Iter 1982/2000 - Loss: -8.995
Iter 1983/2000 - Loss: -8.995
Iter 1984/2000 - Loss: -8.995
Iter 1985/2000 - Loss: -8.995
Iter 1986/2000 - Loss: -8.995
Iter 1987/2000 - Loss: -8.995
Iter 1988/2000 - Loss: -8.995
Iter 1989/2000 - Loss: -8.995
Iter 1990/2000 - Loss: -8.995
Iter 1991/2000 - Loss: -8.995
Iter 1992/2000 - Loss: -8.995
Iter 1993/2000 - Loss: -8.995
Iter 1994/2000 - Loss: -8.995
Iter 1995/2000 - Loss: -8.995
Iter 1996/2000 - Loss: -8.995
Iter 1997/2000 - Loss: -8.995
Iter 1998/2000 - Loss: -8.995
Iter 1999/2000 - Loss: -8.995
Iter 2000/2000 - Loss: -8.995
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.5099,  5.3529, 19.7206,  7.9787, 16.5327, 35.0875]],

        [[13.0189, 23.0106, 11.5808,  2.1498,  1.0831, 18.5208]],

        [[13.5465, 23.5190, 17.1651,  0.9492,  1.4323, 16.9179]],

        [[12.7392, 22.1143, 15.2094,  2.9488,  2.0496, 38.7634]]])
Signal Variance: tensor([ 0.0841,  0.9905, 11.5841,  0.3185])
Estimated target variance: tensor([0.0097, 0.0368, 0.5057, 0.0099])
N: 440
Signal to noise ratio: tensor([16.5503, 50.5648, 69.5868, 33.8241])
Bound on condition number: tensor([ 120522.1358, 1124990.3769, 2130622.6556,  503391.5977])
Policy Optimizer learning rate:
0.09557059262922256
Experience 44, Iter 0, disc loss: 2.9490134022158064e-05, policy loss: 11.053103041941412
Experience 44, Iter 1, disc loss: 4.267298736448076e-05, policy loss: 10.713539993687997
Experience 44, Iter 2, disc loss: 3.116639103859684e-05, policy loss: 10.93073871350509
Experience 44, Iter 3, disc loss: 2.8275457382138582e-05, policy loss: 10.983949981789857
Experience 44, Iter 4, disc loss: 3.4222929463464844e-05, policy loss: 10.798626284332553
Experience 44, Iter 5, disc loss: 3.0123914701923142e-05, policy loss: 10.792869249277999
Experience 44, Iter 6, disc loss: 3.99201834110926e-05, policy loss: 10.685437033655777
Experience 44, Iter 7, disc loss: 3.2397949723466274e-05, policy loss: 10.875912289843372
Experience 44, Iter 8, disc loss: 3.487838467068886e-05, policy loss: 10.728156014353516
Experience 44, Iter 9, disc loss: 3.013695279470293e-05, policy loss: 11.026800399972757
Experience 44, Iter 10, disc loss: 3.2479100485634027e-05, policy loss: 10.923308798315038
Experience 44, Iter 11, disc loss: 3.843684991134599e-05, policy loss: 10.810461905035377
Experience 44, Iter 12, disc loss: 3.185048873907336e-05, policy loss: 11.017236798185694
Experience 44, Iter 13, disc loss: 3.085679434355917e-05, policy loss: 10.89910610338337
Experience 44, Iter 14, disc loss: 3.139541963155374e-05, policy loss: 11.10465385364948
Experience 44, Iter 15, disc loss: 3.375553162850335e-05, policy loss: 10.979701483150926
Experience 44, Iter 16, disc loss: 3.113010389276111e-05, policy loss: 10.797298636120845
Experience 44, Iter 17, disc loss: 4.6248058479598925e-05, policy loss: 10.56779912416582
Experience 44, Iter 18, disc loss: 3.205698707990374e-05, policy loss: 10.786151008364424
Experience 44, Iter 19, disc loss: 2.7842379183519865e-05, policy loss: 10.988014311169593
Experience 44, Iter 20, disc loss: 2.937393829078094e-05, policy loss: 10.901097448511472
Experience 44, Iter 21, disc loss: 4.10415647531745e-05, policy loss: 10.817428689503886
Experience 44, Iter 22, disc loss: 2.946229431641756e-05, policy loss: 10.950787001561668
Experience 44, Iter 23, disc loss: 3.08402713310207e-05, policy loss: 11.06430463672266
Experience 44, Iter 24, disc loss: 3.4398492018120316e-05, policy loss: 10.989630942117339
Experience 44, Iter 25, disc loss: 3.7112383504143216e-05, policy loss: 10.905283261693539
Experience 44, Iter 26, disc loss: 2.9055129900868828e-05, policy loss: 10.980251408585518
Experience 44, Iter 27, disc loss: 2.6718344604092383e-05, policy loss: 11.075484577492464
Experience 44, Iter 28, disc loss: 2.7046253424811357e-05, policy loss: 11.038002769495138
Experience 44, Iter 29, disc loss: 3.122200029276982e-05, policy loss: 10.91761822452934
Experience 44, Iter 30, disc loss: 3.622488743418813e-05, policy loss: 10.811545654532278
Experience 44, Iter 31, disc loss: 3.092327299005663e-05, policy loss: 10.926408279717409
Experience 44, Iter 32, disc loss: 3.430878361890016e-05, policy loss: 10.850583730515837
Experience 44, Iter 33, disc loss: 2.5227976379782744e-05, policy loss: 11.235791867781746
Experience 44, Iter 34, disc loss: 3.0056979030718595e-05, policy loss: 11.07960914752185
Experience 44, Iter 35, disc loss: 3.288708390447991e-05, policy loss: 10.856152380972786
Experience 44, Iter 36, disc loss: 2.766092494342547e-05, policy loss: 11.137330836917792
Experience 44, Iter 37, disc loss: 3.0808083779415026e-05, policy loss: 10.854641897700024
Experience 44, Iter 38, disc loss: 3.141913995043205e-05, policy loss: 10.836170413233113
Experience 44, Iter 39, disc loss: 2.7548751225895856e-05, policy loss: 10.976997949851473
Experience 44, Iter 40, disc loss: 4.1175659066448726e-05, policy loss: 10.747226426301143
Experience 44, Iter 41, disc loss: 3.457276547770934e-05, policy loss: 10.782348063769273
Experience 44, Iter 42, disc loss: 3.286538039265572e-05, policy loss: 10.892967562940502
Experience 44, Iter 43, disc loss: 4.046723269560726e-05, policy loss: 10.55825866674127
Experience 44, Iter 44, disc loss: 3.118453696612327e-05, policy loss: 11.01865129178259
Experience 44, Iter 45, disc loss: 4.0693825810665725e-05, policy loss: 10.738997729213253
Experience 44, Iter 46, disc loss: 3.05852464512319e-05, policy loss: 10.860541167692212
Experience 44, Iter 47, disc loss: 3.068981163540309e-05, policy loss: 10.924254726942177
Experience 44, Iter 48, disc loss: 3.3313054135300644e-05, policy loss: 10.940920772154646
Experience 44, Iter 49, disc loss: 2.961601075826192e-05, policy loss: 11.100929216751194
Experience 44, Iter 50, disc loss: 3.0001203069091164e-05, policy loss: 10.891534144823856
Experience 44, Iter 51, disc loss: 3.421554047057221e-05, policy loss: 10.795029849801452
Experience 44, Iter 52, disc loss: 3.281264439687554e-05, policy loss: 10.856323452307526
Experience 44, Iter 53, disc loss: 3.2926404844788284e-05, policy loss: 10.918802406148028
Experience 44, Iter 54, disc loss: 3.204511917252357e-05, policy loss: 10.934558343930032
Experience 44, Iter 55, disc loss: 3.117066290034158e-05, policy loss: 10.9603540994547
Experience 44, Iter 56, disc loss: 3.091749144382497e-05, policy loss: 10.879161898021543
Experience 44, Iter 57, disc loss: 3.2563485921928576e-05, policy loss: 10.939676854738828
Experience 44, Iter 58, disc loss: 3.183791769269469e-05, policy loss: 10.916555400974477
Experience 44, Iter 59, disc loss: 3.611468958483566e-05, policy loss: 11.030035149407755
Experience 44, Iter 60, disc loss: 3.226658646890673e-05, policy loss: 10.796299363273535
Experience 44, Iter 61, disc loss: 3.417423028433316e-05, policy loss: 10.756610825322593
Experience 44, Iter 62, disc loss: 2.6780343161401148e-05, policy loss: 11.105229025031775
Experience 44, Iter 63, disc loss: 2.76520895982342e-05, policy loss: 11.032101981257942
Experience 44, Iter 64, disc loss: 4.094155498744756e-05, policy loss: 10.656619405530018
Experience 44, Iter 65, disc loss: 2.5704382130034677e-05, policy loss: 11.231012608534463
Experience 44, Iter 66, disc loss: 2.5889963234053973e-05, policy loss: 11.1753549066513
Experience 44, Iter 67, disc loss: 2.9920289587700253e-05, policy loss: 10.896073518591276
Experience 44, Iter 68, disc loss: 3.218120381441343e-05, policy loss: 10.853114798343924
Experience 44, Iter 69, disc loss: 2.8122058090478157e-05, policy loss: 11.054871652101856
Experience 44, Iter 70, disc loss: 3.693162718736749e-05, policy loss: 10.6624627769665
Experience 44, Iter 71, disc loss: 2.7976490986522558e-05, policy loss: 11.298418305758712
Experience 44, Iter 72, disc loss: 3.436996015360419e-05, policy loss: 10.929599936882628
Experience 44, Iter 73, disc loss: 3.042987006991305e-05, policy loss: 10.976988010392525
Experience 44, Iter 74, disc loss: 3.6624321694046605e-05, policy loss: 10.795760274445884
Experience 44, Iter 75, disc loss: 2.914626438942008e-05, policy loss: 11.064835240804326
Experience 44, Iter 76, disc loss: 2.8497633744118925e-05, policy loss: 11.105503359117547
Experience 44, Iter 77, disc loss: 2.8490969101923946e-05, policy loss: 11.099388889542379
Experience 44, Iter 78, disc loss: 2.8618484141157776e-05, policy loss: 10.924760996584068
Experience 44, Iter 79, disc loss: 3.116732274992809e-05, policy loss: 10.91755363008016
Experience 44, Iter 80, disc loss: 2.7232408970129856e-05, policy loss: 11.017790419279095
Experience 44, Iter 81, disc loss: 2.8067778533579714e-05, policy loss: 10.918107909913655
Experience 44, Iter 82, disc loss: 2.493678501412141e-05, policy loss: 11.29398358238551
Experience 44, Iter 83, disc loss: 3.3842798243661016e-05, policy loss: 10.807395906662938
Experience 44, Iter 84, disc loss: 2.8625414372552853e-05, policy loss: 10.99062738571276
Experience 44, Iter 85, disc loss: 2.884934147218663e-05, policy loss: 11.125542916560052
Experience 44, Iter 86, disc loss: 3.559007933657486e-05, policy loss: 11.063040932229743
Experience 44, Iter 87, disc loss: 2.8048549684665176e-05, policy loss: 11.067271297769127
Experience 44, Iter 88, disc loss: 3.2283011767556096e-05, policy loss: 10.980443480167892
Experience 44, Iter 89, disc loss: 3.0465548850477955e-05, policy loss: 10.878953720444695
Experience 44, Iter 90, disc loss: 2.578358741784075e-05, policy loss: 11.051562243710714
Experience 44, Iter 91, disc loss: 2.9638034079021102e-05, policy loss: 10.867413550611076
Experience 44, Iter 92, disc loss: 3.077618079590847e-05, policy loss: 10.93772133971401
Experience 44, Iter 93, disc loss: 3.103632806419977e-05, policy loss: 10.819675276597804
Experience 44, Iter 94, disc loss: 3.269557755325427e-05, policy loss: 10.839874878342883
Experience 44, Iter 95, disc loss: 3.198413259026224e-05, policy loss: 10.940835284680144
Experience 44, Iter 96, disc loss: 3.119339570335487e-05, policy loss: 11.131311902408456
Experience 44, Iter 97, disc loss: 3.86812800642109e-05, policy loss: 10.7191639783829
Experience 44, Iter 98, disc loss: 2.907447790034573e-05, policy loss: 11.054271513282721
Experience 44, Iter 99, disc loss: 3.254093512885418e-05, policy loss: 10.893808334902385
Experience: 45
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.0090],
        [0.1261],
        [0.0025]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0139, 0.0862, 0.1113, 0.0033, 0.0012, 0.3351]],

        [[0.0139, 0.0862, 0.1113, 0.0033, 0.0012, 0.3351]],

        [[0.0139, 0.0862, 0.1113, 0.0033, 0.0012, 0.3351]],

        [[0.0139, 0.0862, 0.1113, 0.0033, 0.0012, 0.3351]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0095, 0.0362, 0.5045, 0.0099], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0095, 0.0362, 0.5045, 0.0099])
N: 450
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1801.0000, 1801.0000, 1801.0000, 1801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.916
Iter 2/2000 - Loss: -0.389
Iter 3/2000 - Loss: -0.979
Iter 4/2000 - Loss: -0.964
Iter 5/2000 - Loss: -0.797
Iter 6/2000 - Loss: -0.903
Iter 7/2000 - Loss: -1.130
Iter 8/2000 - Loss: -1.302
Iter 9/2000 - Loss: -1.399
Iter 10/2000 - Loss: -1.493
Iter 11/2000 - Loss: -1.638
Iter 12/2000 - Loss: -1.841
Iter 13/2000 - Loss: -2.082
Iter 14/2000 - Loss: -2.341
Iter 15/2000 - Loss: -2.614
Iter 16/2000 - Loss: -2.902
Iter 17/2000 - Loss: -3.207
Iter 18/2000 - Loss: -3.530
Iter 19/2000 - Loss: -3.866
Iter 20/2000 - Loss: -4.211
Iter 1981/2000 - Loss: -9.012
Iter 1982/2000 - Loss: -9.012
Iter 1983/2000 - Loss: -9.012
Iter 1984/2000 - Loss: -9.012
Iter 1985/2000 - Loss: -9.012
Iter 1986/2000 - Loss: -9.012
Iter 1987/2000 - Loss: -9.012
Iter 1988/2000 - Loss: -9.012
Iter 1989/2000 - Loss: -9.012
Iter 1990/2000 - Loss: -9.012
Iter 1991/2000 - Loss: -9.012
Iter 1992/2000 - Loss: -9.012
Iter 1993/2000 - Loss: -9.012
Iter 1994/2000 - Loss: -9.012
Iter 1995/2000 - Loss: -9.012
Iter 1996/2000 - Loss: -9.012
Iter 1997/2000 - Loss: -9.012
Iter 1998/2000 - Loss: -9.012
Iter 1999/2000 - Loss: -9.012
Iter 2000/2000 - Loss: -9.012
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.4303,  5.4529, 19.7822,  7.8738, 16.0437, 34.6919]],

        [[12.8659, 22.8642, 11.5033,  2.1941,  1.1008, 18.8158]],

        [[13.4036, 23.3425, 17.0654,  0.9378,  1.4497, 16.6279]],

        [[12.5886, 22.2019, 15.0618,  3.1027,  2.0169, 38.5823]]])
Signal Variance: tensor([ 0.0848,  1.0159, 11.3143,  0.3139])
Estimated target variance: tensor([0.0095, 0.0362, 0.5045, 0.0099])
N: 450
Signal to noise ratio: tensor([16.6258, 51.4623, 68.9423, 33.6418])
Bound on condition number: tensor([ 124388.9836, 1191768.4209, 2138867.6698,  509299.1783])
Policy Optimizer learning rate:
0.0954699519870709
Experience 45, Iter 0, disc loss: 3.4719408842477374e-05, policy loss: 10.856321271275391
Experience 45, Iter 1, disc loss: 2.9254350156162598e-05, policy loss: 10.92601132382487
Experience 45, Iter 2, disc loss: 3.7673866525764754e-05, policy loss: 10.850999674541082
Experience 45, Iter 3, disc loss: 3.0781910995605235e-05, policy loss: 10.963651071302232
Experience 45, Iter 4, disc loss: 2.6997291211155856e-05, policy loss: 11.214618163760353
Experience 45, Iter 5, disc loss: 2.6238187210763847e-05, policy loss: 11.074916779779299
Experience 45, Iter 6, disc loss: 2.6375219400352914e-05, policy loss: 11.046617473377752
Experience 45, Iter 7, disc loss: 3.1061020455953425e-05, policy loss: 10.735906615588748
Experience 45, Iter 8, disc loss: 2.793597051227712e-05, policy loss: 10.96770374164921
Experience 45, Iter 9, disc loss: 3.0242845633056722e-05, policy loss: 10.951772354177539
Experience 45, Iter 10, disc loss: 2.88630496790799e-05, policy loss: 11.046928225410051
Experience 45, Iter 11, disc loss: 3.243484676886264e-05, policy loss: 10.848005184306963
Experience 45, Iter 12, disc loss: 3.0407818639152197e-05, policy loss: 11.04381214526878
Experience 45, Iter 13, disc loss: 3.428382728744491e-05, policy loss: 11.014671149896467
Experience 45, Iter 14, disc loss: 3.3743084371769755e-05, policy loss: 10.894454439349808
Experience 45, Iter 15, disc loss: 2.433197298026057e-05, policy loss: 11.166894676826047
Experience 45, Iter 16, disc loss: 3.219172541516439e-05, policy loss: 10.900308804812969
Experience 45, Iter 17, disc loss: 2.868108382508106e-05, policy loss: 11.013702338496127
Experience 45, Iter 18, disc loss: 2.7818337844310992e-05, policy loss: 10.955232654305096
Experience 45, Iter 19, disc loss: 2.8874807975051626e-05, policy loss: 10.885845187922468
Experience 45, Iter 20, disc loss: 2.6909919364625556e-05, policy loss: 11.130062151416219
Experience 45, Iter 21, disc loss: 2.529267207823075e-05, policy loss: 11.141707418716404
Experience 45, Iter 22, disc loss: 2.7932309559075265e-05, policy loss: 10.950101487032956
Experience 45, Iter 23, disc loss: 3.342325947877918e-05, policy loss: 10.811672438006731
Experience 45, Iter 24, disc loss: 2.9953252031398958e-05, policy loss: 10.946056409802534
Experience 45, Iter 25, disc loss: 2.929674931413974e-05, policy loss: 10.92545516036753
Experience 45, Iter 26, disc loss: 3.762751891835198e-05, policy loss: 10.70114962133215
Experience 45, Iter 27, disc loss: 3.0295335214704977e-05, policy loss: 10.865488364059669
Experience 45, Iter 28, disc loss: 2.838788061112633e-05, policy loss: 11.200987987747723
Experience 45, Iter 29, disc loss: 3.076702773846929e-05, policy loss: 10.85075606848569
Experience 45, Iter 30, disc loss: 2.877286472954742e-05, policy loss: 10.966860335295676
Experience 45, Iter 31, disc loss: 2.6091616127353108e-05, policy loss: 11.16385759605517
Experience 45, Iter 32, disc loss: 2.8669690324059756e-05, policy loss: 10.93368455761745
Experience 45, Iter 33, disc loss: 2.810946431565314e-05, policy loss: 11.031776210652225
Experience 45, Iter 34, disc loss: 2.5752459776577915e-05, policy loss: 11.234044629751807
Experience 45, Iter 35, disc loss: 3.166071971877263e-05, policy loss: 10.860803734393095
Experience 45, Iter 36, disc loss: 2.6085433087094167e-05, policy loss: 11.012113060780411
Experience 45, Iter 37, disc loss: 2.6859458763646593e-05, policy loss: 11.055537125101853
Experience 45, Iter 38, disc loss: 2.8996383105895096e-05, policy loss: 11.088219113898361
Experience 45, Iter 39, disc loss: 2.526197721433231e-05, policy loss: 11.163222346292025
Experience 45, Iter 40, disc loss: 2.5280283356276545e-05, policy loss: 11.144367669934685
Experience 45, Iter 41, disc loss: 3.154900371879796e-05, policy loss: 10.888303038371193
Experience 45, Iter 42, disc loss: 2.8606625061038512e-05, policy loss: 11.020671425293969
Experience 45, Iter 43, disc loss: 2.396556070982523e-05, policy loss: 11.226231282735142
Experience 45, Iter 44, disc loss: 2.924549433561797e-05, policy loss: 11.0327168174235
Experience 45, Iter 45, disc loss: 3.050946531541186e-05, policy loss: 11.04702815998774
Experience 45, Iter 46, disc loss: 2.920689870337158e-05, policy loss: 11.004137323610989
Experience 45, Iter 47, disc loss: 2.750033942598152e-05, policy loss: 11.004104490778225
Experience 45, Iter 48, disc loss: 2.8264810798987045e-05, policy loss: 11.047038378096184
Experience 45, Iter 49, disc loss: 2.971662183404425e-05, policy loss: 10.938664752129547
Experience 45, Iter 50, disc loss: 3.718652853928334e-05, policy loss: 10.657520768557836
Experience 45, Iter 51, disc loss: 2.910207149002935e-05, policy loss: 11.155447953575116
Experience 45, Iter 52, disc loss: 3.210601866213645e-05, policy loss: 10.878008077346664
Experience 45, Iter 53, disc loss: 3.350569562022429e-05, policy loss: 10.784110084087317
Experience 45, Iter 54, disc loss: 3.717243767745667e-05, policy loss: 10.922639082937058
Experience 45, Iter 55, disc loss: 2.98321870822223e-05, policy loss: 11.115427845923577
Experience 45, Iter 56, disc loss: 3.0756426989791243e-05, policy loss: 10.82957341907537
Experience 45, Iter 57, disc loss: 2.981697774130668e-05, policy loss: 11.021921832213135
Experience 45, Iter 58, disc loss: 3.551669379885394e-05, policy loss: 11.047516861695016
Experience 45, Iter 59, disc loss: 3.1762644524264314e-05, policy loss: 11.102726460909679
Experience 45, Iter 60, disc loss: 2.8728428412465686e-05, policy loss: 10.930676938487084
Experience 45, Iter 61, disc loss: 2.9621928438861646e-05, policy loss: 10.975644410335505
Experience 45, Iter 62, disc loss: 2.602146918847605e-05, policy loss: 11.125837935028237
Experience 45, Iter 63, disc loss: 3.0325755262130043e-05, policy loss: 10.95243137191256
Experience 45, Iter 64, disc loss: 2.7053142510554268e-05, policy loss: 11.019608004350935
Experience 45, Iter 65, disc loss: 2.589528044201978e-05, policy loss: 11.004445051172288
Experience 45, Iter 66, disc loss: 2.6151317379995255e-05, policy loss: 11.044789425106575
Experience 45, Iter 67, disc loss: 3.0026843351966936e-05, policy loss: 10.941037927522022
Experience 45, Iter 68, disc loss: 2.936770062381891e-05, policy loss: 10.943719623430516
Experience 45, Iter 69, disc loss: 3.284747518913781e-05, policy loss: 10.77965041230528
Experience 45, Iter 70, disc loss: 3.0435598437175908e-05, policy loss: 10.866817709306677
Experience 45, Iter 71, disc loss: 3.5080204821620056e-05, policy loss: 10.728738294901468
Experience 45, Iter 72, disc loss: 3.266319068121075e-05, policy loss: 10.92689960970354
Experience 45, Iter 73, disc loss: 3.4519491526222196e-05, policy loss: 10.969545938760998
Experience 45, Iter 74, disc loss: 2.8125678276990972e-05, policy loss: 11.045290643559213
Experience 45, Iter 75, disc loss: 3.966337166234236e-05, policy loss: 11.07195931420235
Experience 45, Iter 76, disc loss: 3.5528150236631515e-05, policy loss: 10.840755508686268
Experience 45, Iter 77, disc loss: 2.4157851742275877e-05, policy loss: 11.142380930873145
Experience 45, Iter 78, disc loss: 2.7383671463102403e-05, policy loss: 11.038436769261663
Experience 45, Iter 79, disc loss: 2.6581229845838778e-05, policy loss: 11.10210355239672
Experience 45, Iter 80, disc loss: 2.9064546552634776e-05, policy loss: 11.061625163464978
Experience 45, Iter 81, disc loss: 2.6665387634823426e-05, policy loss: 11.095732573802952
Experience 45, Iter 82, disc loss: 2.9725637984282192e-05, policy loss: 11.01012763924741
Experience 45, Iter 83, disc loss: 2.8775623581368625e-05, policy loss: 11.06219257799168
Experience 45, Iter 84, disc loss: 3.2004131709342957e-05, policy loss: 10.9514252032928
Experience 45, Iter 85, disc loss: 2.7419208368660223e-05, policy loss: 11.031268694641533
Experience 45, Iter 86, disc loss: 2.8816365873307142e-05, policy loss: 10.987501811130073
Experience 45, Iter 87, disc loss: 3.191015651226779e-05, policy loss: 10.998818047547523
Experience 45, Iter 88, disc loss: 2.803156863427188e-05, policy loss: 11.072504504723383
Experience 45, Iter 89, disc loss: 2.6813099809903583e-05, policy loss: 11.102520611223511
Experience 45, Iter 90, disc loss: 2.895089715422076e-05, policy loss: 11.02258450348284
Experience 45, Iter 91, disc loss: 2.70581941774964e-05, policy loss: 10.993756484272648
Experience 45, Iter 92, disc loss: 2.445997011392748e-05, policy loss: 11.142827675167492
Experience 45, Iter 93, disc loss: 2.3305629781105448e-05, policy loss: 11.274361594290504
Experience 45, Iter 94, disc loss: 3.1611146808100466e-05, policy loss: 10.907522350933483
Experience 45, Iter 95, disc loss: 2.405488183554133e-05, policy loss: 11.128853616954796
Experience 45, Iter 96, disc loss: 2.8883490648869923e-05, policy loss: 10.999068459734321
Experience 45, Iter 97, disc loss: 2.7240617284896348e-05, policy loss: 11.084661059049719
Experience 45, Iter 98, disc loss: 2.8114700990762597e-05, policy loss: 10.890787336950686
Experience 45, Iter 99, disc loss: 3.075567917556274e-05, policy loss: 11.136172955031151
Experience: 46
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0089],
        [0.1236],
        [0.0024]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0136, 0.0843, 0.1090, 0.0032, 0.0012, 0.3281]],

        [[0.0136, 0.0843, 0.1090, 0.0032, 0.0012, 0.3281]],

        [[0.0136, 0.0843, 0.1090, 0.0032, 0.0012, 0.3281]],

        [[0.0136, 0.0843, 0.1090, 0.0032, 0.0012, 0.3281]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0093, 0.0354, 0.4944, 0.0097], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0093, 0.0354, 0.4944, 0.0097])
N: 460
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1841.0000, 1841.0000, 1841.0000, 1841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.955
Iter 2/2000 - Loss: -0.405
Iter 3/2000 - Loss: -1.015
Iter 4/2000 - Loss: -0.995
Iter 5/2000 - Loss: -0.818
Iter 6/2000 - Loss: -0.925
Iter 7/2000 - Loss: -1.154
Iter 8/2000 - Loss: -1.323
Iter 9/2000 - Loss: -1.417
Iter 10/2000 - Loss: -1.505
Iter 11/2000 - Loss: -1.646
Iter 12/2000 - Loss: -1.846
Iter 13/2000 - Loss: -2.085
Iter 14/2000 - Loss: -2.343
Iter 15/2000 - Loss: -2.615
Iter 16/2000 - Loss: -2.902
Iter 17/2000 - Loss: -3.206
Iter 18/2000 - Loss: -3.529
Iter 19/2000 - Loss: -3.866
Iter 20/2000 - Loss: -4.211
Iter 1981/2000 - Loss: -9.023
Iter 1982/2000 - Loss: -9.023
Iter 1983/2000 - Loss: -9.023
Iter 1984/2000 - Loss: -9.024
Iter 1985/2000 - Loss: -9.024
Iter 1986/2000 - Loss: -9.024
Iter 1987/2000 - Loss: -9.024
Iter 1988/2000 - Loss: -9.024
Iter 1989/2000 - Loss: -9.024
Iter 1990/2000 - Loss: -9.024
Iter 1991/2000 - Loss: -9.024
Iter 1992/2000 - Loss: -9.024
Iter 1993/2000 - Loss: -9.024
Iter 1994/2000 - Loss: -9.024
Iter 1995/2000 - Loss: -9.024
Iter 1996/2000 - Loss: -9.024
Iter 1997/2000 - Loss: -9.024
Iter 1998/2000 - Loss: -9.024
Iter 1999/2000 - Loss: -9.024
Iter 2000/2000 - Loss: -9.024
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.3789,  5.4129, 19.6781,  7.8335, 16.0661, 34.4751]],

        [[12.7978, 22.7524, 11.5443,  2.1797,  1.0991, 18.8577]],

        [[13.3269, 23.3210, 16.9463,  0.9407,  1.4577, 16.5536]],

        [[12.5090, 22.1524, 15.0678,  3.0954,  2.0228, 38.4340]]])
Signal Variance: tensor([ 0.0840,  1.0162, 11.0848,  0.3134])
Estimated target variance: tensor([0.0093, 0.0354, 0.4944, 0.0097])
N: 460
Signal to noise ratio: tensor([16.6004, 51.5704, 68.0396, 33.6425])
Bound on condition number: tensor([ 126764.2558, 1223374.0089, 2129516.9149,  520637.5807])
Policy Optimizer learning rate:
0.0953694173245786
Experience 46, Iter 0, disc loss: 2.74946847863226e-05, policy loss: 11.21976835842743
Experience 46, Iter 1, disc loss: 2.9943625214965694e-05, policy loss: 10.93445480313655
Experience 46, Iter 2, disc loss: 2.8981774506319524e-05, policy loss: 11.019378738377428
Experience 46, Iter 3, disc loss: 2.5517353134455495e-05, policy loss: 11.212741527560475
Experience 46, Iter 4, disc loss: 2.6183155672004177e-05, policy loss: 10.994207930215428
Experience 46, Iter 5, disc loss: 2.5698991385519315e-05, policy loss: 11.29763986491241
Experience 46, Iter 6, disc loss: 2.859295058501009e-05, policy loss: 10.993099336390184
Experience 46, Iter 7, disc loss: 2.8523120575087517e-05, policy loss: 11.039672149299022
Experience 46, Iter 8, disc loss: 2.516023382845988e-05, policy loss: 11.209024617052696
Experience 46, Iter 9, disc loss: 3.170976179968467e-05, policy loss: 10.818053284600566
Experience 46, Iter 10, disc loss: 2.752998145140823e-05, policy loss: 10.974432372810734
Experience 46, Iter 11, disc loss: 3.2037324207456236e-05, policy loss: 11.054248509989089
Experience 46, Iter 12, disc loss: 2.478396464346938e-05, policy loss: 11.119729209443763
Experience 46, Iter 13, disc loss: 2.6357501831600226e-05, policy loss: 11.216093153955297
Experience 46, Iter 14, disc loss: 2.6628500414354516e-05, policy loss: 11.129409466927438
Experience 46, Iter 15, disc loss: 2.7355622044960844e-05, policy loss: 11.148053985939882
Experience 46, Iter 16, disc loss: 2.6263349706767436e-05, policy loss: 11.151088882232177
Experience 46, Iter 17, disc loss: 2.6203202741282695e-05, policy loss: 11.024189337147519
Experience 46, Iter 18, disc loss: 2.984252319123596e-05, policy loss: 11.024630219390874
Experience 46, Iter 19, disc loss: 2.6415238883774175e-05, policy loss: 10.920393417041147
Experience 46, Iter 20, disc loss: 3.128937094151545e-05, policy loss: 11.181226507392548
Experience 46, Iter 21, disc loss: 2.9189516592329923e-05, policy loss: 11.106792003075633
Experience 46, Iter 22, disc loss: 2.975809542464108e-05, policy loss: 11.034313300563515
Experience 46, Iter 23, disc loss: 2.9199271168314504e-05, policy loss: 10.999276672026266
Experience 46, Iter 24, disc loss: 2.7626820534680298e-05, policy loss: 11.174435034746303
Experience 46, Iter 25, disc loss: 2.6706519253986235e-05, policy loss: 11.235557365052612
Experience 46, Iter 26, disc loss: 2.7002195185965355e-05, policy loss: 11.099095875703291
Experience 46, Iter 27, disc loss: 2.322666664236628e-05, policy loss: 11.360861751046976
Experience 46, Iter 28, disc loss: 2.55212867882865e-05, policy loss: 11.102264774790406
Experience 46, Iter 29, disc loss: 2.3994316152075494e-05, policy loss: 11.127361298169255
Experience 46, Iter 30, disc loss: 2.4709441585916366e-05, policy loss: 10.999355684326517
Experience 46, Iter 31, disc loss: 2.7666030048487067e-05, policy loss: 10.935195141494399
Experience 46, Iter 32, disc loss: 3.367091999484063e-05, policy loss: 10.812957988034558
Experience 46, Iter 33, disc loss: 3.333718077939236e-05, policy loss: 10.902728129796603
Experience 46, Iter 34, disc loss: 2.74412538105808e-05, policy loss: 11.184619742406742
Experience 46, Iter 35, disc loss: 2.7582741273319963e-05, policy loss: 11.23723157247227
Experience 46, Iter 36, disc loss: 3.090084461244214e-05, policy loss: 10.923562219408083
Experience 46, Iter 37, disc loss: 2.8823548378843093e-05, policy loss: 11.11895965180797
Experience 46, Iter 38, disc loss: 2.5967988543838415e-05, policy loss: 11.164897081705773
Experience 46, Iter 39, disc loss: 2.8019540246918986e-05, policy loss: 11.125752406957766
Experience 46, Iter 40, disc loss: 2.7902513191246702e-05, policy loss: 10.964948118083564
Experience 46, Iter 41, disc loss: 2.2068891644996538e-05, policy loss: 11.159402075885861
Experience 46, Iter 42, disc loss: 2.44515441319399e-05, policy loss: 11.017761202566586
Experience 46, Iter 43, disc loss: 2.2256772665236153e-05, policy loss: 11.151264577636383
Experience 46, Iter 44, disc loss: 2.801845115089637e-05, policy loss: 11.135573091889206
Experience 46, Iter 45, disc loss: 3.1106895927076014e-05, policy loss: 10.881096037471004
Experience 46, Iter 46, disc loss: 3.232929069023688e-05, policy loss: 11.203880479009957
Experience 46, Iter 47, disc loss: 2.5448800856523775e-05, policy loss: 11.165036560470977
Experience 46, Iter 48, disc loss: 2.3125598535970095e-05, policy loss: 11.322500051461997
Experience 46, Iter 49, disc loss: 2.4425370664877837e-05, policy loss: 11.328302426157471
Experience 46, Iter 50, disc loss: 2.4258888062489378e-05, policy loss: 11.195308961418041
Experience 46, Iter 51, disc loss: 2.6464881030309457e-05, policy loss: 10.991508393667083
Experience 46, Iter 52, disc loss: 2.3249949634461366e-05, policy loss: 11.190632391185012
Experience 46, Iter 53, disc loss: 2.4991043057067496e-05, policy loss: 11.044159343044218
Experience 46, Iter 54, disc loss: 2.558433257418675e-05, policy loss: 11.035992770499657
Experience 46, Iter 55, disc loss: 2.6216375889593883e-05, policy loss: 11.15770774160465
Experience 46, Iter 56, disc loss: 2.593797684296834e-05, policy loss: 11.10199019308369
Experience 46, Iter 57, disc loss: 2.3901055748311526e-05, policy loss: 11.31066564938579
Experience 46, Iter 58, disc loss: 3.129136023270754e-05, policy loss: 10.825255656825831
Experience 46, Iter 59, disc loss: 2.7617036677674783e-05, policy loss: 11.107083339281374
Experience 46, Iter 60, disc loss: 2.75286693230701e-05, policy loss: 11.016118820887666
Experience 46, Iter 61, disc loss: 2.91688390553442e-05, policy loss: 11.15224935962206
Experience 46, Iter 62, disc loss: 2.6075983806846143e-05, policy loss: 11.15359090082804
Experience 46, Iter 63, disc loss: 2.7075559944692394e-05, policy loss: 10.934732170046114
Experience 46, Iter 64, disc loss: 2.8276972973526375e-05, policy loss: 11.028863864706254
Experience 46, Iter 65, disc loss: 2.5089220428695777e-05, policy loss: 11.094392625518278
Experience 46, Iter 66, disc loss: 2.4443913526945184e-05, policy loss: 11.110453595276095
Experience 46, Iter 67, disc loss: 2.386852317259503e-05, policy loss: 11.262121047708543
Experience 46, Iter 68, disc loss: 2.6635532691717496e-05, policy loss: 11.264884660667498
Experience 46, Iter 69, disc loss: 2.9997497213380875e-05, policy loss: 10.719947304439923
Experience 46, Iter 70, disc loss: 2.895694896876758e-05, policy loss: 11.073583415484482
Experience 46, Iter 71, disc loss: 2.8248537325007294e-05, policy loss: 10.947104413405963
Experience 46, Iter 72, disc loss: 2.9615759330303574e-05, policy loss: 10.830654554824255
Experience 46, Iter 73, disc loss: 2.8931311716635758e-05, policy loss: 10.902144426717555
Experience 46, Iter 74, disc loss: 2.739945988599747e-05, policy loss: 11.120505976970186
Experience 46, Iter 75, disc loss: 2.546236068820378e-05, policy loss: 11.242646955627698
Experience 46, Iter 76, disc loss: 2.7951957248154712e-05, policy loss: 11.119143815178525
Experience 46, Iter 77, disc loss: 2.5014981543781262e-05, policy loss: 11.216210753697432
Experience 46, Iter 78, disc loss: 2.282376046036856e-05, policy loss: 11.16928944356819
Experience 46, Iter 79, disc loss: 2.275477207768638e-05, policy loss: 11.17312569679346
Experience 46, Iter 80, disc loss: 2.3054587976105236e-05, policy loss: 11.24248455170995
Experience 46, Iter 81, disc loss: 2.700171164547485e-05, policy loss: 11.010584914096395
Experience 46, Iter 82, disc loss: 2.773013915467086e-05, policy loss: 11.065094340141533
Experience 46, Iter 83, disc loss: 2.8782897657215486e-05, policy loss: 11.065307811378759
Experience 46, Iter 84, disc loss: 2.4644043480935408e-05, policy loss: 11.097894069187346
Experience 46, Iter 85, disc loss: 2.7897405904286945e-05, policy loss: 11.031687850418828
Experience 46, Iter 86, disc loss: 3.144824482638635e-05, policy loss: 11.047896638160413
Experience 46, Iter 87, disc loss: 2.9254117562996835e-05, policy loss: 10.938207055164838
Experience 46, Iter 88, disc loss: 2.6370328942867092e-05, policy loss: 11.096984441861014
Experience 46, Iter 89, disc loss: 2.631766193231958e-05, policy loss: 11.206491694188426
Experience 46, Iter 90, disc loss: 2.7428920849733118e-05, policy loss: 11.172623207691519
Experience 46, Iter 91, disc loss: 2.3036621876183247e-05, policy loss: 11.217304992809968
Experience 46, Iter 92, disc loss: 2.316785311848282e-05, policy loss: 11.13812711804197
Experience 46, Iter 93, disc loss: 2.280666459189478e-05, policy loss: 11.397940453386237
Experience 46, Iter 94, disc loss: 2.662804194408621e-05, policy loss: 11.1719704539267
Experience 46, Iter 95, disc loss: 2.74231021210925e-05, policy loss: 10.973441843229962
Experience 46, Iter 96, disc loss: 3.177480936485363e-05, policy loss: 10.98282370487726
Experience 46, Iter 97, disc loss: 2.215935543859785e-05, policy loss: 11.41113017132504
Experience 46, Iter 98, disc loss: 2.5326194522549735e-05, policy loss: 11.147695520891006
Experience 46, Iter 99, disc loss: 2.6389981804942387e-05, policy loss: 11.006588075245507
Experience: 47
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0087],
        [0.1210],
        [0.0024]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0134, 0.0826, 0.1067, 0.0032, 0.0012, 0.3213]],

        [[0.0134, 0.0826, 0.1067, 0.0032, 0.0012, 0.3213]],

        [[0.0134, 0.0826, 0.1067, 0.0032, 0.0012, 0.3213]],

        [[0.0134, 0.0826, 0.1067, 0.0032, 0.0012, 0.3213]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0091, 0.0347, 0.4839, 0.0095], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0091, 0.0347, 0.4839, 0.0095])
N: 470
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1881.0000, 1881.0000, 1881.0000, 1881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.998
Iter 2/2000 - Loss: -0.438
Iter 3/2000 - Loss: -1.059
Iter 4/2000 - Loss: -1.039
Iter 5/2000 - Loss: -0.861
Iter 6/2000 - Loss: -0.971
Iter 7/2000 - Loss: -1.204
Iter 8/2000 - Loss: -1.377
Iter 9/2000 - Loss: -1.472
Iter 10/2000 - Loss: -1.561
Iter 11/2000 - Loss: -1.703
Iter 12/2000 - Loss: -1.903
Iter 13/2000 - Loss: -2.143
Iter 14/2000 - Loss: -2.401
Iter 15/2000 - Loss: -2.672
Iter 16/2000 - Loss: -2.957
Iter 17/2000 - Loss: -3.259
Iter 18/2000 - Loss: -3.579
Iter 19/2000 - Loss: -3.913
Iter 20/2000 - Loss: -4.256
Iter 1981/2000 - Loss: -9.021
Iter 1982/2000 - Loss: -9.021
Iter 1983/2000 - Loss: -9.021
Iter 1984/2000 - Loss: -9.021
Iter 1985/2000 - Loss: -9.021
Iter 1986/2000 - Loss: -9.021
Iter 1987/2000 - Loss: -9.021
Iter 1988/2000 - Loss: -9.021
Iter 1989/2000 - Loss: -9.021
Iter 1990/2000 - Loss: -9.021
Iter 1991/2000 - Loss: -9.021
Iter 1992/2000 - Loss: -9.021
Iter 1993/2000 - Loss: -9.021
Iter 1994/2000 - Loss: -9.021
Iter 1995/2000 - Loss: -9.021
Iter 1996/2000 - Loss: -9.021
Iter 1997/2000 - Loss: -9.021
Iter 1998/2000 - Loss: -9.021
Iter 1999/2000 - Loss: -9.021
Iter 2000/2000 - Loss: -9.021
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.3568,  5.4257, 19.5729,  7.8439, 16.0903, 34.5660]],

        [[12.7411, 22.6566, 11.6124,  2.3002,  1.0915, 19.2565]],

        [[13.1740, 22.9270, 16.9690,  0.9383,  1.4485, 16.5513]],

        [[12.4171, 21.8555, 14.9723,  3.0638,  2.0085, 38.1044]]])
Signal Variance: tensor([ 0.0842,  1.0640, 11.1500,  0.3092])
Estimated target variance: tensor([0.0091, 0.0347, 0.4839, 0.0095])
N: 470
Signal to noise ratio: tensor([16.4868, 52.8728, 68.0852, 33.3467])
Bound on condition number: tensor([ 127753.4457, 1313902.2050, 2178728.0007,  522641.0351])
Policy Optimizer learning rate:
0.09526898853014376
Experience 47, Iter 0, disc loss: 2.310082976572608e-05, policy loss: 11.189121054059019
Experience 47, Iter 1, disc loss: 2.6560487649780782e-05, policy loss: 11.128335981112487
Experience 47, Iter 2, disc loss: 2.5527204322020178e-05, policy loss: 11.176289211225606
Experience 47, Iter 3, disc loss: 2.2162492228465892e-05, policy loss: 11.165905799821296
Experience 47, Iter 4, disc loss: 2.2467764316691318e-05, policy loss: 11.19094631986303
Experience 47, Iter 5, disc loss: 2.1522607021840716e-05, policy loss: 11.394712801911506
Experience 47, Iter 6, disc loss: 2.2025824791741182e-05, policy loss: 11.297816572864871
Experience 47, Iter 7, disc loss: 2.8234177639607472e-05, policy loss: 11.098991251276136
Experience 47, Iter 8, disc loss: 2.1825200978727108e-05, policy loss: 11.337414957086608
Experience 47, Iter 9, disc loss: 2.2661246666871416e-05, policy loss: 11.236706567202203
Experience 47, Iter 10, disc loss: 2.4900434547999274e-05, policy loss: 11.235533932350153
Experience 47, Iter 11, disc loss: 2.437682564851122e-05, policy loss: 11.114906549412163
Experience 47, Iter 12, disc loss: 2.5375905452461374e-05, policy loss: 11.143354709837364
Experience 47, Iter 13, disc loss: 2.4719325325540587e-05, policy loss: 11.071626726135097
Experience 47, Iter 14, disc loss: 2.6362036544706368e-05, policy loss: 11.093531874476707
Experience 47, Iter 15, disc loss: 2.2348199904473082e-05, policy loss: 11.232895126837093
Experience 47, Iter 16, disc loss: 2.5890540861999306e-05, policy loss: 11.152628739624777
Experience 47, Iter 17, disc loss: 2.738519695637145e-05, policy loss: 11.010612724110487
Experience 47, Iter 18, disc loss: 2.7283040969552057e-05, policy loss: 11.084305419898563
Experience 47, Iter 19, disc loss: 2.1437277643885193e-05, policy loss: 11.433775144398206
Experience 47, Iter 20, disc loss: 2.7630214378768072e-05, policy loss: 11.125425829058443
Experience 47, Iter 21, disc loss: 2.5700070951390076e-05, policy loss: 11.061100009361564
Experience 47, Iter 22, disc loss: 2.313994044645273e-05, policy loss: 11.1982687222463
Experience 47, Iter 23, disc loss: 2.1048484761698738e-05, policy loss: 11.440821983441438
Experience 47, Iter 24, disc loss: 2.4486735045388924e-05, policy loss: 11.12691801395406
Experience 47, Iter 25, disc loss: 2.925294176554451e-05, policy loss: 11.063683366587089
Experience 47, Iter 26, disc loss: 2.1877595239248608e-05, policy loss: 11.242220471603845
Experience 47, Iter 27, disc loss: 2.6344060907309968e-05, policy loss: 11.0517605283265
Experience 47, Iter 28, disc loss: 2.5320308256647745e-05, policy loss: 11.201098268970323
Experience 47, Iter 29, disc loss: 2.5939819014695422e-05, policy loss: 11.107905514067838
Experience 47, Iter 30, disc loss: 2.833038699361982e-05, policy loss: 11.161937465984012
Experience 47, Iter 31, disc loss: 2.88574554895191e-05, policy loss: 11.182542008385028
Experience 47, Iter 32, disc loss: 2.430383171401293e-05, policy loss: 11.344212667051153
Experience 47, Iter 33, disc loss: 2.954521788918994e-05, policy loss: 10.954987013268292
Experience 47, Iter 34, disc loss: 2.4479802077330677e-05, policy loss: 11.183391058220042
Experience 47, Iter 35, disc loss: 2.3432257504304065e-05, policy loss: 11.35564301015112
Experience 47, Iter 36, disc loss: 2.119831238316877e-05, policy loss: 11.357527300832318
Experience 47, Iter 37, disc loss: 2.3793196126235086e-05, policy loss: 11.119895095212016
Experience 47, Iter 38, disc loss: 2.2570378099716977e-05, policy loss: 11.243246705382099
Experience 47, Iter 39, disc loss: 2.337050918614473e-05, policy loss: 11.213139595441845
Experience 47, Iter 40, disc loss: 2.6633785651454928e-05, policy loss: 11.136366520110117
Experience 47, Iter 41, disc loss: 2.2439503588960086e-05, policy loss: 11.386227167501868
Experience 47, Iter 42, disc loss: 2.430656851812117e-05, policy loss: 11.348777973734943
Experience 47, Iter 43, disc loss: 2.413355348674125e-05, policy loss: 11.200779725937522
Experience 47, Iter 44, disc loss: 2.2563915684165452e-05, policy loss: 11.283718581500395
Experience 47, Iter 45, disc loss: 2.2271331381517642e-05, policy loss: 11.333549081702685
Experience 47, Iter 46, disc loss: 2.6172685068494194e-05, policy loss: 11.021616506975988
Experience 47, Iter 47, disc loss: 2.2649676590230954e-05, policy loss: 11.330208680347223
Experience 47, Iter 48, disc loss: 2.223161610752019e-05, policy loss: 11.437819247084356
Experience 47, Iter 49, disc loss: 2.2475916007266916e-05, policy loss: 11.174212959940807
Experience 47, Iter 50, disc loss: 1.8855109507065123e-05, policy loss: 11.49551737628008
Experience 47, Iter 51, disc loss: 1.986859100152159e-05, policy loss: 11.434733784814725
Experience 47, Iter 52, disc loss: 2.297462013819996e-05, policy loss: 11.199115154724339
Experience 47, Iter 53, disc loss: 2.633097711214807e-05, policy loss: 11.14158814750386
Experience 47, Iter 54, disc loss: 2.547583169410169e-05, policy loss: 11.146568181287284
Experience 47, Iter 55, disc loss: 2.776553188012039e-05, policy loss: 11.10114273597463
Experience 47, Iter 56, disc loss: 2.7386687517391144e-05, policy loss: 11.053875984607808
Experience 47, Iter 57, disc loss: 2.4683549772645984e-05, policy loss: 11.131796222586875
Experience 47, Iter 58, disc loss: 2.5190436049620227e-05, policy loss: 11.126820391441385
Experience 47, Iter 59, disc loss: 2.196048494120745e-05, policy loss: 11.238893104934757
Experience 47, Iter 60, disc loss: 2.378001071407458e-05, policy loss: 11.133400564560452
Experience 47, Iter 61, disc loss: 2.0194912699856383e-05, policy loss: 11.303988097513606
Experience 47, Iter 62, disc loss: 2.5432296996795338e-05, policy loss: 11.131282399340567
Experience 47, Iter 63, disc loss: 2.233134556981785e-05, policy loss: 11.132199311706415
Experience 47, Iter 64, disc loss: 2.0052096219185035e-05, policy loss: 11.416353019944715
Experience 47, Iter 65, disc loss: 2.4465841509496743e-05, policy loss: 11.130297830237074
Experience 47, Iter 66, disc loss: 2.234845957127249e-05, policy loss: 11.218292172568685
Experience 47, Iter 67, disc loss: 2.7265627044220493e-05, policy loss: 11.083018932235841
Experience 47, Iter 68, disc loss: 2.7297371884151938e-05, policy loss: 11.22111383692471
Experience 47, Iter 69, disc loss: 3.938426263401765e-05, policy loss: 11.153828173944738
Experience 47, Iter 70, disc loss: 2.6395590104074045e-05, policy loss: 11.088953445412661
Experience 47, Iter 71, disc loss: 2.169550628400309e-05, policy loss: 11.224810762403902
Experience 47, Iter 72, disc loss: 2.466811271621833e-05, policy loss: 11.168542508854646
Experience 47, Iter 73, disc loss: 2.5774078529115388e-05, policy loss: 11.042433758691253
Experience 47, Iter 74, disc loss: 2.2992328902169e-05, policy loss: 11.236307575542881
Experience 47, Iter 75, disc loss: 2.5671998130712707e-05, policy loss: 11.301968448827388
Experience 47, Iter 76, disc loss: 3.6770143460853794e-05, policy loss: 10.93133233777614
Experience 47, Iter 77, disc loss: 2.1035462934476432e-05, policy loss: 11.413012102312287
Experience 47, Iter 78, disc loss: 2.504350385360147e-05, policy loss: 11.258413476556045
Experience 47, Iter 79, disc loss: 2.5191414588567456e-05, policy loss: 11.15703808191014
Experience 47, Iter 80, disc loss: 3.102002516860633e-05, policy loss: 10.868471216259962
Experience 47, Iter 81, disc loss: 2.5396548607607295e-05, policy loss: 10.99829924454783
Experience 47, Iter 82, disc loss: 2.0790347791773838e-05, policy loss: 11.417690525951713
Experience 47, Iter 83, disc loss: 2.8774038168272487e-05, policy loss: 10.945544198334034
Experience 47, Iter 84, disc loss: 2.2572080800289087e-05, policy loss: 11.386620379758668
Experience 47, Iter 85, disc loss: 2.4017774433861014e-05, policy loss: 11.114184638641664
Experience 47, Iter 86, disc loss: 2.3489138853232028e-05, policy loss: 11.090542460477064
Experience 47, Iter 87, disc loss: 3.2078594501612584e-05, policy loss: 11.214846042339204
Experience 47, Iter 88, disc loss: 2.4892070932171125e-05, policy loss: 11.12877066340874
Experience 47, Iter 89, disc loss: 2.7654488315683018e-05, policy loss: 10.89679492126661
Experience 47, Iter 90, disc loss: 2.827962403033809e-05, policy loss: 11.061906855334353
Experience 47, Iter 91, disc loss: 2.3809512030570855e-05, policy loss: 11.325056715453671
Experience 47, Iter 92, disc loss: 2.4027159837202233e-05, policy loss: 11.260825960068043
Experience 47, Iter 93, disc loss: 2.5249290985282485e-05, policy loss: 11.126861146128471
Experience 47, Iter 94, disc loss: 2.5477403103624807e-05, policy loss: 11.158448152880016
Experience 47, Iter 95, disc loss: 2.3297196994345045e-05, policy loss: 11.135415925131625
Experience 47, Iter 96, disc loss: 2.2074593674512598e-05, policy loss: 11.249371324979311
Experience 47, Iter 97, disc loss: 2.6122361912501227e-05, policy loss: 10.955288650405231
Experience 47, Iter 98, disc loss: 2.414862567428546e-05, policy loss: 11.220580004446434
Experience 47, Iter 99, disc loss: 2.489173374108061e-05, policy loss: 11.194805102083532
Experience: 48
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0085],
        [0.1186],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0131, 0.0809, 0.1046, 0.0031, 0.0011, 0.3148]],

        [[0.0131, 0.0809, 0.1046, 0.0031, 0.0011, 0.3148]],

        [[0.0131, 0.0809, 0.1046, 0.0031, 0.0011, 0.3148]],

        [[0.0131, 0.0809, 0.1046, 0.0031, 0.0011, 0.3148]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0089, 0.0340, 0.4745, 0.0093], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0089, 0.0340, 0.4745, 0.0093])
N: 480
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1921.0000, 1921.0000, 1921.0000, 1921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.036
Iter 2/2000 - Loss: -0.459
Iter 3/2000 - Loss: -1.096
Iter 4/2000 - Loss: -1.073
Iter 5/2000 - Loss: -0.889
Iter 6/2000 - Loss: -1.000
Iter 7/2000 - Loss: -1.237
Iter 8/2000 - Loss: -1.411
Iter 9/2000 - Loss: -1.505
Iter 10/2000 - Loss: -1.592
Iter 11/2000 - Loss: -1.732
Iter 12/2000 - Loss: -1.932
Iter 13/2000 - Loss: -2.170
Iter 14/2000 - Loss: -2.428
Iter 15/2000 - Loss: -2.698
Iter 16/2000 - Loss: -2.982
Iter 17/2000 - Loss: -3.283
Iter 18/2000 - Loss: -3.601
Iter 19/2000 - Loss: -3.933
Iter 20/2000 - Loss: -4.275
Iter 1981/2000 - Loss: -9.024
Iter 1982/2000 - Loss: -9.024
Iter 1983/2000 - Loss: -9.024
Iter 1984/2000 - Loss: -9.024
Iter 1985/2000 - Loss: -9.024
Iter 1986/2000 - Loss: -9.024
Iter 1987/2000 - Loss: -9.024
Iter 1988/2000 - Loss: -9.024
Iter 1989/2000 - Loss: -9.024
Iter 1990/2000 - Loss: -9.024
Iter 1991/2000 - Loss: -9.024
Iter 1992/2000 - Loss: -9.024
Iter 1993/2000 - Loss: -9.024
Iter 1994/2000 - Loss: -9.024
Iter 1995/2000 - Loss: -9.024
Iter 1996/2000 - Loss: -9.024
Iter 1997/2000 - Loss: -9.024
Iter 1998/2000 - Loss: -9.024
Iter 1999/2000 - Loss: -9.024
Iter 2000/2000 - Loss: -9.024
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.3496,  5.4552, 19.4824,  7.8562, 16.2157, 34.9343]],

        [[12.6819, 22.4565, 11.6266,  2.3065,  1.0906, 19.3287]],

        [[13.0879, 22.7394, 16.9358,  0.9461,  1.4381, 16.7589]],

        [[12.3328, 21.5748, 14.8690,  3.0100,  1.9922, 37.7631]]])
Signal Variance: tensor([ 0.0845,  1.0687, 11.2584,  0.3044])
Estimated target variance: tensor([0.0089, 0.0340, 0.4745, 0.0093])
N: 480
Signal to noise ratio: tensor([16.5314, 52.9095, 68.2771, 33.0223])
Bound on condition number: tensor([ 131179.0059, 1343720.1797, 2237646.2947,  523427.8298])
Policy Optimizer learning rate:
0.09516866549228199
Experience 48, Iter 0, disc loss: 2.657191775348538e-05, policy loss: 11.118684076921513
Experience 48, Iter 1, disc loss: 2.673841576830366e-05, policy loss: 11.415638392785878
Experience 48, Iter 2, disc loss: 2.3560276261794404e-05, policy loss: 11.27145343166358
Experience 48, Iter 3, disc loss: 2.1095046382617084e-05, policy loss: 11.33942251429161
Experience 48, Iter 4, disc loss: 2.5493122097812764e-05, policy loss: 11.065396310032035
Experience 48, Iter 5, disc loss: 2.4296251427472178e-05, policy loss: 11.268847950238808
Experience 48, Iter 6, disc loss: 2.1244154008836207e-05, policy loss: 11.197501983726543
Experience 48, Iter 7, disc loss: 2.1783225991199598e-05, policy loss: 11.197748814029449
Experience 48, Iter 8, disc loss: 2.670158418244671e-05, policy loss: 11.161879391582572
Experience 48, Iter 9, disc loss: 2.8679609490991596e-05, policy loss: 10.882726451101394
Experience 48, Iter 10, disc loss: 2.687000983895343e-05, policy loss: 11.155378645245037
Experience 48, Iter 11, disc loss: 2.475604635531697e-05, policy loss: 11.289917958645704
Experience 48, Iter 12, disc loss: 3.031700967777786e-05, policy loss: 10.977871362207873
Experience 48, Iter 13, disc loss: 2.565133224282938e-05, policy loss: 11.136169296245715
Experience 48, Iter 14, disc loss: 2.6261024350346566e-05, policy loss: 11.07353017167938
Experience 48, Iter 15, disc loss: 2.280150076109524e-05, policy loss: 11.269154362804818
Experience 48, Iter 16, disc loss: 2.3778540821662907e-05, policy loss: 11.149791027462168
Experience 48, Iter 17, disc loss: 2.7565021391980595e-05, policy loss: 11.063777435915608
Experience 48, Iter 18, disc loss: 2.7133732916421573e-05, policy loss: 11.093651671817957
Experience 48, Iter 19, disc loss: 2.362293795227977e-05, policy loss: 11.177111296827887
Experience 48, Iter 20, disc loss: 1.9593839579193827e-05, policy loss: 11.466074221401948
Experience 48, Iter 21, disc loss: 2.2426956529999164e-05, policy loss: 11.300058763575803
Experience 48, Iter 22, disc loss: 3.2365407548890685e-05, policy loss: 11.197152777382655
Experience 48, Iter 23, disc loss: 2.306655566907879e-05, policy loss: 11.200044207161856
Experience 48, Iter 24, disc loss: 2.5680105628049444e-05, policy loss: 11.181371515783827
Experience 48, Iter 25, disc loss: 2.634505777293503e-05, policy loss: 11.067415280841827
Experience 48, Iter 26, disc loss: 2.5091514762783767e-05, policy loss: 11.18809542055543
Experience 48, Iter 27, disc loss: 2.0871857884949796e-05, policy loss: 11.300841915249016
Experience 48, Iter 28, disc loss: 2.3525406345894994e-05, policy loss: 11.323767771990958
Experience 48, Iter 29, disc loss: 2.0634996910131715e-05, policy loss: 11.243860806145037
Experience 48, Iter 30, disc loss: 2.4631975652183696e-05, policy loss: 11.062566977447576
Experience 48, Iter 31, disc loss: 2.4373737845910007e-05, policy loss: 11.021264807970473
Experience 48, Iter 32, disc loss: 2.2878294082854816e-05, policy loss: 11.205057543247035
Experience 48, Iter 33, disc loss: 2.623199566163759e-05, policy loss: 11.261496275368996
Experience 48, Iter 34, disc loss: 2.2084469469745032e-05, policy loss: 11.344246703431708
Experience 48, Iter 35, disc loss: 2.4903197760638908e-05, policy loss: 11.272048334019406
Experience 48, Iter 36, disc loss: 2.1673229110191943e-05, policy loss: 11.354545251636747
Experience 48, Iter 37, disc loss: 2.495537944938674e-05, policy loss: 11.174409642343477
Experience 48, Iter 38, disc loss: 2.3840809279071805e-05, policy loss: 11.06587654773151
Experience 48, Iter 39, disc loss: 2.3870462143404437e-05, policy loss: 11.236295006729236
Experience 48, Iter 40, disc loss: 2.5858847022610727e-05, policy loss: 11.115866404276982
Experience 48, Iter 41, disc loss: 2.0507961357897276e-05, policy loss: 11.302198367399278
Experience 48, Iter 42, disc loss: 2.760467248486764e-05, policy loss: 11.17049746105378
Experience 48, Iter 43, disc loss: 2.1499888915419204e-05, policy loss: 11.260500051624417
Experience 48, Iter 44, disc loss: 2.5649487626804906e-05, policy loss: 11.072324334249267
Experience 48, Iter 45, disc loss: 2.228587078076051e-05, policy loss: 11.270151403902034
Experience 48, Iter 46, disc loss: 1.8617368022092447e-05, policy loss: 11.612631939248905
Experience 48, Iter 47, disc loss: 2.1498933339060403e-05, policy loss: 11.119760503938728
Experience 48, Iter 48, disc loss: 2.170572922538532e-05, policy loss: 11.191461776142358
Experience 48, Iter 49, disc loss: 2.245050096092887e-05, policy loss: 11.363905222670702
Experience 48, Iter 50, disc loss: 2.07538402560966e-05, policy loss: 11.271917411060434
Experience 48, Iter 51, disc loss: 2.4841028941484746e-05, policy loss: 11.168387102974691
Experience 48, Iter 52, disc loss: 2.2123716819811534e-05, policy loss: 11.20921516258419
Experience 48, Iter 53, disc loss: 2.4381010996384825e-05, policy loss: 11.345322948777639
Experience 48, Iter 54, disc loss: 2.6181061911135258e-05, policy loss: 11.412100817717757
Experience 48, Iter 55, disc loss: 2.475637729975037e-05, policy loss: 11.18897349811132
Experience 48, Iter 56, disc loss: 2.872636432107193e-05, policy loss: 10.9915335932216
Experience 48, Iter 57, disc loss: 2.3960790814478817e-05, policy loss: 11.29364556595013
Experience 48, Iter 58, disc loss: 2.5323263997429587e-05, policy loss: 11.153057533991653
Experience 48, Iter 59, disc loss: 2.3334308014348382e-05, policy loss: 11.161986681855417
Experience 48, Iter 60, disc loss: 2.666083924362286e-05, policy loss: 11.128460675563908
Experience 48, Iter 61, disc loss: 2.292962968693214e-05, policy loss: 11.26518621236367
Experience 48, Iter 62, disc loss: 2.1750960271461398e-05, policy loss: 11.359343718597525
Experience 48, Iter 63, disc loss: 2.2242671961256072e-05, policy loss: 11.306094848465362
Experience 48, Iter 64, disc loss: 1.8237022589993886e-05, policy loss: 11.523396579574083
Experience 48, Iter 65, disc loss: 2.6488569256332828e-05, policy loss: 11.238882703036055
Experience 48, Iter 66, disc loss: 1.856182414791114e-05, policy loss: 11.469558937485889
Experience 48, Iter 67, disc loss: 2.140164952547109e-05, policy loss: 11.312183504743993
Experience 48, Iter 68, disc loss: 1.970981980079884e-05, policy loss: 11.328439467416114
Experience 48, Iter 69, disc loss: 2.3776057456800135e-05, policy loss: 11.005885923149243
Experience 48, Iter 70, disc loss: 2.3244905040223896e-05, policy loss: 11.152854203922391
Experience 48, Iter 71, disc loss: 2.2806830795348256e-05, policy loss: 11.331537791085863
Experience 48, Iter 72, disc loss: 2.7376746586679587e-05, policy loss: 11.206099129232351
Experience 48, Iter 73, disc loss: 2.0413518659722436e-05, policy loss: 11.427817570498762
Experience 48, Iter 74, disc loss: 2.201942562000262e-05, policy loss: 11.392855794965177
Experience 48, Iter 75, disc loss: 2.3708353823458868e-05, policy loss: 11.249841531551727
Experience 48, Iter 76, disc loss: 2.635331177336583e-05, policy loss: 11.209444952596757
Experience 48, Iter 77, disc loss: 2.0016100217899537e-05, policy loss: 11.370906453729763
Experience 48, Iter 78, disc loss: 1.953607113433813e-05, policy loss: 11.395357577672083
Experience 48, Iter 79, disc loss: 1.9403386213476084e-05, policy loss: 11.389896144262696
Experience 48, Iter 80, disc loss: 1.9113437048015025e-05, policy loss: 11.307302329392733
Experience 48, Iter 81, disc loss: 2.2085690790627164e-05, policy loss: 11.157389920679101
Experience 48, Iter 82, disc loss: 2.3764729496716423e-05, policy loss: 11.147856627483156
Experience 48, Iter 83, disc loss: 2.6219133331724656e-05, policy loss: 11.110189358431512
Experience 48, Iter 84, disc loss: 2.8648652446686543e-05, policy loss: 11.220004373905887
Experience 48, Iter 85, disc loss: 2.13638915791052e-05, policy loss: 11.495151350227736
Experience 48, Iter 86, disc loss: 2.791125017176513e-05, policy loss: 11.147132320479928
Experience 48, Iter 87, disc loss: 2.276117682650875e-05, policy loss: 11.347623068362608
Experience 48, Iter 88, disc loss: 2.2589199554826192e-05, policy loss: 11.475010725405673
Experience 48, Iter 89, disc loss: 2.0818673903144158e-05, policy loss: 11.23881129012709
Experience 48, Iter 90, disc loss: 2.0151716546757493e-05, policy loss: 11.34757555199371
Experience 48, Iter 91, disc loss: 1.786768530934783e-05, policy loss: 11.511695266241071
Experience 48, Iter 92, disc loss: 1.9576525416030057e-05, policy loss: 11.29643837142591
Experience 48, Iter 93, disc loss: 1.9714189841231414e-05, policy loss: 11.298755965517467
Experience 48, Iter 94, disc loss: 2.2493199670960155e-05, policy loss: 11.242271466959599
Experience 48, Iter 95, disc loss: 2.2780107011022794e-05, policy loss: 11.289270097292292
Experience 48, Iter 96, disc loss: 1.9344038110675734e-05, policy loss: 11.608661456375454
Experience 48, Iter 97, disc loss: 2.3250151707230226e-05, policy loss: 11.153854030700623
Experience 48, Iter 98, disc loss: 2.89442872519123e-05, policy loss: 11.32819050177755
Experience 48, Iter 99, disc loss: 2.255895517302354e-05, policy loss: 11.241006544082044
Experience: 49
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0084],
        [0.1180],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0130, 0.0795, 0.1041, 0.0031, 0.0011, 0.3088]],

        [[0.0130, 0.0795, 0.1041, 0.0031, 0.0011, 0.3088]],

        [[0.0130, 0.0795, 0.1041, 0.0031, 0.0011, 0.3088]],

        [[0.0130, 0.0795, 0.1041, 0.0031, 0.0011, 0.3088]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0088, 0.0335, 0.4721, 0.0093], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0088, 0.0335, 0.4721, 0.0093])
N: 490
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1961.0000, 1961.0000, 1961.0000, 1961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.056
Iter 2/2000 - Loss: -0.460
Iter 3/2000 - Loss: -1.112
Iter 4/2000 - Loss: -1.084
Iter 5/2000 - Loss: -0.891
Iter 6/2000 - Loss: -1.004
Iter 7/2000 - Loss: -1.242
Iter 8/2000 - Loss: -1.412
Iter 9/2000 - Loss: -1.498
Iter 10/2000 - Loss: -1.577
Iter 11/2000 - Loss: -1.711
Iter 12/2000 - Loss: -1.906
Iter 13/2000 - Loss: -2.141
Iter 14/2000 - Loss: -2.394
Iter 15/2000 - Loss: -2.657
Iter 16/2000 - Loss: -2.934
Iter 17/2000 - Loss: -3.228
Iter 18/2000 - Loss: -3.542
Iter 19/2000 - Loss: -3.872
Iter 20/2000 - Loss: -4.213
Iter 1981/2000 - Loss: -9.038
Iter 1982/2000 - Loss: -9.038
Iter 1983/2000 - Loss: -9.038
Iter 1984/2000 - Loss: -9.038
Iter 1985/2000 - Loss: -9.038
Iter 1986/2000 - Loss: -9.038
Iter 1987/2000 - Loss: -9.038
Iter 1988/2000 - Loss: -9.038
Iter 1989/2000 - Loss: -9.038
Iter 1990/2000 - Loss: -9.038
Iter 1991/2000 - Loss: -9.038
Iter 1992/2000 - Loss: -9.038
Iter 1993/2000 - Loss: -9.038
Iter 1994/2000 - Loss: -9.038
Iter 1995/2000 - Loss: -9.038
Iter 1996/2000 - Loss: -9.038
Iter 1997/2000 - Loss: -9.038
Iter 1998/2000 - Loss: -9.039
Iter 1999/2000 - Loss: -9.039
Iter 2000/2000 - Loss: -9.039
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 9.1545,  5.4475, 19.5270,  7.5568, 15.6228, 35.2250]],

        [[12.6669, 22.2491, 11.6215,  2.4171,  1.1002, 19.6205]],

        [[12.8550, 22.4196, 16.9344,  0.9563,  1.4255, 16.8307]],

        [[12.1688, 21.4603, 14.6658,  2.8769,  1.9837, 37.5360]]])
Signal Variance: tensor([ 0.0847,  1.1067, 11.3454,  0.2991])
Estimated target variance: tensor([0.0088, 0.0335, 0.4721, 0.0093])
N: 490
Signal to noise ratio: tensor([16.5680, 53.5739, 68.9632, 32.9513])
Bound on condition number: tensor([ 134504.8164, 1406381.3793, 2330405.0168,  532036.6214])
Policy Optimizer learning rate:
0.09506844809962631
Experience 49, Iter 0, disc loss: 1.8774896637335912e-05, policy loss: 11.455672782797604
Experience 49, Iter 1, disc loss: 2.3561799211162128e-05, policy loss: 11.354963671676593
Experience 49, Iter 2, disc loss: 2.0197806655634528e-05, policy loss: 11.215596780186107
Experience 49, Iter 3, disc loss: 2.3314925601851266e-05, policy loss: 11.346891603741158
Experience 49, Iter 4, disc loss: 2.1501303323514702e-05, policy loss: 11.211157540934622
Experience 49, Iter 5, disc loss: 2.224405718381459e-05, policy loss: 11.293617661814816
Experience 49, Iter 6, disc loss: 2.002271704009366e-05, policy loss: 11.52236791206008
Experience 49, Iter 7, disc loss: 2.6283955936608895e-05, policy loss: 11.294325223955603
Experience 49, Iter 8, disc loss: 2.2792645417775104e-05, policy loss: 11.28654726973856
Experience 49, Iter 9, disc loss: 2.0456049257227415e-05, policy loss: 11.328419193248205
Experience 49, Iter 10, disc loss: 2.212791637009697e-05, policy loss: 11.28831059651801
Experience 49, Iter 11, disc loss: 2.1497494573273578e-05, policy loss: 11.34805147800429
Experience 49, Iter 12, disc loss: 2.2256635799322955e-05, policy loss: 11.082202750441594
Experience 49, Iter 13, disc loss: 2.6581111352803675e-05, policy loss: 11.537113341916399
Experience 49, Iter 14, disc loss: 2.4886223290667418e-05, policy loss: 11.192390829912014
Experience 49, Iter 15, disc loss: 2.1795841075591386e-05, policy loss: 11.257589843152658
Experience 49, Iter 16, disc loss: 2.652038041106997e-05, policy loss: 11.222336540884626
Experience 49, Iter 17, disc loss: 2.2901088949435182e-05, policy loss: 11.29640519228558
Experience 49, Iter 18, disc loss: 2.1506555349481456e-05, policy loss: 11.407522328902663
Experience 49, Iter 19, disc loss: 2.285627504424786e-05, policy loss: 11.373991021672541
Experience 49, Iter 20, disc loss: 2.2131506495306385e-05, policy loss: 11.227239432671542
Experience 49, Iter 21, disc loss: 2.309162753208623e-05, policy loss: 11.182228706441538
Experience 49, Iter 22, disc loss: 2.1576535192720655e-05, policy loss: 11.536514529664663
Experience 49, Iter 23, disc loss: 2.2070680136663105e-05, policy loss: 11.207741332165554
Experience 49, Iter 24, disc loss: 2.0976055578463226e-05, policy loss: 11.38991157885835
Experience 49, Iter 25, disc loss: 2.4023468704653855e-05, policy loss: 11.081257797321935
Experience 49, Iter 26, disc loss: 2.8796403770452166e-05, policy loss: 11.07776549865979
Experience 49, Iter 27, disc loss: 2.3711299290837865e-05, policy loss: 11.164493974468144
Experience 49, Iter 28, disc loss: 2.937343601716809e-05, policy loss: 11.210545907176874
Experience 49, Iter 29, disc loss: 1.8972830422930416e-05, policy loss: 11.369297700699741
Experience 49, Iter 30, disc loss: 2.4625618038432805e-05, policy loss: 11.070045911570489
Experience 49, Iter 31, disc loss: 2.295537267490761e-05, policy loss: 11.245499893030807
Experience 49, Iter 32, disc loss: 2.305005520936918e-05, policy loss: 11.156786894361193
Experience 49, Iter 33, disc loss: 2.383776618125082e-05, policy loss: 11.272295754615978
Experience 49, Iter 34, disc loss: 2.3364071576177004e-05, policy loss: 11.353116752336849
Experience 49, Iter 35, disc loss: 2.1138924342019355e-05, policy loss: 11.318422503798772
Experience 49, Iter 36, disc loss: 2.03597535453881e-05, policy loss: 11.1916399255885
Experience 49, Iter 37, disc loss: 2.09479443432015e-05, policy loss: 11.25501815305133
Experience 49, Iter 38, disc loss: 2.108174141393268e-05, policy loss: 11.419931220761734
Experience 49, Iter 39, disc loss: 2.673472082101904e-05, policy loss: 11.121847152035947
Experience 49, Iter 40, disc loss: 2.2100066587422132e-05, policy loss: 11.46446256166595
Experience 49, Iter 41, disc loss: 2.2748842702839482e-05, policy loss: 11.368640138161656
Experience 49, Iter 42, disc loss: 1.9990610332665992e-05, policy loss: 11.36652356026951
Experience 49, Iter 43, disc loss: 2.4884572285775383e-05, policy loss: 11.117069141747633
Experience 49, Iter 44, disc loss: 2.4892590072220234e-05, policy loss: 11.21849385112596
Experience 49, Iter 45, disc loss: 2.052760195774909e-05, policy loss: 11.242283579203631
Experience 49, Iter 46, disc loss: 2.261780557116524e-05, policy loss: 11.247367927036034
Experience 49, Iter 47, disc loss: 1.7830635911102635e-05, policy loss: 11.46027790515661
Experience 49, Iter 48, disc loss: 2.2623111062294608e-05, policy loss: 11.296987080382761
Experience 49, Iter 49, disc loss: 3.179087473228112e-05, policy loss: 10.97223679643899
Experience 49, Iter 50, disc loss: 2.3944659016451046e-05, policy loss: 11.200102375202865
Experience 49, Iter 51, disc loss: 3.389821898992077e-05, policy loss: 11.171493955848735
Experience 49, Iter 52, disc loss: 3.8267014162898005e-05, policy loss: 10.90589285084357
Experience 49, Iter 53, disc loss: 2.8341531334905204e-05, policy loss: 11.240065609442032
Experience 49, Iter 54, disc loss: 3.232563029694004e-05, policy loss: 10.919372234900745
Experience 49, Iter 55, disc loss: 2.5465207687355785e-05, policy loss: 11.256756203109376
Experience 49, Iter 56, disc loss: 2.975371991179473e-05, policy loss: 10.993661373710061
Experience 49, Iter 57, disc loss: 2.0418820502098058e-05, policy loss: 11.507416925143993
Experience 49, Iter 58, disc loss: 2.634658753229067e-05, policy loss: 11.192353666095926
Experience 49, Iter 59, disc loss: 2.179928072337046e-05, policy loss: 11.18903729687236
Experience 49, Iter 60, disc loss: 1.871546830292893e-05, policy loss: 11.45212368433636
Experience 49, Iter 61, disc loss: 2.2919574796807917e-05, policy loss: 11.109667345243931
Experience 49, Iter 62, disc loss: 2.0101290858984094e-05, policy loss: 11.342225509410143
Experience 49, Iter 63, disc loss: 2.0891910838826987e-05, policy loss: 11.477873137207713
Experience 49, Iter 64, disc loss: 2.100370522943239e-05, policy loss: 11.381127476338838
Experience 49, Iter 65, disc loss: 1.7283469635969796e-05, policy loss: 11.583461240891218
Experience 49, Iter 66, disc loss: 2.298932749440614e-05, policy loss: 11.175779698969137
Experience 49, Iter 67, disc loss: 2.182447423298622e-05, policy loss: 11.220050549860531
Experience 49, Iter 68, disc loss: 2.321111332204183e-05, policy loss: 11.07398861604281
Experience 49, Iter 69, disc loss: 2.3507212559775595e-05, policy loss: 11.251763009100692
Experience 49, Iter 70, disc loss: 2.0227162770361982e-05, policy loss: 11.372646663129139
Experience 49, Iter 71, disc loss: 2.2684147085083918e-05, policy loss: 11.267452485164387
Experience 49, Iter 72, disc loss: 2.313117627081991e-05, policy loss: 11.314887904502802
Experience 49, Iter 73, disc loss: 2.5160547501691343e-05, policy loss: 11.336868567813228
Experience 49, Iter 74, disc loss: 1.557473111108669e-05, policy loss: 11.72234137273865
Experience 49, Iter 75, disc loss: 2.0284648370555498e-05, policy loss: 11.166488202029585
Experience 49, Iter 76, disc loss: 2.2104263909048495e-05, policy loss: 11.324745782816468
Experience 49, Iter 77, disc loss: 2.0854744133655513e-05, policy loss: 11.283447851387002
Experience 49, Iter 78, disc loss: 2.1195375068963094e-05, policy loss: 11.262956971198374
Experience 49, Iter 79, disc loss: 2.040994779132953e-05, policy loss: 11.314635972187348
Experience 49, Iter 80, disc loss: 2.3839702419468226e-05, policy loss: 11.191794905375493
Experience 49, Iter 81, disc loss: 2.12341348577738e-05, policy loss: 11.320332005781795
Experience 49, Iter 82, disc loss: 2.104757864119392e-05, policy loss: 11.532376604878003
Experience 49, Iter 83, disc loss: 2.3016487717117483e-05, policy loss: 11.227734087819865
Experience 49, Iter 84, disc loss: 1.9934237207614668e-05, policy loss: 11.406760472382665
Experience 49, Iter 85, disc loss: 2.4428861300404533e-05, policy loss: 11.329198268273794
Experience 49, Iter 86, disc loss: 2.2135599259139074e-05, policy loss: 11.15857577074701
Experience 49, Iter 87, disc loss: 2.3213551174432778e-05, policy loss: 11.261368222262819
Experience 49, Iter 88, disc loss: 2.005154647297351e-05, policy loss: 11.330085053034509
Experience 49, Iter 89, disc loss: 2.2588019789095165e-05, policy loss: 11.298850853447338
Experience 49, Iter 90, disc loss: 1.8575562800874593e-05, policy loss: 11.541584391058635
Experience 49, Iter 91, disc loss: 2.6946113975022673e-05, policy loss: 11.105947902220349
Experience 49, Iter 92, disc loss: 2.2139477050809706e-05, policy loss: 11.384127903328306
Experience 49, Iter 93, disc loss: 2.2216200028755975e-05, policy loss: 11.429918838863
Experience 49, Iter 94, disc loss: 2.410601815166365e-05, policy loss: 11.244234798385795
Experience 49, Iter 95, disc loss: 2.3232292599724456e-05, policy loss: 11.531204583875077
Experience 49, Iter 96, disc loss: 2.3634561241755094e-05, policy loss: 11.273640436713462
Experience 49, Iter 97, disc loss: 2.1942550805371905e-05, policy loss: 11.254119231384102
Experience 49, Iter 98, disc loss: 2.1817544544823602e-05, policy loss: 11.343899699243533
Experience 49, Iter 99, disc loss: 2.2076683124977877e-05, policy loss: 11.219348200251332
Experience: 50
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0082],
        [0.1165],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.0779, 0.1026, 0.0030, 0.0011, 0.3029]],

        [[0.0128, 0.0779, 0.1026, 0.0030, 0.0011, 0.3029]],

        [[0.0128, 0.0779, 0.1026, 0.0030, 0.0011, 0.3029]],

        [[0.0128, 0.0779, 0.1026, 0.0030, 0.0011, 0.3029]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0086, 0.0329, 0.4659, 0.0092], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0086, 0.0329, 0.4659, 0.0092])
N: 500
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2001.0000, 2001.0000, 2001.0000, 2001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.085
Iter 2/2000 - Loss: -0.468
Iter 3/2000 - Loss: -1.138
Iter 4/2000 - Loss: -1.105
Iter 5/2000 - Loss: -0.902
Iter 6/2000 - Loss: -1.015
Iter 7/2000 - Loss: -1.254
Iter 8/2000 - Loss: -1.420
Iter 9/2000 - Loss: -1.498
Iter 10/2000 - Loss: -1.568
Iter 11/2000 - Loss: -1.695
Iter 12/2000 - Loss: -1.884
Iter 13/2000 - Loss: -2.112
Iter 14/2000 - Loss: -2.356
Iter 15/2000 - Loss: -2.610
Iter 16/2000 - Loss: -2.878
Iter 17/2000 - Loss: -3.164
Iter 18/2000 - Loss: -3.471
Iter 19/2000 - Loss: -3.795
Iter 20/2000 - Loss: -4.132
Iter 1981/2000 - Loss: -9.046
Iter 1982/2000 - Loss: -9.046
Iter 1983/2000 - Loss: -9.046
Iter 1984/2000 - Loss: -9.047
Iter 1985/2000 - Loss: -9.047
Iter 1986/2000 - Loss: -9.047
Iter 1987/2000 - Loss: -9.047
Iter 1988/2000 - Loss: -9.047
Iter 1989/2000 - Loss: -9.047
Iter 1990/2000 - Loss: -9.047
Iter 1991/2000 - Loss: -9.047
Iter 1992/2000 - Loss: -9.047
Iter 1993/2000 - Loss: -9.047
Iter 1994/2000 - Loss: -9.047
Iter 1995/2000 - Loss: -9.047
Iter 1996/2000 - Loss: -9.047
Iter 1997/2000 - Loss: -9.047
Iter 1998/2000 - Loss: -9.047
Iter 1999/2000 - Loss: -9.047
Iter 2000/2000 - Loss: -9.047
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[ 8.8970,  5.3385, 19.3354,  7.5381, 15.6263, 34.4941]],

        [[12.4790, 22.2496, 11.6012,  2.4414,  1.1098, 19.7545]],

        [[12.7907, 22.2804, 16.7689,  0.9714,  1.4351, 16.9943]],

        [[12.0820, 21.4742, 14.6906,  2.8903,  2.0331, 37.6090]]])
Signal Variance: tensor([ 0.0834,  1.1233, 11.3723,  0.3003])
Estimated target variance: tensor([0.0086, 0.0329, 0.4659, 0.0092])
N: 500
Signal to noise ratio: tensor([16.5024, 54.3328, 68.8254, 32.8195])
Bound on condition number: tensor([ 136165.9487, 1476028.3427, 2368471.1692,  538561.8771])
Policy Optimizer learning rate:
0.09496833624092697
Experience 50, Iter 0, disc loss: 1.9303538274380443e-05, policy loss: 11.29836389603317
Experience 50, Iter 1, disc loss: 2.2094955493021142e-05, policy loss: 11.411853939028532
Experience 50, Iter 2, disc loss: 2.174178072656859e-05, policy loss: 11.251950739832683
Experience 50, Iter 3, disc loss: 2.410834497845574e-05, policy loss: 11.268923924283909
Experience 50, Iter 4, disc loss: 2.0110808974674557e-05, policy loss: 11.422685771091873
Experience 50, Iter 5, disc loss: 2.7561241307439135e-05, policy loss: 11.136667522208713
Experience 50, Iter 6, disc loss: 2.2966145342877822e-05, policy loss: 11.247955865522577
Experience 50, Iter 7, disc loss: 1.9226761296246884e-05, policy loss: 11.339727006301162
Experience 50, Iter 8, disc loss: 1.936543925977995e-05, policy loss: 11.39995018749823
Experience 50, Iter 9, disc loss: 2.725034072678765e-05, policy loss: 11.164587918214298
Experience 50, Iter 10, disc loss: 2.4540520407419935e-05, policy loss: 11.373063402490242
Experience 50, Iter 11, disc loss: 2.002555383090411e-05, policy loss: 11.395326901225904
Experience 50, Iter 12, disc loss: 2.397955463802271e-05, policy loss: 11.250830974550137
Experience 50, Iter 13, disc loss: 2.1501266497668378e-05, policy loss: 11.375695574848068
Experience 50, Iter 14, disc loss: 2.3707111759511532e-05, policy loss: 11.286347227279755
Experience 50, Iter 15, disc loss: 2.0395860738710958e-05, policy loss: 11.417526801305803
Experience 50, Iter 16, disc loss: 1.9505207463137053e-05, policy loss: 11.369740049897967
Experience 50, Iter 17, disc loss: 1.9686709150093364e-05, policy loss: 11.536093573164813
Experience 50, Iter 18, disc loss: 2.0692322234157635e-05, policy loss: 11.391558055969988
Experience 50, Iter 19, disc loss: 1.7375917780524914e-05, policy loss: 11.541619051586133
Experience 50, Iter 20, disc loss: 1.877033366590636e-05, policy loss: 11.595949651861792
Experience 50, Iter 21, disc loss: 1.9290838896062923e-05, policy loss: 11.44419139189168
Experience 50, Iter 22, disc loss: 1.6987568542126076e-05, policy loss: 11.573456464509423
Experience 50, Iter 23, disc loss: 2.1169238554076283e-05, policy loss: 11.352122251412613
Experience 50, Iter 24, disc loss: 1.8016222230629317e-05, policy loss: 11.401841934176856
Experience 50, Iter 25, disc loss: 2.1663851610723213e-05, policy loss: 11.31844475471528
Experience 50, Iter 26, disc loss: 1.94327833646435e-05, policy loss: 11.396263406242905
Experience 50, Iter 27, disc loss: 1.8215271720782333e-05, policy loss: 11.43317591554064
Experience 50, Iter 28, disc loss: 2.5869654595358992e-05, policy loss: 11.236055959334976
Experience 50, Iter 29, disc loss: 1.996018486448595e-05, policy loss: 11.31333076598591
Experience 50, Iter 30, disc loss: 2.0711755043933194e-05, policy loss: 11.462920160477978
Experience 50, Iter 31, disc loss: 1.915450887481962e-05, policy loss: 11.603380417385264
Experience 50, Iter 32, disc loss: 2.06619693994621e-05, policy loss: 11.44574066467037
Experience 50, Iter 33, disc loss: 2.011872725078372e-05, policy loss: 11.364691765383066
Experience 50, Iter 34, disc loss: 1.7813492544768877e-05, policy loss: 11.410393983430282
Experience 50, Iter 35, disc loss: 1.9702669978809558e-05, policy loss: 11.38908933609887
Experience 50, Iter 36, disc loss: 1.8093969842501513e-05, policy loss: 11.435674001591554
Experience 50, Iter 37, disc loss: 2.128146471097417e-05, policy loss: 11.376722979001885
Experience 50, Iter 38, disc loss: 2.262339029128062e-05, policy loss: 11.285692840711137
Experience 50, Iter 39, disc loss: 2.0848184741482702e-05, policy loss: 11.43203805877487
Experience 50, Iter 40, disc loss: 2.046595205592309e-05, policy loss: 11.399723555262598
Experience 50, Iter 41, disc loss: 2.7977930418687238e-05, policy loss: 11.028575620000108
Experience 50, Iter 42, disc loss: 2.032241142814474e-05, policy loss: 11.432961713648247
Experience 50, Iter 43, disc loss: 2.2991769691661776e-05, policy loss: 11.365819059669974
Experience 50, Iter 44, disc loss: 1.942281492910433e-05, policy loss: 11.667438704720515
Experience 50, Iter 45, disc loss: 1.83204660604831e-05, policy loss: 11.563531700729833
Experience 50, Iter 46, disc loss: 1.8526558156999652e-05, policy loss: 11.528533496652754
Experience 50, Iter 47, disc loss: 1.9249268944177257e-05, policy loss: 11.418804142822422
Experience 50, Iter 48, disc loss: 1.918630981321948e-05, policy loss: 11.403769517730286
Experience 50, Iter 49, disc loss: 1.9797592687341595e-05, policy loss: 11.344658683029412
Experience 50, Iter 50, disc loss: 1.9457640328194446e-05, policy loss: 11.40243479105444
Experience 50, Iter 51, disc loss: 2.0762257659795916e-05, policy loss: 11.391019914691798
Experience 50, Iter 52, disc loss: 2.0602535482451725e-05, policy loss: 11.333631489847724
Experience 50, Iter 53, disc loss: 1.9092616055477315e-05, policy loss: 11.489486839604648
Experience 50, Iter 54, disc loss: 2.7125104773133385e-05, policy loss: 11.163558942885548
Experience 50, Iter 55, disc loss: 1.7041377811026047e-05, policy loss: 11.695083332360396
Experience 50, Iter 56, disc loss: 1.942823311474813e-05, policy loss: 11.420782932087114
Experience 50, Iter 57, disc loss: 1.868163040408707e-05, policy loss: 11.330388521134283
Experience 50, Iter 58, disc loss: 1.9210072012308454e-05, policy loss: 11.396418415800355
Experience 50, Iter 59, disc loss: 1.7682240791468637e-05, policy loss: 11.309042426498939
Experience 50, Iter 60, disc loss: 2.1747356527421403e-05, policy loss: 11.300190948577109
Experience 50, Iter 61, disc loss: 2.073006618106099e-05, policy loss: 11.381942676616008
Experience 50, Iter 62, disc loss: 1.8609936124065015e-05, policy loss: 11.424470118330536
Experience 50, Iter 63, disc loss: 2.0957924102068758e-05, policy loss: 11.284574616420667
Experience 50, Iter 64, disc loss: 1.9569790055345576e-05, policy loss: 11.520355615375149
Experience 50, Iter 65, disc loss: 1.892243342583007e-05, policy loss: 11.45099278962998
Experience 50, Iter 66, disc loss: 1.736526900002431e-05, policy loss: 11.515998689821153
Experience 50, Iter 67, disc loss: 1.7296842433218584e-05, policy loss: 11.560418814482793
Experience 50, Iter 68, disc loss: 2.042157172638401e-05, policy loss: 11.218599886319705
Experience 50, Iter 69, disc loss: 2.2381509790657792e-05, policy loss: 11.145505885152414
Experience 50, Iter 70, disc loss: 2.1157629285711608e-05, policy loss: 11.334244447573369
Experience 50, Iter 71, disc loss: 2.218315140662159e-05, policy loss: 11.365066669464085
Experience 50, Iter 72, disc loss: 2.1012486905183613e-05, policy loss: 11.409932484708891
Experience 50, Iter 73, disc loss: 1.9428523149995594e-05, policy loss: 11.476946227584765
Experience 50, Iter 74, disc loss: 2.0205247888242248e-05, policy loss: 11.554946941401349
Experience 50, Iter 75, disc loss: 2.512275361564719e-05, policy loss: 11.150756641185101
Experience 50, Iter 76, disc loss: 2.1213948788364354e-05, policy loss: 11.222324395888439
Experience 50, Iter 77, disc loss: 1.6895706684731332e-05, policy loss: 11.558920515325296
Experience 50, Iter 78, disc loss: 1.966797835981603e-05, policy loss: 11.283577150827739
Experience 50, Iter 79, disc loss: 1.7032911384816847e-05, policy loss: 11.693715679235398
Experience 50, Iter 80, disc loss: 2.1367161127424246e-05, policy loss: 11.253790563449279
Experience 50, Iter 81, disc loss: 1.9234666908431395e-05, policy loss: 11.539294520562368
Experience 50, Iter 82, disc loss: 1.924784998004883e-05, policy loss: 11.467615561383088
Experience 50, Iter 83, disc loss: 2.235183976427104e-05, policy loss: 11.416254597802132
Experience 50, Iter 84, disc loss: 1.9757791162773306e-05, policy loss: 11.51142994885029
Experience 50, Iter 85, disc loss: 1.9926777351371e-05, policy loss: 11.445721656651038
Experience 50, Iter 86, disc loss: 1.9610117158454725e-05, policy loss: 11.429507150437622
Experience 50, Iter 87, disc loss: 1.9662372065867074e-05, policy loss: 11.329997455974711
Experience 50, Iter 88, disc loss: 1.6709689875902046e-05, policy loss: 11.673941094348013
Experience 50, Iter 89, disc loss: 2.0853224522946196e-05, policy loss: 11.157939388198688
Experience 50, Iter 90, disc loss: 1.8712978720855523e-05, policy loss: 11.49297183586036
Experience 50, Iter 91, disc loss: 2.1710444121424794e-05, policy loss: 11.388883955026039
Experience 50, Iter 92, disc loss: 1.924924257421373e-05, policy loss: 11.511531855580142
Experience 50, Iter 93, disc loss: 1.8771660716586877e-05, policy loss: 11.416326804952785
Experience 50, Iter 94, disc loss: 1.9506777055498135e-05, policy loss: 11.384336273813465
Experience 50, Iter 95, disc loss: 2.097916055324878e-05, policy loss: 11.309027539022066
Experience 50, Iter 96, disc loss: 2.032939954450079e-05, policy loss: 11.396714742599093
Experience 50, Iter 97, disc loss: 1.8280608237196626e-05, policy loss: 11.48320310477551
Experience 50, Iter 98, disc loss: 2.233326975530674e-05, policy loss: 11.34002318540582
Experience 50, Iter 99, disc loss: 1.645809447578177e-05, policy loss: 11.589665549364014
