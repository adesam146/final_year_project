Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0013],
        [0.0603],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0053, 0.2413, 0.0035], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0053, 0.2413, 0.0035])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.201
Iter 2/2000 - Loss: -1.418
Iter 3/2000 - Loss: -2.913
Iter 4/2000 - Loss: -2.243
Iter 5/2000 - Loss: -1.586
Iter 6/2000 - Loss: -1.755
Iter 7/2000 - Loss: -2.255
Iter 8/2000 - Loss: -2.606
Iter 9/2000 - Loss: -2.695
Iter 10/2000 - Loss: -2.662
Iter 11/2000 - Loss: -2.677
Iter 12/2000 - Loss: -2.782
Iter 13/2000 - Loss: -2.906
Iter 14/2000 - Loss: -2.958
Iter 15/2000 - Loss: -2.906
Iter 16/2000 - Loss: -2.796
Iter 17/2000 - Loss: -2.715
Iter 18/2000 - Loss: -2.737
Iter 19/2000 - Loss: -2.868
Iter 20/2000 - Loss: -3.045
Iter 1981/2000 - Loss: -3.558
Iter 1982/2000 - Loss: -3.560
Iter 1983/2000 - Loss: -3.560
Iter 1984/2000 - Loss: -3.558
Iter 1985/2000 - Loss: -3.557
Iter 1986/2000 - Loss: -3.558
Iter 1987/2000 - Loss: -3.560
Iter 1988/2000 - Loss: -3.560
Iter 1989/2000 - Loss: -3.559
Iter 1990/2000 - Loss: -3.559
Iter 1991/2000 - Loss: -3.560
Iter 1992/2000 - Loss: -3.560
Iter 1993/2000 - Loss: -3.559
Iter 1994/2000 - Loss: -3.559
Iter 1995/2000 - Loss: -3.559
Iter 1996/2000 - Loss: -3.560
Iter 1997/2000 - Loss: -3.560
Iter 1998/2000 - Loss: -3.560
Iter 1999/2000 - Loss: -3.559
Iter 2000/2000 - Loss: -3.560
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0009],
        [0.0415],
        [0.0006]])
Lengthscale: tensor([[[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]],

        [[9.9912e-03, 3.3104e-02, 4.0000e-02, 8.5184e-04, 3.1756e-05,
          5.7800e-03]]])
Signal Variance: tensor([0.0024, 0.0038, 0.1756, 0.0025])
Estimated target variance: tensor([0.0033, 0.0053, 0.2413, 0.0035])
N: 10
Signal to noise ratio: tensor([2.0013, 2.0019, 2.0563, 2.0010])
Bound on condition number: tensor([41.0517, 41.0770, 43.2823, 41.0396])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4259261974061244, policy loss: 0.6533389388195333
Experience 1, Iter 1, disc loss: 1.4160712464787828, policy loss: 0.6557766035996584
Experience 1, Iter 2, disc loss: 1.4049703782062513, policy loss: 0.6589739604454853
Experience 1, Iter 3, disc loss: 1.394162182688358, policy loss: 0.6619756137427716
Experience 1, Iter 4, disc loss: 1.3861013878663546, policy loss: 0.6626756774060707
Experience 1, Iter 5, disc loss: 1.3742552986936065, policy loss: 0.6669235117010012
Experience 1, Iter 6, disc loss: 1.3648274014547679, policy loss: 0.669404159390873
Experience 1, Iter 7, disc loss: 1.35508209491406, policy loss: 0.6722014176170192
Experience 1, Iter 8, disc loss: 1.348507024872465, policy loss: 0.6729348325509404
Experience 1, Iter 9, disc loss: 1.344357521637967, policy loss: 0.6725869430106817
Experience 1, Iter 10, disc loss: 1.3356908219789787, policy loss: 0.6765208276683249
Experience 1, Iter 11, disc loss: 1.330822746767398, policy loss: 0.6766948175950533
Experience 1, Iter 12, disc loss: 1.3228528331569416, policy loss: 0.679291062864325
Experience 1, Iter 13, disc loss: 1.3181249228662881, policy loss: 0.6790899417270994
Experience 1, Iter 14, disc loss: 1.3111831630453537, policy loss: 0.6808065608252217
Experience 1, Iter 15, disc loss: 1.3055358119581517, policy loss: 0.6813527826482145
Experience 1, Iter 16, disc loss: 1.2983682850483977, policy loss: 0.6832329453189381
Experience 1, Iter 17, disc loss: 1.2916967889889168, policy loss: 0.6845987085983696
Experience 1, Iter 18, disc loss: 1.2859914261527643, policy loss: 0.684937299330366
Experience 1, Iter 19, disc loss: 1.2812880216589728, policy loss: 0.6840852097904518
Experience 1, Iter 20, disc loss: 1.2741668739214251, policy loss: 0.685756883620691
Experience 1, Iter 21, disc loss: 1.2691457677094102, policy loss: 0.6855682897190014
Experience 1, Iter 22, disc loss: 1.2604386266963465, policy loss: 0.689485417539046
Experience 1, Iter 23, disc loss: 1.2565631780370847, policy loss: 0.6883908740204967
Experience 1, Iter 24, disc loss: 1.250302577269419, policy loss: 0.6897423716096028
Experience 1, Iter 25, disc loss: 1.242774813592272, policy loss: 0.6923579563533431
Experience 1, Iter 26, disc loss: 1.2348326180447113, policy loss: 0.6952700084873411
Experience 1, Iter 27, disc loss: 1.232102499623972, policy loss: 0.6926483477083216
Experience 1, Iter 28, disc loss: 1.2231729641669435, policy loss: 0.6963351517285286
Experience 1, Iter 29, disc loss: 1.2166872606659043, policy loss: 0.6972720720561603
Experience 1, Iter 30, disc loss: 1.2111336800575871, policy loss: 0.6973450461105937
Experience 1, Iter 31, disc loss: 1.2007255538305186, policy loss: 0.7021593684673246
Experience 1, Iter 32, disc loss: 1.192392580252112, policy loss: 0.7046940021976048
Experience 1, Iter 33, disc loss: 1.1875741875917423, policy loss: 0.7035191227901009
Experience 1, Iter 34, disc loss: 1.1771414273383587, policy loss: 0.7082016614760023
Experience 1, Iter 35, disc loss: 1.169133155217833, policy loss: 0.7101774395752142
Experience 1, Iter 36, disc loss: 1.161929946868561, policy loss: 0.7113192719466913
Experience 1, Iter 37, disc loss: 1.1520438029881679, policy loss: 0.715112277696171
Experience 1, Iter 38, disc loss: 1.1455439944065549, policy loss: 0.7154560930696066
Experience 1, Iter 39, disc loss: 1.1370879526734206, policy loss: 0.7175552518767072
Experience 1, Iter 40, disc loss: 1.125528691236266, policy loss: 0.72325917464595
Experience 1, Iter 41, disc loss: 1.1135296432126136, policy loss: 0.7291806679994222
Experience 1, Iter 42, disc loss: 1.101087661002545, policy loss: 0.7354195018065726
Experience 1, Iter 43, disc loss: 1.0910357875632777, policy loss: 0.7390891856252462
Experience 1, Iter 44, disc loss: 1.0789998402453473, policy loss: 0.7447435053527252
Experience 1, Iter 45, disc loss: 1.0657184181592598, policy loss: 0.7518410478086419
Experience 1, Iter 46, disc loss: 1.0592700494364302, policy loss: 0.7512297710096352
Experience 1, Iter 47, disc loss: 1.0462545609187945, policy loss: 0.7578562515404003
Experience 1, Iter 48, disc loss: 1.031260764029525, policy loss: 0.766445638081856
Experience 1, Iter 49, disc loss: 1.0170429306830546, policy loss: 0.7748105324630059
Experience 1, Iter 50, disc loss: 1.0099873001085107, policy loss: 0.7742257845915168
Experience 1, Iter 51, disc loss: 0.9860698753242725, policy loss: 0.7928074306786546
Experience 1, Iter 52, disc loss: 0.9766914601202539, policy loss: 0.7938006344732194
Experience 1, Iter 53, disc loss: 0.953803774991211, policy loss: 0.8101206529403377
Experience 1, Iter 54, disc loss: 0.9406231646728309, policy loss: 0.8149208844971036
Experience 1, Iter 55, disc loss: 0.9223476331507049, policy loss: 0.8250699285801024
Experience 1, Iter 56, disc loss: 0.8993321235758719, policy loss: 0.8422353805678704
Experience 1, Iter 57, disc loss: 0.8787780787690448, policy loss: 0.8559026035545637
Experience 1, Iter 58, disc loss: 0.8660637876801425, policy loss: 0.859057407804299
Experience 1, Iter 59, disc loss: 0.844525787521744, policy loss: 0.8744467897072151
Experience 1, Iter 60, disc loss: 0.8346378747129839, policy loss: 0.8738623064853663
Experience 1, Iter 61, disc loss: 0.802303565300092, policy loss: 0.9060119885614857
Experience 1, Iter 62, disc loss: 0.7857300703162948, policy loss: 0.915168952153647
Experience 1, Iter 63, disc loss: 0.7663305161543486, policy loss: 0.9295783932086861
Experience 1, Iter 64, disc loss: 0.7370887529660195, policy loss: 0.9587574811039616
Experience 1, Iter 65, disc loss: 0.7275982368903463, policy loss: 0.9592748174249197
Experience 1, Iter 66, disc loss: 0.7111197030861248, policy loss: 0.970971260972997
Experience 1, Iter 67, disc loss: 0.6863339268650572, policy loss: 0.9974123833932392
Experience 1, Iter 68, disc loss: 0.660407349070033, policy loss: 1.0259621428547208
Experience 1, Iter 69, disc loss: 0.6500842156724678, policy loss: 1.0322182081552018
Experience 1, Iter 70, disc loss: 0.6234567188905245, policy loss: 1.0633728948705907
Experience 1, Iter 71, disc loss: 0.5969563553579339, policy loss: 1.0981654297205052
Experience 1, Iter 72, disc loss: 0.5813048638545797, policy loss: 1.1127468842722017
Experience 1, Iter 73, disc loss: 0.5729329443363933, policy loss: 1.1179662198542362
Experience 1, Iter 74, disc loss: 0.5477887618051441, policy loss: 1.1494275870886383
Experience 1, Iter 75, disc loss: 0.5288209197716508, policy loss: 1.1769781289466796
Experience 1, Iter 76, disc loss: 0.5180694807404724, policy loss: 1.187435207800923
Experience 1, Iter 77, disc loss: 0.4949132515970769, policy loss: 1.2258657102758912
Experience 1, Iter 78, disc loss: 0.48719905978946854, policy loss: 1.2302985228420433
Experience 1, Iter 79, disc loss: 0.4558853309841812, policy loss: 1.290401263528453
Experience 1, Iter 80, disc loss: 0.43920959940677856, policy loss: 1.3198006489367087
Experience 1, Iter 81, disc loss: 0.4248364416323279, policy loss: 1.3492928823707517
Experience 1, Iter 82, disc loss: 0.4220082290927192, policy loss: 1.3430623346188084
Experience 1, Iter 83, disc loss: 0.3960295800562542, policy loss: 1.402887735655343
Experience 1, Iter 84, disc loss: 0.38814413835082073, policy loss: 1.412177812596671
Experience 1, Iter 85, disc loss: 0.3625242655702799, policy loss: 1.4847780684520098
Experience 1, Iter 86, disc loss: 0.3512927558588632, policy loss: 1.503600235204677
Experience 1, Iter 87, disc loss: 0.3534932185411793, policy loss: 1.4912014968348781
Experience 1, Iter 88, disc loss: 0.31999801664538285, policy loss: 1.5870400065720207
Experience 1, Iter 89, disc loss: 0.32081813956420396, policy loss: 1.5694439075270767
Experience 1, Iter 90, disc loss: 0.304475706738647, policy loss: 1.626108208354609
Experience 1, Iter 91, disc loss: 0.2901855330407608, policy loss: 1.6827065267444363
Experience 1, Iter 92, disc loss: 0.2798959942761553, policy loss: 1.6977651875234732
Experience 1, Iter 93, disc loss: 0.2597207930407931, policy loss: 1.777160904971264
Experience 1, Iter 94, disc loss: 0.2536123198687406, policy loss: 1.794076808916918
Experience 1, Iter 95, disc loss: 0.24144394238980726, policy loss: 1.839718978652813
Experience 1, Iter 96, disc loss: 0.24800275880405287, policy loss: 1.819286515200175
Experience 1, Iter 97, disc loss: 0.23618852213831137, policy loss: 1.8480194782223596
Experience 1, Iter 98, disc loss: 0.2126660326617254, policy loss: 1.976973052820591
Experience 1, Iter 99, disc loss: 0.2051825436093254, policy loss: 1.9856783418364181
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0027],
        [0.1165],
        [0.0018]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0109, 0.4658, 0.0070], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0109, 0.4658, 0.0070])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.822
Iter 2/2000 - Loss: -1.224
Iter 3/2000 - Loss: -1.787
Iter 4/2000 - Loss: -1.428
Iter 5/2000 - Loss: -1.247
Iter 6/2000 - Loss: -1.419
Iter 7/2000 - Loss: -1.631
Iter 8/2000 - Loss: -1.698
Iter 9/2000 - Loss: -1.660
Iter 10/2000 - Loss: -1.652
Iter 11/2000 - Loss: -1.751
Iter 12/2000 - Loss: -1.917
Iter 13/2000 - Loss: -2.046
Iter 14/2000 - Loss: -2.064
Iter 15/2000 - Loss: -1.973
Iter 16/2000 - Loss: -1.855
Iter 17/2000 - Loss: -1.811
Iter 18/2000 - Loss: -1.882
Iter 19/2000 - Loss: -2.024
Iter 20/2000 - Loss: -2.146
Iter 1981/2000 - Loss: -2.398
Iter 1982/2000 - Loss: -2.398
Iter 1983/2000 - Loss: -2.398
Iter 1984/2000 - Loss: -2.398
Iter 1985/2000 - Loss: -2.398
Iter 1986/2000 - Loss: -2.398
Iter 1987/2000 - Loss: -2.398
Iter 1988/2000 - Loss: -2.398
Iter 1989/2000 - Loss: -2.398
Iter 1990/2000 - Loss: -2.398
Iter 1991/2000 - Loss: -2.398
Iter 1992/2000 - Loss: -2.397
Iter 1993/2000 - Loss: -2.397
Iter 1994/2000 - Loss: -2.397
Iter 1995/2000 - Loss: -2.397
Iter 1996/2000 - Loss: -2.397
Iter 1997/2000 - Loss: -2.398
Iter 1998/2000 - Loss: -2.398
Iter 1999/2000 - Loss: -2.397
Iter 2000/2000 - Loss: -2.397
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0021],
        [0.0825],
        [0.0013]])
Lengthscale: tensor([[[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]],

        [[1.1132e-02, 3.3272e-02, 8.0444e-02, 1.8815e-03, 8.3428e-05,
          8.3704e-03]]])
Signal Variance: tensor([0.0025, 0.0083, 0.3616, 0.0054])
Estimated target variance: tensor([0.0033, 0.0109, 0.4658, 0.0070])
N: 20
Signal to noise ratio: tensor([2.0010, 2.0042, 2.0941, 2.0019])
Bound on condition number: tensor([81.0819, 81.3353, 88.7012, 81.1541])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.1946234602064615, policy loss: 2.1072605948374252
Experience 2, Iter 1, disc loss: 0.19389938730989575, policy loss: 2.1198680213424366
Experience 2, Iter 2, disc loss: 0.17037916047831075, policy loss: 2.22534890428323
Experience 2, Iter 3, disc loss: 0.16492221507140073, policy loss: 2.2654660925528574
Experience 2, Iter 4, disc loss: 0.15747378347853805, policy loss: 2.3408725180104026
Experience 2, Iter 5, disc loss: 0.14645071226269096, policy loss: 2.40320112211021
Experience 2, Iter 6, disc loss: 0.15035277573186065, policy loss: 2.406975449313097
Experience 2, Iter 7, disc loss: 0.14663350320760027, policy loss: 2.390229477745656
Experience 2, Iter 8, disc loss: 0.12403382037211355, policy loss: 2.6256631083714934
Experience 2, Iter 9, disc loss: 0.13765566558167808, policy loss: 2.474865582461258
Experience 2, Iter 10, disc loss: 0.12399164837550115, policy loss: 2.622093274337961
Experience 2, Iter 11, disc loss: 0.13041957719179526, policy loss: 2.5301203149218416
Experience 2, Iter 12, disc loss: 0.12008527923504836, policy loss: 2.6574177175917906
Experience 2, Iter 13, disc loss: 0.12348253401115541, policy loss: 2.558538238179135
Experience 2, Iter 14, disc loss: 0.10597969055712488, policy loss: 2.7758407405945036
Experience 2, Iter 15, disc loss: 0.11264808251136645, policy loss: 2.720687070729832
Experience 2, Iter 16, disc loss: 0.11306269753982692, policy loss: 2.7072109809667007
Experience 2, Iter 17, disc loss: 0.10168633191133794, policy loss: 2.789989344876025
Experience 2, Iter 18, disc loss: 0.10232800462019256, policy loss: 2.916534411932057
Experience 2, Iter 19, disc loss: 0.09436473017173574, policy loss: 2.9600881447467797
Experience 2, Iter 20, disc loss: 0.08387242011129775, policy loss: 3.07668103529221
Experience 2, Iter 21, disc loss: 0.08613739756728166, policy loss: 3.0485502195830927
Experience 2, Iter 22, disc loss: 0.07655722437421414, policy loss: 3.159128398278496
Experience 2, Iter 23, disc loss: 0.0792582477017638, policy loss: 3.1543286472768832
Experience 2, Iter 24, disc loss: 0.07781877634048884, policy loss: 3.177517066815863
Experience 2, Iter 25, disc loss: 0.06494511536536274, policy loss: 3.315327987619768
Experience 2, Iter 26, disc loss: 0.07167927646305014, policy loss: 3.2087666945206283
Experience 2, Iter 27, disc loss: 0.07701206781418743, policy loss: 3.15387759267292
Experience 2, Iter 28, disc loss: 0.06779325880939138, policy loss: 3.3438493828905616
Experience 2, Iter 29, disc loss: 0.06779827689218035, policy loss: 3.243925132723292
Experience 2, Iter 30, disc loss: 0.0763359567056263, policy loss: 3.1470154221664757
Experience 2, Iter 31, disc loss: 0.06305669416527385, policy loss: 3.364426431523915
Experience 2, Iter 32, disc loss: 0.057879663680311694, policy loss: 3.4978931787574825
Experience 2, Iter 33, disc loss: 0.07518653502189043, policy loss: 3.4311131912553563
Experience 2, Iter 34, disc loss: 0.05411800445394091, policy loss: 3.4892980348302167
Experience 2, Iter 35, disc loss: 0.056960806332910834, policy loss: 3.48314764320731
Experience 2, Iter 36, disc loss: 0.06122976604081483, policy loss: 3.3896963014668104
Experience 2, Iter 37, disc loss: 0.05396788469410886, policy loss: 3.577473562165106
Experience 2, Iter 38, disc loss: 0.05737542455041955, policy loss: 3.579389472057961
Experience 2, Iter 39, disc loss: 0.06047353953102415, policy loss: 3.475005278078969
Experience 2, Iter 40, disc loss: 0.0493897285447073, policy loss: 3.6041294827897308
Experience 2, Iter 41, disc loss: 0.04456038600661639, policy loss: 3.7305390819742934
Experience 2, Iter 42, disc loss: 0.047272514341548826, policy loss: 3.741014185198866
Experience 2, Iter 43, disc loss: 0.04933150402681123, policy loss: 3.754382590086788
Experience 2, Iter 44, disc loss: 0.04624008476155529, policy loss: 3.737057847905378
Experience 2, Iter 45, disc loss: 0.041695993157046855, policy loss: 3.858985993372401
Experience 2, Iter 46, disc loss: 0.04685951760808886, policy loss: 3.826769557023542
Experience 2, Iter 47, disc loss: 0.04469572798647872, policy loss: 3.81938779345979
Experience 2, Iter 48, disc loss: 0.04333613597882486, policy loss: 3.911161926651516
Experience 2, Iter 49, disc loss: 0.04276545536510211, policy loss: 3.839532636773335
Experience 2, Iter 50, disc loss: 0.0408614235085459, policy loss: 3.9972560320548767
Experience 2, Iter 51, disc loss: 0.037936499994251305, policy loss: 3.9245252474553856
Experience 2, Iter 52, disc loss: 0.045684088825048604, policy loss: 3.75888082839718
Experience 2, Iter 53, disc loss: 0.03677851582137036, policy loss: 3.9277935851726564
Experience 2, Iter 54, disc loss: 0.039100369115812945, policy loss: 3.8942413577786033
Experience 2, Iter 55, disc loss: 0.035718132825259125, policy loss: 4.063646392090949
Experience 2, Iter 56, disc loss: 0.03564841967510331, policy loss: 4.125971793117536
Experience 2, Iter 57, disc loss: 0.03562023686158369, policy loss: 4.091971001628666
Experience 2, Iter 58, disc loss: 0.03498036980423534, policy loss: 3.985915137795225
Experience 2, Iter 59, disc loss: 0.030436366847924674, policy loss: 4.08275557216
Experience 2, Iter 60, disc loss: 0.03296328475248599, policy loss: 4.165259913029467
Experience 2, Iter 61, disc loss: 0.02830153545639634, policy loss: 4.287458694312687
Experience 2, Iter 62, disc loss: 0.02993549848962624, policy loss: 4.253773566210634
Experience 2, Iter 63, disc loss: 0.03177670602805323, policy loss: 4.096729273251342
Experience 2, Iter 64, disc loss: 0.03146914430866293, policy loss: 4.097788099760597
Experience 2, Iter 65, disc loss: 0.03127855252945988, policy loss: 4.13063321935604
Experience 2, Iter 66, disc loss: 0.02989401430217737, policy loss: 4.176950811454275
Experience 2, Iter 67, disc loss: 0.032189220611314316, policy loss: 4.115001777800678
Experience 2, Iter 68, disc loss: 0.029197774672496, policy loss: 4.25368109626018
Experience 2, Iter 69, disc loss: 0.028355050097215992, policy loss: 4.27375863536866
Experience 2, Iter 70, disc loss: 0.028376601105567324, policy loss: 4.260273934793414
Experience 2, Iter 71, disc loss: 0.024483465305716205, policy loss: 4.351717285596406
Experience 2, Iter 72, disc loss: 0.024335322677454224, policy loss: 4.431476542934991
Experience 2, Iter 73, disc loss: 0.027730536760311956, policy loss: 4.3803693428612185
Experience 2, Iter 74, disc loss: 0.026652413199386542, policy loss: 4.364690895170013
Experience 2, Iter 75, disc loss: 0.023186376837040893, policy loss: 4.620994405522037
Experience 2, Iter 76, disc loss: 0.022462071924184368, policy loss: 4.547568199978812
Experience 2, Iter 77, disc loss: 0.0236633249208109, policy loss: 4.443921417216187
Experience 2, Iter 78, disc loss: 0.022917595037053134, policy loss: 4.547617746176364
Experience 2, Iter 79, disc loss: 0.020929389073201136, policy loss: 4.509841283139936
Experience 2, Iter 80, disc loss: 0.020998968834556132, policy loss: 4.604359773164939
Experience 2, Iter 81, disc loss: 0.02693953960672019, policy loss: 4.394286466891392
Experience 2, Iter 82, disc loss: 0.02716404670546781, policy loss: 4.34274398735492
Experience 2, Iter 83, disc loss: 0.020502230003923788, policy loss: 4.728743717834997
Experience 2, Iter 84, disc loss: 0.02565663447648253, policy loss: 4.392026910290376
Experience 2, Iter 85, disc loss: 0.02116639707058047, policy loss: 4.557581224865583
Experience 2, Iter 86, disc loss: 0.01794826341052755, policy loss: 4.785491449221629
Experience 2, Iter 87, disc loss: 0.02198938585638426, policy loss: 4.65308217180389
Experience 2, Iter 88, disc loss: 0.02210692180164492, policy loss: 4.546186562243856
Experience 2, Iter 89, disc loss: 0.017330060924519657, policy loss: 4.834590138080732
Experience 2, Iter 90, disc loss: 0.016506558037077804, policy loss: 4.830701032982312
Experience 2, Iter 91, disc loss: 0.019428908372042492, policy loss: 4.8301575702709805
Experience 2, Iter 92, disc loss: 0.021511415893517367, policy loss: 4.683116897789552
Experience 2, Iter 93, disc loss: 0.017870449363835292, policy loss: 4.709693382116901
Experience 2, Iter 94, disc loss: 0.01795702321204521, policy loss: 4.747999636359403
Experience 2, Iter 95, disc loss: 0.015950609777054134, policy loss: 4.989460680463727
Experience 2, Iter 96, disc loss: 0.018565748659568226, policy loss: 4.659664640001591
Experience 2, Iter 97, disc loss: 0.014672293609218836, policy loss: 5.054285401222357
Experience 2, Iter 98, disc loss: 0.017675263789536322, policy loss: 4.841398748258193
Experience 2, Iter 99, disc loss: 0.016571441059670542, policy loss: 4.923053715309186
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0037],
        [0.1939],
        [0.0042]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0031, 0.0149, 0.7758, 0.0168], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0031, 0.0149, 0.7758, 0.0168])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.192
Iter 2/2000 - Loss: -0.966
Iter 3/2000 - Loss: -0.985
Iter 4/2000 - Loss: -0.596
Iter 5/2000 - Loss: -0.606
Iter 6/2000 - Loss: -0.865
Iter 7/2000 - Loss: -1.059
Iter 8/2000 - Loss: -1.070
Iter 9/2000 - Loss: -0.986
Iter 10/2000 - Loss: -0.965
Iter 11/2000 - Loss: -1.061
Iter 12/2000 - Loss: -1.195
Iter 13/2000 - Loss: -1.270
Iter 14/2000 - Loss: -1.254
Iter 15/2000 - Loss: -1.189
Iter 16/2000 - Loss: -1.142
Iter 17/2000 - Loss: -1.148
Iter 18/2000 - Loss: -1.202
Iter 19/2000 - Loss: -1.281
Iter 20/2000 - Loss: -1.358
Iter 1981/2000 - Loss: -1.559
Iter 1982/2000 - Loss: -1.554
Iter 1983/2000 - Loss: -1.547
Iter 1984/2000 - Loss: -1.549
Iter 1985/2000 - Loss: -1.556
Iter 1986/2000 - Loss: -1.559
Iter 1987/2000 - Loss: -1.554
Iter 1988/2000 - Loss: -1.553
Iter 1989/2000 - Loss: -1.558
Iter 1990/2000 - Loss: -1.558
Iter 1991/2000 - Loss: -1.555
Iter 1992/2000 - Loss: -1.556
Iter 1993/2000 - Loss: -1.558
Iter 1994/2000 - Loss: -1.559
Iter 1995/2000 - Loss: -1.556
Iter 1996/2000 - Loss: -1.556
Iter 1997/2000 - Loss: -1.558
Iter 1998/2000 - Loss: -1.559
Iter 1999/2000 - Loss: -1.558
Iter 2000/2000 - Loss: -1.557
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0029],
        [0.1337],
        [0.0032]])
Lengthscale: tensor([[[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]],

        [[0.0133, 0.0316, 0.1855, 0.0040, 0.0002, 0.0407]]])
Signal Variance: tensor([0.0024, 0.0115, 0.6157, 0.0130])
Estimated target variance: tensor([0.0031, 0.0149, 0.7758, 0.0168])
N: 30
Signal to noise ratio: tensor([2.0010, 2.0050, 2.1457, 2.0063])
Bound on condition number: tensor([121.1206, 121.6063, 139.1151, 121.7550])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.02977554926308763, policy loss: 4.482071582389078
Experience 3, Iter 1, disc loss: 0.02246146988478063, policy loss: 4.75624450507118
Experience 3, Iter 2, disc loss: 0.030864421826857433, policy loss: 4.411086880112464
Experience 3, Iter 3, disc loss: 0.022251958758146517, policy loss: 4.763341241854265
Experience 3, Iter 4, disc loss: 0.030741597867781217, policy loss: 4.534908454739595
Experience 3, Iter 5, disc loss: 0.024031457062885005, policy loss: 4.75580582475053
Experience 3, Iter 6, disc loss: 0.019049608581015072, policy loss: 4.779809388565253
Experience 3, Iter 7, disc loss: 0.01724622995419208, policy loss: 4.941934399035166
Experience 3, Iter 8, disc loss: 0.022916420983699885, policy loss: 4.7027892036239685
Experience 3, Iter 9, disc loss: 0.01673387741662385, policy loss: 4.921332888570922
Experience 3, Iter 10, disc loss: 0.024508514155853183, policy loss: 4.662385095701415
Experience 3, Iter 11, disc loss: 0.022513419000140124, policy loss: 5.018588043306281
Experience 3, Iter 12, disc loss: 0.020960366936445683, policy loss: 4.825867836429973
Experience 3, Iter 13, disc loss: 0.016822388099229056, policy loss: 4.8910846542689
Experience 3, Iter 14, disc loss: 0.01684732874753778, policy loss: 4.971782656397598
Experience 3, Iter 15, disc loss: 0.01616767019596229, policy loss: 4.940296019558366
Experience 3, Iter 16, disc loss: 0.020767239741312817, policy loss: 5.160898641943966
Experience 3, Iter 17, disc loss: 0.014126575703277851, policy loss: 5.022695106194015
Experience 3, Iter 18, disc loss: 0.021874719017741705, policy loss: 4.892557448714531
Experience 3, Iter 19, disc loss: 0.01811935006553913, policy loss: 4.946487446321455
Experience 3, Iter 20, disc loss: 0.013937549379816878, policy loss: 5.196055580764014
Experience 3, Iter 21, disc loss: 0.016572850703548497, policy loss: 4.972708009307944
Experience 3, Iter 22, disc loss: 0.017218618314519377, policy loss: 4.920034650006686
Experience 3, Iter 23, disc loss: 0.019751678954066067, policy loss: 5.075653295993653
Experience 3, Iter 24, disc loss: 0.013654181390002917, policy loss: 5.260503639356099
Experience 3, Iter 25, disc loss: 0.01670555478290205, policy loss: 4.939820287654061
Experience 3, Iter 26, disc loss: 0.015794062275123588, policy loss: 5.020298385898446
Experience 3, Iter 27, disc loss: 0.01629311717558468, policy loss: 4.996758699610901
Experience 3, Iter 28, disc loss: 0.02152711237692578, policy loss: 5.070240695759601
Experience 3, Iter 29, disc loss: 0.016193301227603513, policy loss: 5.138832539483785
Experience 3, Iter 30, disc loss: 0.012004295010750395, policy loss: 5.298215144216552
Experience 3, Iter 31, disc loss: 0.01477693349446908, policy loss: 5.041743363873236
Experience 3, Iter 32, disc loss: 0.01212468035052363, policy loss: 5.309383264479455
Experience 3, Iter 33, disc loss: 0.015191644551148326, policy loss: 5.223975884725126
Experience 3, Iter 34, disc loss: 0.013658468097762017, policy loss: 5.218675040171896
Experience 3, Iter 35, disc loss: 0.012159819726144877, policy loss: 5.392488540467036
Experience 3, Iter 36, disc loss: 0.01631151988524719, policy loss: 5.110201087009057
Experience 3, Iter 37, disc loss: 0.013705589431665867, policy loss: 5.116155478402232
Experience 3, Iter 38, disc loss: 0.010879720695545022, policy loss: 5.354163582381932
Experience 3, Iter 39, disc loss: 0.015139099215583634, policy loss: 5.116180215302479
Experience 3, Iter 40, disc loss: 0.01135630986888524, policy loss: 5.329343446320916
Experience 3, Iter 41, disc loss: 0.011302174611979592, policy loss: 5.470042834498922
Experience 3, Iter 42, disc loss: 0.010237124995869413, policy loss: 5.495163731415252
Experience 3, Iter 43, disc loss: 0.01689059563468882, policy loss: 5.136506141404329
Experience 3, Iter 44, disc loss: 0.0121219678137, policy loss: 5.340552373669354
Experience 3, Iter 45, disc loss: 0.010496551582002283, policy loss: 5.510687110264437
Experience 3, Iter 46, disc loss: 0.01379448147670587, policy loss: 5.708559211634192
Experience 3, Iter 47, disc loss: 0.010726061619536023, policy loss: 5.486951934431687
Experience 3, Iter 48, disc loss: 0.013006974788848642, policy loss: 5.371037154606926
Experience 3, Iter 49, disc loss: 0.010916304717998634, policy loss: 5.366322867660608
Experience 3, Iter 50, disc loss: 0.010061831490956262, policy loss: 5.465234836049798
Experience 3, Iter 51, disc loss: 0.010418916636811113, policy loss: 5.46120282361848
Experience 3, Iter 52, disc loss: 0.011103702638346119, policy loss: 5.614336940391655
Experience 3, Iter 53, disc loss: 0.01165526630820304, policy loss: 5.228952461499416
Experience 3, Iter 54, disc loss: 0.009899160688246037, policy loss: 5.456326898959539
Experience 3, Iter 55, disc loss: 0.010282309259156689, policy loss: 5.378327074659088
Experience 3, Iter 56, disc loss: 0.010648490179577221, policy loss: 5.463947371653462
Experience 3, Iter 57, disc loss: 0.01694549992622256, policy loss: 5.383833811477006
Experience 3, Iter 58, disc loss: 0.009806324373354387, policy loss: 5.593511416027352
Experience 3, Iter 59, disc loss: 0.013010671344832268, policy loss: 5.400307150266114
Experience 3, Iter 60, disc loss: 0.010239105806815387, policy loss: 5.426856014407917
Experience 3, Iter 61, disc loss: 0.009581020618051255, policy loss: 5.518554116157278
Experience 3, Iter 62, disc loss: 0.011094950890412236, policy loss: 5.426801614389939
Experience 3, Iter 63, disc loss: 0.008043161721428105, policy loss: 5.660588956124869
Experience 3, Iter 64, disc loss: 0.008258702272451264, policy loss: 5.658455575502539
Experience 3, Iter 65, disc loss: 0.008459069792151774, policy loss: 5.7266346875151335
Experience 3, Iter 66, disc loss: 0.010092394663009832, policy loss: 5.558950084498045
Experience 3, Iter 67, disc loss: 0.009503464382360676, policy loss: 5.4099089521268695
Experience 3, Iter 68, disc loss: 0.007572211232812584, policy loss: 5.751111647422598
Experience 3, Iter 69, disc loss: 0.008325209149182215, policy loss: 5.762336228980527
Experience 3, Iter 70, disc loss: 0.009133250482576204, policy loss: 5.845580026660597
Experience 3, Iter 71, disc loss: 0.007649610681513293, policy loss: 5.86556730815137
Experience 3, Iter 72, disc loss: 0.008622645346336697, policy loss: 5.590846278077523
Experience 3, Iter 73, disc loss: 0.007753788471049507, policy loss: 5.809519797422233
Experience 3, Iter 74, disc loss: 0.008150755791594804, policy loss: 5.617274347633074
Experience 3, Iter 75, disc loss: 0.007466018042605576, policy loss: 5.8671483156920985
Experience 3, Iter 76, disc loss: 0.006987463822695727, policy loss: 5.901427793163229
Experience 3, Iter 77, disc loss: 0.006954992086498921, policy loss: 5.953095922267211
Experience 3, Iter 78, disc loss: 0.007252810565918427, policy loss: 5.964906443871595
Experience 3, Iter 79, disc loss: 0.007208617957701372, policy loss: 5.815662617931275
Experience 3, Iter 80, disc loss: 0.00802772197788723, policy loss: 5.660159845801741
Experience 3, Iter 81, disc loss: 0.007350205482730221, policy loss: 5.825618345754552
Experience 3, Iter 82, disc loss: 0.006862107426200538, policy loss: 5.891032328418125
Experience 3, Iter 83, disc loss: 0.007680767212495868, policy loss: 5.735674019353182
Experience 3, Iter 84, disc loss: 0.006278723223747759, policy loss: 5.943567329880131
Experience 3, Iter 85, disc loss: 0.005552436342179921, policy loss: 6.229869096297685
Experience 3, Iter 86, disc loss: 0.006509797003646511, policy loss: 5.89117217284609
Experience 3, Iter 87, disc loss: 0.006625462827689468, policy loss: 5.960257658537954
Experience 3, Iter 88, disc loss: 0.0059281301242170535, policy loss: 6.083578667066169
Experience 3, Iter 89, disc loss: 0.007476237581692689, policy loss: 5.847898259801985
Experience 3, Iter 90, disc loss: 0.005984766989287757, policy loss: 6.040771130237163
Experience 3, Iter 91, disc loss: 0.00598953326089907, policy loss: 6.084626093952274
Experience 3, Iter 92, disc loss: 0.008168689953858204, policy loss: 5.758217067486758
Experience 3, Iter 93, disc loss: 0.006349047676391757, policy loss: 6.011641817453378
Experience 3, Iter 94, disc loss: 0.0066907491306401785, policy loss: 5.985715903339033
Experience 3, Iter 95, disc loss: 0.005628514572847675, policy loss: 6.0777572191701985
Experience 3, Iter 96, disc loss: 0.007180686975775953, policy loss: 6.088435443850585
Experience 3, Iter 97, disc loss: 0.008417793188717935, policy loss: 6.067882581176006
Experience 3, Iter 98, disc loss: 0.007854433060936895, policy loss: 5.835094952249484
Experience 3, Iter 99, disc loss: 0.00566900287706512, policy loss: 6.181446473054741
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0053],
        [0.2792],
        [0.0060]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0214, 1.1168, 0.0239], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0214, 1.1168, 0.0239])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.561
Iter 2/2000 - Loss: -0.390
Iter 3/2000 - Loss: -0.430
Iter 4/2000 - Loss: -0.105
Iter 5/2000 - Loss: -0.129
Iter 6/2000 - Loss: -0.349
Iter 7/2000 - Loss: -0.494
Iter 8/2000 - Loss: -0.477
Iter 9/2000 - Loss: -0.389
Iter 10/2000 - Loss: -0.370
Iter 11/2000 - Loss: -0.457
Iter 12/2000 - Loss: -0.585
Iter 13/2000 - Loss: -0.675
Iter 14/2000 - Loss: -0.691
Iter 15/2000 - Loss: -0.654
Iter 16/2000 - Loss: -0.602
Iter 17/2000 - Loss: -0.573
Iter 18/2000 - Loss: -0.589
Iter 19/2000 - Loss: -0.651
Iter 20/2000 - Loss: -0.736
Iter 1981/2000 - Loss: -0.928
Iter 1982/2000 - Loss: -0.928
Iter 1983/2000 - Loss: -0.928
Iter 1984/2000 - Loss: -0.928
Iter 1985/2000 - Loss: -0.928
Iter 1986/2000 - Loss: -0.928
Iter 1987/2000 - Loss: -0.928
Iter 1988/2000 - Loss: -0.928
Iter 1989/2000 - Loss: -0.928
Iter 1990/2000 - Loss: -0.928
Iter 1991/2000 - Loss: -0.928
Iter 1992/2000 - Loss: -0.928
Iter 1993/2000 - Loss: -0.928
Iter 1994/2000 - Loss: -0.928
Iter 1995/2000 - Loss: -0.928
Iter 1996/2000 - Loss: -0.928
Iter 1997/2000 - Loss: -0.928
Iter 1998/2000 - Loss: -0.928
Iter 1999/2000 - Loss: -0.928
Iter 2000/2000 - Loss: -0.928
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0041],
        [0.1871],
        [0.0046]])
Lengthscale: tensor([[[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]],

        [[0.0136, 0.0389, 0.2660, 0.0058, 0.0005, 0.0611]]])
Signal Variance: tensor([0.0028, 0.0167, 0.9018, 0.0187])
Estimated target variance: tensor([0.0036, 0.0214, 1.1168, 0.0239])
N: 40
Signal to noise ratio: tensor([2.0012, 2.0068, 2.1954, 2.0087])
Bound on condition number: tensor([161.1901, 162.0977, 193.7917, 162.3978])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.006066555029996716, policy loss: 6.473815183408979
Experience 4, Iter 1, disc loss: 0.005017198884875092, policy loss: 6.373302364748421
Experience 4, Iter 2, disc loss: 0.005033063294277506, policy loss: 6.33699164434365
Experience 4, Iter 3, disc loss: 0.0061330299896500936, policy loss: 6.226153974008245
Experience 4, Iter 4, disc loss: 0.005578824513839499, policy loss: 6.382963837640783
Experience 4, Iter 5, disc loss: 0.0058513868843619865, policy loss: 6.318997779265078
Experience 4, Iter 6, disc loss: 0.005722385513252512, policy loss: 6.272922258008902
Experience 4, Iter 7, disc loss: 0.004735381956879194, policy loss: 6.659165519216711
Experience 4, Iter 8, disc loss: 0.005481912275472253, policy loss: 6.374788351228151
Experience 4, Iter 9, disc loss: 0.004640042275881934, policy loss: 6.629331952819489
Experience 4, Iter 10, disc loss: 0.006927136453251886, policy loss: 6.286937943779301
Experience 4, Iter 11, disc loss: 0.005021072209940209, policy loss: 6.434204025216344
Experience 4, Iter 12, disc loss: 0.006403072127371071, policy loss: 6.2994451000865785
Experience 4, Iter 13, disc loss: 0.004303396804696244, policy loss: 6.71372161389756
Experience 4, Iter 14, disc loss: 0.005002927658935218, policy loss: 6.574096000549927
Experience 4, Iter 15, disc loss: 0.004810937855173582, policy loss: 6.509663926929907
Experience 4, Iter 16, disc loss: 0.0049080298990000545, policy loss: 6.416656287795631
Experience 4, Iter 17, disc loss: 0.004972224396380432, policy loss: 6.440014883070394
Experience 4, Iter 18, disc loss: 0.004834737084321986, policy loss: 6.753473726901964
Experience 4, Iter 19, disc loss: 0.004901081556828327, policy loss: 6.572188852335721
Experience 4, Iter 20, disc loss: 0.004598607893708433, policy loss: 6.5177136940699905
Experience 4, Iter 21, disc loss: 0.004009441411120184, policy loss: 6.735768546959726
Experience 4, Iter 22, disc loss: 0.004700665274305861, policy loss: 6.598972211344995
Experience 4, Iter 23, disc loss: 0.00346451121803886, policy loss: 6.941744319047145
Experience 4, Iter 24, disc loss: 0.004970349737037495, policy loss: 6.665148850440646
Experience 4, Iter 25, disc loss: 0.004816198210231755, policy loss: 6.696047252044592
Experience 4, Iter 26, disc loss: 0.004060136476851623, policy loss: 6.809073672002842
Experience 4, Iter 27, disc loss: 0.004671180117179413, policy loss: 6.675291150636221
Experience 4, Iter 28, disc loss: 0.003911686863866877, policy loss: 6.886534090136714
Experience 4, Iter 29, disc loss: 0.0075907424553320725, policy loss: 6.659153385150139
Experience 4, Iter 30, disc loss: 0.0039192007827815265, policy loss: 6.851723316145163
Experience 4, Iter 31, disc loss: 0.003877266469150721, policy loss: 6.74718016205618
Experience 4, Iter 32, disc loss: 0.0045209737132213155, policy loss: 6.81919148307722
Experience 4, Iter 33, disc loss: 0.003325020092676812, policy loss: 7.107948245647114
Experience 4, Iter 34, disc loss: 0.004759541258784573, policy loss: 6.517143619533298
Experience 4, Iter 35, disc loss: 0.004771162318642509, policy loss: 6.524036719710058
Experience 4, Iter 36, disc loss: 0.0038839947646994205, policy loss: 6.727678265572429
Experience 4, Iter 37, disc loss: 0.0037039017103236973, policy loss: 6.90562029576196
Experience 4, Iter 38, disc loss: 0.0037478417461708803, policy loss: 6.904438709669531
Experience 4, Iter 39, disc loss: 0.005070421499999818, policy loss: 6.620535566166121
Experience 4, Iter 40, disc loss: 0.0038654512120247864, policy loss: 6.794314004117928
Experience 4, Iter 41, disc loss: 0.004106712432310342, policy loss: 6.764747231791111
Experience 4, Iter 42, disc loss: 0.0036220394079983075, policy loss: 6.831672993549654
Experience 4, Iter 43, disc loss: 0.003426486430084682, policy loss: 7.037545302580542
Experience 4, Iter 44, disc loss: 0.005133778982174628, policy loss: 6.635695228252913
Experience 4, Iter 45, disc loss: 0.0034524189832348066, policy loss: 6.860105210926476
Experience 4, Iter 46, disc loss: 0.004147447070193147, policy loss: 6.6941478940863846
Experience 4, Iter 47, disc loss: 0.004436751391367644, policy loss: 6.579844903270006
Experience 4, Iter 48, disc loss: 0.004931845183223554, policy loss: 6.781262649663853
Experience 4, Iter 49, disc loss: 0.0035469181296955655, policy loss: 7.029440748329359
Experience 4, Iter 50, disc loss: 0.0036162621992944007, policy loss: 6.851887030448622
Experience 4, Iter 51, disc loss: 0.003269096595593988, policy loss: 6.959279230721982
Experience 4, Iter 52, disc loss: 0.0038755352071198016, policy loss: 6.87806271761705
Experience 4, Iter 53, disc loss: 0.003248962555480764, policy loss: 6.952326293839803
Experience 4, Iter 54, disc loss: 0.0051595965790523024, policy loss: 6.903284670126687
Experience 4, Iter 55, disc loss: 0.0031359804931126124, policy loss: 7.089576514371514
Experience 4, Iter 56, disc loss: 0.005031291026427787, policy loss: 6.741902462664646
Experience 4, Iter 57, disc loss: 0.0032123510029876358, policy loss: 6.883963921975886
Experience 4, Iter 58, disc loss: 0.003205706501526852, policy loss: 6.873362020374133
Experience 4, Iter 59, disc loss: 0.0033086412032548944, policy loss: 6.984784660552089
Experience 4, Iter 60, disc loss: 0.0038237850362849877, policy loss: 7.077337605688859
Experience 4, Iter 61, disc loss: 0.0035767367024308362, policy loss: 6.976853908226033
Experience 4, Iter 62, disc loss: 0.002967078383385841, policy loss: 7.263962912905475
Experience 4, Iter 63, disc loss: 0.003497374120490385, policy loss: 6.9623581033078175
Experience 4, Iter 64, disc loss: 0.003048185518468873, policy loss: 7.013258460824405
Experience 4, Iter 65, disc loss: 0.0031614884815664573, policy loss: 6.956777895367672
Experience 4, Iter 66, disc loss: 0.0036721428970273038, policy loss: 7.012861928686379
Experience 4, Iter 67, disc loss: 0.0033490666318331547, policy loss: 7.227836884095788
Experience 4, Iter 68, disc loss: 0.003093125148925934, policy loss: 7.038971181139759
Experience 4, Iter 69, disc loss: 0.0028847788112193627, policy loss: 7.052854748606579
Experience 4, Iter 70, disc loss: 0.0027140574350328346, policy loss: 7.199897109658436
Experience 4, Iter 71, disc loss: 0.0028349578090431276, policy loss: 7.15019865948723
Experience 4, Iter 72, disc loss: 0.002771190772265674, policy loss: 7.17005396290287
Experience 4, Iter 73, disc loss: 0.0033331403780012285, policy loss: 7.106523445626236
Experience 4, Iter 74, disc loss: 0.003277389616858945, policy loss: 6.8516921124719055
Experience 4, Iter 75, disc loss: 0.0041667772509310155, policy loss: 7.114767170021709
Experience 4, Iter 76, disc loss: 0.003171062277618662, policy loss: 7.015465427993595
Experience 4, Iter 77, disc loss: 0.0031753958011494724, policy loss: 6.888018152711352
Experience 4, Iter 78, disc loss: 0.004081474012253607, policy loss: 6.814749801753486
Experience 4, Iter 79, disc loss: 0.0029206437792249134, policy loss: 7.217648305730197
Experience 4, Iter 80, disc loss: 0.00273862790672825, policy loss: 7.1813884089398
Experience 4, Iter 81, disc loss: 0.002976037729476615, policy loss: 7.149724519261232
Experience 4, Iter 82, disc loss: 0.0027589332354832725, policy loss: 7.2210970226759414
Experience 4, Iter 83, disc loss: 0.0031326853355885262, policy loss: 7.029922507178898
Experience 4, Iter 84, disc loss: 0.0031983602943935685, policy loss: 7.308137937260291
Experience 4, Iter 85, disc loss: 0.0027325062854416955, policy loss: 7.188942402278693
Experience 4, Iter 86, disc loss: 0.003000157656905223, policy loss: 7.128136838263489
Experience 4, Iter 87, disc loss: 0.0024748031130068784, policy loss: 7.392247121017986
Experience 4, Iter 88, disc loss: 0.0031384126513651182, policy loss: 7.137770065645894
Experience 4, Iter 89, disc loss: 0.002757596676252449, policy loss: 7.191979389676105
Experience 4, Iter 90, disc loss: 0.002872388939848398, policy loss: 7.081885247310492
Experience 4, Iter 91, disc loss: 0.002644681940116749, policy loss: 7.21255960059227
Experience 4, Iter 92, disc loss: 0.0030921421004424746, policy loss: 7.255233189156621
Experience 4, Iter 93, disc loss: 0.0025320958681173253, policy loss: 7.2618485611805035
Experience 4, Iter 94, disc loss: 0.0024018158086239784, policy loss: 7.343016101944038
Experience 4, Iter 95, disc loss: 0.0035033052354420478, policy loss: 7.217780723212031
Experience 4, Iter 96, disc loss: 0.0024447394735293247, policy loss: 7.431725850674757
Experience 4, Iter 97, disc loss: 0.002393642713258699, policy loss: 7.435855039302563
Experience 4, Iter 98, disc loss: 0.0027555009621328787, policy loss: 7.543396256425413
Experience 4, Iter 99, disc loss: 0.0025215214571026583, policy loss: 7.159229469933819
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0056],
        [0.2305],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0116, 0.0431, 0.2175, 0.0053, 0.0005, 0.1112]],

        [[0.0116, 0.0431, 0.2175, 0.0053, 0.0005, 0.1112]],

        [[0.0116, 0.0431, 0.2175, 0.0053, 0.0005, 0.1112]],

        [[0.0116, 0.0431, 0.2175, 0.0053, 0.0005, 0.1112]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0222, 0.9220, 0.0195], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0222, 0.9220, 0.0195])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.652
Iter 2/2000 - Loss: -0.616
Iter 3/2000 - Loss: -0.539
Iter 4/2000 - Loss: -0.380
Iter 5/2000 - Loss: -0.445
Iter 6/2000 - Loss: -0.602
Iter 7/2000 - Loss: -0.669
Iter 8/2000 - Loss: -0.626
Iter 9/2000 - Loss: -0.586
Iter 10/2000 - Loss: -0.643
Iter 11/2000 - Loss: -0.768
Iter 12/2000 - Loss: -0.860
Iter 13/2000 - Loss: -0.856
Iter 14/2000 - Loss: -0.785
Iter 15/2000 - Loss: -0.733
Iter 16/2000 - Loss: -0.760
Iter 17/2000 - Loss: -0.854
Iter 18/2000 - Loss: -0.951
Iter 19/2000 - Loss: -0.994
Iter 20/2000 - Loss: -0.980
Iter 1981/2000 - Loss: -6.603
Iter 1982/2000 - Loss: -6.603
Iter 1983/2000 - Loss: -6.603
Iter 1984/2000 - Loss: -6.603
Iter 1985/2000 - Loss: -6.604
Iter 1986/2000 - Loss: -6.604
Iter 1987/2000 - Loss: -6.604
Iter 1988/2000 - Loss: -6.604
Iter 1989/2000 - Loss: -6.604
Iter 1990/2000 - Loss: -6.604
Iter 1991/2000 - Loss: -6.604
Iter 1992/2000 - Loss: -6.604
Iter 1993/2000 - Loss: -6.604
Iter 1994/2000 - Loss: -6.604
Iter 1995/2000 - Loss: -6.604
Iter 1996/2000 - Loss: -6.604
Iter 1997/2000 - Loss: -6.604
Iter 1998/2000 - Loss: -6.604
Iter 1999/2000 - Loss: -6.604
Iter 2000/2000 - Loss: -6.604
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0015],
        [0.0038]])
Lengthscale: tensor([[[1.5629e+01, 2.2018e+00, 4.4481e+01, 5.0381e+00, 8.1093e+00,
          3.0838e+01]],

        [[2.3562e+01, 2.4532e+01, 1.3921e+01, 8.9596e-01, 4.3414e+00,
          5.3743e+00]],

        [[2.6934e+01, 4.2938e+01, 2.0572e+01, 9.3097e-01, 5.6955e+00,
          1.1642e+01]],

        [[8.9481e-03, 3.2919e-02, 1.7511e-01, 4.0587e-03, 3.7307e-04,
          9.6874e-02]]])
Signal Variance: tensor([0.0144, 0.1418, 6.8956, 0.0153])
Estimated target variance: tensor([0.0041, 0.0222, 0.9220, 0.0195])
N: 50
Signal to noise ratio: tensor([ 7.2270, 28.4263, 67.2003,  2.0073])
Bound on condition number: tensor([2.6125e+03, 4.0404e+04, 2.2580e+05, 2.0246e+02])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.45600807326748816, policy loss: 7.0205470422358625
Experience 5, Iter 1, disc loss: 0.8713690729843676, policy loss: 6.617774268859925
Experience 5, Iter 2, disc loss: 0.994403690781017, policy loss: 5.143742038784262
Experience 5, Iter 3, disc loss: 1.7018538681016833, policy loss: 3.5629070311353206
Experience 5, Iter 4, disc loss: 1.7651406888362857, policy loss: 2.778142939716084
Experience 5, Iter 5, disc loss: 2.2091754037005855, policy loss: 1.5572621908251227
Experience 5, Iter 6, disc loss: 1.4180545905461672, policy loss: 1.9131088591454777
Experience 5, Iter 7, disc loss: 0.976396925088014, policy loss: 1.9975943151044393
Experience 5, Iter 8, disc loss: 0.4892169181348682, policy loss: 3.326969338522103
Experience 5, Iter 9, disc loss: 0.36938705465191135, policy loss: 4.0482640461695
Experience 5, Iter 10, disc loss: 0.17867932836940947, policy loss: 5.90341777459571
Experience 5, Iter 11, disc loss: 0.14246393404427957, policy loss: 6.017702801055693
Experience 5, Iter 12, disc loss: 0.17051391272044453, policy loss: 6.3296375617814995
Experience 5, Iter 13, disc loss: 0.2144381020254243, policy loss: 5.953481305261751
Experience 5, Iter 14, disc loss: 0.23680890931686116, policy loss: 6.454409467524747
Experience 5, Iter 15, disc loss: 0.27470659473816633, policy loss: 5.235960509421773
Experience 5, Iter 16, disc loss: 0.29028585405171176, policy loss: 5.0181862455891055
Experience 5, Iter 17, disc loss: 0.29604802523708396, policy loss: 4.7469360391366795
Experience 5, Iter 18, disc loss: 0.28337526279194764, policy loss: 4.75357727219224
Experience 5, Iter 19, disc loss: 0.2561915481590984, policy loss: 4.805982295989637
Experience 5, Iter 20, disc loss: 0.22583340447997743, policy loss: 4.786304428942165
Experience 5, Iter 21, disc loss: 0.19212591615956268, policy loss: 4.9204997865938225
Experience 5, Iter 22, disc loss: 0.16140781850285157, policy loss: 4.895871099089944
Experience 5, Iter 23, disc loss: 0.13627588502376434, policy loss: 4.560103106165922
Experience 5, Iter 24, disc loss: 0.11145593340704592, policy loss: 4.743293713361966
Experience 5, Iter 25, disc loss: 0.09581245972773965, policy loss: 4.6001141396920495
Experience 5, Iter 26, disc loss: 0.08067686573013295, policy loss: 4.5458637589226125
Experience 5, Iter 27, disc loss: 0.06668426366604843, policy loss: 4.639037523964148
Experience 5, Iter 28, disc loss: 0.06224500003636641, policy loss: 4.541553629528984
Experience 5, Iter 29, disc loss: 0.05519292202227607, policy loss: 4.4064252073952375
Experience 5, Iter 30, disc loss: 0.05251777199667195, policy loss: 4.414957288092607
Experience 5, Iter 31, disc loss: 0.0513499635147485, policy loss: 4.1641793320402165
Experience 5, Iter 32, disc loss: 0.05025229772446847, policy loss: 4.085609547615132
Experience 5, Iter 33, disc loss: 0.05837283376810075, policy loss: 3.7644228264234485
Experience 5, Iter 34, disc loss: 0.054039032306492985, policy loss: 3.929968009422364
Experience 5, Iter 35, disc loss: 0.053397898577859586, policy loss: 3.918050891450038
Experience 5, Iter 36, disc loss: 0.07453240347463444, policy loss: 3.5692786080928336
Experience 5, Iter 37, disc loss: 0.06937480062131553, policy loss: 3.6412775458802766
Experience 5, Iter 38, disc loss: 0.0634165955720654, policy loss: 3.6742577159309953
Experience 5, Iter 39, disc loss: 0.07604912805597099, policy loss: 3.5129859720228107
Experience 5, Iter 40, disc loss: 0.07754090089599656, policy loss: 3.564461462025481
Experience 5, Iter 41, disc loss: 0.06813854199030484, policy loss: 3.7152272749989397
Experience 5, Iter 42, disc loss: 0.06988512928577076, policy loss: 3.4759391795380954
Experience 5, Iter 43, disc loss: 0.06846553639170144, policy loss: 3.630973603969062
Experience 5, Iter 44, disc loss: 0.0766572909349198, policy loss: 3.605849187342468
Experience 5, Iter 45, disc loss: 0.05981317749550703, policy loss: 3.762995433906103
Experience 5, Iter 46, disc loss: 0.0980276206207795, policy loss: 3.3897207648883603
Experience 5, Iter 47, disc loss: 0.07055722878669035, policy loss: 3.5539448248597623
Experience 5, Iter 48, disc loss: 0.0715327700597606, policy loss: 3.5839633381738336
Experience 5, Iter 49, disc loss: 0.06251734362676482, policy loss: 3.6446569773181596
Experience 5, Iter 50, disc loss: 0.07141802298183654, policy loss: 3.667597167214448
Experience 5, Iter 51, disc loss: 0.04676862854513232, policy loss: 3.9845316434492783
Experience 5, Iter 52, disc loss: 0.08961878698970276, policy loss: 3.57781092972712
Experience 5, Iter 53, disc loss: 0.04575153678853844, policy loss: 4.20020283179513
Experience 5, Iter 54, disc loss: 0.05133414657827051, policy loss: 4.009101101574399
Experience 5, Iter 55, disc loss: 0.050540629121177794, policy loss: 4.0658883184249675
Experience 5, Iter 56, disc loss: 0.06098045111522643, policy loss: 3.8655240785158806
Experience 5, Iter 57, disc loss: 0.05235974062483993, policy loss: 3.958847798037421
Experience 5, Iter 58, disc loss: 0.05219302104727691, policy loss: 3.9156940395442876
Experience 5, Iter 59, disc loss: 0.05709275469043516, policy loss: 3.839189220466954
Experience 5, Iter 60, disc loss: 0.04505540904576953, policy loss: 4.462598557565814
Experience 5, Iter 61, disc loss: 0.04543318460083074, policy loss: 4.225831761709269
Experience 5, Iter 62, disc loss: 0.0428837332549985, policy loss: 4.288061573365508
Experience 5, Iter 63, disc loss: 0.04069059236164133, policy loss: 4.432343734009235
Experience 5, Iter 64, disc loss: 0.046678518212488175, policy loss: 4.293908114865491
Experience 5, Iter 65, disc loss: 0.04268766480532754, policy loss: 4.222624508456958
Experience 5, Iter 66, disc loss: 0.04453188089283911, policy loss: 4.210278601502476
Experience 5, Iter 67, disc loss: 0.046671197945363924, policy loss: 4.23269204075119
Experience 5, Iter 68, disc loss: 0.035510949044001275, policy loss: 4.427271063752716
Experience 5, Iter 69, disc loss: 0.043340870213745435, policy loss: 4.20332019024435
Experience 5, Iter 70, disc loss: 0.03932405915494551, policy loss: 4.450187109233573
Experience 5, Iter 71, disc loss: 0.029085687160426747, policy loss: 4.827041035263864
Experience 5, Iter 72, disc loss: 0.04273975224672448, policy loss: 4.311475662077655
Experience 5, Iter 73, disc loss: 0.04137171642075927, policy loss: 4.298250405159572
Experience 5, Iter 74, disc loss: 0.03936136045800709, policy loss: 4.32967672712664
Experience 5, Iter 75, disc loss: 0.03792718801078522, policy loss: 4.367959653264885
Experience 5, Iter 76, disc loss: 0.03816698145041483, policy loss: 4.503761583823301
Experience 5, Iter 77, disc loss: 0.03459059653447798, policy loss: 4.4392278802309235
Experience 5, Iter 78, disc loss: 0.039574712526725564, policy loss: 4.6410140256507235
Experience 5, Iter 79, disc loss: 0.036908874022150456, policy loss: 4.469143878222974
Experience 5, Iter 80, disc loss: 0.03715229336458875, policy loss: 4.545078649160141
Experience 5, Iter 81, disc loss: 0.036218024113824236, policy loss: 4.560347009095984
Experience 5, Iter 82, disc loss: 0.03336668005454879, policy loss: 4.955700209132501
Experience 5, Iter 83, disc loss: 0.04000966345442426, policy loss: 4.513106419321891
Experience 5, Iter 84, disc loss: 0.0355484258813193, policy loss: 4.595513280975604
Experience 5, Iter 85, disc loss: 0.03590032091335689, policy loss: 4.47122295137499
Experience 5, Iter 86, disc loss: 0.03410981939299604, policy loss: 4.5047226706382
Experience 5, Iter 87, disc loss: 0.03130955767572729, policy loss: 4.749819911074075
Experience 5, Iter 88, disc loss: 0.029918718393720546, policy loss: 4.840641039529439
Experience 5, Iter 89, disc loss: 0.030092309608963846, policy loss: 4.669984819089212
Experience 5, Iter 90, disc loss: 0.033984954707158344, policy loss: 4.631039281543345
Experience 5, Iter 91, disc loss: 0.030456624701046625, policy loss: 4.55997051501728
Experience 5, Iter 92, disc loss: 0.03674051197904744, policy loss: 4.282551628084402
Experience 5, Iter 93, disc loss: 0.03219673455678237, policy loss: 4.731167961398605
Experience 5, Iter 94, disc loss: 0.02985288148019908, policy loss: 4.6477917855541815
Experience 5, Iter 95, disc loss: 0.0329937511593445, policy loss: 4.53727998494941
Experience 5, Iter 96, disc loss: 0.027267905522057163, policy loss: 4.785739271483361
Experience 5, Iter 97, disc loss: 0.0315748002702024, policy loss: 4.449335832794349
Experience 5, Iter 98, disc loss: 0.0334083681876216, policy loss: 4.496990697825896
Experience 5, Iter 99, disc loss: 0.02660472980658098, policy loss: 4.774510302421003
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0064],
        [0.1917],
        [0.0041]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0110, 0.0444, 0.1810, 0.0050, 0.0004, 0.1714]],

        [[0.0110, 0.0444, 0.1810, 0.0050, 0.0004, 0.1714]],

        [[0.0110, 0.0444, 0.1810, 0.0050, 0.0004, 0.1714]],

        [[0.0110, 0.0444, 0.1810, 0.0050, 0.0004, 0.1714]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0044, 0.0255, 0.7668, 0.0163], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0044, 0.0255, 0.7668, 0.0163])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.025
Iter 2/2000 - Loss: -0.774
Iter 3/2000 - Loss: -0.706
Iter 4/2000 - Loss: -0.661
Iter 5/2000 - Loss: -0.742
Iter 6/2000 - Loss: -0.877
Iter 7/2000 - Loss: -0.943
Iter 8/2000 - Loss: -0.952
Iter 9/2000 - Loss: -0.981
Iter 10/2000 - Loss: -1.061
Iter 11/2000 - Loss: -1.155
Iter 12/2000 - Loss: -1.207
Iter 13/2000 - Loss: -1.212
Iter 14/2000 - Loss: -1.224
Iter 15/2000 - Loss: -1.296
Iter 16/2000 - Loss: -1.413
Iter 17/2000 - Loss: -1.506
Iter 18/2000 - Loss: -1.535
Iter 19/2000 - Loss: -1.540
Iter 20/2000 - Loss: -1.594
Iter 1981/2000 - Loss: -8.528
Iter 1982/2000 - Loss: -8.528
Iter 1983/2000 - Loss: -8.528
Iter 1984/2000 - Loss: -8.529
Iter 1985/2000 - Loss: -8.529
Iter 1986/2000 - Loss: -8.529
Iter 1987/2000 - Loss: -8.529
Iter 1988/2000 - Loss: -8.529
Iter 1989/2000 - Loss: -8.529
Iter 1990/2000 - Loss: -8.529
Iter 1991/2000 - Loss: -8.529
Iter 1992/2000 - Loss: -8.529
Iter 1993/2000 - Loss: -8.529
Iter 1994/2000 - Loss: -8.529
Iter 1995/2000 - Loss: -8.529
Iter 1996/2000 - Loss: -8.529
Iter 1997/2000 - Loss: -8.529
Iter 1998/2000 - Loss: -8.529
Iter 1999/2000 - Loss: -8.529
Iter 2000/2000 - Loss: -8.529
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[14.8709,  2.4552, 43.0767,  5.7923,  8.5981, 26.2100]],

        [[22.9959, 30.2842, 18.0245,  0.9644,  3.9328,  7.9114]],

        [[24.3193, 37.1829, 19.6325,  0.9324,  5.2779, 11.8616]],

        [[19.8909, 35.3260,  9.6082,  3.0422,  8.7525, 22.4611]]])
Signal Variance: tensor([0.0178, 0.2721, 6.2382, 0.2190])
Estimated target variance: tensor([0.0044, 0.0255, 0.7668, 0.0163])
N: 60
Signal to noise ratio: tensor([ 7.6459, 37.9804, 64.2139, 27.9089])
Bound on condition number: tensor([  3508.5990,  86551.6081, 247406.6328,  46735.4736])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.02998132252282544, policy loss: 4.460161836492366
Experience 6, Iter 1, disc loss: 0.03652464960629881, policy loss: 4.155550416773723
Experience 6, Iter 2, disc loss: 0.046038445390004415, policy loss: 3.8135279406176594
Experience 6, Iter 3, disc loss: 0.056679944666977525, policy loss: 3.4081618025145843
Experience 6, Iter 4, disc loss: 0.06170923241631981, policy loss: 3.5220063985731533
Experience 6, Iter 5, disc loss: 0.07602389765404616, policy loss: 3.15215447003392
Experience 6, Iter 6, disc loss: 0.05964999955132104, policy loss: 3.6550766792816054
Experience 6, Iter 7, disc loss: 0.07474465056340429, policy loss: 3.472110167542554
Experience 6, Iter 8, disc loss: 0.08413006736461048, policy loss: 3.3369420195235264
Experience 6, Iter 9, disc loss: 0.07943542106502798, policy loss: 3.35988922350918
Experience 6, Iter 10, disc loss: 0.08001910382845745, policy loss: 3.765989772115337
Experience 6, Iter 11, disc loss: 0.09897006554095242, policy loss: 3.2485371092273043
Experience 6, Iter 12, disc loss: 0.09503748593155262, policy loss: 3.323492184419409
Experience 6, Iter 13, disc loss: 0.08930348800781987, policy loss: 3.3854152812610323
Experience 6, Iter 14, disc loss: 0.07777563062034466, policy loss: 3.394230099000284
Experience 6, Iter 15, disc loss: 0.070448564638331, policy loss: 3.7067383972983627
Experience 6, Iter 16, disc loss: 0.06348011249714236, policy loss: 3.8121366476665353
Experience 6, Iter 17, disc loss: 0.059511329556649194, policy loss: 3.755543124590873
Experience 6, Iter 18, disc loss: 0.06441445327752904, policy loss: 3.79238223846553
Experience 6, Iter 19, disc loss: 0.0699750918243853, policy loss: 3.844033667444627
Experience 6, Iter 20, disc loss: 0.0707161383545709, policy loss: 4.026223642369811
Experience 6, Iter 21, disc loss: 0.0657408077670904, policy loss: 4.149693569909976
Experience 6, Iter 22, disc loss: 0.05797005793071687, policy loss: 4.3015206628315195
Experience 6, Iter 23, disc loss: 0.056971318725573344, policy loss: 4.58476772455623
Experience 6, Iter 24, disc loss: 0.05384708830344688, policy loss: 4.275103348914771
Experience 6, Iter 25, disc loss: 0.05734372470446755, policy loss: 4.170450453140553
Experience 6, Iter 26, disc loss: 0.0474581740870798, policy loss: 4.478665394274093
Experience 6, Iter 27, disc loss: 0.04899813252219601, policy loss: 4.304798178416439
Experience 6, Iter 28, disc loss: 0.04778856288993906, policy loss: 4.388942138271047
Experience 6, Iter 29, disc loss: 0.047268433686681946, policy loss: 4.388130261647656
Experience 6, Iter 30, disc loss: 0.048280018935300685, policy loss: 4.314464989060106
Experience 6, Iter 31, disc loss: 0.04447145229850878, policy loss: 4.584951385659525
Experience 6, Iter 32, disc loss: 0.043941443381974615, policy loss: 4.635688272660586
Experience 6, Iter 33, disc loss: 0.042161029029406466, policy loss: 4.488393684615067
Experience 6, Iter 34, disc loss: 0.044771195765884966, policy loss: 4.481568537930004
Experience 6, Iter 35, disc loss: 0.040286076678092995, policy loss: 4.557597296145034
Experience 6, Iter 36, disc loss: 0.03919569623040223, policy loss: 4.550467999975979
Experience 6, Iter 37, disc loss: 0.03603646989742182, policy loss: 4.629244330566095
Experience 6, Iter 38, disc loss: 0.03715487898523131, policy loss: 4.49066063874419
Experience 6, Iter 39, disc loss: 0.031908769167425015, policy loss: 4.7623960353185275
Experience 6, Iter 40, disc loss: 0.034757721692818835, policy loss: 4.526210161213522
Experience 6, Iter 41, disc loss: 0.03220650828773497, policy loss: 4.612871663763355
Experience 6, Iter 42, disc loss: 0.031002739665433775, policy loss: 4.77453576542997
Experience 6, Iter 43, disc loss: 0.033316014151545795, policy loss: 4.481950538591333
Experience 6, Iter 44, disc loss: 0.0316773213079639, policy loss: 4.632437473518131
Experience 6, Iter 45, disc loss: 0.03067733876401659, policy loss: 4.65747821444661
Experience 6, Iter 46, disc loss: 0.03011395245024956, policy loss: 4.718418593563706
Experience 6, Iter 47, disc loss: 0.03107677354807107, policy loss: 4.502004486132875
Experience 6, Iter 48, disc loss: 0.027895337570274015, policy loss: 4.732240472658159
Experience 6, Iter 49, disc loss: 0.02847357488772518, policy loss: 4.517102223636705
Experience 6, Iter 50, disc loss: 0.02880636499829732, policy loss: 4.554960013281579
Experience 6, Iter 51, disc loss: 0.02651380166704415, policy loss: 4.628202345374011
Experience 6, Iter 52, disc loss: 0.027541060080753604, policy loss: 4.521189065763524
Experience 6, Iter 53, disc loss: 0.02980398172531233, policy loss: 4.406475792767857
Experience 6, Iter 54, disc loss: 0.02561503634079182, policy loss: 4.580840223742445
Experience 6, Iter 55, disc loss: 0.02388612404532984, policy loss: 4.824790736210573
Experience 6, Iter 56, disc loss: 0.027388377382872503, policy loss: 4.466773092466269
Experience 6, Iter 57, disc loss: 0.023840879593956685, policy loss: 4.745119964920573
Experience 6, Iter 58, disc loss: 0.025443237659778604, policy loss: 4.629034214545758
Experience 6, Iter 59, disc loss: 0.02734534173736, policy loss: 4.525668647475132
Experience 6, Iter 60, disc loss: 0.025198265836679345, policy loss: 4.481550634704078
Experience 6, Iter 61, disc loss: 0.026080916702914073, policy loss: 4.523315356918198
Experience 6, Iter 62, disc loss: 0.026130269703250577, policy loss: 4.455634399748288
Experience 6, Iter 63, disc loss: 0.025835959365309785, policy loss: 4.42465153946862
Experience 6, Iter 64, disc loss: 0.024999754283394647, policy loss: 4.458255985406027
Experience 6, Iter 65, disc loss: 0.02188322678674094, policy loss: 4.825625813266685
Experience 6, Iter 66, disc loss: 0.027063854107189467, policy loss: 4.492139784126236
Experience 6, Iter 67, disc loss: 0.02109429575916358, policy loss: 4.95080501106791
Experience 6, Iter 68, disc loss: 0.023911932320951346, policy loss: 4.623614160233493
Experience 6, Iter 69, disc loss: 0.022461869263117357, policy loss: 4.729771835395687
Experience 6, Iter 70, disc loss: 0.023121001100735616, policy loss: 4.597780178852098
Experience 6, Iter 71, disc loss: 0.02185258689648674, policy loss: 4.692786386379522
Experience 6, Iter 72, disc loss: 0.020675200966156628, policy loss: 4.742543307877199
Experience 6, Iter 73, disc loss: 0.022860732325500795, policy loss: 4.588010541434176
Experience 6, Iter 74, disc loss: 0.021172196440346923, policy loss: 4.714716622713627
Experience 6, Iter 75, disc loss: 0.0212494752296409, policy loss: 4.861131495911372
Experience 6, Iter 76, disc loss: 0.021446570289988927, policy loss: 4.580510274046926
Experience 6, Iter 77, disc loss: 0.0233002265230317, policy loss: 4.484061955435465
Experience 6, Iter 78, disc loss: 0.02014239307762685, policy loss: 4.883854949049172
Experience 6, Iter 79, disc loss: 0.020568133524139113, policy loss: 4.660764408636227
Experience 6, Iter 80, disc loss: 0.020935231466615058, policy loss: 4.671474241365354
Experience 6, Iter 81, disc loss: 0.02178098110385008, policy loss: 4.51796560106635
Experience 6, Iter 82, disc loss: 0.020616916938179394, policy loss: 4.658195915453428
Experience 6, Iter 83, disc loss: 0.02198977022963547, policy loss: 4.529430193103202
Experience 6, Iter 84, disc loss: 0.020707379011343, policy loss: 4.679195440396455
Experience 6, Iter 85, disc loss: 0.020242437903560638, policy loss: 4.799455085944268
Experience 6, Iter 86, disc loss: 0.020059505693374863, policy loss: 4.669062025744398
Experience 6, Iter 87, disc loss: 0.018112215190958653, policy loss: 4.756863918064355
Experience 6, Iter 88, disc loss: 0.021683946693214596, policy loss: 4.475390054279017
Experience 6, Iter 89, disc loss: 0.020664020547291183, policy loss: 4.664083236584442
Experience 6, Iter 90, disc loss: 0.019687249768171752, policy loss: 4.830416143546901
Experience 6, Iter 91, disc loss: 0.019440724987168496, policy loss: 4.818355386623626
Experience 6, Iter 92, disc loss: 0.017992540494993967, policy loss: 4.97517156202194
Experience 6, Iter 93, disc loss: 0.0181298096250875, policy loss: 4.798891124743517
Experience 6, Iter 94, disc loss: 0.01753106816215078, policy loss: 4.9435895785862884
Experience 6, Iter 95, disc loss: 0.01873527118891253, policy loss: 4.724786350888033
Experience 6, Iter 96, disc loss: 0.018964259136485973, policy loss: 4.6903141229322225
Experience 6, Iter 97, disc loss: 0.020646246346295805, policy loss: 4.610423827732179
Experience 6, Iter 98, disc loss: 0.018842801465212652, policy loss: 4.8191615799059875
Experience 6, Iter 99, disc loss: 0.018484918573689522, policy loss: 4.789115293898162
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.0261],
        [0.2015],
        [0.0044]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0188, 0.1668, 0.1890, 0.0107, 0.0017, 1.2060]],

        [[0.0188, 0.1668, 0.1890, 0.0107, 0.0017, 1.2060]],

        [[0.0188, 0.1668, 0.1890, 0.0107, 0.0017, 1.2060]],

        [[0.0188, 0.1668, 0.1890, 0.0107, 0.0017, 1.2060]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0183, 0.1044, 0.8061, 0.0177], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0183, 0.1044, 0.8061, 0.0177])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.429
Iter 2/2000 - Loss: 0.494
Iter 3/2000 - Loss: 0.327
Iter 4/2000 - Loss: 0.275
Iter 5/2000 - Loss: 0.316
Iter 6/2000 - Loss: 0.253
Iter 7/2000 - Loss: 0.164
Iter 8/2000 - Loss: 0.129
Iter 9/2000 - Loss: 0.113
Iter 10/2000 - Loss: 0.055
Iter 11/2000 - Loss: -0.031
Iter 12/2000 - Loss: -0.107
Iter 13/2000 - Loss: -0.166
Iter 14/2000 - Loss: -0.231
Iter 15/2000 - Loss: -0.320
Iter 16/2000 - Loss: -0.431
Iter 17/2000 - Loss: -0.550
Iter 18/2000 - Loss: -0.672
Iter 19/2000 - Loss: -0.805
Iter 20/2000 - Loss: -0.959
Iter 1981/2000 - Loss: -8.313
Iter 1982/2000 - Loss: -8.313
Iter 1983/2000 - Loss: -8.313
Iter 1984/2000 - Loss: -8.313
Iter 1985/2000 - Loss: -8.313
Iter 1986/2000 - Loss: -8.313
Iter 1987/2000 - Loss: -8.313
Iter 1988/2000 - Loss: -8.313
Iter 1989/2000 - Loss: -8.313
Iter 1990/2000 - Loss: -8.313
Iter 1991/2000 - Loss: -8.313
Iter 1992/2000 - Loss: -8.314
Iter 1993/2000 - Loss: -8.314
Iter 1994/2000 - Loss: -8.314
Iter 1995/2000 - Loss: -8.314
Iter 1996/2000 - Loss: -8.314
Iter 1997/2000 - Loss: -8.314
Iter 1998/2000 - Loss: -8.314
Iter 1999/2000 - Loss: -8.314
Iter 2000/2000 - Loss: -8.314
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[19.3846,  8.0125, 31.9273, 13.2382, 11.2530, 52.1410]],

        [[20.6443, 38.4225, 23.2433,  1.0986,  4.4581, 12.1121]],

        [[22.4913, 40.5349, 19.0420,  0.9553,  1.5696, 11.7941]],

        [[20.2088, 41.9661, 10.8318,  3.5260, 10.8516, 38.1304]]])
Signal Variance: tensor([0.1338, 0.5318, 6.0035, 0.3357])
Estimated target variance: tensor([0.0183, 0.1044, 0.8061, 0.0177])
N: 70
Signal to noise ratio: tensor([21.2193, 53.9326, 62.4525, 33.6007])
Bound on condition number: tensor([ 31519.0821, 203611.5713, 273023.1963,  79031.3568])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.022667253073524497, policy loss: 4.401970492232113
Experience 7, Iter 1, disc loss: 0.02184846950167537, policy loss: 4.39757264668637
Experience 7, Iter 2, disc loss: 0.021288512178309443, policy loss: 4.496126518851769
Experience 7, Iter 3, disc loss: 0.020506086958738293, policy loss: 4.528132236727581
Experience 7, Iter 4, disc loss: 0.02113677290187256, policy loss: 4.586089950137711
Experience 7, Iter 5, disc loss: 0.01986324348968682, policy loss: 4.722533190133078
Experience 7, Iter 6, disc loss: 0.021226554630570486, policy loss: 4.513587542766696
Experience 7, Iter 7, disc loss: 0.020295970553500187, policy loss: 4.6235462131538405
Experience 7, Iter 8, disc loss: 0.01961275057534339, policy loss: 4.616158292997468
Experience 7, Iter 9, disc loss: 0.019520929510672247, policy loss: 4.659372882309885
Experience 7, Iter 10, disc loss: 0.019005144910765076, policy loss: 4.758935122269396
Experience 7, Iter 11, disc loss: 0.018392667848923086, policy loss: 4.79533029529161
Experience 7, Iter 12, disc loss: 0.018032438332883843, policy loss: 4.772247554354909
Experience 7, Iter 13, disc loss: 0.017821610286371317, policy loss: 4.736774938188085
Experience 7, Iter 14, disc loss: 0.016579050204271584, policy loss: 4.961100268784267
Experience 7, Iter 15, disc loss: 0.016948273801493434, policy loss: 4.810032722763204
Experience 7, Iter 16, disc loss: 0.017909076561684008, policy loss: 4.703863933462662
Experience 7, Iter 17, disc loss: 0.015580960479195861, policy loss: 5.166696550551476
Experience 7, Iter 18, disc loss: 0.01718749523590278, policy loss: 4.7746797378919705
Experience 7, Iter 19, disc loss: 0.016075482175522092, policy loss: 5.033715458699114
Experience 7, Iter 20, disc loss: 0.01546242426790407, policy loss: 5.024923768188422
Experience 7, Iter 21, disc loss: 0.015518070376074763, policy loss: 4.9681108585623175
Experience 7, Iter 22, disc loss: 0.015129994653683669, policy loss: 5.088828249881849
Experience 7, Iter 23, disc loss: 0.015045705780361629, policy loss: 5.060270151978516
Experience 7, Iter 24, disc loss: 0.014685499380869119, policy loss: 5.0979318008935675
Experience 7, Iter 25, disc loss: 0.015254918596631485, policy loss: 5.002508197184584
Experience 7, Iter 26, disc loss: 0.0140327816991992, policy loss: 5.12954505420659
Experience 7, Iter 27, disc loss: 0.01457848796805468, policy loss: 5.0211511242969245
Experience 7, Iter 28, disc loss: 0.014234129894665774, policy loss: 5.258298198721759
Experience 7, Iter 29, disc loss: 0.01409611527666658, policy loss: 5.113340505411884
Experience 7, Iter 30, disc loss: 0.012723407586167098, policy loss: 5.356777742772547
Experience 7, Iter 31, disc loss: 0.013630168448629576, policy loss: 5.17161541828468
Experience 7, Iter 32, disc loss: 0.013495057053236333, policy loss: 5.245302411050767
Experience 7, Iter 33, disc loss: 0.011783018458133676, policy loss: 5.768719124833405
Experience 7, Iter 34, disc loss: 0.012884942095421025, policy loss: 5.1862336929376855
Experience 7, Iter 35, disc loss: 0.011717593090563331, policy loss: 5.435147025499968
Experience 7, Iter 36, disc loss: 0.011996399052022412, policy loss: 5.330068954524603
Experience 7, Iter 37, disc loss: 0.012725654648286754, policy loss: 5.139436148246057
Experience 7, Iter 38, disc loss: 0.012013077820772398, policy loss: 5.3073781796203665
Experience 7, Iter 39, disc loss: 0.01217056350077031, policy loss: 5.308603326921039
Experience 7, Iter 40, disc loss: 0.011883859083692713, policy loss: 5.255909105634023
Experience 7, Iter 41, disc loss: 0.011138247112178604, policy loss: 5.376289167931132
Experience 7, Iter 42, disc loss: 0.011087075177840978, policy loss: 5.352890286120958
Experience 7, Iter 43, disc loss: 0.011651334959304958, policy loss: 5.207182412287339
Experience 7, Iter 44, disc loss: 0.010679818083761627, policy loss: 5.65385035175025
Experience 7, Iter 45, disc loss: 0.010240762709604617, policy loss: 5.802501619754674
Experience 7, Iter 46, disc loss: 0.01071459298618882, policy loss: 5.447641069626265
Experience 7, Iter 47, disc loss: 0.009676528501843504, policy loss: 5.6393601562744875
Experience 7, Iter 48, disc loss: 0.009916986324336357, policy loss: 5.630841925513884
Experience 7, Iter 49, disc loss: 0.010737226766736285, policy loss: 5.384425329117585
Experience 7, Iter 50, disc loss: 0.009838095269715984, policy loss: 5.768398134973001
Experience 7, Iter 51, disc loss: 0.010210687684956506, policy loss: 5.46583747286464
Experience 7, Iter 52, disc loss: 0.01029493009339939, policy loss: 5.46897657758681
Experience 7, Iter 53, disc loss: 0.009726562204611705, policy loss: 5.497013667568423
Experience 7, Iter 54, disc loss: 0.009590691909566549, policy loss: 5.552899972456097
Experience 7, Iter 55, disc loss: 0.010453093366553132, policy loss: 5.3388341785328715
Experience 7, Iter 56, disc loss: 0.00975976333909309, policy loss: 5.572192575862006
Experience 7, Iter 57, disc loss: 0.010101591049153431, policy loss: 5.36154619337632
Experience 7, Iter 58, disc loss: 0.009806814214117068, policy loss: 5.489376057839897
Experience 7, Iter 59, disc loss: 0.00878493774053462, policy loss: 5.602034456446827
Experience 7, Iter 60, disc loss: 0.008817683763993871, policy loss: 5.809348394123261
Experience 7, Iter 61, disc loss: 0.00961523667583214, policy loss: 5.530874548648882
Experience 7, Iter 62, disc loss: 0.009271829626963877, policy loss: 5.4559801491022535
Experience 7, Iter 63, disc loss: 0.008686501499161068, policy loss: 5.87468302291877
Experience 7, Iter 64, disc loss: 0.008623259501466914, policy loss: 5.588866822324615
Experience 7, Iter 65, disc loss: 0.007473204146301597, policy loss: 6.025607524017359
Experience 7, Iter 66, disc loss: 0.007442739019164689, policy loss: 6.018231354790851
Experience 7, Iter 67, disc loss: 0.008704130077557194, policy loss: 5.508327976116739
Experience 7, Iter 68, disc loss: 0.008830112399532478, policy loss: 5.616270310842706
Experience 7, Iter 69, disc loss: 0.00826026100648545, policy loss: 5.76841448575285
Experience 7, Iter 70, disc loss: 0.008546546785126768, policy loss: 5.708172657690226
Experience 7, Iter 71, disc loss: 0.007743230063749117, policy loss: 5.763003570525728
Experience 7, Iter 72, disc loss: 0.007493150354537332, policy loss: 5.778851516772792
Experience 7, Iter 73, disc loss: 0.007798184845176732, policy loss: 5.716972265179733
Experience 7, Iter 74, disc loss: 0.007740398233633249, policy loss: 5.743226302754936
Experience 7, Iter 75, disc loss: 0.008035868031732079, policy loss: 5.682156826228252
Experience 7, Iter 76, disc loss: 0.007224815793725912, policy loss: 5.948113436093812
Experience 7, Iter 77, disc loss: 0.007414153791194446, policy loss: 5.8578600009500965
Experience 7, Iter 78, disc loss: 0.007466391975372888, policy loss: 5.8097462612227355
Experience 7, Iter 79, disc loss: 0.00731282113151515, policy loss: 5.810062998192519
Experience 7, Iter 80, disc loss: 0.007555081143585475, policy loss: 5.709094072495615
Experience 7, Iter 81, disc loss: 0.007477281573398981, policy loss: 5.734634520661024
Experience 7, Iter 82, disc loss: 0.007407900543523914, policy loss: 5.81882522603386
Experience 7, Iter 83, disc loss: 0.0072053488777918786, policy loss: 5.94734649801576
Experience 7, Iter 84, disc loss: 0.0071201827826558135, policy loss: 5.790925992235477
Experience 7, Iter 85, disc loss: 0.0074730654081705036, policy loss: 5.760610748575019
Experience 7, Iter 86, disc loss: 0.00750318937787212, policy loss: 5.7637637465264735
Experience 7, Iter 87, disc loss: 0.0070925583444724095, policy loss: 5.903449087278705
Experience 7, Iter 88, disc loss: 0.006936876848039052, policy loss: 5.843898847989625
Experience 7, Iter 89, disc loss: 0.007088882537789618, policy loss: 5.842675879276436
Experience 7, Iter 90, disc loss: 0.0067476749528314565, policy loss: 5.893613309816669
Experience 7, Iter 91, disc loss: 0.006729506595905557, policy loss: 5.913279711799788
Experience 7, Iter 92, disc loss: 0.007015951781036892, policy loss: 5.910741564318
Experience 7, Iter 93, disc loss: 0.006600495039131084, policy loss: 5.998788602135276
Experience 7, Iter 94, disc loss: 0.006131571352778115, policy loss: 6.039498855137686
Experience 7, Iter 95, disc loss: 0.006784900097740609, policy loss: 5.860667514879223
Experience 7, Iter 96, disc loss: 0.00639114311615735, policy loss: 6.030242648602952
Experience 7, Iter 97, disc loss: 0.0064572562782496505, policy loss: 6.172909806664587
Experience 7, Iter 98, disc loss: 0.006384361387582493, policy loss: 6.15609791692038
Experience 7, Iter 99, disc loss: 0.005977573087423046, policy loss: 6.060690988380872
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0060],
        [0.0369],
        [0.2221],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0240, 0.2153, 0.2011, 0.0137, 0.0028, 1.7743]],

        [[0.0240, 0.2153, 0.2011, 0.0137, 0.0028, 1.7743]],

        [[0.0240, 0.2153, 0.2011, 0.0137, 0.0028, 1.7743]],

        [[0.0240, 0.2153, 0.2011, 0.0137, 0.0028, 1.7743]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0242, 0.1477, 0.8885, 0.0191], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0242, 0.1477, 0.8885, 0.0191])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.767
Iter 2/2000 - Loss: 0.874
Iter 3/2000 - Loss: 0.672
Iter 4/2000 - Loss: 0.646
Iter 5/2000 - Loss: 0.686
Iter 6/2000 - Loss: 0.605
Iter 7/2000 - Loss: 0.501
Iter 8/2000 - Loss: 0.456
Iter 9/2000 - Loss: 0.433
Iter 10/2000 - Loss: 0.373
Iter 11/2000 - Loss: 0.277
Iter 12/2000 - Loss: 0.179
Iter 13/2000 - Loss: 0.092
Iter 14/2000 - Loss: 0.003
Iter 15/2000 - Loss: -0.105
Iter 16/2000 - Loss: -0.240
Iter 17/2000 - Loss: -0.397
Iter 18/2000 - Loss: -0.569
Iter 19/2000 - Loss: -0.757
Iter 20/2000 - Loss: -0.962
Iter 1981/2000 - Loss: -8.265
Iter 1982/2000 - Loss: -8.265
Iter 1983/2000 - Loss: -8.265
Iter 1984/2000 - Loss: -8.265
Iter 1985/2000 - Loss: -8.265
Iter 1986/2000 - Loss: -8.265
Iter 1987/2000 - Loss: -8.265
Iter 1988/2000 - Loss: -8.265
Iter 1989/2000 - Loss: -8.265
Iter 1990/2000 - Loss: -8.265
Iter 1991/2000 - Loss: -8.265
Iter 1992/2000 - Loss: -8.265
Iter 1993/2000 - Loss: -8.265
Iter 1994/2000 - Loss: -8.266
Iter 1995/2000 - Loss: -8.266
Iter 1996/2000 - Loss: -8.266
Iter 1997/2000 - Loss: -8.266
Iter 1998/2000 - Loss: -8.266
Iter 1999/2000 - Loss: -8.266
Iter 2000/2000 - Loss: -8.266
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[18.2104,  8.2644, 29.7055, 13.2502, 11.3568, 51.3849]],

        [[18.3676, 39.0852, 18.2330,  1.1556,  4.0785, 18.1342]],

        [[23.1913, 42.9237, 19.3409,  0.9405,  1.5889, 15.8306]],

        [[20.9620, 37.7616, 10.5195,  3.5641, 10.2216, 36.9308]]])
Signal Variance: tensor([0.1274, 0.9703, 9.3810, 0.3172])
Estimated target variance: tensor([0.0242, 0.1477, 0.8885, 0.0191])
N: 80
Signal to noise ratio: tensor([21.5618, 64.2703, 74.2669, 32.9388])
Bound on condition number: tensor([ 37193.9327, 330454.9481, 441247.3566,  86798.2937])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.005986176189655771, policy loss: 6.13821219398523
Experience 8, Iter 1, disc loss: 0.006094727877636107, policy loss: 6.270238958635986
Experience 8, Iter 2, disc loss: 0.006641176051652294, policy loss: 5.938253003826529
Experience 8, Iter 3, disc loss: 0.00625470472359347, policy loss: 6.0077602769543255
Experience 8, Iter 4, disc loss: 0.006089752310743546, policy loss: 6.075948624306936
Experience 8, Iter 5, disc loss: 0.005810001740558946, policy loss: 6.220770108840004
Experience 8, Iter 6, disc loss: 0.006014391290557924, policy loss: 5.9751814459561405
Experience 8, Iter 7, disc loss: 0.006139847847150893, policy loss: 5.915050336059421
Experience 8, Iter 8, disc loss: 0.00563834157854457, policy loss: 6.169418791243078
Experience 8, Iter 9, disc loss: 0.00545225811559264, policy loss: 6.197010747874749
Experience 8, Iter 10, disc loss: 0.005702858637555366, policy loss: 6.001588804799383
Experience 8, Iter 11, disc loss: 0.005679148959315022, policy loss: 6.190924163291162
Experience 8, Iter 12, disc loss: 0.005809519743304839, policy loss: 6.067773813108722
Experience 8, Iter 13, disc loss: 0.005459006785414649, policy loss: 6.26199171429772
Experience 8, Iter 14, disc loss: 0.005097913989044437, policy loss: 6.517425566202766
Experience 8, Iter 15, disc loss: 0.005136099619738091, policy loss: 6.360078132309221
Experience 8, Iter 16, disc loss: 0.005308129938699266, policy loss: 6.226761085287538
Experience 8, Iter 17, disc loss: 0.005197159787859747, policy loss: 6.283091371769062
Experience 8, Iter 18, disc loss: 0.005210493352575242, policy loss: 6.197030687436635
Experience 8, Iter 19, disc loss: 0.004727986168350252, policy loss: 6.375681740156018
Experience 8, Iter 20, disc loss: 0.005691176155444977, policy loss: 6.007467798621835
Experience 8, Iter 21, disc loss: 0.00557148761200478, policy loss: 6.0902077798198215
Experience 8, Iter 22, disc loss: 0.0057602949000283964, policy loss: 6.101555690303032
Experience 8, Iter 23, disc loss: 0.0047657250984979105, policy loss: 6.310473272874926
Experience 8, Iter 24, disc loss: 0.005081243738085403, policy loss: 6.237432198990781
Experience 8, Iter 25, disc loss: 0.004848689785875859, policy loss: 6.469823381783671
Experience 8, Iter 26, disc loss: 0.0051683089117800516, policy loss: 6.291384110856277
Experience 8, Iter 27, disc loss: 0.005235820418467022, policy loss: 6.129935067004409
Experience 8, Iter 28, disc loss: 0.004949724996161992, policy loss: 6.193511488430961
Experience 8, Iter 29, disc loss: 0.004977255316042562, policy loss: 6.313321655654761
Experience 8, Iter 30, disc loss: 0.004873039310769161, policy loss: 6.310062092670042
Experience 8, Iter 31, disc loss: 0.004984014064339632, policy loss: 6.229075062497998
Experience 8, Iter 32, disc loss: 0.004815533845786014, policy loss: 6.351920545070978
Experience 8, Iter 33, disc loss: 0.004644852703896075, policy loss: 6.400950246432739
Experience 8, Iter 34, disc loss: 0.004587573531088643, policy loss: 6.489497722948738
Experience 8, Iter 35, disc loss: 0.004603993289296659, policy loss: 6.360715017268366
Experience 8, Iter 36, disc loss: 0.004548105478605588, policy loss: 6.429162016901428
Experience 8, Iter 37, disc loss: 0.0046548295643896835, policy loss: 6.430768617002395
Experience 8, Iter 38, disc loss: 0.004823178139861205, policy loss: 6.33513276453459
Experience 8, Iter 39, disc loss: 0.004409964239585271, policy loss: 6.427193961311352
Experience 8, Iter 40, disc loss: 0.00425954130963795, policy loss: 6.5000609404134995
Experience 8, Iter 41, disc loss: 0.004623870427132975, policy loss: 6.416573779437603
Experience 8, Iter 42, disc loss: 0.004659996629168815, policy loss: 6.266130679339028
Experience 8, Iter 43, disc loss: 0.00446700132887699, policy loss: 6.347481725756736
Experience 8, Iter 44, disc loss: 0.004663143116368477, policy loss: 6.309285689804348
Experience 8, Iter 45, disc loss: 0.004717518744465726, policy loss: 6.256421210585999
Experience 8, Iter 46, disc loss: 0.004391729559865873, policy loss: 6.580119717566845
Experience 8, Iter 47, disc loss: 0.004316529762048558, policy loss: 6.427685032909212
Experience 8, Iter 48, disc loss: 0.004369866130428267, policy loss: 6.353901521199108
Experience 8, Iter 49, disc loss: 0.004391032238667162, policy loss: 6.580905236842636
Experience 8, Iter 50, disc loss: 0.0046127049737206965, policy loss: 6.281794300821203
Experience 8, Iter 51, disc loss: 0.004327507863179003, policy loss: 6.452092982810997
Experience 8, Iter 52, disc loss: 0.004104735654034504, policy loss: 6.516338722366446
Experience 8, Iter 53, disc loss: 0.004117365899001171, policy loss: 6.478845199461434
Experience 8, Iter 54, disc loss: 0.004167365307283857, policy loss: 6.521902316591916
Experience 8, Iter 55, disc loss: 0.004322304940625087, policy loss: 6.4975713826883625
Experience 8, Iter 56, disc loss: 0.004017221394054759, policy loss: 6.527072731080272
Experience 8, Iter 57, disc loss: 0.00406696165960051, policy loss: 6.469077114646122
Experience 8, Iter 58, disc loss: 0.003936026933729305, policy loss: 6.514904958007072
Experience 8, Iter 59, disc loss: 0.003952576720910085, policy loss: 6.622225799477347
Experience 8, Iter 60, disc loss: 0.004455965037618328, policy loss: 6.301044976995605
Experience 8, Iter 61, disc loss: 0.003955433417325064, policy loss: 6.658408707923641
Experience 8, Iter 62, disc loss: 0.004080410322818247, policy loss: 6.498374335040715
Experience 8, Iter 63, disc loss: 0.003790363881198273, policy loss: 6.673554522692982
Experience 8, Iter 64, disc loss: 0.003247217460018744, policy loss: 6.918942200870666
Experience 8, Iter 65, disc loss: 0.0033731162554945715, policy loss: 6.897625792846726
Experience 8, Iter 66, disc loss: 0.00392264683025755, policy loss: 6.450170962489433
Experience 8, Iter 67, disc loss: 0.0038140463861973018, policy loss: 6.872927752457572
Experience 8, Iter 68, disc loss: 0.0036180868065033432, policy loss: 6.992453982955537
Experience 8, Iter 69, disc loss: 0.0034655058936211795, policy loss: 6.830799353862803
Experience 8, Iter 70, disc loss: 0.003370994397182539, policy loss: 6.830073122434719
Experience 8, Iter 71, disc loss: 0.0030667931778560805, policy loss: 7.182106169167468
Experience 8, Iter 72, disc loss: 0.003182826152226063, policy loss: 6.872843832684058
Experience 8, Iter 73, disc loss: 0.00372145132400618, policy loss: 6.586074872876355
Experience 8, Iter 74, disc loss: 0.003319636171892474, policy loss: 7.160127469442493
Experience 8, Iter 75, disc loss: 0.0032337941651139636, policy loss: 7.114080217647498
Experience 8, Iter 76, disc loss: 0.003510656645836005, policy loss: 6.677747636640234
Experience 8, Iter 77, disc loss: 0.0031201989536929695, policy loss: 6.929777940667396
Experience 8, Iter 78, disc loss: 0.002885038905894567, policy loss: 7.150912015279939
Experience 8, Iter 79, disc loss: 0.003221230661477673, policy loss: 6.7357234599869535
Experience 8, Iter 80, disc loss: 0.0032113692840086746, policy loss: 6.973655536593403
Experience 8, Iter 81, disc loss: 0.0035587235184138706, policy loss: 6.902631479755246
Experience 8, Iter 82, disc loss: 0.0034423460710678363, policy loss: 6.874782318587661
Experience 8, Iter 83, disc loss: 0.0034076843325798476, policy loss: 6.794366998940509
Experience 8, Iter 84, disc loss: 0.0032567779168394474, policy loss: 6.723682500333392
Experience 8, Iter 85, disc loss: 0.003195583995483779, policy loss: 6.783016572105616
Experience 8, Iter 86, disc loss: 0.0032156824466042875, policy loss: 6.807929137515502
Experience 8, Iter 87, disc loss: 0.003096696045360262, policy loss: 6.950927732597015
Experience 8, Iter 88, disc loss: 0.0032761730844366064, policy loss: 6.77538332575552
Experience 8, Iter 89, disc loss: 0.003222774928527742, policy loss: 6.820522047926683
Experience 8, Iter 90, disc loss: 0.003172829097996791, policy loss: 6.885857704099637
Experience 8, Iter 91, disc loss: 0.003257985821338607, policy loss: 6.7245524895780315
Experience 8, Iter 92, disc loss: 0.003053907028575588, policy loss: 6.81255143203786
Experience 8, Iter 93, disc loss: 0.002899134735877635, policy loss: 7.0178889223248895
Experience 8, Iter 94, disc loss: 0.0030838963473094347, policy loss: 6.824988682380148
Experience 8, Iter 95, disc loss: 0.0031396892697625914, policy loss: 6.745054526417909
Experience 8, Iter 96, disc loss: 0.0033417639905314383, policy loss: 6.74259793258447
Experience 8, Iter 97, disc loss: 0.003255693418651254, policy loss: 6.864641788441137
Experience 8, Iter 98, disc loss: 0.0033177576694458527, policy loss: 6.667306199234929
Experience 8, Iter 99, disc loss: 0.0030558538556600444, policy loss: 6.904052491883768
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.0429],
        [0.2341],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0238, 0.2368, 0.2069, 0.0153, 0.0034, 2.0878]],

        [[0.0238, 0.2368, 0.2069, 0.0153, 0.0034, 2.0878]],

        [[0.0238, 0.2368, 0.2069, 0.0153, 0.0034, 2.0878]],

        [[0.0238, 0.2368, 0.2069, 0.0153, 0.0034, 2.0878]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0269, 0.1717, 0.9362, 0.0195], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0269, 0.1717, 0.9362, 0.0195])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.930
Iter 2/2000 - Loss: 0.985
Iter 3/2000 - Loss: 0.815
Iter 4/2000 - Loss: 0.753
Iter 5/2000 - Loss: 0.775
Iter 6/2000 - Loss: 0.714
Iter 7/2000 - Loss: 0.596
Iter 8/2000 - Loss: 0.502
Iter 9/2000 - Loss: 0.449
Iter 10/2000 - Loss: 0.391
Iter 11/2000 - Loss: 0.298
Iter 12/2000 - Loss: 0.179
Iter 13/2000 - Loss: 0.056
Iter 14/2000 - Loss: -0.064
Iter 15/2000 - Loss: -0.191
Iter 16/2000 - Loss: -0.337
Iter 17/2000 - Loss: -0.508
Iter 18/2000 - Loss: -0.702
Iter 19/2000 - Loss: -0.918
Iter 20/2000 - Loss: -1.154
Iter 1981/2000 - Loss: -8.382
Iter 1982/2000 - Loss: -8.382
Iter 1983/2000 - Loss: -8.382
Iter 1984/2000 - Loss: -8.382
Iter 1985/2000 - Loss: -8.382
Iter 1986/2000 - Loss: -8.382
Iter 1987/2000 - Loss: -8.382
Iter 1988/2000 - Loss: -8.382
Iter 1989/2000 - Loss: -8.382
Iter 1990/2000 - Loss: -8.382
Iter 1991/2000 - Loss: -8.382
Iter 1992/2000 - Loss: -8.382
Iter 1993/2000 - Loss: -8.382
Iter 1994/2000 - Loss: -8.382
Iter 1995/2000 - Loss: -8.382
Iter 1996/2000 - Loss: -8.382
Iter 1997/2000 - Loss: -8.382
Iter 1998/2000 - Loss: -8.382
Iter 1999/2000 - Loss: -8.382
Iter 2000/2000 - Loss: -8.382
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[16.5527,  7.5917, 28.3405, 12.8987,  9.7322, 50.4451]],

        [[20.8298, 22.5658, 11.6220,  2.2347,  1.6495, 18.0912]],

        [[22.1148, 30.9130, 16.3223,  0.9639,  1.5183, 15.5280]],

        [[19.8175, 38.1375,  9.9048,  3.5351,  9.1030, 36.9931]]])
Signal Variance: tensor([0.1079, 1.1619, 9.2257, 0.2870])
Estimated target variance: tensor([0.0269, 0.1717, 0.9362, 0.0195])
N: 90
Signal to noise ratio: tensor([19.8696, 69.1279, 75.5652, 32.2123])
Bound on condition number: tensor([ 35533.2541, 430080.6407, 513909.7948,  93388.0170])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.003053552984505109, policy loss: 6.94369558488582
Experience 9, Iter 1, disc loss: 0.0027491145515887274, policy loss: 7.062268631268415
Experience 9, Iter 2, disc loss: 0.0029357618352834464, policy loss: 6.845846595936944
Experience 9, Iter 3, disc loss: 0.003366182649975841, policy loss: 6.655178598016356
Experience 9, Iter 4, disc loss: 0.002941882133244194, policy loss: 7.351553613197531
Experience 9, Iter 5, disc loss: 0.0031517586748060706, policy loss: 6.819586476802218
Experience 9, Iter 6, disc loss: 0.0029143151507890343, policy loss: 7.095677047952462
Experience 9, Iter 7, disc loss: 0.002830078121441092, policy loss: 7.189978438469565
Experience 9, Iter 8, disc loss: 0.002782591342567562, policy loss: 7.006437121882244
Experience 9, Iter 9, disc loss: 0.002845732125865914, policy loss: 6.931359290559516
Experience 9, Iter 10, disc loss: 0.0030520047138939722, policy loss: 6.875579451789566
Experience 9, Iter 11, disc loss: 0.002716616549601131, policy loss: 7.1596857016879465
Experience 9, Iter 12, disc loss: 0.002973627140378986, policy loss: 6.94613721118154
Experience 9, Iter 13, disc loss: 0.0030179324525836998, policy loss: 6.755564432155626
Experience 9, Iter 14, disc loss: 0.0025542062067931684, policy loss: 7.0583407910048255
Experience 9, Iter 15, disc loss: 0.002988455994182053, policy loss: 6.8150098310706575
Experience 9, Iter 16, disc loss: 0.0029496199463682465, policy loss: 6.875222750563947
Experience 9, Iter 17, disc loss: 0.002956190215531781, policy loss: 6.8795016811512735
Experience 9, Iter 18, disc loss: 0.003019121730928896, policy loss: 6.808468888729126
Experience 9, Iter 19, disc loss: 0.0031019640195151625, policy loss: 6.782841768687
Experience 9, Iter 20, disc loss: 0.002952945881799687, policy loss: 6.847729227417053
Experience 9, Iter 21, disc loss: 0.00287465267233554, policy loss: 6.863365334601704
Experience 9, Iter 22, disc loss: 0.00291656039281434, policy loss: 6.83399788671791
Experience 9, Iter 23, disc loss: 0.0026155570203339994, policy loss: 7.105268885733203
Experience 9, Iter 24, disc loss: 0.002645592078701926, policy loss: 6.9404813019529525
Experience 9, Iter 25, disc loss: 0.002813608171958829, policy loss: 6.972518384784374
Experience 9, Iter 26, disc loss: 0.002807446157968889, policy loss: 6.903279284315746
Experience 9, Iter 27, disc loss: 0.0027648935343593827, policy loss: 6.911980985917506
Experience 9, Iter 28, disc loss: 0.0029130167751415252, policy loss: 6.947943594151265
Experience 9, Iter 29, disc loss: 0.0026961331309170973, policy loss: 7.129939743428092
Experience 9, Iter 30, disc loss: 0.002874255351696404, policy loss: 6.761090295404279
Experience 9, Iter 31, disc loss: 0.002690888010762003, policy loss: 6.9418966336326715
Experience 9, Iter 32, disc loss: 0.0027224516248852597, policy loss: 6.890931789862661
Experience 9, Iter 33, disc loss: 0.002952779470618837, policy loss: 6.969273206631657
Experience 9, Iter 34, disc loss: 0.002668840479699758, policy loss: 6.970632745506817
Experience 9, Iter 35, disc loss: 0.002687914193428607, policy loss: 6.974893739066541
Experience 9, Iter 36, disc loss: 0.002785564258879941, policy loss: 7.014450628171378
Experience 9, Iter 37, disc loss: 0.00258125064955554, policy loss: 6.920273433861766
Experience 9, Iter 38, disc loss: 0.0027946494380140354, policy loss: 6.914245110231188
Experience 9, Iter 39, disc loss: 0.0026907526621090293, policy loss: 7.03331809618588
Experience 9, Iter 40, disc loss: 0.002586244793938366, policy loss: 6.932074128922133
Experience 9, Iter 41, disc loss: 0.002936615287612695, policy loss: 6.922359238632691
Experience 9, Iter 42, disc loss: 0.0025791426761227355, policy loss: 7.040690402321744
Experience 9, Iter 43, disc loss: 0.0024735639431277007, policy loss: 7.101109291210001
Experience 9, Iter 44, disc loss: 0.0026777168422296225, policy loss: 6.965327363640763
Experience 9, Iter 45, disc loss: 0.0027258659900838375, policy loss: 6.88268949291699
Experience 9, Iter 46, disc loss: 0.0027965094069653997, policy loss: 6.862854158093243
Experience 9, Iter 47, disc loss: 0.002979953100054048, policy loss: 6.7300310111121044
Experience 9, Iter 48, disc loss: 0.0026648020334728052, policy loss: 6.948233964565883
Experience 9, Iter 49, disc loss: 0.002608938583031335, policy loss: 6.907772671365056
Experience 9, Iter 50, disc loss: 0.002652673103730492, policy loss: 6.926955003104185
Experience 9, Iter 51, disc loss: 0.002530306089160851, policy loss: 7.129013664635594
Experience 9, Iter 52, disc loss: 0.002448163462165224, policy loss: 7.065041848043209
Experience 9, Iter 53, disc loss: 0.0024281781626494968, policy loss: 6.96415331990279
Experience 9, Iter 54, disc loss: 0.002529745038152977, policy loss: 7.212656530709388
Experience 9, Iter 55, disc loss: 0.0027844356135595634, policy loss: 6.872234831001615
Experience 9, Iter 56, disc loss: 0.0024988595675636957, policy loss: 6.977448272649653
Experience 9, Iter 57, disc loss: 0.0026685856334098504, policy loss: 6.907467971061361
Experience 9, Iter 58, disc loss: 0.0024844645795567694, policy loss: 7.190949833319888
Experience 9, Iter 59, disc loss: 0.002489392162515761, policy loss: 6.956608773162465
Experience 9, Iter 60, disc loss: 0.00279679098613284, policy loss: 6.967615106034693
Experience 9, Iter 61, disc loss: 0.002655663720972776, policy loss: 7.004714154983148
Experience 9, Iter 62, disc loss: 0.0027902788603893862, policy loss: 6.830954885765633
Experience 9, Iter 63, disc loss: 0.0025029803624134683, policy loss: 7.0290793049988585
Experience 9, Iter 64, disc loss: 0.002337102808978486, policy loss: 7.201069652817011
Experience 9, Iter 65, disc loss: 0.0024637015657224494, policy loss: 7.219920002317452
Experience 9, Iter 66, disc loss: 0.002457064296541869, policy loss: 7.125354591689548
Experience 9, Iter 67, disc loss: 0.0025314422315542424, policy loss: 6.9900543888760875
Experience 9, Iter 68, disc loss: 0.002426533081431516, policy loss: 7.117776714279696
Experience 9, Iter 69, disc loss: 0.002319984761937349, policy loss: 7.077008519903677
Experience 9, Iter 70, disc loss: 0.002413474864367442, policy loss: 7.057551859120119
Experience 9, Iter 71, disc loss: 0.0025806652078611946, policy loss: 7.127252963378706
Experience 9, Iter 72, disc loss: 0.0022718357476470647, policy loss: 7.262544174070616
Experience 9, Iter 73, disc loss: 0.002610565609339309, policy loss: 6.924234073934831
Experience 9, Iter 74, disc loss: 0.002531531736921797, policy loss: 6.978184114283012
Experience 9, Iter 75, disc loss: 0.002436235631826652, policy loss: 6.938770881137399
Experience 9, Iter 76, disc loss: 0.0025782831610128417, policy loss: 7.098606736270006
Experience 9, Iter 77, disc loss: 0.002257901049495706, policy loss: 7.302808896220503
Experience 9, Iter 78, disc loss: 0.002227066324142917, policy loss: 7.117475212520198
Experience 9, Iter 79, disc loss: 0.0025229537158872376, policy loss: 7.158925626210793
Experience 9, Iter 80, disc loss: 0.002215568893454502, policy loss: 7.183977940496281
Experience 9, Iter 81, disc loss: 0.0022338140325694786, policy loss: 7.137876816841388
Experience 9, Iter 82, disc loss: 0.002178364081786491, policy loss: 7.133125804056316
Experience 9, Iter 83, disc loss: 0.0022284585478619754, policy loss: 7.25426068295411
Experience 9, Iter 84, disc loss: 0.0022798020884221837, policy loss: 7.14538553455094
Experience 9, Iter 85, disc loss: 0.0023053152458791595, policy loss: 7.138098734334212
Experience 9, Iter 86, disc loss: 0.002376938964590944, policy loss: 7.092126323744809
Experience 9, Iter 87, disc loss: 0.0023193670552331515, policy loss: 7.203681860922034
Experience 9, Iter 88, disc loss: 0.0024728690341294674, policy loss: 6.99350953971824
Experience 9, Iter 89, disc loss: 0.0022339512945164326, policy loss: 7.338984320320062
Experience 9, Iter 90, disc loss: 0.0023168931278783177, policy loss: 7.142525285443166
Experience 9, Iter 91, disc loss: 0.002385483432637011, policy loss: 7.025750469327336
Experience 9, Iter 92, disc loss: 0.0021428682081504166, policy loss: 7.338406139603565
Experience 9, Iter 93, disc loss: 0.002401949614829672, policy loss: 7.349743629362578
Experience 9, Iter 94, disc loss: 0.002393196359176391, policy loss: 6.999979995553428
Experience 9, Iter 95, disc loss: 0.0023678148876231367, policy loss: 7.141254903288852
Experience 9, Iter 96, disc loss: 0.0022600272993555694, policy loss: 7.268530614690455
Experience 9, Iter 97, disc loss: 0.0022422316006307976, policy loss: 7.156303514449686
Experience 9, Iter 98, disc loss: 0.002390321742809882, policy loss: 7.009028637251459
Experience 9, Iter 99, disc loss: 0.002419705397265521, policy loss: 7.26306403322793
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.0443],
        [0.2282],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0228, 0.2318, 0.2097, 0.0146, 0.0032, 2.1405]],

        [[0.0228, 0.2318, 0.2097, 0.0146, 0.0032, 2.1405]],

        [[0.0228, 0.2318, 0.2097, 0.0146, 0.0032, 2.1405]],

        [[0.0228, 0.2318, 0.2097, 0.0146, 0.0032, 2.1405]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0267, 0.1771, 0.9126, 0.0199], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0267, 0.1771, 0.9126, 0.0199])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.969
Iter 2/2000 - Loss: 0.949
Iter 3/2000 - Loss: 0.837
Iter 4/2000 - Loss: 0.766
Iter 5/2000 - Loss: 0.744
Iter 6/2000 - Loss: 0.678
Iter 7/2000 - Loss: 0.573
Iter 8/2000 - Loss: 0.481
Iter 9/2000 - Loss: 0.409
Iter 10/2000 - Loss: 0.328
Iter 11/2000 - Loss: 0.218
Iter 12/2000 - Loss: 0.086
Iter 13/2000 - Loss: -0.050
Iter 14/2000 - Loss: -0.185
Iter 15/2000 - Loss: -0.327
Iter 16/2000 - Loss: -0.492
Iter 17/2000 - Loss: -0.684
Iter 18/2000 - Loss: -0.900
Iter 19/2000 - Loss: -1.132
Iter 20/2000 - Loss: -1.377
Iter 1981/2000 - Loss: -8.476
Iter 1982/2000 - Loss: -8.476
Iter 1983/2000 - Loss: -8.476
Iter 1984/2000 - Loss: -8.476
Iter 1985/2000 - Loss: -8.476
Iter 1986/2000 - Loss: -8.476
Iter 1987/2000 - Loss: -8.476
Iter 1988/2000 - Loss: -8.476
Iter 1989/2000 - Loss: -8.476
Iter 1990/2000 - Loss: -8.476
Iter 1991/2000 - Loss: -8.476
Iter 1992/2000 - Loss: -8.476
Iter 1993/2000 - Loss: -8.476
Iter 1994/2000 - Loss: -8.476
Iter 1995/2000 - Loss: -8.476
Iter 1996/2000 - Loss: -8.477
Iter 1997/2000 - Loss: -8.477
Iter 1998/2000 - Loss: -8.477
Iter 1999/2000 - Loss: -8.477
Iter 2000/2000 - Loss: -8.477
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[15.6803,  7.2341, 28.9812, 11.3501, 10.6911, 49.7965]],

        [[19.7004, 34.0673, 11.7632,  1.6244,  1.6311, 17.4062]],

        [[21.5031, 32.9477, 16.1547,  0.9574,  1.5397, 15.9954]],

        [[19.2149, 36.2434, 11.2754,  3.9295,  9.9026, 38.5121]]])
Signal Variance: tensor([0.0999, 0.8949, 9.0112, 0.3289])
Estimated target variance: tensor([0.0267, 0.1771, 0.9126, 0.0199])
N: 100
Signal to noise ratio: tensor([18.5052, 58.3941, 75.7270, 34.8394])
Bound on condition number: tensor([ 34245.2540, 340988.0062, 573458.6171, 121379.7084])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0021776634837597543, policy loss: 7.330491712103412
Experience 10, Iter 1, disc loss: 0.002122474414367418, policy loss: 7.273536544176701
Experience 10, Iter 2, disc loss: 0.0020092638290057706, policy loss: 7.339858468432304
Experience 10, Iter 3, disc loss: 0.0020665315263783267, policy loss: 7.170509994449409
Experience 10, Iter 4, disc loss: 0.002179688267060034, policy loss: 7.207526277244508
Experience 10, Iter 5, disc loss: 0.0020301948688972262, policy loss: 7.259561707788388
Experience 10, Iter 6, disc loss: 0.002230117042139318, policy loss: 7.102975434771164
Experience 10, Iter 7, disc loss: 0.002136252402385533, policy loss: 7.184945686023227
Experience 10, Iter 8, disc loss: 0.0021696323923105863, policy loss: 7.097255572622016
Experience 10, Iter 9, disc loss: 0.002246941235667029, policy loss: 7.293382852712352
Experience 10, Iter 10, disc loss: 0.0019750869590830105, policy loss: 7.2661124507920745
Experience 10, Iter 11, disc loss: 0.002333519172042534, policy loss: 7.0759901265762855
Experience 10, Iter 12, disc loss: 0.0022923576008115534, policy loss: 7.026664344774413
Experience 10, Iter 13, disc loss: 0.0021240338379849154, policy loss: 7.19466486292223
Experience 10, Iter 14, disc loss: 0.002004908316900686, policy loss: 7.31035206978137
Experience 10, Iter 15, disc loss: 0.002123733237088414, policy loss: 7.144998991939467
Experience 10, Iter 16, disc loss: 0.0020871712668121454, policy loss: 7.187523543179683
Experience 10, Iter 17, disc loss: 0.0020743443388077597, policy loss: 7.2659757353696985
Experience 10, Iter 18, disc loss: 0.0021255183738588122, policy loss: 7.441417976218575
Experience 10, Iter 19, disc loss: 0.002087237050030485, policy loss: 7.163801739261743
Experience 10, Iter 20, disc loss: 0.0019356424982225073, policy loss: 7.400455189044312
Experience 10, Iter 21, disc loss: 0.0020899190610755153, policy loss: 7.222567733886276
Experience 10, Iter 22, disc loss: 0.0021077970840323035, policy loss: 7.347100357882033
Experience 10, Iter 23, disc loss: 0.0019258772978942389, policy loss: 7.384693528186866
Experience 10, Iter 24, disc loss: 0.0023813108315003796, policy loss: 7.063229801768278
Experience 10, Iter 25, disc loss: 0.0022699965623649535, policy loss: 7.149182732010014
Experience 10, Iter 26, disc loss: 0.00227899741468669, policy loss: 7.1699157600271946
Experience 10, Iter 27, disc loss: 0.0020064220919538247, policy loss: 7.320274629244482
Experience 10, Iter 28, disc loss: 0.002092168428109914, policy loss: 7.120460552378726
Experience 10, Iter 29, disc loss: 0.002262884291891072, policy loss: 7.203388002604761
Experience 10, Iter 30, disc loss: 0.002134669755070183, policy loss: 7.212277431832103
Experience 10, Iter 31, disc loss: 0.0019175713431957987, policy loss: 7.348178761347572
Experience 10, Iter 32, disc loss: 0.0019268391475820872, policy loss: 7.467746290886598
Experience 10, Iter 33, disc loss: 0.0021673055919564877, policy loss: 7.150358772298231
Experience 10, Iter 34, disc loss: 0.0019249179159722047, policy loss: 7.396954069083215
Experience 10, Iter 35, disc loss: 0.002117290911131113, policy loss: 7.237514669827357
Experience 10, Iter 36, disc loss: 0.0022336612929278092, policy loss: 7.366254276947139
Experience 10, Iter 37, disc loss: 0.0019654564020393844, policy loss: 7.2910365964105655
Experience 10, Iter 38, disc loss: 0.002129247060948379, policy loss: 7.192611846799316
Experience 10, Iter 39, disc loss: 0.0021172851913161387, policy loss: 7.211142488263681
Experience 10, Iter 40, disc loss: 0.0020025698014482403, policy loss: 7.204487371168128
Experience 10, Iter 41, disc loss: 0.0020971511126987403, policy loss: 7.148157050213188
Experience 10, Iter 42, disc loss: 0.002000119083985057, policy loss: 7.213115765535054
Experience 10, Iter 43, disc loss: 0.001930685724146385, policy loss: 7.278027390355565
Experience 10, Iter 44, disc loss: 0.0021524924385151038, policy loss: 7.12205534806907
Experience 10, Iter 45, disc loss: 0.0021350067135414235, policy loss: 7.278608884373931
Experience 10, Iter 46, disc loss: 0.0019524039259993112, policy loss: 7.419971750773254
Experience 10, Iter 47, disc loss: 0.0019253198311144303, policy loss: 7.402102878124667
Experience 10, Iter 48, disc loss: 0.0019028346864700088, policy loss: 7.331563981901505
Experience 10, Iter 49, disc loss: 0.002100237572304781, policy loss: 7.166500051340957
Experience 10, Iter 50, disc loss: 0.0018144741580495881, policy loss: 7.569209267159405
Experience 10, Iter 51, disc loss: 0.002004034185524578, policy loss: 7.2610172318409045
Experience 10, Iter 52, disc loss: 0.002137917244448088, policy loss: 7.325063776868799
Experience 10, Iter 53, disc loss: 0.0018828761926399451, policy loss: 7.391498243257599
Experience 10, Iter 54, disc loss: 0.0018605127451321991, policy loss: 7.37997883359435
Experience 10, Iter 55, disc loss: 0.0016788012115580297, policy loss: 7.446513039563306
Experience 10, Iter 56, disc loss: 0.002015043285016737, policy loss: 7.2435327463662995
Experience 10, Iter 57, disc loss: 0.001817312924124842, policy loss: 7.270692294050244
Experience 10, Iter 58, disc loss: 0.002008886368509393, policy loss: 7.302920986794443
Experience 10, Iter 59, disc loss: 0.0019721666962280397, policy loss: 7.299032247903227
Experience 10, Iter 60, disc loss: 0.0018342875043463594, policy loss: 7.325601983423779
Experience 10, Iter 61, disc loss: 0.0017457278311439446, policy loss: 7.573876223657491
Experience 10, Iter 62, disc loss: 0.001869812607240411, policy loss: 7.356966158763019
Experience 10, Iter 63, disc loss: 0.0017592632889912151, policy loss: 7.352469009606366
Experience 10, Iter 64, disc loss: 0.0017733618071081392, policy loss: 7.449738539291062
Experience 10, Iter 65, disc loss: 0.002021362726467784, policy loss: 7.312899675965053
Experience 10, Iter 66, disc loss: 0.00187567788217972, policy loss: 7.530932981552596
Experience 10, Iter 67, disc loss: 0.0018831874227048686, policy loss: 7.385299237888387
Experience 10, Iter 68, disc loss: 0.0018688013473016657, policy loss: 7.195443113811865
Experience 10, Iter 69, disc loss: 0.0019298308089869018, policy loss: 7.375765125749071
Experience 10, Iter 70, disc loss: 0.0018855791372670346, policy loss: 7.359651126425477
Experience 10, Iter 71, disc loss: 0.001904089494164049, policy loss: 7.350944794739735
Experience 10, Iter 72, disc loss: 0.0018959493666332442, policy loss: 7.435457180729586
Experience 10, Iter 73, disc loss: 0.0021348443041464375, policy loss: 7.251644792009756
Experience 10, Iter 74, disc loss: 0.0021017352628052164, policy loss: 7.280326146224346
Experience 10, Iter 75, disc loss: 0.0018231711898659471, policy loss: 7.460824695746677
Experience 10, Iter 76, disc loss: 0.0021753134777492485, policy loss: 7.089832287566237
Experience 10, Iter 77, disc loss: 0.0018768654922133553, policy loss: 7.394636482813339
Experience 10, Iter 78, disc loss: 0.001765435833748424, policy loss: 7.451062016614222
Experience 10, Iter 79, disc loss: 0.0017233143609765018, policy loss: 7.477821618132337
Experience 10, Iter 80, disc loss: 0.0020807082985533254, policy loss: 7.161826528894148
Experience 10, Iter 81, disc loss: 0.0020659232204321655, policy loss: 7.172550558061605
Experience 10, Iter 82, disc loss: 0.0022518680888272986, policy loss: 7.216640172188841
Experience 10, Iter 83, disc loss: 0.0020033753644289093, policy loss: 7.464790320202
Experience 10, Iter 84, disc loss: 0.0017788129735774694, policy loss: 7.438557954295976
Experience 10, Iter 85, disc loss: 0.0016892481758831977, policy loss: 7.510889888024861
Experience 10, Iter 86, disc loss: 0.0016878755033867456, policy loss: 7.532322416984769
Experience 10, Iter 87, disc loss: 0.0019836375325356756, policy loss: 7.374398355798595
Experience 10, Iter 88, disc loss: 0.002095887284096297, policy loss: 7.327377505748266
Experience 10, Iter 89, disc loss: 0.0020012908976569594, policy loss: 7.360691311551675
Experience 10, Iter 90, disc loss: 0.0017253671959187099, policy loss: 7.522797953189073
Experience 10, Iter 91, disc loss: 0.0017520780167458545, policy loss: 7.539093741751895
Experience 10, Iter 92, disc loss: 0.001856594068125071, policy loss: 7.40217797428496
Experience 10, Iter 93, disc loss: 0.001609082891826264, policy loss: 7.762356317193422
Experience 10, Iter 94, disc loss: 0.001833157557568141, policy loss: 7.4536271724003615
Experience 10, Iter 95, disc loss: 0.0018960193273747862, policy loss: 7.334291785139037
Experience 10, Iter 96, disc loss: 0.0018634263178148474, policy loss: 7.373883180744087
Experience 10, Iter 97, disc loss: 0.001721424661967527, policy loss: 7.547547490921364
Experience 10, Iter 98, disc loss: 0.002062297210937702, policy loss: 7.361335018629368
Experience 10, Iter 99, disc loss: 0.001659481732439961, policy loss: 7.603051565443794
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0065],
        [0.0448],
        [0.2167],
        [0.0049]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0210, 0.2231, 0.2047, 0.0143, 0.0030, 2.1701]],

        [[0.0210, 0.2231, 0.2047, 0.0143, 0.0030, 2.1701]],

        [[0.0210, 0.2231, 0.2047, 0.0143, 0.0030, 2.1701]],

        [[0.0210, 0.2231, 0.2047, 0.0143, 0.0030, 2.1701]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0259, 0.1791, 0.8666, 0.0196], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0259, 0.1791, 0.8666, 0.0196])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.970
Iter 2/2000 - Loss: 0.894
Iter 3/2000 - Loss: 0.813
Iter 4/2000 - Loss: 0.738
Iter 5/2000 - Loss: 0.694
Iter 6/2000 - Loss: 0.626
Iter 7/2000 - Loss: 0.528
Iter 8/2000 - Loss: 0.429
Iter 9/2000 - Loss: 0.339
Iter 10/2000 - Loss: 0.243
Iter 11/2000 - Loss: 0.127
Iter 12/2000 - Loss: -0.008
Iter 13/2000 - Loss: -0.155
Iter 14/2000 - Loss: -0.307
Iter 15/2000 - Loss: -0.467
Iter 16/2000 - Loss: -0.642
Iter 17/2000 - Loss: -0.840
Iter 18/2000 - Loss: -1.062
Iter 19/2000 - Loss: -1.303
Iter 20/2000 - Loss: -1.556
Iter 1981/2000 - Loss: -8.603
Iter 1982/2000 - Loss: -8.603
Iter 1983/2000 - Loss: -8.603
Iter 1984/2000 - Loss: -8.603
Iter 1985/2000 - Loss: -8.603
Iter 1986/2000 - Loss: -8.603
Iter 1987/2000 - Loss: -8.603
Iter 1988/2000 - Loss: -8.603
Iter 1989/2000 - Loss: -8.603
Iter 1990/2000 - Loss: -8.603
Iter 1991/2000 - Loss: -8.603
Iter 1992/2000 - Loss: -8.603
Iter 1993/2000 - Loss: -8.604
Iter 1994/2000 - Loss: -8.604
Iter 1995/2000 - Loss: -8.604
Iter 1996/2000 - Loss: -8.604
Iter 1997/2000 - Loss: -8.604
Iter 1998/2000 - Loss: -8.604
Iter 1999/2000 - Loss: -8.604
Iter 2000/2000 - Loss: -8.604
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0003]])
Lengthscale: tensor([[[15.1696,  7.2405, 28.6170, 11.1470, 10.5173, 49.6828]],

        [[18.5405, 33.6611, 12.5427,  1.4483,  1.4880, 17.1541]],

        [[19.7788, 31.8807, 14.8098,  0.9913,  1.5576, 16.0420]],

        [[18.1808, 35.4688, 10.5528,  3.8857,  8.8050, 36.6150]]])
Signal Variance: tensor([0.0990, 0.8301, 8.9116, 0.2965])
Estimated target variance: tensor([0.0259, 0.1791, 0.8666, 0.0196])
N: 110
Signal to noise ratio: tensor([18.3485, 56.3075, 77.6740, 33.2254])
Bound on condition number: tensor([ 37034.3727, 348759.4255, 663658.3115, 121432.8785])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0020195067585246254, policy loss: 7.308170803102647
Experience 11, Iter 1, disc loss: 0.001889108557753033, policy loss: 7.473651055995484
Experience 11, Iter 2, disc loss: 0.0015941308503760057, policy loss: 7.5702092017757705
Experience 11, Iter 3, disc loss: 0.0020554682726496303, policy loss: 7.347007665006538
Experience 11, Iter 4, disc loss: 0.0018470152006294712, policy loss: 7.355404422329593
Experience 11, Iter 5, disc loss: 0.0017607609640971095, policy loss: 7.533236647052645
Experience 11, Iter 6, disc loss: 0.001578524322991666, policy loss: 7.901787120939322
Experience 11, Iter 7, disc loss: 0.0019648069793865453, policy loss: 7.267905131136201
Experience 11, Iter 8, disc loss: 0.0017406950799949395, policy loss: 7.602857310718172
Experience 11, Iter 9, disc loss: 0.0019997713312185864, policy loss: 7.315379690347221
Experience 11, Iter 10, disc loss: 0.00167011225062709, policy loss: 7.439574584514103
Experience 11, Iter 11, disc loss: 0.001520151412959313, policy loss: 7.739154897409705
Experience 11, Iter 12, disc loss: 0.0015655911564365524, policy loss: 7.735340033650589
Experience 11, Iter 13, disc loss: 0.0018995267792993895, policy loss: 7.51528063184022
Experience 11, Iter 14, disc loss: 0.001641772066835729, policy loss: 7.598436936475084
Experience 11, Iter 15, disc loss: 0.0020414654919698716, policy loss: 7.471698137940715
Experience 11, Iter 16, disc loss: 0.002035647108329371, policy loss: 7.3642439036766625
Experience 11, Iter 17, disc loss: 0.001653821882393235, policy loss: 7.44972493177944
Experience 11, Iter 18, disc loss: 0.001581114158251384, policy loss: 7.579827464556871
Experience 11, Iter 19, disc loss: 0.0016795038276016432, policy loss: 7.530037952346905
Experience 11, Iter 20, disc loss: 0.0016115166275133016, policy loss: 7.560978574241234
Experience 11, Iter 21, disc loss: 0.0018279517409713916, policy loss: 7.484837442669578
Experience 11, Iter 22, disc loss: 0.001952663287076778, policy loss: 7.522553483049496
Experience 11, Iter 23, disc loss: 0.0017125991703213508, policy loss: 7.474462013792717
Experience 11, Iter 24, disc loss: 0.0016093823382390107, policy loss: 7.454724445011129
Experience 11, Iter 25, disc loss: 0.0015057425324306603, policy loss: 7.689421635994382
Experience 11, Iter 26, disc loss: 0.002092933706426083, policy loss: 7.4406447397402395
Experience 11, Iter 27, disc loss: 0.0017931237660845545, policy loss: 7.643422422710852
Experience 11, Iter 28, disc loss: 0.0016901301152097846, policy loss: 7.472335920614566
Experience 11, Iter 29, disc loss: 0.0017024614571581307, policy loss: 7.397493756790748
Experience 11, Iter 30, disc loss: 0.001642376532936154, policy loss: 7.695125492364898
Experience 11, Iter 31, disc loss: 0.001868767344669293, policy loss: 7.417120380478785
Experience 11, Iter 32, disc loss: 0.0019799294256523, policy loss: 7.346623148066439
Experience 11, Iter 33, disc loss: 0.0017845472692126346, policy loss: 7.562814084329474
Experience 11, Iter 34, disc loss: 0.0016341780886276196, policy loss: 7.591229791605137
Experience 11, Iter 35, disc loss: 0.0018900855066359434, policy loss: 7.303429411405078
Experience 11, Iter 36, disc loss: 0.001853041495026169, policy loss: 7.65940268523531
Experience 11, Iter 37, disc loss: 0.001435749185855999, policy loss: 7.564958982709113
Experience 11, Iter 38, disc loss: 0.001854643274821749, policy loss: 7.54710863665799
Experience 11, Iter 39, disc loss: 0.0017681906543882083, policy loss: 7.479463985763231
Experience 11, Iter 40, disc loss: 0.0016524946214009923, policy loss: 7.53406246269233
Experience 11, Iter 41, disc loss: 0.0017933992103061876, policy loss: 7.560942516549446
Experience 11, Iter 42, disc loss: 0.0019122170214864243, policy loss: 7.749427277880967
Experience 11, Iter 43, disc loss: 0.0014919207619799343, policy loss: 7.92734009377811
Experience 11, Iter 44, disc loss: 0.0017924316182159014, policy loss: 7.508148325455393
Experience 11, Iter 45, disc loss: 0.0018792461717811967, policy loss: 7.54306231417621
Experience 11, Iter 46, disc loss: 0.0016432965780131628, policy loss: 7.599466535070821
Experience 11, Iter 47, disc loss: 0.0017465034288697695, policy loss: 7.557770076805626
Experience 11, Iter 48, disc loss: 0.0016863961724818562, policy loss: 7.766075970117605
Experience 11, Iter 49, disc loss: 0.0018115608854348156, policy loss: 7.632944432497473
Experience 11, Iter 50, disc loss: 0.0014770226194856725, policy loss: 7.6901037589934464
Experience 11, Iter 51, disc loss: 0.0014983729303036113, policy loss: 7.72956151798085
Experience 11, Iter 52, disc loss: 0.001614796256499348, policy loss: 7.600974545915278
Experience 11, Iter 53, disc loss: 0.0018160644473250295, policy loss: 7.58001387388896
Experience 11, Iter 54, disc loss: 0.0016271765370386888, policy loss: 7.749920979352427
Experience 11, Iter 55, disc loss: 0.0016223574055840668, policy loss: 7.573214108071192
Experience 11, Iter 56, disc loss: 0.00152485845008366, policy loss: 7.762484147160341
Experience 11, Iter 57, disc loss: 0.0016745435786379996, policy loss: 7.609904186357669
Experience 11, Iter 58, disc loss: 0.002065157536035535, policy loss: 7.445970959484053
Experience 11, Iter 59, disc loss: 0.001577169469529873, policy loss: 7.563917540555279
Experience 11, Iter 60, disc loss: 0.0015834432426233718, policy loss: 7.682198273848679
Experience 11, Iter 61, disc loss: 0.0016376570323764909, policy loss: 7.601751442449206
Experience 11, Iter 62, disc loss: 0.0015962986441649153, policy loss: 7.533950240894749
Experience 11, Iter 63, disc loss: 0.0022026552877152045, policy loss: 7.276873337925783
Experience 11, Iter 64, disc loss: 0.0015456062485138813, policy loss: 7.707165070063176
Experience 11, Iter 65, disc loss: 0.0021247735995376152, policy loss: 7.273677756240374
Experience 11, Iter 66, disc loss: 0.0015874962831685046, policy loss: 7.622066432311028
Experience 11, Iter 67, disc loss: 0.0015198047007013971, policy loss: 7.762003119605267
Experience 11, Iter 68, disc loss: 0.0017200335223349762, policy loss: 7.578281916252536
Experience 11, Iter 69, disc loss: 0.001626527015207061, policy loss: 7.571020355834652
Experience 11, Iter 70, disc loss: 0.0015071026401373736, policy loss: 7.825130996821722
Experience 11, Iter 71, disc loss: 0.0023592519436384223, policy loss: 7.333879534644668
Experience 11, Iter 72, disc loss: 0.0018974107912731466, policy loss: 7.439558775584284
Experience 11, Iter 73, disc loss: 0.0015140786483224384, policy loss: 7.844652979728004
Experience 11, Iter 74, disc loss: 0.0017859787696855492, policy loss: 7.551337087128484
Experience 11, Iter 75, disc loss: 0.001561490074408465, policy loss: 7.9223439826876385
Experience 11, Iter 76, disc loss: 0.0018688080470595783, policy loss: 7.510619898870398
Experience 11, Iter 77, disc loss: 0.0018298306362306042, policy loss: 7.5664633329954345
Experience 11, Iter 78, disc loss: 0.002015171930184694, policy loss: 7.746119917590761
Experience 11, Iter 79, disc loss: 0.0022121373843394937, policy loss: 7.486711171193738
Experience 11, Iter 80, disc loss: 0.0018944526306504505, policy loss: 7.4522210047059705
Experience 11, Iter 81, disc loss: 0.0019032841058139658, policy loss: 7.413212665008043
Experience 11, Iter 82, disc loss: 0.0021301345956533705, policy loss: 7.465649536974145
Experience 11, Iter 83, disc loss: 0.0018575556790926852, policy loss: 7.606919427811448
Experience 11, Iter 84, disc loss: 0.0020228210173663507, policy loss: 7.573661132188564
Experience 11, Iter 85, disc loss: 0.002068470400287581, policy loss: 7.751288735175242
Experience 11, Iter 86, disc loss: 0.002657617181329909, policy loss: 7.505875996596077
Experience 11, Iter 87, disc loss: 0.0017026602806571264, policy loss: 7.875509765575448
Experience 11, Iter 88, disc loss: 0.0018209811941361713, policy loss: 7.883328782889649
Experience 11, Iter 89, disc loss: 0.00178154714335394, policy loss: 7.727816207783911
Experience 11, Iter 90, disc loss: 0.001956761582084089, policy loss: 7.7952766199162795
Experience 11, Iter 91, disc loss: 0.0019227857330208899, policy loss: 7.730166319635434
Experience 11, Iter 92, disc loss: 0.0019108369861745673, policy loss: 7.785192798752842
Experience 11, Iter 93, disc loss: 0.0016287325199116863, policy loss: 7.827734565106054
Experience 11, Iter 94, disc loss: 0.0022027297470655375, policy loss: 7.570468362946102
Experience 11, Iter 95, disc loss: 0.002418185242923645, policy loss: 7.691909947423036
Experience 11, Iter 96, disc loss: 0.00186300363475632, policy loss: 7.863358312461278
Experience 11, Iter 97, disc loss: 0.001569797346251066, policy loss: 7.9228117228767845
Experience 11, Iter 98, disc loss: 0.0020897169832772475, policy loss: 7.806657183153204
Experience 11, Iter 99, disc loss: 0.0016977769961305674, policy loss: 7.884845715898857
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.0464],
        [0.2170],
        [0.0048]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0206, 0.2291, 0.2015, 0.0150, 0.0033, 2.2751]],

        [[0.0206, 0.2291, 0.2015, 0.0150, 0.0033, 2.2751]],

        [[0.0206, 0.2291, 0.2015, 0.0150, 0.0033, 2.2751]],

        [[0.0206, 0.2291, 0.2015, 0.0150, 0.0033, 2.2751]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0267, 0.1854, 0.8678, 0.0194], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0267, 0.1854, 0.8678, 0.0194])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.040
Iter 2/2000 - Loss: 0.897
Iter 3/2000 - Loss: 0.846
Iter 4/2000 - Loss: 0.762
Iter 5/2000 - Loss: 0.705
Iter 6/2000 - Loss: 0.637
Iter 7/2000 - Loss: 0.549
Iter 8/2000 - Loss: 0.444
Iter 9/2000 - Loss: 0.333
Iter 10/2000 - Loss: 0.219
Iter 11/2000 - Loss: 0.100
Iter 12/2000 - Loss: -0.031
Iter 13/2000 - Loss: -0.182
Iter 14/2000 - Loss: -0.352
Iter 15/2000 - Loss: -0.539
Iter 16/2000 - Loss: -0.739
Iter 17/2000 - Loss: -0.955
Iter 18/2000 - Loss: -1.188
Iter 19/2000 - Loss: -1.440
Iter 20/2000 - Loss: -1.708
Iter 1981/2000 - Loss: -8.547
Iter 1982/2000 - Loss: -8.547
Iter 1983/2000 - Loss: -8.547
Iter 1984/2000 - Loss: -8.547
Iter 1985/2000 - Loss: -8.547
Iter 1986/2000 - Loss: -8.547
Iter 1987/2000 - Loss: -8.547
Iter 1988/2000 - Loss: -8.547
Iter 1989/2000 - Loss: -8.547
Iter 1990/2000 - Loss: -8.547
Iter 1991/2000 - Loss: -8.547
Iter 1992/2000 - Loss: -8.547
Iter 1993/2000 - Loss: -8.547
Iter 1994/2000 - Loss: -8.547
Iter 1995/2000 - Loss: -8.548
Iter 1996/2000 - Loss: -8.548
Iter 1997/2000 - Loss: -8.548
Iter 1998/2000 - Loss: -8.548
Iter 1999/2000 - Loss: -8.548
Iter 2000/2000 - Loss: -8.548
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.1683,  7.3248, 27.3823, 11.0439, 10.2749, 49.4715]],

        [[17.2428, 34.8921, 12.1366,  1.3772,  1.7511, 17.8420]],

        [[18.4711, 36.4550, 14.3479,  0.9551,  1.5419, 16.0069]],

        [[17.4818, 37.3879, 12.1723,  4.1809,  8.0426, 39.9199]]])
Signal Variance: tensor([0.1009, 0.8584, 8.1542, 0.3527])
Estimated target variance: tensor([0.0267, 0.1854, 0.8678, 0.0194])
N: 120
Signal to noise ratio: tensor([17.8738, 57.1060, 69.1047, 34.2908])
Bound on condition number: tensor([ 38337.5956, 391332.2216, 573056.3888, 141104.3373])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0019191767198387445, policy loss: 7.8027151430527635
Experience 12, Iter 1, disc loss: 0.0021666751301438144, policy loss: 7.702413709532299
Experience 12, Iter 2, disc loss: 0.002033223374666708, policy loss: 7.822418213815478
Experience 12, Iter 3, disc loss: 0.002573338209961223, policy loss: 7.39775258982202
Experience 12, Iter 4, disc loss: 0.0020095601918337096, policy loss: 7.787162710486326
Experience 12, Iter 5, disc loss: 0.0015783127418357889, policy loss: 7.9951657552547815
Experience 12, Iter 6, disc loss: 0.0021915145779017164, policy loss: 7.69746064167435
Experience 12, Iter 7, disc loss: 0.0024897013709348982, policy loss: 7.509316282654387
Experience 12, Iter 8, disc loss: 0.0019040268089379096, policy loss: 7.920461142502173
Experience 12, Iter 9, disc loss: 0.002625344376861377, policy loss: 7.837454391721107
Experience 12, Iter 10, disc loss: 0.0025560506887886193, policy loss: 7.426789857695543
Experience 12, Iter 11, disc loss: 0.002165623866995407, policy loss: 7.761112485244478
Experience 12, Iter 12, disc loss: 0.002455691726189372, policy loss: 7.621398738621778
Experience 12, Iter 13, disc loss: 0.0029574310431268232, policy loss: 7.9273250872766
Experience 12, Iter 14, disc loss: 0.0024165090722345723, policy loss: 7.82276232881597
Experience 12, Iter 15, disc loss: 0.0038318305412801713, policy loss: 7.4662872654187655
Experience 12, Iter 16, disc loss: 0.00286402657544391, policy loss: 7.815613776934445
Experience 12, Iter 17, disc loss: 0.003959263942098637, policy loss: 7.680467572989306
Experience 12, Iter 18, disc loss: 0.004915665843420326, policy loss: 7.154580821658261
Experience 12, Iter 19, disc loss: 0.0026384133962480964, policy loss: 7.748490655394345
Experience 12, Iter 20, disc loss: 0.0035424764630037357, policy loss: 7.525533391265595
Experience 12, Iter 21, disc loss: 0.0034977641836974405, policy loss: 7.720341871768072
Experience 12, Iter 22, disc loss: 0.004514157602991682, policy loss: 7.351662070158372
Experience 12, Iter 23, disc loss: 0.003430192413653156, policy loss: 7.551534861774849
Experience 12, Iter 24, disc loss: 0.006271110075134574, policy loss: 7.206726838418517
Experience 12, Iter 25, disc loss: 0.004995123813372458, policy loss: 7.29388335392259
Experience 12, Iter 26, disc loss: 0.00788260864911575, policy loss: 6.864503355646846
Experience 12, Iter 27, disc loss: 0.009006607142751352, policy loss: 6.448863359510928
Experience 12, Iter 28, disc loss: 0.00691829251074496, policy loss: 6.825176471957274
Experience 12, Iter 29, disc loss: 0.009732840499910331, policy loss: 6.3502389177671565
Experience 12, Iter 30, disc loss: 0.010571834708960873, policy loss: 5.935055745275589
Experience 12, Iter 31, disc loss: 0.0112923230796607, policy loss: 5.768405888094188
Experience 12, Iter 32, disc loss: 0.009841758592821288, policy loss: 5.919910643379545
Experience 12, Iter 33, disc loss: 0.011065370344208946, policy loss: 5.758117960623846
Experience 12, Iter 34, disc loss: 0.008674823883697828, policy loss: 5.812857976631295
Experience 12, Iter 35, disc loss: 0.011601690757007045, policy loss: 5.4316998504570115
Experience 12, Iter 36, disc loss: 0.013034718828131009, policy loss: 5.220071943313378
Experience 12, Iter 37, disc loss: 0.015050951776198949, policy loss: 5.062592070603139
Experience 12, Iter 38, disc loss: 0.014626688943818057, policy loss: 5.103773800333423
Experience 12, Iter 39, disc loss: 0.012901612026359411, policy loss: 5.233077594238761
Experience 12, Iter 40, disc loss: 0.014203907245792603, policy loss: 5.05188025070108
Experience 12, Iter 41, disc loss: 0.015075801193800724, policy loss: 4.806578379321708
Experience 12, Iter 42, disc loss: 0.013929747574028158, policy loss: 4.818944935102901
Experience 12, Iter 43, disc loss: 0.015066803949833396, policy loss: 4.931953401627992
Experience 12, Iter 44, disc loss: 0.014858283257426292, policy loss: 5.041376393131832
Experience 12, Iter 45, disc loss: 0.01638521115155639, policy loss: 4.9347784661512115
Experience 12, Iter 46, disc loss: 0.013015204996866845, policy loss: 5.129364276479357
Experience 12, Iter 47, disc loss: 0.015273994337747343, policy loss: 4.937415200741974
Experience 12, Iter 48, disc loss: 0.014944083066584287, policy loss: 5.1217395984568475
Experience 12, Iter 49, disc loss: 0.014485014850788501, policy loss: 5.023935005901478
Experience 12, Iter 50, disc loss: 0.018127549593260857, policy loss: 5.083836469845516
Experience 12, Iter 51, disc loss: 0.013190958560439626, policy loss: 5.371850837862464
Experience 12, Iter 52, disc loss: 0.014442332946164376, policy loss: 5.306738049207236
Experience 12, Iter 53, disc loss: 0.01321204421796908, policy loss: 5.6683836502320135
Experience 12, Iter 54, disc loss: 0.015006312449157297, policy loss: 5.616865945364186
Experience 12, Iter 55, disc loss: 0.012811577606808655, policy loss: 5.806400598054891
Experience 12, Iter 56, disc loss: 0.014966812325407636, policy loss: 5.739263061260251
Experience 12, Iter 57, disc loss: 0.01780178711924825, policy loss: 5.357140670917532
Experience 12, Iter 58, disc loss: 0.015277766909988359, policy loss: 5.6141264129725545
Experience 12, Iter 59, disc loss: 0.014560778877503977, policy loss: 5.663792424511198
Experience 12, Iter 60, disc loss: 0.015668747284909683, policy loss: 5.587645141578464
Experience 12, Iter 61, disc loss: 0.016094158046471053, policy loss: 5.738882423245721
Experience 12, Iter 62, disc loss: 0.013216924311560923, policy loss: 5.823748821780936
Experience 12, Iter 63, disc loss: 0.014432190058547883, policy loss: 5.574855591860001
Experience 12, Iter 64, disc loss: 0.014108158240028038, policy loss: 5.538805993579071
Experience 12, Iter 65, disc loss: 0.014890147200088584, policy loss: 5.445408847409238
Experience 12, Iter 66, disc loss: 0.012262673391736017, policy loss: 5.912539591325031
Experience 12, Iter 67, disc loss: 0.01739786246630592, policy loss: 5.52532077331235
Experience 12, Iter 68, disc loss: 0.013536260166996633, policy loss: 5.593500745532161
Experience 12, Iter 69, disc loss: 0.014029126118170919, policy loss: 5.482992711441947
Experience 12, Iter 70, disc loss: 0.012947103982689778, policy loss: 5.559521859373815
Experience 12, Iter 71, disc loss: 0.014439395626071591, policy loss: 5.939589847856066
Experience 12, Iter 72, disc loss: 0.011624398674681291, policy loss: 5.750236055920746
Experience 12, Iter 73, disc loss: 0.010822812306340227, policy loss: 5.987840077668213
Experience 12, Iter 74, disc loss: 0.014684169785901674, policy loss: 5.932028147033694
Experience 12, Iter 75, disc loss: 0.012979291024669569, policy loss: 5.8409199062877715
Experience 12, Iter 76, disc loss: 0.015267395917103025, policy loss: 5.798965376830688
Experience 12, Iter 77, disc loss: 0.010743087315284481, policy loss: 6.027707890930182
Experience 12, Iter 78, disc loss: 0.010026193102687722, policy loss: 6.1212501154845445
Experience 12, Iter 79, disc loss: 0.009456287950633822, policy loss: 6.216491843226996
Experience 12, Iter 80, disc loss: 0.010240372449363887, policy loss: 6.004492908117205
Experience 12, Iter 81, disc loss: 0.010954065792897653, policy loss: 6.098423141212941
Experience 12, Iter 82, disc loss: 0.009803498322133786, policy loss: 6.24163054226419
Experience 12, Iter 83, disc loss: 0.010749483402173198, policy loss: 5.972720823504967
Experience 12, Iter 84, disc loss: 0.011124556044878619, policy loss: 6.087812193736341
Experience 12, Iter 85, disc loss: 0.007966373177806055, policy loss: 6.423539635943586
Experience 12, Iter 86, disc loss: 0.00864180486669904, policy loss: 6.388317420529798
Experience 12, Iter 87, disc loss: 0.01086013673156732, policy loss: 6.470558506542103
Experience 12, Iter 88, disc loss: 0.008161361216751047, policy loss: 6.6430854634759555
Experience 12, Iter 89, disc loss: 0.009079926307861573, policy loss: 6.790883940983593
Experience 12, Iter 90, disc loss: 0.012814700754491065, policy loss: 6.048363019019394
Experience 12, Iter 91, disc loss: 0.00893697728795534, policy loss: 6.480695375373562
Experience 12, Iter 92, disc loss: 0.008173451336678135, policy loss: 6.497646825534318
Experience 12, Iter 93, disc loss: 0.0065951778141558805, policy loss: 7.031937700977215
Experience 12, Iter 94, disc loss: 0.008690700763066314, policy loss: 6.704426052130079
Experience 12, Iter 95, disc loss: 0.00848352319572002, policy loss: 6.544614024221419
Experience 12, Iter 96, disc loss: 0.00705139616809005, policy loss: 6.623283702006373
Experience 12, Iter 97, disc loss: 0.006626170031913181, policy loss: 6.782304332887227
Experience 12, Iter 98, disc loss: 0.00789987964752022, policy loss: 6.826793339128785
Experience 12, Iter 99, disc loss: 0.020979020272044503, policy loss: 6.576665489693852
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.0605],
        [0.4458],
        [0.0098]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0202, 0.2322, 0.4283, 0.0174, 0.0068, 2.5942]],

        [[0.0202, 0.2322, 0.4283, 0.0174, 0.0068, 2.5942]],

        [[0.0202, 0.2322, 0.4283, 0.0174, 0.0068, 2.5942]],

        [[0.0202, 0.2322, 0.4283, 0.0174, 0.0068, 2.5942]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0268, 0.2419, 1.7833, 0.0392], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0268, 0.2419, 1.7833, 0.0392])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.828
Iter 2/2000 - Loss: 1.704
Iter 3/2000 - Loss: 1.623
Iter 4/2000 - Loss: 1.570
Iter 5/2000 - Loss: 1.509
Iter 6/2000 - Loss: 1.413
Iter 7/2000 - Loss: 1.319
Iter 8/2000 - Loss: 1.228
Iter 9/2000 - Loss: 1.120
Iter 10/2000 - Loss: 0.989
Iter 11/2000 - Loss: 0.845
Iter 12/2000 - Loss: 0.694
Iter 13/2000 - Loss: 0.535
Iter 14/2000 - Loss: 0.360
Iter 15/2000 - Loss: 0.163
Iter 16/2000 - Loss: -0.055
Iter 17/2000 - Loss: -0.290
Iter 18/2000 - Loss: -0.538
Iter 19/2000 - Loss: -0.796
Iter 20/2000 - Loss: -1.063
Iter 1981/2000 - Loss: -8.088
Iter 1982/2000 - Loss: -8.088
Iter 1983/2000 - Loss: -8.088
Iter 1984/2000 - Loss: -8.088
Iter 1985/2000 - Loss: -8.088
Iter 1986/2000 - Loss: -8.088
Iter 1987/2000 - Loss: -8.088
Iter 1988/2000 - Loss: -8.088
Iter 1989/2000 - Loss: -8.088
Iter 1990/2000 - Loss: -8.088
Iter 1991/2000 - Loss: -8.088
Iter 1992/2000 - Loss: -8.088
Iter 1993/2000 - Loss: -8.088
Iter 1994/2000 - Loss: -8.088
Iter 1995/2000 - Loss: -8.088
Iter 1996/2000 - Loss: -8.088
Iter 1997/2000 - Loss: -8.088
Iter 1998/2000 - Loss: -8.088
Iter 1999/2000 - Loss: -8.088
Iter 2000/2000 - Loss: -8.088
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[14.2295,  7.8349, 35.0065, 11.9348, 14.3975, 51.2122]],

        [[19.1339, 37.3488,  9.9229,  1.3033,  3.5186, 23.9170]],

        [[19.6858, 37.0274,  9.4109,  1.0937,  2.0032, 19.8248]],

        [[15.8837, 29.6446, 17.5166,  4.2644,  1.8784, 38.1276]]])
Signal Variance: tensor([ 0.1040,  1.9010, 12.9396,  0.5637])
Estimated target variance: tensor([0.0268, 0.2419, 1.7833, 0.0392])
N: 130
Signal to noise ratio: tensor([17.7548, 85.6185, 85.7694, 43.3041])
Bound on condition number: tensor([ 40981.3699, 952970.3966, 956331.9935, 243783.1644])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.004882649164192771, policy loss: 7.680063880095698
Experience 13, Iter 1, disc loss: 0.0060516936049837405, policy loss: 7.2724358082812195
Experience 13, Iter 2, disc loss: 0.005245715556229514, policy loss: 7.80921373940423
Experience 13, Iter 3, disc loss: 0.005403504608023537, policy loss: 7.411053943204093
Experience 13, Iter 4, disc loss: 0.005037528753502718, policy loss: 7.503364172391104
Experience 13, Iter 5, disc loss: 0.008502448169657206, policy loss: 7.030476459155379
Experience 13, Iter 6, disc loss: 0.009317790040300299, policy loss: 6.71504970994426
Experience 13, Iter 7, disc loss: 0.0072377511477720174, policy loss: 6.489203634624534
Experience 13, Iter 8, disc loss: 0.012387217717786813, policy loss: 6.050906271426725
Experience 13, Iter 9, disc loss: 0.007230086678756215, policy loss: 6.5159244459230905
Experience 13, Iter 10, disc loss: 0.007675619757653107, policy loss: 6.424516692154128
Experience 13, Iter 11, disc loss: 0.009240198680954565, policy loss: 6.529652007684552
Experience 13, Iter 12, disc loss: 0.009991505499128344, policy loss: 6.2990789809924035
Experience 13, Iter 13, disc loss: 0.009209769152135991, policy loss: 6.531634556601026
Experience 13, Iter 14, disc loss: 0.006639585592497163, policy loss: 6.68163961406489
Experience 13, Iter 15, disc loss: 0.0070508376746788465, policy loss: 6.452373623664018
Experience 13, Iter 16, disc loss: 0.007322191982401964, policy loss: 6.4309111754035335
Experience 13, Iter 17, disc loss: 0.010307729595526269, policy loss: 6.167375412948887
Experience 13, Iter 18, disc loss: 0.00879015518650117, policy loss: 6.1611443238823895
Experience 13, Iter 19, disc loss: 0.018862561759754617, policy loss: 5.805738645241115
Experience 13, Iter 20, disc loss: 0.00824916953909513, policy loss: 6.028977190150805
Experience 13, Iter 21, disc loss: 0.008386401696198614, policy loss: 6.307435749751267
Experience 13, Iter 22, disc loss: 0.01168762241224068, policy loss: 6.550469798899708
Experience 13, Iter 23, disc loss: 0.00938900667189133, policy loss: 6.417322027899891
Experience 13, Iter 24, disc loss: 0.00821279413222652, policy loss: 6.532092458634637
Experience 13, Iter 25, disc loss: 0.007311593440979421, policy loss: 6.533092011836203
Experience 13, Iter 26, disc loss: 0.0081622652081304, policy loss: 6.592989469583639
Experience 13, Iter 27, disc loss: 0.008525001126708644, policy loss: 6.484497553270892
Experience 13, Iter 28, disc loss: 0.008917655356528095, policy loss: 6.380231951266955
Experience 13, Iter 29, disc loss: 0.007882328968897208, policy loss: 6.610390441361084
Experience 13, Iter 30, disc loss: 0.009050653473133575, policy loss: 6.422504474419856
Experience 13, Iter 31, disc loss: 0.006447174360251181, policy loss: 6.827793451637996
Experience 13, Iter 32, disc loss: 0.0070148510253128064, policy loss: 6.618043973563525
Experience 13, Iter 33, disc loss: 0.007140753280474618, policy loss: 6.335993065081775
Experience 13, Iter 34, disc loss: 0.00691437548666624, policy loss: 6.613816110423124
Experience 13, Iter 35, disc loss: 0.0077276875531334995, policy loss: 6.472763256106291
Experience 13, Iter 36, disc loss: 0.007914857312153235, policy loss: 6.300956154644435
Experience 13, Iter 37, disc loss: 0.006537723246866464, policy loss: 6.767208835335351
Experience 13, Iter 38, disc loss: 0.006961783038454354, policy loss: 6.510451889754625
Experience 13, Iter 39, disc loss: 0.007144396810637404, policy loss: 6.5984048655219
Experience 13, Iter 40, disc loss: 0.007499521063808146, policy loss: 6.263819376656771
Experience 13, Iter 41, disc loss: 0.010069699768330626, policy loss: 6.4114347675455665
Experience 13, Iter 42, disc loss: 0.010197422210201337, policy loss: 6.239874617835682
Experience 13, Iter 43, disc loss: 0.008627131716299787, policy loss: 5.873712332867121
Experience 13, Iter 44, disc loss: 0.009637189511122243, policy loss: 5.938527088742573
Experience 13, Iter 45, disc loss: 0.010676799400147919, policy loss: 5.796390803127072
Experience 13, Iter 46, disc loss: 0.018818637214048486, policy loss: 6.390889885129705
Experience 13, Iter 47, disc loss: 0.009182844424341412, policy loss: 6.508246084238882
Experience 13, Iter 48, disc loss: 0.007485301736656048, policy loss: 6.596397238233914
Experience 13, Iter 49, disc loss: 0.009342562252983723, policy loss: 6.55489972171709
Experience 13, Iter 50, disc loss: 0.008895157618439781, policy loss: 6.682532323672421
Experience 13, Iter 51, disc loss: 0.009263005629688011, policy loss: 6.866037598925605
Experience 13, Iter 52, disc loss: 0.007897507931778389, policy loss: 6.52180723603157
Experience 13, Iter 53, disc loss: 0.008222813507027638, policy loss: 6.2918069867526025
Experience 13, Iter 54, disc loss: 0.008163475032923858, policy loss: 6.718230675539664
Experience 13, Iter 55, disc loss: 0.007173130965259372, policy loss: 7.150650398555895
Experience 13, Iter 56, disc loss: 0.00766036507607057, policy loss: 6.574771688253777
Experience 13, Iter 57, disc loss: 0.008396681252672589, policy loss: 6.656647197245564
Experience 13, Iter 58, disc loss: 0.006366027739835318, policy loss: 6.946982353639626
Experience 13, Iter 59, disc loss: 0.00739055866162341, policy loss: 6.5256429110733425
Experience 13, Iter 60, disc loss: 0.008995673164490368, policy loss: 6.575888295193381
Experience 13, Iter 61, disc loss: 0.007634856440086545, policy loss: 7.079563294983252
Experience 13, Iter 62, disc loss: 0.007148519782732399, policy loss: 6.671505851196361
Experience 13, Iter 63, disc loss: 0.007522013342466487, policy loss: 6.488538746832552
Experience 13, Iter 64, disc loss: 0.007743662573675531, policy loss: 6.3437558328469095
Experience 13, Iter 65, disc loss: 0.006989317290920817, policy loss: 6.746501470040197
Experience 13, Iter 66, disc loss: 0.006731513523467123, policy loss: 7.376466494881173
Experience 13, Iter 67, disc loss: 0.008337115466174022, policy loss: 6.7403882082759985
Experience 13, Iter 68, disc loss: 0.007701065641345477, policy loss: 6.487944255746823
Experience 13, Iter 69, disc loss: 0.008018137481161484, policy loss: 6.369820631516099
Experience 13, Iter 70, disc loss: 0.006735745365522719, policy loss: 7.030126085640797
Experience 13, Iter 71, disc loss: 0.011357771978755716, policy loss: 6.283098635347262
Experience 13, Iter 72, disc loss: 0.007877546496918384, policy loss: 6.392066035167699
Experience 13, Iter 73, disc loss: 0.00798228935533541, policy loss: 6.573634695618971
Experience 13, Iter 74, disc loss: 0.006990547893519387, policy loss: 6.834217849698003
Experience 13, Iter 75, disc loss: 0.007177236267962731, policy loss: 6.523006020401313
Experience 13, Iter 76, disc loss: 0.00850190867581571, policy loss: 6.519545774983205
Experience 13, Iter 77, disc loss: 0.007778449461926464, policy loss: 6.481127932778328
Experience 13, Iter 78, disc loss: 0.0075998970903884255, policy loss: 6.581529949104564
Experience 13, Iter 79, disc loss: 0.006999319037213601, policy loss: 6.721175997997136
Experience 13, Iter 80, disc loss: 0.005940993244361391, policy loss: 6.956500228734457
Experience 13, Iter 81, disc loss: 0.005979798972467934, policy loss: 7.45061097205733
Experience 13, Iter 82, disc loss: 0.0064302471129395605, policy loss: 6.73465629089613
Experience 13, Iter 83, disc loss: 0.007241153867972747, policy loss: 6.692460029489272
Experience 13, Iter 84, disc loss: 0.006951859851685038, policy loss: 6.60518386013284
Experience 13, Iter 85, disc loss: 0.01000782724662512, policy loss: 6.232898505909987
Experience 13, Iter 86, disc loss: 0.007386284896696308, policy loss: 6.668225070463269
Experience 13, Iter 87, disc loss: 0.008950163025314428, policy loss: 6.7217264850950045
Experience 13, Iter 88, disc loss: 0.005672001088153657, policy loss: 7.1055036752754575
Experience 13, Iter 89, disc loss: 0.01018012943410318, policy loss: 6.77285701753156
Experience 13, Iter 90, disc loss: 0.0061546809095903675, policy loss: 7.482543575394908
Experience 13, Iter 91, disc loss: 0.007148179349124981, policy loss: 6.8174346854029855
Experience 13, Iter 92, disc loss: 0.007682590001858616, policy loss: 6.821433621598619
Experience 13, Iter 93, disc loss: 0.007761746924320172, policy loss: 6.752880324062019
Experience 13, Iter 94, disc loss: 0.008034768242167017, policy loss: 7.307885721530317
Experience 13, Iter 95, disc loss: 0.006902199442087262, policy loss: 7.03071414522097
Experience 13, Iter 96, disc loss: 0.007185543150855542, policy loss: 6.882107699983058
Experience 13, Iter 97, disc loss: 0.006527599571308187, policy loss: 7.291968789253568
Experience 13, Iter 98, disc loss: 0.006250947087471028, policy loss: 7.100768828084575
Experience 13, Iter 99, disc loss: 0.006111709785400071, policy loss: 7.101717667818143
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0064],
        [0.0809],
        [0.6828],
        [0.0122]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0191, 0.2277, 0.5556, 0.0195, 0.0072, 2.8965]],

        [[0.0191, 0.2277, 0.5556, 0.0195, 0.0072, 2.8965]],

        [[0.0191, 0.2277, 0.5556, 0.0195, 0.0072, 2.8965]],

        [[0.0191, 0.2277, 0.5556, 0.0195, 0.0072, 2.8965]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0257, 0.3237, 2.7311, 0.0489], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0257, 0.3237, 2.7311, 0.0489])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.253
Iter 2/2000 - Loss: 2.162
Iter 3/2000 - Loss: 2.069
Iter 4/2000 - Loss: 2.024
Iter 5/2000 - Loss: 1.968
Iter 6/2000 - Loss: 1.870
Iter 7/2000 - Loss: 1.774
Iter 8/2000 - Loss: 1.684
Iter 9/2000 - Loss: 1.582
Iter 10/2000 - Loss: 1.455
Iter 11/2000 - Loss: 1.310
Iter 12/2000 - Loss: 1.156
Iter 13/2000 - Loss: 0.995
Iter 14/2000 - Loss: 0.822
Iter 15/2000 - Loss: 0.630
Iter 16/2000 - Loss: 0.417
Iter 17/2000 - Loss: 0.187
Iter 18/2000 - Loss: -0.054
Iter 19/2000 - Loss: -0.304
Iter 20/2000 - Loss: -0.561
Iter 1981/2000 - Loss: -7.931
Iter 1982/2000 - Loss: -7.931
Iter 1983/2000 - Loss: -7.931
Iter 1984/2000 - Loss: -7.931
Iter 1985/2000 - Loss: -7.931
Iter 1986/2000 - Loss: -7.931
Iter 1987/2000 - Loss: -7.932
Iter 1988/2000 - Loss: -7.932
Iter 1989/2000 - Loss: -7.932
Iter 1990/2000 - Loss: -7.932
Iter 1991/2000 - Loss: -7.932
Iter 1992/2000 - Loss: -7.932
Iter 1993/2000 - Loss: -7.932
Iter 1994/2000 - Loss: -7.932
Iter 1995/2000 - Loss: -7.932
Iter 1996/2000 - Loss: -7.932
Iter 1997/2000 - Loss: -7.932
Iter 1998/2000 - Loss: -7.932
Iter 1999/2000 - Loss: -7.932
Iter 2000/2000 - Loss: -7.932
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.5517,  8.1625, 33.8808, 12.0510, 13.7432, 59.0489]],

        [[19.4566, 38.2068,  9.0114,  1.4748,  3.4872, 25.9995]],

        [[19.6607, 37.8422,  9.9645,  1.2061,  1.8434, 22.5548]],

        [[17.2432, 34.9823, 14.0129,  3.4626,  1.7894, 34.2522]]])
Signal Variance: tensor([ 0.1026,  2.2970, 18.7870,  0.4439])
Estimated target variance: tensor([0.0257, 0.3237, 2.7311, 0.0489])
N: 140
Signal to noise ratio: tensor([ 17.3813,  95.2331, 103.2231,  39.2585])
Bound on condition number: tensor([  42296.2423, 1269709.0069, 1491701.8528,  215773.6103])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.004493036505395792, policy loss: 8.770954409570974
Experience 14, Iter 1, disc loss: 0.004490747360464823, policy loss: 8.56103921600232
Experience 14, Iter 2, disc loss: 0.004440539157883833, policy loss: 7.978855005198932
Experience 14, Iter 3, disc loss: 0.004145901048001983, policy loss: 8.32028630631913
Experience 14, Iter 4, disc loss: 0.0041516946291073565, policy loss: 8.171716023789404
Experience 14, Iter 5, disc loss: 0.004101873515095911, policy loss: 8.448865487245016
Experience 14, Iter 6, disc loss: 0.003888987320929976, policy loss: 8.52158913068635
Experience 14, Iter 7, disc loss: 0.003707631106835991, policy loss: 8.621697453698225
Experience 14, Iter 8, disc loss: 0.0036600021896687363, policy loss: 8.03112446577789
Experience 14, Iter 9, disc loss: 0.004252124384571157, policy loss: 7.907973440702773
Experience 14, Iter 10, disc loss: 0.004040695421523211, policy loss: 7.31693378315406
Experience 14, Iter 11, disc loss: 0.003507090407424281, policy loss: 7.499363136242899
Experience 14, Iter 12, disc loss: 0.0030566889295190434, policy loss: 8.165482873432563
Experience 14, Iter 13, disc loss: 0.003432260323453886, policy loss: 7.858801230736932
Experience 14, Iter 14, disc loss: 0.0033188342778927137, policy loss: 7.76692285116895
Experience 14, Iter 15, disc loss: 0.003495657194084529, policy loss: 7.278515810453114
Experience 14, Iter 16, disc loss: 0.0035815567868913182, policy loss: 7.936743954019303
Experience 14, Iter 17, disc loss: 0.005046184440723381, policy loss: 8.408293453387088
Experience 14, Iter 18, disc loss: 0.0034257037749415853, policy loss: 7.4844052607459
Experience 14, Iter 19, disc loss: 0.0030712975599779297, policy loss: 8.02790788050471
Experience 14, Iter 20, disc loss: 0.003306705661141951, policy loss: 7.919431064705263
Experience 14, Iter 21, disc loss: 0.0031038792871809035, policy loss: 7.696592225689251
Experience 14, Iter 22, disc loss: 0.002583454260227899, policy loss: 7.896610175318462
Experience 14, Iter 23, disc loss: 0.0029390300020502554, policy loss: 7.723560045624935
Experience 14, Iter 24, disc loss: 0.0037145551069633614, policy loss: 7.752462312903204
Experience 14, Iter 25, disc loss: 0.0030821847603724477, policy loss: 7.524103161987391
Experience 14, Iter 26, disc loss: 0.003242175698985287, policy loss: 7.4831399043746565
Experience 14, Iter 27, disc loss: 0.0037565822912196366, policy loss: 8.094564109190507
Experience 14, Iter 28, disc loss: 0.003984384676873748, policy loss: 7.573144583721767
Experience 14, Iter 29, disc loss: 0.003265277862473389, policy loss: 8.04780942653489
Experience 14, Iter 30, disc loss: 0.003310079638304114, policy loss: 7.134512784404381
Experience 14, Iter 31, disc loss: 0.003152933871901284, policy loss: 8.113305621584457
Experience 14, Iter 32, disc loss: 0.003298083593273509, policy loss: 7.235624874554405
Experience 14, Iter 33, disc loss: 0.0029102599627865867, policy loss: 7.920702947730271
Experience 14, Iter 34, disc loss: 0.003049416992693744, policy loss: 7.770093832942514
Experience 14, Iter 35, disc loss: 0.003120475781500685, policy loss: 7.813977530876105
Experience 14, Iter 36, disc loss: 0.002686158850793426, policy loss: 7.811012134546637
Experience 14, Iter 37, disc loss: 0.0033379720338433446, policy loss: 7.825206728960899
Experience 14, Iter 38, disc loss: 0.0027306760732992803, policy loss: 7.765277718251567
Experience 14, Iter 39, disc loss: 0.003118551671493489, policy loss: 7.515295567199091
Experience 14, Iter 40, disc loss: 0.0028313600445283816, policy loss: 7.77053014148455
Experience 14, Iter 41, disc loss: 0.0031528940031999406, policy loss: 7.599511457364069
Experience 14, Iter 42, disc loss: 0.0026115668852775566, policy loss: 8.494496658920973
Experience 14, Iter 43, disc loss: 0.004503809879280445, policy loss: 7.503346541499685
Experience 14, Iter 44, disc loss: 0.0032557611607194853, policy loss: 7.269911479617856
Experience 14, Iter 45, disc loss: 0.00269050765650369, policy loss: 7.957530004713432
Experience 14, Iter 46, disc loss: 0.003056652475910967, policy loss: 8.099409456077439
Experience 14, Iter 47, disc loss: 0.0035666032151102063, policy loss: 7.752366344401703
Experience 14, Iter 48, disc loss: 0.002544991656140849, policy loss: 8.268374098256295
Experience 14, Iter 49, disc loss: 0.002818357639693812, policy loss: 7.770115348959526
Experience 14, Iter 50, disc loss: 0.008836735420628499, policy loss: 7.708837307422966
Experience 14, Iter 51, disc loss: 0.007438239903709246, policy loss: 7.797270567921473
Experience 14, Iter 52, disc loss: 0.002594928543984136, policy loss: 8.278237746481626
Experience 14, Iter 53, disc loss: 0.003909051474556417, policy loss: 7.681793169593518
Experience 14, Iter 54, disc loss: 0.0030042748141413077, policy loss: 7.896272752545024
Experience 14, Iter 55, disc loss: 0.0035170309593172615, policy loss: 7.961124918391279
Experience 14, Iter 56, disc loss: 0.002878939427249815, policy loss: 7.902507980935269
Experience 14, Iter 57, disc loss: 0.0035715146097000925, policy loss: 7.677482599987434
Experience 14, Iter 58, disc loss: 0.002931586166741735, policy loss: 8.258000728231789
Experience 14, Iter 59, disc loss: 0.002709085332119149, policy loss: 8.281688917271575
Experience 14, Iter 60, disc loss: 0.0027789667916976657, policy loss: 8.645336452525314
Experience 14, Iter 61, disc loss: 0.0029932374405892003, policy loss: 8.114702026046391
Experience 14, Iter 62, disc loss: 0.0029085731580304397, policy loss: 8.0348627544738
Experience 14, Iter 63, disc loss: 0.0030080966361039334, policy loss: 7.942747325638936
Experience 14, Iter 64, disc loss: 0.0027364489364442596, policy loss: 8.618672783576088
Experience 14, Iter 65, disc loss: 0.0029511190996006306, policy loss: 8.437777249135788
Experience 14, Iter 66, disc loss: 0.0028744194859423233, policy loss: 8.23410070601845
Experience 14, Iter 67, disc loss: 0.0026798581251685923, policy loss: 8.52678445257968
Experience 14, Iter 68, disc loss: 0.002880404433010129, policy loss: 8.038757667780198
Experience 14, Iter 69, disc loss: 0.0026397363063156883, policy loss: 8.511658128374233
Experience 14, Iter 70, disc loss: 0.003286251470906702, policy loss: 8.496527841949154
Experience 14, Iter 71, disc loss: 0.002425699463409378, policy loss: 8.21286735150046
Experience 14, Iter 72, disc loss: 0.00821894176471695, policy loss: 7.694368485922487
Experience 14, Iter 73, disc loss: 0.0034627185727265338, policy loss: 7.757540126547212
Experience 14, Iter 74, disc loss: 0.004159942329337193, policy loss: 8.093942193172932
Experience 14, Iter 75, disc loss: 0.002981978776902985, policy loss: 8.404013655643187
Experience 14, Iter 76, disc loss: 0.0035087159529272275, policy loss: 8.03247704255782
Experience 14, Iter 77, disc loss: 0.002806192054054144, policy loss: 8.340129651024139
Experience 14, Iter 78, disc loss: 0.0032395498835468663, policy loss: 8.864352135459765
Experience 14, Iter 79, disc loss: 0.0031423745188106063, policy loss: 8.019948974924027
Experience 14, Iter 80, disc loss: 0.0027231347927984895, policy loss: 8.461222577263936
Experience 14, Iter 81, disc loss: 0.012345616871752346, policy loss: 8.723210785931514
Experience 14, Iter 82, disc loss: 0.002721207277114052, policy loss: 8.533034262321282
Experience 14, Iter 83, disc loss: 0.003489360284999372, policy loss: 8.840117521104538
Experience 14, Iter 84, disc loss: 0.0025955494560287876, policy loss: 8.770211979196084
Experience 14, Iter 85, disc loss: 0.002616102265093474, policy loss: 8.80703814676642
Experience 14, Iter 86, disc loss: 0.002680260596548999, policy loss: 8.455139201739941
Experience 14, Iter 87, disc loss: 0.0025488478827147636, policy loss: 8.686565463413029
Experience 14, Iter 88, disc loss: 0.002522189010700464, policy loss: 8.828758585125696
Experience 14, Iter 89, disc loss: 0.0030948505928649864, policy loss: 8.933744768581262
Experience 14, Iter 90, disc loss: 0.0030143112758547036, policy loss: 8.701124738378612
Experience 14, Iter 91, disc loss: 0.002309620778787148, policy loss: 8.776720533435402
Experience 14, Iter 92, disc loss: 0.0021785057779125706, policy loss: 8.9543776566972
Experience 14, Iter 93, disc loss: 0.0021927493026340705, policy loss: 9.061472137575704
Experience 14, Iter 94, disc loss: 0.03018025858317653, policy loss: 8.370851438570902
Experience 14, Iter 95, disc loss: 0.002263789373237379, policy loss: 9.184770992351977
Experience 14, Iter 96, disc loss: 0.002403703790669864, policy loss: 9.131020663381173
Experience 14, Iter 97, disc loss: 0.002629187881433, policy loss: 8.889194017609839
Experience 14, Iter 98, disc loss: 0.003075487871736187, policy loss: 8.904006415316138
Experience 14, Iter 99, disc loss: 0.0029402870256095177, policy loss: 9.172159612875337
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0062],
        [0.1011],
        [0.8957],
        [0.0140]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0185, 0.2238, 0.6579, 0.0209, 0.0071, 3.2002]],

        [[0.0185, 0.2238, 0.6579, 0.0209, 0.0071, 3.2002]],

        [[0.0185, 0.2238, 0.6579, 0.0209, 0.0071, 3.2002]],

        [[0.0185, 0.2238, 0.6579, 0.0209, 0.0071, 3.2002]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0248, 0.4042, 3.5829, 0.0558], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0248, 0.4042, 3.5829, 0.0558])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.533
Iter 2/2000 - Loss: 2.465
Iter 3/2000 - Loss: 2.360
Iter 4/2000 - Loss: 2.319
Iter 5/2000 - Loss: 2.267
Iter 6/2000 - Loss: 2.166
Iter 7/2000 - Loss: 2.065
Iter 8/2000 - Loss: 1.974
Iter 9/2000 - Loss: 1.873
Iter 10/2000 - Loss: 1.746
Iter 11/2000 - Loss: 1.596
Iter 12/2000 - Loss: 1.436
Iter 13/2000 - Loss: 1.268
Iter 14/2000 - Loss: 1.087
Iter 15/2000 - Loss: 0.886
Iter 16/2000 - Loss: 0.663
Iter 17/2000 - Loss: 0.420
Iter 18/2000 - Loss: 0.163
Iter 19/2000 - Loss: -0.101
Iter 20/2000 - Loss: -0.372
Iter 1981/2000 - Loss: -7.899
Iter 1982/2000 - Loss: -7.899
Iter 1983/2000 - Loss: -7.899
Iter 1984/2000 - Loss: -7.899
Iter 1985/2000 - Loss: -7.899
Iter 1986/2000 - Loss: -7.899
Iter 1987/2000 - Loss: -7.899
Iter 1988/2000 - Loss: -7.899
Iter 1989/2000 - Loss: -7.899
Iter 1990/2000 - Loss: -7.899
Iter 1991/2000 - Loss: -7.899
Iter 1992/2000 - Loss: -7.899
Iter 1993/2000 - Loss: -7.899
Iter 1994/2000 - Loss: -7.899
Iter 1995/2000 - Loss: -7.900
Iter 1996/2000 - Loss: -7.900
Iter 1997/2000 - Loss: -7.900
Iter 1998/2000 - Loss: -7.900
Iter 1999/2000 - Loss: -7.900
Iter 2000/2000 - Loss: -7.900
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[13.1324,  8.0957, 33.4388, 11.3858, 12.9396, 59.2081]],

        [[18.2885, 40.0664,  8.3769,  1.4052,  3.6908, 25.7610]],

        [[18.6110, 38.9973,  8.6248,  1.1484,  2.1715, 22.9981]],

        [[16.7725, 32.5682, 15.8258,  3.6332,  1.7099, 37.2735]]])
Signal Variance: tensor([ 0.1000,  1.9075, 15.9271,  0.4692])
Estimated target variance: tensor([0.0248, 0.4042, 3.5829, 0.0558])
N: 150
Signal to noise ratio: tensor([17.3151, 84.1561, 92.6436, 39.0797])
Bound on condition number: tensor([  44973.0857, 1062338.2272, 1287425.6783,  229084.7738])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0025473005462291563, policy loss: 9.307854257820635
Experience 15, Iter 1, disc loss: 0.002558259824170503, policy loss: 9.236035799561996
Experience 15, Iter 2, disc loss: 0.0025476306348126846, policy loss: 9.171731355509205
Experience 15, Iter 3, disc loss: 0.0025267117519346934, policy loss: 9.332578332427973
Experience 15, Iter 4, disc loss: 0.0025112968242406914, policy loss: 9.582196453765068
Experience 15, Iter 5, disc loss: 0.0025332703864849107, policy loss: 9.22962667119096
Experience 15, Iter 6, disc loss: 0.002475787543428248, policy loss: 9.28852174416448
Experience 15, Iter 7, disc loss: 0.00354707791947384, policy loss: 8.681176038963164
Experience 15, Iter 8, disc loss: 0.002468990630995241, policy loss: 8.819639286051238
Experience 15, Iter 9, disc loss: 0.0024399430602571343, policy loss: 8.441113656570968
Experience 15, Iter 10, disc loss: 0.0023079720724266976, policy loss: 8.732028571089202
Experience 15, Iter 11, disc loss: 0.0022798613221577495, policy loss: 8.899695657273451
Experience 15, Iter 12, disc loss: 0.0021149613755063355, policy loss: 8.996219526153354
Experience 15, Iter 13, disc loss: 0.0024472162886228785, policy loss: 8.853447929209082
Experience 15, Iter 14, disc loss: 0.002406312072875885, policy loss: 8.347414310398069
Experience 15, Iter 15, disc loss: 0.002921075474609996, policy loss: 8.7501798551382
Experience 15, Iter 16, disc loss: 0.002152342606819144, policy loss: 8.490040876698789
Experience 15, Iter 17, disc loss: 0.001995677958199869, policy loss: 9.334940895510961
Experience 15, Iter 18, disc loss: 0.0022542678521531526, policy loss: 9.15603543818472
Experience 15, Iter 19, disc loss: 0.0021427303845687816, policy loss: 8.530606122895348
Experience 15, Iter 20, disc loss: 0.001883605164159354, policy loss: 8.665729258501202
Experience 15, Iter 21, disc loss: 0.002359782583348504, policy loss: 8.073703789099923
Experience 15, Iter 22, disc loss: 0.001883355627060137, policy loss: 8.459894633343755
Experience 15, Iter 23, disc loss: 0.002051028794151436, policy loss: 8.755693501139888
Experience 15, Iter 24, disc loss: 0.0018200837821489525, policy loss: 8.527577342911314
Experience 15, Iter 25, disc loss: 0.002200286550948682, policy loss: 8.689146130641028
Experience 15, Iter 26, disc loss: 0.0019696650495306034, policy loss: 8.523268281724842
Experience 15, Iter 27, disc loss: 0.0022533423161899086, policy loss: 8.367052584120705
Experience 15, Iter 28, disc loss: 0.0019948301641768747, policy loss: 8.61875080247822
Experience 15, Iter 29, disc loss: 0.002109255078870408, policy loss: 8.181202830043397
Experience 15, Iter 30, disc loss: 0.0021897822844705065, policy loss: 8.043459194249268
Experience 15, Iter 31, disc loss: 0.0021820065168067755, policy loss: 8.06538020301254
Experience 15, Iter 32, disc loss: 0.0020356952527171525, policy loss: 8.425417151428064
Experience 15, Iter 33, disc loss: 0.00249054834481965, policy loss: 8.038827082791405
Experience 15, Iter 34, disc loss: 0.0018235917232584497, policy loss: 8.294102017846667
Experience 15, Iter 35, disc loss: 0.00249122558823937, policy loss: 8.092191617311235
Experience 15, Iter 36, disc loss: 0.0020180782875658543, policy loss: 8.470618382794463
Experience 15, Iter 37, disc loss: 0.0018683660001469692, policy loss: 9.251947975545598
Experience 15, Iter 38, disc loss: 0.002250439039995695, policy loss: 8.799014678585433
Experience 15, Iter 39, disc loss: 0.001995567710723453, policy loss: 9.669323316755039
Experience 15, Iter 40, disc loss: 0.0019276213017042482, policy loss: 8.396026261562133
Experience 15, Iter 41, disc loss: 0.00169659614758691, policy loss: 8.767051736998436
Experience 15, Iter 42, disc loss: 0.0014857751068891372, policy loss: 9.301987740495145
Experience 15, Iter 43, disc loss: 0.0013948233485161685, policy loss: 9.40961911762396
Experience 15, Iter 44, disc loss: 0.0013671095846205185, policy loss: 10.102982016045928
Experience 15, Iter 45, disc loss: 0.001374988569106582, policy loss: 9.396237036584292
Experience 15, Iter 46, disc loss: 0.0011816624085880382, policy loss: 9.773075866024822
Experience 15, Iter 47, disc loss: 0.0012775539771889772, policy loss: 9.524283324943067
Experience 15, Iter 48, disc loss: 0.0015389887414773904, policy loss: 9.36220439523448
Experience 15, Iter 49, disc loss: 0.0011952956517222025, policy loss: 9.30388711708081
Experience 15, Iter 50, disc loss: 0.0010989838266183766, policy loss: 10.033215258063851
Experience 15, Iter 51, disc loss: 0.0011395889176834292, policy loss: 9.132144019859394
Experience 15, Iter 52, disc loss: 0.001043924065274637, policy loss: 9.884949517552634
Experience 15, Iter 53, disc loss: 0.0010517278154211682, policy loss: 9.542756040962054
Experience 15, Iter 54, disc loss: 0.0010524620632283014, policy loss: 9.682277631471527
Experience 15, Iter 55, disc loss: 0.0010083449004043205, policy loss: 9.677191258057778
Experience 15, Iter 56, disc loss: 0.0010972575079178982, policy loss: 9.350318545283447
Experience 15, Iter 57, disc loss: 0.001023204421818724, policy loss: 9.52687245109629
Experience 15, Iter 58, disc loss: 0.0011756274302552605, policy loss: 9.48259601214743
Experience 15, Iter 59, disc loss: 0.0010412551452867254, policy loss: 9.305319046701012
Experience 15, Iter 60, disc loss: 0.0010974306102908284, policy loss: 9.643768541683343
Experience 15, Iter 61, disc loss: 0.0010623143299930038, policy loss: 9.325669236853477
Experience 15, Iter 62, disc loss: 0.0011358950015363427, policy loss: 9.694311646511595
Experience 15, Iter 63, disc loss: 0.0010541336836362393, policy loss: 9.303307434519947
Experience 15, Iter 64, disc loss: 0.0009813714667872675, policy loss: 9.471105735036119
Experience 15, Iter 65, disc loss: 0.0010961362441930353, policy loss: 9.419213766559512
Experience 15, Iter 66, disc loss: 0.0012002986601032403, policy loss: 9.169645099949385
Experience 15, Iter 67, disc loss: 0.0012657468008766073, policy loss: 8.806721664836095
Experience 15, Iter 68, disc loss: 0.001188481826431588, policy loss: 8.854397291861734
Experience 15, Iter 69, disc loss: 0.0011567989039281482, policy loss: 8.68890392284371
Experience 15, Iter 70, disc loss: 0.0012378081304243, policy loss: 8.968849550765368
Experience 15, Iter 71, disc loss: 0.001539842921020802, policy loss: 8.622727654218888
Experience 15, Iter 72, disc loss: 0.00155865337164435, policy loss: 8.510217966885335
Experience 15, Iter 73, disc loss: 0.001607738368489158, policy loss: 9.010526902237764
Experience 15, Iter 74, disc loss: 0.0015957905066709265, policy loss: 8.597254576192205
Experience 15, Iter 75, disc loss: 0.0015458692988107746, policy loss: 8.834752377238054
Experience 15, Iter 76, disc loss: 0.0014007997493551082, policy loss: 8.701824088327072
Experience 15, Iter 77, disc loss: 0.006606466818827353, policy loss: 8.93674469505829
Experience 15, Iter 78, disc loss: 0.0024532195491718655, policy loss: 8.376218439269572
Experience 15, Iter 79, disc loss: 0.002019336197543336, policy loss: 8.071245369914877
Experience 15, Iter 80, disc loss: 0.0014805740231324128, policy loss: 8.473569679345871
Experience 15, Iter 81, disc loss: 0.0014618249921488262, policy loss: 8.73501410261649
Experience 15, Iter 82, disc loss: 0.0022921776194577284, policy loss: 8.03855525293552
Experience 15, Iter 83, disc loss: 0.0020784962659221255, policy loss: 8.923336246369923
Experience 15, Iter 84, disc loss: 0.0020169913066274423, policy loss: 9.022837035645086
Experience 15, Iter 85, disc loss: 0.0026863349235734437, policy loss: 8.960224024547099
Experience 15, Iter 86, disc loss: 0.0017715680688862603, policy loss: 8.401804196807952
Experience 15, Iter 87, disc loss: 0.0018894143813729296, policy loss: 8.79444948306833
Experience 15, Iter 88, disc loss: 0.001886092367954877, policy loss: 8.485944310438235
Experience 15, Iter 89, disc loss: 0.0014225717786688157, policy loss: 8.942250251433636
Experience 15, Iter 90, disc loss: 0.0015947721131331573, policy loss: 8.725518590013698
Experience 15, Iter 91, disc loss: 0.0038565154817128155, policy loss: 8.615710602126118
Experience 15, Iter 92, disc loss: 0.0017081679549899066, policy loss: 9.173638973818665
Experience 15, Iter 93, disc loss: 0.0014844325837620183, policy loss: 9.580414095520862
Experience 15, Iter 94, disc loss: 0.0018916866977311624, policy loss: 8.647262693492966
Experience 15, Iter 95, disc loss: 0.001827882335449256, policy loss: 8.319905675942488
Experience 15, Iter 96, disc loss: 0.0016129476528916353, policy loss: 8.881811841205081
Experience 15, Iter 97, disc loss: 0.0015801610197862044, policy loss: 9.011612606209328
Experience 15, Iter 98, disc loss: 0.001732640428417857, policy loss: 8.828299693046997
Experience 15, Iter 99, disc loss: 0.0015033192329628805, policy loss: 9.547971185957056
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0060],
        [0.1217],
        [1.0971],
        [0.0157]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0176, 0.2212, 0.7586, 0.0223, 0.0070, 3.5660]],

        [[0.0176, 0.2212, 0.7586, 0.0223, 0.0070, 3.5660]],

        [[0.0176, 0.2212, 0.7586, 0.0223, 0.0070, 3.5660]],

        [[0.0176, 0.2212, 0.7586, 0.0223, 0.0070, 3.5660]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0241, 0.4868, 4.3884, 0.0628], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0241, 0.4868, 4.3884, 0.0628])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.754
Iter 2/2000 - Loss: 2.702
Iter 3/2000 - Loss: 2.584
Iter 4/2000 - Loss: 2.546
Iter 5/2000 - Loss: 2.496
Iter 6/2000 - Loss: 2.393
Iter 7/2000 - Loss: 2.289
Iter 8/2000 - Loss: 2.199
Iter 9/2000 - Loss: 2.100
Iter 10/2000 - Loss: 1.976
Iter 11/2000 - Loss: 1.829
Iter 12/2000 - Loss: 1.671
Iter 13/2000 - Loss: 1.505
Iter 14/2000 - Loss: 1.326
Iter 15/2000 - Loss: 1.128
Iter 16/2000 - Loss: 0.905
Iter 17/2000 - Loss: 0.661
Iter 18/2000 - Loss: 0.401
Iter 19/2000 - Loss: 0.132
Iter 20/2000 - Loss: -0.146
Iter 1981/2000 - Loss: -7.880
Iter 1982/2000 - Loss: -7.880
Iter 1983/2000 - Loss: -7.880
Iter 1984/2000 - Loss: -7.880
Iter 1985/2000 - Loss: -7.880
Iter 1986/2000 - Loss: -7.881
Iter 1987/2000 - Loss: -7.881
Iter 1988/2000 - Loss: -7.881
Iter 1989/2000 - Loss: -7.881
Iter 1990/2000 - Loss: -7.881
Iter 1991/2000 - Loss: -7.881
Iter 1992/2000 - Loss: -7.881
Iter 1993/2000 - Loss: -7.881
Iter 1994/2000 - Loss: -7.881
Iter 1995/2000 - Loss: -7.881
Iter 1996/2000 - Loss: -7.881
Iter 1997/2000 - Loss: -7.881
Iter 1998/2000 - Loss: -7.881
Iter 1999/2000 - Loss: -7.881
Iter 2000/2000 - Loss: -7.881
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.7420,  8.1983, 36.2923,  9.8892, 12.2729, 57.5307]],

        [[18.2867, 39.2362,  6.9965,  1.4360,  3.7123, 24.5467]],

        [[19.5281, 37.8091,  7.6446,  1.2032,  1.7579, 22.6360]],

        [[16.3246, 34.0352, 16.9711,  3.9350,  1.5486, 40.9647]]])
Signal Variance: tensor([ 0.1028,  1.8605, 14.4153,  0.5051])
Estimated target variance: tensor([0.0241, 0.4868, 4.3884, 0.0628])
N: 160
Signal to noise ratio: tensor([17.9364, 83.0598, 88.9781, 40.1528])
Bound on condition number: tensor([  51475.2794, 1103828.8266, 1266736.0378,  257960.9991])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0014999064926487973, policy loss: 8.965751178781385
Experience 16, Iter 1, disc loss: 0.0017245635127790546, policy loss: 9.277644770481821
Experience 16, Iter 2, disc loss: 0.0016317064861393832, policy loss: 9.26880812816405
Experience 16, Iter 3, disc loss: 0.0014534367912297274, policy loss: 9.044600102320013
Experience 16, Iter 4, disc loss: 0.001653667684734688, policy loss: 8.600985500581
Experience 16, Iter 5, disc loss: 0.0014203880870802565, policy loss: 9.262557236126767
Experience 16, Iter 6, disc loss: 0.0015705860844065442, policy loss: 8.995313413081051
Experience 16, Iter 7, disc loss: 0.0014807137001341224, policy loss: 9.383656099903375
Experience 16, Iter 8, disc loss: 0.0014478121100407726, policy loss: 9.305607579566493
Experience 16, Iter 9, disc loss: 0.0016846730260546491, policy loss: 9.298892876679652
Experience 16, Iter 10, disc loss: 0.001460428587841589, policy loss: 9.077431169557748
Experience 16, Iter 11, disc loss: 0.0015296869009360373, policy loss: 9.07073220529216
Experience 16, Iter 12, disc loss: 0.0014200545884223452, policy loss: 9.606763447145287
Experience 16, Iter 13, disc loss: 0.0016748796712058318, policy loss: 8.490613063448244
Experience 16, Iter 14, disc loss: 0.0016154698576088524, policy loss: 8.53053477471115
Experience 16, Iter 15, disc loss: 0.0016446336357694505, policy loss: 9.183247760071449
Experience 16, Iter 16, disc loss: 0.0014126429621089764, policy loss: 9.050251806899778
Experience 16, Iter 17, disc loss: 0.002377484017583841, policy loss: 8.929795993904143
Experience 16, Iter 18, disc loss: 0.0018601921764592035, policy loss: 9.492967592444787
Experience 16, Iter 19, disc loss: 0.01121472814336046, policy loss: 8.497039638447568
Experience 16, Iter 20, disc loss: 0.001405940559262534, policy loss: 9.80503732587335
Experience 16, Iter 21, disc loss: 0.0013885863102875012, policy loss: 9.425518879259062
Experience 16, Iter 22, disc loss: 0.001592325525723396, policy loss: 8.937978551214526
Experience 16, Iter 23, disc loss: 0.0018735789683022078, policy loss: 8.943940976490756
Experience 16, Iter 24, disc loss: 0.001470425968665436, policy loss: 9.452077373104476
Experience 16, Iter 25, disc loss: 0.0013198142770465496, policy loss: 9.941184822596874
Experience 16, Iter 26, disc loss: 0.0014860002480643073, policy loss: 9.416923546708134
Experience 16, Iter 27, disc loss: 0.009331472779360757, policy loss: 8.840135325240016
Experience 16, Iter 28, disc loss: 0.0035858823598248824, policy loss: 9.472610412449379
Experience 16, Iter 29, disc loss: 0.0016203774231250754, policy loss: 9.685958589760403
Experience 16, Iter 30, disc loss: 0.0020033870965753316, policy loss: 9.369005813224192
Experience 16, Iter 31, disc loss: 0.0015964398278903462, policy loss: 9.755770038696141
Experience 16, Iter 32, disc loss: 0.001574731044826159, policy loss: 9.79724029786432
Experience 16, Iter 33, disc loss: 0.0016834930752598026, policy loss: 9.261982751449322
Experience 16, Iter 34, disc loss: 0.0016393238030624976, policy loss: 9.36910661259462
Experience 16, Iter 35, disc loss: 0.0017444004646798926, policy loss: 9.73203045429808
Experience 16, Iter 36, disc loss: 0.0018067317026165298, policy loss: 9.801191656701633
Experience 16, Iter 37, disc loss: 0.0016953372362140235, policy loss: 9.289351772392518
Experience 16, Iter 38, disc loss: 0.0016200736494679457, policy loss: 9.7156157150063
Experience 16, Iter 39, disc loss: 0.0015631564014360008, policy loss: 10.009893021806965
Experience 16, Iter 40, disc loss: 0.0015822696022047464, policy loss: 9.425590242702265
Experience 16, Iter 41, disc loss: 0.0015766034961618588, policy loss: 9.880109341008756
Experience 16, Iter 42, disc loss: 0.0016642267214483013, policy loss: 9.374797771112563
Experience 16, Iter 43, disc loss: 0.0016194808468955463, policy loss: 9.662260450251955
Experience 16, Iter 44, disc loss: 0.0014736578108434889, policy loss: 9.518492698446686
Experience 16, Iter 45, disc loss: 0.0015195073493710475, policy loss: 9.22217595756151
Experience 16, Iter 46, disc loss: 0.0015816431567375075, policy loss: 9.557220340978493
Experience 16, Iter 47, disc loss: 0.0015354659752096272, policy loss: 9.085365012483154
Experience 16, Iter 48, disc loss: 0.0017196705092216295, policy loss: 9.057338870301296
Experience 16, Iter 49, disc loss: 0.0015146735141464299, policy loss: 9.630312515468024
Experience 16, Iter 50, disc loss: 0.0017323028318210322, policy loss: 8.944768697438024
Experience 16, Iter 51, disc loss: 0.0016499375234251671, policy loss: 8.867702971369772
Experience 16, Iter 52, disc loss: 0.0017762199527844033, policy loss: 9.330146459542078
Experience 16, Iter 53, disc loss: 0.0021928622427093976, policy loss: 9.055180764532722
Experience 16, Iter 54, disc loss: 0.001808248741968093, policy loss: 8.93126828992203
Experience 16, Iter 55, disc loss: 0.003180499478029593, policy loss: 9.262480506210707
Experience 16, Iter 56, disc loss: 0.0015373633095502933, policy loss: 9.27661915538041
Experience 16, Iter 57, disc loss: 0.0016660346960495173, policy loss: 9.390010697917159
Experience 16, Iter 58, disc loss: 0.0014654195117585755, policy loss: 9.500034862893491
Experience 16, Iter 59, disc loss: 0.001492784587362355, policy loss: 8.867149268396531
Experience 16, Iter 60, disc loss: 0.0014458783200874538, policy loss: 9.458106026847519
Experience 16, Iter 61, disc loss: 0.0013425286522533932, policy loss: 9.812619884181062
Experience 16, Iter 62, disc loss: 0.0019589045151611, policy loss: 9.819710261041163
Experience 16, Iter 63, disc loss: 0.0015574754014473931, policy loss: 9.682060480646566
Experience 16, Iter 64, disc loss: 0.0013948224424870405, policy loss: 9.533155132291657
Experience 16, Iter 65, disc loss: 0.0015972284080308478, policy loss: 9.344155051332608
Experience 16, Iter 66, disc loss: 0.0013219077087005308, policy loss: 9.801402927775884
Experience 16, Iter 67, disc loss: 0.0015435088347541745, policy loss: 9.26562408599549
Experience 16, Iter 68, disc loss: 0.0033214465582674093, policy loss: 9.416191364323877
Experience 16, Iter 69, disc loss: 0.0010098745531439462, policy loss: 10.25835656824405
Experience 16, Iter 70, disc loss: 0.0010572960388807236, policy loss: 10.128464185438434
Experience 16, Iter 71, disc loss: 0.0020276459638150007, policy loss: 9.943191385446458
Experience 16, Iter 72, disc loss: 0.002667510857282766, policy loss: 10.43890497167585
Experience 16, Iter 73, disc loss: 0.001026948412886613, policy loss: 11.110277348869438
Experience 16, Iter 74, disc loss: 0.009610298423154518, policy loss: 11.1171798048219
Experience 16, Iter 75, disc loss: 0.0009455115841616497, policy loss: 11.110882297252813
Experience 16, Iter 76, disc loss: 0.0012842718224356188, policy loss: 10.95734208527518
Experience 16, Iter 77, disc loss: 0.0010361229049703016, policy loss: 12.01755258524567
Experience 16, Iter 78, disc loss: 0.0009879633228685265, policy loss: 12.0476284895455
Experience 16, Iter 79, disc loss: 0.0010107419926625929, policy loss: 11.670261086460597
Experience 16, Iter 80, disc loss: 0.0010010750591611611, policy loss: 12.033685943567708
Experience 16, Iter 81, disc loss: 0.001069294118833234, policy loss: 11.800153150289805
Experience 16, Iter 82, disc loss: 0.0010205428759597842, policy loss: 12.332072685758094
Experience 16, Iter 83, disc loss: 0.002086290426707287, policy loss: 11.814982610341417
Experience 16, Iter 84, disc loss: 0.000978563687206056, policy loss: 12.628024979332526
Experience 16, Iter 85, disc loss: 0.0009777819981614969, policy loss: 12.487712771555774
Experience 16, Iter 86, disc loss: 0.0010760353002032016, policy loss: 12.375193046352678
Experience 16, Iter 87, disc loss: 0.0009575556330387628, policy loss: 12.723447290190943
Experience 16, Iter 88, disc loss: 0.0009975475313513701, policy loss: 12.276615842562181
Experience 16, Iter 89, disc loss: 0.001005597441963563, policy loss: 12.48172851701758
Experience 16, Iter 90, disc loss: 0.004263756265685685, policy loss: 11.762362752184947
Experience 16, Iter 91, disc loss: 0.001001799721547064, policy loss: 12.26002405951365
Experience 16, Iter 92, disc loss: 0.000933908731955415, policy loss: 11.996371610524776
Experience 16, Iter 93, disc loss: 0.0009403188689012023, policy loss: 12.182788721725176
Experience 16, Iter 94, disc loss: 0.0009782012384177837, policy loss: 12.272677102258529
Experience 16, Iter 95, disc loss: 0.0012589400767943333, policy loss: 11.910272356710532
Experience 16, Iter 96, disc loss: 0.0023137866829485362, policy loss: 11.73837631121841
Experience 16, Iter 97, disc loss: 0.0009956160403628753, policy loss: 11.72209309510556
Experience 16, Iter 98, disc loss: 0.0009618417430648554, policy loss: 11.175675772320027
Experience 16, Iter 99, disc loss: 0.0014290863477857004, policy loss: 11.464064566831428
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.1411],
        [1.2249],
        [0.0200]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0174, 0.2262, 0.9532, 0.0246, 0.0086, 3.9127]],

        [[0.0174, 0.2262, 0.9532, 0.0246, 0.0086, 3.9127]],

        [[0.0174, 0.2262, 0.9532, 0.0246, 0.0086, 3.9127]],

        [[0.0174, 0.2262, 0.9532, 0.0246, 0.0086, 3.9127]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0242, 0.5645, 4.8996, 0.0799], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0242, 0.5645, 4.8996, 0.0799])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.990
Iter 2/2000 - Loss: 2.955
Iter 3/2000 - Loss: 2.821
Iter 4/2000 - Loss: 2.794
Iter 5/2000 - Loss: 2.747
Iter 6/2000 - Loss: 2.635
Iter 7/2000 - Loss: 2.520
Iter 8/2000 - Loss: 2.427
Iter 9/2000 - Loss: 2.335
Iter 10/2000 - Loss: 2.215
Iter 11/2000 - Loss: 2.066
Iter 12/2000 - Loss: 1.901
Iter 13/2000 - Loss: 1.728
Iter 14/2000 - Loss: 1.547
Iter 15/2000 - Loss: 1.352
Iter 16/2000 - Loss: 1.136
Iter 17/2000 - Loss: 0.898
Iter 18/2000 - Loss: 0.641
Iter 19/2000 - Loss: 0.373
Iter 20/2000 - Loss: 0.099
Iter 1981/2000 - Loss: -7.575
Iter 1982/2000 - Loss: -7.575
Iter 1983/2000 - Loss: -7.575
Iter 1984/2000 - Loss: -7.575
Iter 1985/2000 - Loss: -7.575
Iter 1986/2000 - Loss: -7.575
Iter 1987/2000 - Loss: -7.575
Iter 1988/2000 - Loss: -7.575
Iter 1989/2000 - Loss: -7.575
Iter 1990/2000 - Loss: -7.575
Iter 1991/2000 - Loss: -7.575
Iter 1992/2000 - Loss: -7.575
Iter 1993/2000 - Loss: -7.575
Iter 1994/2000 - Loss: -7.575
Iter 1995/2000 - Loss: -7.575
Iter 1996/2000 - Loss: -7.575
Iter 1997/2000 - Loss: -7.575
Iter 1998/2000 - Loss: -7.575
Iter 1999/2000 - Loss: -7.575
Iter 2000/2000 - Loss: -7.576
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[13.0684,  5.9583, 36.4621,  3.3874,  2.3608, 62.0588]],

        [[18.0963, 36.4701,  8.3727,  1.2538,  1.5289, 23.7698]],

        [[19.5875, 36.3661,  8.1572,  1.1240,  0.8094, 18.4581]],

        [[15.2490, 32.0307, 15.0958,  3.6380,  1.8102, 37.3295]]])
Signal Variance: tensor([ 0.0854,  1.9325, 10.2411,  0.4030])
Estimated target variance: tensor([0.0242, 0.5645, 4.8996, 0.0799])
N: 170
Signal to noise ratio: tensor([16.5473, 81.5715, 74.7041, 34.6125])
Bound on condition number: tensor([  46549.1074, 1131165.1828,  948721.5819,  203665.4429])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.043104696463883034, policy loss: 4.601450929008411
Experience 17, Iter 1, disc loss: 0.037017345316666464, policy loss: 4.829518681301726
Experience 17, Iter 2, disc loss: 0.02594986938330531, policy loss: 4.9785964237933324
Experience 17, Iter 3, disc loss: 0.01846994947070868, policy loss: 5.742897178685803
Experience 17, Iter 4, disc loss: 0.010751904456777008, policy loss: 6.599783679105753
Experience 17, Iter 5, disc loss: 0.010334879344482916, policy loss: 7.129864908630647
Experience 17, Iter 6, disc loss: 0.011010130410353076, policy loss: 6.568661471014387
Experience 17, Iter 7, disc loss: 0.011810119727857766, policy loss: 7.417217055796536
Experience 17, Iter 8, disc loss: 0.015180151631047543, policy loss: 7.379460522953691
Experience 17, Iter 9, disc loss: 0.017824666178844897, policy loss: 6.962890451162599
Experience 17, Iter 10, disc loss: 0.01935637782211713, policy loss: 6.749795140077875
Experience 17, Iter 11, disc loss: 0.019150762915915085, policy loss: 7.328062173418663
Experience 17, Iter 12, disc loss: 0.017223959989576657, policy loss: 6.899934220874685
Experience 17, Iter 13, disc loss: 0.015625285747250847, policy loss: 6.52093904980669
Experience 17, Iter 14, disc loss: 0.013860730343669869, policy loss: 7.0105877532469085
Experience 17, Iter 15, disc loss: 0.010816557514217093, policy loss: 6.795644939757581
Experience 17, Iter 16, disc loss: 0.01107511904251153, policy loss: 6.424279893516839
Experience 17, Iter 17, disc loss: 0.009728084478808314, policy loss: 6.215540380184255
Experience 17, Iter 18, disc loss: 0.010048014465650347, policy loss: 5.991677340633842
Experience 17, Iter 19, disc loss: 0.009433353050574828, policy loss: 5.634190114639695
Experience 17, Iter 20, disc loss: 0.009583513722762543, policy loss: 5.673159447267295
Experience 17, Iter 21, disc loss: 0.010128594584666275, policy loss: 5.647663544740626
Experience 17, Iter 22, disc loss: 0.009576512337217295, policy loss: 5.842190281737372
Experience 17, Iter 23, disc loss: 0.014061115633670147, policy loss: 5.373027656928809
Experience 17, Iter 24, disc loss: 0.011614860658443029, policy loss: 5.787110110787973
Experience 17, Iter 25, disc loss: 0.011206119420939761, policy loss: 6.120808218192437
Experience 17, Iter 26, disc loss: 0.011569765993503496, policy loss: 5.8640726884693395
Experience 17, Iter 27, disc loss: 0.009495640275985883, policy loss: 6.184372181814301
Experience 17, Iter 28, disc loss: 0.008165628218915665, policy loss: 6.44432142979339
Experience 17, Iter 29, disc loss: 0.007459686198286573, policy loss: 6.589384111695763
Experience 17, Iter 30, disc loss: 0.008586583774426487, policy loss: 6.310385696943966
Experience 17, Iter 31, disc loss: 0.00813521189003194, policy loss: 6.465136781034307
Experience 17, Iter 32, disc loss: 0.008856130039777388, policy loss: 6.2617223613703406
Experience 17, Iter 33, disc loss: 0.00888302265753892, policy loss: 6.335396594013833
Experience 17, Iter 34, disc loss: 0.008286372140022102, policy loss: 7.13800551249374
Experience 17, Iter 35, disc loss: 0.008827221987936737, policy loss: 6.408203288556331
Experience 17, Iter 36, disc loss: 0.00888117350595825, policy loss: 6.71054646224488
Experience 17, Iter 37, disc loss: 0.009174897889679633, policy loss: 6.945415338699517
Experience 17, Iter 38, disc loss: 0.008868040226795127, policy loss: 6.446953690992183
Experience 17, Iter 39, disc loss: 0.008276718560261093, policy loss: 6.812501934674536
Experience 17, Iter 40, disc loss: 0.007603095030623537, policy loss: 6.739017247355993
Experience 17, Iter 41, disc loss: 0.0068972232746137094, policy loss: 6.670844295933085
Experience 17, Iter 42, disc loss: 0.007099403897865094, policy loss: 6.453968046727482
Experience 17, Iter 43, disc loss: 0.006228239611038593, policy loss: 6.546325370888615
Experience 17, Iter 44, disc loss: 0.00611991455938511, policy loss: 6.512453560695208
Experience 17, Iter 45, disc loss: 0.005999079365752398, policy loss: 6.311359040910418
Experience 17, Iter 46, disc loss: 0.006574191598087066, policy loss: 6.26176377966909
Experience 17, Iter 47, disc loss: 0.007296166441786751, policy loss: 5.9253564980374325
Experience 17, Iter 48, disc loss: 0.006323251054091263, policy loss: 6.33304214195875
Experience 17, Iter 49, disc loss: 0.0080402568917444, policy loss: 6.11064249761678
Experience 17, Iter 50, disc loss: 0.008785034043345397, policy loss: 5.800678634540645
Experience 17, Iter 51, disc loss: 0.008549002544359197, policy loss: 5.899327150164029
Experience 17, Iter 52, disc loss: 0.008786615422853205, policy loss: 6.039174859526858
Experience 17, Iter 53, disc loss: 0.009041531820913771, policy loss: 5.814812238619147
Experience 17, Iter 54, disc loss: 0.010135205292099221, policy loss: 5.629671650928234
Experience 17, Iter 55, disc loss: 0.009015116067878021, policy loss: 6.337317944405536
Experience 17, Iter 56, disc loss: 0.007536959856279996, policy loss: 6.8322329149715655
Experience 17, Iter 57, disc loss: 0.00781617377672068, policy loss: 6.5579982299326245
Experience 17, Iter 58, disc loss: 0.008545878804658208, policy loss: 6.290318890334715
Experience 17, Iter 59, disc loss: 0.009250739769444799, policy loss: 6.073974472941924
Experience 17, Iter 60, disc loss: 0.00890934491847923, policy loss: 6.591581350930517
Experience 17, Iter 61, disc loss: 0.010360025534269453, policy loss: 6.505418078799391
Experience 17, Iter 62, disc loss: 0.009992544403804142, policy loss: 6.711261816688898
Experience 17, Iter 63, disc loss: 0.01094103293367326, policy loss: 7.296747055987524
Experience 17, Iter 64, disc loss: 0.009469404047185274, policy loss: 6.448077725671462
Experience 17, Iter 65, disc loss: 0.009908488409919704, policy loss: 6.449944510709992
Experience 17, Iter 66, disc loss: 0.008593519938998612, policy loss: 6.997733834437586
Experience 17, Iter 67, disc loss: 0.008538900007092724, policy loss: 6.718484875562082
Experience 17, Iter 68, disc loss: 0.009050960509442523, policy loss: 6.516294799662984
Experience 17, Iter 69, disc loss: 0.008294294931226794, policy loss: 6.565898974476674
Experience 17, Iter 70, disc loss: 0.008129837921104846, policy loss: 6.420757319307773
Experience 17, Iter 71, disc loss: 0.0082729780843015, policy loss: 6.285468553096386
Experience 17, Iter 72, disc loss: 0.009537100697961431, policy loss: 5.802985984514663
Experience 17, Iter 73, disc loss: 0.01053122510599635, policy loss: 6.015070234391259
Experience 17, Iter 74, disc loss: 0.00915166001655147, policy loss: 5.983346085344095
Experience 17, Iter 75, disc loss: 0.011718104323897284, policy loss: 5.940009104717969
Experience 17, Iter 76, disc loss: 0.01084033990264682, policy loss: 6.073601972280507
Experience 17, Iter 77, disc loss: 0.011145410158338687, policy loss: 5.848304095258156
Experience 17, Iter 78, disc loss: 0.01148144558184239, policy loss: 5.893454482375507
Experience 17, Iter 79, disc loss: 0.011932868538912983, policy loss: 5.748712845832248
Experience 17, Iter 80, disc loss: 0.012601382133501053, policy loss: 5.929205597441202
Experience 17, Iter 81, disc loss: 0.012174633043761391, policy loss: 6.372949997358654
Experience 17, Iter 82, disc loss: 0.01309185968299656, policy loss: 5.948711485572533
Experience 17, Iter 83, disc loss: 0.014003658011608053, policy loss: 6.017807351006743
Experience 17, Iter 84, disc loss: 0.0127521124156576, policy loss: 6.403571462106608
Experience 17, Iter 85, disc loss: 0.012358861266173832, policy loss: 6.410124792760973
Experience 17, Iter 86, disc loss: 0.013653822261028141, policy loss: 6.29020954751547
Experience 17, Iter 87, disc loss: 0.01378908572597384, policy loss: 6.356361291459174
Experience 17, Iter 88, disc loss: 0.015217409476423159, policy loss: 5.824940496372538
Experience 17, Iter 89, disc loss: 0.014939109674661947, policy loss: 6.338268538713038
Experience 17, Iter 90, disc loss: 0.015224644694796676, policy loss: 5.903165943326824
Experience 17, Iter 91, disc loss: 0.014008864099012934, policy loss: 6.559646419387853
Experience 17, Iter 92, disc loss: 0.01489027015103332, policy loss: 6.33229920484197
Experience 17, Iter 93, disc loss: 0.014156965720037806, policy loss: 6.170418160819667
Experience 17, Iter 94, disc loss: 0.014839009991275992, policy loss: 5.565788240195337
Experience 17, Iter 95, disc loss: 0.016675991236204175, policy loss: 5.42623850949871
Experience 17, Iter 96, disc loss: 0.01775302364584297, policy loss: 5.603409712319488
Experience 17, Iter 97, disc loss: 0.01991791802153581, policy loss: 5.367229190252994
Experience 17, Iter 98, disc loss: 0.01966676117002837, policy loss: 5.743449587295561
Experience 17, Iter 99, disc loss: 0.022731717177029622, policy loss: 5.524140431513966
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0060],
        [0.1522],
        [1.3076],
        [0.0240]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0171, 0.2262, 1.1004, 0.0252, 0.0119, 4.1698]],

        [[0.0171, 0.2262, 1.1004, 0.0252, 0.0119, 4.1698]],

        [[0.0171, 0.2262, 1.1004, 0.0252, 0.0119, 4.1698]],

        [[0.0171, 0.2262, 1.1004, 0.0252, 0.0119, 4.1698]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0239, 0.6087, 5.2302, 0.0960], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0239, 0.6087, 5.2302, 0.0960])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.154
Iter 2/2000 - Loss: 3.123
Iter 3/2000 - Loss: 2.975
Iter 4/2000 - Loss: 2.957
Iter 5/2000 - Loss: 2.916
Iter 6/2000 - Loss: 2.797
Iter 7/2000 - Loss: 2.671
Iter 8/2000 - Loss: 2.573
Iter 9/2000 - Loss: 2.482
Iter 10/2000 - Loss: 2.364
Iter 11/2000 - Loss: 2.213
Iter 12/2000 - Loss: 2.040
Iter 13/2000 - Loss: 1.859
Iter 14/2000 - Loss: 1.670
Iter 15/2000 - Loss: 1.470
Iter 16/2000 - Loss: 1.250
Iter 17/2000 - Loss: 1.008
Iter 18/2000 - Loss: 0.749
Iter 19/2000 - Loss: 0.477
Iter 20/2000 - Loss: 0.197
Iter 1981/2000 - Loss: -7.491
Iter 1982/2000 - Loss: -7.491
Iter 1983/2000 - Loss: -7.491
Iter 1984/2000 - Loss: -7.491
Iter 1985/2000 - Loss: -7.491
Iter 1986/2000 - Loss: -7.491
Iter 1987/2000 - Loss: -7.491
Iter 1988/2000 - Loss: -7.491
Iter 1989/2000 - Loss: -7.491
Iter 1990/2000 - Loss: -7.491
Iter 1991/2000 - Loss: -7.491
Iter 1992/2000 - Loss: -7.491
Iter 1993/2000 - Loss: -7.491
Iter 1994/2000 - Loss: -7.491
Iter 1995/2000 - Loss: -7.491
Iter 1996/2000 - Loss: -7.491
Iter 1997/2000 - Loss: -7.491
Iter 1998/2000 - Loss: -7.491
Iter 1999/2000 - Loss: -7.491
Iter 2000/2000 - Loss: -7.491
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[12.3623,  5.9071, 34.4746,  3.3766,  2.3942, 60.8922]],

        [[18.0504, 32.5405,  8.2182,  1.1027,  1.1275, 20.3585]],

        [[19.1881, 33.8689,  8.1419,  1.1582,  0.7451, 18.5599]],

        [[15.2383, 30.7666, 14.8392,  4.0950,  2.0922, 36.8924]]])
Signal Variance: tensor([ 0.0796,  1.6551, 11.0568,  0.4776])
Estimated target variance: tensor([0.0239, 0.6087, 5.2302, 0.0960])
N: 180
Signal to noise ratio: tensor([16.0945, 77.7791, 78.5726, 37.8272])
Bound on condition number: tensor([  46627.1686, 1088927.4461, 1111257.9141,  257561.9413])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.022738873369619217, policy loss: 5.497718341315739
Experience 18, Iter 1, disc loss: 0.02557956570907553, policy loss: 5.888005342835591
Experience 18, Iter 2, disc loss: 0.03589670485488364, policy loss: 5.1768439889219575
Experience 18, Iter 3, disc loss: 0.03358101117527022, policy loss: 5.806356388981845
Experience 18, Iter 4, disc loss: 0.04916630375105409, policy loss: 6.672428287193641
Experience 18, Iter 5, disc loss: 0.044347341826168306, policy loss: 6.158172775758722
Experience 18, Iter 6, disc loss: 0.050434888194225244, policy loss: 5.954699561865034
Experience 18, Iter 7, disc loss: 0.04985048705302873, policy loss: 7.8234719418981555
Experience 18, Iter 8, disc loss: 0.051584194653519704, policy loss: 6.813491392186378
Experience 18, Iter 9, disc loss: 0.05178155463775587, policy loss: 7.641189182675889
Experience 18, Iter 10, disc loss: 0.04902440926380244, policy loss: 8.291197874889015
Experience 18, Iter 11, disc loss: 0.047243397754222136, policy loss: 6.9067699285188
Experience 18, Iter 12, disc loss: 0.040238577912751816, policy loss: 7.047028267331098
Experience 18, Iter 13, disc loss: 0.057697851996752175, policy loss: 5.629044040610713
Experience 18, Iter 14, disc loss: 0.04541563019308929, policy loss: 5.416318189627422
Experience 18, Iter 15, disc loss: 0.032435448723087956, policy loss: 7.10365399731543
Experience 18, Iter 16, disc loss: 0.030428213635358248, policy loss: 6.789598992880167
Experience 18, Iter 17, disc loss: 0.027748495526268152, policy loss: 6.409443928897205
Experience 18, Iter 18, disc loss: 0.026172970796563735, policy loss: 5.881700852495832
Experience 18, Iter 19, disc loss: 0.030084709122737297, policy loss: 5.81183680784979
Experience 18, Iter 20, disc loss: 0.02411155270659856, policy loss: 6.569001926996126
Experience 18, Iter 21, disc loss: 0.035540875238881026, policy loss: 5.017992494096749
Experience 18, Iter 22, disc loss: 0.027990861179733283, policy loss: 5.972610896689714
Experience 18, Iter 23, disc loss: 0.04851326763897231, policy loss: 5.558058668211068
Experience 18, Iter 24, disc loss: 0.029356457163004508, policy loss: 5.88560565378475
Experience 18, Iter 25, disc loss: 0.03185225349296975, policy loss: 5.864352167223618
Experience 18, Iter 26, disc loss: 0.03037979977976149, policy loss: 5.501860356177212
Experience 18, Iter 27, disc loss: 0.03732771269991378, policy loss: 5.81617431860013
Experience 18, Iter 28, disc loss: 0.03792336675398066, policy loss: 7.543192612461912
Experience 18, Iter 29, disc loss: 0.03367156649297969, policy loss: 6.224736157090758
Experience 18, Iter 30, disc loss: 0.032961297817058664, policy loss: 6.566652562990899
Experience 18, Iter 31, disc loss: 0.03379433511473592, policy loss: 7.078138164445099
Experience 18, Iter 32, disc loss: 0.035058709902590496, policy loss: 5.949157106711352
Experience 18, Iter 33, disc loss: 0.03215774947049607, policy loss: 6.744621871234948
Experience 18, Iter 34, disc loss: 0.044384059592913624, policy loss: 5.182048675287454
Experience 18, Iter 35, disc loss: 0.03483741789889506, policy loss: 7.600325468601891
Experience 18, Iter 36, disc loss: 0.03254705569122271, policy loss: 5.490400992998666
Experience 18, Iter 37, disc loss: 0.03644877679017694, policy loss: 7.983805166754292
Experience 18, Iter 38, disc loss: 0.039968324842478975, policy loss: 7.2380046913296985
Experience 18, Iter 39, disc loss: 0.04511959094061717, policy loss: 5.26603270712045
Experience 18, Iter 40, disc loss: 0.037614967238789265, policy loss: 7.237361821698259
Experience 18, Iter 41, disc loss: 0.03712986269332848, policy loss: 7.490591782205621
Experience 18, Iter 42, disc loss: 0.032180688488818665, policy loss: 6.421095908506218
Experience 18, Iter 43, disc loss: 0.03743097861443186, policy loss: 5.811533997066697
Experience 18, Iter 44, disc loss: 0.03578368540107064, policy loss: 5.8494410021354915
Experience 18, Iter 45, disc loss: 0.0371124157241153, policy loss: 7.263095976436164
Experience 18, Iter 46, disc loss: 0.04625235396753728, policy loss: 6.370379713752691
Experience 18, Iter 47, disc loss: 0.03389337445622888, policy loss: 6.855851914265388
Experience 18, Iter 48, disc loss: 0.03596387141281805, policy loss: 6.957522908272711
Experience 18, Iter 49, disc loss: 0.035319358333174136, policy loss: 6.435771651075818
Experience 18, Iter 50, disc loss: 0.03498981236724859, policy loss: 6.809231833232284
Experience 18, Iter 51, disc loss: 0.035389848951999245, policy loss: 6.941624234694329
Experience 18, Iter 52, disc loss: 0.04094633796708329, policy loss: 5.434781464132154
Experience 18, Iter 53, disc loss: 0.03759996710916881, policy loss: 6.540829925122029
Experience 18, Iter 54, disc loss: 0.036341002635017196, policy loss: 7.793350610020185
Experience 18, Iter 55, disc loss: 0.03417971549277874, policy loss: 6.384487807113555
Experience 18, Iter 56, disc loss: 0.0342421801054311, policy loss: 6.010457514910156
Experience 18, Iter 57, disc loss: 0.029665944525143158, policy loss: 6.5595348471395125
Experience 18, Iter 58, disc loss: 0.03133334177367633, policy loss: 6.229463146708389
Experience 18, Iter 59, disc loss: 0.029949229918573007, policy loss: 6.065620887003065
Experience 18, Iter 60, disc loss: 0.032808885816999034, policy loss: 6.519392812828107
Experience 18, Iter 61, disc loss: 0.027505928773474346, policy loss: 6.009730389267201
Experience 18, Iter 62, disc loss: 0.030910172359979983, policy loss: 7.211760625998771
Experience 18, Iter 63, disc loss: 0.03199500741971933, policy loss: 5.901756487937988
Experience 18, Iter 64, disc loss: 0.022250115784195598, policy loss: 6.589903745362281
Experience 18, Iter 65, disc loss: 0.022799599917054755, policy loss: 7.41512170804782
Experience 18, Iter 66, disc loss: 0.02503004401095148, policy loss: 6.012569945187915
Experience 18, Iter 67, disc loss: 0.02663812729731599, policy loss: 6.5671872657166706
Experience 18, Iter 68, disc loss: 0.032903935136613584, policy loss: 5.418763958715227
Experience 18, Iter 69, disc loss: 0.0285770433298607, policy loss: 6.234902632116649
Experience 18, Iter 70, disc loss: 0.022563585608679968, policy loss: 7.378233617403328
Experience 18, Iter 71, disc loss: 0.027960017748069263, policy loss: 6.239738326122268
Experience 18, Iter 72, disc loss: 0.026098861318314626, policy loss: 6.773865928763142
Experience 18, Iter 73, disc loss: 0.02722929241703117, policy loss: 6.865790583269648
Experience 18, Iter 74, disc loss: 0.030088245708706773, policy loss: 5.8997598284350685
Experience 18, Iter 75, disc loss: 0.033094610228296475, policy loss: 6.158077084225134
Experience 18, Iter 76, disc loss: 0.030120803393441115, policy loss: 6.45915453341194
Experience 18, Iter 77, disc loss: 0.026566891936264984, policy loss: 5.966941505738458
Experience 18, Iter 78, disc loss: 0.023280315487914663, policy loss: 8.092370853296066
Experience 18, Iter 79, disc loss: 0.02548455223136134, policy loss: 6.014932845692522
Experience 18, Iter 80, disc loss: 0.026462049502651637, policy loss: 7.1865072371835215
Experience 18, Iter 81, disc loss: 0.022547748485902498, policy loss: 6.689214033739103
Experience 18, Iter 82, disc loss: 0.022930388234986804, policy loss: 6.55295322921551
Experience 18, Iter 83, disc loss: 0.02011191372709545, policy loss: 6.037377226607186
Experience 18, Iter 84, disc loss: 0.017501285105926072, policy loss: 6.880496757361046
Experience 18, Iter 85, disc loss: 0.0175551891461773, policy loss: 6.401783749618333
Experience 18, Iter 86, disc loss: 0.016667482421810917, policy loss: 6.537556681388418
Experience 18, Iter 87, disc loss: 0.018129492075197807, policy loss: 7.0199225971784465
Experience 18, Iter 88, disc loss: 0.019311271379889784, policy loss: 5.905819981022211
Experience 18, Iter 89, disc loss: 0.018730011385851564, policy loss: 6.592128461830867
Experience 18, Iter 90, disc loss: 0.018035835297209587, policy loss: 6.482920148882317
Experience 18, Iter 91, disc loss: 0.016983982966266103, policy loss: 6.394644847085186
Experience 18, Iter 92, disc loss: 0.015728410065172733, policy loss: 7.044231452975385
Experience 18, Iter 93, disc loss: 0.017186536863384894, policy loss: 7.427498975033848
Experience 18, Iter 94, disc loss: 0.018722268047522247, policy loss: 6.843071981079638
Experience 18, Iter 95, disc loss: 0.01596086801576658, policy loss: 6.976976037831171
Experience 18, Iter 96, disc loss: 0.015283808826362445, policy loss: 8.213214051360476
Experience 18, Iter 97, disc loss: 0.015492382649968014, policy loss: 8.135740635603156
Experience 18, Iter 98, disc loss: 0.01667634480102676, policy loss: 6.816471345734106
Experience 18, Iter 99, disc loss: 0.01750826463109482, policy loss: 6.034256978046357
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.1590],
        [1.3748],
        [0.0263]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0165, 0.2277, 1.2036, 0.0260, 0.0137, 4.2992]],

        [[0.0165, 0.2277, 1.2036, 0.0260, 0.0137, 4.2992]],

        [[0.0165, 0.2277, 1.2036, 0.0260, 0.0137, 4.2992]],

        [[0.0165, 0.2277, 1.2036, 0.0260, 0.0137, 4.2992]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0237, 0.6360, 5.4990, 0.1050], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0237, 0.6360, 5.4990, 0.1050])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.249
Iter 2/2000 - Loss: 3.209
Iter 3/2000 - Loss: 3.056
Iter 4/2000 - Loss: 3.033
Iter 5/2000 - Loss: 2.990
Iter 6/2000 - Loss: 2.865
Iter 7/2000 - Loss: 2.729
Iter 8/2000 - Loss: 2.621
Iter 9/2000 - Loss: 2.522
Iter 10/2000 - Loss: 2.397
Iter 11/2000 - Loss: 2.240
Iter 12/2000 - Loss: 2.062
Iter 13/2000 - Loss: 1.875
Iter 14/2000 - Loss: 1.681
Iter 15/2000 - Loss: 1.473
Iter 16/2000 - Loss: 1.246
Iter 17/2000 - Loss: 1.000
Iter 18/2000 - Loss: 0.738
Iter 19/2000 - Loss: 0.465
Iter 20/2000 - Loss: 0.186
Iter 1981/2000 - Loss: -7.476
Iter 1982/2000 - Loss: -7.476
Iter 1983/2000 - Loss: -7.476
Iter 1984/2000 - Loss: -7.476
Iter 1985/2000 - Loss: -7.476
Iter 1986/2000 - Loss: -7.476
Iter 1987/2000 - Loss: -7.476
Iter 1988/2000 - Loss: -7.476
Iter 1989/2000 - Loss: -7.476
Iter 1990/2000 - Loss: -7.476
Iter 1991/2000 - Loss: -7.476
Iter 1992/2000 - Loss: -7.476
Iter 1993/2000 - Loss: -7.476
Iter 1994/2000 - Loss: -7.476
Iter 1995/2000 - Loss: -7.476
Iter 1996/2000 - Loss: -7.476
Iter 1997/2000 - Loss: -7.476
Iter 1998/2000 - Loss: -7.477
Iter 1999/2000 - Loss: -7.477
Iter 2000/2000 - Loss: -7.477
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.6629,  5.6521, 31.9926,  3.0815,  2.4755, 60.2966]],

        [[17.7160, 37.8663,  8.0688,  1.1380,  0.9672, 20.8179]],

        [[19.2944, 37.3395,  8.1847,  1.1118,  0.7226, 19.0137]],

        [[13.9333, 28.4897, 14.0670,  3.9721,  1.9232, 40.1061]]])
Signal Variance: tensor([0.0713, 1.4592, 9.9787, 0.4782])
Estimated target variance: tensor([0.0237, 0.6360, 5.4990, 0.1050])
N: 190
Signal to noise ratio: tensor([15.2454, 71.0991, 72.8550, 37.5558])
Bound on condition number: tensor([  44161.0193,  960466.0793, 1008493.9591,  267983.9002])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.01357890455330692, policy loss: 8.454655209225026
Experience 19, Iter 1, disc loss: 0.016130883829097685, policy loss: 7.034372797311349
Experience 19, Iter 2, disc loss: 0.014059715869855634, policy loss: 6.854086563117232
Experience 19, Iter 3, disc loss: 0.013623332627593156, policy loss: 6.300573529339598
Experience 19, Iter 4, disc loss: 0.013330507467510262, policy loss: 6.37033827262475
Experience 19, Iter 5, disc loss: 0.011845115192833563, policy loss: 6.851664552101658
Experience 19, Iter 6, disc loss: 0.012926064061713758, policy loss: 6.721372896845623
Experience 19, Iter 7, disc loss: 0.012792499019995797, policy loss: 6.838409292023795
Experience 19, Iter 8, disc loss: 0.012272105893069173, policy loss: 6.489994166731036
Experience 19, Iter 9, disc loss: 0.013544576956960992, policy loss: 5.633994264364178
Experience 19, Iter 10, disc loss: 0.012101508235618473, policy loss: 7.677834514959008
Experience 19, Iter 11, disc loss: 0.011065131636433731, policy loss: 6.477746585676332
Experience 19, Iter 12, disc loss: 0.011626628236810523, policy loss: 7.956834456967137
Experience 19, Iter 13, disc loss: 0.011717098955209606, policy loss: 6.861180654104
Experience 19, Iter 14, disc loss: 0.011655623254677578, policy loss: 6.433919134953865
Experience 19, Iter 15, disc loss: 0.01170014494553886, policy loss: 6.557244266310837
Experience 19, Iter 16, disc loss: 0.011834374905045842, policy loss: 7.195167869593402
Experience 19, Iter 17, disc loss: 0.012101740525705727, policy loss: 6.588033565351337
Experience 19, Iter 18, disc loss: 0.011844180150162115, policy loss: 6.968397455212951
Experience 19, Iter 19, disc loss: 0.010845449701658006, policy loss: 6.9559756068367316
Experience 19, Iter 20, disc loss: 0.012585925804272084, policy loss: 7.059680195258401
Experience 19, Iter 21, disc loss: 0.012855296808401114, policy loss: 6.277341354143511
Experience 19, Iter 22, disc loss: 0.013034390476185025, policy loss: 6.325575576262098
Experience 19, Iter 23, disc loss: 0.011171106443343307, policy loss: 7.705593169240778
Experience 19, Iter 24, disc loss: 0.01319064587906881, policy loss: 6.572842107266944
Experience 19, Iter 25, disc loss: 0.011081248690298282, policy loss: 7.884464287400284
Experience 19, Iter 26, disc loss: 0.010562298154244858, policy loss: 8.269210557563218
Experience 19, Iter 27, disc loss: 0.01112207636586662, policy loss: 6.40759018461034
Experience 19, Iter 28, disc loss: 0.01097424231527525, policy loss: 7.547633074014391
Experience 19, Iter 29, disc loss: 0.010158879317207613, policy loss: 7.080694753367598
Experience 19, Iter 30, disc loss: 0.01063066862512168, policy loss: 6.839411399318528
Experience 19, Iter 31, disc loss: 0.009213138503973445, policy loss: 8.03831357259151
Experience 19, Iter 32, disc loss: 0.009358683095932617, policy loss: 7.203538533525832
Experience 19, Iter 33, disc loss: 0.00954508811921376, policy loss: 6.94087602220691
Experience 19, Iter 34, disc loss: 0.008865478601860597, policy loss: 7.171157480114379
Experience 19, Iter 35, disc loss: 0.009731464297968227, policy loss: 6.911887489446045
Experience 19, Iter 36, disc loss: 0.008339447685240114, policy loss: 8.054926159433839
Experience 19, Iter 37, disc loss: 0.009086019572020649, policy loss: 7.267224824143998
Experience 19, Iter 38, disc loss: 0.00846773077758128, policy loss: 7.249964590131697
Experience 19, Iter 39, disc loss: 0.008931601347900825, policy loss: 6.0514964707939285
Experience 19, Iter 40, disc loss: 0.010771062573066446, policy loss: 6.745723797318325
Experience 19, Iter 41, disc loss: 0.009522383607411599, policy loss: 7.291951859175965
Experience 19, Iter 42, disc loss: 0.009564762892134253, policy loss: 6.954870603149796
Experience 19, Iter 43, disc loss: 0.009113828187018978, policy loss: 6.863081618497731
Experience 19, Iter 44, disc loss: 0.009041892222605184, policy loss: 7.161526841121341
Experience 19, Iter 45, disc loss: 0.009546621105444887, policy loss: 6.825515509710529
Experience 19, Iter 46, disc loss: 0.00900709651728961, policy loss: 7.204136011789197
Experience 19, Iter 47, disc loss: 0.008080688782478397, policy loss: 7.620612649833653
Experience 19, Iter 48, disc loss: 0.009138500473495082, policy loss: 7.451773938965447
Experience 19, Iter 49, disc loss: 0.008740340767293742, policy loss: 6.803301634105917
Experience 19, Iter 50, disc loss: 0.007226714318254497, policy loss: 8.74940890253761
Experience 19, Iter 51, disc loss: 0.008688490045767001, policy loss: 6.627861693536693
Experience 19, Iter 52, disc loss: 0.009535324785640651, policy loss: 6.690763359137868
Experience 19, Iter 53, disc loss: 0.0098793524790193, policy loss: 6.884268955565187
Experience 19, Iter 54, disc loss: 0.008472721972522045, policy loss: 7.340904812070814
Experience 19, Iter 55, disc loss: 0.007181423446693076, policy loss: 7.562879224685577
Experience 19, Iter 56, disc loss: 0.007871544442741345, policy loss: 6.986440563832852
Experience 19, Iter 57, disc loss: 0.009598803602851438, policy loss: 6.596926673673658
Experience 19, Iter 58, disc loss: 0.007591325756809002, policy loss: 7.418406152290993
Experience 19, Iter 59, disc loss: 0.007172389210208066, policy loss: 7.874844016749359
Experience 19, Iter 60, disc loss: 0.006947390880945107, policy loss: 7.2265531533407
Experience 19, Iter 61, disc loss: 0.007368558069263689, policy loss: 7.1588274498750035
Experience 19, Iter 62, disc loss: 0.008405692394007794, policy loss: 6.984885144707948
Experience 19, Iter 63, disc loss: 0.007814406355454273, policy loss: 7.944423685491513
Experience 19, Iter 64, disc loss: 0.007670368292192747, policy loss: 7.598757836378228
Experience 19, Iter 65, disc loss: 0.00976166675267405, policy loss: 6.9854270466753965
Experience 19, Iter 66, disc loss: 0.008451842015704668, policy loss: 7.26731233621474
Experience 19, Iter 67, disc loss: 0.00734249367618143, policy loss: 7.306045374045107
Experience 19, Iter 68, disc loss: 0.007403178101072395, policy loss: 7.001935875529928
Experience 19, Iter 69, disc loss: 0.007212625708002176, policy loss: 6.907413470478907
Experience 19, Iter 70, disc loss: 0.007425618720276136, policy loss: 6.695681681007739
Experience 19, Iter 71, disc loss: 0.006671465855065024, policy loss: 7.2521892162378885
Experience 19, Iter 72, disc loss: 0.006845125626741097, policy loss: 7.224853137068292
Experience 19, Iter 73, disc loss: 0.006924272075499528, policy loss: 6.653820551097629
Experience 19, Iter 74, disc loss: 0.006959050740363177, policy loss: 7.335082720720788
Experience 19, Iter 75, disc loss: 0.006555397869386601, policy loss: 7.553346692640711
Experience 19, Iter 76, disc loss: 0.007652472122052746, policy loss: 6.89236442738066
Experience 19, Iter 77, disc loss: 0.007073594297018188, policy loss: 7.6397212217642245
Experience 19, Iter 78, disc loss: 0.006651096859011851, policy loss: 7.348278074182851
Experience 19, Iter 79, disc loss: 0.006958860393187405, policy loss: 7.198165736288989
Experience 19, Iter 80, disc loss: 0.00603333845085874, policy loss: 7.520943355265633
Experience 19, Iter 81, disc loss: 0.00677969491942741, policy loss: 7.279976882261884
Experience 19, Iter 82, disc loss: 0.005967656857028474, policy loss: 8.463981958572557
Experience 19, Iter 83, disc loss: 0.005867205260239098, policy loss: 7.541461293549286
Experience 19, Iter 84, disc loss: 0.005785658423506266, policy loss: 8.318777901051897
Experience 19, Iter 85, disc loss: 0.006536632204405424, policy loss: 6.953935931062991
Experience 19, Iter 86, disc loss: 0.0055899360435154675, policy loss: 7.270128950573646
Experience 19, Iter 87, disc loss: 0.005824377018133832, policy loss: 7.111571096941804
Experience 19, Iter 88, disc loss: 0.005682172501508756, policy loss: 7.787938775339989
Experience 19, Iter 89, disc loss: 0.006114546786047072, policy loss: 7.536202120580668
Experience 19, Iter 90, disc loss: 0.005449365080648554, policy loss: 7.6045772010800885
Experience 19, Iter 91, disc loss: 0.005659692259375734, policy loss: 7.98405250011197
Experience 19, Iter 92, disc loss: 0.005432650831938407, policy loss: 7.671418585733758
Experience 19, Iter 93, disc loss: 0.006475264975545923, policy loss: 7.383130338204152
Experience 19, Iter 94, disc loss: 0.006910787317889829, policy loss: 7.056188620389722
Experience 19, Iter 95, disc loss: 0.005161099415659553, policy loss: 7.8410457778269596
Experience 19, Iter 96, disc loss: 0.005943689431864529, policy loss: 7.82414576024635
Experience 19, Iter 97, disc loss: 0.006145158119114423, policy loss: 6.663326899816229
Experience 19, Iter 98, disc loss: 0.006489081847856052, policy loss: 7.70942058289161
Experience 19, Iter 99, disc loss: 0.007233884848603683, policy loss: 7.675480099259555
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.1661],
        [1.4451],
        [0.0287]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0160, 0.2291, 1.3063, 0.0265, 0.0160, 4.4162]],

        [[0.0160, 0.2291, 1.3063, 0.0265, 0.0160, 4.4162]],

        [[0.0160, 0.2291, 1.3063, 0.0265, 0.0160, 4.4162]],

        [[0.0160, 0.2291, 1.3063, 0.0265, 0.0160, 4.4162]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0235, 0.6642, 5.7806, 0.1148], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0235, 0.6642, 5.7806, 0.1148])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.327
Iter 2/2000 - Loss: 3.269
Iter 3/2000 - Loss: 3.104
Iter 4/2000 - Loss: 3.070
Iter 5/2000 - Loss: 3.026
Iter 6/2000 - Loss: 2.897
Iter 7/2000 - Loss: 2.754
Iter 8/2000 - Loss: 2.639
Iter 9/2000 - Loss: 2.531
Iter 10/2000 - Loss: 2.400
Iter 11/2000 - Loss: 2.239
Iter 12/2000 - Loss: 2.059
Iter 13/2000 - Loss: 1.870
Iter 14/2000 - Loss: 1.671
Iter 15/2000 - Loss: 1.457
Iter 16/2000 - Loss: 1.222
Iter 17/2000 - Loss: 0.970
Iter 18/2000 - Loss: 0.705
Iter 19/2000 - Loss: 0.430
Iter 20/2000 - Loss: 0.148
Iter 1981/2000 - Loss: -7.505
Iter 1982/2000 - Loss: -7.505
Iter 1983/2000 - Loss: -7.505
Iter 1984/2000 - Loss: -7.505
Iter 1985/2000 - Loss: -7.505
Iter 1986/2000 - Loss: -7.505
Iter 1987/2000 - Loss: -7.506
Iter 1988/2000 - Loss: -7.506
Iter 1989/2000 - Loss: -7.506
Iter 1990/2000 - Loss: -7.506
Iter 1991/2000 - Loss: -7.506
Iter 1992/2000 - Loss: -7.506
Iter 1993/2000 - Loss: -7.506
Iter 1994/2000 - Loss: -7.506
Iter 1995/2000 - Loss: -7.506
Iter 1996/2000 - Loss: -7.506
Iter 1997/2000 - Loss: -7.506
Iter 1998/2000 - Loss: -7.506
Iter 1999/2000 - Loss: -7.506
Iter 2000/2000 - Loss: -7.506
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[12.6157,  6.2111, 34.6113,  3.4906,  2.9321, 62.0076]],

        [[18.1543, 37.6172,  7.4675,  1.1097,  1.1035, 22.1140]],

        [[19.1357, 37.9920,  8.0454,  1.0658,  0.7389, 19.7430]],

        [[13.6074, 28.6297, 13.9099,  2.9638,  1.6980, 39.6046]]])
Signal Variance: tensor([ 0.0785,  1.5865, 10.1982,  0.4065])
Estimated target variance: tensor([0.0235, 0.6642, 5.7806, 0.1148])
N: 200
Signal to noise ratio: tensor([15.5772, 75.3807, 73.0375, 35.1426])
Bound on condition number: tensor([  48530.5435, 1136450.5118, 1066897.4756,  247001.4832])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.006780524788191133, policy loss: 6.966274640774044
Experience 20, Iter 1, disc loss: 0.0068851292504260335, policy loss: 7.206888134074256
Experience 20, Iter 2, disc loss: 0.007752754504042366, policy loss: 7.238346702555249
Experience 20, Iter 3, disc loss: 0.005412914774212659, policy loss: 8.921953485738891
Experience 20, Iter 4, disc loss: 0.006010207238141628, policy loss: 7.564930407465738
Experience 20, Iter 5, disc loss: 0.006646564077436235, policy loss: 7.973366451363218
Experience 20, Iter 6, disc loss: 0.005910331134981328, policy loss: 7.432608401949894
Experience 20, Iter 7, disc loss: 0.005796472043722929, policy loss: 7.116648974190754
Experience 20, Iter 8, disc loss: 0.006064199367069209, policy loss: 7.868249366645326
Experience 20, Iter 9, disc loss: 0.005364376934082241, policy loss: 7.348977679116725
Experience 20, Iter 10, disc loss: 0.005400109555193632, policy loss: 7.521926978435718
Experience 20, Iter 11, disc loss: 0.006003228047245896, policy loss: 8.017998693489934
Experience 20, Iter 12, disc loss: 0.00548167775723453, policy loss: 7.69439164733885
Experience 20, Iter 13, disc loss: 0.005826879769269662, policy loss: 7.940824978163641
Experience 20, Iter 14, disc loss: 0.00555183864579362, policy loss: 7.417268788200319
Experience 20, Iter 15, disc loss: 0.005282222199068747, policy loss: 7.0862534910671116
Experience 20, Iter 16, disc loss: 0.00565506362125914, policy loss: 8.013644791537955
Experience 20, Iter 17, disc loss: 0.005399032589184233, policy loss: 7.6588557301592415
Experience 20, Iter 18, disc loss: 0.005566038227943674, policy loss: 7.881705445411761
Experience 20, Iter 19, disc loss: 0.0050999464982038915, policy loss: 7.737901639769513
Experience 20, Iter 20, disc loss: 0.005252249154980916, policy loss: 6.78195990082796
Experience 20, Iter 21, disc loss: 0.004686686307064132, policy loss: 7.287667041093249
Experience 20, Iter 22, disc loss: 0.005402343277389724, policy loss: 7.454375240877152
Experience 20, Iter 23, disc loss: 0.004962353696483205, policy loss: 7.497658967809933
Experience 20, Iter 24, disc loss: 0.004745460848364708, policy loss: 7.270808187609416
Experience 20, Iter 25, disc loss: 0.005785522143292958, policy loss: 6.37987738033586
Experience 20, Iter 26, disc loss: 0.004998177582493187, policy loss: 7.6675334109031965
Experience 20, Iter 27, disc loss: 0.0055084704230003445, policy loss: 7.327706636195687
Experience 20, Iter 28, disc loss: 0.005470519184755684, policy loss: 7.471687628876115
Experience 20, Iter 29, disc loss: 0.005112141844582454, policy loss: 7.755363641064976
Experience 20, Iter 30, disc loss: 0.005013667309231973, policy loss: 7.554915859535468
Experience 20, Iter 31, disc loss: 0.0058557312849044155, policy loss: 7.100757422726314
Experience 20, Iter 32, disc loss: 0.004728239718570425, policy loss: 8.424077322614263
Experience 20, Iter 33, disc loss: 0.004944323912072343, policy loss: 7.68774698160585
Experience 20, Iter 34, disc loss: 0.005812683476849977, policy loss: 7.257331326623584
Experience 20, Iter 35, disc loss: 0.005485096678064136, policy loss: 7.847094626663809
Experience 20, Iter 36, disc loss: 0.0046843972733116735, policy loss: 7.482118918988132
Experience 20, Iter 37, disc loss: 0.005049468011891258, policy loss: 7.304706396283615
Experience 20, Iter 38, disc loss: 0.004624147080683425, policy loss: 7.936865962360086
Experience 20, Iter 39, disc loss: 0.004786713579356188, policy loss: 7.881483776065018
Experience 20, Iter 40, disc loss: 0.00570304657087593, policy loss: 7.2878514641456125
Experience 20, Iter 41, disc loss: 0.0052694250546822165, policy loss: 7.3799028570874485
Experience 20, Iter 42, disc loss: 0.004798453537864504, policy loss: 7.552812012673888
Experience 20, Iter 43, disc loss: 0.004655966434028151, policy loss: 7.277645126040801
Experience 20, Iter 44, disc loss: 0.004456001974246421, policy loss: 7.631166248863903
Experience 20, Iter 45, disc loss: 0.0051321377788807335, policy loss: 7.824476762075967
Experience 20, Iter 46, disc loss: 0.005175147634837578, policy loss: 7.957364870663659
Experience 20, Iter 47, disc loss: 0.005599959366348837, policy loss: 7.00362239738759
Experience 20, Iter 48, disc loss: 0.004983656508961928, policy loss: 7.368457564791296
Experience 20, Iter 49, disc loss: 0.005210701607724678, policy loss: 7.3757511788075965
Experience 20, Iter 50, disc loss: 0.005050990503175105, policy loss: 7.374564916751341
Experience 20, Iter 51, disc loss: 0.0046231385906195825, policy loss: 7.942544292868259
Experience 20, Iter 52, disc loss: 0.005334501405661548, policy loss: 7.013981064894457
Experience 20, Iter 53, disc loss: 0.006051460503371699, policy loss: 7.1245939655262305
Experience 20, Iter 54, disc loss: 0.005361744986652641, policy loss: 6.737438795596303
Experience 20, Iter 55, disc loss: 0.00568690493610957, policy loss: 7.171427793000337
Experience 20, Iter 56, disc loss: 0.004819962014012529, policy loss: 7.954310870909818
Experience 20, Iter 57, disc loss: 0.005147795643791201, policy loss: 7.739415135866188
Experience 20, Iter 58, disc loss: 0.005310856488868713, policy loss: 7.206469267876394
Experience 20, Iter 59, disc loss: 0.004791507328501298, policy loss: 7.572537405929421
Experience 20, Iter 60, disc loss: 0.0049179557647469506, policy loss: 8.041880397539398
Experience 20, Iter 61, disc loss: 0.00527484239024523, policy loss: 7.223013139396212
Experience 20, Iter 62, disc loss: 0.004622988544271093, policy loss: 7.70628718871227
Experience 20, Iter 63, disc loss: 0.004410393148431107, policy loss: 7.60920816735157
Experience 20, Iter 64, disc loss: 0.004361201781886854, policy loss: 7.4883807226881824
Experience 20, Iter 65, disc loss: 0.004719866767402313, policy loss: 7.386242107442956
Experience 20, Iter 66, disc loss: 0.005086034168090581, policy loss: 7.610239217199144
Experience 20, Iter 67, disc loss: 0.004923128753702499, policy loss: 7.468794984330954
Experience 20, Iter 68, disc loss: 0.005364492107376744, policy loss: 8.230011643965046
Experience 20, Iter 69, disc loss: 0.004301113414285761, policy loss: 7.63226285672831
Experience 20, Iter 70, disc loss: 0.005088481545997992, policy loss: 7.21871077476465
Experience 20, Iter 71, disc loss: 0.006324510446483619, policy loss: 7.249625526631861
Experience 20, Iter 72, disc loss: 0.005275997346908768, policy loss: 8.1051889214365
Experience 20, Iter 73, disc loss: 0.004520214220716253, policy loss: 8.029623258151029
Experience 20, Iter 74, disc loss: 0.0051138841953365256, policy loss: 7.079911215116162
Experience 20, Iter 75, disc loss: 0.005208040741805825, policy loss: 7.4650772731992765
Experience 20, Iter 76, disc loss: 0.005211491523975039, policy loss: 7.0305033962989585
Experience 20, Iter 77, disc loss: 0.00517794299373309, policy loss: 7.222458212519838
Experience 20, Iter 78, disc loss: 0.004670983492122955, policy loss: 7.413443349004344
Experience 20, Iter 79, disc loss: 0.004572104044421788, policy loss: 7.432361808460136
Experience 20, Iter 80, disc loss: 0.004491925886969241, policy loss: 7.389854604195008
Experience 20, Iter 81, disc loss: 0.004688261091550532, policy loss: 7.751535209390466
Experience 20, Iter 82, disc loss: 0.004549366718262986, policy loss: 8.05160517155307
Experience 20, Iter 83, disc loss: 0.004243947367016927, policy loss: 7.651097729205026
Experience 20, Iter 84, disc loss: 0.004439970601416258, policy loss: 8.293040279955632
Experience 20, Iter 85, disc loss: 0.004559987965203039, policy loss: 7.894807582705413
Experience 20, Iter 86, disc loss: 0.004864103453922498, policy loss: 7.2696494531171885
Experience 20, Iter 87, disc loss: 0.004079428870892769, policy loss: 7.687992735019613
Experience 20, Iter 88, disc loss: 0.0056837320031628506, policy loss: 8.319467588781226
Experience 20, Iter 89, disc loss: 0.004708515677099076, policy loss: 8.391492568390738
Experience 20, Iter 90, disc loss: 0.004203540164562157, policy loss: 7.9155891171479
Experience 20, Iter 91, disc loss: 0.004608053680990858, policy loss: 6.971991835736713
Experience 20, Iter 92, disc loss: 0.004997052809584511, policy loss: 7.541352452108944
Experience 20, Iter 93, disc loss: 0.004476423516230808, policy loss: 7.556356412221148
Experience 20, Iter 94, disc loss: 0.005125901472506487, policy loss: 8.383303943896586
Experience 20, Iter 95, disc loss: 0.004562644548325648, policy loss: 8.183406596413748
Experience 20, Iter 96, disc loss: 0.0051015514421612045, policy loss: 8.25289360316791
Experience 20, Iter 97, disc loss: 0.0051977524564036336, policy loss: 7.231201678928392
Experience 20, Iter 98, disc loss: 0.0050618835153973165, policy loss: 8.18680097826238
Experience 20, Iter 99, disc loss: 0.004310151673246504, policy loss: 7.696985909713087
