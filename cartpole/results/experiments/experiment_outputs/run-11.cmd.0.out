Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0010],
        [0.0517],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0039, 0.2066, 0.0032], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0039, 0.2066, 0.0032])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.858
Iter 2/2000 - Loss: -0.338
Iter 3/2000 - Loss: -3.818
Iter 4/2000 - Loss: -4.018
Iter 5/2000 - Loss: -2.673
Iter 6/2000 - Loss: -2.883
Iter 7/2000 - Loss: -3.902
Iter 8/2000 - Loss: -4.370
Iter 9/2000 - Loss: -4.039
Iter 10/2000 - Loss: -3.576
Iter 11/2000 - Loss: -3.556
Iter 12/2000 - Loss: -3.875
Iter 13/2000 - Loss: -4.153
Iter 14/2000 - Loss: -4.239
Iter 15/2000 - Loss: -4.178
Iter 16/2000 - Loss: -4.025
Iter 17/2000 - Loss: -3.907
Iter 18/2000 - Loss: -3.982
Iter 19/2000 - Loss: -4.189
Iter 20/2000 - Loss: -4.310
Iter 1981/2000 - Loss: -4.605
Iter 1982/2000 - Loss: -4.605
Iter 1983/2000 - Loss: -4.605
Iter 1984/2000 - Loss: -4.605
Iter 1985/2000 - Loss: -4.605
Iter 1986/2000 - Loss: -4.605
Iter 1987/2000 - Loss: -4.605
Iter 1988/2000 - Loss: -4.604
Iter 1989/2000 - Loss: -4.604
Iter 1990/2000 - Loss: -4.603
Iter 1991/2000 - Loss: -4.602
Iter 1992/2000 - Loss: -4.600
Iter 1993/2000 - Loss: -4.597
Iter 1994/2000 - Loss: -4.592
Iter 1995/2000 - Loss: -4.586
Iter 1996/2000 - Loss: -4.579
Iter 1997/2000 - Loss: -4.572
Iter 1998/2000 - Loss: -4.569
Iter 1999/2000 - Loss: -4.574
Iter 2000/2000 - Loss: -4.587
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0007],
        [0.0358],
        [0.0006]])
Lengthscale: tensor([[[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]],

        [[4.7779e-04, 4.8687e-03, 4.5646e-02, 1.0375e-03, 4.0709e-06,
          3.5588e-03]]])
Signal Variance: tensor([0.0005, 0.0028, 0.1502, 0.0023])
Estimated target variance: tensor([0.0007, 0.0039, 0.2066, 0.0032])
N: 10
Signal to noise ratio: tensor([1.9987, 2.0031, 2.0486, 2.0009])
Bound on condition number: tensor([40.9476, 41.1255, 42.9692, 41.0345])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4720038870739676, policy loss: 0.7469296738567981
Experience 1, Iter 1, disc loss: 1.4518015545112042, policy loss: 0.7472340258416041
Experience 1, Iter 2, disc loss: 1.4276811157829281, policy loss: 0.7520844954915609
Experience 1, Iter 3, disc loss: 1.4140317809270058, policy loss: 0.7496999799199185
Experience 1, Iter 4, disc loss: 1.4007639065395123, policy loss: 0.7502638815168663
Experience 1, Iter 5, disc loss: 1.3879002837867915, policy loss: 0.7502092735896401
Experience 1, Iter 6, disc loss: 1.377229327331285, policy loss: 0.7485884101130384
Experience 1, Iter 7, disc loss: 1.3646289297416456, policy loss: 0.7487574667114326
Experience 1, Iter 8, disc loss: 1.3489231044962833, policy loss: 0.7534082717609187
Experience 1, Iter 9, disc loss: 1.337465732077875, policy loss: 0.7531473633721923
Experience 1, Iter 10, disc loss: 1.3251831302653847, policy loss: 0.7537970177117137
Experience 1, Iter 11, disc loss: 1.314410313688571, policy loss: 0.7535391361554082
Experience 1, Iter 12, disc loss: 1.3000723925343505, policy loss: 0.7571423291378926
Experience 1, Iter 13, disc loss: 1.288329394334545, policy loss: 0.758157209333662
Experience 1, Iter 14, disc loss: 1.2814685949022526, policy loss: 0.7538810260488801
Experience 1, Iter 15, disc loss: 1.268483720619779, policy loss: 0.7576817445442889
Experience 1, Iter 16, disc loss: 1.2585219814699586, policy loss: 0.7579447667425199
Experience 1, Iter 17, disc loss: 1.2473742693343142, policy loss: 0.7600625631267409
Experience 1, Iter 18, disc loss: 1.2372380477986933, policy loss: 0.7613708567914279
Experience 1, Iter 19, disc loss: 1.2285402269199097, policy loss: 0.7616112810963673
Experience 1, Iter 20, disc loss: 1.2164881218751238, policy loss: 0.7652071453447747
Experience 1, Iter 21, disc loss: 1.2073312798052283, policy loss: 0.7655211752310956
Experience 1, Iter 22, disc loss: 1.1989562411449197, policy loss: 0.7650396639889925
Experience 1, Iter 23, disc loss: 1.185759790221482, policy loss: 0.7705800741517327
Experience 1, Iter 24, disc loss: 1.1788150007811722, policy loss: 0.768923296938273
Experience 1, Iter 25, disc loss: 1.1674608211180522, policy loss: 0.7722373974776449
Experience 1, Iter 26, disc loss: 1.1572220516737717, policy loss: 0.7748454195240551
Experience 1, Iter 27, disc loss: 1.15076545805948, policy loss: 0.7722957773286876
Experience 1, Iter 28, disc loss: 1.1346407983863132, policy loss: 0.7817578644300639
Experience 1, Iter 29, disc loss: 1.1257120189351988, policy loss: 0.7820498061669108
Experience 1, Iter 30, disc loss: 1.1085718672152378, policy loss: 0.7930629013650712
Experience 1, Iter 31, disc loss: 1.0971799625770062, policy loss: 0.7970206195348257
Experience 1, Iter 32, disc loss: 1.0969984467853968, policy loss: 0.7874180402912239
Experience 1, Iter 33, disc loss: 1.0802437930442528, policy loss: 0.7981851663688805
Experience 1, Iter 34, disc loss: 1.064297489473633, policy loss: 0.8078920169022754
Experience 1, Iter 35, disc loss: 1.0550742780002331, policy loss: 0.8096397853987058
Experience 1, Iter 36, disc loss: 1.0379438375806478, policy loss: 0.8213483168227127
Experience 1, Iter 37, disc loss: 1.02323412670395, policy loss: 0.8304634813253069
Experience 1, Iter 38, disc loss: 1.014028493562034, policy loss: 0.831904613395452
Experience 1, Iter 39, disc loss: 1.0063977356833642, policy loss: 0.8312721622088414
Experience 1, Iter 40, disc loss: 0.9985988458702704, policy loss: 0.8310197108087111
Experience 1, Iter 41, disc loss: 0.9843316110084463, policy loss: 0.8397616879987377
Experience 1, Iter 42, disc loss: 0.9695516495990671, policy loss: 0.8496428785987
Experience 1, Iter 43, disc loss: 0.9528420381777465, policy loss: 0.8620809021587001
Experience 1, Iter 44, disc loss: 0.949573791910681, policy loss: 0.8556248830470821
Experience 1, Iter 45, disc loss: 0.9291848222961515, policy loss: 0.8737483535419164
Experience 1, Iter 46, disc loss: 0.9301989317734953, policy loss: 0.8622581089396941
Experience 1, Iter 47, disc loss: 0.9042297106002044, policy loss: 0.8883875769248066
Experience 1, Iter 48, disc loss: 0.9003621257535543, policy loss: 0.8857895901917303
Experience 1, Iter 49, disc loss: 0.8807429045249784, policy loss: 0.9027219672803406
Experience 1, Iter 50, disc loss: 0.8818235153913023, policy loss: 0.8898730574250426
Experience 1, Iter 51, disc loss: 0.8531264255172377, policy loss: 0.9237977622151514
Experience 1, Iter 52, disc loss: 0.8455635175107427, policy loss: 0.9236154936936204
Experience 1, Iter 53, disc loss: 0.8431088289849297, policy loss: 0.9151148884068278
Experience 1, Iter 54, disc loss: 0.8205523325652561, policy loss: 0.9385474656339103
Experience 1, Iter 55, disc loss: 0.8130335611016453, policy loss: 0.9397481258283211
Experience 1, Iter 56, disc loss: 0.7941734832624106, policy loss: 0.9589511868831211
Experience 1, Iter 57, disc loss: 0.7896730093671555, policy loss: 0.9559554449353626
Experience 1, Iter 58, disc loss: 0.7680679742756582, policy loss: 0.9788144857262135
Experience 1, Iter 59, disc loss: 0.7626245586829231, policy loss: 0.9776556468723359
Experience 1, Iter 60, disc loss: 0.7486437580133617, policy loss: 0.9895019229891777
Experience 1, Iter 61, disc loss: 0.7425208840119335, policy loss: 0.9887298760928733
Experience 1, Iter 62, disc loss: 0.7358462792594391, policy loss: 0.9890101329533638
Experience 1, Iter 63, disc loss: 0.7157901264779345, policy loss: 1.0106351106635225
Experience 1, Iter 64, disc loss: 0.7055597655712467, policy loss: 1.0195326780814957
Experience 1, Iter 65, disc loss: 0.6902245148400248, policy loss: 1.0384540516996843
Experience 1, Iter 66, disc loss: 0.6699755529008753, policy loss: 1.0645341779953452
Experience 1, Iter 67, disc loss: 0.6773405549786287, policy loss: 1.0377688980700017
Experience 1, Iter 68, disc loss: 0.6488135922691703, policy loss: 1.0831015201039997
Experience 1, Iter 69, disc loss: 0.642558238483782, policy loss: 1.08410727427067
Experience 1, Iter 70, disc loss: 0.6311749416363612, policy loss: 1.0971867029725209
Experience 1, Iter 71, disc loss: 0.624019588884318, policy loss: 1.1018784212679336
Experience 1, Iter 72, disc loss: 0.6185053649425756, policy loss: 1.102438498306237
Experience 1, Iter 73, disc loss: 0.6201942514864051, policy loss: 1.0895409425969262
Experience 1, Iter 74, disc loss: 0.5831609836411449, policy loss: 1.152416220702801
Experience 1, Iter 75, disc loss: 0.5893064762685075, policy loss: 1.1314145301239935
Experience 1, Iter 76, disc loss: 0.5800729947153968, policy loss: 1.1390291436885167
Experience 1, Iter 77, disc loss: 0.5621390229822654, policy loss: 1.1706759576634878
Experience 1, Iter 78, disc loss: 0.5513203460796424, policy loss: 1.1805165855916069
Experience 1, Iter 79, disc loss: 0.540538752679059, policy loss: 1.2009684524435813
Experience 1, Iter 80, disc loss: 0.5302410335427332, policy loss: 1.2127688558328362
Experience 1, Iter 81, disc loss: 0.5241280223340254, policy loss: 1.2153312604257436
Experience 1, Iter 82, disc loss: 0.5059057016242585, policy loss: 1.2505012749009417
Experience 1, Iter 83, disc loss: 0.4908599499432819, policy loss: 1.2757910727747546
Experience 1, Iter 84, disc loss: 0.4843570763543094, policy loss: 1.283881026030354
Experience 1, Iter 85, disc loss: 0.47839062696851065, policy loss: 1.2969460388855427
Experience 1, Iter 86, disc loss: 0.4825713294394034, policy loss: 1.2674252272963267
Experience 1, Iter 87, disc loss: 0.4528386288464646, policy loss: 1.346516719618304
Experience 1, Iter 88, disc loss: 0.45100750200932677, policy loss: 1.335583348778764
Experience 1, Iter 89, disc loss: 0.43496845650189375, policy loss: 1.3715852418371175
Experience 1, Iter 90, disc loss: 0.41650468365023047, policy loss: 1.4203575278895877
Experience 1, Iter 91, disc loss: 0.4341006346945848, policy loss: 1.3634365704517712
Experience 1, Iter 92, disc loss: 0.4174454558913706, policy loss: 1.4011733188898183
Experience 1, Iter 93, disc loss: 0.4002863261151104, policy loss: 1.446001526217702
Experience 1, Iter 94, disc loss: 0.3828285843764607, policy loss: 1.4832645875742738
Experience 1, Iter 95, disc loss: 0.38317877462869004, policy loss: 1.4780469671533238
Experience 1, Iter 96, disc loss: 0.3833640789502456, policy loss: 1.464362982333775
Experience 1, Iter 97, disc loss: 0.37865858416071546, policy loss: 1.4813082000503521
Experience 1, Iter 98, disc loss: 0.35791121826600436, policy loss: 1.5332153783961326
Experience 1, Iter 99, disc loss: 0.35694935295161506, policy loss: 1.5288142559329425
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0007],
        [0.0279],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0029, 0.1114, 0.0021], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0029, 0.1114, 0.0021])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.869
Iter 2/2000 - Loss: -0.545
Iter 3/2000 - Loss: -3.928
Iter 4/2000 - Loss: -4.383
Iter 5/2000 - Loss: -3.122
Iter 6/2000 - Loss: -3.162
Iter 7/2000 - Loss: -4.068
Iter 8/2000 - Loss: -4.690
Iter 9/2000 - Loss: -4.638
Iter 10/2000 - Loss: -4.243
Iter 11/2000 - Loss: -4.001
Iter 12/2000 - Loss: -4.094
Iter 13/2000 - Loss: -4.394
Iter 14/2000 - Loss: -4.661
Iter 15/2000 - Loss: -4.735
Iter 16/2000 - Loss: -4.625
Iter 17/2000 - Loss: -4.482
Iter 18/2000 - Loss: -4.462
Iter 19/2000 - Loss: -4.578
Iter 20/2000 - Loss: -4.700
Iter 1981/2000 - Loss: -5.180
Iter 1982/2000 - Loss: -5.179
Iter 1983/2000 - Loss: -5.178
Iter 1984/2000 - Loss: -5.178
Iter 1985/2000 - Loss: -5.179
Iter 1986/2000 - Loss: -5.180
Iter 1987/2000 - Loss: -5.180
Iter 1988/2000 - Loss: -5.180
Iter 1989/2000 - Loss: -5.179
Iter 1990/2000 - Loss: -5.179
Iter 1991/2000 - Loss: -5.179
Iter 1992/2000 - Loss: -5.180
Iter 1993/2000 - Loss: -5.180
Iter 1994/2000 - Loss: -5.180
Iter 1995/2000 - Loss: -5.180
Iter 1996/2000 - Loss: -5.180
Iter 1997/2000 - Loss: -5.180
Iter 1998/2000 - Loss: -5.180
Iter 1999/2000 - Loss: -5.180
Iter 2000/2000 - Loss: -5.180
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0005],
        [0.0208],
        [0.0004]])
Lengthscale: tensor([[[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]],

        [[2.0518e-03, 5.7989e-03, 2.5104e-02, 6.0538e-04, 2.5424e-06,
          7.6587e-03]]])
Signal Variance: tensor([0.0005, 0.0022, 0.0851, 0.0016])
Estimated target variance: tensor([0.0007, 0.0029, 0.1114, 0.0021])
N: 20
Signal to noise ratio: tensor([1.9986, 2.0010, 2.0234, 2.0004])
Bound on condition number: tensor([80.8846, 81.0804, 82.8824, 81.0302])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.34011237820043927, policy loss: 1.5795183362350338
Experience 2, Iter 1, disc loss: 0.3543297740667584, policy loss: 1.522029808310866
Experience 2, Iter 2, disc loss: 0.32069564726728106, policy loss: 1.6312782416375808
Experience 2, Iter 3, disc loss: 0.3193078901846547, policy loss: 1.6346900643659747
Experience 2, Iter 4, disc loss: 0.31160364736967056, policy loss: 1.6552598369286926
Experience 2, Iter 5, disc loss: 0.29497836620579715, policy loss: 1.7169084197581934
Experience 2, Iter 6, disc loss: 0.3031341807924359, policy loss: 1.6701699155766538
Experience 2, Iter 7, disc loss: 0.28622080828733754, policy loss: 1.73460827181895
Experience 2, Iter 8, disc loss: 0.29320443025005843, policy loss: 1.702388144613589
Experience 2, Iter 9, disc loss: 0.27268989363875096, policy loss: 1.7770261296220125
Experience 2, Iter 10, disc loss: 0.2722990270983653, policy loss: 1.7823832065929128
Experience 2, Iter 11, disc loss: 0.25610152653455426, policy loss: 1.8524764759279362
Experience 2, Iter 12, disc loss: 0.2635751563215304, policy loss: 1.8083305957440814
Experience 2, Iter 13, disc loss: 0.2644352261595513, policy loss: 1.8119051140681706
Experience 2, Iter 14, disc loss: 0.2602115269154167, policy loss: 1.8148777781869279
Experience 2, Iter 15, disc loss: 0.2319653171016489, policy loss: 1.9479975685170507
Experience 2, Iter 16, disc loss: 0.23682452013763822, policy loss: 1.915389992820927
Experience 2, Iter 17, disc loss: 0.22726468310223677, policy loss: 1.9640763175263225
Experience 2, Iter 18, disc loss: 0.2216510948979924, policy loss: 1.9873293072187794
Experience 2, Iter 19, disc loss: 0.2088001703542639, policy loss: 2.0688724174819693
Experience 2, Iter 20, disc loss: 0.2201741698142482, policy loss: 1.979853122212793
Experience 2, Iter 21, disc loss: 0.1964155929341731, policy loss: 2.1326200842083445
Experience 2, Iter 22, disc loss: 0.20932739466910405, policy loss: 2.0315173396598447
Experience 2, Iter 23, disc loss: 0.19918152933082273, policy loss: 2.0839220055574064
Experience 2, Iter 24, disc loss: 0.1882117935919279, policy loss: 2.1656822675937617
Experience 2, Iter 25, disc loss: 0.18163104055532264, policy loss: 2.233468667957808
Experience 2, Iter 26, disc loss: 0.1851699698806522, policy loss: 2.178788206140951
Experience 2, Iter 27, disc loss: 0.1857431843973005, policy loss: 2.154441369543397
Experience 2, Iter 28, disc loss: 0.17827733906064952, policy loss: 2.2173040587549364
Experience 2, Iter 29, disc loss: 0.17472225279318235, policy loss: 2.233726004783968
Experience 2, Iter 30, disc loss: 0.16223753183442893, policy loss: 2.311399929958781
Experience 2, Iter 31, disc loss: 0.16419189635744844, policy loss: 2.270374708744023
Experience 2, Iter 32, disc loss: 0.1514784773115042, policy loss: 2.4009453773865532
Experience 2, Iter 33, disc loss: 0.1588656491777144, policy loss: 2.3323478940212428
Experience 2, Iter 34, disc loss: 0.15501595471660048, policy loss: 2.3432981906070633
Experience 2, Iter 35, disc loss: 0.13998763093707653, policy loss: 2.4838524489080394
Experience 2, Iter 36, disc loss: 0.13922704790605614, policy loss: 2.4653154799247807
Experience 2, Iter 37, disc loss: 0.1371193859961667, policy loss: 2.472473200310613
Experience 2, Iter 38, disc loss: 0.15061426492438232, policy loss: 2.3972286648682113
Experience 2, Iter 39, disc loss: 0.13319206689320287, policy loss: 2.501369557258765
Experience 2, Iter 40, disc loss: 0.12231160056172749, policy loss: 2.62556371938787
Experience 2, Iter 41, disc loss: 0.12853167504802143, policy loss: 2.5577099816395004
Experience 2, Iter 42, disc loss: 0.12587977028193723, policy loss: 2.5643746206505
Experience 2, Iter 43, disc loss: 0.13248738721005723, policy loss: 2.5095462541457554
Experience 2, Iter 44, disc loss: 0.12214460873879399, policy loss: 2.620990744225284
Experience 2, Iter 45, disc loss: 0.11935319157265212, policy loss: 2.661447625014473
Experience 2, Iter 46, disc loss: 0.1185517887582197, policy loss: 2.6662722993626655
Experience 2, Iter 47, disc loss: 0.10946625848467635, policy loss: 2.7332274038092628
Experience 2, Iter 48, disc loss: 0.11119291334768505, policy loss: 2.715400280071006
Experience 2, Iter 49, disc loss: 0.10656632675281208, policy loss: 2.756277853319169
Experience 2, Iter 50, disc loss: 0.1021605954030443, policy loss: 2.829826348699827
Experience 2, Iter 51, disc loss: 0.10564858708820599, policy loss: 2.794613040245166
Experience 2, Iter 52, disc loss: 0.10105399448550681, policy loss: 2.84696076133572
Experience 2, Iter 53, disc loss: 0.09635204634238029, policy loss: 2.918933893356553
Experience 2, Iter 54, disc loss: 0.09845086636057085, policy loss: 2.80971113238472
Experience 2, Iter 55, disc loss: 0.09453521456740258, policy loss: 2.875410069012325
Experience 2, Iter 56, disc loss: 0.09190302865776422, policy loss: 2.976926485481384
Experience 2, Iter 57, disc loss: 0.092942267430355, policy loss: 2.894004738482152
Experience 2, Iter 58, disc loss: 0.08703025636247011, policy loss: 3.025029671438519
Experience 2, Iter 59, disc loss: 0.08657758387963191, policy loss: 3.011693024477969
Experience 2, Iter 60, disc loss: 0.08929981569130066, policy loss: 2.9948188159255515
Experience 2, Iter 61, disc loss: 0.07979768407800585, policy loss: 3.0743034820529926
Experience 2, Iter 62, disc loss: 0.0856528127891896, policy loss: 3.0085305409540486
Experience 2, Iter 63, disc loss: 0.07715133260539138, policy loss: 3.192376949439577
Experience 2, Iter 64, disc loss: 0.0727112101572666, policy loss: 3.185189424958337
Experience 2, Iter 65, disc loss: 0.07679626827894769, policy loss: 3.1167021592149116
Experience 2, Iter 66, disc loss: 0.07759359404484753, policy loss: 3.129379032701153
Experience 2, Iter 67, disc loss: 0.07469543841434631, policy loss: 3.166264295271956
Experience 2, Iter 68, disc loss: 0.07279812100740274, policy loss: 3.2301168440124925
Experience 2, Iter 69, disc loss: 0.07045532317477991, policy loss: 3.1900555523770464
Experience 2, Iter 70, disc loss: 0.07254444825037143, policy loss: 3.21866322038763
Experience 2, Iter 71, disc loss: 0.06833251091005277, policy loss: 3.2450453137232502
Experience 2, Iter 72, disc loss: 0.06761351928835796, policy loss: 3.250270467814527
Experience 2, Iter 73, disc loss: 0.06923384571556468, policy loss: 3.2226252424549062
Experience 2, Iter 74, disc loss: 0.06710729050231796, policy loss: 3.322527475001371
Experience 2, Iter 75, disc loss: 0.06638128703064564, policy loss: 3.2730902643604467
Experience 2, Iter 76, disc loss: 0.0619108205881493, policy loss: 3.3818046986934496
Experience 2, Iter 77, disc loss: 0.0608009971680415, policy loss: 3.428125729746222
Experience 2, Iter 78, disc loss: 0.0665975115939057, policy loss: 3.2833286594374407
Experience 2, Iter 79, disc loss: 0.06361126970361394, policy loss: 3.335018326313313
Experience 2, Iter 80, disc loss: 0.05477679434599348, policy loss: 3.5920875251002577
Experience 2, Iter 81, disc loss: 0.06413806611820785, policy loss: 3.2775251868941373
Experience 2, Iter 82, disc loss: 0.056999477058020886, policy loss: 3.4957939888628653
Experience 2, Iter 83, disc loss: 0.05474409122247833, policy loss: 3.5405435626361
Experience 2, Iter 84, disc loss: 0.06099849461055421, policy loss: 3.4001706773204763
Experience 2, Iter 85, disc loss: 0.05452975156091332, policy loss: 3.5591874854773073
Experience 2, Iter 86, disc loss: 0.05422429302910449, policy loss: 3.5122827776693786
Experience 2, Iter 87, disc loss: 0.053264121624722036, policy loss: 3.5073460372736878
Experience 2, Iter 88, disc loss: 0.05663151849158829, policy loss: 3.489534508067668
Experience 2, Iter 89, disc loss: 0.04886925654336624, policy loss: 3.670503368124334
Experience 2, Iter 90, disc loss: 0.05384485343575214, policy loss: 3.4713968436309273
Experience 2, Iter 91, disc loss: 0.047506734312207144, policy loss: 3.7079135221387647
Experience 2, Iter 92, disc loss: 0.05268932398706559, policy loss: 3.5536521823612315
Experience 2, Iter 93, disc loss: 0.04712167108707732, policy loss: 3.676945215092473
Experience 2, Iter 94, disc loss: 0.04931781504501592, policy loss: 3.5906541808232033
Experience 2, Iter 95, disc loss: 0.04375043124710504, policy loss: 3.792385549472593
Experience 2, Iter 96, disc loss: 0.0417447442744405, policy loss: 3.9218679662735703
Experience 2, Iter 97, disc loss: 0.04693912939409253, policy loss: 3.724492113941002
Experience 2, Iter 98, disc loss: 0.04041855331707707, policy loss: 3.8823868450074857
Experience 2, Iter 99, disc loss: 0.04489142706332033, policy loss: 3.7471122095180354
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0012],
        [0.0225],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0024, 0.0048, 0.0902, 0.0019], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0024, 0.0048, 0.0902, 0.0019])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.985
Iter 2/2000 - Loss: -1.782
Iter 3/2000 - Loss: -3.433
Iter 4/2000 - Loss: -3.329
Iter 5/2000 - Loss: -2.845
Iter 6/2000 - Loss: -3.094
Iter 7/2000 - Loss: -3.629
Iter 8/2000 - Loss: -3.955
Iter 9/2000 - Loss: -3.921
Iter 10/2000 - Loss: -3.695
Iter 11/2000 - Loss: -3.537
Iter 12/2000 - Loss: -3.597
Iter 13/2000 - Loss: -3.828
Iter 14/2000 - Loss: -4.049
Iter 15/2000 - Loss: -4.116
Iter 16/2000 - Loss: -4.040
Iter 17/2000 - Loss: -3.960
Iter 18/2000 - Loss: -3.977
Iter 19/2000 - Loss: -4.047
Iter 20/2000 - Loss: -4.076
Iter 1981/2000 - Loss: -4.391
Iter 1982/2000 - Loss: -4.403
Iter 1983/2000 - Loss: -4.421
Iter 1984/2000 - Loss: -4.427
Iter 1985/2000 - Loss: -4.421
Iter 1986/2000 - Loss: -4.409
Iter 1987/2000 - Loss: -4.399
Iter 1988/2000 - Loss: -4.400
Iter 1989/2000 - Loss: -4.414
Iter 1990/2000 - Loss: -4.426
Iter 1991/2000 - Loss: -4.423
Iter 1992/2000 - Loss: -4.413
Iter 1993/2000 - Loss: -4.416
Iter 1994/2000 - Loss: -4.422
Iter 1995/2000 - Loss: -4.416
Iter 1996/2000 - Loss: -4.403
Iter 1997/2000 - Loss: -4.393
Iter 1998/2000 - Loss: -4.390
Iter 1999/2000 - Loss: -4.399
Iter 2000/2000 - Loss: -4.415
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0010],
        [0.0172],
        [0.0003]])
Lengthscale: tensor([[[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]],

        [[6.5707e-03, 2.3170e-02, 2.0316e-02, 7.2974e-04, 6.8668e-06,
          3.4886e-02]]])
Signal Variance: tensor([0.0018, 0.0041, 0.0700, 0.0014])
Estimated target variance: tensor([0.0024, 0.0048, 0.0902, 0.0019])
N: 30
Signal to noise ratio: tensor([2.0009, 2.0017, 2.0191, 2.0002])
Bound on condition number: tensor([121.1061, 121.2004, 123.3045, 121.0254])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.03145267424667336, policy loss: 4.296099079479195
Experience 3, Iter 1, disc loss: 0.0344150960171614, policy loss: 4.2860042187200165
Experience 3, Iter 2, disc loss: 0.031408932077418024, policy loss: 4.327963823159758
Experience 3, Iter 3, disc loss: 0.029491984868538317, policy loss: 4.46123956644081
Experience 3, Iter 4, disc loss: 0.030603900444193352, policy loss: 4.31537789979459
Experience 3, Iter 5, disc loss: 0.027553541224661186, policy loss: 4.5231016506037065
Experience 3, Iter 6, disc loss: 0.028109703285874074, policy loss: 4.460256002804131
Experience 3, Iter 7, disc loss: 0.028039824158486187, policy loss: 4.471880050812823
Experience 3, Iter 8, disc loss: 0.027478588955173067, policy loss: 4.5627044611073995
Experience 3, Iter 9, disc loss: 0.02733119848419509, policy loss: 4.445771785005739
Experience 3, Iter 10, disc loss: 0.027910561048763223, policy loss: 4.429156875597973
Experience 3, Iter 11, disc loss: 0.026570006347423902, policy loss: 4.491810548041521
Experience 3, Iter 12, disc loss: 0.027033961464141304, policy loss: 4.532763425202728
Experience 3, Iter 13, disc loss: 0.023771660972757035, policy loss: 4.646776926894864
Experience 3, Iter 14, disc loss: 0.024808983257936512, policy loss: 4.671950430172913
Experience 3, Iter 15, disc loss: 0.023300411558483017, policy loss: 4.712715559841083
Experience 3, Iter 16, disc loss: 0.026943106286695405, policy loss: 4.4855155508044176
Experience 3, Iter 17, disc loss: 0.022624541804762453, policy loss: 4.7368887062993785
Experience 3, Iter 18, disc loss: 0.024018107269589153, policy loss: 4.713732299926944
Experience 3, Iter 19, disc loss: 0.026189880984454156, policy loss: 4.473668627917401
Experience 3, Iter 20, disc loss: 0.021261924224219644, policy loss: 4.869607123295973
Experience 3, Iter 21, disc loss: 0.023195296003489532, policy loss: 4.698371735523635
Experience 3, Iter 22, disc loss: 0.023026502044872516, policy loss: 4.759259139599812
Experience 3, Iter 23, disc loss: 0.02293576667669313, policy loss: 4.663104563768533
Experience 3, Iter 24, disc loss: 0.026658548394027155, policy loss: 4.479028820782293
Experience 3, Iter 25, disc loss: 0.0226660205249931, policy loss: 4.79367926959655
Experience 3, Iter 26, disc loss: 0.02424929659887639, policy loss: 4.6544126684588765
Experience 3, Iter 27, disc loss: 0.020138040334431825, policy loss: 4.849369539078925
Experience 3, Iter 28, disc loss: 0.02261117160277823, policy loss: 4.782961900397503
Experience 3, Iter 29, disc loss: 0.022962410797904537, policy loss: 4.657314188665696
Experience 3, Iter 30, disc loss: 0.022419122824371522, policy loss: 4.743316045445912
Experience 3, Iter 31, disc loss: 0.020816168215211886, policy loss: 4.861274045158771
Experience 3, Iter 32, disc loss: 0.021645538368280346, policy loss: 4.765726947591953
Experience 3, Iter 33, disc loss: 0.020465798064733077, policy loss: 4.81546249297957
Experience 3, Iter 34, disc loss: 0.021649323117236496, policy loss: 4.8106526747208775
Experience 3, Iter 35, disc loss: 0.01967837925308546, policy loss: 4.861479517897183
Experience 3, Iter 36, disc loss: 0.021048380361131845, policy loss: 4.734391377377519
Experience 3, Iter 37, disc loss: 0.018848395001825877, policy loss: 4.8478492842791105
Experience 3, Iter 38, disc loss: 0.018821481778850045, policy loss: 5.016955312805312
Experience 3, Iter 39, disc loss: 0.019052432701641295, policy loss: 4.894229227397186
Experience 3, Iter 40, disc loss: 0.01855317444325677, policy loss: 4.911688097418621
Experience 3, Iter 41, disc loss: 0.0168859330042375, policy loss: 5.08441442393886
Experience 3, Iter 42, disc loss: 0.01814517177881711, policy loss: 4.992060120754429
Experience 3, Iter 43, disc loss: 0.01792028957311152, policy loss: 4.974874043627329
Experience 3, Iter 44, disc loss: 0.01721651685551594, policy loss: 5.0225393570663615
Experience 3, Iter 45, disc loss: 0.017621223852493387, policy loss: 4.9482038484594515
Experience 3, Iter 46, disc loss: 0.016588322746267103, policy loss: 5.071280719802479
Experience 3, Iter 47, disc loss: 0.0184084717769309, policy loss: 4.909574098353381
Experience 3, Iter 48, disc loss: 0.017253393112405438, policy loss: 4.990602533903247
Experience 3, Iter 49, disc loss: 0.016907821901011835, policy loss: 5.132157838640424
Experience 3, Iter 50, disc loss: 0.01656019346607645, policy loss: 5.085497064980959
Experience 3, Iter 51, disc loss: 0.0178896696655796, policy loss: 4.974468453609521
Experience 3, Iter 52, disc loss: 0.01665820217271141, policy loss: 5.069745347988514
Experience 3, Iter 53, disc loss: 0.014985952650601316, policy loss: 5.178952714374427
Experience 3, Iter 54, disc loss: 0.01594767215316143, policy loss: 5.160750362197067
Experience 3, Iter 55, disc loss: 0.017584497977683283, policy loss: 5.0217060532983515
Experience 3, Iter 56, disc loss: 0.01813561103162014, policy loss: 5.036270138849385
Experience 3, Iter 57, disc loss: 0.016835667923077204, policy loss: 5.006588345722469
Experience 3, Iter 58, disc loss: 0.015338308510464381, policy loss: 5.1452319844846945
Experience 3, Iter 59, disc loss: 0.01810717139626854, policy loss: 4.93745820908879
Experience 3, Iter 60, disc loss: 0.014264379536120413, policy loss: 5.275700440687574
Experience 3, Iter 61, disc loss: 0.014266252436821, policy loss: 5.322329549883355
Experience 3, Iter 62, disc loss: 0.016588249115797482, policy loss: 5.034029198017019
Experience 3, Iter 63, disc loss: 0.015914635243066702, policy loss: 5.054216367312723
Experience 3, Iter 64, disc loss: 0.015806492633933758, policy loss: 5.0859579014248855
Experience 3, Iter 65, disc loss: 0.015238907963897638, policy loss: 5.058123470909211
Experience 3, Iter 66, disc loss: 0.01567942675965545, policy loss: 5.086915218915795
Experience 3, Iter 67, disc loss: 0.013585147398344747, policy loss: 5.337458135514336
Experience 3, Iter 68, disc loss: 0.014728575333149758, policy loss: 5.187846192731779
Experience 3, Iter 69, disc loss: 0.014518454502613295, policy loss: 5.17012353569706
Experience 3, Iter 70, disc loss: 0.015566335862837458, policy loss: 5.026184409960159
Experience 3, Iter 71, disc loss: 0.014027108730560844, policy loss: 5.344990500588356
Experience 3, Iter 72, disc loss: 0.01550117250805872, policy loss: 5.074734538658079
Experience 3, Iter 73, disc loss: 0.013396705946574756, policy loss: 5.330524996052543
Experience 3, Iter 74, disc loss: 0.01435954223535663, policy loss: 5.198637608307352
Experience 3, Iter 75, disc loss: 0.013895962300634232, policy loss: 5.297848599343747
Experience 3, Iter 76, disc loss: 0.013756832187749837, policy loss: 5.276433730165562
Experience 3, Iter 77, disc loss: 0.0134361049488637, policy loss: 5.322232761669441
Experience 3, Iter 78, disc loss: 0.013702309382553955, policy loss: 5.386489739572282
Experience 3, Iter 79, disc loss: 0.011830779656636776, policy loss: 5.4884011542863025
Experience 3, Iter 80, disc loss: 0.01230782000420702, policy loss: 5.31252440771551
Experience 3, Iter 81, disc loss: 0.013079424759832158, policy loss: 5.333658069647581
Experience 3, Iter 82, disc loss: 0.01285137987265312, policy loss: 5.334213229478431
Experience 3, Iter 83, disc loss: 0.012978227962227529, policy loss: 5.336192529603287
Experience 3, Iter 84, disc loss: 0.01375445353677991, policy loss: 5.202730761612063
Experience 3, Iter 85, disc loss: 0.012233613574590364, policy loss: 5.425172968395483
Experience 3, Iter 86, disc loss: 0.011482371624500012, policy loss: 5.495356921384605
Experience 3, Iter 87, disc loss: 0.012713930325814178, policy loss: 5.319631841197706
Experience 3, Iter 88, disc loss: 0.01089804495954878, policy loss: 5.600570604934999
Experience 3, Iter 89, disc loss: 0.012788769322994338, policy loss: 5.281151561473889
Experience 3, Iter 90, disc loss: 0.012456124629643996, policy loss: 5.366123255744183
Experience 3, Iter 91, disc loss: 0.011428785853375685, policy loss: 5.515259133255588
Experience 3, Iter 92, disc loss: 0.013324778585354028, policy loss: 5.272449931914272
Experience 3, Iter 93, disc loss: 0.011448950520083817, policy loss: 5.400059291170313
Experience 3, Iter 94, disc loss: 0.011603825383676079, policy loss: 5.475965442450358
Experience 3, Iter 95, disc loss: 0.011264074117336471, policy loss: 5.570438068399083
Experience 3, Iter 96, disc loss: 0.010596982768557068, policy loss: 5.509576986626363
Experience 3, Iter 97, disc loss: 0.010561638150646233, policy loss: 5.670790000637892
Experience 3, Iter 98, disc loss: 0.01114532483429902, policy loss: 5.5175965931499205
Experience 3, Iter 99, disc loss: 0.011937844451565168, policy loss: 5.379213789206171
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0036],
        [0.0194],
        [0.0004]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0050, 0.0145, 0.0774, 0.0017], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0050, 0.0145, 0.0774, 0.0017])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.445
Iter 2/2000 - Loss: -0.492
Iter 3/2000 - Loss: -3.353
Iter 4/2000 - Loss: -3.020
Iter 5/2000 - Loss: -1.989
Iter 6/2000 - Loss: -2.308
Iter 7/2000 - Loss: -3.098
Iter 8/2000 - Loss: -3.462
Iter 9/2000 - Loss: -3.315
Iter 10/2000 - Loss: -3.035
Iter 11/2000 - Loss: -2.918
Iter 12/2000 - Loss: -2.997
Iter 13/2000 - Loss: -3.158
Iter 14/2000 - Loss: -3.296
Iter 15/2000 - Loss: -3.375
Iter 16/2000 - Loss: -3.391
Iter 17/2000 - Loss: -3.347
Iter 18/2000 - Loss: -3.269
Iter 19/2000 - Loss: -3.214
Iter 20/2000 - Loss: -3.237
Iter 1981/2000 - Loss: -3.596
Iter 1982/2000 - Loss: -3.602
Iter 1983/2000 - Loss: -3.608
Iter 1984/2000 - Loss: -3.606
Iter 1985/2000 - Loss: -3.600
Iter 1986/2000 - Loss: -3.597
Iter 1987/2000 - Loss: -3.600
Iter 1988/2000 - Loss: -3.605
Iter 1989/2000 - Loss: -3.608
Iter 1990/2000 - Loss: -3.607
Iter 1991/2000 - Loss: -3.605
Iter 1992/2000 - Loss: -3.604
Iter 1993/2000 - Loss: -3.607
Iter 1994/2000 - Loss: -3.608
Iter 1995/2000 - Loss: -3.607
Iter 1996/2000 - Loss: -3.606
Iter 1997/2000 - Loss: -3.606
Iter 1998/2000 - Loss: -3.608
Iter 1999/2000 - Loss: -3.608
Iter 2000/2000 - Loss: -3.607
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0028],
        [0.0149],
        [0.0003]])
Lengthscale: tensor([[[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]],

        [[6.8357e-03, 4.4652e-02, 1.7745e-02, 1.7116e-03, 6.2266e-06,
          1.5159e-01]]])
Signal Variance: tensor([0.0039, 0.0113, 0.0606, 0.0013])
Estimated target variance: tensor([0.0050, 0.0145, 0.0774, 0.0017])
N: 40
Signal to noise ratio: tensor([2.0023, 2.0103, 2.0170, 2.0002])
Bound on condition number: tensor([161.3743, 162.6590, 163.7264, 161.0302])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.02158172498067147, policy loss: 4.528894844646978
Experience 4, Iter 1, disc loss: 0.02260470235662076, policy loss: 4.4259846414745185
Experience 4, Iter 2, disc loss: 0.02450327300213541, policy loss: 4.303138692616228
Experience 4, Iter 3, disc loss: 0.021829831741772665, policy loss: 4.552375630404583
Experience 4, Iter 4, disc loss: 0.023855146394895464, policy loss: 4.369137913057049
Experience 4, Iter 5, disc loss: 0.01801498698390322, policy loss: 4.737724314144818
Experience 4, Iter 6, disc loss: 0.02404208837577876, policy loss: 4.4799574999858525
Experience 4, Iter 7, disc loss: 0.02352705481941006, policy loss: 4.308343010720034
Experience 4, Iter 8, disc loss: 0.02448553409589493, policy loss: 4.364814658250292
Experience 4, Iter 9, disc loss: 0.019709796857255317, policy loss: 4.711938097395825
Experience 4, Iter 10, disc loss: 0.021577344473404293, policy loss: 4.461208535919417
Experience 4, Iter 11, disc loss: 0.021021312122099654, policy loss: 4.547688029306109
Experience 4, Iter 12, disc loss: 0.02137025599386032, policy loss: 4.479956009168821
Experience 4, Iter 13, disc loss: 0.01918573599580342, policy loss: 4.612067252901301
Experience 4, Iter 14, disc loss: 0.022174987040447677, policy loss: 4.623510961819111
Experience 4, Iter 15, disc loss: 0.020362259472092907, policy loss: 4.582635405600369
Experience 4, Iter 16, disc loss: 0.01973961039537329, policy loss: 4.588075332895157
Experience 4, Iter 17, disc loss: 0.021241080933230704, policy loss: 4.567273652703984
Experience 4, Iter 18, disc loss: 0.017299773577774655, policy loss: 4.810114795888685
Experience 4, Iter 19, disc loss: 0.019491181279831647, policy loss: 4.645924618508946
Experience 4, Iter 20, disc loss: 0.02128825586515499, policy loss: 4.5372224408365724
Experience 4, Iter 21, disc loss: 0.01930353123253441, policy loss: 4.734079912912357
Experience 4, Iter 22, disc loss: 0.02143328068567603, policy loss: 4.5318781901468475
Experience 4, Iter 23, disc loss: 0.01895159313646319, policy loss: 4.593804368468399
Experience 4, Iter 24, disc loss: 0.016881144403673394, policy loss: 4.7027137270096055
Experience 4, Iter 25, disc loss: 0.018512171204701274, policy loss: 4.64751789027417
Experience 4, Iter 26, disc loss: 0.019517796888617815, policy loss: 4.6113744408816535
Experience 4, Iter 27, disc loss: 0.020810235876903074, policy loss: 4.712937016151125
Experience 4, Iter 28, disc loss: 0.016615690195032777, policy loss: 4.83481119335407
Experience 4, Iter 29, disc loss: 0.017067173268423157, policy loss: 4.849156954602359
Experience 4, Iter 30, disc loss: 0.017513539224439163, policy loss: 4.862657187458083
Experience 4, Iter 31, disc loss: 0.019274730371352675, policy loss: 4.6353081141196695
Experience 4, Iter 32, disc loss: 0.01649863544893427, policy loss: 4.744994387059257
Experience 4, Iter 33, disc loss: 0.016754381409453664, policy loss: 4.884846829527483
Experience 4, Iter 34, disc loss: 0.014695915946417617, policy loss: 5.020038506976333
Experience 4, Iter 35, disc loss: 0.016888722908489223, policy loss: 4.861440176668884
Experience 4, Iter 36, disc loss: 0.01563527657897935, policy loss: 5.053014876211081
Experience 4, Iter 37, disc loss: 0.018410814606717903, policy loss: 4.7645762882610185
Experience 4, Iter 38, disc loss: 0.016406498917221655, policy loss: 4.837127621110934
Experience 4, Iter 39, disc loss: 0.016283486923721446, policy loss: 4.795993188912213
Experience 4, Iter 40, disc loss: 0.015622359647927821, policy loss: 4.982932353479506
Experience 4, Iter 41, disc loss: 0.01755980576335031, policy loss: 4.74804130926319
Experience 4, Iter 42, disc loss: 0.014865254304360264, policy loss: 4.923341755525081
Experience 4, Iter 43, disc loss: 0.01671045626875834, policy loss: 4.855901532094453
Experience 4, Iter 44, disc loss: 0.01429438760710761, policy loss: 4.931267237420029
Experience 4, Iter 45, disc loss: 0.01692098797982129, policy loss: 4.820657201051734
Experience 4, Iter 46, disc loss: 0.01427621330234772, policy loss: 5.018795607065219
Experience 4, Iter 47, disc loss: 0.014630900119468306, policy loss: 5.000662814642728
Experience 4, Iter 48, disc loss: 0.01418521823463291, policy loss: 5.079788141331131
Experience 4, Iter 49, disc loss: 0.016780501865960244, policy loss: 4.84217286240986
Experience 4, Iter 50, disc loss: 0.01625138965309409, policy loss: 4.837870127424704
Experience 4, Iter 51, disc loss: 0.01401905817093589, policy loss: 5.060892954338479
Experience 4, Iter 52, disc loss: 0.014849874236614755, policy loss: 4.971958111058822
Experience 4, Iter 53, disc loss: 0.013193528469824408, policy loss: 5.013026955813062
Experience 4, Iter 54, disc loss: 0.012645245251574814, policy loss: 5.075150214064366
Experience 4, Iter 55, disc loss: 0.013442606450976316, policy loss: 5.024340046232659
Experience 4, Iter 56, disc loss: 0.012507289811448441, policy loss: 5.127897632371099
Experience 4, Iter 57, disc loss: 0.01347710246741896, policy loss: 5.0704193848235475
Experience 4, Iter 58, disc loss: 0.010811164608955292, policy loss: 5.366229490000696
Experience 4, Iter 59, disc loss: 0.01129342970837518, policy loss: 5.271819758858297
Experience 4, Iter 60, disc loss: 0.013126876659434003, policy loss: 5.208342204840757
Experience 4, Iter 61, disc loss: 0.011615129084096124, policy loss: 5.249078012056617
Experience 4, Iter 62, disc loss: 0.013282408732392512, policy loss: 5.052229195138191
Experience 4, Iter 63, disc loss: 0.011933889107895224, policy loss: 5.108010130847454
Experience 4, Iter 64, disc loss: 0.010839645405239523, policy loss: 5.300630159784497
Experience 4, Iter 65, disc loss: 0.012628146414085918, policy loss: 5.218629573577499
Experience 4, Iter 66, disc loss: 0.011958937756242037, policy loss: 5.302726336016531
Experience 4, Iter 67, disc loss: 0.010573647897945855, policy loss: 5.3632596019448915
Experience 4, Iter 68, disc loss: 0.012113828274384938, policy loss: 5.082461159461448
Experience 4, Iter 69, disc loss: 0.009861548163055043, policy loss: 5.386399490945485
Experience 4, Iter 70, disc loss: 0.01044149333914463, policy loss: 5.329729581016498
Experience 4, Iter 71, disc loss: 0.013253392626737328, policy loss: 5.136767823706025
Experience 4, Iter 72, disc loss: 0.011426182086607678, policy loss: 5.212176031493729
Experience 4, Iter 73, disc loss: 0.009882961481667696, policy loss: 5.48557145236138
Experience 4, Iter 74, disc loss: 0.013110495183639801, policy loss: 5.072814431170622
Experience 4, Iter 75, disc loss: 0.010455962553004716, policy loss: 5.23185831815907
Experience 4, Iter 76, disc loss: 0.010603561902879845, policy loss: 5.314033501314943
Experience 4, Iter 77, disc loss: 0.008809254784605937, policy loss: 5.407488205263041
Experience 4, Iter 78, disc loss: 0.012603389828127337, policy loss: 5.259011525821764
Experience 4, Iter 79, disc loss: 0.011688553310315826, policy loss: 5.154822840698504
Experience 4, Iter 80, disc loss: 0.010537296863302834, policy loss: 5.28776444633544
Experience 4, Iter 81, disc loss: 0.009710928713154949, policy loss: 5.332559379974232
Experience 4, Iter 82, disc loss: 0.00905812175500804, policy loss: 5.4979303581825905
Experience 4, Iter 83, disc loss: 0.008492523617368993, policy loss: 5.461602631537237
Experience 4, Iter 84, disc loss: 0.008241532709183472, policy loss: 5.516415340427973
Experience 4, Iter 85, disc loss: 0.009176506217453103, policy loss: 5.578361817552359
Experience 4, Iter 86, disc loss: 0.013580041924700928, policy loss: 5.128244564011204
Experience 4, Iter 87, disc loss: 0.012181313474424216, policy loss: 5.101442134543618
Experience 4, Iter 88, disc loss: 0.007604090501920777, policy loss: 5.486271124292511
Experience 4, Iter 89, disc loss: 0.010486309413532473, policy loss: 5.146116938401091
Experience 4, Iter 90, disc loss: 0.011378426121355124, policy loss: 5.239704147256208
Experience 4, Iter 91, disc loss: 0.01024595030768868, policy loss: 5.235146001850992
Experience 4, Iter 92, disc loss: 0.008968133492139096, policy loss: 5.519380176200733
Experience 4, Iter 93, disc loss: 0.011292543538492504, policy loss: 5.296475064699729
Experience 4, Iter 94, disc loss: 0.009099748460416598, policy loss: 5.5179842634588425
Experience 4, Iter 95, disc loss: 0.008167525842591223, policy loss: 5.418840916347136
Experience 4, Iter 96, disc loss: 0.009828864076944301, policy loss: 5.35019612434519
Experience 4, Iter 97, disc loss: 0.010375139681678448, policy loss: 5.237805513279271
Experience 4, Iter 98, disc loss: 0.009279101034082582, policy loss: 5.2774636332453655
Experience 4, Iter 99, disc loss: 0.006746259889464789, policy loss: 5.73802604973737
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0037],
        [0.0515],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.1172e-03, 4.1006e-02, 5.0141e-02, 2.3473e-03, 2.4397e-05,
          1.3666e-01]],

        [[7.1172e-03, 4.1006e-02, 5.0141e-02, 2.3473e-03, 2.4397e-05,
          1.3666e-01]],

        [[7.1172e-03, 4.1006e-02, 5.0141e-02, 2.3473e-03, 2.4397e-05,
          1.3666e-01]],

        [[7.1172e-03, 4.1006e-02, 5.0141e-02, 2.3473e-03, 2.4397e-05,
          1.3666e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0045, 0.0150, 0.2060, 0.0049], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0045, 0.0150, 0.2060, 0.0049])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.365
Iter 2/2000 - Loss: -1.385
Iter 3/2000 - Loss: -2.361
Iter 4/2000 - Loss: -2.391
Iter 5/2000 - Loss: -1.984
Iter 6/2000 - Loss: -2.051
Iter 7/2000 - Loss: -2.392
Iter 8/2000 - Loss: -2.540
Iter 9/2000 - Loss: -2.415
Iter 10/2000 - Loss: -2.285
Iter 11/2000 - Loss: -2.322
Iter 12/2000 - Loss: -2.424
Iter 13/2000 - Loss: -2.460
Iter 14/2000 - Loss: -2.470
Iter 15/2000 - Loss: -2.508
Iter 16/2000 - Loss: -2.510
Iter 17/2000 - Loss: -2.450
Iter 18/2000 - Loss: -2.436
Iter 19/2000 - Loss: -2.514
Iter 20/2000 - Loss: -2.581
Iter 1981/2000 - Loss: -2.616
Iter 1982/2000 - Loss: -2.616
Iter 1983/2000 - Loss: -2.616
Iter 1984/2000 - Loss: -2.616
Iter 1985/2000 - Loss: -2.616
Iter 1986/2000 - Loss: -2.616
Iter 1987/2000 - Loss: -2.616
Iter 1988/2000 - Loss: -2.616
Iter 1989/2000 - Loss: -2.616
Iter 1990/2000 - Loss: -2.616
Iter 1991/2000 - Loss: -2.616
Iter 1992/2000 - Loss: -2.616
Iter 1993/2000 - Loss: -2.616
Iter 1994/2000 - Loss: -2.616
Iter 1995/2000 - Loss: -2.616
Iter 1996/2000 - Loss: -2.616
Iter 1997/2000 - Loss: -2.616
Iter 1998/2000 - Loss: -2.616
Iter 1999/2000 - Loss: -2.616
Iter 2000/2000 - Loss: -2.616
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0029],
        [0.0391],
        [0.0010]])
Lengthscale: tensor([[[7.1183e-03, 4.1006e-02, 5.0146e-02, 2.3473e-03, 2.4399e-05,
          1.3666e-01]],

        [[7.1171e-03, 4.1006e-02, 5.0141e-02, 2.3473e-03, 2.4396e-05,
          1.3666e-01]],

        [[7.1202e-03, 4.1007e-02, 5.0154e-02, 2.3473e-03, 2.4404e-05,
          1.3666e-01]],

        [[7.1177e-03, 4.1006e-02, 5.0143e-02, 2.3473e-03, 2.4398e-05,
          1.3666e-01]]])
Signal Variance: tensor([0.0035, 0.0118, 0.1628, 0.0039])
Estimated target variance: tensor([0.0045, 0.0150, 0.2060, 0.0049])
N: 50
Signal to noise ratio: tensor([2.0026, 2.0112, 2.0393, 2.0016])
Bound on condition number: tensor([201.5154, 203.2423, 208.9294, 201.3200])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.007707357967198646, policy loss: 5.8858072234391345
Experience 5, Iter 1, disc loss: 0.005924562598630863, policy loss: 6.270377659646135
Experience 5, Iter 2, disc loss: 0.0064818734752965824, policy loss: 6.319608517852261
Experience 5, Iter 3, disc loss: 0.008499966021855945, policy loss: 6.026703433445572
Experience 5, Iter 4, disc loss: 0.008368209741018227, policy loss: 5.995440194214574
Experience 5, Iter 5, disc loss: 0.00727084050772695, policy loss: 5.863971394507082
Experience 5, Iter 6, disc loss: 0.006559373510516027, policy loss: 6.118765546322638
Experience 5, Iter 7, disc loss: 0.006980608298052523, policy loss: 6.138657386788001
Experience 5, Iter 8, disc loss: 0.0055210251959881065, policy loss: 6.215497903580848
Experience 5, Iter 9, disc loss: 0.006665548169607631, policy loss: 6.144071115567292
Experience 5, Iter 10, disc loss: 0.00815119953187039, policy loss: 6.13627044351864
Experience 5, Iter 11, disc loss: 0.006659668138936407, policy loss: 5.916568636028431
Experience 5, Iter 12, disc loss: 0.0073077304539219306, policy loss: 6.183203161389187
Experience 5, Iter 13, disc loss: 0.006771733001061055, policy loss: 6.114201826244854
Experience 5, Iter 14, disc loss: 0.005203414900616167, policy loss: 6.323436770298787
Experience 5, Iter 15, disc loss: 0.006764258743891211, policy loss: 6.181305359360367
Experience 5, Iter 16, disc loss: 0.004954553067430603, policy loss: 6.293175547528749
Experience 5, Iter 17, disc loss: 0.006040267324341017, policy loss: 6.21623048234904
Experience 5, Iter 18, disc loss: 0.009407861495435512, policy loss: 6.121180779333933
Experience 5, Iter 19, disc loss: 0.005789263008716628, policy loss: 6.081998268316815
Experience 5, Iter 20, disc loss: 0.006022104353044858, policy loss: 6.180538446965036
Experience 5, Iter 21, disc loss: 0.007206087121884466, policy loss: 6.091971929838507
Experience 5, Iter 22, disc loss: 0.004748049971429037, policy loss: 6.747043251514137
Experience 5, Iter 23, disc loss: 0.005642513489686767, policy loss: 6.465317927571932
Experience 5, Iter 24, disc loss: 0.0050133104808147465, policy loss: 6.377689097810183
Experience 5, Iter 25, disc loss: 0.005388262084763758, policy loss: 6.324164410757313
Experience 5, Iter 26, disc loss: 0.008027275527541218, policy loss: 6.0241606102238725
Experience 5, Iter 27, disc loss: 0.006495203665519934, policy loss: 6.114637381753074
Experience 5, Iter 28, disc loss: 0.005967873128801215, policy loss: 6.400331724955059
Experience 5, Iter 29, disc loss: 0.0059639600806260665, policy loss: 6.347376587211594
Experience 5, Iter 30, disc loss: 0.007408451817850409, policy loss: 6.15227536885449
Experience 5, Iter 31, disc loss: 0.004921833015474335, policy loss: 6.63485919189001
Experience 5, Iter 32, disc loss: 0.007838378209005382, policy loss: 6.203628649610377
Experience 5, Iter 33, disc loss: 0.0057263263467735, policy loss: 6.298026563097384
Experience 5, Iter 34, disc loss: 0.004725096788429842, policy loss: 6.32312829174957
Experience 5, Iter 35, disc loss: 0.005307378504156294, policy loss: 6.198883107311636
Experience 5, Iter 36, disc loss: 0.006297111425381527, policy loss: 6.539242551335057
Experience 5, Iter 37, disc loss: 0.005005454181648454, policy loss: 6.215118729348454
Experience 5, Iter 38, disc loss: 0.00664477431808275, policy loss: 6.253650295536129
Experience 5, Iter 39, disc loss: 0.0037235755332456323, policy loss: 6.539774622483246
Experience 5, Iter 40, disc loss: 0.006138988254795099, policy loss: 6.678713383612586
Experience 5, Iter 41, disc loss: 0.005644565826769146, policy loss: 6.296937161643928
Experience 5, Iter 42, disc loss: 0.004888265922196164, policy loss: 6.455824192634839
Experience 5, Iter 43, disc loss: 0.005670462006795833, policy loss: 6.469265374500454
Experience 5, Iter 44, disc loss: 0.0049841692380834905, policy loss: 6.3572477260699145
Experience 5, Iter 45, disc loss: 0.006219935192253124, policy loss: 6.3105478272415185
Experience 5, Iter 46, disc loss: 0.004978825617479333, policy loss: 6.336381269929512
Experience 5, Iter 47, disc loss: 0.005289862780928911, policy loss: 6.475551491025176
Experience 5, Iter 48, disc loss: 0.00397488419058282, policy loss: 6.809167812544047
Experience 5, Iter 49, disc loss: 0.005522983757086873, policy loss: 6.39731785124668
Experience 5, Iter 50, disc loss: 0.004391958389711679, policy loss: 6.587738600648734
Experience 5, Iter 51, disc loss: 0.005858919666622164, policy loss: 6.411432011359359
Experience 5, Iter 52, disc loss: 0.0029789809027801744, policy loss: 6.918818172740205
Experience 5, Iter 53, disc loss: 0.004799333646364244, policy loss: 6.346871989444573
Experience 5, Iter 54, disc loss: 0.005832771847649732, policy loss: 6.603368700398027
Experience 5, Iter 55, disc loss: 0.004541505623729734, policy loss: 6.4852333075408595
Experience 5, Iter 56, disc loss: 0.004372792990319546, policy loss: 6.282255672555111
Experience 5, Iter 57, disc loss: 0.002879586616763499, policy loss: 6.946443262761482
Experience 5, Iter 58, disc loss: 0.003904554786502524, policy loss: 6.626061147684691
Experience 5, Iter 59, disc loss: 0.004007101502262656, policy loss: 6.851983558447877
Experience 5, Iter 60, disc loss: 0.004584737466889808, policy loss: 6.560310602852207
Experience 5, Iter 61, disc loss: 0.0070253554482935885, policy loss: 6.125549979487211
Experience 5, Iter 62, disc loss: 0.005757132113587977, policy loss: 6.543854056053164
Experience 5, Iter 63, disc loss: 0.004247839656697491, policy loss: 6.794945516245009
Experience 5, Iter 64, disc loss: 0.007152616995061069, policy loss: 6.368070566084249
Experience 5, Iter 65, disc loss: 0.005637416288559247, policy loss: 6.385484486043168
Experience 5, Iter 66, disc loss: 0.005248986369810731, policy loss: 6.556548377048316
Experience 5, Iter 67, disc loss: 0.00374528552647817, policy loss: 6.687268673076088
Experience 5, Iter 68, disc loss: 0.00391829358297611, policy loss: 6.784764723715595
Experience 5, Iter 69, disc loss: 0.004875516134799626, policy loss: 6.472929981308397
Experience 5, Iter 70, disc loss: 0.004576788400740385, policy loss: 6.735129318781798
Experience 5, Iter 71, disc loss: 0.004764128537253113, policy loss: 6.828499468995225
Experience 5, Iter 72, disc loss: 0.003870053105246971, policy loss: 7.146115562926199
Experience 5, Iter 73, disc loss: 0.005038563987335888, policy loss: 6.630404568104589
Experience 5, Iter 74, disc loss: 0.004090563321695707, policy loss: 6.816123275681803
Experience 5, Iter 75, disc loss: 0.0032158811199508846, policy loss: 6.727695523391737
Experience 5, Iter 76, disc loss: 0.004211847215519819, policy loss: 6.717790593797159
Experience 5, Iter 77, disc loss: 0.002868664781196507, policy loss: 6.8467732542772275
Experience 5, Iter 78, disc loss: 0.0034612680101187643, policy loss: 7.09080215057217
Experience 5, Iter 79, disc loss: 0.004731374578449067, policy loss: 6.464871050970679
Experience 5, Iter 80, disc loss: 0.004154601011790569, policy loss: 6.720547686821963
Experience 5, Iter 81, disc loss: 0.004081947883997667, policy loss: 6.8250603934769405
Experience 5, Iter 82, disc loss: 0.004230917562322646, policy loss: 6.432065544630187
Experience 5, Iter 83, disc loss: 0.004511413486131505, policy loss: 6.519505149918328
Experience 5, Iter 84, disc loss: 0.005794484202001988, policy loss: 6.507127000981786
Experience 5, Iter 85, disc loss: 0.006554882604650583, policy loss: 6.77257248896077
Experience 5, Iter 86, disc loss: 0.004239204126660086, policy loss: 6.392851569113992
Experience 5, Iter 87, disc loss: 0.0021438747903045822, policy loss: 7.107852349100263
Experience 5, Iter 88, disc loss: 0.0049917139570998205, policy loss: 6.6451787745187065
Experience 5, Iter 89, disc loss: 0.002918926817332083, policy loss: 6.7461404304990475
Experience 5, Iter 90, disc loss: 0.003867065509151843, policy loss: 6.77500320262896
Experience 5, Iter 91, disc loss: 0.003584480922733185, policy loss: 7.111672693879648
Experience 5, Iter 92, disc loss: 0.00437741002511188, policy loss: 6.5642678541493105
Experience 5, Iter 93, disc loss: 0.0029995878877550754, policy loss: 6.854725696224481
Experience 5, Iter 94, disc loss: 0.004842646822099771, policy loss: 6.739060667664903
Experience 5, Iter 95, disc loss: 0.004232142902620612, policy loss: 6.7556617694226695
Experience 5, Iter 96, disc loss: 0.004051845180594451, policy loss: 6.63895952537627
Experience 5, Iter 97, disc loss: 0.002765858916325131, policy loss: 6.901475234624473
Experience 5, Iter 98, disc loss: 0.0037612434735269785, policy loss: 6.903006362578514
Experience 5, Iter 99, disc loss: 0.005099601822427775, policy loss: 6.7466170380289565
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0035],
        [0.0519],
        [0.0012]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.9440e-03, 3.7705e-02, 4.9359e-02, 2.2113e-03, 2.3072e-05,
          1.2383e-01]],

        [[8.9440e-03, 3.7705e-02, 4.9359e-02, 2.2113e-03, 2.3072e-05,
          1.2383e-01]],

        [[8.9440e-03, 3.7705e-02, 4.9359e-02, 2.2113e-03, 2.3072e-05,
          1.2383e-01]],

        [[8.9440e-03, 3.7705e-02, 4.9359e-02, 2.2113e-03, 2.3072e-05,
          1.2383e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0140, 0.2075, 0.0049], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0140, 0.2075, 0.0049])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.340
Iter 2/2000 - Loss: -1.440
Iter 3/2000 - Loss: -2.319
Iter 4/2000 - Loss: -2.407
Iter 5/2000 - Loss: -2.084
Iter 6/2000 - Loss: -2.069
Iter 7/2000 - Loss: -2.349
Iter 8/2000 - Loss: -2.583
Iter 9/2000 - Loss: -2.544
Iter 10/2000 - Loss: -2.366
Iter 11/2000 - Loss: -2.327
Iter 12/2000 - Loss: -2.440
Iter 13/2000 - Loss: -2.529
Iter 14/2000 - Loss: -2.539
Iter 15/2000 - Loss: -2.555
Iter 16/2000 - Loss: -2.571
Iter 17/2000 - Loss: -2.532
Iter 18/2000 - Loss: -2.498
Iter 19/2000 - Loss: -2.544
Iter 20/2000 - Loss: -2.613
Iter 1981/2000 - Loss: -2.653
Iter 1982/2000 - Loss: -2.644
Iter 1983/2000 - Loss: -2.655
Iter 1984/2000 - Loss: -2.674
Iter 1985/2000 - Loss: -2.675
Iter 1986/2000 - Loss: -2.640
Iter 1987/2000 - Loss: -2.604
Iter 1988/2000 - Loss: -2.619
Iter 1989/2000 - Loss: -2.673
Iter 1990/2000 - Loss: -2.653
Iter 1991/2000 - Loss: -2.643
Iter 1992/2000 - Loss: -2.685
Iter 1993/2000 - Loss: -2.671
Iter 1994/2000 - Loss: -2.629
Iter 1995/2000 - Loss: -2.621
Iter 1996/2000 - Loss: -2.660
Iter 1997/2000 - Loss: -2.673
Iter 1998/2000 - Loss: -2.650
Iter 1999/2000 - Loss: -2.678
Iter 2000/2000 - Loss: -2.665
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0029],
        [0.0396],
        [0.0010]])
Lengthscale: tensor([[[8.9719e-03, 3.7715e-02, 4.9516e-02, 2.2113e-03, 2.3163e-05,
          1.2384e-01]],

        [[8.9421e-03, 3.7704e-02, 4.9349e-02, 2.2113e-03, 2.3066e-05,
          1.2383e-01]],

        [[8.9868e-03, 3.7720e-02, 4.9600e-02, 2.2114e-03, 2.3212e-05,
          1.2385e-01]],

        [[8.9467e-03, 3.7706e-02, 4.9374e-02, 2.2113e-03, 2.3081e-05,
          1.2383e-01]]])
Signal Variance: tensor([0.0032, 0.0115, 0.1645, 0.0039])
Estimated target variance: tensor([0.0041, 0.0140, 0.2075, 0.0049])
N: 60
Signal to noise ratio: tensor([2.0027, 2.0074, 2.0393, 2.0016])
Bound on condition number: tensor([241.6566, 242.7909, 250.5146, 241.3868])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.002498548146841293, policy loss: 7.290390582545626
Experience 6, Iter 1, disc loss: 0.0047252222659699035, policy loss: 6.742013704704689
Experience 6, Iter 2, disc loss: 0.002803121374759014, policy loss: 7.121947367426396
Experience 6, Iter 3, disc loss: 0.0038401338224267187, policy loss: 6.993912446495181
Experience 6, Iter 4, disc loss: 0.0030776243470075405, policy loss: 7.001304613571446
Experience 6, Iter 5, disc loss: 0.0038610128158563586, policy loss: 6.795698945173781
Experience 6, Iter 6, disc loss: 0.003322945704229024, policy loss: 7.1435173493792234
Experience 6, Iter 7, disc loss: 0.0026667698113403433, policy loss: 7.356340824502813
Experience 6, Iter 8, disc loss: 0.0034308612763242962, policy loss: 6.899599781175641
Experience 6, Iter 9, disc loss: 0.003106910508753711, policy loss: 6.979746892351685
Experience 6, Iter 10, disc loss: 0.0029260574394394793, policy loss: 7.134350940559127
Experience 6, Iter 11, disc loss: 0.0022381603241520593, policy loss: 7.268677545470069
Experience 6, Iter 12, disc loss: 0.003003922444884722, policy loss: 7.02982971697076
Experience 6, Iter 13, disc loss: 0.0024953406777747603, policy loss: 7.020372490900634
Experience 6, Iter 14, disc loss: 0.0053869765357974116, policy loss: 6.824129048785304
Experience 6, Iter 15, disc loss: 0.003779696837707725, policy loss: 7.061574182287006
Experience 6, Iter 16, disc loss: 0.0025054587781527297, policy loss: 7.252889780875331
Experience 6, Iter 17, disc loss: 0.003965520918939526, policy loss: 6.969934631380575
Experience 6, Iter 18, disc loss: 0.0038900788919307913, policy loss: 6.8600309581266
Experience 6, Iter 19, disc loss: 0.0025090218405054696, policy loss: 7.343068401552552
Experience 6, Iter 20, disc loss: 0.002889340425625149, policy loss: 7.179261086665554
Experience 6, Iter 21, disc loss: 0.001985489530161176, policy loss: 7.197378801959827
Experience 6, Iter 22, disc loss: 0.003040832910325409, policy loss: 7.325900728816601
Experience 6, Iter 23, disc loss: 0.002512443631395748, policy loss: 7.060853854062846
Experience 6, Iter 24, disc loss: 0.0028339648607066397, policy loss: 7.084995821935014
Experience 6, Iter 25, disc loss: 0.0020277044230910175, policy loss: 7.1964290570892775
Experience 6, Iter 26, disc loss: 0.003032543310189451, policy loss: 6.867844844387305
Experience 6, Iter 27, disc loss: 0.002173848858728035, policy loss: 7.255254409472683
Experience 6, Iter 28, disc loss: 0.0019188014581519863, policy loss: 7.3399546463482155
Experience 6, Iter 29, disc loss: 0.0035298205523295817, policy loss: 7.032395606697399
Experience 6, Iter 30, disc loss: 0.005390007179426586, policy loss: 7.113079552493881
Experience 6, Iter 31, disc loss: 0.002552062312647138, policy loss: 7.049399948992131
Experience 6, Iter 32, disc loss: 0.0027114883628341383, policy loss: 7.280239174766175
Experience 6, Iter 33, disc loss: 0.005164256893587986, policy loss: 6.939792014129958
Experience 6, Iter 34, disc loss: 0.004573410770173426, policy loss: 6.994965863532922
Experience 6, Iter 35, disc loss: 0.001956965844889107, policy loss: 7.6074278100949035
Experience 6, Iter 36, disc loss: 0.002142544091894747, policy loss: 7.514592973826158
Experience 6, Iter 37, disc loss: 0.0032840923865182403, policy loss: 7.051601852550629
Experience 6, Iter 38, disc loss: 0.0029126881705740302, policy loss: 7.078582930513539
Experience 6, Iter 39, disc loss: 0.0036781528178197716, policy loss: 6.823985947641556
Experience 6, Iter 40, disc loss: 0.004953559964264609, policy loss: 6.847312989587308
Experience 6, Iter 41, disc loss: 0.0028002169226605273, policy loss: 6.963434054969437
Experience 6, Iter 42, disc loss: 0.002698750508687477, policy loss: 7.3268941747280545
Experience 6, Iter 43, disc loss: 0.0019975604613490716, policy loss: 7.628504061631892
Experience 6, Iter 44, disc loss: 0.002030770100770727, policy loss: 7.18131389210909
Experience 6, Iter 45, disc loss: 0.0032389328262635176, policy loss: 6.949220884765365
Experience 6, Iter 46, disc loss: 0.0019201475857042144, policy loss: 7.303464540807808
Experience 6, Iter 47, disc loss: 0.002539194009128807, policy loss: 7.224912435769879
Experience 6, Iter 48, disc loss: 0.00351258426805396, policy loss: 7.291185658431632
Experience 6, Iter 49, disc loss: 0.0025113617694190213, policy loss: 7.450745444296438
Experience 6, Iter 50, disc loss: 0.0025435730279737166, policy loss: 7.391502852178839
Experience 6, Iter 51, disc loss: 0.002679648895813894, policy loss: 7.333260613546331
Experience 6, Iter 52, disc loss: 0.002340494490666567, policy loss: 7.3660371145592
Experience 6, Iter 53, disc loss: 0.0026869166131893354, policy loss: 7.2006517440632765
Experience 6, Iter 54, disc loss: 0.002128423816565682, policy loss: 7.510253839767982
Experience 6, Iter 55, disc loss: 0.0020220592341863245, policy loss: 7.464753616619749
Experience 6, Iter 56, disc loss: 0.002199873363581283, policy loss: 7.724501996907303
Experience 6, Iter 57, disc loss: 0.0022743658977397276, policy loss: 7.476368852870508
Experience 6, Iter 58, disc loss: 0.001929779347287627, policy loss: 7.439056885117905
Experience 6, Iter 59, disc loss: 0.0018219752781141337, policy loss: 7.533144461981362
Experience 6, Iter 60, disc loss: 0.0025446928573471476, policy loss: 7.511231675478392
Experience 6, Iter 61, disc loss: 0.0016837230495283409, policy loss: 7.342002526986523
Experience 6, Iter 62, disc loss: 0.0024669346449921747, policy loss: 7.451183255645668
Experience 6, Iter 63, disc loss: 0.0018421337894410144, policy loss: 7.356620247000421
Experience 6, Iter 64, disc loss: 0.0024356291401716485, policy loss: 7.157163422559122
Experience 6, Iter 65, disc loss: 0.0016297777331501276, policy loss: 7.434326293837893
Experience 6, Iter 66, disc loss: 0.002386847271953155, policy loss: 7.239567873818743
Experience 6, Iter 67, disc loss: 0.003135982196897823, policy loss: 7.401317516646984
Experience 6, Iter 68, disc loss: 0.002698477515948681, policy loss: 7.221736247039393
Experience 6, Iter 69, disc loss: 0.0031299127179128692, policy loss: 7.168149658890467
Experience 6, Iter 70, disc loss: 0.0034471841530116566, policy loss: 7.193144426376774
Experience 6, Iter 71, disc loss: 0.0025401376199244013, policy loss: 7.674260397527777
Experience 6, Iter 72, disc loss: 0.003040999799678418, policy loss: 7.329213772953387
Experience 6, Iter 73, disc loss: 0.00257024124087878, policy loss: 7.311265197310448
Experience 6, Iter 74, disc loss: 0.002115371597231309, policy loss: 7.397809464172929
Experience 6, Iter 75, disc loss: 0.0027869554107995687, policy loss: 7.558099164886032
Experience 6, Iter 76, disc loss: 0.002220821470977085, policy loss: 7.435357560538137
Experience 6, Iter 77, disc loss: 0.0026352454554353965, policy loss: 7.297790260371375
Experience 6, Iter 78, disc loss: 0.0025112492491433736, policy loss: 7.442852462933609
Experience 6, Iter 79, disc loss: 0.0026905177427375684, policy loss: 7.297936073412245
Experience 6, Iter 80, disc loss: 0.0029508505138364735, policy loss: 6.847155630904063
Experience 6, Iter 81, disc loss: 0.002997782464842732, policy loss: 7.481432642195275
Experience 6, Iter 82, disc loss: 0.0014591417704248146, policy loss: 7.60390583262234
Experience 6, Iter 83, disc loss: 0.0027613418466727693, policy loss: 7.0291814335478975
Experience 6, Iter 84, disc loss: 0.002505758176130926, policy loss: 7.428325993637647
Experience 6, Iter 85, disc loss: 0.0033018337693168994, policy loss: 7.6116530921211165
Experience 6, Iter 86, disc loss: 0.001999129181239129, policy loss: 7.633912978111145
Experience 6, Iter 87, disc loss: 0.001571925763503582, policy loss: 8.038592006495222
Experience 6, Iter 88, disc loss: 0.001583095214330008, policy loss: 7.680812777539538
Experience 6, Iter 89, disc loss: 0.0021649363983161567, policy loss: 7.5780325247303235
Experience 6, Iter 90, disc loss: 0.0016086956287628509, policy loss: 7.581428689487906
Experience 6, Iter 91, disc loss: 0.0025383458723894055, policy loss: 7.322324504916143
Experience 6, Iter 92, disc loss: 0.0020802694494065363, policy loss: 7.190649986342011
Experience 6, Iter 93, disc loss: 0.0023638957291126374, policy loss: 7.725937052036802
Experience 6, Iter 94, disc loss: 0.0020660721252224066, policy loss: 7.244439695124004
Experience 6, Iter 95, disc loss: 0.002280474247069268, policy loss: 7.468651750900226
Experience 6, Iter 96, disc loss: 0.002318432898403328, policy loss: 7.143637885134368
Experience 6, Iter 97, disc loss: 0.0022713753001078877, policy loss: 7.558260533230648
Experience 6, Iter 98, disc loss: 0.0011752296107688312, policy loss: 7.803814815005737
Experience 6, Iter 99, disc loss: 0.0029831027184687446, policy loss: 7.516612214115764
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0032],
        [0.0445],
        [0.0011]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.5932e-03, 3.4456e-02, 4.2306e-02, 1.9560e-03, 2.1706e-05,
          1.1344e-01]],

        [[9.5932e-03, 3.4456e-02, 4.2306e-02, 1.9560e-03, 2.1706e-05,
          1.1344e-01]],

        [[9.5932e-03, 3.4456e-02, 4.2306e-02, 1.9560e-03, 2.1706e-05,
          1.1344e-01]],

        [[9.5932e-03, 3.4456e-02, 4.2306e-02, 1.9560e-03, 2.1706e-05,
          1.1344e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0038, 0.0128, 0.1780, 0.0042], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0038, 0.0128, 0.1780, 0.0042])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.639
Iter 2/2000 - Loss: -1.392
Iter 3/2000 - Loss: -2.624
Iter 4/2000 - Loss: -2.648
Iter 5/2000 - Loss: -2.146
Iter 6/2000 - Loss: -2.201
Iter 7/2000 - Loss: -2.604
Iter 8/2000 - Loss: -2.822
Iter 9/2000 - Loss: -2.723
Iter 10/2000 - Loss: -2.565
Iter 11/2000 - Loss: -2.565
Iter 12/2000 - Loss: -2.664
Iter 13/2000 - Loss: -2.731
Iter 14/2000 - Loss: -2.760
Iter 15/2000 - Loss: -2.822
Iter 16/2000 - Loss: -2.881
Iter 17/2000 - Loss: -2.861
Iter 18/2000 - Loss: -2.807
Iter 19/2000 - Loss: -2.839
Iter 20/2000 - Loss: -2.944
Iter 1981/2000 - Loss: -8.578
Iter 1982/2000 - Loss: -8.578
Iter 1983/2000 - Loss: -8.578
Iter 1984/2000 - Loss: -8.578
Iter 1985/2000 - Loss: -8.578
Iter 1986/2000 - Loss: -8.578
Iter 1987/2000 - Loss: -8.578
Iter 1988/2000 - Loss: -8.578
Iter 1989/2000 - Loss: -8.578
Iter 1990/2000 - Loss: -8.578
Iter 1991/2000 - Loss: -8.578
Iter 1992/2000 - Loss: -8.578
Iter 1993/2000 - Loss: -8.578
Iter 1994/2000 - Loss: -8.578
Iter 1995/2000 - Loss: -8.578
Iter 1996/2000 - Loss: -8.578
Iter 1997/2000 - Loss: -8.578
Iter 1998/2000 - Loss: -8.578
Iter 1999/2000 - Loss: -8.578
Iter 2000/2000 - Loss: -8.578
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[20.1163,  1.0509, 29.0512, 15.4703,  3.5736, 24.3200]],

        [[29.5636, 34.4693, 25.4939,  1.7655,  4.9385,  7.2762]],

        [[29.7914, 49.0903, 19.8458,  1.4078,  4.7977, 12.8106]],

        [[32.3393, 38.1671,  9.2749,  2.5769,  6.6588, 20.8401]]])
Signal Variance: tensor([0.0136, 0.3715, 9.2070, 0.2552])
Estimated target variance: tensor([0.0038, 0.0128, 0.1780, 0.0042])
N: 70
Signal to noise ratio: tensor([ 7.7322, 33.8494, 58.5401, 29.6968])
Bound on condition number: tensor([  4186.0721,  80205.5210, 239886.8266,  61733.9141])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.029306693539149005, policy loss: 5.450706810089816
Experience 7, Iter 1, disc loss: 0.02065813776875838, policy loss: 5.301764798823614
Experience 7, Iter 2, disc loss: 0.028018782067459107, policy loss: 5.324934742167078
Experience 7, Iter 3, disc loss: 0.019260772970649483, policy loss: 5.140703740529362
Experience 7, Iter 4, disc loss: 0.023843121073257968, policy loss: 5.244495456540958
Experience 7, Iter 5, disc loss: 0.03039330173446382, policy loss: 4.670202584767385
Experience 7, Iter 6, disc loss: 0.02569631627377303, policy loss: 4.495930655028242
Experience 7, Iter 7, disc loss: 0.01520962194396404, policy loss: 5.067877176661261
Experience 7, Iter 8, disc loss: 0.018787352961097584, policy loss: 4.935895947293923
Experience 7, Iter 9, disc loss: 0.015346212190734466, policy loss: 4.830473650771517
Experience 7, Iter 10, disc loss: 0.016707463841314378, policy loss: 4.672483001761684
Experience 7, Iter 11, disc loss: 0.018911328284817796, policy loss: 4.690974530979342
Experience 7, Iter 12, disc loss: 0.013144872498618933, policy loss: 4.831130319706895
Experience 7, Iter 13, disc loss: 0.016675146454596293, policy loss: 4.840523826853087
Experience 7, Iter 14, disc loss: 0.011112783925362904, policy loss: 5.304830235761933
Experience 7, Iter 15, disc loss: 0.012545497666619084, policy loss: 4.8432684211992205
Experience 7, Iter 16, disc loss: 0.012107236505656753, policy loss: 4.985377144348611
Experience 7, Iter 17, disc loss: 0.014861880012400793, policy loss: 4.6941001549788215
Experience 7, Iter 18, disc loss: 0.011918590743306472, policy loss: 4.891530981748943
Experience 7, Iter 19, disc loss: 0.01179610452559706, policy loss: 4.908591025221139
Experience 7, Iter 20, disc loss: 0.011670997811827344, policy loss: 5.027954933336242
Experience 7, Iter 21, disc loss: 0.014100807719748473, policy loss: 4.732290178040693
Experience 7, Iter 22, disc loss: 0.010975330237905026, policy loss: 4.974302601260515
Experience 7, Iter 23, disc loss: 0.01313993633047702, policy loss: 4.875284336646937
Experience 7, Iter 24, disc loss: 0.011376841026925235, policy loss: 4.834423524821292
Experience 7, Iter 25, disc loss: 0.01142107086117427, policy loss: 5.012600186092765
Experience 7, Iter 26, disc loss: 0.012389068304492112, policy loss: 4.9579127573796375
Experience 7, Iter 27, disc loss: 0.012926595280013999, policy loss: 4.808348652209825
Experience 7, Iter 28, disc loss: 0.010605338396047503, policy loss: 4.98026909269248
Experience 7, Iter 29, disc loss: 0.011169698661688201, policy loss: 4.920029387699172
Experience 7, Iter 30, disc loss: 0.009575213135938811, policy loss: 5.0749438576686865
Experience 7, Iter 31, disc loss: 0.011771327416912418, policy loss: 4.764138109630032
Experience 7, Iter 32, disc loss: 0.011768419954702363, policy loss: 4.885509613623892
Experience 7, Iter 33, disc loss: 0.011385218035871442, policy loss: 4.847148403609473
Experience 7, Iter 34, disc loss: 0.010184604293296293, policy loss: 5.116075299850112
Experience 7, Iter 35, disc loss: 0.011285525463124686, policy loss: 4.990733102689831
Experience 7, Iter 36, disc loss: 0.010663566514684596, policy loss: 5.021676061506049
Experience 7, Iter 37, disc loss: 0.010110270708852407, policy loss: 5.036218772275264
Experience 7, Iter 38, disc loss: 0.008803867104998149, policy loss: 5.310467252895422
Experience 7, Iter 39, disc loss: 0.008704521390863262, policy loss: 5.161159717845266
Experience 7, Iter 40, disc loss: 0.008349600255515191, policy loss: 5.186714588546318
Experience 7, Iter 41, disc loss: 0.008878187379151926, policy loss: 5.306643328491338
Experience 7, Iter 42, disc loss: 0.008745836222935217, policy loss: 5.172774658476503
Experience 7, Iter 43, disc loss: 0.008581084618546503, policy loss: 5.09357363395256
Experience 7, Iter 44, disc loss: 0.009259479975442022, policy loss: 5.291875891730079
Experience 7, Iter 45, disc loss: 0.007006551364825207, policy loss: 5.313829155545699
Experience 7, Iter 46, disc loss: 0.007930326109237879, policy loss: 5.351441950886189
Experience 7, Iter 47, disc loss: 0.006915075438297653, policy loss: 5.423048273223506
Experience 7, Iter 48, disc loss: 0.007299380945070253, policy loss: 5.367159511621908
Experience 7, Iter 49, disc loss: 0.007144337514580508, policy loss: 5.453631516817378
Experience 7, Iter 50, disc loss: 0.008114924690920622, policy loss: 5.3458993865434135
Experience 7, Iter 51, disc loss: 0.006927657267673297, policy loss: 5.34065482461307
Experience 7, Iter 52, disc loss: 0.00652160518699267, policy loss: 5.534781824843069
Experience 7, Iter 53, disc loss: 0.008940380595008573, policy loss: 5.086196461119751
Experience 7, Iter 54, disc loss: 0.006784905034315567, policy loss: 5.381943298844222
Experience 7, Iter 55, disc loss: 0.006662672396965805, policy loss: 5.478424946813002
Experience 7, Iter 56, disc loss: 0.0065943056148932856, policy loss: 5.50389392332057
Experience 7, Iter 57, disc loss: 0.0075317348711724415, policy loss: 5.422980858149114
Experience 7, Iter 58, disc loss: 0.005672859616841259, policy loss: 5.7805218233444755
Experience 7, Iter 59, disc loss: 0.0060324683528214205, policy loss: 5.535985185415445
Experience 7, Iter 60, disc loss: 0.005591274112211474, policy loss: 5.631236484396551
Experience 7, Iter 61, disc loss: 0.006344383732871141, policy loss: 5.38642386727078
Experience 7, Iter 62, disc loss: 0.005374300019789468, policy loss: 5.8080316249566195
Experience 7, Iter 63, disc loss: 0.006199189725004359, policy loss: 5.435491050809273
Experience 7, Iter 64, disc loss: 0.0056470960928203355, policy loss: 5.603472827065246
Experience 7, Iter 65, disc loss: 0.005973380441673521, policy loss: 5.510975026769981
Experience 7, Iter 66, disc loss: 0.006194882566677985, policy loss: 5.4963890790099965
Experience 7, Iter 67, disc loss: 0.005697894194419625, policy loss: 5.65381711717146
Experience 7, Iter 68, disc loss: 0.005260686359276509, policy loss: 5.80698857319756
Experience 7, Iter 69, disc loss: 0.004535934388452588, policy loss: 5.917094838039273
Experience 7, Iter 70, disc loss: 0.005231557290104718, policy loss: 5.8707780655115265
Experience 7, Iter 71, disc loss: 0.004577512160820097, policy loss: 6.026413331597645
Experience 7, Iter 72, disc loss: 0.006011215490765659, policy loss: 5.511210133596905
Experience 7, Iter 73, disc loss: 0.004642996103005878, policy loss: 5.815268239902345
Experience 7, Iter 74, disc loss: 0.004169549782418237, policy loss: 5.886202283692078
Experience 7, Iter 75, disc loss: 0.004525789503954376, policy loss: 5.954463678861262
Experience 7, Iter 76, disc loss: 0.004937386981488481, policy loss: 5.750521826838594
Experience 7, Iter 77, disc loss: 0.004811288229598885, policy loss: 5.952533784392386
Experience 7, Iter 78, disc loss: 0.004214386139823915, policy loss: 5.895804963129331
Experience 7, Iter 79, disc loss: 0.0045338036770244915, policy loss: 5.856278938156123
Experience 7, Iter 80, disc loss: 0.004544070652588839, policy loss: 5.907366927675344
Experience 7, Iter 81, disc loss: 0.003944522953372995, policy loss: 6.022313007166964
Experience 7, Iter 82, disc loss: 0.00456290220211414, policy loss: 5.859947319767022
Experience 7, Iter 83, disc loss: 0.004119057208475822, policy loss: 5.9632502278489365
Experience 7, Iter 84, disc loss: 0.003514225240124383, policy loss: 6.139073501428932
Experience 7, Iter 85, disc loss: 0.003945916240874985, policy loss: 6.026045216324864
Experience 7, Iter 86, disc loss: 0.003621392634172163, policy loss: 6.045290862117941
Experience 7, Iter 87, disc loss: 0.0033237695164879183, policy loss: 6.179010684219844
Experience 7, Iter 88, disc loss: 0.0037895187909759744, policy loss: 5.946071070610735
Experience 7, Iter 89, disc loss: 0.003732200145306049, policy loss: 6.0198544066674735
Experience 7, Iter 90, disc loss: 0.0036733150535396162, policy loss: 5.994282424542308
Experience 7, Iter 91, disc loss: 0.0041359068344990705, policy loss: 5.987777061593009
Experience 7, Iter 92, disc loss: 0.004513852235598802, policy loss: 5.876207170525988
Experience 7, Iter 93, disc loss: 0.003122496777578554, policy loss: 6.209790093793243
Experience 7, Iter 94, disc loss: 0.003826596995757999, policy loss: 6.126166758668001
Experience 7, Iter 95, disc loss: 0.003978141238598529, policy loss: 6.053128028458893
Experience 7, Iter 96, disc loss: 0.0032452038236105664, policy loss: 6.145451805268101
Experience 7, Iter 97, disc loss: 0.0033749977503337615, policy loss: 6.178248606964473
Experience 7, Iter 98, disc loss: 0.003443529252721947, policy loss: 6.04535849197603
Experience 7, Iter 99, disc loss: 0.003528585848430285, policy loss: 6.154862510389076
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0049],
        [0.0531],
        [0.0010]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.1398e-03, 3.7045e-02, 3.9399e-02, 2.0178e-03, 1.9409e-05,
          1.5279e-01]],

        [[9.1398e-03, 3.7045e-02, 3.9399e-02, 2.0178e-03, 1.9409e-05,
          1.5279e-01]],

        [[9.1398e-03, 3.7045e-02, 3.9399e-02, 2.0178e-03, 1.9409e-05,
          1.5279e-01]],

        [[9.1398e-03, 3.7045e-02, 3.9399e-02, 2.0178e-03, 1.9409e-05,
          1.5279e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0195, 0.2123, 0.0039], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0195, 0.2123, 0.0039])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.465
Iter 2/2000 - Loss: -0.834
Iter 3/2000 - Loss: -2.448
Iter 4/2000 - Loss: -2.262
Iter 5/2000 - Loss: -1.679
Iter 6/2000 - Loss: -1.892
Iter 7/2000 - Loss: -2.335
Iter 8/2000 - Loss: -2.490
Iter 9/2000 - Loss: -2.360
Iter 10/2000 - Loss: -2.203
Iter 11/2000 - Loss: -2.182
Iter 12/2000 - Loss: -2.275
Iter 13/2000 - Loss: -2.384
Iter 14/2000 - Loss: -2.443
Iter 15/2000 - Loss: -2.453
Iter 16/2000 - Loss: -2.449
Iter 17/2000 - Loss: -2.454
Iter 18/2000 - Loss: -2.471
Iter 19/2000 - Loss: -2.495
Iter 20/2000 - Loss: -2.533
Iter 1981/2000 - Loss: -8.584
Iter 1982/2000 - Loss: -8.584
Iter 1983/2000 - Loss: -8.584
Iter 1984/2000 - Loss: -8.584
Iter 1985/2000 - Loss: -8.584
Iter 1986/2000 - Loss: -8.584
Iter 1987/2000 - Loss: -8.584
Iter 1988/2000 - Loss: -8.584
Iter 1989/2000 - Loss: -8.584
Iter 1990/2000 - Loss: -8.584
Iter 1991/2000 - Loss: -8.584
Iter 1992/2000 - Loss: -8.584
Iter 1993/2000 - Loss: -8.584
Iter 1994/2000 - Loss: -8.584
Iter 1995/2000 - Loss: -8.584
Iter 1996/2000 - Loss: -8.585
Iter 1997/2000 - Loss: -8.585
Iter 1998/2000 - Loss: -8.585
Iter 1999/2000 - Loss: -8.585
Iter 2000/2000 - Loss: -8.585
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[18.4481,  1.2414, 29.1403, 15.5244,  3.2936, 17.4771]],

        [[31.2017, 35.1268, 32.1421,  2.5920,  6.3039, 11.9518]],

        [[27.5119, 45.6518, 22.2736,  1.6128,  5.0189, 16.5520]],

        [[31.4965, 49.5257,  9.1187,  2.5836,  6.4328, 23.5384]]])
Signal Variance: tensor([ 0.0153,  0.8184, 12.5875,  0.2397])
Estimated target variance: tensor([0.0042, 0.0195, 0.2123, 0.0039])
N: 80
Signal to noise ratio: tensor([ 7.5240, 47.2969, 71.4765, 29.5065])
Bound on condition number: tensor([  4529.9029, 178961.0407, 408711.8774,  69651.5998])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.0037456493338867197, policy loss: 5.992902668105821
Experience 8, Iter 1, disc loss: 0.0037955455657572084, policy loss: 6.014620899648535
Experience 8, Iter 2, disc loss: 0.0039631664073962476, policy loss: 5.940310820507586
Experience 8, Iter 3, disc loss: 0.0039194123087874615, policy loss: 6.018422623075737
Experience 8, Iter 4, disc loss: 0.0035095762990228255, policy loss: 6.064662521748225
Experience 8, Iter 5, disc loss: 0.002909370636123982, policy loss: 6.2872203458604465
Experience 8, Iter 6, disc loss: 0.003445765191625935, policy loss: 6.279117972704055
Experience 8, Iter 7, disc loss: 0.0034760683200146855, policy loss: 6.066700418256866
Experience 8, Iter 8, disc loss: 0.003249504803335662, policy loss: 6.234190826396409
Experience 8, Iter 9, disc loss: 0.0034683378295869553, policy loss: 6.0396938969788065
Experience 8, Iter 10, disc loss: 0.002950059068697002, policy loss: 6.2828877075812954
Experience 8, Iter 11, disc loss: 0.00301813645544654, policy loss: 6.259662464633348
Experience 8, Iter 12, disc loss: 0.003553396878967372, policy loss: 6.096805920494851
Experience 8, Iter 13, disc loss: 0.003680149318421584, policy loss: 6.227477291744699
Experience 8, Iter 14, disc loss: 0.003623760977824703, policy loss: 6.074725822839662
Experience 8, Iter 15, disc loss: 0.0032881686425300137, policy loss: 6.282166234585846
Experience 8, Iter 16, disc loss: 0.003310442429559257, policy loss: 6.154009388887182
Experience 8, Iter 17, disc loss: 0.0033764838603854773, policy loss: 6.227355518423045
Experience 8, Iter 18, disc loss: 0.0036192930809071733, policy loss: 6.035532318044838
Experience 8, Iter 19, disc loss: 0.0032814575923922535, policy loss: 6.11689709741916
Experience 8, Iter 20, disc loss: 0.002887460044175609, policy loss: 6.292559344308409
Experience 8, Iter 21, disc loss: 0.0035355324602900696, policy loss: 6.1335526919624765
Experience 8, Iter 22, disc loss: 0.0034660400482068583, policy loss: 6.212694259789377
Experience 8, Iter 23, disc loss: 0.0033386952029380718, policy loss: 6.154004542910751
Experience 8, Iter 24, disc loss: 0.0022308522356874346, policy loss: 6.620923845617435
Experience 8, Iter 25, disc loss: 0.0029060323416287242, policy loss: 6.319861699624573
Experience 8, Iter 26, disc loss: 0.002898082414289742, policy loss: 6.363250709318331
Experience 8, Iter 27, disc loss: 0.0028712964628039507, policy loss: 6.291184801959021
Experience 8, Iter 28, disc loss: 0.002685463812370663, policy loss: 6.5168664440169515
Experience 8, Iter 29, disc loss: 0.003254539774514453, policy loss: 6.195770366844803
Experience 8, Iter 30, disc loss: 0.0030056560014739865, policy loss: 6.209655368864987
Experience 8, Iter 31, disc loss: 0.0029134593082049626, policy loss: 6.293495896990215
Experience 8, Iter 32, disc loss: 0.0026003708494513533, policy loss: 6.443601597914032
Experience 8, Iter 33, disc loss: 0.002897946709489416, policy loss: 6.319294750716182
Experience 8, Iter 34, disc loss: 0.002827354587875502, policy loss: 6.3032960269247384
Experience 8, Iter 35, disc loss: 0.0031137796781927018, policy loss: 6.122050978016897
Experience 8, Iter 36, disc loss: 0.00293742687579591, policy loss: 6.389527727894512
Experience 8, Iter 37, disc loss: 0.0021594445016432954, policy loss: 6.635460644711678
Experience 8, Iter 38, disc loss: 0.0024008620065087125, policy loss: 6.583805192402832
Experience 8, Iter 39, disc loss: 0.0026796089812508015, policy loss: 6.416631779102324
Experience 8, Iter 40, disc loss: 0.002759019582524657, policy loss: 6.436188066665512
Experience 8, Iter 41, disc loss: 0.0029401651170839083, policy loss: 6.2970598152768
Experience 8, Iter 42, disc loss: 0.002741065644754393, policy loss: 6.272275072377839
Experience 8, Iter 43, disc loss: 0.002742991593936408, policy loss: 6.437564361257429
Experience 8, Iter 44, disc loss: 0.0019216506395883031, policy loss: 6.811387290015465
Experience 8, Iter 45, disc loss: 0.0026929300315914136, policy loss: 6.284443031661947
Experience 8, Iter 46, disc loss: 0.003012130126274174, policy loss: 6.186126016545284
Experience 8, Iter 47, disc loss: 0.0030589845412807575, policy loss: 6.409934036243399
Experience 8, Iter 48, disc loss: 0.002470099362519641, policy loss: 6.565337144200802
Experience 8, Iter 49, disc loss: 0.0024501485019786592, policy loss: 6.462611609918879
Experience 8, Iter 50, disc loss: 0.0028199245553842188, policy loss: 6.294018201943876
Experience 8, Iter 51, disc loss: 0.0027003589860156918, policy loss: 6.468938784100084
Experience 8, Iter 52, disc loss: 0.0021312378987220285, policy loss: 6.769794574987939
Experience 8, Iter 53, disc loss: 0.0026176106991970638, policy loss: 6.5519350194196
Experience 8, Iter 54, disc loss: 0.0027524160879807153, policy loss: 6.316528328367798
Experience 8, Iter 55, disc loss: 0.0025075683510717797, policy loss: 6.526206654866018
Experience 8, Iter 56, disc loss: 0.00222225135706613, policy loss: 6.837523217685664
Experience 8, Iter 57, disc loss: 0.002112828292427234, policy loss: 6.702237003268804
Experience 8, Iter 58, disc loss: 0.002135767056435856, policy loss: 6.7395918272655
Experience 8, Iter 59, disc loss: 0.002521769204941808, policy loss: 6.361811709052146
Experience 8, Iter 60, disc loss: 0.0019026938537995758, policy loss: 6.883254652932233
Experience 8, Iter 61, disc loss: 0.002497345160617409, policy loss: 6.430400939846129
Experience 8, Iter 62, disc loss: 0.0023672788236999633, policy loss: 6.62976019845613
Experience 8, Iter 63, disc loss: 0.0024247607866835015, policy loss: 6.515885458385636
Experience 8, Iter 64, disc loss: 0.0021730048572847316, policy loss: 6.663642305031833
Experience 8, Iter 65, disc loss: 0.0023325219509361616, policy loss: 6.7135420436073066
Experience 8, Iter 66, disc loss: 0.0022387195597741998, policy loss: 6.705358861304412
Experience 8, Iter 67, disc loss: 0.0020512389118472465, policy loss: 6.910715881560292
Experience 8, Iter 68, disc loss: 0.002016439665101179, policy loss: 7.023421899390769
Experience 8, Iter 69, disc loss: 0.0018797565788097694, policy loss: 6.929215325687878
Experience 8, Iter 70, disc loss: 0.002913288154055406, policy loss: 6.367416565455001
Experience 8, Iter 71, disc loss: 0.002352604896708608, policy loss: 6.493864462742697
Experience 8, Iter 72, disc loss: 0.0021845287244054903, policy loss: 6.735458070466487
Experience 8, Iter 73, disc loss: 0.002409408290561661, policy loss: 6.440305087982008
Experience 8, Iter 74, disc loss: 0.0016595919354953025, policy loss: 7.089199227296719
Experience 8, Iter 75, disc loss: 0.0022496805202917585, policy loss: 6.6203620836656825
Experience 8, Iter 76, disc loss: 0.0017361187778736895, policy loss: 6.93766667050431
Experience 8, Iter 77, disc loss: 0.0021411208935444427, policy loss: 6.7707648254347195
Experience 8, Iter 78, disc loss: 0.002221970717566733, policy loss: 6.827789128360077
Experience 8, Iter 79, disc loss: 0.002350056342937561, policy loss: 6.671983581470862
Experience 8, Iter 80, disc loss: 0.0021102700173891544, policy loss: 6.844706733761797
Experience 8, Iter 81, disc loss: 0.0019727136557941556, policy loss: 7.016142417707823
Experience 8, Iter 82, disc loss: 0.001736485417176375, policy loss: 6.923202859982187
Experience 8, Iter 83, disc loss: 0.0021746205785138817, policy loss: 6.741334466639698
Experience 8, Iter 84, disc loss: 0.001986909406217546, policy loss: 6.814484871459
Experience 8, Iter 85, disc loss: 0.0017453963929857119, policy loss: 6.838905317946357
Experience 8, Iter 86, disc loss: 0.0018459794263788682, policy loss: 6.671252597348435
Experience 8, Iter 87, disc loss: 0.0016480882649449404, policy loss: 7.083933786670488
Experience 8, Iter 88, disc loss: 0.001720605523310839, policy loss: 7.024315662103284
Experience 8, Iter 89, disc loss: 0.0020819802046630133, policy loss: 6.773653466104371
Experience 8, Iter 90, disc loss: 0.0016609693460397693, policy loss: 7.171957001671844
Experience 8, Iter 91, disc loss: 0.0019720321913420337, policy loss: 6.934851876320991
Experience 8, Iter 92, disc loss: 0.0018968416458192156, policy loss: 6.9510084663447405
Experience 8, Iter 93, disc loss: 0.001661714292494043, policy loss: 7.053028924297643
Experience 8, Iter 94, disc loss: 0.0017291758556135387, policy loss: 6.909565951637768
Experience 8, Iter 95, disc loss: 0.0018185209319912835, policy loss: 7.038643467516566
Experience 8, Iter 96, disc loss: 0.0015106506689388285, policy loss: 7.011871192926483
Experience 8, Iter 97, disc loss: 0.0015749110140503798, policy loss: 6.966458333578723
Experience 8, Iter 98, disc loss: 0.0016617024146837944, policy loss: 6.917726013720497
Experience 8, Iter 99, disc loss: 0.0015092149939493226, policy loss: 7.01429742142994
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0087],
        [0.0865],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.8392e-03, 3.6207e-02, 4.0998e-02, 1.9560e-03, 1.7706e-05,
          2.1470e-01]],

        [[8.8392e-03, 3.6207e-02, 4.0998e-02, 1.9560e-03, 1.7706e-05,
          2.1470e-01]],

        [[8.8392e-03, 3.6207e-02, 4.0998e-02, 1.9560e-03, 1.7706e-05,
          2.1470e-01]],

        [[8.8392e-03, 3.6207e-02, 4.0998e-02, 1.9560e-03, 1.7706e-05,
          2.1470e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0040, 0.0348, 0.3458, 0.0036], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0040, 0.0348, 0.3458, 0.0036])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.023
Iter 2/2000 - Loss: -0.247
Iter 3/2000 - Loss: -1.983
Iter 4/2000 - Loss: -1.737
Iter 5/2000 - Loss: -1.134
Iter 6/2000 - Loss: -1.377
Iter 7/2000 - Loss: -1.849
Iter 8/2000 - Loss: -2.017
Iter 9/2000 - Loss: -1.871
Iter 10/2000 - Loss: -1.679
Iter 11/2000 - Loss: -1.644
Iter 12/2000 - Loss: -1.759
Iter 13/2000 - Loss: -1.902
Iter 14/2000 - Loss: -1.975
Iter 15/2000 - Loss: -1.962
Iter 16/2000 - Loss: -1.917
Iter 17/2000 - Loss: -1.902
Iter 18/2000 - Loss: -1.939
Iter 19/2000 - Loss: -2.003
Iter 20/2000 - Loss: -2.059
Iter 1981/2000 - Loss: -8.560
Iter 1982/2000 - Loss: -8.560
Iter 1983/2000 - Loss: -8.560
Iter 1984/2000 - Loss: -8.560
Iter 1985/2000 - Loss: -8.560
Iter 1986/2000 - Loss: -8.560
Iter 1987/2000 - Loss: -8.560
Iter 1988/2000 - Loss: -8.560
Iter 1989/2000 - Loss: -8.560
Iter 1990/2000 - Loss: -8.561
Iter 1991/2000 - Loss: -8.561
Iter 1992/2000 - Loss: -8.561
Iter 1993/2000 - Loss: -8.561
Iter 1994/2000 - Loss: -8.561
Iter 1995/2000 - Loss: -8.561
Iter 1996/2000 - Loss: -8.561
Iter 1997/2000 - Loss: -8.561
Iter 1998/2000 - Loss: -8.561
Iter 1999/2000 - Loss: -8.561
Iter 2000/2000 - Loss: -8.561
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[21.4833,  4.0142, 36.8844, 16.4887,  2.9079, 26.7268]],

        [[29.2458, 35.8461, 35.0920,  3.1676,  6.8636, 16.3491]],

        [[16.5288, 38.4357, 26.0358,  1.8595,  5.1904, 22.2711]],

        [[29.2788, 53.8926,  8.3418,  2.4550,  5.3735, 26.2821]]])
Signal Variance: tensor([ 0.0427,  1.3454, 19.6477,  0.1907])
Estimated target variance: tensor([0.0040, 0.0348, 0.3458, 0.0036])
N: 90
Signal to noise ratio: tensor([12.2454, 61.7871, 84.1333, 25.5706])
Bound on condition number: tensor([ 13496.4526, 343588.6377, 637058.2146,  58848.1430])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0013969497774258966, policy loss: 7.299317466880739
Experience 9, Iter 1, disc loss: 0.001407551486424277, policy loss: 7.203243940833762
Experience 9, Iter 2, disc loss: 0.001240291159151988, policy loss: 7.126909078978519
Experience 9, Iter 3, disc loss: 0.0013039586831575156, policy loss: 7.247368893259888
Experience 9, Iter 4, disc loss: 0.0014620206818316763, policy loss: 7.206878293733713
Experience 9, Iter 5, disc loss: 0.0011099929984584235, policy loss: 7.368452214727838
Experience 9, Iter 6, disc loss: 0.0012456683555553058, policy loss: 7.138586910759045
Experience 9, Iter 7, disc loss: 0.0011957092633058269, policy loss: 7.323592538237712
Experience 9, Iter 8, disc loss: 0.0012610700170594812, policy loss: 7.102588771433143
Experience 9, Iter 9, disc loss: 0.001195167386869227, policy loss: 7.374911916089067
Experience 9, Iter 10, disc loss: 0.001160880130923418, policy loss: 7.349146907035356
Experience 9, Iter 11, disc loss: 0.0013851870654860974, policy loss: 6.919135376657349
Experience 9, Iter 12, disc loss: 0.001425443931085711, policy loss: 7.282478485572694
Experience 9, Iter 13, disc loss: 0.0014553045323085667, policy loss: 7.052215478429309
Experience 9, Iter 14, disc loss: 0.0013256685139474702, policy loss: 7.145569561100971
Experience 9, Iter 15, disc loss: 0.0013536846755367394, policy loss: 7.163044365837565
Experience 9, Iter 16, disc loss: 0.0011829078257773332, policy loss: 7.341778050714694
Experience 9, Iter 17, disc loss: 0.0012174987040718705, policy loss: 7.372322786124699
Experience 9, Iter 18, disc loss: 0.0011139986558492817, policy loss: 7.564712803798873
Experience 9, Iter 19, disc loss: 0.0012230509980403319, policy loss: 7.165281216854346
Experience 9, Iter 20, disc loss: 0.0013890514523835957, policy loss: 7.362688055424822
Experience 9, Iter 21, disc loss: 0.0014219864081562964, policy loss: 7.320182433209967
Experience 9, Iter 22, disc loss: 0.0012314717566672844, policy loss: 7.249016614951033
Experience 9, Iter 23, disc loss: 0.0011221770641139156, policy loss: 7.65060512714717
Experience 9, Iter 24, disc loss: 0.0014528825363938778, policy loss: 7.007222210268566
Experience 9, Iter 25, disc loss: 0.001147497124243269, policy loss: 7.2324244404598375
Experience 9, Iter 26, disc loss: 0.0012043665738483017, policy loss: 7.184457194687251
Experience 9, Iter 27, disc loss: 0.0010669535580497987, policy loss: 7.357204131375728
Experience 9, Iter 28, disc loss: 0.0011441129728512426, policy loss: 7.33474147958257
Experience 9, Iter 29, disc loss: 0.0009672661718395554, policy loss: 7.667040728818025
Experience 9, Iter 30, disc loss: 0.0011286322496903028, policy loss: 7.296646201312735
Experience 9, Iter 31, disc loss: 0.0013912998264904038, policy loss: 6.9979895129702205
Experience 9, Iter 32, disc loss: 0.001228683187327432, policy loss: 7.225288675425983
Experience 9, Iter 33, disc loss: 0.0010603896951680283, policy loss: 7.229258726366692
Experience 9, Iter 34, disc loss: 0.001180387146089573, policy loss: 7.581654653646458
Experience 9, Iter 35, disc loss: 0.0010552302013341077, policy loss: 7.43656731862566
Experience 9, Iter 36, disc loss: 0.0009905060385790164, policy loss: 7.698090873710417
Experience 9, Iter 37, disc loss: 0.0009621351090679309, policy loss: 7.433747529517594
Experience 9, Iter 38, disc loss: 0.0010849209752921802, policy loss: 7.375947146569187
Experience 9, Iter 39, disc loss: 0.0010999587506365074, policy loss: 7.370270573436825
Experience 9, Iter 40, disc loss: 0.0011268144522112427, policy loss: 7.294729583645111
Experience 9, Iter 41, disc loss: 0.0010615118599910326, policy loss: 7.389547598469056
Experience 9, Iter 42, disc loss: 0.001093997259446743, policy loss: 7.3524196772295625
Experience 9, Iter 43, disc loss: 0.0009306720814989429, policy loss: 7.488613244856138
Experience 9, Iter 44, disc loss: 0.001015917799205116, policy loss: 7.37964317104322
Experience 9, Iter 45, disc loss: 0.0009617377816361424, policy loss: 7.492535177634162
Experience 9, Iter 46, disc loss: 0.0011217128076712664, policy loss: 7.367813216879345
Experience 9, Iter 47, disc loss: 0.000963787822382804, policy loss: 7.437625214371003
Experience 9, Iter 48, disc loss: 0.0011918201395534147, policy loss: 7.19946956936102
Experience 9, Iter 49, disc loss: 0.0011182335644021938, policy loss: 7.577331461722089
Experience 9, Iter 50, disc loss: 0.0010240277342863063, policy loss: 7.362872731768381
Experience 9, Iter 51, disc loss: 0.0008523994919331002, policy loss: 7.7236060930272945
Experience 9, Iter 52, disc loss: 0.0010529847298880034, policy loss: 7.482269316578199
Experience 9, Iter 53, disc loss: 0.0012260349653989517, policy loss: 7.320705949363582
Experience 9, Iter 54, disc loss: 0.0011578180122679654, policy loss: 7.336525918382106
Experience 9, Iter 55, disc loss: 0.0010311764523822787, policy loss: 7.414229601427751
Experience 9, Iter 56, disc loss: 0.0008114945651099272, policy loss: 7.639063105713799
Experience 9, Iter 57, disc loss: 0.0009552038072717634, policy loss: 7.508950283287463
Experience 9, Iter 58, disc loss: 0.0009908768540342543, policy loss: 7.4608181128506015
Experience 9, Iter 59, disc loss: 0.0010228837749694823, policy loss: 7.315554085118956
Experience 9, Iter 60, disc loss: 0.001095414941055854, policy loss: 7.173837012638103
Experience 9, Iter 61, disc loss: 0.0010869041723066272, policy loss: 7.39366844162629
Experience 9, Iter 62, disc loss: 0.0010946092824920838, policy loss: 7.467201053421006
Experience 9, Iter 63, disc loss: 0.0009251476655479113, policy loss: 7.580237648124877
Experience 9, Iter 64, disc loss: 0.0009881716789846586, policy loss: 7.4200410484648875
Experience 9, Iter 65, disc loss: 0.00087826238622296, policy loss: 7.567941891403551
Experience 9, Iter 66, disc loss: 0.0009110135526582281, policy loss: 7.470724274657526
Experience 9, Iter 67, disc loss: 0.0010006962752543093, policy loss: 7.684736883324408
Experience 9, Iter 68, disc loss: 0.0010943571195323521, policy loss: 7.261299672640166
Experience 9, Iter 69, disc loss: 0.0009157052700770146, policy loss: 7.7175820958914905
Experience 9, Iter 70, disc loss: 0.0011850213821192755, policy loss: 7.391854132993476
Experience 9, Iter 71, disc loss: 0.0010007441631936678, policy loss: 7.370506409536995
Experience 9, Iter 72, disc loss: 0.0011205227845670497, policy loss: 7.20048819501153
Experience 9, Iter 73, disc loss: 0.000923864962586974, policy loss: 7.5129776832349355
Experience 9, Iter 74, disc loss: 0.0009733651042379115, policy loss: 7.521273188553656
Experience 9, Iter 75, disc loss: 0.0009594865087857862, policy loss: 7.433400193589929
Experience 9, Iter 76, disc loss: 0.0011401100816209053, policy loss: 7.252713636781786
Experience 9, Iter 77, disc loss: 0.0011217189323483971, policy loss: 7.221103434750738
Experience 9, Iter 78, disc loss: 0.0011523915275825957, policy loss: 7.459592616565525
Experience 9, Iter 79, disc loss: 0.0009158797055271308, policy loss: 7.7526429409389195
Experience 9, Iter 80, disc loss: 0.0008654360948173328, policy loss: 7.827080715804128
Experience 9, Iter 81, disc loss: 0.001037589998458354, policy loss: 7.4890550521456865
Experience 9, Iter 82, disc loss: 0.001173916345855761, policy loss: 7.3177751798820925
Experience 9, Iter 83, disc loss: 0.0010074873902771858, policy loss: 7.5489755801278555
Experience 9, Iter 84, disc loss: 0.0009623809316436778, policy loss: 7.759279477783451
Experience 9, Iter 85, disc loss: 0.0007531110478535827, policy loss: 8.203962859540448
Experience 9, Iter 86, disc loss: 0.000799539606492536, policy loss: 7.988779990973688
Experience 9, Iter 87, disc loss: 0.0009834138503267142, policy loss: 7.513591832396916
Experience 9, Iter 88, disc loss: 0.001071398617049738, policy loss: 7.393019697588169
Experience 9, Iter 89, disc loss: 0.0009569127398072803, policy loss: 7.627233469803064
Experience 9, Iter 90, disc loss: 0.001144466153046157, policy loss: 7.413258778372108
Experience 9, Iter 91, disc loss: 0.0009819672871842985, policy loss: 7.625300558284337
Experience 9, Iter 92, disc loss: 0.0011051167328609097, policy loss: 7.593674042256518
Experience 9, Iter 93, disc loss: 0.0009382349237128069, policy loss: 7.705027639706069
Experience 9, Iter 94, disc loss: 0.0010152822683307267, policy loss: 7.652852831690016
Experience 9, Iter 95, disc loss: 0.0010144872271733705, policy loss: 7.5518996731040255
Experience 9, Iter 96, disc loss: 0.0008491647007394831, policy loss: 7.807450068111437
Experience 9, Iter 97, disc loss: 0.0009143075613703773, policy loss: 7.614927748060049
Experience 9, Iter 98, disc loss: 0.0009750773996155487, policy loss: 7.667304737188248
Experience 9, Iter 99, disc loss: 0.0007110438134378554, policy loss: 8.041068523711461
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0099],
        [0.0944],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.6097e-03, 3.8323e-02, 3.9997e-02, 2.0009e-03, 1.6123e-05,
          2.4314e-01]],

        [[8.6097e-03, 3.8323e-02, 3.9997e-02, 2.0009e-03, 1.6123e-05,
          2.4314e-01]],

        [[8.6097e-03, 3.8323e-02, 3.9997e-02, 2.0009e-03, 1.6123e-05,
          2.4314e-01]],

        [[8.6097e-03, 3.8323e-02, 3.9997e-02, 2.0009e-03, 1.6123e-05,
          2.4314e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0395, 0.3777, 0.0034], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0395, 0.3777, 0.0034])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.957
Iter 2/2000 - Loss: 0.070
Iter 3/2000 - Loss: -1.881
Iter 4/2000 - Loss: -1.586
Iter 5/2000 - Loss: -0.906
Iter 6/2000 - Loss: -1.156
Iter 7/2000 - Loss: -1.690
Iter 8/2000 - Loss: -1.919
Iter 9/2000 - Loss: -1.784
Iter 10/2000 - Loss: -1.552
Iter 11/2000 - Loss: -1.469
Iter 12/2000 - Loss: -1.572
Iter 13/2000 - Loss: -1.742
Iter 14/2000 - Loss: -1.856
Iter 15/2000 - Loss: -1.864
Iter 16/2000 - Loss: -1.804
Iter 17/2000 - Loss: -1.751
Iter 18/2000 - Loss: -1.761
Iter 19/2000 - Loss: -1.831
Iter 20/2000 - Loss: -1.917
Iter 1981/2000 - Loss: -8.725
Iter 1982/2000 - Loss: -8.725
Iter 1983/2000 - Loss: -8.725
Iter 1984/2000 - Loss: -8.725
Iter 1985/2000 - Loss: -8.725
Iter 1986/2000 - Loss: -8.725
Iter 1987/2000 - Loss: -8.725
Iter 1988/2000 - Loss: -8.725
Iter 1989/2000 - Loss: -8.725
Iter 1990/2000 - Loss: -8.725
Iter 1991/2000 - Loss: -8.725
Iter 1992/2000 - Loss: -8.725
Iter 1993/2000 - Loss: -8.725
Iter 1994/2000 - Loss: -8.725
Iter 1995/2000 - Loss: -8.725
Iter 1996/2000 - Loss: -8.725
Iter 1997/2000 - Loss: -8.726
Iter 1998/2000 - Loss: -8.726
Iter 1999/2000 - Loss: -8.726
Iter 2000/2000 - Loss: -8.726
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[22.0257,  4.2320, 35.5344, 15.1557,  3.5137, 27.0431]],

        [[28.0432, 34.7828, 32.9226,  3.0643,  6.3223, 15.6226]],

        [[20.7795, 39.4747, 25.6006,  1.7927,  4.6316, 22.0410]],

        [[28.6819, 53.0143,  8.4709,  2.4024,  4.8929, 26.1419]]])
Signal Variance: tensor([ 0.0455,  1.2073, 18.5298,  0.1833])
Estimated target variance: tensor([0.0042, 0.0395, 0.3777, 0.0034])
N: 100
Signal to noise ratio: tensor([12.6607, 60.0040, 85.8298, 25.8862])
Bound on condition number: tensor([ 16030.3194, 360048.8634, 736675.9635,  67010.3245])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.0008805140197149723, policy loss: 7.947151734174768
Experience 10, Iter 1, disc loss: 0.000981893418142238, policy loss: 7.494823293895039
Experience 10, Iter 2, disc loss: 0.0009435114620237661, policy loss: 7.535119578303149
Experience 10, Iter 3, disc loss: 0.0008624084217615289, policy loss: 7.6200713519090595
Experience 10, Iter 4, disc loss: 0.0010447838797093547, policy loss: 7.279957627202042
Experience 10, Iter 5, disc loss: 0.0009823196309989798, policy loss: 7.3936668933651735
Experience 10, Iter 6, disc loss: 0.0011887521209635998, policy loss: 7.259620874637074
Experience 10, Iter 7, disc loss: 0.0009169765568699162, policy loss: 7.809174688242721
Experience 10, Iter 8, disc loss: 0.0008878214276366077, policy loss: 7.5503543304153435
Experience 10, Iter 9, disc loss: 0.0009527794206612378, policy loss: 7.501996112031579
Experience 10, Iter 10, disc loss: 0.0009367982345774477, policy loss: 7.669498900748621
Experience 10, Iter 11, disc loss: 0.0010150415147691387, policy loss: 7.68619115069304
Experience 10, Iter 12, disc loss: 0.0009943891128459156, policy loss: 7.39923014235747
Experience 10, Iter 13, disc loss: 0.0008890893868975445, policy loss: 7.563710090299624
Experience 10, Iter 14, disc loss: 0.0010319477508900193, policy loss: 7.485854905662034
Experience 10, Iter 15, disc loss: 0.0009285141710127847, policy loss: 7.577480504093332
Experience 10, Iter 16, disc loss: 0.0009710171201184801, policy loss: 7.701257899193029
Experience 10, Iter 17, disc loss: 0.0011276064901893706, policy loss: 7.497223737546664
Experience 10, Iter 18, disc loss: 0.0009151576022515661, policy loss: 7.865440412145179
Experience 10, Iter 19, disc loss: 0.0009350778305036573, policy loss: 7.772363913169892
Experience 10, Iter 20, disc loss: 0.0008283083564938754, policy loss: 7.607486832866506
Experience 10, Iter 21, disc loss: 0.0009490246715446102, policy loss: 7.752874052589244
Experience 10, Iter 22, disc loss: 0.0009509499290017104, policy loss: 7.394552974655132
Experience 10, Iter 23, disc loss: 0.0008420665905217072, policy loss: 7.675664202198669
Experience 10, Iter 24, disc loss: 0.000888681857954347, policy loss: 7.474122131129547
Experience 10, Iter 25, disc loss: 0.000899805299351961, policy loss: 7.564763666248208
Experience 10, Iter 26, disc loss: 0.000892114974041564, policy loss: 7.657752697257385
Experience 10, Iter 27, disc loss: 0.0009659225777559651, policy loss: 7.761820448144683
Experience 10, Iter 28, disc loss: 0.000841000158457082, policy loss: 7.611667549249765
Experience 10, Iter 29, disc loss: 0.0007946128832108423, policy loss: 7.853844017628606
Experience 10, Iter 30, disc loss: 0.0009422408647978329, policy loss: 7.745108059431516
Experience 10, Iter 31, disc loss: 0.0008533753820026968, policy loss: 7.781904556555075
Experience 10, Iter 32, disc loss: 0.0008773082479778595, policy loss: 7.75995848304292
Experience 10, Iter 33, disc loss: 0.0007596862927252676, policy loss: 7.875154621746161
Experience 10, Iter 34, disc loss: 0.0008120996787100911, policy loss: 7.659762613263784
Experience 10, Iter 35, disc loss: 0.0008226969786175402, policy loss: 7.6160757662794785
Experience 10, Iter 36, disc loss: 0.0008387290905347411, policy loss: 7.58109163025701
Experience 10, Iter 37, disc loss: 0.0007738832353400372, policy loss: 7.888101694028869
Experience 10, Iter 38, disc loss: 0.000777296900124482, policy loss: 7.794516435841281
Experience 10, Iter 39, disc loss: 0.0007753976692010614, policy loss: 7.660800548341111
Experience 10, Iter 40, disc loss: 0.0007393314803767827, policy loss: 7.674334695403631
Experience 10, Iter 41, disc loss: 0.0008681786052461371, policy loss: 7.803141285152578
Experience 10, Iter 42, disc loss: 0.0008332760107275859, policy loss: 7.513400037158398
Experience 10, Iter 43, disc loss: 0.0009630220095253002, policy loss: 7.474111042773208
Experience 10, Iter 44, disc loss: 0.0006775447420033656, policy loss: 8.053973735852185
Experience 10, Iter 45, disc loss: 0.0006859956830110087, policy loss: 7.817972731358537
Experience 10, Iter 46, disc loss: 0.000905369761075745, policy loss: 7.6839380148334495
Experience 10, Iter 47, disc loss: 0.000723974019165633, policy loss: 7.881993068059449
Experience 10, Iter 48, disc loss: 0.0007604135427778584, policy loss: 7.719642942224907
Experience 10, Iter 49, disc loss: 0.0008359975025856818, policy loss: 7.6157675848186805
Experience 10, Iter 50, disc loss: 0.0008053609953071376, policy loss: 7.535739826216798
Experience 10, Iter 51, disc loss: 0.0007496411774730843, policy loss: 7.714104971378525
Experience 10, Iter 52, disc loss: 0.0006951853205603453, policy loss: 7.888455152350905
Experience 10, Iter 53, disc loss: 0.0007551663957232287, policy loss: 7.762183695172196
Experience 10, Iter 54, disc loss: 0.0007057268275255757, policy loss: 7.8889360187211155
Experience 10, Iter 55, disc loss: 0.0007049096403268023, policy loss: 8.075318102064948
Experience 10, Iter 56, disc loss: 0.0007077069868615672, policy loss: 8.127043781247542
Experience 10, Iter 57, disc loss: 0.0007206946653789992, policy loss: 7.828953514917443
Experience 10, Iter 58, disc loss: 0.0006967387110351658, policy loss: 7.941473215267261
Experience 10, Iter 59, disc loss: 0.0006891807736704363, policy loss: 7.938836318437843
Experience 10, Iter 60, disc loss: 0.000535618942583932, policy loss: 8.354933490044125
Experience 10, Iter 61, disc loss: 0.0006222408845151227, policy loss: 8.138594584169171
Experience 10, Iter 62, disc loss: 0.0006850567745235082, policy loss: 7.826127762570511
Experience 10, Iter 63, disc loss: 0.0005794975713234871, policy loss: 8.15460130189772
Experience 10, Iter 64, disc loss: 0.0007375490841242011, policy loss: 7.765994516432659
Experience 10, Iter 65, disc loss: 0.0006614693824120683, policy loss: 8.00382625767666
Experience 10, Iter 66, disc loss: 0.0006955853663032024, policy loss: 7.853146454487358
Experience 10, Iter 67, disc loss: 0.000614832448560907, policy loss: 7.987850665881982
Experience 10, Iter 68, disc loss: 0.0006264172915924691, policy loss: 8.036648294757244
Experience 10, Iter 69, disc loss: 0.000815935910185376, policy loss: 7.951795097913109
Experience 10, Iter 70, disc loss: 0.0007049149915742986, policy loss: 7.812782338544302
Experience 10, Iter 71, disc loss: 0.0008526180981316574, policy loss: 7.685058489752429
Experience 10, Iter 72, disc loss: 0.0006816908110562729, policy loss: 7.784319563671333
Experience 10, Iter 73, disc loss: 0.0006536304818363664, policy loss: 8.10434030994751
Experience 10, Iter 74, disc loss: 0.0006644226374119494, policy loss: 7.964171508279338
Experience 10, Iter 75, disc loss: 0.0008827534657949459, policy loss: 7.678855108323617
Experience 10, Iter 76, disc loss: 0.000849776548049718, policy loss: 7.683043703805043
Experience 10, Iter 77, disc loss: 0.0008375671041235544, policy loss: 7.900884816768688
Experience 10, Iter 78, disc loss: 0.0007385428219112508, policy loss: 7.96241269087313
Experience 10, Iter 79, disc loss: 0.0006427274123189813, policy loss: 8.347384752205752
Experience 10, Iter 80, disc loss: 0.0007169911036325838, policy loss: 8.22908243065132
Experience 10, Iter 81, disc loss: 0.0006947919944901605, policy loss: 8.429812029946712
Experience 10, Iter 82, disc loss: 0.0007758620060227911, policy loss: 7.9032820786166305
Experience 10, Iter 83, disc loss: 0.0006653321390113989, policy loss: 8.188883990401248
Experience 10, Iter 84, disc loss: 0.0007691988049747522, policy loss: 7.805118145087608
Experience 10, Iter 85, disc loss: 0.000733180883313935, policy loss: 7.8433347027525535
Experience 10, Iter 86, disc loss: 0.0006938084588054317, policy loss: 7.859098635454034
Experience 10, Iter 87, disc loss: 0.0008053609841535326, policy loss: 7.542647799692141
Experience 10, Iter 88, disc loss: 0.000615836644255033, policy loss: 7.901296673924106
Experience 10, Iter 89, disc loss: 0.0006520308843954599, policy loss: 8.190706007424664
Experience 10, Iter 90, disc loss: 0.0006621207682127313, policy loss: 7.918056688635044
Experience 10, Iter 91, disc loss: 0.0006122788202176748, policy loss: 8.118916382378025
Experience 10, Iter 92, disc loss: 0.0005307162538143795, policy loss: 8.240445715656582
Experience 10, Iter 93, disc loss: 0.0007278434154376864, policy loss: 7.850196460457956
Experience 10, Iter 94, disc loss: 0.0005362937027384506, policy loss: 8.370447662155375
Experience 10, Iter 95, disc loss: 0.0006811904164574004, policy loss: 8.031692197456607
Experience 10, Iter 96, disc loss: 0.0006366382943162012, policy loss: 8.029901680121213
Experience 10, Iter 97, disc loss: 0.0006844575532953543, policy loss: 7.936186917995065
Experience 10, Iter 98, disc loss: 0.0006842989755271336, policy loss: 7.973565387048084
Experience 10, Iter 99, disc loss: 0.0005509163023210455, policy loss: 8.064324929794632
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0126],
        [0.1238],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.8531e-03, 3.6662e-02, 4.3169e-02, 1.9279e-03, 1.4902e-05,
          2.8798e-01]],

        [[7.8531e-03, 3.6662e-02, 4.3169e-02, 1.9279e-03, 1.4902e-05,
          2.8798e-01]],

        [[7.8531e-03, 3.6662e-02, 4.3169e-02, 1.9279e-03, 1.4902e-05,
          2.8798e-01]],

        [[7.8531e-03, 3.6662e-02, 4.3169e-02, 1.9279e-03, 1.4902e-05,
          2.8798e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0040, 0.0503, 0.4951, 0.0033], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0040, 0.0503, 0.4951, 0.0033])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.743
Iter 2/2000 - Loss: 0.382
Iter 3/2000 - Loss: -1.657
Iter 4/2000 - Loss: -1.346
Iter 5/2000 - Loss: -0.634
Iter 6/2000 - Loss: -0.890
Iter 7/2000 - Loss: -1.451
Iter 8/2000 - Loss: -1.702
Iter 9/2000 - Loss: -1.566
Iter 10/2000 - Loss: -1.312
Iter 11/2000 - Loss: -1.211
Iter 12/2000 - Loss: -1.315
Iter 13/2000 - Loss: -1.503
Iter 14/2000 - Loss: -1.633
Iter 15/2000 - Loss: -1.645
Iter 16/2000 - Loss: -1.571
Iter 17/2000 - Loss: -1.497
Iter 18/2000 - Loss: -1.495
Iter 19/2000 - Loss: -1.571
Iter 20/2000 - Loss: -1.677
Iter 1981/2000 - Loss: -8.759
Iter 1982/2000 - Loss: -8.760
Iter 1983/2000 - Loss: -8.760
Iter 1984/2000 - Loss: -8.760
Iter 1985/2000 - Loss: -8.760
Iter 1986/2000 - Loss: -8.760
Iter 1987/2000 - Loss: -8.760
Iter 1988/2000 - Loss: -8.760
Iter 1989/2000 - Loss: -8.760
Iter 1990/2000 - Loss: -8.760
Iter 1991/2000 - Loss: -8.760
Iter 1992/2000 - Loss: -8.760
Iter 1993/2000 - Loss: -8.760
Iter 1994/2000 - Loss: -8.760
Iter 1995/2000 - Loss: -8.760
Iter 1996/2000 - Loss: -8.760
Iter 1997/2000 - Loss: -8.760
Iter 1998/2000 - Loss: -8.760
Iter 1999/2000 - Loss: -8.760
Iter 2000/2000 - Loss: -8.760
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[20.4558,  4.1341, 36.0111, 12.4789,  3.1708, 29.5819]],

        [[25.0097, 35.4165, 32.8086,  2.8799,  5.3819, 15.8611]],

        [[21.9239, 39.9792, 26.9810,  1.8475,  4.5253, 24.1315]],

        [[26.6973, 50.0284,  8.4372,  2.3788,  4.7380, 23.7514]]])
Signal Variance: tensor([ 0.0442,  1.1677, 19.9121,  0.1734])
Estimated target variance: tensor([0.0040, 0.0503, 0.4951, 0.0033])
N: 110
Signal to noise ratio: tensor([12.4974, 56.9281, 89.9777, 25.5185])
Bound on condition number: tensor([ 17181.3960, 356489.6950, 890559.1735,  71632.3894])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0005491939540009501, policy loss: 8.17158086419995
Experience 11, Iter 1, disc loss: 0.0007044474210761561, policy loss: 7.972807476574053
Experience 11, Iter 2, disc loss: 0.000785008624197562, policy loss: 7.811384306668472
Experience 11, Iter 3, disc loss: 0.0007260231181708248, policy loss: 7.920762211068662
Experience 11, Iter 4, disc loss: 0.0006634709789160524, policy loss: 7.7880509793635815
Experience 11, Iter 5, disc loss: 0.0006180301805035823, policy loss: 8.097052085544714
Experience 11, Iter 6, disc loss: 0.0006473267394677719, policy loss: 7.9923057220086875
Experience 11, Iter 7, disc loss: 0.0005439059694173656, policy loss: 8.122335594301113
Experience 11, Iter 8, disc loss: 0.0005616238325333323, policy loss: 8.038851281065197
Experience 11, Iter 9, disc loss: 0.0005508533719638235, policy loss: 8.068484339995273
Experience 11, Iter 10, disc loss: 0.0006585401679342697, policy loss: 7.93737578268974
Experience 11, Iter 11, disc loss: 0.0005696575756970328, policy loss: 8.230810773958757
Experience 11, Iter 12, disc loss: 0.000521668981761326, policy loss: 8.144794940113503
Experience 11, Iter 13, disc loss: 0.0009280014562223264, policy loss: 7.872891372964583
Experience 11, Iter 14, disc loss: 0.0006352692762964606, policy loss: 7.818019349562748
Experience 11, Iter 15, disc loss: 0.00049739884686444, policy loss: 8.237718548710454
Experience 11, Iter 16, disc loss: 0.0005300617209343665, policy loss: 8.360365223828119
Experience 11, Iter 17, disc loss: 0.0005695176796623957, policy loss: 8.115002867924863
Experience 11, Iter 18, disc loss: 0.000571480165136972, policy loss: 8.003559302506046
Experience 11, Iter 19, disc loss: 0.000501194016421588, policy loss: 8.157580602164694
Experience 11, Iter 20, disc loss: 0.0005808857293274911, policy loss: 8.33693393695077
Experience 11, Iter 21, disc loss: 0.0006122446040452422, policy loss: 7.939474992269153
Experience 11, Iter 22, disc loss: 0.00047017168351006785, policy loss: 8.30545237047144
Experience 11, Iter 23, disc loss: 0.0005735255645973116, policy loss: 8.106206927385218
Experience 11, Iter 24, disc loss: 0.0005773849105864641, policy loss: 7.981626540515104
Experience 11, Iter 25, disc loss: 0.0006542046659434122, policy loss: 7.85819622918717
Experience 11, Iter 26, disc loss: 0.0006540829846454697, policy loss: 7.946199559967054
Experience 11, Iter 27, disc loss: 0.0005685349655443216, policy loss: 8.129057460053275
Experience 11, Iter 28, disc loss: 0.0006524234762555267, policy loss: 8.117836922094995
Experience 11, Iter 29, disc loss: 0.0005247305638387424, policy loss: 8.528697447960946
Experience 11, Iter 30, disc loss: 0.0005645332145867859, policy loss: 8.245668606690195
Experience 11, Iter 31, disc loss: 0.000616287691884596, policy loss: 7.8271105712899285
Experience 11, Iter 32, disc loss: 0.00046995511620459405, policy loss: 8.524086750543047
Experience 11, Iter 33, disc loss: 0.0005072747462389497, policy loss: 8.418962164803945
Experience 11, Iter 34, disc loss: 0.0005403300805488492, policy loss: 8.06223341047318
Experience 11, Iter 35, disc loss: 0.0005610539312034367, policy loss: 8.038005955253745
Experience 11, Iter 36, disc loss: 0.00046549355342165506, policy loss: 8.331650892603282
Experience 11, Iter 37, disc loss: 0.0005390627126564457, policy loss: 8.103661933330475
Experience 11, Iter 38, disc loss: 0.00048123497393689617, policy loss: 8.208939038701265
Experience 11, Iter 39, disc loss: 0.0005035982261780125, policy loss: 8.030613948424197
Experience 11, Iter 40, disc loss: 0.00043691578862055077, policy loss: 8.567459877181467
Experience 11, Iter 41, disc loss: 0.0005872106002778662, policy loss: 8.081474453072795
Experience 11, Iter 42, disc loss: 0.0005209634153172928, policy loss: 8.355980775245595
Experience 11, Iter 43, disc loss: 0.0005617050808797595, policy loss: 7.883260754913467
Experience 11, Iter 44, disc loss: 0.000536970040801674, policy loss: 8.531514104516475
Experience 11, Iter 45, disc loss: 0.0007068970304140833, policy loss: 7.811485411235496
Experience 11, Iter 46, disc loss: 0.0006893592889833667, policy loss: 8.000839726897512
Experience 11, Iter 47, disc loss: 0.0007322184008644182, policy loss: 7.88009737189773
Experience 11, Iter 48, disc loss: 0.0005206158571218886, policy loss: 8.396712970321992
Experience 11, Iter 49, disc loss: 0.000556676744687205, policy loss: 8.327090689344374
Experience 11, Iter 50, disc loss: 0.0005962464134610626, policy loss: 8.033396110075056
Experience 11, Iter 51, disc loss: 0.000491044249098069, policy loss: 8.505065582944141
Experience 11, Iter 52, disc loss: 0.0005950388459777949, policy loss: 8.271819235471314
Experience 11, Iter 53, disc loss: 0.0004968591063238559, policy loss: 8.168775329509167
Experience 11, Iter 54, disc loss: 0.0005517503124565373, policy loss: 8.316979338360014
Experience 11, Iter 55, disc loss: 0.000569700520232315, policy loss: 8.188049287981418
Experience 11, Iter 56, disc loss: 0.0005598483766024591, policy loss: 8.029582272724909
Experience 11, Iter 57, disc loss: 0.0005924235607225826, policy loss: 8.351616871419662
Experience 11, Iter 58, disc loss: 0.000533060226140791, policy loss: 8.264190401141995
Experience 11, Iter 59, disc loss: 0.0005287649263547638, policy loss: 8.136908551575313
Experience 11, Iter 60, disc loss: 0.0005967236425681706, policy loss: 7.888544911584356
Experience 11, Iter 61, disc loss: 0.00045400568401506665, policy loss: 8.498997079943923
Experience 11, Iter 62, disc loss: 0.0005884814484334962, policy loss: 8.114777307690817
Experience 11, Iter 63, disc loss: 0.0005465507752022542, policy loss: 8.201264123362744
Experience 11, Iter 64, disc loss: 0.00048809482544107455, policy loss: 8.603889460722405
Experience 11, Iter 65, disc loss: 0.0005278114530269467, policy loss: 8.336912884791392
Experience 11, Iter 66, disc loss: 0.0005395755131126488, policy loss: 8.264668228036236
Experience 11, Iter 67, disc loss: 0.0005903589822340617, policy loss: 8.272289741373596
Experience 11, Iter 68, disc loss: 0.0005884189223813676, policy loss: 8.339029786004627
Experience 11, Iter 69, disc loss: 0.0005520665951610481, policy loss: 8.193844244730315
Experience 11, Iter 70, disc loss: 0.0005470449653083267, policy loss: 8.47469407714154
Experience 11, Iter 71, disc loss: 0.0005484239701492957, policy loss: 8.39661171375053
Experience 11, Iter 72, disc loss: 0.00042938053013452074, policy loss: 8.480107474501503
Experience 11, Iter 73, disc loss: 0.0005250032592051926, policy loss: 8.15522386204546
Experience 11, Iter 74, disc loss: 0.00048233295112115067, policy loss: 8.27155858947382
Experience 11, Iter 75, disc loss: 0.0004645215885005499, policy loss: 8.43546706755864
Experience 11, Iter 76, disc loss: 0.0005266832607540527, policy loss: 8.252651967910806
Experience 11, Iter 77, disc loss: 0.0006009665687276121, policy loss: 8.211673909352658
Experience 11, Iter 78, disc loss: 0.00045422544894284085, policy loss: 8.473143080879579
Experience 11, Iter 79, disc loss: 0.0005202191507813883, policy loss: 8.498826298337711
Experience 11, Iter 80, disc loss: 0.0005308833813681812, policy loss: 8.341388440810638
Experience 11, Iter 81, disc loss: 0.0005967202484260665, policy loss: 8.321434780075634
Experience 11, Iter 82, disc loss: 0.000573821984471557, policy loss: 8.348480638325801
Experience 11, Iter 83, disc loss: 0.0005280678694913652, policy loss: 8.569622793460564
Experience 11, Iter 84, disc loss: 0.0006460035054164301, policy loss: 8.312649021659787
Experience 11, Iter 85, disc loss: 0.0005331036282840183, policy loss: 8.31556147442432
Experience 11, Iter 86, disc loss: 0.0005800922526587789, policy loss: 8.307280410913824
Experience 11, Iter 87, disc loss: 0.00042393325133898915, policy loss: 8.537727163899032
Experience 11, Iter 88, disc loss: 0.0004055421982620523, policy loss: 8.452819919728343
Experience 11, Iter 89, disc loss: 0.0005572104570165434, policy loss: 8.260545156040367
Experience 11, Iter 90, disc loss: 0.0004868164125531143, policy loss: 8.23691478942886
Experience 11, Iter 91, disc loss: 0.0005432784527601228, policy loss: 8.238044199317383
Experience 11, Iter 92, disc loss: 0.0006515623014682786, policy loss: 7.971827273674051
Experience 11, Iter 93, disc loss: 0.00044981074375507405, policy loss: 8.768992977935298
Experience 11, Iter 94, disc loss: 0.0006089163527634127, policy loss: 8.152924907643103
Experience 11, Iter 95, disc loss: 0.000647476112569807, policy loss: 8.294071681507797
Experience 11, Iter 96, disc loss: 0.00045502221568893684, policy loss: 8.532887412486465
Experience 11, Iter 97, disc loss: 0.00044845811570344666, policy loss: 8.529137073562223
Experience 11, Iter 98, disc loss: 0.0005291600228971474, policy loss: 8.562060511966177
Experience 11, Iter 99, disc loss: 0.0004953332566220055, policy loss: 8.61860875767117
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0137],
        [0.1345],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.7686e-03, 3.4860e-02, 4.3618e-02, 1.8469e-03, 1.3933e-05,
          3.0214e-01]],

        [[7.7686e-03, 3.4860e-02, 4.3618e-02, 1.8469e-03, 1.3933e-05,
          3.0214e-01]],

        [[7.7686e-03, 3.4860e-02, 4.3618e-02, 1.8469e-03, 1.3933e-05,
          3.0214e-01]],

        [[7.7686e-03, 3.4860e-02, 4.3618e-02, 1.8469e-03, 1.3933e-05,
          3.0214e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0038, 0.0549, 0.5381, 0.0033], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0038, 0.0549, 0.5381, 0.0033])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.690
Iter 2/2000 - Loss: 0.507
Iter 3/2000 - Loss: -1.599
Iter 4/2000 - Loss: -1.278
Iter 5/2000 - Loss: -0.543
Iter 6/2000 - Loss: -0.803
Iter 7/2000 - Loss: -1.383
Iter 8/2000 - Loss: -1.648
Iter 9/2000 - Loss: -1.510
Iter 10/2000 - Loss: -1.244
Iter 11/2000 - Loss: -1.133
Iter 12/2000 - Loss: -1.237
Iter 13/2000 - Loss: -1.434
Iter 14/2000 - Loss: -1.574
Iter 15/2000 - Loss: -1.587
Iter 16/2000 - Loss: -1.506
Iter 17/2000 - Loss: -1.421
Iter 18/2000 - Loss: -1.412
Iter 19/2000 - Loss: -1.489
Iter 20/2000 - Loss: -1.602
Iter 1981/2000 - Loss: -8.846
Iter 1982/2000 - Loss: -8.846
Iter 1983/2000 - Loss: -8.846
Iter 1984/2000 - Loss: -8.846
Iter 1985/2000 - Loss: -8.846
Iter 1986/2000 - Loss: -8.846
Iter 1987/2000 - Loss: -8.846
Iter 1988/2000 - Loss: -8.846
Iter 1989/2000 - Loss: -8.846
Iter 1990/2000 - Loss: -8.846
Iter 1991/2000 - Loss: -8.846
Iter 1992/2000 - Loss: -8.846
Iter 1993/2000 - Loss: -8.846
Iter 1994/2000 - Loss: -8.846
Iter 1995/2000 - Loss: -8.846
Iter 1996/2000 - Loss: -8.846
Iter 1997/2000 - Loss: -8.846
Iter 1998/2000 - Loss: -8.846
Iter 1999/2000 - Loss: -8.846
Iter 2000/2000 - Loss: -8.846
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[19.5546,  4.1074, 34.7399, 10.6790,  3.2800, 30.4875]],

        [[23.2499, 33.3749, 32.5219,  2.8720,  5.1942, 15.6983]],

        [[22.1158, 39.3715, 26.3301,  1.7747,  4.3372, 23.5792]],

        [[25.0982, 47.0435,  8.1690,  2.3653,  4.9917, 21.7923]]])
Signal Variance: tensor([ 0.0444,  1.1335, 18.5082,  0.1598])
Estimated target variance: tensor([0.0038, 0.0549, 0.5381, 0.0033])
N: 120
Signal to noise ratio: tensor([12.6320, 57.0676, 88.0929, 24.5997])
Bound on condition number: tensor([ 19149.1334, 390806.1514, 931244.4508,  72618.4916])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.000495282540955096, policy loss: 8.250185179090773
Experience 12, Iter 1, disc loss: 0.0005263792593238223, policy loss: 8.282809839115817
Experience 12, Iter 2, disc loss: 0.0004520303847223587, policy loss: 8.452706551186168
Experience 12, Iter 3, disc loss: 0.00044417051910662696, policy loss: 8.4146857848849
Experience 12, Iter 4, disc loss: 0.0004075006064536764, policy loss: 8.734186900846353
Experience 12, Iter 5, disc loss: 0.0005868834060023134, policy loss: 8.207594710188571
Experience 12, Iter 6, disc loss: 0.0004867909258242871, policy loss: 8.55009757593406
Experience 12, Iter 7, disc loss: 0.0005856449893350691, policy loss: 7.958238986759747
Experience 12, Iter 8, disc loss: 0.0004946128637044164, policy loss: 8.42184164307775
Experience 12, Iter 9, disc loss: 0.0005379258991269862, policy loss: 8.24673478782892
Experience 12, Iter 10, disc loss: 0.0005273304430885986, policy loss: 8.2370196345204
Experience 12, Iter 11, disc loss: 0.00047898642836430294, policy loss: 8.25749253273443
Experience 12, Iter 12, disc loss: 0.00039989663439167397, policy loss: 8.633810104956499
Experience 12, Iter 13, disc loss: 0.0005975664267893121, policy loss: 8.343534684081146
Experience 12, Iter 14, disc loss: 0.0004178175099355493, policy loss: 8.755899736387514
Experience 12, Iter 15, disc loss: 0.0004264971516914414, policy loss: 8.53528554318003
Experience 12, Iter 16, disc loss: 0.0003933664855665537, policy loss: 8.74068532901919
Experience 12, Iter 17, disc loss: 0.0005032204650050954, policy loss: 8.339704637138372
Experience 12, Iter 18, disc loss: 0.0004641338305916502, policy loss: 8.42061554058219
Experience 12, Iter 19, disc loss: 0.0004451441407743235, policy loss: 8.436417468704072
Experience 12, Iter 20, disc loss: 0.0004509533004334334, policy loss: 8.357641626823074
Experience 12, Iter 21, disc loss: 0.00048292462052273305, policy loss: 8.327028155332556
Experience 12, Iter 22, disc loss: 0.000508381344360864, policy loss: 8.331375974721466
Experience 12, Iter 23, disc loss: 0.0006059474244271036, policy loss: 8.095779583950375
Experience 12, Iter 24, disc loss: 0.0004549641069305746, policy loss: 8.76999896304859
Experience 12, Iter 25, disc loss: 0.0003904808098369409, policy loss: 8.853161463941131
Experience 12, Iter 26, disc loss: 0.0003536050447170459, policy loss: 8.618083030047789
Experience 12, Iter 27, disc loss: 0.00047184677713039107, policy loss: 8.519073540149027
Experience 12, Iter 28, disc loss: 0.0003760084540175955, policy loss: 8.6046120176239
Experience 12, Iter 29, disc loss: 0.00039828654893189395, policy loss: 8.482196698901545
Experience 12, Iter 30, disc loss: 0.0005227399581775994, policy loss: 8.758561827777372
Experience 12, Iter 31, disc loss: 0.000454874009531503, policy loss: 8.748744596244086
Experience 12, Iter 32, disc loss: 0.000457517560958381, policy loss: 8.318423496446156
Experience 12, Iter 33, disc loss: 0.0004824618323125521, policy loss: 8.23036013899381
Experience 12, Iter 34, disc loss: 0.000505622635855025, policy loss: 8.260449287510832
Experience 12, Iter 35, disc loss: 0.0006048142341379683, policy loss: 8.411235030861484
Experience 12, Iter 36, disc loss: 0.00044387316189171955, policy loss: 8.208020561526283
Experience 12, Iter 37, disc loss: 0.0003664804714802673, policy loss: 8.648990978247237
Experience 12, Iter 38, disc loss: 0.0004637234663344926, policy loss: 8.461463181209183
Experience 12, Iter 39, disc loss: 0.0004532632898240252, policy loss: 8.408881001368604
Experience 12, Iter 40, disc loss: 0.00039121411894303985, policy loss: 8.539140246364804
Experience 12, Iter 41, disc loss: 0.00035438350451880444, policy loss: 8.599684093251735
Experience 12, Iter 42, disc loss: 0.00039664690269276446, policy loss: 8.660900285156872
Experience 12, Iter 43, disc loss: 0.0004701449026984945, policy loss: 8.21227043076341
Experience 12, Iter 44, disc loss: 0.00040701666385107606, policy loss: 8.36043657155966
Experience 12, Iter 45, disc loss: 0.0003733816539444325, policy loss: 8.958617042262846
Experience 12, Iter 46, disc loss: 0.00044925543482896046, policy loss: 8.590611379592694
Experience 12, Iter 47, disc loss: 0.0003394055910218192, policy loss: 8.98952185361669
Experience 12, Iter 48, disc loss: 0.00038961973155011975, policy loss: 8.46626821733079
Experience 12, Iter 49, disc loss: 0.0004655245871695139, policy loss: 8.301106199402227
Experience 12, Iter 50, disc loss: 0.0004047529748378016, policy loss: 8.765210289115085
Experience 12, Iter 51, disc loss: 0.0005372421059811425, policy loss: 8.59509232259544
Experience 12, Iter 52, disc loss: 0.0003489959891158632, policy loss: 8.780191861263107
Experience 12, Iter 53, disc loss: 0.0004303874917872488, policy loss: 8.434397076997376
Experience 12, Iter 54, disc loss: 0.00035535555372810733, policy loss: 8.824380414404644
Experience 12, Iter 55, disc loss: 0.0003563339473888416, policy loss: 8.957260432035698
Experience 12, Iter 56, disc loss: 0.0004190507261938224, policy loss: 9.064835313618378
Experience 12, Iter 57, disc loss: 0.00045130510556002154, policy loss: 8.388511313237418
Experience 12, Iter 58, disc loss: 0.00044776025244355744, policy loss: 8.370618534802237
Experience 12, Iter 59, disc loss: 0.00039790934395065355, policy loss: 8.624981376425822
Experience 12, Iter 60, disc loss: 0.0004002384892963035, policy loss: 8.637423469087299
Experience 12, Iter 61, disc loss: 0.0004082043202664848, policy loss: 8.73831754965112
Experience 12, Iter 62, disc loss: 0.00039327235890700076, policy loss: 8.776439079288142
Experience 12, Iter 63, disc loss: 0.00041056498666817927, policy loss: 8.72579193738579
Experience 12, Iter 64, disc loss: 0.0003247506700473747, policy loss: 8.78946155328096
Experience 12, Iter 65, disc loss: 0.0004198768847174645, policy loss: 8.421997957705036
Experience 12, Iter 66, disc loss: 0.0004432120607796359, policy loss: 8.419465348978097
Experience 12, Iter 67, disc loss: 0.00041153922073555857, policy loss: 8.832636471419201
Experience 12, Iter 68, disc loss: 0.0004076629984005139, policy loss: 8.817484307786428
Experience 12, Iter 69, disc loss: 0.0004930669107380907, policy loss: 8.476387586639952
Experience 12, Iter 70, disc loss: 0.0004615437455375806, policy loss: 8.379021519217359
Experience 12, Iter 71, disc loss: 0.00046073951124054257, policy loss: 8.297370469081955
Experience 12, Iter 72, disc loss: 0.00038283107155401253, policy loss: 8.898943744243489
Experience 12, Iter 73, disc loss: 0.0003724942086822178, policy loss: 8.822821357971591
Experience 12, Iter 74, disc loss: 0.00042962787360534916, policy loss: 8.380381002505167
Experience 12, Iter 75, disc loss: 0.0003015515162082532, policy loss: 9.452389894162103
Experience 12, Iter 76, disc loss: 0.0004194536570234935, policy loss: 8.51157169106041
Experience 12, Iter 77, disc loss: 0.0003835006732971914, policy loss: 8.500064915565439
Experience 12, Iter 78, disc loss: 0.00040234632520938724, policy loss: 8.687687071381944
Experience 12, Iter 79, disc loss: 0.000365955342000328, policy loss: 8.462429519862184
Experience 12, Iter 80, disc loss: 0.0003172627577708968, policy loss: 9.066731050582199
Experience 12, Iter 81, disc loss: 0.00040921134846597513, policy loss: 8.679988607686193
Experience 12, Iter 82, disc loss: 0.0003645673602571058, policy loss: 9.005936482477999
Experience 12, Iter 83, disc loss: 0.0003991941634472723, policy loss: 8.806671215737413
Experience 12, Iter 84, disc loss: 0.0003692312087150455, policy loss: 8.577070765293595
Experience 12, Iter 85, disc loss: 0.0004893457165562453, policy loss: 8.253167512482419
Experience 12, Iter 86, disc loss: 0.0003430351878297614, policy loss: 8.899892270270708
Experience 12, Iter 87, disc loss: 0.00032263000145339656, policy loss: 8.96510492361459
Experience 12, Iter 88, disc loss: 0.0003804347572056203, policy loss: 8.892203308819923
Experience 12, Iter 89, disc loss: 0.0003687883479727485, policy loss: 8.762833094617168
Experience 12, Iter 90, disc loss: 0.0004158305122247226, policy loss: 8.56631264552156
Experience 12, Iter 91, disc loss: 0.0004180020054019971, policy loss: 8.696050011062882
Experience 12, Iter 92, disc loss: 0.0003372134908620736, policy loss: 8.742634300868108
Experience 12, Iter 93, disc loss: 0.0003405352742160194, policy loss: 9.043654749214156
Experience 12, Iter 94, disc loss: 0.00046008658469981543, policy loss: 8.142307721255522
Experience 12, Iter 95, disc loss: 0.0003604572266225082, policy loss: 8.718493425786967
Experience 12, Iter 96, disc loss: 0.0004019280281493484, policy loss: 8.502664828710632
Experience 12, Iter 97, disc loss: 0.00038787436850492435, policy loss: 8.514459660720192
Experience 12, Iter 98, disc loss: 0.00038349773824883013, policy loss: 8.538925180188228
Experience 12, Iter 99, disc loss: 0.0003730248401406938, policy loss: 8.687621214175582
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0152],
        [0.1479],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.4907e-03, 3.3284e-02, 4.3613e-02, 1.7465e-03, 1.3155e-05,
          3.2226e-01]],

        [[7.4907e-03, 3.3284e-02, 4.3613e-02, 1.7465e-03, 1.3155e-05,
          3.2226e-01]],

        [[7.4907e-03, 3.3284e-02, 4.3613e-02, 1.7465e-03, 1.3155e-05,
          3.2226e-01]],

        [[7.4907e-03, 3.3284e-02, 4.3613e-02, 1.7465e-03, 1.3155e-05,
          3.2226e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0608, 0.5915, 0.0031], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0608, 0.5915, 0.0031])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.647
Iter 2/2000 - Loss: 0.712
Iter 3/2000 - Loss: -1.545
Iter 4/2000 - Loss: -1.207
Iter 5/2000 - Loss: -0.417
Iter 6/2000 - Loss: -0.683
Iter 7/2000 - Loss: -1.301
Iter 8/2000 - Loss: -1.597
Iter 9/2000 - Loss: -1.467
Iter 10/2000 - Loss: -1.186
Iter 11/2000 - Loss: -1.054
Iter 12/2000 - Loss: -1.147
Iter 13/2000 - Loss: -1.348
Iter 14/2000 - Loss: -1.502
Iter 15/2000 - Loss: -1.532
Iter 16/2000 - Loss: -1.457
Iter 17/2000 - Loss: -1.364
Iter 18/2000 - Loss: -1.334
Iter 19/2000 - Loss: -1.394
Iter 20/2000 - Loss: -1.504
Iter 1981/2000 - Loss: -8.951
Iter 1982/2000 - Loss: -8.951
Iter 1983/2000 - Loss: -8.951
Iter 1984/2000 - Loss: -8.951
Iter 1985/2000 - Loss: -8.951
Iter 1986/2000 - Loss: -8.951
Iter 1987/2000 - Loss: -8.951
Iter 1988/2000 - Loss: -8.951
Iter 1989/2000 - Loss: -8.951
Iter 1990/2000 - Loss: -8.951
Iter 1991/2000 - Loss: -8.951
Iter 1992/2000 - Loss: -8.951
Iter 1993/2000 - Loss: -8.951
Iter 1994/2000 - Loss: -8.951
Iter 1995/2000 - Loss: -8.951
Iter 1996/2000 - Loss: -8.951
Iter 1997/2000 - Loss: -8.951
Iter 1998/2000 - Loss: -8.951
Iter 1999/2000 - Loss: -8.951
Iter 2000/2000 - Loss: -8.951
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[18.3444,  3.9912, 34.6846, 10.0173,  3.1409, 30.1273]],

        [[21.0206, 32.9444, 31.6200,  2.9529,  5.1948, 16.1722]],

        [[19.0958, 40.4312, 26.6628,  1.8525,  4.5543, 24.1522]],

        [[23.5441, 44.7187,  8.1458,  2.3598,  4.9219, 22.5462]]])
Signal Variance: tensor([ 0.0422,  1.2152, 19.2582,  0.1563])
Estimated target variance: tensor([0.0036, 0.0608, 0.5915, 0.0031])
N: 130
Signal to noise ratio: tensor([12.7107, 59.7609, 90.2372, 24.8301])
Bound on condition number: tensor([  21004.1046,  464278.5818, 1058558.7550,   80150.1384])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.00046273796830360165, policy loss: 8.242333889581888
Experience 13, Iter 1, disc loss: 0.0003673082980720268, policy loss: 8.894879781816158
Experience 13, Iter 2, disc loss: 0.00036660007827978046, policy loss: 8.78539717465475
Experience 13, Iter 3, disc loss: 0.0003942728344725822, policy loss: 8.664674829617546
Experience 13, Iter 4, disc loss: 0.000497115928271868, policy loss: 8.501593917372043
Experience 13, Iter 5, disc loss: 0.00037675940371007324, policy loss: 8.522856893253582
Experience 13, Iter 6, disc loss: 0.0004092758195026921, policy loss: 8.797995381837826
Experience 13, Iter 7, disc loss: 0.0003424765001467508, policy loss: 9.125693017353559
Experience 13, Iter 8, disc loss: 0.0005015624576446317, policy loss: 8.518728831805502
Experience 13, Iter 9, disc loss: 0.0003893716694412609, policy loss: 8.836489949962058
Experience 13, Iter 10, disc loss: 0.0003948174021985542, policy loss: 8.792359884760016
Experience 13, Iter 11, disc loss: 0.0003605234591088825, policy loss: 8.665319734050227
Experience 13, Iter 12, disc loss: 0.00032983411058391864, policy loss: 8.924611511368482
Experience 13, Iter 13, disc loss: 0.00040062306446547724, policy loss: 8.89679259815098
Experience 13, Iter 14, disc loss: 0.00042568357281450985, policy loss: 8.373378651187991
Experience 13, Iter 15, disc loss: 0.0004634215713947607, policy loss: 8.489109251420128
Experience 13, Iter 16, disc loss: 0.00035620207201486463, policy loss: 8.73905171354411
Experience 13, Iter 17, disc loss: 0.0003608083014344675, policy loss: 8.853054365513964
Experience 13, Iter 18, disc loss: 0.00043802759931356515, policy loss: 8.659492542382612
Experience 13, Iter 19, disc loss: 0.00033787035172919155, policy loss: 9.050293960023495
Experience 13, Iter 20, disc loss: 0.0005139457815036504, policy loss: 8.56555308043469
Experience 13, Iter 21, disc loss: 0.0004104417525178644, policy loss: 8.656200094075999
Experience 13, Iter 22, disc loss: 0.0003615372859745097, policy loss: 8.654339735662154
Experience 13, Iter 23, disc loss: 0.00033710758804854574, policy loss: 9.00938023311891
Experience 13, Iter 24, disc loss: 0.00043098764943783703, policy loss: 8.532611886606151
Experience 13, Iter 25, disc loss: 0.0004207978464816026, policy loss: 8.441476244736712
Experience 13, Iter 26, disc loss: 0.000407506876689665, policy loss: 8.782059065001572
Experience 13, Iter 27, disc loss: 0.0003891676426776082, policy loss: 8.916105913231355
Experience 13, Iter 28, disc loss: 0.0004263589440450122, policy loss: 8.403834813640106
Experience 13, Iter 29, disc loss: 0.0003906838260974964, policy loss: 8.591281231621211
Experience 13, Iter 30, disc loss: 0.0004520410611282351, policy loss: 8.587842873164794
Experience 13, Iter 31, disc loss: 0.00036575496363133167, policy loss: 8.891099646784413
Experience 13, Iter 32, disc loss: 0.0004174734833450053, policy loss: 8.63197964989719
Experience 13, Iter 33, disc loss: 0.00037856886188874264, policy loss: 8.709616175993673
Experience 13, Iter 34, disc loss: 0.00035178530254155574, policy loss: 9.051596578418431
Experience 13, Iter 35, disc loss: 0.00040205284081129425, policy loss: 8.939312313140151
Experience 13, Iter 36, disc loss: 0.000490311908243719, policy loss: 8.757394486620782
Experience 13, Iter 37, disc loss: 0.0003526884505167237, policy loss: 8.810121303563141
Experience 13, Iter 38, disc loss: 0.0003771874454093193, policy loss: 8.465613136712003
Experience 13, Iter 39, disc loss: 0.00034209758298441934, policy loss: 8.708830127174664
Experience 13, Iter 40, disc loss: 0.00028696933991179796, policy loss: 8.770389953181201
Experience 13, Iter 41, disc loss: 0.00033700837803470664, policy loss: 8.806057491599848
Experience 13, Iter 42, disc loss: 0.00029594345840550703, policy loss: 8.929166710638174
Experience 13, Iter 43, disc loss: 0.00039601469849370497, policy loss: 8.57073301903124
Experience 13, Iter 44, disc loss: 0.00038183222075286794, policy loss: 8.673183951279954
Experience 13, Iter 45, disc loss: 0.00031428605741716276, policy loss: 9.026306216789436
Experience 13, Iter 46, disc loss: 0.00040210196595175124, policy loss: 8.85684439333454
Experience 13, Iter 47, disc loss: 0.0004071147117113014, policy loss: 8.716512371097302
Experience 13, Iter 48, disc loss: 0.00045779327957672214, policy loss: 8.693610731198568
Experience 13, Iter 49, disc loss: 0.0003892928664610989, policy loss: 8.49802047963894
Experience 13, Iter 50, disc loss: 0.0004433322340285968, policy loss: 8.285385978812684
Experience 13, Iter 51, disc loss: 0.00031834058727476806, policy loss: 9.037154702015336
Experience 13, Iter 52, disc loss: 0.00036496651867309166, policy loss: 8.785098683055114
Experience 13, Iter 53, disc loss: 0.0003754169832739749, policy loss: 8.794331896056079
Experience 13, Iter 54, disc loss: 0.0003046248595120514, policy loss: 9.100974942534718
Experience 13, Iter 55, disc loss: 0.0003154934179584759, policy loss: 8.900859268917484
Experience 13, Iter 56, disc loss: 0.0002958162065439833, policy loss: 9.082727944387093
Experience 13, Iter 57, disc loss: 0.0003752753154801516, policy loss: 8.570738509339044
Experience 13, Iter 58, disc loss: 0.0002729620107935099, policy loss: 9.041316207096841
Experience 13, Iter 59, disc loss: 0.00036481047439655196, policy loss: 8.883361720196646
Experience 13, Iter 60, disc loss: 0.0002794309704101592, policy loss: 8.89669297493713
Experience 13, Iter 61, disc loss: 0.0002673607121351615, policy loss: 9.050776319094403
Experience 13, Iter 62, disc loss: 0.00032998266079769893, policy loss: 9.169630350420706
Experience 13, Iter 63, disc loss: 0.0003105147605456701, policy loss: 9.075160053838108
Experience 13, Iter 64, disc loss: 0.000280537298960208, policy loss: 8.958113208726086
Experience 13, Iter 65, disc loss: 0.0002441029875675867, policy loss: 9.287571266194488
Experience 13, Iter 66, disc loss: 0.00029441600779064815, policy loss: 9.03690441711386
Experience 13, Iter 67, disc loss: 0.0003870852739000188, policy loss: 8.820252792658401
Experience 13, Iter 68, disc loss: 0.00043123830217201253, policy loss: 8.419654091089395
Experience 13, Iter 69, disc loss: 0.00029737995685503066, policy loss: 9.079571306027608
Experience 13, Iter 70, disc loss: 0.00028684448744609127, policy loss: 8.875322275835051
Experience 13, Iter 71, disc loss: 0.0002752818080151898, policy loss: 9.008891282717043
Experience 13, Iter 72, disc loss: 0.0002626760706524771, policy loss: 9.050723997511597
Experience 13, Iter 73, disc loss: 0.00026527903438340535, policy loss: 9.143933774375276
Experience 13, Iter 74, disc loss: 0.00026765689299106094, policy loss: 8.904953432860736
Experience 13, Iter 75, disc loss: 0.0003392898094189071, policy loss: 8.570256049606325
Experience 13, Iter 76, disc loss: 0.0002967380200827886, policy loss: 9.005061602184018
Experience 13, Iter 77, disc loss: 0.0003060598835021663, policy loss: 8.86271290498587
Experience 13, Iter 78, disc loss: 0.00026365879502850673, policy loss: 9.178427024374574
Experience 13, Iter 79, disc loss: 0.00026347820650751, policy loss: 8.990976305910444
Experience 13, Iter 80, disc loss: 0.0003073757918049637, policy loss: 8.766024794393056
Experience 13, Iter 81, disc loss: 0.0002876429895747925, policy loss: 8.835761951681258
Experience 13, Iter 82, disc loss: 0.00036899890307161455, policy loss: 8.74423682840124
Experience 13, Iter 83, disc loss: 0.0002720247309442922, policy loss: 8.760718312004819
Experience 13, Iter 84, disc loss: 0.00031979608731852, policy loss: 8.929269721393402
Experience 13, Iter 85, disc loss: 0.0002645865254292372, policy loss: 9.040028072072051
Experience 13, Iter 86, disc loss: 0.00038930760920142767, policy loss: 8.629715092448903
Experience 13, Iter 87, disc loss: 0.00032312962158978064, policy loss: 8.746555419862014
Experience 13, Iter 88, disc loss: 0.0003196082487183917, policy loss: 8.945100042287297
Experience 13, Iter 89, disc loss: 0.00035027355598062993, policy loss: 8.77325630305418
Experience 13, Iter 90, disc loss: 0.0002943082991368569, policy loss: 8.814823401143805
Experience 13, Iter 91, disc loss: 0.00029988269563552234, policy loss: 9.045821773337696
Experience 13, Iter 92, disc loss: 0.0003670094302644595, policy loss: 8.837413902546771
Experience 13, Iter 93, disc loss: 0.0003437090127205361, policy loss: 8.837521683082382
Experience 13, Iter 94, disc loss: 0.0003837832506996866, policy loss: 9.042321652722222
Experience 13, Iter 95, disc loss: 0.0002762509039285796, policy loss: 8.940664478130138
Experience 13, Iter 96, disc loss: 0.0002718639758168042, policy loss: 9.244024885517305
Experience 13, Iter 97, disc loss: 0.0002603367997153869, policy loss: 9.05534276765605
Experience 13, Iter 98, disc loss: 0.0003140980768858622, policy loss: 8.861384718048937
Experience 13, Iter 99, disc loss: 0.0002861213954746189, policy loss: 8.984249424837815
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0146],
        [0.1388],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.7602e-03, 3.4108e-02, 4.4094e-02, 1.8907e-03, 1.4054e-05,
          3.2154e-01]],

        [[8.7602e-03, 3.4108e-02, 4.4094e-02, 1.8907e-03, 1.4054e-05,
          3.2154e-01]],

        [[8.7602e-03, 3.4108e-02, 4.4094e-02, 1.8907e-03, 1.4054e-05,
          3.2154e-01]],

        [[8.7602e-03, 3.4108e-02, 4.4094e-02, 1.8907e-03, 1.4054e-05,
          3.2154e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0585, 0.5553, 0.0033], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0585, 0.5553, 0.0033])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.667
Iter 2/2000 - Loss: 0.812
Iter 3/2000 - Loss: -1.546
Iter 4/2000 - Loss: -1.215
Iter 5/2000 - Loss: -0.371
Iter 6/2000 - Loss: -0.598
Iter 7/2000 - Loss: -1.235
Iter 8/2000 - Loss: -1.588
Iter 9/2000 - Loss: -1.510
Iter 10/2000 - Loss: -1.234
Iter 11/2000 - Loss: -1.057
Iter 12/2000 - Loss: -1.099
Iter 13/2000 - Loss: -1.281
Iter 14/2000 - Loss: -1.461
Iter 15/2000 - Loss: -1.540
Iter 16/2000 - Loss: -1.505
Iter 17/2000 - Loss: -1.415
Iter 18/2000 - Loss: -1.352
Iter 19/2000 - Loss: -1.368
Iter 20/2000 - Loss: -1.459
Iter 1981/2000 - Loss: -8.933
Iter 1982/2000 - Loss: -8.933
Iter 1983/2000 - Loss: -8.933
Iter 1984/2000 - Loss: -8.933
Iter 1985/2000 - Loss: -8.933
Iter 1986/2000 - Loss: -8.933
Iter 1987/2000 - Loss: -8.933
Iter 1988/2000 - Loss: -8.933
Iter 1989/2000 - Loss: -8.933
Iter 1990/2000 - Loss: -8.933
Iter 1991/2000 - Loss: -8.934
Iter 1992/2000 - Loss: -8.934
Iter 1993/2000 - Loss: -8.934
Iter 1994/2000 - Loss: -8.934
Iter 1995/2000 - Loss: -8.934
Iter 1996/2000 - Loss: -8.934
Iter 1997/2000 - Loss: -8.934
Iter 1998/2000 - Loss: -8.934
Iter 1999/2000 - Loss: -8.934
Iter 2000/2000 - Loss: -8.934
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[18.6194,  3.7348, 32.1797, 15.3693,  1.1926, 28.2597]],

        [[22.5908, 33.0415, 31.0259,  2.9330,  4.9203, 16.4420]],

        [[25.0453, 37.1303, 27.2613,  1.6107,  3.8001, 24.2528]],

        [[23.8170, 42.2155,  8.6683,  2.5663,  5.3918, 26.2839]]])
Signal Variance: tensor([ 0.0350,  1.1995, 17.8480,  0.1836])
Estimated target variance: tensor([0.0037, 0.0585, 0.5553, 0.0033])
N: 140
Signal to noise ratio: tensor([11.2968, 59.9636, 85.7637, 26.2372])
Bound on condition number: tensor([  17867.4058,  503390.3598, 1029759.8020,   96375.9630])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0003169552687183048, policy loss: 9.001274196385502
Experience 14, Iter 1, disc loss: 0.0002909171133454068, policy loss: 8.891188604749544
Experience 14, Iter 2, disc loss: 0.0002715752530461833, policy loss: 9.072333580553748
Experience 14, Iter 3, disc loss: 0.0003109748222843716, policy loss: 8.840036271233853
Experience 14, Iter 4, disc loss: 0.00026402584302788094, policy loss: 9.002847241346672
Experience 14, Iter 5, disc loss: 0.00026467822411717416, policy loss: 9.08436652655752
Experience 14, Iter 6, disc loss: 0.0003451715109879633, policy loss: 8.756637376056602
Experience 14, Iter 7, disc loss: 0.00030114488595700214, policy loss: 8.819426422459305
Experience 14, Iter 8, disc loss: 0.00026834759123017457, policy loss: 9.105355273272396
Experience 14, Iter 9, disc loss: 0.00031632332610091675, policy loss: 8.92161182399063
Experience 14, Iter 10, disc loss: 0.00028528090814240254, policy loss: 8.911198548721625
Experience 14, Iter 11, disc loss: 0.00032642781526207975, policy loss: 8.89150681107892
Experience 14, Iter 12, disc loss: 0.00027809806355786373, policy loss: 9.042078179603278
Experience 14, Iter 13, disc loss: 0.0002726829408388056, policy loss: 9.082791055522291
Experience 14, Iter 14, disc loss: 0.00025906700189527184, policy loss: 8.970157803156173
Experience 14, Iter 15, disc loss: 0.00029227806508626817, policy loss: 9.077827067693953
Experience 14, Iter 16, disc loss: 0.00026599443071559836, policy loss: 9.090895807654366
Experience 14, Iter 17, disc loss: 0.00035762143155713895, policy loss: 8.957229568374272
Experience 14, Iter 18, disc loss: 0.00021004064357760352, policy loss: 9.388215665085045
Experience 14, Iter 19, disc loss: 0.00019471968137373835, policy loss: 9.636996337545469
Experience 14, Iter 20, disc loss: 0.00025408222454310214, policy loss: 9.49099100178626
Experience 14, Iter 21, disc loss: 0.00020440129367157273, policy loss: 9.848061850410573
Experience 14, Iter 22, disc loss: 0.00017976613691232326, policy loss: 9.847791245610782
Experience 14, Iter 23, disc loss: 0.00019346417684348985, policy loss: 9.713693757767441
Experience 14, Iter 24, disc loss: 0.00023686077458272747, policy loss: 9.323821141453617
Experience 14, Iter 25, disc loss: 0.00023428808128380998, policy loss: 9.295467425518073
Experience 14, Iter 26, disc loss: 0.00019472657268836913, policy loss: 9.52918196503749
Experience 14, Iter 27, disc loss: 0.0002532427322958413, policy loss: 9.08700007995952
Experience 14, Iter 28, disc loss: 0.00024143964805961626, policy loss: 9.254710046884552
Experience 14, Iter 29, disc loss: 0.0002594365262642815, policy loss: 9.184820565599733
Experience 14, Iter 30, disc loss: 0.0002278708138921881, policy loss: 9.406488857606334
Experience 14, Iter 31, disc loss: 0.0002871013048810792, policy loss: 8.766042827351512
Experience 14, Iter 32, disc loss: 0.0003127928834605555, policy loss: 9.237882224143041
Experience 14, Iter 33, disc loss: 0.00024051689571450466, policy loss: 9.417658474857333
Experience 14, Iter 34, disc loss: 0.00020469075566168432, policy loss: 9.483685334426287
Experience 14, Iter 35, disc loss: 0.0003178647485974153, policy loss: 9.249472205668942
Experience 14, Iter 36, disc loss: 0.00028735778933650145, policy loss: 8.998154764385564
Experience 14, Iter 37, disc loss: 0.00024087695000437942, policy loss: 9.418329083666782
Experience 14, Iter 38, disc loss: 0.00040235607387250884, policy loss: 8.878037098877208
Experience 14, Iter 39, disc loss: 0.0003618131082743468, policy loss: 8.767253031491386
Experience 14, Iter 40, disc loss: 0.00023123385988187546, policy loss: 9.483432459531297
Experience 14, Iter 41, disc loss: 0.000326097228279937, policy loss: 9.501944225261802
Experience 14, Iter 42, disc loss: 0.00030678339807147035, policy loss: 8.897489757759876
Experience 14, Iter 43, disc loss: 0.00031273198237099473, policy loss: 9.065793522536662
Experience 14, Iter 44, disc loss: 0.00031401911249145237, policy loss: 8.935592588013208
Experience 14, Iter 45, disc loss: 0.0003294746124129683, policy loss: 9.036698792732532
Experience 14, Iter 46, disc loss: 0.0002573258004011219, policy loss: 9.13788433112937
Experience 14, Iter 47, disc loss: 0.0003466961317554362, policy loss: 9.111517890569761
Experience 14, Iter 48, disc loss: 0.00027077549999083244, policy loss: 9.66456557266866
Experience 14, Iter 49, disc loss: 0.00019836003331972387, policy loss: 9.686680857506932
Experience 14, Iter 50, disc loss: 0.0002231564809554985, policy loss: 9.800800091347746
Experience 14, Iter 51, disc loss: 0.0002491194049897525, policy loss: 9.608411795420585
Experience 14, Iter 52, disc loss: 0.0002998284215289538, policy loss: 9.291700213880455
Experience 14, Iter 53, disc loss: 0.00019998007218660998, policy loss: 9.902195830193866
Experience 14, Iter 54, disc loss: 0.0002493743718606051, policy loss: 9.381669340102958
Experience 14, Iter 55, disc loss: 0.00024866143312764303, policy loss: 9.711067750632301
Experience 14, Iter 56, disc loss: 0.00031517258207449354, policy loss: 9.13919416264872
Experience 14, Iter 57, disc loss: 0.0003230878091106555, policy loss: 8.972279007559344
Experience 14, Iter 58, disc loss: 0.0002911047185278664, policy loss: 9.00420891218307
Experience 14, Iter 59, disc loss: 0.000329335986767439, policy loss: 8.953798250301967
Experience 14, Iter 60, disc loss: 0.00021807760942029486, policy loss: 9.571715062516322
Experience 14, Iter 61, disc loss: 0.0002584652559764942, policy loss: 9.036844318508525
Experience 14, Iter 62, disc loss: 0.0002280923599979107, policy loss: 9.261027472237005
Experience 14, Iter 63, disc loss: 0.00025273705152994866, policy loss: 9.383818045056904
Experience 14, Iter 64, disc loss: 0.0002548842124185866, policy loss: 9.313462760282079
Experience 14, Iter 65, disc loss: 0.0002667906783233296, policy loss: 9.12108474825994
Experience 14, Iter 66, disc loss: 0.000276645947751073, policy loss: 9.359849286184282
Experience 14, Iter 67, disc loss: 0.00022841975204882902, policy loss: 9.267443725841762
Experience 14, Iter 68, disc loss: 0.00034865557345870133, policy loss: 9.123290944541221
Experience 14, Iter 69, disc loss: 0.00039898325076983366, policy loss: 8.912677771465486
Experience 14, Iter 70, disc loss: 0.00026480713462805294, policy loss: 9.134872458249045
Experience 14, Iter 71, disc loss: 0.0003348232074837678, policy loss: 8.896743958136721
Experience 14, Iter 72, disc loss: 0.00024899761618698147, policy loss: 9.348263546699467
Experience 14, Iter 73, disc loss: 0.0002894309184089122, policy loss: 9.376422566026582
Experience 14, Iter 74, disc loss: 0.0003459004305000249, policy loss: 8.809556085691682
Experience 14, Iter 75, disc loss: 0.00031163064388887703, policy loss: 9.106576006820196
Experience 14, Iter 76, disc loss: 0.0002971480176664892, policy loss: 9.103984417896141
Experience 14, Iter 77, disc loss: 0.000291754339190431, policy loss: 9.13507605725566
Experience 14, Iter 78, disc loss: 0.0002509262005223968, policy loss: 9.65354765386813
Experience 14, Iter 79, disc loss: 0.0002527714750074027, policy loss: 9.104978601992329
Experience 14, Iter 80, disc loss: 0.00027576028262542166, policy loss: 9.208809057519446
Experience 14, Iter 81, disc loss: 0.00032299699537614835, policy loss: 9.05938328989964
Experience 14, Iter 82, disc loss: 0.00023123460842578691, policy loss: 9.33921410286672
Experience 14, Iter 83, disc loss: 0.00028658427064205704, policy loss: 9.188897141265091
Experience 14, Iter 84, disc loss: 0.00028363736503050235, policy loss: 9.17201578608323
Experience 14, Iter 85, disc loss: 0.00040271880079394536, policy loss: 8.608691573568432
Experience 14, Iter 86, disc loss: 0.00037839056982354077, policy loss: 8.811584068932065
Experience 14, Iter 87, disc loss: 0.0002639443613511806, policy loss: 9.029587002901927
Experience 14, Iter 88, disc loss: 0.00034563733973057433, policy loss: 9.201782319921193
Experience 14, Iter 89, disc loss: 0.0002689386783135252, policy loss: 9.257284360742762
Experience 14, Iter 90, disc loss: 0.0002451314212317514, policy loss: 9.526943602129785
Experience 14, Iter 91, disc loss: 0.00032198490649163624, policy loss: 8.72905157899779
Experience 14, Iter 92, disc loss: 0.00036589748236504494, policy loss: 8.756731575894918
Experience 14, Iter 93, disc loss: 0.00028222692671641954, policy loss: 8.981812480738316
Experience 14, Iter 94, disc loss: 0.00036689581178972496, policy loss: 8.87631308517334
Experience 14, Iter 95, disc loss: 0.00037598089522737383, policy loss: 8.978069722880207
Experience 14, Iter 96, disc loss: 0.0003247353230917771, policy loss: 8.975119795430256
Experience 14, Iter 97, disc loss: 0.0002442541059574769, policy loss: 9.655181719764801
Experience 14, Iter 98, disc loss: 0.00021227548673133218, policy loss: 9.489207353632413
Experience 14, Iter 99, disc loss: 0.00019375006489970862, policy loss: 9.691017472295478
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0150],
        [0.1418],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.6395e-03, 3.3725e-02, 4.3624e-02, 1.8467e-03, 1.3222e-05,
          3.2673e-01]],

        [[8.6395e-03, 3.3725e-02, 4.3624e-02, 1.8467e-03, 1.3222e-05,
          3.2673e-01]],

        [[8.6395e-03, 3.3725e-02, 4.3624e-02, 1.8467e-03, 1.3222e-05,
          3.2673e-01]],

        [[8.6395e-03, 3.3725e-02, 4.3624e-02, 1.8467e-03, 1.3222e-05,
          3.2673e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0600, 0.5672, 0.0032], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0600, 0.5672, 0.0032])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.660
Iter 2/2000 - Loss: 0.818
Iter 3/2000 - Loss: -1.541
Iter 4/2000 - Loss: -1.204
Iter 5/2000 - Loss: -0.364
Iter 6/2000 - Loss: -0.603
Iter 7/2000 - Loss: -1.241
Iter 8/2000 - Loss: -1.585
Iter 9/2000 - Loss: -1.497
Iter 10/2000 - Loss: -1.219
Iter 11/2000 - Loss: -1.048
Iter 12/2000 - Loss: -1.097
Iter 13/2000 - Loss: -1.283
Iter 14/2000 - Loss: -1.461
Iter 15/2000 - Loss: -1.535
Iter 16/2000 - Loss: -1.493
Iter 17/2000 - Loss: -1.398
Iter 18/2000 - Loss: -1.336
Iter 19/2000 - Loss: -1.358
Iter 20/2000 - Loss: -1.455
Iter 1981/2000 - Loss: -8.908
Iter 1982/2000 - Loss: -8.908
Iter 1983/2000 - Loss: -8.908
Iter 1984/2000 - Loss: -8.908
Iter 1985/2000 - Loss: -8.908
Iter 1986/2000 - Loss: -8.908
Iter 1987/2000 - Loss: -8.908
Iter 1988/2000 - Loss: -8.908
Iter 1989/2000 - Loss: -8.908
Iter 1990/2000 - Loss: -8.908
Iter 1991/2000 - Loss: -8.908
Iter 1992/2000 - Loss: -8.908
Iter 1993/2000 - Loss: -8.908
Iter 1994/2000 - Loss: -8.908
Iter 1995/2000 - Loss: -8.908
Iter 1996/2000 - Loss: -8.908
Iter 1997/2000 - Loss: -8.908
Iter 1998/2000 - Loss: -8.908
Iter 1999/2000 - Loss: -8.908
Iter 2000/2000 - Loss: -8.908
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[17.8102,  3.7579, 31.7185, 13.5870,  1.4387, 27.7349]],

        [[22.0912, 32.3772, 28.8448,  2.8548,  4.1573, 16.1374]],

        [[24.3687, 36.9056, 26.7049,  1.6234,  3.7475, 23.9761]],

        [[24.2835, 40.5074,  8.7444,  2.5854,  5.2505, 26.6594]]])
Signal Variance: tensor([ 0.0358,  1.2046, 17.3203,  0.1857])
Estimated target variance: tensor([0.0036, 0.0600, 0.5672, 0.0032])
N: 150
Signal to noise ratio: tensor([11.3561, 57.7182, 84.2353, 26.1111])
Bound on condition number: tensor([  19345.1541,  499709.8300, 1064339.5447,  102269.2597])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.00020975102044541727, policy loss: 9.465543125120979
Experience 15, Iter 1, disc loss: 0.00019162974293664945, policy loss: 9.813412241432427
Experience 15, Iter 2, disc loss: 0.00022771465960353998, policy loss: 9.727708087141794
Experience 15, Iter 3, disc loss: 0.00021917300305774332, policy loss: 10.089455204082537
Experience 15, Iter 4, disc loss: 0.0002448695427693549, policy loss: 9.679521093922139
Experience 15, Iter 5, disc loss: 0.00023537604762266363, policy loss: 9.774935676993552
Experience 15, Iter 6, disc loss: 0.0002979650138633223, policy loss: 9.497383792573007
Experience 15, Iter 7, disc loss: 0.0003713371780975794, policy loss: 9.24199763852771
Experience 15, Iter 8, disc loss: 0.00032410288712135657, policy loss: 8.779579923761727
Experience 15, Iter 9, disc loss: 0.00021349279272910008, policy loss: 9.931490255292545
Experience 15, Iter 10, disc loss: 0.00019086663619796116, policy loss: 9.649664590193003
Experience 15, Iter 11, disc loss: 0.0001541643854624525, policy loss: 9.996214995811032
Experience 15, Iter 12, disc loss: 0.00018376917720934794, policy loss: 9.533288782652361
Experience 15, Iter 13, disc loss: 0.00014173636574328825, policy loss: 9.895788617763014
Experience 15, Iter 14, disc loss: 0.00018784362830789116, policy loss: 9.4935027023595
Experience 15, Iter 15, disc loss: 0.00023487480977899146, policy loss: 9.227445097245074
Experience 15, Iter 16, disc loss: 0.0002099579061453395, policy loss: 9.245560327201531
Experience 15, Iter 17, disc loss: 0.00024921992559458206, policy loss: 9.354317469782139
Experience 15, Iter 18, disc loss: 0.0002626143647598975, policy loss: 9.264221106927426
Experience 15, Iter 19, disc loss: 0.00024354921903113714, policy loss: 9.823201723329818
Experience 15, Iter 20, disc loss: 0.000275930056815591, policy loss: 9.073423542131051
Experience 15, Iter 21, disc loss: 0.00024407483912605352, policy loss: 9.646717542763893
Experience 15, Iter 22, disc loss: 0.00023313446009841945, policy loss: 9.824941197762822
Experience 15, Iter 23, disc loss: 0.0002572995547642758, policy loss: 9.386605858798298
Experience 15, Iter 24, disc loss: 0.00023404916382997997, policy loss: 9.56981191567301
Experience 15, Iter 25, disc loss: 0.00024075749094553084, policy loss: 9.747666563256168
Experience 15, Iter 26, disc loss: 0.00027188704408304533, policy loss: 9.596344524079097
Experience 15, Iter 27, disc loss: 0.0002835478146510681, policy loss: 8.951788524572171
Experience 15, Iter 28, disc loss: 0.0002115852016102428, policy loss: 9.67196710266666
Experience 15, Iter 29, disc loss: 0.00026808299949698886, policy loss: 9.254542341009866
Experience 15, Iter 30, disc loss: 0.0002826316850488663, policy loss: 9.363307171361127
Experience 15, Iter 31, disc loss: 0.0003713522963166731, policy loss: 8.832074866675779
Experience 15, Iter 32, disc loss: 0.00029614273066510725, policy loss: 9.330161365312653
Experience 15, Iter 33, disc loss: 0.0002825514098544057, policy loss: 9.21100723716901
Experience 15, Iter 34, disc loss: 0.0003279753082471536, policy loss: 8.851216279636802
Experience 15, Iter 35, disc loss: 0.0003009746952255682, policy loss: 9.180560570618006
Experience 15, Iter 36, disc loss: 0.00030030562338305455, policy loss: 9.713921689996091
Experience 15, Iter 37, disc loss: 0.0002251260766009292, policy loss: 9.674298829232654
Experience 15, Iter 38, disc loss: 0.0002522066359239873, policy loss: 9.731298381120318
Experience 15, Iter 39, disc loss: 0.00029241136419838647, policy loss: 9.334007948722885
Experience 15, Iter 40, disc loss: 0.00028808634965438683, policy loss: 9.691815404210143
Experience 15, Iter 41, disc loss: 0.00030939208660473317, policy loss: 9.397776549008523
Experience 15, Iter 42, disc loss: 0.0002807293768783143, policy loss: 9.091050052745517
Experience 15, Iter 43, disc loss: 0.00028055105470420734, policy loss: 8.92292277181355
Experience 15, Iter 44, disc loss: 0.0002843714188981453, policy loss: 9.310797770437663
Experience 15, Iter 45, disc loss: 0.0002764793411754091, policy loss: 9.492553548554627
Experience 15, Iter 46, disc loss: 0.00031297134949005977, policy loss: 9.10010924999007
Experience 15, Iter 47, disc loss: 0.000300225012874783, policy loss: 9.212795971851918
Experience 15, Iter 48, disc loss: 0.00018642268026784487, policy loss: 9.413805530463112
Experience 15, Iter 49, disc loss: 0.00021948493283265489, policy loss: 9.684553754065561
Experience 15, Iter 50, disc loss: 0.00022113144751561366, policy loss: 9.704894420650893
Experience 15, Iter 51, disc loss: 0.00020253489732960101, policy loss: 9.255732376767785
Experience 15, Iter 52, disc loss: 0.0002822854319944358, policy loss: 9.16245902784575
Experience 15, Iter 53, disc loss: 0.0002519176967373991, policy loss: 9.267357954787364
Experience 15, Iter 54, disc loss: 0.00024601004620134376, policy loss: 9.545868547714454
Experience 15, Iter 55, disc loss: 0.0002604571135570939, policy loss: 9.296301982964978
Experience 15, Iter 56, disc loss: 0.00024433616968157847, policy loss: 9.694779716641655
Experience 15, Iter 57, disc loss: 0.00024534510188726436, policy loss: 9.4876476874418
Experience 15, Iter 58, disc loss: 0.0002491008932444406, policy loss: 9.08680009130989
Experience 15, Iter 59, disc loss: 0.00022670376459169256, policy loss: 9.345815806334468
Experience 15, Iter 60, disc loss: 0.00024567029527721573, policy loss: 9.098082006262793
Experience 15, Iter 61, disc loss: 0.0003075248063686248, policy loss: 9.240921529466906
Experience 15, Iter 62, disc loss: 0.00022227510366770958, policy loss: 9.972281429756043
Experience 15, Iter 63, disc loss: 0.0002459188523976175, policy loss: 9.336838139788243
Experience 15, Iter 64, disc loss: 0.0002572938147634352, policy loss: 9.184687858101368
Experience 15, Iter 65, disc loss: 0.00026132634650337903, policy loss: 9.540065075897576
Experience 15, Iter 66, disc loss: 0.00023765586018085794, policy loss: 9.793101646943835
Experience 15, Iter 67, disc loss: 0.000273450976494001, policy loss: 9.310747614856183
Experience 15, Iter 68, disc loss: 0.00029936355017392066, policy loss: 9.017494316300748
Experience 15, Iter 69, disc loss: 0.0002252422198137959, policy loss: 9.506057599777947
Experience 15, Iter 70, disc loss: 0.0003017858270687834, policy loss: 9.101040968901781
Experience 15, Iter 71, disc loss: 0.00028086105937717406, policy loss: 9.039869756188258
Experience 15, Iter 72, disc loss: 0.00025962869078347917, policy loss: 9.358911019618343
Experience 15, Iter 73, disc loss: 0.00023712463987252542, policy loss: 9.296985839567018
Experience 15, Iter 74, disc loss: 0.00020159189996766115, policy loss: 9.447883835079246
Experience 15, Iter 75, disc loss: 0.0002701050316423996, policy loss: 9.560375844896082
Experience 15, Iter 76, disc loss: 0.00022163317747787736, policy loss: 9.194345402951928
Experience 15, Iter 77, disc loss: 0.0002231616216997458, policy loss: 9.229207347746785
Experience 15, Iter 78, disc loss: 0.0002395484014293489, policy loss: 9.075035683278557
Experience 15, Iter 79, disc loss: 0.00026541103208073066, policy loss: 9.400678832586484
Experience 15, Iter 80, disc loss: 0.00017253085881666167, policy loss: 9.74577553364999
Experience 15, Iter 81, disc loss: 0.0001718937870086713, policy loss: 9.746408413847748
Experience 15, Iter 82, disc loss: 0.00019589589438935242, policy loss: 9.57175761130214
Experience 15, Iter 83, disc loss: 0.0001564139855480042, policy loss: 9.780870579063137
Experience 15, Iter 84, disc loss: 0.00019756525555882242, policy loss: 9.750345804093268
Experience 15, Iter 85, disc loss: 0.0002257361996440979, policy loss: 9.100367273782176
Experience 15, Iter 86, disc loss: 0.00024552149516444564, policy loss: 9.391836101066632
Experience 15, Iter 87, disc loss: 0.0002468845212499729, policy loss: 9.14045696287315
Experience 15, Iter 88, disc loss: 0.00023133881818361825, policy loss: 9.671628846060589
Experience 15, Iter 89, disc loss: 0.00023171817668880433, policy loss: 9.69636308795814
Experience 15, Iter 90, disc loss: 0.0002490510785899407, policy loss: 9.24613778835375
Experience 15, Iter 91, disc loss: 0.00014913480039005243, policy loss: 9.795591283683734
Experience 15, Iter 92, disc loss: 0.00020513398217564792, policy loss: 9.812836280615425
Experience 15, Iter 93, disc loss: 0.00022905363391832252, policy loss: 9.145771915320747
Experience 15, Iter 94, disc loss: 0.00021618776372286474, policy loss: 9.34011852781887
Experience 15, Iter 95, disc loss: 0.0001871223680314646, policy loss: 9.609868012279781
Experience 15, Iter 96, disc loss: 0.0001924081202012005, policy loss: 9.723594521860399
Experience 15, Iter 97, disc loss: 0.0002454695134349281, policy loss: 9.392470430983057
Experience 15, Iter 98, disc loss: 0.00023330201328963153, policy loss: 9.116883016896168
Experience 15, Iter 99, disc loss: 0.00023377627326720562, policy loss: 9.346875694335498
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0165],
        [0.1565],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[8.1288e-03, 3.2361e-02, 4.4222e-02, 1.7532e-03, 1.2660e-05,
          3.4854e-01]],

        [[8.1288e-03, 3.2361e-02, 4.4222e-02, 1.7532e-03, 1.2660e-05,
          3.4854e-01]],

        [[8.1288e-03, 3.2361e-02, 4.4222e-02, 1.7532e-03, 1.2660e-05,
          3.4854e-01]],

        [[8.1288e-03, 3.2361e-02, 4.4222e-02, 1.7532e-03, 1.2660e-05,
          3.4854e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0662, 0.6259, 0.0031], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0662, 0.6259, 0.0031])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.596
Iter 2/2000 - Loss: 0.914
Iter 3/2000 - Loss: -1.478
Iter 4/2000 - Loss: -1.131
Iter 5/2000 - Loss: -0.285
Iter 6/2000 - Loss: -0.538
Iter 7/2000 - Loss: -1.185
Iter 8/2000 - Loss: -1.525
Iter 9/2000 - Loss: -1.426
Iter 10/2000 - Loss: -1.142
Iter 11/2000 - Loss: -0.975
Iter 12/2000 - Loss: -1.033
Iter 13/2000 - Loss: -1.226
Iter 14/2000 - Loss: -1.403
Iter 15/2000 - Loss: -1.470
Iter 16/2000 - Loss: -1.419
Iter 17/2000 - Loss: -1.320
Iter 18/2000 - Loss: -1.259
Iter 19/2000 - Loss: -1.288
Iter 20/2000 - Loss: -1.389
Iter 1981/2000 - Loss: -8.986
Iter 1982/2000 - Loss: -8.986
Iter 1983/2000 - Loss: -8.986
Iter 1984/2000 - Loss: -8.986
Iter 1985/2000 - Loss: -8.986
Iter 1986/2000 - Loss: -8.986
Iter 1987/2000 - Loss: -8.986
Iter 1988/2000 - Loss: -8.986
Iter 1989/2000 - Loss: -8.986
Iter 1990/2000 - Loss: -8.986
Iter 1991/2000 - Loss: -8.986
Iter 1992/2000 - Loss: -8.986
Iter 1993/2000 - Loss: -8.986
Iter 1994/2000 - Loss: -8.986
Iter 1995/2000 - Loss: -8.986
Iter 1996/2000 - Loss: -8.986
Iter 1997/2000 - Loss: -8.986
Iter 1998/2000 - Loss: -8.986
Iter 1999/2000 - Loss: -8.986
Iter 2000/2000 - Loss: -8.986
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[16.7809,  3.7369, 33.5586, 11.3164,  1.7152, 26.4673]],

        [[20.4573, 31.5785, 27.3728,  2.8419,  4.0018, 16.1046]],

        [[22.6290, 36.2335, 26.9902,  1.7102,  3.8837, 24.8815]],

        [[23.0682, 39.5512,  8.5797,  2.5667,  5.1852, 27.0665]]])
Signal Variance: tensor([ 0.0361,  1.2176, 18.2922,  0.1792])
Estimated target variance: tensor([0.0035, 0.0662, 0.6259, 0.0031])
N: 160
Signal to noise ratio: tensor([11.3453, 59.1265, 88.2635, 26.2245])
Bound on condition number: tensor([  20595.5890,  559351.3578, 1246471.2050,  110036.7213])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.00022072196874438075, policy loss: 9.692414065280168
Experience 16, Iter 1, disc loss: 0.0002626088050200711, policy loss: 9.032032411328151
Experience 16, Iter 2, disc loss: 0.00022833011669186521, policy loss: 9.03504655584664
Experience 16, Iter 3, disc loss: 0.00021450755090193828, policy loss: 9.160077654178576
Experience 16, Iter 4, disc loss: 0.0002525703407949779, policy loss: 9.239735631373144
Experience 16, Iter 5, disc loss: 0.00019520196994718717, policy loss: 9.899608455659392
Experience 16, Iter 6, disc loss: 0.00023646694282873014, policy loss: 9.748529872795315
Experience 16, Iter 7, disc loss: 0.00013017906896418122, policy loss: 9.916838886598258
Experience 16, Iter 8, disc loss: 0.00017472224391693717, policy loss: 10.069990308327144
Experience 16, Iter 9, disc loss: 0.00022450993509331283, policy loss: 9.606245771959601
Experience 16, Iter 10, disc loss: 0.00022352523696076302, policy loss: 9.61363503662469
Experience 16, Iter 11, disc loss: 0.00012140077159939809, policy loss: 9.84019920961434
Experience 16, Iter 12, disc loss: 0.00010936969656885522, policy loss: 10.11906555011721
Experience 16, Iter 13, disc loss: 0.0002491119030905463, policy loss: 9.932182732537687
Experience 16, Iter 14, disc loss: 0.0001476446632245253, policy loss: 10.125743270801546
Experience 16, Iter 15, disc loss: 0.00013935055658769756, policy loss: 9.922777359165671
Experience 16, Iter 16, disc loss: 0.0001965525932171559, policy loss: 9.967376504108032
Experience 16, Iter 17, disc loss: 0.00019539779287727738, policy loss: 9.862037713543094
Experience 16, Iter 18, disc loss: 0.0001829503351699707, policy loss: 9.494666640219204
Experience 16, Iter 19, disc loss: 0.00017639143234424574, policy loss: 9.604799339820218
Experience 16, Iter 20, disc loss: 0.00019869912015745385, policy loss: 9.497882056236076
Experience 16, Iter 21, disc loss: 0.00015543918695872194, policy loss: 9.62807865697371
Experience 16, Iter 22, disc loss: 0.0001950536365412578, policy loss: 9.27930676464216
Experience 16, Iter 23, disc loss: 0.00015264266401158788, policy loss: 9.609681255524563
Experience 16, Iter 24, disc loss: 0.0001658683001704404, policy loss: 9.438190360716217
Experience 16, Iter 25, disc loss: 0.000173593916353652, policy loss: 9.45861556506259
Experience 16, Iter 26, disc loss: 0.00013907911089291097, policy loss: 9.826944313182194
Experience 16, Iter 27, disc loss: 0.00019501371652590768, policy loss: 9.448786668596588
Experience 16, Iter 28, disc loss: 0.0001704778037449249, policy loss: 9.758319953099022
Experience 16, Iter 29, disc loss: 0.0001330240304665777, policy loss: 9.85165589954006
Experience 16, Iter 30, disc loss: 0.00013261163688743944, policy loss: 9.962610828860022
Experience 16, Iter 31, disc loss: 0.0002825407389923905, policy loss: 9.26316079098905
Experience 16, Iter 32, disc loss: 0.00021853094478068582, policy loss: 9.098060911633615
Experience 16, Iter 33, disc loss: 0.00023384092214164182, policy loss: 9.582927396745541
Experience 16, Iter 34, disc loss: 0.00013973236052259428, policy loss: 10.192191039580287
Experience 16, Iter 35, disc loss: 0.00026697402333376245, policy loss: 9.295033006352167
Experience 16, Iter 36, disc loss: 0.00018128011542264017, policy loss: 9.658767838221682
Experience 16, Iter 37, disc loss: 0.000261122267752635, policy loss: 9.380487107683045
Experience 16, Iter 38, disc loss: 0.00031941793016594796, policy loss: 9.245131452672684
Experience 16, Iter 39, disc loss: 0.00020047039997859333, policy loss: 9.54979196352654
Experience 16, Iter 40, disc loss: 0.0002022069159368466, policy loss: 9.324073928734
Experience 16, Iter 41, disc loss: 0.0001486588892438296, policy loss: 9.930833528201939
Experience 16, Iter 42, disc loss: 0.0001503174440652859, policy loss: 9.844761002062668
Experience 16, Iter 43, disc loss: 0.00025850388245412474, policy loss: 9.224637183399372
Experience 16, Iter 44, disc loss: 0.00020973565823344774, policy loss: 9.398644677798359
Experience 16, Iter 45, disc loss: 0.000233399625266096, policy loss: 9.180445526500502
Experience 16, Iter 46, disc loss: 0.0003019416964147074, policy loss: 9.505043720589416
Experience 16, Iter 47, disc loss: 0.00022113924767348464, policy loss: 9.610547537122704
Experience 16, Iter 48, disc loss: 0.00022791641325778202, policy loss: 9.686769293043316
Experience 16, Iter 49, disc loss: 0.00019919799083878762, policy loss: 9.340696881941852
Experience 16, Iter 50, disc loss: 0.0003075626478114975, policy loss: 9.286209069146086
Experience 16, Iter 51, disc loss: 0.0003364931476822474, policy loss: 9.1375940987723
Experience 16, Iter 52, disc loss: 0.00022669398809303193, policy loss: 9.434622263656733
Experience 16, Iter 53, disc loss: 0.00021433794967577575, policy loss: 9.754226848116268
Experience 16, Iter 54, disc loss: 0.00022372998827616252, policy loss: 9.440357234945138
Experience 16, Iter 55, disc loss: 0.00018812429327800238, policy loss: 9.871367796868066
Experience 16, Iter 56, disc loss: 0.0002843780587273605, policy loss: 9.139226255171653
Experience 16, Iter 57, disc loss: 0.00030324367635485474, policy loss: 9.353820311748025
Experience 16, Iter 58, disc loss: 0.00017275840371388861, policy loss: 9.598250797190165
Experience 16, Iter 59, disc loss: 0.0002556087099557953, policy loss: 9.134200665845121
Experience 16, Iter 60, disc loss: 0.00025054049745675016, policy loss: 9.5808292621293
Experience 16, Iter 61, disc loss: 0.00017865332661860832, policy loss: 9.622883287571607
Experience 16, Iter 62, disc loss: 0.00022476700026147095, policy loss: 9.29739841408177
Experience 16, Iter 63, disc loss: 0.00021653087617248725, policy loss: 9.63806469491109
Experience 16, Iter 64, disc loss: 0.0001931936448293082, policy loss: 9.332868941632544
Experience 16, Iter 65, disc loss: 0.00016690770786631685, policy loss: 9.860907938209412
Experience 16, Iter 66, disc loss: 0.00020054486083600755, policy loss: 9.474452414512772
Experience 16, Iter 67, disc loss: 0.0002010639661544463, policy loss: 9.437436839955623
Experience 16, Iter 68, disc loss: 0.00018920642897821683, policy loss: 9.582917069537547
Experience 16, Iter 69, disc loss: 0.00019299532875435588, policy loss: 9.670004402597812
Experience 16, Iter 70, disc loss: 0.00017614007881309383, policy loss: 9.607639544278213
Experience 16, Iter 71, disc loss: 0.00016023503618523745, policy loss: 9.923207091864658
Experience 16, Iter 72, disc loss: 0.00015946873961618496, policy loss: 9.655407512668434
Experience 16, Iter 73, disc loss: 0.00022829929294752806, policy loss: 9.727373664805574
Experience 16, Iter 74, disc loss: 0.00018046264388690787, policy loss: 10.580706841491867
Experience 16, Iter 75, disc loss: 0.0001968544642803224, policy loss: 9.902798333783196
Experience 16, Iter 76, disc loss: 0.00016084553254492033, policy loss: 10.258571243800793
Experience 16, Iter 77, disc loss: 0.0001730179082316111, policy loss: 9.972915784682232
Experience 16, Iter 78, disc loss: 0.00015865970508907205, policy loss: 9.970345473407162
Experience 16, Iter 79, disc loss: 0.00026728597673310355, policy loss: 9.769074951354355
Experience 16, Iter 80, disc loss: 0.00026052430925196793, policy loss: 9.54018636092539
Experience 16, Iter 81, disc loss: 0.00015634583686372037, policy loss: 9.82352644093874
Experience 16, Iter 82, disc loss: 0.0002627228458419909, policy loss: 9.593105810157496
Experience 16, Iter 83, disc loss: 0.00015374891805854292, policy loss: 9.705146288677609
Experience 16, Iter 84, disc loss: 0.00018352168783376533, policy loss: 9.231587036071275
Experience 16, Iter 85, disc loss: 0.0001609449260948607, policy loss: 9.835996770278982
Experience 16, Iter 86, disc loss: 0.00019029903220103492, policy loss: 9.689258494063358
Experience 16, Iter 87, disc loss: 0.0002605032261093789, policy loss: 9.746342074897306
Experience 16, Iter 88, disc loss: 0.00021853690597892754, policy loss: 9.429668149017326
Experience 16, Iter 89, disc loss: 0.00022485329271717045, policy loss: 10.071426339872886
Experience 16, Iter 90, disc loss: 0.0002419763516186354, policy loss: 9.33580142075089
Experience 16, Iter 91, disc loss: 0.00014870031725595014, policy loss: 9.656797670204774
Experience 16, Iter 92, disc loss: 0.00023242113054635112, policy loss: 9.62712759110999
Experience 16, Iter 93, disc loss: 0.00021275176471717555, policy loss: 9.282093861401293
Experience 16, Iter 94, disc loss: 0.00019034288774275875, policy loss: 9.816249343270824
Experience 16, Iter 95, disc loss: 0.00025328502175286173, policy loss: 9.395332131875293
Experience 16, Iter 96, disc loss: 0.00017565793540852992, policy loss: 9.476766960210554
Experience 16, Iter 97, disc loss: 0.00017049824815670595, policy loss: 9.794751801521443
Experience 16, Iter 98, disc loss: 0.0002177519880670843, policy loss: 9.719568168524582
Experience 16, Iter 99, disc loss: 0.0002485324049224964, policy loss: 9.42752329613656
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0172],
        [0.1638],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.7462e-03, 3.1190e-02, 4.4155e-02, 1.6849e-03, 1.2097e-05,
          3.5778e-01]],

        [[7.7462e-03, 3.1190e-02, 4.4155e-02, 1.6849e-03, 1.2097e-05,
          3.5778e-01]],

        [[7.7462e-03, 3.1190e-02, 4.4155e-02, 1.6849e-03, 1.2097e-05,
          3.5778e-01]],

        [[7.7462e-03, 3.1190e-02, 4.4155e-02, 1.6849e-03, 1.2097e-05,
          3.5778e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0689, 0.6550, 0.0030], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0689, 0.6550, 0.0030])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.589
Iter 2/2000 - Loss: 0.957
Iter 3/2000 - Loss: -1.472
Iter 4/2000 - Loss: -1.116
Iter 5/2000 - Loss: -0.263
Iter 6/2000 - Loss: -0.532
Iter 7/2000 - Loss: -1.190
Iter 8/2000 - Loss: -1.524
Iter 9/2000 - Loss: -1.411
Iter 10/2000 - Loss: -1.120
Iter 11/2000 - Loss: -0.960
Iter 12/2000 - Loss: -1.030
Iter 13/2000 - Loss: -1.230
Iter 14/2000 - Loss: -1.406
Iter 15/2000 - Loss: -1.464
Iter 16/2000 - Loss: -1.404
Iter 17/2000 - Loss: -1.301
Iter 18/2000 - Loss: -1.245
Iter 19/2000 - Loss: -1.282
Iter 20/2000 - Loss: -1.388
Iter 1981/2000 - Loss: -9.033
Iter 1982/2000 - Loss: -9.033
Iter 1983/2000 - Loss: -9.033
Iter 1984/2000 - Loss: -9.033
Iter 1985/2000 - Loss: -9.033
Iter 1986/2000 - Loss: -9.033
Iter 1987/2000 - Loss: -9.033
Iter 1988/2000 - Loss: -9.033
Iter 1989/2000 - Loss: -9.033
Iter 1990/2000 - Loss: -9.033
Iter 1991/2000 - Loss: -9.033
Iter 1992/2000 - Loss: -9.033
Iter 1993/2000 - Loss: -9.033
Iter 1994/2000 - Loss: -9.033
Iter 1995/2000 - Loss: -9.033
Iter 1996/2000 - Loss: -9.033
Iter 1997/2000 - Loss: -9.033
Iter 1998/2000 - Loss: -9.033
Iter 1999/2000 - Loss: -9.033
Iter 2000/2000 - Loss: -9.033
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[16.2381,  3.7703, 31.8021, 12.5883,  1.5220, 27.6365]],

        [[19.4218, 31.3294, 26.8330,  2.8971,  3.9017, 16.2064]],

        [[18.1849, 42.1350, 25.4128,  1.7242,  4.1802, 24.1535]],

        [[22.0286, 37.7888,  9.0870,  2.6985,  5.2893, 28.8289]]])
Signal Variance: tensor([ 0.0360,  1.2097, 17.0949,  0.2010])
Estimated target variance: tensor([0.0033, 0.0689, 0.6550, 0.0030])
N: 170
Signal to noise ratio: tensor([11.5858, 59.6457, 84.6854, 27.5806])
Bound on condition number: tensor([  22820.0651,  604795.2728, 1219175.8388,  129318.5011])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0002444466879566451, policy loss: 9.527037431272536
Experience 17, Iter 1, disc loss: 0.00026957888502424305, policy loss: 9.92388051379867
Experience 17, Iter 2, disc loss: 0.0002909800808877077, policy loss: 9.489095390861895
Experience 17, Iter 3, disc loss: 0.00017132138440185515, policy loss: 9.903227005316442
Experience 17, Iter 4, disc loss: 0.000328506225423292, policy loss: 9.311124803143027
Experience 17, Iter 5, disc loss: 0.00023407932079484812, policy loss: 9.66586911338436
Experience 17, Iter 6, disc loss: 0.00022539415009863346, policy loss: 9.732686239780671
Experience 17, Iter 7, disc loss: 0.00021766303168329582, policy loss: 9.791469893653094
Experience 17, Iter 8, disc loss: 0.00021435804103544793, policy loss: 9.63104745686957
Experience 17, Iter 9, disc loss: 0.00029566134752957794, policy loss: 9.256333001692491
Experience 17, Iter 10, disc loss: 0.0002569052819147258, policy loss: 9.432221833306837
Experience 17, Iter 11, disc loss: 0.00017227278421699712, policy loss: 9.607343480104296
Experience 17, Iter 12, disc loss: 0.00023362683575502526, policy loss: 9.16981643729464
Experience 17, Iter 13, disc loss: 0.00022083741159464155, policy loss: 9.482269395392175
Experience 17, Iter 14, disc loss: 0.00015948136961681263, policy loss: 9.731064512880568
Experience 17, Iter 15, disc loss: 0.00046081382437711526, policy loss: 9.105787883729983
Experience 17, Iter 16, disc loss: 0.000226212593334898, policy loss: 9.475651887438268
Experience 17, Iter 17, disc loss: 0.00033153919034755933, policy loss: 9.755782972649044
Experience 17, Iter 18, disc loss: 0.0001650914363740979, policy loss: 10.0899968769175
Experience 17, Iter 19, disc loss: 0.0002815099786189498, policy loss: 9.191326923516439
Experience 17, Iter 20, disc loss: 0.0002840703816845243, policy loss: 9.35484272811009
Experience 17, Iter 21, disc loss: 0.00019825418984447548, policy loss: 9.545976433252825
Experience 17, Iter 22, disc loss: 0.00023384340317280118, policy loss: 9.774542201663007
Experience 17, Iter 23, disc loss: 0.0001919592575948285, policy loss: 9.562941256211069
Experience 17, Iter 24, disc loss: 0.0001959953153286093, policy loss: 9.894353319706159
Experience 17, Iter 25, disc loss: 0.00016147205126206273, policy loss: 9.942128035143346
Experience 17, Iter 26, disc loss: 0.00015501755983285097, policy loss: 10.233272728221916
Experience 17, Iter 27, disc loss: 0.00015541156708409915, policy loss: 9.96158785364833
Experience 17, Iter 28, disc loss: 0.00016308191573485126, policy loss: 9.848554499804228
Experience 17, Iter 29, disc loss: 0.000296452960744812, policy loss: 9.998805510263868
Experience 17, Iter 30, disc loss: 0.00014584033139199308, policy loss: 10.263850165667966
Experience 17, Iter 31, disc loss: 0.00019011293841716227, policy loss: 10.085052810787143
Experience 17, Iter 32, disc loss: 0.00018055923457162949, policy loss: 9.81129943007462
Experience 17, Iter 33, disc loss: 0.0001861807397943061, policy loss: 9.66071475197327
Experience 17, Iter 34, disc loss: 0.00018587134829669714, policy loss: 9.935202369047479
Experience 17, Iter 35, disc loss: 0.00022253743730955627, policy loss: 9.498491187437015
Experience 17, Iter 36, disc loss: 0.00020162236615952955, policy loss: 9.754664273872681
Experience 17, Iter 37, disc loss: 0.00018650350831754818, policy loss: 9.731158662585564
Experience 17, Iter 38, disc loss: 0.00019572941971715432, policy loss: 9.710205177149152
Experience 17, Iter 39, disc loss: 0.00020276096628543084, policy loss: 10.114582616775685
Experience 17, Iter 40, disc loss: 0.00013506495906985975, policy loss: 10.087100456907574
Experience 17, Iter 41, disc loss: 0.000182358248909096, policy loss: 10.11941423057992
Experience 17, Iter 42, disc loss: 0.00016613167525320697, policy loss: 9.770105431998777
Experience 17, Iter 43, disc loss: 0.00022775964941650192, policy loss: 9.50501636971115
Experience 17, Iter 44, disc loss: 0.00028474498766435874, policy loss: 10.170824332260578
Experience 17, Iter 45, disc loss: 0.00018829760650769027, policy loss: 9.939732102846294
Experience 17, Iter 46, disc loss: 0.0002000880583101774, policy loss: 9.85924494263766
Experience 17, Iter 47, disc loss: 0.0002282820739068132, policy loss: 9.592203802533053
Experience 17, Iter 48, disc loss: 0.0001545937538786384, policy loss: 9.867853980451807
Experience 17, Iter 49, disc loss: 0.00019225819142620784, policy loss: 9.748932480644488
Experience 17, Iter 50, disc loss: 0.0002172129376182751, policy loss: 9.386731896905223
Experience 17, Iter 51, disc loss: 0.00017823697189280058, policy loss: 9.59122911078065
Experience 17, Iter 52, disc loss: 0.0002295025861103809, policy loss: 9.374496791391763
Experience 17, Iter 53, disc loss: 0.0002760218155981369, policy loss: 9.869585105074897
Experience 17, Iter 54, disc loss: 0.0002493037513795354, policy loss: 9.06600739879195
Experience 17, Iter 55, disc loss: 0.0002724136135450862, policy loss: 9.282636568419793
Experience 17, Iter 56, disc loss: 0.00026846324435609345, policy loss: 9.40321808254581
Experience 17, Iter 57, disc loss: 0.00021735717735898134, policy loss: 9.910302650270788
Experience 17, Iter 58, disc loss: 0.0001946461006789617, policy loss: 9.850050002699103
Experience 17, Iter 59, disc loss: 0.00018163829892399534, policy loss: 10.257562508022769
Experience 17, Iter 60, disc loss: 0.00018293241433522858, policy loss: 9.865979316382449
Experience 17, Iter 61, disc loss: 0.00021758760340625136, policy loss: 9.708653010061935
Experience 17, Iter 62, disc loss: 0.00022523988702701273, policy loss: 9.74763115246048
Experience 17, Iter 63, disc loss: 0.00015291943670045212, policy loss: 9.813138398934381
Experience 17, Iter 64, disc loss: 0.00014767381387306708, policy loss: 10.04341231055707
Experience 17, Iter 65, disc loss: 0.00016323031191130952, policy loss: 10.032063102292323
Experience 17, Iter 66, disc loss: 0.00018904291070346432, policy loss: 9.664589580710414
Experience 17, Iter 67, disc loss: 0.00018536553472782368, policy loss: 9.606267782283354
Experience 17, Iter 68, disc loss: 0.0002637425628285757, policy loss: 9.613508412246016
Experience 17, Iter 69, disc loss: 0.0002354901535141226, policy loss: 9.83323895673974
Experience 17, Iter 70, disc loss: 0.00014713496577153972, policy loss: 10.294428908875544
Experience 17, Iter 71, disc loss: 0.0003180711342954107, policy loss: 9.263261245028861
Experience 17, Iter 72, disc loss: 0.0002583936505281976, policy loss: 9.47863027329992
Experience 17, Iter 73, disc loss: 0.00015321607679951083, policy loss: 9.68755724895123
Experience 17, Iter 74, disc loss: 0.00020244441020079366, policy loss: 9.714667170091545
Experience 17, Iter 75, disc loss: 0.00016052591998291338, policy loss: 10.141837831669356
Experience 17, Iter 76, disc loss: 0.0001649731598249374, policy loss: 9.831364265917305
Experience 17, Iter 77, disc loss: 0.00019492051101510836, policy loss: 9.626365804306971
Experience 17, Iter 78, disc loss: 0.00010699597370394321, policy loss: 10.526709209424086
Experience 17, Iter 79, disc loss: 0.0002097937227240665, policy loss: 9.652596045130554
Experience 17, Iter 80, disc loss: 0.00015071132601124896, policy loss: 10.172910922512594
Experience 17, Iter 81, disc loss: 0.00014695375323786362, policy loss: 10.277793909614045
Experience 17, Iter 82, disc loss: 0.000210386368545502, policy loss: 9.850175103331587
Experience 17, Iter 83, disc loss: 0.00025681274014552574, policy loss: 9.502633012556617
Experience 17, Iter 84, disc loss: 0.00013908213131948622, policy loss: 9.944878139602706
Experience 17, Iter 85, disc loss: 0.0002245909518743932, policy loss: 9.615719490051664
Experience 17, Iter 86, disc loss: 0.00017192678372068406, policy loss: 9.7846493969264
Experience 17, Iter 87, disc loss: 0.00021824437835938045, policy loss: 9.306260035959824
Experience 17, Iter 88, disc loss: 0.00019436141454602836, policy loss: 9.461609878801031
Experience 17, Iter 89, disc loss: 0.0001805570554240768, policy loss: 10.021262800804099
Experience 17, Iter 90, disc loss: 0.00032729312861228997, policy loss: 9.244770663082495
Experience 17, Iter 91, disc loss: 0.00017237399427389076, policy loss: 9.75423688119252
Experience 17, Iter 92, disc loss: 0.00013864051642753673, policy loss: 9.86590246949427
Experience 17, Iter 93, disc loss: 0.00022908448154220225, policy loss: 9.70360450745865
Experience 17, Iter 94, disc loss: 0.0002295467034956766, policy loss: 9.967779911754636
Experience 17, Iter 95, disc loss: 0.0001761754948142919, policy loss: 9.645922072954798
Experience 17, Iter 96, disc loss: 0.0002086195711432668, policy loss: 10.029235376254722
Experience 17, Iter 97, disc loss: 0.00021683972982370326, policy loss: 9.649542314687722
Experience 17, Iter 98, disc loss: 0.0002010896155468236, policy loss: 10.006540293445578
Experience 17, Iter 99, disc loss: 0.00018099028403691211, policy loss: 9.55482955177857
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0172],
        [0.1644],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.3995e-03, 3.0319e-02, 4.3717e-02, 1.6450e-03, 1.1556e-05,
          3.5395e-01]],

        [[7.3995e-03, 3.0319e-02, 4.3717e-02, 1.6450e-03, 1.1556e-05,
          3.5395e-01]],

        [[7.3995e-03, 3.0319e-02, 4.3717e-02, 1.6450e-03, 1.1556e-05,
          3.5395e-01]],

        [[7.3995e-03, 3.0319e-02, 4.3717e-02, 1.6450e-03, 1.1556e-05,
          3.5395e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0033, 0.0687, 0.6575, 0.0029], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0033, 0.0687, 0.6575, 0.0029])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.603
Iter 2/2000 - Loss: 0.913
Iter 3/2000 - Loss: -1.493
Iter 4/2000 - Loss: -1.135
Iter 5/2000 - Loss: -0.297
Iter 6/2000 - Loss: -0.579
Iter 7/2000 - Loss: -1.232
Iter 8/2000 - Loss: -1.547
Iter 9/2000 - Loss: -1.418
Iter 10/2000 - Loss: -1.127
Iter 11/2000 - Loss: -0.981
Iter 12/2000 - Loss: -1.068
Iter 13/2000 - Loss: -1.273
Iter 14/2000 - Loss: -1.440
Iter 15/2000 - Loss: -1.481
Iter 16/2000 - Loss: -1.409
Iter 17/2000 - Loss: -1.306
Iter 18/2000 - Loss: -1.262
Iter 19/2000 - Loss: -1.313
Iter 20/2000 - Loss: -1.425
Iter 1981/2000 - Loss: -9.026
Iter 1982/2000 - Loss: -9.026
Iter 1983/2000 - Loss: -9.027
Iter 1984/2000 - Loss: -9.027
Iter 1985/2000 - Loss: -9.027
Iter 1986/2000 - Loss: -9.027
Iter 1987/2000 - Loss: -9.027
Iter 1988/2000 - Loss: -9.027
Iter 1989/2000 - Loss: -9.027
Iter 1990/2000 - Loss: -9.027
Iter 1991/2000 - Loss: -9.027
Iter 1992/2000 - Loss: -9.027
Iter 1993/2000 - Loss: -9.027
Iter 1994/2000 - Loss: -9.027
Iter 1995/2000 - Loss: -9.027
Iter 1996/2000 - Loss: -9.027
Iter 1997/2000 - Loss: -9.027
Iter 1998/2000 - Loss: -9.027
Iter 1999/2000 - Loss: -9.027
Iter 2000/2000 - Loss: -9.027
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[15.8983,  3.6933, 30.6904, 14.4863,  1.4703, 27.2843]],

        [[19.0429, 30.4043, 26.2089,  2.9749,  3.8439, 16.4688]],

        [[20.0703, 46.0316, 26.0736,  1.7615,  4.4274, 24.5456]],

        [[21.1498, 35.0900,  9.2223,  2.7849,  5.2624, 30.0250]]])
Signal Variance: tensor([ 0.0342,  1.2539, 17.7032,  0.2133])
Estimated target variance: tensor([0.0033, 0.0687, 0.6575, 0.0029])
N: 180
Signal to noise ratio: tensor([10.9068, 61.0800, 86.6056, 28.1412])
Bound on condition number: tensor([  21413.5862,  671538.2486, 1350097.0383,  142547.7946])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.00013721454246453795, policy loss: 10.009740397690063
Experience 18, Iter 1, disc loss: 0.00018296311646532498, policy loss: 9.949293415739563
Experience 18, Iter 2, disc loss: 0.00026785032621257196, policy loss: 10.120274884118276
Experience 18, Iter 3, disc loss: 0.00020822169341126024, policy loss: 9.75444791088798
Experience 18, Iter 4, disc loss: 0.00032034952112708896, policy loss: 9.690050691028867
Experience 18, Iter 5, disc loss: 0.0001454661417429071, policy loss: 10.10729628517488
Experience 18, Iter 6, disc loss: 0.00015907206592233835, policy loss: 9.989081803281248
Experience 18, Iter 7, disc loss: 0.0002481985281472392, policy loss: 10.03112090020446
Experience 18, Iter 8, disc loss: 0.0001486244320547264, policy loss: 9.947686679696918
Experience 18, Iter 9, disc loss: 0.00018731925447471336, policy loss: 9.497423508081077
Experience 18, Iter 10, disc loss: 0.00023841978316003595, policy loss: 9.374539002340104
Experience 18, Iter 11, disc loss: 0.0002060550766723852, policy loss: 9.369017122900301
Experience 18, Iter 12, disc loss: 0.00022142502913605024, policy loss: 9.795181362053842
Experience 18, Iter 13, disc loss: 0.00022856550660029849, policy loss: 9.919933513426912
Experience 18, Iter 14, disc loss: 0.0002110155366574939, policy loss: 9.762084095001658
Experience 18, Iter 15, disc loss: 0.0002053974065387161, policy loss: 9.740045497835432
Experience 18, Iter 16, disc loss: 0.00017210692165413322, policy loss: 9.840661989891988
Experience 18, Iter 17, disc loss: 0.00011513726010554423, policy loss: 10.116616115070059
Experience 18, Iter 18, disc loss: 0.0003332107500162141, policy loss: 9.3008688094075
Experience 18, Iter 19, disc loss: 0.00029119830010519704, policy loss: 9.960053340619236
Experience 18, Iter 20, disc loss: 0.00029749543374367086, policy loss: 9.565043600945518
Experience 18, Iter 21, disc loss: 0.00023891176021654013, policy loss: 9.604252100068013
Experience 18, Iter 22, disc loss: 0.00018710400155249045, policy loss: 9.500320415857505
Experience 18, Iter 23, disc loss: 0.00014499702278288586, policy loss: 9.884491814950236
Experience 18, Iter 24, disc loss: 0.0003765172669714131, policy loss: 9.739265271323456
Experience 18, Iter 25, disc loss: 0.00022414579594185884, policy loss: 9.91019679714958
Experience 18, Iter 26, disc loss: 0.00031536472240700477, policy loss: 9.17727753197714
Experience 18, Iter 27, disc loss: 0.0001902969432914235, policy loss: 9.644193238307551
Experience 18, Iter 28, disc loss: 0.00023810164702788993, policy loss: 9.81092692455612
Experience 18, Iter 29, disc loss: 0.0002096321205358868, policy loss: 9.532460673646806
Experience 18, Iter 30, disc loss: 0.00014702561000200747, policy loss: 10.297267975901935
Experience 18, Iter 31, disc loss: 0.00018875877283152946, policy loss: 9.731895586297028
Experience 18, Iter 32, disc loss: 0.00019710386821846588, policy loss: 9.473774246962973
Experience 18, Iter 33, disc loss: 0.0001889236717460621, policy loss: 9.634061146959949
Experience 18, Iter 34, disc loss: 0.0001988948632551958, policy loss: 9.796303196382398
Experience 18, Iter 35, disc loss: 0.0002454190430782467, policy loss: 9.276934406942894
Experience 18, Iter 36, disc loss: 0.0002782959504889529, policy loss: 9.480322146249982
Experience 18, Iter 37, disc loss: 0.00019597010969589027, policy loss: 9.469331628085616
Experience 18, Iter 38, disc loss: 0.00022053883103066873, policy loss: 9.240567054886096
Experience 18, Iter 39, disc loss: 0.00023616723389713592, policy loss: 9.578314137506407
Experience 18, Iter 40, disc loss: 0.00026374627662125713, policy loss: 9.703278331071811
Experience 18, Iter 41, disc loss: 0.00030954050469663585, policy loss: 9.458578820041394
Experience 18, Iter 42, disc loss: 0.0001194602614884778, policy loss: 10.59795865032619
Experience 18, Iter 43, disc loss: 0.00015260354084547866, policy loss: 9.612203229455856
Experience 18, Iter 44, disc loss: 0.00018645492750063919, policy loss: 9.739105017980886
Experience 18, Iter 45, disc loss: 0.0002844653766785578, policy loss: 9.509743849178331
Experience 18, Iter 46, disc loss: 0.00023398724795062749, policy loss: 9.678449058157302
Experience 18, Iter 47, disc loss: 0.0002492676010572364, policy loss: 9.861456285394928
Experience 18, Iter 48, disc loss: 0.0003951136640812932, policy loss: 9.558831184773624
Experience 18, Iter 49, disc loss: 0.00019581604145144755, policy loss: 9.593090581854998
Experience 18, Iter 50, disc loss: 0.00024351976458445064, policy loss: 9.688312316812986
Experience 18, Iter 51, disc loss: 0.00020280787402267436, policy loss: 9.587038140428307
Experience 18, Iter 52, disc loss: 0.00020535582527394882, policy loss: 9.451034371564644
Experience 18, Iter 53, disc loss: 0.00016533874298022867, policy loss: 9.71484714707487
Experience 18, Iter 54, disc loss: 0.000241636388260489, policy loss: 9.990039445631329
Experience 18, Iter 55, disc loss: 0.00025088581078814664, policy loss: 9.548387228757864
Experience 18, Iter 56, disc loss: 0.0003166445135034728, policy loss: 9.74902726227465
Experience 18, Iter 57, disc loss: 0.00013866226147804409, policy loss: 10.054348815198551
Experience 18, Iter 58, disc loss: 0.000254005658658818, policy loss: 9.87440982876971
Experience 18, Iter 59, disc loss: 0.0002510986267791765, policy loss: 10.07465944827326
Experience 18, Iter 60, disc loss: 0.000405323558821976, policy loss: 9.705641642505476
Experience 18, Iter 61, disc loss: 0.0001912565226257622, policy loss: 9.967381266697162
Experience 18, Iter 62, disc loss: 0.0001973792652339718, policy loss: 10.127193257181142
Experience 18, Iter 63, disc loss: 0.0002027869130694674, policy loss: 10.259988233684535
Experience 18, Iter 64, disc loss: 0.00031390810855168983, policy loss: 9.482512538414197
Experience 18, Iter 65, disc loss: 0.0002988401497710606, policy loss: 9.408735133975197
Experience 18, Iter 66, disc loss: 0.0001724433086296945, policy loss: 9.994283408816951
Experience 18, Iter 67, disc loss: 0.0002345102761576992, policy loss: 10.233422392818277
Experience 18, Iter 68, disc loss: 0.0001236680849275639, policy loss: 10.326472824058975
Experience 18, Iter 69, disc loss: 0.0001696576440845216, policy loss: 9.97606081588222
Experience 18, Iter 70, disc loss: 0.00015776067093041648, policy loss: 9.759914244515272
Experience 18, Iter 71, disc loss: 0.0003149195276993665, policy loss: 9.903805788697186
Experience 18, Iter 72, disc loss: 0.00023502668024997237, policy loss: 9.70663466109231
Experience 18, Iter 73, disc loss: 0.0003146113331975594, policy loss: 9.286380944527767
Experience 18, Iter 74, disc loss: 0.0001922548158270223, policy loss: 9.891366314933865
Experience 18, Iter 75, disc loss: 0.00015705417024187442, policy loss: 10.160715036475288
Experience 18, Iter 76, disc loss: 0.00018481016677660033, policy loss: 10.028973005574352
Experience 18, Iter 77, disc loss: 0.00016527120979147657, policy loss: 9.937023801653055
Experience 18, Iter 78, disc loss: 0.0002674975435909524, policy loss: 9.511546464188369
Experience 18, Iter 79, disc loss: 0.00023450601561562166, policy loss: 9.89500091599957
Experience 18, Iter 80, disc loss: 0.00025214081585197206, policy loss: 9.518537355117058
Experience 18, Iter 81, disc loss: 0.00013674516876778018, policy loss: 10.2533488844232
Experience 18, Iter 82, disc loss: 0.00027738866655746265, policy loss: 9.683363873151572
Experience 18, Iter 83, disc loss: 0.0004324368995488833, policy loss: 9.405460197062563
Experience 18, Iter 84, disc loss: 0.00023845639895107498, policy loss: 9.599729932921441
Experience 18, Iter 85, disc loss: 0.0004566802159165415, policy loss: 9.595494464249057
Experience 18, Iter 86, disc loss: 0.00026132830241063393, policy loss: 9.638768102381851
Experience 18, Iter 87, disc loss: 0.0003581917075361624, policy loss: 9.493789069130484
Experience 18, Iter 88, disc loss: 0.00016937494305463815, policy loss: 9.822724781228459
Experience 18, Iter 89, disc loss: 0.00027940972780555535, policy loss: 9.848087529614972
Experience 18, Iter 90, disc loss: 0.000218644820550303, policy loss: 9.916399098618761
Experience 18, Iter 91, disc loss: 0.00023690811234678864, policy loss: 10.255410589930305
Experience 18, Iter 92, disc loss: 0.00034339630050476245, policy loss: 9.788255927808207
Experience 18, Iter 93, disc loss: 0.00015742314619901496, policy loss: 9.960003006374354
Experience 18, Iter 94, disc loss: 0.0002468407889207016, policy loss: 9.807458938793967
Experience 18, Iter 95, disc loss: 0.00023120444494056585, policy loss: 9.710647462536281
Experience 18, Iter 96, disc loss: 0.00017139609821346154, policy loss: 9.905609507686657
Experience 18, Iter 97, disc loss: 0.00022562008067848768, policy loss: 9.881581701599417
Experience 18, Iter 98, disc loss: 0.0002574925848325458, policy loss: 9.588682417022888
Experience 18, Iter 99, disc loss: 0.0002601237126657251, policy loss: 9.530594373539769
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0188],
        [0.1804],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[7.0916e-03, 2.9488e-02, 4.5116e-02, 1.5883e-03, 1.1071e-05,
          3.7944e-01]],

        [[7.0916e-03, 2.9488e-02, 4.5116e-02, 1.5883e-03, 1.1071e-05,
          3.7944e-01]],

        [[7.0916e-03, 2.9488e-02, 4.5116e-02, 1.5883e-03, 1.1071e-05,
          3.7944e-01]],

        [[7.0916e-03, 2.9488e-02, 4.5116e-02, 1.5883e-03, 1.1071e-05,
          3.7944e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0031, 0.0754, 0.7215, 0.0029], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0031, 0.0754, 0.7215, 0.0029])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.531
Iter 2/2000 - Loss: 0.991
Iter 3/2000 - Loss: -1.425
Iter 4/2000 - Loss: -1.064
Iter 5/2000 - Loss: -0.227
Iter 6/2000 - Loss: -0.518
Iter 7/2000 - Loss: -1.174
Iter 8/2000 - Loss: -1.482
Iter 9/2000 - Loss: -1.343
Iter 10/2000 - Loss: -1.049
Iter 11/2000 - Loss: -0.911
Iter 12/2000 - Loss: -1.006
Iter 13/2000 - Loss: -1.215
Iter 14/2000 - Loss: -1.378
Iter 15/2000 - Loss: -1.412
Iter 16/2000 - Loss: -1.332
Iter 17/2000 - Loss: -1.227
Iter 18/2000 - Loss: -1.190
Iter 19/2000 - Loss: -1.249
Iter 20/2000 - Loss: -1.363
Iter 1981/2000 - Loss: -9.047
Iter 1982/2000 - Loss: -9.047
Iter 1983/2000 - Loss: -9.047
Iter 1984/2000 - Loss: -9.047
Iter 1985/2000 - Loss: -9.047
Iter 1986/2000 - Loss: -9.047
Iter 1987/2000 - Loss: -9.047
Iter 1988/2000 - Loss: -9.047
Iter 1989/2000 - Loss: -9.047
Iter 1990/2000 - Loss: -9.047
Iter 1991/2000 - Loss: -9.047
Iter 1992/2000 - Loss: -9.047
Iter 1993/2000 - Loss: -9.047
Iter 1994/2000 - Loss: -9.047
Iter 1995/2000 - Loss: -9.047
Iter 1996/2000 - Loss: -9.047
Iter 1997/2000 - Loss: -9.048
Iter 1998/2000 - Loss: -9.048
Iter 1999/2000 - Loss: -9.048
Iter 2000/2000 - Loss: -9.048
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[15.8032,  3.8960, 31.2506, 14.6851,  1.6234, 29.3174]],

        [[19.5453, 29.4870, 29.3817,  3.3608,  4.1421, 19.4691]],

        [[16.6009, 38.2501, 27.0886,  1.7709,  4.4399, 25.2488]],

        [[19.9658, 35.7399,  8.7296,  2.6583,  5.0035, 29.0236]]])
Signal Variance: tensor([ 0.0370,  1.5310, 17.5045,  0.1859])
Estimated target variance: tensor([0.0031, 0.0754, 0.7215, 0.0029])
N: 190
Signal to noise ratio: tensor([11.4568, 67.6257, 86.0620, 26.0059])
Bound on condition number: tensor([  24940.1129,  868914.5765, 1407268.1142,  128499.5045])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.000274688393639454, policy loss: 9.451831205560165
Experience 19, Iter 1, disc loss: 0.00028029541317092555, policy loss: 9.743869962835701
Experience 19, Iter 2, disc loss: 0.0002486959089990855, policy loss: 9.44768203269711
Experience 19, Iter 3, disc loss: 0.00016907161939120594, policy loss: 10.005968107217795
Experience 19, Iter 4, disc loss: 0.00027049746600501227, policy loss: 10.29175068573451
Experience 19, Iter 5, disc loss: 0.00038682324967789035, policy loss: 9.326661707491606
Experience 19, Iter 6, disc loss: 0.000287757807213533, policy loss: 9.803131586580367
Experience 19, Iter 7, disc loss: 0.00043096901789040174, policy loss: 9.24023734326052
Experience 19, Iter 8, disc loss: 0.0002216972294245623, policy loss: 9.811546524287923
Experience 19, Iter 9, disc loss: 0.00044188390701244005, policy loss: 9.708712490532122
Experience 19, Iter 10, disc loss: 0.00033945113294928363, policy loss: 9.812268460830056
Experience 19, Iter 11, disc loss: 0.0002178207176065109, policy loss: 10.023862074392127
Experience 19, Iter 12, disc loss: 0.00022793014732167433, policy loss: 9.755102694039673
Experience 19, Iter 13, disc loss: 0.00038733224202833146, policy loss: 9.546629706172606
Experience 19, Iter 14, disc loss: 0.0005009463911409717, policy loss: 9.803727135321246
Experience 19, Iter 15, disc loss: 0.0003537822412412733, policy loss: 9.663744743382264
Experience 19, Iter 16, disc loss: 0.00017271388702498006, policy loss: 10.042521067897676
Experience 19, Iter 17, disc loss: 0.0002497813675930416, policy loss: 10.076779770797403
Experience 19, Iter 18, disc loss: 0.0002861719736934739, policy loss: 9.724780026363376
Experience 19, Iter 19, disc loss: 0.00028949003932707604, policy loss: 9.972712006838
Experience 19, Iter 20, disc loss: 0.0002787809651998517, policy loss: 9.666444753042411
Experience 19, Iter 21, disc loss: 0.0002350734990218346, policy loss: 10.184339683706327
Experience 19, Iter 22, disc loss: 0.0003375187219608759, policy loss: 9.855774555325407
Experience 19, Iter 23, disc loss: 0.00043544193077087184, policy loss: 10.008251529868476
Experience 19, Iter 24, disc loss: 0.00020555232264243942, policy loss: 10.02283456413441
Experience 19, Iter 25, disc loss: 0.00017853127431600915, policy loss: 10.287610810043589
Experience 19, Iter 26, disc loss: 0.00029748113689531024, policy loss: 9.875592210771604
Experience 19, Iter 27, disc loss: 0.0002725344663758251, policy loss: 9.457059077264146
Experience 19, Iter 28, disc loss: 0.0002607042909860863, policy loss: 10.061046455127501
Experience 19, Iter 29, disc loss: 0.00028579140311338005, policy loss: 9.542824592966582
Experience 19, Iter 30, disc loss: 0.0003166142222380539, policy loss: 9.967022964879625
Experience 19, Iter 31, disc loss: 0.0003822167022852149, policy loss: 9.544546301961795
Experience 19, Iter 32, disc loss: 0.0003326990751885315, policy loss: 9.830029037748218
Experience 19, Iter 33, disc loss: 0.00040720800064533106, policy loss: 9.796868058923977
Experience 19, Iter 34, disc loss: 0.00016470797835092647, policy loss: 10.16449808778155
Experience 19, Iter 35, disc loss: 0.0002114047229335964, policy loss: 9.545683108417311
Experience 19, Iter 36, disc loss: 0.00033463956424092635, policy loss: 10.257343537365621
Experience 19, Iter 37, disc loss: 0.0003456069535538869, policy loss: 9.383470257953046
Experience 19, Iter 38, disc loss: 0.0004023185098363453, policy loss: 9.720520793750879
Experience 19, Iter 39, disc loss: 0.00026984184382657816, policy loss: 10.164408064012777
Experience 19, Iter 40, disc loss: 0.0003604429677378372, policy loss: 9.902616338087018
Experience 19, Iter 41, disc loss: 0.0002920057226005964, policy loss: 9.739576565020862
Experience 19, Iter 42, disc loss: 0.00031985721458881037, policy loss: 9.92716380116511
Experience 19, Iter 43, disc loss: 0.0003164799199558579, policy loss: 9.778754545997668
Experience 19, Iter 44, disc loss: 0.0001786013391557752, policy loss: 10.094915338595644
Experience 19, Iter 45, disc loss: 0.0003402751250287946, policy loss: 9.979025061208393
Experience 19, Iter 46, disc loss: 0.00015180241805665782, policy loss: 9.939892834699487
Experience 19, Iter 47, disc loss: 0.00029175427145325264, policy loss: 10.110536263922885
Experience 19, Iter 48, disc loss: 0.0003119040215324943, policy loss: 9.987878874656229
Experience 19, Iter 49, disc loss: 0.0004387938044381659, policy loss: 10.128171004102605
Experience 19, Iter 50, disc loss: 0.00020932153933134003, policy loss: 10.058053739005668
Experience 19, Iter 51, disc loss: 0.0002564688342452131, policy loss: 10.064696525138846
Experience 19, Iter 52, disc loss: 0.0002721191010208332, policy loss: 9.90567955310907
Experience 19, Iter 53, disc loss: 0.00024500820733350194, policy loss: 10.001472080068352
Experience 19, Iter 54, disc loss: 0.00026258542497833564, policy loss: 10.41368009503701
Experience 19, Iter 55, disc loss: 0.00022782846474182577, policy loss: 9.910609185963482
Experience 19, Iter 56, disc loss: 0.00030117474659508515, policy loss: 9.656383163566614
Experience 19, Iter 57, disc loss: 0.00017852652820737577, policy loss: 10.115846240780769
Experience 19, Iter 58, disc loss: 0.00032253268703759245, policy loss: 9.887258768695807
Experience 19, Iter 59, disc loss: 0.0003177595394699146, policy loss: 10.184381060617186
Experience 19, Iter 60, disc loss: 0.00036316407650920443, policy loss: 9.876008502199277
Experience 19, Iter 61, disc loss: 0.0002917202892907562, policy loss: 9.626736842988525
Experience 19, Iter 62, disc loss: 0.00041795504144165926, policy loss: 9.874210216326794
Experience 19, Iter 63, disc loss: 0.00034544143419333145, policy loss: 9.668977656362644
Experience 19, Iter 64, disc loss: 0.00034024604502582844, policy loss: 9.586760366985697
Experience 19, Iter 65, disc loss: 0.0002525352801562429, policy loss: 10.33861230802668
Experience 19, Iter 66, disc loss: 0.00028088406321200634, policy loss: 9.877664613615313
Experience 19, Iter 67, disc loss: 0.00016341470856665383, policy loss: 10.342474275082683
Experience 19, Iter 68, disc loss: 0.0003552587572853139, policy loss: 10.014840362817102
Experience 19, Iter 69, disc loss: 0.0003235781406581139, policy loss: 10.208325438223607
Experience 19, Iter 70, disc loss: 0.000595792390762058, policy loss: 9.64047942815938
Experience 19, Iter 71, disc loss: 0.0003800798305072175, policy loss: 10.012015388575847
Experience 19, Iter 72, disc loss: 0.0003267900376425696, policy loss: 9.88135744899558
Experience 19, Iter 73, disc loss: 0.0003442495553563822, policy loss: 9.883510758150386
Experience 19, Iter 74, disc loss: 0.0001977237987912085, policy loss: 9.8792869260151
Experience 19, Iter 75, disc loss: 0.00030000155958919073, policy loss: 9.914652671517327
Experience 19, Iter 76, disc loss: 0.00022163150946099977, policy loss: 9.945476397017524
Experience 19, Iter 77, disc loss: 0.00025706567742332377, policy loss: 10.238019156217112
Experience 19, Iter 78, disc loss: 0.00022511970173550843, policy loss: 10.04187226148081
Experience 19, Iter 79, disc loss: 0.0001806684668756693, policy loss: 10.257297590467108
Experience 19, Iter 80, disc loss: 0.00017124009833939153, policy loss: 10.116877022667211
Experience 19, Iter 81, disc loss: 0.00016217408631930187, policy loss: 10.414267042117864
Experience 19, Iter 82, disc loss: 0.00028512527071879873, policy loss: 10.026536007866252
Experience 19, Iter 83, disc loss: 0.00045303453435424347, policy loss: 9.74419069259929
Experience 19, Iter 84, disc loss: 0.0005286119467204692, policy loss: 9.593253338487498
Experience 19, Iter 85, disc loss: 0.0002925898450951463, policy loss: 10.289306574797468
Experience 19, Iter 86, disc loss: 0.00036140401928446504, policy loss: 9.782306536296097
Experience 19, Iter 87, disc loss: 0.0002200826979315485, policy loss: 10.575256979448515
Experience 19, Iter 88, disc loss: 0.00015959264082617726, policy loss: 10.711646337092807
Experience 19, Iter 89, disc loss: 0.00035412092521941094, policy loss: 10.370928392795722
Experience 19, Iter 90, disc loss: 0.00024504676468390936, policy loss: 10.13086903945199
Experience 19, Iter 91, disc loss: 0.00019508095060058624, policy loss: 10.355635437800101
Experience 19, Iter 92, disc loss: 0.0004261312866114014, policy loss: 10.189060165934041
Experience 19, Iter 93, disc loss: 0.0004021797641885017, policy loss: 10.225651400100329
Experience 19, Iter 94, disc loss: 0.0006872617082808073, policy loss: 10.345154591915819
Experience 19, Iter 95, disc loss: 0.00018994463873489556, policy loss: 10.155689172963012
Experience 19, Iter 96, disc loss: 0.0003249985728791878, policy loss: 10.56796412044541
Experience 19, Iter 97, disc loss: 0.0003007407071581477, policy loss: 10.448017579995337
Experience 19, Iter 98, disc loss: 0.0005506766770659408, policy loss: 10.27890551441999
Experience 19, Iter 99, disc loss: 0.0003134667280589101, policy loss: 10.32971144403999
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0215],
        [0.2058],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.7419e-03, 2.8537e-02, 4.7296e-02, 1.5483e-03, 1.0773e-05,
          4.2122e-01]],

        [[6.7419e-03, 2.8537e-02, 4.7296e-02, 1.5483e-03, 1.0773e-05,
          4.2122e-01]],

        [[6.7419e-03, 2.8537e-02, 4.7296e-02, 1.5483e-03, 1.0773e-05,
          4.2122e-01]],

        [[6.7419e-03, 2.8537e-02, 4.7296e-02, 1.5483e-03, 1.0773e-05,
          4.2122e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0030, 0.0861, 0.8230, 0.0028], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0030, 0.0861, 0.8230, 0.0028])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.429
Iter 2/2000 - Loss: 1.161
Iter 3/2000 - Loss: -1.318
Iter 4/2000 - Loss: -0.946
Iter 5/2000 - Loss: -0.089
Iter 6/2000 - Loss: -0.389
Iter 7/2000 - Loss: -1.063
Iter 8/2000 - Loss: -1.379
Iter 9/2000 - Loss: -1.235
Iter 10/2000 - Loss: -0.932
Iter 11/2000 - Loss: -0.789
Iter 12/2000 - Loss: -0.889
Iter 13/2000 - Loss: -1.105
Iter 14/2000 - Loss: -1.272
Iter 15/2000 - Loss: -1.305
Iter 16/2000 - Loss: -1.222
Iter 17/2000 - Loss: -1.113
Iter 18/2000 - Loss: -1.072
Iter 19/2000 - Loss: -1.130
Iter 20/2000 - Loss: -1.247
Iter 1981/2000 - Loss: -9.055
Iter 1982/2000 - Loss: -9.055
Iter 1983/2000 - Loss: -9.055
Iter 1984/2000 - Loss: -9.055
Iter 1985/2000 - Loss: -9.055
Iter 1986/2000 - Loss: -9.055
Iter 1987/2000 - Loss: -9.055
Iter 1988/2000 - Loss: -9.055
Iter 1989/2000 - Loss: -9.055
Iter 1990/2000 - Loss: -9.055
Iter 1991/2000 - Loss: -9.055
Iter 1992/2000 - Loss: -9.055
Iter 1993/2000 - Loss: -9.055
Iter 1994/2000 - Loss: -9.055
Iter 1995/2000 - Loss: -9.055
Iter 1996/2000 - Loss: -9.056
Iter 1997/2000 - Loss: -9.056
Iter 1998/2000 - Loss: -9.056
Iter 1999/2000 - Loss: -9.056
Iter 2000/2000 - Loss: -9.056
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[15.8077,  3.9497, 31.1463, 13.5648,  2.0987, 26.9376]],

        [[19.4525, 27.6421, 28.9968,  3.5131,  4.1753, 20.1041]],

        [[15.0186, 30.8516, 25.6878,  1.6689,  4.0641, 24.1163]],

        [[19.5360, 34.6936,  9.0161,  2.7185,  5.1128, 29.7425]]])
Signal Variance: tensor([ 0.0367,  1.6212, 15.1057,  0.1949])
Estimated target variance: tensor([0.0030, 0.0861, 0.8230, 0.0028])
N: 200
Signal to noise ratio: tensor([11.2632, 69.7349, 80.2420, 26.6509])
Bound on condition number: tensor([  25372.9786,  972593.1837, 1287758.0057,  142054.7773])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0003888018347137982, policy loss: 10.528863939967726
Experience 20, Iter 1, disc loss: 0.0003090716747149424, policy loss: 10.559877567134588
Experience 20, Iter 2, disc loss: 0.0005857999208835943, policy loss: 9.952105913811344
Experience 20, Iter 3, disc loss: 0.0003597554936923691, policy loss: 10.787223638494421
Experience 20, Iter 4, disc loss: 0.0003290902132133834, policy loss: 10.514484133320316
Experience 20, Iter 5, disc loss: 0.00020496455498303736, policy loss: 10.391985332524461
Experience 20, Iter 6, disc loss: 0.0003774952569453691, policy loss: 9.877647833235011
Experience 20, Iter 7, disc loss: 0.00031953838958267804, policy loss: 9.939152300172381
Experience 20, Iter 8, disc loss: 0.00021342424135893542, policy loss: 10.160618876876455
Experience 20, Iter 9, disc loss: 0.00021734163803641484, policy loss: 10.105729411731023
Experience 20, Iter 10, disc loss: 0.0003228266511814667, policy loss: 10.150572729539643
Experience 20, Iter 11, disc loss: 0.00043323141821287764, policy loss: 9.924827214396203
Experience 20, Iter 12, disc loss: 0.0001555474179285806, policy loss: 10.53082573381538
Experience 20, Iter 13, disc loss: 0.00017982881662419073, policy loss: 10.055183979446056
Experience 20, Iter 14, disc loss: 0.00030194618797388563, policy loss: 10.448739696505903
Experience 20, Iter 15, disc loss: 0.00020121880173823355, policy loss: 10.254896697267139
Experience 20, Iter 16, disc loss: 0.00038813738007855775, policy loss: 10.178359991040896
Experience 20, Iter 17, disc loss: 0.000385008982981513, policy loss: 10.146262300408793
Experience 20, Iter 18, disc loss: 0.00024039806672735705, policy loss: 10.190639827010298
Experience 20, Iter 19, disc loss: 0.0004316904753019488, policy loss: 10.522494121149045
Experience 20, Iter 20, disc loss: 0.000361452929209018, policy loss: 10.106885483892665
Experience 20, Iter 21, disc loss: 0.0004266949625780709, policy loss: 10.744255171939846
Experience 20, Iter 22, disc loss: 0.0006620501506397442, policy loss: 10.647934762824935
Experience 20, Iter 23, disc loss: 0.0003163637345607929, policy loss: 10.556414543948303
Experience 20, Iter 24, disc loss: 0.00041596053718357794, policy loss: 10.45724553681521
Experience 20, Iter 25, disc loss: 0.0004170010435902299, policy loss: 10.018395846013824
Experience 20, Iter 26, disc loss: 0.0004703908915581307, policy loss: 9.863201764909126
Experience 20, Iter 27, disc loss: 0.0004804400674472385, policy loss: 10.09050096769355
Experience 20, Iter 28, disc loss: 0.0004149346671431889, policy loss: 10.408931659778922
Experience 20, Iter 29, disc loss: 0.000977270493629273, policy loss: 9.632345073190752
Experience 20, Iter 30, disc loss: 0.0006945834295419649, policy loss: 9.902232319233603
Experience 20, Iter 31, disc loss: 0.0005027712804933915, policy loss: 9.936101084745395
Experience 20, Iter 32, disc loss: 0.00047339776351472385, policy loss: 10.628478173346666
Experience 20, Iter 33, disc loss: 0.0006065186010639542, policy loss: 10.151863193445902
Experience 20, Iter 34, disc loss: 0.00045501735202349396, policy loss: 10.196245840156227
Experience 20, Iter 35, disc loss: 0.000486559857973379, policy loss: 10.807614010859648
Experience 20, Iter 36, disc loss: 0.0004057581538221562, policy loss: 10.602719625046381
Experience 20, Iter 37, disc loss: 0.00037399536107227214, policy loss: 10.434794731421931
Experience 20, Iter 38, disc loss: 0.0005054870880713249, policy loss: 10.164709303328998
Experience 20, Iter 39, disc loss: 0.0001404226605491732, policy loss: 10.782901918771774
Experience 20, Iter 40, disc loss: 0.0005008561380600397, policy loss: 10.360625483367265
Experience 20, Iter 41, disc loss: 0.00030472926369963407, policy loss: 10.450790090698304
Experience 20, Iter 42, disc loss: 0.0004907709487945071, policy loss: 10.387992177177539
Experience 20, Iter 43, disc loss: 0.0003893733632855149, policy loss: 10.452791592544568
Experience 20, Iter 44, disc loss: 0.0003972851335236787, policy loss: 10.773557292486938
Experience 20, Iter 45, disc loss: 0.0001227003855210085, policy loss: 10.870341808154409
Experience 20, Iter 46, disc loss: 0.0003138352291081053, policy loss: 10.424748494541774
Experience 20, Iter 47, disc loss: 0.00017452923083653411, policy loss: 11.12362745069331
Experience 20, Iter 48, disc loss: 0.00019918045944859112, policy loss: 10.280113405151647
Experience 20, Iter 49, disc loss: 0.0005210362937186597, policy loss: 10.345472210325378
Experience 20, Iter 50, disc loss: 0.0003307770839983419, policy loss: 9.894072933175856
Experience 20, Iter 51, disc loss: 0.0006322592091885427, policy loss: 10.153032295933874
Experience 20, Iter 52, disc loss: 0.0002957137170598599, policy loss: 10.497381364862385
Experience 20, Iter 53, disc loss: 0.0006832550023434409, policy loss: 10.025549436042716
Experience 20, Iter 54, disc loss: 0.0006941202940528774, policy loss: 10.725297620420589
Experience 20, Iter 55, disc loss: 0.00043298003314837485, policy loss: 10.351781788084025
Experience 20, Iter 56, disc loss: 0.0017138846635413164, policy loss: 10.204429597427492
Experience 20, Iter 57, disc loss: 0.0009195584146600459, policy loss: 10.455889002242994
Experience 20, Iter 58, disc loss: 0.0003360235795623027, policy loss: 10.30606444917078
Experience 20, Iter 59, disc loss: 0.0002611184644778706, policy loss: 11.157924205557787
Experience 20, Iter 60, disc loss: 0.000640859055302617, policy loss: 10.076973068333746
Experience 20, Iter 61, disc loss: 0.0004707833839426015, policy loss: 10.546291788129817
Experience 20, Iter 62, disc loss: 0.00041896978860839575, policy loss: 10.793145638092302
Experience 20, Iter 63, disc loss: 0.00033094830292845545, policy loss: 11.264655682768522
Experience 20, Iter 64, disc loss: 0.0004710071811927399, policy loss: 10.890983917991067
Experience 20, Iter 65, disc loss: 0.000932773881320394, policy loss: 10.079359609948472
Experience 20, Iter 66, disc loss: 0.0005431154815761467, policy loss: 10.433769482278198
Experience 20, Iter 67, disc loss: 0.000702742528263242, policy loss: 10.487085311463117
Experience 20, Iter 68, disc loss: 0.0008890495633530471, policy loss: 10.632262309238772
Experience 20, Iter 69, disc loss: 0.0026038062008403116, policy loss: 10.191967791488533
Experience 20, Iter 70, disc loss: 0.004396471057822979, policy loss: 10.455209566853403
Experience 20, Iter 71, disc loss: 0.0006461825846690065, policy loss: 9.910855164936951
Experience 20, Iter 72, disc loss: 0.0013542962038686865, policy loss: 10.848318460903068
Experience 20, Iter 73, disc loss: 0.001951296451473284, policy loss: 10.29322814651135
Experience 20, Iter 74, disc loss: 0.00264336973345452, policy loss: 10.204412671173618
Experience 20, Iter 75, disc loss: 0.0015002130621376288, policy loss: 10.371456429057897
Experience 20, Iter 76, disc loss: 0.0007866948249337234, policy loss: 10.773849413406236
Experience 20, Iter 77, disc loss: 0.0006362128722885415, policy loss: 11.223693901680928
Experience 20, Iter 78, disc loss: 0.000644656571811651, policy loss: 10.465051453949018
Experience 20, Iter 79, disc loss: 0.0004637228588866188, policy loss: 10.862704340537901
Experience 20, Iter 80, disc loss: 0.0009742384260420241, policy loss: 10.135998136855914
Experience 20, Iter 81, disc loss: 0.0014376550549732396, policy loss: 10.827049869011702
Experience 20, Iter 82, disc loss: 0.0006455540983584988, policy loss: 10.688437349123816
Experience 20, Iter 83, disc loss: 0.0007237827789489664, policy loss: 10.750420914810238
Experience 20, Iter 84, disc loss: 0.0015677075173956845, policy loss: 10.477254063111875
Experience 20, Iter 85, disc loss: 0.0008906644409043449, policy loss: 10.56731079715832
Experience 20, Iter 86, disc loss: 0.0005432710162601575, policy loss: 11.086223218392856
Experience 20, Iter 87, disc loss: 0.0006209121827751539, policy loss: 11.165543493184078
Experience 20, Iter 88, disc loss: 0.0011524242357762661, policy loss: 11.239421106825874
Experience 20, Iter 89, disc loss: 0.0009459611069634432, policy loss: 11.454574562248174
Experience 20, Iter 90, disc loss: 0.0005689748822241634, policy loss: 11.065777298959507
Experience 20, Iter 91, disc loss: 0.0007748986738223545, policy loss: 11.213808341608335
Experience 20, Iter 92, disc loss: 0.0006073244963714073, policy loss: 11.439204190195515
Experience 20, Iter 93, disc loss: 0.0010994904639987325, policy loss: 11.069324190301561
Experience 20, Iter 94, disc loss: 0.0008838483655905726, policy loss: 10.430600814013445
Experience 20, Iter 95, disc loss: 0.0015743682078706973, policy loss: 10.339966272748747
Experience 20, Iter 96, disc loss: 0.001726130947535541, policy loss: 10.827223282010731
Experience 20, Iter 97, disc loss: 0.0003615457082526719, policy loss: 11.81410121272636
Experience 20, Iter 98, disc loss: 0.0004637230363639924, policy loss: 11.624991731470306
Experience 20, Iter 99, disc loss: 0.00042113339888409277, policy loss: 12.133705378117664
