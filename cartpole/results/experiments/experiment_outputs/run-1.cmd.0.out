Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0104],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0010, 0.0008, 0.0417, 0.0008], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0010, 0.0008, 0.0417, 0.0008])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.504
Iter 2/2000 - Loss: 2.264
Iter 3/2000 - Loss: -5.315
Iter 4/2000 - Loss: -5.418
Iter 5/2000 - Loss: -2.571
Iter 6/2000 - Loss: -2.904
Iter 7/2000 - Loss: -4.997
Iter 8/2000 - Loss: -6.262
Iter 9/2000 - Loss: -6.006
Iter 10/2000 - Loss: -5.090
Iter 11/2000 - Loss: -4.598
Iter 12/2000 - Loss: -4.853
Iter 13/2000 - Loss: -5.466
Iter 14/2000 - Loss: -5.928
Iter 15/2000 - Loss: -6.019
Iter 16/2000 - Loss: -5.857
Iter 17/2000 - Loss: -5.691
Iter 18/2000 - Loss: -5.662
Iter 19/2000 - Loss: -5.735
Iter 20/2000 - Loss: -5.807
Iter 1981/2000 - Loss: -6.717
Iter 1982/2000 - Loss: -6.717
Iter 1983/2000 - Loss: -6.717
Iter 1984/2000 - Loss: -6.717
Iter 1985/2000 - Loss: -6.717
Iter 1986/2000 - Loss: -6.717
Iter 1987/2000 - Loss: -6.717
Iter 1988/2000 - Loss: -6.717
Iter 1989/2000 - Loss: -6.717
Iter 1990/2000 - Loss: -6.717
Iter 1991/2000 - Loss: -6.717
Iter 1992/2000 - Loss: -6.717
Iter 1993/2000 - Loss: -6.717
Iter 1994/2000 - Loss: -6.717
Iter 1995/2000 - Loss: -6.717
Iter 1996/2000 - Loss: -6.717
Iter 1997/2000 - Loss: -6.717
Iter 1998/2000 - Loss: -6.717
Iter 1999/2000 - Loss: -6.717
Iter 2000/2000 - Loss: -6.717
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0001],
        [0.0074],
        [0.0001]])
Lengthscale: tensor([[[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]],

        [[6.5567e-04, 2.6721e-03, 7.7950e-03, 1.9013e-04, 6.9533e-07,
          2.2383e-03]]])
Signal Variance: tensor([0.0007, 0.0006, 0.0301, 0.0006])
Estimated target variance: tensor([0.0010, 0.0008, 0.0417, 0.0008])
N: 10
Signal to noise ratio: tensor([1.9989, 1.9985, 2.0103, 1.9984])
Bound on condition number: tensor([40.9574, 40.9403, 41.4144, 40.9359])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.4405129212017251, policy loss: 0.8151821006026686
Experience 1, Iter 1, disc loss: 1.427131876703867, policy loss: 0.8150591507307496
Experience 1, Iter 2, disc loss: 1.4177002930696299, policy loss: 0.8105928173717418
Experience 1, Iter 3, disc loss: 1.4038514813390588, policy loss: 0.8113892761296086
Experience 1, Iter 4, disc loss: 1.3920129776971288, policy loss: 0.8100930161884228
Experience 1, Iter 5, disc loss: 1.3804709641685924, policy loss: 0.8082845768521101
Experience 1, Iter 6, disc loss: 1.3685700700609442, policy loss: 0.8072951466720092
Experience 1, Iter 7, disc loss: 1.3574035867146241, policy loss: 0.8054651573775247
Experience 1, Iter 8, disc loss: 1.3447101947663487, policy loss: 0.8057059761661072
Experience 1, Iter 9, disc loss: 1.3339309217929416, policy loss: 0.803650654462987
Experience 1, Iter 10, disc loss: 1.3218300315643767, policy loss: 0.8033451961527807
Experience 1, Iter 11, disc loss: 1.3123068586722515, policy loss: 0.8000873497867982
Experience 1, Iter 12, disc loss: 1.2994187016608287, policy loss: 0.8009869403233668
Experience 1, Iter 13, disc loss: 1.2908681141793026, policy loss: 0.7970280973564946
Experience 1, Iter 14, disc loss: 1.2783410251608558, policy loss: 0.7979134729255069
Experience 1, Iter 15, disc loss: 1.267342961162484, policy loss: 0.7969813343362666
Experience 1, Iter 16, disc loss: 1.2556525796059474, policy loss: 0.7970535093363671
Experience 1, Iter 17, disc loss: 1.2455258917071945, policy loss: 0.7951632436088915
Experience 1, Iter 18, disc loss: 1.2343938218173571, policy loss: 0.7946668753098619
Experience 1, Iter 19, disc loss: 1.2223286876506814, policy loss: 0.7947170414951689
Experience 1, Iter 20, disc loss: 1.210677407430099, policy loss: 0.7938075416323473
Experience 1, Iter 21, disc loss: 1.198063822339142, policy loss: 0.7936639385565847
Experience 1, Iter 22, disc loss: 1.1881702662646154, policy loss: 0.790023450369532
Experience 1, Iter 23, disc loss: 1.1741079898738485, policy loss: 0.7913493670675144
Experience 1, Iter 24, disc loss: 1.1664171747137217, policy loss: 0.7846201628406124
Experience 1, Iter 25, disc loss: 1.1499720798546615, policy loss: 0.7888390166772987
Experience 1, Iter 26, disc loss: 1.1347787146797326, policy loss: 0.7917642853765641
Experience 1, Iter 27, disc loss: 1.123508365045319, policy loss: 0.7896342416141473
Experience 1, Iter 28, disc loss: 1.1126524864350875, policy loss: 0.7868526249897586
Experience 1, Iter 29, disc loss: 1.097033160458337, policy loss: 0.7896914184754621
Experience 1, Iter 30, disc loss: 1.0855922509379974, policy loss: 0.7868675105167011
Experience 1, Iter 31, disc loss: 1.069243377346949, policy loss: 0.7899559063134461
Experience 1, Iter 32, disc loss: 1.0564014219189768, policy loss: 0.7887847391882749
Experience 1, Iter 33, disc loss: 1.0399188133255628, policy loss: 0.7924722728426814
Experience 1, Iter 34, disc loss: 1.0259236568439811, policy loss: 0.7933593287418506
Experience 1, Iter 35, disc loss: 1.0129420457068823, policy loss: 0.7929399375479855
Experience 1, Iter 36, disc loss: 1.000092471949482, policy loss: 0.7926841466950487
Experience 1, Iter 37, disc loss: 0.9873368822958002, policy loss: 0.7919632557513497
Experience 1, Iter 38, disc loss: 0.97588599505111, policy loss: 0.7895548277192004
Experience 1, Iter 39, disc loss: 0.9587375876461169, policy loss: 0.7935172677614637
Experience 1, Iter 40, disc loss: 0.9449239824549738, policy loss: 0.792841687770552
Experience 1, Iter 41, disc loss: 0.9305753498826073, policy loss: 0.7921406526673855
Experience 1, Iter 42, disc loss: 0.9067267477503934, policy loss: 0.8035576740151991
Experience 1, Iter 43, disc loss: 0.8977113808696309, policy loss: 0.7958190531055197
Experience 1, Iter 44, disc loss: 0.8790137609924964, policy loss: 0.8010458394286217
Experience 1, Iter 45, disc loss: 0.8637538741505675, policy loss: 0.8023813989360801
Experience 1, Iter 46, disc loss: 0.8478288058169897, policy loss: 0.8050689686579753
Experience 1, Iter 47, disc loss: 0.8275169201174115, policy loss: 0.8138135188754484
Experience 1, Iter 48, disc loss: 0.821356242646375, policy loss: 0.8050764918655688
Experience 1, Iter 49, disc loss: 0.8022950470563578, policy loss: 0.8137118892541303
Experience 1, Iter 50, disc loss: 0.7916276200461221, policy loss: 0.8123546242724176
Experience 1, Iter 51, disc loss: 0.783615649684793, policy loss: 0.8079945590639089
Experience 1, Iter 52, disc loss: 0.7747116094161246, policy loss: 0.8057480208915584
Experience 1, Iter 53, disc loss: 0.7574358928626943, policy loss: 0.814262249285546
Experience 1, Iter 54, disc loss: 0.744515036077205, policy loss: 0.8187041990434348
Experience 1, Iter 55, disc loss: 0.7375238224900497, policy loss: 0.8166413395480258
Experience 1, Iter 56, disc loss: 0.7242445787778882, policy loss: 0.8226857930105957
Experience 1, Iter 57, disc loss: 0.7213951432586084, policy loss: 0.8156554069013235
Experience 1, Iter 58, disc loss: 0.7120957078998617, policy loss: 0.8183219136913199
Experience 1, Iter 59, disc loss: 0.6905041065777908, policy loss: 0.8365831586898373
Experience 1, Iter 60, disc loss: 0.6879179985172891, policy loss: 0.8319443816419195
Experience 1, Iter 61, disc loss: 0.6751996228848683, policy loss: 0.8401508802118212
Experience 1, Iter 62, disc loss: 0.6680264111318505, policy loss: 0.8418506654499065
Experience 1, Iter 63, disc loss: 0.6536198649809281, policy loss: 0.8532079590219582
Experience 1, Iter 64, disc loss: 0.6617177080639179, policy loss: 0.8365863315712116
Experience 1, Iter 65, disc loss: 0.652644607399269, policy loss: 0.8421504491543432
Experience 1, Iter 66, disc loss: 0.6445565275710496, policy loss: 0.8472555479388805
Experience 1, Iter 67, disc loss: 0.6371484065773271, policy loss: 0.8515318084227238
Experience 1, Iter 68, disc loss: 0.627947075120524, policy loss: 0.8583893509160097
Experience 1, Iter 69, disc loss: 0.6195598490427283, policy loss: 0.8649365143715356
Experience 1, Iter 70, disc loss: 0.6224837996158792, policy loss: 0.8572579077502942
Experience 1, Iter 71, disc loss: 0.6110995489507752, policy loss: 0.8674443555185518
Experience 1, Iter 72, disc loss: 0.6056393646180486, policy loss: 0.8712118301168448
Experience 1, Iter 73, disc loss: 0.5930353726304478, policy loss: 0.8839400941491147
Experience 1, Iter 74, disc loss: 0.5966119666169306, policy loss: 0.8759482530945681
Experience 1, Iter 75, disc loss: 0.6019157117326772, policy loss: 0.8662497497571557
Experience 1, Iter 76, disc loss: 0.5894718991197516, policy loss: 0.8800576462021927
Experience 1, Iter 77, disc loss: 0.5820394823059323, policy loss: 0.8870048244915447
Experience 1, Iter 78, disc loss: 0.571442310085552, policy loss: 0.8988657246107556
Experience 1, Iter 79, disc loss: 0.5798397688505064, policy loss: 0.8854990319215515
Experience 1, Iter 80, disc loss: 0.5625664124735552, policy loss: 0.9063185818813476
Experience 1, Iter 81, disc loss: 0.5753817807352358, policy loss: 0.8877171207679695
Experience 1, Iter 82, disc loss: 0.565697987432024, policy loss: 0.8978020731761309
Experience 1, Iter 83, disc loss: 0.5618497505793005, policy loss: 0.9015074688611961
Experience 1, Iter 84, disc loss: 0.5495713140907246, policy loss: 0.9169644184543535
Experience 1, Iter 85, disc loss: 0.5538111797498811, policy loss: 0.9094995987806802
Experience 1, Iter 86, disc loss: 0.5503090226634564, policy loss: 0.9131184166824217
Experience 1, Iter 87, disc loss: 0.5427083126558445, policy loss: 0.9220232335746914
Experience 1, Iter 88, disc loss: 0.538661399340989, policy loss: 0.9264682628772192
Experience 1, Iter 89, disc loss: 0.5363292525545273, policy loss: 0.9285472120237517
Experience 1, Iter 90, disc loss: 0.5322830018866281, policy loss: 0.9327385247642033
Experience 1, Iter 91, disc loss: 0.5273051607679302, policy loss: 0.9388493187562863
Experience 1, Iter 92, disc loss: 0.5277356539190974, policy loss: 0.9370746544539721
Experience 1, Iter 93, disc loss: 0.528995364617256, policy loss: 0.9348160780848952
Experience 1, Iter 94, disc loss: 0.5216360886769053, policy loss: 0.9438651003484598
Experience 1, Iter 95, disc loss: 0.5201685140484097, policy loss: 0.9452131325248394
Experience 1, Iter 96, disc loss: 0.5122941069447832, policy loss: 0.955682767380218
Experience 1, Iter 97, disc loss: 0.5126431395352524, policy loss: 0.9548500090874337
Experience 1, Iter 98, disc loss: 0.5095005180218479, policy loss: 0.9587677463137549
Experience 1, Iter 99, disc loss: 0.5072073042105077, policy loss: 0.9618456827748741
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0269],
        [0.0005]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0018, 0.1077, 0.0021], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0018, 0.1077, 0.0021])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.292
Iter 2/2000 - Loss: -0.213
Iter 3/2000 - Loss: -4.315
Iter 4/2000 - Loss: -4.884
Iter 5/2000 - Loss: -3.233
Iter 6/2000 - Loss: -3.196
Iter 7/2000 - Loss: -4.367
Iter 8/2000 - Loss: -5.137
Iter 9/2000 - Loss: -4.968
Iter 10/2000 - Loss: -4.386
Iter 11/2000 - Loss: -4.121
Iter 12/2000 - Loss: -4.357
Iter 13/2000 - Loss: -4.769
Iter 14/2000 - Loss: -4.993
Iter 15/2000 - Loss: -4.943
Iter 16/2000 - Loss: -4.797
Iter 17/2000 - Loss: -4.741
Iter 18/2000 - Loss: -4.792
Iter 19/2000 - Loss: -4.862
Iter 20/2000 - Loss: -4.914
Iter 1981/2000 - Loss: -5.422
Iter 1982/2000 - Loss: -5.409
Iter 1983/2000 - Loss: -5.416
Iter 1984/2000 - Loss: -5.446
Iter 1985/2000 - Loss: -5.471
Iter 1986/2000 - Loss: -5.461
Iter 1987/2000 - Loss: -5.443
Iter 1988/2000 - Loss: -5.448
Iter 1989/2000 - Loss: -5.468
Iter 1990/2000 - Loss: -5.476
Iter 1991/2000 - Loss: -5.467
Iter 1992/2000 - Loss: -5.448
Iter 1993/2000 - Loss: -5.423
Iter 1994/2000 - Loss: -5.387
Iter 1995/2000 - Loss: -5.339
Iter 1996/2000 - Loss: -5.317
Iter 1997/2000 - Loss: -5.383
Iter 1998/2000 - Loss: -5.466
Iter 1999/2000 - Loss: -5.420
Iter 2000/2000 - Loss: -5.408
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0201],
        [0.0004]])
Lengthscale: tensor([[[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]],

        [[1.1456e-03, 2.3613e-03, 2.6537e-02, 5.1878e-04, 1.9317e-06,
          6.0325e-03]]])
Signal Variance: tensor([0.0007, 0.0013, 0.0822, 0.0016])
Estimated target variance: tensor([0.0007, 0.0018, 0.1077, 0.0021])
N: 20
Signal to noise ratio: tensor([2.0005, 2.0004, 2.0226, 2.0004])
Bound on condition number: tensor([81.0376, 81.0355, 82.8218, 81.0299])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.5040510231843671, policy loss: 0.9668443355175214
Experience 2, Iter 1, disc loss: 0.5145518121237028, policy loss: 0.9519544995208986
Experience 2, Iter 2, disc loss: 0.4933040238133389, policy loss: 0.9826402793852014
Experience 2, Iter 3, disc loss: 0.5045310565529446, policy loss: 0.9662284980532825
Experience 2, Iter 4, disc loss: 0.4966040169327026, policy loss: 0.9783803425185738
Experience 2, Iter 5, disc loss: 0.4957343849674234, policy loss: 0.9791372771622171
Experience 2, Iter 6, disc loss: 0.4847837211762962, policy loss: 0.9942288634630156
Experience 2, Iter 7, disc loss: 0.4775195505153736, policy loss: 1.0053902302800521
Experience 2, Iter 8, disc loss: 0.4846846581628479, policy loss: 0.9938581745197803
Experience 2, Iter 9, disc loss: 0.4772677029260762, policy loss: 1.0053847371086504
Experience 2, Iter 10, disc loss: 0.47492063023524816, policy loss: 1.0108250308504083
Experience 2, Iter 11, disc loss: 0.47154876496791714, policy loss: 1.0142077742648103
Experience 2, Iter 12, disc loss: 0.4626395952442872, policy loss: 1.0292837615237007
Experience 2, Iter 13, disc loss: 0.46718341359688226, policy loss: 1.0208928625211495
Experience 2, Iter 14, disc loss: 0.4635390894749906, policy loss: 1.0262591062803856
Experience 2, Iter 15, disc loss: 0.45746909129310576, policy loss: 1.0378980784929412
Experience 2, Iter 16, disc loss: 0.4527720029173998, policy loss: 1.045637128382805
Experience 2, Iter 17, disc loss: 0.451244399957847, policy loss: 1.047741868343122
Experience 2, Iter 18, disc loss: 0.4550324571406447, policy loss: 1.0408265877264096
Experience 2, Iter 19, disc loss: 0.44038682724700534, policy loss: 1.0674446028858258
Experience 2, Iter 20, disc loss: 0.4353379250602239, policy loss: 1.0764034894514958
Experience 2, Iter 21, disc loss: 0.4343346847675828, policy loss: 1.0771482899926463
Experience 2, Iter 22, disc loss: 0.435473500254454, policy loss: 1.0756509457718724
Experience 2, Iter 23, disc loss: 0.4242446164600352, policy loss: 1.096092263374548
Experience 2, Iter 24, disc loss: 0.4232008945040097, policy loss: 1.0991181917311144
Experience 2, Iter 25, disc loss: 0.4271707023950868, policy loss: 1.091319301121922
Experience 2, Iter 26, disc loss: 0.41832251989595726, policy loss: 1.1075642909708687
Experience 2, Iter 27, disc loss: 0.4133735557852301, policy loss: 1.1182909364160674
Experience 2, Iter 28, disc loss: 0.4141003691936426, policy loss: 1.1159833428321835
Experience 2, Iter 29, disc loss: 0.404963139431501, policy loss: 1.1364488560112622
Experience 2, Iter 30, disc loss: 0.40426180648212273, policy loss: 1.1347702822854306
Experience 2, Iter 31, disc loss: 0.39880317972033374, policy loss: 1.147321316712737
Experience 2, Iter 32, disc loss: 0.3926073895735892, policy loss: 1.159456786043161
Experience 2, Iter 33, disc loss: 0.3939353199861802, policy loss: 1.1580473298008362
Experience 2, Iter 34, disc loss: 0.38847222425007827, policy loss: 1.1694082062592028
Experience 2, Iter 35, disc loss: 0.3813687226709644, policy loss: 1.1855638855993478
Experience 2, Iter 36, disc loss: 0.38168374931044563, policy loss: 1.1830368119133876
Experience 2, Iter 37, disc loss: 0.3799766354212909, policy loss: 1.1875415921654922
Experience 2, Iter 38, disc loss: 0.3775919352994074, policy loss: 1.192874892855892
Experience 2, Iter 39, disc loss: 0.36369837480376804, policy loss: 1.2240375486117963
Experience 2, Iter 40, disc loss: 0.36113671640596695, policy loss: 1.2300139446366316
Experience 2, Iter 41, disc loss: 0.3588682793861248, policy loss: 1.2357482522333147
Experience 2, Iter 42, disc loss: 0.3528694226671158, policy loss: 1.24970048501835
Experience 2, Iter 43, disc loss: 0.3594405879689575, policy loss: 1.2358118624082344
Experience 2, Iter 44, disc loss: 0.34783979781566593, policy loss: 1.262547276574814
Experience 2, Iter 45, disc loss: 0.3431465286937019, policy loss: 1.2778990942754673
Experience 2, Iter 46, disc loss: 0.3362412373372914, policy loss: 1.2916781314184071
Experience 2, Iter 47, disc loss: 0.3397672944169159, policy loss: 1.2836364567453775
Experience 2, Iter 48, disc loss: 0.3339326585997629, policy loss: 1.2949615637857577
Experience 2, Iter 49, disc loss: 0.33247539191693876, policy loss: 1.3045295719591996
Experience 2, Iter 50, disc loss: 0.3189784398882755, policy loss: 1.3377893891924846
Experience 2, Iter 51, disc loss: 0.32538094422019653, policy loss: 1.318701514514078
Experience 2, Iter 52, disc loss: 0.31942511846711086, policy loss: 1.3397956832424027
Experience 2, Iter 53, disc loss: 0.30357566754404725, policy loss: 1.381869813685418
Experience 2, Iter 54, disc loss: 0.3021690439720248, policy loss: 1.3894205938815558
Experience 2, Iter 55, disc loss: 0.2994828628266817, policy loss: 1.3964349222054853
Experience 2, Iter 56, disc loss: 0.29972694875370115, policy loss: 1.3949392850342548
Experience 2, Iter 57, disc loss: 0.2923621551049705, policy loss: 1.4139770970269667
Experience 2, Iter 58, disc loss: 0.2912841399300794, policy loss: 1.419729108050121
Experience 2, Iter 59, disc loss: 0.2808331924708707, policy loss: 1.4529178618381864
Experience 2, Iter 60, disc loss: 0.2735938001777434, policy loss: 1.4781741370989754
Experience 2, Iter 61, disc loss: 0.2819112371944355, policy loss: 1.4494559769110087
Experience 2, Iter 62, disc loss: 0.270380084047098, policy loss: 1.4893197612303588
Experience 2, Iter 63, disc loss: 0.26448259644102234, policy loss: 1.5081801924006806
Experience 2, Iter 64, disc loss: 0.2636814421143246, policy loss: 1.5107000520468574
Experience 2, Iter 65, disc loss: 0.25571760122970444, policy loss: 1.5391023068167011
Experience 2, Iter 66, disc loss: 0.25504959844922387, policy loss: 1.5387208067454914
Experience 2, Iter 67, disc loss: 0.25234461413578374, policy loss: 1.5494297826022594
Experience 2, Iter 68, disc loss: 0.24883743138415304, policy loss: 1.562803755470196
Experience 2, Iter 69, disc loss: 0.24839521501215822, policy loss: 1.5685373342564297
Experience 2, Iter 70, disc loss: 0.2408799192302945, policy loss: 1.595769922952096
Experience 2, Iter 71, disc loss: 0.23695569504027494, policy loss: 1.607788065126354
Experience 2, Iter 72, disc loss: 0.22787894779850335, policy loss: 1.6451496449154965
Experience 2, Iter 73, disc loss: 0.22827165647847347, policy loss: 1.6469543535300084
Experience 2, Iter 74, disc loss: 0.21748052174285915, policy loss: 1.6940307750085042
Experience 2, Iter 75, disc loss: 0.22298581399773898, policy loss: 1.666772747141685
Experience 2, Iter 76, disc loss: 0.20934440340150856, policy loss: 1.7269776824319414
Experience 2, Iter 77, disc loss: 0.21480924577328914, policy loss: 1.6989860949105493
Experience 2, Iter 78, disc loss: 0.21051473168290294, policy loss: 1.7225677439917892
Experience 2, Iter 79, disc loss: 0.2028872436204039, policy loss: 1.7560071172466487
Experience 2, Iter 80, disc loss: 0.19812630607525067, policy loss: 1.7752813948457271
Experience 2, Iter 81, disc loss: 0.18999146827523397, policy loss: 1.8254039425596962
Experience 2, Iter 82, disc loss: 0.1834066001545118, policy loss: 1.851674342319419
Experience 2, Iter 83, disc loss: 0.186227849979264, policy loss: 1.8375203899643955
Experience 2, Iter 84, disc loss: 0.18092752737650947, policy loss: 1.869650623465849
Experience 2, Iter 85, disc loss: 0.17662848105302828, policy loss: 1.8927974610538456
Experience 2, Iter 86, disc loss: 0.17405204528022497, policy loss: 1.9059782504917886
Experience 2, Iter 87, disc loss: 0.16917848755435938, policy loss: 1.9342039930736024
Experience 2, Iter 88, disc loss: 0.16610058228906202, policy loss: 1.948291529615919
Experience 2, Iter 89, disc loss: 0.17147181434231057, policy loss: 1.9219398323198384
Experience 2, Iter 90, disc loss: 0.1593107963942277, policy loss: 1.9893756653479764
Experience 2, Iter 91, disc loss: 0.15669853593717242, policy loss: 2.001530814952459
Experience 2, Iter 92, disc loss: 0.14862752866840204, policy loss: 2.0593137648922655
Experience 2, Iter 93, disc loss: 0.15105077478520784, policy loss: 2.0351058699947404
Experience 2, Iter 94, disc loss: 0.15299405746717704, policy loss: 2.026441288803067
Experience 2, Iter 95, disc loss: 0.1458168516457998, policy loss: 2.073272568842205
Experience 2, Iter 96, disc loss: 0.14506688001167614, policy loss: 2.087461063956553
Experience 2, Iter 97, disc loss: 0.13144295682060606, policy loss: 2.174874846296111
Experience 2, Iter 98, disc loss: 0.1389046927445989, policy loss: 2.142479316863939
Experience 2, Iter 99, disc loss: 0.1350020040811962, policy loss: 2.154607239458801
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0318],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0006, 0.0021, 0.1274, 0.0026], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0006, 0.0021, 0.1274, 0.0026])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -3.973
Iter 2/2000 - Loss: -0.513
Iter 3/2000 - Loss: -4.031
Iter 4/2000 - Loss: -4.673
Iter 5/2000 - Loss: -3.238
Iter 6/2000 - Loss: -3.141
Iter 7/2000 - Loss: -4.157
Iter 8/2000 - Loss: -4.854
Iter 9/2000 - Loss: -4.709
Iter 10/2000 - Loss: -4.182
Iter 11/2000 - Loss: -3.947
Iter 12/2000 - Loss: -4.180
Iter 13/2000 - Loss: -4.566
Iter 14/2000 - Loss: -4.758
Iter 15/2000 - Loss: -4.691
Iter 16/2000 - Loss: -4.551
Iter 17/2000 - Loss: -4.503
Iter 18/2000 - Loss: -4.545
Iter 19/2000 - Loss: -4.611
Iter 20/2000 - Loss: -4.696
Iter 1981/2000 - Loss: -5.168
Iter 1982/2000 - Loss: -5.168
Iter 1983/2000 - Loss: -5.168
Iter 1984/2000 - Loss: -5.168
Iter 1985/2000 - Loss: -5.168
Iter 1986/2000 - Loss: -5.168
Iter 1987/2000 - Loss: -5.168
Iter 1988/2000 - Loss: -5.168
Iter 1989/2000 - Loss: -5.168
Iter 1990/2000 - Loss: -5.168
Iter 1991/2000 - Loss: -5.168
Iter 1992/2000 - Loss: -5.168
Iter 1993/2000 - Loss: -5.168
Iter 1994/2000 - Loss: -5.168
Iter 1995/2000 - Loss: -5.168
Iter 1996/2000 - Loss: -5.168
Iter 1997/2000 - Loss: -5.168
Iter 1998/2000 - Loss: -5.168
Iter 1999/2000 - Loss: -5.168
Iter 2000/2000 - Loss: -5.168
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0004],
        [0.0241],
        [0.0005]])
Lengthscale: tensor([[[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]],

        [[1.4361e-03, 1.9600e-03, 3.0635e-02, 6.6671e-04, 2.3632e-06,
          6.8535e-03]]])
Signal Variance: tensor([0.0005, 0.0016, 0.0990, 0.0020])
Estimated target variance: tensor([0.0006, 0.0021, 0.1274, 0.0026])
N: 30
Signal to noise ratio: tensor([1.9985, 2.0007, 2.0257, 2.0035])
Bound on condition number: tensor([120.8184, 121.0879, 124.1010, 121.4157])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.1305938258806556, policy loss: 2.2050485976985637
Experience 3, Iter 1, disc loss: 0.1238342363591195, policy loss: 2.2484236848308594
Experience 3, Iter 2, disc loss: 0.12285978353717979, policy loss: 2.2456945254699554
Experience 3, Iter 3, disc loss: 0.11644315752693923, policy loss: 2.3031938536962855
Experience 3, Iter 4, disc loss: 0.11220925658509073, policy loss: 2.3482564525530676
Experience 3, Iter 5, disc loss: 0.11521063444388736, policy loss: 2.3095928906649705
Experience 3, Iter 6, disc loss: 0.11156117482760125, policy loss: 2.3557354263748143
Experience 3, Iter 7, disc loss: 0.1132210745920289, policy loss: 2.3349458615783196
Experience 3, Iter 8, disc loss: 0.10931227972144358, policy loss: 2.3787062976470654
Experience 3, Iter 9, disc loss: 0.09427479827419212, policy loss: 2.5191989451668357
Experience 3, Iter 10, disc loss: 0.10450747535197921, policy loss: 2.421746592655471
Experience 3, Iter 11, disc loss: 0.10236395040098582, policy loss: 2.4578274473239263
Experience 3, Iter 12, disc loss: 0.0943149571849703, policy loss: 2.5209956170928094
Experience 3, Iter 13, disc loss: 0.09555023185111962, policy loss: 2.5078542849871863
Experience 3, Iter 14, disc loss: 0.09690813496332853, policy loss: 2.4917048876249224
Experience 3, Iter 15, disc loss: 0.08922024556854037, policy loss: 2.5776166595605625
Experience 3, Iter 16, disc loss: 0.08891231383261496, policy loss: 2.5844691150264145
Experience 3, Iter 17, disc loss: 0.08845853961138762, policy loss: 2.5953519362203465
Experience 3, Iter 18, disc loss: 0.08214719674829164, policy loss: 2.6754865530595624
Experience 3, Iter 19, disc loss: 0.0856461563217958, policy loss: 2.6172531699155996
Experience 3, Iter 20, disc loss: 0.08384052170888982, policy loss: 2.6468923891721996
Experience 3, Iter 21, disc loss: 0.08897106708524677, policy loss: 2.5780115623971604
Experience 3, Iter 22, disc loss: 0.08051603141170806, policy loss: 2.7113406803553977
Experience 3, Iter 23, disc loss: 0.07627461444023607, policy loss: 2.7628561447908484
Experience 3, Iter 24, disc loss: 0.07527005468899424, policy loss: 2.77189823113699
Experience 3, Iter 25, disc loss: 0.0713986520836655, policy loss: 2.8079228786675783
Experience 3, Iter 26, disc loss: 0.07256711795046251, policy loss: 2.7859221563894243
Experience 3, Iter 27, disc loss: 0.0749904049527194, policy loss: 2.7547072914399373
Experience 3, Iter 28, disc loss: 0.07441354315313836, policy loss: 2.7908667748950737
Experience 3, Iter 29, disc loss: 0.06663966776827271, policy loss: 2.8834901910097646
Experience 3, Iter 30, disc loss: 0.07287347822844, policy loss: 2.7873845526604444
Experience 3, Iter 31, disc loss: 0.06597417089743787, policy loss: 2.887544409639707
Experience 3, Iter 32, disc loss: 0.06884380359880668, policy loss: 2.8545849033453496
Experience 3, Iter 33, disc loss: 0.062274498995148295, policy loss: 2.9522044891339316
Experience 3, Iter 34, disc loss: 0.058600274586540387, policy loss: 3.0120848833193676
Experience 3, Iter 35, disc loss: 0.061372266690243794, policy loss: 2.992783788142485
Experience 3, Iter 36, disc loss: 0.057542751852713146, policy loss: 3.034093646347465
Experience 3, Iter 37, disc loss: 0.05692338110724959, policy loss: 3.0479888331112255
Experience 3, Iter 38, disc loss: 0.05244274116557426, policy loss: 3.1539373566461064
Experience 3, Iter 39, disc loss: 0.06233539083895083, policy loss: 2.968978438464472
Experience 3, Iter 40, disc loss: 0.05921909030142564, policy loss: 3.005424642265338
Experience 3, Iter 41, disc loss: 0.053073388295671646, policy loss: 3.1240989512404758
Experience 3, Iter 42, disc loss: 0.055113438166076174, policy loss: 3.1034884366407924
Experience 3, Iter 43, disc loss: 0.051887888002252705, policy loss: 3.1476287699752756
Experience 3, Iter 44, disc loss: 0.05048284462222771, policy loss: 3.195886050464698
Experience 3, Iter 45, disc loss: 0.05382387401244471, policy loss: 3.110576717693768
Experience 3, Iter 46, disc loss: 0.04448814952271902, policy loss: 3.305715201759016
Experience 3, Iter 47, disc loss: 0.04740980853781013, policy loss: 3.2296987971443856
Experience 3, Iter 48, disc loss: 0.04528368670839934, policy loss: 3.2639906349088124
Experience 3, Iter 49, disc loss: 0.045375553437861454, policy loss: 3.2838597123679945
Experience 3, Iter 50, disc loss: 0.04899570351252229, policy loss: 3.1979250824555816
Experience 3, Iter 51, disc loss: 0.04592941494972118, policy loss: 3.3003719327145373
Experience 3, Iter 52, disc loss: 0.045418855768535875, policy loss: 3.2611963025863595
Experience 3, Iter 53, disc loss: 0.043979776699312224, policy loss: 3.3417868799448405
Experience 3, Iter 54, disc loss: 0.04562850041692899, policy loss: 3.274369043139081
Experience 3, Iter 55, disc loss: 0.045690400189930605, policy loss: 3.306980166237265
Experience 3, Iter 56, disc loss: 0.04223688481538835, policy loss: 3.366240338760437
Experience 3, Iter 57, disc loss: 0.04002240746366201, policy loss: 3.414386106604002
Experience 3, Iter 58, disc loss: 0.043535571698329154, policy loss: 3.328149800968462
Experience 3, Iter 59, disc loss: 0.03932217079867383, policy loss: 3.4393881694231503
Experience 3, Iter 60, disc loss: 0.040388410152986214, policy loss: 3.4537359458774546
Experience 3, Iter 61, disc loss: 0.04007455793347444, policy loss: 3.4192105534404567
Experience 3, Iter 62, disc loss: 0.03935662959386083, policy loss: 3.4293134647570276
Experience 3, Iter 63, disc loss: 0.03819574389093758, policy loss: 3.46985810701838
Experience 3, Iter 64, disc loss: 0.03704729211969691, policy loss: 3.534337845810403
Experience 3, Iter 65, disc loss: 0.03332759132219417, policy loss: 3.5928352387038944
Experience 3, Iter 66, disc loss: 0.03441711868358052, policy loss: 3.5907534995900816
Experience 3, Iter 67, disc loss: 0.033265779487537256, policy loss: 3.596780373456072
Experience 3, Iter 68, disc loss: 0.03484678979462703, policy loss: 3.5803784994608847
Experience 3, Iter 69, disc loss: 0.03572930262485733, policy loss: 3.5045214159055447
Experience 3, Iter 70, disc loss: 0.03433356316760423, policy loss: 3.60057141733303
Experience 3, Iter 71, disc loss: 0.03295899403949146, policy loss: 3.621731648827407
Experience 3, Iter 72, disc loss: 0.032346339732503476, policy loss: 3.6553704119888124
Experience 3, Iter 73, disc loss: 0.032015218510272746, policy loss: 3.663543186918405
Experience 3, Iter 74, disc loss: 0.031153459393761043, policy loss: 3.70126579140957
Experience 3, Iter 75, disc loss: 0.02998710575105911, policy loss: 3.7329803844445966
Experience 3, Iter 76, disc loss: 0.030842134266192683, policy loss: 3.7219044759443833
Experience 3, Iter 77, disc loss: 0.031046965235478174, policy loss: 3.7063071888988093
Experience 3, Iter 78, disc loss: 0.030585472399917824, policy loss: 3.73325345054089
Experience 3, Iter 79, disc loss: 0.029694252848373147, policy loss: 3.7265377594534854
Experience 3, Iter 80, disc loss: 0.025993811032938106, policy loss: 3.918693210135058
Experience 3, Iter 81, disc loss: 0.02868820485680834, policy loss: 3.7603979679334643
Experience 3, Iter 82, disc loss: 0.027034830383281198, policy loss: 3.841600422769781
Experience 3, Iter 83, disc loss: 0.02769521065696234, policy loss: 3.8089795470971306
Experience 3, Iter 84, disc loss: 0.028707746224152293, policy loss: 3.7723110851179023
Experience 3, Iter 85, disc loss: 0.027671882091926604, policy loss: 3.822088926600261
Experience 3, Iter 86, disc loss: 0.02666626912560128, policy loss: 3.8449700709638774
Experience 3, Iter 87, disc loss: 0.02500632367541661, policy loss: 3.891362066674489
Experience 3, Iter 88, disc loss: 0.024396309484090366, policy loss: 3.94433863143105
Experience 3, Iter 89, disc loss: 0.024921071344757066, policy loss: 3.91614957546183
Experience 3, Iter 90, disc loss: 0.028220986579863792, policy loss: 3.807453022136926
Experience 3, Iter 91, disc loss: 0.02473879631980069, policy loss: 3.9242169983894772
Experience 3, Iter 92, disc loss: 0.02574018161514419, policy loss: 3.940884105640796
Experience 3, Iter 93, disc loss: 0.02237587290171276, policy loss: 4.01795678123033
Experience 3, Iter 94, disc loss: 0.023597359613350733, policy loss: 3.987148396789469
Experience 3, Iter 95, disc loss: 0.02355854464264386, policy loss: 3.9825518865705503
Experience 3, Iter 96, disc loss: 0.023146564309410252, policy loss: 4.03675673508349
Experience 3, Iter 97, disc loss: 0.02259226042569994, policy loss: 4.029828483542449
Experience 3, Iter 98, disc loss: 0.022177444390851276, policy loss: 4.095735638743191
Experience 3, Iter 99, disc loss: 0.023636005486206615, policy loss: 3.9979965817009444
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0005],
        [0.0274],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0006, 0.0021, 0.1096, 0.0023], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0006, 0.0021, 0.1096, 0.0023])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.175
Iter 2/2000 - Loss: -0.176
Iter 3/2000 - Loss: -4.209
Iter 4/2000 - Loss: -4.770
Iter 5/2000 - Loss: -3.156
Iter 6/2000 - Loss: -3.163
Iter 7/2000 - Loss: -4.323
Iter 8/2000 - Loss: -5.038
Iter 9/2000 - Loss: -4.820
Iter 10/2000 - Loss: -4.239
Iter 11/2000 - Loss: -4.022
Iter 12/2000 - Loss: -4.305
Iter 13/2000 - Loss: -4.726
Iter 14/2000 - Loss: -4.920
Iter 15/2000 - Loss: -4.833
Iter 16/2000 - Loss: -4.670
Iter 17/2000 - Loss: -4.611
Iter 18/2000 - Loss: -4.661
Iter 19/2000 - Loss: -4.743
Iter 20/2000 - Loss: -4.838
Iter 1981/2000 - Loss: -5.331
Iter 1982/2000 - Loss: -5.331
Iter 1983/2000 - Loss: -5.331
Iter 1984/2000 - Loss: -5.331
Iter 1985/2000 - Loss: -5.330
Iter 1986/2000 - Loss: -5.330
Iter 1987/2000 - Loss: -5.329
Iter 1988/2000 - Loss: -5.328
Iter 1989/2000 - Loss: -5.327
Iter 1990/2000 - Loss: -5.324
Iter 1991/2000 - Loss: -5.321
Iter 1992/2000 - Loss: -5.317
Iter 1993/2000 - Loss: -5.314
Iter 1994/2000 - Loss: -5.313
Iter 1995/2000 - Loss: -5.317
Iter 1996/2000 - Loss: -5.324
Iter 1997/2000 - Loss: -5.329
Iter 1998/2000 - Loss: -5.331
Iter 1999/2000 - Loss: -5.328
Iter 2000/2000 - Loss: -5.322
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0004],
        [0.0210],
        [0.0004]])
Lengthscale: tensor([[[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]],

        [[1.3794e-03, 1.9799e-03, 2.6836e-02, 5.8168e-04, 2.0175e-06,
          6.4015e-03]]])
Signal Variance: tensor([0.0005, 0.0016, 0.0859, 0.0018])
Estimated target variance: tensor([0.0006, 0.0021, 0.1096, 0.0023])
N: 40
Signal to noise ratio: tensor([1.9984, 2.0007, 2.0222, 2.0006])
Bound on condition number: tensor([160.7499, 161.1058, 164.5725, 161.0945])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.0243335778072257, policy loss: 3.9337939227222987
Experience 4, Iter 1, disc loss: 0.026122797341715444, policy loss: 3.8731620567210117
Experience 4, Iter 2, disc loss: 0.028106104676635966, policy loss: 3.7891394496202953
Experience 4, Iter 3, disc loss: 0.025276936117531623, policy loss: 3.8815625278188106
Experience 4, Iter 4, disc loss: 0.027169018809885013, policy loss: 3.8304344646915154
Experience 4, Iter 5, disc loss: 0.025371737093680128, policy loss: 3.9131345110295106
Experience 4, Iter 6, disc loss: 0.02648126022558548, policy loss: 3.84118054762268
Experience 4, Iter 7, disc loss: 0.023955230178952784, policy loss: 3.947600259388703
Experience 4, Iter 8, disc loss: 0.023252550775233145, policy loss: 4.020594822414198
Experience 4, Iter 9, disc loss: 0.022953915796893588, policy loss: 4.044417080703273
Experience 4, Iter 10, disc loss: 0.022211600896169522, policy loss: 4.046258287645898
Experience 4, Iter 11, disc loss: 0.023782848467425642, policy loss: 3.9882813806056765
Experience 4, Iter 12, disc loss: 0.021940179965939013, policy loss: 4.090813834212212
Experience 4, Iter 13, disc loss: 0.023416570533830193, policy loss: 4.0251542645122695
Experience 4, Iter 14, disc loss: 0.02040981472592277, policy loss: 4.154418486300107
Experience 4, Iter 15, disc loss: 0.021488618364093968, policy loss: 4.060671152209896
Experience 4, Iter 16, disc loss: 0.021722146655508354, policy loss: 4.051212930111683
Experience 4, Iter 17, disc loss: 0.02218340550774955, policy loss: 4.013726050794179
Experience 4, Iter 18, disc loss: 0.02050967362006201, policy loss: 4.134194518665817
Experience 4, Iter 19, disc loss: 0.023482972539886612, policy loss: 4.013975444784738
Experience 4, Iter 20, disc loss: 0.021152686578301067, policy loss: 4.135541912193051
Experience 4, Iter 21, disc loss: 0.021470108283584116, policy loss: 4.036704744869106
Experience 4, Iter 22, disc loss: 0.016571650973642923, policy loss: 4.355656202588975
Experience 4, Iter 23, disc loss: 0.01891154719464209, policy loss: 4.211523988529619
Experience 4, Iter 24, disc loss: 0.021064659520311568, policy loss: 4.125954351286226
Experience 4, Iter 25, disc loss: 0.01986081173420199, policy loss: 4.20410224433391
Experience 4, Iter 26, disc loss: 0.019306783329570514, policy loss: 4.191869199297299
Experience 4, Iter 27, disc loss: 0.01848067841634528, policy loss: 4.243122875836846
Experience 4, Iter 28, disc loss: 0.017998779890560493, policy loss: 4.28309058356748
Experience 4, Iter 29, disc loss: 0.020263884406534358, policy loss: 4.166379590033536
Experience 4, Iter 30, disc loss: 0.019018298099822608, policy loss: 4.20821515527208
Experience 4, Iter 31, disc loss: 0.01837310644612086, policy loss: 4.309147949092569
Experience 4, Iter 32, disc loss: 0.01658738691229061, policy loss: 4.332793706942763
Experience 4, Iter 33, disc loss: 0.01909239985507155, policy loss: 4.228325145399651
Experience 4, Iter 34, disc loss: 0.019283889560999943, policy loss: 4.184106884099335
Experience 4, Iter 35, disc loss: 0.01731463164979118, policy loss: 4.309728133699032
Experience 4, Iter 36, disc loss: 0.01695447037917577, policy loss: 4.373155102925319
Experience 4, Iter 37, disc loss: 0.01767729040730203, policy loss: 4.31909698718964
Experience 4, Iter 38, disc loss: 0.016656225988947265, policy loss: 4.362678518829551
Experience 4, Iter 39, disc loss: 0.018694297122123445, policy loss: 4.308194600427407
Experience 4, Iter 40, disc loss: 0.01599692478672529, policy loss: 4.414414846778908
Experience 4, Iter 41, disc loss: 0.01815290121422333, policy loss: 4.354242224674584
Experience 4, Iter 42, disc loss: 0.015223071123891705, policy loss: 4.4408006159141395
Experience 4, Iter 43, disc loss: 0.01776141679069939, policy loss: 4.335164266332995
Experience 4, Iter 44, disc loss: 0.013535158843074208, policy loss: 4.52583800614711
Experience 4, Iter 45, disc loss: 0.016927249871705522, policy loss: 4.351675992214075
Experience 4, Iter 46, disc loss: 0.01818033479576807, policy loss: 4.310920632803845
Experience 4, Iter 47, disc loss: 0.014805311761291155, policy loss: 4.544018051534197
Experience 4, Iter 48, disc loss: 0.01410400786234272, policy loss: 4.571288768576731
Experience 4, Iter 49, disc loss: 0.014960141613610976, policy loss: 4.501402776636534
Experience 4, Iter 50, disc loss: 0.017776919534329964, policy loss: 4.280834608996419
Experience 4, Iter 51, disc loss: 0.01360758623042758, policy loss: 4.564053997642791
Experience 4, Iter 52, disc loss: 0.016629970109246644, policy loss: 4.333733570707375
Experience 4, Iter 53, disc loss: 0.013494985109527081, policy loss: 4.590525583307977
Experience 4, Iter 54, disc loss: 0.014721449544386499, policy loss: 4.519954910252094
Experience 4, Iter 55, disc loss: 0.015907841428311253, policy loss: 4.441275458960434
Experience 4, Iter 56, disc loss: 0.014056864851577654, policy loss: 4.604000236875028
Experience 4, Iter 57, disc loss: 0.014203686247468574, policy loss: 4.562599524163463
Experience 4, Iter 58, disc loss: 0.01350357996671045, policy loss: 4.5638166070941555
Experience 4, Iter 59, disc loss: 0.014624984971685805, policy loss: 4.472489478906145
Experience 4, Iter 60, disc loss: 0.013353286568495232, policy loss: 4.577881497608017
Experience 4, Iter 61, disc loss: 0.014509884058678616, policy loss: 4.608758396191456
Experience 4, Iter 62, disc loss: 0.013347720742675508, policy loss: 4.600066504377804
Experience 4, Iter 63, disc loss: 0.013171567613580066, policy loss: 4.660252676861029
Experience 4, Iter 64, disc loss: 0.013350803615069982, policy loss: 4.663035603941127
Experience 4, Iter 65, disc loss: 0.012682057821407208, policy loss: 4.68580437829336
Experience 4, Iter 66, disc loss: 0.01196684395098085, policy loss: 4.674724360775743
Experience 4, Iter 67, disc loss: 0.011947769109190622, policy loss: 4.718658884973294
Experience 4, Iter 68, disc loss: 0.013475230516308055, policy loss: 4.580877373905624
Experience 4, Iter 69, disc loss: 0.014619359802317416, policy loss: 4.5419371979631755
Experience 4, Iter 70, disc loss: 0.013266642207020295, policy loss: 4.596043229020678
Experience 4, Iter 71, disc loss: 0.012130457133895482, policy loss: 4.692279830715088
Experience 4, Iter 72, disc loss: 0.01226478525754223, policy loss: 4.690602376166996
Experience 4, Iter 73, disc loss: 0.01148476601907633, policy loss: 4.736182553049321
Experience 4, Iter 74, disc loss: 0.012113140328373103, policy loss: 4.720211730366323
Experience 4, Iter 75, disc loss: 0.012206053701784729, policy loss: 4.761078425928054
Experience 4, Iter 76, disc loss: 0.011745087687694982, policy loss: 4.70711350746763
Experience 4, Iter 77, disc loss: 0.012196655844331238, policy loss: 4.700893576809103
Experience 4, Iter 78, disc loss: 0.012974993662174911, policy loss: 4.631775050567046
Experience 4, Iter 79, disc loss: 0.011387916575198435, policy loss: 4.785621631416506
Experience 4, Iter 80, disc loss: 0.010145050695422117, policy loss: 4.89046316120702
Experience 4, Iter 81, disc loss: 0.010887712144297521, policy loss: 4.853695083989768
Experience 4, Iter 82, disc loss: 0.011374377378056195, policy loss: 4.8241910363503955
Experience 4, Iter 83, disc loss: 0.01137151551264156, policy loss: 4.814979720315391
Experience 4, Iter 84, disc loss: 0.011508977097215826, policy loss: 4.783005583498277
Experience 4, Iter 85, disc loss: 0.010850241226493551, policy loss: 4.833881611080855
Experience 4, Iter 86, disc loss: 0.010781267955734806, policy loss: 4.817590689705845
Experience 4, Iter 87, disc loss: 0.009490018403236905, policy loss: 4.959627352610727
Experience 4, Iter 88, disc loss: 0.010316049114702686, policy loss: 4.895049203163257
Experience 4, Iter 89, disc loss: 0.009908126306858238, policy loss: 4.938457812150222
Experience 4, Iter 90, disc loss: 0.009851773382427438, policy loss: 4.941788016669975
Experience 4, Iter 91, disc loss: 0.011747116740289372, policy loss: 4.790764790633753
Experience 4, Iter 92, disc loss: 0.010785416315994676, policy loss: 4.814728569261524
Experience 4, Iter 93, disc loss: 0.009507172597392649, policy loss: 4.937006118976541
Experience 4, Iter 94, disc loss: 0.010138203569172869, policy loss: 4.913647728105492
Experience 4, Iter 95, disc loss: 0.010192910579430801, policy loss: 4.863977143080761
Experience 4, Iter 96, disc loss: 0.01036202389679839, policy loss: 4.827536421285476
Experience 4, Iter 97, disc loss: 0.00917424258525154, policy loss: 5.012313433206744
Experience 4, Iter 98, disc loss: 0.009066712416806767, policy loss: 5.099185100933449
Experience 4, Iter 99, disc loss: 0.010963853892911782, policy loss: 4.889470266967049
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0254],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0019, 0.1018, 0.0023], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0019, 0.1018, 0.0023])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.253
Iter 2/2000 - Loss: -0.291
Iter 3/2000 - Loss: -4.263
Iter 4/2000 - Loss: -4.759
Iter 5/2000 - Loss: -3.177
Iter 6/2000 - Loss: -3.183
Iter 7/2000 - Loss: -4.310
Iter 8/2000 - Loss: -5.025
Iter 9/2000 - Loss: -4.843
Iter 10/2000 - Loss: -4.286
Iter 11/2000 - Loss: -4.047
Iter 12/2000 - Loss: -4.291
Iter 13/2000 - Loss: -4.694
Iter 14/2000 - Loss: -4.903
Iter 15/2000 - Loss: -4.838
Iter 16/2000 - Loss: -4.685
Iter 17/2000 - Loss: -4.633
Iter 18/2000 - Loss: -4.693
Iter 19/2000 - Loss: -4.768
Iter 20/2000 - Loss: -4.824
Iter 1981/2000 - Loss: -5.329
Iter 1982/2000 - Loss: -5.323
Iter 1983/2000 - Loss: -5.326
Iter 1984/2000 - Loss: -5.332
Iter 1985/2000 - Loss: -5.334
Iter 1986/2000 - Loss: -5.330
Iter 1987/2000 - Loss: -5.325
Iter 1988/2000 - Loss: -5.326
Iter 1989/2000 - Loss: -5.332
Iter 1990/2000 - Loss: -5.335
Iter 1991/2000 - Loss: -5.333
Iter 1992/2000 - Loss: -5.330
Iter 1993/2000 - Loss: -5.332
Iter 1994/2000 - Loss: -5.334
Iter 1995/2000 - Loss: -5.334
Iter 1996/2000 - Loss: -5.332
Iter 1997/2000 - Loss: -5.332
Iter 1998/2000 - Loss: -5.334
Iter 1999/2000 - Loss: -5.335
Iter 2000/2000 - Loss: -5.334
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0004],
        [0.0196],
        [0.0004]])
Lengthscale: tensor([[[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]],

        [[1.3433e-03, 2.2416e-03, 2.4862e-02, 5.5374e-04, 1.8668e-06,
          6.2663e-03]]])
Signal Variance: tensor([0.0005, 0.0015, 0.0801, 0.0018])
Estimated target variance: tensor([0.0007, 0.0019, 0.1018, 0.0023])
N: 50
Signal to noise ratio: tensor([1.9987, 2.0010, 2.0208, 2.0006])
Bound on condition number: tensor([200.7341, 201.1902, 205.1749, 201.1175])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.007613552471421737, policy loss: 5.219861516610386
Experience 5, Iter 1, disc loss: 0.00822955924889536, policy loss: 5.174520541611833
Experience 5, Iter 2, disc loss: 0.008134873235957648, policy loss: 5.129958209361526
Experience 5, Iter 3, disc loss: 0.0087749815792017, policy loss: 5.108963875809697
Experience 5, Iter 4, disc loss: 0.00816336319694231, policy loss: 5.134482201571716
Experience 5, Iter 5, disc loss: 0.007239750606313691, policy loss: 5.247401098638321
Experience 5, Iter 6, disc loss: 0.007561379918514043, policy loss: 5.196940562182709
Experience 5, Iter 7, disc loss: 0.007525009542315736, policy loss: 5.176141631662217
Experience 5, Iter 8, disc loss: 0.007928495115415707, policy loss: 5.188733531913753
Experience 5, Iter 9, disc loss: 0.009344681052243163, policy loss: 5.053134537820833
Experience 5, Iter 10, disc loss: 0.007619733720771526, policy loss: 5.230834959302401
Experience 5, Iter 11, disc loss: 0.008553481929062306, policy loss: 5.161772338133998
Experience 5, Iter 12, disc loss: 0.007749877529513956, policy loss: 5.195692005434239
Experience 5, Iter 13, disc loss: 0.007039277895215201, policy loss: 5.247438242715691
Experience 5, Iter 14, disc loss: 0.007323443796311017, policy loss: 5.281796612999623
Experience 5, Iter 15, disc loss: 0.0071546922264025, policy loss: 5.324080103637161
Experience 5, Iter 16, disc loss: 0.007892911776904666, policy loss: 5.232163155685136
Experience 5, Iter 17, disc loss: 0.0077485507205430765, policy loss: 5.176734091105052
Experience 5, Iter 18, disc loss: 0.006961542605644053, policy loss: 5.327486833803429
Experience 5, Iter 19, disc loss: 0.006883727726987509, policy loss: 5.325811391135484
Experience 5, Iter 20, disc loss: 0.0077768569398537985, policy loss: 5.264491551593619
Experience 5, Iter 21, disc loss: 0.007422967283344312, policy loss: 5.230703829262254
Experience 5, Iter 22, disc loss: 0.007432276656481422, policy loss: 5.2395508884239685
Experience 5, Iter 23, disc loss: 0.007784826095713596, policy loss: 5.248146795693593
Experience 5, Iter 24, disc loss: 0.007591778809212661, policy loss: 5.182483022311617
Experience 5, Iter 25, disc loss: 0.007716711391265854, policy loss: 5.240642705555481
Experience 5, Iter 26, disc loss: 0.00722479147728485, policy loss: 5.295980145974909
Experience 5, Iter 27, disc loss: 0.006981975815506498, policy loss: 5.402495193151673
Experience 5, Iter 28, disc loss: 0.005856398756889931, policy loss: 5.529194769562743
Experience 5, Iter 29, disc loss: 0.006490190965211104, policy loss: 5.404397394961276
Experience 5, Iter 30, disc loss: 0.006462463650248211, policy loss: 5.391676853326726
Experience 5, Iter 31, disc loss: 0.007725081786528375, policy loss: 5.199487801319438
Experience 5, Iter 32, disc loss: 0.006642617652204381, policy loss: 5.3841382590119515
Experience 5, Iter 33, disc loss: 0.007257921320751406, policy loss: 5.294879307992737
Experience 5, Iter 34, disc loss: 0.006728356753463585, policy loss: 5.369127444196966
Experience 5, Iter 35, disc loss: 0.007387683039480408, policy loss: 5.310728925499529
Experience 5, Iter 36, disc loss: 0.006889178624523328, policy loss: 5.350214188959523
Experience 5, Iter 37, disc loss: 0.006933231275063019, policy loss: 5.372040678306817
Experience 5, Iter 38, disc loss: 0.005863115537632079, policy loss: 5.471238605891714
Experience 5, Iter 39, disc loss: 0.006228606569798763, policy loss: 5.470892369171539
Experience 5, Iter 40, disc loss: 0.006161975686011168, policy loss: 5.440811000037327
Experience 5, Iter 41, disc loss: 0.0066458939378581635, policy loss: 5.347368641322953
Experience 5, Iter 42, disc loss: 0.005891417329156552, policy loss: 5.48548666860054
Experience 5, Iter 43, disc loss: 0.00614852799042771, policy loss: 5.401205201015575
Experience 5, Iter 44, disc loss: 0.006320308188759901, policy loss: 5.459018589559658
Experience 5, Iter 45, disc loss: 0.006251564146452594, policy loss: 5.443615658070243
Experience 5, Iter 46, disc loss: 0.00585713081340741, policy loss: 5.594482847190326
Experience 5, Iter 47, disc loss: 0.00616972888853684, policy loss: 5.43400912753655
Experience 5, Iter 48, disc loss: 0.006519278821007583, policy loss: 5.410566429584994
Experience 5, Iter 49, disc loss: 0.00573339739861722, policy loss: 5.585746667993348
Experience 5, Iter 50, disc loss: 0.005788935840675682, policy loss: 5.6440824667652265
Experience 5, Iter 51, disc loss: 0.00592447685922297, policy loss: 5.445897683376251
Experience 5, Iter 52, disc loss: 0.006501836394379335, policy loss: 5.421673214585162
Experience 5, Iter 53, disc loss: 0.006164434727941825, policy loss: 5.448233122505659
Experience 5, Iter 54, disc loss: 0.0072065603555075824, policy loss: 5.335975620759621
Experience 5, Iter 55, disc loss: 0.005593013378760515, policy loss: 5.523877324791134
Experience 5, Iter 56, disc loss: 0.005306135438860258, policy loss: 5.631276360007959
Experience 5, Iter 57, disc loss: 0.005946671603832368, policy loss: 5.467316692365502
Experience 5, Iter 58, disc loss: 0.005548856692654261, policy loss: 5.534067712902625
Experience 5, Iter 59, disc loss: 0.0053947403285944575, policy loss: 5.631810243356696
Experience 5, Iter 60, disc loss: 0.006038440501368574, policy loss: 5.457407060127961
Experience 5, Iter 61, disc loss: 0.005519980025382374, policy loss: 5.576512287165167
Experience 5, Iter 62, disc loss: 0.005373700117239904, policy loss: 5.579989144484298
Experience 5, Iter 63, disc loss: 0.0058445234292977865, policy loss: 5.547949123081315
Experience 5, Iter 64, disc loss: 0.006329319623720082, policy loss: 5.4345199172732315
Experience 5, Iter 65, disc loss: 0.005554422058548983, policy loss: 5.543860573436174
Experience 5, Iter 66, disc loss: 0.00565601400885215, policy loss: 5.583668122961184
Experience 5, Iter 67, disc loss: 0.005924466752421423, policy loss: 5.557251144733416
Experience 5, Iter 68, disc loss: 0.0054998352635133784, policy loss: 5.5896269325988595
Experience 5, Iter 69, disc loss: 0.005092597990785992, policy loss: 5.614878727144021
Experience 5, Iter 70, disc loss: 0.005227933899742686, policy loss: 5.649211545059327
Experience 5, Iter 71, disc loss: 0.005286030651029797, policy loss: 5.5926291614312005
Experience 5, Iter 72, disc loss: 0.005762219391534256, policy loss: 5.5190271251156515
Experience 5, Iter 73, disc loss: 0.004905727243746662, policy loss: 5.614317633129132
Experience 5, Iter 74, disc loss: 0.005062436144767572, policy loss: 5.781280464032958
Experience 5, Iter 75, disc loss: 0.005748745991236759, policy loss: 5.605692050064593
Experience 5, Iter 76, disc loss: 0.006372897714919745, policy loss: 5.435309742867198
Experience 5, Iter 77, disc loss: 0.0054463697388291285, policy loss: 5.650836561744734
Experience 5, Iter 78, disc loss: 0.005571181090634732, policy loss: 5.558257046712115
Experience 5, Iter 79, disc loss: 0.0043528780067283775, policy loss: 5.798631230544263
Experience 5, Iter 80, disc loss: 0.005249614070568227, policy loss: 5.624123225665313
Experience 5, Iter 81, disc loss: 0.00561904050117598, policy loss: 5.608991371347065
Experience 5, Iter 82, disc loss: 0.005381716908611571, policy loss: 5.620193579286468
Experience 5, Iter 83, disc loss: 0.005156763123330952, policy loss: 5.674964265721581
Experience 5, Iter 84, disc loss: 0.004770298502462018, policy loss: 5.710297559256363
Experience 5, Iter 85, disc loss: 0.004570808644933147, policy loss: 5.767759276132878
Experience 5, Iter 86, disc loss: 0.004840993243826904, policy loss: 5.736459132661744
Experience 5, Iter 87, disc loss: 0.004788821699313556, policy loss: 5.715883346542277
Experience 5, Iter 88, disc loss: 0.004505339211847275, policy loss: 5.734443008275456
Experience 5, Iter 89, disc loss: 0.004801421818275862, policy loss: 5.786114067306608
Experience 5, Iter 90, disc loss: 0.0046646635649362465, policy loss: 5.757883294733144
Experience 5, Iter 91, disc loss: 0.004876270699212553, policy loss: 5.736954446972139
Experience 5, Iter 92, disc loss: 0.004732099736010152, policy loss: 5.730286231011183
Experience 5, Iter 93, disc loss: 0.0041556240495278125, policy loss: 5.8667968010732725
Experience 5, Iter 94, disc loss: 0.005151414706418837, policy loss: 5.6674056383612
Experience 5, Iter 95, disc loss: 0.0036119792737330883, policy loss: 6.045479479359755
Experience 5, Iter 96, disc loss: 0.00506270794752223, policy loss: 5.68900432180019
Experience 5, Iter 97, disc loss: 0.00476524947693625, policy loss: 5.720148248098018
Experience 5, Iter 98, disc loss: 0.0050247942515992995, policy loss: 5.666329414397765
Experience 5, Iter 99, disc loss: 0.0041817928170462424, policy loss: 5.875497929326909
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0008],
        [0.0269],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0011, 0.0032, 0.1075, 0.0024], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0011, 0.0032, 0.1075, 0.0024])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.419
Iter 2/2000 - Loss: -0.208
Iter 3/2000 - Loss: -4.286
Iter 4/2000 - Loss: -3.996
Iter 5/2000 - Loss: -2.521
Iter 6/2000 - Loss: -2.933
Iter 7/2000 - Loss: -4.035
Iter 8/2000 - Loss: -4.516
Iter 9/2000 - Loss: -4.227
Iter 10/2000 - Loss: -3.764
Iter 11/2000 - Loss: -3.642
Iter 12/2000 - Loss: -3.877
Iter 13/2000 - Loss: -4.184
Iter 14/2000 - Loss: -4.328
Iter 15/2000 - Loss: -4.283
Iter 16/2000 - Loss: -4.182
Iter 17/2000 - Loss: -4.147
Iter 18/2000 - Loss: -4.192
Iter 19/2000 - Loss: -4.257
Iter 20/2000 - Loss: -4.299
Iter 1981/2000 - Loss: -4.754
Iter 1982/2000 - Loss: -4.755
Iter 1983/2000 - Loss: -4.755
Iter 1984/2000 - Loss: -4.756
Iter 1985/2000 - Loss: -4.755
Iter 1986/2000 - Loss: -4.755
Iter 1987/2000 - Loss: -4.755
Iter 1988/2000 - Loss: -4.755
Iter 1989/2000 - Loss: -4.755
Iter 1990/2000 - Loss: -4.755
Iter 1991/2000 - Loss: -4.755
Iter 1992/2000 - Loss: -4.756
Iter 1993/2000 - Loss: -4.756
Iter 1994/2000 - Loss: -4.756
Iter 1995/2000 - Loss: -4.756
Iter 1996/2000 - Loss: -4.755
Iter 1997/2000 - Loss: -4.755
Iter 1998/2000 - Loss: -4.755
Iter 1999/2000 - Loss: -4.755
Iter 2000/2000 - Loss: -4.755
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0006],
        [0.0208],
        [0.0005]])
Lengthscale: tensor([[[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]],

        [[1.7193e-03, 6.0519e-03, 2.4976e-02, 6.6837e-04, 1.7331e-06,
          1.8870e-02]]])
Signal Variance: tensor([0.0009, 0.0025, 0.0849, 0.0019])
Estimated target variance: tensor([0.0011, 0.0032, 0.1075, 0.0024])
N: 60
Signal to noise ratio: tensor([1.9996, 2.0012, 2.0211, 2.0007])
Bound on condition number: tensor([240.8989, 241.2950, 246.1004, 241.1663])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.008689534918403683, policy loss: 5.252551326861998
Experience 6, Iter 1, disc loss: 0.007223226428156901, policy loss: 5.243821769592597
Experience 6, Iter 2, disc loss: 0.007341165699028053, policy loss: 5.236258457685394
Experience 6, Iter 3, disc loss: 0.006832842097356578, policy loss: 5.382059192649612
Experience 6, Iter 4, disc loss: 0.007108799871229088, policy loss: 5.3334718105399705
Experience 6, Iter 5, disc loss: 0.008761669879529884, policy loss: 5.2451435485952524
Experience 6, Iter 6, disc loss: 0.008326321877876892, policy loss: 5.206513351100231
Experience 6, Iter 7, disc loss: 0.00690005056324272, policy loss: 5.306084015692424
Experience 6, Iter 8, disc loss: 0.007009804616654526, policy loss: 5.3574833719229025
Experience 6, Iter 9, disc loss: 0.006285629350382719, policy loss: 5.427381860393812
Experience 6, Iter 10, disc loss: 0.005850952344186734, policy loss: 5.495698521443481
Experience 6, Iter 11, disc loss: 0.006310722155749392, policy loss: 5.491808402170516
Experience 6, Iter 12, disc loss: 0.00834927725955762, policy loss: 5.2291391183245555
Experience 6, Iter 13, disc loss: 0.007295784440487576, policy loss: 5.391256789549886
Experience 6, Iter 14, disc loss: 0.006075068143327685, policy loss: 5.50559509584827
Experience 6, Iter 15, disc loss: 0.006784728872315201, policy loss: 5.33862013090636
Experience 6, Iter 16, disc loss: 0.005752520139207564, policy loss: 5.5032927507391625
Experience 6, Iter 17, disc loss: 0.006049626397364625, policy loss: 5.477708732423851
Experience 6, Iter 18, disc loss: 0.006751758776525455, policy loss: 5.438119196978613
Experience 6, Iter 19, disc loss: 0.006900593418177716, policy loss: 5.43743498648011
Experience 6, Iter 20, disc loss: 0.006986423339274281, policy loss: 5.356381256904111
Experience 6, Iter 21, disc loss: 0.005969829054784032, policy loss: 5.532833359134395
Experience 6, Iter 22, disc loss: 0.0063578170217055635, policy loss: 5.458307349938924
Experience 6, Iter 23, disc loss: 0.006091653622406657, policy loss: 5.493630315307034
Experience 6, Iter 24, disc loss: 0.007028914573677891, policy loss: 5.407937475793203
Experience 6, Iter 25, disc loss: 0.007383347916087358, policy loss: 5.303758587533736
Experience 6, Iter 26, disc loss: 0.005430420614249553, policy loss: 5.6732160368381575
Experience 6, Iter 27, disc loss: 0.00532877762749547, policy loss: 5.635157877463242
Experience 6, Iter 28, disc loss: 0.004899199450089146, policy loss: 5.805515631040906
Experience 6, Iter 29, disc loss: 0.005965871112805374, policy loss: 5.472594114982848
Experience 6, Iter 30, disc loss: 0.00687267028492156, policy loss: 5.373706670276137
Experience 6, Iter 31, disc loss: 0.0064176817990410345, policy loss: 5.3969491586319585
Experience 6, Iter 32, disc loss: 0.006712873175283927, policy loss: 5.44559126525113
Experience 6, Iter 33, disc loss: 0.005951829769088369, policy loss: 5.5796092938825925
Experience 6, Iter 34, disc loss: 0.005464053277860097, policy loss: 5.569970343831455
Experience 6, Iter 35, disc loss: 0.006822826781583922, policy loss: 5.371076846909902
Experience 6, Iter 36, disc loss: 0.007087277740333249, policy loss: 5.374413784133597
Experience 6, Iter 37, disc loss: 0.006611966994193118, policy loss: 5.53564931791339
Experience 6, Iter 38, disc loss: 0.0062823669581160355, policy loss: 5.485922490215798
Experience 6, Iter 39, disc loss: 0.0060389581335502105, policy loss: 5.4642254431611
Experience 6, Iter 40, disc loss: 0.006172623418220398, policy loss: 5.55903719152732
Experience 6, Iter 41, disc loss: 0.005378957576306792, policy loss: 5.683510873651501
Experience 6, Iter 42, disc loss: 0.005576323060696073, policy loss: 5.6047922968314925
Experience 6, Iter 43, disc loss: 0.006494505850359197, policy loss: 5.744836411204079
Experience 6, Iter 44, disc loss: 0.0051430012392467165, policy loss: 5.610376934862634
Experience 6, Iter 45, disc loss: 0.005799966709696081, policy loss: 5.545637639208149
Experience 6, Iter 46, disc loss: 0.0050823691873782394, policy loss: 5.667823613521073
Experience 6, Iter 47, disc loss: 0.004767278726784814, policy loss: 5.743807574747368
Experience 6, Iter 48, disc loss: 0.0051731094381872005, policy loss: 5.718632956613712
Experience 6, Iter 49, disc loss: 0.005360467476204098, policy loss: 5.735259173799448
Experience 6, Iter 50, disc loss: 0.006071142392503851, policy loss: 5.580323946380066
Experience 6, Iter 51, disc loss: 0.004886661511605733, policy loss: 5.725213194940455
Experience 6, Iter 52, disc loss: 0.005500411487201784, policy loss: 5.650038039653403
Experience 6, Iter 53, disc loss: 0.005928874097355754, policy loss: 5.590365606975502
Experience 6, Iter 54, disc loss: 0.005015246992283302, policy loss: 5.737397858778623
Experience 6, Iter 55, disc loss: 0.004873948263978748, policy loss: 5.626842467401499
Experience 6, Iter 56, disc loss: 0.0056146904829641655, policy loss: 5.653055557795198
Experience 6, Iter 57, disc loss: 0.004333343715893929, policy loss: 5.817505238020683
Experience 6, Iter 58, disc loss: 0.00588439438682509, policy loss: 5.662774572202862
Experience 6, Iter 59, disc loss: 0.006417420338811213, policy loss: 5.614079111313026
Experience 6, Iter 60, disc loss: 0.005998427515397584, policy loss: 5.559639041540508
Experience 6, Iter 61, disc loss: 0.004669537357725448, policy loss: 5.794455030470449
Experience 6, Iter 62, disc loss: 0.006355454274233524, policy loss: 5.533442034251287
Experience 6, Iter 63, disc loss: 0.005326513033817769, policy loss: 5.733623588969763
Experience 6, Iter 64, disc loss: 0.005038653677246388, policy loss: 5.70999507908276
Experience 6, Iter 65, disc loss: 0.004879112888249683, policy loss: 5.868648977760212
Experience 6, Iter 66, disc loss: 0.00443433810785822, policy loss: 5.77561142437108
Experience 6, Iter 67, disc loss: 0.00472119864633414, policy loss: 5.753105645203268
Experience 6, Iter 68, disc loss: 0.004095522770534892, policy loss: 5.9487758931749015
Experience 6, Iter 69, disc loss: 0.004198957398799702, policy loss: 5.877190203836756
Experience 6, Iter 70, disc loss: 0.00413231166608587, policy loss: 5.909951368190011
Experience 6, Iter 71, disc loss: 0.004950651684306637, policy loss: 5.680104199581674
Experience 6, Iter 72, disc loss: 0.004905565289547369, policy loss: 5.746648922545135
Experience 6, Iter 73, disc loss: 0.005636659372901576, policy loss: 5.61627364170554
Experience 6, Iter 74, disc loss: 0.004849989258068403, policy loss: 5.755334180387681
Experience 6, Iter 75, disc loss: 0.0048129864194871525, policy loss: 5.828021360432594
Experience 6, Iter 76, disc loss: 0.006335586839193358, policy loss: 5.70428986266284
Experience 6, Iter 77, disc loss: 0.0041754999292452424, policy loss: 5.949678984648932
Experience 6, Iter 78, disc loss: 0.004037737697245612, policy loss: 5.9759840847124295
Experience 6, Iter 79, disc loss: 0.004958993496648559, policy loss: 5.805873872395224
Experience 6, Iter 80, disc loss: 0.00551027622127301, policy loss: 5.639266028620015
Experience 6, Iter 81, disc loss: 0.005327088785694363, policy loss: 5.737741655618933
Experience 6, Iter 82, disc loss: 0.00496813074991275, policy loss: 5.707723786007318
Experience 6, Iter 83, disc loss: 0.004447697131808127, policy loss: 5.833770398802539
Experience 6, Iter 84, disc loss: 0.004789634014654416, policy loss: 5.774309334683643
Experience 6, Iter 85, disc loss: 0.005104612688667252, policy loss: 5.719403954130987
Experience 6, Iter 86, disc loss: 0.004375122314618362, policy loss: 5.907800310497004
Experience 6, Iter 87, disc loss: 0.0046823066703496905, policy loss: 5.783328844934022
Experience 6, Iter 88, disc loss: 0.00396020317605761, policy loss: 5.950178079835922
Experience 6, Iter 89, disc loss: 0.005156242937056208, policy loss: 5.805150808413696
Experience 6, Iter 90, disc loss: 0.0037477885842509538, policy loss: 6.012797871689192
Experience 6, Iter 91, disc loss: 0.0032113515611693934, policy loss: 6.157100328381442
Experience 6, Iter 92, disc loss: 0.00507700919179562, policy loss: 5.812274894945603
Experience 6, Iter 93, disc loss: 0.004383856587367381, policy loss: 5.913350681069532
Experience 6, Iter 94, disc loss: 0.003970407050596019, policy loss: 5.998940874612183
Experience 6, Iter 95, disc loss: 0.004609792089901977, policy loss: 5.790652445204088
Experience 6, Iter 96, disc loss: 0.004661209861720119, policy loss: 5.857190416627773
Experience 6, Iter 97, disc loss: 0.0043269239564181885, policy loss: 5.902973974820501
Experience 6, Iter 98, disc loss: 0.003972491034443117, policy loss: 6.062536627932277
Experience 6, Iter 99, disc loss: 0.004390751923886729, policy loss: 5.876055553499678
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0022],
        [0.0675],
        [0.0014]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.9595e-03, 1.4318e-02, 6.0108e-02, 1.7273e-03, 3.3916e-05,
          5.3562e-02]],

        [[2.9595e-03, 1.4318e-02, 6.0108e-02, 1.7273e-03, 3.3916e-05,
          5.3562e-02]],

        [[2.9595e-03, 1.4318e-02, 6.0108e-02, 1.7273e-03, 3.3916e-05,
          5.3562e-02]],

        [[2.9595e-03, 1.4318e-02, 6.0108e-02, 1.7273e-03, 3.3916e-05,
          5.3562e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0020, 0.0087, 0.2701, 0.0058], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0020, 0.0087, 0.2701, 0.0058])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.998
Iter 2/2000 - Loss: -0.047
Iter 3/2000 - Loss: -2.867
Iter 4/2000 - Loss: -2.445
Iter 5/2000 - Loss: -1.461
Iter 6/2000 - Loss: -1.804
Iter 7/2000 - Loss: -2.580
Iter 8/2000 - Loss: -2.942
Iter 9/2000 - Loss: -2.764
Iter 10/2000 - Loss: -2.402
Iter 11/2000 - Loss: -2.251
Iter 12/2000 - Loss: -2.400
Iter 13/2000 - Loss: -2.678
Iter 14/2000 - Loss: -2.871
Iter 15/2000 - Loss: -2.876
Iter 16/2000 - Loss: -2.746
Iter 17/2000 - Loss: -2.626
Iter 18/2000 - Loss: -2.625
Iter 19/2000 - Loss: -2.736
Iter 20/2000 - Loss: -2.866
Iter 1981/2000 - Loss: -8.801
Iter 1982/2000 - Loss: -8.801
Iter 1983/2000 - Loss: -8.801
Iter 1984/2000 - Loss: -8.801
Iter 1985/2000 - Loss: -8.801
Iter 1986/2000 - Loss: -8.801
Iter 1987/2000 - Loss: -8.801
Iter 1988/2000 - Loss: -8.801
Iter 1989/2000 - Loss: -8.801
Iter 1990/2000 - Loss: -8.801
Iter 1991/2000 - Loss: -8.801
Iter 1992/2000 - Loss: -8.801
Iter 1993/2000 - Loss: -8.801
Iter 1994/2000 - Loss: -8.801
Iter 1995/2000 - Loss: -8.801
Iter 1996/2000 - Loss: -8.801
Iter 1997/2000 - Loss: -8.801
Iter 1998/2000 - Loss: -8.801
Iter 1999/2000 - Loss: -8.801
Iter 2000/2000 - Loss: -8.801
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[12.7225,  1.3931, 42.4152,  5.3736,  1.1472, 13.1711]],

        [[18.4907, 30.6402, 24.0531,  1.6593,  5.1717,  6.3169]],

        [[20.1420, 34.6150, 20.3042,  1.1856,  4.5356, 11.3751]],

        [[18.6108,  9.5907,  6.1472,  2.1396,  6.3273, 36.3035]]])
Signal Variance: tensor([8.0623e-03, 2.9640e-01, 8.1631e+00, 1.2212e-01])
Estimated target variance: tensor([0.0020, 0.0087, 0.2701, 0.0058])
N: 70
Signal to noise ratio: tensor([ 4.7545, 31.5887, 62.4682, 25.5716])
Bound on condition number: tensor([  1583.3810,  69850.0145, 273160.7522,  45774.5878])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.05298721633844065, policy loss: 3.4897856921919725
Experience 7, Iter 1, disc loss: 0.04906558326081156, policy loss: 3.3094744073503906
Experience 7, Iter 2, disc loss: 0.06401026725777191, policy loss: 3.061821028770799
Experience 7, Iter 3, disc loss: 0.07318108473476471, policy loss: 2.93634466734877
Experience 7, Iter 4, disc loss: 0.06336771458557544, policy loss: 3.0117710121875447
Experience 7, Iter 5, disc loss: 0.06335578067647081, policy loss: 2.9512621152637806
Experience 7, Iter 6, disc loss: 0.06546492298651095, policy loss: 2.9452631255616053
Experience 7, Iter 7, disc loss: 0.06992417948090701, policy loss: 2.884417608103057
Experience 7, Iter 8, disc loss: 0.07753552239286063, policy loss: 2.7878339401344068
Experience 7, Iter 9, disc loss: 0.07647769325006486, policy loss: 2.8140267324423
Experience 7, Iter 10, disc loss: 0.07844797416186235, policy loss: 2.780356642462377
Experience 7, Iter 11, disc loss: 0.07866618103155744, policy loss: 2.789886044775032
Experience 7, Iter 12, disc loss: 0.08301198347531094, policy loss: 2.726313614754826
Experience 7, Iter 13, disc loss: 0.08244873120436645, policy loss: 2.713070326180838
Experience 7, Iter 14, disc loss: 0.08635859858017014, policy loss: 2.6356097860414884
Experience 7, Iter 15, disc loss: 0.08102086990634884, policy loss: 2.7223441193300837
Experience 7, Iter 16, disc loss: 0.08037583594441704, policy loss: 2.7112896540119262
Experience 7, Iter 17, disc loss: 0.0681182954875506, policy loss: 2.9099544411559224
Experience 7, Iter 18, disc loss: 0.06952622693999001, policy loss: 2.9118594497921824
Experience 7, Iter 19, disc loss: 0.06339065623760508, policy loss: 2.999140854768578
Experience 7, Iter 20, disc loss: 0.056893980731685896, policy loss: 3.0966290811075825
Experience 7, Iter 21, disc loss: 0.05646299000504227, policy loss: 3.109480373047828
Experience 7, Iter 22, disc loss: 0.04674743698172224, policy loss: 3.2988068584196033
Experience 7, Iter 23, disc loss: 0.04816994334850269, policy loss: 3.276948773280176
Experience 7, Iter 24, disc loss: 0.04109558018143681, policy loss: 3.4843979257036697
Experience 7, Iter 25, disc loss: 0.04340465913694057, policy loss: 3.4414448046123978
Experience 7, Iter 26, disc loss: 0.04209305652395817, policy loss: 3.4845125040159672
Experience 7, Iter 27, disc loss: 0.039095170011540815, policy loss: 3.5478378746261
Experience 7, Iter 28, disc loss: 0.039727796109910056, policy loss: 3.569243729709637
Experience 7, Iter 29, disc loss: 0.03647638130107769, policy loss: 3.671948228512065
Experience 7, Iter 30, disc loss: 0.035151168641447385, policy loss: 3.742793984460166
Experience 7, Iter 31, disc loss: 0.03548999385488447, policy loss: 3.7417498586013425
Experience 7, Iter 32, disc loss: 0.03474561459453703, policy loss: 3.7757649560127096
Experience 7, Iter 33, disc loss: 0.0329547246114305, policy loss: 3.8715712313281285
Experience 7, Iter 34, disc loss: 0.03375152223182805, policy loss: 3.8204332694498184
Experience 7, Iter 35, disc loss: 0.030232698917027293, policy loss: 3.963055668788364
Experience 7, Iter 36, disc loss: 0.029551565041142935, policy loss: 3.996386495099661
Experience 7, Iter 37, disc loss: 0.02783485792670241, policy loss: 4.051198962239626
Experience 7, Iter 38, disc loss: 0.02912755489279589, policy loss: 3.962116172198447
Experience 7, Iter 39, disc loss: 0.029104925839298877, policy loss: 3.956694735061748
Experience 7, Iter 40, disc loss: 0.026298969246127846, policy loss: 4.071127763708905
Experience 7, Iter 41, disc loss: 0.0226541890834085, policy loss: 4.267029446251096
Experience 7, Iter 42, disc loss: 0.024287135630325515, policy loss: 4.1334892591118955
Experience 7, Iter 43, disc loss: 0.024491460078977036, policy loss: 4.105379730436937
Experience 7, Iter 44, disc loss: 0.0225202564711268, policy loss: 4.202714360516
Experience 7, Iter 45, disc loss: 0.023425573664214286, policy loss: 4.121442258016424
Experience 7, Iter 46, disc loss: 0.021744033827602732, policy loss: 4.1959005439753625
Experience 7, Iter 47, disc loss: 0.02227744197340461, policy loss: 4.13322341848463
Experience 7, Iter 48, disc loss: 0.022164886069377575, policy loss: 4.17994667205459
Experience 7, Iter 49, disc loss: 0.021658603435232782, policy loss: 4.162882931289104
Experience 7, Iter 50, disc loss: 0.020131128237369306, policy loss: 4.2877838496790694
Experience 7, Iter 51, disc loss: 0.020087487422651155, policy loss: 4.293564736516354
Experience 7, Iter 52, disc loss: 0.018678733209703056, policy loss: 4.3599670274039735
Experience 7, Iter 53, disc loss: 0.01956066466291905, policy loss: 4.300056042524948
Experience 7, Iter 54, disc loss: 0.01846560400764914, policy loss: 4.368982131708196
Experience 7, Iter 55, disc loss: 0.01979650357305103, policy loss: 4.290303073254689
Experience 7, Iter 56, disc loss: 0.01841288085942033, policy loss: 4.401738290032288
Experience 7, Iter 57, disc loss: 0.020113222325075245, policy loss: 4.319136679721293
Experience 7, Iter 58, disc loss: 0.018904466895340127, policy loss: 4.411861514338057
Experience 7, Iter 59, disc loss: 0.018013322167064943, policy loss: 4.434586611562052
Experience 7, Iter 60, disc loss: 0.016105292751936763, policy loss: 4.618321837107198
Experience 7, Iter 61, disc loss: 0.018762816352500863, policy loss: 4.425550424322212
Experience 7, Iter 62, disc loss: 0.018569157907157255, policy loss: 4.538375768119561
Experience 7, Iter 63, disc loss: 0.016263447444153888, policy loss: 4.564702782603389
Experience 7, Iter 64, disc loss: 0.015706331862753997, policy loss: 4.64016977657582
Experience 7, Iter 65, disc loss: 0.017098074390705056, policy loss: 4.594688665081773
Experience 7, Iter 66, disc loss: 0.020496053452076383, policy loss: 4.442490512528805
Experience 7, Iter 67, disc loss: 0.015562731319605876, policy loss: 4.7130879565813455
Experience 7, Iter 68, disc loss: 0.01618005384968727, policy loss: 4.658596321014659
Experience 7, Iter 69, disc loss: 0.015141094349913675, policy loss: 4.809695174398256
Experience 7, Iter 70, disc loss: 0.016920659939886867, policy loss: 4.629663736373897
Experience 7, Iter 71, disc loss: 0.016599092527732142, policy loss: 4.821735031539044
Experience 7, Iter 72, disc loss: 0.012741660642363031, policy loss: 5.148852062407294
Experience 7, Iter 73, disc loss: 0.014989179028917853, policy loss: 4.871091863911585
Experience 7, Iter 74, disc loss: 0.014056068623752256, policy loss: 5.110341119178578
Experience 7, Iter 75, disc loss: 0.0131917724354854, policy loss: 5.039501217934596
Experience 7, Iter 76, disc loss: 0.013901911251333396, policy loss: 5.086391380359613
Experience 7, Iter 77, disc loss: 0.014439439590070564, policy loss: 4.983400471825293
Experience 7, Iter 78, disc loss: 0.013343781248742189, policy loss: 5.204769581865934
Experience 7, Iter 79, disc loss: 0.014447508870557277, policy loss: 5.058373155940661
Experience 7, Iter 80, disc loss: 0.013304997927798216, policy loss: 5.184304653621706
Experience 7, Iter 81, disc loss: 0.011978147936034562, policy loss: 5.350385134795824
Experience 7, Iter 82, disc loss: 0.012838343126117946, policy loss: 5.199894085976717
Experience 7, Iter 83, disc loss: 0.011783526037774902, policy loss: 5.3305845824068445
Experience 7, Iter 84, disc loss: 0.012313040346783547, policy loss: 5.3690842924974245
Experience 7, Iter 85, disc loss: 0.011127781974201613, policy loss: 5.550750254771909
Experience 7, Iter 86, disc loss: 0.010272078896821158, policy loss: 5.521422581737228
Experience 7, Iter 87, disc loss: 0.010088335999949292, policy loss: 5.67623654690628
Experience 7, Iter 88, disc loss: 0.009331089565581225, policy loss: 5.732636927178535
Experience 7, Iter 89, disc loss: 0.009514136365928934, policy loss: 5.644202048086238
Experience 7, Iter 90, disc loss: 0.010128338444177983, policy loss: 5.458477300038006
Experience 7, Iter 91, disc loss: 0.008564101281590878, policy loss: 5.736814664354292
Experience 7, Iter 92, disc loss: 0.009153239118313347, policy loss: 5.639806150259261
Experience 7, Iter 93, disc loss: 0.0099453938376519, policy loss: 5.466837809500466
Experience 7, Iter 94, disc loss: 0.007797998463585202, policy loss: 5.918904063110152
Experience 7, Iter 95, disc loss: 0.008829138063541521, policy loss: 5.694398877643058
Experience 7, Iter 96, disc loss: 0.007595831929673667, policy loss: 5.950509323415822
Experience 7, Iter 97, disc loss: 0.008340946938538865, policy loss: 5.7347405476299205
Experience 7, Iter 98, disc loss: 0.00885490938589818, policy loss: 5.520925171806786
Experience 7, Iter 99, disc loss: 0.007592569487275885, policy loss: 5.671914354894451
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.0113],
        [0.1407],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0083, 0.0697, 0.0933, 0.0048, 0.0011, 0.4317]],

        [[0.0083, 0.0697, 0.0933, 0.0048, 0.0011, 0.4317]],

        [[0.0083, 0.0697, 0.0933, 0.0048, 0.0011, 0.4317]],

        [[0.0083, 0.0697, 0.0933, 0.0048, 0.0011, 0.4317]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0075, 0.0453, 0.5628, 0.0091], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0075, 0.0453, 0.5628, 0.0091])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.907
Iter 2/2000 - Loss: -0.070
Iter 3/2000 - Loss: -0.905
Iter 4/2000 - Loss: -0.778
Iter 5/2000 - Loss: -0.482
Iter 6/2000 - Loss: -0.619
Iter 7/2000 - Loss: -0.861
Iter 8/2000 - Loss: -0.937
Iter 9/2000 - Loss: -0.854
Iter 10/2000 - Loss: -0.766
Iter 11/2000 - Loss: -0.786
Iter 12/2000 - Loss: -0.893
Iter 13/2000 - Loss: -0.999
Iter 14/2000 - Loss: -1.040
Iter 15/2000 - Loss: -1.026
Iter 16/2000 - Loss: -1.023
Iter 17/2000 - Loss: -1.084
Iter 18/2000 - Loss: -1.202
Iter 19/2000 - Loss: -1.333
Iter 20/2000 - Loss: -1.449
Iter 1981/2000 - Loss: -8.455
Iter 1982/2000 - Loss: -8.455
Iter 1983/2000 - Loss: -8.455
Iter 1984/2000 - Loss: -8.455
Iter 1985/2000 - Loss: -8.455
Iter 1986/2000 - Loss: -8.455
Iter 1987/2000 - Loss: -8.455
Iter 1988/2000 - Loss: -8.455
Iter 1989/2000 - Loss: -8.455
Iter 1990/2000 - Loss: -8.455
Iter 1991/2000 - Loss: -8.455
Iter 1992/2000 - Loss: -8.455
Iter 1993/2000 - Loss: -8.455
Iter 1994/2000 - Loss: -8.455
Iter 1995/2000 - Loss: -8.455
Iter 1996/2000 - Loss: -8.455
Iter 1997/2000 - Loss: -8.455
Iter 1998/2000 - Loss: -8.455
Iter 1999/2000 - Loss: -8.455
Iter 2000/2000 - Loss: -8.455
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.8783,  4.0422, 25.5019,  9.6455,  8.3583, 17.9064]],

        [[15.2834, 30.9335, 22.3698,  1.4248,  1.5636, 11.5094]],

        [[17.8659, 36.9127, 21.5687,  1.0292,  9.4677, 15.3961]],

        [[13.4553, 16.3513,  6.2055,  1.9274, 10.4940, 26.1222]]])
Signal Variance: tensor([0.0407, 0.4622, 8.9542, 0.1083])
Estimated target variance: tensor([0.0075, 0.0453, 0.5628, 0.0091])
N: 80
Signal to noise ratio: tensor([10.5089, 39.0277, 65.4476, 23.3373])
Bound on condition number: tensor([  8835.9612, 121853.7558, 342672.2865,  43571.4557])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.015828437562767902, policy loss: 4.767468160335934
Experience 8, Iter 1, disc loss: 0.019974568394467176, policy loss: 4.476449712441814
Experience 8, Iter 2, disc loss: 0.012259240155883889, policy loss: 5.063590109193253
Experience 8, Iter 3, disc loss: 0.016346185399426078, policy loss: 4.698532934766921
Experience 8, Iter 4, disc loss: 0.016463314623053844, policy loss: 4.647853588401832
Experience 8, Iter 5, disc loss: 0.01643663793751695, policy loss: 4.634720870659674
Experience 8, Iter 6, disc loss: 0.016842587851704487, policy loss: 4.6965003402585985
Experience 8, Iter 7, disc loss: 0.016178581808632932, policy loss: 4.7964595092923155
Experience 8, Iter 8, disc loss: 0.01415600749072159, policy loss: 5.039467997099276
Experience 8, Iter 9, disc loss: 0.016314583904397287, policy loss: 4.864445754184346
Experience 8, Iter 10, disc loss: 0.017380820798883995, policy loss: 4.744575567476531
Experience 8, Iter 11, disc loss: 0.012501025612206774, policy loss: 5.304720248629209
Experience 8, Iter 12, disc loss: 0.015448378435381204, policy loss: 4.979917942307992
Experience 8, Iter 13, disc loss: 0.016805956088179703, policy loss: 4.98789184963406
Experience 8, Iter 14, disc loss: 0.017184716453554398, policy loss: 4.845286911865017
Experience 8, Iter 15, disc loss: 0.01447110598986439, policy loss: 5.182053516542607
Experience 8, Iter 16, disc loss: 0.015878994941926287, policy loss: 5.131029392859979
Experience 8, Iter 17, disc loss: 0.013141175957182795, policy loss: 5.2768761790107845
Experience 8, Iter 18, disc loss: 0.01603854184175065, policy loss: 4.922480715081848
Experience 8, Iter 19, disc loss: 0.014496085486961937, policy loss: 5.119076775812729
Experience 8, Iter 20, disc loss: 0.01443945204614567, policy loss: 5.337979285708777
Experience 8, Iter 21, disc loss: 0.015080507117118724, policy loss: 5.249950422472629
Experience 8, Iter 22, disc loss: 0.018638326574043025, policy loss: 4.822825716496878
Experience 8, Iter 23, disc loss: 0.014406354253318512, policy loss: 5.159612242833496
Experience 8, Iter 24, disc loss: 0.013807423926650068, policy loss: 5.4315789806746
Experience 8, Iter 25, disc loss: 0.01434922112292451, policy loss: 5.515372969760117
Experience 8, Iter 26, disc loss: 0.015805868810450378, policy loss: 5.222494503885667
Experience 8, Iter 27, disc loss: 0.014115233450126063, policy loss: 5.243905576434925
Experience 8, Iter 28, disc loss: 0.014288640068788109, policy loss: 5.326531174799548
Experience 8, Iter 29, disc loss: 0.014127046222614193, policy loss: 5.463309362067823
Experience 8, Iter 30, disc loss: 0.011026949887475362, policy loss: 5.659639782070398
Experience 8, Iter 31, disc loss: 0.012843474387662712, policy loss: 5.340381124096758
Experience 8, Iter 32, disc loss: 0.011880511471768586, policy loss: 5.6036434938598845
Experience 8, Iter 33, disc loss: 0.010953276023935834, policy loss: 5.8061680019051725
Experience 8, Iter 34, disc loss: 0.011384814804774549, policy loss: 5.68694554912708
Experience 8, Iter 35, disc loss: 0.012049667682142533, policy loss: 5.766185819638534
Experience 8, Iter 36, disc loss: 0.012113183424632707, policy loss: 5.628112380184064
Experience 8, Iter 37, disc loss: 0.012749611758433081, policy loss: 5.559037825271551
Experience 8, Iter 38, disc loss: 0.011672724394485648, policy loss: 6.070410365700516
Experience 8, Iter 39, disc loss: 0.013733322700389322, policy loss: 5.595369324280588
Experience 8, Iter 40, disc loss: 0.010559714259832789, policy loss: 5.867803191521957
Experience 8, Iter 41, disc loss: 0.013525343936590096, policy loss: 5.90957003693539
Experience 8, Iter 42, disc loss: 0.010584771868297663, policy loss: 5.796316754105128
Experience 8, Iter 43, disc loss: 0.011016873067845323, policy loss: 5.9664968208980005
Experience 8, Iter 44, disc loss: 0.012111852109568094, policy loss: 5.815436230278614
Experience 8, Iter 45, disc loss: 0.009455959400018586, policy loss: 5.915298679208215
Experience 8, Iter 46, disc loss: 0.013362048644208877, policy loss: 5.842773165047594
Experience 8, Iter 47, disc loss: 0.011042339555074965, policy loss: 5.737223721091243
Experience 8, Iter 48, disc loss: 0.013637483249317496, policy loss: 5.78010921096641
Experience 8, Iter 49, disc loss: 0.010252945804190866, policy loss: 6.031467981207181
Experience 8, Iter 50, disc loss: 0.010789681768045683, policy loss: 5.93819094657988
Experience 8, Iter 51, disc loss: 0.011377893400411777, policy loss: 6.074708109828709
Experience 8, Iter 52, disc loss: 0.009617767752049023, policy loss: 5.8491919851587175
Experience 8, Iter 53, disc loss: 0.010873431102290922, policy loss: 5.951389603771073
Experience 8, Iter 54, disc loss: 0.01060885151393921, policy loss: 5.967596991323383
Experience 8, Iter 55, disc loss: 0.010713312336129583, policy loss: 5.780137966134589
Experience 8, Iter 56, disc loss: 0.012100112404336914, policy loss: 5.738380762419554
Experience 8, Iter 57, disc loss: 0.01553250206426662, policy loss: 5.895705397565377
Experience 8, Iter 58, disc loss: 0.014798325108223598, policy loss: 5.9110292065066385
Experience 8, Iter 59, disc loss: 0.011275111601542277, policy loss: 6.079537660729754
Experience 8, Iter 60, disc loss: 0.011784934394206983, policy loss: 6.275147305406026
Experience 8, Iter 61, disc loss: 0.011953218250901357, policy loss: 6.400992620474131
Experience 8, Iter 62, disc loss: 0.013035433825007246, policy loss: 6.116425486731225
Experience 8, Iter 63, disc loss: 0.011498348164897853, policy loss: 6.736976779718699
Experience 8, Iter 64, disc loss: 0.014562725774350943, policy loss: 5.966784783855197
Experience 8, Iter 65, disc loss: 0.014588872922510642, policy loss: 5.832535814332726
Experience 8, Iter 66, disc loss: 0.008940903305957723, policy loss: 6.233813174466063
Experience 8, Iter 67, disc loss: 0.010979651479332251, policy loss: 6.320716437049315
Experience 8, Iter 68, disc loss: 0.010573370032607294, policy loss: 6.300838747977997
Experience 8, Iter 69, disc loss: 0.008708085587979445, policy loss: 6.274452600033404
Experience 8, Iter 70, disc loss: 0.01523707630354302, policy loss: 6.232441798402677
Experience 8, Iter 71, disc loss: 0.008742578497109684, policy loss: 6.400852038462491
Experience 8, Iter 72, disc loss: 0.008107818189587144, policy loss: 6.505517001716434
Experience 8, Iter 73, disc loss: 0.010220173464735829, policy loss: 6.305643793166305
Experience 8, Iter 74, disc loss: 0.00882399608684223, policy loss: 6.457317837138545
Experience 8, Iter 75, disc loss: 0.008550492392620939, policy loss: 6.219979378467267
Experience 8, Iter 76, disc loss: 0.008892307082191193, policy loss: 6.211504566403708
Experience 8, Iter 77, disc loss: 0.010059434507549782, policy loss: 6.549954916128474
Experience 8, Iter 78, disc loss: 0.008254870960797136, policy loss: 7.029827771597688
Experience 8, Iter 79, disc loss: 0.01003067875023521, policy loss: 8.171033873379955
Experience 8, Iter 80, disc loss: 0.009421687378569178, policy loss: 7.6957431769083495
Experience 8, Iter 81, disc loss: 0.009260529521830593, policy loss: 7.2041319296663335
Experience 8, Iter 82, disc loss: 0.008475934653293387, policy loss: 6.2761413755200195
Experience 8, Iter 83, disc loss: 0.008111899212203611, policy loss: 7.123815594108724
Experience 8, Iter 84, disc loss: 0.0072953039299677, policy loss: 7.288769251878532
Experience 8, Iter 85, disc loss: 0.0052552656166996465, policy loss: 7.527505651381032
Experience 8, Iter 86, disc loss: 0.005426998052583313, policy loss: 7.221108911642549
Experience 8, Iter 87, disc loss: 0.004808854414660998, policy loss: 6.9224891329780744
Experience 8, Iter 88, disc loss: 0.0045734430838939785, policy loss: 6.904204922708735
Experience 8, Iter 89, disc loss: 0.0037626514907420565, policy loss: 6.988706495204882
Experience 8, Iter 90, disc loss: 0.003541002377252599, policy loss: 7.116292766378924
Experience 8, Iter 91, disc loss: 0.0035541180916159056, policy loss: 7.089186574312703
Experience 8, Iter 92, disc loss: 0.0033636875882509572, policy loss: 6.881717748490292
Experience 8, Iter 93, disc loss: 0.002655309547445378, policy loss: 7.075277963633656
Experience 8, Iter 94, disc loss: 0.002655393880371287, policy loss: 6.867756993472631
Experience 8, Iter 95, disc loss: 0.002193315919864988, policy loss: 7.1326062870655775
Experience 8, Iter 96, disc loss: 0.0020311167172845325, policy loss: 7.151244359854181
Experience 8, Iter 97, disc loss: 0.001852434144203351, policy loss: 7.055377470932704
Experience 8, Iter 98, disc loss: 0.0018320738669771654, policy loss: 7.045545790254123
Experience 8, Iter 99, disc loss: 0.001499346631059997, policy loss: 7.267412856291969
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.0368],
        [0.3777],
        [0.0021]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0074, 0.0645, 0.1091, 0.0043, 0.0010, 0.8367]],

        [[0.0074, 0.0645, 0.1091, 0.0043, 0.0010, 0.8367]],

        [[0.0074, 0.0645, 0.1091, 0.0043, 0.0010, 0.8367]],

        [[0.0074, 0.0645, 0.1091, 0.0043, 0.0010, 0.8367]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0067, 0.1471, 1.5110, 0.0083], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0067, 0.1471, 1.5110, 0.0083])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.064
Iter 2/2000 - Loss: 1.006
Iter 3/2000 - Loss: 0.078
Iter 4/2000 - Loss: 0.220
Iter 5/2000 - Loss: 0.541
Iter 6/2000 - Loss: 0.401
Iter 7/2000 - Loss: 0.130
Iter 8/2000 - Loss: 0.020
Iter 9/2000 - Loss: 0.090
Iter 10/2000 - Loss: 0.190
Iter 11/2000 - Loss: 0.181
Iter 12/2000 - Loss: 0.063
Iter 13/2000 - Loss: -0.075
Iter 14/2000 - Loss: -0.157
Iter 15/2000 - Loss: -0.172
Iter 16/2000 - Loss: -0.177
Iter 17/2000 - Loss: -0.238
Iter 18/2000 - Loss: -0.369
Iter 19/2000 - Loss: -0.533
Iter 20/2000 - Loss: -0.691
Iter 1981/2000 - Loss: -8.291
Iter 1982/2000 - Loss: -8.291
Iter 1983/2000 - Loss: -8.291
Iter 1984/2000 - Loss: -8.291
Iter 1985/2000 - Loss: -8.291
Iter 1986/2000 - Loss: -8.291
Iter 1987/2000 - Loss: -8.291
Iter 1988/2000 - Loss: -8.291
Iter 1989/2000 - Loss: -8.291
Iter 1990/2000 - Loss: -8.291
Iter 1991/2000 - Loss: -8.291
Iter 1992/2000 - Loss: -8.291
Iter 1993/2000 - Loss: -8.291
Iter 1994/2000 - Loss: -8.291
Iter 1995/2000 - Loss: -8.291
Iter 1996/2000 - Loss: -8.291
Iter 1997/2000 - Loss: -8.291
Iter 1998/2000 - Loss: -8.291
Iter 1999/2000 - Loss: -8.291
Iter 2000/2000 - Loss: -8.291
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[11.5590,  4.3877, 27.7913, 10.6159,  9.3250, 30.8671]],

        [[16.3999, 19.0532, 28.8365,  1.7055,  6.4616, 26.9363]],

        [[17.1820, 29.0139, 26.0542,  1.1545, 10.8744, 29.7911]],

        [[ 9.1384, 26.5074,  9.2643,  1.6707,  2.9365, 32.2808]]])
Signal Variance: tensor([ 0.0479,  2.5155, 24.9572,  0.1712])
Estimated target variance: tensor([0.0067, 0.1471, 1.5110, 0.0083])
N: 90
Signal to noise ratio: tensor([ 10.7124,  93.0776, 106.0276,  28.1799])
Bound on condition number: tensor([  10329.0525,  779710.5810, 1011768.3240,   71470.5951])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0008310687635276409, policy loss: 8.400622626344852
Experience 9, Iter 1, disc loss: 0.0008173413805359938, policy loss: 8.198158038449325
Experience 9, Iter 2, disc loss: 0.0008628886644518482, policy loss: 7.9888523775222
Experience 9, Iter 3, disc loss: 0.0009391917027801939, policy loss: 7.776827092154276
Experience 9, Iter 4, disc loss: 0.0011861443062445535, policy loss: 7.3434646338335465
Experience 9, Iter 5, disc loss: 0.0013050643471451948, policy loss: 7.040324606845719
Experience 9, Iter 6, disc loss: 0.0017265362531968375, policy loss: 6.6396728155743325
Experience 9, Iter 7, disc loss: 0.0020084699440980243, policy loss: 6.432209161677223
Experience 9, Iter 8, disc loss: 0.0018675976746525265, policy loss: 6.565736912924664
Experience 9, Iter 9, disc loss: 0.0019301264383546988, policy loss: 6.499136362945874
Experience 9, Iter 10, disc loss: 0.002024686449539627, policy loss: 6.417252412431036
Experience 9, Iter 11, disc loss: 0.001849583716955627, policy loss: 6.524681516074646
Experience 9, Iter 12, disc loss: 0.0015452652623790016, policy loss: 6.729832140836583
Experience 9, Iter 13, disc loss: 0.0016993638714631887, policy loss: 6.597764410165214
Experience 9, Iter 14, disc loss: 0.0016975893377160624, policy loss: 6.608617050147149
Experience 9, Iter 15, disc loss: 0.001706262683694604, policy loss: 6.586459974967498
Experience 9, Iter 16, disc loss: 0.0017910527098847122, policy loss: 6.54431017311142
Experience 9, Iter 17, disc loss: 0.002119885883927935, policy loss: 6.318546695845001
Experience 9, Iter 18, disc loss: 0.002233898129637077, policy loss: 6.296043337813552
Experience 9, Iter 19, disc loss: 0.0027330951715348896, policy loss: 6.073315329052687
Experience 9, Iter 20, disc loss: 0.002589293879887902, policy loss: 6.118667948269655
Experience 9, Iter 21, disc loss: 0.0024748512590740818, policy loss: 6.1662766148396
Experience 9, Iter 22, disc loss: 0.0024140282826560877, policy loss: 6.186040116247931
Experience 9, Iter 23, disc loss: 0.0027180034857430305, policy loss: 6.093009674276114
Experience 9, Iter 24, disc loss: 0.0023455649839989323, policy loss: 6.237136108786834
Experience 9, Iter 25, disc loss: 0.0024480192780949473, policy loss: 6.200258338463119
Experience 9, Iter 26, disc loss: 0.0024479201870421254, policy loss: 6.198543825444751
Experience 9, Iter 27, disc loss: 0.002496345048779309, policy loss: 6.182215512771093
Experience 9, Iter 28, disc loss: 0.002500604628941244, policy loss: 6.164294777765038
Experience 9, Iter 29, disc loss: 0.0026997159828604186, policy loss: 6.084598667206075
Experience 9, Iter 30, disc loss: 0.002654758676082795, policy loss: 6.125797887721372
Experience 9, Iter 31, disc loss: 0.002680112004689734, policy loss: 6.100303131584168
Experience 9, Iter 32, disc loss: 0.00269708683816712, policy loss: 6.0822654091023995
Experience 9, Iter 33, disc loss: 0.002545027778339891, policy loss: 6.1449126121190485
Experience 9, Iter 34, disc loss: 0.0027577335379039927, policy loss: 6.0665465954449775
Experience 9, Iter 35, disc loss: 0.002661016894045209, policy loss: 6.070789318986485
Experience 9, Iter 36, disc loss: 0.0028497697943614115, policy loss: 5.995506319486285
Experience 9, Iter 37, disc loss: 0.002584034852983389, policy loss: 6.133526108694035
Experience 9, Iter 38, disc loss: 0.0025632405357486634, policy loss: 6.136059188207881
Experience 9, Iter 39, disc loss: 0.0025133748821748736, policy loss: 6.182231913616048
Experience 9, Iter 40, disc loss: 0.002746600352066366, policy loss: 6.088264669349895
Experience 9, Iter 41, disc loss: 0.002724907487814624, policy loss: 6.117102459221078
Experience 9, Iter 42, disc loss: 0.00269534505972393, policy loss: 6.074505910655358
Experience 9, Iter 43, disc loss: 0.0025423811154803673, policy loss: 6.188204342290967
Experience 9, Iter 44, disc loss: 0.0028636927650895457, policy loss: 6.0012387235359625
Experience 9, Iter 45, disc loss: 0.0025509059745139656, policy loss: 6.137874594378456
Experience 9, Iter 46, disc loss: 0.0026835197848905272, policy loss: 6.079712288325278
Experience 9, Iter 47, disc loss: 0.0024234316362770963, policy loss: 6.2073135710119995
Experience 9, Iter 48, disc loss: 0.0024597634050215348, policy loss: 6.185625578879068
Experience 9, Iter 49, disc loss: 0.0025978549179654503, policy loss: 6.10929632884751
Experience 9, Iter 50, disc loss: 0.0027128188514529464, policy loss: 6.086130570522979
Experience 9, Iter 51, disc loss: 0.0025159192380394147, policy loss: 6.149344651933099
Experience 9, Iter 52, disc loss: 0.002556954758234285, policy loss: 6.134138120183744
Experience 9, Iter 53, disc loss: 0.002396273890321493, policy loss: 6.223132970468118
Experience 9, Iter 54, disc loss: 0.002675525776965712, policy loss: 6.081559986424676
Experience 9, Iter 55, disc loss: 0.0024721945305367105, policy loss: 6.164176225186465
Experience 9, Iter 56, disc loss: 0.0025266050708491706, policy loss: 6.164047320121837
Experience 9, Iter 57, disc loss: 0.002679938182591973, policy loss: 6.087161137340711
Experience 9, Iter 58, disc loss: 0.0025102916033783425, policy loss: 6.175213150553589
Experience 9, Iter 59, disc loss: 0.002436633600117068, policy loss: 6.22188933570009
Experience 9, Iter 60, disc loss: 0.002628874514668587, policy loss: 6.102407523140944
Experience 9, Iter 61, disc loss: 0.0025086132494832785, policy loss: 6.1067851030134825
Experience 9, Iter 62, disc loss: 0.0022299097899098302, policy loss: 6.2899226981628455
Experience 9, Iter 63, disc loss: 0.002271500270900023, policy loss: 6.28619267867186
Experience 9, Iter 64, disc loss: 0.002376202362701651, policy loss: 6.264364936026306
Experience 9, Iter 65, disc loss: 0.002581245558465509, policy loss: 6.155195417419694
Experience 9, Iter 66, disc loss: 0.0024057073428797398, policy loss: 6.2241603726026
Experience 9, Iter 67, disc loss: 0.002182681463615292, policy loss: 6.321134961856142
Experience 9, Iter 68, disc loss: 0.0023974797264904794, policy loss: 6.2093540106037
Experience 9, Iter 69, disc loss: 0.002384671922005706, policy loss: 6.233476336002565
Experience 9, Iter 70, disc loss: 0.0025063319076834966, policy loss: 6.153775252215786
Experience 9, Iter 71, disc loss: 0.0025631430759354393, policy loss: 6.108324225971842
Experience 9, Iter 72, disc loss: 0.002403304760861093, policy loss: 6.207140029332246
Experience 9, Iter 73, disc loss: 0.0022870087867204003, policy loss: 6.32092625814856
Experience 9, Iter 74, disc loss: 0.002440114825515007, policy loss: 6.208511130191026
Experience 9, Iter 75, disc loss: 0.0025182785799346195, policy loss: 6.151784237139944
Experience 9, Iter 76, disc loss: 0.002393416573024996, policy loss: 6.239460034180201
Experience 9, Iter 77, disc loss: 0.002441285689252404, policy loss: 6.177918572796087
Experience 9, Iter 78, disc loss: 0.0022239991224981136, policy loss: 6.287286639239616
Experience 9, Iter 79, disc loss: 0.002319900740557655, policy loss: 6.295814952908149
Experience 9, Iter 80, disc loss: 0.002390291741572401, policy loss: 6.202398299084207
Experience 9, Iter 81, disc loss: 0.0024133809167602572, policy loss: 6.219235225486834
Experience 9, Iter 82, disc loss: 0.00224698338965136, policy loss: 6.26985230889616
Experience 9, Iter 83, disc loss: 0.002267583533271437, policy loss: 6.283140145159598
Experience 9, Iter 84, disc loss: 0.002534543404377406, policy loss: 6.141265702742116
Experience 9, Iter 85, disc loss: 0.002345172123552513, policy loss: 6.219143859646328
Experience 9, Iter 86, disc loss: 0.002007465427157838, policy loss: 6.432118722148072
Experience 9, Iter 87, disc loss: 0.0025391425097877705, policy loss: 6.136761443517948
Experience 9, Iter 88, disc loss: 0.002201543899525587, policy loss: 6.3687739554485585
Experience 9, Iter 89, disc loss: 0.002327340437264856, policy loss: 6.252034312774956
Experience 9, Iter 90, disc loss: 0.0021504035711264985, policy loss: 6.330986860285171
Experience 9, Iter 91, disc loss: 0.0022162251447746684, policy loss: 6.317424216459346
Experience 9, Iter 92, disc loss: 0.0021471362592728025, policy loss: 6.316721380777901
Experience 9, Iter 93, disc loss: 0.002176700294003016, policy loss: 6.364187254565429
Experience 9, Iter 94, disc loss: 0.00215407626021434, policy loss: 6.350735717290894
Experience 9, Iter 95, disc loss: 0.0021600152573357195, policy loss: 6.385893173127512
Experience 9, Iter 96, disc loss: 0.002294155994042176, policy loss: 6.3028845514756435
Experience 9, Iter 97, disc loss: 0.0021701228615200532, policy loss: 6.312364142397972
Experience 9, Iter 98, disc loss: 0.0022697910596672535, policy loss: 6.230906153977647
Experience 9, Iter 99, disc loss: 0.0022460012041437827, policy loss: 6.273303183272949
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0350],
        [0.3615],
        [0.0019]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0068, 0.0585, 0.1037, 0.0039, 0.0009, 0.7856]],

        [[0.0068, 0.0585, 0.1037, 0.0039, 0.0009, 0.7856]],

        [[0.0068, 0.0585, 0.1037, 0.0039, 0.0009, 0.7856]],

        [[0.0068, 0.0585, 0.1037, 0.0039, 0.0009, 0.7856]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0061, 0.1401, 1.4459, 0.0078], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0061, 0.1401, 1.4459, 0.0078])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.063
Iter 2/2000 - Loss: 0.961
Iter 3/2000 - Loss: -0.045
Iter 4/2000 - Loss: 0.107
Iter 5/2000 - Loss: 0.456
Iter 6/2000 - Loss: 0.311
Iter 7/2000 - Loss: 0.022
Iter 8/2000 - Loss: -0.100
Iter 9/2000 - Loss: -0.026
Iter 10/2000 - Loss: 0.084
Iter 11/2000 - Loss: 0.087
Iter 12/2000 - Loss: -0.027
Iter 13/2000 - Loss: -0.169
Iter 14/2000 - Loss: -0.256
Iter 15/2000 - Loss: -0.273
Iter 16/2000 - Loss: -0.271
Iter 17/2000 - Loss: -0.319
Iter 18/2000 - Loss: -0.438
Iter 19/2000 - Loss: -0.598
Iter 20/2000 - Loss: -0.757
Iter 1981/2000 - Loss: -8.401
Iter 1982/2000 - Loss: -8.401
Iter 1983/2000 - Loss: -8.401
Iter 1984/2000 - Loss: -8.401
Iter 1985/2000 - Loss: -8.401
Iter 1986/2000 - Loss: -8.401
Iter 1987/2000 - Loss: -8.401
Iter 1988/2000 - Loss: -8.401
Iter 1989/2000 - Loss: -8.401
Iter 1990/2000 - Loss: -8.401
Iter 1991/2000 - Loss: -8.401
Iter 1992/2000 - Loss: -8.401
Iter 1993/2000 - Loss: -8.401
Iter 1994/2000 - Loss: -8.401
Iter 1995/2000 - Loss: -8.401
Iter 1996/2000 - Loss: -8.401
Iter 1997/2000 - Loss: -8.401
Iter 1998/2000 - Loss: -8.401
Iter 1999/2000 - Loss: -8.401
Iter 2000/2000 - Loss: -8.401
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.0502,  4.5793, 27.6781,  8.9539,  8.9067, 30.4171]],

        [[14.9258, 16.8137, 25.9250,  1.9341,  9.6505, 28.8977]],

        [[15.7312, 27.5845, 18.8639,  1.2204, 10.5380, 14.8196]],

        [[12.7536, 28.8533,  8.6910,  1.8410,  1.4690, 28.6900]]])
Signal Variance: tensor([ 0.0549,  2.8782, 15.5990,  0.1445])
Estimated target variance: tensor([0.0061, 0.1401, 1.4459, 0.0078])
N: 100
Signal to noise ratio: tensor([ 11.7513, 100.4377,  86.3994,  25.6176])
Bound on condition number: tensor([  13810.2428, 1008773.4424,  746486.1146,   65627.0050])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.002075638407887188, policy loss: 6.381318636744587
Experience 10, Iter 1, disc loss: 0.0021835849030914204, policy loss: 6.3492194511947035
Experience 10, Iter 2, disc loss: 0.0020934991297090868, policy loss: 6.356826427780996
Experience 10, Iter 3, disc loss: 0.0022204477489849517, policy loss: 6.267187982603112
Experience 10, Iter 4, disc loss: 0.0023647035625775103, policy loss: 6.2293687496842995
Experience 10, Iter 5, disc loss: 0.0019578094296978108, policy loss: 6.476255069341568
Experience 10, Iter 6, disc loss: 0.002059617581102191, policy loss: 6.36036126362885
Experience 10, Iter 7, disc loss: 0.0021664004570440846, policy loss: 6.3517440007835235
Experience 10, Iter 8, disc loss: 0.002303436970950348, policy loss: 6.301140259868985
Experience 10, Iter 9, disc loss: 0.0020690766933152247, policy loss: 6.3622795990188
Experience 10, Iter 10, disc loss: 0.001912813136618357, policy loss: 6.4658007905328585
Experience 10, Iter 11, disc loss: 0.002003429523791376, policy loss: 6.433825542179286
Experience 10, Iter 12, disc loss: 0.0020164062403321545, policy loss: 6.414640166636735
Experience 10, Iter 13, disc loss: 0.0019712432684041836, policy loss: 6.4645507176559445
Experience 10, Iter 14, disc loss: 0.002093842907901084, policy loss: 6.390953020031546
Experience 10, Iter 15, disc loss: 0.002085124475928218, policy loss: 6.39759455413763
Experience 10, Iter 16, disc loss: 0.0018495792998502228, policy loss: 6.489594825561995
Experience 10, Iter 17, disc loss: 0.0020578086028116608, policy loss: 6.44189152583918
Experience 10, Iter 18, disc loss: 0.002041639614958351, policy loss: 6.386516912301916
Experience 10, Iter 19, disc loss: 0.0019232076450954026, policy loss: 6.480553670278104
Experience 10, Iter 20, disc loss: 0.0020657209358367974, policy loss: 6.424779170378639
Experience 10, Iter 21, disc loss: 0.0020751712179699544, policy loss: 6.41776229166156
Experience 10, Iter 22, disc loss: 0.0020966916530186204, policy loss: 6.364449743545741
Experience 10, Iter 23, disc loss: 0.0019345007598720525, policy loss: 6.4413533631583615
Experience 10, Iter 24, disc loss: 0.0019461270813501373, policy loss: 6.471851742289925
Experience 10, Iter 25, disc loss: 0.0018849915919229073, policy loss: 6.5003620114740945
Experience 10, Iter 26, disc loss: 0.001710495595010511, policy loss: 6.593937770472776
Experience 10, Iter 27, disc loss: 0.0019928034944542623, policy loss: 6.433864127541669
Experience 10, Iter 28, disc loss: 0.0018646951441999653, policy loss: 6.502277615776208
Experience 10, Iter 29, disc loss: 0.0020184531047751633, policy loss: 6.399573776579839
Experience 10, Iter 30, disc loss: 0.0020358608814722242, policy loss: 6.4312421048883275
Experience 10, Iter 31, disc loss: 0.0020963621587435353, policy loss: 6.357244276036175
Experience 10, Iter 32, disc loss: 0.002031111088649169, policy loss: 6.39519839373218
Experience 10, Iter 33, disc loss: 0.001862344007005148, policy loss: 6.492257188324316
Experience 10, Iter 34, disc loss: 0.0020220704332643523, policy loss: 6.36372268100695
Experience 10, Iter 35, disc loss: 0.0019267440372815192, policy loss: 6.457455735115841
Experience 10, Iter 36, disc loss: 0.0018324436093480907, policy loss: 6.490783497367526
Experience 10, Iter 37, disc loss: 0.0018849655306925506, policy loss: 6.491760297674201
Experience 10, Iter 38, disc loss: 0.0018692681448750099, policy loss: 6.4634369793527995
Experience 10, Iter 39, disc loss: 0.0019604462114957326, policy loss: 6.399542566327022
Experience 10, Iter 40, disc loss: 0.0020835331480109982, policy loss: 6.374037971502033
Experience 10, Iter 41, disc loss: 0.001766696341270445, policy loss: 6.587584481839682
Experience 10, Iter 42, disc loss: 0.0019348441166919077, policy loss: 6.479161049220501
Experience 10, Iter 43, disc loss: 0.0018413763997602276, policy loss: 6.534677395205668
Experience 10, Iter 44, disc loss: 0.0018086524836714065, policy loss: 6.5259270438863375
Experience 10, Iter 45, disc loss: 0.001858282187595846, policy loss: 6.4709886336344145
Experience 10, Iter 46, disc loss: 0.001994855767458258, policy loss: 6.479361711877848
Experience 10, Iter 47, disc loss: 0.001800585683722816, policy loss: 6.603310833020986
Experience 10, Iter 48, disc loss: 0.00207422724311366, policy loss: 6.343879053803436
Experience 10, Iter 49, disc loss: 0.0018583306622161084, policy loss: 6.553715593259246
Experience 10, Iter 50, disc loss: 0.0017399190276610127, policy loss: 6.5320393387308675
Experience 10, Iter 51, disc loss: 0.0018369405579328438, policy loss: 6.58027406052879
Experience 10, Iter 52, disc loss: 0.0017275849345022522, policy loss: 6.565184181087143
Experience 10, Iter 53, disc loss: 0.0018733302005528901, policy loss: 6.48256652873048
Experience 10, Iter 54, disc loss: 0.0017684699269008577, policy loss: 6.571206611276969
Experience 10, Iter 55, disc loss: 0.0016915013723393963, policy loss: 6.631017377925357
Experience 10, Iter 56, disc loss: 0.001844430126400261, policy loss: 6.486472412285183
Experience 10, Iter 57, disc loss: 0.0019131235647743532, policy loss: 6.478265909851265
Experience 10, Iter 58, disc loss: 0.0019205751241548204, policy loss: 6.451233016684977
Experience 10, Iter 59, disc loss: 0.001748211729706362, policy loss: 6.539896789266213
Experience 10, Iter 60, disc loss: 0.0016810865101079384, policy loss: 6.598542525653437
Experience 10, Iter 61, disc loss: 0.0019227715909696287, policy loss: 6.429744256089996
Experience 10, Iter 62, disc loss: 0.0017392719105277358, policy loss: 6.607196322335192
Experience 10, Iter 63, disc loss: 0.001722384793469488, policy loss: 6.642313607960223
Experience 10, Iter 64, disc loss: 0.0016665291438622567, policy loss: 6.593050389346671
Experience 10, Iter 65, disc loss: 0.0019517542251825096, policy loss: 6.459182474453897
Experience 10, Iter 66, disc loss: 0.0016314329613650023, policy loss: 6.6773076630422965
Experience 10, Iter 67, disc loss: 0.0016539502461373683, policy loss: 6.6715084144276755
Experience 10, Iter 68, disc loss: 0.0017666913015392527, policy loss: 6.535261862491774
Experience 10, Iter 69, disc loss: 0.001737751137904801, policy loss: 6.577207065721868
Experience 10, Iter 70, disc loss: 0.001904815265864122, policy loss: 6.490782235472945
Experience 10, Iter 71, disc loss: 0.00153561356126279, policy loss: 6.746062533669583
Experience 10, Iter 72, disc loss: 0.0016320392965205061, policy loss: 6.654632521088157
Experience 10, Iter 73, disc loss: 0.001685508663601274, policy loss: 6.622274599836461
Experience 10, Iter 74, disc loss: 0.0017742019583070626, policy loss: 6.559639809409155
Experience 10, Iter 75, disc loss: 0.0017895642660477773, policy loss: 6.526740026545113
Experience 10, Iter 76, disc loss: 0.0018589425645459733, policy loss: 6.4828539001698395
Experience 10, Iter 77, disc loss: 0.0016472972052412234, policy loss: 6.63664128683061
Experience 10, Iter 78, disc loss: 0.001645634741555272, policy loss: 6.621049558254002
Experience 10, Iter 79, disc loss: 0.0016235380888210272, policy loss: 6.68949016445025
Experience 10, Iter 80, disc loss: 0.0017613873283440428, policy loss: 6.578104687276034
Experience 10, Iter 81, disc loss: 0.001577003678037912, policy loss: 6.662869729051107
Experience 10, Iter 82, disc loss: 0.001825315938387907, policy loss: 6.501429002338784
Experience 10, Iter 83, disc loss: 0.0016571175554178507, policy loss: 6.62518063089927
Experience 10, Iter 84, disc loss: 0.001688467359442738, policy loss: 6.566767416354015
Experience 10, Iter 85, disc loss: 0.0015743590076976392, policy loss: 6.659623594174837
Experience 10, Iter 86, disc loss: 0.0016782215764088006, policy loss: 6.697348205783511
Experience 10, Iter 87, disc loss: 0.0015633574841980766, policy loss: 6.705068314174587
Experience 10, Iter 88, disc loss: 0.0014727601742266258, policy loss: 6.731738205122354
Experience 10, Iter 89, disc loss: 0.0015606571356913293, policy loss: 6.721176303696554
Experience 10, Iter 90, disc loss: 0.0015333953530599221, policy loss: 6.691354684246983
Experience 10, Iter 91, disc loss: 0.001594649470613065, policy loss: 6.676437970121245
Experience 10, Iter 92, disc loss: 0.0016620740515966488, policy loss: 6.653381142475164
Experience 10, Iter 93, disc loss: 0.0015409941220198814, policy loss: 6.693669468525936
Experience 10, Iter 94, disc loss: 0.0016027188735051566, policy loss: 6.636925752474077
Experience 10, Iter 95, disc loss: 0.0015526702585135258, policy loss: 6.7266035367520915
Experience 10, Iter 96, disc loss: 0.001650368792645975, policy loss: 6.5846653752250885
Experience 10, Iter 97, disc loss: 0.001674223836087652, policy loss: 6.621069342516697
Experience 10, Iter 98, disc loss: 0.0014978482404280438, policy loss: 6.736665667447326
Experience 10, Iter 99, disc loss: 0.0016573672989958885, policy loss: 6.607164477723876
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0014],
        [0.0400],
        [0.4174],
        [0.0021]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[6.2900e-03, 5.4788e-02, 1.1407e-01, 3.6739e-03, 7.8849e-04,
          8.4703e-01]],

        [[6.2900e-03, 5.4788e-02, 1.1407e-01, 3.6739e-03, 7.8849e-04,
          8.4703e-01]],

        [[6.2900e-03, 5.4788e-02, 1.1407e-01, 3.6739e-03, 7.8849e-04,
          8.4703e-01]],

        [[6.2900e-03, 5.4788e-02, 1.1407e-01, 3.6739e-03, 7.8849e-04,
          8.4703e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0056, 0.1602, 1.6696, 0.0082], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0056, 0.1602, 1.6696, 0.0082])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.064
Iter 2/2000 - Loss: 1.091
Iter 3/2000 - Loss: 0.081
Iter 4/2000 - Loss: 0.234
Iter 5/2000 - Loss: 0.584
Iter 6/2000 - Loss: 0.437
Iter 7/2000 - Loss: 0.148
Iter 8/2000 - Loss: 0.033
Iter 9/2000 - Loss: 0.115
Iter 10/2000 - Loss: 0.229
Iter 11/2000 - Loss: 0.231
Iter 12/2000 - Loss: 0.116
Iter 13/2000 - Loss: -0.022
Iter 14/2000 - Loss: -0.100
Iter 15/2000 - Loss: -0.105
Iter 16/2000 - Loss: -0.095
Iter 17/2000 - Loss: -0.139
Iter 18/2000 - Loss: -0.255
Iter 19/2000 - Loss: -0.406
Iter 20/2000 - Loss: -0.550
Iter 1981/2000 - Loss: -8.414
Iter 1982/2000 - Loss: -8.414
Iter 1983/2000 - Loss: -8.414
Iter 1984/2000 - Loss: -8.414
Iter 1985/2000 - Loss: -8.414
Iter 1986/2000 - Loss: -8.414
Iter 1987/2000 - Loss: -8.414
Iter 1988/2000 - Loss: -8.414
Iter 1989/2000 - Loss: -8.415
Iter 1990/2000 - Loss: -8.415
Iter 1991/2000 - Loss: -8.415
Iter 1992/2000 - Loss: -8.415
Iter 1993/2000 - Loss: -8.415
Iter 1994/2000 - Loss: -8.415
Iter 1995/2000 - Loss: -8.415
Iter 1996/2000 - Loss: -8.415
Iter 1997/2000 - Loss: -8.415
Iter 1998/2000 - Loss: -8.415
Iter 1999/2000 - Loss: -8.415
Iter 2000/2000 - Loss: -8.415
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0023],
        [0.0002]])
Lengthscale: tensor([[[10.1608,  2.0465, 27.1775,  9.1429,  5.2465, 16.7622]],

        [[13.4290, 16.8137, 27.2754,  1.8211,  3.2966, 12.3803]],

        [[15.4224, 25.5637, 26.1096,  1.1495,  9.7251, 30.8379]],

        [[11.4497, 29.2032,  9.6035,  2.0874,  1.6923, 32.4919]]])
Signal Variance: tensor([ 0.0252,  1.2403, 24.4308,  0.1835])
Estimated target variance: tensor([0.0056, 0.1602, 1.6696, 0.0082])
N: 110
Signal to noise ratio: tensor([  8.1787,  66.1344, 103.1243,  28.3015])
Bound on condition number: tensor([   7359.1014,  481114.3952, 1169808.8122,   88108.1787])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0014523334416223603, policy loss: 6.728963158182349
Experience 11, Iter 1, disc loss: 0.0014463761778979159, policy loss: 6.80369139756546
Experience 11, Iter 2, disc loss: 0.0015138623922926627, policy loss: 6.699664462351041
Experience 11, Iter 3, disc loss: 0.0014373257328081832, policy loss: 6.854910722714057
Experience 11, Iter 4, disc loss: 0.0017168334463471186, policy loss: 6.582855342590231
Experience 11, Iter 5, disc loss: 0.0015074603636581782, policy loss: 6.765491291636785
Experience 11, Iter 6, disc loss: 0.001530916578052274, policy loss: 6.6846730388021465
Experience 11, Iter 7, disc loss: 0.0017119995673628443, policy loss: 6.592554317197918
Experience 11, Iter 8, disc loss: 0.0015100472022107209, policy loss: 6.7390913461305795
Experience 11, Iter 9, disc loss: 0.001421997495053566, policy loss: 6.858294013826593
Experience 11, Iter 10, disc loss: 0.0015825188484175927, policy loss: 6.729345140099975
Experience 11, Iter 11, disc loss: 0.001433276669477096, policy loss: 6.824866277194744
Experience 11, Iter 12, disc loss: 0.0014988059283728048, policy loss: 6.80209526331737
Experience 11, Iter 13, disc loss: 0.0015742467000633858, policy loss: 6.7236945705700455
Experience 11, Iter 14, disc loss: 0.001431054169909991, policy loss: 6.787282828281382
Experience 11, Iter 15, disc loss: 0.0013030681274908544, policy loss: 6.929929433690184
Experience 11, Iter 16, disc loss: 0.0016068652995242603, policy loss: 6.716881856607942
Experience 11, Iter 17, disc loss: 0.0015403112197582651, policy loss: 6.718434970459828
Experience 11, Iter 18, disc loss: 0.0015960149808736004, policy loss: 6.6410166953290375
Experience 11, Iter 19, disc loss: 0.001304892408027317, policy loss: 6.984523235556898
Experience 11, Iter 20, disc loss: 0.0014690083588206099, policy loss: 6.814961393680645
Experience 11, Iter 21, disc loss: 0.001440430183432015, policy loss: 6.819637204318965
Experience 11, Iter 22, disc loss: 0.0014912829468624368, policy loss: 6.743435800661269
Experience 11, Iter 23, disc loss: 0.001419343608401779, policy loss: 6.832057347923529
Experience 11, Iter 24, disc loss: 0.001602050957847972, policy loss: 6.653928066639365
Experience 11, Iter 25, disc loss: 0.0013089569859563808, policy loss: 6.989873512096321
Experience 11, Iter 26, disc loss: 0.0013729187557634476, policy loss: 6.946534083500232
Experience 11, Iter 27, disc loss: 0.0015319586381857108, policy loss: 6.730968554362272
Experience 11, Iter 28, disc loss: 0.0015265151999319618, policy loss: 6.713516466924836
Experience 11, Iter 29, disc loss: 0.001618234007749879, policy loss: 6.753580088641243
Experience 11, Iter 30, disc loss: 0.0013545771317080466, policy loss: 6.895212148054395
Experience 11, Iter 31, disc loss: 0.0014412620882281995, policy loss: 6.7843418007026965
Experience 11, Iter 32, disc loss: 0.0014769318442636684, policy loss: 6.73130424750649
Experience 11, Iter 33, disc loss: 0.0013858211989358814, policy loss: 6.877737513799478
Experience 11, Iter 34, disc loss: 0.0015730343000534743, policy loss: 6.716750862405583
Experience 11, Iter 35, disc loss: 0.0013774701454202548, policy loss: 6.920874862539986
Experience 11, Iter 36, disc loss: 0.0014432844027853675, policy loss: 6.769814454087497
Experience 11, Iter 37, disc loss: 0.0014715930195179023, policy loss: 6.7991640839902985
Experience 11, Iter 38, disc loss: 0.001380117013632987, policy loss: 6.842683314268893
Experience 11, Iter 39, disc loss: 0.0013556851484871925, policy loss: 6.862769894799625
Experience 11, Iter 40, disc loss: 0.0015628904814562015, policy loss: 6.693006887149732
Experience 11, Iter 41, disc loss: 0.0014187233924476156, policy loss: 6.833151596664517
Experience 11, Iter 42, disc loss: 0.001392552650705669, policy loss: 6.822475877941753
Experience 11, Iter 43, disc loss: 0.0014772848715920688, policy loss: 6.837384536663355
Experience 11, Iter 44, disc loss: 0.001587186792909145, policy loss: 6.700178078088517
Experience 11, Iter 45, disc loss: 0.0014539624921633698, policy loss: 6.7976043483420705
Experience 11, Iter 46, disc loss: 0.0014398658666524133, policy loss: 6.878012238456558
Experience 11, Iter 47, disc loss: 0.0013299825088275742, policy loss: 6.893850554611838
Experience 11, Iter 48, disc loss: 0.0013773574059809785, policy loss: 6.948540040147337
Experience 11, Iter 49, disc loss: 0.0015248183289809812, policy loss: 6.710263044500929
Experience 11, Iter 50, disc loss: 0.0014372860342151935, policy loss: 6.817494574002059
Experience 11, Iter 51, disc loss: 0.001335277235528795, policy loss: 6.9218824498294325
Experience 11, Iter 52, disc loss: 0.0014256935288545925, policy loss: 6.800943916304639
Experience 11, Iter 53, disc loss: 0.0013091795818580404, policy loss: 7.0505315387952985
Experience 11, Iter 54, disc loss: 0.0013379921876550936, policy loss: 6.962158924056057
Experience 11, Iter 55, disc loss: 0.0012226735731553887, policy loss: 6.986823464209066
Experience 11, Iter 56, disc loss: 0.0014050580037526029, policy loss: 6.955195850084819
Experience 11, Iter 57, disc loss: 0.0012900382305235136, policy loss: 6.990953481464968
Experience 11, Iter 58, disc loss: 0.001418740351789115, policy loss: 6.874209833185825
Experience 11, Iter 59, disc loss: 0.0014651153127260403, policy loss: 6.778485486452586
Experience 11, Iter 60, disc loss: 0.0012297225591337813, policy loss: 6.966439973084462
Experience 11, Iter 61, disc loss: 0.0013774097867908406, policy loss: 6.998207277338659
Experience 11, Iter 62, disc loss: 0.0014587207098998706, policy loss: 6.852328940038466
Experience 11, Iter 63, disc loss: 0.0013553651403614942, policy loss: 6.970029595145619
Experience 11, Iter 64, disc loss: 0.0013492243469201358, policy loss: 6.889223671963273
Experience 11, Iter 65, disc loss: 0.0011900729509691289, policy loss: 7.028120261913769
Experience 11, Iter 66, disc loss: 0.0012758172112262555, policy loss: 6.97140669302799
Experience 11, Iter 67, disc loss: 0.0015126155908306565, policy loss: 6.746008023501654
Experience 11, Iter 68, disc loss: 0.0013155365258512104, policy loss: 6.870515113955774
Experience 11, Iter 69, disc loss: 0.001558816934370704, policy loss: 6.71805790483354
Experience 11, Iter 70, disc loss: 0.0015238271385789228, policy loss: 6.7267679408847485
Experience 11, Iter 71, disc loss: 0.001409472491254285, policy loss: 6.8871220199225345
Experience 11, Iter 72, disc loss: 0.0014237822643133333, policy loss: 6.8227740960863095
Experience 11, Iter 73, disc loss: 0.0015022681382065482, policy loss: 6.758539788219759
Experience 11, Iter 74, disc loss: 0.0013539790871429998, policy loss: 6.871372788183999
Experience 11, Iter 75, disc loss: 0.001286398382926333, policy loss: 6.990040263061954
Experience 11, Iter 76, disc loss: 0.0016250799017171057, policy loss: 6.6639609543889335
Experience 11, Iter 77, disc loss: 0.0014825625502554879, policy loss: 6.803378990623961
Experience 11, Iter 78, disc loss: 0.0013269762511552811, policy loss: 6.987394038295037
Experience 11, Iter 79, disc loss: 0.001288899732254163, policy loss: 7.145004747755884
Experience 11, Iter 80, disc loss: 0.0013579473589762385, policy loss: 6.895402707391153
Experience 11, Iter 81, disc loss: 0.001419321604997145, policy loss: 6.843192703958056
Experience 11, Iter 82, disc loss: 0.0012602296860745297, policy loss: 7.013174673065654
Experience 11, Iter 83, disc loss: 0.0016692774960998702, policy loss: 6.653506029365494
Experience 11, Iter 84, disc loss: 0.0014363953104971424, policy loss: 6.813243067162231
Experience 11, Iter 85, disc loss: 0.0016832311400126908, policy loss: 6.779090421440419
Experience 11, Iter 86, disc loss: 0.001313923557088652, policy loss: 6.967034205852434
Experience 11, Iter 87, disc loss: 0.0013883888045677095, policy loss: 6.92226508609942
Experience 11, Iter 88, disc loss: 0.0014707934915001403, policy loss: 6.90467523508254
Experience 11, Iter 89, disc loss: 0.001489190562703762, policy loss: 6.86208953827859
Experience 11, Iter 90, disc loss: 0.0015842373719224157, policy loss: 6.890623101801774
Experience 11, Iter 91, disc loss: 0.001263807806166268, policy loss: 7.046924348905759
Experience 11, Iter 92, disc loss: 0.0014156840332103088, policy loss: 7.002273962360851
Experience 11, Iter 93, disc loss: 0.0015081325642857444, policy loss: 6.848616010654788
Experience 11, Iter 94, disc loss: 0.0015394704897486672, policy loss: 6.875920579478791
Experience 11, Iter 95, disc loss: 0.0016894079762003295, policy loss: 6.77638251039734
Experience 11, Iter 96, disc loss: 0.0014633186314013627, policy loss: 6.928182073636519
Experience 11, Iter 97, disc loss: 0.001477487770971902, policy loss: 6.913071121388104
Experience 11, Iter 98, disc loss: 0.001702937772502524, policy loss: 6.810378648000804
Experience 11, Iter 99, disc loss: 0.0012328950607410725, policy loss: 7.095613630221396
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0443],
        [0.4668],
        [0.0022]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.8093e-03, 5.1758e-02, 1.2464e-01, 3.4655e-03, 7.2431e-04,
          8.9434e-01]],

        [[5.8093e-03, 5.1758e-02, 1.2464e-01, 3.4655e-03, 7.2431e-04,
          8.9434e-01]],

        [[5.8093e-03, 5.1758e-02, 1.2464e-01, 3.4655e-03, 7.2431e-04,
          8.9434e-01]],

        [[5.8093e-03, 5.1758e-02, 1.2464e-01, 3.4655e-03, 7.2431e-04,
          8.9434e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0052, 0.1773, 1.8673, 0.0087], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0052, 0.1773, 1.8673, 0.0087])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.167
Iter 2/2000 - Loss: 1.212
Iter 3/2000 - Loss: 0.185
Iter 4/2000 - Loss: 0.342
Iter 5/2000 - Loss: 0.697
Iter 6/2000 - Loss: 0.549
Iter 7/2000 - Loss: 0.256
Iter 8/2000 - Loss: 0.142
Iter 9/2000 - Loss: 0.228
Iter 10/2000 - Loss: 0.349
Iter 11/2000 - Loss: 0.354
Iter 12/2000 - Loss: 0.239
Iter 13/2000 - Loss: 0.103
Iter 14/2000 - Loss: 0.028
Iter 15/2000 - Loss: 0.028
Iter 16/2000 - Loss: 0.042
Iter 17/2000 - Loss: -0.002
Iter 18/2000 - Loss: -0.122
Iter 19/2000 - Loss: -0.277
Iter 20/2000 - Loss: -0.424
Iter 1981/2000 - Loss: -8.450
Iter 1982/2000 - Loss: -8.450
Iter 1983/2000 - Loss: -8.450
Iter 1984/2000 - Loss: -8.450
Iter 1985/2000 - Loss: -8.450
Iter 1986/2000 - Loss: -8.450
Iter 1987/2000 - Loss: -8.450
Iter 1988/2000 - Loss: -8.450
Iter 1989/2000 - Loss: -8.450
Iter 1990/2000 - Loss: -8.450
Iter 1991/2000 - Loss: -8.450
Iter 1992/2000 - Loss: -8.450
Iter 1993/2000 - Loss: -8.450
Iter 1994/2000 - Loss: -8.450
Iter 1995/2000 - Loss: -8.450
Iter 1996/2000 - Loss: -8.450
Iter 1997/2000 - Loss: -8.450
Iter 1998/2000 - Loss: -8.450
Iter 1999/2000 - Loss: -8.450
Iter 2000/2000 - Loss: -8.450
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[12.0894,  4.3625, 28.7046, 11.1745,  8.3144, 19.4904]],

        [[15.2803,  8.4417, 27.9542,  4.0129, 12.0967, 25.5215]],

        [[15.6408, 18.1259, 20.7348,  1.1881,  9.0957, 13.6989]],

        [[10.8339, 26.2442,  9.0172,  2.0290,  1.9818, 28.1033]]])
Signal Variance: tensor([ 0.0386,  2.9647, 13.9807,  0.1567])
Estimated target variance: tensor([0.0052, 0.1773, 1.8673, 0.0087])
N: 120
Signal to noise ratio: tensor([10.0918, 94.5379, 80.0557, 26.3724])
Bound on condition number: tensor([  12222.3062, 1072490.4312,  769070.6083,   83461.4244])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0028168214525042725, policy loss: 6.799908922113779
Experience 12, Iter 1, disc loss: 0.0017417763452401854, policy loss: 6.78789933644891
Experience 12, Iter 2, disc loss: 0.0015553467717494, policy loss: 6.836088093598315
Experience 12, Iter 3, disc loss: 0.0016931048120308065, policy loss: 7.010821446511665
Experience 12, Iter 4, disc loss: 0.0017625074417471688, policy loss: 6.714209930954182
Experience 12, Iter 5, disc loss: 0.0015087707233804228, policy loss: 6.97743406731092
Experience 12, Iter 6, disc loss: 0.0018672927684666087, policy loss: 6.9605569565733285
Experience 12, Iter 7, disc loss: 0.0013393728249792743, policy loss: 7.0720771970555765
Experience 12, Iter 8, disc loss: 0.0016139077333588057, policy loss: 7.07546300067045
Experience 12, Iter 9, disc loss: 0.002278410191606879, policy loss: 6.839842104336208
Experience 12, Iter 10, disc loss: 0.0024348518831425205, policy loss: 6.698307946286152
Experience 12, Iter 11, disc loss: 0.0030198402238224135, policy loss: 6.610520184612129
Experience 12, Iter 12, disc loss: 0.003384566719746782, policy loss: 6.741829632307252
Experience 12, Iter 13, disc loss: 0.009002976587922594, policy loss: 6.306159124418885
Experience 12, Iter 14, disc loss: 0.004614614763663448, policy loss: 6.734123651819109
Experience 12, Iter 15, disc loss: 0.022729926846404354, policy loss: 5.815393871839102
Experience 12, Iter 16, disc loss: 0.0363474164754451, policy loss: 5.771346175646155
Experience 12, Iter 17, disc loss: 0.036622576095851164, policy loss: 5.3392647857497115
Experience 12, Iter 18, disc loss: 0.03281001963660551, policy loss: 5.555077900928491
Experience 12, Iter 19, disc loss: 0.07919819508978361, policy loss: 5.907415332150383
Experience 12, Iter 20, disc loss: 0.05149835225185658, policy loss: 6.225628403562729
Experience 12, Iter 21, disc loss: 0.031955265545025424, policy loss: 6.300359470451303
Experience 12, Iter 22, disc loss: 0.033397733424735426, policy loss: 6.070024429711146
Experience 12, Iter 23, disc loss: 0.02396806030307768, policy loss: 6.066194639262625
Experience 12, Iter 24, disc loss: 0.028080152541920603, policy loss: 6.872859344696921
Experience 12, Iter 25, disc loss: 0.029230904701942124, policy loss: 6.507466561714047
Experience 12, Iter 26, disc loss: 0.028235347302453424, policy loss: 7.454460572679629
Experience 12, Iter 27, disc loss: 0.03226883920167064, policy loss: 6.538400258192244
Experience 12, Iter 28, disc loss: 0.027556060670499663, policy loss: 7.0524593993549
Experience 12, Iter 29, disc loss: 0.020953599568962207, policy loss: 6.707543681535356
Experience 12, Iter 30, disc loss: 0.02497275129830015, policy loss: 6.207643766838738
Experience 12, Iter 31, disc loss: 0.020277032549449424, policy loss: 6.779741405015184
Experience 12, Iter 32, disc loss: 0.020410218610395273, policy loss: 6.505538368835301
Experience 12, Iter 33, disc loss: 0.030333641744587712, policy loss: 7.117381028335361
Experience 12, Iter 34, disc loss: 0.038536118088165455, policy loss: 6.199504915741617
Experience 12, Iter 35, disc loss: 0.07174348907000833, policy loss: 6.417640349952188
Experience 12, Iter 36, disc loss: 0.06981903877907093, policy loss: 6.76709311145801
Experience 12, Iter 37, disc loss: 0.03740767037528456, policy loss: 6.305532724059742
Experience 12, Iter 38, disc loss: 0.020430510129914876, policy loss: 7.323652215230922
Experience 12, Iter 39, disc loss: 0.032675199196818716, policy loss: 7.129892262316602
Experience 12, Iter 40, disc loss: 0.023637359269579328, policy loss: 7.822486852531157
Experience 12, Iter 41, disc loss: 0.032102877811596375, policy loss: 7.1202102106317335
Experience 12, Iter 42, disc loss: 0.06101108682320625, policy loss: 6.571447417305596
Experience 12, Iter 43, disc loss: 0.0313469374994549, policy loss: 7.961516200248352
Experience 12, Iter 44, disc loss: 0.0388183086056304, policy loss: 7.637239825293071
Experience 12, Iter 45, disc loss: 0.06693494762883745, policy loss: 7.586351690176398
Experience 12, Iter 46, disc loss: 0.035141667330655664, policy loss: 7.644574906626469
Experience 12, Iter 47, disc loss: 0.048465524289488596, policy loss: 7.0993079245385236
Experience 12, Iter 48, disc loss: 0.042869093720125914, policy loss: 7.489198724783947
Experience 12, Iter 49, disc loss: 0.022630616664723116, policy loss: 7.907351242479331
Experience 12, Iter 50, disc loss: 0.018013252888070763, policy loss: 7.426417267871509
Experience 12, Iter 51, disc loss: 0.013790215064110671, policy loss: 8.121339124983985
Experience 12, Iter 52, disc loss: 0.018881835116125976, policy loss: 7.530825433563714
Experience 12, Iter 53, disc loss: 0.010912095322953898, policy loss: 7.785660476435435
Experience 12, Iter 54, disc loss: 0.013125072377326755, policy loss: 7.040647405959545
Experience 12, Iter 55, disc loss: 0.018018482483831165, policy loss: 7.5018282401865735
Experience 12, Iter 56, disc loss: 0.009543176758373071, policy loss: 7.18202497991459
Experience 12, Iter 57, disc loss: 0.010294686750850563, policy loss: 7.254904583859066
Experience 12, Iter 58, disc loss: 0.015478826276318414, policy loss: 7.373076347352062
Experience 12, Iter 59, disc loss: 0.008767410110438136, policy loss: 7.53947805988154
Experience 12, Iter 60, disc loss: 0.008410992308443667, policy loss: 7.282555835572568
Experience 12, Iter 61, disc loss: 0.00962218713290635, policy loss: 6.905948851540723
Experience 12, Iter 62, disc loss: 0.01197960055558588, policy loss: 7.578029064368607
Experience 12, Iter 63, disc loss: 0.01319860660660811, policy loss: 6.668102898297527
Experience 12, Iter 64, disc loss: 0.007512871401478017, policy loss: 7.771963685912622
Experience 12, Iter 65, disc loss: 0.011054490802212352, policy loss: 7.175938972779027
Experience 12, Iter 66, disc loss: 0.008996152568943589, policy loss: 7.091280885999865
Experience 12, Iter 67, disc loss: 0.01904528198120653, policy loss: 7.704931686165363
Experience 12, Iter 68, disc loss: 0.00749996668607905, policy loss: 7.411533719599966
Experience 12, Iter 69, disc loss: 0.007791368782740827, policy loss: 7.158721594205008
Experience 12, Iter 70, disc loss: 0.010060783237033749, policy loss: 7.6887166101252316
Experience 12, Iter 71, disc loss: 0.005842016543932844, policy loss: 7.623695858783287
Experience 12, Iter 72, disc loss: 0.010244388635294267, policy loss: 7.361596514875189
Experience 12, Iter 73, disc loss: 0.0060034645150921235, policy loss: 7.589526396235737
Experience 12, Iter 74, disc loss: 0.022861729127272586, policy loss: 7.960336697613947
Experience 12, Iter 75, disc loss: 0.022773739840222965, policy loss: 7.270646612118937
Experience 12, Iter 76, disc loss: 0.007722515924419148, policy loss: 7.986761761387784
Experience 12, Iter 77, disc loss: 0.010228947472989844, policy loss: 7.732344397938778
Experience 12, Iter 78, disc loss: 0.014439854086348541, policy loss: 8.043403083071622
Experience 12, Iter 79, disc loss: 0.010100352721733916, policy loss: 7.67265018094645
Experience 12, Iter 80, disc loss: 0.014976645251934602, policy loss: 7.794574059718302
Experience 12, Iter 81, disc loss: 0.012499181913927621, policy loss: 7.707149534517113
Experience 12, Iter 82, disc loss: 0.023408555417909774, policy loss: 8.14720807217166
Experience 12, Iter 83, disc loss: 0.011254998909693411, policy loss: 8.18773680153711
Experience 12, Iter 84, disc loss: 0.009115120768675676, policy loss: 7.856269352775211
Experience 12, Iter 85, disc loss: 0.012318134618818187, policy loss: 7.5962960162021655
Experience 12, Iter 86, disc loss: 0.011380643821857082, policy loss: 8.263604121959728
Experience 12, Iter 87, disc loss: 0.00827145221887312, policy loss: 7.756509643367739
Experience 12, Iter 88, disc loss: 0.014495590458029369, policy loss: 7.956525899901601
Experience 12, Iter 89, disc loss: 0.009334716251721283, policy loss: 7.544414458702367
Experience 12, Iter 90, disc loss: 0.020971486253429032, policy loss: 7.468764098938884
Experience 12, Iter 91, disc loss: 0.028206760649642157, policy loss: 7.498377958331978
Experience 12, Iter 92, disc loss: 0.009886497636680161, policy loss: 7.637246982499462
Experience 12, Iter 93, disc loss: 0.014623376867810239, policy loss: 8.241398841456773
Experience 12, Iter 94, disc loss: 0.009929933301300656, policy loss: 8.219626090064574
Experience 12, Iter 95, disc loss: 0.021347192025786284, policy loss: 7.683787706068307
Experience 12, Iter 96, disc loss: 0.01422850406265836, policy loss: 7.503980357935811
Experience 12, Iter 97, disc loss: 0.008004382938217224, policy loss: 8.466622753574441
Experience 12, Iter 98, disc loss: 0.010355312179749282, policy loss: 8.331573358145668
Experience 12, Iter 99, disc loss: 0.008429836450037642, policy loss: 7.977419712419282
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0597],
        [0.6613],
        [0.0043]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.7140e-03, 5.4851e-02, 2.2650e-01, 5.0258e-03, 8.4992e-04,
          1.1479e+00]],

        [[5.7140e-03, 5.4851e-02, 2.2650e-01, 5.0258e-03, 8.4992e-04,
          1.1479e+00]],

        [[5.7140e-03, 5.4851e-02, 2.2650e-01, 5.0258e-03, 8.4992e-04,
          1.1479e+00]],

        [[5.7140e-03, 5.4851e-02, 2.2650e-01, 5.0258e-03, 8.4992e-04,
          1.1479e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0053, 0.2389, 2.6451, 0.0172], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0053, 0.2389, 2.6451, 0.0172])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.835
Iter 2/2000 - Loss: 1.677
Iter 3/2000 - Loss: 0.844
Iter 4/2000 - Loss: 0.971
Iter 5/2000 - Loss: 1.257
Iter 6/2000 - Loss: 1.128
Iter 7/2000 - Loss: 0.883
Iter 8/2000 - Loss: 0.787
Iter 9/2000 - Loss: 0.857
Iter 10/2000 - Loss: 0.949
Iter 11/2000 - Loss: 0.938
Iter 12/2000 - Loss: 0.831
Iter 13/2000 - Loss: 0.712
Iter 14/2000 - Loss: 0.644
Iter 15/2000 - Loss: 0.623
Iter 16/2000 - Loss: 0.601
Iter 17/2000 - Loss: 0.532
Iter 18/2000 - Loss: 0.409
Iter 19/2000 - Loss: 0.252
Iter 20/2000 - Loss: 0.087
Iter 1981/2000 - Loss: -8.204
Iter 1982/2000 - Loss: -8.204
Iter 1983/2000 - Loss: -8.204
Iter 1984/2000 - Loss: -8.204
Iter 1985/2000 - Loss: -8.204
Iter 1986/2000 - Loss: -8.204
Iter 1987/2000 - Loss: -8.204
Iter 1988/2000 - Loss: -8.204
Iter 1989/2000 - Loss: -8.204
Iter 1990/2000 - Loss: -8.204
Iter 1991/2000 - Loss: -8.204
Iter 1992/2000 - Loss: -8.204
Iter 1993/2000 - Loss: -8.204
Iter 1994/2000 - Loss: -8.204
Iter 1995/2000 - Loss: -8.204
Iter 1996/2000 - Loss: -8.204
Iter 1997/2000 - Loss: -8.204
Iter 1998/2000 - Loss: -8.204
Iter 1999/2000 - Loss: -8.204
Iter 2000/2000 - Loss: -8.204
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[12.8262,  6.0251, 34.6115, 13.6039, 12.4716, 36.6384]],

        [[15.6832, 26.2292, 15.9934,  1.1853,  5.8116, 13.0396]],

        [[14.7139, 26.5470, 11.8866,  1.0818,  8.0660, 13.2727]],

        [[14.3925, 30.5996, 16.9133,  3.9955,  1.2757, 32.4667]]])
Signal Variance: tensor([ 0.0708,  1.3301, 12.6382,  0.4721])
Estimated target variance: tensor([0.0053, 0.2389, 2.6451, 0.0172])
N: 130
Signal to noise ratio: tensor([13.9473, 65.2804, 76.9173, 45.3640])
Bound on condition number: tensor([ 25289.4969, 553999.8847, 769117.1270, 267526.5710])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.006356590303052827, policy loss: 8.489680411512655
Experience 13, Iter 1, disc loss: 0.005541826129373944, policy loss: 8.673464642511995
Experience 13, Iter 2, disc loss: 0.005776843208177255, policy loss: 8.003751280698832
Experience 13, Iter 3, disc loss: 0.006063296272108766, policy loss: 7.827959652967991
Experience 13, Iter 4, disc loss: 0.005499698514406971, policy loss: 7.924392323063961
Experience 13, Iter 5, disc loss: 0.006508751686995476, policy loss: 7.64183590352322
Experience 13, Iter 6, disc loss: 0.004642436456130688, policy loss: 7.630730719922898
Experience 13, Iter 7, disc loss: 0.00439512400343417, policy loss: 7.955167684284505
Experience 13, Iter 8, disc loss: 0.0037654987779077165, policy loss: 8.098844794693594
Experience 13, Iter 9, disc loss: 0.003995164594000723, policy loss: 8.123194308818015
Experience 13, Iter 10, disc loss: 0.0043000111789484865, policy loss: 8.107062572127258
Experience 13, Iter 11, disc loss: 0.004013293814431041, policy loss: 8.172516056786522
Experience 13, Iter 12, disc loss: 0.003817649857305509, policy loss: 7.61969350170785
Experience 13, Iter 13, disc loss: 0.009204225674891627, policy loss: 6.8988372908638365
Experience 13, Iter 14, disc loss: 0.005480817439376501, policy loss: 7.584465802714537
Experience 13, Iter 15, disc loss: 0.0051723571874579466, policy loss: 6.916992289447753
Experience 13, Iter 16, disc loss: 0.004534658350201373, policy loss: 7.127291520422919
Experience 13, Iter 17, disc loss: 0.0045842540465815314, policy loss: 7.310495029982744
Experience 13, Iter 18, disc loss: 0.0041094715941015734, policy loss: 7.415395796791698
Experience 13, Iter 19, disc loss: 0.0037401324751656397, policy loss: 7.579834067088252
Experience 13, Iter 20, disc loss: 0.004417859108737356, policy loss: 7.265198980778262
Experience 13, Iter 21, disc loss: 0.00373402730835374, policy loss: 8.00763662141748
Experience 13, Iter 22, disc loss: 0.00383174423017273, policy loss: 7.395036170976766
Experience 13, Iter 23, disc loss: 0.004705994047995059, policy loss: 7.318642159948585
Experience 13, Iter 24, disc loss: 0.00407123339855196, policy loss: 7.148689354790228
Experience 13, Iter 25, disc loss: 0.004713680695546976, policy loss: 6.921223859318022
Experience 13, Iter 26, disc loss: 0.005693310554482797, policy loss: 6.807906274143795
Experience 13, Iter 27, disc loss: 0.005468852161388718, policy loss: 7.288973725429239
Experience 13, Iter 28, disc loss: 0.0050991149597277765, policy loss: 7.825480816498084
Experience 13, Iter 29, disc loss: 0.004346702522533434, policy loss: 7.174486818424516
Experience 13, Iter 30, disc loss: 0.010219159691301393, policy loss: 7.2268783647620465
Experience 13, Iter 31, disc loss: 0.008074401516707257, policy loss: 7.190760293977
Experience 13, Iter 32, disc loss: 0.003861657014587022, policy loss: 7.425190766883522
Experience 13, Iter 33, disc loss: 0.003280052415628198, policy loss: 7.665574903709399
Experience 13, Iter 34, disc loss: 0.004426029631944769, policy loss: 7.614232861005721
Experience 13, Iter 35, disc loss: 0.007097152205986147, policy loss: 6.8454047905704964
Experience 13, Iter 36, disc loss: 0.010034232349098695, policy loss: 7.101903280622904
Experience 13, Iter 37, disc loss: 0.004955330500413814, policy loss: 7.205395838277673
Experience 13, Iter 38, disc loss: 0.010421686901450148, policy loss: 7.271976446965406
Experience 13, Iter 39, disc loss: 0.009159007720392888, policy loss: 6.87751828256207
Experience 13, Iter 40, disc loss: 0.005068536983972581, policy loss: 7.117116213212343
Experience 13, Iter 41, disc loss: 0.010252151214373534, policy loss: 7.23731376230067
Experience 13, Iter 42, disc loss: 0.00626040709302445, policy loss: 7.2635577746198745
Experience 13, Iter 43, disc loss: 0.0071136129892353794, policy loss: 7.973203575894065
Experience 13, Iter 44, disc loss: 0.007631123256934873, policy loss: 7.734976712538593
Experience 13, Iter 45, disc loss: 0.005341538525756091, policy loss: 7.645535338570459
Experience 13, Iter 46, disc loss: 0.006298235716426395, policy loss: 7.141586176312606
Experience 13, Iter 47, disc loss: 0.007847889688762232, policy loss: 7.496313522635359
Experience 13, Iter 48, disc loss: 0.006625744911100536, policy loss: 8.32818365455711
Experience 13, Iter 49, disc loss: 0.006809689310190622, policy loss: 7.151589392664965
Experience 13, Iter 50, disc loss: 0.005376927928011779, policy loss: 7.964614184798348
Experience 13, Iter 51, disc loss: 0.004785741359748024, policy loss: 7.703433342705736
Experience 13, Iter 52, disc loss: 0.005256410347612086, policy loss: 7.258184122826692
Experience 13, Iter 53, disc loss: 0.004887000179126034, policy loss: 7.551638019389861
Experience 13, Iter 54, disc loss: 0.004674102207039909, policy loss: 7.747223838758618
Experience 13, Iter 55, disc loss: 0.0059957217275220555, policy loss: 6.840465283528943
Experience 13, Iter 56, disc loss: 0.0068002770359212785, policy loss: 7.346166878213224
Experience 13, Iter 57, disc loss: 0.005513037180238002, policy loss: 7.595337407179607
Experience 13, Iter 58, disc loss: 0.009057966770783184, policy loss: 6.909734723868777
Experience 13, Iter 59, disc loss: 0.005470140892600043, policy loss: 7.322238078736475
Experience 13, Iter 60, disc loss: 0.004828576593828621, policy loss: 8.001839879634689
Experience 13, Iter 61, disc loss: 0.005106336340037105, policy loss: 8.13984176907377
Experience 13, Iter 62, disc loss: 0.0055359595736007156, policy loss: 7.460413793233921
Experience 13, Iter 63, disc loss: 0.004808419298198985, policy loss: 7.680681607081087
Experience 13, Iter 64, disc loss: 0.004699394994292555, policy loss: 7.816859518809578
Experience 13, Iter 65, disc loss: 0.004970947776897428, policy loss: 7.492829285219446
Experience 13, Iter 66, disc loss: 0.003732802623628633, policy loss: 7.869535204592935
Experience 13, Iter 67, disc loss: 0.0037506037152768303, policy loss: 7.847762521029395
Experience 13, Iter 68, disc loss: 0.0033460966305608056, policy loss: 8.021288660479208
Experience 13, Iter 69, disc loss: 0.003998821856990301, policy loss: 7.7129278834113775
Experience 13, Iter 70, disc loss: 0.004859965931997962, policy loss: 7.483342788729832
Experience 13, Iter 71, disc loss: 0.0034856421326559354, policy loss: 7.667845074196159
Experience 13, Iter 72, disc loss: 0.0034520517475460184, policy loss: 8.079937149816196
Experience 13, Iter 73, disc loss: 0.004542421366607881, policy loss: 7.494955372382576
Experience 13, Iter 74, disc loss: 0.004739008109252318, policy loss: 7.405393764410863
Experience 13, Iter 75, disc loss: 0.003933167058895284, policy loss: 7.466993188196993
Experience 13, Iter 76, disc loss: 0.003688120943151147, policy loss: 7.2564700763573216
Experience 13, Iter 77, disc loss: 0.004136720346914976, policy loss: 7.05676257877534
Experience 13, Iter 78, disc loss: 0.0040213182872222905, policy loss: 7.307480211788491
Experience 13, Iter 79, disc loss: 0.00522401781457633, policy loss: 6.838678048177847
Experience 13, Iter 80, disc loss: 0.004284997703686333, policy loss: 6.8689476842505535
Experience 13, Iter 81, disc loss: 0.005960084561896915, policy loss: 6.9281232535888195
Experience 13, Iter 82, disc loss: 0.0049323590407184615, policy loss: 7.40564326218597
Experience 13, Iter 83, disc loss: 0.007999615193304783, policy loss: 7.056563383822026
Experience 13, Iter 84, disc loss: 0.005555363318039964, policy loss: 6.9898758637126175
Experience 13, Iter 85, disc loss: 0.00470766643493571, policy loss: 7.77627234030501
Experience 13, Iter 86, disc loss: 0.0042152869380444125, policy loss: 7.1746026231546844
Experience 13, Iter 87, disc loss: 0.0048948768037313135, policy loss: 7.156547197768635
Experience 13, Iter 88, disc loss: 0.005746566140601869, policy loss: 6.827810132020871
Experience 13, Iter 89, disc loss: 0.004451656701029349, policy loss: 7.329140696616563
Experience 13, Iter 90, disc loss: 0.0046853577572275665, policy loss: 7.3139769280036315
Experience 13, Iter 91, disc loss: 0.004090755536629492, policy loss: 7.3689379968576585
Experience 13, Iter 92, disc loss: 0.004955045943523032, policy loss: 6.8846040068979715
Experience 13, Iter 93, disc loss: 0.0054092249967469955, policy loss: 7.020295563029006
Experience 13, Iter 94, disc loss: 0.005803795133021454, policy loss: 6.963786290572855
Experience 13, Iter 95, disc loss: 0.004989379229659714, policy loss: 6.990808133036554
Experience 13, Iter 96, disc loss: 0.004038034498858234, policy loss: 7.325800272893305
Experience 13, Iter 97, disc loss: 0.0041544568043716205, policy loss: 7.5313913843766365
Experience 13, Iter 98, disc loss: 0.0043559642595766505, policy loss: 7.376577130165783
Experience 13, Iter 99, disc loss: 0.004878104565199163, policy loss: 7.3174122965745605
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0014],
        [0.0736],
        [0.8370],
        [0.0061]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.3943e-03, 5.6881e-02, 2.9096e-01, 5.8527e-03, 8.5225e-04,
          1.3539e+00]],

        [[5.3943e-03, 5.6881e-02, 2.9096e-01, 5.8527e-03, 8.5225e-04,
          1.3539e+00]],

        [[5.3943e-03, 5.6881e-02, 2.9096e-01, 5.8527e-03, 8.5225e-04,
          1.3539e+00]],

        [[5.3943e-03, 5.6881e-02, 2.9096e-01, 5.8527e-03, 8.5225e-04,
          1.3539e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0056, 0.2946, 3.3482, 0.0245], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0056, 0.2946, 3.3482, 0.0245])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.256
Iter 2/2000 - Loss: 2.052
Iter 3/2000 - Loss: 1.263
Iter 4/2000 - Loss: 1.379
Iter 5/2000 - Loss: 1.650
Iter 6/2000 - Loss: 1.533
Iter 7/2000 - Loss: 1.299
Iter 8/2000 - Loss: 1.195
Iter 9/2000 - Loss: 1.246
Iter 10/2000 - Loss: 1.326
Iter 11/2000 - Loss: 1.323
Iter 12/2000 - Loss: 1.232
Iter 13/2000 - Loss: 1.117
Iter 14/2000 - Loss: 1.032
Iter 15/2000 - Loss: 0.986
Iter 16/2000 - Loss: 0.948
Iter 17/2000 - Loss: 0.882
Iter 18/2000 - Loss: 0.770
Iter 19/2000 - Loss: 0.620
Iter 20/2000 - Loss: 0.448
Iter 1981/2000 - Loss: -8.221
Iter 1982/2000 - Loss: -8.221
Iter 1983/2000 - Loss: -8.221
Iter 1984/2000 - Loss: -8.221
Iter 1985/2000 - Loss: -8.221
Iter 1986/2000 - Loss: -8.221
Iter 1987/2000 - Loss: -8.221
Iter 1988/2000 - Loss: -8.221
Iter 1989/2000 - Loss: -8.221
Iter 1990/2000 - Loss: -8.221
Iter 1991/2000 - Loss: -8.221
Iter 1992/2000 - Loss: -8.221
Iter 1993/2000 - Loss: -8.221
Iter 1994/2000 - Loss: -8.221
Iter 1995/2000 - Loss: -8.221
Iter 1996/2000 - Loss: -8.221
Iter 1997/2000 - Loss: -8.221
Iter 1998/2000 - Loss: -8.222
Iter 1999/2000 - Loss: -8.222
Iter 2000/2000 - Loss: -8.222
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[13.3402,  6.3101, 35.8398, 13.9642, 12.4206, 40.2053]],

        [[15.4259, 32.0105, 13.6532,  1.3138,  8.5744, 30.8013]],

        [[14.6426, 28.5870, 12.8386,  1.0301,  7.6313, 15.1444]],

        [[12.3432, 26.6160, 19.7543,  6.0725,  0.9051, 44.3375]]])
Signal Variance: tensor([ 0.0748,  3.7299, 14.6524,  0.6960])
Estimated target variance: tensor([0.0056, 0.2946, 3.3482, 0.0245])
N: 140
Signal to noise ratio: tensor([ 14.7909, 110.7357,  82.0342,  54.7189])
Bound on condition number: tensor([  30628.8106, 1716736.6046,  942145.7898,  419183.3351])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.004564853576833865, policy loss: 7.065010511718336
Experience 14, Iter 1, disc loss: 0.005181058021185637, policy loss: 7.18749409521277
Experience 14, Iter 2, disc loss: 0.004933693625858225, policy loss: 7.967093458744603
Experience 14, Iter 3, disc loss: 0.005511517904459438, policy loss: 7.270105709564493
Experience 14, Iter 4, disc loss: 0.00731799736531012, policy loss: 6.871587271193981
Experience 14, Iter 5, disc loss: 0.007188750262060932, policy loss: 7.812056914557018
Experience 14, Iter 6, disc loss: 0.0064729025132247995, policy loss: 7.465445380405203
Experience 14, Iter 7, disc loss: 0.009478146180240385, policy loss: 7.2425474497037765
Experience 14, Iter 8, disc loss: 0.022720050325800965, policy loss: 8.14771585591302
Experience 14, Iter 9, disc loss: 0.009942221783698774, policy loss: 8.039158685352763
Experience 14, Iter 10, disc loss: 0.018395175604812592, policy loss: 7.1021256607450916
Experience 14, Iter 11, disc loss: 0.009336311923715845, policy loss: 8.012866246358385
Experience 14, Iter 12, disc loss: 0.03372823187182967, policy loss: 7.7964953207539684
Experience 14, Iter 13, disc loss: 0.01142192872342436, policy loss: 8.726274445462147
Experience 14, Iter 14, disc loss: 0.010907630408578518, policy loss: 8.37505412504026
Experience 14, Iter 15, disc loss: 0.015085407433506007, policy loss: 10.036615427843921
Experience 14, Iter 16, disc loss: 0.012313433248863505, policy loss: 9.141420243671334
Experience 14, Iter 17, disc loss: 0.013274645685771844, policy loss: 9.419752592540029
Experience 14, Iter 18, disc loss: 0.028676637013159156, policy loss: 8.286616503116345
Experience 14, Iter 19, disc loss: 0.026496456614705642, policy loss: 8.169501406130795
Experience 14, Iter 20, disc loss: 0.1164470308675114, policy loss: 8.599585112045483
Experience 14, Iter 21, disc loss: 0.12531620069789962, policy loss: 8.82239472938846
Experience 14, Iter 22, disc loss: 0.1614360762450348, policy loss: 8.364170217768343
Experience 14, Iter 23, disc loss: 0.07590435869361646, policy loss: 10.02997900392044
Experience 14, Iter 24, disc loss: 0.1668877607374817, policy loss: 10.044104366126454
Experience 14, Iter 25, disc loss: 0.12822125179499258, policy loss: 8.583526115193656
Experience 14, Iter 26, disc loss: 0.1266156297811013, policy loss: 9.634123957007644
Experience 14, Iter 27, disc loss: 0.10833088930091136, policy loss: 7.861922654314009
Experience 14, Iter 28, disc loss: 0.09416594491360517, policy loss: 7.78875442494116
Experience 14, Iter 29, disc loss: 0.06859467613959096, policy loss: 8.225690481010185
Experience 14, Iter 30, disc loss: 0.06505242617352011, policy loss: 8.656774838742697
Experience 14, Iter 31, disc loss: 0.035823619427017346, policy loss: 9.026347280870926
Experience 14, Iter 32, disc loss: 0.03267251360759829, policy loss: 9.284915493298096
Experience 14, Iter 33, disc loss: 0.09526457643975463, policy loss: 9.999264246289975
Experience 14, Iter 34, disc loss: 0.07113537303019496, policy loss: 8.476461318031406
Experience 14, Iter 35, disc loss: 0.03995054725072141, policy loss: 10.015692245375442
Experience 14, Iter 36, disc loss: 0.03652037112163931, policy loss: 10.419614726688469
Experience 14, Iter 37, disc loss: 0.04568367993750291, policy loss: 10.783008707399706
Experience 14, Iter 38, disc loss: 0.03441983217562219, policy loss: 11.796869671561833
Experience 14, Iter 39, disc loss: 0.07069791669464132, policy loss: 11.687487120177222
Experience 14, Iter 40, disc loss: 0.030899463788270264, policy loss: 11.377633976164551
Experience 14, Iter 41, disc loss: 0.02712074486402926, policy loss: 11.916881328120521
Experience 14, Iter 42, disc loss: 0.01967396089280983, policy loss: 11.987770108458673
Experience 14, Iter 43, disc loss: 0.015003577202708928, policy loss: 11.88325068009749
Experience 14, Iter 44, disc loss: 0.010953047560957696, policy loss: 13.006917109988057
Experience 14, Iter 45, disc loss: 0.007848176765141452, policy loss: 13.450272373023388
Experience 14, Iter 46, disc loss: 0.005620573336537816, policy loss: 13.19911379747445
Experience 14, Iter 47, disc loss: 0.004055594629919967, policy loss: 12.859177955687516
Experience 14, Iter 48, disc loss: 0.0029728233938609017, policy loss: 13.029507440447336
Experience 14, Iter 49, disc loss: 0.0022344151891675154, policy loss: 12.270403298702927
Experience 14, Iter 50, disc loss: 0.001708610670572539, policy loss: 12.722056557721258
Experience 14, Iter 51, disc loss: 0.0014760638993646812, policy loss: 11.2190890292695
Experience 14, Iter 52, disc loss: 0.001244087981747063, policy loss: 11.4448681796866
Experience 14, Iter 53, disc loss: 0.0037543086076693545, policy loss: 10.217391215416773
Experience 14, Iter 54, disc loss: 0.02518281475852411, policy loss: 9.341127926854448
Experience 14, Iter 55, disc loss: 0.03408356707890189, policy loss: 10.453590606836734
Experience 14, Iter 56, disc loss: 0.013604343338069575, policy loss: 12.757716500607888
Experience 14, Iter 57, disc loss: 0.036485903126175126, policy loss: 11.473205409593326
Experience 14, Iter 58, disc loss: 0.020932395557116394, policy loss: 12.39958850577423
Experience 14, Iter 59, disc loss: 0.04028287673571578, policy loss: 12.121680826572325
Experience 14, Iter 60, disc loss: 0.0854198986483776, policy loss: 11.172715207194381
Experience 14, Iter 61, disc loss: 0.003616598268140235, policy loss: 11.037526001975564
Experience 14, Iter 62, disc loss: 0.013028694782921742, policy loss: 11.258141477043232
Experience 14, Iter 63, disc loss: 0.005826081611723266, policy loss: 11.903501607785145
Experience 14, Iter 64, disc loss: 0.005891553911944034, policy loss: 11.74480152315148
Experience 14, Iter 65, disc loss: 0.00909321039762916, policy loss: 10.727869101159044
Experience 14, Iter 66, disc loss: 0.012370548501716403, policy loss: 12.299732833526265
Experience 14, Iter 67, disc loss: 0.010673636709150922, policy loss: 11.085715463293651
Experience 14, Iter 68, disc loss: 0.011114243122899388, policy loss: 11.625836355068454
Experience 14, Iter 69, disc loss: 0.011316432113358098, policy loss: 11.465051372826434
Experience 14, Iter 70, disc loss: 0.01092521700640053, policy loss: 11.064472818810318
Experience 14, Iter 71, disc loss: 0.009835246083775916, policy loss: 10.777058472719048
Experience 14, Iter 72, disc loss: 0.008570146086245586, policy loss: 10.96696446602046
Experience 14, Iter 73, disc loss: 0.007241137673311336, policy loss: 10.447407971597986
Experience 14, Iter 74, disc loss: 0.006078140754302732, policy loss: 10.25647701018466
Experience 14, Iter 75, disc loss: 0.004928568617840961, policy loss: 10.300201621406066
Experience 14, Iter 76, disc loss: 0.004259732696587619, policy loss: 9.905078064230707
Experience 14, Iter 77, disc loss: 0.0038463075982511654, policy loss: 9.298785794522782
Experience 14, Iter 78, disc loss: 0.0030633249685921753, policy loss: 9.417471726155664
Experience 14, Iter 79, disc loss: 0.0028411565791233483, policy loss: 9.255580486639534
Experience 14, Iter 80, disc loss: 0.002813109614558126, policy loss: 9.3087263911939
Experience 14, Iter 81, disc loss: 0.0028031285997402802, policy loss: 8.934622019951238
Experience 14, Iter 82, disc loss: 0.0024855923996775988, policy loss: 8.797843352771912
Experience 14, Iter 83, disc loss: 0.003021492728098961, policy loss: 8.394736840455334
Experience 14, Iter 84, disc loss: 0.005631974971832791, policy loss: 8.094074925588503
Experience 14, Iter 85, disc loss: 0.004423769410734036, policy loss: 7.388444934181287
Experience 14, Iter 86, disc loss: 0.004419261154411323, policy loss: 7.868031750301496
Experience 14, Iter 87, disc loss: 0.006068007202849451, policy loss: 7.775041768337895
Experience 14, Iter 88, disc loss: 0.0030679471379334213, policy loss: 7.887182487366787
Experience 14, Iter 89, disc loss: 0.004622011849954397, policy loss: 7.327091481875269
Experience 14, Iter 90, disc loss: 0.00834011424791287, policy loss: 7.049111011721402
Experience 14, Iter 91, disc loss: 0.019810256484351384, policy loss: 6.523249756501723
Experience 14, Iter 92, disc loss: 0.007493374430271063, policy loss: 7.822791943924753
Experience 14, Iter 93, disc loss: 0.012150535780270231, policy loss: 8.19360022363057
Experience 14, Iter 94, disc loss: 0.012013105858466915, policy loss: 8.079595661686914
Experience 14, Iter 95, disc loss: 0.005610432137042277, policy loss: 8.415709712093292
Experience 14, Iter 96, disc loss: 0.005656690477355726, policy loss: 7.7364525104591895
Experience 14, Iter 97, disc loss: 0.005805638588241919, policy loss: 8.113085150371866
Experience 14, Iter 98, disc loss: 0.0061880261317545145, policy loss: 8.216232198540782
Experience 14, Iter 99, disc loss: 0.005697573226716184, policy loss: 8.439557310977374
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0883],
        [1.0115],
        [0.0082]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.2306e-03, 6.1221e-02, 3.9679e-01, 7.4790e-03, 1.2795e-03,
          1.6065e+00]],

        [[5.2306e-03, 6.1221e-02, 3.9679e-01, 7.4790e-03, 1.2795e-03,
          1.6065e+00]],

        [[5.2306e-03, 6.1221e-02, 3.9679e-01, 7.4790e-03, 1.2795e-03,
          1.6065e+00]],

        [[5.2306e-03, 6.1221e-02, 3.9679e-01, 7.4790e-03, 1.2795e-03,
          1.6065e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0059, 0.3533, 4.0461, 0.0328], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0059, 0.3533, 4.0461, 0.0328])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.613
Iter 2/2000 - Loss: 2.322
Iter 3/2000 - Loss: 1.612
Iter 4/2000 - Loss: 1.715
Iter 5/2000 - Loss: 1.955
Iter 6/2000 - Loss: 1.846
Iter 7/2000 - Loss: 1.637
Iter 8/2000 - Loss: 1.545
Iter 9/2000 - Loss: 1.582
Iter 10/2000 - Loss: 1.638
Iter 11/2000 - Loss: 1.628
Iter 12/2000 - Loss: 1.548
Iter 13/2000 - Loss: 1.441
Iter 14/2000 - Loss: 1.350
Iter 15/2000 - Loss: 1.283
Iter 16/2000 - Loss: 1.223
Iter 17/2000 - Loss: 1.144
Iter 18/2000 - Loss: 1.025
Iter 19/2000 - Loss: 0.867
Iter 20/2000 - Loss: 0.682
Iter 1981/2000 - Loss: -8.157
Iter 1982/2000 - Loss: -8.157
Iter 1983/2000 - Loss: -8.157
Iter 1984/2000 - Loss: -8.157
Iter 1985/2000 - Loss: -8.157
Iter 1986/2000 - Loss: -8.157
Iter 1987/2000 - Loss: -8.157
Iter 1988/2000 - Loss: -8.157
Iter 1989/2000 - Loss: -8.157
Iter 1990/2000 - Loss: -8.157
Iter 1991/2000 - Loss: -8.157
Iter 1992/2000 - Loss: -8.157
Iter 1993/2000 - Loss: -8.157
Iter 1994/2000 - Loss: -8.157
Iter 1995/2000 - Loss: -8.157
Iter 1996/2000 - Loss: -8.157
Iter 1997/2000 - Loss: -8.157
Iter 1998/2000 - Loss: -8.157
Iter 1999/2000 - Loss: -8.157
Iter 2000/2000 - Loss: -8.157
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[12.9981,  6.4019, 42.4458, 14.5421, 12.3007, 38.6471]],

        [[16.0250, 32.8117, 13.1803,  1.3344,  3.7862, 32.0659]],

        [[15.0345, 31.1531, 11.6800,  1.0687,  2.1282, 16.7954]],

        [[ 8.6770, 10.5963, 30.0233,  7.7738,  1.4030, 48.2267]]])
Signal Variance: tensor([ 0.0689,  3.5584, 15.6061,  1.3752])
Estimated target variance: tensor([0.0059, 0.3533, 4.0461, 0.0328])
N: 150
Signal to noise ratio: tensor([ 14.1054, 109.5205,  85.0997,  77.2977])
Bound on condition number: tensor([  29845.5328, 1799210.4099, 1086294.3969,  896240.3319])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.015739183890846043, policy loss: 6.6776247033470195
Experience 15, Iter 1, disc loss: 0.022603639297059195, policy loss: 7.1814999926494005
Experience 15, Iter 2, disc loss: 0.029246344723052257, policy loss: 7.091694725619773
Experience 15, Iter 3, disc loss: 0.013679143274671126, policy loss: 7.502587594071399
Experience 15, Iter 4, disc loss: 0.015422076266190379, policy loss: 7.9334594980854085
Experience 15, Iter 5, disc loss: 0.017119791149549486, policy loss: 8.284414087346734
Experience 15, Iter 6, disc loss: 0.017671889725620473, policy loss: 7.755195922795437
Experience 15, Iter 7, disc loss: 0.017056809690040933, policy loss: 8.221711095574248
Experience 15, Iter 8, disc loss: 0.020396359837058283, policy loss: 7.7219334374138295
Experience 15, Iter 9, disc loss: 0.017998207725369628, policy loss: 7.652564091613387
Experience 15, Iter 10, disc loss: 0.019724827590286237, policy loss: 7.124693165423193
Experience 15, Iter 11, disc loss: 0.0182764932995648, policy loss: 7.7272085644357436
Experience 15, Iter 12, disc loss: 0.018701281739603823, policy loss: 7.003338057797565
Experience 15, Iter 13, disc loss: 0.020725821613648582, policy loss: 7.215041367447265
Experience 15, Iter 14, disc loss: 0.017887063550860845, policy loss: 7.872171141417844
Experience 15, Iter 15, disc loss: 0.036597911293765745, policy loss: 6.591506302078111
Experience 15, Iter 16, disc loss: 0.026069651113385307, policy loss: 7.835814748838631
Experience 15, Iter 17, disc loss: 0.029562749605633563, policy loss: 7.882417861575517
Experience 15, Iter 18, disc loss: 0.03669868437404089, policy loss: 8.462115340371554
Experience 15, Iter 19, disc loss: 0.026038828711506395, policy loss: 9.175571633714343
Experience 15, Iter 20, disc loss: 0.015510110973884557, policy loss: 9.028375227198556
Experience 15, Iter 21, disc loss: 0.02797744942774284, policy loss: 9.614477848554989
Experience 15, Iter 22, disc loss: 0.02272286188914869, policy loss: 10.323888802173048
Experience 15, Iter 23, disc loss: 0.02746787070709452, policy loss: 9.081607151750914
Experience 15, Iter 24, disc loss: 0.02223697039264788, policy loss: 8.496333715931769
Experience 15, Iter 25, disc loss: 0.028706792105680848, policy loss: 8.301796971560165
Experience 15, Iter 26, disc loss: 0.019172794589879135, policy loss: 8.480082036666838
Experience 15, Iter 27, disc loss: 0.016021367984123285, policy loss: 8.684597117550869
Experience 15, Iter 28, disc loss: 0.014008481776410639, policy loss: 8.540052548059728
Experience 15, Iter 29, disc loss: 0.022101657054336026, policy loss: 7.8321574265880685
Experience 15, Iter 30, disc loss: 0.03679281076067003, policy loss: 6.71172347766535
Experience 15, Iter 31, disc loss: 0.022193010532947078, policy loss: 7.435834200595387
Experience 15, Iter 32, disc loss: 0.11191392862128918, policy loss: 7.444280503649557
Experience 15, Iter 33, disc loss: 0.027225695344336165, policy loss: 7.044883059048524
Experience 15, Iter 34, disc loss: 0.044887130499716606, policy loss: 8.111095180009455
Experience 15, Iter 35, disc loss: 0.030693892664939916, policy loss: 9.977776422292989
Experience 15, Iter 36, disc loss: 0.03704158255380764, policy loss: 9.25736864591562
Experience 15, Iter 37, disc loss: 0.038634599117193774, policy loss: 9.062216092486237
Experience 15, Iter 38, disc loss: 0.039725691918471154, policy loss: 8.153304981212019
Experience 15, Iter 39, disc loss: 0.029215111157308765, policy loss: 8.824182612964803
Experience 15, Iter 40, disc loss: 0.020972314512813228, policy loss: 8.41506822331134
Experience 15, Iter 41, disc loss: 0.014292916965727884, policy loss: 8.542427276368077
Experience 15, Iter 42, disc loss: 0.010048896429257362, policy loss: 8.812428017218824
Experience 15, Iter 43, disc loss: 0.020139714127102457, policy loss: 7.058291867085815
Experience 15, Iter 44, disc loss: 0.026836172503301375, policy loss: 6.6670950694730715
Experience 15, Iter 45, disc loss: 0.06467867657652879, policy loss: 6.382021445743541
Experience 15, Iter 46, disc loss: 0.06289995361400709, policy loss: 7.248020897118019
Experience 15, Iter 47, disc loss: 0.025615095369245347, policy loss: 7.4160507594032365
Experience 15, Iter 48, disc loss: 0.04658644842166245, policy loss: 7.585462162144518
Experience 15, Iter 49, disc loss: 0.06564115441118491, policy loss: 8.86369085675875
Experience 15, Iter 50, disc loss: 0.04675213605106031, policy loss: 9.305095147363751
Experience 15, Iter 51, disc loss: 0.03924084565033457, policy loss: 7.375075997360472
Experience 15, Iter 52, disc loss: 0.04804431289668594, policy loss: 9.040228773122514
Experience 15, Iter 53, disc loss: 0.05415603606815772, policy loss: 9.348876619125878
Experience 15, Iter 54, disc loss: 0.044643713405867806, policy loss: 8.811327956300637
Experience 15, Iter 55, disc loss: 0.054336123056708804, policy loss: 8.099990087475677
Experience 15, Iter 56, disc loss: 0.02669583607334385, policy loss: 9.425321525653057
Experience 15, Iter 57, disc loss: 0.019846493631651294, policy loss: 8.646751829685535
Experience 15, Iter 58, disc loss: 0.022482196375412936, policy loss: 8.109264608216508
Experience 15, Iter 59, disc loss: 0.08759169185113978, policy loss: 9.114602408546212
Experience 15, Iter 60, disc loss: 0.09993654568940546, policy loss: 8.242716736156337
Experience 15, Iter 61, disc loss: 0.10056448854213505, policy loss: 8.457826486642649
Experience 15, Iter 62, disc loss: 0.07105554222424715, policy loss: 8.714190069707803
Experience 15, Iter 63, disc loss: 0.03565215573183992, policy loss: 9.82773323024279
Experience 15, Iter 64, disc loss: 0.02844474163498343, policy loss: 9.780026398382118
Experience 15, Iter 65, disc loss: 0.06606907369384686, policy loss: 8.64688806653055
Experience 15, Iter 66, disc loss: 0.053117650658361854, policy loss: 10.044408698189072
Experience 15, Iter 67, disc loss: 0.0636714517765528, policy loss: 10.058066669164884
Experience 15, Iter 68, disc loss: 0.054047411960548325, policy loss: 9.184332426667773
Experience 15, Iter 69, disc loss: 0.05993609476155707, policy loss: 9.154897125403886
Experience 15, Iter 70, disc loss: 0.04086099984086317, policy loss: 8.681534982064377
Experience 15, Iter 71, disc loss: 0.03570976645454002, policy loss: 9.559946463139973
Experience 15, Iter 72, disc loss: 0.03616629524450852, policy loss: 9.185468659209128
Experience 15, Iter 73, disc loss: 0.04108621167103274, policy loss: 7.72434609989339
Experience 15, Iter 74, disc loss: 0.0915954694531602, policy loss: 8.384201113501511
Experience 15, Iter 75, disc loss: 0.08358719331686054, policy loss: 7.515086532885325
Experience 15, Iter 76, disc loss: 0.027030212399968702, policy loss: 8.460405605039371
Experience 15, Iter 77, disc loss: 0.07853010036250227, policy loss: 6.9312402048501385
Experience 15, Iter 78, disc loss: 0.05237456575585127, policy loss: 8.733027573141094
Experience 15, Iter 79, disc loss: 0.04141586124540267, policy loss: 9.008336815373813
Experience 15, Iter 80, disc loss: 0.0506064036342823, policy loss: 9.006667274597456
Experience 15, Iter 81, disc loss: 0.04291449225030696, policy loss: 9.022939290652136
Experience 15, Iter 82, disc loss: 0.047670501440224805, policy loss: 8.829393468494148
Experience 15, Iter 83, disc loss: 0.03504634203236617, policy loss: 9.023452832405532
Experience 15, Iter 84, disc loss: 0.02950722293415904, policy loss: 8.175947382508344
Experience 15, Iter 85, disc loss: 0.01670487375339942, policy loss: 9.033508356173225
Experience 15, Iter 86, disc loss: 0.03444579592524499, policy loss: 8.542293637620837
Experience 15, Iter 87, disc loss: 0.016724874671097373, policy loss: 9.04728239131873
Experience 15, Iter 88, disc loss: 0.0621091244207833, policy loss: 8.100490618447196
Experience 15, Iter 89, disc loss: 0.039734520025031805, policy loss: 9.292713423827458
Experience 15, Iter 90, disc loss: 0.02657961739290214, policy loss: 9.303558533220038
Experience 15, Iter 91, disc loss: 0.04296120810106363, policy loss: 8.860268043399824
Experience 15, Iter 92, disc loss: 0.02749160415645042, policy loss: 9.127795170434037
Experience 15, Iter 93, disc loss: 0.037583529967239314, policy loss: 9.547054478279893
Experience 15, Iter 94, disc loss: 0.022688095541862994, policy loss: 9.152107542106368
Experience 15, Iter 95, disc loss: 0.04047869505096181, policy loss: 9.292669095361388
Experience 15, Iter 96, disc loss: 0.028522427328740954, policy loss: 9.716104239370782
Experience 15, Iter 97, disc loss: 0.050533333732197344, policy loss: 9.584514688667301
Experience 15, Iter 98, disc loss: 0.048429631442988674, policy loss: 8.93245206405907
Experience 15, Iter 99, disc loss: 0.05148417934042765, policy loss: 9.267677570918856
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0017],
        [0.1100],
        [1.2037],
        [0.0120]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0053, 0.0734, 0.5848, 0.0095, 0.0037, 2.0284]],

        [[0.0053, 0.0734, 0.5848, 0.0095, 0.0037, 2.0284]],

        [[0.0053, 0.0734, 0.5848, 0.0095, 0.0037, 2.0284]],

        [[0.0053, 0.0734, 0.5848, 0.0095, 0.0037, 2.0284]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0068, 0.4401, 4.8149, 0.0479], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0068, 0.4401, 4.8149, 0.0479])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.066
Iter 2/2000 - Loss: 2.603
Iter 3/2000 - Loss: 2.048
Iter 4/2000 - Loss: 2.123
Iter 5/2000 - Loss: 2.304
Iter 6/2000 - Loss: 2.209
Iter 7/2000 - Loss: 2.042
Iter 8/2000 - Loss: 1.970
Iter 9/2000 - Loss: 1.989
Iter 10/2000 - Loss: 2.015
Iter 11/2000 - Loss: 1.986
Iter 12/2000 - Loss: 1.902
Iter 13/2000 - Loss: 1.797
Iter 14/2000 - Loss: 1.700
Iter 15/2000 - Loss: 1.618
Iter 16/2000 - Loss: 1.535
Iter 17/2000 - Loss: 1.428
Iter 18/2000 - Loss: 1.282
Iter 19/2000 - Loss: 1.098
Iter 20/2000 - Loss: 0.886
Iter 1981/2000 - Loss: -7.922
Iter 1982/2000 - Loss: -7.922
Iter 1983/2000 - Loss: -7.922
Iter 1984/2000 - Loss: -7.922
Iter 1985/2000 - Loss: -7.922
Iter 1986/2000 - Loss: -7.922
Iter 1987/2000 - Loss: -7.922
Iter 1988/2000 - Loss: -7.922
Iter 1989/2000 - Loss: -7.922
Iter 1990/2000 - Loss: -7.922
Iter 1991/2000 - Loss: -7.922
Iter 1992/2000 - Loss: -7.922
Iter 1993/2000 - Loss: -7.922
Iter 1994/2000 - Loss: -7.922
Iter 1995/2000 - Loss: -7.922
Iter 1996/2000 - Loss: -7.922
Iter 1997/2000 - Loss: -7.922
Iter 1998/2000 - Loss: -7.922
Iter 1999/2000 - Loss: -7.922
Iter 2000/2000 - Loss: -7.922
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0002]])
Lengthscale: tensor([[[12.3954,  6.2686, 43.4284, 13.6876, 12.8255, 41.0053]],

        [[14.5632, 28.5148,  9.4290,  1.4829,  2.2355, 35.7201]],

        [[15.0234, 29.0119,  9.3861,  1.1839,  1.2047, 16.9593]],

        [[ 5.5201, 11.2265, 27.0593,  6.9347,  1.3667, 47.5986]]])
Signal Variance: tensor([ 0.0633,  3.7119, 16.5262,  1.0171])
Estimated target variance: tensor([0.0068, 0.4401, 4.8149, 0.0479])
N: 160
Signal to noise ratio: tensor([ 13.7997, 109.1048,  87.0208,  66.6025])
Bound on condition number: tensor([  30470.0940, 1904619.8744, 1211619.2924,  709744.1819])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.08051474446054381, policy loss: 6.183742993568045
Experience 16, Iter 1, disc loss: 0.051148321225753704, policy loss: 6.430898016884242
Experience 16, Iter 2, disc loss: 0.09520540592464964, policy loss: 6.105072848938847
Experience 16, Iter 3, disc loss: 0.10007308797327524, policy loss: 6.257629425037319
Experience 16, Iter 4, disc loss: 0.11835636616057761, policy loss: 6.6967528562671275
Experience 16, Iter 5, disc loss: 0.10303223446199439, policy loss: 6.0914937370553215
Experience 16, Iter 6, disc loss: 0.1482253150421621, policy loss: 6.322797242890182
Experience 16, Iter 7, disc loss: 0.13433663665995033, policy loss: 6.5176046623162165
Experience 16, Iter 8, disc loss: 0.10862162912228261, policy loss: 6.37454879362876
Experience 16, Iter 9, disc loss: 0.09749606701221675, policy loss: 5.616961778756068
Experience 16, Iter 10, disc loss: 0.0905871455793617, policy loss: 6.387698600513477
Experience 16, Iter 11, disc loss: 0.1045376170571697, policy loss: 5.767059728755762
Experience 16, Iter 12, disc loss: 0.13984747641248496, policy loss: 6.6099753788428215
Experience 16, Iter 13, disc loss: 0.12204463839873042, policy loss: 5.969892517369671
Experience 16, Iter 14, disc loss: 0.1227342698865796, policy loss: 6.846485949905153
Experience 16, Iter 15, disc loss: 0.12385782230894989, policy loss: 6.863738539492308
Experience 16, Iter 16, disc loss: 0.12239186620089075, policy loss: 6.967140318599894
Experience 16, Iter 17, disc loss: 0.12174706262611049, policy loss: 6.181692824052128
Experience 16, Iter 18, disc loss: 0.10329730998415798, policy loss: 6.572115332125347
Experience 16, Iter 19, disc loss: 0.0748348186620359, policy loss: 7.325466270160355
Experience 16, Iter 20, disc loss: 0.07794145601690114, policy loss: 7.020740369535302
Experience 16, Iter 21, disc loss: 0.08467812638590419, policy loss: 7.972112403034978
Experience 16, Iter 22, disc loss: 0.05363193247710411, policy loss: 5.814616663608335
Experience 16, Iter 23, disc loss: 0.10144439149046162, policy loss: 6.331106384467745
Experience 16, Iter 24, disc loss: 0.1049805593977161, policy loss: 6.153182324581454
Experience 16, Iter 25, disc loss: 0.11918469168563268, policy loss: 6.946230611351194
Experience 16, Iter 26, disc loss: 0.11963231420255759, policy loss: 7.490328322330944
Experience 16, Iter 27, disc loss: 0.1203350487699782, policy loss: 8.025342485716587
Experience 16, Iter 28, disc loss: 0.12147812293627858, policy loss: 7.147062241445698
Experience 16, Iter 29, disc loss: 0.15516022200292037, policy loss: 7.045465973514304
Experience 16, Iter 30, disc loss: 0.1566632228974868, policy loss: 5.655911222013152
Experience 16, Iter 31, disc loss: 0.12078362683702941, policy loss: 6.424486857873727
Experience 16, Iter 32, disc loss: 0.13538753799018044, policy loss: 6.456087944831831
Experience 16, Iter 33, disc loss: 0.1178998842326762, policy loss: 8.12659410034946
Experience 16, Iter 34, disc loss: 0.12004085073300058, policy loss: 7.276034110922403
Experience 16, Iter 35, disc loss: 0.1375624047743983, policy loss: 7.683391257545148
Experience 16, Iter 36, disc loss: 0.12457577329072034, policy loss: 6.641168110225365
Experience 16, Iter 37, disc loss: 0.12332128339642445, policy loss: 7.81549414815318
Experience 16, Iter 38, disc loss: 0.1424201258630607, policy loss: 6.4438611117665845
Experience 16, Iter 39, disc loss: 0.12184595783561365, policy loss: 7.460970688855758
Experience 16, Iter 40, disc loss: 0.1558342249591136, policy loss: 7.8746736921012666
Experience 16, Iter 41, disc loss: 0.14583021921562112, policy loss: 7.542324255380382
Experience 16, Iter 42, disc loss: 0.16939767088607222, policy loss: 8.25943507177435
Experience 16, Iter 43, disc loss: 0.18794930830898604, policy loss: 7.182096552113393
Experience 16, Iter 44, disc loss: 0.15013028484518098, policy loss: 6.021951896485713
Experience 16, Iter 45, disc loss: 0.13339194788597114, policy loss: 6.474060638432294
Experience 16, Iter 46, disc loss: 0.18361907012529616, policy loss: 6.129197062864026
Experience 16, Iter 47, disc loss: 0.11420897957065777, policy loss: 7.089034185499071
Experience 16, Iter 48, disc loss: 0.12086122341194844, policy loss: 7.675220802465702
Experience 16, Iter 49, disc loss: 0.14662460247015768, policy loss: 7.5903869068798855
Experience 16, Iter 50, disc loss: 0.1307733525075279, policy loss: 8.491710472530857
Experience 16, Iter 51, disc loss: 0.1385600565872006, policy loss: 7.7558275479114025
Experience 16, Iter 52, disc loss: 0.11615627660423983, policy loss: 9.807333131392133
Experience 16, Iter 53, disc loss: 0.23731606117189624, policy loss: 7.9611543559950455
Experience 16, Iter 54, disc loss: 0.1696076359229139, policy loss: 9.219837158446119
Experience 16, Iter 55, disc loss: 0.21361362268641942, policy loss: 7.390481463800973
Experience 16, Iter 56, disc loss: 0.1420141594781924, policy loss: 8.753722070861835
Experience 16, Iter 57, disc loss: 0.18511878268729837, policy loss: 8.033504441281975
Experience 16, Iter 58, disc loss: 0.15051633602548853, policy loss: 6.723813278103522
Experience 16, Iter 59, disc loss: 0.1400507699392379, policy loss: 7.335847321885675
Experience 16, Iter 60, disc loss: 0.1586664648914901, policy loss: 9.389549055810729
Experience 16, Iter 61, disc loss: 0.09843927542141916, policy loss: 7.484221064243579
Experience 16, Iter 62, disc loss: 0.09745979984420404, policy loss: 9.039860311182059
Experience 16, Iter 63, disc loss: 0.10026484374320027, policy loss: 8.648726854971457
Experience 16, Iter 64, disc loss: 0.15219624241845792, policy loss: 7.305966787586734
Experience 16, Iter 65, disc loss: 0.16893031665989652, policy loss: 9.397947164315344
Experience 16, Iter 66, disc loss: 0.18796769439253525, policy loss: 9.331565730528919
Experience 16, Iter 67, disc loss: 0.21776788235904773, policy loss: 7.704179568314337
Experience 16, Iter 68, disc loss: 0.23639973331122197, policy loss: 8.249284502874579
Experience 16, Iter 69, disc loss: 0.18470890611288648, policy loss: 8.663456807387956
Experience 16, Iter 70, disc loss: 0.17846021599250228, policy loss: 7.369270503019339
Experience 16, Iter 71, disc loss: 0.2908773507136791, policy loss: 7.253626654839606
Experience 16, Iter 72, disc loss: 0.1776915025724048, policy loss: 7.085282348630925
Experience 16, Iter 73, disc loss: 0.20292601271981314, policy loss: 7.163565735321267
Experience 16, Iter 74, disc loss: 0.2072298919865836, policy loss: 6.8433857678064784
Experience 16, Iter 75, disc loss: 0.21048193204866752, policy loss: 6.181119994816156
Experience 16, Iter 76, disc loss: 0.23141334030131916, policy loss: 6.809384345296865
Experience 16, Iter 77, disc loss: 0.1994592119136624, policy loss: 6.689442314027597
Experience 16, Iter 78, disc loss: 0.22707624749045663, policy loss: 7.378086692839659
Experience 16, Iter 79, disc loss: 0.2598208448931448, policy loss: 6.355671759501236
Experience 16, Iter 80, disc loss: 0.22780198971788237, policy loss: 6.393762805329457
Experience 16, Iter 81, disc loss: 0.20840368335870071, policy loss: 6.567447277987593
Experience 16, Iter 82, disc loss: 0.19912455045882616, policy loss: 7.4836908969134734
Experience 16, Iter 83, disc loss: 0.2199325296590021, policy loss: 4.475261796216923
Experience 16, Iter 84, disc loss: 0.2823346217818897, policy loss: 5.834500776930771
Experience 16, Iter 85, disc loss: 0.2174253407445762, policy loss: 5.0312877237917
Experience 16, Iter 86, disc loss: 0.20548355610340752, policy loss: 5.108396448153339
Experience 16, Iter 87, disc loss: 0.15333764987402712, policy loss: 6.89038662683821
Experience 16, Iter 88, disc loss: 0.2417512509074267, policy loss: 5.200113300276369
Experience 16, Iter 89, disc loss: 0.20850857897878328, policy loss: 6.115108396790987
Experience 16, Iter 90, disc loss: 0.18373622724633265, policy loss: 5.358995890751487
Experience 16, Iter 91, disc loss: 0.19836660636228698, policy loss: 6.311669680161721
Experience 16, Iter 92, disc loss: 0.21930294041997292, policy loss: 6.386736143082347
Experience 16, Iter 93, disc loss: 0.2831407429412413, policy loss: 5.691924902647793
Experience 16, Iter 94, disc loss: 0.21753369091313612, policy loss: 5.585844908050903
Experience 16, Iter 95, disc loss: 0.2367968385274651, policy loss: 5.879617616373841
Experience 16, Iter 96, disc loss: 0.28496525778979775, policy loss: 5.23569121743776
Experience 16, Iter 97, disc loss: 0.2655888788576218, policy loss: 6.231686753956325
Experience 16, Iter 98, disc loss: 0.3729996000522613, policy loss: 5.2241703839845455
Experience 16, Iter 99, disc loss: 0.3256060057451834, policy loss: 6.093553484485471
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0019],
        [0.1302],
        [1.3500],
        [0.0156]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0055, 0.0842, 0.7441, 0.0111, 0.0066, 2.4489]],

        [[0.0055, 0.0842, 0.7441, 0.0111, 0.0066, 2.4489]],

        [[0.0055, 0.0842, 0.7441, 0.0111, 0.0066, 2.4489]],

        [[0.0055, 0.0842, 0.7441, 0.0111, 0.0066, 2.4489]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0076, 0.5207, 5.3999, 0.0624], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0076, 0.5207, 5.3999, 0.0624])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.402
Iter 2/2000 - Loss: 2.817
Iter 3/2000 - Loss: 2.373
Iter 4/2000 - Loss: 2.421
Iter 5/2000 - Loss: 2.564
Iter 6/2000 - Loss: 2.484
Iter 7/2000 - Loss: 2.339
Iter 8/2000 - Loss: 2.273
Iter 9/2000 - Loss: 2.280
Iter 10/2000 - Loss: 2.288
Iter 11/2000 - Loss: 2.243
Iter 12/2000 - Loss: 2.149
Iter 13/2000 - Loss: 2.038
Iter 14/2000 - Loss: 1.935
Iter 15/2000 - Loss: 1.842
Iter 16/2000 - Loss: 1.741
Iter 17/2000 - Loss: 1.609
Iter 18/2000 - Loss: 1.436
Iter 19/2000 - Loss: 1.228
Iter 20/2000 - Loss: 0.996
Iter 1981/2000 - Loss: -7.871
Iter 1982/2000 - Loss: -7.871
Iter 1983/2000 - Loss: -7.871
Iter 1984/2000 - Loss: -7.872
Iter 1985/2000 - Loss: -7.872
Iter 1986/2000 - Loss: -7.872
Iter 1987/2000 - Loss: -7.872
Iter 1988/2000 - Loss: -7.872
Iter 1989/2000 - Loss: -7.872
Iter 1990/2000 - Loss: -7.872
Iter 1991/2000 - Loss: -7.872
Iter 1992/2000 - Loss: -7.872
Iter 1993/2000 - Loss: -7.872
Iter 1994/2000 - Loss: -7.872
Iter 1995/2000 - Loss: -7.872
Iter 1996/2000 - Loss: -7.872
Iter 1997/2000 - Loss: -7.872
Iter 1998/2000 - Loss: -7.872
Iter 1999/2000 - Loss: -7.872
Iter 2000/2000 - Loss: -7.872
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.6002,  5.3637, 38.8092, 12.3549, 16.0508, 41.1725]],

        [[14.1616, 28.9203,  8.9041,  1.4636,  3.8638, 35.5161]],

        [[15.5246, 31.2385, 10.0970,  1.1700,  1.0297, 16.0927]],

        [[12.4282, 26.5579, 20.6931,  3.5206,  1.3346, 42.1822]]])
Signal Variance: tensor([ 0.0546,  4.1108, 17.1351,  0.7307])
Estimated target variance: tensor([0.0076, 0.5207, 5.3999, 0.0624])
N: 170
Signal to noise ratio: tensor([ 12.7711, 116.0851,  89.7271,  54.6295])
Bound on condition number: tensor([  27728.1272, 2290877.2459, 1368662.2671,  507345.3093])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.40465495769789694, policy loss: 3.5352700822710954
Experience 17, Iter 1, disc loss: 0.40933317282942655, policy loss: 4.052703331983901
Experience 17, Iter 2, disc loss: 0.3585327380030621, policy loss: 6.801682424166912
Experience 17, Iter 3, disc loss: 0.33985300303652766, policy loss: 5.651159023663807
Experience 17, Iter 4, disc loss: 0.4222539473414855, policy loss: 3.9183957544515438
Experience 17, Iter 5, disc loss: 0.3630113102779841, policy loss: 5.177141617944308
Experience 17, Iter 6, disc loss: 0.24743851838958564, policy loss: 4.922516315115331
Experience 17, Iter 7, disc loss: 0.26880905674069844, policy loss: 5.259255151576763
Experience 17, Iter 8, disc loss: 0.27539472379382435, policy loss: 5.119512668354011
Experience 17, Iter 9, disc loss: 0.23377349633477137, policy loss: 4.289567045748916
Experience 17, Iter 10, disc loss: 0.38736867911685285, policy loss: 3.719645751184338
Experience 17, Iter 11, disc loss: 0.31595719958997015, policy loss: 4.604592823506267
Experience 17, Iter 12, disc loss: 0.3083321206740993, policy loss: 5.562145825906933
Experience 17, Iter 13, disc loss: 0.3136024335000229, policy loss: 5.127302763553956
Experience 17, Iter 14, disc loss: 0.30401031860275585, policy loss: 4.324242493860186
Experience 17, Iter 15, disc loss: 0.28964520316774306, policy loss: 3.4588474641748936
Experience 17, Iter 16, disc loss: 0.14629235430132104, policy loss: 4.661634205316824
Experience 17, Iter 17, disc loss: 0.22444508650719086, policy loss: 4.649361669526302
Experience 17, Iter 18, disc loss: 0.1881055824140921, policy loss: 4.080328206985652
Experience 17, Iter 19, disc loss: 0.14602822704635582, policy loss: 5.1707279711351015
Experience 17, Iter 20, disc loss: 0.21445618983321496, policy loss: 4.031413903875755
Experience 17, Iter 21, disc loss: 0.1582359119811993, policy loss: 4.653803621501541
Experience 17, Iter 22, disc loss: 0.13922293772120997, policy loss: 4.910746639269796
Experience 17, Iter 23, disc loss: 0.18367447145405785, policy loss: 3.9263281135977186
Experience 17, Iter 24, disc loss: 0.19519092683905417, policy loss: 4.190862373301682
Experience 17, Iter 25, disc loss: 0.15975390280327711, policy loss: 4.836283553240465
Experience 17, Iter 26, disc loss: 0.23280404721075892, policy loss: 4.483453197024482
Experience 17, Iter 27, disc loss: 0.1739415350535487, policy loss: 5.310573270948495
Experience 17, Iter 28, disc loss: 0.26589263169156024, policy loss: 4.277934619036904
Experience 17, Iter 29, disc loss: 0.4281432344027782, policy loss: 4.4146121469146475
Experience 17, Iter 30, disc loss: 0.2699729647370857, policy loss: 5.389067132969859
Experience 17, Iter 31, disc loss: 0.25798471996361966, policy loss: 5.736195857935147
Experience 17, Iter 32, disc loss: 0.31284531231251084, policy loss: 5.216262399421906
Experience 17, Iter 33, disc loss: 0.36669890463340415, policy loss: 5.137984111250674
Experience 17, Iter 34, disc loss: 0.24381467454293596, policy loss: 4.226953036654398
Experience 17, Iter 35, disc loss: 0.1672385609583319, policy loss: 4.254188365846092
Experience 17, Iter 36, disc loss: 0.2805059613334861, policy loss: 3.5463077185156062
Experience 17, Iter 37, disc loss: 0.17281354482055625, policy loss: 4.105218842105912
Experience 17, Iter 38, disc loss: 0.1316914130383821, policy loss: 4.99783227573545
Experience 17, Iter 39, disc loss: 0.22858729788995064, policy loss: 4.066822612940109
Experience 17, Iter 40, disc loss: 0.19035168930297502, policy loss: 4.519231905142135
Experience 17, Iter 41, disc loss: 0.17962437410943197, policy loss: 4.232156062569766
Experience 17, Iter 42, disc loss: 0.3558115494735219, policy loss: 3.731516789499078
Experience 17, Iter 43, disc loss: 0.2459360389102329, policy loss: 5.19955552710959
Experience 17, Iter 44, disc loss: 0.28098057800220516, policy loss: 5.058561178479926
Experience 17, Iter 45, disc loss: 0.349495844447073, policy loss: 4.151626114120644
Experience 17, Iter 46, disc loss: 0.3524122460735947, policy loss: 5.5910549908916
Experience 17, Iter 47, disc loss: 0.3883699038820909, policy loss: 4.4325697166894065
Experience 17, Iter 48, disc loss: 0.3824729427947211, policy loss: 3.7749225548109866
Experience 17, Iter 49, disc loss: 0.3710681705904519, policy loss: 4.641774364673019
Experience 17, Iter 50, disc loss: 0.2880207197282606, policy loss: 4.838562171275041
Experience 17, Iter 51, disc loss: 0.23034193644142534, policy loss: 3.87243003406465
Experience 17, Iter 52, disc loss: 0.32008942708518295, policy loss: 4.671730671978874
Experience 17, Iter 53, disc loss: 0.2653864159952403, policy loss: 3.876155968192333
Experience 17, Iter 54, disc loss: 0.23725317485086447, policy loss: 3.321055918322258
Experience 17, Iter 55, disc loss: 0.2071411111330535, policy loss: 4.256787094709572
Experience 17, Iter 56, disc loss: 0.1750108615677773, policy loss: 4.970613713616784
Experience 17, Iter 57, disc loss: 0.19027817149355386, policy loss: 5.038955575817203
Experience 17, Iter 58, disc loss: 0.24679589668908897, policy loss: 4.2867827414473165
Experience 17, Iter 59, disc loss: 0.1861443836681016, policy loss: 4.232485612677992
Experience 17, Iter 60, disc loss: 0.1665300361846543, policy loss: 4.485354209007474
Experience 17, Iter 61, disc loss: 0.33025491270666735, policy loss: 3.5551920213212087
Experience 17, Iter 62, disc loss: 0.5312719584397747, policy loss: 4.363389604110265
Experience 17, Iter 63, disc loss: 0.4840620756767133, policy loss: 5.957341296144417
Experience 17, Iter 64, disc loss: 0.5296237053777871, policy loss: 4.720350030174553
Experience 17, Iter 65, disc loss: 0.45722390352716563, policy loss: 4.996025371182313
Experience 17, Iter 66, disc loss: 0.3255250884673887, policy loss: 3.9053955029091902
Experience 17, Iter 67, disc loss: 0.20206352475000972, policy loss: 4.11531606760842
Experience 17, Iter 68, disc loss: 0.1874020258640559, policy loss: 4.895867954556335
Experience 17, Iter 69, disc loss: 0.19902606980638404, policy loss: 4.277581261662684
Experience 17, Iter 70, disc loss: 0.15621440529017266, policy loss: 3.8743936250568574
Experience 17, Iter 71, disc loss: 0.2588210378478081, policy loss: 5.477743121710935
Experience 17, Iter 72, disc loss: 0.1713181988079417, policy loss: 5.0607711969404185
Experience 17, Iter 73, disc loss: 0.19661790600270213, policy loss: 4.308585904409992
Experience 17, Iter 74, disc loss: 0.253200183086921, policy loss: 5.17726443291652
Experience 17, Iter 75, disc loss: 0.38672018264900154, policy loss: 4.5491995568453
Experience 17, Iter 76, disc loss: 0.4789406992798708, policy loss: 4.351851423502369
Experience 17, Iter 77, disc loss: 0.4661686863923065, policy loss: 4.519636346465224
Experience 17, Iter 78, disc loss: 0.4711702245346079, policy loss: 5.824475525167744
Experience 17, Iter 79, disc loss: 0.41548609950061244, policy loss: 4.671172389116058
Experience 17, Iter 80, disc loss: 0.3021867692792466, policy loss: 5.211832944695456
Experience 17, Iter 81, disc loss: 0.29022560238399997, policy loss: 4.639761810118223
Experience 17, Iter 82, disc loss: 0.3358345642066952, policy loss: 3.651970772243142
Experience 17, Iter 83, disc loss: 0.42765066719275857, policy loss: 4.415700148441313
Experience 17, Iter 84, disc loss: 0.5846138123360285, policy loss: 3.9820850594894277
Experience 17, Iter 85, disc loss: 0.7920889561048811, policy loss: 4.988121606798114
Experience 17, Iter 86, disc loss: 0.6800749998768252, policy loss: 6.922506566179726
Experience 17, Iter 87, disc loss: 0.6974175052256069, policy loss: 5.579489476023282
Experience 17, Iter 88, disc loss: 0.5964806415903054, policy loss: 3.7101814543485823
Experience 17, Iter 89, disc loss: 0.3997772534966152, policy loss: 5.611558028332901
Experience 17, Iter 90, disc loss: 0.33647988736000756, policy loss: 5.15648506148442
Experience 17, Iter 91, disc loss: 0.22698362384696172, policy loss: 3.661173307683725
Experience 17, Iter 92, disc loss: 0.16979658789696028, policy loss: 4.401211826512518
Experience 17, Iter 93, disc loss: 0.08825276878229282, policy loss: 5.334259658696231
Experience 17, Iter 94, disc loss: 0.06793952384785451, policy loss: 6.592074390438965
Experience 17, Iter 95, disc loss: 0.05387736376727292, policy loss: 7.17424247645479
Experience 17, Iter 96, disc loss: 0.04145126274077567, policy loss: 9.2985711952781
Experience 17, Iter 97, disc loss: 0.036618905647993416, policy loss: 9.532940751297316
Experience 17, Iter 98, disc loss: 0.02535160578620599, policy loss: 10.47835183595163
Experience 17, Iter 99, disc loss: 0.02003646317545274, policy loss: 11.02044736586732
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0020],
        [0.1470],
        [1.5027],
        [0.0177]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0056, 0.0920, 0.8501, 0.0125, 0.0079, 2.8105]],

        [[0.0056, 0.0920, 0.8501, 0.0125, 0.0079, 2.8105]],

        [[0.0056, 0.0920, 0.8501, 0.0125, 0.0079, 2.8105]],

        [[0.0056, 0.0920, 0.8501, 0.0125, 0.0079, 2.8105]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.5879, 6.0109, 0.0708], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.5879, 6.0109, 0.0708])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.621
Iter 2/2000 - Loss: 2.965
Iter 3/2000 - Loss: 2.585
Iter 4/2000 - Loss: 2.618
Iter 5/2000 - Loss: 2.737
Iter 6/2000 - Loss: 2.665
Iter 7/2000 - Loss: 2.533
Iter 8/2000 - Loss: 2.468
Iter 9/2000 - Loss: 2.471
Iter 10/2000 - Loss: 2.466
Iter 11/2000 - Loss: 2.405
Iter 12/2000 - Loss: 2.298
Iter 13/2000 - Loss: 2.182
Iter 14/2000 - Loss: 2.076
Iter 15/2000 - Loss: 1.976
Iter 16/2000 - Loss: 1.858
Iter 17/2000 - Loss: 1.704
Iter 18/2000 - Loss: 1.509
Iter 19/2000 - Loss: 1.285
Iter 20/2000 - Loss: 1.043
Iter 1981/2000 - Loss: -7.859
Iter 1982/2000 - Loss: -7.859
Iter 1983/2000 - Loss: -7.859
Iter 1984/2000 - Loss: -7.859
Iter 1985/2000 - Loss: -7.859
Iter 1986/2000 - Loss: -7.859
Iter 1987/2000 - Loss: -7.859
Iter 1988/2000 - Loss: -7.859
Iter 1989/2000 - Loss: -7.859
Iter 1990/2000 - Loss: -7.859
Iter 1991/2000 - Loss: -7.859
Iter 1992/2000 - Loss: -7.859
Iter 1993/2000 - Loss: -7.859
Iter 1994/2000 - Loss: -7.859
Iter 1995/2000 - Loss: -7.859
Iter 1996/2000 - Loss: -7.859
Iter 1997/2000 - Loss: -7.859
Iter 1998/2000 - Loss: -7.859
Iter 1999/2000 - Loss: -7.859
Iter 2000/2000 - Loss: -7.859
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[10.9916,  5.8057, 40.6156, 10.8802, 14.2848, 42.9703]],

        [[14.9966, 27.6487,  9.0617,  1.4632,  2.9353, 31.2510]],

        [[14.5236, 30.2537,  9.3257,  1.1835,  1.0211, 17.1819]],

        [[12.6049, 28.8340, 11.0327,  1.5387,  1.7672, 42.4155]]])
Signal Variance: tensor([ 0.0625,  3.5673, 16.6965,  0.3922])
Estimated target variance: tensor([0.0082, 0.5879, 6.0109, 0.0708])
N: 180
Signal to noise ratio: tensor([ 13.8050, 107.5095,  88.5460,  40.7112])
Bound on condition number: tensor([  34305.0828, 2080491.7778, 1411273.1838,  298333.4946])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.014291956998916404, policy loss: 14.41646136649973
Experience 18, Iter 1, disc loss: 0.011963420777322549, policy loss: 14.601859492182996
Experience 18, Iter 2, disc loss: 0.009863561509994564, policy loss: 14.532887236542173
Experience 18, Iter 3, disc loss: 0.0072794955973304425, policy loss: 16.303179065187685
Experience 18, Iter 4, disc loss: 0.008904251956075807, policy loss: 14.946199358897513
Experience 18, Iter 5, disc loss: 0.0049690899736894055, policy loss: 17.2005691985904
Experience 18, Iter 6, disc loss: 0.0065424973068574106, policy loss: 14.728289165586332
Experience 18, Iter 7, disc loss: 0.0035702657561019775, policy loss: 17.117881897933675
Experience 18, Iter 8, disc loss: 0.0028712560929228734, policy loss: 16.431975823885487
Experience 18, Iter 9, disc loss: 0.002288645496561474, policy loss: 17.017322368429333
Experience 18, Iter 10, disc loss: 0.006727829592481101, policy loss: 15.320412997541352
Experience 18, Iter 11, disc loss: 0.0035427406867149496, policy loss: 16.287402042263956
Experience 18, Iter 12, disc loss: 0.005293907668276093, policy loss: 15.655043106887291
Experience 18, Iter 13, disc loss: 0.004868076655192736, policy loss: 15.541386755393562
Experience 18, Iter 14, disc loss: 0.0013208988181239803, policy loss: 18.14711987109486
Experience 18, Iter 15, disc loss: 0.007444451671403252, policy loss: 14.541748973144614
Experience 18, Iter 16, disc loss: 0.001645504366960451, policy loss: 17.02768073631826
Experience 18, Iter 17, disc loss: 0.002052819255334984, policy loss: 16.117084009938285
Experience 18, Iter 18, disc loss: 0.003711911643974636, policy loss: 14.811906568818964
Experience 18, Iter 19, disc loss: 0.005584977296468671, policy loss: 16.36833130434288
Experience 18, Iter 20, disc loss: 0.008354928394274184, policy loss: 15.108570105563182
Experience 18, Iter 21, disc loss: 0.013317413491955045, policy loss: 15.254570231467033
Experience 18, Iter 22, disc loss: 0.010058020913072941, policy loss: 15.188019382372783
Experience 18, Iter 23, disc loss: 0.0016791180280914656, policy loss: 15.589037228723384
Experience 18, Iter 24, disc loss: 0.004133655443447386, policy loss: 14.02218584447036
Experience 18, Iter 25, disc loss: 0.007716528688114846, policy loss: 13.159184068540242
Experience 18, Iter 26, disc loss: 0.02321853744505491, policy loss: 14.168412012834265
Experience 18, Iter 27, disc loss: 0.004288129670087577, policy loss: 14.427880626398485
Experience 18, Iter 28, disc loss: 0.008172411903786722, policy loss: 12.784798757064252
Experience 18, Iter 29, disc loss: 0.023152300819003196, policy loss: 13.083842098545771
Experience 18, Iter 30, disc loss: 0.018400602495287754, policy loss: 13.267114731747562
Experience 18, Iter 31, disc loss: 0.0149023164302107, policy loss: 11.136189261367091
Experience 18, Iter 32, disc loss: 0.015599819965367065, policy loss: 12.749833183228708
Experience 18, Iter 33, disc loss: 0.009101601844535706, policy loss: 11.931854075027044
Experience 18, Iter 34, disc loss: 0.010076096628047496, policy loss: 11.13643511242919
Experience 18, Iter 35, disc loss: 0.01735374684947201, policy loss: 10.653073775821372
Experience 18, Iter 36, disc loss: 0.012721393678248337, policy loss: 12.306916068946235
Experience 18, Iter 37, disc loss: 0.017629341169727628, policy loss: 9.854507506391268
Experience 18, Iter 38, disc loss: 0.01989018694506295, policy loss: 11.129193820352898
Experience 18, Iter 39, disc loss: 0.018833948434183897, policy loss: 11.116511721084205
Experience 18, Iter 40, disc loss: 0.021160189304040986, policy loss: 8.52550336402919
Experience 18, Iter 41, disc loss: 0.03042437427869052, policy loss: 9.263865371343941
Experience 18, Iter 42, disc loss: 0.024913530249743764, policy loss: 8.863451267422608
Experience 18, Iter 43, disc loss: 0.03532387676644505, policy loss: 8.32428947952311
Experience 18, Iter 44, disc loss: 0.03670999850806328, policy loss: 8.315707495900284
Experience 18, Iter 45, disc loss: 0.032561469329984105, policy loss: 8.471600319545914
Experience 18, Iter 46, disc loss: 0.043749399893906776, policy loss: 7.929578074637126
Experience 18, Iter 47, disc loss: 0.05078837557338958, policy loss: 6.63053441459235
Experience 18, Iter 48, disc loss: 0.06170519422331591, policy loss: 6.893454536906653
Experience 18, Iter 49, disc loss: 0.069655245806486, policy loss: 6.951481132172273
Experience 18, Iter 50, disc loss: 0.09673251731770982, policy loss: 6.8301468731179575
Experience 18, Iter 51, disc loss: 0.09084481613326398, policy loss: 6.622473740519026
Experience 18, Iter 52, disc loss: 0.0777662052633726, policy loss: 7.232397659309184
Experience 18, Iter 53, disc loss: 0.09368109850173639, policy loss: 6.381989084052368
Experience 18, Iter 54, disc loss: 0.10448652321209942, policy loss: 6.753113527880016
Experience 18, Iter 55, disc loss: 0.09466066083292496, policy loss: 7.6037432988731375
Experience 18, Iter 56, disc loss: 0.11256241135527198, policy loss: 6.20974959518074
Experience 18, Iter 57, disc loss: 0.14596157592471434, policy loss: 7.581322383356835
Experience 18, Iter 58, disc loss: 0.14838966440079865, policy loss: 6.6368152208131885
Experience 18, Iter 59, disc loss: 0.1556181418844781, policy loss: 6.74545784414144
Experience 18, Iter 60, disc loss: 0.12095167984388135, policy loss: 6.7371701766121745
Experience 18, Iter 61, disc loss: 0.111023474084428, policy loss: 7.12787105084241
Experience 18, Iter 62, disc loss: 0.08818729439149567, policy loss: 8.066204702112875
Experience 18, Iter 63, disc loss: 0.10004959759226317, policy loss: 7.186012989826316
Experience 18, Iter 64, disc loss: 0.11058659725336695, policy loss: 8.519107248946517
Experience 18, Iter 65, disc loss: 0.11943189046136196, policy loss: 6.714295639480502
Experience 18, Iter 66, disc loss: 0.09878207303641014, policy loss: 5.957928608315748
Experience 18, Iter 67, disc loss: 0.12124394755329906, policy loss: 6.41022430029656
Experience 18, Iter 68, disc loss: 0.1313312841199352, policy loss: 5.265272325487272
Experience 18, Iter 69, disc loss: 0.08544708897045947, policy loss: 7.599654587032784
Experience 18, Iter 70, disc loss: 0.07410023896493102, policy loss: 7.059708786323979
Experience 18, Iter 71, disc loss: 0.07632602003494017, policy loss: 7.173207782965339
Experience 18, Iter 72, disc loss: 0.07815703767641934, policy loss: 7.153931205554839
Experience 18, Iter 73, disc loss: 0.07044593120477814, policy loss: 6.1123150034306875
Experience 18, Iter 74, disc loss: 0.05767492203398353, policy loss: 6.7000759925588955
Experience 18, Iter 75, disc loss: 0.07146619627906732, policy loss: 6.020940567140993
Experience 18, Iter 76, disc loss: 0.06168265077688707, policy loss: 5.785720572448827
Experience 18, Iter 77, disc loss: 0.06487086425751284, policy loss: 5.325568974031642
Experience 18, Iter 78, disc loss: 0.0626368035615173, policy loss: 5.728185208001058
Experience 18, Iter 79, disc loss: 0.05806400517749541, policy loss: 5.6348766688659255
Experience 18, Iter 80, disc loss: 0.06201410688198339, policy loss: 6.39681537436538
Experience 18, Iter 81, disc loss: 0.054606037767069425, policy loss: 6.34768943278903
Experience 18, Iter 82, disc loss: 0.054828063965394795, policy loss: 6.500311536352455
Experience 18, Iter 83, disc loss: 0.05267319913533758, policy loss: 7.804178522902937
Experience 18, Iter 84, disc loss: 0.05078803738209352, policy loss: 8.107848074412658
Experience 18, Iter 85, disc loss: 0.062392682276758385, policy loss: 6.501676174172035
Experience 18, Iter 86, disc loss: 0.05522019187523193, policy loss: 5.974440071483471
Experience 18, Iter 87, disc loss: 0.05535596742856692, policy loss: 6.676215972676003
Experience 18, Iter 88, disc loss: 0.06772261663024888, policy loss: 5.715987571863947
Experience 18, Iter 89, disc loss: 0.07793136544853628, policy loss: 5.854472035443276
Experience 18, Iter 90, disc loss: 0.07711201529207287, policy loss: 5.530996447052566
Experience 18, Iter 91, disc loss: 0.10421007681452102, policy loss: 5.750558076324111
Experience 18, Iter 92, disc loss: 0.08294328969361547, policy loss: 6.045594183462164
Experience 18, Iter 93, disc loss: 0.1092095277032098, policy loss: 8.02194610168963
Experience 18, Iter 94, disc loss: 0.09957619973619618, policy loss: 6.584198238866258
Experience 18, Iter 95, disc loss: 0.10566323545421268, policy loss: 7.415142966289034
Experience 18, Iter 96, disc loss: 0.12398290341223735, policy loss: 6.178433681317127
Experience 18, Iter 97, disc loss: 0.11239970314447602, policy loss: 6.727471640545051
Experience 18, Iter 98, disc loss: 0.09701147045059183, policy loss: 7.767876121175076
Experience 18, Iter 99, disc loss: 0.13750564350252203, policy loss: 5.845797463261746
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.1584],
        [1.5928],
        [0.0195]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0056, 0.0976, 0.9398, 0.0138, 0.0095, 3.0861]],

        [[0.0056, 0.0976, 0.9398, 0.0138, 0.0095, 3.0861]],

        [[0.0056, 0.0976, 0.9398, 0.0138, 0.0095, 3.0861]],

        [[0.0056, 0.0976, 0.9398, 0.0138, 0.0095, 3.0861]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0086, 0.6336, 6.3712, 0.0779], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0086, 0.6336, 6.3712, 0.0779])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.764
Iter 2/2000 - Loss: 3.062
Iter 3/2000 - Loss: 2.724
Iter 4/2000 - Loss: 2.748
Iter 5/2000 - Loss: 2.847
Iter 6/2000 - Loss: 2.782
Iter 7/2000 - Loss: 2.660
Iter 8/2000 - Loss: 2.594
Iter 9/2000 - Loss: 2.590
Iter 10/2000 - Loss: 2.578
Iter 11/2000 - Loss: 2.509
Iter 12/2000 - Loss: 2.394
Iter 13/2000 - Loss: 2.271
Iter 14/2000 - Loss: 2.158
Iter 15/2000 - Loss: 2.047
Iter 16/2000 - Loss: 1.913
Iter 17/2000 - Loss: 1.738
Iter 18/2000 - Loss: 1.523
Iter 19/2000 - Loss: 1.282
Iter 20/2000 - Loss: 1.026
Iter 1981/2000 - Loss: -7.902
Iter 1982/2000 - Loss: -7.902
Iter 1983/2000 - Loss: -7.902
Iter 1984/2000 - Loss: -7.902
Iter 1985/2000 - Loss: -7.902
Iter 1986/2000 - Loss: -7.903
Iter 1987/2000 - Loss: -7.903
Iter 1988/2000 - Loss: -7.903
Iter 1989/2000 - Loss: -7.903
Iter 1990/2000 - Loss: -7.903
Iter 1991/2000 - Loss: -7.903
Iter 1992/2000 - Loss: -7.903
Iter 1993/2000 - Loss: -7.903
Iter 1994/2000 - Loss: -7.903
Iter 1995/2000 - Loss: -7.903
Iter 1996/2000 - Loss: -7.903
Iter 1997/2000 - Loss: -7.903
Iter 1998/2000 - Loss: -7.903
Iter 1999/2000 - Loss: -7.903
Iter 2000/2000 - Loss: -7.903
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[10.7362,  6.2929, 42.3551, 11.8326, 14.1892, 44.3800]],

        [[14.6829, 28.8727,  8.7929,  1.4515,  2.8679, 27.9800]],

        [[15.6534, 29.4001,  8.9088,  1.2396,  1.0798, 19.1555]],

        [[12.5364, 27.7244, 11.1038,  1.5440,  1.8528, 42.9349]]])
Signal Variance: tensor([ 0.0658,  3.2504, 20.1100,  0.3972])
Estimated target variance: tensor([0.0086, 0.6336, 6.3712, 0.0779])
N: 190
Signal to noise ratio: tensor([14.2878, 99.8341, 98.9943, 41.3507])
Bound on condition number: tensor([  38787.6748, 1893702.7205, 1861976.5834,  324878.0758])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.11986620873074817, policy loss: 5.661637571283054
Experience 19, Iter 1, disc loss: 0.18540006633793282, policy loss: 6.5349414906505885
Experience 19, Iter 2, disc loss: 0.18128646215296648, policy loss: 6.048356943865244
Experience 19, Iter 3, disc loss: 0.1465339660610046, policy loss: 6.376433695361763
Experience 19, Iter 4, disc loss: 0.1723551013572327, policy loss: 6.67321251011586
Experience 19, Iter 5, disc loss: 0.18141231918059025, policy loss: 6.396843043221784
Experience 19, Iter 6, disc loss: 0.167893968414446, policy loss: 5.998674499649335
Experience 19, Iter 7, disc loss: 0.14965501465151898, policy loss: 6.617639872242761
Experience 19, Iter 8, disc loss: 0.14998112546592623, policy loss: 6.850037005246751
Experience 19, Iter 9, disc loss: 0.16407554690086185, policy loss: 6.195981145236026
Experience 19, Iter 10, disc loss: 0.15763502568147947, policy loss: 5.817683367978674
Experience 19, Iter 11, disc loss: 0.14078086333151374, policy loss: 5.9754739907784815
Experience 19, Iter 12, disc loss: 0.11677855955002944, policy loss: 6.724990321413809
Experience 19, Iter 13, disc loss: 0.1331280838450456, policy loss: 5.702846343509995
Experience 19, Iter 14, disc loss: 0.11732280572280307, policy loss: 6.042373751494038
Experience 19, Iter 15, disc loss: 0.12900533896120053, policy loss: 5.580832003584713
Experience 19, Iter 16, disc loss: 0.12031779076254984, policy loss: 6.286211796742192
Experience 19, Iter 17, disc loss: 0.1310178437128562, policy loss: 5.20103374783115
Experience 19, Iter 18, disc loss: 0.13192976611196777, policy loss: 6.3798803608774435
Experience 19, Iter 19, disc loss: 0.11342095538843114, policy loss: 7.228399748094571
Experience 19, Iter 20, disc loss: 0.1353439595814153, policy loss: 5.444576255753084
Experience 19, Iter 21, disc loss: 0.12806551846863412, policy loss: 5.715520680378653
Experience 19, Iter 22, disc loss: 0.15291330432254252, policy loss: 6.334531885749009
Experience 19, Iter 23, disc loss: 0.15751664676602983, policy loss: 6.275541435676027
Experience 19, Iter 24, disc loss: 0.15240692361000707, policy loss: 6.830847730058621
Experience 19, Iter 25, disc loss: 0.17933221138132482, policy loss: 6.266800340197229
Experience 19, Iter 26, disc loss: 0.17807182391008752, policy loss: 6.295465361518449
Experience 19, Iter 27, disc loss: 0.2209468940193725, policy loss: 5.660642848886073
Experience 19, Iter 28, disc loss: 0.22219231059232414, policy loss: 6.246373006836159
Experience 19, Iter 29, disc loss: 0.23352247049136696, policy loss: 4.8547980572645955
Experience 19, Iter 30, disc loss: 0.2553686415202991, policy loss: 5.89438107322696
Experience 19, Iter 31, disc loss: 0.2706710533923803, policy loss: 4.716926871416446
Experience 19, Iter 32, disc loss: 0.2704940503362431, policy loss: 5.586473641394403
Experience 19, Iter 33, disc loss: 0.2630091780317482, policy loss: 6.795160109010881
Experience 19, Iter 34, disc loss: 0.3279832951504788, policy loss: 6.088060082943528
Experience 19, Iter 35, disc loss: 0.2753227690721777, policy loss: 6.669134752288619
Experience 19, Iter 36, disc loss: 0.3015443549442685, policy loss: 7.080665890564661
Experience 19, Iter 37, disc loss: 0.3099607383769853, policy loss: 5.8745116428980095
Experience 19, Iter 38, disc loss: 0.31512342115983916, policy loss: 6.905679732757234
Experience 19, Iter 39, disc loss: 0.35796200320990856, policy loss: 5.678222952172829
Experience 19, Iter 40, disc loss: 0.31456516075476615, policy loss: 5.259632642004824
Experience 19, Iter 41, disc loss: 0.31085486577322413, policy loss: 5.778761794239774
Experience 19, Iter 42, disc loss: 0.29433677181660056, policy loss: 6.555392178707088
Experience 19, Iter 43, disc loss: 0.2857746302792563, policy loss: 5.6743225762222735
Experience 19, Iter 44, disc loss: 0.3868592700501529, policy loss: 5.275910618347481
Experience 19, Iter 45, disc loss: 0.3721110640216978, policy loss: 5.221836657312925
Experience 19, Iter 46, disc loss: 0.3886956672131124, policy loss: 5.777118101600994
Experience 19, Iter 47, disc loss: 0.4169654477467253, policy loss: 5.370197488563954
Experience 19, Iter 48, disc loss: 0.36920553328662636, policy loss: 5.362982514178022
Experience 19, Iter 49, disc loss: 0.33155814543860307, policy loss: 4.481862544869445
Experience 19, Iter 50, disc loss: 0.3677045215350818, policy loss: 5.389917812281368
Experience 19, Iter 51, disc loss: 0.38475532081337277, policy loss: 6.644261009153864
Experience 19, Iter 52, disc loss: 0.37662988234147216, policy loss: 6.126094074872646
Experience 19, Iter 53, disc loss: 0.48059123777183094, policy loss: 5.899938489279797
Experience 19, Iter 54, disc loss: 0.47569695757756203, policy loss: 5.505732803611997
Experience 19, Iter 55, disc loss: 0.5029365067678913, policy loss: 4.931974663427304
Experience 19, Iter 56, disc loss: 0.46600761208181285, policy loss: 5.506424328105064
Experience 19, Iter 57, disc loss: 0.4176845630737334, policy loss: 6.317492819938185
Experience 19, Iter 58, disc loss: 0.47711371403823133, policy loss: 4.989024261597463
Experience 19, Iter 59, disc loss: 0.45320180706341634, policy loss: 5.568147752030976
Experience 19, Iter 60, disc loss: 0.417115835507657, policy loss: 5.868553040023496
Experience 19, Iter 61, disc loss: 0.4809503234128375, policy loss: 5.626885218032612
Experience 19, Iter 62, disc loss: 0.48007278627724004, policy loss: 6.282094681996039
Experience 19, Iter 63, disc loss: 0.6015677740854419, policy loss: 6.219988655735886
Experience 19, Iter 64, disc loss: 0.7587459812781308, policy loss: 6.772035147860949
Experience 19, Iter 65, disc loss: 0.6531597917858526, policy loss: 6.303768579520025
Experience 19, Iter 66, disc loss: 0.6816999399337416, policy loss: 6.7654368186524785
Experience 19, Iter 67, disc loss: 0.6543943034374253, policy loss: 6.658002740818699
Experience 19, Iter 68, disc loss: 0.5059171518702473, policy loss: 5.623582815681064
Experience 19, Iter 69, disc loss: 0.5702797058818122, policy loss: 4.1081177286051265
Experience 19, Iter 70, disc loss: 0.46435682799960526, policy loss: 5.587996909154
Experience 19, Iter 71, disc loss: 0.38048051647252257, policy loss: 6.12064465614225
Experience 19, Iter 72, disc loss: 0.3481186871825316, policy loss: 5.256902377889991
Experience 19, Iter 73, disc loss: 0.308388355442086, policy loss: 5.929883461996743
Experience 19, Iter 74, disc loss: 0.47254856693569824, policy loss: 3.544231255938872
Experience 19, Iter 75, disc loss: 0.3029581392691111, policy loss: 4.8456709636088
Experience 19, Iter 76, disc loss: 0.3094807633341498, policy loss: 4.9215209476981885
Experience 19, Iter 77, disc loss: 0.2657143867250811, policy loss: 5.619447901852691
Experience 19, Iter 78, disc loss: 0.3177285993009147, policy loss: 5.292871709427622
Experience 19, Iter 79, disc loss: 0.326688578236716, policy loss: 4.519554591166276
Experience 19, Iter 80, disc loss: 0.34719952440278207, policy loss: 6.308205026843787
Experience 19, Iter 81, disc loss: 0.37627859498595706, policy loss: 5.127153320493761
Experience 19, Iter 82, disc loss: 0.3829104282574324, policy loss: 4.970802447011531
Experience 19, Iter 83, disc loss: 0.38465318513867924, policy loss: 4.390355521266544
Experience 19, Iter 84, disc loss: 0.3545482626987314, policy loss: 5.319468729375864
Experience 19, Iter 85, disc loss: 0.39121547632076503, policy loss: 4.081582214451027
Experience 19, Iter 86, disc loss: 0.44197814511897093, policy loss: 4.857133591554137
Experience 19, Iter 87, disc loss: 0.4689912981167046, policy loss: 4.962430650777163
Experience 19, Iter 88, disc loss: 0.49613019048212603, policy loss: 5.3136060782318015
Experience 19, Iter 89, disc loss: 0.4957024419099848, policy loss: 5.990558609498455
Experience 19, Iter 90, disc loss: 0.5059154418039544, policy loss: 5.493688000133721
Experience 19, Iter 91, disc loss: 0.6106692403897082, policy loss: 3.747227992883137
Experience 19, Iter 92, disc loss: 0.5390910858159791, policy loss: 5.182467788081418
Experience 19, Iter 93, disc loss: 0.5641333508702613, policy loss: 4.5520999794605554
Experience 19, Iter 94, disc loss: 0.49762990558917, policy loss: 5.139084923219539
Experience 19, Iter 95, disc loss: 0.5573337401201705, policy loss: 3.939826968771677
Experience 19, Iter 96, disc loss: 0.538122065306073, policy loss: 5.3794730172613505
Experience 19, Iter 97, disc loss: 0.45886015557096127, policy loss: 6.742432533683203
Experience 19, Iter 98, disc loss: 0.32641842863432236, policy loss: 4.164271399675027
Experience 19, Iter 99, disc loss: 0.21982915084311866, policy loss: 4.901209384290038
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1697],
        [1.6719],
        [0.0218]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0056, 0.1036, 1.0466, 0.0150, 0.0110, 3.3222]],

        [[0.0056, 0.1036, 1.0466, 0.0150, 0.0110, 3.3222]],

        [[0.0056, 0.1036, 1.0466, 0.0150, 0.0110, 3.3222]],

        [[0.0056, 0.1036, 1.0466, 0.0150, 0.0110, 3.3222]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0090, 0.6786, 6.6876, 0.0872], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0090, 0.6786, 6.6876, 0.0872])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.905
Iter 2/2000 - Loss: 3.166
Iter 3/2000 - Loss: 2.861
Iter 4/2000 - Loss: 2.880
Iter 5/2000 - Loss: 2.961
Iter 6/2000 - Loss: 2.896
Iter 7/2000 - Loss: 2.784
Iter 8/2000 - Loss: 2.717
Iter 9/2000 - Loss: 2.700
Iter 10/2000 - Loss: 2.672
Iter 11/2000 - Loss: 2.590
Iter 12/2000 - Loss: 2.466
Iter 13/2000 - Loss: 2.330
Iter 14/2000 - Loss: 2.201
Iter 15/2000 - Loss: 2.068
Iter 16/2000 - Loss: 1.910
Iter 17/2000 - Loss: 1.711
Iter 18/2000 - Loss: 1.476
Iter 19/2000 - Loss: 1.219
Iter 20/2000 - Loss: 0.950
Iter 1981/2000 - Loss: -7.957
Iter 1982/2000 - Loss: -7.957
Iter 1983/2000 - Loss: -7.957
Iter 1984/2000 - Loss: -7.957
Iter 1985/2000 - Loss: -7.957
Iter 1986/2000 - Loss: -7.957
Iter 1987/2000 - Loss: -7.957
Iter 1988/2000 - Loss: -7.957
Iter 1989/2000 - Loss: -7.957
Iter 1990/2000 - Loss: -7.957
Iter 1991/2000 - Loss: -7.957
Iter 1992/2000 - Loss: -7.957
Iter 1993/2000 - Loss: -7.957
Iter 1994/2000 - Loss: -7.957
Iter 1995/2000 - Loss: -7.957
Iter 1996/2000 - Loss: -7.957
Iter 1997/2000 - Loss: -7.957
Iter 1998/2000 - Loss: -7.957
Iter 1999/2000 - Loss: -7.957
Iter 2000/2000 - Loss: -7.957
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[ 9.8232,  6.3166, 43.4885, 11.0761, 13.9335, 44.1922]],

        [[14.3538, 30.5784,  8.5916,  1.4863,  2.7960, 28.8197]],

        [[15.0594, 31.0471,  8.9634,  1.1961,  1.0424, 19.2779]],

        [[12.0310, 26.4723, 12.3253,  1.5368,  1.9211, 44.6236]]])
Signal Variance: tensor([ 0.0655,  3.3281, 19.6556,  0.4637])
Estimated target variance: tensor([0.0090, 0.6786, 6.6876, 0.0872])
N: 200
Signal to noise ratio: tensor([ 14.4075, 101.0354,  97.6628,  44.7419])
Bound on condition number: tensor([  41516.3187, 2041631.3477, 1907605.3520,  400368.7457])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.17088869659463302, policy loss: 4.751138654640835
Experience 20, Iter 1, disc loss: 0.15693819316278101, policy loss: 5.057355072001627
Experience 20, Iter 2, disc loss: 0.1575161007598373, policy loss: 5.677338183931434
Experience 20, Iter 3, disc loss: 0.13490069107285652, policy loss: 7.348577346065711
Experience 20, Iter 4, disc loss: 0.12856900192878118, policy loss: 5.716618068751633
Experience 20, Iter 5, disc loss: 0.13885653823946625, policy loss: 5.744146818519663
Experience 20, Iter 6, disc loss: 0.07263533914343308, policy loss: 8.77867857515274
Experience 20, Iter 7, disc loss: 0.06601671064529792, policy loss: 8.972306958912458
Experience 20, Iter 8, disc loss: 0.05475508318190148, policy loss: 10.767978349709637
Experience 20, Iter 9, disc loss: 0.03954277131270624, policy loss: 11.307069761956669
Experience 20, Iter 10, disc loss: 0.026042953545980498, policy loss: 12.067496857077288
Experience 20, Iter 11, disc loss: 0.016210529561423194, policy loss: 11.75156202417684
Experience 20, Iter 12, disc loss: 0.010156300423612562, policy loss: 11.44417745532984
Experience 20, Iter 13, disc loss: 0.009136256659231777, policy loss: 8.212216741484207
Experience 20, Iter 14, disc loss: 0.014293459858850886, policy loss: 9.662062395194635
Experience 20, Iter 15, disc loss: 0.037674328172737324, policy loss: 9.320310358139448
Experience 20, Iter 16, disc loss: 0.08294587715329296, policy loss: 7.274045081873235
Experience 20, Iter 17, disc loss: 0.07344169410378588, policy loss: 7.675268235550043
Experience 20, Iter 18, disc loss: 0.06505097626235053, policy loss: 7.512805514530206
Experience 20, Iter 19, disc loss: 0.06173740672274506, policy loss: 7.681398609639808
Experience 20, Iter 20, disc loss: 0.03811002319426851, policy loss: 7.501706886994151
Experience 20, Iter 21, disc loss: 0.04067739647637779, policy loss: 8.870679631944988
Experience 20, Iter 22, disc loss: 0.037037439913477965, policy loss: 7.696549209236034
Experience 20, Iter 23, disc loss: 0.05673970922831263, policy loss: 8.75730398010904
Experience 20, Iter 24, disc loss: 0.06347429832323935, policy loss: 9.038296056533474
Experience 20, Iter 25, disc loss: 0.08014871859734139, policy loss: 8.595794160289318
Experience 20, Iter 26, disc loss: 0.08172431095959118, policy loss: 7.684706836769793
Experience 20, Iter 27, disc loss: 0.09297289773945905, policy loss: 7.562381671640149
Experience 20, Iter 28, disc loss: 0.09823157180144522, policy loss: 7.334777555695216
Experience 20, Iter 29, disc loss: 0.10014703860060453, policy loss: 5.650743013630015
Experience 20, Iter 30, disc loss: 0.18245748984964358, policy loss: 5.461188396724746
Experience 20, Iter 31, disc loss: 0.1422748068646815, policy loss: 7.046247780241746
Experience 20, Iter 32, disc loss: 0.17500304700797692, policy loss: 5.77676486805852
Experience 20, Iter 33, disc loss: 0.16763611085823704, policy loss: 6.080611710240783
Experience 20, Iter 34, disc loss: 0.2048675515471967, policy loss: 5.966256860781353
Experience 20, Iter 35, disc loss: 0.2245699342068635, policy loss: 6.181775176300227
Experience 20, Iter 36, disc loss: 0.26257071753029904, policy loss: 6.129076838068903
Experience 20, Iter 37, disc loss: 0.2451523003407819, policy loss: 6.108519117542814
Experience 20, Iter 38, disc loss: 0.26444044944619793, policy loss: 3.7228205330553923
Experience 20, Iter 39, disc loss: 0.2938084623053722, policy loss: 5.722400319327756
Experience 20, Iter 40, disc loss: 0.2939688578806618, policy loss: 4.318723599098567
Experience 20, Iter 41, disc loss: 0.36150875477459055, policy loss: 4.40148393058505
Experience 20, Iter 42, disc loss: 0.35168405701423905, policy loss: 6.36078067072468
Experience 20, Iter 43, disc loss: 0.39417390657731866, policy loss: 5.636688905053385
Experience 20, Iter 44, disc loss: 0.37286164482189676, policy loss: 5.333423481386846
Experience 20, Iter 45, disc loss: 0.35472291097591613, policy loss: 4.402542202873259
Experience 20, Iter 46, disc loss: 0.4084029497319417, policy loss: 5.242029132898353
Experience 20, Iter 47, disc loss: 0.4123224063531701, policy loss: 5.0917068896186946
Experience 20, Iter 48, disc loss: 0.36549770681225524, policy loss: 4.842600625540387
Experience 20, Iter 49, disc loss: 0.4163369044863651, policy loss: 4.941536337031521
Experience 20, Iter 50, disc loss: 0.4105513478388698, policy loss: 4.920704687748685
Experience 20, Iter 51, disc loss: 0.43374969607248925, policy loss: 4.8920366228896635
Experience 20, Iter 52, disc loss: 0.38972379027723436, policy loss: 5.111920213605915
Experience 20, Iter 53, disc loss: 0.3845320002986504, policy loss: 4.3835501529468
Experience 20, Iter 54, disc loss: 0.4455161250584887, policy loss: 5.821514407279356
Experience 20, Iter 55, disc loss: 0.35861196122683225, policy loss: 5.60016564337673
Experience 20, Iter 56, disc loss: 0.34316328203780155, policy loss: 4.744115566679863
Experience 20, Iter 57, disc loss: 0.3733006508799083, policy loss: 4.663373966018327
Experience 20, Iter 58, disc loss: 0.44594545964478977, policy loss: 7.393341114495614
Experience 20, Iter 59, disc loss: 0.3617057050727165, policy loss: 7.126730476027353
Experience 20, Iter 60, disc loss: 0.4240628275361654, policy loss: 3.729537289706885
Experience 20, Iter 61, disc loss: 0.39497457379590895, policy loss: 4.049510268751434
Experience 20, Iter 62, disc loss: 0.4444686702630892, policy loss: 5.663439224076686
Experience 20, Iter 63, disc loss: 0.5097642911332294, policy loss: 4.807573675298316
Experience 20, Iter 64, disc loss: 0.4953579406115994, policy loss: 5.102350986450529
Experience 20, Iter 65, disc loss: 0.4537924393849281, policy loss: 6.177935890201857
Experience 20, Iter 66, disc loss: 0.4993493632852831, policy loss: 4.447317221737805
Experience 20, Iter 67, disc loss: 0.5793277267348949, policy loss: 4.880754287175769
Experience 20, Iter 68, disc loss: 0.6133259558428925, policy loss: 4.855431843261582
Experience 20, Iter 69, disc loss: 0.6102710096084946, policy loss: 4.846741073584475
Experience 20, Iter 70, disc loss: 0.6100382503811477, policy loss: 4.762994963791515
Experience 20, Iter 71, disc loss: 0.604111626682977, policy loss: 5.037037637142001
Experience 20, Iter 72, disc loss: 0.6130771847182543, policy loss: 5.29651234620524
Experience 20, Iter 73, disc loss: 0.6422717787822976, policy loss: 6.841324926257769
Experience 20, Iter 74, disc loss: 0.5459624684519739, policy loss: 4.6723489524535236
Experience 20, Iter 75, disc loss: 0.5031286608298338, policy loss: 4.946682806847028
Experience 20, Iter 76, disc loss: 0.5237009937517193, policy loss: 4.174698234270529
Experience 20, Iter 77, disc loss: 0.5021756135797327, policy loss: 4.558661735547614
Experience 20, Iter 78, disc loss: 0.561676233305257, policy loss: 3.6976422883768487
Experience 20, Iter 79, disc loss: 0.5671983861749835, policy loss: 5.021415759647509
Experience 20, Iter 80, disc loss: 0.5559035328525932, policy loss: 3.7703883363991295
Experience 20, Iter 81, disc loss: 0.6018506542151192, policy loss: 5.26703378161005
Experience 20, Iter 82, disc loss: 0.6475281587062728, policy loss: 5.278112167223645
Experience 20, Iter 83, disc loss: 0.6407313677429118, policy loss: 5.320060935799347
Experience 20, Iter 84, disc loss: 0.5672741616554632, policy loss: 5.534524487505676
Experience 20, Iter 85, disc loss: 0.4921319468888415, policy loss: 4.505224160363641
Experience 20, Iter 86, disc loss: 0.5424287644776163, policy loss: 3.8328416519547766
Experience 20, Iter 87, disc loss: 0.48905441541811834, policy loss: 4.970093747497713
Experience 20, Iter 88, disc loss: 0.48636819823072613, policy loss: 4.784229930234852
Experience 20, Iter 89, disc loss: 0.49072921205017983, policy loss: 4.604757351894241
Experience 20, Iter 90, disc loss: 0.4713609703757709, policy loss: 3.8171954506327372
Experience 20, Iter 91, disc loss: 0.4315345077262539, policy loss: 5.368994537085118
Experience 20, Iter 92, disc loss: 0.4716068016958744, policy loss: 6.414839950234537
Experience 20, Iter 93, disc loss: 0.3484260136633656, policy loss: 5.73369981869495
Experience 20, Iter 94, disc loss: 0.38532906274583245, policy loss: 5.422242018247756
Experience 20, Iter 95, disc loss: 0.5478529080257278, policy loss: 4.116414901437862
Experience 20, Iter 96, disc loss: 0.451489773862902, policy loss: 4.496768034119439
Experience 20, Iter 97, disc loss: 0.4997839206661443, policy loss: 5.9351033303887935
Experience 20, Iter 98, disc loss: 0.5342806575170579, policy loss: 5.070887634416685
Experience 20, Iter 99, disc loss: 0.5915156717870413, policy loss: 6.474262536421962
