Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0130],
        [0.8980],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0518, 3.5922, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 1.196
Iter 3/2000 - Loss: 0.953
Iter 4/2000 - Loss: 1.383
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.493
Iter 7/2000 - Loss: 1.248
Iter 8/2000 - Loss: 1.053
Iter 9/2000 - Loss: 0.964
Iter 10/2000 - Loss: 0.972
Iter 11/2000 - Loss: 1.033
Iter 12/2000 - Loss: 1.091
Iter 13/2000 - Loss: 1.105
Iter 14/2000 - Loss: 1.071
Iter 15/2000 - Loss: 1.012
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.889
Iter 20/2000 - Loss: 0.912
Iter 1981/2000 - Loss: 0.698
Iter 1982/2000 - Loss: 0.698
Iter 1983/2000 - Loss: 0.698
Iter 1984/2000 - Loss: 0.698
Iter 1985/2000 - Loss: 0.698
Iter 1986/2000 - Loss: 0.698
Iter 1987/2000 - Loss: 0.698
Iter 1988/2000 - Loss: 0.698
Iter 1989/2000 - Loss: 0.698
Iter 1990/2000 - Loss: 0.698
Iter 1991/2000 - Loss: 0.698
Iter 1992/2000 - Loss: 0.698
Iter 1993/2000 - Loss: 0.698
Iter 1994/2000 - Loss: 0.698
Iter 1995/2000 - Loss: 0.698
Iter 1996/2000 - Loss: 0.698
Iter 1997/2000 - Loss: 0.698
Iter 1998/2000 - Loss: 0.698
Iter 1999/2000 - Loss: 0.698
Iter 2000/2000 - Loss: 0.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0093],
        [0.4134],
        [0.0149]])
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]])
Signal Variance: tensor([0.0033, 0.0374, 2.8196, 0.0608])
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([1.9681, 2.0097, 2.6117, 2.0186])
Bound on condition number: tensor([39.7326, 41.3892, 69.2095, 41.7472])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.2474456489459007, policy loss: 0.7896666701180209
Experience 1, Iter 1, disc loss: 1.2385530979859949, policy loss: 0.7873822945602964
Experience 1, Iter 2, disc loss: 1.2212872343106467, policy loss: 0.7971593157938196
Experience 1, Iter 3, disc loss: 1.2120392261271329, policy loss: 0.7965308816260855
Experience 1, Iter 4, disc loss: 1.1978704671483684, policy loss: 0.8019593015361349
Experience 1, Iter 5, disc loss: 1.1900267018444524, policy loss: 0.8005851728977406
Experience 1, Iter 6, disc loss: 1.1811046799201224, policy loss: 0.7995222319118466
Experience 1, Iter 7, disc loss: 1.1722204252018167, policy loss: 0.8011044303819035
Experience 1, Iter 8, disc loss: 1.1530986896030975, policy loss: 0.8132670046728927
Experience 1, Iter 9, disc loss: 1.122415110939838, policy loss: 0.8429443459769241
Experience 1, Iter 10, disc loss: 1.137939527465872, policy loss: 0.8168084797480879
Experience 1, Iter 11, disc loss: 1.1172312974112513, policy loss: 0.8307876220992817
Experience 1, Iter 12, disc loss: 1.0989825755034626, policy loss: 0.8478121508151631
Experience 1, Iter 13, disc loss: 1.1018771084620833, policy loss: 0.8298853463867228
Experience 1, Iter 14, disc loss: 1.0776462477562365, policy loss: 0.8528754015520523
Experience 1, Iter 15, disc loss: 1.0669842366524818, policy loss: 0.8582043623049599
Experience 1, Iter 16, disc loss: 1.0539026143511445, policy loss: 0.8622557414716107
Experience 1, Iter 17, disc loss: 1.0581097317580164, policy loss: 0.8483693529123395
Experience 1, Iter 18, disc loss: 1.0190379666848497, policy loss: 0.8856899301413108
Experience 1, Iter 19, disc loss: 1.0282507496854867, policy loss: 0.8659186574437038
Experience 1, Iter 20, disc loss: 1.0055480166122641, policy loss: 0.8822813126624008
Experience 1, Iter 21, disc loss: 0.991123076209983, policy loss: 0.8927332393313934
Experience 1, Iter 22, disc loss: 0.9756035050031296, policy loss: 0.9004540220604262
Experience 1, Iter 23, disc loss: 0.9525114556008829, policy loss: 0.924119681502718
Experience 1, Iter 24, disc loss: 0.9698991883528149, policy loss: 0.8907441111607901
Experience 1, Iter 25, disc loss: 0.9357376445706223, policy loss: 0.9279156680830282
Experience 1, Iter 26, disc loss: 0.9433121581978576, policy loss: 0.9104329485541991
Experience 1, Iter 27, disc loss: 0.8940964873350346, policy loss: 0.970421429770483
Experience 1, Iter 28, disc loss: 0.8965261489602461, policy loss: 0.9472819731863473
Experience 1, Iter 29, disc loss: 0.8748092151333206, policy loss: 0.9639569108965493
Experience 1, Iter 30, disc loss: 0.8543125160684968, policy loss: 0.9879279206612215
Experience 1, Iter 31, disc loss: 0.8306419214044616, policy loss: 1.0119217230623685
Experience 1, Iter 32, disc loss: 0.8303189324913292, policy loss: 1.0040077924518944
Experience 1, Iter 33, disc loss: 0.8114118075385344, policy loss: 1.032452597224527
Experience 1, Iter 34, disc loss: 0.7925580336692666, policy loss: 1.0438208411853034
Experience 1, Iter 35, disc loss: 0.7655832185680335, policy loss: 1.0864901647527123
Experience 1, Iter 36, disc loss: 0.7450282776223407, policy loss: 1.09925558129618
Experience 1, Iter 37, disc loss: 0.7451763524842552, policy loss: 1.08950660376082
Experience 1, Iter 38, disc loss: 0.7391957017057247, policy loss: 1.0864347211099674
Experience 1, Iter 39, disc loss: 0.7031279566161275, policy loss: 1.1408569190729776
Experience 1, Iter 40, disc loss: 0.7074779846240586, policy loss: 1.1217040478447502
Experience 1, Iter 41, disc loss: 0.6955395148614529, policy loss: 1.125496115495468
Experience 1, Iter 42, disc loss: 0.6716132454127373, policy loss: 1.1548405222758344
Experience 1, Iter 43, disc loss: 0.6463584232770891, policy loss: 1.198552877131831
Experience 1, Iter 44, disc loss: 0.6482404137504563, policy loss: 1.179945895910561
Experience 1, Iter 45, disc loss: 0.6435348135040968, policy loss: 1.168890485970992
Experience 1, Iter 46, disc loss: 0.6185003211283597, policy loss: 1.213299788882587
Experience 1, Iter 47, disc loss: 0.6347612202464707, policy loss: 1.1773163923292413
Experience 1, Iter 48, disc loss: 0.589231876334702, policy loss: 1.2653969090554351
Experience 1, Iter 49, disc loss: 0.5718233039174505, policy loss: 1.2904060844616274
Experience 1, Iter 50, disc loss: 0.5457122922017144, policy loss: 1.3307934968543567
Experience 1, Iter 51, disc loss: 0.5465700888015633, policy loss: 1.310985262653187
Experience 1, Iter 52, disc loss: 0.5410658305367522, policy loss: 1.3517790546422705
Experience 1, Iter 53, disc loss: 0.5376620421286208, policy loss: 1.3224427911489869
Experience 1, Iter 54, disc loss: 0.4941385413698628, policy loss: 1.4198386927416284
Experience 1, Iter 55, disc loss: 0.4713478753894938, policy loss: 1.452970514427048
Experience 1, Iter 56, disc loss: 0.4683066567912222, policy loss: 1.4443162228972084
Experience 1, Iter 57, disc loss: 0.4812180556012719, policy loss: 1.416560991648133
Experience 1, Iter 58, disc loss: 0.44128602068857414, policy loss: 1.5047681760992064
Experience 1, Iter 59, disc loss: 0.41501670795711176, policy loss: 1.5714058188195383
Experience 1, Iter 60, disc loss: 0.44074763897409186, policy loss: 1.4896810590533787
Experience 1, Iter 61, disc loss: 0.44597391609956893, policy loss: 1.5004096133490394
Experience 1, Iter 62, disc loss: 0.4056793236717391, policy loss: 1.5695277901415308
Experience 1, Iter 63, disc loss: 0.38968459741630684, policy loss: 1.6341978180937484
Experience 1, Iter 64, disc loss: 0.4077951956606906, policy loss: 1.5841239865025112
Experience 1, Iter 65, disc loss: 0.40196269206920043, policy loss: 1.5955024824419262
Experience 1, Iter 66, disc loss: 0.37135707058452977, policy loss: 1.6582917710661946
Experience 1, Iter 67, disc loss: 0.38268617216653045, policy loss: 1.5964423093985374
Experience 1, Iter 68, disc loss: 0.34846831312860915, policy loss: 1.723580641502941
Experience 1, Iter 69, disc loss: 0.341379596218031, policy loss: 1.7296870035866554
Experience 1, Iter 70, disc loss: 0.32640712703978675, policy loss: 1.7894287933723914
Experience 1, Iter 71, disc loss: 0.3106753876244759, policy loss: 1.8484221660568934
Experience 1, Iter 72, disc loss: 0.30787795307745414, policy loss: 1.870411639302901
Experience 1, Iter 73, disc loss: 0.32102086179715444, policy loss: 1.8479912371930536
Experience 1, Iter 74, disc loss: 0.29200207418989066, policy loss: 1.9160752785680812
Experience 1, Iter 75, disc loss: 0.297013137530249, policy loss: 1.894805861116803
Experience 1, Iter 76, disc loss: 0.2633525837503968, policy loss: 1.9923463501348393
Experience 1, Iter 77, disc loss: 0.27535334597032385, policy loss: 1.9633559617765037
Experience 1, Iter 78, disc loss: 0.27301087471492347, policy loss: 1.9728392539668733
Experience 1, Iter 79, disc loss: 0.22216760913955072, policy loss: 2.197733274545391
Experience 1, Iter 80, disc loss: 0.2644386381807199, policy loss: 2.000139330941984
Experience 1, Iter 81, disc loss: 0.23908078033932292, policy loss: 2.0899771939891307
Experience 1, Iter 82, disc loss: 0.22892454722376876, policy loss: 2.129954937732974
Experience 1, Iter 83, disc loss: 0.240415023095088, policy loss: 1.998263753451364
Experience 1, Iter 84, disc loss: 0.19043877430338607, policy loss: 2.2999595534607105
Experience 1, Iter 85, disc loss: 0.21093084986284966, policy loss: 2.236909078719342
Experience 1, Iter 86, disc loss: 0.2106146862000806, policy loss: 2.171343065804076
Experience 1, Iter 87, disc loss: 0.21282495545468375, policy loss: 2.1307602525213793
Experience 1, Iter 88, disc loss: 0.1892591853189308, policy loss: 2.370391538275627
Experience 1, Iter 89, disc loss: 0.19847074762292197, policy loss: 2.2775519779363664
Experience 1, Iter 90, disc loss: 0.16999565453932614, policy loss: 2.3677817990483447
Experience 1, Iter 91, disc loss: 0.19266028870868904, policy loss: 2.4404499786079863
Experience 1, Iter 92, disc loss: 0.16938633341088288, policy loss: 2.496129761657333
Experience 1, Iter 93, disc loss: 0.16839332261313836, policy loss: 2.541071615143302
Experience 1, Iter 94, disc loss: 0.17853419322223546, policy loss: 2.4053491965118687
Experience 1, Iter 95, disc loss: 0.16759397981656585, policy loss: 2.4891914514931046
Experience 1, Iter 96, disc loss: 0.17402995905365193, policy loss: 2.463129287063885
Experience 1, Iter 97, disc loss: 0.167276424584687, policy loss: 2.498912799933705
Experience 1, Iter 98, disc loss: 0.13416838989494817, policy loss: 2.8243731982833413
Experience 1, Iter 99, disc loss: 0.15285710612555092, policy loss: 2.4776344389852163
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0073],
        [0.0373],
        [0.6645],
        [0.0136]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0451, 0.2804, 0.6420, 0.0166, 0.0019, 1.1740]],

        [[0.0451, 0.2804, 0.6420, 0.0166, 0.0019, 1.1740]],

        [[0.0451, 0.2804, 0.6420, 0.0166, 0.0019, 1.1740]],

        [[0.0451, 0.2804, 0.6420, 0.0166, 0.0019, 1.1740]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0293, 0.1490, 2.6580, 0.0545], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0293, 0.1490, 2.6580, 0.0545])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.311
Iter 2/2000 - Loss: 2.591
Iter 3/2000 - Loss: 2.192
Iter 4/2000 - Loss: 2.115
Iter 5/2000 - Loss: 2.173
Iter 6/2000 - Loss: 2.210
Iter 7/2000 - Loss: 2.214
Iter 8/2000 - Loss: 2.200
Iter 9/2000 - Loss: 2.164
Iter 10/2000 - Loss: 2.112
Iter 11/2000 - Loss: 2.069
Iter 12/2000 - Loss: 2.050
Iter 13/2000 - Loss: 2.048
Iter 14/2000 - Loss: 2.043
Iter 15/2000 - Loss: 2.018
Iter 16/2000 - Loss: 1.971
Iter 17/2000 - Loss: 1.909
Iter 18/2000 - Loss: 1.844
Iter 19/2000 - Loss: 1.784
Iter 20/2000 - Loss: 1.740
Iter 1981/2000 - Loss: -4.332
Iter 1982/2000 - Loss: -4.332
Iter 1983/2000 - Loss: -4.332
Iter 1984/2000 - Loss: -4.332
Iter 1985/2000 - Loss: -4.332
Iter 1986/2000 - Loss: -4.332
Iter 1987/2000 - Loss: -4.332
Iter 1988/2000 - Loss: -4.332
Iter 1989/2000 - Loss: -4.332
Iter 1990/2000 - Loss: -4.332
Iter 1991/2000 - Loss: -4.332
Iter 1992/2000 - Loss: -4.332
Iter 1993/2000 - Loss: -4.332
Iter 1994/2000 - Loss: -4.332
Iter 1995/2000 - Loss: -4.332
Iter 1996/2000 - Loss: -4.332
Iter 1997/2000 - Loss: -4.332
Iter 1998/2000 - Loss: -4.332
Iter 1999/2000 - Loss: -4.332
Iter 2000/2000 - Loss: -4.332
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0002],
        [0.0005],
        [0.0004]])
Lengthscale: tensor([[[21.2084,  5.0134, 63.8854, 25.5033,  4.7544, 64.4314]],

        [[32.7594, 56.6635,  4.4358,  1.2835,  2.5645,  6.2242]],

        [[ 3.9484, 37.2525, 14.3009,  0.8179,  8.4071, 16.1984]],

        [[33.1323, 50.4337,  8.0630,  1.1310,  0.9371, 27.3292]]])
Signal Variance: tensor([ 0.0809,  0.3121, 10.2554,  0.1571])
Estimated target variance: tensor([0.0293, 0.1490, 2.6580, 0.0545])
N: 20
Signal to noise ratio: tensor([ 13.8960,  43.3830, 143.4849,  21.1053])
Bound on condition number: tensor([  3862.9553,  37642.6828, 411759.5164,   8909.6637])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.06500978543987715, policy loss: 3.6044384200354624
Experience 2, Iter 1, disc loss: 0.06688249569256527, policy loss: 3.508994833216807
Experience 2, Iter 2, disc loss: 0.07526271684420185, policy loss: 3.2549201311851865
Experience 2, Iter 3, disc loss: 0.09170202509999587, policy loss: 2.9141659524229766
Experience 2, Iter 4, disc loss: 0.1104757522657994, policy loss: 2.6406655905040335
Experience 2, Iter 5, disc loss: 0.14484698323694523, policy loss: 2.295733821185454
Experience 2, Iter 6, disc loss: 0.17308081394808553, policy loss: 2.129278040371527
Experience 2, Iter 7, disc loss: 0.24237851762307544, policy loss: 1.9756487494510244
Experience 2, Iter 8, disc loss: 0.21659462671518165, policy loss: 2.3272803862886624
Experience 2, Iter 9, disc loss: 0.3264883506070016, policy loss: 1.8299506782553023
Experience 2, Iter 10, disc loss: 0.3128433509016384, policy loss: 1.6971321286913672
Experience 2, Iter 11, disc loss: 0.3730358331658849, policy loss: 1.3567714820085908
Experience 2, Iter 12, disc loss: 0.3551781095432766, policy loss: 1.4452650276083832
Experience 2, Iter 13, disc loss: 0.3709353520225434, policy loss: 1.3968820342865889
Experience 2, Iter 14, disc loss: 0.40454814654570476, policy loss: 1.3284291810422808
Experience 2, Iter 15, disc loss: 0.4120518369148927, policy loss: 1.2906436392571314
Experience 2, Iter 16, disc loss: 0.444564540758398, policy loss: 1.2475755692550397
Experience 2, Iter 17, disc loss: 0.3891788306019557, policy loss: 1.4923964398777891
Experience 2, Iter 18, disc loss: 0.4330101873258441, policy loss: 1.383695145651949
Experience 2, Iter 19, disc loss: 0.40063613258417374, policy loss: 1.4200027782509954
Experience 2, Iter 20, disc loss: 0.42875914819850114, policy loss: 1.4782122023243383
Experience 2, Iter 21, disc loss: 0.34190985117706324, policy loss: 1.642221285393262
Experience 2, Iter 22, disc loss: 0.4249089320229286, policy loss: 1.3998524668592327
Experience 2, Iter 23, disc loss: 0.4564893180223254, policy loss: 1.4117700445097765
Experience 2, Iter 24, disc loss: 0.4159470301898139, policy loss: 1.5104025135182195
Experience 2, Iter 25, disc loss: 0.3841129346732904, policy loss: 1.686395593704363
Experience 2, Iter 26, disc loss: 0.36310171378461864, policy loss: 1.7577619485551013
Experience 2, Iter 27, disc loss: 0.4057668319802146, policy loss: 1.8110764843017766
Experience 2, Iter 28, disc loss: 0.34310254500838505, policy loss: 1.802635757032631
Experience 2, Iter 29, disc loss: 0.34109495640410065, policy loss: 1.8526841500283984
Experience 2, Iter 30, disc loss: 0.37154580487207123, policy loss: 1.6477917642156712
Experience 2, Iter 31, disc loss: 0.342527161127821, policy loss: 1.7389579294454944
Experience 2, Iter 32, disc loss: 0.34845367664868543, policy loss: 1.7367644725007283
Experience 2, Iter 33, disc loss: 0.3537174477985307, policy loss: 1.68567961505537
Experience 2, Iter 34, disc loss: 0.30967896771452297, policy loss: 1.9614578081071812
Experience 2, Iter 35, disc loss: 0.33106261146557464, policy loss: 1.8644810038500088
Experience 2, Iter 36, disc loss: 0.3184424180530354, policy loss: 1.9201765587851551
Experience 2, Iter 37, disc loss: 0.37126857655690726, policy loss: 1.8089229445615276
Experience 2, Iter 38, disc loss: 0.2977718220065967, policy loss: 2.0417361093827946
Experience 2, Iter 39, disc loss: 0.3529151173910877, policy loss: 1.8801819678709482
Experience 2, Iter 40, disc loss: 0.3498197912054315, policy loss: 2.1471730219498766
Experience 2, Iter 41, disc loss: 0.30081728447237416, policy loss: 2.180213504188211
Experience 2, Iter 42, disc loss: 0.24691278036050585, policy loss: 2.3844615289104905
Experience 2, Iter 43, disc loss: 0.26516482219446175, policy loss: 2.0187890615352515
Experience 2, Iter 44, disc loss: 0.29162268006505, policy loss: 1.8500664512620029
Experience 2, Iter 45, disc loss: 0.27335911630491194, policy loss: 2.0306710048453165
Experience 2, Iter 46, disc loss: 0.2829750683705802, policy loss: 1.9131160159814358
Experience 2, Iter 47, disc loss: 0.2627982691584295, policy loss: 2.3443966126903746
Experience 2, Iter 48, disc loss: 0.3137103692267255, policy loss: 1.9057498255781782
Experience 2, Iter 49, disc loss: 0.22412106443789606, policy loss: 2.2377725233764636
Experience 2, Iter 50, disc loss: 0.1916548553543714, policy loss: 2.569169817588426
Experience 2, Iter 51, disc loss: 0.190142455883744, policy loss: 2.3905556663408025
Experience 2, Iter 52, disc loss: 0.19965951887537672, policy loss: 2.4159310885578114
Experience 2, Iter 53, disc loss: 0.20083687763484595, policy loss: 2.2374645960051636
Experience 2, Iter 54, disc loss: 0.20384534322456255, policy loss: 2.1446264015581633
Experience 2, Iter 55, disc loss: 0.20256550996816283, policy loss: 2.155588310150619
Experience 2, Iter 56, disc loss: 0.21962259963927117, policy loss: 2.1046467781529943
Experience 2, Iter 57, disc loss: 0.20702390322351977, policy loss: 2.39562635705637
Experience 2, Iter 58, disc loss: 0.19461468907093316, policy loss: 2.4290301046407867
Experience 2, Iter 59, disc loss: 0.21879960931135814, policy loss: 2.3083567617029344
Experience 2, Iter 60, disc loss: 0.22846721835467693, policy loss: 2.2910584683790756
Experience 2, Iter 61, disc loss: 0.1927984957320776, policy loss: 2.371316399569282
Experience 2, Iter 62, disc loss: 0.17894311556616607, policy loss: 2.2625828159976367
Experience 2, Iter 63, disc loss: 0.22407601303144126, policy loss: 2.105258344145114
Experience 2, Iter 64, disc loss: 0.16925206534302556, policy loss: 2.3254065026486073
Experience 2, Iter 65, disc loss: 0.16470247154219916, policy loss: 2.3410196051729204
Experience 2, Iter 66, disc loss: 0.14896184696272546, policy loss: 2.3403479230897446
Experience 2, Iter 67, disc loss: 0.14736096188779593, policy loss: 2.4953531454293936
Experience 2, Iter 68, disc loss: 0.1332759501178407, policy loss: 2.582322039975078
Experience 2, Iter 69, disc loss: 0.16355844462691002, policy loss: 2.3198802931374
Experience 2, Iter 70, disc loss: 0.15232439444766843, policy loss: 2.408676998656924
Experience 2, Iter 71, disc loss: 0.1679697099156959, policy loss: 2.4366062766155934
Experience 2, Iter 72, disc loss: 0.18018432968155165, policy loss: 2.482780109834831
Experience 2, Iter 73, disc loss: 0.17467471978469618, policy loss: 2.37950030254001
Experience 2, Iter 74, disc loss: 0.19839904476573803, policy loss: 2.4335868256648134
Experience 2, Iter 75, disc loss: 0.2031455990188866, policy loss: 2.6857397677098165
Experience 2, Iter 76, disc loss: 0.18025638744533984, policy loss: 3.033896191833774
Experience 2, Iter 77, disc loss: 0.16483020253024708, policy loss: 3.0129347416198544
Experience 2, Iter 78, disc loss: 0.1496392521765367, policy loss: 2.831488538767559
Experience 2, Iter 79, disc loss: 0.12769304985054428, policy loss: 2.7932127122216137
Experience 2, Iter 80, disc loss: 0.13407963358296654, policy loss: 2.6765078798553255
Experience 2, Iter 81, disc loss: 0.1241771233451531, policy loss: 2.7383037579905753
Experience 2, Iter 82, disc loss: 0.11147947333784826, policy loss: 2.8345905124309656
Experience 2, Iter 83, disc loss: 0.11180374094117657, policy loss: 2.843865105488992
Experience 2, Iter 84, disc loss: 0.09737353418720227, policy loss: 3.058368701830342
Experience 2, Iter 85, disc loss: 0.09314427352975345, policy loss: 3.1381230933556816
Experience 2, Iter 86, disc loss: 0.08860456673549802, policy loss: 3.2039647397300763
Experience 2, Iter 87, disc loss: 0.09021905813842668, policy loss: 3.166123732241113
Experience 2, Iter 88, disc loss: 0.10002854320085566, policy loss: 2.8821470809634255
Experience 2, Iter 89, disc loss: 0.0885133117930151, policy loss: 3.0050892373839164
Experience 2, Iter 90, disc loss: 0.09441510133199316, policy loss: 2.8948313705138853
Experience 2, Iter 91, disc loss: 0.09145468222934516, policy loss: 3.0239671743186136
Experience 2, Iter 92, disc loss: 0.09832435482313757, policy loss: 2.9790643227904625
Experience 2, Iter 93, disc loss: 0.11023401216303517, policy loss: 2.9340180443656685
Experience 2, Iter 94, disc loss: 0.10343092596056368, policy loss: 3.117245887854791
Experience 2, Iter 95, disc loss: 0.14021110222365404, policy loss: 2.649799315256713
Experience 2, Iter 96, disc loss: 0.15409554955985788, policy loss: 2.7466657995956707
Experience 2, Iter 97, disc loss: 0.14907449833354416, policy loss: 3.0524846771353547
Experience 2, Iter 98, disc loss: 0.16498421641196223, policy loss: 2.74816293692221
Experience 2, Iter 99, disc loss: 0.11399871972134672, policy loss: 2.995727683657719
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0079],
        [0.0463],
        [0.7728],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0553, 0.2886, 0.6461, 0.0206, 0.0033, 1.4580]],

        [[0.0553, 0.2886, 0.6461, 0.0206, 0.0033, 1.4580]],

        [[0.0553, 0.2886, 0.6461, 0.0206, 0.0033, 1.4580]],

        [[0.0553, 0.2886, 0.6461, 0.0206, 0.0033, 1.4580]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0314, 0.1851, 3.0913, 0.0585], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0314, 0.1851, 3.0913, 0.0585])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.835
Iter 2/2000 - Loss: 2.436
Iter 3/2000 - Loss: 2.312
Iter 4/2000 - Loss: 2.335
Iter 5/2000 - Loss: 2.374
Iter 6/2000 - Loss: 2.343
Iter 7/2000 - Loss: 2.287
Iter 8/2000 - Loss: 2.250
Iter 9/2000 - Loss: 2.232
Iter 10/2000 - Loss: 2.224
Iter 11/2000 - Loss: 2.215
Iter 12/2000 - Loss: 2.188
Iter 13/2000 - Loss: 2.138
Iter 14/2000 - Loss: 2.082
Iter 15/2000 - Loss: 2.041
Iter 16/2000 - Loss: 2.022
Iter 17/2000 - Loss: 2.008
Iter 18/2000 - Loss: 1.972
Iter 19/2000 - Loss: 1.904
Iter 20/2000 - Loss: 1.811
Iter 1981/2000 - Loss: -4.958
Iter 1982/2000 - Loss: -4.958
Iter 1983/2000 - Loss: -4.959
Iter 1984/2000 - Loss: -4.959
Iter 1985/2000 - Loss: -4.959
Iter 1986/2000 - Loss: -4.959
Iter 1987/2000 - Loss: -4.959
Iter 1988/2000 - Loss: -4.959
Iter 1989/2000 - Loss: -4.959
Iter 1990/2000 - Loss: -4.959
Iter 1991/2000 - Loss: -4.959
Iter 1992/2000 - Loss: -4.959
Iter 1993/2000 - Loss: -4.959
Iter 1994/2000 - Loss: -4.959
Iter 1995/2000 - Loss: -4.959
Iter 1996/2000 - Loss: -4.959
Iter 1997/2000 - Loss: -4.959
Iter 1998/2000 - Loss: -4.959
Iter 1999/2000 - Loss: -4.960
Iter 2000/2000 - Loss: -4.960
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0002],
        [0.0036],
        [0.0005]])
Lengthscale: tensor([[[22.5653,  5.3174, 59.3582, 18.1238, 16.3139, 34.0198]],

        [[24.5679, 44.0655,  4.9475,  1.0339,  8.8296,  9.8463]],

        [[30.7719, 54.1955, 15.3928,  0.9964,  7.5971, 20.4341]],

        [[28.0224, 55.2529, 13.3776,  2.4713, 11.2916, 28.5416]]])
Signal Variance: tensor([ 0.1048,  0.4157, 13.0680,  0.3542])
Estimated target variance: tensor([0.0314, 0.1851, 3.0913, 0.0585])
N: 30
Signal to noise ratio: tensor([17.2595, 41.0469, 60.5236, 25.6471])
Bound on condition number: tensor([  8937.7133,  50546.5036, 109894.0135,  19734.2132])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.5589545279034379, policy loss: 1.0121284799541956
Experience 3, Iter 1, disc loss: 0.502241585398788, policy loss: 1.1652052076278774
Experience 3, Iter 2, disc loss: 0.47982086371012467, policy loss: 1.267911930225896
Experience 3, Iter 3, disc loss: 0.45769187599809064, policy loss: 1.332923981091506
Experience 3, Iter 4, disc loss: 0.4019965295810128, policy loss: 1.513152585798918
Experience 3, Iter 5, disc loss: 0.4004889585748531, policy loss: 1.5274674987952592
Experience 3, Iter 6, disc loss: 0.4291244908732169, policy loss: 1.5201407860900407
Experience 3, Iter 7, disc loss: 0.3723287979648554, policy loss: 1.706518271612129
Experience 3, Iter 8, disc loss: 0.3661758673769122, policy loss: 1.8930128018815717
Experience 3, Iter 9, disc loss: 0.3852313520295998, policy loss: 1.9590317660786063
Experience 3, Iter 10, disc loss: 0.41280624946650835, policy loss: 1.9472052148224215
Experience 3, Iter 11, disc loss: 0.4061701737780633, policy loss: 2.157784872030247
Experience 3, Iter 12, disc loss: 0.4145520472099509, policy loss: 2.13745824221063
Experience 3, Iter 13, disc loss: 0.41060061376570045, policy loss: 2.1239778707982016
Experience 3, Iter 14, disc loss: 0.39936158949313555, policy loss: 2.0555342625464874
Experience 3, Iter 15, disc loss: 0.36806790940050016, policy loss: 2.0618705368486614
Experience 3, Iter 16, disc loss: 0.3386258520257722, policy loss: 2.006349754125776
Experience 3, Iter 17, disc loss: 0.3107600744461681, policy loss: 2.0179402678564626
Experience 3, Iter 18, disc loss: 0.2982293344641993, policy loss: 1.9116265143864963
Experience 3, Iter 19, disc loss: 0.2804103704505791, policy loss: 1.9145940678697597
Experience 3, Iter 20, disc loss: 0.27296496803272086, policy loss: 1.8760100197621652
Experience 3, Iter 21, disc loss: 0.27264217727048223, policy loss: 1.821494400562873
Experience 3, Iter 22, disc loss: 0.2662367530574168, policy loss: 1.856506383570364
Experience 3, Iter 23, disc loss: 0.302507519380587, policy loss: 1.6853911420065644
Experience 3, Iter 24, disc loss: 0.25428555576069495, policy loss: 1.8378087772816545
Experience 3, Iter 25, disc loss: 0.2730205011226776, policy loss: 1.8115285022694796
Experience 3, Iter 26, disc loss: 0.2601250538403316, policy loss: 1.8286518438008286
Experience 3, Iter 27, disc loss: 0.24468166558964355, policy loss: 1.8809523462136712
Experience 3, Iter 28, disc loss: 0.26367960604317553, policy loss: 1.7729908390401514
Experience 3, Iter 29, disc loss: 0.2718333373787631, policy loss: 1.7869483255864635
Experience 3, Iter 30, disc loss: 0.246031093186612, policy loss: 1.8852124765465932
Experience 3, Iter 31, disc loss: 0.24079227108574555, policy loss: 1.933529054125256
Experience 3, Iter 32, disc loss: 0.22495319465212565, policy loss: 2.045346412743602
Experience 3, Iter 33, disc loss: 0.21029471194541316, policy loss: 2.150036084325854
Experience 3, Iter 34, disc loss: 0.19527637821474925, policy loss: 2.2614097571403917
Experience 3, Iter 35, disc loss: 0.18631417168645054, policy loss: 2.3422049463034176
Experience 3, Iter 36, disc loss: 0.1830283247837995, policy loss: 2.4106688341029034
Experience 3, Iter 37, disc loss: 0.1897920987009981, policy loss: 2.3633501410608533
Experience 3, Iter 38, disc loss: 0.18021573456939555, policy loss: 2.477645609046045
Experience 3, Iter 39, disc loss: 0.18116934529814155, policy loss: 2.4444644123332475
Experience 3, Iter 40, disc loss: 0.17786776022712542, policy loss: 2.476097084876784
Experience 3, Iter 41, disc loss: 0.16550588554228118, policy loss: 2.620140684357202
Experience 3, Iter 42, disc loss: 0.15682510717692344, policy loss: 2.687958227085713
Experience 3, Iter 43, disc loss: 0.1565449831265399, policy loss: 2.6647191820547786
Experience 3, Iter 44, disc loss: 0.14662622706665457, policy loss: 2.708783553071804
Experience 3, Iter 45, disc loss: 0.14895600263610725, policy loss: 2.601759226602396
Experience 3, Iter 46, disc loss: 0.13799929778412603, policy loss: 2.673487647777698
Experience 3, Iter 47, disc loss: 0.131817643503546, policy loss: 2.7376326305336507
Experience 3, Iter 48, disc loss: 0.11669036205356287, policy loss: 2.9036261607421467
Experience 3, Iter 49, disc loss: 0.12228328440480246, policy loss: 2.7348894592779693
Experience 3, Iter 50, disc loss: 0.12300005315839069, policy loss: 2.6942959312295702
Experience 3, Iter 51, disc loss: 0.11682171613520635, policy loss: 2.7162279275585215
Experience 3, Iter 52, disc loss: 0.11456472401802104, policy loss: 2.732063400626586
Experience 3, Iter 53, disc loss: 0.11311931658565158, policy loss: 2.7044408345133633
Experience 3, Iter 54, disc loss: 0.1062187830965298, policy loss: 2.7889711730123015
Experience 3, Iter 55, disc loss: 0.10762631944383055, policy loss: 2.732674394506023
Experience 3, Iter 56, disc loss: 0.10732517876430281, policy loss: 2.7217642825792314
Experience 3, Iter 57, disc loss: 0.10691113209458343, policy loss: 2.684529004284437
Experience 3, Iter 58, disc loss: 0.09461501399597022, policy loss: 2.8788224901024706
Experience 3, Iter 59, disc loss: 0.09983009961494496, policy loss: 2.8871118214001568
Experience 3, Iter 60, disc loss: 0.10090267173420256, policy loss: 2.7655492389773157
Experience 3, Iter 61, disc loss: 0.0970959062904946, policy loss: 2.8521922732484817
Experience 3, Iter 62, disc loss: 0.08898801609754363, policy loss: 2.9551930010071774
Experience 3, Iter 63, disc loss: 0.08915206757735158, policy loss: 2.91610411346928
Experience 3, Iter 64, disc loss: 0.08529757023742399, policy loss: 2.978444015701752
Experience 3, Iter 65, disc loss: 0.08912995025158144, policy loss: 2.8946051583701458
Experience 3, Iter 66, disc loss: 0.08524137642078478, policy loss: 2.9873505447812647
Experience 3, Iter 67, disc loss: 0.08231736593386946, policy loss: 2.9957764506309674
Experience 3, Iter 68, disc loss: 0.07674314196229218, policy loss: 3.190441853284242
Experience 3, Iter 69, disc loss: 0.06941445191891404, policy loss: 3.314331121974999
Experience 3, Iter 70, disc loss: 0.07193583702680613, policy loss: 3.2800312502456803
Experience 3, Iter 71, disc loss: 0.07187754012139026, policy loss: 3.1898713329225723
Experience 3, Iter 72, disc loss: 0.07046023413516585, policy loss: 3.2493601305957625
Experience 3, Iter 73, disc loss: 0.07143336363960995, policy loss: 3.1846669974815733
Experience 3, Iter 74, disc loss: 0.0640000777253712, policy loss: 3.350441398777577
Experience 3, Iter 75, disc loss: 0.06766720629453082, policy loss: 3.2403930592414003
Experience 3, Iter 76, disc loss: 0.0658880173846427, policy loss: 3.375124983735886
Experience 3, Iter 77, disc loss: 0.05997023079015265, policy loss: 3.5166130771402826
Experience 3, Iter 78, disc loss: 0.06305921292201588, policy loss: 3.395028796038138
Experience 3, Iter 79, disc loss: 0.06296864280684464, policy loss: 3.2981252002896415
Experience 3, Iter 80, disc loss: 0.0613620889917848, policy loss: 3.3271560059748873
Experience 3, Iter 81, disc loss: 0.05750121597057096, policy loss: 3.460416363603879
Experience 3, Iter 82, disc loss: 0.05661877492979256, policy loss: 3.4407929084104882
Experience 3, Iter 83, disc loss: 0.05671382449367575, policy loss: 3.4340178140033446
Experience 3, Iter 84, disc loss: 0.057201259828943286, policy loss: 3.464947348416323
Experience 3, Iter 85, disc loss: 0.05547658097209299, policy loss: 3.4671093050603
Experience 3, Iter 86, disc loss: 0.05189655236360296, policy loss: 3.529332851613607
Experience 3, Iter 87, disc loss: 0.05376178528065364, policy loss: 3.482302597681695
Experience 3, Iter 88, disc loss: 0.05053984713164804, policy loss: 3.558984845280073
Experience 3, Iter 89, disc loss: 0.04760080411792746, policy loss: 3.6513354289242015
Experience 3, Iter 90, disc loss: 0.05102136597379197, policy loss: 3.502342752553938
Experience 3, Iter 91, disc loss: 0.051893552004744034, policy loss: 3.4906121355733086
Experience 3, Iter 92, disc loss: 0.04901166014348872, policy loss: 3.551950961893408
Experience 3, Iter 93, disc loss: 0.04723679430172077, policy loss: 3.5968100799365286
Experience 3, Iter 94, disc loss: 0.04330756916034987, policy loss: 3.7137643699144296
Experience 3, Iter 95, disc loss: 0.0504101995911595, policy loss: 3.5059392420957307
Experience 3, Iter 96, disc loss: 0.04651546311048073, policy loss: 3.58482399965536
Experience 3, Iter 97, disc loss: 0.04868106024103276, policy loss: 3.537028271039306
Experience 3, Iter 98, disc loss: 0.044236073250466544, policy loss: 3.751833103857659
Experience 3, Iter 99, disc loss: 0.051048127348820846, policy loss: 3.492728902262275
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0075],
        [0.0659],
        [1.0504],
        [0.0172]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0512, 0.2821, 0.7906, 0.0243, 0.0045, 1.8158]],

        [[0.0512, 0.2821, 0.7906, 0.0243, 0.0045, 1.8158]],

        [[0.0512, 0.2821, 0.7906, 0.0243, 0.0045, 1.8158]],

        [[0.0512, 0.2821, 0.7906, 0.0243, 0.0045, 1.8158]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0302, 0.2637, 4.2015, 0.0687], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0302, 0.2637, 4.2015, 0.0687])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.015
Iter 2/2000 - Loss: 2.734
Iter 3/2000 - Loss: 2.715
Iter 4/2000 - Loss: 2.720
Iter 5/2000 - Loss: 2.690
Iter 6/2000 - Loss: 2.647
Iter 7/2000 - Loss: 2.612
Iter 8/2000 - Loss: 2.611
Iter 9/2000 - Loss: 2.624
Iter 10/2000 - Loss: 2.603
Iter 11/2000 - Loss: 2.545
Iter 12/2000 - Loss: 2.486
Iter 13/2000 - Loss: 2.454
Iter 14/2000 - Loss: 2.442
Iter 15/2000 - Loss: 2.422
Iter 16/2000 - Loss: 2.375
Iter 17/2000 - Loss: 2.307
Iter 18/2000 - Loss: 2.231
Iter 19/2000 - Loss: 2.151
Iter 20/2000 - Loss: 2.064
Iter 1981/2000 - Loss: -5.203
Iter 1982/2000 - Loss: -5.203
Iter 1983/2000 - Loss: -5.203
Iter 1984/2000 - Loss: -5.203
Iter 1985/2000 - Loss: -5.203
Iter 1986/2000 - Loss: -5.203
Iter 1987/2000 - Loss: -5.203
Iter 1988/2000 - Loss: -5.203
Iter 1989/2000 - Loss: -5.203
Iter 1990/2000 - Loss: -5.203
Iter 1991/2000 - Loss: -5.203
Iter 1992/2000 - Loss: -5.203
Iter 1993/2000 - Loss: -5.203
Iter 1994/2000 - Loss: -5.203
Iter 1995/2000 - Loss: -5.203
Iter 1996/2000 - Loss: -5.204
Iter 1997/2000 - Loss: -5.204
Iter 1998/2000 - Loss: -5.204
Iter 1999/2000 - Loss: -5.204
Iter 2000/2000 - Loss: -5.204
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0032],
        [0.0004]])
Lengthscale: tensor([[[19.6701,  6.0294, 49.2437, 17.0242, 13.8493, 33.1037]],

        [[29.3980, 52.8093,  9.8956,  2.1241,  1.9817, 27.6714]],

        [[30.6338, 50.8786, 12.6436,  1.0810,  2.1993, 24.3026]],

        [[29.3863, 54.3138, 15.8550,  2.7350, 14.5274, 34.6701]]])
Signal Variance: tensor([ 0.1044,  2.3685, 19.9271,  0.4543])
Estimated target variance: tensor([0.0302, 0.2637, 4.2015, 0.0687])
N: 40
Signal to noise ratio: tensor([16.9549, 59.2875, 79.1742, 33.1612])
Bound on condition number: tensor([ 11499.7547, 140601.4162, 250742.9415,  43987.5748])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.04145460270046887, policy loss: 3.6484266494880297
Experience 4, Iter 1, disc loss: 0.043230461828087104, policy loss: 3.5821694948690688
Experience 4, Iter 2, disc loss: 0.04908173573223143, policy loss: 3.398911471338913
Experience 4, Iter 3, disc loss: 0.0495482213096277, policy loss: 3.4084707971849637
Experience 4, Iter 4, disc loss: 0.05942244301860696, policy loss: 3.197804943777008
Experience 4, Iter 5, disc loss: 0.08982228228759512, policy loss: 2.8619021367688213
Experience 4, Iter 6, disc loss: 0.1800768618404966, policy loss: 2.2660520199691745
Experience 4, Iter 7, disc loss: 0.1877673146760896, policy loss: 2.6696150271494727
Experience 4, Iter 8, disc loss: 0.151980537893326, policy loss: 2.979943811178109
Experience 4, Iter 9, disc loss: 0.27916968261518943, policy loss: 2.9650036562351474
Experience 4, Iter 10, disc loss: 0.1659444807381417, policy loss: 3.0698205058320474
Experience 4, Iter 11, disc loss: 0.13174887270922755, policy loss: 3.4024083398420206
Experience 4, Iter 12, disc loss: 0.1169097024165604, policy loss: 3.136143991165087
Experience 4, Iter 13, disc loss: 0.08187635130500293, policy loss: 3.5438331966408088
Experience 4, Iter 14, disc loss: 0.08795893532822047, policy loss: 3.593592115556258
Experience 4, Iter 15, disc loss: 0.08192733877610987, policy loss: 3.83932234712353
Experience 4, Iter 16, disc loss: 0.0773059170055096, policy loss: 3.925243414417018
Experience 4, Iter 17, disc loss: 0.07513230126653694, policy loss: 3.8907427515536677
Experience 4, Iter 18, disc loss: 0.08465270568332536, policy loss: 3.6475334113201585
Experience 4, Iter 19, disc loss: 0.0756727198859338, policy loss: 3.9384526220476777
Experience 4, Iter 20, disc loss: 0.09236886444005846, policy loss: 3.4005378971632263
Experience 4, Iter 21, disc loss: 0.07986813273745476, policy loss: 3.6893784024877623
Experience 4, Iter 22, disc loss: 0.06729666554651284, policy loss: 4.102662865761752
Experience 4, Iter 23, disc loss: 0.056506240434638325, policy loss: 4.462310996136397
Experience 4, Iter 24, disc loss: 0.05371651995808374, policy loss: 4.454597421034356
Experience 4, Iter 25, disc loss: 0.04381415898836553, policy loss: 4.906442734448792
Experience 4, Iter 26, disc loss: 0.044196808426068754, policy loss: 4.657777144149472
Experience 4, Iter 27, disc loss: 0.0651973628765217, policy loss: 4.530838698688577
Experience 4, Iter 28, disc loss: 0.039920865950121655, policy loss: 4.48690362443358
Experience 4, Iter 29, disc loss: 0.04004107494027186, policy loss: 4.371871479906325
Experience 4, Iter 30, disc loss: 0.03887984119443055, policy loss: 4.640662214344635
Experience 4, Iter 31, disc loss: 0.024825957905065074, policy loss: 5.799730860131441
Experience 4, Iter 32, disc loss: 0.03713393201964028, policy loss: 4.5390419590124
Experience 4, Iter 33, disc loss: 0.032857107865875645, policy loss: 4.650739285977311
Experience 4, Iter 34, disc loss: 0.028669586155191565, policy loss: 4.418861216304272
Experience 4, Iter 35, disc loss: 0.023679514140631405, policy loss: 4.67412872627298
Experience 4, Iter 36, disc loss: 0.022871936188935214, policy loss: 4.576527183868781
Experience 4, Iter 37, disc loss: 0.025639617623171698, policy loss: 4.285209233989266
Experience 4, Iter 38, disc loss: 0.026265486172578408, policy loss: 4.1745145528641645
Experience 4, Iter 39, disc loss: 0.024830019289809074, policy loss: 4.24918056959887
Experience 4, Iter 40, disc loss: 0.02331667984552033, policy loss: 4.408958002343737
Experience 4, Iter 41, disc loss: 0.023034866443633395, policy loss: 4.339040728117275
Experience 4, Iter 42, disc loss: 0.02324922214140333, policy loss: 4.32491319220138
Experience 4, Iter 43, disc loss: 0.023449054920498872, policy loss: 4.292700470572733
Experience 4, Iter 44, disc loss: 0.021970197063109903, policy loss: 4.391986540459895
Experience 4, Iter 45, disc loss: 0.02333775347152922, policy loss: 4.367235809854089
Experience 4, Iter 46, disc loss: 0.02163564529729599, policy loss: 4.459769241497716
Experience 4, Iter 47, disc loss: 0.02420547818670525, policy loss: 4.221982252555213
Experience 4, Iter 48, disc loss: 0.028808104464512416, policy loss: 3.997253485287413
Experience 4, Iter 49, disc loss: 0.030054833204531672, policy loss: 3.881850879807597
Experience 4, Iter 50, disc loss: 0.029957400342087187, policy loss: 3.918223780874529
Experience 4, Iter 51, disc loss: 0.03030816995816001, policy loss: 3.9000508210274583
Experience 4, Iter 52, disc loss: 0.032269398009101724, policy loss: 3.889541058869801
Experience 4, Iter 53, disc loss: 0.03517698229301214, policy loss: 3.9037429277848106
Experience 4, Iter 54, disc loss: 0.035923031299299774, policy loss: 4.008177591487353
Experience 4, Iter 55, disc loss: 0.043848966375185, policy loss: 4.054878636067171
Experience 4, Iter 56, disc loss: 0.03557444786983174, policy loss: 4.169801878100381
Experience 4, Iter 57, disc loss: 0.035558055172988004, policy loss: 4.005897288276175
Experience 4, Iter 58, disc loss: 0.03482136553636821, policy loss: 4.124208329859285
Experience 4, Iter 59, disc loss: 0.03862216373832082, policy loss: 4.185067918512748
Experience 4, Iter 60, disc loss: 0.029231811561039753, policy loss: 4.290710111292958
Experience 4, Iter 61, disc loss: 0.029123779169178705, policy loss: 4.500229836353774
Experience 4, Iter 62, disc loss: 0.03594141096723641, policy loss: 4.219900366294388
Experience 4, Iter 63, disc loss: 0.027517138550114978, policy loss: 4.366908875702876
Experience 4, Iter 64, disc loss: 0.027545956321309087, policy loss: 4.4101879270692
Experience 4, Iter 65, disc loss: 0.023471701917247734, policy loss: 4.651955283701535
Experience 4, Iter 66, disc loss: 0.025120447455849355, policy loss: 4.530018729830358
Experience 4, Iter 67, disc loss: 0.02261177717486134, policy loss: 4.883644891003049
Experience 4, Iter 68, disc loss: 0.02466645570252722, policy loss: 4.650768861873745
Experience 4, Iter 69, disc loss: 0.02271529896436441, policy loss: 5.0387462858876795
Experience 4, Iter 70, disc loss: 0.018831897736832127, policy loss: 5.142032593221627
Experience 4, Iter 71, disc loss: 0.01734847802487899, policy loss: 5.3670568587951974
Experience 4, Iter 72, disc loss: 0.014608820244435387, policy loss: 5.766010633717238
Experience 4, Iter 73, disc loss: 0.014941824396837434, policy loss: 5.679455804390869
Experience 4, Iter 74, disc loss: 0.016138077712697707, policy loss: 5.49019065186453
Experience 4, Iter 75, disc loss: 0.016052124165062482, policy loss: 5.4543958798649985
Experience 4, Iter 76, disc loss: 0.01605568532928584, policy loss: 5.510927281565872
Experience 4, Iter 77, disc loss: 0.01792582139487181, policy loss: 5.294794584949136
Experience 4, Iter 78, disc loss: 0.028409255391335205, policy loss: 5.0513315061876725
Experience 4, Iter 79, disc loss: 0.02168590642062795, policy loss: 4.916571195164541
Experience 4, Iter 80, disc loss: 0.04029106174614408, policy loss: 4.4073206149047115
Experience 4, Iter 81, disc loss: 0.03283747797371928, policy loss: 4.540182485992355
Experience 4, Iter 82, disc loss: 0.04802036194688643, policy loss: 4.135208233882968
Experience 4, Iter 83, disc loss: 0.03935743665917405, policy loss: 4.117565641585042
Experience 4, Iter 84, disc loss: 0.042706509920795784, policy loss: 3.953825679159597
Experience 4, Iter 85, disc loss: 0.03458514446632376, policy loss: 4.5834412416750805
Experience 4, Iter 86, disc loss: 0.042995921129306634, policy loss: 4.385985925992753
Experience 4, Iter 87, disc loss: 0.04477537128404198, policy loss: 4.081846827367846
Experience 4, Iter 88, disc loss: 0.04595228714703051, policy loss: 3.9902710242973387
Experience 4, Iter 89, disc loss: 0.04706582528215525, policy loss: 4.365154623413622
Experience 4, Iter 90, disc loss: 0.04527554472144732, policy loss: 4.157974621070516
Experience 4, Iter 91, disc loss: 0.048900758821816215, policy loss: 4.058243854098215
Experience 4, Iter 92, disc loss: 0.06153716838830364, policy loss: 4.189713876682898
Experience 4, Iter 93, disc loss: 0.050100094400448755, policy loss: 4.289886002728579
Experience 4, Iter 94, disc loss: 0.05328684008161633, policy loss: 4.810252486369287
Experience 4, Iter 95, disc loss: 0.05288692860485815, policy loss: 4.655038399719561
Experience 4, Iter 96, disc loss: 0.0557466164205857, policy loss: 4.175312918885681
Experience 4, Iter 97, disc loss: 0.042515605492946015, policy loss: 4.68255721958584
Experience 4, Iter 98, disc loss: 0.04335395757347501, policy loss: 4.706334307608504
Experience 4, Iter 99, disc loss: 0.05215956657086933, policy loss: 4.521481058345471
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0074],
        [0.1013],
        [1.1542],
        [0.0238]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0471, 0.2781, 1.0621, 0.0278, 0.0095, 2.9374]],

        [[0.0471, 0.2781, 1.0621, 0.0278, 0.0095, 2.9374]],

        [[0.0471, 0.2781, 1.0621, 0.0278, 0.0095, 2.9374]],

        [[0.0471, 0.2781, 1.0621, 0.0278, 0.0095, 2.9374]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0294, 0.4052, 4.6170, 0.0952], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0294, 0.4052, 4.6170, 0.0952])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.285
Iter 2/2000 - Loss: 3.071
Iter 3/2000 - Loss: 3.106
Iter 4/2000 - Loss: 3.106
Iter 5/2000 - Loss: 3.038
Iter 6/2000 - Loss: 2.988
Iter 7/2000 - Loss: 2.990
Iter 8/2000 - Loss: 2.992
Iter 9/2000 - Loss: 2.946
Iter 10/2000 - Loss: 2.877
Iter 11/2000 - Loss: 2.823
Iter 12/2000 - Loss: 2.791
Iter 13/2000 - Loss: 2.754
Iter 14/2000 - Loss: 2.694
Iter 15/2000 - Loss: 2.610
Iter 16/2000 - Loss: 2.517
Iter 17/2000 - Loss: 2.419
Iter 18/2000 - Loss: 2.314
Iter 19/2000 - Loss: 2.194
Iter 20/2000 - Loss: 2.053
Iter 1981/2000 - Loss: -5.032
Iter 1982/2000 - Loss: -5.032
Iter 1983/2000 - Loss: -5.032
Iter 1984/2000 - Loss: -5.032
Iter 1985/2000 - Loss: -5.032
Iter 1986/2000 - Loss: -5.032
Iter 1987/2000 - Loss: -5.033
Iter 1988/2000 - Loss: -5.033
Iter 1989/2000 - Loss: -5.033
Iter 1990/2000 - Loss: -5.033
Iter 1991/2000 - Loss: -5.033
Iter 1992/2000 - Loss: -5.033
Iter 1993/2000 - Loss: -5.033
Iter 1994/2000 - Loss: -5.033
Iter 1995/2000 - Loss: -5.033
Iter 1996/2000 - Loss: -5.033
Iter 1997/2000 - Loss: -5.033
Iter 1998/2000 - Loss: -5.033
Iter 1999/2000 - Loss: -5.033
Iter 2000/2000 - Loss: -5.033
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0036],
        [0.0005]])
Lengthscale: tensor([[[18.8642,  7.3495, 49.4398, 19.7288, 21.4197, 39.6762]],

        [[24.7956, 44.5073,  8.1435,  1.4267, 10.4339, 27.3471]],

        [[30.5012, 49.3162,  9.4547,  1.1187,  1.2883, 22.7042]],

        [[31.7328, 50.6675, 27.1330,  5.5415,  2.0498, 46.2674]]])
Signal Variance: tensor([ 0.1242,  2.0600, 14.2759,  1.0923])
Estimated target variance: tensor([0.0294, 0.4052, 4.6170, 0.0952])
N: 50
Signal to noise ratio: tensor([18.1199, 58.9445, 62.9878, 46.7787])
Bound on condition number: tensor([ 16417.4806, 173723.4493, 198374.4027, 109413.2964])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.36281629757347983, policy loss: 1.7001503442860872
Experience 5, Iter 1, disc loss: 0.743065051061742, policy loss: 1.0803131947205133
Experience 5, Iter 2, disc loss: 0.5423996252560862, policy loss: 1.4885027767926045
Experience 5, Iter 3, disc loss: 0.4385360441465721, policy loss: 2.00314986281475
Experience 5, Iter 4, disc loss: 0.5910769305170167, policy loss: 1.781863816460378
Experience 5, Iter 5, disc loss: 0.732198473576309, policy loss: 1.7729834011780072
Experience 5, Iter 6, disc loss: 0.5995510622608408, policy loss: 3.4006666331336874
Experience 5, Iter 7, disc loss: 0.8010556437766054, policy loss: 2.574423842355187
Experience 5, Iter 8, disc loss: 0.8189684778399344, policy loss: 2.8805344827903565
Experience 5, Iter 9, disc loss: 0.7363135920011268, policy loss: 3.1226820820851935
Experience 5, Iter 10, disc loss: 0.6340870939653749, policy loss: 2.4747660644885587
Experience 5, Iter 11, disc loss: 0.5560931896547168, policy loss: 1.793636275580458
Experience 5, Iter 12, disc loss: 0.4407465415981793, policy loss: 1.727475986843965
Experience 5, Iter 13, disc loss: 0.3664392662648876, policy loss: 1.8117511928439913
Experience 5, Iter 14, disc loss: 0.44693013958448813, policy loss: 1.3781237520797331
Experience 5, Iter 15, disc loss: 0.43181187882707617, policy loss: 1.432124861482628
Experience 5, Iter 16, disc loss: 0.3914230082678639, policy loss: 1.6180758787957832
Experience 5, Iter 17, disc loss: 0.3319118076529896, policy loss: 1.8395642632733522
Experience 5, Iter 18, disc loss: 0.29058599936315405, policy loss: 2.051274635589576
Experience 5, Iter 19, disc loss: 0.28236069077236586, policy loss: 2.1275679355000943
Experience 5, Iter 20, disc loss: 0.24621243790314656, policy loss: 2.3790023375052662
Experience 5, Iter 21, disc loss: 0.23760975663305237, policy loss: 2.567911969215009
Experience 5, Iter 22, disc loss: 0.2531719091232226, policy loss: 2.503362303378212
Experience 5, Iter 23, disc loss: 0.24892297919358744, policy loss: 2.800715135008627
Experience 5, Iter 24, disc loss: 0.2358455058422621, policy loss: 2.7789597248264206
Experience 5, Iter 25, disc loss: 0.2530523827887259, policy loss: 2.6099233073488657
Experience 5, Iter 26, disc loss: 0.22299390788471704, policy loss: 2.971074462954516
Experience 5, Iter 27, disc loss: 0.23039036045961528, policy loss: 3.0690677775055355
Experience 5, Iter 28, disc loss: 0.22504470365739354, policy loss: 2.9159324693781796
Experience 5, Iter 29, disc loss: 0.2148448062305314, policy loss: 2.994061004882865
Experience 5, Iter 30, disc loss: 0.21580512686863135, policy loss: 2.7440341920726308
Experience 5, Iter 31, disc loss: 0.19647411851530588, policy loss: 3.045488641354186
Experience 5, Iter 32, disc loss: 0.1815955746099321, policy loss: 2.8835633958824896
Experience 5, Iter 33, disc loss: 0.16765101339502225, policy loss: 2.900294186278052
Experience 5, Iter 34, disc loss: 0.15735867847493176, policy loss: 2.9540904685344427
Experience 5, Iter 35, disc loss: 0.15192996706619616, policy loss: 2.838939347504131
Experience 5, Iter 36, disc loss: 0.1575892037379109, policy loss: 2.7098564541175945
Experience 5, Iter 37, disc loss: 0.15601642966860343, policy loss: 2.6507985103038365
Experience 5, Iter 38, disc loss: 0.1482177799277745, policy loss: 2.7753031377494235
Experience 5, Iter 39, disc loss: 0.1571541743973886, policy loss: 2.5203591750407437
Experience 5, Iter 40, disc loss: 0.14296462197253967, policy loss: 2.6847910017170493
Experience 5, Iter 41, disc loss: 0.14929067954275327, policy loss: 2.77558295972976
Experience 5, Iter 42, disc loss: 0.1505676537867234, policy loss: 2.6419556180384065
Experience 5, Iter 43, disc loss: 0.12870847470624944, policy loss: 2.897719310645759
Experience 5, Iter 44, disc loss: 0.1358405371537609, policy loss: 2.6549113311191057
Experience 5, Iter 45, disc loss: 0.12099059839062064, policy loss: 2.9923939458520765
Experience 5, Iter 46, disc loss: 0.11879542214458466, policy loss: 2.965638321206052
Experience 5, Iter 47, disc loss: 0.13123885486756942, policy loss: 2.856797244115002
Experience 5, Iter 48, disc loss: 0.11182372583048891, policy loss: 3.1712350420127726
Experience 5, Iter 49, disc loss: 0.11522965438757192, policy loss: 3.0383945772475314
Experience 5, Iter 50, disc loss: 0.11739794941467667, policy loss: 3.075406863311219
Experience 5, Iter 51, disc loss: 0.11626805499349564, policy loss: 3.0323794371333217
Experience 5, Iter 52, disc loss: 0.12204728611629359, policy loss: 2.9964440049059937
Experience 5, Iter 53, disc loss: 0.11488497366974394, policy loss: 3.268686262866814
Experience 5, Iter 54, disc loss: 0.11849886042733351, policy loss: 3.0746498991370776
Experience 5, Iter 55, disc loss: 0.1163007527312322, policy loss: 3.164290444330395
Experience 5, Iter 56, disc loss: 0.11213732090823372, policy loss: 3.182064734267332
Experience 5, Iter 57, disc loss: 0.11140538072881838, policy loss: 3.2194963010835993
Experience 5, Iter 58, disc loss: 0.11234576033926089, policy loss: 3.1383237886755593
Experience 5, Iter 59, disc loss: 0.10108673601187185, policy loss: 3.257904247516472
Experience 5, Iter 60, disc loss: 0.0988579306731083, policy loss: 3.355898093398279
Experience 5, Iter 61, disc loss: 0.11154497669179744, policy loss: 3.0172075812416823
Experience 5, Iter 62, disc loss: 0.10021375283111081, policy loss: 3.5751174781270194
Experience 5, Iter 63, disc loss: 0.11833750744595918, policy loss: 2.931817409950782
Experience 5, Iter 64, disc loss: 0.11077261353215087, policy loss: 2.985091449758241
Experience 5, Iter 65, disc loss: 0.11056575124605603, policy loss: 3.1362717758656586
Experience 5, Iter 66, disc loss: 0.11431339814288907, policy loss: 3.083205639589889
Experience 5, Iter 67, disc loss: 0.101386666955534, policy loss: 3.6802765524714394
Experience 5, Iter 68, disc loss: 0.11133746777663085, policy loss: 2.9657644526372926
Experience 5, Iter 69, disc loss: 0.08987461919103154, policy loss: 3.426382520876243
Experience 5, Iter 70, disc loss: 0.08603647747000417, policy loss: 3.5228990075575553
Experience 5, Iter 71, disc loss: 0.10326980180076004, policy loss: 3.038295526552375
Experience 5, Iter 72, disc loss: 0.11359574870116314, policy loss: 2.9149155937251883
Experience 5, Iter 73, disc loss: 0.09830727868805894, policy loss: 3.4811339701220723
Experience 5, Iter 74, disc loss: 0.10729307029393981, policy loss: 2.9626656183030695
Experience 5, Iter 75, disc loss: 0.08687520248519329, policy loss: 3.534824078766026
Experience 5, Iter 76, disc loss: 0.09328412926761206, policy loss: 3.2320734836656646
Experience 5, Iter 77, disc loss: 0.08966139718806258, policy loss: 3.29850941600396
Experience 5, Iter 78, disc loss: 0.09367981766217537, policy loss: 3.4136871582478703
Experience 5, Iter 79, disc loss: 0.09749733759276948, policy loss: 3.1758452252826377
Experience 5, Iter 80, disc loss: 0.09649352697481689, policy loss: 3.1070817550158942
Experience 5, Iter 81, disc loss: 0.09950171797404272, policy loss: 3.101744108786311
Experience 5, Iter 82, disc loss: 0.09007394647609071, policy loss: 3.309824366821756
Experience 5, Iter 83, disc loss: 0.09038163209929531, policy loss: 3.25601513661105
Experience 5, Iter 84, disc loss: 0.09675669627709137, policy loss: 3.095904750551954
Experience 5, Iter 85, disc loss: 0.09370325134885998, policy loss: 3.1602046409721813
Experience 5, Iter 86, disc loss: 0.09577481723376614, policy loss: 3.2793441855513854
Experience 5, Iter 87, disc loss: 0.09523157309420402, policy loss: 3.1248599423959984
Experience 5, Iter 88, disc loss: 0.09466505450545684, policy loss: 3.232362672961605
Experience 5, Iter 89, disc loss: 0.09221706211363881, policy loss: 3.2602352062518545
Experience 5, Iter 90, disc loss: 0.09472475633079813, policy loss: 3.1259697162644904
Experience 5, Iter 91, disc loss: 0.09178563069452517, policy loss: 3.3397444966195837
Experience 5, Iter 92, disc loss: 0.09336035142884287, policy loss: 3.2412740026792592
Experience 5, Iter 93, disc loss: 0.09404078512704925, policy loss: 3.18463950547048
Experience 5, Iter 94, disc loss: 0.08978573300609306, policy loss: 3.288313177647132
Experience 5, Iter 95, disc loss: 0.07989915760318117, policy loss: 3.557758863218583
Experience 5, Iter 96, disc loss: 0.08371336688919488, policy loss: 3.4092028008149975
Experience 5, Iter 97, disc loss: 0.09293780592307176, policy loss: 3.1910871885180008
Experience 5, Iter 98, disc loss: 0.0860866123704533, policy loss: 3.5280175649633896
Experience 5, Iter 99, disc loss: 0.08379996231928, policy loss: 3.3254314851050264
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0071],
        [0.1314],
        [1.2499],
        [0.0274]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0399, 0.2650, 1.2290, 0.0293, 0.0150, 3.8092]],

        [[0.0399, 0.2650, 1.2290, 0.0293, 0.0150, 3.8092]],

        [[0.0399, 0.2650, 1.2290, 0.0293, 0.0150, 3.8092]],

        [[0.0399, 0.2650, 1.2290, 0.0293, 0.0150, 3.8092]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0283, 0.5255, 4.9997, 0.1097], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0283, 0.5255, 4.9997, 0.1097])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.454
Iter 2/2000 - Loss: 3.268
Iter 3/2000 - Loss: 3.303
Iter 4/2000 - Loss: 3.281
Iter 5/2000 - Loss: 3.237
Iter 6/2000 - Loss: 3.220
Iter 7/2000 - Loss: 3.201
Iter 8/2000 - Loss: 3.162
Iter 9/2000 - Loss: 3.121
Iter 10/2000 - Loss: 3.084
Iter 11/2000 - Loss: 3.040
Iter 12/2000 - Loss: 2.976
Iter 13/2000 - Loss: 2.891
Iter 14/2000 - Loss: 2.798
Iter 15/2000 - Loss: 2.702
Iter 16/2000 - Loss: 2.600
Iter 17/2000 - Loss: 2.480
Iter 18/2000 - Loss: 2.334
Iter 19/2000 - Loss: 2.168
Iter 20/2000 - Loss: 1.988
Iter 1981/2000 - Loss: -5.286
Iter 1982/2000 - Loss: -5.286
Iter 1983/2000 - Loss: -5.286
Iter 1984/2000 - Loss: -5.286
Iter 1985/2000 - Loss: -5.286
Iter 1986/2000 - Loss: -5.286
Iter 1987/2000 - Loss: -5.286
Iter 1988/2000 - Loss: -5.286
Iter 1989/2000 - Loss: -5.286
Iter 1990/2000 - Loss: -5.286
Iter 1991/2000 - Loss: -5.286
Iter 1992/2000 - Loss: -5.286
Iter 1993/2000 - Loss: -5.287
Iter 1994/2000 - Loss: -5.287
Iter 1995/2000 - Loss: -5.287
Iter 1996/2000 - Loss: -5.287
Iter 1997/2000 - Loss: -5.287
Iter 1998/2000 - Loss: -5.287
Iter 1999/2000 - Loss: -5.287
Iter 2000/2000 - Loss: -5.287
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0032],
        [0.0005]])
Lengthscale: tensor([[[14.4808,  7.1743, 26.0686, 18.5044, 21.3490, 47.3287]],

        [[24.8235, 44.8194,  8.4737,  1.3299, 16.0028, 28.0694]],

        [[23.8393, 45.9723,  9.5861,  1.1448,  1.3729, 23.4386]],

        [[25.0148, 43.8066, 20.7697,  2.8612,  1.9775, 41.8610]]])
Signal Variance: tensor([ 0.1334,  2.0517, 15.6361,  0.6935])
Estimated target variance: tensor([0.0283, 0.5255, 4.9997, 0.1097])
N: 60
Signal to noise ratio: tensor([18.4479, 61.8375, 69.5033, 37.2827])
Bound on condition number: tensor([ 20420.5321, 229433.3504, 289843.4252,  83400.9669])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.08011534722099076, policy loss: 3.448573873032017
Experience 6, Iter 1, disc loss: 0.08628849411965797, policy loss: 3.2198712387459616
Experience 6, Iter 2, disc loss: 0.08888532898054285, policy loss: 3.405657014057799
Experience 6, Iter 3, disc loss: 0.08717646555300305, policy loss: 3.1528180007951656
Experience 6, Iter 4, disc loss: 0.07723172734270312, policy loss: 3.499332423589774
Experience 6, Iter 5, disc loss: 0.07898218844952243, policy loss: 3.3646148670564004
Experience 6, Iter 6, disc loss: 0.09115764885854705, policy loss: 3.0788788652290604
Experience 6, Iter 7, disc loss: 0.07947493135191494, policy loss: 3.551025455031982
Experience 6, Iter 8, disc loss: 0.0709527182086615, policy loss: 3.607323380490686
Experience 6, Iter 9, disc loss: 0.05403087366475968, policy loss: 4.64353459103608
Experience 6, Iter 10, disc loss: 0.05941252171295485, policy loss: 4.034065581853919
Experience 6, Iter 11, disc loss: 0.06999061908730526, policy loss: 3.4575967963918925
Experience 6, Iter 12, disc loss: 0.07074238420080889, policy loss: 3.986267897274959
Experience 6, Iter 13, disc loss: 0.07060446385487022, policy loss: 3.3668400445493045
Experience 6, Iter 14, disc loss: 0.0554070528342156, policy loss: 4.0193711931536855
Experience 6, Iter 15, disc loss: 0.04991971725139276, policy loss: 4.258186620429598
Experience 6, Iter 16, disc loss: 0.0570715542011319, policy loss: 3.8127094315036283
Experience 6, Iter 17, disc loss: 0.07274826137982697, policy loss: 3.2228387825579325
Experience 6, Iter 18, disc loss: 0.05297639276956424, policy loss: 4.875332539078315
Experience 6, Iter 19, disc loss: 0.06316884577518779, policy loss: 3.609790713013465
Experience 6, Iter 20, disc loss: 0.04562343220296348, policy loss: 4.303250553817389
Experience 6, Iter 21, disc loss: 0.036860047606315574, policy loss: 5.09246440001296
Experience 6, Iter 22, disc loss: 0.04258628864999116, policy loss: 4.228482997435751
Experience 6, Iter 23, disc loss: 0.04275523338924676, policy loss: 4.208854032672983
Experience 6, Iter 24, disc loss: 0.05718618024491505, policy loss: 3.6317911784173256
Experience 6, Iter 25, disc loss: 0.06525386025701226, policy loss: 3.659730416537
Experience 6, Iter 26, disc loss: 0.051034062117193774, policy loss: 4.379952498919391
Experience 6, Iter 27, disc loss: 0.0483567784766568, policy loss: 3.9892551897057165
Experience 6, Iter 28, disc loss: 0.03821493261579996, policy loss: 4.741910338485874
Experience 6, Iter 29, disc loss: 0.03578004353809058, policy loss: 4.672143869824395
Experience 6, Iter 30, disc loss: 0.03639838521173401, policy loss: 4.354368656325917
Experience 6, Iter 31, disc loss: 0.0326759942954364, policy loss: 4.559661123664092
Experience 6, Iter 32, disc loss: 0.04403974258045676, policy loss: 3.8875598967811387
Experience 6, Iter 33, disc loss: 0.06910456498647749, policy loss: 3.2055960989774572
Experience 6, Iter 34, disc loss: 0.04812620504901078, policy loss: 4.484720343176956
Experience 6, Iter 35, disc loss: 0.06426767337197266, policy loss: 3.2356910725014125
Experience 6, Iter 36, disc loss: 0.041364169521540574, policy loss: 4.273692312440489
Experience 6, Iter 37, disc loss: 0.04199500750720035, policy loss: 4.281910356725671
Experience 6, Iter 38, disc loss: 0.054055557577770426, policy loss: 3.603080163833991
Experience 6, Iter 39, disc loss: 0.06369613562117471, policy loss: 3.3984618517928054
Experience 6, Iter 40, disc loss: 0.06558188230324241, policy loss: 3.502697357355075
Experience 6, Iter 41, disc loss: 0.06319954694879976, policy loss: 3.4407970968835033
Experience 6, Iter 42, disc loss: 0.05084108202647457, policy loss: 3.74896621513555
Experience 6, Iter 43, disc loss: 0.048227847685032334, policy loss: 3.8830891462403
Experience 6, Iter 44, disc loss: 0.05747594833535721, policy loss: 3.580054893007354
Experience 6, Iter 45, disc loss: 0.06097668765558571, policy loss: 3.4396722851502313
Experience 6, Iter 46, disc loss: 0.06096195627125124, policy loss: 3.540623224410085
Experience 6, Iter 47, disc loss: 0.06509313235194514, policy loss: 3.5493465140757277
Experience 6, Iter 48, disc loss: 0.06329370855331189, policy loss: 3.62032963093745
Experience 6, Iter 49, disc loss: 0.055076907554132706, policy loss: 3.7979223494201912
Experience 6, Iter 50, disc loss: 0.060805644347980844, policy loss: 3.598283097159513
Experience 6, Iter 51, disc loss: 0.059905225271923185, policy loss: 3.625552102581913
Experience 6, Iter 52, disc loss: 0.05998609618602373, policy loss: 3.7181276274011643
Experience 6, Iter 53, disc loss: 0.05816475701397271, policy loss: 3.943934590556797
Experience 6, Iter 54, disc loss: 0.05702122515755037, policy loss: 3.7631907659144934
Experience 6, Iter 55, disc loss: 0.05811103578748798, policy loss: 3.7537680129355175
Experience 6, Iter 56, disc loss: 0.052445464305664616, policy loss: 3.9937569791811036
Experience 6, Iter 57, disc loss: 0.056241457493748495, policy loss: 3.9299328063564896
Experience 6, Iter 58, disc loss: 0.05253166228156209, policy loss: 4.052249057457428
Experience 6, Iter 59, disc loss: 0.06100575863331608, policy loss: 3.581051791292337
Experience 6, Iter 60, disc loss: 0.05918180335655083, policy loss: 3.8968154182089423
Experience 6, Iter 61, disc loss: 0.05655991326761803, policy loss: 3.735628938006607
Experience 6, Iter 62, disc loss: 0.0545116730966001, policy loss: 3.7978153109974566
Experience 6, Iter 63, disc loss: 0.05093757477467226, policy loss: 3.899965705879456
Experience 6, Iter 64, disc loss: 0.05594125822664803, policy loss: 3.651474100175139
Experience 6, Iter 65, disc loss: 0.05561468337780143, policy loss: 3.87679841501748
Experience 6, Iter 66, disc loss: 0.047774676956815726, policy loss: 5.019157172505198
Experience 6, Iter 67, disc loss: 0.04421346968580213, policy loss: 4.337778423434356
Experience 6, Iter 68, disc loss: 0.03063207825185566, policy loss: 6.049274588424931
Experience 6, Iter 69, disc loss: 0.03321087706764628, policy loss: 5.118074642770822
Experience 6, Iter 70, disc loss: 0.03263965921670841, policy loss: 4.737944680507548
Experience 6, Iter 71, disc loss: 0.048065874569879624, policy loss: 3.811068800536895
Experience 6, Iter 72, disc loss: 0.024927358652680538, policy loss: 8.80065429105821
Experience 6, Iter 73, disc loss: 0.029373403605189627, policy loss: 6.196272098962386
Experience 6, Iter 74, disc loss: 0.02947052889318394, policy loss: 5.2646198778867745
Experience 6, Iter 75, disc loss: 0.017564062186168735, policy loss: 9.276918219567646
Experience 6, Iter 76, disc loss: 0.016451478543194082, policy loss: 9.643910020816037
Experience 6, Iter 77, disc loss: 0.01543131219250007, policy loss: 9.399903857408063
Experience 6, Iter 78, disc loss: 0.014341896795992536, policy loss: 9.938575503866861
Experience 6, Iter 79, disc loss: 0.013311902158674387, policy loss: 10.371733443940972
Experience 6, Iter 80, disc loss: 0.012340764835926858, policy loss: 10.627089819565075
Experience 6, Iter 81, disc loss: 0.011432512342421191, policy loss: 10.568054952881667
Experience 6, Iter 82, disc loss: 0.010591968810402705, policy loss: 10.525087754630835
Experience 6, Iter 83, disc loss: 0.009831861061324295, policy loss: 10.097679860773923
Experience 6, Iter 84, disc loss: 0.00911836026116392, policy loss: 9.976022255829633
Experience 6, Iter 85, disc loss: 0.008471296923769536, policy loss: 9.825380694689235
Experience 6, Iter 86, disc loss: 0.007917307163063651, policy loss: 9.36442460367655
Experience 6, Iter 87, disc loss: 0.007372722055634172, policy loss: 9.287753791515799
Experience 6, Iter 88, disc loss: 0.006939458061415954, policy loss: 9.038644383393798
Experience 6, Iter 89, disc loss: 0.006521174289248019, policy loss: 8.993184128985693
Experience 6, Iter 90, disc loss: 0.006141884643484908, policy loss: 8.87445406709802
Experience 6, Iter 91, disc loss: 0.005796265662483929, policy loss: 8.84570485836553
Experience 6, Iter 92, disc loss: 0.0055087152875983115, policy loss: 8.647019533830377
Experience 6, Iter 93, disc loss: 0.0053130649455336585, policy loss: 8.550532075778253
Experience 6, Iter 94, disc loss: 0.005021889218997069, policy loss: 8.418219348891624
Experience 6, Iter 95, disc loss: 0.0048713624859839984, policy loss: 8.109850111087665
Experience 6, Iter 96, disc loss: 0.004737378066147869, policy loss: 7.899771141048879
Experience 6, Iter 97, disc loss: 0.004641406285785227, policy loss: 7.6987411632262805
Experience 6, Iter 98, disc loss: 0.004838303947196883, policy loss: 7.235949927259327
Experience 6, Iter 99, disc loss: 0.004528758613965536, policy loss: 7.400176059316058
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0066],
        [0.1543],
        [1.3607],
        [0.0302]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0351, 0.2512, 1.3563, 0.0306, 0.0198, 4.4523]],

        [[0.0351, 0.2512, 1.3563, 0.0306, 0.0198, 4.4523]],

        [[0.0351, 0.2512, 1.3563, 0.0306, 0.0198, 4.4523]],

        [[0.0351, 0.2512, 1.3563, 0.0306, 0.0198, 4.4523]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0266, 0.6172, 5.4428, 0.1210], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0266, 0.6172, 5.4428, 0.1210])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.594
Iter 2/2000 - Loss: 3.419
Iter 3/2000 - Loss: 3.434
Iter 4/2000 - Loss: 3.403
Iter 5/2000 - Loss: 3.372
Iter 6/2000 - Loss: 3.359
Iter 7/2000 - Loss: 3.320
Iter 8/2000 - Loss: 3.263
Iter 9/2000 - Loss: 3.215
Iter 10/2000 - Loss: 3.177
Iter 11/2000 - Loss: 3.126
Iter 12/2000 - Loss: 3.047
Iter 13/2000 - Loss: 2.946
Iter 14/2000 - Loss: 2.838
Iter 15/2000 - Loss: 2.726
Iter 16/2000 - Loss: 2.602
Iter 17/2000 - Loss: 2.457
Iter 18/2000 - Loss: 2.288
Iter 19/2000 - Loss: 2.101
Iter 20/2000 - Loss: 1.896
Iter 1981/2000 - Loss: -5.488
Iter 1982/2000 - Loss: -5.488
Iter 1983/2000 - Loss: -5.488
Iter 1984/2000 - Loss: -5.488
Iter 1985/2000 - Loss: -5.488
Iter 1986/2000 - Loss: -5.488
Iter 1987/2000 - Loss: -5.488
Iter 1988/2000 - Loss: -5.488
Iter 1989/2000 - Loss: -5.488
Iter 1990/2000 - Loss: -5.488
Iter 1991/2000 - Loss: -5.488
Iter 1992/2000 - Loss: -5.488
Iter 1993/2000 - Loss: -5.488
Iter 1994/2000 - Loss: -5.488
Iter 1995/2000 - Loss: -5.488
Iter 1996/2000 - Loss: -5.488
Iter 1997/2000 - Loss: -5.489
Iter 1998/2000 - Loss: -5.489
Iter 1999/2000 - Loss: -5.489
Iter 2000/2000 - Loss: -5.489
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0037],
        [0.0005]])
Lengthscale: tensor([[[15.1118,  7.7936, 28.4435, 13.9003, 23.2669, 50.5303]],

        [[25.4637, 40.9754,  8.5844,  1.2602, 12.4316, 22.9484]],

        [[26.9825, 43.2290,  9.6959,  1.0756,  1.2332, 28.0007]],

        [[23.6189, 40.6971, 18.9869,  2.0424,  1.9975, 39.4383]]])
Signal Variance: tensor([ 0.1361,  1.8668, 21.3451,  0.5561])
Estimated target variance: tensor([0.0266, 0.6172, 5.4428, 0.1210])
N: 70
Signal to noise ratio: tensor([18.9969, 64.1933, 75.8336, 34.7476])
Bound on condition number: tensor([ 25262.8063, 288455.5885, 402552.1000,  84518.8345])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.004902275531324023, policy loss: 7.0500522129649354
Experience 7, Iter 1, disc loss: 0.004957382698293853, policy loss: 6.835061583260483
Experience 7, Iter 2, disc loss: 0.005264010870424628, policy loss: 6.809210722823092
Experience 7, Iter 3, disc loss: 0.007400665443012175, policy loss: 6.053385121463805
Experience 7, Iter 4, disc loss: 0.008350940216171438, policy loss: 5.971692255410339
Experience 7, Iter 5, disc loss: 0.008310903471606478, policy loss: 5.558172139444294
Experience 7, Iter 6, disc loss: 0.007654952970997828, policy loss: 5.758372263710104
Experience 7, Iter 7, disc loss: 0.012559451285990592, policy loss: 4.970120875974676
Experience 7, Iter 8, disc loss: 0.02773270400683451, policy loss: 4.381582780048001
Experience 7, Iter 9, disc loss: 0.03412686091947804, policy loss: 3.8887411536345797
Experience 7, Iter 10, disc loss: 0.05321608523026334, policy loss: 3.6347486888709892
Experience 7, Iter 11, disc loss: 0.07016924049079719, policy loss: 3.371451680224447
Experience 7, Iter 12, disc loss: 0.06248780590507506, policy loss: 3.9208637589880335
Experience 7, Iter 13, disc loss: 0.059393248007023766, policy loss: 3.8926880242929593
Experience 7, Iter 14, disc loss: 0.048369347031452896, policy loss: 4.290181100522104
Experience 7, Iter 15, disc loss: 0.042930729795042165, policy loss: 4.769889764457096
Experience 7, Iter 16, disc loss: 0.05378485589942523, policy loss: 3.596327756622194
Experience 7, Iter 17, disc loss: 0.046968642099671505, policy loss: 4.011649122989869
Experience 7, Iter 18, disc loss: 0.03950549307697181, policy loss: 4.546870201228495
Experience 7, Iter 19, disc loss: 0.04262734076960634, policy loss: 4.105488482599916
Experience 7, Iter 20, disc loss: 0.03555205667590911, policy loss: 4.016568842343934
Experience 7, Iter 21, disc loss: 0.03186381574595424, policy loss: 4.479744288006334
Experience 7, Iter 22, disc loss: 0.035906756683188705, policy loss: 4.160233642911448
Experience 7, Iter 23, disc loss: 0.030821704793529112, policy loss: 4.656712591020705
Experience 7, Iter 24, disc loss: 0.033223519853806824, policy loss: 4.59035557532716
Experience 7, Iter 25, disc loss: 0.03574005101220914, policy loss: 4.476482323675648
Experience 7, Iter 26, disc loss: 0.03925846157423847, policy loss: 4.212495808786498
Experience 7, Iter 27, disc loss: 0.034985879854172744, policy loss: 5.293617286875966
Experience 7, Iter 28, disc loss: 0.03548560375052475, policy loss: 4.644712583404905
Experience 7, Iter 29, disc loss: 0.03329434532147521, policy loss: 4.9024855898020245
Experience 7, Iter 30, disc loss: 0.03325558494480686, policy loss: 5.017543628789561
Experience 7, Iter 31, disc loss: 0.031749182744741575, policy loss: 5.151769272032201
Experience 7, Iter 32, disc loss: 0.03246453849627604, policy loss: 5.096446953220813
Experience 7, Iter 33, disc loss: 0.03474632152075925, policy loss: 4.87350824622513
Experience 7, Iter 34, disc loss: 0.03482292798764311, policy loss: 4.700966802369819
Experience 7, Iter 35, disc loss: 0.037610668852149906, policy loss: 4.438270206471105
Experience 7, Iter 36, disc loss: 0.036815885730144854, policy loss: 4.649031449972155
Experience 7, Iter 37, disc loss: 0.03696575300633891, policy loss: 4.499626425376357
Experience 7, Iter 38, disc loss: 0.036298996409456594, policy loss: 4.303907512283828
Experience 7, Iter 39, disc loss: 0.034663766392406585, policy loss: 4.333610359590896
Experience 7, Iter 40, disc loss: 0.032098918830650344, policy loss: 4.557280311401916
Experience 7, Iter 41, disc loss: 0.03437246209472579, policy loss: 4.2513664981332635
Experience 7, Iter 42, disc loss: 0.03385213043235318, policy loss: 4.203996300752101
Experience 7, Iter 43, disc loss: 0.033873944971382255, policy loss: 4.187263583814161
Experience 7, Iter 44, disc loss: 0.03492594124810484, policy loss: 4.128191884264398
Experience 7, Iter 45, disc loss: 0.03458750539156276, policy loss: 4.363867440290682
Experience 7, Iter 46, disc loss: 0.040628895595456314, policy loss: 3.8565694459576827
Experience 7, Iter 47, disc loss: 0.04013307875050516, policy loss: 3.8835577557826593
Experience 7, Iter 48, disc loss: 0.03531122217060638, policy loss: 4.18533810229804
Experience 7, Iter 49, disc loss: 0.03410891199649629, policy loss: 4.109381190009447
Experience 7, Iter 50, disc loss: 0.036727725191153256, policy loss: 4.098054713635474
Experience 7, Iter 51, disc loss: 0.0356432976640311, policy loss: 4.043686623417656
Experience 7, Iter 52, disc loss: 0.032861258037528825, policy loss: 4.329289561597557
Experience 7, Iter 53, disc loss: 0.038930277983059355, policy loss: 4.06021529845871
Experience 7, Iter 54, disc loss: 0.038755065099213504, policy loss: 4.036789722066324
Experience 7, Iter 55, disc loss: 0.037467193627658255, policy loss: 4.0321776043497035
Experience 7, Iter 56, disc loss: 0.034665487614856036, policy loss: 4.384239355395421
Experience 7, Iter 57, disc loss: 0.03294457229658335, policy loss: 4.313322155195577
Experience 7, Iter 58, disc loss: 0.031831461781197376, policy loss: 4.350230318457112
Experience 7, Iter 59, disc loss: 0.03521807959787668, policy loss: 4.278518056568227
Experience 7, Iter 60, disc loss: 0.03490191830627245, policy loss: 4.260378654323215
Experience 7, Iter 61, disc loss: 0.03376160557659735, policy loss: 4.497292866652129
Experience 7, Iter 62, disc loss: 0.034563721872200526, policy loss: 4.395238204237331
Experience 7, Iter 63, disc loss: 0.033390119610754096, policy loss: 4.450741739996664
Experience 7, Iter 64, disc loss: 0.03575349286121657, policy loss: 4.32090549876229
Experience 7, Iter 65, disc loss: 0.03442789739915902, policy loss: 4.399039133074893
Experience 7, Iter 66, disc loss: 0.0329347259054041, policy loss: 4.387941221688618
Experience 7, Iter 67, disc loss: 0.03425293706132103, policy loss: 4.256398033315158
Experience 7, Iter 68, disc loss: 0.03203769576851606, policy loss: 4.491139478737896
Experience 7, Iter 69, disc loss: 0.03327177257460655, policy loss: 4.4499562060733115
Experience 7, Iter 70, disc loss: 0.03266884750605613, policy loss: 4.405892453295438
Experience 7, Iter 71, disc loss: 0.033033753685955514, policy loss: 4.352590124204102
Experience 7, Iter 72, disc loss: 0.03181551654620405, policy loss: 4.412088580151723
Experience 7, Iter 73, disc loss: 0.032457273435249366, policy loss: 4.355588584887346
Experience 7, Iter 74, disc loss: 0.03362602544382093, policy loss: 4.351616233773071
Experience 7, Iter 75, disc loss: 0.034791994429753864, policy loss: 4.241082370174414
Experience 7, Iter 76, disc loss: 0.03080941259462043, policy loss: 4.594394356306925
Experience 7, Iter 77, disc loss: 0.03122007029434288, policy loss: 4.372419131009522
Experience 7, Iter 78, disc loss: 0.031479108174546605, policy loss: 4.412838177621786
Experience 7, Iter 79, disc loss: 0.0339901953071972, policy loss: 4.195741951166616
Experience 7, Iter 80, disc loss: 0.032236898593634766, policy loss: 4.339333008349859
Experience 7, Iter 81, disc loss: 0.03220995484828301, policy loss: 4.30860416895689
Experience 7, Iter 82, disc loss: 0.03201705342668658, policy loss: 4.3750111047851785
Experience 7, Iter 83, disc loss: 0.031191935628783474, policy loss: 4.431609833563301
Experience 7, Iter 84, disc loss: 0.03154345659333982, policy loss: 4.439366939796933
Experience 7, Iter 85, disc loss: 0.028851232159477887, policy loss: 4.5331703988823016
Experience 7, Iter 86, disc loss: 0.02983372311330841, policy loss: 4.3896005588755065
Experience 7, Iter 87, disc loss: 0.03006017967116137, policy loss: 4.412619520398893
Experience 7, Iter 88, disc loss: 0.03287458251220181, policy loss: 4.342890075468695
Experience 7, Iter 89, disc loss: 0.029756641717304064, policy loss: 4.543644913733458
Experience 7, Iter 90, disc loss: 0.03081934190248998, policy loss: 4.45461616770463
Experience 7, Iter 91, disc loss: 0.024874455257926493, policy loss: 4.990574640841143
Experience 7, Iter 92, disc loss: 0.022270060344068136, policy loss: 5.492493426816031
Experience 7, Iter 93, disc loss: 0.024026386474842558, policy loss: 4.967530265182539
Experience 7, Iter 94, disc loss: 0.030830015492462587, policy loss: 4.488980561571138
Experience 7, Iter 95, disc loss: 0.03202290008561659, policy loss: 4.254639952593779
Experience 7, Iter 96, disc loss: 0.02130980186507065, policy loss: 6.102030303246488
Experience 7, Iter 97, disc loss: 0.016923531364148384, policy loss: 6.559466969384038
Experience 7, Iter 98, disc loss: 0.012872564785958919, policy loss: 9.854503294285559
Experience 7, Iter 99, disc loss: 0.01242836045594774, policy loss: 10.425087909711376
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0064],
        [0.1702],
        [1.4525],
        [0.0322]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0317, 0.2436, 1.4312, 0.0315, 0.0258, 4.8633]],

        [[0.0317, 0.2436, 1.4312, 0.0315, 0.0258, 4.8633]],

        [[0.0317, 0.2436, 1.4312, 0.0315, 0.0258, 4.8633]],

        [[0.0317, 0.2436, 1.4312, 0.0315, 0.0258, 4.8633]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0255, 0.6806, 5.8099, 0.1286], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0255, 0.6806, 5.8099, 0.1286])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.692
Iter 2/2000 - Loss: 3.537
Iter 3/2000 - Loss: 3.507
Iter 4/2000 - Loss: 3.459
Iter 5/2000 - Loss: 3.457
Iter 6/2000 - Loss: 3.441
Iter 7/2000 - Loss: 3.369
Iter 8/2000 - Loss: 3.286
Iter 9/2000 - Loss: 3.224
Iter 10/2000 - Loss: 3.166
Iter 11/2000 - Loss: 3.087
Iter 12/2000 - Loss: 2.983
Iter 13/2000 - Loss: 2.865
Iter 14/2000 - Loss: 2.736
Iter 15/2000 - Loss: 2.594
Iter 16/2000 - Loss: 2.433
Iter 17/2000 - Loss: 2.256
Iter 18/2000 - Loss: 2.068
Iter 19/2000 - Loss: 1.869
Iter 20/2000 - Loss: 1.651
Iter 1981/2000 - Loss: -5.694
Iter 1982/2000 - Loss: -5.694
Iter 1983/2000 - Loss: -5.694
Iter 1984/2000 - Loss: -5.694
Iter 1985/2000 - Loss: -5.694
Iter 1986/2000 - Loss: -5.694
Iter 1987/2000 - Loss: -5.694
Iter 1988/2000 - Loss: -5.694
Iter 1989/2000 - Loss: -5.694
Iter 1990/2000 - Loss: -5.694
Iter 1991/2000 - Loss: -5.694
Iter 1992/2000 - Loss: -5.694
Iter 1993/2000 - Loss: -5.694
Iter 1994/2000 - Loss: -5.694
Iter 1995/2000 - Loss: -5.694
Iter 1996/2000 - Loss: -5.694
Iter 1997/2000 - Loss: -5.694
Iter 1998/2000 - Loss: -5.695
Iter 1999/2000 - Loss: -5.695
Iter 2000/2000 - Loss: -5.695
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0038],
        [0.0004]])
Lengthscale: tensor([[[14.2189,  7.8945, 16.8531, 13.5617, 20.3292, 53.2711]],

        [[23.4755, 38.8421,  8.6737,  1.2675, 11.5444, 20.9252]],

        [[24.8091, 25.0275, 10.2599,  0.9075,  0.9331, 22.9412]],

        [[21.1503, 37.3853, 18.2890,  1.8851,  1.9734, 41.9339]]])
Signal Variance: tensor([ 0.1239,  1.6873, 14.6167,  0.5088])
Estimated target variance: tensor([0.0255, 0.6806, 5.8099, 0.1286])
N: 80
Signal to noise ratio: tensor([18.9512, 63.0985, 62.1385, 34.0397])
Bound on condition number: tensor([ 28732.7976, 318514.4148, 308896.1624,  92697.1979])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.011931272217080392, policy loss: 10.965421218693482
Experience 8, Iter 1, disc loss: 0.011389258078979347, policy loss: 10.915784624095014
Experience 8, Iter 2, disc loss: 0.01079979398659509, policy loss: 11.279110812656729
Experience 8, Iter 3, disc loss: 0.010190236648056626, policy loss: 12.025028272635218
Experience 8, Iter 4, disc loss: 0.009593099854759639, policy loss: 11.265579030973612
Experience 8, Iter 5, disc loss: 0.008990377092332037, policy loss: 11.62874299945775
Experience 8, Iter 6, disc loss: 0.008414784682613008, policy loss: 11.604547376768275
Experience 8, Iter 7, disc loss: 0.007868699664167618, policy loss: 11.47991344583007
Experience 8, Iter 8, disc loss: 0.007355644011384707, policy loss: 11.323878290897778
Experience 8, Iter 9, disc loss: 0.006877890669784031, policy loss: 11.099769963854266
Experience 8, Iter 10, disc loss: 0.006447618530382514, policy loss: 10.867846935705439
Experience 8, Iter 11, disc loss: 0.006043095410793615, policy loss: 10.630652355976798
Experience 8, Iter 12, disc loss: 0.005676569800638761, policy loss: 10.370371504492445
Experience 8, Iter 13, disc loss: 0.005348076104004485, policy loss: 10.00673700132343
Experience 8, Iter 14, disc loss: 0.005059927827783588, policy loss: 9.593508969367145
Experience 8, Iter 15, disc loss: 0.004752512972146046, policy loss: 9.86523471264686
Experience 8, Iter 16, disc loss: 0.004604566203582188, policy loss: 8.880669665042639
Experience 8, Iter 17, disc loss: 0.0044379701907526995, policy loss: 8.55633788044419
Experience 8, Iter 18, disc loss: 0.004350587037037897, policy loss: 8.25668799315681
Experience 8, Iter 19, disc loss: 0.0041102642690708496, policy loss: 8.41055071047818
Experience 8, Iter 20, disc loss: 0.0042426878874207985, policy loss: 7.685659891095417
Experience 8, Iter 21, disc loss: 0.011045918419892647, policy loss: 5.26928531965601
Experience 8, Iter 22, disc loss: 0.01128020284380496, policy loss: 5.027815119107103
Experience 8, Iter 23, disc loss: 0.012304456227370284, policy loss: 5.649816317746507
Experience 8, Iter 24, disc loss: 0.010916431199031434, policy loss: 6.808534712139432
Experience 8, Iter 25, disc loss: 0.011884520194672475, policy loss: 4.997752081828976
Experience 8, Iter 26, disc loss: 0.01099673713725965, policy loss: 5.238112027057846
Experience 8, Iter 27, disc loss: 0.016054997213126473, policy loss: 4.609381764728134
Experience 8, Iter 28, disc loss: 0.009249657415526698, policy loss: 5.791727614741419
Experience 8, Iter 29, disc loss: 0.021560884635474346, policy loss: 4.391957902455016
Experience 8, Iter 30, disc loss: 0.026614065997760412, policy loss: 4.1433605994135725
Experience 8, Iter 31, disc loss: 0.027387462707352914, policy loss: 4.237788532700863
Experience 8, Iter 32, disc loss: 0.02965758744433765, policy loss: 4.352118912190945
Experience 8, Iter 33, disc loss: 0.03042957993875927, policy loss: 5.064575081903659
Experience 8, Iter 34, disc loss: 0.030361312029147103, policy loss: 4.40099486380855
Experience 8, Iter 35, disc loss: 0.029331767072207377, policy loss: 4.307797750457127
Experience 8, Iter 36, disc loss: 0.020441172923130677, policy loss: 4.823990177749363
Experience 8, Iter 37, disc loss: 0.019035030994769553, policy loss: 5.216061595496638
Experience 8, Iter 38, disc loss: 0.018244364477818693, policy loss: 4.9073046296265215
Experience 8, Iter 39, disc loss: 0.021368849580344382, policy loss: 4.745144116610124
Experience 8, Iter 40, disc loss: 0.022849711929238382, policy loss: 4.484744888872985
Experience 8, Iter 41, disc loss: 0.022793307647422278, policy loss: 4.650467124713844
Experience 8, Iter 42, disc loss: 0.020376194880329632, policy loss: 5.388251481441081
Experience 8, Iter 43, disc loss: 0.021020397321704805, policy loss: 4.728494248932015
Experience 8, Iter 44, disc loss: 0.016828300928518954, policy loss: 5.166531811307707
Experience 8, Iter 45, disc loss: 0.014124441313542799, policy loss: 6.096350207834071
Experience 8, Iter 46, disc loss: 0.01543132390355634, policy loss: 5.611121834358869
Experience 8, Iter 47, disc loss: 0.017893761710293766, policy loss: 5.402647802909009
Experience 8, Iter 48, disc loss: 0.02036297308547723, policy loss: 4.8408677771001845
Experience 8, Iter 49, disc loss: 0.022186643233015405, policy loss: 4.967654325334183
Experience 8, Iter 50, disc loss: 0.020022075577014138, policy loss: 5.569685565715307
Experience 8, Iter 51, disc loss: 0.020985341779549044, policy loss: 5.019296323790391
Experience 8, Iter 52, disc loss: 0.021038268402713937, policy loss: 5.1139413885588745
Experience 8, Iter 53, disc loss: 0.017533579651718267, policy loss: 5.7188536835222425
Experience 8, Iter 54, disc loss: 0.017947985350865166, policy loss: 5.756510090485287
Experience 8, Iter 55, disc loss: 0.017937290132957504, policy loss: 5.419555410012377
Experience 8, Iter 56, disc loss: 0.01848244724454807, policy loss: 5.346901317238935
Experience 8, Iter 57, disc loss: 0.02135491991504629, policy loss: 4.900331121226851
Experience 8, Iter 58, disc loss: 0.022717843519704236, policy loss: 4.90319865234955
Experience 8, Iter 59, disc loss: 0.020700537921603448, policy loss: 5.301559715931161
Experience 8, Iter 60, disc loss: 0.021501048214079204, policy loss: 5.104549929572684
Experience 8, Iter 61, disc loss: 0.021959003863001224, policy loss: 4.898091937216625
Experience 8, Iter 62, disc loss: 0.018292226568557404, policy loss: 5.363801208081086
Experience 8, Iter 63, disc loss: 0.01747071361629872, policy loss: 5.234650211124396
Experience 8, Iter 64, disc loss: 0.01894636610065828, policy loss: 5.022545950107481
Experience 8, Iter 65, disc loss: 0.018888678692649537, policy loss: 5.103400629390908
Experience 8, Iter 66, disc loss: 0.01872298685143422, policy loss: 4.966390473641115
Experience 8, Iter 67, disc loss: 0.021999963398773824, policy loss: 4.812062796716928
Experience 8, Iter 68, disc loss: 0.018541328196851205, policy loss: 4.9469163721813745
Experience 8, Iter 69, disc loss: 0.020456253569219915, policy loss: 5.112990693029692
Experience 8, Iter 70, disc loss: 0.021603272076055315, policy loss: 4.850748093470996
Experience 8, Iter 71, disc loss: 0.02239733595208712, policy loss: 4.612961028872649
Experience 8, Iter 72, disc loss: 0.020794986929125354, policy loss: 4.829050551117897
Experience 8, Iter 73, disc loss: 0.020752767883641388, policy loss: 5.017720494170152
Experience 8, Iter 74, disc loss: 0.0203492881366391, policy loss: 4.8462479572154145
Experience 8, Iter 75, disc loss: 0.01990734696821779, policy loss: 4.780550174966081
Experience 8, Iter 76, disc loss: 0.01930734027227198, policy loss: 5.133409670830394
Experience 8, Iter 77, disc loss: 0.023437971144299196, policy loss: 4.761167659098791
Experience 8, Iter 78, disc loss: 0.017504524736042488, policy loss: 5.215648928348267
Experience 8, Iter 79, disc loss: 0.01741719537855882, policy loss: 5.223851850298859
Experience 8, Iter 80, disc loss: 0.01703182875866633, policy loss: 5.2280985301948215
Experience 8, Iter 81, disc loss: 0.014311989521428772, policy loss: 5.717354780340977
Experience 8, Iter 82, disc loss: 0.0174907178974634, policy loss: 5.273794367529582
Experience 8, Iter 83, disc loss: 0.016878260408284856, policy loss: 5.392212858256921
Experience 8, Iter 84, disc loss: 0.01702353109094938, policy loss: 5.100955400697862
Experience 8, Iter 85, disc loss: 0.020586694471962437, policy loss: 4.8233324441795435
Experience 8, Iter 86, disc loss: 0.02079035021862416, policy loss: 4.751022157646004
Experience 8, Iter 87, disc loss: 0.020124748666465214, policy loss: 4.938770503405577
Experience 8, Iter 88, disc loss: 0.018822294248117002, policy loss: 5.028163975741982
Experience 8, Iter 89, disc loss: 0.020559597361690364, policy loss: 4.8572109700353385
Experience 8, Iter 90, disc loss: 0.019279446178226186, policy loss: 4.781271895225194
Experience 8, Iter 91, disc loss: 0.01883383132640496, policy loss: 4.922669064469153
Experience 8, Iter 92, disc loss: 0.018087789050284804, policy loss: 5.104852792457974
Experience 8, Iter 93, disc loss: 0.01898539777353886, policy loss: 4.904236211847026
Experience 8, Iter 94, disc loss: 0.019546257071065592, policy loss: 4.968807197934662
Experience 8, Iter 95, disc loss: 0.019306772204139275, policy loss: 4.995710514870897
Experience 8, Iter 96, disc loss: 0.019108108044777694, policy loss: 4.943639558621565
Experience 8, Iter 97, disc loss: 0.018833908271434358, policy loss: 4.926934702116194
Experience 8, Iter 98, disc loss: 0.020566022690856153, policy loss: 4.664803028347935
Experience 8, Iter 99, disc loss: 0.018675402319395625, policy loss: 5.144539719647836
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0061],
        [0.1789],
        [1.4719],
        [0.0324]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0286, 0.2333, 1.4542, 0.0319, 0.0271, 5.1605]],

        [[0.0286, 0.2333, 1.4542, 0.0319, 0.0271, 5.1605]],

        [[0.0286, 0.2333, 1.4542, 0.0319, 0.0271, 5.1605]],

        [[0.0286, 0.2333, 1.4542, 0.0319, 0.0271, 5.1605]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0242, 0.7157, 5.8875, 0.1295], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0242, 0.7157, 5.8875, 0.1295])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.677
Iter 2/2000 - Loss: 3.532
Iter 3/2000 - Loss: 3.464
Iter 4/2000 - Loss: 3.387
Iter 5/2000 - Loss: 3.372
Iter 6/2000 - Loss: 3.333
Iter 7/2000 - Loss: 3.233
Iter 8/2000 - Loss: 3.125
Iter 9/2000 - Loss: 3.033
Iter 10/2000 - Loss: 2.939
Iter 11/2000 - Loss: 2.826
Iter 12/2000 - Loss: 2.696
Iter 13/2000 - Loss: 2.555
Iter 14/2000 - Loss: 2.400
Iter 15/2000 - Loss: 2.228
Iter 16/2000 - Loss: 2.038
Iter 17/2000 - Loss: 1.836
Iter 18/2000 - Loss: 1.626
Iter 19/2000 - Loss: 1.405
Iter 20/2000 - Loss: 1.168
Iter 1981/2000 - Loss: -6.026
Iter 1982/2000 - Loss: -6.026
Iter 1983/2000 - Loss: -6.026
Iter 1984/2000 - Loss: -6.026
Iter 1985/2000 - Loss: -6.026
Iter 1986/2000 - Loss: -6.026
Iter 1987/2000 - Loss: -6.026
Iter 1988/2000 - Loss: -6.026
Iter 1989/2000 - Loss: -6.026
Iter 1990/2000 - Loss: -6.026
Iter 1991/2000 - Loss: -6.026
Iter 1992/2000 - Loss: -6.026
Iter 1993/2000 - Loss: -6.026
Iter 1994/2000 - Loss: -6.026
Iter 1995/2000 - Loss: -6.026
Iter 1996/2000 - Loss: -6.026
Iter 1997/2000 - Loss: -6.026
Iter 1998/2000 - Loss: -6.026
Iter 1999/2000 - Loss: -6.026
Iter 2000/2000 - Loss: -6.027
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0033],
        [0.0004]])
Lengthscale: tensor([[[14.7393,  8.0260, 15.3223, 12.8041, 19.3342, 53.9461]],

        [[21.0483, 33.8480,  8.9505,  1.1246,  4.9778, 16.8740]],

        [[23.7417, 21.2510, 10.1080,  0.9038,  0.9376, 23.5494]],

        [[20.1570, 35.2956, 19.2877,  1.8944,  1.9673, 43.7816]]])
Signal Variance: tensor([ 0.1257,  1.3585, 14.8152,  0.5512])
Estimated target variance: tensor([0.0242, 0.7157, 5.8875, 0.1295])
N: 90
Signal to noise ratio: tensor([19.7929, 59.1260, 66.9767, 37.1640])
Bound on condition number: tensor([ 35259.2488, 314630.8529, 403730.0594, 124305.7493])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.01984844429349343, policy loss: 4.810376660288481
Experience 9, Iter 1, disc loss: 0.019029462669193906, policy loss: 4.9630873771461035
Experience 9, Iter 2, disc loss: 0.018285936047318412, policy loss: 4.993470545615665
Experience 9, Iter 3, disc loss: 0.018984258606999073, policy loss: 5.01499742968864
Experience 9, Iter 4, disc loss: 0.01880830409293561, policy loss: 5.153493423646725
Experience 9, Iter 5, disc loss: 0.02075230673040393, policy loss: 4.9382437572665
Experience 9, Iter 6, disc loss: 0.01883164065039413, policy loss: 5.036718504798223
Experience 9, Iter 7, disc loss: 0.018621182643210634, policy loss: 5.1971064305122905
Experience 9, Iter 8, disc loss: 0.018650936326477102, policy loss: 5.130912122883494
Experience 9, Iter 9, disc loss: 0.018332833011148332, policy loss: 5.045765107313855
Experience 9, Iter 10, disc loss: 0.015464917649030532, policy loss: 5.751328846460445
Experience 9, Iter 11, disc loss: 0.01478710351750041, policy loss: 5.569809315379077
Experience 9, Iter 12, disc loss: 0.017065800877429226, policy loss: 5.220723656710534
Experience 9, Iter 13, disc loss: 0.01928720884655631, policy loss: 4.914351000615409
Experience 9, Iter 14, disc loss: 0.0176701115699055, policy loss: 5.2432898265961505
Experience 9, Iter 15, disc loss: 0.01574667952938538, policy loss: 5.692884498674994
Experience 9, Iter 16, disc loss: 0.01845413406611071, policy loss: 5.039059840960005
Experience 9, Iter 17, disc loss: 0.013375794043366623, policy loss: 5.921407190334274
Experience 9, Iter 18, disc loss: 0.011559925836131146, policy loss: 5.9469055106618045
Experience 9, Iter 19, disc loss: 0.012842871353781626, policy loss: 6.089991425390965
Experience 9, Iter 20, disc loss: 0.012917512166908401, policy loss: 5.596550484560983
Experience 9, Iter 21, disc loss: 0.01511779652755809, policy loss: 5.258818853122584
Experience 9, Iter 22, disc loss: 0.015909433246666962, policy loss: 5.175798009069032
Experience 9, Iter 23, disc loss: 0.009638546922923872, policy loss: 8.092568817608496
Experience 9, Iter 24, disc loss: 0.008629384286804442, policy loss: 8.175109703047532
Experience 9, Iter 25, disc loss: 0.007312128065015312, policy loss: 9.390228910144296
Experience 9, Iter 26, disc loss: 0.006951942522148634, policy loss: 9.800566174349662
Experience 9, Iter 27, disc loss: 0.006595947040044141, policy loss: 12.236148104164993
Experience 9, Iter 28, disc loss: 0.0063132974025390375, policy loss: 14.968902044291685
Experience 9, Iter 29, disc loss: 0.006031331664931251, policy loss: 15.873027795028063
Experience 9, Iter 30, disc loss: 0.00575085948721784, policy loss: 15.908683775011971
Experience 9, Iter 31, disc loss: 0.005482467510018573, policy loss: 15.40041693122716
Experience 9, Iter 32, disc loss: 0.005193314710166733, policy loss: 15.532982872251178
Experience 9, Iter 33, disc loss: 0.004930338043108351, policy loss: 16.13893939422086
Experience 9, Iter 34, disc loss: 0.004682813406699353, policy loss: 16.412988246596967
Experience 9, Iter 35, disc loss: 0.004429308548984066, policy loss: 16.29843412661921
Experience 9, Iter 36, disc loss: 0.004205206733068217, policy loss: 15.589863111791885
Experience 9, Iter 37, disc loss: 0.003987976292363868, policy loss: 15.585145151026719
Experience 9, Iter 38, disc loss: 0.0037841097748587977, policy loss: 15.492152941869204
Experience 9, Iter 39, disc loss: 0.00359645026696126, policy loss: 14.986522225557092
Experience 9, Iter 40, disc loss: 0.003419845880717532, policy loss: 15.566626628353001
Experience 9, Iter 41, disc loss: 0.00325814410712051, policy loss: 14.972791373288906
Experience 9, Iter 42, disc loss: 0.0031059085900879977, policy loss: 14.93537551447978
Experience 9, Iter 43, disc loss: 0.0029638616344872266, policy loss: 14.766625179878234
Experience 9, Iter 44, disc loss: 0.002832996104499858, policy loss: 14.636816349300407
Experience 9, Iter 45, disc loss: 0.00270967928477622, policy loss: 14.460214624498967
Experience 9, Iter 46, disc loss: 0.002603573804264571, policy loss: 14.463335656944471
Experience 9, Iter 47, disc loss: 0.0024995876088932936, policy loss: 13.959120480527833
Experience 9, Iter 48, disc loss: 0.002403990936573152, policy loss: 13.95812896594807
Experience 9, Iter 49, disc loss: 0.0023126469324204076, policy loss: 13.503110414871127
Experience 9, Iter 50, disc loss: 0.002249006553140534, policy loss: 13.347465170810109
Experience 9, Iter 51, disc loss: 0.002158404035400839, policy loss: 12.529116865135286
Experience 9, Iter 52, disc loss: 0.002090357021093791, policy loss: 12.144377119388418
Experience 9, Iter 53, disc loss: 0.002026200665455447, policy loss: 11.690225758981942
Experience 9, Iter 54, disc loss: 0.001970039710910185, policy loss: 11.180529448436197
Experience 9, Iter 55, disc loss: 0.0019344569736132113, policy loss: 10.475213703717293
Experience 9, Iter 56, disc loss: 0.0019153231598508108, policy loss: 9.929358027076482
Experience 9, Iter 57, disc loss: 0.002007170706443374, policy loss: 9.602005668038311
Experience 9, Iter 58, disc loss: 0.0021172950230124573, policy loss: 9.424680865487527
Experience 9, Iter 59, disc loss: 0.0037177247989787264, policy loss: 6.793502730039833
Experience 9, Iter 60, disc loss: 0.003433330252173116, policy loss: 7.6982065655272685
Experience 9, Iter 61, disc loss: 0.0044319435692711994, policy loss: 7.503962657094435
Experience 9, Iter 62, disc loss: 0.005863121548642247, policy loss: 7.343845361166716
Experience 9, Iter 63, disc loss: 0.004793935329837732, policy loss: 6.647913316159073
Experience 9, Iter 64, disc loss: 0.022635200891071915, policy loss: 5.044982923193336
Experience 9, Iter 65, disc loss: 0.2599803890311012, policy loss: 2.8273838671545044
Experience 9, Iter 66, disc loss: 0.3958469483038558, policy loss: 2.2530063869023644
Experience 9, Iter 67, disc loss: 0.8557666682083671, policy loss: 1.395608276321639
Experience 9, Iter 68, disc loss: 2.336942319259306, policy loss: 0.25859565529164785
Experience 9, Iter 69, disc loss: 3.2498686576822977, policy loss: 0.08434399290147111
Experience 9, Iter 70, disc loss: 2.9946736057217875, policy loss: 0.41677086553617604
Experience 9, Iter 71, disc loss: 2.1815581522214287, policy loss: 0.7551553129818634
Experience 9, Iter 72, disc loss: 2.397423798790718, policy loss: 0.27079313067188837
Experience 9, Iter 73, disc loss: 0.3418190810464131, policy loss: 2.113057537521623
Experience 9, Iter 74, disc loss: 0.03944189877332918, policy loss: 4.850490938814573
Experience 9, Iter 75, disc loss: 0.04076255129849403, policy loss: 5.545454369759978
Experience 9, Iter 76, disc loss: 0.04863709275114136, policy loss: 6.031098507248775
Experience 9, Iter 77, disc loss: 0.0587752477157081, policy loss: 6.353490697092371
Experience 9, Iter 78, disc loss: 0.07248512850736903, policy loss: 5.832313971650476
Experience 9, Iter 79, disc loss: 0.10155834877942194, policy loss: 4.330260672671298
Experience 9, Iter 80, disc loss: 0.16673915545691748, policy loss: 3.008087005563245
Experience 9, Iter 81, disc loss: 0.34964682492540206, policy loss: 2.079378788856264
Experience 9, Iter 82, disc loss: 0.6936749827113201, policy loss: 1.363050340016133
Experience 9, Iter 83, disc loss: 0.9240217632211742, policy loss: 1.0400205473672295
Experience 9, Iter 84, disc loss: 0.6883076482941352, policy loss: 1.4334784859410397
Experience 9, Iter 85, disc loss: 0.7149571236905148, policy loss: 1.3439444005938508
Experience 9, Iter 86, disc loss: 0.7495167036110336, policy loss: 1.429709125479451
Experience 9, Iter 87, disc loss: 0.5714597175841724, policy loss: 1.92929653646992
Experience 9, Iter 88, disc loss: 0.3305346574483239, policy loss: 2.4910256521813396
Experience 9, Iter 89, disc loss: 0.13746430553507266, policy loss: 4.3802150074737956
Experience 9, Iter 90, disc loss: 0.10490093250963244, policy loss: 5.718685029383811
Experience 9, Iter 91, disc loss: 0.09229468088575636, policy loss: 7.096432099135896
Experience 9, Iter 92, disc loss: 0.07991929483576873, policy loss: 7.767571484820717
Experience 9, Iter 93, disc loss: 0.06729454411941925, policy loss: 7.82378965513008
Experience 9, Iter 94, disc loss: 0.05556407467776397, policy loss: 7.55668617249341
Experience 9, Iter 95, disc loss: 0.0459302238239498, policy loss: 6.739695445188855
Experience 9, Iter 96, disc loss: 0.03953905886921451, policy loss: 5.852735707428424
Experience 9, Iter 97, disc loss: 0.03883652878292976, policy loss: 5.007691472443561
Experience 9, Iter 98, disc loss: 0.041214363365513255, policy loss: 4.809679657746896
Experience 9, Iter 99, disc loss: 0.03979894810149072, policy loss: 4.714826899395053
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.1902],
        [1.6093],
        [0.0354]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0330, 0.2339, 1.5969, 0.0310, 0.0249, 5.2007]],

        [[0.0330, 0.2339, 1.5969, 0.0310, 0.0249, 5.2007]],

        [[0.0330, 0.2339, 1.5969, 0.0310, 0.0249, 5.2007]],

        [[0.0330, 0.2339, 1.5969, 0.0310, 0.0249, 5.2007]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0238, 0.7609, 6.4373, 0.1416], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0238, 0.7609, 6.4373, 0.1416])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.770
Iter 2/2000 - Loss: 3.600
Iter 3/2000 - Loss: 3.591
Iter 4/2000 - Loss: 3.524
Iter 5/2000 - Loss: 3.475
Iter 6/2000 - Loss: 3.440
Iter 7/2000 - Loss: 3.361
Iter 8/2000 - Loss: 3.258
Iter 9/2000 - Loss: 3.167
Iter 10/2000 - Loss: 3.085
Iter 11/2000 - Loss: 2.987
Iter 12/2000 - Loss: 2.860
Iter 13/2000 - Loss: 2.714
Iter 14/2000 - Loss: 2.561
Iter 15/2000 - Loss: 2.404
Iter 16/2000 - Loss: 2.236
Iter 17/2000 - Loss: 2.049
Iter 18/2000 - Loss: 1.845
Iter 19/2000 - Loss: 1.626
Iter 20/2000 - Loss: 1.393
Iter 1981/2000 - Loss: -6.004
Iter 1982/2000 - Loss: -6.004
Iter 1983/2000 - Loss: -6.004
Iter 1984/2000 - Loss: -6.004
Iter 1985/2000 - Loss: -6.004
Iter 1986/2000 - Loss: -6.004
Iter 1987/2000 - Loss: -6.005
Iter 1988/2000 - Loss: -6.005
Iter 1989/2000 - Loss: -6.005
Iter 1990/2000 - Loss: -6.005
Iter 1991/2000 - Loss: -6.005
Iter 1992/2000 - Loss: -6.005
Iter 1993/2000 - Loss: -6.005
Iter 1994/2000 - Loss: -6.005
Iter 1995/2000 - Loss: -6.005
Iter 1996/2000 - Loss: -6.005
Iter 1997/2000 - Loss: -6.005
Iter 1998/2000 - Loss: -6.005
Iter 1999/2000 - Loss: -6.005
Iter 2000/2000 - Loss: -6.005
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0030],
        [0.0004]])
Lengthscale: tensor([[[14.4575,  8.6562, 16.5865, 12.8540, 12.7580, 52.8115]],

        [[23.6765, 35.5918,  8.2656,  1.3739,  3.8472, 17.6850]],

        [[24.7791, 20.7426,  9.6407,  0.9569,  0.9227, 24.1546]],

        [[21.0018, 35.9365, 15.0510,  1.2058,  4.4426, 43.3846]]])
Signal Variance: tensor([ 0.1295,  1.7304, 15.6847,  0.5386])
Estimated target variance: tensor([0.0238, 0.7609, 6.4373, 0.1416])
N: 100
Signal to noise ratio: tensor([20.5296, 63.0351, 72.0513, 38.7048])
Bound on condition number: tensor([ 42147.4172, 397343.1547, 519139.4139, 149806.7952])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.01768223338249224, policy loss: 7.307662134823964
Experience 10, Iter 1, disc loss: 0.014804316304495694, policy loss: 6.991671735175329
Experience 10, Iter 2, disc loss: 0.01418208726157384, policy loss: 6.510089354935529
Experience 10, Iter 3, disc loss: 0.03368935475310325, policy loss: 4.992914891415307
Experience 10, Iter 4, disc loss: 0.03038459703949939, policy loss: 4.970598427897943
Experience 10, Iter 5, disc loss: 0.01863663839649009, policy loss: 4.900670583417547
Experience 10, Iter 6, disc loss: 0.00936241864821405, policy loss: 6.3254939168024045
Experience 10, Iter 7, disc loss: 0.006899456412613904, policy loss: 7.476024865308343
Experience 10, Iter 8, disc loss: 0.005859854786644766, policy loss: 8.406506749353316
Experience 10, Iter 9, disc loss: 0.005327441572715055, policy loss: 8.948105354919395
Experience 10, Iter 10, disc loss: 0.005032575104266707, policy loss: 8.786033145846485
Experience 10, Iter 11, disc loss: 0.004764404027580873, policy loss: 8.70916068464787
Experience 10, Iter 12, disc loss: 0.00472219305765946, policy loss: 8.155834082946818
Experience 10, Iter 13, disc loss: 0.005140285772687788, policy loss: 7.15066305175418
Experience 10, Iter 14, disc loss: 0.006198803567570755, policy loss: 6.403084702839241
Experience 10, Iter 15, disc loss: 0.007266801557748614, policy loss: 5.903448677903505
Experience 10, Iter 16, disc loss: 0.009475797031542861, policy loss: 5.516765175952111
Experience 10, Iter 17, disc loss: 0.022059317099240576, policy loss: 4.690912896763727
Experience 10, Iter 18, disc loss: 0.018460712689917586, policy loss: 5.334717191693703
Experience 10, Iter 19, disc loss: 0.017494004160934242, policy loss: 5.764841838433643
Experience 10, Iter 20, disc loss: 0.012198436153467811, policy loss: 6.726227404302637
Experience 10, Iter 21, disc loss: 0.011846252304841093, policy loss: 6.663067985036795
Experience 10, Iter 22, disc loss: 0.013582166778159543, policy loss: 5.706682518596145
Experience 10, Iter 23, disc loss: 0.015010925445589757, policy loss: 5.414032873990038
Experience 10, Iter 24, disc loss: 0.016719392693207328, policy loss: 5.083741096706579
Experience 10, Iter 25, disc loss: 0.014020398094930154, policy loss: 5.088044898921611
Experience 10, Iter 26, disc loss: 0.014408052833362524, policy loss: 4.965456197087673
Experience 10, Iter 27, disc loss: 0.013866283429786762, policy loss: 5.231842056528874
Experience 10, Iter 28, disc loss: 0.014581758518799181, policy loss: 5.0265891202044575
Experience 10, Iter 29, disc loss: 0.012757721480117135, policy loss: 5.410501918776146
Experience 10, Iter 30, disc loss: 0.013220151067566329, policy loss: 5.04635748245592
Experience 10, Iter 31, disc loss: 0.010561654661532728, policy loss: 5.317901123970129
Experience 10, Iter 32, disc loss: 0.007223615552948858, policy loss: 5.77437032703493
Experience 10, Iter 33, disc loss: 0.005294767730665145, policy loss: 6.272930045657917
Experience 10, Iter 34, disc loss: 0.004113630489982551, policy loss: 6.90390486780535
Experience 10, Iter 35, disc loss: 0.003565186064871061, policy loss: 7.633682505800124
Experience 10, Iter 36, disc loss: 0.003375562314118825, policy loss: 7.8583820924602215
Experience 10, Iter 37, disc loss: 0.0031603604541577445, policy loss: 8.201829702925718
Experience 10, Iter 38, disc loss: 0.0031377923276831136, policy loss: 8.247020298370208
Experience 10, Iter 39, disc loss: 0.0031123678795370547, policy loss: 8.258930622949027
Experience 10, Iter 40, disc loss: 0.0034113125042728215, policy loss: 7.876558423685507
Experience 10, Iter 41, disc loss: 0.004049930471754512, policy loss: 7.253093459701783
Experience 10, Iter 42, disc loss: 0.0053785402587644415, policy loss: 7.243270803076612
Experience 10, Iter 43, disc loss: 0.0062937253429379, policy loss: 6.841052630100046
Experience 10, Iter 44, disc loss: 0.009392379694642013, policy loss: 5.811865737786165
Experience 10, Iter 45, disc loss: 0.01138495856724776, policy loss: 5.853950942354629
Experience 10, Iter 46, disc loss: 0.0110592259152429, policy loss: 5.996556766545852
Experience 10, Iter 47, disc loss: 0.010189365401103522, policy loss: 6.1773826058139605
Experience 10, Iter 48, disc loss: 0.015452853687641585, policy loss: 5.916928277456546
Experience 10, Iter 49, disc loss: 0.013769908819361357, policy loss: 5.473526040972931
Experience 10, Iter 50, disc loss: 0.022519265380090376, policy loss: 5.3892902562783425
Experience 10, Iter 51, disc loss: 0.02428133679446027, policy loss: 5.026127330298358
Experience 10, Iter 52, disc loss: 0.021348526679824512, policy loss: 4.971524176326108
Experience 10, Iter 53, disc loss: 0.021350552043732724, policy loss: 4.725083666021677
Experience 10, Iter 54, disc loss: 0.02346761788165414, policy loss: 4.445672477113845
Experience 10, Iter 55, disc loss: 0.029390289566151154, policy loss: 4.360317245715715
Experience 10, Iter 56, disc loss: 0.036528109303496925, policy loss: 4.1732173385028855
Experience 10, Iter 57, disc loss: 0.037055761285454965, policy loss: 4.167393554829115
Experience 10, Iter 58, disc loss: 0.031959121344843085, policy loss: 4.652888309533692
Experience 10, Iter 59, disc loss: 0.023991549436815522, policy loss: 4.660515202078093
Experience 10, Iter 60, disc loss: 0.038123620249011204, policy loss: 4.305400906624463
Experience 10, Iter 61, disc loss: 0.022864135089343002, policy loss: 9.399434434589764
Experience 10, Iter 62, disc loss: 0.002903586583196987, policy loss: 12.134397796849312
Experience 10, Iter 63, disc loss: 0.0035015946383922312, policy loss: 11.285249279205662
Experience 10, Iter 64, disc loss: 0.004463047276745629, policy loss: 10.617841873132262
Experience 10, Iter 65, disc loss: 0.4466839279849596, policy loss: 5.969751482572203
Experience 10, Iter 66, disc loss: 0.7778231234386924, policy loss: 2.2835559595265185
Experience 10, Iter 67, disc loss: 0.45012670297512186, policy loss: 3.3643820049656137
Experience 10, Iter 68, disc loss: 0.07621820526897925, policy loss: 6.742931677034935
Experience 10, Iter 69, disc loss: 0.03183839715834818, policy loss: 8.866633425786159
Experience 10, Iter 70, disc loss: 0.011465150452801047, policy loss: 10.434414848964826
Experience 10, Iter 71, disc loss: 0.008963389387582637, policy loss: 14.86358183051261
Experience 10, Iter 72, disc loss: 0.011463540813831797, policy loss: 11.62507249358454
Experience 10, Iter 73, disc loss: 0.013608016272830667, policy loss: 11.271454243683298
Experience 10, Iter 74, disc loss: 0.015117223215318696, policy loss: 12.123832885313652
Experience 10, Iter 75, disc loss: 0.017022904664759142, policy loss: 10.9746797119033
Experience 10, Iter 76, disc loss: 0.018428004081577305, policy loss: 11.587467040525457
Experience 10, Iter 77, disc loss: 0.019718561139804747, policy loss: 13.209089824382072
Experience 10, Iter 78, disc loss: 0.02068284171243046, policy loss: 15.131953419288067
Experience 10, Iter 79, disc loss: 0.02143276819288397, policy loss: 16.12429001715202
Experience 10, Iter 80, disc loss: 0.02182139108970856, policy loss: 16.976084259805063
Experience 10, Iter 81, disc loss: 0.021862181800348077, policy loss: 17.598129397490133
Experience 10, Iter 82, disc loss: 0.02158815331327545, policy loss: 18.782576347285186
Experience 10, Iter 83, disc loss: 0.021046021274862256, policy loss: 18.98657834962441
Experience 10, Iter 84, disc loss: 0.020284612944999954, policy loss: 19.5999330771126
Experience 10, Iter 85, disc loss: 0.019363933214804663, policy loss: 19.5322981965788
Experience 10, Iter 86, disc loss: 0.01833377749515346, policy loss: 19.070088695576256
Experience 10, Iter 87, disc loss: 0.01724201243118996, policy loss: 19.160520098334516
Experience 10, Iter 88, disc loss: 0.01613187002523546, policy loss: 19.562834330696596
Experience 10, Iter 89, disc loss: 0.015034350960707807, policy loss: 20.484771323570016
Experience 10, Iter 90, disc loss: 0.013976023077914127, policy loss: 19.289264373515067
Experience 10, Iter 91, disc loss: 0.012975433922123183, policy loss: 20.192870695076333
Experience 10, Iter 92, disc loss: 0.012042888939526744, policy loss: 18.142647705549606
Experience 10, Iter 93, disc loss: 0.011188127734789987, policy loss: 18.77862319897482
Experience 10, Iter 94, disc loss: 0.010409017599494063, policy loss: 17.817253559297654
Experience 10, Iter 95, disc loss: 0.009699093041471367, policy loss: 17.31698885472907
Experience 10, Iter 96, disc loss: 0.009059530556350712, policy loss: 15.82747481563596
Experience 10, Iter 97, disc loss: 0.00848269829115422, policy loss: 15.429650912943767
Experience 10, Iter 98, disc loss: 0.007965213385457201, policy loss: 16.07006535878824
Experience 10, Iter 99, disc loss: 0.007495977190249125, policy loss: 15.470725705787558
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0062],
        [0.1853],
        [1.6682],
        [0.0380]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0374, 0.2484, 1.7225, 0.0343, 0.0235, 5.0253]],

        [[0.0374, 0.2484, 1.7225, 0.0343, 0.0235, 5.0253]],

        [[0.0374, 0.2484, 1.7225, 0.0343, 0.0235, 5.0253]],

        [[0.0374, 0.2484, 1.7225, 0.0343, 0.0235, 5.0253]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0249, 0.7414, 6.6728, 0.1522], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0249, 0.7414, 6.6728, 0.1522])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.790
Iter 2/2000 - Loss: 3.673
Iter 3/2000 - Loss: 3.608
Iter 4/2000 - Loss: 3.549
Iter 5/2000 - Loss: 3.531
Iter 6/2000 - Loss: 3.469
Iter 7/2000 - Loss: 3.368
Iter 8/2000 - Loss: 3.280
Iter 9/2000 - Loss: 3.204
Iter 10/2000 - Loss: 3.113
Iter 11/2000 - Loss: 2.999
Iter 12/2000 - Loss: 2.873
Iter 13/2000 - Loss: 2.739
Iter 14/2000 - Loss: 2.592
Iter 15/2000 - Loss: 2.431
Iter 16/2000 - Loss: 2.255
Iter 17/2000 - Loss: 2.069
Iter 18/2000 - Loss: 1.874
Iter 19/2000 - Loss: 1.664
Iter 20/2000 - Loss: 1.436
Iter 1981/2000 - Loss: -5.966
Iter 1982/2000 - Loss: -5.966
Iter 1983/2000 - Loss: -5.967
Iter 1984/2000 - Loss: -5.967
Iter 1985/2000 - Loss: -5.967
Iter 1986/2000 - Loss: -5.967
Iter 1987/2000 - Loss: -5.967
Iter 1988/2000 - Loss: -5.967
Iter 1989/2000 - Loss: -5.967
Iter 1990/2000 - Loss: -5.967
Iter 1991/2000 - Loss: -5.967
Iter 1992/2000 - Loss: -5.967
Iter 1993/2000 - Loss: -5.967
Iter 1994/2000 - Loss: -5.967
Iter 1995/2000 - Loss: -5.967
Iter 1996/2000 - Loss: -5.967
Iter 1997/2000 - Loss: -5.967
Iter 1998/2000 - Loss: -5.967
Iter 1999/2000 - Loss: -5.967
Iter 2000/2000 - Loss: -5.967
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[16.7686,  9.6546, 17.5796, 13.6877,  6.2150, 58.5887]],

        [[23.4417, 35.2169,  7.4255,  1.4693,  2.0221, 25.4439]],

        [[24.5737,  8.4807, 10.2255,  1.0235,  0.9734, 25.1237]],

        [[21.6121, 35.1505, 22.3848,  2.3578,  2.2996, 39.8001]]])
Signal Variance: tensor([ 0.1611,  2.0863, 15.7068,  0.8079])
Estimated target variance: tensor([0.0249, 0.7414, 6.6728, 0.1522])
N: 110
Signal to noise ratio: tensor([23.5339, 70.5434, 77.4177, 45.2271])
Bound on condition number: tensor([ 60924.0155, 547401.8139, 659286.6287, 225004.8067])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.0070696256633290796, policy loss: 13.849046864708036
Experience 11, Iter 1, disc loss: 0.0066841217398862455, policy loss: 13.629438425379345
Experience 11, Iter 2, disc loss: 0.006336725049299619, policy loss: 13.275680184652217
Experience 11, Iter 3, disc loss: 0.00602277319536903, policy loss: 12.750037576859993
Experience 11, Iter 4, disc loss: 0.005738814815362926, policy loss: 12.372601633558112
Experience 11, Iter 5, disc loss: 0.00548348657044833, policy loss: 11.823307083457959
Experience 11, Iter 6, disc loss: 0.005253172083228797, policy loss: 11.592670183163445
Experience 11, Iter 7, disc loss: 0.005045826078413121, policy loss: 11.172712549700908
Experience 11, Iter 8, disc loss: 0.004850868544403666, policy loss: 11.139854374873728
Experience 11, Iter 9, disc loss: 0.004677861401422106, policy loss: 11.147272909619616
Experience 11, Iter 10, disc loss: 0.004513210584390776, policy loss: 11.238979572191493
Experience 11, Iter 11, disc loss: 0.004370259104640395, policy loss: 11.030768083970209
Experience 11, Iter 12, disc loss: 0.0042319260460702566, policy loss: 11.203336645627417
Experience 11, Iter 13, disc loss: 0.00411362922374261, policy loss: 11.109052900450838
Experience 11, Iter 14, disc loss: 0.003991383137333156, policy loss: 11.3343760036373
Experience 11, Iter 15, disc loss: 0.0038878423873544674, policy loss: 11.40032688481821
Experience 11, Iter 16, disc loss: 0.0038021325489330056, policy loss: 10.755994690065322
Experience 11, Iter 17, disc loss: 0.0037096544055295957, policy loss: 10.884953450083117
Experience 11, Iter 18, disc loss: 0.0036396795831253885, policy loss: 10.406761112667356
Experience 11, Iter 19, disc loss: 0.0035547853966032997, policy loss: 10.906458657457229
Experience 11, Iter 20, disc loss: 0.0034900463554653574, policy loss: 10.54963365677005
Experience 11, Iter 21, disc loss: 0.003418766576785997, policy loss: 10.453910802307654
Experience 11, Iter 22, disc loss: 0.0033688514826269492, policy loss: 10.32414723740205
Experience 11, Iter 23, disc loss: 0.0033070211983000264, policy loss: 10.16412224744846
Experience 11, Iter 24, disc loss: 0.003262541847344402, policy loss: 9.903775325488795
Experience 11, Iter 25, disc loss: 0.003203939458492119, policy loss: 9.9589895742909
Experience 11, Iter 26, disc loss: 0.003174488018657085, policy loss: 9.776563743577736
Experience 11, Iter 27, disc loss: 0.0031372960418289625, policy loss: 9.679542089985969
Experience 11, Iter 28, disc loss: 0.003092158030446297, policy loss: 9.717128168632748
Experience 11, Iter 29, disc loss: 0.0030729877730878248, policy loss: 9.55330780781749
Experience 11, Iter 30, disc loss: 0.0030092316557309273, policy loss: 9.69990477159022
Experience 11, Iter 31, disc loss: 0.0029903278147852103, policy loss: 9.48244385062097
Experience 11, Iter 32, disc loss: 0.0029546634667075534, policy loss: 9.304312670236156
Experience 11, Iter 33, disc loss: 0.0029310431225196883, policy loss: 9.455629751957007
Experience 11, Iter 34, disc loss: 0.002877911071211313, policy loss: 9.7397907705185
Experience 11, Iter 35, disc loss: 0.002874856032490118, policy loss: 9.386563457737482
Experience 11, Iter 36, disc loss: 0.0028547861206848603, policy loss: 9.329537423849743
Experience 11, Iter 37, disc loss: 0.0028499788784643043, policy loss: 9.126738371736463
Experience 11, Iter 38, disc loss: 0.002853521364597262, policy loss: 8.96868877295153
Experience 11, Iter 39, disc loss: 0.0027858068773516523, policy loss: 9.245412189710422
Experience 11, Iter 40, disc loss: 0.002806916221674866, policy loss: 8.856877762227056
Experience 11, Iter 41, disc loss: 0.002754254824056215, policy loss: 9.04339502958984
Experience 11, Iter 42, disc loss: 0.002749905196220017, policy loss: 8.979170808109995
Experience 11, Iter 43, disc loss: 0.0027595774528512444, policy loss: 9.273525715916763
Experience 11, Iter 44, disc loss: 0.002691710541915436, policy loss: 9.154601950114051
Experience 11, Iter 45, disc loss: 0.0026667622920660172, policy loss: 9.38180120447721
Experience 11, Iter 46, disc loss: 0.0026696565767494298, policy loss: 9.098636875988984
Experience 11, Iter 47, disc loss: 0.002680764428902769, policy loss: 9.06871241680805
Experience 11, Iter 48, disc loss: 0.002611835558638198, policy loss: 9.117216214465074
Experience 11, Iter 49, disc loss: 0.0026395359564889073, policy loss: 8.815413015808623
Experience 11, Iter 50, disc loss: 0.002633483754594819, policy loss: 8.566456910756472
Experience 11, Iter 51, disc loss: 0.0026090759475380547, policy loss: 8.711624750161443
Experience 11, Iter 52, disc loss: 0.002568926091501434, policy loss: 8.752425397035388
Experience 11, Iter 53, disc loss: 0.0025100275691584445, policy loss: 9.104937062566389
Experience 11, Iter 54, disc loss: 0.0025380235751541764, policy loss: 9.118782132219588
Experience 11, Iter 55, disc loss: 0.002544860589648307, policy loss: 8.733201327138762
Experience 11, Iter 56, disc loss: 0.0025312329726345246, policy loss: 8.74797648750039
Experience 11, Iter 57, disc loss: 0.002537616922734233, policy loss: 8.69246024116767
Experience 11, Iter 58, disc loss: 0.0026190264997674913, policy loss: 8.172651907122091
Experience 11, Iter 59, disc loss: 0.0024868515779189063, policy loss: 8.839761236264867
Experience 11, Iter 60, disc loss: 0.0025113307117479956, policy loss: 8.544472458313395
Experience 11, Iter 61, disc loss: 0.002447288507705469, policy loss: 8.88143768852078
Experience 11, Iter 62, disc loss: 0.002516374471964563, policy loss: 8.357913599502222
Experience 11, Iter 63, disc loss: 0.002502076192767697, policy loss: 8.632284580808594
Experience 11, Iter 64, disc loss: 0.002564990897394652, policy loss: 8.389106252859513
Experience 11, Iter 65, disc loss: 0.0024811060797330304, policy loss: 8.635590691010425
Experience 11, Iter 66, disc loss: 0.0024534186109161525, policy loss: 8.614352703620376
Experience 11, Iter 67, disc loss: 0.002425033961053038, policy loss: 8.55284888757635
Experience 11, Iter 68, disc loss: 0.002435946544222347, policy loss: 8.530727904992073
Experience 11, Iter 69, disc loss: 0.0024287073918663298, policy loss: 8.387866027156232
Experience 11, Iter 70, disc loss: 0.002391633287192356, policy loss: 8.261571208414953
Experience 11, Iter 71, disc loss: 0.0023768055019644605, policy loss: 8.650567488243018
Experience 11, Iter 72, disc loss: 0.0024073284635839055, policy loss: 8.251113353756338
Experience 11, Iter 73, disc loss: 0.0024381173021993598, policy loss: 8.255821971413067
Experience 11, Iter 74, disc loss: 0.002453404608584571, policy loss: 8.143939242494497
Experience 11, Iter 75, disc loss: 0.002371641576777106, policy loss: 8.252573077417285
Experience 11, Iter 76, disc loss: 0.0023612303405419134, policy loss: 8.484473148852198
Experience 11, Iter 77, disc loss: 0.002369089049202025, policy loss: 8.567904428092218
Experience 11, Iter 78, disc loss: 0.0024231291923348364, policy loss: 8.221051350482304
Experience 11, Iter 79, disc loss: 0.0024420296064945873, policy loss: 8.016743704384785
Experience 11, Iter 80, disc loss: 0.002313682124886124, policy loss: 8.36632779018732
Experience 11, Iter 81, disc loss: 0.0023056581039099817, policy loss: 8.343624950735958
Experience 11, Iter 82, disc loss: 0.002361977476900157, policy loss: 8.209962643392604
Experience 11, Iter 83, disc loss: 0.0024242997471341492, policy loss: 7.981891286868615
Experience 11, Iter 84, disc loss: 0.0023563850693892195, policy loss: 8.105897900450296
Experience 11, Iter 85, disc loss: 0.0022826724685342608, policy loss: 8.300655104687529
Experience 11, Iter 86, disc loss: 0.0023565821245056778, policy loss: 7.926313527701109
Experience 11, Iter 87, disc loss: 0.002287491704552715, policy loss: 8.1853606310498
Experience 11, Iter 88, disc loss: 0.002263429281456358, policy loss: 8.339032546503807
Experience 11, Iter 89, disc loss: 0.002338964683797387, policy loss: 8.032788508024103
Experience 11, Iter 90, disc loss: 0.002321197585133785, policy loss: 8.113247600389776
Experience 11, Iter 91, disc loss: 0.0022308598387812597, policy loss: 8.464042040105678
Experience 11, Iter 92, disc loss: 0.0023044279605132206, policy loss: 7.991012452757188
Experience 11, Iter 93, disc loss: 0.0022596020483921647, policy loss: 8.115797455640479
Experience 11, Iter 94, disc loss: 0.0022965020041258916, policy loss: 8.084322439092755
Experience 11, Iter 95, disc loss: 0.002251916264228951, policy loss: 8.141434971412764
Experience 11, Iter 96, disc loss: 0.0023813563858967094, policy loss: 7.777913055685
Experience 11, Iter 97, disc loss: 0.002297760064371609, policy loss: 8.036256418223257
Experience 11, Iter 98, disc loss: 0.0022674518819373137, policy loss: 8.08214885016363
Experience 11, Iter 99, disc loss: 0.002369571412007294, policy loss: 7.85412390096961
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0060],
        [0.1760],
        [1.6598],
        [0.0371]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0352, 0.2368, 1.6859, 0.0345, 0.0223, 4.7604]],

        [[0.0352, 0.2368, 1.6859, 0.0345, 0.0223, 4.7604]],

        [[0.0352, 0.2368, 1.6859, 0.0345, 0.0223, 4.7604]],

        [[0.0352, 0.2368, 1.6859, 0.0345, 0.0223, 4.7604]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0239, 0.7040, 6.6392, 0.1485], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0239, 0.7040, 6.6392, 0.1485])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.721
Iter 2/2000 - Loss: 3.612
Iter 3/2000 - Loss: 3.533
Iter 4/2000 - Loss: 3.477
Iter 5/2000 - Loss: 3.460
Iter 6/2000 - Loss: 3.386
Iter 7/2000 - Loss: 3.281
Iter 8/2000 - Loss: 3.193
Iter 9/2000 - Loss: 3.111
Iter 10/2000 - Loss: 3.009
Iter 11/2000 - Loss: 2.886
Iter 12/2000 - Loss: 2.754
Iter 13/2000 - Loss: 2.612
Iter 14/2000 - Loss: 2.457
Iter 15/2000 - Loss: 2.285
Iter 16/2000 - Loss: 2.101
Iter 17/2000 - Loss: 1.911
Iter 18/2000 - Loss: 1.713
Iter 19/2000 - Loss: 1.500
Iter 20/2000 - Loss: 1.268
Iter 1981/2000 - Loss: -6.187
Iter 1982/2000 - Loss: -6.188
Iter 1983/2000 - Loss: -6.188
Iter 1984/2000 - Loss: -6.188
Iter 1985/2000 - Loss: -6.188
Iter 1986/2000 - Loss: -6.188
Iter 1987/2000 - Loss: -6.188
Iter 1988/2000 - Loss: -6.188
Iter 1989/2000 - Loss: -6.188
Iter 1990/2000 - Loss: -6.188
Iter 1991/2000 - Loss: -6.188
Iter 1992/2000 - Loss: -6.188
Iter 1993/2000 - Loss: -6.188
Iter 1994/2000 - Loss: -6.188
Iter 1995/2000 - Loss: -6.188
Iter 1996/2000 - Loss: -6.188
Iter 1997/2000 - Loss: -6.188
Iter 1998/2000 - Loss: -6.188
Iter 1999/2000 - Loss: -6.188
Iter 2000/2000 - Loss: -6.188
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[15.9961,  9.5035, 17.2214, 13.2131,  7.0658, 58.2407]],

        [[22.2465, 33.2806,  7.3044,  1.3761,  2.4377, 20.7626]],

        [[23.8485,  9.9722,  9.6005,  1.0312,  1.0156, 25.3339]],

        [[20.9267, 35.4058, 14.1618,  1.2468,  3.9274, 41.8677]]])
Signal Variance: tensor([ 0.1602,  1.7796, 15.7325,  0.4984])
Estimated target variance: tensor([0.0239, 0.7040, 6.6392, 0.1485])
N: 120
Signal to noise ratio: tensor([23.4561, 68.3114, 72.9459, 38.7737])
Bound on condition number: tensor([ 66023.4581, 559974.8744, 638532.6982, 180408.5544])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.0022558101797508947, policy loss: 8.069493194841066
Experience 12, Iter 1, disc loss: 0.002273235949371352, policy loss: 7.878276984880459
Experience 12, Iter 2, disc loss: 0.002381277827784847, policy loss: 7.691894444740642
Experience 12, Iter 3, disc loss: 0.002345687491943297, policy loss: 7.873195237302793
Experience 12, Iter 4, disc loss: 0.002265058186183057, policy loss: 8.023428667482811
Experience 12, Iter 5, disc loss: 0.0023149661576867156, policy loss: 7.7558971701109805
Experience 12, Iter 6, disc loss: 0.0022894879004839797, policy loss: 7.692590199025208
Experience 12, Iter 7, disc loss: 0.0021758571172500914, policy loss: 8.079559642050768
Experience 12, Iter 8, disc loss: 0.0022882643251012327, policy loss: 7.6883593299720925
Experience 12, Iter 9, disc loss: 0.0022572212738000163, policy loss: 7.897450081727176
Experience 12, Iter 10, disc loss: 0.0022648509439330768, policy loss: 7.755117018280709
Experience 12, Iter 11, disc loss: 0.002206147454254106, policy loss: 7.821083569569764
Experience 12, Iter 12, disc loss: 0.002178179984846418, policy loss: 8.128183735542516
Experience 12, Iter 13, disc loss: 0.002275593900401221, policy loss: 7.68729698989949
Experience 12, Iter 14, disc loss: 0.002192985323461996, policy loss: 7.906233961582283
Experience 12, Iter 15, disc loss: 0.00231823237292823, policy loss: 7.670658719254925
Experience 12, Iter 16, disc loss: 0.002236194103995767, policy loss: 7.8888372928769295
Experience 12, Iter 17, disc loss: 0.002279521416145298, policy loss: 7.611383626854182
Experience 12, Iter 18, disc loss: 0.002289732308518461, policy loss: 7.652484730774014
Experience 12, Iter 19, disc loss: 0.002198854349605479, policy loss: 8.078409131767982
Experience 12, Iter 20, disc loss: 0.002163587486910303, policy loss: 7.894833141333695
Experience 12, Iter 21, disc loss: 0.002294703985636947, policy loss: 7.681381511527204
Experience 12, Iter 22, disc loss: 0.002189298403244941, policy loss: 7.679175464004395
Experience 12, Iter 23, disc loss: 0.0020767263771749333, policy loss: 8.141003562146288
Experience 12, Iter 24, disc loss: 0.0021624789093257313, policy loss: 7.726626082271858
Experience 12, Iter 25, disc loss: 0.002095921146453765, policy loss: 7.9904263202194645
Experience 12, Iter 26, disc loss: 0.002126898033495698, policy loss: 8.074686644278803
Experience 12, Iter 27, disc loss: 0.0021972364179956893, policy loss: 7.66978818859229
Experience 12, Iter 28, disc loss: 0.002186499271122554, policy loss: 7.7834923646231875
Experience 12, Iter 29, disc loss: 0.0021186782458814802, policy loss: 7.818287544997297
Experience 12, Iter 30, disc loss: 0.002153797385957841, policy loss: 7.890656973171897
Experience 12, Iter 31, disc loss: 0.00216651833518, policy loss: 7.720375978165098
Experience 12, Iter 32, disc loss: 0.002179676717017477, policy loss: 7.645509968528333
Experience 12, Iter 33, disc loss: 0.002154982933066077, policy loss: 7.733228854694362
Experience 12, Iter 34, disc loss: 0.002077838615326737, policy loss: 7.891217432092153
Experience 12, Iter 35, disc loss: 0.002165097863469002, policy loss: 7.633143143786046
Experience 12, Iter 36, disc loss: 0.002100466969991488, policy loss: 7.88358626268889
Experience 12, Iter 37, disc loss: 0.0021065454482133055, policy loss: 7.78179026363302
Experience 12, Iter 38, disc loss: 0.0021835883264481725, policy loss: 7.6477998921746
Experience 12, Iter 39, disc loss: 0.002125249809102012, policy loss: 7.769055597008107
Experience 12, Iter 40, disc loss: 0.0022771022782467846, policy loss: 7.421883178942148
Experience 12, Iter 41, disc loss: 0.0021500557365975105, policy loss: 7.7698592720209785
Experience 12, Iter 42, disc loss: 0.0021656869038783684, policy loss: 7.767465511059742
Experience 12, Iter 43, disc loss: 0.0022380807448467207, policy loss: 7.618693386471975
Experience 12, Iter 44, disc loss: 0.0021668212913540466, policy loss: 7.6863856162818465
Experience 12, Iter 45, disc loss: 0.002072258721189762, policy loss: 7.665456906145954
Experience 12, Iter 46, disc loss: 0.0021933570161422507, policy loss: 7.473699138349826
Experience 12, Iter 47, disc loss: 0.002378623276936201, policy loss: 7.347365129859957
Experience 12, Iter 48, disc loss: 0.0021882207995976614, policy loss: 7.668704667119077
Experience 12, Iter 49, disc loss: 0.0021949114450983244, policy loss: 7.584481561579181
Experience 12, Iter 50, disc loss: 0.002373474802476471, policy loss: 7.30611887014895
Experience 12, Iter 51, disc loss: 0.002194050346364918, policy loss: 7.544559396313794
Experience 12, Iter 52, disc loss: 0.0023042670941384167, policy loss: 7.42481280606731
Experience 12, Iter 53, disc loss: 0.0027289770987764654, policy loss: 7.003817861582048
Experience 12, Iter 54, disc loss: 0.002540794037807198, policy loss: 7.247563444035223
Experience 12, Iter 55, disc loss: 0.0025470960039668095, policy loss: 7.16619270226212
Experience 12, Iter 56, disc loss: 0.0026881488412729025, policy loss: 6.993983102809563
Experience 12, Iter 57, disc loss: 0.002792122934643711, policy loss: 6.949187696708532
Experience 12, Iter 58, disc loss: 0.0026341877099094434, policy loss: 7.166242061462818
Experience 12, Iter 59, disc loss: 0.0029232245021921597, policy loss: 6.908460440066142
Experience 12, Iter 60, disc loss: 0.0026724581840027774, policy loss: 7.047104331345869
Experience 12, Iter 61, disc loss: 0.00290056285601639, policy loss: 7.0678995418455495
Experience 12, Iter 62, disc loss: 0.002881362207579738, policy loss: 6.7686190165537
Experience 12, Iter 63, disc loss: 0.0027825126521562518, policy loss: 6.9252657868539185
Experience 12, Iter 64, disc loss: 0.0029726064132662277, policy loss: 6.871962864218841
Experience 12, Iter 65, disc loss: 0.0028181856435293097, policy loss: 6.9353634112261355
Experience 12, Iter 66, disc loss: 0.002963366203800811, policy loss: 6.873551326576547
Experience 12, Iter 67, disc loss: 0.00309192647170818, policy loss: 6.73626519571469
Experience 12, Iter 68, disc loss: 0.0029374172753855697, policy loss: 6.930012228627044
Experience 12, Iter 69, disc loss: 0.002657368668127139, policy loss: 7.079939020731109
Experience 12, Iter 70, disc loss: 0.00286245552407599, policy loss: 6.998682826637408
Experience 12, Iter 71, disc loss: 0.002767752161625247, policy loss: 6.908762590790163
Experience 12, Iter 72, disc loss: 0.0029161590682279643, policy loss: 6.923208816000054
Experience 12, Iter 73, disc loss: 0.0029937668764837906, policy loss: 6.816408369954506
Experience 12, Iter 74, disc loss: 0.002673898312066917, policy loss: 7.030335090355872
Experience 12, Iter 75, disc loss: 0.002984000478835365, policy loss: 6.705810889423635
Experience 12, Iter 76, disc loss: 0.0028547664259210253, policy loss: 6.909704631951306
Experience 12, Iter 77, disc loss: 0.0029525294944819187, policy loss: 6.763244979239996
Experience 12, Iter 78, disc loss: 0.0029431086692731584, policy loss: 6.8442975826531995
Experience 12, Iter 79, disc loss: 0.0027918068640696196, policy loss: 6.980857435484417
Experience 12, Iter 80, disc loss: 0.0028872253917558398, policy loss: 6.862808930524119
Experience 12, Iter 81, disc loss: 0.002896452151737052, policy loss: 6.863936285241477
Experience 12, Iter 82, disc loss: 0.0029618256045352634, policy loss: 6.747931980096583
Experience 12, Iter 83, disc loss: 0.0030894749057265446, policy loss: 6.836170440414766
Experience 12, Iter 84, disc loss: 0.0029030401803856786, policy loss: 6.778213537313123
Experience 12, Iter 85, disc loss: 0.0027946193145383684, policy loss: 6.986830167736605
Experience 12, Iter 86, disc loss: 0.0027658750937910818, policy loss: 6.959586573652723
Experience 12, Iter 87, disc loss: 0.002938058759182639, policy loss: 6.852615542421607
Experience 12, Iter 88, disc loss: 0.0029549308873712153, policy loss: 6.694779542578835
Experience 12, Iter 89, disc loss: 0.0031544144007807466, policy loss: 6.4900572332973105
Experience 12, Iter 90, disc loss: 0.00299096633743377, policy loss: 6.742955812906676
Experience 12, Iter 91, disc loss: 0.0029674616933155666, policy loss: 6.734666531053767
Experience 12, Iter 92, disc loss: 0.0031945150077596508, policy loss: 6.5892711808659445
Experience 12, Iter 93, disc loss: 0.0029528743453142887, policy loss: 6.830741524553563
Experience 12, Iter 94, disc loss: 0.0029279511026240493, policy loss: 6.846689882071685
Experience 12, Iter 95, disc loss: 0.003042702584379882, policy loss: 6.763719921350571
Experience 12, Iter 96, disc loss: 0.003016921077279667, policy loss: 6.79901029436537
Experience 12, Iter 97, disc loss: 0.0028267187666385515, policy loss: 7.030340198466182
Experience 12, Iter 98, disc loss: 0.003092645814588797, policy loss: 6.836178913966425
Experience 12, Iter 99, disc loss: 0.0033829303333602775, policy loss: 6.544456918537906
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.1710],
        [1.6717],
        [0.0362]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0333, 0.2279, 1.6454, 0.0347, 0.0214, 4.6062]],

        [[0.0333, 0.2279, 1.6454, 0.0347, 0.0214, 4.6062]],

        [[0.0333, 0.2279, 1.6454, 0.0347, 0.0214, 4.6062]],

        [[0.0333, 0.2279, 1.6454, 0.0347, 0.0214, 4.6062]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0229, 0.6840, 6.6870, 0.1450], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0229, 0.6840, 6.6870, 0.1450])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.663
Iter 2/2000 - Loss: 3.555
Iter 3/2000 - Loss: 3.459
Iter 4/2000 - Loss: 3.399
Iter 5/2000 - Loss: 3.376
Iter 6/2000 - Loss: 3.289
Iter 7/2000 - Loss: 3.175
Iter 8/2000 - Loss: 3.080
Iter 9/2000 - Loss: 2.988
Iter 10/2000 - Loss: 2.873
Iter 11/2000 - Loss: 2.741
Iter 12/2000 - Loss: 2.602
Iter 13/2000 - Loss: 2.453
Iter 14/2000 - Loss: 2.288
Iter 15/2000 - Loss: 2.107
Iter 16/2000 - Loss: 1.916
Iter 17/2000 - Loss: 1.718
Iter 18/2000 - Loss: 1.513
Iter 19/2000 - Loss: 1.295
Iter 20/2000 - Loss: 1.059
Iter 1981/2000 - Loss: -6.416
Iter 1982/2000 - Loss: -6.416
Iter 1983/2000 - Loss: -6.417
Iter 1984/2000 - Loss: -6.417
Iter 1985/2000 - Loss: -6.417
Iter 1986/2000 - Loss: -6.417
Iter 1987/2000 - Loss: -6.417
Iter 1988/2000 - Loss: -6.417
Iter 1989/2000 - Loss: -6.417
Iter 1990/2000 - Loss: -6.417
Iter 1991/2000 - Loss: -6.417
Iter 1992/2000 - Loss: -6.417
Iter 1993/2000 - Loss: -6.417
Iter 1994/2000 - Loss: -6.417
Iter 1995/2000 - Loss: -6.417
Iter 1996/2000 - Loss: -6.417
Iter 1997/2000 - Loss: -6.417
Iter 1998/2000 - Loss: -6.417
Iter 1999/2000 - Loss: -6.417
Iter 2000/2000 - Loss: -6.417
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[15.4088,  9.4401, 17.4563, 12.6716,  7.1055, 59.5829]],

        [[22.1455, 32.7724,  7.2681,  1.2919,  2.7362, 16.8773]],

        [[23.4667,  9.7183,  9.6831,  1.0171,  1.0484, 25.8797]],

        [[20.4638, 33.7778, 14.3230,  1.6511,  2.4112, 37.7558]]])
Signal Variance: tensor([ 0.1599,  1.4825, 16.2529,  0.4823])
Estimated target variance: tensor([0.0229, 0.6840, 6.6870, 0.1450])
N: 130
Signal to noise ratio: tensor([23.5120, 65.4542, 78.7837, 37.0119])
Bound on condition number: tensor([ 71866.6880, 556953.9344, 806895.1336, 178085.6624])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.0028925282057158195, policy loss: 7.073184783997824
Experience 13, Iter 1, disc loss: 0.0032600451687832665, policy loss: 6.749720721159117
Experience 13, Iter 2, disc loss: 0.0034441734715931396, policy loss: 6.761039562051177
Experience 13, Iter 3, disc loss: 0.0036263788093107013, policy loss: 6.72930970648328
Experience 13, Iter 4, disc loss: 0.0037736063290609414, policy loss: 6.608033151354345
Experience 13, Iter 5, disc loss: 0.003952765760491181, policy loss: 6.4726704967034285
Experience 13, Iter 6, disc loss: 0.004640883346182368, policy loss: 6.277443581921817
Experience 13, Iter 7, disc loss: 0.004689182602649215, policy loss: 6.0999947998181145
Experience 13, Iter 8, disc loss: 0.004471639564823285, policy loss: 6.259105709190267
Experience 13, Iter 9, disc loss: 0.006788953212668644, policy loss: 5.729168099402363
Experience 13, Iter 10, disc loss: 0.008894063703654545, policy loss: 5.533578917098254
Experience 13, Iter 11, disc loss: 0.013649222397464475, policy loss: 5.053723075305224
Experience 13, Iter 12, disc loss: 0.04879590868939395, policy loss: 3.9569267418540894
Experience 13, Iter 13, disc loss: 0.08265161566810368, policy loss: 3.806184009989087
Experience 13, Iter 14, disc loss: 0.12630011352939916, policy loss: 2.9881286922695325
Experience 13, Iter 15, disc loss: 0.1965383207139025, policy loss: 2.8076477647771556
Experience 13, Iter 16, disc loss: 0.1261752377305006, policy loss: 3.1226788791401945
Experience 13, Iter 17, disc loss: 0.10192263128575328, policy loss: 3.4521237999866354
Experience 13, Iter 18, disc loss: 0.06209385944319423, policy loss: 4.384636602982706
Experience 13, Iter 19, disc loss: 0.06372728990912838, policy loss: 4.160838987339502
Experience 13, Iter 20, disc loss: 0.05551770269663204, policy loss: 4.835618126611751
Experience 13, Iter 21, disc loss: 0.03961851536979864, policy loss: 4.822034419014809
Experience 13, Iter 22, disc loss: 0.04419224337260927, policy loss: 4.794854934429547
Experience 13, Iter 23, disc loss: 0.04840274680051775, policy loss: 4.939741895991591
Experience 13, Iter 24, disc loss: 0.052045465577520544, policy loss: 4.86814136349515
Experience 13, Iter 25, disc loss: 0.06457702767357376, policy loss: 4.505636243056595
Experience 13, Iter 26, disc loss: 0.07011781687736938, policy loss: 4.906920504646581
Experience 13, Iter 27, disc loss: 0.07526592643127714, policy loss: 5.18754106121471
Experience 13, Iter 28, disc loss: 0.08258054035312445, policy loss: 4.887666563586087
Experience 13, Iter 29, disc loss: 0.08175972925533924, policy loss: 5.6055524608457805
Experience 13, Iter 30, disc loss: 0.08127982587937736, policy loss: 5.038897753985623
Experience 13, Iter 31, disc loss: 0.0764496145191864, policy loss: 4.8910982924018285
Experience 13, Iter 32, disc loss: 0.0733255609063039, policy loss: 4.736256728219548
Experience 13, Iter 33, disc loss: 0.06669823269597985, policy loss: 4.712126850437057
Experience 13, Iter 34, disc loss: 0.0633390289764033, policy loss: 4.3106830963264295
Experience 13, Iter 35, disc loss: 0.05548538905407442, policy loss: 4.42233927500066
Experience 13, Iter 36, disc loss: 0.05561688918006226, policy loss: 4.109525033010419
Experience 13, Iter 37, disc loss: 0.049517955480868965, policy loss: 4.268347887930396
Experience 13, Iter 38, disc loss: 0.047920367990008934, policy loss: 4.205771935114129
Experience 13, Iter 39, disc loss: 0.05394566379797469, policy loss: 3.942742624221216
Experience 13, Iter 40, disc loss: 0.053711410032541505, policy loss: 3.9214287132063417
Experience 13, Iter 41, disc loss: 0.05213994071004047, policy loss: 3.8996356572475204
Experience 13, Iter 42, disc loss: 0.0583434907009969, policy loss: 3.558562774273022
Experience 13, Iter 43, disc loss: 0.052967060920245654, policy loss: 3.749415400566315
Experience 13, Iter 44, disc loss: 0.05414525133178612, policy loss: 3.6593739584874614
Experience 13, Iter 45, disc loss: 0.04589861258874002, policy loss: 4.232357578712998
Experience 13, Iter 46, disc loss: 0.053617459064525155, policy loss: 3.8172544626098652
Experience 13, Iter 47, disc loss: 0.04825242101890206, policy loss: 4.14322105629536
Experience 13, Iter 48, disc loss: 0.045589029208098215, policy loss: 4.289536446579871
Experience 13, Iter 49, disc loss: 0.04569904689868354, policy loss: 4.068339903290897
Experience 13, Iter 50, disc loss: 0.045167419813888846, policy loss: 4.1569702590037405
Experience 13, Iter 51, disc loss: 0.04026143801115147, policy loss: 4.369814889893693
Experience 13, Iter 52, disc loss: 0.04334611028808, policy loss: 4.2310108827675625
Experience 13, Iter 53, disc loss: 0.04627067329696949, policy loss: 4.160289472556746
Experience 13, Iter 54, disc loss: 0.04163052261437668, policy loss: 4.445617823197143
Experience 13, Iter 55, disc loss: 0.04058270661514421, policy loss: 4.521989133461143
Experience 13, Iter 56, disc loss: 0.04908838229789572, policy loss: 4.1543089309862875
Experience 13, Iter 57, disc loss: 0.04053356476790982, policy loss: 4.705547649893894
Experience 13, Iter 58, disc loss: 0.03773967551337421, policy loss: 4.7830303340599905
Experience 13, Iter 59, disc loss: 0.041565970532693015, policy loss: 4.671651656719337
Experience 13, Iter 60, disc loss: 0.04019497733326819, policy loss: 4.654676186400102
Experience 13, Iter 61, disc loss: 0.037890180654274616, policy loss: 4.706067624146687
Experience 13, Iter 62, disc loss: 0.0387214927301848, policy loss: 4.618139776352956
Experience 13, Iter 63, disc loss: 0.039682390456665675, policy loss: 4.549625640536988
Experience 13, Iter 64, disc loss: 0.03653327810623196, policy loss: 4.682889369483413
Experience 13, Iter 65, disc loss: 0.03786678701741843, policy loss: 4.532371069425196
Experience 13, Iter 66, disc loss: 0.03640453249507201, policy loss: 4.647578677441022
Experience 13, Iter 67, disc loss: 0.03412658713934234, policy loss: 4.643881276000776
Experience 13, Iter 68, disc loss: 0.03375284719304073, policy loss: 4.793905002302749
Experience 13, Iter 69, disc loss: 0.035052152590493116, policy loss: 4.405113461439168
Experience 13, Iter 70, disc loss: 0.03285381951209243, policy loss: 4.434018612421718
Experience 13, Iter 71, disc loss: 0.03451050734331547, policy loss: 4.420383543166581
Experience 13, Iter 72, disc loss: 0.03257862905364858, policy loss: 4.506727329183801
Experience 13, Iter 73, disc loss: 0.03472029198490029, policy loss: 4.317451939325551
Experience 13, Iter 74, disc loss: 0.03289947118858064, policy loss: 4.42992461046204
Experience 13, Iter 75, disc loss: 0.03274558138353816, policy loss: 4.439737799567219
Experience 13, Iter 76, disc loss: 0.029347965150692795, policy loss: 4.749216411822067
Experience 13, Iter 77, disc loss: 0.030408856320862748, policy loss: 4.6019798999407
Experience 13, Iter 78, disc loss: 0.03359107573301167, policy loss: 4.315360319349112
Experience 13, Iter 79, disc loss: 0.028390391534371978, policy loss: 4.6388114836994925
Experience 13, Iter 80, disc loss: 0.03319960506465119, policy loss: 4.450791059340684
Experience 13, Iter 81, disc loss: 0.02949735596357781, policy loss: 4.602514563876277
Experience 13, Iter 82, disc loss: 0.026831227048875077, policy loss: 4.843482658981638
Experience 13, Iter 83, disc loss: 0.031741001243132276, policy loss: 4.620915535287037
Experience 13, Iter 84, disc loss: 0.02901808401326657, policy loss: 4.693837446840286
Experience 13, Iter 85, disc loss: 0.030195147375460696, policy loss: 4.57383556902072
Experience 13, Iter 86, disc loss: 0.028145393624832274, policy loss: 4.894828994037637
Experience 13, Iter 87, disc loss: 0.026778679894140657, policy loss: 4.857499157738545
Experience 13, Iter 88, disc loss: 0.028643025438566547, policy loss: 4.590522687132941
Experience 13, Iter 89, disc loss: 0.024610609794512732, policy loss: 4.97228680747228
Experience 13, Iter 90, disc loss: 0.029247708698051697, policy loss: 4.619576529917655
Experience 13, Iter 91, disc loss: 0.0290155544561868, policy loss: 4.4587142812349
Experience 13, Iter 92, disc loss: 0.02800840240701875, policy loss: 4.681604196273684
Experience 13, Iter 93, disc loss: 0.027296949225966798, policy loss: 4.693633836036524
Experience 13, Iter 94, disc loss: 0.02699965067496657, policy loss: 4.77991763802882
Experience 13, Iter 95, disc loss: 0.027387159208812878, policy loss: 4.794828971976866
Experience 13, Iter 96, disc loss: 0.02652153278372773, policy loss: 4.81037124325929
Experience 13, Iter 97, disc loss: 0.026731821369853206, policy loss: 4.795384769288985
Experience 13, Iter 98, disc loss: 0.025806918879139294, policy loss: 4.796549948032763
Experience 13, Iter 99, disc loss: 0.026381223662256398, policy loss: 4.6510966083677845
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.1852],
        [1.7388],
        [0.0381]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0312, 0.2252, 1.7305, 0.0351, 0.0225, 4.8626]],

        [[0.0312, 0.2252, 1.7305, 0.0351, 0.0225, 4.8626]],

        [[0.0312, 0.2252, 1.7305, 0.0351, 0.0225, 4.8626]],

        [[0.0312, 0.2252, 1.7305, 0.0351, 0.0225, 4.8626]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0226, 0.7409, 6.9550, 0.1525], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0226, 0.7409, 6.9550, 0.1525])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.745
Iter 2/2000 - Loss: 3.647
Iter 3/2000 - Loss: 3.536
Iter 4/2000 - Loss: 3.475
Iter 5/2000 - Loss: 3.454
Iter 6/2000 - Loss: 3.365
Iter 7/2000 - Loss: 3.247
Iter 8/2000 - Loss: 3.148
Iter 9/2000 - Loss: 3.050
Iter 10/2000 - Loss: 2.930
Iter 11/2000 - Loss: 2.795
Iter 12/2000 - Loss: 2.652
Iter 13/2000 - Loss: 2.499
Iter 14/2000 - Loss: 2.328
Iter 15/2000 - Loss: 2.139
Iter 16/2000 - Loss: 1.939
Iter 17/2000 - Loss: 1.733
Iter 18/2000 - Loss: 1.519
Iter 19/2000 - Loss: 1.292
Iter 20/2000 - Loss: 1.049
Iter 1981/2000 - Loss: -6.443
Iter 1982/2000 - Loss: -6.443
Iter 1983/2000 - Loss: -6.443
Iter 1984/2000 - Loss: -6.443
Iter 1985/2000 - Loss: -6.443
Iter 1986/2000 - Loss: -6.443
Iter 1987/2000 - Loss: -6.443
Iter 1988/2000 - Loss: -6.443
Iter 1989/2000 - Loss: -6.443
Iter 1990/2000 - Loss: -6.443
Iter 1991/2000 - Loss: -6.443
Iter 1992/2000 - Loss: -6.443
Iter 1993/2000 - Loss: -6.444
Iter 1994/2000 - Loss: -6.444
Iter 1995/2000 - Loss: -6.444
Iter 1996/2000 - Loss: -6.444
Iter 1997/2000 - Loss: -6.444
Iter 1998/2000 - Loss: -6.444
Iter 1999/2000 - Loss: -6.444
Iter 2000/2000 - Loss: -6.444
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0032],
        [0.0003]])
Lengthscale: tensor([[[14.1461,  8.5929, 18.2350,  7.6712,  6.7907, 57.7524]],

        [[21.9859, 34.7462,  7.7294,  1.2998,  2.3891, 20.4347]],

        [[23.3682, 26.6133,  8.5744,  1.0765,  1.0311, 25.9938]],

        [[19.0512, 34.8047, 14.4602,  1.0892,  4.6591, 49.3372]]])
Signal Variance: tensor([ 0.1529,  1.8829, 18.2154,  0.5299])
Estimated target variance: tensor([0.0226, 0.7409, 6.9550, 0.1525])
N: 140
Signal to noise ratio: tensor([22.2187, 72.9855, 74.8924, 39.3451])
Bound on condition number: tensor([ 69115.0704, 745764.3500, 785243.4759, 216726.1728])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.033657099749834454, policy loss: 4.416408852372842
Experience 14, Iter 1, disc loss: 0.03577704243924101, policy loss: 4.368637163033413
Experience 14, Iter 2, disc loss: 0.03620504705519119, policy loss: 4.460343624343992
Experience 14, Iter 3, disc loss: 0.03515046290097893, policy loss: 4.194724170747744
Experience 14, Iter 4, disc loss: 0.035632667780206455, policy loss: 4.193846541912983
Experience 14, Iter 5, disc loss: 0.03154983114708977, policy loss: 4.485458091449254
Experience 14, Iter 6, disc loss: 0.036372459656719425, policy loss: 4.254784866041673
Experience 14, Iter 7, disc loss: 0.031568349099141066, policy loss: 4.628321363110084
Experience 14, Iter 8, disc loss: 0.03270400726504722, policy loss: 4.449634138184851
Experience 14, Iter 9, disc loss: 0.030016610041259665, policy loss: 5.147038957584966
Experience 14, Iter 10, disc loss: 0.03181069667320273, policy loss: 4.555323253725482
Experience 14, Iter 11, disc loss: 0.030597502274284042, policy loss: 4.732986995021919
Experience 14, Iter 12, disc loss: 0.03127208672244989, policy loss: 4.603677684471878
Experience 14, Iter 13, disc loss: 0.027968584008784055, policy loss: 5.121266619905813
Experience 14, Iter 14, disc loss: 0.02946351739199809, policy loss: 4.9190644107187715
Experience 14, Iter 15, disc loss: 0.02997548076790843, policy loss: 4.703648281801686
Experience 14, Iter 16, disc loss: 0.029025706826345074, policy loss: 4.887303944543007
Experience 14, Iter 17, disc loss: 0.028991983325445385, policy loss: 4.849392023791962
Experience 14, Iter 18, disc loss: 0.028719718970813095, policy loss: 4.7838301686796445
Experience 14, Iter 19, disc loss: 0.02730953861713093, policy loss: 5.06070846499626
Experience 14, Iter 20, disc loss: 0.027199608265884037, policy loss: 4.733268462229012
Experience 14, Iter 21, disc loss: 0.02756344778313728, policy loss: 4.806949923028418
Experience 14, Iter 22, disc loss: 0.026978505383697178, policy loss: 4.746783403982472
Experience 14, Iter 23, disc loss: 0.026317083631890493, policy loss: 4.773067983313861
Experience 14, Iter 24, disc loss: 0.028019868216749944, policy loss: 4.4821389397318105
Experience 14, Iter 25, disc loss: 0.02650431568511681, policy loss: 5.070585497994276
Experience 14, Iter 26, disc loss: 0.025555090247330173, policy loss: 4.910085604041849
Experience 14, Iter 27, disc loss: 0.025164767846824164, policy loss: 4.915972583555401
Experience 14, Iter 28, disc loss: 0.02406644581090984, policy loss: 4.947510609621846
Experience 14, Iter 29, disc loss: 0.02467942657842549, policy loss: 4.8123255311359845
Experience 14, Iter 30, disc loss: 0.02593374609458919, policy loss: 4.788909857706563
Experience 14, Iter 31, disc loss: 0.023972834960961902, policy loss: 4.865507034893344
Experience 14, Iter 32, disc loss: 0.02399373052003447, policy loss: 4.728320503251315
Experience 14, Iter 33, disc loss: 0.023364700717201384, policy loss: 4.923210819850979
Experience 14, Iter 34, disc loss: 0.023709511115000588, policy loss: 4.703906748538262
Experience 14, Iter 35, disc loss: 0.02290997406717599, policy loss: 5.081633003809043
Experience 14, Iter 36, disc loss: 0.024596816619228107, policy loss: 4.8244937381856605
Experience 14, Iter 37, disc loss: 0.02245278213580032, policy loss: 5.005185774810893
Experience 14, Iter 38, disc loss: 0.022446262901443446, policy loss: 5.101613542084409
Experience 14, Iter 39, disc loss: 0.022194067499649854, policy loss: 4.8544376703398155
Experience 14, Iter 40, disc loss: 0.022970936188394506, policy loss: 4.823527071400516
Experience 14, Iter 41, disc loss: 0.022017292862113014, policy loss: 5.2404625123244255
Experience 14, Iter 42, disc loss: 0.021053211381250967, policy loss: 5.0644970484287
Experience 14, Iter 43, disc loss: 0.021053696168868326, policy loss: 4.954936431115067
Experience 14, Iter 44, disc loss: 0.0214440056867857, policy loss: 5.102993381236131
Experience 14, Iter 45, disc loss: 0.020838872059571644, policy loss: 5.105817897643938
Experience 14, Iter 46, disc loss: 0.02159033703234818, policy loss: 4.876820283872458
Experience 14, Iter 47, disc loss: 0.02113428702318665, policy loss: 4.847334331617263
Experience 14, Iter 48, disc loss: 0.021042198106259537, policy loss: 5.105361376137771
Experience 14, Iter 49, disc loss: 0.019232975848131738, policy loss: 5.156032730958736
Experience 14, Iter 50, disc loss: 0.019100023293775095, policy loss: 5.266573329787661
Experience 14, Iter 51, disc loss: 0.020479606389041914, policy loss: 5.063269574427024
Experience 14, Iter 52, disc loss: 0.01873791901640826, policy loss: 5.367203878940116
Experience 14, Iter 53, disc loss: 0.019952461840289704, policy loss: 5.028479910930371
Experience 14, Iter 54, disc loss: 0.019389106404045557, policy loss: 5.036332113330314
Experience 14, Iter 55, disc loss: 0.020073441195373826, policy loss: 5.073650174021532
Experience 14, Iter 56, disc loss: 0.018562771324875306, policy loss: 5.250756811367131
Experience 14, Iter 57, disc loss: 0.019006142716908148, policy loss: 5.004005303344326
Experience 14, Iter 58, disc loss: 0.019807541773435827, policy loss: 5.043852491015905
Experience 14, Iter 59, disc loss: 0.01851242610972873, policy loss: 5.061086798421235
Experience 14, Iter 60, disc loss: 0.018000721800473664, policy loss: 5.391599201930575
Experience 14, Iter 61, disc loss: 0.017818279608680297, policy loss: 5.50722258972101
Experience 14, Iter 62, disc loss: 0.01828159799560783, policy loss: 5.129303386815897
Experience 14, Iter 63, disc loss: 0.018638645666431523, policy loss: 5.177861219692664
Experience 14, Iter 64, disc loss: 0.01779876034396595, policy loss: 5.209188049991564
Experience 14, Iter 65, disc loss: 0.017715684114560218, policy loss: 5.188999363029631
Experience 14, Iter 66, disc loss: 0.018965712883786845, policy loss: 4.827590034928065
Experience 14, Iter 67, disc loss: 0.017756611267515106, policy loss: 5.074201601011007
Experience 14, Iter 68, disc loss: 0.018032895055321962, policy loss: 4.995608431260523
Experience 14, Iter 69, disc loss: 0.017435166334298556, policy loss: 5.247272318400702
Experience 14, Iter 70, disc loss: 0.016759846278372216, policy loss: 5.457754193848663
Experience 14, Iter 71, disc loss: 0.01745756260000729, policy loss: 5.157341488493266
Experience 14, Iter 72, disc loss: 0.017136765088261733, policy loss: 5.114608715651325
Experience 14, Iter 73, disc loss: 0.017652102394343873, policy loss: 5.368757776777018
Experience 14, Iter 74, disc loss: 0.016640414933787595, policy loss: 5.672110903893207
Experience 14, Iter 75, disc loss: 0.016184430271171293, policy loss: 5.372713686398917
Experience 14, Iter 76, disc loss: 0.016899321753036572, policy loss: 5.2510841269635495
Experience 14, Iter 77, disc loss: 0.016624615302370957, policy loss: 5.3318497733669945
Experience 14, Iter 78, disc loss: 0.01632613956936628, policy loss: 5.289338131685229
Experience 14, Iter 79, disc loss: 0.01655172934537728, policy loss: 5.557387782023971
Experience 14, Iter 80, disc loss: 0.016754153643418892, policy loss: 5.0662425601985275
Experience 14, Iter 81, disc loss: 0.015612964626678801, policy loss: 5.342254339176147
Experience 14, Iter 82, disc loss: 0.01592878247988936, policy loss: 5.1601434762883684
Experience 14, Iter 83, disc loss: 0.015285469528307926, policy loss: 5.712866049969119
Experience 14, Iter 84, disc loss: 0.015608280529002348, policy loss: 5.5290369908407655
Experience 14, Iter 85, disc loss: 0.01624226200507145, policy loss: 5.229147397772361
Experience 14, Iter 86, disc loss: 0.014810429886907983, policy loss: 5.542141600600426
Experience 14, Iter 87, disc loss: 0.015330081651324948, policy loss: 5.999493538540884
Experience 14, Iter 88, disc loss: 0.013892651080831191, policy loss: 6.061772892245522
Experience 14, Iter 89, disc loss: 0.01589863947871748, policy loss: 5.091304986778553
Experience 14, Iter 90, disc loss: 0.013990847084032377, policy loss: 5.485346903521243
Experience 14, Iter 91, disc loss: 0.014629174471252188, policy loss: 5.615890984961189
Experience 14, Iter 92, disc loss: 0.01376760879434634, policy loss: 5.7412434255677915
Experience 14, Iter 93, disc loss: 0.015088948231459905, policy loss: 5.287053762283456
Experience 14, Iter 94, disc loss: 0.014425246799279685, policy loss: 5.683050511732997
Experience 14, Iter 95, disc loss: 0.015037044541696342, policy loss: 5.232560311155927
Experience 14, Iter 96, disc loss: 0.014990277509620993, policy loss: 5.490257646686537
Experience 14, Iter 97, disc loss: 0.01476881007341204, policy loss: 5.297436240488761
Experience 14, Iter 98, disc loss: 0.014061684000965594, policy loss: 5.4718652409288735
Experience 14, Iter 99, disc loss: 0.01428283492190048, policy loss: 5.548166700362527
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.1931],
        [1.7761],
        [0.0383]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0293, 0.2212, 1.7485, 0.0351, 0.0231, 5.0656]],

        [[0.0293, 0.2212, 1.7485, 0.0351, 0.0231, 5.0656]],

        [[0.0293, 0.2212, 1.7485, 0.0351, 0.0231, 5.0656]],

        [[0.0293, 0.2212, 1.7485, 0.0351, 0.0231, 5.0656]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0221, 0.7726, 7.1043, 0.1534], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0221, 0.7726, 7.1043, 0.1534])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.765
Iter 2/2000 - Loss: 3.676
Iter 3/2000 - Loss: 3.547
Iter 4/2000 - Loss: 3.487
Iter 5/2000 - Loss: 3.465
Iter 6/2000 - Loss: 3.368
Iter 7/2000 - Loss: 3.243
Iter 8/2000 - Loss: 3.137
Iter 9/2000 - Loss: 3.027
Iter 10/2000 - Loss: 2.893
Iter 11/2000 - Loss: 2.743
Iter 12/2000 - Loss: 2.586
Iter 13/2000 - Loss: 2.420
Iter 14/2000 - Loss: 2.236
Iter 15/2000 - Loss: 2.034
Iter 16/2000 - Loss: 1.822
Iter 17/2000 - Loss: 1.605
Iter 18/2000 - Loss: 1.382
Iter 19/2000 - Loss: 1.150
Iter 20/2000 - Loss: 0.902
Iter 1981/2000 - Loss: -6.624
Iter 1982/2000 - Loss: -6.624
Iter 1983/2000 - Loss: -6.624
Iter 1984/2000 - Loss: -6.624
Iter 1985/2000 - Loss: -6.624
Iter 1986/2000 - Loss: -6.624
Iter 1987/2000 - Loss: -6.625
Iter 1988/2000 - Loss: -6.625
Iter 1989/2000 - Loss: -6.625
Iter 1990/2000 - Loss: -6.625
Iter 1991/2000 - Loss: -6.625
Iter 1992/2000 - Loss: -6.625
Iter 1993/2000 - Loss: -6.625
Iter 1994/2000 - Loss: -6.625
Iter 1995/2000 - Loss: -6.625
Iter 1996/2000 - Loss: -6.625
Iter 1997/2000 - Loss: -6.625
Iter 1998/2000 - Loss: -6.625
Iter 1999/2000 - Loss: -6.625
Iter 2000/2000 - Loss: -6.625
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0032],
        [0.0003]])
Lengthscale: tensor([[[13.8073,  8.9557, 18.7202,  6.8832,  7.1170, 59.7160]],

        [[20.0601, 34.6580,  8.0624,  1.2459,  2.3057, 21.8060]],

        [[22.6560, 30.9350,  8.6062,  1.0858,  1.0737, 26.5265]],

        [[18.5503, 33.5632, 14.5575,  1.0630,  4.9457, 51.2808]]])
Signal Variance: tensor([ 0.1561,  2.0261, 19.0790,  0.5367])
Estimated target variance: tensor([0.0221, 0.7726, 7.1043, 0.1534])
N: 150
Signal to noise ratio: tensor([22.7088, 75.6547, 76.6263, 40.4322])
Bound on condition number: tensor([ 77354.3324, 858545.3506, 880739.1321, 245215.5112])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.01418465281772733, policy loss: 5.9001095362672595
Experience 15, Iter 1, disc loss: 0.014080586938755503, policy loss: 5.7631049524774625
Experience 15, Iter 2, disc loss: 0.014679598838156453, policy loss: 5.100612145042383
Experience 15, Iter 3, disc loss: 0.01315372564148391, policy loss: 5.682728692810654
Experience 15, Iter 4, disc loss: 0.012916169129609223, policy loss: 5.867572564892598
Experience 15, Iter 5, disc loss: 0.014321626463919719, policy loss: 5.254278056795835
Experience 15, Iter 6, disc loss: 0.013021597415327869, policy loss: 5.501068075484896
Experience 15, Iter 7, disc loss: 0.014638979309830583, policy loss: 5.1847275483060535
Experience 15, Iter 8, disc loss: 0.014200786507872942, policy loss: 5.284858705200208
Experience 15, Iter 9, disc loss: 0.013220000404604582, policy loss: 5.573141988273662
Experience 15, Iter 10, disc loss: 0.014098242431047028, policy loss: 5.619845387151355
Experience 15, Iter 11, disc loss: 0.01276881107095201, policy loss: 6.079037251142752
Experience 15, Iter 12, disc loss: 0.013145788511132257, policy loss: 5.598081760749657
Experience 15, Iter 13, disc loss: 0.012959583975313969, policy loss: 5.674030440476711
Experience 15, Iter 14, disc loss: 0.014022027207965866, policy loss: 5.189484787036357
Experience 15, Iter 15, disc loss: 0.01339115244962166, policy loss: 5.361878099444775
Experience 15, Iter 16, disc loss: 0.013343470300713183, policy loss: 5.440730767114303
Experience 15, Iter 17, disc loss: 0.01301017265915066, policy loss: 5.421838695854546
Experience 15, Iter 18, disc loss: 0.012426008529352422, policy loss: 5.483611161501242
Experience 15, Iter 19, disc loss: 0.012582495865404867, policy loss: 5.67122441719945
Experience 15, Iter 20, disc loss: 0.012750273389309695, policy loss: 5.614031133171579
Experience 15, Iter 21, disc loss: 0.013286605267548547, policy loss: 5.509431718546023
Experience 15, Iter 22, disc loss: 0.012883624852244997, policy loss: 5.346787821529993
Experience 15, Iter 23, disc loss: 0.01273142269890298, policy loss: 5.589735732644465
Experience 15, Iter 24, disc loss: 0.012792289438108003, policy loss: 5.6826546995102625
Experience 15, Iter 25, disc loss: 0.013054811306230235, policy loss: 5.359980618556959
Experience 15, Iter 26, disc loss: 0.01256996730546462, policy loss: 5.655887431213714
Experience 15, Iter 27, disc loss: 0.012623782069849158, policy loss: 5.712691700885688
Experience 15, Iter 28, disc loss: 0.013050854886088935, policy loss: 5.365330068574365
Experience 15, Iter 29, disc loss: 0.012201750962115555, policy loss: 5.640811837728813
Experience 15, Iter 30, disc loss: 0.01194396576908369, policy loss: 5.703864847029669
Experience 15, Iter 31, disc loss: 0.01230662629486895, policy loss: 5.54474685617812
Experience 15, Iter 32, disc loss: 0.011867075834222866, policy loss: 5.82265762631708
Experience 15, Iter 33, disc loss: 0.012400169442040983, policy loss: 5.514605078836266
Experience 15, Iter 34, disc loss: 0.01101112246194224, policy loss: 6.073292651901697
Experience 15, Iter 35, disc loss: 0.012279534566177691, policy loss: 5.321711867971445
Experience 15, Iter 36, disc loss: 0.011717580920650547, policy loss: 5.698005454163688
Experience 15, Iter 37, disc loss: 0.011740300456759476, policy loss: 5.532599790948932
Experience 15, Iter 38, disc loss: 0.011972770177981608, policy loss: 5.889920910590287
Experience 15, Iter 39, disc loss: 0.011913483940339507, policy loss: 5.737034655035147
Experience 15, Iter 40, disc loss: 0.012397750942031622, policy loss: 5.606230652230485
Experience 15, Iter 41, disc loss: 0.011443729409279183, policy loss: 5.7235515724875885
Experience 15, Iter 42, disc loss: 0.012027269967608466, policy loss: 5.44468466967185
Experience 15, Iter 43, disc loss: 0.011011464069810885, policy loss: 5.830606996861584
Experience 15, Iter 44, disc loss: 0.01201155458995554, policy loss: 5.4717805867097855
Experience 15, Iter 45, disc loss: 0.011103007762483616, policy loss: 5.603979972653774
Experience 15, Iter 46, disc loss: 0.010928098709163338, policy loss: 5.837352563034694
Experience 15, Iter 47, disc loss: 0.01094422740539449, policy loss: 5.8558761375612765
Experience 15, Iter 48, disc loss: 0.011054121005660552, policy loss: 5.744955730730344
Experience 15, Iter 49, disc loss: 0.011395232708613888, policy loss: 5.5464233779544365
Experience 15, Iter 50, disc loss: 0.012022669741704711, policy loss: 5.275434836109077
Experience 15, Iter 51, disc loss: 0.010857214571058382, policy loss: 5.666251717001675
Experience 15, Iter 52, disc loss: 0.010931981065609117, policy loss: 5.763479767788079
Experience 15, Iter 53, disc loss: 0.010827496567330078, policy loss: 5.599725585150116
Experience 15, Iter 54, disc loss: 0.010682657080225654, policy loss: 5.8915559020133035
Experience 15, Iter 55, disc loss: 0.010724395836785059, policy loss: 6.045404824009749
Experience 15, Iter 56, disc loss: 0.01074904695423463, policy loss: 5.654821536448383
Experience 15, Iter 57, disc loss: 0.010789150748615129, policy loss: 5.577430911226408
Experience 15, Iter 58, disc loss: 0.010338698732715094, policy loss: 5.632131557305247
Experience 15, Iter 59, disc loss: 0.011202304347112762, policy loss: 5.387719113470616
Experience 15, Iter 60, disc loss: 0.010872300993680063, policy loss: 5.823199980744905
Experience 15, Iter 61, disc loss: 0.0105995926399312, policy loss: 5.809019213371904
Experience 15, Iter 62, disc loss: 0.010561767570294488, policy loss: 5.723082852638861
Experience 15, Iter 63, disc loss: 0.010640880197233792, policy loss: 5.827374661014716
Experience 15, Iter 64, disc loss: 0.010510181200053467, policy loss: 5.758160540269082
Experience 15, Iter 65, disc loss: 0.010745492343497708, policy loss: 5.758124400174818
Experience 15, Iter 66, disc loss: 0.010008088234299575, policy loss: 6.16897232867966
Experience 15, Iter 67, disc loss: 0.009906876515262782, policy loss: 5.9324290218456355
Experience 15, Iter 68, disc loss: 0.010377218039663112, policy loss: 5.714400392377497
Experience 15, Iter 69, disc loss: 0.01048417456761492, policy loss: 5.516872621207552
Experience 15, Iter 70, disc loss: 0.010240966506037666, policy loss: 5.855706264516773
Experience 15, Iter 71, disc loss: 0.01010185767028507, policy loss: 5.948129234644707
Experience 15, Iter 72, disc loss: 0.010398368415608601, policy loss: 5.755538275870902
Experience 15, Iter 73, disc loss: 0.010273729985375027, policy loss: 5.727823359838891
Experience 15, Iter 74, disc loss: 0.009836224228763596, policy loss: 5.790448333917837
Experience 15, Iter 75, disc loss: 0.009856672901120626, policy loss: 6.1457591823126805
Experience 15, Iter 76, disc loss: 0.00953484851067117, policy loss: 5.959623935894575
Experience 15, Iter 77, disc loss: 0.009721633137549396, policy loss: 6.1242325517503495
Experience 15, Iter 78, disc loss: 0.010190180609193914, policy loss: 5.665571336634378
Experience 15, Iter 79, disc loss: 0.009616978929866994, policy loss: 5.742921220692983
Experience 15, Iter 80, disc loss: 0.009988137611630593, policy loss: 5.803865358882259
Experience 15, Iter 81, disc loss: 0.009888183243438181, policy loss: 5.679017499370745
Experience 15, Iter 82, disc loss: 0.00951515819154881, policy loss: 5.988841927873779
Experience 15, Iter 83, disc loss: 0.009798061431075637, policy loss: 5.707998959757012
Experience 15, Iter 84, disc loss: 0.010046003518506774, policy loss: 5.540869920847716
Experience 15, Iter 85, disc loss: 0.009333433179221761, policy loss: 6.584211506699598
Experience 15, Iter 86, disc loss: 0.009829297385067945, policy loss: 5.601179244383207
Experience 15, Iter 87, disc loss: 0.009774645997208816, policy loss: 5.862220774876891
Experience 15, Iter 88, disc loss: 0.009638760883258334, policy loss: 5.865034185468781
Experience 15, Iter 89, disc loss: 0.009371136185123679, policy loss: 6.430385127122917
Experience 15, Iter 90, disc loss: 0.009362465583231807, policy loss: 5.981704402243249
Experience 15, Iter 91, disc loss: 0.009537084340806, policy loss: 5.854255953250618
Experience 15, Iter 92, disc loss: 0.009910328024066321, policy loss: 5.674184978006899
Experience 15, Iter 93, disc loss: 0.010236715108382673, policy loss: 5.675819911531312
Experience 15, Iter 94, disc loss: 0.009844841972533384, policy loss: 5.636802368037034
Experience 15, Iter 95, disc loss: 0.009755231844750937, policy loss: 5.746218558530468
Experience 15, Iter 96, disc loss: 0.009793916545632535, policy loss: 5.808065895979892
Experience 15, Iter 97, disc loss: 0.009889578735185086, policy loss: 5.886410411940244
Experience 15, Iter 98, disc loss: 0.008833212601543296, policy loss: 6.5469638035606526
Experience 15, Iter 99, disc loss: 0.008859447392837134, policy loss: 6.0948840249059
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.2047],
        [1.8282],
        [0.0397]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0277, 0.2190, 1.8144, 0.0355, 0.0239, 5.2740]],

        [[0.0277, 0.2190, 1.8144, 0.0355, 0.0239, 5.2740]],

        [[0.0277, 0.2190, 1.8144, 0.0355, 0.0239, 5.2740]],

        [[0.0277, 0.2190, 1.8144, 0.0355, 0.0239, 5.2740]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0216, 0.8188, 7.3126, 0.1588], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0216, 0.8188, 7.3126, 0.1588])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.761
Iter 2/2000 - Loss: 3.662
Iter 3/2000 - Loss: 3.514
Iter 4/2000 - Loss: 3.437
Iter 5/2000 - Loss: 3.399
Iter 6/2000 - Loss: 3.288
Iter 7/2000 - Loss: 3.150
Iter 8/2000 - Loss: 3.030
Iter 9/2000 - Loss: 2.907
Iter 10/2000 - Loss: 2.760
Iter 11/2000 - Loss: 2.600
Iter 12/2000 - Loss: 2.435
Iter 13/2000 - Loss: 2.260
Iter 14/2000 - Loss: 2.068
Iter 15/2000 - Loss: 1.858
Iter 16/2000 - Loss: 1.638
Iter 17/2000 - Loss: 1.414
Iter 18/2000 - Loss: 1.187
Iter 19/2000 - Loss: 0.952
Iter 20/2000 - Loss: 0.705
Iter 1981/2000 - Loss: -6.770
Iter 1982/2000 - Loss: -6.770
Iter 1983/2000 - Loss: -6.770
Iter 1984/2000 - Loss: -6.770
Iter 1985/2000 - Loss: -6.770
Iter 1986/2000 - Loss: -6.770
Iter 1987/2000 - Loss: -6.770
Iter 1988/2000 - Loss: -6.770
Iter 1989/2000 - Loss: -6.770
Iter 1990/2000 - Loss: -6.770
Iter 1991/2000 - Loss: -6.770
Iter 1992/2000 - Loss: -6.770
Iter 1993/2000 - Loss: -6.770
Iter 1994/2000 - Loss: -6.771
Iter 1995/2000 - Loss: -6.771
Iter 1996/2000 - Loss: -6.771
Iter 1997/2000 - Loss: -6.771
Iter 1998/2000 - Loss: -6.771
Iter 1999/2000 - Loss: -6.771
Iter 2000/2000 - Loss: -6.771
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0031],
        [0.0003]])
Lengthscale: tensor([[[14.4389,  9.3265, 19.0269,  5.4150,  6.2259, 62.1366]],

        [[19.4668, 35.2831,  8.0026,  1.2150,  2.2721, 21.1986]],

        [[22.0281, 30.3798,  8.7752,  1.0394,  1.0496, 27.0069]],

        [[17.8337, 33.4182, 12.4254,  1.0792,  4.3795, 47.4483]]])
Signal Variance: tensor([ 0.1500,  1.9344, 19.0123,  0.4288])
Estimated target variance: tensor([0.0216, 0.8188, 7.3126, 0.1588])
N: 160
Signal to noise ratio: tensor([22.2983, 77.1388, 77.9843, 36.2123])
Bound on condition number: tensor([ 79555.2504, 952063.1136, 973048.4769, 209814.2044])
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 0.009723002696024793, policy loss: 6.151255573167045
Experience 16, Iter 1, disc loss: 0.009213752692237544, policy loss: 5.9512743414116835
Experience 16, Iter 2, disc loss: 0.009081090413823129, policy loss: 6.303510045292914
Experience 16, Iter 3, disc loss: 0.00927699048321944, policy loss: 5.824770584604017
Experience 16, Iter 4, disc loss: 0.009683553047855929, policy loss: 5.784575757920203
Experience 16, Iter 5, disc loss: 0.008654907007654748, policy loss: 5.874126607781
Experience 16, Iter 6, disc loss: 0.009426495212337878, policy loss: 5.882092707927413
Experience 16, Iter 7, disc loss: 0.009399880706051317, policy loss: 5.616947982301554
Experience 16, Iter 8, disc loss: 0.009294283541184738, policy loss: 5.935795800442518
Experience 16, Iter 9, disc loss: 0.00911446919845782, policy loss: 6.0249429415577485
Experience 16, Iter 10, disc loss: 0.009116838085469732, policy loss: 6.166325354318101
Experience 16, Iter 11, disc loss: 0.00928503754554703, policy loss: 5.701284471239161
Experience 16, Iter 12, disc loss: 0.008909005370496744, policy loss: 5.9188876805819755
Experience 16, Iter 13, disc loss: 0.008714105451769264, policy loss: 6.202910115212215
Experience 16, Iter 14, disc loss: 0.008844023739746805, policy loss: 6.073972896201011
Experience 16, Iter 15, disc loss: 0.009937046629579865, policy loss: 5.471680581719521
Experience 16, Iter 16, disc loss: 0.00875436925536707, policy loss: 5.814542201933891
Experience 16, Iter 17, disc loss: 0.008534555230715089, policy loss: 6.724468488899716
Experience 16, Iter 18, disc loss: 0.007569766063459085, policy loss: 6.707059342493962
Experience 16, Iter 19, disc loss: 0.00951863954406413, policy loss: 5.567763368163059
Experience 16, Iter 20, disc loss: 0.009027290024510873, policy loss: 5.928704428087064
Experience 16, Iter 21, disc loss: 0.008523673123144166, policy loss: 6.3291659561103035
Experience 16, Iter 22, disc loss: 0.007958656880179639, policy loss: 6.601428732989243
Experience 16, Iter 23, disc loss: 0.00922020905397534, policy loss: 5.770466870305873
Experience 16, Iter 24, disc loss: 0.009155541010272727, policy loss: 5.632089589295402
Experience 16, Iter 25, disc loss: 0.008491807676370429, policy loss: 5.825914196385419
Experience 16, Iter 26, disc loss: 0.008353032797799555, policy loss: 6.022128794825717
Experience 16, Iter 27, disc loss: 0.008737106645039228, policy loss: 5.873469696271672
Experience 16, Iter 28, disc loss: 0.008698916284716904, policy loss: 5.8589036427669585
Experience 16, Iter 29, disc loss: 0.008510628883529034, policy loss: 6.067962461935734
Experience 16, Iter 30, disc loss: 0.008848916550057425, policy loss: 5.629350140136159
Experience 16, Iter 31, disc loss: 0.009059235580435455, policy loss: 5.829822874851904
Experience 16, Iter 32, disc loss: 0.00829587595373308, policy loss: 6.403519049123479
Experience 16, Iter 33, disc loss: 0.008591505797111893, policy loss: 6.091589634954021
Experience 16, Iter 34, disc loss: 0.008340546550207164, policy loss: 6.164972835790248
Experience 16, Iter 35, disc loss: 0.009073596490254882, policy loss: 5.865629393505003
Experience 16, Iter 36, disc loss: 0.008696203490182478, policy loss: 5.777904470956794
Experience 16, Iter 37, disc loss: 0.007962894711281851, policy loss: 6.226446880084668
Experience 16, Iter 38, disc loss: 0.008179481007886558, policy loss: 5.884698220207659
Experience 16, Iter 39, disc loss: 0.007931738600876415, policy loss: 6.099152182764446
Experience 16, Iter 40, disc loss: 0.00841864988713006, policy loss: 5.8269987190418275
Experience 16, Iter 41, disc loss: 0.008615375680486818, policy loss: 5.797350024004364
Experience 16, Iter 42, disc loss: 0.008397509652705274, policy loss: 5.774753370989345
Experience 16, Iter 43, disc loss: 0.008615698128350136, policy loss: 5.7325686280901
Experience 16, Iter 44, disc loss: 0.008158310382262147, policy loss: 6.149983939708179
Experience 16, Iter 45, disc loss: 0.007748224840601441, policy loss: 6.090970780708648
Experience 16, Iter 46, disc loss: 0.007485565823749453, policy loss: 6.349220771039921
Experience 16, Iter 47, disc loss: 0.007499236496798912, policy loss: 6.13805615911779
Experience 16, Iter 48, disc loss: 0.007776886720710708, policy loss: 6.0571913030717255
Experience 16, Iter 49, disc loss: 0.007935062249783463, policy loss: 6.112900412384685
Experience 16, Iter 50, disc loss: 0.007910240640750625, policy loss: 5.972736231661273
Experience 16, Iter 51, disc loss: 0.008085661437960051, policy loss: 6.22683545618138
Experience 16, Iter 52, disc loss: 0.00794480738748927, policy loss: 6.373497750971518
Experience 16, Iter 53, disc loss: 0.007675774694633833, policy loss: 6.03130504696678
Experience 16, Iter 54, disc loss: 0.007763333149714309, policy loss: 5.880166161405683
Experience 16, Iter 55, disc loss: 0.007686965223483881, policy loss: 6.167552276573597
Experience 16, Iter 56, disc loss: 0.007846301886310615, policy loss: 5.87347146487494
Experience 16, Iter 57, disc loss: 0.007422174240074679, policy loss: 6.139495415352513
Experience 16, Iter 58, disc loss: 0.008037981625846305, policy loss: 5.758949354471893
Experience 16, Iter 59, disc loss: 0.008189749991304436, policy loss: 6.020284918562629
Experience 16, Iter 60, disc loss: 0.007457045888927049, policy loss: 6.433621554803005
Experience 16, Iter 61, disc loss: 0.007549922595310937, policy loss: 6.290959109908318
Experience 16, Iter 62, disc loss: 0.00816139021108105, policy loss: 5.888754102176093
Experience 16, Iter 63, disc loss: 0.007728575740434694, policy loss: 6.164973331535924
Experience 16, Iter 64, disc loss: 0.007538393883170532, policy loss: 6.174093712996836
Experience 16, Iter 65, disc loss: 0.007717290353245478, policy loss: 6.290433846015738
Experience 16, Iter 66, disc loss: 0.006892420353713917, policy loss: 6.321313291810817
Experience 16, Iter 67, disc loss: 0.007545216281017706, policy loss: 6.013620852559612
Experience 16, Iter 68, disc loss: 0.00766124683014616, policy loss: 5.908200022206046
Experience 16, Iter 69, disc loss: 0.0071125315007883, policy loss: 6.261316845293189
Experience 16, Iter 70, disc loss: 0.007777148154935711, policy loss: 5.853397260361148
Experience 16, Iter 71, disc loss: 0.007971690473346914, policy loss: 5.838894867713692
Experience 16, Iter 72, disc loss: 0.007594658826959682, policy loss: 5.992264369952155
Experience 16, Iter 73, disc loss: 0.007174879116407507, policy loss: 6.369542356418203
Experience 16, Iter 74, disc loss: 0.007413078957530979, policy loss: 5.988412296312369
Experience 16, Iter 75, disc loss: 0.007771049970289502, policy loss: 5.968277310116193
Experience 16, Iter 76, disc loss: 0.007465651145649686, policy loss: 6.404484649110002
Experience 16, Iter 77, disc loss: 0.008187899647835903, policy loss: 5.755116846661812
Experience 16, Iter 78, disc loss: 0.0076665954266466815, policy loss: 5.859536746870341
Experience 16, Iter 79, disc loss: 0.007897821473724899, policy loss: 5.822346363626534
Experience 16, Iter 80, disc loss: 0.0076155351991502186, policy loss: 6.216280054212784
Experience 16, Iter 81, disc loss: 0.007217722661524963, policy loss: 6.605485760379349
Experience 16, Iter 82, disc loss: 0.0074418801046747165, policy loss: 6.135021057529176
Experience 16, Iter 83, disc loss: 0.00794590259777112, policy loss: 5.88768725894802
Experience 16, Iter 84, disc loss: 0.007462966371901521, policy loss: 6.196696028960761
Experience 16, Iter 85, disc loss: 0.007517590134288244, policy loss: 6.026714498080791
Experience 16, Iter 86, disc loss: 0.00728229437221121, policy loss: 6.284384705767232
Experience 16, Iter 87, disc loss: 0.007689046133405194, policy loss: 6.052794013927642
Experience 16, Iter 88, disc loss: 0.007321481256029282, policy loss: 6.321570478556516
Experience 16, Iter 89, disc loss: 0.0076554852010529235, policy loss: 6.067134909638693
Experience 16, Iter 90, disc loss: 0.007509501358047349, policy loss: 5.857516184164635
Experience 16, Iter 91, disc loss: 0.00673671126918549, policy loss: 6.080141820270075
Experience 16, Iter 92, disc loss: 0.007606948602021764, policy loss: 5.794482079020977
Experience 16, Iter 93, disc loss: 0.007432241979254848, policy loss: 5.787808315377407
Experience 16, Iter 94, disc loss: 0.0073099618772314665, policy loss: 6.200268691014469
Experience 16, Iter 95, disc loss: 0.007745703149874427, policy loss: 5.793444745473221
Experience 16, Iter 96, disc loss: 0.007235413579579807, policy loss: 6.206143370315607
Experience 16, Iter 97, disc loss: 0.007791415805799232, policy loss: 6.0472753459850885
Experience 16, Iter 98, disc loss: 0.00816399837069346, policy loss: 5.86584916563869
Experience 16, Iter 99, disc loss: 0.007651114545264168, policy loss: 6.246646358024555
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.2108],
        [1.8610],
        [0.0400]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0267, 0.2179, 1.8396, 0.0359, 0.0244, 5.4120]],

        [[0.0267, 0.2179, 1.8396, 0.0359, 0.0244, 5.4120]],

        [[0.0267, 0.2179, 1.8396, 0.0359, 0.0244, 5.4120]],

        [[0.0267, 0.2179, 1.8396, 0.0359, 0.0244, 5.4120]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0215, 0.8431, 7.4438, 0.1600], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0215, 0.8431, 7.4438, 0.1600])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.788
Iter 2/2000 - Loss: 3.705
Iter 3/2000 - Loss: 3.539
Iter 4/2000 - Loss: 3.471
Iter 5/2000 - Loss: 3.435
Iter 6/2000 - Loss: 3.322
Iter 7/2000 - Loss: 3.186
Iter 8/2000 - Loss: 3.069
Iter 9/2000 - Loss: 2.944
Iter 10/2000 - Loss: 2.794
Iter 11/2000 - Loss: 2.630
Iter 12/2000 - Loss: 2.462
Iter 13/2000 - Loss: 2.285
Iter 14/2000 - Loss: 2.089
Iter 15/2000 - Loss: 1.874
Iter 16/2000 - Loss: 1.648
Iter 17/2000 - Loss: 1.417
Iter 18/2000 - Loss: 1.183
Iter 19/2000 - Loss: 0.942
Iter 20/2000 - Loss: 0.690
Iter 1981/2000 - Loss: -6.885
Iter 1982/2000 - Loss: -6.885
Iter 1983/2000 - Loss: -6.885
Iter 1984/2000 - Loss: -6.885
Iter 1985/2000 - Loss: -6.886
Iter 1986/2000 - Loss: -6.886
Iter 1987/2000 - Loss: -6.886
Iter 1988/2000 - Loss: -6.886
Iter 1989/2000 - Loss: -6.886
Iter 1990/2000 - Loss: -6.886
Iter 1991/2000 - Loss: -6.886
Iter 1992/2000 - Loss: -6.886
Iter 1993/2000 - Loss: -6.886
Iter 1994/2000 - Loss: -6.886
Iter 1995/2000 - Loss: -6.886
Iter 1996/2000 - Loss: -6.886
Iter 1997/2000 - Loss: -6.886
Iter 1998/2000 - Loss: -6.886
Iter 1999/2000 - Loss: -6.886
Iter 2000/2000 - Loss: -6.886
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0030],
        [0.0003]])
Lengthscale: tensor([[[14.0042,  9.5568, 18.7591,  4.7446,  7.9043, 63.9695]],

        [[19.2244, 34.9884,  8.0251,  1.2278,  2.2153, 23.5131]],

        [[21.4874, 29.8021,  8.7161,  1.0300,  1.0499, 26.9230]],

        [[17.0789, 32.3182, 12.7591,  1.0887,  4.2208, 46.9070]]])
Signal Variance: tensor([ 0.1516,  2.0857, 18.1944,  0.4297])
Estimated target variance: tensor([0.0215, 0.8431, 7.4438, 0.1600])
N: 170
Signal to noise ratio: tensor([21.9620, 78.2493, 78.1154, 36.7610])
Bound on condition number: tensor([  81997.0874, 1040904.1680, 1037342.8605,  229734.3164])
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 0.007671028210686717, policy loss: 5.8945343579199285
Experience 17, Iter 1, disc loss: 0.007006869946585269, policy loss: 6.383724996196086
Experience 17, Iter 2, disc loss: 0.0071473686578200045, policy loss: 6.1461492278943926
Experience 17, Iter 3, disc loss: 0.006782787982289809, policy loss: 6.2518297086732595
Experience 17, Iter 4, disc loss: 0.007113787378321346, policy loss: 6.28878053255486
Experience 17, Iter 5, disc loss: 0.007084160735544594, policy loss: 5.920098947830194
Experience 17, Iter 6, disc loss: 0.007823426479140031, policy loss: 5.852266651994669
Experience 17, Iter 7, disc loss: 0.006742647726926442, policy loss: 6.483374306436715
Experience 17, Iter 8, disc loss: 0.007126714312734771, policy loss: 6.271276785571914
Experience 17, Iter 9, disc loss: 0.00729837253295477, policy loss: 6.151581946825225
Experience 17, Iter 10, disc loss: 0.0074563160969427465, policy loss: 6.011119028499186
Experience 17, Iter 11, disc loss: 0.0066201037852774425, policy loss: 6.320827313976933
Experience 17, Iter 12, disc loss: 0.007328569409251551, policy loss: 6.185552845846866
Experience 17, Iter 13, disc loss: 0.0069385850599237315, policy loss: 6.402313058826271
Experience 17, Iter 14, disc loss: 0.00738079863960543, policy loss: 5.951265576993831
Experience 17, Iter 15, disc loss: 0.0070440198076727675, policy loss: 5.88871048669284
Experience 17, Iter 16, disc loss: 0.006932866632417932, policy loss: 6.073106278034033
Experience 17, Iter 17, disc loss: 0.007009155478286708, policy loss: 6.147381241947345
Experience 17, Iter 18, disc loss: 0.006882661384064103, policy loss: 5.958645727891929
Experience 17, Iter 19, disc loss: 0.007030081239691501, policy loss: 6.116518797420142
Experience 17, Iter 20, disc loss: 0.00725888294152606, policy loss: 6.049983146576868
Experience 17, Iter 21, disc loss: 0.0072641777811175015, policy loss: 5.878790710475695
Experience 17, Iter 22, disc loss: 0.00685031764078607, policy loss: 6.311517174736396
Experience 17, Iter 23, disc loss: 0.00682710139716361, policy loss: 6.516634853962169
Experience 17, Iter 24, disc loss: 0.007209533151885413, policy loss: 6.053208699969042
Experience 17, Iter 25, disc loss: 0.0077129439858326, policy loss: 5.718557833313497
Experience 17, Iter 26, disc loss: 0.00764644704990486, policy loss: 5.9239935969848165
Experience 17, Iter 27, disc loss: 0.0073466502428932694, policy loss: 5.950142841666784
Experience 17, Iter 28, disc loss: 0.007159959888493585, policy loss: 6.259907400629926
Experience 17, Iter 29, disc loss: 0.007343762769213999, policy loss: 6.111668021739453
Experience 17, Iter 30, disc loss: 0.0068442187754911635, policy loss: 6.252667789283112
Experience 17, Iter 31, disc loss: 0.007246632175318633, policy loss: 5.894523994927446
Experience 17, Iter 32, disc loss: 0.0070341632661427705, policy loss: 6.020030048820646
Experience 17, Iter 33, disc loss: 0.006967575306516572, policy loss: 6.140374902385359
Experience 17, Iter 34, disc loss: 0.006996169942889851, policy loss: 6.202440565770814
Experience 17, Iter 35, disc loss: 0.006737843128153289, policy loss: 6.534951736371809
Experience 17, Iter 36, disc loss: 0.006942532806024276, policy loss: 5.982623698778123
Experience 17, Iter 37, disc loss: 0.007193005713925722, policy loss: 5.991305827992158
Experience 17, Iter 38, disc loss: 0.006845389564583982, policy loss: 5.969687365538864
Experience 17, Iter 39, disc loss: 0.006711371015709936, policy loss: 6.108605688299904
Experience 17, Iter 40, disc loss: 0.0063302184857912695, policy loss: 6.608887245004496
Experience 17, Iter 41, disc loss: 0.0062308983319442224, policy loss: 6.866287686815145
Experience 17, Iter 42, disc loss: 0.007118117647456016, policy loss: 5.918857343329428
Experience 17, Iter 43, disc loss: 0.006452263599568965, policy loss: 6.15937440610509
Experience 17, Iter 44, disc loss: 0.006411568593212221, policy loss: 6.499147333195925
Experience 17, Iter 45, disc loss: 0.0059864450799658984, policy loss: 6.64677389284255
Experience 17, Iter 46, disc loss: 0.0059298229960751715, policy loss: 6.842282505225368
Experience 17, Iter 47, disc loss: 0.006356438153088407, policy loss: 6.18892082582966
Experience 17, Iter 48, disc loss: 0.00660332342651665, policy loss: 6.164289852249458
Experience 17, Iter 49, disc loss: 0.006535623191286692, policy loss: 6.200096474740224
Experience 17, Iter 50, disc loss: 0.006225618081709064, policy loss: 6.440956287855116
Experience 17, Iter 51, disc loss: 0.005930660025106033, policy loss: 6.721342859441033
Experience 17, Iter 52, disc loss: 0.006603392130358248, policy loss: 6.050923555203158
Experience 17, Iter 53, disc loss: 0.006508129385178798, policy loss: 6.246051076854689
Experience 17, Iter 54, disc loss: 0.006305910867855088, policy loss: 6.225626213880043
Experience 17, Iter 55, disc loss: 0.006646045313393678, policy loss: 6.051882549220863
Experience 17, Iter 56, disc loss: 0.00619044949301038, policy loss: 6.392217848115097
Experience 17, Iter 57, disc loss: 0.006246572899531381, policy loss: 6.448863610856756
Experience 17, Iter 58, disc loss: 0.0059839601860201055, policy loss: 6.560669424774046
Experience 17, Iter 59, disc loss: 0.0058688189673288685, policy loss: 6.458308426382358
Experience 17, Iter 60, disc loss: 0.006258546187308579, policy loss: 6.330517373489259
Experience 17, Iter 61, disc loss: 0.005744336212074946, policy loss: 6.49533196052235
Experience 17, Iter 62, disc loss: 0.005771580341925876, policy loss: 6.3432774308572935
Experience 17, Iter 63, disc loss: 0.005703007182742706, policy loss: 6.287769377615906
Experience 17, Iter 64, disc loss: 0.005620319251111429, policy loss: 6.577698974186957
Experience 17, Iter 65, disc loss: 0.006317911383867027, policy loss: 6.124492566491403
Experience 17, Iter 66, disc loss: 0.0060296589082958355, policy loss: 6.084120363597602
Experience 17, Iter 67, disc loss: 0.005714425630062844, policy loss: 6.386667193468074
Experience 17, Iter 68, disc loss: 0.0062169415729254355, policy loss: 6.0901340138835796
Experience 17, Iter 69, disc loss: 0.006493999009828654, policy loss: 6.175363727133533
Experience 17, Iter 70, disc loss: 0.006769057165028263, policy loss: 5.9741857963035505
Experience 17, Iter 71, disc loss: 0.00659033973282774, policy loss: 6.425634227540579
Experience 17, Iter 72, disc loss: 0.00645670395145328, policy loss: 6.496634523067514
Experience 17, Iter 73, disc loss: 0.006299232855592835, policy loss: 6.321934852619091
Experience 17, Iter 74, disc loss: 0.006406008679366599, policy loss: 5.960969882313189
Experience 17, Iter 75, disc loss: 0.006764107159454516, policy loss: 5.95110718864418
Experience 17, Iter 76, disc loss: 0.006285517684362167, policy loss: 5.950648331320592
Experience 17, Iter 77, disc loss: 0.006225064361219853, policy loss: 6.496640464523666
Experience 17, Iter 78, disc loss: 0.006354732833404009, policy loss: 6.336768218313326
Experience 17, Iter 79, disc loss: 0.006134548009162754, policy loss: 6.297123300606726
Experience 17, Iter 80, disc loss: 0.005951381334073762, policy loss: 6.4115015256825485
Experience 17, Iter 81, disc loss: 0.005759320414284223, policy loss: 6.438746459287957
Experience 17, Iter 82, disc loss: 0.0062070252972639735, policy loss: 6.101943462577725
Experience 17, Iter 83, disc loss: 0.005722224371504497, policy loss: 6.301401918187998
Experience 17, Iter 84, disc loss: 0.005814476885762532, policy loss: 6.501734121715573
Experience 17, Iter 85, disc loss: 0.005944199083675066, policy loss: 6.173899590040854
Experience 17, Iter 86, disc loss: 0.005914562064491807, policy loss: 6.19350700531037
Experience 17, Iter 87, disc loss: 0.006411625803637618, policy loss: 6.224726757944328
Experience 17, Iter 88, disc loss: 0.006323301181104864, policy loss: 6.237219335190748
Experience 17, Iter 89, disc loss: 0.00592261237004967, policy loss: 6.8341082484199
Experience 17, Iter 90, disc loss: 0.00658952095615759, policy loss: 6.284754128908019
Experience 17, Iter 91, disc loss: 0.006707562173892982, policy loss: 6.15598043200502
Experience 17, Iter 92, disc loss: 0.006412567890128243, policy loss: 6.086095933514328
Experience 17, Iter 93, disc loss: 0.006088667006509682, policy loss: 6.222726355545135
Experience 17, Iter 94, disc loss: 0.006379881259938361, policy loss: 5.971590356902034
Experience 17, Iter 95, disc loss: 0.005934398770095757, policy loss: 6.259927860494292
Experience 17, Iter 96, disc loss: 0.006259672192641445, policy loss: 6.097257365207986
Experience 17, Iter 97, disc loss: 0.006103799941759351, policy loss: 6.5938841371073185
Experience 17, Iter 98, disc loss: 0.0062649197677994515, policy loss: 6.056492721388064
Experience 17, Iter 99, disc loss: 0.006279742654710757, policy loss: 6.3303016579677305
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.2164],
        [1.8963],
        [0.0406]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0255, 0.2153, 1.8713, 0.0362, 0.0248, 5.5168]],

        [[0.0255, 0.2153, 1.8713, 0.0362, 0.0248, 5.5168]],

        [[0.0255, 0.2153, 1.8713, 0.0362, 0.0248, 5.5168]],

        [[0.0255, 0.2153, 1.8713, 0.0362, 0.0248, 5.5168]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0212, 0.8654, 7.5851, 0.1625], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0212, 0.8654, 7.5851, 0.1625])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.800
Iter 2/2000 - Loss: 3.719
Iter 3/2000 - Loss: 3.538
Iter 4/2000 - Loss: 3.462
Iter 5/2000 - Loss: 3.416
Iter 6/2000 - Loss: 3.290
Iter 7/2000 - Loss: 3.142
Iter 8/2000 - Loss: 3.011
Iter 9/2000 - Loss: 2.874
Iter 10/2000 - Loss: 2.711
Iter 11/2000 - Loss: 2.536
Iter 12/2000 - Loss: 2.358
Iter 13/2000 - Loss: 2.172
Iter 14/2000 - Loss: 1.969
Iter 15/2000 - Loss: 1.747
Iter 16/2000 - Loss: 1.513
Iter 17/2000 - Loss: 1.275
Iter 18/2000 - Loss: 1.036
Iter 19/2000 - Loss: 0.791
Iter 20/2000 - Loss: 0.537
Iter 1981/2000 - Loss: -6.991
Iter 1982/2000 - Loss: -6.991
Iter 1983/2000 - Loss: -6.991
Iter 1984/2000 - Loss: -6.991
Iter 1985/2000 - Loss: -6.991
Iter 1986/2000 - Loss: -6.991
Iter 1987/2000 - Loss: -6.992
Iter 1988/2000 - Loss: -6.992
Iter 1989/2000 - Loss: -6.992
Iter 1990/2000 - Loss: -6.992
Iter 1991/2000 - Loss: -6.992
Iter 1992/2000 - Loss: -6.992
Iter 1993/2000 - Loss: -6.992
Iter 1994/2000 - Loss: -6.992
Iter 1995/2000 - Loss: -6.992
Iter 1996/2000 - Loss: -6.992
Iter 1997/2000 - Loss: -6.992
Iter 1998/2000 - Loss: -6.992
Iter 1999/2000 - Loss: -6.992
Iter 2000/2000 - Loss: -6.992
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0029],
        [0.0003]])
Lengthscale: tensor([[[13.5489,  9.6108, 19.1588,  4.4431,  7.8654, 64.4644]],

        [[18.8280, 34.4163,  8.0531,  1.2304,  2.2975, 24.0420]],

        [[20.8239, 28.0120,  8.8281,  1.0362,  1.0375, 27.5122]],

        [[16.3962, 31.5583, 11.7768,  1.0890,  4.1693, 45.6602]]])
Signal Variance: tensor([ 0.1530,  2.1918, 19.3889,  0.3914])
Estimated target variance: tensor([0.0212, 0.8654, 7.5851, 0.1625])
N: 180
Signal to noise ratio: tensor([21.9682, 80.0158, 81.8456, 35.2652])
Bound on condition number: tensor([  86869.5491, 1152454.9959, 1205766.4696,  223854.8292])
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 0.006891440746228531, policy loss: 5.928070477877177
Experience 18, Iter 1, disc loss: 0.005967597481194977, policy loss: 6.688806736145026
Experience 18, Iter 2, disc loss: 0.006293102140380646, policy loss: 6.244437227897839
Experience 18, Iter 3, disc loss: 0.00627788941733551, policy loss: 6.169568258897394
Experience 18, Iter 4, disc loss: 0.006154404673761568, policy loss: 6.164572053462857
Experience 18, Iter 5, disc loss: 0.006052525879139722, policy loss: 6.291339810824004
Experience 18, Iter 6, disc loss: 0.005426515284423056, policy loss: 6.493622423317326
Experience 18, Iter 7, disc loss: 0.0058390580013430605, policy loss: 6.600442769847386
Experience 18, Iter 8, disc loss: 0.005929414603141887, policy loss: 6.856001024246117
Experience 18, Iter 9, disc loss: 0.005861629880233155, policy loss: 6.921471349293879
Experience 18, Iter 10, disc loss: 0.006291704929578113, policy loss: 6.152467851393203
Experience 18, Iter 11, disc loss: 0.006254989798622222, policy loss: 5.954571319606303
Experience 18, Iter 12, disc loss: 0.006166973646828227, policy loss: 6.091591113668538
Experience 18, Iter 13, disc loss: 0.005931069801615606, policy loss: 6.124339644345512
Experience 18, Iter 14, disc loss: 0.005598132276396814, policy loss: 6.737902469492752
Experience 18, Iter 15, disc loss: 0.006167302573194963, policy loss: 6.125771570547623
Experience 18, Iter 16, disc loss: 0.005402345340017389, policy loss: 6.74334659372686
Experience 18, Iter 17, disc loss: 0.006084015820721262, policy loss: 6.046566231285389
Experience 18, Iter 18, disc loss: 0.005995712495633417, policy loss: 6.744537957111424
Experience 18, Iter 19, disc loss: 0.006273098179661193, policy loss: 6.2347668883242475
Experience 18, Iter 20, disc loss: 0.006053789406714527, policy loss: 6.15612719986642
Experience 18, Iter 21, disc loss: 0.005836269865900677, policy loss: 6.241893680200575
Experience 18, Iter 22, disc loss: 0.005840678288268932, policy loss: 6.487931898432463
Experience 18, Iter 23, disc loss: 0.005758015420668059, policy loss: 6.16469901667981
Experience 18, Iter 24, disc loss: 0.00547458047380308, policy loss: 6.653220244783321
Experience 18, Iter 25, disc loss: 0.0052899009250053945, policy loss: 6.825133168400809
Experience 18, Iter 26, disc loss: 0.0060447591835545355, policy loss: 6.0136322876750254
Experience 18, Iter 27, disc loss: 0.005882360973082391, policy loss: 6.278730589904886
Experience 18, Iter 28, disc loss: 0.0060012348871076975, policy loss: 6.635431306675806
Experience 18, Iter 29, disc loss: 0.006031228539655932, policy loss: 6.298064488044381
Experience 18, Iter 30, disc loss: 0.006019277372486523, policy loss: 6.395453287455423
Experience 18, Iter 31, disc loss: 0.005771745185493461, policy loss: 6.4986356879280684
Experience 18, Iter 32, disc loss: 0.0061644775179806426, policy loss: 6.079722989465649
Experience 18, Iter 33, disc loss: 0.005858165173530355, policy loss: 6.123477086651789
Experience 18, Iter 34, disc loss: 0.00569049106882538, policy loss: 6.513890322482803
Experience 18, Iter 35, disc loss: 0.005957199036828878, policy loss: 6.490430756270369
Experience 18, Iter 36, disc loss: 0.005529525529904891, policy loss: 6.544826813901377
Experience 18, Iter 37, disc loss: 0.005824885287745369, policy loss: 6.422774155029113
Experience 18, Iter 38, disc loss: 0.005722426004240354, policy loss: 6.455383569007072
Experience 18, Iter 39, disc loss: 0.005965072553831911, policy loss: 6.223275926321248
Experience 18, Iter 40, disc loss: 0.006045841705640395, policy loss: 6.2967410591649395
Experience 18, Iter 41, disc loss: 0.005766393044234623, policy loss: 6.278976188359575
Experience 18, Iter 42, disc loss: 0.005808163069170948, policy loss: 6.467625466192766
Experience 18, Iter 43, disc loss: 0.005880131156013756, policy loss: 6.095054852657909
Experience 18, Iter 44, disc loss: 0.0063723560402832845, policy loss: 6.115989731574105
Experience 18, Iter 45, disc loss: 0.0059467874794201184, policy loss: 5.982442028674128
Experience 18, Iter 46, disc loss: 0.005828955303904561, policy loss: 6.222959608737291
Experience 18, Iter 47, disc loss: 0.00582326418415893, policy loss: 6.067368349599384
Experience 18, Iter 48, disc loss: 0.0057654176602571555, policy loss: 6.34439414310087
Experience 18, Iter 49, disc loss: 0.0062747421490960825, policy loss: 5.964242861778899
Experience 18, Iter 50, disc loss: 0.005602957443069146, policy loss: 6.562606201202707
Experience 18, Iter 51, disc loss: 0.00592138044282195, policy loss: 6.430432055646653
Experience 18, Iter 52, disc loss: 0.006033898563958169, policy loss: 6.1943708902153025
Experience 18, Iter 53, disc loss: 0.005762392587797659, policy loss: 6.304432520123692
Experience 18, Iter 54, disc loss: 0.005779410312236711, policy loss: 6.187241345536929
Experience 18, Iter 55, disc loss: 0.006079081680574893, policy loss: 6.19482089477326
Experience 18, Iter 56, disc loss: 0.005558079831406767, policy loss: 6.266108627972512
Experience 18, Iter 57, disc loss: 0.005584331147215905, policy loss: 6.4825206159643916
Experience 18, Iter 58, disc loss: 0.005676459930561146, policy loss: 6.545331260809773
Experience 18, Iter 59, disc loss: 0.005901871613122906, policy loss: 6.364354918677073
Experience 18, Iter 60, disc loss: 0.005590248399253723, policy loss: 6.591820954693127
Experience 18, Iter 61, disc loss: 0.006085401522416529, policy loss: 6.414501744359136
Experience 18, Iter 62, disc loss: 0.006196307005468365, policy loss: 6.216499611703188
Experience 18, Iter 63, disc loss: 0.005729691697043585, policy loss: 6.518987112197957
Experience 18, Iter 64, disc loss: 0.006303828577389739, policy loss: 5.934595008077668
Experience 18, Iter 65, disc loss: 0.005613258895576605, policy loss: 6.750566585545373
Experience 18, Iter 66, disc loss: 0.0059026410704308635, policy loss: 6.2092551632019966
Experience 18, Iter 67, disc loss: 0.005696641601856107, policy loss: 6.261350196019665
Experience 18, Iter 68, disc loss: 0.005379585317142208, policy loss: 6.4015416888202115
Experience 18, Iter 69, disc loss: 0.005383135116996939, policy loss: 6.348108366611537
Experience 18, Iter 70, disc loss: 0.0056040117473684335, policy loss: 6.3292344708279344
Experience 18, Iter 71, disc loss: 0.005434946381607865, policy loss: 6.574719174331936
Experience 18, Iter 72, disc loss: 0.0055332100616813035, policy loss: 6.275175831330873
Experience 18, Iter 73, disc loss: 0.005435406011303517, policy loss: 6.72413163381287
Experience 18, Iter 74, disc loss: 0.005380055179524843, policy loss: 6.262865330018583
Experience 18, Iter 75, disc loss: 0.005928831680325337, policy loss: 6.064329843713539
Experience 18, Iter 76, disc loss: 0.005756812979543119, policy loss: 6.3384491771487586
Experience 18, Iter 77, disc loss: 0.005707395837887257, policy loss: 6.178567067993752
Experience 18, Iter 78, disc loss: 0.005954770790556656, policy loss: 6.279937153744875
Experience 18, Iter 79, disc loss: 0.0059096215005533925, policy loss: 6.323548648208398
Experience 18, Iter 80, disc loss: 0.006169163247758045, policy loss: 6.332685142278246
Experience 18, Iter 81, disc loss: 0.00595430553836572, policy loss: 6.50446134039575
Experience 18, Iter 82, disc loss: 0.005336472968987075, policy loss: 6.614211238540154
Experience 18, Iter 83, disc loss: 0.005238921811608163, policy loss: 6.5896537699143245
Experience 18, Iter 84, disc loss: 0.005443127989521176, policy loss: 6.167622735505718
Experience 18, Iter 85, disc loss: 0.0050932165099107, policy loss: 6.775042072103991
Experience 18, Iter 86, disc loss: 0.005452782803953227, policy loss: 6.443521794144762
Experience 18, Iter 87, disc loss: 0.0057768346526548536, policy loss: 6.072502566701091
Experience 18, Iter 88, disc loss: 0.005866784023870007, policy loss: 6.616719066500906
Experience 18, Iter 89, disc loss: 0.0054647424734240455, policy loss: 6.494349860106657
Experience 18, Iter 90, disc loss: 0.005525245472667155, policy loss: 6.750711004850411
Experience 18, Iter 91, disc loss: 0.00500113001448857, policy loss: 6.456465530271961
Experience 18, Iter 92, disc loss: 0.005173280047222244, policy loss: 6.796821905200936
Experience 18, Iter 93, disc loss: 0.005332621830178039, policy loss: 6.66582762625853
Experience 18, Iter 94, disc loss: 0.00541214897789319, policy loss: 6.24781732621019
Experience 18, Iter 95, disc loss: 0.005861894180713141, policy loss: 6.034383599122035
Experience 18, Iter 96, disc loss: 0.005640045151770167, policy loss: 6.180591646136342
Experience 18, Iter 97, disc loss: 0.0053535623726281515, policy loss: 6.447478359773414
Experience 18, Iter 98, disc loss: 0.005685342588546263, policy loss: 6.67555157486952
Experience 18, Iter 99, disc loss: 0.005593598833182372, policy loss: 6.425892825489026
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2221],
        [1.9267],
        [0.0411]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0243, 0.2126, 1.8975, 0.0364, 0.0252, 5.6347]],

        [[0.0243, 0.2126, 1.8975, 0.0364, 0.0252, 5.6347]],

        [[0.0243, 0.2126, 1.8975, 0.0364, 0.0252, 5.6347]],

        [[0.0243, 0.2126, 1.8975, 0.0364, 0.0252, 5.6347]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0208, 0.8886, 7.7067, 0.1644], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0208, 0.8886, 7.7067, 0.1644])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.801
Iter 2/2000 - Loss: 3.716
Iter 3/2000 - Loss: 3.525
Iter 4/2000 - Loss: 3.441
Iter 5/2000 - Loss: 3.388
Iter 6/2000 - Loss: 3.256
Iter 7/2000 - Loss: 3.100
Iter 8/2000 - Loss: 2.960
Iter 9/2000 - Loss: 2.813
Iter 10/2000 - Loss: 2.641
Iter 11/2000 - Loss: 2.457
Iter 12/2000 - Loss: 2.273
Iter 13/2000 - Loss: 2.081
Iter 14/2000 - Loss: 1.872
Iter 15/2000 - Loss: 1.644
Iter 16/2000 - Loss: 1.404
Iter 17/2000 - Loss: 1.159
Iter 18/2000 - Loss: 0.913
Iter 19/2000 - Loss: 0.664
Iter 20/2000 - Loss: 0.408
Iter 1981/2000 - Loss: -7.133
Iter 1982/2000 - Loss: -7.133
Iter 1983/2000 - Loss: -7.133
Iter 1984/2000 - Loss: -7.133
Iter 1985/2000 - Loss: -7.133
Iter 1986/2000 - Loss: -7.133
Iter 1987/2000 - Loss: -7.134
Iter 1988/2000 - Loss: -7.134
Iter 1989/2000 - Loss: -7.134
Iter 1990/2000 - Loss: -7.134
Iter 1991/2000 - Loss: -7.134
Iter 1992/2000 - Loss: -7.134
Iter 1993/2000 - Loss: -7.134
Iter 1994/2000 - Loss: -7.134
Iter 1995/2000 - Loss: -7.134
Iter 1996/2000 - Loss: -7.134
Iter 1997/2000 - Loss: -7.134
Iter 1998/2000 - Loss: -7.134
Iter 1999/2000 - Loss: -7.134
Iter 2000/2000 - Loss: -7.134
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[13.4463,  9.6709, 19.9624,  4.2459,  7.5717, 64.3572]],

        [[19.1505, 34.1310,  8.0241,  1.2325,  2.2983, 24.2988]],

        [[20.3448, 25.7537,  8.8183,  1.0546,  1.0260, 27.5363]],

        [[15.8337, 31.1916, 11.4965,  1.0789,  4.2481, 44.4532]]])
Signal Variance: tensor([ 0.1490,  2.1791, 19.6675,  0.3750])
Estimated target variance: tensor([0.0208, 0.8886, 7.7067, 0.1644])
N: 190
Signal to noise ratio: tensor([21.7885, 79.4777, 84.8671, 35.3868])
Bound on condition number: tensor([  90201.2990, 1200176.2305, 1368460.3185,  237924.1462])
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 0.005559679684123187, policy loss: 6.343837416592697
Experience 19, Iter 1, disc loss: 0.0058123614605936815, policy loss: 6.169366308620214
Experience 19, Iter 2, disc loss: 0.005328551427659745, policy loss: 6.333863327770871
Experience 19, Iter 3, disc loss: 0.005161058868331948, policy loss: 6.463673622087578
Experience 19, Iter 4, disc loss: 0.005487430273272481, policy loss: 6.3604863275382915
Experience 19, Iter 5, disc loss: 0.005609848749201497, policy loss: 6.243103032530703
Experience 19, Iter 6, disc loss: 0.0050093179180031215, policy loss: 6.674863965028444
Experience 19, Iter 7, disc loss: 0.005488571155181164, policy loss: 6.2263353716686805
Experience 19, Iter 8, disc loss: 0.005544902656734391, policy loss: 6.535430652832087
Experience 19, Iter 9, disc loss: 0.005067904687675532, policy loss: 6.77541080337217
Experience 19, Iter 10, disc loss: 0.005514468407265544, policy loss: 6.213326720464519
Experience 19, Iter 11, disc loss: 0.005544429704340933, policy loss: 6.530735048914503
Experience 19, Iter 12, disc loss: 0.0053047265275230435, policy loss: 6.129889647366268
Experience 19, Iter 13, disc loss: 0.005445159983959388, policy loss: 6.493575361799538
Experience 19, Iter 14, disc loss: 0.0056837443936588634, policy loss: 6.327758469211197
Experience 19, Iter 15, disc loss: 0.00517418117578974, policy loss: 6.7417972677167315
Experience 19, Iter 16, disc loss: 0.005231730016879738, policy loss: 6.515211852062338
Experience 19, Iter 17, disc loss: 0.00501081336232089, policy loss: 6.791472118709088
Experience 19, Iter 18, disc loss: 0.005308585509307427, policy loss: 6.31627319397689
Experience 19, Iter 19, disc loss: 0.004879261863666589, policy loss: 6.498130979318359
Experience 19, Iter 20, disc loss: 0.005237699098891058, policy loss: 6.474465019232853
Experience 19, Iter 21, disc loss: 0.005328051186940127, policy loss: 6.331537269806068
Experience 19, Iter 22, disc loss: 0.00554565802684369, policy loss: 6.383081493111396
Experience 19, Iter 23, disc loss: 0.0052851010900616426, policy loss: 6.3875268226400035
Experience 19, Iter 24, disc loss: 0.00519877722101118, policy loss: 6.270586642805215
Experience 19, Iter 25, disc loss: 0.005066062548531793, policy loss: 6.3681631778673395
Experience 19, Iter 26, disc loss: 0.004884137998008621, policy loss: 6.750584689857975
Experience 19, Iter 27, disc loss: 0.005158315857609515, policy loss: 6.5799675586149995
Experience 19, Iter 28, disc loss: 0.005251070622472964, policy loss: 6.673524187833146
Experience 19, Iter 29, disc loss: 0.0053222047401673064, policy loss: 6.674038235922061
Experience 19, Iter 30, disc loss: 0.0051709527891794105, policy loss: 6.64016327381164
Experience 19, Iter 31, disc loss: 0.0056193068118199265, policy loss: 6.426450454099475
Experience 19, Iter 32, disc loss: 0.005349646944328861, policy loss: 6.35324697469211
Experience 19, Iter 33, disc loss: 0.00475461346436738, policy loss: 6.710237905866551
Experience 19, Iter 34, disc loss: 0.004804281075487514, policy loss: 6.587128942335615
Experience 19, Iter 35, disc loss: 0.005039619372998388, policy loss: 6.2754856342325915
Experience 19, Iter 36, disc loss: 0.00530958493116028, policy loss: 6.161551048837307
Experience 19, Iter 37, disc loss: 0.005131704188202345, policy loss: 6.3728849635577625
Experience 19, Iter 38, disc loss: 0.005148035051597738, policy loss: 6.48095973374381
Experience 19, Iter 39, disc loss: 0.0055079689403766, policy loss: 6.217193989527603
Experience 19, Iter 40, disc loss: 0.005388894631684935, policy loss: 6.184433764148303
Experience 19, Iter 41, disc loss: 0.0048971857179946725, policy loss: 6.791455972677752
Experience 19, Iter 42, disc loss: 0.005080798044592601, policy loss: 6.676181170542474
Experience 19, Iter 43, disc loss: 0.0051912319960711155, policy loss: 6.468244372374637
Experience 19, Iter 44, disc loss: 0.0044827028162666895, policy loss: 6.79379096145653
Experience 19, Iter 45, disc loss: 0.0038854645425841806, policy loss: 7.616580382791705
Experience 19, Iter 46, disc loss: 0.004353280770596201, policy loss: 6.926762564509751
Experience 19, Iter 47, disc loss: 0.004383447194404443, policy loss: 6.80739131915502
Experience 19, Iter 48, disc loss: 0.004982732984517952, policy loss: 6.949958505425245
Experience 19, Iter 49, disc loss: 0.0054652099332830155, policy loss: 6.284577100480298
Experience 19, Iter 50, disc loss: 0.005311814669907454, policy loss: 6.409945794193348
Experience 19, Iter 51, disc loss: 0.004931664645556633, policy loss: 6.986269764530052
Experience 19, Iter 52, disc loss: 0.0057491211161979005, policy loss: 6.3015780624103
Experience 19, Iter 53, disc loss: 0.005221323684892582, policy loss: 6.145432633265859
Experience 19, Iter 54, disc loss: 0.004996437772059725, policy loss: 6.462011012968376
Experience 19, Iter 55, disc loss: 0.005261478912639304, policy loss: 6.3887295086448574
Experience 19, Iter 56, disc loss: 0.005169940525809462, policy loss: 6.520235282048747
Experience 19, Iter 57, disc loss: 0.00497240990616386, policy loss: 6.358965617290618
Experience 19, Iter 58, disc loss: 0.004756114155090638, policy loss: 6.701001435341864
Experience 19, Iter 59, disc loss: 0.005540449788229797, policy loss: 6.064081651009889
Experience 19, Iter 60, disc loss: 0.0051302674400594914, policy loss: 6.208078698078728
Experience 19, Iter 61, disc loss: 0.004927970383958111, policy loss: 6.567507932716987
Experience 19, Iter 62, disc loss: 0.005340536818060857, policy loss: 6.380970079175974
Experience 19, Iter 63, disc loss: 0.004943998583835323, policy loss: 6.647549072448376
Experience 19, Iter 64, disc loss: 0.004619086316920464, policy loss: 6.853412193944234
Experience 19, Iter 65, disc loss: 0.005273595923540611, policy loss: 6.1868821364297135
Experience 19, Iter 66, disc loss: 0.005006600365887881, policy loss: 6.535794643523653
Experience 19, Iter 67, disc loss: 0.005126653111718961, policy loss: 6.316998337774473
Experience 19, Iter 68, disc loss: 0.005216885146689136, policy loss: 6.326407467219314
Experience 19, Iter 69, disc loss: 0.005062172782070697, policy loss: 6.6174799186012585
Experience 19, Iter 70, disc loss: 0.005339857648713888, policy loss: 6.279746824039357
Experience 19, Iter 71, disc loss: 0.005056164155924735, policy loss: 6.584250756867913
Experience 19, Iter 72, disc loss: 0.004869388335589938, policy loss: 6.754396368393927
Experience 19, Iter 73, disc loss: 0.00483129719524413, policy loss: 6.573897717922026
Experience 19, Iter 74, disc loss: 0.005220673640275817, policy loss: 6.324112230229153
Experience 19, Iter 75, disc loss: 0.005267163714913261, policy loss: 6.2313665802676965
Experience 19, Iter 76, disc loss: 0.005330064410440346, policy loss: 6.448131015908495
Experience 19, Iter 77, disc loss: 0.005194305805139349, policy loss: 6.520498951765726
Experience 19, Iter 78, disc loss: 0.005326827495572075, policy loss: 6.217559658203825
Experience 19, Iter 79, disc loss: 0.0055711935718531155, policy loss: 6.356128876002026
Experience 19, Iter 80, disc loss: 0.005405806543535735, policy loss: 6.8359184869125045
Experience 19, Iter 81, disc loss: 0.005384978881231375, policy loss: 6.398243861783378
Experience 19, Iter 82, disc loss: 0.004912750019512809, policy loss: 6.515791888036389
Experience 19, Iter 83, disc loss: 0.005035211050658052, policy loss: 6.395130544879255
Experience 19, Iter 84, disc loss: 0.005284813197619937, policy loss: 6.210237879914694
Experience 19, Iter 85, disc loss: 0.005720146437505606, policy loss: 6.542351913715017
Experience 19, Iter 86, disc loss: 0.005890444373512855, policy loss: 6.00254872145992
Experience 19, Iter 87, disc loss: 0.005283506181823537, policy loss: 6.45404041945251
Experience 19, Iter 88, disc loss: 0.005147509914560435, policy loss: 7.226159480238469
Experience 19, Iter 89, disc loss: 0.005633900570013028, policy loss: 6.213704712467921
Experience 19, Iter 90, disc loss: 0.005566305036386878, policy loss: 6.081603851287706
Experience 19, Iter 91, disc loss: 0.004956589010972356, policy loss: 6.329118499415311
Experience 19, Iter 92, disc loss: 0.005329301770785868, policy loss: 6.360138177861567
Experience 19, Iter 93, disc loss: 0.005046687258747287, policy loss: 6.358407210875434
Experience 19, Iter 94, disc loss: 0.004894134448915604, policy loss: 6.578556948956428
Experience 19, Iter 95, disc loss: 0.005092542212873991, policy loss: 6.677190806311259
Experience 19, Iter 96, disc loss: 0.004940389861103969, policy loss: 6.524573175292756
Experience 19, Iter 97, disc loss: 0.00526415607192374, policy loss: 6.4490687268645015
Experience 19, Iter 98, disc loss: 0.004974886665574952, policy loss: 6.439984780728922
Experience 19, Iter 99, disc loss: 0.005035703634651227, policy loss: 6.800941191419684
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2267],
        [1.9265],
        [0.0416]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0236, 0.2124, 1.9133, 0.0368, 0.0261, 5.7688]],

        [[0.0236, 0.2124, 1.9133, 0.0368, 0.0261, 5.7688]],

        [[0.0236, 0.2124, 1.9133, 0.0368, 0.0261, 5.7688]],

        [[0.0236, 0.2124, 1.9133, 0.0368, 0.0261, 5.7688]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0208, 0.9068, 7.7060, 0.1666], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0208, 0.9068, 7.7060, 0.1666])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.831
Iter 2/2000 - Loss: 3.769
Iter 3/2000 - Loss: 3.560
Iter 4/2000 - Loss: 3.488
Iter 5/2000 - Loss: 3.441
Iter 6/2000 - Loss: 3.310
Iter 7/2000 - Loss: 3.153
Iter 8/2000 - Loss: 3.014
Iter 9/2000 - Loss: 2.871
Iter 10/2000 - Loss: 2.700
Iter 11/2000 - Loss: 2.512
Iter 12/2000 - Loss: 2.320
Iter 13/2000 - Loss: 2.124
Iter 14/2000 - Loss: 1.913
Iter 15/2000 - Loss: 1.683
Iter 16/2000 - Loss: 1.435
Iter 17/2000 - Loss: 1.180
Iter 18/2000 - Loss: 0.924
Iter 19/2000 - Loss: 0.666
Iter 20/2000 - Loss: 0.404
Iter 1981/2000 - Loss: -7.139
Iter 1982/2000 - Loss: -7.139
Iter 1983/2000 - Loss: -7.139
Iter 1984/2000 - Loss: -7.139
Iter 1985/2000 - Loss: -7.139
Iter 1986/2000 - Loss: -7.139
Iter 1987/2000 - Loss: -7.139
Iter 1988/2000 - Loss: -7.139
Iter 1989/2000 - Loss: -7.139
Iter 1990/2000 - Loss: -7.139
Iter 1991/2000 - Loss: -7.139
Iter 1992/2000 - Loss: -7.139
Iter 1993/2000 - Loss: -7.139
Iter 1994/2000 - Loss: -7.140
Iter 1995/2000 - Loss: -7.140
Iter 1996/2000 - Loss: -7.140
Iter 1997/2000 - Loss: -7.140
Iter 1998/2000 - Loss: -7.140
Iter 1999/2000 - Loss: -7.140
Iter 2000/2000 - Loss: -7.140
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[13.6518,  9.5455, 20.0556,  4.3199,  7.5159, 60.7647]],

        [[18.3358, 31.9354,  7.9468,  1.2864,  2.0325, 26.0015]],

        [[21.3177, 27.8127,  8.7053,  0.9907,  1.0341, 27.9366]],

        [[16.9782, 31.2617, 12.3648,  1.1506,  2.8691, 45.9464]]])
Signal Variance: tensor([ 0.1400,  2.3013, 19.4000,  0.3809])
Estimated target variance: tensor([0.0208, 0.9068, 7.7060, 0.1666])
N: 200
Signal to noise ratio: tensor([21.0849, 80.4454, 84.4475, 33.3678])
Bound on condition number: tensor([  88915.5198, 1294293.1712, 1426277.8044,  222683.0832])
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 0.005337166153809198, policy loss: 6.547871297502643
Experience 20, Iter 1, disc loss: 0.004950400783654838, policy loss: 6.649056155749939
Experience 20, Iter 2, disc loss: 0.004876273366016244, policy loss: 6.483354282299192
Experience 20, Iter 3, disc loss: 0.005154887188694703, policy loss: 6.192323963677131
Experience 20, Iter 4, disc loss: 0.005539575195577679, policy loss: 6.2721650558558295
Experience 20, Iter 5, disc loss: 0.005401076643990581, policy loss: 6.4364993319191175
Experience 20, Iter 6, disc loss: 0.00508673216958646, policy loss: 6.4862800542631085
Experience 20, Iter 7, disc loss: 0.005178636438862898, policy loss: 6.575329408466041
Experience 20, Iter 8, disc loss: 0.0055971538142280015, policy loss: 6.24712332397711
Experience 20, Iter 9, disc loss: 0.005138948824395476, policy loss: 6.315988594377631
Experience 20, Iter 10, disc loss: 0.005271837608992583, policy loss: 6.666101488690192
Experience 20, Iter 11, disc loss: 0.005350474903873369, policy loss: 6.464355628709663
Experience 20, Iter 12, disc loss: 0.004766361585120275, policy loss: 6.810891889783145
Experience 20, Iter 13, disc loss: 0.005479633716637743, policy loss: 6.17131425285082
Experience 20, Iter 14, disc loss: 0.004978361396452767, policy loss: 6.449815826712483
Experience 20, Iter 15, disc loss: 0.004866789651401644, policy loss: 6.388553241099793
Experience 20, Iter 16, disc loss: 0.005146196988282503, policy loss: 6.709607723031552
Experience 20, Iter 17, disc loss: 0.005229447428203462, policy loss: 6.415702794586823
Experience 20, Iter 18, disc loss: 0.005339053727803827, policy loss: 6.209021310051338
Experience 20, Iter 19, disc loss: 0.005529132714741166, policy loss: 6.465020088130977
Experience 20, Iter 20, disc loss: 0.00514417058687291, policy loss: 6.47499737536385
Experience 20, Iter 21, disc loss: 0.004914656881347398, policy loss: 6.931345112084594
Experience 20, Iter 22, disc loss: 0.004964064139640418, policy loss: 6.741186941673117
Experience 20, Iter 23, disc loss: 0.004756120232210358, policy loss: 6.4696748622711056
Experience 20, Iter 24, disc loss: 0.004925490468159684, policy loss: 6.318302865485455
Experience 20, Iter 25, disc loss: 0.004354975958728963, policy loss: 6.822731819427668
Experience 20, Iter 26, disc loss: 0.004361155959754708, policy loss: 7.221196429884063
Experience 20, Iter 27, disc loss: 0.0045291921621631375, policy loss: 6.543495271408447
Experience 20, Iter 28, disc loss: 0.00476780220522139, policy loss: 6.470961386997475
Experience 20, Iter 29, disc loss: 0.0049399371465024735, policy loss: 6.670708639972717
Experience 20, Iter 30, disc loss: 0.005105771614220194, policy loss: 6.91821062557414
Experience 20, Iter 31, disc loss: 0.005267727929928915, policy loss: 6.38048677067763
Experience 20, Iter 32, disc loss: 0.004706072583022983, policy loss: 6.458826349619772
Experience 20, Iter 33, disc loss: 0.004252383478642871, policy loss: 6.7449844352004025
Experience 20, Iter 34, disc loss: 0.004216396658445753, policy loss: 6.79140382334335
Experience 20, Iter 35, disc loss: 0.004048699629870141, policy loss: 7.409389675621428
Experience 20, Iter 36, disc loss: 0.004411450155614652, policy loss: 6.79716744713896
Experience 20, Iter 37, disc loss: 0.004697948272090049, policy loss: 6.720642519848944
Experience 20, Iter 38, disc loss: 0.0047394551504037, policy loss: 7.121566047139865
Experience 20, Iter 39, disc loss: 0.004959884998482312, policy loss: 6.635611008119612
Experience 20, Iter 40, disc loss: 0.004924724839219987, policy loss: 6.657813903866908
Experience 20, Iter 41, disc loss: 0.004461639446079104, policy loss: 6.485240095461748
Experience 20, Iter 42, disc loss: 0.003992623401297476, policy loss: 6.750606592728923
Experience 20, Iter 43, disc loss: 0.003967579887109281, policy loss: 7.0784132224075
Experience 20, Iter 44, disc loss: 0.003987989989585191, policy loss: 6.81187373272237
Experience 20, Iter 45, disc loss: 0.004285730835387157, policy loss: 6.6785752513004075
Experience 20, Iter 46, disc loss: 0.0048548326688837365, policy loss: 6.412277120140391
Experience 20, Iter 47, disc loss: 0.004688820763951303, policy loss: 6.552457497130526
Experience 20, Iter 48, disc loss: 0.004930347545827162, policy loss: 6.612221578930585
Experience 20, Iter 49, disc loss: 0.005441690195325184, policy loss: 6.4896987276152505
Experience 20, Iter 50, disc loss: 0.004616567236176292, policy loss: 7.1459082243867655
Experience 20, Iter 51, disc loss: 0.0045644222393924645, policy loss: 6.5829348895595725
Experience 20, Iter 52, disc loss: 0.004160721700501697, policy loss: 6.640499423115115
Experience 20, Iter 53, disc loss: 0.004180265667568685, policy loss: 6.818040573156793
Experience 20, Iter 54, disc loss: 0.004117603885217208, policy loss: 6.800585992047768
Experience 20, Iter 55, disc loss: 0.004693814800705173, policy loss: 6.410950418149763
Experience 20, Iter 56, disc loss: 0.004448906150351607, policy loss: 6.627453761839468
Experience 20, Iter 57, disc loss: 0.004605664985541061, policy loss: 6.367638201340275
Experience 20, Iter 58, disc loss: 0.004649269607472275, policy loss: 6.5783629998818105
Experience 20, Iter 59, disc loss: 0.004889383050884998, policy loss: 6.533274217525699
Experience 20, Iter 60, disc loss: 0.004603418216293646, policy loss: 7.227112214630127
Experience 20, Iter 61, disc loss: 0.004570717390028792, policy loss: 6.699509305842393
Experience 20, Iter 62, disc loss: 0.004145853400970454, policy loss: 6.785793620163408
Experience 20, Iter 63, disc loss: 0.0035884864619181857, policy loss: 7.424103119852846
Experience 20, Iter 64, disc loss: 0.003384405563129203, policy loss: 7.179595430854433
Experience 20, Iter 65, disc loss: 0.0035078412930258087, policy loss: 6.971345909611657
Experience 20, Iter 66, disc loss: 0.0036438136863976974, policy loss: 7.3856150163990755
Experience 20, Iter 67, disc loss: 0.0040007802419480525, policy loss: 6.621906573380311
Experience 20, Iter 68, disc loss: 0.004475947901870611, policy loss: 6.463557773153339
Experience 20, Iter 69, disc loss: 0.00480842275194595, policy loss: 6.441924852876872
Experience 20, Iter 70, disc loss: 0.004216957170705705, policy loss: 7.898820456444614
Experience 20, Iter 71, disc loss: 0.00511555176150261, policy loss: 6.553030689620601
Experience 20, Iter 72, disc loss: 0.004322460084882438, policy loss: 6.778044433862844
Experience 20, Iter 73, disc loss: 0.0038500526258012885, policy loss: 6.791773794786447
Experience 20, Iter 74, disc loss: 0.0036989162472381956, policy loss: 7.591286217662421
Experience 20, Iter 75, disc loss: 0.0035078004134613196, policy loss: 7.18616606675627
Experience 20, Iter 76, disc loss: 0.0036718099736025053, policy loss: 6.91221014569231
Experience 20, Iter 77, disc loss: 0.004124944855284555, policy loss: 6.570448279787753
Experience 20, Iter 78, disc loss: 0.004840206344071434, policy loss: 6.500042811975849
Experience 20, Iter 79, disc loss: 0.00475891159803714, policy loss: 6.652712837307515
Experience 20, Iter 80, disc loss: 0.00490008626581466, policy loss: 7.279591633679561
Experience 20, Iter 81, disc loss: 0.004193401263975978, policy loss: 6.439875549234716
Experience 20, Iter 82, disc loss: 0.004017015222192284, policy loss: 6.949198032936986
Experience 20, Iter 83, disc loss: 0.0034157628597886363, policy loss: 7.806030832670885
Experience 20, Iter 84, disc loss: 0.003638243888370884, policy loss: 6.97507979885931
Experience 20, Iter 85, disc loss: 0.003673553017920641, policy loss: 6.899379653744899
Experience 20, Iter 86, disc loss: 0.003955868265920604, policy loss: 6.680718419506391
Experience 20, Iter 87, disc loss: 0.003991219019298159, policy loss: 6.849406686804048
Experience 20, Iter 88, disc loss: 0.004763473405597471, policy loss: 6.379149458515695
Experience 20, Iter 89, disc loss: 0.0048645006656263725, policy loss: 6.655010293650623
Experience 20, Iter 90, disc loss: 0.00433925980704266, policy loss: 6.894245911164792
Experience 20, Iter 91, disc loss: 0.004539488823182437, policy loss: 7.061683272234082
Experience 20, Iter 92, disc loss: 0.00419174163749364, policy loss: 6.68158574560956
Experience 20, Iter 93, disc loss: 0.004123356914467481, policy loss: 6.616914056732593
Experience 20, Iter 94, disc loss: 0.0033307941043142783, policy loss: 7.204428203711025
Experience 20, Iter 95, disc loss: 0.0034339567044287113, policy loss: 7.066539419963476
Experience 20, Iter 96, disc loss: 0.003245601669503727, policy loss: 7.551717017979296
Experience 20, Iter 97, disc loss: 0.0037176262318199317, policy loss: 6.744975244134795
Experience 20, Iter 98, disc loss: 0.0040716208732470855, policy loss: 6.443044369264806
Experience 20, Iter 99, disc loss: 0.00447846070069417, policy loss: 6.587098020061019
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2307],
        [1.9408],
        [0.0418]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0232, 0.2107, 1.9273, 0.0369, 0.0266, 5.8550]],

        [[0.0232, 0.2107, 1.9273, 0.0369, 0.0266, 5.8550]],

        [[0.0232, 0.2107, 1.9273, 0.0369, 0.0266, 5.8550]],

        [[0.0232, 0.2107, 1.9273, 0.0369, 0.0266, 5.8550]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0206, 0.9226, 7.7631, 0.1672], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0206, 0.9226, 7.7631, 0.1672])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.852
Iter 2/2000 - Loss: 3.806
Iter 3/2000 - Loss: 3.586
Iter 4/2000 - Loss: 3.520
Iter 5/2000 - Loss: 3.477
Iter 6/2000 - Loss: 3.347
Iter 7/2000 - Loss: 3.188
Iter 8/2000 - Loss: 3.045
Iter 9/2000 - Loss: 2.903
Iter 10/2000 - Loss: 2.731
Iter 11/2000 - Loss: 2.535
Iter 12/2000 - Loss: 2.333
Iter 13/2000 - Loss: 2.128
Iter 14/2000 - Loss: 1.912
Iter 15/2000 - Loss: 1.678
Iter 16/2000 - Loss: 1.425
Iter 17/2000 - Loss: 1.161
Iter 18/2000 - Loss: 0.894
Iter 19/2000 - Loss: 0.627
Iter 20/2000 - Loss: 0.359
Iter 1981/2000 - Loss: -7.160
Iter 1982/2000 - Loss: -7.160
Iter 1983/2000 - Loss: -7.160
Iter 1984/2000 - Loss: -7.160
Iter 1985/2000 - Loss: -7.160
Iter 1986/2000 - Loss: -7.160
Iter 1987/2000 - Loss: -7.160
Iter 1988/2000 - Loss: -7.160
Iter 1989/2000 - Loss: -7.160
Iter 1990/2000 - Loss: -7.160
Iter 1991/2000 - Loss: -7.160
Iter 1992/2000 - Loss: -7.160
Iter 1993/2000 - Loss: -7.161
Iter 1994/2000 - Loss: -7.161
Iter 1995/2000 - Loss: -7.161
Iter 1996/2000 - Loss: -7.161
Iter 1997/2000 - Loss: -7.161
Iter 1998/2000 - Loss: -7.161
Iter 1999/2000 - Loss: -7.161
Iter 2000/2000 - Loss: -7.161
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0003]])
Lengthscale: tensor([[[12.9708,  9.0259, 20.2804,  4.1429,  6.8273, 57.1692]],

        [[18.5336, 32.1898,  8.0111,  1.3437,  1.7959, 25.8163]],

        [[21.3562, 28.7900,  8.7540,  0.9433,  1.0377, 27.6031]],

        [[16.0199, 30.9221, 12.5036,  1.1224,  3.0789, 47.3204]]])
Signal Variance: tensor([ 0.1286,  2.3066, 17.4647,  0.3927])
Estimated target variance: tensor([0.0206, 0.9226, 7.7631, 0.1672])
N: 210
Signal to noise ratio: tensor([19.7902, 76.3138, 77.9335, 34.1813])
Bound on condition number: tensor([  82248.3379, 1222997.9576, 1275463.2630,  245356.5662])
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 0.004743017556270031, policy loss: 6.5071545337849415
Experience 21, Iter 1, disc loss: 0.004613760819639642, policy loss: 6.320932211767411
Experience 21, Iter 2, disc loss: 0.005007981342099708, policy loss: 6.325958952564568
Experience 21, Iter 3, disc loss: 0.004660006540354114, policy loss: 6.522420812797287
Experience 21, Iter 4, disc loss: 0.004378390843900836, policy loss: 6.5624956096565645
Experience 21, Iter 5, disc loss: 0.0045385710442177735, policy loss: 6.522282799580764
Experience 21, Iter 6, disc loss: 0.0053417671151514586, policy loss: 6.574137650929103
Experience 21, Iter 7, disc loss: 0.0049060003154068445, policy loss: 7.105657791032289
Experience 21, Iter 8, disc loss: 0.004618254134389461, policy loss: 7.029227701492347
Experience 21, Iter 9, disc loss: 0.0038415349402677757, policy loss: 6.676652659470521
Experience 21, Iter 10, disc loss: 0.0035542580730720725, policy loss: 7.447039693041928
Experience 21, Iter 11, disc loss: 0.0036876945251806442, policy loss: 6.8816480217374165
Experience 21, Iter 12, disc loss: 0.0036856754075134425, policy loss: 6.959511544738064
Experience 21, Iter 13, disc loss: 0.0038948898759666164, policy loss: 7.082212960244913
Experience 21, Iter 14, disc loss: 0.005154112786851484, policy loss: 6.346719123317416
Experience 21, Iter 15, disc loss: 0.00502377391482052, policy loss: 6.883116150655669
Experience 21, Iter 16, disc loss: 0.004422172093020717, policy loss: 6.945170854909124
Experience 21, Iter 17, disc loss: 0.003780068040859603, policy loss: 6.78333982188345
Experience 21, Iter 18, disc loss: 0.003264193856176674, policy loss: 7.620302280800152
Experience 21, Iter 19, disc loss: 0.002980270412868685, policy loss: 7.648647262070424
Experience 21, Iter 20, disc loss: 0.0032974382811479346, policy loss: 7.105656090838101
Experience 21, Iter 21, disc loss: 0.0033992641342358524, policy loss: 7.2562685863313385
Experience 21, Iter 22, disc loss: 0.003423924058506119, policy loss: 7.0903385143361835
Experience 21, Iter 23, disc loss: 0.004128037322804446, policy loss: 6.482668885071778
Experience 21, Iter 24, disc loss: 0.004384666352416239, policy loss: 6.394639532555464
Experience 21, Iter 25, disc loss: 0.004524520054130479, policy loss: 6.764710647476754
Experience 21, Iter 26, disc loss: 0.004255896985307218, policy loss: 7.3187826083807845
Experience 21, Iter 27, disc loss: 0.004394839325815726, policy loss: 6.607626896896291
Experience 21, Iter 28, disc loss: 0.004281429133930445, policy loss: 6.372019997734999
Experience 21, Iter 29, disc loss: 0.004456724305878052, policy loss: 6.580365615231971
Experience 21, Iter 30, disc loss: 0.004060527573954854, policy loss: 6.72468083818544
Experience 21, Iter 31, disc loss: 0.004230114149788954, policy loss: 6.468531516072354
Experience 21, Iter 32, disc loss: 0.004092592717248258, policy loss: 7.326982243909999
Experience 21, Iter 33, disc loss: 0.004721699514741426, policy loss: 6.1704082171069
Experience 21, Iter 34, disc loss: 0.004307383713301241, policy loss: 6.578302958988798
Experience 21, Iter 35, disc loss: 0.004636139628065497, policy loss: 6.840681047015579
Experience 21, Iter 36, disc loss: 0.004894608962390136, policy loss: 6.40663670934134
Experience 21, Iter 37, disc loss: 0.004667433347646294, policy loss: 6.55322942382056
Experience 21, Iter 38, disc loss: 0.0049155238582736325, policy loss: 6.391831600812561
Experience 21, Iter 39, disc loss: 0.004638804537434383, policy loss: 6.6724518205469305
Experience 21, Iter 40, disc loss: 0.004683572395771912, policy loss: 6.413396313775267
Experience 21, Iter 41, disc loss: 0.004570264657144938, policy loss: 6.352827310759927
Experience 21, Iter 42, disc loss: 0.004755036912029434, policy loss: 6.2903457372470974
Experience 21, Iter 43, disc loss: 0.004600625856092683, policy loss: 6.6963320798107
Experience 21, Iter 44, disc loss: 0.004537392004925167, policy loss: 6.750381737951747
Experience 21, Iter 45, disc loss: 0.005052990238496192, policy loss: 6.560481065820156
Experience 21, Iter 46, disc loss: 0.004511816679232519, policy loss: 6.688762651380293
Experience 21, Iter 47, disc loss: 0.004694522553907476, policy loss: 6.604193386902315
Experience 21, Iter 48, disc loss: 0.005373442867240093, policy loss: 6.345118013022853
Experience 21, Iter 49, disc loss: 0.0044991405155791345, policy loss: 6.578896596137385
Experience 21, Iter 50, disc loss: 0.0043059297954770055, policy loss: 6.489612514806019
Experience 21, Iter 51, disc loss: 0.004259763610477349, policy loss: 6.455717875744254
Experience 21, Iter 52, disc loss: 0.004073449170167906, policy loss: 7.1027207527252365
Experience 21, Iter 53, disc loss: 0.004944184150816628, policy loss: 6.342978987187651
Experience 21, Iter 54, disc loss: 0.004464749943452757, policy loss: 6.729957538914395
Experience 21, Iter 55, disc loss: 0.004744710141096135, policy loss: 6.595506872058554
Experience 21, Iter 56, disc loss: 0.004981349702102756, policy loss: 6.526256243480137
Experience 21, Iter 57, disc loss: 0.004461559671345422, policy loss: 6.904847067858855
Experience 21, Iter 58, disc loss: 0.004976763061438023, policy loss: 6.470587271268485
Experience 21, Iter 59, disc loss: 0.00500645995203002, policy loss: 6.498319357489876
Experience 21, Iter 60, disc loss: 0.004659789684751116, policy loss: 6.891715161110666
Experience 21, Iter 61, disc loss: 0.004614818941064667, policy loss: 6.628410773924303
Experience 21, Iter 62, disc loss: 0.0043334151708408234, policy loss: 6.4600820708859015
Experience 21, Iter 63, disc loss: 0.004109356372025998, policy loss: 6.912793223532507
Experience 21, Iter 64, disc loss: 0.004232974763446717, policy loss: 7.120442234909669
Experience 21, Iter 65, disc loss: 0.0049337271426369515, policy loss: 6.476487621529689
Experience 21, Iter 66, disc loss: 0.00459988343948214, policy loss: 7.0944479621359715
Experience 21, Iter 67, disc loss: 0.004679093184227445, policy loss: 6.349142209361792
Experience 21, Iter 68, disc loss: 0.004707244100341576, policy loss: 6.6281685585896275
Experience 21, Iter 69, disc loss: 0.004558778755027694, policy loss: 6.3677241809197245
Experience 21, Iter 70, disc loss: 0.0043042131256724936, policy loss: 6.819034546510114
Experience 21, Iter 71, disc loss: 0.0046882402320173044, policy loss: 6.511185793093165
Experience 21, Iter 72, disc loss: 0.004520985326341671, policy loss: 6.706017410481983
Experience 21, Iter 73, disc loss: 0.0044674938781124555, policy loss: 6.683190231340598
Experience 21, Iter 74, disc loss: 0.0041980502846157095, policy loss: 6.734325115841303
Experience 21, Iter 75, disc loss: 0.004555814021159003, policy loss: 6.623101436928254
Experience 21, Iter 76, disc loss: 0.004798778433960374, policy loss: 6.498970032098182
Experience 21, Iter 77, disc loss: 0.004764861257204077, policy loss: 6.708172817118305
Experience 21, Iter 78, disc loss: 0.004776972777246916, policy loss: 6.317723692242285
Experience 21, Iter 79, disc loss: 0.004340901555430177, policy loss: 6.540159690865109
Experience 21, Iter 80, disc loss: 0.004579795883027037, policy loss: 6.675268292766497
Experience 21, Iter 81, disc loss: 0.0044005556378815085, policy loss: 6.741334506540376
Experience 21, Iter 82, disc loss: 0.0038816304920749178, policy loss: 7.176827164316807
Experience 21, Iter 83, disc loss: 0.0038792026476829463, policy loss: 7.04867497750209
Experience 21, Iter 84, disc loss: 0.0038793479399923617, policy loss: 6.962765725072014
Experience 21, Iter 85, disc loss: 0.004115399023623958, policy loss: 6.789882841905318
Experience 21, Iter 86, disc loss: 0.004684569489419731, policy loss: 6.440661138890289
Experience 21, Iter 87, disc loss: 0.00440567192995194, policy loss: 7.304337253809736
Experience 21, Iter 88, disc loss: 0.004613754279229141, policy loss: 7.23178781943346
Experience 21, Iter 89, disc loss: 0.0050633200250651544, policy loss: 6.373388023648026
Experience 21, Iter 90, disc loss: 0.004142976120347721, policy loss: 6.69745345060235
Experience 21, Iter 91, disc loss: 0.0038281736289259515, policy loss: 7.138241734955399
Experience 21, Iter 92, disc loss: 0.004027554536005822, policy loss: 6.871085793949055
Experience 21, Iter 93, disc loss: 0.0038640102172517923, policy loss: 6.758568188032752
Experience 21, Iter 94, disc loss: 0.004438680557971067, policy loss: 6.637913503020793
Experience 21, Iter 95, disc loss: 0.004191002883956529, policy loss: 6.644313656534517
Experience 21, Iter 96, disc loss: 0.004254492605026009, policy loss: 6.729480690858178
Experience 21, Iter 97, disc loss: 0.004398670592460767, policy loss: 6.799934075577019
Experience 21, Iter 98, disc loss: 0.004676846899169305, policy loss: 6.386294970927303
Experience 21, Iter 99, disc loss: 0.003916690202464959, policy loss: 7.041808456110651
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.2338],
        [1.9649],
        [0.0420]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0226, 0.2089, 1.9444, 0.0369, 0.0269, 5.9098]],

        [[0.0226, 0.2089, 1.9444, 0.0369, 0.0269, 5.9098]],

        [[0.0226, 0.2089, 1.9444, 0.0369, 0.0269, 5.9098]],

        [[0.0226, 0.2089, 1.9444, 0.0369, 0.0269, 5.9098]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0203, 0.9352, 7.8595, 0.1679], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0203, 0.9352, 7.8595, 0.1679])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.867
Iter 2/2000 - Loss: 3.826
Iter 3/2000 - Loss: 3.606
Iter 4/2000 - Loss: 3.543
Iter 5/2000 - Loss: 3.505
Iter 6/2000 - Loss: 3.376
Iter 7/2000 - Loss: 3.216
Iter 8/2000 - Loss: 3.073
Iter 9/2000 - Loss: 2.930
Iter 10/2000 - Loss: 2.756
Iter 11/2000 - Loss: 2.557
Iter 12/2000 - Loss: 2.350
Iter 13/2000 - Loss: 2.140
Iter 14/2000 - Loss: 1.920
Iter 15/2000 - Loss: 1.681
Iter 16/2000 - Loss: 1.423
Iter 17/2000 - Loss: 1.153
Iter 18/2000 - Loss: 0.879
Iter 19/2000 - Loss: 0.604
Iter 20/2000 - Loss: 0.329
Iter 1981/2000 - Loss: -7.241
Iter 1982/2000 - Loss: -7.241
Iter 1983/2000 - Loss: -7.241
Iter 1984/2000 - Loss: -7.241
Iter 1985/2000 - Loss: -7.241
Iter 1986/2000 - Loss: -7.241
Iter 1987/2000 - Loss: -7.241
Iter 1988/2000 - Loss: -7.241
Iter 1989/2000 - Loss: -7.241
Iter 1990/2000 - Loss: -7.242
Iter 1991/2000 - Loss: -7.242
Iter 1992/2000 - Loss: -7.242
Iter 1993/2000 - Loss: -7.242
Iter 1994/2000 - Loss: -7.242
Iter 1995/2000 - Loss: -7.242
Iter 1996/2000 - Loss: -7.242
Iter 1997/2000 - Loss: -7.242
Iter 1998/2000 - Loss: -7.242
Iter 1999/2000 - Loss: -7.242
Iter 2000/2000 - Loss: -7.242
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0003]])
Lengthscale: tensor([[[12.3682,  8.4905, 20.5490,  3.9973,  6.4220, 55.2257]],

        [[19.1520, 32.0257,  7.9684,  1.3566,  1.7608, 25.2187]],

        [[21.7077, 28.1454,  8.8190,  0.9220,  1.0464, 27.6912]],

        [[15.2462, 29.3977, 12.2831,  1.1140,  3.1637, 47.4989]]])
Signal Variance: tensor([ 0.1171,  2.1922, 17.1942,  0.3866])
Estimated target variance: tensor([0.0203, 0.9352, 7.8595, 0.1679])
N: 220
Signal to noise ratio: tensor([19.0748, 74.8883, 76.9865, 33.7992])
Bound on condition number: tensor([  80047.2678, 1233816.0793, 1303924.4140,  251326.2287])
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 0.0036246510843466104, policy loss: 6.945007854042003
Experience 22, Iter 1, disc loss: 0.003977028794110666, policy loss: 6.469386930096029
Experience 22, Iter 2, disc loss: 0.0039134330476282035, policy loss: 6.81589117956973
Experience 22, Iter 3, disc loss: 0.003966958718259479, policy loss: 6.704191884860489
Experience 22, Iter 4, disc loss: 0.003969099102042624, policy loss: 6.7822278919454995
Experience 22, Iter 5, disc loss: 0.004844632547931759, policy loss: 6.566761713795321
Experience 22, Iter 6, disc loss: 0.0041448239569845564, policy loss: 6.712833004335296
Experience 22, Iter 7, disc loss: 0.004152705751536883, policy loss: 6.5662589226910715
Experience 22, Iter 8, disc loss: 0.004179275529626702, policy loss: 6.546686957512596
Experience 22, Iter 9, disc loss: 0.004346749908305831, policy loss: 6.946302334150215
Experience 22, Iter 10, disc loss: 0.004215668388871449, policy loss: 6.7802915952120575
Experience 22, Iter 11, disc loss: 0.0038662968154643497, policy loss: 6.818394752361046
Experience 22, Iter 12, disc loss: 0.004280005827044349, policy loss: 6.501853858713208
Experience 22, Iter 13, disc loss: 0.0048490023056463015, policy loss: 6.55707190505864
Experience 22, Iter 14, disc loss: 0.004698885828151039, policy loss: 6.42509746971651
Experience 22, Iter 15, disc loss: 0.0047301704745952656, policy loss: 6.473930709649884
Experience 22, Iter 16, disc loss: 0.004654170390532368, policy loss: 6.2280146894319195
Experience 22, Iter 17, disc loss: 0.004359725228295672, policy loss: 6.778866377532698
Experience 22, Iter 18, disc loss: 0.004581243964273441, policy loss: 6.266863365868303
Experience 22, Iter 19, disc loss: 0.0042355185024686645, policy loss: 6.712910398343283
Experience 22, Iter 20, disc loss: 0.004184030582572359, policy loss: 6.765769885533375
Experience 22, Iter 21, disc loss: 0.005307480993382019, policy loss: 6.163365530160878
Experience 22, Iter 22, disc loss: 0.0045133053392048764, policy loss: 6.582272035259206
Experience 22, Iter 23, disc loss: 0.004336199784772837, policy loss: 6.418927054127176
Experience 22, Iter 24, disc loss: 0.0048191979075699115, policy loss: 6.697164486280859
Experience 22, Iter 25, disc loss: 0.004317427186698631, policy loss: 6.842476310596448
Experience 22, Iter 26, disc loss: 0.004183016340318063, policy loss: 7.043544710133938
Experience 22, Iter 27, disc loss: 0.004518361324385663, policy loss: 6.340430289833735
Experience 22, Iter 28, disc loss: 0.004416809315618332, policy loss: 6.655346479900446
Experience 22, Iter 29, disc loss: 0.00488566968621108, policy loss: 6.3421527416201755
Experience 22, Iter 30, disc loss: 0.00463976929464439, policy loss: 6.762566849867467
Experience 22, Iter 31, disc loss: 0.0047621815133876404, policy loss: 6.29711239930227
Experience 22, Iter 32, disc loss: 0.004421271526091557, policy loss: 6.622205307283372
Experience 22, Iter 33, disc loss: 0.0046433035571800466, policy loss: 6.426430239672783
Experience 22, Iter 34, disc loss: 0.004588854242766692, policy loss: 6.523840684005048
Experience 22, Iter 35, disc loss: 0.004350695022506051, policy loss: 7.190297279943827
Experience 22, Iter 36, disc loss: 0.004397935865542562, policy loss: 6.491542249382972
Experience 22, Iter 37, disc loss: 0.004472707870127595, policy loss: 6.447160010298973
Experience 22, Iter 38, disc loss: 0.004343908308038774, policy loss: 7.073999350785541
Experience 22, Iter 39, disc loss: 0.004125880323285952, policy loss: 6.760266518102052
Experience 22, Iter 40, disc loss: 0.004296146131100283, policy loss: 6.5743695675739975
Experience 22, Iter 41, disc loss: 0.004182409600901117, policy loss: 6.960376675123116
Experience 22, Iter 42, disc loss: 0.00431384885224829, policy loss: 6.7804649941865325
Experience 22, Iter 43, disc loss: 0.004380142432956181, policy loss: 6.4923239857429635
Experience 22, Iter 44, disc loss: 0.0047552625773858, policy loss: 6.604611003404364
Experience 22, Iter 45, disc loss: 0.004356961356533006, policy loss: 6.53354292332883
Experience 22, Iter 46, disc loss: 0.0044373277230618634, policy loss: 6.430846862602037
Experience 22, Iter 47, disc loss: 0.004647901782795028, policy loss: 6.2728201424989205
Experience 22, Iter 48, disc loss: 0.004896245217376174, policy loss: 6.2077900064133935
Experience 22, Iter 49, disc loss: 0.004530533487888818, policy loss: 6.6781695788966005
Experience 22, Iter 50, disc loss: 0.004344039709497845, policy loss: 6.8632673878378885
Experience 22, Iter 51, disc loss: 0.004980034705698485, policy loss: 6.694207725050944
Experience 22, Iter 52, disc loss: 0.004461140068847503, policy loss: 6.654997577114393
Experience 22, Iter 53, disc loss: 0.004685784996801218, policy loss: 6.685761416856289
Experience 22, Iter 54, disc loss: 0.004639059459643143, policy loss: 6.382038388255739
Experience 22, Iter 55, disc loss: 0.004287524958833259, policy loss: 6.754005477101324
Experience 22, Iter 56, disc loss: 0.004805681301846356, policy loss: 6.522402676703649
Experience 22, Iter 57, disc loss: 0.004280535284237498, policy loss: 6.5414741981480296
Experience 22, Iter 58, disc loss: 0.004304704883404567, policy loss: 6.645721297598076
Experience 22, Iter 59, disc loss: 0.004438696970080213, policy loss: 6.7905374355388926
Experience 22, Iter 60, disc loss: 0.004512111507948597, policy loss: 6.565726752794771
Experience 22, Iter 61, disc loss: 0.004815323151227959, policy loss: 6.583046851903532
Experience 22, Iter 62, disc loss: 0.0044245837641152495, policy loss: 6.680148528706599
Experience 22, Iter 63, disc loss: 0.0041361718924367115, policy loss: 7.024199723460594
Experience 22, Iter 64, disc loss: 0.004307732979136657, policy loss: 6.676612294564144
Experience 22, Iter 65, disc loss: 0.004095060708497047, policy loss: 6.676537610972803
Experience 22, Iter 66, disc loss: 0.004511406196142126, policy loss: 6.80753753360288
Experience 22, Iter 67, disc loss: 0.0048562528838662165, policy loss: 6.39863195123993
Experience 22, Iter 68, disc loss: 0.004966960465397886, policy loss: 6.449068816859306
Experience 22, Iter 69, disc loss: 0.0041876780307042675, policy loss: 6.894401427950184
Experience 22, Iter 70, disc loss: 0.004442945799533349, policy loss: 6.738919102148698
Experience 22, Iter 71, disc loss: 0.004151392171162782, policy loss: 6.671218731725849
Experience 22, Iter 72, disc loss: 0.004222990675930898, policy loss: 6.812429107830397
Experience 22, Iter 73, disc loss: 0.004028113012834845, policy loss: 6.806393040846575
Experience 22, Iter 74, disc loss: 0.004408308121858704, policy loss: 6.985086916403297
Experience 22, Iter 75, disc loss: 0.004314807945834798, policy loss: 7.034527794897429
Experience 22, Iter 76, disc loss: 0.004720321412185292, policy loss: 6.4342088439961405
Experience 22, Iter 77, disc loss: 0.004053648377747238, policy loss: 6.928492435406813
Experience 22, Iter 78, disc loss: 0.004069422323482858, policy loss: 6.729581499437552
Experience 22, Iter 79, disc loss: 0.004133009865112774, policy loss: 6.891409991610504
Experience 22, Iter 80, disc loss: 0.004081394213818488, policy loss: 6.932040256278931
Experience 22, Iter 81, disc loss: 0.0038846590480032094, policy loss: 6.770940680956777
Experience 22, Iter 82, disc loss: 0.004203882812781779, policy loss: 6.6497583507819105
Experience 22, Iter 83, disc loss: 0.00375011777888125, policy loss: 7.259604069366414
Experience 22, Iter 84, disc loss: 0.004142677404195078, policy loss: 6.761608651750203
Experience 22, Iter 85, disc loss: 0.004473731788601056, policy loss: 6.575175314311137
Experience 22, Iter 86, disc loss: 0.004063817483973336, policy loss: 7.1352465331047945
Experience 22, Iter 87, disc loss: 0.0041631834373112055, policy loss: 6.9402973770644785
Experience 22, Iter 88, disc loss: 0.004236450631413643, policy loss: 6.770714035271825
Experience 22, Iter 89, disc loss: 0.0038663782812920627, policy loss: 6.745175754397801
Experience 22, Iter 90, disc loss: 0.004147301553716927, policy loss: 6.517492544164943
Experience 22, Iter 91, disc loss: 0.003761660338476137, policy loss: 6.912535220309079
Experience 22, Iter 92, disc loss: 0.004183596064115228, policy loss: 6.787164793453254
Experience 22, Iter 93, disc loss: 0.004367351125369924, policy loss: 6.573704063870989
Experience 22, Iter 94, disc loss: 0.004341853564479803, policy loss: 6.949646370034621
Experience 22, Iter 95, disc loss: 0.0038834762112710487, policy loss: 6.93547005150188
Experience 22, Iter 96, disc loss: 0.003917622127519211, policy loss: 7.325193271133269
Experience 22, Iter 97, disc loss: 0.004301432643212984, policy loss: 6.604636028312863
Experience 22, Iter 98, disc loss: 0.004147326265872331, policy loss: 6.562513437196642
Experience 22, Iter 99, disc loss: 0.004277371357177536, policy loss: 6.823420204072666
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.2370],
        [1.9847],
        [0.0420]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.2068, 1.9532, 0.0369, 0.0271, 5.9751]],

        [[0.0217, 0.2068, 1.9532, 0.0369, 0.0271, 5.9751]],

        [[0.0217, 0.2068, 1.9532, 0.0369, 0.0271, 5.9751]],

        [[0.0217, 0.2068, 1.9532, 0.0369, 0.0271, 5.9751]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.9481, 7.9388, 0.1681], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.9481, 7.9388, 0.1681])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.836
Iter 2/2000 - Loss: 3.783
Iter 3/2000 - Loss: 3.555
Iter 4/2000 - Loss: 3.481
Iter 5/2000 - Loss: 3.432
Iter 6/2000 - Loss: 3.297
Iter 7/2000 - Loss: 3.132
Iter 8/2000 - Loss: 2.982
Iter 9/2000 - Loss: 2.831
Iter 10/2000 - Loss: 2.650
Iter 11/2000 - Loss: 2.445
Iter 12/2000 - Loss: 2.233
Iter 13/2000 - Loss: 2.020
Iter 14/2000 - Loss: 1.796
Iter 15/2000 - Loss: 1.555
Iter 16/2000 - Loss: 1.295
Iter 17/2000 - Loss: 1.025
Iter 18/2000 - Loss: 0.751
Iter 19/2000 - Loss: 0.477
Iter 20/2000 - Loss: 0.204
Iter 1981/2000 - Loss: -7.346
Iter 1982/2000 - Loss: -7.346
Iter 1983/2000 - Loss: -7.346
Iter 1984/2000 - Loss: -7.346
Iter 1985/2000 - Loss: -7.346
Iter 1986/2000 - Loss: -7.346
Iter 1987/2000 - Loss: -7.346
Iter 1988/2000 - Loss: -7.346
Iter 1989/2000 - Loss: -7.346
Iter 1990/2000 - Loss: -7.346
Iter 1991/2000 - Loss: -7.346
Iter 1992/2000 - Loss: -7.346
Iter 1993/2000 - Loss: -7.346
Iter 1994/2000 - Loss: -7.346
Iter 1995/2000 - Loss: -7.346
Iter 1996/2000 - Loss: -7.346
Iter 1997/2000 - Loss: -7.346
Iter 1998/2000 - Loss: -7.346
Iter 1999/2000 - Loss: -7.346
Iter 2000/2000 - Loss: -7.346
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[12.4920,  8.5756, 20.4239,  3.8162,  6.2906, 55.6447]],

        [[19.3329, 32.0831,  8.0319,  1.3530,  1.7182, 24.2426]],

        [[21.6569, 27.8792,  8.8777,  0.9201,  1.0341, 27.7799]],

        [[15.2861, 28.9138, 12.3894,  1.1112,  3.2881, 47.3618]]])
Signal Variance: tensor([ 0.1167,  2.1158, 17.2786,  0.3961])
Estimated target variance: tensor([0.0200, 0.9481, 7.9388, 0.1681])
N: 230
Signal to noise ratio: tensor([19.3497, 74.5720, 78.9489, 33.9273])
Bound on condition number: tensor([  86115.4132, 1279027.6351, 1433574.0574,  264744.5759])
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 0.003805201117999467, policy loss: 7.118012699048556
Experience 23, Iter 1, disc loss: 0.004443806582232203, policy loss: 6.479607708259747
Experience 23, Iter 2, disc loss: 0.004716160407402782, policy loss: 6.2413418845166255
Experience 23, Iter 3, disc loss: 0.004328952709881163, policy loss: 6.576831545667821
Experience 23, Iter 4, disc loss: 0.0041800503052588835, policy loss: 6.797741876008768
Experience 23, Iter 5, disc loss: 0.003988249859396359, policy loss: 6.648458674641445
Experience 23, Iter 6, disc loss: 0.003942410253285079, policy loss: 7.029350766430996
Experience 23, Iter 7, disc loss: 0.004110247051944748, policy loss: 6.810137819262621
Experience 23, Iter 8, disc loss: 0.004124734073147911, policy loss: 6.661891883833803
Experience 23, Iter 9, disc loss: 0.004130282868117296, policy loss: 6.7894348433391665
Experience 23, Iter 10, disc loss: 0.004447399046822684, policy loss: 6.6579377368082335
Experience 23, Iter 11, disc loss: 0.004333179510540065, policy loss: 7.177289030599248
Experience 23, Iter 12, disc loss: 0.004379345869353018, policy loss: 6.537470644166104
Experience 23, Iter 13, disc loss: 0.003492771800218787, policy loss: 7.432661073778986
Experience 23, Iter 14, disc loss: 0.004257840378610855, policy loss: 6.586308673529176
Experience 23, Iter 15, disc loss: 0.004464305963542858, policy loss: 6.646664819744415
Experience 23, Iter 16, disc loss: 0.004603514038248831, policy loss: 6.571729053947321
Experience 23, Iter 17, disc loss: 0.004460842556562199, policy loss: 7.155140538251313
Experience 23, Iter 18, disc loss: 0.004705604924982627, policy loss: 6.916260615260224
Experience 23, Iter 19, disc loss: 0.005022712230429567, policy loss: 6.535217002624887
Experience 23, Iter 20, disc loss: 0.004142869620494121, policy loss: 6.79215972428742
Experience 23, Iter 21, disc loss: 0.004391362946288414, policy loss: 6.421699993452938
Experience 23, Iter 22, disc loss: 0.004395902084548196, policy loss: 6.684883839846927
Experience 23, Iter 23, disc loss: 0.004453721135900184, policy loss: 6.781079733052586
Experience 23, Iter 24, disc loss: 0.0042116665930851515, policy loss: 6.834042915402088
Experience 23, Iter 25, disc loss: 0.004019280388852024, policy loss: 6.787502884859682
Experience 23, Iter 26, disc loss: 0.004120807615924237, policy loss: 6.924021047133365
Experience 23, Iter 27, disc loss: 0.003915541057212538, policy loss: 6.790496843485515
Experience 23, Iter 28, disc loss: 0.0037331563913532316, policy loss: 6.882037054702094
Experience 23, Iter 29, disc loss: 0.0038208525107456797, policy loss: 6.945499569669735
Experience 23, Iter 30, disc loss: 0.004150801671891776, policy loss: 6.608797346044914
Experience 23, Iter 31, disc loss: 0.004486549411060156, policy loss: 6.7736723846862565
Experience 23, Iter 32, disc loss: 0.0043597218292202415, policy loss: 6.675986870536902
Experience 23, Iter 33, disc loss: 0.004316445532517185, policy loss: 6.852723014294337
Experience 23, Iter 34, disc loss: 0.004476128085425806, policy loss: 6.817272502253427
Experience 23, Iter 35, disc loss: 0.004436296902734565, policy loss: 6.7652930106094775
Experience 23, Iter 36, disc loss: 0.004140792725274912, policy loss: 6.569018912511664
Experience 23, Iter 37, disc loss: 0.004236040802310475, policy loss: 6.6309683303300595
Experience 23, Iter 38, disc loss: 0.003846072349594336, policy loss: 6.697363589216327
Experience 23, Iter 39, disc loss: 0.004462685260160692, policy loss: 6.470195888307583
Experience 23, Iter 40, disc loss: 0.004023452507657412, policy loss: 7.025130459967526
Experience 23, Iter 41, disc loss: 0.004047865258510657, policy loss: 6.647718146075741
Experience 23, Iter 42, disc loss: 0.004224352202743678, policy loss: 6.762330334879328
Experience 23, Iter 43, disc loss: 0.003835648924128752, policy loss: 6.816556235414171
Experience 23, Iter 44, disc loss: 0.004482959181232736, policy loss: 6.588911105861508
Experience 23, Iter 45, disc loss: 0.004349559912572949, policy loss: 6.515979449958197
Experience 23, Iter 46, disc loss: 0.004317428163727755, policy loss: 6.8242662569416295
Experience 23, Iter 47, disc loss: 0.0042223266264637335, policy loss: 7.078815908198937
Experience 23, Iter 48, disc loss: 0.004410605094046541, policy loss: 6.440161515179688
Experience 23, Iter 49, disc loss: 0.0037132201078343974, policy loss: 6.87096809870358
Experience 23, Iter 50, disc loss: 0.004069201044915078, policy loss: 6.6162456561339384
Experience 23, Iter 51, disc loss: 0.003977307660658131, policy loss: 7.422878271474288
Experience 23, Iter 52, disc loss: 0.0038981620021396176, policy loss: 7.398863652363322
Experience 23, Iter 53, disc loss: 0.004436880430091477, policy loss: 6.638761962184963
Experience 23, Iter 54, disc loss: 0.004044797557113207, policy loss: 6.865538238666555
Experience 23, Iter 55, disc loss: 0.003936004536929799, policy loss: 6.799795562930029
Experience 23, Iter 56, disc loss: 0.0043917533993317265, policy loss: 6.638429416827657
Experience 23, Iter 57, disc loss: 0.004337346280126638, policy loss: 6.785089902685931
Experience 23, Iter 58, disc loss: 0.004203567559949694, policy loss: 6.694464329949893
Experience 23, Iter 59, disc loss: 0.004530397596613425, policy loss: 6.703634071222792
Experience 23, Iter 60, disc loss: 0.004693378263634159, policy loss: 6.560825341675135
Experience 23, Iter 61, disc loss: 0.004485586759226877, policy loss: 6.662899279802067
Experience 23, Iter 62, disc loss: 0.0047753178010217015, policy loss: 6.9085467165277175
Experience 23, Iter 63, disc loss: 0.0044606342261066705, policy loss: 6.9473968544824745
Experience 23, Iter 64, disc loss: 0.004405322094341995, policy loss: 6.6640096473641375
Experience 23, Iter 65, disc loss: 0.004322497977115019, policy loss: 6.791799398540635
Experience 23, Iter 66, disc loss: 0.00441307768176455, policy loss: 6.719671201828112
Experience 23, Iter 67, disc loss: 0.004404770409635552, policy loss: 6.48333179374743
Experience 23, Iter 68, disc loss: 0.0048355023838856405, policy loss: 6.646473299754726
Experience 23, Iter 69, disc loss: 0.004893456622917611, policy loss: 6.412761514124899
Experience 23, Iter 70, disc loss: 0.004772795603757447, policy loss: 6.710949374826784
Experience 23, Iter 71, disc loss: 0.004137858169077679, policy loss: 6.6560487453927974
Experience 23, Iter 72, disc loss: 0.004688380361662007, policy loss: 6.494892465072809
Experience 23, Iter 73, disc loss: 0.00435506195441969, policy loss: 6.868764987984849
Experience 23, Iter 74, disc loss: 0.004593841433659115, policy loss: 6.515527789365617
Experience 23, Iter 75, disc loss: 0.00437304364812169, policy loss: 6.886441469508154
Experience 23, Iter 76, disc loss: 0.004508932853315484, policy loss: 6.703951700045447
Experience 23, Iter 77, disc loss: 0.004208911394301372, policy loss: 6.707141915357564
Experience 23, Iter 78, disc loss: 0.004559083223600893, policy loss: 6.531257111870259
Experience 23, Iter 79, disc loss: 0.004238671907647025, policy loss: 6.80168285799064
Experience 23, Iter 80, disc loss: 0.004404286666433327, policy loss: 6.844493572272459
Experience 23, Iter 81, disc loss: 0.005125217335428461, policy loss: 6.266043219866248
Experience 23, Iter 82, disc loss: 0.004652454311497765, policy loss: 6.718148095059135
Experience 23, Iter 83, disc loss: 0.004425142314216385, policy loss: 7.084770202647464
Experience 23, Iter 84, disc loss: 0.004632629403976431, policy loss: 6.506497543755028
Experience 23, Iter 85, disc loss: 0.0044126277559387445, policy loss: 7.087935731642091
Experience 23, Iter 86, disc loss: 0.003988359452129689, policy loss: 6.808974198263626
Experience 23, Iter 87, disc loss: 0.004527156480444102, policy loss: 6.901741605152022
Experience 23, Iter 88, disc loss: 0.004758168804286954, policy loss: 6.60523060874457
Experience 23, Iter 89, disc loss: 0.00460125833747475, policy loss: 6.539338813772336
Experience 23, Iter 90, disc loss: 0.004426915900278645, policy loss: 6.564570210743071
Experience 23, Iter 91, disc loss: 0.004412527497078799, policy loss: 6.681003594401307
Experience 23, Iter 92, disc loss: 0.0044321293776382106, policy loss: 6.738518668510514
Experience 23, Iter 93, disc loss: 0.004057471704446491, policy loss: 6.809430232103648
Experience 23, Iter 94, disc loss: 0.004280540200787868, policy loss: 6.72179473522066
Experience 23, Iter 95, disc loss: 0.0052364171979113645, policy loss: 6.523635580129561
Experience 23, Iter 96, disc loss: 0.0046839826894108025, policy loss: 6.470962425212292
Experience 23, Iter 97, disc loss: 0.004225546703059138, policy loss: 6.97966967913455
Experience 23, Iter 98, disc loss: 0.0043018602100215655, policy loss: 6.627669324830838
Experience 23, Iter 99, disc loss: 0.00416066791066787, policy loss: 6.815110808692614
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.2402],
        [1.9989],
        [0.0422]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0210, 0.2048, 1.9640, 0.0370, 0.0274, 6.0463]],

        [[0.0210, 0.2048, 1.9640, 0.0370, 0.0274, 6.0463]],

        [[0.0210, 0.2048, 1.9640, 0.0370, 0.0274, 6.0463]],

        [[0.0210, 0.2048, 1.9640, 0.0370, 0.0274, 6.0463]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0198, 0.9608, 7.9956, 0.1688], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0198, 0.9608, 7.9956, 0.1688])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.832
Iter 2/2000 - Loss: 3.774
Iter 3/2000 - Loss: 3.539
Iter 4/2000 - Loss: 3.457
Iter 5/2000 - Loss: 3.402
Iter 6/2000 - Loss: 3.261
Iter 7/2000 - Loss: 3.091
Iter 8/2000 - Loss: 2.935
Iter 9/2000 - Loss: 2.775
Iter 10/2000 - Loss: 2.585
Iter 11/2000 - Loss: 2.372
Iter 12/2000 - Loss: 2.155
Iter 13/2000 - Loss: 1.938
Iter 14/2000 - Loss: 1.712
Iter 15/2000 - Loss: 1.468
Iter 16/2000 - Loss: 1.206
Iter 17/2000 - Loss: 0.932
Iter 18/2000 - Loss: 0.655
Iter 19/2000 - Loss: 0.379
Iter 20/2000 - Loss: 0.103
Iter 1981/2000 - Loss: -7.443
Iter 1982/2000 - Loss: -7.443
Iter 1983/2000 - Loss: -7.443
Iter 1984/2000 - Loss: -7.443
Iter 1985/2000 - Loss: -7.443
Iter 1986/2000 - Loss: -7.443
Iter 1987/2000 - Loss: -7.443
Iter 1988/2000 - Loss: -7.443
Iter 1989/2000 - Loss: -7.443
Iter 1990/2000 - Loss: -7.443
Iter 1991/2000 - Loss: -7.443
Iter 1992/2000 - Loss: -7.443
Iter 1993/2000 - Loss: -7.443
Iter 1994/2000 - Loss: -7.443
Iter 1995/2000 - Loss: -7.443
Iter 1996/2000 - Loss: -7.443
Iter 1997/2000 - Loss: -7.443
Iter 1998/2000 - Loss: -7.443
Iter 1999/2000 - Loss: -7.443
Iter 2000/2000 - Loss: -7.443
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[12.3724,  8.2272, 20.0170,  3.7400,  6.6695, 54.8423]],

        [[19.0508, 31.7016,  7.9223,  1.3627,  1.7191, 24.3049]],

        [[21.2946, 27.1176,  8.8401,  0.9163,  1.0337, 27.8705]],

        [[15.0592, 28.5069, 12.2423,  1.1126,  3.2653, 47.0789]]])
Signal Variance: tensor([ 0.1129,  2.1188, 17.2751,  0.3892])
Estimated target variance: tensor([0.0198, 0.9608, 7.9956, 0.1688])
N: 240
Signal to noise ratio: tensor([19.1580, 74.1055, 81.0652, 34.1360])
Bound on condition number: tensor([  88087.7973, 1317990.2831, 1577177.3260,  279665.3851])
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 0.00421020087824473, policy loss: 6.716188233260722
Experience 24, Iter 1, disc loss: 0.004090311569152804, policy loss: 7.482864149093244
Experience 24, Iter 2, disc loss: 0.004408966857206894, policy loss: 6.965748794555259
Experience 24, Iter 3, disc loss: 0.004908593345789041, policy loss: 6.58270162107507
Experience 24, Iter 4, disc loss: 0.004495800995252364, policy loss: 6.712270523959744
Experience 24, Iter 5, disc loss: 0.004218560392250309, policy loss: 6.77475134809689
Experience 24, Iter 6, disc loss: 0.004329944627720254, policy loss: 6.981617820846559
Experience 24, Iter 7, disc loss: 0.004662890820202501, policy loss: 6.730440692876341
Experience 24, Iter 8, disc loss: 0.004578799454903488, policy loss: 6.475134791518249
Experience 24, Iter 9, disc loss: 0.004652745861735943, policy loss: 6.472747803524221
Experience 24, Iter 10, disc loss: 0.0042134001345376934, policy loss: 6.972077070948652
Experience 24, Iter 11, disc loss: 0.004450139565401973, policy loss: 6.462805171377314
Experience 24, Iter 12, disc loss: 0.004545081416600521, policy loss: 6.613412746049054
Experience 24, Iter 13, disc loss: 0.004328592463975581, policy loss: 6.7488756238202035
Experience 24, Iter 14, disc loss: 0.004417130701622604, policy loss: 6.7282949906159
Experience 24, Iter 15, disc loss: 0.00459662220018729, policy loss: 6.6811089998510305
Experience 24, Iter 16, disc loss: 0.004305742786828904, policy loss: 6.889108892686405
Experience 24, Iter 17, disc loss: 0.0045767620837147, policy loss: 6.400721079611648
Experience 24, Iter 18, disc loss: 0.004191163556223673, policy loss: 6.994271320978016
Experience 24, Iter 19, disc loss: 0.004678859898260926, policy loss: 6.5665034165676035
Experience 24, Iter 20, disc loss: 0.004981385875220376, policy loss: 6.491520549464352
Experience 24, Iter 21, disc loss: 0.00460274250625012, policy loss: 6.768383804753887
Experience 24, Iter 22, disc loss: 0.004594029566303073, policy loss: 6.564819905912037
Experience 24, Iter 23, disc loss: 0.004237213847771192, policy loss: 7.173114082534361
Experience 24, Iter 24, disc loss: 0.004586969872505185, policy loss: 6.386576587317144
Experience 24, Iter 25, disc loss: 0.004433119937251973, policy loss: 6.5789782571912365
Experience 24, Iter 26, disc loss: 0.004567282432284161, policy loss: 6.712995088864347
Experience 24, Iter 27, disc loss: 0.004454093367751831, policy loss: 6.557242651332709
Experience 24, Iter 28, disc loss: 0.004762476400991042, policy loss: 6.477672473565099
Experience 24, Iter 29, disc loss: 0.0040145576950367315, policy loss: 7.354790457021206
Experience 24, Iter 30, disc loss: 0.005030363462537514, policy loss: 6.459293850596007
Experience 24, Iter 31, disc loss: 0.004408686864404257, policy loss: 6.7439078710658595
Experience 24, Iter 32, disc loss: 0.004754094253153584, policy loss: 6.877955151219792
Experience 24, Iter 33, disc loss: 0.004712188296669197, policy loss: 6.784229545740949
Experience 24, Iter 34, disc loss: 0.004383837420027679, policy loss: 6.773049007182177
Experience 24, Iter 35, disc loss: 0.005332165988851056, policy loss: 6.23778505262594
Experience 24, Iter 36, disc loss: 0.004921530897694877, policy loss: 6.856412535585992
Experience 24, Iter 37, disc loss: 0.004525100107573684, policy loss: 6.534502442866852
Experience 24, Iter 38, disc loss: 0.005007803167044399, policy loss: 7.017631758590607
Experience 24, Iter 39, disc loss: 0.004683356911935324, policy loss: 6.848341530025817
Experience 24, Iter 40, disc loss: 0.004464922472043376, policy loss: 6.870860065814198
Experience 24, Iter 41, disc loss: 0.004700101513059614, policy loss: 6.474478439056686
Experience 24, Iter 42, disc loss: 0.004118356175299892, policy loss: 7.10619424866174
Experience 24, Iter 43, disc loss: 0.004997198760260442, policy loss: 6.571199696355825
Experience 24, Iter 44, disc loss: 0.004945351689652478, policy loss: 6.620284652341574
Experience 24, Iter 45, disc loss: 0.0038515107029196, policy loss: 7.344938415670521
Experience 24, Iter 46, disc loss: 0.00421304018051233, policy loss: 6.8087996684528935
Experience 24, Iter 47, disc loss: 0.0043378337193525, policy loss: 7.090485656863755
Experience 24, Iter 48, disc loss: 0.004738760518852648, policy loss: 6.535822673512781
Experience 24, Iter 49, disc loss: 0.004392033222901786, policy loss: 6.890009271234717
Experience 24, Iter 50, disc loss: 0.004267533184956446, policy loss: 6.964834805775206
Experience 24, Iter 51, disc loss: 0.0042990727386866025, policy loss: 7.048587982754927
Experience 24, Iter 52, disc loss: 0.004614888329508054, policy loss: 6.5681806261656455
Experience 24, Iter 53, disc loss: 0.004234459288859779, policy loss: 6.893376214976536
Experience 24, Iter 54, disc loss: 0.004214846830982633, policy loss: 7.131798745948322
Experience 24, Iter 55, disc loss: 0.0046701771049166944, policy loss: 6.67985428141916
Experience 24, Iter 56, disc loss: 0.004053435181294338, policy loss: 7.9288825407302
Experience 24, Iter 57, disc loss: 0.004578699106593759, policy loss: 6.492232802392271
Experience 24, Iter 58, disc loss: 0.004499987502128831, policy loss: 6.420255862764634
Experience 24, Iter 59, disc loss: 0.004431417400114268, policy loss: 6.89750943424726
Experience 24, Iter 60, disc loss: 0.0042918148713775185, policy loss: 6.867140466715913
Experience 24, Iter 61, disc loss: 0.00464512731102318, policy loss: 6.475212996592871
Experience 24, Iter 62, disc loss: 0.004635417643653607, policy loss: 6.7915062989595025
Experience 24, Iter 63, disc loss: 0.004471304468158278, policy loss: 6.822853951900989
Experience 24, Iter 64, disc loss: 0.004028745861166354, policy loss: 7.030692225519512
Experience 24, Iter 65, disc loss: 0.0035301531515447744, policy loss: 7.322611312178223
Experience 24, Iter 66, disc loss: 0.004249098523810542, policy loss: 7.100582738288134
Experience 24, Iter 67, disc loss: 0.00425976807028437, policy loss: 6.993721867713325
Experience 24, Iter 68, disc loss: 0.00395219159664943, policy loss: 7.134803589260672
Experience 24, Iter 69, disc loss: 0.004275012071605213, policy loss: 6.791987702956691
Experience 24, Iter 70, disc loss: 0.0035850023057171177, policy loss: 7.484381701094678
Experience 24, Iter 71, disc loss: 0.004280403246761256, policy loss: 7.164443352166725
Experience 24, Iter 72, disc loss: 0.004959230725357197, policy loss: 6.820097844205506
Experience 24, Iter 73, disc loss: 0.004587548466960687, policy loss: 6.566476274321365
Experience 24, Iter 74, disc loss: 0.004827773475500442, policy loss: 6.507173164000015
Experience 24, Iter 75, disc loss: 0.004897232090041418, policy loss: 6.471114269692921
Experience 24, Iter 76, disc loss: 0.0049748768622272675, policy loss: 6.451560357996094
Experience 24, Iter 77, disc loss: 0.004656861439881652, policy loss: 7.080311377129008
Experience 24, Iter 78, disc loss: 0.004231211317304301, policy loss: 6.8427220744985195
Experience 24, Iter 79, disc loss: 0.00414010242243441, policy loss: 6.827866564565121
Experience 24, Iter 80, disc loss: 0.004236671828922145, policy loss: 6.68817842470901
Experience 24, Iter 81, disc loss: 0.004149798546138081, policy loss: 6.656728284701455
Experience 24, Iter 82, disc loss: 0.004402871677587785, policy loss: 6.876166181943415
Experience 24, Iter 83, disc loss: 0.004187242423613626, policy loss: 6.677761740704193
Experience 24, Iter 84, disc loss: 0.0035324279517802704, policy loss: 7.241132364782139
Experience 24, Iter 85, disc loss: 0.004170157578008142, policy loss: 6.639669308276863
Experience 24, Iter 86, disc loss: 0.004912158719318511, policy loss: 6.391029399194357
Experience 24, Iter 87, disc loss: 0.003808139317639606, policy loss: 7.065716379907887
Experience 24, Iter 88, disc loss: 0.004523341264823626, policy loss: 6.600015879031531
Experience 24, Iter 89, disc loss: 0.004305416725213522, policy loss: 6.68434453184611
Experience 24, Iter 90, disc loss: 0.003938323026918272, policy loss: 7.545398821074085
Experience 24, Iter 91, disc loss: 0.003618030768479219, policy loss: 7.191632331153219
Experience 24, Iter 92, disc loss: 0.0035682385139533556, policy loss: 7.161017445843351
Experience 24, Iter 93, disc loss: 0.0033719541431382577, policy loss: 7.186510300428706
Experience 24, Iter 94, disc loss: 0.0033091851816945722, policy loss: 7.252894918760334
Experience 24, Iter 95, disc loss: 0.004472726791097719, policy loss: 6.555570331062746
Experience 24, Iter 96, disc loss: 0.004391917353816853, policy loss: 6.815944439578337
Experience 24, Iter 97, disc loss: 0.004093652285772278, policy loss: 6.608559608083752
Experience 24, Iter 98, disc loss: 0.0038384398535367768, policy loss: 6.61341928801663
Experience 24, Iter 99, disc loss: 0.004006416317604079, policy loss: 6.9869870761946276
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.2430],
        [2.0138],
        [0.0424]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0203, 0.2031, 1.9760, 0.0371, 0.0277, 6.0959]],

        [[0.0203, 0.2031, 1.9760, 0.0371, 0.0277, 6.0959]],

        [[0.0203, 0.2031, 1.9760, 0.0371, 0.0277, 6.0959]],

        [[0.0203, 0.2031, 1.9760, 0.0371, 0.0277, 6.0959]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0196, 0.9719, 8.0552, 0.1695], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0196, 0.9719, 8.0552, 0.1695])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.789
Iter 2/2000 - Loss: 3.724
Iter 3/2000 - Loss: 3.479
Iter 4/2000 - Loss: 3.386
Iter 5/2000 - Loss: 3.322
Iter 6/2000 - Loss: 3.174
Iter 7/2000 - Loss: 2.996
Iter 8/2000 - Loss: 2.831
Iter 9/2000 - Loss: 2.665
Iter 10/2000 - Loss: 2.469
Iter 11/2000 - Loss: 2.252
Iter 12/2000 - Loss: 2.032
Iter 13/2000 - Loss: 1.812
Iter 14/2000 - Loss: 1.585
Iter 15/2000 - Loss: 1.341
Iter 16/2000 - Loss: 1.079
Iter 17/2000 - Loss: 0.807
Iter 18/2000 - Loss: 0.531
Iter 19/2000 - Loss: 0.256
Iter 20/2000 - Loss: -0.018
Iter 1981/2000 - Loss: -7.531
Iter 1982/2000 - Loss: -7.531
Iter 1983/2000 - Loss: -7.531
Iter 1984/2000 - Loss: -7.531
Iter 1985/2000 - Loss: -7.531
Iter 1986/2000 - Loss: -7.532
Iter 1987/2000 - Loss: -7.532
Iter 1988/2000 - Loss: -7.532
Iter 1989/2000 - Loss: -7.532
Iter 1990/2000 - Loss: -7.532
Iter 1991/2000 - Loss: -7.532
Iter 1992/2000 - Loss: -7.532
Iter 1993/2000 - Loss: -7.532
Iter 1994/2000 - Loss: -7.532
Iter 1995/2000 - Loss: -7.532
Iter 1996/2000 - Loss: -7.532
Iter 1997/2000 - Loss: -7.532
Iter 1998/2000 - Loss: -7.532
Iter 1999/2000 - Loss: -7.532
Iter 2000/2000 - Loss: -7.532
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[12.0587,  6.7000, 20.6170,  3.4756,  6.8857, 52.7486]],

        [[18.8035, 31.8694,  7.9564,  1.3424,  1.7061, 23.7392]],

        [[20.9264, 27.0386,  8.8352,  0.9086,  1.0411, 27.5907]],

        [[14.8627, 28.3385, 12.4622,  1.1188,  3.2370, 47.7614]]])
Signal Variance: tensor([ 0.0940,  2.0502, 16.9248,  0.3977])
Estimated target variance: tensor([0.0196, 0.9719, 8.0552, 0.1695])
N: 250
Signal to noise ratio: tensor([17.5590, 73.5154, 80.5399, 34.9076])
Bound on condition number: tensor([  77080.3887, 1351129.8001, 1621669.2668,  304636.9519])
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 0.003883718482325559, policy loss: 7.359335672500476
Experience 25, Iter 1, disc loss: 0.004587415103594266, policy loss: 6.422848593940218
Experience 25, Iter 2, disc loss: 0.004715377813451347, policy loss: 6.768159712961714
Experience 25, Iter 3, disc loss: 0.004321223216367382, policy loss: 6.5708158547510305
Experience 25, Iter 4, disc loss: 0.004194459232946963, policy loss: 6.9960668383368
Experience 25, Iter 5, disc loss: 0.0049886849084987035, policy loss: 6.290676921848458
Experience 25, Iter 6, disc loss: 0.004230525083905173, policy loss: 7.103280054799328
Experience 25, Iter 7, disc loss: 0.004748323824824901, policy loss: 6.499341284481821
Experience 25, Iter 8, disc loss: 0.004216908626209019, policy loss: 7.036220942780963
Experience 25, Iter 9, disc loss: 0.004035883801652486, policy loss: 6.953792477817607
Experience 25, Iter 10, disc loss: 0.003982159989505023, policy loss: 6.769017547660162
Experience 25, Iter 11, disc loss: 0.0041765879675265665, policy loss: 6.950829127695459
Experience 25, Iter 12, disc loss: 0.0046194895285053845, policy loss: 6.429904967330846
Experience 25, Iter 13, disc loss: 0.004282245954124086, policy loss: 6.591978755896633
Experience 25, Iter 14, disc loss: 0.0042301823500087935, policy loss: 6.884579513412758
Experience 25, Iter 15, disc loss: 0.0040902192802543275, policy loss: 6.679461336720998
Experience 25, Iter 16, disc loss: 0.004425684041519449, policy loss: 6.579901010657148
Experience 25, Iter 17, disc loss: 0.003912261175273877, policy loss: 6.730314183476129
Experience 25, Iter 18, disc loss: 0.004181467274448012, policy loss: 6.794739861555767
Experience 25, Iter 19, disc loss: 0.00432927242075197, policy loss: 6.890808874341346
Experience 25, Iter 20, disc loss: 0.005191325473951425, policy loss: 6.738969562067996
Experience 25, Iter 21, disc loss: 0.0045474007231931565, policy loss: 6.456405730643599
Experience 25, Iter 22, disc loss: 0.00529157774168937, policy loss: 6.560640280881701
Experience 25, Iter 23, disc loss: 0.004404864815379709, policy loss: 6.8378840187419785
Experience 25, Iter 24, disc loss: 0.004112566712839861, policy loss: 6.616982783990444
Experience 25, Iter 25, disc loss: 0.0048902432872600515, policy loss: 6.528990096934593
Experience 25, Iter 26, disc loss: 0.004461084887227228, policy loss: 6.658296021056321
Experience 25, Iter 27, disc loss: 0.004630310793276707, policy loss: 6.647854632295458
Experience 25, Iter 28, disc loss: 0.004294926499715032, policy loss: 7.1993396039068465
Experience 25, Iter 29, disc loss: 0.004484229116683266, policy loss: 6.769181402384307
Experience 25, Iter 30, disc loss: 0.00442874302517869, policy loss: 6.835603524738476
Experience 25, Iter 31, disc loss: 0.005085946545817085, policy loss: 6.382894751797207
Experience 25, Iter 32, disc loss: 0.004359802875431131, policy loss: 6.870096380738803
Experience 25, Iter 33, disc loss: 0.004666653645010894, policy loss: 7.023246312689534
Experience 25, Iter 34, disc loss: 0.0050217590930603795, policy loss: 6.451983775048854
Experience 25, Iter 35, disc loss: 0.004577837140138268, policy loss: 6.697064736001828
Experience 25, Iter 36, disc loss: 0.0050581989937518446, policy loss: 6.550926374433498
Experience 25, Iter 37, disc loss: 0.005620856925322711, policy loss: 6.563207273487864
Experience 25, Iter 38, disc loss: 0.004434264596280274, policy loss: 6.701295956753114
Experience 25, Iter 39, disc loss: 0.00448696983257465, policy loss: 6.880014319864453
Experience 25, Iter 40, disc loss: 0.004319258242500091, policy loss: 6.9791849428441575
Experience 25, Iter 41, disc loss: 0.0045056506667154345, policy loss: 6.6570081892601145
Experience 25, Iter 42, disc loss: 0.00446223277041009, policy loss: 6.7172932117058375
Experience 25, Iter 43, disc loss: 0.004255917853250942, policy loss: 6.915774075240801
Experience 25, Iter 44, disc loss: 0.004629023349178282, policy loss: 6.891149177912385
Experience 25, Iter 45, disc loss: 0.004672212789798408, policy loss: 6.7944890694282645
Experience 25, Iter 46, disc loss: 0.005208573459285217, policy loss: 6.274001102601361
Experience 25, Iter 47, disc loss: 0.004958977933151721, policy loss: 7.114019720692291
Experience 25, Iter 48, disc loss: 0.004099294195065923, policy loss: 7.227150396664708
Experience 25, Iter 49, disc loss: 0.0043689807549033705, policy loss: 7.144703591212287
Experience 25, Iter 50, disc loss: 0.004569280396439385, policy loss: 6.8939544526953505
Experience 25, Iter 51, disc loss: 0.004465074051732861, policy loss: 6.713554245932715
Experience 25, Iter 52, disc loss: 0.0043575197317184955, policy loss: 6.9863799729938005
Experience 25, Iter 53, disc loss: 0.004222586563719958, policy loss: 6.678937866147767
Experience 25, Iter 54, disc loss: 0.004661695741509548, policy loss: 6.645967546525897
Experience 25, Iter 55, disc loss: 0.004805057604461692, policy loss: 6.727739799646863
Experience 25, Iter 56, disc loss: 0.004486582351537233, policy loss: 6.548581008803489
Experience 25, Iter 57, disc loss: 0.004147664957807234, policy loss: 6.996873034273241
Experience 25, Iter 58, disc loss: 0.004370695534596304, policy loss: 6.9209985436531625
Experience 25, Iter 59, disc loss: 0.004764437047061018, policy loss: 6.655195168591673
Experience 25, Iter 60, disc loss: 0.004118396682267775, policy loss: 6.822575337774623
Experience 25, Iter 61, disc loss: 0.004431864224952432, policy loss: 7.006205718622649
Experience 25, Iter 62, disc loss: 0.004622210595799003, policy loss: 6.841218765694281
Experience 25, Iter 63, disc loss: 0.005009594812964509, policy loss: 6.511797971351439
Experience 25, Iter 64, disc loss: 0.004634812961028739, policy loss: 6.643425651998653
Experience 25, Iter 65, disc loss: 0.004155965202144719, policy loss: 7.350933778815129
Experience 25, Iter 66, disc loss: 0.004489980507008846, policy loss: 6.870710512370749
Experience 25, Iter 67, disc loss: 0.004947717652154055, policy loss: 6.554199645599465
Experience 25, Iter 68, disc loss: 0.005100244975804476, policy loss: 6.51874358639008
Experience 25, Iter 69, disc loss: 0.004558638893576286, policy loss: 6.682968128533799
Experience 25, Iter 70, disc loss: 0.004447448593864433, policy loss: 7.134426071494952
Experience 25, Iter 71, disc loss: 0.004534760084001178, policy loss: 6.841449046927663
Experience 25, Iter 72, disc loss: 0.004720954467361187, policy loss: 6.470491181767766
Experience 25, Iter 73, disc loss: 0.004723351197575662, policy loss: 6.450685320794266
Experience 25, Iter 74, disc loss: 0.004454817656605277, policy loss: 6.936420762064589
Experience 25, Iter 75, disc loss: 0.004149343666908651, policy loss: 7.4082784925277165
Experience 25, Iter 76, disc loss: 0.00418598381514226, policy loss: 6.725083326681985
Experience 25, Iter 77, disc loss: 0.004216472299983844, policy loss: 6.974023312753798
Experience 25, Iter 78, disc loss: 0.004665432709664656, policy loss: 6.757769499007834
Experience 25, Iter 79, disc loss: 0.004541250783226938, policy loss: 6.59190596175645
Experience 25, Iter 80, disc loss: 0.004482378796763055, policy loss: 7.133232734252374
Experience 25, Iter 81, disc loss: 0.0049676312265177994, policy loss: 6.5084181736862465
Experience 25, Iter 82, disc loss: 0.004192859201450537, policy loss: 6.731515503685952
Experience 25, Iter 83, disc loss: 0.00463288770717166, policy loss: 6.588063692975591
Experience 25, Iter 84, disc loss: 0.004495386386074324, policy loss: 6.997662686801432
Experience 25, Iter 85, disc loss: 0.0043559226741997185, policy loss: 6.660251842534348
Experience 25, Iter 86, disc loss: 0.004891840976303755, policy loss: 6.6911888738116705
Experience 25, Iter 87, disc loss: 0.0045441531197945185, policy loss: 6.928875848141878
Experience 25, Iter 88, disc loss: 0.003976947898666356, policy loss: 6.956272350310827
Experience 25, Iter 89, disc loss: 0.004659908252545399, policy loss: 7.376210105032721
Experience 25, Iter 90, disc loss: 0.004530821107351722, policy loss: 6.9443862632387905
Experience 25, Iter 91, disc loss: 0.004298895647164436, policy loss: 7.18622465589564
Experience 25, Iter 92, disc loss: 0.004396918878292156, policy loss: 6.832588593130122
Experience 25, Iter 93, disc loss: 0.004432634726447316, policy loss: 6.833243431282487
Experience 25, Iter 94, disc loss: 0.004117752774692864, policy loss: 6.770554191108461
Experience 25, Iter 95, disc loss: 0.004244485209807276, policy loss: 7.237719938945835
Experience 25, Iter 96, disc loss: 0.003906513102289412, policy loss: 7.207115553826673
Experience 25, Iter 97, disc loss: 0.004068459203783005, policy loss: 7.151301467769824
Experience 25, Iter 98, disc loss: 0.004110042236203023, policy loss: 6.744058281733647
Experience 25, Iter 99, disc loss: 0.0043089849248425915, policy loss: 6.722857978733902
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2451],
        [2.0193],
        [0.0424]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0197, 0.2012, 1.9770, 0.0371, 0.0280, 6.1541]],

        [[0.0197, 0.2012, 1.9770, 0.0371, 0.0280, 6.1541]],

        [[0.0197, 0.2012, 1.9770, 0.0371, 0.0280, 6.1541]],

        [[0.0197, 0.2012, 1.9770, 0.0371, 0.0280, 6.1541]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0194, 0.9805, 8.0771, 0.1694], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0194, 0.9805, 8.0771, 0.1694])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.795
Iter 2/2000 - Loss: 3.731
Iter 3/2000 - Loss: 3.479
Iter 4/2000 - Loss: 3.385
Iter 5/2000 - Loss: 3.318
Iter 6/2000 - Loss: 3.168
Iter 7/2000 - Loss: 2.987
Iter 8/2000 - Loss: 2.819
Iter 9/2000 - Loss: 2.647
Iter 10/2000 - Loss: 2.445
Iter 11/2000 - Loss: 2.220
Iter 12/2000 - Loss: 1.992
Iter 13/2000 - Loss: 1.766
Iter 14/2000 - Loss: 1.533
Iter 15/2000 - Loss: 1.285
Iter 16/2000 - Loss: 1.019
Iter 17/2000 - Loss: 0.742
Iter 18/2000 - Loss: 0.462
Iter 19/2000 - Loss: 0.182
Iter 20/2000 - Loss: -0.096
Iter 1981/2000 - Loss: -7.612
Iter 1982/2000 - Loss: -7.612
Iter 1983/2000 - Loss: -7.612
Iter 1984/2000 - Loss: -7.612
Iter 1985/2000 - Loss: -7.612
Iter 1986/2000 - Loss: -7.612
Iter 1987/2000 - Loss: -7.612
Iter 1988/2000 - Loss: -7.612
Iter 1989/2000 - Loss: -7.612
Iter 1990/2000 - Loss: -7.612
Iter 1991/2000 - Loss: -7.613
Iter 1992/2000 - Loss: -7.613
Iter 1993/2000 - Loss: -7.613
Iter 1994/2000 - Loss: -7.613
Iter 1995/2000 - Loss: -7.613
Iter 1996/2000 - Loss: -7.613
Iter 1997/2000 - Loss: -7.613
Iter 1998/2000 - Loss: -7.613
Iter 1999/2000 - Loss: -7.613
Iter 2000/2000 - Loss: -7.613
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[11.9341,  6.7082, 20.4199,  3.5736,  7.1953, 52.3688]],

        [[18.3198, 31.5221,  8.0312,  1.3182,  1.7267, 24.1467]],

        [[20.8079, 26.9512,  8.8545,  0.9224,  1.0396, 27.5000]],

        [[14.8301, 28.0474, 12.2081,  1.1208,  3.1672, 47.4997]]])
Signal Variance: tensor([ 0.0944,  2.0531, 16.7836,  0.3862])
Estimated target variance: tensor([0.0194, 0.9805, 8.0771, 0.1694])
N: 260
Signal to noise ratio: tensor([17.7437, 74.1399, 79.2086, 34.8313])
Bound on condition number: tensor([  81859.4302, 1429148.7257, 1631240.8314,  315438.6713])
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 0.004340501972652825, policy loss: 6.841120430588689
Experience 26, Iter 1, disc loss: 0.0038310365048087715, policy loss: 6.834932752434202
Experience 26, Iter 2, disc loss: 0.0034773773957729693, policy loss: 7.18630070330186
Experience 26, Iter 3, disc loss: 0.0039293107534720245, policy loss: 6.744168737641237
Experience 26, Iter 4, disc loss: 0.0037285337187554406, policy loss: 6.977365851108722
Experience 26, Iter 5, disc loss: 0.0037976029249615563, policy loss: 6.864753189103723
Experience 26, Iter 6, disc loss: 0.003961938078937291, policy loss: 7.3336644539603535
Experience 26, Iter 7, disc loss: 0.004127396416070177, policy loss: 7.084705166758027
Experience 26, Iter 8, disc loss: 0.004079311734706563, policy loss: 6.821948653949428
Experience 26, Iter 9, disc loss: 0.004773680457880274, policy loss: 6.768207414721197
Experience 26, Iter 10, disc loss: 0.0037511780743287367, policy loss: 6.98305575831779
Experience 26, Iter 11, disc loss: 0.004105657489891908, policy loss: 6.716721126316664
Experience 26, Iter 12, disc loss: 0.004230973881270974, policy loss: 7.42613031799475
Experience 26, Iter 13, disc loss: 0.003957358273973437, policy loss: 6.9440174087914635
Experience 26, Iter 14, disc loss: 0.00436858824309557, policy loss: 7.3818884860129526
Experience 26, Iter 15, disc loss: 0.0043225432409063425, policy loss: 6.869056775778596
Experience 26, Iter 16, disc loss: 0.003936337459570848, policy loss: 7.556135929295095
Experience 26, Iter 17, disc loss: 0.004545818783877095, policy loss: 7.0254233137560576
Experience 26, Iter 18, disc loss: 0.004887021426273446, policy loss: 6.564311135099237
Experience 26, Iter 19, disc loss: 0.004355341574567464, policy loss: 7.063032250860846
Experience 26, Iter 20, disc loss: 0.004605681285763328, policy loss: 6.541127189386788
Experience 26, Iter 21, disc loss: 0.004376225556738424, policy loss: 6.645509082258412
Experience 26, Iter 22, disc loss: 0.00406579966183163, policy loss: 6.803405269106329
Experience 26, Iter 23, disc loss: 0.004497480933943029, policy loss: 6.504488688032637
Experience 26, Iter 24, disc loss: 0.004175642411406271, policy loss: 6.658709342859964
Experience 26, Iter 25, disc loss: 0.003978841015215588, policy loss: 6.937580011646126
Experience 26, Iter 26, disc loss: 0.004305977151404154, policy loss: 7.087340754896276
Experience 26, Iter 27, disc loss: 0.003938018461038012, policy loss: 6.960099033687726
Experience 26, Iter 28, disc loss: 0.0036879863180292916, policy loss: 7.158325369022299
Experience 26, Iter 29, disc loss: 0.003763642565551095, policy loss: 7.403708623549697
Experience 26, Iter 30, disc loss: 0.003924884422655723, policy loss: 6.686342291852322
Experience 26, Iter 31, disc loss: 0.00454217008981558, policy loss: 6.6872493415548115
Experience 26, Iter 32, disc loss: 0.0037531624657862446, policy loss: 7.188260065308926
Experience 26, Iter 33, disc loss: 0.004044728729775757, policy loss: 7.055722156291484
Experience 26, Iter 34, disc loss: 0.003983266643135401, policy loss: 7.017614138454295
Experience 26, Iter 35, disc loss: 0.005006040092003679, policy loss: 6.378395855463035
Experience 26, Iter 36, disc loss: 0.004381333618255477, policy loss: 6.7161271757947985
Experience 26, Iter 37, disc loss: 0.004324423453948725, policy loss: 6.798151893402659
Experience 26, Iter 38, disc loss: 0.004340037037845602, policy loss: 6.697517686114502
Experience 26, Iter 39, disc loss: 0.005149178400546645, policy loss: 7.001576134716071
Experience 26, Iter 40, disc loss: 0.0040079724331894395, policy loss: 7.517163450560581
Experience 26, Iter 41, disc loss: 0.004275368656019181, policy loss: 6.9683630801316205
Experience 26, Iter 42, disc loss: 0.0038835945675137766, policy loss: 7.247063011946772
Experience 26, Iter 43, disc loss: 0.004574289059787656, policy loss: 7.147420360064158
Experience 26, Iter 44, disc loss: 0.004373549182566477, policy loss: 6.486293086737169
Experience 26, Iter 45, disc loss: 0.0038683126656153992, policy loss: 7.504011610095547
Experience 26, Iter 46, disc loss: 0.003774225588929237, policy loss: 7.2178582225175525
Experience 26, Iter 47, disc loss: 0.004346641772502576, policy loss: 7.090164280205842
Experience 26, Iter 48, disc loss: 0.0040602277085557015, policy loss: 7.517386330444149
Experience 26, Iter 49, disc loss: 0.003985635828578376, policy loss: 7.035456214822268
Experience 26, Iter 50, disc loss: 0.004409576493894953, policy loss: 6.835447563692144
Experience 26, Iter 51, disc loss: 0.003299660851698394, policy loss: 7.5533082534821325
Experience 26, Iter 52, disc loss: 0.0033478606623693306, policy loss: 7.404979900701268
Experience 26, Iter 53, disc loss: 0.0035541967738766224, policy loss: 7.382741032025764
Experience 26, Iter 54, disc loss: 0.004510569309634576, policy loss: 6.55444643440814
Experience 26, Iter 55, disc loss: 0.003958933748688907, policy loss: 7.1257335186125275
Experience 26, Iter 56, disc loss: 0.003090380463770602, policy loss: 7.6432322741250935
Experience 26, Iter 57, disc loss: 0.0028898488387997558, policy loss: 7.921076877367652
Experience 26, Iter 58, disc loss: 0.002788850431669772, policy loss: 7.6648363426640085
Experience 26, Iter 59, disc loss: 0.003911348536665344, policy loss: 6.927826796113331
Experience 26, Iter 60, disc loss: 0.0036746610043213203, policy loss: 7.200808041658111
Experience 26, Iter 61, disc loss: 0.0038107312553678644, policy loss: 7.108798836579136
Experience 26, Iter 62, disc loss: 0.003405563179506098, policy loss: 7.121638710210579
Experience 26, Iter 63, disc loss: 0.0038018596264155838, policy loss: 6.869830242283882
Experience 26, Iter 64, disc loss: 0.004181603446326847, policy loss: 6.9883661433481485
Experience 26, Iter 65, disc loss: 0.0031310296204882823, policy loss: 7.384888720069488
Experience 26, Iter 66, disc loss: 0.003576996814996777, policy loss: 7.198787334732456
Experience 26, Iter 67, disc loss: 0.0038417360851131497, policy loss: 7.372052114783532
Experience 26, Iter 68, disc loss: 0.004721963268079523, policy loss: 7.436857742157503
Experience 26, Iter 69, disc loss: 0.003425322420227854, policy loss: 7.374197453365262
Experience 26, Iter 70, disc loss: 0.0037218926295859572, policy loss: 6.846824705957643
Experience 26, Iter 71, disc loss: 0.004052817505804907, policy loss: 6.70325764726193
Experience 26, Iter 72, disc loss: 0.00395409665315683, policy loss: 6.717673593552934
Experience 26, Iter 73, disc loss: 0.003260096619465193, policy loss: 7.090097616227142
Experience 26, Iter 74, disc loss: 0.0031712283922398056, policy loss: 7.486121765641452
Experience 26, Iter 75, disc loss: 0.003813654739074881, policy loss: 7.248287765630481
Experience 26, Iter 76, disc loss: 0.003213049361362166, policy loss: 7.303728211512023
Experience 26, Iter 77, disc loss: 0.003187743338211347, policy loss: 7.29247672641435
Experience 26, Iter 78, disc loss: 0.0028218119645854745, policy loss: 7.387574436658069
Experience 26, Iter 79, disc loss: 0.003437669369431742, policy loss: 7.008293041835256
Experience 26, Iter 80, disc loss: 0.003341283454903944, policy loss: 7.479445114950883
Experience 26, Iter 81, disc loss: 0.0027642766687097278, policy loss: 8.204507062468846
Experience 26, Iter 82, disc loss: 0.0021477105255228992, policy loss: 8.683185450126235
Experience 26, Iter 83, disc loss: 0.0017521740110055116, policy loss: 10.204918492338138
Experience 26, Iter 84, disc loss: 0.0016945663815982062, policy loss: 12.323932698472952
Experience 26, Iter 85, disc loss: 0.0016287426595061108, policy loss: 15.190517948022121
Experience 26, Iter 86, disc loss: 0.001589968197051036, policy loss: 15.794586430458093
Experience 26, Iter 87, disc loss: 0.0015477042063377989, policy loss: 15.48087348780302
Experience 26, Iter 88, disc loss: 0.0015172546085020027, policy loss: 11.813643587013011
Experience 26, Iter 89, disc loss: 0.001456821232051273, policy loss: 15.011879465182808
Experience 26, Iter 90, disc loss: 0.0014119666361131968, policy loss: 13.932565366262555
Experience 26, Iter 91, disc loss: 0.0013632545003873662, policy loss: 14.401162755953985
Experience 26, Iter 92, disc loss: 0.001311038999695808, policy loss: 17.72289121492533
Experience 26, Iter 93, disc loss: 0.0012643327952741588, policy loss: 14.636044658227906
Experience 26, Iter 94, disc loss: 0.0012287350700313623, policy loss: 13.20419211122958
Experience 26, Iter 95, disc loss: 0.0011804455132494503, policy loss: 13.371085512233641
Experience 26, Iter 96, disc loss: 0.001269989942958955, policy loss: 10.754175579168635
Experience 26, Iter 97, disc loss: 0.0020469684439273056, policy loss: 10.076398313477807
Experience 26, Iter 98, disc loss: 0.001531942033633467, policy loss: 10.523662209657836
Experience 26, Iter 99, disc loss: 0.001066276567074039, policy loss: 11.360827532753095
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2469],
        [2.0292],
        [0.0421]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0195, 0.2006, 1.9686, 0.0371, 0.0290, 6.2174]],

        [[0.0195, 0.2006, 1.9686, 0.0371, 0.0290, 6.2174]],

        [[0.0195, 0.2006, 1.9686, 0.0371, 0.0290, 6.2174]],

        [[0.0195, 0.2006, 1.9686, 0.0371, 0.0290, 6.2174]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0192, 0.9877, 8.1169, 0.1682], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0192, 0.9877, 8.1169, 0.1682])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.803
Iter 2/2000 - Loss: 3.733
Iter 3/2000 - Loss: 3.482
Iter 4/2000 - Loss: 3.384
Iter 5/2000 - Loss: 3.315
Iter 6/2000 - Loss: 3.164
Iter 7/2000 - Loss: 2.986
Iter 8/2000 - Loss: 2.819
Iter 9/2000 - Loss: 2.647
Iter 10/2000 - Loss: 2.442
Iter 11/2000 - Loss: 2.216
Iter 12/2000 - Loss: 1.988
Iter 13/2000 - Loss: 1.762
Iter 14/2000 - Loss: 1.528
Iter 15/2000 - Loss: 1.278
Iter 16/2000 - Loss: 1.009
Iter 17/2000 - Loss: 0.730
Iter 18/2000 - Loss: 0.448
Iter 19/2000 - Loss: 0.167
Iter 20/2000 - Loss: -0.114
Iter 1981/2000 - Loss: -7.636
Iter 1982/2000 - Loss: -7.636
Iter 1983/2000 - Loss: -7.636
Iter 1984/2000 - Loss: -7.636
Iter 1985/2000 - Loss: -7.636
Iter 1986/2000 - Loss: -7.636
Iter 1987/2000 - Loss: -7.637
Iter 1988/2000 - Loss: -7.637
Iter 1989/2000 - Loss: -7.637
Iter 1990/2000 - Loss: -7.637
Iter 1991/2000 - Loss: -7.637
Iter 1992/2000 - Loss: -7.637
Iter 1993/2000 - Loss: -7.637
Iter 1994/2000 - Loss: -7.637
Iter 1995/2000 - Loss: -7.637
Iter 1996/2000 - Loss: -7.637
Iter 1997/2000 - Loss: -7.637
Iter 1998/2000 - Loss: -7.637
Iter 1999/2000 - Loss: -7.637
Iter 2000/2000 - Loss: -7.637
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[11.5251,  4.3962, 21.7063,  3.8897, 10.0820, 47.9323]],

        [[18.0874, 31.3613,  8.2325,  1.2326,  1.6970, 21.7093]],

        [[20.9081, 29.2076,  9.0365,  0.8637,  1.0273, 27.2567]],

        [[15.3248, 27.9461, 12.2038,  1.1301,  3.0553, 45.2817]]])
Signal Variance: tensor([ 0.0661,  1.8777, 16.6867,  0.3731])
Estimated target variance: tensor([0.0192, 0.9877, 8.1169, 0.1682])
N: 270
Signal to noise ratio: tensor([14.7685, 71.6584, 79.2336, 33.7774])
Bound on condition number: tensor([  58889.9239, 1386429.6489, 1695052.9779,  308047.4077])
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 0.0009834261519678813, policy loss: 20.5755448502262
Experience 27, Iter 1, disc loss: 0.0009518524016459202, policy loss: 22.24916579766045
Experience 27, Iter 2, disc loss: 0.0009214322883171179, policy loss: 23.17610355592147
Experience 27, Iter 3, disc loss: 0.0008921765300432683, policy loss: 23.07846087145375
Experience 27, Iter 4, disc loss: 0.000864110133747766, policy loss: 22.567626034191353
Experience 27, Iter 5, disc loss: 0.0008372813091723608, policy loss: 22.532480511948965
Experience 27, Iter 6, disc loss: 0.0008116592410336947, policy loss: 23.146853894620694
Experience 27, Iter 7, disc loss: 0.0007872412808940806, policy loss: 22.734874527689193
Experience 27, Iter 8, disc loss: 0.0007639899409911652, policy loss: 22.825127990500782
Experience 27, Iter 9, disc loss: 0.000741864020718869, policy loss: 24.15251080344487
Experience 27, Iter 10, disc loss: 0.0007208307770140312, policy loss: 23.52231612149334
Experience 27, Iter 11, disc loss: 0.0007007995817324473, policy loss: 23.321388852807253
Experience 27, Iter 12, disc loss: 0.0006817512422603849, policy loss: 22.419427627150434
Experience 27, Iter 13, disc loss: 0.000663635995847612, policy loss: 22.212051540778457
Experience 27, Iter 14, disc loss: 0.0006464112818670176, policy loss: 22.748333914951253
Experience 27, Iter 15, disc loss: 0.0006300248899404331, policy loss: 23.57766445923647
Experience 27, Iter 16, disc loss: 0.0006144610337622516, policy loss: 22.64924833139271
Experience 27, Iter 17, disc loss: 0.0005996497761852755, policy loss: 22.586545925120795
Experience 27, Iter 18, disc loss: 0.000585558759413368, policy loss: 22.496174729664272
Experience 27, Iter 19, disc loss: 0.0005721717460644706, policy loss: 21.14795532339978
Experience 27, Iter 20, disc loss: 0.0005594213164692976, policy loss: 20.110720484999987
Experience 27, Iter 21, disc loss: 0.0005472482763940889, policy loss: 20.040351577821696
Experience 27, Iter 22, disc loss: 0.0005356614170311169, policy loss: 20.691143015751653
Experience 27, Iter 23, disc loss: 0.000524639419816608, policy loss: 19.85716682537566
Experience 27, Iter 24, disc loss: 0.0005141469827685397, policy loss: 19.033392625472253
Experience 27, Iter 25, disc loss: 0.000504306769301924, policy loss: 18.597868095578278
Experience 27, Iter 26, disc loss: 0.0004951791232823547, policy loss: 17.199727061361095
Experience 27, Iter 27, disc loss: 0.00048649988605393274, policy loss: 16.706138167741624
Experience 27, Iter 28, disc loss: 0.00047831569747746596, policy loss: 17.073145818704113
Experience 27, Iter 29, disc loss: 0.00047349479978243183, policy loss: 16.746083925960562
Experience 27, Iter 30, disc loss: 0.00047339407317527885, policy loss: 16.907369637193128
Experience 27, Iter 31, disc loss: 0.0004909498117460332, policy loss: 15.844236973330315
Experience 27, Iter 32, disc loss: 0.0004789981346283681, policy loss: 15.34716642908128
Experience 27, Iter 33, disc loss: 0.0004915570519304162, policy loss: 15.186471092186933
Experience 27, Iter 34, disc loss: 0.0004469175152043163, policy loss: 16.159843368907488
Experience 27, Iter 35, disc loss: 0.00043859723698627727, policy loss: 14.885031924965094
Experience 27, Iter 36, disc loss: 0.00043378758890996174, policy loss: 15.99047102023144
Experience 27, Iter 37, disc loss: 0.00043206583743391066, policy loss: 17.586245651837043
Experience 27, Iter 38, disc loss: 0.00042995615867986043, policy loss: 16.826326042047754
Experience 27, Iter 39, disc loss: 0.00040779103390637864, policy loss: 16.509610123738053
Experience 27, Iter 40, disc loss: 0.0004037551518161665, policy loss: 16.366457660258796
Experience 27, Iter 41, disc loss: 0.00040478360450999504, policy loss: 16.008348505354718
Experience 27, Iter 42, disc loss: 0.00038915685307681664, policy loss: 16.12093109745227
Experience 27, Iter 43, disc loss: 0.00038969245024650666, policy loss: 14.649554482182879
Experience 27, Iter 44, disc loss: 0.00037884433052661593, policy loss: 15.601951082676145
Experience 27, Iter 45, disc loss: 0.0003758307038689204, policy loss: 15.970327974600405
Experience 27, Iter 46, disc loss: 0.0003764410832934897, policy loss: 14.331291548232551
Experience 27, Iter 47, disc loss: 0.0003722348974486429, policy loss: 14.636597270286526
Experience 27, Iter 48, disc loss: 0.0003679865450909897, policy loss: 14.429029595784714
Experience 27, Iter 49, disc loss: 0.0003713517033825007, policy loss: 14.140541847962846
Experience 27, Iter 50, disc loss: 0.0003899883221511415, policy loss: 13.256563322516724
Experience 27, Iter 51, disc loss: 0.0004098452540674532, policy loss: 13.106515980550396
Experience 27, Iter 52, disc loss: 0.000394976988971185, policy loss: 14.641792466025032
Experience 27, Iter 53, disc loss: 0.0004291065111237989, policy loss: 12.978190152547345
Experience 27, Iter 54, disc loss: 0.00039816490576645354, policy loss: 15.371435174734845
Experience 27, Iter 55, disc loss: 0.00036605592238224557, policy loss: 15.199803473909485
Experience 27, Iter 56, disc loss: 0.0003668704136954989, policy loss: 15.092022291773683
Experience 27, Iter 57, disc loss: 0.00034089983484095096, policy loss: 17.29636026663713
Experience 27, Iter 58, disc loss: 0.00033079581119338665, policy loss: 17.79415541425532
Experience 27, Iter 59, disc loss: 0.00032713601122658553, policy loss: 16.612524329601932
Experience 27, Iter 60, disc loss: 0.0003202449396222139, policy loss: 18.933176320283195
Experience 27, Iter 61, disc loss: 0.00031700683898866494, policy loss: 19.11028541586592
Experience 27, Iter 62, disc loss: 0.000313526533101538, policy loss: 18.94658681227301
Experience 27, Iter 63, disc loss: 0.0003104550074025076, policy loss: 38.2248560858279
Experience 27, Iter 64, disc loss: 0.0003077644527473039, policy loss: 39.62584016113463
Experience 27, Iter 65, disc loss: 0.0003051255882320155, policy loss: 40.61205645124138
Experience 27, Iter 66, disc loss: 0.000302530489473249, policy loss: 41.22490382508006
Experience 27, Iter 67, disc loss: 0.00029997817855111097, policy loss: 40.749653493812175
Experience 27, Iter 68, disc loss: 0.000297468148921141, policy loss: 36.053040326002886
Experience 27, Iter 69, disc loss: 0.00029499984248291145, policy loss: 35.98126098398299
Experience 27, Iter 70, disc loss: 0.00029257265215342534, policy loss: 35.47713641619812
Experience 27, Iter 71, disc loss: 0.00029018594415712725, policy loss: 35.71144313877943
Experience 27, Iter 72, disc loss: 0.00028783904769425135, policy loss: 35.305125778626866
Experience 27, Iter 73, disc loss: 0.00028553127828050313, policy loss: 40.05052648223858
Experience 27, Iter 74, disc loss: 0.00028326193043502106, policy loss: 37.12459413270955
Experience 27, Iter 75, disc loss: 0.0002810302927269075, policy loss: 36.39521266296495
Experience 27, Iter 76, disc loss: 0.0002789398199759457, policy loss: 33.780309315827154
Experience 27, Iter 77, disc loss: 0.0002767110255782911, policy loss: 30.66308615439076
Experience 27, Iter 78, disc loss: 0.00028045275185245436, policy loss: 29.13332916192902
Experience 27, Iter 79, disc loss: 0.00027266899697339495, policy loss: 26.321283956903184
Experience 27, Iter 80, disc loss: 0.00027172628947550137, policy loss: 19.44147606002917
Experience 27, Iter 81, disc loss: 0.0002950926047100541, policy loss: 17.73003423653725
Experience 27, Iter 82, disc loss: 0.000266435186557788, policy loss: 21.741358132579535
Experience 27, Iter 83, disc loss: 0.0002645356433598267, policy loss: 20.651867932881537
Experience 27, Iter 84, disc loss: 0.00026265791692677254, policy loss: 20.003250109517943
Experience 27, Iter 85, disc loss: 0.0002606924006310231, policy loss: 20.496186062656314
Experience 27, Iter 86, disc loss: 0.00025883048735017485, policy loss: 21.621177713707226
Experience 27, Iter 87, disc loss: 0.00025701917446700214, policy loss: 21.835289621432317
Experience 27, Iter 88, disc loss: 0.00025991668566069034, policy loss: 17.04186619613325
Experience 27, Iter 89, disc loss: 0.00839773176314941, policy loss: 7.607669211285388
Experience 27, Iter 90, disc loss: 0.08368620063447475, policy loss: 5.969324212379485
Experience 27, Iter 91, disc loss: 0.048706836681086094, policy loss: 6.82617514096408
Experience 27, Iter 92, disc loss: 0.01216289976132439, policy loss: 6.915753187957135
Experience 27, Iter 93, disc loss: 0.03321910234699424, policy loss: 7.674474381959328
Experience 27, Iter 94, disc loss: 0.012652481444263682, policy loss: 9.24306971324037
Experience 27, Iter 95, disc loss: 0.025813095922746575, policy loss: 8.862528131679833
Experience 27, Iter 96, disc loss: 0.06313562172651098, policy loss: 7.865710204030849
Experience 27, Iter 97, disc loss: 0.02931270144528855, policy loss: 8.354204723851264
Experience 27, Iter 98, disc loss: 0.003949907448105689, policy loss: 9.265925604173443
Experience 27, Iter 99, disc loss: 0.00620453717806135, policy loss: 9.768812882195395
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2454],
        [2.0550],
        [0.0422]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0195, 0.2028, 1.9774, 0.0375, 0.0283, 6.1389]],

        [[0.0195, 0.2028, 1.9774, 0.0375, 0.0283, 6.1389]],

        [[0.0195, 0.2028, 1.9774, 0.0375, 0.0283, 6.1389]],

        [[0.0195, 0.2028, 1.9774, 0.0375, 0.0283, 6.1389]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0193, 0.9814, 8.2199, 0.1688], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0193, 0.9814, 8.2199, 0.1688])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.809
Iter 2/2000 - Loss: 3.765
Iter 3/2000 - Loss: 3.507
Iter 4/2000 - Loss: 3.422
Iter 5/2000 - Loss: 3.365
Iter 6/2000 - Loss: 3.218
Iter 7/2000 - Loss: 3.037
Iter 8/2000 - Loss: 2.871
Iter 9/2000 - Loss: 2.709
Iter 10/2000 - Loss: 2.520
Iter 11/2000 - Loss: 2.303
Iter 12/2000 - Loss: 2.077
Iter 13/2000 - Loss: 1.851
Iter 14/2000 - Loss: 1.621
Iter 15/2000 - Loss: 1.378
Iter 16/2000 - Loss: 1.116
Iter 17/2000 - Loss: 0.840
Iter 18/2000 - Loss: 0.556
Iter 19/2000 - Loss: 0.272
Iter 20/2000 - Loss: -0.011
Iter 1981/2000 - Loss: -7.569
Iter 1982/2000 - Loss: -7.569
Iter 1983/2000 - Loss: -7.569
Iter 1984/2000 - Loss: -7.569
Iter 1985/2000 - Loss: -7.569
Iter 1986/2000 - Loss: -7.569
Iter 1987/2000 - Loss: -7.569
Iter 1988/2000 - Loss: -7.569
Iter 1989/2000 - Loss: -7.569
Iter 1990/2000 - Loss: -7.569
Iter 1991/2000 - Loss: -7.569
Iter 1992/2000 - Loss: -7.569
Iter 1993/2000 - Loss: -7.569
Iter 1994/2000 - Loss: -7.569
Iter 1995/2000 - Loss: -7.569
Iter 1996/2000 - Loss: -7.569
Iter 1997/2000 - Loss: -7.569
Iter 1998/2000 - Loss: -7.569
Iter 1999/2000 - Loss: -7.569
Iter 2000/2000 - Loss: -7.569
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[11.5931,  5.3402, 18.2356,  3.8168, 11.6425, 55.2048]],

        [[17.6655, 31.9339,  8.4377,  1.2205,  1.4288, 17.4867]],

        [[20.6284, 30.1186,  9.1213,  0.9152,  0.8447, 22.6814]],

        [[15.1587, 28.3258, 11.6259,  1.1402,  2.0346, 50.3563]]])
Signal Variance: tensor([ 0.0761,  1.5487, 13.2909,  0.3425])
Estimated target variance: tensor([0.0193, 0.9814, 8.2199, 0.1688])
N: 280
Signal to noise ratio: tensor([15.6575, 65.7761, 69.0469, 32.2954])
Bound on condition number: tensor([  68644.9556, 1211420.9187, 1334895.5870,  292039.0387])
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 0.008774375398115647, policy loss: 8.048826794007086
Experience 28, Iter 1, disc loss: 0.01244507950476792, policy loss: 8.448840920918906
Experience 28, Iter 2, disc loss: 0.005308060231924444, policy loss: 8.528916225607183
Experience 28, Iter 3, disc loss: 0.011412506907626739, policy loss: 7.882444384151134
Experience 28, Iter 4, disc loss: 0.005935667642040751, policy loss: 9.18943308692925
Experience 28, Iter 5, disc loss: 0.008825840212980447, policy loss: 7.8686528926697905
Experience 28, Iter 6, disc loss: 0.0039101981269715645, policy loss: 9.123351387905414
Experience 28, Iter 7, disc loss: 0.003078290030846635, policy loss: 8.115838276702855
Experience 28, Iter 8, disc loss: 0.0022009361666864113, policy loss: 8.71356526176925
Experience 28, Iter 9, disc loss: 0.00570361845345046, policy loss: 10.983806324552171
Experience 28, Iter 10, disc loss: 0.006121646758571212, policy loss: 12.499383840443402
Experience 28, Iter 11, disc loss: 0.0012431736711600119, policy loss: 12.143373710581926
Experience 28, Iter 12, disc loss: 0.0014843276691838927, policy loss: 12.524516841544697
Experience 28, Iter 13, disc loss: 0.0008386412854139458, policy loss: 18.59954195690547
Experience 28, Iter 14, disc loss: 0.0008631100326792269, policy loss: 21.86856510957093
Experience 28, Iter 15, disc loss: 0.0008833888927395642, policy loss: 23.67082872255227
Experience 28, Iter 16, disc loss: 0.0008989953677106038, policy loss: 22.891635359613893
Experience 28, Iter 17, disc loss: 0.0009106560188190658, policy loss: 24.35152703119026
Experience 28, Iter 18, disc loss: 0.0009184605761021987, policy loss: 25.450622032333186
Experience 28, Iter 19, disc loss: 0.03703107506315514, policy loss: 25.558291421753868
Experience 28, Iter 20, disc loss: 0.0009350959448714151, policy loss: 27.3570140240799
Experience 28, Iter 21, disc loss: 0.0009369703791888854, policy loss: 29.70495506889248
Experience 28, Iter 22, disc loss: 0.000938895701667862, policy loss: 33.261031647743664
Experience 28, Iter 23, disc loss: 0.0009412431443414004, policy loss: 39.32052585145644
Experience 28, Iter 24, disc loss: 0.0009330383405798635, policy loss: 39.669983571992496
Experience 28, Iter 25, disc loss: 0.000926273946575956, policy loss: 38.780962184609955
Experience 28, Iter 26, disc loss: 0.0009173520020869272, policy loss: 35.784542214911454
Experience 28, Iter 27, disc loss: 0.0009065646007203694, policy loss: 32.47015663333182
Experience 28, Iter 28, disc loss: 0.0014729228596785503, policy loss: 19.183059050419992
Experience 28, Iter 29, disc loss: 0.0008813278247951904, policy loss: 20.135506141835446
Experience 28, Iter 30, disc loss: 0.0008671926711396119, policy loss: 21.850726298940078
Experience 28, Iter 31, disc loss: 0.0010682769897127087, policy loss: 21.549621303499812
Experience 28, Iter 32, disc loss: 0.0008366441971160482, policy loss: 46.44314533833993
Experience 28, Iter 33, disc loss: 0.0008206979969012807, policy loss: 37.800587309150735
Experience 28, Iter 34, disc loss: 0.1777062020479441, policy loss: 11.797455772341454
Experience 28, Iter 35, disc loss: 0.008273824282310487, policy loss: 17.356184717571878
Experience 28, Iter 36, disc loss: 0.0008433821688518521, policy loss: 18.637251469444102
Experience 28, Iter 37, disc loss: 0.0008502353703764131, policy loss: 47.40920170949197
Experience 28, Iter 38, disc loss: 0.0008601325570477663, policy loss: 44.57014408177951
Experience 28, Iter 39, disc loss: 0.0008666529050388143, policy loss: 28.661274727740732
Experience 28, Iter 40, disc loss: 0.02162348554311103, policy loss: 15.284030730250812
Experience 28, Iter 41, disc loss: 0.05921376317872059, policy loss: 12.803639697845593
Experience 28, Iter 42, disc loss: 0.0009186724038828003, policy loss: 25.46103939247092
Experience 28, Iter 43, disc loss: 0.0009461275721926647, policy loss: 41.868730459497726
Experience 28, Iter 44, disc loss: 0.0009687200515008201, policy loss: 43.85584302631119
Experience 28, Iter 45, disc loss: 0.0009863023244828305, policy loss: 42.224338166015556
Experience 28, Iter 46, disc loss: 0.0009990414514931456, policy loss: 35.91322979280484
Experience 28, Iter 47, disc loss: 0.0010074663544449626, policy loss: 30.013732405589057
Experience 28, Iter 48, disc loss: 0.0010137296344094107, policy loss: 23.63735813290985
Experience 28, Iter 49, disc loss: 0.0010183346414575805, policy loss: 21.16021577113783
Experience 28, Iter 50, disc loss: 0.08477784025708168, policy loss: 18.87013983860451
Experience 28, Iter 51, disc loss: 0.032478904666311946, policy loss: 15.766934319818995
Experience 28, Iter 52, disc loss: 0.31362254681497004, policy loss: 18.022384613302826
Experience 28, Iter 53, disc loss: 0.012427996974322335, policy loss: 24.715376752398395
Experience 28, Iter 54, disc loss: 0.0077050931893910815, policy loss: 24.795592057927163
Experience 28, Iter 55, disc loss: 0.009184687689527949, policy loss: 25.59778689573725
Experience 28, Iter 56, disc loss: 0.017607879826561278, policy loss: 24.04941141642346
Experience 28, Iter 57, disc loss: 0.002086037225000455, policy loss: 24.192172093556497
Experience 28, Iter 58, disc loss: 0.012135051560470187, policy loss: 25.514131221009464
Experience 28, Iter 59, disc loss: 0.002577029501851009, policy loss: 24.825933631805533
Experience 28, Iter 60, disc loss: 0.055419613488414565, policy loss: 23.340937569484012
Experience 28, Iter 61, disc loss: 0.004118929203275075, policy loss: 25.750383390083137
Experience 28, Iter 62, disc loss: 0.024516631601048413, policy loss: 20.284009415775646
Experience 28, Iter 63, disc loss: 0.05728301955158721, policy loss: 19.759775937410126
Experience 28, Iter 64, disc loss: 0.051541095451414325, policy loss: 19.020926624563067
Experience 28, Iter 65, disc loss: 0.009329757004823422, policy loss: 19.342303135946246
Experience 28, Iter 66, disc loss: 0.34634498684394394, policy loss: 16.033967255872575
Experience 28, Iter 67, disc loss: 0.2008111889809551, policy loss: 17.118643608663053
Experience 28, Iter 68, disc loss: 0.07049921225948334, policy loss: 16.423094526468137
Experience 28, Iter 69, disc loss: 0.12398862674473204, policy loss: 16.108332548079737
Experience 28, Iter 70, disc loss: 0.03053960700628129, policy loss: 16.88875742994124
Experience 28, Iter 71, disc loss: 0.05626894957120451, policy loss: 20.590078484034528
Experience 28, Iter 72, disc loss: 0.013361364310846209, policy loss: 21.81417289231035
Experience 28, Iter 73, disc loss: 0.01750504360219666, policy loss: 22.666320357897312
Experience 28, Iter 74, disc loss: 0.05817656860117397, policy loss: 25.130135483981284
Experience 28, Iter 75, disc loss: 0.11470618936350185, policy loss: 20.802312983350312
Experience 28, Iter 76, disc loss: 0.015935726513077383, policy loss: 20.06170260861797
Experience 28, Iter 77, disc loss: 0.07073126756812542, policy loss: 20.690350778062914
Experience 28, Iter 78, disc loss: 0.07048624365805217, policy loss: 19.07685403505692
Experience 28, Iter 79, disc loss: 0.013661806539182687, policy loss: 21.852452958800118
Experience 28, Iter 80, disc loss: 0.012205692362028505, policy loss: 20.891914497899528
Experience 28, Iter 81, disc loss: 0.013356082790094948, policy loss: 21.700127507258564
Experience 28, Iter 82, disc loss: 0.0422985638375672, policy loss: 21.155911491524385
Experience 28, Iter 83, disc loss: 0.02769688934351216, policy loss: 20.968695511622656
Experience 28, Iter 84, disc loss: 0.011695100764923654, policy loss: 23.050878315506804
Experience 28, Iter 85, disc loss: 0.016954207635866442, policy loss: 24.85748413303123
Experience 28, Iter 86, disc loss: 0.006220849573768292, policy loss: 24.015437878017522
Experience 28, Iter 87, disc loss: 0.009317212888278798, policy loss: 26.915998905600958
Experience 28, Iter 88, disc loss: 0.05794370295936937, policy loss: 27.68564559309462
Experience 28, Iter 89, disc loss: 0.044119696289872695, policy loss: 30.90148901125331
Experience 28, Iter 90, disc loss: 0.004164985010142438, policy loss: 32.26772949107873
Experience 28, Iter 91, disc loss: 0.0038408445541077544, policy loss: 38.81118232370578
Experience 28, Iter 92, disc loss: 0.00356707923114922, policy loss: 40.740048884436966
Experience 28, Iter 93, disc loss: 0.003305073773261342, policy loss: 44.71745152652791
Experience 28, Iter 94, disc loss: 0.003062654774657013, policy loss: 43.56245107752002
Experience 28, Iter 95, disc loss: 0.002834648023418225, policy loss: 41.62701464533218
Experience 28, Iter 96, disc loss: 0.0026400746960236273, policy loss: 44.54414847672273
Experience 28, Iter 97, disc loss: 0.00246724856524671, policy loss: 43.896379649670706
Experience 28, Iter 98, disc loss: 0.0023072922580544934, policy loss: 43.09129596369617
Experience 28, Iter 99, disc loss: 0.00215973812050435, policy loss: 41.91454868783218
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.2494],
        [2.1188],
        [0.0424]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0196, 0.2135, 1.9913, 0.0375, 0.0278, 6.1813]],

        [[0.0196, 0.2135, 1.9913, 0.0375, 0.0278, 6.1813]],

        [[0.0196, 0.2135, 1.9913, 0.0375, 0.0278, 6.1813]],

        [[0.0196, 0.2135, 1.9913, 0.0375, 0.0278, 6.1813]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0204, 0.9974, 8.4754, 0.1696], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0204, 0.9974, 8.4754, 0.1696])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.849
Iter 2/2000 - Loss: 3.830
Iter 3/2000 - Loss: 3.566
Iter 4/2000 - Loss: 3.493
Iter 5/2000 - Loss: 3.448
Iter 6/2000 - Loss: 3.303
Iter 7/2000 - Loss: 3.117
Iter 8/2000 - Loss: 2.947
Iter 9/2000 - Loss: 2.793
Iter 10/2000 - Loss: 2.621
Iter 11/2000 - Loss: 2.419
Iter 12/2000 - Loss: 2.196
Iter 13/2000 - Loss: 1.967
Iter 14/2000 - Loss: 1.734
Iter 15/2000 - Loss: 1.493
Iter 16/2000 - Loss: 1.238
Iter 17/2000 - Loss: 0.968
Iter 18/2000 - Loss: 0.686
Iter 19/2000 - Loss: 0.399
Iter 20/2000 - Loss: 0.111
Iter 1981/2000 - Loss: -7.584
Iter 1982/2000 - Loss: -7.584
Iter 1983/2000 - Loss: -7.584
Iter 1984/2000 - Loss: -7.584
Iter 1985/2000 - Loss: -7.584
Iter 1986/2000 - Loss: -7.584
Iter 1987/2000 - Loss: -7.584
Iter 1988/2000 - Loss: -7.584
Iter 1989/2000 - Loss: -7.584
Iter 1990/2000 - Loss: -7.584
Iter 1991/2000 - Loss: -7.584
Iter 1992/2000 - Loss: -7.584
Iter 1993/2000 - Loss: -7.584
Iter 1994/2000 - Loss: -7.584
Iter 1995/2000 - Loss: -7.584
Iter 1996/2000 - Loss: -7.584
Iter 1997/2000 - Loss: -7.584
Iter 1998/2000 - Loss: -7.584
Iter 1999/2000 - Loss: -7.584
Iter 2000/2000 - Loss: -7.584
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0003]])
Lengthscale: tensor([[[11.6301,  5.0014, 18.5076,  5.0568, 11.4704, 53.8470]],

        [[18.0217, 31.8479,  8.0625,  1.1761,  1.6006, 17.0879]],

        [[20.6047, 29.5371,  8.8643,  0.8910,  0.9095, 21.3444]],

        [[15.3282, 29.0223, 13.4232,  1.0894,  3.1452, 51.6184]]])
Signal Variance: tensor([ 0.0809,  1.4333, 12.6746,  0.4181])
Estimated target variance: tensor([0.0204, 0.9974, 8.4754, 0.1696])
N: 290
Signal to noise ratio: tensor([16.1482, 63.8357, 67.4496, 35.6942])
Bound on condition number: tensor([  75622.8700, 1181748.8917, 1319342.7941,  369483.6547])
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.0020253651092177406, policy loss: 40.85392050017426
Experience 29, Iter 1, disc loss: 0.0019026766791866568, policy loss: 40.59432130839484
Experience 29, Iter 2, disc loss: 0.0017902129792777479, policy loss: 38.997727051644695
Experience 29, Iter 3, disc loss: 0.0016876030225883912, policy loss: 36.80537753134753
Experience 29, Iter 4, disc loss: 0.0017253789484864878, policy loss: 37.89591304958218
Experience 29, Iter 5, disc loss: 0.0025681789577846337, policy loss: 33.47921023526008
Experience 29, Iter 6, disc loss: 0.0018442401314352872, policy loss: 32.01257804396904
Experience 29, Iter 7, disc loss: 0.0015225393916106864, policy loss: 32.13526377492073
Experience 29, Iter 8, disc loss: 0.08183227034460619, policy loss: 36.249944672374916
Experience 29, Iter 9, disc loss: 0.001276220568588134, policy loss: 42.680795900167695
Experience 29, Iter 10, disc loss: 0.0012525470155093097, policy loss: 39.914833804781395
Experience 29, Iter 11, disc loss: 0.0012300566635290646, policy loss: 32.33118555732206
Experience 29, Iter 12, disc loss: 0.0012030715896945048, policy loss: 28.109538040669733
Experience 29, Iter 13, disc loss: 0.0011814327172250062, policy loss: 25.343392290521425
Experience 29, Iter 14, disc loss: 0.0011527736566658742, policy loss: 23.93257778287458
Experience 29, Iter 15, disc loss: 0.02648038614717635, policy loss: 23.13314941448747
Experience 29, Iter 16, disc loss: 0.0011311664552296663, policy loss: 24.664784082580972
Experience 29, Iter 17, disc loss: 0.0011065001551634256, policy loss: 25.401110260038983
Experience 29, Iter 18, disc loss: 0.05094817817157868, policy loss: 24.16364114105039
Experience 29, Iter 19, disc loss: 0.004780408906973331, policy loss: 26.869170263178134
Experience 29, Iter 20, disc loss: 0.0011207542683153197, policy loss: 28.580075223316065
Experience 29, Iter 21, disc loss: 0.009200023315475958, policy loss: 31.75313083485276
Experience 29, Iter 22, disc loss: 0.0037246669740673656, policy loss: 32.224748285215846
Experience 29, Iter 23, disc loss: 0.004542119287832167, policy loss: 36.189724925491866
Experience 29, Iter 24, disc loss: 0.0018443835704470418, policy loss: 38.41573822624039
Experience 29, Iter 25, disc loss: 0.0011946211580657795, policy loss: 40.56450945033402
Experience 29, Iter 26, disc loss: 0.0012027678039474906, policy loss: 42.679642862936085
Experience 29, Iter 27, disc loss: 0.0012968789843486419, policy loss: 44.20271306402832
Experience 29, Iter 28, disc loss: 0.001206876568314204, policy loss: 42.27223655010306
Experience 29, Iter 29, disc loss: 0.00645448545903354, policy loss: 43.62026456687683
Experience 29, Iter 30, disc loss: 0.001205488549757987, policy loss: 44.70825334176925
Experience 29, Iter 31, disc loss: 0.0012035452869743719, policy loss: 44.97469931599552
Experience 29, Iter 32, disc loss: 0.0011983838910222145, policy loss: 43.845487841721074
Experience 29, Iter 33, disc loss: 0.0011902905950742328, policy loss: 42.077670480264345
Experience 29, Iter 34, disc loss: 0.0011796499528286578, policy loss: 43.11727270648419
Experience 29, Iter 35, disc loss: 0.0011668621750708184, policy loss: 44.141876350673634
Experience 29, Iter 36, disc loss: 0.001152292993091277, policy loss: 43.67480854516783
Experience 29, Iter 37, disc loss: 0.0011361758761965028, policy loss: 42.314925803004
Experience 29, Iter 38, disc loss: 0.00435439616606213, policy loss: 46.93123447268599
Experience 29, Iter 39, disc loss: 0.0011075797082007739, policy loss: 44.90564256377047
Experience 29, Iter 40, disc loss: 0.0010935064377023962, policy loss: 40.77291914141338
Experience 29, Iter 41, disc loss: 0.0010790543564621882, policy loss: 41.63908098315572
Experience 29, Iter 42, disc loss: 0.0010755291468710522, policy loss: 41.79759420961331
Experience 29, Iter 43, disc loss: 0.001170332008506227, policy loss: 38.25206024407642
Experience 29, Iter 44, disc loss: 0.0010298519953210248, policy loss: 38.52743623425267
Experience 29, Iter 45, disc loss: 0.0010125239246112638, policy loss: 41.62178218551416
Experience 29, Iter 46, disc loss: 0.0009957827349226604, policy loss: 37.105698294888754
Experience 29, Iter 47, disc loss: 0.0009939669218939516, policy loss: 38.972662339529734
Experience 29, Iter 48, disc loss: 0.0009593602485642669, policy loss: 37.54624947101159
Experience 29, Iter 49, disc loss: 0.0009415974810442145, policy loss: 41.49738402895023
Experience 29, Iter 50, disc loss: 0.0009240087399451581, policy loss: 38.78015936329295
Experience 29, Iter 51, disc loss: 0.0009216644929766757, policy loss: 38.74627960484522
Experience 29, Iter 52, disc loss: 0.0008895128370432982, policy loss: 36.35206684276409
Experience 29, Iter 53, disc loss: 0.0008727232913713647, policy loss: 38.09152197639174
Experience 29, Iter 54, disc loss: 0.0008580788040181009, policy loss: 40.11112432899961
Experience 29, Iter 55, disc loss: 0.0008652825090587662, policy loss: 38.67233102056211
Experience 29, Iter 56, disc loss: 0.0008246040990597462, policy loss: 37.855806192499784
Experience 29, Iter 57, disc loss: 0.0008095272905670328, policy loss: 36.89220972647364
Experience 29, Iter 58, disc loss: 0.0008048449830421079, policy loss: 37.14665990616318
Experience 29, Iter 59, disc loss: 0.000780332860705342, policy loss: 39.520603551055046
Experience 29, Iter 60, disc loss: 0.00079182148354611, policy loss: 39.477720518797135
Experience 29, Iter 61, disc loss: 0.0007528208451682301, policy loss: 45.47925272157631
Experience 29, Iter 62, disc loss: 0.0007397641759011468, policy loss: 46.74296858144017
Experience 29, Iter 63, disc loss: 0.00072723717185699, policy loss: 46.739685095629916
Experience 29, Iter 64, disc loss: 0.0007153472414309188, policy loss: 45.63292418354196
Experience 29, Iter 65, disc loss: 0.06972075086376497, policy loss: 43.50345671367759
Experience 29, Iter 66, disc loss: 0.0007113890874254615, policy loss: 45.31832543430468
Experience 29, Iter 67, disc loss: 0.0021725019526708878, policy loss: 40.56163049493392
Experience 29, Iter 68, disc loss: 0.0007266875306119759, policy loss: 41.106747651076816
Experience 29, Iter 69, disc loss: 0.0007654426792687366, policy loss: 41.164094495371906
Experience 29, Iter 70, disc loss: 0.005175920466246341, policy loss: 38.288335786966634
Experience 29, Iter 71, disc loss: 0.0008472128891071038, policy loss: 38.89193108756008
Experience 29, Iter 72, disc loss: 0.07019747407960096, policy loss: 30.516557896504825
Experience 29, Iter 73, disc loss: 0.011154329420298993, policy loss: 29.959099300135
Experience 29, Iter 74, disc loss: 0.06109102580030307, policy loss: 23.831159813933137
Experience 29, Iter 75, disc loss: 0.0008439020060593289, policy loss: 26.036407561868753
Experience 29, Iter 76, disc loss: 0.000982663755404908, policy loss: 23.870489217832933
Experience 29, Iter 77, disc loss: 0.0052227001093289455, policy loss: 24.835390496063514
Experience 29, Iter 78, disc loss: 0.0010295007276559303, policy loss: 27.323049171619665
Experience 29, Iter 79, disc loss: 0.0010099216912021172, policy loss: 25.183013649161175
Experience 29, Iter 80, disc loss: 0.0010493932438615152, policy loss: 28.07872166549806
Experience 29, Iter 81, disc loss: 0.0010731968201876116, policy loss: 26.69221000914976
Experience 29, Iter 82, disc loss: 0.0011040624934069434, policy loss: 27.833951187781963
Experience 29, Iter 83, disc loss: 0.001126039434143492, policy loss: 30.88605975852115
Experience 29, Iter 84, disc loss: 0.0011316560835837458, policy loss: 35.61408876948445
Experience 29, Iter 85, disc loss: 0.0011421190925207248, policy loss: 48.51840978627652
Experience 29, Iter 86, disc loss: 0.03391516386245271, policy loss: 35.368413866866
Experience 29, Iter 87, disc loss: 0.0011792972723801399, policy loss: 31.351746265346637
Experience 29, Iter 88, disc loss: 0.001313758705161183, policy loss: 27.54692922134938
Experience 29, Iter 89, disc loss: 0.001222789649921837, policy loss: 27.2705294196171
Experience 29, Iter 90, disc loss: 0.0012570462728855125, policy loss: 33.17984894796882
Experience 29, Iter 91, disc loss: 0.0014765817362302363, policy loss: 39.77679315404472
Experience 29, Iter 92, disc loss: 0.001260613847793026, policy loss: 48.16260435611795
Experience 29, Iter 93, disc loss: 0.0012506992510694427, policy loss: 48.67435270902858
Experience 29, Iter 94, disc loss: 0.0012475510530820585, policy loss: 48.35832886114402
Experience 29, Iter 95, disc loss: 0.0012761271061189898, policy loss: 43.52070793170875
Experience 29, Iter 96, disc loss: 0.001230801727403938, policy loss: 39.853412836742464
Experience 29, Iter 97, disc loss: 0.0012388506933736628, policy loss: 34.37057985896982
Experience 29, Iter 98, disc loss: 0.0012453806740831774, policy loss: 19.222108350951935
Experience 29, Iter 99, disc loss: 0.0020460102791972413, policy loss: 27.4650038667637
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.2474],
        [2.1288],
        [0.0442]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0221, 0.2160, 2.0599, 0.0377, 0.0272, 6.1653]],

        [[0.0221, 0.2160, 2.0599, 0.0377, 0.0272, 6.1653]],

        [[0.0221, 0.2160, 2.0599, 0.0377, 0.0272, 6.1653]],

        [[0.0221, 0.2160, 2.0599, 0.0377, 0.0272, 6.1653]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.9894, 8.5153, 0.1767], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.9894, 8.5153, 0.1767])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.838
Iter 2/2000 - Loss: 3.800
Iter 3/2000 - Loss: 3.560
Iter 4/2000 - Loss: 3.484
Iter 5/2000 - Loss: 3.427
Iter 6/2000 - Loss: 3.274
Iter 7/2000 - Loss: 3.091
Iter 8/2000 - Loss: 2.928
Iter 9/2000 - Loss: 2.773
Iter 10/2000 - Loss: 2.594
Iter 11/2000 - Loss: 2.386
Iter 12/2000 - Loss: 2.164
Iter 13/2000 - Loss: 1.940
Iter 14/2000 - Loss: 1.711
Iter 15/2000 - Loss: 1.470
Iter 16/2000 - Loss: 1.212
Iter 17/2000 - Loss: 0.940
Iter 18/2000 - Loss: 0.659
Iter 19/2000 - Loss: 0.375
Iter 20/2000 - Loss: 0.088
Iter 1981/2000 - Loss: -7.569
Iter 1982/2000 - Loss: -7.569
Iter 1983/2000 - Loss: -7.569
Iter 1984/2000 - Loss: -7.569
Iter 1985/2000 - Loss: -7.569
Iter 1986/2000 - Loss: -7.569
Iter 1987/2000 - Loss: -7.569
Iter 1988/2000 - Loss: -7.569
Iter 1989/2000 - Loss: -7.569
Iter 1990/2000 - Loss: -7.569
Iter 1991/2000 - Loss: -7.569
Iter 1992/2000 - Loss: -7.569
Iter 1993/2000 - Loss: -7.569
Iter 1994/2000 - Loss: -7.569
Iter 1995/2000 - Loss: -7.569
Iter 1996/2000 - Loss: -7.569
Iter 1997/2000 - Loss: -7.569
Iter 1998/2000 - Loss: -7.570
Iter 1999/2000 - Loss: -7.570
Iter 2000/2000 - Loss: -7.570
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[11.8113,  5.0501, 17.0112,  6.1731, 12.3041, 51.4043]],

        [[19.0314, 32.9524,  7.3002,  1.2173,  1.6851, 16.6768]],

        [[21.2300, 30.6777,  8.6285,  0.8596,  0.8973, 21.7226]],

        [[15.4603, 28.2267, 13.4023,  1.0778,  3.6551, 50.6752]]])
Signal Variance: tensor([ 0.0816,  1.3465, 12.1314,  0.4186])
Estimated target variance: tensor([0.0205, 0.9894, 8.5153, 0.1767])
N: 300
Signal to noise ratio: tensor([16.2184, 62.0950, 66.9212, 35.9432])
Bound on condition number: tensor([  78911.9584, 1156736.2674, 1343536.3560,  387574.0831])
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.013103602740952574, policy loss: 38.16044135053784
Experience 30, Iter 1, disc loss: 0.0011679389395888211, policy loss: 42.321143789755155
Experience 30, Iter 2, disc loss: 0.0011666880958412266, policy loss: 37.59746235520549
Experience 30, Iter 3, disc loss: 0.0011550407605755472, policy loss: 29.74366632137861
Experience 30, Iter 4, disc loss: 0.0011443924661966765, policy loss: 26.604758644217167
Experience 30, Iter 5, disc loss: 0.0011380392413754352, policy loss: 23.75169440847717
Experience 30, Iter 6, disc loss: 0.0037234850795513034, policy loss: 22.6773047202156
Experience 30, Iter 7, disc loss: 0.0335470293966243, policy loss: 20.590290904003187
Experience 30, Iter 8, disc loss: 0.006130966184966474, policy loss: 25.876674896960612
Experience 30, Iter 9, disc loss: 0.005533162307346585, policy loss: 27.181851030913954
Experience 30, Iter 10, disc loss: 0.0013936174131552583, policy loss: 31.453188280338757
Experience 30, Iter 11, disc loss: 0.0011338437314039977, policy loss: 40.13552038803799
Experience 30, Iter 12, disc loss: 0.0011638079576135282, policy loss: 41.99446716679872
Experience 30, Iter 13, disc loss: 0.0011361689366733364, policy loss: 43.25099721973625
Experience 30, Iter 14, disc loss: 0.0011325149623605812, policy loss: 41.7487972863471
Experience 30, Iter 15, disc loss: 0.0011260112192214289, policy loss: 42.44822109034318
Experience 30, Iter 16, disc loss: 0.0011171043988040862, policy loss: 41.035417169364045
Experience 30, Iter 17, disc loss: 0.0011061033751760656, policy loss: 37.33567429626791
Experience 30, Iter 18, disc loss: 0.0010945691803616367, policy loss: 32.74390750359255
Experience 30, Iter 19, disc loss: 0.0010792902880716714, policy loss: 28.5612598276376
Experience 30, Iter 20, disc loss: 0.0012794930487105298, policy loss: 28.128098525159167
Experience 30, Iter 21, disc loss: 0.001050219823616504, policy loss: 26.73851213319084
Experience 30, Iter 22, disc loss: 0.019780519748961577, policy loss: 23.51970333603525
Experience 30, Iter 23, disc loss: 0.004353543451129131, policy loss: 23.928049000378998
Experience 30, Iter 24, disc loss: 0.026970799754610707, policy loss: 22.806376237905713
Experience 30, Iter 25, disc loss: 0.0013186464137827781, policy loss: 24.76102626539071
Experience 30, Iter 26, disc loss: 0.013246464015705483, policy loss: 22.426151049114516
Experience 30, Iter 27, disc loss: 0.003033992884276944, policy loss: 20.980581052531388
Experience 30, Iter 28, disc loss: 0.008793868398008315, policy loss: 21.900018881866885
Experience 30, Iter 29, disc loss: 0.015121432531891112, policy loss: 21.417312759845775
Experience 30, Iter 30, disc loss: 0.0014419481881130542, policy loss: 21.477998702545204
Experience 30, Iter 31, disc loss: 0.0012834612135951633, policy loss: 19.56409802500137
Experience 30, Iter 32, disc loss: 0.001267111906354892, policy loss: 20.497605328654963
Experience 30, Iter 33, disc loss: 0.005484019628929342, policy loss: 19.454348392318376
Experience 30, Iter 34, disc loss: 0.0018905771276396797, policy loss: 19.644159577537618
Experience 30, Iter 35, disc loss: 0.003394375346384657, policy loss: 19.715904598201902
Experience 30, Iter 36, disc loss: 0.0032683006281361334, policy loss: 21.91430002594024
Experience 30, Iter 37, disc loss: 0.00138126538848544, policy loss: 24.775419100453696
Experience 30, Iter 38, disc loss: 0.005343616962808487, policy loss: 25.475108047057965
Experience 30, Iter 39, disc loss: 0.001452107717853987, policy loss: 23.881799792371456
Experience 30, Iter 40, disc loss: 0.00267554117606191, policy loss: 24.333102263255185
Experience 30, Iter 41, disc loss: 0.006986884779822672, policy loss: 22.28189396081237
Experience 30, Iter 42, disc loss: 0.031110616060374276, policy loss: 24.843393739173354
Experience 30, Iter 43, disc loss: 0.0012850921990604969, policy loss: 24.42711204565187
Experience 30, Iter 44, disc loss: 0.0015592140071535926, policy loss: 23.526476292773587
Experience 30, Iter 45, disc loss: 0.0013355863437285167, policy loss: 23.819829904170568
Experience 30, Iter 46, disc loss: 0.008384850178628889, policy loss: 25.066053014426558
Experience 30, Iter 47, disc loss: 0.0013419282785873826, policy loss: 25.572706254919872
Experience 30, Iter 48, disc loss: 0.001370154663895328, policy loss: 29.337286911392745
Experience 30, Iter 49, disc loss: 0.0013481276632359997, policy loss: 27.122207427529435
Experience 30, Iter 50, disc loss: 0.0030917707915024953, policy loss: 28.095603274923718
Experience 30, Iter 51, disc loss: 0.001550553348573157, policy loss: 28.663026702698673
Experience 30, Iter 52, disc loss: 0.004562556336254474, policy loss: 30.890378319906443
Experience 30, Iter 53, disc loss: 0.0034905750451262414, policy loss: 31.784912389485452
Experience 30, Iter 54, disc loss: 0.0013196686397028473, policy loss: 31.95714432004466
Experience 30, Iter 55, disc loss: 0.0013092938778721192, policy loss: 29.908907842265528
Experience 30, Iter 56, disc loss: 0.01570690476386682, policy loss: 32.53368111840976
Experience 30, Iter 57, disc loss: 0.0012922520501813866, policy loss: 35.762032103262776
Experience 30, Iter 58, disc loss: 0.001286046407284238, policy loss: 33.4151903504204
Experience 30, Iter 59, disc loss: 0.0012772812802957871, policy loss: 34.814709700815136
Experience 30, Iter 60, disc loss: 0.0012623655976501608, policy loss: 36.89717587354548
Experience 30, Iter 61, disc loss: 0.001245942363111875, policy loss: 36.20751412020351
Experience 30, Iter 62, disc loss: 0.001227104126996373, policy loss: 35.30856670807061
Experience 30, Iter 63, disc loss: 0.001206313571681763, policy loss: 36.508280155286094
Experience 30, Iter 64, disc loss: 0.0011893540971695882, policy loss: 37.13229277908036
Experience 30, Iter 65, disc loss: 0.0011603466431314667, policy loss: 34.06942168653044
Experience 30, Iter 66, disc loss: 0.0011358972016658325, policy loss: 35.90234100486606
Experience 30, Iter 67, disc loss: 0.0011111710897464737, policy loss: 36.56314858555855
Experience 30, Iter 68, disc loss: 0.0010859985185723965, policy loss: 38.1935845803433
Experience 30, Iter 69, disc loss: 0.0010608241748291923, policy loss: 35.6267819442145
Experience 30, Iter 70, disc loss: 0.001036071616050046, policy loss: 35.49477117514067
Experience 30, Iter 71, disc loss: 0.001011202888464657, policy loss: 37.43743742395945
Experience 30, Iter 72, disc loss: 0.0010357878255705388, policy loss: 34.326214280175336
Experience 30, Iter 73, disc loss: 0.0009632328066842761, policy loss: 35.03434331821883
Experience 30, Iter 74, disc loss: 0.0009576494032935396, policy loss: 34.23995542157357
Experience 30, Iter 75, disc loss: 0.0009179300491115204, policy loss: 35.465043850161614
Experience 30, Iter 76, disc loss: 0.000895592539215843, policy loss: 32.85320178385547
Experience 30, Iter 77, disc loss: 0.0008771054853657619, policy loss: 36.67437788496725
Experience 30, Iter 78, disc loss: 0.0008541338488738722, policy loss: 33.78952667964377
Experience 30, Iter 79, disc loss: 0.0008348830477985379, policy loss: 33.381785355153994
Experience 30, Iter 80, disc loss: 0.0008154844276697698, policy loss: 37.51065458854512
Experience 30, Iter 81, disc loss: 0.000798417219969873, policy loss: 34.97184482410512
Experience 30, Iter 82, disc loss: 0.0007799825838790201, policy loss: 34.49018995865336
Experience 30, Iter 83, disc loss: 0.0007630948409358205, policy loss: 34.98611752132975
Experience 30, Iter 84, disc loss: 0.0007473010484956979, policy loss: 34.908344388495294
Experience 30, Iter 85, disc loss: 0.0007329774388197719, policy loss: 32.53862792234563
Experience 30, Iter 86, disc loss: 0.0007169583804915296, policy loss: 36.62437266028931
Experience 30, Iter 87, disc loss: 0.0007028644273722855, policy loss: 37.05610476448485
Experience 30, Iter 88, disc loss: 0.0006894453678599595, policy loss: 34.69825039464909
Experience 30, Iter 89, disc loss: 0.0006766719522886637, policy loss: 35.45318997409332
Experience 30, Iter 90, disc loss: 0.0006645135616266191, policy loss: 34.15323655602861
Experience 30, Iter 91, disc loss: 0.0006526853767519995, policy loss: 34.0574924792074
Experience 30, Iter 92, disc loss: 0.0006413483916965205, policy loss: 33.20247128235661
Experience 30, Iter 93, disc loss: 0.0006304242641707286, policy loss: 36.039788489081005
Experience 30, Iter 94, disc loss: 0.0006199926771865302, policy loss: 35.96075284912304
Experience 30, Iter 95, disc loss: 0.0006099201575491646, policy loss: 33.851758110377304
Experience 30, Iter 96, disc loss: 0.0006003079321341956, policy loss: 33.35856499622159
Experience 30, Iter 97, disc loss: 0.0005910899951596066, policy loss: 34.61095273156281
Experience 30, Iter 98, disc loss: 0.0005829248313966192, policy loss: 32.38224155105239
Experience 30, Iter 99, disc loss: 0.0005737495141944287, policy loss: 32.896032997478564
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.2475],
        [2.1577],
        [0.0443]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0221, 0.2164, 2.0640, 0.0376, 0.0266, 6.0960]],

        [[0.0221, 0.2164, 2.0640, 0.0376, 0.0266, 6.0960]],

        [[0.0221, 0.2164, 2.0640, 0.0376, 0.0266, 6.0960]],

        [[0.0221, 0.2164, 2.0640, 0.0376, 0.0266, 6.0960]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0204, 0.9901, 8.6309, 0.1772], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0204, 0.9901, 8.6309, 0.1772])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.841
Iter 2/2000 - Loss: 3.819
Iter 3/2000 - Loss: 3.568
Iter 4/2000 - Loss: 3.497
Iter 5/2000 - Loss: 3.444
Iter 6/2000 - Loss: 3.292
Iter 7/2000 - Loss: 3.105
Iter 8/2000 - Loss: 2.938
Iter 9/2000 - Loss: 2.785
Iter 10/2000 - Loss: 2.612
Iter 11/2000 - Loss: 2.407
Iter 12/2000 - Loss: 2.184
Iter 13/2000 - Loss: 1.956
Iter 14/2000 - Loss: 1.724
Iter 15/2000 - Loss: 1.483
Iter 16/2000 - Loss: 1.227
Iter 17/2000 - Loss: 0.956
Iter 18/2000 - Loss: 0.673
Iter 19/2000 - Loss: 0.386
Iter 20/2000 - Loss: 0.096
Iter 1981/2000 - Loss: -7.591
Iter 1982/2000 - Loss: -7.592
Iter 1983/2000 - Loss: -7.592
Iter 1984/2000 - Loss: -7.592
Iter 1985/2000 - Loss: -7.592
Iter 1986/2000 - Loss: -7.592
Iter 1987/2000 - Loss: -7.592
Iter 1988/2000 - Loss: -7.592
Iter 1989/2000 - Loss: -7.592
Iter 1990/2000 - Loss: -7.592
Iter 1991/2000 - Loss: -7.592
Iter 1992/2000 - Loss: -7.592
Iter 1993/2000 - Loss: -7.592
Iter 1994/2000 - Loss: -7.592
Iter 1995/2000 - Loss: -7.592
Iter 1996/2000 - Loss: -7.592
Iter 1997/2000 - Loss: -7.592
Iter 1998/2000 - Loss: -7.592
Iter 1999/2000 - Loss: -7.592
Iter 2000/2000 - Loss: -7.592
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[11.6551,  5.2889, 17.0925,  5.6239, 10.5244, 53.4462]],

        [[18.9729, 32.5358,  7.3532,  1.2033,  1.7241, 16.9988]],

        [[20.8568, 30.2674,  8.6531,  0.8130,  0.9406, 21.5838]],

        [[15.1474, 27.0905, 13.3740,  1.0708,  3.7792, 51.4641]]])
Signal Variance: tensor([ 0.0845,  1.3554, 12.3388,  0.4183])
Estimated target variance: tensor([0.0204, 0.9901, 8.6309, 0.1772])
N: 310
Signal to noise ratio: tensor([16.4085, 61.5226, 68.1452, 35.9129])
Bound on condition number: tensor([  83465.3721, 1173359.7103, 1439567.3543,  399819.0711])
Policy Optimizer learning rate:
0.009688861611972634
Experience 31, Iter 0, disc loss: 0.0005656804506167628, policy loss: 33.64316647858371
Experience 31, Iter 1, disc loss: 0.0005642126542983496, policy loss: 31.445038266086637
Experience 31, Iter 2, disc loss: 0.011293903797623083, policy loss: 32.697006611187525
Experience 31, Iter 3, disc loss: 0.0052680408410442775, policy loss: 28.912937193468817
Experience 31, Iter 4, disc loss: 0.000560989205399629, policy loss: 27.92651369986718
Experience 31, Iter 5, disc loss: 0.0005525617117773296, policy loss: 31.874553749960373
Experience 31, Iter 6, disc loss: 0.0005524488256217332, policy loss: 32.93549912298177
Experience 31, Iter 7, disc loss: 0.0005558054106702074, policy loss: 34.70622176985143
Experience 31, Iter 8, disc loss: 0.0011400498876945925, policy loss: 31.68926176962884
Experience 31, Iter 9, disc loss: 0.0005616428015968863, policy loss: 28.4910266908057
Experience 31, Iter 10, disc loss: 0.03448996886207216, policy loss: 23.184168823974936
Experience 31, Iter 11, disc loss: 0.040431848922252496, policy loss: 21.271535228792963
Experience 31, Iter 12, disc loss: 0.0013331962398436088, policy loss: 20.857001734979885
Experience 31, Iter 13, disc loss: 0.001352301981697454, policy loss: 22.691013144687304
Experience 31, Iter 14, disc loss: 0.0008519638965375717, policy loss: 23.638943449436038
Experience 31, Iter 15, disc loss: 0.0006883479349367628, policy loss: 32.086086018163506
Experience 31, Iter 16, disc loss: 0.0009144289685720538, policy loss: 30.598194332435728
Experience 31, Iter 17, disc loss: 0.007940641740066074, policy loss: 34.42479057492861
Experience 31, Iter 18, disc loss: 0.000677704812304225, policy loss: 36.38536499466757
Experience 31, Iter 19, disc loss: 0.0006949713411615339, policy loss: 39.249205527396114
Experience 31, Iter 20, disc loss: 0.0010872374044492866, policy loss: 40.76669003765336
Experience 31, Iter 21, disc loss: 0.0007196952633490701, policy loss: 40.20047657418238
Experience 31, Iter 22, disc loss: 0.0007292262208178594, policy loss: 41.41898580953697
Experience 31, Iter 23, disc loss: 0.0007370477179534639, policy loss: 40.239459517407376
Experience 31, Iter 24, disc loss: 0.0007653840607431334, policy loss: 40.40895001918743
Experience 31, Iter 25, disc loss: 0.0028980267102027783, policy loss: 40.14914502069015
Experience 31, Iter 26, disc loss: 0.0007505661832259855, policy loss: 38.013524269980145
Experience 31, Iter 27, disc loss: 0.0007664123458540537, policy loss: 37.26624745044191
Experience 31, Iter 28, disc loss: 0.0007541251890055258, policy loss: 38.70423909096893
Experience 31, Iter 29, disc loss: 0.0007546244260911334, policy loss: 34.469681699961015
Experience 31, Iter 30, disc loss: 0.0008613031496620442, policy loss: 37.564726278419364
Experience 31, Iter 31, disc loss: 0.0007547325889790386, policy loss: 32.108782094780715
Experience 31, Iter 32, disc loss: 0.0010163365981453764, policy loss: 33.38297633962307
Experience 31, Iter 33, disc loss: 0.001655177943678309, policy loss: 32.009026723262735
Experience 31, Iter 34, disc loss: 0.004989872188601408, policy loss: 33.748426137114805
Experience 31, Iter 35, disc loss: 0.0127771861599692, policy loss: 31.114061901877882
Experience 31, Iter 36, disc loss: 0.0008704897996490752, policy loss: 34.440627920129366
Experience 31, Iter 37, disc loss: 0.0012106648549724095, policy loss: 30.37060491768011
Experience 31, Iter 38, disc loss: 0.0038959554808796833, policy loss: 31.05546882135694
Experience 31, Iter 39, disc loss: 0.000846885139420365, policy loss: 30.281872919621904
Experience 31, Iter 40, disc loss: 0.0008911369601133968, policy loss: 30.038547214362303
Experience 31, Iter 41, disc loss: 0.00711641709149005, policy loss: 32.19798611356548
Experience 31, Iter 42, disc loss: 0.0008511103455316585, policy loss: 31.911871412965468
Experience 31, Iter 43, disc loss: 0.020633573110570656, policy loss: 29.308404115142785
Experience 31, Iter 44, disc loss: 0.0008375025313024613, policy loss: 32.37054878245362
Experience 31, Iter 45, disc loss: 0.000932849052268224, policy loss: 28.609522665677957
Experience 31, Iter 46, disc loss: 0.0012685492882611924, policy loss: 29.598516657015608
Experience 31, Iter 47, disc loss: 0.0009146797031396875, policy loss: 29.14005797377399
Experience 31, Iter 48, disc loss: 0.0009159187784158842, policy loss: 29.855665819711334
Experience 31, Iter 49, disc loss: 0.0023985880562734, policy loss: 28.0246177600196
Experience 31, Iter 50, disc loss: 0.0012782240167172182, policy loss: 28.092810446944405
Experience 31, Iter 51, disc loss: 0.0009368282945972008, policy loss: 29.695697039465372
Experience 31, Iter 52, disc loss: 0.0009474372987148459, policy loss: 28.929323075676706
Experience 31, Iter 53, disc loss: 0.0016249229785912844, policy loss: 29.184052594838736
Experience 31, Iter 54, disc loss: 0.0009679406808983036, policy loss: 33.25390060493642
Experience 31, Iter 55, disc loss: 0.0023465117185893364, policy loss: 29.846607735833043
Experience 31, Iter 56, disc loss: 0.001074506423450221, policy loss: 33.13985876098246
Experience 31, Iter 57, disc loss: 0.0010198519669782473, policy loss: 33.40152415067716
Experience 31, Iter 58, disc loss: 0.00101180372699631, policy loss: 32.79632093371175
Experience 31, Iter 59, disc loss: 0.0010899496715801003, policy loss: 31.323026790086253
Experience 31, Iter 60, disc loss: 0.001020672608119235, policy loss: 32.56063850783171
Experience 31, Iter 61, disc loss: 0.0046006719982722944, policy loss: 33.44947621992394
Experience 31, Iter 62, disc loss: 0.0009501803729008771, policy loss: 33.40447438377653
Experience 31, Iter 63, disc loss: 0.0008826920933948258, policy loss: 34.52670542997962
Experience 31, Iter 64, disc loss: 0.008339368799923122, policy loss: 35.16108406677901
Experience 31, Iter 65, disc loss: 0.000911644750032776, policy loss: 34.24646867443356
Experience 31, Iter 66, disc loss: 0.0011902708543488833, policy loss: 34.25560189320066
Experience 31, Iter 67, disc loss: 0.0018135827804508266, policy loss: 33.69975277109547
Experience 31, Iter 68, disc loss: 0.001215472470131588, policy loss: 32.84368030408817
Experience 31, Iter 69, disc loss: 0.001278583062712512, policy loss: 33.30129282705093
Experience 31, Iter 70, disc loss: 0.006011323443042221, policy loss: 34.3238066476344
Experience 31, Iter 71, disc loss: 0.01860970177235281, policy loss: 32.794886866612615
Experience 31, Iter 72, disc loss: 0.0011892658436800357, policy loss: 33.54829132296872
Experience 31, Iter 73, disc loss: 0.0010370571462288395, policy loss: 34.0540857256718
Experience 31, Iter 74, disc loss: 0.002450403392794109, policy loss: 35.155379432287084
Experience 31, Iter 75, disc loss: 0.00140034539045668, policy loss: 31.47748112944321
Experience 31, Iter 76, disc loss: 0.0010264426581233573, policy loss: 33.25898719063505
Experience 31, Iter 77, disc loss: 0.0010327262183121569, policy loss: 36.87372991752775
Experience 31, Iter 78, disc loss: 0.0010660048184330446, policy loss: 35.745099948477844
Experience 31, Iter 79, disc loss: 0.0010469388893279091, policy loss: 37.64293946859925
Experience 31, Iter 80, disc loss: 0.001050579315436351, policy loss: 35.16167310179317
Experience 31, Iter 81, disc loss: 0.001166425347650635, policy loss: 35.81930795879199
Experience 31, Iter 82, disc loss: 0.0010962819039254002, policy loss: 36.825201308475314
Experience 31, Iter 83, disc loss: 0.0010335506721144134, policy loss: 33.514821816075795
Experience 31, Iter 84, disc loss: 0.0010236168382102397, policy loss: 37.09654755686104
Experience 31, Iter 85, disc loss: 0.002129514657089169, policy loss: 33.12864714057874
Experience 31, Iter 86, disc loss: 0.0010491059972573304, policy loss: 36.06193534487901
Experience 31, Iter 87, disc loss: 0.0009885122975834913, policy loss: 33.096116933823716
Experience 31, Iter 88, disc loss: 0.000992008180348493, policy loss: 33.830064808643655
Experience 31, Iter 89, disc loss: 0.000979850136009793, policy loss: 37.28374955468853
Experience 31, Iter 90, disc loss: 0.000960250665548765, policy loss: 33.79023441424899
Experience 31, Iter 91, disc loss: 0.0011119146077102403, policy loss: 36.028096745398635
Experience 31, Iter 92, disc loss: 0.0026365339360089795, policy loss: 36.69013473948594
Experience 31, Iter 93, disc loss: 0.002043742386916532, policy loss: 37.97570294719586
Experience 31, Iter 94, disc loss: 0.0008899229020758377, policy loss: 35.74887224948398
Experience 31, Iter 95, disc loss: 0.0008676860628428871, policy loss: 35.72377576363348
Experience 31, Iter 96, disc loss: 0.0009005780706197814, policy loss: 35.368855495557966
Experience 31, Iter 97, disc loss: 0.0008394250151727531, policy loss: 34.40478632527535
Experience 31, Iter 98, disc loss: 0.000824387412646816, policy loss: 35.835967148784206
Experience 31, Iter 99, disc loss: 0.0017766502137018443, policy loss: 32.93405522960736
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.2489],
        [2.2084],
        [0.0447]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0215, 0.2138, 2.0772, 0.0377, 0.0260, 6.0753]],

        [[0.0215, 0.2138, 2.0772, 0.0377, 0.0260, 6.0753]],

        [[0.0215, 0.2138, 2.0772, 0.0377, 0.0260, 6.0753]],

        [[0.0215, 0.2138, 2.0772, 0.0377, 0.0260, 6.0753]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.9954, 8.8336, 0.1788], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.9954, 8.8336, 0.1788])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.856
Iter 2/2000 - Loss: 3.843
Iter 3/2000 - Loss: 3.596
Iter 4/2000 - Loss: 3.532
Iter 5/2000 - Loss: 3.484
Iter 6/2000 - Loss: 3.334
Iter 7/2000 - Loss: 3.148
Iter 8/2000 - Loss: 2.985
Iter 9/2000 - Loss: 2.838
Iter 10/2000 - Loss: 2.670
Iter 11/2000 - Loss: 2.467
Iter 12/2000 - Loss: 2.245
Iter 13/2000 - Loss: 2.017
Iter 14/2000 - Loss: 1.787
Iter 15/2000 - Loss: 1.547
Iter 16/2000 - Loss: 1.293
Iter 17/2000 - Loss: 1.023
Iter 18/2000 - Loss: 0.742
Iter 19/2000 - Loss: 0.456
Iter 20/2000 - Loss: 0.167
Iter 1981/2000 - Loss: -7.605
Iter 1982/2000 - Loss: -7.605
Iter 1983/2000 - Loss: -7.605
Iter 1984/2000 - Loss: -7.605
Iter 1985/2000 - Loss: -7.605
Iter 1986/2000 - Loss: -7.605
Iter 1987/2000 - Loss: -7.605
Iter 1988/2000 - Loss: -7.605
Iter 1989/2000 - Loss: -7.605
Iter 1990/2000 - Loss: -7.605
Iter 1991/2000 - Loss: -7.605
Iter 1992/2000 - Loss: -7.605
Iter 1993/2000 - Loss: -7.606
Iter 1994/2000 - Loss: -7.606
Iter 1995/2000 - Loss: -7.606
Iter 1996/2000 - Loss: -7.606
Iter 1997/2000 - Loss: -7.606
Iter 1998/2000 - Loss: -7.606
Iter 1999/2000 - Loss: -7.606
Iter 2000/2000 - Loss: -7.606
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[11.5631,  5.2894, 17.0668,  5.3802, 11.0209, 52.8114]],

        [[18.6182, 32.3117,  7.4060,  1.2125,  1.6374, 18.1740]],

        [[21.1592, 32.5453,  8.8377,  0.7359,  0.8939, 22.6070]],

        [[14.7932, 27.8046, 11.0105,  1.0439,  3.1191, 51.4399]]])
Signal Variance: tensor([ 0.0829,  1.4502, 11.8548,  0.3255])
Estimated target variance: tensor([0.0200, 0.9954, 8.8336, 0.1788])
N: 320
Signal to noise ratio: tensor([16.4469, 62.9800, 67.4150, 31.8962])
Bound on condition number: tensor([  86561.0605, 1269274.9263, 1454330.8444,  325558.0716])
Policy Optimizer learning rate:
0.009678658753253001
Experience 32, Iter 0, disc loss: 0.0008137983926846646, policy loss: 36.14149195734056
Experience 32, Iter 1, disc loss: 0.03161443940131513, policy loss: 36.1041538389303
Experience 32, Iter 2, disc loss: 0.0015755999793606852, policy loss: 35.0026059063337
Experience 32, Iter 3, disc loss: 0.0008051365134025295, policy loss: 34.65585307903335
Experience 32, Iter 4, disc loss: 0.000990493959667616, policy loss: 35.456085145995495
Experience 32, Iter 5, disc loss: 0.0015501912558333584, policy loss: 32.958300585288896
Experience 32, Iter 6, disc loss: 0.0018147027857516526, policy loss: 33.950060451395245
Experience 32, Iter 7, disc loss: 0.000804742548190296, policy loss: 35.777332120740525
Experience 32, Iter 8, disc loss: 0.0008031445377182392, policy loss: 34.07327174999017
Experience 32, Iter 9, disc loss: 0.0029989953346336615, policy loss: 32.97530244843331
Experience 32, Iter 10, disc loss: 0.0009892263539921246, policy loss: 34.49509360738431
Experience 32, Iter 11, disc loss: 0.00137210481148747, policy loss: 34.80330463083486
Experience 32, Iter 12, disc loss: 0.0007935278546821957, policy loss: 36.68608303808751
Experience 32, Iter 13, disc loss: 0.0007886369496611579, policy loss: 37.45873443270274
Experience 32, Iter 14, disc loss: 0.0014761800836975577, policy loss: 35.88185309401741
Experience 32, Iter 15, disc loss: 0.0007777552436323238, policy loss: 36.988374525375164
Experience 32, Iter 16, disc loss: 0.0007701317905374402, policy loss: 36.84964658642379
Experience 32, Iter 17, disc loss: 0.0008937931054186282, policy loss: 34.06011617648246
Experience 32, Iter 18, disc loss: 0.006267120020009084, policy loss: 35.06327447660543
Experience 32, Iter 19, disc loss: 0.007381036776721336, policy loss: 37.11335783587212
Experience 32, Iter 20, disc loss: 0.0013433490178027798, policy loss: 35.01310850781148
Experience 32, Iter 21, disc loss: 0.0007821368715672508, policy loss: 35.47581035330732
Experience 32, Iter 22, disc loss: 0.000776409444338949, policy loss: 36.71592504724033
Experience 32, Iter 23, disc loss: 0.004346057890917073, policy loss: 37.73861544484542
Experience 32, Iter 24, disc loss: 0.004006944036137154, policy loss: 38.52228271019622
Experience 32, Iter 25, disc loss: 0.0008085310718564645, policy loss: 36.19135272391195
Experience 32, Iter 26, disc loss: 0.0008105299731731133, policy loss: 37.579990855490166
Experience 32, Iter 27, disc loss: 0.001044697227683845, policy loss: 37.41663134632827
Experience 32, Iter 28, disc loss: 0.0009703779429940855, policy loss: 37.18144346332655
Experience 32, Iter 29, disc loss: 0.006601076596261338, policy loss: 37.541087955824956
Experience 32, Iter 30, disc loss: 0.0008341080821095003, policy loss: 37.85672660140088
Experience 32, Iter 31, disc loss: 0.0008517625792593411, policy loss: 38.39795676577052
Experience 32, Iter 32, disc loss: 0.001543489360014322, policy loss: 36.17706755932453
Experience 32, Iter 33, disc loss: 0.0008552693275233537, policy loss: 38.868274052523
Experience 32, Iter 34, disc loss: 0.0013610140499716338, policy loss: 39.85505733276
Experience 32, Iter 35, disc loss: 0.0008585201818750418, policy loss: 41.2511927484682
Experience 32, Iter 36, disc loss: 0.0029429282767166245, policy loss: 38.37600627129359
Experience 32, Iter 37, disc loss: 0.0008576379064506205, policy loss: 36.466436220493655
Experience 32, Iter 38, disc loss: 0.0008558871868218552, policy loss: 40.16876897854691
Experience 32, Iter 39, disc loss: 0.0009007769645780886, policy loss: 38.78793746578869
Experience 32, Iter 40, disc loss: 0.0008914406123136237, policy loss: 38.41569327015143
Experience 32, Iter 41, disc loss: 0.000842247262830923, policy loss: 37.87391442360827
Experience 32, Iter 42, disc loss: 0.0008301545931468228, policy loss: 38.21579728773459
Experience 32, Iter 43, disc loss: 0.0032031028116700335, policy loss: 38.11943717675581
Experience 32, Iter 44, disc loss: 0.002033289829632531, policy loss: 39.865686677164156
Experience 32, Iter 45, disc loss: 0.0031333379787900554, policy loss: 37.876741076330845
Experience 32, Iter 46, disc loss: 0.002895639045432779, policy loss: 39.41428243509496
Experience 32, Iter 47, disc loss: 0.0008067362130839196, policy loss: 38.91707183887833
Experience 32, Iter 48, disc loss: 0.0008065374634524862, policy loss: 38.955865613724995
Experience 32, Iter 49, disc loss: 0.0008030484639653491, policy loss: 40.13296555346304
Experience 32, Iter 50, disc loss: 0.0007984647096324065, policy loss: 40.202798134597394
Experience 32, Iter 51, disc loss: 0.0007943760361539293, policy loss: 39.0074236785255
Experience 32, Iter 52, disc loss: 0.0008581921661174867, policy loss: 38.55264699349496
Experience 32, Iter 53, disc loss: 0.0007777672922825777, policy loss: 37.628799775936535
Experience 32, Iter 54, disc loss: 0.0007674201028831994, policy loss: 37.56110700260251
Experience 32, Iter 55, disc loss: 0.0007568198278287929, policy loss: 39.702248256759006
Experience 32, Iter 56, disc loss: 0.0007458962291391082, policy loss: 39.04335271393565
Experience 32, Iter 57, disc loss: 0.0007358152086399355, policy loss: 37.186428916602395
Experience 32, Iter 58, disc loss: 0.0007229243133927507, policy loss: 38.29411565231155
Experience 32, Iter 59, disc loss: 0.003070479527366974, policy loss: 37.92388413607502
Experience 32, Iter 60, disc loss: 0.000719450651363875, policy loss: 37.78124267335012
Experience 32, Iter 61, disc loss: 0.000694676520953907, policy loss: 40.002440015782426
Experience 32, Iter 62, disc loss: 0.0006857592722467564, policy loss: 37.58136386784693
Experience 32, Iter 63, disc loss: 0.0006769797429710924, policy loss: 37.67304568253827
Experience 32, Iter 64, disc loss: 0.000667162736461315, policy loss: 37.54680544729857
Experience 32, Iter 65, disc loss: 0.0008854896221432034, policy loss: 35.87303167778902
Experience 32, Iter 66, disc loss: 0.000648105786684372, policy loss: 36.35924884904503
Experience 32, Iter 67, disc loss: 0.0006390299164021072, policy loss: 36.243843619614864
Experience 32, Iter 68, disc loss: 0.0006293848221334983, policy loss: 37.509046679318786
Experience 32, Iter 69, disc loss: 0.0006200631736776275, policy loss: 35.79213495186199
Experience 32, Iter 70, disc loss: 0.0006573341489948795, policy loss: 35.62216266399277
Experience 32, Iter 71, disc loss: 0.0073122600494159055, policy loss: 37.713218675456055
Experience 32, Iter 72, disc loss: 0.003662904563256491, policy loss: 36.90523450394057
Experience 32, Iter 73, disc loss: 0.0005974692058555136, policy loss: 37.857904878886245
Experience 32, Iter 74, disc loss: 0.0006684435432465045, policy loss: 37.694286153113595
Experience 32, Iter 75, disc loss: 0.0005980885571830578, policy loss: 36.4322183332492
Experience 32, Iter 76, disc loss: 0.0005995844283929379, policy loss: 37.17611565995871
Experience 32, Iter 77, disc loss: 0.0010451565813171823, policy loss: 34.66372463186991
Experience 32, Iter 78, disc loss: 0.000585680151407017, policy loss: 37.0971883720885
Experience 32, Iter 79, disc loss: 0.0029276351854158034, policy loss: 36.823895329689876
Experience 32, Iter 80, disc loss: 0.00058047537748924, policy loss: 36.816150490006514
Experience 32, Iter 81, disc loss: 0.0006473156637905774, policy loss: 35.58254071652382
Experience 32, Iter 82, disc loss: 0.0011368446780849473, policy loss: 34.843468167113926
Experience 32, Iter 83, disc loss: 0.0005736516731828038, policy loss: 36.690794684632976
Experience 32, Iter 84, disc loss: 0.0005703755860362751, policy loss: 34.969418535418896
Experience 32, Iter 85, disc loss: 0.0006978226486634709, policy loss: 37.15056932431657
Experience 32, Iter 86, disc loss: 0.0005737741332994154, policy loss: 37.76569538070057
Experience 32, Iter 87, disc loss: 0.0005583111261502337, policy loss: 36.907163149883566
Experience 32, Iter 88, disc loss: 0.0006106437978645188, policy loss: 37.616219727199066
Experience 32, Iter 89, disc loss: 0.0006528774280719042, policy loss: 36.745199384858324
Experience 32, Iter 90, disc loss: 0.007596276403243551, policy loss: 34.403897935272774
Experience 32, Iter 91, disc loss: 0.01662045104633673, policy loss: 34.872647215287685
Experience 32, Iter 92, disc loss: 0.014754197176210532, policy loss: 35.96471986583016
Experience 32, Iter 93, disc loss: 0.001138480376700193, policy loss: 33.49940321240972
Experience 32, Iter 94, disc loss: 0.004427843386475887, policy loss: 37.291593793537544
Experience 32, Iter 95, disc loss: 0.0006700157888003172, policy loss: 37.3293052246503
Experience 32, Iter 96, disc loss: 0.0009180998196417985, policy loss: 37.46993712678599
Experience 32, Iter 97, disc loss: 0.0007353302081585381, policy loss: 36.248958481385266
Experience 32, Iter 98, disc loss: 0.0008501884719618083, policy loss: 34.450780169817364
Experience 32, Iter 99, disc loss: 0.0007883537745083019, policy loss: 36.58887583570451
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.2515],
        [2.2650],
        [0.0457]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0210, 0.2111, 2.1143, 0.0375, 0.0254, 6.0460]],

        [[0.0210, 0.2111, 2.1143, 0.0375, 0.0254, 6.0460]],

        [[0.0210, 0.2111, 2.1143, 0.0375, 0.0254, 6.0460]],

        [[0.0210, 0.2111, 2.1143, 0.0375, 0.0254, 6.0460]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0197, 1.0061, 9.0602, 0.1827], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0197, 1.0061, 9.0602, 0.1827])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.880
Iter 2/2000 - Loss: 3.876
Iter 3/2000 - Loss: 3.634
Iter 4/2000 - Loss: 3.577
Iter 5/2000 - Loss: 3.532
Iter 6/2000 - Loss: 3.382
Iter 7/2000 - Loss: 3.200
Iter 8/2000 - Loss: 3.044
Iter 9/2000 - Loss: 2.904
Iter 10/2000 - Loss: 2.742
Iter 11/2000 - Loss: 2.544
Iter 12/2000 - Loss: 2.326
Iter 13/2000 - Loss: 2.103
Iter 14/2000 - Loss: 1.879
Iter 15/2000 - Loss: 1.645
Iter 16/2000 - Loss: 1.395
Iter 17/2000 - Loss: 1.130
Iter 18/2000 - Loss: 0.853
Iter 19/2000 - Loss: 0.570
Iter 20/2000 - Loss: 0.284
Iter 1981/2000 - Loss: -7.641
Iter 1982/2000 - Loss: -7.641
Iter 1983/2000 - Loss: -7.641
Iter 1984/2000 - Loss: -7.641
Iter 1985/2000 - Loss: -7.641
Iter 1986/2000 - Loss: -7.641
Iter 1987/2000 - Loss: -7.641
Iter 1988/2000 - Loss: -7.641
Iter 1989/2000 - Loss: -7.641
Iter 1990/2000 - Loss: -7.641
Iter 1991/2000 - Loss: -7.641
Iter 1992/2000 - Loss: -7.641
Iter 1993/2000 - Loss: -7.641
Iter 1994/2000 - Loss: -7.642
Iter 1995/2000 - Loss: -7.642
Iter 1996/2000 - Loss: -7.642
Iter 1997/2000 - Loss: -7.642
Iter 1998/2000 - Loss: -7.642
Iter 1999/2000 - Loss: -7.642
Iter 2000/2000 - Loss: -7.642
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[11.5908,  7.0100, 18.5564,  3.9365,  9.3098, 54.9090]],

        [[19.1387, 32.6426,  7.6401,  1.2474,  1.4056, 18.3136]],

        [[21.6311, 34.3376,  8.8940,  0.7238,  0.9076, 22.5324]],

        [[14.9281, 27.8125, 11.2014,  1.0827,  2.7149, 50.9536]]])
Signal Variance: tensor([ 0.0999,  1.4544, 11.6036,  0.3261])
Estimated target variance: tensor([0.0197, 1.0061, 9.0602, 0.1827])
N: 330
Signal to noise ratio: tensor([17.7524, 63.7757, 67.6326, 32.0760])
Bound on condition number: tensor([ 103999.2300, 1342222.8026, 1509477.2507,  339528.7169])
Policy Optimizer learning rate:
0.009668466638656902
Experience 33, Iter 0, disc loss: 0.0008098012507185815, policy loss: 38.26475440869429
Experience 33, Iter 1, disc loss: 0.0008273772295869826, policy loss: 36.41825794987648
Experience 33, Iter 2, disc loss: 0.0008395572991333244, policy loss: 36.65397748762309
Experience 33, Iter 3, disc loss: 0.000849483982972017, policy loss: 35.657197652298514
Experience 33, Iter 4, disc loss: 0.0008595083992102187, policy loss: 36.93272626058315
Experience 33, Iter 5, disc loss: 0.0008677693674729712, policy loss: 35.98615555882296
Experience 33, Iter 6, disc loss: 0.0008595443741918398, policy loss: 36.64746175279402
Experience 33, Iter 7, disc loss: 0.000879541552468256, policy loss: 38.09728219903816
Experience 33, Iter 8, disc loss: 0.0008542544140058606, policy loss: 35.93685010137409
Experience 33, Iter 9, disc loss: 0.008262362205013793, policy loss: 36.61468767964097
Experience 33, Iter 10, disc loss: 0.0016475900730869323, policy loss: 35.08002991374207
Experience 33, Iter 11, disc loss: 0.0010139676534564246, policy loss: 36.820807644564916
Experience 33, Iter 12, disc loss: 0.0008632200128520414, policy loss: 36.57155284295319
Experience 33, Iter 13, disc loss: 0.0008588323044946135, policy loss: 37.61739935033272
Experience 33, Iter 14, disc loss: 0.0008618640610021219, policy loss: 35.61310680947523
Experience 33, Iter 15, disc loss: 0.0008501501520425206, policy loss: 37.765888676773415
Experience 33, Iter 16, disc loss: 0.0008452491635888695, policy loss: 37.145460534428366
Experience 33, Iter 17, disc loss: 0.0008335303850143222, policy loss: 36.49267291824553
Experience 33, Iter 18, disc loss: 0.002291097607777487, policy loss: 35.88764773582572
Experience 33, Iter 19, disc loss: 0.0008172222216514867, policy loss: 35.063766151864066
Experience 33, Iter 20, disc loss: 0.00080450948203495, policy loss: 37.94940064736126
Experience 33, Iter 21, disc loss: 0.0007927429290314589, policy loss: 34.83491345368943
Experience 33, Iter 22, disc loss: 0.000784691934705352, policy loss: 38.28047899716948
Experience 33, Iter 23, disc loss: 0.0007755639282806317, policy loss: 34.550285606494754
Experience 33, Iter 24, disc loss: 0.001109308299936527, policy loss: 35.373489732229146
Experience 33, Iter 25, disc loss: 0.0007424852514784393, policy loss: 35.1952891530924
Experience 33, Iter 26, disc loss: 0.0007289771385732889, policy loss: 36.40783873751691
Experience 33, Iter 27, disc loss: 0.010152927377467019, policy loss: 36.3466541084468
Experience 33, Iter 28, disc loss: 0.0007186119738723682, policy loss: 37.716160688538416
Experience 33, Iter 29, disc loss: 0.0014140760551426372, policy loss: 35.072859792984644
Experience 33, Iter 30, disc loss: 0.0007244069794163697, policy loss: 33.90856234100468
Experience 33, Iter 31, disc loss: 0.0007188297167554271, policy loss: 35.645952844778265
Experience 33, Iter 32, disc loss: 0.0007160314763283884, policy loss: 37.38418722449397
Experience 33, Iter 33, disc loss: 0.0007721884434434868, policy loss: 36.13229925949935
Experience 33, Iter 34, disc loss: 0.0007176811495159827, policy loss: 38.594996996825024
Experience 33, Iter 35, disc loss: 0.0007874178515218096, policy loss: 37.38868588656644
Experience 33, Iter 36, disc loss: 0.0007078924897546436, policy loss: 35.3896894446385
Experience 33, Iter 37, disc loss: 0.0006854462056008211, policy loss: 36.278267424693354
Experience 33, Iter 38, disc loss: 0.0009108687971701461, policy loss: 38.022373535167006
Experience 33, Iter 39, disc loss: 0.0006682266580896233, policy loss: 35.06632676070748
Experience 33, Iter 40, disc loss: 0.000663028869911048, policy loss: 35.630931408741425
Experience 33, Iter 41, disc loss: 0.008174303412588594, policy loss: 34.33429348481478
Experience 33, Iter 42, disc loss: 0.0016866790636684231, policy loss: 34.80324802464504
Experience 33, Iter 43, disc loss: 0.0006834158025271202, policy loss: 35.328241641471735
Experience 33, Iter 44, disc loss: 0.0006607118894029558, policy loss: 34.5061385335191
Experience 33, Iter 45, disc loss: 0.0006574109441689397, policy loss: 37.310137973989384
Experience 33, Iter 46, disc loss: 0.000661840278923335, policy loss: 35.541859521563424
Experience 33, Iter 47, disc loss: 0.0006526376578067002, policy loss: 37.29872440285553
Experience 33, Iter 48, disc loss: 0.000649075509129045, policy loss: 35.04272801572607
Experience 33, Iter 49, disc loss: 0.0017331444633024704, policy loss: 35.57404838527221
Experience 33, Iter 50, disc loss: 0.0006582072878305867, policy loss: 36.57701087001535
Experience 33, Iter 51, disc loss: 0.001267752802273688, policy loss: 33.376853038472674
Experience 33, Iter 52, disc loss: 0.0006485367926107227, policy loss: 35.455930868168885
Experience 33, Iter 53, disc loss: 0.0006311999323728139, policy loss: 37.83845372647825
Experience 33, Iter 54, disc loss: 0.0014208415677030915, policy loss: 35.89705139059563
Experience 33, Iter 55, disc loss: 0.0006179619115875652, policy loss: 35.18878939887
Experience 33, Iter 56, disc loss: 0.000991060965419198, policy loss: 33.02792781230211
Experience 33, Iter 57, disc loss: 0.001373514012601074, policy loss: 34.97449197827309
Experience 33, Iter 58, disc loss: 0.0006301633160262479, policy loss: 35.53233024329925
Experience 33, Iter 59, disc loss: 0.002489984558753309, policy loss: 34.44670099415823
Experience 33, Iter 60, disc loss: 0.0005950637829475095, policy loss: 35.75715967684337
Experience 33, Iter 61, disc loss: 0.0007229210157198465, policy loss: 34.8229622944841
Experience 33, Iter 62, disc loss: 0.0005982040446461137, policy loss: 34.03416794450018
Experience 33, Iter 63, disc loss: 0.0005832058482881553, policy loss: 33.88138210068007
Experience 33, Iter 64, disc loss: 0.0005843427786508339, policy loss: 35.03016679776708
Experience 33, Iter 65, disc loss: 0.0005727340485438994, policy loss: 38.73951190810277
Experience 33, Iter 66, disc loss: 0.0005741183465544832, policy loss: 34.488125312190185
Experience 33, Iter 67, disc loss: 0.0005669410897987287, policy loss: 35.191222341465576
Experience 33, Iter 68, disc loss: 0.0005551879769423308, policy loss: 33.592756936197894
Experience 33, Iter 69, disc loss: 0.0005718979872938605, policy loss: 32.8279533754636
Experience 33, Iter 70, disc loss: 0.000594258827692803, policy loss: 36.30000842444697
Experience 33, Iter 71, disc loss: 0.0005358605446409216, policy loss: 33.10938242360227
Experience 33, Iter 72, disc loss: 0.000925447857648361, policy loss: 31.964112581584494
Experience 33, Iter 73, disc loss: 0.0006830788229504926, policy loss: 31.724112065207002
Experience 33, Iter 74, disc loss: 0.000553164692324028, policy loss: 27.657983186445456
Experience 33, Iter 75, disc loss: 0.0008684558468212865, policy loss: 31.44235251722669
Experience 33, Iter 76, disc loss: 0.0033548208037825155, policy loss: 25.995030887085896
Experience 33, Iter 77, disc loss: 0.0005184509803903837, policy loss: 29.134996055950978
Experience 33, Iter 78, disc loss: 0.0005099542800453742, policy loss: 30.264925845225548
Experience 33, Iter 79, disc loss: 0.0006846831179952945, policy loss: 28.954990083622093
Experience 33, Iter 80, disc loss: 0.0005010841570639071, policy loss: 27.5401137517551
Experience 33, Iter 81, disc loss: 0.0007863740021451914, policy loss: 30.166547027836124
Experience 33, Iter 82, disc loss: 0.0005163519157794238, policy loss: 29.539895732026554
Experience 33, Iter 83, disc loss: 0.0005025818936334341, policy loss: 32.0229511027821
Experience 33, Iter 84, disc loss: 0.0006825857181090617, policy loss: 31.38599131618257
Experience 33, Iter 85, disc loss: 0.0007008203758519633, policy loss: 26.578691622698912
Experience 33, Iter 86, disc loss: 0.0005362148213210715, policy loss: 31.972123207783852
Experience 33, Iter 87, disc loss: 0.0005430348297845071, policy loss: 30.87972795470926
Experience 33, Iter 88, disc loss: 0.000467775009164538, policy loss: 28.749668292982136
Experience 33, Iter 89, disc loss: 0.0006122619780980814, policy loss: 29.300434389238806
Experience 33, Iter 90, disc loss: 0.00045938463361842137, policy loss: 28.78168488373509
Experience 33, Iter 91, disc loss: 0.00046220364326278473, policy loss: 29.160900023154266
Experience 33, Iter 92, disc loss: 0.00045820324536301134, policy loss: 29.8999337004739
Experience 33, Iter 93, disc loss: 0.0004673826302462097, policy loss: 30.30473653838467
Experience 33, Iter 94, disc loss: 0.001053111725503849, policy loss: 26.970676368614093
Experience 33, Iter 95, disc loss: 0.0004377995943404642, policy loss: 27.64437272891987
Experience 33, Iter 96, disc loss: 0.004412451356913466, policy loss: 28.364875802597886
Experience 33, Iter 97, disc loss: 0.0007821224862410592, policy loss: 30.917164509323108
Experience 33, Iter 98, disc loss: 0.0035545522316334562, policy loss: 29.35635236864757
Experience 33, Iter 99, disc loss: 0.0004733189874649445, policy loss: 29.605037600272997
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.2515],
        [2.2878],
        [0.0463]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0211, 0.2084, 2.1439, 0.0379, 0.0252, 6.0189]],

        [[0.0211, 0.2084, 2.1439, 0.0379, 0.0252, 6.0189]],

        [[0.0211, 0.2084, 2.1439, 0.0379, 0.0252, 6.0189]],

        [[0.0211, 0.2084, 2.1439, 0.0379, 0.0252, 6.0189]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0194, 1.0059, 9.1513, 0.1852], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0194, 1.0059, 9.1513, 0.1852])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.878
Iter 2/2000 - Loss: 3.876
Iter 3/2000 - Loss: 3.644
Iter 4/2000 - Loss: 3.590
Iter 5/2000 - Loss: 3.543
Iter 6/2000 - Loss: 3.393
Iter 7/2000 - Loss: 3.215
Iter 8/2000 - Loss: 3.066
Iter 9/2000 - Loss: 2.933
Iter 10/2000 - Loss: 2.774
Iter 11/2000 - Loss: 2.578
Iter 12/2000 - Loss: 2.364
Iter 13/2000 - Loss: 2.146
Iter 14/2000 - Loss: 1.927
Iter 15/2000 - Loss: 1.697
Iter 16/2000 - Loss: 1.451
Iter 17/2000 - Loss: 1.188
Iter 18/2000 - Loss: 0.914
Iter 19/2000 - Loss: 0.632
Iter 20/2000 - Loss: 0.346
Iter 1981/2000 - Loss: -7.633
Iter 1982/2000 - Loss: -7.633
Iter 1983/2000 - Loss: -7.633
Iter 1984/2000 - Loss: -7.633
Iter 1985/2000 - Loss: -7.633
Iter 1986/2000 - Loss: -7.633
Iter 1987/2000 - Loss: -7.633
Iter 1988/2000 - Loss: -7.633
Iter 1989/2000 - Loss: -7.633
Iter 1990/2000 - Loss: -7.633
Iter 1991/2000 - Loss: -7.633
Iter 1992/2000 - Loss: -7.633
Iter 1993/2000 - Loss: -7.633
Iter 1994/2000 - Loss: -7.633
Iter 1995/2000 - Loss: -7.633
Iter 1996/2000 - Loss: -7.633
Iter 1997/2000 - Loss: -7.633
Iter 1998/2000 - Loss: -7.633
Iter 1999/2000 - Loss: -7.634
Iter 2000/2000 - Loss: -7.634
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[12.7295, 11.7843, 19.8217,  3.9678,  6.7343, 60.8099]],

        [[19.0571, 33.3550,  7.6661,  1.2045,  1.4084, 18.2788]],

        [[21.8574, 34.7222,  8.7792,  0.7157,  0.9185, 22.6278]],

        [[14.7099, 27.7102, 11.3731,  1.1903,  2.2419, 49.8168]]])
Signal Variance: tensor([ 0.1565,  1.3654, 11.3807,  0.3301])
Estimated target variance: tensor([0.0194, 1.0059, 9.1513, 0.1852])
N: 340
Signal to noise ratio: tensor([22.2978, 60.8756, 67.5335, 31.9777])
Bound on condition number: tensor([ 169045.8407, 1259987.5106, 1550663.6657,  347676.5049])
Policy Optimizer learning rate:
0.009658285256870233
Experience 34, Iter 0, disc loss: 0.016730395249091087, policy loss: 25.254236830894442
Experience 34, Iter 1, disc loss: 0.0006462604404779985, policy loss: 24.69912815387675
Experience 34, Iter 2, disc loss: 0.0005081018916250991, policy loss: 24.89018159442786
Experience 34, Iter 3, disc loss: 0.0005399605201846314, policy loss: 25.3978870611257
Experience 34, Iter 4, disc loss: 0.001012883727199796, policy loss: 23.650995141330817
Experience 34, Iter 5, disc loss: 0.0005068129868155294, policy loss: 23.1512091480448
Experience 34, Iter 6, disc loss: 0.0021877529280491803, policy loss: 23.02623992492108
Experience 34, Iter 7, disc loss: 0.053113901412508706, policy loss: 21.581921933675495
Experience 34, Iter 8, disc loss: 0.026520616354541653, policy loss: 21.20831260795584
Experience 34, Iter 9, disc loss: 0.0006386367895943424, policy loss: 20.320947592010327
Experience 34, Iter 10, disc loss: 0.0059176518440312706, policy loss: 20.670184467751696
Experience 34, Iter 11, disc loss: 0.002230309125881161, policy loss: 20.68382788075334
Experience 34, Iter 12, disc loss: 0.0008000228489036558, policy loss: 21.00346478252297
Experience 34, Iter 13, disc loss: 0.001101988295609059, policy loss: 20.792514700204464
Experience 34, Iter 14, disc loss: 0.0016910011698365141, policy loss: 20.603228936427506
Experience 34, Iter 15, disc loss: 0.0008953568527143566, policy loss: 20.77291680993958
Experience 34, Iter 16, disc loss: 0.0009136541167052539, policy loss: 20.851266425695805
Experience 34, Iter 17, disc loss: 0.0009705806800275595, policy loss: 21.09577690150194
Experience 34, Iter 18, disc loss: 0.0012928421303357051, policy loss: 19.124352171198105
Experience 34, Iter 19, disc loss: 0.0016155135738624318, policy loss: 21.025114472338206
Experience 34, Iter 20, disc loss: 0.0010788553015672296, policy loss: 20.127095827688926
Experience 34, Iter 21, disc loss: 0.016900868845998066, policy loss: 18.4256899871626
Experience 34, Iter 22, disc loss: 0.0010962571913419388, policy loss: 20.913481588522306
Experience 34, Iter 23, disc loss: 0.001695562605577362, policy loss: 18.837512909854333
Experience 34, Iter 24, disc loss: 0.0015135397437364146, policy loss: 21.44357066400569
Experience 34, Iter 25, disc loss: 0.0013583890323472126, policy loss: 22.442758869622438
Experience 34, Iter 26, disc loss: 0.001084925257190088, policy loss: 19.905197635779885
Experience 34, Iter 27, disc loss: 0.0025482026225991947, policy loss: 19.24937318998287
Experience 34, Iter 28, disc loss: 0.0011182191639560403, policy loss: 20.432396372332327
Experience 34, Iter 29, disc loss: 0.001067294889493709, policy loss: 21.168534365674557
Experience 34, Iter 30, disc loss: 0.0015124857233119243, policy loss: 19.426489969085782
Experience 34, Iter 31, disc loss: 0.0013770288076121247, policy loss: 20.22433014584759
Experience 34, Iter 32, disc loss: 0.009805119474749734, policy loss: 20.41869972609834
Experience 34, Iter 33, disc loss: 0.0010814880472507313, policy loss: 19.866672955900693
Experience 34, Iter 34, disc loss: 0.0012287299600212127, policy loss: 20.387151757275678
Experience 34, Iter 35, disc loss: 0.003636205350768819, policy loss: 20.729342804075277
Experience 34, Iter 36, disc loss: 0.0009800269197256376, policy loss: 21.59224851471661
Experience 34, Iter 37, disc loss: 0.0016276067117735366, policy loss: 21.050686089484785
Experience 34, Iter 38, disc loss: 0.0011250586478331645, policy loss: 22.28328819588768
Experience 34, Iter 39, disc loss: 0.0010326127093609665, policy loss: 21.922442520342095
Experience 34, Iter 40, disc loss: 0.001014978978051305, policy loss: 19.775940124982537
Experience 34, Iter 41, disc loss: 0.004064249655703268, policy loss: 20.71219317137625
Experience 34, Iter 42, disc loss: 0.0008867009077557946, policy loss: 21.118539609657965
Experience 34, Iter 43, disc loss: 0.0009218652477556235, policy loss: 20.212906130173252
Experience 34, Iter 44, disc loss: 0.0012545423771071046, policy loss: 19.556159005383982
Experience 34, Iter 45, disc loss: 0.0008452501198366078, policy loss: 20.744182428327335
Experience 34, Iter 46, disc loss: 0.0008765296224940852, policy loss: 21.095244623932345
Experience 34, Iter 47, disc loss: 0.0009806274977362587, policy loss: 20.370688990963608
Experience 34, Iter 48, disc loss: 0.0010613469646052602, policy loss: 21.157826197735346
Experience 34, Iter 49, disc loss: 0.0008327701453467601, policy loss: 20.27322544921125
Experience 34, Iter 50, disc loss: 0.000874390214014252, policy loss: 20.957743800910794
Experience 34, Iter 51, disc loss: 0.0008068218008084604, policy loss: 21.251374809922332
Experience 34, Iter 52, disc loss: 0.0007870109732824458, policy loss: 21.50151226991963
Experience 34, Iter 53, disc loss: 0.0008551848766422299, policy loss: 21.40289241299815
Experience 34, Iter 54, disc loss: 0.0008694119163480742, policy loss: 19.61750314210031
Experience 34, Iter 55, disc loss: 0.0013449865002248878, policy loss: 20.98639902790895
Experience 34, Iter 56, disc loss: 0.0007165069196261336, policy loss: 20.726356558975503
Experience 34, Iter 57, disc loss: 0.0006610048458463881, policy loss: 22.23129412463568
Experience 34, Iter 58, disc loss: 0.0011316426378115499, policy loss: 21.156590529379358
Experience 34, Iter 59, disc loss: 0.0007575918579183977, policy loss: 21.831241517487292
Experience 34, Iter 60, disc loss: 0.0006710583676032985, policy loss: 22.678140498319546
Experience 34, Iter 61, disc loss: 0.0006181343335477283, policy loss: 23.046255184320604
Experience 34, Iter 62, disc loss: 0.0006143087851311909, policy loss: 23.864037581111223
Experience 34, Iter 63, disc loss: 0.0008183555155641803, policy loss: 21.86749659888693
Experience 34, Iter 64, disc loss: 0.0005830737852997941, policy loss: 21.124189928397275
Experience 34, Iter 65, disc loss: 0.0005630480078219553, policy loss: 22.84062234597622
Experience 34, Iter 66, disc loss: 0.0015865067659947608, policy loss: 22.282913593687045
Experience 34, Iter 67, disc loss: 0.0005811104760544424, policy loss: 22.73041130697527
Experience 34, Iter 68, disc loss: 0.0009229138979212643, policy loss: 21.3002361319446
Experience 34, Iter 69, disc loss: 0.0037728408895431696, policy loss: 21.18548768466178
Experience 34, Iter 70, disc loss: 0.000647516454723572, policy loss: 21.994287274633443
Experience 34, Iter 71, disc loss: 0.0006023316096713173, policy loss: 21.379044849985394
Experience 34, Iter 72, disc loss: 0.0028357539142730426, policy loss: 20.073174391837817
Experience 34, Iter 73, disc loss: 0.0018104919220453578, policy loss: 23.055197736993073
Experience 34, Iter 74, disc loss: 0.0009567514412037075, policy loss: 22.399220269783754
Experience 34, Iter 75, disc loss: 0.013018738035252608, policy loss: 22.218043563364148
Experience 34, Iter 76, disc loss: 0.0005069219650947403, policy loss: 22.51187081372068
Experience 34, Iter 77, disc loss: 0.000570352727418703, policy loss: 23.67881956780387
Experience 34, Iter 78, disc loss: 0.002162118419693035, policy loss: 21.170388409693295
Experience 34, Iter 79, disc loss: 0.0005955634001375421, policy loss: 20.20532511557781
Experience 34, Iter 80, disc loss: 0.0016127762559277864, policy loss: 23.43940445529484
Experience 34, Iter 81, disc loss: 0.0035672891666145897, policy loss: 21.69336616879044
Experience 34, Iter 82, disc loss: 0.0012307852495943494, policy loss: 21.443375922890823
Experience 34, Iter 83, disc loss: 0.0015018055290340836, policy loss: 21.857713820091767
Experience 34, Iter 84, disc loss: 0.006661151529661712, policy loss: 22.29014110173831
Experience 34, Iter 85, disc loss: 0.0007159674973578634, policy loss: 22.843644522377787
Experience 34, Iter 86, disc loss: 0.0007917546157183342, policy loss: 21.5452681120463
Experience 34, Iter 87, disc loss: 0.03169149200201416, policy loss: 22.59954775350454
Experience 34, Iter 88, disc loss: 0.0008110065822353356, policy loss: 22.33139895452176
Experience 34, Iter 89, disc loss: 0.000741386330765913, policy loss: 21.40166663357568
Experience 34, Iter 90, disc loss: 0.0007935844548993644, policy loss: 21.313044993279338
Experience 34, Iter 91, disc loss: 0.000664333634400858, policy loss: 22.047650615418082
Experience 34, Iter 92, disc loss: 0.0007859472850679915, policy loss: 23.697229861450495
Experience 34, Iter 93, disc loss: 0.0009503080075987614, policy loss: 21.66080499145908
Experience 34, Iter 94, disc loss: 0.0011841430849312802, policy loss: 21.716461632802268
Experience 34, Iter 95, disc loss: 0.0008905910954527532, policy loss: 22.12115518539924
Experience 34, Iter 96, disc loss: 0.0013958710408601487, policy loss: 21.42646922324188
Experience 34, Iter 97, disc loss: 0.0023419765686289278, policy loss: 21.931238275692557
Experience 34, Iter 98, disc loss: 0.0006886495491485284, policy loss: 23.0581629856658
Experience 34, Iter 99, disc loss: 0.005285833316667665, policy loss: 22.53757579622647
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2517],
        [2.3211],
        [0.0467]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0208, 0.2075, 2.1641, 0.0384, 0.0250, 5.9861]],

        [[0.0208, 0.2075, 2.1641, 0.0384, 0.0250, 5.9861]],

        [[0.0208, 0.2075, 2.1641, 0.0384, 0.0250, 5.9861]],

        [[0.0208, 0.2075, 2.1641, 0.0384, 0.0250, 5.9861]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0192, 1.0070, 9.2842, 0.1869], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0192, 1.0070, 9.2842, 0.1869])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.890
Iter 2/2000 - Loss: 3.904
Iter 3/2000 - Loss: 3.670
Iter 4/2000 - Loss: 3.622
Iter 5/2000 - Loss: 3.581
Iter 6/2000 - Loss: 3.432
Iter 7/2000 - Loss: 3.256
Iter 8/2000 - Loss: 3.111
Iter 9/2000 - Loss: 2.985
Iter 10/2000 - Loss: 2.835
Iter 11/2000 - Loss: 2.646
Iter 12/2000 - Loss: 2.434
Iter 13/2000 - Loss: 2.218
Iter 14/2000 - Loss: 2.001
Iter 15/2000 - Loss: 1.777
Iter 16/2000 - Loss: 1.535
Iter 17/2000 - Loss: 1.275
Iter 18/2000 - Loss: 1.000
Iter 19/2000 - Loss: 0.716
Iter 20/2000 - Loss: 0.426
Iter 1981/2000 - Loss: -7.642
Iter 1982/2000 - Loss: -7.642
Iter 1983/2000 - Loss: -7.642
Iter 1984/2000 - Loss: -7.642
Iter 1985/2000 - Loss: -7.642
Iter 1986/2000 - Loss: -7.642
Iter 1987/2000 - Loss: -7.642
Iter 1988/2000 - Loss: -7.642
Iter 1989/2000 - Loss: -7.642
Iter 1990/2000 - Loss: -7.642
Iter 1991/2000 - Loss: -7.642
Iter 1992/2000 - Loss: -7.642
Iter 1993/2000 - Loss: -7.642
Iter 1994/2000 - Loss: -7.642
Iter 1995/2000 - Loss: -7.642
Iter 1996/2000 - Loss: -7.642
Iter 1997/2000 - Loss: -7.643
Iter 1998/2000 - Loss: -7.643
Iter 1999/2000 - Loss: -7.643
Iter 2000/2000 - Loss: -7.643
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[12.9623, 12.3009, 20.8572,  3.8266,  6.1847, 60.8277]],

        [[19.1027, 33.5612,  7.6826,  1.3296,  1.2157, 17.8852]],

        [[21.5939, 35.4840,  8.7649,  0.6875,  0.9630, 23.0931]],

        [[14.2576, 26.8499, 11.9644,  1.2544,  2.0573, 50.4264]]])
Signal Variance: tensor([ 0.1691,  1.3192, 11.3292,  0.3550])
Estimated target variance: tensor([0.0192, 1.0070, 9.2842, 0.1869])
N: 350
Signal to noise ratio: tensor([23.2560, 59.7840, 66.8761, 33.3031])
Bound on condition number: tensor([ 189295.2853, 1250946.6494, 1565347.0483,  388183.9240])
Policy Optimizer learning rate:
0.009648114596590806
Experience 35, Iter 0, disc loss: 0.000755637405855335, policy loss: 23.402455323177584
Experience 35, Iter 1, disc loss: 0.00077790127872338, policy loss: 22.767781777271438
Experience 35, Iter 2, disc loss: 0.0008923627354368509, policy loss: 21.50735053928046
Experience 35, Iter 3, disc loss: 0.0011315727253368817, policy loss: 21.04116057596146
Experience 35, Iter 4, disc loss: 0.003309873253006034, policy loss: 23.055018536428648
Experience 35, Iter 5, disc loss: 0.0015456206994419307, policy loss: 23.068250060090296
Experience 35, Iter 6, disc loss: 0.0009805107043698662, policy loss: 20.683556733044465
Experience 35, Iter 7, disc loss: 0.008640405308743205, policy loss: 24.332698102624747
Experience 35, Iter 8, disc loss: 0.0007852310871769149, policy loss: 22.9121816851355
Experience 35, Iter 9, disc loss: 0.001282253131103472, policy loss: 23.23175021695225
Experience 35, Iter 10, disc loss: 0.0008403876099794452, policy loss: 21.82567218295695
Experience 35, Iter 11, disc loss: 0.0007422425855271876, policy loss: 25.803228659204144
Experience 35, Iter 12, disc loss: 0.0008230704187495581, policy loss: 23.676432214729985
Experience 35, Iter 13, disc loss: 0.0007390417800107352, policy loss: 23.15928852194506
Experience 35, Iter 14, disc loss: 0.0017590008300160697, policy loss: 25.149557407918575
Experience 35, Iter 15, disc loss: 0.001173242307122798, policy loss: 25.091820050730217
Experience 35, Iter 16, disc loss: 0.0009162566842678154, policy loss: 23.653110332507584
Experience 35, Iter 17, disc loss: 0.0019738350596586384, policy loss: 27.108628618614272
Experience 35, Iter 18, disc loss: 0.0008214875800258664, policy loss: 27.10911354420907
Experience 35, Iter 19, disc loss: 0.001290386206626147, policy loss: 26.762460850324473
Experience 35, Iter 20, disc loss: 0.0012920592995752434, policy loss: 24.383019843384584
Experience 35, Iter 21, disc loss: 0.0018444715814760816, policy loss: 24.472362405908676
Experience 35, Iter 22, disc loss: 0.0007604442782562535, policy loss: 26.573177930156383
Experience 35, Iter 23, disc loss: 0.0006913435639317066, policy loss: 24.71867838439834
Experience 35, Iter 24, disc loss: 0.0007556586437659449, policy loss: 25.676442028658293
Experience 35, Iter 25, disc loss: 0.0008136196198853525, policy loss: 23.924231388016093
Experience 35, Iter 26, disc loss: 0.000654500933118013, policy loss: 27.85660221842621
Experience 35, Iter 27, disc loss: 0.0006578575412347023, policy loss: 27.020989554511555
Experience 35, Iter 28, disc loss: 0.004037630790275751, policy loss: 24.527259077779675
Experience 35, Iter 29, disc loss: 0.0031578448551571213, policy loss: 24.728377972267737
Experience 35, Iter 30, disc loss: 0.021091766846719297, policy loss: 27.578944080427448
Experience 35, Iter 31, disc loss: 0.0009001345304966637, policy loss: 29.300173443682922
Experience 35, Iter 32, disc loss: 0.0014534183946347293, policy loss: 30.43888083259442
Experience 35, Iter 33, disc loss: 0.000685638861276258, policy loss: 31.75505602310463
Experience 35, Iter 34, disc loss: 0.0006701511369482809, policy loss: 31.860831100198002
Experience 35, Iter 35, disc loss: 0.0006933492730344047, policy loss: 34.900524365195096
Experience 35, Iter 36, disc loss: 0.0008548065407135137, policy loss: 32.39027985251851
Experience 35, Iter 37, disc loss: 0.006029105787574514, policy loss: 32.15987759073683
Experience 35, Iter 38, disc loss: 0.0006910301557269745, policy loss: 36.29991869840315
Experience 35, Iter 39, disc loss: 0.0006949658383211914, policy loss: 38.12218940246492
Experience 35, Iter 40, disc loss: 0.0025695017293075914, policy loss: 34.27300321263414
Experience 35, Iter 41, disc loss: 0.000799076845084466, policy loss: 35.912502651196704
Experience 35, Iter 42, disc loss: 0.000700274400376553, policy loss: 36.01226236108185
Experience 35, Iter 43, disc loss: 0.0006988253587837849, policy loss: 35.81494417195193
Experience 35, Iter 44, disc loss: 0.0007324753390969859, policy loss: 36.54651244160347
Experience 35, Iter 45, disc loss: 0.0006910779720632811, policy loss: 35.39145249749426
Experience 35, Iter 46, disc loss: 0.0008469457346342954, policy loss: 36.77325984455264
Experience 35, Iter 47, disc loss: 0.0006788925519879091, policy loss: 37.20453612693104
Experience 35, Iter 48, disc loss: 0.0006702356585335965, policy loss: 40.136083769416615
Experience 35, Iter 49, disc loss: 0.0006749076921837259, policy loss: 36.60125851683422
Experience 35, Iter 50, disc loss: 0.0006534724029783285, policy loss: 36.74993102814901
Experience 35, Iter 51, disc loss: 0.014953317083405067, policy loss: 39.07510870622141
Experience 35, Iter 52, disc loss: 0.0006470149825657228, policy loss: 37.84672266306005
Experience 35, Iter 53, disc loss: 0.0006494095536426609, policy loss: 39.42485377906877
Experience 35, Iter 54, disc loss: 0.0010159701157155827, policy loss: 38.11890720255985
Experience 35, Iter 55, disc loss: 0.0006519950819056922, policy loss: 39.406715403578005
Experience 35, Iter 56, disc loss: 0.0006483586925673399, policy loss: 36.062073078112235
Experience 35, Iter 57, disc loss: 0.0006707453668131793, policy loss: 35.67804529890439
Experience 35, Iter 58, disc loss: 0.0007364762266953466, policy loss: 36.621922770653256
Experience 35, Iter 59, disc loss: 0.0006793622604084582, policy loss: 35.611415830992364
Experience 35, Iter 60, disc loss: 0.0006293205561118807, policy loss: 36.06822475828314
Experience 35, Iter 61, disc loss: 0.0006671010224932622, policy loss: 33.20522792673947
Experience 35, Iter 62, disc loss: 0.0006154316291870682, policy loss: 34.33954438710319
Experience 35, Iter 63, disc loss: 0.0006069538442390105, policy loss: 33.328857279797134
Experience 35, Iter 64, disc loss: 0.0005999234711427971, policy loss: 36.08909855970707
Experience 35, Iter 65, disc loss: 0.0012995961440529467, policy loss: 35.26418745148484
Experience 35, Iter 66, disc loss: 0.0005813613710886484, policy loss: 34.40425606168655
Experience 35, Iter 67, disc loss: 0.0005729587935144337, policy loss: 33.94051153522806
Experience 35, Iter 68, disc loss: 0.0005643722480854322, policy loss: 32.92083241006263
Experience 35, Iter 69, disc loss: 0.000945444629743085, policy loss: 31.393828534932105
Experience 35, Iter 70, disc loss: 0.003582651694967082, policy loss: 35.057604460443386
Experience 35, Iter 71, disc loss: 0.0005425224927583007, policy loss: 33.85226329477874
Experience 35, Iter 72, disc loss: 0.0005364372938193864, policy loss: 34.7862241085017
Experience 35, Iter 73, disc loss: 0.0005303698050867846, policy loss: 34.58575953528607
Experience 35, Iter 74, disc loss: 0.0005240106447104714, policy loss: 36.94896656369465
Experience 35, Iter 75, disc loss: 0.0005597899165667715, policy loss: 33.02543721093151
Experience 35, Iter 76, disc loss: 0.0006390347858586828, policy loss: 31.70912611659162
Experience 35, Iter 77, disc loss: 0.0005044934549999371, policy loss: 34.468855275585945
Experience 35, Iter 78, disc loss: 0.0005147655800050218, policy loss: 36.150546924283354
Experience 35, Iter 79, disc loss: 0.0022407988666228234, policy loss: 34.99352031250578
Experience 35, Iter 80, disc loss: 0.0004912281328362775, policy loss: 33.48496526887453
Experience 35, Iter 81, disc loss: 0.0004794990239856972, policy loss: 33.79576778497888
Experience 35, Iter 82, disc loss: 0.0005408946659321437, policy loss: 34.62647763101215
Experience 35, Iter 83, disc loss: 0.00046774661019720754, policy loss: 36.19465488285135
Experience 35, Iter 84, disc loss: 0.000461937748242202, policy loss: 34.81171488162108
Experience 35, Iter 85, disc loss: 0.00048686487339613845, policy loss: 33.533607495594794
Experience 35, Iter 86, disc loss: 0.0004582302875667774, policy loss: 33.50037536673229
Experience 35, Iter 87, disc loss: 0.0004447119640095902, policy loss: 35.42423246665689
Experience 35, Iter 88, disc loss: 0.00043911157996944877, policy loss: 37.734447845062206
Experience 35, Iter 89, disc loss: 0.0004330738445764163, policy loss: 35.03809077563324
Experience 35, Iter 90, disc loss: 0.0004273949700068061, policy loss: 34.38763451801236
Experience 35, Iter 91, disc loss: 0.0004695528727195243, policy loss: 34.43478339521891
Experience 35, Iter 92, disc loss: 0.00182304092997645, policy loss: 32.52691453700777
Experience 35, Iter 93, disc loss: 0.00042371181987886045, policy loss: 35.993590088653704
Experience 35, Iter 94, disc loss: 0.0025707294820795485, policy loss: 34.796678434449916
Experience 35, Iter 95, disc loss: 0.004894721606306646, policy loss: 34.2118925196024
Experience 35, Iter 96, disc loss: 0.00043068832998167657, policy loss: 33.991621262216405
Experience 35, Iter 97, disc loss: 0.0007911195509341209, policy loss: 34.931930384733
Experience 35, Iter 98, disc loss: 0.00041731812772835525, policy loss: 38.23885417083085
Experience 35, Iter 99, disc loss: 0.0011892150641531268, policy loss: 35.897917361151244
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2497],
        [2.3025],
        [0.0466]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0219, 0.2077, 2.1574, 0.0383, 0.0244, 5.9554]],

        [[0.0219, 0.2077, 2.1574, 0.0383, 0.0244, 5.9554]],

        [[0.0219, 0.2077, 2.1574, 0.0383, 0.0244, 5.9554]],

        [[0.0219, 0.2077, 2.1574, 0.0383, 0.0244, 5.9554]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0193, 0.9986, 9.2102, 0.1866], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0193, 0.9986, 9.2102, 0.1866])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.871
Iter 2/2000 - Loss: 3.871
Iter 3/2000 - Loss: 3.652
Iter 4/2000 - Loss: 3.600
Iter 5/2000 - Loss: 3.550
Iter 6/2000 - Loss: 3.399
Iter 7/2000 - Loss: 3.227
Iter 8/2000 - Loss: 3.088
Iter 9/2000 - Loss: 2.962
Iter 10/2000 - Loss: 2.804
Iter 11/2000 - Loss: 2.610
Iter 12/2000 - Loss: 2.399
Iter 13/2000 - Loss: 2.188
Iter 14/2000 - Loss: 1.974
Iter 15/2000 - Loss: 1.749
Iter 16/2000 - Loss: 1.505
Iter 17/2000 - Loss: 1.243
Iter 18/2000 - Loss: 0.968
Iter 19/2000 - Loss: 0.686
Iter 20/2000 - Loss: 0.397
Iter 1981/2000 - Loss: -7.678
Iter 1982/2000 - Loss: -7.678
Iter 1983/2000 - Loss: -7.678
Iter 1984/2000 - Loss: -7.678
Iter 1985/2000 - Loss: -7.678
Iter 1986/2000 - Loss: -7.678
Iter 1987/2000 - Loss: -7.678
Iter 1988/2000 - Loss: -7.678
Iter 1989/2000 - Loss: -7.678
Iter 1990/2000 - Loss: -7.678
Iter 1991/2000 - Loss: -7.678
Iter 1992/2000 - Loss: -7.678
Iter 1993/2000 - Loss: -7.678
Iter 1994/2000 - Loss: -7.678
Iter 1995/2000 - Loss: -7.678
Iter 1996/2000 - Loss: -7.678
Iter 1997/2000 - Loss: -7.678
Iter 1998/2000 - Loss: -7.678
Iter 1999/2000 - Loss: -7.678
Iter 2000/2000 - Loss: -7.678
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[12.8655, 11.8412, 20.8904,  3.9518,  6.1436, 58.2334]],

        [[19.3329, 33.2087,  7.5952,  1.2985,  1.2781, 18.2349]],

        [[21.9379, 35.7631,  8.8204,  0.6810,  0.9814, 23.0604]],

        [[14.6301, 27.2827, 11.8001,  1.3005,  1.9774, 50.8199]]])
Signal Variance: tensor([ 0.1618,  1.3352, 11.2783,  0.3501])
Estimated target variance: tensor([0.0193, 0.9986, 9.2102, 0.1866])
N: 360
Signal to noise ratio: tensor([22.4984, 60.2936, 67.6490, 33.3005])
Bound on condition number: tensor([ 182225.7833, 1308715.8833, 1647500.9020,  399213.2114])
Policy Optimizer learning rate:
0.009637954646528333
