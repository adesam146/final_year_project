Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0130],
        [0.8980],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0518, 3.5922, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 1.196
Iter 3/2000 - Loss: 0.953
Iter 4/2000 - Loss: 1.383
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.493
Iter 7/2000 - Loss: 1.248
Iter 8/2000 - Loss: 1.053
Iter 9/2000 - Loss: 0.964
Iter 10/2000 - Loss: 0.972
Iter 11/2000 - Loss: 1.033
Iter 12/2000 - Loss: 1.091
Iter 13/2000 - Loss: 1.105
Iter 14/2000 - Loss: 1.071
Iter 15/2000 - Loss: 1.012
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.889
Iter 20/2000 - Loss: 0.912
Iter 1981/2000 - Loss: 0.698
Iter 1982/2000 - Loss: 0.698
Iter 1983/2000 - Loss: 0.698
Iter 1984/2000 - Loss: 0.698
Iter 1985/2000 - Loss: 0.698
Iter 1986/2000 - Loss: 0.698
Iter 1987/2000 - Loss: 0.698
Iter 1988/2000 - Loss: 0.698
Iter 1989/2000 - Loss: 0.698
Iter 1990/2000 - Loss: 0.698
Iter 1991/2000 - Loss: 0.698
Iter 1992/2000 - Loss: 0.698
Iter 1993/2000 - Loss: 0.698
Iter 1994/2000 - Loss: 0.698
Iter 1995/2000 - Loss: 0.698
Iter 1996/2000 - Loss: 0.698
Iter 1997/2000 - Loss: 0.698
Iter 1998/2000 - Loss: 0.698
Iter 1999/2000 - Loss: 0.698
Iter 2000/2000 - Loss: 0.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0093],
        [0.4134],
        [0.0149]])
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]])
Signal Variance: tensor([0.0033, 0.0374, 2.8196, 0.0608])
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([1.9681, 2.0097, 2.6117, 2.0186])
Bound on condition number: tensor([39.7326, 41.3892, 69.2095, 41.7472])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2474456489459007, policy loss: 0.7896666701180209
Experience 1, Iter 1, disc loss: 1.2385531918507136, policy loss: 0.7873821973000745
Experience 1, Iter 2, disc loss: 1.2212897815354118, policy loss: 0.7971549956316779
Experience 1, Iter 3, disc loss: 1.2120402931975693, policy loss: 0.7965295156419951
Experience 1, Iter 4, disc loss: 1.197902971705078, policy loss: 0.8019190476639999
Experience 1, Iter 5, disc loss: 1.190033381805849, policy loss: 0.8005765830916368
Experience 1, Iter 6, disc loss: 1.181139960153723, policy loss: 0.7994869907213111
Experience 1, Iter 7, disc loss: 1.1722257182845042, policy loss: 0.8011000780964663
Experience 1, Iter 8, disc loss: 1.153104369574698, policy loss: 0.8132605123798955
Experience 1, Iter 9, disc loss: 1.1224170457251015, policy loss: 0.8429432388516404
Experience 1, Iter 10, disc loss: 1.1379390788645738, policy loss: 0.816809959267213
Experience 1, Iter 11, disc loss: 1.1172306148283617, policy loss: 0.8307882214099476
Experience 1, Iter 12, disc loss: 1.09901539563721, policy loss: 0.8477698262252082
Experience 1, Iter 13, disc loss: 1.101925855257222, policy loss: 0.8298140382165464
Experience 1, Iter 14, disc loss: 1.0776826226781542, policy loss: 0.8528265008296603
Experience 1, Iter 15, disc loss: 1.0670463341487202, policy loss: 0.8581163936917341
Experience 1, Iter 16, disc loss: 1.0540709332169198, policy loss: 0.862005558120709
Experience 1, Iter 17, disc loss: 1.0581007384931267, policy loss: 0.8483832228959503
Experience 1, Iter 18, disc loss: 1.0191336856377295, policy loss: 0.8855649355368592
Experience 1, Iter 19, disc loss: 1.0283318303451865, policy loss: 0.8657861934967082
Experience 1, Iter 20, disc loss: 1.0055535465506542, policy loss: 0.8822679989594887
Experience 1, Iter 21, disc loss: 0.9911740582474973, policy loss: 0.892649863184688
Experience 1, Iter 22, disc loss: 0.9756453755812586, policy loss: 0.900404234467851
Experience 1, Iter 23, disc loss: 0.952521764501953, policy loss: 0.9240969728421321
Experience 1, Iter 24, disc loss: 0.9698868692201202, policy loss: 0.8907544611774891
Experience 1, Iter 25, disc loss: 0.9357473131725084, policy loss: 0.9278979524041223
Experience 1, Iter 26, disc loss: 0.9435471033714395, policy loss: 0.9100303847889553
Experience 1, Iter 27, disc loss: 0.8940934535552183, policy loss: 0.9704219574551181
Experience 1, Iter 28, disc loss: 0.8965340197188861, policy loss: 0.9472721533309466
Experience 1, Iter 29, disc loss: 0.8748503552352667, policy loss: 0.9639159021327878
Experience 1, Iter 30, disc loss: 0.8543232359321205, policy loss: 0.9879292798076718
Experience 1, Iter 31, disc loss: 0.8306454012517545, policy loss: 1.011940394609799
Experience 1, Iter 32, disc loss: 0.8303763580964079, policy loss: 1.0039501965648183
Experience 1, Iter 33, disc loss: 0.8114255238856356, policy loss: 1.0324361447109158
Experience 1, Iter 34, disc loss: 0.7925805174332047, policy loss: 1.0437819945578322
Experience 1, Iter 35, disc loss: 0.7656001168364329, policy loss: 1.086461110002284
Experience 1, Iter 36, disc loss: 0.7450509352668286, policy loss: 1.0992084124865977
Experience 1, Iter 37, disc loss: 0.7451801303839484, policy loss: 1.0895013467615833
Experience 1, Iter 38, disc loss: 0.7391923598148977, policy loss: 1.0864479880983988
Experience 1, Iter 39, disc loss: 0.7031651915162589, policy loss: 1.1407942174529602
Experience 1, Iter 40, disc loss: 0.7074794218421132, policy loss: 1.121705879430794
Experience 1, Iter 41, disc loss: 0.6955371432440273, policy loss: 1.1255160974108358
Experience 1, Iter 42, disc loss: 0.6716718900813538, policy loss: 1.1547400444532336
Experience 1, Iter 43, disc loss: 0.6463550298475382, policy loss: 1.198603871155162
Experience 1, Iter 44, disc loss: 0.6482844549874079, policy loss: 1.1798919552339524
Experience 1, Iter 45, disc loss: 0.6435565048806114, policy loss: 1.1689060755537817
Experience 1, Iter 46, disc loss: 0.6184827652000124, policy loss: 1.2133745525073625
Experience 1, Iter 47, disc loss: 0.6348080144009958, policy loss: 1.177305475217787
Experience 1, Iter 48, disc loss: 0.5892107406952843, policy loss: 1.2654541828538077
Experience 1, Iter 49, disc loss: 0.5718353063620938, policy loss: 1.2904168341627558
Experience 1, Iter 50, disc loss: 0.5457025928396745, policy loss: 1.3308390119628828
Experience 1, Iter 51, disc loss: 0.5466297705681182, policy loss: 1.3107927018481924
Experience 1, Iter 52, disc loss: 0.5410532604937096, policy loss: 1.3518383733389725
Experience 1, Iter 53, disc loss: 0.5374750668180412, policy loss: 1.3232578652771951
Experience 1, Iter 54, disc loss: 0.4941950718296611, policy loss: 1.4197210981529318
Experience 1, Iter 55, disc loss: 0.47134660391008726, policy loss: 1.453037739513706
Experience 1, Iter 56, disc loss: 0.46828025757696834, policy loss: 1.4444471565426256
Experience 1, Iter 57, disc loss: 0.48125422392402056, policy loss: 1.4164236827067351
Experience 1, Iter 58, disc loss: 0.4413486226783918, policy loss: 1.5046045350787958
Experience 1, Iter 59, disc loss: 0.41508053163347175, policy loss: 1.5713174397531444
Experience 1, Iter 60, disc loss: 0.440787284441827, policy loss: 1.4895845622426553
Experience 1, Iter 61, disc loss: 0.44584436413836726, policy loss: 1.5009213260157779
Experience 1, Iter 62, disc loss: 0.40565893754755433, policy loss: 1.569563692407962
Experience 1, Iter 63, disc loss: 0.38964297430415407, policy loss: 1.634303851476104
Experience 1, Iter 64, disc loss: 0.4078632242950616, policy loss: 1.5839279789485188
Experience 1, Iter 65, disc loss: 0.40194290171962355, policy loss: 1.5955854910095622
Experience 1, Iter 66, disc loss: 0.37133186201376356, policy loss: 1.6583896212049045
Experience 1, Iter 67, disc loss: 0.38268252740592223, policy loss: 1.5965897512879557
Experience 1, Iter 68, disc loss: 0.3485036126715727, policy loss: 1.7236887871024544
Experience 1, Iter 69, disc loss: 0.34148101798587266, policy loss: 1.7295496178861618
Experience 1, Iter 70, disc loss: 0.3264714902621854, policy loss: 1.7894187864257565
Experience 1, Iter 71, disc loss: 0.31072273594437994, policy loss: 1.8483948922702775
Experience 1, Iter 72, disc loss: 0.3078877681353323, policy loss: 1.8703695896580619
Experience 1, Iter 73, disc loss: 0.3210343519310225, policy loss: 1.848044122873199
Experience 1, Iter 74, disc loss: 0.2920740922755045, policy loss: 1.9158249268518672
Experience 1, Iter 75, disc loss: 0.2970611713038376, policy loss: 1.8946052806721148
Experience 1, Iter 76, disc loss: 0.26341740214388004, policy loss: 1.9921880964730558
Experience 1, Iter 77, disc loss: 0.27536365512640415, policy loss: 1.9632710646526124
Experience 1, Iter 78, disc loss: 0.2730445671735041, policy loss: 1.972586706605791
Experience 1, Iter 79, disc loss: 0.2221730432348728, policy loss: 2.1976122217322764
Experience 1, Iter 80, disc loss: 0.26449634861857874, policy loss: 1.9998293664200475
Experience 1, Iter 81, disc loss: 0.23909510755425678, policy loss: 2.0897635160586177
Experience 1, Iter 82, disc loss: 0.22897027815746102, policy loss: 2.1297710711625752
Experience 1, Iter 83, disc loss: 0.2404680916877837, policy loss: 1.998127804572225
Experience 1, Iter 84, disc loss: 0.1905040852651111, policy loss: 2.299903181801514
Experience 1, Iter 85, disc loss: 0.21100613153791895, policy loss: 2.236650921093272
Experience 1, Iter 86, disc loss: 0.2106619960091393, policy loss: 2.171216329384843
Experience 1, Iter 87, disc loss: 0.21286859706719302, policy loss: 2.1305218474867074
Experience 1, Iter 88, disc loss: 0.18930023467495882, policy loss: 2.370163286991139
Experience 1, Iter 89, disc loss: 0.19853616545676642, policy loss: 2.2771678595174834
Experience 1, Iter 90, disc loss: 0.17004785094143698, policy loss: 2.3673581443454883
Experience 1, Iter 91, disc loss: 0.19267821810167518, policy loss: 2.44001225937372
Experience 1, Iter 92, disc loss: 0.16942973870087874, policy loss: 2.495592744743767
Experience 1, Iter 93, disc loss: 0.1684472333253703, policy loss: 2.5404493359835456
Experience 1, Iter 94, disc loss: 0.17857933328481923, policy loss: 2.404832169712531
Experience 1, Iter 95, disc loss: 0.16762913299219356, policy loss: 2.488622807692546
Experience 1, Iter 96, disc loss: 0.17407774045605115, policy loss: 2.462630735533673
Experience 1, Iter 97, disc loss: 0.16731052879894126, policy loss: 2.498489179222381
Experience 1, Iter 98, disc loss: 0.13419767450249778, policy loss: 2.8239966867634214
Experience 1, Iter 99, disc loss: 0.1528927085417305, policy loss: 2.477299428339072
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.0197],
        [0.8398],
        [0.0182]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0382, 0.1590, 0.8460, 0.0179, 0.0033, 0.3765]],

        [[0.0382, 0.1590, 0.8460, 0.0179, 0.0033, 0.3765]],

        [[0.0382, 0.1590, 0.8460, 0.0179, 0.0033, 0.3765]],

        [[0.0382, 0.1590, 0.8460, 0.0179, 0.0033, 0.3765]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.0786, 3.3591, 0.0728], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.0786, 3.3591, 0.0728])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.534
Iter 2/2000 - Loss: 2.366
Iter 3/2000 - Loss: 1.826
Iter 4/2000 - Loss: 1.785
Iter 5/2000 - Loss: 1.920
Iter 6/2000 - Loss: 1.998
Iter 7/2000 - Loss: 1.995
Iter 8/2000 - Loss: 1.955
Iter 9/2000 - Loss: 1.902
Iter 10/2000 - Loss: 1.846
Iter 11/2000 - Loss: 1.799
Iter 12/2000 - Loss: 1.774
Iter 13/2000 - Loss: 1.765
Iter 14/2000 - Loss: 1.752
Iter 15/2000 - Loss: 1.716
Iter 16/2000 - Loss: 1.650
Iter 17/2000 - Loss: 1.564
Iter 18/2000 - Loss: 1.474
Iter 19/2000 - Loss: 1.398
Iter 20/2000 - Loss: 1.343
Iter 1981/2000 - Loss: -4.150
Iter 1982/2000 - Loss: -4.150
Iter 1983/2000 - Loss: -4.150
Iter 1984/2000 - Loss: -4.150
Iter 1985/2000 - Loss: -4.150
Iter 1986/2000 - Loss: -4.150
Iter 1987/2000 - Loss: -4.150
Iter 1988/2000 - Loss: -4.150
Iter 1989/2000 - Loss: -4.150
Iter 1990/2000 - Loss: -4.150
Iter 1991/2000 - Loss: -4.150
Iter 1992/2000 - Loss: -4.150
Iter 1993/2000 - Loss: -4.150
Iter 1994/2000 - Loss: -4.150
Iter 1995/2000 - Loss: -4.150
Iter 1996/2000 - Loss: -4.150
Iter 1997/2000 - Loss: -4.150
Iter 1998/2000 - Loss: -4.150
Iter 1999/2000 - Loss: -4.150
Iter 2000/2000 - Loss: -4.150
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0001],
        [0.0006]])
Lengthscale: tensor([[[22.8369,  4.2291, 63.6447, 23.0555,  3.3226, 42.3263]],

        [[16.1736, 36.6207,  9.0859,  0.5378,  3.5284,  4.4152]],

        [[30.1832, 21.1071,  7.2277,  1.8967,  2.5623, 16.2078]],

        [[40.0278, 64.5050, 14.5138,  3.0247, 11.6864, 36.4656]]])
Signal Variance: tensor([ 0.0599,  0.1344, 24.3222,  0.4706])
Estimated target variance: tensor([0.0159, 0.0786, 3.3591, 0.0728])
N: 20
Signal to noise ratio: tensor([ 11.7565,  20.3192, 488.1059,  27.3507])
Bound on condition number: tensor([2.7653e+03, 8.2584e+03, 4.7649e+06, 1.4962e+04])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.07808792972796616, policy loss: 3.345273675151799
Experience 2, Iter 1, disc loss: 0.07997745996899322, policy loss: 3.2287996139521877
Experience 2, Iter 2, disc loss: 0.08741078983388784, policy loss: 3.045380906857692
Experience 2, Iter 3, disc loss: 0.09877739573956668, policy loss: 2.833920303736263
Experience 2, Iter 4, disc loss: 0.10661238366608872, policy loss: 2.6944178688432316
Experience 2, Iter 5, disc loss: 0.11819078864211344, policy loss: 2.5611861843418158
Experience 2, Iter 6, disc loss: 0.12644964134354586, policy loss: 2.4345480358628926
Experience 2, Iter 7, disc loss: 0.13684266091649422, policy loss: 2.3343228097218383
Experience 2, Iter 8, disc loss: 0.14815409282584086, policy loss: 2.227741308209121
Experience 2, Iter 9, disc loss: 0.15385841351913881, policy loss: 2.169525208491892
Experience 2, Iter 10, disc loss: 0.16777118215617567, policy loss: 2.0704785201700115
Experience 2, Iter 11, disc loss: 0.1693483138171564, policy loss: 2.049164595114794
Experience 2, Iter 12, disc loss: 0.1698445521228305, policy loss: 2.0339608110214185
Experience 2, Iter 13, disc loss: 0.17819258075767797, policy loss: 1.9999368915023865
Experience 2, Iter 14, disc loss: 0.19466567420203912, policy loss: 1.8994437315007022
Experience 2, Iter 15, disc loss: 0.2031810999117511, policy loss: 1.8406253851414012
Experience 2, Iter 16, disc loss: 0.1937489231458892, policy loss: 1.911985489690842
Experience 2, Iter 17, disc loss: 0.19434786951515812, policy loss: 1.930836024355498
Experience 2, Iter 18, disc loss: 0.1963585225161866, policy loss: 1.9483403457201813
Experience 2, Iter 19, disc loss: 0.21243856488200957, policy loss: 1.8196677032498332
Experience 2, Iter 20, disc loss: 0.2234127125277504, policy loss: 1.8338520433432277
Experience 2, Iter 21, disc loss: 0.19646198738235005, policy loss: 1.904389932880484
Experience 2, Iter 22, disc loss: 0.19074017147381567, policy loss: 1.9989486076644694
Experience 2, Iter 23, disc loss: 0.19710109038290408, policy loss: 1.992529180391216
Experience 2, Iter 24, disc loss: 0.17826743650055374, policy loss: 2.040543235696349
Experience 2, Iter 25, disc loss: 0.17006082693888275, policy loss: 2.090215113669691
Experience 2, Iter 26, disc loss: 0.19081883181880524, policy loss: 1.9934290150007028
Experience 2, Iter 27, disc loss: 0.17825644359612944, policy loss: 2.0721710651648646
Experience 2, Iter 28, disc loss: 0.1707417850935864, policy loss: 2.1291591868330793
Experience 2, Iter 29, disc loss: 0.17174128850176082, policy loss: 2.1159838792594527
Experience 2, Iter 30, disc loss: 0.1824709982744974, policy loss: 1.979736597064165
Experience 2, Iter 31, disc loss: 0.1602245543008376, policy loss: 2.1533922642791907
Experience 2, Iter 32, disc loss: 0.1710698531628469, policy loss: 2.0611775389393383
Experience 2, Iter 33, disc loss: 0.1544729115960695, policy loss: 2.1721876189625062
Experience 2, Iter 34, disc loss: 0.14400716180017806, policy loss: 2.3073672181577205
Experience 2, Iter 35, disc loss: 0.1354907560701739, policy loss: 2.3249779078301422
Experience 2, Iter 36, disc loss: 0.1306394510167842, policy loss: 2.3069927516285693
Experience 2, Iter 37, disc loss: 0.14684816563054262, policy loss: 2.234563552152545
Experience 2, Iter 38, disc loss: 0.12743620797681252, policy loss: 2.3418586609465555
Experience 2, Iter 39, disc loss: 0.150847231092978, policy loss: 2.189497031405675
Experience 2, Iter 40, disc loss: 0.14320012972988144, policy loss: 2.2625710959226346
Experience 2, Iter 41, disc loss: 0.12480750120444714, policy loss: 2.4194368252526157
Experience 2, Iter 42, disc loss: 0.1093713218317313, policy loss: 2.5394931000619247
Experience 2, Iter 43, disc loss: 0.1285946023958647, policy loss: 2.37523605194439
Experience 2, Iter 44, disc loss: 0.13388812789340881, policy loss: 2.3194491154077905
Experience 2, Iter 45, disc loss: 0.12483112486769608, policy loss: 2.4744615065149294
Experience 2, Iter 46, disc loss: 0.12337305107585166, policy loss: 2.4045978085833606
Experience 2, Iter 47, disc loss: 0.10862303943035947, policy loss: 2.635628234112551
Experience 2, Iter 48, disc loss: 0.1364617462067128, policy loss: 2.320129702592758
Experience 2, Iter 49, disc loss: 0.09104664037375068, policy loss: 2.7363301221238734
Experience 2, Iter 50, disc loss: 0.09526885646167116, policy loss: 2.741062422032538
Experience 2, Iter 51, disc loss: 0.0834043040026059, policy loss: 2.8548997023826193
Experience 2, Iter 52, disc loss: 0.12168899539127606, policy loss: 2.7579210580075593
Experience 2, Iter 53, disc loss: 0.10338664577735668, policy loss: 2.710159439055478
Experience 2, Iter 54, disc loss: 0.11493653659270596, policy loss: 2.6687194355134682
Experience 2, Iter 55, disc loss: 0.10543977615676264, policy loss: 2.6448394920175913
Experience 2, Iter 56, disc loss: 0.09712122822935229, policy loss: 2.758140277409603
Experience 2, Iter 57, disc loss: 0.08720410499514057, policy loss: 2.887423950795918
Experience 2, Iter 58, disc loss: 0.0880279140293757, policy loss: 2.9151925923834354
Experience 2, Iter 59, disc loss: 0.08579964734148411, policy loss: 2.84552780847034
Experience 2, Iter 60, disc loss: 0.08366234693276323, policy loss: 2.8723761201133633
Experience 2, Iter 61, disc loss: 0.08225055646554183, policy loss: 2.9169959964255208
Experience 2, Iter 62, disc loss: 0.07944578732219996, policy loss: 2.9118913313300987
Experience 2, Iter 63, disc loss: 0.09131133409109449, policy loss: 2.7937485064811862
Experience 2, Iter 64, disc loss: 0.07266077364887201, policy loss: 3.0121670738868156
Experience 2, Iter 65, disc loss: 0.08067646238376158, policy loss: 2.9959821747505617
Experience 2, Iter 66, disc loss: 0.069378917136721, policy loss: 3.0187960212080736
Experience 2, Iter 67, disc loss: 0.07273234604993108, policy loss: 3.061483492853478
Experience 2, Iter 68, disc loss: 0.0821839009400013, policy loss: 2.9115718178668804
Experience 2, Iter 69, disc loss: 0.07251731454461931, policy loss: 3.012078554130801
Experience 2, Iter 70, disc loss: 0.07011285147523252, policy loss: 3.1023013449171146
Experience 2, Iter 71, disc loss: 0.07092552212174051, policy loss: 3.178100016155199
Experience 2, Iter 72, disc loss: 0.0679211154791507, policy loss: 3.1759725925783844
Experience 2, Iter 73, disc loss: 0.0651584104861634, policy loss: 3.159181622481968
Experience 2, Iter 74, disc loss: 0.06978184155190556, policy loss: 3.286045799964022
Experience 2, Iter 75, disc loss: 0.06816391360245998, policy loss: 3.10406239958467
Experience 2, Iter 76, disc loss: 0.056112219894055064, policy loss: 3.301855121771817
Experience 2, Iter 77, disc loss: 0.06370383921757843, policy loss: 3.261099578755784
Experience 2, Iter 78, disc loss: 0.06436442931688757, policy loss: 3.205534920058576
Experience 2, Iter 79, disc loss: 0.05581492174172274, policy loss: 3.271031489788026
Experience 2, Iter 80, disc loss: 0.05744474336720631, policy loss: 3.363448659005389
Experience 2, Iter 81, disc loss: 0.060819658431564345, policy loss: 3.2447967843701906
Experience 2, Iter 82, disc loss: 0.062398726980871455, policy loss: 3.243084110381411
Experience 2, Iter 83, disc loss: 0.059805805810493294, policy loss: 3.25270502265834
Experience 2, Iter 84, disc loss: 0.06181949541179992, policy loss: 3.2384813211330488
Experience 2, Iter 85, disc loss: 0.05445759187121626, policy loss: 3.4452621245742328
Experience 2, Iter 86, disc loss: 0.057630538960626476, policy loss: 3.4181397593064053
Experience 2, Iter 87, disc loss: 0.04817376183311455, policy loss: 3.6157312049883394
Experience 2, Iter 88, disc loss: 0.05702697177065011, policy loss: 3.361484570234323
Experience 2, Iter 89, disc loss: 0.05422019404368384, policy loss: 3.502987079008752
Experience 2, Iter 90, disc loss: 0.05600437917535876, policy loss: 3.5151615493868182
Experience 2, Iter 91, disc loss: 0.04683616094453437, policy loss: 3.568242946408988
Experience 2, Iter 92, disc loss: 0.061253576174058674, policy loss: 3.6170229573644237
Experience 2, Iter 93, disc loss: 0.055775742175907056, policy loss: 3.514185409387551
Experience 2, Iter 94, disc loss: 0.0491156574947949, policy loss: 3.9431527305672702
Experience 2, Iter 95, disc loss: 0.05591898925340745, policy loss: 3.529994397434325
Experience 2, Iter 96, disc loss: 0.052356160832380844, policy loss: 3.6224963132307515
Experience 2, Iter 97, disc loss: 0.046471407894647224, policy loss: 3.886410349224428
Experience 2, Iter 98, disc loss: 0.05380100711746784, policy loss: 3.565350656369983
Experience 2, Iter 99, disc loss: 0.0474606643405778, policy loss: 3.726024991985608
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0064],
        [0.0267],
        [0.5506],
        [0.0120]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0533, 0.2355, 0.5546, 0.0164, 0.0028, 0.9064]],

        [[0.0533, 0.2355, 0.5546, 0.0164, 0.0028, 0.9064]],

        [[0.0533, 0.2355, 0.5546, 0.0164, 0.0028, 0.9064]],

        [[0.0533, 0.2355, 0.5546, 0.0164, 0.0028, 0.9064]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0257, 0.1069, 2.2026, 0.0478], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0257, 0.1069, 2.2026, 0.0478])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.912
Iter 2/2000 - Loss: 1.528
Iter 3/2000 - Loss: 1.297
Iter 4/2000 - Loss: 1.266
Iter 5/2000 - Loss: 1.232
Iter 6/2000 - Loss: 1.110
Iter 7/2000 - Loss: 0.960
Iter 8/2000 - Loss: 0.834
Iter 9/2000 - Loss: 0.740
Iter 10/2000 - Loss: 0.648
Iter 11/2000 - Loss: 0.524
Iter 12/2000 - Loss: 0.371
Iter 13/2000 - Loss: 0.220
Iter 14/2000 - Loss: 0.102
Iter 15/2000 - Loss: 0.021
Iter 16/2000 - Loss: -0.054
Iter 17/2000 - Loss: -0.157
Iter 18/2000 - Loss: -0.290
Iter 19/2000 - Loss: -0.428
Iter 20/2000 - Loss: -0.546
Iter 1981/2000 - Loss: -5.717
Iter 1982/2000 - Loss: -5.718
Iter 1983/2000 - Loss: -5.718
Iter 1984/2000 - Loss: -5.718
Iter 1985/2000 - Loss: -5.718
Iter 1986/2000 - Loss: -5.718
Iter 1987/2000 - Loss: -5.718
Iter 1988/2000 - Loss: -5.718
Iter 1989/2000 - Loss: -5.718
Iter 1990/2000 - Loss: -5.718
Iter 1991/2000 - Loss: -5.718
Iter 1992/2000 - Loss: -5.718
Iter 1993/2000 - Loss: -5.718
Iter 1994/2000 - Loss: -5.718
Iter 1995/2000 - Loss: -5.718
Iter 1996/2000 - Loss: -5.718
Iter 1997/2000 - Loss: -5.718
Iter 1998/2000 - Loss: -5.718
Iter 1999/2000 - Loss: -5.718
Iter 2000/2000 - Loss: -5.718
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0006],
        [0.0005]])
Lengthscale: tensor([[[14.7814,  5.2463, 62.2324, 17.3581,  4.4799, 38.4067]],

        [[18.5031, 36.5945,  8.8794,  0.5786,  5.1796,  6.0390]],

        [[25.3314, 39.9287, 12.5674,  0.5955,  0.8715,  7.7723]],

        [[27.3595, 52.9384, 14.5743,  2.8221, 12.4548, 35.4421]]])
Signal Variance: tensor([0.0788, 0.1942, 5.3070, 0.4300])
Estimated target variance: tensor([0.0257, 0.1069, 2.2026, 0.0478])
N: 30
Signal to noise ratio: tensor([15.3023, 25.4587, 97.2894, 30.8816])
Bound on condition number: tensor([  7025.7673,  19445.4210, 283957.9911,  28611.2157])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.16204093122012758, policy loss: 1.9559447740654
Experience 3, Iter 1, disc loss: 0.1593872945331532, policy loss: 1.9730155853014057
Experience 3, Iter 2, disc loss: 0.16336253899514638, policy loss: 1.949258269340565
Experience 3, Iter 3, disc loss: 0.1620797414611114, policy loss: 1.9599685996872835
Experience 3, Iter 4, disc loss: 0.1652001629333475, policy loss: 1.9454126231248516
Experience 3, Iter 5, disc loss: 0.16866427843576076, policy loss: 1.9260522263158004
Experience 3, Iter 6, disc loss: 0.16499830621276998, policy loss: 1.9566321500626214
Experience 3, Iter 7, disc loss: 0.1631565968560451, policy loss: 1.9589613546303988
Experience 3, Iter 8, disc loss: 0.17590289222713978, policy loss: 1.944415376588824
Experience 3, Iter 9, disc loss: 0.18392915428367923, policy loss: 1.9725105772801284
Experience 3, Iter 10, disc loss: 0.17983821881194292, policy loss: 1.9428337359887085
Experience 3, Iter 11, disc loss: 0.24975550218733727, policy loss: 1.97651054715023
Experience 3, Iter 12, disc loss: 0.18976964053680256, policy loss: 1.948928157784948
Experience 3, Iter 13, disc loss: 0.22009635714368503, policy loss: 2.0267242684930844
Experience 3, Iter 14, disc loss: 0.22727742854910365, policy loss: 2.111009289558293
Experience 3, Iter 15, disc loss: 0.1618249447653378, policy loss: 2.253983648377313
Experience 3, Iter 16, disc loss: 0.19105908065067484, policy loss: 2.1844364138949097
Experience 3, Iter 17, disc loss: 0.2749035444053901, policy loss: 2.4011381014866724
Experience 3, Iter 18, disc loss: 0.1699408625421273, policy loss: 2.364741400268672
Experience 3, Iter 19, disc loss: 0.21075957468389317, policy loss: 2.370591132233475
Experience 3, Iter 20, disc loss: 0.2323424546853991, policy loss: 2.5640362384084208
Experience 3, Iter 21, disc loss: 0.2244920347418658, policy loss: 2.4700115897712625
Experience 3, Iter 22, disc loss: 0.14574720225805046, policy loss: 2.7021607273034096
Experience 3, Iter 23, disc loss: 0.38934452225221994, policy loss: 2.522063112714719
Experience 3, Iter 24, disc loss: 0.11181332442797512, policy loss: 2.6123307146387433
Experience 3, Iter 25, disc loss: 0.17946046404947447, policy loss: 2.596065731452844
Experience 3, Iter 26, disc loss: 0.10140709349168547, policy loss: 2.7202553499787823
Experience 3, Iter 27, disc loss: 0.1885801150044505, policy loss: 2.67744655262869
Experience 3, Iter 28, disc loss: 0.08875991168445724, policy loss: 2.735891481287937
Experience 3, Iter 29, disc loss: 0.089157938465631, policy loss: 2.79840154662103
Experience 3, Iter 30, disc loss: 0.08076804458481336, policy loss: 2.854992217424467
Experience 3, Iter 31, disc loss: 0.07691683324699983, policy loss: 2.852945406925036
Experience 3, Iter 32, disc loss: 0.07342404940513736, policy loss: 2.922563776619807
Experience 3, Iter 33, disc loss: 0.07528423368424751, policy loss: 2.947408318643622
Experience 3, Iter 34, disc loss: 0.07066068507736724, policy loss: 2.954239654208278
Experience 3, Iter 35, disc loss: 0.07020497910036824, policy loss: 2.9525421363431716
Experience 3, Iter 36, disc loss: 0.06583534980583498, policy loss: 3.0259109314752797
Experience 3, Iter 37, disc loss: 0.0658592898501941, policy loss: 3.018290232173261
Experience 3, Iter 38, disc loss: 0.06326963182639424, policy loss: 3.0557070545309317
Experience 3, Iter 39, disc loss: 0.06359118702679822, policy loss: 3.042990646221618
Experience 3, Iter 40, disc loss: 0.05999957395995319, policy loss: 3.096967677893283
Experience 3, Iter 41, disc loss: 0.05873015234260317, policy loss: 3.110684857091287
Experience 3, Iter 42, disc loss: 0.05754867119297604, policy loss: 3.1196391768315963
Experience 3, Iter 43, disc loss: 0.05461154278236053, policy loss: 3.1676284599192637
Experience 3, Iter 44, disc loss: 0.053241930975043504, policy loss: 3.183328173833017
Experience 3, Iter 45, disc loss: 0.05238319281646797, policy loss: 3.1846548884727435
Experience 3, Iter 46, disc loss: 0.0527861093727684, policy loss: 3.16442209272191
Experience 3, Iter 47, disc loss: 0.049641531242977543, policy loss: 3.2193953191222793
Experience 3, Iter 48, disc loss: 0.04908604955355423, policy loss: 3.2231988919155725
Experience 3, Iter 49, disc loss: 0.04946480736277048, policy loss: 3.201055106164081
Experience 3, Iter 50, disc loss: 0.048419311076128055, policy loss: 3.2147126013571867
Experience 3, Iter 51, disc loss: 0.04715853329590676, policy loss: 3.2357073116093904
Experience 3, Iter 52, disc loss: 0.046117564103137394, policy loss: 3.2498721530429737
Experience 3, Iter 53, disc loss: 0.045995535452756205, policy loss: 3.2470267600118565
Experience 3, Iter 54, disc loss: 0.04504829102389953, policy loss: 3.26239774844237
Experience 3, Iter 55, disc loss: 0.043680523030123486, policy loss: 3.291211202941201
Experience 3, Iter 56, disc loss: 0.04315800567106828, policy loss: 3.2976583670034287
Experience 3, Iter 57, disc loss: 0.043648016896969836, policy loss: 3.2819672281865757
Experience 3, Iter 58, disc loss: 0.04123934556940599, policy loss: 3.3391398427073895
Experience 3, Iter 59, disc loss: 0.04166692878466901, policy loss: 3.320729754631026
Experience 3, Iter 60, disc loss: 0.04187444868624798, policy loss: 3.3152074306067028
Experience 3, Iter 61, disc loss: 0.0419066973630685, policy loss: 3.30984974479463
Experience 3, Iter 62, disc loss: 0.039462985643811094, policy loss: 3.3698823858578004
Experience 3, Iter 63, disc loss: 0.04005112388020406, policy loss: 3.3520612790956816
Experience 3, Iter 64, disc loss: 0.03977377437921771, policy loss: 3.3598670967560205
Experience 3, Iter 65, disc loss: 0.038547877238384744, policy loss: 3.388850014203811
Experience 3, Iter 66, disc loss: 0.039138558686913565, policy loss: 3.372715394391631
Experience 3, Iter 67, disc loss: 0.039232983552815764, policy loss: 3.3673369470344263
Experience 3, Iter 68, disc loss: 0.03806206658644117, policy loss: 3.400376875781335
Experience 3, Iter 69, disc loss: 0.03734836405075934, policy loss: 3.4143150236224082
Experience 3, Iter 70, disc loss: 0.03840303600947424, policy loss: 3.3879487927580003
Experience 3, Iter 71, disc loss: 0.04060500812265715, policy loss: 3.3301571934664675
Experience 3, Iter 72, disc loss: 0.03879767808283422, policy loss: 3.3861163343737672
Experience 3, Iter 73, disc loss: 0.0402621839393161, policy loss: 3.335899437717238
Experience 3, Iter 74, disc loss: 0.03774761555358139, policy loss: 3.4130545079439942
Experience 3, Iter 75, disc loss: 0.04214217230389451, policy loss: 3.307836830083958
Experience 3, Iter 76, disc loss: 0.04175943379059131, policy loss: 3.335610910002419
Experience 3, Iter 77, disc loss: 0.04497682634939876, policy loss: 3.2897264098313426
Experience 3, Iter 78, disc loss: 0.052426232053153395, policy loss: 3.1359290369243835
Experience 3, Iter 79, disc loss: 0.05754114820728421, policy loss: 3.0920119258685643
Experience 3, Iter 80, disc loss: 0.07162272668937511, policy loss: 2.9082028416355126
Experience 3, Iter 81, disc loss: 0.08720134293901272, policy loss: 2.8663197603626513
Experience 3, Iter 82, disc loss: 0.09503245318430541, policy loss: 2.7580138021235445
Experience 3, Iter 83, disc loss: 0.14236468548149342, policy loss: 2.4753015527824993
Experience 3, Iter 84, disc loss: 0.16287403182994245, policy loss: 2.5742261516485443
Experience 3, Iter 85, disc loss: 0.21744815983787907, policy loss: 2.2182991892741035
Experience 3, Iter 86, disc loss: 0.2092439130480064, policy loss: 2.402067592844996
Experience 3, Iter 87, disc loss: 0.21979976751344305, policy loss: 2.7123612384214555
Experience 3, Iter 88, disc loss: 0.21685153543145852, policy loss: 2.4559620473903068
Experience 3, Iter 89, disc loss: 0.20731943013576914, policy loss: 2.6423085663106427
Experience 3, Iter 90, disc loss: 0.20390954603871583, policy loss: 2.4851654912733427
Experience 3, Iter 91, disc loss: 0.19536036705640591, policy loss: 2.5297403576114483
Experience 3, Iter 92, disc loss: 0.19073286090046015, policy loss: 2.6469328776984824
Experience 3, Iter 93, disc loss: 0.17847509230878505, policy loss: 2.839155189375208
Experience 3, Iter 94, disc loss: 0.16085101551214603, policy loss: 3.1369471128334974
Experience 3, Iter 95, disc loss: 0.17996212680847662, policy loss: 3.0429955922603007
Experience 3, Iter 96, disc loss: 0.21402111464376913, policy loss: 2.954546961766119
Experience 3, Iter 97, disc loss: 0.18774815880393236, policy loss: 3.3700687554050055
Experience 3, Iter 98, disc loss: 0.21057342584032196, policy loss: 3.2714303287834388
Experience 3, Iter 99, disc loss: 0.21632479426339507, policy loss: 2.5468197322786654
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0057],
        [0.0315],
        [0.6511],
        [0.0132]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0447, 0.2161, 0.5959, 0.0183, 0.0045, 1.0698]],

        [[0.0447, 0.2161, 0.5959, 0.0183, 0.0045, 1.0698]],

        [[0.0447, 0.2161, 0.5959, 0.0183, 0.0045, 1.0698]],

        [[0.0447, 0.2161, 0.5959, 0.0183, 0.0045, 1.0698]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0230, 0.1259, 2.6042, 0.0528], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0230, 0.1259, 2.6042, 0.0528])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.055
Iter 2/2000 - Loss: 1.710
Iter 3/2000 - Loss: 1.582
Iter 4/2000 - Loss: 1.581
Iter 5/2000 - Loss: 1.535
Iter 6/2000 - Loss: 1.424
Iter 7/2000 - Loss: 1.322
Iter 8/2000 - Loss: 1.263
Iter 9/2000 - Loss: 1.214
Iter 10/2000 - Loss: 1.127
Iter 11/2000 - Loss: 1.002
Iter 12/2000 - Loss: 0.877
Iter 13/2000 - Loss: 0.782
Iter 14/2000 - Loss: 0.710
Iter 15/2000 - Loss: 0.627
Iter 16/2000 - Loss: 0.510
Iter 17/2000 - Loss: 0.370
Iter 18/2000 - Loss: 0.229
Iter 19/2000 - Loss: 0.103
Iter 20/2000 - Loss: -0.016
Iter 1981/2000 - Loss: -6.002
Iter 1982/2000 - Loss: -6.002
Iter 1983/2000 - Loss: -6.002
Iter 1984/2000 - Loss: -6.002
Iter 1985/2000 - Loss: -6.002
Iter 1986/2000 - Loss: -6.002
Iter 1987/2000 - Loss: -6.002
Iter 1988/2000 - Loss: -6.002
Iter 1989/2000 - Loss: -6.002
Iter 1990/2000 - Loss: -6.002
Iter 1991/2000 - Loss: -6.002
Iter 1992/2000 - Loss: -6.002
Iter 1993/2000 - Loss: -6.002
Iter 1994/2000 - Loss: -6.002
Iter 1995/2000 - Loss: -6.002
Iter 1996/2000 - Loss: -6.002
Iter 1997/2000 - Loss: -6.002
Iter 1998/2000 - Loss: -6.002
Iter 1999/2000 - Loss: -6.002
Iter 2000/2000 - Loss: -6.002
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0018],
        [0.0004]])
Lengthscale: tensor([[[17.1782,  5.5951, 46.4965, 10.6184, 14.5601, 38.6507]],

        [[24.5211, 44.4098,  9.7707,  0.9080,  2.5814, 11.3657]],

        [[26.1205, 38.8069, 12.6044,  0.8300,  2.9212, 16.5093]],

        [[18.1430, 41.6634, 14.0532,  1.8205, 10.4912, 27.9067]]])
Signal Variance: tensor([ 0.0811,  0.5912, 11.1146,  0.3387])
Estimated target variance: tensor([0.0230, 0.1259, 2.6042, 0.0528])
N: 40
Signal to noise ratio: tensor([15.1101, 39.4696, 79.0861, 30.0577])
Bound on condition number: tensor([  9133.5667,  62314.9690, 250185.6362,  36139.6774])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.27963751059355435, policy loss: 1.9865307307940772
Experience 4, Iter 1, disc loss: 0.26208233261061853, policy loss: 2.0173897561723377
Experience 4, Iter 2, disc loss: 0.26597252892103285, policy loss: 1.9220896788235442
Experience 4, Iter 3, disc loss: 0.2717538574603362, policy loss: 1.9140117641674217
Experience 4, Iter 4, disc loss: 0.2869739605874213, policy loss: 2.1945938802792666
Experience 4, Iter 5, disc loss: 0.27508033002251364, policy loss: 2.1268700778279674
Experience 4, Iter 6, disc loss: 0.3020078682218683, policy loss: 1.8710819307197508
Experience 4, Iter 7, disc loss: 0.28273616276996216, policy loss: 2.094148156178564
Experience 4, Iter 8, disc loss: 0.3014209162145131, policy loss: 2.1397606284537076
Experience 4, Iter 9, disc loss: 0.27736442953450424, policy loss: 2.54797717441694
Experience 4, Iter 10, disc loss: 0.26010281289802883, policy loss: 2.631997469287903
Experience 4, Iter 11, disc loss: 0.23395350937839007, policy loss: 3.2453462170026723
Experience 4, Iter 12, disc loss: 0.2540615292582111, policy loss: 3.138676726210145
Experience 4, Iter 13, disc loss: 0.22340067685770576, policy loss: 3.1343414891921526
Experience 4, Iter 14, disc loss: 0.23102799843549246, policy loss: 2.844842038867707
Experience 4, Iter 15, disc loss: 0.18230686664472806, policy loss: 3.1266008864258437
Experience 4, Iter 16, disc loss: 0.1664970419486654, policy loss: 3.141551663175676
Experience 4, Iter 17, disc loss: 0.1469456797022845, policy loss: 3.1013289662905654
Experience 4, Iter 18, disc loss: 0.13555684540969165, policy loss: 3.0281912930297405
Experience 4, Iter 19, disc loss: 0.11074926113614122, policy loss: 3.2570786150495037
Experience 4, Iter 20, disc loss: 0.11153991079706746, policy loss: 3.175427803680342
Experience 4, Iter 21, disc loss: 0.10253439886948996, policy loss: 3.0724775230340797
Experience 4, Iter 22, disc loss: 0.08942392559160149, policy loss: 3.181759712487244
Experience 4, Iter 23, disc loss: 0.08843993713715481, policy loss: 3.2013739480780776
Experience 4, Iter 24, disc loss: 0.09679629544647246, policy loss: 3.1762160378490725
Experience 4, Iter 25, disc loss: 0.08274895161325377, policy loss: 3.4211672575347913
Experience 4, Iter 26, disc loss: 0.08966707773393218, policy loss: 3.3166480326233096
Experience 4, Iter 27, disc loss: 0.09186683904576307, policy loss: 3.189528326069247
Experience 4, Iter 28, disc loss: 0.07778403016716401, policy loss: 3.321803319508307
Experience 4, Iter 29, disc loss: 0.07987970823531809, policy loss: 3.433343405553406
Experience 4, Iter 30, disc loss: 0.08057225445707013, policy loss: 3.468866727936749
Experience 4, Iter 31, disc loss: 0.08164055517414343, policy loss: 3.4815736669486497
Experience 4, Iter 32, disc loss: 0.10393780901540083, policy loss: 3.2883046249185783
Experience 4, Iter 33, disc loss: 0.0918394908110917, policy loss: 3.408475276187753
Experience 4, Iter 34, disc loss: 0.10061073980658158, policy loss: 3.0135407324404153
Experience 4, Iter 35, disc loss: 0.10062234280614753, policy loss: 3.163721477583362
Experience 4, Iter 36, disc loss: 0.09038002006253221, policy loss: 3.6637972368181515
Experience 4, Iter 37, disc loss: 0.08799819589421867, policy loss: 3.389914275272524
Experience 4, Iter 38, disc loss: 0.09960597885505476, policy loss: 3.2365390479080136
Experience 4, Iter 39, disc loss: 0.08860314193097946, policy loss: 3.6052605781167255
Experience 4, Iter 40, disc loss: 0.0817476925497011, policy loss: 3.703773190895455
Experience 4, Iter 41, disc loss: 0.08581799358885507, policy loss: 3.5136125108943084
Experience 4, Iter 42, disc loss: 0.08577357460773125, policy loss: 3.5437463581587565
Experience 4, Iter 43, disc loss: 0.09913753206430403, policy loss: 3.404882017968852
Experience 4, Iter 44, disc loss: 0.08727019856202073, policy loss: 3.618118027239763
Experience 4, Iter 45, disc loss: 0.10017429665876904, policy loss: 3.7074884905608867
Experience 4, Iter 46, disc loss: 0.08656776465413998, policy loss: 3.3889731877112657
Experience 4, Iter 47, disc loss: 0.07591049279333192, policy loss: 3.583604475203132
Experience 4, Iter 48, disc loss: 0.1029936454564184, policy loss: 3.088744981713025
Experience 4, Iter 49, disc loss: 0.07836957886048615, policy loss: 3.450372037631854
Experience 4, Iter 50, disc loss: 0.08371619697808463, policy loss: 3.2872173432376455
Experience 4, Iter 51, disc loss: 0.07715252989413901, policy loss: 3.5493919147368
Experience 4, Iter 52, disc loss: 0.07110860276909692, policy loss: 3.532722741532932
Experience 4, Iter 53, disc loss: 0.0807212543350238, policy loss: 3.4089631698430565
Experience 4, Iter 54, disc loss: 0.07933434987195971, policy loss: 3.192062185396121
Experience 4, Iter 55, disc loss: 0.07138047925789169, policy loss: 3.3326639949005656
Experience 4, Iter 56, disc loss: 0.08536876650214931, policy loss: 3.347774983220525
Experience 4, Iter 57, disc loss: 0.07139916845745345, policy loss: 3.425711877956734
Experience 4, Iter 58, disc loss: 0.07217048627410277, policy loss: 3.4979558373229795
Experience 4, Iter 59, disc loss: 0.07155712060216447, policy loss: 3.420825791573206
Experience 4, Iter 60, disc loss: 0.06911939304809434, policy loss: 3.439357486226596
Experience 4, Iter 61, disc loss: 0.0735788558868844, policy loss: 3.5400763623632576
Experience 4, Iter 62, disc loss: 0.08531613880929788, policy loss: 3.259231663766089
Experience 4, Iter 63, disc loss: 0.07364754463668369, policy loss: 3.4134783501348362
Experience 4, Iter 64, disc loss: 0.08436971439775715, policy loss: 3.3715751973979167
Experience 4, Iter 65, disc loss: 0.06278388961514948, policy loss: 3.669485853236954
Experience 4, Iter 66, disc loss: 0.059391423902378525, policy loss: 3.6435937629228166
Experience 4, Iter 67, disc loss: 0.06117247953768072, policy loss: 3.6729876952586524
Experience 4, Iter 68, disc loss: 0.060437150516331695, policy loss: 3.5521087232878106
Experience 4, Iter 69, disc loss: 0.058845937142388026, policy loss: 3.587571517680666
Experience 4, Iter 70, disc loss: 0.056131134398665475, policy loss: 3.6434260505026117
Experience 4, Iter 71, disc loss: 0.05660389315427461, policy loss: 3.540374471666803
Experience 4, Iter 72, disc loss: 0.05603869385078721, policy loss: 3.7569704040785785
Experience 4, Iter 73, disc loss: 0.060565020182575174, policy loss: 3.534705087863539
Experience 4, Iter 74, disc loss: 0.05798991269727993, policy loss: 3.6266989755539236
Experience 4, Iter 75, disc loss: 0.06219922126828743, policy loss: 3.3511417342346332
Experience 4, Iter 76, disc loss: 0.06068584441977126, policy loss: 3.5686916635287913
Experience 4, Iter 77, disc loss: 0.059816104922905455, policy loss: 3.4557468020953808
Experience 4, Iter 78, disc loss: 0.05847123161006519, policy loss: 3.5324340478831164
Experience 4, Iter 79, disc loss: 0.05185757378388943, policy loss: 3.703027308944082
Experience 4, Iter 80, disc loss: 0.05162437470679219, policy loss: 3.6166732466833458
Experience 4, Iter 81, disc loss: 0.05424423602565513, policy loss: 3.7457778848695766
Experience 4, Iter 82, disc loss: 0.05512769982152407, policy loss: 3.489952299271355
Experience 4, Iter 83, disc loss: 0.05152747091454614, policy loss: 3.582285745311226
Experience 4, Iter 84, disc loss: 0.04412378218082384, policy loss: 3.918645201025232
Experience 4, Iter 85, disc loss: 0.04838106598603831, policy loss: 3.796952960767532
Experience 4, Iter 86, disc loss: 0.04993672815355482, policy loss: 3.6872655086221355
Experience 4, Iter 87, disc loss: 0.044304969239237754, policy loss: 3.8285489059149045
Experience 4, Iter 88, disc loss: 0.04836893926482455, policy loss: 3.7038417329887876
Experience 4, Iter 89, disc loss: 0.044858186168910144, policy loss: 3.9576416759271202
Experience 4, Iter 90, disc loss: 0.04194369505269108, policy loss: 3.862698176441578
Experience 4, Iter 91, disc loss: 0.042526693157585456, policy loss: 3.912913018408625
Experience 4, Iter 92, disc loss: 0.04666761045158782, policy loss: 3.5977517510813284
Experience 4, Iter 93, disc loss: 0.03977421515104031, policy loss: 3.8539495193502606
Experience 4, Iter 94, disc loss: 0.03735156636301761, policy loss: 4.068576549129862
Experience 4, Iter 95, disc loss: 0.03800511069258347, policy loss: 3.9317229649910503
Experience 4, Iter 96, disc loss: 0.042013213111790994, policy loss: 3.814310093455072
Experience 4, Iter 97, disc loss: 0.03384558101673796, policy loss: 4.228031517906775
Experience 4, Iter 98, disc loss: 0.0420481058285435, policy loss: 3.789499346486636
Experience 4, Iter 99, disc loss: 0.0379456600059026, policy loss: 3.896186760862874
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.0338],
        [0.6215],
        [0.0131]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0429, 0.2113, 0.5920, 0.0202, 0.0042, 1.2550]],

        [[0.0429, 0.2113, 0.5920, 0.0202, 0.0042, 1.2550]],

        [[0.0429, 0.2113, 0.5920, 0.0202, 0.0042, 1.2550]],

        [[0.0429, 0.2113, 0.5920, 0.0202, 0.0042, 1.2550]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0221, 0.1353, 2.4861, 0.0524], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0221, 0.1353, 2.4861, 0.0524])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.962
Iter 2/2000 - Loss: 1.693
Iter 3/2000 - Loss: 1.662
Iter 4/2000 - Loss: 1.603
Iter 5/2000 - Loss: 1.533
Iter 6/2000 - Loss: 1.476
Iter 7/2000 - Loss: 1.445
Iter 8/2000 - Loss: 1.405
Iter 9/2000 - Loss: 1.317
Iter 10/2000 - Loss: 1.209
Iter 11/2000 - Loss: 1.124
Iter 12/2000 - Loss: 1.069
Iter 13/2000 - Loss: 1.007
Iter 14/2000 - Loss: 0.914
Iter 15/2000 - Loss: 0.799
Iter 16/2000 - Loss: 0.679
Iter 17/2000 - Loss: 0.565
Iter 18/2000 - Loss: 0.450
Iter 19/2000 - Loss: 0.325
Iter 20/2000 - Loss: 0.186
Iter 1981/2000 - Loss: -6.154
Iter 1982/2000 - Loss: -6.154
Iter 1983/2000 - Loss: -6.154
Iter 1984/2000 - Loss: -6.154
Iter 1985/2000 - Loss: -6.154
Iter 1986/2000 - Loss: -6.154
Iter 1987/2000 - Loss: -6.154
Iter 1988/2000 - Loss: -6.154
Iter 1989/2000 - Loss: -6.154
Iter 1990/2000 - Loss: -6.154
Iter 1991/2000 - Loss: -6.154
Iter 1992/2000 - Loss: -6.154
Iter 1993/2000 - Loss: -6.154
Iter 1994/2000 - Loss: -6.154
Iter 1995/2000 - Loss: -6.154
Iter 1996/2000 - Loss: -6.154
Iter 1997/2000 - Loss: -6.154
Iter 1998/2000 - Loss: -6.154
Iter 1999/2000 - Loss: -6.154
Iter 2000/2000 - Loss: -6.154
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[18.6397,  6.2051, 47.7364, 13.7349, 12.0863, 37.4286]],

        [[29.2899, 49.1329,  9.3502,  1.6377,  1.4069, 18.2219]],

        [[26.4200, 42.7206, 14.4795,  0.8505,  7.2530, 19.5540]],

        [[23.0376, 40.6386, 13.5907,  1.5005,  8.3344, 29.0898]]])
Signal Variance: tensor([ 0.0832,  1.2948, 15.0914,  0.2690])
Estimated target variance: tensor([0.0221, 0.1353, 2.4861, 0.0524])
N: 50
Signal to noise ratio: tensor([15.3147, 51.4064, 76.5581, 25.9149])
Bound on condition number: tensor([ 11728.0653, 132131.9864, 293058.1856,  33580.2237])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.04014681351317419, policy loss: 3.683621267341426
Experience 5, Iter 1, disc loss: 0.040352671655799865, policy loss: 3.6617962645480397
Experience 5, Iter 2, disc loss: 0.04162777671704778, policy loss: 3.605388704624989
Experience 5, Iter 3, disc loss: 0.04214950709496588, policy loss: 3.60093110868472
Experience 5, Iter 4, disc loss: 0.04153612504948491, policy loss: 3.6187786346805146
Experience 5, Iter 5, disc loss: 0.04094672367547402, policy loss: 3.725073235903025
Experience 5, Iter 6, disc loss: 0.03789376238309852, policy loss: 3.8172430806437885
Experience 5, Iter 7, disc loss: 0.03917429974133049, policy loss: 3.7501313911626957
Experience 5, Iter 8, disc loss: 0.036864415361164624, policy loss: 3.8161809556606454
Experience 5, Iter 9, disc loss: 0.037287538719800666, policy loss: 3.8409232440629215
Experience 5, Iter 10, disc loss: 0.03597600835697631, policy loss: 3.8297239628998216
Experience 5, Iter 11, disc loss: 0.03562637197805249, policy loss: 3.843614095161099
Experience 5, Iter 12, disc loss: 0.03342170171146569, policy loss: 3.9559301645211726
Experience 5, Iter 13, disc loss: 0.03350534604428994, policy loss: 3.9273319964382694
Experience 5, Iter 14, disc loss: 0.03415344420872868, policy loss: 3.9163746503102432
Experience 5, Iter 15, disc loss: 0.03311198563207332, policy loss: 3.9847440167977624
Experience 5, Iter 16, disc loss: 0.031780446803217385, policy loss: 4.0319281357973935
Experience 5, Iter 17, disc loss: 0.034301397768599716, policy loss: 3.892090571708092
Experience 5, Iter 18, disc loss: 0.03294857169324154, policy loss: 3.955891196676805
Experience 5, Iter 19, disc loss: 0.03308168498500752, policy loss: 3.9538282879553135
Experience 5, Iter 20, disc loss: 0.034156222997710206, policy loss: 3.912406363478909
Experience 5, Iter 21, disc loss: 0.031813147588270606, policy loss: 4.07457239242522
Experience 5, Iter 22, disc loss: 0.03267378502231987, policy loss: 4.0810821055783855
Experience 5, Iter 23, disc loss: 0.031405419219865144, policy loss: 4.113706397945189
Experience 5, Iter 24, disc loss: 0.0340501431136264, policy loss: 3.9469152393534195
Experience 5, Iter 25, disc loss: 0.03468890938418337, policy loss: 3.8861068091981084
Experience 5, Iter 26, disc loss: 0.029825934926931363, policy loss: 4.191585189741901
Experience 5, Iter 27, disc loss: 0.031658060102734774, policy loss: 4.162159593643184
Experience 5, Iter 28, disc loss: 0.032820901948065195, policy loss: 4.017610165366795
Experience 5, Iter 29, disc loss: 0.032616771843977634, policy loss: 4.075032666315362
Experience 5, Iter 30, disc loss: 0.032466239156347415, policy loss: 4.09556235148907
Experience 5, Iter 31, disc loss: 0.03393359155498817, policy loss: 4.302329993483868
Experience 5, Iter 32, disc loss: 0.03450242649735017, policy loss: 4.083533037662557
Experience 5, Iter 33, disc loss: 0.03566163896356698, policy loss: 4.039844641116028
Experience 5, Iter 34, disc loss: 0.03403496917959013, policy loss: 4.175844494129975
Experience 5, Iter 35, disc loss: 0.03579609413721819, policy loss: 4.069260456138209
Experience 5, Iter 36, disc loss: 0.03883732948340507, policy loss: 4.001389394659681
Experience 5, Iter 37, disc loss: 0.03489487950524747, policy loss: 4.1829578760520105
Experience 5, Iter 38, disc loss: 0.033162525108464916, policy loss: 4.299885390553218
Experience 5, Iter 39, disc loss: 0.03681435934331646, policy loss: 4.187464011209342
Experience 5, Iter 40, disc loss: 0.03344889127535848, policy loss: 4.364579646374524
Experience 5, Iter 41, disc loss: 0.03482890046425053, policy loss: 4.311613072953254
Experience 5, Iter 42, disc loss: 0.02847960302080653, policy loss: 4.706998973917061
Experience 5, Iter 43, disc loss: 0.029759565463687784, policy loss: 4.493724183825167
Experience 5, Iter 44, disc loss: 0.03228180099202297, policy loss: 4.361340936035344
Experience 5, Iter 45, disc loss: 0.03062976724844813, policy loss: 4.4755134092897695
Experience 5, Iter 46, disc loss: 0.03056034461501319, policy loss: 4.571775634674829
Experience 5, Iter 47, disc loss: 0.03450499659623756, policy loss: 4.340038717871988
Experience 5, Iter 48, disc loss: 0.03224570826850642, policy loss: 4.351721631151673
Experience 5, Iter 49, disc loss: 0.028403747204874437, policy loss: 4.472794923712858
Experience 5, Iter 50, disc loss: 0.02947556738128637, policy loss: 4.6620882264602646
Experience 5, Iter 51, disc loss: 0.026919765095975344, policy loss: 4.6034974016395
Experience 5, Iter 52, disc loss: 0.02910760810090642, policy loss: 4.282303398691952
Experience 5, Iter 53, disc loss: 0.03115883283296121, policy loss: 4.181206621892332
Experience 5, Iter 54, disc loss: 0.03002296657736324, policy loss: 4.264696947577611
Experience 5, Iter 55, disc loss: 0.028328065667882863, policy loss: 4.374574845177736
Experience 5, Iter 56, disc loss: 0.030041681356030074, policy loss: 4.314970177387893
Experience 5, Iter 57, disc loss: 0.025562162891352824, policy loss: 4.5488493168577095
Experience 5, Iter 58, disc loss: 0.025754675135608602, policy loss: 4.529450392743961
Experience 5, Iter 59, disc loss: 0.028176311460548838, policy loss: 4.333034358748412
Experience 5, Iter 60, disc loss: 0.02688565666817181, policy loss: 4.417070328482787
Experience 5, Iter 61, disc loss: 0.026434732766979895, policy loss: 4.404635649764907
Experience 5, Iter 62, disc loss: 0.02447862218617968, policy loss: 4.640809037007946
Experience 5, Iter 63, disc loss: 0.03475784032719677, policy loss: 4.24129086094495
Experience 5, Iter 64, disc loss: 0.025545377084778206, policy loss: 4.543295244468273
Experience 5, Iter 65, disc loss: 0.022628702244254728, policy loss: 4.764717071381147
Experience 5, Iter 66, disc loss: 0.02447532543571213, policy loss: 4.7388609962159345
Experience 5, Iter 67, disc loss: 0.02772002649149466, policy loss: 4.4088731776672985
Experience 5, Iter 68, disc loss: 0.02174790093372478, policy loss: 4.906657160590132
Experience 5, Iter 69, disc loss: 0.02352067539428556, policy loss: 4.552466408787559
Experience 5, Iter 70, disc loss: 0.02117421434095798, policy loss: 4.833404940725025
Experience 5, Iter 71, disc loss: 0.022408937450245463, policy loss: 4.676423754862707
Experience 5, Iter 72, disc loss: 0.022467318119181966, policy loss: 4.777983514090936
Experience 5, Iter 73, disc loss: 0.021179046532942017, policy loss: 4.863273838377669
Experience 5, Iter 74, disc loss: 0.02730071328099158, policy loss: 4.483123243678493
Experience 5, Iter 75, disc loss: 0.02252875072450812, policy loss: 4.765585778574765
Experience 5, Iter 76, disc loss: 0.023228843207537698, policy loss: 4.851744178048648
Experience 5, Iter 77, disc loss: 0.024432849870828863, policy loss: 4.610540601407093
Experience 5, Iter 78, disc loss: 0.022005409304007097, policy loss: 4.751348489675799
Experience 5, Iter 79, disc loss: 0.02065142233958269, policy loss: 4.814233220739176
Experience 5, Iter 80, disc loss: 0.022868580126828273, policy loss: 4.759570095211445
Experience 5, Iter 81, disc loss: 0.019991909085451016, policy loss: 4.774616813745045
Experience 5, Iter 82, disc loss: 0.020752285178917113, policy loss: 5.021577502557611
Experience 5, Iter 83, disc loss: 0.02084463489233853, policy loss: 5.05373389143665
Experience 5, Iter 84, disc loss: 0.02035578640211314, policy loss: 4.858665517728628
Experience 5, Iter 85, disc loss: 0.02031077816776425, policy loss: 4.903557804155376
Experience 5, Iter 86, disc loss: 0.022338173595884497, policy loss: 4.903179505918923
Experience 5, Iter 87, disc loss: 0.022294694778347324, policy loss: 4.678678406418663
Experience 5, Iter 88, disc loss: 0.0188044384481662, policy loss: 4.956680551805207
Experience 5, Iter 89, disc loss: 0.019262432414868615, policy loss: 4.8113082021665985
Experience 5, Iter 90, disc loss: 0.020858572208998408, policy loss: 4.776889111309236
Experience 5, Iter 91, disc loss: 0.02029808149252131, policy loss: 4.82209714792506
Experience 5, Iter 92, disc loss: 0.02135944803336602, policy loss: 4.77286209180234
Experience 5, Iter 93, disc loss: 0.018676880169430754, policy loss: 5.023267866084888
Experience 5, Iter 94, disc loss: 0.018181443713700493, policy loss: 5.01519599872262
Experience 5, Iter 95, disc loss: 0.019360620652813992, policy loss: 4.8396465222998195
Experience 5, Iter 96, disc loss: 0.01916275876618989, policy loss: 5.115114654888488
Experience 5, Iter 97, disc loss: 0.01970872039375829, policy loss: 4.944079816286086
Experience 5, Iter 98, disc loss: 0.019335722258073555, policy loss: 5.067006223600579
Experience 5, Iter 99, disc loss: 0.01998936478406139, policy loss: 4.732242901321065
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0054],
        [0.0484],
        [0.9192],
        [0.0186]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0375, 0.2078, 0.8446, 0.0243, 0.0088, 1.4821]],

        [[0.0375, 0.2078, 0.8446, 0.0243, 0.0088, 1.4821]],

        [[0.0375, 0.2078, 0.8446, 0.0243, 0.0088, 1.4821]],

        [[0.0375, 0.2078, 0.8446, 0.0243, 0.0088, 1.4821]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0216, 0.1936, 3.6767, 0.0743], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0216, 0.1936, 3.6767, 0.0743])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.455
Iter 2/2000 - Loss: 2.225
Iter 3/2000 - Loss: 2.238
Iter 4/2000 - Loss: 2.148
Iter 5/2000 - Loss: 2.094
Iter 6/2000 - Loss: 2.103
Iter 7/2000 - Loss: 2.079
Iter 8/2000 - Loss: 1.994
Iter 9/2000 - Loss: 1.907
Iter 10/2000 - Loss: 1.850
Iter 11/2000 - Loss: 1.806
Iter 12/2000 - Loss: 1.744
Iter 13/2000 - Loss: 1.658
Iter 14/2000 - Loss: 1.561
Iter 15/2000 - Loss: 1.462
Iter 16/2000 - Loss: 1.359
Iter 17/2000 - Loss: 1.241
Iter 18/2000 - Loss: 1.104
Iter 19/2000 - Loss: 0.959
Iter 20/2000 - Loss: 0.812
Iter 1981/2000 - Loss: -5.947
Iter 1982/2000 - Loss: -5.947
Iter 1983/2000 - Loss: -5.947
Iter 1984/2000 - Loss: -5.947
Iter 1985/2000 - Loss: -5.948
Iter 1986/2000 - Loss: -5.948
Iter 1987/2000 - Loss: -5.948
Iter 1988/2000 - Loss: -5.948
Iter 1989/2000 - Loss: -5.948
Iter 1990/2000 - Loss: -5.948
Iter 1991/2000 - Loss: -5.948
Iter 1992/2000 - Loss: -5.948
Iter 1993/2000 - Loss: -5.948
Iter 1994/2000 - Loss: -5.948
Iter 1995/2000 - Loss: -5.948
Iter 1996/2000 - Loss: -5.948
Iter 1997/2000 - Loss: -5.948
Iter 1998/2000 - Loss: -5.948
Iter 1999/2000 - Loss: -5.948
Iter 2000/2000 - Loss: -5.948
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[18.2407,  6.3864, 20.6936, 18.5491,  2.7913, 43.1914]],

        [[28.2447, 49.1295, 10.0014,  1.1857,  5.7747, 22.8068]],

        [[26.7785, 45.7677, 14.4410,  0.9139,  2.2703, 19.7944]],

        [[25.7182, 45.1804, 19.6290,  1.8429,  2.6721, 46.4032]]])
Signal Variance: tensor([ 0.0802,  1.7172, 14.5458,  0.7065])
Estimated target variance: tensor([0.0216, 0.1936, 3.6767, 0.0743])
N: 60
Signal to noise ratio: tensor([14.7233, 60.8278, 76.8469, 40.4413])
Bound on condition number: tensor([ 13007.6067, 222002.3747, 354327.7532,  98130.7268])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.02001315843783489, policy loss: 4.569735574220024
Experience 6, Iter 1, disc loss: 0.023009335991426162, policy loss: 4.458759746231198
Experience 6, Iter 2, disc loss: 0.025429863644511382, policy loss: 4.356937299637957
Experience 6, Iter 3, disc loss: 0.02427014144008126, policy loss: 4.4071770571229045
Experience 6, Iter 4, disc loss: 0.02312343097731193, policy loss: 4.472967630534899
Experience 6, Iter 5, disc loss: 0.02256055542545627, policy loss: 4.795572363068774
Experience 6, Iter 6, disc loss: 0.024556311609306204, policy loss: 4.400016596447912
Experience 6, Iter 7, disc loss: 0.026905648108151374, policy loss: 4.332027344985093
Experience 6, Iter 8, disc loss: 0.024069985113125116, policy loss: 4.472429889690236
Experience 6, Iter 9, disc loss: 0.024534599351463078, policy loss: 4.505210322350841
Experience 6, Iter 10, disc loss: 0.02328503389007267, policy loss: 4.626823607624704
Experience 6, Iter 11, disc loss: 0.022710550120533593, policy loss: 4.6977358912044895
Experience 6, Iter 12, disc loss: 0.023750341817072453, policy loss: 4.62500489665843
Experience 6, Iter 13, disc loss: 0.025348819577863037, policy loss: 4.540505595803408
Experience 6, Iter 14, disc loss: 0.026673469483118405, policy loss: 4.712791915865791
Experience 6, Iter 15, disc loss: 0.025521549491111666, policy loss: 4.602432639062583
Experience 6, Iter 16, disc loss: 0.025128743816779542, policy loss: 4.670012639626393
Experience 6, Iter 17, disc loss: 0.02856343695985683, policy loss: 4.555184447797889
Experience 6, Iter 18, disc loss: 0.025520967745575787, policy loss: 4.765880688931792
Experience 6, Iter 19, disc loss: 0.03376345431198876, policy loss: 4.64139503215285
Experience 6, Iter 20, disc loss: 0.030139217665768908, policy loss: 4.478742163775962
Experience 6, Iter 21, disc loss: 0.02820662145035525, policy loss: 4.542488803089502
Experience 6, Iter 22, disc loss: 0.03326608463595708, policy loss: 4.320783530960785
Experience 6, Iter 23, disc loss: 0.03654164235419975, policy loss: 4.264850717459675
Experience 6, Iter 24, disc loss: 0.03256657675676159, policy loss: 4.558263274699008
Experience 6, Iter 25, disc loss: 0.03588769936037828, policy loss: 4.57304391667948
Experience 6, Iter 26, disc loss: 0.03140235184151635, policy loss: 4.7390049396226175
Experience 6, Iter 27, disc loss: 0.033175338160498655, policy loss: 4.633270160757076
Experience 6, Iter 28, disc loss: 0.03773177684796114, policy loss: 4.633503552209974
Experience 6, Iter 29, disc loss: 0.03379941054465693, policy loss: 4.918600448931233
Experience 6, Iter 30, disc loss: 0.03628488146617681, policy loss: 4.419225951191898
Experience 6, Iter 31, disc loss: 0.03859164401255048, policy loss: 4.5607727097876465
Experience 6, Iter 32, disc loss: 0.043379775924804326, policy loss: 4.437866766414221
Experience 6, Iter 33, disc loss: 0.03490202667482952, policy loss: 4.666424618759074
Experience 6, Iter 34, disc loss: 0.03804552500431206, policy loss: 4.734926711924676
Experience 6, Iter 35, disc loss: 0.033462538079084655, policy loss: 4.966113809388136
Experience 6, Iter 36, disc loss: 0.0434562193141091, policy loss: 4.435069142910854
Experience 6, Iter 37, disc loss: 0.04453637898008328, policy loss: 4.652701932304355
Experience 6, Iter 38, disc loss: 0.04114298980673213, policy loss: 4.709488282498386
Experience 6, Iter 39, disc loss: 0.042407125437210605, policy loss: 4.66544134727457
Experience 6, Iter 40, disc loss: 0.04134614396643493, policy loss: 4.587054235547432
Experience 6, Iter 41, disc loss: 0.04419125848991624, policy loss: 4.469687389808656
Experience 6, Iter 42, disc loss: 0.052550149546260576, policy loss: 4.480038843910033
Experience 6, Iter 43, disc loss: 0.04151745079220079, policy loss: 4.861217410985113
Experience 6, Iter 44, disc loss: 0.04917200707998691, policy loss: 4.998240734887872
Experience 6, Iter 45, disc loss: 0.048538981440840206, policy loss: 4.490615817341181
Experience 6, Iter 46, disc loss: 0.05496923011175696, policy loss: 4.386045122927975
Experience 6, Iter 47, disc loss: 0.04626937911159135, policy loss: 4.635083898339642
Experience 6, Iter 48, disc loss: 0.041659260971997764, policy loss: 4.78701292065959
Experience 6, Iter 49, disc loss: 0.04984061980589751, policy loss: 4.484718919358983
Experience 6, Iter 50, disc loss: 0.045590439935065966, policy loss: 4.642318061563334
Experience 6, Iter 51, disc loss: 0.050316796579812084, policy loss: 4.473776658270163
Experience 6, Iter 52, disc loss: 0.04441885066149611, policy loss: 4.662266914779761
Experience 6, Iter 53, disc loss: 0.05488225122804101, policy loss: 4.748368551892796
Experience 6, Iter 54, disc loss: 0.04957502061696008, policy loss: 4.818640073486879
Experience 6, Iter 55, disc loss: 0.05261588343573778, policy loss: 4.3504391945078025
Experience 6, Iter 56, disc loss: 0.04748110826408401, policy loss: 4.792384330715006
Experience 6, Iter 57, disc loss: 0.052512511042108245, policy loss: 4.625864661212482
Experience 6, Iter 58, disc loss: 0.04228272583749956, policy loss: 5.246332563205854
Experience 6, Iter 59, disc loss: 0.046970041796302806, policy loss: 4.917637216415697
Experience 6, Iter 60, disc loss: 0.04006279288341977, policy loss: 5.182015375676631
Experience 6, Iter 61, disc loss: 0.051937023888386405, policy loss: 4.722214183622514
Experience 6, Iter 62, disc loss: 0.04449717344546217, policy loss: 4.787638213987252
Experience 6, Iter 63, disc loss: 0.05269400652675023, policy loss: 4.3380008128520195
Experience 6, Iter 64, disc loss: 0.04155178663489342, policy loss: 4.690767473494272
Experience 6, Iter 65, disc loss: 0.040211194758245944, policy loss: 5.468484074209461
Experience 6, Iter 66, disc loss: 0.03940306235243564, policy loss: 5.106269797995484
Experience 6, Iter 67, disc loss: 0.04339569950102293, policy loss: 4.71838673409804
Experience 6, Iter 68, disc loss: 0.05291546860016745, policy loss: 4.652418833127425
Experience 6, Iter 69, disc loss: 0.04087189510953632, policy loss: 4.844908983020404
Experience 6, Iter 70, disc loss: 0.04794811439744724, policy loss: 4.961219839702111
Experience 6, Iter 71, disc loss: 0.03837878053390365, policy loss: 4.836646485122201
Experience 6, Iter 72, disc loss: 0.042905010491512754, policy loss: 5.157125100290841
Experience 6, Iter 73, disc loss: 0.04309403987142992, policy loss: 5.273262939674332
Experience 6, Iter 74, disc loss: 0.04147997709291872, policy loss: 4.917321550397654
Experience 6, Iter 75, disc loss: 0.04550962865667563, policy loss: 5.239621060657724
Experience 6, Iter 76, disc loss: 0.05419039647923303, policy loss: 5.376783314984722
Experience 6, Iter 77, disc loss: 0.03897850164192372, policy loss: 4.883415344834565
Experience 6, Iter 78, disc loss: 0.04446698713273536, policy loss: 4.730926659504262
Experience 6, Iter 79, disc loss: 0.041576696231459265, policy loss: 5.209162603266333
Experience 6, Iter 80, disc loss: 0.05158663871781198, policy loss: 5.086512242119978
Experience 6, Iter 81, disc loss: 0.048356371795426185, policy loss: 4.992935820363035
Experience 6, Iter 82, disc loss: 0.04775768468484837, policy loss: 4.945495898932365
Experience 6, Iter 83, disc loss: 0.036627673165934634, policy loss: 5.744609602763464
Experience 6, Iter 84, disc loss: 0.0453435411684769, policy loss: 4.967568843207347
Experience 6, Iter 85, disc loss: 0.05036989604796174, policy loss: 5.525098668765761
Experience 6, Iter 86, disc loss: 0.02968555366498441, policy loss: 5.650548675210856
Experience 6, Iter 87, disc loss: 0.03584909633962715, policy loss: 5.314410685768118
Experience 6, Iter 88, disc loss: 0.032026219343672016, policy loss: 5.172698716722609
Experience 6, Iter 89, disc loss: 0.029989733255116863, policy loss: 5.575318277407549
Experience 6, Iter 90, disc loss: 0.03314349911785896, policy loss: 5.250738571756422
Experience 6, Iter 91, disc loss: 0.02844098966390241, policy loss: 5.348334775324643
Experience 6, Iter 92, disc loss: 0.025051339483191853, policy loss: 5.168981526276069
Experience 6, Iter 93, disc loss: 0.03175395553483282, policy loss: 5.206780974752746
Experience 6, Iter 94, disc loss: 0.026653172582364035, policy loss: 5.321556396585827
Experience 6, Iter 95, disc loss: 0.02993519173722025, policy loss: 5.13074765731016
Experience 6, Iter 96, disc loss: 0.02951006838462407, policy loss: 4.97159802710034
Experience 6, Iter 97, disc loss: 0.033629673965948424, policy loss: 4.92046887729823
Experience 6, Iter 98, disc loss: 0.05706846958794113, policy loss: 4.538793415654495
Experience 6, Iter 99, disc loss: 0.030902858828969995, policy loss: 5.4006471001846545
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.0851],
        [1.2076],
        [0.0238]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0333, 0.2068, 1.0982, 0.0279, 0.0106, 2.2055]],

        [[0.0333, 0.2068, 1.0982, 0.0279, 0.0106, 2.2055]],

        [[0.0333, 0.2068, 1.0982, 0.0279, 0.0106, 2.2055]],

        [[0.0333, 0.2068, 1.0982, 0.0279, 0.0106, 2.2055]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0211, 0.3403, 4.8304, 0.0951], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0211, 0.3403, 4.8304, 0.0951])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.919
Iter 2/2000 - Loss: 2.775
Iter 3/2000 - Loss: 2.781
Iter 4/2000 - Loss: 2.709
Iter 5/2000 - Loss: 2.679
Iter 6/2000 - Loss: 2.669
Iter 7/2000 - Loss: 2.616
Iter 8/2000 - Loss: 2.546
Iter 9/2000 - Loss: 2.491
Iter 10/2000 - Loss: 2.442
Iter 11/2000 - Loss: 2.378
Iter 12/2000 - Loss: 2.300
Iter 13/2000 - Loss: 2.215
Iter 14/2000 - Loss: 2.127
Iter 15/2000 - Loss: 2.028
Iter 16/2000 - Loss: 1.913
Iter 17/2000 - Loss: 1.786
Iter 18/2000 - Loss: 1.654
Iter 19/2000 - Loss: 1.518
Iter 20/2000 - Loss: 1.369
Iter 1981/2000 - Loss: -5.729
Iter 1982/2000 - Loss: -5.729
Iter 1983/2000 - Loss: -5.729
Iter 1984/2000 - Loss: -5.729
Iter 1985/2000 - Loss: -5.729
Iter 1986/2000 - Loss: -5.729
Iter 1987/2000 - Loss: -5.729
Iter 1988/2000 - Loss: -5.729
Iter 1989/2000 - Loss: -5.729
Iter 1990/2000 - Loss: -5.729
Iter 1991/2000 - Loss: -5.729
Iter 1992/2000 - Loss: -5.729
Iter 1993/2000 - Loss: -5.729
Iter 1994/2000 - Loss: -5.729
Iter 1995/2000 - Loss: -5.729
Iter 1996/2000 - Loss: -5.729
Iter 1997/2000 - Loss: -5.730
Iter 1998/2000 - Loss: -5.730
Iter 1999/2000 - Loss: -5.730
Iter 2000/2000 - Loss: -5.730
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0028],
        [0.0005]])
Lengthscale: tensor([[[17.2183,  6.7477, 34.3839,  9.4812, 17.7300, 42.8063]],

        [[26.1695, 40.4705,  7.4087,  1.3667,  5.6387, 22.9125]],

        [[26.7447, 40.2159,  9.7877,  1.1747,  1.3603, 23.4575]],

        [[25.3491, 45.1819, 19.2116,  2.2132,  2.1424, 44.7124]]])
Signal Variance: tensor([ 0.0863,  1.7668, 17.4441,  0.6413])
Estimated target variance: tensor([0.0211, 0.3403, 4.8304, 0.0951])
N: 70
Signal to noise ratio: tensor([15.1198, 65.3472, 79.1659, 37.3110])
Bound on condition number: tensor([ 16003.4920, 298919.0895, 438707.9976,  97448.9952])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.022250022845498183, policy loss: 5.6485293716596425
Experience 7, Iter 1, disc loss: 0.0357802683633553, policy loss: 4.699994102163617
Experience 7, Iter 2, disc loss: 0.04037677688277136, policy loss: 4.804348371800529
Experience 7, Iter 3, disc loss: 0.05867505276860746, policy loss: 4.745770569826658
Experience 7, Iter 4, disc loss: 0.05135971481972634, policy loss: 4.672669700913037
Experience 7, Iter 5, disc loss: 0.04986231079679475, policy loss: 4.603720151106028
Experience 7, Iter 6, disc loss: 0.042438635988783543, policy loss: 5.120962171169635
Experience 7, Iter 7, disc loss: 0.054987108094380566, policy loss: 4.6528250538085665
Experience 7, Iter 8, disc loss: 0.04682388625482993, policy loss: 4.831205194066339
Experience 7, Iter 9, disc loss: 0.05784091114434784, policy loss: 4.196340595828708
Experience 7, Iter 10, disc loss: 0.0546124701716585, policy loss: 4.875215700561769
Experience 7, Iter 11, disc loss: 0.05217093926626491, policy loss: 4.5763679340037235
Experience 7, Iter 12, disc loss: 0.05105364056851157, policy loss: 4.502127631771587
Experience 7, Iter 13, disc loss: 0.057564677970105026, policy loss: 4.760679800236186
Experience 7, Iter 14, disc loss: 0.05904742537787345, policy loss: 4.531391176973453
Experience 7, Iter 15, disc loss: 0.07568603246752165, policy loss: 4.5163781872985815
Experience 7, Iter 16, disc loss: 0.06491019782735247, policy loss: 4.665545670858874
Experience 7, Iter 17, disc loss: 0.05900811100258861, policy loss: 4.688606401509743
Experience 7, Iter 18, disc loss: 0.07024391990325876, policy loss: 4.663667585334583
Experience 7, Iter 19, disc loss: 0.059136326495465005, policy loss: 5.552200463247752
Experience 7, Iter 20, disc loss: 0.07232618014010334, policy loss: 4.732890324550786
Experience 7, Iter 21, disc loss: 0.08300213671735193, policy loss: 4.491546915030851
Experience 7, Iter 22, disc loss: 0.057860771346785245, policy loss: 4.789995189649154
Experience 7, Iter 23, disc loss: 0.0552718975630629, policy loss: 5.1058744324866705
Experience 7, Iter 24, disc loss: 0.0589295902598568, policy loss: 5.272293913356102
Experience 7, Iter 25, disc loss: 0.04787660583334724, policy loss: 5.368981116694359
Experience 7, Iter 26, disc loss: 0.05620207549861926, policy loss: 5.253425133133103
Experience 7, Iter 27, disc loss: 0.05130649100902539, policy loss: 5.5412769924048035
Experience 7, Iter 28, disc loss: 0.06068848290807276, policy loss: 5.37883261948128
Experience 7, Iter 29, disc loss: 0.049713095984970895, policy loss: 5.980505285471654
Experience 7, Iter 30, disc loss: 0.03989722445120053, policy loss: 5.969657871498953
Experience 7, Iter 31, disc loss: 0.039482876037406106, policy loss: 6.425024009738171
Experience 7, Iter 32, disc loss: 0.035388712999862904, policy loss: 6.795111906280023
Experience 7, Iter 33, disc loss: 0.03351912118398792, policy loss: 6.12141578799642
Experience 7, Iter 34, disc loss: 0.02908848533576397, policy loss: 6.987870520794484
Experience 7, Iter 35, disc loss: 0.02992688778544788, policy loss: 6.347738960822135
Experience 7, Iter 36, disc loss: 0.027567155340274523, policy loss: 6.409124904122684
Experience 7, Iter 37, disc loss: 0.031102759607454518, policy loss: 6.5672561315584215
Experience 7, Iter 38, disc loss: 0.038178898588890675, policy loss: 6.712596306604081
Experience 7, Iter 39, disc loss: 0.044768478552478466, policy loss: 6.32781009815835
Experience 7, Iter 40, disc loss: 0.02905164965912329, policy loss: 6.416126226416184
Experience 7, Iter 41, disc loss: 0.027171216652727356, policy loss: 6.801351497409175
Experience 7, Iter 42, disc loss: 0.024521673683402592, policy loss: 6.693365436408264
Experience 7, Iter 43, disc loss: 0.022416064346797555, policy loss: 7.324485623486974
Experience 7, Iter 44, disc loss: 0.025305834139451686, policy loss: 6.759998219311891
Experience 7, Iter 45, disc loss: 0.0221283940007758, policy loss: 6.688003557592122
Experience 7, Iter 46, disc loss: 0.02243796599628059, policy loss: 6.773216285354886
Experience 7, Iter 47, disc loss: 0.021420150279700494, policy loss: 6.913171013355656
Experience 7, Iter 48, disc loss: 0.019286443744366288, policy loss: 7.067726078856237
Experience 7, Iter 49, disc loss: 0.01844127479640096, policy loss: 6.48231914459134
Experience 7, Iter 50, disc loss: 0.020483339149609307, policy loss: 6.777627959317867
Experience 7, Iter 51, disc loss: 0.01844448777179232, policy loss: 6.638599096827628
Experience 7, Iter 52, disc loss: 0.036598458901605804, policy loss: 6.450108837172751
Experience 7, Iter 53, disc loss: 0.024051176782158488, policy loss: 6.924592392348595
Experience 7, Iter 54, disc loss: 0.039045648582492494, policy loss: 5.957701184027153
Experience 7, Iter 55, disc loss: 0.018134077530431267, policy loss: 6.879984169141339
Experience 7, Iter 56, disc loss: 0.01717414098933594, policy loss: 7.093850376060403
Experience 7, Iter 57, disc loss: 0.03298331200215038, policy loss: 6.793231282374508
Experience 7, Iter 58, disc loss: 0.023338224234934743, policy loss: 6.881397558685991
Experience 7, Iter 59, disc loss: 0.02135337992854821, policy loss: 6.765772612493459
Experience 7, Iter 60, disc loss: 0.018535262238111416, policy loss: 6.839683920281605
Experience 7, Iter 61, disc loss: 0.023518901745161355, policy loss: 6.453209041468391
Experience 7, Iter 62, disc loss: 0.021058495151996284, policy loss: 6.596221581166024
Experience 7, Iter 63, disc loss: 0.01756613826370962, policy loss: 6.923841075600256
Experience 7, Iter 64, disc loss: 0.019198360682520628, policy loss: 6.278801511254973
Experience 7, Iter 65, disc loss: 0.021490989227305787, policy loss: 6.323902462962738
Experience 7, Iter 66, disc loss: 0.020840073080431808, policy loss: 6.006543488857787
Experience 7, Iter 67, disc loss: 0.02100896383639766, policy loss: 5.881350674098373
Experience 7, Iter 68, disc loss: 0.0304479217137349, policy loss: 5.989556851483618
Experience 7, Iter 69, disc loss: 0.020607222079129933, policy loss: 5.891261037244345
Experience 7, Iter 70, disc loss: 0.020056929934224192, policy loss: 6.32425534546426
Experience 7, Iter 71, disc loss: 0.020762269742666095, policy loss: 5.7802472469653345
Experience 7, Iter 72, disc loss: 0.02758205017155068, policy loss: 5.967766806030766
Experience 7, Iter 73, disc loss: 0.027607287484793196, policy loss: 5.708657761646329
Experience 7, Iter 74, disc loss: 0.03314285583839788, policy loss: 5.664224454866673
Experience 7, Iter 75, disc loss: 0.021915185300269077, policy loss: 5.931153833179782
Experience 7, Iter 76, disc loss: 0.02652536454381696, policy loss: 6.152725677808947
Experience 7, Iter 77, disc loss: 0.02797990274301137, policy loss: 6.014853325137279
Experience 7, Iter 78, disc loss: 0.02928103714119459, policy loss: 6.020313950337339
Experience 7, Iter 79, disc loss: 0.027787389488474024, policy loss: 5.619263036889251
Experience 7, Iter 80, disc loss: 0.031026621999808598, policy loss: 5.716272489827361
Experience 7, Iter 81, disc loss: 0.025254259957574247, policy loss: 6.197964349792101
Experience 7, Iter 82, disc loss: 0.02968315952569779, policy loss: 5.474215454935157
Experience 7, Iter 83, disc loss: 0.026543161981281566, policy loss: 5.74098991554181
Experience 7, Iter 84, disc loss: 0.033430265552143894, policy loss: 5.30997331064234
Experience 7, Iter 85, disc loss: 0.026158339037423477, policy loss: 5.686679305293793
Experience 7, Iter 86, disc loss: 0.03451736285883808, policy loss: 5.049879553409707
Experience 7, Iter 87, disc loss: 0.02660326588846394, policy loss: 5.328332085109693
Experience 7, Iter 88, disc loss: 0.02960550472674279, policy loss: 5.611309706654347
Experience 7, Iter 89, disc loss: 0.031109796612716175, policy loss: 5.451297999458616
Experience 7, Iter 90, disc loss: 0.02726212651433671, policy loss: 5.513708997772189
Experience 7, Iter 91, disc loss: 0.030656683187844973, policy loss: 5.303087271332208
Experience 7, Iter 92, disc loss: 0.02741896158177485, policy loss: 5.413573303207225
Experience 7, Iter 93, disc loss: 0.02395793026486645, policy loss: 5.593323136968048
Experience 7, Iter 94, disc loss: 0.028140851642860086, policy loss: 5.428935979430678
Experience 7, Iter 95, disc loss: 0.02268648936943925, policy loss: 6.1182876902486925
Experience 7, Iter 96, disc loss: 0.027923688617408276, policy loss: 5.328543420636348
Experience 7, Iter 97, disc loss: 0.028305738452371724, policy loss: 5.6599532835460575
Experience 7, Iter 98, disc loss: 0.02464728428814838, policy loss: 5.566392245980928
Experience 7, Iter 99, disc loss: 0.024594839910660183, policy loss: 5.898365881373675
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.0910],
        [1.2170],
        [0.0235]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0298, 0.1940, 1.0690, 0.0287, 0.0118, 2.5137]],

        [[0.0298, 0.1940, 1.0690, 0.0287, 0.0118, 2.5137]],

        [[0.0298, 0.1940, 1.0690, 0.0287, 0.0118, 2.5137]],

        [[0.0298, 0.1940, 1.0690, 0.0287, 0.0118, 2.5137]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0197, 0.3639, 4.8681, 0.0940], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0197, 0.3639, 4.8681, 0.0940])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.919
Iter 2/2000 - Loss: 2.795
Iter 3/2000 - Loss: 2.778
Iter 4/2000 - Loss: 2.718
Iter 5/2000 - Loss: 2.699
Iter 6/2000 - Loss: 2.664
Iter 7/2000 - Loss: 2.601
Iter 8/2000 - Loss: 2.547
Iter 9/2000 - Loss: 2.500
Iter 10/2000 - Loss: 2.435
Iter 11/2000 - Loss: 2.350
Iter 12/2000 - Loss: 2.262
Iter 13/2000 - Loss: 2.176
Iter 14/2000 - Loss: 2.081
Iter 15/2000 - Loss: 1.967
Iter 16/2000 - Loss: 1.835
Iter 17/2000 - Loss: 1.697
Iter 18/2000 - Loss: 1.556
Iter 19/2000 - Loss: 1.405
Iter 20/2000 - Loss: 1.236
Iter 1981/2000 - Loss: -5.998
Iter 1982/2000 - Loss: -5.998
Iter 1983/2000 - Loss: -5.998
Iter 1984/2000 - Loss: -5.998
Iter 1985/2000 - Loss: -5.998
Iter 1986/2000 - Loss: -5.998
Iter 1987/2000 - Loss: -5.999
Iter 1988/2000 - Loss: -5.999
Iter 1989/2000 - Loss: -5.999
Iter 1990/2000 - Loss: -5.999
Iter 1991/2000 - Loss: -5.999
Iter 1992/2000 - Loss: -5.999
Iter 1993/2000 - Loss: -5.999
Iter 1994/2000 - Loss: -5.999
Iter 1995/2000 - Loss: -5.999
Iter 1996/2000 - Loss: -5.999
Iter 1997/2000 - Loss: -5.999
Iter 1998/2000 - Loss: -5.999
Iter 1999/2000 - Loss: -5.999
Iter 2000/2000 - Loss: -5.999
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0028],
        [0.0004]])
Lengthscale: tensor([[[16.1293,  6.8156, 32.9268,  5.9621, 16.0953, 43.1521]],

        [[21.8365, 37.0046,  7.3269,  1.2590,  5.5131, 22.3175]],

        [[26.6234, 40.8068, 10.3691,  0.9266,  1.3486, 20.6135]],

        [[24.2805, 42.2756, 19.7854,  2.5802,  2.2152, 44.6373]]])
Signal Variance: tensor([ 0.0863,  1.6073, 13.0056,  0.7264])
Estimated target variance: tensor([0.0197, 0.3639, 4.8681, 0.0940])
N: 80
Signal to noise ratio: tensor([15.3885, 66.7794, 67.9907, 41.2615])
Bound on condition number: tensor([ 18945.5429, 356759.7145, 369819.4250, 136201.8553])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.021744506212543743, policy loss: 6.047995657783852
Experience 8, Iter 1, disc loss: 0.023570076483070843, policy loss: 5.960903853490803
Experience 8, Iter 2, disc loss: 0.0226111912185677, policy loss: 6.189231004266224
Experience 8, Iter 3, disc loss: 0.024562800274514587, policy loss: 5.962938949830503
Experience 8, Iter 4, disc loss: 0.019251834743890597, policy loss: 5.959163323744654
Experience 8, Iter 5, disc loss: 0.01908701736857555, policy loss: 6.039878211361837
Experience 8, Iter 6, disc loss: 0.024207818107575923, policy loss: 5.629796870136301
Experience 8, Iter 7, disc loss: 0.021397974360304682, policy loss: 5.887340806965921
Experience 8, Iter 8, disc loss: 0.017307678860700598, policy loss: 5.8963618014476635
Experience 8, Iter 9, disc loss: 0.020690528401961616, policy loss: 5.712559330956311
Experience 8, Iter 10, disc loss: 0.020400348266253844, policy loss: 5.9211863177483846
Experience 8, Iter 11, disc loss: 0.017898330179922192, policy loss: 6.064119141315716
Experience 8, Iter 12, disc loss: 0.03144458551633586, policy loss: 5.323173148149326
Experience 8, Iter 13, disc loss: 0.019434822477177024, policy loss: 6.125469691614869
Experience 8, Iter 14, disc loss: 0.02570580773701927, policy loss: 6.090841118975574
Experience 8, Iter 15, disc loss: 0.016521820821991367, policy loss: 6.373463374946528
Experience 8, Iter 16, disc loss: 0.016234944430835292, policy loss: 6.461101507901603
Experience 8, Iter 17, disc loss: 0.02351191811703504, policy loss: 5.882953935058337
Experience 8, Iter 18, disc loss: 0.017279331753541953, policy loss: 6.376211547468179
Experience 8, Iter 19, disc loss: 0.017640730468319717, policy loss: 6.4756776992578065
Experience 8, Iter 20, disc loss: 0.025033716972252595, policy loss: 6.163180641524518
Experience 8, Iter 21, disc loss: 0.017378712625633654, policy loss: 6.265340261206305
Experience 8, Iter 22, disc loss: 0.022298225314456348, policy loss: 6.481191021198171
Experience 8, Iter 23, disc loss: 0.020209233293013877, policy loss: 6.006822448300977
Experience 8, Iter 24, disc loss: 0.014662997330497318, policy loss: 6.769244358996508
Experience 8, Iter 25, disc loss: 0.01782694218238628, policy loss: 5.9503920624159665
Experience 8, Iter 26, disc loss: 0.01737881168268879, policy loss: 6.5405173798322025
Experience 8, Iter 27, disc loss: 0.015901941558620826, policy loss: 6.0595299643942315
Experience 8, Iter 28, disc loss: 0.014778116341433091, policy loss: 6.323696993691785
Experience 8, Iter 29, disc loss: 0.013924466805375329, policy loss: 6.085645947699325
Experience 8, Iter 30, disc loss: 0.022120183370619145, policy loss: 5.944128009040327
Experience 8, Iter 31, disc loss: 0.021453182458788946, policy loss: 6.049744555726978
Experience 8, Iter 32, disc loss: 0.014743820449996347, policy loss: 6.385036328948512
Experience 8, Iter 33, disc loss: 0.01940908517732104, policy loss: 6.200711101767254
Experience 8, Iter 34, disc loss: 0.014141900993311612, policy loss: 6.557275060820899
Experience 8, Iter 35, disc loss: 0.026507274918670932, policy loss: 6.019222780826593
Experience 8, Iter 36, disc loss: 0.020512337345159296, policy loss: 6.578495634541194
Experience 8, Iter 37, disc loss: 0.016753093731244353, policy loss: 6.471403724345643
Experience 8, Iter 38, disc loss: 0.02208633170024728, policy loss: 6.761469008116398
Experience 8, Iter 39, disc loss: 0.01913181712439014, policy loss: 6.822173453419477
Experience 8, Iter 40, disc loss: 0.015341720467842941, policy loss: 7.037248715660549
Experience 8, Iter 41, disc loss: 0.016541620476844716, policy loss: 6.963598356440572
Experience 8, Iter 42, disc loss: 0.01698472375455189, policy loss: 6.491388807648209
Experience 8, Iter 43, disc loss: 0.014431386202535831, policy loss: 6.91309165050186
Experience 8, Iter 44, disc loss: 0.018477891973529947, policy loss: 6.808249475687717
Experience 8, Iter 45, disc loss: 0.015983957947512343, policy loss: 6.502697904378325
Experience 8, Iter 46, disc loss: 0.015472677748790954, policy loss: 6.1679571397157895
Experience 8, Iter 47, disc loss: 0.018248402116901354, policy loss: 6.3359581525279385
Experience 8, Iter 48, disc loss: 0.01478880304178784, policy loss: 6.266618163710076
Experience 8, Iter 49, disc loss: 0.012475986498706883, policy loss: 6.47254176868585
Experience 8, Iter 50, disc loss: 0.014338645968721097, policy loss: 6.451142588324386
Experience 8, Iter 51, disc loss: 0.018780068668397994, policy loss: 6.361189646475143
Experience 8, Iter 52, disc loss: 0.01930106012352877, policy loss: 6.080514447088436
Experience 8, Iter 53, disc loss: 0.010173476353077445, policy loss: 7.06549898993055
Experience 8, Iter 54, disc loss: 0.017323700834180396, policy loss: 6.780855326556761
Experience 8, Iter 55, disc loss: 0.01493076592418344, policy loss: 6.404019791577576
Experience 8, Iter 56, disc loss: 0.014898205999364993, policy loss: 6.738784855114595
Experience 8, Iter 57, disc loss: 0.012148621135206468, policy loss: 6.334753149131502
Experience 8, Iter 58, disc loss: 0.01902881875436884, policy loss: 6.492271793925896
Experience 8, Iter 59, disc loss: 0.015056331646828047, policy loss: 6.66660887515968
Experience 8, Iter 60, disc loss: 0.015678916316381006, policy loss: 6.664508525693832
Experience 8, Iter 61, disc loss: 0.016735701454085337, policy loss: 6.721648125278134
Experience 8, Iter 62, disc loss: 0.011894544041671694, policy loss: 6.993191590603541
Experience 8, Iter 63, disc loss: 0.013674848703676286, policy loss: 6.757977853942363
Experience 8, Iter 64, disc loss: 0.012134958564947092, policy loss: 6.6435805022806065
Experience 8, Iter 65, disc loss: 0.016106964699136053, policy loss: 6.704870325298064
Experience 8, Iter 66, disc loss: 0.010868490586110724, policy loss: 7.172618664416097
Experience 8, Iter 67, disc loss: 0.01217992687386087, policy loss: 6.581986071581278
Experience 8, Iter 68, disc loss: 0.010380111572148005, policy loss: 7.30308671109666
Experience 8, Iter 69, disc loss: 0.011174623441594922, policy loss: 6.654388019248405
Experience 8, Iter 70, disc loss: 0.009724511221824089, policy loss: 6.806286281182139
Experience 8, Iter 71, disc loss: 0.019510866032497888, policy loss: 5.852511617725561
Experience 8, Iter 72, disc loss: 0.010699803813785994, policy loss: 6.613117277072041
Experience 8, Iter 73, disc loss: 0.015557385607638023, policy loss: 6.3535348234189915
Experience 8, Iter 74, disc loss: 0.012909482981456662, policy loss: 6.653251542645354
Experience 8, Iter 75, disc loss: 0.010796174586946332, policy loss: 6.826909533778713
Experience 8, Iter 76, disc loss: 0.011557903598687918, policy loss: 7.044550829499574
Experience 8, Iter 77, disc loss: 0.013387117745303033, policy loss: 6.895663987086188
Experience 8, Iter 78, disc loss: 0.012008387245736368, policy loss: 6.752627054438429
Experience 8, Iter 79, disc loss: 0.008642861729805381, policy loss: 7.314264196488217
Experience 8, Iter 80, disc loss: 0.024542182488659102, policy loss: 6.578605652758517
Experience 8, Iter 81, disc loss: 0.010855654474249727, policy loss: 6.980488099633713
Experience 8, Iter 82, disc loss: 0.013045396997328033, policy loss: 6.467076069029032
Experience 8, Iter 83, disc loss: 0.014323698370747852, policy loss: 7.193468697916026
Experience 8, Iter 84, disc loss: 0.009339437371654153, policy loss: 7.149282672332438
Experience 8, Iter 85, disc loss: 0.011593953942271508, policy loss: 7.1891736847164465
Experience 8, Iter 86, disc loss: 0.01316818398249775, policy loss: 6.909329450186991
Experience 8, Iter 87, disc loss: 0.009148312545090437, policy loss: 7.219960401773509
Experience 8, Iter 88, disc loss: 0.010256408000956473, policy loss: 7.111632645907806
Experience 8, Iter 89, disc loss: 0.010962637993656658, policy loss: 7.115866961885075
Experience 8, Iter 90, disc loss: 0.012581060512435115, policy loss: 6.785176706844408
Experience 8, Iter 91, disc loss: 0.014681702078088757, policy loss: 6.9441140448641505
Experience 8, Iter 92, disc loss: 0.009289008133644392, policy loss: 7.135817210455918
Experience 8, Iter 93, disc loss: 0.009654683651176579, policy loss: 7.007792463453699
Experience 8, Iter 94, disc loss: 0.014065954660528924, policy loss: 6.707950661816273
Experience 8, Iter 95, disc loss: 0.012653977284026601, policy loss: 6.922188563756174
Experience 8, Iter 96, disc loss: 0.010305080997579118, policy loss: 6.9106793095909
Experience 8, Iter 97, disc loss: 0.009623431624079928, policy loss: 6.873140472222929
Experience 8, Iter 98, disc loss: 0.011225350613689494, policy loss: 6.592422332334159
Experience 8, Iter 99, disc loss: 0.011837114287022473, policy loss: 7.010191555471188
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1191],
        [1.3501],
        [0.0290]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0270, 0.1939, 1.2658, 0.0295, 0.0170, 3.2126]],

        [[0.0270, 0.1939, 1.2658, 0.0295, 0.0170, 3.2126]],

        [[0.0270, 0.1939, 1.2658, 0.0295, 0.0170, 3.2126]],

        [[0.0270, 0.1939, 1.2658, 0.0295, 0.0170, 3.2126]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0197, 0.4764, 5.4005, 0.1160], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0197, 0.4764, 5.4005, 0.1160])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.207
Iter 2/2000 - Loss: 3.114
Iter 3/2000 - Loss: 3.057
Iter 4/2000 - Loss: 3.021
Iter 5/2000 - Loss: 3.014
Iter 6/2000 - Loss: 2.951
Iter 7/2000 - Loss: 2.874
Iter 8/2000 - Loss: 2.829
Iter 9/2000 - Loss: 2.785
Iter 10/2000 - Loss: 2.707
Iter 11/2000 - Loss: 2.609
Iter 12/2000 - Loss: 2.509
Iter 13/2000 - Loss: 2.408
Iter 14/2000 - Loss: 2.295
Iter 15/2000 - Loss: 2.162
Iter 16/2000 - Loss: 2.015
Iter 17/2000 - Loss: 1.858
Iter 18/2000 - Loss: 1.689
Iter 19/2000 - Loss: 1.503
Iter 20/2000 - Loss: 1.298
Iter 1981/2000 - Loss: -5.890
Iter 1982/2000 - Loss: -5.890
Iter 1983/2000 - Loss: -5.890
Iter 1984/2000 - Loss: -5.890
Iter 1985/2000 - Loss: -5.890
Iter 1986/2000 - Loss: -5.890
Iter 1987/2000 - Loss: -5.890
Iter 1988/2000 - Loss: -5.890
Iter 1989/2000 - Loss: -5.890
Iter 1990/2000 - Loss: -5.890
Iter 1991/2000 - Loss: -5.890
Iter 1992/2000 - Loss: -5.890
Iter 1993/2000 - Loss: -5.891
Iter 1994/2000 - Loss: -5.891
Iter 1995/2000 - Loss: -5.891
Iter 1996/2000 - Loss: -5.891
Iter 1997/2000 - Loss: -5.891
Iter 1998/2000 - Loss: -5.891
Iter 1999/2000 - Loss: -5.891
Iter 2000/2000 - Loss: -5.891
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[12.3691,  6.0037, 24.2647,  2.9618, 18.5367, 43.1598]],

        [[22.6283, 37.9029,  6.8828,  1.2101,  2.6594, 22.4947]],

        [[25.5662, 35.7901,  9.5496,  1.0289,  0.8443, 21.3092]],

        [[22.9717, 39.2706, 19.9120,  2.6270,  2.1439, 47.1555]]])
Signal Variance: tensor([ 0.0744,  1.4659, 12.7104,  0.8409])
Estimated target variance: tensor([0.0197, 0.4764, 5.4005, 0.1160])
N: 90
Signal to noise ratio: tensor([14.8113, 66.3131, 65.7509, 45.2675])
Bound on condition number: tensor([ 19744.6827, 395769.8526, 389087.2016, 184423.9518])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.5343580831049239, policy loss: 2.259899799983701
Experience 9, Iter 1, disc loss: 0.2437763682709654, policy loss: 3.2527250102011496
Experience 9, Iter 2, disc loss: 0.09903172288660372, policy loss: 4.6528048705282545
Experience 9, Iter 3, disc loss: 0.1290302897214314, policy loss: 5.551245937010787
Experience 9, Iter 4, disc loss: 0.2386499277075335, policy loss: 6.24146842432085
Experience 9, Iter 5, disc loss: 0.29368209541280554, policy loss: 5.900780245450507
Experience 9, Iter 6, disc loss: 0.2406748096971432, policy loss: 5.420063782706219
Experience 9, Iter 7, disc loss: 0.15101181264598929, policy loss: 4.588353930942727
Experience 9, Iter 8, disc loss: 0.08683917793438034, policy loss: 4.108511441321124
Experience 9, Iter 9, disc loss: 0.06947841555247056, policy loss: 3.269788415336969
Experience 9, Iter 10, disc loss: 0.06517941669017016, policy loss: 3.2416028161448236
Experience 9, Iter 11, disc loss: 0.0860887203170797, policy loss: 2.9673119487272177
Experience 9, Iter 12, disc loss: 0.1199632352306406, policy loss: 2.7251344621621953
Experience 9, Iter 13, disc loss: 0.13466539632303476, policy loss: 2.843989120822138
Experience 9, Iter 14, disc loss: 0.10088449801879741, policy loss: 3.4566906503400032
Experience 9, Iter 15, disc loss: 0.0706253830340144, policy loss: 3.659903493897222
Experience 9, Iter 16, disc loss: 0.06113587195409456, policy loss: 3.9616592099492904
Experience 9, Iter 17, disc loss: 0.041702023384209815, policy loss: 4.735342416331175
Experience 9, Iter 18, disc loss: 0.037469927350711016, policy loss: 4.816840799727626
Experience 9, Iter 19, disc loss: 0.041919143124601585, policy loss: 5.114881194715393
Experience 9, Iter 20, disc loss: 0.04054692612847214, policy loss: 5.350116695901643
Experience 9, Iter 21, disc loss: 0.04292526333888852, policy loss: 5.56111624302882
Experience 9, Iter 22, disc loss: 0.04579707554541648, policy loss: 5.958685296693515
Experience 9, Iter 23, disc loss: 0.045734615671045295, policy loss: 6.103115101231687
Experience 9, Iter 24, disc loss: 0.04523363316064225, policy loss: 6.204839375479501
Experience 9, Iter 25, disc loss: 0.04647967830924492, policy loss: 5.544907621539236
Experience 9, Iter 26, disc loss: 0.04046709948998136, policy loss: 5.60711307013816
Experience 9, Iter 27, disc loss: 0.03790046452307015, policy loss: 5.4377288972758695
Experience 9, Iter 28, disc loss: 0.03490888008016414, policy loss: 5.264303491832024
Experience 9, Iter 29, disc loss: 0.032079832748377474, policy loss: 5.114129739692281
Experience 9, Iter 30, disc loss: 0.0299009865380613, policy loss: 5.0994488752284
Experience 9, Iter 31, disc loss: 0.028229100280133743, policy loss: 5.228501195084649
Experience 9, Iter 32, disc loss: 0.02946630616129111, policy loss: 4.819137129544834
Experience 9, Iter 33, disc loss: 0.02859045461594911, policy loss: 5.05396438550356
Experience 9, Iter 34, disc loss: 0.030153823714425963, policy loss: 4.583121897763327
Experience 9, Iter 35, disc loss: 0.030240138678612173, policy loss: 5.035336644755396
Experience 9, Iter 36, disc loss: 0.02861955172832078, policy loss: 4.773624700244923
Experience 9, Iter 37, disc loss: 0.023164198600035622, policy loss: 4.839432341739649
Experience 9, Iter 38, disc loss: 0.026978732588748364, policy loss: 4.808411926414066
Experience 9, Iter 39, disc loss: 0.028724719918033277, policy loss: 4.888203360581754
Experience 9, Iter 40, disc loss: 0.01944795936552094, policy loss: 5.3165449455118505
Experience 9, Iter 41, disc loss: 0.024924203259561233, policy loss: 5.091407660921448
Experience 9, Iter 42, disc loss: 0.023693623426247364, policy loss: 5.125298964372495
Experience 9, Iter 43, disc loss: 0.021464464489665276, policy loss: 5.225788340851988
Experience 9, Iter 44, disc loss: 0.024975787236644703, policy loss: 5.030224572356823
Experience 9, Iter 45, disc loss: 0.021492426814388618, policy loss: 5.62972678062845
Experience 9, Iter 46, disc loss: 0.02044108673219716, policy loss: 5.410334301210088
Experience 9, Iter 47, disc loss: 0.02280965007811133, policy loss: 5.175898680923096
Experience 9, Iter 48, disc loss: 0.020217064656544395, policy loss: 5.641134140523533
Experience 9, Iter 49, disc loss: 0.0210021983120491, policy loss: 5.356058787096586
Experience 9, Iter 50, disc loss: 0.01983476761393737, policy loss: 5.371856116214669
Experience 9, Iter 51, disc loss: 0.021240485961673817, policy loss: 5.187911905249118
Experience 9, Iter 52, disc loss: 0.021243587990290958, policy loss: 5.499521110089818
Experience 9, Iter 53, disc loss: 0.01991595202146084, policy loss: 5.7739085801556955
Experience 9, Iter 54, disc loss: 0.02191301796563349, policy loss: 5.228761308868249
Experience 9, Iter 55, disc loss: 0.022121771241762594, policy loss: 5.3449969544204
Experience 9, Iter 56, disc loss: 0.018059462809824442, policy loss: 5.967691505454597
Experience 9, Iter 57, disc loss: 0.019450867519450594, policy loss: 5.828593143192861
Experience 9, Iter 58, disc loss: 0.01977706278829105, policy loss: 5.366157529438419
Experience 9, Iter 59, disc loss: 0.020886490528465893, policy loss: 5.406026411849202
Experience 9, Iter 60, disc loss: 0.019885120616797883, policy loss: 5.474599915890897
Experience 9, Iter 61, disc loss: 0.01739037226800862, policy loss: 5.479624219515352
Experience 9, Iter 62, disc loss: 0.019632090042542474, policy loss: 5.390808232255309
Experience 9, Iter 63, disc loss: 0.017935788370586948, policy loss: 5.791761327018263
Experience 9, Iter 64, disc loss: 0.01948291807438025, policy loss: 5.187527611889916
Experience 9, Iter 65, disc loss: 0.01874088227821514, policy loss: 5.415077809610037
Experience 9, Iter 66, disc loss: 0.01927119343717551, policy loss: 5.5724356120809215
Experience 9, Iter 67, disc loss: 0.019177221453323137, policy loss: 5.403961734770126
Experience 9, Iter 68, disc loss: 0.018523760945766018, policy loss: 5.592827501515037
Experience 9, Iter 69, disc loss: 0.01778839374413252, policy loss: 5.553447967075405
Experience 9, Iter 70, disc loss: 0.017738193343942166, policy loss: 5.491172568090443
Experience 9, Iter 71, disc loss: 0.017431430643040716, policy loss: 5.490560214338173
Experience 9, Iter 72, disc loss: 0.01881844655616742, policy loss: 5.407815486040045
Experience 9, Iter 73, disc loss: 0.01732066874062997, policy loss: 5.33753083146293
Experience 9, Iter 74, disc loss: 0.017877470468647676, policy loss: 5.294913315786126
Experience 9, Iter 75, disc loss: 0.017028167298484453, policy loss: 5.540437505168902
Experience 9, Iter 76, disc loss: 0.017258345215819627, policy loss: 5.562250401838728
Experience 9, Iter 77, disc loss: 0.01657378559660663, policy loss: 5.660184078275763
Experience 9, Iter 78, disc loss: 0.016872299436725864, policy loss: 5.9036808417153965
Experience 9, Iter 79, disc loss: 0.01740730870842786, policy loss: 5.519456415679865
Experience 9, Iter 80, disc loss: 0.01659206763200069, policy loss: 5.479231673902398
Experience 9, Iter 81, disc loss: 0.016957009452100706, policy loss: 5.689195949936712
Experience 9, Iter 82, disc loss: 0.01587347070824206, policy loss: 5.7089668281867
Experience 9, Iter 83, disc loss: 0.0156918721905268, policy loss: 5.959724636172874
Experience 9, Iter 84, disc loss: 0.01728069563879246, policy loss: 5.601353374657091
Experience 9, Iter 85, disc loss: 0.015546108874641737, policy loss: 5.5653854562846234
Experience 9, Iter 86, disc loss: 0.01649697969167126, policy loss: 5.5523910980663125
Experience 9, Iter 87, disc loss: 0.014888925249207501, policy loss: 5.587859767640289
Experience 9, Iter 88, disc loss: 0.016027755473877015, policy loss: 5.468642393072068
Experience 9, Iter 89, disc loss: 0.017716864309232874, policy loss: 5.356731544019166
Experience 9, Iter 90, disc loss: 0.014490414402463035, policy loss: 5.692101252837373
Experience 9, Iter 91, disc loss: 0.016270822435798013, policy loss: 5.547954009310929
Experience 9, Iter 92, disc loss: 0.01808469345451676, policy loss: 5.677962676233793
Experience 9, Iter 93, disc loss: 0.01641147088804045, policy loss: 5.55765445180815
Experience 9, Iter 94, disc loss: 0.01555865729520234, policy loss: 5.96402622025316
Experience 9, Iter 95, disc loss: 0.016841403207309192, policy loss: 5.384081169355315
Experience 9, Iter 96, disc loss: 0.015162386188770482, policy loss: 5.793115685281181
Experience 9, Iter 97, disc loss: 0.015969394641014274, policy loss: 5.432633058325594
Experience 9, Iter 98, disc loss: 0.01610236387297737, policy loss: 5.635543821729046
Experience 9, Iter 99, disc loss: 0.017330710808489767, policy loss: 5.4489129779637615
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1459],
        [1.4894],
        [0.0323]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0247, 0.1946, 1.4152, 0.0302, 0.0198, 3.7586]],

        [[0.0247, 0.1946, 1.4152, 0.0302, 0.0198, 3.7586]],

        [[0.0247, 0.1946, 1.4152, 0.0302, 0.0198, 3.7586]],

        [[0.0247, 0.1946, 1.4152, 0.0302, 0.0198, 3.7586]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0196, 0.5836, 5.9576, 0.1293], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0196, 0.5836, 5.9576, 0.1293])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.412
Iter 2/2000 - Loss: 3.334
Iter 3/2000 - Loss: 3.248
Iter 4/2000 - Loss: 3.216
Iter 5/2000 - Loss: 3.214
Iter 6/2000 - Loss: 3.141
Iter 7/2000 - Loss: 3.052
Iter 8/2000 - Loss: 2.994
Iter 9/2000 - Loss: 2.938
Iter 10/2000 - Loss: 2.854
Iter 11/2000 - Loss: 2.750
Iter 12/2000 - Loss: 2.642
Iter 13/2000 - Loss: 2.525
Iter 14/2000 - Loss: 2.391
Iter 15/2000 - Loss: 2.237
Iter 16/2000 - Loss: 2.071
Iter 17/2000 - Loss: 1.896
Iter 18/2000 - Loss: 1.708
Iter 19/2000 - Loss: 1.497
Iter 20/2000 - Loss: 1.265
Iter 1981/2000 - Loss: -6.088
Iter 1982/2000 - Loss: -6.089
Iter 1983/2000 - Loss: -6.089
Iter 1984/2000 - Loss: -6.089
Iter 1985/2000 - Loss: -6.089
Iter 1986/2000 - Loss: -6.089
Iter 1987/2000 - Loss: -6.089
Iter 1988/2000 - Loss: -6.089
Iter 1989/2000 - Loss: -6.089
Iter 1990/2000 - Loss: -6.089
Iter 1991/2000 - Loss: -6.089
Iter 1992/2000 - Loss: -6.089
Iter 1993/2000 - Loss: -6.089
Iter 1994/2000 - Loss: -6.089
Iter 1995/2000 - Loss: -6.089
Iter 1996/2000 - Loss: -6.089
Iter 1997/2000 - Loss: -6.089
Iter 1998/2000 - Loss: -6.089
Iter 1999/2000 - Loss: -6.089
Iter 2000/2000 - Loss: -6.089
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[11.7535,  5.8865, 25.4247,  2.5848, 17.1181, 45.4467]],

        [[23.2839, 36.6356,  6.7588,  1.2474,  3.0776, 23.4914]],

        [[23.8563, 37.3721,  9.3074,  0.9870,  0.7617, 22.3753]],

        [[22.2394, 36.5201, 19.1903,  2.4007,  2.2029, 43.9046]]])
Signal Variance: tensor([ 0.0706,  1.6490, 12.1889,  0.7757])
Estimated target variance: tensor([0.0196, 0.5836, 5.9576, 0.1293])
N: 100
Signal to noise ratio: tensor([15.0207, 69.2268, 64.7139, 45.4581])
Bound on condition number: tensor([ 22563.0455, 479235.9518, 418790.0988, 206645.0333])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.016408064133127377, policy loss: 5.440879471505149
Experience 10, Iter 1, disc loss: 0.017053121195309953, policy loss: 5.525942236197597
Experience 10, Iter 2, disc loss: 0.021861172198895183, policy loss: 5.045207534493858
Experience 10, Iter 3, disc loss: 0.017447572660351546, policy loss: 5.8448435410072
Experience 10, Iter 4, disc loss: 0.020359741232294393, policy loss: 5.496818539430258
Experience 10, Iter 5, disc loss: 0.0171829720887595, policy loss: 5.523475619318001
Experience 10, Iter 6, disc loss: 0.014103373017057524, policy loss: 5.987287763619633
Experience 10, Iter 7, disc loss: 0.01496035249777344, policy loss: 5.876375237167448
Experience 10, Iter 8, disc loss: 0.016277784538516152, policy loss: 5.454798355144806
Experience 10, Iter 9, disc loss: 0.017127900376344957, policy loss: 5.430549936152204
Experience 10, Iter 10, disc loss: 0.01789095533946692, policy loss: 5.545541735682945
Experience 10, Iter 11, disc loss: 0.01577233261209992, policy loss: 5.642284527159735
Experience 10, Iter 12, disc loss: 0.017819213692053658, policy loss: 5.596690084648936
Experience 10, Iter 13, disc loss: 0.01621297604175425, policy loss: 5.507635053750732
Experience 10, Iter 14, disc loss: 0.01721844108699403, policy loss: 5.6432244622704815
Experience 10, Iter 15, disc loss: 0.01748298830524804, policy loss: 5.2410754753166975
Experience 10, Iter 16, disc loss: 0.01442623380263726, policy loss: 6.259384187165354
Experience 10, Iter 17, disc loss: 0.015227806352698062, policy loss: 5.585533756288903
Experience 10, Iter 18, disc loss: 0.016137027534138926, policy loss: 5.410458844045242
Experience 10, Iter 19, disc loss: 0.01618205335351757, policy loss: 5.88454442090524
Experience 10, Iter 20, disc loss: 0.015062968900650007, policy loss: 5.648569604894244
Experience 10, Iter 21, disc loss: 0.01737161845154906, policy loss: 5.6192650361105505
Experience 10, Iter 22, disc loss: 0.014302198285118658, policy loss: 5.879996915201222
Experience 10, Iter 23, disc loss: 0.01562202599538978, policy loss: 5.661114410286151
Experience 10, Iter 24, disc loss: 0.014197728600038532, policy loss: 5.724195461071686
Experience 10, Iter 25, disc loss: 0.015048648332055352, policy loss: 5.650013689191491
Experience 10, Iter 26, disc loss: 0.014042385884109422, policy loss: 6.101028329610237
Experience 10, Iter 27, disc loss: 0.015575120501005877, policy loss: 5.522431971185872
Experience 10, Iter 28, disc loss: 0.01421182300255193, policy loss: 5.81391544603294
Experience 10, Iter 29, disc loss: 0.014507044137972065, policy loss: 5.707992669931878
Experience 10, Iter 30, disc loss: 0.012137485234712989, policy loss: 5.957899337007282
Experience 10, Iter 31, disc loss: 0.014936995309703386, policy loss: 5.656921138820096
Experience 10, Iter 32, disc loss: 0.014487951678292641, policy loss: 5.763430415335022
Experience 10, Iter 33, disc loss: 0.013725721379423187, policy loss: 5.879136639049938
Experience 10, Iter 34, disc loss: 0.011919544627601728, policy loss: 7.1293776457819895
Experience 10, Iter 35, disc loss: 0.014045705686398543, policy loss: 5.997920560191728
Experience 10, Iter 36, disc loss: 0.015237265763317104, policy loss: 5.5893235254166695
Experience 10, Iter 37, disc loss: 0.01229889577701766, policy loss: 5.8899955102131445
Experience 10, Iter 38, disc loss: 0.012011105543455233, policy loss: 6.175292550146636
Experience 10, Iter 39, disc loss: 0.013898702599065988, policy loss: 5.595097333536693
Experience 10, Iter 40, disc loss: 0.013910572106710664, policy loss: 5.54838524134999
Experience 10, Iter 41, disc loss: 0.014210193005328641, policy loss: 5.567599582957377
Experience 10, Iter 42, disc loss: 0.014564893041886285, policy loss: 5.833868859317115
Experience 10, Iter 43, disc loss: 0.014923380430396125, policy loss: 5.693310317770148
Experience 10, Iter 44, disc loss: 0.013039205653405161, policy loss: 5.7191703032844785
Experience 10, Iter 45, disc loss: 0.012066504184553519, policy loss: 5.8458016062592595
Experience 10, Iter 46, disc loss: 0.011632925935194029, policy loss: 5.974844005728953
Experience 10, Iter 47, disc loss: 0.014470362592312448, policy loss: 5.586857312198495
Experience 10, Iter 48, disc loss: 0.013735457830734293, policy loss: 5.786103300508388
Experience 10, Iter 49, disc loss: 0.015024777446326073, policy loss: 5.612920851980235
Experience 10, Iter 50, disc loss: 0.01369259980787249, policy loss: 5.782244236395627
Experience 10, Iter 51, disc loss: 0.012811589637650297, policy loss: 5.930478467295172
Experience 10, Iter 52, disc loss: 0.013954454702801381, policy loss: 5.5364187571234185
Experience 10, Iter 53, disc loss: 0.014912160980856216, policy loss: 5.553148366342196
Experience 10, Iter 54, disc loss: 0.0117912251092048, policy loss: 5.906452502348806
Experience 10, Iter 55, disc loss: 0.012465655261703002, policy loss: 5.7856663928667516
Experience 10, Iter 56, disc loss: 0.012168938831026647, policy loss: 5.754515303106134
Experience 10, Iter 57, disc loss: 0.012039827430671567, policy loss: 6.089138036604
Experience 10, Iter 58, disc loss: 0.01330278459757277, policy loss: 6.031137918825361
Experience 10, Iter 59, disc loss: 0.014353613797262414, policy loss: 5.68541023148509
Experience 10, Iter 60, disc loss: 0.01392247063782327, policy loss: 5.840991854184485
Experience 10, Iter 61, disc loss: 0.013273265298820178, policy loss: 5.699077923235542
Experience 10, Iter 62, disc loss: 0.013289247431463105, policy loss: 6.032841035434263
Experience 10, Iter 63, disc loss: 0.011619914726746356, policy loss: 6.276397641188092
Experience 10, Iter 64, disc loss: 0.011466362823178393, policy loss: 6.111353249873818
Experience 10, Iter 65, disc loss: 0.012275649175169878, policy loss: 6.162102488721093
Experience 10, Iter 66, disc loss: 0.013105330156602427, policy loss: 5.882459067050249
Experience 10, Iter 67, disc loss: 0.012801667218299137, policy loss: 5.8552127731598045
Experience 10, Iter 68, disc loss: 0.012182046334734124, policy loss: 5.773935026806081
Experience 10, Iter 69, disc loss: 0.012742252984288941, policy loss: 5.971997904479881
Experience 10, Iter 70, disc loss: 0.011427424691556281, policy loss: 5.9581964696548235
Experience 10, Iter 71, disc loss: 0.013678621142417715, policy loss: 5.484805933119112
Experience 10, Iter 72, disc loss: 0.0133095703494895, policy loss: 5.83619380869156
Experience 10, Iter 73, disc loss: 0.012077469267288267, policy loss: 5.968046015347551
Experience 10, Iter 74, disc loss: 0.013590707716531553, policy loss: 5.76943727841339
Experience 10, Iter 75, disc loss: 0.011194357761858585, policy loss: 6.001126030612664
Experience 10, Iter 76, disc loss: 0.012282897212059156, policy loss: 6.02404841352473
Experience 10, Iter 77, disc loss: 0.011170101332244435, policy loss: 5.934173822642338
Experience 10, Iter 78, disc loss: 0.012795497614121718, policy loss: 5.611046375764003
Experience 10, Iter 79, disc loss: 0.011980372298149338, policy loss: 5.8078306636595265
Experience 10, Iter 80, disc loss: 0.01225308901244671, policy loss: 5.7877603286885115
Experience 10, Iter 81, disc loss: 0.011327598245424158, policy loss: 6.16495711526373
Experience 10, Iter 82, disc loss: 0.013584218633126964, policy loss: 5.768358291209497
Experience 10, Iter 83, disc loss: 0.01108462627542271, policy loss: 6.149728807549691
Experience 10, Iter 84, disc loss: 0.01220955719470079, policy loss: 6.179283735251053
Experience 10, Iter 85, disc loss: 0.01274773808447296, policy loss: 5.96138137353478
Experience 10, Iter 86, disc loss: 0.01182471753276233, policy loss: 5.890910298369647
Experience 10, Iter 87, disc loss: 0.012327770519689843, policy loss: 5.853158364321525
Experience 10, Iter 88, disc loss: 0.012051116387388124, policy loss: 5.749667943862994
Experience 10, Iter 89, disc loss: 0.011313431649023327, policy loss: 6.224533399028953
Experience 10, Iter 90, disc loss: 0.011647711617873976, policy loss: 5.768978963611557
Experience 10, Iter 91, disc loss: 0.01032892134995729, policy loss: 6.2974837176874
Experience 10, Iter 92, disc loss: 0.011251630778489697, policy loss: 6.0319621432493165
Experience 10, Iter 93, disc loss: 0.010932354631202238, policy loss: 6.417544195575718
Experience 10, Iter 94, disc loss: 0.011253527349838342, policy loss: 5.883088090707763
Experience 10, Iter 95, disc loss: 0.010948981330609509, policy loss: 6.190664977611526
Experience 10, Iter 96, disc loss: 0.011341880156439988, policy loss: 5.972456144256021
Experience 10, Iter 97, disc loss: 0.012346065138016464, policy loss: 5.784612379381439
Experience 10, Iter 98, disc loss: 0.01112123693829489, policy loss: 6.126800905682962
Experience 10, Iter 99, disc loss: 0.01061457523769114, policy loss: 6.32220915119176
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0049],
        [0.1714],
        [1.6303],
        [0.0355]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0231, 0.1976, 1.5627, 0.0308, 0.0219, 4.2187]],

        [[0.0231, 0.1976, 1.5627, 0.0308, 0.0219, 4.2187]],

        [[0.0231, 0.1976, 1.5627, 0.0308, 0.0219, 4.2187]],

        [[0.0231, 0.1976, 1.5627, 0.0308, 0.0219, 4.2187]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0195, 0.6856, 6.5213, 0.1421], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0195, 0.6856, 6.5213, 0.1421])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.573
Iter 2/2000 - Loss: 3.493
Iter 3/2000 - Loss: 3.386
Iter 4/2000 - Loss: 3.342
Iter 5/2000 - Loss: 3.340
Iter 6/2000 - Loss: 3.257
Iter 7/2000 - Loss: 3.150
Iter 8/2000 - Loss: 3.070
Iter 9/2000 - Loss: 2.993
Iter 10/2000 - Loss: 2.889
Iter 11/2000 - Loss: 2.766
Iter 12/2000 - Loss: 2.638
Iter 13/2000 - Loss: 2.500
Iter 14/2000 - Loss: 2.343
Iter 15/2000 - Loss: 2.166
Iter 16/2000 - Loss: 1.977
Iter 17/2000 - Loss: 1.781
Iter 18/2000 - Loss: 1.573
Iter 19/2000 - Loss: 1.346
Iter 20/2000 - Loss: 1.097
Iter 1981/2000 - Loss: -6.235
Iter 1982/2000 - Loss: -6.235
Iter 1983/2000 - Loss: -6.235
Iter 1984/2000 - Loss: -6.235
Iter 1985/2000 - Loss: -6.235
Iter 1986/2000 - Loss: -6.235
Iter 1987/2000 - Loss: -6.235
Iter 1988/2000 - Loss: -6.235
Iter 1989/2000 - Loss: -6.235
Iter 1990/2000 - Loss: -6.235
Iter 1991/2000 - Loss: -6.236
Iter 1992/2000 - Loss: -6.236
Iter 1993/2000 - Loss: -6.236
Iter 1994/2000 - Loss: -6.236
Iter 1995/2000 - Loss: -6.236
Iter 1996/2000 - Loss: -6.236
Iter 1997/2000 - Loss: -6.236
Iter 1998/2000 - Loss: -6.236
Iter 1999/2000 - Loss: -6.236
Iter 2000/2000 - Loss: -6.236
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[12.6828,  5.6463, 21.0561,  2.4755, 17.8770, 46.8963]],

        [[20.0285, 33.3386,  6.6855,  1.1624,  2.9787, 24.5590]],

        [[22.8606, 36.1484,  9.0062,  1.0039,  0.7911, 23.2762]],

        [[20.9881, 34.8346, 18.9884,  2.3616,  2.1334, 44.2482]]])
Signal Variance: tensor([ 0.0607,  1.6252, 12.8957,  0.7374])
Estimated target variance: tensor([0.0195, 0.6856, 6.5213, 0.1421])
N: 110
Signal to noise ratio: tensor([14.1400, 62.7945, 66.5072, 45.7566])
Bound on condition number: tensor([ 21994.2569, 433746.9908, 486553.8813, 230304.4006])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.010837781540415956, policy loss: 5.917290968884195
Experience 11, Iter 1, disc loss: 0.010953963402049455, policy loss: 5.726304309526303
Experience 11, Iter 2, disc loss: 0.011738190540265097, policy loss: 5.857981686685497
Experience 11, Iter 3, disc loss: 0.010303268841051729, policy loss: 6.145244526183616
Experience 11, Iter 4, disc loss: 0.011848692436046311, policy loss: 5.952744373099565
Experience 11, Iter 5, disc loss: 0.011695049794387947, policy loss: 5.872876764160825
Experience 11, Iter 6, disc loss: 0.011305100880204009, policy loss: 5.5984654258443145
Experience 11, Iter 7, disc loss: 0.010524497802445477, policy loss: 6.056153355413182
Experience 11, Iter 8, disc loss: 0.0114961643069566, policy loss: 6.002200619711349
Experience 11, Iter 9, disc loss: 0.011546256584610531, policy loss: 6.001028911137924
Experience 11, Iter 10, disc loss: 0.010914693787358605, policy loss: 5.9322582262401085
Experience 11, Iter 11, disc loss: 0.010775474570713633, policy loss: 5.909230799543355
Experience 11, Iter 12, disc loss: 0.010854656959200504, policy loss: 5.992767462268207
Experience 11, Iter 13, disc loss: 0.010556865344075298, policy loss: 6.20800439324161
Experience 11, Iter 14, disc loss: 0.01078440959917579, policy loss: 6.096838668747729
Experience 11, Iter 15, disc loss: 0.011215886442470756, policy loss: 5.887612202980913
Experience 11, Iter 16, disc loss: 0.01069288329609517, policy loss: 5.92620548648166
Experience 11, Iter 17, disc loss: 0.010613194412969408, policy loss: 6.306739826899706
Experience 11, Iter 18, disc loss: 0.009385882917136705, policy loss: 6.291525657728977
Experience 11, Iter 19, disc loss: 0.01000994585605701, policy loss: 6.176260664681427
Experience 11, Iter 20, disc loss: 0.010332600440218631, policy loss: 6.0556142471990135
Experience 11, Iter 21, disc loss: 0.01002328222595789, policy loss: 6.178626772282718
Experience 11, Iter 22, disc loss: 0.010057192195179289, policy loss: 5.991015544819894
Experience 11, Iter 23, disc loss: 0.010388252075371055, policy loss: 6.018650124875768
Experience 11, Iter 24, disc loss: 0.009992823989862124, policy loss: 6.066626384801454
Experience 11, Iter 25, disc loss: 0.010219100882191388, policy loss: 5.929911935645899
Experience 11, Iter 26, disc loss: 0.010610633501113013, policy loss: 5.890976866833409
Experience 11, Iter 27, disc loss: 0.009536005962778246, policy loss: 6.3548363328601685
Experience 11, Iter 28, disc loss: 0.009905691074980332, policy loss: 6.191199573501324
Experience 11, Iter 29, disc loss: 0.010871462169179575, policy loss: 5.852268113365753
Experience 11, Iter 30, disc loss: 0.009436547118051593, policy loss: 6.332607290146233
Experience 11, Iter 31, disc loss: 0.00925026139211438, policy loss: 6.12481527239736
Experience 11, Iter 32, disc loss: 0.00930776235124205, policy loss: 6.10994819122101
Experience 11, Iter 33, disc loss: 0.009287644430540906, policy loss: 6.163822050470015
Experience 11, Iter 34, disc loss: 0.010090524028071732, policy loss: 6.278147374147666
Experience 11, Iter 35, disc loss: 0.008243697979614419, policy loss: 6.492974394581143
Experience 11, Iter 36, disc loss: 0.010288639690354854, policy loss: 5.950368275292793
Experience 11, Iter 37, disc loss: 0.009298838409014052, policy loss: 6.065351100254611
Experience 11, Iter 38, disc loss: 0.00906123994642322, policy loss: 5.954800950905883
Experience 11, Iter 39, disc loss: 0.008698336136624586, policy loss: 6.32750017093537
Experience 11, Iter 40, disc loss: 0.010655431807107952, policy loss: 5.676004109919385
Experience 11, Iter 41, disc loss: 0.01045581883152764, policy loss: 5.999802616516331
Experience 11, Iter 42, disc loss: 0.00973496128077274, policy loss: 5.9549189355790215
Experience 11, Iter 43, disc loss: 0.009494093254561999, policy loss: 6.413381487640474
Experience 11, Iter 44, disc loss: 0.009333725738843834, policy loss: 6.223582432488396
Experience 11, Iter 45, disc loss: 0.01007220436625075, policy loss: 6.356520777070289
Experience 11, Iter 46, disc loss: 0.008833630824832479, policy loss: 6.447590079364737
Experience 11, Iter 47, disc loss: 0.010224795660812616, policy loss: 6.196468980306131
Experience 11, Iter 48, disc loss: 0.009352899502003322, policy loss: 6.372110295732778
Experience 11, Iter 49, disc loss: 0.008908097672061657, policy loss: 6.187283419946904
Experience 11, Iter 50, disc loss: 0.01044736519079845, policy loss: 5.7766471512465145
Experience 11, Iter 51, disc loss: 0.009255154539266355, policy loss: 6.128265233645211
Experience 11, Iter 52, disc loss: 0.010393482213235338, policy loss: 5.8409547612123465
Experience 11, Iter 53, disc loss: 0.008329258950901848, policy loss: 6.631637587403878
Experience 11, Iter 54, disc loss: 0.008839670820027602, policy loss: 6.49892168276174
Experience 11, Iter 55, disc loss: 0.0088454155017909, policy loss: 6.3455134197694445
Experience 11, Iter 56, disc loss: 0.008847002590581406, policy loss: 6.2902312543630154
Experience 11, Iter 57, disc loss: 0.008474834088058152, policy loss: 6.367134152172056
Experience 11, Iter 58, disc loss: 0.008282182343954912, policy loss: 6.0326792281087505
Experience 11, Iter 59, disc loss: 0.008320857408860084, policy loss: 6.7429704826540195
Experience 11, Iter 60, disc loss: 0.008317724563218155, policy loss: 6.554013158670946
Experience 11, Iter 61, disc loss: 0.006854609630759109, policy loss: 6.932609274159327
Experience 11, Iter 62, disc loss: 0.007188917217532368, policy loss: 6.407955354861424
Experience 11, Iter 63, disc loss: 0.007167084258467486, policy loss: 6.606060534435401
Experience 11, Iter 64, disc loss: 0.007698399048006184, policy loss: 6.549508564454342
Experience 11, Iter 65, disc loss: 0.008995599490204287, policy loss: 6.385441695687668
Experience 11, Iter 66, disc loss: 0.007225835939492824, policy loss: 6.958763285158296
Experience 11, Iter 67, disc loss: 0.007909578807872164, policy loss: 6.4498004647154055
Experience 11, Iter 68, disc loss: 0.006842459303831085, policy loss: 6.814877557872211
Experience 11, Iter 69, disc loss: 0.006608071251870882, policy loss: 6.6660171679290166
Experience 11, Iter 70, disc loss: 0.006806556481403209, policy loss: 6.549028149449262
Experience 11, Iter 71, disc loss: 0.007234813012141548, policy loss: 6.64371770552734
Experience 11, Iter 72, disc loss: 0.007943576858282776, policy loss: 6.220000753718007
Experience 11, Iter 73, disc loss: 0.006284865566783615, policy loss: 7.094873699305062
Experience 11, Iter 74, disc loss: 0.008706573476116263, policy loss: 5.987400299582106
Experience 11, Iter 75, disc loss: 0.007527078925436312, policy loss: 6.295881287441042
Experience 11, Iter 76, disc loss: 0.007010849370985689, policy loss: 6.501863878296369
Experience 11, Iter 77, disc loss: 0.006235012479058439, policy loss: 6.930089117670405
Experience 11, Iter 78, disc loss: 0.007898163149395251, policy loss: 6.192154426211969
Experience 11, Iter 79, disc loss: 0.009257558750387302, policy loss: 5.747188944958473
Experience 11, Iter 80, disc loss: 0.007209932182469081, policy loss: 6.531486429014338
Experience 11, Iter 81, disc loss: 0.008551613519703224, policy loss: 6.168293787454668
Experience 11, Iter 82, disc loss: 0.006598641626035418, policy loss: 6.512347651993423
Experience 11, Iter 83, disc loss: 0.008555463626852628, policy loss: 5.907013398595682
Experience 11, Iter 84, disc loss: 0.007982585089438585, policy loss: 6.155241246911067
Experience 11, Iter 85, disc loss: 0.0073720711286852664, policy loss: 6.514141700389566
Experience 11, Iter 86, disc loss: 0.00846692606026497, policy loss: 6.10484612965921
Experience 11, Iter 87, disc loss: 0.009505283832367997, policy loss: 6.146697275004522
Experience 11, Iter 88, disc loss: 0.0074244620074355614, policy loss: 6.6024187984148766
Experience 11, Iter 89, disc loss: 0.007462662970905967, policy loss: 6.319078342151585
Experience 11, Iter 90, disc loss: 0.007551537301824682, policy loss: 6.256206532867446
Experience 11, Iter 91, disc loss: 0.007175880762454068, policy loss: 7.016507485688472
Experience 11, Iter 92, disc loss: 0.008704932076474992, policy loss: 6.31572347214359
Experience 11, Iter 93, disc loss: 0.0064268780418225615, policy loss: 6.889451875807261
Experience 11, Iter 94, disc loss: 0.007542438038850597, policy loss: 6.541840194806531
Experience 11, Iter 95, disc loss: 0.006830443450152425, policy loss: 6.638476227437619
Experience 11, Iter 96, disc loss: 0.0070572431690512916, policy loss: 6.399770462555199
Experience 11, Iter 97, disc loss: 0.006413036950494772, policy loss: 6.855463716410254
Experience 11, Iter 98, disc loss: 0.007396840436979611, policy loss: 6.515706958699945
Experience 11, Iter 99, disc loss: 0.009713748258419724, policy loss: 5.818026812793377
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.1865],
        [1.6976],
        [0.0374]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0216, 0.1964, 1.6500, 0.0313, 0.0236, 4.5552]],

        [[0.0216, 0.1964, 1.6500, 0.0313, 0.0236, 4.5552]],

        [[0.0216, 0.1964, 1.6500, 0.0313, 0.0236, 4.5552]],

        [[0.0216, 0.1964, 1.6500, 0.0313, 0.0236, 4.5552]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0194, 0.7458, 6.7906, 0.1495], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0194, 0.7458, 6.7906, 0.1495])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.666
Iter 2/2000 - Loss: 3.600
Iter 3/2000 - Loss: 3.468
Iter 4/2000 - Loss: 3.426
Iter 5/2000 - Loss: 3.422
Iter 6/2000 - Loss: 3.329
Iter 7/2000 - Loss: 3.212
Iter 8/2000 - Loss: 3.119
Iter 9/2000 - Loss: 3.024
Iter 10/2000 - Loss: 2.900
Iter 11/2000 - Loss: 2.761
Iter 12/2000 - Loss: 2.616
Iter 13/2000 - Loss: 2.462
Iter 14/2000 - Loss: 2.286
Iter 15/2000 - Loss: 2.087
Iter 16/2000 - Loss: 1.876
Iter 17/2000 - Loss: 1.660
Iter 18/2000 - Loss: 1.434
Iter 19/2000 - Loss: 1.192
Iter 20/2000 - Loss: 0.930
Iter 1981/2000 - Loss: -6.424
Iter 1982/2000 - Loss: -6.424
Iter 1983/2000 - Loss: -6.424
Iter 1984/2000 - Loss: -6.424
Iter 1985/2000 - Loss: -6.424
Iter 1986/2000 - Loss: -6.424
Iter 1987/2000 - Loss: -6.424
Iter 1988/2000 - Loss: -6.424
Iter 1989/2000 - Loss: -6.424
Iter 1990/2000 - Loss: -6.424
Iter 1991/2000 - Loss: -6.424
Iter 1992/2000 - Loss: -6.424
Iter 1993/2000 - Loss: -6.424
Iter 1994/2000 - Loss: -6.425
Iter 1995/2000 - Loss: -6.425
Iter 1996/2000 - Loss: -6.425
Iter 1997/2000 - Loss: -6.425
Iter 1998/2000 - Loss: -6.425
Iter 1999/2000 - Loss: -6.425
Iter 2000/2000 - Loss: -6.425
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0031],
        [0.0004]])
Lengthscale: tensor([[[11.9049,  5.5679, 20.5369,  2.0578, 18.4350, 45.3711]],

        [[17.9650, 33.4874,  7.0265,  1.0808,  2.8622, 24.4434]],

        [[21.3915, 37.6381,  9.0587,  0.9763,  0.8139, 22.5457]],

        [[19.2026, 33.6079, 18.0817,  2.3224,  2.0921, 43.1125]]])
Signal Variance: tensor([ 0.0567,  1.6875, 12.4162,  0.6723])
Estimated target variance: tensor([0.0194, 0.7458, 6.7906, 0.1495])
N: 120
Signal to noise ratio: tensor([14.0317, 65.5351, 63.2111, 43.5311])
Bound on condition number: tensor([ 23627.7651, 515382.1377, 479478.0071, 227395.9178])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.006986923452659124, policy loss: 7.072929175618272
Experience 12, Iter 1, disc loss: 0.00770124683258654, policy loss: 6.328523552841433
Experience 12, Iter 2, disc loss: 0.005669603504853857, policy loss: 6.924321613767971
Experience 12, Iter 3, disc loss: 0.006195115739802544, policy loss: 6.908279156543653
Experience 12, Iter 4, disc loss: 0.006491501825202308, policy loss: 6.830967456928739
Experience 12, Iter 5, disc loss: 0.007317202021364255, policy loss: 6.247467330921765
Experience 12, Iter 6, disc loss: 0.007835211791634322, policy loss: 6.013464934587916
Experience 12, Iter 7, disc loss: 0.006233199131059302, policy loss: 6.9832153508529
Experience 12, Iter 8, disc loss: 0.007564382537447678, policy loss: 6.168658271184163
Experience 12, Iter 9, disc loss: 0.0069086191426796315, policy loss: 6.406258166399484
Experience 12, Iter 10, disc loss: 0.006553138661103004, policy loss: 6.502301857186147
Experience 12, Iter 11, disc loss: 0.006141766162649053, policy loss: 6.619787525068385
Experience 12, Iter 12, disc loss: 0.006154364580899574, policy loss: 7.118101638924743
Experience 12, Iter 13, disc loss: 0.008841600190205655, policy loss: 5.862695582376242
Experience 12, Iter 14, disc loss: 0.006800407119509246, policy loss: 6.894003518662322
Experience 12, Iter 15, disc loss: 0.00927753456348375, policy loss: 5.777255983516882
Experience 12, Iter 16, disc loss: 0.0071474102580271134, policy loss: 6.41550303530388
Experience 12, Iter 17, disc loss: 0.006474423682828699, policy loss: 6.361133554503269
Experience 12, Iter 18, disc loss: 0.006383574710524611, policy loss: 6.376803447745475
Experience 12, Iter 19, disc loss: 0.006871744103811219, policy loss: 6.761679686548607
Experience 12, Iter 20, disc loss: 0.009532011188121503, policy loss: 5.997755102520291
Experience 12, Iter 21, disc loss: 0.007645622124223102, policy loss: 6.530504499280492
Experience 12, Iter 22, disc loss: 0.008161714972377457, policy loss: 6.0687585223269105
Experience 12, Iter 23, disc loss: 0.007517510275193355, policy loss: 6.763409468257581
Experience 12, Iter 24, disc loss: 0.0072520597970813575, policy loss: 6.267147718369884
Experience 12, Iter 25, disc loss: 0.006770296118442583, policy loss: 6.491716635460712
Experience 12, Iter 26, disc loss: 0.006584709168266559, policy loss: 7.139625153781866
Experience 12, Iter 27, disc loss: 0.007693044590296225, policy loss: 6.186216919493
Experience 12, Iter 28, disc loss: 0.007592165175190943, policy loss: 6.340060467556804
Experience 12, Iter 29, disc loss: 0.006605454633702368, policy loss: 6.629430802642699
Experience 12, Iter 30, disc loss: 0.007587916159533797, policy loss: 6.429932028445291
Experience 12, Iter 31, disc loss: 0.007347855467751394, policy loss: 6.407419914982273
Experience 12, Iter 32, disc loss: 0.007106066542046499, policy loss: 6.441950415945218
Experience 12, Iter 33, disc loss: 0.007474462322496265, policy loss: 6.4322599573164965
Experience 12, Iter 34, disc loss: 0.006853075129330965, policy loss: 6.759910453965356
Experience 12, Iter 35, disc loss: 0.006898330623213736, policy loss: 6.572261313946054
Experience 12, Iter 36, disc loss: 0.006794031575712775, policy loss: 6.871294486337683
Experience 12, Iter 37, disc loss: 0.0070372428041352774, policy loss: 6.64009850780489
Experience 12, Iter 38, disc loss: 0.007716285802980348, policy loss: 6.2916074855357955
Experience 12, Iter 39, disc loss: 0.006476751974149365, policy loss: 6.783931274172828
Experience 12, Iter 40, disc loss: 0.007913271675653706, policy loss: 6.024654446466008
Experience 12, Iter 41, disc loss: 0.006651461086518885, policy loss: 6.668226280160851
Experience 12, Iter 42, disc loss: 0.006972661545912446, policy loss: 6.795171768052791
Experience 12, Iter 43, disc loss: 0.007213329188701939, policy loss: 6.610866081477372
Experience 12, Iter 44, disc loss: 0.007163498161443637, policy loss: 6.612146430046355
Experience 12, Iter 45, disc loss: 0.007020268733707968, policy loss: 6.490817638400523
Experience 12, Iter 46, disc loss: 0.007428814480178744, policy loss: 6.410216023920031
Experience 12, Iter 47, disc loss: 0.006743498923971377, policy loss: 6.452445197751259
Experience 12, Iter 48, disc loss: 0.006962938303001005, policy loss: 6.812170654567606
Experience 12, Iter 49, disc loss: 0.006902593925253846, policy loss: 6.683031419021848
Experience 12, Iter 50, disc loss: 0.006460487283792971, policy loss: 6.612251170870915
Experience 12, Iter 51, disc loss: 0.006408107420436942, policy loss: 6.625127422757185
Experience 12, Iter 52, disc loss: 0.0070264971947068275, policy loss: 6.662657180677704
Experience 12, Iter 53, disc loss: 0.0070035050338187336, policy loss: 6.415876225064595
Experience 12, Iter 54, disc loss: 0.006965214678222794, policy loss: 6.626481788640417
Experience 12, Iter 55, disc loss: 0.00811915729647042, policy loss: 6.140808474712238
Experience 12, Iter 56, disc loss: 0.007135328744501035, policy loss: 6.252390573823153
Experience 12, Iter 57, disc loss: 0.007095137932482192, policy loss: 6.482603395525176
Experience 12, Iter 58, disc loss: 0.007154184964655227, policy loss: 6.622338812730394
Experience 12, Iter 59, disc loss: 0.006121432886121285, policy loss: 6.629656484873427
Experience 12, Iter 60, disc loss: 0.006714736233093033, policy loss: 6.525925664427302
Experience 12, Iter 61, disc loss: 0.006994381658930016, policy loss: 6.710582040049977
Experience 12, Iter 62, disc loss: 0.007066106211160418, policy loss: 6.437057002548004
Experience 12, Iter 63, disc loss: 0.007574221573315589, policy loss: 6.267627777143114
Experience 12, Iter 64, disc loss: 0.006082435795539189, policy loss: 6.565055142744072
Experience 12, Iter 65, disc loss: 0.0063003594314312746, policy loss: 6.585081820413507
Experience 12, Iter 66, disc loss: 0.006566272249120213, policy loss: 6.702796685152597
Experience 12, Iter 67, disc loss: 0.007607322594068711, policy loss: 6.183713474288449
Experience 12, Iter 68, disc loss: 0.00752232151577565, policy loss: 6.695678584236514
Experience 12, Iter 69, disc loss: 0.007244185736925637, policy loss: 6.475367350437919
Experience 12, Iter 70, disc loss: 0.006600716235725822, policy loss: 6.5850486819390115
Experience 12, Iter 71, disc loss: 0.006492740712158046, policy loss: 6.718110547653263
Experience 12, Iter 72, disc loss: 0.008004107864859248, policy loss: 6.301941876719139
Experience 12, Iter 73, disc loss: 0.008083656371653251, policy loss: 6.148241458756425
Experience 12, Iter 74, disc loss: 0.007222440630420156, policy loss: 6.403342167550534
Experience 12, Iter 75, disc loss: 0.006430096825123268, policy loss: 6.490430296213203
Experience 12, Iter 76, disc loss: 0.007148468816482498, policy loss: 6.482175938965875
Experience 12, Iter 77, disc loss: 0.007076077221719672, policy loss: 6.4521143224621404
Experience 12, Iter 78, disc loss: 0.0070240172284834965, policy loss: 6.608392274806288
Experience 12, Iter 79, disc loss: 0.0067578751100767775, policy loss: 6.627644423675894
Experience 12, Iter 80, disc loss: 0.006057732936831309, policy loss: 7.061448095289917
Experience 12, Iter 81, disc loss: 0.006162018355254457, policy loss: 6.835786616612609
Experience 12, Iter 82, disc loss: 0.006728861721953198, policy loss: 6.542047263815668
Experience 12, Iter 83, disc loss: 0.006882136774269935, policy loss: 6.683797988290808
Experience 12, Iter 84, disc loss: 0.0062828421035199785, policy loss: 6.6862612096753695
Experience 12, Iter 85, disc loss: 0.006343238389018681, policy loss: 6.863214137249257
Experience 12, Iter 86, disc loss: 0.0061590166600697036, policy loss: 6.813005894493152
Experience 12, Iter 87, disc loss: 0.0065980430129623565, policy loss: 6.660842040333138
Experience 12, Iter 88, disc loss: 0.006527022711998114, policy loss: 6.732478182201656
Experience 12, Iter 89, disc loss: 0.006532400747116562, policy loss: 6.178182149073878
Experience 12, Iter 90, disc loss: 0.006370139724234439, policy loss: 6.456653022184702
Experience 12, Iter 91, disc loss: 0.00631310308905705, policy loss: 6.359140985430524
Experience 12, Iter 92, disc loss: 0.006589174932296394, policy loss: 6.44118264562583
Experience 12, Iter 93, disc loss: 0.006014473361283995, policy loss: 6.756978159726527
Experience 12, Iter 94, disc loss: 0.007083528910800327, policy loss: 6.397918228173892
Experience 12, Iter 95, disc loss: 0.006023718539118595, policy loss: 6.5821416380416995
Experience 12, Iter 96, disc loss: 0.006563595106075255, policy loss: 6.39217919192031
Experience 12, Iter 97, disc loss: 0.008000152973972517, policy loss: 6.4547986013200696
Experience 12, Iter 98, disc loss: 0.006628006033823225, policy loss: 6.852306256542455
Experience 12, Iter 99, disc loss: 0.0063544789488074686, policy loss: 6.555912841565899
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0047],
        [0.1937],
        [1.7174],
        [0.0375]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0203, 0.1931, 1.6647, 0.0315, 0.0246, 4.7787]],

        [[0.0203, 0.1931, 1.6647, 0.0315, 0.0246, 4.7787]],

        [[0.0203, 0.1931, 1.6647, 0.0315, 0.0246, 4.7787]],

        [[0.0203, 0.1931, 1.6647, 0.0315, 0.0246, 4.7787]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0190, 0.7749, 6.8696, 0.1500], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0190, 0.7749, 6.8696, 0.1500])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.687
Iter 2/2000 - Loss: 3.630
Iter 3/2000 - Loss: 3.480
Iter 4/2000 - Loss: 3.438
Iter 5/2000 - Loss: 3.432
Iter 6/2000 - Loss: 3.332
Iter 7/2000 - Loss: 3.205
Iter 8/2000 - Loss: 3.103
Iter 9/2000 - Loss: 2.997
Iter 10/2000 - Loss: 2.861
Iter 11/2000 - Loss: 2.708
Iter 12/2000 - Loss: 2.552
Iter 13/2000 - Loss: 2.387
Iter 14/2000 - Loss: 2.200
Iter 15/2000 - Loss: 1.988
Iter 16/2000 - Loss: 1.761
Iter 17/2000 - Loss: 1.528
Iter 18/2000 - Loss: 1.289
Iter 19/2000 - Loss: 1.037
Iter 20/2000 - Loss: 0.770
Iter 1981/2000 - Loss: -6.533
Iter 1982/2000 - Loss: -6.533
Iter 1983/2000 - Loss: -6.533
Iter 1984/2000 - Loss: -6.533
Iter 1985/2000 - Loss: -6.533
Iter 1986/2000 - Loss: -6.533
Iter 1987/2000 - Loss: -6.533
Iter 1988/2000 - Loss: -6.533
Iter 1989/2000 - Loss: -6.533
Iter 1990/2000 - Loss: -6.533
Iter 1991/2000 - Loss: -6.533
Iter 1992/2000 - Loss: -6.533
Iter 1993/2000 - Loss: -6.533
Iter 1994/2000 - Loss: -6.533
Iter 1995/2000 - Loss: -6.533
Iter 1996/2000 - Loss: -6.533
Iter 1997/2000 - Loss: -6.533
Iter 1998/2000 - Loss: -6.534
Iter 1999/2000 - Loss: -6.534
Iter 2000/2000 - Loss: -6.534
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0030],
        [0.0004]])
Lengthscale: tensor([[[12.1638,  5.5022, 20.0238,  1.9109, 18.0181, 44.5294]],

        [[17.3935, 34.3859,  7.7517,  1.0162,  2.0110, 28.3243]],

        [[21.3293, 37.3066,  8.5240,  0.9813,  0.7616, 24.0456]],

        [[18.4376, 32.1456, 18.4940,  2.0317,  2.0258, 44.7533]]])
Signal Variance: tensor([ 0.0561,  1.9966, 12.5390,  0.6190])
Estimated target variance: tensor([0.0190, 0.7749, 6.8696, 0.1500])
N: 130
Signal to noise ratio: tensor([13.9338, 73.8717, 64.6273, 39.9860])
Bound on condition number: tensor([ 25240.7169, 709415.2440, 542970.4198, 207855.4065])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.006238482165907326, policy loss: 7.056228588085178
Experience 13, Iter 1, disc loss: 0.005360340286972888, policy loss: 7.257022786862802
Experience 13, Iter 2, disc loss: 0.0056986008965793054, policy loss: 7.1495424566749985
Experience 13, Iter 3, disc loss: 0.005625703547221537, policy loss: 6.9194250149794545
Experience 13, Iter 4, disc loss: 0.005924240833674953, policy loss: 6.639395867931544
Experience 13, Iter 5, disc loss: 0.006178257394355116, policy loss: 6.932773987905931
Experience 13, Iter 6, disc loss: 0.005871100713067056, policy loss: 6.73333922296165
Experience 13, Iter 7, disc loss: 0.004891529235465092, policy loss: 7.350640335845691
Experience 13, Iter 8, disc loss: 0.005842977619548944, policy loss: 6.864779647435994
Experience 13, Iter 9, disc loss: 0.006040190289919827, policy loss: 6.549104625570915
Experience 13, Iter 10, disc loss: 0.005899570520084527, policy loss: 6.781478122916829
Experience 13, Iter 11, disc loss: 0.005287816029406778, policy loss: 6.917889073894871
Experience 13, Iter 12, disc loss: 0.005476305720362666, policy loss: 6.929958469251635
Experience 13, Iter 13, disc loss: 0.005554642539575038, policy loss: 6.995286217573758
Experience 13, Iter 14, disc loss: 0.005076772316632663, policy loss: 6.947555953565528
Experience 13, Iter 15, disc loss: 0.005383325195313997, policy loss: 6.807908294202152
Experience 13, Iter 16, disc loss: 0.005687667108680867, policy loss: 6.9134034785155425
Experience 13, Iter 17, disc loss: 0.005759067825286143, policy loss: 6.599074809497562
Experience 13, Iter 18, disc loss: 0.005154148951077681, policy loss: 7.029377874019779
Experience 13, Iter 19, disc loss: 0.0054054835320195005, policy loss: 6.91407477136734
Experience 13, Iter 20, disc loss: 0.005392843554409986, policy loss: 7.163588331126245
Experience 13, Iter 21, disc loss: 0.005549755307617387, policy loss: 6.754201578320376
Experience 13, Iter 22, disc loss: 0.006151595505541764, policy loss: 6.559002333197418
Experience 13, Iter 23, disc loss: 0.004869127888003241, policy loss: 6.9973719544343265
Experience 13, Iter 24, disc loss: 0.0053589472852102514, policy loss: 6.67829535607468
Experience 13, Iter 25, disc loss: 0.005107824095744301, policy loss: 6.88646793230183
Experience 13, Iter 26, disc loss: 0.005076451146950041, policy loss: 6.838890351932395
Experience 13, Iter 27, disc loss: 0.00554838630768879, policy loss: 6.72727339625917
Experience 13, Iter 28, disc loss: 0.005212234242481398, policy loss: 7.017298373574561
Experience 13, Iter 29, disc loss: 0.004887335605050429, policy loss: 7.660931385156658
Experience 13, Iter 30, disc loss: 0.005731079182520522, policy loss: 6.78679729074064
Experience 13, Iter 31, disc loss: 0.005391783759021447, policy loss: 6.800895928337089
Experience 13, Iter 32, disc loss: 0.005651040633097886, policy loss: 6.599006273018408
Experience 13, Iter 33, disc loss: 0.005404478805085718, policy loss: 6.721722686693711
Experience 13, Iter 34, disc loss: 0.0049045754873743115, policy loss: 6.6537646109747115
Experience 13, Iter 35, disc loss: 0.005005830312150614, policy loss: 7.024849229830556
Experience 13, Iter 36, disc loss: 0.005368711157046734, policy loss: 6.715457888022177
Experience 13, Iter 37, disc loss: 0.005635689952986611, policy loss: 7.043279066245117
Experience 13, Iter 38, disc loss: 0.005715752933758203, policy loss: 6.644096464598235
Experience 13, Iter 39, disc loss: 0.004835857237570226, policy loss: 6.859046308228562
Experience 13, Iter 40, disc loss: 0.0050600005423974935, policy loss: 6.692104477175048
Experience 13, Iter 41, disc loss: 0.0046488835226587925, policy loss: 7.119255619440801
Experience 13, Iter 42, disc loss: 0.0047900455674820645, policy loss: 6.82544140245629
Experience 13, Iter 43, disc loss: 0.005354671653684335, policy loss: 6.473206676869648
Experience 13, Iter 44, disc loss: 0.004571530445904713, policy loss: 7.0087237762465335
Experience 13, Iter 45, disc loss: 0.005462458334137867, policy loss: 7.003650316509426
Experience 13, Iter 46, disc loss: 0.005825637043123746, policy loss: 6.764741718615501
Experience 13, Iter 47, disc loss: 0.005084885337092037, policy loss: 6.606842693170503
Experience 13, Iter 48, disc loss: 0.005627724589063357, policy loss: 7.0216452688208975
Experience 13, Iter 49, disc loss: 0.005719326232818337, policy loss: 6.612812251504131
Experience 13, Iter 50, disc loss: 0.005261988378543546, policy loss: 6.873247331369453
Experience 13, Iter 51, disc loss: 0.0053409787525417725, policy loss: 6.907974173444788
Experience 13, Iter 52, disc loss: 0.005280482433648495, policy loss: 6.737898038275453
Experience 13, Iter 53, disc loss: 0.004529869556208723, policy loss: 7.031189372668761
Experience 13, Iter 54, disc loss: 0.005287699876432647, policy loss: 6.739573041799842
Experience 13, Iter 55, disc loss: 0.0049571490159832764, policy loss: 6.959707414114092
Experience 13, Iter 56, disc loss: 0.0054143128230672265, policy loss: 6.767865743305692
Experience 13, Iter 57, disc loss: 0.005707924002420792, policy loss: 6.675074890478399
Experience 13, Iter 58, disc loss: 0.00513516533628589, policy loss: 6.675511120901114
Experience 13, Iter 59, disc loss: 0.005111228768017213, policy loss: 7.131824271461377
Experience 13, Iter 60, disc loss: 0.004846625304786253, policy loss: 7.149631964990633
Experience 13, Iter 61, disc loss: 0.005640193271456453, policy loss: 6.612564258679236
Experience 13, Iter 62, disc loss: 0.005246086081070881, policy loss: 6.956618834860685
Experience 13, Iter 63, disc loss: 0.004624819947628224, policy loss: 7.2786910112169245
Experience 13, Iter 64, disc loss: 0.0060886875548598965, policy loss: 6.347573719118667
Experience 13, Iter 65, disc loss: 0.005102686391241076, policy loss: 7.032849077793107
Experience 13, Iter 66, disc loss: 0.005079104034522621, policy loss: 7.337596301427231
Experience 13, Iter 67, disc loss: 0.0058346400531978935, policy loss: 6.7472812804131035
Experience 13, Iter 68, disc loss: 0.005836147015161199, policy loss: 7.058031370586598
Experience 13, Iter 69, disc loss: 0.00484026813867605, policy loss: 7.380879388410025
Experience 13, Iter 70, disc loss: 0.005570994234450767, policy loss: 6.549648555688459
Experience 13, Iter 71, disc loss: 0.005461076827286878, policy loss: 7.081386364699634
Experience 13, Iter 72, disc loss: 0.00506510103119294, policy loss: 7.02830603942297
Experience 13, Iter 73, disc loss: 0.004373655091959007, policy loss: 7.190352773088025
Experience 13, Iter 74, disc loss: 0.004723154274030569, policy loss: 7.1197633993881
Experience 13, Iter 75, disc loss: 0.005046201088797633, policy loss: 6.8948496868258
Experience 13, Iter 76, disc loss: 0.004848224291062927, policy loss: 7.1918204019786725
Experience 13, Iter 77, disc loss: 0.004921859586731704, policy loss: 7.171841340561419
Experience 13, Iter 78, disc loss: 0.004396635749882034, policy loss: 6.880282864989088
Experience 13, Iter 79, disc loss: 0.004410052473001301, policy loss: 7.031250671340835
Experience 13, Iter 80, disc loss: 0.004120718779867964, policy loss: 7.3500761163105235
Experience 13, Iter 81, disc loss: 0.00472981962597795, policy loss: 6.966012053239204
Experience 13, Iter 82, disc loss: 0.005360041441168406, policy loss: 6.71914599113029
Experience 13, Iter 83, disc loss: 0.004028567880303942, policy loss: 7.5959771310137
Experience 13, Iter 84, disc loss: 0.004193631732222572, policy loss: 7.206073017361238
Experience 13, Iter 85, disc loss: 0.004184425475937878, policy loss: 7.127246471424835
Experience 13, Iter 86, disc loss: 0.0037129087382956547, policy loss: 7.451524127694941
Experience 13, Iter 87, disc loss: 0.003866381745794581, policy loss: 7.42228743991145
Experience 13, Iter 88, disc loss: 0.004656980186850169, policy loss: 7.148232390156496
Experience 13, Iter 89, disc loss: 0.004242198986001441, policy loss: 7.631353469805481
Experience 13, Iter 90, disc loss: 0.004648551578558948, policy loss: 7.07455932657612
Experience 13, Iter 91, disc loss: 0.004067244694251677, policy loss: 6.969987888545979
Experience 13, Iter 92, disc loss: 0.0038506226996656845, policy loss: 7.122219090579307
Experience 13, Iter 93, disc loss: 0.003807359081360915, policy loss: 7.275140873036708
Experience 13, Iter 94, disc loss: 0.0042990760408117825, policy loss: 7.046075831502718
Experience 13, Iter 95, disc loss: 0.004058004705817192, policy loss: 7.524846019280566
Experience 13, Iter 96, disc loss: 0.004010651224193373, policy loss: 7.386609341109224
Experience 13, Iter 97, disc loss: 0.004389592097119921, policy loss: 6.975290502596331
Experience 13, Iter 98, disc loss: 0.0035384955641623644, policy loss: 7.407024853564393
Experience 13, Iter 99, disc loss: 0.003301251292668574, policy loss: 7.358091475250923
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0047],
        [0.2055],
        [1.7823],
        [0.0386]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0192, 0.1926, 1.7245, 0.0319, 0.0254, 5.0234]],

        [[0.0192, 0.1926, 1.7245, 0.0319, 0.0254, 5.0234]],

        [[0.0192, 0.1926, 1.7245, 0.0319, 0.0254, 5.0234]],

        [[0.0192, 0.1926, 1.7245, 0.0319, 0.0254, 5.0234]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0189, 0.8218, 7.1292, 0.1542], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0189, 0.8218, 7.1292, 0.1542])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.753
Iter 2/2000 - Loss: 3.698
Iter 3/2000 - Loss: 3.543
Iter 4/2000 - Loss: 3.500
Iter 5/2000 - Loss: 3.496
Iter 6/2000 - Loss: 3.398
Iter 7/2000 - Loss: 3.271
Iter 8/2000 - Loss: 3.166
Iter 9/2000 - Loss: 3.057
Iter 10/2000 - Loss: 2.917
Iter 11/2000 - Loss: 2.759
Iter 12/2000 - Loss: 2.596
Iter 13/2000 - Loss: 2.424
Iter 14/2000 - Loss: 2.228
Iter 15/2000 - Loss: 2.005
Iter 16/2000 - Loss: 1.766
Iter 17/2000 - Loss: 1.518
Iter 18/2000 - Loss: 1.265
Iter 19/2000 - Loss: 1.002
Iter 20/2000 - Loss: 0.724
Iter 1981/2000 - Loss: -6.674
Iter 1982/2000 - Loss: -6.674
Iter 1983/2000 - Loss: -6.674
Iter 1984/2000 - Loss: -6.674
Iter 1985/2000 - Loss: -6.674
Iter 1986/2000 - Loss: -6.675
Iter 1987/2000 - Loss: -6.675
Iter 1988/2000 - Loss: -6.675
Iter 1989/2000 - Loss: -6.675
Iter 1990/2000 - Loss: -6.675
Iter 1991/2000 - Loss: -6.675
Iter 1992/2000 - Loss: -6.675
Iter 1993/2000 - Loss: -6.675
Iter 1994/2000 - Loss: -6.675
Iter 1995/2000 - Loss: -6.675
Iter 1996/2000 - Loss: -6.675
Iter 1997/2000 - Loss: -6.675
Iter 1998/2000 - Loss: -6.675
Iter 1999/2000 - Loss: -6.675
Iter 2000/2000 - Loss: -6.675
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[12.2111,  5.5520, 18.9643,  1.8806, 17.3712, 46.2506]],

        [[19.5847, 34.4475,  7.9492,  1.0275,  1.8003, 28.3924]],

        [[21.1860, 37.5437,  8.5417,  0.9343,  0.7979, 24.4477]],

        [[17.5601, 31.1943, 17.4568,  1.9396,  1.9393, 45.4773]]])
Signal Variance: tensor([ 0.0573,  1.9195, 12.8276,  0.5718])
Estimated target variance: tensor([0.0189, 0.8218, 7.1292, 0.1542])
N: 140
Signal to noise ratio: tensor([13.9060, 71.2698, 66.2236, 38.3331])
Bound on condition number: tensor([ 27073.6621, 711114.6385, 613980.7532, 205720.2874])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.003599527436429938, policy loss: 7.552708630026938
Experience 14, Iter 1, disc loss: 0.004755766060489956, policy loss: 6.867001554419826
Experience 14, Iter 2, disc loss: 0.004996587253925777, policy loss: 6.72335006682489
Experience 14, Iter 3, disc loss: 0.0038259049987444185, policy loss: 7.698334177587135
Experience 14, Iter 4, disc loss: 0.004433970953612674, policy loss: 7.06641265488949
Experience 14, Iter 5, disc loss: 0.003763218845500175, policy loss: 7.304567164435088
Experience 14, Iter 6, disc loss: 0.003090763908413132, policy loss: 7.42782402543977
Experience 14, Iter 7, disc loss: 0.0031618630304985776, policy loss: 7.8391971800934215
Experience 14, Iter 8, disc loss: 0.002964758197171691, policy loss: 7.633608263130878
Experience 14, Iter 9, disc loss: 0.003946788070289493, policy loss: 7.462030784809188
Experience 14, Iter 10, disc loss: 0.003811497489697852, policy loss: 7.111626697455973
Experience 14, Iter 11, disc loss: 0.0031043641392667753, policy loss: 7.962916753143716
Experience 14, Iter 12, disc loss: 0.0045176316401337934, policy loss: 6.830625773498479
Experience 14, Iter 13, disc loss: 0.0038727958840770104, policy loss: 7.418771643080838
Experience 14, Iter 14, disc loss: 0.0030156939026624065, policy loss: 7.661176726833753
Experience 14, Iter 15, disc loss: 0.00329966261954122, policy loss: 7.709211009268457
Experience 14, Iter 16, disc loss: 0.0032799702173669618, policy loss: 7.4424555398201875
Experience 14, Iter 17, disc loss: 0.0036878334808808115, policy loss: 7.615479687973745
Experience 14, Iter 18, disc loss: 0.004755813683716385, policy loss: 6.632843734053086
Experience 14, Iter 19, disc loss: 0.003415719935437151, policy loss: 8.177623925735936
Experience 14, Iter 20, disc loss: 0.004781808604604703, policy loss: 6.592692637358674
Experience 14, Iter 21, disc loss: 0.004550545400650621, policy loss: 6.98782969592736
Experience 14, Iter 22, disc loss: 0.004005973598766449, policy loss: 7.029741139765251
Experience 14, Iter 23, disc loss: 0.0037257964709225353, policy loss: 6.7891834197533
Experience 14, Iter 24, disc loss: 0.003730030947489461, policy loss: 7.019671573718907
Experience 14, Iter 25, disc loss: 0.004095742307299321, policy loss: 7.305823743510542
Experience 14, Iter 26, disc loss: 0.004811929787947247, policy loss: 6.764720797000702
Experience 14, Iter 27, disc loss: 0.004456671808495071, policy loss: 7.115292415750757
Experience 14, Iter 28, disc loss: 0.004227394547091965, policy loss: 7.00824772385894
Experience 14, Iter 29, disc loss: 0.003795256901843591, policy loss: 7.483654650793081
Experience 14, Iter 30, disc loss: 0.003258222164951678, policy loss: 7.542849621307439
Experience 14, Iter 31, disc loss: 0.003863863437864038, policy loss: 7.182748721944432
Experience 14, Iter 32, disc loss: 0.004724972700984033, policy loss: 7.104076643405859
Experience 14, Iter 33, disc loss: 0.004782233983324129, policy loss: 7.019141353988449
Experience 14, Iter 34, disc loss: 0.0043844974177552985, policy loss: 7.01152578215157
Experience 14, Iter 35, disc loss: 0.0042406848722655495, policy loss: 7.226036840349411
Experience 14, Iter 36, disc loss: 0.004263328096256039, policy loss: 6.890246713711415
Experience 14, Iter 37, disc loss: 0.00413796580642538, policy loss: 7.395604883434648
Experience 14, Iter 38, disc loss: 0.004522244261789087, policy loss: 7.172945276959758
Experience 14, Iter 39, disc loss: 0.004444569659141919, policy loss: 7.123844471001227
Experience 14, Iter 40, disc loss: 0.0035002186776386295, policy loss: 7.464887582284599
Experience 14, Iter 41, disc loss: 0.004700043426073363, policy loss: 6.625223981414459
Experience 14, Iter 42, disc loss: 0.004559247701627936, policy loss: 6.872822165012586
Experience 14, Iter 43, disc loss: 0.004624238059788067, policy loss: 6.953655037363257
Experience 14, Iter 44, disc loss: 0.004126491678750439, policy loss: 7.478604259783355
Experience 14, Iter 45, disc loss: 0.00427923497516439, policy loss: 7.211932550483524
Experience 14, Iter 46, disc loss: 0.004035397128850938, policy loss: 7.397738671874474
Experience 14, Iter 47, disc loss: 0.004502764223191806, policy loss: 6.754823503145296
Experience 14, Iter 48, disc loss: 0.004123171761951832, policy loss: 7.252046287508567
Experience 14, Iter 49, disc loss: 0.0042997019202343665, policy loss: 7.442555142348469
Experience 14, Iter 50, disc loss: 0.004454015431512323, policy loss: 7.354501001074201
Experience 14, Iter 51, disc loss: 0.004314789441415332, policy loss: 6.900399683948649
Experience 14, Iter 52, disc loss: 0.0037359519214237168, policy loss: 7.70204661965794
Experience 14, Iter 53, disc loss: 0.004427633933095934, policy loss: 6.789061554221758
Experience 14, Iter 54, disc loss: 0.003399398513899951, policy loss: 7.8061441272416365
Experience 14, Iter 55, disc loss: 0.0035352902532685623, policy loss: 7.281217288542151
Experience 14, Iter 56, disc loss: 0.003894598360987921, policy loss: 7.134565720403489
Experience 14, Iter 57, disc loss: 0.0038754276652655443, policy loss: 7.428271987683236
Experience 14, Iter 58, disc loss: 0.0033710995221881888, policy loss: 7.688748748343728
Experience 14, Iter 59, disc loss: 0.004288057115620545, policy loss: 7.113841128332028
Experience 14, Iter 60, disc loss: 0.004750826582285343, policy loss: 6.76171436740078
Experience 14, Iter 61, disc loss: 0.004356461221133598, policy loss: 7.5963007710960175
Experience 14, Iter 62, disc loss: 0.004813136479884857, policy loss: 6.93257177538384
Experience 14, Iter 63, disc loss: 0.003920071939737711, policy loss: 7.256096613211021
Experience 14, Iter 64, disc loss: 0.004395934655526581, policy loss: 7.03997658853395
Experience 14, Iter 65, disc loss: 0.004695010233139842, policy loss: 6.915616979674214
Experience 14, Iter 66, disc loss: 0.00401175984864789, policy loss: 7.124899995096525
Experience 14, Iter 67, disc loss: 0.004031085742495012, policy loss: 6.940190405361173
Experience 14, Iter 68, disc loss: 0.004229476244069956, policy loss: 6.854992151543517
Experience 14, Iter 69, disc loss: 0.004447761603655972, policy loss: 6.938219306868378
Experience 14, Iter 70, disc loss: 0.003936209366204837, policy loss: 7.40416598524056
Experience 14, Iter 71, disc loss: 0.004563345499495511, policy loss: 7.023054065458015
Experience 14, Iter 72, disc loss: 0.004547935716474858, policy loss: 6.795000980763765
Experience 14, Iter 73, disc loss: 0.003438177281963999, policy loss: 7.977199049738299
Experience 14, Iter 74, disc loss: 0.0036314173215821924, policy loss: 7.575047467985223
Experience 14, Iter 75, disc loss: 0.003881291071824093, policy loss: 7.154033774739874
Experience 14, Iter 76, disc loss: 0.004065335303327724, policy loss: 7.2961308233933275
Experience 14, Iter 77, disc loss: 0.0034790799594145803, policy loss: 7.652349484739744
Experience 14, Iter 78, disc loss: 0.0040098198086241325, policy loss: 7.141810338380585
Experience 14, Iter 79, disc loss: 0.00386312822062299, policy loss: 7.315514669822401
Experience 14, Iter 80, disc loss: 0.004264094842057715, policy loss: 6.982146909240889
Experience 14, Iter 81, disc loss: 0.0037033643051218, policy loss: 7.486108199469043
Experience 14, Iter 82, disc loss: 0.004032099373093339, policy loss: 7.305352709129163
Experience 14, Iter 83, disc loss: 0.00472556207519975, policy loss: 7.069503302643243
Experience 14, Iter 84, disc loss: 0.003964177693163231, policy loss: 7.2027456342140015
Experience 14, Iter 85, disc loss: 0.00405425761467117, policy loss: 7.3538718405719
Experience 14, Iter 86, disc loss: 0.003970122031964592, policy loss: 7.049194657778388
Experience 14, Iter 87, disc loss: 0.004070864772216228, policy loss: 7.519011841953569
Experience 14, Iter 88, disc loss: 0.0043761467326026565, policy loss: 7.228650563491337
Experience 14, Iter 89, disc loss: 0.004053243410281282, policy loss: 7.129962772363434
Experience 14, Iter 90, disc loss: 0.0041158688700106555, policy loss: 7.1837829141926814
Experience 14, Iter 91, disc loss: 0.004047118337123369, policy loss: 7.088432068738278
Experience 14, Iter 92, disc loss: 0.003748203345198641, policy loss: 7.555737793665119
Experience 14, Iter 93, disc loss: 0.003823213965070289, policy loss: 7.149304869518328
Experience 14, Iter 94, disc loss: 0.003770415343279125, policy loss: 7.343821483369897
Experience 14, Iter 95, disc loss: 0.003592165420991395, policy loss: 7.52543947270051
Experience 14, Iter 96, disc loss: 0.003932522158017202, policy loss: 7.42412897703649
Experience 14, Iter 97, disc loss: 0.0035156351498318627, policy loss: 7.448990536648481
Experience 14, Iter 98, disc loss: 0.0041789161790635465, policy loss: 6.928875356622597
Experience 14, Iter 99, disc loss: 0.0033332400816027527, policy loss: 8.112629093734823
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.2125],
        [1.8168],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0182, 0.1911, 1.7443, 0.0320, 0.0261, 5.2090]],

        [[0.0182, 0.1911, 1.7443, 0.0320, 0.0261, 5.2090]],

        [[0.0182, 0.1911, 1.7443, 0.0320, 0.0261, 5.2090]],

        [[0.0182, 0.1911, 1.7443, 0.0320, 0.0261, 5.2090]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0186, 0.8499, 7.2673, 0.1553], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0186, 0.8499, 7.2673, 0.1553])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.771
Iter 2/2000 - Loss: 3.721
Iter 3/2000 - Loss: 3.550
Iter 4/2000 - Loss: 3.504
Iter 5/2000 - Loss: 3.494
Iter 6/2000 - Loss: 3.390
Iter 7/2000 - Loss: 3.254
Iter 8/2000 - Loss: 3.139
Iter 9/2000 - Loss: 3.020
Iter 10/2000 - Loss: 2.867
Iter 11/2000 - Loss: 2.694
Iter 12/2000 - Loss: 2.518
Iter 13/2000 - Loss: 2.333
Iter 14/2000 - Loss: 2.125
Iter 15/2000 - Loss: 1.891
Iter 16/2000 - Loss: 1.638
Iter 17/2000 - Loss: 1.379
Iter 18/2000 - Loss: 1.116
Iter 19/2000 - Loss: 0.848
Iter 20/2000 - Loss: 0.569
Iter 1981/2000 - Loss: -6.829
Iter 1982/2000 - Loss: -6.829
Iter 1983/2000 - Loss: -6.829
Iter 1984/2000 - Loss: -6.829
Iter 1985/2000 - Loss: -6.829
Iter 1986/2000 - Loss: -6.829
Iter 1987/2000 - Loss: -6.829
Iter 1988/2000 - Loss: -6.829
Iter 1989/2000 - Loss: -6.829
Iter 1990/2000 - Loss: -6.830
Iter 1991/2000 - Loss: -6.830
Iter 1992/2000 - Loss: -6.830
Iter 1993/2000 - Loss: -6.830
Iter 1994/2000 - Loss: -6.830
Iter 1995/2000 - Loss: -6.830
Iter 1996/2000 - Loss: -6.830
Iter 1997/2000 - Loss: -6.830
Iter 1998/2000 - Loss: -6.830
Iter 1999/2000 - Loss: -6.830
Iter 2000/2000 - Loss: -6.830
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0004]])
Lengthscale: tensor([[[11.9224,  5.7956, 20.5851,  2.0543, 17.1150, 45.8636]],

        [[18.9095, 34.5808,  8.4299,  1.0239,  1.6679, 30.3015]],

        [[20.5349, 38.4788,  8.6177,  0.9317,  0.7866, 24.7000]],

        [[16.9145, 30.6112, 17.3701,  1.8254,  1.9000, 45.5221]]])
Signal Variance: tensor([ 0.0610,  2.1387, 13.0634,  0.5466])
Estimated target variance: tensor([0.0186, 0.8499, 7.2673, 0.1553])
N: 150
Signal to noise ratio: tensor([14.4198, 73.6766, 68.1976, 38.0089])
Bound on condition number: tensor([ 31190.4577, 814236.4731, 697638.0104, 216702.4166])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0035227917633428724, policy loss: 7.34026388394037
Experience 15, Iter 1, disc loss: 0.003975065764344906, policy loss: 7.358072451702485
Experience 15, Iter 2, disc loss: 0.0034604150496275804, policy loss: 7.078149427043515
Experience 15, Iter 3, disc loss: 0.003595731594301825, policy loss: 7.316234273754948
Experience 15, Iter 4, disc loss: 0.003552581143753915, policy loss: 7.57615634010988
Experience 15, Iter 5, disc loss: 0.0038504919673030408, policy loss: 7.103659775924762
Experience 15, Iter 6, disc loss: 0.0038628165520620114, policy loss: 7.250600526633513
Experience 15, Iter 7, disc loss: 0.003940908519948272, policy loss: 7.047465271083752
Experience 15, Iter 8, disc loss: 0.003765953434602562, policy loss: 7.237095456426157
Experience 15, Iter 9, disc loss: 0.003826184153064709, policy loss: 7.542770126946349
Experience 15, Iter 10, disc loss: 0.00415137355059406, policy loss: 6.9948499735868666
Experience 15, Iter 11, disc loss: 0.0038986240163131013, policy loss: 7.163470341510587
Experience 15, Iter 12, disc loss: 0.003690723727710285, policy loss: 7.1922898189383115
Experience 15, Iter 13, disc loss: 0.0038556694706440063, policy loss: 7.3041591152825625
Experience 15, Iter 14, disc loss: 0.003410271228220897, policy loss: 7.272200298151537
Experience 15, Iter 15, disc loss: 0.0036595138884835086, policy loss: 7.2325314497481195
Experience 15, Iter 16, disc loss: 0.004691912062338643, policy loss: 6.869242473177299
Experience 15, Iter 17, disc loss: 0.004275414705063721, policy loss: 7.128250724454585
Experience 15, Iter 18, disc loss: 0.003618341958983984, policy loss: 7.307367530205429
Experience 15, Iter 19, disc loss: 0.0031959204397694735, policy loss: 7.568051002553853
Experience 15, Iter 20, disc loss: 0.003665817999301657, policy loss: 7.57029697957721
Experience 15, Iter 21, disc loss: 0.003928394705249959, policy loss: 7.094809994488117
Experience 15, Iter 22, disc loss: 0.003587550806202055, policy loss: 7.335572118506191
Experience 15, Iter 23, disc loss: 0.003824012832051773, policy loss: 7.549479678221686
Experience 15, Iter 24, disc loss: 0.0035151246419224233, policy loss: 7.576902136675055
Experience 15, Iter 25, disc loss: 0.004132579255417607, policy loss: 7.010488944584461
Experience 15, Iter 26, disc loss: 0.0032187605881514036, policy loss: 7.592206660813794
Experience 15, Iter 27, disc loss: 0.00378505394417176, policy loss: 7.577756780421458
Experience 15, Iter 28, disc loss: 0.0034770288712053284, policy loss: 7.250360292673026
Experience 15, Iter 29, disc loss: 0.0028578958484087582, policy loss: 8.236594384791632
Experience 15, Iter 30, disc loss: 0.0037788242547957948, policy loss: 7.549681866638115
Experience 15, Iter 31, disc loss: 0.0037977472199913235, policy loss: 7.064661123175554
Experience 15, Iter 32, disc loss: 0.0031822383761249884, policy loss: 7.872914291765683
Experience 15, Iter 33, disc loss: 0.003643646528128258, policy loss: 7.205730791725408
Experience 15, Iter 34, disc loss: 0.003188408666535714, policy loss: 7.942874852356409
Experience 15, Iter 35, disc loss: 0.0032834673557514925, policy loss: 7.4878715906103475
Experience 15, Iter 36, disc loss: 0.0040733502866048925, policy loss: 7.116039439214031
Experience 15, Iter 37, disc loss: 0.003938880935750532, policy loss: 7.016521234072247
Experience 15, Iter 38, disc loss: 0.0037760603373199865, policy loss: 7.205440225425603
Experience 15, Iter 39, disc loss: 0.0035648584538631534, policy loss: 7.3655227079109045
Experience 15, Iter 40, disc loss: 0.0031385311858643863, policy loss: 7.790387494570895
Experience 15, Iter 41, disc loss: 0.003858901870606238, policy loss: 7.282049453618733
Experience 15, Iter 42, disc loss: 0.0035554907811411783, policy loss: 7.297103358956829
Experience 15, Iter 43, disc loss: 0.0035635783335142045, policy loss: 7.438759921645894
Experience 15, Iter 44, disc loss: 0.0036182413256684817, policy loss: 7.15659841632581
Experience 15, Iter 45, disc loss: 0.0030581393796062953, policy loss: 7.678295725735458
Experience 15, Iter 46, disc loss: 0.0031501353523373664, policy loss: 8.03243586704381
Experience 15, Iter 47, disc loss: 0.0037106775292840705, policy loss: 7.341600537740515
Experience 15, Iter 48, disc loss: 0.0031076804553189456, policy loss: 7.795778067184189
Experience 15, Iter 49, disc loss: 0.003103196045105095, policy loss: 7.70302841624475
Experience 15, Iter 50, disc loss: 0.0035180403780842324, policy loss: 7.147463125798409
Experience 15, Iter 51, disc loss: 0.0030213069017803454, policy loss: 8.000708566927786
Experience 15, Iter 52, disc loss: 0.0035593862033558576, policy loss: 7.352039674164512
Experience 15, Iter 53, disc loss: 0.0032572761723185214, policy loss: 7.910345020049188
Experience 15, Iter 54, disc loss: 0.0034647125700687483, policy loss: 7.245590284063837
Experience 15, Iter 55, disc loss: 0.003295418310313655, policy loss: 7.686739712678249
Experience 15, Iter 56, disc loss: 0.0029188913788401865, policy loss: 7.806789447984924
Experience 15, Iter 57, disc loss: 0.0035901943831378137, policy loss: 7.429504661819406
Experience 15, Iter 58, disc loss: 0.00370632126273535, policy loss: 7.248316262152574
Experience 15, Iter 59, disc loss: 0.003352488437247581, policy loss: 7.3145689871667114
Experience 15, Iter 60, disc loss: 0.0033107800122079483, policy loss: 7.41838293923891
Experience 15, Iter 61, disc loss: 0.0031376478298931953, policy loss: 7.515718358463982
Experience 15, Iter 62, disc loss: 0.003332693350664227, policy loss: 7.417551946747428
Experience 15, Iter 63, disc loss: 0.003438821846976311, policy loss: 7.401706219920462
Experience 15, Iter 64, disc loss: 0.003232379833927059, policy loss: 7.395851445117335
Experience 15, Iter 65, disc loss: 0.0028665888523115527, policy loss: 7.557978908332388
Experience 15, Iter 66, disc loss: 0.003385088480770162, policy loss: 7.532776391879865
Experience 15, Iter 67, disc loss: 0.0033650144367901504, policy loss: 7.43845502521102
Experience 15, Iter 68, disc loss: 0.0031580606079601686, policy loss: 7.398762852682551
Experience 15, Iter 69, disc loss: 0.0034395456634180206, policy loss: 7.075993709386933
Experience 15, Iter 70, disc loss: 0.0031972765816925533, policy loss: 7.663879890211365
Experience 15, Iter 71, disc loss: 0.0036726469169331526, policy loss: 7.4618466614486865
Experience 15, Iter 72, disc loss: 0.003998422654133313, policy loss: 6.920991481791469
Experience 15, Iter 73, disc loss: 0.0036625213500050356, policy loss: 7.165597005501473
Experience 15, Iter 74, disc loss: 0.0032709225756974315, policy loss: 7.419506266792512
Experience 15, Iter 75, disc loss: 0.002970572827502625, policy loss: 7.616974449460471
Experience 15, Iter 76, disc loss: 0.003191979248929843, policy loss: 8.029490662876288
Experience 15, Iter 77, disc loss: 0.003807664798693512, policy loss: 7.349043227842697
Experience 15, Iter 78, disc loss: 0.0038112925580837944, policy loss: 6.90480654308012
Experience 15, Iter 79, disc loss: 0.0027796503778540083, policy loss: 7.741191479816569
Experience 15, Iter 80, disc loss: 0.003181059973273539, policy loss: 7.831002399228202
Experience 15, Iter 81, disc loss: 0.0033008087002863846, policy loss: 7.602460677506887
Experience 15, Iter 82, disc loss: 0.00306030627441837, policy loss: 7.7133985286748885
Experience 15, Iter 83, disc loss: 0.0030394392504533123, policy loss: 7.546385005499892
Experience 15, Iter 84, disc loss: 0.00301926944455351, policy loss: 7.543274185291573
Experience 15, Iter 85, disc loss: 0.002827422302208917, policy loss: 8.1970913698541
Experience 15, Iter 86, disc loss: 0.0029934894879971308, policy loss: 7.574175663393852
Experience 15, Iter 87, disc loss: 0.0029758804349806424, policy loss: 7.595874792650948
Experience 15, Iter 88, disc loss: 0.0035685126586583937, policy loss: 7.320055129724024
Experience 15, Iter 89, disc loss: 0.002811347631602965, policy loss: 8.126537445992208
Experience 15, Iter 90, disc loss: 0.0031566992359100083, policy loss: 8.102156360183532
Experience 15, Iter 91, disc loss: 0.0032642237492783725, policy loss: 7.600293044171352
Experience 15, Iter 92, disc loss: 0.0027335698569162193, policy loss: 7.725090167822034
Experience 15, Iter 93, disc loss: 0.002942503485012334, policy loss: 7.625531483196093
Experience 15, Iter 94, disc loss: 0.0033390870408333438, policy loss: 7.158183152407443
Experience 15, Iter 95, disc loss: 0.0029136097473206413, policy loss: 7.38167958575543
Experience 15, Iter 96, disc loss: 0.0030811548918237754, policy loss: 7.256420309327033
Experience 15, Iter 97, disc loss: 0.002815847030094112, policy loss: 7.579512114963016
Experience 15, Iter 98, disc loss: 0.002850499996354192, policy loss: 8.251868774696032
Experience 15, Iter 99, disc loss: 0.0033597630529519644, policy loss: 7.331080237437941
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.2187],
        [1.8564],
        [0.0391]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0174, 0.1893, 1.7725, 0.0322, 0.0264, 5.3450]],

        [[0.0174, 0.1893, 1.7725, 0.0322, 0.0264, 5.3450]],

        [[0.0174, 0.1893, 1.7725, 0.0322, 0.0264, 5.3450]],

        [[0.0174, 0.1893, 1.7725, 0.0322, 0.0264, 5.3450]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0181, 0.8748, 7.4257, 0.1565], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0181, 0.8748, 7.4257, 0.1565])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.795
Iter 2/2000 - Loss: 3.747
Iter 3/2000 - Loss: 3.573
Iter 4/2000 - Loss: 3.525
Iter 5/2000 - Loss: 3.518
Iter 6/2000 - Loss: 3.416
Iter 7/2000 - Loss: 3.283
Iter 8/2000 - Loss: 3.170
Iter 9/2000 - Loss: 3.050
Iter 10/2000 - Loss: 2.898
Iter 11/2000 - Loss: 2.724
Iter 12/2000 - Loss: 2.547
Iter 13/2000 - Loss: 2.361
Iter 14/2000 - Loss: 2.151
Iter 15/2000 - Loss: 1.914
Iter 16/2000 - Loss: 1.656
Iter 17/2000 - Loss: 1.390
Iter 18/2000 - Loss: 1.121
Iter 19/2000 - Loss: 0.846
Iter 20/2000 - Loss: 0.559
Iter 1981/2000 - Loss: -6.949
Iter 1982/2000 - Loss: -6.949
Iter 1983/2000 - Loss: -6.949
Iter 1984/2000 - Loss: -6.949
Iter 1985/2000 - Loss: -6.949
Iter 1986/2000 - Loss: -6.949
Iter 1987/2000 - Loss: -6.949
Iter 1988/2000 - Loss: -6.949
Iter 1989/2000 - Loss: -6.949
Iter 1990/2000 - Loss: -6.949
Iter 1991/2000 - Loss: -6.949
Iter 1992/2000 - Loss: -6.949
Iter 1993/2000 - Loss: -6.950
Iter 1994/2000 - Loss: -6.950
Iter 1995/2000 - Loss: -6.950
Iter 1996/2000 - Loss: -6.950
Iter 1997/2000 - Loss: -6.950
Iter 1998/2000 - Loss: -6.950
Iter 1999/2000 - Loss: -6.950
Iter 2000/2000 - Loss: -6.950
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0004]])
Lengthscale: tensor([[[12.7325,  6.0513, 20.4146,  1.8966, 16.8446, 44.4933]],

        [[18.9786, 34.3464,  8.1978,  1.0777,  1.5777, 31.1768]],

        [[19.5444, 38.0827,  8.5084,  0.9353,  0.7542, 24.5276]],

        [[16.4476, 30.0007, 16.8672,  1.8451,  1.8776, 47.6468]]])
Signal Variance: tensor([ 0.0635,  2.2472, 12.2777,  0.5292])
Estimated target variance: tensor([0.0181, 0.8748, 7.4257, 0.1565])
N: 160
Signal to noise ratio: tensor([14.5551, 77.0361, 67.2300, 37.4419])
Bound on condition number: tensor([ 33897.2208, 949531.7889, 723179.9363, 224304.9361])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.00274233983930027, policy loss: 7.826163288390136
Experience 16, Iter 1, disc loss: 0.0034502927226923038, policy loss: 7.341384418307712
Experience 16, Iter 2, disc loss: 0.0027454095467707917, policy loss: 8.151157012604575
Experience 16, Iter 3, disc loss: 0.0030722800733241074, policy loss: 7.557435703594098
Experience 16, Iter 4, disc loss: 0.0033795900756608804, policy loss: 7.063563644711806
Experience 16, Iter 5, disc loss: 0.00342668754867148, policy loss: 7.919191533889249
Experience 16, Iter 6, disc loss: 0.003537405709524456, policy loss: 7.261431233790007
Experience 16, Iter 7, disc loss: 0.0029774975527780876, policy loss: 7.393088263636266
Experience 16, Iter 8, disc loss: 0.0030262661207744916, policy loss: 7.473778262167976
Experience 16, Iter 9, disc loss: 0.0032806138400479023, policy loss: 7.3084798213427336
Experience 16, Iter 10, disc loss: 0.0032275902914656665, policy loss: 8.021323230509775
Experience 16, Iter 11, disc loss: 0.0026270862885205484, policy loss: 7.855090663858044
Experience 16, Iter 12, disc loss: 0.0028843232934819035, policy loss: 7.499828006810506
Experience 16, Iter 13, disc loss: 0.00248743068182619, policy loss: 7.8845064293003455
Experience 16, Iter 14, disc loss: 0.002861393872293047, policy loss: 7.786305520655672
Experience 16, Iter 15, disc loss: 0.0030601288499547, policy loss: 7.468838766948452
Experience 16, Iter 16, disc loss: 0.002669306282858442, policy loss: 7.445366053466519
Experience 16, Iter 17, disc loss: 0.0029275769481712966, policy loss: 7.845772353150629
Experience 16, Iter 18, disc loss: 0.0025312313446893145, policy loss: 8.323231098375524
Experience 16, Iter 19, disc loss: 0.0025310891720963197, policy loss: 7.373070639031758
Experience 16, Iter 20, disc loss: 0.0030772089965585437, policy loss: 7.631025062699093
Experience 16, Iter 21, disc loss: 0.0031458931740606703, policy loss: 7.46079565024529
Experience 16, Iter 22, disc loss: 0.0023684590473155333, policy loss: 8.637447391874773
Experience 16, Iter 23, disc loss: 0.0021399541740961536, policy loss: 8.20146627270457
Experience 16, Iter 24, disc loss: 0.0016477471916175731, policy loss: 9.264644811837321
Experience 16, Iter 25, disc loss: 0.001588435396830303, policy loss: 10.113524987415207
Experience 16, Iter 26, disc loss: 0.0015441722393724237, policy loss: 10.16373170448258
Experience 16, Iter 27, disc loss: 0.0015348597423800565, policy loss: 9.377457384897502
Experience 16, Iter 28, disc loss: 0.0016192087150952233, policy loss: 9.190806633810078
Experience 16, Iter 29, disc loss: 0.001877998457194428, policy loss: 8.511635004340329
Experience 16, Iter 30, disc loss: 0.0021578064151207836, policy loss: 7.836567807691061
Experience 16, Iter 31, disc loss: 0.002261869237704204, policy loss: 7.8807481406293896
Experience 16, Iter 32, disc loss: 0.002221481477797785, policy loss: 7.8894660914848895
Experience 16, Iter 33, disc loss: 0.0019895521652003225, policy loss: 8.38610311435032
Experience 16, Iter 34, disc loss: 0.0025429332482073156, policy loss: 7.8659337473725035
Experience 16, Iter 35, disc loss: 0.001849865699967684, policy loss: 7.835066987483767
Experience 16, Iter 36, disc loss: 0.0015899747346787728, policy loss: 8.549512399254416
Experience 16, Iter 37, disc loss: 0.0017356860883199318, policy loss: 9.272890543701937
Experience 16, Iter 38, disc loss: 0.001676775382982006, policy loss: 8.667632323071725
Experience 16, Iter 39, disc loss: 0.0018389893881201414, policy loss: 8.636853814334346
Experience 16, Iter 40, disc loss: 0.001711299333048456, policy loss: 8.418525574431515
Experience 16, Iter 41, disc loss: 0.0019097350658449736, policy loss: 8.092611393187774
Experience 16, Iter 42, disc loss: 0.0026136992619919513, policy loss: 7.155621341963927
Experience 16, Iter 43, disc loss: 0.0025971854719200474, policy loss: 7.14929985301312
Experience 16, Iter 44, disc loss: 0.0018351188641862342, policy loss: 9.815593407821474
Experience 16, Iter 45, disc loss: 0.0018357424825083922, policy loss: 7.7739868218682595
Experience 16, Iter 46, disc loss: 0.0012774362166756222, policy loss: 9.446458546936716
Experience 16, Iter 47, disc loss: 0.0011338334265256253, policy loss: 10.303150130368198
Experience 16, Iter 48, disc loss: 0.0011625384193969317, policy loss: 11.165550511011283
Experience 16, Iter 49, disc loss: 0.0011080682099787287, policy loss: 10.970690025070972
Experience 16, Iter 50, disc loss: 0.0010520086772318144, policy loss: 10.793115528001607
Experience 16, Iter 51, disc loss: 0.0010268522244435729, policy loss: 10.236837300277045
Experience 16, Iter 52, disc loss: 0.0009971536111561385, policy loss: 10.457045315987353
Experience 16, Iter 53, disc loss: 0.0009648623529072382, policy loss: 10.654096327271596
Experience 16, Iter 54, disc loss: 0.0009654353439617765, policy loss: 10.20037402287469
Experience 16, Iter 55, disc loss: 0.0009866801379016677, policy loss: 9.73818240537162
Experience 16, Iter 56, disc loss: 0.0009623492716596186, policy loss: 9.616072542740362
Experience 16, Iter 57, disc loss: 0.0009412667010658445, policy loss: 9.92665686952983
Experience 16, Iter 58, disc loss: 0.0009539471306105605, policy loss: 9.596266400804948
Experience 16, Iter 59, disc loss: 0.0009813353029187342, policy loss: 9.353452463874417
Experience 16, Iter 60, disc loss: 0.000958175395095645, policy loss: 9.078497418237443
Experience 16, Iter 61, disc loss: 0.000961685738974294, policy loss: 8.960067259711769
Experience 16, Iter 62, disc loss: 0.0009595741290281876, policy loss: 9.028399623417933
Experience 16, Iter 63, disc loss: 0.0011246430946435233, policy loss: 8.240524021901983
Experience 16, Iter 64, disc loss: 0.001363450020523235, policy loss: 8.043449336790195
Experience 16, Iter 65, disc loss: 0.0016784052479727998, policy loss: 7.926166753187468
Experience 16, Iter 66, disc loss: 0.001650670243186359, policy loss: 7.973405867730178
Experience 16, Iter 67, disc loss: 0.002685862440864891, policy loss: 6.998985805039721
Experience 16, Iter 68, disc loss: 0.003350646319946345, policy loss: 6.640039642217498
Experience 16, Iter 69, disc loss: 0.0028349174829051483, policy loss: 7.480787752848756
Experience 16, Iter 70, disc loss: 0.0031670236988197787, policy loss: 7.516345689542469
Experience 16, Iter 71, disc loss: 0.00203759546124502, policy loss: 7.4981758932716165
Experience 16, Iter 72, disc loss: 0.0019012615932744567, policy loss: 7.593415445289128
Experience 16, Iter 73, disc loss: 0.0031499051845742436, policy loss: 6.989309941688229
Experience 16, Iter 74, disc loss: 0.0034808004227484925, policy loss: 7.218620440012815
Experience 16, Iter 75, disc loss: 0.0026128469196967158, policy loss: 7.270021263865042
Experience 16, Iter 76, disc loss: 0.002674118968906112, policy loss: 7.377748254860921
Experience 16, Iter 77, disc loss: 0.0030212091764869565, policy loss: 6.976455761363246
Experience 16, Iter 78, disc loss: 0.0023035343563676666, policy loss: 7.236589075119071
Experience 16, Iter 79, disc loss: 0.0028287564650803752, policy loss: 7.229488967266337
Experience 16, Iter 80, disc loss: 0.002410604978697873, policy loss: 7.494054563248186
Experience 16, Iter 81, disc loss: 0.0026612681487833477, policy loss: 7.733427290161908
Experience 16, Iter 82, disc loss: 0.0031470563218022225, policy loss: 7.192375506882464
Experience 16, Iter 83, disc loss: 0.0027243728231859093, policy loss: 7.351996807656214
Experience 16, Iter 84, disc loss: 0.0028810739310606488, policy loss: 7.10923960575022
Experience 16, Iter 85, disc loss: 0.0029307100993810654, policy loss: 7.374501801412694
Experience 16, Iter 86, disc loss: 0.002822665260164692, policy loss: 7.5531533266175295
Experience 16, Iter 87, disc loss: 0.0032053057800229025, policy loss: 7.025981387810138
Experience 16, Iter 88, disc loss: 0.002432738303301686, policy loss: 8.14282369347281
Experience 16, Iter 89, disc loss: 0.003341850644753402, policy loss: 6.856870217194473
Experience 16, Iter 90, disc loss: 0.0029516576037675827, policy loss: 7.543007438549947
Experience 16, Iter 91, disc loss: 0.002247001644083536, policy loss: 7.940684446612927
Experience 16, Iter 92, disc loss: 0.0025334557103337024, policy loss: 7.611925877169046
Experience 16, Iter 93, disc loss: 0.0027814067890516887, policy loss: 7.533775115124028
Experience 16, Iter 94, disc loss: 0.0028145091272649436, policy loss: 7.732451854804738
Experience 16, Iter 95, disc loss: 0.0027597636265008587, policy loss: 7.586450758771942
Experience 16, Iter 96, disc loss: 0.0027265897763173947, policy loss: 7.821133259124243
Experience 16, Iter 97, disc loss: 0.002650212041207009, policy loss: 7.603820205432716
Experience 16, Iter 98, disc loss: 0.002906326124708327, policy loss: 7.539320839100522
Experience 16, Iter 99, disc loss: 0.0027805862998216274, policy loss: 7.58585023313586
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.2249],
        [1.8752],
        [0.0396]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0166, 0.1890, 1.7934, 0.0324, 0.0275, 5.5160]],

        [[0.0166, 0.1890, 1.7934, 0.0324, 0.0275, 5.5160]],

        [[0.0166, 0.1890, 1.7934, 0.0324, 0.0275, 5.5160]],

        [[0.0166, 0.1890, 1.7934, 0.0324, 0.0275, 5.5160]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0181, 0.8995, 7.5007, 0.1582], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0181, 0.8995, 7.5007, 0.1582])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.815
Iter 2/2000 - Loss: 3.767
Iter 3/2000 - Loss: 3.577
Iter 4/2000 - Loss: 3.521
Iter 5/2000 - Loss: 3.505
Iter 6/2000 - Loss: 3.397
Iter 7/2000 - Loss: 3.257
Iter 8/2000 - Loss: 3.137
Iter 9/2000 - Loss: 3.010
Iter 10/2000 - Loss: 2.847
Iter 11/2000 - Loss: 2.663
Iter 12/2000 - Loss: 2.476
Iter 13/2000 - Loss: 2.281
Iter 14/2000 - Loss: 2.064
Iter 15/2000 - Loss: 1.818
Iter 16/2000 - Loss: 1.552
Iter 17/2000 - Loss: 1.277
Iter 18/2000 - Loss: 1.001
Iter 19/2000 - Loss: 0.719
Iter 20/2000 - Loss: 0.429
Iter 1981/2000 - Loss: -7.068
Iter 1982/2000 - Loss: -7.068
Iter 1983/2000 - Loss: -7.068
Iter 1984/2000 - Loss: -7.068
Iter 1985/2000 - Loss: -7.068
Iter 1986/2000 - Loss: -7.068
Iter 1987/2000 - Loss: -7.068
Iter 1988/2000 - Loss: -7.068
Iter 1989/2000 - Loss: -7.068
Iter 1990/2000 - Loss: -7.068
Iter 1991/2000 - Loss: -7.068
Iter 1992/2000 - Loss: -7.068
Iter 1993/2000 - Loss: -7.068
Iter 1994/2000 - Loss: -7.069
Iter 1995/2000 - Loss: -7.069
Iter 1996/2000 - Loss: -7.069
Iter 1997/2000 - Loss: -7.069
Iter 1998/2000 - Loss: -7.069
Iter 1999/2000 - Loss: -7.069
Iter 2000/2000 - Loss: -7.069
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[12.2498,  6.1942, 20.1786,  1.9334, 17.1402, 44.5812]],

        [[18.7734, 33.8622,  7.8086,  1.1346,  1.5551, 31.0942]],

        [[18.1426, 37.4981,  8.5151,  0.9513,  0.7181, 24.6559]],

        [[16.1437, 29.4538, 16.2807,  1.7861,  1.7916, 48.0768]]])
Signal Variance: tensor([ 0.0660,  2.1889, 12.3833,  0.4928])
Estimated target variance: tensor([0.0181, 0.8995, 7.5007, 0.1582])
N: 170
Signal to noise ratio: tensor([14.6135, 76.9056, 69.6285, 36.5729])
Bound on condition number: tensor([  36305.2176, 1005459.8208,  824181.8617,  227388.6455])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.003115137728200108, policy loss: 7.0160833901083475
Experience 17, Iter 1, disc loss: 0.002913970843429047, policy loss: 7.7044394862711085
Experience 17, Iter 2, disc loss: 0.003042447022688754, policy loss: 7.435729684596314
Experience 17, Iter 3, disc loss: 0.002460377103670383, policy loss: 8.3175275940702
Experience 17, Iter 4, disc loss: 0.0028247965483725063, policy loss: 7.907308432234039
Experience 17, Iter 5, disc loss: 0.0027130276084680618, policy loss: 7.8588710537873325
Experience 17, Iter 6, disc loss: 0.0031727504413438037, policy loss: 7.130754644623296
Experience 17, Iter 7, disc loss: 0.0027848815060160016, policy loss: 8.257514250869109
Experience 17, Iter 8, disc loss: 0.0026569125840515983, policy loss: 8.060503072067
Experience 17, Iter 9, disc loss: 0.00270386061774809, policy loss: 7.653578497824401
Experience 17, Iter 10, disc loss: 0.002862554179444912, policy loss: 7.385434748689468
Experience 17, Iter 11, disc loss: 0.0029983851257978375, policy loss: 7.650360031007226
Experience 17, Iter 12, disc loss: 0.0032194343611518196, policy loss: 7.548133354506459
Experience 17, Iter 13, disc loss: 0.0027644456778256237, policy loss: 8.037678763409815
Experience 17, Iter 14, disc loss: 0.002819105282273487, policy loss: 7.5525706263298495
Experience 17, Iter 15, disc loss: 0.002671466215705056, policy loss: 7.534207560164753
Experience 17, Iter 16, disc loss: 0.002797062450447085, policy loss: 7.928517794616373
Experience 17, Iter 17, disc loss: 0.0023797974042682616, policy loss: 8.45297630560947
Experience 17, Iter 18, disc loss: 0.002767549751699877, policy loss: 7.56618866487993
Experience 17, Iter 19, disc loss: 0.0025758404610226092, policy loss: 7.989386024546283
Experience 17, Iter 20, disc loss: 0.002454275370590472, policy loss: 7.668506499370287
Experience 17, Iter 21, disc loss: 0.0025940618839223953, policy loss: 7.516109035381716
Experience 17, Iter 22, disc loss: 0.002633321064847881, policy loss: 8.158153373249535
Experience 17, Iter 23, disc loss: 0.0026976574127194172, policy loss: 8.235588311012792
Experience 17, Iter 24, disc loss: 0.0023503551649298633, policy loss: 7.896449543111781
Experience 17, Iter 25, disc loss: 0.002456432552513827, policy loss: 7.50292509620585
Experience 17, Iter 26, disc loss: 0.0025582079864708324, policy loss: 7.831410916868753
Experience 17, Iter 27, disc loss: 0.002594881776467106, policy loss: 7.742409870754019
Experience 17, Iter 28, disc loss: 0.0028371452450048104, policy loss: 7.908643827720014
Experience 17, Iter 29, disc loss: 0.002614565853507055, policy loss: 7.673510311888865
Experience 17, Iter 30, disc loss: 0.002832315041330885, policy loss: 8.23060468333218
Experience 17, Iter 31, disc loss: 0.0022979804883898054, policy loss: 7.941204929006575
Experience 17, Iter 32, disc loss: 0.002623481408172636, policy loss: 7.990821991200729
Experience 17, Iter 33, disc loss: 0.0024596446470708157, policy loss: 8.022244360037883
Experience 17, Iter 34, disc loss: 0.002497490430488089, policy loss: 8.128549100323278
Experience 17, Iter 35, disc loss: 0.002507908768273732, policy loss: 8.03115814957945
Experience 17, Iter 36, disc loss: 0.00257327822461015, policy loss: 7.762388135406727
Experience 17, Iter 37, disc loss: 0.0025120914768903156, policy loss: 7.690485418286626
Experience 17, Iter 38, disc loss: 0.0024757634812327942, policy loss: 7.88142809493543
Experience 17, Iter 39, disc loss: 0.0023451941573338726, policy loss: 8.11527257031982
Experience 17, Iter 40, disc loss: 0.002792599486973048, policy loss: 7.735500027526308
Experience 17, Iter 41, disc loss: 0.002882399457668848, policy loss: 7.725958699167481
Experience 17, Iter 42, disc loss: 0.0026112462758660113, policy loss: 7.544427336397236
Experience 17, Iter 43, disc loss: 0.0024514780780743113, policy loss: 7.989998601882586
Experience 17, Iter 44, disc loss: 0.0025489837599854785, policy loss: 8.041851357896928
Experience 17, Iter 45, disc loss: 0.0029286508005897834, policy loss: 7.792331039596191
Experience 17, Iter 46, disc loss: 0.002416953387739503, policy loss: 8.14274045324111
Experience 17, Iter 47, disc loss: 0.0022471464288970084, policy loss: 7.889477350881045
Experience 17, Iter 48, disc loss: 0.002792723035817234, policy loss: 7.413917483632069
Experience 17, Iter 49, disc loss: 0.002649423899724321, policy loss: 7.478076450461682
Experience 17, Iter 50, disc loss: 0.002392377906463839, policy loss: 8.154639666465531
Experience 17, Iter 51, disc loss: 0.002372698571759259, policy loss: 8.67832700456012
Experience 17, Iter 52, disc loss: 0.0022780185336752495, policy loss: 7.640028554180283
Experience 17, Iter 53, disc loss: 0.0024368714934066794, policy loss: 8.053573822510208
Experience 17, Iter 54, disc loss: 0.002684974180176579, policy loss: 8.178316053917431
Experience 17, Iter 55, disc loss: 0.0024973243225591786, policy loss: 7.968148104898923
Experience 17, Iter 56, disc loss: 0.00277528316579709, policy loss: 7.5458581496788675
Experience 17, Iter 57, disc loss: 0.0025360385611600906, policy loss: 7.969603767989858
Experience 17, Iter 58, disc loss: 0.0024924045955457136, policy loss: 8.179905151200343
Experience 17, Iter 59, disc loss: 0.002310660470969607, policy loss: 8.106439902568251
Experience 17, Iter 60, disc loss: 0.002686784945920548, policy loss: 7.578291517330322
Experience 17, Iter 61, disc loss: 0.0025126867792019955, policy loss: 7.855574841287324
Experience 17, Iter 62, disc loss: 0.0023016144062186017, policy loss: 7.835021919572424
Experience 17, Iter 63, disc loss: 0.0022357837080963, policy loss: 7.957952488823668
Experience 17, Iter 64, disc loss: 0.0022161724737868324, policy loss: 8.17025223760388
Experience 17, Iter 65, disc loss: 0.0025997556456799284, policy loss: 7.847630517476388
Experience 17, Iter 66, disc loss: 0.0025290984584725247, policy loss: 7.549903762589547
Experience 17, Iter 67, disc loss: 0.0021196925847159066, policy loss: 8.43811263333395
Experience 17, Iter 68, disc loss: 0.002418983107737651, policy loss: 7.87982624003683
Experience 17, Iter 69, disc loss: 0.002378787157556434, policy loss: 7.6649216709263985
Experience 17, Iter 70, disc loss: 0.0025424163444857442, policy loss: 7.570710431770438
Experience 17, Iter 71, disc loss: 0.0025565647756491854, policy loss: 7.833436995125338
Experience 17, Iter 72, disc loss: 0.002142562079344509, policy loss: 8.145964932541116
Experience 17, Iter 73, disc loss: 0.002537729448737023, policy loss: 7.918000972708631
Experience 17, Iter 74, disc loss: 0.002134985036814417, policy loss: 7.785133795466993
Experience 17, Iter 75, disc loss: 0.0023140362509264534, policy loss: 7.665762616943843
Experience 17, Iter 76, disc loss: 0.0022922455397360987, policy loss: 7.725236356154827
Experience 17, Iter 77, disc loss: 0.0024324484419950146, policy loss: 7.732235662729873
Experience 17, Iter 78, disc loss: 0.0026332516954965914, policy loss: 7.5351050871692475
Experience 17, Iter 79, disc loss: 0.0023246996338427, policy loss: 7.853641876717761
Experience 17, Iter 80, disc loss: 0.0022813347432526785, policy loss: 8.113199444234782
Experience 17, Iter 81, disc loss: 0.002289324550405195, policy loss: 8.334138902157033
Experience 17, Iter 82, disc loss: 0.002340029524024947, policy loss: 7.592293838562772
Experience 17, Iter 83, disc loss: 0.0020602305305680094, policy loss: 7.905859168595834
Experience 17, Iter 84, disc loss: 0.002302560341832572, policy loss: 8.15643663377923
Experience 17, Iter 85, disc loss: 0.0022201478143326053, policy loss: 8.065327585140897
Experience 17, Iter 86, disc loss: 0.002359925224853061, policy loss: 8.15134033220051
Experience 17, Iter 87, disc loss: 0.0023107554617487307, policy loss: 7.893371589380889
Experience 17, Iter 88, disc loss: 0.0024988554878446843, policy loss: 7.798303191297409
Experience 17, Iter 89, disc loss: 0.0021430061681095135, policy loss: 8.122358145528427
Experience 17, Iter 90, disc loss: 0.0023888324055458075, policy loss: 7.757776490774459
Experience 17, Iter 91, disc loss: 0.0022086702045354583, policy loss: 7.844876732527241
Experience 17, Iter 92, disc loss: 0.0029007142697174805, policy loss: 7.349412895529137
Experience 17, Iter 93, disc loss: 0.002187626353619768, policy loss: 7.657892493147815
Experience 17, Iter 94, disc loss: 0.002540880104505719, policy loss: 7.504273964709266
Experience 17, Iter 95, disc loss: 0.0021433083562227197, policy loss: 7.92379288203208
Experience 17, Iter 96, disc loss: 0.002414025992186019, policy loss: 7.908048212429769
Experience 17, Iter 97, disc loss: 0.0024009062393867556, policy loss: 7.887534535901578
Experience 17, Iter 98, disc loss: 0.0025378916141180286, policy loss: 7.66526532490245
Experience 17, Iter 99, disc loss: 0.002746043869186433, policy loss: 7.441140383821282
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.2292],
        [1.8977],
        [0.0397]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0159, 0.1873, 1.8040, 0.0324, 0.0279, 5.6293]],

        [[0.0159, 0.1873, 1.8040, 0.0324, 0.0279, 5.6293]],

        [[0.0159, 0.1873, 1.8040, 0.0324, 0.0279, 5.6293]],

        [[0.0159, 0.1873, 1.8040, 0.0324, 0.0279, 5.6293]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0179, 0.9166, 7.5909, 0.1587], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0179, 0.9166, 7.5909, 0.1587])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.814
Iter 2/2000 - Loss: 3.768
Iter 3/2000 - Loss: 3.575
Iter 4/2000 - Loss: 3.506
Iter 5/2000 - Loss: 3.474
Iter 6/2000 - Loss: 3.360
Iter 7/2000 - Loss: 3.221
Iter 8/2000 - Loss: 3.096
Iter 9/2000 - Loss: 2.955
Iter 10/2000 - Loss: 2.777
Iter 11/2000 - Loss: 2.582
Iter 12/2000 - Loss: 2.388
Iter 13/2000 - Loss: 2.189
Iter 14/2000 - Loss: 1.966
Iter 15/2000 - Loss: 1.713
Iter 16/2000 - Loss: 1.441
Iter 17/2000 - Loss: 1.160
Iter 18/2000 - Loss: 0.878
Iter 19/2000 - Loss: 0.591
Iter 20/2000 - Loss: 0.297
Iter 1981/2000 - Loss: -7.163
Iter 1982/2000 - Loss: -7.163
Iter 1983/2000 - Loss: -7.163
Iter 1984/2000 - Loss: -7.163
Iter 1985/2000 - Loss: -7.163
Iter 1986/2000 - Loss: -7.164
Iter 1987/2000 - Loss: -7.164
Iter 1988/2000 - Loss: -7.164
Iter 1989/2000 - Loss: -7.164
Iter 1990/2000 - Loss: -7.164
Iter 1991/2000 - Loss: -7.164
Iter 1992/2000 - Loss: -7.164
Iter 1993/2000 - Loss: -7.164
Iter 1994/2000 - Loss: -7.164
Iter 1995/2000 - Loss: -7.164
Iter 1996/2000 - Loss: -7.164
Iter 1997/2000 - Loss: -7.164
Iter 1998/2000 - Loss: -7.164
Iter 1999/2000 - Loss: -7.164
Iter 2000/2000 - Loss: -7.164
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[12.0402,  6.3311, 22.1837,  1.9665, 16.9410, 42.7055]],

        [[17.9882, 33.4216,  7.1098,  1.2007,  1.5525, 30.9725]],

        [[17.2033, 36.3055,  8.5432,  0.9558,  0.6973, 24.9948]],

        [[15.4476, 28.7663, 14.1304,  1.6588,  1.7805, 48.6678]]])
Signal Variance: tensor([ 0.0687,  2.1643, 12.3316,  0.4260])
Estimated target variance: tensor([0.0179, 0.9166, 7.5909, 0.1587])
N: 180
Signal to noise ratio: tensor([14.7771, 78.4270, 69.0839, 34.3167])
Bound on condition number: tensor([  39306.2995, 1107143.5701,  859065.6273,  211975.6172])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0025073918463427427, policy loss: 7.5909035554743935
Experience 18, Iter 1, disc loss: 0.002199421511018957, policy loss: 8.447864895181391
Experience 18, Iter 2, disc loss: 0.0020929891358457365, policy loss: 7.948927841436869
Experience 18, Iter 3, disc loss: 0.002284903275047461, policy loss: 7.755035586813127
Experience 18, Iter 4, disc loss: 0.0022365213028172053, policy loss: 7.824901737212153
Experience 18, Iter 5, disc loss: 0.002410895581802199, policy loss: 8.024473128018064
Experience 18, Iter 6, disc loss: 0.0021718805953894202, policy loss: 8.580608445609707
Experience 18, Iter 7, disc loss: 0.0022311271828100747, policy loss: 8.620714682472693
Experience 18, Iter 8, disc loss: 0.0024050922651491706, policy loss: 7.9622681478681
Experience 18, Iter 9, disc loss: 0.0023168667765756943, policy loss: 8.079348049393245
Experience 18, Iter 10, disc loss: 0.0022750559226881358, policy loss: 7.625539677295088
Experience 18, Iter 11, disc loss: 0.0022691883241006254, policy loss: 7.713055770408186
Experience 18, Iter 12, disc loss: 0.002172096821111246, policy loss: 7.754950557078806
Experience 18, Iter 13, disc loss: 0.0020958365186139107, policy loss: 7.873220086098035
Experience 18, Iter 14, disc loss: 0.0021809812768119126, policy loss: 8.372053796493308
Experience 18, Iter 15, disc loss: 0.002369010919709814, policy loss: 7.6410699427934805
Experience 18, Iter 16, disc loss: 0.0020921587202256697, policy loss: 8.497036544776105
Experience 18, Iter 17, disc loss: 0.0023542791778005953, policy loss: 7.584122346149732
Experience 18, Iter 18, disc loss: 0.002275725215363845, policy loss: 8.144815016321246
Experience 18, Iter 19, disc loss: 0.002510324662779298, policy loss: 7.504898761791558
Experience 18, Iter 20, disc loss: 0.0023019864972353447, policy loss: 7.830751169195908
Experience 18, Iter 21, disc loss: 0.0019747506578653004, policy loss: 7.851544176403156
Experience 18, Iter 22, disc loss: 0.0020379177535771445, policy loss: 8.171240546463954
Experience 18, Iter 23, disc loss: 0.0020349675390525417, policy loss: 7.811869462775414
Experience 18, Iter 24, disc loss: 0.0023120500952879667, policy loss: 8.045086226898162
Experience 18, Iter 25, disc loss: 0.002170087878708023, policy loss: 8.349866091644149
Experience 18, Iter 26, disc loss: 0.0022476735571356275, policy loss: 7.79644406347536
Experience 18, Iter 27, disc loss: 0.002072779720312997, policy loss: 8.070011506858162
Experience 18, Iter 28, disc loss: 0.0023388716248213277, policy loss: 7.925992448244586
Experience 18, Iter 29, disc loss: 0.002460273989875663, policy loss: 7.8933659520458495
Experience 18, Iter 30, disc loss: 0.0022598830067391057, policy loss: 8.313601882308443
Experience 18, Iter 31, disc loss: 0.002317942803187995, policy loss: 7.9062524321485235
Experience 18, Iter 32, disc loss: 0.0023339197665617896, policy loss: 7.694488526191982
Experience 18, Iter 33, disc loss: 0.0028086578036455884, policy loss: 7.144618250293888
Experience 18, Iter 34, disc loss: 0.002105544770337335, policy loss: 7.902141484583483
Experience 18, Iter 35, disc loss: 0.002427415468835711, policy loss: 7.927131775282068
Experience 18, Iter 36, disc loss: 0.002113931053518573, policy loss: 8.06385652612703
Experience 18, Iter 37, disc loss: 0.002023354037993766, policy loss: 8.0862942709614
Experience 18, Iter 38, disc loss: 0.0024487472962599595, policy loss: 7.795268084687327
Experience 18, Iter 39, disc loss: 0.0022699509064421516, policy loss: 8.203184187638335
Experience 18, Iter 40, disc loss: 0.002222331517714023, policy loss: 7.943735945581561
Experience 18, Iter 41, disc loss: 0.0021415694994518786, policy loss: 8.089567119321744
Experience 18, Iter 42, disc loss: 0.0021944218617251975, policy loss: 8.35382898985123
Experience 18, Iter 43, disc loss: 0.002183313868249487, policy loss: 7.779698514958365
Experience 18, Iter 44, disc loss: 0.002033055379539885, policy loss: 7.922430422613635
Experience 18, Iter 45, disc loss: 0.0024429152034631703, policy loss: 7.382320825976922
Experience 18, Iter 46, disc loss: 0.0020456416880379985, policy loss: 7.891394914899829
Experience 18, Iter 47, disc loss: 0.002157188301932194, policy loss: 7.910116046289548
Experience 18, Iter 48, disc loss: 0.0022439542740565455, policy loss: 8.205331401282276
Experience 18, Iter 49, disc loss: 0.0023888188865400658, policy loss: 7.286904699895829
Experience 18, Iter 50, disc loss: 0.0020929398372523103, policy loss: 7.873102267408678
Experience 18, Iter 51, disc loss: 0.00217849653171482, policy loss: 7.98417600457584
Experience 18, Iter 52, disc loss: 0.0021067875891004188, policy loss: 7.800923074095854
Experience 18, Iter 53, disc loss: 0.001928788394680164, policy loss: 8.326475654191256
Experience 18, Iter 54, disc loss: 0.002470036733115737, policy loss: 7.713427969722169
Experience 18, Iter 55, disc loss: 0.00197126923767821, policy loss: 8.373128253855771
Experience 18, Iter 56, disc loss: 0.0019460830742767535, policy loss: 8.156423403057442
Experience 18, Iter 57, disc loss: 0.002068744505836743, policy loss: 8.311347322613933
Experience 18, Iter 58, disc loss: 0.0019999401961321795, policy loss: 8.476082816109827
Experience 18, Iter 59, disc loss: 0.002128350497339699, policy loss: 8.21122272561567
Experience 18, Iter 60, disc loss: 0.001837222745380823, policy loss: 8.61157390046914
Experience 18, Iter 61, disc loss: 0.0021317941928183743, policy loss: 7.96366284808936
Experience 18, Iter 62, disc loss: 0.00239564879345163, policy loss: 7.76829097989017
Experience 18, Iter 63, disc loss: 0.001973265486012245, policy loss: 8.438421186119196
Experience 18, Iter 64, disc loss: 0.0016612822855057866, policy loss: 8.154617161672991
Experience 18, Iter 65, disc loss: 0.0015720918844830305, policy loss: 8.489300530821634
Experience 18, Iter 66, disc loss: 0.0014662666073913832, policy loss: 9.10737161786719
Experience 18, Iter 67, disc loss: 0.0014049415480229305, policy loss: 9.07179856744069
Experience 18, Iter 68, disc loss: 0.0014984344068898986, policy loss: 8.950347866108178
Experience 18, Iter 69, disc loss: 0.0018487293870975057, policy loss: 8.228336359162315
Experience 18, Iter 70, disc loss: 0.0017595417608720224, policy loss: 8.269973648032032
Experience 18, Iter 71, disc loss: 0.0013139352035308341, policy loss: 9.366944517282722
Experience 18, Iter 72, disc loss: 0.0018225992340653015, policy loss: 8.066216828474595
Experience 18, Iter 73, disc loss: 0.0012749059442823303, policy loss: 9.703031170062225
Experience 18, Iter 74, disc loss: 0.0012006762982863235, policy loss: 9.732187150038376
Experience 18, Iter 75, disc loss: 0.0012132400527723741, policy loss: 9.381123598086356
Experience 18, Iter 76, disc loss: 0.0010933704300414896, policy loss: 10.279431540501786
Experience 18, Iter 77, disc loss: 0.001086783436442623, policy loss: 9.636950061804251
Experience 18, Iter 78, disc loss: 0.0011642907916497647, policy loss: 9.332034419008037
Experience 18, Iter 79, disc loss: 0.0013806195870652955, policy loss: 8.965559506011896
Experience 18, Iter 80, disc loss: 0.001707068882183548, policy loss: 8.469567495920382
Experience 18, Iter 81, disc loss: 0.0016833105607182756, policy loss: 8.364093600178657
Experience 18, Iter 82, disc loss: 0.0014438918122787452, policy loss: 8.554520448537799
Experience 18, Iter 83, disc loss: 0.0011302003318464525, policy loss: 9.333273102902655
Experience 18, Iter 84, disc loss: 0.0015180734012850406, policy loss: 8.239093693275484
Experience 18, Iter 85, disc loss: 0.0013573143651278764, policy loss: 8.780191911375356
Experience 18, Iter 86, disc loss: 0.0012664859827455008, policy loss: 9.03130396571359
Experience 18, Iter 87, disc loss: 0.0012411025863809696, policy loss: 8.486247158087995
Experience 18, Iter 88, disc loss: 0.0014837706841066617, policy loss: 8.104406014096636
Experience 18, Iter 89, disc loss: 0.0014140204528945348, policy loss: 8.260240286739645
Experience 18, Iter 90, disc loss: 0.0016982685310629054, policy loss: 8.214729006661448
Experience 18, Iter 91, disc loss: 0.0017915078950508693, policy loss: 8.112811204744075
Experience 18, Iter 92, disc loss: 0.0022960337861526996, policy loss: 8.505185996476413
Experience 18, Iter 93, disc loss: 0.001648438694855405, policy loss: 8.128063793214293
Experience 18, Iter 94, disc loss: 0.0017264562076093762, policy loss: 7.724028510664773
Experience 18, Iter 95, disc loss: 0.001723141385181135, policy loss: 7.864687051237675
Experience 18, Iter 96, disc loss: 0.001978576106245581, policy loss: 7.504629853257128
Experience 18, Iter 97, disc loss: 0.0016326986894694378, policy loss: 8.002678219196136
Experience 18, Iter 98, disc loss: 0.0019415578466736544, policy loss: 8.342396877154203
Experience 18, Iter 99, disc loss: 0.002039933752801567, policy loss: 7.654662298032747
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0044],
        [0.2341],
        [1.9270],
        [0.0400]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.1862, 1.8257, 0.0326, 0.0281, 5.7334]],

        [[0.0153, 0.1862, 1.8257, 0.0326, 0.0281, 5.7334]],

        [[0.0153, 0.1862, 1.8257, 0.0326, 0.0281, 5.7334]],

        [[0.0153, 0.1862, 1.8257, 0.0326, 0.0281, 5.7334]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0176, 0.9363, 7.7079, 0.1600], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0176, 0.9363, 7.7079, 0.1600])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.807
Iter 2/2000 - Loss: 3.747
Iter 3/2000 - Loss: 3.539
Iter 4/2000 - Loss: 3.465
Iter 5/2000 - Loss: 3.431
Iter 6/2000 - Loss: 3.308
Iter 7/2000 - Loss: 3.154
Iter 8/2000 - Loss: 3.014
Iter 9/2000 - Loss: 2.863
Iter 10/2000 - Loss: 2.676
Iter 11/2000 - Loss: 2.471
Iter 12/2000 - Loss: 2.266
Iter 13/2000 - Loss: 2.057
Iter 14/2000 - Loss: 1.827
Iter 15/2000 - Loss: 1.569
Iter 16/2000 - Loss: 1.290
Iter 17/2000 - Loss: 1.005
Iter 18/2000 - Loss: 0.718
Iter 19/2000 - Loss: 0.429
Iter 20/2000 - Loss: 0.136
Iter 1981/2000 - Loss: -7.284
Iter 1982/2000 - Loss: -7.284
Iter 1983/2000 - Loss: -7.284
Iter 1984/2000 - Loss: -7.284
Iter 1985/2000 - Loss: -7.284
Iter 1986/2000 - Loss: -7.284
Iter 1987/2000 - Loss: -7.284
Iter 1988/2000 - Loss: -7.284
Iter 1989/2000 - Loss: -7.284
Iter 1990/2000 - Loss: -7.284
Iter 1991/2000 - Loss: -7.284
Iter 1992/2000 - Loss: -7.284
Iter 1993/2000 - Loss: -7.284
Iter 1994/2000 - Loss: -7.284
Iter 1995/2000 - Loss: -7.284
Iter 1996/2000 - Loss: -7.284
Iter 1997/2000 - Loss: -7.284
Iter 1998/2000 - Loss: -7.284
Iter 1999/2000 - Loss: -7.285
Iter 2000/2000 - Loss: -7.285
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[11.9782,  6.4691, 23.4540,  1.9563, 17.0173, 42.7129]],

        [[17.4492, 33.4858,  6.8937,  1.2308,  1.5575, 31.5857]],

        [[16.9627, 35.5699,  8.6043,  0.9542,  0.6983, 25.4775]],

        [[15.1637, 28.3993, 13.1671,  1.6180,  1.7305, 48.3318]]])
Signal Variance: tensor([ 0.0686,  2.2074, 12.8057,  0.3901])
Estimated target variance: tensor([0.0176, 0.9363, 7.7079, 0.1600])
N: 190
Signal to noise ratio: tensor([14.8725, 78.8181, 71.4765, 33.5869])
Bound on condition number: tensor([  42027.6085, 1180335.7899,  970689.3896,  214335.6991])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0016111114766282563, policy loss: 8.408013358338643
Experience 19, Iter 1, disc loss: 0.0019181103606121539, policy loss: 7.782606737219226
Experience 19, Iter 2, disc loss: 0.0018129667973820793, policy loss: 7.965524948109586
Experience 19, Iter 3, disc loss: 0.0019126390839818153, policy loss: 8.39347376316157
Experience 19, Iter 4, disc loss: 0.0020396372252974183, policy loss: 7.73943829275721
Experience 19, Iter 5, disc loss: 0.001908577477962357, policy loss: 7.950377478574301
Experience 19, Iter 6, disc loss: 0.0018457760928197089, policy loss: 7.983055569403723
Experience 19, Iter 7, disc loss: 0.0017865661899802412, policy loss: 8.079683618594128
Experience 19, Iter 8, disc loss: 0.002073777897424776, policy loss: 7.869851980230979
Experience 19, Iter 9, disc loss: 0.0019411761804255, policy loss: 8.279508750138472
Experience 19, Iter 10, disc loss: 0.0018153824038701807, policy loss: 7.694527238967871
Experience 19, Iter 11, disc loss: 0.0019821389523752937, policy loss: 8.16006760359447
Experience 19, Iter 12, disc loss: 0.0018560015697381559, policy loss: 7.782922034060796
Experience 19, Iter 13, disc loss: 0.0018272741490668715, policy loss: 8.532340331410289
Experience 19, Iter 14, disc loss: 0.0020988426172767133, policy loss: 8.007360328050126
Experience 19, Iter 15, disc loss: 0.0018406467032423307, policy loss: 7.955073785457131
Experience 19, Iter 16, disc loss: 0.0015085266228628646, policy loss: 8.260619516742665
Experience 19, Iter 17, disc loss: 0.0017433119252957038, policy loss: 8.657162153223746
Experience 19, Iter 18, disc loss: 0.0018777446994736103, policy loss: 8.154257302469755
Experience 19, Iter 19, disc loss: 0.0018699507084612734, policy loss: 7.871925898726842
Experience 19, Iter 20, disc loss: 0.002193218873668249, policy loss: 7.8961656035242
Experience 19, Iter 21, disc loss: 0.001978473216716868, policy loss: 8.181919074073711
Experience 19, Iter 22, disc loss: 0.001750398079655138, policy loss: 7.845234104804824
Experience 19, Iter 23, disc loss: 0.0019106525355286133, policy loss: 7.782022843697036
Experience 19, Iter 24, disc loss: 0.0018973896880498961, policy loss: 7.800568788963311
Experience 19, Iter 25, disc loss: 0.0018935717399461605, policy loss: 8.121178449057535
Experience 19, Iter 26, disc loss: 0.0017806965986169488, policy loss: 8.43833773364929
Experience 19, Iter 27, disc loss: 0.0020552290390128446, policy loss: 7.820070061590357
Experience 19, Iter 28, disc loss: 0.001978042062401586, policy loss: 8.270347045971848
Experience 19, Iter 29, disc loss: 0.002007274973934148, policy loss: 8.503973713411245
Experience 19, Iter 30, disc loss: 0.0015394679751143522, policy loss: 8.426136512122591
Experience 19, Iter 31, disc loss: 0.0020387169862202755, policy loss: 7.81791389845753
Experience 19, Iter 32, disc loss: 0.002117882228374818, policy loss: 7.66029116789578
Experience 19, Iter 33, disc loss: 0.002494679713271753, policy loss: 7.870047713478042
Experience 19, Iter 34, disc loss: 0.0021207791640986606, policy loss: 8.102657086151382
Experience 19, Iter 35, disc loss: 0.001972061526887079, policy loss: 7.848665359508017
Experience 19, Iter 36, disc loss: 0.002172423509006852, policy loss: 7.952285093780566
Experience 19, Iter 37, disc loss: 0.0018417791701500147, policy loss: 7.961097214345552
Experience 19, Iter 38, disc loss: 0.0017653023404818834, policy loss: 8.417161027662793
Experience 19, Iter 39, disc loss: 0.0017662404648267987, policy loss: 8.250757147636975
Experience 19, Iter 40, disc loss: 0.001645710315256716, policy loss: 8.243074538733268
Experience 19, Iter 41, disc loss: 0.0018919704310307243, policy loss: 8.31146486798908
Experience 19, Iter 42, disc loss: 0.0017331859864521942, policy loss: 8.352566191209835
Experience 19, Iter 43, disc loss: 0.0017777832271823066, policy loss: 7.896020626745527
Experience 19, Iter 44, disc loss: 0.0017546339508560738, policy loss: 8.349034240943281
Experience 19, Iter 45, disc loss: 0.001648629588662436, policy loss: 8.834400028374626
Experience 19, Iter 46, disc loss: 0.0019460314795728478, policy loss: 7.996224392900223
Experience 19, Iter 47, disc loss: 0.0016399447794372326, policy loss: 8.501228807313433
Experience 19, Iter 48, disc loss: 0.001833169153454735, policy loss: 8.58700870642295
Experience 19, Iter 49, disc loss: 0.0016851407114431163, policy loss: 8.362992499463545
Experience 19, Iter 50, disc loss: 0.0016300809435460034, policy loss: 8.464936362326014
Experience 19, Iter 51, disc loss: 0.0016209430608825264, policy loss: 8.45537792629382
Experience 19, Iter 52, disc loss: 0.002028704242724825, policy loss: 7.881829579623276
Experience 19, Iter 53, disc loss: 0.0016711347759203249, policy loss: 8.53595174701373
Experience 19, Iter 54, disc loss: 0.002010923865261348, policy loss: 8.398870476934988
Experience 19, Iter 55, disc loss: 0.0018304241032942173, policy loss: 7.957860218338734
Experience 19, Iter 56, disc loss: 0.0015361274145749833, policy loss: 8.676441040162608
Experience 19, Iter 57, disc loss: 0.001740506769215407, policy loss: 8.702577678755029
Experience 19, Iter 58, disc loss: 0.0020193082644167984, policy loss: 8.331380307704325
Experience 19, Iter 59, disc loss: 0.0018596689563275203, policy loss: 7.967852562700585
Experience 19, Iter 60, disc loss: 0.0015106041936655177, policy loss: 8.270033363048164
Experience 19, Iter 61, disc loss: 0.001783168682748742, policy loss: 8.40000797334378
Experience 19, Iter 62, disc loss: 0.0020098208011971705, policy loss: 8.287023342008855
Experience 19, Iter 63, disc loss: 0.0018133758744185032, policy loss: 8.677947042908354
Experience 19, Iter 64, disc loss: 0.001690036109585172, policy loss: 9.054223547518198
Experience 19, Iter 65, disc loss: 0.001812401491713319, policy loss: 7.964096948312012
Experience 19, Iter 66, disc loss: 0.0017939389460941496, policy loss: 8.149148604698189
Experience 19, Iter 67, disc loss: 0.0016313530752268747, policy loss: 8.478022860624394
Experience 19, Iter 68, disc loss: 0.0016669782888162894, policy loss: 8.722113678766902
Experience 19, Iter 69, disc loss: 0.0015976725646898063, policy loss: 8.376738109553424
Experience 19, Iter 70, disc loss: 0.0016512658627674025, policy loss: 8.217509689133959
Experience 19, Iter 71, disc loss: 0.0018717433627299838, policy loss: 8.344119539631198
Experience 19, Iter 72, disc loss: 0.0018387696992268228, policy loss: 8.19169066209195
Experience 19, Iter 73, disc loss: 0.0019775362238522933, policy loss: 8.077427806196471
Experience 19, Iter 74, disc loss: 0.0019416234597621701, policy loss: 7.83190139056667
Experience 19, Iter 75, disc loss: 0.0017270687319700536, policy loss: 7.829736871177765
Experience 19, Iter 76, disc loss: 0.0017243113461535408, policy loss: 8.326975192615153
Experience 19, Iter 77, disc loss: 0.0019226870295962208, policy loss: 8.147728101198666
Experience 19, Iter 78, disc loss: 0.0015143956789784366, policy loss: 8.31617259894401
Experience 19, Iter 79, disc loss: 0.0013745364480654428, policy loss: 8.634250346603801
Experience 19, Iter 80, disc loss: 0.0014346523735538459, policy loss: 9.330880119449313
Experience 19, Iter 81, disc loss: 0.0015793694572841534, policy loss: 8.435026039772307
Experience 19, Iter 82, disc loss: 0.0016937381065834068, policy loss: 8.393258012143544
Experience 19, Iter 83, disc loss: 0.0014829395753157325, policy loss: 8.645837418869657
Experience 19, Iter 84, disc loss: 0.0019654328157396794, policy loss: 7.968466147189046
Experience 19, Iter 85, disc loss: 0.0019672057477759424, policy loss: 7.954737181463926
Experience 19, Iter 86, disc loss: 0.0017820141215026185, policy loss: 7.740809367481494
Experience 19, Iter 87, disc loss: 0.0014929767565627237, policy loss: 8.3912911757308
Experience 19, Iter 88, disc loss: 0.001555935439774825, policy loss: 8.8797786305737
Experience 19, Iter 89, disc loss: 0.0017632713457841512, policy loss: 8.164918438445243
Experience 19, Iter 90, disc loss: 0.0019615514098837846, policy loss: 7.725643218478002
Experience 19, Iter 91, disc loss: 0.0017792589804567953, policy loss: 8.124677077656948
Experience 19, Iter 92, disc loss: 0.001800432886124984, policy loss: 8.149976910800248
Experience 19, Iter 93, disc loss: 0.0015536578643682555, policy loss: 8.230416473505755
Experience 19, Iter 94, disc loss: 0.001587452625515187, policy loss: 8.217673304120602
Experience 19, Iter 95, disc loss: 0.0014825625059931265, policy loss: 8.659147078997893
Experience 19, Iter 96, disc loss: 0.001644164037282076, policy loss: 8.316651593480964
Experience 19, Iter 97, disc loss: 0.001603256000774704, policy loss: 8.345106751036553
Experience 19, Iter 98, disc loss: 0.0015269104371760148, policy loss: 8.727762479937041
Experience 19, Iter 99, disc loss: 0.0014751786936421751, policy loss: 8.96824559820229
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0043],
        [0.2304],
        [1.8954],
        [0.0392]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0147, 0.1825, 1.7909, 0.0329, 0.0282, 5.7320]],

        [[0.0147, 0.1825, 1.7909, 0.0329, 0.0282, 5.7320]],

        [[0.0147, 0.1825, 1.7909, 0.0329, 0.0282, 5.7320]],

        [[0.0147, 0.1825, 1.7909, 0.0329, 0.0282, 5.7320]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0173, 0.9215, 7.5815, 0.1567], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0173, 0.9215, 7.5815, 0.1567])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.778
Iter 2/2000 - Loss: 3.747
Iter 3/2000 - Loss: 3.532
Iter 4/2000 - Loss: 3.463
Iter 5/2000 - Loss: 3.429
Iter 6/2000 - Loss: 3.304
Iter 7/2000 - Loss: 3.152
Iter 8/2000 - Loss: 3.017
Iter 9/2000 - Loss: 2.871
Iter 10/2000 - Loss: 2.686
Iter 11/2000 - Loss: 2.478
Iter 12/2000 - Loss: 2.270
Iter 13/2000 - Loss: 2.061
Iter 14/2000 - Loss: 1.831
Iter 15/2000 - Loss: 1.572
Iter 16/2000 - Loss: 1.288
Iter 17/2000 - Loss: 0.995
Iter 18/2000 - Loss: 0.700
Iter 19/2000 - Loss: 0.405
Iter 20/2000 - Loss: 0.106
Iter 1981/2000 - Loss: -7.349
Iter 1982/2000 - Loss: -7.349
Iter 1983/2000 - Loss: -7.349
Iter 1984/2000 - Loss: -7.349
Iter 1985/2000 - Loss: -7.349
Iter 1986/2000 - Loss: -7.349
Iter 1987/2000 - Loss: -7.349
Iter 1988/2000 - Loss: -7.349
Iter 1989/2000 - Loss: -7.349
Iter 1990/2000 - Loss: -7.349
Iter 1991/2000 - Loss: -7.349
Iter 1992/2000 - Loss: -7.350
Iter 1993/2000 - Loss: -7.350
Iter 1994/2000 - Loss: -7.350
Iter 1995/2000 - Loss: -7.350
Iter 1996/2000 - Loss: -7.350
Iter 1997/2000 - Loss: -7.350
Iter 1998/2000 - Loss: -7.350
Iter 1999/2000 - Loss: -7.350
Iter 2000/2000 - Loss: -7.350
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[11.9459,  6.3841, 23.3623,  1.8900, 16.9099, 41.6294]],

        [[17.2163, 33.5658,  7.5199,  1.1651,  1.6565, 33.3378]],

        [[16.5601, 35.8739,  8.7990,  0.8945,  0.7306, 26.2540]],

        [[14.7128, 27.7533, 13.9255,  1.6033,  1.7349, 47.4666]]])
Signal Variance: tensor([ 0.0651,  2.4168, 13.0621,  0.3872])
Estimated target variance: tensor([0.0173, 0.9215, 7.5815, 0.1567])
N: 200
Signal to noise ratio: tensor([14.5960, 80.1976, 72.3831, 33.0004])
Bound on condition number: tensor([  42609.4714, 1286332.7135, 1047863.9120,  217806.2618])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0013051278524748456, policy loss: 8.896839092742148
Experience 20, Iter 1, disc loss: 0.001204984536792982, policy loss: 9.4025384937926
Experience 20, Iter 2, disc loss: 0.001309299599352139, policy loss: 8.818295403445095
Experience 20, Iter 3, disc loss: 0.001894767573311115, policy loss: 7.9088596678710825
Experience 20, Iter 4, disc loss: 0.0016112183983447016, policy loss: 8.12140837540786
Experience 20, Iter 5, disc loss: 0.0016944456540546053, policy loss: 8.384067101517608
Experience 20, Iter 6, disc loss: 0.0017399869163108766, policy loss: 8.056296029456881
Experience 20, Iter 7, disc loss: 0.0016501370254165737, policy loss: 8.36487777610515
Experience 20, Iter 8, disc loss: 0.0015506756314986221, policy loss: 8.343706291435515
Experience 20, Iter 9, disc loss: 0.0017891962632207116, policy loss: 8.035285198794162
Experience 20, Iter 10, disc loss: 0.00175808938343571, policy loss: 8.118697832996839
Experience 20, Iter 11, disc loss: 0.0019441322791864798, policy loss: 7.842738429578113
Experience 20, Iter 12, disc loss: 0.0014194332419397544, policy loss: 9.045575493259028
Experience 20, Iter 13, disc loss: 0.001810516858122119, policy loss: 7.881373690978708
Experience 20, Iter 14, disc loss: 0.0016022794456903684, policy loss: 8.079293200437153
Experience 20, Iter 15, disc loss: 0.0017763827017752837, policy loss: 8.064134652083036
Experience 20, Iter 16, disc loss: 0.001856003759284567, policy loss: 8.053214817191712
Experience 20, Iter 17, disc loss: 0.001938852556806268, policy loss: 7.886828975999899
Experience 20, Iter 18, disc loss: 0.001740828401262347, policy loss: 7.915153113827042
Experience 20, Iter 19, disc loss: 0.0019424403923273314, policy loss: 7.767026612024821
Experience 20, Iter 20, disc loss: 0.0017983582359233836, policy loss: 8.17223286302843
Experience 20, Iter 21, disc loss: 0.001628356733481612, policy loss: 8.339239049739291
Experience 20, Iter 22, disc loss: 0.0018596031000364616, policy loss: 8.2898644965537
Experience 20, Iter 23, disc loss: 0.0016691288270947462, policy loss: 8.169557250024756
Experience 20, Iter 24, disc loss: 0.001787580868099396, policy loss: 7.977260704109563
Experience 20, Iter 25, disc loss: 0.0017451782996807381, policy loss: 8.332623527343692
Experience 20, Iter 26, disc loss: 0.0014836977371974007, policy loss: 9.281616264885113
Experience 20, Iter 27, disc loss: 0.0015683571120332892, policy loss: 8.70784179546552
Experience 20, Iter 28, disc loss: 0.0017144985369949434, policy loss: 8.438323628357404
Experience 20, Iter 29, disc loss: 0.0014594120435224203, policy loss: 8.304621442837004
Experience 20, Iter 30, disc loss: 0.0016986958839281274, policy loss: 8.334436929799619
Experience 20, Iter 31, disc loss: 0.002103517882851065, policy loss: 8.02114578546402
Experience 20, Iter 32, disc loss: 0.0015656291624887402, policy loss: 8.419161335784636
Experience 20, Iter 33, disc loss: 0.0014727322143079083, policy loss: 8.406973492571879
Experience 20, Iter 34, disc loss: 0.00179757823524353, policy loss: 8.521640772088823
Experience 20, Iter 35, disc loss: 0.0015066552224337459, policy loss: 9.637120464643367
Experience 20, Iter 36, disc loss: 0.0016806808065195182, policy loss: 8.761126807664741
Experience 20, Iter 37, disc loss: 0.0017661339108529567, policy loss: 8.543992892537199
Experience 20, Iter 38, disc loss: 0.001602242060868282, policy loss: 8.377260907307098
Experience 20, Iter 39, disc loss: 0.0017261895777540576, policy loss: 7.868307248606677
Experience 20, Iter 40, disc loss: 0.0019989791910276777, policy loss: 7.789484058921274
Experience 20, Iter 41, disc loss: 0.001658136459595287, policy loss: 8.344650560319664
Experience 20, Iter 42, disc loss: 0.0016855162860942591, policy loss: 8.208042568607048
Experience 20, Iter 43, disc loss: 0.0019557652233056994, policy loss: 8.38391925462522
Experience 20, Iter 44, disc loss: 0.0016973019240448515, policy loss: 8.5200721756077
Experience 20, Iter 45, disc loss: 0.0015583395596885238, policy loss: 8.633371962006306
Experience 20, Iter 46, disc loss: 0.0018488021205177762, policy loss: 8.24560596770789
Experience 20, Iter 47, disc loss: 0.0015992824006606425, policy loss: 8.478320743980685
Experience 20, Iter 48, disc loss: 0.0014866629190488708, policy loss: 8.927813415654999
Experience 20, Iter 49, disc loss: 0.0016393748422713047, policy loss: 7.722513928624105
Experience 20, Iter 50, disc loss: 0.0014521322545075236, policy loss: 9.08675150683463
Experience 20, Iter 51, disc loss: 0.0016180452598252603, policy loss: 9.057758951463503
Experience 20, Iter 52, disc loss: 0.001484609766741269, policy loss: 8.435823128705291
Experience 20, Iter 53, disc loss: 0.0016972773366849005, policy loss: 8.207990018064033
Experience 20, Iter 54, disc loss: 0.001654876347972085, policy loss: 8.61591426110903
Experience 20, Iter 55, disc loss: 0.0018452153308417978, policy loss: 8.142905722561046
Experience 20, Iter 56, disc loss: 0.0015048947156972299, policy loss: 8.722085600723025
Experience 20, Iter 57, disc loss: 0.0014734371272773325, policy loss: 8.58937011225544
Experience 20, Iter 58, disc loss: 0.001622939683667822, policy loss: 8.189866837768584
Experience 20, Iter 59, disc loss: 0.0013794533710452276, policy loss: 8.826946765570078
Experience 20, Iter 60, disc loss: 0.0015923747714621367, policy loss: 8.249809462350857
Experience 20, Iter 61, disc loss: 0.001677861569373018, policy loss: 8.398570711489526
Experience 20, Iter 62, disc loss: 0.0018465716056936783, policy loss: 8.11535300654309
Experience 20, Iter 63, disc loss: 0.0015535120776615577, policy loss: 8.602469218932633
Experience 20, Iter 64, disc loss: 0.0015084465702683019, policy loss: 8.273393225590404
Experience 20, Iter 65, disc loss: 0.0018022849182214025, policy loss: 8.189329183527553
Experience 20, Iter 66, disc loss: 0.0019127170385035569, policy loss: 8.289209791302156
Experience 20, Iter 67, disc loss: 0.001559694664611291, policy loss: 8.258676140112575
Experience 20, Iter 68, disc loss: 0.0015540046599460495, policy loss: 8.654247677504884
Experience 20, Iter 69, disc loss: 0.0017614788585547598, policy loss: 8.173920357284999
Experience 20, Iter 70, disc loss: 0.0015821062397048079, policy loss: 8.177885853260582
Experience 20, Iter 71, disc loss: 0.001815923498289539, policy loss: 7.956369415942212
Experience 20, Iter 72, disc loss: 0.001500203307594606, policy loss: 8.907451753405859
Experience 20, Iter 73, disc loss: 0.0010500107893694618, policy loss: 11.471573119927623
Experience 20, Iter 74, disc loss: 0.000892339289765536, policy loss: 11.070539341795286
Experience 20, Iter 75, disc loss: 0.0008033079186755334, policy loss: 13.781801616917011
Experience 20, Iter 76, disc loss: 0.0007576441459459703, policy loss: 16.509680183666767
Experience 20, Iter 77, disc loss: 0.0007504825105593892, policy loss: 16.090132998667336
Experience 20, Iter 78, disc loss: 0.0007345202389604604, policy loss: 16.147953994238733
Experience 20, Iter 79, disc loss: 0.0007207034437258294, policy loss: 16.39103663269485
Experience 20, Iter 80, disc loss: 0.0007068829595488766, policy loss: 15.331445005506117
Experience 20, Iter 81, disc loss: 0.0006877928414089438, policy loss: 14.991610890447326
Experience 20, Iter 82, disc loss: 0.0006702479754860058, policy loss: 14.770499183980311
Experience 20, Iter 83, disc loss: 0.0006522760396078757, policy loss: 15.353940600031216
Experience 20, Iter 84, disc loss: 0.0006339002558666754, policy loss: 15.206519950289284
Experience 20, Iter 85, disc loss: 0.0006154997198409251, policy loss: 15.314993317203408
Experience 20, Iter 86, disc loss: 0.0005971182040331198, policy loss: 15.660400480759828
Experience 20, Iter 87, disc loss: 0.0005790150895988233, policy loss: 15.989125718722894
Experience 20, Iter 88, disc loss: 0.0005614235486088358, policy loss: 15.958902297654006
Experience 20, Iter 89, disc loss: 0.0005441229536161188, policy loss: 15.267981614974936
Experience 20, Iter 90, disc loss: 0.0005272204282697558, policy loss: 15.658228703915647
Experience 20, Iter 91, disc loss: 0.0005108374293935721, policy loss: 15.94249539906119
Experience 20, Iter 92, disc loss: 0.0004951054097373865, policy loss: 15.393798698949617
Experience 20, Iter 93, disc loss: 0.0004800986176448732, policy loss: 14.691666488851936
Experience 20, Iter 94, disc loss: 0.0004654251077723449, policy loss: 14.869580525035076
Experience 20, Iter 95, disc loss: 0.0004515780332359687, policy loss: 14.418440494228516
Experience 20, Iter 96, disc loss: 0.0004388701242892202, policy loss: 14.396920631495886
Experience 20, Iter 97, disc loss: 0.0004258663778654565, policy loss: 14.05238669773924
Experience 20, Iter 98, disc loss: 0.00041443378683809573, policy loss: 13.496477719978087
Experience 20, Iter 99, disc loss: 0.00040325780495224366, policy loss: 13.885106532695715
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0043],
        [0.2299],
        [1.9367],
        [0.0391]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0146, 0.1825, 1.7948, 0.0335, 0.0283, 5.7251]],

        [[0.0146, 0.1825, 1.7948, 0.0335, 0.0283, 5.7251]],

        [[0.0146, 0.1825, 1.7948, 0.0335, 0.0283, 5.7251]],

        [[0.0146, 0.1825, 1.7948, 0.0335, 0.0283, 5.7251]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0171, 0.9198, 7.7466, 0.1565], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0171, 0.9198, 7.7466, 0.1565])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.785
Iter 2/2000 - Loss: 3.761
Iter 3/2000 - Loss: 3.544
Iter 4/2000 - Loss: 3.480
Iter 5/2000 - Loss: 3.449
Iter 6/2000 - Loss: 3.326
Iter 7/2000 - Loss: 3.176
Iter 8/2000 - Loss: 3.043
Iter 9/2000 - Loss: 2.903
Iter 10/2000 - Loss: 2.724
Iter 11/2000 - Loss: 2.521
Iter 12/2000 - Loss: 2.319
Iter 13/2000 - Loss: 2.116
Iter 14/2000 - Loss: 1.893
Iter 15/2000 - Loss: 1.641
Iter 16/2000 - Loss: 1.364
Iter 17/2000 - Loss: 1.076
Iter 18/2000 - Loss: 0.786
Iter 19/2000 - Loss: 0.493
Iter 20/2000 - Loss: 0.195
Iter 1981/2000 - Loss: -7.372
Iter 1982/2000 - Loss: -7.372
Iter 1983/2000 - Loss: -7.372
Iter 1984/2000 - Loss: -7.372
Iter 1985/2000 - Loss: -7.372
Iter 1986/2000 - Loss: -7.372
Iter 1987/2000 - Loss: -7.372
Iter 1988/2000 - Loss: -7.372
Iter 1989/2000 - Loss: -7.372
Iter 1990/2000 - Loss: -7.372
Iter 1991/2000 - Loss: -7.372
Iter 1992/2000 - Loss: -7.372
Iter 1993/2000 - Loss: -7.372
Iter 1994/2000 - Loss: -7.372
Iter 1995/2000 - Loss: -7.372
Iter 1996/2000 - Loss: -7.372
Iter 1997/2000 - Loss: -7.372
Iter 1998/2000 - Loss: -7.372
Iter 1999/2000 - Loss: -7.372
Iter 2000/2000 - Loss: -7.373
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[11.7358,  5.9971, 23.6671,  1.9115, 17.1963, 36.8836]],

        [[17.1133, 34.6400,  7.3785,  1.1677,  1.7160, 33.4318]],

        [[16.6393, 35.9379,  8.6671,  0.8893,  0.7174, 25.8001]],

        [[14.3829, 28.1829, 14.9751,  1.5756,  1.8380, 49.4114]]])
Signal Variance: tensor([ 0.0625,  2.4305, 12.4176,  0.4090])
Estimated target variance: tensor([0.0171, 0.9198, 7.7466, 0.1565])
N: 210
Signal to noise ratio: tensor([14.2024, 79.2207, 70.8596, 33.7390])
Bound on condition number: tensor([  42359.4838, 1317945.1820, 1054429.8585,  239047.9281])
Policy Optimizer learning rate:
0.000979148362360977
Experience 21, Iter 0, disc loss: 0.0003918831394790599, policy loss: 13.772677365609827
Experience 21, Iter 1, disc loss: 0.00038162123061541827, policy loss: 13.39272418150027
Experience 21, Iter 2, disc loss: 0.0003726142976713254, policy loss: 13.020441157964216
Experience 21, Iter 3, disc loss: 0.00036499585940775054, policy loss: 12.899475246578158
Experience 21, Iter 4, disc loss: 0.0003590087337292479, policy loss: 12.60120597936171
Experience 21, Iter 5, disc loss: 0.00036014110772209564, policy loss: 12.474276433634758
Experience 21, Iter 6, disc loss: 0.00037594335089794926, policy loss: 11.623432622670293
Experience 21, Iter 7, disc loss: 0.00036353569467417046, policy loss: 11.219196061302663
Experience 21, Iter 8, disc loss: 0.0003789319584005551, policy loss: 11.178876159803394
Experience 21, Iter 9, disc loss: 0.00043406680472101144, policy loss: 10.059358176111425
Experience 21, Iter 10, disc loss: 0.0005386079430135286, policy loss: 9.622882188536519
Experience 21, Iter 11, disc loss: 0.0007017508913650704, policy loss: 8.95897811496567
Experience 21, Iter 12, disc loss: 0.0009282262862467263, policy loss: 8.540667813556942
Experience 21, Iter 13, disc loss: 0.0011271172335755456, policy loss: 8.802754466855301
Experience 21, Iter 14, disc loss: 0.00115322939656244, policy loss: 9.013223366484947
Experience 21, Iter 15, disc loss: 0.0011318847454496817, policy loss: 9.541772329133664
Experience 21, Iter 16, disc loss: 0.001668583281886476, policy loss: 7.899471074008304
Experience 21, Iter 17, disc loss: 0.001098875017696863, policy loss: 8.007643653989728
Experience 21, Iter 18, disc loss: 0.0008405875717725911, policy loss: 8.470740043186789
Experience 21, Iter 19, disc loss: 0.0007306905876812387, policy loss: 8.691385225583431
Experience 21, Iter 20, disc loss: 0.0007769836867950984, policy loss: 8.390540781037522
Experience 21, Iter 21, disc loss: 0.0006906396760954195, policy loss: 8.595566212220449
Experience 21, Iter 22, disc loss: 0.0006081315857605102, policy loss: 9.261975701066444
Experience 21, Iter 23, disc loss: 0.0007636740921373546, policy loss: 8.55392736428313
Experience 21, Iter 24, disc loss: 0.0008400273309008339, policy loss: 8.369266567690449
Experience 21, Iter 25, disc loss: 0.000835041479811508, policy loss: 8.742865539810982
Experience 21, Iter 26, disc loss: 0.0010134599633238564, policy loss: 8.277385868290974
Experience 21, Iter 27, disc loss: 0.0013420693036364255, policy loss: 8.317469531446177
Experience 21, Iter 28, disc loss: 0.001465308813527115, policy loss: 7.73694952679835
Experience 21, Iter 29, disc loss: 0.0017325193270099262, policy loss: 8.83369582995875
Experience 21, Iter 30, disc loss: 0.0014585406394416148, policy loss: 8.155901779238086
Experience 21, Iter 31, disc loss: 0.001554201443747617, policy loss: 7.750865625939671
Experience 21, Iter 32, disc loss: 0.0012043097693484267, policy loss: 8.719316630340419
Experience 21, Iter 33, disc loss: 0.0011431476987054166, policy loss: 7.822439063365424
Experience 21, Iter 34, disc loss: 0.000742858687146284, policy loss: 9.164795085721767
Experience 21, Iter 35, disc loss: 0.0009952394120111008, policy loss: 8.611031888065643
Experience 21, Iter 36, disc loss: 0.0008971373290769292, policy loss: 8.487042891698572
Experience 21, Iter 37, disc loss: 0.001153142790710741, policy loss: 8.647996240956036
Experience 21, Iter 38, disc loss: 0.0013203004121515472, policy loss: 8.225201766928162
Experience 21, Iter 39, disc loss: 0.0014549011146987546, policy loss: 8.423160489173084
Experience 21, Iter 40, disc loss: 0.0013404567456812815, policy loss: 8.39506050163267
Experience 21, Iter 41, disc loss: 0.0015810746448898505, policy loss: 7.937383741314171
Experience 21, Iter 42, disc loss: 0.0015746019476632438, policy loss: 7.722959703454681
Experience 21, Iter 43, disc loss: 0.001214035941666841, policy loss: 8.470239272081084
Experience 21, Iter 44, disc loss: 0.0011303909219353096, policy loss: 8.745214206309948
Experience 21, Iter 45, disc loss: 0.0011731825876815376, policy loss: 8.57939130482642
Experience 21, Iter 46, disc loss: 0.001109593206074727, policy loss: 9.18399735972055
Experience 21, Iter 47, disc loss: 0.0012332821928256578, policy loss: 8.319962930141124
Experience 21, Iter 48, disc loss: 0.0016202562791175473, policy loss: 7.905129155479472
Experience 21, Iter 49, disc loss: 0.001590403558650767, policy loss: 8.185879654434117
Experience 21, Iter 50, disc loss: 0.0015494129794786198, policy loss: 8.136436417425303
Experience 21, Iter 51, disc loss: 0.0014350977067837702, policy loss: 7.930284170874245
Experience 21, Iter 52, disc loss: 0.0010213766964222639, policy loss: 9.127760612497408
Experience 21, Iter 53, disc loss: 0.0012968126658065415, policy loss: 8.196683521225355
Experience 21, Iter 54, disc loss: 0.0010833633114957705, policy loss: 8.973666467176216
Experience 21, Iter 55, disc loss: 0.0012854562856623496, policy loss: 8.358062933439529
Experience 21, Iter 56, disc loss: 0.00122934488592487, policy loss: 8.580742799644877
Experience 21, Iter 57, disc loss: 0.0013702901315744524, policy loss: 8.641600757617622
Experience 21, Iter 58, disc loss: 0.0012394376696388291, policy loss: 8.513741701535851
Experience 21, Iter 59, disc loss: 0.0016361935747229452, policy loss: 7.948117970367887
Experience 21, Iter 60, disc loss: 0.0015843181279473434, policy loss: 8.688431817504473
Experience 21, Iter 61, disc loss: 0.0016809021350126015, policy loss: 8.014460208186058
Experience 21, Iter 62, disc loss: 0.0014333732898803235, policy loss: 8.152324811934564
Experience 21, Iter 63, disc loss: 0.0013153137511988286, policy loss: 8.591121263390523
Experience 21, Iter 64, disc loss: 0.0014044619647802592, policy loss: 8.75320188719628
Experience 21, Iter 65, disc loss: 0.0015322005014900013, policy loss: 8.313946546511708
Experience 21, Iter 66, disc loss: 0.0013348278076898469, policy loss: 9.013626134949517
Experience 21, Iter 67, disc loss: 0.001495899583097556, policy loss: 7.995636107246341
Experience 21, Iter 68, disc loss: 0.0013493486083859395, policy loss: 8.690875570549203
Experience 21, Iter 69, disc loss: 0.0012152983953069045, policy loss: 8.40310828603224
Experience 21, Iter 70, disc loss: 0.0014099023989887754, policy loss: 8.695117910708575
Experience 21, Iter 71, disc loss: 0.0015208738567056652, policy loss: 8.270836044220825
Experience 21, Iter 72, disc loss: 0.0014004643677843604, policy loss: 8.652010258452664
Experience 21, Iter 73, disc loss: 0.0015690923145090965, policy loss: 9.148812151003906
Experience 21, Iter 74, disc loss: 0.0013449916322445833, policy loss: 9.046521651795999
Experience 21, Iter 75, disc loss: 0.0011935206565901243, policy loss: 9.025230925595213
Experience 21, Iter 76, disc loss: 0.0012675172253879055, policy loss: 8.765831777480301
Experience 21, Iter 77, disc loss: 0.0014086972638228251, policy loss: 8.72044897031491
Experience 21, Iter 78, disc loss: 0.001358913024461567, policy loss: 8.419035965420335
Experience 21, Iter 79, disc loss: 0.0015882060562016592, policy loss: 8.140489178287208
Experience 21, Iter 80, disc loss: 0.0013241400590084094, policy loss: 8.610341309540832
Experience 21, Iter 81, disc loss: 0.0014989689164505774, policy loss: 8.45102823327642
Experience 21, Iter 82, disc loss: 0.0012721320655014955, policy loss: 8.816193658283275
Experience 21, Iter 83, disc loss: 0.0011587010644111425, policy loss: 8.968898291834718
Experience 21, Iter 84, disc loss: 0.001297303790214616, policy loss: 9.017964541480202
Experience 21, Iter 85, disc loss: 0.001343027174950726, policy loss: 8.922417705829252
Experience 21, Iter 86, disc loss: 0.0014038607834491066, policy loss: 8.688886315676696
Experience 21, Iter 87, disc loss: 0.0014133533573237969, policy loss: 9.339340054680248
Experience 21, Iter 88, disc loss: 0.0013093984065047927, policy loss: 8.710023558565194
Experience 21, Iter 89, disc loss: 0.0016663114264709564, policy loss: 8.11833441408482
Experience 21, Iter 90, disc loss: 0.0014170677133655196, policy loss: 8.469762900968979
Experience 21, Iter 91, disc loss: 0.0012714420351743023, policy loss: 9.03000854606358
Experience 21, Iter 92, disc loss: 0.0013860684137057125, policy loss: 8.652096475124962
Experience 21, Iter 93, disc loss: 0.0012367525173870497, policy loss: 8.73365679410595
Experience 21, Iter 94, disc loss: 0.0013552799586671327, policy loss: 8.514027158895184
Experience 21, Iter 95, disc loss: 0.0013232319042454004, policy loss: 9.009617031737992
Experience 21, Iter 96, disc loss: 0.0012189305477362368, policy loss: 8.762972393682784
Experience 21, Iter 97, disc loss: 0.0014002033899921753, policy loss: 8.383976129039247
Experience 21, Iter 98, disc loss: 0.001560361248128784, policy loss: 8.156273702647324
Experience 21, Iter 99, disc loss: 0.001347103974312555, policy loss: 8.853705733778938
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2342],
        [1.9641],
        [0.0392]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0143, 0.1826, 1.8094, 0.0335, 0.0285, 5.8149]],

        [[0.0143, 0.1826, 1.8094, 0.0335, 0.0285, 5.8149]],

        [[0.0143, 0.1826, 1.8094, 0.0335, 0.0285, 5.8149]],

        [[0.0143, 0.1826, 1.8094, 0.0335, 0.0285, 5.8149]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0170, 0.9368, 7.8565, 0.1568], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0170, 0.9368, 7.8565, 0.1568])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.800
Iter 2/2000 - Loss: 3.774
Iter 3/2000 - Loss: 3.560
Iter 4/2000 - Loss: 3.496
Iter 5/2000 - Loss: 3.467
Iter 6/2000 - Loss: 3.346
Iter 7/2000 - Loss: 3.197
Iter 8/2000 - Loss: 3.065
Iter 9/2000 - Loss: 2.923
Iter 10/2000 - Loss: 2.743
Iter 11/2000 - Loss: 2.539
Iter 12/2000 - Loss: 2.337
Iter 13/2000 - Loss: 2.134
Iter 14/2000 - Loss: 1.911
Iter 15/2000 - Loss: 1.656
Iter 16/2000 - Loss: 1.377
Iter 17/2000 - Loss: 1.085
Iter 18/2000 - Loss: 0.791
Iter 19/2000 - Loss: 0.493
Iter 20/2000 - Loss: 0.190
Iter 1981/2000 - Loss: -7.430
Iter 1982/2000 - Loss: -7.430
Iter 1983/2000 - Loss: -7.430
Iter 1984/2000 - Loss: -7.430
Iter 1985/2000 - Loss: -7.430
Iter 1986/2000 - Loss: -7.430
Iter 1987/2000 - Loss: -7.430
Iter 1988/2000 - Loss: -7.430
Iter 1989/2000 - Loss: -7.430
Iter 1990/2000 - Loss: -7.430
Iter 1991/2000 - Loss: -7.430
Iter 1992/2000 - Loss: -7.430
Iter 1993/2000 - Loss: -7.430
Iter 1994/2000 - Loss: -7.430
Iter 1995/2000 - Loss: -7.430
Iter 1996/2000 - Loss: -7.430
Iter 1997/2000 - Loss: -7.430
Iter 1998/2000 - Loss: -7.430
Iter 1999/2000 - Loss: -7.431
Iter 2000/2000 - Loss: -7.431
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[11.8266,  5.9529, 23.9602,  1.7702, 17.0753, 37.7104]],

        [[16.7940, 34.4492,  7.3797,  1.1751,  1.7226, 33.7617]],

        [[16.8986, 33.6993,  8.6148,  0.8966,  0.7155, 26.5034]],

        [[14.0940, 26.7476, 12.7659,  1.5501,  1.8127, 48.1741]]])
Signal Variance: tensor([ 0.0615,  2.5057, 13.2595,  0.3626])
Estimated target variance: tensor([0.0170, 0.9368, 7.8565, 0.1568])
N: 220
Signal to noise ratio: tensor([14.2580, 80.3537, 73.4568, 31.8870])
Bound on condition number: tensor([  44724.7828, 1420477.3859, 1187099.9629,  223692.3636])
Policy Optimizer learning rate:
0.0009781172698749016
Experience 22, Iter 0, disc loss: 0.0013133646566942316, policy loss: 8.734229138853143
Experience 22, Iter 1, disc loss: 0.0015934869391890693, policy loss: 7.9289612784138
Experience 22, Iter 2, disc loss: 0.001359273445080159, policy loss: 8.401363637846718
Experience 22, Iter 3, disc loss: 0.0013103345492854988, policy loss: 8.695547661133544
Experience 22, Iter 4, disc loss: 0.0011526130426188655, policy loss: 8.900501558841354
Experience 22, Iter 5, disc loss: 0.0012899720886601121, policy loss: 8.551349166280769
Experience 22, Iter 6, disc loss: 0.0012119082720131735, policy loss: 8.85147194751768
Experience 22, Iter 7, disc loss: 0.0011447717686963246, policy loss: 9.054493905275017
Experience 22, Iter 8, disc loss: 0.0012577984813622839, policy loss: 8.471213135177614
Experience 22, Iter 9, disc loss: 0.0013447618860454587, policy loss: 8.765783902451451
Experience 22, Iter 10, disc loss: 0.0014821664186179897, policy loss: 8.57903044266962
Experience 22, Iter 11, disc loss: 0.0011683533061450724, policy loss: 8.750086287519869
Experience 22, Iter 12, disc loss: 0.0013941563131898934, policy loss: 8.340251415118383
Experience 22, Iter 13, disc loss: 0.0014238448648514787, policy loss: 8.407983611721873
Experience 22, Iter 14, disc loss: 0.0015616093489182001, policy loss: 8.23110760365961
Experience 22, Iter 15, disc loss: 0.0011501049954608421, policy loss: 8.908252841598117
Experience 22, Iter 16, disc loss: 0.00138227021378934, policy loss: 8.380802759158843
Experience 22, Iter 17, disc loss: 0.0012295418228965625, policy loss: 9.124571110325554
Experience 22, Iter 18, disc loss: 0.0011614556269687187, policy loss: 8.499456489259629
Experience 22, Iter 19, disc loss: 0.0012343860239857176, policy loss: 8.769776712896807
Experience 22, Iter 20, disc loss: 0.0011726597547630205, policy loss: 9.088316019125177
Experience 22, Iter 21, disc loss: 0.0017109697157471051, policy loss: 7.978371412978474
Experience 22, Iter 22, disc loss: 0.0012571740512939131, policy loss: 8.481496608945141
Experience 22, Iter 23, disc loss: 0.0012362699069867781, policy loss: 8.441875271783392
Experience 22, Iter 24, disc loss: 0.0017202538275213988, policy loss: 8.06304122370165
Experience 22, Iter 25, disc loss: 0.0013458127355741843, policy loss: 9.05101204557513
Experience 22, Iter 26, disc loss: 0.0013074578607081971, policy loss: 9.29332343182601
Experience 22, Iter 27, disc loss: 0.0014108484604642936, policy loss: 8.030744753993835
Experience 22, Iter 28, disc loss: 0.0012751132699686204, policy loss: 8.627088704913074
Experience 22, Iter 29, disc loss: 0.0014657692844554492, policy loss: 8.471493682562265
Experience 22, Iter 30, disc loss: 0.0014347037621965045, policy loss: 8.486874533485569
Experience 22, Iter 31, disc loss: 0.0013103025119979676, policy loss: 8.3571485152798
Experience 22, Iter 32, disc loss: 0.0012985859608939145, policy loss: 8.62381734416985
Experience 22, Iter 33, disc loss: 0.001320944779806507, policy loss: 8.346603368297682
Experience 22, Iter 34, disc loss: 0.0011909287929996776, policy loss: 8.67771029239839
Experience 22, Iter 35, disc loss: 0.001291515853406445, policy loss: 9.173488937726468
Experience 22, Iter 36, disc loss: 0.0013435415323581768, policy loss: 8.171594180708782
Experience 22, Iter 37, disc loss: 0.0013462265873921179, policy loss: 8.255123943307916
Experience 22, Iter 38, disc loss: 0.0014678476132616769, policy loss: 8.341150785480412
Experience 22, Iter 39, disc loss: 0.001460652360407118, policy loss: 8.321057071782967
Experience 22, Iter 40, disc loss: 0.0012766406670362117, policy loss: 8.703030236760691
Experience 22, Iter 41, disc loss: 0.0011286753088033375, policy loss: 9.178362587930383
Experience 22, Iter 42, disc loss: 0.0012263215363459172, policy loss: 8.752629855599071
Experience 22, Iter 43, disc loss: 0.0012583090350288218, policy loss: 8.632956118864977
Experience 22, Iter 44, disc loss: 0.0015771846306393584, policy loss: 8.102426581194923
Experience 22, Iter 45, disc loss: 0.0011182775881833761, policy loss: 8.74796262472503
Experience 22, Iter 46, disc loss: 0.0013330408863989627, policy loss: 8.537897456024758
Experience 22, Iter 47, disc loss: 0.001349327209325348, policy loss: 7.853682245342327
Experience 22, Iter 48, disc loss: 0.0012747252054944016, policy loss: 8.197826243774662
Experience 22, Iter 49, disc loss: 0.0011765328834769935, policy loss: 9.044106743341546
Experience 22, Iter 50, disc loss: 0.0013839012380594661, policy loss: 8.977129896934215
Experience 22, Iter 51, disc loss: 0.0014813802937745516, policy loss: 8.203902702519887
Experience 22, Iter 52, disc loss: 0.001167328681679305, policy loss: 8.823957495031443
Experience 22, Iter 53, disc loss: 0.0013090690551148302, policy loss: 8.830055152846931
Experience 22, Iter 54, disc loss: 0.0014776920406006997, policy loss: 8.18307304443664
Experience 22, Iter 55, disc loss: 0.0012802981696322412, policy loss: 8.766618681695856
Experience 22, Iter 56, disc loss: 0.001299851659501224, policy loss: 8.51649506164202
Experience 22, Iter 57, disc loss: 0.0012638688145197128, policy loss: 8.372238737996604
Experience 22, Iter 58, disc loss: 0.0010040001685062629, policy loss: 9.2649748982749
Experience 22, Iter 59, disc loss: 0.0013020628902017232, policy loss: 9.142451334827792
Experience 22, Iter 60, disc loss: 0.0013388224121579514, policy loss: 8.337717636779626
Experience 22, Iter 61, disc loss: 0.0011190485246645129, policy loss: 8.585673117252899
Experience 22, Iter 62, disc loss: 0.0012803739792604145, policy loss: 8.638660970290008
Experience 22, Iter 63, disc loss: 0.001307598920716655, policy loss: 9.13320690394132
Experience 22, Iter 64, disc loss: 0.00125620847066592, policy loss: 8.559037607224802
Experience 22, Iter 65, disc loss: 0.0011620390071649957, policy loss: 8.728477394146985
Experience 22, Iter 66, disc loss: 0.0011414412115135057, policy loss: 8.679414234748922
Experience 22, Iter 67, disc loss: 0.0014437601880272202, policy loss: 8.14633454931673
Experience 22, Iter 68, disc loss: 0.0011328786999335707, policy loss: 9.197382039735622
Experience 22, Iter 69, disc loss: 0.0010880668792720744, policy loss: 8.76216771635393
Experience 22, Iter 70, disc loss: 0.0012318380730798927, policy loss: 8.903189138822217
Experience 22, Iter 71, disc loss: 0.001210988827236586, policy loss: 8.493494690246948
Experience 22, Iter 72, disc loss: 0.001414859650696299, policy loss: 8.658940439553609
Experience 22, Iter 73, disc loss: 0.0013140563343530297, policy loss: 8.657518737778037
Experience 22, Iter 74, disc loss: 0.0014273758049289456, policy loss: 8.789596584353175
Experience 22, Iter 75, disc loss: 0.0013239577589721323, policy loss: 8.741196344855076
Experience 22, Iter 76, disc loss: 0.0012886453181777749, policy loss: 8.736511748027144
Experience 22, Iter 77, disc loss: 0.0014823065643410442, policy loss: 8.793646456724922
Experience 22, Iter 78, disc loss: 0.0011472226926459964, policy loss: 8.831941682926175
Experience 22, Iter 79, disc loss: 0.0013004840917562159, policy loss: 8.987058705692425
Experience 22, Iter 80, disc loss: 0.0013569240755015029, policy loss: 8.61519146635975
Experience 22, Iter 81, disc loss: 0.0010818210784848343, policy loss: 8.807408850850974
Experience 22, Iter 82, disc loss: 0.0012280449888754687, policy loss: 8.353388604917273
Experience 22, Iter 83, disc loss: 0.0013090562703578336, policy loss: 9.044555857667769
Experience 22, Iter 84, disc loss: 0.0011058277043735167, policy loss: 9.153814668588492
Experience 22, Iter 85, disc loss: 0.0009538829865815383, policy loss: 8.677725553001927
Experience 22, Iter 86, disc loss: 0.0010461208953756504, policy loss: 9.347909732136513
Experience 22, Iter 87, disc loss: 0.001101269367462587, policy loss: 9.028284149018624
Experience 22, Iter 88, disc loss: 0.0012498918998824437, policy loss: 9.033028706221883
Experience 22, Iter 89, disc loss: 0.0011939626705990498, policy loss: 8.656686764492022
Experience 22, Iter 90, disc loss: 0.001292858591298449, policy loss: 8.697284882804048
Experience 22, Iter 91, disc loss: 0.0010788778704190333, policy loss: 8.937183277307492
Experience 22, Iter 92, disc loss: 0.0012511921459730357, policy loss: 9.033308221519174
Experience 22, Iter 93, disc loss: 0.001068463515644501, policy loss: 8.984077071405942
Experience 22, Iter 94, disc loss: 0.0010611219669671764, policy loss: 9.253237217406188
Experience 22, Iter 95, disc loss: 0.0009154663239973443, policy loss: 9.433286518166977
Experience 22, Iter 96, disc loss: 0.0009897476130621711, policy loss: 10.025184370569338
Experience 22, Iter 97, disc loss: 0.001303171593407322, policy loss: 8.697852945721031
Experience 22, Iter 98, disc loss: 0.0012206092148743341, policy loss: 8.522643709497327
Experience 22, Iter 99, disc loss: 0.0012148092075321966, policy loss: 8.677744893674465
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2374],
        [1.9804],
        [0.0393]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0139, 0.1817, 1.8192, 0.0335, 0.0287, 5.8824]],

        [[0.0139, 0.1817, 1.8192, 0.0335, 0.0287, 5.8824]],

        [[0.0139, 0.1817, 1.8192, 0.0335, 0.0287, 5.8824]],

        [[0.0139, 0.1817, 1.8192, 0.0335, 0.0287, 5.8824]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0168, 0.9497, 7.9217, 0.1570], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0168, 0.9497, 7.9217, 0.1570])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.775
Iter 2/2000 - Loss: 3.734
Iter 3/2000 - Loss: 3.507
Iter 4/2000 - Loss: 3.439
Iter 5/2000 - Loss: 3.408
Iter 6/2000 - Loss: 3.279
Iter 7/2000 - Loss: 3.117
Iter 8/2000 - Loss: 2.974
Iter 9/2000 - Loss: 2.826
Iter 10/2000 - Loss: 2.641
Iter 11/2000 - Loss: 2.433
Iter 12/2000 - Loss: 2.223
Iter 13/2000 - Loss: 2.012
Iter 14/2000 - Loss: 1.784
Iter 15/2000 - Loss: 1.526
Iter 16/2000 - Loss: 1.243
Iter 17/2000 - Loss: 0.950
Iter 18/2000 - Loss: 0.654
Iter 19/2000 - Loss: 0.356
Iter 20/2000 - Loss: 0.054
Iter 1981/2000 - Loss: -7.527
Iter 1982/2000 - Loss: -7.527
Iter 1983/2000 - Loss: -7.527
Iter 1984/2000 - Loss: -7.527
Iter 1985/2000 - Loss: -7.527
Iter 1986/2000 - Loss: -7.527
Iter 1987/2000 - Loss: -7.527
Iter 1988/2000 - Loss: -7.527
Iter 1989/2000 - Loss: -7.527
Iter 1990/2000 - Loss: -7.527
Iter 1991/2000 - Loss: -7.527
Iter 1992/2000 - Loss: -7.527
Iter 1993/2000 - Loss: -7.527
Iter 1994/2000 - Loss: -7.527
Iter 1995/2000 - Loss: -7.527
Iter 1996/2000 - Loss: -7.527
Iter 1997/2000 - Loss: -7.527
Iter 1998/2000 - Loss: -7.527
Iter 1999/2000 - Loss: -7.527
Iter 2000/2000 - Loss: -7.527
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0004]])
Lengthscale: tensor([[[11.5912,  5.9253, 24.1623,  1.7679, 17.0586, 37.1241]],

        [[16.7537, 34.2117,  7.2865,  1.1773,  1.7308, 33.7926]],

        [[16.7192, 33.3187,  8.6824,  0.8969,  0.7061, 26.4056]],

        [[13.9002, 26.4321, 11.7677,  1.5201,  1.8198, 47.1083]]])
Signal Variance: tensor([ 0.0608,  2.5302, 13.2521,  0.3366])
Estimated target variance: tensor([0.0168, 0.9497, 7.9217, 0.1570])
N: 230
Signal to noise ratio: tensor([14.3847, 82.0920, 74.9808, 30.5208])
Bound on condition number: tensor([  47592.1827, 1549994.2481, 1293087.6005,  214250.3658])
Policy Optimizer learning rate:
0.000977087263181088
Experience 23, Iter 0, disc loss: 0.0012400545392187142, policy loss: 9.344730259209445
Experience 23, Iter 1, disc loss: 0.0013203117753219692, policy loss: 8.330265052024764
Experience 23, Iter 2, disc loss: 0.0011848123837373157, policy loss: 8.343473178994476
Experience 23, Iter 3, disc loss: 0.0011536246053509632, policy loss: 8.39756854960564
Experience 23, Iter 4, disc loss: 0.001205012933480412, policy loss: 8.76782491114588
Experience 23, Iter 5, disc loss: 0.0010839373644745203, policy loss: 8.893803677628814
Experience 23, Iter 6, disc loss: 0.0010517137496616773, policy loss: 9.118032369618925
Experience 23, Iter 7, disc loss: 0.0011428541735853566, policy loss: 9.063292940688944
Experience 23, Iter 8, disc loss: 0.001207603111648537, policy loss: 8.530335187081207
Experience 23, Iter 9, disc loss: 0.0010618335436167828, policy loss: 9.061712729922005
Experience 23, Iter 10, disc loss: 0.0012189645617159796, policy loss: 8.349681049503324
Experience 23, Iter 11, disc loss: 0.0013104741248800966, policy loss: 8.394183000452932
Experience 23, Iter 12, disc loss: 0.001300821825990851, policy loss: 8.715748442012362
Experience 23, Iter 13, disc loss: 0.0010231654634228788, policy loss: 9.48013050492158
Experience 23, Iter 14, disc loss: 0.0012389150046234931, policy loss: 8.425066972092585
Experience 23, Iter 15, disc loss: 0.0013179111634619729, policy loss: 8.634394330167966
Experience 23, Iter 16, disc loss: 0.001137058440280737, policy loss: 8.713793712236257
Experience 23, Iter 17, disc loss: 0.0011797537864256876, policy loss: 9.287063048005985
Experience 23, Iter 18, disc loss: 0.0010647679552093356, policy loss: 9.136333716084255
Experience 23, Iter 19, disc loss: 0.0012468153313857776, policy loss: 8.247755170385034
Experience 23, Iter 20, disc loss: 0.0013277617905637234, policy loss: 8.74710394292524
Experience 23, Iter 21, disc loss: 0.0011851244950707883, policy loss: 8.531002214836004
Experience 23, Iter 22, disc loss: 0.001166022229137673, policy loss: 8.635349488420232
Experience 23, Iter 23, disc loss: 0.0012643771825661602, policy loss: 9.048011268525407
Experience 23, Iter 24, disc loss: 0.0012586052981734277, policy loss: 8.634158733759744
Experience 23, Iter 25, disc loss: 0.0010827468163906906, policy loss: 9.396710278923258
Experience 23, Iter 26, disc loss: 0.0010844338452151822, policy loss: 9.092688773908552
Experience 23, Iter 27, disc loss: 0.0011683495027699307, policy loss: 9.175594540984704
Experience 23, Iter 28, disc loss: 0.0011869821097728957, policy loss: 9.239007494573265
Experience 23, Iter 29, disc loss: 0.0009569989663788876, policy loss: 9.000951858228188
Experience 23, Iter 30, disc loss: 0.0011394510295825947, policy loss: 8.408590400040627
Experience 23, Iter 31, disc loss: 0.0011939324653008278, policy loss: 8.716210788486672
Experience 23, Iter 32, disc loss: 0.0010879297910889683, policy loss: 8.593036016268083
Experience 23, Iter 33, disc loss: 0.0014682937521239506, policy loss: 8.695090720229375
Experience 23, Iter 34, disc loss: 0.0012818573697407935, policy loss: 9.055548151536996
Experience 23, Iter 35, disc loss: 0.00116709863742852, policy loss: 8.762395237628109
Experience 23, Iter 36, disc loss: 0.001289836300195502, policy loss: 8.524760310922257
Experience 23, Iter 37, disc loss: 0.0011824055757380068, policy loss: 8.752485614574304
Experience 23, Iter 38, disc loss: 0.001100983018509105, policy loss: 9.00885422079966
Experience 23, Iter 39, disc loss: 0.0012164774936770503, policy loss: 8.46016880457851
Experience 23, Iter 40, disc loss: 0.0011424157299215387, policy loss: 9.161965722855847
Experience 23, Iter 41, disc loss: 0.0011278882135466678, policy loss: 8.520453467250196
Experience 23, Iter 42, disc loss: 0.0010446441563674964, policy loss: 9.146032204980536
Experience 23, Iter 43, disc loss: 0.001116763850851468, policy loss: 9.17914388121349
Experience 23, Iter 44, disc loss: 0.0013075484274335588, policy loss: 8.568657593116692
Experience 23, Iter 45, disc loss: 0.0014356776142772248, policy loss: 8.23526998090166
Experience 23, Iter 46, disc loss: 0.0011123236524593562, policy loss: 8.86580686969936
Experience 23, Iter 47, disc loss: 0.0010907836373124538, policy loss: 9.181168847900015
Experience 23, Iter 48, disc loss: 0.0010739584621018415, policy loss: 8.304967210754569
Experience 23, Iter 49, disc loss: 0.0011286221755159196, policy loss: 8.860761089947953
Experience 23, Iter 50, disc loss: 0.0011918369541973348, policy loss: 8.375123660299428
Experience 23, Iter 51, disc loss: 0.0010734453149931333, policy loss: 9.578501663820402
Experience 23, Iter 52, disc loss: 0.0011538971644771763, policy loss: 9.483329417918867
Experience 23, Iter 53, disc loss: 0.0010938375793671808, policy loss: 8.51258179959842
Experience 23, Iter 54, disc loss: 0.0011291843536711701, policy loss: 8.52826283359154
Experience 23, Iter 55, disc loss: 0.0011150023119230429, policy loss: 8.603350860808424
Experience 23, Iter 56, disc loss: 0.0010784334714754344, policy loss: 8.70443013008115
Experience 23, Iter 57, disc loss: 0.001077608997622773, policy loss: 8.983036960197119
Experience 23, Iter 58, disc loss: 0.0010281998511969352, policy loss: 8.723595005440508
Experience 23, Iter 59, disc loss: 0.0011332058690223753, policy loss: 8.722697282311973
Experience 23, Iter 60, disc loss: 0.0010000582361881175, policy loss: 9.012283577773054
Experience 23, Iter 61, disc loss: 0.0010958296114726017, policy loss: 8.845970931383835
Experience 23, Iter 62, disc loss: 0.0010647820099881134, policy loss: 9.24205889432012
Experience 23, Iter 63, disc loss: 0.0011080025297803505, policy loss: 9.467733745265306
Experience 23, Iter 64, disc loss: 0.0010947602629473503, policy loss: 9.070428596761936
Experience 23, Iter 65, disc loss: 0.0011975218911546557, policy loss: 8.679111844385007
Experience 23, Iter 66, disc loss: 0.0010364864112156376, policy loss: 8.842084419271014
Experience 23, Iter 67, disc loss: 0.0012269335115384034, policy loss: 8.26913349870016
Experience 23, Iter 68, disc loss: 0.001197268919344046, policy loss: 8.671351145634866
Experience 23, Iter 69, disc loss: 0.0011397153296400387, policy loss: 8.518042517890002
Experience 23, Iter 70, disc loss: 0.0012880265712919255, policy loss: 8.70318073919189
Experience 23, Iter 71, disc loss: 0.0012093090359654585, policy loss: 8.536270705114106
Experience 23, Iter 72, disc loss: 0.00117436741547446, policy loss: 8.338515472017535
Experience 23, Iter 73, disc loss: 0.0010684020368925476, policy loss: 9.023982627613872
Experience 23, Iter 74, disc loss: 0.001025735457172514, policy loss: 8.657294845025062
Experience 23, Iter 75, disc loss: 0.0010817585085475174, policy loss: 8.921296201804388
Experience 23, Iter 76, disc loss: 0.0009415811979929969, policy loss: 9.310030157564864
Experience 23, Iter 77, disc loss: 0.0010689842052708487, policy loss: 8.760605636324522
Experience 23, Iter 78, disc loss: 0.0011738753336654588, policy loss: 8.65383980212903
Experience 23, Iter 79, disc loss: 0.001094781942811666, policy loss: 9.065606367198583
Experience 23, Iter 80, disc loss: 0.0011516917709764821, policy loss: 9.0918626485598
Experience 23, Iter 81, disc loss: 0.0011392685302254362, policy loss: 8.60477866584766
Experience 23, Iter 82, disc loss: 0.0013206671729580957, policy loss: 8.490095696749222
Experience 23, Iter 83, disc loss: 0.001030836886169617, policy loss: 9.470993643484363
Experience 23, Iter 84, disc loss: 0.000996796127237782, policy loss: 8.568436074733503
Experience 23, Iter 85, disc loss: 0.0010837712767186227, policy loss: 9.371824631598578
Experience 23, Iter 86, disc loss: 0.0009627621123165431, policy loss: 9.064624388715881
Experience 23, Iter 87, disc loss: 0.001291196640693252, policy loss: 8.992327974033572
Experience 23, Iter 88, disc loss: 0.0010009424563607798, policy loss: 8.666579693549139
Experience 23, Iter 89, disc loss: 0.0010772979887806499, policy loss: 8.734931892262882
Experience 23, Iter 90, disc loss: 0.0011710179833033867, policy loss: 8.456458747724994
Experience 23, Iter 91, disc loss: 0.001085215590200455, policy loss: 8.851268166394691
Experience 23, Iter 92, disc loss: 0.001316959620907696, policy loss: 8.81234611823739
Experience 23, Iter 93, disc loss: 0.0010042076696763454, policy loss: 8.874734475204729
Experience 23, Iter 94, disc loss: 0.001041770744911165, policy loss: 8.717006845868877
Experience 23, Iter 95, disc loss: 0.0011217112206098048, policy loss: 8.47064276939687
Experience 23, Iter 96, disc loss: 0.0011131307758092805, policy loss: 8.644332141780598
Experience 23, Iter 97, disc loss: 0.0009396137857055286, policy loss: 9.148482317098965
Experience 23, Iter 98, disc loss: 0.0010603816805095016, policy loss: 8.700073781056556
Experience 23, Iter 99, disc loss: 0.0012076929955437059, policy loss: 8.503789100081386
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.2389],
        [1.9861],
        [0.0391]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0134, 0.1801, 1.8174, 0.0335, 0.0289, 5.9419]],

        [[0.0134, 0.1801, 1.8174, 0.0335, 0.0289, 5.9419]],

        [[0.0134, 0.1801, 1.8174, 0.0335, 0.0289, 5.9419]],

        [[0.0134, 0.1801, 1.8174, 0.0335, 0.0289, 5.9419]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0166, 0.9555, 7.9442, 0.1566], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0166, 0.9555, 7.9442, 0.1566])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.767
Iter 2/2000 - Loss: 3.734
Iter 3/2000 - Loss: 3.505
Iter 4/2000 - Loss: 3.427
Iter 5/2000 - Loss: 3.383
Iter 6/2000 - Loss: 3.248
Iter 7/2000 - Loss: 3.083
Iter 8/2000 - Loss: 2.936
Iter 9/2000 - Loss: 2.780
Iter 10/2000 - Loss: 2.584
Iter 11/2000 - Loss: 2.366
Iter 12/2000 - Loss: 2.150
Iter 13/2000 - Loss: 1.935
Iter 14/2000 - Loss: 1.704
Iter 15/2000 - Loss: 1.444
Iter 16/2000 - Loss: 1.160
Iter 17/2000 - Loss: 0.865
Iter 18/2000 - Loss: 0.568
Iter 19/2000 - Loss: 0.270
Iter 20/2000 - Loss: -0.032
Iter 1981/2000 - Loss: -7.609
Iter 1982/2000 - Loss: -7.609
Iter 1983/2000 - Loss: -7.609
Iter 1984/2000 - Loss: -7.609
Iter 1985/2000 - Loss: -7.610
Iter 1986/2000 - Loss: -7.610
Iter 1987/2000 - Loss: -7.610
Iter 1988/2000 - Loss: -7.610
Iter 1989/2000 - Loss: -7.610
Iter 1990/2000 - Loss: -7.610
Iter 1991/2000 - Loss: -7.610
Iter 1992/2000 - Loss: -7.610
Iter 1993/2000 - Loss: -7.610
Iter 1994/2000 - Loss: -7.610
Iter 1995/2000 - Loss: -7.610
Iter 1996/2000 - Loss: -7.610
Iter 1997/2000 - Loss: -7.610
Iter 1998/2000 - Loss: -7.610
Iter 1999/2000 - Loss: -7.610
Iter 2000/2000 - Loss: -7.610
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.4544,  5.9604, 23.1346,  1.7505, 16.9369, 38.3088]],

        [[16.5766, 33.8499,  7.2451,  1.1891,  1.7512, 34.1433]],

        [[16.3273, 32.8463,  8.7685,  0.8886,  0.7014, 25.8552]],

        [[13.5807, 25.8538, 11.4041,  1.5075,  1.8128, 47.3937]]])
Signal Variance: tensor([ 0.0599,  2.6400, 13.1648,  0.3254])
Estimated target variance: tensor([0.0166, 0.9555, 7.9442, 0.1566])
N: 240
Signal to noise ratio: tensor([14.3570, 82.6974, 76.3250, 30.5241])
Bound on condition number: tensor([  49470.8966, 1641327.0846, 1398121.8328,  223613.2978])
Policy Optimizer learning rate:
0.000976058341136142
Experience 24, Iter 0, disc loss: 0.0010855507835160843, policy loss: 8.84237730010211
Experience 24, Iter 1, disc loss: 0.0009516125716910593, policy loss: 9.753259382623055
Experience 24, Iter 2, disc loss: 0.0010622174650672805, policy loss: 8.925897005502728
Experience 24, Iter 3, disc loss: 0.001050418998451898, policy loss: 8.544063868842537
Experience 24, Iter 4, disc loss: 0.0009081528577183493, policy loss: 9.160670191451153
Experience 24, Iter 5, disc loss: 0.0010182975385099957, policy loss: 8.850660686247796
Experience 24, Iter 6, disc loss: 0.0010497471346706112, policy loss: 8.747915305702847
Experience 24, Iter 7, disc loss: 0.0010176950279977988, policy loss: 8.960972121928942
Experience 24, Iter 8, disc loss: 0.0009565213551298085, policy loss: 8.931972398586284
Experience 24, Iter 9, disc loss: 0.0010755594665589675, policy loss: 8.578702075585724
Experience 24, Iter 10, disc loss: 0.0009723965323055143, policy loss: 9.335169056250697
Experience 24, Iter 11, disc loss: 0.0012864045038739535, policy loss: 8.242494498378658
Experience 24, Iter 12, disc loss: 0.0011629727211389977, policy loss: 8.559613767208639
Experience 24, Iter 13, disc loss: 0.001077244176169961, policy loss: 8.903981991741155
Experience 24, Iter 14, disc loss: 0.0008924517183051378, policy loss: 9.060523934411744
Experience 24, Iter 15, disc loss: 0.000990073213789713, policy loss: 8.907261474729383
Experience 24, Iter 16, disc loss: 0.0010775060091734586, policy loss: 9.021061385173361
Experience 24, Iter 17, disc loss: 0.001127584477671863, policy loss: 8.408281830542009
Experience 24, Iter 18, disc loss: 0.0009653167168463481, policy loss: 9.924944245765179
Experience 24, Iter 19, disc loss: 0.0008230045105922258, policy loss: 9.294200804850131
Experience 24, Iter 20, disc loss: 0.000642578497001495, policy loss: 10.220517809271005
Experience 24, Iter 21, disc loss: 0.0005969885818208204, policy loss: 10.819578450347661
Experience 24, Iter 22, disc loss: 0.0005829076547384866, policy loss: 11.223279169250564
Experience 24, Iter 23, disc loss: 0.0005471357305140865, policy loss: 12.40645391688819
Experience 24, Iter 24, disc loss: 0.0005502192033946562, policy loss: 11.23930387884403
Experience 24, Iter 25, disc loss: 0.0005776261695730766, policy loss: 10.755749830757276
Experience 24, Iter 26, disc loss: 0.000599371378781034, policy loss: 10.914545620833882
Experience 24, Iter 27, disc loss: 0.000687114176407352, policy loss: 9.873982197266312
Experience 24, Iter 28, disc loss: 0.0007732269310462551, policy loss: 9.174902660804815
Experience 24, Iter 29, disc loss: 0.0008182226680913045, policy loss: 9.644792334608935
Experience 24, Iter 30, disc loss: 0.0005391034793146267, policy loss: 13.395286581779153
Experience 24, Iter 31, disc loss: 0.0006618228813769298, policy loss: 9.787469448295584
Experience 24, Iter 32, disc loss: 0.0004770862824747043, policy loss: 11.869762800113442
Experience 24, Iter 33, disc loss: 0.00045696576236949505, policy loss: 14.137662731990286
Experience 24, Iter 34, disc loss: 0.0004228964316265734, policy loss: 14.696925052407765
Experience 24, Iter 35, disc loss: 0.0004143375419866983, policy loss: 13.276568544275635
Experience 24, Iter 36, disc loss: 0.000398690756365898, policy loss: 14.164027419271708
Experience 24, Iter 37, disc loss: 0.00038991935274603547, policy loss: 13.482155832909854
Experience 24, Iter 38, disc loss: 0.00037848036348811975, policy loss: 13.919934612985745
Experience 24, Iter 39, disc loss: 0.0003686547620297665, policy loss: 13.979393583614764
Experience 24, Iter 40, disc loss: 0.0003595894044416911, policy loss: 14.39977424026472
Experience 24, Iter 41, disc loss: 0.00035058585917876773, policy loss: 14.310847076559313
Experience 24, Iter 42, disc loss: 0.0003413719917962091, policy loss: 14.839573329251383
Experience 24, Iter 43, disc loss: 0.00033370413083740226, policy loss: 14.615051593542264
Experience 24, Iter 44, disc loss: 0.00032576683964923963, policy loss: 14.264237886742617
Experience 24, Iter 45, disc loss: 0.00031744982994474024, policy loss: 14.719069183713165
Experience 24, Iter 46, disc loss: 0.0003115100154754292, policy loss: 13.709032143922457
Experience 24, Iter 47, disc loss: 0.0003052294567505878, policy loss: 13.892783745780706
Experience 24, Iter 48, disc loss: 0.0002992825012367852, policy loss: 13.225656154521303
Experience 24, Iter 49, disc loss: 0.0002940177756691686, policy loss: 13.29510677050636
Experience 24, Iter 50, disc loss: 0.0002888264109492313, policy loss: 12.997813414676653
Experience 24, Iter 51, disc loss: 0.0002839693979788229, policy loss: 12.77439407421764
Experience 24, Iter 52, disc loss: 0.0002740167058269918, policy loss: 12.899913098956898
Experience 24, Iter 53, disc loss: 0.0002764993984526165, policy loss: 12.346810919667329
Experience 24, Iter 54, disc loss: 0.0002714552889370741, policy loss: 12.180890312812116
Experience 24, Iter 55, disc loss: 0.00027538511466961073, policy loss: 11.786686979426971
Experience 24, Iter 56, disc loss: 0.00026417226201700777, policy loss: 12.189042945168728
Experience 24, Iter 57, disc loss: 0.00026275547488602757, policy loss: 11.363738221509024
Experience 24, Iter 58, disc loss: 0.00026477267105466787, policy loss: 11.077140550004131
Experience 24, Iter 59, disc loss: 0.0002742714924946077, policy loss: 10.815982908957249
Experience 24, Iter 60, disc loss: 0.00028908116775428665, policy loss: 10.8407681712244
Experience 24, Iter 61, disc loss: 0.00030386790913389873, policy loss: 10.330606368376959
Experience 24, Iter 62, disc loss: 0.00033829767715860836, policy loss: 9.992621512976758
Experience 24, Iter 63, disc loss: 0.00037362477931378367, policy loss: 9.672046768383982
Experience 24, Iter 64, disc loss: 0.0004206908425385763, policy loss: 9.393030543858632
Experience 24, Iter 65, disc loss: 0.000410108966080463, policy loss: 9.827093235751498
Experience 24, Iter 66, disc loss: 0.0005480666181600585, policy loss: 9.272554449842108
Experience 24, Iter 67, disc loss: 0.00048745589462999327, policy loss: 9.25409827484053
Experience 24, Iter 68, disc loss: 0.0008084540810762611, policy loss: 8.766658740771069
Experience 24, Iter 69, disc loss: 0.0008438759077398111, policy loss: 8.641354822082507
Experience 24, Iter 70, disc loss: 0.0009712371128734947, policy loss: 9.034734714460408
Experience 24, Iter 71, disc loss: 0.0008017073850961719, policy loss: 8.94275271485401
Experience 24, Iter 72, disc loss: 0.0009698167825982847, policy loss: 8.515018843769601
Experience 24, Iter 73, disc loss: 0.000770522955821289, policy loss: 8.877635415485003
Experience 24, Iter 74, disc loss: 0.0008703827754694552, policy loss: 8.727240711982498
Experience 24, Iter 75, disc loss: 0.0008621676112236833, policy loss: 8.616181834779194
Experience 24, Iter 76, disc loss: 0.000997829259508062, policy loss: 8.528017038167702
Experience 24, Iter 77, disc loss: 0.0010306833163270263, policy loss: 8.961065881667341
Experience 24, Iter 78, disc loss: 0.0009540979602290089, policy loss: 8.467927312986603
Experience 24, Iter 79, disc loss: 0.0007886908257454862, policy loss: 8.802803702095666
Experience 24, Iter 80, disc loss: 0.0010018684537879386, policy loss: 8.5744832447269
Experience 24, Iter 81, disc loss: 0.001232250920836897, policy loss: 8.146017171791012
Experience 24, Iter 82, disc loss: 0.0006415593240028822, policy loss: 8.884474827657694
Experience 24, Iter 83, disc loss: 0.0007221646102000632, policy loss: 8.81782442196121
Experience 24, Iter 84, disc loss: 0.0006952087140427814, policy loss: 9.21237755050473
Experience 24, Iter 85, disc loss: 0.000738776852365571, policy loss: 8.64344560046128
Experience 24, Iter 86, disc loss: 0.0009854839421065441, policy loss: 8.206887794984064
Experience 24, Iter 87, disc loss: 0.0007630002737714792, policy loss: 8.935944863628716
Experience 24, Iter 88, disc loss: 0.0009360353402648634, policy loss: 8.528076012053646
Experience 24, Iter 89, disc loss: 0.0009865316430053496, policy loss: 8.53215301426786
Experience 24, Iter 90, disc loss: 0.0009593931263106304, policy loss: 9.563077046644016
Experience 24, Iter 91, disc loss: 0.0008732171696709831, policy loss: 9.787654131737742
Experience 24, Iter 92, disc loss: 0.0007325211847458631, policy loss: 9.15485491865716
Experience 24, Iter 93, disc loss: 0.0007488224819817357, policy loss: 9.050408823113447
Experience 24, Iter 94, disc loss: 0.0007141544387247956, policy loss: 9.364164802068407
Experience 24, Iter 95, disc loss: 0.0005806239147818841, policy loss: 9.744499022904726
Experience 24, Iter 96, disc loss: 0.0005985680525794586, policy loss: 9.509333442346335
Experience 24, Iter 97, disc loss: 0.0005716027782495914, policy loss: 9.526266470172263
Experience 24, Iter 98, disc loss: 0.0005366595048271871, policy loss: 9.612303757905922
Experience 24, Iter 99, disc loss: 0.0005704318091447934, policy loss: 9.665053241678528
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.2414],
        [1.9938],
        [0.0391]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0130, 0.1790, 1.8210, 0.0336, 0.0291, 6.0162]],

        [[0.0130, 0.1790, 1.8210, 0.0336, 0.0291, 6.0162]],

        [[0.0130, 0.1790, 1.8210, 0.0336, 0.0291, 6.0162]],

        [[0.0130, 0.1790, 1.8210, 0.0336, 0.0291, 6.0162]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0165, 0.9657, 7.9754, 0.1563], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0165, 0.9657, 7.9754, 0.1563])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.732
Iter 2/2000 - Loss: 3.686
Iter 3/2000 - Loss: 3.444
Iter 4/2000 - Loss: 3.366
Iter 5/2000 - Loss: 3.323
Iter 6/2000 - Loss: 3.182
Iter 7/2000 - Loss: 3.007
Iter 8/2000 - Loss: 2.850
Iter 9/2000 - Loss: 2.690
Iter 10/2000 - Loss: 2.494
Iter 11/2000 - Loss: 2.273
Iter 12/2000 - Loss: 2.052
Iter 13/2000 - Loss: 1.833
Iter 14/2000 - Loss: 1.599
Iter 15/2000 - Loss: 1.338
Iter 16/2000 - Loss: 1.054
Iter 17/2000 - Loss: 0.758
Iter 18/2000 - Loss: 0.461
Iter 19/2000 - Loss: 0.163
Iter 20/2000 - Loss: -0.137
Iter 1981/2000 - Loss: -7.701
Iter 1982/2000 - Loss: -7.701
Iter 1983/2000 - Loss: -7.701
Iter 1984/2000 - Loss: -7.701
Iter 1985/2000 - Loss: -7.701
Iter 1986/2000 - Loss: -7.701
Iter 1987/2000 - Loss: -7.701
Iter 1988/2000 - Loss: -7.701
Iter 1989/2000 - Loss: -7.701
Iter 1990/2000 - Loss: -7.701
Iter 1991/2000 - Loss: -7.701
Iter 1992/2000 - Loss: -7.701
Iter 1993/2000 - Loss: -7.701
Iter 1994/2000 - Loss: -7.701
Iter 1995/2000 - Loss: -7.701
Iter 1996/2000 - Loss: -7.701
Iter 1997/2000 - Loss: -7.701
Iter 1998/2000 - Loss: -7.701
Iter 1999/2000 - Loss: -7.701
Iter 2000/2000 - Loss: -7.701
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[11.1840,  5.8646, 23.3321,  1.7989, 17.1825, 36.5469]],

        [[16.5574, 33.4209,  7.2447,  1.1941,  1.7568, 34.8594]],

        [[16.0588, 31.5693,  8.7924,  0.8892,  0.6981, 25.8165]],

        [[13.4136, 25.4984, 11.4888,  1.5268,  1.7828, 47.6894]]])
Signal Variance: tensor([ 0.0593,  2.7035, 13.2772,  0.3281])
Estimated target variance: tensor([0.0165, 0.9657, 7.9754, 0.1563])
N: 250
Signal to noise ratio: tensor([14.3957, 84.2762, 77.6130, 31.0696])
Bound on condition number: tensor([  51810.2815, 1775618.7827, 1505945.9275,  241330.4601])
Policy Optimizer learning rate:
0.000975030502597874
Experience 25, Iter 0, disc loss: 0.0006437755818375236, policy loss: 9.744383965014146
Experience 25, Iter 1, disc loss: 0.0009577961521875034, policy loss: 8.541603846493906
Experience 25, Iter 2, disc loss: 0.0008192461796071024, policy loss: 9.027363743369358
Experience 25, Iter 3, disc loss: 0.0008030329512742327, policy loss: 9.357998809599238
Experience 25, Iter 4, disc loss: 0.0007294178163422165, policy loss: 9.67373937129479
Experience 25, Iter 5, disc loss: 0.0006034168783704402, policy loss: 9.31929838453147
Experience 25, Iter 6, disc loss: 0.0005197160151636866, policy loss: 10.689847844591439
Experience 25, Iter 7, disc loss: 0.0004577373176823559, policy loss: 10.969931720734424
Experience 25, Iter 8, disc loss: 0.00046366934728359, policy loss: 11.17693829437943
Experience 25, Iter 9, disc loss: 0.0004503740985930461, policy loss: 11.177102817815953
Experience 25, Iter 10, disc loss: 0.000453847877465547, policy loss: 11.02736294644675
Experience 25, Iter 11, disc loss: 0.00044958927773556075, policy loss: 11.180195811480601
Experience 25, Iter 12, disc loss: 0.00044875984504896035, policy loss: 10.517746699014744
Experience 25, Iter 13, disc loss: 0.0005444212027232351, policy loss: 10.018631907307327
Experience 25, Iter 14, disc loss: 0.0004953990539779594, policy loss: 9.942690328178694
Experience 25, Iter 15, disc loss: 0.0006607752321279722, policy loss: 9.382042660760003
Experience 25, Iter 16, disc loss: 0.0008205063285258965, policy loss: 8.780221670469974
Experience 25, Iter 17, disc loss: 0.0006889356224442154, policy loss: 9.100480692444737
Experience 25, Iter 18, disc loss: 0.000515550879478385, policy loss: 10.476063418066587
Experience 25, Iter 19, disc loss: 0.0007568959153847424, policy loss: 9.092365840414324
Experience 25, Iter 20, disc loss: 0.0007954040912664395, policy loss: 9.095752755599943
Experience 25, Iter 21, disc loss: 0.0006444110554549691, policy loss: 9.071179857921166
Experience 25, Iter 22, disc loss: 0.0006901624492284022, policy loss: 9.358615569023812
Experience 25, Iter 23, disc loss: 0.0005538343898825695, policy loss: 9.444889802729254
Experience 25, Iter 24, disc loss: 0.0005029253683237686, policy loss: 9.67748090686386
Experience 25, Iter 25, disc loss: 0.0005597768638002747, policy loss: 9.475066884353932
Experience 25, Iter 26, disc loss: 0.0006096233540379965, policy loss: 9.578855656430132
Experience 25, Iter 27, disc loss: 0.000586404128752264, policy loss: 9.409608226864249
Experience 25, Iter 28, disc loss: 0.000611974013275819, policy loss: 10.140036051822992
Experience 25, Iter 29, disc loss: 0.000736582056119886, policy loss: 9.336484837658531
Experience 25, Iter 30, disc loss: 0.0006663643366789547, policy loss: 9.216422308500858
Experience 25, Iter 31, disc loss: 0.0007480777556540819, policy loss: 8.665888320369389
Experience 25, Iter 32, disc loss: 0.0009130169668955788, policy loss: 8.852113757730944
Experience 25, Iter 33, disc loss: 0.0008090769206200722, policy loss: 9.17958182259519
Experience 25, Iter 34, disc loss: 0.0009784949663093777, policy loss: 8.498180772839365
Experience 25, Iter 35, disc loss: 0.0008489504096387136, policy loss: 8.789330470828643
Experience 25, Iter 36, disc loss: 0.0007442308144280642, policy loss: 9.034047843095813
Experience 25, Iter 37, disc loss: 0.0008276262662317698, policy loss: 8.642577328559968
Experience 25, Iter 38, disc loss: 0.0006844648308932748, policy loss: 9.018365241050063
Experience 25, Iter 39, disc loss: 0.0006113783013539079, policy loss: 9.60024166619353
Experience 25, Iter 40, disc loss: 0.0008478851906400053, policy loss: 8.923232292407956
Experience 25, Iter 41, disc loss: 0.0007833710642184113, policy loss: 8.776665511722058
Experience 25, Iter 42, disc loss: 0.0007961752284781436, policy loss: 8.890447166299317
Experience 25, Iter 43, disc loss: 0.000784519267474109, policy loss: 9.36029155579484
Experience 25, Iter 44, disc loss: 0.0009756271613035597, policy loss: 8.932815515719774
Experience 25, Iter 45, disc loss: 0.0011514302401008515, policy loss: 8.27407053783788
Experience 25, Iter 46, disc loss: 0.0010871724898184017, policy loss: 8.148099791686604
Experience 25, Iter 47, disc loss: 0.0009083513536968298, policy loss: 8.92178642949353
Experience 25, Iter 48, disc loss: 0.0007369945631954027, policy loss: 9.250706699712085
Experience 25, Iter 49, disc loss: 0.0007061577188019803, policy loss: 9.453428673082968
Experience 25, Iter 50, disc loss: 0.0009314824362210055, policy loss: 8.963664116092943
Experience 25, Iter 51, disc loss: 0.0008374926234637484, policy loss: 9.018139886348695
Experience 25, Iter 52, disc loss: 0.0008305682513086267, policy loss: 9.103383972959556
Experience 25, Iter 53, disc loss: 0.0009215026464345381, policy loss: 8.592347595631038
Experience 25, Iter 54, disc loss: 0.0012459230798550574, policy loss: 8.472301103550517
Experience 25, Iter 55, disc loss: 0.0009178227586441433, policy loss: 8.782071119348085
Experience 25, Iter 56, disc loss: 0.000853568481662882, policy loss: 8.774027172255924
Experience 25, Iter 57, disc loss: 0.000899098528868568, policy loss: 9.214762649818267
Experience 25, Iter 58, disc loss: 0.0008746704436800199, policy loss: 9.18576763149398
Experience 25, Iter 59, disc loss: 0.0008101767803070868, policy loss: 9.031202956129555
Experience 25, Iter 60, disc loss: 0.0008144161250046608, policy loss: 9.232493180534036
Experience 25, Iter 61, disc loss: 0.001036252930593964, policy loss: 9.067278742500886
Experience 25, Iter 62, disc loss: 0.0008678970602969639, policy loss: 9.201276667408699
Experience 25, Iter 63, disc loss: 0.001296605361452131, policy loss: 8.425162719657953
Experience 25, Iter 64, disc loss: 0.0009428753595871425, policy loss: 8.994670806332248
Experience 25, Iter 65, disc loss: 0.0008429227443907839, policy loss: 9.225217166698581
Experience 25, Iter 66, disc loss: 0.0007752124189109147, policy loss: 9.241065539173075
Experience 25, Iter 67, disc loss: 0.0009899564456750222, policy loss: 8.633288052948538
Experience 25, Iter 68, disc loss: 0.0009260271764412397, policy loss: 8.990238403489004
Experience 25, Iter 69, disc loss: 0.001062871510393888, policy loss: 9.117691870724407
Experience 25, Iter 70, disc loss: 0.0008418614368346643, policy loss: 9.625067704108815
Experience 25, Iter 71, disc loss: 0.0009146356654596674, policy loss: 9.241131384414349
Experience 25, Iter 72, disc loss: 0.0009456845310200758, policy loss: 8.808547785588724
Experience 25, Iter 73, disc loss: 0.0010162585687748112, policy loss: 8.574463399260187
Experience 25, Iter 74, disc loss: 0.0009488512804365868, policy loss: 9.065737392276741
Experience 25, Iter 75, disc loss: 0.0009319406566881915, policy loss: 9.34712948497129
Experience 25, Iter 76, disc loss: 0.0008369467065741836, policy loss: 8.939029335472163
Experience 25, Iter 77, disc loss: 0.0007967777419302226, policy loss: 9.300602618935672
Experience 25, Iter 78, disc loss: 0.000940189282551021, policy loss: 9.165191824510249
Experience 25, Iter 79, disc loss: 0.0009231801174532092, policy loss: 8.734188250600866
Experience 25, Iter 80, disc loss: 0.0008838703542657039, policy loss: 9.198404378320696
Experience 25, Iter 81, disc loss: 0.0011472906087207479, policy loss: 8.517233917992378
Experience 25, Iter 82, disc loss: 0.0009395261545926177, policy loss: 8.755398145635075
Experience 25, Iter 83, disc loss: 0.0010047513902993557, policy loss: 8.942921277503743
Experience 25, Iter 84, disc loss: 0.0009656304781563406, policy loss: 9.279302995390774
Experience 25, Iter 85, disc loss: 0.0007842961416919124, policy loss: 9.146270756648704
Experience 25, Iter 86, disc loss: 0.0009498948298016729, policy loss: 8.964953229206557
Experience 25, Iter 87, disc loss: 0.0008925243476324727, policy loss: 9.21483592991251
Experience 25, Iter 88, disc loss: 0.0008757043347562141, policy loss: 9.220453972854495
Experience 25, Iter 89, disc loss: 0.0010386900164479627, policy loss: 9.360322063500693
Experience 25, Iter 90, disc loss: 0.0010395685315735697, policy loss: 8.962316155903796
Experience 25, Iter 91, disc loss: 0.0009062580689354269, policy loss: 9.317168351619415
Experience 25, Iter 92, disc loss: 0.0007590611103108099, policy loss: 9.545455047107298
Experience 25, Iter 93, disc loss: 0.0008456775547214367, policy loss: 9.37504016473266
Experience 25, Iter 94, disc loss: 0.0008375259379152276, policy loss: 9.057741500568694
Experience 25, Iter 95, disc loss: 0.0008348935612879453, policy loss: 9.462773104681531
Experience 25, Iter 96, disc loss: 0.0008053546970271928, policy loss: 9.374007474453215
Experience 25, Iter 97, disc loss: 0.0008609698397649731, policy loss: 9.339700926784495
Experience 25, Iter 98, disc loss: 0.0008852378786366347, policy loss: 8.752011663393498
Experience 25, Iter 99, disc loss: 0.0007613263279401046, policy loss: 9.401332308330897
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.2418],
        [1.9912],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0127, 0.1774, 1.8123, 0.0336, 0.0292, 6.0547]],

        [[0.0127, 0.1774, 1.8123, 0.0336, 0.0292, 6.0547]],

        [[0.0127, 0.1774, 1.8123, 0.0336, 0.0292, 6.0547]],

        [[0.0127, 0.1774, 1.8123, 0.0336, 0.0292, 6.0547]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0164, 0.9672, 7.9648, 0.1553], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0164, 0.9672, 7.9648, 0.1553])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.709
Iter 2/2000 - Loss: 3.662
Iter 3/2000 - Loss: 3.413
Iter 4/2000 - Loss: 3.331
Iter 5/2000 - Loss: 3.285
Iter 6/2000 - Loss: 3.140
Iter 7/2000 - Loss: 2.960
Iter 8/2000 - Loss: 2.798
Iter 9/2000 - Loss: 2.634
Iter 10/2000 - Loss: 2.435
Iter 11/2000 - Loss: 2.210
Iter 12/2000 - Loss: 1.985
Iter 13/2000 - Loss: 1.761
Iter 14/2000 - Loss: 1.525
Iter 15/2000 - Loss: 1.262
Iter 16/2000 - Loss: 0.976
Iter 17/2000 - Loss: 0.678
Iter 18/2000 - Loss: 0.378
Iter 19/2000 - Loss: 0.079
Iter 20/2000 - Loss: -0.222
Iter 1981/2000 - Loss: -7.779
Iter 1982/2000 - Loss: -7.779
Iter 1983/2000 - Loss: -7.779
Iter 1984/2000 - Loss: -7.779
Iter 1985/2000 - Loss: -7.779
Iter 1986/2000 - Loss: -7.779
Iter 1987/2000 - Loss: -7.779
Iter 1988/2000 - Loss: -7.779
Iter 1989/2000 - Loss: -7.779
Iter 1990/2000 - Loss: -7.779
Iter 1991/2000 - Loss: -7.779
Iter 1992/2000 - Loss: -7.779
Iter 1993/2000 - Loss: -7.779
Iter 1994/2000 - Loss: -7.779
Iter 1995/2000 - Loss: -7.779
Iter 1996/2000 - Loss: -7.779
Iter 1997/2000 - Loss: -7.779
Iter 1998/2000 - Loss: -7.779
Iter 1999/2000 - Loss: -7.779
Iter 2000/2000 - Loss: -7.779
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[10.8913,  5.8969, 22.8063,  1.8235, 17.0937, 38.3671]],

        [[16.3706, 33.0257,  7.1073,  1.1986,  1.7420, 34.1837]],

        [[16.4678, 31.1441,  8.7957,  0.9053,  0.6797, 26.4702]],

        [[13.3240, 25.0925, 11.5167,  1.4866,  1.8063, 47.6491]]])
Signal Variance: tensor([ 0.0601,  2.5900, 13.2768,  0.3274])
Estimated target variance: tensor([0.0164, 0.9672, 7.9648, 0.1553])
N: 260
Signal to noise ratio: tensor([14.6040, 83.6221, 76.6886, 31.4950])
Bound on condition number: tensor([  55452.9710, 1818089.3079, 1529099.1471,  257903.6212])
Policy Optimizer learning rate:
0.0009740037464252969
Experience 26, Iter 0, disc loss: 0.0009167938398239128, policy loss: 9.138199008059532
Experience 26, Iter 1, disc loss: 0.0009300052261329427, policy loss: 8.895783362344805
Experience 26, Iter 2, disc loss: 0.0007220817954997404, policy loss: 9.577683471078686
Experience 26, Iter 3, disc loss: 0.0008557747063835208, policy loss: 8.920359575167538
Experience 26, Iter 4, disc loss: 0.0007810320307982807, policy loss: 9.176736219940933
Experience 26, Iter 5, disc loss: 0.0007564462946004363, policy loss: 9.384438564173934
Experience 26, Iter 6, disc loss: 0.0007276164292622115, policy loss: 9.790547011337654
Experience 26, Iter 7, disc loss: 0.0008902856561439833, policy loss: 8.932076654551878
Experience 26, Iter 8, disc loss: 0.0008356997522296775, policy loss: 8.78084607083076
Experience 26, Iter 9, disc loss: 0.0008693363873982058, policy loss: 9.115563373689165
Experience 26, Iter 10, disc loss: 0.0007775676045459056, policy loss: 9.34637398850668
Experience 26, Iter 11, disc loss: 0.0008147588945164718, policy loss: 9.108168586908604
Experience 26, Iter 12, disc loss: 0.0008859685846226636, policy loss: 9.460771631217236
Experience 26, Iter 13, disc loss: 0.0007819517810611964, policy loss: 9.35039034010661
Experience 26, Iter 14, disc loss: 0.0008926698092095605, policy loss: 9.60212735653954
Experience 26, Iter 15, disc loss: 0.0009109877236693335, policy loss: 8.792891865393104
Experience 26, Iter 16, disc loss: 0.0008773311882531185, policy loss: 9.751681265657954
Experience 26, Iter 17, disc loss: 0.000834399906873476, policy loss: 9.469006595726967
Experience 26, Iter 18, disc loss: 0.0008403628020454228, policy loss: 8.840213561340342
Experience 26, Iter 19, disc loss: 0.0007812354158644358, policy loss: 9.905726235243868
Experience 26, Iter 20, disc loss: 0.0007557986280545387, policy loss: 8.912398446049025
Experience 26, Iter 21, disc loss: 0.0008378674066803671, policy loss: 8.956660761766967
Experience 26, Iter 22, disc loss: 0.000805399917956892, policy loss: 8.887660810598376
Experience 26, Iter 23, disc loss: 0.0008265408812663624, policy loss: 8.53383006332407
Experience 26, Iter 24, disc loss: 0.0009070800329782959, policy loss: 8.584989203503062
Experience 26, Iter 25, disc loss: 0.0008559038989441268, policy loss: 9.366894340069644
Experience 26, Iter 26, disc loss: 0.0008274180767822719, policy loss: 9.068902625804228
Experience 26, Iter 27, disc loss: 0.0009235054818312493, policy loss: 8.936490994773058
Experience 26, Iter 28, disc loss: 0.0008374681288419023, policy loss: 9.273395133096583
Experience 26, Iter 29, disc loss: 0.0008599800701700848, policy loss: 9.615988668846015
Experience 26, Iter 30, disc loss: 0.0008113504511697513, policy loss: 8.884849614473886
Experience 26, Iter 31, disc loss: 0.0008101273664396072, policy loss: 8.957515689358424
Experience 26, Iter 32, disc loss: 0.0008496791561187918, policy loss: 9.261312028258752
Experience 26, Iter 33, disc loss: 0.0008221942791799677, policy loss: 9.478876141972707
Experience 26, Iter 34, disc loss: 0.000889570409536902, policy loss: 9.189949949060754
Experience 26, Iter 35, disc loss: 0.0009392301784495025, policy loss: 8.482141059699781
Experience 26, Iter 36, disc loss: 0.0010139065534036965, policy loss: 8.810469765894775
Experience 26, Iter 37, disc loss: 0.0008233899332424801, policy loss: 9.042076626013671
Experience 26, Iter 38, disc loss: 0.0008549661006887561, policy loss: 9.030753034365098
Experience 26, Iter 39, disc loss: 0.001139344882559292, policy loss: 9.088434913142278
Experience 26, Iter 40, disc loss: 0.000980259681682139, policy loss: 9.525519752153945
Experience 26, Iter 41, disc loss: 0.0008523763611134776, policy loss: 9.229738227256583
Experience 26, Iter 42, disc loss: 0.0007214394144480349, policy loss: 9.395148941527157
Experience 26, Iter 43, disc loss: 0.0007887466134959189, policy loss: 9.237899589717024
Experience 26, Iter 44, disc loss: 0.0007949928918291704, policy loss: 8.836664959261409
Experience 26, Iter 45, disc loss: 0.0009036655542577908, policy loss: 9.450263277496344
Experience 26, Iter 46, disc loss: 0.0008293881177475757, policy loss: 9.631169118013569
Experience 26, Iter 47, disc loss: 0.0008439186806361392, policy loss: 9.28561478174477
Experience 26, Iter 48, disc loss: 0.0007634727011522816, policy loss: 10.046339388540583
Experience 26, Iter 49, disc loss: 0.0009119009046404884, policy loss: 9.531301481814973
Experience 26, Iter 50, disc loss: 0.0009179520278477943, policy loss: 8.986657907104703
Experience 26, Iter 51, disc loss: 0.0007141306031217143, policy loss: 9.673978842411882
Experience 26, Iter 52, disc loss: 0.0008761551595457777, policy loss: 9.266717716738059
Experience 26, Iter 53, disc loss: 0.0008272059654645359, policy loss: 9.363821843740679
Experience 26, Iter 54, disc loss: 0.0008840872709601848, policy loss: 8.796217382650074
Experience 26, Iter 55, disc loss: 0.0008684304246579333, policy loss: 9.02078676119924
Experience 26, Iter 56, disc loss: 0.0009044591449873815, policy loss: 9.242119979961338
Experience 26, Iter 57, disc loss: 0.0008617627222060738, policy loss: 9.477774873852468
Experience 26, Iter 58, disc loss: 0.0007602717838757348, policy loss: 9.103474743297047
Experience 26, Iter 59, disc loss: 0.0010185900378343062, policy loss: 8.860348774455655
Experience 26, Iter 60, disc loss: 0.0008049834882113639, policy loss: 9.242503106678182
Experience 26, Iter 61, disc loss: 0.0008938576877919754, policy loss: 8.793007454325258
Experience 26, Iter 62, disc loss: 0.0007828847790593437, policy loss: 9.291856840496097
Experience 26, Iter 63, disc loss: 0.0008465034964398929, policy loss: 8.974213385162281
Experience 26, Iter 64, disc loss: 0.0008955619474572987, policy loss: 9.185272639010478
Experience 26, Iter 65, disc loss: 0.0008475692812795396, policy loss: 9.479336008412503
Experience 26, Iter 66, disc loss: 0.0008719310850196556, policy loss: 9.142577697487308
Experience 26, Iter 67, disc loss: 0.0008360962970141799, policy loss: 9.412072191299071
Experience 26, Iter 68, disc loss: 0.0008892287717896447, policy loss: 9.206454882931693
Experience 26, Iter 69, disc loss: 0.0006528898458434452, policy loss: 10.106755956767858
Experience 26, Iter 70, disc loss: 0.0009124787899828725, policy loss: 8.769051278467046
Experience 26, Iter 71, disc loss: 0.0008658556363303155, policy loss: 8.929943092889557
Experience 26, Iter 72, disc loss: 0.0007270318120369181, policy loss: 9.29000905787007
Experience 26, Iter 73, disc loss: 0.000633759092153203, policy loss: 9.403766588161666
Experience 26, Iter 74, disc loss: 0.0007909364896983796, policy loss: 9.879765099859641
Experience 26, Iter 75, disc loss: 0.0008472334683786496, policy loss: 9.254365380711674
Experience 26, Iter 76, disc loss: 0.0007791125190662606, policy loss: 9.564022004497108
Experience 26, Iter 77, disc loss: 0.0008177983357655118, policy loss: 9.374628261490171
Experience 26, Iter 78, disc loss: 0.0007486319141501532, policy loss: 9.41237516944901
Experience 26, Iter 79, disc loss: 0.0008297807239660002, policy loss: 9.00561714599891
Experience 26, Iter 80, disc loss: 0.0008871600404745884, policy loss: 9.635463627140897
Experience 26, Iter 81, disc loss: 0.0007644538664241614, policy loss: 9.136549123021524
Experience 26, Iter 82, disc loss: 0.0008712824563564519, policy loss: 9.29608515190388
Experience 26, Iter 83, disc loss: 0.0007595782385194931, policy loss: 9.160795488519836
Experience 26, Iter 84, disc loss: 0.0007990193158941095, policy loss: 9.24100443222753
Experience 26, Iter 85, disc loss: 0.00097784418658583, policy loss: 8.709548638895214
Experience 26, Iter 86, disc loss: 0.0007449705399566193, policy loss: 9.290956893357766
Experience 26, Iter 87, disc loss: 0.0007947278389399408, policy loss: 9.292493433390273
Experience 26, Iter 88, disc loss: 0.0007824667384293991, policy loss: 9.226933069841351
Experience 26, Iter 89, disc loss: 0.0007170503492764639, policy loss: 9.206384681404892
Experience 26, Iter 90, disc loss: 0.0006813160345620199, policy loss: 9.270712406489821
Experience 26, Iter 91, disc loss: 0.0008325586044201354, policy loss: 9.2848609403452
Experience 26, Iter 92, disc loss: 0.0006872992026219289, policy loss: 10.111993470792115
Experience 26, Iter 93, disc loss: 0.0009820840589978258, policy loss: 8.607306352860725
Experience 26, Iter 94, disc loss: 0.0009123187257539609, policy loss: 8.652288552206347
Experience 26, Iter 95, disc loss: 0.0007807768875177181, policy loss: 9.37942914604243
Experience 26, Iter 96, disc loss: 0.0008255454944521203, policy loss: 9.354460798478243
Experience 26, Iter 97, disc loss: 0.0009069484548861657, policy loss: 9.114878903647668
Experience 26, Iter 98, disc loss: 0.0007742222751094187, policy loss: 9.12552787156473
Experience 26, Iter 99, disc loss: 0.000792685021132718, policy loss: 9.040722915455927
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.2450],
        [2.0058],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0124, 0.1773, 1.8173, 0.0337, 0.0301, 6.1220]],

        [[0.0124, 0.1773, 1.8173, 0.0337, 0.0301, 6.1220]],

        [[0.0124, 0.1773, 1.8173, 0.0337, 0.0301, 6.1220]],

        [[0.0124, 0.1773, 1.8173, 0.0337, 0.0301, 6.1220]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0164, 0.9799, 8.0234, 0.1553], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0164, 0.9799, 8.0234, 0.1553])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.710
Iter 2/2000 - Loss: 3.660
Iter 3/2000 - Loss: 3.404
Iter 4/2000 - Loss: 3.318
Iter 5/2000 - Loss: 3.266
Iter 6/2000 - Loss: 3.118
Iter 7/2000 - Loss: 2.935
Iter 8/2000 - Loss: 2.768
Iter 9/2000 - Loss: 2.598
Iter 10/2000 - Loss: 2.394
Iter 11/2000 - Loss: 2.164
Iter 12/2000 - Loss: 1.933
Iter 13/2000 - Loss: 1.706
Iter 14/2000 - Loss: 1.467
Iter 15/2000 - Loss: 1.205
Iter 16/2000 - Loss: 0.919
Iter 17/2000 - Loss: 0.620
Iter 18/2000 - Loss: 0.319
Iter 19/2000 - Loss: 0.020
Iter 20/2000 - Loss: -0.279
Iter 1981/2000 - Loss: -7.821
Iter 1982/2000 - Loss: -7.821
Iter 1983/2000 - Loss: -7.821
Iter 1984/2000 - Loss: -7.821
Iter 1985/2000 - Loss: -7.821
Iter 1986/2000 - Loss: -7.821
Iter 1987/2000 - Loss: -7.821
Iter 1988/2000 - Loss: -7.821
Iter 1989/2000 - Loss: -7.821
Iter 1990/2000 - Loss: -7.821
Iter 1991/2000 - Loss: -7.821
Iter 1992/2000 - Loss: -7.821
Iter 1993/2000 - Loss: -7.821
Iter 1994/2000 - Loss: -7.821
Iter 1995/2000 - Loss: -7.821
Iter 1996/2000 - Loss: -7.821
Iter 1997/2000 - Loss: -7.821
Iter 1998/2000 - Loss: -7.821
Iter 1999/2000 - Loss: -7.821
Iter 2000/2000 - Loss: -7.821
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[10.6479,  6.2482, 22.8033,  1.8973, 17.1476, 38.0464]],

        [[16.1438, 32.1472,  7.1850,  1.2017,  1.7485, 34.5437]],

        [[17.3549, 29.8914,  8.6748,  0.9119,  0.6627, 26.4100]],

        [[13.2957, 24.2500, 11.2974,  1.4758,  1.7322, 48.4208]]])
Signal Variance: tensor([ 0.0632,  2.6720, 13.1971,  0.3209])
Estimated target variance: tensor([0.0164, 0.9799, 8.0234, 0.1553])
N: 270
Signal to noise ratio: tensor([14.8428, 84.9956, 77.4623, 31.2572])
Bound on condition number: tensor([  59484.4429, 1950551.0295, 1620110.4020,  263793.9010])
Policy Optimizer learning rate:
0.0009729780714786251
Experience 27, Iter 0, disc loss: 0.0007228460656739155, policy loss: 9.39984612719591
Experience 27, Iter 1, disc loss: 0.0007370799063119806, policy loss: 9.627402576585382
Experience 27, Iter 2, disc loss: 0.0008684554264299438, policy loss: 9.061429267525696
Experience 27, Iter 3, disc loss: 0.0009285605896339482, policy loss: 8.603841001283488
Experience 27, Iter 4, disc loss: 0.0007121820327774867, policy loss: 9.69759374933863
Experience 27, Iter 5, disc loss: 0.000875092710328521, policy loss: 8.911054174560373
Experience 27, Iter 6, disc loss: 0.0008324744238806683, policy loss: 9.111092033800208
Experience 27, Iter 7, disc loss: 0.0008958485906315614, policy loss: 9.408673489573395
Experience 27, Iter 8, disc loss: 0.0008043259906078952, policy loss: 9.181703866819614
Experience 27, Iter 9, disc loss: 0.000716855821985386, policy loss: 9.730229729882902
Experience 27, Iter 10, disc loss: 0.0007935202169902563, policy loss: 9.510253849208718
Experience 27, Iter 11, disc loss: 0.000723356604771635, policy loss: 9.42296275914639
Experience 27, Iter 12, disc loss: 0.0007134754357078284, policy loss: 8.933272667574812
Experience 27, Iter 13, disc loss: 0.000755639069380904, policy loss: 9.018003065306456
Experience 27, Iter 14, disc loss: 0.0008240616175306166, policy loss: 8.868135892807626
Experience 27, Iter 15, disc loss: 0.0007921480510037133, policy loss: 9.733629595016428
Experience 27, Iter 16, disc loss: 0.00078260833778252, policy loss: 8.991171518265926
Experience 27, Iter 17, disc loss: 0.0007329943969384724, policy loss: 9.047620627537171
Experience 27, Iter 18, disc loss: 0.0009250207253708102, policy loss: 8.802138844113857
Experience 27, Iter 19, disc loss: 0.0009353761327722625, policy loss: 9.04502132034332
Experience 27, Iter 20, disc loss: 0.0007537541861239377, policy loss: 9.193913360739785
Experience 27, Iter 21, disc loss: 0.0007379539635417344, policy loss: 9.44147529592393
Experience 27, Iter 22, disc loss: 0.0006499980177756639, policy loss: 9.611041211294065
Experience 27, Iter 23, disc loss: 0.0007660048070034987, policy loss: 9.259859099491385
Experience 27, Iter 24, disc loss: 0.000878227346099247, policy loss: 8.904154498096956
Experience 27, Iter 25, disc loss: 0.000695417900899314, policy loss: 9.48452475104197
Experience 27, Iter 26, disc loss: 0.0007911154508862736, policy loss: 9.18599042299806
Experience 27, Iter 27, disc loss: 0.0007734950822466747, policy loss: 9.116067643362516
Experience 27, Iter 28, disc loss: 0.0007382051233539109, policy loss: 9.357114503441162
Experience 27, Iter 29, disc loss: 0.0007093675294357923, policy loss: 9.981113306112126
Experience 27, Iter 30, disc loss: 0.0008023484581193032, policy loss: 9.077096508609632
Experience 27, Iter 31, disc loss: 0.0006010174542249908, policy loss: 9.893586213586094
Experience 27, Iter 32, disc loss: 0.0006953381203905462, policy loss: 9.70897818128999
Experience 27, Iter 33, disc loss: 0.0008306959369859983, policy loss: 9.156312820705427
Experience 27, Iter 34, disc loss: 0.0006905840142725291, policy loss: 9.357117174132743
Experience 27, Iter 35, disc loss: 0.0008235453780587694, policy loss: 8.352490614462967
Experience 27, Iter 36, disc loss: 0.0006470880137573007, policy loss: 9.48293338072644
Experience 27, Iter 37, disc loss: 0.0006937552312955809, policy loss: 9.80197082193706
Experience 27, Iter 38, disc loss: 0.0006397270011873443, policy loss: 9.525738299468536
Experience 27, Iter 39, disc loss: 0.0006872844972767899, policy loss: 9.852142843684756
Experience 27, Iter 40, disc loss: 0.0007898566141147504, policy loss: 9.301005416213115
Experience 27, Iter 41, disc loss: 0.0009331963907656653, policy loss: 8.91570893788358
Experience 27, Iter 42, disc loss: 0.000896994810772061, policy loss: 9.18291059373616
Experience 27, Iter 43, disc loss: 0.0008169232569393008, policy loss: 9.140915132924718
Experience 27, Iter 44, disc loss: 0.000708391405331701, policy loss: 9.528827284197567
Experience 27, Iter 45, disc loss: 0.000769649228540336, policy loss: 9.34206245404919
Experience 27, Iter 46, disc loss: 0.0008011630075866012, policy loss: 8.868965791934617
Experience 27, Iter 47, disc loss: 0.0007313895816891969, policy loss: 8.98960174456385
Experience 27, Iter 48, disc loss: 0.0007547802832929323, policy loss: 9.192941440476876
Experience 27, Iter 49, disc loss: 0.0006971790266827703, policy loss: 9.790630293462158
Experience 27, Iter 50, disc loss: 0.0007103011530558627, policy loss: 9.270279409673092
Experience 27, Iter 51, disc loss: 0.0005958058393208902, policy loss: 9.461840555951236
Experience 27, Iter 52, disc loss: 0.0007394672711921284, policy loss: 9.463241108346853
Experience 27, Iter 53, disc loss: 0.000750960037636577, policy loss: 9.307256848872386
Experience 27, Iter 54, disc loss: 0.0007311585229741785, policy loss: 9.448632951066076
Experience 27, Iter 55, disc loss: 0.0007300977830514499, policy loss: 9.788596672149296
Experience 27, Iter 56, disc loss: 0.0006832756456215451, policy loss: 9.93706904271102
Experience 27, Iter 57, disc loss: 0.0007047111598522301, policy loss: 9.589008102703508
Experience 27, Iter 58, disc loss: 0.0007338966169109726, policy loss: 9.161326231093414
Experience 27, Iter 59, disc loss: 0.0007694680527072068, policy loss: 9.308173682022439
Experience 27, Iter 60, disc loss: 0.0006436899011917599, policy loss: 9.85519770761514
Experience 27, Iter 61, disc loss: 0.0006410217928746819, policy loss: 9.389601158072518
Experience 27, Iter 62, disc loss: 0.00086203559478163, policy loss: 8.666130253868129
Experience 27, Iter 63, disc loss: 0.0007635722107749926, policy loss: 9.511415807809644
Experience 27, Iter 64, disc loss: 0.0005985036372577864, policy loss: 9.852717117798349
Experience 27, Iter 65, disc loss: 0.0007045264457123075, policy loss: 8.938623600198682
Experience 27, Iter 66, disc loss: 0.0006838140634660708, policy loss: 9.23081562231431
Experience 27, Iter 67, disc loss: 0.0006554341892476167, policy loss: 9.99066501032048
Experience 27, Iter 68, disc loss: 0.0007421357291027335, policy loss: 8.817848155697146
Experience 27, Iter 69, disc loss: 0.0006333278952086801, policy loss: 9.176122605625018
Experience 27, Iter 70, disc loss: 0.0007189871389668382, policy loss: 9.190957163501116
Experience 27, Iter 71, disc loss: 0.0006130347974061413, policy loss: 9.667257559128029
Experience 27, Iter 72, disc loss: 0.0006214805464814796, policy loss: 9.575366777235088
Experience 27, Iter 73, disc loss: 0.0007262357937656009, policy loss: 9.115937093698745
Experience 27, Iter 74, disc loss: 0.000695824841428508, policy loss: 9.71659438007929
Experience 27, Iter 75, disc loss: 0.0006625969532403635, policy loss: 9.234915958661416
Experience 27, Iter 76, disc loss: 0.0007582501058086466, policy loss: 8.911334634151572
Experience 27, Iter 77, disc loss: 0.0007111972256267794, policy loss: 9.234559212394576
Experience 27, Iter 78, disc loss: 0.0007094445417534868, policy loss: 9.247914928882725
Experience 27, Iter 79, disc loss: 0.000653057383350981, policy loss: 9.750535696034984
Experience 27, Iter 80, disc loss: 0.0007074446503932033, policy loss: 9.481970323042388
Experience 27, Iter 81, disc loss: 0.0007838105077795738, policy loss: 9.530394752054075
Experience 27, Iter 82, disc loss: 0.000680473285304947, policy loss: 9.185237467776599
Experience 27, Iter 83, disc loss: 0.0007430059016823381, policy loss: 9.351108686790772
Experience 27, Iter 84, disc loss: 0.000787702514345701, policy loss: 9.321446894584396
Experience 27, Iter 85, disc loss: 0.0006418309040447389, policy loss: 9.197349282435534
Experience 27, Iter 86, disc loss: 0.0007806736768378909, policy loss: 8.885674935973674
Experience 27, Iter 87, disc loss: 0.0006141838612022791, policy loss: 9.991122157537346
Experience 27, Iter 88, disc loss: 0.0007555516159535585, policy loss: 9.08145929921244
Experience 27, Iter 89, disc loss: 0.0008658945464073702, policy loss: 9.07333334809264
Experience 27, Iter 90, disc loss: 0.0006591895694409145, policy loss: 9.634751637417853
Experience 27, Iter 91, disc loss: 0.000794579821702038, policy loss: 8.988776692782821
Experience 27, Iter 92, disc loss: 0.0007314227844419015, policy loss: 9.310378137006765
Experience 27, Iter 93, disc loss: 0.0007288800395298595, policy loss: 9.120883590505036
Experience 27, Iter 94, disc loss: 0.0006164528483439079, policy loss: 9.224052195951014
Experience 27, Iter 95, disc loss: 0.0009558564515300824, policy loss: 8.352044702223973
Experience 27, Iter 96, disc loss: 0.0007917969311060785, policy loss: 8.915174815813078
Experience 27, Iter 97, disc loss: 0.0006772496239671898, policy loss: 9.292278272465344
Experience 27, Iter 98, disc loss: 0.0007546048019413616, policy loss: 9.047990420956435
Experience 27, Iter 99, disc loss: 0.0008191005025557062, policy loss: 9.197381147850127
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.2470],
        [2.0192],
        [0.0389]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0121, 0.1769, 1.8242, 0.0337, 0.0302, 6.1678]],

        [[0.0121, 0.1769, 1.8242, 0.0337, 0.0302, 6.1678]],

        [[0.0121, 0.1769, 1.8242, 0.0337, 0.0302, 6.1678]],

        [[0.0121, 0.1769, 1.8242, 0.0337, 0.0302, 6.1678]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0163, 0.9880, 8.0770, 0.1557], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0163, 0.9880, 8.0770, 0.1557])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.674
Iter 2/2000 - Loss: 3.621
Iter 3/2000 - Loss: 3.363
Iter 4/2000 - Loss: 3.273
Iter 5/2000 - Loss: 3.216
Iter 6/2000 - Loss: 3.060
Iter 7/2000 - Loss: 2.870
Iter 8/2000 - Loss: 2.698
Iter 9/2000 - Loss: 2.525
Iter 10/2000 - Loss: 2.318
Iter 11/2000 - Loss: 2.086
Iter 12/2000 - Loss: 1.854
Iter 13/2000 - Loss: 1.625
Iter 14/2000 - Loss: 1.385
Iter 15/2000 - Loss: 1.122
Iter 16/2000 - Loss: 0.837
Iter 17/2000 - Loss: 0.540
Iter 18/2000 - Loss: 0.241
Iter 19/2000 - Loss: -0.056
Iter 20/2000 - Loss: -0.352
Iter 1981/2000 - Loss: -7.847
Iter 1982/2000 - Loss: -7.847
Iter 1983/2000 - Loss: -7.847
Iter 1984/2000 - Loss: -7.847
Iter 1985/2000 - Loss: -7.847
Iter 1986/2000 - Loss: -7.847
Iter 1987/2000 - Loss: -7.847
Iter 1988/2000 - Loss: -7.847
Iter 1989/2000 - Loss: -7.847
Iter 1990/2000 - Loss: -7.847
Iter 1991/2000 - Loss: -7.847
Iter 1992/2000 - Loss: -7.847
Iter 1993/2000 - Loss: -7.847
Iter 1994/2000 - Loss: -7.847
Iter 1995/2000 - Loss: -7.847
Iter 1996/2000 - Loss: -7.847
Iter 1997/2000 - Loss: -7.847
Iter 1998/2000 - Loss: -7.847
Iter 1999/2000 - Loss: -7.848
Iter 2000/2000 - Loss: -7.848
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[10.5773,  6.3031, 24.5017,  1.9166, 17.1070, 37.0211]],

        [[15.8091, 31.6985,  7.0986,  1.1958,  1.7569, 34.1985]],

        [[16.9477, 28.0857,  8.8115,  0.9041,  0.6567, 26.5114]],

        [[13.0175, 23.5847, 11.1115,  1.4693,  1.7213, 48.7663]]])
Signal Variance: tensor([ 0.0653,  2.6336, 13.6549,  0.3116])
Estimated target variance: tensor([0.0163, 0.9880, 8.0770, 0.1557])
N: 280
Signal to noise ratio: tensor([14.8580, 85.8340, 77.2974, 30.7485])
Bound on condition number: tensor([  61814.1834, 2062892.3135, 1672969.3518,  264732.6775])
Policy Optimizer learning rate:
0.0009719534766192735
Experience 28, Iter 0, disc loss: 0.0006857103859087895, policy loss: 9.467682156434808
Experience 28, Iter 1, disc loss: 0.0006567875895640914, policy loss: 9.264651150112655
Experience 28, Iter 2, disc loss: 0.0007944337527175418, policy loss: 8.857664526764555
Experience 28, Iter 3, disc loss: 0.0008152335077672788, policy loss: 9.186718876534776
Experience 28, Iter 4, disc loss: 0.0006946916729069796, policy loss: 9.207941282686948
Experience 28, Iter 5, disc loss: 0.000829158924733773, policy loss: 8.881740408928101
Experience 28, Iter 6, disc loss: 0.000722995834317629, policy loss: 9.334561815771584
Experience 28, Iter 7, disc loss: 0.0007015543033261308, policy loss: 9.283222134488499
Experience 28, Iter 8, disc loss: 0.0007162217869078301, policy loss: 8.809561865581163
Experience 28, Iter 9, disc loss: 0.0007139763638612056, policy loss: 9.74170098091599
Experience 28, Iter 10, disc loss: 0.00061963776262393, policy loss: 10.210279503587255
Experience 28, Iter 11, disc loss: 0.0006159302157295505, policy loss: 9.706522120939614
Experience 28, Iter 12, disc loss: 0.000816468180741113, policy loss: 8.922697042585463
Experience 28, Iter 13, disc loss: 0.0007532606201426975, policy loss: 9.011249595785582
Experience 28, Iter 14, disc loss: 0.000640231301955512, policy loss: 10.180130716454578
Experience 28, Iter 15, disc loss: 0.0005774180600334074, policy loss: 10.310643592989535
Experience 28, Iter 16, disc loss: 0.0006583163437129519, policy loss: 9.887168639495453
Experience 28, Iter 17, disc loss: 0.000540074271974444, policy loss: 9.975682145479908
Experience 28, Iter 18, disc loss: 0.0007009569679839894, policy loss: 9.012614444147477
Experience 28, Iter 19, disc loss: 0.0006656730548053867, policy loss: 9.773588237498481
Experience 28, Iter 20, disc loss: 0.000825375494898086, policy loss: 8.70544845272574
Experience 28, Iter 21, disc loss: 0.0006908264298037997, policy loss: 9.362949486312983
Experience 28, Iter 22, disc loss: 0.0006792730346214829, policy loss: 9.806912388297281
Experience 28, Iter 23, disc loss: 0.0006013343376856095, policy loss: 9.689480446369659
Experience 28, Iter 24, disc loss: 0.0007047822747717387, policy loss: 9.48884625545504
Experience 28, Iter 25, disc loss: 0.0007556303681788583, policy loss: 9.217278353003518
Experience 28, Iter 26, disc loss: 0.0006480899463162388, policy loss: 9.246039933142205
Experience 28, Iter 27, disc loss: 0.0008297158043077972, policy loss: 8.948717989347362
Experience 28, Iter 28, disc loss: 0.0006559601317652367, policy loss: 9.750891166900022
Experience 28, Iter 29, disc loss: 0.0009066871260459543, policy loss: 9.121850580535073
Experience 28, Iter 30, disc loss: 0.0008917447290536737, policy loss: 8.901868624974712
Experience 28, Iter 31, disc loss: 0.0006233361826607769, policy loss: 9.438354778781047
Experience 28, Iter 32, disc loss: 0.0008440517275151152, policy loss: 9.255610211562114
Experience 28, Iter 33, disc loss: 0.0007355031155758701, policy loss: 9.146220385566128
Experience 28, Iter 34, disc loss: 0.0008394346007592041, policy loss: 8.862861577421402
Experience 28, Iter 35, disc loss: 0.0006923464876669547, policy loss: 9.411632642523951
Experience 28, Iter 36, disc loss: 0.0005795746352582505, policy loss: 9.869514715743371
Experience 28, Iter 37, disc loss: 0.0007709543553426409, policy loss: 9.467504800101507
Experience 28, Iter 38, disc loss: 0.0005773729822147537, policy loss: 9.64497311402645
Experience 28, Iter 39, disc loss: 0.0007629513150672017, policy loss: 9.026007094763031
Experience 28, Iter 40, disc loss: 0.000651974885388421, policy loss: 9.306056700596017
Experience 28, Iter 41, disc loss: 0.0006452614103138851, policy loss: 9.731641482665625
Experience 28, Iter 42, disc loss: 0.0007476507845458844, policy loss: 9.190249440931574
Experience 28, Iter 43, disc loss: 0.000683109185807583, policy loss: 9.37005779572743
Experience 28, Iter 44, disc loss: 0.0007143751705759357, policy loss: 9.567477260972364
Experience 28, Iter 45, disc loss: 0.0007674736823006778, policy loss: 9.078467214407866
Experience 28, Iter 46, disc loss: 0.0006214933910100168, policy loss: 10.023753275635947
Experience 28, Iter 47, disc loss: 0.0007718135863004345, policy loss: 9.28779491107031
Experience 28, Iter 48, disc loss: 0.0005748250329283633, policy loss: 9.753108001901346
Experience 28, Iter 49, disc loss: 0.0006189793247570708, policy loss: 9.889044732193206
Experience 28, Iter 50, disc loss: 0.0006467337451534164, policy loss: 9.994075052330729
Experience 28, Iter 51, disc loss: 0.0007009447167877879, policy loss: 9.612622138697219
Experience 28, Iter 52, disc loss: 0.0006851627597842039, policy loss: 9.229490258710115
Experience 28, Iter 53, disc loss: 0.0006090681975665674, policy loss: 9.472817722078226
Experience 28, Iter 54, disc loss: 0.0007995098496427486, policy loss: 9.07740313506669
Experience 28, Iter 55, disc loss: 0.0007171958697337813, policy loss: 8.912660996068116
Experience 28, Iter 56, disc loss: 0.0007286743951941562, policy loss: 9.231865631377115
Experience 28, Iter 57, disc loss: 0.000712159824681691, policy loss: 9.382571101718469
Experience 28, Iter 58, disc loss: 0.0006654793160371692, policy loss: 9.506904660233408
Experience 28, Iter 59, disc loss: 0.0007577716031998204, policy loss: 8.919550667367707
Experience 28, Iter 60, disc loss: 0.0007617356641206735, policy loss: 9.300336075136201
Experience 28, Iter 61, disc loss: 0.000796981712965956, policy loss: 9.09102316975575
Experience 28, Iter 62, disc loss: 0.0008705191726090275, policy loss: 9.282112423756097
Experience 28, Iter 63, disc loss: 0.0006503523794868603, policy loss: 9.176189120059574
Experience 28, Iter 64, disc loss: 0.0006748369154400786, policy loss: 9.061786836375978
Experience 28, Iter 65, disc loss: 0.0008044461258534725, policy loss: 9.226825337579289
Experience 28, Iter 66, disc loss: 0.0006895586205218195, policy loss: 9.362443686570924
Experience 28, Iter 67, disc loss: 0.0004387711947309788, policy loss: 19.551038772732063
Experience 28, Iter 68, disc loss: 0.00038211373590068653, policy loss: 12.568397208385647
Experience 28, Iter 69, disc loss: 0.0004763402937453178, policy loss: 11.108444794389698
Experience 28, Iter 70, disc loss: 0.0005718178736675664, policy loss: 9.46308528293534
Experience 28, Iter 71, disc loss: 0.0005866372631364936, policy loss: 8.980178809025563
Experience 28, Iter 72, disc loss: 0.0008558317029225547, policy loss: 8.746711893327237
Experience 28, Iter 73, disc loss: 0.0008338483997098334, policy loss: 9.046431798953058
Experience 28, Iter 74, disc loss: 0.0014380818455608825, policy loss: 7.821569839165523
Experience 28, Iter 75, disc loss: 0.0024710859396664757, policy loss: 8.501547751741107
Experience 28, Iter 76, disc loss: 0.0044463002282391475, policy loss: 6.946793967935264
Experience 28, Iter 77, disc loss: 0.026384947258590925, policy loss: 5.225429397535229
Experience 28, Iter 78, disc loss: 0.05201634296493255, policy loss: 5.515638262877021
Experience 28, Iter 79, disc loss: 0.07633097308354385, policy loss: 4.494874027226935
Experience 28, Iter 80, disc loss: 0.08618179200672083, policy loss: 3.9872566273326906
Experience 28, Iter 81, disc loss: 0.10218409206677787, policy loss: 3.94429097122018
Experience 28, Iter 82, disc loss: 0.09963192131906291, policy loss: 3.7535875311156275
Experience 28, Iter 83, disc loss: 0.07387897403005492, policy loss: 4.333776074272091
Experience 28, Iter 84, disc loss: 0.04532771729411526, policy loss: 7.639050911983624
Experience 28, Iter 85, disc loss: 0.0477527965789924, policy loss: 11.837659888733677
Experience 28, Iter 86, disc loss: 0.0298612575002197, policy loss: 12.940372853546265
Experience 28, Iter 87, disc loss: 0.10916320889037767, policy loss: 11.279172161201307
Experience 28, Iter 88, disc loss: 0.031818201855880954, policy loss: 11.212424817369042
Experience 28, Iter 89, disc loss: 0.1559873852426407, policy loss: 9.056947643259038
Experience 28, Iter 90, disc loss: 0.05133320227595248, policy loss: 10.144272613896176
Experience 28, Iter 91, disc loss: 0.05313912031271738, policy loss: 10.152245458197818
Experience 28, Iter 92, disc loss: 0.040563120264675664, policy loss: 10.413373523279349
Experience 28, Iter 93, disc loss: 0.02937476765531439, policy loss: 11.261694197351385
Experience 28, Iter 94, disc loss: 0.019056745371356668, policy loss: 11.644289590489512
Experience 28, Iter 95, disc loss: 0.01189672102880968, policy loss: 11.284466108714305
Experience 28, Iter 96, disc loss: 0.007015906253012212, policy loss: 11.362865784649648
Experience 28, Iter 97, disc loss: 0.07612647777490585, policy loss: 8.018890803254083
Experience 28, Iter 98, disc loss: 0.2821228562137529, policy loss: 6.302650613236322
Experience 28, Iter 99, disc loss: 0.17670871818967812, policy loss: 7.219347361484186
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2424],
        [2.0062],
        [0.0383]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0117, 0.1724, 1.7910, 0.0330, 0.0295, 6.0085]],

        [[0.0117, 0.1724, 1.7910, 0.0330, 0.0295, 6.0085]],

        [[0.0117, 0.1724, 1.7910, 0.0330, 0.0295, 6.0085]],

        [[0.0117, 0.1724, 1.7910, 0.0330, 0.0295, 6.0085]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.9695, 8.0247, 0.1531], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.9695, 8.0247, 0.1531])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.652
Iter 2/2000 - Loss: 3.620
Iter 3/2000 - Loss: 3.358
Iter 4/2000 - Loss: 3.282
Iter 5/2000 - Loss: 3.238
Iter 6/2000 - Loss: 3.087
Iter 7/2000 - Loss: 2.898
Iter 8/2000 - Loss: 2.730
Iter 9/2000 - Loss: 2.569
Iter 10/2000 - Loss: 2.374
Iter 11/2000 - Loss: 2.150
Iter 12/2000 - Loss: 1.921
Iter 13/2000 - Loss: 1.694
Iter 14/2000 - Loss: 1.459
Iter 15/2000 - Loss: 1.201
Iter 16/2000 - Loss: 0.918
Iter 17/2000 - Loss: 0.622
Iter 18/2000 - Loss: 0.322
Iter 19/2000 - Loss: 0.023
Iter 20/2000 - Loss: -0.276
Iter 1981/2000 - Loss: -7.882
Iter 1982/2000 - Loss: -7.882
Iter 1983/2000 - Loss: -7.882
Iter 1984/2000 - Loss: -7.882
Iter 1985/2000 - Loss: -7.882
Iter 1986/2000 - Loss: -7.882
Iter 1987/2000 - Loss: -7.882
Iter 1988/2000 - Loss: -7.882
Iter 1989/2000 - Loss: -7.882
Iter 1990/2000 - Loss: -7.882
Iter 1991/2000 - Loss: -7.882
Iter 1992/2000 - Loss: -7.882
Iter 1993/2000 - Loss: -7.882
Iter 1994/2000 - Loss: -7.882
Iter 1995/2000 - Loss: -7.882
Iter 1996/2000 - Loss: -7.882
Iter 1997/2000 - Loss: -7.882
Iter 1998/2000 - Loss: -7.882
Iter 1999/2000 - Loss: -7.882
Iter 2000/2000 - Loss: -7.882
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[10.4498,  6.3683, 23.6446,  1.9307, 17.2325, 38.0859]],

        [[16.2113, 31.2339,  7.2182,  1.1384,  1.6897, 33.7032]],

        [[16.7562, 29.3150,  8.4238,  0.9146,  0.6715, 27.4058]],

        [[12.8938, 23.5603, 11.4619,  1.5251,  1.7087, 48.5157]]])
Signal Variance: tensor([ 0.0646,  2.5244, 13.2307,  0.3167])
Estimated target variance: tensor([0.0159, 0.9695, 8.0247, 0.1531])
N: 290
Signal to noise ratio: tensor([14.7438, 84.4341, 75.4662, 31.1210])
Bound on condition number: tensor([  63040.9357, 2067444.1405, 1651593.5497,  280870.0311])
Policy Optimizer learning rate:
0.0009709299607098556
Experience 29, Iter 0, disc loss: 0.4577817823463236, policy loss: 5.931661264074771
Experience 29, Iter 1, disc loss: 0.29527230163675516, policy loss: 6.154869901527327
Experience 29, Iter 2, disc loss: 0.21690931239274266, policy loss: 6.662602207210345
Experience 29, Iter 3, disc loss: 0.16481666196422157, policy loss: 6.632233086407586
Experience 29, Iter 4, disc loss: 0.5328046621097762, policy loss: 5.50281189113319
Experience 29, Iter 5, disc loss: 0.9256824629691524, policy loss: 3.6153790808533586
Experience 29, Iter 6, disc loss: 0.48893995324473793, policy loss: 4.964239767598725
Experience 29, Iter 7, disc loss: 0.5123869744780456, policy loss: 5.581196503349668
Experience 29, Iter 8, disc loss: 0.31743219374370135, policy loss: 6.076556712520319
Experience 29, Iter 9, disc loss: 0.18979260338792636, policy loss: 7.763323917004537
Experience 29, Iter 10, disc loss: 0.10964365404291244, policy loss: 7.3222923823168
Experience 29, Iter 11, disc loss: 0.048194521598989064, policy loss: 7.268955059434264
Experience 29, Iter 12, disc loss: 0.02755943106160414, policy loss: 8.161394742234696
Experience 29, Iter 13, disc loss: 0.016573285862728007, policy loss: 8.529580172164106
Experience 29, Iter 14, disc loss: 0.019565980454726992, policy loss: 9.522581960074946
Experience 29, Iter 15, disc loss: 0.009464442773676435, policy loss: 10.059432701809566
Experience 29, Iter 16, disc loss: 0.029636526900169314, policy loss: 9.531385543963431
Experience 29, Iter 17, disc loss: 0.0033489186071816533, policy loss: 10.451863923584616
Experience 29, Iter 18, disc loss: 0.0036222920655007715, policy loss: 11.84845520055875
Experience 29, Iter 19, disc loss: 0.0014422658669567877, policy loss: 13.710359491791387
Experience 29, Iter 20, disc loss: 0.000801126818367007, policy loss: 14.268314636952919
Experience 29, Iter 21, disc loss: 0.000953211749956282, policy loss: 14.168519762215652
Experience 29, Iter 22, disc loss: 0.0006589064484024231, policy loss: 14.114487967487776
Experience 29, Iter 23, disc loss: 0.003656964628862473, policy loss: 17.927653120392055
Experience 29, Iter 24, disc loss: 0.0004070311147683973, policy loss: 20.186881702725586
Experience 29, Iter 25, disc loss: 0.0004074345554873063, policy loss: 20.786606374850045
Experience 29, Iter 26, disc loss: 0.00046011147699167134, policy loss: 19.70399132542565
Experience 29, Iter 27, disc loss: 0.0003206504398129608, policy loss: 16.00130689964233
Experience 29, Iter 28, disc loss: 0.0002795969524452617, policy loss: 17.738235712398968
Experience 29, Iter 29, disc loss: 0.0002948163776432324, policy loss: 18.08093842303076
Experience 29, Iter 30, disc loss: 0.0002474875269223427, policy loss: 17.95307246921012
Experience 29, Iter 31, disc loss: 0.00023032439207700388, policy loss: 18.433393219159043
Experience 29, Iter 32, disc loss: 0.00022314135124741353, policy loss: 18.069817488580238
Experience 29, Iter 33, disc loss: 0.00021045461579347532, policy loss: 19.058891202519046
Experience 29, Iter 34, disc loss: 0.00020172242930098679, policy loss: 19.39699387893886
Experience 29, Iter 35, disc loss: 0.00019543261651501232, policy loss: 18.964768292221756
Experience 29, Iter 36, disc loss: 0.00018936937991176727, policy loss: 19.76709983956271
Experience 29, Iter 37, disc loss: 0.0001841892748619021, policy loss: 19.85502350806108
Experience 29, Iter 38, disc loss: 0.00018161277552185325, policy loss: 18.861002790897683
Experience 29, Iter 39, disc loss: 0.000175985428546135, policy loss: 19.261002799683816
Experience 29, Iter 40, disc loss: 0.0001729760002881792, policy loss: 19.40048247936484
Experience 29, Iter 41, disc loss: 0.00017029798112139544, policy loss: 19.0397487005733
Experience 29, Iter 42, disc loss: 0.0001660043302927933, policy loss: 18.780907339376746
Experience 29, Iter 43, disc loss: 0.0001640674161040167, policy loss: 18.227191747557765
Experience 29, Iter 44, disc loss: 0.0001687273398041692, policy loss: 18.782820423347008
Experience 29, Iter 45, disc loss: 0.00016149099019180537, policy loss: 17.683182863376892
Experience 29, Iter 46, disc loss: 0.0002014490125667388, policy loss: 16.963363561770723
Experience 29, Iter 47, disc loss: 0.0001655394701080376, policy loss: 16.131967116772316
Experience 29, Iter 48, disc loss: 0.00017180089069529898, policy loss: 14.906336934354794
Experience 29, Iter 49, disc loss: 0.00019887786650317355, policy loss: 14.81210906684068
Experience 29, Iter 50, disc loss: 0.00019415596176133404, policy loss: 13.376289885449797
Experience 29, Iter 51, disc loss: 0.00020336272304630968, policy loss: 12.811666405883289
Experience 29, Iter 52, disc loss: 0.00024048183608792065, policy loss: 11.965209485941253
Experience 29, Iter 53, disc loss: 0.0002538361299092351, policy loss: 12.123326528044732
Experience 29, Iter 54, disc loss: 0.0004483409374781783, policy loss: 10.992750204096712
Experience 29, Iter 55, disc loss: 0.00991042483205243, policy loss: 10.878861201567904
Experience 29, Iter 56, disc loss: 0.0003912835889461425, policy loss: 10.310317222856131
Experience 29, Iter 57, disc loss: 0.0010250957275793346, policy loss: 9.307063411964204
Experience 29, Iter 58, disc loss: 0.0005837257618641226, policy loss: 9.83277143147355
Experience 29, Iter 59, disc loss: 0.001020873885256881, policy loss: 8.822317215255643
Experience 29, Iter 60, disc loss: 0.0009654521041465277, policy loss: 9.043923361561033
Experience 29, Iter 61, disc loss: 0.00197094224946609, policy loss: 9.224191569450277
Experience 29, Iter 62, disc loss: 0.0027025323456642937, policy loss: 8.232643125038248
Experience 29, Iter 63, disc loss: 0.005095167263670065, policy loss: 7.6767871290816
Experience 29, Iter 64, disc loss: 0.006834119042380362, policy loss: 6.966947446072222
Experience 29, Iter 65, disc loss: 0.0058941066649032415, policy loss: 6.9790813463753665
Experience 29, Iter 66, disc loss: 0.00986187680658139, policy loss: 6.34797113387946
Experience 29, Iter 67, disc loss: 0.01279290017562006, policy loss: 7.49931966195383
Experience 29, Iter 68, disc loss: 0.008892249927546657, policy loss: 6.808221460233167
Experience 29, Iter 69, disc loss: 0.012552528991968577, policy loss: 6.473091374293239
Experience 29, Iter 70, disc loss: 0.03465889492446645, policy loss: 6.008359334259524
Experience 29, Iter 71, disc loss: 0.025644899669527564, policy loss: 5.2741773893993775
Experience 29, Iter 72, disc loss: 0.0178342571054529, policy loss: 6.736294227482227
Experience 29, Iter 73, disc loss: 0.015634996665670444, policy loss: 6.837993566977834
Experience 29, Iter 74, disc loss: 0.053208037628207965, policy loss: 6.072418347140546
Experience 29, Iter 75, disc loss: 0.08132561625121534, policy loss: 6.128043839409339
Experience 29, Iter 76, disc loss: 0.14041732172831367, policy loss: 7.515468881850955
Experience 29, Iter 77, disc loss: 0.16690149495438908, policy loss: 8.173001135774262
Experience 29, Iter 78, disc loss: 0.3207885479364823, policy loss: 8.99405826631467
Experience 29, Iter 79, disc loss: 0.23410129871282875, policy loss: 10.205186880403023
Experience 29, Iter 80, disc loss: 0.3843950711227802, policy loss: 7.633437681269374
Experience 29, Iter 81, disc loss: 0.28122003609976853, policy loss: 5.504443659369668
Experience 29, Iter 82, disc loss: 0.015522755935438535, policy loss: 11.031134410288594
Experience 29, Iter 83, disc loss: 0.006369706581591089, policy loss: 14.993613125082922
Experience 29, Iter 84, disc loss: 0.00847395757769818, policy loss: 16.652556461275957
Experience 29, Iter 85, disc loss: 0.010669782309926188, policy loss: 18.58490004137952
Experience 29, Iter 86, disc loss: 0.012703921624733562, policy loss: 20.071443795010133
Experience 29, Iter 87, disc loss: 0.01429957144042744, policy loss: 21.343986632914124
Experience 29, Iter 88, disc loss: 0.01523596533257035, policy loss: 22.625263163966913
Experience 29, Iter 89, disc loss: 0.015413287060911145, policy loss: 22.999953713262663
Experience 29, Iter 90, disc loss: 0.014872050413227342, policy loss: 24.138772089114095
Experience 29, Iter 91, disc loss: 0.013770922806250602, policy loss: 24.897419767850337
Experience 29, Iter 92, disc loss: 0.012324788975891307, policy loss: 24.71039490377241
Experience 29, Iter 93, disc loss: 0.010744006410796236, policy loss: 25.939284321700587
Experience 29, Iter 94, disc loss: 0.009188862509572243, policy loss: 25.565429751441748
Experience 29, Iter 95, disc loss: 0.00776134943750472, policy loss: 25.774344762149386
Experience 29, Iter 96, disc loss: 0.006507365218487514, policy loss: 26.148369655865594
Experience 29, Iter 97, disc loss: 0.005440959673852219, policy loss: 25.83320277281318
Experience 29, Iter 98, disc loss: 0.004552711259276822, policy loss: 25.83880488058527
Experience 29, Iter 99, disc loss: 0.003822077853303145, policy loss: 25.227128834881782
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0048],
        [0.2436],
        [2.0096],
        [0.0380]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0179, 0.2067, 1.7741, 0.0329, 0.0288, 6.0992]],

        [[0.0179, 0.2067, 1.7741, 0.0329, 0.0288, 6.0992]],

        [[0.0179, 0.2067, 1.7741, 0.0329, 0.0288, 6.0992]],

        [[0.0179, 0.2067, 1.7741, 0.0329, 0.0288, 6.0992]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0192, 0.9745, 8.0382, 0.1521], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0192, 0.9745, 8.0382, 0.1521])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.648
Iter 2/2000 - Loss: 3.514
Iter 3/2000 - Loss: 3.298
Iter 4/2000 - Loss: 3.166
Iter 5/2000 - Loss: 3.075
Iter 6/2000 - Loss: 2.913
Iter 7/2000 - Loss: 2.719
Iter 8/2000 - Loss: 2.532
Iter 9/2000 - Loss: 2.334
Iter 10/2000 - Loss: 2.113
Iter 11/2000 - Loss: 1.883
Iter 12/2000 - Loss: 1.655
Iter 13/2000 - Loss: 1.421
Iter 14/2000 - Loss: 1.170
Iter 15/2000 - Loss: 0.900
Iter 16/2000 - Loss: 0.615
Iter 17/2000 - Loss: 0.324
Iter 18/2000 - Loss: 0.031
Iter 19/2000 - Loss: -0.264
Iter 20/2000 - Loss: -0.560
Iter 1981/2000 - Loss: -7.880
Iter 1982/2000 - Loss: -7.880
Iter 1983/2000 - Loss: -7.880
Iter 1984/2000 - Loss: -7.880
Iter 1985/2000 - Loss: -7.880
Iter 1986/2000 - Loss: -7.880
Iter 1987/2000 - Loss: -7.880
Iter 1988/2000 - Loss: -7.880
Iter 1989/2000 - Loss: -7.880
Iter 1990/2000 - Loss: -7.881
Iter 1991/2000 - Loss: -7.881
Iter 1992/2000 - Loss: -7.881
Iter 1993/2000 - Loss: -7.881
Iter 1994/2000 - Loss: -7.881
Iter 1995/2000 - Loss: -7.881
Iter 1996/2000 - Loss: -7.881
Iter 1997/2000 - Loss: -7.881
Iter 1998/2000 - Loss: -7.881
Iter 1999/2000 - Loss: -7.881
Iter 2000/2000 - Loss: -7.881
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.5760,  7.6983, 20.9663,  2.0520, 15.9873, 49.5073]],

        [[18.4626, 33.5683,  7.2720,  1.1411,  1.7244, 34.7016]],

        [[18.2137, 30.0616,  8.6656,  0.9164,  0.6594, 28.0095]],

        [[14.0894, 26.4505, 11.0079,  1.4729,  1.5918, 49.1022]]])
Signal Variance: tensor([ 0.0876,  2.6133, 13.7475,  0.2934])
Estimated target variance: tensor([0.0192, 0.9745, 8.0382, 0.1521])
N: 300
Signal to noise ratio: tensor([16.9849, 85.8897, 77.1352, 30.2290])
Bound on condition number: tensor([  86546.9948, 2213114.0776, 1784952.2058,  274137.9550])
Policy Optimizer learning rate:
0.0009699075226141829
Experience 30, Iter 0, disc loss: 0.0032253956061128438, policy loss: 25.838139687139982
Experience 30, Iter 1, disc loss: 0.0027396667422485725, policy loss: 25.06663875382653
Experience 30, Iter 2, disc loss: 0.002344546783191483, policy loss: 24.735333941467147
Experience 30, Iter 3, disc loss: 0.002022548662679273, policy loss: 25.264865406349312
Experience 30, Iter 4, disc loss: 0.0017601651441753225, policy loss: 24.588926536606486
Experience 30, Iter 5, disc loss: 0.0015458661328153047, policy loss: 24.044451563496978
Experience 30, Iter 6, disc loss: 0.001370175820857978, policy loss: 23.936404282745862
Experience 30, Iter 7, disc loss: 0.0012250785663834652, policy loss: 23.11450084777205
Experience 30, Iter 8, disc loss: 0.0011050439690920182, policy loss: 21.87492131710721
Experience 30, Iter 9, disc loss: 0.0010048747616521049, policy loss: 20.962267207626287
Experience 30, Iter 10, disc loss: 0.0009205005336960036, policy loss: 20.663559450008567
Experience 30, Iter 11, disc loss: 0.0008485216458840103, policy loss: 19.4854018057083
Experience 30, Iter 12, disc loss: 0.000786758775466506, policy loss: 20.21525362528379
Experience 30, Iter 13, disc loss: 0.0007339018635755938, policy loss: 19.67110711440442
Experience 30, Iter 14, disc loss: 0.0006880506934806204, policy loss: 19.811047065656943
Experience 30, Iter 15, disc loss: 0.000648520827811661, policy loss: 20.104147760121766
Experience 30, Iter 16, disc loss: 0.0006142177429304276, policy loss: 20.373804131508717
Experience 30, Iter 17, disc loss: 0.0005840098605140795, policy loss: 19.17751203962125
Experience 30, Iter 18, disc loss: 0.0005572136764612818, policy loss: 19.78765829836312
Experience 30, Iter 19, disc loss: 0.0005335940659561669, policy loss: 20.254609414045518
Experience 30, Iter 20, disc loss: 0.0005123492379022668, policy loss: 20.326225214026216
Experience 30, Iter 21, disc loss: 0.0004936260842795084, policy loss: 19.733886916397367
Experience 30, Iter 22, disc loss: 0.0004769482171870629, policy loss: 20.121785562478454
Experience 30, Iter 23, disc loss: 0.00046218855772183534, policy loss: 20.420068868883536
Experience 30, Iter 24, disc loss: 0.00044869690729781907, policy loss: 20.862668705474007
Experience 30, Iter 25, disc loss: 0.0004366771126786245, policy loss: 20.91672126510889
Experience 30, Iter 26, disc loss: 0.0004262135874757686, policy loss: 20.59109313054073
Experience 30, Iter 27, disc loss: 0.0004153038340618617, policy loss: 21.00870246997935
Experience 30, Iter 28, disc loss: 0.0004059628288723005, policy loss: 20.81991447910771
Experience 30, Iter 29, disc loss: 0.0003973934597379785, policy loss: 20.45815025889808
Experience 30, Iter 30, disc loss: 0.0003894040074079478, policy loss: 20.250006931270523
Experience 30, Iter 31, disc loss: 0.0003820021137399175, policy loss: 20.402799503824127
Experience 30, Iter 32, disc loss: 0.0003751699889717849, policy loss: 20.65313695737177
Experience 30, Iter 33, disc loss: 0.0003688852462314687, policy loss: 20.671895708853512
Experience 30, Iter 34, disc loss: 0.0003629381979370557, policy loss: 20.043196214412816
Experience 30, Iter 35, disc loss: 0.00035751112149731533, policy loss: 20.479036296615774
Experience 30, Iter 36, disc loss: 0.0003523241462846546, policy loss: 19.88423669399115
Experience 30, Iter 37, disc loss: 0.0003474587413206997, policy loss: 20.193029674515728
Experience 30, Iter 38, disc loss: 0.0003429229192656658, policy loss: 20.201072018406038
Experience 30, Iter 39, disc loss: 0.0003385237810571146, policy loss: 20.60202665930742
Experience 30, Iter 40, disc loss: 0.0003344238150879528, policy loss: 19.895019593132904
Experience 30, Iter 41, disc loss: 0.000330537019271224, policy loss: 20.11815952696706
Experience 30, Iter 42, disc loss: 0.0003268073393531658, policy loss: 19.581593047203143
Experience 30, Iter 43, disc loss: 0.0003234615174497057, policy loss: 20.356634504842415
Experience 30, Iter 44, disc loss: 0.0003200197946744604, policy loss: 19.051407297065403
Experience 30, Iter 45, disc loss: 0.0003166725642362127, policy loss: 19.708090492233392
Experience 30, Iter 46, disc loss: 0.00031357185296676185, policy loss: 19.74128134079231
Experience 30, Iter 47, disc loss: 0.00031077098661723613, policy loss: 18.88837140737063
Experience 30, Iter 48, disc loss: 0.00030772725232874805, policy loss: 19.108545642492025
Experience 30, Iter 49, disc loss: 0.0003048716707461578, policy loss: 19.50722713433646
Experience 30, Iter 50, disc loss: 0.0003021381322405794, policy loss: 19.55877699047755
Experience 30, Iter 51, disc loss: 0.00029958721813235916, policy loss: 19.78084150697437
Experience 30, Iter 52, disc loss: 0.0002972604165534032, policy loss: 18.5248666821665
Experience 30, Iter 53, disc loss: 0.0002946957668670984, policy loss: 18.374004654571152
Experience 30, Iter 54, disc loss: 0.00029223092316294077, policy loss: 18.34541476005189
Experience 30, Iter 55, disc loss: 0.00028998752871521485, policy loss: 18.259189416596463
Experience 30, Iter 56, disc loss: 0.0002877203015044092, policy loss: 18.882574674304124
Experience 30, Iter 57, disc loss: 0.00028548426389896265, policy loss: 18.167646208579164
Experience 30, Iter 58, disc loss: 0.00028329079169901674, policy loss: 18.611452101675333
Experience 30, Iter 59, disc loss: 0.0002812860471051116, policy loss: 18.2410121521037
Experience 30, Iter 60, disc loss: 0.000279292045654266, policy loss: 17.907111492476993
Experience 30, Iter 61, disc loss: 0.00027719569677144836, policy loss: 18.618820297652974
Experience 30, Iter 62, disc loss: 0.0002754547910856524, policy loss: 17.071216825688857
Experience 30, Iter 63, disc loss: 0.0002735401013972203, policy loss: 18.262786662981576
Experience 30, Iter 64, disc loss: 0.00027152081016607423, policy loss: 17.49674129443613
Experience 30, Iter 65, disc loss: 0.0002698447074778457, policy loss: 18.362935510715225
Experience 30, Iter 66, disc loss: 0.0002681360703348184, policy loss: 17.354287889595813
Experience 30, Iter 67, disc loss: 0.0002665470640734776, policy loss: 17.704216837828227
Experience 30, Iter 68, disc loss: 0.00026448099791096587, policy loss: 17.687935402120413
Experience 30, Iter 69, disc loss: 0.00026272321521125363, policy loss: 17.26676793660438
Experience 30, Iter 70, disc loss: 0.00026109213697338103, policy loss: 17.33242612632529
Experience 30, Iter 71, disc loss: 0.00025930908112607634, policy loss: 17.41662025105334
Experience 30, Iter 72, disc loss: 0.00025828733909590056, policy loss: 17.14811887667757
Experience 30, Iter 73, disc loss: 0.0002561265590330652, policy loss: 17.601528181288174
Experience 30, Iter 74, disc loss: 0.0002553536618590652, policy loss: 17.370936620217925
Experience 30, Iter 75, disc loss: 0.0002539266312754644, policy loss: 17.467429642873306
Experience 30, Iter 76, disc loss: 0.00025171652301406573, policy loss: 17.082003830367697
Experience 30, Iter 77, disc loss: 0.00024996642678557575, policy loss: 17.430073037275328
Experience 30, Iter 78, disc loss: 0.0002487873584479618, policy loss: 18.1375209364709
Experience 30, Iter 79, disc loss: 0.0002478169364279716, policy loss: 16.65397565941067
Experience 30, Iter 80, disc loss: 0.0002475046157117058, policy loss: 16.97179096926788
Experience 30, Iter 81, disc loss: 0.0002442846075329358, policy loss: 17.47513193375025
Experience 30, Iter 82, disc loss: 0.0002430836084215681, policy loss: 16.862470945244045
Experience 30, Iter 83, disc loss: 0.00024228427447364295, policy loss: 16.361162300313254
Experience 30, Iter 84, disc loss: 0.00024037108931554245, policy loss: 16.77978774113469
Experience 30, Iter 85, disc loss: 0.0002399025028269062, policy loss: 16.455948889937364
Experience 30, Iter 86, disc loss: 0.00023975316212553069, policy loss: 16.476630501933617
Experience 30, Iter 87, disc loss: 0.00023786990014599792, policy loss: 17.09331085824191
Experience 30, Iter 88, disc loss: 0.00023567186010059766, policy loss: 16.442264666034113
Experience 30, Iter 89, disc loss: 0.00023373975535344514, policy loss: 17.355104169123173
Experience 30, Iter 90, disc loss: 0.00023355525549428213, policy loss: 16.348590106130438
Experience 30, Iter 91, disc loss: 0.00023184799121326292, policy loss: 16.651573757719508
Experience 30, Iter 92, disc loss: 0.00023027024261533605, policy loss: 16.66918998503556
Experience 30, Iter 93, disc loss: 0.0002318622819607057, policy loss: 15.298288821932221
Experience 30, Iter 94, disc loss: 0.0002275370348569546, policy loss: 16.649035495117644
Experience 30, Iter 95, disc loss: 0.00022716785980147333, policy loss: 16.099318195683754
Experience 30, Iter 96, disc loss: 0.0002264950574862095, policy loss: 15.80544374493082
Experience 30, Iter 97, disc loss: 0.00022645288042748396, policy loss: 15.615180756161898
Experience 30, Iter 98, disc loss: 0.00022643868460223734, policy loss: 15.405457349539253
Experience 30, Iter 99, disc loss: 0.00022515198640281282, policy loss: 15.011287863119504
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.2420],
        [1.9929],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0228, 0.2215, 1.7876, 0.0327, 0.0281, 6.1202]],

        [[0.0228, 0.2215, 1.7876, 0.0327, 0.0281, 6.1202]],

        [[0.0228, 0.2215, 1.7876, 0.0327, 0.0281, 6.1202]],

        [[0.0228, 0.2215, 1.7876, 0.0327, 0.0281, 6.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0205, 0.9680, 7.9715, 0.1551], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0205, 0.9680, 7.9715, 0.1551])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.614
Iter 2/2000 - Loss: 3.435
Iter 3/2000 - Loss: 3.247
Iter 4/2000 - Loss: 3.088
Iter 5/2000 - Loss: 2.979
Iter 6/2000 - Loss: 2.819
Iter 7/2000 - Loss: 2.614
Iter 8/2000 - Loss: 2.410
Iter 9/2000 - Loss: 2.204
Iter 10/2000 - Loss: 1.982
Iter 11/2000 - Loss: 1.749
Iter 12/2000 - Loss: 1.514
Iter 13/2000 - Loss: 1.273
Iter 14/2000 - Loss: 1.019
Iter 15/2000 - Loss: 0.749
Iter 16/2000 - Loss: 0.468
Iter 17/2000 - Loss: 0.181
Iter 18/2000 - Loss: -0.105
Iter 19/2000 - Loss: -0.391
Iter 20/2000 - Loss: -0.676
Iter 1981/2000 - Loss: -7.856
Iter 1982/2000 - Loss: -7.856
Iter 1983/2000 - Loss: -7.856
Iter 1984/2000 - Loss: -7.856
Iter 1985/2000 - Loss: -7.856
Iter 1986/2000 - Loss: -7.856
Iter 1987/2000 - Loss: -7.856
Iter 1988/2000 - Loss: -7.856
Iter 1989/2000 - Loss: -7.856
Iter 1990/2000 - Loss: -7.856
Iter 1991/2000 - Loss: -7.856
Iter 1992/2000 - Loss: -7.856
Iter 1993/2000 - Loss: -7.856
Iter 1994/2000 - Loss: -7.856
Iter 1995/2000 - Loss: -7.856
Iter 1996/2000 - Loss: -7.856
Iter 1997/2000 - Loss: -7.856
Iter 1998/2000 - Loss: -7.856
Iter 1999/2000 - Loss: -7.856
Iter 2000/2000 - Loss: -7.856
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[12.0712,  9.3300, 19.9543,  2.3269, 15.2330, 49.8369]],

        [[19.3771, 33.3853,  7.4980,  1.1536,  1.7869, 35.1078]],

        [[18.6782, 31.4773,  8.1913,  0.9380,  0.6751, 26.7151]],

        [[14.2470, 25.8758, 12.1952,  1.5138,  1.5394, 50.9715]]])
Signal Variance: tensor([ 0.1160,  2.7665, 12.5892,  0.3420])
Estimated target variance: tensor([0.0205, 0.9680, 7.9715, 0.1551])
N: 310
Signal to noise ratio: tensor([19.3627, 86.6429, 74.4701, 32.4557])
Bound on condition number: tensor([ 116224.4657, 2327169.9696, 1719199.8282,  326547.2854])
Policy Optimizer learning rate:
0.0009688861611972634
Experience 31, Iter 0, disc loss: 0.00022231605024601326, policy loss: 15.272819584234451
Experience 31, Iter 1, disc loss: 0.00022068056494130368, policy loss: 15.042293609192617
Experience 31, Iter 2, disc loss: 0.00022066852583947682, policy loss: 15.261253604565523
Experience 31, Iter 3, disc loss: 0.00021968661612262167, policy loss: 15.21348495683068
Experience 31, Iter 4, disc loss: 0.00022034673993345558, policy loss: 14.326597811735853
Experience 31, Iter 5, disc loss: 0.00021772362802751835, policy loss: 14.974956987535034
Experience 31, Iter 6, disc loss: 0.0002184401101476239, policy loss: 14.276421021308503
Experience 31, Iter 7, disc loss: 0.0002203052403803132, policy loss: 13.164286550117772
Experience 31, Iter 8, disc loss: 0.0002178713936547883, policy loss: 13.514514677724737
Experience 31, Iter 9, disc loss: 0.00021890477028477278, policy loss: 13.128518962657555
Experience 31, Iter 10, disc loss: 0.00022276106188214581, policy loss: 13.252649137198873
Experience 31, Iter 11, disc loss: 0.00022297153239676237, policy loss: 12.230885736471308
Experience 31, Iter 12, disc loss: 0.0002296624845058766, policy loss: 12.474233058675214
Experience 31, Iter 13, disc loss: 0.00023513005625519195, policy loss: 11.214022144682618
Experience 31, Iter 14, disc loss: 0.0002324436518501496, policy loss: 11.881409714543214
Experience 31, Iter 15, disc loss: 0.0003013681509912938, policy loss: 10.434755274513883
Experience 31, Iter 16, disc loss: 0.0004575081343074782, policy loss: 10.82255005854099
Experience 31, Iter 17, disc loss: 0.0003389177040987675, policy loss: 10.173936085047789
Experience 31, Iter 18, disc loss: 0.0004279291324969952, policy loss: 9.619398933441971
Experience 31, Iter 19, disc loss: 0.00046390531447502737, policy loss: 9.684816043543549
Experience 31, Iter 20, disc loss: 0.0011485869626669946, policy loss: 8.760054509888707
Experience 31, Iter 21, disc loss: 0.0007648994206598343, policy loss: 8.791222782109049
Experience 31, Iter 22, disc loss: 0.0007148681668724976, policy loss: 8.765260561541709
Experience 31, Iter 23, disc loss: 0.0009729030557050163, policy loss: 8.390477469894746
Experience 31, Iter 24, disc loss: 0.0014486016798850659, policy loss: 7.752752359186103
Experience 31, Iter 25, disc loss: 0.001486386209328382, policy loss: 7.706870582070855
Experience 31, Iter 26, disc loss: 0.002495416552920752, policy loss: 7.317334322030327
Experience 31, Iter 27, disc loss: 0.0018028516164163208, policy loss: 7.961723534346495
Experience 31, Iter 28, disc loss: 0.0017533095259856509, policy loss: 7.618337828924453
Experience 31, Iter 29, disc loss: 0.0028551292991619336, policy loss: 8.627778443632508
Experience 31, Iter 30, disc loss: 0.002128276624262981, policy loss: 10.13697446342955
Experience 31, Iter 31, disc loss: 0.004831125862460828, policy loss: 9.319504903132936
Experience 31, Iter 32, disc loss: 0.00207894490686744, policy loss: 8.630388486811054
Experience 31, Iter 33, disc loss: 0.002654922893347071, policy loss: 9.641185940739852
Experience 31, Iter 34, disc loss: 0.0016404537677032628, policy loss: 8.088339876109616
Experience 31, Iter 35, disc loss: 0.002104694864672421, policy loss: 7.704326161865273
Experience 31, Iter 36, disc loss: 0.0010228366912589713, policy loss: 8.321649134866128
Experience 31, Iter 37, disc loss: 0.0012962096166006512, policy loss: 7.74201985539308
Experience 31, Iter 38, disc loss: 0.0011989265963210337, policy loss: 8.204924032998054
Experience 31, Iter 39, disc loss: 0.0009529252104303361, policy loss: 8.16262455063835
Experience 31, Iter 40, disc loss: 0.0009706483474494152, policy loss: 8.243704209099473
Experience 31, Iter 41, disc loss: 0.0006769456830285744, policy loss: 8.57022570337878
Experience 31, Iter 42, disc loss: 0.000666426811341815, policy loss: 8.546705611925042
Experience 31, Iter 43, disc loss: 0.0009090171111099964, policy loss: 8.102929108769203
Experience 31, Iter 44, disc loss: 0.0006647670210253621, policy loss: 8.847124643743838
Experience 31, Iter 45, disc loss: 0.0007617330029130087, policy loss: 8.079936899924606
Experience 31, Iter 46, disc loss: 0.0009777228577088314, policy loss: 8.121920705579324
Experience 31, Iter 47, disc loss: 0.0007500065091841748, policy loss: 8.422244924364598
Experience 31, Iter 48, disc loss: 0.0010756902803699072, policy loss: 8.122794035387498
Experience 31, Iter 49, disc loss: 0.0010012490112572053, policy loss: 7.97312500448384
Experience 31, Iter 50, disc loss: 0.0014556068629642564, policy loss: 7.733707463021103
Experience 31, Iter 51, disc loss: 0.0011527751173304648, policy loss: 8.11644193044852
Experience 31, Iter 52, disc loss: 0.0012878435518159446, policy loss: 7.7112272660849825
Experience 31, Iter 53, disc loss: 0.000929182541609082, policy loss: 7.909795500026353
Experience 31, Iter 54, disc loss: 0.0010247128014994756, policy loss: 7.892883520207835
Experience 31, Iter 55, disc loss: 0.0020015370960651613, policy loss: 7.460129166723304
Experience 31, Iter 56, disc loss: 0.0011842586442743243, policy loss: 7.693823402280532
Experience 31, Iter 57, disc loss: 0.0018219503644155877, policy loss: 7.677406057949737
Experience 31, Iter 58, disc loss: 0.0015624853612433921, policy loss: 7.516622575979848
Experience 31, Iter 59, disc loss: 0.003031869441917865, policy loss: 7.168536103704985
Experience 31, Iter 60, disc loss: 0.002924340903332866, policy loss: 7.262042816346375
Experience 31, Iter 61, disc loss: 0.0035794265828870066, policy loss: 6.898090295445568
Experience 31, Iter 62, disc loss: 0.002118109987521521, policy loss: 7.551749392344936
Experience 31, Iter 63, disc loss: 0.002015243107228545, policy loss: 7.299458904372312
Experience 31, Iter 64, disc loss: 0.002438393930192616, policy loss: 7.341652937645522
Experience 31, Iter 65, disc loss: 0.002870420764985463, policy loss: 7.115417891962642
Experience 31, Iter 66, disc loss: 0.0026477960347940743, policy loss: 6.917427272323224
Experience 31, Iter 67, disc loss: 0.005244320077390939, policy loss: 6.903541440296508
Experience 31, Iter 68, disc loss: 0.0025786376301221766, policy loss: 7.17730336109325
Experience 31, Iter 69, disc loss: 0.001855730631390753, policy loss: 7.432716469017423
Experience 31, Iter 70, disc loss: 0.0025433943180827646, policy loss: 7.236041275363727
Experience 31, Iter 71, disc loss: 0.002474769970961853, policy loss: 7.215863762409607
Experience 31, Iter 72, disc loss: 0.003746630981935193, policy loss: 6.87184883920291
Experience 31, Iter 73, disc loss: 0.002174140006947834, policy loss: 7.279864998940717
Experience 31, Iter 74, disc loss: 0.003924951430974815, policy loss: 6.877615960716875
Experience 31, Iter 75, disc loss: 0.0026981974750927415, policy loss: 7.002142071897903
Experience 31, Iter 76, disc loss: 0.002630678767507626, policy loss: 7.295618012074544
Experience 31, Iter 77, disc loss: 0.0017175322179216613, policy loss: 7.5955345645858365
Experience 31, Iter 78, disc loss: 0.0025832737817925566, policy loss: 6.972016790939481
Experience 31, Iter 79, disc loss: 0.002575584134423815, policy loss: 7.449245035859114
Experience 31, Iter 80, disc loss: 0.002714089812658203, policy loss: 8.17076543983541
Experience 31, Iter 81, disc loss: 0.0027005490254149835, policy loss: 7.25763009257161
Experience 31, Iter 82, disc loss: 0.0030700397376592385, policy loss: 7.325349229841094
Experience 31, Iter 83, disc loss: 0.002310721809424002, policy loss: 8.139870103053624
Experience 31, Iter 84, disc loss: 0.0024124137192058403, policy loss: 6.965769002437668
Experience 31, Iter 85, disc loss: 0.0030990893064113856, policy loss: 6.698136129341746
Experience 31, Iter 86, disc loss: 0.0018080838417652081, policy loss: 7.3451204872601235
Experience 31, Iter 87, disc loss: 0.0019541319611112, policy loss: 7.430424181320941
Experience 31, Iter 88, disc loss: 0.001571168806000698, policy loss: 7.812941467874192
Experience 31, Iter 89, disc loss: 0.0013109771838152773, policy loss: 7.925313248944322
Experience 31, Iter 90, disc loss: 0.0018003690438016617, policy loss: 7.285603014888284
Experience 31, Iter 91, disc loss: 0.0009064371243827756, policy loss: 7.915592182277668
Experience 31, Iter 92, disc loss: 0.0012627926173986199, policy loss: 8.000010563732172
Experience 31, Iter 93, disc loss: 0.0008879368269782647, policy loss: 8.169635736102776
Experience 31, Iter 94, disc loss: 0.0011352386040575009, policy loss: 8.08111045404105
Experience 31, Iter 95, disc loss: 0.0014197005340579579, policy loss: 7.687347755572627
Experience 31, Iter 96, disc loss: 0.0016538037036734506, policy loss: 7.802851681250631
Experience 31, Iter 97, disc loss: 0.0016017565724166256, policy loss: 7.826512828922342
Experience 31, Iter 98, disc loss: 0.0017022495664858802, policy loss: 7.409526279829467
Experience 31, Iter 99, disc loss: 0.0014607645905676569, policy loss: 7.414268236210258
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0052],
        [0.2401],
        [1.9664],
        [0.0393]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0254, 0.2269, 1.7970, 0.0323, 0.0273, 6.1428]],

        [[0.0254, 0.2269, 1.7970, 0.0323, 0.0273, 6.1428]],

        [[0.0254, 0.2269, 1.7970, 0.0323, 0.0273, 6.1428]],

        [[0.0254, 0.2269, 1.7970, 0.0323, 0.0273, 6.1428]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0210, 0.9603, 7.8658, 0.1572], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0210, 0.9603, 7.8658, 0.1572])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.595
Iter 2/2000 - Loss: 3.395
Iter 3/2000 - Loss: 3.234
Iter 4/2000 - Loss: 3.067
Iter 5/2000 - Loss: 2.946
Iter 6/2000 - Loss: 2.794
Iter 7/2000 - Loss: 2.589
Iter 8/2000 - Loss: 2.377
Iter 9/2000 - Loss: 2.174
Iter 10/2000 - Loss: 1.962
Iter 11/2000 - Loss: 1.735
Iter 12/2000 - Loss: 1.499
Iter 13/2000 - Loss: 1.258
Iter 14/2000 - Loss: 1.006
Iter 15/2000 - Loss: 0.742
Iter 16/2000 - Loss: 0.466
Iter 17/2000 - Loss: 0.183
Iter 18/2000 - Loss: -0.100
Iter 19/2000 - Loss: -0.383
Iter 20/2000 - Loss: -0.665
Iter 1981/2000 - Loss: -7.866
Iter 1982/2000 - Loss: -7.866
Iter 1983/2000 - Loss: -7.866
Iter 1984/2000 - Loss: -7.866
Iter 1985/2000 - Loss: -7.866
Iter 1986/2000 - Loss: -7.866
Iter 1987/2000 - Loss: -7.866
Iter 1988/2000 - Loss: -7.867
Iter 1989/2000 - Loss: -7.867
Iter 1990/2000 - Loss: -7.867
Iter 1991/2000 - Loss: -7.867
Iter 1992/2000 - Loss: -7.867
Iter 1993/2000 - Loss: -7.867
Iter 1994/2000 - Loss: -7.867
Iter 1995/2000 - Loss: -7.867
Iter 1996/2000 - Loss: -7.867
Iter 1997/2000 - Loss: -7.867
Iter 1998/2000 - Loss: -7.867
Iter 1999/2000 - Loss: -7.867
Iter 2000/2000 - Loss: -7.867
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[12.1105,  9.5101, 20.5589,  2.2703, 15.0142, 48.0131]],

        [[19.5521, 33.9191,  6.9011,  1.1793,  1.8803, 36.0785]],

        [[18.7511, 31.9919,  7.9347,  0.9015,  0.6940, 26.1618]],

        [[14.7388, 26.3929, 12.1166,  1.5021,  1.5830, 51.3399]]])
Signal Variance: tensor([ 0.1220,  2.6423, 11.4156,  0.3430])
Estimated target variance: tensor([0.0210, 0.9603, 7.8658, 0.1572])
N: 320
Signal to noise ratio: tensor([20.0724, 84.7416, 70.0016, 32.5012])
Bound on condition number: tensor([ 128928.7701, 2297964.2476, 1568074.6250,  338026.8517])
Policy Optimizer learning rate:
0.0009678658753253001
Experience 32, Iter 0, disc loss: 0.0021331524581545084, policy loss: 7.313711406493919
Experience 32, Iter 1, disc loss: 0.0024544786927072236, policy loss: 7.195780365310969
Experience 32, Iter 2, disc loss: 0.0019516562726295647, policy loss: 7.248425251879064
Experience 32, Iter 3, disc loss: 0.0023714241100841514, policy loss: 7.1407418286728275
Experience 32, Iter 4, disc loss: 0.002203849703590753, policy loss: 7.097881775942774
Experience 32, Iter 5, disc loss: 0.0023972827708554707, policy loss: 7.140062854074166
Experience 32, Iter 6, disc loss: 0.0024709415292647934, policy loss: 7.062275535517859
Experience 32, Iter 7, disc loss: 0.0020020265310922436, policy loss: 7.187341675982084
Experience 32, Iter 8, disc loss: 0.0027517589959694614, policy loss: 6.92543625563221
Experience 32, Iter 9, disc loss: 0.0024269309552039124, policy loss: 7.3408643378498155
Experience 32, Iter 10, disc loss: 0.0028227464981465636, policy loss: 6.821834395549848
Experience 32, Iter 11, disc loss: 0.0024815788306737623, policy loss: 7.034920369808218
Experience 32, Iter 12, disc loss: 0.0024784728786666284, policy loss: 6.9346512634607285
Experience 32, Iter 13, disc loss: 0.00239937223519893, policy loss: 7.463320202665335
Experience 32, Iter 14, disc loss: 0.001963555136211619, policy loss: 7.313950130449058
Experience 32, Iter 15, disc loss: 0.002314596622465797, policy loss: 7.386319420702705
Experience 32, Iter 16, disc loss: 0.002594917942855963, policy loss: 6.7727202966445255
Experience 32, Iter 17, disc loss: 0.0019911136135441314, policy loss: 7.269162663446265
Experience 32, Iter 18, disc loss: 0.0017775871214070075, policy loss: 8.195769891944305
Experience 32, Iter 19, disc loss: 0.002576457731093666, policy loss: 7.276711429607575
Experience 32, Iter 20, disc loss: 0.0019520801624412143, policy loss: 7.491558792163611
Experience 32, Iter 21, disc loss: 0.0019748888387401695, policy loss: 7.064326744420951
Experience 32, Iter 22, disc loss: 0.001651140077973141, policy loss: 7.91371857713664
Experience 32, Iter 23, disc loss: 0.0021437836547216456, policy loss: 7.490044675349966
Experience 32, Iter 24, disc loss: 0.0016634606708773516, policy loss: 8.177983323992782
Experience 32, Iter 25, disc loss: 0.0013226490983103652, policy loss: 7.714723128802442
Experience 32, Iter 26, disc loss: 0.0013232890950460575, policy loss: 7.689889676846221
Experience 32, Iter 27, disc loss: 0.002081356601098717, policy loss: 7.357233687690197
Experience 32, Iter 28, disc loss: 0.001309635410029184, policy loss: 7.747394460048653
Experience 32, Iter 29, disc loss: 0.001030379170017056, policy loss: 7.641707477587962
Experience 32, Iter 30, disc loss: 0.0013212637525424917, policy loss: 8.017550710179876
Experience 32, Iter 31, disc loss: 0.0013947261688763777, policy loss: 7.66197768161676
Experience 32, Iter 32, disc loss: 0.0013548332325396773, policy loss: 7.4656754590788115
Experience 32, Iter 33, disc loss: 0.0010191409755706247, policy loss: 7.68847594483524
Experience 32, Iter 34, disc loss: 0.0016391867465320813, policy loss: 7.404254262563493
Experience 32, Iter 35, disc loss: 0.0012832216104998314, policy loss: 7.537545748561791
Experience 32, Iter 36, disc loss: 0.0010639934853168241, policy loss: 7.7215212201673395
Experience 32, Iter 37, disc loss: 0.001302108258382149, policy loss: 7.547604808546996
Experience 32, Iter 38, disc loss: 0.0008995400437302914, policy loss: 7.950539194873897
Experience 32, Iter 39, disc loss: 0.0012900442250650308, policy loss: 7.373950930082814
Experience 32, Iter 40, disc loss: 0.00101965567746357, policy loss: 7.6599297198087095
Experience 32, Iter 41, disc loss: 0.0012974743564365193, policy loss: 7.788819024371543
Experience 32, Iter 42, disc loss: 0.000846264480459932, policy loss: 7.9208974502014255
Experience 32, Iter 43, disc loss: 0.0012172524936475226, policy loss: 7.792113678163297
Experience 32, Iter 44, disc loss: 0.0011802921582123864, policy loss: 7.691341603822674
Experience 32, Iter 45, disc loss: 0.00118298762487176, policy loss: 8.200165067105544
Experience 32, Iter 46, disc loss: 0.0011107431605617993, policy loss: 7.8700739464129015
Experience 32, Iter 47, disc loss: 0.0012422003528836164, policy loss: 7.775366705428486
Experience 32, Iter 48, disc loss: 0.0011531587414707147, policy loss: 7.572238481577468
Experience 32, Iter 49, disc loss: 0.0009911644169822678, policy loss: 7.542613647417811
Experience 32, Iter 50, disc loss: 0.0010830746649135596, policy loss: 7.627491751689732
Experience 32, Iter 51, disc loss: 0.0012365587722913895, policy loss: 7.748483276921102
Experience 32, Iter 52, disc loss: 0.0011684870723274246, policy loss: 7.628143319169188
Experience 32, Iter 53, disc loss: 0.0009316800023359354, policy loss: 7.88517882980752
Experience 32, Iter 54, disc loss: 0.0012698522055847425, policy loss: 7.620340885667746
Experience 32, Iter 55, disc loss: 0.0012305800864244381, policy loss: 7.61928194764727
Experience 32, Iter 56, disc loss: 0.0009297601912481929, policy loss: 7.914997881211397
Experience 32, Iter 57, disc loss: 0.0011348583845224426, policy loss: 7.663439979692851
Experience 32, Iter 58, disc loss: 0.0010802520010163054, policy loss: 7.603231486445472
Experience 32, Iter 59, disc loss: 0.0009969141187128896, policy loss: 7.576774459313947
Experience 32, Iter 60, disc loss: 0.0007947473576014397, policy loss: 7.79779059397696
Experience 32, Iter 61, disc loss: 0.0007612017008203383, policy loss: 7.832324504544565
Experience 32, Iter 62, disc loss: 0.0008008177718670908, policy loss: 7.966242616358903
Experience 32, Iter 63, disc loss: 0.0008905803948511907, policy loss: 7.761924733544273
Experience 32, Iter 64, disc loss: 0.0007304881897140629, policy loss: 8.21798605214785
Experience 32, Iter 65, disc loss: 0.0008871734744751122, policy loss: 7.796834313493912
Experience 32, Iter 66, disc loss: 0.0007681687619163309, policy loss: 8.145393110699048
Experience 32, Iter 67, disc loss: 0.0007762760269342436, policy loss: 8.182046149745613
Experience 32, Iter 68, disc loss: 0.0007624160591598359, policy loss: 8.03695986817274
Experience 32, Iter 69, disc loss: 0.0008359291061191393, policy loss: 7.987065910727429
Experience 32, Iter 70, disc loss: 0.0008382127507371198, policy loss: 7.9687098780988554
Experience 32, Iter 71, disc loss: 0.0005785061490138927, policy loss: 8.450796867122342
Experience 32, Iter 72, disc loss: 0.0006676421889544232, policy loss: 8.132371546211111
Experience 32, Iter 73, disc loss: 0.0007485248582191554, policy loss: 8.145330956135474
Experience 32, Iter 74, disc loss: 0.0007193428863808812, policy loss: 8.180457229496152
Experience 32, Iter 75, disc loss: 0.0007510114436774099, policy loss: 8.018588892423606
Experience 32, Iter 76, disc loss: 0.0006270662083506539, policy loss: 8.30906776829738
Experience 32, Iter 77, disc loss: 0.0006436770338690225, policy loss: 8.261830216538662
Experience 32, Iter 78, disc loss: 0.0006122366255662135, policy loss: 8.257317820094208
Experience 32, Iter 79, disc loss: 0.000640343474849518, policy loss: 8.194686898935792
Experience 32, Iter 80, disc loss: 0.0006067872146044421, policy loss: 8.316215731164107
Experience 32, Iter 81, disc loss: 0.000670643659748579, policy loss: 8.167829848107349
Experience 32, Iter 82, disc loss: 0.0007342975101525546, policy loss: 8.153260971070898
Experience 32, Iter 83, disc loss: 0.0006824185313496077, policy loss: 7.9525420717738
Experience 32, Iter 84, disc loss: 0.0008657375585936951, policy loss: 7.990668949717499
Experience 32, Iter 85, disc loss: 0.0005488487653981938, policy loss: 8.494449086004124
Experience 32, Iter 86, disc loss: 0.0005994126139115545, policy loss: 8.312541435337376
Experience 32, Iter 87, disc loss: 0.0006617708638256175, policy loss: 8.027937915283617
Experience 32, Iter 88, disc loss: 0.0005086265040777303, policy loss: 8.569228744032012
Experience 32, Iter 89, disc loss: 0.0005973962790390841, policy loss: 8.409714281065522
Experience 32, Iter 90, disc loss: 0.0005162425452680114, policy loss: 8.558937147677575
Experience 32, Iter 91, disc loss: 0.0005487537710143467, policy loss: 8.39086052012014
Experience 32, Iter 92, disc loss: 0.000562205148156692, policy loss: 8.416761249385221
Experience 32, Iter 93, disc loss: 0.0006563591276082365, policy loss: 8.436565695990838
Experience 32, Iter 94, disc loss: 0.0005550829532324042, policy loss: 8.531010943691667
Experience 32, Iter 95, disc loss: 0.000609965338144055, policy loss: 8.32359873387317
Experience 32, Iter 96, disc loss: 0.0005910691475176047, policy loss: 8.43159095835501
Experience 32, Iter 97, disc loss: 0.0005427520433798331, policy loss: 8.495060285638164
Experience 32, Iter 98, disc loss: 0.0006852424763982596, policy loss: 8.004246917312924
Experience 32, Iter 99, disc loss: 0.0006452553077286831, policy loss: 8.165480075359444
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.2372],
        [1.9484],
        [0.0400]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0278, 0.2292, 1.8134, 0.0320, 0.0266, 6.1199]],

        [[0.0278, 0.2292, 1.8134, 0.0320, 0.0266, 6.1199]],

        [[0.0278, 0.2292, 1.8134, 0.0320, 0.0266, 6.1199]],

        [[0.0278, 0.2292, 1.8134, 0.0320, 0.0266, 6.1199]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0212, 0.9487, 7.7938, 0.1600], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0212, 0.9487, 7.7938, 0.1600])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.579
Iter 2/2000 - Loss: 3.363
Iter 3/2000 - Loss: 3.221
Iter 4/2000 - Loss: 3.049
Iter 5/2000 - Loss: 2.909
Iter 6/2000 - Loss: 2.758
Iter 7/2000 - Loss: 2.551
Iter 8/2000 - Loss: 2.328
Iter 9/2000 - Loss: 2.118
Iter 10/2000 - Loss: 1.909
Iter 11/2000 - Loss: 1.683
Iter 12/2000 - Loss: 1.440
Iter 13/2000 - Loss: 1.189
Iter 14/2000 - Loss: 0.932
Iter 15/2000 - Loss: 0.668
Iter 16/2000 - Loss: 0.395
Iter 17/2000 - Loss: 0.114
Iter 18/2000 - Loss: -0.169
Iter 19/2000 - Loss: -0.453
Iter 20/2000 - Loss: -0.734
Iter 1981/2000 - Loss: -7.876
Iter 1982/2000 - Loss: -7.876
Iter 1983/2000 - Loss: -7.876
Iter 1984/2000 - Loss: -7.876
Iter 1985/2000 - Loss: -7.877
Iter 1986/2000 - Loss: -7.877
Iter 1987/2000 - Loss: -7.877
Iter 1988/2000 - Loss: -7.877
Iter 1989/2000 - Loss: -7.877
Iter 1990/2000 - Loss: -7.877
Iter 1991/2000 - Loss: -7.877
Iter 1992/2000 - Loss: -7.877
Iter 1993/2000 - Loss: -7.877
Iter 1994/2000 - Loss: -7.877
Iter 1995/2000 - Loss: -7.877
Iter 1996/2000 - Loss: -7.877
Iter 1997/2000 - Loss: -7.877
Iter 1998/2000 - Loss: -7.877
Iter 1999/2000 - Loss: -7.877
Iter 2000/2000 - Loss: -7.877
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.8384,  8.4246, 24.8516,  1.8707, 14.4541, 48.8236]],

        [[19.4901, 33.0960,  6.9921,  1.1926,  1.8194, 34.9612]],

        [[18.3653, 31.0971,  8.1351,  0.8865,  0.6879, 26.6155]],

        [[14.8120, 25.8881, 12.2313,  1.4701,  1.5926, 51.9441]]])
Signal Variance: tensor([ 0.1059,  2.5577, 11.7515,  0.3453])
Estimated target variance: tensor([0.0212, 0.9487, 7.7938, 0.1600])
N: 330
Signal to noise ratio: tensor([18.5187, 82.1506, 71.4348, 32.7143])
Bound on condition number: tensor([ 113171.9396, 2227081.2162, 1683968.8190,  353175.6251])
Policy Optimizer learning rate:
0.0009668466638656902
Experience 33, Iter 0, disc loss: 0.00047499786308337783, policy loss: 8.728094795580708
Experience 33, Iter 1, disc loss: 0.0005304090892968805, policy loss: 8.356803683602507
Experience 33, Iter 2, disc loss: 0.00045633872244553214, policy loss: 8.636014419339826
Experience 33, Iter 3, disc loss: 0.0005066514399142388, policy loss: 8.400190398079955
Experience 33, Iter 4, disc loss: 0.0005295461776822399, policy loss: 8.315818102956948
Experience 33, Iter 5, disc loss: 0.000643562565426447, policy loss: 8.096654955500533
Experience 33, Iter 6, disc loss: 0.0005659451544158831, policy loss: 8.149760397267936
Experience 33, Iter 7, disc loss: 0.0005650286152495656, policy loss: 8.18545720127828
Experience 33, Iter 8, disc loss: 0.0005240256433616936, policy loss: 8.488261384604828
Experience 33, Iter 9, disc loss: 0.000608644908004709, policy loss: 8.23047004096132
Experience 33, Iter 10, disc loss: 0.0005600131216236068, policy loss: 8.3974427086647
Experience 33, Iter 11, disc loss: 0.0005748811278270667, policy loss: 8.282841735376607
Experience 33, Iter 12, disc loss: 0.0005712830267879761, policy loss: 8.306429783197032
Experience 33, Iter 13, disc loss: 0.0005861615780551342, policy loss: 8.21569059848581
Experience 33, Iter 14, disc loss: 0.0005742993442401664, policy loss: 8.20941937845457
Experience 33, Iter 15, disc loss: 0.0005628893973015747, policy loss: 8.373504979265391
Experience 33, Iter 16, disc loss: 0.0005951740495520256, policy loss: 8.219257173071632
Experience 33, Iter 17, disc loss: 0.0006756607743245321, policy loss: 7.995825412484363
Experience 33, Iter 18, disc loss: 0.0005459631373027962, policy loss: 8.373383772395309
Experience 33, Iter 19, disc loss: 0.00048547491522381943, policy loss: 8.814245085888
Experience 33, Iter 20, disc loss: 0.0006269974576276877, policy loss: 8.07016063772597
Experience 33, Iter 21, disc loss: 0.000501166544345735, policy loss: 8.308390006701107
Experience 33, Iter 22, disc loss: 0.0005635431633653377, policy loss: 8.356999071846339
Experience 33, Iter 23, disc loss: 0.0005959135958381881, policy loss: 8.275263589739644
Experience 33, Iter 24, disc loss: 0.0005222341343787223, policy loss: 8.438809826664166
Experience 33, Iter 25, disc loss: 0.0004925279198486269, policy loss: 8.480379257285923
Experience 33, Iter 26, disc loss: 0.0004912876555550942, policy loss: 8.669618029049214
Experience 33, Iter 27, disc loss: 0.0005164316572943746, policy loss: 8.805947331357196
Experience 33, Iter 28, disc loss: 0.00042668045567400333, policy loss: 8.853533528879819
Experience 33, Iter 29, disc loss: 0.00037896524177291326, policy loss: 8.874248374354117
Experience 33, Iter 30, disc loss: 0.0003242326486176647, policy loss: 9.013964437806079
Experience 33, Iter 31, disc loss: 0.0002994609097906642, policy loss: 9.389200321840827
Experience 33, Iter 32, disc loss: 0.0003006366353167894, policy loss: 9.25917999606249
Experience 33, Iter 33, disc loss: 0.0002820236698246061, policy loss: 9.322019087637113
Experience 33, Iter 34, disc loss: 0.00026872625843652094, policy loss: 9.371256245458486
Experience 33, Iter 35, disc loss: 0.0002707363157347973, policy loss: 9.445540876119907
Experience 33, Iter 36, disc loss: 0.0002967371334072241, policy loss: 9.159811512420486
Experience 33, Iter 37, disc loss: 0.0002602622284161433, policy loss: 9.410079049523564
Experience 33, Iter 38, disc loss: 0.0003494447976422607, policy loss: 9.256992185161318
Experience 33, Iter 39, disc loss: 0.00032497283395645395, policy loss: 9.298623121253769
Experience 33, Iter 40, disc loss: 0.00028594107157073215, policy loss: 9.250746851939141
Experience 33, Iter 41, disc loss: 0.00031776652106050413, policy loss: 9.056333570827638
Experience 33, Iter 42, disc loss: 0.00040107976397850425, policy loss: 8.884996984531062
Experience 33, Iter 43, disc loss: 0.00028861518357835677, policy loss: 9.243750189267587
Experience 33, Iter 44, disc loss: 0.0003362844982011539, policy loss: 9.037167501193707
Experience 33, Iter 45, disc loss: 0.00034811595578847936, policy loss: 8.929044619367458
Experience 33, Iter 46, disc loss: 0.0004193964456789699, policy loss: 8.728782786467136
Experience 33, Iter 47, disc loss: 0.0003119315118712035, policy loss: 9.048466164298747
Experience 33, Iter 48, disc loss: 0.0003942002820520932, policy loss: 8.921829237180473
Experience 33, Iter 49, disc loss: 0.0004947205784177884, policy loss: 8.468628007657589
Experience 33, Iter 50, disc loss: 0.0003619014598978632, policy loss: 8.877274857636566
Experience 33, Iter 51, disc loss: 0.0004710845165830366, policy loss: 8.603795957997853
Experience 33, Iter 52, disc loss: 0.0004733085403939379, policy loss: 8.434253660166002
Experience 33, Iter 53, disc loss: 0.0005047205936094615, policy loss: 8.596180461749409
Experience 33, Iter 54, disc loss: 0.0004749888224992167, policy loss: 8.379736922307302
Experience 33, Iter 55, disc loss: 0.0004983372973060987, policy loss: 8.355944128703307
Experience 33, Iter 56, disc loss: 0.0005093187146869046, policy loss: 8.426428986529519
Experience 33, Iter 57, disc loss: 0.00042320706289167525, policy loss: 8.672845959297902
Experience 33, Iter 58, disc loss: 0.0004993867584947751, policy loss: 8.349404858421476
Experience 33, Iter 59, disc loss: 0.0004970987404639832, policy loss: 8.573744125451043
Experience 33, Iter 60, disc loss: 0.0005296033867381784, policy loss: 8.345734332265856
Experience 33, Iter 61, disc loss: 0.0005190628035679773, policy loss: 8.329599052483363
Experience 33, Iter 62, disc loss: 0.0004902248698671014, policy loss: 8.220253322888748
Experience 33, Iter 63, disc loss: 0.0005887889162400271, policy loss: 8.240606294057226
Experience 33, Iter 64, disc loss: 0.0005620326701544507, policy loss: 8.268158483879787
Experience 33, Iter 65, disc loss: 0.00044179020755382857, policy loss: 8.599951226585695
Experience 33, Iter 66, disc loss: 0.000605105066893414, policy loss: 8.107316352179655
Experience 33, Iter 67, disc loss: 0.0005864745280574512, policy loss: 8.226301531181317
Experience 33, Iter 68, disc loss: 0.000562865185159643, policy loss: 8.995311964966566
Experience 33, Iter 69, disc loss: 0.0006178832458556532, policy loss: 8.413787286494092
Experience 33, Iter 70, disc loss: 0.0004248910128637972, policy loss: 8.588975906260952
Experience 33, Iter 71, disc loss: 0.0005554662070463417, policy loss: 8.285619823197406
Experience 33, Iter 72, disc loss: 0.0006526082476088497, policy loss: 8.202770804683645
Experience 33, Iter 73, disc loss: 0.0005817692237731403, policy loss: 8.182371033744605
Experience 33, Iter 74, disc loss: 0.0005463040639656317, policy loss: 8.34073598290929
Experience 33, Iter 75, disc loss: 0.0005999629685799584, policy loss: 8.222953439912319
Experience 33, Iter 76, disc loss: 0.0007141428375604287, policy loss: 8.052211239326006
Experience 33, Iter 77, disc loss: 0.0007168237903191353, policy loss: 7.9174079300008335
Experience 33, Iter 78, disc loss: 0.0006296567020117247, policy loss: 8.202211341829765
Experience 33, Iter 79, disc loss: 0.0008297677430051843, policy loss: 7.6393979674729895
Experience 33, Iter 80, disc loss: 0.0005970783066178507, policy loss: 8.669291999302423
Experience 33, Iter 81, disc loss: 0.0005628546513851576, policy loss: 8.675301506269927
Experience 33, Iter 82, disc loss: 0.0004508141572974257, policy loss: 9.2655730691856
Experience 33, Iter 83, disc loss: 0.0005102056804280071, policy loss: 9.33486059437563
Experience 33, Iter 84, disc loss: 0.0005823414544345532, policy loss: 8.738882919900332
Experience 33, Iter 85, disc loss: 0.0005010987066424966, policy loss: 8.778726871720323
Experience 33, Iter 86, disc loss: 0.0005205213736778069, policy loss: 9.840872127457022
Experience 33, Iter 87, disc loss: 0.0004930392952201625, policy loss: 10.09765335082172
Experience 33, Iter 88, disc loss: 0.0005840982672619226, policy loss: 8.777882855623929
Experience 33, Iter 89, disc loss: 0.0004947315660020092, policy loss: 8.887373266829565
Experience 33, Iter 90, disc loss: 0.0005888042567715819, policy loss: 8.20726761298445
Experience 33, Iter 91, disc loss: 0.0005463799157523094, policy loss: 7.994419563676265
Experience 33, Iter 92, disc loss: 0.0005420647468436339, policy loss: 8.608519052408733
Experience 33, Iter 93, disc loss: 0.0005190263673292447, policy loss: 8.291529977731349
Experience 33, Iter 94, disc loss: 0.0006437506728291525, policy loss: 7.988871390206503
Experience 33, Iter 95, disc loss: 0.0005384860421971687, policy loss: 8.274583734161553
Experience 33, Iter 96, disc loss: 0.0006056645988491334, policy loss: 7.966733915602818
Experience 33, Iter 97, disc loss: 0.0005259137059252293, policy loss: 8.261414419022664
Experience 33, Iter 98, disc loss: 0.0004833506511034523, policy loss: 8.572750275233407
Experience 33, Iter 99, disc loss: 0.0005882180096701126, policy loss: 9.059960952484586
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.2347],
        [1.9262],
        [0.0403]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0294, 0.2313, 1.8160, 0.0316, 0.0260, 6.1074]],

        [[0.0294, 0.2313, 1.8160, 0.0316, 0.0260, 6.1074]],

        [[0.0294, 0.2313, 1.8160, 0.0316, 0.0260, 6.1074]],

        [[0.0294, 0.2313, 1.8160, 0.0316, 0.0260, 6.1074]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0214, 0.9387, 7.7047, 0.1613], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0214, 0.9387, 7.7047, 0.1613])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.569
Iter 2/2000 - Loss: 3.342
Iter 3/2000 - Loss: 3.218
Iter 4/2000 - Loss: 3.045
Iter 5/2000 - Loss: 2.890
Iter 6/2000 - Loss: 2.739
Iter 7/2000 - Loss: 2.539
Iter 8/2000 - Loss: 2.312
Iter 9/2000 - Loss: 2.095
Iter 10/2000 - Loss: 1.885
Iter 11/2000 - Loss: 1.660
Iter 12/2000 - Loss: 1.414
Iter 13/2000 - Loss: 1.154
Iter 14/2000 - Loss: 0.889
Iter 15/2000 - Loss: 0.621
Iter 16/2000 - Loss: 0.347
Iter 17/2000 - Loss: 0.067
Iter 18/2000 - Loss: -0.218
Iter 19/2000 - Loss: -0.504
Iter 20/2000 - Loss: -0.789
Iter 1981/2000 - Loss: -7.919
Iter 1982/2000 - Loss: -7.919
Iter 1983/2000 - Loss: -7.919
Iter 1984/2000 - Loss: -7.919
Iter 1985/2000 - Loss: -7.919
Iter 1986/2000 - Loss: -7.919
Iter 1987/2000 - Loss: -7.919
Iter 1988/2000 - Loss: -7.919
Iter 1989/2000 - Loss: -7.919
Iter 1990/2000 - Loss: -7.919
Iter 1991/2000 - Loss: -7.919
Iter 1992/2000 - Loss: -7.919
Iter 1993/2000 - Loss: -7.919
Iter 1994/2000 - Loss: -7.920
Iter 1995/2000 - Loss: -7.920
Iter 1996/2000 - Loss: -7.920
Iter 1997/2000 - Loss: -7.920
Iter 1998/2000 - Loss: -7.920
Iter 1999/2000 - Loss: -7.920
Iter 2000/2000 - Loss: -7.920
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.7526,  8.3433, 24.0689,  1.8715, 14.3738, 49.1778]],

        [[19.6086, 33.6985,  6.9939,  1.1486,  1.8132, 34.3910]],

        [[18.7547, 31.4490,  8.1196,  0.8888,  0.6885, 26.7323]],

        [[14.4091, 25.7354, 12.3003,  1.4801,  1.5986, 51.7300]]])
Signal Variance: tensor([ 0.1045,  2.4628, 11.8541,  0.3479])
Estimated target variance: tensor([0.0214, 0.9387, 7.7047, 0.1613])
N: 340
Signal to noise ratio: tensor([18.4606, 80.1202, 72.4722, 33.0986])
Bound on condition number: tensor([ 115871.4920, 2182542.5067, 1785754.7051,  372476.8903])
Policy Optimizer learning rate:
0.0009658285256870233
Experience 34, Iter 0, disc loss: 0.0005410516784877992, policy loss: 8.820163638072959
Experience 34, Iter 1, disc loss: 0.00043741421144389826, policy loss: 8.738810237809936
Experience 34, Iter 2, disc loss: 0.0005027909886836555, policy loss: 8.517911382034526
Experience 34, Iter 3, disc loss: 0.0003629039126102876, policy loss: 8.96360093974095
Experience 34, Iter 4, disc loss: 0.0004276333171075457, policy loss: 8.73999320057523
Experience 34, Iter 5, disc loss: 0.00039947672658371497, policy loss: 9.389638167448556
Experience 34, Iter 6, disc loss: 0.0004268799648518587, policy loss: 8.796216482558124
Experience 34, Iter 7, disc loss: 0.00035579375729699516, policy loss: 9.188003572967178
Experience 34, Iter 8, disc loss: 0.0004341864961499224, policy loss: 8.599266105695841
Experience 34, Iter 9, disc loss: 0.000457061872178666, policy loss: 8.67254966040321
Experience 34, Iter 10, disc loss: 0.0004843007797417177, policy loss: 8.753646686442323
Experience 34, Iter 11, disc loss: 0.00039545419785491035, policy loss: 8.617892081652418
Experience 34, Iter 12, disc loss: 0.00034360525057307225, policy loss: 9.239737452257526
Experience 34, Iter 13, disc loss: 0.000299379272145417, policy loss: 9.410907823821155
Experience 34, Iter 14, disc loss: 0.0004315654623837855, policy loss: 8.926948689380064
Experience 34, Iter 15, disc loss: 0.0003600378822212182, policy loss: 8.902501680316
Experience 34, Iter 16, disc loss: 0.0004347377527578445, policy loss: 8.903905254592388
Experience 34, Iter 17, disc loss: 0.00040282302079860114, policy loss: 8.74922100899904
Experience 34, Iter 18, disc loss: 0.00039241096649759945, policy loss: 8.800898897290022
Experience 34, Iter 19, disc loss: 0.000506968635914309, policy loss: 8.532001967766838
Experience 34, Iter 20, disc loss: 0.0005148739722422956, policy loss: 8.463933421983857
Experience 34, Iter 21, disc loss: 0.0004081978035935934, policy loss: 8.757802034132736
Experience 34, Iter 22, disc loss: 0.0004008191749275233, policy loss: 8.793829158911581
Experience 34, Iter 23, disc loss: 0.00045187164250285746, policy loss: 8.704664648797175
Experience 34, Iter 24, disc loss: 0.000397094423919476, policy loss: 8.830643540488548
Experience 34, Iter 25, disc loss: 0.00037893585043086945, policy loss: 9.059708111008094
Experience 34, Iter 26, disc loss: 0.00043232509325546607, policy loss: 8.478566875463533
Experience 34, Iter 27, disc loss: 0.00038150823948351485, policy loss: 9.280237477702544
Experience 34, Iter 28, disc loss: 0.0004545680173817625, policy loss: 8.452526561188371
Experience 34, Iter 29, disc loss: 0.00038391526103030563, policy loss: 8.668306042741435
Experience 34, Iter 30, disc loss: 0.00033947010022651716, policy loss: 9.022999786524036
Experience 34, Iter 31, disc loss: 0.0004060291743832416, policy loss: 8.83194224300738
Experience 34, Iter 32, disc loss: 0.0005009116585330689, policy loss: 8.503973158149487
Experience 34, Iter 33, disc loss: 0.00036581861140852273, policy loss: 9.04076359657417
Experience 34, Iter 34, disc loss: 0.00036019649997455527, policy loss: 9.068955192833434
Experience 34, Iter 35, disc loss: 0.0003231579975608272, policy loss: 9.004281436577838
Experience 34, Iter 36, disc loss: 0.0002606130552404486, policy loss: 9.252717367342774
Experience 34, Iter 37, disc loss: 0.00037419200103595634, policy loss: 8.838522479458524
Experience 34, Iter 38, disc loss: 0.0002866422765156851, policy loss: 9.121632363485787
Experience 34, Iter 39, disc loss: 0.0003550029644092997, policy loss: 8.752314009513011
Experience 34, Iter 40, disc loss: 0.0003415500007338374, policy loss: 8.85213731304071
Experience 34, Iter 41, disc loss: 0.000355417267542644, policy loss: 8.754867658211252
Experience 34, Iter 42, disc loss: 0.0003877211471375032, policy loss: 8.626289566685243
Experience 34, Iter 43, disc loss: 0.000501609168796947, policy loss: 8.270639369678554
Experience 34, Iter 44, disc loss: 0.0003623572341858215, policy loss: 8.662759494516358
Experience 34, Iter 45, disc loss: 0.0003297980917435608, policy loss: 8.848009246957972
Experience 34, Iter 46, disc loss: 0.0003873534806044435, policy loss: 8.511447029502017
Experience 34, Iter 47, disc loss: 0.000301272827077546, policy loss: 8.870214942330705
Experience 34, Iter 48, disc loss: 0.0003200627094482526, policy loss: 8.728246168575508
Experience 34, Iter 49, disc loss: 0.0003154946995203035, policy loss: 8.783231678267015
Experience 34, Iter 50, disc loss: 0.0003308439055248476, policy loss: 8.660850156842237
Experience 34, Iter 51, disc loss: 0.00032579893509837506, policy loss: 8.724452716257748
Experience 34, Iter 52, disc loss: 0.000355262332731347, policy loss: 8.74468475147357
Experience 34, Iter 53, disc loss: 0.00036658031200164433, policy loss: 8.648048207428722
Experience 34, Iter 54, disc loss: 0.0003281922123624179, policy loss: 8.75878637997792
Experience 34, Iter 55, disc loss: 0.00032181373541743583, policy loss: 8.906004561956784
Experience 34, Iter 56, disc loss: 0.0003565200088682035, policy loss: 8.637723210398484
Experience 34, Iter 57, disc loss: 0.0003900320014920438, policy loss: 8.416053123619722
Experience 34, Iter 58, disc loss: 0.0003244529187962494, policy loss: 8.723272336960564
Experience 34, Iter 59, disc loss: 0.0003801666125408884, policy loss: 8.418643099939553
Experience 34, Iter 60, disc loss: 0.00033430747296699906, policy loss: 8.562483130900276
Experience 34, Iter 61, disc loss: 0.00043094112493394823, policy loss: 8.38542649193835
Experience 34, Iter 62, disc loss: 0.0003425084932799923, policy loss: 8.604143065245562
Experience 34, Iter 63, disc loss: 0.00035130278876282584, policy loss: 8.494534503050827
Experience 34, Iter 64, disc loss: 0.0003762533702724471, policy loss: 8.523752686156588
Experience 34, Iter 65, disc loss: 0.00034472860435773496, policy loss: 8.620159683513696
Experience 34, Iter 66, disc loss: 0.0003817065537288723, policy loss: 8.36061447370964
Experience 34, Iter 67, disc loss: 0.000343993376584915, policy loss: 8.58589613660827
Experience 34, Iter 68, disc loss: 0.0003626060863513317, policy loss: 8.482215265523976
Experience 34, Iter 69, disc loss: 0.0003648150437665985, policy loss: 8.395194652782449
Experience 34, Iter 70, disc loss: 0.00040659702692740196, policy loss: 8.383161015946504
Experience 34, Iter 71, disc loss: 0.0003585998999971297, policy loss: 8.700877081424576
Experience 34, Iter 72, disc loss: 0.0003535062006386666, policy loss: 8.524236359046306
Experience 34, Iter 73, disc loss: 0.00038811623536758706, policy loss: 8.424691627437204
Experience 34, Iter 74, disc loss: 0.00036067623658505335, policy loss: 8.555814303599371
Experience 34, Iter 75, disc loss: 0.00035478537817029975, policy loss: 8.661891916519014
Experience 34, Iter 76, disc loss: 0.00037133587982579727, policy loss: 8.472761903700407
Experience 34, Iter 77, disc loss: 0.00032052282241361676, policy loss: 8.721209012938235
Experience 34, Iter 78, disc loss: 0.0003635891379056936, policy loss: 8.595693145464757
Experience 34, Iter 79, disc loss: 0.00034384315620250366, policy loss: 8.646787755793909
Experience 34, Iter 80, disc loss: 0.00029348945816270707, policy loss: 9.506607497698026
Experience 34, Iter 81, disc loss: 0.0003505896387459418, policy loss: 8.806777009163024
Experience 34, Iter 82, disc loss: 0.00030958136847163766, policy loss: 9.36458769873668
Experience 34, Iter 83, disc loss: 0.00034608228397660335, policy loss: 8.656480446226972
Experience 34, Iter 84, disc loss: 0.00036205717205328605, policy loss: 8.681976847454138
Experience 34, Iter 85, disc loss: 0.00034648555665112775, policy loss: 8.557840148045262
Experience 34, Iter 86, disc loss: 0.00036584202139389004, policy loss: 8.410564786597327
Experience 34, Iter 87, disc loss: 0.0003435280739576254, policy loss: 8.638884496214128
Experience 34, Iter 88, disc loss: 0.0004006112575023706, policy loss: 8.341952493280402
Experience 34, Iter 89, disc loss: 0.00038608557583341273, policy loss: 8.298857792566888
Experience 34, Iter 90, disc loss: 0.0003187373695506681, policy loss: 8.60411054823324
Experience 34, Iter 91, disc loss: 0.0003041988138595532, policy loss: 8.90709147011058
Experience 34, Iter 92, disc loss: 0.0003593937427643242, policy loss: 8.477856669433212
Experience 34, Iter 93, disc loss: 0.0003497507264573637, policy loss: 8.522048876837937
Experience 34, Iter 94, disc loss: 0.0003233414443756321, policy loss: 8.56094542188151
Experience 34, Iter 95, disc loss: 0.00031360824996415966, policy loss: 8.659260447482941
Experience 34, Iter 96, disc loss: 0.00033224581937668846, policy loss: 8.54846576831229
Experience 34, Iter 97, disc loss: 0.00031711261623942514, policy loss: 8.733123980130046
Experience 34, Iter 98, disc loss: 0.00030291145023718364, policy loss: 8.606807665395433
Experience 34, Iter 99, disc loss: 0.0003554333462109945, policy loss: 8.667912115881954
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0053],
        [0.2329],
        [1.9145],
        [0.0405]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0309, 0.2297, 1.8149, 0.0312, 0.0254, 6.0929]],

        [[0.0309, 0.2297, 1.8149, 0.0312, 0.0254, 6.0929]],

        [[0.0309, 0.2297, 1.8149, 0.0312, 0.0254, 6.0929]],

        [[0.0309, 0.2297, 1.8149, 0.0312, 0.0254, 6.0929]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0212, 0.9316, 7.6581, 0.1621], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0212, 0.9316, 7.6581, 0.1621])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.560
Iter 2/2000 - Loss: 3.326
Iter 3/2000 - Loss: 3.215
Iter 4/2000 - Loss: 3.043
Iter 5/2000 - Loss: 2.877
Iter 6/2000 - Loss: 2.722
Iter 7/2000 - Loss: 2.528
Iter 8/2000 - Loss: 2.301
Iter 9/2000 - Loss: 2.081
Iter 10/2000 - Loss: 1.870
Iter 11/2000 - Loss: 1.648
Iter 12/2000 - Loss: 1.401
Iter 13/2000 - Loss: 1.140
Iter 14/2000 - Loss: 0.873
Iter 15/2000 - Loss: 0.605
Iter 16/2000 - Loss: 0.334
Iter 17/2000 - Loss: 0.057
Iter 18/2000 - Loss: -0.227
Iter 19/2000 - Loss: -0.514
Iter 20/2000 - Loss: -0.801
Iter 1981/2000 - Loss: -7.952
Iter 1982/2000 - Loss: -7.952
Iter 1983/2000 - Loss: -7.952
Iter 1984/2000 - Loss: -7.952
Iter 1985/2000 - Loss: -7.952
Iter 1986/2000 - Loss: -7.952
Iter 1987/2000 - Loss: -7.952
Iter 1988/2000 - Loss: -7.952
Iter 1989/2000 - Loss: -7.952
Iter 1990/2000 - Loss: -7.952
Iter 1991/2000 - Loss: -7.952
Iter 1992/2000 - Loss: -7.953
Iter 1993/2000 - Loss: -7.953
Iter 1994/2000 - Loss: -7.953
Iter 1995/2000 - Loss: -7.953
Iter 1996/2000 - Loss: -7.953
Iter 1997/2000 - Loss: -7.953
Iter 1998/2000 - Loss: -7.953
Iter 1999/2000 - Loss: -7.953
Iter 2000/2000 - Loss: -7.953
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[11.8501,  8.3872, 22.9660,  1.9215, 14.2186, 49.5413]],

        [[19.2697, 33.5965,  6.9541,  1.1449,  1.8133, 32.1369]],

        [[19.3275, 32.7099,  8.0565,  0.9035,  0.6908, 25.9950]],

        [[14.4426, 24.8594, 11.8979,  1.5050,  1.6100, 51.5039]]])
Signal Variance: tensor([ 0.1043,  2.3474, 11.5288,  0.3436])
Estimated target variance: tensor([0.0212, 0.9316, 7.6581, 0.1621])
N: 350
Signal to noise ratio: tensor([18.5688, 78.4460, 70.7887, 33.1206])
Bound on condition number: tensor([ 120681.3954, 2153820.1747, 1753867.0896,  383941.0062])
Policy Optimizer learning rate:
0.0009648114596590805
Experience 35, Iter 0, disc loss: 0.00037651485276337765, policy loss: 8.461105578564075
Experience 35, Iter 1, disc loss: 0.0003649743206364792, policy loss: 8.394271010145784
Experience 35, Iter 2, disc loss: 0.0003881363823970223, policy loss: 8.376384968857476
Experience 35, Iter 3, disc loss: 0.0003463719703596253, policy loss: 8.719891284124218
Experience 35, Iter 4, disc loss: 0.0003717968939102313, policy loss: 8.445630255637147
Experience 35, Iter 5, disc loss: 0.0003884428616051141, policy loss: 8.425480194083054
Experience 35, Iter 6, disc loss: 0.00031687768657693855, policy loss: 8.726071350985872
Experience 35, Iter 7, disc loss: 0.00034481882055128923, policy loss: 8.559617358362202
Experience 35, Iter 8, disc loss: 0.000338274180902544, policy loss: 8.555020903720433
Experience 35, Iter 9, disc loss: 0.0003756756390961614, policy loss: 8.533732671864417
Experience 35, Iter 10, disc loss: 0.0003450989780807769, policy loss: 8.644034218951736
Experience 35, Iter 11, disc loss: 0.00041041823823399114, policy loss: 8.509958168522251
Experience 35, Iter 12, disc loss: 0.000357894077820309, policy loss: 8.535987395960563
Experience 35, Iter 13, disc loss: 0.0003473401522519425, policy loss: 8.502673226104244
Experience 35, Iter 14, disc loss: 0.00033284387710561506, policy loss: 8.655345623047337
Experience 35, Iter 15, disc loss: 0.0003508072916796522, policy loss: 8.764141960887546
Experience 35, Iter 16, disc loss: 0.00036569101258213533, policy loss: 8.463872483507611
Experience 35, Iter 17, disc loss: 0.00039284281655537666, policy loss: 8.4327542303591
Experience 35, Iter 18, disc loss: 0.0003685877733039019, policy loss: 8.391206617671186
Experience 35, Iter 19, disc loss: 0.0003727115097035734, policy loss: 8.47522479074503
Experience 35, Iter 20, disc loss: 0.00029354747756738143, policy loss: 8.801354925389937
Experience 35, Iter 21, disc loss: 0.0003585887941202938, policy loss: 8.582821247292951
Experience 35, Iter 22, disc loss: 0.00037424641471140883, policy loss: 8.35954057405427
Experience 35, Iter 23, disc loss: 0.00034458431574962685, policy loss: 8.50138956814164
Experience 35, Iter 24, disc loss: 0.00033945998761092427, policy loss: 8.541686440868844
Experience 35, Iter 25, disc loss: 0.0003519947691848301, policy loss: 8.407525872191526
Experience 35, Iter 26, disc loss: 0.00040845100976106256, policy loss: 8.337623835637816
Experience 35, Iter 27, disc loss: 0.00039255001208313487, policy loss: 8.48283027540894
Experience 35, Iter 28, disc loss: 0.0003524414697877969, policy loss: 8.801563694816274
Experience 35, Iter 29, disc loss: 0.00034779812201864277, policy loss: 8.623135932823581
Experience 35, Iter 30, disc loss: 0.00039815433730460857, policy loss: 8.404099590923892
Experience 35, Iter 31, disc loss: 0.0003533870154200362, policy loss: 8.482350410235023
Experience 35, Iter 32, disc loss: 0.00036628036319733524, policy loss: 8.415396563926283
Experience 35, Iter 33, disc loss: 0.0003740907229140243, policy loss: 8.483324490815399
Experience 35, Iter 34, disc loss: 0.00030668732648125407, policy loss: 8.84362927151853
Experience 35, Iter 35, disc loss: 0.0003683795368078628, policy loss: 8.477865989323252
Experience 35, Iter 36, disc loss: 0.0003436884125430436, policy loss: 8.671602615711041
Experience 35, Iter 37, disc loss: 0.00034406132371947966, policy loss: 8.747665771855392
Experience 35, Iter 38, disc loss: 0.0003716065291559617, policy loss: 8.519519565591647
Experience 35, Iter 39, disc loss: 0.00038052400855386154, policy loss: 8.424288679761812
Experience 35, Iter 40, disc loss: 0.0003506923615318227, policy loss: 8.517520825651935
Experience 35, Iter 41, disc loss: 0.0003955134856294285, policy loss: 8.472540377628036
Experience 35, Iter 42, disc loss: 0.00031264078857889347, policy loss: 8.97543579548207
Experience 35, Iter 43, disc loss: 0.0003381417667366437, policy loss: 8.70326698889256
Experience 35, Iter 44, disc loss: 0.0003225126328611757, policy loss: 8.714067420432961
Experience 35, Iter 45, disc loss: 0.000306416731374478, policy loss: 8.710617139250736
Experience 35, Iter 46, disc loss: 0.000360351126804884, policy loss: 8.671121291348282
Experience 35, Iter 47, disc loss: 0.0003315862223845591, policy loss: 8.616754431905257
Experience 35, Iter 48, disc loss: 0.0004144730370430976, policy loss: 8.438766519959508
Experience 35, Iter 49, disc loss: 0.00033406633690003884, policy loss: 8.621521483171154
Experience 35, Iter 50, disc loss: 0.0003287308555302276, policy loss: 8.60182924861602
Experience 35, Iter 51, disc loss: 0.0003497212026258833, policy loss: 8.521628329701834
Experience 35, Iter 52, disc loss: 0.0003142997590310436, policy loss: 8.896511844518841
Experience 35, Iter 53, disc loss: 0.00032933282022037924, policy loss: 8.830319767115348
Experience 35, Iter 54, disc loss: 0.0003332987238717521, policy loss: 8.884922631639668
Experience 35, Iter 55, disc loss: 0.0003124839560517612, policy loss: 8.907587845745699
Experience 35, Iter 56, disc loss: 0.0002924342062781269, policy loss: 8.80843425158075
Experience 35, Iter 57, disc loss: 0.00034844246774593825, policy loss: 8.612079824787546
Experience 35, Iter 58, disc loss: 0.00035751944214949436, policy loss: 8.617055520864437
Experience 35, Iter 59, disc loss: 0.00028981754719122294, policy loss: 9.214700258342894
Experience 35, Iter 60, disc loss: 0.00034313977695038164, policy loss: 8.617421274199447
Experience 35, Iter 61, disc loss: 0.0002745004244567039, policy loss: 8.944500997926301
Experience 35, Iter 62, disc loss: 0.0003317073771244459, policy loss: 8.623105488832916
Experience 35, Iter 63, disc loss: 0.0003035091481472357, policy loss: 8.772705449927038
Experience 35, Iter 64, disc loss: 0.00037878086851250687, policy loss: 8.562461798053977
Experience 35, Iter 65, disc loss: 0.00031982876774391475, policy loss: 8.648644265768151
Experience 35, Iter 66, disc loss: 0.0003233182593984775, policy loss: 8.66813996378325
Experience 35, Iter 67, disc loss: 0.00027123621118947357, policy loss: 8.78985064855555
Experience 35, Iter 68, disc loss: 0.00027524051943147163, policy loss: 8.782311983369404
Experience 35, Iter 69, disc loss: 0.00026213038214431913, policy loss: 8.816368939856467
Experience 35, Iter 70, disc loss: 0.0002750396094665889, policy loss: 8.76430186624599
Experience 35, Iter 71, disc loss: 0.00023793542717107671, policy loss: 8.931547484477928
Experience 35, Iter 72, disc loss: 0.0002447042796967289, policy loss: 9.109582877471263
Experience 35, Iter 73, disc loss: 0.0002962986505308329, policy loss: 8.745347924490599
Experience 35, Iter 74, disc loss: 0.00029533923995855816, policy loss: 8.7544700472566
Experience 35, Iter 75, disc loss: 0.0003391501687827899, policy loss: 8.72357862189322
Experience 35, Iter 76, disc loss: 0.0002944714537650244, policy loss: 8.846100031339493
Experience 35, Iter 77, disc loss: 0.0002982678949092986, policy loss: 8.82638768557235
Experience 35, Iter 78, disc loss: 0.00028405309827047676, policy loss: 8.913381615758132
Experience 35, Iter 79, disc loss: 0.0003685659435952864, policy loss: 8.587025598413668
Experience 35, Iter 80, disc loss: 0.0003462146238342992, policy loss: 8.704343999098718
Experience 35, Iter 81, disc loss: 0.00027426174082894326, policy loss: 8.958395537119998
Experience 35, Iter 82, disc loss: 0.0003922115081070171, policy loss: 8.58932183783457
Experience 35, Iter 83, disc loss: 0.00033277183736344566, policy loss: 8.615831268839282
Experience 35, Iter 84, disc loss: 0.00027209238428859236, policy loss: 9.134001859019461
Experience 35, Iter 85, disc loss: 0.00035717027544546437, policy loss: 8.674127486191649
