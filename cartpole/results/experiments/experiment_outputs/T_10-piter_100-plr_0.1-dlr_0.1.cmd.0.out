Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0130],
        [0.8980],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0518, 3.5922, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 1.196
Iter 3/2000 - Loss: 0.953
Iter 4/2000 - Loss: 1.383
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.493
Iter 7/2000 - Loss: 1.248
Iter 8/2000 - Loss: 1.053
Iter 9/2000 - Loss: 0.964
Iter 10/2000 - Loss: 0.972
Iter 11/2000 - Loss: 1.033
Iter 12/2000 - Loss: 1.091
Iter 13/2000 - Loss: 1.105
Iter 14/2000 - Loss: 1.071
Iter 15/2000 - Loss: 1.012
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.889
Iter 20/2000 - Loss: 0.912
Iter 1981/2000 - Loss: 0.698
Iter 1982/2000 - Loss: 0.698
Iter 1983/2000 - Loss: 0.698
Iter 1984/2000 - Loss: 0.698
Iter 1985/2000 - Loss: 0.698
Iter 1986/2000 - Loss: 0.698
Iter 1987/2000 - Loss: 0.698
Iter 1988/2000 - Loss: 0.698
Iter 1989/2000 - Loss: 0.698
Iter 1990/2000 - Loss: 0.698
Iter 1991/2000 - Loss: 0.698
Iter 1992/2000 - Loss: 0.698
Iter 1993/2000 - Loss: 0.698
Iter 1994/2000 - Loss: 0.698
Iter 1995/2000 - Loss: 0.698
Iter 1996/2000 - Loss: 0.698
Iter 1997/2000 - Loss: 0.698
Iter 1998/2000 - Loss: 0.698
Iter 1999/2000 - Loss: 0.698
Iter 2000/2000 - Loss: 0.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0093],
        [0.4134],
        [0.0149]])
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]])
Signal Variance: tensor([0.0033, 0.0374, 2.8196, 0.0608])
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([1.9681, 2.0097, 2.6117, 2.0186])
Bound on condition number: tensor([39.7326, 41.3892, 69.2095, 41.7472])
Policy Optimizer learning rate:
0.1
Experience 1, Iter 0, disc loss: 1.2474456489459007, policy loss: 0.7896666701180209
Experience 1, Iter 1, disc loss: 1.238553029485584, policy loss: 0.7873823670980868
Experience 1, Iter 2, disc loss: 1.2212872825827055, policy loss: 0.7971592526674758
Experience 1, Iter 3, disc loss: 1.2120395918085893, policy loss: 0.79653040296369
Experience 1, Iter 4, disc loss: 1.1978797741017342, policy loss: 0.8019463292522113
Experience 1, Iter 5, disc loss: 1.1900252120800126, policy loss: 0.8005873955116716
Experience 1, Iter 6, disc loss: 1.1811046881683351, policy loss: 0.799522400585756
Experience 1, Iter 7, disc loss: 1.1722303488454298, policy loss: 0.8010939797526858
Experience 1, Iter 8, disc loss: 1.1530986823729092, policy loss: 0.8132673026661053
Experience 1, Iter 9, disc loss: 1.1224149042033402, policy loss: 0.8429450448975797
Experience 1, Iter 10, disc loss: 1.1379394700679035, policy loss: 0.8168089438629441
Experience 1, Iter 11, disc loss: 1.11723136670857, policy loss: 0.830788008935896
Experience 1, Iter 12, disc loss: 1.0989821541058173, policy loss: 0.8478130990744922
Experience 1, Iter 13, disc loss: 1.1018768742913356, policy loss: 0.8298861603765388
Experience 1, Iter 14, disc loss: 1.0776463100383284, policy loss: 0.8528759247514688
Experience 1, Iter 15, disc loss: 1.0669832805419133, policy loss: 0.8582064860055778
Experience 1, Iter 16, disc loss: 1.0539026925314963, policy loss: 0.862256393264676
Experience 1, Iter 17, disc loss: 1.0581092548724298, policy loss: 0.8483706847560377
Experience 1, Iter 18, disc loss: 1.0190379260903346, policy loss: 0.8856908889551394
Experience 1, Iter 19, disc loss: 1.0282507187257046, policy loss: 0.8659194958097213
Experience 1, Iter 20, disc loss: 1.0055483879313294, policy loss: 0.8822816902946391
Experience 1, Iter 21, disc loss: 0.9911228667655432, policy loss: 0.8927344315600375
Experience 1, Iter 22, disc loss: 0.9756035573145887, policy loss: 0.9004548871246298
Experience 1, Iter 23, disc loss: 0.9525116964282807, policy loss: 0.9241203264014076
Experience 1, Iter 24, disc loss: 0.9698975905107295, policy loss: 0.8907474692271219
Experience 1, Iter 25, disc loss: 0.9357377918213434, policy loss: 0.9279166946158637
Experience 1, Iter 26, disc loss: 0.9433116314578827, policy loss: 0.9104344148038408
Experience 1, Iter 27, disc loss: 0.8940834587394509, policy loss: 0.9704408297342136
Experience 1, Iter 28, disc loss: 0.8965259758323522, policy loss: 0.9472832040466153
Experience 1, Iter 29, disc loss: 0.8748088898832991, policy loss: 0.9639586244514715
Experience 1, Iter 30, disc loss: 0.8543092131914396, policy loss: 0.9879345322654833
Experience 1, Iter 31, disc loss: 0.8306393551031432, policy loss: 1.011928035809603
Experience 1, Iter 32, disc loss: 0.8303186534301912, policy loss: 1.004009488411406
Experience 1, Iter 33, disc loss: 0.8114116548418078, policy loss: 1.032453844734035
Experience 1, Iter 34, disc loss: 0.7925629760538839, policy loss: 1.0438133513191097
Experience 1, Iter 35, disc loss: 0.7655832838212355, policy loss: 1.086491366097989
Experience 1, Iter 36, disc loss: 0.7450284345989349, policy loss: 1.099256675433487
Experience 1, Iter 37, disc loss: 0.745176253493456, policy loss: 1.089508080612077
Experience 1, Iter 38, disc loss: 0.7391953593610159, policy loss: 1.0864367269322202
Experience 1, Iter 39, disc loss: 0.7031283198090588, policy loss: 1.1408576008806635
Experience 1, Iter 40, disc loss: 0.7074780373228851, policy loss: 1.1217054749813617
Experience 1, Iter 41, disc loss: 0.6955393274237833, policy loss: 1.1254981210729118
Experience 1, Iter 42, disc loss: 0.6716142459051222, policy loss: 1.1548400821871256
Experience 1, Iter 43, disc loss: 0.646358353690726, policy loss: 1.1985548210660608
Experience 1, Iter 44, disc loss: 0.6482411662297269, policy loss: 1.1799464679568539
Experience 1, Iter 45, disc loss: 0.6435343820080627, policy loss: 1.1688927755044176
Experience 1, Iter 46, disc loss: 0.6184995058058891, policy loss: 1.2133031792473887
Experience 1, Iter 47, disc loss: 0.6347722789579925, policy loss: 1.1772969452325357
Experience 1, Iter 48, disc loss: 0.5892318571471296, policy loss: 1.2653990017397958
Experience 1, Iter 49, disc loss: 0.5718219988909076, policy loss: 1.2904116343366945
Experience 1, Iter 50, disc loss: 0.5457124425748691, policy loss: 1.3307958762624792
Experience 1, Iter 51, disc loss: 0.5465699265273, policy loss: 1.310988875316919
Experience 1, Iter 52, disc loss: 0.5410649595542371, policy loss: 1.3517850158018603
Experience 1, Iter 53, disc loss: 0.5376612254219583, policy loss: 1.322447896088453
Experience 1, Iter 54, disc loss: 0.49413925086807364, policy loss: 1.4198448179837064
Experience 1, Iter 55, disc loss: 0.47134904325710614, policy loss: 1.4529772689236509
Experience 1, Iter 56, disc loss: 0.4683083607905725, policy loss: 1.4443236849995544
Experience 1, Iter 57, disc loss: 0.48122008519399617, policy loss: 1.4165665589218874
Experience 1, Iter 58, disc loss: 0.44128985886751537, policy loss: 1.5047704028837119
Experience 1, Iter 59, disc loss: 0.41501989237586523, policy loss: 1.5714125196468218
Experience 1, Iter 60, disc loss: 0.44075183794782336, policy loss: 1.4896835339300392
Experience 1, Iter 61, disc loss: 0.44597580550639454, policy loss: 1.5004137797741377
Experience 1, Iter 62, disc loss: 0.40568228785391713, policy loss: 1.5695326810216517
Experience 1, Iter 63, disc loss: 0.3896883909988049, policy loss: 1.634186469943812
Experience 1, Iter 64, disc loss: 0.4077992024508277, policy loss: 1.58410756111725
Experience 1, Iter 65, disc loss: 0.40197060989028816, policy loss: 1.595496077671578
Experience 1, Iter 66, disc loss: 0.37134553457823577, policy loss: 1.6583345532349596
Experience 1, Iter 67, disc loss: 0.3826917786442977, policy loss: 1.5964950222487788
Experience 1, Iter 68, disc loss: 0.3484840160430782, policy loss: 1.7236320614335394
Experience 1, Iter 69, disc loss: 0.34139787372074676, policy loss: 1.7297467305395893
Experience 1, Iter 70, disc loss: 0.32642192837957196, policy loss: 1.7894729687421258
Experience 1, Iter 71, disc loss: 0.3106790956713622, policy loss: 1.8484318270775568
Experience 1, Iter 72, disc loss: 0.3078755583877544, policy loss: 1.870394119240495
Experience 1, Iter 73, disc loss: 0.3209504461776851, policy loss: 1.8480546902410864
Experience 1, Iter 74, disc loss: 0.29199835216160774, policy loss: 1.9160323512384798
Experience 1, Iter 75, disc loss: 0.29701687382859, policy loss: 1.894720594417704
Experience 1, Iter 76, disc loss: 0.26334912648796927, policy loss: 1.992230912165475
Experience 1, Iter 77, disc loss: 0.2753678586013201, policy loss: 1.963192275311119
Experience 1, Iter 78, disc loss: 0.27301253838285455, policy loss: 1.9727394767192257
Experience 1, Iter 79, disc loss: 0.22216719964243997, policy loss: 2.1976328273021
Experience 1, Iter 80, disc loss: 0.26443454651680653, policy loss: 2.0001191927089144
Experience 1, Iter 81, disc loss: 0.23908646217305593, policy loss: 2.0899608501480595
Experience 1, Iter 82, disc loss: 0.22893308752552716, policy loss: 2.129937385240385
Experience 1, Iter 83, disc loss: 0.24042260137599558, policy loss: 1.9982694638709382
Experience 1, Iter 84, disc loss: 0.1904466480510864, policy loss: 2.2999618094826397
Experience 1, Iter 85, disc loss: 0.21093412900006211, policy loss: 2.2369533521425695
Experience 1, Iter 86, disc loss: 0.21061822450002088, policy loss: 2.171360343920792
Experience 1, Iter 87, disc loss: 0.21282651399099756, policy loss: 2.130708835265109
Experience 1, Iter 88, disc loss: 0.18923126009329982, policy loss: 2.3704161354069795
Experience 1, Iter 89, disc loss: 0.1984690309075286, policy loss: 2.277455074908448
Experience 1, Iter 90, disc loss: 0.1699971455202011, policy loss: 2.367654630074444
Experience 1, Iter 91, disc loss: 0.19267909320824939, policy loss: 2.4402422867319222
Experience 1, Iter 92, disc loss: 0.16939385798687817, policy loss: 2.4958953247858235
Experience 1, Iter 93, disc loss: 0.16839178145818895, policy loss: 2.540865484951595
Experience 1, Iter 94, disc loss: 0.17854375586918392, policy loss: 2.4051280416072345
Experience 1, Iter 95, disc loss: 0.16760087640431343, policy loss: 2.488923766313383
Experience 1, Iter 96, disc loss: 0.17403598678965737, policy loss: 2.4628331343260905
Experience 1, Iter 97, disc loss: 0.1672938236891615, policy loss: 2.49866940836859
Experience 1, Iter 98, disc loss: 0.13417347309071043, policy loss: 2.8240653899309773
Experience 1, Iter 99, disc loss: 0.15287409483568729, policy loss: 2.4773933312559824
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0041],
        [0.1466],
        [2.0291],
        [0.0428]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0364, 0.1735, 1.9727, 0.0319, 0.0063, 3.4807]],

        [[0.0364, 0.1735, 1.9727, 0.0319, 0.0063, 3.4807]],

        [[0.0364, 0.1735, 1.9727, 0.0319, 0.0063, 3.4807]],

        [[0.0364, 0.1735, 1.9727, 0.0319, 0.0063, 3.4807]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0163, 0.5862, 8.1164, 0.1713], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0163, 0.5862, 8.1164, 0.1713])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.649
Iter 2/2000 - Loss: 3.490
Iter 3/2000 - Loss: 3.543
Iter 4/2000 - Loss: 3.466
Iter 5/2000 - Loss: 3.441
Iter 6/2000 - Loss: 3.484
Iter 7/2000 - Loss: 3.486
Iter 8/2000 - Loss: 3.442
Iter 9/2000 - Loss: 3.418
Iter 10/2000 - Loss: 3.428
Iter 11/2000 - Loss: 3.434
Iter 12/2000 - Loss: 3.416
Iter 13/2000 - Loss: 3.393
Iter 14/2000 - Loss: 3.385
Iter 15/2000 - Loss: 3.383
Iter 16/2000 - Loss: 3.368
Iter 17/2000 - Loss: 3.343
Iter 18/2000 - Loss: 3.319
Iter 19/2000 - Loss: 3.304
Iter 20/2000 - Loss: 3.287
Iter 1981/2000 - Loss: -2.129
Iter 1982/2000 - Loss: -2.129
Iter 1983/2000 - Loss: -2.129
Iter 1984/2000 - Loss: -2.129
Iter 1985/2000 - Loss: -2.129
Iter 1986/2000 - Loss: -2.129
Iter 1987/2000 - Loss: -2.129
Iter 1988/2000 - Loss: -2.129
Iter 1989/2000 - Loss: -2.130
Iter 1990/2000 - Loss: -2.130
Iter 1991/2000 - Loss: -2.130
Iter 1992/2000 - Loss: -2.130
Iter 1993/2000 - Loss: -2.130
Iter 1994/2000 - Loss: -2.130
Iter 1995/2000 - Loss: -2.130
Iter 1996/2000 - Loss: -2.130
Iter 1997/2000 - Loss: -2.130
Iter 1998/2000 - Loss: -2.130
Iter 1999/2000 - Loss: -2.130
Iter 2000/2000 - Loss: -2.130
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0005],
        [0.0001],
        [0.0007]])
Lengthscale: tensor([[[14.5319,  5.8506, 63.0699, 13.9827, 17.8865, 56.9075]],

        [[37.2814, 54.4960, 11.0813,  3.5232,  0.7134, 17.9207]],

        [[28.8734, 61.6161, 10.1741,  0.9576,  2.6366, 16.8906]],

        [[38.3849, 64.6814, 34.0366,  9.1646, 22.6524, 56.2229]]])
Signal Variance: tensor([ 0.1059,  1.6350, 13.6138,  2.4779])
Estimated target variance: tensor([0.0163, 0.5862, 8.1164, 0.1713])
N: 20
Signal to noise ratio: tensor([ 14.0965,  58.0335, 367.3550,  60.6072])
Bound on condition number: tensor([   3975.2208,   67358.8076, 2698995.3779,   73465.7145])
Policy Optimizer learning rate:
0.09989469496904545
Experience 2, Iter 0, disc loss: 1.0309118746811203, policy loss: 1.0504435693894003
Experience 2, Iter 1, disc loss: 0.5449434990619111, policy loss: 1.689390421599783
Experience 2, Iter 2, disc loss: 0.8549982695339801, policy loss: 1.418628056552463
Experience 2, Iter 3, disc loss: 0.718102154001057, policy loss: 1.6515399692880544
Experience 2, Iter 4, disc loss: 0.4654359480551752, policy loss: 2.424810031331141
Experience 2, Iter 5, disc loss: 0.4956622952788783, policy loss: 2.3936949888464767
Experience 2, Iter 6, disc loss: 0.47000100689316904, policy loss: 2.310916206906532
Experience 2, Iter 7, disc loss: 0.4505717796335302, policy loss: 2.4013454084236767
Experience 2, Iter 8, disc loss: 0.4761053814018485, policy loss: 2.168838749447222
Experience 2, Iter 9, disc loss: 0.598262746874247, policy loss: 2.13608294724331
Experience 2, Iter 10, disc loss: 0.45110012344585854, policy loss: 2.3568885516282863
Experience 2, Iter 11, disc loss: 0.3401929743864198, policy loss: 2.5162560418558386
Experience 2, Iter 12, disc loss: 0.4694848024977393, policy loss: 2.5061525783808527
Experience 2, Iter 13, disc loss: 0.4322206464481858, policy loss: 2.675137831761062
Experience 2, Iter 14, disc loss: 0.38094918823320695, policy loss: 2.7666417865835156
Experience 2, Iter 15, disc loss: 0.46517544990326243, policy loss: 2.4809041491619315
Experience 2, Iter 16, disc loss: 0.46176022227381086, policy loss: 2.714103638471391
Experience 2, Iter 17, disc loss: 0.39906685288593685, policy loss: 2.8306853965776906
Experience 2, Iter 18, disc loss: 0.40911652678646954, policy loss: 2.831610211045831
Experience 2, Iter 19, disc loss: 0.3496185257123211, policy loss: 2.888017171126611
Experience 2, Iter 20, disc loss: 0.40793350808831724, policy loss: 2.95504096463991
Experience 2, Iter 21, disc loss: 0.36179601601642714, policy loss: 2.979998259740965
Experience 2, Iter 22, disc loss: 0.33322495086863, policy loss: 3.0006418042487133
Experience 2, Iter 23, disc loss: 0.39755978884049326, policy loss: 2.654942497033241
Experience 2, Iter 24, disc loss: 0.3530944966524851, policy loss: 3.0122230105333214
Experience 2, Iter 25, disc loss: 0.3545692082153957, policy loss: 2.9398496363840803
Experience 2, Iter 26, disc loss: 0.3296153059896375, policy loss: 3.100403007332517
Experience 2, Iter 27, disc loss: 0.29984838577802747, policy loss: 3.000675306750509
Experience 2, Iter 28, disc loss: 0.2727206823540528, policy loss: 3.0874657561797183
Experience 2, Iter 29, disc loss: 0.324069521186348, policy loss: 3.2774967845125547
Experience 2, Iter 30, disc loss: 0.3076372313616252, policy loss: 2.9638702246622364
Experience 2, Iter 31, disc loss: 0.2429433400384953, policy loss: 3.163314034768197
Experience 2, Iter 32, disc loss: 0.29387850143215594, policy loss: 3.0271307517191683
Experience 2, Iter 33, disc loss: 0.2835217772730607, policy loss: 3.122937888610906
Experience 2, Iter 34, disc loss: 0.2668464038239926, policy loss: 3.179643935599139
Experience 2, Iter 35, disc loss: 0.2715238129509482, policy loss: 3.0448294777000897
Experience 2, Iter 36, disc loss: 0.26800420076101494, policy loss: 3.1636840568525795
Experience 2, Iter 37, disc loss: 0.24615897544229665, policy loss: 3.109089528800091
Experience 2, Iter 38, disc loss: 0.27165426864570974, policy loss: 3.041905115984959
Experience 2, Iter 39, disc loss: 0.24806373255531816, policy loss: 3.2660213960744904
Experience 2, Iter 40, disc loss: 0.2538956364126118, policy loss: 3.1798042978162093
Experience 2, Iter 41, disc loss: 0.2337404656033123, policy loss: 3.2980459608529316
Experience 2, Iter 42, disc loss: 0.2093815968488582, policy loss: 3.5398733838460026
Experience 2, Iter 43, disc loss: 0.24950286393561114, policy loss: 2.9224459853768625
Experience 2, Iter 44, disc loss: 0.2453438658497949, policy loss: 3.498640312169851
Experience 2, Iter 45, disc loss: 0.2571049351006064, policy loss: 3.277673571642959
Experience 2, Iter 46, disc loss: 0.2024850436481923, policy loss: 3.509063890401596
Experience 2, Iter 47, disc loss: 0.20627511211295257, policy loss: 3.6533130688014244
Experience 2, Iter 48, disc loss: 0.232737261177332, policy loss: 3.3680011217850234
Experience 2, Iter 49, disc loss: 0.2028945342394551, policy loss: 3.639001336121379
Experience 2, Iter 50, disc loss: 0.21270355429095328, policy loss: 3.554402984759851
Experience 2, Iter 51, disc loss: 0.21713936638027503, policy loss: 3.55837279913269
Experience 2, Iter 52, disc loss: 0.16634406881892128, policy loss: 3.6583586902567373
Experience 2, Iter 53, disc loss: 0.24828851736263075, policy loss: 3.1931877028927635
Experience 2, Iter 54, disc loss: 0.16365386005647586, policy loss: 3.8373861162717637
Experience 2, Iter 55, disc loss: 0.15692507680132128, policy loss: 3.6369567851233837
Experience 2, Iter 56, disc loss: 0.22917584092589394, policy loss: 3.590335999458011
Experience 2, Iter 57, disc loss: 0.20936920591594116, policy loss: 3.5183324583459665
Experience 2, Iter 58, disc loss: 0.17609468654086327, policy loss: 3.8607017910144625
Experience 2, Iter 59, disc loss: 0.16333751277318856, policy loss: 4.111710274448089
Experience 2, Iter 60, disc loss: 0.15976777505367257, policy loss: 4.286733871558047
Experience 2, Iter 61, disc loss: 0.14570892441085803, policy loss: 3.9967895092553403
Experience 2, Iter 62, disc loss: 0.11370858029354215, policy loss: 4.004963129011245
Experience 2, Iter 63, disc loss: 0.16483707153225433, policy loss: 3.925047049530669
Experience 2, Iter 64, disc loss: 0.22734391316059033, policy loss: 3.714642032042394
Experience 2, Iter 65, disc loss: 0.13880006795655664, policy loss: 4.037540585822153
Experience 2, Iter 66, disc loss: 0.13676861159871398, policy loss: 4.117899467421857
Experience 2, Iter 67, disc loss: 0.12109279456820544, policy loss: 4.287084201030592
Experience 2, Iter 68, disc loss: 0.11029219041094188, policy loss: 4.17216415410312
Experience 2, Iter 69, disc loss: 0.148851120943541, policy loss: 4.1751641516506215
Experience 2, Iter 70, disc loss: 0.1429945281006285, policy loss: 4.407245115810204
Experience 2, Iter 71, disc loss: 0.18821394356288676, policy loss: 3.791272289475829
Experience 2, Iter 72, disc loss: 0.14920892012983789, policy loss: 3.960927550724338
Experience 2, Iter 73, disc loss: 0.12368678491154868, policy loss: 4.3680579856668915
Experience 2, Iter 74, disc loss: 0.12623056906461805, policy loss: 4.1259482045701095
Experience 2, Iter 75, disc loss: 0.11190341190751224, policy loss: 4.306846307247195
Experience 2, Iter 76, disc loss: 0.12727018052730793, policy loss: 4.298568444927284
Experience 2, Iter 77, disc loss: 0.1416680984407551, policy loss: 4.166463624618087
Experience 2, Iter 78, disc loss: 0.1172756458980563, policy loss: 4.315412500104513
Experience 2, Iter 79, disc loss: 0.13037005230832366, policy loss: 4.050439433343701
Experience 2, Iter 80, disc loss: 0.14306776717999634, policy loss: 4.330225758361403
Experience 2, Iter 81, disc loss: 0.1314000401470144, policy loss: 4.086486089953373
Experience 2, Iter 82, disc loss: 0.14299805646421251, policy loss: 4.247192873633293
Experience 2, Iter 83, disc loss: 0.13501603288535735, policy loss: 4.4721543024595825
Experience 2, Iter 84, disc loss: 0.10425729850040177, policy loss: 4.311064960652874
Experience 2, Iter 85, disc loss: 0.11395757108083926, policy loss: 4.309140998470017
Experience 2, Iter 86, disc loss: 0.0732078284345736, policy loss: 4.630272335290323
Experience 2, Iter 87, disc loss: 0.12067727993527363, policy loss: 4.548810405047999
Experience 2, Iter 88, disc loss: 0.11506011203918788, policy loss: 4.498556725243281
Experience 2, Iter 89, disc loss: 0.15321029823449775, policy loss: 4.257133573904143
Experience 2, Iter 90, disc loss: 0.1787373006796027, policy loss: 4.364512070265388
Experience 2, Iter 91, disc loss: 0.16677013436496424, policy loss: 4.33035992224837
Experience 2, Iter 92, disc loss: 0.1053345025380544, policy loss: 4.952755664167463
Experience 2, Iter 93, disc loss: 0.08962279124185157, policy loss: 4.841502521526109
Experience 2, Iter 94, disc loss: 0.1443395737259668, policy loss: 4.464285202418669
Experience 2, Iter 95, disc loss: 0.1291146423477408, policy loss: 5.0741283461280595
Experience 2, Iter 96, disc loss: 0.11601924656569658, policy loss: 4.738755926144238
Experience 2, Iter 97, disc loss: 0.11874986906594909, policy loss: 4.66424787040152
Experience 2, Iter 98, disc loss: 0.15164537148012797, policy loss: 4.755806312842395
Experience 2, Iter 99, disc loss: 0.11099172134941654, policy loss: 4.65272011505731
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0074],
        [0.1514],
        [1.9567],
        [0.0382]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0516, 0.2989, 1.7536, 0.0315, 0.0080, 3.9827]],

        [[0.0516, 0.2989, 1.7536, 0.0315, 0.0080, 3.9827]],

        [[0.0516, 0.2989, 1.7536, 0.0315, 0.0080, 3.9827]],

        [[0.0516, 0.2989, 1.7536, 0.0315, 0.0080, 3.9827]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0298, 0.6054, 7.8267, 0.1526], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0298, 0.6054, 7.8267, 0.1526])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.120
Iter 2/2000 - Loss: 3.816
Iter 3/2000 - Loss: 3.785
Iter 4/2000 - Loss: 3.827
Iter 5/2000 - Loss: 3.827
Iter 6/2000 - Loss: 3.789
Iter 7/2000 - Loss: 3.740
Iter 8/2000 - Loss: 3.709
Iter 9/2000 - Loss: 3.713
Iter 10/2000 - Loss: 3.729
Iter 11/2000 - Loss: 3.722
Iter 12/2000 - Loss: 3.683
Iter 13/2000 - Loss: 3.636
Iter 14/2000 - Loss: 3.607
Iter 15/2000 - Loss: 3.599
Iter 16/2000 - Loss: 3.594
Iter 17/2000 - Loss: 3.574
Iter 18/2000 - Loss: 3.535
Iter 19/2000 - Loss: 3.486
Iter 20/2000 - Loss: 3.437
Iter 1981/2000 - Loss: -3.016
Iter 1982/2000 - Loss: -3.016
Iter 1983/2000 - Loss: -3.016
Iter 1984/2000 - Loss: -3.016
Iter 1985/2000 - Loss: -3.016
Iter 1986/2000 - Loss: -3.016
Iter 1987/2000 - Loss: -3.016
Iter 1988/2000 - Loss: -3.017
Iter 1989/2000 - Loss: -3.017
Iter 1990/2000 - Loss: -3.017
Iter 1991/2000 - Loss: -3.017
Iter 1992/2000 - Loss: -3.017
Iter 1993/2000 - Loss: -3.017
Iter 1994/2000 - Loss: -3.017
Iter 1995/2000 - Loss: -3.017
Iter 1996/2000 - Loss: -3.017
Iter 1997/2000 - Loss: -3.017
Iter 1998/2000 - Loss: -3.017
Iter 1999/2000 - Loss: -3.017
Iter 2000/2000 - Loss: -3.017
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0007],
        [0.0001],
        [0.0005]])
Lengthscale: tensor([[[22.4517,  7.7018, 43.0048, 26.8301, 19.8445, 73.6085]],

        [[36.5537, 55.2530, 11.0344,  1.3988,  2.7905, 22.2933]],

        [[44.1310, 69.9118, 11.7773,  0.9673, 12.6163, 17.7506]],

        [[46.2196, 68.3091, 28.7617,  7.4950,  1.6501, 55.5869]]])
Signal Variance: tensor([ 0.2005,  2.1538, 15.6437,  1.3202])
Estimated target variance: tensor([0.0298, 0.6054, 7.8267, 0.1526])
N: 30
Signal to noise ratio: tensor([ 21.2708,  57.4507, 392.6060,  51.6464])
Bound on condition number: tensor([  13574.3920,   99018.5927, 4624185.8262,   80021.4373])
Policy Optimizer learning rate:
0.09978950082958633
Experience 3, Iter 0, disc loss: 0.10125541310771943, policy loss: 4.467520345906706
Experience 3, Iter 1, disc loss: 0.09772205653099864, policy loss: 4.630698366558823
Experience 3, Iter 2, disc loss: 0.15981095032792422, policy loss: 4.21511704036245
Experience 3, Iter 3, disc loss: 0.13215908399077766, policy loss: 4.884553040799965
Experience 3, Iter 4, disc loss: 0.12334657141243544, policy loss: 4.639336442288488
Experience 3, Iter 5, disc loss: 0.11910743111490996, policy loss: 4.81753013874978
Experience 3, Iter 6, disc loss: 0.09458532870707598, policy loss: 5.276448173339153
Experience 3, Iter 7, disc loss: 0.1405347371074449, policy loss: 4.387242311578728
Experience 3, Iter 8, disc loss: 0.1149488688660269, policy loss: 5.012186108760961
Experience 3, Iter 9, disc loss: 0.137414314145205, policy loss: 4.75065644147952
Experience 3, Iter 10, disc loss: 0.1111863223324682, policy loss: 4.915110823591445
Experience 3, Iter 11, disc loss: 0.10563533235572364, policy loss: 4.945347776996165
Experience 3, Iter 12, disc loss: 0.08414604941515422, policy loss: 4.739995040518243
Experience 3, Iter 13, disc loss: 0.11685361853646542, policy loss: 4.730868246849841
Experience 3, Iter 14, disc loss: 0.10417320607130051, policy loss: 5.1104697362305025
Experience 3, Iter 15, disc loss: 0.09067985379462415, policy loss: 4.681140554950439
Experience 3, Iter 16, disc loss: 0.149518534428871, policy loss: 4.52600360854243
Experience 3, Iter 17, disc loss: 0.09182028163859166, policy loss: 5.2061195672768275
Experience 3, Iter 18, disc loss: 0.1095366386385813, policy loss: 4.898992154044332
Experience 3, Iter 19, disc loss: 0.0764906071777954, policy loss: 5.208981197915901
Experience 3, Iter 20, disc loss: 0.09819414660946826, policy loss: 4.937522078658959
Experience 3, Iter 21, disc loss: 0.11294436791829071, policy loss: 4.7672424971490726
Experience 3, Iter 22, disc loss: 0.08311931137690554, policy loss: 4.858489912690556
Experience 3, Iter 23, disc loss: 0.12316680974763154, policy loss: 4.983020900752873
Experience 3, Iter 24, disc loss: 0.12075840310267151, policy loss: 4.967421530473295
Experience 3, Iter 25, disc loss: 0.10867759898240453, policy loss: 4.921677113598259
Experience 3, Iter 26, disc loss: 0.08248887763024289, policy loss: 5.429266952409174
Experience 3, Iter 27, disc loss: 0.08507144311727993, policy loss: 5.192371635070891
Experience 3, Iter 28, disc loss: 0.048437736529462164, policy loss: 5.214824905863078
Experience 3, Iter 29, disc loss: 0.06790532710391649, policy loss: 4.896790704897482
Experience 3, Iter 30, disc loss: 0.07897994835005835, policy loss: 5.187301944693564
Experience 3, Iter 31, disc loss: 0.10960953019331332, policy loss: 4.9791238526039745
Experience 3, Iter 32, disc loss: 0.0669786520514471, policy loss: 5.443654753670925
Experience 3, Iter 33, disc loss: 0.06968549839836324, policy loss: 5.250338636658919
Experience 3, Iter 34, disc loss: 0.09299746118973848, policy loss: 4.888934863728861
Experience 3, Iter 35, disc loss: 0.06773162816571171, policy loss: 5.188539061635652
Experience 3, Iter 36, disc loss: 0.07137096561890066, policy loss: 5.539695768716795
Experience 3, Iter 37, disc loss: 0.06133008756580001, policy loss: 5.307078384569161
Experience 3, Iter 38, disc loss: 0.11356410521576307, policy loss: 5.186773631200979
Experience 3, Iter 39, disc loss: 0.07875763228630828, policy loss: 5.365625329933911
Experience 3, Iter 40, disc loss: 0.13919598728923893, policy loss: 4.7441498208212
Experience 3, Iter 41, disc loss: 0.08377489911265731, policy loss: 4.909672160839465
Experience 3, Iter 42, disc loss: 0.07945451798435836, policy loss: 5.095736308498901
Experience 3, Iter 43, disc loss: 0.04090971124185052, policy loss: 5.542321347992325
Experience 3, Iter 44, disc loss: 0.08758577148337743, policy loss: 5.503501228374273
Experience 3, Iter 45, disc loss: 0.10687107321607113, policy loss: 4.877789326819519
Experience 3, Iter 46, disc loss: 0.11240333044874916, policy loss: 4.910646930956352
Experience 3, Iter 47, disc loss: 0.051519073581464, policy loss: 5.169478473063635
Experience 3, Iter 48, disc loss: 0.05941133046315031, policy loss: 5.588683305803418
Experience 3, Iter 49, disc loss: 0.0750635690473075, policy loss: 5.49231499879459
Experience 3, Iter 50, disc loss: 0.08806220423310009, policy loss: 5.0532545225541075
Experience 3, Iter 51, disc loss: 0.05612748585139804, policy loss: 5.470228186977291
Experience 3, Iter 52, disc loss: 0.059169450532920914, policy loss: 5.388423118294838
Experience 3, Iter 53, disc loss: 0.07609908067137722, policy loss: 5.288925536113911
Experience 3, Iter 54, disc loss: 0.08460158977791937, policy loss: 5.261799077713625
Experience 3, Iter 55, disc loss: 0.13840377942624124, policy loss: 4.978265620296296
Experience 3, Iter 56, disc loss: 0.04811806796928925, policy loss: 5.746539176374568
Experience 3, Iter 57, disc loss: 0.07442914297464925, policy loss: 5.519977252794808
Experience 3, Iter 58, disc loss: 0.09608789365919661, policy loss: 5.369620987634594
Experience 3, Iter 59, disc loss: 0.10451926137855662, policy loss: 5.6687589190883285
Experience 3, Iter 60, disc loss: 0.07698172507731493, policy loss: 5.248533322470706
Experience 3, Iter 61, disc loss: 0.08044953170046881, policy loss: 5.731585391230471
Experience 3, Iter 62, disc loss: 0.10530949740485035, policy loss: 5.446139882202485
Experience 3, Iter 63, disc loss: 0.06720608906724347, policy loss: 5.50220438696995
Experience 3, Iter 64, disc loss: 0.10491885700937816, policy loss: 5.460895649951859
Experience 3, Iter 65, disc loss: 0.043732781495349274, policy loss: 5.466427943889837
Experience 3, Iter 66, disc loss: 0.0343266076814638, policy loss: 6.449945046824396
Experience 3, Iter 67, disc loss: 0.09236545203736249, policy loss: 5.605375267625935
Experience 3, Iter 68, disc loss: 0.06555595548917843, policy loss: 5.757513192329101
Experience 3, Iter 69, disc loss: 0.080086664088826, policy loss: 5.825521533546953
Experience 3, Iter 70, disc loss: 0.04375569119704634, policy loss: 5.927937343908423
Experience 3, Iter 71, disc loss: 0.08332293486711631, policy loss: 5.546016374375214
Experience 3, Iter 72, disc loss: 0.08941901604085686, policy loss: 5.484537409993244
Experience 3, Iter 73, disc loss: 0.03987007550728612, policy loss: 5.7647811633514365
Experience 3, Iter 74, disc loss: 0.06150748527584807, policy loss: 5.3544444551676555
Experience 3, Iter 75, disc loss: 0.05923086087665652, policy loss: 5.576145176666845
Experience 3, Iter 76, disc loss: 0.08038886292818924, policy loss: 5.287371428731419
Experience 3, Iter 77, disc loss: 0.054808938338243005, policy loss: 5.475285020424267
Experience 3, Iter 78, disc loss: 0.07376420420404507, policy loss: 5.4184470910547216
Experience 3, Iter 79, disc loss: 0.09027572303300826, policy loss: 5.64235877520563
Experience 3, Iter 80, disc loss: 0.052175174286886644, policy loss: 5.616133931048497
Experience 3, Iter 81, disc loss: 0.08797017640660096, policy loss: 5.831683573942332
Experience 3, Iter 82, disc loss: 0.06839218479095105, policy loss: 5.171169536714048
Experience 3, Iter 83, disc loss: 0.052694021879033145, policy loss: 6.0095860283259075
Experience 3, Iter 84, disc loss: 0.04091167844418186, policy loss: 5.68909235481579
Experience 3, Iter 85, disc loss: 0.051316219884458884, policy loss: 5.943845499928489
Experience 3, Iter 86, disc loss: 0.052404574872900955, policy loss: 5.795607178934942
Experience 3, Iter 87, disc loss: 0.10386951627810768, policy loss: 5.348355908876528
Experience 3, Iter 88, disc loss: 0.04400533150287057, policy loss: 5.914590379069474
Experience 3, Iter 89, disc loss: 0.07616772962110639, policy loss: 5.599583718921796
Experience 3, Iter 90, disc loss: 0.058812011564917534, policy loss: 5.844784141443846
Experience 3, Iter 91, disc loss: 0.08192282955147628, policy loss: 5.488344578554129
Experience 3, Iter 92, disc loss: 0.1009047042540402, policy loss: 5.6860601566566
Experience 3, Iter 93, disc loss: 0.04738893767850459, policy loss: 5.99693813322159
Experience 3, Iter 94, disc loss: 0.05653652273858562, policy loss: 5.975137535408496
Experience 3, Iter 95, disc loss: 0.05592994811418825, policy loss: 5.809452723659939
Experience 3, Iter 96, disc loss: 0.09401657936437563, policy loss: 5.684948407896053
Experience 3, Iter 97, disc loss: 0.045388245374538505, policy loss: 6.222880938671986
Experience 3, Iter 98, disc loss: 0.03752235507924628, policy loss: 6.302395218112894
Experience 3, Iter 99, disc loss: 0.04260447494081025, policy loss: 6.362384973991592
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0077],
        [0.1911],
        [2.2932],
        [0.0321]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0449, 0.3162, 1.5605, 0.0271, 0.0085, 4.6183]],

        [[0.0449, 0.3162, 1.5605, 0.0271, 0.0085, 4.6183]],

        [[0.0449, 0.3162, 1.5605, 0.0271, 0.0085, 4.6183]],

        [[0.0449, 0.3162, 1.5605, 0.0271, 0.0085, 4.6183]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0310, 0.7645, 9.1729, 0.1285], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0310, 0.7645, 9.1729, 0.1285])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.322
Iter 2/2000 - Loss: 3.980
Iter 3/2000 - Loss: 3.935
Iter 4/2000 - Loss: 3.982
Iter 5/2000 - Loss: 3.995
Iter 6/2000 - Loss: 3.962
Iter 7/2000 - Loss: 3.915
Iter 8/2000 - Loss: 3.873
Iter 9/2000 - Loss: 3.858
Iter 10/2000 - Loss: 3.869
Iter 11/2000 - Loss: 3.881
Iter 12/2000 - Loss: 3.865
Iter 13/2000 - Loss: 3.819
Iter 14/2000 - Loss: 3.763
Iter 15/2000 - Loss: 3.717
Iter 16/2000 - Loss: 3.689
Iter 17/2000 - Loss: 3.666
Iter 18/2000 - Loss: 3.630
Iter 19/2000 - Loss: 3.573
Iter 20/2000 - Loss: 3.500
Iter 1981/2000 - Loss: -3.737
Iter 1982/2000 - Loss: -3.737
Iter 1983/2000 - Loss: -3.737
Iter 1984/2000 - Loss: -3.737
Iter 1985/2000 - Loss: -3.737
Iter 1986/2000 - Loss: -3.737
Iter 1987/2000 - Loss: -3.737
Iter 1988/2000 - Loss: -3.737
Iter 1989/2000 - Loss: -3.737
Iter 1990/2000 - Loss: -3.737
Iter 1991/2000 - Loss: -3.737
Iter 1992/2000 - Loss: -3.738
Iter 1993/2000 - Loss: -3.738
Iter 1994/2000 - Loss: -3.738
Iter 1995/2000 - Loss: -3.738
Iter 1996/2000 - Loss: -3.738
Iter 1997/2000 - Loss: -3.738
Iter 1998/2000 - Loss: -3.738
Iter 1999/2000 - Loss: -3.738
Iter 2000/2000 - Loss: -3.738
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0034],
        [0.0004]])
Lengthscale: tensor([[[25.9690,  9.1160, 53.6541, 17.3538, 19.7812, 71.6632]],

        [[36.0659, 55.4845, 10.6977,  1.4119,  3.1131, 26.5677]],

        [[26.2954, 46.5091, 11.0320,  1.0026,  1.9362, 21.3796]],

        [[37.3770, 58.8740, 28.2264,  6.3732,  1.6371, 53.0474]]])
Signal Variance: tensor([ 0.1971,  2.3291, 15.6819,  1.1448])
Estimated target variance: tensor([0.0310, 0.7645, 9.1729, 0.1285])
N: 40
Signal to noise ratio: tensor([20.2812, 60.6617, 68.0150, 50.7011])
Bound on condition number: tensor([ 16454.1569, 147194.8846, 185042.3416, 102824.9259])
Policy Optimizer learning rate:
0.09968441746484834
Experience 4, Iter 0, disc loss: 0.05123586508879205, policy loss: 6.369762194142918
Experience 4, Iter 1, disc loss: 0.07662933736362545, policy loss: 5.683764004528427
Experience 4, Iter 2, disc loss: 0.11973671002743842, policy loss: 5.67044184427961
Experience 4, Iter 3, disc loss: 0.06716384275344683, policy loss: 5.994439174310629
Experience 4, Iter 4, disc loss: 0.07423623008797277, policy loss: 5.799462338214292
Experience 4, Iter 5, disc loss: 0.08142733544629968, policy loss: 6.063128629333922
Experience 4, Iter 6, disc loss: 0.05109504692940886, policy loss: 6.4439319522315675
Experience 4, Iter 7, disc loss: 0.033509508838462, policy loss: 6.532866327041549
Experience 4, Iter 8, disc loss: 0.031026423580344957, policy loss: 6.073124100979776
Experience 4, Iter 9, disc loss: 0.023844615489917342, policy loss: 7.075029627050833
Experience 4, Iter 10, disc loss: 0.029958539005597556, policy loss: 6.64702682913085
Experience 4, Iter 11, disc loss: 0.05017089654290657, policy loss: 6.394626652110016
Experience 4, Iter 12, disc loss: 0.039109330672333756, policy loss: 6.344446944341197
Experience 4, Iter 13, disc loss: 0.045117910432261335, policy loss: 6.051903576606884
Experience 4, Iter 14, disc loss: 0.0257288865384595, policy loss: 6.940426633800252
Experience 4, Iter 15, disc loss: 0.04666975086052389, policy loss: 6.38294322684748
Experience 4, Iter 16, disc loss: 0.050618567264254524, policy loss: 6.3859309950484535
Experience 4, Iter 17, disc loss: 0.05551028451605651, policy loss: 6.467693296455099
Experience 4, Iter 18, disc loss: 0.09922692003630022, policy loss: 6.460090855361702
Experience 4, Iter 19, disc loss: 0.024185792585533303, policy loss: 6.530366352981183
Experience 4, Iter 20, disc loss: 0.039371155786289275, policy loss: 6.472557705742055
Experience 4, Iter 21, disc loss: 0.03158380788450413, policy loss: 6.4902861183515075
Experience 4, Iter 22, disc loss: 0.03777056156198973, policy loss: 6.296437115129205
Experience 4, Iter 23, disc loss: 0.03074974492430234, policy loss: 6.410764719442151
Experience 4, Iter 24, disc loss: 0.05517538709830074, policy loss: 6.346191045206687
Experience 4, Iter 25, disc loss: 0.031014317727373193, policy loss: 6.703522201612116
Experience 4, Iter 26, disc loss: 0.037371058482767006, policy loss: 6.643667854210413
Experience 4, Iter 27, disc loss: 0.05077041569349085, policy loss: 6.289508605065475
Experience 4, Iter 28, disc loss: 0.11164068764868508, policy loss: 6.260433553992083
Experience 4, Iter 29, disc loss: 0.10146838295421484, policy loss: 6.3025536764555135
Experience 4, Iter 30, disc loss: 0.08820848622823879, policy loss: 6.521128157362002
Experience 4, Iter 31, disc loss: 0.030655241277025146, policy loss: 6.665167448599821
Experience 4, Iter 32, disc loss: 0.05361306092624071, policy loss: 6.449270185991038
Experience 4, Iter 33, disc loss: 0.0461355562675893, policy loss: 6.246912645600116
Experience 4, Iter 34, disc loss: 0.02109165852829121, policy loss: 6.85740344518345
Experience 4, Iter 35, disc loss: 0.05036208105346489, policy loss: 6.83644680569774
Experience 4, Iter 36, disc loss: 0.06399639440799773, policy loss: 6.88984913181569
Experience 4, Iter 37, disc loss: 0.05256274125691473, policy loss: 7.111691425180813
Experience 4, Iter 38, disc loss: 0.042463847797880386, policy loss: 6.949375786827146
Experience 4, Iter 39, disc loss: 0.03853149133029156, policy loss: 6.744756487326854
Experience 4, Iter 40, disc loss: 0.03742133189380406, policy loss: 6.8477891671729125
Experience 4, Iter 41, disc loss: 0.04730627188866887, policy loss: 6.744579903240218
Experience 4, Iter 42, disc loss: 0.03398997048042468, policy loss: 6.69625396010822
Experience 4, Iter 43, disc loss: 0.05897985696651274, policy loss: 7.163834130201955
Experience 4, Iter 44, disc loss: 0.04507047948405447, policy loss: 6.648397778585999
Experience 4, Iter 45, disc loss: 0.03356092111308232, policy loss: 7.020174526986558
Experience 4, Iter 46, disc loss: 0.03845458869426566, policy loss: 7.069380576251442
Experience 4, Iter 47, disc loss: 0.023376648918075178, policy loss: 7.103883269440674
Experience 4, Iter 48, disc loss: 0.031743296053251636, policy loss: 6.785467734106328
Experience 4, Iter 49, disc loss: 0.052066957769303035, policy loss: 6.5656187933689685
Experience 4, Iter 50, disc loss: 0.055108575782190264, policy loss: 6.438458501011173
Experience 4, Iter 51, disc loss: 0.043705250234293634, policy loss: 7.096603587658555
Experience 4, Iter 52, disc loss: 0.021342063901041916, policy loss: 7.3718404328439755
Experience 4, Iter 53, disc loss: 0.029738948253356472, policy loss: 7.04808488900151
Experience 4, Iter 54, disc loss: 0.039229488859548065, policy loss: 7.011934025098017
Experience 4, Iter 55, disc loss: 0.07185136825436324, policy loss: 6.60705777366391
Experience 4, Iter 56, disc loss: 0.026591080890583544, policy loss: 7.197248408274309
Experience 4, Iter 57, disc loss: 0.016064386289593002, policy loss: 7.4463017212422615
Experience 4, Iter 58, disc loss: 0.08147576329389242, policy loss: 6.853039851691362
Experience 4, Iter 59, disc loss: 0.03306301391452678, policy loss: 7.03538558990177
Experience 4, Iter 60, disc loss: 0.02476493654294104, policy loss: 6.9783740129759035
Experience 4, Iter 61, disc loss: 0.020833807132005114, policy loss: 7.991189741179507
Experience 4, Iter 62, disc loss: 0.031150577138732656, policy loss: 7.1437321772249724
Experience 4, Iter 63, disc loss: 0.028424236257614706, policy loss: 7.331110323465687
Experience 4, Iter 64, disc loss: 0.03329834546551429, policy loss: 7.219737446088457
Experience 4, Iter 65, disc loss: 0.09734160498845561, policy loss: 6.711234052201185
Experience 4, Iter 66, disc loss: 0.016238412160523914, policy loss: 7.691224285633294
Experience 4, Iter 67, disc loss: 0.019926593519943754, policy loss: 7.551398862647707
Experience 4, Iter 68, disc loss: 0.028022466080440178, policy loss: 6.9857924063842525
Experience 4, Iter 69, disc loss: 0.08884080596049523, policy loss: 7.265645162954798
Experience 4, Iter 70, disc loss: 0.029214522276521573, policy loss: 7.116612723380348
Experience 4, Iter 71, disc loss: 0.0161723592885218, policy loss: 7.71553276055713
Experience 4, Iter 72, disc loss: 0.016298396209055054, policy loss: 7.449127474921411
Experience 4, Iter 73, disc loss: 0.03132822197111255, policy loss: 7.306849291428776
Experience 4, Iter 74, disc loss: 0.02211239427111615, policy loss: 7.342315745024864
Experience 4, Iter 75, disc loss: 0.045635855092919356, policy loss: 6.976092777177998
Experience 4, Iter 76, disc loss: 0.02759066943131045, policy loss: 6.8485624493327775
Experience 4, Iter 77, disc loss: 0.02055207073042095, policy loss: 7.203028138399158
Experience 4, Iter 78, disc loss: 0.025685647519497132, policy loss: 7.201838484351902
Experience 4, Iter 79, disc loss: 0.03322499298266538, policy loss: 7.105701104822211
Experience 4, Iter 80, disc loss: 0.05591326591581764, policy loss: 7.269288765395264
Experience 4, Iter 81, disc loss: 0.029114696999421725, policy loss: 7.828837963216998
Experience 4, Iter 82, disc loss: 0.021575085060161522, policy loss: 7.365041692608952
Experience 4, Iter 83, disc loss: 0.02794106734024122, policy loss: 7.117788907004247
Experience 4, Iter 84, disc loss: 0.032417895142095574, policy loss: 7.097316678396389
Experience 4, Iter 85, disc loss: 0.07362055439201433, policy loss: 7.507299847418865
Experience 4, Iter 86, disc loss: 0.04793271313025191, policy loss: 6.476958024325954
Experience 4, Iter 87, disc loss: 0.051017061419271426, policy loss: 7.0907923961622465
Experience 4, Iter 88, disc loss: 0.0699323101014527, policy loss: 7.262284668231445
Experience 4, Iter 89, disc loss: 0.019951428203345934, policy loss: 7.17683939713507
Experience 4, Iter 90, disc loss: 0.03237886320745413, policy loss: 7.258619499032054
Experience 4, Iter 91, disc loss: 0.041923993992964564, policy loss: 7.139931441096394
Experience 4, Iter 92, disc loss: 0.021799321502438598, policy loss: 8.031884497859433
Experience 4, Iter 93, disc loss: 0.03971775432603744, policy loss: 7.110231777760624
Experience 4, Iter 94, disc loss: 0.0751289667562868, policy loss: 7.416365648104936
Experience 4, Iter 95, disc loss: 0.07107876029656437, policy loss: 7.169332863032523
Experience 4, Iter 96, disc loss: 0.03559559755154853, policy loss: 7.258928879141332
Experience 4, Iter 97, disc loss: 0.05069375863183225, policy loss: 7.180422645734979
Experience 4, Iter 98, disc loss: 0.0533153807575929, policy loss: 6.93897312359059
Experience 4, Iter 99, disc loss: 0.020671039787218327, policy loss: 7.822084985085868
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0068],
        [0.2245],
        [2.4388],
        [0.0282]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0389, 0.2817, 1.3991, 0.0253, 0.0075, 5.3077]],

        [[0.0389, 0.2817, 1.3991, 0.0253, 0.0075, 5.3077]],

        [[0.0389, 0.2817, 1.3991, 0.0253, 0.0075, 5.3077]],

        [[0.0389, 0.2817, 1.3991, 0.0253, 0.0075, 5.3077]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0272, 0.8979, 9.7552, 0.1128], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0272, 0.8979, 9.7552, 0.1128])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.231
Iter 2/2000 - Loss: 3.918
Iter 3/2000 - Loss: 3.912
Iter 4/2000 - Loss: 3.958
Iter 5/2000 - Loss: 3.941
Iter 6/2000 - Loss: 3.887
Iter 7/2000 - Loss: 3.835
Iter 8/2000 - Loss: 3.803
Iter 9/2000 - Loss: 3.799
Iter 10/2000 - Loss: 3.801
Iter 11/2000 - Loss: 3.778
Iter 12/2000 - Loss: 3.724
Iter 13/2000 - Loss: 3.658
Iter 14/2000 - Loss: 3.603
Iter 15/2000 - Loss: 3.564
Iter 16/2000 - Loss: 3.526
Iter 17/2000 - Loss: 3.472
Iter 18/2000 - Loss: 3.395
Iter 19/2000 - Loss: 3.305
Iter 20/2000 - Loss: 3.212
Iter 1981/2000 - Loss: -4.436
Iter 1982/2000 - Loss: -4.436
Iter 1983/2000 - Loss: -4.436
Iter 1984/2000 - Loss: -4.436
Iter 1985/2000 - Loss: -4.436
Iter 1986/2000 - Loss: -4.436
Iter 1987/2000 - Loss: -4.436
Iter 1988/2000 - Loss: -4.436
Iter 1989/2000 - Loss: -4.436
Iter 1990/2000 - Loss: -4.436
Iter 1991/2000 - Loss: -4.436
Iter 1992/2000 - Loss: -4.437
Iter 1993/2000 - Loss: -4.437
Iter 1994/2000 - Loss: -4.437
Iter 1995/2000 - Loss: -4.437
Iter 1996/2000 - Loss: -4.437
Iter 1997/2000 - Loss: -4.437
Iter 1998/2000 - Loss: -4.437
Iter 1999/2000 - Loss: -4.437
Iter 2000/2000 - Loss: -4.437
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0032],
        [0.0005]])
Lengthscale: tensor([[[22.0961,  8.7549, 39.5453, 22.2434, 15.3907, 61.1438]],

        [[34.9805, 54.6079, 11.7504,  1.3808,  3.9644, 29.3895]],

        [[25.8630, 45.3963, 10.5274,  1.0358,  1.5608, 22.5046]],

        [[32.6163, 53.2920, 24.8502,  5.7620,  1.5654, 58.1462]]])
Signal Variance: tensor([ 0.2032,  2.7740, 14.8855,  0.9642])
Estimated target variance: tensor([0.0272, 0.8979, 9.7552, 0.1128])
N: 50
Signal to noise ratio: tensor([21.1255, 67.0294, 68.5726, 43.8567])
Bound on condition number: tensor([ 22315.3526, 224647.9838, 235110.9339,  96171.4486])
Policy Optimizer learning rate:
0.0995794447581801
Experience 5, Iter 0, disc loss: 0.055444710050282474, policy loss: 6.9732640898222495
Experience 5, Iter 1, disc loss: 0.03506091552097683, policy loss: 7.706103864620349
Experience 5, Iter 2, disc loss: 0.024330436523374792, policy loss: 7.831858163367796
Experience 5, Iter 3, disc loss: 0.018115878752920745, policy loss: 7.662858438799161
Experience 5, Iter 4, disc loss: 0.02568377886682297, policy loss: 7.0638196437248935
Experience 5, Iter 5, disc loss: 0.05172511497698526, policy loss: 7.139987823791056
Experience 5, Iter 6, disc loss: 0.05490180741583493, policy loss: 7.415831332818767
Experience 5, Iter 7, disc loss: 0.022057117062108195, policy loss: 7.495789853152398
Experience 5, Iter 8, disc loss: 0.02657826872207854, policy loss: 7.5420500657615195
Experience 5, Iter 9, disc loss: 0.055254480227995686, policy loss: 7.282447687778765
Experience 5, Iter 10, disc loss: 0.032467028330587, policy loss: 7.535797946399193
Experience 5, Iter 11, disc loss: 0.01688497977600491, policy loss: 7.595785943439323
Experience 5, Iter 12, disc loss: 0.04780188003228772, policy loss: 7.793862620936063
Experience 5, Iter 13, disc loss: 0.017646116842369993, policy loss: 7.710046575097084
Experience 5, Iter 14, disc loss: 0.05621572739717147, policy loss: 7.145965063273987
Experience 5, Iter 15, disc loss: 0.017646054807018502, policy loss: 7.915246798098181
Experience 5, Iter 16, disc loss: 0.028915203862719986, policy loss: 7.122313382646788
Experience 5, Iter 17, disc loss: 0.0318849583769413, policy loss: 7.315511645926623
Experience 5, Iter 18, disc loss: 0.017407804901894766, policy loss: 7.856664659251134
Experience 5, Iter 19, disc loss: 0.01530565350075989, policy loss: 7.565128264895761
Experience 5, Iter 20, disc loss: 0.03778340192770825, policy loss: 7.604124174551073
Experience 5, Iter 21, disc loss: 0.022839388284738346, policy loss: 7.037202854939062
Experience 5, Iter 22, disc loss: 0.02240071563583354, policy loss: 7.487696667138574
Experience 5, Iter 23, disc loss: 0.012992295069251326, policy loss: 8.188882089891491
Experience 5, Iter 24, disc loss: 0.014797210023868152, policy loss: 7.525732338680733
Experience 5, Iter 25, disc loss: 0.025569223133274907, policy loss: 7.82788462938866
Experience 5, Iter 26, disc loss: 0.02231281254595376, policy loss: 7.96753251957453
Experience 5, Iter 27, disc loss: 0.037458251903242853, policy loss: 7.269333845427955
Experience 5, Iter 28, disc loss: 0.034241360828384165, policy loss: 7.401105518950196
Experience 5, Iter 29, disc loss: 0.030677783817453667, policy loss: 7.71396783480733
Experience 5, Iter 30, disc loss: 0.04168877067938416, policy loss: 7.437032015103947
Experience 5, Iter 31, disc loss: 0.01582631298502767, policy loss: 7.920310005392114
Experience 5, Iter 32, disc loss: 0.025498638057956272, policy loss: 7.971353334339426
Experience 5, Iter 33, disc loss: 0.011755140108753198, policy loss: 7.908543699260266
Experience 5, Iter 34, disc loss: 0.013417154283071369, policy loss: 7.548111921820713
Experience 5, Iter 35, disc loss: 0.01967707844189905, policy loss: 7.973438334935964
Experience 5, Iter 36, disc loss: 0.03049537836382457, policy loss: 7.659272476503947
Experience 5, Iter 37, disc loss: 0.01901327475841288, policy loss: 7.770507743830799
Experience 5, Iter 38, disc loss: 0.013982494938267363, policy loss: 7.949911937441494
Experience 5, Iter 39, disc loss: 0.01554458928467204, policy loss: 8.011585968681663
Experience 5, Iter 40, disc loss: 0.03594932989958302, policy loss: 7.823825020946695
Experience 5, Iter 41, disc loss: 0.013404457611787258, policy loss: 8.106193502930914
Experience 5, Iter 42, disc loss: 0.024715669515645838, policy loss: 8.033583241609898
Experience 5, Iter 43, disc loss: 0.012804450025446768, policy loss: 7.62609340800682
Experience 5, Iter 44, disc loss: 0.013756389796416697, policy loss: 7.888766194845809
Experience 5, Iter 45, disc loss: 0.02633317835548512, policy loss: 7.465461746439297
Experience 5, Iter 46, disc loss: 0.01672385239043317, policy loss: 7.738031691243602
Experience 5, Iter 47, disc loss: 0.013253696717705574, policy loss: 8.182658621765203
Experience 5, Iter 48, disc loss: 0.01312059770418091, policy loss: 8.048115133191073
Experience 5, Iter 49, disc loss: 0.012586884552989973, policy loss: 8.031078279082347
Experience 5, Iter 50, disc loss: 0.013014519882923617, policy loss: 8.06164534050127
Experience 5, Iter 51, disc loss: 0.02175599801938096, policy loss: 7.7805823717837335
Experience 5, Iter 52, disc loss: 0.011464013917196817, policy loss: 8.250257072065308
Experience 5, Iter 53, disc loss: 0.01468460917420066, policy loss: 7.515044000933518
Experience 5, Iter 54, disc loss: 0.01784267979561773, policy loss: 8.053183131337637
Experience 5, Iter 55, disc loss: 0.022887981479421053, policy loss: 7.947708819547573
Experience 5, Iter 56, disc loss: 0.015296936445645013, policy loss: 7.806300388925836
Experience 5, Iter 57, disc loss: 0.0194214121968423, policy loss: 8.154931798358547
Experience 5, Iter 58, disc loss: 0.02050487223996123, policy loss: 8.694252126211328
Experience 5, Iter 59, disc loss: 0.018559557793564958, policy loss: 8.109439457660594
Experience 5, Iter 60, disc loss: 0.03166090901327771, policy loss: 8.121918689821689
Experience 5, Iter 61, disc loss: 0.018324393180119677, policy loss: 8.039962624049625
Experience 5, Iter 62, disc loss: 0.025505072089810888, policy loss: 8.066486432987034
Experience 5, Iter 63, disc loss: 0.015367171788712934, policy loss: 8.046877210464974
Experience 5, Iter 64, disc loss: 0.029297789356298794, policy loss: 8.253572342451076
Experience 5, Iter 65, disc loss: 0.0374141636910841, policy loss: 7.861904111276367
Experience 5, Iter 66, disc loss: 0.023598728285545375, policy loss: 8.073089613230035
Experience 5, Iter 67, disc loss: 0.010785744817107288, policy loss: 8.18542169472935
Experience 5, Iter 68, disc loss: 0.025240018397772755, policy loss: 7.824430703766295
Experience 5, Iter 69, disc loss: 0.03216481222989418, policy loss: 8.402094658076225
Experience 5, Iter 70, disc loss: 0.00961839049967487, policy loss: 8.596660444162055
Experience 5, Iter 71, disc loss: 0.012588088425849034, policy loss: 8.227481815110831
Experience 5, Iter 72, disc loss: 0.032597871768426864, policy loss: 7.661057460405138
Experience 5, Iter 73, disc loss: 0.04956055380368313, policy loss: 8.3028300895084
Experience 5, Iter 74, disc loss: 0.012064885110737542, policy loss: 8.45593218069689
Experience 5, Iter 75, disc loss: 0.010672700906136505, policy loss: 7.860691536650748
Experience 5, Iter 76, disc loss: 0.02318234123797321, policy loss: 8.544174783045388
Experience 5, Iter 77, disc loss: 0.0127834953412253, policy loss: 8.060967775858837
Experience 5, Iter 78, disc loss: 0.011257250952381871, policy loss: 8.356528640760523
Experience 5, Iter 79, disc loss: 0.018736504800552778, policy loss: 7.95938732594822
Experience 5, Iter 80, disc loss: 0.01173692620972043, policy loss: 8.040564284693069
Experience 5, Iter 81, disc loss: 0.013543881015021953, policy loss: 8.29402814572626
Experience 5, Iter 82, disc loss: 0.027930902406334224, policy loss: 7.705738170374449
Experience 5, Iter 83, disc loss: 0.011994342462152082, policy loss: 7.523057160543135
Experience 5, Iter 84, disc loss: 0.02351747859403166, policy loss: 7.768359969549808
Experience 5, Iter 85, disc loss: 0.021438549198943835, policy loss: 7.73699142118235
Experience 5, Iter 86, disc loss: 0.022098849226524976, policy loss: 7.920529574826867
Experience 5, Iter 87, disc loss: 0.018577511333072408, policy loss: 7.285580537204062
Experience 5, Iter 88, disc loss: 0.015922589622312786, policy loss: 7.794664739404155
Experience 5, Iter 89, disc loss: 0.02779398553644177, policy loss: 7.552272167263869
Experience 5, Iter 90, disc loss: 0.07722789972912263, policy loss: 7.642196062704601
Experience 5, Iter 91, disc loss: 0.042270747580524994, policy loss: 8.067472346638809
Experience 5, Iter 92, disc loss: 0.014211200761804205, policy loss: 8.339977106459814
Experience 5, Iter 93, disc loss: 0.025857965786547213, policy loss: 8.209713912188437
Experience 5, Iter 94, disc loss: 0.02747730780136126, policy loss: 8.132134969998532
Experience 5, Iter 95, disc loss: 0.01202285298587601, policy loss: 8.70228793614331
Experience 5, Iter 96, disc loss: 0.012323580030833264, policy loss: 8.188965997716023
Experience 5, Iter 97, disc loss: 0.035955407854785654, policy loss: 8.34880254321878
Experience 5, Iter 98, disc loss: 0.011202100015321449, policy loss: 7.965482277748643
Experience 5, Iter 99, disc loss: 0.03978251845595714, policy loss: 7.307671301140572
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0062],
        [0.2338],
        [2.5141],
        [0.0315]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0345, 0.2620, 1.5015, 0.0269, 0.0088, 5.6140]],

        [[0.0345, 0.2620, 1.5015, 0.0269, 0.0088, 5.6140]],

        [[0.0345, 0.2620, 1.5015, 0.0269, 0.0088, 5.6140]],

        [[0.0345, 0.2620, 1.5015, 0.0269, 0.0088, 5.6140]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0248,  0.9354, 10.0565,  0.1261], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0248,  0.9354, 10.0565,  0.1261])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.268
Iter 2/2000 - Loss: 3.965
Iter 3/2000 - Loss: 3.969
Iter 4/2000 - Loss: 4.018
Iter 5/2000 - Loss: 3.998
Iter 6/2000 - Loss: 3.942
Iter 7/2000 - Loss: 3.887
Iter 8/2000 - Loss: 3.860
Iter 9/2000 - Loss: 3.864
Iter 10/2000 - Loss: 3.864
Iter 11/2000 - Loss: 3.828
Iter 12/2000 - Loss: 3.761
Iter 13/2000 - Loss: 3.692
Iter 14/2000 - Loss: 3.641
Iter 15/2000 - Loss: 3.604
Iter 16/2000 - Loss: 3.560
Iter 17/2000 - Loss: 3.491
Iter 18/2000 - Loss: 3.400
Iter 19/2000 - Loss: 3.301
Iter 20/2000 - Loss: 3.202
Iter 1981/2000 - Loss: -4.880
Iter 1982/2000 - Loss: -4.880
Iter 1983/2000 - Loss: -4.880
Iter 1984/2000 - Loss: -4.880
Iter 1985/2000 - Loss: -4.880
Iter 1986/2000 - Loss: -4.880
Iter 1987/2000 - Loss: -4.880
Iter 1988/2000 - Loss: -4.880
Iter 1989/2000 - Loss: -4.880
Iter 1990/2000 - Loss: -4.880
Iter 1991/2000 - Loss: -4.880
Iter 1992/2000 - Loss: -4.880
Iter 1993/2000 - Loss: -4.880
Iter 1994/2000 - Loss: -4.881
Iter 1995/2000 - Loss: -4.881
Iter 1996/2000 - Loss: -4.881
Iter 1997/2000 - Loss: -4.881
Iter 1998/2000 - Loss: -4.881
Iter 1999/2000 - Loss: -4.881
Iter 2000/2000 - Loss: -4.881
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0006],
        [0.0024],
        [0.0006]])
Lengthscale: tensor([[[20.5113,  8.8012, 40.1562, 19.6538, 16.7499, 64.7601]],

        [[33.6411, 51.2102, 11.7461,  1.3895,  5.1465, 28.9063]],

        [[26.9640, 46.9433,  9.8671,  1.0573,  1.8108, 21.5689]],

        [[29.6011, 48.2718, 25.6836,  5.5198,  1.9255, 51.3283]]])
Signal Variance: tensor([ 0.1912,  3.0258, 14.6112,  1.0407])
Estimated target variance: tensor([ 0.0248,  0.9354, 10.0565,  0.1261])
N: 60
Signal to noise ratio: tensor([20.5923, 73.8638, 77.3132, 42.9714])
Bound on condition number: tensor([ 25443.6259, 327352.5183, 358641.1008, 110793.4713])
Policy Optimizer learning rate:
0.09947458259305313
Experience 6, Iter 0, disc loss: 0.01045384056346147, policy loss: 8.23048065409519
Experience 6, Iter 1, disc loss: 0.017928082593043675, policy loss: 8.208927741868486
Experience 6, Iter 2, disc loss: 0.014646272015531965, policy loss: 8.511113824311412
Experience 6, Iter 3, disc loss: 0.010241001724160545, policy loss: 8.878332087983214
Experience 6, Iter 4, disc loss: 0.010464525808275707, policy loss: 8.198949517903566
Experience 6, Iter 5, disc loss: 0.014051268719463331, policy loss: 9.028382882135668
Experience 6, Iter 6, disc loss: 0.03848833488341648, policy loss: 8.432918129242685
Experience 6, Iter 7, disc loss: 0.030521149313982292, policy loss: 8.39066533158809
Experience 6, Iter 8, disc loss: 0.020551759348469374, policy loss: 8.222159528081978
Experience 6, Iter 9, disc loss: 0.03639924100480745, policy loss: 8.373239778978743
Experience 6, Iter 10, disc loss: 0.009232675668833165, policy loss: 8.586851542723249
Experience 6, Iter 11, disc loss: 0.022736227391374337, policy loss: 8.36548394112419
Experience 6, Iter 12, disc loss: 0.008280059536820827, policy loss: 9.09356194907393
Experience 6, Iter 13, disc loss: 0.015842994296046534, policy loss: 8.575148628103397
Experience 6, Iter 14, disc loss: 0.021548455391425515, policy loss: 8.527904425515837
Experience 6, Iter 15, disc loss: 0.012963335924898258, policy loss: 8.584828492075243
Experience 6, Iter 16, disc loss: 0.022183604421242065, policy loss: 8.067913068440102
Experience 6, Iter 17, disc loss: 0.01790111954875736, policy loss: 8.53190001995177
Experience 6, Iter 18, disc loss: 0.0156612274164293, policy loss: 8.165872057100671
Experience 6, Iter 19, disc loss: 0.019391742143815803, policy loss: 8.466951963384465
Experience 6, Iter 20, disc loss: 0.00903739958787142, policy loss: 8.590821238094463
Experience 6, Iter 21, disc loss: 0.012695350154988388, policy loss: 8.221899394714535
Experience 6, Iter 22, disc loss: 0.021394778408990983, policy loss: 8.776838819995465
Experience 6, Iter 23, disc loss: 0.010590508616896617, policy loss: 8.715385321882518
Experience 6, Iter 24, disc loss: 0.007781164296829041, policy loss: 8.950609319739934
Experience 6, Iter 25, disc loss: 0.028113789247240853, policy loss: 8.408623643164852
Experience 6, Iter 26, disc loss: 0.007941687313493461, policy loss: 8.706180873817303
Experience 6, Iter 27, disc loss: 0.007319264907595794, policy loss: 8.948060856768786
Experience 6, Iter 28, disc loss: 0.009339584417390176, policy loss: 8.719990460726278
Experience 6, Iter 29, disc loss: 0.010020040527014338, policy loss: 8.559158336196907
Experience 6, Iter 30, disc loss: 0.017835709041071488, policy loss: 8.003231045519124
Experience 6, Iter 31, disc loss: 0.009457383270811033, policy loss: 8.465082137307602
Experience 6, Iter 32, disc loss: 0.008575749042734239, policy loss: 8.946405076594134
Experience 6, Iter 33, disc loss: 0.006346093296019924, policy loss: 9.200108966472104
Experience 6, Iter 34, disc loss: 0.009178554546677514, policy loss: 8.994280569222665
Experience 6, Iter 35, disc loss: 0.04505072166337922, policy loss: 8.697676492410901
Experience 6, Iter 36, disc loss: 0.051944324143837664, policy loss: 7.757976457832266
Experience 6, Iter 37, disc loss: 0.007862082780782386, policy loss: 8.805315565650735
Experience 6, Iter 38, disc loss: 0.00819724635577738, policy loss: 9.023949330292986
Experience 6, Iter 39, disc loss: 0.03189553321315723, policy loss: 8.828152175928228
Experience 6, Iter 40, disc loss: 0.010373776034780811, policy loss: 8.790388437381782
Experience 6, Iter 41, disc loss: 0.006256898669749496, policy loss: 9.168958273371159
Experience 6, Iter 42, disc loss: 0.017534039173504053, policy loss: 8.720811351850404
Experience 6, Iter 43, disc loss: 0.010355693304247945, policy loss: 8.718652263929744
Experience 6, Iter 44, disc loss: 0.007361454339196341, policy loss: 8.751371826582233
Experience 6, Iter 45, disc loss: 0.011655971984794806, policy loss: 9.069577359038533
Experience 6, Iter 46, disc loss: 0.006496061795549913, policy loss: 9.338497714075057
Experience 6, Iter 47, disc loss: 0.021657161243757155, policy loss: 9.093743929585823
Experience 6, Iter 48, disc loss: 0.010347436635885745, policy loss: 8.872365692924673
Experience 6, Iter 49, disc loss: 0.07198424087249117, policy loss: 8.752893998497314
Experience 6, Iter 50, disc loss: 0.013684383446677361, policy loss: 9.066879205953342
Experience 6, Iter 51, disc loss: 0.016764153164834872, policy loss: 8.592630487193183
Experience 6, Iter 52, disc loss: 0.007611372227466063, policy loss: 9.172118267564842
Experience 6, Iter 53, disc loss: 0.009254383919820433, policy loss: 9.042935913663587
Experience 6, Iter 54, disc loss: 0.00800206688399973, policy loss: 8.921992632138563
Experience 6, Iter 55, disc loss: 0.0270604445082284, policy loss: 8.74487974183667
Experience 6, Iter 56, disc loss: 0.011018568791404164, policy loss: 9.325216814221147
Experience 6, Iter 57, disc loss: 0.00898366351988434, policy loss: 9.039159802152831
Experience 6, Iter 58, disc loss: 0.01697150755593889, policy loss: 8.210140128698399
Experience 6, Iter 59, disc loss: 0.007842837291691083, policy loss: 9.43633505225128
Experience 6, Iter 60, disc loss: 0.010626138549289536, policy loss: 9.135273301812266
Experience 6, Iter 61, disc loss: 0.01347992865117503, policy loss: 8.243695501379877
Experience 6, Iter 62, disc loss: 0.009244988336931562, policy loss: 8.188962938869032
Experience 6, Iter 63, disc loss: 0.007161391174540718, policy loss: 9.527135526925612
Experience 6, Iter 64, disc loss: 0.03284194357802101, policy loss: 8.217216629743994
Experience 6, Iter 65, disc loss: 0.029690075680838916, policy loss: 9.337652453315563
Experience 6, Iter 66, disc loss: 0.02509479706035718, policy loss: 9.234580543856115
Experience 6, Iter 67, disc loss: 0.02190570641216605, policy loss: 8.63540659920427
Experience 6, Iter 68, disc loss: 0.021208319925140627, policy loss: 7.803773637361074
Experience 6, Iter 69, disc loss: 0.008031204628073888, policy loss: 8.756099865426165
Experience 6, Iter 70, disc loss: 0.011543667604818095, policy loss: 8.578121619748522
Experience 6, Iter 71, disc loss: 0.006304041879515984, policy loss: 9.388502730591723
Experience 6, Iter 72, disc loss: 0.008810124031708316, policy loss: 9.302944052594128
Experience 6, Iter 73, disc loss: 0.00746823228982145, policy loss: 9.35500888990645
Experience 6, Iter 74, disc loss: 0.014711569369035803, policy loss: 8.997913533272559
Experience 6, Iter 75, disc loss: 0.014445361172694376, policy loss: 8.93515811514349
Experience 6, Iter 76, disc loss: 0.15725042334118364, policy loss: 8.94607539886284
Experience 6, Iter 77, disc loss: 0.008747293679279866, policy loss: 9.455963511391584
Experience 6, Iter 78, disc loss: 0.009379923272700675, policy loss: 9.552354425042948
Experience 6, Iter 79, disc loss: 0.014172829485329526, policy loss: 9.873776538730457
Experience 6, Iter 80, disc loss: 0.007015304871060702, policy loss: 9.09410475510375
Experience 6, Iter 81, disc loss: 0.008403240344717266, policy loss: 9.477893970417362
Experience 6, Iter 82, disc loss: 0.011544863689574247, policy loss: 9.012431111357044
Experience 6, Iter 83, disc loss: 0.019639758381205557, policy loss: 9.487299604563926
Experience 6, Iter 84, disc loss: 0.006784022491717749, policy loss: 9.726782271373867
Experience 6, Iter 85, disc loss: 0.01655317916427989, policy loss: 9.48715448784997
Experience 6, Iter 86, disc loss: 0.019202914155320047, policy loss: 9.074375732729845
Experience 6, Iter 87, disc loss: 0.008758415158323582, policy loss: 9.023679635663516
Experience 6, Iter 88, disc loss: 0.006444890634436507, policy loss: 9.21193493046181
Experience 6, Iter 89, disc loss: 0.009436295081763316, policy loss: 9.768957605123
Experience 6, Iter 90, disc loss: 0.019388455025375746, policy loss: 9.090662529423977
Experience 6, Iter 91, disc loss: 0.006889070458579535, policy loss: 9.891926866007589
Experience 6, Iter 92, disc loss: 0.006899519807373564, policy loss: 9.752278797759566
Experience 6, Iter 93, disc loss: 0.0061032618938004326, policy loss: 9.659790369861518
Experience 6, Iter 94, disc loss: 0.016174233689082718, policy loss: 8.883356763598053
Experience 6, Iter 95, disc loss: 0.012348327182927589, policy loss: 9.034971003240335
Experience 6, Iter 96, disc loss: 0.013325858557408757, policy loss: 9.86899506735604
Experience 6, Iter 97, disc loss: 0.006168320668285124, policy loss: 9.32211500432049
Experience 6, Iter 98, disc loss: 0.017290739155827357, policy loss: 9.057409159734963
Experience 6, Iter 99, disc loss: 0.005598960836053407, policy loss: 9.808650639354568
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0132],
        [0.2459],
        [2.5110],
        [0.0372]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0794, 0.5297, 1.7512, 0.0361, 0.0106, 6.4955]],

        [[0.0794, 0.5297, 1.7512, 0.0361, 0.0106, 6.4955]],

        [[0.0794, 0.5297, 1.7512, 0.0361, 0.0106, 6.4955]],

        [[0.0794, 0.5297, 1.7512, 0.0361, 0.0106, 6.4955]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0526,  0.9835, 10.0440,  0.1488], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0526,  0.9835, 10.0440,  0.1488])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.444
Iter 2/2000 - Loss: 4.357
Iter 3/2000 - Loss: 4.341
Iter 4/2000 - Loss: 4.289
Iter 5/2000 - Loss: 4.259
Iter 6/2000 - Loss: 4.244
Iter 7/2000 - Loss: 4.205
Iter 8/2000 - Loss: 4.148
Iter 9/2000 - Loss: 4.094
Iter 10/2000 - Loss: 4.043
Iter 11/2000 - Loss: 3.986
Iter 12/2000 - Loss: 3.920
Iter 13/2000 - Loss: 3.849
Iter 14/2000 - Loss: 3.771
Iter 15/2000 - Loss: 3.687
Iter 16/2000 - Loss: 3.592
Iter 17/2000 - Loss: 3.487
Iter 18/2000 - Loss: 3.374
Iter 19/2000 - Loss: 3.251
Iter 20/2000 - Loss: 3.119
Iter 1981/2000 - Loss: -4.794
Iter 1982/2000 - Loss: -4.794
Iter 1983/2000 - Loss: -4.794
Iter 1984/2000 - Loss: -4.794
Iter 1985/2000 - Loss: -4.794
Iter 1986/2000 - Loss: -4.795
Iter 1987/2000 - Loss: -4.795
Iter 1988/2000 - Loss: -4.795
Iter 1989/2000 - Loss: -4.795
Iter 1990/2000 - Loss: -4.795
Iter 1991/2000 - Loss: -4.795
Iter 1992/2000 - Loss: -4.795
Iter 1993/2000 - Loss: -4.795
Iter 1994/2000 - Loss: -4.795
Iter 1995/2000 - Loss: -4.795
Iter 1996/2000 - Loss: -4.795
Iter 1997/2000 - Loss: -4.795
Iter 1998/2000 - Loss: -4.795
Iter 1999/2000 - Loss: -4.795
Iter 2000/2000 - Loss: -4.795
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0006],
        [0.0026],
        [0.0006]])
Lengthscale: tensor([[[24.7913,  6.1683, 46.5196, 16.8386, 16.5881, 65.4636]],

        [[35.9286, 44.1082, 11.9184,  1.3820,  4.2712, 30.2493]],

        [[32.9464, 46.3048, 10.0214,  1.0093,  1.5711, 22.0944]],

        [[32.9951, 49.9041, 19.5123,  4.4555,  1.9462, 35.0272]]])
Signal Variance: tensor([ 0.1601,  2.8318, 13.3415,  0.5961])
Estimated target variance: tensor([ 0.0526,  0.9835, 10.0440,  0.1488])
N: 70
Signal to noise ratio: tensor([19.4900, 69.5390, 72.3245, 32.1440])
Bound on condition number: tensor([ 26591.2645, 338498.2511, 366159.0239,  72327.4404])
Policy Optimizer learning rate:
0.09936983085306159
Experience 7, Iter 0, disc loss: 0.008642932568802612, policy loss: 9.425465349327233
Experience 7, Iter 1, disc loss: 0.00610483822807613, policy loss: 9.408536188869222
Experience 7, Iter 2, disc loss: 0.025473431367148032, policy loss: 9.386728515318072
Experience 7, Iter 3, disc loss: 0.007133916570828543, policy loss: 9.483048373929584
Experience 7, Iter 4, disc loss: 0.019860871709342656, policy loss: 9.1833201020385
Experience 7, Iter 5, disc loss: 0.013632125680187797, policy loss: 9.866968868758233
Experience 7, Iter 6, disc loss: 0.02742599603821412, policy loss: 9.473766097718823
Experience 7, Iter 7, disc loss: 0.00511475268623953, policy loss: 9.783573916237172
Experience 7, Iter 8, disc loss: 0.011408709341761962, policy loss: 9.559414964285473
Experience 7, Iter 9, disc loss: 0.021640866185013076, policy loss: 9.28842515785082
Experience 7, Iter 10, disc loss: 0.018084281244888006, policy loss: 9.131919765644192
Experience 7, Iter 11, disc loss: 0.012888893896731715, policy loss: 9.345714872985285
Experience 7, Iter 12, disc loss: 0.01184951530266114, policy loss: 9.328626732669955
Experience 7, Iter 13, disc loss: 0.005358396458364723, policy loss: 9.775701332577924
Experience 7, Iter 14, disc loss: 0.011117455211864045, policy loss: 9.2173908319335
Experience 7, Iter 15, disc loss: 0.014958372381784133, policy loss: 9.406817847250233
Experience 7, Iter 16, disc loss: 0.0073095446640494, policy loss: 9.440514470659707
Experience 7, Iter 17, disc loss: 0.00744870153828113, policy loss: 9.390592452520398
Experience 7, Iter 18, disc loss: 0.019752373080125934, policy loss: 9.643216283549659
Experience 7, Iter 19, disc loss: 0.005482145338774248, policy loss: 10.560921415922833
Experience 7, Iter 20, disc loss: 0.0067895578959735765, policy loss: 9.496574593934186
Experience 7, Iter 21, disc loss: 0.00908643120030552, policy loss: 8.92778070575674
Experience 7, Iter 22, disc loss: 0.01058666894522819, policy loss: 8.940887704874624
Experience 7, Iter 23, disc loss: 0.03283299186940163, policy loss: 9.586601918529533
Experience 7, Iter 24, disc loss: 0.02101996109363568, policy loss: 9.594653253420086
Experience 7, Iter 25, disc loss: 0.007993262013823752, policy loss: 9.315911537215442
Experience 7, Iter 26, disc loss: 0.005685873281652454, policy loss: 9.278396398000341
Experience 7, Iter 27, disc loss: 0.006098728812127163, policy loss: 9.871149644700163
Experience 7, Iter 28, disc loss: 0.004835951103812215, policy loss: 9.516478625120241
Experience 7, Iter 29, disc loss: 0.005059820821925074, policy loss: 9.663634540459828
Experience 7, Iter 30, disc loss: 0.011071915115509921, policy loss: 9.728806511404898
Experience 7, Iter 31, disc loss: 0.027068612011583285, policy loss: 9.367469268225243
Experience 7, Iter 32, disc loss: 0.005372460713477304, policy loss: 9.680804992685333
Experience 7, Iter 33, disc loss: 0.010552182581217609, policy loss: 9.241511209847324
Experience 7, Iter 34, disc loss: 0.013069564580654593, policy loss: 9.695290406606706
Experience 7, Iter 35, disc loss: 0.0060453511175748145, policy loss: 9.518739374402744
Experience 7, Iter 36, disc loss: 0.005154533372894306, policy loss: 10.032420229530025
Experience 7, Iter 37, disc loss: 0.005572556983083125, policy loss: 9.902929546668322
Experience 7, Iter 38, disc loss: 0.004451834054107398, policy loss: 9.947799156309589
Experience 7, Iter 39, disc loss: 0.006097870132464881, policy loss: 9.394808938676565
Experience 7, Iter 40, disc loss: 0.004905152934155015, policy loss: 10.692854364185097
Experience 7, Iter 41, disc loss: 0.006611411397094627, policy loss: 9.704938685674549
Experience 7, Iter 42, disc loss: 0.007471991370123524, policy loss: 9.302910929277486
Experience 7, Iter 43, disc loss: 0.005426400767228141, policy loss: 10.38027415545448
Experience 7, Iter 44, disc loss: 0.037174385645161914, policy loss: 10.20461708814175
Experience 7, Iter 45, disc loss: 0.007466975720362175, policy loss: 9.881161986639627
Experience 7, Iter 46, disc loss: 0.005708589982763156, policy loss: 9.176497913555048
Experience 7, Iter 47, disc loss: 0.006776145220780558, policy loss: 9.434491585848889
Experience 7, Iter 48, disc loss: 0.004139358316693196, policy loss: 9.754680401036854
Experience 7, Iter 49, disc loss: 0.025397367980887212, policy loss: 9.669570889023284
Experience 7, Iter 50, disc loss: 0.015651146562536942, policy loss: 9.632062519934998
Experience 7, Iter 51, disc loss: 0.00513535887964154, policy loss: 9.629298687057805
Experience 7, Iter 52, disc loss: 0.005587991731466512, policy loss: 10.022837286282595
Experience 7, Iter 53, disc loss: 0.005511145751653718, policy loss: 10.033299964180905
Experience 7, Iter 54, disc loss: 0.00757736251662273, policy loss: 9.933061093529133
Experience 7, Iter 55, disc loss: 0.038444166106348134, policy loss: 9.256839637023662
Experience 7, Iter 56, disc loss: 0.00473031159570513, policy loss: 10.196114854090752
Experience 7, Iter 57, disc loss: 0.015535759697197244, policy loss: 9.556830930637144
Experience 7, Iter 58, disc loss: 0.004450847836836752, policy loss: 9.727402192451649
Experience 7, Iter 59, disc loss: 0.005607166464386094, policy loss: 9.694457198525974
Experience 7, Iter 60, disc loss: 0.0041283065371855235, policy loss: 10.061362716949457
Experience 7, Iter 61, disc loss: 0.0044296347920521895, policy loss: 10.111744538992154
Experience 7, Iter 62, disc loss: 0.004378779586172318, policy loss: 9.9792658580548
Experience 7, Iter 63, disc loss: 0.004939603224267904, policy loss: 10.021728795375346
Experience 7, Iter 64, disc loss: 0.0074233702061866635, policy loss: 9.64899481543507
Experience 7, Iter 65, disc loss: 0.004040546808943642, policy loss: 10.170126518325855
Experience 7, Iter 66, disc loss: 0.004788552879315058, policy loss: 9.364976461117298
Experience 7, Iter 67, disc loss: 0.005742457582089842, policy loss: 9.422936866184882
Experience 7, Iter 68, disc loss: 0.05621745336199759, policy loss: 9.895220901494781
Experience 7, Iter 69, disc loss: 0.017807980961069295, policy loss: 9.892849517940745
Experience 7, Iter 70, disc loss: 0.01821506605028394, policy loss: 9.326611736573113
Experience 7, Iter 71, disc loss: 0.006099172180551756, policy loss: 9.895532773225082
Experience 7, Iter 72, disc loss: 0.003943687942365849, policy loss: 10.085473518004903
Experience 7, Iter 73, disc loss: 0.007828493769963401, policy loss: 9.912367759086674
Experience 7, Iter 74, disc loss: 0.00399693544084568, policy loss: 9.961874581967685
Experience 7, Iter 75, disc loss: 0.015768743135463135, policy loss: 9.922087467757665
Experience 7, Iter 76, disc loss: 0.005371778065122756, policy loss: 9.838725157390618
Experience 7, Iter 77, disc loss: 0.051465784725398325, policy loss: 10.393718530432704
Experience 7, Iter 78, disc loss: 0.008120227674117481, policy loss: 9.534359830028972
Experience 7, Iter 79, disc loss: 0.005639497894167237, policy loss: 9.592397563003926
Experience 7, Iter 80, disc loss: 0.011610013099172643, policy loss: 9.874048730611934
Experience 7, Iter 81, disc loss: 0.010262431550886483, policy loss: 9.708505210935932
Experience 7, Iter 82, disc loss: 0.007942380209136714, policy loss: 10.046668347283623
Experience 7, Iter 83, disc loss: 0.010653266751400804, policy loss: 10.191769053863702
Experience 7, Iter 84, disc loss: 0.012872430424924516, policy loss: 9.680477476269697
Experience 7, Iter 85, disc loss: 0.009460975771253112, policy loss: 10.072794303163425
Experience 7, Iter 86, disc loss: 0.04190472780479416, policy loss: 10.12155350421343
Experience 7, Iter 87, disc loss: 0.004773086664760064, policy loss: 10.090042556922468
Experience 7, Iter 88, disc loss: 0.020071469877106837, policy loss: 9.653886068972891
Experience 7, Iter 89, disc loss: 0.02679530693582294, policy loss: 10.42551022898898
Experience 7, Iter 90, disc loss: 0.006483190573504305, policy loss: 10.236385085574405
Experience 7, Iter 91, disc loss: 0.011116220746518227, policy loss: 9.924631009305365
Experience 7, Iter 92, disc loss: 0.0076859342337924665, policy loss: 10.432051961007225
Experience 7, Iter 93, disc loss: 0.004875981813259601, policy loss: 10.540781088081538
Experience 7, Iter 94, disc loss: 0.004956400699999411, policy loss: 10.324116671501397
Experience 7, Iter 95, disc loss: 0.004661431046745468, policy loss: 10.47315455886543
Experience 7, Iter 96, disc loss: 0.03799247135571258, policy loss: 10.111889076817093
Experience 7, Iter 97, disc loss: 0.005011511003265393, policy loss: 9.780179266792548
Experience 7, Iter 98, disc loss: 0.006472603521965962, policy loss: 10.1151425456181
Experience 7, Iter 99, disc loss: 0.004738163270775659, policy loss: 10.394531300742646
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0120],
        [0.2751],
        [2.7843],
        [0.0356]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0696, 0.4831, 1.7003, 0.0327, 0.0099, 6.7729]],

        [[0.0696, 0.4831, 1.7003, 0.0327, 0.0099, 6.7729]],

        [[0.0696, 0.4831, 1.7003, 0.0327, 0.0099, 6.7729]],

        [[0.0696, 0.4831, 1.7003, 0.0327, 0.0099, 6.7729]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0479,  1.1005, 11.1372,  0.1425], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0479,  1.1005, 11.1372,  0.1425])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.462
Iter 2/2000 - Loss: 4.408
Iter 3/2000 - Loss: 4.375
Iter 4/2000 - Loss: 4.328
Iter 5/2000 - Loss: 4.315
Iter 6/2000 - Loss: 4.290
Iter 7/2000 - Loss: 4.239
Iter 8/2000 - Loss: 4.189
Iter 9/2000 - Loss: 4.141
Iter 10/2000 - Loss: 4.082
Iter 11/2000 - Loss: 4.010
Iter 12/2000 - Loss: 3.930
Iter 13/2000 - Loss: 3.844
Iter 14/2000 - Loss: 3.750
Iter 15/2000 - Loss: 3.646
Iter 16/2000 - Loss: 3.530
Iter 17/2000 - Loss: 3.405
Iter 18/2000 - Loss: 3.272
Iter 19/2000 - Loss: 3.129
Iter 20/2000 - Loss: 2.976
Iter 1981/2000 - Loss: -5.234
Iter 1982/2000 - Loss: -5.234
Iter 1983/2000 - Loss: -5.234
Iter 1984/2000 - Loss: -5.234
Iter 1985/2000 - Loss: -5.234
Iter 1986/2000 - Loss: -5.234
Iter 1987/2000 - Loss: -5.234
Iter 1988/2000 - Loss: -5.234
Iter 1989/2000 - Loss: -5.234
Iter 1990/2000 - Loss: -5.235
Iter 1991/2000 - Loss: -5.235
Iter 1992/2000 - Loss: -5.235
Iter 1993/2000 - Loss: -5.235
Iter 1994/2000 - Loss: -5.235
Iter 1995/2000 - Loss: -5.235
Iter 1996/2000 - Loss: -5.235
Iter 1997/2000 - Loss: -5.235
Iter 1998/2000 - Loss: -5.235
Iter 1999/2000 - Loss: -5.235
Iter 2000/2000 - Loss: -5.235
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0005]])
Lengthscale: tensor([[[24.0141,  6.2802, 45.0878, 16.2734, 16.6957, 65.3643]],

        [[35.2273, 44.6226, 11.4443,  1.3101,  4.0722, 27.7521]],

        [[32.9216, 48.2876, 10.3672,  1.0159,  1.6904, 18.5109]],

        [[31.7640, 47.8027, 18.6224,  4.2632,  1.8804, 36.4387]]])
Signal Variance: tensor([ 0.1597,  2.5080, 12.7129,  0.5655])
Estimated target variance: tensor([ 0.0479,  1.1005, 11.1372,  0.1425])
N: 80
Signal to noise ratio: tensor([20.3464, 68.8084, 72.2021, 33.0350])
Bound on condition number: tensor([ 33119.1227, 378768.9070, 417052.7182,  87305.6364])
Policy Optimizer learning rate:
0.09926518942192228
Experience 8, Iter 0, disc loss: 0.005475859460294967, policy loss: 10.675510352174115
Experience 8, Iter 1, disc loss: 0.006876707264208706, policy loss: 9.924822729835027
Experience 8, Iter 2, disc loss: 0.013737361684938224, policy loss: 9.509759399241975
Experience 8, Iter 3, disc loss: 0.005928796871941857, policy loss: 10.248408083232677
Experience 8, Iter 4, disc loss: 0.00569856365093589, policy loss: 9.939279524197055
Experience 8, Iter 5, disc loss: 0.004255074404516865, policy loss: 10.258244269923033
Experience 8, Iter 6, disc loss: 0.004040218794432445, policy loss: 10.244345193582594
Experience 8, Iter 7, disc loss: 0.01307292366752947, policy loss: 9.691615736699598
Experience 8, Iter 8, disc loss: 0.004663826174990059, policy loss: 10.078201971730408
Experience 8, Iter 9, disc loss: 0.004766103594596199, policy loss: 10.145139835294515
Experience 8, Iter 10, disc loss: 0.0190989328246598, policy loss: 10.484861688423347
Experience 8, Iter 11, disc loss: 0.009885021032920896, policy loss: 9.949594750824568
Experience 8, Iter 12, disc loss: 0.003592495979639042, policy loss: 10.572828663586522
Experience 8, Iter 13, disc loss: 0.0077300526264869026, policy loss: 9.529445756922092
Experience 8, Iter 14, disc loss: 0.006966146092452342, policy loss: 9.93318595035266
Experience 8, Iter 15, disc loss: 0.005509266051807885, policy loss: 10.668954407292418
Experience 8, Iter 16, disc loss: 0.005030822710461492, policy loss: 10.77693107623372
Experience 8, Iter 17, disc loss: 0.003957712586632597, policy loss: 10.083199801201925
Experience 8, Iter 18, disc loss: 0.003954560968141747, policy loss: 10.159455251228744
Experience 8, Iter 19, disc loss: 0.0039417879475993015, policy loss: 10.50736007087685
Experience 8, Iter 20, disc loss: 0.0038489409843120983, policy loss: 10.605142062713911
Experience 8, Iter 21, disc loss: 0.003905266405092842, policy loss: 10.50322485579565
Experience 8, Iter 22, disc loss: 0.004823477110194641, policy loss: 9.934814138166054
Experience 8, Iter 23, disc loss: 0.003369953245396912, policy loss: 10.497894118856783
Experience 8, Iter 24, disc loss: 0.03964034226279448, policy loss: 10.49296558083842
Experience 8, Iter 25, disc loss: 0.0035788029257097747, policy loss: 9.856601140446703
Experience 8, Iter 26, disc loss: 0.01706541491153127, policy loss: 10.580274853157913
Experience 8, Iter 27, disc loss: 0.0071041656130397465, policy loss: 10.164557925980418
Experience 8, Iter 28, disc loss: 0.0034284188868182203, policy loss: 10.498373845093472
Experience 8, Iter 29, disc loss: 0.003269437380568124, policy loss: 9.86428178393825
Experience 8, Iter 30, disc loss: 0.006085227629211902, policy loss: 10.184942041126744
Experience 8, Iter 31, disc loss: 0.029899356754290257, policy loss: 10.047804508730575
Experience 8, Iter 32, disc loss: 0.0030336683622610343, policy loss: 10.589320650351588
Experience 8, Iter 33, disc loss: 0.019669172964533146, policy loss: 10.139494376535588
Experience 8, Iter 34, disc loss: 0.0034988624157231392, policy loss: 10.637958632783038
Experience 8, Iter 35, disc loss: 0.003363128087833004, policy loss: 10.110524661821698
Experience 8, Iter 36, disc loss: 0.010497740283160592, policy loss: 10.504273084661646
Experience 8, Iter 37, disc loss: 0.0030656955934161103, policy loss: 10.48781823644352
Experience 8, Iter 38, disc loss: 0.0034261810461326514, policy loss: 9.991426966538842
Experience 8, Iter 39, disc loss: 0.006855271314639247, policy loss: 9.759270269298348
Experience 8, Iter 40, disc loss: 0.004949685360838183, policy loss: 9.736595655939752
Experience 8, Iter 41, disc loss: 0.005443191250503641, policy loss: 10.36293951029717
Experience 8, Iter 42, disc loss: 0.004683086691666818, policy loss: 10.368365394481426
Experience 8, Iter 43, disc loss: 0.004264693739397145, policy loss: 10.406237549576216
Experience 8, Iter 44, disc loss: 0.009092640413986304, policy loss: 10.395151532172685
Experience 8, Iter 45, disc loss: 0.004490101370946263, policy loss: 10.225627828471916
Experience 8, Iter 46, disc loss: 0.020754940132237724, policy loss: 10.469945555677915
Experience 8, Iter 47, disc loss: 0.004316453751381708, policy loss: 10.171605112044514
Experience 8, Iter 48, disc loss: 0.010187864549589795, policy loss: 10.44791590313902
Experience 8, Iter 49, disc loss: 0.009692113575445124, policy loss: 10.102750372096985
Experience 8, Iter 50, disc loss: 0.0033082393103566823, policy loss: 10.342190421637747
Experience 8, Iter 51, disc loss: 0.003598135879392138, policy loss: 10.341943858011506
Experience 8, Iter 52, disc loss: 0.03702869651131248, policy loss: 10.289658771351508
Experience 8, Iter 53, disc loss: 0.003423803994735877, policy loss: 11.002041879197364
Experience 8, Iter 54, disc loss: 0.004296025473269472, policy loss: 10.426904954019548
Experience 8, Iter 55, disc loss: 0.009071188032837066, policy loss: 10.68471688264477
Experience 8, Iter 56, disc loss: 0.00612255898170182, policy loss: 10.328182158265294
Experience 8, Iter 57, disc loss: 0.006188990175537614, policy loss: 10.565738643031171
Experience 8, Iter 58, disc loss: 0.004313080792638415, policy loss: 10.27588972766465
Experience 8, Iter 59, disc loss: 0.00394846580267838, policy loss: 10.44565238267656
Experience 8, Iter 60, disc loss: 0.013783498989168978, policy loss: 10.485827832774282
Experience 8, Iter 61, disc loss: 0.0408086750727763, policy loss: 10.401020328809919
Experience 8, Iter 62, disc loss: 0.005261092954355567, policy loss: 10.347292932773785
Experience 8, Iter 63, disc loss: 0.0037122841350843835, policy loss: 10.088432684980619
Experience 8, Iter 64, disc loss: 0.006718804402424136, policy loss: 11.126856588318947
Experience 8, Iter 65, disc loss: 0.0034816769703370195, policy loss: 10.689084520558776
Experience 8, Iter 66, disc loss: 0.0058259094206502505, policy loss: 10.375524973051997
Experience 8, Iter 67, disc loss: 0.010152507528970746, policy loss: 10.765881378033605
Experience 8, Iter 68, disc loss: 0.0037818240387950735, policy loss: 10.98542138767555
Experience 8, Iter 69, disc loss: 0.0037316420094436874, policy loss: 10.800159114195518
Experience 8, Iter 70, disc loss: 0.009354119169939631, policy loss: 10.104422146739862
Experience 8, Iter 71, disc loss: 0.00589848407767519, policy loss: 9.93419265561357
Experience 8, Iter 72, disc loss: 0.0037064532434414113, policy loss: 10.54724993730122
Experience 8, Iter 73, disc loss: 0.0029964317106213555, policy loss: 11.353754824865812
Experience 8, Iter 74, disc loss: 0.00430770943955364, policy loss: 10.698964092908167
Experience 8, Iter 75, disc loss: 0.0033292616344884956, policy loss: 11.085667204138787
Experience 8, Iter 76, disc loss: 0.004689569959776651, policy loss: 10.611459143522758
Experience 8, Iter 77, disc loss: 0.003225933073333781, policy loss: 10.868272617924355
Experience 8, Iter 78, disc loss: 0.00920809967577397, policy loss: 10.595541157447343
Experience 8, Iter 79, disc loss: 0.017579949014830714, policy loss: 10.396936525727554
Experience 8, Iter 80, disc loss: 0.0033751971520184563, policy loss: 10.581080540159041
Experience 8, Iter 81, disc loss: 0.010548554467394411, policy loss: 10.580977634783823
Experience 8, Iter 82, disc loss: 0.0025765333236790386, policy loss: 11.056181949275274
Experience 8, Iter 83, disc loss: 0.00302123164831561, policy loss: 10.664130568776308
Experience 8, Iter 84, disc loss: 0.03161718778265939, policy loss: 10.828048211257684
Experience 8, Iter 85, disc loss: 0.004549384615922894, policy loss: 10.210350248988625
Experience 8, Iter 86, disc loss: 0.0032012125971829066, policy loss: 11.188389184535934
Experience 8, Iter 87, disc loss: 0.0041512046451713525, policy loss: 10.796721448882499
Experience 8, Iter 88, disc loss: 0.004608308648005523, policy loss: 11.009762905658064
Experience 8, Iter 89, disc loss: 0.003101323672046373, policy loss: 10.79700374014432
Experience 8, Iter 90, disc loss: 0.011726142433886049, policy loss: 10.265527416538497
Experience 8, Iter 91, disc loss: 0.0029188965632870835, policy loss: 10.473816970057202
Experience 8, Iter 92, disc loss: 0.007771670999817792, policy loss: 10.915953048034897
Experience 8, Iter 93, disc loss: 0.004333360091360674, policy loss: 11.119703080561777
Experience 8, Iter 94, disc loss: 0.002954845893512368, policy loss: 11.075056434986841
Experience 8, Iter 95, disc loss: 0.006779228746729018, policy loss: 9.812354951928993
Experience 8, Iter 96, disc loss: 0.008306940230417507, policy loss: 10.77318611636754
Experience 8, Iter 97, disc loss: 0.0026180639324479187, policy loss: 10.359139556306372
Experience 8, Iter 98, disc loss: 0.023799974063199725, policy loss: 9.521033655115026
Experience 8, Iter 99, disc loss: 0.002633725293588393, policy loss: 10.914565035945357
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0116],
        [0.2648],
        [2.6461],
        [0.0379]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0770, 0.4751, 1.7755, 0.0332, 0.0098, 6.8016]],

        [[0.0770, 0.4751, 1.7755, 0.0332, 0.0098, 6.8016]],

        [[0.0770, 0.4751, 1.7755, 0.0332, 0.0098, 6.8016]],

        [[0.0770, 0.4751, 1.7755, 0.0332, 0.0098, 6.8016]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0463,  1.0593, 10.5844,  0.1517], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0463,  1.0593, 10.5844,  0.1517])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.428
Iter 2/2000 - Loss: 4.351
Iter 3/2000 - Loss: 4.337
Iter 4/2000 - Loss: 4.286
Iter 5/2000 - Loss: 4.254
Iter 6/2000 - Loss: 4.234
Iter 7/2000 - Loss: 4.191
Iter 8/2000 - Loss: 4.132
Iter 9/2000 - Loss: 4.073
Iter 10/2000 - Loss: 4.014
Iter 11/2000 - Loss: 3.945
Iter 12/2000 - Loss: 3.863
Iter 13/2000 - Loss: 3.769
Iter 14/2000 - Loss: 3.667
Iter 15/2000 - Loss: 3.558
Iter 16/2000 - Loss: 3.440
Iter 17/2000 - Loss: 3.310
Iter 18/2000 - Loss: 3.169
Iter 19/2000 - Loss: 3.016
Iter 20/2000 - Loss: 2.854
Iter 1981/2000 - Loss: -5.464
Iter 1982/2000 - Loss: -5.464
Iter 1983/2000 - Loss: -5.464
Iter 1984/2000 - Loss: -5.464
Iter 1985/2000 - Loss: -5.464
Iter 1986/2000 - Loss: -5.464
Iter 1987/2000 - Loss: -5.464
Iter 1988/2000 - Loss: -5.464
Iter 1989/2000 - Loss: -5.464
Iter 1990/2000 - Loss: -5.464
Iter 1991/2000 - Loss: -5.464
Iter 1992/2000 - Loss: -5.464
Iter 1993/2000 - Loss: -5.464
Iter 1994/2000 - Loss: -5.465
Iter 1995/2000 - Loss: -5.465
Iter 1996/2000 - Loss: -5.465
Iter 1997/2000 - Loss: -5.465
Iter 1998/2000 - Loss: -5.465
Iter 1999/2000 - Loss: -5.465
Iter 2000/2000 - Loss: -5.465
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0026],
        [0.0005]])
Lengthscale: tensor([[[27.5303, 14.6073, 46.3072, 18.4011,  6.1573, 65.4667]],

        [[35.2934, 45.0646, 11.2996,  1.3182,  4.1447, 27.2173]],

        [[34.5798, 48.9234, 10.1678,  1.0616,  1.7306, 18.7484]],

        [[29.9692, 47.8858, 14.9887,  3.6178,  1.9356, 39.9834]]])
Signal Variance: tensor([ 0.2956,  2.5222, 13.6843,  0.5793])
Estimated target variance: tensor([ 0.0463,  1.0593, 10.5844,  0.1517])
N: 90
Signal to noise ratio: tensor([27.8497, 72.9251, 73.1212, 35.0384])
Bound on condition number: tensor([ 69805.5203, 478627.1327, 481204.5429, 110493.2891])
Policy Optimizer learning rate:
0.09916065818347441
Experience 9, Iter 0, disc loss: 0.0039123901853875395, policy loss: 10.228184071182243
Experience 9, Iter 1, disc loss: 0.0027627364957287293, policy loss: 11.175261066361182
Experience 9, Iter 2, disc loss: 0.0024843655583459694, policy loss: 10.860434889210737
Experience 9, Iter 3, disc loss: 0.0026077503945507847, policy loss: 10.82569245033777
Experience 9, Iter 4, disc loss: 0.031025692224322377, policy loss: 10.89807581662688
Experience 9, Iter 5, disc loss: 0.005426068643383087, policy loss: 11.260577037177995
Experience 9, Iter 6, disc loss: 0.002790607889517465, policy loss: 10.58913626772905
Experience 9, Iter 7, disc loss: 0.0026693081147309804, policy loss: 10.868585269687458
Experience 9, Iter 8, disc loss: 0.02492442625481498, policy loss: 10.404523481648221
Experience 9, Iter 9, disc loss: 0.002716682705868358, policy loss: 10.702047079139458
Experience 9, Iter 10, disc loss: 0.004328136804694927, policy loss: 10.519872739853316
Experience 9, Iter 11, disc loss: 0.004129611476124132, policy loss: 10.556428641955998
Experience 9, Iter 12, disc loss: 0.0036304180928232806, policy loss: 10.366677590347068
Experience 9, Iter 13, disc loss: 0.003512592417191188, policy loss: 11.47354611222314
Experience 9, Iter 14, disc loss: 0.009322631633573743, policy loss: 11.073393640376068
Experience 9, Iter 15, disc loss: 0.0348788226888497, policy loss: 10.312223020058155
Experience 9, Iter 16, disc loss: 0.0031780093504572733, policy loss: 10.617019150193187
Experience 9, Iter 17, disc loss: 0.011101286439001896, policy loss: 10.385473105359612
Experience 9, Iter 18, disc loss: 0.0034330715575964887, policy loss: 10.629991469582317
Experience 9, Iter 19, disc loss: 0.0030000113068322444, policy loss: 11.612731506467654
Experience 9, Iter 20, disc loss: 0.0034894119534621282, policy loss: 10.678041094380863
Experience 9, Iter 21, disc loss: 0.004285750854022576, policy loss: 10.902547961884395
Experience 9, Iter 22, disc loss: 0.011455920961110262, policy loss: 11.428400594120124
Experience 9, Iter 23, disc loss: 0.004041675893699983, policy loss: 10.293211395327312
Experience 9, Iter 24, disc loss: 0.010563553792479991, policy loss: 10.309229841638794
Experience 9, Iter 25, disc loss: 0.0036467384955878824, policy loss: 11.367185688050167
Experience 9, Iter 26, disc loss: 0.003328119712875813, policy loss: 11.260985583987043
Experience 9, Iter 27, disc loss: 0.005622854199615926, policy loss: 10.962342574341642
Experience 9, Iter 28, disc loss: 0.004112378523399508, policy loss: 10.758833169880837
Experience 9, Iter 29, disc loss: 0.0035365193884001633, policy loss: 11.186345344580564
Experience 9, Iter 30, disc loss: 0.003575214612298581, policy loss: 11.0827241867871
Experience 9, Iter 31, disc loss: 0.004055785338221766, policy loss: 10.941097911912443
Experience 9, Iter 32, disc loss: 0.01066913863244479, policy loss: 11.244525620824426
Experience 9, Iter 33, disc loss: 0.011409749954119459, policy loss: 10.523029753647656
Experience 9, Iter 34, disc loss: 0.019838410067291708, policy loss: 11.216263606951738
Experience 9, Iter 35, disc loss: 0.0026639195047292886, policy loss: 12.115888617124876
Experience 9, Iter 36, disc loss: 0.0028301325270528627, policy loss: 11.399263086402723
Experience 9, Iter 37, disc loss: 0.0034747269081733543, policy loss: 11.279551632217327
Experience 9, Iter 38, disc loss: 0.004935233235101802, policy loss: 11.468894204980632
Experience 9, Iter 39, disc loss: 0.11597468300197396, policy loss: 10.587000191384053
Experience 9, Iter 40, disc loss: 0.002756632531892325, policy loss: 12.100513048039119
Experience 9, Iter 41, disc loss: 0.0038585132719832866, policy loss: 10.510655966262345
Experience 9, Iter 42, disc loss: 0.0034848842872373526, policy loss: 11.18146844551438
Experience 9, Iter 43, disc loss: 0.004852598794588659, policy loss: 11.336942328273599
Experience 9, Iter 44, disc loss: 0.003559800457542642, policy loss: 10.907940482959622
Experience 9, Iter 45, disc loss: 0.004834872087149196, policy loss: 11.45602050682919
Experience 9, Iter 46, disc loss: 0.004280456679575192, policy loss: 10.927235116920823
Experience 9, Iter 47, disc loss: 0.003070636050777694, policy loss: 11.000542109886762
Experience 9, Iter 48, disc loss: 0.004298522952365819, policy loss: 11.37864573805343
Experience 9, Iter 49, disc loss: 0.007634605798731212, policy loss: 11.100505097318383
Experience 9, Iter 50, disc loss: 0.004968839505287285, policy loss: 10.530296418609327
Experience 9, Iter 51, disc loss: 0.0031009712810210867, policy loss: 10.933820448626303
Experience 9, Iter 52, disc loss: 0.0030706203410421126, policy loss: 11.038855475457984
Experience 9, Iter 53, disc loss: 0.0032232226607495176, policy loss: 11.486153090259714
Experience 9, Iter 54, disc loss: 0.002925261045297734, policy loss: 11.229851870168108
Experience 9, Iter 55, disc loss: 0.0028794842065118986, policy loss: 11.794811368105977
Experience 9, Iter 56, disc loss: 0.0030375197140538506, policy loss: 11.431629710009322
Experience 9, Iter 57, disc loss: 0.003407784830184183, policy loss: 10.762088680374731
Experience 9, Iter 58, disc loss: 0.010846561656135784, policy loss: 10.67283560156244
Experience 9, Iter 59, disc loss: 0.0035831936315027634, policy loss: 10.607317089325644
Experience 9, Iter 60, disc loss: 0.0032874599104171405, policy loss: 10.817640793940145
Experience 9, Iter 61, disc loss: 0.0036172155843852663, policy loss: 11.122092961187363
Experience 9, Iter 62, disc loss: 0.0033011437048179283, policy loss: 10.871935691201491
Experience 9, Iter 63, disc loss: 0.00517442616147592, policy loss: 10.886685972036634
Experience 9, Iter 64, disc loss: 0.002275200920005869, policy loss: 10.90804188569702
Experience 9, Iter 65, disc loss: 0.0024669368429803677, policy loss: 11.395376709773009
Experience 9, Iter 66, disc loss: 0.002201943842290051, policy loss: 11.481107583770854
Experience 9, Iter 67, disc loss: 0.002108150542626611, policy loss: 11.277784573415202
Experience 9, Iter 68, disc loss: 0.002714991630092442, policy loss: 11.065436174097298
Experience 9, Iter 69, disc loss: 0.002338768002051352, policy loss: 11.206494015884264
Experience 9, Iter 70, disc loss: 0.06163989081536961, policy loss: 10.888495376311184
Experience 9, Iter 71, disc loss: 0.005238822479899447, policy loss: 11.146653793914405
Experience 9, Iter 72, disc loss: 0.004439177467956168, policy loss: 10.89012062705573
Experience 9, Iter 73, disc loss: 0.02415359956139789, policy loss: 11.07150445684023
Experience 9, Iter 74, disc loss: 0.003597478211181753, policy loss: 11.347190170613127
Experience 9, Iter 75, disc loss: 0.006164777126277686, policy loss: 11.21398373111079
Experience 9, Iter 76, disc loss: 0.00533293620336121, policy loss: 10.75852036976504
Experience 9, Iter 77, disc loss: 0.005824195310073821, policy loss: 11.112449991739181
Experience 9, Iter 78, disc loss: 0.0023795850039572803, policy loss: 11.277992100404326
Experience 9, Iter 79, disc loss: 0.005363714205018345, policy loss: 11.69159018162759
Experience 9, Iter 80, disc loss: 0.002593268006820173, policy loss: 11.042841696253342
Experience 9, Iter 81, disc loss: 0.005209168930838799, policy loss: 11.289305482580598
Experience 9, Iter 82, disc loss: 0.01348017678458065, policy loss: 10.970190432983786
Experience 9, Iter 83, disc loss: 0.0026811457962254513, policy loss: 10.762298795953217
Experience 9, Iter 84, disc loss: 0.002532728332613887, policy loss: 11.33718031012486
Experience 9, Iter 85, disc loss: 0.003134208166449188, policy loss: 11.125091279821918
Experience 9, Iter 86, disc loss: 0.004961713045674526, policy loss: 10.796562061393626
Experience 9, Iter 87, disc loss: 0.0043130927149623004, policy loss: 11.686906605723127
Experience 9, Iter 88, disc loss: 0.0036776112862026413, policy loss: 11.424236615187032
Experience 9, Iter 89, disc loss: 0.0023205169307485146, policy loss: 11.507777378784274
Experience 9, Iter 90, disc loss: 0.004784174662339678, policy loss: 11.495201291993885
Experience 9, Iter 91, disc loss: 0.005147236674987596, policy loss: 11.161060184174191
Experience 9, Iter 92, disc loss: 0.0072595115330834695, policy loss: 11.174532014749797
Experience 9, Iter 93, disc loss: 0.00511157382399153, policy loss: 11.212279461642282
Experience 9, Iter 94, disc loss: 0.005289446164463017, policy loss: 11.121218532609449
Experience 9, Iter 95, disc loss: 0.005171905708711039, policy loss: 11.167086177755884
Experience 9, Iter 96, disc loss: 0.002548037592093669, policy loss: 10.552178147210007
Experience 9, Iter 97, disc loss: 0.00315507871355399, policy loss: 11.180989961856051
Experience 9, Iter 98, disc loss: 0.0023936441596473096, policy loss: 11.778018461419531
Experience 9, Iter 99, disc loss: 0.0021505203714639676, policy loss: 11.954576116968616
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0110],
        [0.2793],
        [2.7558],
        [0.0357]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0698, 0.4535, 1.6842, 0.0334, 0.0091, 7.0401]],

        [[0.0698, 0.4535, 1.6842, 0.0334, 0.0091, 7.0401]],

        [[0.0698, 0.4535, 1.6842, 0.0334, 0.0091, 7.0401]],

        [[0.0698, 0.4535, 1.6842, 0.0334, 0.0091, 7.0401]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0439,  1.1173, 11.0233,  0.1429], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0439,  1.1173, 11.0233,  0.1429])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.406
Iter 2/2000 - Loss: 4.356
Iter 3/2000 - Loss: 4.315
Iter 4/2000 - Loss: 4.263
Iter 5/2000 - Loss: 4.242
Iter 6/2000 - Loss: 4.205
Iter 7/2000 - Loss: 4.142
Iter 8/2000 - Loss: 4.081
Iter 9/2000 - Loss: 4.022
Iter 10/2000 - Loss: 3.950
Iter 11/2000 - Loss: 3.862
Iter 12/2000 - Loss: 3.767
Iter 13/2000 - Loss: 3.667
Iter 14/2000 - Loss: 3.560
Iter 15/2000 - Loss: 3.442
Iter 16/2000 - Loss: 3.311
Iter 17/2000 - Loss: 3.170
Iter 18/2000 - Loss: 3.021
Iter 19/2000 - Loss: 2.863
Iter 20/2000 - Loss: 2.696
Iter 1981/2000 - Loss: -5.735
Iter 1982/2000 - Loss: -5.735
Iter 1983/2000 - Loss: -5.735
Iter 1984/2000 - Loss: -5.735
Iter 1985/2000 - Loss: -5.735
Iter 1986/2000 - Loss: -5.735
Iter 1987/2000 - Loss: -5.735
Iter 1988/2000 - Loss: -5.735
Iter 1989/2000 - Loss: -5.735
Iter 1990/2000 - Loss: -5.735
Iter 1991/2000 - Loss: -5.735
Iter 1992/2000 - Loss: -5.735
Iter 1993/2000 - Loss: -5.735
Iter 1994/2000 - Loss: -5.735
Iter 1995/2000 - Loss: -5.735
Iter 1996/2000 - Loss: -5.735
Iter 1997/2000 - Loss: -5.736
Iter 1998/2000 - Loss: -5.736
Iter 1999/2000 - Loss: -5.736
Iter 2000/2000 - Loss: -5.736
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0005],
        [0.0024],
        [0.0005]])
Lengthscale: tensor([[[25.9298, 14.1924, 44.8315, 18.4753,  5.5905, 66.5902]],

        [[32.8130, 42.7146, 10.9203,  1.3184,  3.6157, 23.4653]],

        [[31.2173, 43.4534, 10.0300,  1.0572,  1.7717, 18.4936]],

        [[28.2422, 45.9161, 15.8063,  3.6101,  1.9948, 40.0417]]])
Signal Variance: tensor([ 0.2740,  2.1912, 13.3874,  0.6232])
Estimated target variance: tensor([ 0.0439,  1.1173, 11.0233,  0.1429])
N: 100
Signal to noise ratio: tensor([27.9690, 67.5802, 74.8777, 37.0640])
Bound on condition number: tensor([ 78227.4109, 456708.9918, 560667.5611, 137375.0840])
Policy Optimizer learning rate:
0.09905623702167957
Experience 10, Iter 0, disc loss: 0.002387650566120635, policy loss: 11.719230329060117
Experience 10, Iter 1, disc loss: 0.0030662054839445466, policy loss: 11.601089679896294
Experience 10, Iter 2, disc loss: 0.004233103502764722, policy loss: 10.611698875694088
Experience 10, Iter 3, disc loss: 0.002693409571256372, policy loss: 11.56914855771042
Experience 10, Iter 4, disc loss: 0.0024427802729797532, policy loss: 10.859287459581525
Experience 10, Iter 5, disc loss: 0.0024283849335830798, policy loss: 11.471091795767357
Experience 10, Iter 6, disc loss: 0.0035592815156176034, policy loss: 11.524375239770968
Experience 10, Iter 7, disc loss: 0.002005905907681794, policy loss: 11.819269490567216
Experience 10, Iter 8, disc loss: 0.004179965189109108, policy loss: 11.211687854376766
Experience 10, Iter 9, disc loss: 0.02339585109243901, policy loss: 11.069662908623986
Experience 10, Iter 10, disc loss: 0.005213019570660513, policy loss: 11.482126725028058
Experience 10, Iter 11, disc loss: 0.005753428714037849, policy loss: 10.48644584910313
Experience 10, Iter 12, disc loss: 0.0041730070772939085, policy loss: 11.561251743007396
Experience 10, Iter 13, disc loss: 0.02309895956390593, policy loss: 10.546608216872283
Experience 10, Iter 14, disc loss: 0.0020334237817252274, policy loss: 11.85269678893717
Experience 10, Iter 15, disc loss: 0.002795502469769716, policy loss: 11.246445034489259
Experience 10, Iter 16, disc loss: 0.0024918118895851105, policy loss: 11.74727538434061
Experience 10, Iter 17, disc loss: 0.004681751865132833, policy loss: 11.060078565279941
Experience 10, Iter 18, disc loss: 0.0024036643053837437, policy loss: 11.193756741215235
Experience 10, Iter 19, disc loss: 0.001956967579969416, policy loss: 11.856656626914894
Experience 10, Iter 20, disc loss: 0.0021998685634700013, policy loss: 11.735790957819516
Experience 10, Iter 21, disc loss: 0.0028737540050202647, policy loss: 11.159501217297603
Experience 10, Iter 22, disc loss: 0.003428232201157923, policy loss: 11.156118472738877
Experience 10, Iter 23, disc loss: 0.011193431731339075, policy loss: 11.93038387241552
Experience 10, Iter 24, disc loss: 0.002596670176475509, policy loss: 11.261950727310202
Experience 10, Iter 25, disc loss: 0.0027598755847226225, policy loss: 11.6911098081811
Experience 10, Iter 26, disc loss: 0.0021863810439660242, policy loss: 11.301813516336129
Experience 10, Iter 27, disc loss: 0.0029470717757425057, policy loss: 11.965270101303169
Experience 10, Iter 28, disc loss: 0.002539852882636992, policy loss: 11.239875509669854
Experience 10, Iter 29, disc loss: 0.0047423217959482455, policy loss: 10.633923525134161
Experience 10, Iter 30, disc loss: 0.002612038043370411, policy loss: 11.782715491935674
Experience 10, Iter 31, disc loss: 0.011882132170684739, policy loss: 11.557304092531947
Experience 10, Iter 32, disc loss: 0.0018700166869242726, policy loss: 11.974624897104855
Experience 10, Iter 33, disc loss: 0.0025130355061893017, policy loss: 11.47576356608652
Experience 10, Iter 34, disc loss: 0.010886618940462113, policy loss: 11.421164455694434
Experience 10, Iter 35, disc loss: 0.002477174182642821, policy loss: 11.111035187637752
Experience 10, Iter 36, disc loss: 0.0027409867650369435, policy loss: 11.146991316576635
Experience 10, Iter 37, disc loss: 0.002562556291761981, policy loss: 11.480714992691619
Experience 10, Iter 38, disc loss: 0.002113924249587865, policy loss: 11.721732293202422
Experience 10, Iter 39, disc loss: 0.0028048842745091156, policy loss: 11.9951972429499
Experience 10, Iter 40, disc loss: 0.0019284250273169706, policy loss: 11.406854507867736
Experience 10, Iter 41, disc loss: 0.002720731662764682, policy loss: 11.890397052183358
Experience 10, Iter 42, disc loss: 0.002383385457000454, policy loss: 11.625383556605662
Experience 10, Iter 43, disc loss: 0.0026512957719823952, policy loss: 11.81429263633777
Experience 10, Iter 44, disc loss: 0.0018416555387255238, policy loss: 11.460885868565638
Experience 10, Iter 45, disc loss: 0.0022263610304959595, policy loss: 11.015455818316305
Experience 10, Iter 46, disc loss: 0.0031287863146126794, policy loss: 11.278690686647092
Experience 10, Iter 47, disc loss: 0.001988312787183229, policy loss: 11.872113291905096
Experience 10, Iter 48, disc loss: 0.0017198965680893608, policy loss: 11.81853235152095
Experience 10, Iter 49, disc loss: 0.005008280425996559, policy loss: 11.8680534243735
Experience 10, Iter 50, disc loss: 0.005520769413700578, policy loss: 11.582264043041931
Experience 10, Iter 51, disc loss: 0.0017185296747606977, policy loss: 11.583551542269555
Experience 10, Iter 52, disc loss: 0.0036693655718760968, policy loss: 11.488186491240853
Experience 10, Iter 53, disc loss: 0.009589866639792444, policy loss: 11.61758889705956
Experience 10, Iter 54, disc loss: 0.0027840545493000445, policy loss: 11.910968433638876
Experience 10, Iter 55, disc loss: 0.0016448164098421055, policy loss: 11.340405368653261
Experience 10, Iter 56, disc loss: 0.002903762758835909, policy loss: 11.367026854457759
Experience 10, Iter 57, disc loss: 0.0015967456052431205, policy loss: 11.854400909193128
Experience 10, Iter 58, disc loss: 0.01703384131221962, policy loss: 10.954656434325498
Experience 10, Iter 59, disc loss: 0.0020166380956328674, policy loss: 11.326991751663133
Experience 10, Iter 60, disc loss: 0.0016460926008586402, policy loss: 12.394684424818356
Experience 10, Iter 61, disc loss: 0.0028619179139721647, policy loss: 11.552915122965226
Experience 10, Iter 62, disc loss: 0.0018165533026075977, policy loss: 12.286557350129549
Experience 10, Iter 63, disc loss: 0.010669941432301296, policy loss: 10.900972895983173
Experience 10, Iter 64, disc loss: 0.006887320429010534, policy loss: 11.192173559719869
Experience 10, Iter 65, disc loss: 0.008011230548556732, policy loss: 11.164267908587806
Experience 10, Iter 66, disc loss: 0.0016680287080301256, policy loss: 11.972990166037116
Experience 10, Iter 67, disc loss: 0.0025018288857669027, policy loss: 11.568029612040807
Experience 10, Iter 68, disc loss: 0.01057204565326222, policy loss: 11.529471553435908
Experience 10, Iter 69, disc loss: 0.035314294950535154, policy loss: 11.60965014114158
Experience 10, Iter 70, disc loss: 0.0028300290972044363, policy loss: 11.691839739404621
Experience 10, Iter 71, disc loss: 0.0020414095642243587, policy loss: 11.45226496308522
Experience 10, Iter 72, disc loss: 0.0020115790764698217, policy loss: 11.252655150974412
Experience 10, Iter 73, disc loss: 0.002074782002987728, policy loss: 11.643801429023522
Experience 10, Iter 74, disc loss: 0.0022467624137409554, policy loss: 12.131770769378374
Experience 10, Iter 75, disc loss: 0.004933900267077279, policy loss: 11.920745612842335
Experience 10, Iter 76, disc loss: 0.008096141803644374, policy loss: 11.697876457182872
Experience 10, Iter 77, disc loss: 0.002240844162818595, policy loss: 11.58093505687613
Experience 10, Iter 78, disc loss: 0.0023696804573101237, policy loss: 12.636895493338526
Experience 10, Iter 79, disc loss: 0.002981399396383049, policy loss: 11.569655337415
Experience 10, Iter 80, disc loss: 0.002632962120393807, policy loss: 11.479676296704218
Experience 10, Iter 81, disc loss: 0.0027595418543743112, policy loss: 12.190734158101655
Experience 10, Iter 82, disc loss: 0.005551215725571247, policy loss: 11.113602365145963
Experience 10, Iter 83, disc loss: 0.0022805007967387795, policy loss: 11.639601795704383
Experience 10, Iter 84, disc loss: 0.002395336896666021, policy loss: 11.894527148495161
Experience 10, Iter 85, disc loss: 0.005069236951042402, policy loss: 11.79736681477345
Experience 10, Iter 86, disc loss: 0.006035336660205627, policy loss: 12.150724304570687
Experience 10, Iter 87, disc loss: 0.008459675372186469, policy loss: 11.965755635951849
Experience 10, Iter 88, disc loss: 0.0020962118195167875, policy loss: 11.560177531890828
Experience 10, Iter 89, disc loss: 0.006053279587189306, policy loss: 11.693614486165579
Experience 10, Iter 90, disc loss: 0.002910899257944813, policy loss: 12.183966689296343
Experience 10, Iter 91, disc loss: 0.0021394157591936628, policy loss: 13.441409643012467
Experience 10, Iter 92, disc loss: 0.0024138003826032458, policy loss: 12.060094984468773
Experience 10, Iter 93, disc loss: 0.0018362921057193983, policy loss: 12.572018324509354
Experience 10, Iter 94, disc loss: 0.0025828966664842986, policy loss: 12.040598490692082
Experience 10, Iter 95, disc loss: 0.009870884853936644, policy loss: 11.920491695964314
Experience 10, Iter 96, disc loss: 0.0027800770378242764, policy loss: 11.403324540184615
Experience 10, Iter 97, disc loss: 0.010139289860405975, policy loss: 11.611300099758708
Experience 10, Iter 98, disc loss: 0.001821568327762976, policy loss: 12.101860892262254
Experience 10, Iter 99, disc loss: 0.0017925809313983877, policy loss: 12.557855988625906
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0102],
        [0.2776],
        [2.7571],
        [0.0347]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0643, 0.4222, 1.6531, 0.0327, 0.0088, 6.9084]],

        [[0.0643, 0.4222, 1.6531, 0.0327, 0.0088, 6.9084]],

        [[0.0643, 0.4222, 1.6531, 0.0327, 0.0088, 6.9084]],

        [[0.0643, 0.4222, 1.6531, 0.0327, 0.0088, 6.9084]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0407,  1.1106, 11.0285,  0.1389], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0407,  1.1106, 11.0285,  0.1389])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.360
Iter 2/2000 - Loss: 4.309
Iter 3/2000 - Loss: 4.267
Iter 4/2000 - Loss: 4.211
Iter 5/2000 - Loss: 4.194
Iter 6/2000 - Loss: 4.155
Iter 7/2000 - Loss: 4.088
Iter 8/2000 - Loss: 4.025
Iter 9/2000 - Loss: 3.965
Iter 10/2000 - Loss: 3.890
Iter 11/2000 - Loss: 3.798
Iter 12/2000 - Loss: 3.699
Iter 13/2000 - Loss: 3.597
Iter 14/2000 - Loss: 3.487
Iter 15/2000 - Loss: 3.364
Iter 16/2000 - Loss: 3.229
Iter 17/2000 - Loss: 3.084
Iter 18/2000 - Loss: 2.931
Iter 19/2000 - Loss: 2.769
Iter 20/2000 - Loss: 2.594
Iter 1981/2000 - Loss: -5.902
Iter 1982/2000 - Loss: -5.902
Iter 1983/2000 - Loss: -5.902
Iter 1984/2000 - Loss: -5.902
Iter 1985/2000 - Loss: -5.902
Iter 1986/2000 - Loss: -5.902
Iter 1987/2000 - Loss: -5.902
Iter 1988/2000 - Loss: -5.902
Iter 1989/2000 - Loss: -5.903
Iter 1990/2000 - Loss: -5.903
Iter 1991/2000 - Loss: -5.903
Iter 1992/2000 - Loss: -5.903
Iter 1993/2000 - Loss: -5.903
Iter 1994/2000 - Loss: -5.903
Iter 1995/2000 - Loss: -5.903
Iter 1996/2000 - Loss: -5.903
Iter 1997/2000 - Loss: -5.903
Iter 1998/2000 - Loss: -5.903
Iter 1999/2000 - Loss: -5.903
Iter 2000/2000 - Loss: -5.903
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0006],
        [0.0028],
        [0.0004]])
Lengthscale: tensor([[[24.7217, 13.7159, 42.1131, 18.0756,  4.8879, 69.2321]],

        [[29.8036, 42.2929, 11.1408,  1.1000,  3.6158, 24.0401]],

        [[31.9810, 48.0886, 10.0621,  1.0039,  1.6609, 20.2500]],

        [[27.1871, 45.7293, 15.4570,  3.3736,  1.9043, 42.6744]]])
Signal Variance: tensor([ 0.2541,  1.8113, 12.5349,  0.6098])
Estimated target variance: tensor([ 0.0407,  1.1106, 11.0285,  0.1389])
N: 110
Signal to noise ratio: tensor([26.9802, 56.3558, 66.4720, 37.4947])
Bound on condition number: tensor([ 80073.2947, 349358.8791, 486038.5028, 154644.9476])
Policy Optimizer learning rate:
0.09895192582062147
Experience 11, Iter 0, disc loss: 0.001805049253125041, policy loss: 11.760154613276278
Experience 11, Iter 1, disc loss: 0.0030254573904468568, policy loss: 12.294399204862852
Experience 11, Iter 2, disc loss: 0.0019391798629306057, policy loss: 11.342253284216605
Experience 11, Iter 3, disc loss: 0.0016794726315384238, policy loss: 11.879223272905426
Experience 11, Iter 4, disc loss: 0.0016875137888828818, policy loss: 12.125582837808118
Experience 11, Iter 5, disc loss: 0.00356090513386004, policy loss: 11.714796865865193
Experience 11, Iter 6, disc loss: 0.0026216985517295197, policy loss: 11.05664191656247
Experience 11, Iter 7, disc loss: 0.0021337111140249557, policy loss: 11.854074794426065
Experience 11, Iter 8, disc loss: 0.006648655081285556, policy loss: 11.843934443460078
Experience 11, Iter 9, disc loss: 0.0029321039614239814, policy loss: 11.529295854646955
Experience 11, Iter 10, disc loss: 0.0028420932881982617, policy loss: 11.814889609728336
Experience 11, Iter 11, disc loss: 0.0015193063108084627, policy loss: 12.101828922171753
Experience 11, Iter 12, disc loss: 0.002752119602239026, policy loss: 11.247646366479097
Experience 11, Iter 13, disc loss: 0.006618049712446927, policy loss: 11.774293637985839
Experience 11, Iter 14, disc loss: 0.0024601809407271674, policy loss: 12.076949101701084
Experience 11, Iter 15, disc loss: 0.0019008346112389615, policy loss: 10.701566017675216
Experience 11, Iter 16, disc loss: 0.0024322394810377054, policy loss: 11.896580314854727
Experience 11, Iter 17, disc loss: 0.0037563923249112627, policy loss: 11.780798966874851
Experience 11, Iter 18, disc loss: 0.009282138581045726, policy loss: 12.314782120489067
Experience 11, Iter 19, disc loss: 0.0015447226818836074, policy loss: 11.824751618372702
Experience 11, Iter 20, disc loss: 0.006013493093227034, policy loss: 11.509152515051916
Experience 11, Iter 21, disc loss: 0.0024506490689347364, policy loss: 11.943368211513326
Experience 11, Iter 22, disc loss: 0.006330869898131912, policy loss: 11.733160846940049
Experience 11, Iter 23, disc loss: 0.0041002578979338, policy loss: 11.578449134199062
Experience 11, Iter 24, disc loss: 0.0019255531965980278, policy loss: 11.443766955638885
Experience 11, Iter 25, disc loss: 0.0016178059610839402, policy loss: 11.565721627503716
Experience 11, Iter 26, disc loss: 0.007932320431316042, policy loss: 11.79346988814703
Experience 11, Iter 27, disc loss: 0.0016084050835531126, policy loss: 12.260703134978057
Experience 11, Iter 28, disc loss: 0.0019295602415695659, policy loss: 11.07770012320194
Experience 11, Iter 29, disc loss: 0.0024710526146055145, policy loss: 11.324993731724193
Experience 11, Iter 30, disc loss: 0.005843768302166397, policy loss: 11.259296045053517
Experience 11, Iter 31, disc loss: 0.003621962924044921, policy loss: 11.477614415135106
Experience 11, Iter 32, disc loss: 0.007184385874012721, policy loss: 11.112522069281063
Experience 11, Iter 33, disc loss: 0.0017437772351957047, policy loss: 12.150162560895904
Experience 11, Iter 34, disc loss: 0.0015707858856363137, policy loss: 11.2423930015406
Experience 11, Iter 35, disc loss: 0.001863438501528163, policy loss: 11.048951748696703
Experience 11, Iter 36, disc loss: 0.0029743058360642335, policy loss: 11.008103634879252
Experience 11, Iter 37, disc loss: 0.0026499395774399683, policy loss: 11.406651779467467
Experience 11, Iter 38, disc loss: 0.0021110261714254145, policy loss: 11.608470644077553
Experience 11, Iter 39, disc loss: 0.002317319743785437, policy loss: 11.761397469339144
Experience 11, Iter 40, disc loss: 0.0022064419003146197, policy loss: 11.282471398658489
Experience 11, Iter 41, disc loss: 0.0016329412326396829, policy loss: 11.890365352593804
Experience 11, Iter 42, disc loss: 0.00403825311030173, policy loss: 11.901468082941228
Experience 11, Iter 43, disc loss: 0.0023536114893261233, policy loss: 11.351779229586754
Experience 11, Iter 44, disc loss: 0.0022448858225247808, policy loss: 12.459798050516778
Experience 11, Iter 45, disc loss: 0.0032182289728204293, policy loss: 11.898990626257493
Experience 11, Iter 46, disc loss: 0.0013866439575955809, policy loss: 12.06090717322543
Experience 11, Iter 47, disc loss: 0.003554590330687317, policy loss: 11.191149900254892
Experience 11, Iter 48, disc loss: 0.0034759373292994784, policy loss: 11.741217438301963
Experience 11, Iter 49, disc loss: 0.0022023253881827834, policy loss: 12.021375455660378
Experience 11, Iter 50, disc loss: 0.001358109639432311, policy loss: 11.751025530151523
Experience 11, Iter 51, disc loss: 0.0019260765000476733, policy loss: 11.469577384471425
Experience 11, Iter 52, disc loss: 0.001826415395940563, policy loss: 11.566925894120851
Experience 11, Iter 53, disc loss: 0.0014034933387757804, policy loss: 12.64869189272562
Experience 11, Iter 54, disc loss: 0.0013239264321995787, policy loss: 12.220217202856581
Experience 11, Iter 55, disc loss: 0.002117668920200404, policy loss: 11.722514756203633
Experience 11, Iter 56, disc loss: 0.0014886718076361715, policy loss: 11.909715301066338
Experience 11, Iter 57, disc loss: 0.003752642085537684, policy loss: 11.740194107843703
Experience 11, Iter 58, disc loss: 0.0013818609401274053, policy loss: 12.27802167012388
Experience 11, Iter 59, disc loss: 0.0014288614082940514, policy loss: 11.824351608715222
Experience 11, Iter 60, disc loss: 0.002068370792973058, policy loss: 11.612169866778943
Experience 11, Iter 61, disc loss: 0.0024934483222312045, policy loss: 11.04208459269997
Experience 11, Iter 62, disc loss: 0.004647177896005324, policy loss: 11.819823921121055
Experience 11, Iter 63, disc loss: 0.001168046546805854, policy loss: 12.128221635688258
Experience 11, Iter 64, disc loss: 0.014738144129646646, policy loss: 11.712154839716147
Experience 11, Iter 65, disc loss: 0.002366307038177675, policy loss: 12.556211830047568
Experience 11, Iter 66, disc loss: 0.0023505195748806306, policy loss: 12.119664350527245
Experience 11, Iter 67, disc loss: 0.0026872284928650693, policy loss: 11.715311477436632
Experience 11, Iter 68, disc loss: 0.00408010001249548, policy loss: 11.613062075682226
Experience 11, Iter 69, disc loss: 0.002156560193068568, policy loss: 10.883517241029587
Experience 11, Iter 70, disc loss: 0.0012710152519295737, policy loss: 12.192090918223993
Experience 11, Iter 71, disc loss: 0.003710371387965904, policy loss: 12.121174830067496
Experience 11, Iter 72, disc loss: 0.012975134946805755, policy loss: 11.478572987913015
Experience 11, Iter 73, disc loss: 0.0012485172015320736, policy loss: 12.799642849399099
Experience 11, Iter 74, disc loss: 0.0012675947639444768, policy loss: 11.810138421126338
Experience 11, Iter 75, disc loss: 0.0017051894147077692, policy loss: 11.607643191046682
Experience 11, Iter 76, disc loss: 0.0017909402919734053, policy loss: 12.744540117797854
Experience 11, Iter 77, disc loss: 0.001915127394211118, policy loss: 12.025825645362925
Experience 11, Iter 78, disc loss: 0.0015044630064278992, policy loss: 11.964484683819496
Experience 11, Iter 79, disc loss: 0.004723301845212422, policy loss: 12.360639900928955
Experience 11, Iter 80, disc loss: 0.001904315240505842, policy loss: 11.9011933747855
Experience 11, Iter 81, disc loss: 0.004513838407104499, policy loss: 12.037485269175889
Experience 11, Iter 82, disc loss: 0.005710083165889926, policy loss: 12.380243249061682
Experience 11, Iter 83, disc loss: 0.00889362280722741, policy loss: 11.390348172572939
Experience 11, Iter 84, disc loss: 0.003208659417932401, policy loss: 12.2198515132777
Experience 11, Iter 85, disc loss: 0.001899568159719086, policy loss: 12.224393945841932
Experience 11, Iter 86, disc loss: 0.0015697063642025311, policy loss: 11.874960911421798
Experience 11, Iter 87, disc loss: 0.0024150452802340966, policy loss: 12.119158538074249
Experience 11, Iter 88, disc loss: 0.0018305091052632626, policy loss: 11.9806934421117
Experience 11, Iter 89, disc loss: 0.001530158440100776, policy loss: 11.717171753063692
Experience 11, Iter 90, disc loss: 0.0018320328555766036, policy loss: 12.20497047025891
Experience 11, Iter 91, disc loss: 0.0015645369874887813, policy loss: 12.021240800914615
Experience 11, Iter 92, disc loss: 0.0015505498906163272, policy loss: 11.288531846584988
Experience 11, Iter 93, disc loss: 0.008085456843078537, policy loss: 11.487966760457768
Experience 11, Iter 94, disc loss: 0.0017683545856999358, policy loss: 11.10551187086379
Experience 11, Iter 95, disc loss: 0.032021690705605135, policy loss: 11.828092646436357
Experience 11, Iter 96, disc loss: 0.004372260029278826, policy loss: 11.426793151208994
Experience 11, Iter 97, disc loss: 0.001852190057248678, policy loss: 12.681522294469644
Experience 11, Iter 98, disc loss: 0.001828758626410749, policy loss: 12.189362746314169
Experience 11, Iter 99, disc loss: 0.002210880786873224, policy loss: 12.499629723525487
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0107],
        [0.2736],
        [2.6834],
        [0.0355]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0723, 0.4449, 1.6892, 0.0348, 0.0085, 6.9884]],

        [[0.0723, 0.4449, 1.6892, 0.0348, 0.0085, 6.9884]],

        [[0.0723, 0.4449, 1.6892, 0.0348, 0.0085, 6.9884]],

        [[0.0723, 0.4449, 1.6892, 0.0348, 0.0085, 6.9884]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0430,  1.0945, 10.7338,  0.1421], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0430,  1.0945, 10.7338,  0.1421])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.338
Iter 2/2000 - Loss: 4.317
Iter 3/2000 - Loss: 4.241
Iter 4/2000 - Loss: 4.198
Iter 5/2000 - Loss: 4.175
Iter 6/2000 - Loss: 4.117
Iter 7/2000 - Loss: 4.048
Iter 8/2000 - Loss: 3.990
Iter 9/2000 - Loss: 3.926
Iter 10/2000 - Loss: 3.843
Iter 11/2000 - Loss: 3.746
Iter 12/2000 - Loss: 3.648
Iter 13/2000 - Loss: 3.547
Iter 14/2000 - Loss: 3.438
Iter 15/2000 - Loss: 3.314
Iter 16/2000 - Loss: 3.179
Iter 17/2000 - Loss: 3.034
Iter 18/2000 - Loss: 2.882
Iter 19/2000 - Loss: 2.720
Iter 20/2000 - Loss: 2.546
Iter 1981/2000 - Loss: -5.955
Iter 1982/2000 - Loss: -5.955
Iter 1983/2000 - Loss: -5.956
Iter 1984/2000 - Loss: -5.956
Iter 1985/2000 - Loss: -5.956
Iter 1986/2000 - Loss: -5.956
Iter 1987/2000 - Loss: -5.956
Iter 1988/2000 - Loss: -5.956
Iter 1989/2000 - Loss: -5.956
Iter 1990/2000 - Loss: -5.956
Iter 1991/2000 - Loss: -5.956
Iter 1992/2000 - Loss: -5.956
Iter 1993/2000 - Loss: -5.956
Iter 1994/2000 - Loss: -5.956
Iter 1995/2000 - Loss: -5.956
Iter 1996/2000 - Loss: -5.957
Iter 1997/2000 - Loss: -5.957
Iter 1998/2000 - Loss: -5.957
Iter 1999/2000 - Loss: -5.957
Iter 2000/2000 - Loss: -5.957
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0028],
        [0.0004]])
Lengthscale: tensor([[[24.4913, 16.1261, 42.6552, 17.7840,  4.3340, 70.4577]],

        [[27.4479, 40.8233, 10.2257,  0.9894,  1.5927, 17.4089]],

        [[33.3342, 51.6723,  9.6976,  0.9707,  1.2147, 21.3480]],

        [[27.6171, 46.0639, 17.4483,  3.0680,  2.1001, 43.4692]]])
Signal Variance: tensor([ 0.3259,  1.1787, 12.2434,  0.7334])
Estimated target variance: tensor([ 0.0430,  1.0945, 10.7338,  0.1421])
N: 120
Signal to noise ratio: tensor([30.8319, 51.8745, 66.2530, 41.5246])
Bound on condition number: tensor([114073.7360, 322916.4701, 526735.5720, 206915.9533])
Policy Optimizer learning rate:
0.09884772446450593
Experience 12, Iter 0, disc loss: 0.014766689607422239, policy loss: 12.614069259137693
Experience 12, Iter 1, disc loss: 0.0017642829808595432, policy loss: 12.266246262747185
Experience 12, Iter 2, disc loss: 0.0015855919130918388, policy loss: 12.50025952129838
Experience 12, Iter 3, disc loss: 0.0017296245961182643, policy loss: 12.332173127463893
Experience 12, Iter 4, disc loss: 0.0018965999951893646, policy loss: 12.246357764787579
Experience 12, Iter 5, disc loss: 0.0017333171024186658, policy loss: 12.803453399305512
Experience 12, Iter 6, disc loss: 0.001954927087776674, policy loss: 12.729059997470216
Experience 12, Iter 7, disc loss: 0.0016813292283955872, policy loss: 12.341464910712507
Experience 12, Iter 8, disc loss: 0.0019282515804224325, policy loss: 12.685880305483103
Experience 12, Iter 9, disc loss: 0.002481074746091794, policy loss: 12.636150832115648
Experience 12, Iter 10, disc loss: 0.001559013329481028, policy loss: 12.514279359337324
Experience 12, Iter 11, disc loss: 0.0015317282321736629, policy loss: 12.375935399862206
Experience 12, Iter 12, disc loss: 0.0015345883917361176, policy loss: 12.417012266032774
Experience 12, Iter 13, disc loss: 0.015882102943866456, policy loss: 12.619485245907237
Experience 12, Iter 14, disc loss: 0.0023620377439041453, policy loss: 12.093114730224865
Experience 12, Iter 15, disc loss: 0.004429324322093951, policy loss: 11.786970834002267
Experience 12, Iter 16, disc loss: 0.0024579536681240436, policy loss: 12.699028020454513
Experience 12, Iter 17, disc loss: 0.0017743850626828358, policy loss: 13.091344570694186
Experience 12, Iter 18, disc loss: 0.001870882042596041, policy loss: 12.346240608794325
Experience 12, Iter 19, disc loss: 0.022242834655141944, policy loss: 12.643313410772546
Experience 12, Iter 20, disc loss: 0.0018465267211485708, policy loss: 12.236085757578994
Experience 12, Iter 21, disc loss: 0.0019068381250506425, policy loss: 12.855128624607403
Experience 12, Iter 22, disc loss: 0.0019624103485182355, policy loss: 12.813722490105949
Experience 12, Iter 23, disc loss: 0.0015575668083334774, policy loss: 12.721687412146505
Experience 12, Iter 24, disc loss: 0.0016069070518191918, policy loss: 11.848951598487488
Experience 12, Iter 25, disc loss: 0.0021299232809096308, policy loss: 12.420765192755605
Experience 12, Iter 26, disc loss: 0.0016798896822190824, policy loss: 12.689260463321807
Experience 12, Iter 27, disc loss: 0.0016312154720381472, policy loss: 12.456300640411941
Experience 12, Iter 28, disc loss: 0.001861956582798279, policy loss: 12.656845578940274
Experience 12, Iter 29, disc loss: 0.004601966048106443, policy loss: 12.790473822976336
Experience 12, Iter 30, disc loss: 0.001506733956718851, policy loss: 12.40307463910535
Experience 12, Iter 31, disc loss: 0.004298019134822096, policy loss: 11.552509545734623
Experience 12, Iter 32, disc loss: 0.001810349078462262, policy loss: 12.168034013286467
Experience 12, Iter 33, disc loss: 0.002601674149401742, policy loss: 11.816592865655986
Experience 12, Iter 34, disc loss: 0.0019633038952183413, policy loss: 12.717296137277541
Experience 12, Iter 35, disc loss: 0.001737519539877625, policy loss: 11.941651665705919
Experience 12, Iter 36, disc loss: 0.0016131359047036197, policy loss: 12.501413736321794
Experience 12, Iter 37, disc loss: 0.009668516128395012, policy loss: 12.21546252244149
Experience 12, Iter 38, disc loss: 0.0017791525754251168, policy loss: 12.357662890703349
Experience 12, Iter 39, disc loss: 0.05037228940777793, policy loss: 11.700462234058392
Experience 12, Iter 40, disc loss: 0.0013069104973960723, policy loss: 13.152233342198661
Experience 12, Iter 41, disc loss: 0.002345008572168835, policy loss: 11.867859487649731
Experience 12, Iter 42, disc loss: 0.0016360156180872318, policy loss: 11.650410833596347
Experience 12, Iter 43, disc loss: 0.0015010555601846056, policy loss: 12.82862316262296
Experience 12, Iter 44, disc loss: 0.0016400361502833698, policy loss: 13.180089950400268
Experience 12, Iter 45, disc loss: 0.00364569212687018, policy loss: 11.884049673072191
Experience 12, Iter 46, disc loss: 0.021656026633753322, policy loss: 11.688197337523064
Experience 12, Iter 47, disc loss: 0.002768958147811067, policy loss: 12.528679252679858
Experience 12, Iter 48, disc loss: 0.001756692879490655, policy loss: 12.445422768421567
Experience 12, Iter 49, disc loss: 0.0017299348632456783, policy loss: 12.161020812836554
Experience 12, Iter 50, disc loss: 0.0015749832387216556, policy loss: 13.365239818345934
Experience 12, Iter 51, disc loss: 0.0019321099330225067, policy loss: 13.047150558340935
Experience 12, Iter 52, disc loss: 0.011295872589448905, policy loss: 12.828348132529882
Experience 12, Iter 53, disc loss: 0.002945279560371612, policy loss: 12.217309074988044
Experience 12, Iter 54, disc loss: 0.0017130138502557098, policy loss: 12.68881438406337
Experience 12, Iter 55, disc loss: 0.0025219654918853564, policy loss: 12.046741450325523
Experience 12, Iter 56, disc loss: 0.002405367204609002, policy loss: 12.96089987604895
Experience 12, Iter 57, disc loss: 0.002363272249466318, policy loss: 12.866825026336695
Experience 12, Iter 58, disc loss: 0.001687102620592997, policy loss: 12.374382881173542
Experience 12, Iter 59, disc loss: 0.0019664021908060276, policy loss: 12.47750163166383
Experience 12, Iter 60, disc loss: 0.0017239619960051694, policy loss: 12.311978296625973
Experience 12, Iter 61, disc loss: 0.0017775311460970647, policy loss: 12.439307826702585
Experience 12, Iter 62, disc loss: 0.0025843842160413327, policy loss: 12.24238145607593
Experience 12, Iter 63, disc loss: 0.0016588446370722405, policy loss: 12.238888849601526
Experience 12, Iter 64, disc loss: 0.0036319379146868883, policy loss: 12.929621057507855
Experience 12, Iter 65, disc loss: 0.033844394600313195, policy loss: 12.392306284971312
Experience 12, Iter 66, disc loss: 0.0015950969186940293, policy loss: 12.792320578340941
Experience 12, Iter 67, disc loss: 0.0018251674991716469, policy loss: 12.50706229342638
Experience 12, Iter 68, disc loss: 0.0018600214240462242, policy loss: 12.898585911903869
Experience 12, Iter 69, disc loss: 0.0020478628733832647, policy loss: 12.379245797079996
Experience 12, Iter 70, disc loss: 0.002176001126833252, policy loss: 12.740662391849531
Experience 12, Iter 71, disc loss: 0.0033871201538560347, policy loss: 12.886873111205041
Experience 12, Iter 72, disc loss: 0.0028620522962213126, policy loss: 12.831198169560153
Experience 12, Iter 73, disc loss: 0.0019443804032874311, policy loss: 12.652134145946174
Experience 12, Iter 74, disc loss: 0.0018179031439920694, policy loss: 13.196223065989503
Experience 12, Iter 75, disc loss: 0.001963367518823263, policy loss: 12.61942759083147
Experience 12, Iter 76, disc loss: 0.004729450452402071, policy loss: 12.697449995358475
Experience 12, Iter 77, disc loss: 0.0015285748102690775, policy loss: 12.646945843866684
Experience 12, Iter 78, disc loss: 0.0017964835260158765, policy loss: 12.987355785825635
Experience 12, Iter 79, disc loss: 0.0019586450722773752, policy loss: 12.52693535190101
Experience 12, Iter 80, disc loss: 0.0018482420708942934, policy loss: 12.626296666868651
Experience 12, Iter 81, disc loss: 0.0015016049595972364, policy loss: 13.231882585026678
Experience 12, Iter 82, disc loss: 0.0027970291082811804, policy loss: 13.007762683682461
Experience 12, Iter 83, disc loss: 0.0015281189755506864, policy loss: 12.354647183104909
Experience 12, Iter 84, disc loss: 0.02526234016871576, policy loss: 13.585729977874665
Experience 12, Iter 85, disc loss: 0.001413379163284755, policy loss: 13.354824446053744
Experience 12, Iter 86, disc loss: 0.0014818814164109652, policy loss: 12.60730384241622
Experience 12, Iter 87, disc loss: 0.0014511022694297464, policy loss: 12.805611417609036
Experience 12, Iter 88, disc loss: 0.005207483817232463, policy loss: 12.472811004344832
Experience 12, Iter 89, disc loss: 0.0014486887081024294, policy loss: 12.67775359755177
Experience 12, Iter 90, disc loss: 0.051532252720506266, policy loss: 12.592279593293465
Experience 12, Iter 91, disc loss: 0.0013561540541639576, policy loss: 13.41782906297328
Experience 12, Iter 92, disc loss: 0.0028235253603765918, policy loss: 12.165063286868378
Experience 12, Iter 93, disc loss: 0.0014003956664319836, policy loss: 14.296240295119654
Experience 12, Iter 94, disc loss: 0.0023417039860543585, policy loss: 12.683238752693391
Experience 12, Iter 95, disc loss: 0.002310321396232336, policy loss: 12.903461254459051
Experience 12, Iter 96, disc loss: 0.0014283328643325329, policy loss: 13.876035241414616
Experience 12, Iter 97, disc loss: 0.001641771456881087, policy loss: 12.633407630883031
Experience 12, Iter 98, disc loss: 0.0026146585966466285, policy loss: 12.705436535246713
Experience 12, Iter 99, disc loss: 0.0015002541281162625, policy loss: 12.41051280628152
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0103],
        [0.2732],
        [2.6956],
        [0.0372]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0705, 0.4289, 1.7680, 0.0368, 0.0096, 6.9947]],

        [[0.0705, 0.4289, 1.7680, 0.0368, 0.0096, 6.9947]],

        [[0.0705, 0.4289, 1.7680, 0.0368, 0.0096, 6.9947]],

        [[0.0705, 0.4289, 1.7680, 0.0368, 0.0096, 6.9947]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0413,  1.0927, 10.7824,  0.1487], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0413,  1.0927, 10.7824,  0.1487])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.331
Iter 2/2000 - Loss: 4.312
Iter 3/2000 - Loss: 4.227
Iter 4/2000 - Loss: 4.185
Iter 5/2000 - Loss: 4.151
Iter 6/2000 - Loss: 4.085
Iter 7/2000 - Loss: 4.013
Iter 8/2000 - Loss: 3.949
Iter 9/2000 - Loss: 3.877
Iter 10/2000 - Loss: 3.787
Iter 11/2000 - Loss: 3.686
Iter 12/2000 - Loss: 3.582
Iter 13/2000 - Loss: 3.474
Iter 14/2000 - Loss: 3.359
Iter 15/2000 - Loss: 3.231
Iter 16/2000 - Loss: 3.090
Iter 17/2000 - Loss: 2.938
Iter 18/2000 - Loss: 2.778
Iter 19/2000 - Loss: 2.607
Iter 20/2000 - Loss: 2.424
Iter 1981/2000 - Loss: -6.036
Iter 1982/2000 - Loss: -6.036
Iter 1983/2000 - Loss: -6.036
Iter 1984/2000 - Loss: -6.036
Iter 1985/2000 - Loss: -6.036
Iter 1986/2000 - Loss: -6.036
Iter 1987/2000 - Loss: -6.036
Iter 1988/2000 - Loss: -6.036
Iter 1989/2000 - Loss: -6.036
Iter 1990/2000 - Loss: -6.036
Iter 1991/2000 - Loss: -6.036
Iter 1992/2000 - Loss: -6.037
Iter 1993/2000 - Loss: -6.037
Iter 1994/2000 - Loss: -6.037
Iter 1995/2000 - Loss: -6.037
Iter 1996/2000 - Loss: -6.037
Iter 1997/2000 - Loss: -6.037
Iter 1998/2000 - Loss: -6.037
Iter 1999/2000 - Loss: -6.037
Iter 2000/2000 - Loss: -6.037
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[21.3203, 16.2694, 42.5416, 15.5408,  5.0604, 68.1127]],

        [[24.8669, 41.3977,  9.1555,  1.0688,  1.3297, 16.7832]],

        [[31.8659, 49.4508,  9.4327,  0.9234,  1.2586, 20.0873]],

        [[25.9236, 44.1297, 16.4625,  2.7924,  1.7811, 44.0985]]])
Signal Variance: tensor([ 0.3284,  1.1649, 11.3565,  0.5624])
Estimated target variance: tensor([ 0.0413,  1.0927, 10.7824,  0.1487])
N: 130
Signal to noise ratio: tensor([31.7241, 53.6278, 67.8493, 36.9100])
Bound on condition number: tensor([130835.6285, 373872.8531, 598460.2281, 177105.8332])
Policy Optimizer learning rate:
0.0987436328376607
Experience 13, Iter 0, disc loss: 0.0015052159710586162, policy loss: 13.667464658578627
Experience 13, Iter 1, disc loss: 0.0015364229580800443, policy loss: 12.815127729493803
Experience 13, Iter 2, disc loss: 0.001575771774184486, policy loss: 11.900275077644269
Experience 13, Iter 3, disc loss: 0.003149591504056742, policy loss: 11.930752400176837
Experience 13, Iter 4, disc loss: 0.0024589653155419945, policy loss: 12.175872218309623
Experience 13, Iter 5, disc loss: 0.019337946339541195, policy loss: 12.032638286768263
Experience 13, Iter 6, disc loss: 0.004991797789835001, policy loss: 11.300807176418271
Experience 13, Iter 7, disc loss: 0.002229849876969651, policy loss: 12.212247030575318
Experience 13, Iter 8, disc loss: 0.001710414588597955, policy loss: 12.190333248985727
Experience 13, Iter 9, disc loss: 0.0018962034682281882, policy loss: 13.457922598478763
Experience 13, Iter 10, disc loss: 0.0030000037049459373, policy loss: 12.63481455137642
Experience 13, Iter 11, disc loss: 0.001599311529276086, policy loss: 12.18609048108682
Experience 13, Iter 12, disc loss: 0.0029305186404670596, policy loss: 12.336637805126486
Experience 13, Iter 13, disc loss: 0.002895855631560645, policy loss: 12.491164832442061
Experience 13, Iter 14, disc loss: 0.0017127828459697187, policy loss: 12.935731199245897
Experience 13, Iter 15, disc loss: 0.00916081180310115, policy loss: 13.038779005146868
Experience 13, Iter 16, disc loss: 0.005677095983456561, policy loss: 12.473458632188725
Experience 13, Iter 17, disc loss: 0.0023076676366991095, policy loss: 12.49131273317591
Experience 13, Iter 18, disc loss: 0.049215172861383855, policy loss: 12.673922479506846
Experience 13, Iter 19, disc loss: 0.0016140202448951884, policy loss: 12.344270232328066
Experience 13, Iter 20, disc loss: 0.003414728673281883, policy loss: 13.393064311222965
Experience 13, Iter 21, disc loss: 0.001730403973544945, policy loss: 12.681032652849755
Experience 13, Iter 22, disc loss: 0.0018575310642658881, policy loss: 13.229949510385893
Experience 13, Iter 23, disc loss: 0.04461729700650214, policy loss: 11.785399048162889
Experience 13, Iter 24, disc loss: 0.0023299981647997015, policy loss: 12.78834455867469
Experience 13, Iter 25, disc loss: 0.002001643480549975, policy loss: 14.15150201384731
Experience 13, Iter 26, disc loss: 0.0028565875832596422, policy loss: 12.287712182139137
Experience 13, Iter 27, disc loss: 0.0022689327928927688, policy loss: 13.252136899936248
Experience 13, Iter 28, disc loss: 0.002271772396176979, policy loss: 12.407691945355644
Experience 13, Iter 29, disc loss: 0.0023098092546677816, policy loss: 12.743299521114283
Experience 13, Iter 30, disc loss: 0.0024386158065522997, policy loss: 12.734521647189291
Experience 13, Iter 31, disc loss: 0.0024047630141352636, policy loss: 13.160270569710915
Experience 13, Iter 32, disc loss: 0.0024226932399922844, policy loss: 12.934313800545937
Experience 13, Iter 33, disc loss: 0.0022239128588128526, policy loss: 12.80344821927257
Experience 13, Iter 34, disc loss: 0.0022908565400451067, policy loss: 12.904466786478665
Experience 13, Iter 35, disc loss: 0.006738625172629151, policy loss: 12.870273730239317
Experience 13, Iter 36, disc loss: 0.007128613307162678, policy loss: 13.250498404698355
Experience 13, Iter 37, disc loss: 0.00335482277421362, policy loss: 13.172015811471947
Experience 13, Iter 38, disc loss: 0.0035658934190500287, policy loss: 12.719862813203392
Experience 13, Iter 39, disc loss: 0.0023990600186180296, policy loss: 12.502133337169084
Experience 13, Iter 40, disc loss: 0.002090871768526311, policy loss: 12.969198614787366
Experience 13, Iter 41, disc loss: 0.0018861669209483676, policy loss: 14.149354589742167
Experience 13, Iter 42, disc loss: 0.0020974883490429345, policy loss: 13.605300881556467
Experience 13, Iter 43, disc loss: 0.0018087277816772442, policy loss: 13.532648525931808
Experience 13, Iter 44, disc loss: 0.001998280551180781, policy loss: 13.603235349477167
Experience 13, Iter 45, disc loss: 0.0021747174009848083, policy loss: 12.839376834520131
Experience 13, Iter 46, disc loss: 0.003063653296084573, policy loss: 12.637418250634285
Experience 13, Iter 47, disc loss: 0.0029454425860160945, policy loss: 11.898664721569663
Experience 13, Iter 48, disc loss: 0.001723430530900559, policy loss: 13.493615683730997
Experience 13, Iter 49, disc loss: 0.0022019887390005953, policy loss: 12.766244064428284
Experience 13, Iter 50, disc loss: 0.0028910824304201953, policy loss: 12.739382517650053
Experience 13, Iter 51, disc loss: 0.0014488808919395212, policy loss: 13.666339815692437
Experience 13, Iter 52, disc loss: 0.017913747965540926, policy loss: 12.174084132730965
Experience 13, Iter 53, disc loss: 0.0013142667937073734, policy loss: 14.217673546690559
Experience 13, Iter 54, disc loss: 0.03842855883691923, policy loss: 12.217315114508352
Experience 13, Iter 55, disc loss: 0.0013271575778992643, policy loss: 13.787475859258894
Experience 13, Iter 56, disc loss: 0.001898149438917895, policy loss: 13.192422329929526
Experience 13, Iter 57, disc loss: 0.0019030601734590286, policy loss: 12.853171341513514
Experience 13, Iter 58, disc loss: 0.0034282836658109826, policy loss: 12.362106181304405
Experience 13, Iter 59, disc loss: 0.0019547721094928866, policy loss: 12.882568488841462
Experience 13, Iter 60, disc loss: 0.0019821799458736933, policy loss: 13.168890168908812
Experience 13, Iter 61, disc loss: 0.0014822583957282169, policy loss: 12.761323888904661
Experience 13, Iter 62, disc loss: 0.001935947010186279, policy loss: 13.107705082972483
Experience 13, Iter 63, disc loss: 0.0016185623990235222, policy loss: 13.15413351891031
Experience 13, Iter 64, disc loss: 0.0030104197945616602, policy loss: 12.578738524773692
Experience 13, Iter 65, disc loss: 0.001778475959532092, policy loss: 13.427453500984132
Experience 13, Iter 66, disc loss: 0.002119698531472018, policy loss: 13.421196203691643
Experience 13, Iter 67, disc loss: 0.003953734914728033, policy loss: 13.568654935066771
Experience 13, Iter 68, disc loss: 0.001412020891435155, policy loss: 13.2923497138659
Experience 13, Iter 69, disc loss: 0.0017395780874383546, policy loss: 13.069066595493982
Experience 13, Iter 70, disc loss: 0.0017075884697795528, policy loss: 12.863218286908129
Experience 13, Iter 71, disc loss: 0.005353524754578503, policy loss: 13.171695130568388
Experience 13, Iter 72, disc loss: 0.002380954715277367, policy loss: 13.589273098937316
Experience 13, Iter 73, disc loss: 0.00973718757108986, policy loss: 13.244210163981723
Experience 13, Iter 74, disc loss: 0.0012591544717902709, policy loss: 13.699638606559267
Experience 13, Iter 75, disc loss: 0.0021641101734655053, policy loss: 13.14221902452309
Experience 13, Iter 76, disc loss: 0.005048484696465981, policy loss: 13.356381216002351
Experience 13, Iter 77, disc loss: 0.0014442532817407784, policy loss: 13.110055818871832
Experience 13, Iter 78, disc loss: 0.0013729002701144777, policy loss: 13.374541688742648
Experience 13, Iter 79, disc loss: 0.0014814072429036288, policy loss: 12.689965352253996
Experience 13, Iter 80, disc loss: 0.005028335050052403, policy loss: 13.020954096631005
Experience 13, Iter 81, disc loss: 0.0013257380788893331, policy loss: 12.58408601069528
Experience 13, Iter 82, disc loss: 0.0011853282292632842, policy loss: 12.928628235224625
Experience 13, Iter 83, disc loss: 0.008277742804630972, policy loss: 13.035360938528186
Experience 13, Iter 84, disc loss: 0.0019349935749330529, policy loss: 12.866319414367604
Experience 13, Iter 85, disc loss: 0.003084467177720593, policy loss: 12.911466650516315
Experience 13, Iter 86, disc loss: 0.001608817845306658, policy loss: 12.89960002023318
Experience 13, Iter 87, disc loss: 0.0011370074176907275, policy loss: 13.385901094596715
Experience 13, Iter 88, disc loss: 0.0011772649308215612, policy loss: 13.453432123794826
Experience 13, Iter 89, disc loss: 0.004927864726200337, policy loss: 12.532004280225154
Experience 13, Iter 90, disc loss: 0.001626386833356721, policy loss: 13.230432409558881
Experience 13, Iter 91, disc loss: 0.0012381777980426203, policy loss: 12.157997741731542
Experience 13, Iter 92, disc loss: 0.0010872258205507528, policy loss: 13.379661310557516
Experience 13, Iter 93, disc loss: 0.001426703040326367, policy loss: 13.12998410837674
Experience 13, Iter 94, disc loss: 0.001115487471453359, policy loss: 13.240654184303756
Experience 13, Iter 95, disc loss: 0.0010678554228066562, policy loss: 13.831747657438838
Experience 13, Iter 96, disc loss: 0.0012182884479102502, policy loss: 12.634869225369442
Experience 13, Iter 97, disc loss: 0.004122277553467393, policy loss: 13.237600111441363
Experience 13, Iter 98, disc loss: 0.0023174590027502267, policy loss: 12.43560987242358
Experience 13, Iter 99, disc loss: 0.0010021275352956954, policy loss: 13.810178309774422
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0098],
        [0.2754],
        [2.7326],
        [0.0393]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0665, 0.4075, 1.8006, 0.0366, 0.0097, 6.9909]],

        [[0.0665, 0.4075, 1.8006, 0.0366, 0.0097, 6.9909]],

        [[0.0665, 0.4075, 1.8006, 0.0366, 0.0097, 6.9909]],

        [[0.0665, 0.4075, 1.8006, 0.0366, 0.0097, 6.9909]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0392,  1.1014, 10.9302,  0.1570], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0392,  1.1014, 10.9302,  0.1570])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.347
Iter 2/2000 - Loss: 4.335
Iter 3/2000 - Loss: 4.245
Iter 4/2000 - Loss: 4.199
Iter 5/2000 - Loss: 4.168
Iter 6/2000 - Loss: 4.099
Iter 7/2000 - Loss: 4.022
Iter 8/2000 - Loss: 3.955
Iter 9/2000 - Loss: 3.883
Iter 10/2000 - Loss: 3.790
Iter 11/2000 - Loss: 3.685
Iter 12/2000 - Loss: 3.578
Iter 13/2000 - Loss: 3.469
Iter 14/2000 - Loss: 3.352
Iter 15/2000 - Loss: 3.221
Iter 16/2000 - Loss: 3.077
Iter 17/2000 - Loss: 2.923
Iter 18/2000 - Loss: 2.760
Iter 19/2000 - Loss: 2.588
Iter 20/2000 - Loss: 2.403
Iter 1981/2000 - Loss: -6.137
Iter 1982/2000 - Loss: -6.137
Iter 1983/2000 - Loss: -6.137
Iter 1984/2000 - Loss: -6.137
Iter 1985/2000 - Loss: -6.137
Iter 1986/2000 - Loss: -6.138
Iter 1987/2000 - Loss: -6.138
Iter 1988/2000 - Loss: -6.138
Iter 1989/2000 - Loss: -6.138
Iter 1990/2000 - Loss: -6.138
Iter 1991/2000 - Loss: -6.138
Iter 1992/2000 - Loss: -6.138
Iter 1993/2000 - Loss: -6.138
Iter 1994/2000 - Loss: -6.138
Iter 1995/2000 - Loss: -6.138
Iter 1996/2000 - Loss: -6.138
Iter 1997/2000 - Loss: -6.138
Iter 1998/2000 - Loss: -6.138
Iter 1999/2000 - Loss: -6.138
Iter 2000/2000 - Loss: -6.139
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0004]])
Lengthscale: tensor([[[21.4100, 15.9989, 42.2251, 16.4515,  4.4015, 67.9999]],

        [[26.4938, 42.0755,  9.2591,  1.0630,  1.2267, 16.9581]],

        [[32.1805, 47.3719,  9.3656,  0.9153,  1.1973, 20.4876]],

        [[25.9937, 43.3248, 15.9319,  1.6355,  1.8487, 50.3584]]])
Signal Variance: tensor([ 0.3063,  1.1136, 11.1599,  0.5491])
Estimated target variance: tensor([ 0.0392,  1.1014, 10.9302,  0.1570])
N: 140
Signal to noise ratio: tensor([30.1255, 51.8862, 69.3689, 37.6186])
Bound on condition number: tensor([127057.3231, 376906.4562, 673686.6611, 198123.2665])
Policy Optimizer learning rate:
0.09863965082453535
Experience 14, Iter 0, disc loss: 0.00226874515878086, policy loss: 12.268600156175076
Experience 14, Iter 1, disc loss: 0.001002955842277385, policy loss: 13.796708266531663
Experience 14, Iter 2, disc loss: 0.009060031933439381, policy loss: 13.549574142914947
Experience 14, Iter 3, disc loss: 0.0018671549457887023, policy loss: 12.910497119042482
Experience 14, Iter 4, disc loss: 0.0010788640539086836, policy loss: 12.768081180546403
Experience 14, Iter 5, disc loss: 0.0016417630846369542, policy loss: 14.288487562450477
Experience 14, Iter 6, disc loss: 0.0013039492010743828, policy loss: 12.6826579622189
Experience 14, Iter 7, disc loss: 0.0009634341452527586, policy loss: 14.125784504918292
Experience 14, Iter 8, disc loss: 0.0010625738751480322, policy loss: 13.155782756718361
Experience 14, Iter 9, disc loss: 0.0010525114301383365, policy loss: 13.794681065855745
Experience 14, Iter 10, disc loss: 0.0010862161265828554, policy loss: 13.16183300067311
Experience 14, Iter 11, disc loss: 0.0011525334288404387, policy loss: 13.406526942188531
Experience 14, Iter 12, disc loss: 0.002091980278261673, policy loss: 12.412481578079298
Experience 14, Iter 13, disc loss: 0.0009187037684399668, policy loss: 13.456556987229305
Experience 14, Iter 14, disc loss: 0.0009075681195851814, policy loss: 14.071067702087674
Experience 14, Iter 15, disc loss: 0.004853363543113708, policy loss: 12.168551980646363
Experience 14, Iter 16, disc loss: 0.023028185059580843, policy loss: 13.668713121457172
Experience 14, Iter 17, disc loss: 0.0030137187200602487, policy loss: 12.817694918509481
Experience 14, Iter 18, disc loss: 0.0012689004353801496, policy loss: 13.11271210860992
Experience 14, Iter 19, disc loss: 0.0010430637112522848, policy loss: 13.33149616036594
Experience 14, Iter 20, disc loss: 0.0011220026725596139, policy loss: 12.96212997358493
Experience 14, Iter 21, disc loss: 0.0012259564693503403, policy loss: 12.854049664399867
Experience 14, Iter 22, disc loss: 0.0016804384094490105, policy loss: 12.95745901721194
Experience 14, Iter 23, disc loss: 0.0011009263550464873, policy loss: 13.480675210184891
Experience 14, Iter 24, disc loss: 0.0009942928372148423, policy loss: 13.831690574893871
Experience 14, Iter 25, disc loss: 0.001011823934256604, policy loss: 13.014873851415778
Experience 14, Iter 26, disc loss: 0.002308471601589837, policy loss: 13.485590638979414
Experience 14, Iter 27, disc loss: 0.0017085804890167944, policy loss: 13.296679664386774
Experience 14, Iter 28, disc loss: 0.001080097165824246, policy loss: 13.971073892207315
Experience 14, Iter 29, disc loss: 0.023463271924498538, policy loss: 14.096860995441656
Experience 14, Iter 30, disc loss: 0.0014124751876797097, policy loss: 12.49710015401547
Experience 14, Iter 31, disc loss: 0.0030887715134273997, policy loss: 12.312553306305146
Experience 14, Iter 32, disc loss: 0.0009616847463701593, policy loss: 13.780911790396004
Experience 14, Iter 33, disc loss: 0.0010519759927382815, policy loss: 13.983713465430327
Experience 14, Iter 34, disc loss: 0.0021021412096561744, policy loss: 12.494394633240061
Experience 14, Iter 35, disc loss: 0.0030121553331840037, policy loss: 12.997190613972297
Experience 14, Iter 36, disc loss: 0.0011243696498144883, policy loss: 13.27237938000437
Experience 14, Iter 37, disc loss: 0.0030418305424927787, policy loss: 12.977315454289693
Experience 14, Iter 38, disc loss: 0.0017152082662204903, policy loss: 12.00090555539294
Experience 14, Iter 39, disc loss: 0.0016380345705399504, policy loss: 12.177545959836419
Experience 14, Iter 40, disc loss: 0.0010955277256080715, policy loss: 13.491184654108707
Experience 14, Iter 41, disc loss: 0.00816103586651292, policy loss: 12.650857684996431
Experience 14, Iter 42, disc loss: 0.0010871152733821184, policy loss: 13.590544559572688
Experience 14, Iter 43, disc loss: 0.001225046445553389, policy loss: 13.260887873231882
Experience 14, Iter 44, disc loss: 0.0014353531015673002, policy loss: 13.509432680088208
Experience 14, Iter 45, disc loss: 0.0011754242467175969, policy loss: 13.32436088198663
Experience 14, Iter 46, disc loss: 0.0030623254200958724, policy loss: 13.437904242312534
Experience 14, Iter 47, disc loss: 0.001058651226098304, policy loss: 13.832741074504678
Experience 14, Iter 48, disc loss: 0.001086037707534249, policy loss: 13.381386878049273
Experience 14, Iter 49, disc loss: 0.0012066322811978812, policy loss: 13.22904531353114
Experience 14, Iter 50, disc loss: 0.0035836320716432254, policy loss: 11.823184976541965
Experience 14, Iter 51, disc loss: 0.008114399297554153, policy loss: 13.78404956727374
Experience 14, Iter 52, disc loss: 0.01188941957461939, policy loss: 13.220727738916864
Experience 14, Iter 53, disc loss: 0.001567349611851737, policy loss: 13.076182109610372
Experience 14, Iter 54, disc loss: 0.0010686047434750505, policy loss: 13.788104444113358
Experience 14, Iter 55, disc loss: 0.0016007079156309225, policy loss: 14.08896447635727
Experience 14, Iter 56, disc loss: 0.010644175549457532, policy loss: 13.151452553676947
Experience 14, Iter 57, disc loss: 0.0011897456557836221, policy loss: 12.984570363357575
Experience 14, Iter 58, disc loss: 0.012972574301275094, policy loss: 13.48463140356905
Experience 14, Iter 59, disc loss: 0.0021717431564336354, policy loss: 13.287900159259806
Experience 14, Iter 60, disc loss: 0.005129883885737903, policy loss: 13.013233256440959
Experience 14, Iter 61, disc loss: 0.006742895430342516, policy loss: 12.854276491053962
Experience 14, Iter 62, disc loss: 0.001413679666059807, policy loss: 13.83469173157507
Experience 14, Iter 63, disc loss: 0.001515004120842247, policy loss: 14.41870659290809
Experience 14, Iter 64, disc loss: 0.003470220478341404, policy loss: 12.771130138263018
Experience 14, Iter 65, disc loss: 0.0016493328354551407, policy loss: 14.093273522891133
Experience 14, Iter 66, disc loss: 0.00167935647322538, policy loss: 13.738093723944656
Experience 14, Iter 67, disc loss: 0.0018991688136756352, policy loss: 14.158370114789825
Experience 14, Iter 68, disc loss: 0.0018629240334628087, policy loss: 13.87858848903897
Experience 14, Iter 69, disc loss: 0.002164820683664882, policy loss: 13.01490733879033
Experience 14, Iter 70, disc loss: 0.0018903895728893834, policy loss: 14.042786541876724
Experience 14, Iter 71, disc loss: 0.0024360497675225603, policy loss: 13.235460255722145
Experience 14, Iter 72, disc loss: 0.0016401021131264846, policy loss: 13.367465728585703
Experience 14, Iter 73, disc loss: 0.0015882899290164815, policy loss: 13.629556988104099
Experience 14, Iter 74, disc loss: 0.00181758159262832, policy loss: 14.056237793714441
Experience 14, Iter 75, disc loss: 0.0018370481246768222, policy loss: 13.55100366691917
Experience 14, Iter 76, disc loss: 0.0017464324907217903, policy loss: 12.873981936101929
Experience 14, Iter 77, disc loss: 0.044941920829472896, policy loss: 13.11897856246723
Experience 14, Iter 78, disc loss: 0.005414105198081189, policy loss: 14.16138633555704
Experience 14, Iter 79, disc loss: 0.001536214954356323, policy loss: 13.849769834060695
Experience 14, Iter 80, disc loss: 0.0016443499092829842, policy loss: 12.340649662846047
Experience 14, Iter 81, disc loss: 0.0029669593595954024, policy loss: 13.781952276295824
Experience 14, Iter 82, disc loss: 0.0017005732094101467, policy loss: 12.93878494497419
Experience 14, Iter 83, disc loss: 0.0017876652085135171, policy loss: 12.809086832390388
Experience 14, Iter 84, disc loss: 0.0016598405666423013, policy loss: 13.327572582009472
Experience 14, Iter 85, disc loss: 0.0019234025582842925, policy loss: 13.128841503281798
Experience 14, Iter 86, disc loss: 0.007918792785181827, policy loss: 12.577174445674888
Experience 14, Iter 87, disc loss: 0.0028095161020547117, policy loss: 14.420171099268753
Experience 14, Iter 88, disc loss: 0.001983309075156526, policy loss: 13.65080993685489
Experience 14, Iter 89, disc loss: 0.0575997986618952, policy loss: 13.170631515563382
Experience 14, Iter 90, disc loss: 0.0019062842094175688, policy loss: 13.394620217442181
Experience 14, Iter 91, disc loss: 0.0017291425133702497, policy loss: 14.286008695457346
Experience 14, Iter 92, disc loss: 0.0025557522106767873, policy loss: 13.794217286538064
Experience 14, Iter 93, disc loss: 0.0018708542204925135, policy loss: 13.97416511492235
Experience 14, Iter 94, disc loss: 0.0019888248318898215, policy loss: 13.601251501502809
Experience 14, Iter 95, disc loss: 0.002067265565884043, policy loss: 13.61488439688743
Experience 14, Iter 96, disc loss: 0.0019380332153281347, policy loss: 13.299830147401227
Experience 14, Iter 97, disc loss: 0.001867717085009047, policy loss: 13.552745736868157
Experience 14, Iter 98, disc loss: 0.0026822102842714137, policy loss: 14.319235651502277
Experience 14, Iter 99, disc loss: 0.03788901508866679, policy loss: 13.882396482222267
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0096],
        [0.2861],
        [2.8633],
        [0.0414]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0624, 0.4018, 1.9128, 0.0366, 0.0094, 7.0644]],

        [[0.0624, 0.4018, 1.9128, 0.0366, 0.0094, 7.0644]],

        [[0.0624, 0.4018, 1.9128, 0.0366, 0.0094, 7.0644]],

        [[0.0624, 0.4018, 1.9128, 0.0366, 0.0094, 7.0644]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0382,  1.1445, 11.4530,  0.1657], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0382,  1.1445, 11.4530,  0.1657])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.412
Iter 2/2000 - Loss: 4.394
Iter 3/2000 - Loss: 4.310
Iter 4/2000 - Loss: 4.263
Iter 5/2000 - Loss: 4.231
Iter 6/2000 - Loss: 4.163
Iter 7/2000 - Loss: 4.089
Iter 8/2000 - Loss: 4.022
Iter 9/2000 - Loss: 3.948
Iter 10/2000 - Loss: 3.856
Iter 11/2000 - Loss: 3.752
Iter 12/2000 - Loss: 3.646
Iter 13/2000 - Loss: 3.536
Iter 14/2000 - Loss: 3.416
Iter 15/2000 - Loss: 3.284
Iter 16/2000 - Loss: 3.139
Iter 17/2000 - Loss: 2.983
Iter 18/2000 - Loss: 2.820
Iter 19/2000 - Loss: 2.646
Iter 20/2000 - Loss: 2.459
Iter 1981/2000 - Loss: -6.206
Iter 1982/2000 - Loss: -6.206
Iter 1983/2000 - Loss: -6.206
Iter 1984/2000 - Loss: -6.206
Iter 1985/2000 - Loss: -6.206
Iter 1986/2000 - Loss: -6.206
Iter 1987/2000 - Loss: -6.206
Iter 1988/2000 - Loss: -6.206
Iter 1989/2000 - Loss: -6.206
Iter 1990/2000 - Loss: -6.206
Iter 1991/2000 - Loss: -6.206
Iter 1992/2000 - Loss: -6.206
Iter 1993/2000 - Loss: -6.206
Iter 1994/2000 - Loss: -6.206
Iter 1995/2000 - Loss: -6.206
Iter 1996/2000 - Loss: -6.207
Iter 1997/2000 - Loss: -6.207
Iter 1998/2000 - Loss: -6.207
Iter 1999/2000 - Loss: -6.207
Iter 2000/2000 - Loss: -6.207
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0004]])
Lengthscale: tensor([[[22.7640, 16.4045, 35.0989, 18.6168,  4.6542, 69.4929]],

        [[26.3805, 43.2389,  8.9338,  1.0996,  1.1859, 18.1445]],

        [[32.1785, 48.6150,  9.2144,  0.9336,  1.0921, 21.0610]],

        [[25.2827, 43.6209, 16.0922,  1.4738,  1.7511, 52.3137]]])
Signal Variance: tensor([ 0.3088,  1.2167, 11.1494,  0.5155])
Estimated target variance: tensor([ 0.0382,  1.1445, 11.4530,  0.1657])
N: 150
Signal to noise ratio: tensor([29.9393, 54.4028, 69.1026, 37.4532])
Bound on condition number: tensor([134455.3817, 443950.9832, 716275.9316, 210412.0248])
Policy Optimizer learning rate:
0.0985357783097011
Experience 15, Iter 0, disc loss: 0.001959201574116243, policy loss: 13.565420433855266
Experience 15, Iter 1, disc loss: 0.002019304018454185, policy loss: 14.369477044262318
Experience 15, Iter 2, disc loss: 0.012532694488399453, policy loss: 12.953839452482823
Experience 15, Iter 3, disc loss: 0.0021095966979094057, policy loss: 14.6380448666594
Experience 15, Iter 4, disc loss: 0.0020857406550473167, policy loss: 14.15222456306186
Experience 15, Iter 5, disc loss: 0.0036557364870590248, policy loss: 13.935974852549956
Experience 15, Iter 6, disc loss: 0.0024365621531110037, policy loss: 14.278969780520448
Experience 15, Iter 7, disc loss: 0.01019828821486964, policy loss: 13.025524143494186
Experience 15, Iter 8, disc loss: 0.0022002901258600174, policy loss: 13.053319355277862
Experience 15, Iter 9, disc loss: 0.0026831902858813995, policy loss: 14.312954621221573
Experience 15, Iter 10, disc loss: 0.0021848624325851384, policy loss: 14.082403808090358
Experience 15, Iter 11, disc loss: 0.002149407197303806, policy loss: 13.03922430947544
Experience 15, Iter 12, disc loss: 0.0022456212757047344, policy loss: 13.600042501114608
Experience 15, Iter 13, disc loss: 0.014346672000497877, policy loss: 13.64444281604384
Experience 15, Iter 14, disc loss: 0.0023169951261216917, policy loss: 14.352071258866982
Experience 15, Iter 15, disc loss: 0.002025534287228121, policy loss: 14.680428674152921
Experience 15, Iter 16, disc loss: 0.001987785669671742, policy loss: 15.02382139362495
Experience 15, Iter 17, disc loss: 0.0024889267854830447, policy loss: 13.588248409178721
Experience 15, Iter 18, disc loss: 0.002501114745820143, policy loss: 14.28225849721691
Experience 15, Iter 19, disc loss: 0.002628836784155296, policy loss: 14.254633526256871
Experience 15, Iter 20, disc loss: 0.0020678989348483353, policy loss: 14.612377195223488
Experience 15, Iter 21, disc loss: 0.0017750265631836402, policy loss: 13.21947414029836
Experience 15, Iter 22, disc loss: 0.0019854951728887913, policy loss: 13.198646055391848
Experience 15, Iter 23, disc loss: 0.0058512124441055325, policy loss: 13.494507944591728
Experience 15, Iter 24, disc loss: 0.0016389450100132426, policy loss: 14.793254277945216
Experience 15, Iter 25, disc loss: 0.0015039873869614716, policy loss: 14.432040884649645
Experience 15, Iter 26, disc loss: 0.0024561509622849304, policy loss: 14.64186322042485
Experience 15, Iter 27, disc loss: 0.0016333379710925583, policy loss: 13.400878869636397
Experience 15, Iter 28, disc loss: 0.001550409706313626, policy loss: 14.429232832865797
Experience 15, Iter 29, disc loss: 0.001544099109120378, policy loss: 13.967110506470426
Experience 15, Iter 30, disc loss: 0.0014189555590196417, policy loss: 14.037788615535987
Experience 15, Iter 31, disc loss: 0.0016261986604324465, policy loss: 13.92641945124063
Experience 15, Iter 32, disc loss: 0.0011515994558206528, policy loss: 13.277894832102032
Experience 15, Iter 33, disc loss: 0.0010842031528744945, policy loss: 14.54777683779787
Experience 15, Iter 34, disc loss: 0.0015515603325993456, policy loss: 13.846275858128134
Experience 15, Iter 35, disc loss: 0.002286792375044488, policy loss: 13.19489998983498
Experience 15, Iter 36, disc loss: 0.0016503862831808774, policy loss: 13.174371329680639
Experience 15, Iter 37, disc loss: 0.0011602148038252747, policy loss: 13.966521193788275
Experience 15, Iter 38, disc loss: 0.0010616384175438827, policy loss: 13.656449742629325
Experience 15, Iter 39, disc loss: 0.0010927809007100395, policy loss: 14.226036428033556
Experience 15, Iter 40, disc loss: 0.001981552474800024, policy loss: 13.582080008555966
Experience 15, Iter 41, disc loss: 0.0008847152263472503, policy loss: 14.782016350265518
Experience 15, Iter 42, disc loss: 0.022974245374304775, policy loss: 13.972406570217931
Experience 15, Iter 43, disc loss: 0.0015582301161745915, policy loss: 14.0326913622474
Experience 15, Iter 44, disc loss: 0.06355704023118137, policy loss: 12.753799709265744
Experience 15, Iter 45, disc loss: 0.0011201513715163966, policy loss: 13.353270355548219
Experience 15, Iter 46, disc loss: 0.0011937818027793175, policy loss: 13.824397900552608
Experience 15, Iter 47, disc loss: 0.0010910734787209799, policy loss: 13.922375032854749
Experience 15, Iter 48, disc loss: 0.001023908802759615, policy loss: 13.898638007418452
Experience 15, Iter 49, disc loss: 0.0010699430491251044, policy loss: 13.624257252320263
Experience 15, Iter 50, disc loss: 0.0012244720794318533, policy loss: 12.359291205938625
Experience 15, Iter 51, disc loss: 0.0013555884633655452, policy loss: 13.601013798722253
Experience 15, Iter 52, disc loss: 0.04316873277416056, policy loss: 14.212487794913587
Experience 15, Iter 53, disc loss: 0.002644025605469437, policy loss: 14.14636283239551
Experience 15, Iter 54, disc loss: 0.002796434291865317, policy loss: 13.934685844874673
Experience 15, Iter 55, disc loss: 0.012306269973427587, policy loss: 13.956646256257374
Experience 15, Iter 56, disc loss: 0.0014938951785018037, policy loss: 13.474859169647631
Experience 15, Iter 57, disc loss: 0.0029719668001106214, policy loss: 14.524292034147908
Experience 15, Iter 58, disc loss: 0.0019183309784545413, policy loss: 14.228357773168314
Experience 15, Iter 59, disc loss: 0.0015265069548549626, policy loss: 14.401984089030833
Experience 15, Iter 60, disc loss: 0.0016362532938080942, policy loss: 15.133172748531354
Experience 15, Iter 61, disc loss: 0.0015695774583357404, policy loss: 14.932330222822664
Experience 15, Iter 62, disc loss: 0.0018472067945432356, policy loss: 14.260023972405847
Experience 15, Iter 63, disc loss: 0.0015805571680470216, policy loss: 14.302330980135684
Experience 15, Iter 64, disc loss: 0.0017791629082501148, policy loss: 14.299679703004726
Experience 15, Iter 65, disc loss: 0.001744421048040579, policy loss: 15.169613806210236
Experience 15, Iter 66, disc loss: 0.001749678388037748, policy loss: 14.038802877260698
Experience 15, Iter 67, disc loss: 0.0027715251844190993, policy loss: 14.018047893033675
Experience 15, Iter 68, disc loss: 0.00151927553712678, policy loss: 13.883659998441189
Experience 15, Iter 69, disc loss: 0.002482511726440631, policy loss: 14.177672848645248
Experience 15, Iter 70, disc loss: 0.001595386570803295, policy loss: 14.545198391213656
Experience 15, Iter 71, disc loss: 0.005637112762216152, policy loss: 14.09496723559393
Experience 15, Iter 72, disc loss: 0.001356578226527784, policy loss: 13.69696585874684
Experience 15, Iter 73, disc loss: 0.0062335245053034005, policy loss: 13.90149101693494
Experience 15, Iter 74, disc loss: 0.0013075784336516085, policy loss: 14.970472403361306
Experience 15, Iter 75, disc loss: 0.002826195889286685, policy loss: 14.225910756184433
Experience 15, Iter 76, disc loss: 0.0013011226430224847, policy loss: 14.048520377566946
Experience 15, Iter 77, disc loss: 0.0014625007381846495, policy loss: 13.352650280908597
Experience 15, Iter 78, disc loss: 0.0012481199304226367, policy loss: 14.38149545758027
Experience 15, Iter 79, disc loss: 0.0012386356740304567, policy loss: 14.15453089168276
Experience 15, Iter 80, disc loss: 0.002353434531722847, policy loss: 14.09477482441426
Experience 15, Iter 81, disc loss: 0.01577809522023927, policy loss: 14.380685078295073
Experience 15, Iter 82, disc loss: 0.0011708569703768818, policy loss: 14.701373957990128
Experience 15, Iter 83, disc loss: 0.0030656521556702418, policy loss: 13.265634939968471
Experience 15, Iter 84, disc loss: 0.0012140668343708684, policy loss: 14.441245063318487
Experience 15, Iter 85, disc loss: 0.013528583585051775, policy loss: 13.965857098930302
Experience 15, Iter 86, disc loss: 0.0013760969643444884, policy loss: 14.48125564921104
Experience 15, Iter 87, disc loss: 0.0012071128949085624, policy loss: 14.686648317136004
Experience 15, Iter 88, disc loss: 0.0017796770509554962, policy loss: 13.903969643291582
Experience 15, Iter 89, disc loss: 0.0014667694838384586, policy loss: 14.407847855859107
Experience 15, Iter 90, disc loss: 0.0013377188986858908, policy loss: 14.385348163371647
Experience 15, Iter 91, disc loss: 0.0013168282436445693, policy loss: 14.873302689461338
Experience 15, Iter 92, disc loss: 0.0014753962253279396, policy loss: 13.787908490828688
Experience 15, Iter 93, disc loss: 0.0016624265289955976, policy loss: 13.994817149374022
Experience 15, Iter 94, disc loss: 0.0013821487163729503, policy loss: 14.54790882774423
Experience 15, Iter 95, disc loss: 0.0013796924798538657, policy loss: 13.878610630171789
Experience 15, Iter 96, disc loss: 0.0013016669928603752, policy loss: 14.53603532901473
Experience 15, Iter 97, disc loss: 0.0011448733735458226, policy loss: 14.9686305920923
Experience 15, Iter 98, disc loss: 0.0011685837365420033, policy loss: 13.793088613542782
Experience 15, Iter 99, disc loss: 0.0016544925853256478, policy loss: 15.026788154757309
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0094],
        [0.2877],
        [2.8878],
        [0.0404]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0595, 0.3961, 1.8802, 0.0360, 0.0090, 7.0286]],

        [[0.0595, 0.3961, 1.8802, 0.0360, 0.0090, 7.0286]],

        [[0.0595, 0.3961, 1.8802, 0.0360, 0.0090, 7.0286]],

        [[0.0595, 0.3961, 1.8802, 0.0360, 0.0090, 7.0286]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0376,  1.1507, 11.5510,  0.1615], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0376,  1.1507, 11.5510,  0.1615])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.394
Iter 2/2000 - Loss: 4.400
Iter 3/2000 - Loss: 4.297
Iter 4/2000 - Loss: 4.258
Iter 5/2000 - Loss: 4.231
Iter 6/2000 - Loss: 4.159
Iter 7/2000 - Loss: 4.077
Iter 8/2000 - Loss: 4.009
Iter 9/2000 - Loss: 3.940
Iter 10/2000 - Loss: 3.850
Iter 11/2000 - Loss: 3.742
Iter 12/2000 - Loss: 3.629
Iter 13/2000 - Loss: 3.516
Iter 14/2000 - Loss: 3.396
Iter 15/2000 - Loss: 3.264
Iter 16/2000 - Loss: 3.116
Iter 17/2000 - Loss: 2.954
Iter 18/2000 - Loss: 2.782
Iter 19/2000 - Loss: 2.602
Iter 20/2000 - Loss: 2.412
Iter 1981/2000 - Loss: -6.344
Iter 1982/2000 - Loss: -6.344
Iter 1983/2000 - Loss: -6.344
Iter 1984/2000 - Loss: -6.344
Iter 1985/2000 - Loss: -6.344
Iter 1986/2000 - Loss: -6.345
Iter 1987/2000 - Loss: -6.345
Iter 1988/2000 - Loss: -6.345
Iter 1989/2000 - Loss: -6.345
Iter 1990/2000 - Loss: -6.345
Iter 1991/2000 - Loss: -6.345
Iter 1992/2000 - Loss: -6.345
Iter 1993/2000 - Loss: -6.345
Iter 1994/2000 - Loss: -6.345
Iter 1995/2000 - Loss: -6.345
Iter 1996/2000 - Loss: -6.345
Iter 1997/2000 - Loss: -6.345
Iter 1998/2000 - Loss: -6.345
Iter 1999/2000 - Loss: -6.345
Iter 2000/2000 - Loss: -6.345
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[22.0871, 16.5190, 34.4171, 18.2236,  4.6556, 68.6184]],

        [[26.6001, 41.7416,  8.6029,  1.1740,  1.2394, 19.9584]],

        [[32.1542, 46.5552,  9.0586,  0.9390,  1.1222, 21.3130]],

        [[24.9574, 42.9600, 16.0338,  1.5681,  1.7047, 49.7499]]])
Signal Variance: tensor([ 0.3123,  1.3707, 11.6833,  0.5084])
Estimated target variance: tensor([ 0.0376,  1.1507, 11.5510,  0.1615])
N: 160
Signal to noise ratio: tensor([30.7475, 56.5553, 68.3595, 37.7405])
Bound on condition number: tensor([151266.7836, 511761.7254, 747684.6228, 227896.1768])
Policy Optimizer learning rate:
0.09843201517785076
Experience 16, Iter 0, disc loss: 0.0012313934669687043, policy loss: 15.373786436812647
Experience 16, Iter 1, disc loss: 0.0011467745302990882, policy loss: 13.733894716142572
Experience 16, Iter 2, disc loss: 0.02979178775090756, policy loss: 14.120477669053521
Experience 16, Iter 3, disc loss: 0.004011491117703856, policy loss: 13.637634344623606
Experience 16, Iter 4, disc loss: 0.003017672597914375, policy loss: 14.241051132520303
Experience 16, Iter 5, disc loss: 0.003650947047537119, policy loss: 13.507296726682759
Experience 16, Iter 6, disc loss: 0.001084517632084402, policy loss: 14.70257513282053
Experience 16, Iter 7, disc loss: 0.001149276200523306, policy loss: 14.372097289392904
Experience 16, Iter 8, disc loss: 0.0010947423805770782, policy loss: 14.371125537291004
Experience 16, Iter 9, disc loss: 0.0014132194416550238, policy loss: 14.487558413949728
Experience 16, Iter 10, disc loss: 0.0010747132220195702, policy loss: 15.248247344830922
Experience 16, Iter 11, disc loss: 0.0022804333996684157, policy loss: 13.895979655749265
Experience 16, Iter 12, disc loss: 0.0019384697875568999, policy loss: 14.806802633567118
Experience 16, Iter 13, disc loss: 0.0012479796575792907, policy loss: 14.234144316821437
Experience 16, Iter 14, disc loss: 0.038319712816202715, policy loss: 13.442551905978998
Experience 16, Iter 15, disc loss: 0.00122539837791643, policy loss: 15.203583883342802
Experience 16, Iter 16, disc loss: 0.001214458602221251, policy loss: 14.014508608753331
Experience 16, Iter 17, disc loss: 0.0016866774344930257, policy loss: 14.923890008972856
Experience 16, Iter 18, disc loss: 0.0018326066608802816, policy loss: 13.984485484905237
Experience 16, Iter 19, disc loss: 0.0012885595772110453, policy loss: 13.953380672595078
Experience 16, Iter 20, disc loss: 0.00246897794595225, policy loss: 13.669817739860804
Experience 16, Iter 21, disc loss: 0.001275343315527267, policy loss: 14.330413458975446
Experience 16, Iter 22, disc loss: 0.0012704985196897193, policy loss: 15.074548557862474
Experience 16, Iter 23, disc loss: 0.0013208232916481764, policy loss: 14.522970101869031
Experience 16, Iter 24, disc loss: 0.018941074153365885, policy loss: 13.777747941201323
Experience 16, Iter 25, disc loss: 0.0029151885099040417, policy loss: 13.800598056381078
Experience 16, Iter 26, disc loss: 0.0038550322860055166, policy loss: 14.543482699264583
Experience 16, Iter 27, disc loss: 0.0014177128911952526, policy loss: 14.245162847448011
Experience 16, Iter 28, disc loss: 0.0016640487233074064, policy loss: 14.316222488687842
Experience 16, Iter 29, disc loss: 0.0014262263993724965, policy loss: 14.943858464203812
Experience 16, Iter 30, disc loss: 0.001698442774862352, policy loss: 14.283808952231878
Experience 16, Iter 31, disc loss: 0.012656282805394572, policy loss: 14.610388396172352
Experience 16, Iter 32, disc loss: 0.001435742439371376, policy loss: 15.351985570239108
Experience 16, Iter 33, disc loss: 0.0014770239037685952, policy loss: 13.953006485416456
Experience 16, Iter 34, disc loss: 0.0056440587294974975, policy loss: 14.149557778290742
Experience 16, Iter 35, disc loss: 0.0015273453388026108, policy loss: 14.38381642134396
Experience 16, Iter 36, disc loss: 0.0015925772287142732, policy loss: 14.075316796956756
Experience 16, Iter 37, disc loss: 0.0015234335177226846, policy loss: 15.789767575139537
Experience 16, Iter 38, disc loss: 0.0023254765314099186, policy loss: 14.773244486166245
Experience 16, Iter 39, disc loss: 0.0019260121527295932, policy loss: 14.323766007032201
Experience 16, Iter 40, disc loss: 0.0014741737328837106, policy loss: 14.43928466785233
Experience 16, Iter 41, disc loss: 0.0016211008789660834, policy loss: 14.869391283737382
Experience 16, Iter 42, disc loss: 0.0016779324037427652, policy loss: 13.918103860078919
Experience 16, Iter 43, disc loss: 0.0014204383151360323, policy loss: 15.38012087488803
Experience 16, Iter 44, disc loss: 0.001455347446055385, policy loss: 14.770177679614708
Experience 16, Iter 45, disc loss: 0.0013993062546764547, policy loss: 15.088635851135791
Experience 16, Iter 46, disc loss: 0.0012675950839902649, policy loss: 15.000501736416268
Experience 16, Iter 47, disc loss: 0.0012575218149070881, policy loss: 13.939527524622676
Experience 16, Iter 48, disc loss: 0.0012731492393046702, policy loss: 14.54805308901496
Experience 16, Iter 49, disc loss: 0.0011272276243461388, policy loss: 14.856924554334284
Experience 16, Iter 50, disc loss: 0.0012063936162732144, policy loss: 14.020151348739862
Experience 16, Iter 51, disc loss: 0.0013263271846252539, policy loss: 14.323289739897142
Experience 16, Iter 52, disc loss: 0.0013305279917352648, policy loss: 14.540801739950014
Experience 16, Iter 53, disc loss: 0.0010862290438195853, policy loss: 14.473854843244123
Experience 16, Iter 54, disc loss: 0.003932897977984324, policy loss: 13.185315418598499
Experience 16, Iter 55, disc loss: 0.00382648364952415, policy loss: 13.044041544305058
Experience 16, Iter 56, disc loss: 0.001739345245207842, policy loss: 15.082365144949497
Experience 16, Iter 57, disc loss: 0.001540020905090673, policy loss: 13.604692234249544
Experience 16, Iter 58, disc loss: 0.001367492701519167, policy loss: 14.609115879577931
Experience 16, Iter 59, disc loss: 0.0008502613025963683, policy loss: 14.448257181307662
Experience 16, Iter 60, disc loss: 0.0008712952301932864, policy loss: 14.12907001202696
Experience 16, Iter 61, disc loss: 0.016195828981132157, policy loss: 14.868163228530149
Experience 16, Iter 62, disc loss: 0.00933267206010451, policy loss: 12.94885502531143
Experience 16, Iter 63, disc loss: 0.002292200037294548, policy loss: 14.57582692819932
Experience 16, Iter 64, disc loss: 0.0009379490860975085, policy loss: 14.92020226824575
Experience 16, Iter 65, disc loss: 0.0009077543878486087, policy loss: 15.393426659163806
Experience 16, Iter 66, disc loss: 0.005189853832748803, policy loss: 14.415778067479666
Experience 16, Iter 67, disc loss: 0.0010108034657806274, policy loss: 14.434881415457475
Experience 16, Iter 68, disc loss: 0.0010630227496879721, policy loss: 14.808315352344318
Experience 16, Iter 69, disc loss: 0.0019677953646111816, policy loss: 14.067761457724243
Experience 16, Iter 70, disc loss: 0.006197797709273992, policy loss: 13.345723576125618
Experience 16, Iter 71, disc loss: 0.001036134622605967, policy loss: 15.964300097574785
Experience 16, Iter 72, disc loss: 0.0010304334044253661, policy loss: 14.334183104748188
Experience 16, Iter 73, disc loss: 0.001197151459327659, policy loss: 14.288554799819
Experience 16, Iter 74, disc loss: 0.005369097296814135, policy loss: 14.651057139029676
Experience 16, Iter 75, disc loss: 0.0010732330357735708, policy loss: 15.571389613475658
Experience 16, Iter 76, disc loss: 0.0010869776648543634, policy loss: 14.689473397095245
Experience 16, Iter 77, disc loss: 0.0011971265081416583, policy loss: 14.20411342792458
Experience 16, Iter 78, disc loss: 0.0012891115389707776, policy loss: 14.806775424938412
Experience 16, Iter 79, disc loss: 0.0022597337947491674, policy loss: 13.835552568884111
Experience 16, Iter 80, disc loss: 0.001161459235292168, policy loss: 15.16629982363553
Experience 16, Iter 81, disc loss: 0.0010753406383437712, policy loss: 14.772661249012447
Experience 16, Iter 82, disc loss: 0.0010393248714301345, policy loss: 14.902305108876925
Experience 16, Iter 83, disc loss: 0.01274785833668812, policy loss: 13.433029616253354
Experience 16, Iter 84, disc loss: 0.001380314917783802, policy loss: 14.247814416343136
Experience 16, Iter 85, disc loss: 0.001549104587928003, policy loss: 13.507970151754549
Experience 16, Iter 86, disc loss: 0.00135094723769167, policy loss: 14.309652104874282
Experience 16, Iter 87, disc loss: 0.0033522840630929754, policy loss: 13.985833604237527
Experience 16, Iter 88, disc loss: 0.00125812362080406, policy loss: 14.11518367241996
Experience 16, Iter 89, disc loss: 0.001076386679023402, policy loss: 15.225378734727814
Experience 16, Iter 90, disc loss: 0.0010245922386466874, policy loss: 14.78403798514634
Experience 16, Iter 91, disc loss: 0.0009790326278012395, policy loss: 14.493267699341846
Experience 16, Iter 92, disc loss: 0.001191173857522263, policy loss: 14.716565377798453
Experience 16, Iter 93, disc loss: 0.04373109148915688, policy loss: 13.09907303236845
Experience 16, Iter 94, disc loss: 0.0010290863507201, policy loss: 15.185296231312886
Experience 16, Iter 95, disc loss: 0.0010192717291666485, policy loss: 15.000025420669118
Experience 16, Iter 96, disc loss: 0.002627224303485101, policy loss: 14.35860910572548
Experience 16, Iter 97, disc loss: 0.0011852695268668506, policy loss: 15.199467676772924
Experience 16, Iter 98, disc loss: 0.0011413986029697883, policy loss: 14.780092643766784
Experience 16, Iter 99, disc loss: 0.0013002500159564842, policy loss: 14.45229758739325
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0092],
        [0.2832],
        [2.8664],
        [0.0409]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0584, 0.3857, 1.9136, 0.0368, 0.0093, 6.9538]],

        [[0.0584, 0.3857, 1.9136, 0.0368, 0.0093, 6.9538]],

        [[0.0584, 0.3857, 1.9136, 0.0368, 0.0093, 6.9538]],

        [[0.0584, 0.3857, 1.9136, 0.0368, 0.0093, 6.9538]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0366,  1.1329, 11.4657,  0.1638], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0366,  1.1329, 11.4657,  0.1638])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.376
Iter 2/2000 - Loss: 4.391
Iter 3/2000 - Loss: 4.278
Iter 4/2000 - Loss: 4.242
Iter 5/2000 - Loss: 4.215
Iter 6/2000 - Loss: 4.140
Iter 7/2000 - Loss: 4.054
Iter 8/2000 - Loss: 3.982
Iter 9/2000 - Loss: 3.913
Iter 10/2000 - Loss: 3.826
Iter 11/2000 - Loss: 3.719
Iter 12/2000 - Loss: 3.603
Iter 13/2000 - Loss: 3.486
Iter 14/2000 - Loss: 3.364
Iter 15/2000 - Loss: 3.232
Iter 16/2000 - Loss: 3.085
Iter 17/2000 - Loss: 2.921
Iter 18/2000 - Loss: 2.744
Iter 19/2000 - Loss: 2.556
Iter 20/2000 - Loss: 2.358
Iter 1981/2000 - Loss: -6.457
Iter 1982/2000 - Loss: -6.457
Iter 1983/2000 - Loss: -6.457
Iter 1984/2000 - Loss: -6.457
Iter 1985/2000 - Loss: -6.457
Iter 1986/2000 - Loss: -6.457
Iter 1987/2000 - Loss: -6.457
Iter 1988/2000 - Loss: -6.457
Iter 1989/2000 - Loss: -6.457
Iter 1990/2000 - Loss: -6.457
Iter 1991/2000 - Loss: -6.457
Iter 1992/2000 - Loss: -6.458
Iter 1993/2000 - Loss: -6.458
Iter 1994/2000 - Loss: -6.458
Iter 1995/2000 - Loss: -6.458
Iter 1996/2000 - Loss: -6.458
Iter 1997/2000 - Loss: -6.458
Iter 1998/2000 - Loss: -6.458
Iter 1999/2000 - Loss: -6.458
Iter 2000/2000 - Loss: -6.458
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[20.2373, 16.0285, 33.9089, 16.9801,  5.5229, 68.0018]],

        [[27.8982, 42.4911,  8.7749,  1.2028,  1.2481, 21.5505]],

        [[32.1668, 46.7242,  8.9797,  0.9396,  1.0681, 22.0603]],

        [[24.8485, 42.2478, 15.5498,  1.4606,  1.6539, 51.6710]]])
Signal Variance: tensor([ 0.3023,  1.4376, 11.6652,  0.4749])
Estimated target variance: tensor([ 0.0366,  1.1329, 11.4657,  0.1638])
N: 170
Signal to noise ratio: tensor([29.7410, 55.1734, 70.3328, 37.1615])
Bound on condition number: tensor([150370.4085, 517497.8332, 840940.9883, 234767.7149])
Policy Optimizer learning rate:
0.09832836131379853
Experience 17, Iter 0, disc loss: 0.0013947953148732211, policy loss: 14.765700791467431
Experience 17, Iter 1, disc loss: 0.004213341678136638, policy loss: 14.211979637947929
Experience 17, Iter 2, disc loss: 0.001179031206066055, policy loss: 13.915689202472619
Experience 17, Iter 3, disc loss: 0.001866219744090515, policy loss: 13.923109459219628
Experience 17, Iter 4, disc loss: 0.001167383057171312, policy loss: 14.915677321676855
Experience 17, Iter 5, disc loss: 0.0016149393036248355, policy loss: 13.680144369081296
Experience 17, Iter 6, disc loss: 0.0014367589664039517, policy loss: 14.797072274648015
Experience 17, Iter 7, disc loss: 0.001533574643538635, policy loss: 15.291245715507792
Experience 17, Iter 8, disc loss: 0.0011797393919030289, policy loss: 14.481307666186702
Experience 17, Iter 9, disc loss: 0.0010123439927256863, policy loss: 16.212288646708224
Experience 17, Iter 10, disc loss: 0.0011342510424143667, policy loss: 14.339115702204364
Experience 17, Iter 11, disc loss: 0.0009698838343902183, policy loss: 15.063116062457874
Experience 17, Iter 12, disc loss: 0.001123852390406057, policy loss: 13.955275264491442
Experience 17, Iter 13, disc loss: 0.0014090998837549252, policy loss: 14.906814823738294
Experience 17, Iter 14, disc loss: 0.0008974376467352561, policy loss: 14.802401391008432
Experience 17, Iter 15, disc loss: 0.0008977815329041408, policy loss: 15.094784433928272
Experience 17, Iter 16, disc loss: 0.01564312620810091, policy loss: 13.718230483921673
Experience 17, Iter 17, disc loss: 0.0012742323820068248, policy loss: 14.4147480782769
Experience 17, Iter 18, disc loss: 0.0008803048111877468, policy loss: 15.373002681475674
Experience 17, Iter 19, disc loss: 0.0010976023507968803, policy loss: 14.660163579899791
Experience 17, Iter 20, disc loss: 0.0008848986381098969, policy loss: 16.532678883005044
Experience 17, Iter 21, disc loss: 0.0009518724880150653, policy loss: 14.87826729618734
Experience 17, Iter 22, disc loss: 0.0008782879342798495, policy loss: 15.195171815500881
Experience 17, Iter 23, disc loss: 0.001249351481778385, policy loss: 13.372489790621255
Experience 17, Iter 24, disc loss: 0.001272349100715961, policy loss: 14.96748094307499
Experience 17, Iter 25, disc loss: 0.0027754257664793923, policy loss: 14.282418373149815
Experience 17, Iter 26, disc loss: 0.0008544622931459628, policy loss: 15.261403596051682
Experience 17, Iter 27, disc loss: 0.0008813321969838648, policy loss: 14.65832407020606
Experience 17, Iter 28, disc loss: 0.000939586235830407, policy loss: 15.230289116722146
Experience 17, Iter 29, disc loss: 0.0008358043594261618, policy loss: 15.504054065267276
Experience 17, Iter 30, disc loss: 0.0008931777790400128, policy loss: 14.64485884433318
Experience 17, Iter 31, disc loss: 0.0009010591397021891, policy loss: 14.744350558554384
Experience 17, Iter 32, disc loss: 0.0025957363442287245, policy loss: 14.472076698635291
Experience 17, Iter 33, disc loss: 0.0008806894403835994, policy loss: 15.01922444380819
Experience 17, Iter 34, disc loss: 0.0032327790380542194, policy loss: 14.147267240966876
Experience 17, Iter 35, disc loss: 0.0008732484010912563, policy loss: 15.893712735625199
Experience 17, Iter 36, disc loss: 0.0007780516917319508, policy loss: 15.067008606136069
Experience 17, Iter 37, disc loss: 0.0009017678134897166, policy loss: 13.74865464827127
Experience 17, Iter 38, disc loss: 0.000922199193996063, policy loss: 15.871598837900756
Experience 17, Iter 39, disc loss: 0.0038496069744682103, policy loss: 15.088977736828502
Experience 17, Iter 40, disc loss: 0.00428528686115445, policy loss: 15.018229503234307
Experience 17, Iter 41, disc loss: 0.09272738743692326, policy loss: 14.358930284489764
Experience 17, Iter 42, disc loss: 0.004431722677232896, policy loss: 13.783664599749638
Experience 17, Iter 43, disc loss: 0.0011698637228597392, policy loss: 15.02518662074489
Experience 17, Iter 44, disc loss: 0.000940469991581638, policy loss: 14.749564261816301
Experience 17, Iter 45, disc loss: 0.0012407194807657663, policy loss: 14.734793588015723
Experience 17, Iter 46, disc loss: 0.0025433626229691628, policy loss: 15.04680066573203
Experience 17, Iter 47, disc loss: 0.001140280352376718, policy loss: 14.591432154043268
Experience 17, Iter 48, disc loss: 0.0012882860128723258, policy loss: 15.396727380100707
Experience 17, Iter 49, disc loss: 0.001229863324158241, policy loss: 14.297765820945273
Experience 17, Iter 50, disc loss: 0.0017644684561494788, policy loss: 15.32034343118558
Experience 17, Iter 51, disc loss: 0.0012268003633835794, policy loss: 13.837756509776927
Experience 17, Iter 52, disc loss: 0.0024212421205963824, policy loss: 16.39491988223476
Experience 17, Iter 53, disc loss: 0.03856615307028314, policy loss: 15.246280327749147
Experience 17, Iter 54, disc loss: 0.0013728192064641222, policy loss: 15.694267016283833
Experience 17, Iter 55, disc loss: 0.0016555107769479778, policy loss: 14.969354568352173
Experience 17, Iter 56, disc loss: 0.0016931348145576167, policy loss: 15.631490541586139
Experience 17, Iter 57, disc loss: 0.0016631995010505877, policy loss: 14.977917500541869
Experience 17, Iter 58, disc loss: 0.0017317608353416343, policy loss: 15.246484217923916
Experience 17, Iter 59, disc loss: 0.0017869434745107873, policy loss: 15.077272864283112
Experience 17, Iter 60, disc loss: 0.0018411230156251124, policy loss: 15.741809282504851
Experience 17, Iter 61, disc loss: 0.002341898922339135, policy loss: 15.045570534741314
Experience 17, Iter 62, disc loss: 0.001856564774697422, policy loss: 15.681302606379786
Experience 17, Iter 63, disc loss: 0.00209969166124395, policy loss: 14.525219381720753
Experience 17, Iter 64, disc loss: 0.0018750449960453805, policy loss: 15.524285738373106
Experience 17, Iter 65, disc loss: 0.0017518640276364395, policy loss: 14.943360892133025
Experience 17, Iter 66, disc loss: 0.002525596077460485, policy loss: 15.64722767556976
Experience 17, Iter 67, disc loss: 0.0016443731493810214, policy loss: 14.950226243386108
Experience 17, Iter 68, disc loss: 0.0015887948099609817, policy loss: 14.709568979640341
Experience 17, Iter 69, disc loss: 0.004024043695510177, policy loss: 15.496357012608192
Experience 17, Iter 70, disc loss: 0.001603371861645633, policy loss: 15.025528377141695
Experience 17, Iter 71, disc loss: 0.0014555189944452263, policy loss: 16.040559072677127
Experience 17, Iter 72, disc loss: 0.001456587338425095, policy loss: 15.290157997573473
Experience 17, Iter 73, disc loss: 0.0013395657587736353, policy loss: 16.490502501627375
Experience 17, Iter 74, disc loss: 0.0031798100046195625, policy loss: 14.947973275230503
Experience 17, Iter 75, disc loss: 0.0013987045123538253, policy loss: 14.456728237706509
Experience 17, Iter 76, disc loss: 0.001144629114441695, policy loss: 15.40539296998464
Experience 17, Iter 77, disc loss: 0.0014548585505743423, policy loss: 14.676320475528449
Experience 17, Iter 78, disc loss: 0.001106702457599805, policy loss: 15.125988441550707
Experience 17, Iter 79, disc loss: 0.0016519870674247066, policy loss: 15.353581317963156
Experience 17, Iter 80, disc loss: 0.0011322501200351645, policy loss: 16.051937064353105
Experience 17, Iter 81, disc loss: 0.030400619225321404, policy loss: 14.235554916273578
Experience 17, Iter 82, disc loss: 0.0012281656328503956, policy loss: 14.887123501337967
Experience 17, Iter 83, disc loss: 0.0011225344517915583, policy loss: 16.097296929950726
Experience 17, Iter 84, disc loss: 0.000973450627699388, policy loss: 16.5701770262863
Experience 17, Iter 85, disc loss: 0.0011701034159407302, policy loss: 15.416793164167583
Experience 17, Iter 86, disc loss: 0.0010285284989751684, policy loss: 15.356287779926468
Experience 17, Iter 87, disc loss: 0.004417570921933208, policy loss: 15.694045249160643
Experience 17, Iter 88, disc loss: 0.0009868376273496004, policy loss: 15.62404176171302
Experience 17, Iter 89, disc loss: 0.0010491954685367024, policy loss: 15.435876120991715
Experience 17, Iter 90, disc loss: 0.06629221644737085, policy loss: 14.630635487300585
Experience 17, Iter 91, disc loss: 0.001998398730443417, policy loss: 14.752222272717443
Experience 17, Iter 92, disc loss: 0.0011210274898828254, policy loss: 14.900577197605703
Experience 17, Iter 93, disc loss: 0.0011569256709707003, policy loss: 15.83369284216087
Experience 17, Iter 94, disc loss: 0.0015089708265977665, policy loss: 16.004850683450968
Experience 17, Iter 95, disc loss: 0.001225854410366835, policy loss: 15.62891146290646
Experience 17, Iter 96, disc loss: 0.0015736344843651207, policy loss: 15.08147648755637
Experience 17, Iter 97, disc loss: 0.0013394535984996873, policy loss: 15.470248977846719
Experience 17, Iter 98, disc loss: 0.0012362654822604042, policy loss: 16.200917037940208
Experience 17, Iter 99, disc loss: 0.0012198801563864934, policy loss: 15.883897141961675
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0088],
        [0.2921],
        [2.9510],
        [0.0404]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0569, 0.3726, 1.9053, 0.0363, 0.0089, 7.0437]],

        [[0.0569, 0.3726, 1.9053, 0.0363, 0.0089, 7.0437]],

        [[0.0569, 0.3726, 1.9053, 0.0363, 0.0089, 7.0437]],

        [[0.0569, 0.3726, 1.9053, 0.0363, 0.0089, 7.0437]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0350,  1.1683, 11.8039,  0.1615], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0350,  1.1683, 11.8039,  0.1615])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.368
Iter 2/2000 - Loss: 4.387
Iter 3/2000 - Loss: 4.274
Iter 4/2000 - Loss: 4.240
Iter 5/2000 - Loss: 4.218
Iter 6/2000 - Loss: 4.145
Iter 7/2000 - Loss: 4.061
Iter 8/2000 - Loss: 3.995
Iter 9/2000 - Loss: 3.932
Iter 10/2000 - Loss: 3.846
Iter 11/2000 - Loss: 3.740
Iter 12/2000 - Loss: 3.627
Iter 13/2000 - Loss: 3.514
Iter 14/2000 - Loss: 3.397
Iter 15/2000 - Loss: 3.267
Iter 16/2000 - Loss: 3.119
Iter 17/2000 - Loss: 2.955
Iter 18/2000 - Loss: 2.779
Iter 19/2000 - Loss: 2.593
Iter 20/2000 - Loss: 2.397
Iter 1981/2000 - Loss: -6.560
Iter 1982/2000 - Loss: -6.560
Iter 1983/2000 - Loss: -6.560
Iter 1984/2000 - Loss: -6.560
Iter 1985/2000 - Loss: -6.560
Iter 1986/2000 - Loss: -6.560
Iter 1987/2000 - Loss: -6.561
Iter 1988/2000 - Loss: -6.561
Iter 1989/2000 - Loss: -6.561
Iter 1990/2000 - Loss: -6.561
Iter 1991/2000 - Loss: -6.561
Iter 1992/2000 - Loss: -6.561
Iter 1993/2000 - Loss: -6.561
Iter 1994/2000 - Loss: -6.561
Iter 1995/2000 - Loss: -6.561
Iter 1996/2000 - Loss: -6.561
Iter 1997/2000 - Loss: -6.561
Iter 1998/2000 - Loss: -6.561
Iter 1999/2000 - Loss: -6.561
Iter 2000/2000 - Loss: -6.561
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[19.4953, 16.2466, 37.8047, 16.8938,  5.7460, 67.7116]],

        [[28.2079, 41.8523,  8.4173,  1.3197,  1.0959, 20.4890]],

        [[32.6290, 46.1703,  8.7586,  0.9655,  1.0038, 22.4627]],

        [[23.1526, 41.5776, 15.5139,  1.4602,  1.6199, 50.6082]]])
Signal Variance: tensor([ 0.3070,  1.3721, 11.8826,  0.4689])
Estimated target variance: tensor([ 0.0350,  1.1683, 11.8039,  0.1615])
N: 180
Signal to noise ratio: tensor([29.5739, 56.0159, 72.4572, 37.6441])
Bound on condition number: tensor([157431.6405, 564802.1712, 945008.2438, 255075.5249])
Policy Optimizer learning rate:
0.09822481660247992
Experience 18, Iter 0, disc loss: 0.0013594293925985688, policy loss: 15.186449151802206
Experience 18, Iter 1, disc loss: 0.0012767702996757525, policy loss: 15.52769680753568
Experience 18, Iter 2, disc loss: 0.0012272970571437156, policy loss: 16.08432749538005
Experience 18, Iter 3, disc loss: 0.001330169875626376, policy loss: 15.185896308771934
Experience 18, Iter 4, disc loss: 0.0011349258129771174, policy loss: 15.598605462824207
Experience 18, Iter 5, disc loss: 0.004091757949294278, policy loss: 14.357243279526628
Experience 18, Iter 6, disc loss: 0.04469838602160818, policy loss: 15.117492371464074
Experience 18, Iter 7, disc loss: 0.002969542275366803, policy loss: 14.171442405865088
Experience 18, Iter 8, disc loss: 0.001293260405988825, policy loss: 14.924015539924977
Experience 18, Iter 9, disc loss: 0.002567092632695076, policy loss: 14.818336739333935
Experience 18, Iter 10, disc loss: 0.0013117384967397405, policy loss: 15.743730956651255
Experience 18, Iter 11, disc loss: 0.002271860911992856, policy loss: 15.905578294563474
Experience 18, Iter 12, disc loss: 0.0021993086368045324, policy loss: 14.575875263912431
Experience 18, Iter 13, disc loss: 0.0015937363561168078, policy loss: 15.553843197949067
Experience 18, Iter 14, disc loss: 0.0016724059881539305, policy loss: 15.049460571896184
Experience 18, Iter 15, disc loss: 0.001497084638007331, policy loss: 16.330378676553472
Experience 18, Iter 16, disc loss: 0.0015268859541794483, policy loss: 15.934869239293082
Experience 18, Iter 17, disc loss: 0.001499862300017499, policy loss: 14.98941162747142
Experience 18, Iter 18, disc loss: 0.0014899132674766678, policy loss: 14.501757063102445
Experience 18, Iter 19, disc loss: 0.0014727870534492085, policy loss: 16.0946092916721
Experience 18, Iter 20, disc loss: 0.0014800475846196094, policy loss: 15.437069641477274
Experience 18, Iter 21, disc loss: 0.0012682574123679224, policy loss: 15.376148447186953
Experience 18, Iter 22, disc loss: 0.02385782161171289, policy loss: 14.046426440681138
Experience 18, Iter 23, disc loss: 0.001282268178707863, policy loss: 15.441887836962426
Experience 18, Iter 24, disc loss: 0.0013259624351173876, policy loss: 16.046784591214852
Experience 18, Iter 25, disc loss: 0.001431381880301362, policy loss: 16.05514785756593
Experience 18, Iter 26, disc loss: 0.001428604771994781, policy loss: 15.642060537899305
Experience 18, Iter 27, disc loss: 0.0024241692336760042, policy loss: 16.249589034397957
Experience 18, Iter 28, disc loss: 0.0015759863674330269, policy loss: 14.885983982189765
Experience 18, Iter 29, disc loss: 0.0026600747331563092, policy loss: 14.167700028502288
Experience 18, Iter 30, disc loss: 0.010060178751786227, policy loss: 15.162524096657435
Experience 18, Iter 31, disc loss: 0.0014785776818361063, policy loss: 15.090864056034636
Experience 18, Iter 32, disc loss: 0.0014047272488543518, policy loss: 15.064051163454442
Experience 18, Iter 33, disc loss: 0.0014624261977190885, policy loss: 15.356607109106832
Experience 18, Iter 34, disc loss: 0.0014440273364023102, policy loss: 14.586817255236632
Experience 18, Iter 35, disc loss: 0.001421517152359487, policy loss: 15.369597853984994
Experience 18, Iter 36, disc loss: 0.001414454326978687, policy loss: 14.812937309848529
Experience 18, Iter 37, disc loss: 0.011526231253972792, policy loss: 15.581385556589543
Experience 18, Iter 38, disc loss: 0.0015103468669773583, policy loss: 14.965580783278567
Experience 18, Iter 39, disc loss: 0.0013299228946059246, policy loss: 15.555573652476468
Experience 18, Iter 40, disc loss: 0.0013840876005567124, policy loss: 16.74919085071612
Experience 18, Iter 41, disc loss: 0.0013928927644277765, policy loss: 15.372558325270864
Experience 18, Iter 42, disc loss: 0.0012850160559920406, policy loss: 16.313548449601285
Experience 18, Iter 43, disc loss: 0.0012784887726204413, policy loss: 14.172775103456908
Experience 18, Iter 44, disc loss: 0.0015530752217547835, policy loss: 15.91480244849199
Experience 18, Iter 45, disc loss: 0.0012055938448825132, policy loss: 16.159985727678787
Experience 18, Iter 46, disc loss: 0.00117557818187943, policy loss: 16.48581054832793
Experience 18, Iter 47, disc loss: 0.001244394952961745, policy loss: 14.477226084801494
Experience 18, Iter 48, disc loss: 0.0012166555214093992, policy loss: 15.284229043532003
Experience 18, Iter 49, disc loss: 0.0010507772629844423, policy loss: 15.062334431891456
Experience 18, Iter 50, disc loss: 0.004339108256421118, policy loss: 16.277064881499612
Experience 18, Iter 51, disc loss: 0.0009750145954476582, policy loss: 16.21345343726044
Experience 18, Iter 52, disc loss: 0.0011120213715989406, policy loss: 16.465274662741244
Experience 18, Iter 53, disc loss: 0.004416654224720759, policy loss: 15.560159882950355
Experience 18, Iter 54, disc loss: 0.017777396255260165, policy loss: 15.107303143930068
Experience 18, Iter 55, disc loss: 0.0018841105220304662, policy loss: 15.804677712930832
Experience 18, Iter 56, disc loss: 0.0010067012863773106, policy loss: 15.191174323030173
Experience 18, Iter 57, disc loss: 0.0014596220559472033, policy loss: 15.479534081268465
Experience 18, Iter 58, disc loss: 0.0015875046021522252, policy loss: 14.135774854150126
Experience 18, Iter 59, disc loss: 0.005839503001050935, policy loss: 15.871917626816199
Experience 18, Iter 60, disc loss: 0.0016743860603311556, policy loss: 15.611411937506597
Experience 18, Iter 61, disc loss: 0.0009686418332247367, policy loss: 16.726109012785503
Experience 18, Iter 62, disc loss: 0.0010032734781110014, policy loss: 16.484952955329582
Experience 18, Iter 63, disc loss: 0.0010126328803528762, policy loss: 15.299525126254037
Experience 18, Iter 64, disc loss: 0.0012897562431548268, policy loss: 14.343480964971974
Experience 18, Iter 65, disc loss: 0.0009934906189333465, policy loss: 16.394642223855424
Experience 18, Iter 66, disc loss: 0.0010053516254157051, policy loss: 15.943209598208693
Experience 18, Iter 67, disc loss: 0.0014287072949188025, policy loss: 15.615492963445607
Experience 18, Iter 68, disc loss: 0.0009358438792732594, policy loss: 14.894744964961337
Experience 18, Iter 69, disc loss: 0.0008891790552615821, policy loss: 16.279588301773487
Experience 18, Iter 70, disc loss: 0.0008595605108103742, policy loss: 15.916129803155687
Experience 18, Iter 71, disc loss: 0.001203343416443387, policy loss: 15.536097889628648
Experience 18, Iter 72, disc loss: 0.0022759537785304283, policy loss: 14.86037858716493
Experience 18, Iter 73, disc loss: 0.0014448080494886897, policy loss: 15.145693541185846
Experience 18, Iter 74, disc loss: 0.0008220850311114726, policy loss: 15.240494760053023
Experience 18, Iter 75, disc loss: 0.000786867558202316, policy loss: 15.689313864649966
Experience 18, Iter 76, disc loss: 0.0009707712025055381, policy loss: 16.03175853541923
Experience 18, Iter 77, disc loss: 0.001239401548102544, policy loss: 14.060353219120493
Experience 18, Iter 78, disc loss: 0.0007116211983795654, policy loss: 15.70625290883175
Experience 18, Iter 79, disc loss: 0.0009618650939911232, policy loss: 15.002208268596968
Experience 18, Iter 80, disc loss: 0.0007191524092093579, policy loss: 16.042157191986803
Experience 18, Iter 81, disc loss: 0.0010168226135798914, policy loss: 14.558192629292444
Experience 18, Iter 82, disc loss: 0.0006886453922041975, policy loss: 15.097411893465754
Experience 18, Iter 83, disc loss: 0.001885992955150946, policy loss: 14.450049978995436
Experience 18, Iter 84, disc loss: 0.019722319846952146, policy loss: 14.759871112514517
Experience 18, Iter 85, disc loss: 0.0013435516858273232, policy loss: 15.49806293476433
Experience 18, Iter 86, disc loss: 0.0010814352026360292, policy loss: 14.912468304802216
Experience 18, Iter 87, disc loss: 0.0006782462879796829, policy loss: 15.22682994263561
Experience 18, Iter 88, disc loss: 0.0036851539876407166, policy loss: 14.397444288487986
Experience 18, Iter 89, disc loss: 0.0007229366808443013, policy loss: 15.399537874273783
Experience 18, Iter 90, disc loss: 0.000697064208334635, policy loss: 15.602054297420377
Experience 18, Iter 91, disc loss: 0.0009093945960357768, policy loss: 14.860355141185494
Experience 18, Iter 92, disc loss: 0.0007609517565776759, policy loss: 14.990776443116077
Experience 18, Iter 93, disc loss: 0.0010385747987717731, policy loss: 14.723461656024583
Experience 18, Iter 94, disc loss: 0.0010804765754925407, policy loss: 15.46591203482426
Experience 18, Iter 95, disc loss: 0.0008577285987220019, policy loss: 15.61749503184393
Experience 18, Iter 96, disc loss: 0.0006812455522166324, policy loss: 15.731867705967732
Experience 18, Iter 97, disc loss: 0.0006866118144573078, policy loss: 15.712579543458482
Experience 18, Iter 98, disc loss: 0.00899192886843429, policy loss: 14.534585423670617
Experience 18, Iter 99, disc loss: 0.0022135800509419865, policy loss: 14.143476031079452
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0090],
        [0.2863],
        [2.9063],
        [0.0416]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0599, 0.3821, 1.9523, 0.0377, 0.0091, 7.0272]],

        [[0.0599, 0.3821, 1.9523, 0.0377, 0.0091, 7.0272]],

        [[0.0599, 0.3821, 1.9523, 0.0377, 0.0091, 7.0272]],

        [[0.0599, 0.3821, 1.9523, 0.0377, 0.0091, 7.0272]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0358,  1.1453, 11.6254,  0.1665], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0358,  1.1453, 11.6254,  0.1665])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.348
Iter 2/2000 - Loss: 4.376
Iter 3/2000 - Loss: 4.246
Iter 4/2000 - Loss: 4.209
Iter 5/2000 - Loss: 4.185
Iter 6/2000 - Loss: 4.109
Iter 7/2000 - Loss: 4.016
Iter 8/2000 - Loss: 3.937
Iter 9/2000 - Loss: 3.869
Iter 10/2000 - Loss: 3.789
Iter 11/2000 - Loss: 3.685
Iter 12/2000 - Loss: 3.565
Iter 13/2000 - Loss: 3.441
Iter 14/2000 - Loss: 3.316
Iter 15/2000 - Loss: 3.186
Iter 16/2000 - Loss: 3.041
Iter 17/2000 - Loss: 2.878
Iter 18/2000 - Loss: 2.698
Iter 19/2000 - Loss: 2.503
Iter 20/2000 - Loss: 2.297
Iter 1981/2000 - Loss: -6.622
Iter 1982/2000 - Loss: -6.622
Iter 1983/2000 - Loss: -6.622
Iter 1984/2000 - Loss: -6.622
Iter 1985/2000 - Loss: -6.622
Iter 1986/2000 - Loss: -6.622
Iter 1987/2000 - Loss: -6.622
Iter 1988/2000 - Loss: -6.622
Iter 1989/2000 - Loss: -6.622
Iter 1990/2000 - Loss: -6.622
Iter 1991/2000 - Loss: -6.622
Iter 1992/2000 - Loss: -6.622
Iter 1993/2000 - Loss: -6.622
Iter 1994/2000 - Loss: -6.622
Iter 1995/2000 - Loss: -6.623
Iter 1996/2000 - Loss: -6.623
Iter 1997/2000 - Loss: -6.623
Iter 1998/2000 - Loss: -6.623
Iter 1999/2000 - Loss: -6.623
Iter 2000/2000 - Loss: -6.623
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[19.6771, 15.6922, 37.9428, 12.0545,  4.3507, 66.2077]],

        [[27.9949, 41.6811,  8.5497,  1.3383,  1.0054, 21.0303]],

        [[31.3973, 43.3590,  8.9359,  0.9364,  1.0026, 22.0521]],

        [[22.9617, 41.1589, 15.4093,  1.4298,  1.6312, 50.2321]]])
Signal Variance: tensor([ 0.2732,  1.3724, 11.2951,  0.4539])
Estimated target variance: tensor([ 0.0358,  1.1453, 11.6254,  0.1665])
N: 190
Signal to noise ratio: tensor([28.0556, 55.7902, 67.7711, 37.9868])
Bound on condition number: tensor([149552.7272, 591384.5667, 872655.0867, 274171.1231])
Policy Optimizer learning rate:
0.09812138092895162
Experience 19, Iter 0, disc loss: 0.000720603735185901, policy loss: 15.229126301816759
Experience 19, Iter 1, disc loss: 0.000868564713090086, policy loss: 15.351223946599072
Experience 19, Iter 2, disc loss: 0.0009997964143367525, policy loss: 15.305127662430998
Experience 19, Iter 3, disc loss: 0.0009680959665056609, policy loss: 16.287512686058463
Experience 19, Iter 4, disc loss: 0.0007498911478034682, policy loss: 14.546713758512146
Experience 19, Iter 5, disc loss: 0.0008807959918729291, policy loss: 15.090549073696106
Experience 19, Iter 6, disc loss: 0.0007238175999440094, policy loss: 15.375779726120136
Experience 19, Iter 7, disc loss: 0.025211916149743736, policy loss: 16.6097739211744
Experience 19, Iter 8, disc loss: 0.0009613125448239871, policy loss: 15.785971061185359
Experience 19, Iter 9, disc loss: 0.0008500170242407729, policy loss: 14.424931548826475
Experience 19, Iter 10, disc loss: 0.0008050832516172903, policy loss: 16.24053912298389
Experience 19, Iter 11, disc loss: 0.001352186038333362, policy loss: 15.376116691797545
Experience 19, Iter 12, disc loss: 0.0008418298290440919, policy loss: 15.146084260507335
Experience 19, Iter 13, disc loss: 0.0008952030290343431, policy loss: 15.832147269151577
Experience 19, Iter 14, disc loss: 0.0018723057194827471, policy loss: 16.919892049899204
Experience 19, Iter 15, disc loss: 0.001357859468648872, policy loss: 16.359952718710932
Experience 19, Iter 16, disc loss: 0.0009133731612166421, policy loss: 15.278150756099967
Experience 19, Iter 17, disc loss: 0.000892459278949888, policy loss: 14.446799187494577
Experience 19, Iter 18, disc loss: 0.0009091405993131039, policy loss: 17.258165881229612
Experience 19, Iter 19, disc loss: 0.0020220086159088175, policy loss: 15.943291780507447
Experience 19, Iter 20, disc loss: 0.0012289599766860304, policy loss: 15.26625715742097
Experience 19, Iter 21, disc loss: 0.0009318281532686894, policy loss: 16.395796540995068
Experience 19, Iter 22, disc loss: 0.0014329350242399215, policy loss: 15.823499023994719
Experience 19, Iter 23, disc loss: 0.0009028607466717463, policy loss: 15.499516454618936
Experience 19, Iter 24, disc loss: 0.023645069125919974, policy loss: 14.972000668821266
Experience 19, Iter 25, disc loss: 0.0013169881904877088, policy loss: 15.50742412927593
Experience 19, Iter 26, disc loss: 0.0018596327243220287, policy loss: 15.402779329116298
Experience 19, Iter 27, disc loss: 0.001678613146761044, policy loss: 14.196342131927771
Experience 19, Iter 28, disc loss: 0.0011266799802938164, policy loss: 15.983574045129407
Experience 19, Iter 29, disc loss: 0.0011330410868321081, policy loss: 16.097365301883244
Experience 19, Iter 30, disc loss: 0.0011228281492044832, policy loss: 16.709828977358555
Experience 19, Iter 31, disc loss: 0.0012792633141626062, policy loss: 15.728918729626091
Experience 19, Iter 32, disc loss: 0.0010219666585773246, policy loss: 16.762203171710176
Experience 19, Iter 33, disc loss: 0.0017805233415938897, policy loss: 14.827859474161183
Experience 19, Iter 34, disc loss: 0.0014415943904318366, policy loss: 14.679455991452704
Experience 19, Iter 35, disc loss: 0.001032171801851024, policy loss: 16.468409988866824
Experience 19, Iter 36, disc loss: 0.0016774743031352031, policy loss: 15.781198557220572
Experience 19, Iter 37, disc loss: 0.0014048643459315167, policy loss: 16.477094680913805
Experience 19, Iter 38, disc loss: 0.0012214887375306847, policy loss: 16.82870214587463
Experience 19, Iter 39, disc loss: 0.0013464363949122534, policy loss: 15.764463691880682
Experience 19, Iter 40, disc loss: 0.002164617619053547, policy loss: 16.32511418921903
Experience 19, Iter 41, disc loss: 0.0010187218506866437, policy loss: 16.731138918210764
Experience 19, Iter 42, disc loss: 0.0009060238383446094, policy loss: 15.742605370391594
Experience 19, Iter 43, disc loss: 0.0009379017891795881, policy loss: 16.185377383868754
Experience 19, Iter 44, disc loss: 0.0011574563600275584, policy loss: 15.995223533047525
Experience 19, Iter 45, disc loss: 0.0010310377433455135, policy loss: 15.808401843815748
Experience 19, Iter 46, disc loss: 0.0010963397303853325, policy loss: 15.383308390314966
Experience 19, Iter 47, disc loss: 0.0008192689099158668, policy loss: 14.907168898315668
Experience 19, Iter 48, disc loss: 0.00735397781082529, policy loss: 16.25344447826535
Experience 19, Iter 49, disc loss: 0.0016346762477194188, policy loss: 14.116694341354492
Experience 19, Iter 50, disc loss: 0.0007785546812322752, policy loss: 15.984324993739563
Experience 19, Iter 51, disc loss: 0.0011425012822798143, policy loss: 16.59292606712715
Experience 19, Iter 52, disc loss: 0.0007161280877896391, policy loss: 16.035651953614533
Experience 19, Iter 53, disc loss: 0.0011599229574125462, policy loss: 14.381766791494545
Experience 19, Iter 54, disc loss: 0.001276694030424958, policy loss: 16.3416749936445
Experience 19, Iter 55, disc loss: 0.0006971810147233532, policy loss: 16.490863647530546
Experience 19, Iter 56, disc loss: 0.0007845772123884773, policy loss: 15.768188982006507
Experience 19, Iter 57, disc loss: 0.0014631531446366753, policy loss: 15.257229292276586
Experience 19, Iter 58, disc loss: 0.0018534590125285908, policy loss: 16.60377487911849
Experience 19, Iter 59, disc loss: 0.000817034729375893, policy loss: 16.47677045046621
Experience 19, Iter 60, disc loss: 0.0006319619261179334, policy loss: 16.149757229477622
Experience 19, Iter 61, disc loss: 0.0006303988416343045, policy loss: 15.937976217839822
Experience 19, Iter 62, disc loss: 0.0006678972239728223, policy loss: 16.671295986725553
Experience 19, Iter 63, disc loss: 0.0005970375843730661, policy loss: 16.321937534181473
Experience 19, Iter 64, disc loss: 0.001046116664487192, policy loss: 15.678527408884724
Experience 19, Iter 65, disc loss: 0.0007402497264247894, policy loss: 16.048449940265527
Experience 19, Iter 66, disc loss: 0.0006008217251585214, policy loss: 16.373074861443406
Experience 19, Iter 67, disc loss: 0.0006236979800267322, policy loss: 16.342928561686506
Experience 19, Iter 68, disc loss: 0.000702706770992677, policy loss: 15.639013688305143
Experience 19, Iter 69, disc loss: 0.0005603341257390057, policy loss: 15.866179809446873
Experience 19, Iter 70, disc loss: 0.0007524037723658742, policy loss: 14.936184203356778
Experience 19, Iter 71, disc loss: 0.0005232726457733931, policy loss: 16.085891764778964
Experience 19, Iter 72, disc loss: 0.0016862946097917911, policy loss: 15.675103814177033
Experience 19, Iter 73, disc loss: 0.0006633907515570721, policy loss: 15.791634733099611
Experience 19, Iter 74, disc loss: 0.000515763765610408, policy loss: 17.318984008111194
Experience 19, Iter 75, disc loss: 0.0005417906312880211, policy loss: 15.994557450430438
Experience 19, Iter 76, disc loss: 0.000877802212469202, policy loss: 15.10388702186259
Experience 19, Iter 77, disc loss: 0.003192089996369495, policy loss: 15.02561510841885
Experience 19, Iter 78, disc loss: 0.00048724715020961006, policy loss: 15.442136634397327
Experience 19, Iter 79, disc loss: 0.0004928337877137341, policy loss: 15.95873847424542
Experience 19, Iter 80, disc loss: 0.0008840933078869077, policy loss: 14.129484452241218
Experience 19, Iter 81, disc loss: 0.0014747418346047411, policy loss: 15.526298521075635
Experience 19, Iter 82, disc loss: 0.0005289324403475288, policy loss: 14.919324676598341
Experience 19, Iter 83, disc loss: 0.002052638736287205, policy loss: 15.876565679627316
Experience 19, Iter 84, disc loss: 0.0004633025152589012, policy loss: 15.792412489551364
Experience 19, Iter 85, disc loss: 0.0026897004749847873, policy loss: 15.701960366936003
Experience 19, Iter 86, disc loss: 0.002195346997448543, policy loss: 17.47381579294501
Experience 19, Iter 87, disc loss: 0.0007341375645782415, policy loss: 16.076812261370414
Experience 19, Iter 88, disc loss: 0.0042098515453114155, policy loss: 16.115533802188743
Experience 19, Iter 89, disc loss: 0.0004707214392635226, policy loss: 15.27457868645134
Experience 19, Iter 90, disc loss: 0.0004862599538753477, policy loss: 17.374393865188217
Experience 19, Iter 91, disc loss: 0.0008186108587928101, policy loss: 14.68321903665657
Experience 19, Iter 92, disc loss: 0.0013208803866947547, policy loss: 15.51553542603925
Experience 19, Iter 93, disc loss: 0.0005439497486124669, policy loss: 15.208316034012146
Experience 19, Iter 94, disc loss: 0.0004500700221914125, policy loss: 15.751399059937967
Experience 19, Iter 95, disc loss: 0.002757089238465166, policy loss: 16.187464852808866
Experience 19, Iter 96, disc loss: 0.035266853936899385, policy loss: 15.667802252302355
Experience 19, Iter 97, disc loss: 0.03183362183859523, policy loss: 15.337991898055032
Experience 19, Iter 98, disc loss: 0.12507177951064863, policy loss: 15.366899353295347
Experience 19, Iter 99, disc loss: 0.0008077239021923252, policy loss: 16.42549547469934
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0086],
        [0.2847],
        [2.9205],
        [0.0422]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0571, 0.3690, 1.9826, 0.0373, 0.0088, 6.9177]],

        [[0.0571, 0.3690, 1.9826, 0.0373, 0.0088, 6.9177]],

        [[0.0571, 0.3690, 1.9826, 0.0373, 0.0088, 6.9177]],

        [[0.0571, 0.3690, 1.9826, 0.0373, 0.0088, 6.9177]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0346,  1.1387, 11.6822,  0.1688], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0346,  1.1387, 11.6822,  0.1688])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.343
Iter 2/2000 - Loss: 4.387
Iter 3/2000 - Loss: 4.246
Iter 4/2000 - Loss: 4.212
Iter 5/2000 - Loss: 4.197
Iter 6/2000 - Loss: 4.121
Iter 7/2000 - Loss: 4.022
Iter 8/2000 - Loss: 3.940
Iter 9/2000 - Loss: 3.875
Iter 10/2000 - Loss: 3.798
Iter 11/2000 - Loss: 3.693
Iter 12/2000 - Loss: 3.570
Iter 13/2000 - Loss: 3.444
Iter 14/2000 - Loss: 3.317
Iter 15/2000 - Loss: 3.186
Iter 16/2000 - Loss: 3.040
Iter 17/2000 - Loss: 2.875
Iter 18/2000 - Loss: 2.692
Iter 19/2000 - Loss: 2.495
Iter 20/2000 - Loss: 2.286
Iter 1981/2000 - Loss: -6.709
Iter 1982/2000 - Loss: -6.709
Iter 1983/2000 - Loss: -6.709
Iter 1984/2000 - Loss: -6.709
Iter 1985/2000 - Loss: -6.709
Iter 1986/2000 - Loss: -6.710
Iter 1987/2000 - Loss: -6.710
Iter 1988/2000 - Loss: -6.710
Iter 1989/2000 - Loss: -6.710
Iter 1990/2000 - Loss: -6.710
Iter 1991/2000 - Loss: -6.710
Iter 1992/2000 - Loss: -6.710
Iter 1993/2000 - Loss: -6.710
Iter 1994/2000 - Loss: -6.710
Iter 1995/2000 - Loss: -6.710
Iter 1996/2000 - Loss: -6.710
Iter 1997/2000 - Loss: -6.710
Iter 1998/2000 - Loss: -6.710
Iter 1999/2000 - Loss: -6.710
Iter 2000/2000 - Loss: -6.710
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[18.8399, 15.4310, 34.8182, 12.9762,  4.6868, 66.6989]],

        [[27.6412, 41.1443,  8.6565,  1.1927,  1.0649, 20.9428]],

        [[30.4323, 42.5724,  8.9925,  0.9251,  1.0139, 22.3565]],

        [[23.1119, 40.0869, 16.0093,  1.4608,  1.6890, 51.0413]]])
Signal Variance: tensor([ 0.2670,  1.2956, 11.2888,  0.4886])
Estimated target variance: tensor([ 0.0346,  1.1387, 11.6822,  0.1688])
N: 200
Signal to noise ratio: tensor([27.7978, 54.1602, 67.3480, 39.0132])
Bound on condition number: tensor([154544.2339, 586666.9428, 907152.2195, 304407.0091])
Policy Optimizer learning rate:
0.09801805417839134
Experience 20, Iter 0, disc loss: 0.0007435570823943423, policy loss: 15.992829778392249
Experience 20, Iter 1, disc loss: 0.000969917403916928, policy loss: 16.816580073807607
Experience 20, Iter 2, disc loss: 0.001202045096806745, policy loss: 15.555848273255453
Experience 20, Iter 3, disc loss: 0.0014516256258551886, policy loss: 16.25694475498327
Experience 20, Iter 4, disc loss: 0.0012362349687128345, policy loss: 15.810897284055448
Experience 20, Iter 5, disc loss: 0.0013951987873412962, policy loss: 16.439980026484108
Experience 20, Iter 6, disc loss: 0.001666995137758398, policy loss: 15.940508128723298
Experience 20, Iter 7, disc loss: 0.001665702573900977, policy loss: 15.16836810568731
Experience 20, Iter 8, disc loss: 0.0016208746341079665, policy loss: 17.440428268675586
Experience 20, Iter 9, disc loss: 0.0016266549620254905, policy loss: 15.840318833820294
Experience 20, Iter 10, disc loss: 0.0017476222055777392, policy loss: 16.443814048685503
Experience 20, Iter 11, disc loss: 0.0016853469951639644, policy loss: 17.00659438173311
Experience 20, Iter 12, disc loss: 0.0017048343477319547, policy loss: 17.28917428560714
Experience 20, Iter 13, disc loss: 0.0016780932952042919, policy loss: 15.918901825871913
Experience 20, Iter 14, disc loss: 0.0016157572106905147, policy loss: 16.953169045928178
Experience 20, Iter 15, disc loss: 0.0016511961156703478, policy loss: 15.755959661108122
Experience 20, Iter 16, disc loss: 0.0015127805672050774, policy loss: 16.284627310768073
Experience 20, Iter 17, disc loss: 0.003499661443365771, policy loss: 17.06245055157025
Experience 20, Iter 18, disc loss: 0.0014114788942534721, policy loss: 16.946877625573077
Experience 20, Iter 19, disc loss: 0.08323515434667315, policy loss: 16.74647895086725
Experience 20, Iter 20, disc loss: 0.0016883173520267436, policy loss: 15.423529279050344
Experience 20, Iter 21, disc loss: 0.0015221450978728642, policy loss: 17.66876579851315
Experience 20, Iter 22, disc loss: 0.0016659545012778212, policy loss: 16.270114577353574
Experience 20, Iter 23, disc loss: 0.001596226828817433, policy loss: 16.644321159448964
Experience 20, Iter 24, disc loss: 0.0016198638817479212, policy loss: 17.251651958493916
Experience 20, Iter 25, disc loss: 0.0020585727916091075, policy loss: 15.356586340853177
Experience 20, Iter 26, disc loss: 0.001624722502170217, policy loss: 16.776726697151773
Experience 20, Iter 27, disc loss: 0.0018615504933530381, policy loss: 15.10668956605797
Experience 20, Iter 28, disc loss: 0.0015924956556247173, policy loss: 16.780905614142
Experience 20, Iter 29, disc loss: 0.0015162039929128167, policy loss: 16.212011565807998
Experience 20, Iter 30, disc loss: 0.0014807909772978217, policy loss: 17.16979710600222
Experience 20, Iter 31, disc loss: 0.0015615167774252743, policy loss: 15.193804504387977
Experience 20, Iter 32, disc loss: 0.0015389212462291405, policy loss: 16.30790489264222
Experience 20, Iter 33, disc loss: 0.0013183664704461485, policy loss: 16.777273891636163
Experience 20, Iter 34, disc loss: 0.0018504866432547192, policy loss: 15.876162995076967
Experience 20, Iter 35, disc loss: 0.0012034501629898113, policy loss: 16.57918835813489
Experience 20, Iter 36, disc loss: 0.0011945049022904905, policy loss: 15.56592549927705
Experience 20, Iter 37, disc loss: 0.001076685843632477, policy loss: 17.4408086616342
Experience 20, Iter 38, disc loss: 0.0010238791659282957, policy loss: 16.696100197355793
Experience 20, Iter 39, disc loss: 0.001430343617507127, policy loss: 16.278803770865217
Experience 20, Iter 40, disc loss: 0.0009273565896032751, policy loss: 17.211519156466863
Experience 20, Iter 41, disc loss: 0.0010068926376045734, policy loss: 15.789074391572626
Experience 20, Iter 42, disc loss: 0.0008876715125466043, policy loss: 16.675619502191758
Experience 20, Iter 43, disc loss: 0.0008374560229680676, policy loss: 16.140357193722195
Experience 20, Iter 44, disc loss: 0.0009248717712887669, policy loss: 17.146274399125062
Experience 20, Iter 45, disc loss: 0.001469894837627887, policy loss: 15.936717377273052
Experience 20, Iter 46, disc loss: 0.0072247884822091825, policy loss: 16.204448798109855
Experience 20, Iter 47, disc loss: 0.0007017593944828098, policy loss: 15.759195720320845
Experience 20, Iter 48, disc loss: 0.0006912085213908032, policy loss: 15.763782542489682
Experience 20, Iter 49, disc loss: 0.001006404389954816, policy loss: 15.396341160065722
Experience 20, Iter 50, disc loss: 0.005603249210165857, policy loss: 16.487633664722626
Experience 20, Iter 51, disc loss: 0.0006771045911768035, policy loss: 16.22344002720217
Experience 20, Iter 52, disc loss: 0.0006570102470441572, policy loss: 16.476113094503823
Experience 20, Iter 53, disc loss: 0.0015416067049738042, policy loss: 15.68048260991652
Experience 20, Iter 54, disc loss: 0.0006760834577896919, policy loss: 16.203128680656494
Experience 20, Iter 55, disc loss: 0.0011928627623870565, policy loss: 15.58557050130959
Experience 20, Iter 56, disc loss: 0.0006463446540089439, policy loss: 16.648721783850476
Experience 20, Iter 57, disc loss: 0.0006185517359501251, policy loss: 16.15836712184698
Experience 20, Iter 58, disc loss: 0.0007121527816744043, policy loss: 16.73354082481931
Experience 20, Iter 59, disc loss: 0.0006278944896027268, policy loss: 15.994596819720002
Experience 20, Iter 60, disc loss: 0.0008100755702271104, policy loss: 16.42762146027958
Experience 20, Iter 61, disc loss: 0.001089795028552366, policy loss: 16.56019661100882
Experience 20, Iter 62, disc loss: 0.0006737740994146073, policy loss: 16.584826687606125
Experience 20, Iter 63, disc loss: 0.0006363268339725218, policy loss: 16.684350169636957
Experience 20, Iter 64, disc loss: 0.0005999009792431434, policy loss: 15.04504314215444
Experience 20, Iter 65, disc loss: 0.0005583450382318198, policy loss: 15.664814305232493
Experience 20, Iter 66, disc loss: 0.0005611861967761235, policy loss: 16.81832428209098
Experience 20, Iter 67, disc loss: 0.000761975473828728, policy loss: 15.696881033279364
Experience 20, Iter 68, disc loss: 0.000567320910060331, policy loss: 15.683823746424782
Experience 20, Iter 69, disc loss: 0.003154014136221556, policy loss: 16.0847100879869
Experience 20, Iter 70, disc loss: 0.0006417797461413282, policy loss: 16.66955710687633
Experience 20, Iter 71, disc loss: 0.0007284467599586302, policy loss: 16.796857518796326
Experience 20, Iter 72, disc loss: 0.0013501088412485025, policy loss: 16.032320900936433
Experience 20, Iter 73, disc loss: 0.0008786096094227373, policy loss: 15.462321528904322
Experience 20, Iter 74, disc loss: 0.00048391884301683436, policy loss: 16.049399909478527
Experience 20, Iter 75, disc loss: 0.0005162026167568718, policy loss: 14.476780784405346
Experience 20, Iter 76, disc loss: 0.00048063016015312997, policy loss: 16.63319885081055
Experience 20, Iter 77, disc loss: 0.0007196788144076193, policy loss: 15.417458002730108
Experience 20, Iter 78, disc loss: 0.005996880523500323, policy loss: 17.942496421373427
Experience 20, Iter 79, disc loss: 0.0006555340800918764, policy loss: 15.603349017769197
Experience 20, Iter 80, disc loss: 0.0006167534673700404, policy loss: 16.717814938928
Experience 20, Iter 81, disc loss: 0.00048265804466509526, policy loss: 16.393460893924143
Experience 20, Iter 82, disc loss: 0.0004959062633544376, policy loss: 16.558636781202544
Experience 20, Iter 83, disc loss: 0.00048398870910820583, policy loss: 15.737955404003124
Experience 20, Iter 84, disc loss: 0.0010090082738747025, policy loss: 17.15944765424446
Experience 20, Iter 85, disc loss: 0.0005217927232688377, policy loss: 16.30366632167125
Experience 20, Iter 86, disc loss: 0.00047899463751485314, policy loss: 16.050318574531598
Experience 20, Iter 87, disc loss: 0.0005177962246930904, policy loss: 16.081455055778132
Experience 20, Iter 88, disc loss: 0.0009727227288530207, policy loss: 16.317171313663614
Experience 20, Iter 89, disc loss: 0.0005156280379505964, policy loss: 15.549366100329905
Experience 20, Iter 90, disc loss: 0.00046799255560985947, policy loss: 16.319259920755492
Experience 20, Iter 91, disc loss: 0.0005573044477110338, policy loss: 15.974914389239796
Experience 20, Iter 92, disc loss: 0.003358131595474489, policy loss: 15.361788516378603
Experience 20, Iter 93, disc loss: 0.0010514251862011933, policy loss: 16.003035088972922
Experience 20, Iter 94, disc loss: 0.0008406110564615969, policy loss: 17.014188158411976
Experience 20, Iter 95, disc loss: 0.0006650456864436982, policy loss: 16.522528898784028
Experience 20, Iter 96, disc loss: 0.0007140940403094665, policy loss: 15.37879494425347
Experience 20, Iter 97, disc loss: 0.0006493453818979082, policy loss: 15.478315863524195
Experience 20, Iter 98, disc loss: 0.00045420773547374314, policy loss: 16.620298070913847
Experience 20, Iter 99, disc loss: 0.0006175123023912751, policy loss: 16.37554334737002
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0084],
        [0.2874],
        [2.9460],
        [0.0434]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0545, 0.3581, 2.0032, 0.0379, 0.0094, 6.9823]],

        [[0.0545, 0.3581, 2.0032, 0.0379, 0.0094, 6.9823]],

        [[0.0545, 0.3581, 2.0032, 0.0379, 0.0094, 6.9823]],

        [[0.0545, 0.3581, 2.0032, 0.0379, 0.0094, 6.9823]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0334,  1.1496, 11.7840,  0.1736], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0334,  1.1496, 11.7840,  0.1736])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.351
Iter 2/2000 - Loss: 4.402
Iter 3/2000 - Loss: 4.256
Iter 4/2000 - Loss: 4.223
Iter 5/2000 - Loss: 4.212
Iter 6/2000 - Loss: 4.135
Iter 7/2000 - Loss: 4.035
Iter 8/2000 - Loss: 3.954
Iter 9/2000 - Loss: 3.891
Iter 10/2000 - Loss: 3.814
Iter 11/2000 - Loss: 3.708
Iter 12/2000 - Loss: 3.584
Iter 13/2000 - Loss: 3.456
Iter 14/2000 - Loss: 3.328
Iter 15/2000 - Loss: 3.193
Iter 16/2000 - Loss: 3.042
Iter 17/2000 - Loss: 2.872
Iter 18/2000 - Loss: 2.684
Iter 19/2000 - Loss: 2.480
Iter 20/2000 - Loss: 2.265
Iter 1981/2000 - Loss: -6.771
Iter 1982/2000 - Loss: -6.771
Iter 1983/2000 - Loss: -6.771
Iter 1984/2000 - Loss: -6.771
Iter 1985/2000 - Loss: -6.771
Iter 1986/2000 - Loss: -6.771
Iter 1987/2000 - Loss: -6.771
Iter 1988/2000 - Loss: -6.771
Iter 1989/2000 - Loss: -6.771
Iter 1990/2000 - Loss: -6.771
Iter 1991/2000 - Loss: -6.771
Iter 1992/2000 - Loss: -6.771
Iter 1993/2000 - Loss: -6.771
Iter 1994/2000 - Loss: -6.771
Iter 1995/2000 - Loss: -6.772
Iter 1996/2000 - Loss: -6.772
Iter 1997/2000 - Loss: -6.772
Iter 1998/2000 - Loss: -6.772
Iter 1999/2000 - Loss: -6.772
Iter 2000/2000 - Loss: -6.772
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[18.0824, 15.0573, 32.7827, 14.5641,  4.5139, 64.4787]],

        [[26.4384, 41.1461,  8.7027,  1.1991,  0.9758, 20.8424]],

        [[29.7594, 42.7493,  8.7138,  0.9323,  0.9890, 22.8436]],

        [[22.6151, 39.8646, 17.3858,  1.5159,  1.9669, 51.4942]]])
Signal Variance: tensor([ 0.2594,  1.2657, 11.2362,  0.5630])
Estimated target variance: tensor([ 0.0334,  1.1496, 11.7840,  0.1736])
N: 210
Signal to noise ratio: tensor([27.1154, 54.5484, 67.6507, 41.6245])
Bound on condition number: tensor([154402.2835, 624862.2163, 961091.7087, 363847.0979])
Policy Optimizer learning rate:
0.09791483623609773
Experience 21, Iter 0, disc loss: 0.0004438975618421625, policy loss: 16.70934804688504
Experience 21, Iter 1, disc loss: 0.001419330893454219, policy loss: 15.15702055420549
Experience 21, Iter 2, disc loss: 0.0005018323722851454, policy loss: 16.5976806975793
Experience 21, Iter 3, disc loss: 0.006336346094252733, policy loss: 15.798923748816652
Experience 21, Iter 4, disc loss: 0.0004248915068553924, policy loss: 16.869238252985586
Experience 21, Iter 5, disc loss: 0.0006366402387868963, policy loss: 16.38343773707482
Experience 21, Iter 6, disc loss: 0.0007482107893469758, policy loss: 16.500223659174445
Experience 21, Iter 7, disc loss: 0.0013266929304375122, policy loss: 17.14243040255039
Experience 21, Iter 8, disc loss: 0.000543167241099386, policy loss: 16.47069233538764
Experience 21, Iter 9, disc loss: 0.008600699063358568, policy loss: 17.116042824763547
Experience 21, Iter 10, disc loss: 0.0006895262318233305, policy loss: 15.701464433635067
Experience 21, Iter 11, disc loss: 0.0006588758060849676, policy loss: 15.116977231089248
Experience 21, Iter 12, disc loss: 0.0012136939552096532, policy loss: 16.058579411534964
Experience 21, Iter 13, disc loss: 0.0005034756834326504, policy loss: 16.321682492070615
Experience 21, Iter 14, disc loss: 0.0010436725416796255, policy loss: 15.372518116127853
Experience 21, Iter 15, disc loss: 0.0004949861458550641, policy loss: 16.42608645129344
Experience 21, Iter 16, disc loss: 0.00046714000180226727, policy loss: 16.117785117296123
Experience 21, Iter 17, disc loss: 0.0008668764959723269, policy loss: 15.551104083916112
Experience 21, Iter 18, disc loss: 0.00195107427019668, policy loss: 16.350677244526704
Experience 21, Iter 19, disc loss: 0.03240715662068263, policy loss: 14.824311498473264
Experience 21, Iter 20, disc loss: 0.00121587055580588, policy loss: 15.717522144554483
Experience 21, Iter 21, disc loss: 0.0007499762034985902, policy loss: 16.466207722931717
Experience 21, Iter 22, disc loss: 0.0005686196015965911, policy loss: 16.614653467979874
Experience 21, Iter 23, disc loss: 0.0005805226042878026, policy loss: 16.864227665608915
Experience 21, Iter 24, disc loss: 0.0006991293952935559, policy loss: 17.21034678890812
Experience 21, Iter 25, disc loss: 0.0006471835271762337, policy loss: 16.07600466703902
Experience 21, Iter 26, disc loss: 0.0007230502824747125, policy loss: 16.963092435918735
Experience 21, Iter 27, disc loss: 0.0008173366819374549, policy loss: 15.632318762138972
Experience 21, Iter 28, disc loss: 0.0007216247214926271, policy loss: 16.342103053355892
Experience 21, Iter 29, disc loss: 0.0007255773730970662, policy loss: 17.025346212259947
Experience 21, Iter 30, disc loss: 0.0010026018934363142, policy loss: 15.730406075114601
Experience 21, Iter 31, disc loss: 0.001559551273708118, policy loss: 15.553793827026887
Experience 21, Iter 32, disc loss: 0.0006865992188394306, policy loss: 17.621028037102576
Experience 21, Iter 33, disc loss: 0.0007200266036753358, policy loss: 16.057074792645665
Experience 21, Iter 34, disc loss: 0.0007275683705948662, policy loss: 15.874778570475225
Experience 21, Iter 35, disc loss: 0.000740305088852798, policy loss: 15.345684467587223
Experience 21, Iter 36, disc loss: 0.008831879110124697, policy loss: 15.869559348403378
Experience 21, Iter 37, disc loss: 0.0006780458037402237, policy loss: 16.850235010662292
Experience 21, Iter 38, disc loss: 0.0006934329223012101, policy loss: 16.00335719049233
Experience 21, Iter 39, disc loss: 0.0006702457564283073, policy loss: 16.70665493327771
Experience 21, Iter 40, disc loss: 0.001155268688639299, policy loss: 16.06170572207267
Experience 21, Iter 41, disc loss: 0.0006772328389035672, policy loss: 16.317546202179436
Experience 21, Iter 42, disc loss: 0.0016735520249075621, policy loss: 15.84971257893244
Experience 21, Iter 43, disc loss: 0.0013822264141197902, policy loss: 17.403007762977122
Experience 21, Iter 44, disc loss: 0.0006876147683747338, policy loss: 17.463007623881783
Experience 21, Iter 45, disc loss: 0.07386266616048781, policy loss: 15.454789079627917
Experience 21, Iter 46, disc loss: 0.0006947673014266928, policy loss: 17.014343294291976
Experience 21, Iter 47, disc loss: 0.0036492428668942253, policy loss: 17.028322169998653
Experience 21, Iter 48, disc loss: 0.0007832077348853114, policy loss: 16.372267991529093
Experience 21, Iter 49, disc loss: 0.0009560430253080033, policy loss: 16.518697005857202
Experience 21, Iter 50, disc loss: 0.0008925710039276682, policy loss: 16.291382541789588
Experience 21, Iter 51, disc loss: 0.0009443547170090712, policy loss: 18.37617527530893
Experience 21, Iter 52, disc loss: 0.0018265217098546267, policy loss: 16.381597507473835
Experience 21, Iter 53, disc loss: 0.0009486299800855797, policy loss: 17.516902571204778
Experience 21, Iter 54, disc loss: 0.0009497470851739607, policy loss: 17.14201818982953
Experience 21, Iter 55, disc loss: 0.0010110040089040206, policy loss: 17.73727560280507
Experience 21, Iter 56, disc loss: 0.0011990826711368514, policy loss: 16.146286129814563
Experience 21, Iter 57, disc loss: 0.0010388938381111186, policy loss: 17.32317277694637
Experience 21, Iter 58, disc loss: 0.0010288153245569204, policy loss: 16.95772325482586
Experience 21, Iter 59, disc loss: 0.0011457267899103777, policy loss: 16.374517855691163
Experience 21, Iter 60, disc loss: 0.0009295241622954199, policy loss: 17.824685290767427
Experience 21, Iter 61, disc loss: 0.0011508212200620786, policy loss: 17.241579565353568
Experience 21, Iter 62, disc loss: 0.0009477427763341273, policy loss: 16.293021129763083
Experience 21, Iter 63, disc loss: 0.0009248195843601714, policy loss: 17.275163213886362
Experience 21, Iter 64, disc loss: 0.0008386535004429667, policy loss: 16.53849031717604
Experience 21, Iter 65, disc loss: 0.0010533713485899843, policy loss: 16.500377741810237
Experience 21, Iter 66, disc loss: 0.0007954492845629014, policy loss: 17.20216716772285
Experience 21, Iter 67, disc loss: 0.0007657291851903831, policy loss: 16.70481301113073
Experience 21, Iter 68, disc loss: 0.0008399954881560035, policy loss: 16.28167794303462
Experience 21, Iter 69, disc loss: 0.0008986420289165614, policy loss: 16.197713588347106
Experience 21, Iter 70, disc loss: 0.0007213087599192461, policy loss: 17.094090334700027
Experience 21, Iter 71, disc loss: 0.0012852033268176653, policy loss: 15.756984228187743
Experience 21, Iter 72, disc loss: 0.004263014070725545, policy loss: 16.201229592547918
Experience 21, Iter 73, disc loss: 0.0006477077874308538, policy loss: 17.48157201132155
Experience 21, Iter 74, disc loss: 0.0006367187207658335, policy loss: 17.2581112085
Experience 21, Iter 75, disc loss: 0.0006430946113242725, policy loss: 16.72644600959685
Experience 21, Iter 76, disc loss: 0.000611274344624135, policy loss: 17.36297043609558
Experience 21, Iter 77, disc loss: 0.0005987568995376623, policy loss: 15.43588399153318
Experience 21, Iter 78, disc loss: 0.0006252452362356865, policy loss: 16.91141671449205
Experience 21, Iter 79, disc loss: 0.003119595854978407, policy loss: 16.8802338902035
Experience 21, Iter 80, disc loss: 0.003409332415414974, policy loss: 15.790300546844454
Experience 21, Iter 81, disc loss: 0.0005464915698843552, policy loss: 16.467625982396086
Experience 21, Iter 82, disc loss: 0.0005383036430185135, policy loss: 17.117940649230718
Experience 21, Iter 83, disc loss: 0.0005565802153999627, policy loss: 16.719003755536818
Experience 21, Iter 84, disc loss: 0.0006246049549807623, policy loss: 16.953084269139367
Experience 21, Iter 85, disc loss: 0.0032461139132008176, policy loss: 15.635144340303018
Experience 21, Iter 86, disc loss: 0.0007203355469192077, policy loss: 17.10971095177299
Experience 21, Iter 87, disc loss: 0.0005814835612664545, policy loss: 16.130820476438405
Experience 21, Iter 88, disc loss: 0.0007951315398334804, policy loss: 16.43003031643957
Experience 21, Iter 89, disc loss: 0.0005121824318221229, policy loss: 15.826582048337212
Experience 21, Iter 90, disc loss: 0.0007839394181790735, policy loss: 17.178364327486875
Experience 21, Iter 91, disc loss: 0.0005189464264539674, policy loss: 16.021662944470943
Experience 21, Iter 92, disc loss: 0.0005301945643073468, policy loss: 16.672893877380563
Experience 21, Iter 93, disc loss: 0.0006102865072105737, policy loss: 15.583791856041259
Experience 21, Iter 94, disc loss: 0.010025065493105839, policy loss: 16.446597433610208
Experience 21, Iter 95, disc loss: 0.0013862758777674945, policy loss: 15.662807920603905
Experience 21, Iter 96, disc loss: 0.0011002222314655408, policy loss: 17.394128682518506
Experience 21, Iter 97, disc loss: 0.0007572127511718588, policy loss: 14.549528454808069
Experience 21, Iter 98, disc loss: 0.0014464942656422777, policy loss: 14.399910375655265
Experience 21, Iter 99, disc loss: 0.0009235345022349489, policy loss: 14.129987571629663
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0081],
        [0.2835],
        [2.9217],
        [0.0438]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0523, 0.3479, 2.0198, 0.0381, 0.0095, 6.9080]],

        [[0.0523, 0.3479, 2.0198, 0.0381, 0.0095, 6.9080]],

        [[0.0523, 0.3479, 2.0198, 0.0381, 0.0095, 6.9080]],

        [[0.0523, 0.3479, 2.0198, 0.0381, 0.0095, 6.9080]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0324,  1.1340, 11.6867,  0.1751], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0324,  1.1340, 11.6867,  0.1751])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.335
Iter 2/2000 - Loss: 4.395
Iter 3/2000 - Loss: 4.244
Iter 4/2000 - Loss: 4.215
Iter 5/2000 - Loss: 4.208
Iter 6/2000 - Loss: 4.129
Iter 7/2000 - Loss: 4.028
Iter 8/2000 - Loss: 3.951
Iter 9/2000 - Loss: 3.890
Iter 10/2000 - Loss: 3.810
Iter 11/2000 - Loss: 3.701
Iter 12/2000 - Loss: 3.577
Iter 13/2000 - Loss: 3.448
Iter 14/2000 - Loss: 3.318
Iter 15/2000 - Loss: 3.180
Iter 16/2000 - Loss: 3.026
Iter 17/2000 - Loss: 2.854
Iter 18/2000 - Loss: 2.663
Iter 19/2000 - Loss: 2.457
Iter 20/2000 - Loss: 2.238
Iter 1981/2000 - Loss: -6.848
Iter 1982/2000 - Loss: -6.848
Iter 1983/2000 - Loss: -6.848
Iter 1984/2000 - Loss: -6.849
Iter 1985/2000 - Loss: -6.849
Iter 1986/2000 - Loss: -6.849
Iter 1987/2000 - Loss: -6.849
Iter 1988/2000 - Loss: -6.849
Iter 1989/2000 - Loss: -6.849
Iter 1990/2000 - Loss: -6.849
Iter 1991/2000 - Loss: -6.849
Iter 1992/2000 - Loss: -6.849
Iter 1993/2000 - Loss: -6.849
Iter 1994/2000 - Loss: -6.849
Iter 1995/2000 - Loss: -6.849
Iter 1996/2000 - Loss: -6.849
Iter 1997/2000 - Loss: -6.849
Iter 1998/2000 - Loss: -6.849
Iter 1999/2000 - Loss: -6.849
Iter 2000/2000 - Loss: -6.850
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[18.4603, 15.2741, 32.3423, 13.8179,  4.2523, 64.3345]],

        [[26.3488, 41.8481,  8.4333,  1.3213,  0.9530, 20.4377]],

        [[29.6913, 41.9770,  8.3502,  0.9432,  0.9982, 23.3801]],

        [[22.3422, 39.1169, 17.1596,  1.5559,  1.9795, 50.7490]]])
Signal Variance: tensor([ 0.2577,  1.2243, 10.9916,  0.5529])
Estimated target variance: tensor([ 0.0324,  1.1340, 11.6867,  0.1751])
N: 220
Signal to noise ratio: tensor([27.5004, 53.0308, 67.9827, 41.1411])
Bound on condition number: tensor([ 166380.5646,  618699.7792, 1016763.8884,  372371.5477])
Policy Optimizer learning rate:
0.0978117269874902
Experience 22, Iter 0, disc loss: 0.0013723963763065164, policy loss: 13.354237940221251
Experience 22, Iter 1, disc loss: 0.0006469823679207066, policy loss: 15.83229466222815
Experience 22, Iter 2, disc loss: 0.0006022391376009447, policy loss: 15.247397332064116
Experience 22, Iter 3, disc loss: 0.0008582829647315886, policy loss: 13.325846934764524
Experience 22, Iter 4, disc loss: 0.0011571876413409244, policy loss: 14.167198041557334
Experience 22, Iter 5, disc loss: 0.05229920467355058, policy loss: 13.137205232013011
Experience 22, Iter 6, disc loss: 0.001303287676717801, policy loss: 15.414020449167907
Experience 22, Iter 7, disc loss: 0.0013523345039559915, policy loss: 13.436576863606422
Experience 22, Iter 8, disc loss: 0.0008616483336442412, policy loss: 12.951236709027242
Experience 22, Iter 9, disc loss: 0.0010309505860865466, policy loss: 14.641218667299912
Experience 22, Iter 10, disc loss: 0.000810028485932225, policy loss: 13.191334176496293
Experience 22, Iter 11, disc loss: 0.0014400663664955171, policy loss: 13.387457145490384
Experience 22, Iter 12, disc loss: 0.0014284649317058752, policy loss: 12.718601768048451
Experience 22, Iter 13, disc loss: 0.0015426821936196113, policy loss: 12.878704257259626
Experience 22, Iter 14, disc loss: 0.0009152679283006963, policy loss: 13.787581203876048
Experience 22, Iter 15, disc loss: 0.0009311825810194721, policy loss: 14.938209304575532
Experience 22, Iter 16, disc loss: 0.001022289424129417, policy loss: 13.772795192565553
Experience 22, Iter 17, disc loss: 0.0009146143902356805, policy loss: 15.172714765686923
Experience 22, Iter 18, disc loss: 0.00101568188859192, policy loss: 13.592515654520913
Experience 22, Iter 19, disc loss: 0.0009386977400266462, policy loss: 13.696460104093664
Experience 22, Iter 20, disc loss: 0.0010581494398997993, policy loss: 14.057024535774467
Experience 22, Iter 21, disc loss: 0.0009061354240412763, policy loss: 13.697339106721653
Experience 22, Iter 22, disc loss: 0.0008433455289502108, policy loss: 15.099613390373438
Experience 22, Iter 23, disc loss: 0.0009370659823314394, policy loss: 13.674479128671754
Experience 22, Iter 24, disc loss: 0.0008966731346668843, policy loss: 13.000170948590618
Experience 22, Iter 25, disc loss: 0.0008291695242954801, policy loss: 14.293356704694661
Experience 22, Iter 26, disc loss: 0.0008319783401464434, policy loss: 14.485187138217189
Experience 22, Iter 27, disc loss: 0.0013313301349947271, policy loss: 13.455046115223567
Experience 22, Iter 28, disc loss: 0.0007424660722889449, policy loss: 14.503152918232797
Experience 22, Iter 29, disc loss: 0.0007514787559515132, policy loss: 13.140979208181207
Experience 22, Iter 30, disc loss: 0.0017145964701584973, policy loss: 13.049947176556273
Experience 22, Iter 31, disc loss: 0.0009203843809983555, policy loss: 13.470853846151982
Experience 22, Iter 32, disc loss: 0.0015001418202775108, policy loss: 13.502075012779766
Experience 22, Iter 33, disc loss: 0.0011100597944932538, policy loss: 13.912665527924142
Experience 22, Iter 34, disc loss: 0.0012798879583078323, policy loss: 13.177012710862098
Experience 22, Iter 35, disc loss: 0.00100449862479674, policy loss: 13.328200172618583
Experience 22, Iter 36, disc loss: 0.0006749626632872225, policy loss: 13.420096895512476
Experience 22, Iter 37, disc loss: 0.0007150880365624978, policy loss: 12.777676750126215
Experience 22, Iter 38, disc loss: 0.0006219799359500564, policy loss: 13.32257673078497
Experience 22, Iter 39, disc loss: 0.0006881469559887103, policy loss: 13.653450204458554
Experience 22, Iter 40, disc loss: 0.0005023066456037996, policy loss: 13.869595299265375
Experience 22, Iter 41, disc loss: 0.0006514655902048743, policy loss: 13.612882360246058
Experience 22, Iter 42, disc loss: 0.0006072939792381493, policy loss: 13.50471799390976
Experience 22, Iter 43, disc loss: 0.0005790926207651515, policy loss: 12.676524429280542
Experience 22, Iter 44, disc loss: 0.0007876129835645771, policy loss: 14.14987828225232
Experience 22, Iter 45, disc loss: 0.000505249938343801, policy loss: 13.683535192754796
Experience 22, Iter 46, disc loss: 0.0015747159489165766, policy loss: 13.040255824632956
Experience 22, Iter 47, disc loss: 0.001306380627347716, policy loss: 12.463501292689912
Experience 22, Iter 48, disc loss: 0.01308939835944928, policy loss: 12.377808894416596
Experience 22, Iter 49, disc loss: 0.0007444111177955934, policy loss: 13.565754281553781
Experience 22, Iter 50, disc loss: 0.0005123396175521861, policy loss: 13.785490240948505
Experience 22, Iter 51, disc loss: 0.0006400245884693645, policy loss: 13.354099038399218
Experience 22, Iter 52, disc loss: 0.0006229581968542141, policy loss: 13.500237771008239
Experience 22, Iter 53, disc loss: 0.004145340552813497, policy loss: 13.334613322069078
Experience 22, Iter 54, disc loss: 0.000558800734355012, policy loss: 13.053374582933756
Experience 22, Iter 55, disc loss: 0.0015626818659949, policy loss: 13.806048283629067
Experience 22, Iter 56, disc loss: 0.0005600798664881095, policy loss: 15.183897884351829
Experience 22, Iter 57, disc loss: 0.0005021101668924972, policy loss: 15.768335764321805
Experience 22, Iter 58, disc loss: 0.0005290723713230986, policy loss: 15.829983857137705
Experience 22, Iter 59, disc loss: 0.0005130101935613551, policy loss: 15.446911021331193
Experience 22, Iter 60, disc loss: 0.0005153662707202866, policy loss: 16.550098812080513
Experience 22, Iter 61, disc loss: 0.0005307408290088088, policy loss: 15.318897856850297
Experience 22, Iter 62, disc loss: 0.0014364077304683748, policy loss: 14.755476396194927
Experience 22, Iter 63, disc loss: 0.0005902024816656298, policy loss: 13.997560054775382
Experience 22, Iter 64, disc loss: 0.0022645636627670355, policy loss: 12.301973246323895
Experience 22, Iter 65, disc loss: 0.000505791940061192, policy loss: 16.275026300771202
Experience 22, Iter 66, disc loss: 0.0005928325006075975, policy loss: 15.446862287180702
Experience 22, Iter 67, disc loss: 0.0009724961896888287, policy loss: 13.020385259925261
Experience 22, Iter 68, disc loss: 0.001648279495709767, policy loss: 13.9998491836769
Experience 22, Iter 69, disc loss: 0.0005163411650779205, policy loss: 16.678012474073576
Experience 22, Iter 70, disc loss: 0.0005336621670544082, policy loss: 14.33599425621463
Experience 22, Iter 71, disc loss: 0.0009799139844372143, policy loss: 13.054828432758127
Experience 22, Iter 72, disc loss: 0.0005235313756191158, policy loss: 15.393810108233316
Experience 22, Iter 73, disc loss: 0.0004669394650403984, policy loss: 16.736787730485315
Experience 22, Iter 74, disc loss: 0.0005754892397905057, policy loss: 14.072350700557838
Experience 22, Iter 75, disc loss: 0.000707155825257783, policy loss: 14.539165481402847
Experience 22, Iter 76, disc loss: 0.0009388960507948905, policy loss: 13.296886674065993
Experience 22, Iter 77, disc loss: 0.0005709839260941503, policy loss: 14.635523754384092
Experience 22, Iter 78, disc loss: 0.0004342612260574054, policy loss: 15.709405232872093
Experience 22, Iter 79, disc loss: 0.007959125952722645, policy loss: 15.758826856105195
Experience 22, Iter 80, disc loss: 0.0006538804052793834, policy loss: 15.432965496416031
Experience 22, Iter 81, disc loss: 0.0005416608627412546, policy loss: 16.534653629992746
Experience 22, Iter 82, disc loss: 0.007426111410724638, policy loss: 13.719280182190506
Experience 22, Iter 83, disc loss: 0.0007756933012106637, policy loss: 13.470275404467287
Experience 22, Iter 84, disc loss: 0.0007734736542196666, policy loss: 14.32915690034354
Experience 22, Iter 85, disc loss: 0.0006682136228614929, policy loss: 14.744044455292617
Experience 22, Iter 86, disc loss: 0.0010254383136367427, policy loss: 14.30890195961153
Experience 22, Iter 87, disc loss: 0.0005085732746653029, policy loss: 16.153338405555335
Experience 22, Iter 88, disc loss: 0.0007725178268539143, policy loss: 17.272770951742874
Experience 22, Iter 89, disc loss: 0.0005052301342430786, policy loss: 16.16002508825096
Experience 22, Iter 90, disc loss: 0.0008924015497329442, policy loss: 13.893775499168584
Experience 22, Iter 91, disc loss: 0.0005648807855365395, policy loss: 14.209550013198722
Experience 22, Iter 92, disc loss: 0.0005895646091837719, policy loss: 14.855356546124499
Experience 22, Iter 93, disc loss: 0.0005062250470081422, policy loss: 16.192050392763345
Experience 22, Iter 94, disc loss: 0.0005356874435276265, policy loss: 15.433246250727244
Experience 22, Iter 95, disc loss: 0.0005245068385125163, policy loss: 14.162955889877088
Experience 22, Iter 96, disc loss: 0.0012737057597145384, policy loss: 14.644677775395376
Experience 22, Iter 97, disc loss: 0.0022077557660988385, policy loss: 13.01863993627893
Experience 22, Iter 98, disc loss: 0.0016973900176416444, policy loss: 14.434547434047003
Experience 22, Iter 99, disc loss: 0.0006072053662638873, policy loss: 15.217187583581195
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0080],
        [0.2839],
        [2.9391],
        [0.0435]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0502, 0.3425, 2.0166, 0.0380, 0.0101, 6.9383]],

        [[0.0502, 0.3425, 2.0166, 0.0380, 0.0101, 6.9383]],

        [[0.0502, 0.3425, 2.0166, 0.0380, 0.0101, 6.9383]],

        [[0.0502, 0.3425, 2.0166, 0.0380, 0.0101, 6.9383]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0320,  1.1356, 11.7562,  0.1741], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0320,  1.1356, 11.7562,  0.1741])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.321
Iter 2/2000 - Loss: 4.370
Iter 3/2000 - Loss: 4.226
Iter 4/2000 - Loss: 4.194
Iter 5/2000 - Loss: 4.182
Iter 6/2000 - Loss: 4.102
Iter 7/2000 - Loss: 4.002
Iter 8/2000 - Loss: 3.924
Iter 9/2000 - Loss: 3.859
Iter 10/2000 - Loss: 3.775
Iter 11/2000 - Loss: 3.662
Iter 12/2000 - Loss: 3.533
Iter 13/2000 - Loss: 3.401
Iter 14/2000 - Loss: 3.268
Iter 15/2000 - Loss: 3.125
Iter 16/2000 - Loss: 2.964
Iter 17/2000 - Loss: 2.783
Iter 18/2000 - Loss: 2.584
Iter 19/2000 - Loss: 2.372
Iter 20/2000 - Loss: 2.148
Iter 1981/2000 - Loss: -6.916
Iter 1982/2000 - Loss: -6.916
Iter 1983/2000 - Loss: -6.916
Iter 1984/2000 - Loss: -6.916
Iter 1985/2000 - Loss: -6.916
Iter 1986/2000 - Loss: -6.916
Iter 1987/2000 - Loss: -6.916
Iter 1988/2000 - Loss: -6.916
Iter 1989/2000 - Loss: -6.916
Iter 1990/2000 - Loss: -6.916
Iter 1991/2000 - Loss: -6.916
Iter 1992/2000 - Loss: -6.916
Iter 1993/2000 - Loss: -6.917
Iter 1994/2000 - Loss: -6.917
Iter 1995/2000 - Loss: -6.917
Iter 1996/2000 - Loss: -6.917
Iter 1997/2000 - Loss: -6.917
Iter 1998/2000 - Loss: -6.917
Iter 1999/2000 - Loss: -6.917
Iter 2000/2000 - Loss: -6.917
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[17.7587, 15.1874, 30.0588, 11.8746,  4.8226, 63.4068]],

        [[25.8792, 41.7743,  8.5414,  1.2368,  1.0232, 20.0806]],

        [[28.8416, 40.9267,  8.4142,  0.9348,  1.0082, 23.5689]],

        [[21.5999, 38.6450, 16.2813,  1.6043,  2.0124, 47.6194]]])
Signal Variance: tensor([ 0.2621,  1.2025, 11.0843,  0.5193])
Estimated target variance: tensor([ 0.0320,  1.1356, 11.7562,  0.1741])
N: 230
Signal to noise ratio: tensor([28.0618, 52.3687, 68.2101, 39.6003])
Bound on condition number: tensor([ 181118.1364,  630772.2076, 1070104.3154,  360683.2794])
Policy Optimizer learning rate:
0.09770872631810884
Experience 23, Iter 0, disc loss: 0.0006609920537162524, policy loss: 16.820884260899042
Experience 23, Iter 1, disc loss: 0.0010832937845870175, policy loss: 17.75875241424225
Experience 23, Iter 2, disc loss: 0.0005900126276323583, policy loss: 16.841503908797517
Experience 23, Iter 3, disc loss: 0.0004392325346302241, policy loss: 16.58784577363263
Experience 23, Iter 4, disc loss: 0.0008244354010728922, policy loss: 15.764392933458758
Experience 23, Iter 5, disc loss: 0.0009902140056890739, policy loss: 14.636258525088603
Experience 23, Iter 6, disc loss: 0.0006002710801217201, policy loss: 15.595368735832084
Experience 23, Iter 7, disc loss: 0.0004750925932238671, policy loss: 15.6692527982681
Experience 23, Iter 8, disc loss: 0.0007644381542750913, policy loss: 15.361350545988577
Experience 23, Iter 9, disc loss: 0.00046391760969008726, policy loss: 14.780518424857947
Experience 23, Iter 10, disc loss: 0.0004383140228946785, policy loss: 14.312852762834465
Experience 23, Iter 11, disc loss: 0.0004641274765515443, policy loss: 14.886936534692383
Experience 23, Iter 12, disc loss: 0.000513908065808694, policy loss: 15.468513924732292
Experience 23, Iter 13, disc loss: 0.0004927912298882257, policy loss: 14.78286887109545
Experience 23, Iter 14, disc loss: 0.0006550254362479135, policy loss: 14.49996707508868
Experience 23, Iter 15, disc loss: 0.00038045085484219476, policy loss: 15.98536339106909
Experience 23, Iter 16, disc loss: 0.000663916627753702, policy loss: 14.795392033363186
Experience 23, Iter 17, disc loss: 0.0004386109235138598, policy loss: 15.279535860684318
Experience 23, Iter 18, disc loss: 0.0004267070843302905, policy loss: 17.49259900108575
Experience 23, Iter 19, disc loss: 0.0004603614522646886, policy loss: 16.171266184575323
Experience 23, Iter 20, disc loss: 0.00034403781559404353, policy loss: 16.513534243363694
Experience 23, Iter 21, disc loss: 0.0005065216808389398, policy loss: 14.328632841720356
Experience 23, Iter 22, disc loss: 0.0006518753797814581, policy loss: 15.723484654765896
Experience 23, Iter 23, disc loss: 0.001717274955977276, policy loss: 14.913919923885134
Experience 23, Iter 24, disc loss: 0.0004036386865113673, policy loss: 16.50027385320867
Experience 23, Iter 25, disc loss: 0.0003408257781809084, policy loss: 15.573996925907611
Experience 23, Iter 26, disc loss: 0.0003071468524548439, policy loss: 17.928953134394842
Experience 23, Iter 27, disc loss: 0.0003620971122415492, policy loss: 14.665323193728454
Experience 23, Iter 28, disc loss: 0.0002992848761943227, policy loss: 17.26593312454129
Experience 23, Iter 29, disc loss: 0.000400395843984085, policy loss: 15.125729648470866
Experience 23, Iter 30, disc loss: 0.000374647708092059, policy loss: 15.221855039119779
Experience 23, Iter 31, disc loss: 0.0003750684677721341, policy loss: 15.017285043911736
Experience 23, Iter 32, disc loss: 0.004518341756852762, policy loss: 15.597038755486011
Experience 23, Iter 33, disc loss: 0.0002993963670022832, policy loss: 14.725361240320009
Experience 23, Iter 34, disc loss: 0.0007593057951506524, policy loss: 14.014516561463632
Experience 23, Iter 35, disc loss: 0.00029145607075928433, policy loss: 16.150516315187975
Experience 23, Iter 36, disc loss: 0.00037061011890471285, policy loss: 16.160515837464114
Experience 23, Iter 37, disc loss: 0.00031963296224179875, policy loss: 13.459273579763707
Experience 23, Iter 38, disc loss: 0.0004927834225783948, policy loss: 13.543604501135503
Experience 23, Iter 39, disc loss: 0.00046619610104752126, policy loss: 13.364936742981559
Experience 23, Iter 40, disc loss: 0.00031846164515601745, policy loss: 15.082179222001495
Experience 23, Iter 41, disc loss: 0.000296204324062606, policy loss: 16.975851312338197
Experience 23, Iter 42, disc loss: 0.00037587478650918206, policy loss: 16.187519773963658
Experience 23, Iter 43, disc loss: 0.00032196289503127736, policy loss: 15.47128134603242
Experience 23, Iter 44, disc loss: 0.008987730335464184, policy loss: 14.669084350657801
Experience 23, Iter 45, disc loss: 0.000532186524926239, policy loss: 13.547485837038774
Experience 23, Iter 46, disc loss: 0.009845625140008913, policy loss: 13.540053960885842
Experience 23, Iter 47, disc loss: 0.00041988941638965583, policy loss: 14.586937018891248
Experience 23, Iter 48, disc loss: 0.0007210380068199227, policy loss: 13.131791325403663
Experience 23, Iter 49, disc loss: 0.0005413165100629216, policy loss: 13.979416477044056
Experience 23, Iter 50, disc loss: 0.0006875925145893656, policy loss: 13.912808532926148
Experience 23, Iter 51, disc loss: 0.0006170172288402569, policy loss: 14.793685446017651
Experience 23, Iter 52, disc loss: 0.0005139928052085233, policy loss: 13.602517488981059
Experience 23, Iter 53, disc loss: 0.0005688512492553119, policy loss: 14.292327180260324
Experience 23, Iter 54, disc loss: 0.00044702434053629504, policy loss: 13.906844325173036
Experience 23, Iter 55, disc loss: 0.0005306025666670612, policy loss: 13.148453863107942
Experience 23, Iter 56, disc loss: 0.0005289278506029112, policy loss: 14.17457135705999
Experience 23, Iter 57, disc loss: 0.00046175510887045405, policy loss: 14.235547083116419
Experience 23, Iter 58, disc loss: 0.001830189136319007, policy loss: 12.233581409133684
Experience 23, Iter 59, disc loss: 0.0004847308110277728, policy loss: 14.380045718212873
Experience 23, Iter 60, disc loss: 0.0005000066015330767, policy loss: 14.425675279812012
Experience 23, Iter 61, disc loss: 0.0005721693508536688, policy loss: 14.41182074907957
Experience 23, Iter 62, disc loss: 0.00043593564903855807, policy loss: 14.544066987603841
Experience 23, Iter 63, disc loss: 0.0004446395897587323, policy loss: 16.277474202508273
Experience 23, Iter 64, disc loss: 0.0005384485215218428, policy loss: 15.275292600317183
Experience 23, Iter 65, disc loss: 0.00045771258708594907, policy loss: 15.74183934017303
Experience 23, Iter 66, disc loss: 0.0004182497424506723, policy loss: 14.796385392520131
Experience 23, Iter 67, disc loss: 0.0018565455908342633, policy loss: 15.092087928347764
Experience 23, Iter 68, disc loss: 0.0004017843412762251, policy loss: 15.523683916145796
Experience 23, Iter 69, disc loss: 0.0007286378817073836, policy loss: 15.0473133559918
Experience 23, Iter 70, disc loss: 0.0005212502487675592, policy loss: 16.141827384714777
Experience 23, Iter 71, disc loss: 0.0016083605426076624, policy loss: 16.304987786385162
Experience 23, Iter 72, disc loss: 0.000418513737375478, policy loss: 15.960354159116488
Experience 23, Iter 73, disc loss: 0.0014573088530058283, policy loss: 14.600798928551063
Experience 23, Iter 74, disc loss: 0.010925187695074383, policy loss: 15.07531685637457
Experience 23, Iter 75, disc loss: 0.0004413117393804554, policy loss: 16.096861028897916
Experience 23, Iter 76, disc loss: 0.000443876601777593, policy loss: 15.746051284929644
Experience 23, Iter 77, disc loss: 0.00043569974026469894, policy loss: 15.351285704676684
Experience 23, Iter 78, disc loss: 0.005404052921988338, policy loss: 15.521068239039042
Experience 23, Iter 79, disc loss: 0.000478980178137168, policy loss: 15.721739296713302
Experience 23, Iter 80, disc loss: 0.00045667439074284673, policy loss: 15.630601780035207
Experience 23, Iter 81, disc loss: 0.0005173225414596052, policy loss: 15.97220543019899
Experience 23, Iter 82, disc loss: 0.0004577110934608395, policy loss: 15.819365123161756
Experience 23, Iter 83, disc loss: 0.0005551256185161551, policy loss: 16.075710760761805
Experience 23, Iter 84, disc loss: 0.0027065834265223923, policy loss: 16.18327513460578
Experience 23, Iter 85, disc loss: 0.0005626956052521021, policy loss: 16.183693911635935
Experience 23, Iter 86, disc loss: 0.0005087201519416733, policy loss: 14.678416789104173
Experience 23, Iter 87, disc loss: 0.0004758901189778573, policy loss: 16.916543203645034
Experience 23, Iter 88, disc loss: 0.0006056522647617618, policy loss: 15.546799791268011
Experience 23, Iter 89, disc loss: 0.0025147522346639366, policy loss: 15.634470953842824
Experience 23, Iter 90, disc loss: 0.0006563588772044017, policy loss: 14.732365775750633
Experience 23, Iter 91, disc loss: 0.0019653171465618133, policy loss: 15.67422756210722
Experience 23, Iter 92, disc loss: 0.0005405836912244283, policy loss: 16.23066775280072
Experience 23, Iter 93, disc loss: 0.0005181193876196475, policy loss: 15.616263679297974
Experience 23, Iter 94, disc loss: 0.0005968673455231432, policy loss: 16.161711349326147
Experience 23, Iter 95, disc loss: 0.0005024292194876921, policy loss: 15.263836315756095
Experience 23, Iter 96, disc loss: 0.000849370012806894, policy loss: 15.023887758870657
Experience 23, Iter 97, disc loss: 0.0009678429385770563, policy loss: 15.82604733746384
Experience 23, Iter 98, disc loss: 0.0004969559721015604, policy loss: 15.4370869557698
Experience 23, Iter 99, disc loss: 0.0006051836173685292, policy loss: 16.789470237389725
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0078],
        [0.2820],
        [2.9136],
        [0.0425]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0486, 0.3326, 1.9754, 0.0374, 0.0099, 6.8767]],

        [[0.0486, 0.3326, 1.9754, 0.0374, 0.0099, 6.8767]],

        [[0.0486, 0.3326, 1.9754, 0.0374, 0.0099, 6.8767]],

        [[0.0486, 0.3326, 1.9754, 0.0374, 0.0099, 6.8767]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0311,  1.1281, 11.6544,  0.1701], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0311,  1.1281, 11.6544,  0.1701])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.287
Iter 2/2000 - Loss: 4.344
Iter 3/2000 - Loss: 4.192
Iter 4/2000 - Loss: 4.160
Iter 5/2000 - Loss: 4.152
Iter 6/2000 - Loss: 4.073
Iter 7/2000 - Loss: 3.969
Iter 8/2000 - Loss: 3.886
Iter 9/2000 - Loss: 3.821
Iter 10/2000 - Loss: 3.740
Iter 11/2000 - Loss: 3.628
Iter 12/2000 - Loss: 3.496
Iter 13/2000 - Loss: 3.360
Iter 14/2000 - Loss: 3.223
Iter 15/2000 - Loss: 3.079
Iter 16/2000 - Loss: 2.919
Iter 17/2000 - Loss: 2.739
Iter 18/2000 - Loss: 2.539
Iter 19/2000 - Loss: 2.324
Iter 20/2000 - Loss: 2.096
Iter 1981/2000 - Loss: -7.006
Iter 1982/2000 - Loss: -7.006
Iter 1983/2000 - Loss: -7.006
Iter 1984/2000 - Loss: -7.006
Iter 1985/2000 - Loss: -7.006
Iter 1986/2000 - Loss: -7.006
Iter 1987/2000 - Loss: -7.006
Iter 1988/2000 - Loss: -7.007
Iter 1989/2000 - Loss: -7.007
Iter 1990/2000 - Loss: -7.007
Iter 1991/2000 - Loss: -7.007
Iter 1992/2000 - Loss: -7.007
Iter 1993/2000 - Loss: -7.007
Iter 1994/2000 - Loss: -7.007
Iter 1995/2000 - Loss: -7.007
Iter 1996/2000 - Loss: -7.007
Iter 1997/2000 - Loss: -7.007
Iter 1998/2000 - Loss: -7.007
Iter 1999/2000 - Loss: -7.007
Iter 2000/2000 - Loss: -7.007
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[17.5398, 15.1261, 30.3895, 10.9401,  4.7557, 62.1005]],

        [[25.7505, 41.4423,  8.6790,  1.2149,  1.0277, 19.8890]],

        [[29.1255, 43.1650,  8.3251,  0.9170,  1.0001, 24.0203]],

        [[21.3428, 38.2125, 16.3348,  1.5902,  1.9994, 47.5328]]])
Signal Variance: tensor([ 0.2544,  1.1922, 10.9052,  0.5108])
Estimated target variance: tensor([ 0.0311,  1.1281, 11.6544,  0.1701])
N: 240
Signal to noise ratio: tensor([27.9079, 51.0258, 67.6982, 39.9361])
Bound on condition number: tensor([ 186925.5605,  624871.6993, 1099933.5929,  382774.6724])
Policy Optimizer learning rate:
0.09760583411361425
Experience 24, Iter 0, disc loss: 0.0020258159592135283, policy loss: 14.828788829997873
Experience 24, Iter 1, disc loss: 0.0005138089433215375, policy loss: 14.958432592188199
Experience 24, Iter 2, disc loss: 0.0004719197350140122, policy loss: 15.170900859061152
Experience 24, Iter 3, disc loss: 0.0006373260337570524, policy loss: 15.709919200398096
Experience 24, Iter 4, disc loss: 0.0012768691172852316, policy loss: 15.672941827777265
Experience 24, Iter 5, disc loss: 0.0005371418994229652, policy loss: 14.884136219829209
Experience 24, Iter 6, disc loss: 0.0005153967449915827, policy loss: 15.597952756733239
Experience 24, Iter 7, disc loss: 0.0007181328884928748, policy loss: 15.103742131129895
Experience 24, Iter 8, disc loss: 0.0007273094285730985, policy loss: 14.855585106963193
Experience 24, Iter 9, disc loss: 0.0004196923547577123, policy loss: 16.26691066605368
Experience 24, Iter 10, disc loss: 0.0004221352724985985, policy loss: 14.409436626621352
Experience 24, Iter 11, disc loss: 0.010182070002466799, policy loss: 16.623768985275134
Experience 24, Iter 12, disc loss: 0.000688861894382287, policy loss: 14.71954234865297
Experience 24, Iter 13, disc loss: 0.0019239550394397785, policy loss: 15.353585542703469
Experience 24, Iter 14, disc loss: 0.0004682575275875688, policy loss: 14.357527907161447
Experience 24, Iter 15, disc loss: 0.0004507192686988804, policy loss: 15.507489316174755
Experience 24, Iter 16, disc loss: 0.0006184793399107887, policy loss: 15.98813480211857
Experience 24, Iter 17, disc loss: 0.0004650955779291385, policy loss: 16.136839167752537
Experience 24, Iter 18, disc loss: 0.00044236430039623387, policy loss: 15.60151672946685
Experience 24, Iter 19, disc loss: 0.0004682415249268064, policy loss: 15.090178283347893
Experience 24, Iter 20, disc loss: 0.0004458274736313256, policy loss: 15.183399264488292
Experience 24, Iter 21, disc loss: 0.001826621223164717, policy loss: 15.322225983330089
Experience 24, Iter 22, disc loss: 0.0007043034218975589, policy loss: 15.85081828469277
Experience 24, Iter 23, disc loss: 0.0004509391763531087, policy loss: 16.076334724000823
Experience 24, Iter 24, disc loss: 0.00047921100798791486, policy loss: 15.862499835930205
Experience 24, Iter 25, disc loss: 0.0004836763511000109, policy loss: 14.927967856313312
Experience 24, Iter 26, disc loss: 0.00044512928427229966, policy loss: 16.088234132850104
Experience 24, Iter 27, disc loss: 0.0006248258954518618, policy loss: 15.4326906699765
Experience 24, Iter 28, disc loss: 0.0036828877999019708, policy loss: 16.29491318718655
Experience 24, Iter 29, disc loss: 0.0004977577979167174, policy loss: 14.954015616978932
Experience 24, Iter 30, disc loss: 0.00044391881500949876, policy loss: 14.706790849134343
Experience 24, Iter 31, disc loss: 0.0004514702445909573, policy loss: 15.577233213909405
Experience 24, Iter 32, disc loss: 0.0014184998879745454, policy loss: 14.824325763379285
Experience 24, Iter 33, disc loss: 0.0004328223951610128, policy loss: 16.38835470020411
Experience 24, Iter 34, disc loss: 0.0004229572065519029, policy loss: 15.842467343591192
Experience 24, Iter 35, disc loss: 0.0004670970472649904, policy loss: 17.263208136177596
Experience 24, Iter 36, disc loss: 0.0004446160335617867, policy loss: 15.488251646447836
Experience 24, Iter 37, disc loss: 0.0008246124921785279, policy loss: 15.05413606105569
Experience 24, Iter 38, disc loss: 0.0005796434050087203, policy loss: 15.34792921245864
Experience 24, Iter 39, disc loss: 0.0004463768836255167, policy loss: 14.439150434572984
Experience 24, Iter 40, disc loss: 0.0004090350652777717, policy loss: 16.1708172694344
Experience 24, Iter 41, disc loss: 0.00040408918894548854, policy loss: 14.405154755348619
Experience 24, Iter 42, disc loss: 0.00041947561085767373, policy loss: 15.930212993028228
Experience 24, Iter 43, disc loss: 0.0006278646663772374, policy loss: 15.143003956503787
Experience 24, Iter 44, disc loss: 0.0008126651575517961, policy loss: 15.47240424715472
Experience 24, Iter 45, disc loss: 0.0004476420624684345, policy loss: 15.698732146551498
Experience 24, Iter 46, disc loss: 0.0016018109045339121, policy loss: 15.560015525905687
Experience 24, Iter 47, disc loss: 0.0004329239295009029, policy loss: 14.521182420398723
Experience 24, Iter 48, disc loss: 0.0010299695551026688, policy loss: 16.87887137099525
Experience 24, Iter 49, disc loss: 0.0005524647054697285, policy loss: 14.764922939502352
Experience 24, Iter 50, disc loss: 0.00033847485640157396, policy loss: 16.03390610018975
Experience 24, Iter 51, disc loss: 0.000597582936476763, policy loss: 16.009733160802416
Experience 24, Iter 52, disc loss: 0.0005170171662073663, policy loss: 14.220036659291255
Experience 24, Iter 53, disc loss: 0.0005909254994663996, policy loss: 14.622870590806155
Experience 24, Iter 54, disc loss: 0.0003617526720908166, policy loss: 15.221384338107073
Experience 24, Iter 55, disc loss: 0.0005950885274546091, policy loss: 13.993951740144597
Experience 24, Iter 56, disc loss: 0.0004438813484991755, policy loss: 14.622883089531015
Experience 24, Iter 57, disc loss: 0.00033718704560022257, policy loss: 15.452216614371949
Experience 24, Iter 58, disc loss: 0.00034673323004793307, policy loss: 15.159787249867037
Experience 24, Iter 59, disc loss: 0.0003359792302060275, policy loss: 13.973186664350472
Experience 24, Iter 60, disc loss: 0.0004748337002880997, policy loss: 15.379902069735927
Experience 24, Iter 61, disc loss: 0.0003379480252529686, policy loss: 15.116761855444258
Experience 24, Iter 62, disc loss: 0.00031120336441513855, policy loss: 15.48981384908719
Experience 24, Iter 63, disc loss: 0.00028921721261603955, policy loss: 14.776844826107208
Experience 24, Iter 64, disc loss: 0.0003769128323658035, policy loss: 14.770061850764801
Experience 24, Iter 65, disc loss: 0.000490166430219949, policy loss: 14.261228741039535
Experience 24, Iter 66, disc loss: 0.00029994649455733637, policy loss: 14.519717349546582
Experience 24, Iter 67, disc loss: 0.0006538176362275203, policy loss: 14.765370866294823
Experience 24, Iter 68, disc loss: 0.002167619936235725, policy loss: 14.53887565985838
Experience 24, Iter 69, disc loss: 0.0003122850815625151, policy loss: 15.382726650720501
Experience 24, Iter 70, disc loss: 0.0006804458714617248, policy loss: 15.638765340623008
Experience 24, Iter 71, disc loss: 0.0020778307456290634, policy loss: 15.696188117170333
Experience 24, Iter 72, disc loss: 0.0002755435807570447, policy loss: 14.77515653998869
Experience 24, Iter 73, disc loss: 0.00029743837852245223, policy loss: 15.5719451716408
Experience 24, Iter 74, disc loss: 0.0002892019735437127, policy loss: 16.20706000251583
Experience 24, Iter 75, disc loss: 0.00034841577297227574, policy loss: 15.44185274546495
Experience 24, Iter 76, disc loss: 0.00027250366782471843, policy loss: 15.442049331252859
Experience 24, Iter 77, disc loss: 0.0009838430358522952, policy loss: 15.569821993683078
Experience 24, Iter 78, disc loss: 0.00028061865660730125, policy loss: 15.624305339242422
Experience 24, Iter 79, disc loss: 0.00031795503208707776, policy loss: 14.37658521917804
Experience 24, Iter 80, disc loss: 0.00042995741398334984, policy loss: 14.984552059926887
Experience 24, Iter 81, disc loss: 0.0022502819110906186, policy loss: 14.516980786737964
Experience 24, Iter 82, disc loss: 0.0005900760931536057, policy loss: 14.334420092534847
Experience 24, Iter 83, disc loss: 0.00034348850376249285, policy loss: 14.225961377925557
Experience 24, Iter 84, disc loss: 0.000344100966106759, policy loss: 15.677983390851018
Experience 24, Iter 85, disc loss: 0.00030790719349561434, policy loss: 14.534914315839009
Experience 24, Iter 86, disc loss: 0.0005078718113923045, policy loss: 15.246719422218819
Experience 24, Iter 87, disc loss: 0.00042328106987900787, policy loss: 14.838426341720297
Experience 24, Iter 88, disc loss: 0.0003609116290761294, policy loss: 15.4433675669152
Experience 24, Iter 89, disc loss: 0.00026988328665380703, policy loss: 15.817855176263445
Experience 24, Iter 90, disc loss: 0.0002595390384919044, policy loss: 15.941470201265954
Experience 24, Iter 91, disc loss: 0.00029287261408130275, policy loss: 15.289297235602907
Experience 24, Iter 92, disc loss: 0.0002788154425734994, policy loss: 16.4591616085253
Experience 24, Iter 93, disc loss: 0.0004113175379128418, policy loss: 14.4708001153056
Experience 24, Iter 94, disc loss: 0.0006640032882936735, policy loss: 16.41606020482905
Experience 24, Iter 95, disc loss: 0.00027133559182678576, policy loss: 16.198000016710544
Experience 24, Iter 96, disc loss: 0.0002895759516463205, policy loss: 14.689379141239744
Experience 24, Iter 97, disc loss: 0.0005449933663824193, policy loss: 15.31612707479885
Experience 24, Iter 98, disc loss: 0.0007116348356532296, policy loss: 15.678800152958019
Experience 24, Iter 99, disc loss: 0.00034500733755878793, policy loss: 15.798354655712593
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0081],
        [0.2812],
        [2.9175],
        [0.0427]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0512, 0.3418, 1.9901, 0.0382, 0.0098, 6.8555]],

        [[0.0512, 0.3418, 1.9901, 0.0382, 0.0098, 6.8555]],

        [[0.0512, 0.3418, 1.9901, 0.0382, 0.0098, 6.8555]],

        [[0.0512, 0.3418, 1.9901, 0.0382, 0.0098, 6.8555]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0323,  1.1247, 11.6698,  0.1708], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0323,  1.1247, 11.6698,  0.1708])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.296
Iter 2/2000 - Loss: 4.349
Iter 3/2000 - Loss: 4.196
Iter 4/2000 - Loss: 4.154
Iter 5/2000 - Loss: 4.142
Iter 6/2000 - Loss: 4.065
Iter 7/2000 - Loss: 3.958
Iter 8/2000 - Loss: 3.869
Iter 9/2000 - Loss: 3.799
Iter 10/2000 - Loss: 3.716
Iter 11/2000 - Loss: 3.603
Iter 12/2000 - Loss: 3.469
Iter 13/2000 - Loss: 3.328
Iter 14/2000 - Loss: 3.186
Iter 15/2000 - Loss: 3.038
Iter 16/2000 - Loss: 2.874
Iter 17/2000 - Loss: 2.691
Iter 18/2000 - Loss: 2.488
Iter 19/2000 - Loss: 2.269
Iter 20/2000 - Loss: 2.036
Iter 1981/2000 - Loss: -7.060
Iter 1982/2000 - Loss: -7.060
Iter 1983/2000 - Loss: -7.060
Iter 1984/2000 - Loss: -7.060
Iter 1985/2000 - Loss: -7.060
Iter 1986/2000 - Loss: -7.060
Iter 1987/2000 - Loss: -7.061
Iter 1988/2000 - Loss: -7.061
Iter 1989/2000 - Loss: -7.061
Iter 1990/2000 - Loss: -7.061
Iter 1991/2000 - Loss: -7.061
Iter 1992/2000 - Loss: -7.061
Iter 1993/2000 - Loss: -7.061
Iter 1994/2000 - Loss: -7.061
Iter 1995/2000 - Loss: -7.061
Iter 1996/2000 - Loss: -7.061
Iter 1997/2000 - Loss: -7.061
Iter 1998/2000 - Loss: -7.061
Iter 1999/2000 - Loss: -7.061
Iter 2000/2000 - Loss: -7.061
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[17.0612, 15.2073, 27.0776,  7.1652,  4.9294, 60.0709]],

        [[26.0323, 42.3090,  8.5343,  1.2483,  0.9872, 20.1123]],

        [[27.7828, 42.7178,  7.8473,  0.9196,  0.9770, 23.7476]],

        [[20.4774, 36.4596, 15.8671,  1.5208,  1.9522, 48.0436]]])
Signal Variance: tensor([ 0.2476,  1.1775, 10.5465,  0.4806])
Estimated target variance: tensor([ 0.0323,  1.1247, 11.6698,  0.1708])
N: 250
Signal to noise ratio: tensor([28.0441, 50.8711, 69.1780, 38.6790])
Bound on condition number: tensor([ 196618.5859,  646967.5704, 1196400.2196,  374016.5016])
Policy Optimizer learning rate:
0.09750305025978745
Experience 25, Iter 0, disc loss: 0.00029985638831735487, policy loss: 15.512130010688308
Experience 25, Iter 1, disc loss: 0.000574687195710889, policy loss: 13.393663101922737
Experience 25, Iter 2, disc loss: 0.0002610915310614025, policy loss: 15.573425348850849
Experience 25, Iter 3, disc loss: 0.00029626231930769367, policy loss: 16.045156719064
Experience 25, Iter 4, disc loss: 0.0003981593213275816, policy loss: 15.284301151843316
Experience 25, Iter 5, disc loss: 0.00025709727356551624, policy loss: 16.271039005682155
Experience 25, Iter 6, disc loss: 0.00048240998898363657, policy loss: 15.516222218177536
Experience 25, Iter 7, disc loss: 0.00024049269011693514, policy loss: 15.137682454294884
Experience 25, Iter 8, disc loss: 0.00034075474609107973, policy loss: 15.538358093090196
Experience 25, Iter 9, disc loss: 0.0003740029208499406, policy loss: 15.380829768461885
Experience 25, Iter 10, disc loss: 0.00023547138175547614, policy loss: 16.024999370012218
Experience 25, Iter 11, disc loss: 0.0005906895130208161, policy loss: 14.99465864060811
Experience 25, Iter 12, disc loss: 0.0002714844449768844, policy loss: 15.786561531269367
Experience 25, Iter 13, disc loss: 0.0002460637936023993, policy loss: 15.362521672554688
Experience 25, Iter 14, disc loss: 0.00023886872380199088, policy loss: 16.427635046714443
Experience 25, Iter 15, disc loss: 0.0002592017798524858, policy loss: 15.78042862626112
Experience 25, Iter 16, disc loss: 0.0005226894336495686, policy loss: 15.267997653498199
Experience 25, Iter 17, disc loss: 0.000601727978209856, policy loss: 15.976332580666186
Experience 25, Iter 18, disc loss: 0.0002906584114915872, policy loss: 15.005409416787554
Experience 25, Iter 19, disc loss: 0.0003223032517767683, policy loss: 15.304856247054747
Experience 25, Iter 20, disc loss: 0.0004018852311912489, policy loss: 14.420221500029394
Experience 25, Iter 21, disc loss: 0.00042323321326861183, policy loss: 16.436078494973955
Experience 25, Iter 22, disc loss: 0.0002522329036662429, policy loss: 15.738791120461835
Experience 25, Iter 23, disc loss: 0.00023442369914055647, policy loss: 15.344934708171364
Experience 25, Iter 24, disc loss: 0.000226653845543935, policy loss: 16.40970887158698
Experience 25, Iter 25, disc loss: 0.00029655062119902996, policy loss: 15.974786566037762
Experience 25, Iter 26, disc loss: 0.00023974943821393943, policy loss: 16.28252991206894
Experience 25, Iter 27, disc loss: 0.00021583693964416061, policy loss: 15.823416925569518
Experience 25, Iter 28, disc loss: 0.0002090809288398126, policy loss: 16.295505588910764
Experience 25, Iter 29, disc loss: 0.00019937963132492058, policy loss: 16.305822517655315
Experience 25, Iter 30, disc loss: 0.0002068762387465863, policy loss: 15.438003098558006
Experience 25, Iter 31, disc loss: 0.0003657907946054854, policy loss: 15.572568499504708
Experience 25, Iter 32, disc loss: 0.010623505199338273, policy loss: 15.084653629172669
Experience 25, Iter 33, disc loss: 0.009197991783206587, policy loss: 15.536225444340449
Experience 25, Iter 34, disc loss: 0.0003298908038891701, policy loss: 15.010207308392822
Experience 25, Iter 35, disc loss: 0.0005119334704282424, policy loss: 16.02544057904592
Experience 25, Iter 36, disc loss: 0.001705468066990378, policy loss: 15.09303412470022
Experience 25, Iter 37, disc loss: 0.0003005443277232567, policy loss: 15.82910250840238
Experience 25, Iter 38, disc loss: 0.0009968532833474185, policy loss: 16.406629211578498
Experience 25, Iter 39, disc loss: 0.0005454700678534194, policy loss: 15.756501579121085
Experience 25, Iter 40, disc loss: 0.0010045605700294766, policy loss: 16.003771812213962
Experience 25, Iter 41, disc loss: 0.0005200681744277089, policy loss: 14.030652239794959
Experience 25, Iter 42, disc loss: 0.00034743408265055424, policy loss: 15.363850688408146
Experience 25, Iter 43, disc loss: 0.0004016899120207955, policy loss: 14.992523897010622
Experience 25, Iter 44, disc loss: 0.00100226714573049, policy loss: 13.854545932573807
Experience 25, Iter 45, disc loss: 0.00035642149631960645, policy loss: 14.537766982469666
Experience 25, Iter 46, disc loss: 0.00042153107635959513, policy loss: 18.042986893786324
Experience 25, Iter 47, disc loss: 0.0007237699433288944, policy loss: 15.620536345964918
Experience 25, Iter 48, disc loss: 0.00037461596905155793, policy loss: 16.25536514163485
Experience 25, Iter 49, disc loss: 0.00040209923360763546, policy loss: 16.753893182487047
Experience 25, Iter 50, disc loss: 0.0011089560220739563, policy loss: 15.761915051619074
Experience 25, Iter 51, disc loss: 0.00039035682708355736, policy loss: 14.24296190715345
Experience 25, Iter 52, disc loss: 0.0005810305971278019, policy loss: 15.195322224540114
Experience 25, Iter 53, disc loss: 0.0005420736932935668, policy loss: 14.227500118445743
Experience 25, Iter 54, disc loss: 0.0004297808576495854, policy loss: 13.837679013352288
Experience 25, Iter 55, disc loss: 0.0010827880194989167, policy loss: 13.255636520232354
Experience 25, Iter 56, disc loss: 0.0005403372616884669, policy loss: 14.141890658893965
Experience 25, Iter 57, disc loss: 0.000559294187171577, policy loss: 13.701746516331175
Experience 25, Iter 58, disc loss: 0.0009051542671086195, policy loss: 14.787124743339959
Experience 25, Iter 59, disc loss: 0.0004232190117002575, policy loss: 15.46461120074598
Experience 25, Iter 60, disc loss: 0.0003890050321812981, policy loss: 14.825723821985482
Experience 25, Iter 61, disc loss: 0.0005030926507295682, policy loss: 14.599999079284501
Experience 25, Iter 62, disc loss: 0.0004152965872716698, policy loss: 14.850530755552466
Experience 25, Iter 63, disc loss: 0.0004321919059028746, policy loss: 14.112612793257021
Experience 25, Iter 64, disc loss: 0.0005033139178889235, policy loss: 13.657517681220092
Experience 25, Iter 65, disc loss: 0.0005220743758071768, policy loss: 13.852598224269101
Experience 25, Iter 66, disc loss: 0.0021406841343138408, policy loss: 14.732609117811478
Experience 25, Iter 67, disc loss: 0.0005529437709073225, policy loss: 13.753418660915093
Experience 25, Iter 68, disc loss: 0.0004789905075013666, policy loss: 14.076895482991674
Experience 25, Iter 69, disc loss: 0.0006650687455132279, policy loss: 12.83421725807643
Experience 25, Iter 70, disc loss: 0.0005936816719808597, policy loss: 14.834966990672804
Experience 25, Iter 71, disc loss: 0.0004732087007137056, policy loss: 15.310302092284356
Experience 25, Iter 72, disc loss: 0.0005793243248920358, policy loss: 13.967832722006404
Experience 25, Iter 73, disc loss: 0.0008656043279799883, policy loss: 14.519129986147824
Experience 25, Iter 74, disc loss: 0.00038722523234899193, policy loss: 15.735139365556154
Experience 25, Iter 75, disc loss: 0.0005174782196883587, policy loss: 15.354402244053933
Experience 25, Iter 76, disc loss: 0.0007805552905146803, policy loss: 14.367479099842159
Experience 25, Iter 77, disc loss: 0.0004038460420388928, policy loss: 14.220954212684717
Experience 25, Iter 78, disc loss: 0.000387842790751755, policy loss: 14.279880634916992
Experience 25, Iter 79, disc loss: 0.0005803873060933316, policy loss: 13.767398041585711
Experience 25, Iter 80, disc loss: 0.0037380356097669867, policy loss: 14.514308846262336
Experience 25, Iter 81, disc loss: 0.0006618669385774332, policy loss: 15.356204977091288
Experience 25, Iter 82, disc loss: 0.0023194209498377193, policy loss: 13.103834118730049
Experience 25, Iter 83, disc loss: 0.0015925216089156887, policy loss: 13.813709323907162
Experience 25, Iter 84, disc loss: 0.0008195684682313596, policy loss: 15.351630453880986
Experience 25, Iter 85, disc loss: 0.0006417074017954842, policy loss: 14.885390533554148
Experience 25, Iter 86, disc loss: 0.0012648734928387521, policy loss: 13.983298635840898
Experience 25, Iter 87, disc loss: 0.000530212927671302, policy loss: 14.819256801906754
Experience 25, Iter 88, disc loss: 0.0028314437047094335, policy loss: 14.830896471728103
Experience 25, Iter 89, disc loss: 0.0005080215548175683, policy loss: 15.462165614648628
Experience 25, Iter 90, disc loss: 0.0004967246208736913, policy loss: 14.012715241067468
Experience 25, Iter 91, disc loss: 0.0007224025442493576, policy loss: 15.012941096876297
Experience 25, Iter 92, disc loss: 0.0005092095871557378, policy loss: 15.557213694360954
Experience 25, Iter 93, disc loss: 0.0007232328661806251, policy loss: 14.247455446694008
Experience 25, Iter 94, disc loss: 0.0006555239248380855, policy loss: 14.947185369300547
Experience 25, Iter 95, disc loss: 0.0005103443529151873, policy loss: 13.879334555523783
Experience 25, Iter 96, disc loss: 0.0005122479530410141, policy loss: 14.79149298317899
Experience 25, Iter 97, disc loss: 0.000592460327556949, policy loss: 13.690563503679478
Experience 25, Iter 98, disc loss: 0.00040911773127885395, policy loss: 14.85351679817047
Experience 25, Iter 99, disc loss: 0.000389710596475127, policy loss: 14.326780714727873
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0081],
        [0.2779],
        [2.8715],
        [0.0438]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0540, 0.3466, 2.0280, 0.0387, 0.0101, 6.9261]],

        [[0.0540, 0.3466, 2.0280, 0.0387, 0.0101, 6.9261]],

        [[0.0540, 0.3466, 2.0280, 0.0387, 0.0101, 6.9261]],

        [[0.0540, 0.3466, 2.0280, 0.0387, 0.0101, 6.9261]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0325,  1.1117, 11.4860,  0.1753], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0325,  1.1117, 11.4860,  0.1753])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.287
Iter 2/2000 - Loss: 4.340
Iter 3/2000 - Loss: 4.181
Iter 4/2000 - Loss: 4.142
Iter 5/2000 - Loss: 4.127
Iter 6/2000 - Loss: 4.042
Iter 7/2000 - Loss: 3.931
Iter 8/2000 - Loss: 3.841
Iter 9/2000 - Loss: 3.769
Iter 10/2000 - Loss: 3.681
Iter 11/2000 - Loss: 3.563
Iter 12/2000 - Loss: 3.427
Iter 13/2000 - Loss: 3.284
Iter 14/2000 - Loss: 3.139
Iter 15/2000 - Loss: 2.985
Iter 16/2000 - Loss: 2.816
Iter 17/2000 - Loss: 2.627
Iter 18/2000 - Loss: 2.418
Iter 19/2000 - Loss: 2.192
Iter 20/2000 - Loss: 1.955
Iter 1981/2000 - Loss: -7.130
Iter 1982/2000 - Loss: -7.130
Iter 1983/2000 - Loss: -7.130
Iter 1984/2000 - Loss: -7.130
Iter 1985/2000 - Loss: -7.130
Iter 1986/2000 - Loss: -7.130
Iter 1987/2000 - Loss: -7.130
Iter 1988/2000 - Loss: -7.130
Iter 1989/2000 - Loss: -7.130
Iter 1990/2000 - Loss: -7.130
Iter 1991/2000 - Loss: -7.130
Iter 1992/2000 - Loss: -7.130
Iter 1993/2000 - Loss: -7.131
Iter 1994/2000 - Loss: -7.131
Iter 1995/2000 - Loss: -7.131
Iter 1996/2000 - Loss: -7.131
Iter 1997/2000 - Loss: -7.131
Iter 1998/2000 - Loss: -7.131
Iter 1999/2000 - Loss: -7.131
Iter 2000/2000 - Loss: -7.131
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[16.4172, 16.2779, 25.4344,  4.3856,  6.1841, 60.7703]],

        [[26.2579, 42.3061,  8.4379,  1.2408,  1.0045, 20.9345]],

        [[27.7362, 42.8126,  7.9077,  0.9414,  0.9628, 23.8502]],

        [[19.4661, 35.9202, 15.3167,  1.5215,  1.8531, 47.1588]]])
Signal Variance: tensor([ 0.2712,  1.1995, 10.6028,  0.4602])
Estimated target variance: tensor([ 0.0325,  1.1117, 11.4860,  0.1753])
N: 260
Signal to noise ratio: tensor([29.9022, 51.2609, 69.4838, 38.4575])
Bound on condition number: tensor([ 232478.3794,  683199.0398, 1255282.4067,  384536.4001])
Policy Optimizer learning rate:
0.09740037464252974
Experience 26, Iter 0, disc loss: 0.000839576819316613, policy loss: 13.41231409390387
Experience 26, Iter 1, disc loss: 0.0003524536129543762, policy loss: 15.291703764354638
Experience 26, Iter 2, disc loss: 0.00031279380033631877, policy loss: 15.217448286784641
Experience 26, Iter 3, disc loss: 0.00042071822694396757, policy loss: 15.020957326809354
Experience 26, Iter 4, disc loss: 0.0003238757973192933, policy loss: 15.168182902527235
Experience 26, Iter 5, disc loss: 0.0003353445602802282, policy loss: 15.708088764993747
Experience 26, Iter 6, disc loss: 0.0004983637362799539, policy loss: 13.704412646634227
Experience 26, Iter 7, disc loss: 0.0006404544968417917, policy loss: 14.6724460291993
Experience 26, Iter 8, disc loss: 0.000798980270702155, policy loss: 14.059949805033387
Experience 26, Iter 9, disc loss: 0.0003920292190906199, policy loss: 14.93286826330642
Experience 26, Iter 10, disc loss: 0.0002886937418561536, policy loss: 15.327875059644963
Experience 26, Iter 11, disc loss: 0.000299270558711596, policy loss: 15.484898406262786
Experience 26, Iter 12, disc loss: 0.00027938114990639566, policy loss: 15.525327875025688
Experience 26, Iter 13, disc loss: 0.0010323680106641637, policy loss: 14.585061943228778
Experience 26, Iter 14, disc loss: 0.0015186163034936729, policy loss: 14.543945967148021
Experience 26, Iter 15, disc loss: 0.00031186943219669677, policy loss: 14.593209348547976
Experience 26, Iter 16, disc loss: 0.0004444659963404612, policy loss: 14.435041736828609
Experience 26, Iter 17, disc loss: 0.000393556587900646, policy loss: 16.20930532524034
Experience 26, Iter 18, disc loss: 0.0004605918386447654, policy loss: 14.063437395189279
Experience 26, Iter 19, disc loss: 0.00025973493861343986, policy loss: 15.256292416058393
Experience 26, Iter 20, disc loss: 0.0006440615576633948, policy loss: 14.808472151062436
Experience 26, Iter 21, disc loss: 0.00040364379103119494, policy loss: 15.890821384797212
Experience 26, Iter 22, disc loss: 0.0008079858766353747, policy loss: 15.556334123369403
Experience 26, Iter 23, disc loss: 0.00048631110363672774, policy loss: 15.840488843611173
Experience 26, Iter 24, disc loss: 0.0004649640046641223, policy loss: 15.616916319422058
Experience 26, Iter 25, disc loss: 0.000359396429570349, policy loss: 15.208515112775705
Experience 26, Iter 26, disc loss: 0.0005320841162035131, policy loss: 14.880493242117913
Experience 26, Iter 27, disc loss: 0.0002567024620440498, policy loss: 16.1661220162603
Experience 26, Iter 28, disc loss: 0.0006789087076349636, policy loss: 15.046260891461674
Experience 26, Iter 29, disc loss: 0.00025288959642328994, policy loss: 14.850519320285171
Experience 26, Iter 30, disc loss: 0.00023288673840897428, policy loss: 16.993380158995762
Experience 26, Iter 31, disc loss: 0.00042243125957502675, policy loss: 17.237640213057084
Experience 26, Iter 32, disc loss: 0.00022271827010361195, policy loss: 17.19752680208753
Experience 26, Iter 33, disc loss: 0.0005791704186660734, policy loss: 16.256317356340567
Experience 26, Iter 34, disc loss: 0.00023342118446211297, policy loss: 15.841446578359024
Experience 26, Iter 35, disc loss: 0.0005999493082793942, policy loss: 14.468432111884997
Experience 26, Iter 36, disc loss: 0.00041519797030989533, policy loss: 14.681296782629321
Experience 26, Iter 37, disc loss: 0.00027956678283635474, policy loss: 14.28166250425031
Experience 26, Iter 38, disc loss: 0.0003024768985464288, policy loss: 14.738906833059177
Experience 26, Iter 39, disc loss: 0.0002749880799172891, policy loss: 16.24341435071983
Experience 26, Iter 40, disc loss: 0.0002429235676119884, policy loss: 14.608826912350256
Experience 26, Iter 41, disc loss: 0.00031312217559056497, policy loss: 15.06778803121085
Experience 26, Iter 42, disc loss: 0.0002586029043534627, policy loss: 15.380733948695841
Experience 26, Iter 43, disc loss: 0.0003473572528720037, policy loss: 13.781485154385827
Experience 26, Iter 44, disc loss: 0.0006296629292844614, policy loss: 15.884948859629638
Experience 26, Iter 45, disc loss: 0.0002612545896452136, policy loss: 15.632235718533936
Experience 26, Iter 46, disc loss: 0.0002077111165431663, policy loss: 14.54597106076954
Experience 26, Iter 47, disc loss: 0.00027140585892744215, policy loss: 14.851084159656134
Experience 26, Iter 48, disc loss: 0.00025884472842856686, policy loss: 15.45610319716132
Experience 26, Iter 49, disc loss: 0.00023634212305782693, policy loss: 14.971654372644677
Experience 26, Iter 50, disc loss: 0.00019291975043546626, policy loss: 16.545768751820738
Experience 26, Iter 51, disc loss: 0.00021030332975457159, policy loss: 14.898662013094118
Experience 26, Iter 52, disc loss: 0.0005026699750702609, policy loss: 15.313946664217628
Experience 26, Iter 53, disc loss: 0.00019530239322228013, policy loss: 16.467328321144294
Experience 26, Iter 54, disc loss: 0.00024893096860548555, policy loss: 15.612499489888485
Experience 26, Iter 55, disc loss: 0.0002629255275349, policy loss: 15.025231996554828
Experience 26, Iter 56, disc loss: 0.0002144822972556568, policy loss: 15.601928569303357
Experience 26, Iter 57, disc loss: 0.00041814582553425714, policy loss: 14.768679017892783
Experience 26, Iter 58, disc loss: 0.0003511627663275382, policy loss: 15.10833060791721
Experience 26, Iter 59, disc loss: 0.0004614203079571535, policy loss: 14.33751630058918
Experience 26, Iter 60, disc loss: 0.0001964556478059315, policy loss: 15.354275291924532
Experience 26, Iter 61, disc loss: 0.0003047681540178556, policy loss: 14.205575163473917
Experience 26, Iter 62, disc loss: 0.0005897197974723058, policy loss: 14.025690153287485
Experience 26, Iter 63, disc loss: 0.00017873260267374868, policy loss: 15.876683559655332
Experience 26, Iter 64, disc loss: 0.0002162187553362134, policy loss: 14.763031804708053
Experience 26, Iter 65, disc loss: 0.0003107205259888788, policy loss: 15.449307763784592
Experience 26, Iter 66, disc loss: 0.0005894541205348114, policy loss: 15.015501743345357
Experience 26, Iter 67, disc loss: 0.00045112505421413164, policy loss: 14.630608006711483
Experience 26, Iter 68, disc loss: 0.0008079144776508977, policy loss: 14.221554108762003
Experience 26, Iter 69, disc loss: 0.0006131429695631126, policy loss: 14.065573528850113
Experience 26, Iter 70, disc loss: 0.0005153524690919749, policy loss: 14.725715357081246
Experience 26, Iter 71, disc loss: 0.00045049284800957915, policy loss: 15.331448355048085
Experience 26, Iter 72, disc loss: 0.0011246794182949113, policy loss: 15.15628035519146
Experience 26, Iter 73, disc loss: 0.00023387031446421452, policy loss: 16.05315778183381
Experience 26, Iter 74, disc loss: 0.00016072718971308034, policy loss: 15.255137465662365
Experience 26, Iter 75, disc loss: 0.00017116314808293543, policy loss: 15.518691435318805
Experience 26, Iter 76, disc loss: 0.00019240971225888522, policy loss: 15.167929070761613
Experience 26, Iter 77, disc loss: 0.0003908897007084052, policy loss: 14.320704316535439
Experience 26, Iter 78, disc loss: 0.0002883753579275561, policy loss: 15.228578127413382
Experience 26, Iter 79, disc loss: 0.0003420500056359367, policy loss: 14.906803218925575
Experience 26, Iter 80, disc loss: 0.00027085449882455616, policy loss: 15.295482325964961
Experience 26, Iter 81, disc loss: 0.0002492144579391953, policy loss: 14.301364700417679
Experience 26, Iter 82, disc loss: 0.0009040384043779773, policy loss: 15.115655977249586
Experience 26, Iter 83, disc loss: 0.00021635361245752802, policy loss: 15.102317554530755
Experience 26, Iter 84, disc loss: 0.00023320616787629323, policy loss: 14.181134837586894
Experience 26, Iter 85, disc loss: 0.0002827904301352313, policy loss: 15.198443893993245
Experience 26, Iter 86, disc loss: 0.0003764141482765088, policy loss: 14.782363507805483
Experience 26, Iter 87, disc loss: 0.0004288698845010685, policy loss: 15.829324953140375
Experience 26, Iter 88, disc loss: 0.000212477965568114, policy loss: 14.93913349368874
Experience 26, Iter 89, disc loss: 0.019193432355450995, policy loss: 15.72277060183425
Experience 26, Iter 90, disc loss: 0.0002303710086182109, policy loss: 14.22909158019089
Experience 26, Iter 91, disc loss: 0.0002359640955120646, policy loss: 15.660804970257152
Experience 26, Iter 92, disc loss: 0.0007986216872257711, policy loss: 16.80248522001847
Experience 26, Iter 93, disc loss: 0.00024729889349775773, policy loss: 13.616604982967608
Experience 26, Iter 94, disc loss: 0.00023571960101575955, policy loss: 15.241099112374439
Experience 26, Iter 95, disc loss: 0.00025291907599789146, policy loss: 16.003009700993307
Experience 26, Iter 96, disc loss: 0.0004566007868453234, policy loss: 15.174206834545586
Experience 26, Iter 97, disc loss: 0.00034310013330358615, policy loss: 15.573496416456303
Experience 26, Iter 98, disc loss: 0.00348636090465188, policy loss: 15.774250094209052
Experience 26, Iter 99, disc loss: 0.0002425356212199341, policy loss: 16.989438876949563
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0081],
        [0.2827],
        [2.8706],
        [0.0462]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0536, 0.3468, 2.1198, 0.0391, 0.0103, 7.0074]],

        [[0.0536, 0.3468, 2.1198, 0.0391, 0.0103, 7.0074]],

        [[0.0536, 0.3468, 2.1198, 0.0391, 0.0103, 7.0074]],

        [[0.0536, 0.3468, 2.1198, 0.0391, 0.0103, 7.0074]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0324,  1.1306, 11.4825,  0.1849], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0324,  1.1306, 11.4825,  0.1849])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.315
Iter 2/2000 - Loss: 4.369
Iter 3/2000 - Loss: 4.207
Iter 4/2000 - Loss: 4.168
Iter 5/2000 - Loss: 4.151
Iter 6/2000 - Loss: 4.061
Iter 7/2000 - Loss: 3.949
Iter 8/2000 - Loss: 3.859
Iter 9/2000 - Loss: 3.784
Iter 10/2000 - Loss: 3.691
Iter 11/2000 - Loss: 3.571
Iter 12/2000 - Loss: 3.432
Iter 13/2000 - Loss: 3.287
Iter 14/2000 - Loss: 3.139
Iter 15/2000 - Loss: 2.982
Iter 16/2000 - Loss: 2.810
Iter 17/2000 - Loss: 2.619
Iter 18/2000 - Loss: 2.409
Iter 19/2000 - Loss: 2.183
Iter 20/2000 - Loss: 1.945
Iter 1981/2000 - Loss: -7.087
Iter 1982/2000 - Loss: -7.088
Iter 1983/2000 - Loss: -7.088
Iter 1984/2000 - Loss: -7.088
Iter 1985/2000 - Loss: -7.088
Iter 1986/2000 - Loss: -7.088
Iter 1987/2000 - Loss: -7.088
Iter 1988/2000 - Loss: -7.088
Iter 1989/2000 - Loss: -7.088
Iter 1990/2000 - Loss: -7.088
Iter 1991/2000 - Loss: -7.088
Iter 1992/2000 - Loss: -7.088
Iter 1993/2000 - Loss: -7.088
Iter 1994/2000 - Loss: -7.088
Iter 1995/2000 - Loss: -7.088
Iter 1996/2000 - Loss: -7.088
Iter 1997/2000 - Loss: -7.088
Iter 1998/2000 - Loss: -7.088
Iter 1999/2000 - Loss: -7.088
Iter 2000/2000 - Loss: -7.089
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[16.0862, 15.0807, 23.2862,  5.2458,  5.6027, 59.3478]],

        [[26.4291, 42.6029,  8.7142,  1.4210,  0.8502, 22.1995]],

        [[27.9846, 43.6042,  7.5889,  0.9523,  0.8629, 22.3545]],

        [[19.0287, 36.1463, 17.2823,  1.5481,  1.9410, 51.0449]]])
Signal Variance: tensor([0.2409, 1.4017, 9.7188, 0.5836])
Estimated target variance: tensor([ 0.0324,  1.1306, 11.4825,  0.1849])
N: 270
Signal to noise ratio: tensor([27.8852, 55.1806, 66.6018, 43.1154])
Bound on condition number: tensor([ 209948.0562,  822124.6095, 1197665.5569,  501914.1840])
Policy Optimizer learning rate:
0.09729780714786256
Experience 27, Iter 0, disc loss: 0.00024279581753569623, policy loss: 16.656615746025974
Experience 27, Iter 1, disc loss: 0.00031813466367675643, policy loss: 16.517823801594375
Experience 27, Iter 2, disc loss: 0.05144373746944928, policy loss: 16.118703085580776
Experience 27, Iter 3, disc loss: 0.0003407097569669482, policy loss: 16.7178895794917
Experience 27, Iter 4, disc loss: 0.00039908873634217595, policy loss: 15.034598087600436
Experience 27, Iter 5, disc loss: 0.00045554028240046444, policy loss: 15.9542911441434
Experience 27, Iter 6, disc loss: 0.0004388886928578533, policy loss: 17.536013924733233
Experience 27, Iter 7, disc loss: 0.0005105754748679405, policy loss: 15.673510481420152
Experience 27, Iter 8, disc loss: 0.0007220527622023779, policy loss: 16.276105725926726
Experience 27, Iter 9, disc loss: 0.0006259831701385847, policy loss: 16.797201124973704
Experience 27, Iter 10, disc loss: 0.0006425574508937427, policy loss: 16.261465906653534
Experience 27, Iter 11, disc loss: 0.000777019299759266, policy loss: 17.791388882674987
Experience 27, Iter 12, disc loss: 0.0007107863126418702, policy loss: 16.49624232528928
Experience 27, Iter 13, disc loss: 0.0012723889505585756, policy loss: 16.691993878083665
Experience 27, Iter 14, disc loss: 0.0007740007970839577, policy loss: 15.98387777024167
Experience 27, Iter 15, disc loss: 0.0007598938437041889, policy loss: 15.749446607348657
Experience 27, Iter 16, disc loss: 0.000761143903715264, policy loss: 16.793071537156763
Experience 27, Iter 17, disc loss: 0.0007577737498739237, policy loss: 17.02153107143946
Experience 27, Iter 18, disc loss: 0.0007620356571410303, policy loss: 17.268367783282322
Experience 27, Iter 19, disc loss: 0.0007402348965570467, policy loss: 16.223220684568986
Experience 27, Iter 20, disc loss: 0.0007354943976194407, policy loss: 16.392179807828924
Experience 27, Iter 21, disc loss: 0.0008003623831752567, policy loss: 16.919542689075527
Experience 27, Iter 22, disc loss: 0.0006843677695442203, policy loss: 17.232394483947584
Experience 27, Iter 23, disc loss: 0.0007022329519997528, policy loss: 18.280563549174722
Experience 27, Iter 24, disc loss: 0.0007241060483775066, policy loss: 15.539677449073228
Experience 27, Iter 25, disc loss: 0.0006382248885266607, policy loss: 17.40131172018441
Experience 27, Iter 26, disc loss: 0.0006293532655422026, policy loss: 18.497729439754277
Experience 27, Iter 27, disc loss: 0.0005974676361734281, policy loss: 16.116795972579425
Experience 27, Iter 28, disc loss: 0.0005752667251480222, policy loss: 16.612090844233087
Experience 27, Iter 29, disc loss: 0.0006511975112793437, policy loss: 16.684673535455506
Experience 27, Iter 30, disc loss: 0.0006599433528150418, policy loss: 16.75040678323331
Experience 27, Iter 31, disc loss: 0.0006878068944233402, policy loss: 16.90113173916819
Experience 27, Iter 32, disc loss: 0.000826619529714688, policy loss: 16.758955505879683
Experience 27, Iter 33, disc loss: 0.0009745261510774749, policy loss: 15.324772803968163
Experience 27, Iter 34, disc loss: 0.0007324411249402723, policy loss: 14.483722749915824
Experience 27, Iter 35, disc loss: 0.0005355224626904249, policy loss: 15.097299151061323
Experience 27, Iter 36, disc loss: 0.0005102555368758874, policy loss: 16.11142001028282
Experience 27, Iter 37, disc loss: 0.0004862003945764319, policy loss: 15.695316519358537
Experience 27, Iter 38, disc loss: 0.0005987684811696587, policy loss: 14.327000592440317
Experience 27, Iter 39, disc loss: 0.0004081140793287686, policy loss: 15.398416891158266
Experience 27, Iter 40, disc loss: 0.0010593674328603898, policy loss: 15.553644545810254
Experience 27, Iter 41, disc loss: 0.0005584392678745589, policy loss: 15.03198937374185
Experience 27, Iter 42, disc loss: 0.0004000024892700904, policy loss: 15.073010188575205
Experience 27, Iter 43, disc loss: 0.0004793276043808001, policy loss: 15.225654708850929
Experience 27, Iter 44, disc loss: 0.0006100028568496207, policy loss: 15.953473970626803
Experience 27, Iter 45, disc loss: 0.0003568573517803106, policy loss: 15.408393440244415
Experience 27, Iter 46, disc loss: 0.0003404782591800604, policy loss: 15.49850195492784
Experience 27, Iter 47, disc loss: 0.0003199039468576063, policy loss: 14.022961476794965
Experience 27, Iter 48, disc loss: 0.0007770554186724121, policy loss: 15.632486996634295
Experience 27, Iter 49, disc loss: 0.00045379970674984437, policy loss: 15.514972883935469
Experience 27, Iter 50, disc loss: 0.0003402854688509101, policy loss: 14.401527421673794
Experience 27, Iter 51, disc loss: 0.00042869379643647993, policy loss: 16.25698131748157
Experience 27, Iter 52, disc loss: 0.0003283019829389065, policy loss: 15.45365589750389
Experience 27, Iter 53, disc loss: 0.0004336698909220216, policy loss: 15.017008841249764
Experience 27, Iter 54, disc loss: 0.0012090436860155344, policy loss: 15.682039584450987
Experience 27, Iter 55, disc loss: 0.0003211480394149345, policy loss: 14.32715394141077
Experience 27, Iter 56, disc loss: 0.0002929031249553181, policy loss: 14.323922489037251
Experience 27, Iter 57, disc loss: 0.00026995213671478484, policy loss: 16.09152383035505
Experience 27, Iter 58, disc loss: 0.0010231297392915737, policy loss: 14.94516484614973
Experience 27, Iter 59, disc loss: 0.0004035168996990838, policy loss: 14.51481689354806
Experience 27, Iter 60, disc loss: 0.0006275778836990211, policy loss: 15.611208500707262
Experience 27, Iter 61, disc loss: 0.00023818666393003236, policy loss: 15.155425611690369
Experience 27, Iter 62, disc loss: 0.00027704156676996343, policy loss: 16.342597277788272
Experience 27, Iter 63, disc loss: 0.00033940183355718364, policy loss: 14.476729507124896
Experience 27, Iter 64, disc loss: 0.0036891256773827472, policy loss: 15.186638662360364
Experience 27, Iter 65, disc loss: 0.0008015588045381628, policy loss: 14.226659399977738
Experience 27, Iter 66, disc loss: 0.0003221551011210165, policy loss: 15.317580598485854
Experience 27, Iter 67, disc loss: 0.00024997330354716447, policy loss: 15.911329886273927
Experience 27, Iter 68, disc loss: 0.00031755882846495366, policy loss: 15.69512110190651
Experience 27, Iter 69, disc loss: 0.0003285431604895728, policy loss: 14.362248600139651
Experience 27, Iter 70, disc loss: 0.000345604103132553, policy loss: 16.028647528656222
Experience 27, Iter 71, disc loss: 0.00031243308652310225, policy loss: 13.799579849529312
Experience 27, Iter 72, disc loss: 0.0002730617968047947, policy loss: 15.054725103380965
Experience 27, Iter 73, disc loss: 0.07185882017986782, policy loss: 16.15676398361347
Experience 27, Iter 74, disc loss: 0.0002769514173267537, policy loss: 15.259413438268574
Experience 27, Iter 75, disc loss: 0.0003585714142292906, policy loss: 15.471826658358896
Experience 27, Iter 76, disc loss: 0.00032906507327766326, policy loss: 15.370629220011665
Experience 27, Iter 77, disc loss: 0.0005396882485912761, policy loss: 15.22315441314951
Experience 27, Iter 78, disc loss: 0.0004334681753521372, policy loss: 16.00635473828364
Experience 27, Iter 79, disc loss: 0.00044659576085007667, policy loss: 15.632282444409658
Experience 27, Iter 80, disc loss: 0.0004673972236884491, policy loss: 14.440901140237028
Experience 27, Iter 81, disc loss: 0.00041624679379807404, policy loss: 15.498281305500093
Experience 27, Iter 82, disc loss: 0.00041995929236490466, policy loss: 15.786814407858593
Experience 27, Iter 83, disc loss: 0.0007107494361268548, policy loss: 14.799875320048663
Experience 27, Iter 84, disc loss: 0.0005245622648076129, policy loss: 14.874887409739294
Experience 27, Iter 85, disc loss: 0.0006162168224059905, policy loss: 14.468400638443185
Experience 27, Iter 86, disc loss: 0.000507939067521635, policy loss: 17.349572498526634
Experience 27, Iter 87, disc loss: 0.0004860077493872006, policy loss: 15.507486844468207
Experience 27, Iter 88, disc loss: 0.0004928422999564895, policy loss: 15.417661578406161
Experience 27, Iter 89, disc loss: 0.0005171316168314475, policy loss: 15.33722855562732
Experience 27, Iter 90, disc loss: 0.00047250988923259503, policy loss: 15.277507631447968
Experience 27, Iter 91, disc loss: 0.0005761555813539501, policy loss: 15.109465067112424
Experience 27, Iter 92, disc loss: 0.0005251735587321211, policy loss: 14.42497614137524
Experience 27, Iter 93, disc loss: 0.0005988564986257446, policy loss: 14.610718830409166
Experience 27, Iter 94, disc loss: 0.0004383568352480225, policy loss: 15.747474930840681
Experience 27, Iter 95, disc loss: 0.00044359535465287106, policy loss: 14.823196111063808
Experience 27, Iter 96, disc loss: 0.00044582389472973814, policy loss: 16.082361505546146
Experience 27, Iter 97, disc loss: 0.0005729140540987698, policy loss: 14.46755656179971
Experience 27, Iter 98, disc loss: 0.0004288673231552502, policy loss: 16.33229712954931
Experience 27, Iter 99, disc loss: 0.00038756272652784335, policy loss: 15.154949713666458
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0080],
        [0.2803],
        [2.8401],
        [0.0465]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0543, 0.3442, 2.1345, 0.0394, 0.0102, 7.0111]],

        [[0.0543, 0.3442, 2.1345, 0.0394, 0.0102, 7.0111]],

        [[0.0543, 0.3442, 2.1345, 0.0394, 0.0102, 7.0111]],

        [[0.0543, 0.3442, 2.1345, 0.0394, 0.0102, 7.0111]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0322,  1.1211, 11.3606,  0.1860], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0322,  1.1211, 11.3606,  0.1860])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.305
Iter 2/2000 - Loss: 4.351
Iter 3/2000 - Loss: 4.194
Iter 4/2000 - Loss: 4.156
Iter 5/2000 - Loss: 4.134
Iter 6/2000 - Loss: 4.041
Iter 7/2000 - Loss: 3.933
Iter 8/2000 - Loss: 3.849
Iter 9/2000 - Loss: 3.773
Iter 10/2000 - Loss: 3.675
Iter 11/2000 - Loss: 3.549
Iter 12/2000 - Loss: 3.411
Iter 13/2000 - Loss: 3.269
Iter 14/2000 - Loss: 3.121
Iter 15/2000 - Loss: 2.962
Iter 16/2000 - Loss: 2.785
Iter 17/2000 - Loss: 2.588
Iter 18/2000 - Loss: 2.373
Iter 19/2000 - Loss: 2.146
Iter 20/2000 - Loss: 1.907
Iter 1981/2000 - Loss: -7.124
Iter 1982/2000 - Loss: -7.124
Iter 1983/2000 - Loss: -7.124
Iter 1984/2000 - Loss: -7.125
Iter 1985/2000 - Loss: -7.125
Iter 1986/2000 - Loss: -7.125
Iter 1987/2000 - Loss: -7.125
Iter 1988/2000 - Loss: -7.125
Iter 1989/2000 - Loss: -7.125
Iter 1990/2000 - Loss: -7.125
Iter 1991/2000 - Loss: -7.125
Iter 1992/2000 - Loss: -7.125
Iter 1993/2000 - Loss: -7.125
Iter 1994/2000 - Loss: -7.125
Iter 1995/2000 - Loss: -7.125
Iter 1996/2000 - Loss: -7.125
Iter 1997/2000 - Loss: -7.125
Iter 1998/2000 - Loss: -7.125
Iter 1999/2000 - Loss: -7.125
Iter 2000/2000 - Loss: -7.125
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[16.5692, 15.9386, 22.6417,  4.9505,  5.5635, 58.0457]],

        [[26.5567, 42.2076,  8.5579,  1.4044,  0.8657, 22.5112]],

        [[28.1466, 43.5952,  7.6861,  0.9522,  0.8310, 21.6040]],

        [[19.9655, 36.4502, 16.8899,  1.5245,  1.9041, 52.3651]]])
Signal Variance: tensor([0.2578, 1.3732, 9.0983, 0.5585])
Estimated target variance: tensor([ 0.0322,  1.1211, 11.3606,  0.1860])
N: 280
Signal to noise ratio: tensor([28.7152, 54.6247, 63.8302, 41.9314])
Bound on condition number: tensor([ 230878.1430,  835480.2810, 1140804.1753,  492309.6572])
Policy Optimizer learning rate:
0.09719534766192739
Experience 28, Iter 0, disc loss: 0.0006633996530596866, policy loss: 14.6251787538729
Experience 28, Iter 1, disc loss: 0.00042659889233746334, policy loss: 15.55209795835587
Experience 28, Iter 2, disc loss: 0.00036358630556358436, policy loss: 16.06934088495914
Experience 28, Iter 3, disc loss: 0.0008228264382408152, policy loss: 15.465117482455483
Experience 28, Iter 4, disc loss: 0.0004515574564797726, policy loss: 15.015861379262375
Experience 28, Iter 5, disc loss: 0.0003533223452418582, policy loss: 15.853541718379958
Experience 28, Iter 6, disc loss: 0.0003461729135759537, policy loss: 14.88547163637305
Experience 28, Iter 7, disc loss: 0.0004381142535543358, policy loss: 15.95715293405513
Experience 28, Iter 8, disc loss: 0.0003314909285369638, policy loss: 15.7330201887032
Experience 28, Iter 9, disc loss: 0.00031763573677948847, policy loss: 15.227569514055848
Experience 28, Iter 10, disc loss: 0.00039792384962630063, policy loss: 15.59762605076288
Experience 28, Iter 11, disc loss: 0.022823599071876595, policy loss: 13.886742763047597
Experience 28, Iter 12, disc loss: 0.0003470354714484633, policy loss: 14.873364535366939
Experience 28, Iter 13, disc loss: 0.0004263841043572218, policy loss: 14.841482543409736
Experience 28, Iter 14, disc loss: 0.0003918360481511837, policy loss: 15.890845204994102
Experience 28, Iter 15, disc loss: 0.0003885828804671295, policy loss: 16.341161895004586
Experience 28, Iter 16, disc loss: 0.000890740341596974, policy loss: 15.82337343545494
Experience 28, Iter 17, disc loss: 0.00061023914501614, policy loss: 15.799737324398937
Experience 28, Iter 18, disc loss: 0.001551416438303462, policy loss: 14.657079902082849
Experience 28, Iter 19, disc loss: 0.0005026826159192524, policy loss: 16.194371865637827
Experience 28, Iter 20, disc loss: 0.0004684893850232239, policy loss: 15.182827368996954
Experience 28, Iter 21, disc loss: 0.0004672970128683953, policy loss: 15.78256566911239
Experience 28, Iter 22, disc loss: 0.0004874443075461204, policy loss: 14.461342050054727
Experience 28, Iter 23, disc loss: 0.0005993134809154066, policy loss: 14.772379506488882
Experience 28, Iter 24, disc loss: 0.0005428917842477728, policy loss: 15.465029490113528
Experience 28, Iter 25, disc loss: 0.0006561227449764326, policy loss: 15.09598349508439
Experience 28, Iter 26, disc loss: 0.002620212173609147, policy loss: 14.962698848950271
Experience 28, Iter 27, disc loss: 0.000563853713630968, policy loss: 15.299297144823909
Experience 28, Iter 28, disc loss: 0.0005447332655853953, policy loss: 15.796664944796396
Experience 28, Iter 29, disc loss: 0.0004936735234210112, policy loss: 15.478916072681564
Experience 28, Iter 30, disc loss: 0.0005341706988155342, policy loss: 15.659250618999625
Experience 28, Iter 31, disc loss: 0.0005125705477681794, policy loss: 17.17393004020829
Experience 28, Iter 32, disc loss: 0.0006051972330592865, policy loss: 13.561911437097297
Experience 28, Iter 33, disc loss: 0.0008552601183153248, policy loss: 14.2511074342638
Experience 28, Iter 34, disc loss: 0.0005042936442216355, policy loss: 16.089729267175038
Experience 28, Iter 35, disc loss: 0.0004825239120125058, policy loss: 15.967379584751509
Experience 28, Iter 36, disc loss: 0.0004979765523117804, policy loss: 14.872253937450015
Experience 28, Iter 37, disc loss: 0.0005152924540919947, policy loss: 14.79515659299278
Experience 28, Iter 38, disc loss: 0.0005601464075223435, policy loss: 15.402497655330144
Experience 28, Iter 39, disc loss: 0.0025971408489512187, policy loss: 16.04086859217172
Experience 28, Iter 40, disc loss: 0.000773716493971765, policy loss: 15.634489712533982
Experience 28, Iter 41, disc loss: 0.0005291739283185076, policy loss: 15.554313854340167
Experience 28, Iter 42, disc loss: 0.0004973563483651982, policy loss: 15.440916290100656
Experience 28, Iter 43, disc loss: 0.0006192424288441645, policy loss: 15.031736065261374
Experience 28, Iter 44, disc loss: 0.0004963773324024479, policy loss: 15.185175041892666
Experience 28, Iter 45, disc loss: 0.0007495396331098478, policy loss: 16.431752962974766
Experience 28, Iter 46, disc loss: 0.0005011478246710376, policy loss: 14.987915953940472
Experience 28, Iter 47, disc loss: 0.006021295228252676, policy loss: 15.247925865579015
Experience 28, Iter 48, disc loss: 0.0004579042748921286, policy loss: 16.806360832931848
Experience 28, Iter 49, disc loss: 0.00040289394348444387, policy loss: 15.644367217800355
Experience 28, Iter 50, disc loss: 0.00045527203540312244, policy loss: 16.040152124230723
Experience 28, Iter 51, disc loss: 0.0004007672982567045, policy loss: 14.97221183515421
Experience 28, Iter 52, disc loss: 0.00041245403621816354, policy loss: 15.936548011634827
Experience 28, Iter 53, disc loss: 0.0005141196859804155, policy loss: 15.865183421858301
Experience 28, Iter 54, disc loss: 0.0004097993959225942, policy loss: 16.469656674978946
Experience 28, Iter 55, disc loss: 0.0008085701305687803, policy loss: 14.148957988426764
Experience 28, Iter 56, disc loss: 0.0003921430003538868, policy loss: 16.124117341142885
Experience 28, Iter 57, disc loss: 0.00047012042487457897, policy loss: 17.219520297335578
Experience 28, Iter 58, disc loss: 0.0005533667840343264, policy loss: 15.235628102551235
Experience 28, Iter 59, disc loss: 0.00040068873816994684, policy loss: 15.83759060088748
Experience 28, Iter 60, disc loss: 0.00037846324171575803, policy loss: 15.490951097429903
Experience 28, Iter 61, disc loss: 0.00038224368611120246, policy loss: 15.673176066182046
Experience 28, Iter 62, disc loss: 0.0006868037878297356, policy loss: 14.500013250045468
Experience 28, Iter 63, disc loss: 0.0005090044663736592, policy loss: 14.756764009670848
Experience 28, Iter 64, disc loss: 0.00036268804256579497, policy loss: 15.838863902644114
Experience 28, Iter 65, disc loss: 0.0009119095146914418, policy loss: 15.466178383568979
Experience 28, Iter 66, disc loss: 0.00035532046677620656, policy loss: 15.46882716589242
Experience 28, Iter 67, disc loss: 0.00032327893999035956, policy loss: 15.759941382663976
Experience 28, Iter 68, disc loss: 0.00041846035838055375, policy loss: 15.240029312684394
Experience 28, Iter 69, disc loss: 0.0004072431809706127, policy loss: 14.818103018288287
Experience 28, Iter 70, disc loss: 0.000387851962200949, policy loss: 17.62946134044967
Experience 28, Iter 71, disc loss: 0.00037106536141420966, policy loss: 15.263857658393617
Experience 28, Iter 72, disc loss: 0.0005296009536034636, policy loss: 15.728002006011161
Experience 28, Iter 73, disc loss: 0.0003124155347554311, policy loss: 16.108129099103298
Experience 28, Iter 74, disc loss: 0.000451805549598874, policy loss: 15.083109794396382
Experience 28, Iter 75, disc loss: 0.0003543061633039723, policy loss: 14.68445314086286
Experience 28, Iter 76, disc loss: 0.0002822321730865044, policy loss: 15.05443809530514
Experience 28, Iter 77, disc loss: 0.0002946763399361401, policy loss: 15.824108718227006
Experience 28, Iter 78, disc loss: 0.00038500412452567654, policy loss: 14.175663276114602
Experience 28, Iter 79, disc loss: 0.003066921759099304, policy loss: 15.601545461283973
Experience 28, Iter 80, disc loss: 0.0002818228483418161, policy loss: 16.79592514848855
Experience 28, Iter 81, disc loss: 0.00025888683894443573, policy loss: 15.773919387826536
Experience 28, Iter 82, disc loss: 0.0004035149771037443, policy loss: 15.491108751692796
Experience 28, Iter 83, disc loss: 0.00118996333609712, policy loss: 15.488152404831908
Experience 28, Iter 84, disc loss: 0.0006776151802156634, policy loss: 14.990456517521222
Experience 28, Iter 85, disc loss: 0.0003214141638970662, policy loss: 16.939282054963066
Experience 28, Iter 86, disc loss: 0.00032645818189031266, policy loss: 15.436339399957191
Experience 28, Iter 87, disc loss: 0.0002652478258551324, policy loss: 16.587825948346527
Experience 28, Iter 88, disc loss: 0.00028265495814009046, policy loss: 16.10425177433956
Experience 28, Iter 89, disc loss: 0.00026117851864704674, policy loss: 15.232726600117495
Experience 28, Iter 90, disc loss: 0.0012697036935754517, policy loss: 15.501803912875161
Experience 28, Iter 91, disc loss: 0.0007431284137891328, policy loss: 14.56955720673741
Experience 28, Iter 92, disc loss: 0.00036769571537669465, policy loss: 15.011059491475438
Experience 28, Iter 93, disc loss: 0.00030541922125199624, policy loss: 16.31869483371224
Experience 28, Iter 94, disc loss: 0.0003181311020098735, policy loss: 16.586711657040095
Experience 28, Iter 95, disc loss: 0.00031241592097064005, policy loss: 16.941226566562676
Experience 28, Iter 96, disc loss: 0.00035509299019610685, policy loss: 15.803480226786988
Experience 28, Iter 97, disc loss: 0.00037316756663062105, policy loss: 15.592937891313214
Experience 28, Iter 98, disc loss: 0.0003709467125720653, policy loss: 17.15875350712618
Experience 28, Iter 99, disc loss: 0.0002983020704555064, policy loss: 16.065008688777294
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0079],
        [0.2867],
        [2.9139],
        [0.0470]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0531, 0.3400, 2.1668, 0.0391, 0.0102, 7.0301]],

        [[0.0531, 0.3400, 2.1668, 0.0391, 0.0102, 7.0301]],

        [[0.0531, 0.3400, 2.1668, 0.0391, 0.0102, 7.0301]],

        [[0.0531, 0.3400, 2.1668, 0.0391, 0.0102, 7.0301]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0317,  1.1470, 11.6555,  0.1881], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0317,  1.1470, 11.6555,  0.1881])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.324
Iter 2/2000 - Loss: 4.371
Iter 3/2000 - Loss: 4.212
Iter 4/2000 - Loss: 4.174
Iter 5/2000 - Loss: 4.151
Iter 6/2000 - Loss: 4.055
Iter 7/2000 - Loss: 3.946
Iter 8/2000 - Loss: 3.862
Iter 9/2000 - Loss: 3.785
Iter 10/2000 - Loss: 3.685
Iter 11/2000 - Loss: 3.558
Iter 12/2000 - Loss: 3.417
Iter 13/2000 - Loss: 3.273
Iter 14/2000 - Loss: 3.125
Iter 15/2000 - Loss: 2.963
Iter 16/2000 - Loss: 2.783
Iter 17/2000 - Loss: 2.583
Iter 18/2000 - Loss: 2.366
Iter 19/2000 - Loss: 2.135
Iter 20/2000 - Loss: 1.893
Iter 1981/2000 - Loss: -7.162
Iter 1982/2000 - Loss: -7.162
Iter 1983/2000 - Loss: -7.162
Iter 1984/2000 - Loss: -7.162
Iter 1985/2000 - Loss: -7.162
Iter 1986/2000 - Loss: -7.162
Iter 1987/2000 - Loss: -7.162
Iter 1988/2000 - Loss: -7.162
Iter 1989/2000 - Loss: -7.162
Iter 1990/2000 - Loss: -7.162
Iter 1991/2000 - Loss: -7.162
Iter 1992/2000 - Loss: -7.162
Iter 1993/2000 - Loss: -7.162
Iter 1994/2000 - Loss: -7.163
Iter 1995/2000 - Loss: -7.163
Iter 1996/2000 - Loss: -7.163
Iter 1997/2000 - Loss: -7.163
Iter 1998/2000 - Loss: -7.163
Iter 1999/2000 - Loss: -7.163
Iter 2000/2000 - Loss: -7.163
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[16.3536, 15.9134, 23.6787,  4.6086,  5.6965, 57.6614]],

        [[26.2502, 41.7834,  8.5925,  1.3919,  0.8928, 21.9933]],

        [[28.0776, 43.5054,  7.6629,  0.9732,  0.8210, 21.0418]],

        [[19.5650, 36.1872, 16.2969,  1.5182,  1.8158, 51.2213]]])
Signal Variance: tensor([0.2548, 1.3474, 8.9012, 0.5342])
Estimated target variance: tensor([ 0.0317,  1.1470, 11.6555,  0.1881])
N: 290
Signal to noise ratio: tensor([28.3486, 54.1754, 62.6170, 41.0335])
Bound on condition number: tensor([ 233057.5057,  851144.9799, 1137057.8609,  488289.0945])
Policy Optimizer learning rate:
0.09709299607098561
Experience 29, Iter 0, disc loss: 0.0005108691785054005, policy loss: 16.507687085198686
Experience 29, Iter 1, disc loss: 0.0004977484111133297, policy loss: 16.144887217398477
Experience 29, Iter 2, disc loss: 0.00023780087815429853, policy loss: 15.432393608105992
Experience 29, Iter 3, disc loss: 0.0005291688886507435, policy loss: 16.051812193503796
Experience 29, Iter 4, disc loss: 0.00029845015367825287, policy loss: 15.880095378944143
Experience 29, Iter 5, disc loss: 0.0003296864778584809, policy loss: 16.26179218725644
Experience 29, Iter 6, disc loss: 0.0006768209125785759, policy loss: 16.413625173856012
Experience 29, Iter 7, disc loss: 0.0002983641795444782, policy loss: 16.721705965679885
Experience 29, Iter 8, disc loss: 0.0005535818654157189, policy loss: 15.572595801507362
Experience 29, Iter 9, disc loss: 0.0004027195967581682, policy loss: 15.891135730376817
Experience 29, Iter 10, disc loss: 0.00043792104679303816, policy loss: 17.23633198762554
Experience 29, Iter 11, disc loss: 0.001111486036485249, policy loss: 17.471297951703285
Experience 29, Iter 12, disc loss: 0.00030000064874062635, policy loss: 16.714915853035997
Experience 29, Iter 13, disc loss: 0.00021274307256551644, policy loss: 18.48028089741573
Experience 29, Iter 14, disc loss: 0.0002118697812126644, policy loss: 17.462260307591617
Experience 29, Iter 15, disc loss: 0.00031609513637907303, policy loss: 15.88942285926519
Experience 29, Iter 16, disc loss: 0.0014961163070206143, policy loss: 16.72173767617737
Experience 29, Iter 17, disc loss: 0.00021744297593875498, policy loss: 17.771343483266975
Experience 29, Iter 18, disc loss: 0.00033680356553702074, policy loss: 15.737087843247581
Experience 29, Iter 19, disc loss: 0.0008577060490684507, policy loss: 14.892952814237047
Experience 29, Iter 20, disc loss: 0.00021041150397134187, policy loss: 16.647079663203584
Experience 29, Iter 21, disc loss: 0.00078417629432175, policy loss: 16.2621543727762
Experience 29, Iter 22, disc loss: 0.000283194413410785, policy loss: 16.18733047151123
Experience 29, Iter 23, disc loss: 0.0004783118989340016, policy loss: 16.58765615548912
Experience 29, Iter 24, disc loss: 0.001216086134283409, policy loss: 16.501910919755716
Experience 29, Iter 25, disc loss: 0.0021554073001337866, policy loss: 16.480845561247637
Experience 29, Iter 26, disc loss: 0.00035575820627795616, policy loss: 16.452911271439778
Experience 29, Iter 27, disc loss: 0.0007830704507548159, policy loss: 16.362171252328583
Experience 29, Iter 28, disc loss: 0.00021346355952112737, policy loss: 16.973419655459153
Experience 29, Iter 29, disc loss: 0.0009642469087363646, policy loss: 17.564482704652498
Experience 29, Iter 30, disc loss: 0.0008455429479190152, policy loss: 17.604535455073208
Experience 29, Iter 31, disc loss: 0.00021143740086119553, policy loss: 18.499448657502843
Experience 29, Iter 32, disc loss: 0.0007674877848551029, policy loss: 16.408752652319038
Experience 29, Iter 33, disc loss: 0.00022894350685539005, policy loss: 17.231297047493626
Experience 29, Iter 34, disc loss: 0.00022166292458660394, policy loss: 17.14101617758808
Experience 29, Iter 35, disc loss: 0.00023118771070946792, policy loss: 16.433190514316923
Experience 29, Iter 36, disc loss: 0.0003467068992297423, policy loss: 16.399234338082003
Experience 29, Iter 37, disc loss: 0.00022117713749996692, policy loss: 17.31303100351654
Experience 29, Iter 38, disc loss: 0.000263965243610769, policy loss: 16.77258761123081
Experience 29, Iter 39, disc loss: 0.00022361509288516585, policy loss: 17.742564933571337
Experience 29, Iter 40, disc loss: 0.00021574829407253746, policy loss: 17.883579249497863
Experience 29, Iter 41, disc loss: 0.0005550853607712542, policy loss: 18.642985454388125
Experience 29, Iter 42, disc loss: 0.00043774749112684107, policy loss: 17.665986733526502
Experience 29, Iter 43, disc loss: 0.00022770852157986884, policy loss: 18.122032590317232
Experience 29, Iter 44, disc loss: 0.01725818389202178, policy loss: 17.974432057242566
Experience 29, Iter 45, disc loss: 0.00022505447967088974, policy loss: 18.158003754649084
Experience 29, Iter 46, disc loss: 0.00025706438748378095, policy loss: 16.852896762917766
Experience 29, Iter 47, disc loss: 0.0002816838595835386, policy loss: 17.726304500104906
Experience 29, Iter 48, disc loss: 0.0002585354200794781, policy loss: 18.824680996004115
Experience 29, Iter 49, disc loss: 0.00038321935470586295, policy loss: 18.039383413600195
Experience 29, Iter 50, disc loss: 0.0004868037899544551, policy loss: 17.48688588905319
Experience 29, Iter 51, disc loss: 0.0004144090465024372, policy loss: 16.57197792688291
Experience 29, Iter 52, disc loss: 0.00031151049008674417, policy loss: 18.54176823258996
Experience 29, Iter 53, disc loss: 0.0003748026442868099, policy loss: 16.05672012123598
Experience 29, Iter 54, disc loss: 0.00040117301237777394, policy loss: 17.120177863240986
Experience 29, Iter 55, disc loss: 0.00040188313162878086, policy loss: 18.087941855445802
Experience 29, Iter 56, disc loss: 0.0004360905375753289, policy loss: 17.040392356629383
Experience 29, Iter 57, disc loss: 0.0007758911290467732, policy loss: 18.18444296430521
Experience 29, Iter 58, disc loss: 0.00045124479604815107, policy loss: 16.693921894693883
Experience 29, Iter 59, disc loss: 0.0003458223844717728, policy loss: 17.363140159824805
Experience 29, Iter 60, disc loss: 0.0005365123360926785, policy loss: 16.224377973562774
Experience 29, Iter 61, disc loss: 0.0003293798763118945, policy loss: 18.42394001644347
Experience 29, Iter 62, disc loss: 0.0004874101512650675, policy loss: 19.056966679142576
Experience 29, Iter 63, disc loss: 0.00032173410907956337, policy loss: 18.40975024216936
Experience 29, Iter 64, disc loss: 0.00034643420298329934, policy loss: 16.091704924146
Experience 29, Iter 65, disc loss: 0.0003587866515695118, policy loss: 18.019487502138496
Experience 29, Iter 66, disc loss: 0.00033468455693571556, policy loss: 17.37489909965705
Experience 29, Iter 67, disc loss: 0.0003496844482777701, policy loss: 17.30315974815226
Experience 29, Iter 68, disc loss: 0.007993372019642002, policy loss: 16.82137880020895
Experience 29, Iter 69, disc loss: 0.0006502330868775961, policy loss: 15.897939989398811
Experience 29, Iter 70, disc loss: 0.00036056329686466593, policy loss: 17.46323517831018
Experience 29, Iter 71, disc loss: 0.0003831069208283354, policy loss: 17.233425972408895
Experience 29, Iter 72, disc loss: 0.00043892754991316934, policy loss: 17.189338424951806
Experience 29, Iter 73, disc loss: 0.00035094113859046805, policy loss: 16.562080817849665
Experience 29, Iter 74, disc loss: 0.00034648739457367503, policy loss: 17.11453788876525
Experience 29, Iter 75, disc loss: 0.0003418818949698752, policy loss: 17.13056528061969
Experience 29, Iter 76, disc loss: 0.0003371630674442855, policy loss: 17.981195733566814
Experience 29, Iter 77, disc loss: 0.0006486444346585339, policy loss: 18.349285582350408
Experience 29, Iter 78, disc loss: 0.0003681790004398329, policy loss: 16.99319610237665
Experience 29, Iter 79, disc loss: 0.0003448677558316564, policy loss: 16.231722801737966
Experience 29, Iter 80, disc loss: 0.00032955473671090856, policy loss: 18.56032847465791
Experience 29, Iter 81, disc loss: 0.00048134209421663336, policy loss: 16.46432380664885
Experience 29, Iter 82, disc loss: 0.0006632659608128802, policy loss: 16.93227565701455
Experience 29, Iter 83, disc loss: 0.0003675675773357373, policy loss: 16.795005346794248
Experience 29, Iter 84, disc loss: 0.0003387392538246695, policy loss: 17.801962901135933
Experience 29, Iter 85, disc loss: 0.000311844756061368, policy loss: 16.307603571007128
Experience 29, Iter 86, disc loss: 0.00044755678747347873, policy loss: 17.503216533446967
Experience 29, Iter 87, disc loss: 0.0003063824027790655, policy loss: 16.94420877983478
Experience 29, Iter 88, disc loss: 0.00039483154103812506, policy loss: 15.928311197902758
Experience 29, Iter 89, disc loss: 0.000473426308275288, policy loss: 16.963662244170614
Experience 29, Iter 90, disc loss: 0.00028454025970214684, policy loss: 16.475911350377707
Experience 29, Iter 91, disc loss: 0.0003642713446306622, policy loss: 17.270234630867844
Experience 29, Iter 92, disc loss: 0.0006826593966852029, policy loss: 17.648550588211336
Experience 29, Iter 93, disc loss: 0.0005198365634999921, policy loss: 17.62771920661471
Experience 29, Iter 94, disc loss: 0.0002633337804794162, policy loss: 16.51060041939042
Experience 29, Iter 95, disc loss: 0.00042561766543429717, policy loss: 16.478442963929428
Experience 29, Iter 96, disc loss: 0.0002605991012560529, policy loss: 16.41422928349084
Experience 29, Iter 97, disc loss: 0.0002691706557450201, policy loss: 17.225490208495184
Experience 29, Iter 98, disc loss: 0.0002554530025032392, policy loss: 17.719592275832053
Experience 29, Iter 99, disc loss: 0.00024434301912736424, policy loss: 17.098321937342483
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0077],
        [0.2920],
        [2.9774],
        [0.0462]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0514, 0.3320, 2.1456, 0.0383, 0.0100, 7.0353]],

        [[0.0514, 0.3320, 2.1456, 0.0383, 0.0100, 7.0353]],

        [[0.0514, 0.3320, 2.1456, 0.0383, 0.0100, 7.0353]],

        [[0.0514, 0.3320, 2.1456, 0.0383, 0.0100, 7.0353]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0308,  1.1680, 11.9095,  0.1849], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0308,  1.1680, 11.9095,  0.1849])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.324
Iter 2/2000 - Loss: 4.376
Iter 3/2000 - Loss: 4.213
Iter 4/2000 - Loss: 4.175
Iter 5/2000 - Loss: 4.153
Iter 6/2000 - Loss: 4.057
Iter 7/2000 - Loss: 3.945
Iter 8/2000 - Loss: 3.859
Iter 9/2000 - Loss: 3.781
Iter 10/2000 - Loss: 3.680
Iter 11/2000 - Loss: 3.550
Iter 12/2000 - Loss: 3.406
Iter 13/2000 - Loss: 3.259
Iter 14/2000 - Loss: 3.107
Iter 15/2000 - Loss: 2.944
Iter 16/2000 - Loss: 2.763
Iter 17/2000 - Loss: 2.561
Iter 18/2000 - Loss: 2.341
Iter 19/2000 - Loss: 2.107
Iter 20/2000 - Loss: 1.863
Iter 1981/2000 - Loss: -7.200
Iter 1982/2000 - Loss: -7.200
Iter 1983/2000 - Loss: -7.200
Iter 1984/2000 - Loss: -7.200
Iter 1985/2000 - Loss: -7.200
Iter 1986/2000 - Loss: -7.200
Iter 1987/2000 - Loss: -7.201
Iter 1988/2000 - Loss: -7.201
Iter 1989/2000 - Loss: -7.201
Iter 1990/2000 - Loss: -7.201
Iter 1991/2000 - Loss: -7.201
Iter 1992/2000 - Loss: -7.201
Iter 1993/2000 - Loss: -7.201
Iter 1994/2000 - Loss: -7.201
Iter 1995/2000 - Loss: -7.201
Iter 1996/2000 - Loss: -7.201
Iter 1997/2000 - Loss: -7.201
Iter 1998/2000 - Loss: -7.201
Iter 1999/2000 - Loss: -7.201
Iter 2000/2000 - Loss: -7.201
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[16.0253, 15.9112, 23.5006,  4.5651,  5.7047, 58.9478]],

        [[25.6952, 41.1316,  8.6087,  1.3764,  0.9067, 22.4883]],

        [[27.8811, 42.5920,  7.5047,  0.9505,  0.8380, 21.4925]],

        [[19.3589, 35.8536, 16.2306,  1.5063,  1.8442, 51.4761]]])
Signal Variance: tensor([0.2567, 1.4203, 9.0835, 0.5367])
Estimated target variance: tensor([ 0.0308,  1.1680, 11.9095,  0.1849])
N: 300
Signal to noise ratio: tensor([28.5311, 54.7704, 63.0590, 41.3024])
Bound on condition number: tensor([ 244207.8734,  899938.8816, 1192932.5911,  511767.3998])
Policy Optimizer learning rate:
0.09699075226141834
Experience 30, Iter 0, disc loss: 0.00030470951009199426, policy loss: 18.128495689770478
Experience 30, Iter 1, disc loss: 0.0003569384389889772, policy loss: 17.268002901266684
Experience 30, Iter 2, disc loss: 0.0004592652620025973, policy loss: 16.529500801669098
Experience 30, Iter 3, disc loss: 0.000248823251956529, policy loss: 17.52052157591183
Experience 30, Iter 4, disc loss: 0.00024489566374927843, policy loss: 16.53133699162408
Experience 30, Iter 5, disc loss: 0.004555831004339656, policy loss: 18.459146964024196
Experience 30, Iter 6, disc loss: 0.0002493512668942557, policy loss: 18.247417591448894
Experience 30, Iter 7, disc loss: 0.00023472063103994106, policy loss: 17.333298960671076
Experience 30, Iter 8, disc loss: 0.00029179768896559556, policy loss: 16.68674359829081
Experience 30, Iter 9, disc loss: 0.00022618140115251729, policy loss: 18.059051845039686
Experience 30, Iter 10, disc loss: 0.00022992444996877348, policy loss: 18.040631768303008
Experience 30, Iter 11, disc loss: 0.0002658905932644283, policy loss: 17.541093446950107
Experience 30, Iter 12, disc loss: 0.00026423849948687946, policy loss: 17.738638955518695
Experience 30, Iter 13, disc loss: 0.0006141364012994905, policy loss: 16.45788957084306
Experience 30, Iter 14, disc loss: 0.00038181854136621317, policy loss: 17.377964530094452
Experience 30, Iter 15, disc loss: 0.00022728659952157566, policy loss: 18.094303331005534
Experience 30, Iter 16, disc loss: 0.002459623373659995, policy loss: 16.26554059481355
Experience 30, Iter 17, disc loss: 0.00022638741368668168, policy loss: 16.976245333154658
Experience 30, Iter 18, disc loss: 0.0003193053512279383, policy loss: 17.067101576916826
Experience 30, Iter 19, disc loss: 0.0002916958457656406, policy loss: 17.19749023768361
Experience 30, Iter 20, disc loss: 0.0002562865069745447, policy loss: 17.188817742319333
Experience 30, Iter 21, disc loss: 0.00024305090510672104, policy loss: 15.42277860812129
Experience 30, Iter 22, disc loss: 0.00022522268585415295, policy loss: 17.86134231236504
Experience 30, Iter 23, disc loss: 0.021067809259065498, policy loss: 16.90254664527468
Experience 30, Iter 24, disc loss: 0.0002746942782787213, policy loss: 16.817647194670524
Experience 30, Iter 25, disc loss: 0.006695218852326726, policy loss: 17.399767010511624
Experience 30, Iter 26, disc loss: 0.00038047873067446105, policy loss: 17.0260248179325
Experience 30, Iter 27, disc loss: 0.0003167919833354879, policy loss: 18.546901218721295
Experience 30, Iter 28, disc loss: 0.0003432150565203118, policy loss: 17.7403140462072
Experience 30, Iter 29, disc loss: 0.00038622702325793833, policy loss: 17.307778496858322
Experience 30, Iter 30, disc loss: 0.00041303928790223756, policy loss: 17.069447138353183
Experience 30, Iter 31, disc loss: 0.0005129936376722091, policy loss: 17.46132773649026
Experience 30, Iter 32, disc loss: 0.000633804458282564, policy loss: 18.024838791113797
Experience 30, Iter 33, disc loss: 0.0004398789351995593, policy loss: 17.90115568025865
Experience 30, Iter 34, disc loss: 0.00046842363881110323, policy loss: 17.761998620389733
Experience 30, Iter 35, disc loss: 0.0004565417937380001, policy loss: 17.65385086202477
Experience 30, Iter 36, disc loss: 0.0004929412403833557, policy loss: 17.390230622563486
Experience 30, Iter 37, disc loss: 0.00045656332824862365, policy loss: 18.13456308343728
Experience 30, Iter 38, disc loss: 0.0007196550122842071, policy loss: 16.170802955452608
Experience 30, Iter 39, disc loss: 0.0006771087341872831, policy loss: 17.30240095791084
Experience 30, Iter 40, disc loss: 0.00047162607666297827, policy loss: 16.594815410939475
Experience 30, Iter 41, disc loss: 0.0005218873746192418, policy loss: 17.256525040102762
Experience 30, Iter 42, disc loss: 0.0004507233918743559, policy loss: 17.288919482070312
Experience 30, Iter 43, disc loss: 0.00043752603366854255, policy loss: 18.65110399788083
Experience 30, Iter 44, disc loss: 0.00043885297166313905, policy loss: 15.904454679062779
Experience 30, Iter 45, disc loss: 0.00046518043139405295, policy loss: 16.64519207638294
Experience 30, Iter 46, disc loss: 0.004762067186590552, policy loss: 18.185339224995467
Experience 30, Iter 47, disc loss: 0.0006391475706113041, policy loss: 16.69831267509675
Experience 30, Iter 48, disc loss: 0.0004732497583978768, policy loss: 17.924102135724485
Experience 30, Iter 49, disc loss: 0.00040951927704535, policy loss: 18.642853487806715
Experience 30, Iter 50, disc loss: 0.00040721420077812035, policy loss: 18.306700845303464
Experience 30, Iter 51, disc loss: 0.0004442028323749227, policy loss: 17.208052102784556
Experience 30, Iter 52, disc loss: 0.0007992252331739326, policy loss: 17.829827460051902
Experience 30, Iter 53, disc loss: 0.00040118878917332144, policy loss: 18.393670562335735
Experience 30, Iter 54, disc loss: 0.00040463588896304725, policy loss: 17.76043528636935
Experience 30, Iter 55, disc loss: 0.00039967549560342467, policy loss: 17.895838116692573
Experience 30, Iter 56, disc loss: 0.0005190502545800961, policy loss: 16.559747236296317
Experience 30, Iter 57, disc loss: 0.000579116446910497, policy loss: 16.89580722726157
Experience 30, Iter 58, disc loss: 0.0005615310974805164, policy loss: 16.290475847066013
Experience 30, Iter 59, disc loss: 0.0003560176049885882, policy loss: 18.105075374306338
Experience 30, Iter 60, disc loss: 0.00036277132512650483, policy loss: 19.93691913358326
Experience 30, Iter 61, disc loss: 0.0003702041103069209, policy loss: 17.46163490647721
Experience 30, Iter 62, disc loss: 0.0003298661804710429, policy loss: 17.7567843344967
Experience 30, Iter 63, disc loss: 0.00047664249126723983, policy loss: 17.140890722815055
Experience 30, Iter 64, disc loss: 0.00031789692406261963, policy loss: 17.06870105500783
Experience 30, Iter 65, disc loss: 0.0003805203344472575, policy loss: 17.59897504168147
Experience 30, Iter 66, disc loss: 0.00039834200696002537, policy loss: 16.71354351081549
Experience 30, Iter 67, disc loss: 0.0003617988599353496, policy loss: 16.44539093782291
Experience 30, Iter 68, disc loss: 0.00035195617769398925, policy loss: 17.501135443183554
Experience 30, Iter 69, disc loss: 0.0003026271572336771, policy loss: 16.50033406082989
Experience 30, Iter 70, disc loss: 0.00035906686727043554, policy loss: 17.362905744959303
Experience 30, Iter 71, disc loss: 0.00030680427242448253, policy loss: 16.853115750098603
Experience 30, Iter 72, disc loss: 0.0002584618047348138, policy loss: 19.010818664617045
Experience 30, Iter 73, disc loss: 0.00024280282757227188, policy loss: 17.536125888463175
Experience 30, Iter 74, disc loss: 0.00024955965941660796, policy loss: 16.797261869428098
Experience 30, Iter 75, disc loss: 0.00025478001123147866, policy loss: 17.961541229698867
Experience 30, Iter 76, disc loss: 0.0002778620384586668, policy loss: 16.945549018035496
Experience 30, Iter 77, disc loss: 0.0003136920018060668, policy loss: 17.3221256275653
Experience 30, Iter 78, disc loss: 0.0002242198120796461, policy loss: 17.61875647241709
Experience 30, Iter 79, disc loss: 0.0002525181078089719, policy loss: 15.772180689520244
Experience 30, Iter 80, disc loss: 0.0002760815387871092, policy loss: 17.025995941293527
Experience 30, Iter 81, disc loss: 0.0006445507838043986, policy loss: 16.939517074619012
Experience 30, Iter 82, disc loss: 0.0002751125308331545, policy loss: 16.988364327263888
Experience 30, Iter 83, disc loss: 0.0002763698572810928, policy loss: 18.479631918430556
Experience 30, Iter 84, disc loss: 0.00023500744449236595, policy loss: 16.93175193186167
Experience 30, Iter 85, disc loss: 0.00021454252814293934, policy loss: 16.927605197506324
Experience 30, Iter 86, disc loss: 0.000384472033454656, policy loss: 17.52346286132
Experience 30, Iter 87, disc loss: 0.0003775318174969598, policy loss: 16.573237749708586
Experience 30, Iter 88, disc loss: 0.00510371760457844, policy loss: 18.266636606821013
Experience 30, Iter 89, disc loss: 0.0002598322339198375, policy loss: 18.145101683063004
Experience 30, Iter 90, disc loss: 0.00024198213591201544, policy loss: 16.7391633605206
Experience 30, Iter 91, disc loss: 0.0001942799585159187, policy loss: 16.393945127149408
Experience 30, Iter 92, disc loss: 0.00020691264407658296, policy loss: 16.178823439914908
Experience 30, Iter 93, disc loss: 0.00019793517124050614, policy loss: 16.12920014495727
Experience 30, Iter 94, disc loss: 0.00020784274440108992, policy loss: 18.40516181956378
Experience 30, Iter 95, disc loss: 0.0002380787590926609, policy loss: 16.58632020567547
Experience 30, Iter 96, disc loss: 0.0002088095265215808, policy loss: 17.949688758928858
Experience 30, Iter 97, disc loss: 0.00020136020599608131, policy loss: 18.859997979176086
Experience 30, Iter 98, disc loss: 0.00025093638288660604, policy loss: 17.684299904450377
Experience 30, Iter 99, disc loss: 0.0001993058035717826, policy loss: 17.203116760795343
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0085],
        [0.2922],
        [2.9646],
        [0.0455]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0561, 0.3628, 2.1201, 0.0387, 0.0099, 7.0914]],

        [[0.0561, 0.3628, 2.1201, 0.0387, 0.0099, 7.0914]],

        [[0.0561, 0.3628, 2.1201, 0.0387, 0.0099, 7.0914]],

        [[0.0561, 0.3628, 2.1201, 0.0387, 0.0099, 7.0914]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0341,  1.1689, 11.8583,  0.1821], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0341,  1.1689, 11.8583,  0.1821])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.333
Iter 2/2000 - Loss: 4.387
Iter 3/2000 - Loss: 4.215
Iter 4/2000 - Loss: 4.170
Iter 5/2000 - Loss: 4.149
Iter 6/2000 - Loss: 4.055
Iter 7/2000 - Loss: 3.937
Iter 8/2000 - Loss: 3.838
Iter 9/2000 - Loss: 3.753
Iter 10/2000 - Loss: 3.654
Iter 11/2000 - Loss: 3.528
Iter 12/2000 - Loss: 3.383
Iter 13/2000 - Loss: 3.229
Iter 14/2000 - Loss: 3.069
Iter 15/2000 - Loss: 2.902
Iter 16/2000 - Loss: 2.721
Iter 17/2000 - Loss: 2.523
Iter 18/2000 - Loss: 2.305
Iter 19/2000 - Loss: 2.071
Iter 20/2000 - Loss: 1.823
Iter 1981/2000 - Loss: -7.214
Iter 1982/2000 - Loss: -7.214
Iter 1983/2000 - Loss: -7.214
Iter 1984/2000 - Loss: -7.214
Iter 1985/2000 - Loss: -7.214
Iter 1986/2000 - Loss: -7.214
Iter 1987/2000 - Loss: -7.214
Iter 1988/2000 - Loss: -7.214
Iter 1989/2000 - Loss: -7.214
Iter 1990/2000 - Loss: -7.214
Iter 1991/2000 - Loss: -7.214
Iter 1992/2000 - Loss: -7.214
Iter 1993/2000 - Loss: -7.215
Iter 1994/2000 - Loss: -7.215
Iter 1995/2000 - Loss: -7.215
Iter 1996/2000 - Loss: -7.215
Iter 1997/2000 - Loss: -7.215
Iter 1998/2000 - Loss: -7.215
Iter 1999/2000 - Loss: -7.215
Iter 2000/2000 - Loss: -7.215
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[16.8012, 16.3582, 23.5922,  4.8927,  5.7561, 58.3163]],

        [[26.3824, 42.6528,  8.5959,  1.3853,  0.8950, 21.9643]],

        [[29.1573, 46.3653,  7.4444,  0.9500,  0.8519, 21.8015]],

        [[19.6748, 36.1412, 15.8096,  1.4953,  1.8026, 51.1114]]])
Signal Variance: tensor([0.2624, 1.3738, 9.1399, 0.4980])
Estimated target variance: tensor([ 0.0341,  1.1689, 11.8583,  0.1821])
N: 310
Signal to noise ratio: tensor([28.6634, 53.7666, 61.4978, 39.4603])
Bound on condition number: tensor([ 254693.1766,  896164.4043, 1172414.7155,  482705.7530])
Policy Optimizer learning rate:
0.09688861611972639
Experience 31, Iter 0, disc loss: 0.00025300238568770877, policy loss: 17.898800153011212
Experience 31, Iter 1, disc loss: 0.0006247513047923545, policy loss: 15.372701961712853
Experience 31, Iter 2, disc loss: 0.00028379220540620447, policy loss: 17.498483524113624
Experience 31, Iter 3, disc loss: 0.00018789478692925516, policy loss: 17.931192113209516
Experience 31, Iter 4, disc loss: 0.0002346873345781702, policy loss: 17.788169311117546
Experience 31, Iter 5, disc loss: 0.05794142322256871, policy loss: 16.775563856501794
Experience 31, Iter 6, disc loss: 0.0004964953390116504, policy loss: 17.280382547842564
Experience 31, Iter 7, disc loss: 0.0004326202278709464, policy loss: 18.22393307929978
Experience 31, Iter 8, disc loss: 0.00041343594672069953, policy loss: 16.481912658350353
Experience 31, Iter 9, disc loss: 0.00030515538689942996, policy loss: 17.666218474669442
Experience 31, Iter 10, disc loss: 0.00033148432392374384, policy loss: 17.08661297868427
Experience 31, Iter 11, disc loss: 0.00038763466335164747, policy loss: 17.340420643712257
Experience 31, Iter 12, disc loss: 0.00045663877006684323, policy loss: 17.954050571732445
Experience 31, Iter 13, disc loss: 0.0005513259985740854, policy loss: 17.00774044444914
Experience 31, Iter 14, disc loss: 0.00042904700678384313, policy loss: 18.124744506960734
Experience 31, Iter 15, disc loss: 0.0004960329500096431, policy loss: 17.135593174617547
Experience 31, Iter 16, disc loss: 0.0005548871275400265, policy loss: 17.96340664357694
Experience 31, Iter 17, disc loss: 0.000497709752362593, policy loss: 17.283123142848535
Experience 31, Iter 18, disc loss: 0.00048264250807361777, policy loss: 16.848613469673097
Experience 31, Iter 19, disc loss: 0.0004944771912960429, policy loss: 17.761936183022897
Experience 31, Iter 20, disc loss: 0.0005182156426062723, policy loss: 17.704898590076247
Experience 31, Iter 21, disc loss: 0.0005103877801751334, policy loss: 19.61082094732768
Experience 31, Iter 22, disc loss: 0.00047607147789742037, policy loss: 19.28899309909978
Experience 31, Iter 23, disc loss: 0.0004886663366954245, policy loss: 16.919301651370237
Experience 31, Iter 24, disc loss: 0.0004846939430835738, policy loss: 17.06631454258151
Experience 31, Iter 25, disc loss: 0.000472135460147708, policy loss: 18.342095493604617
Experience 31, Iter 26, disc loss: 0.00044861529562713527, policy loss: 18.30439117408006
Experience 31, Iter 27, disc loss: 0.000590162473563704, policy loss: 17.819421642456042
Experience 31, Iter 28, disc loss: 0.00046948125210575375, policy loss: 18.626251209147675
Experience 31, Iter 29, disc loss: 0.0004371519034229458, policy loss: 17.553345888927502
Experience 31, Iter 30, disc loss: 0.00048604515118368845, policy loss: 19.728861390910183
Experience 31, Iter 31, disc loss: 0.00042557664645797785, policy loss: 18.278392158212526
Experience 31, Iter 32, disc loss: 0.0004215620970898746, policy loss: 16.465036859537097
Experience 31, Iter 33, disc loss: 0.000809639151273149, policy loss: 16.960016572616528
Experience 31, Iter 34, disc loss: 0.00040271184703000813, policy loss: 19.12667389211806
Experience 31, Iter 35, disc loss: 0.0004177596419325375, policy loss: 15.881529995857857
Experience 31, Iter 36, disc loss: 0.00035107255130793546, policy loss: 17.63971821289058
Experience 31, Iter 37, disc loss: 0.0003446953123822658, policy loss: 17.921988756625435
Experience 31, Iter 38, disc loss: 0.00032591005415191, policy loss: 18.76628521536334
Experience 31, Iter 39, disc loss: 0.00031598879820201833, policy loss: 18.067406953166795
Experience 31, Iter 40, disc loss: 0.0003199790022017846, policy loss: 17.384235267981968
Experience 31, Iter 41, disc loss: 0.0003071765826422265, policy loss: 18.049084423617167
Experience 31, Iter 42, disc loss: 0.009515714318868423, policy loss: 18.3682216554778
Experience 31, Iter 43, disc loss: 0.00037393708667622335, policy loss: 17.10029460630839
Experience 31, Iter 44, disc loss: 0.0003817356549902245, policy loss: 18.933895066624878
Experience 31, Iter 45, disc loss: 0.0003399505714856752, policy loss: 18.130781204867255
Experience 31, Iter 46, disc loss: 0.00036281045228868353, policy loss: 17.9623403458995
Experience 31, Iter 47, disc loss: 0.0003459699090279194, policy loss: 18.26408866623715
Experience 31, Iter 48, disc loss: 0.00040185147712446033, policy loss: 16.606483856986625
Experience 31, Iter 49, disc loss: 0.00035800272334451683, policy loss: 16.83188765053359
Experience 31, Iter 50, disc loss: 0.0005037214706898354, policy loss: 18.61650652214542
Experience 31, Iter 51, disc loss: 0.0004169324008660536, policy loss: 16.60501573762921
Experience 31, Iter 52, disc loss: 0.0003837420039702611, policy loss: 16.67867973024743
Experience 31, Iter 53, disc loss: 0.000415811448991253, policy loss: 17.237328047022544
Experience 31, Iter 54, disc loss: 0.0004348362440711436, policy loss: 17.481260396476788
Experience 31, Iter 55, disc loss: 0.0003660106111397091, policy loss: 17.93212298478921
Experience 31, Iter 56, disc loss: 0.044951614757099544, policy loss: 18.2841244374552
Experience 31, Iter 57, disc loss: 0.0005070931560192876, policy loss: 17.029601232475386
Experience 31, Iter 58, disc loss: 0.0004419115096411518, policy loss: 16.52911748509029
Experience 31, Iter 59, disc loss: 0.0004650648570799714, policy loss: 18.129560983917248
Experience 31, Iter 60, disc loss: 0.0005070267026410485, policy loss: 18.251679463048752
Experience 31, Iter 61, disc loss: 0.0005325762577426168, policy loss: 18.957147485777934
Experience 31, Iter 62, disc loss: 0.0005596325598850678, policy loss: 18.412056739355346
Experience 31, Iter 63, disc loss: 0.000593180900276044, policy loss: 17.71931578406856
Experience 31, Iter 64, disc loss: 0.000594613555024664, policy loss: 18.546148686510236
Experience 31, Iter 65, disc loss: 0.0006287564291452794, policy loss: 18.52768238602433
Experience 31, Iter 66, disc loss: 0.0006650568091822653, policy loss: 16.140850687312724
Experience 31, Iter 67, disc loss: 0.0020614996882803776, policy loss: 19.30023366857862
Experience 31, Iter 68, disc loss: 0.0006798553130070981, policy loss: 18.58173229107197
Experience 31, Iter 69, disc loss: 0.0006688707404552666, policy loss: 16.944319493552815
Experience 31, Iter 70, disc loss: 0.0006317627434638382, policy loss: 17.779877718807732
Experience 31, Iter 71, disc loss: 0.0007970072082755751, policy loss: 18.149762037813247
Experience 31, Iter 72, disc loss: 0.001965017202729317, policy loss: 18.31372851993155
Experience 31, Iter 73, disc loss: 0.0006434215010123463, policy loss: 16.503674163943753
Experience 31, Iter 74, disc loss: 0.0006270902306098516, policy loss: 18.398593291485433
Experience 31, Iter 75, disc loss: 0.0006065168940068994, policy loss: 17.245074324951958
Experience 31, Iter 76, disc loss: 0.0006746122098958963, policy loss: 16.904713532107145
Experience 31, Iter 77, disc loss: 0.0006094742901968182, policy loss: 18.212480791662344
Experience 31, Iter 78, disc loss: 0.0006250994914475243, policy loss: 16.08781036466169
Experience 31, Iter 79, disc loss: 0.0005829452362833751, policy loss: 18.050367415073495
Experience 31, Iter 80, disc loss: 0.0005442301146269842, policy loss: 16.799199864466893
Experience 31, Iter 81, disc loss: 0.0005240479215977014, policy loss: 17.912269124957945
Experience 31, Iter 82, disc loss: 0.0005924925865655232, policy loss: 19.25984774805635
Experience 31, Iter 83, disc loss: 0.000500049180030135, policy loss: 17.208228213523377
Experience 31, Iter 84, disc loss: 0.00045983144954028825, policy loss: 18.24887728117221
Experience 31, Iter 85, disc loss: 0.0004678402939931474, policy loss: 17.321112935389287
Experience 31, Iter 86, disc loss: 0.0005046888937935719, policy loss: 18.063666490861827
Experience 31, Iter 87, disc loss: 0.0004246301992425279, policy loss: 16.412293363719982
Experience 31, Iter 88, disc loss: 0.00040659946320351423, policy loss: 17.57086027778184
Experience 31, Iter 89, disc loss: 0.0005622020324095845, policy loss: 18.82453063321304
Experience 31, Iter 90, disc loss: 0.0003818046295276614, policy loss: 17.214319616568524
Experience 31, Iter 91, disc loss: 0.00036716919139692395, policy loss: 18.19935550629271
Experience 31, Iter 92, disc loss: 0.00035686557578592127, policy loss: 17.897755771862208
Experience 31, Iter 93, disc loss: 0.00036755429956827684, policy loss: 17.178042985659438
Experience 31, Iter 94, disc loss: 0.00033823804296098127, policy loss: 17.79719579760437
Experience 31, Iter 95, disc loss: 0.0003048413845060364, policy loss: 16.316702050410775
Experience 31, Iter 96, disc loss: 0.0003392163760316236, policy loss: 18.38627265950942
Experience 31, Iter 97, disc loss: 0.0002927506471613024, policy loss: 18.885616995459582
Experience 31, Iter 98, disc loss: 0.0009142892052752563, policy loss: 17.08018438905081
Experience 31, Iter 99, disc loss: 0.0004793817932460867, policy loss: 17.532705899301924
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0083],
        [0.2914],
        [2.9452],
        [0.0449]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0545, 0.3542, 2.0971, 0.0387, 0.0097, 7.0768]],

        [[0.0545, 0.3542, 2.0971, 0.0387, 0.0097, 7.0768]],

        [[0.0545, 0.3542, 2.0971, 0.0387, 0.0097, 7.0768]],

        [[0.0545, 0.3542, 2.0971, 0.0387, 0.0097, 7.0768]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0332,  1.1655, 11.7808,  0.1798], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0332,  1.1655, 11.7808,  0.1798])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.316
Iter 2/2000 - Loss: 4.378
Iter 3/2000 - Loss: 4.202
Iter 4/2000 - Loss: 4.158
Iter 5/2000 - Loss: 4.140
Iter 6/2000 - Loss: 4.046
Iter 7/2000 - Loss: 3.928
Iter 8/2000 - Loss: 3.829
Iter 9/2000 - Loss: 3.745
Iter 10/2000 - Loss: 3.648
Iter 11/2000 - Loss: 3.522
Iter 12/2000 - Loss: 3.377
Iter 13/2000 - Loss: 3.222
Iter 14/2000 - Loss: 3.063
Iter 15/2000 - Loss: 2.896
Iter 16/2000 - Loss: 2.716
Iter 17/2000 - Loss: 2.517
Iter 18/2000 - Loss: 2.299
Iter 19/2000 - Loss: 2.064
Iter 20/2000 - Loss: 1.814
Iter 1981/2000 - Loss: -7.287
Iter 1982/2000 - Loss: -7.287
Iter 1983/2000 - Loss: -7.287
Iter 1984/2000 - Loss: -7.287
Iter 1985/2000 - Loss: -7.287
Iter 1986/2000 - Loss: -7.287
Iter 1987/2000 - Loss: -7.287
Iter 1988/2000 - Loss: -7.287
Iter 1989/2000 - Loss: -7.287
Iter 1990/2000 - Loss: -7.287
Iter 1991/2000 - Loss: -7.287
Iter 1992/2000 - Loss: -7.287
Iter 1993/2000 - Loss: -7.287
Iter 1994/2000 - Loss: -7.287
Iter 1995/2000 - Loss: -7.287
Iter 1996/2000 - Loss: -7.287
Iter 1997/2000 - Loss: -7.287
Iter 1998/2000 - Loss: -7.288
Iter 1999/2000 - Loss: -7.288
Iter 2000/2000 - Loss: -7.288
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[16.5388, 16.6028, 23.4753,  4.8865,  5.8687, 58.0466]],

        [[26.3141, 42.4061,  8.5925,  1.3853,  0.8928, 22.0391]],

        [[28.9437, 45.8715,  7.4883,  0.9464,  0.8455, 22.0111]],

        [[19.2081, 35.7409, 16.2322,  1.5299,  1.8170, 50.8431]]])
Signal Variance: tensor([0.2673, 1.3704, 9.2823, 0.5143])
Estimated target variance: tensor([ 0.0332,  1.1655, 11.7808,  0.1798])
N: 320
Signal to noise ratio: tensor([29.1457, 54.5857, 62.6124, 40.1243])
Bound on condition number: tensor([ 271831.3886,  953474.2302, 1254502.6439,  515188.5504])
Policy Optimizer learning rate:
0.09678658753253007
Experience 32, Iter 0, disc loss: 0.0002902363728588867, policy loss: 16.048770792210064
Experience 32, Iter 1, disc loss: 0.0003072728513184544, policy loss: 17.87014039801229
Experience 32, Iter 2, disc loss: 0.0004673094345576587, policy loss: 18.218645366987364
Experience 32, Iter 3, disc loss: 0.0002506879069397924, policy loss: 19.079227352906948
Experience 32, Iter 4, disc loss: 0.000272485466204928, policy loss: 16.700704904963494
Experience 32, Iter 5, disc loss: 0.00025418737219423416, policy loss: 18.428974442182117
Experience 32, Iter 6, disc loss: 0.0002316711461280152, policy loss: 17.5538635783543
Experience 32, Iter 7, disc loss: 0.00022314984234519125, policy loss: 18.069511642053385
Experience 32, Iter 8, disc loss: 0.000533680522396206, policy loss: 17.288370624796297
Experience 32, Iter 9, disc loss: 0.0002267071904806443, policy loss: 17.504179744144455
Experience 32, Iter 10, disc loss: 0.00020557227029046452, policy loss: 16.706808600710772
Experience 32, Iter 11, disc loss: 0.0002050604927125521, policy loss: 17.94289300384805
Experience 32, Iter 12, disc loss: 0.00022411627425040947, policy loss: 16.761776193238486
Experience 32, Iter 13, disc loss: 0.0001906947000076208, policy loss: 19.975971578650736
Experience 32, Iter 14, disc loss: 0.00018698167002549414, policy loss: 17.994538473975418
Experience 32, Iter 15, disc loss: 0.00020217741632252992, policy loss: 18.471530615199562
Experience 32, Iter 16, disc loss: 0.0002760568097004708, policy loss: 19.54043035218487
Experience 32, Iter 17, disc loss: 0.00018291664858765668, policy loss: 17.906100504342554
Experience 32, Iter 18, disc loss: 0.00019101518094058153, policy loss: 16.64619834102087
Experience 32, Iter 19, disc loss: 0.0003823824927976475, policy loss: 18.637040046309615
Experience 32, Iter 20, disc loss: 0.0001914231601535713, policy loss: 17.421547975828474
Experience 32, Iter 21, disc loss: 0.0002227771986716364, policy loss: 18.86344000380456
Experience 32, Iter 22, disc loss: 0.0009823073051111902, policy loss: 17.22749447499372
Experience 32, Iter 23, disc loss: 0.0001853785895737151, policy loss: 18.392556058741224
Experience 32, Iter 24, disc loss: 0.0001929301811673696, policy loss: 18.462508981622847
Experience 32, Iter 25, disc loss: 0.00016509021818659187, policy loss: 17.61051312672769
Experience 32, Iter 26, disc loss: 0.00021428887111848988, policy loss: 16.030148306439308
Experience 32, Iter 27, disc loss: 0.00023492749114993882, policy loss: 18.38340862320443
Experience 32, Iter 28, disc loss: 0.00024245997926375702, policy loss: 16.843817951131577
Experience 32, Iter 29, disc loss: 0.0004210809162414282, policy loss: 17.588421306722424
Experience 32, Iter 30, disc loss: 0.00022502300833559662, policy loss: 17.156852454158624
Experience 32, Iter 31, disc loss: 0.00025426595192565293, policy loss: 16.666746800147298
Experience 32, Iter 32, disc loss: 0.0002715996464381436, policy loss: 17.344573308966627
Experience 32, Iter 33, disc loss: 0.00023248142491941583, policy loss: 17.276444619819376
Experience 32, Iter 34, disc loss: 0.0001451201421092424, policy loss: 18.07138409728909
Experience 32, Iter 35, disc loss: 0.00015149332572609187, policy loss: 18.63474755057299
Experience 32, Iter 36, disc loss: 0.00029433655983310584, policy loss: 18.244080397842364
Experience 32, Iter 37, disc loss: 0.00016061360540315975, policy loss: 17.899212387761054
Experience 32, Iter 38, disc loss: 0.00015986464741827097, policy loss: 17.297148467786926
Experience 32, Iter 39, disc loss: 0.00013616170264725287, policy loss: 19.332917851035877
Experience 32, Iter 40, disc loss: 0.00019913987767851182, policy loss: 17.450902994564185
Experience 32, Iter 41, disc loss: 0.0006082908618873098, policy loss: 19.349125344752384
Experience 32, Iter 42, disc loss: 0.00014482061707716398, policy loss: 17.14115117450738
Experience 32, Iter 43, disc loss: 0.00014707070109164482, policy loss: 16.785173461545995
Experience 32, Iter 44, disc loss: 0.00017026106418758607, policy loss: 17.453627746715664
Experience 32, Iter 45, disc loss: 0.03758331371597829, policy loss: 18.588887724446806
Experience 32, Iter 46, disc loss: 0.000328240522316478, policy loss: 17.23802925975655
Experience 32, Iter 47, disc loss: 0.00017813166734536752, policy loss: 20.27212994621251
Experience 32, Iter 48, disc loss: 0.00018842436912009674, policy loss: 17.60219291677019
Experience 32, Iter 49, disc loss: 0.00020430303825677662, policy loss: 17.930878768407112
Experience 32, Iter 50, disc loss: 0.00020979568040232235, policy loss: 18.806125704986933
Experience 32, Iter 51, disc loss: 0.000239977713121943, policy loss: 18.02840939642048
Experience 32, Iter 52, disc loss: 0.00024095141287181832, policy loss: 18.8966098621306
Experience 32, Iter 53, disc loss: 0.00027046552438497235, policy loss: 16.768485094304523
Experience 32, Iter 54, disc loss: 0.0003938548656363223, policy loss: 18.121701651045605
Experience 32, Iter 55, disc loss: 0.00031403663137010405, policy loss: 17.53880297226249
Experience 32, Iter 56, disc loss: 0.00030429229057179433, policy loss: 17.462257886205684
Experience 32, Iter 57, disc loss: 0.0004404555376696067, policy loss: 18.367555964358473
Experience 32, Iter 58, disc loss: 0.00032478118487795843, policy loss: 18.837371069594457
Experience 32, Iter 59, disc loss: 0.00034165174480905953, policy loss: 17.810775476188624
Experience 32, Iter 60, disc loss: 0.0004823867986195418, policy loss: 18.05915931253712
Experience 32, Iter 61, disc loss: 0.00032080391806433385, policy loss: 18.685153530636942
Experience 32, Iter 62, disc loss: 0.00033809842292455226, policy loss: 18.936287914353187
Experience 32, Iter 63, disc loss: 0.00031608550640345283, policy loss: 18.002187004547434
Experience 32, Iter 64, disc loss: 0.00040094278140683393, policy loss: 18.442234581913183
Experience 32, Iter 65, disc loss: 0.0003459697223412922, policy loss: 16.99114547245197
Experience 32, Iter 66, disc loss: 0.0004266725098609312, policy loss: 16.997915232664774
Experience 32, Iter 67, disc loss: 0.00038747289042279694, policy loss: 16.810362990570137
Experience 32, Iter 68, disc loss: 0.0003647676368877406, policy loss: 18.091653825146313
Experience 32, Iter 69, disc loss: 0.0003088034017159433, policy loss: 17.29884235895197
Experience 32, Iter 70, disc loss: 0.00035027849410928456, policy loss: 19.315211638759116
Experience 32, Iter 71, disc loss: 0.0003133834687496762, policy loss: 18.160762964538787
Experience 32, Iter 72, disc loss: 0.0006296058001779394, policy loss: 17.617931250757223
Experience 32, Iter 73, disc loss: 0.00029108295746833873, policy loss: 18.380626162683754
Experience 32, Iter 74, disc loss: 0.0002809001152494734, policy loss: 17.98458453607406
Experience 32, Iter 75, disc loss: 0.000283169066563817, policy loss: 18.56988090348604
Experience 32, Iter 76, disc loss: 0.0002779820350798065, policy loss: 18.13982834647651
Experience 32, Iter 77, disc loss: 0.0006798016269001419, policy loss: 16.180577191793695
Experience 32, Iter 78, disc loss: 0.0002573644472727219, policy loss: 17.750627915963182
Experience 32, Iter 79, disc loss: 0.0006212172433024386, policy loss: 16.663263835204358
Experience 32, Iter 80, disc loss: 0.00025045639463790653, policy loss: 17.658956566682903
Experience 32, Iter 81, disc loss: 0.0002406756567919885, policy loss: 19.23681484021071
Experience 32, Iter 82, disc loss: 0.0003447827131028606, policy loss: 16.732252895029966
Experience 32, Iter 83, disc loss: 0.00034526487686551164, policy loss: 17.53178714358981
Experience 32, Iter 84, disc loss: 0.00028079502723781695, policy loss: 17.621231076296727
Experience 32, Iter 85, disc loss: 0.0002840469082785662, policy loss: 17.314589416300404
Experience 32, Iter 86, disc loss: 0.0005710851704851128, policy loss: 16.883019686361067
Experience 32, Iter 87, disc loss: 0.00033330491883661414, policy loss: 17.334402435093672
Experience 32, Iter 88, disc loss: 0.00028715736676255426, policy loss: 17.903403097039785
Experience 32, Iter 89, disc loss: 0.00021732217399224134, policy loss: 18.89690643537691
Experience 32, Iter 90, disc loss: 0.0006824368022904437, policy loss: 17.21952183146947
Experience 32, Iter 91, disc loss: 0.00019971886435454227, policy loss: 18.11320584073021
Experience 32, Iter 92, disc loss: 0.0004737997351825863, policy loss: 17.550904902884533
Experience 32, Iter 93, disc loss: 0.0005030598922441129, policy loss: 18.427192078070505
Experience 32, Iter 94, disc loss: 0.00020207565164014356, policy loss: 16.279136798409155
Experience 32, Iter 95, disc loss: 0.0002037368044973943, policy loss: 19.633284870605063
Experience 32, Iter 96, disc loss: 0.0012714048216998582, policy loss: 18.927630898879656
Experience 32, Iter 97, disc loss: 0.0001815138531660551, policy loss: 17.75395659112935
Experience 32, Iter 98, disc loss: 0.00021768752335427207, policy loss: 18.431532409746183
Experience 32, Iter 99, disc loss: 0.00023223927009049887, policy loss: 18.471969781489527
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0082],
        [0.2935],
        [2.9613],
        [0.0442]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0538, 0.3499, 2.0706, 0.0384, 0.0095, 7.1034]],

        [[0.0538, 0.3499, 2.0706, 0.0384, 0.0095, 7.1034]],

        [[0.0538, 0.3499, 2.0706, 0.0384, 0.0095, 7.1034]],

        [[0.0538, 0.3499, 2.0706, 0.0384, 0.0095, 7.1034]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([ 0.0328,  1.1741, 11.8453,  0.1769], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([ 0.0328,  1.1741, 11.8453,  0.1769])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.296
Iter 2/2000 - Loss: 4.351
Iter 3/2000 - Loss: 4.180
Iter 4/2000 - Loss: 4.136
Iter 5/2000 - Loss: 4.116
Iter 6/2000 - Loss: 4.021
Iter 7/2000 - Loss: 3.905
Iter 8/2000 - Loss: 3.811
Iter 9/2000 - Loss: 3.729
Iter 10/2000 - Loss: 3.630
Iter 11/2000 - Loss: 3.504
Iter 12/2000 - Loss: 3.358
Iter 13/2000 - Loss: 3.206
Iter 14/2000 - Loss: 3.049
Iter 15/2000 - Loss: 2.884
Iter 16/2000 - Loss: 2.704
Iter 17/2000 - Loss: 2.504
Iter 18/2000 - Loss: 2.285
Iter 19/2000 - Loss: 2.050
Iter 20/2000 - Loss: 1.802
Iter 1981/2000 - Loss: -7.341
Iter 1982/2000 - Loss: -7.341
Iter 1983/2000 - Loss: -7.341
Iter 1984/2000 - Loss: -7.341
Iter 1985/2000 - Loss: -7.341
Iter 1986/2000 - Loss: -7.341
Iter 1987/2000 - Loss: -7.341
Iter 1988/2000 - Loss: -7.341
Iter 1989/2000 - Loss: -7.341
Iter 1990/2000 - Loss: -7.341
Iter 1991/2000 - Loss: -7.341
Iter 1992/2000 - Loss: -7.341
Iter 1993/2000 - Loss: -7.341
Iter 1994/2000 - Loss: -7.341
Iter 1995/2000 - Loss: -7.341
Iter 1996/2000 - Loss: -7.342
Iter 1997/2000 - Loss: -7.342
Iter 1998/2000 - Loss: -7.342
Iter 1999/2000 - Loss: -7.342
Iter 2000/2000 - Loss: -7.342
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0005],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[16.2825, 16.9258, 23.1672,  5.1198,  6.1346, 60.3447]],

        [[26.4907, 42.5231,  8.6410,  1.3673,  0.8959, 21.7281]],

        [[29.0560, 45.4959,  7.4596,  0.9451,  0.8466, 21.8244]],

        [[19.1333, 35.8505, 15.9229,  1.5559,  1.8145, 50.0984]]])
Signal Variance: tensor([0.2860, 1.3387, 9.1968, 0.5022])
Estimated target variance: tensor([ 0.0328,  1.1741, 11.8453,  0.1769])
N: 330
Signal to noise ratio: tensor([29.8560, 54.3380, 62.6137, 40.0172])
Bound on condition number: tensor([ 294156.3504,  974365.4831, 1293756.7536,  528456.0157])
Policy Optimizer learning rate:
0.09668466638656907
