Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0014],
        [0.0040],
        [0.2936],
        [0.0056]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0055, 0.0158, 1.1742, 0.0225], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0055, 0.0158, 1.1742, 0.0225])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.509
Iter 2/2000 - Loss: 0.086
Iter 3/2000 - Loss: -0.671
Iter 4/2000 - Loss: -0.370
Iter 5/2000 - Loss: -0.053
Iter 6/2000 - Loss: -0.061
Iter 7/2000 - Loss: -0.220
Iter 8/2000 - Loss: -0.350
Iter 9/2000 - Loss: -0.396
Iter 10/2000 - Loss: -0.409
Iter 11/2000 - Loss: -0.437
Iter 12/2000 - Loss: -0.486
Iter 13/2000 - Loss: -0.540
Iter 14/2000 - Loss: -0.591
Iter 15/2000 - Loss: -0.638
Iter 16/2000 - Loss: -0.676
Iter 17/2000 - Loss: -0.687
Iter 18/2000 - Loss: -0.662
Iter 19/2000 - Loss: -0.620
Iter 20/2000 - Loss: -0.598
Iter 1981/2000 - Loss: -1.022
Iter 1982/2000 - Loss: -1.022
Iter 1983/2000 - Loss: -1.022
Iter 1984/2000 - Loss: -1.022
Iter 1985/2000 - Loss: -1.022
Iter 1986/2000 - Loss: -1.022
Iter 1987/2000 - Loss: -1.022
Iter 1988/2000 - Loss: -1.022
Iter 1989/2000 - Loss: -1.022
Iter 1990/2000 - Loss: -1.022
Iter 1991/2000 - Loss: -1.022
Iter 1992/2000 - Loss: -1.022
Iter 1993/2000 - Loss: -1.022
Iter 1994/2000 - Loss: -1.022
Iter 1995/2000 - Loss: -1.022
Iter 1996/2000 - Loss: -1.022
Iter 1997/2000 - Loss: -1.022
Iter 1998/2000 - Loss: -1.022
Iter 1999/2000 - Loss: -1.022
Iter 2000/2000 - Loss: -1.022
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0028],
        [0.1740],
        [0.0040]])
Lengthscale: tensor([[[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]],

        [[0.0185, 0.0545, 0.2707, 0.0061, 0.0005, 0.0103]]])
Signal Variance: tensor([0.0040, 0.0114, 0.8828, 0.0162])
Estimated target variance: tensor([0.0055, 0.0158, 1.1742, 0.0225])
N: 10
Signal to noise ratio: tensor([2.0019, 2.0061, 2.2527, 2.0079])
Bound on condition number: tensor([41.0751, 41.2434, 51.7468, 41.3165])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.5241234960857604, policy loss: 0.7484900158923656
Experience 1, Iter 1, disc loss: 1.5092438434699538, policy loss: 0.7522217966841434
Experience 1, Iter 2, disc loss: 1.4978408398145855, policy loss: 0.7523585764319217
Experience 1, Iter 3, disc loss: 1.4845928434640714, policy loss: 0.7551811117763855
Experience 1, Iter 4, disc loss: 1.4748409790746395, policy loss: 0.754531925106394
Experience 1, Iter 5, disc loss: 1.4627245300304166, policy loss: 0.7570961198903327
Experience 1, Iter 6, disc loss: 1.4529714604653217, policy loss: 0.758961392237761
Experience 1, Iter 7, disc loss: 1.4484100701745177, policy loss: 0.7577982796573903
Experience 1, Iter 8, disc loss: 1.443501004118568, policy loss: 0.7589596035475821
Experience 1, Iter 9, disc loss: 1.4384916469258853, policy loss: 0.7607950205110237
Experience 1, Iter 10, disc loss: 1.434703244695077, policy loss: 0.7616486571600608
Experience 1, Iter 11, disc loss: 1.432166066567332, policy loss: 0.7606375892996913
Experience 1, Iter 12, disc loss: 1.4261425704507826, policy loss: 0.7635185463721901
Experience 1, Iter 13, disc loss: 1.4234588426382233, policy loss: 0.7625620161706037
Experience 1, Iter 14, disc loss: 1.4172231468629417, policy loss: 0.7648100498704444
Experience 1, Iter 15, disc loss: 1.4118239822012184, policy loss: 0.7657039448966126
Experience 1, Iter 16, disc loss: 1.4065252237190502, policy loss: 0.766451478671355
Experience 1, Iter 17, disc loss: 1.399424540278565, policy loss: 0.768906235278868
Experience 1, Iter 18, disc loss: 1.393112930801934, policy loss: 0.7706318467925009
Experience 1, Iter 19, disc loss: 1.3896274252853762, policy loss: 0.7688004020247319
Experience 1, Iter 20, disc loss: 1.3817490534512062, policy loss: 0.7720397984702826
Experience 1, Iter 21, disc loss: 1.3772124597889017, policy loss: 0.7714060217003184
Experience 1, Iter 22, disc loss: 1.3714759434203188, policy loss: 0.772080796104964
Experience 1, Iter 23, disc loss: 1.3650974416019837, policy loss: 0.7732239429262289
Experience 1, Iter 24, disc loss: 1.3583405034308913, policy loss: 0.7749569088572199
Experience 1, Iter 25, disc loss: 1.3530192812508677, policy loss: 0.7747473928190091
Experience 1, Iter 26, disc loss: 1.3455652912955425, policy loss: 0.7770756255773772
Experience 1, Iter 27, disc loss: 1.3368909979806833, policy loss: 0.78117726959231
Experience 1, Iter 28, disc loss: 1.3324243174542274, policy loss: 0.779765252571401
Experience 1, Iter 29, disc loss: 1.327495210832601, policy loss: 0.7789704294787256
Experience 1, Iter 30, disc loss: 1.3213717183879914, policy loss: 0.7795666955055822
Experience 1, Iter 31, disc loss: 1.3102052910510877, policy loss: 0.7864493226313044
Experience 1, Iter 32, disc loss: 1.3041176183873913, policy loss: 0.7865993956230707
Experience 1, Iter 33, disc loss: 1.297624432978954, policy loss: 0.787378410819294
Experience 1, Iter 34, disc loss: 1.2885632222721637, policy loss: 0.7914030657758886
Experience 1, Iter 35, disc loss: 1.2840238904152117, policy loss: 0.789527063914693
Experience 1, Iter 36, disc loss: 1.2765384310516417, policy loss: 0.791105976379272
Experience 1, Iter 37, disc loss: 1.2677876253639997, policy loss: 0.7944997099827057
Experience 1, Iter 38, disc loss: 1.2588717714990252, policy loss: 0.7981705339836771
Experience 1, Iter 39, disc loss: 1.2559398403639084, policy loss: 0.7928443547643989
Experience 1, Iter 40, disc loss: 1.244829261227116, policy loss: 0.797142919442893
Experience 1, Iter 41, disc loss: 1.2340459172295368, policy loss: 0.7999686686389298
Experience 1, Iter 42, disc loss: 1.2223324154960244, policy loss: 0.8033989191062193
Experience 1, Iter 43, disc loss: 1.2097502542752157, policy loss: 0.8071947904322064
Experience 1, Iter 44, disc loss: 1.199649899517376, policy loss: 0.8070899606902788
Experience 1, Iter 45, disc loss: 1.1888163342496725, policy loss: 0.8076477440972092
Experience 1, Iter 46, disc loss: 1.174357977779586, policy loss: 0.8129256867199721
Experience 1, Iter 47, disc loss: 1.161151195672614, policy loss: 0.8160438380901885
Experience 1, Iter 48, disc loss: 1.1512547753860405, policy loss: 0.8143441900895935
Experience 1, Iter 49, disc loss: 1.1321756225109907, policy loss: 0.8253832415300106
Experience 1, Iter 50, disc loss: 1.1218012296461946, policy loss: 0.8239629467444143
Experience 1, Iter 51, disc loss: 1.1072485498991247, policy loss: 0.8286612658058367
Experience 1, Iter 52, disc loss: 1.091298658014308, policy loss: 0.8349187263150213
Experience 1, Iter 53, disc loss: 1.080860156661681, policy loss: 0.834339722980034
Experience 1, Iter 54, disc loss: 1.0643953402637365, policy loss: 0.8414480581088586
Experience 1, Iter 55, disc loss: 1.0487051063082977, policy loss: 0.8477760857303738
Experience 1, Iter 56, disc loss: 1.0370958527868201, policy loss: 0.8477907624861359
Experience 1, Iter 57, disc loss: 1.022235333844393, policy loss: 0.8526515574532829
Experience 1, Iter 58, disc loss: 1.0145421511004646, policy loss: 0.8476859845183755
Experience 1, Iter 59, disc loss: 1.0014701441237586, policy loss: 0.8511632923054243
Experience 1, Iter 60, disc loss: 0.9772696616020595, policy loss: 0.8701209346548102
Experience 1, Iter 61, disc loss: 0.9698416563409714, policy loss: 0.8653156088040865
Experience 1, Iter 62, disc loss: 0.9494082673294281, policy loss: 0.879139941835317
Experience 1, Iter 63, disc loss: 0.9326156472428438, policy loss: 0.8893026014079752
Experience 1, Iter 64, disc loss: 0.9226291428430862, policy loss: 0.8887394641975475
Experience 1, Iter 65, disc loss: 0.9061788691041393, policy loss: 0.8974454768349103
Experience 1, Iter 66, disc loss: 0.8977128838437922, policy loss: 0.8949101689951152
Experience 1, Iter 67, disc loss: 0.8792358568305724, policy loss: 0.9088703697409113
Experience 1, Iter 68, disc loss: 0.8691232448936441, policy loss: 0.9090296301433443
Experience 1, Iter 69, disc loss: 0.8390987763865878, policy loss: 0.9440708347159663
Experience 1, Iter 70, disc loss: 0.8317056716690154, policy loss: 0.939899188865795
Experience 1, Iter 71, disc loss: 0.8272056018919374, policy loss: 0.9318460800733537
Experience 1, Iter 72, disc loss: 0.7901061360404984, policy loss: 0.9789468140848037
Experience 1, Iter 73, disc loss: 0.7880718724306746, policy loss: 0.9702675587006473
Experience 1, Iter 74, disc loss: 0.7679681665751327, policy loss: 0.9874617195571135
Experience 1, Iter 75, disc loss: 0.7487993027053148, policy loss: 1.0068646402723846
Experience 1, Iter 76, disc loss: 0.7325918190569265, policy loss: 1.0232640863258353
Experience 1, Iter 77, disc loss: 0.7172231275222104, policy loss: 1.0340049067136623
Experience 1, Iter 78, disc loss: 0.7045529676405211, policy loss: 1.0432810491234283
Experience 1, Iter 79, disc loss: 0.6790054111785727, policy loss: 1.0806233006024213
Experience 1, Iter 80, disc loss: 0.6742647518875656, policy loss: 1.0720884981771435
Experience 1, Iter 81, disc loss: 0.6569662929986568, policy loss: 1.092387126419559
Experience 1, Iter 82, disc loss: 0.6219324495669439, policy loss: 1.1511873991748582
Experience 1, Iter 83, disc loss: 0.6228320647881918, policy loss: 1.1320742806968265
Experience 1, Iter 84, disc loss: 0.599206433078747, policy loss: 1.1714032315017184
Experience 1, Iter 85, disc loss: 0.572245506427552, policy loss: 1.2165282146597527
Experience 1, Iter 86, disc loss: 0.568710360685748, policy loss: 1.2104336675131195
Experience 1, Iter 87, disc loss: 0.5493603992419556, policy loss: 1.2470357473738585
Experience 1, Iter 88, disc loss: 0.5185004580237706, policy loss: 1.312732698347432
Experience 1, Iter 89, disc loss: 0.5293228303886264, policy loss: 1.2701157376690597
Experience 1, Iter 90, disc loss: 0.5148972283423382, policy loss: 1.2937001395725494
Experience 1, Iter 91, disc loss: 0.486320295928316, policy loss: 1.3613096486963308
Experience 1, Iter 92, disc loss: 0.47860835876956853, policy loss: 1.3645848023024174
Experience 1, Iter 93, disc loss: 0.4666920786932274, policy loss: 1.3874841838479808
Experience 1, Iter 94, disc loss: 0.4499188144141, policy loss: 1.4270660626598253
Experience 1, Iter 95, disc loss: 0.44437900564316984, policy loss: 1.4269918551030547
Experience 1, Iter 96, disc loss: 0.415003343402116, policy loss: 1.4972073136890451
Experience 1, Iter 97, disc loss: 0.3934682611115707, policy loss: 1.5559658787405302
Experience 1, Iter 98, disc loss: 0.3973936484193431, policy loss: 1.535460732407386
Experience 1, Iter 99, disc loss: 0.36981524781844655, policy loss: 1.6254311766708813
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0021],
        [0.0040],
        [0.2280],
        [0.0043]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0082, 0.0160, 0.9122, 0.0170], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0082, 0.0160, 0.9122, 0.0170])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.138
Iter 2/2000 - Loss: 0.672
Iter 3/2000 - Loss: -0.527
Iter 4/2000 - Loss: -0.530
Iter 5/2000 - Loss: -0.218
Iter 6/2000 - Loss: -0.094
Iter 7/2000 - Loss: -0.121
Iter 8/2000 - Loss: -0.176
Iter 9/2000 - Loss: -0.210
Iter 10/2000 - Loss: -0.249
Iter 11/2000 - Loss: -0.315
Iter 12/2000 - Loss: -0.388
Iter 13/2000 - Loss: -0.443
Iter 14/2000 - Loss: -0.479
Iter 15/2000 - Loss: -0.524
Iter 16/2000 - Loss: -0.592
Iter 17/2000 - Loss: -0.662
Iter 18/2000 - Loss: -0.694
Iter 19/2000 - Loss: -0.673
Iter 20/2000 - Loss: -0.623
Iter 1981/2000 - Loss: -0.977
Iter 1982/2000 - Loss: -0.977
Iter 1983/2000 - Loss: -0.977
Iter 1984/2000 - Loss: -0.977
Iter 1985/2000 - Loss: -0.977
Iter 1986/2000 - Loss: -0.977
Iter 1987/2000 - Loss: -0.977
Iter 1988/2000 - Loss: -0.977
Iter 1989/2000 - Loss: -0.977
Iter 1990/2000 - Loss: -0.977
Iter 1991/2000 - Loss: -0.977
Iter 1992/2000 - Loss: -0.977
Iter 1993/2000 - Loss: -0.977
Iter 1994/2000 - Loss: -0.977
Iter 1995/2000 - Loss: -0.977
Iter 1996/2000 - Loss: -0.977
Iter 1997/2000 - Loss: -0.977
Iter 1998/2000 - Loss: -0.977
Iter 1999/2000 - Loss: -0.977
Iter 2000/2000 - Loss: -0.977
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0030],
        [0.1510],
        [0.0032]])
Lengthscale: tensor([[[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]],

        [[0.0260, 0.0874, 0.2051, 0.0047, 0.0005, 0.0466]]])
Signal Variance: tensor([0.0062, 0.0122, 0.7155, 0.0130])
Estimated target variance: tensor([0.0082, 0.0160, 0.9122, 0.0170])
N: 20
Signal to noise ratio: tensor([2.0029, 2.0066, 2.1766, 2.0060])
Bound on condition number: tensor([81.2348, 81.5283, 95.7520, 81.4845])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.3511456699230854, policy loss: 1.6664104662635975
Experience 2, Iter 1, disc loss: 0.32809958927628063, policy loss: 1.7423069265067364
Experience 2, Iter 2, disc loss: 0.31803195841177934, policy loss: 1.7917975029947797
Experience 2, Iter 3, disc loss: 0.2968891918249601, policy loss: 1.8536983785867098
Experience 2, Iter 4, disc loss: 0.31992604719880646, policy loss: 1.7192890436905437
Experience 2, Iter 5, disc loss: 0.2939233569195244, policy loss: 1.834755967151917
Experience 2, Iter 6, disc loss: 0.27703202072441946, policy loss: 1.9239442530312865
Experience 2, Iter 7, disc loss: 0.277594660853104, policy loss: 1.9019067136369279
Experience 2, Iter 8, disc loss: 0.25685278874829687, policy loss: 2.0037271869972137
Experience 2, Iter 9, disc loss: 0.26001937454492496, policy loss: 1.945768919266331
Experience 2, Iter 10, disc loss: 0.25234798350122645, policy loss: 2.0057714829380586
Experience 2, Iter 11, disc loss: 0.22723759074008776, policy loss: 2.125301675633364
Experience 2, Iter 12, disc loss: 0.22505310477612087, policy loss: 2.1121387430222125
Experience 2, Iter 13, disc loss: 0.21865587830549255, policy loss: 2.1540644415839765
Experience 2, Iter 14, disc loss: 0.1983157357533804, policy loss: 2.2805472270754814
Experience 2, Iter 15, disc loss: 0.18862889369395416, policy loss: 2.3301583111093516
Experience 2, Iter 16, disc loss: 0.1893655942175201, policy loss: 2.324328823975846
Experience 2, Iter 17, disc loss: 0.17635474726829586, policy loss: 2.4206496201768015
Experience 2, Iter 18, disc loss: 0.17353317306712662, policy loss: 2.432411146377515
Experience 2, Iter 19, disc loss: 0.16690718267738236, policy loss: 2.4476458160022814
Experience 2, Iter 20, disc loss: 0.16085797199134177, policy loss: 2.4796758457824923
Experience 2, Iter 21, disc loss: 0.15476069289525635, policy loss: 2.5417397792580605
Experience 2, Iter 22, disc loss: 0.15213467985132353, policy loss: 2.584436589937489
Experience 2, Iter 23, disc loss: 0.14039737211382886, policy loss: 2.702308111469514
Experience 2, Iter 24, disc loss: 0.14913811762123605, policy loss: 2.60572430445498
Experience 2, Iter 25, disc loss: 0.1347304051690649, policy loss: 2.7227049474015685
Experience 2, Iter 26, disc loss: 0.12895360781143, policy loss: 2.766385442200723
Experience 2, Iter 27, disc loss: 0.12959222440564364, policy loss: 2.715100383384919
Experience 2, Iter 28, disc loss: 0.12761837933795075, policy loss: 2.7473795528908482
Experience 2, Iter 29, disc loss: 0.11424497402296888, policy loss: 2.901713986328949
Experience 2, Iter 30, disc loss: 0.1202964813532076, policy loss: 2.807714295764571
Experience 2, Iter 31, disc loss: 0.11949337186232353, policy loss: 2.8001095819993127
Experience 2, Iter 32, disc loss: 0.11044844787643629, policy loss: 2.900506300960337
Experience 2, Iter 33, disc loss: 0.10756225946754942, policy loss: 2.9490834264828907
Experience 2, Iter 34, disc loss: 0.10331703683842255, policy loss: 2.9982849539755705
Experience 2, Iter 35, disc loss: 0.09977141271474503, policy loss: 3.007692204156167
Experience 2, Iter 36, disc loss: 0.08905772147848608, policy loss: 3.244908255445367
Experience 2, Iter 37, disc loss: 0.09140303518637842, policy loss: 3.1863013293244107
Experience 2, Iter 38, disc loss: 0.09298206201121129, policy loss: 3.182256551718578
Experience 2, Iter 39, disc loss: 0.0919323442631366, policy loss: 3.1178975542444416
Experience 2, Iter 40, disc loss: 0.0862779444220802, policy loss: 3.1946628151917054
Experience 2, Iter 41, disc loss: 0.09166329391641927, policy loss: 3.076478148519996
Experience 2, Iter 42, disc loss: 0.07907614576161759, policy loss: 3.3560353651998245
Experience 2, Iter 43, disc loss: 0.0715699811046133, policy loss: 3.5582558209533377
Experience 2, Iter 44, disc loss: 0.07352440131979668, policy loss: 3.4332745897949977
Experience 2, Iter 45, disc loss: 0.06504841964838523, policy loss: 3.6756526399682308
Experience 2, Iter 46, disc loss: 0.0697262385274839, policy loss: 3.4858823368537313
Experience 2, Iter 47, disc loss: 0.06773403424628696, policy loss: 3.548067858847464
Experience 2, Iter 48, disc loss: 0.06881110682282371, policy loss: 3.494512376804364
Experience 2, Iter 49, disc loss: 0.0706875839449334, policy loss: 3.416216090632529
Experience 2, Iter 50, disc loss: 0.0657132795817866, policy loss: 3.5073515327919753
Experience 2, Iter 51, disc loss: 0.059731969493653614, policy loss: 3.7051837109173187
Experience 2, Iter 52, disc loss: 0.05759689491368791, policy loss: 3.7776761756322643
Experience 2, Iter 53, disc loss: 0.06536178012661592, policy loss: 3.526220859891421
Experience 2, Iter 54, disc loss: 0.05730353091642938, policy loss: 3.7357302921315587
Experience 2, Iter 55, disc loss: 0.05882699453325188, policy loss: 3.731043003513746
Experience 2, Iter 56, disc loss: 0.05804490319554245, policy loss: 3.7612451986113653
Experience 2, Iter 57, disc loss: 0.051202178224518766, policy loss: 3.877581646747264
Experience 2, Iter 58, disc loss: 0.05246844966071336, policy loss: 3.877168254727498
Experience 2, Iter 59, disc loss: 0.05176333129081912, policy loss: 3.866657156887481
Experience 2, Iter 60, disc loss: 0.05199626377599752, policy loss: 3.8329795802994524
Experience 2, Iter 61, disc loss: 0.05162758847527839, policy loss: 3.838991491012776
Experience 2, Iter 62, disc loss: 0.05103909275391306, policy loss: 3.868632475682514
Experience 2, Iter 63, disc loss: 0.04627811639790875, policy loss: 3.9754721823697827
Experience 2, Iter 64, disc loss: 0.044877846262895954, policy loss: 4.03022441003823
Experience 2, Iter 65, disc loss: 0.046980882150824146, policy loss: 3.9662190588148745
Experience 2, Iter 66, disc loss: 0.0422991884629903, policy loss: 4.164480026356792
Experience 2, Iter 67, disc loss: 0.047141781080923345, policy loss: 4.047106158690266
Experience 2, Iter 68, disc loss: 0.04435697616512319, policy loss: 3.9703586377292757
Experience 2, Iter 69, disc loss: 0.03863957710015091, policy loss: 4.1780204977753055
Experience 2, Iter 70, disc loss: 0.039505092210738835, policy loss: 4.176832273344191
Experience 2, Iter 71, disc loss: 0.04205751869808935, policy loss: 4.136602258143537
Experience 2, Iter 72, disc loss: 0.03913214659112252, policy loss: 4.281654181625559
Experience 2, Iter 73, disc loss: 0.03603285711089513, policy loss: 4.347993092555303
Experience 2, Iter 74, disc loss: 0.037312718326527015, policy loss: 4.248168075221727
Experience 2, Iter 75, disc loss: 0.036933153726951384, policy loss: 4.297473503541125
Experience 2, Iter 76, disc loss: 0.033833910433718195, policy loss: 4.444440114924031
Experience 2, Iter 77, disc loss: 0.03566120820049659, policy loss: 4.265758679362696
Experience 2, Iter 78, disc loss: 0.03624791662189675, policy loss: 4.2213767462971585
Experience 2, Iter 79, disc loss: 0.032538749414194004, policy loss: 4.389890230426769
Experience 2, Iter 80, disc loss: 0.033387750515044126, policy loss: 4.368864179216599
Experience 2, Iter 81, disc loss: 0.03582193741081459, policy loss: 4.242976320990207
Experience 2, Iter 82, disc loss: 0.03284080416966754, policy loss: 4.445362655588961
Experience 2, Iter 83, disc loss: 0.0328499699944762, policy loss: 4.389660637929149
Experience 2, Iter 84, disc loss: 0.032946876979740906, policy loss: 4.403625434853886
Experience 2, Iter 85, disc loss: 0.03166838575271851, policy loss: 4.396697148673898
Experience 2, Iter 86, disc loss: 0.03230156413913488, policy loss: 4.4359979775994205
Experience 2, Iter 87, disc loss: 0.02979187011643876, policy loss: 4.471796559356695
Experience 2, Iter 88, disc loss: 0.0289348207976065, policy loss: 4.601657049103013
Experience 2, Iter 89, disc loss: 0.032080245002545624, policy loss: 4.413762297800439
Experience 2, Iter 90, disc loss: 0.025004918095814725, policy loss: 4.861315754960913
Experience 2, Iter 91, disc loss: 0.03165827236615955, policy loss: 4.398513270532965
Experience 2, Iter 92, disc loss: 0.03039110596068434, policy loss: 4.422128491289664
Experience 2, Iter 93, disc loss: 0.02661565623712405, policy loss: 4.611258861449018
Experience 2, Iter 94, disc loss: 0.02767746235018557, policy loss: 4.5290744345321166
Experience 2, Iter 95, disc loss: 0.024718214646124323, policy loss: 4.744995665696685
Experience 2, Iter 96, disc loss: 0.02712692324509315, policy loss: 4.620867736776347
Experience 2, Iter 97, disc loss: 0.024237662379753444, policy loss: 4.712837844176497
Experience 2, Iter 98, disc loss: 0.023542874286775085, policy loss: 4.842654296291521
Experience 2, Iter 99, disc loss: 0.025342769878438975, policy loss: 4.760315391654361
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.0076],
        [0.3724],
        [0.0070]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0103, 0.0306, 1.4897, 0.0279], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0103, 0.0306, 1.4897, 0.0279])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.284
Iter 2/2000 - Loss: 1.377
Iter 3/2000 - Loss: 0.424
Iter 4/2000 - Loss: 0.360
Iter 5/2000 - Loss: 0.572
Iter 6/2000 - Loss: 0.663
Iter 7/2000 - Loss: 0.640
Iter 8/2000 - Loss: 0.596
Iter 9/2000 - Loss: 0.571
Iter 10/2000 - Loss: 0.547
Iter 11/2000 - Loss: 0.513
Iter 12/2000 - Loss: 0.484
Iter 13/2000 - Loss: 0.470
Iter 14/2000 - Loss: 0.461
Iter 15/2000 - Loss: 0.432
Iter 16/2000 - Loss: 0.371
Iter 17/2000 - Loss: 0.291
Iter 18/2000 - Loss: 0.222
Iter 19/2000 - Loss: 0.190
Iter 20/2000 - Loss: 0.195
Iter 1981/2000 - Loss: -0.014
Iter 1982/2000 - Loss: -0.014
Iter 1983/2000 - Loss: -0.014
Iter 1984/2000 - Loss: -0.014
Iter 1985/2000 - Loss: -0.014
Iter 1986/2000 - Loss: -0.014
Iter 1987/2000 - Loss: -0.014
Iter 1988/2000 - Loss: -0.014
Iter 1989/2000 - Loss: -0.014
Iter 1990/2000 - Loss: -0.014
Iter 1991/2000 - Loss: -0.014
Iter 1992/2000 - Loss: -0.014
Iter 1993/2000 - Loss: -0.014
Iter 1994/2000 - Loss: -0.014
Iter 1995/2000 - Loss: -0.014
Iter 1996/2000 - Loss: -0.014
Iter 1997/2000 - Loss: -0.014
Iter 1998/2000 - Loss: -0.014
Iter 1999/2000 - Loss: -0.014
Iter 2000/2000 - Loss: -0.014
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0020],
        [0.0059],
        [0.2368],
        [0.0054]])
Lengthscale: tensor([[[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]],

        [[0.0314, 0.1139, 0.3183, 0.0069, 0.0012, 0.0916]]])
Signal Variance: tensor([0.0080, 0.0237, 1.2038, 0.0217])
Estimated target variance: tensor([0.0103, 0.0306, 1.4897, 0.0279])
N: 30
Signal to noise ratio: tensor([2.0037, 2.0119, 2.2549, 2.0094])
Bound on condition number: tensor([121.4416, 122.4350, 153.5333, 122.1342])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.021362220691123136, policy loss: 5.4337782265493555
Experience 3, Iter 1, disc loss: 0.019376027243216618, policy loss: 5.437290387388965
Experience 3, Iter 2, disc loss: 0.019819706078124527, policy loss: 5.3926676092835315
Experience 3, Iter 3, disc loss: 0.018294610451315897, policy loss: 5.541339934344339
Experience 3, Iter 4, disc loss: 0.022034368518923632, policy loss: 5.296822763698713
Experience 3, Iter 5, disc loss: 0.01750479980557686, policy loss: 5.634336629386216
Experience 3, Iter 6, disc loss: 0.018634052932808027, policy loss: 5.6603376783355905
Experience 3, Iter 7, disc loss: 0.017202118889481713, policy loss: 5.72604014003449
Experience 3, Iter 8, disc loss: 0.01877205060876379, policy loss: 5.351097774099595
Experience 3, Iter 9, disc loss: 0.018584662179681043, policy loss: 5.522042528646958
Experience 3, Iter 10, disc loss: 0.01912337150326359, policy loss: 5.35361849470144
Experience 3, Iter 11, disc loss: 0.018640213088583893, policy loss: 5.555860298063925
Experience 3, Iter 12, disc loss: 0.01662937655041405, policy loss: 5.529581711001824
Experience 3, Iter 13, disc loss: 0.01684338348936298, policy loss: 5.5740122954198865
Experience 3, Iter 14, disc loss: 0.01534253470377153, policy loss: 5.807781066250224
Experience 3, Iter 15, disc loss: 0.01757165901094973, policy loss: 5.409128039776181
Experience 3, Iter 16, disc loss: 0.016993714494755428, policy loss: 5.663273090595142
Experience 3, Iter 17, disc loss: 0.016021962250432505, policy loss: 5.541408088818894
Experience 3, Iter 18, disc loss: 0.01841189169007735, policy loss: 5.450324336991166
Experience 3, Iter 19, disc loss: 0.01635199544226327, policy loss: 5.5525001755056715
Experience 3, Iter 20, disc loss: 0.014897307986831358, policy loss: 5.654319822914708
Experience 3, Iter 21, disc loss: 0.015787210694862535, policy loss: 5.564308563330712
Experience 3, Iter 22, disc loss: 0.017663226013187757, policy loss: 5.4613418894354515
Experience 3, Iter 23, disc loss: 0.01533660136027442, policy loss: 5.690676787350042
Experience 3, Iter 24, disc loss: 0.016012034999472534, policy loss: 5.664887857928978
Experience 3, Iter 25, disc loss: 0.01437461772852935, policy loss: 5.758510103686932
Experience 3, Iter 26, disc loss: 0.014505593753356496, policy loss: 5.6390157827478244
Experience 3, Iter 27, disc loss: 0.014680397753915842, policy loss: 5.771115475944704
Experience 3, Iter 28, disc loss: 0.014291914387125074, policy loss: 5.701310927715206
Experience 3, Iter 29, disc loss: 0.014586364558417799, policy loss: 5.773495111712291
Experience 3, Iter 30, disc loss: 0.020191775838186012, policy loss: 5.375186025075903
Experience 3, Iter 31, disc loss: 0.014375743701971276, policy loss: 5.767200673894788
Experience 3, Iter 32, disc loss: 0.013665145277449782, policy loss: 5.929045195664635
Experience 3, Iter 33, disc loss: 0.014539663759602629, policy loss: 5.706152744379378
Experience 3, Iter 34, disc loss: 0.012360101659984496, policy loss: 6.096586222041227
Experience 3, Iter 35, disc loss: 0.013988537381871998, policy loss: 5.736237425368486
Experience 3, Iter 36, disc loss: 0.012764934457066083, policy loss: 5.8834820709256315
Experience 3, Iter 37, disc loss: 0.012857721356469577, policy loss: 5.92977757915012
Experience 3, Iter 38, disc loss: 0.013090932317892327, policy loss: 5.926378743800526
Experience 3, Iter 39, disc loss: 0.013674620517048798, policy loss: 5.5052039514810795
Experience 3, Iter 40, disc loss: 0.011871246474982566, policy loss: 5.996239341786336
Experience 3, Iter 41, disc loss: 0.01343867037824024, policy loss: 5.787393988150011
Experience 3, Iter 42, disc loss: 0.012190771657839012, policy loss: 6.064797076389698
Experience 3, Iter 43, disc loss: 0.015252766875979566, policy loss: 5.472684334111673
Experience 3, Iter 44, disc loss: 0.013536171541369574, policy loss: 6.050257832546579
Experience 3, Iter 45, disc loss: 0.011130924848345929, policy loss: 6.214100195304727
Experience 3, Iter 46, disc loss: 0.016770186514528412, policy loss: 5.741507567816247
Experience 3, Iter 47, disc loss: 0.011582867053306349, policy loss: 6.073457909610046
Experience 3, Iter 48, disc loss: 0.014494269213121833, policy loss: 5.682016892441097
Experience 3, Iter 49, disc loss: 0.010876804666515222, policy loss: 6.118844763530685
Experience 3, Iter 50, disc loss: 0.01151148970632086, policy loss: 6.023177861518427
Experience 3, Iter 51, disc loss: 0.011074240482867558, policy loss: 6.118074379993044
Experience 3, Iter 52, disc loss: 0.013336229092681447, policy loss: 5.93161551850447
Experience 3, Iter 53, disc loss: 0.014423609900324184, policy loss: 6.00064135214955
Experience 3, Iter 54, disc loss: 0.014176457808581362, policy loss: 5.8777360898491215
Experience 3, Iter 55, disc loss: 0.011455279973800636, policy loss: 6.052911684867231
Experience 3, Iter 56, disc loss: 0.011369857125376642, policy loss: 5.970598537235615
Experience 3, Iter 57, disc loss: 0.011105918603889655, policy loss: 6.001603003045423
Experience 3, Iter 58, disc loss: 0.010916571305571197, policy loss: 6.30991109957526
Experience 3, Iter 59, disc loss: 0.011414835613661278, policy loss: 6.217541500302149
Experience 3, Iter 60, disc loss: 0.010287357935837758, policy loss: 6.200287895477891
Experience 3, Iter 61, disc loss: 0.010814809124094127, policy loss: 6.181478665627486
Experience 3, Iter 62, disc loss: 0.010885441037461703, policy loss: 6.059431850167064
Experience 3, Iter 63, disc loss: 0.012185375564186415, policy loss: 5.946745430136764
Experience 3, Iter 64, disc loss: 0.01094882620402043, policy loss: 6.0242529454376745
Experience 3, Iter 65, disc loss: 0.009660394413804334, policy loss: 6.21525779721745
Experience 3, Iter 66, disc loss: 0.009810217597885482, policy loss: 6.336949974480607
Experience 3, Iter 67, disc loss: 0.009525632163540979, policy loss: 6.236805227226377
Experience 3, Iter 68, disc loss: 0.010510268352062832, policy loss: 6.224584120850189
Experience 3, Iter 69, disc loss: 0.008757140248425908, policy loss: 6.4185957077182385
Experience 3, Iter 70, disc loss: 0.010306075329232497, policy loss: 6.152250167206247
Experience 3, Iter 71, disc loss: 0.009311229925673075, policy loss: 6.325789945537485
Experience 3, Iter 72, disc loss: 0.009388973119009892, policy loss: 6.29753666454024
Experience 3, Iter 73, disc loss: 0.008881409439463386, policy loss: 6.463453308829002
Experience 3, Iter 74, disc loss: 0.008993900627907309, policy loss: 6.253159346849494
Experience 3, Iter 75, disc loss: 0.008994082362646195, policy loss: 6.374728320548851
Experience 3, Iter 76, disc loss: 0.010389790222967675, policy loss: 5.879225205221587
Experience 3, Iter 77, disc loss: 0.011102078209778696, policy loss: 5.946066306222425
Experience 3, Iter 78, disc loss: 0.008395506739013141, policy loss: 6.421070225522922
Experience 3, Iter 79, disc loss: 0.009372932891530036, policy loss: 6.243106011891093
Experience 3, Iter 80, disc loss: 0.009668868360052714, policy loss: 6.204126967472163
Experience 3, Iter 81, disc loss: 0.008909731201056575, policy loss: 6.468255325638996
Experience 3, Iter 82, disc loss: 0.008861637266648612, policy loss: 6.2523072193288876
Experience 3, Iter 83, disc loss: 0.008090870079806935, policy loss: 6.344343821414496
Experience 3, Iter 84, disc loss: 0.008477947297667345, policy loss: 6.43279496765693
Experience 3, Iter 85, disc loss: 0.009520891918301027, policy loss: 6.2954774955164785
Experience 3, Iter 86, disc loss: 0.00958912429903811, policy loss: 6.614288765748112
Experience 3, Iter 87, disc loss: 0.008151942171331404, policy loss: 6.465277117947624
Experience 3, Iter 88, disc loss: 0.008865578503760768, policy loss: 6.3852531078966015
Experience 3, Iter 89, disc loss: 0.008183212169668549, policy loss: 6.46915118012878
Experience 3, Iter 90, disc loss: 0.007749755838390595, policy loss: 6.682406184533809
Experience 3, Iter 91, disc loss: 0.007770892300601792, policy loss: 6.449328271422054
Experience 3, Iter 92, disc loss: 0.010400668451010234, policy loss: 6.410012394569052
Experience 3, Iter 93, disc loss: 0.007811030548375127, policy loss: 6.452629772435953
Experience 3, Iter 94, disc loss: 0.007382931003207908, policy loss: 6.647950160681038
Experience 3, Iter 95, disc loss: 0.008837780722462246, policy loss: 6.07610011848232
Experience 3, Iter 96, disc loss: 0.008886752150265954, policy loss: 6.3860829435520925
Experience 3, Iter 97, disc loss: 0.007529920469863432, policy loss: 6.441118338977292
Experience 3, Iter 98, disc loss: 0.009346424857875853, policy loss: 6.108575680739419
Experience 3, Iter 99, disc loss: 0.009298328295709118, policy loss: 6.174372424134681
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0071],
        [0.0201],
        [0.5920],
        [0.0106]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0672, 0.2886, 0.4858, 0.0098, 0.0041, 0.5269]],

        [[0.0672, 0.2886, 0.4858, 0.0098, 0.0041, 0.5269]],

        [[0.0672, 0.2886, 0.4858, 0.0098, 0.0041, 0.5269]],

        [[0.0672, 0.2886, 0.4858, 0.0098, 0.0041, 0.5269]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0286, 0.0805, 2.3678, 0.0426], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0286, 0.0805, 2.3678, 0.0426])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.531
Iter 2/2000 - Loss: 2.556
Iter 3/2000 - Loss: 1.911
Iter 4/2000 - Loss: 1.722
Iter 5/2000 - Loss: 1.746
Iter 6/2000 - Loss: 1.776
Iter 7/2000 - Loss: 1.786
Iter 8/2000 - Loss: 1.790
Iter 9/2000 - Loss: 1.787
Iter 10/2000 - Loss: 1.761
Iter 11/2000 - Loss: 1.714
Iter 12/2000 - Loss: 1.663
Iter 13/2000 - Loss: 1.627
Iter 14/2000 - Loss: 1.605
Iter 15/2000 - Loss: 1.582
Iter 16/2000 - Loss: 1.539
Iter 17/2000 - Loss: 1.469
Iter 18/2000 - Loss: 1.376
Iter 19/2000 - Loss: 1.272
Iter 20/2000 - Loss: 1.167
Iter 1981/2000 - Loss: -6.650
Iter 1982/2000 - Loss: -6.650
Iter 1983/2000 - Loss: -6.650
Iter 1984/2000 - Loss: -6.650
Iter 1985/2000 - Loss: -6.650
Iter 1986/2000 - Loss: -6.650
Iter 1987/2000 - Loss: -6.650
Iter 1988/2000 - Loss: -6.650
Iter 1989/2000 - Loss: -6.650
Iter 1990/2000 - Loss: -6.650
Iter 1991/2000 - Loss: -6.650
Iter 1992/2000 - Loss: -6.651
Iter 1993/2000 - Loss: -6.651
Iter 1994/2000 - Loss: -6.651
Iter 1995/2000 - Loss: -6.651
Iter 1996/2000 - Loss: -6.651
Iter 1997/2000 - Loss: -6.651
Iter 1998/2000 - Loss: -6.651
Iter 1999/2000 - Loss: -6.651
Iter 2000/2000 - Loss: -6.651
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[18.6423,  8.8002, 36.0062, 17.8500, 15.7726, 43.8617]],

        [[35.5512, 46.9636,  7.7785,  1.7549,  1.6869, 14.1528]],

        [[36.1268, 46.0986, 15.2903,  1.0442,  3.4111, 18.2094]],

        [[36.1019, 50.6487, 15.5589,  2.2351, 12.4127, 29.3952]]])
Signal Variance: tensor([ 0.1637,  0.7872, 15.6896,  0.4162])
Estimated target variance: tensor([0.0286, 0.0805, 2.3678, 0.0426])
N: 40
Signal to noise ratio: tensor([25.3684, 53.0205, 86.6830, 34.7088])
Bound on condition number: tensor([ 25743.1861, 112447.8634, 300558.7273,  48189.0978])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.004597185908026271, policy loss: 12.176127684333121
Experience 4, Iter 1, disc loss: 0.004564903082681517, policy loss: 11.899653576888149
Experience 4, Iter 2, disc loss: 0.00453393305261849, policy loss: 11.516011579152678
Experience 4, Iter 3, disc loss: 0.004507792175849709, policy loss: 11.020862936176233
Experience 4, Iter 4, disc loss: 0.004498232879191589, policy loss: 10.274991734934844
Experience 4, Iter 5, disc loss: 0.004532163319093422, policy loss: 9.402264685365342
Experience 4, Iter 6, disc loss: 0.00473533372581926, policy loss: 8.297541024282076
Experience 4, Iter 7, disc loss: 0.005101736808627274, policy loss: 7.3982707199278686
Experience 4, Iter 8, disc loss: 0.005938493398774237, policy loss: 6.5530192135658885
Experience 4, Iter 9, disc loss: 0.008162632415325716, policy loss: 5.650412843467622
Experience 4, Iter 10, disc loss: 0.011626546766036729, policy loss: 4.987633796908433
Experience 4, Iter 11, disc loss: 0.01868796980129153, policy loss: 4.284479702023771
Experience 4, Iter 12, disc loss: 0.026521899208977098, policy loss: 3.868003144965028
Experience 4, Iter 13, disc loss: 0.03669399437372782, policy loss: 3.481911721457437
Experience 4, Iter 14, disc loss: 0.05136171616265048, policy loss: 3.1230210043808353
Experience 4, Iter 15, disc loss: 0.07283796127041604, policy loss: 2.77086872243695
Experience 4, Iter 16, disc loss: 0.09656839065044558, policy loss: 2.480164990597573
Experience 4, Iter 17, disc loss: 0.13942402723283764, policy loss: 2.1779586655801437
Experience 4, Iter 18, disc loss: 0.2226594455071163, policy loss: 1.7872745402085761
Experience 4, Iter 19, disc loss: 0.2528166850297983, policy loss: 1.6588247278576127
Experience 4, Iter 20, disc loss: 0.36879486453928395, policy loss: 1.511882613636252
Experience 4, Iter 21, disc loss: 0.37967547298466064, policy loss: 1.9558341051240296
Experience 4, Iter 22, disc loss: 0.41266693238872665, policy loss: 1.7357477269250983
Experience 4, Iter 23, disc loss: 0.41892348085740094, policy loss: 1.819879919817288
Experience 4, Iter 24, disc loss: 0.44621168998712085, policy loss: 1.5632558392157265
Experience 4, Iter 25, disc loss: 0.42056794546791143, policy loss: 1.7266140557882947
Experience 4, Iter 26, disc loss: 0.3314116331297943, policy loss: 1.7640935910437427
Experience 4, Iter 27, disc loss: 0.3145512664814102, policy loss: 1.6686817575798925
Experience 4, Iter 28, disc loss: 0.3369131675390996, policy loss: 1.5682157098138474
Experience 4, Iter 29, disc loss: 0.26731982100195123, policy loss: 1.842624384074706
Experience 4, Iter 30, disc loss: 0.26717212087083736, policy loss: 1.761819371168944
Experience 4, Iter 31, disc loss: 0.19718974895740315, policy loss: 2.0315209277744133
Experience 4, Iter 32, disc loss: 0.20067307465215756, policy loss: 1.9787752185553709
Experience 4, Iter 33, disc loss: 0.1728925347021736, policy loss: 2.2208794970331773
Experience 4, Iter 34, disc loss: 0.17527486629926703, policy loss: 2.165210321428822
Experience 4, Iter 35, disc loss: 0.17313764578354485, policy loss: 2.1615578353746363
Experience 4, Iter 36, disc loss: 0.14707167092240417, policy loss: 2.327216827043517
Experience 4, Iter 37, disc loss: 0.17435909710044897, policy loss: 2.11265848264739
Experience 4, Iter 38, disc loss: 0.14159679511234002, policy loss: 2.4289926028750606
Experience 4, Iter 39, disc loss: 0.1488028008688576, policy loss: 2.32774960099312
Experience 4, Iter 40, disc loss: 0.1584919765642719, policy loss: 2.4105761283270146
Experience 4, Iter 41, disc loss: 0.1542562916595007, policy loss: 2.271029090558495
Experience 4, Iter 42, disc loss: 0.1383554875542246, policy loss: 2.450354355579141
Experience 4, Iter 43, disc loss: 0.1436416181495305, policy loss: 2.4077087344071115
Experience 4, Iter 44, disc loss: 0.16759215698496552, policy loss: 2.34133352046599
Experience 4, Iter 45, disc loss: 0.13844434603701017, policy loss: 2.4617875528633797
Experience 4, Iter 46, disc loss: 0.12619100460754104, policy loss: 2.556468670059089
Experience 4, Iter 47, disc loss: 0.12093062318736479, policy loss: 2.691892908316447
Experience 4, Iter 48, disc loss: 0.14494966481220306, policy loss: 2.337662446104183
Experience 4, Iter 49, disc loss: 0.12491824747729666, policy loss: 2.6434994354562056
Experience 4, Iter 50, disc loss: 0.13721744309700643, policy loss: 2.6323343206598793
Experience 4, Iter 51, disc loss: 0.13153321302843474, policy loss: 2.481952429991605
Experience 4, Iter 52, disc loss: 0.15117059538076213, policy loss: 2.3834491244875227
Experience 4, Iter 53, disc loss: 0.11493725765619159, policy loss: 2.6640519358492534
Experience 4, Iter 54, disc loss: 0.10369476412846022, policy loss: 2.692284007482061
Experience 4, Iter 55, disc loss: 0.11159626568186515, policy loss: 2.6607089941010855
Experience 4, Iter 56, disc loss: 0.11367824555398803, policy loss: 2.6277901765345106
Experience 4, Iter 57, disc loss: 0.10098544556489933, policy loss: 2.726419334105432
Experience 4, Iter 58, disc loss: 0.1269144765677822, policy loss: 2.667846664104807
Experience 4, Iter 59, disc loss: 0.08406905566597758, policy loss: 2.8647134070850653
Experience 4, Iter 60, disc loss: 0.09670545103724545, policy loss: 2.8128293385722674
Experience 4, Iter 61, disc loss: 0.10940944460556236, policy loss: 2.661191950360447
Experience 4, Iter 62, disc loss: 0.09180376905160947, policy loss: 2.8346367089590006
Experience 4, Iter 63, disc loss: 0.09460911289035648, policy loss: 2.8601560769689964
Experience 4, Iter 64, disc loss: 0.09179673636917447, policy loss: 2.8139329963500193
Experience 4, Iter 65, disc loss: 0.08986601886591869, policy loss: 2.974783648928061
Experience 4, Iter 66, disc loss: 0.07991772230941088, policy loss: 2.970995915845955
Experience 4, Iter 67, disc loss: 0.07956807790195042, policy loss: 2.942955374165428
Experience 4, Iter 68, disc loss: 0.08893916225752461, policy loss: 2.9719834249303716
Experience 4, Iter 69, disc loss: 0.07662638648871811, policy loss: 3.0640469144839386
Experience 4, Iter 70, disc loss: 0.07597102464367549, policy loss: 3.194113743144036
Experience 4, Iter 71, disc loss: 0.07664634520754099, policy loss: 3.0767421837302344
Experience 4, Iter 72, disc loss: 0.0763803792169811, policy loss: 3.1118628398748625
Experience 4, Iter 73, disc loss: 0.07407999424301634, policy loss: 3.099448791622336
Experience 4, Iter 74, disc loss: 0.08182164227300184, policy loss: 3.0295208767505524
Experience 4, Iter 75, disc loss: 0.06882207286370816, policy loss: 3.240544810187475
Experience 4, Iter 76, disc loss: 0.06796931090116629, policy loss: 3.281539723464665
Experience 4, Iter 77, disc loss: 0.07270143991267569, policy loss: 3.1443477265080038
Experience 4, Iter 78, disc loss: 0.07054740396027855, policy loss: 3.18182851938976
Experience 4, Iter 79, disc loss: 0.058248535828628045, policy loss: 3.492817471469454
Experience 4, Iter 80, disc loss: 0.060445713315143056, policy loss: 3.3717873171683808
Experience 4, Iter 81, disc loss: 0.06600179526748456, policy loss: 3.2639437942601917
Experience 4, Iter 82, disc loss: 0.06412154082746702, policy loss: 3.271965287942577
Experience 4, Iter 83, disc loss: 0.049756570136217135, policy loss: 3.5403234422207173
Experience 4, Iter 84, disc loss: 0.05682261729469965, policy loss: 3.4367784179413627
Experience 4, Iter 85, disc loss: 0.06836770044555646, policy loss: 3.167155954122891
Experience 4, Iter 86, disc loss: 0.055625224931806855, policy loss: 3.502588532418619
Experience 4, Iter 87, disc loss: 0.05089543954171313, policy loss: 3.553542342323884
Experience 4, Iter 88, disc loss: 0.054119142641021474, policy loss: 3.5239524278914374
Experience 4, Iter 89, disc loss: 0.04540131818175987, policy loss: 3.6418260894355168
Experience 4, Iter 90, disc loss: 0.051324133556604874, policy loss: 3.507725158229511
Experience 4, Iter 91, disc loss: 0.04808508356697046, policy loss: 3.674651158060957
Experience 4, Iter 92, disc loss: 0.05510968892826965, policy loss: 3.6103754229406912
Experience 4, Iter 93, disc loss: 0.05187246704196188, policy loss: 3.559200127780665
Experience 4, Iter 94, disc loss: 0.04667500592871973, policy loss: 3.72322596118188
Experience 4, Iter 95, disc loss: 0.05278335887404669, policy loss: 3.5598749305787134
Experience 4, Iter 96, disc loss: 0.05649249569148102, policy loss: 3.4227650170061032
Experience 4, Iter 97, disc loss: 0.0466768365681064, policy loss: 3.702120901618238
Experience 4, Iter 98, disc loss: 0.04710785110056499, policy loss: 3.5687093415490394
Experience 4, Iter 99, disc loss: 0.056320612844783145, policy loss: 3.4796075515074123
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0094],
        [0.0344],
        [0.5485],
        [0.0094]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0677, 0.3657, 0.4339, 0.0128, 0.0035, 1.1318]],

        [[0.0677, 0.3657, 0.4339, 0.0128, 0.0035, 1.1318]],

        [[0.0677, 0.3657, 0.4339, 0.0128, 0.0035, 1.1318]],

        [[0.0677, 0.3657, 0.4339, 0.0128, 0.0035, 1.1318]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0377, 0.1376, 2.1940, 0.0378], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0377, 0.1376, 2.1940, 0.0378])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.575
Iter 2/2000 - Loss: 2.195
Iter 3/2000 - Loss: 1.908
Iter 4/2000 - Loss: 1.915
Iter 5/2000 - Loss: 1.992
Iter 6/2000 - Loss: 1.979
Iter 7/2000 - Loss: 1.918
Iter 8/2000 - Loss: 1.867
Iter 9/2000 - Loss: 1.844
Iter 10/2000 - Loss: 1.826
Iter 11/2000 - Loss: 1.798
Iter 12/2000 - Loss: 1.758
Iter 13/2000 - Loss: 1.708
Iter 14/2000 - Loss: 1.647
Iter 15/2000 - Loss: 1.580
Iter 16/2000 - Loss: 1.509
Iter 17/2000 - Loss: 1.440
Iter 18/2000 - Loss: 1.376
Iter 19/2000 - Loss: 1.313
Iter 20/2000 - Loss: 1.240
Iter 1981/2000 - Loss: -6.855
Iter 1982/2000 - Loss: -6.855
Iter 1983/2000 - Loss: -6.855
Iter 1984/2000 - Loss: -6.855
Iter 1985/2000 - Loss: -6.855
Iter 1986/2000 - Loss: -6.855
Iter 1987/2000 - Loss: -6.855
Iter 1988/2000 - Loss: -6.855
Iter 1989/2000 - Loss: -6.855
Iter 1990/2000 - Loss: -6.855
Iter 1991/2000 - Loss: -6.855
Iter 1992/2000 - Loss: -6.855
Iter 1993/2000 - Loss: -6.855
Iter 1994/2000 - Loss: -6.855
Iter 1995/2000 - Loss: -6.855
Iter 1996/2000 - Loss: -6.855
Iter 1997/2000 - Loss: -6.855
Iter 1998/2000 - Loss: -6.855
Iter 1999/2000 - Loss: -6.856
Iter 2000/2000 - Loss: -6.856
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0018],
        [0.0004]])
Lengthscale: tensor([[[17.5478, 12.7476, 33.9061, 17.0885, 16.1349, 54.9901]],

        [[36.5960, 47.8078,  8.3749,  1.7480,  5.2943, 27.7841]],

        [[28.2401, 41.4409, 15.3339,  1.0855,  3.7963, 20.5933]],

        [[31.9337, 44.0772, 17.5577,  3.6594, 12.1987, 46.9343]]])
Signal Variance: tensor([ 0.2653,  2.2795, 19.2264,  0.6055])
Estimated target variance: tensor([0.0377, 0.1376, 2.1940, 0.0378])
N: 50
Signal to noise ratio: tensor([ 32.7031,  91.7793, 103.6482,  40.2077])
Bound on condition number: tensor([ 53475.7725, 421172.7128, 537148.7127,  80833.9577])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.09848486184759801, policy loss: 2.7656862505399626
Experience 5, Iter 1, disc loss: 0.1223102036806233, policy loss: 2.5717204875868878
Experience 5, Iter 2, disc loss: 0.16051750898709627, policy loss: 2.248982748312583
Experience 5, Iter 3, disc loss: 0.1428224734508719, policy loss: 2.5407881054102877
Experience 5, Iter 4, disc loss: 0.14992242168661024, policy loss: 2.541956600301445
Experience 5, Iter 5, disc loss: 0.147335129149401, policy loss: 2.5662742251309543
Experience 5, Iter 6, disc loss: 0.1465106213263696, policy loss: 2.411473857745468
Experience 5, Iter 7, disc loss: 0.12350353612044356, policy loss: 2.604395818561039
Experience 5, Iter 8, disc loss: 0.12994755494295665, policy loss: 2.4912786546114316
Experience 5, Iter 9, disc loss: 0.13348803845702678, policy loss: 2.4822870674465554
Experience 5, Iter 10, disc loss: 0.14454217624442176, policy loss: 2.5300808026589428
Experience 5, Iter 11, disc loss: 0.1474223148294399, policy loss: 2.5151037529176277
Experience 5, Iter 12, disc loss: 0.13570176577959844, policy loss: 2.6280657809708563
Experience 5, Iter 13, disc loss: 0.13595899820506469, policy loss: 2.6563683898273878
Experience 5, Iter 14, disc loss: 0.11544525269600882, policy loss: 2.9019018140855426
Experience 5, Iter 15, disc loss: 0.12102202841026788, policy loss: 2.9284003718817546
Experience 5, Iter 16, disc loss: 0.1321241746007536, policy loss: 2.7219191136710372
Experience 5, Iter 17, disc loss: 0.125820597721533, policy loss: 2.819564157636939
Experience 5, Iter 18, disc loss: 0.12511142504646083, policy loss: 2.9260012905725103
Experience 5, Iter 19, disc loss: 0.12308492376393507, policy loss: 2.8879641394804714
Experience 5, Iter 20, disc loss: 0.1091443097977727, policy loss: 3.245427212036126
Experience 5, Iter 21, disc loss: 0.10349163283153698, policy loss: 3.3285389806130974
Experience 5, Iter 22, disc loss: 0.10159157260116095, policy loss: 3.207646235956587
Experience 5, Iter 23, disc loss: 0.09308898404981689, policy loss: 3.2794980972329113
Experience 5, Iter 24, disc loss: 0.09916268050375043, policy loss: 3.186639024474399
Experience 5, Iter 25, disc loss: 0.0941880158192494, policy loss: 3.0977175169115236
Experience 5, Iter 26, disc loss: 0.0796659374046806, policy loss: 3.5022090453236787
Experience 5, Iter 27, disc loss: 0.08124355220461242, policy loss: 3.189391583937559
Experience 5, Iter 28, disc loss: 0.07704764622687733, policy loss: 3.2197223549491296
Experience 5, Iter 29, disc loss: 0.07305673635216364, policy loss: 3.255379442066202
Experience 5, Iter 30, disc loss: 0.07372752333047689, policy loss: 3.288273674030706
Experience 5, Iter 31, disc loss: 0.0706759528843622, policy loss: 3.2364322465955215
Experience 5, Iter 32, disc loss: 0.06887885874402155, policy loss: 3.3333479744796444
Experience 5, Iter 33, disc loss: 0.06432119163931786, policy loss: 3.2368271474439707
Experience 5, Iter 34, disc loss: 0.06393276047480917, policy loss: 3.359576022719914
Experience 5, Iter 35, disc loss: 0.06222057656668988, policy loss: 3.2652697701789153
Experience 5, Iter 36, disc loss: 0.06576109716572998, policy loss: 3.143302990502985
Experience 5, Iter 37, disc loss: 0.06351002880709028, policy loss: 3.343762197937435
Experience 5, Iter 38, disc loss: 0.05907145923735503, policy loss: 3.533712283200821
Experience 5, Iter 39, disc loss: 0.054556871211378866, policy loss: 3.500039542797574
Experience 5, Iter 40, disc loss: 0.06060373645479843, policy loss: 3.3541325138028055
Experience 5, Iter 41, disc loss: 0.060581823317531314, policy loss: 3.4925181212438545
Experience 5, Iter 42, disc loss: 0.06354491372529578, policy loss: 3.3560996021400435
Experience 5, Iter 43, disc loss: 0.05476933001997014, policy loss: 3.603420897288933
Experience 5, Iter 44, disc loss: 0.05701577122537846, policy loss: 3.4964294363602892
Experience 5, Iter 45, disc loss: 0.05233315028017287, policy loss: 3.7090356232014274
Experience 5, Iter 46, disc loss: 0.060915945178675754, policy loss: 3.395634378759979
Experience 5, Iter 47, disc loss: 0.055211284766298094, policy loss: 3.60067261941122
Experience 5, Iter 48, disc loss: 0.05615359629360447, policy loss: 3.4828140567000947
Experience 5, Iter 49, disc loss: 0.05604240233331548, policy loss: 3.544738026178145
Experience 5, Iter 50, disc loss: 0.05795746303591873, policy loss: 3.479813174171004
Experience 5, Iter 51, disc loss: 0.0629774916750244, policy loss: 3.554351046306807
Experience 5, Iter 52, disc loss: 0.06337707560542019, policy loss: 3.345390635220386
Experience 5, Iter 53, disc loss: 0.06380004903361046, policy loss: 3.541877052155841
Experience 5, Iter 54, disc loss: 0.06083510089422122, policy loss: 3.7505516587840573
Experience 5, Iter 55, disc loss: 0.08058645341909751, policy loss: 3.283666911706992
Experience 5, Iter 56, disc loss: 0.07712678964807954, policy loss: 3.325568172261627
Experience 5, Iter 57, disc loss: 0.08794471263003237, policy loss: 3.1290739909346392
Experience 5, Iter 58, disc loss: 0.09021336283207754, policy loss: 3.0660973279569905
Experience 5, Iter 59, disc loss: 0.10218274433020255, policy loss: 3.208226095347288
Experience 5, Iter 60, disc loss: 0.11050928880636636, policy loss: 3.0714472137250852
Experience 5, Iter 61, disc loss: 0.11589822539451189, policy loss: 3.155312433071601
Experience 5, Iter 62, disc loss: 0.1251206333925346, policy loss: 3.2613233223443236
Experience 5, Iter 63, disc loss: 0.12208636477843762, policy loss: 3.28864493142674
Experience 5, Iter 64, disc loss: 0.12309175278152192, policy loss: 3.1623485091521073
Experience 5, Iter 65, disc loss: 0.1369530912079245, policy loss: 3.4387447030543057
Experience 5, Iter 66, disc loss: 0.11834535189681093, policy loss: 3.7959536717259716
Experience 5, Iter 67, disc loss: 0.13065374333352592, policy loss: 3.409614590386955
Experience 5, Iter 68, disc loss: 0.12765037602205892, policy loss: 3.5789739671497243
Experience 5, Iter 69, disc loss: 0.12947263388104158, policy loss: 3.519282042762252
Experience 5, Iter 70, disc loss: 0.1302482515697438, policy loss: 3.531313382260899
Experience 5, Iter 71, disc loss: 0.11680131366858183, policy loss: 3.7774840120333666
Experience 5, Iter 72, disc loss: 0.12640165709677625, policy loss: 3.4127996155310845
Experience 5, Iter 73, disc loss: 0.11020457361682437, policy loss: 3.4568622251504406
Experience 5, Iter 74, disc loss: 0.12420823068444095, policy loss: 3.254724711302546
Experience 5, Iter 75, disc loss: 0.11237660789453534, policy loss: 3.435882829592099
Experience 5, Iter 76, disc loss: 0.0894305929324386, policy loss: 3.998705361091588
Experience 5, Iter 77, disc loss: 0.0973782916140371, policy loss: 3.744602204662253
Experience 5, Iter 78, disc loss: 0.10285696526331752, policy loss: 3.2992444348917878
Experience 5, Iter 79, disc loss: 0.09453265474827197, policy loss: 3.453203265319263
Experience 5, Iter 80, disc loss: 0.10167087574793097, policy loss: 3.489778209942104
Experience 5, Iter 81, disc loss: 0.09982505300255073, policy loss: 3.7474068686788913
Experience 5, Iter 82, disc loss: 0.09329247870334058, policy loss: 3.970139655508016
Experience 5, Iter 83, disc loss: 0.08034331025694916, policy loss: 3.8463745779041516
Experience 5, Iter 84, disc loss: 0.08872971815549921, policy loss: 3.898870263899362
Experience 5, Iter 85, disc loss: 0.07899547547186392, policy loss: 4.017810542335392
Experience 5, Iter 86, disc loss: 0.08405348760738632, policy loss: 3.729346071704369
Experience 5, Iter 87, disc loss: 0.07802461253623799, policy loss: 3.9948308832020656
Experience 5, Iter 88, disc loss: 0.06848077942850664, policy loss: 4.3067131253176605
Experience 5, Iter 89, disc loss: 0.07098568698059383, policy loss: 4.167820295074223
Experience 5, Iter 90, disc loss: 0.06895488011117072, policy loss: 3.9785432429346104
Experience 5, Iter 91, disc loss: 0.073786734226271, policy loss: 3.8115300934340586
Experience 5, Iter 92, disc loss: 0.06817574104984761, policy loss: 3.868013289107818
Experience 5, Iter 93, disc loss: 0.06484036367210043, policy loss: 4.029679809978422
Experience 5, Iter 94, disc loss: 0.060403642552547, policy loss: 4.3806371646761075
Experience 5, Iter 95, disc loss: 0.06229725154958458, policy loss: 4.099840391130556
Experience 5, Iter 96, disc loss: 0.06185210812847381, policy loss: 3.9641631087507108
Experience 5, Iter 97, disc loss: 0.05859889417671094, policy loss: 3.976042909502893
Experience 5, Iter 98, disc loss: 0.053798766916500734, policy loss: 4.0979673912898456
Experience 5, Iter 99, disc loss: 0.057509128338227855, policy loss: 4.130938514530568
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0092],
        [0.0551],
        [0.8370],
        [0.0175]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0650, 0.3633, 0.8163, 0.0174, 0.0110, 1.7365]],

        [[0.0650, 0.3633, 0.8163, 0.0174, 0.0110, 1.7365]],

        [[0.0650, 0.3633, 0.8163, 0.0174, 0.0110, 1.7365]],

        [[0.0650, 0.3633, 0.8163, 0.0174, 0.0110, 1.7365]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0368, 0.2205, 3.3479, 0.0702], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0368, 0.2205, 3.3479, 0.0702])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.084
Iter 2/2000 - Loss: 2.726
Iter 3/2000 - Loss: 2.603
Iter 4/2000 - Loss: 2.609
Iter 5/2000 - Loss: 2.640
Iter 6/2000 - Loss: 2.615
Iter 7/2000 - Loss: 2.556
Iter 8/2000 - Loss: 2.502
Iter 9/2000 - Loss: 2.459
Iter 10/2000 - Loss: 2.426
Iter 11/2000 - Loss: 2.396
Iter 12/2000 - Loss: 2.357
Iter 13/2000 - Loss: 2.294
Iter 14/2000 - Loss: 2.211
Iter 15/2000 - Loss: 2.120
Iter 16/2000 - Loss: 2.030
Iter 17/2000 - Loss: 1.942
Iter 18/2000 - Loss: 1.848
Iter 19/2000 - Loss: 1.735
Iter 20/2000 - Loss: 1.598
Iter 1981/2000 - Loss: -6.421
Iter 1982/2000 - Loss: -6.421
Iter 1983/2000 - Loss: -6.421
Iter 1984/2000 - Loss: -6.422
Iter 1985/2000 - Loss: -6.422
Iter 1986/2000 - Loss: -6.422
Iter 1987/2000 - Loss: -6.422
Iter 1988/2000 - Loss: -6.422
Iter 1989/2000 - Loss: -6.422
Iter 1990/2000 - Loss: -6.422
Iter 1991/2000 - Loss: -6.422
Iter 1992/2000 - Loss: -6.422
Iter 1993/2000 - Loss: -6.422
Iter 1994/2000 - Loss: -6.422
Iter 1995/2000 - Loss: -6.422
Iter 1996/2000 - Loss: -6.422
Iter 1997/2000 - Loss: -6.422
Iter 1998/2000 - Loss: -6.422
Iter 1999/2000 - Loss: -6.422
Iter 2000/2000 - Loss: -6.422
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0004]])
Lengthscale: tensor([[[21.3271, 13.8647, 27.0085, 15.2636, 13.8848, 55.4308]],

        [[34.7009, 45.3942,  9.2713,  1.3090,  6.6753, 28.6568]],

        [[26.0171, 38.7379, 15.6178,  0.9322,  1.6798, 21.2145]],

        [[27.3309, 36.9902, 19.9658,  4.3767,  3.2929, 45.6900]]])
Signal Variance: tensor([ 0.2940,  2.1821, 16.3006,  0.6883])
Estimated target variance: tensor([0.0368, 0.2205, 3.3479, 0.0702])
N: 60
Signal to noise ratio: tensor([32.6190, 92.2607, 99.3477, 43.5312])
Bound on condition number: tensor([ 63840.8234, 510723.4160, 592199.0036, 113698.9245])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.10293432620818554, policy loss: 3.0778983602574703
Experience 6, Iter 1, disc loss: 0.09335914225964367, policy loss: 3.3359255042444005
Experience 6, Iter 2, disc loss: 0.09186765153376657, policy loss: 3.267862938329311
Experience 6, Iter 3, disc loss: 0.0881918227959097, policy loss: 3.715391187426629
Experience 6, Iter 4, disc loss: 0.07953848073791012, policy loss: 3.682739901664487
Experience 6, Iter 5, disc loss: 0.08135615946890831, policy loss: 3.9014024295114975
Experience 6, Iter 6, disc loss: 0.08834913324991525, policy loss: 3.7432654199912427
Experience 6, Iter 7, disc loss: 0.09353329236404383, policy loss: 4.0010619887929515
Experience 6, Iter 8, disc loss: 0.09174958756164081, policy loss: 4.162052972431555
Experience 6, Iter 9, disc loss: 0.08775080349797448, policy loss: 4.090307010224831
Experience 6, Iter 10, disc loss: 0.09703447798574157, policy loss: 3.5896996791620324
Experience 6, Iter 11, disc loss: 0.08769860847916516, policy loss: 3.71554133114009
Experience 6, Iter 12, disc loss: 0.08574715637092437, policy loss: 3.688848066582642
Experience 6, Iter 13, disc loss: 0.08180615690748452, policy loss: 3.8956734564158877
Experience 6, Iter 14, disc loss: 0.07621398780935928, policy loss: 3.947697128243046
Experience 6, Iter 15, disc loss: 0.07723755374752894, policy loss: 4.022759613509957
Experience 6, Iter 16, disc loss: 0.07951154651897538, policy loss: 3.4898982660139297
Experience 6, Iter 17, disc loss: 0.08500066759709486, policy loss: 3.8405200295002357
Experience 6, Iter 18, disc loss: 0.08965875960209334, policy loss: 3.2798835875569314
Experience 6, Iter 19, disc loss: 0.09189389494061212, policy loss: 3.659793737903793
Experience 6, Iter 20, disc loss: 0.08626130317695804, policy loss: 3.353860836453241
Experience 6, Iter 21, disc loss: 0.08662683353539297, policy loss: 3.4192938196388685
Experience 6, Iter 22, disc loss: 0.08941239400764786, policy loss: 3.8070397835725074
Experience 6, Iter 23, disc loss: 0.08474414823198421, policy loss: 3.7212539977931183
Experience 6, Iter 24, disc loss: 0.08452497402163245, policy loss: 3.892777257431725
Experience 6, Iter 25, disc loss: 0.08286098258097596, policy loss: 4.080834391387638
Experience 6, Iter 26, disc loss: 0.08178198766318892, policy loss: 3.9902086398930425
Experience 6, Iter 27, disc loss: 0.08115994791386641, policy loss: 3.885479125519964
Experience 6, Iter 28, disc loss: 0.08481573252431154, policy loss: 3.6431276835174256
Experience 6, Iter 29, disc loss: 0.07344955693045668, policy loss: 4.2650926667728815
Experience 6, Iter 30, disc loss: 0.07166015569848623, policy loss: 4.072419966452031
Experience 6, Iter 31, disc loss: 0.07384225276202425, policy loss: 3.9427786254869712
Experience 6, Iter 32, disc loss: 0.05782526679122652, policy loss: 4.2350016683054275
Experience 6, Iter 33, disc loss: 0.06573597090749375, policy loss: 3.9965415451316466
Experience 6, Iter 34, disc loss: 0.06482371027031339, policy loss: 3.874134298157287
Experience 6, Iter 35, disc loss: 0.06747921520321279, policy loss: 3.693277568417237
Experience 6, Iter 36, disc loss: 0.06998691733390704, policy loss: 3.6839571097097386
Experience 6, Iter 37, disc loss: 0.07046706038817946, policy loss: 3.7488894963427972
Experience 6, Iter 38, disc loss: 0.06156776294714181, policy loss: 3.896835066036755
Experience 6, Iter 39, disc loss: 0.060846122307844364, policy loss: 3.8132539749669663
Experience 6, Iter 40, disc loss: 0.05861528360197936, policy loss: 4.239295460626344
Experience 6, Iter 41, disc loss: 0.06323295354472042, policy loss: 3.817571906191012
Experience 6, Iter 42, disc loss: 0.06567380120600404, policy loss: 3.8794927243551025
Experience 6, Iter 43, disc loss: 0.06003676465672911, policy loss: 4.188263756527965
Experience 6, Iter 44, disc loss: 0.05567865645402416, policy loss: 4.494710012949069
Experience 6, Iter 45, disc loss: 0.055987010290199736, policy loss: 4.285666508996018
Experience 6, Iter 46, disc loss: 0.05884560847750065, policy loss: 3.942459563394267
Experience 6, Iter 47, disc loss: 0.056551487498746354, policy loss: 4.1261509306774204
Experience 6, Iter 48, disc loss: 0.06033494530998272, policy loss: 3.9485769265778163
Experience 6, Iter 49, disc loss: 0.04707487523588539, policy loss: 4.608372994005657
Experience 6, Iter 50, disc loss: 0.051102031101455614, policy loss: 4.500390494113876
Experience 6, Iter 51, disc loss: 0.055320202165066376, policy loss: 4.212392771848446
Experience 6, Iter 52, disc loss: 0.04821738721726851, policy loss: 4.175539619678265
Experience 6, Iter 53, disc loss: 0.0428341965554868, policy loss: 4.751104185007721
Experience 6, Iter 54, disc loss: 0.04950632559254987, policy loss: 4.0466639885440685
Experience 6, Iter 55, disc loss: 0.045288160338566884, policy loss: 4.361104396599533
Experience 6, Iter 56, disc loss: 0.04675235720839308, policy loss: 4.200536618183397
Experience 6, Iter 57, disc loss: 0.04502331436980525, policy loss: 4.424751818274331
Experience 6, Iter 58, disc loss: 0.043939496817097086, policy loss: 4.2792308490497994
Experience 6, Iter 59, disc loss: 0.04628336104318309, policy loss: 4.061848440913417
Experience 6, Iter 60, disc loss: 0.04283723518293643, policy loss: 4.5609417383050435
Experience 6, Iter 61, disc loss: 0.040174155534504444, policy loss: 4.313884281628592
Experience 6, Iter 62, disc loss: 0.03918064286374717, policy loss: 4.4827634973926385
Experience 6, Iter 63, disc loss: 0.0403581299044736, policy loss: 4.434772376164062
Experience 6, Iter 64, disc loss: 0.04715011594015113, policy loss: 4.200078393318772
Experience 6, Iter 65, disc loss: 0.04141215184967115, policy loss: 4.504238092389065
Experience 6, Iter 66, disc loss: 0.03835749303626658, policy loss: 4.522455817868837
Experience 6, Iter 67, disc loss: 0.039500697231708806, policy loss: 4.6097399800287535
Experience 6, Iter 68, disc loss: 0.040655071671129896, policy loss: 4.478667701055289
Experience 6, Iter 69, disc loss: 0.03813761190623789, policy loss: 4.671478360309004
Experience 6, Iter 70, disc loss: 0.041449342942628135, policy loss: 4.481699465119389
Experience 6, Iter 71, disc loss: 0.03738759799528896, policy loss: 4.65489325304323
Experience 6, Iter 72, disc loss: 0.04108977403447631, policy loss: 4.727366787863178
Experience 6, Iter 73, disc loss: 0.034387872879204376, policy loss: 5.079956220997523
Experience 6, Iter 74, disc loss: 0.03472796762679638, policy loss: 4.651994828186843
Experience 6, Iter 75, disc loss: 0.03361509400378097, policy loss: 4.739731538396262
Experience 6, Iter 76, disc loss: 0.037758070927419715, policy loss: 4.459567530852322
Experience 6, Iter 77, disc loss: 0.035926844507364095, policy loss: 4.513459500975779
Experience 6, Iter 78, disc loss: 0.038137319660460245, policy loss: 4.277833281533123
Experience 6, Iter 79, disc loss: 0.03451318374930269, policy loss: 4.834659473415095
Experience 6, Iter 80, disc loss: 0.03598249263066593, policy loss: 4.465430214514358
Experience 6, Iter 81, disc loss: 0.03908828851818631, policy loss: 4.286147171777612
Experience 6, Iter 82, disc loss: 0.03475647919099175, policy loss: 4.331312626484983
Experience 6, Iter 83, disc loss: 0.03759074631598556, policy loss: 4.421283216825667
Experience 6, Iter 84, disc loss: 0.032591553177332394, policy loss: 4.833222186311668
Experience 6, Iter 85, disc loss: 0.035387589602875955, policy loss: 4.637090531220958
Experience 6, Iter 86, disc loss: 0.03279687013828575, policy loss: 4.885898075099446
Experience 6, Iter 87, disc loss: 0.031371534822274685, policy loss: 4.723619898111804
Experience 6, Iter 88, disc loss: 0.034254418835498346, policy loss: 4.68763152727057
Experience 6, Iter 89, disc loss: 0.03212103065215513, policy loss: 4.715905275234579
Experience 6, Iter 90, disc loss: 0.033151560459617174, policy loss: 4.533833711881108
Experience 6, Iter 91, disc loss: 0.032298413690537695, policy loss: 4.512918554577528
Experience 6, Iter 92, disc loss: 0.030674478006642657, policy loss: 4.789066898863988
Experience 6, Iter 93, disc loss: 0.03239395531874479, policy loss: 4.428718265589137
Experience 6, Iter 94, disc loss: 0.0292260633552407, policy loss: 5.082598277639898
Experience 6, Iter 95, disc loss: 0.03511094580606848, policy loss: 4.910915480550658
Experience 6, Iter 96, disc loss: 0.032276943307430986, policy loss: 5.028505668974171
Experience 6, Iter 97, disc loss: 0.03200673410831042, policy loss: 4.706373206028005
Experience 6, Iter 98, disc loss: 0.029331663499119, policy loss: 4.894934123125168
Experience 6, Iter 99, disc loss: 0.03162036763430861, policy loss: 4.522050918484049
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0091],
        [0.0833],
        [1.0491],
        [0.0240]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0613, 0.3595, 1.1196, 0.0205, 0.0177, 2.3169]],

        [[0.0613, 0.3595, 1.1196, 0.0205, 0.0177, 2.3169]],

        [[0.0613, 0.3595, 1.1196, 0.0205, 0.0177, 2.3169]],

        [[0.0613, 0.3595, 1.1196, 0.0205, 0.0177, 2.3169]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0363, 0.3333, 4.1966, 0.0960], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0363, 0.3333, 4.1966, 0.0960])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.366
Iter 2/2000 - Loss: 3.052
Iter 3/2000 - Loss: 2.991
Iter 4/2000 - Loss: 2.997
Iter 5/2000 - Loss: 2.985
Iter 6/2000 - Loss: 2.943
Iter 7/2000 - Loss: 2.872
Iter 8/2000 - Loss: 2.797
Iter 9/2000 - Loss: 2.743
Iter 10/2000 - Loss: 2.707
Iter 11/2000 - Loss: 2.660
Iter 12/2000 - Loss: 2.583
Iter 13/2000 - Loss: 2.480
Iter 14/2000 - Loss: 2.369
Iter 15/2000 - Loss: 2.258
Iter 16/2000 - Loss: 2.146
Iter 17/2000 - Loss: 2.021
Iter 18/2000 - Loss: 1.873
Iter 19/2000 - Loss: 1.701
Iter 20/2000 - Loss: 1.507
Iter 1981/2000 - Loss: -6.013
Iter 1982/2000 - Loss: -6.013
Iter 1983/2000 - Loss: -6.013
Iter 1984/2000 - Loss: -6.013
Iter 1985/2000 - Loss: -6.013
Iter 1986/2000 - Loss: -6.013
Iter 1987/2000 - Loss: -6.013
Iter 1988/2000 - Loss: -6.013
Iter 1989/2000 - Loss: -6.013
Iter 1990/2000 - Loss: -6.013
Iter 1991/2000 - Loss: -6.013
Iter 1992/2000 - Loss: -6.013
Iter 1993/2000 - Loss: -6.013
Iter 1994/2000 - Loss: -6.013
Iter 1995/2000 - Loss: -6.013
Iter 1996/2000 - Loss: -6.013
Iter 1997/2000 - Loss: -6.013
Iter 1998/2000 - Loss: -6.013
Iter 1999/2000 - Loss: -6.013
Iter 2000/2000 - Loss: -6.013
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[15.9815,  4.2917,  8.7317, 13.0298, 10.0049, 56.8536]],

        [[29.9562, 44.7875,  9.2956,  1.4912,  6.8887, 29.5461]],

        [[28.9857, 38.6848,  8.1467,  1.2337,  1.3954, 24.6497]],

        [[26.7353, 37.2238, 20.1714,  3.6172,  1.8455, 45.2040]]])
Signal Variance: tensor([ 0.0961,  3.3208, 20.8914,  0.6761])
Estimated target variance: tensor([0.0363, 0.3333, 4.1966, 0.0960])
N: 70
Signal to noise ratio: tensor([ 20.9350, 102.8685, 103.2335,  45.6041])
Bound on condition number: tensor([ 30680.2261, 740735.8038, 746002.3204, 145582.3094])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.07412956927627756, policy loss: 3.409012987279093
Experience 7, Iter 1, disc loss: 0.06777136634387157, policy loss: 3.846847058878688
Experience 7, Iter 2, disc loss: 0.08283942231164519, policy loss: 3.3688999383876967
Experience 7, Iter 3, disc loss: 0.06861553026038981, policy loss: 3.8010076842578777
Experience 7, Iter 4, disc loss: 0.06408636336080477, policy loss: 3.9556778563026156
Experience 7, Iter 5, disc loss: 0.07544800665863154, policy loss: 3.7710840210602914
Experience 7, Iter 6, disc loss: 0.06940086276372719, policy loss: 4.087998190954014
Experience 7, Iter 7, disc loss: 0.06934976100184662, policy loss: 4.560374900601447
Experience 7, Iter 8, disc loss: 0.07316008074713837, policy loss: 4.433246432309197
Experience 7, Iter 9, disc loss: 0.07350181421651292, policy loss: 4.046830019547136
Experience 7, Iter 10, disc loss: 0.0733733411266928, policy loss: 3.873958166597855
Experience 7, Iter 11, disc loss: 0.06784719749833862, policy loss: 3.9746630232164866
Experience 7, Iter 12, disc loss: 0.06780449847880418, policy loss: 3.7089140310254214
Experience 7, Iter 13, disc loss: 0.06601378953498165, policy loss: 3.784792204628951
Experience 7, Iter 14, disc loss: 0.07435581217760313, policy loss: 3.3709648694240872
Experience 7, Iter 15, disc loss: 0.07555597211389281, policy loss: 3.469566091377227
Experience 7, Iter 16, disc loss: 0.0763370682230747, policy loss: 3.7134558322245397
Experience 7, Iter 17, disc loss: 0.06944319428496583, policy loss: 3.5451688650460422
Experience 7, Iter 18, disc loss: 0.0818264415603441, policy loss: 3.345804244843298
Experience 7, Iter 19, disc loss: 0.0705584709911295, policy loss: 4.185717099081677
Experience 7, Iter 20, disc loss: 0.08256227304343931, policy loss: 3.495655541127948
Experience 7, Iter 21, disc loss: 0.07374610001283766, policy loss: 3.7398694108658814
Experience 7, Iter 22, disc loss: 0.0739444531355677, policy loss: 3.839078458161772
Experience 7, Iter 23, disc loss: 0.08329864014788634, policy loss: 3.6163634455340405
Experience 7, Iter 24, disc loss: 0.08580961541066656, policy loss: 3.65032530339575
Experience 7, Iter 25, disc loss: 0.08118773445223644, policy loss: 3.925043435038274
Experience 7, Iter 26, disc loss: 0.09603152674949933, policy loss: 3.583134852446757
Experience 7, Iter 27, disc loss: 0.0838742244819644, policy loss: 3.9228311595248004
Experience 7, Iter 28, disc loss: 0.08165988110792619, policy loss: 3.9059310690439086
Experience 7, Iter 29, disc loss: 0.09867502247466761, policy loss: 3.321394893983921
Experience 7, Iter 30, disc loss: 0.10043871973590565, policy loss: 3.4422201016403804
Experience 7, Iter 31, disc loss: 0.10759007409775745, policy loss: 3.1830827352965883
Experience 7, Iter 32, disc loss: 0.09879098211494443, policy loss: 3.3895220952215057
Experience 7, Iter 33, disc loss: 0.1035908044947519, policy loss: 3.4538961355515223
Experience 7, Iter 34, disc loss: 0.10406051901852889, policy loss: 3.554744318413479
Experience 7, Iter 35, disc loss: 0.1068428548219689, policy loss: 3.3194193199971354
Experience 7, Iter 36, disc loss: 0.10845930407087792, policy loss: 3.2330614373453987
Experience 7, Iter 37, disc loss: 0.10269855649306685, policy loss: 3.4145646338900066
Experience 7, Iter 38, disc loss: 0.09309392469902969, policy loss: 3.7464154950458566
Experience 7, Iter 39, disc loss: 0.09734884530353541, policy loss: 3.6759752733166646
Experience 7, Iter 40, disc loss: 0.10866830480069062, policy loss: 3.0754125779872323
Experience 7, Iter 41, disc loss: 0.09973751944995954, policy loss: 3.43912911218031
Experience 7, Iter 42, disc loss: 0.09063195675837246, policy loss: 3.437788695029843
Experience 7, Iter 43, disc loss: 0.10480588780152152, policy loss: 3.4346328939526773
Experience 7, Iter 44, disc loss: 0.1025847419192299, policy loss: 3.2391175292308443
Experience 7, Iter 45, disc loss: 0.08563793709229484, policy loss: 3.5749244547861765
Experience 7, Iter 46, disc loss: 0.09006422867549832, policy loss: 3.3413434193472664
Experience 7, Iter 47, disc loss: 0.08766134647650499, policy loss: 3.348976909412711
Experience 7, Iter 48, disc loss: 0.08649343310283142, policy loss: 3.5959205070255393
Experience 7, Iter 49, disc loss: 0.08124333755442781, policy loss: 3.6491738360161685
Experience 7, Iter 50, disc loss: 0.08323047857529056, policy loss: 3.8945489681914536
Experience 7, Iter 51, disc loss: 0.08087824443750274, policy loss: 3.437925437653581
Experience 7, Iter 52, disc loss: 0.0784329608444419, policy loss: 3.6408445035103956
Experience 7, Iter 53, disc loss: 0.08296974371726908, policy loss: 3.462843933195855
Experience 7, Iter 54, disc loss: 0.07559485983279463, policy loss: 3.6495372862399003
Experience 7, Iter 55, disc loss: 0.07060054115730907, policy loss: 3.5871383518435076
Experience 7, Iter 56, disc loss: 0.07130026338780053, policy loss: 3.682998437446622
Experience 7, Iter 57, disc loss: 0.0691952389417175, policy loss: 3.731908359903835
Experience 7, Iter 58, disc loss: 0.07056867668085492, policy loss: 3.8155395280662336
Experience 7, Iter 59, disc loss: 0.0709872303977799, policy loss: 3.56398994325128
Experience 7, Iter 60, disc loss: 0.07049906106395379, policy loss: 3.688467702269776
Experience 7, Iter 61, disc loss: 0.06354172470674294, policy loss: 3.9296219164779522
Experience 7, Iter 62, disc loss: 0.07238134244688466, policy loss: 3.5832579717985125
Experience 7, Iter 63, disc loss: 0.06953295918887485, policy loss: 3.951002502790179
Experience 7, Iter 64, disc loss: 0.0626403087091764, policy loss: 3.8977194346241792
Experience 7, Iter 65, disc loss: 0.057788937371205225, policy loss: 4.414255779993582
Experience 7, Iter 66, disc loss: 0.060505158554546634, policy loss: 4.915679111532272
Experience 7, Iter 67, disc loss: 0.05762435383057285, policy loss: 4.027531240234567
Experience 7, Iter 68, disc loss: 0.05316779135564417, policy loss: 4.151543215109441
Experience 7, Iter 69, disc loss: 0.05294581032298806, policy loss: 3.958890334592705
Experience 7, Iter 70, disc loss: 0.053525929819911264, policy loss: 3.8502565449495276
Experience 7, Iter 71, disc loss: 0.05470076300237791, policy loss: 4.184291842851801
Experience 7, Iter 72, disc loss: 0.05124438518452183, policy loss: 4.354773978063294
Experience 7, Iter 73, disc loss: 0.04903901444302816, policy loss: 4.044591789637712
Experience 7, Iter 74, disc loss: 0.04850769422269879, policy loss: 4.215075143257435
Experience 7, Iter 75, disc loss: 0.053318465114516655, policy loss: 3.9589386873177044
Experience 7, Iter 76, disc loss: 0.05083159882360228, policy loss: 3.948102567045847
Experience 7, Iter 77, disc loss: 0.044777399416790396, policy loss: 4.213435179493095
Experience 7, Iter 78, disc loss: 0.053796273906505224, policy loss: 4.141104590930109
Experience 7, Iter 79, disc loss: 0.04794218275882807, policy loss: 4.4341832941566794
Experience 7, Iter 80, disc loss: 0.04561457369674907, policy loss: 4.363907457859158
Experience 7, Iter 81, disc loss: 0.04433906004008514, policy loss: 4.111312907168539
Experience 7, Iter 82, disc loss: 0.04306668404800241, policy loss: 4.429537419231239
Experience 7, Iter 83, disc loss: 0.04069361911558467, policy loss: 4.366745087194564
Experience 7, Iter 84, disc loss: 0.043800595115628366, policy loss: 4.260216510494784
Experience 7, Iter 85, disc loss: 0.043154522682681565, policy loss: 4.407631610103499
Experience 7, Iter 86, disc loss: 0.04002369798857638, policy loss: 4.337471047009664
Experience 7, Iter 87, disc loss: 0.041937681790409, policy loss: 4.48015168543747
Experience 7, Iter 88, disc loss: 0.04289752641614485, policy loss: 4.0926522900054145
Experience 7, Iter 89, disc loss: 0.03724490717675068, policy loss: 4.568445715998836
Experience 7, Iter 90, disc loss: 0.03782500071135708, policy loss: 4.634929186920004
Experience 7, Iter 91, disc loss: 0.03665154199766182, policy loss: 4.653581910877244
Experience 7, Iter 92, disc loss: 0.0384947287065396, policy loss: 4.231163032453136
Experience 7, Iter 93, disc loss: 0.0349976593129494, policy loss: 4.677623407047829
Experience 7, Iter 94, disc loss: 0.03533359658114167, policy loss: 4.76298670163579
Experience 7, Iter 95, disc loss: 0.029838414918263192, policy loss: 4.995107203597994
Experience 7, Iter 96, disc loss: 0.027651522575364573, policy loss: 5.291527225045707
Experience 7, Iter 97, disc loss: 0.028171562035930228, policy loss: 4.922446812319777
Experience 7, Iter 98, disc loss: 0.031372356716278874, policy loss: 4.530387035796846
Experience 7, Iter 99, disc loss: 0.029937801704584287, policy loss: 4.898059312925879
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0090],
        [0.1220],
        [1.2732],
        [0.0287]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0576, 0.3534, 1.3403, 0.0230, 0.0196, 3.1736]],

        [[0.0576, 0.3534, 1.3403, 0.0230, 0.0196, 3.1736]],

        [[0.0576, 0.3534, 1.3403, 0.0230, 0.0196, 3.1736]],

        [[0.0576, 0.3534, 1.3403, 0.0230, 0.0196, 3.1736]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0358, 0.4879, 5.0927, 0.1148], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0358, 0.4879, 5.0927, 0.1148])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.633
Iter 2/2000 - Loss: 3.361
Iter 3/2000 - Loss: 3.329
Iter 4/2000 - Loss: 3.347
Iter 5/2000 - Loss: 3.306
Iter 6/2000 - Loss: 3.225
Iter 7/2000 - Loss: 3.147
Iter 8/2000 - Loss: 3.088
Iter 9/2000 - Loss: 3.038
Iter 10/2000 - Loss: 2.975
Iter 11/2000 - Loss: 2.884
Iter 12/2000 - Loss: 2.772
Iter 13/2000 - Loss: 2.653
Iter 14/2000 - Loss: 2.532
Iter 15/2000 - Loss: 2.407
Iter 16/2000 - Loss: 2.266
Iter 17/2000 - Loss: 2.104
Iter 18/2000 - Loss: 1.920
Iter 19/2000 - Loss: 1.719
Iter 20/2000 - Loss: 1.504
Iter 1981/2000 - Loss: -6.140
Iter 1982/2000 - Loss: -6.140
Iter 1983/2000 - Loss: -6.140
Iter 1984/2000 - Loss: -6.140
Iter 1985/2000 - Loss: -6.140
Iter 1986/2000 - Loss: -6.140
Iter 1987/2000 - Loss: -6.140
Iter 1988/2000 - Loss: -6.140
Iter 1989/2000 - Loss: -6.140
Iter 1990/2000 - Loss: -6.140
Iter 1991/2000 - Loss: -6.140
Iter 1992/2000 - Loss: -6.140
Iter 1993/2000 - Loss: -6.140
Iter 1994/2000 - Loss: -6.140
Iter 1995/2000 - Loss: -6.140
Iter 1996/2000 - Loss: -6.140
Iter 1997/2000 - Loss: -6.140
Iter 1998/2000 - Loss: -6.140
Iter 1999/2000 - Loss: -6.140
Iter 2000/2000 - Loss: -6.140
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[23.0021, 12.4807, 23.2380,  5.8354,  3.1481, 58.5511]],

        [[28.5973, 43.0209,  7.6463,  1.3170,  5.0470, 15.8581]],

        [[27.7491, 38.5735,  8.0003,  1.2629,  1.1609, 26.3356]],

        [[24.9168, 38.8208, 16.5982,  3.6802,  2.0576, 44.3850]]])
Signal Variance: tensor([ 0.2723,  1.3608, 22.4542,  0.6321])
Estimated target variance: tensor([0.0358, 0.4879, 5.0927, 0.1148])
N: 80
Signal to noise ratio: tensor([ 33.9413,  73.8306, 112.7050,  45.5957])
Bound on condition number: tensor([  92161.8444,  436077.4577, 1016194.8103,  166318.1857])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.022220027698142304, policy loss: 5.730178446211035
Experience 8, Iter 1, disc loss: 0.026274404285423394, policy loss: 4.580314807489039
Experience 8, Iter 2, disc loss: 0.02605790908507518, policy loss: 4.710954017556996
Experience 8, Iter 3, disc loss: 0.02510714254915043, policy loss: 4.62625735592005
Experience 8, Iter 4, disc loss: 0.024481356588813882, policy loss: 4.751035089861036
Experience 8, Iter 5, disc loss: 0.025009876375399462, policy loss: 4.578926690548011
Experience 8, Iter 6, disc loss: 0.02617384673965542, policy loss: 4.499406700399593
Experience 8, Iter 7, disc loss: 0.02367619611000716, policy loss: 5.051650488173513
Experience 8, Iter 8, disc loss: 0.027680481772523913, policy loss: 4.791390492632958
Experience 8, Iter 9, disc loss: 0.025193434621537607, policy loss: 4.484096155410406
Experience 8, Iter 10, disc loss: 0.024958815236164677, policy loss: 4.696513965814988
Experience 8, Iter 11, disc loss: 0.024743311257985793, policy loss: 4.702796838071259
Experience 8, Iter 12, disc loss: 0.02546685816542207, policy loss: 4.594691164581067
Experience 8, Iter 13, disc loss: 0.022525408255281973, policy loss: 5.061768930882288
Experience 8, Iter 14, disc loss: 0.0236348457208082, policy loss: 4.859752707286884
Experience 8, Iter 15, disc loss: 0.02468365993861716, policy loss: 4.719042459357068
Experience 8, Iter 16, disc loss: 0.024011366698038057, policy loss: 4.836386451149044
Experience 8, Iter 17, disc loss: 0.023678884274054242, policy loss: 4.8032279448030915
Experience 8, Iter 18, disc loss: 0.024132351659581705, policy loss: 4.879771047039705
Experience 8, Iter 19, disc loss: 0.023212108782213782, policy loss: 4.949719727235908
Experience 8, Iter 20, disc loss: 0.02319134860870199, policy loss: 5.275166228948659
Experience 8, Iter 21, disc loss: 0.022919054248346643, policy loss: 5.400627368118386
Experience 8, Iter 22, disc loss: 0.022164611380884644, policy loss: 4.970923160695817
Experience 8, Iter 23, disc loss: 0.02079251264228344, policy loss: 5.253881161658291
Experience 8, Iter 24, disc loss: 0.02258735268307179, policy loss: 4.907758029127459
Experience 8, Iter 25, disc loss: 0.02239895890943668, policy loss: 4.864484320195844
Experience 8, Iter 26, disc loss: 0.018022845829675407, policy loss: 5.498006844884809
Experience 8, Iter 27, disc loss: 0.019850433481589647, policy loss: 5.229300443621922
Experience 8, Iter 28, disc loss: 0.02136022783937902, policy loss: 4.727487250697664
Experience 8, Iter 29, disc loss: 0.02009581609582209, policy loss: 5.226544729364777
Experience 8, Iter 30, disc loss: 0.020330721373236705, policy loss: 4.9723858882737435
Experience 8, Iter 31, disc loss: 0.021695667594704628, policy loss: 4.655154752817774
Experience 8, Iter 32, disc loss: 0.020579411163261802, policy loss: 5.227318964244214
Experience 8, Iter 33, disc loss: 0.01963337819832536, policy loss: 5.202607087492675
Experience 8, Iter 34, disc loss: 0.021088858006184676, policy loss: 4.858766903072984
Experience 8, Iter 35, disc loss: 0.0197333799223345, policy loss: 5.26014325885992
Experience 8, Iter 36, disc loss: 0.018430597379785175, policy loss: 4.930055311259039
Experience 8, Iter 37, disc loss: 0.01851029708689603, policy loss: 5.241794570174608
Experience 8, Iter 38, disc loss: 0.02007982381497518, policy loss: 5.015299225759591
Experience 8, Iter 39, disc loss: 0.018111945894395916, policy loss: 5.139211682627547
Experience 8, Iter 40, disc loss: 0.02055686234219936, policy loss: 4.9245217255816565
Experience 8, Iter 41, disc loss: 0.018603346866911452, policy loss: 5.202346633741171
Experience 8, Iter 42, disc loss: 0.017441924652822183, policy loss: 5.194366217678815
Experience 8, Iter 43, disc loss: 0.017910932827929757, policy loss: 5.172959028457775
Experience 8, Iter 44, disc loss: 0.019113578723827407, policy loss: 5.006840356500808
Experience 8, Iter 45, disc loss: 0.017800631620731158, policy loss: 5.305634043377729
Experience 8, Iter 46, disc loss: 0.015442192276901953, policy loss: 5.660694355840707
Experience 8, Iter 47, disc loss: 0.018405452030180198, policy loss: 5.202286731810392
Experience 8, Iter 48, disc loss: 0.0187990741104231, policy loss: 5.016137767633829
Experience 8, Iter 49, disc loss: 0.01728782360272236, policy loss: 5.095399844909952
Experience 8, Iter 50, disc loss: 0.018257841598209076, policy loss: 5.303020198910944
Experience 8, Iter 51, disc loss: 0.01638352462428569, policy loss: 5.3607279313932725
Experience 8, Iter 52, disc loss: 0.016981510606642075, policy loss: 5.411901063974085
Experience 8, Iter 53, disc loss: 0.017681402336866138, policy loss: 5.044187797909835
Experience 8, Iter 54, disc loss: 0.016158710170528202, policy loss: 5.300858488022177
Experience 8, Iter 55, disc loss: 0.016430462996097784, policy loss: 5.360237379371151
Experience 8, Iter 56, disc loss: 0.01603342279822577, policy loss: 5.219253253062142
Experience 8, Iter 57, disc loss: 0.01676631299367771, policy loss: 4.961008952446241
Experience 8, Iter 58, disc loss: 0.01543571980312421, policy loss: 5.496086215263761
Experience 8, Iter 59, disc loss: 0.01728332422142556, policy loss: 4.957874526974131
Experience 8, Iter 60, disc loss: 0.017075628978498125, policy loss: 4.931137229453257
Experience 8, Iter 61, disc loss: 0.017408845444712998, policy loss: 5.036757123471405
Experience 8, Iter 62, disc loss: 0.017233762971818355, policy loss: 4.872337191358108
Experience 8, Iter 63, disc loss: 0.01622435468188689, policy loss: 5.277316980680375
Experience 8, Iter 64, disc loss: 0.015324985399008681, policy loss: 5.87850863476193
Experience 8, Iter 65, disc loss: 0.0159502235876605, policy loss: 5.256358820762347
Experience 8, Iter 66, disc loss: 0.015967222279158016, policy loss: 5.100106747225523
Experience 8, Iter 67, disc loss: 0.016934884113660228, policy loss: 4.94766339852212
Experience 8, Iter 68, disc loss: 0.016222476390467023, policy loss: 5.209421301743155
Experience 8, Iter 69, disc loss: 0.016101647098297624, policy loss: 5.088386390505878
Experience 8, Iter 70, disc loss: 0.01671227811682953, policy loss: 5.127338794300223
Experience 8, Iter 71, disc loss: 0.016041360778329163, policy loss: 5.3140281029217
Experience 8, Iter 72, disc loss: 0.017092128352576945, policy loss: 4.853102213633489
Experience 8, Iter 73, disc loss: 0.01583483479801607, policy loss: 5.227037609084867
Experience 8, Iter 74, disc loss: 0.015218642863435865, policy loss: 5.539930693424428
Experience 8, Iter 75, disc loss: 0.015308966196099923, policy loss: 5.340952553209455
Experience 8, Iter 76, disc loss: 0.014774397806390204, policy loss: 5.5674296204356555
Experience 8, Iter 77, disc loss: 0.015679743717494143, policy loss: 5.902681062062074
Experience 8, Iter 78, disc loss: 0.015220401209136265, policy loss: 5.331425015335041
Experience 8, Iter 79, disc loss: 0.014895085497482773, policy loss: 5.663986030728158
Experience 8, Iter 80, disc loss: 0.014580537995435726, policy loss: 5.817623418635405
Experience 8, Iter 81, disc loss: 0.014600516395117717, policy loss: 5.438093170890508
Experience 8, Iter 82, disc loss: 0.013185242234698115, policy loss: 5.770685828264059
Experience 8, Iter 83, disc loss: 0.014025131867228313, policy loss: 5.470268150165432
Experience 8, Iter 84, disc loss: 0.014738341973033265, policy loss: 5.291634189636996
Experience 8, Iter 85, disc loss: 0.015032615785876755, policy loss: 5.1422205780743635
Experience 8, Iter 86, disc loss: 0.01425896780589041, policy loss: 5.273340375596849
Experience 8, Iter 87, disc loss: 0.013924594360193126, policy loss: 5.676419059306401
Experience 8, Iter 88, disc loss: 0.014454624361826832, policy loss: 5.204995556562688
Experience 8, Iter 89, disc loss: 0.013690142239688147, policy loss: 5.2090474019875534
Experience 8, Iter 90, disc loss: 0.015041704181529602, policy loss: 5.013394394427945
Experience 8, Iter 91, disc loss: 0.013514628725244152, policy loss: 5.348101319418679
Experience 8, Iter 92, disc loss: 0.013925256780452599, policy loss: 5.613580278866474
Experience 8, Iter 93, disc loss: 0.013283586461501684, policy loss: 5.496322453171753
Experience 8, Iter 94, disc loss: 0.012058190090454399, policy loss: 5.708472159032125
Experience 8, Iter 95, disc loss: 0.012931231649421848, policy loss: 5.874173213047895
Experience 8, Iter 96, disc loss: 0.012799092644643111, policy loss: 5.401498866011056
Experience 8, Iter 97, disc loss: 0.013665009146130048, policy loss: 5.261181955447382
Experience 8, Iter 98, disc loss: 0.013264107287816379, policy loss: 5.297516796254992
Experience 8, Iter 99, disc loss: 0.012935154742354254, policy loss: 5.907909423975344
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0089],
        [0.1542],
        [1.4185],
        [0.0326]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0547, 0.3494, 1.4992, 0.0246, 0.0223, 3.8577]],

        [[0.0547, 0.3494, 1.4992, 0.0246, 0.0223, 3.8577]],

        [[0.0547, 0.3494, 1.4992, 0.0246, 0.0223, 3.8577]],

        [[0.0547, 0.3494, 1.4992, 0.0246, 0.0223, 3.8577]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0358, 0.6166, 5.6741, 0.1303], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0358, 0.6166, 5.6741, 0.1303])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.770
Iter 2/2000 - Loss: 3.516
Iter 3/2000 - Loss: 3.476
Iter 4/2000 - Loss: 3.488
Iter 5/2000 - Loss: 3.424
Iter 6/2000 - Loss: 3.313
Iter 7/2000 - Loss: 3.216
Iter 8/2000 - Loss: 3.153
Iter 9/2000 - Loss: 3.096
Iter 10/2000 - Loss: 3.005
Iter 11/2000 - Loss: 2.878
Iter 12/2000 - Loss: 2.735
Iter 13/2000 - Loss: 2.592
Iter 14/2000 - Loss: 2.449
Iter 15/2000 - Loss: 2.296
Iter 16/2000 - Loss: 2.122
Iter 17/2000 - Loss: 1.924
Iter 18/2000 - Loss: 1.707
Iter 19/2000 - Loss: 1.476
Iter 20/2000 - Loss: 1.232
Iter 1981/2000 - Loss: -6.280
Iter 1982/2000 - Loss: -6.280
Iter 1983/2000 - Loss: -6.280
Iter 1984/2000 - Loss: -6.280
Iter 1985/2000 - Loss: -6.280
Iter 1986/2000 - Loss: -6.280
Iter 1987/2000 - Loss: -6.280
Iter 1988/2000 - Loss: -6.280
Iter 1989/2000 - Loss: -6.280
Iter 1990/2000 - Loss: -6.280
Iter 1991/2000 - Loss: -6.280
Iter 1992/2000 - Loss: -6.280
Iter 1993/2000 - Loss: -6.280
Iter 1994/2000 - Loss: -6.280
Iter 1995/2000 - Loss: -6.280
Iter 1996/2000 - Loss: -6.280
Iter 1997/2000 - Loss: -6.280
Iter 1998/2000 - Loss: -6.281
Iter 1999/2000 - Loss: -6.281
Iter 2000/2000 - Loss: -6.281
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[22.1490, 13.3653, 33.7380,  5.6916,  4.0897, 66.7027]],

        [[28.1499, 43.5263,  7.8068,  1.3906,  4.0892, 20.5641]],

        [[27.1980, 38.8775,  8.8499,  1.2873,  1.2421, 25.1700]],

        [[21.9315, 40.8628,  9.1890,  3.0714,  2.0406, 40.2565]]])
Signal Variance: tensor([ 0.3029,  2.0309, 26.4891,  0.4585])
Estimated target variance: tensor([0.0358, 0.6166, 5.6741, 0.1303])
N: 90
Signal to noise ratio: tensor([ 32.4383,  93.2227, 117.2091,  40.0502])
Bound on condition number: tensor([  94703.1468,  782143.5887, 1236418.7307,  144362.3391])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.013221643356523191, policy loss: 5.405766202154736
Experience 9, Iter 1, disc loss: 0.013155535827721311, policy loss: 5.875807849820733
Experience 9, Iter 2, disc loss: 0.013247360566838853, policy loss: 5.458118021415504
Experience 9, Iter 3, disc loss: 0.013858329957548743, policy loss: 5.379525866120876
Experience 9, Iter 4, disc loss: 0.013690026464245296, policy loss: 5.471571209570433
Experience 9, Iter 5, disc loss: 0.013149133374744547, policy loss: 5.909248705519266
Experience 9, Iter 6, disc loss: 0.013291126391830761, policy loss: 5.589031228088778
Experience 9, Iter 7, disc loss: 0.012843152114911736, policy loss: 5.781368057056608
Experience 9, Iter 8, disc loss: 0.012810903618444185, policy loss: 5.7953266425038015
Experience 9, Iter 9, disc loss: 0.012620132118168672, policy loss: 5.730446173230214
Experience 9, Iter 10, disc loss: 0.01338762068909019, policy loss: 5.350927254823015
Experience 9, Iter 11, disc loss: 0.0127100586546449, policy loss: 5.5381470112606
Experience 9, Iter 12, disc loss: 0.0129124966623126, policy loss: 5.42463578705739
Experience 9, Iter 13, disc loss: 0.013944964820427418, policy loss: 5.487652686566347
Experience 9, Iter 14, disc loss: 0.011617811074336157, policy loss: 5.92390901206997
Experience 9, Iter 15, disc loss: 0.012139917708214243, policy loss: 5.728816268224405
Experience 9, Iter 16, disc loss: 0.012195744846708222, policy loss: 5.702984089530968
Experience 9, Iter 17, disc loss: 0.012056308805418487, policy loss: 5.498942172017248
Experience 9, Iter 18, disc loss: 0.012531647276570787, policy loss: 5.538875307239753
Experience 9, Iter 19, disc loss: 0.01157316847650009, policy loss: 5.74950376629741
Experience 9, Iter 20, disc loss: 0.011394420812420227, policy loss: 5.823343548062297
Experience 9, Iter 21, disc loss: 0.012173311290154917, policy loss: 5.602547356647483
Experience 9, Iter 22, disc loss: 0.012333261810749301, policy loss: 5.449675748597717
Experience 9, Iter 23, disc loss: 0.01201340864949178, policy loss: 5.52010139797293
Experience 9, Iter 24, disc loss: 0.011079970617990091, policy loss: 5.709616290731951
Experience 9, Iter 25, disc loss: 0.011756905194320419, policy loss: 5.64041954303135
Experience 9, Iter 26, disc loss: 0.01096800659042661, policy loss: 6.222154571937359
Experience 9, Iter 27, disc loss: 0.01086576966483421, policy loss: 6.187240502167905
Experience 9, Iter 28, disc loss: 0.011232610994639864, policy loss: 5.650516427860914
Experience 9, Iter 29, disc loss: 0.011642089804565216, policy loss: 5.537508221160399
Experience 9, Iter 30, disc loss: 0.01096290755302602, policy loss: 5.791580339529749
Experience 9, Iter 31, disc loss: 0.011037377663873887, policy loss: 5.609111089279874
Experience 9, Iter 32, disc loss: 0.011542147721035933, policy loss: 5.625875441964259
Experience 9, Iter 33, disc loss: 0.011050398364105323, policy loss: 5.660247476621069
Experience 9, Iter 34, disc loss: 0.011207648553598695, policy loss: 5.796652591488557
Experience 9, Iter 35, disc loss: 0.011565649451728511, policy loss: 5.468234175701648
Experience 9, Iter 36, disc loss: 0.01012772566739591, policy loss: 6.008837258415804
Experience 9, Iter 37, disc loss: 0.011045354747474784, policy loss: 5.507464808199307
Experience 9, Iter 38, disc loss: 0.01029392046025762, policy loss: 5.9700971981058615
Experience 9, Iter 39, disc loss: 0.010281471495095947, policy loss: 5.685812916029979
Experience 9, Iter 40, disc loss: 0.009857234065055297, policy loss: 6.020250687787652
Experience 9, Iter 41, disc loss: 0.01022823232250843, policy loss: 6.018763345926651
Experience 9, Iter 42, disc loss: 0.010379199739444941, policy loss: 5.5692114567476025
Experience 9, Iter 43, disc loss: 0.011128061318443063, policy loss: 5.678224147683256
Experience 9, Iter 44, disc loss: 0.009793149926809578, policy loss: 5.895065705072897
Experience 9, Iter 45, disc loss: 0.011474205633450977, policy loss: 5.431255508603125
Experience 9, Iter 46, disc loss: 0.010603608571942416, policy loss: 5.469026498382766
Experience 9, Iter 47, disc loss: 0.010027731868809106, policy loss: 5.693468251230092
Experience 9, Iter 48, disc loss: 0.010326603535877717, policy loss: 5.7975614302634755
Experience 9, Iter 49, disc loss: 0.010633839030444616, policy loss: 5.928617102739098
Experience 9, Iter 50, disc loss: 0.010488064543433805, policy loss: 5.833746499024411
Experience 9, Iter 51, disc loss: 0.010240172167137317, policy loss: 5.6519123394217194
Experience 9, Iter 52, disc loss: 0.010565815836095234, policy loss: 5.699143433369027
Experience 9, Iter 53, disc loss: 0.009936249965666489, policy loss: 6.150653945426308
Experience 9, Iter 54, disc loss: 0.009874383503447267, policy loss: 5.873421880500315
Experience 9, Iter 55, disc loss: 0.010584970206009087, policy loss: 5.580237793356225
Experience 9, Iter 56, disc loss: 0.01001457227761142, policy loss: 6.089930359304888
Experience 9, Iter 57, disc loss: 0.010389509923455026, policy loss: 5.792458404600496
Experience 9, Iter 58, disc loss: 0.009179680448217233, policy loss: 6.26923302643567
Experience 9, Iter 59, disc loss: 0.01028580361538339, policy loss: 5.670559431351573
Experience 9, Iter 60, disc loss: 0.010125882403029623, policy loss: 5.708507851767919
Experience 9, Iter 61, disc loss: 0.010306805917601244, policy loss: 5.5364512293300026
Experience 9, Iter 62, disc loss: 0.009808096998108296, policy loss: 5.573464683303529
Experience 9, Iter 63, disc loss: 0.009792370901628019, policy loss: 6.09479488938722
Experience 9, Iter 64, disc loss: 0.009625460878517405, policy loss: 6.2262679369806975
Experience 9, Iter 65, disc loss: 0.00982820039362987, policy loss: 5.8125577749361605
Experience 9, Iter 66, disc loss: 0.009458099938512056, policy loss: 6.160856438026528
Experience 9, Iter 67, disc loss: 0.009323231509476646, policy loss: 6.066777946312784
Experience 9, Iter 68, disc loss: 0.009283951173477842, policy loss: 5.8675696453462205
Experience 9, Iter 69, disc loss: 0.009271763662545847, policy loss: 5.93709427444491
Experience 9, Iter 70, disc loss: 0.00901420172750938, policy loss: 6.058233611041556
Experience 9, Iter 71, disc loss: 0.009991545867578486, policy loss: 6.121525667565232
Experience 9, Iter 72, disc loss: 0.008984494468970409, policy loss: 6.138042942968494
Experience 9, Iter 73, disc loss: 0.010102070382186886, policy loss: 5.5388850933325555
Experience 9, Iter 74, disc loss: 0.00972070673435613, policy loss: 5.622679488674158
Experience 9, Iter 75, disc loss: 0.009221657901376199, policy loss: 6.098080061509434
Experience 9, Iter 76, disc loss: 0.008661311337844878, policy loss: 5.918661475481983
Experience 9, Iter 77, disc loss: 0.009514189543307858, policy loss: 5.774557661809728
Experience 9, Iter 78, disc loss: 0.009842144176377486, policy loss: 5.683259694312423
Experience 9, Iter 79, disc loss: 0.00863129155096436, policy loss: 6.235102955101995
Experience 9, Iter 80, disc loss: 0.009148924350897165, policy loss: 5.821911870841021
Experience 9, Iter 81, disc loss: 0.009248544054907808, policy loss: 6.584527458263036
Experience 9, Iter 82, disc loss: 0.009861105997061626, policy loss: 5.638249605534908
Experience 9, Iter 83, disc loss: 0.009183085267313259, policy loss: 5.909157952800474
Experience 9, Iter 84, disc loss: 0.008963282665121978, policy loss: 6.053154213051039
Experience 9, Iter 85, disc loss: 0.00889916517073691, policy loss: 6.327722294609962
Experience 9, Iter 86, disc loss: 0.008708571833377674, policy loss: 5.832406582556574
Experience 9, Iter 87, disc loss: 0.009269756054839495, policy loss: 5.813432214367124
Experience 9, Iter 88, disc loss: 0.009164971182235908, policy loss: 5.762654929911291
Experience 9, Iter 89, disc loss: 0.007879450965960051, policy loss: 6.500310653303011
Experience 9, Iter 90, disc loss: 0.008280029710543745, policy loss: 6.0756312086814805
Experience 9, Iter 91, disc loss: 0.008039124780780036, policy loss: 6.587152054075345
Experience 9, Iter 92, disc loss: 0.008548674506867502, policy loss: 6.510751086342396
Experience 9, Iter 93, disc loss: 0.008848979583777759, policy loss: 5.802792549588704
Experience 9, Iter 94, disc loss: 0.008070002653985626, policy loss: 6.0895475005721895
Experience 9, Iter 95, disc loss: 0.008270647795271514, policy loss: 6.096149560875208
Experience 9, Iter 96, disc loss: 0.007926172474555635, policy loss: 5.936947471921689
Experience 9, Iter 97, disc loss: 0.008348268649722953, policy loss: 5.937798256564621
Experience 9, Iter 98, disc loss: 0.008192561812431, policy loss: 6.321044544170462
Experience 9, Iter 99, disc loss: 0.008911970136704004, policy loss: 6.23114728777148
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0087],
        [0.1787],
        [1.5398],
        [0.0352]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0524, 0.3405, 1.6128, 0.0257, 0.0245, 4.3827]],

        [[0.0524, 0.3405, 1.6128, 0.0257, 0.0245, 4.3827]],

        [[0.0524, 0.3405, 1.6128, 0.0257, 0.0245, 4.3827]],

        [[0.0524, 0.3405, 1.6128, 0.0257, 0.0245, 4.3827]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0349, 0.7150, 6.1591, 0.1408], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0349, 0.7150, 6.1591, 0.1408])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.837
Iter 2/2000 - Loss: 3.575
Iter 3/2000 - Loss: 3.512
Iter 4/2000 - Loss: 3.487
Iter 5/2000 - Loss: 3.385
Iter 6/2000 - Loss: 3.241
Iter 7/2000 - Loss: 3.115
Iter 8/2000 - Loss: 3.020
Iter 9/2000 - Loss: 2.925
Iter 10/2000 - Loss: 2.797
Iter 11/2000 - Loss: 2.640
Iter 12/2000 - Loss: 2.472
Iter 13/2000 - Loss: 2.304
Iter 14/2000 - Loss: 2.133
Iter 15/2000 - Loss: 1.949
Iter 16/2000 - Loss: 1.747
Iter 17/2000 - Loss: 1.528
Iter 18/2000 - Loss: 1.296
Iter 19/2000 - Loss: 1.053
Iter 20/2000 - Loss: 0.800
Iter 1981/2000 - Loss: -6.552
Iter 1982/2000 - Loss: -6.552
Iter 1983/2000 - Loss: -6.552
Iter 1984/2000 - Loss: -6.552
Iter 1985/2000 - Loss: -6.552
Iter 1986/2000 - Loss: -6.552
Iter 1987/2000 - Loss: -6.552
Iter 1988/2000 - Loss: -6.552
Iter 1989/2000 - Loss: -6.553
Iter 1990/2000 - Loss: -6.553
Iter 1991/2000 - Loss: -6.553
Iter 1992/2000 - Loss: -6.553
Iter 1993/2000 - Loss: -6.553
Iter 1994/2000 - Loss: -6.553
Iter 1995/2000 - Loss: -6.553
Iter 1996/2000 - Loss: -6.553
Iter 1997/2000 - Loss: -6.553
Iter 1998/2000 - Loss: -6.553
Iter 1999/2000 - Loss: -6.553
Iter 2000/2000 - Loss: -6.553
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0002],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[19.9474, 12.9271, 33.9218,  4.7099,  3.9160, 68.6380]],

        [[26.5597, 41.6321,  7.6087,  1.4054,  3.6574, 19.5692]],

        [[26.7772, 37.7954,  8.7989,  1.2914,  1.2054, 24.3090]],

        [[19.4095, 38.3161,  9.1836,  2.7110,  1.9722, 37.5637]]])
Signal Variance: tensor([ 0.2866,  1.8736, 23.9035,  0.4359])
Estimated target variance: tensor([0.0349, 0.7150, 6.1591, 0.1408])
N: 100
Signal to noise ratio: tensor([ 30.0649,  94.0998, 113.0690,  40.5450])
Bound on condition number: tensor([  90390.9890,  885478.9019, 1278461.8011,  164391.0841])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.009146734023225536, policy loss: 5.668870626453055
Experience 10, Iter 1, disc loss: 0.007349847887239079, policy loss: 6.447658461832825
Experience 10, Iter 2, disc loss: 0.007752168180199697, policy loss: 6.203803499884948
Experience 10, Iter 3, disc loss: 0.007221525512117948, policy loss: 6.550129378706583
Experience 10, Iter 4, disc loss: 0.0070997102884980175, policy loss: 6.579222230723043
Experience 10, Iter 5, disc loss: 0.007837407989291292, policy loss: 6.0411380445856775
Experience 10, Iter 6, disc loss: 0.008029116007702671, policy loss: 5.96833571383749
Experience 10, Iter 7, disc loss: 0.007614244894688512, policy loss: 6.237030343021543
Experience 10, Iter 8, disc loss: 0.007559629136071272, policy loss: 5.8839768881002925
Experience 10, Iter 9, disc loss: 0.0073638337877862045, policy loss: 6.299485054905926
Experience 10, Iter 10, disc loss: 0.007137729880487994, policy loss: 6.1769575729183455
Experience 10, Iter 11, disc loss: 0.007638355783438337, policy loss: 5.965613322161673
Experience 10, Iter 12, disc loss: 0.007165623005502247, policy loss: 6.140857213968609
Experience 10, Iter 13, disc loss: 0.007354243298115766, policy loss: 6.560429689507911
Experience 10, Iter 14, disc loss: 0.006997976921013798, policy loss: 6.445082837855617
Experience 10, Iter 15, disc loss: 0.007684433697512528, policy loss: 5.888908972506049
Experience 10, Iter 16, disc loss: 0.007462066078151406, policy loss: 5.923338575117716
Experience 10, Iter 17, disc loss: 0.007739594526276059, policy loss: 6.008806272502932
Experience 10, Iter 18, disc loss: 0.006786980443440385, policy loss: 6.162634469989984
Experience 10, Iter 19, disc loss: 0.0072333536649280585, policy loss: 6.101407435594352
Experience 10, Iter 20, disc loss: 0.008108969074457536, policy loss: 5.716349787612607
Experience 10, Iter 21, disc loss: 0.006994795438284317, policy loss: 6.111574547553917
Experience 10, Iter 22, disc loss: 0.007229514730869727, policy loss: 6.4140786054574175
Experience 10, Iter 23, disc loss: 0.007204292727838015, policy loss: 6.16114685287007
Experience 10, Iter 24, disc loss: 0.007038306077309249, policy loss: 6.132153305128502
Experience 10, Iter 25, disc loss: 0.007417154338891181, policy loss: 6.153230802208179
Experience 10, Iter 26, disc loss: 0.0066677135142451105, policy loss: 6.270113471225018
Experience 10, Iter 27, disc loss: 0.0066145838347799225, policy loss: 6.2651033017076925
Experience 10, Iter 28, disc loss: 0.0065937840383771635, policy loss: 6.097266676940066
Experience 10, Iter 29, disc loss: 0.007115032593024032, policy loss: 6.041577222778738
Experience 10, Iter 30, disc loss: 0.0067188675368911755, policy loss: 6.124664670401967
Experience 10, Iter 31, disc loss: 0.006870608470417504, policy loss: 6.778041988275589
Experience 10, Iter 32, disc loss: 0.006913315253931129, policy loss: 6.080495154502216
Experience 10, Iter 33, disc loss: 0.006699598121806295, policy loss: 6.3124974644929726
Experience 10, Iter 34, disc loss: 0.006947023674373496, policy loss: 6.1670304298750915
Experience 10, Iter 35, disc loss: 0.007772528224836068, policy loss: 5.812506258855137
Experience 10, Iter 36, disc loss: 0.00652866762300685, policy loss: 6.557977494707293
Experience 10, Iter 37, disc loss: 0.006636769109831109, policy loss: 6.356061720962822
Experience 10, Iter 38, disc loss: 0.007300977714103516, policy loss: 5.952182718188525
Experience 10, Iter 39, disc loss: 0.006731807891412802, policy loss: 6.572124515774735
Experience 10, Iter 40, disc loss: 0.006631890852369035, policy loss: 6.36672663912049
Experience 10, Iter 41, disc loss: 0.007096109681567922, policy loss: 6.022450679165946
Experience 10, Iter 42, disc loss: 0.00675365126949634, policy loss: 6.543419467186208
Experience 10, Iter 43, disc loss: 0.007380311093527631, policy loss: 6.247843857345217
Experience 10, Iter 44, disc loss: 0.00694115097601241, policy loss: 6.0722449071353655
Experience 10, Iter 45, disc loss: 0.006654524049282009, policy loss: 6.28421235743323
Experience 10, Iter 46, disc loss: 0.00635144804297331, policy loss: 6.398658821805747
Experience 10, Iter 47, disc loss: 0.005952925272350794, policy loss: 6.785599523705803
Experience 10, Iter 48, disc loss: 0.006833203407147338, policy loss: 6.231737566210688
Experience 10, Iter 49, disc loss: 0.0064801174912949315, policy loss: 6.560929480627256
Experience 10, Iter 50, disc loss: 0.00611797066047484, policy loss: 6.320320060653806
Experience 10, Iter 51, disc loss: 0.00707722888780681, policy loss: 6.120006688018478
Experience 10, Iter 52, disc loss: 0.006582514212346912, policy loss: 6.150234710384623
Experience 10, Iter 53, disc loss: 0.005885838468600888, policy loss: 7.078066582061787
Experience 10, Iter 54, disc loss: 0.006758402800123247, policy loss: 6.1642538413058725
Experience 10, Iter 55, disc loss: 0.006555826621860548, policy loss: 6.3960762617192515
Experience 10, Iter 56, disc loss: 0.00656382740840767, policy loss: 6.203234749122904
Experience 10, Iter 57, disc loss: 0.00685662824605539, policy loss: 6.0405415087867125
Experience 10, Iter 58, disc loss: 0.00614343300759339, policy loss: 6.2534008522897135
Experience 10, Iter 59, disc loss: 0.006316563473312157, policy loss: 6.409633386464886
Experience 10, Iter 60, disc loss: 0.006343663837479209, policy loss: 6.217416181496811
Experience 10, Iter 61, disc loss: 0.006013912994381584, policy loss: 6.491581503130809
Experience 10, Iter 62, disc loss: 0.006104170235850838, policy loss: 6.454749913914276
Experience 10, Iter 63, disc loss: 0.006497758190945624, policy loss: 6.073804933483281
Experience 10, Iter 64, disc loss: 0.006495603934469773, policy loss: 6.292643344936287
Experience 10, Iter 65, disc loss: 0.00620665502404053, policy loss: 6.464928043083485
Experience 10, Iter 66, disc loss: 0.005926333735587805, policy loss: 6.516107877721859
Experience 10, Iter 67, disc loss: 0.0060638373107046, policy loss: 6.126601564410448
Experience 10, Iter 68, disc loss: 0.0053779332489178955, policy loss: 6.820931238541718
Experience 10, Iter 69, disc loss: 0.005514638454247682, policy loss: 6.643442951349222
Experience 10, Iter 70, disc loss: 0.005623914874810178, policy loss: 6.408213208534541
Experience 10, Iter 71, disc loss: 0.00574653499910506, policy loss: 6.38539579115165
Experience 10, Iter 72, disc loss: 0.005504304436469555, policy loss: 6.808439864119489
Experience 10, Iter 73, disc loss: 0.0052399094719068305, policy loss: 6.764368218765394
Experience 10, Iter 74, disc loss: 0.005337241601593409, policy loss: 6.7383266821798475
Experience 10, Iter 75, disc loss: 0.004546970489408152, policy loss: 6.7874575369923775
Experience 10, Iter 76, disc loss: 0.004360549171130554, policy loss: 7.345994365488582
Experience 10, Iter 77, disc loss: 0.005042008416170259, policy loss: 6.64604333220945
Experience 10, Iter 78, disc loss: 0.00545896220768116, policy loss: 6.342976659196209
Experience 10, Iter 79, disc loss: 0.00536562178650263, policy loss: 6.434368235006028
Experience 10, Iter 80, disc loss: 0.0037589640014001383, policy loss: 8.03155607180767
Experience 10, Iter 81, disc loss: 0.0052921992859496095, policy loss: 6.439877674224354
Experience 10, Iter 82, disc loss: 0.0039701543277607075, policy loss: 7.155418095740457
Experience 10, Iter 83, disc loss: 0.0032551492960192266, policy loss: 8.232835720867277
Experience 10, Iter 84, disc loss: 0.0032767280282000877, policy loss: 8.340893584012374
Experience 10, Iter 85, disc loss: 0.0034837954073312564, policy loss: 7.649606591111332
Experience 10, Iter 86, disc loss: 0.004361178485823852, policy loss: 7.149755616777594
Experience 10, Iter 87, disc loss: 0.005193080238897039, policy loss: 6.576833850299929
Experience 10, Iter 88, disc loss: 0.003570143729395091, policy loss: 7.848295573170244
Experience 10, Iter 89, disc loss: 0.005059678173940485, policy loss: 6.512296001655997
Experience 10, Iter 90, disc loss: 0.004591681511449956, policy loss: 6.528416392218049
Experience 10, Iter 91, disc loss: 0.004434785988418567, policy loss: 6.422376701393044
Experience 10, Iter 92, disc loss: 0.0041773554016811525, policy loss: 6.525673295117345
Experience 10, Iter 93, disc loss: 0.004581538593494667, policy loss: 6.50121184556619
Experience 10, Iter 94, disc loss: 0.004558108936251059, policy loss: 6.529679150719137
Experience 10, Iter 95, disc loss: 0.005374162120595076, policy loss: 6.281182673352081
Experience 10, Iter 96, disc loss: 0.005878347863431023, policy loss: 6.257701559812678
Experience 10, Iter 97, disc loss: 0.004532770977106648, policy loss: 7.15794416328189
Experience 10, Iter 98, disc loss: 0.005682051777138685, policy loss: 6.5831335642201365
Experience 10, Iter 99, disc loss: 0.0048069435249915625, policy loss: 6.71698776695265
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0086],
        [0.2037],
        [1.6820],
        [0.0389]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0491, 0.3369, 1.7665, 0.0272, 0.0264, 4.8693]],

        [[0.0491, 0.3369, 1.7665, 0.0272, 0.0264, 4.8693]],

        [[0.0491, 0.3369, 1.7665, 0.0272, 0.0264, 4.8693]],

        [[0.0491, 0.3369, 1.7665, 0.0272, 0.0264, 4.8693]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0342, 0.8149, 6.7280, 0.1557], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0342, 0.8149, 6.7280, 0.1557])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.961
Iter 2/2000 - Loss: 3.707
Iter 3/2000 - Loss: 3.634
Iter 4/2000 - Loss: 3.587
Iter 5/2000 - Loss: 3.477
Iter 6/2000 - Loss: 3.333
Iter 7/2000 - Loss: 3.205
Iter 8/2000 - Loss: 3.096
Iter 9/2000 - Loss: 2.975
Iter 10/2000 - Loss: 2.825
Iter 11/2000 - Loss: 2.654
Iter 12/2000 - Loss: 2.477
Iter 13/2000 - Loss: 2.296
Iter 14/2000 - Loss: 2.103
Iter 15/2000 - Loss: 1.892
Iter 16/2000 - Loss: 1.665
Iter 17/2000 - Loss: 1.426
Iter 18/2000 - Loss: 1.179
Iter 19/2000 - Loss: 0.927
Iter 20/2000 - Loss: 0.669
Iter 1981/2000 - Loss: -6.465
Iter 1982/2000 - Loss: -6.465
Iter 1983/2000 - Loss: -6.465
Iter 1984/2000 - Loss: -6.465
Iter 1985/2000 - Loss: -6.465
Iter 1986/2000 - Loss: -6.465
Iter 1987/2000 - Loss: -6.466
Iter 1988/2000 - Loss: -6.466
Iter 1989/2000 - Loss: -6.466
Iter 1990/2000 - Loss: -6.466
Iter 1991/2000 - Loss: -6.466
Iter 1992/2000 - Loss: -6.466
Iter 1993/2000 - Loss: -6.466
Iter 1994/2000 - Loss: -6.466
Iter 1995/2000 - Loss: -6.466
Iter 1996/2000 - Loss: -6.466
Iter 1997/2000 - Loss: -6.466
Iter 1998/2000 - Loss: -6.466
Iter 1999/2000 - Loss: -6.466
Iter 2000/2000 - Loss: -6.466
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[18.8305, 12.2681, 33.6323,  5.3064,  4.5653, 71.0614]],

        [[24.6362, 42.0951,  8.6156,  1.3741,  1.4864, 32.6041]],

        [[24.1153, 37.1208,  9.3248,  1.0772,  0.8343, 24.2138]],

        [[19.4934, 37.5187, 10.2451,  2.8759,  1.8660, 41.8434]]])
Signal Variance: tensor([ 0.2531,  3.4075, 19.8087,  0.4784])
Estimated target variance: tensor([0.0342, 0.8149, 6.7280, 0.1557])
N: 110
Signal to noise ratio: tensor([ 27.8983, 115.6910,  94.4717,  40.4749])
Bound on condition number: tensor([  85615.6891, 1472284.7919,  981739.4474,  180204.7440])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.0043151019187758315, policy loss: 7.753182816478571
Experience 11, Iter 1, disc loss: 0.0042008537324848515, policy loss: 7.065439914118656
Experience 11, Iter 2, disc loss: 0.004252182724262341, policy loss: 6.633871844035763
Experience 11, Iter 3, disc loss: 0.005018551541879707, policy loss: 6.399379636287322
Experience 11, Iter 4, disc loss: 0.00519736629915043, policy loss: 6.379768514675452
Experience 11, Iter 5, disc loss: 0.005648590276978903, policy loss: 6.1756187328311345
Experience 11, Iter 6, disc loss: 0.004351715186463875, policy loss: 7.079604918288085
Experience 11, Iter 7, disc loss: 0.005364217163272635, policy loss: 6.266610377055736
Experience 11, Iter 8, disc loss: 0.004877437314019204, policy loss: 7.098087411814112
Experience 11, Iter 9, disc loss: 0.004551759892471998, policy loss: 6.702806929127274
Experience 11, Iter 10, disc loss: 0.004481083714856806, policy loss: 6.967996058964143
Experience 11, Iter 11, disc loss: 0.00426881546256999, policy loss: 7.281753118421015
Experience 11, Iter 12, disc loss: 0.004734331718285189, policy loss: 7.0613896899531685
Experience 11, Iter 13, disc loss: 0.004797601205823126, policy loss: 6.743997180171471
Experience 11, Iter 14, disc loss: 0.005548313894909449, policy loss: 6.492248000025839
Experience 11, Iter 15, disc loss: 0.004718639034519886, policy loss: 7.569054690893901
Experience 11, Iter 16, disc loss: 0.005286677004926296, policy loss: 6.452034732182188
Experience 11, Iter 17, disc loss: 0.005661100659767387, policy loss: 6.3652521264263004
Experience 11, Iter 18, disc loss: 0.005522024451922992, policy loss: 6.4378803322130365
Experience 11, Iter 19, disc loss: 0.004814532092216711, policy loss: 6.650502411372187
Experience 11, Iter 20, disc loss: 0.004883405414829995, policy loss: 6.465615946039959
Experience 11, Iter 21, disc loss: 0.005766941631966822, policy loss: 6.134795454174705
Experience 11, Iter 22, disc loss: 0.005953226900635664, policy loss: 6.294665775955359
Experience 11, Iter 23, disc loss: 0.0046776225920275305, policy loss: 6.753435382657873
Experience 11, Iter 24, disc loss: 0.005088553476410797, policy loss: 6.571054609779255
Experience 11, Iter 25, disc loss: 0.005000752614485952, policy loss: 6.886363588677048
Experience 11, Iter 26, disc loss: 0.0047038922311781, policy loss: 6.795002171771288
Experience 11, Iter 27, disc loss: 0.004986586908932363, policy loss: 6.45173502774435
Experience 11, Iter 28, disc loss: 0.005340092585783282, policy loss: 6.551086629528434
Experience 11, Iter 29, disc loss: 0.005052520284303498, policy loss: 6.316581332401816
Experience 11, Iter 30, disc loss: 0.005651567425217682, policy loss: 6.258059998624082
Experience 11, Iter 31, disc loss: 0.004744050723488329, policy loss: 6.799859635378804
Experience 11, Iter 32, disc loss: 0.005357342527696346, policy loss: 6.832080081657771
Experience 11, Iter 33, disc loss: 0.0050859776141249265, policy loss: 6.320540342282921
Experience 11, Iter 34, disc loss: 0.005242148790426134, policy loss: 6.617191707546903
Experience 11, Iter 35, disc loss: 0.0050381839439905506, policy loss: 6.671996134597235
Experience 11, Iter 36, disc loss: 0.005220699431808747, policy loss: 6.497003694878666
Experience 11, Iter 37, disc loss: 0.005132031288731681, policy loss: 6.805623058567127
Experience 11, Iter 38, disc loss: 0.005446834437632526, policy loss: 6.6110819000648045
Experience 11, Iter 39, disc loss: 0.0048567709312951676, policy loss: 6.715707764714257
Experience 11, Iter 40, disc loss: 0.004971720074865422, policy loss: 6.763896366210801
Experience 11, Iter 41, disc loss: 0.004450555970770835, policy loss: 6.835688452289045
Experience 11, Iter 42, disc loss: 0.005033925081872872, policy loss: 6.513828203832399
Experience 11, Iter 43, disc loss: 0.0055598698696577705, policy loss: 6.251523469779822
Experience 11, Iter 44, disc loss: 0.0049967872028093665, policy loss: 6.822079621282873
Experience 11, Iter 45, disc loss: 0.005627378871321125, policy loss: 6.1107801653076255
Experience 11, Iter 46, disc loss: 0.005256898008857196, policy loss: 6.826841477165501
Experience 11, Iter 47, disc loss: 0.005250281492491024, policy loss: 6.247109748578161
Experience 11, Iter 48, disc loss: 0.004899477006068903, policy loss: 6.769171536598681
Experience 11, Iter 49, disc loss: 0.005083684434007821, policy loss: 6.53518502639205
Experience 11, Iter 50, disc loss: 0.004588408171666558, policy loss: 6.503490454512829
Experience 11, Iter 51, disc loss: 0.005172043577104504, policy loss: 6.506079554250065
Experience 11, Iter 52, disc loss: 0.004629278276750026, policy loss: 7.594223902456579
Experience 11, Iter 53, disc loss: 0.005375325142163045, policy loss: 6.713066036867859
Experience 11, Iter 54, disc loss: 0.0046700152523342815, policy loss: 6.944260963575411
Experience 11, Iter 55, disc loss: 0.00477930637362755, policy loss: 6.5141077343984755
Experience 11, Iter 56, disc loss: 0.004982469832475877, policy loss: 6.900101792541467
Experience 11, Iter 57, disc loss: 0.004737169600364634, policy loss: 6.672644713126737
Experience 11, Iter 58, disc loss: 0.004461807554571564, policy loss: 6.6348582653731345
Experience 11, Iter 59, disc loss: 0.005063675235436169, policy loss: 6.430043580756493
Experience 11, Iter 60, disc loss: 0.004973905587878643, policy loss: 6.293952043093147
Experience 11, Iter 61, disc loss: 0.004755538345149284, policy loss: 6.647948005435973
Experience 11, Iter 62, disc loss: 0.004556623520049163, policy loss: 6.82617648525582
Experience 11, Iter 63, disc loss: 0.004934663751871706, policy loss: 6.851674882219205
Experience 11, Iter 64, disc loss: 0.00466185095170049, policy loss: 7.506631778893466
Experience 11, Iter 65, disc loss: 0.00465012748188923, policy loss: 6.625538626922278
Experience 11, Iter 66, disc loss: 0.004816636896976803, policy loss: 6.822431422707329
Experience 11, Iter 67, disc loss: 0.004613073133228827, policy loss: 7.425614719295698
Experience 11, Iter 68, disc loss: 0.0043968468980141815, policy loss: 6.707977360354982
Experience 11, Iter 69, disc loss: 0.004665025401030722, policy loss: 6.730472548614425
Experience 11, Iter 70, disc loss: 0.00424640578596089, policy loss: 7.212953337110351
Experience 11, Iter 71, disc loss: 0.004473222979892946, policy loss: 6.614536410156564
Experience 11, Iter 72, disc loss: 0.004607675657729597, policy loss: 6.634311126711079
Experience 11, Iter 73, disc loss: 0.004381526900555698, policy loss: 6.708550960668447
Experience 11, Iter 74, disc loss: 0.004691408197429763, policy loss: 6.748288388210113
Experience 11, Iter 75, disc loss: 0.00425895061860994, policy loss: 6.576112516351834
Experience 11, Iter 76, disc loss: 0.004558728599478198, policy loss: 6.520030317659917
Experience 11, Iter 77, disc loss: 0.004384233768632758, policy loss: 6.655546690733999
Experience 11, Iter 78, disc loss: 0.004893665825819523, policy loss: 6.336503793421198
Experience 11, Iter 79, disc loss: 0.004301281494778738, policy loss: 6.552490084295483
Experience 11, Iter 80, disc loss: 0.004220161296411701, policy loss: 6.569231298929541
Experience 11, Iter 81, disc loss: 0.004170703069081878, policy loss: 6.536109046895194
Experience 11, Iter 82, disc loss: 0.004414506784893397, policy loss: 6.745873388263729
Experience 11, Iter 83, disc loss: 0.004433052764662316, policy loss: 6.639773028237221
Experience 11, Iter 84, disc loss: 0.0045784780300682695, policy loss: 6.474964123978571
Experience 11, Iter 85, disc loss: 0.004317142562156904, policy loss: 7.383499738454094
Experience 11, Iter 86, disc loss: 0.004354114200763653, policy loss: 6.633379754233566
Experience 11, Iter 87, disc loss: 0.004170613050074968, policy loss: 6.722525694051079
Experience 11, Iter 88, disc loss: 0.004530616389255835, policy loss: 6.483493948462767
Experience 11, Iter 89, disc loss: 0.004393494387065235, policy loss: 6.869635398134388
Experience 11, Iter 90, disc loss: 0.004314473407369769, policy loss: 6.5766149282941
Experience 11, Iter 91, disc loss: 0.0042968850212596645, policy loss: 6.708707855402493
Experience 11, Iter 92, disc loss: 0.003923955694760816, policy loss: 7.783816550968506
Experience 11, Iter 93, disc loss: 0.0046583508083101665, policy loss: 6.620400649513346
Experience 11, Iter 94, disc loss: 0.004381688081965757, policy loss: 6.758676481588978
Experience 11, Iter 95, disc loss: 0.00435657249925244, policy loss: 6.68806968945981
Experience 11, Iter 96, disc loss: 0.00422826815343006, policy loss: 6.753699981701099
Experience 11, Iter 97, disc loss: 0.004285216011824807, policy loss: 8.04537169507246
Experience 11, Iter 98, disc loss: 0.004296996713712231, policy loss: 6.793324919114649
Experience 11, Iter 99, disc loss: 0.004709746145484337, policy loss: 7.0092961783136545
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0083],
        [0.2202],
        [1.7740],
        [0.0407]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0461, 0.3300, 1.8493, 0.0281, 0.0275, 5.2024]],

        [[0.0461, 0.3300, 1.8493, 0.0281, 0.0275, 5.2024]],

        [[0.0461, 0.3300, 1.8493, 0.0281, 0.0275, 5.2024]],

        [[0.0461, 0.3300, 1.8493, 0.0281, 0.0275, 5.2024]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0334, 0.8807, 7.0959, 0.1626], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0334, 0.8807, 7.0959, 0.1626])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.920
Iter 2/2000 - Loss: 3.647
Iter 3/2000 - Loss: 3.571
Iter 4/2000 - Loss: 3.491
Iter 5/2000 - Loss: 3.355
Iter 6/2000 - Loss: 3.210
Iter 7/2000 - Loss: 3.079
Iter 8/2000 - Loss: 2.943
Iter 9/2000 - Loss: 2.785
Iter 10/2000 - Loss: 2.611
Iter 11/2000 - Loss: 2.430
Iter 12/2000 - Loss: 2.246
Iter 13/2000 - Loss: 2.053
Iter 14/2000 - Loss: 1.846
Iter 15/2000 - Loss: 1.621
Iter 16/2000 - Loss: 1.382
Iter 17/2000 - Loss: 1.134
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.629
Iter 20/2000 - Loss: 0.372
Iter 1981/2000 - Loss: -6.725
Iter 1982/2000 - Loss: -6.725
Iter 1983/2000 - Loss: -6.725
Iter 1984/2000 - Loss: -6.725
Iter 1985/2000 - Loss: -6.725
Iter 1986/2000 - Loss: -6.725
Iter 1987/2000 - Loss: -6.725
Iter 1988/2000 - Loss: -6.725
Iter 1989/2000 - Loss: -6.725
Iter 1990/2000 - Loss: -6.725
Iter 1991/2000 - Loss: -6.725
Iter 1992/2000 - Loss: -6.725
Iter 1993/2000 - Loss: -6.726
Iter 1994/2000 - Loss: -6.726
Iter 1995/2000 - Loss: -6.726
Iter 1996/2000 - Loss: -6.726
Iter 1997/2000 - Loss: -6.726
Iter 1998/2000 - Loss: -6.726
Iter 1999/2000 - Loss: -6.726
Iter 2000/2000 - Loss: -6.726
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[18.0869, 11.7412, 31.7543,  5.2671,  4.3838, 70.3222]],

        [[23.3031, 42.3739,  8.5890,  1.3611,  1.5480, 31.7154]],

        [[23.7817, 37.3043,  9.3065,  1.1190,  0.8860, 23.4271]],

        [[19.0147, 37.0626,  9.9249,  2.8253,  1.8460, 41.3158]]])
Signal Variance: tensor([ 0.2308,  3.3419, 19.2763,  0.4576])
Estimated target variance: tensor([0.0334, 0.8807, 7.0959, 0.1626])
N: 120
Signal to noise ratio: tensor([ 27.1627, 115.5171,  90.3529,  40.7415])
Bound on condition number: tensor([  88538.2885, 1601304.2361,  979638.8010,  199185.0550])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.0046106255604432475, policy loss: 6.9400397921928825
Experience 12, Iter 1, disc loss: 0.004142845094417002, policy loss: 7.446058646140628
Experience 12, Iter 2, disc loss: 0.004388287312668647, policy loss: 6.652220239375707
Experience 12, Iter 3, disc loss: 0.0040440155764534125, policy loss: 6.893007242325897
Experience 12, Iter 4, disc loss: 0.003988576004387892, policy loss: 7.1448500770134435
Experience 12, Iter 5, disc loss: 0.003856672383524484, policy loss: 7.587942894940737
Experience 12, Iter 6, disc loss: 0.004221388717489954, policy loss: 6.652029540854362
Experience 12, Iter 7, disc loss: 0.004116057413160876, policy loss: 6.694097049681343
Experience 12, Iter 8, disc loss: 0.004007393564576478, policy loss: 7.236037648230353
Experience 12, Iter 9, disc loss: 0.003943145386195921, policy loss: 6.816873385417086
Experience 12, Iter 10, disc loss: 0.003844257514633806, policy loss: 6.938628879500582
Experience 12, Iter 11, disc loss: 0.004081836886969175, policy loss: 6.567305453674061
Experience 12, Iter 12, disc loss: 0.003849966368911424, policy loss: 6.727468321873691
Experience 12, Iter 13, disc loss: 0.004494830511113025, policy loss: 6.396500311937482
Experience 12, Iter 14, disc loss: 0.003939850131737288, policy loss: 7.126290299799096
Experience 12, Iter 15, disc loss: 0.004319237882538395, policy loss: 6.425168618213975
Experience 12, Iter 16, disc loss: 0.0038101106334030134, policy loss: 6.953562001071524
Experience 12, Iter 17, disc loss: 0.004194609241986572, policy loss: 7.201402695853733
Experience 12, Iter 18, disc loss: 0.0039841700490368965, policy loss: 7.034221928050486
Experience 12, Iter 19, disc loss: 0.0042536139835852105, policy loss: 6.653670566379073
Experience 12, Iter 20, disc loss: 0.004021842706566952, policy loss: 6.853452859906765
Experience 12, Iter 21, disc loss: 0.004097223719681915, policy loss: 6.795047024921379
Experience 12, Iter 22, disc loss: 0.004082241521964447, policy loss: 6.943082601142395
Experience 12, Iter 23, disc loss: 0.003623876671573985, policy loss: 7.014856368706193
Experience 12, Iter 24, disc loss: 0.0037689194643031063, policy loss: 6.970631785074815
Experience 12, Iter 25, disc loss: 0.004026804865963924, policy loss: 6.543684570168115
Experience 12, Iter 26, disc loss: 0.0040223306570439, policy loss: 6.660722528097462
Experience 12, Iter 27, disc loss: 0.004101947384795296, policy loss: 6.596623598729806
Experience 12, Iter 28, disc loss: 0.004102002188000755, policy loss: 7.057033531482022
Experience 12, Iter 29, disc loss: 0.003901517269276522, policy loss: 6.603320746602217
Experience 12, Iter 30, disc loss: 0.0038216507570605343, policy loss: 6.712685478673148
Experience 12, Iter 31, disc loss: 0.0037388165835470126, policy loss: 6.953221888390822
Experience 12, Iter 32, disc loss: 0.003819411508987627, policy loss: 6.936352416761134
Experience 12, Iter 33, disc loss: 0.0037497092037492028, policy loss: 6.754498620947214
Experience 12, Iter 34, disc loss: 0.003748521498937499, policy loss: 6.976817572257772
Experience 12, Iter 35, disc loss: 0.003887495061828999, policy loss: 6.789753964195528
Experience 12, Iter 36, disc loss: 0.0038465492646291488, policy loss: 6.647112291904708
Experience 12, Iter 37, disc loss: 0.004027952670962498, policy loss: 6.720049095570741
Experience 12, Iter 38, disc loss: 0.003981919731781294, policy loss: 6.828028994930207
Experience 12, Iter 39, disc loss: 0.0034752584968749953, policy loss: 7.004477964692992
Experience 12, Iter 40, disc loss: 0.003839844275152019, policy loss: 6.881509311836352
Experience 12, Iter 41, disc loss: 0.0036548680480332587, policy loss: 7.071132855359584
Experience 12, Iter 42, disc loss: 0.00391443625270415, policy loss: 7.359255815725602
Experience 12, Iter 43, disc loss: 0.004132393105576473, policy loss: 7.122505743617976
Experience 12, Iter 44, disc loss: 0.0034610392941752752, policy loss: 7.077387909574959
Experience 12, Iter 45, disc loss: 0.003717566229622431, policy loss: 6.709373316286099
Experience 12, Iter 46, disc loss: 0.0034837642143289147, policy loss: 7.079922831363637
Experience 12, Iter 47, disc loss: 0.0034994938831391046, policy loss: 7.264130524103536
Experience 12, Iter 48, disc loss: 0.003611113840061297, policy loss: 7.090102537834324
Experience 12, Iter 49, disc loss: 0.0036198326040751327, policy loss: 7.38863018638744
Experience 12, Iter 50, disc loss: 0.003509527451199014, policy loss: 7.3561594798022885
Experience 12, Iter 51, disc loss: 0.0035109512183538905, policy loss: 6.953069277907561
Experience 12, Iter 52, disc loss: 0.003455968027750845, policy loss: 6.792286843840443
Experience 12, Iter 53, disc loss: 0.0037433075516100627, policy loss: 7.104641172778476
Experience 12, Iter 54, disc loss: 0.003709185887615892, policy loss: 7.016771392611821
Experience 12, Iter 55, disc loss: 0.0035565710299228434, policy loss: 7.456524037146343
Experience 12, Iter 56, disc loss: 0.0033780258837281373, policy loss: 7.3957062842939445
Experience 12, Iter 57, disc loss: 0.0035488290243141456, policy loss: 6.823403612986846
Experience 12, Iter 58, disc loss: 0.003775407519809314, policy loss: 6.96222411708248
Experience 12, Iter 59, disc loss: 0.003670289037246482, policy loss: 6.771756317401981
Experience 12, Iter 60, disc loss: 0.0037881422731905904, policy loss: 6.60329237398171
Experience 12, Iter 61, disc loss: 0.00343753416875049, policy loss: 6.83636000951953
Experience 12, Iter 62, disc loss: 0.0033091546252597866, policy loss: 6.994663276770763
Experience 12, Iter 63, disc loss: 0.0034104601598945084, policy loss: 7.374973760617463
Experience 12, Iter 64, disc loss: 0.0036562619820685303, policy loss: 7.923215047410545
Experience 12, Iter 65, disc loss: 0.0038262652240623808, policy loss: 7.193331922944937
Experience 12, Iter 66, disc loss: 0.0036687908080447326, policy loss: 6.869388917818329
Experience 12, Iter 67, disc loss: 0.003646941145559119, policy loss: 6.993453275734215
Experience 12, Iter 68, disc loss: 0.0032514295354522382, policy loss: 7.002393857330594
Experience 12, Iter 69, disc loss: 0.0034560431472431255, policy loss: 7.229045761865363
Experience 12, Iter 70, disc loss: 0.003350544940413539, policy loss: 6.771392901228074
Experience 12, Iter 71, disc loss: 0.0031232292243698985, policy loss: 7.627973572146632
Experience 12, Iter 72, disc loss: 0.003207020988522468, policy loss: 7.1463190859587575
Experience 12, Iter 73, disc loss: 0.003206366426922458, policy loss: 7.4072999887572175
Experience 12, Iter 74, disc loss: 0.003030903100961306, policy loss: 6.96350583713645
Experience 12, Iter 75, disc loss: 0.0034020383576539503, policy loss: 7.678580979860012
Experience 12, Iter 76, disc loss: 0.003141080889476019, policy loss: 7.09773372253102
Experience 12, Iter 77, disc loss: 0.003549323831882496, policy loss: 6.921704255038703
Experience 12, Iter 78, disc loss: 0.0035225093573201523, policy loss: 6.8031381721533215
Experience 12, Iter 79, disc loss: 0.0034829355306238183, policy loss: 7.4248846514466305
Experience 12, Iter 80, disc loss: 0.003720228269692178, policy loss: 6.683531646300011
Experience 12, Iter 81, disc loss: 0.0032061686264752746, policy loss: 7.237632281764556
Experience 12, Iter 82, disc loss: 0.003337578244175773, policy loss: 6.826539203384524
Experience 12, Iter 83, disc loss: 0.0036451997938232905, policy loss: 6.7350505984671845
Experience 12, Iter 84, disc loss: 0.003845699856706011, policy loss: 6.5579377032624535
Experience 12, Iter 85, disc loss: 0.0034943735238382742, policy loss: 6.665573314617751
Experience 12, Iter 86, disc loss: 0.0033134541036614206, policy loss: 7.677782926837527
Experience 12, Iter 87, disc loss: 0.0033714778714004578, policy loss: 7.402693345566327
Experience 12, Iter 88, disc loss: 0.0032009144441981323, policy loss: 7.0033827331933995
Experience 12, Iter 89, disc loss: 0.003322420196023254, policy loss: 6.9860834870519355
Experience 12, Iter 90, disc loss: 0.0035746554898444392, policy loss: 6.930904485281852
Experience 12, Iter 91, disc loss: 0.0034439393283510287, policy loss: 6.8692453385186205
Experience 12, Iter 92, disc loss: 0.003242461516028884, policy loss: 6.9637206596682235
Experience 12, Iter 93, disc loss: 0.003202368576255894, policy loss: 6.905661650744898
Experience 12, Iter 94, disc loss: 0.00325242838705137, policy loss: 6.885874765468948
Experience 12, Iter 95, disc loss: 0.00340273518472426, policy loss: 6.826444523784253
Experience 12, Iter 96, disc loss: 0.0036064486660234646, policy loss: 7.141247865009568
Experience 12, Iter 97, disc loss: 0.003605930554975834, policy loss: 6.698252004008879
Experience 12, Iter 98, disc loss: 0.003520788513002651, policy loss: 7.236739566495057
Experience 12, Iter 99, disc loss: 0.003302109013356325, policy loss: 6.807412754370909
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0081],
        [0.2334],
        [1.8697],
        [0.0420]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0431, 0.3228, 1.9228, 0.0292, 0.0278, 5.4738]],

        [[0.0431, 0.3228, 1.9228, 0.0292, 0.0278, 5.4738]],

        [[0.0431, 0.3228, 1.9228, 0.0292, 0.0278, 5.4738]],

        [[0.0431, 0.3228, 1.9228, 0.0292, 0.0278, 5.4738]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0325, 0.9336, 7.4786, 0.1679], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0325, 0.9336, 7.4786, 0.1679])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.994
Iter 2/2000 - Loss: 3.732
Iter 3/2000 - Loss: 3.660
Iter 4/2000 - Loss: 3.569
Iter 5/2000 - Loss: 3.434
Iter 6/2000 - Loss: 3.298
Iter 7/2000 - Loss: 3.173
Iter 8/2000 - Loss: 3.031
Iter 9/2000 - Loss: 2.861
Iter 10/2000 - Loss: 2.677
Iter 11/2000 - Loss: 2.490
Iter 12/2000 - Loss: 2.298
Iter 13/2000 - Loss: 2.094
Iter 14/2000 - Loss: 1.872
Iter 15/2000 - Loss: 1.631
Iter 16/2000 - Loss: 1.376
Iter 17/2000 - Loss: 1.117
Iter 18/2000 - Loss: 0.856
Iter 19/2000 - Loss: 0.594
Iter 20/2000 - Loss: 0.329
Iter 1981/2000 - Loss: -6.877
Iter 1982/2000 - Loss: -6.877
Iter 1983/2000 - Loss: -6.877
Iter 1984/2000 - Loss: -6.878
Iter 1985/2000 - Loss: -6.878
Iter 1986/2000 - Loss: -6.878
Iter 1987/2000 - Loss: -6.878
Iter 1988/2000 - Loss: -6.878
Iter 1989/2000 - Loss: -6.878
Iter 1990/2000 - Loss: -6.878
Iter 1991/2000 - Loss: -6.878
Iter 1992/2000 - Loss: -6.878
Iter 1993/2000 - Loss: -6.878
Iter 1994/2000 - Loss: -6.878
Iter 1995/2000 - Loss: -6.878
Iter 1996/2000 - Loss: -6.878
Iter 1997/2000 - Loss: -6.878
Iter 1998/2000 - Loss: -6.878
Iter 1999/2000 - Loss: -6.878
Iter 2000/2000 - Loss: -6.878
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[17.1307, 11.6799, 33.0377,  5.3066,  4.8199, 69.4139]],

        [[21.9798, 41.7630,  8.6136,  1.2959,  1.6513, 32.8437]],

        [[20.4510, 36.7429,  9.3823,  1.0894,  0.8359, 24.3389]],

        [[18.0173, 37.2427,  9.4915,  2.5100,  1.7467, 41.9947]]])
Signal Variance: tensor([ 0.2340,  3.2672, 18.7992,  0.4256])
Estimated target variance: tensor([0.0325, 0.9336, 7.4786, 0.1679])
N: 130
Signal to noise ratio: tensor([ 27.1746, 111.5774,  92.2595,  39.7532])
Bound on condition number: tensor([  96000.9651, 1618437.0192, 1106537.2886,  205442.5843])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.003555296184983147, policy loss: 6.993146054353186
Experience 13, Iter 1, disc loss: 0.003838304392428348, policy loss: 7.003068868805088
Experience 13, Iter 2, disc loss: 0.003627451991583516, policy loss: 6.8378046867163675
Experience 13, Iter 3, disc loss: 0.0033208758494426447, policy loss: 7.29277293187968
Experience 13, Iter 4, disc loss: 0.003407537010868795, policy loss: 7.056842564816918
Experience 13, Iter 5, disc loss: 0.003137555434393171, policy loss: 7.249515437091525
Experience 13, Iter 6, disc loss: 0.003641043444249262, policy loss: 7.259762641453001
Experience 13, Iter 7, disc loss: 0.003367619656283477, policy loss: 7.295043292208467
Experience 13, Iter 8, disc loss: 0.0031118640756148193, policy loss: 7.942781689667921
Experience 13, Iter 9, disc loss: 0.0034138040770841215, policy loss: 7.268057289418004
Experience 13, Iter 10, disc loss: 0.003441691187292791, policy loss: 6.959374790287832
Experience 13, Iter 11, disc loss: 0.0033448489167002843, policy loss: 7.475670969711219
Experience 13, Iter 12, disc loss: 0.00322599258320305, policy loss: 7.505280131233096
Experience 13, Iter 13, disc loss: 0.0033389765330843883, policy loss: 6.808000893109899
Experience 13, Iter 14, disc loss: 0.0031782232427416896, policy loss: 6.940415264324874
Experience 13, Iter 15, disc loss: 0.0029837939267621056, policy loss: 7.449485148553607
Experience 13, Iter 16, disc loss: 0.0031846578538080577, policy loss: 7.214433608425972
Experience 13, Iter 17, disc loss: 0.003592881792100316, policy loss: 6.66996840851977
Experience 13, Iter 18, disc loss: 0.0032500750823610445, policy loss: 7.053744924296828
Experience 13, Iter 19, disc loss: 0.003192053578103629, policy loss: 7.461650645519801
Experience 13, Iter 20, disc loss: 0.003395118297129355, policy loss: 6.723423191745823
Experience 13, Iter 21, disc loss: 0.003416701295273378, policy loss: 7.4868108352509894
Experience 13, Iter 22, disc loss: 0.003114053936307186, policy loss: 7.136293430664077
Experience 13, Iter 23, disc loss: 0.003078666569741984, policy loss: 7.026990717721796
Experience 13, Iter 24, disc loss: 0.0030825092151919036, policy loss: 7.3389483509501
Experience 13, Iter 25, disc loss: 0.003103518462878112, policy loss: 7.090863732782806
Experience 13, Iter 26, disc loss: 0.0031715559068052463, policy loss: 6.958336761739879
Experience 13, Iter 27, disc loss: 0.0031721479307392914, policy loss: 6.857913683484707
Experience 13, Iter 28, disc loss: 0.0031356024457739363, policy loss: 6.931319683124807
Experience 13, Iter 29, disc loss: 0.0030435409639123813, policy loss: 6.89342852397534
Experience 13, Iter 30, disc loss: 0.003024492077229341, policy loss: 7.113896393196522
Experience 13, Iter 31, disc loss: 0.003321355980969702, policy loss: 6.880588720368175
Experience 13, Iter 32, disc loss: 0.0032649838270769243, policy loss: 6.769145687596811
Experience 13, Iter 33, disc loss: 0.0031981549503798277, policy loss: 7.074721713180958
Experience 13, Iter 34, disc loss: 0.003070006485967144, policy loss: 7.251911850730286
Experience 13, Iter 35, disc loss: 0.003532532038130288, policy loss: 7.006393222411019
Experience 13, Iter 36, disc loss: 0.0033008748421201294, policy loss: 8.117860509802146
Experience 13, Iter 37, disc loss: 0.003103003551297284, policy loss: 6.805176994031362
Experience 13, Iter 38, disc loss: 0.0032884325500983505, policy loss: 7.006741046514032
Experience 13, Iter 39, disc loss: 0.003203377788745071, policy loss: 6.947700270880725
Experience 13, Iter 40, disc loss: 0.0030842160564205348, policy loss: 7.786212340655327
Experience 13, Iter 41, disc loss: 0.002939472923465747, policy loss: 7.802933769623039
Experience 13, Iter 42, disc loss: 0.0033264710602608346, policy loss: 7.330968160917177
Experience 13, Iter 43, disc loss: 0.0029862106045944214, policy loss: 7.066768881599282
Experience 13, Iter 44, disc loss: 0.0033943381988967994, policy loss: 6.709766757724476
Experience 13, Iter 45, disc loss: 0.003155842631114628, policy loss: 7.022003978417382
Experience 13, Iter 46, disc loss: 0.0032152308508821264, policy loss: 7.160874699076951
Experience 13, Iter 47, disc loss: 0.0032828068681515126, policy loss: 6.93010757069924
Experience 13, Iter 48, disc loss: 0.003223215315283681, policy loss: 7.399406549442578
Experience 13, Iter 49, disc loss: 0.0030778549915905164, policy loss: 6.898269997218791
Experience 13, Iter 50, disc loss: 0.002973002989475338, policy loss: 7.138851147809053
Experience 13, Iter 51, disc loss: 0.00273483597690697, policy loss: 7.983156698522262
Experience 13, Iter 52, disc loss: 0.002989390523086873, policy loss: 7.4948917776879584
Experience 13, Iter 53, disc loss: 0.00322974697330006, policy loss: 7.434113407529861
Experience 13, Iter 54, disc loss: 0.0031448924845796076, policy loss: 6.994282347215828
Experience 13, Iter 55, disc loss: 0.003267574027806772, policy loss: 6.825643836931373
Experience 13, Iter 56, disc loss: 0.0027443280201418264, policy loss: 7.913443391669545
Experience 13, Iter 57, disc loss: 0.0028889920683532674, policy loss: 7.474849270465434
Experience 13, Iter 58, disc loss: 0.0027619733029751736, policy loss: 7.60056770796991
Experience 13, Iter 59, disc loss: 0.0030893435697429564, policy loss: 7.4079381504576
Experience 13, Iter 60, disc loss: 0.0029431879449715826, policy loss: 7.216748846746341
Experience 13, Iter 61, disc loss: 0.0029401147484947646, policy loss: 7.320574752238312
Experience 13, Iter 62, disc loss: 0.0027851168549866967, policy loss: 7.027678885814877
Experience 13, Iter 63, disc loss: 0.0028144483141815, policy loss: 7.796152274876032
Experience 13, Iter 64, disc loss: 0.002841664725254007, policy loss: 7.2483566048517565
Experience 13, Iter 65, disc loss: 0.00293247975123238, policy loss: 7.7025430411824
Experience 13, Iter 66, disc loss: 0.0029735469498459715, policy loss: 7.037913955385219
Experience 13, Iter 67, disc loss: 0.0030328106386100515, policy loss: 7.054031172629768
Experience 13, Iter 68, disc loss: 0.0031809629558134786, policy loss: 6.8666964021755525
Experience 13, Iter 69, disc loss: 0.0028738834560261923, policy loss: 7.534898264075231
Experience 13, Iter 70, disc loss: 0.0028093899981684947, policy loss: 6.916675018698877
Experience 13, Iter 71, disc loss: 0.002803321931769965, policy loss: 8.003163839472688
Experience 13, Iter 72, disc loss: 0.0030718241416943263, policy loss: 6.949115035982804
Experience 13, Iter 73, disc loss: 0.0028555989556147895, policy loss: 7.055024945649583
Experience 13, Iter 74, disc loss: 0.0029313632770820225, policy loss: 6.910987316738668
Experience 13, Iter 75, disc loss: 0.0029048741659981523, policy loss: 7.706691226314368
Experience 13, Iter 76, disc loss: 0.003054967958220514, policy loss: 7.427430227596923
Experience 13, Iter 77, disc loss: 0.003065678794842105, policy loss: 7.883945977250907
Experience 13, Iter 78, disc loss: 0.002931095507537816, policy loss: 7.145203510185582
Experience 13, Iter 79, disc loss: 0.0027352889283005274, policy loss: 7.368639106591306
Experience 13, Iter 80, disc loss: 0.002893922255728161, policy loss: 7.553541108971241
Experience 13, Iter 81, disc loss: 0.002743046889031142, policy loss: 6.969892486613399
Experience 13, Iter 82, disc loss: 0.0027625587790172227, policy loss: 7.776889299267429
Experience 13, Iter 83, disc loss: 0.002253119743870106, policy loss: 7.427368902805287
Experience 13, Iter 84, disc loss: 0.0023522467816854934, policy loss: 7.4223117242897745
Experience 13, Iter 85, disc loss: 0.0028812311644405165, policy loss: 7.02808899754756
Experience 13, Iter 86, disc loss: 0.002795968176599102, policy loss: 7.651459721862031
Experience 13, Iter 87, disc loss: 0.0027183462357146023, policy loss: 7.693752357620932
Experience 13, Iter 88, disc loss: 0.0029625812855880182, policy loss: 6.941949183656231
Experience 13, Iter 89, disc loss: 0.0029112046500234306, policy loss: 7.853691320458793
Experience 13, Iter 90, disc loss: 0.0025404863419280716, policy loss: 7.481637833074435
Experience 13, Iter 91, disc loss: 0.0025991666434071886, policy loss: 7.764308224809055
Experience 13, Iter 92, disc loss: 0.002890021824957888, policy loss: 7.315284594323877
Experience 13, Iter 93, disc loss: 0.00278473319035502, policy loss: 7.383767936323047
Experience 13, Iter 94, disc loss: 0.0025814591254844075, policy loss: 7.595604714183173
Experience 13, Iter 95, disc loss: 0.002996758901831599, policy loss: 6.841585965695202
Experience 13, Iter 96, disc loss: 0.002378615256824223, policy loss: 7.545353345902389
Experience 13, Iter 97, disc loss: 0.0028302191410808433, policy loss: 7.057730436808587
Experience 13, Iter 98, disc loss: 0.002318817660689752, policy loss: 7.612289296399404
Experience 13, Iter 99, disc loss: 0.0026313732440041687, policy loss: 7.171981284172312
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0079],
        [0.2455],
        [1.9424],
        [0.0432]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0406, 0.3169, 1.9824, 0.0300, 0.0284, 5.7086]],

        [[0.0406, 0.3169, 1.9824, 0.0300, 0.0284, 5.7086]],

        [[0.0406, 0.3169, 1.9824, 0.0300, 0.0284, 5.7086]],

        [[0.0406, 0.3169, 1.9824, 0.0300, 0.0284, 5.7086]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0315, 0.9818, 7.7695, 0.1726], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0315, 0.9818, 7.7695, 0.1726])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.011
Iter 2/2000 - Loss: 3.743
Iter 3/2000 - Loss: 3.654
Iter 4/2000 - Loss: 3.539
Iter 5/2000 - Loss: 3.391
Iter 6/2000 - Loss: 3.251
Iter 7/2000 - Loss: 3.116
Iter 8/2000 - Loss: 2.956
Iter 9/2000 - Loss: 2.764
Iter 10/2000 - Loss: 2.560
Iter 11/2000 - Loss: 2.358
Iter 12/2000 - Loss: 2.154
Iter 13/2000 - Loss: 1.938
Iter 14/2000 - Loss: 1.703
Iter 15/2000 - Loss: 1.452
Iter 16/2000 - Loss: 1.189
Iter 17/2000 - Loss: 0.924
Iter 18/2000 - Loss: 0.659
Iter 19/2000 - Loss: 0.394
Iter 20/2000 - Loss: 0.128
Iter 1981/2000 - Loss: -7.041
Iter 1982/2000 - Loss: -7.041
Iter 1983/2000 - Loss: -7.041
Iter 1984/2000 - Loss: -7.041
Iter 1985/2000 - Loss: -7.041
Iter 1986/2000 - Loss: -7.041
Iter 1987/2000 - Loss: -7.041
Iter 1988/2000 - Loss: -7.041
Iter 1989/2000 - Loss: -7.041
Iter 1990/2000 - Loss: -7.041
Iter 1991/2000 - Loss: -7.041
Iter 1992/2000 - Loss: -7.041
Iter 1993/2000 - Loss: -7.041
Iter 1994/2000 - Loss: -7.042
Iter 1995/2000 - Loss: -7.042
Iter 1996/2000 - Loss: -7.042
Iter 1997/2000 - Loss: -7.042
Iter 1998/2000 - Loss: -7.042
Iter 1999/2000 - Loss: -7.042
Iter 2000/2000 - Loss: -7.042
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[16.4339, 11.4283, 31.5620,  4.9966,  4.5022, 69.4999]],

        [[21.0000, 41.2971,  8.5346,  1.2808,  1.6194, 32.1907]],

        [[17.6650, 37.0304,  9.3048,  1.1246,  0.9009, 24.2975]],

        [[17.6978, 36.9118,  9.3363,  2.5420,  1.7596, 43.4559]]])
Signal Variance: tensor([ 0.2208,  3.1374, 19.8747,  0.4323])
Estimated target variance: tensor([0.0315, 0.9818, 7.7695, 0.1726])
N: 140
Signal to noise ratio: tensor([ 26.9410, 110.3013,  90.4132,  40.5635])
Bound on condition number: tensor([ 101615.3409, 1703293.3543, 1144436.6488,  230356.1399])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0026583193749602483, policy loss: 7.398880086595146
Experience 14, Iter 1, disc loss: 0.0026050592228735814, policy loss: 7.555750547560349
Experience 14, Iter 2, disc loss: 0.002690921677446858, policy loss: 7.750806672063665
Experience 14, Iter 3, disc loss: 0.0027409909240131303, policy loss: 7.090277490067675
Experience 14, Iter 4, disc loss: 0.002823942204218499, policy loss: 7.319283223809265
Experience 14, Iter 5, disc loss: 0.0024544912023953544, policy loss: 7.287762912664993
Experience 14, Iter 6, disc loss: 0.002641627942016036, policy loss: 7.666046194443135
Experience 14, Iter 7, disc loss: 0.0024087022404768897, policy loss: 7.586612392756545
Experience 14, Iter 8, disc loss: 0.0024816432622677027, policy loss: 7.33897513221931
Experience 14, Iter 9, disc loss: 0.0027968203996891717, policy loss: 7.185962583342736
Experience 14, Iter 10, disc loss: 0.0024925202838026704, policy loss: 7.514676718268205
Experience 14, Iter 11, disc loss: 0.0026611921850917815, policy loss: 7.179430405680951
Experience 14, Iter 12, disc loss: 0.0023945826735335507, policy loss: 7.5070170458349255
Experience 14, Iter 13, disc loss: 0.0026237341422002767, policy loss: 7.153279096332287
Experience 14, Iter 14, disc loss: 0.002401555452796558, policy loss: 7.8444006061811375
Experience 14, Iter 15, disc loss: 0.002563895471614796, policy loss: 7.106341040798574
Experience 14, Iter 16, disc loss: 0.002625519434632499, policy loss: 7.004091182402586
Experience 14, Iter 17, disc loss: 0.002437563358200866, policy loss: 7.178440813981542
Experience 14, Iter 18, disc loss: 0.002442652363345185, policy loss: 7.142016933950481
Experience 14, Iter 19, disc loss: 0.002762136656945458, policy loss: 6.90922919467202
Experience 14, Iter 20, disc loss: 0.0025698286826790645, policy loss: 7.735198129566815
Experience 14, Iter 21, disc loss: 0.0025698380300929295, policy loss: 7.280557206165566
Experience 14, Iter 22, disc loss: 0.0029937502262013895, policy loss: 7.144026675374702
Experience 14, Iter 23, disc loss: 0.0027127582114013357, policy loss: 7.316366994860669
Experience 14, Iter 24, disc loss: 0.002466244088671166, policy loss: 8.375528351589315
Experience 14, Iter 25, disc loss: 0.0023682071432470113, policy loss: 7.704282131494001
Experience 14, Iter 26, disc loss: 0.0024323333146733317, policy loss: 7.118119025701082
Experience 14, Iter 27, disc loss: 0.0026744428491732317, policy loss: 7.1498930132407
Experience 14, Iter 28, disc loss: 0.002503931500292515, policy loss: 7.463219654335236
Experience 14, Iter 29, disc loss: 0.002530191014761648, policy loss: 7.658112263314123
Experience 14, Iter 30, disc loss: 0.00253405541201924, policy loss: 7.357353010082911
Experience 14, Iter 31, disc loss: 0.0026214117986124346, policy loss: 7.355212386138994
Experience 14, Iter 32, disc loss: 0.002471956666838437, policy loss: 7.6818416053149985
Experience 14, Iter 33, disc loss: 0.0023230523947624206, policy loss: 7.433955503007274
Experience 14, Iter 34, disc loss: 0.002364492112145697, policy loss: 7.882630965431003
Experience 14, Iter 35, disc loss: 0.002616317996224979, policy loss: 7.294737435433598
Experience 14, Iter 36, disc loss: 0.002467371882007561, policy loss: 7.484346871871621
Experience 14, Iter 37, disc loss: 0.0026201795408221547, policy loss: 7.578718658418436
Experience 14, Iter 38, disc loss: 0.0025363183753671775, policy loss: 7.3133868084988105
Experience 14, Iter 39, disc loss: 0.002339645700144879, policy loss: 7.429926200506018
Experience 14, Iter 40, disc loss: 0.0025161053605873658, policy loss: 7.805112774392699
Experience 14, Iter 41, disc loss: 0.0023745000244417324, policy loss: 7.506765892125644
Experience 14, Iter 42, disc loss: 0.002819412313191905, policy loss: 7.055196644502699
Experience 14, Iter 43, disc loss: 0.0024962599192361306, policy loss: 7.227636275745525
Experience 14, Iter 44, disc loss: 0.002162993605554678, policy loss: 8.023383727066253
Experience 14, Iter 45, disc loss: 0.0026142219672488282, policy loss: 7.30461464184976
Experience 14, Iter 46, disc loss: 0.002386095425985562, policy loss: 8.59948610499946
Experience 14, Iter 47, disc loss: 0.0027834037707830453, policy loss: 7.015083944535851
Experience 14, Iter 48, disc loss: 0.0022878132262534414, policy loss: 7.795366545150689
Experience 14, Iter 49, disc loss: 0.002193532022353115, policy loss: 7.4664193779162416
Experience 14, Iter 50, disc loss: 0.002471066729634652, policy loss: 7.387060675439307
Experience 14, Iter 51, disc loss: 0.002344466420023931, policy loss: 7.880524781876627
Experience 14, Iter 52, disc loss: 0.0024926714843031927, policy loss: 7.239522422103726
Experience 14, Iter 53, disc loss: 0.002516763237151784, policy loss: 7.251237543328702
Experience 14, Iter 54, disc loss: 0.002534881659982504, policy loss: 7.628599715893749
Experience 14, Iter 55, disc loss: 0.0021721002916235636, policy loss: 7.619538845754767
Experience 14, Iter 56, disc loss: 0.002131413509348843, policy loss: 7.440033164563255
Experience 14, Iter 57, disc loss: 0.0020166258716436468, policy loss: 7.930522030213284
Experience 14, Iter 58, disc loss: 0.0020930360347428422, policy loss: 7.898802584946446
Experience 14, Iter 59, disc loss: 0.00232694039617014, policy loss: 7.509344167843844
Experience 14, Iter 60, disc loss: 0.0023238422125026665, policy loss: 7.536389731181896
Experience 14, Iter 61, disc loss: 0.002387680549261324, policy loss: 7.461159034177326
Experience 14, Iter 62, disc loss: 0.002488894766561318, policy loss: 7.804246231319661
Experience 14, Iter 63, disc loss: 0.002074151816884177, policy loss: 7.39173315245718
Experience 14, Iter 64, disc loss: 0.0021338135485245505, policy loss: 7.652641457650185
Experience 14, Iter 65, disc loss: 0.0022878346108596566, policy loss: 7.2175976442453695
Experience 14, Iter 66, disc loss: 0.0024202113619277207, policy loss: 7.315778586176551
Experience 14, Iter 67, disc loss: 0.0024535428666936305, policy loss: 7.937677556479088
Experience 14, Iter 68, disc loss: 0.0021019925693855253, policy loss: 7.744659928370866
Experience 14, Iter 69, disc loss: 0.00227899998690776, policy loss: 7.616687083785383
Experience 14, Iter 70, disc loss: 0.0025249315850105246, policy loss: 7.365333623907223
Experience 14, Iter 71, disc loss: 0.0021968252784060277, policy loss: 7.637626408413898
Experience 14, Iter 72, disc loss: 0.0023984092649907028, policy loss: 7.530093629354612
Experience 14, Iter 73, disc loss: 0.0022153002546617803, policy loss: 8.029211017737397
Experience 14, Iter 74, disc loss: 0.0022948590697330004, policy loss: 7.4344997622789455
Experience 14, Iter 75, disc loss: 0.0023390213850684803, policy loss: 8.12860318913004
Experience 14, Iter 76, disc loss: 0.0023550497570919626, policy loss: 7.088217274437888
Experience 14, Iter 77, disc loss: 0.002123685318284256, policy loss: 7.704242831497153
Experience 14, Iter 78, disc loss: 0.002129176445492635, policy loss: 7.593545473461785
Experience 14, Iter 79, disc loss: 0.0021213521942579977, policy loss: 7.993374745716289
Experience 14, Iter 80, disc loss: 0.002465560197779868, policy loss: 7.200514490249662
Experience 14, Iter 81, disc loss: 0.0020777078559340014, policy loss: 7.788335435079837
Experience 14, Iter 82, disc loss: 0.0022768162996072584, policy loss: 7.308363337922698
Experience 14, Iter 83, disc loss: 0.0022443304553530774, policy loss: 7.171510964755523
Experience 14, Iter 84, disc loss: 0.002211668220041277, policy loss: 7.268273005273475
Experience 14, Iter 85, disc loss: 0.0018354136527812684, policy loss: 8.69499503356925
Experience 14, Iter 86, disc loss: 0.002059562305314471, policy loss: 8.234050296086714
Experience 14, Iter 87, disc loss: 0.0020580648451462595, policy loss: 7.497676858005109
Experience 14, Iter 88, disc loss: 0.0022950241278068763, policy loss: 7.315153945592889
Experience 14, Iter 89, disc loss: 0.002390171205554014, policy loss: 7.192092660375508
Experience 14, Iter 90, disc loss: 0.0023364833552671598, policy loss: 7.219829363874552
Experience 14, Iter 91, disc loss: 0.0023644813391421908, policy loss: 7.515090917996398
Experience 14, Iter 92, disc loss: 0.0022423710235490463, policy loss: 7.975862720607944
Experience 14, Iter 93, disc loss: 0.0024677150792345656, policy loss: 7.902921514784914
Experience 14, Iter 94, disc loss: 0.00227781636094736, policy loss: 7.458118886832301
Experience 14, Iter 95, disc loss: 0.002060881162074452, policy loss: 7.573099980724199
Experience 14, Iter 96, disc loss: 0.00201106775646806, policy loss: 7.6445919222825145
Experience 14, Iter 97, disc loss: 0.0021822807551208166, policy loss: 7.435780248832444
Experience 14, Iter 98, disc loss: 0.0020521415327235206, policy loss: 7.829978287750908
Experience 14, Iter 99, disc loss: 0.002378504580865315, policy loss: 7.2155349927482675
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0078],
        [0.2581],
        [2.0199],
        [0.0446]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0384, 0.3129, 2.0529, 0.0307, 0.0289, 5.9360]],

        [[0.0384, 0.3129, 2.0529, 0.0307, 0.0289, 5.9360]],

        [[0.0384, 0.3129, 2.0529, 0.0307, 0.0289, 5.9360]],

        [[0.0384, 0.3129, 2.0529, 0.0307, 0.0289, 5.9360]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0310, 1.0325, 8.0796, 0.1784], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0310, 1.0325, 8.0796, 0.1784])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.010
Iter 2/2000 - Loss: 3.736
Iter 3/2000 - Loss: 3.632
Iter 4/2000 - Loss: 3.498
Iter 5/2000 - Loss: 3.339
Iter 6/2000 - Loss: 3.193
Iter 7/2000 - Loss: 3.050
Iter 8/2000 - Loss: 2.875
Iter 9/2000 - Loss: 2.667
Iter 10/2000 - Loss: 2.452
Iter 11/2000 - Loss: 2.240
Iter 12/2000 - Loss: 2.029
Iter 13/2000 - Loss: 1.807
Iter 14/2000 - Loss: 1.569
Iter 15/2000 - Loss: 1.315
Iter 16/2000 - Loss: 1.053
Iter 17/2000 - Loss: 0.788
Iter 18/2000 - Loss: 0.524
Iter 19/2000 - Loss: 0.261
Iter 20/2000 - Loss: -0.003
Iter 1981/2000 - Loss: -7.145
Iter 1982/2000 - Loss: -7.145
Iter 1983/2000 - Loss: -7.146
Iter 1984/2000 - Loss: -7.146
Iter 1985/2000 - Loss: -7.146
Iter 1986/2000 - Loss: -7.146
Iter 1987/2000 - Loss: -7.146
Iter 1988/2000 - Loss: -7.146
Iter 1989/2000 - Loss: -7.146
Iter 1990/2000 - Loss: -7.146
Iter 1991/2000 - Loss: -7.146
Iter 1992/2000 - Loss: -7.146
Iter 1993/2000 - Loss: -7.146
Iter 1994/2000 - Loss: -7.146
Iter 1995/2000 - Loss: -7.146
Iter 1996/2000 - Loss: -7.146
Iter 1997/2000 - Loss: -7.146
Iter 1998/2000 - Loss: -7.146
Iter 1999/2000 - Loss: -7.146
Iter 2000/2000 - Loss: -7.146
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[16.1010, 11.7018, 32.8325,  4.5399,  4.3269, 71.1586]],

        [[21.5366, 41.1600,  8.3792,  1.2944,  1.6364, 31.3776]],

        [[22.8458, 35.5185,  9.3537,  1.0787,  0.8153, 24.1643]],

        [[17.6215, 36.2459,  9.4652,  2.5508,  1.7809, 42.5871]]])
Signal Variance: tensor([ 0.2291,  3.0107, 17.7769,  0.4283])
Estimated target variance: tensor([0.0310, 1.0325, 8.0796, 0.1784])
N: 150
Signal to noise ratio: tensor([ 27.6079, 109.0154,  81.1845,  39.6561])
Bound on condition number: tensor([ 114330.6285, 1782654.2583,  988640.2344,  235891.6011])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.002249024719652574, policy loss: 7.266997568564939
Experience 15, Iter 1, disc loss: 0.0019748772085919327, policy loss: 7.757844024851616
Experience 15, Iter 2, disc loss: 0.0018200098951680539, policy loss: 8.314346994806257
Experience 15, Iter 3, disc loss: 0.0019416249676395792, policy loss: 7.882619631977573
Experience 15, Iter 4, disc loss: 0.002022564798621384, policy loss: 7.314961632194365
Experience 15, Iter 5, disc loss: 0.002168580125940228, policy loss: 7.700484101078519
Experience 15, Iter 6, disc loss: 0.002272609560841493, policy loss: 7.375380245221742
Experience 15, Iter 7, disc loss: 0.0021458419627271553, policy loss: 7.744213814323474
Experience 15, Iter 8, disc loss: 0.0018587345715357362, policy loss: 8.357708357839105
Experience 15, Iter 9, disc loss: 0.0019187938789529929, policy loss: 7.73906572574509
Experience 15, Iter 10, disc loss: 0.0018887601665406551, policy loss: 7.677549183625573
Experience 15, Iter 11, disc loss: 0.0020644573279833102, policy loss: 8.067956419362124
Experience 15, Iter 12, disc loss: 0.0020321481655899134, policy loss: 7.59594586304646
Experience 15, Iter 13, disc loss: 0.002219739684667905, policy loss: 7.258363662475164
Experience 15, Iter 14, disc loss: 0.0021944753231533084, policy loss: 7.690631185768457
Experience 15, Iter 15, disc loss: 0.0021536540119208476, policy loss: 7.623426084356211
Experience 15, Iter 16, disc loss: 0.0020985159654093107, policy loss: 7.851829966006241
Experience 15, Iter 17, disc loss: 0.0021001225403572037, policy loss: 7.2708555601474645
Experience 15, Iter 18, disc loss: 0.002067469321761453, policy loss: 8.820573145018221
Experience 15, Iter 19, disc loss: 0.0019475185284266107, policy loss: 7.670414508887756
Experience 15, Iter 20, disc loss: 0.00193034989754259, policy loss: 8.008535543901218
Experience 15, Iter 21, disc loss: 0.0021117930509482784, policy loss: 8.000273433143898
Experience 15, Iter 22, disc loss: 0.0020583036012025353, policy loss: 7.368748949451516
Experience 15, Iter 23, disc loss: 0.0022704464581929833, policy loss: 7.644760030078606
Experience 15, Iter 24, disc loss: 0.001926781694835896, policy loss: 7.793444785061145
Experience 15, Iter 25, disc loss: 0.0021577046847582417, policy loss: 7.809823775318765
Experience 15, Iter 26, disc loss: 0.002257701109041982, policy loss: 7.785155886198982
Experience 15, Iter 27, disc loss: 0.0019550205526800966, policy loss: 7.516535386960937
Experience 15, Iter 28, disc loss: 0.0018195354503507384, policy loss: 7.902821761319502
Experience 15, Iter 29, disc loss: 0.001883609820095307, policy loss: 7.726965690520927
Experience 15, Iter 30, disc loss: 0.0020968934893218767, policy loss: 7.457477592231589
Experience 15, Iter 31, disc loss: 0.0018687302171895939, policy loss: 7.921736482702737
Experience 15, Iter 32, disc loss: 0.0020980042188238138, policy loss: 7.599484588238485
Experience 15, Iter 33, disc loss: 0.0018858376887107026, policy loss: 7.692199114209921
Experience 15, Iter 34, disc loss: 0.001907815418738606, policy loss: 8.112726954737212
Experience 15, Iter 35, disc loss: 0.001981557194929178, policy loss: 8.013416879785998
Experience 15, Iter 36, disc loss: 0.0018066343436871405, policy loss: 7.992673273586837
Experience 15, Iter 37, disc loss: 0.0021049523210620877, policy loss: 7.886292657333258
Experience 15, Iter 38, disc loss: 0.0019554818196464633, policy loss: 7.424347275313887
Experience 15, Iter 39, disc loss: 0.002071965310948792, policy loss: 7.368565690039771
Experience 15, Iter 40, disc loss: 0.0019904659229013242, policy loss: 7.496002012329625
Experience 15, Iter 41, disc loss: 0.001885421803247012, policy loss: 8.039942516027303
Experience 15, Iter 42, disc loss: 0.0018289674250304548, policy loss: 7.579793930644294
Experience 15, Iter 43, disc loss: 0.002019748253875797, policy loss: 7.441464953837708
Experience 15, Iter 44, disc loss: 0.0018801285801012988, policy loss: 7.967365116423151
Experience 15, Iter 45, disc loss: 0.001925201338763842, policy loss: 7.430160060342073
Experience 15, Iter 46, disc loss: 0.0020230080056799606, policy loss: 7.5385578637824455
Experience 15, Iter 47, disc loss: 0.0019968767282346166, policy loss: 7.552798081618809
Experience 15, Iter 48, disc loss: 0.0018549216305555247, policy loss: 7.763377108196723
Experience 15, Iter 49, disc loss: 0.002166789406061618, policy loss: 7.224636874949166
Experience 15, Iter 50, disc loss: 0.0017524382470612424, policy loss: 8.231525296169101
Experience 15, Iter 51, disc loss: 0.0017090426699454698, policy loss: 7.933958962397343
Experience 15, Iter 52, disc loss: 0.0019408059791786478, policy loss: 8.595096768387817
Experience 15, Iter 53, disc loss: 0.0021321701133041734, policy loss: 7.279916610807421
Experience 15, Iter 54, disc loss: 0.0020654955476233955, policy loss: 7.659031775155784
Experience 15, Iter 55, disc loss: 0.0020179319185626926, policy loss: 8.114671781016316
Experience 15, Iter 56, disc loss: 0.0017736917820761443, policy loss: 7.8649585473806605
Experience 15, Iter 57, disc loss: 0.002089395567795971, policy loss: 7.520304777499492
Experience 15, Iter 58, disc loss: 0.0018493286865725292, policy loss: 7.7581433674640685
Experience 15, Iter 59, disc loss: 0.0017170093971767015, policy loss: 7.842769008102849
Experience 15, Iter 60, disc loss: 0.001964761238719314, policy loss: 8.09466005213498
Experience 15, Iter 61, disc loss: 0.0018701793670777948, policy loss: 7.803995989507203
Experience 15, Iter 62, disc loss: 0.0019360832965943267, policy loss: 7.786912128972623
Experience 15, Iter 63, disc loss: 0.0018620431331649537, policy loss: 7.854803968228722
Experience 15, Iter 64, disc loss: 0.0017091773823907884, policy loss: 7.774099518863673
Experience 15, Iter 65, disc loss: 0.0017525584880273126, policy loss: 8.239851494434362
Experience 15, Iter 66, disc loss: 0.0016798065597380353, policy loss: 8.136107028739936
Experience 15, Iter 67, disc loss: 0.001960414543686219, policy loss: 7.434456365946858
Experience 15, Iter 68, disc loss: 0.0019142072438634074, policy loss: 7.733236935279808
Experience 15, Iter 69, disc loss: 0.0018806402598850417, policy loss: 7.690024230136194
Experience 15, Iter 70, disc loss: 0.0021279167463614497, policy loss: 7.356356273560287
Experience 15, Iter 71, disc loss: 0.001923679048125953, policy loss: 8.774111192051132
Experience 15, Iter 72, disc loss: 0.0017231808553812352, policy loss: 8.158971588403855
Experience 15, Iter 73, disc loss: 0.001721433966370378, policy loss: 7.917717333754668
Experience 15, Iter 74, disc loss: 0.0018282326248513327, policy loss: 7.476447103597972
Experience 15, Iter 75, disc loss: 0.001670111165272385, policy loss: 7.72807189234109
Experience 15, Iter 76, disc loss: 0.0017200104858678589, policy loss: 8.191703082919178
Experience 15, Iter 77, disc loss: 0.0016824603085301504, policy loss: 8.430746133497998
Experience 15, Iter 78, disc loss: 0.001734080982919317, policy loss: 8.46445265344757
Experience 15, Iter 79, disc loss: 0.0017668260709216344, policy loss: 7.804667328562413
Experience 15, Iter 80, disc loss: 0.0017932187057111007, policy loss: 7.512121787732513
Experience 15, Iter 81, disc loss: 0.001790931112799029, policy loss: 7.770246779842468
Experience 15, Iter 82, disc loss: 0.001672577634686406, policy loss: 7.575338831537683
Experience 15, Iter 83, disc loss: 0.0017726048535475062, policy loss: 7.4606984648292745
Experience 15, Iter 84, disc loss: 0.001787057158161454, policy loss: 7.568042073246429
Experience 15, Iter 85, disc loss: 0.001839567315064167, policy loss: 7.464101381117728
Experience 15, Iter 86, disc loss: 0.001711643065305148, policy loss: 7.988500542350186
Experience 15, Iter 87, disc loss: 0.001914891939269484, policy loss: 7.942988449139346
Experience 15, Iter 88, disc loss: 0.001795774413420889, policy loss: 8.155983347379694
Experience 15, Iter 89, disc loss: 0.001969612017147783, policy loss: 7.209722414541549
Experience 15, Iter 90, disc loss: 0.001668282900352108, policy loss: 8.333078266915631
Experience 15, Iter 91, disc loss: 0.00189214691890842, policy loss: 8.132047354556798
Experience 15, Iter 92, disc loss: 0.0016367833386986919, policy loss: 7.779028968746355
Experience 15, Iter 93, disc loss: 0.0018287934451612627, policy loss: 7.696339501240144
Experience 15, Iter 94, disc loss: 0.0018238501560590321, policy loss: 7.6880237647807075
Experience 15, Iter 95, disc loss: 0.0018043103796163413, policy loss: 7.649598287560453
Experience 15, Iter 96, disc loss: 0.001642369587089041, policy loss: 7.796163778904187
Experience 15, Iter 97, disc loss: 0.0018406281031785612, policy loss: 7.487303099050608
Experience 15, Iter 98, disc loss: 0.0018989809408475656, policy loss: 7.619978326898604
Experience 15, Iter 99, disc loss: 0.0017047219304176127, policy loss: 7.826801745655894
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0076],
        [0.2680],
        [2.0813],
        [0.0455]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0369, 0.3077, 2.1022, 0.0313, 0.0293, 6.1151]],

        [[0.0369, 0.3077, 2.1022, 0.0313, 0.0293, 6.1151]],

        [[0.0369, 0.3077, 2.1022, 0.0313, 0.0293, 6.1151]],

        [[0.0369, 0.3077, 2.1022, 0.0313, 0.0293, 6.1151]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0305, 1.0720, 8.3251, 0.1819], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0305, 1.0720, 8.3251, 0.1819])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.013
Iter 2/2000 - Loss: 3.739
Iter 3/2000 - Loss: 3.626
Iter 4/2000 - Loss: 3.479
Iter 5/2000 - Loss: 3.310
Iter 6/2000 - Loss: 3.155
Iter 7/2000 - Loss: 3.001
Iter 8/2000 - Loss: 2.814
Iter 9/2000 - Loss: 2.596
Iter 10/2000 - Loss: 2.373
Iter 11/2000 - Loss: 2.154
Iter 12/2000 - Loss: 1.936
Iter 13/2000 - Loss: 1.707
Iter 14/2000 - Loss: 1.463
Iter 15/2000 - Loss: 1.204
Iter 16/2000 - Loss: 0.940
Iter 17/2000 - Loss: 0.675
Iter 18/2000 - Loss: 0.412
Iter 19/2000 - Loss: 0.151
Iter 20/2000 - Loss: -0.110
Iter 1981/2000 - Loss: -7.222
Iter 1982/2000 - Loss: -7.222
Iter 1983/2000 - Loss: -7.222
Iter 1984/2000 - Loss: -7.222
Iter 1985/2000 - Loss: -7.222
Iter 1986/2000 - Loss: -7.222
Iter 1987/2000 - Loss: -7.222
Iter 1988/2000 - Loss: -7.222
Iter 1989/2000 - Loss: -7.222
Iter 1990/2000 - Loss: -7.222
Iter 1991/2000 - Loss: -7.222
Iter 1992/2000 - Loss: -7.223
Iter 1993/2000 - Loss: -7.223
Iter 1994/2000 - Loss: -7.223
Iter 1995/2000 - Loss: -7.223
Iter 1996/2000 - Loss: -7.223
Iter 1997/2000 - Loss: -7.223
Iter 1998/2000 - Loss: -7.223
Iter 1999/2000 - Loss: -7.223
Iter 2000/2000 - Loss: -7.223
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[15.5187, 11.3473, 32.9117,  4.4346,  4.4453, 69.6615]],

        [[21.9080, 36.8403,  8.3511,  1.2620,  1.6093, 30.3975]],

        [[22.6791, 34.3494,  9.3883,  1.0642,  0.7908, 24.6289]],

        [[17.4396, 33.2605,  9.4867,  2.7037,  1.8473, 41.8075]]])
Signal Variance: tensor([ 0.2153,  2.8547, 17.4334,  0.4444])
Estimated target variance: tensor([0.0305, 1.0720, 8.3251, 0.1819])
N: 160
Signal to noise ratio: tensor([ 25.7827, 104.7432,  82.4390,  39.6307])
Bound on condition number: tensor([ 106360.5836, 1755384.0694, 1087392.1897,  251295.3867])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0017967512666396024, policy loss: 7.502552965745184
Experience 16, Iter 1, disc loss: 0.0017901130937121958, policy loss: 8.083792243809912
Experience 16, Iter 2, disc loss: 0.0016577985262856824, policy loss: 7.7045893567614225
Experience 16, Iter 3, disc loss: 0.0018772196822338626, policy loss: 7.363152400047562
Experience 16, Iter 4, disc loss: 0.0018859428204942642, policy loss: 7.980748921687718
Experience 16, Iter 5, disc loss: 0.0017250079276380965, policy loss: 8.563875524631387
Experience 16, Iter 6, disc loss: 0.0018629469473314272, policy loss: 7.627271918570173
Experience 16, Iter 7, disc loss: 0.0016259256758922088, policy loss: 8.133239922070917
Experience 16, Iter 8, disc loss: 0.0016509867397950073, policy loss: 7.8392214317613025
Experience 16, Iter 9, disc loss: 0.0019947352267423494, policy loss: 7.327237211180335
Experience 16, Iter 10, disc loss: 0.001687116860288231, policy loss: 7.570253283790728
Experience 16, Iter 11, disc loss: 0.0017016207350302914, policy loss: 7.973827590309863
Experience 16, Iter 12, disc loss: 0.0020207648085481785, policy loss: 7.391109560371733
Experience 16, Iter 13, disc loss: 0.00172296164846167, policy loss: 7.838844025045878
Experience 16, Iter 14, disc loss: 0.0018736555797738558, policy loss: 7.559441731994083
Experience 16, Iter 15, disc loss: 0.0016930052514275429, policy loss: 8.207719549279489
Experience 16, Iter 16, disc loss: 0.0017521213294987065, policy loss: 7.592436458237673
Experience 16, Iter 17, disc loss: 0.00180997705317637, policy loss: 7.442701864126928
Experience 16, Iter 18, disc loss: 0.0015878368770331746, policy loss: 8.230639501731412
Experience 16, Iter 19, disc loss: 0.0018334056633467358, policy loss: 7.7344651417064885
Experience 16, Iter 20, disc loss: 0.0015539964917718053, policy loss: 8.28065129874294
Experience 16, Iter 21, disc loss: 0.001545503623257562, policy loss: 8.66791383794804
Experience 16, Iter 22, disc loss: 0.001646727674956981, policy loss: 8.045408486678888
Experience 16, Iter 23, disc loss: 0.001964691689130224, policy loss: 7.353033025019468
Experience 16, Iter 24, disc loss: 0.001823584865812752, policy loss: 7.989902629243435
Experience 16, Iter 25, disc loss: 0.0017992565693464252, policy loss: 7.433704235702104
Experience 16, Iter 26, disc loss: 0.0017633352811049102, policy loss: 7.57333245718927
Experience 16, Iter 27, disc loss: 0.001875711904415343, policy loss: 7.923814961104505
Experience 16, Iter 28, disc loss: 0.0017343582505888813, policy loss: 7.847035595490031
Experience 16, Iter 29, disc loss: 0.0017163519964967329, policy loss: 8.217369263825567
Experience 16, Iter 30, disc loss: 0.0015345598600683447, policy loss: 8.633474466246888
Experience 16, Iter 31, disc loss: 0.0015483604611177346, policy loss: 7.800270250585877
Experience 16, Iter 32, disc loss: 0.0016864331189551632, policy loss: 7.5131386278697825
Experience 16, Iter 33, disc loss: 0.0015938230357800541, policy loss: 8.079384889211504
Experience 16, Iter 34, disc loss: 0.001512712080354427, policy loss: 8.234707039328201
Experience 16, Iter 35, disc loss: 0.0017305415331038054, policy loss: 7.792243488371084
Experience 16, Iter 36, disc loss: 0.0018530149224557468, policy loss: 7.645666448469618
Experience 16, Iter 37, disc loss: 0.0017273339897087272, policy loss: 8.187887891373403
Experience 16, Iter 38, disc loss: 0.001682447347772747, policy loss: 7.729862106568925
Experience 16, Iter 39, disc loss: 0.0017072415744567623, policy loss: 7.59685070477634
Experience 16, Iter 40, disc loss: 0.0015104050311433707, policy loss: 7.728742862261518
Experience 16, Iter 41, disc loss: 0.0016665127573100744, policy loss: 8.302250077725551
Experience 16, Iter 42, disc loss: 0.0014053596767122241, policy loss: 8.684275005360583
Experience 16, Iter 43, disc loss: 0.0018578557569205977, policy loss: 7.480731667821928
Experience 16, Iter 44, disc loss: 0.0018010044387950105, policy loss: 7.4941933011191875
Experience 16, Iter 45, disc loss: 0.0016158731841162022, policy loss: 7.785053678788813
Experience 16, Iter 46, disc loss: 0.0015231732503520551, policy loss: 8.352205837623398
Experience 16, Iter 47, disc loss: 0.001705032009102478, policy loss: 8.073249388842452
Experience 16, Iter 48, disc loss: 0.0016691459435710567, policy loss: 8.18207777296479
Experience 16, Iter 49, disc loss: 0.0018198207450725974, policy loss: 7.613854910266051
Experience 16, Iter 50, disc loss: 0.001576297431357574, policy loss: 7.750594240374177
Experience 16, Iter 51, disc loss: 0.001733598679672985, policy loss: 7.814037204986759
Experience 16, Iter 52, disc loss: 0.0016799115147220624, policy loss: 8.066065839351735
Experience 16, Iter 53, disc loss: 0.001583050646255975, policy loss: 7.960604479607797
Experience 16, Iter 54, disc loss: 0.0018174913466011524, policy loss: 7.72646469785971
Experience 16, Iter 55, disc loss: 0.0016770211566641733, policy loss: 7.445993414333715
Experience 16, Iter 56, disc loss: 0.001679567230976524, policy loss: 7.800052382840072
Experience 16, Iter 57, disc loss: 0.0016264677680108753, policy loss: 8.216872023406847
Experience 16, Iter 58, disc loss: 0.0019985968826113664, policy loss: 8.465483266214987
Experience 16, Iter 59, disc loss: 0.0015533859324488772, policy loss: 8.278999104134357
Experience 16, Iter 60, disc loss: 0.0015356845089403255, policy loss: 8.198273862850723
Experience 16, Iter 61, disc loss: 0.0015104393065367378, policy loss: 8.002052071522563
Experience 16, Iter 62, disc loss: 0.0016167724979372225, policy loss: 8.131821828376319
Experience 16, Iter 63, disc loss: 0.0015001144053984135, policy loss: 7.8652380544272
Experience 16, Iter 64, disc loss: 0.001766065162417433, policy loss: 7.763889262472408
Experience 16, Iter 65, disc loss: 0.001587657059680478, policy loss: 8.01383502687534
Experience 16, Iter 66, disc loss: 0.0017805188383470704, policy loss: 8.025718781405468
Experience 16, Iter 67, disc loss: 0.0016893761876124007, policy loss: 8.09083454863699
Experience 16, Iter 68, disc loss: 0.0014678599338984856, policy loss: 8.411153298896133
Experience 16, Iter 69, disc loss: 0.0015533293486839402, policy loss: 7.639657831598873
Experience 16, Iter 70, disc loss: 0.0015065965475596944, policy loss: 8.001085275012786
Experience 16, Iter 71, disc loss: 0.001416619632759007, policy loss: 8.403313331495104
Experience 16, Iter 72, disc loss: 0.001644509348545304, policy loss: 7.809696842491632
Experience 16, Iter 73, disc loss: 0.0014452930784558404, policy loss: 8.269947751186711
Experience 16, Iter 74, disc loss: 0.0017137241942576592, policy loss: 8.401972334736175
Experience 16, Iter 75, disc loss: 0.0015864019276332598, policy loss: 8.166850027416297
Experience 16, Iter 76, disc loss: 0.001580222452371539, policy loss: 9.182702080808873
Experience 16, Iter 77, disc loss: 0.001752237803610592, policy loss: 8.087391038195573
Experience 16, Iter 78, disc loss: 0.0015597693998239933, policy loss: 7.947157483494714
Experience 16, Iter 79, disc loss: 0.0014710700557579205, policy loss: 7.872313162963451
Experience 16, Iter 80, disc loss: 0.001542028239943655, policy loss: 7.992355155407424
Experience 16, Iter 81, disc loss: 0.0014374916850946472, policy loss: 8.434260580953522
Experience 16, Iter 82, disc loss: 0.0015399600060776644, policy loss: 7.872514319901306
Experience 16, Iter 83, disc loss: 0.0013543741488810234, policy loss: 7.9571183641883945
Experience 16, Iter 84, disc loss: 0.0014471581843062814, policy loss: 7.932396668168969
Experience 16, Iter 85, disc loss: 0.0015797665635918566, policy loss: 7.742256954057407
Experience 16, Iter 86, disc loss: 0.001448013175147085, policy loss: 7.743628802490975
Experience 16, Iter 87, disc loss: 0.0013104063498082254, policy loss: 7.788103033572869
Experience 16, Iter 88, disc loss: 0.001560236964087894, policy loss: 7.801894843455752
Experience 16, Iter 89, disc loss: 0.0014935484221621405, policy loss: 8.175821215085392
Experience 16, Iter 90, disc loss: 0.001424148352260392, policy loss: 8.300666925137698
Experience 16, Iter 91, disc loss: 0.0015326554946814546, policy loss: 7.73097515086636
Experience 16, Iter 92, disc loss: 0.0013809016688882234, policy loss: 8.617727135388776
Experience 16, Iter 93, disc loss: 0.0013361768292912911, policy loss: 9.171939449408466
Experience 16, Iter 94, disc loss: 0.0013639621009962969, policy loss: 7.953905592422577
Experience 16, Iter 95, disc loss: 0.001466596228907394, policy loss: 8.061922959840427
Experience 16, Iter 96, disc loss: 0.0012347998364108928, policy loss: 8.31162173021092
Experience 16, Iter 97, disc loss: 0.001506808012538724, policy loss: 7.603819280100774
Experience 16, Iter 98, disc loss: 0.001542280744208248, policy loss: 7.983717851582687
Experience 16, Iter 99, disc loss: 0.0015844008684043275, policy loss: 7.799928197501642
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0075],
        [0.2725],
        [2.0896],
        [0.0458]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0358, 0.3038, 2.1175, 0.0319, 0.0298, 6.2315]],

        [[0.0358, 0.3038, 2.1175, 0.0319, 0.0298, 6.2315]],

        [[0.0358, 0.3038, 2.1175, 0.0319, 0.0298, 6.2315]],

        [[0.0358, 0.3038, 2.1175, 0.0319, 0.0298, 6.2315]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0301, 1.0900, 8.3586, 0.1833], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0301, 1.0900, 8.3586, 0.1833])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.030
Iter 2/2000 - Loss: 3.770
Iter 3/2000 - Loss: 3.653
Iter 4/2000 - Loss: 3.494
Iter 5/2000 - Loss: 3.332
Iter 6/2000 - Loss: 3.192
Iter 7/2000 - Loss: 3.041
Iter 8/2000 - Loss: 2.847
Iter 9/2000 - Loss: 2.628
Iter 10/2000 - Loss: 2.408
Iter 11/2000 - Loss: 2.194
Iter 12/2000 - Loss: 1.975
Iter 13/2000 - Loss: 1.742
Iter 14/2000 - Loss: 1.491
Iter 15/2000 - Loss: 1.228
Iter 16/2000 - Loss: 0.960
Iter 17/2000 - Loss: 0.692
Iter 18/2000 - Loss: 0.423
Iter 19/2000 - Loss: 0.155
Iter 20/2000 - Loss: -0.116
Iter 1981/2000 - Loss: -7.357
Iter 1982/2000 - Loss: -7.357
Iter 1983/2000 - Loss: -7.357
Iter 1984/2000 - Loss: -7.357
Iter 1985/2000 - Loss: -7.357
Iter 1986/2000 - Loss: -7.357
Iter 1987/2000 - Loss: -7.358
Iter 1988/2000 - Loss: -7.358
Iter 1989/2000 - Loss: -7.358
Iter 1990/2000 - Loss: -7.358
Iter 1991/2000 - Loss: -7.358
Iter 1992/2000 - Loss: -7.358
Iter 1993/2000 - Loss: -7.358
Iter 1994/2000 - Loss: -7.358
Iter 1995/2000 - Loss: -7.358
Iter 1996/2000 - Loss: -7.358
Iter 1997/2000 - Loss: -7.358
Iter 1998/2000 - Loss: -7.358
Iter 1999/2000 - Loss: -7.358
Iter 2000/2000 - Loss: -7.358
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[15.3720, 11.2266, 31.6500,  4.8209,  4.9074, 68.3515]],

        [[22.1259, 37.6602,  8.2286,  1.2901,  1.6147, 29.8146]],

        [[23.0284, 34.9705,  9.3494,  1.0489,  0.7975, 24.6255]],

        [[17.1976, 31.8950,  9.2301,  2.6770,  1.8259, 40.8414]]])
Signal Variance: tensor([ 0.2093,  2.7398, 17.1967,  0.4180])
Estimated target variance: tensor([0.0301, 1.0900, 8.3586, 0.1833])
N: 170
Signal to noise ratio: tensor([ 25.8560, 101.5504,  83.8511,  39.3787])
Bound on condition number: tensor([ 113651.2287, 1753124.2955, 1195271.1825,  263616.4817])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0013994800267717864, policy loss: 8.311432611932686
Experience 17, Iter 1, disc loss: 0.0015128305529905055, policy loss: 8.20023450677705
Experience 17, Iter 2, disc loss: 0.0016536590095305927, policy loss: 7.750277082842048
Experience 17, Iter 3, disc loss: 0.0015078816249290677, policy loss: 7.976199352289738
Experience 17, Iter 4, disc loss: 0.0014872911100900362, policy loss: 7.770368580054511
Experience 17, Iter 5, disc loss: 0.0014390277814112762, policy loss: 7.721660221257595
Experience 17, Iter 6, disc loss: 0.001558301544655894, policy loss: 7.512537278691687
Experience 17, Iter 7, disc loss: 0.0014908102859137865, policy loss: 7.75528388637707
Experience 17, Iter 8, disc loss: 0.0013933473927453914, policy loss: 8.968557489551369
Experience 17, Iter 9, disc loss: 0.0015461657051110216, policy loss: 7.680742536306577
Experience 17, Iter 10, disc loss: 0.0015433581709607706, policy loss: 7.769730433781085
Experience 17, Iter 11, disc loss: 0.0013986593463246177, policy loss: 8.044031874997659
Experience 17, Iter 12, disc loss: 0.0017717681199646371, policy loss: 7.524368460214932
Experience 17, Iter 13, disc loss: 0.0016345735423394163, policy loss: 7.605863634194779
Experience 17, Iter 14, disc loss: 0.0017088520859098334, policy loss: 8.18219905942569
Experience 17, Iter 15, disc loss: 0.0015307653891575945, policy loss: 7.444542912738099
Experience 17, Iter 16, disc loss: 0.0018391314657464132, policy loss: 7.90705646592702
Experience 17, Iter 17, disc loss: 0.001583715092615791, policy loss: 8.298875549983464
Experience 17, Iter 18, disc loss: 0.0016039707924833378, policy loss: 8.190065471095936
Experience 17, Iter 19, disc loss: 0.0015410115926727695, policy loss: 8.339516967383952
Experience 17, Iter 20, disc loss: 0.001449503672637638, policy loss: 8.568756808673108
Experience 17, Iter 21, disc loss: 0.0017813995723447008, policy loss: 7.344553483420723
Experience 17, Iter 22, disc loss: 0.001665928861236096, policy loss: 7.594557976906024
Experience 17, Iter 23, disc loss: 0.001417845157308833, policy loss: 8.265863711810468
Experience 17, Iter 24, disc loss: 0.0016015961203679156, policy loss: 7.877377581679994
Experience 17, Iter 25, disc loss: 0.0016732670622600096, policy loss: 7.990943976915482
Experience 17, Iter 26, disc loss: 0.0015467940772533313, policy loss: 7.969777751948513
Experience 17, Iter 27, disc loss: 0.0013231126246378766, policy loss: 8.515695255920352
Experience 17, Iter 28, disc loss: 0.0013831368470343017, policy loss: 8.686919614249593
Experience 17, Iter 29, disc loss: 0.00147257526411398, policy loss: 7.979551598677184
Experience 17, Iter 30, disc loss: 0.0016319594567609404, policy loss: 8.09989256638712
Experience 17, Iter 31, disc loss: 0.0013053625777164374, policy loss: 8.873557924001462
Experience 17, Iter 32, disc loss: 0.0016531539787748334, policy loss: 7.6681867376138815
Experience 17, Iter 33, disc loss: 0.0013778447110253398, policy loss: 8.279767762456192
Experience 17, Iter 34, disc loss: 0.001474246285800371, policy loss: 8.668096130949204
Experience 17, Iter 35, disc loss: 0.0013765208854931022, policy loss: 8.813323009995127
Experience 17, Iter 36, disc loss: 0.0013623544388533314, policy loss: 9.194449116904337
Experience 17, Iter 37, disc loss: 0.0014545397399877472, policy loss: 8.1125621409175
Experience 17, Iter 38, disc loss: 0.0012740313822745307, policy loss: 8.509291744089598
Experience 17, Iter 39, disc loss: 0.0013705393402606168, policy loss: 7.975535477318479
Experience 17, Iter 40, disc loss: 0.0011324611983383275, policy loss: 8.411664719032569
Experience 17, Iter 41, disc loss: 0.0013963423409453688, policy loss: 8.14531226638878
Experience 17, Iter 42, disc loss: 0.0013806272164129459, policy loss: 8.112703692646367
Experience 17, Iter 43, disc loss: 0.0010958120655639842, policy loss: 9.951998382901737
Experience 17, Iter 44, disc loss: 0.0008194464478329345, policy loss: 10.106252896082285
Experience 17, Iter 45, disc loss: 0.0007115322511267561, policy loss: 13.444138791452161
Experience 17, Iter 46, disc loss: 0.000692998659958517, policy loss: 13.980367312103287
Experience 17, Iter 47, disc loss: 0.0006740906520057959, policy loss: 15.452690965421546
Experience 17, Iter 48, disc loss: 0.0006535729097417314, policy loss: 21.273283126875484
Experience 17, Iter 49, disc loss: 0.0006334075986890492, policy loss: 21.875936050170147
Experience 17, Iter 50, disc loss: 0.0006125341706697459, policy loss: 22.615731546334843
Experience 17, Iter 51, disc loss: 0.0005912202800552834, policy loss: 33.17719828338961
Experience 17, Iter 52, disc loss: 0.0005713086723179479, policy loss: 15.815957980005981
Experience 17, Iter 53, disc loss: 0.0005512182536232976, policy loss: 15.985964908119985
Experience 17, Iter 54, disc loss: 0.000542997820718488, policy loss: 12.659000981933069
Experience 17, Iter 55, disc loss: 0.0005281220965435404, policy loss: 12.488035722540598
Experience 17, Iter 56, disc loss: 0.0004964091026225246, policy loss: 12.500560226544673
Experience 17, Iter 57, disc loss: 0.00047138018496093776, policy loss: 14.947474939718878
Experience 17, Iter 58, disc loss: 0.0004512148562818096, policy loss: 14.853277882052126
Experience 17, Iter 59, disc loss: 0.0004336249432382274, policy loss: 15.002413159061883
Experience 17, Iter 60, disc loss: 0.00041727389869838867, policy loss: 14.928076625992482
Experience 17, Iter 61, disc loss: 0.0004055345908973501, policy loss: 13.519725391368002
Experience 17, Iter 62, disc loss: 0.00039161270806174677, policy loss: 13.780954272720713
Experience 17, Iter 63, disc loss: 0.00038182498695941026, policy loss: 13.15305860189271
Experience 17, Iter 64, disc loss: 0.0003764640516133682, policy loss: 13.19545842817616
Experience 17, Iter 65, disc loss: 0.00039503938439137173, policy loss: 11.502961337859048
Experience 17, Iter 66, disc loss: 0.0007291728738171102, policy loss: 8.934283655177902
Experience 17, Iter 67, disc loss: 0.0008314256210502527, policy loss: 10.616913353274036
Experience 17, Iter 68, disc loss: 0.0013875216190495658, policy loss: 9.563256627064835
Experience 17, Iter 69, disc loss: 0.0009569880536707586, policy loss: 8.458078507201751
Experience 17, Iter 70, disc loss: 0.0008951675405112846, policy loss: 8.691843564006781
Experience 17, Iter 71, disc loss: 0.0008256379737984698, policy loss: 8.15764309973196
Experience 17, Iter 72, disc loss: 0.0009181964908163563, policy loss: 9.444070194120316
Experience 17, Iter 73, disc loss: 0.0009175552325221225, policy loss: 8.585292314172026
Experience 17, Iter 74, disc loss: 0.0010404373786273655, policy loss: 7.898966766070934
Experience 17, Iter 75, disc loss: 0.0012056318802941952, policy loss: 8.328534998533938
Experience 17, Iter 76, disc loss: 0.001158732833144962, policy loss: 8.531022440825362
Experience 17, Iter 77, disc loss: 0.0010996217965132497, policy loss: 7.751610223189616
Experience 17, Iter 78, disc loss: 0.001389916040440664, policy loss: 7.759125079721169
Experience 17, Iter 79, disc loss: 0.0011208463234894232, policy loss: 8.565087707868878
Experience 17, Iter 80, disc loss: 0.001192893488808011, policy loss: 8.688656202219196
Experience 17, Iter 81, disc loss: 0.0009467315422238093, policy loss: 8.39568906164546
Experience 17, Iter 82, disc loss: 0.0009259401452019148, policy loss: 8.835024966956006
Experience 17, Iter 83, disc loss: 0.0008483545335094245, policy loss: 8.296680128136732
Experience 17, Iter 84, disc loss: 0.001213592205271435, policy loss: 8.966094555064617
Experience 17, Iter 85, disc loss: 0.0010445533310839378, policy loss: 8.51313278858203
Experience 17, Iter 86, disc loss: 0.001075764798711177, policy loss: 8.6633782346268
Experience 17, Iter 87, disc loss: 0.0013822782170236014, policy loss: 7.756463956886452
Experience 17, Iter 88, disc loss: 0.0011599461855225524, policy loss: 9.47795734444962
Experience 17, Iter 89, disc loss: 0.0012513614098500253, policy loss: 9.143140226445993
Experience 17, Iter 90, disc loss: 0.0011452111842638578, policy loss: 7.942954292670425
Experience 17, Iter 91, disc loss: 0.0011352244210577282, policy loss: 8.385728095655562
Experience 17, Iter 92, disc loss: 0.001099050428072765, policy loss: 7.843236210414864
Experience 17, Iter 93, disc loss: 0.0009897001820977516, policy loss: 8.912283119227393
Experience 17, Iter 94, disc loss: 0.0009529772689405446, policy loss: 8.632335738937199
Experience 17, Iter 95, disc loss: 0.0009684247469948856, policy loss: 8.657713303661229
Experience 17, Iter 96, disc loss: 0.0010379009755747686, policy loss: 8.161733587248632
Experience 17, Iter 97, disc loss: 0.0010498641249638645, policy loss: 8.383730316290066
Experience 17, Iter 98, disc loss: 0.001176376045472841, policy loss: 8.432707478768414
Experience 17, Iter 99, disc loss: 0.0011079465406490234, policy loss: 8.40862794504392
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0075],
        [0.2793],
        [2.1202],
        [0.0474]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0346, 0.3034, 2.1699, 0.0323, 0.0307, 6.3805]],

        [[0.0346, 0.3034, 2.1699, 0.0323, 0.0307, 6.3805]],

        [[0.0346, 0.3034, 2.1699, 0.0323, 0.0307, 6.3805]],

        [[0.0346, 0.3034, 2.1699, 0.0323, 0.0307, 6.3805]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0300, 1.1173, 8.4807, 0.1895], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0300, 1.1173, 8.4807, 0.1895])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.052
Iter 2/2000 - Loss: 3.803
Iter 3/2000 - Loss: 3.673
Iter 4/2000 - Loss: 3.503
Iter 5/2000 - Loss: 3.342
Iter 6/2000 - Loss: 3.208
Iter 7/2000 - Loss: 3.051
Iter 8/2000 - Loss: 2.849
Iter 9/2000 - Loss: 2.629
Iter 10/2000 - Loss: 2.412
Iter 11/2000 - Loss: 2.198
Iter 12/2000 - Loss: 1.976
Iter 13/2000 - Loss: 1.739
Iter 14/2000 - Loss: 1.488
Iter 15/2000 - Loss: 1.228
Iter 16/2000 - Loss: 0.963
Iter 17/2000 - Loss: 0.693
Iter 18/2000 - Loss: 0.422
Iter 19/2000 - Loss: 0.148
Iter 20/2000 - Loss: -0.128
Iter 1981/2000 - Loss: -7.368
Iter 1982/2000 - Loss: -7.368
Iter 1983/2000 - Loss: -7.368
Iter 1984/2000 - Loss: -7.368
Iter 1985/2000 - Loss: -7.368
Iter 1986/2000 - Loss: -7.368
Iter 1987/2000 - Loss: -7.369
Iter 1988/2000 - Loss: -7.369
Iter 1989/2000 - Loss: -7.369
Iter 1990/2000 - Loss: -7.369
Iter 1991/2000 - Loss: -7.369
Iter 1992/2000 - Loss: -7.369
Iter 1993/2000 - Loss: -7.369
Iter 1994/2000 - Loss: -7.369
Iter 1995/2000 - Loss: -7.369
Iter 1996/2000 - Loss: -7.369
Iter 1997/2000 - Loss: -7.369
Iter 1998/2000 - Loss: -7.369
Iter 1999/2000 - Loss: -7.369
Iter 2000/2000 - Loss: -7.369
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[15.1637, 11.3632, 32.4766,  5.3202,  5.1152, 68.6017]],

        [[21.7508, 37.1358,  7.8713,  1.3034,  1.6844, 30.9622]],

        [[23.1302, 35.1462,  8.8838,  1.0726,  0.8665, 24.5070]],

        [[16.9305, 31.8614, 10.1382,  2.4648,  1.9561, 39.6498]]])
Signal Variance: tensor([ 0.2096,  2.8221, 17.9405,  0.4709])
Estimated target variance: tensor([0.0300, 1.1173, 8.4807, 0.1895])
N: 180
Signal to noise ratio: tensor([25.5594, 97.1248, 85.8594, 41.7344])
Bound on condition number: tensor([ 117592.2477, 1697980.7793, 1326932.4658,  313517.9877])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.000897004614160715, policy loss: 9.295690647644093
Experience 18, Iter 1, disc loss: 0.0010805360985810262, policy loss: 8.234137419168372
Experience 18, Iter 2, disc loss: 0.0010576978397142827, policy loss: 8.488159538817904
Experience 18, Iter 3, disc loss: 0.000982933762371249, policy loss: 8.624504195558263
Experience 18, Iter 4, disc loss: 0.0009757694919377482, policy loss: 8.916359748869533
Experience 18, Iter 5, disc loss: 0.001023014365338942, policy loss: 8.413210130999957
Experience 18, Iter 6, disc loss: 0.0011352584978940638, policy loss: 8.233102260772592
Experience 18, Iter 7, disc loss: 0.000983926239634587, policy loss: 8.557427231316066
Experience 18, Iter 8, disc loss: 0.000977204591642862, policy loss: 9.029294871949283
Experience 18, Iter 9, disc loss: 0.0010901887219602812, policy loss: 8.295998983351069
Experience 18, Iter 10, disc loss: 0.0011159137108088307, policy loss: 8.279535937885354
Experience 18, Iter 11, disc loss: 0.0009636563559239654, policy loss: 8.643363692907233
Experience 18, Iter 12, disc loss: 0.0011377549099813285, policy loss: 8.581026042745865
Experience 18, Iter 13, disc loss: 0.0009254066635192018, policy loss: 8.498458737357044
Experience 18, Iter 14, disc loss: 0.001079356546043797, policy loss: 8.318059090377407
Experience 18, Iter 15, disc loss: 0.001072682011315323, policy loss: 8.501754163710668
Experience 18, Iter 16, disc loss: 0.00106908221161058, policy loss: 8.483687923296976
Experience 18, Iter 17, disc loss: 0.001173734887046778, policy loss: 8.605176767790724
Experience 18, Iter 18, disc loss: 0.0010382551430696972, policy loss: 8.544940921992023
Experience 18, Iter 19, disc loss: 0.0011062806078181871, policy loss: 8.463477834953025
Experience 18, Iter 20, disc loss: 0.0010150265186378452, policy loss: 9.096534091152662
Experience 18, Iter 21, disc loss: 0.0009635376835100016, policy loss: 8.41841617916582
Experience 18, Iter 22, disc loss: 0.0009501259058960049, policy loss: 8.992396560764929
Experience 18, Iter 23, disc loss: 0.0009228417771708409, policy loss: 8.618108297980795
Experience 18, Iter 24, disc loss: 0.0011637825096365469, policy loss: 8.417684063426172
Experience 18, Iter 25, disc loss: 0.0011021836841092262, policy loss: 8.203670239248506
Experience 18, Iter 26, disc loss: 0.0011886970060485391, policy loss: 9.178914468035156
Experience 18, Iter 27, disc loss: 0.0010445238712891473, policy loss: 8.07866661387003
Experience 18, Iter 28, disc loss: 0.0010536190204871077, policy loss: 8.583612263143255
Experience 18, Iter 29, disc loss: 0.0010121270387605436, policy loss: 8.57027043225131
Experience 18, Iter 30, disc loss: 0.0011285323141100563, policy loss: 8.132711628514159
Experience 18, Iter 31, disc loss: 0.0010642691533519254, policy loss: 8.284249941605786
Experience 18, Iter 32, disc loss: 0.0010573033146567044, policy loss: 8.68372600158656
Experience 18, Iter 33, disc loss: 0.0011023366476697128, policy loss: 8.345406662010852
Experience 18, Iter 34, disc loss: 0.0011995715930534423, policy loss: 8.207119730810888
Experience 18, Iter 35, disc loss: 0.001056389377360559, policy loss: 9.00180336789354
Experience 18, Iter 36, disc loss: 0.0012421392910288744, policy loss: 8.131338683439758
Experience 18, Iter 37, disc loss: 0.0009508757751515494, policy loss: 8.665375615475885
Experience 18, Iter 38, disc loss: 0.0010572541849751343, policy loss: 8.139636496277127
Experience 18, Iter 39, disc loss: 0.0010724612018513536, policy loss: 8.039759551289652
Experience 18, Iter 40, disc loss: 0.0010083387036444644, policy loss: 8.515343616059916
Experience 18, Iter 41, disc loss: 0.0010206498694849634, policy loss: 8.27095477905495
Experience 18, Iter 42, disc loss: 0.0010197260079998025, policy loss: 8.437897932277984
Experience 18, Iter 43, disc loss: 0.0009653795335802334, policy loss: 9.064703039264337
Experience 18, Iter 44, disc loss: 0.0009911911231349612, policy loss: 9.367194621153416
Experience 18, Iter 45, disc loss: 0.0010485097321129026, policy loss: 8.121115706582662
Experience 18, Iter 46, disc loss: 0.001143272326199098, policy loss: 8.509556250910201
Experience 18, Iter 47, disc loss: 0.0010631919186099596, policy loss: 7.976630594563242
Experience 18, Iter 48, disc loss: 0.0010454540205298592, policy loss: 8.162205249221893
Experience 18, Iter 49, disc loss: 0.0010486812723692405, policy loss: 8.614036923051923
Experience 18, Iter 50, disc loss: 0.00112015648376841, policy loss: 7.968848324433953
Experience 18, Iter 51, disc loss: 0.001007090771201201, policy loss: 8.584167964615336
Experience 18, Iter 52, disc loss: 0.0010832239298317586, policy loss: 9.161869784031603
Experience 18, Iter 53, disc loss: 0.0010234630802815866, policy loss: 8.193806230192223
Experience 18, Iter 54, disc loss: 0.0010362683916142722, policy loss: 8.556397644915798
Experience 18, Iter 55, disc loss: 0.0010785376946411615, policy loss: 8.536610790008393
Experience 18, Iter 56, disc loss: 0.0010210403865926069, policy loss: 8.433488270824236
Experience 18, Iter 57, disc loss: 0.001043456663961924, policy loss: 8.1541872571853
Experience 18, Iter 58, disc loss: 0.0010422860886475226, policy loss: 8.010105681873881
Experience 18, Iter 59, disc loss: 0.0009659744175711472, policy loss: 8.614860978492214
Experience 18, Iter 60, disc loss: 0.00111173770729093, policy loss: 8.447941150058515
Experience 18, Iter 61, disc loss: 0.0010463079620628597, policy loss: 8.210378935866519
Experience 18, Iter 62, disc loss: 0.0012181535971450348, policy loss: 7.904645714761342
Experience 18, Iter 63, disc loss: 0.0010210712361137467, policy loss: 8.194556181077154
Experience 18, Iter 64, disc loss: 0.0011126439558771526, policy loss: 8.334518583944563
Experience 18, Iter 65, disc loss: 0.0011396087528458528, policy loss: 8.077669208380078
Experience 18, Iter 66, disc loss: 0.0011695312929045673, policy loss: 8.403864652645515
Experience 18, Iter 67, disc loss: 0.0012087099645026207, policy loss: 8.416202359132111
Experience 18, Iter 68, disc loss: 0.0011738799369632028, policy loss: 8.671340953467496
Experience 18, Iter 69, disc loss: 0.0009950759242738304, policy loss: 9.186626783658054
Experience 18, Iter 70, disc loss: 0.0010084679518679663, policy loss: 8.617628951448511
Experience 18, Iter 71, disc loss: 0.0010338081108555532, policy loss: 8.132030975395814
Experience 18, Iter 72, disc loss: 0.0013096531340385364, policy loss: 7.668923691167041
Experience 18, Iter 73, disc loss: 0.0008983023370434634, policy loss: 8.707021721242489
Experience 18, Iter 74, disc loss: 0.0011311968679819102, policy loss: 8.273255988033313
Experience 18, Iter 75, disc loss: 0.0011382796820179294, policy loss: 7.896686204855532
Experience 18, Iter 76, disc loss: 0.000990855595594951, policy loss: 8.310904531085704
Experience 18, Iter 77, disc loss: 0.0009993527795512623, policy loss: 8.035991050550152
Experience 18, Iter 78, disc loss: 0.0011857006765418855, policy loss: 8.422186068931635
Experience 18, Iter 79, disc loss: 0.001031924778270371, policy loss: 8.81120452871311
Experience 18, Iter 80, disc loss: 0.0009858838148450579, policy loss: 8.83203658408071
Experience 18, Iter 81, disc loss: 0.000994324378581087, policy loss: 8.259725004638021
Experience 18, Iter 82, disc loss: 0.001083178873395827, policy loss: 8.63401886846931
Experience 18, Iter 83, disc loss: 0.0011413671126224408, policy loss: 8.183291462483208
Experience 18, Iter 84, disc loss: 0.0010564817895689727, policy loss: 8.272055925257138
Experience 18, Iter 85, disc loss: 0.0009897852526734002, policy loss: 8.701345689480785
Experience 18, Iter 86, disc loss: 0.001113154668586227, policy loss: 7.928370365364888
Experience 18, Iter 87, disc loss: 0.0010796057728275506, policy loss: 8.209246749435874
Experience 18, Iter 88, disc loss: 0.0011776924076510895, policy loss: 7.936326797117763
Experience 18, Iter 89, disc loss: 0.0009990539896362841, policy loss: 8.192903909494685
Experience 18, Iter 90, disc loss: 0.001010822797160361, policy loss: 8.917089519152519
Experience 18, Iter 91, disc loss: 0.0011157092653316884, policy loss: 8.094781416474515
Experience 18, Iter 92, disc loss: 0.0009822796625553283, policy loss: 8.186906669539734
Experience 18, Iter 93, disc loss: 0.001100476903046608, policy loss: 8.0102969542308
Experience 18, Iter 94, disc loss: 0.0009919082186855002, policy loss: 8.207512980710094
Experience 18, Iter 95, disc loss: 0.0010058472937115102, policy loss: 8.2382677565353
Experience 18, Iter 96, disc loss: 0.0011341468011949454, policy loss: 8.15745798712395
Experience 18, Iter 97, disc loss: 0.001061292868123562, policy loss: 8.166057880253286
Experience 18, Iter 98, disc loss: 0.0010666381564968354, policy loss: 8.32337420052644
Experience 18, Iter 99, disc loss: 0.0009916714444879941, policy loss: 8.484902936854862
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0074],
        [0.2836],
        [2.1385],
        [0.0476]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0336, 0.2994, 2.1851, 0.0328, 0.0311, 6.4741]],

        [[0.0336, 0.2994, 2.1851, 0.0328, 0.0311, 6.4741]],

        [[0.0336, 0.2994, 2.1851, 0.0328, 0.0311, 6.4741]],

        [[0.0336, 0.2994, 2.1851, 0.0328, 0.0311, 6.4741]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0297, 1.1343, 8.5539, 0.1905], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0297, 1.1343, 8.5539, 0.1905])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.036
Iter 2/2000 - Loss: 3.790
Iter 3/2000 - Loss: 3.646
Iter 4/2000 - Loss: 3.464
Iter 5/2000 - Loss: 3.302
Iter 6/2000 - Loss: 3.162
Iter 7/2000 - Loss: 2.992
Iter 8/2000 - Loss: 2.779
Iter 9/2000 - Loss: 2.554
Iter 10/2000 - Loss: 2.333
Iter 11/2000 - Loss: 2.111
Iter 12/2000 - Loss: 1.880
Iter 13/2000 - Loss: 1.635
Iter 14/2000 - Loss: 1.381
Iter 15/2000 - Loss: 1.121
Iter 16/2000 - Loss: 0.856
Iter 17/2000 - Loss: 0.587
Iter 18/2000 - Loss: 0.315
Iter 19/2000 - Loss: 0.042
Iter 20/2000 - Loss: -0.231
Iter 1981/2000 - Loss: -7.442
Iter 1982/2000 - Loss: -7.442
Iter 1983/2000 - Loss: -7.442
Iter 1984/2000 - Loss: -7.442
Iter 1985/2000 - Loss: -7.442
Iter 1986/2000 - Loss: -7.442
Iter 1987/2000 - Loss: -7.442
Iter 1988/2000 - Loss: -7.442
Iter 1989/2000 - Loss: -7.442
Iter 1990/2000 - Loss: -7.442
Iter 1991/2000 - Loss: -7.442
Iter 1992/2000 - Loss: -7.442
Iter 1993/2000 - Loss: -7.442
Iter 1994/2000 - Loss: -7.442
Iter 1995/2000 - Loss: -7.442
Iter 1996/2000 - Loss: -7.443
Iter 1997/2000 - Loss: -7.443
Iter 1998/2000 - Loss: -7.443
Iter 1999/2000 - Loss: -7.443
Iter 2000/2000 - Loss: -7.443
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[15.0068, 11.3719, 32.1874,  5.7025,  5.6060, 70.4970]],

        [[21.3988, 35.3887,  7.8896,  1.2902,  1.7022, 30.8592]],

        [[22.6018, 34.8946,  8.7011,  1.0872,  0.8777, 24.3953]],

        [[16.4517, 31.6566, 10.7451,  2.3029,  1.9792, 38.0943]]])
Signal Variance: tensor([ 0.2080,  2.7842, 17.3046,  0.4664])
Estimated target variance: tensor([0.0297, 1.1343, 8.5539, 0.1905])
N: 190
Signal to noise ratio: tensor([25.0401, 97.6009, 83.7381, 40.8645])
Bound on condition number: tensor([ 119132.3138, 1809927.9234, 1332293.8069,  317283.6104])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0011421113333272576, policy loss: 7.883911077923323
Experience 19, Iter 1, disc loss: 0.0010287983235736388, policy loss: 8.921214122653833
Experience 19, Iter 2, disc loss: 0.001138878756430429, policy loss: 8.361113100561138
Experience 19, Iter 3, disc loss: 0.0009441463737030766, policy loss: 8.69980267357257
Experience 19, Iter 4, disc loss: 0.0009510368471448089, policy loss: 8.696099304895315
Experience 19, Iter 5, disc loss: 0.0010353037859905936, policy loss: 8.075713603494112
Experience 19, Iter 6, disc loss: 0.0010366450379492673, policy loss: 8.345013226296041
Experience 19, Iter 7, disc loss: 0.0009556642249860739, policy loss: 8.314239741480126
Experience 19, Iter 8, disc loss: 0.0010948662296022466, policy loss: 8.030750211050968
Experience 19, Iter 9, disc loss: 0.0009335038618657853, policy loss: 8.76285200267309
Experience 19, Iter 10, disc loss: 0.0011229155703006656, policy loss: 8.088090491668176
Experience 19, Iter 11, disc loss: 0.0011405160450211058, policy loss: 8.643820484998773
Experience 19, Iter 12, disc loss: 0.0011878875807029166, policy loss: 7.766755908561063
Experience 19, Iter 13, disc loss: 0.0009503243466514934, policy loss: 8.361391731157644
Experience 19, Iter 14, disc loss: 0.0011456010125826636, policy loss: 8.068379482052805
Experience 19, Iter 15, disc loss: 0.0010046408610727615, policy loss: 8.365757496288776
Experience 19, Iter 16, disc loss: 0.001046154550407631, policy loss: 8.111951430123352
Experience 19, Iter 17, disc loss: 0.001084800929915761, policy loss: 8.216974092053084
Experience 19, Iter 18, disc loss: 0.0009077997526680782, policy loss: 8.688201054192817
Experience 19, Iter 19, disc loss: 0.0010446114152138565, policy loss: 8.424316962757608
Experience 19, Iter 20, disc loss: 0.0010999438075731528, policy loss: 8.109939342819526
Experience 19, Iter 21, disc loss: 0.000958574309707293, policy loss: 8.648146098802671
Experience 19, Iter 22, disc loss: 0.0010229007835030875, policy loss: 8.890544157017672
Experience 19, Iter 23, disc loss: 0.0010084800867305624, policy loss: 8.210603241431016
Experience 19, Iter 24, disc loss: 0.0009456326951619483, policy loss: 8.83595277703169
Experience 19, Iter 25, disc loss: 0.0010375383771450218, policy loss: 9.284289082830288
Experience 19, Iter 26, disc loss: 0.0010234378067969095, policy loss: 8.799916307629498
Experience 19, Iter 27, disc loss: 0.0009434641834953826, policy loss: 9.138789136010757
Experience 19, Iter 28, disc loss: 0.0010475433868908479, policy loss: 8.297402570723435
Experience 19, Iter 29, disc loss: 0.0010624977009984207, policy loss: 8.36695340653527
Experience 19, Iter 30, disc loss: 0.0009837758299223842, policy loss: 8.402065386060631
Experience 19, Iter 31, disc loss: 0.0008783319554304824, policy loss: 9.368652342597311
Experience 19, Iter 32, disc loss: 0.0009341767712547501, policy loss: 8.312955713074295
Experience 19, Iter 33, disc loss: 0.000985509611887073, policy loss: 8.227469090291091
Experience 19, Iter 34, disc loss: 0.0009531520535187986, policy loss: 8.370253574004073
Experience 19, Iter 35, disc loss: 0.0009720242392433259, policy loss: 8.770932011379657
Experience 19, Iter 36, disc loss: 0.0010413672063277941, policy loss: 8.477701000454854
Experience 19, Iter 37, disc loss: 0.0008827135088787379, policy loss: 8.379892435108353
Experience 19, Iter 38, disc loss: 0.0010811964621795656, policy loss: 8.066844435358824
Experience 19, Iter 39, disc loss: 0.0008330381139157602, policy loss: 8.466892712321249
Experience 19, Iter 40, disc loss: 0.001125792694412829, policy loss: 8.179288134636906
Experience 19, Iter 41, disc loss: 0.0011452752080530473, policy loss: 8.209055411081785
Experience 19, Iter 42, disc loss: 0.0008872116180818803, policy loss: 8.304992842589645
Experience 19, Iter 43, disc loss: 0.0009680487159518811, policy loss: 9.443267090798784
Experience 19, Iter 44, disc loss: 0.0008740308234691067, policy loss: 8.617713070371755
Experience 19, Iter 45, disc loss: 0.001065866350834556, policy loss: 8.233386424691258
Experience 19, Iter 46, disc loss: 0.00097997032434016, policy loss: 8.777889842521207
Experience 19, Iter 47, disc loss: 0.0009036013959687608, policy loss: 8.78858647245408
Experience 19, Iter 48, disc loss: 0.0008784375166363209, policy loss: 9.247648562650175
Experience 19, Iter 49, disc loss: 0.0010882024368833315, policy loss: 8.774678553022067
Experience 19, Iter 50, disc loss: 0.0010144985738801575, policy loss: 8.72018061810454
Experience 19, Iter 51, disc loss: 0.0008961291573697742, policy loss: 8.404760071511571
Experience 19, Iter 52, disc loss: 0.0009638167330934166, policy loss: 8.242244957480885
Experience 19, Iter 53, disc loss: 0.0010118975357014453, policy loss: 8.178304426966221
Experience 19, Iter 54, disc loss: 0.0009862415223598178, policy loss: 8.468774366910418
Experience 19, Iter 55, disc loss: 0.0008648285862458238, policy loss: 9.022738011886778
Experience 19, Iter 56, disc loss: 0.0009361642208154666, policy loss: 8.519521962420795
Experience 19, Iter 57, disc loss: 0.00087808680482979, policy loss: 8.546125614120008
Experience 19, Iter 58, disc loss: 0.0009149820664926535, policy loss: 8.41503098934177
Experience 19, Iter 59, disc loss: 0.0009574657286254152, policy loss: 8.93439787351802
Experience 19, Iter 60, disc loss: 0.0009672932229408825, policy loss: 8.620533112265047
Experience 19, Iter 61, disc loss: 0.0009192430628047646, policy loss: 8.746896603394713
Experience 19, Iter 62, disc loss: 0.0008978717292629335, policy loss: 8.388396773114387
Experience 19, Iter 63, disc loss: 0.0009113182475666106, policy loss: 8.190201643381222
Experience 19, Iter 64, disc loss: 0.001003114456111551, policy loss: 8.851944540279662
Experience 19, Iter 65, disc loss: 0.0008724317044567528, policy loss: 9.210370950573862
Experience 19, Iter 66, disc loss: 0.0010774842502431301, policy loss: 8.049064629868774
Experience 19, Iter 67, disc loss: 0.0009839370311630598, policy loss: 8.603840099678349
Experience 19, Iter 68, disc loss: 0.0008342297463650847, policy loss: 9.173943228476775
Experience 19, Iter 69, disc loss: 0.0011090140272880362, policy loss: 8.368485624303863
Experience 19, Iter 70, disc loss: 0.0009046142762841373, policy loss: 9.105061426730273
Experience 19, Iter 71, disc loss: 0.001019650072913162, policy loss: 8.317980735913775
Experience 19, Iter 72, disc loss: 0.0009793266776123607, policy loss: 8.139620305805792
Experience 19, Iter 73, disc loss: 0.0009201405265876953, policy loss: 8.650044791848194
Experience 19, Iter 74, disc loss: 0.0009521934832212373, policy loss: 9.527004729967302
Experience 19, Iter 75, disc loss: 0.0007914854829702767, policy loss: 8.542965586914232
Experience 19, Iter 76, disc loss: 0.0009219291861901007, policy loss: 8.610672526153104
Experience 19, Iter 77, disc loss: 0.0008008303002649444, policy loss: 9.027424531401206
Experience 19, Iter 78, disc loss: 0.000933606024644886, policy loss: 8.339095804040419
Experience 19, Iter 79, disc loss: 0.0009848874978888606, policy loss: 8.297169452298807
Experience 19, Iter 80, disc loss: 0.0008086104610919118, policy loss: 8.475425302187077
Experience 19, Iter 81, disc loss: 0.0009134931047914211, policy loss: 8.558558685076012
Experience 19, Iter 82, disc loss: 0.0009449949817559376, policy loss: 8.724353848487397
Experience 19, Iter 83, disc loss: 0.0009459173637297616, policy loss: 8.326645980879425
Experience 19, Iter 84, disc loss: 0.0009265736879428795, policy loss: 8.262435031347007
Experience 19, Iter 85, disc loss: 0.0009495311358331647, policy loss: 9.379527926235227
Experience 19, Iter 86, disc loss: 0.0010095051530076126, policy loss: 7.922082194472205
Experience 19, Iter 87, disc loss: 0.0008607140058808012, policy loss: 8.296079226932171
Experience 19, Iter 88, disc loss: 0.0008415710801864233, policy loss: 8.311171094033362
Experience 19, Iter 89, disc loss: 0.0008415903614136024, policy loss: 8.71307001707159
Experience 19, Iter 90, disc loss: 0.000938814623657684, policy loss: 8.43497745094272
Experience 19, Iter 91, disc loss: 0.0009145185657260669, policy loss: 8.141694059738942
Experience 19, Iter 92, disc loss: 0.0009203793310599783, policy loss: 8.585285942167799
Experience 19, Iter 93, disc loss: 0.001013142657353612, policy loss: 8.315923625576485
Experience 19, Iter 94, disc loss: 0.0010442518989967086, policy loss: 7.791448650116958
Experience 19, Iter 95, disc loss: 0.000951992027113679, policy loss: 8.457265032575929
Experience 19, Iter 96, disc loss: 0.0009007227679658427, policy loss: 9.302897136140311
Experience 19, Iter 97, disc loss: 0.0008959645218207821, policy loss: 8.889024917309047
Experience 19, Iter 98, disc loss: 0.0008962265967150664, policy loss: 8.963179518684836
Experience 19, Iter 99, disc loss: 0.0010706248512368096, policy loss: 8.547745537924763
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0073],
        [0.2908],
        [2.1823],
        [0.0485]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0322, 0.2973, 2.2251, 0.0332, 0.0316, 6.5987]],

        [[0.0322, 0.2973, 2.2251, 0.0332, 0.0316, 6.5987]],

        [[0.0322, 0.2973, 2.2251, 0.0332, 0.0316, 6.5987]],

        [[0.0322, 0.2973, 2.2251, 0.0332, 0.0316, 6.5987]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0293, 1.1633, 8.7293, 0.1942], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0293, 1.1633, 8.7293, 0.1942])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.983
Iter 2/2000 - Loss: 3.740
Iter 3/2000 - Loss: 3.586
Iter 4/2000 - Loss: 3.397
Iter 5/2000 - Loss: 3.231
Iter 6/2000 - Loss: 3.085
Iter 7/2000 - Loss: 2.903
Iter 8/2000 - Loss: 2.684
Iter 9/2000 - Loss: 2.456
Iter 10/2000 - Loss: 2.233
Iter 11/2000 - Loss: 2.006
Iter 12/2000 - Loss: 1.769
Iter 13/2000 - Loss: 1.523
Iter 14/2000 - Loss: 1.269
Iter 15/2000 - Loss: 1.010
Iter 16/2000 - Loss: 0.746
Iter 17/2000 - Loss: 0.479
Iter 18/2000 - Loss: 0.209
Iter 19/2000 - Loss: -0.062
Iter 20/2000 - Loss: -0.332
Iter 1981/2000 - Loss: -7.568
Iter 1982/2000 - Loss: -7.568
Iter 1983/2000 - Loss: -7.568
Iter 1984/2000 - Loss: -7.568
Iter 1985/2000 - Loss: -7.568
Iter 1986/2000 - Loss: -7.568
Iter 1987/2000 - Loss: -7.568
Iter 1988/2000 - Loss: -7.568
Iter 1989/2000 - Loss: -7.568
Iter 1990/2000 - Loss: -7.568
Iter 1991/2000 - Loss: -7.568
Iter 1992/2000 - Loss: -7.568
Iter 1993/2000 - Loss: -7.568
Iter 1994/2000 - Loss: -7.568
Iter 1995/2000 - Loss: -7.568
Iter 1996/2000 - Loss: -7.568
Iter 1997/2000 - Loss: -7.568
Iter 1998/2000 - Loss: -7.568
Iter 1999/2000 - Loss: -7.568
Iter 2000/2000 - Loss: -7.568
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0024],
        [0.0003]])
Lengthscale: tensor([[[14.8614, 11.4142, 32.0732,  5.7401,  5.5084, 70.8629]],

        [[20.9777, 34.7202,  7.6875,  1.2914,  1.7322, 30.0690]],

        [[22.5081, 34.9129,  8.6698,  1.0401,  0.8724, 24.5785]],

        [[16.1538, 30.9773, 11.0372,  2.1826,  1.9549, 37.4555]]])
Signal Variance: tensor([ 0.2056,  2.6986, 17.0867,  0.4653])
Estimated target variance: tensor([0.0293, 1.1633, 8.7293, 0.1942])
N: 200
Signal to noise ratio: tensor([25.0862, 97.3191, 84.5004, 41.8558])
Bound on condition number: tensor([ 125864.6596, 1894204.3467, 1428063.7902,  350382.3892])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.00095547703519703, policy loss: 8.066209404969598
Experience 20, Iter 1, disc loss: 0.001017974860638862, policy loss: 8.234329343710012
Experience 20, Iter 2, disc loss: 0.0010420907490671324, policy loss: 8.128755711207555
Experience 20, Iter 3, disc loss: 0.0009393391090404869, policy loss: 8.666360941735038
Experience 20, Iter 4, disc loss: 0.0008979790350579171, policy loss: 8.283966855755406
Experience 20, Iter 5, disc loss: 0.0008789331939565552, policy loss: 8.416122298948508
Experience 20, Iter 6, disc loss: 0.0009848994285040092, policy loss: 8.582385029541586
Experience 20, Iter 7, disc loss: 0.0008891760426249728, policy loss: 8.748781571486568
Experience 20, Iter 8, disc loss: 0.000900310147547578, policy loss: 8.301164661278285
Experience 20, Iter 9, disc loss: 0.0008586374058256554, policy loss: 9.178618160430833
Experience 20, Iter 10, disc loss: 0.0009029510331545125, policy loss: 9.4512142072638
Experience 20, Iter 11, disc loss: 0.0009180405632874311, policy loss: 8.253827465769891
Experience 20, Iter 12, disc loss: 0.0008903766586565951, policy loss: 8.817784800073833
Experience 20, Iter 13, disc loss: 0.0009314482552304317, policy loss: 8.230744669571948
Experience 20, Iter 14, disc loss: 0.0008811652924609866, policy loss: 9.417787126388804
Experience 20, Iter 15, disc loss: 0.0009708224728430505, policy loss: 8.333058463347484
Experience 20, Iter 16, disc loss: 0.0008355095738039563, policy loss: 9.333916487543593
Experience 20, Iter 17, disc loss: 0.0009913451144396884, policy loss: 8.226322069365782
Experience 20, Iter 18, disc loss: 0.0008618371475551984, policy loss: 9.478367345384697
Experience 20, Iter 19, disc loss: 0.0009527705035467689, policy loss: 8.699818434551393
Experience 20, Iter 20, disc loss: 0.0008643467333717778, policy loss: 8.609208717857605
Experience 20, Iter 21, disc loss: 0.0010804355326839996, policy loss: 8.78076935990704
Experience 20, Iter 22, disc loss: 0.0007865803682176135, policy loss: 8.74422386397373
Experience 20, Iter 23, disc loss: 0.0010204957938549307, policy loss: 8.60067677112332
Experience 20, Iter 24, disc loss: 0.000920582965816603, policy loss: 9.333773712525486
Experience 20, Iter 25, disc loss: 0.0008753696695004446, policy loss: 9.616579469519419
Experience 20, Iter 26, disc loss: 0.0008237758878996682, policy loss: 9.935584551380465
Experience 20, Iter 27, disc loss: 0.0008051791178003457, policy loss: 8.554228228487183
Experience 20, Iter 28, disc loss: 0.0009430125673485048, policy loss: 8.897764601771215
Experience 20, Iter 29, disc loss: 0.0009038141349219705, policy loss: 8.671838423270795
Experience 20, Iter 30, disc loss: 0.0008725893744636667, policy loss: 9.365897143422918
Experience 20, Iter 31, disc loss: 0.0008386349294780303, policy loss: 8.921216455382588
Experience 20, Iter 32, disc loss: 0.0008513146469609217, policy loss: 8.445236509608714
Experience 20, Iter 33, disc loss: 0.0008172752666934794, policy loss: 9.68017145801156
Experience 20, Iter 34, disc loss: 0.0007745811638264059, policy loss: 8.827904866589563
Experience 20, Iter 35, disc loss: 0.0008675364278929968, policy loss: 8.35729153232207
Experience 20, Iter 36, disc loss: 0.0008656126771763506, policy loss: 8.309989869521964
Experience 20, Iter 37, disc loss: 0.0008293843135740819, policy loss: 8.897412542927778
Experience 20, Iter 38, disc loss: 0.0009130632610032249, policy loss: 8.563013432285853
Experience 20, Iter 39, disc loss: 0.0007698139611600536, policy loss: 8.96480888915626
Experience 20, Iter 40, disc loss: 0.0009453917308125256, policy loss: 8.27683905276607
Experience 20, Iter 41, disc loss: 0.000788606195054536, policy loss: 9.613715938195627
Experience 20, Iter 42, disc loss: 0.0008708233463510562, policy loss: 8.474524849638856
Experience 20, Iter 43, disc loss: 0.0008249090910527419, policy loss: 8.34945415804751
Experience 20, Iter 44, disc loss: 0.0007872735814191088, policy loss: 8.495413850644878
Experience 20, Iter 45, disc loss: 0.0009604898728349809, policy loss: 8.2448633316618
Experience 20, Iter 46, disc loss: 0.000878200567370787, policy loss: 8.298493702824924
Experience 20, Iter 47, disc loss: 0.000982693796648064, policy loss: 8.48411197375915
Experience 20, Iter 48, disc loss: 0.0008650417785725114, policy loss: 8.275203977648276
Experience 20, Iter 49, disc loss: 0.0008863726587866979, policy loss: 8.051960291955279
Experience 20, Iter 50, disc loss: 0.0008905561637066015, policy loss: 8.961082235145263
Experience 20, Iter 51, disc loss: 0.0008519520520446353, policy loss: 8.787544263609389
Experience 20, Iter 52, disc loss: 0.0007338741556951271, policy loss: 9.218427414754304
Experience 20, Iter 53, disc loss: 0.0008332874196306972, policy loss: 8.308985639194042
Experience 20, Iter 54, disc loss: 0.0008112228617123456, policy loss: 9.702992217269367
Experience 20, Iter 55, disc loss: 0.0007622974836076712, policy loss: 9.942769844461134
Experience 20, Iter 56, disc loss: 0.0008002317838242392, policy loss: 8.94891929425248
Experience 20, Iter 57, disc loss: 0.0008342100959787414, policy loss: 8.845304681254003
Experience 20, Iter 58, disc loss: 0.0007920733069229167, policy loss: 8.518160415997691
Experience 20, Iter 59, disc loss: 0.0008725174668711975, policy loss: 8.887657799356553
Experience 20, Iter 60, disc loss: 0.0008369631432524687, policy loss: 8.728614224611277
Experience 20, Iter 61, disc loss: 0.0007902377668123999, policy loss: 9.238834966386595
Experience 20, Iter 62, disc loss: 0.0008497064646872561, policy loss: 8.644823120653697
Experience 20, Iter 63, disc loss: 0.0007429339385217383, policy loss: 8.86275212140048
Experience 20, Iter 64, disc loss: 0.0009865161233595667, policy loss: 8.220844842653765
Experience 20, Iter 65, disc loss: 0.0008329036263110542, policy loss: 9.110867838608183
Experience 20, Iter 66, disc loss: 0.0007198537046773824, policy loss: 8.605130600200997
Experience 20, Iter 67, disc loss: 0.0007341811464454807, policy loss: 8.704710805032224
Experience 20, Iter 68, disc loss: 0.000697149338822252, policy loss: 9.550537593047487
Experience 20, Iter 69, disc loss: 0.0007740142711644797, policy loss: 9.325590678928881
Experience 20, Iter 70, disc loss: 0.0008129168444834395, policy loss: 8.853672507164763
Experience 20, Iter 71, disc loss: 0.0008371799837463943, policy loss: 9.034079965388537
Experience 20, Iter 72, disc loss: 0.0007989026784026544, policy loss: 8.344690631314414
Experience 20, Iter 73, disc loss: 0.0009053609901089255, policy loss: 8.218904140205208
Experience 20, Iter 74, disc loss: 0.0007995842133936414, policy loss: 9.352289127963314
Experience 20, Iter 75, disc loss: 0.0007565815730128098, policy loss: 9.228728757146682
Experience 20, Iter 76, disc loss: 0.0008842212539365713, policy loss: 8.113692033715878
Experience 20, Iter 77, disc loss: 0.0007622291169242879, policy loss: 9.465669392284815
Experience 20, Iter 78, disc loss: 0.0007467903817103678, policy loss: 8.668456358066285
Experience 20, Iter 79, disc loss: 0.0008051428582447319, policy loss: 8.211766768672021
Experience 20, Iter 80, disc loss: 0.0007295524646429891, policy loss: 8.867515279753647
Experience 20, Iter 81, disc loss: 0.000718862911159673, policy loss: 9.089260989511885
Experience 20, Iter 82, disc loss: 0.0007916591010624869, policy loss: 8.35834031090876
Experience 20, Iter 83, disc loss: 0.0007205351489558782, policy loss: 8.601982950163453
Experience 20, Iter 84, disc loss: 0.0006954364538108335, policy loss: 8.581081425301438
Experience 20, Iter 85, disc loss: 0.0008171558828768153, policy loss: 8.354155397025217
Experience 20, Iter 86, disc loss: 0.0007528700467864367, policy loss: 9.108432119532159
Experience 20, Iter 87, disc loss: 0.0008471552777523048, policy loss: 8.699428134758264
Experience 20, Iter 88, disc loss: 0.0008287533874000281, policy loss: 8.368249807608091
Experience 20, Iter 89, disc loss: 0.0007164968060721624, policy loss: 9.949990264593081
Experience 20, Iter 90, disc loss: 0.000813226714972344, policy loss: 8.417585175403698
Experience 20, Iter 91, disc loss: 0.0007129589619700896, policy loss: 8.935934335216631
Experience 20, Iter 92, disc loss: 0.0007678208067112617, policy loss: 8.464130302854922
Experience 20, Iter 93, disc loss: 0.0008920499133401375, policy loss: 8.376809037282113
Experience 20, Iter 94, disc loss: 0.0008865242264432413, policy loss: 9.065569985427906
Experience 20, Iter 95, disc loss: 0.0007536448193229794, policy loss: 9.888496667101581
Experience 20, Iter 96, disc loss: 0.0007959449659084492, policy loss: 8.685373638525672
Experience 20, Iter 97, disc loss: 0.0007467019381355393, policy loss: 8.85882724635951
Experience 20, Iter 98, disc loss: 0.0008570158478053761, policy loss: 8.464056695050292
Experience 20, Iter 99, disc loss: 0.0007700904356691508, policy loss: 8.79945962291718
