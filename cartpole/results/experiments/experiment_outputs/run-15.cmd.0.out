Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0034],
        [0.0973],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0040, 0.0134, 0.3893, 0.0054], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0040, 0.0134, 0.3893, 0.0054])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.363
Iter 2/2000 - Loss: -1.430
Iter 3/2000 - Loss: -1.998
Iter 4/2000 - Loss: -1.630
Iter 5/2000 - Loss: -1.435
Iter 6/2000 - Loss: -1.633
Iter 7/2000 - Loss: -1.864
Iter 8/2000 - Loss: -1.909
Iter 9/2000 - Loss: -1.821
Iter 10/2000 - Loss: -1.795
Iter 11/2000 - Loss: -1.919
Iter 12/2000 - Loss: -2.107
Iter 13/2000 - Loss: -2.219
Iter 14/2000 - Loss: -2.198
Iter 15/2000 - Loss: -2.103
Iter 16/2000 - Loss: -2.039
Iter 17/2000 - Loss: -2.056
Iter 18/2000 - Loss: -2.125
Iter 19/2000 - Loss: -2.201
Iter 20/2000 - Loss: -2.270
Iter 1981/2000 - Loss: -2.532
Iter 1982/2000 - Loss: -2.531
Iter 1983/2000 - Loss: -2.531
Iter 1984/2000 - Loss: -2.531
Iter 1985/2000 - Loss: -2.532
Iter 1986/2000 - Loss: -2.532
Iter 1987/2000 - Loss: -2.532
Iter 1988/2000 - Loss: -2.532
Iter 1989/2000 - Loss: -2.532
Iter 1990/2000 - Loss: -2.533
Iter 1991/2000 - Loss: -2.532
Iter 1992/2000 - Loss: -2.532
Iter 1993/2000 - Loss: -2.532
Iter 1994/2000 - Loss: -2.533
Iter 1995/2000 - Loss: -2.533
Iter 1996/2000 - Loss: -2.532
Iter 1997/2000 - Loss: -2.532
Iter 1998/2000 - Loss: -2.532
Iter 1999/2000 - Loss: -2.533
Iter 2000/2000 - Loss: -2.533
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0024],
        [0.0652],
        [0.0010]])
Lengthscale: tensor([[[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]],

        [[1.0080e-02, 4.2468e-02, 6.7427e-02, 1.5042e-03, 5.7703e-05,
          2.5531e-02]]])
Signal Variance: tensor([0.0029, 0.0097, 0.2849, 0.0039])
Estimated target variance: tensor([0.0040, 0.0134, 0.3893, 0.0054])
N: 10
Signal to noise ratio: tensor([2.0012, 2.0049, 2.0902, 2.0021])
Bound on condition number: tensor([41.0477, 41.1951, 44.6893, 41.0851])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.2920696075482154, policy loss: 0.722217008687343
Experience 1, Iter 1, disc loss: 1.2741760744057469, policy loss: 0.7251044639347941
Experience 1, Iter 2, disc loss: 1.257013953249278, policy loss: 0.7272083029444529
Experience 1, Iter 3, disc loss: 1.2431462267969238, policy loss: 0.7256294887009138
Experience 1, Iter 4, disc loss: 1.2290765600229316, policy loss: 0.7242447390825375
Experience 1, Iter 5, disc loss: 1.210490498637629, policy loss: 0.728609127012767
Experience 1, Iter 6, disc loss: 1.1956871751686655, policy loss: 0.7289046041494072
Experience 1, Iter 7, disc loss: 1.1808948058333428, policy loss: 0.7294406950454708
Experience 1, Iter 8, disc loss: 1.1650026716618902, policy loss: 0.7313580397845993
Experience 1, Iter 9, disc loss: 1.1518245343258413, policy loss: 0.7307379411257432
Experience 1, Iter 10, disc loss: 1.138463495995632, policy loss: 0.7301219305159422
Experience 1, Iter 11, disc loss: 1.1226570111928393, policy loss: 0.7331071241114773
Experience 1, Iter 12, disc loss: 1.1114356772984433, policy loss: 0.7307980645837604
Experience 1, Iter 13, disc loss: 1.0930302421581648, policy loss: 0.7366434257593201
Experience 1, Iter 14, disc loss: 1.0822251968066525, policy loss: 0.7345437305612728
Experience 1, Iter 15, disc loss: 1.065634804941941, policy loss: 0.7390670496988652
Experience 1, Iter 16, disc loss: 1.055731105654929, policy loss: 0.7365038427226595
Experience 1, Iter 17, disc loss: 1.041501185164478, policy loss: 0.7389055056358891
Experience 1, Iter 18, disc loss: 1.0301012264622336, policy loss: 0.7385501230377794
Experience 1, Iter 19, disc loss: 1.0133239435966697, policy loss: 0.7445142705125067
Experience 1, Iter 20, disc loss: 0.99896534916656, policy loss: 0.7478656229833576
Experience 1, Iter 21, disc loss: 0.988285548670222, policy loss: 0.7480849293837013
Experience 1, Iter 22, disc loss: 0.9721198547674351, policy loss: 0.7560223635752059
Experience 1, Iter 23, disc loss: 0.9634151701850915, policy loss: 0.7558847938903239
Experience 1, Iter 24, disc loss: 0.9493721970531557, policy loss: 0.7622714691763397
Experience 1, Iter 25, disc loss: 0.9439577894590344, policy loss: 0.7584650302839229
Experience 1, Iter 26, disc loss: 0.9302281906753116, policy loss: 0.7643794373519383
Experience 1, Iter 27, disc loss: 0.9190925885085803, policy loss: 0.7676304476498963
Experience 1, Iter 28, disc loss: 0.9065622942758649, policy loss: 0.7725690484572965
Experience 1, Iter 29, disc loss: 0.8927916611164011, policy loss: 0.779163642083173
Experience 1, Iter 30, disc loss: 0.8883510350053017, policy loss: 0.7747993888911888
Experience 1, Iter 31, disc loss: 0.870220249425498, policy loss: 0.7873693934404743
Experience 1, Iter 32, disc loss: 0.8647881380865661, policy loss: 0.783804458135125
Experience 1, Iter 33, disc loss: 0.8481656501302771, policy loss: 0.7949136454883712
Experience 1, Iter 34, disc loss: 0.8406729127963108, policy loss: 0.7944848222801824
Experience 1, Iter 35, disc loss: 0.8287586467099226, policy loss: 0.7998706947676925
Experience 1, Iter 36, disc loss: 0.8117375251286263, policy loss: 0.8122179296685246
Experience 1, Iter 37, disc loss: 0.8037999376849234, policy loss: 0.8127333340747791
Experience 1, Iter 38, disc loss: 0.7883000930264565, policy loss: 0.8234330945491695
Experience 1, Iter 39, disc loss: 0.7772215448040425, policy loss: 0.8276419427662212
Experience 1, Iter 40, disc loss: 0.7627309931639206, policy loss: 0.8362960272216486
Experience 1, Iter 41, disc loss: 0.7480982580906661, policy loss: 0.8460618519887615
Experience 1, Iter 42, disc loss: 0.7339736713285131, policy loss: 0.8534569339130647
Experience 1, Iter 43, disc loss: 0.7257183324225713, policy loss: 0.8529894640568606
Experience 1, Iter 44, disc loss: 0.7125056431217376, policy loss: 0.8609185900288312
Experience 1, Iter 45, disc loss: 0.6951949646060986, policy loss: 0.8735920296993913
Experience 1, Iter 46, disc loss: 0.6861379258888938, policy loss: 0.876292278444748
Experience 1, Iter 47, disc loss: 0.6734767071088078, policy loss: 0.8839869183260776
Experience 1, Iter 48, disc loss: 0.6578485312496389, policy loss: 0.896578426300696
Experience 1, Iter 49, disc loss: 0.645307772137742, policy loss: 0.9052876391473275
Experience 1, Iter 50, disc loss: 0.6331484721456637, policy loss: 0.9140392151772857
Experience 1, Iter 51, disc loss: 0.6260908155936483, policy loss: 0.9156417307465363
Experience 1, Iter 52, disc loss: 0.6043537157493327, policy loss: 0.9403735227084612
Experience 1, Iter 53, disc loss: 0.5893863044086691, policy loss: 0.95574808339969
Experience 1, Iter 54, disc loss: 0.5833436898172061, policy loss: 0.9573467315544204
Experience 1, Iter 55, disc loss: 0.5681265526458321, policy loss: 0.974228216251142
Experience 1, Iter 56, disc loss: 0.5565237678421588, policy loss: 0.9859932844108319
Experience 1, Iter 57, disc loss: 0.5497047794550872, policy loss: 0.9906414659621969
Experience 1, Iter 58, disc loss: 0.5423704736683093, policy loss: 0.9971902917702276
Experience 1, Iter 59, disc loss: 0.518234792824367, policy loss: 1.032820200138944
Experience 1, Iter 60, disc loss: 0.516543714726729, policy loss: 1.0289427258791943
Experience 1, Iter 61, disc loss: 0.5004745483419075, policy loss: 1.0518352676407636
Experience 1, Iter 62, disc loss: 0.48815004726056815, policy loss: 1.0683655162385621
Experience 1, Iter 63, disc loss: 0.47295455883618903, policy loss: 1.092411856619434
Experience 1, Iter 64, disc loss: 0.46375017415262587, policy loss: 1.1057360498201811
Experience 1, Iter 65, disc loss: 0.4553216528455145, policy loss: 1.1176351845479346
Experience 1, Iter 66, disc loss: 0.44258084895745164, policy loss: 1.1399189080840397
Experience 1, Iter 67, disc loss: 0.4357063432310199, policy loss: 1.1495041603688534
Experience 1, Iter 68, disc loss: 0.4279584828953518, policy loss: 1.1621142821435482
Experience 1, Iter 69, disc loss: 0.4128502907068088, policy loss: 1.189557973474615
Experience 1, Iter 70, disc loss: 0.4068519316919194, policy loss: 1.1989651745361125
Experience 1, Iter 71, disc loss: 0.3961690046644943, policy loss: 1.2206074685601915
Experience 1, Iter 72, disc loss: 0.3871618911548192, policy loss: 1.2398928566279017
Experience 1, Iter 73, disc loss: 0.3767866813502074, policy loss: 1.2596028712447396
Experience 1, Iter 74, disc loss: 0.3616926521944779, policy loss: 1.2952116953363755
Experience 1, Iter 75, disc loss: 0.3528804853262381, policy loss: 1.3214028943967802
Experience 1, Iter 76, disc loss: 0.350252899875168, policy loss: 1.3207238164252493
Experience 1, Iter 77, disc loss: 0.3266719300423134, policy loss: 1.3847457926534332
Experience 1, Iter 78, disc loss: 0.33341280580217564, policy loss: 1.3607245482112051
Experience 1, Iter 79, disc loss: 0.3197102185788027, policy loss: 1.3983896819337813
Experience 1, Iter 80, disc loss: 0.3139026179265343, policy loss: 1.4181274130162103
Experience 1, Iter 81, disc loss: 0.30030626129915, policy loss: 1.4547188207147823
Experience 1, Iter 82, disc loss: 0.30011266977576995, policy loss: 1.450829514598353
Experience 1, Iter 83, disc loss: 0.2874804215985754, policy loss: 1.492865198433963
Experience 1, Iter 84, disc loss: 0.27490666890759063, policy loss: 1.5349438721797746
Experience 1, Iter 85, disc loss: 0.2688870154670051, policy loss: 1.554617500997278
Experience 1, Iter 86, disc loss: 0.26635641289036627, policy loss: 1.5621357529057818
Experience 1, Iter 87, disc loss: 0.25883470204695275, policy loss: 1.5978884644927736
Experience 1, Iter 88, disc loss: 0.24314557441949028, policy loss: 1.6441693542132008
Experience 1, Iter 89, disc loss: 0.23093904055704106, policy loss: 1.6912955712303428
Experience 1, Iter 90, disc loss: 0.22685703304640603, policy loss: 1.70406551131855
Experience 1, Iter 91, disc loss: 0.2267507964150333, policy loss: 1.7073697443087603
Experience 1, Iter 92, disc loss: 0.21451339738015124, policy loss: 1.7706439172925528
Experience 1, Iter 93, disc loss: 0.2062519910236559, policy loss: 1.8044545295228345
Experience 1, Iter 94, disc loss: 0.19944681934748032, policy loss: 1.8316388170313371
Experience 1, Iter 95, disc loss: 0.20171444608420253, policy loss: 1.8261009708593354
Experience 1, Iter 96, disc loss: 0.18798514106229697, policy loss: 1.8863054707818154
Experience 1, Iter 97, disc loss: 0.19006192729352084, policy loss: 1.8838442001481959
Experience 1, Iter 98, disc loss: 0.17005915291346713, policy loss: 1.9854532917215448
Experience 1, Iter 99, disc loss: 0.1811676885116058, policy loss: 1.9185727617132335
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0014],
        [0.0029],
        [0.0789],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0057, 0.0115, 0.3155, 0.0050], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0057, 0.0115, 0.3155, 0.0050])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.035
Iter 2/2000 - Loss: -0.972
Iter 3/2000 - Loss: -2.014
Iter 4/2000 - Loss: -1.762
Iter 5/2000 - Loss: -1.442
Iter 6/2000 - Loss: -1.500
Iter 7/2000 - Loss: -1.665
Iter 8/2000 - Loss: -1.717
Iter 9/2000 - Loss: -1.661
Iter 10/2000 - Loss: -1.645
Iter 11/2000 - Loss: -1.762
Iter 12/2000 - Loss: -1.959
Iter 13/2000 - Loss: -2.121
Iter 14/2000 - Loss: -2.171
Iter 15/2000 - Loss: -2.125
Iter 16/2000 - Loss: -2.056
Iter 17/2000 - Loss: -2.019
Iter 18/2000 - Loss: -2.021
Iter 19/2000 - Loss: -2.042
Iter 20/2000 - Loss: -2.077
Iter 1981/2000 - Loss: -2.469
Iter 1982/2000 - Loss: -2.469
Iter 1983/2000 - Loss: -2.469
Iter 1984/2000 - Loss: -2.469
Iter 1985/2000 - Loss: -2.469
Iter 1986/2000 - Loss: -2.469
Iter 1987/2000 - Loss: -2.469
Iter 1988/2000 - Loss: -2.469
Iter 1989/2000 - Loss: -2.469
Iter 1990/2000 - Loss: -2.469
Iter 1991/2000 - Loss: -2.469
Iter 1992/2000 - Loss: -2.469
Iter 1993/2000 - Loss: -2.469
Iter 1994/2000 - Loss: -2.469
Iter 1995/2000 - Loss: -2.469
Iter 1996/2000 - Loss: -2.469
Iter 1997/2000 - Loss: -2.469
Iter 1998/2000 - Loss: -2.469
Iter 1999/2000 - Loss: -2.469
Iter 2000/2000 - Loss: -2.469
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0022],
        [0.0569],
        [0.0010]])
Lengthscale: tensor([[[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]],

        [[1.2040e-02, 5.6519e-02, 5.6803e-02, 1.3988e-03, 7.4256e-05,
          3.3127e-02]]])
Signal Variance: tensor([0.0043, 0.0087, 0.2428, 0.0038])
Estimated target variance: tensor([0.0057, 0.0115, 0.3155, 0.0050])
N: 20
Signal to noise ratio: tensor([2.0019, 2.0046, 2.0657, 2.0020])
Bound on condition number: tensor([81.1492, 81.3706, 86.3439, 81.1601])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.15015769924738037, policy loss: 2.118586982979773
Experience 2, Iter 1, disc loss: 0.1444523937826554, policy loss: 2.153576986304105
Experience 2, Iter 2, disc loss: 0.12864693382578213, policy loss: 2.2683929079936247
Experience 2, Iter 3, disc loss: 0.13180507935088512, policy loss: 2.240274994927799
Experience 2, Iter 4, disc loss: 0.12646844450114622, policy loss: 2.283075406713582
Experience 2, Iter 5, disc loss: 0.11764122436846952, policy loss: 2.346556821701837
Experience 2, Iter 6, disc loss: 0.11282841505864151, policy loss: 2.4017430210295183
Experience 2, Iter 7, disc loss: 0.1178751152951534, policy loss: 2.360548595756891
Experience 2, Iter 8, disc loss: 0.10750880173388519, policy loss: 2.439322414798177
Experience 2, Iter 9, disc loss: 0.10826517055868888, policy loss: 2.453426687346001
Experience 2, Iter 10, disc loss: 0.09482788256173041, policy loss: 2.571156809436419
Experience 2, Iter 11, disc loss: 0.10197017600057391, policy loss: 2.53134024761272
Experience 2, Iter 12, disc loss: 0.09804561302139456, policy loss: 2.543875412355956
Experience 2, Iter 13, disc loss: 0.09666685232302658, policy loss: 2.543445761686672
Experience 2, Iter 14, disc loss: 0.08480578827732434, policy loss: 2.689609662208547
Experience 2, Iter 15, disc loss: 0.08658903444869692, policy loss: 2.677219473625464
Experience 2, Iter 16, disc loss: 0.08375986336604228, policy loss: 2.708958291892871
Experience 2, Iter 17, disc loss: 0.08236618678880195, policy loss: 2.7427503508243536
Experience 2, Iter 18, disc loss: 0.07900714988423477, policy loss: 2.755813436043206
Experience 2, Iter 19, disc loss: 0.07199233178994653, policy loss: 2.866954879967519
Experience 2, Iter 20, disc loss: 0.06951148115042195, policy loss: 2.9111183697733756
Experience 2, Iter 21, disc loss: 0.06879777012766708, policy loss: 2.925059297730569
Experience 2, Iter 22, disc loss: 0.06822237127406042, policy loss: 2.909632781464813
Experience 2, Iter 23, disc loss: 0.06592465585218303, policy loss: 2.9427036708234473
Experience 2, Iter 24, disc loss: 0.060883963822640226, policy loss: 3.03498341776249
Experience 2, Iter 25, disc loss: 0.060279540131608696, policy loss: 3.0608225216070446
Experience 2, Iter 26, disc loss: 0.06615110094046742, policy loss: 2.969423829106689
Experience 2, Iter 27, disc loss: 0.060959583420682965, policy loss: 3.0362193695388697
Experience 2, Iter 28, disc loss: 0.05768760892689119, policy loss: 3.090133757589585
Experience 2, Iter 29, disc loss: 0.053647316567322974, policy loss: 3.17677536012231
Experience 2, Iter 30, disc loss: 0.05877131940169418, policy loss: 3.0807167572796357
Experience 2, Iter 31, disc loss: 0.05453843482347011, policy loss: 3.138591177698338
Experience 2, Iter 32, disc loss: 0.050643431634474365, policy loss: 3.2242658053313935
Experience 2, Iter 33, disc loss: 0.049048048870985864, policy loss: 3.2695240338849496
Experience 2, Iter 34, disc loss: 0.04716257910806551, policy loss: 3.288578871959029
Experience 2, Iter 35, disc loss: 0.046860165223966445, policy loss: 3.295172686204545
Experience 2, Iter 36, disc loss: 0.04335447411223155, policy loss: 3.394860319475606
Experience 2, Iter 37, disc loss: 0.04577532473273058, policy loss: 3.3579444296617713
Experience 2, Iter 38, disc loss: 0.045734848245568505, policy loss: 3.33237170963782
Experience 2, Iter 39, disc loss: 0.04299277246766254, policy loss: 3.4193999396985757
Experience 2, Iter 40, disc loss: 0.04322417734829412, policy loss: 3.400497114018542
Experience 2, Iter 41, disc loss: 0.04216429279705348, policy loss: 3.4034533297296394
Experience 2, Iter 42, disc loss: 0.03702613537014937, policy loss: 3.554498980503398
Experience 2, Iter 43, disc loss: 0.038288526619351594, policy loss: 3.5162738551853954
Experience 2, Iter 44, disc loss: 0.04002787208833798, policy loss: 3.4428522554534116
Experience 2, Iter 45, disc loss: 0.038476930957809796, policy loss: 3.5199118224843273
Experience 2, Iter 46, disc loss: 0.03881608641998815, policy loss: 3.5213556380437816
Experience 2, Iter 47, disc loss: 0.03703494419135903, policy loss: 3.5474979940350897
Experience 2, Iter 48, disc loss: 0.03325504343965362, policy loss: 3.6792430733784567
Experience 2, Iter 49, disc loss: 0.03744930558687882, policy loss: 3.5366353450776056
Experience 2, Iter 50, disc loss: 0.03350097746335016, policy loss: 3.6719294309801143
Experience 2, Iter 51, disc loss: 0.03546383544637486, policy loss: 3.6510171205331403
Experience 2, Iter 52, disc loss: 0.03563580830625597, policy loss: 3.6320060023608343
Experience 2, Iter 53, disc loss: 0.03231829028495367, policy loss: 3.714650174683019
Experience 2, Iter 54, disc loss: 0.03175185815313367, policy loss: 3.7322161512957783
Experience 2, Iter 55, disc loss: 0.02993815601056528, policy loss: 3.770598816191703
Experience 2, Iter 56, disc loss: 0.03149991491204301, policy loss: 3.746875612152202
Experience 2, Iter 57, disc loss: 0.028221586693916063, policy loss: 3.861736012528141
Experience 2, Iter 58, disc loss: 0.028208097728904667, policy loss: 3.83686659236158
Experience 2, Iter 59, disc loss: 0.027592650119527353, policy loss: 3.866372398314064
Experience 2, Iter 60, disc loss: 0.026088755801355303, policy loss: 3.8856152602109155
Experience 2, Iter 61, disc loss: 0.029236500497706625, policy loss: 3.8495567958750425
Experience 2, Iter 62, disc loss: 0.02567086399926569, policy loss: 3.953621070543835
Experience 2, Iter 63, disc loss: 0.025473014353484865, policy loss: 4.0115456165799985
Experience 2, Iter 64, disc loss: 0.02330925396138067, policy loss: 4.017168687097797
Experience 2, Iter 65, disc loss: 0.02397091850078123, policy loss: 4.043713429007465
Experience 2, Iter 66, disc loss: 0.022544109418744702, policy loss: 4.123627631973776
Experience 2, Iter 67, disc loss: 0.02760681689091482, policy loss: 3.9064578066670483
Experience 2, Iter 68, disc loss: 0.024316551021777, policy loss: 4.03953371042703
Experience 2, Iter 69, disc loss: 0.020241043723610803, policy loss: 4.217875112323685
Experience 2, Iter 70, disc loss: 0.0243674847782287, policy loss: 4.077431536398332
Experience 2, Iter 71, disc loss: 0.022977624169134188, policy loss: 4.129675925457622
Experience 2, Iter 72, disc loss: 0.023153745007991895, policy loss: 4.116514247527913
Experience 2, Iter 73, disc loss: 0.024930324763424584, policy loss: 4.117949987043768
Experience 2, Iter 74, disc loss: 0.02007791156102347, policy loss: 4.197082443885591
Experience 2, Iter 75, disc loss: 0.02169290049938436, policy loss: 4.1394429941412145
Experience 2, Iter 76, disc loss: 0.021013648281875755, policy loss: 4.169882920438951
Experience 2, Iter 77, disc loss: 0.020087345893658928, policy loss: 4.203464146712396
Experience 2, Iter 78, disc loss: 0.019703417880543153, policy loss: 4.238904722331642
Experience 2, Iter 79, disc loss: 0.01917775202532384, policy loss: 4.256750790188962
Experience 2, Iter 80, disc loss: 0.019453776249990614, policy loss: 4.21382341814348
Experience 2, Iter 81, disc loss: 0.01811454078540145, policy loss: 4.306536524441623
Experience 2, Iter 82, disc loss: 0.016460960369570513, policy loss: 4.4830158198226915
Experience 2, Iter 83, disc loss: 0.019284748361794012, policy loss: 4.301750089536184
Experience 2, Iter 84, disc loss: 0.019118065002565382, policy loss: 4.2938032262113595
Experience 2, Iter 85, disc loss: 0.019227062695580433, policy loss: 4.345375168217717
Experience 2, Iter 86, disc loss: 0.017465109034660913, policy loss: 4.362349263303737
Experience 2, Iter 87, disc loss: 0.01787519475893672, policy loss: 4.381510438047524
Experience 2, Iter 88, disc loss: 0.016483612420165415, policy loss: 4.428052515314033
Experience 2, Iter 89, disc loss: 0.01886478425220422, policy loss: 4.269340055911185
Experience 2, Iter 90, disc loss: 0.018201534951137955, policy loss: 4.342974392977533
Experience 2, Iter 91, disc loss: 0.015037932582712531, policy loss: 4.5631411577783725
Experience 2, Iter 92, disc loss: 0.018828971954867775, policy loss: 4.302502281161222
Experience 2, Iter 93, disc loss: 0.015005130847513046, policy loss: 4.535000698548907
Experience 2, Iter 94, disc loss: 0.01526224434180288, policy loss: 4.5012189331939
Experience 2, Iter 95, disc loss: 0.016085512017789516, policy loss: 4.521651091035025
Experience 2, Iter 96, disc loss: 0.017155183338364494, policy loss: 4.429405309251521
Experience 2, Iter 97, disc loss: 0.015010774035713034, policy loss: 4.535748831159058
Experience 2, Iter 98, disc loss: 0.01462819874723379, policy loss: 4.531067938966132
Experience 2, Iter 99, disc loss: 0.014143076538702495, policy loss: 4.624642705762511
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0055],
        [0.1556],
        [0.0026]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0065, 0.0222, 0.6225, 0.0103], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0065, 0.0222, 0.6225, 0.0103])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.326
Iter 2/2000 - Loss: -0.383
Iter 3/2000 - Loss: -0.957
Iter 4/2000 - Loss: -0.768
Iter 5/2000 - Loss: -0.619
Iter 6/2000 - Loss: -0.707
Iter 7/2000 - Loss: -0.821
Iter 8/2000 - Loss: -0.828
Iter 9/2000 - Loss: -0.763
Iter 10/2000 - Loss: -0.741
Iter 11/2000 - Loss: -0.802
Iter 12/2000 - Loss: -0.901
Iter 13/2000 - Loss: -0.989
Iter 14/2000 - Loss: -1.060
Iter 15/2000 - Loss: -1.119
Iter 16/2000 - Loss: -1.139
Iter 17/2000 - Loss: -1.097
Iter 18/2000 - Loss: -1.029
Iter 19/2000 - Loss: -1.004
Iter 20/2000 - Loss: -1.048
Iter 1981/2000 - Loss: -1.340
Iter 1982/2000 - Loss: -1.340
Iter 1983/2000 - Loss: -1.340
Iter 1984/2000 - Loss: -1.340
Iter 1985/2000 - Loss: -1.340
Iter 1986/2000 - Loss: -1.340
Iter 1987/2000 - Loss: -1.340
Iter 1988/2000 - Loss: -1.340
Iter 1989/2000 - Loss: -1.340
Iter 1990/2000 - Loss: -1.340
Iter 1991/2000 - Loss: -1.340
Iter 1992/2000 - Loss: -1.340
Iter 1993/2000 - Loss: -1.340
Iter 1994/2000 - Loss: -1.340
Iter 1995/2000 - Loss: -1.340
Iter 1996/2000 - Loss: -1.340
Iter 1997/2000 - Loss: -1.340
Iter 1998/2000 - Loss: -1.340
Iter 1999/2000 - Loss: -1.340
Iter 2000/2000 - Loss: -1.340
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0043],
        [0.1096],
        [0.0020]])
Lengthscale: tensor([[[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]],

        [[0.0133, 0.0639, 0.1151, 0.0029, 0.0002, 0.0708]]])
Signal Variance: tensor([0.0050, 0.0172, 0.4921, 0.0080])
Estimated target variance: tensor([0.0065, 0.0222, 0.6225, 0.0103])
N: 30
Signal to noise ratio: tensor([2.0021, 2.0082, 2.1189, 2.0048])
Bound on condition number: tensor([121.2473, 121.9831, 135.6866, 121.5821])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.01029619949414089, policy loss: 5.021600146544414
Experience 3, Iter 1, disc loss: 0.012910701929281964, policy loss: 4.959113886010579
Experience 3, Iter 2, disc loss: 0.009260767671119167, policy loss: 5.215502649445444
Experience 3, Iter 3, disc loss: 0.011527003629082294, policy loss: 5.067951320033491
Experience 3, Iter 4, disc loss: 0.0121480178261147, policy loss: 5.194067694974133
Experience 3, Iter 5, disc loss: 0.00963526985955616, policy loss: 5.211383592702238
Experience 3, Iter 6, disc loss: 0.011515085400603671, policy loss: 4.891510250334926
Experience 3, Iter 7, disc loss: 0.010636634815308795, policy loss: 5.028050094457784
Experience 3, Iter 8, disc loss: 0.012658962114865413, policy loss: 5.10579791293935
Experience 3, Iter 9, disc loss: 0.010573028723055745, policy loss: 5.136872382162411
Experience 3, Iter 10, disc loss: 0.007697951853232098, policy loss: 5.389399645323817
Experience 3, Iter 11, disc loss: 0.011017922353410765, policy loss: 5.2478289290085
Experience 3, Iter 12, disc loss: 0.00943490966295678, policy loss: 5.182550787837015
Experience 3, Iter 13, disc loss: 0.010041456318162662, policy loss: 5.1158043201656405
Experience 3, Iter 14, disc loss: 0.010927262920648134, policy loss: 5.03022319535525
Experience 3, Iter 15, disc loss: 0.01081957433181638, policy loss: 5.119593922904577
Experience 3, Iter 16, disc loss: 0.010516176724217697, policy loss: 5.152481305183235
Experience 3, Iter 17, disc loss: 0.007472627159059535, policy loss: 5.410429878649138
Experience 3, Iter 18, disc loss: 0.008471939356256172, policy loss: 5.203036247589779
Experience 3, Iter 19, disc loss: 0.007842165893300003, policy loss: 5.259170624291667
Experience 3, Iter 20, disc loss: 0.008472746564527216, policy loss: 5.363076453785803
Experience 3, Iter 21, disc loss: 0.008313702163548002, policy loss: 5.310860170097364
Experience 3, Iter 22, disc loss: 0.009220109906613845, policy loss: 5.203495342697571
Experience 3, Iter 23, disc loss: 0.0074357568402839535, policy loss: 5.444305290493762
Experience 3, Iter 24, disc loss: 0.00873179405176982, policy loss: 5.242389932601341
Experience 3, Iter 25, disc loss: 0.008291334059410157, policy loss: 5.331895840265548
Experience 3, Iter 26, disc loss: 0.009427424993675326, policy loss: 5.186701305296118
Experience 3, Iter 27, disc loss: 0.008632107427500564, policy loss: 5.333309464669674
Experience 3, Iter 28, disc loss: 0.009076960521530241, policy loss: 5.244893488548138
Experience 3, Iter 29, disc loss: 0.006216928104036758, policy loss: 5.664558960122264
Experience 3, Iter 30, disc loss: 0.00959189389026012, policy loss: 5.419746121401821
Experience 3, Iter 31, disc loss: 0.0070949731868375195, policy loss: 5.570834018694034
Experience 3, Iter 32, disc loss: 0.008297602218982288, policy loss: 5.449396753889207
Experience 3, Iter 33, disc loss: 0.006423854028276242, policy loss: 5.5433267418245435
Experience 3, Iter 34, disc loss: 0.006025008653004175, policy loss: 5.623343540686558
Experience 3, Iter 35, disc loss: 0.007639509934928519, policy loss: 5.603786436825155
Experience 3, Iter 36, disc loss: 0.006826781202270459, policy loss: 5.537723268513223
Experience 3, Iter 37, disc loss: 0.00674517463439773, policy loss: 5.586510608509633
Experience 3, Iter 38, disc loss: 0.009420170409290008, policy loss: 5.320906093324853
Experience 3, Iter 39, disc loss: 0.007737767032715068, policy loss: 5.547056909183302
Experience 3, Iter 40, disc loss: 0.00656290787995163, policy loss: 5.570981722465653
Experience 3, Iter 41, disc loss: 0.006734584021553802, policy loss: 5.5773301387029495
Experience 3, Iter 42, disc loss: 0.006420230771719185, policy loss: 5.601025310881504
Experience 3, Iter 43, disc loss: 0.006853494406831734, policy loss: 5.461945684179547
Experience 3, Iter 44, disc loss: 0.007360628578681096, policy loss: 5.467677262274376
Experience 3, Iter 45, disc loss: 0.006851454738504308, policy loss: 5.516571139953298
Experience 3, Iter 46, disc loss: 0.007459047246859345, policy loss: 5.48022409900342
Experience 3, Iter 47, disc loss: 0.007532295268269238, policy loss: 5.401847694677203
Experience 3, Iter 48, disc loss: 0.007606121511587197, policy loss: 5.464222925498264
Experience 3, Iter 49, disc loss: 0.006267323334487722, policy loss: 5.589778224190032
Experience 3, Iter 50, disc loss: 0.005315346442032614, policy loss: 5.7279091935902535
Experience 3, Iter 51, disc loss: 0.005854177756425429, policy loss: 5.674918642166212
Experience 3, Iter 52, disc loss: 0.00971023692574418, policy loss: 5.216627372500044
Experience 3, Iter 53, disc loss: 0.006057406512049664, policy loss: 5.6344593562220755
Experience 3, Iter 54, disc loss: 0.0064267769230704945, policy loss: 5.580044384134116
Experience 3, Iter 55, disc loss: 0.006029256837397348, policy loss: 5.667272965077059
Experience 3, Iter 56, disc loss: 0.006637804432957677, policy loss: 5.573632049845601
Experience 3, Iter 57, disc loss: 0.006396923293677961, policy loss: 5.6454444794880985
Experience 3, Iter 58, disc loss: 0.0058766133322118354, policy loss: 5.694792467440159
Experience 3, Iter 59, disc loss: 0.006795833104590559, policy loss: 5.567726148747006
Experience 3, Iter 60, disc loss: 0.006272869695487956, policy loss: 5.547560940272749
Experience 3, Iter 61, disc loss: 0.006295587917263013, policy loss: 5.665587155411398
Experience 3, Iter 62, disc loss: 0.005785964090079088, policy loss: 5.71129199603144
Experience 3, Iter 63, disc loss: 0.005687432952097286, policy loss: 5.767434959923681
Experience 3, Iter 64, disc loss: 0.007728335780873129, policy loss: 5.5002555300626135
Experience 3, Iter 65, disc loss: 0.011517371734320641, policy loss: 5.57015908914824
Experience 3, Iter 66, disc loss: 0.005052541660016055, policy loss: 5.811169492672213
Experience 3, Iter 67, disc loss: 0.005995270693718145, policy loss: 5.582410670027628
Experience 3, Iter 68, disc loss: 0.006117178186611553, policy loss: 5.712315355572024
Experience 3, Iter 69, disc loss: 0.007662526696591782, policy loss: 5.633218210245884
Experience 3, Iter 70, disc loss: 0.006723437645683778, policy loss: 5.588528795034696
Experience 3, Iter 71, disc loss: 0.0060397747153015215, policy loss: 5.630535258756309
Experience 3, Iter 72, disc loss: 0.0054188023198678, policy loss: 5.8121509371158
Experience 3, Iter 73, disc loss: 0.004957101058593974, policy loss: 5.806054209014014
Experience 3, Iter 74, disc loss: 0.00558806156237746, policy loss: 5.698320024281758
Experience 3, Iter 75, disc loss: 0.0055087595220730395, policy loss: 5.793549954231846
Experience 3, Iter 76, disc loss: 0.005150884878861941, policy loss: 5.8016778291758895
Experience 3, Iter 77, disc loss: 0.005281745085245953, policy loss: 5.795742961391295
Experience 3, Iter 78, disc loss: 0.005239593296578018, policy loss: 5.835814757857852
Experience 3, Iter 79, disc loss: 0.005732803102719867, policy loss: 5.810438747621298
Experience 3, Iter 80, disc loss: 0.0053282680869338956, policy loss: 5.836938513912909
Experience 3, Iter 81, disc loss: 0.004907610819284183, policy loss: 5.881196017449423
Experience 3, Iter 82, disc loss: 0.004607783590979203, policy loss: 5.928299811996652
Experience 3, Iter 83, disc loss: 0.005748807457180425, policy loss: 5.800455416633772
Experience 3, Iter 84, disc loss: 0.004742223873581116, policy loss: 5.955284993673395
Experience 3, Iter 85, disc loss: 0.008807590014424764, policy loss: 5.676331023342073
Experience 3, Iter 86, disc loss: 0.005753384658430754, policy loss: 5.9022450393609125
Experience 3, Iter 87, disc loss: 0.005074326740863531, policy loss: 5.803742786765343
Experience 3, Iter 88, disc loss: 0.004706840065673915, policy loss: 5.840792375287817
Experience 3, Iter 89, disc loss: 0.00475172510654434, policy loss: 5.94433235075131
Experience 3, Iter 90, disc loss: 0.005432255714335189, policy loss: 5.753723747561235
Experience 3, Iter 91, disc loss: 0.005442440959343579, policy loss: 5.862473923931535
Experience 3, Iter 92, disc loss: 0.0050188960331106714, policy loss: 5.861800228909749
Experience 3, Iter 93, disc loss: 0.005599427070008246, policy loss: 5.947712573744939
Experience 3, Iter 94, disc loss: 0.005612780178971592, policy loss: 5.770904632726589
Experience 3, Iter 95, disc loss: 0.004166417678806064, policy loss: 6.065548128371311
Experience 3, Iter 96, disc loss: 0.004329312070680551, policy loss: 6.036283965963047
Experience 3, Iter 97, disc loss: 0.005362196793860132, policy loss: 5.884219103971314
Experience 3, Iter 98, disc loss: 0.004156711535841711, policy loss: 6.081315769858386
Experience 3, Iter 99, disc loss: 0.00404884197516122, policy loss: 6.055021411959242
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0135],
        [0.1538],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.1196e-02, 1.2478e-01, 1.1479e-01, 5.0888e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1196e-02, 1.2478e-01, 1.1479e-01, 5.0888e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1196e-02, 1.2478e-01, 1.1479e-01, 5.0888e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1196e-02, 1.2478e-01, 1.1479e-01, 5.0888e-03, 1.9561e-04,
          4.3219e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0130, 0.0541, 0.6154, 0.0107], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0130, 0.0541, 0.6154, 0.0107])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.003
Iter 2/2000 - Loss: -0.077
Iter 3/2000 - Loss: -0.299
Iter 4/2000 - Loss: -0.256
Iter 5/2000 - Loss: -0.198
Iter 6/2000 - Loss: -0.266
Iter 7/2000 - Loss: -0.339
Iter 8/2000 - Loss: -0.384
Iter 9/2000 - Loss: -0.407
Iter 10/2000 - Loss: -0.412
Iter 11/2000 - Loss: -0.402
Iter 12/2000 - Loss: -0.400
Iter 13/2000 - Loss: -0.424
Iter 14/2000 - Loss: -0.451
Iter 15/2000 - Loss: -0.456
Iter 16/2000 - Loss: -0.454
Iter 17/2000 - Loss: -0.465
Iter 18/2000 - Loss: -0.479
Iter 19/2000 - Loss: -0.477
Iter 20/2000 - Loss: -0.465
Iter 1981/2000 - Loss: -0.518
Iter 1982/2000 - Loss: -0.518
Iter 1983/2000 - Loss: -0.518
Iter 1984/2000 - Loss: -0.518
Iter 1985/2000 - Loss: -0.518
Iter 1986/2000 - Loss: -0.518
Iter 1987/2000 - Loss: -0.518
Iter 1988/2000 - Loss: -0.518
Iter 1989/2000 - Loss: -0.518
Iter 1990/2000 - Loss: -0.518
Iter 1991/2000 - Loss: -0.518
Iter 1992/2000 - Loss: -0.518
Iter 1993/2000 - Loss: -0.518
Iter 1994/2000 - Loss: -0.518
Iter 1995/2000 - Loss: -0.518
Iter 1996/2000 - Loss: -0.518
Iter 1997/2000 - Loss: -0.518
Iter 1998/2000 - Loss: -0.518
Iter 1999/2000 - Loss: -0.518
Iter 2000/2000 - Loss: -0.518
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0105],
        [0.1096],
        [0.0021]])
Lengthscale: tensor([[[2.1197e-02, 1.2478e-01, 1.1479e-01, 5.0889e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1196e-02, 1.2478e-01, 1.1479e-01, 5.0889e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1197e-02, 1.2478e-01, 1.1480e-01, 5.0889e-03, 1.9561e-04,
          4.3219e-01]],

        [[2.1196e-02, 1.2478e-01, 1.1478e-01, 5.0888e-03, 1.9561e-04,
          4.3219e-01]]])
Signal Variance: tensor([0.0101, 0.0423, 0.4904, 0.0083])
Estimated target variance: tensor([0.0130, 0.0541, 0.6154, 0.0107])
N: 40
Signal to noise ratio: tensor([2.0051, 2.0101, 2.1151, 2.0040])
Bound on condition number: tensor([161.8232, 162.6152, 179.9392, 161.6459])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.030418585872515377, policy loss: 4.192135932165765
Experience 4, Iter 1, disc loss: 0.06265105037475611, policy loss: 3.956016541833083
Experience 4, Iter 2, disc loss: 0.043406129007846364, policy loss: 3.9023928359504154
Experience 4, Iter 3, disc loss: 0.0272554547689616, policy loss: 4.127512578999434
Experience 4, Iter 4, disc loss: 0.0364600587546752, policy loss: 4.128953713843498
Experience 4, Iter 5, disc loss: 0.02595413873721611, policy loss: 4.160450188170536
Experience 4, Iter 6, disc loss: 0.03799896093762986, policy loss: 4.0913276542988894
Experience 4, Iter 7, disc loss: 0.025884464287069343, policy loss: 4.325229189227566
Experience 4, Iter 8, disc loss: 0.03384530886636911, policy loss: 4.247740544080081
Experience 4, Iter 9, disc loss: 0.025133577317936463, policy loss: 4.368185154996238
Experience 4, Iter 10, disc loss: 0.030768425145040534, policy loss: 4.222309196040088
Experience 4, Iter 11, disc loss: 0.028677923267025007, policy loss: 4.155752074400732
Experience 4, Iter 12, disc loss: 0.025941406997963403, policy loss: 4.296727939592985
Experience 4, Iter 13, disc loss: 0.030965980158826505, policy loss: 4.1113831375826555
Experience 4, Iter 14, disc loss: 0.03066254025012102, policy loss: 4.346223878192951
Experience 4, Iter 15, disc loss: 0.019091322669718757, policy loss: 4.516108090480824
Experience 4, Iter 16, disc loss: 0.023216185455673458, policy loss: 4.369507279125329
Experience 4, Iter 17, disc loss: 0.027000955323803217, policy loss: 4.213723649522979
Experience 4, Iter 18, disc loss: 0.028487171910650554, policy loss: 4.272210550829225
Experience 4, Iter 19, disc loss: 0.031126374119095513, policy loss: 4.288404006122269
Experience 4, Iter 20, disc loss: 0.023777980695355243, policy loss: 4.4564180675115015
Experience 4, Iter 21, disc loss: 0.022783876413697928, policy loss: 4.711061911608156
Experience 4, Iter 22, disc loss: 0.019508221610231042, policy loss: 4.584583216103495
Experience 4, Iter 23, disc loss: 0.029311778812152543, policy loss: 4.271075492969974
Experience 4, Iter 24, disc loss: 0.021529868958168306, policy loss: 4.489060854554943
Experience 4, Iter 25, disc loss: 0.02011574914069545, policy loss: 4.772161083081539
Experience 4, Iter 26, disc loss: 0.029376939283111646, policy loss: 4.446372624507628
Experience 4, Iter 27, disc loss: 0.019174173458637096, policy loss: 4.8135585134657
Experience 4, Iter 28, disc loss: 0.022916567118343893, policy loss: 4.648122924695775
Experience 4, Iter 29, disc loss: 0.01964505075138045, policy loss: 4.540247131702127
Experience 4, Iter 30, disc loss: 0.021495571115940734, policy loss: 4.629112964080904
Experience 4, Iter 31, disc loss: 0.018835643572864748, policy loss: 4.6345549500562555
Experience 4, Iter 32, disc loss: 0.019089869035480534, policy loss: 4.613720673355265
Experience 4, Iter 33, disc loss: 0.015511756754114443, policy loss: 4.968787177491347
Experience 4, Iter 34, disc loss: 0.015852909210539622, policy loss: 4.802100158833321
Experience 4, Iter 35, disc loss: 0.02316812840303489, policy loss: 4.705755064319735
Experience 4, Iter 36, disc loss: 0.014371437177120751, policy loss: 5.015698922153108
Experience 4, Iter 37, disc loss: 0.019740905377055362, policy loss: 4.869028758808617
Experience 4, Iter 38, disc loss: 0.020810198668750387, policy loss: 4.8558133704841975
Experience 4, Iter 39, disc loss: 0.019723507070752752, policy loss: 4.723451623583237
Experience 4, Iter 40, disc loss: 0.01774346820124486, policy loss: 4.869141353030266
Experience 4, Iter 41, disc loss: 0.013022467148334757, policy loss: 5.157035814726936
Experience 4, Iter 42, disc loss: 0.026059528230332873, policy loss: 4.953466943844934
Experience 4, Iter 43, disc loss: 0.019680623236494284, policy loss: 4.715724470275091
Experience 4, Iter 44, disc loss: 0.011851453041005818, policy loss: 5.049459940876654
Experience 4, Iter 45, disc loss: 0.011766424507987303, policy loss: 5.292425191672729
Experience 4, Iter 46, disc loss: 0.02058563499045519, policy loss: 4.785422545493629
Experience 4, Iter 47, disc loss: 0.014659431009282898, policy loss: 4.986907408275424
Experience 4, Iter 48, disc loss: 0.020013921489678406, policy loss: 4.911447970755864
Experience 4, Iter 49, disc loss: 0.014212714008986398, policy loss: 5.2269890348166435
Experience 4, Iter 50, disc loss: 0.014058427235723897, policy loss: 5.084229878999223
Experience 4, Iter 51, disc loss: 0.014681845024617625, policy loss: 5.020367404876303
Experience 4, Iter 52, disc loss: 0.016493211094970046, policy loss: 4.907273053567684
Experience 4, Iter 53, disc loss: 0.013626528954561554, policy loss: 5.300232029418739
Experience 4, Iter 54, disc loss: 0.015521407036030116, policy loss: 5.2002138623818315
Experience 4, Iter 55, disc loss: 0.011872888784210677, policy loss: 5.096638782235871
Experience 4, Iter 56, disc loss: 0.011533789852117948, policy loss: 5.280487264092336
Experience 4, Iter 57, disc loss: 0.013223649872476245, policy loss: 5.395825862796001
Experience 4, Iter 58, disc loss: 0.011550461914517387, policy loss: 5.184367474826653
Experience 4, Iter 59, disc loss: 0.011342812665150767, policy loss: 5.117917027379607
Experience 4, Iter 60, disc loss: 0.02132263793354733, policy loss: 4.914676236771825
Experience 4, Iter 61, disc loss: 0.016278323031044085, policy loss: 5.01750077824952
Experience 4, Iter 62, disc loss: 0.014320387478442147, policy loss: 5.330431920419368
Experience 4, Iter 63, disc loss: 0.012229985561986586, policy loss: 5.380957100695761
Experience 4, Iter 64, disc loss: 0.013907330321399198, policy loss: 4.988985803198618
Experience 4, Iter 65, disc loss: 0.013446823249524637, policy loss: 5.155222990729397
Experience 4, Iter 66, disc loss: 0.01825158333034663, policy loss: 5.130725335951686
Experience 4, Iter 67, disc loss: 0.01128522699694578, policy loss: 5.259173374328796
Experience 4, Iter 68, disc loss: 0.011666573423283432, policy loss: 5.382201546790564
Experience 4, Iter 69, disc loss: 0.017634556810425375, policy loss: 5.147326396079557
Experience 4, Iter 70, disc loss: 0.011046553023965944, policy loss: 5.202950254356461
Experience 4, Iter 71, disc loss: 0.01273605106903523, policy loss: 5.189927703552842
Experience 4, Iter 72, disc loss: 0.012298973787164259, policy loss: 5.348638779127666
Experience 4, Iter 73, disc loss: 0.01101635901626162, policy loss: 5.578649452394682
Experience 4, Iter 74, disc loss: 0.00948785420372349, policy loss: 5.608584381955033
Experience 4, Iter 75, disc loss: 0.010144229459118984, policy loss: 5.527934005999253
Experience 4, Iter 76, disc loss: 0.013797143937072973, policy loss: 5.27171381175514
Experience 4, Iter 77, disc loss: 0.00931004053549286, policy loss: 5.438681590550839
Experience 4, Iter 78, disc loss: 0.013652412807456575, policy loss: 5.300283272140641
Experience 4, Iter 79, disc loss: 0.012885559003432733, policy loss: 5.355013183446462
Experience 4, Iter 80, disc loss: 0.014028184807872655, policy loss: 5.231202295772111
Experience 4, Iter 81, disc loss: 0.008996894753557617, policy loss: 5.630662550629244
Experience 4, Iter 82, disc loss: 0.011640456242037198, policy loss: 5.254142021829177
Experience 4, Iter 83, disc loss: 0.01239625164497094, policy loss: 5.423252611777844
Experience 4, Iter 84, disc loss: 0.010619993138097014, policy loss: 5.504097875376035
Experience 4, Iter 85, disc loss: 0.012185559569942175, policy loss: 5.3704372877223445
Experience 4, Iter 86, disc loss: 0.007461775622146895, policy loss: 5.722042379653457
Experience 4, Iter 87, disc loss: 0.009440481112364904, policy loss: 5.84904292030296
Experience 4, Iter 88, disc loss: 0.008215076888404909, policy loss: 5.778826096958887
Experience 4, Iter 89, disc loss: 0.00964004223942645, policy loss: 5.602727319589135
Experience 4, Iter 90, disc loss: 0.00895664591893932, policy loss: 5.703304272793909
Experience 4, Iter 91, disc loss: 0.012372937976803233, policy loss: 5.276913597025198
Experience 4, Iter 92, disc loss: 0.008196322032176058, policy loss: 5.680544218622135
Experience 4, Iter 93, disc loss: 0.009078991008904939, policy loss: 5.597701403003559
Experience 4, Iter 94, disc loss: 0.012743021569314564, policy loss: 5.331365442482404
Experience 4, Iter 95, disc loss: 0.010475311074357679, policy loss: 5.554330034483553
Experience 4, Iter 96, disc loss: 0.00895782649962125, policy loss: 5.569901699779355
Experience 4, Iter 97, disc loss: 0.008364899140668626, policy loss: 5.633215091293977
Experience 4, Iter 98, disc loss: 0.007921939823825523, policy loss: 5.726323676393912
Experience 4, Iter 99, disc loss: 0.009717747324638478, policy loss: 5.514602901030898
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.0114],
        [0.1320],
        [0.0023]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0112, 0.0458, 0.5282, 0.0092], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0112, 0.0458, 0.5282, 0.0092])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.355
Iter 2/2000 - Loss: -0.304
Iter 3/2000 - Loss: -0.595
Iter 4/2000 - Loss: -0.571
Iter 5/2000 - Loss: -0.476
Iter 6/2000 - Loss: -0.516
Iter 7/2000 - Loss: -0.630
Iter 8/2000 - Loss: -0.736
Iter 9/2000 - Loss: -0.742
Iter 10/2000 - Loss: -0.664
Iter 11/2000 - Loss: -0.620
Iter 12/2000 - Loss: -0.679
Iter 13/2000 - Loss: -0.766
Iter 14/2000 - Loss: -0.786
Iter 15/2000 - Loss: -0.744
Iter 16/2000 - Loss: -0.719
Iter 17/2000 - Loss: -0.745
Iter 18/2000 - Loss: -0.784
Iter 19/2000 - Loss: -0.789
Iter 20/2000 - Loss: -0.767
Iter 1981/2000 - Loss: -0.819
Iter 1982/2000 - Loss: -0.819
Iter 1983/2000 - Loss: -0.819
Iter 1984/2000 - Loss: -0.819
Iter 1985/2000 - Loss: -0.819
Iter 1986/2000 - Loss: -0.819
Iter 1987/2000 - Loss: -0.819
Iter 1988/2000 - Loss: -0.819
Iter 1989/2000 - Loss: -0.819
Iter 1990/2000 - Loss: -0.819
Iter 1991/2000 - Loss: -0.819
Iter 1992/2000 - Loss: -0.819
Iter 1993/2000 - Loss: -0.819
Iter 1994/2000 - Loss: -0.819
Iter 1995/2000 - Loss: -0.819
Iter 1996/2000 - Loss: -0.819
Iter 1997/2000 - Loss: -0.819
Iter 1998/2000 - Loss: -0.819
Iter 1999/2000 - Loss: -0.819
Iter 2000/2000 - Loss: -0.819
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0089],
        [0.0958],
        [0.0018]])
Lengthscale: tensor([[[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8178e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8197e-02, 4.4351e-03, 1.7498e-04,
          3.6985e-01]],

        [[1.9581e-02, 1.0564e-01, 9.8176e-02, 4.4349e-03, 1.7497e-04,
          3.6985e-01]]])
Signal Variance: tensor([0.0088, 0.0359, 0.4218, 0.0072])
Estimated target variance: tensor([0.0112, 0.0458, 0.5282, 0.0092])
N: 50
Signal to noise ratio: tensor([2.0055, 2.0082, 2.0985, 2.0027])
Bound on condition number: tensor([202.1034, 202.6483, 221.1868, 201.5412])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.017131917003186437, policy loss: 5.097610692975394
Experience 5, Iter 1, disc loss: 0.015660731218678323, policy loss: 5.081357951426966
Experience 5, Iter 2, disc loss: 0.01649516549244769, policy loss: 4.985478544021833
Experience 5, Iter 3, disc loss: 0.011457997162430771, policy loss: 5.268401209925146
Experience 5, Iter 4, disc loss: 0.013073321711806347, policy loss: 5.101907230843311
Experience 5, Iter 5, disc loss: 0.01086938918183033, policy loss: 5.295319366026827
Experience 5, Iter 6, disc loss: 0.014048365636253762, policy loss: 5.245006400404188
Experience 5, Iter 7, disc loss: 0.016734196871059256, policy loss: 5.0576295662785435
Experience 5, Iter 8, disc loss: 0.012871945011202422, policy loss: 5.453695682160975
Experience 5, Iter 9, disc loss: 0.013918853207484004, policy loss: 5.099430197735139
Experience 5, Iter 10, disc loss: 0.013593329703198877, policy loss: 5.113006034966773
Experience 5, Iter 11, disc loss: 0.013519809029816176, policy loss: 5.102813903298379
Experience 5, Iter 12, disc loss: 0.014488478548562398, policy loss: 5.194683372917739
Experience 5, Iter 13, disc loss: 0.009531901563602136, policy loss: 5.467514108379751
Experience 5, Iter 14, disc loss: 0.009272036066299897, policy loss: 5.333101091747542
Experience 5, Iter 15, disc loss: 0.010743537107808435, policy loss: 5.3684432779365885
Experience 5, Iter 16, disc loss: 0.010692885310901257, policy loss: 5.370089443072697
Experience 5, Iter 17, disc loss: 0.01616517908624502, policy loss: 5.245909379154652
Experience 5, Iter 18, disc loss: 0.010406808560569837, policy loss: 5.507470717910171
Experience 5, Iter 19, disc loss: 0.013456975219268256, policy loss: 5.295374542113226
Experience 5, Iter 20, disc loss: 0.01067852291713818, policy loss: 5.4380507740354584
Experience 5, Iter 21, disc loss: 0.010898674938920366, policy loss: 5.180692198289121
Experience 5, Iter 22, disc loss: 0.009454270862389341, policy loss: 5.3847968190845314
Experience 5, Iter 23, disc loss: 0.011724021524632417, policy loss: 5.216703079362643
Experience 5, Iter 24, disc loss: 0.01083124600000682, policy loss: 5.335397645509049
Experience 5, Iter 25, disc loss: 0.011647637969154174, policy loss: 5.422087573089819
Experience 5, Iter 26, disc loss: 0.007840920166117437, policy loss: 5.6823445165611135
Experience 5, Iter 27, disc loss: 0.01646850265221507, policy loss: 5.107994240270613
Experience 5, Iter 28, disc loss: 0.010471458438409671, policy loss: 5.4703845684661125
Experience 5, Iter 29, disc loss: 0.013013620314572991, policy loss: 5.1807007708212085
Experience 5, Iter 30, disc loss: 0.01470355591681428, policy loss: 5.518479244351113
Experience 5, Iter 31, disc loss: 0.009119065915026515, policy loss: 5.413058791112874
Experience 5, Iter 32, disc loss: 0.009323218138135475, policy loss: 5.510390537041138
Experience 5, Iter 33, disc loss: 0.00636917781152444, policy loss: 5.862108501621031
Experience 5, Iter 34, disc loss: 0.007968797580687675, policy loss: 5.6277018274375825
Experience 5, Iter 35, disc loss: 0.008159757200698425, policy loss: 5.654281573422183
Experience 5, Iter 36, disc loss: 0.009708108287975897, policy loss: 5.655841936682723
Experience 5, Iter 37, disc loss: 0.008327372742495998, policy loss: 5.629665676899527
Experience 5, Iter 38, disc loss: 0.007368479169830065, policy loss: 5.629883686091788
Experience 5, Iter 39, disc loss: 0.0074933875999873304, policy loss: 5.716932547645916
Experience 5, Iter 40, disc loss: 0.009480161565264057, policy loss: 5.669509967358415
Experience 5, Iter 41, disc loss: 0.009558047204861028, policy loss: 5.7064786549179605
Experience 5, Iter 42, disc loss: 0.008593213535457743, policy loss: 5.531433454602341
Experience 5, Iter 43, disc loss: 0.008291228419157215, policy loss: 5.698776538650639
Experience 5, Iter 44, disc loss: 0.010160253029510753, policy loss: 5.708763627467356
Experience 5, Iter 45, disc loss: 0.008333550543465788, policy loss: 5.937363395848895
Experience 5, Iter 46, disc loss: 0.008988141465881085, policy loss: 5.477595658338901
Experience 5, Iter 47, disc loss: 0.008773253251285926, policy loss: 5.9578299550799585
Experience 5, Iter 48, disc loss: 0.009718226321908796, policy loss: 5.807634344328507
Experience 5, Iter 49, disc loss: 0.010246839503003695, policy loss: 5.502032271741068
Experience 5, Iter 50, disc loss: 0.008025576740552974, policy loss: 5.837038121599462
Experience 5, Iter 51, disc loss: 0.005989496138911853, policy loss: 5.821134149081799
Experience 5, Iter 52, disc loss: 0.010355740126064519, policy loss: 5.510184151040889
Experience 5, Iter 53, disc loss: 0.006998774574213638, policy loss: 5.930376319075274
Experience 5, Iter 54, disc loss: 0.008282500244664504, policy loss: 5.878478659331004
Experience 5, Iter 55, disc loss: 0.010120174427565416, policy loss: 5.757952264005799
Experience 5, Iter 56, disc loss: 0.008864844571940426, policy loss: 5.864780471825153
Experience 5, Iter 57, disc loss: 0.006520513936044069, policy loss: 5.797000706127164
Experience 5, Iter 58, disc loss: 0.006631118210103541, policy loss: 5.863681584432052
Experience 5, Iter 59, disc loss: 0.009603330468641022, policy loss: 5.8605322580980435
Experience 5, Iter 60, disc loss: 0.010025646100561636, policy loss: 5.79805606261522
Experience 5, Iter 61, disc loss: 0.006887834924594182, policy loss: 5.925015908525708
Experience 5, Iter 62, disc loss: 0.008923195214649361, policy loss: 5.597808494754217
Experience 5, Iter 63, disc loss: 0.007116154989894294, policy loss: 6.077463316916226
Experience 5, Iter 64, disc loss: 0.00836050480597735, policy loss: 5.916799358263736
Experience 5, Iter 65, disc loss: 0.010088511024812783, policy loss: 5.819776087005115
Experience 5, Iter 66, disc loss: 0.004885569920479607, policy loss: 6.306471505092571
Experience 5, Iter 67, disc loss: 0.008206926150803167, policy loss: 5.735772851053677
Experience 5, Iter 68, disc loss: 0.007282320736677667, policy loss: 5.899622225123516
Experience 5, Iter 69, disc loss: 0.009641696349677526, policy loss: 5.993703876260156
Experience 5, Iter 70, disc loss: 0.010828850552999286, policy loss: 5.965455864887778
Experience 5, Iter 71, disc loss: 0.007309785593318203, policy loss: 5.998910663216235
Experience 5, Iter 72, disc loss: 0.007408607434049515, policy loss: 5.93959485383496
Experience 5, Iter 73, disc loss: 0.005949733360748847, policy loss: 6.1092936167576415
Experience 5, Iter 74, disc loss: 0.007551537017537567, policy loss: 6.111497006426258
Experience 5, Iter 75, disc loss: 0.007835966233751204, policy loss: 5.984481413533322
Experience 5, Iter 76, disc loss: 0.004393873738141851, policy loss: 6.529375812086064
Experience 5, Iter 77, disc loss: 0.010311446774346129, policy loss: 5.67615526377749
Experience 5, Iter 78, disc loss: 0.0060377533931531705, policy loss: 6.0839505894706285
Experience 5, Iter 79, disc loss: 0.007494869554606693, policy loss: 5.88814246173919
Experience 5, Iter 80, disc loss: 0.00834940653544891, policy loss: 5.904442991395262
Experience 5, Iter 81, disc loss: 0.005729234983257619, policy loss: 6.2728894214430175
Experience 5, Iter 82, disc loss: 0.006911372567109575, policy loss: 6.0767706123092005
Experience 5, Iter 83, disc loss: 0.005432292921524091, policy loss: 6.285758464508589
Experience 5, Iter 84, disc loss: 0.0077479378119645, policy loss: 6.123363432118573
Experience 5, Iter 85, disc loss: 0.006821914743205101, policy loss: 5.986077058202096
Experience 5, Iter 86, disc loss: 0.006895195535172663, policy loss: 6.007563051514217
Experience 5, Iter 87, disc loss: 0.004954082031950438, policy loss: 6.289585145729159
Experience 5, Iter 88, disc loss: 0.005689767350334682, policy loss: 6.345409940708963
Experience 5, Iter 89, disc loss: 0.006304021731110314, policy loss: 6.114522379264907
Experience 5, Iter 90, disc loss: 0.004587980287666523, policy loss: 6.332564943312933
Experience 5, Iter 91, disc loss: 0.005244183466162461, policy loss: 6.2916222795157974
Experience 5, Iter 92, disc loss: 0.004245111922862048, policy loss: 6.403858905579797
Experience 5, Iter 93, disc loss: 0.005187185912013589, policy loss: 6.37176400046526
Experience 5, Iter 94, disc loss: 0.006393444488014519, policy loss: 6.364747944976221
Experience 5, Iter 95, disc loss: 0.006442715174562044, policy loss: 6.476026846258465
Experience 5, Iter 96, disc loss: 0.008434753496509134, policy loss: 6.013420030572318
Experience 5, Iter 97, disc loss: 0.00678230031717406, policy loss: 5.999246085920074
Experience 5, Iter 98, disc loss: 0.006079489531224886, policy loss: 5.860126170881837
Experience 5, Iter 99, disc loss: 0.009173014750078302, policy loss: 5.923382660985863
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0147],
        [0.2207],
        [0.0042]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.2485e-02, 1.1099e-01, 1.9307e-01, 7.0658e-03, 2.6827e-04,
          4.5962e-01]],

        [[2.2485e-02, 1.1099e-01, 1.9307e-01, 7.0658e-03, 2.6827e-04,
          4.5962e-01]],

        [[2.2485e-02, 1.1099e-01, 1.9307e-01, 7.0658e-03, 2.6827e-04,
          4.5962e-01]],

        [[2.2485e-02, 1.1099e-01, 1.9307e-01, 7.0658e-03, 2.6827e-04,
          4.5962e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0117, 0.0586, 0.8828, 0.0169], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0117, 0.0586, 0.8828, 0.0169])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.136
Iter 2/2000 - Loss: 0.205
Iter 3/2000 - Loss: 0.001
Iter 4/2000 - Loss: 0.005
Iter 5/2000 - Loss: 0.083
Iter 6/2000 - Loss: 0.013
Iter 7/2000 - Loss: -0.083
Iter 8/2000 - Loss: -0.062
Iter 9/2000 - Loss: 0.010
Iter 10/2000 - Loss: -0.001
Iter 11/2000 - Loss: -0.068
Iter 12/2000 - Loss: -0.097
Iter 13/2000 - Loss: -0.071
Iter 14/2000 - Loss: -0.047
Iter 15/2000 - Loss: -0.059
Iter 16/2000 - Loss: -0.085
Iter 17/2000 - Loss: -0.097
Iter 18/2000 - Loss: -0.096
Iter 19/2000 - Loss: -0.099
Iter 20/2000 - Loss: -0.112
Iter 1981/2000 - Loss: -6.134
Iter 1982/2000 - Loss: -6.134
Iter 1983/2000 - Loss: -6.134
Iter 1984/2000 - Loss: -6.134
Iter 1985/2000 - Loss: -6.134
Iter 1986/2000 - Loss: -6.134
Iter 1987/2000 - Loss: -6.134
Iter 1988/2000 - Loss: -6.134
Iter 1989/2000 - Loss: -6.134
Iter 1990/2000 - Loss: -6.134
Iter 1991/2000 - Loss: -6.134
Iter 1992/2000 - Loss: -6.134
Iter 1993/2000 - Loss: -6.134
Iter 1994/2000 - Loss: -6.134
Iter 1995/2000 - Loss: -6.134
Iter 1996/2000 - Loss: -6.134
Iter 1997/2000 - Loss: -6.134
Iter 1998/2000 - Loss: -6.134
Iter 1999/2000 - Loss: -6.134
Iter 2000/2000 - Loss: -6.134
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0015],
        [0.0033]])
Lengthscale: tensor([[[2.1633e+01, 6.0839e+00, 3.7251e+01, 9.6242e+00, 8.1159e+00,
          3.9266e+01]],

        [[2.7828e+01, 4.3176e+01, 1.7728e+01, 1.1340e+00, 6.3228e-01,
          1.1853e+01]],

        [[2.8361e+01, 3.7585e+01, 1.1779e+01, 8.9885e-01, 4.9330e-01,
          1.1300e+01]],

        [[1.4006e-02, 7.8283e-02, 1.4647e-01, 5.1338e-03, 1.9302e-04,
          3.0411e-01]]])
Signal Variance: tensor([0.0930, 0.5228, 4.9559, 0.0133])
Estimated target variance: tensor([0.0117, 0.0586, 0.8828, 0.0169])
N: 60
Signal to noise ratio: tensor([17.4818, 38.2703, 57.9448,  2.0073])
Bound on condition number: tensor([ 18337.7646,  87878.1501, 201457.1585,    242.7546])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 3.626208919629919, policy loss: 0.7879287757803145
Experience 6, Iter 1, disc loss: 3.0122298868442803, policy loss: 0.6576335569048128
Experience 6, Iter 2, disc loss: 2.460385158330718, policy loss: 0.8163361103756721
Experience 6, Iter 3, disc loss: 1.952231368058471, policy loss: 1.107096824919208
Experience 6, Iter 4, disc loss: 1.2145689015634897, policy loss: 1.3573553729573158
Experience 6, Iter 5, disc loss: 0.9424865070841382, policy loss: 1.8931627640648685
Experience 6, Iter 6, disc loss: 0.546786618545122, policy loss: 2.5527714374933996
Experience 6, Iter 7, disc loss: 0.2890384821572409, policy loss: 3.361730174630895
Experience 6, Iter 8, disc loss: 0.30621191861935637, policy loss: 2.9675037715279866
Experience 6, Iter 9, disc loss: 0.24165268799295203, policy loss: 3.644123658253312
Experience 6, Iter 10, disc loss: 0.2457235771670715, policy loss: 3.787221630683567
Experience 6, Iter 11, disc loss: 0.27221343660490704, policy loss: 4.4507609360555795
Experience 6, Iter 12, disc loss: 0.31931210278953837, policy loss: 4.627783313316311
Experience 6, Iter 13, disc loss: 0.3634614535601903, policy loss: 4.1885115127603445
Experience 6, Iter 14, disc loss: 0.38359705145022116, policy loss: 4.439542074611213
Experience 6, Iter 15, disc loss: 0.37994308171416746, policy loss: 4.639717343755668
Experience 6, Iter 16, disc loss: 0.34004538880707474, policy loss: 4.4917203230091385
Experience 6, Iter 17, disc loss: 0.28852627985067625, policy loss: 4.842913319165429
Experience 6, Iter 18, disc loss: 0.25982963230069234, policy loss: 4.498929707129138
Experience 6, Iter 19, disc loss: 0.2214941154378051, policy loss: 4.113991286115894
Experience 6, Iter 20, disc loss: 0.21428513221299178, policy loss: 3.762110967735828
Experience 6, Iter 21, disc loss: 0.14388669295783563, policy loss: 4.768415504392082
Experience 6, Iter 22, disc loss: 0.15766232902076766, policy loss: 3.852103671020594
Experience 6, Iter 23, disc loss: 0.15044277921411933, policy loss: 3.7641634219362463
Experience 6, Iter 24, disc loss: 0.14020428504132176, policy loss: 3.7858631646213845
Experience 6, Iter 25, disc loss: 0.14381677784410302, policy loss: 3.258539412725207
Experience 6, Iter 26, disc loss: 0.1308233726332097, policy loss: 3.154406545045625
Experience 6, Iter 27, disc loss: 0.16066779518250457, policy loss: 3.001826960484885
Experience 6, Iter 28, disc loss: 0.18791836208873258, policy loss: 3.246305060230675
Experience 6, Iter 29, disc loss: 0.15833165004732522, policy loss: 3.3391367471669167
Experience 6, Iter 30, disc loss: 0.16467218916483647, policy loss: 3.4866588210333633
Experience 6, Iter 31, disc loss: 0.15798261324349805, policy loss: 3.3116365180205465
Experience 6, Iter 32, disc loss: 0.14727997545630334, policy loss: 3.3406319395924466
Experience 6, Iter 33, disc loss: 0.09531632590443294, policy loss: 3.7556778567340983
Experience 6, Iter 34, disc loss: 0.09320795727594265, policy loss: 3.6989997563851897
Experience 6, Iter 35, disc loss: 0.10787983968140064, policy loss: 3.152796027988907
Experience 6, Iter 36, disc loss: 0.07215621629671201, policy loss: 3.761351700689283
Experience 6, Iter 37, disc loss: 0.10639363114702842, policy loss: 3.4985959141965655
Experience 6, Iter 38, disc loss: 0.08910852414195074, policy loss: 3.8114353065199564
Experience 6, Iter 39, disc loss: 0.16903840340277362, policy loss: 3.189925307476292
Experience 6, Iter 40, disc loss: 0.08786405598701198, policy loss: 4.461194622261058
Experience 6, Iter 41, disc loss: 0.1536515215987971, policy loss: 3.3900942950367052
Experience 6, Iter 42, disc loss: 0.14697458579031866, policy loss: 3.493149615825331
Experience 6, Iter 43, disc loss: 0.15271220617087747, policy loss: 3.4726837988877586
Experience 6, Iter 44, disc loss: 0.1206930206239339, policy loss: 3.4351249906747667
Experience 6, Iter 45, disc loss: 0.07829501441070558, policy loss: 3.8315785945467793
Experience 6, Iter 46, disc loss: 0.07848573175841211, policy loss: 3.8348534163738757
Experience 6, Iter 47, disc loss: 0.07863204521325422, policy loss: 4.0067718952618705
Experience 6, Iter 48, disc loss: 0.07236838890923931, policy loss: 3.781737274277808
Experience 6, Iter 49, disc loss: 0.08650889259962966, policy loss: 3.6158972468231614
Experience 6, Iter 50, disc loss: 0.06945817890570774, policy loss: 4.058767785834576
Experience 6, Iter 51, disc loss: 0.0646339972801907, policy loss: 3.921326525003176
Experience 6, Iter 52, disc loss: 0.07263462244134385, policy loss: 3.8530262053170494
Experience 6, Iter 53, disc loss: 0.07865637763173344, policy loss: 3.7503336243642047
Experience 6, Iter 54, disc loss: 0.11572408243568327, policy loss: 3.7997351063987206
Experience 6, Iter 55, disc loss: 0.08410962056767418, policy loss: 4.184830040664599
Experience 6, Iter 56, disc loss: 0.09764250449976197, policy loss: 4.277784778126471
Experience 6, Iter 57, disc loss: 0.07735710267925008, policy loss: 4.418730887056608
Experience 6, Iter 58, disc loss: 0.08169216215722225, policy loss: 4.021149738971701
Experience 6, Iter 59, disc loss: 0.07075403692528878, policy loss: 4.040385530373709
Experience 6, Iter 60, disc loss: 0.06586073375818324, policy loss: 3.9719687254334657
Experience 6, Iter 61, disc loss: 0.0505863033148872, policy loss: 4.373517387570383
Experience 6, Iter 62, disc loss: 0.08075836379340867, policy loss: 4.008660172481604
Experience 6, Iter 63, disc loss: 0.06579109502805051, policy loss: 4.81548657495969
Experience 6, Iter 64, disc loss: 0.057945159221876916, policy loss: 4.23652277686258
Experience 6, Iter 65, disc loss: 0.06308627741695906, policy loss: 4.618036325633964
Experience 6, Iter 66, disc loss: 0.08024844012477325, policy loss: 4.4967907629603685
Experience 6, Iter 67, disc loss: 0.07645261093715597, policy loss: 4.699827427853455
Experience 6, Iter 68, disc loss: 0.07299867315094762, policy loss: 4.385181661772479
Experience 6, Iter 69, disc loss: 0.048650405423592814, policy loss: 4.389625150389632
Experience 6, Iter 70, disc loss: 0.07549615611160257, policy loss: 4.5000752626378535
Experience 6, Iter 71, disc loss: 0.06086586081663334, policy loss: 4.314656987474166
Experience 6, Iter 72, disc loss: 0.06501421840218334, policy loss: 4.554863076287397
Experience 6, Iter 73, disc loss: 0.058288074616884046, policy loss: 4.471840767048677
Experience 6, Iter 74, disc loss: 0.08003256444028325, policy loss: 4.3679372232357085
Experience 6, Iter 75, disc loss: 0.05174815489073886, policy loss: 4.751894798259963
Experience 6, Iter 76, disc loss: 0.06584517143510046, policy loss: 4.33412240205982
Experience 6, Iter 77, disc loss: 0.058921724504378305, policy loss: 4.573624209357679
Experience 6, Iter 78, disc loss: 0.0703581534552157, policy loss: 4.436800432060037
Experience 6, Iter 79, disc loss: 0.06408442109497027, policy loss: 4.311340055343958
Experience 6, Iter 80, disc loss: 0.06722002522872662, policy loss: 4.186803735135373
Experience 6, Iter 81, disc loss: 0.04506058154113772, policy loss: 4.547928450168742
Experience 6, Iter 82, disc loss: 0.05148797775446392, policy loss: 4.5844747070002345
Experience 6, Iter 83, disc loss: 0.064673592366292, policy loss: 4.427645134435465
Experience 6, Iter 84, disc loss: 0.07128604874814526, policy loss: 4.425737803209018
Experience 6, Iter 85, disc loss: 0.049365126834725104, policy loss: 4.736578953227287
Experience 6, Iter 86, disc loss: 0.05462617088000647, policy loss: 4.384100081827471
Experience 6, Iter 87, disc loss: 0.07853282274461956, policy loss: 4.426393789213993
Experience 6, Iter 88, disc loss: 0.04159502458889314, policy loss: 4.7166106166016295
Experience 6, Iter 89, disc loss: 0.04841172098233819, policy loss: 4.920098957728397
Experience 6, Iter 90, disc loss: 0.05021816720352728, policy loss: 4.7857068345258815
Experience 6, Iter 91, disc loss: 0.05711094651167359, policy loss: 4.594126937175606
Experience 6, Iter 92, disc loss: 0.04670685292895072, policy loss: 4.59071938384594
Experience 6, Iter 93, disc loss: 0.0572656138299998, policy loss: 4.865964499679915
Experience 6, Iter 94, disc loss: 0.05323070012077649, policy loss: 4.849314235986461
Experience 6, Iter 95, disc loss: 0.06058218186382219, policy loss: 4.656434643432986
Experience 6, Iter 96, disc loss: 0.05307086401830727, policy loss: 4.792012304048897
Experience 6, Iter 97, disc loss: 0.06457593333785028, policy loss: 4.711173407346053
Experience 6, Iter 98, disc loss: 0.043634456289499066, policy loss: 4.647010730013906
Experience 6, Iter 99, disc loss: 0.04422103874386458, policy loss: 4.852017849332105
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0142],
        [0.2037],
        [0.0037]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.9280e-02, 9.6094e-02, 1.6816e-01, 6.2436e-03, 2.4595e-04,
          4.2897e-01]],

        [[1.9280e-02, 9.6094e-02, 1.6816e-01, 6.2436e-03, 2.4595e-04,
          4.2897e-01]],

        [[1.9280e-02, 9.6094e-02, 1.6816e-01, 6.2436e-03, 2.4595e-04,
          4.2897e-01]],

        [[1.9280e-02, 9.6094e-02, 1.6816e-01, 6.2436e-03, 2.4595e-04,
          4.2897e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0102, 0.0568, 0.8149, 0.0147], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0102, 0.0568, 0.8149, 0.0147])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.077
Iter 2/2000 - Loss: 0.068
Iter 3/2000 - Loss: -0.185
Iter 4/2000 - Loss: -0.184
Iter 5/2000 - Loss: -0.092
Iter 6/2000 - Loss: -0.169
Iter 7/2000 - Loss: -0.266
Iter 8/2000 - Loss: -0.244
Iter 9/2000 - Loss: -0.174
Iter 10/2000 - Loss: -0.193
Iter 11/2000 - Loss: -0.267
Iter 12/2000 - Loss: -0.291
Iter 13/2000 - Loss: -0.255
Iter 14/2000 - Loss: -0.234
Iter 15/2000 - Loss: -0.259
Iter 16/2000 - Loss: -0.293
Iter 17/2000 - Loss: -0.304
Iter 18/2000 - Loss: -0.301
Iter 19/2000 - Loss: -0.307
Iter 20/2000 - Loss: -0.329
Iter 1981/2000 - Loss: -8.110
Iter 1982/2000 - Loss: -8.110
Iter 1983/2000 - Loss: -8.110
Iter 1984/2000 - Loss: -8.111
Iter 1985/2000 - Loss: -8.111
Iter 1986/2000 - Loss: -8.111
Iter 1987/2000 - Loss: -8.111
Iter 1988/2000 - Loss: -8.111
Iter 1989/2000 - Loss: -8.111
Iter 1990/2000 - Loss: -8.111
Iter 1991/2000 - Loss: -8.111
Iter 1992/2000 - Loss: -8.111
Iter 1993/2000 - Loss: -8.111
Iter 1994/2000 - Loss: -8.111
Iter 1995/2000 - Loss: -8.111
Iter 1996/2000 - Loss: -8.111
Iter 1997/2000 - Loss: -8.111
Iter 1998/2000 - Loss: -8.111
Iter 1999/2000 - Loss: -8.111
Iter 2000/2000 - Loss: -8.111
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[19.1823,  5.9505, 36.6700, 11.1117,  8.0039, 41.7365]],

        [[24.9075, 46.1563, 17.0140,  1.2782,  0.5338, 13.2189]],

        [[25.1234, 37.3570, 11.3248,  0.8583,  0.5301, 11.8829]],

        [[22.1009, 42.6571, 10.5870,  3.3311,  8.8238, 33.0042]]])
Signal Variance: tensor([0.0884, 0.6052, 5.1906, 0.2635])
Estimated target variance: tensor([0.0102, 0.0568, 0.8149, 0.0147])
N: 70
Signal to noise ratio: tensor([16.9125, 43.1116, 57.3768, 32.1479])
Bound on condition number: tensor([ 20023.3439, 130103.5814, 230447.5928,  72345.3099])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.09413927117544676, policy loss: 3.1829382185246593
Experience 7, Iter 1, disc loss: 0.09034415570779099, policy loss: 3.3742468728918404
Experience 7, Iter 2, disc loss: 0.10851372613005485, policy loss: 3.0601725482227033
Experience 7, Iter 3, disc loss: 0.11374216209444916, policy loss: 2.986358617904488
Experience 7, Iter 4, disc loss: 0.1041344515222167, policy loss: 3.3132083036051427
Experience 7, Iter 5, disc loss: 0.11348882801907934, policy loss: 3.0406138382006676
Experience 7, Iter 6, disc loss: 0.12725396236853215, policy loss: 3.027716880075644
Experience 7, Iter 7, disc loss: 0.15640994929573454, policy loss: 2.664339746997434
Experience 7, Iter 8, disc loss: 0.15035653096018775, policy loss: 2.6769875112037447
Experience 7, Iter 9, disc loss: 0.1422189448103438, policy loss: 2.927051517202938
Experience 7, Iter 10, disc loss: 0.14278724272805404, policy loss: 2.893930854415113
Experience 7, Iter 11, disc loss: 0.14539346565496467, policy loss: 2.8499949266824816
Experience 7, Iter 12, disc loss: 0.14534188568908973, policy loss: 2.758764330111368
Experience 7, Iter 13, disc loss: 0.13737134696446696, policy loss: 2.7837980919860006
Experience 7, Iter 14, disc loss: 0.10901740212941018, policy loss: 2.963900310422372
Experience 7, Iter 15, disc loss: 0.11123542760484117, policy loss: 2.9251731402411645
Experience 7, Iter 16, disc loss: 0.12529925893783753, policy loss: 2.9352881423908928
Experience 7, Iter 17, disc loss: 0.1252704621136428, policy loss: 3.1047992146505208
Experience 7, Iter 18, disc loss: 0.12193028060432731, policy loss: 3.220433105930758
Experience 7, Iter 19, disc loss: 0.12811078443608784, policy loss: 3.101427608387115
Experience 7, Iter 20, disc loss: 0.13968987934887017, policy loss: 3.0802996178219018
Experience 7, Iter 21, disc loss: 0.12233071453057724, policy loss: 3.310382534251845
Experience 7, Iter 22, disc loss: 0.14527364819169503, policy loss: 3.086051540407628
Experience 7, Iter 23, disc loss: 0.10433983025978927, policy loss: 3.838803160766619
Experience 7, Iter 24, disc loss: 0.13315747336622763, policy loss: 3.278146274312702
Experience 7, Iter 25, disc loss: 0.13332908383488148, policy loss: 3.3810163807663693
Experience 7, Iter 26, disc loss: 0.13649302848745337, policy loss: 3.459300153270736
Experience 7, Iter 27, disc loss: 0.12564846987829204, policy loss: 3.2905327628735863
Experience 7, Iter 28, disc loss: 0.12352605560102725, policy loss: 3.40375239566593
Experience 7, Iter 29, disc loss: 0.14437022822272952, policy loss: 3.1512273663183543
Experience 7, Iter 30, disc loss: 0.14235788754415812, policy loss: 3.1006093564043913
Experience 7, Iter 31, disc loss: 0.15375601702804959, policy loss: 2.887941078728848
Experience 7, Iter 32, disc loss: 0.16257466573547003, policy loss: 2.6982145026171374
Experience 7, Iter 33, disc loss: 0.1488868076938314, policy loss: 2.85069926962876
Experience 7, Iter 34, disc loss: 0.14935611643671387, policy loss: 2.8392330205719767
Experience 7, Iter 35, disc loss: 0.1441087798971181, policy loss: 3.058860625598963
Experience 7, Iter 36, disc loss: 0.14840357195451254, policy loss: 2.775338672781279
Experience 7, Iter 37, disc loss: 0.14112372593295341, policy loss: 2.8517838829230104
Experience 7, Iter 38, disc loss: 0.14595498097560192, policy loss: 2.9510901751657297
Experience 7, Iter 39, disc loss: 0.1520686798348533, policy loss: 2.694294243900983
Experience 7, Iter 40, disc loss: 0.14738812657945488, policy loss: 2.717375541638206
Experience 7, Iter 41, disc loss: 0.1407659737229381, policy loss: 3.035243351196663
Experience 7, Iter 42, disc loss: 0.1592201576775604, policy loss: 2.7017930929172738
Experience 7, Iter 43, disc loss: 0.14221979256451805, policy loss: 2.795014890001136
Experience 7, Iter 44, disc loss: 0.13475927060706153, policy loss: 2.885886180075877
Experience 7, Iter 45, disc loss: 0.1237002542734901, policy loss: 2.982572452241784
Experience 7, Iter 46, disc loss: 0.12555974447354185, policy loss: 3.1160822442594407
Experience 7, Iter 47, disc loss: 0.11807451103088525, policy loss: 3.404017789250243
Experience 7, Iter 48, disc loss: 0.12609758707370025, policy loss: 3.181562749788136
Experience 7, Iter 49, disc loss: 0.13070659679683727, policy loss: 2.883356473571469
Experience 7, Iter 50, disc loss: 0.11060370845641729, policy loss: 3.262107341575366
Experience 7, Iter 51, disc loss: 0.12464827573304993, policy loss: 2.988020097020993
Experience 7, Iter 52, disc loss: 0.11855171379009677, policy loss: 3.205030661664822
Experience 7, Iter 53, disc loss: 0.12247050795084517, policy loss: 3.226360595613148
Experience 7, Iter 54, disc loss: 0.13013578704183495, policy loss: 2.9794644934488748
Experience 7, Iter 55, disc loss: 0.12839015890216732, policy loss: 2.9277103912929032
Experience 7, Iter 56, disc loss: 0.13138768299350273, policy loss: 2.791451534453258
Experience 7, Iter 57, disc loss: 0.12134742073094634, policy loss: 3.125262251610813
Experience 7, Iter 58, disc loss: 0.11425240923045232, policy loss: 3.1411718472917043
Experience 7, Iter 59, disc loss: 0.11526256281933177, policy loss: 3.5609850163976877
Experience 7, Iter 60, disc loss: 0.11946532898199348, policy loss: 3.080560308650222
Experience 7, Iter 61, disc loss: 0.10337239922376369, policy loss: 3.4104786276905767
Experience 7, Iter 62, disc loss: 0.10015899961117403, policy loss: 3.3684839566604294
Experience 7, Iter 63, disc loss: 0.10670801267781202, policy loss: 3.3437240777407604
Experience 7, Iter 64, disc loss: 0.12178151729231547, policy loss: 3.0452032494503425
Experience 7, Iter 65, disc loss: 0.10246620903659923, policy loss: 3.4766696975030413
Experience 7, Iter 66, disc loss: 0.11379127674042244, policy loss: 3.115930121033717
Experience 7, Iter 67, disc loss: 0.11386147802757784, policy loss: 3.176700950705073
Experience 7, Iter 68, disc loss: 0.10320341061178864, policy loss: 3.6014471269265425
Experience 7, Iter 69, disc loss: 0.09069480647938827, policy loss: 3.856361612312895
Experience 7, Iter 70, disc loss: 0.10762042048977812, policy loss: 3.640036983565236
Experience 7, Iter 71, disc loss: 0.08430401491803385, policy loss: 3.7757621035560645
Experience 7, Iter 72, disc loss: 0.09642007340696657, policy loss: 3.6393695248526123
Experience 7, Iter 73, disc loss: 0.09361006423303395, policy loss: 4.06550194027073
Experience 7, Iter 74, disc loss: 0.08870855125539243, policy loss: 3.7511164281521783
Experience 7, Iter 75, disc loss: 0.08833138279524758, policy loss: 3.5831049534529775
Experience 7, Iter 76, disc loss: 0.08962887231743905, policy loss: 3.644540044742046
Experience 7, Iter 77, disc loss: 0.0950596564543503, policy loss: 3.2905842030650554
Experience 7, Iter 78, disc loss: 0.07532951282083433, policy loss: 3.786462969429392
Experience 7, Iter 79, disc loss: 0.08411742352408967, policy loss: 3.579422305519161
Experience 7, Iter 80, disc loss: 0.0871793061093209, policy loss: 3.485263080318002
Experience 7, Iter 81, disc loss: 0.0788885849316488, policy loss: 3.6317929122853987
Experience 7, Iter 82, disc loss: 0.08234172163005739, policy loss: 3.5158075557529522
Experience 7, Iter 83, disc loss: 0.07552001448326856, policy loss: 3.840705299561199
Experience 7, Iter 84, disc loss: 0.07352028113517498, policy loss: 3.7668852729050526
Experience 7, Iter 85, disc loss: 0.07717314588797403, policy loss: 3.4617306738164566
Experience 7, Iter 86, disc loss: 0.07482722964642743, policy loss: 3.725629322398429
Experience 7, Iter 87, disc loss: 0.0823377175273231, policy loss: 3.4637431230350493
Experience 7, Iter 88, disc loss: 0.08378324465566858, policy loss: 3.552493082558142
Experience 7, Iter 89, disc loss: 0.06227404533550806, policy loss: 4.283577721955606
Experience 7, Iter 90, disc loss: 0.07754179470368666, policy loss: 3.6299691476864853
Experience 7, Iter 91, disc loss: 0.0726969722783126, policy loss: 3.6475473038867015
Experience 7, Iter 92, disc loss: 0.07594740535369826, policy loss: 3.5627225661795556
Experience 7, Iter 93, disc loss: 0.07245898501970215, policy loss: 3.7552499454509847
Experience 7, Iter 94, disc loss: 0.0779505123684545, policy loss: 3.590038636523525
Experience 7, Iter 95, disc loss: 0.07864820300533118, policy loss: 3.64070197784377
Experience 7, Iter 96, disc loss: 0.07640750276590436, policy loss: 3.922201287777478
Experience 7, Iter 97, disc loss: 0.07368402962617183, policy loss: 4.123548066493626
Experience 7, Iter 98, disc loss: 0.07007369085052043, policy loss: 3.8907640094781453
Experience 7, Iter 99, disc loss: 0.058765366395871405, policy loss: 4.224513802021318
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0314],
        [0.3562],
        [0.0064]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0179, 0.1186, 0.2938, 0.0105, 0.0027, 1.1005]],

        [[0.0179, 0.1186, 0.2938, 0.0105, 0.0027, 1.1005]],

        [[0.0179, 0.1186, 0.2938, 0.0105, 0.0027, 1.1005]],

        [[0.0179, 0.1186, 0.2938, 0.0105, 0.0027, 1.1005]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0129, 0.1258, 1.4248, 0.0255], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0129, 0.1258, 1.4248, 0.0255])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.861
Iter 2/2000 - Loss: 1.160
Iter 3/2000 - Loss: 0.827
Iter 4/2000 - Loss: 0.855
Iter 5/2000 - Loss: 0.979
Iter 6/2000 - Loss: 0.909
Iter 7/2000 - Loss: 0.797
Iter 8/2000 - Loss: 0.780
Iter 9/2000 - Loss: 0.827
Iter 10/2000 - Loss: 0.834
Iter 11/2000 - Loss: 0.780
Iter 12/2000 - Loss: 0.721
Iter 13/2000 - Loss: 0.691
Iter 14/2000 - Loss: 0.671
Iter 15/2000 - Loss: 0.631
Iter 16/2000 - Loss: 0.564
Iter 17/2000 - Loss: 0.481
Iter 18/2000 - Loss: 0.388
Iter 19/2000 - Loss: 0.278
Iter 20/2000 - Loss: 0.145
Iter 1981/2000 - Loss: -7.904
Iter 1982/2000 - Loss: -7.904
Iter 1983/2000 - Loss: -7.904
Iter 1984/2000 - Loss: -7.904
Iter 1985/2000 - Loss: -7.904
Iter 1986/2000 - Loss: -7.904
Iter 1987/2000 - Loss: -7.904
Iter 1988/2000 - Loss: -7.904
Iter 1989/2000 - Loss: -7.904
Iter 1990/2000 - Loss: -7.904
Iter 1991/2000 - Loss: -7.904
Iter 1992/2000 - Loss: -7.904
Iter 1993/2000 - Loss: -7.904
Iter 1994/2000 - Loss: -7.904
Iter 1995/2000 - Loss: -7.905
Iter 1996/2000 - Loss: -7.905
Iter 1997/2000 - Loss: -7.905
Iter 1998/2000 - Loss: -7.905
Iter 1999/2000 - Loss: -7.905
Iter 2000/2000 - Loss: -7.905
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[16.8850,  7.1401, 30.6122, 11.0753, 11.1630, 46.5411]],

        [[23.6674, 43.9604, 11.6707,  1.1798, 11.0549, 28.4270]],

        [[19.2346, 37.1899, 12.5744,  1.0194,  2.1436, 17.2644]],

        [[19.8751, 36.5417, 15.7478,  4.4281,  6.5682, 46.0115]]])
Signal Variance: tensor([ 0.1355,  2.2610, 13.8775,  0.4548])
Estimated target variance: tensor([0.0129, 0.1258, 1.4248, 0.0255])
N: 80
Signal to noise ratio: tensor([22.0995, 87.6404, 89.6194, 43.4637])
Bound on condition number: tensor([ 39072.2000, 614467.8664, 642532.3267, 151128.1261])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.14097468368133906, policy loss: 2.963984822501812
Experience 8, Iter 1, disc loss: 0.14922688724739697, policy loss: 2.731274359264786
Experience 8, Iter 2, disc loss: 0.13875359052438518, policy loss: 3.051963440862918
Experience 8, Iter 3, disc loss: 0.12745169235989334, policy loss: 3.108690912888298
Experience 8, Iter 4, disc loss: 0.14319404371638816, policy loss: 2.989104830940583
Experience 8, Iter 5, disc loss: 0.11424574223940878, policy loss: 3.620746053328919
Experience 8, Iter 6, disc loss: 0.14021745050483514, policy loss: 2.9225199708711025
Experience 8, Iter 7, disc loss: 0.11053303309885919, policy loss: 3.611262600268566
Experience 8, Iter 8, disc loss: 0.13399815089802017, policy loss: 3.4716025882394144
Experience 8, Iter 9, disc loss: 0.11013412098653226, policy loss: 3.6734001672564642
Experience 8, Iter 10, disc loss: 0.11697151987045029, policy loss: 3.6236511095103303
Experience 8, Iter 11, disc loss: 0.12397104693592698, policy loss: 3.374206051678406
Experience 8, Iter 12, disc loss: 0.12968208787782698, policy loss: 3.3257946245994985
Experience 8, Iter 13, disc loss: 0.11076903371357251, policy loss: 3.591971722120637
Experience 8, Iter 14, disc loss: 0.11569425589853527, policy loss: 3.5870582538348854
Experience 8, Iter 15, disc loss: 0.10823867906803109, policy loss: 3.599685509287206
Experience 8, Iter 16, disc loss: 0.11919771644612528, policy loss: 3.7477045326452885
Experience 8, Iter 17, disc loss: 0.1130255609528556, policy loss: 3.5632512095249385
Experience 8, Iter 18, disc loss: 0.10696917750302484, policy loss: 3.448316641988293
Experience 8, Iter 19, disc loss: 0.09600664546433083, policy loss: 3.706713072325294
Experience 8, Iter 20, disc loss: 0.10050709246386402, policy loss: 3.6227112202045637
Experience 8, Iter 21, disc loss: 0.09695295102576107, policy loss: 3.395089169155432
Experience 8, Iter 22, disc loss: 0.0888985043818521, policy loss: 3.6385277642087486
Experience 8, Iter 23, disc loss: 0.08700444841867794, policy loss: 3.922808940913487
Experience 8, Iter 24, disc loss: 0.08102342007143284, policy loss: 3.8220164191678663
Experience 8, Iter 25, disc loss: 0.08386994364702353, policy loss: 3.5679263577244944
Experience 8, Iter 26, disc loss: 0.0981419023584924, policy loss: 3.2209205565568255
Experience 8, Iter 27, disc loss: 0.09454990804539118, policy loss: 3.4569873155386954
Experience 8, Iter 28, disc loss: 0.09191753512262656, policy loss: 3.3394447663461198
Experience 8, Iter 29, disc loss: 0.08542609982540382, policy loss: 3.527876550306827
Experience 8, Iter 30, disc loss: 0.09231273575073151, policy loss: 3.7540371174937253
Experience 8, Iter 31, disc loss: 0.09119189525137117, policy loss: 3.585259944866335
Experience 8, Iter 32, disc loss: 0.08466911214746409, policy loss: 3.842109740926495
Experience 8, Iter 33, disc loss: 0.07632433850623098, policy loss: 3.7453639970501262
Experience 8, Iter 34, disc loss: 0.08854139536384252, policy loss: 3.323866770494626
Experience 8, Iter 35, disc loss: 0.10643590244043616, policy loss: 3.3412454047104583
Experience 8, Iter 36, disc loss: 0.08270018438252066, policy loss: 3.9177264242870935
Experience 8, Iter 37, disc loss: 0.08127236525548061, policy loss: 3.7925305559208367
Experience 8, Iter 38, disc loss: 0.08350855734258898, policy loss: 3.7449628765348715
Experience 8, Iter 39, disc loss: 0.08915344583879002, policy loss: 3.7136002806543518
Experience 8, Iter 40, disc loss: 0.08008653937713231, policy loss: 3.8563142253867158
Experience 8, Iter 41, disc loss: 0.08928890851904445, policy loss: 3.6179373843576585
Experience 8, Iter 42, disc loss: 0.07978130648345338, policy loss: 4.052386653885394
Experience 8, Iter 43, disc loss: 0.08385275613339598, policy loss: 3.6352339245302514
Experience 8, Iter 44, disc loss: 0.07574578405801882, policy loss: 3.9307302278154204
Experience 8, Iter 45, disc loss: 0.09074664006199752, policy loss: 3.819903439836142
Experience 8, Iter 46, disc loss: 0.0896451808108112, policy loss: 3.5504363951989357
Experience 8, Iter 47, disc loss: 0.07682314547872475, policy loss: 4.141657176619746
Experience 8, Iter 48, disc loss: 0.08570072576412818, policy loss: 3.9701721504997964
Experience 8, Iter 49, disc loss: 0.07623910731354885, policy loss: 4.344276262641815
Experience 8, Iter 50, disc loss: 0.07360180011305659, policy loss: 4.197499018020234
Experience 8, Iter 51, disc loss: 0.07659915157040798, policy loss: 3.997122112782939
Experience 8, Iter 52, disc loss: 0.07289781150763866, policy loss: 4.206807171253677
Experience 8, Iter 53, disc loss: 0.06852280778641093, policy loss: 4.219316523931845
Experience 8, Iter 54, disc loss: 0.0766494742665457, policy loss: 4.29154613457399
Experience 8, Iter 55, disc loss: 0.07434034853393084, policy loss: 3.9883532342258126
Experience 8, Iter 56, disc loss: 0.06946875147861238, policy loss: 4.102837808935781
Experience 8, Iter 57, disc loss: 0.06371070731724356, policy loss: 4.239499438490964
Experience 8, Iter 58, disc loss: 0.07159507415512055, policy loss: 3.984276250385818
Experience 8, Iter 59, disc loss: 0.06893306168662695, policy loss: 4.141153237657544
Experience 8, Iter 60, disc loss: 0.06870951944474257, policy loss: 3.971006285822064
Experience 8, Iter 61, disc loss: 0.07655145528892271, policy loss: 3.98365160635188
Experience 8, Iter 62, disc loss: 0.06109797250569859, policy loss: 4.356409427025265
Experience 8, Iter 63, disc loss: 0.06251608039172514, policy loss: 4.388590741507898
Experience 8, Iter 64, disc loss: 0.06535986873449072, policy loss: 4.4353039819964515
Experience 8, Iter 65, disc loss: 0.07542377535708264, policy loss: 3.919816784423122
Experience 8, Iter 66, disc loss: 0.062207342488847156, policy loss: 4.141225180915525
Experience 8, Iter 67, disc loss: 0.05840225399317178, policy loss: 4.232577746844282
Experience 8, Iter 68, disc loss: 0.06479450193227199, policy loss: 4.136082974824889
Experience 8, Iter 69, disc loss: 0.06261899355617707, policy loss: 4.023391760651451
Experience 8, Iter 70, disc loss: 0.06211861075093937, policy loss: 4.065895529067403
Experience 8, Iter 71, disc loss: 0.06716191003917901, policy loss: 4.153005757456677
Experience 8, Iter 72, disc loss: 0.0663066788678159, policy loss: 4.160642092995526
Experience 8, Iter 73, disc loss: 0.062289051716077945, policy loss: 4.189482969921269
Experience 8, Iter 74, disc loss: 0.06668317748840832, policy loss: 4.134271646259975
Experience 8, Iter 75, disc loss: 0.057251442648245075, policy loss: 4.447235997232011
Experience 8, Iter 76, disc loss: 0.0729175537436993, policy loss: 4.0138794764797625
Experience 8, Iter 77, disc loss: 0.05914131673795617, policy loss: 4.263931474556369
Experience 8, Iter 78, disc loss: 0.05041653257193002, policy loss: 4.938105363962208
Experience 8, Iter 79, disc loss: 0.06025270285196968, policy loss: 4.574926732350178
Experience 8, Iter 80, disc loss: 0.05378226276431479, policy loss: 4.632160913989441
Experience 8, Iter 81, disc loss: 0.056420830962352286, policy loss: 4.638237115195195
Experience 8, Iter 82, disc loss: 0.05744355208914752, policy loss: 4.689101557366685
Experience 8, Iter 83, disc loss: 0.057125231891085954, policy loss: 4.432307224130968
Experience 8, Iter 84, disc loss: 0.05989204959337889, policy loss: 4.530106968396032
Experience 8, Iter 85, disc loss: 0.04853377484182464, policy loss: 4.55700377303726
Experience 8, Iter 86, disc loss: 0.056639765168878524, policy loss: 4.438015612960313
Experience 8, Iter 87, disc loss: 0.05468737460550589, policy loss: 4.380521030266773
Experience 8, Iter 88, disc loss: 0.051787914872403716, policy loss: 4.673943953585605
Experience 8, Iter 89, disc loss: 0.06999805897777878, policy loss: 4.09797123801146
Experience 8, Iter 90, disc loss: 0.06142349550544192, policy loss: 3.9245897995548704
Experience 8, Iter 91, disc loss: 0.052516645702053416, policy loss: 4.3075014936655815
Experience 8, Iter 92, disc loss: 0.06170081512750321, policy loss: 3.9604393421680477
Experience 8, Iter 93, disc loss: 0.056740041249767115, policy loss: 4.551801365665582
Experience 8, Iter 94, disc loss: 0.058331373816901, policy loss: 4.182928042739065
Experience 8, Iter 95, disc loss: 0.05373257291180037, policy loss: 4.329248396800098
Experience 8, Iter 96, disc loss: 0.05334360969546721, policy loss: 4.8985357565021515
Experience 8, Iter 97, disc loss: 0.04914768981513136, policy loss: 4.657620933760997
Experience 8, Iter 98, disc loss: 0.0567846445220882, policy loss: 4.5658222549705
Experience 8, Iter 99, disc loss: 0.04824846419174462, policy loss: 4.535834218978275
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0045],
        [0.0597],
        [0.7039],
        [0.0124]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0172, 0.1636, 0.5660, 0.0136, 0.0083, 1.8966]],

        [[0.0172, 0.1636, 0.5660, 0.0136, 0.0083, 1.8966]],

        [[0.0172, 0.1636, 0.5660, 0.0136, 0.0083, 1.8966]],

        [[0.0172, 0.1636, 0.5660, 0.0136, 0.0083, 1.8966]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0179, 0.2387, 2.8156, 0.0495], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0179, 0.2387, 2.8156, 0.0495])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.990
Iter 2/2000 - Loss: 2.199
Iter 3/2000 - Loss: 1.950
Iter 4/2000 - Loss: 1.977
Iter 5/2000 - Loss: 2.049
Iter 6/2000 - Loss: 1.981
Iter 7/2000 - Loss: 1.893
Iter 8/2000 - Loss: 1.867
Iter 9/2000 - Loss: 1.876
Iter 10/2000 - Loss: 1.859
Iter 11/2000 - Loss: 1.798
Iter 12/2000 - Loss: 1.716
Iter 13/2000 - Loss: 1.637
Iter 14/2000 - Loss: 1.560
Iter 15/2000 - Loss: 1.471
Iter 16/2000 - Loss: 1.355
Iter 17/2000 - Loss: 1.209
Iter 18/2000 - Loss: 1.040
Iter 19/2000 - Loss: 0.857
Iter 20/2000 - Loss: 0.660
Iter 1981/2000 - Loss: -7.534
Iter 1982/2000 - Loss: -7.534
Iter 1983/2000 - Loss: -7.534
Iter 1984/2000 - Loss: -7.534
Iter 1985/2000 - Loss: -7.534
Iter 1986/2000 - Loss: -7.535
Iter 1987/2000 - Loss: -7.535
Iter 1988/2000 - Loss: -7.535
Iter 1989/2000 - Loss: -7.535
Iter 1990/2000 - Loss: -7.535
Iter 1991/2000 - Loss: -7.535
Iter 1992/2000 - Loss: -7.535
Iter 1993/2000 - Loss: -7.535
Iter 1994/2000 - Loss: -7.535
Iter 1995/2000 - Loss: -7.535
Iter 1996/2000 - Loss: -7.535
Iter 1997/2000 - Loss: -7.535
Iter 1998/2000 - Loss: -7.535
Iter 1999/2000 - Loss: -7.535
Iter 2000/2000 - Loss: -7.535
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0003]])
Lengthscale: tensor([[[15.1935,  7.2311, 28.6166,  4.5071,  5.0458, 54.5865]],

        [[20.9366, 42.5522, 12.1144,  1.1982,  2.2570, 27.8352]],

        [[17.9173, 34.6831, 12.8328,  0.9092,  1.6491, 18.0138]],

        [[19.6076, 38.8278, 15.4255,  3.1137,  2.6436, 44.6177]]])
Signal Variance: tensor([ 0.1085,  2.1800, 11.4213,  0.5165])
Estimated target variance: tensor([0.0179, 0.2387, 2.8156, 0.0495])
N: 90
Signal to noise ratio: tensor([19.4757, 84.9505, 83.6275, 45.2707])
Bound on condition number: tensor([ 34138.1366, 649494.3622, 629420.5447, 184450.2376])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.0487098176507407, policy loss: 4.802987283555374
Experience 9, Iter 1, disc loss: 0.05866489557931423, policy loss: 4.391117939920752
Experience 9, Iter 2, disc loss: 0.061427815183825334, policy loss: 4.169837244660464
Experience 9, Iter 3, disc loss: 0.07510193796221254, policy loss: 3.867673257004877
Experience 9, Iter 4, disc loss: 0.0582347193801181, policy loss: 4.462837721875699
Experience 9, Iter 5, disc loss: 0.06257391817114974, policy loss: 3.9189379966855338
Experience 9, Iter 6, disc loss: 0.054725856846901594, policy loss: 4.689420965785231
Experience 9, Iter 7, disc loss: 0.054025818562940905, policy loss: 4.646107275039418
Experience 9, Iter 8, disc loss: 0.05704645491933136, policy loss: 4.318796975119813
Experience 9, Iter 9, disc loss: 0.05634067130508329, policy loss: 4.430064693035541
Experience 9, Iter 10, disc loss: 0.058499709191038696, policy loss: 4.383507865915487
Experience 9, Iter 11, disc loss: 0.05793115825698343, policy loss: 4.37505065298966
Experience 9, Iter 12, disc loss: 0.05919236509476667, policy loss: 4.600414017935596
Experience 9, Iter 13, disc loss: 0.06056492162192145, policy loss: 4.181641129653581
Experience 9, Iter 14, disc loss: 0.05555243330050707, policy loss: 4.351397294332953
Experience 9, Iter 15, disc loss: 0.05657073728816889, policy loss: 4.418625236294089
Experience 9, Iter 16, disc loss: 0.053276302371747054, policy loss: 4.666253629823666
Experience 9, Iter 17, disc loss: 0.059306349560190846, policy loss: 4.329139958513342
Experience 9, Iter 18, disc loss: 0.054741816243197516, policy loss: 4.623803603650469
Experience 9, Iter 19, disc loss: 0.05830106499578752, policy loss: 4.257114689462804
Experience 9, Iter 20, disc loss: 0.050805739691874095, policy loss: 4.624477487476222
Experience 9, Iter 21, disc loss: 0.05634822502173134, policy loss: 4.4067223177753085
Experience 9, Iter 22, disc loss: 0.05068070987926635, policy loss: 4.937580361355209
Experience 9, Iter 23, disc loss: 0.049150486415776615, policy loss: 4.445553617519482
Experience 9, Iter 24, disc loss: 0.051021566549859745, policy loss: 4.242435904009756
Experience 9, Iter 25, disc loss: 0.05863117604561206, policy loss: 4.300580424900111
Experience 9, Iter 26, disc loss: 0.05378503499096331, policy loss: 4.399544865104804
Experience 9, Iter 27, disc loss: 0.052778059868657204, policy loss: 4.2137680500937025
Experience 9, Iter 28, disc loss: 0.05961546050572342, policy loss: 4.089811619350443
Experience 9, Iter 29, disc loss: 0.05542434878307782, policy loss: 4.337504473855568
Experience 9, Iter 30, disc loss: 0.057290427113587034, policy loss: 4.3487479249092384
Experience 9, Iter 31, disc loss: 0.05284845629013299, policy loss: 4.856035665026537
Experience 9, Iter 32, disc loss: 0.04603049153482104, policy loss: 5.023686757056276
Experience 9, Iter 33, disc loss: 0.055487659686089416, policy loss: 4.483600446686031
Experience 9, Iter 34, disc loss: 0.04954313785381953, policy loss: 4.878210878653286
Experience 9, Iter 35, disc loss: 0.048953453642175015, policy loss: 4.824170058863265
Experience 9, Iter 36, disc loss: 0.046695560783719295, policy loss: 5.130265224839306
Experience 9, Iter 37, disc loss: 0.04287102975681542, policy loss: 5.32192813046084
Experience 9, Iter 38, disc loss: 0.05080114735314664, policy loss: 4.303512397605952
Experience 9, Iter 39, disc loss: 0.04579941943014018, policy loss: 4.454597650794585
Experience 9, Iter 40, disc loss: 0.04962881945177336, policy loss: 4.298381798852644
Experience 9, Iter 41, disc loss: 0.04680289050908942, policy loss: 4.752067138756379
Experience 9, Iter 42, disc loss: 0.04568459641067568, policy loss: 5.00265386356502
Experience 9, Iter 43, disc loss: 0.047128071003945546, policy loss: 4.913047125060773
Experience 9, Iter 44, disc loss: 0.042887651559391476, policy loss: 4.83069586492744
Experience 9, Iter 45, disc loss: 0.045244004870396023, policy loss: 4.238320413787593
Experience 9, Iter 46, disc loss: 0.039711852005698656, policy loss: 4.860447167800487
Experience 9, Iter 47, disc loss: 0.037368999545271786, policy loss: 4.8586529741572955
Experience 9, Iter 48, disc loss: 0.04455799454122371, policy loss: 4.726952674686299
Experience 9, Iter 49, disc loss: 0.045667653499562526, policy loss: 4.690443007670045
Experience 9, Iter 50, disc loss: 0.04501737717893463, policy loss: 4.753020002821367
Experience 9, Iter 51, disc loss: 0.04885927851412119, policy loss: 4.228844368629631
Experience 9, Iter 52, disc loss: 0.04368783634995923, policy loss: 4.6841266592476245
Experience 9, Iter 53, disc loss: 0.03681903086677299, policy loss: 5.396440669259163
Experience 9, Iter 54, disc loss: 0.04008539503220254, policy loss: 4.674500399281046
Experience 9, Iter 55, disc loss: 0.04139907755311889, policy loss: 4.617495849259686
Experience 9, Iter 56, disc loss: 0.03738323616372234, policy loss: 4.923912409926291
Experience 9, Iter 57, disc loss: 0.042618989404595076, policy loss: 4.744859445698626
Experience 9, Iter 58, disc loss: 0.04296378820952088, policy loss: 4.680407145763585
Experience 9, Iter 59, disc loss: 0.0363992722862749, policy loss: 5.165817794867939
Experience 9, Iter 60, disc loss: 0.03937412365134867, policy loss: 5.044873279524262
Experience 9, Iter 61, disc loss: 0.0355720294413656, policy loss: 5.1271318072861565
Experience 9, Iter 62, disc loss: 0.03782625241803313, policy loss: 4.844269644129362
Experience 9, Iter 63, disc loss: 0.036615084406716056, policy loss: 4.746138784967096
Experience 9, Iter 64, disc loss: 0.03193154323671076, policy loss: 5.659386094403896
Experience 9, Iter 65, disc loss: 0.038925193846062106, policy loss: 4.727922513983363
Experience 9, Iter 66, disc loss: 0.04206638631156147, policy loss: 4.512077249065671
Experience 9, Iter 67, disc loss: 0.03236953742558498, policy loss: 4.989880858287984
Experience 9, Iter 68, disc loss: 0.03786332055971769, policy loss: 4.58017422002087
Experience 9, Iter 69, disc loss: 0.03149193287109957, policy loss: 5.028409916045824
Experience 9, Iter 70, disc loss: 0.031863986688329377, policy loss: 4.8764363911653685
Experience 9, Iter 71, disc loss: 0.03508513789873806, policy loss: 4.92683723780228
Experience 9, Iter 72, disc loss: 0.03159791854155252, policy loss: 5.0482227208735235
Experience 9, Iter 73, disc loss: 0.03187390418380087, policy loss: 4.97734676804181
Experience 9, Iter 74, disc loss: 0.035448470034846774, policy loss: 4.6392384498521615
Experience 9, Iter 75, disc loss: 0.03470048816177462, policy loss: 4.6259803240495945
Experience 9, Iter 76, disc loss: 0.034316896590269104, policy loss: 4.913545760466192
Experience 9, Iter 77, disc loss: 0.03969945541609353, policy loss: 4.717851968281319
Experience 9, Iter 78, disc loss: 0.034483791408214184, policy loss: 4.657540442710229
Experience 9, Iter 79, disc loss: 0.034774343505523914, policy loss: 4.713439522196855
Experience 9, Iter 80, disc loss: 0.036887856608908184, policy loss: 4.883099488930688
Experience 9, Iter 81, disc loss: 0.04078754630862804, policy loss: 4.417229882816169
Experience 9, Iter 82, disc loss: 0.0345690190997405, policy loss: 5.139824088707112
Experience 9, Iter 83, disc loss: 0.03857996238961956, policy loss: 4.880988327461029
Experience 9, Iter 84, disc loss: 0.034827378448765337, policy loss: 5.361002485215848
Experience 9, Iter 85, disc loss: 0.036634547050129015, policy loss: 4.9058972706145925
Experience 9, Iter 86, disc loss: 0.03438804946783662, policy loss: 5.254770565582614
Experience 9, Iter 87, disc loss: 0.03731331586499647, policy loss: 4.786869093182457
Experience 9, Iter 88, disc loss: 0.036953807223983295, policy loss: 4.737971654533479
Experience 9, Iter 89, disc loss: 0.03930488614359855, policy loss: 4.709644517087215
Experience 9, Iter 90, disc loss: 0.03479246315972277, policy loss: 4.705010952711945
Experience 9, Iter 91, disc loss: 0.031222082407857656, policy loss: 5.539157709335848
Experience 9, Iter 92, disc loss: 0.02920933624778379, policy loss: 5.497225556186372
Experience 9, Iter 93, disc loss: 0.03036499261395066, policy loss: 5.195936723621202
Experience 9, Iter 94, disc loss: 0.029459413466753397, policy loss: 5.173523803157065
Experience 9, Iter 95, disc loss: 0.029003957447421704, policy loss: 5.2858774991241315
Experience 9, Iter 96, disc loss: 0.033720449351598826, policy loss: 4.543979591073931
Experience 9, Iter 97, disc loss: 0.03485023561130235, policy loss: 4.703403993525423
Experience 9, Iter 98, disc loss: 0.03009763474154531, policy loss: 5.202995548821656
Experience 9, Iter 99, disc loss: 0.03144580224903336, policy loss: 4.738126996893879
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0055],
        [0.0804],
        [0.9658],
        [0.0163]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0170, 0.1983, 0.7458, 0.0161, 0.0121, 2.4745]],

        [[0.0170, 0.1983, 0.7458, 0.0161, 0.0121, 2.4745]],

        [[0.0170, 0.1983, 0.7458, 0.0161, 0.0121, 2.4745]],

        [[0.0170, 0.1983, 0.7458, 0.0161, 0.0121, 2.4745]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0218, 0.3216, 3.8632, 0.0652], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0218, 0.3216, 3.8632, 0.0652])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.537
Iter 2/2000 - Loss: 2.673
Iter 3/2000 - Loss: 2.485
Iter 4/2000 - Loss: 2.488
Iter 5/2000 - Loss: 2.541
Iter 6/2000 - Loss: 2.490
Iter 7/2000 - Loss: 2.405
Iter 8/2000 - Loss: 2.351
Iter 9/2000 - Loss: 2.326
Iter 10/2000 - Loss: 2.289
Iter 11/2000 - Loss: 2.214
Iter 12/2000 - Loss: 2.110
Iter 13/2000 - Loss: 1.991
Iter 14/2000 - Loss: 1.863
Iter 15/2000 - Loss: 1.718
Iter 16/2000 - Loss: 1.546
Iter 17/2000 - Loss: 1.340
Iter 18/2000 - Loss: 1.104
Iter 19/2000 - Loss: 0.846
Iter 20/2000 - Loss: 0.573
Iter 1981/2000 - Loss: -7.529
Iter 1982/2000 - Loss: -7.529
Iter 1983/2000 - Loss: -7.530
Iter 1984/2000 - Loss: -7.530
Iter 1985/2000 - Loss: -7.530
Iter 1986/2000 - Loss: -7.530
Iter 1987/2000 - Loss: -7.530
Iter 1988/2000 - Loss: -7.530
Iter 1989/2000 - Loss: -7.530
Iter 1990/2000 - Loss: -7.530
Iter 1991/2000 - Loss: -7.530
Iter 1992/2000 - Loss: -7.530
Iter 1993/2000 - Loss: -7.530
Iter 1994/2000 - Loss: -7.530
Iter 1995/2000 - Loss: -7.530
Iter 1996/2000 - Loss: -7.530
Iter 1997/2000 - Loss: -7.530
Iter 1998/2000 - Loss: -7.530
Iter 1999/2000 - Loss: -7.530
Iter 2000/2000 - Loss: -7.530
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[13.8929,  7.9383, 29.8057,  5.2091,  5.5171, 57.8072]],

        [[19.3487, 41.6142, 11.6501,  1.2534,  2.3245, 31.9463]],

        [[17.2273, 34.4972, 10.2111,  1.0857,  1.9161, 25.0667]],

        [[17.0523, 39.3028, 17.0979,  3.2574,  2.6425, 50.2373]]])
Signal Variance: tensor([ 0.1256,  2.6009, 16.1413,  0.5575])
Estimated target variance: tensor([0.0218, 0.3216, 3.8632, 0.0652])
N: 100
Signal to noise ratio: tensor([20.3630, 84.5806, 98.3217, 46.4080])
Bound on condition number: tensor([ 41466.1065, 715389.4273, 966716.8781, 215371.2645])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.030571112887061436, policy loss: 5.071946548131814
Experience 10, Iter 1, disc loss: 0.03071918017258369, policy loss: 4.964849696577065
Experience 10, Iter 2, disc loss: 0.029127803441511245, policy loss: 5.034809013513916
Experience 10, Iter 3, disc loss: 0.032354569826198, policy loss: 5.21353663039862
Experience 10, Iter 4, disc loss: 0.028486352086532964, policy loss: 5.215814604191129
Experience 10, Iter 5, disc loss: 0.033039091643715254, policy loss: 4.751946321722519
Experience 10, Iter 6, disc loss: 0.029736902997104228, policy loss: 4.727286610039689
Experience 10, Iter 7, disc loss: 0.031546254812713136, policy loss: 5.077135095742241
Experience 10, Iter 8, disc loss: 0.032563540101792715, policy loss: 4.860807212083052
Experience 10, Iter 9, disc loss: 0.03213354937575347, policy loss: 4.862119130103374
Experience 10, Iter 10, disc loss: 0.030417160810855638, policy loss: 4.874973558359617
Experience 10, Iter 11, disc loss: 0.02822511001597154, policy loss: 5.2793447971129215
Experience 10, Iter 12, disc loss: 0.028858466783470885, policy loss: 5.10465093582838
Experience 10, Iter 13, disc loss: 0.027250235278762164, policy loss: 5.543971501527473
Experience 10, Iter 14, disc loss: 0.026638063663269256, policy loss: 5.528231374395082
Experience 10, Iter 15, disc loss: 0.026370219618368582, policy loss: 5.1547732612144905
Experience 10, Iter 16, disc loss: 0.025577588651789128, policy loss: 5.2262652872704365
Experience 10, Iter 17, disc loss: 0.028847160465054755, policy loss: 5.043471835752838
Experience 10, Iter 18, disc loss: 0.02483355583571699, policy loss: 5.382706000858004
Experience 10, Iter 19, disc loss: 0.024109509283194407, policy loss: 5.382939392112787
Experience 10, Iter 20, disc loss: 0.028331805176096347, policy loss: 5.164833305995912
Experience 10, Iter 21, disc loss: 0.027631515097541266, policy loss: 5.387050484258216
Experience 10, Iter 22, disc loss: 0.02348336252196355, policy loss: 5.869835917341312
Experience 10, Iter 23, disc loss: 0.023493019921644993, policy loss: 5.365889140469509
Experience 10, Iter 24, disc loss: 0.020366952236920866, policy loss: 6.16136549069933
Experience 10, Iter 25, disc loss: 0.01634463542932587, policy loss: 6.52177095050409
Experience 10, Iter 26, disc loss: 0.015139530239895279, policy loss: 6.7748122059594
Experience 10, Iter 27, disc loss: 0.015139432105235943, policy loss: 6.725744221446778
Experience 10, Iter 28, disc loss: 0.017399264711651808, policy loss: 5.604776324865632
Experience 10, Iter 29, disc loss: 0.019553439924296655, policy loss: 5.297462421137759
Experience 10, Iter 30, disc loss: 0.01966741549285432, policy loss: 5.4316997646372345
Experience 10, Iter 31, disc loss: 0.01899692545914198, policy loss: 5.361117674407449
Experience 10, Iter 32, disc loss: 0.012636916646035373, policy loss: 7.046598619460579
Experience 10, Iter 33, disc loss: 0.018661152427084132, policy loss: 5.102798520988786
Experience 10, Iter 34, disc loss: 0.01697181660797211, policy loss: 5.482031024995806
Experience 10, Iter 35, disc loss: 0.012669086965601945, policy loss: 6.14259357671905
Experience 10, Iter 36, disc loss: 0.01190480819782666, policy loss: 6.6254126768042045
Experience 10, Iter 37, disc loss: 0.010379920974430383, policy loss: 7.675152188133605
Experience 10, Iter 38, disc loss: 0.010363461474280598, policy loss: 6.627424368084321
Experience 10, Iter 39, disc loss: 0.01215307080433926, policy loss: 6.187873866487209
Experience 10, Iter 40, disc loss: 0.01256450372045683, policy loss: 6.591281226289715
Experience 10, Iter 41, disc loss: 0.016610421568435996, policy loss: 5.108139752524522
Experience 10, Iter 42, disc loss: 0.02052948219398634, policy loss: 5.228984171480843
Experience 10, Iter 43, disc loss: 0.021274538422661263, policy loss: 4.87570631879314
Experience 10, Iter 44, disc loss: 0.010591667143120435, policy loss: 7.912114520724053
Experience 10, Iter 45, disc loss: 0.01781161902560256, policy loss: 5.400156658410496
Experience 10, Iter 46, disc loss: 0.01382886407345349, policy loss: 5.460689908347655
Experience 10, Iter 47, disc loss: 0.01161169001254983, policy loss: 5.861443708415457
Experience 10, Iter 48, disc loss: 0.009255558977220994, policy loss: 6.681456640682555
Experience 10, Iter 49, disc loss: 0.008351100489521972, policy loss: 6.935429890100126
Experience 10, Iter 50, disc loss: 0.008053435547583648, policy loss: 7.481446718980527
Experience 10, Iter 51, disc loss: 0.00816904668422146, policy loss: 7.090682914231431
Experience 10, Iter 52, disc loss: 0.008129123676146594, policy loss: 6.920136623663559
Experience 10, Iter 53, disc loss: 0.009001408208584697, policy loss: 6.408076572232435
Experience 10, Iter 54, disc loss: 0.010441291192098856, policy loss: 5.702638715012677
Experience 10, Iter 55, disc loss: 0.012747240392403165, policy loss: 5.431784296531426
Experience 10, Iter 56, disc loss: 0.015087893104799875, policy loss: 5.3407016608624325
Experience 10, Iter 57, disc loss: 0.015210389349475554, policy loss: 5.146669282111396
Experience 10, Iter 58, disc loss: 0.014392484982987046, policy loss: 5.180234692004127
Experience 10, Iter 59, disc loss: 0.009725730150568171, policy loss: 6.064184856476974
Experience 10, Iter 60, disc loss: 0.014682488764991595, policy loss: 5.46413487862891
Experience 10, Iter 61, disc loss: 0.018424176008838, policy loss: 5.021437579025212
Experience 10, Iter 62, disc loss: 0.01727335523977942, policy loss: 5.124243559099128
Experience 10, Iter 63, disc loss: 0.018848249539749035, policy loss: 4.926234036903761
Experience 10, Iter 64, disc loss: 0.014828088740584383, policy loss: 5.584260567870223
Experience 10, Iter 65, disc loss: 0.01759548704565519, policy loss: 5.177674335102047
Experience 10, Iter 66, disc loss: 0.018042642208553443, policy loss: 5.295767736560259
Experience 10, Iter 67, disc loss: 0.017273234961646018, policy loss: 5.113173839770361
Experience 10, Iter 68, disc loss: 0.018230679864710102, policy loss: 5.277838414788851
Experience 10, Iter 69, disc loss: 0.01531269584805036, policy loss: 5.66459280000446
Experience 10, Iter 70, disc loss: 0.016181239875996067, policy loss: 5.630498282498818
Experience 10, Iter 71, disc loss: 0.017719547875642297, policy loss: 5.524231105180167
Experience 10, Iter 72, disc loss: 0.016094231516244447, policy loss: 5.7702710682813025
Experience 10, Iter 73, disc loss: 0.015646047247893553, policy loss: 5.987593910780336
Experience 10, Iter 74, disc loss: 0.016029826103558763, policy loss: 6.325635335988618
Experience 10, Iter 75, disc loss: 0.013318818836292528, policy loss: 6.4970849183734165
Experience 10, Iter 76, disc loss: 0.014554397156014642, policy loss: 5.843970692140822
Experience 10, Iter 77, disc loss: 0.016134628522474888, policy loss: 5.5669703599222435
Experience 10, Iter 78, disc loss: 0.015607433458261552, policy loss: 5.7647931643299675
Experience 10, Iter 79, disc loss: 0.014920165251042266, policy loss: 5.7892449650918225
Experience 10, Iter 80, disc loss: 0.01531850520964036, policy loss: 5.464838992522162
Experience 10, Iter 81, disc loss: 0.015373678139109417, policy loss: 6.282976404945082
Experience 10, Iter 82, disc loss: 0.014661441355267073, policy loss: 5.596123279012124
Experience 10, Iter 83, disc loss: 0.01562886593416336, policy loss: 5.585719128823302
Experience 10, Iter 84, disc loss: 0.016198691249812457, policy loss: 5.653741097044352
Experience 10, Iter 85, disc loss: 0.015902190504997908, policy loss: 5.675221766806289
Experience 10, Iter 86, disc loss: 0.017859436211055617, policy loss: 5.566457774769302
Experience 10, Iter 87, disc loss: 0.014473081045835674, policy loss: 6.120901170604461
Experience 10, Iter 88, disc loss: 0.018753606710365293, policy loss: 5.881751009781681
Experience 10, Iter 89, disc loss: 0.015716123752200318, policy loss: 5.492599133327555
Experience 10, Iter 90, disc loss: 0.01489424884976749, policy loss: 5.653012589290718
Experience 10, Iter 91, disc loss: 0.014734105241076928, policy loss: 5.811001647122908
Experience 10, Iter 92, disc loss: 0.0153459775821838, policy loss: 5.477203320316821
Experience 10, Iter 93, disc loss: 0.014608111333950013, policy loss: 6.07369744953716
Experience 10, Iter 94, disc loss: 0.014353945327062235, policy loss: 5.839468701150204
Experience 10, Iter 95, disc loss: 0.014133955082944072, policy loss: 5.496196461204319
Experience 10, Iter 96, disc loss: 0.014513795722743358, policy loss: 5.392232811387297
Experience 10, Iter 97, disc loss: 0.013259759557391594, policy loss: 5.654895670284375
Experience 10, Iter 98, disc loss: 0.015269118531990526, policy loss: 5.516534955708108
Experience 10, Iter 99, disc loss: 0.014513122585836596, policy loss: 5.812274414857182
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0059],
        [0.0968],
        [1.1900],
        [0.0198]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0165, 0.2155, 0.9098, 0.0181, 0.0147, 2.8667]],

        [[0.0165, 0.2155, 0.9098, 0.0181, 0.0147, 2.8667]],

        [[0.0165, 0.2155, 0.9098, 0.0181, 0.0147, 2.8667]],

        [[0.0165, 0.2155, 0.9098, 0.0181, 0.0147, 2.8667]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0236, 0.3874, 4.7599, 0.0791], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0236, 0.3874, 4.7599, 0.0791])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.881
Iter 2/2000 - Loss: 2.942
Iter 3/2000 - Loss: 2.808
Iter 4/2000 - Loss: 2.803
Iter 5/2000 - Loss: 2.823
Iter 6/2000 - Loss: 2.769
Iter 7/2000 - Loss: 2.692
Iter 8/2000 - Loss: 2.641
Iter 9/2000 - Loss: 2.605
Iter 10/2000 - Loss: 2.540
Iter 11/2000 - Loss: 2.437
Iter 12/2000 - Loss: 2.312
Iter 13/2000 - Loss: 2.178
Iter 14/2000 - Loss: 2.032
Iter 15/2000 - Loss: 1.859
Iter 16/2000 - Loss: 1.648
Iter 17/2000 - Loss: 1.402
Iter 18/2000 - Loss: 1.130
Iter 19/2000 - Loss: 0.840
Iter 20/2000 - Loss: 0.539
Iter 1981/2000 - Loss: -7.620
Iter 1982/2000 - Loss: -7.620
Iter 1983/2000 - Loss: -7.620
Iter 1984/2000 - Loss: -7.620
Iter 1985/2000 - Loss: -7.620
Iter 1986/2000 - Loss: -7.620
Iter 1987/2000 - Loss: -7.620
Iter 1988/2000 - Loss: -7.620
Iter 1989/2000 - Loss: -7.620
Iter 1990/2000 - Loss: -7.620
Iter 1991/2000 - Loss: -7.620
Iter 1992/2000 - Loss: -7.620
Iter 1993/2000 - Loss: -7.620
Iter 1994/2000 - Loss: -7.620
Iter 1995/2000 - Loss: -7.620
Iter 1996/2000 - Loss: -7.620
Iter 1997/2000 - Loss: -7.620
Iter 1998/2000 - Loss: -7.620
Iter 1999/2000 - Loss: -7.620
Iter 2000/2000 - Loss: -7.620
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[12.8020,  8.3799, 27.6457,  6.3704,  7.0145, 59.1602]],

        [[19.1850, 40.3991, 11.7594,  1.2709,  2.2490, 32.8192]],

        [[16.4599, 35.0353,  8.8302,  1.1494,  1.9245, 25.8190]],

        [[16.0629, 37.5450, 16.9203,  3.4339,  2.5292, 50.7215]]])
Signal Variance: tensor([ 0.1311,  2.6104, 17.9921,  0.5425])
Estimated target variance: tensor([0.0236, 0.3874, 4.7599, 0.0791])
N: 110
Signal to noise ratio: tensor([ 20.8422,  84.3963, 104.3630,  47.0169])
Bound on condition number: tensor([  47784.7174,  783502.3359, 1198081.0356,  243165.8315])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.014127772374430097, policy loss: 5.629094511152399
Experience 11, Iter 1, disc loss: 0.013968850211713504, policy loss: 5.750384487086038
Experience 11, Iter 2, disc loss: 0.015328357987399798, policy loss: 5.766658415265061
Experience 11, Iter 3, disc loss: 0.015050052010910892, policy loss: 5.224180739406787
Experience 11, Iter 4, disc loss: 0.014327730740925734, policy loss: 5.477775141477198
Experience 11, Iter 5, disc loss: 0.015384833790957925, policy loss: 5.472820375445228
Experience 11, Iter 6, disc loss: 0.014846601849123287, policy loss: 5.8133255370095664
Experience 11, Iter 7, disc loss: 0.01428935058678337, policy loss: 5.494905457199807
Experience 11, Iter 8, disc loss: 0.01557624478634016, policy loss: 5.48485141968604
Experience 11, Iter 9, disc loss: 0.014700853236130483, policy loss: 5.484541643025951
Experience 11, Iter 10, disc loss: 0.012936230390860792, policy loss: 5.873201223064144
Experience 11, Iter 11, disc loss: 0.014054414920658911, policy loss: 5.4797436548743095
Experience 11, Iter 12, disc loss: 0.014307857009232974, policy loss: 5.78135809138843
Experience 11, Iter 13, disc loss: 0.015036449978974738, policy loss: 5.483044074092235
Experience 11, Iter 14, disc loss: 0.013862604188159848, policy loss: 5.700624898322082
Experience 11, Iter 15, disc loss: 0.015535233760232681, policy loss: 5.40860159249411
Experience 11, Iter 16, disc loss: 0.013666938518305147, policy loss: 5.865243449699646
Experience 11, Iter 17, disc loss: 0.014817488208003996, policy loss: 5.883842916460412
Experience 11, Iter 18, disc loss: 0.011917775618280061, policy loss: 6.185080952624881
Experience 11, Iter 19, disc loss: 0.01407276209813893, policy loss: 6.008733355235727
Experience 11, Iter 20, disc loss: 0.012551662889217136, policy loss: 6.014491248094533
Experience 11, Iter 21, disc loss: 0.013226244039722345, policy loss: 5.715617226371785
Experience 11, Iter 22, disc loss: 0.0136735574749419, policy loss: 6.4274156743432425
Experience 11, Iter 23, disc loss: 0.011965662838154915, policy loss: 6.070433071975144
Experience 11, Iter 24, disc loss: 0.014068259283032698, policy loss: 5.915523656603821
Experience 11, Iter 25, disc loss: 0.013398846018648533, policy loss: 5.834922459154593
Experience 11, Iter 26, disc loss: 0.013401402701212695, policy loss: 6.354579106637107
Experience 11, Iter 27, disc loss: 0.011782047999964166, policy loss: 6.246943317656482
Experience 11, Iter 28, disc loss: 0.013127713282501313, policy loss: 6.118505873290744
Experience 11, Iter 29, disc loss: 0.012170618598728145, policy loss: 6.099571127326467
Experience 11, Iter 30, disc loss: 0.01131388957124341, policy loss: 6.494599265281087
Experience 11, Iter 31, disc loss: 0.012468337939286148, policy loss: 6.287371036162929
Experience 11, Iter 32, disc loss: 0.01189778282223522, policy loss: 6.048941222835785
Experience 11, Iter 33, disc loss: 0.01122167343819083, policy loss: 5.868873688409009
Experience 11, Iter 34, disc loss: 0.010821212152586994, policy loss: 5.901130894818202
Experience 11, Iter 35, disc loss: 0.012443885341113892, policy loss: 5.786940351325895
Experience 11, Iter 36, disc loss: 0.013377624888396755, policy loss: 5.574364438363977
Experience 11, Iter 37, disc loss: 0.0096398990396406, policy loss: 6.510992766927268
Experience 11, Iter 38, disc loss: 0.010286138760804451, policy loss: 6.277321555570783
Experience 11, Iter 39, disc loss: 0.012348039007670758, policy loss: 5.713677272172916
Experience 11, Iter 40, disc loss: 0.011805633735543641, policy loss: 5.9966298601853625
Experience 11, Iter 41, disc loss: 0.012354935529861114, policy loss: 5.783762638919165
Experience 11, Iter 42, disc loss: 0.012162043951209105, policy loss: 5.954048372068323
Experience 11, Iter 43, disc loss: 0.01107676704037871, policy loss: 6.546815590490819
Experience 11, Iter 44, disc loss: 0.010943502009006019, policy loss: 5.752855471311616
Experience 11, Iter 45, disc loss: 0.011838104063911977, policy loss: 5.788284555391815
Experience 11, Iter 46, disc loss: 0.011563564024823586, policy loss: 5.769662905363321
Experience 11, Iter 47, disc loss: 0.011367378818126756, policy loss: 6.647964754939662
Experience 11, Iter 48, disc loss: 0.010321675241025216, policy loss: 6.41355786743248
Experience 11, Iter 49, disc loss: 0.011289609468759863, policy loss: 5.799627527769841
Experience 11, Iter 50, disc loss: 0.010410030971777756, policy loss: 6.513743964552765
Experience 11, Iter 51, disc loss: 0.009774594363562265, policy loss: 6.056599689056039
Experience 11, Iter 52, disc loss: 0.01163340339902141, policy loss: 6.031293258486866
Experience 11, Iter 53, disc loss: 0.011565380165604766, policy loss: 5.775672411679089
Experience 11, Iter 54, disc loss: 0.010997430300002942, policy loss: 5.804772862686987
Experience 11, Iter 55, disc loss: 0.010564088324969868, policy loss: 6.011891807561679
Experience 11, Iter 56, disc loss: 0.008942484692978253, policy loss: 6.277453179653813
Experience 11, Iter 57, disc loss: 0.01045705019584839, policy loss: 6.668810972961172
Experience 11, Iter 58, disc loss: 0.01134908375528509, policy loss: 6.251134450963914
Experience 11, Iter 59, disc loss: 0.011788930628980891, policy loss: 5.921862375938039
Experience 11, Iter 60, disc loss: 0.010048275574448779, policy loss: 5.682581710326743
Experience 11, Iter 61, disc loss: 0.012389460596413682, policy loss: 5.511668459109689
Experience 11, Iter 62, disc loss: 0.011573132291002132, policy loss: 5.536567800501132
Experience 11, Iter 63, disc loss: 0.010370969910044388, policy loss: 6.102587014463934
Experience 11, Iter 64, disc loss: 0.0111786644450956, policy loss: 6.462450116048046
Experience 11, Iter 65, disc loss: 0.012827054046084143, policy loss: 5.923360546486288
Experience 11, Iter 66, disc loss: 0.011673356021061919, policy loss: 5.782190528050586
Experience 11, Iter 67, disc loss: 0.010382503674490421, policy loss: 6.062924628322252
Experience 11, Iter 68, disc loss: 0.009523679295190756, policy loss: 6.284265000719966
Experience 11, Iter 69, disc loss: 0.00948002595118164, policy loss: 6.245632847607349
Experience 11, Iter 70, disc loss: 0.011323419979429313, policy loss: 6.106226505254407
Experience 11, Iter 71, disc loss: 0.011742228990589715, policy loss: 5.89490481603646
Experience 11, Iter 72, disc loss: 0.011840419299047637, policy loss: 5.997048644656387
Experience 11, Iter 73, disc loss: 0.011177550500201458, policy loss: 6.185750447812242
Experience 11, Iter 74, disc loss: 0.011558301391896071, policy loss: 5.869882536438473
Experience 11, Iter 75, disc loss: 0.010184434838783063, policy loss: 6.095243929284253
Experience 11, Iter 76, disc loss: 0.011321791780380976, policy loss: 5.855793177902324
Experience 11, Iter 77, disc loss: 0.010165821632432999, policy loss: 6.035459518859488
Experience 11, Iter 78, disc loss: 0.010962074661625786, policy loss: 5.980660333083861
Experience 11, Iter 79, disc loss: 0.009885372106475287, policy loss: 6.346808502593856
Experience 11, Iter 80, disc loss: 0.010183465092088388, policy loss: 6.320109676844419
Experience 11, Iter 81, disc loss: 0.01142363628410083, policy loss: 5.893761852837169
Experience 11, Iter 82, disc loss: 0.010081933376885696, policy loss: 6.6087603256455685
Experience 11, Iter 83, disc loss: 0.012356833836412, policy loss: 5.820820702160287
Experience 11, Iter 84, disc loss: 0.010625489660570243, policy loss: 6.096982400774736
Experience 11, Iter 85, disc loss: 0.011165069605894522, policy loss: 6.103939084878794
Experience 11, Iter 86, disc loss: 0.010266521823043837, policy loss: 6.213802981974662
Experience 11, Iter 87, disc loss: 0.011181065974556818, policy loss: 6.0052827456958795
Experience 11, Iter 88, disc loss: 0.010603013465899263, policy loss: 6.602155834705419
Experience 11, Iter 89, disc loss: 0.010209379454300463, policy loss: 5.951493237121428
Experience 11, Iter 90, disc loss: 0.009816223024658101, policy loss: 6.312181822740653
Experience 11, Iter 91, disc loss: 0.01034640890358042, policy loss: 6.1033627511673405
Experience 11, Iter 92, disc loss: 0.01070677467167086, policy loss: 6.597751917118537
Experience 11, Iter 93, disc loss: 0.009111436732784, policy loss: 7.217834671136906
Experience 11, Iter 94, disc loss: 0.009922758225459948, policy loss: 6.139876419401611
Experience 11, Iter 95, disc loss: 0.008949929544758393, policy loss: 6.329352478114083
Experience 11, Iter 96, disc loss: 0.00929452182575174, policy loss: 6.532220065943061
Experience 11, Iter 97, disc loss: 0.00809585769297118, policy loss: 6.595788571946105
Experience 11, Iter 98, disc loss: 0.008302881288721838, policy loss: 7.284325586383504
Experience 11, Iter 99, disc loss: 0.008323791406780587, policy loss: 6.633179453985193
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0064],
        [0.1137],
        [1.4069],
        [0.0231]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0161, 0.2358, 1.0617, 0.0201, 0.0171, 3.2424]],

        [[0.0161, 0.2358, 1.0617, 0.0201, 0.0171, 3.2424]],

        [[0.0161, 0.2358, 1.0617, 0.0201, 0.0171, 3.2424]],

        [[0.0161, 0.2358, 1.0617, 0.0201, 0.0171, 3.2424]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0258, 0.4549, 5.6276, 0.0925], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0258, 0.4549, 5.6276, 0.0925])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.173
Iter 2/2000 - Loss: 3.168
Iter 3/2000 - Loss: 3.065
Iter 4/2000 - Loss: 3.036
Iter 5/2000 - Loss: 3.020
Iter 6/2000 - Loss: 2.950
Iter 7/2000 - Loss: 2.864
Iter 8/2000 - Loss: 2.788
Iter 9/2000 - Loss: 2.705
Iter 10/2000 - Loss: 2.591
Iter 11/2000 - Loss: 2.451
Iter 12/2000 - Loss: 2.294
Iter 13/2000 - Loss: 2.121
Iter 14/2000 - Loss: 1.924
Iter 15/2000 - Loss: 1.696
Iter 16/2000 - Loss: 1.439
Iter 17/2000 - Loss: 1.161
Iter 18/2000 - Loss: 0.867
Iter 19/2000 - Loss: 0.561
Iter 20/2000 - Loss: 0.244
Iter 1981/2000 - Loss: -7.584
Iter 1982/2000 - Loss: -7.584
Iter 1983/2000 - Loss: -7.584
Iter 1984/2000 - Loss: -7.584
Iter 1985/2000 - Loss: -7.584
Iter 1986/2000 - Loss: -7.584
Iter 1987/2000 - Loss: -7.584
Iter 1988/2000 - Loss: -7.584
Iter 1989/2000 - Loss: -7.584
Iter 1990/2000 - Loss: -7.584
Iter 1991/2000 - Loss: -7.584
Iter 1992/2000 - Loss: -7.584
Iter 1993/2000 - Loss: -7.584
Iter 1994/2000 - Loss: -7.584
Iter 1995/2000 - Loss: -7.584
Iter 1996/2000 - Loss: -7.584
Iter 1997/2000 - Loss: -7.584
Iter 1998/2000 - Loss: -7.585
Iter 1999/2000 - Loss: -7.585
Iter 2000/2000 - Loss: -7.585
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.9602,  7.7633, 26.4062,  3.1592,  4.6719, 56.7652]],

        [[18.2420, 36.7632, 11.9028,  1.3126,  2.5436, 27.6470]],

        [[15.8222, 35.5865,  7.9701,  1.1277,  1.3968, 18.5568]],

        [[15.8466, 36.9456, 18.8054,  2.9255,  1.9086, 50.5689]]])
Signal Variance: tensor([ 0.1076,  2.5708, 14.2575,  0.6202])
Estimated target variance: tensor([0.0258, 0.4549, 5.6276, 0.0925])
N: 120
Signal to noise ratio: tensor([18.9685, 83.3598, 93.0316, 50.4045])
Bound on condition number: tensor([  43177.6527,  833863.9870, 1038585.6424,  304874.0654])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.007873432105496308, policy loss: 6.497517542217487
Experience 12, Iter 1, disc loss: 0.007616484205761982, policy loss: 6.645811426576238
Experience 12, Iter 2, disc loss: 0.007381818657806459, policy loss: 7.175190090756173
Experience 12, Iter 3, disc loss: 0.006477821315896729, policy loss: 7.519353561982827
Experience 12, Iter 4, disc loss: 0.007263210117625455, policy loss: 6.833350417538448
Experience 12, Iter 5, disc loss: 0.008077965984924603, policy loss: 6.459094522455934
Experience 12, Iter 6, disc loss: 0.009569732609985485, policy loss: 6.454124063557345
Experience 12, Iter 7, disc loss: 0.00829402731393735, policy loss: 6.559878278878221
Experience 12, Iter 8, disc loss: 0.008428976459347766, policy loss: 6.017891105649252
Experience 12, Iter 9, disc loss: 0.00881158833872666, policy loss: 6.621593685895324
Experience 12, Iter 10, disc loss: 0.007834102457256348, policy loss: 6.453319251405602
Experience 12, Iter 11, disc loss: 0.007574306766739109, policy loss: 6.651712952432328
Experience 12, Iter 12, disc loss: 0.00701178937320749, policy loss: 6.421427173305107
Experience 12, Iter 13, disc loss: 0.007573840724501091, policy loss: 6.767224472464376
Experience 12, Iter 14, disc loss: 0.00872885298362333, policy loss: 6.2953560899225725
Experience 12, Iter 15, disc loss: 0.007833076514326747, policy loss: 6.546730931872393
Experience 12, Iter 16, disc loss: 0.00874467732163931, policy loss: 6.345212050487447
Experience 12, Iter 17, disc loss: 0.008119721620184591, policy loss: 6.1495598210374
Experience 12, Iter 18, disc loss: 0.007662902025530353, policy loss: 6.334564565821352
Experience 12, Iter 19, disc loss: 0.008006715257860142, policy loss: 6.429219887457574
Experience 12, Iter 20, disc loss: 0.008219000306077782, policy loss: 6.184856187960264
Experience 12, Iter 21, disc loss: 0.008151142852898082, policy loss: 6.07746882086281
Experience 12, Iter 22, disc loss: 0.008466699240448976, policy loss: 6.175332517230706
Experience 12, Iter 23, disc loss: 0.009353300529071119, policy loss: 6.250647546782989
Experience 12, Iter 24, disc loss: 0.008991457220203648, policy loss: 5.898493009555378
Experience 12, Iter 25, disc loss: 0.00733182274747957, policy loss: 6.4332618256800815
Experience 12, Iter 26, disc loss: 0.008983219454482553, policy loss: 6.3942823560193425
Experience 12, Iter 27, disc loss: 0.006920513966381629, policy loss: 6.457111803187979
Experience 12, Iter 28, disc loss: 0.008501486952361137, policy loss: 6.0793769929580925
Experience 12, Iter 29, disc loss: 0.007180143657864844, policy loss: 6.951737303832085
Experience 12, Iter 30, disc loss: 0.007453010583397797, policy loss: 6.280660573852967
Experience 12, Iter 31, disc loss: 0.007024631571283038, policy loss: 6.464291457238286
Experience 12, Iter 32, disc loss: 0.007357382172599404, policy loss: 6.741301310768126
Experience 12, Iter 33, disc loss: 0.007324959367523002, policy loss: 6.713591255200792
Experience 12, Iter 34, disc loss: 0.0070786620698651955, policy loss: 6.302139727919488
Experience 12, Iter 35, disc loss: 0.009392081467837005, policy loss: 6.1139370820740915
Experience 12, Iter 36, disc loss: 0.009272955136064406, policy loss: 6.416798983678271
Experience 12, Iter 37, disc loss: 0.007655440753106088, policy loss: 6.480365293375088
Experience 12, Iter 38, disc loss: 0.006830408171470322, policy loss: 7.197876906806518
Experience 12, Iter 39, disc loss: 0.00860207053464554, policy loss: 6.173378097956056
Experience 12, Iter 40, disc loss: 0.0076027812858831915, policy loss: 6.454843185800165
Experience 12, Iter 41, disc loss: 0.007674737046450953, policy loss: 6.905837266355554
Experience 12, Iter 42, disc loss: 0.007978180138225145, policy loss: 6.3941330045444005
Experience 12, Iter 43, disc loss: 0.00691326371796447, policy loss: 6.571636726547828
Experience 12, Iter 44, disc loss: 0.0067717302811055, policy loss: 7.027063314467246
Experience 12, Iter 45, disc loss: 0.0067391423394238, policy loss: 6.6293326647587385
Experience 12, Iter 46, disc loss: 0.007299471144485863, policy loss: 6.472464629949673
Experience 12, Iter 47, disc loss: 0.007325195850576817, policy loss: 6.357732583194084
Experience 12, Iter 48, disc loss: 0.0065926639789521, policy loss: 6.355739801035079
Experience 12, Iter 49, disc loss: 0.006500464783272769, policy loss: 6.537824119825506
Experience 12, Iter 50, disc loss: 0.007936302256831902, policy loss: 6.092675846774909
Experience 12, Iter 51, disc loss: 0.007969235228571098, policy loss: 6.321778147146572
Experience 12, Iter 52, disc loss: 0.006327392334556007, policy loss: 6.836372313823467
Experience 12, Iter 53, disc loss: 0.007021467586023146, policy loss: 6.9813920477102585
Experience 12, Iter 54, disc loss: 0.008239435394019312, policy loss: 6.364716541535694
Experience 12, Iter 55, disc loss: 0.006476961332320627, policy loss: 7.050418135978951
Experience 12, Iter 56, disc loss: 0.007413351251417395, policy loss: 6.548297617267038
Experience 12, Iter 57, disc loss: 0.006647817420491419, policy loss: 6.925907692664019
Experience 12, Iter 58, disc loss: 0.0077107009832329465, policy loss: 6.423071587746438
Experience 12, Iter 59, disc loss: 0.007436996893010677, policy loss: 6.488730859727646
Experience 12, Iter 60, disc loss: 0.006580241312273386, policy loss: 7.172801545910959
Experience 12, Iter 61, disc loss: 0.007137369332433901, policy loss: 6.616937585755704
Experience 12, Iter 62, disc loss: 0.007658891902938636, policy loss: 6.358582123719662
Experience 12, Iter 63, disc loss: 0.00703844918264869, policy loss: 7.032168733476139
Experience 12, Iter 64, disc loss: 0.006959894370293632, policy loss: 6.286471168193397
Experience 12, Iter 65, disc loss: 0.007159126905472332, policy loss: 6.617754987375138
Experience 12, Iter 66, disc loss: 0.008155689563252577, policy loss: 6.151639410960953
Experience 12, Iter 67, disc loss: 0.006328210590707566, policy loss: 6.368793527011494
Experience 12, Iter 68, disc loss: 0.0072342253608518835, policy loss: 6.444292494955187
Experience 12, Iter 69, disc loss: 0.007688630711039926, policy loss: 6.345652978730367
Experience 12, Iter 70, disc loss: 0.0079589182008432, policy loss: 6.011708752932698
Experience 12, Iter 71, disc loss: 0.007090574490601829, policy loss: 6.904670799520773
Experience 12, Iter 72, disc loss: 0.0073242003663480555, policy loss: 6.419711285378402
Experience 12, Iter 73, disc loss: 0.008809681274031835, policy loss: 6.510178652334194
Experience 12, Iter 74, disc loss: 0.007524798612758538, policy loss: 6.164586773105677
Experience 12, Iter 75, disc loss: 0.007475645759486086, policy loss: 6.402419282508524
Experience 12, Iter 76, disc loss: 0.006696318142843461, policy loss: 6.676372721696418
Experience 12, Iter 77, disc loss: 0.007033548410426623, policy loss: 6.534220234154088
Experience 12, Iter 78, disc loss: 0.007151249777584995, policy loss: 6.443691836792528
Experience 12, Iter 79, disc loss: 0.006798387093815965, policy loss: 6.329339035493526
Experience 12, Iter 80, disc loss: 0.006775894561284477, policy loss: 6.627945632567827
Experience 12, Iter 81, disc loss: 0.006320739441010125, policy loss: 6.725302098791104
Experience 12, Iter 82, disc loss: 0.007285877166884143, policy loss: 6.2257546216019914
Experience 12, Iter 83, disc loss: 0.006629547336293498, policy loss: 6.39215968985056
Experience 12, Iter 84, disc loss: 0.00649398726333923, policy loss: 6.512387906907872
Experience 12, Iter 85, disc loss: 0.0065579249175799995, policy loss: 6.314065130778899
Experience 12, Iter 86, disc loss: 0.00622970218915762, policy loss: 6.950992149007205
Experience 12, Iter 87, disc loss: 0.006431076919105447, policy loss: 6.641702800205887
Experience 12, Iter 88, disc loss: 0.006263514772750091, policy loss: 6.900926633169378
Experience 12, Iter 89, disc loss: 0.007769476685697574, policy loss: 6.767787046540578
Experience 12, Iter 90, disc loss: 0.006477349166173919, policy loss: 6.622057727576575
Experience 12, Iter 91, disc loss: 0.006709372584356426, policy loss: 6.450672311451591
Experience 12, Iter 92, disc loss: 0.006790340468536368, policy loss: 6.226323847308276
Experience 12, Iter 93, disc loss: 0.007180759725856864, policy loss: 6.547589059618805
Experience 12, Iter 94, disc loss: 0.008055801438512492, policy loss: 6.464139584487805
Experience 12, Iter 95, disc loss: 0.006442336513897978, policy loss: 6.804112392499432
Experience 12, Iter 96, disc loss: 0.00748616455084389, policy loss: 6.503036191224453
Experience 12, Iter 97, disc loss: 0.0071340759887277465, policy loss: 6.545849426135194
Experience 12, Iter 98, disc loss: 0.007971438051132622, policy loss: 6.366528247648385
Experience 12, Iter 99, disc loss: 0.006468831046178795, policy loss: 6.547690398994211
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0066],
        [0.1224],
        [1.5362],
        [0.0248]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.2420, 1.1390, 0.0214, 0.0184, 3.4501]],

        [[0.0153, 0.2420, 1.1390, 0.0214, 0.0184, 3.4501]],

        [[0.0153, 0.2420, 1.1390, 0.0214, 0.0184, 3.4501]],

        [[0.0153, 0.2420, 1.1390, 0.0214, 0.0184, 3.4501]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0264, 0.4896, 6.1446, 0.0990], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0264, 0.4896, 6.1446, 0.0990])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.294
Iter 2/2000 - Loss: 3.255
Iter 3/2000 - Loss: 3.156
Iter 4/2000 - Loss: 3.112
Iter 5/2000 - Loss: 3.079
Iter 6/2000 - Loss: 2.998
Iter 7/2000 - Loss: 2.907
Iter 8/2000 - Loss: 2.819
Iter 9/2000 - Loss: 2.715
Iter 10/2000 - Loss: 2.583
Iter 11/2000 - Loss: 2.433
Iter 12/2000 - Loss: 2.272
Iter 13/2000 - Loss: 2.093
Iter 14/2000 - Loss: 1.885
Iter 15/2000 - Loss: 1.646
Iter 16/2000 - Loss: 1.379
Iter 17/2000 - Loss: 1.094
Iter 18/2000 - Loss: 0.797
Iter 19/2000 - Loss: 0.487
Iter 20/2000 - Loss: 0.165
Iter 1981/2000 - Loss: -7.713
Iter 1982/2000 - Loss: -7.713
Iter 1983/2000 - Loss: -7.713
Iter 1984/2000 - Loss: -7.714
Iter 1985/2000 - Loss: -7.714
Iter 1986/2000 - Loss: -7.714
Iter 1987/2000 - Loss: -7.714
Iter 1988/2000 - Loss: -7.714
Iter 1989/2000 - Loss: -7.714
Iter 1990/2000 - Loss: -7.714
Iter 1991/2000 - Loss: -7.714
Iter 1992/2000 - Loss: -7.714
Iter 1993/2000 - Loss: -7.714
Iter 1994/2000 - Loss: -7.714
Iter 1995/2000 - Loss: -7.714
Iter 1996/2000 - Loss: -7.714
Iter 1997/2000 - Loss: -7.714
Iter 1998/2000 - Loss: -7.714
Iter 1999/2000 - Loss: -7.714
Iter 2000/2000 - Loss: -7.714
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.4693,  7.9521, 25.9801,  3.3036,  5.2028, 56.6277]],

        [[17.9602, 34.2061, 12.0537,  1.3109,  2.5540, 27.7390]],

        [[16.0853, 33.2189,  8.5066,  1.0295,  1.4649, 18.2396]],

        [[15.5604, 35.2569, 18.9550,  2.8284,  1.8141, 51.8791]]])
Signal Variance: tensor([ 0.1095,  2.5365, 12.9663,  0.6208])
Estimated target variance: tensor([0.0264, 0.4896, 6.1446, 0.0990])
N: 130
Signal to noise ratio: tensor([19.1954, 85.7161, 81.9260, 52.3342])
Bound on condition number: tensor([ 47901.3579, 955143.5534, 872543.7996, 356054.1448])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.006838962519304925, policy loss: 6.968336362483516
Experience 13, Iter 1, disc loss: 0.006835447646673669, policy loss: 7.24006457442334
Experience 13, Iter 2, disc loss: 0.006501380002208381, policy loss: 6.899693919300114
Experience 13, Iter 3, disc loss: 0.007019459056391509, policy loss: 6.484640883301557
Experience 13, Iter 4, disc loss: 0.006379861416868901, policy loss: 6.519537992947674
Experience 13, Iter 5, disc loss: 0.006372473692058945, policy loss: 6.755397419014839
Experience 13, Iter 6, disc loss: 0.007125709671790034, policy loss: 6.800254596651005
Experience 13, Iter 7, disc loss: 0.006452132800347186, policy loss: 7.073275654715662
Experience 13, Iter 8, disc loss: 0.006396441054174702, policy loss: 7.353317957332999
Experience 13, Iter 9, disc loss: 0.0063608561555247274, policy loss: 6.9891253261406305
Experience 13, Iter 10, disc loss: 0.006477490569214891, policy loss: 6.8953857187541665
Experience 13, Iter 11, disc loss: 0.006804159386715547, policy loss: 6.348489816513385
Experience 13, Iter 12, disc loss: 0.0067985457828891906, policy loss: 6.419170234856556
Experience 13, Iter 13, disc loss: 0.006792697568368816, policy loss: 6.480923605228385
Experience 13, Iter 14, disc loss: 0.006819881306038113, policy loss: 6.292570199949186
Experience 13, Iter 15, disc loss: 0.005341196345748117, policy loss: 6.999463546207465
Experience 13, Iter 16, disc loss: 0.006348280880579071, policy loss: 6.358919830661459
Experience 13, Iter 17, disc loss: 0.006867910898665608, policy loss: 6.519435151665782
Experience 13, Iter 18, disc loss: 0.0062935258153291925, policy loss: 7.237725163248939
Experience 13, Iter 19, disc loss: 0.005161482958559454, policy loss: 7.329244490095894
Experience 13, Iter 20, disc loss: 0.00551629891743699, policy loss: 7.36268330780181
Experience 13, Iter 21, disc loss: 0.005583809899591046, policy loss: 6.927868942184338
Experience 13, Iter 22, disc loss: 0.006666650733030337, policy loss: 6.50854280494271
Experience 13, Iter 23, disc loss: 0.0058915113170307214, policy loss: 6.828124905678022
Experience 13, Iter 24, disc loss: 0.006443576572632132, policy loss: 6.866928493877236
Experience 13, Iter 25, disc loss: 0.004877270183136359, policy loss: 6.895364864412238
Experience 13, Iter 26, disc loss: 0.005141150543181048, policy loss: 7.12618097369562
Experience 13, Iter 27, disc loss: 0.005830501687332194, policy loss: 7.004475822231193
Experience 13, Iter 28, disc loss: 0.005887845368347589, policy loss: 6.7759844596475185
Experience 13, Iter 29, disc loss: 0.005883989223792793, policy loss: 6.755672715662889
Experience 13, Iter 30, disc loss: 0.005414977392460227, policy loss: 6.876865525986222
Experience 13, Iter 31, disc loss: 0.004894037848890514, policy loss: 7.447523519580821
Experience 13, Iter 32, disc loss: 0.004942978838623754, policy loss: 6.9872877946993475
Experience 13, Iter 33, disc loss: 0.005776167082101839, policy loss: 7.454423515729118
Experience 13, Iter 34, disc loss: 0.0054323494113319585, policy loss: 6.899564680920966
Experience 13, Iter 35, disc loss: 0.004578939840343294, policy loss: 7.235990601654014
Experience 13, Iter 36, disc loss: 0.005332418951825132, policy loss: 7.541393685882292
Experience 13, Iter 37, disc loss: 0.006131451512095941, policy loss: 6.576695371887527
Experience 13, Iter 38, disc loss: 0.004669336352089786, policy loss: 7.268621571550641
Experience 13, Iter 39, disc loss: 0.004756431749430573, policy loss: 7.321905702674426
Experience 13, Iter 40, disc loss: 0.005580795413924131, policy loss: 7.132765340503049
Experience 13, Iter 41, disc loss: 0.005333693978251177, policy loss: 6.6137898858110855
Experience 13, Iter 42, disc loss: 0.006032341613808418, policy loss: 7.096586137542528
Experience 13, Iter 43, disc loss: 0.006080518042281224, policy loss: 6.264335585677662
Experience 13, Iter 44, disc loss: 0.005256481694993723, policy loss: 6.700844690183077
Experience 13, Iter 45, disc loss: 0.0049038858073893864, policy loss: 6.802401559391693
Experience 13, Iter 46, disc loss: 0.004848815359557288, policy loss: 7.177743674561222
Experience 13, Iter 47, disc loss: 0.0056339774138018, policy loss: 7.240291362174878
Experience 13, Iter 48, disc loss: 0.005644613961522426, policy loss: 6.859120535527527
Experience 13, Iter 49, disc loss: 0.004883106062354099, policy loss: 6.74648810577623
Experience 13, Iter 50, disc loss: 0.006120160829007982, policy loss: 6.970409293826784
Experience 13, Iter 51, disc loss: 0.005289725062803528, policy loss: 6.629444147082833
Experience 13, Iter 52, disc loss: 0.0048228188297889645, policy loss: 7.056180813024502
Experience 13, Iter 53, disc loss: 0.005417254062166612, policy loss: 6.781586080071319
Experience 13, Iter 54, disc loss: 0.005367794274765916, policy loss: 7.233366928948824
Experience 13, Iter 55, disc loss: 0.005402840626481459, policy loss: 6.9517809803850295
Experience 13, Iter 56, disc loss: 0.005756343559839791, policy loss: 6.8291168621276
Experience 13, Iter 57, disc loss: 0.00548164963279567, policy loss: 6.654627794921353
Experience 13, Iter 58, disc loss: 0.005224631060951451, policy loss: 6.643837705742589
Experience 13, Iter 59, disc loss: 0.005241236244182214, policy loss: 6.573910785526911
Experience 13, Iter 60, disc loss: 0.005790848446527993, policy loss: 6.653606435172173
Experience 13, Iter 61, disc loss: 0.00427910725269209, policy loss: 7.736835965561945
Experience 13, Iter 62, disc loss: 0.005420889659065698, policy loss: 6.538681494110426
Experience 13, Iter 63, disc loss: 0.004933172132972811, policy loss: 6.948164597941886
Experience 13, Iter 64, disc loss: 0.0053710105832110114, policy loss: 7.178100674239122
Experience 13, Iter 65, disc loss: 0.00532777792457787, policy loss: 6.893878318486923
Experience 13, Iter 66, disc loss: 0.005404138297679552, policy loss: 6.48483523860368
Experience 13, Iter 67, disc loss: 0.005222030301034771, policy loss: 6.967867243954694
Experience 13, Iter 68, disc loss: 0.005026181155395918, policy loss: 7.160468529253755
Experience 13, Iter 69, disc loss: 0.005442956803399179, policy loss: 7.131308718859696
Experience 13, Iter 70, disc loss: 0.004971137934842755, policy loss: 6.64716435783743
Experience 13, Iter 71, disc loss: 0.004695133517316949, policy loss: 6.871637825731446
Experience 13, Iter 72, disc loss: 0.006339960515801294, policy loss: 6.317073753945395
Experience 13, Iter 73, disc loss: 0.004948646442021782, policy loss: 6.971873500570448
Experience 13, Iter 74, disc loss: 0.005903485092691051, policy loss: 6.436036495052635
Experience 13, Iter 75, disc loss: 0.005206307730254265, policy loss: 7.069824800865671
Experience 13, Iter 76, disc loss: 0.0053555910279037494, policy loss: 6.771344851961133
Experience 13, Iter 77, disc loss: 0.0049100410506594284, policy loss: 6.932823614928002
Experience 13, Iter 78, disc loss: 0.005846554396757563, policy loss: 6.676852211468412
Experience 13, Iter 79, disc loss: 0.005031078441509092, policy loss: 7.122434916558664
Experience 13, Iter 80, disc loss: 0.005071141719981133, policy loss: 6.908904452309909
Experience 13, Iter 81, disc loss: 0.00513737068279772, policy loss: 7.175154925987558
Experience 13, Iter 82, disc loss: 0.005257059190800986, policy loss: 6.624334796406803
Experience 13, Iter 83, disc loss: 0.005269118573352902, policy loss: 6.885790227467773
Experience 13, Iter 84, disc loss: 0.004844898199726752, policy loss: 7.372171874341954
Experience 13, Iter 85, disc loss: 0.004894382007314955, policy loss: 7.062900928056807
Experience 13, Iter 86, disc loss: 0.005437041795778159, policy loss: 6.484963153897597
Experience 13, Iter 87, disc loss: 0.005379084167854177, policy loss: 6.7634552150644005
Experience 13, Iter 88, disc loss: 0.005317265826006121, policy loss: 6.613151147579609
Experience 13, Iter 89, disc loss: 0.004703528359337427, policy loss: 6.941603431653638
Experience 13, Iter 90, disc loss: 0.004423764656699061, policy loss: 7.227236472550742
Experience 13, Iter 91, disc loss: 0.004971890876496623, policy loss: 7.466916270932195
Experience 13, Iter 92, disc loss: 0.005244425485772343, policy loss: 6.818308839198794
Experience 13, Iter 93, disc loss: 0.005432866526524096, policy loss: 6.701490270614897
Experience 13, Iter 94, disc loss: 0.004851937070025288, policy loss: 7.231596140680537
Experience 13, Iter 95, disc loss: 0.004816504790926317, policy loss: 7.03636273336856
Experience 13, Iter 96, disc loss: 0.004427384174631559, policy loss: 7.447187653232006
Experience 13, Iter 97, disc loss: 0.004166555464544456, policy loss: 7.186838979583591
Experience 13, Iter 98, disc loss: 0.005321686109705976, policy loss: 6.454181142355226
Experience 13, Iter 99, disc loss: 0.005259458997777662, policy loss: 6.487122523065247
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0068],
        [0.1319],
        [1.6659],
        [0.0266]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0149, 0.2494, 1.2149, 0.0229, 0.0201, 3.6760]],

        [[0.0149, 0.2494, 1.2149, 0.0229, 0.0201, 3.6760]],

        [[0.0149, 0.2494, 1.2149, 0.0229, 0.0201, 3.6760]],

        [[0.0149, 0.2494, 1.2149, 0.0229, 0.0201, 3.6760]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0273, 0.5276, 6.6635, 0.1064], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0273, 0.5276, 6.6635, 0.1064])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.434
Iter 2/2000 - Loss: 3.373
Iter 3/2000 - Loss: 3.284
Iter 4/2000 - Loss: 3.235
Iter 5/2000 - Loss: 3.201
Iter 6/2000 - Loss: 3.123
Iter 7/2000 - Loss: 3.036
Iter 8/2000 - Loss: 2.951
Iter 9/2000 - Loss: 2.847
Iter 10/2000 - Loss: 2.716
Iter 11/2000 - Loss: 2.570
Iter 12/2000 - Loss: 2.416
Iter 13/2000 - Loss: 2.244
Iter 14/2000 - Loss: 2.044
Iter 15/2000 - Loss: 1.813
Iter 16/2000 - Loss: 1.555
Iter 17/2000 - Loss: 1.280
Iter 18/2000 - Loss: 0.991
Iter 19/2000 - Loss: 0.689
Iter 20/2000 - Loss: 0.374
Iter 1981/2000 - Loss: -7.734
Iter 1982/2000 - Loss: -7.734
Iter 1983/2000 - Loss: -7.734
Iter 1984/2000 - Loss: -7.734
Iter 1985/2000 - Loss: -7.734
Iter 1986/2000 - Loss: -7.734
Iter 1987/2000 - Loss: -7.734
Iter 1988/2000 - Loss: -7.734
Iter 1989/2000 - Loss: -7.734
Iter 1990/2000 - Loss: -7.734
Iter 1991/2000 - Loss: -7.734
Iter 1992/2000 - Loss: -7.734
Iter 1993/2000 - Loss: -7.734
Iter 1994/2000 - Loss: -7.734
Iter 1995/2000 - Loss: -7.734
Iter 1996/2000 - Loss: -7.734
Iter 1997/2000 - Loss: -7.734
Iter 1998/2000 - Loss: -7.734
Iter 1999/2000 - Loss: -7.734
Iter 2000/2000 - Loss: -7.734
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.5762,  7.5681, 23.5676,  3.4138,  5.4145, 58.2194]],

        [[16.6713, 36.6624,  9.4138,  1.1374,  2.3758, 23.2618]],

        [[18.9090, 35.1901,  8.1775,  1.1124,  1.2164, 18.8661]],

        [[14.9205, 35.1347, 19.0339,  2.6774,  1.8141, 47.6825]]])
Signal Variance: tensor([ 0.1052,  1.7932, 12.1341,  0.6177])
Estimated target variance: tensor([0.0273, 0.5276, 6.6635, 0.1064])
N: 140
Signal to noise ratio: tensor([18.6859, 73.3044, 79.4464, 52.4778])
Bound on condition number: tensor([ 48883.6653, 752294.9617, 883643.6895, 385549.6490])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.005154921660487653, policy loss: 6.888107979272277
Experience 14, Iter 1, disc loss: 0.004365851956705094, policy loss: 7.252677777830629
Experience 14, Iter 2, disc loss: 0.005117904000780032, policy loss: 6.813120522358703
Experience 14, Iter 3, disc loss: 0.00508404259827166, policy loss: 7.060512206726308
Experience 14, Iter 4, disc loss: 0.0051159598610808835, policy loss: 7.273707005106735
Experience 14, Iter 5, disc loss: 0.004401258203464285, policy loss: 7.30095583589329
Experience 14, Iter 6, disc loss: 0.004814110431175913, policy loss: 7.50261377473857
Experience 14, Iter 7, disc loss: 0.004396510818655103, policy loss: 7.053120422338522
Experience 14, Iter 8, disc loss: 0.00444530339584991, policy loss: 6.933601095997641
Experience 14, Iter 9, disc loss: 0.004432706007199734, policy loss: 7.276882729108024
Experience 14, Iter 10, disc loss: 0.004300632469158549, policy loss: 7.3583222792881475
Experience 14, Iter 11, disc loss: 0.004194066670557197, policy loss: 7.1413658647160165
Experience 14, Iter 12, disc loss: 0.0046144481057750165, policy loss: 7.045809370176167
Experience 14, Iter 13, disc loss: 0.004482752556248018, policy loss: 7.422204452668109
Experience 14, Iter 14, disc loss: 0.0037734375017543357, policy loss: 7.353597233507111
Experience 14, Iter 15, disc loss: 0.004518052904423632, policy loss: 6.971732667046249
Experience 14, Iter 16, disc loss: 0.004557785315075319, policy loss: 7.247761522595348
Experience 14, Iter 17, disc loss: 0.0037832288546125383, policy loss: 7.56647353796071
Experience 14, Iter 18, disc loss: 0.004692631956618827, policy loss: 6.949713641478954
Experience 14, Iter 19, disc loss: 0.0042211269708037555, policy loss: 7.04634380045455
Experience 14, Iter 20, disc loss: 0.0043098233465467956, policy loss: 7.017072500777111
Experience 14, Iter 21, disc loss: 0.004890561143619396, policy loss: 7.229194195831499
Experience 14, Iter 22, disc loss: 0.003996927052605864, policy loss: 7.258133728201954
Experience 14, Iter 23, disc loss: 0.004570841444879325, policy loss: 6.792789318904975
Experience 14, Iter 24, disc loss: 0.0047424108825172544, policy loss: 6.640272050403545
Experience 14, Iter 25, disc loss: 0.004101422846551616, policy loss: 7.222304752149094
Experience 14, Iter 26, disc loss: 0.004140299309204367, policy loss: 6.983700861585849
Experience 14, Iter 27, disc loss: 0.004557459675618964, policy loss: 7.0686880211256105
Experience 14, Iter 28, disc loss: 0.004761893896587852, policy loss: 7.409735854587548
Experience 14, Iter 29, disc loss: 0.0045816642000948, policy loss: 6.778540218589492
Experience 14, Iter 30, disc loss: 0.004705078153953646, policy loss: 6.845635754522172
Experience 14, Iter 31, disc loss: 0.004136291106686757, policy loss: 7.568640164907523
Experience 14, Iter 32, disc loss: 0.004229620774491077, policy loss: 7.360998629915873
Experience 14, Iter 33, disc loss: 0.004458788611433609, policy loss: 7.140361258325083
Experience 14, Iter 34, disc loss: 0.004620391885767005, policy loss: 6.90411231737223
Experience 14, Iter 35, disc loss: 0.004697892416738978, policy loss: 7.509422091478081
Experience 14, Iter 36, disc loss: 0.004756040168199926, policy loss: 7.487863553643866
Experience 14, Iter 37, disc loss: 0.0041507246353975225, policy loss: 7.387535343073176
Experience 14, Iter 38, disc loss: 0.003981579090137802, policy loss: 7.42802037490673
Experience 14, Iter 39, disc loss: 0.004796361314667003, policy loss: 6.920014387925384
Experience 14, Iter 40, disc loss: 0.004306035606300451, policy loss: 6.853187930700529
Experience 14, Iter 41, disc loss: 0.004041033400962182, policy loss: 7.2632699345422935
Experience 14, Iter 42, disc loss: 0.0040426802642374365, policy loss: 7.247130278282723
Experience 14, Iter 43, disc loss: 0.004636529919967066, policy loss: 7.550903051313843
Experience 14, Iter 44, disc loss: 0.004396661558403849, policy loss: 6.819437872753161
Experience 14, Iter 45, disc loss: 0.003731167485746937, policy loss: 6.923029716941326
Experience 14, Iter 46, disc loss: 0.004032574559336309, policy loss: 7.402308232033498
Experience 14, Iter 47, disc loss: 0.004895086577465137, policy loss: 6.869107961894821
Experience 14, Iter 48, disc loss: 0.004242946872136976, policy loss: 7.2679947730340135
Experience 14, Iter 49, disc loss: 0.004389722267222956, policy loss: 6.7965344788417585
Experience 14, Iter 50, disc loss: 0.003829543442278608, policy loss: 7.178112415376454
Experience 14, Iter 51, disc loss: 0.0038305891553174058, policy loss: 7.485536271271398
Experience 14, Iter 52, disc loss: 0.004027952810463338, policy loss: 6.906795527315365
Experience 14, Iter 53, disc loss: 0.0038191717867660613, policy loss: 6.905397480221681
Experience 14, Iter 54, disc loss: 0.0038931600958272113, policy loss: 7.95022900303387
Experience 14, Iter 55, disc loss: 0.0038628333808492605, policy loss: 7.298994772663092
Experience 14, Iter 56, disc loss: 0.003657839741422327, policy loss: 7.531066237931751
Experience 14, Iter 57, disc loss: 0.0038493406795169307, policy loss: 7.514783954853334
Experience 14, Iter 58, disc loss: 0.003489332111503414, policy loss: 7.795538439077335
Experience 14, Iter 59, disc loss: 0.00376516115263784, policy loss: 7.345591322631117
Experience 14, Iter 60, disc loss: 0.0038415866422724, policy loss: 7.149286308670387
Experience 14, Iter 61, disc loss: 0.003627873731216606, policy loss: 7.563034500405754
Experience 14, Iter 62, disc loss: 0.00410054011403559, policy loss: 6.793352475229014
Experience 14, Iter 63, disc loss: 0.003667470311919021, policy loss: 7.130046169739229
Experience 14, Iter 64, disc loss: 0.00502053257658037, policy loss: 6.88751569025901
Experience 14, Iter 65, disc loss: 0.0041426053778810985, policy loss: 6.925574294839335
Experience 14, Iter 66, disc loss: 0.0041418351805997304, policy loss: 7.126531288408212
Experience 14, Iter 67, disc loss: 0.0037084961280243406, policy loss: 6.944049160829332
Experience 14, Iter 68, disc loss: 0.004316636003691782, policy loss: 6.8504475354915275
Experience 14, Iter 69, disc loss: 0.0038329493291071413, policy loss: 7.240179426143128
Experience 14, Iter 70, disc loss: 0.0036129085512550886, policy loss: 7.414732026487216
Experience 14, Iter 71, disc loss: 0.0037129241876410314, policy loss: 7.055079428098148
Experience 14, Iter 72, disc loss: 0.003761899041395852, policy loss: 7.04224203295149
Experience 14, Iter 73, disc loss: 0.0035253078917577194, policy loss: 7.427269781004789
Experience 14, Iter 74, disc loss: 0.004159374889089612, policy loss: 6.975788520597235
Experience 14, Iter 75, disc loss: 0.004147325852849097, policy loss: 6.94715823987918
Experience 14, Iter 76, disc loss: 0.003756382591445953, policy loss: 7.068501488545967
Experience 14, Iter 77, disc loss: 0.003522747306887927, policy loss: 7.201413043632867
Experience 14, Iter 78, disc loss: 0.004346797549286564, policy loss: 6.995372002767384
Experience 14, Iter 79, disc loss: 0.0038325939413975355, policy loss: 7.090227298969063
Experience 14, Iter 80, disc loss: 0.004083823847290425, policy loss: 7.163205536565856
Experience 14, Iter 81, disc loss: 0.0033724466496395716, policy loss: 7.39558856015518
Experience 14, Iter 82, disc loss: 0.004227687921784484, policy loss: 7.436794428935974
Experience 14, Iter 83, disc loss: 0.003785912693903621, policy loss: 7.2541374654948605
Experience 14, Iter 84, disc loss: 0.004455484417230957, policy loss: 6.844153265668668
Experience 14, Iter 85, disc loss: 0.004105542626006979, policy loss: 7.112701221119369
Experience 14, Iter 86, disc loss: 0.0033400043324288476, policy loss: 7.0724109629001
Experience 14, Iter 87, disc loss: 0.004315553198921877, policy loss: 7.475296106618634
Experience 14, Iter 88, disc loss: 0.003530845711327027, policy loss: 7.638143472921804
Experience 14, Iter 89, disc loss: 0.0039679840685112894, policy loss: 7.037734884053069
Experience 14, Iter 90, disc loss: 0.0039031463701077176, policy loss: 7.204685277356033
Experience 14, Iter 91, disc loss: 0.004021314874649663, policy loss: 7.359330770581693
Experience 14, Iter 92, disc loss: 0.003101571028751846, policy loss: 7.6761395729105075
Experience 14, Iter 93, disc loss: 0.003940150720896374, policy loss: 7.053401604679082
Experience 14, Iter 94, disc loss: 0.004017847929119469, policy loss: 7.396938863371776
Experience 14, Iter 95, disc loss: 0.0035631799753276052, policy loss: 7.14637318378389
Experience 14, Iter 96, disc loss: 0.004028096384252621, policy loss: 6.951373735263642
Experience 14, Iter 97, disc loss: 0.0039349743456760875, policy loss: 7.231805603132248
Experience 14, Iter 98, disc loss: 0.003359129270426182, policy loss: 7.487580072483469
Experience 14, Iter 99, disc loss: 0.00400801439586135, policy loss: 6.834139596282078
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0068],
        [0.1371],
        [1.7462],
        [0.0275]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0144, 0.2502, 1.2569, 0.0239, 0.0208, 3.8007]],

        [[0.0144, 0.2502, 1.2569, 0.0239, 0.0208, 3.8007]],

        [[0.0144, 0.2502, 1.2569, 0.0239, 0.0208, 3.8007]],

        [[0.0144, 0.2502, 1.2569, 0.0239, 0.0208, 3.8007]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0272, 0.5484, 6.9848, 0.1099], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0272, 0.5484, 6.9848, 0.1099])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.502
Iter 2/2000 - Loss: 3.426
Iter 3/2000 - Loss: 3.340
Iter 4/2000 - Loss: 3.288
Iter 5/2000 - Loss: 3.253
Iter 6/2000 - Loss: 3.174
Iter 7/2000 - Loss: 3.087
Iter 8/2000 - Loss: 3.003
Iter 9/2000 - Loss: 2.899
Iter 10/2000 - Loss: 2.765
Iter 11/2000 - Loss: 2.617
Iter 12/2000 - Loss: 2.460
Iter 13/2000 - Loss: 2.289
Iter 14/2000 - Loss: 2.089
Iter 15/2000 - Loss: 1.857
Iter 16/2000 - Loss: 1.597
Iter 17/2000 - Loss: 1.317
Iter 18/2000 - Loss: 1.021
Iter 19/2000 - Loss: 0.710
Iter 20/2000 - Loss: 0.386
Iter 1981/2000 - Loss: -7.852
Iter 1982/2000 - Loss: -7.852
Iter 1983/2000 - Loss: -7.852
Iter 1984/2000 - Loss: -7.852
Iter 1985/2000 - Loss: -7.852
Iter 1986/2000 - Loss: -7.852
Iter 1987/2000 - Loss: -7.852
Iter 1988/2000 - Loss: -7.852
Iter 1989/2000 - Loss: -7.852
Iter 1990/2000 - Loss: -7.852
Iter 1991/2000 - Loss: -7.852
Iter 1992/2000 - Loss: -7.852
Iter 1993/2000 - Loss: -7.852
Iter 1994/2000 - Loss: -7.852
Iter 1995/2000 - Loss: -7.852
Iter 1996/2000 - Loss: -7.852
Iter 1997/2000 - Loss: -7.852
Iter 1998/2000 - Loss: -7.852
Iter 1999/2000 - Loss: -7.852
Iter 2000/2000 - Loss: -7.852
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.4855,  8.5427, 23.2493,  4.6763,  6.8343, 58.7531]],

        [[16.3692, 35.9215,  9.7041,  1.1158,  2.1714, 23.7118]],

        [[18.3086, 36.1980,  8.5334,  1.1186,  1.2006, 18.6561]],

        [[15.0561, 32.8400, 18.3044,  2.4787,  1.8750, 45.8265]]])
Signal Variance: tensor([ 0.1222,  1.8454, 12.2664,  0.5851])
Estimated target variance: tensor([0.0272, 0.5484, 6.9848, 0.1099])
N: 150
Signal to noise ratio: tensor([19.7642, 76.6610, 80.8894, 51.2126])
Bound on condition number: tensor([ 58594.7423, 881536.5668, 981465.9965, 393410.0050])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.004020207737844759, policy loss: 7.428129121313415
Experience 15, Iter 1, disc loss: 0.0032328088808833795, policy loss: 7.528610682253484
Experience 15, Iter 2, disc loss: 0.003574802851303065, policy loss: 7.036874436089823
Experience 15, Iter 3, disc loss: 0.003659618158210121, policy loss: 7.044109817845025
Experience 15, Iter 4, disc loss: 0.003892512035197402, policy loss: 7.564020396688674
Experience 15, Iter 5, disc loss: 0.0036191550525146794, policy loss: 7.28412008028733
Experience 15, Iter 6, disc loss: 0.0040455216327260485, policy loss: 7.514573537051375
Experience 15, Iter 7, disc loss: 0.004008780552572612, policy loss: 6.907107145322417
Experience 15, Iter 8, disc loss: 0.0036281051239006206, policy loss: 6.910056099042033
Experience 15, Iter 9, disc loss: 0.003007255429724625, policy loss: 7.5310201654838105
Experience 15, Iter 10, disc loss: 0.0030862961298506473, policy loss: 7.62633965566344
Experience 15, Iter 11, disc loss: 0.0034795968192690633, policy loss: 7.237629340857125
Experience 15, Iter 12, disc loss: 0.004193471928851087, policy loss: 6.857885900627883
Experience 15, Iter 13, disc loss: 0.0037755669238457136, policy loss: 7.39922710743014
Experience 15, Iter 14, disc loss: 0.0036264009020654476, policy loss: 7.11634963202213
Experience 15, Iter 15, disc loss: 0.00358372579281874, policy loss: 7.111824622416272
Experience 15, Iter 16, disc loss: 0.0035506618271683946, policy loss: 7.392204812318006
Experience 15, Iter 17, disc loss: 0.0036443752622373243, policy loss: 7.951268386195785
Experience 15, Iter 18, disc loss: 0.0038341892137768703, policy loss: 7.068730185691081
Experience 15, Iter 19, disc loss: 0.003174834490006676, policy loss: 7.928983513548551
Experience 15, Iter 20, disc loss: 0.003824845509790278, policy loss: 7.426360067379935
Experience 15, Iter 21, disc loss: 0.0033661325180908697, policy loss: 7.3456580263702245
Experience 15, Iter 22, disc loss: 0.0038769862865811605, policy loss: 6.924148106578792
Experience 15, Iter 23, disc loss: 0.0038855681877519306, policy loss: 7.2692118595771795
Experience 15, Iter 24, disc loss: 0.003279084910917753, policy loss: 7.297374111143348
Experience 15, Iter 25, disc loss: 0.0029923861847274405, policy loss: 7.875260576626118
Experience 15, Iter 26, disc loss: 0.003174998708565198, policy loss: 7.619038132714483
Experience 15, Iter 27, disc loss: 0.0037028254251946177, policy loss: 7.216069422208422
Experience 15, Iter 28, disc loss: 0.0033915146333509914, policy loss: 7.326984152215205
Experience 15, Iter 29, disc loss: 0.0036554099725309708, policy loss: 7.517153247714212
Experience 15, Iter 30, disc loss: 0.0032397471343440037, policy loss: 7.794837309910662
Experience 15, Iter 31, disc loss: 0.004013966653610296, policy loss: 7.64607282507604
Experience 15, Iter 32, disc loss: 0.0029247607888729177, policy loss: 8.197267175454
Experience 15, Iter 33, disc loss: 0.0035315848902203327, policy loss: 7.152048549857833
Experience 15, Iter 34, disc loss: 0.0031670790400476056, policy loss: 7.86227391042799
Experience 15, Iter 35, disc loss: 0.002802465242698875, policy loss: 8.38082786300723
Experience 15, Iter 36, disc loss: 0.0028643562922546382, policy loss: 7.987303314757258
Experience 15, Iter 37, disc loss: 0.0024994938059910055, policy loss: 7.9868491585989
Experience 15, Iter 38, disc loss: 0.002661468501836214, policy loss: 8.528374432582186
Experience 15, Iter 39, disc loss: 0.002540374620923944, policy loss: 8.703448430466956
Experience 15, Iter 40, disc loss: 0.0022677118654142294, policy loss: 8.584911279612278
Experience 15, Iter 41, disc loss: 0.0025374557947718795, policy loss: 8.021130006137895
Experience 15, Iter 42, disc loss: 0.0024236044534841662, policy loss: 8.18338782936383
Experience 15, Iter 43, disc loss: 0.002795458415810862, policy loss: 7.855833681322062
Experience 15, Iter 44, disc loss: 0.0025988340317464106, policy loss: 7.538152187598235
Experience 15, Iter 45, disc loss: 0.003047652547213098, policy loss: 7.670892438033299
Experience 15, Iter 46, disc loss: 0.002917847681764921, policy loss: 7.5333746130121115
Experience 15, Iter 47, disc loss: 0.0031420517337381266, policy loss: 7.613154372966157
Experience 15, Iter 48, disc loss: 0.0032534986419211886, policy loss: 7.384626945464524
Experience 15, Iter 49, disc loss: 0.0032489249432276464, policy loss: 7.8254890746812595
Experience 15, Iter 50, disc loss: 0.0030877728732495377, policy loss: 7.411670329209734
Experience 15, Iter 51, disc loss: 0.0027474692529916734, policy loss: 7.584868128242942
Experience 15, Iter 52, disc loss: 0.0028296876395177096, policy loss: 7.50914817147752
Experience 15, Iter 53, disc loss: 0.00376477408891749, policy loss: 7.009644914822295
Experience 15, Iter 54, disc loss: 0.0028144875201140335, policy loss: 7.670558117677147
Experience 15, Iter 55, disc loss: 0.0028549113664976733, policy loss: 7.787401391290297
Experience 15, Iter 56, disc loss: 0.002931052841384571, policy loss: 7.342604631103335
Experience 15, Iter 57, disc loss: 0.0030879512089918484, policy loss: 7.573045808974691
Experience 15, Iter 58, disc loss: 0.0027244822079176365, policy loss: 7.400982371934461
Experience 15, Iter 59, disc loss: 0.0024193118997868357, policy loss: 7.409264652294175
Experience 15, Iter 60, disc loss: 0.0027596894575799293, policy loss: 7.470391237296715
Experience 15, Iter 61, disc loss: 0.0024559928524904824, policy loss: 7.757417593919952
Experience 15, Iter 62, disc loss: 0.002742494376218786, policy loss: 7.537387489380325
Experience 15, Iter 63, disc loss: 0.0027380322018356823, policy loss: 8.00157177499797
Experience 15, Iter 64, disc loss: 0.0024877774617315046, policy loss: 7.742097306617733
Experience 15, Iter 65, disc loss: 0.0036107575612639763, policy loss: 7.095632891473878
Experience 15, Iter 66, disc loss: 0.002984128940560942, policy loss: 7.277545630548103
Experience 15, Iter 67, disc loss: 0.003180299529323714, policy loss: 7.575810333985633
Experience 15, Iter 68, disc loss: 0.0024258107176112747, policy loss: 7.96038687545506
Experience 15, Iter 69, disc loss: 0.003036648009884432, policy loss: 7.648071737558556
Experience 15, Iter 70, disc loss: 0.003027851849287295, policy loss: 7.500690089085822
Experience 15, Iter 71, disc loss: 0.0032914973503989503, policy loss: 7.359099227736691
Experience 15, Iter 72, disc loss: 0.0031221215140095587, policy loss: 7.222973627632907
Experience 15, Iter 73, disc loss: 0.0030828650622494276, policy loss: 7.163414105099421
Experience 15, Iter 74, disc loss: 0.002774787262069531, policy loss: 7.438510531706477
Experience 15, Iter 75, disc loss: 0.003398164468428555, policy loss: 7.343670842172733
Experience 15, Iter 76, disc loss: 0.0026704010481783365, policy loss: 7.28841445220265
Experience 15, Iter 77, disc loss: 0.002947739001880221, policy loss: 7.2333438088032285
Experience 15, Iter 78, disc loss: 0.0035849725431262303, policy loss: 6.823358195761385
Experience 15, Iter 79, disc loss: 0.002770633243416449, policy loss: 8.449621483834143
Experience 15, Iter 80, disc loss: 0.003184191217682446, policy loss: 7.462398959926164
Experience 15, Iter 81, disc loss: 0.002936930734523998, policy loss: 7.719954810759979
Experience 15, Iter 82, disc loss: 0.0025215504859640485, policy loss: 7.924661381224544
Experience 15, Iter 83, disc loss: 0.003006779423919885, policy loss: 7.6613030011290935
Experience 15, Iter 84, disc loss: 0.0028219127786939923, policy loss: 7.476592600024599
Experience 15, Iter 85, disc loss: 0.0031781966226052845, policy loss: 7.439639226518221
Experience 15, Iter 86, disc loss: 0.0027031216483141504, policy loss: 7.603132426606155
Experience 15, Iter 87, disc loss: 0.003033850818682819, policy loss: 7.34924002025697
Experience 15, Iter 88, disc loss: 0.0029270058260836224, policy loss: 7.285369689828423
Experience 15, Iter 89, disc loss: 0.002920860488381565, policy loss: 7.715690296092056
Experience 15, Iter 90, disc loss: 0.0030573209564238677, policy loss: 7.283141828023003
Experience 15, Iter 91, disc loss: 0.002952085067576456, policy loss: 8.272184865684059
Experience 15, Iter 92, disc loss: 0.0031080680556479353, policy loss: 7.370057835581756
Experience 15, Iter 93, disc loss: 0.0029986186804250962, policy loss: 7.3887832882278595
Experience 15, Iter 94, disc loss: 0.002488451497098639, policy loss: 8.182717470035058
Experience 15, Iter 95, disc loss: 0.0032260034183154212, policy loss: 7.344652611004009
Experience 15, Iter 96, disc loss: 0.0027531876482965595, policy loss: 7.796571536314065
Experience 15, Iter 97, disc loss: 0.0026796699537410783, policy loss: 7.60480227510748
Experience 15, Iter 98, disc loss: 0.0024629846009682635, policy loss: 8.088061532028162
Experience 15, Iter 99, disc loss: 0.0028605603851553744, policy loss: 7.334637334131937
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0070],
        [0.1454],
        [1.8663],
        [0.0291]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0140, 0.2561, 1.3354, 0.0250, 0.0213, 3.9370]],

        [[0.0140, 0.2561, 1.3354, 0.0250, 0.0213, 3.9370]],

        [[0.0140, 0.2561, 1.3354, 0.0250, 0.0213, 3.9370]],

        [[0.0140, 0.2561, 1.3354, 0.0250, 0.0213, 3.9370]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0279, 0.5817, 7.4651, 0.1165], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0279, 0.5817, 7.4651, 0.1165])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.613
Iter 2/2000 - Loss: 3.519
Iter 3/2000 - Loss: 3.440
Iter 4/2000 - Loss: 3.380
Iter 5/2000 - Loss: 3.341
Iter 6/2000 - Loss: 3.264
Iter 7/2000 - Loss: 3.173
Iter 8/2000 - Loss: 3.083
Iter 9/2000 - Loss: 2.975
Iter 10/2000 - Loss: 2.834
Iter 11/2000 - Loss: 2.674
Iter 12/2000 - Loss: 2.504
Iter 13/2000 - Loss: 2.319
Iter 14/2000 - Loss: 2.106
Iter 15/2000 - Loss: 1.862
Iter 16/2000 - Loss: 1.591
Iter 17/2000 - Loss: 1.299
Iter 18/2000 - Loss: 0.992
Iter 19/2000 - Loss: 0.671
Iter 20/2000 - Loss: 0.337
Iter 1981/2000 - Loss: -7.906
Iter 1982/2000 - Loss: -7.906
Iter 1983/2000 - Loss: -7.906
Iter 1984/2000 - Loss: -7.906
Iter 1985/2000 - Loss: -7.906
Iter 1986/2000 - Loss: -7.906
Iter 1987/2000 - Loss: -7.906
Iter 1988/2000 - Loss: -7.906
Iter 1989/2000 - Loss: -7.907
Iter 1990/2000 - Loss: -7.907
Iter 1991/2000 - Loss: -7.907
Iter 1992/2000 - Loss: -7.907
Iter 1993/2000 - Loss: -7.907
Iter 1994/2000 - Loss: -7.907
Iter 1995/2000 - Loss: -7.907
Iter 1996/2000 - Loss: -7.907
Iter 1997/2000 - Loss: -7.907
Iter 1998/2000 - Loss: -7.907
Iter 1999/2000 - Loss: -7.907
Iter 2000/2000 - Loss: -7.907
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[10.9125,  8.4164, 20.2398,  5.1881,  7.4189, 58.3151]],

        [[15.5729, 35.3524,  9.2563,  1.2455,  1.2903, 24.4084]],

        [[18.0035, 35.6947,  8.1900,  1.1912,  1.2160, 18.4958]],

        [[14.6821, 32.4856, 18.7637,  2.5854,  2.0420, 46.9212]]])
Signal Variance: tensor([ 0.1157,  1.7094, 11.9364,  0.6175])
Estimated target variance: tensor([0.0279, 0.5817, 7.4651, 0.1165])
N: 160
Signal to noise ratio: tensor([19.3656, 76.0294, 77.8663, 51.5277])
Bound on condition number: tensor([ 60005.0330, 924876.3556, 970107.3886, 424817.0218])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0027937030409562204, policy loss: 7.430242682444251
Experience 16, Iter 1, disc loss: 0.0028688098481739754, policy loss: 7.388330790751484
Experience 16, Iter 2, disc loss: 0.0025723517518298974, policy loss: 7.603927147733094
Experience 16, Iter 3, disc loss: 0.0030156924833529453, policy loss: 7.288249203501019
Experience 16, Iter 4, disc loss: 0.0027080258488327725, policy loss: 7.732299581577136
Experience 16, Iter 5, disc loss: 0.0026745289357010963, policy loss: 7.583312221127837
Experience 16, Iter 6, disc loss: 0.002894839867979779, policy loss: 7.348508447443582
Experience 16, Iter 7, disc loss: 0.0029964740461883785, policy loss: 7.135783349182101
Experience 16, Iter 8, disc loss: 0.0028656566388209387, policy loss: 7.42007677328442
Experience 16, Iter 9, disc loss: 0.002906184036353757, policy loss: 7.477897971690152
Experience 16, Iter 10, disc loss: 0.0035086989181505115, policy loss: 7.144898525332947
Experience 16, Iter 11, disc loss: 0.002668653978302198, policy loss: 8.03154441623043
Experience 16, Iter 12, disc loss: 0.0034270652146415562, policy loss: 7.021868243412429
Experience 16, Iter 13, disc loss: 0.0027565301237969467, policy loss: 7.673500517233439
Experience 16, Iter 14, disc loss: 0.003574244714078535, policy loss: 7.811741574359677
Experience 16, Iter 15, disc loss: 0.003995234076879169, policy loss: 6.793541746972316
Experience 16, Iter 16, disc loss: 0.0033116923440248093, policy loss: 7.607545616690718
Experience 16, Iter 17, disc loss: 0.002696733201134011, policy loss: 8.006041546792924
Experience 16, Iter 18, disc loss: 0.0030609228781750914, policy loss: 8.12609348539274
Experience 16, Iter 19, disc loss: 0.003265867872004372, policy loss: 7.104186050215233
Experience 16, Iter 20, disc loss: 0.003056431114265588, policy loss: 7.461758872356956
Experience 16, Iter 21, disc loss: 0.0028520613361467033, policy loss: 7.624445286940732
Experience 16, Iter 22, disc loss: 0.003820093150487019, policy loss: 6.842353661371465
Experience 16, Iter 23, disc loss: 0.003138456871195004, policy loss: 7.570370120060083
Experience 16, Iter 24, disc loss: 0.003278663779864708, policy loss: 7.704209775095615
Experience 16, Iter 25, disc loss: 0.0026297234789475505, policy loss: 8.198076559859663
Experience 16, Iter 26, disc loss: 0.003065190869810426, policy loss: 7.451376686467954
Experience 16, Iter 27, disc loss: 0.0030225484504993053, policy loss: 8.072146274580362
Experience 16, Iter 28, disc loss: 0.0028454753405027103, policy loss: 7.678986537442951
Experience 16, Iter 29, disc loss: 0.0031178221280223992, policy loss: 7.53909031673091
Experience 16, Iter 30, disc loss: 0.0028651580275811227, policy loss: 7.625918437155779
Experience 16, Iter 31, disc loss: 0.0025990100692679027, policy loss: 7.631560057165307
Experience 16, Iter 32, disc loss: 0.0032190218117879007, policy loss: 7.164958656032835
Experience 16, Iter 33, disc loss: 0.00290831587842398, policy loss: 7.791442484939126
Experience 16, Iter 34, disc loss: 0.00303288619771622, policy loss: 7.51888934880386
Experience 16, Iter 35, disc loss: 0.002663370762960961, policy loss: 7.633099490745311
Experience 16, Iter 36, disc loss: 0.002953757730496096, policy loss: 7.690581027246485
Experience 16, Iter 37, disc loss: 0.002863711270642543, policy loss: 8.06000794451747
Experience 16, Iter 38, disc loss: 0.003253307546500847, policy loss: 7.5669824773999945
Experience 16, Iter 39, disc loss: 0.0027385010914653487, policy loss: 8.837242020624341
Experience 16, Iter 40, disc loss: 0.0027873858188627067, policy loss: 7.413659500216387
Experience 16, Iter 41, disc loss: 0.002643099342908242, policy loss: 7.829440218706513
Experience 16, Iter 42, disc loss: 0.0027690353558816275, policy loss: 7.526270298372587
Experience 16, Iter 43, disc loss: 0.0030496531446670876, policy loss: 7.190394011306678
Experience 16, Iter 44, disc loss: 0.002904604381453337, policy loss: 7.518182727185819
Experience 16, Iter 45, disc loss: 0.002918638446581512, policy loss: 7.5538339483555825
Experience 16, Iter 46, disc loss: 0.003327909470941168, policy loss: 7.873563807789656
Experience 16, Iter 47, disc loss: 0.002660061601958223, policy loss: 8.048585059125191
Experience 16, Iter 48, disc loss: 0.003035906963304813, policy loss: 7.624329454909392
Experience 16, Iter 49, disc loss: 0.003217349058435898, policy loss: 7.7038861670376075
Experience 16, Iter 50, disc loss: 0.0027351587315210127, policy loss: 8.770851053668064
Experience 16, Iter 51, disc loss: 0.003170270536202735, policy loss: 7.17795704517256
Experience 16, Iter 52, disc loss: 0.002574015077290365, policy loss: 7.5519498921154
Experience 16, Iter 53, disc loss: 0.002688552149940775, policy loss: 7.8358915261469475
Experience 16, Iter 54, disc loss: 0.003020292889096236, policy loss: 7.966496928606141
Experience 16, Iter 55, disc loss: 0.0026722700035543474, policy loss: 7.7617733012902805
Experience 16, Iter 56, disc loss: 0.002771092579268447, policy loss: 7.3138775761940815
Experience 16, Iter 57, disc loss: 0.002570900639614442, policy loss: 7.978188997416695
Experience 16, Iter 58, disc loss: 0.002415890398182504, policy loss: 8.231538014232367
Experience 16, Iter 59, disc loss: 0.0029565767992741515, policy loss: 7.885814873206703
Experience 16, Iter 60, disc loss: 0.0027988427406293037, policy loss: 7.603210705702038
Experience 16, Iter 61, disc loss: 0.0029214372218528987, policy loss: 7.593892611985688
Experience 16, Iter 62, disc loss: 0.003297558138701336, policy loss: 7.465083879651681
Experience 16, Iter 63, disc loss: 0.002827608062958996, policy loss: 7.476645503779424
Experience 16, Iter 64, disc loss: 0.00259981299051781, policy loss: 7.717346888059799
Experience 16, Iter 65, disc loss: 0.0025347977743293147, policy loss: 7.789525507633174
Experience 16, Iter 66, disc loss: 0.002910588766293732, policy loss: 7.374087867802469
Experience 16, Iter 67, disc loss: 0.0026316404991764087, policy loss: 7.671797164669101
Experience 16, Iter 68, disc loss: 0.0026595944219857696, policy loss: 7.946492402234224
Experience 16, Iter 69, disc loss: 0.0030203980016922888, policy loss: 7.212425407541955
Experience 16, Iter 70, disc loss: 0.002686599175705554, policy loss: 7.88209201974862
Experience 16, Iter 71, disc loss: 0.002513587449001162, policy loss: 7.811384126202665
Experience 16, Iter 72, disc loss: 0.0027337898581751376, policy loss: 7.736555900898498
Experience 16, Iter 73, disc loss: 0.0025424274916464305, policy loss: 7.493594686302361
Experience 16, Iter 74, disc loss: 0.002415532640870444, policy loss: 8.039578774091318
Experience 16, Iter 75, disc loss: 0.002272452719573829, policy loss: 7.824275331637778
Experience 16, Iter 76, disc loss: 0.002652921393163611, policy loss: 7.500262218761381
Experience 16, Iter 77, disc loss: 0.0025072363158373753, policy loss: 7.957548591102825
Experience 16, Iter 78, disc loss: 0.002500299295400362, policy loss: 7.560915424112366
Experience 16, Iter 79, disc loss: 0.002690573575190376, policy loss: 7.892982761695694
Experience 16, Iter 80, disc loss: 0.0025889334011278808, policy loss: 7.592403228291953
Experience 16, Iter 81, disc loss: 0.002191744425418973, policy loss: 8.455295848922677
Experience 16, Iter 82, disc loss: 0.0022771569857929137, policy loss: 7.990847406960848
Experience 16, Iter 83, disc loss: 0.0025117778959711417, policy loss: 8.005519007026649
Experience 16, Iter 84, disc loss: 0.002341907918173563, policy loss: 7.951957382432489
Experience 16, Iter 85, disc loss: 0.0027562429527296876, policy loss: 7.945365890225111
Experience 16, Iter 86, disc loss: 0.0026484397940989943, policy loss: 7.952604827607148
Experience 16, Iter 87, disc loss: 0.0026849426753212608, policy loss: 7.560979284489075
Experience 16, Iter 88, disc loss: 0.002337277255118256, policy loss: 8.128980589010418
Experience 16, Iter 89, disc loss: 0.002669819432455664, policy loss: 7.3669538505560475
Experience 16, Iter 90, disc loss: 0.002674484360443849, policy loss: 7.767216402600413
Experience 16, Iter 91, disc loss: 0.0024623177431171294, policy loss: 7.582897326442495
Experience 16, Iter 92, disc loss: 0.002707146416362329, policy loss: 7.557869073310958
Experience 16, Iter 93, disc loss: 0.002412665912919042, policy loss: 7.882055289883668
Experience 16, Iter 94, disc loss: 0.0025564671223672324, policy loss: 8.004869227787184
Experience 16, Iter 95, disc loss: 0.002413430755446127, policy loss: 7.759494310779859
Experience 16, Iter 96, disc loss: 0.0026528936827001862, policy loss: 7.495779223987528
Experience 16, Iter 97, disc loss: 0.0023894632268366607, policy loss: 7.849580685457765
Experience 16, Iter 98, disc loss: 0.0028355599908730517, policy loss: 7.7306590380180635
Experience 16, Iter 99, disc loss: 0.0023514802817104132, policy loss: 7.5742026696648175
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0069],
        [0.1543],
        [1.9636],
        [0.0298]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0135, 0.2554, 1.3777, 0.0259, 0.0211, 4.0608]],

        [[0.0135, 0.2554, 1.3777, 0.0259, 0.0211, 4.0608]],

        [[0.0135, 0.2554, 1.3777, 0.0259, 0.0211, 4.0608]],

        [[0.0135, 0.2554, 1.3777, 0.0259, 0.0211, 4.0608]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0277, 0.6170, 7.8544, 0.1192], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0277, 0.6170, 7.8544, 0.1192])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.680
Iter 2/2000 - Loss: 3.578
Iter 3/2000 - Loss: 3.501
Iter 4/2000 - Loss: 3.435
Iter 5/2000 - Loss: 3.390
Iter 6/2000 - Loss: 3.307
Iter 7/2000 - Loss: 3.204
Iter 8/2000 - Loss: 3.102
Iter 9/2000 - Loss: 2.981
Iter 10/2000 - Loss: 2.829
Iter 11/2000 - Loss: 2.656
Iter 12/2000 - Loss: 2.473
Iter 13/2000 - Loss: 2.278
Iter 14/2000 - Loss: 2.061
Iter 15/2000 - Loss: 1.816
Iter 16/2000 - Loss: 1.544
Iter 17/2000 - Loss: 1.253
Iter 18/2000 - Loss: 0.946
Iter 19/2000 - Loss: 0.626
Iter 20/2000 - Loss: 0.293
Iter 1981/2000 - Loss: -7.938
Iter 1982/2000 - Loss: -7.938
Iter 1983/2000 - Loss: -7.938
Iter 1984/2000 - Loss: -7.938
Iter 1985/2000 - Loss: -7.938
Iter 1986/2000 - Loss: -7.938
Iter 1987/2000 - Loss: -7.938
Iter 1988/2000 - Loss: -7.938
Iter 1989/2000 - Loss: -7.938
Iter 1990/2000 - Loss: -7.938
Iter 1991/2000 - Loss: -7.938
Iter 1992/2000 - Loss: -7.938
Iter 1993/2000 - Loss: -7.938
Iter 1994/2000 - Loss: -7.938
Iter 1995/2000 - Loss: -7.938
Iter 1996/2000 - Loss: -7.938
Iter 1997/2000 - Loss: -7.938
Iter 1998/2000 - Loss: -7.938
Iter 1999/2000 - Loss: -7.938
Iter 2000/2000 - Loss: -7.938
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[11.0280,  8.5106, 18.8341,  5.0256,  6.3848, 61.4343]],

        [[15.3045, 35.3419,  9.1823,  1.1800,  1.3927, 26.8660]],

        [[17.7425, 35.7048,  8.0766,  1.1356,  1.2557, 20.9564]],

        [[14.4705, 31.1614, 18.3042,  2.6778,  2.0593, 48.4878]]])
Signal Variance: tensor([ 0.1170,  1.8176, 12.7450,  0.5847])
Estimated target variance: tensor([0.0277, 0.6170, 7.8544, 0.1192])
N: 170
Signal to noise ratio: tensor([19.4892, 76.8788, 78.4996, 50.6042])
Bound on condition number: tensor([  64572.0171, 1004760.7647, 1047572.2188,  435334.9631])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.002286014756434109, policy loss: 7.782028061650215
Experience 17, Iter 1, disc loss: 0.002334966974600448, policy loss: 7.7023409848707995
Experience 17, Iter 2, disc loss: 0.0026391895525065064, policy loss: 7.5362405069613025
Experience 17, Iter 3, disc loss: 0.0026266974225427576, policy loss: 7.592920020110993
Experience 17, Iter 4, disc loss: 0.0025300435918452165, policy loss: 7.699175486839003
Experience 17, Iter 5, disc loss: 0.002127933362756555, policy loss: 7.903959643517489
Experience 17, Iter 6, disc loss: 0.002739323515903693, policy loss: 7.488745324887081
Experience 17, Iter 7, disc loss: 0.002366282090817814, policy loss: 8.558050367194397
Experience 17, Iter 8, disc loss: 0.0021204134637259385, policy loss: 8.320076330243067
Experience 17, Iter 9, disc loss: 0.0021785710825285133, policy loss: 7.774725985245824
Experience 17, Iter 10, disc loss: 0.0026804248889916197, policy loss: 7.388773280170664
Experience 17, Iter 11, disc loss: 0.002120270104327721, policy loss: 8.359241710786193
Experience 17, Iter 12, disc loss: 0.002411282786116925, policy loss: 7.849555317673997
Experience 17, Iter 13, disc loss: 0.002406557533936174, policy loss: 7.64601871454672
Experience 17, Iter 14, disc loss: 0.0026755568635122165, policy loss: 7.63911636050403
Experience 17, Iter 15, disc loss: 0.0021538445051315657, policy loss: 7.558103214921189
Experience 17, Iter 16, disc loss: 0.0023613035689208214, policy loss: 8.065724885173758
Experience 17, Iter 17, disc loss: 0.0027330365168363796, policy loss: 7.86792560455306
Experience 17, Iter 18, disc loss: 0.0022086176791616266, policy loss: 7.671469522292194
Experience 17, Iter 19, disc loss: 0.0025784111984892347, policy loss: 8.210062325943031
Experience 17, Iter 20, disc loss: 0.0023821599016980687, policy loss: 7.889265020467014
Experience 17, Iter 21, disc loss: 0.0028430873581442705, policy loss: 7.62660109244458
Experience 17, Iter 22, disc loss: 0.002532013560214218, policy loss: 7.8554735337178965
Experience 17, Iter 23, disc loss: 0.0025461191574495433, policy loss: 7.7831055351556415
Experience 17, Iter 24, disc loss: 0.0028055093838658244, policy loss: 7.6219219557446785
Experience 17, Iter 25, disc loss: 0.0022022936935035264, policy loss: 7.749603879367919
Experience 17, Iter 26, disc loss: 0.0023787496731895665, policy loss: 7.663947305144063
Experience 17, Iter 27, disc loss: 0.0026456032078633813, policy loss: 8.166829178250085
Experience 17, Iter 28, disc loss: 0.0022741768049802883, policy loss: 7.668509594234653
Experience 17, Iter 29, disc loss: 0.002609880415727889, policy loss: 7.33083951488108
Experience 17, Iter 30, disc loss: 0.0020598079872043004, policy loss: 7.859337065615683
Experience 17, Iter 31, disc loss: 0.0025916284588580604, policy loss: 7.7729322916550085
Experience 17, Iter 32, disc loss: 0.002626078838847819, policy loss: 7.695136645077842
Experience 17, Iter 33, disc loss: 0.0026089149586556457, policy loss: 7.760710765205216
Experience 17, Iter 34, disc loss: 0.0025096749595446524, policy loss: 8.154898034870907
Experience 17, Iter 35, disc loss: 0.002605158492372468, policy loss: 7.056735702864822
Experience 17, Iter 36, disc loss: 0.0028039104922088333, policy loss: 7.543171791337938
Experience 17, Iter 37, disc loss: 0.0028776259667877466, policy loss: 7.338626830781019
Experience 17, Iter 38, disc loss: 0.0024648219659628087, policy loss: 7.9080185385836845
Experience 17, Iter 39, disc loss: 0.00262356802889343, policy loss: 7.836892745753367
Experience 17, Iter 40, disc loss: 0.002646615079040905, policy loss: 7.694701997425208
Experience 17, Iter 41, disc loss: 0.002534172646574731, policy loss: 8.14176413828319
Experience 17, Iter 42, disc loss: 0.0030314405325003293, policy loss: 7.7780215960676395
Experience 17, Iter 43, disc loss: 0.002558056709477026, policy loss: 8.480678618987538
Experience 17, Iter 44, disc loss: 0.002650673299737049, policy loss: 7.635291360246447
Experience 17, Iter 45, disc loss: 0.002815140150308227, policy loss: 7.7340807999466685
Experience 17, Iter 46, disc loss: 0.002565888578930267, policy loss: 8.641564859275194
Experience 17, Iter 47, disc loss: 0.002988741127365597, policy loss: 7.31836541365621
Experience 17, Iter 48, disc loss: 0.0022767318394259035, policy loss: 8.64267908217344
Experience 17, Iter 49, disc loss: 0.0027786488478848495, policy loss: 7.370479506589152
Experience 17, Iter 50, disc loss: 0.0021882343850056687, policy loss: 8.094090159550886
Experience 17, Iter 51, disc loss: 0.0027633833166853853, policy loss: 7.707275264293959
Experience 17, Iter 52, disc loss: 0.0023130440726458184, policy loss: 8.106283660180939
Experience 17, Iter 53, disc loss: 0.0025104962515697443, policy loss: 7.836192158459867
Experience 17, Iter 54, disc loss: 0.002638746187262414, policy loss: 8.114349778776274
Experience 17, Iter 55, disc loss: 0.002138207814904905, policy loss: 8.38696132477642
Experience 17, Iter 56, disc loss: 0.0021130470465010455, policy loss: 8.019993899781916
Experience 17, Iter 57, disc loss: 0.0020151351802869377, policy loss: 7.868670755754115
Experience 17, Iter 58, disc loss: 0.002008968433023532, policy loss: 8.414528533012056
Experience 17, Iter 59, disc loss: 0.001984248969803872, policy loss: 8.077173390103987
Experience 17, Iter 60, disc loss: 0.0022506450314917267, policy loss: 7.83213459655092
Experience 17, Iter 61, disc loss: 0.001971225535460298, policy loss: 8.103924814123769
Experience 17, Iter 62, disc loss: 0.002133938802878058, policy loss: 8.501138859368105
Experience 17, Iter 63, disc loss: 0.0025237589245342487, policy loss: 7.653450900022734
Experience 17, Iter 64, disc loss: 0.0020415524301631233, policy loss: 8.413215480216522
Experience 17, Iter 65, disc loss: 0.002330355310488478, policy loss: 7.864524194832796
Experience 17, Iter 66, disc loss: 0.0021587800551248395, policy loss: 8.027595556098372
Experience 17, Iter 67, disc loss: 0.001959094932008308, policy loss: 8.345459038899074
Experience 17, Iter 68, disc loss: 0.002051886358071473, policy loss: 7.585198823285742
Experience 17, Iter 69, disc loss: 0.002874291027823216, policy loss: 7.987005894227098
Experience 17, Iter 70, disc loss: 0.0022185262928943947, policy loss: 7.642587912433369
Experience 17, Iter 71, disc loss: 0.002281308572947759, policy loss: 8.0027667826416
Experience 17, Iter 72, disc loss: 0.0022103174383305306, policy loss: 7.405178052290994
Experience 17, Iter 73, disc loss: 0.00214921527646913, policy loss: 8.48843333661869
Experience 17, Iter 74, disc loss: 0.002633018150113425, policy loss: 7.648019753794905
Experience 17, Iter 75, disc loss: 0.0026430497641900482, policy loss: 7.27706164526632
Experience 17, Iter 76, disc loss: 0.002749976650753183, policy loss: 7.539532804117607
Experience 17, Iter 77, disc loss: 0.0020909881123310887, policy loss: 7.789365141910711
Experience 17, Iter 78, disc loss: 0.002129933559353532, policy loss: 7.586142713697034
Experience 17, Iter 79, disc loss: 0.0017798121859896965, policy loss: 8.468740153394702
Experience 17, Iter 80, disc loss: 0.002193345190292949, policy loss: 7.951795698314676
Experience 17, Iter 81, disc loss: 0.002142428557072748, policy loss: 8.220386577772931
Experience 17, Iter 82, disc loss: 0.0019156157342888067, policy loss: 8.293720697458685
Experience 17, Iter 83, disc loss: 0.001787379762871823, policy loss: 8.4817407160871
Experience 17, Iter 84, disc loss: 0.0018249590205514326, policy loss: 8.006217056113144
Experience 17, Iter 85, disc loss: 0.001739363257774598, policy loss: 8.752567658038483
Experience 17, Iter 86, disc loss: 0.0017005825917580286, policy loss: 8.4441972969428
Experience 17, Iter 87, disc loss: 0.00183700830075895, policy loss: 8.343315312799131
Experience 17, Iter 88, disc loss: 0.0021119435656404935, policy loss: 8.500322323595201
Experience 17, Iter 89, disc loss: 0.0020306709693650752, policy loss: 7.818270820488852
Experience 17, Iter 90, disc loss: 0.0021851296860088904, policy loss: 7.994784839303982
Experience 17, Iter 91, disc loss: 0.00225515532733478, policy loss: 8.137507126232085
Experience 17, Iter 92, disc loss: 0.0016125191238047611, policy loss: 9.61248794073132
Experience 17, Iter 93, disc loss: 0.0020913004780636997, policy loss: 8.867113431311994
Experience 17, Iter 94, disc loss: 0.0022417310828734558, policy loss: 8.053920140266115
Experience 17, Iter 95, disc loss: 0.001672944531775617, policy loss: 8.72155296243939
Experience 17, Iter 96, disc loss: 0.0020572672365284698, policy loss: 7.814131478688986
Experience 17, Iter 97, disc loss: 0.0024170869710407406, policy loss: 7.583647517807309
Experience 17, Iter 98, disc loss: 0.0021855834067875733, policy loss: 7.606357523278811
Experience 17, Iter 99, disc loss: 0.0017685305728760941, policy loss: 8.545229999460638
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0067],
        [0.1636],
        [2.0371],
        [0.0300]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0132, 0.2504, 1.3949, 0.0267, 0.0204, 4.2182]],

        [[0.0132, 0.2504, 1.3949, 0.0267, 0.0204, 4.2182]],

        [[0.0132, 0.2504, 1.3949, 0.0267, 0.0204, 4.2182]],

        [[0.0132, 0.2504, 1.3949, 0.0267, 0.0204, 4.2182]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0270, 0.6542, 8.1483, 0.1200], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0270, 0.6542, 8.1483, 0.1200])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.719
Iter 2/2000 - Loss: 3.619
Iter 3/2000 - Loss: 3.539
Iter 4/2000 - Loss: 3.476
Iter 5/2000 - Loss: 3.432
Iter 6/2000 - Loss: 3.347
Iter 7/2000 - Loss: 3.244
Iter 8/2000 - Loss: 3.144
Iter 9/2000 - Loss: 3.025
Iter 10/2000 - Loss: 2.873
Iter 11/2000 - Loss: 2.701
Iter 12/2000 - Loss: 2.518
Iter 13/2000 - Loss: 2.324
Iter 14/2000 - Loss: 2.108
Iter 15/2000 - Loss: 1.866
Iter 16/2000 - Loss: 1.598
Iter 17/2000 - Loss: 1.312
Iter 18/2000 - Loss: 1.012
Iter 19/2000 - Loss: 0.699
Iter 20/2000 - Loss: 0.375
Iter 1981/2000 - Loss: -7.972
Iter 1982/2000 - Loss: -7.972
Iter 1983/2000 - Loss: -7.972
Iter 1984/2000 - Loss: -7.972
Iter 1985/2000 - Loss: -7.972
Iter 1986/2000 - Loss: -7.972
Iter 1987/2000 - Loss: -7.972
Iter 1988/2000 - Loss: -7.972
Iter 1989/2000 - Loss: -7.972
Iter 1990/2000 - Loss: -7.972
Iter 1991/2000 - Loss: -7.972
Iter 1992/2000 - Loss: -7.972
Iter 1993/2000 - Loss: -7.972
Iter 1994/2000 - Loss: -7.972
Iter 1995/2000 - Loss: -7.972
Iter 1996/2000 - Loss: -7.972
Iter 1997/2000 - Loss: -7.972
Iter 1998/2000 - Loss: -7.972
Iter 1999/2000 - Loss: -7.972
Iter 2000/2000 - Loss: -7.972
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[10.8189,  8.5633, 20.0285,  5.1021,  6.5882, 61.6877]],

        [[16.9796, 35.0333,  8.0127,  1.4143,  1.7037, 30.5593]],

        [[18.3131, 35.6299,  8.2693,  1.1443,  1.2325, 22.0314]],

        [[14.2412, 31.7847, 17.2056,  2.6517,  1.9522, 49.9090]]])
Signal Variance: tensor([ 0.1126,  2.0749, 13.0142,  0.5579])
Estimated target variance: tensor([0.0270, 0.6542, 8.1483, 0.1200])
N: 180
Signal to noise ratio: tensor([19.0940, 80.6469, 78.0573, 49.7242])
Bound on condition number: tensor([  65625.5230, 1170708.0785, 1096731.7267,  445049.7708])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.002552524454167061, policy loss: 7.792868422183391
Experience 18, Iter 1, disc loss: 0.002123292395408373, policy loss: 7.804057050846742
Experience 18, Iter 2, disc loss: 0.0024607970970201426, policy loss: 7.711314097342045
Experience 18, Iter 3, disc loss: 0.001796388424773278, policy loss: 7.993654735800655
Experience 18, Iter 4, disc loss: 0.0021670075714005713, policy loss: 7.813104728732874
Experience 18, Iter 5, disc loss: 0.0018628268983865409, policy loss: 8.054560979878566
Experience 18, Iter 6, disc loss: 0.0020191289071353233, policy loss: 8.346999466725325
Experience 18, Iter 7, disc loss: 0.002325446688041646, policy loss: 8.15225779342953
Experience 18, Iter 8, disc loss: 0.0020637756765403098, policy loss: 7.932389193486294
Experience 18, Iter 9, disc loss: 0.002279339732176889, policy loss: 8.040870726140339
Experience 18, Iter 10, disc loss: 0.0022018204022315564, policy loss: 7.845317701849278
Experience 18, Iter 11, disc loss: 0.0022648036514415493, policy loss: 7.743125386339528
Experience 18, Iter 12, disc loss: 0.0021318824223774263, policy loss: 7.857028738130726
Experience 18, Iter 13, disc loss: 0.002108061006218727, policy loss: 8.066866645926037
Experience 18, Iter 14, disc loss: 0.002183807644873868, policy loss: 7.72733073404238
Experience 18, Iter 15, disc loss: 0.0022739618532681676, policy loss: 8.265224304556643
Experience 18, Iter 16, disc loss: 0.002396826835837109, policy loss: 7.848783337852123
Experience 18, Iter 17, disc loss: 0.0019004823827395727, policy loss: 8.11428651183835
Experience 18, Iter 18, disc loss: 0.002027739177513448, policy loss: 7.729981583101536
Experience 18, Iter 19, disc loss: 0.0017925919525708643, policy loss: 8.916284693483231
Experience 18, Iter 20, disc loss: 0.0023718471449515974, policy loss: 7.569393986285926
Experience 18, Iter 21, disc loss: 0.001883996889278683, policy loss: 8.120631395327575
Experience 18, Iter 22, disc loss: 0.001989704178837199, policy loss: 8.199294287170588
Experience 18, Iter 23, disc loss: 0.002222420389327573, policy loss: 7.911771703667001
Experience 18, Iter 24, disc loss: 0.0019190050668300247, policy loss: 8.434987962243678
Experience 18, Iter 25, disc loss: 0.002138704539600454, policy loss: 7.643345901668532
Experience 18, Iter 26, disc loss: 0.0023790268894461047, policy loss: 7.913914818785781
Experience 18, Iter 27, disc loss: 0.0021862359106952604, policy loss: 7.976405679920399
Experience 18, Iter 28, disc loss: 0.0015741765823368212, policy loss: 8.500450128418219
Experience 18, Iter 29, disc loss: 0.0018458997711469944, policy loss: 7.998048344454878
Experience 18, Iter 30, disc loss: 0.002110852737256256, policy loss: 8.371768402004635
Experience 18, Iter 31, disc loss: 0.0018986704834827754, policy loss: 8.774111529839809
Experience 18, Iter 32, disc loss: 0.0020219551966234196, policy loss: 7.848223400889205
Experience 18, Iter 33, disc loss: 0.001835512934178963, policy loss: 8.432928814646568
Experience 18, Iter 34, disc loss: 0.0015996660478448804, policy loss: 8.14656330673927
Experience 18, Iter 35, disc loss: 0.0015753509720832374, policy loss: 8.293087006649564
Experience 18, Iter 36, disc loss: 0.001795048482149416, policy loss: 8.49025669270868
Experience 18, Iter 37, disc loss: 0.0018793422433407412, policy loss: 8.606331858950028
Experience 18, Iter 38, disc loss: 0.0016894430727135397, policy loss: 8.605058103899047
Experience 18, Iter 39, disc loss: 0.0017581264546862028, policy loss: 8.292973819590388
Experience 18, Iter 40, disc loss: 0.0016575973772329428, policy loss: 8.014786521569123
Experience 18, Iter 41, disc loss: 0.0016862371455846617, policy loss: 8.513297067875257
Experience 18, Iter 42, disc loss: 0.0019008744232041613, policy loss: 7.919339653632218
Experience 18, Iter 43, disc loss: 0.001835003232855553, policy loss: 8.332640747096635
Experience 18, Iter 44, disc loss: 0.0020854912189216127, policy loss: 7.69244480744989
Experience 18, Iter 45, disc loss: 0.001724354832493453, policy loss: 8.176443465519867
Experience 18, Iter 46, disc loss: 0.0019071333699666824, policy loss: 8.16151325883345
Experience 18, Iter 47, disc loss: 0.0019157386282468987, policy loss: 7.613587656721797
Experience 18, Iter 48, disc loss: 0.0018983739964761, policy loss: 8.78625324890291
Experience 18, Iter 49, disc loss: 0.0018789198453162468, policy loss: 8.204689184954253
Experience 18, Iter 50, disc loss: 0.0021608135755867477, policy loss: 8.10099051798016
Experience 18, Iter 51, disc loss: 0.0016322642434902837, policy loss: 8.747494344900021
Experience 18, Iter 52, disc loss: 0.00198268089683288, policy loss: 7.647836633580069
Experience 18, Iter 53, disc loss: 0.001746058988525525, policy loss: 8.055493307649826
Experience 18, Iter 54, disc loss: 0.0019035510798591465, policy loss: 7.653371159378311
Experience 18, Iter 55, disc loss: 0.0019233232727145257, policy loss: 8.26364160703038
Experience 18, Iter 56, disc loss: 0.001902013000922733, policy loss: 7.889267627270569
Experience 18, Iter 57, disc loss: 0.0019430341648212217, policy loss: 8.290767098648839
Experience 18, Iter 58, disc loss: 0.0021288171041431905, policy loss: 7.9351193059609235
Experience 18, Iter 59, disc loss: 0.0019706155949942078, policy loss: 7.816886253686539
Experience 18, Iter 60, disc loss: 0.002053161863245486, policy loss: 8.008070650587001
Experience 18, Iter 61, disc loss: 0.0017968179261522359, policy loss: 8.09646973653485
Experience 18, Iter 62, disc loss: 0.002298052484678996, policy loss: 8.012368545768368
Experience 18, Iter 63, disc loss: 0.002108552293874645, policy loss: 7.786316135510164
Experience 18, Iter 64, disc loss: 0.0018869784364381444, policy loss: 8.19875365327345
Experience 18, Iter 65, disc loss: 0.0019987841458587803, policy loss: 8.052663578155354
Experience 18, Iter 66, disc loss: 0.0021408243017206167, policy loss: 7.770077013476703
Experience 18, Iter 67, disc loss: 0.00190193656737802, policy loss: 8.384993976684694
Experience 18, Iter 68, disc loss: 0.0018164817498676996, policy loss: 8.250993840666485
Experience 18, Iter 69, disc loss: 0.0015952409382226313, policy loss: 8.069526496099629
Experience 18, Iter 70, disc loss: 0.0015042320131898354, policy loss: 9.924379469012013
Experience 18, Iter 71, disc loss: 0.002126252704004296, policy loss: 8.501979525516123
Experience 18, Iter 72, disc loss: 0.0018248147553088785, policy loss: 8.836504351394463
Experience 18, Iter 73, disc loss: 0.0015828831293725541, policy loss: 8.933099845325355
Experience 18, Iter 74, disc loss: 0.001848039404495226, policy loss: 8.020273643218676
Experience 18, Iter 75, disc loss: 0.001470809081238829, policy loss: 8.999162737133837
Experience 18, Iter 76, disc loss: 0.0015575717578838618, policy loss: 8.178141588259352
Experience 18, Iter 77, disc loss: 0.0015765577020265735, policy loss: 8.099069542269913
Experience 18, Iter 78, disc loss: 0.0013846398057380783, policy loss: 8.556197003851825
Experience 18, Iter 79, disc loss: 0.0014004722272356312, policy loss: 8.367344591452657
Experience 18, Iter 80, disc loss: 0.0014946331812931586, policy loss: 8.261489749787838
Experience 18, Iter 81, disc loss: 0.001523977692891849, policy loss: 8.441877537439655
Experience 18, Iter 82, disc loss: 0.0014728680919523676, policy loss: 8.229686452132775
Experience 18, Iter 83, disc loss: 0.0014607908595645023, policy loss: 8.426466326074848
Experience 18, Iter 84, disc loss: 0.0018919290321381655, policy loss: 8.3706238601627
Experience 18, Iter 85, disc loss: 0.002028949718784052, policy loss: 8.27361137573094
Experience 18, Iter 86, disc loss: 0.0016633400463499323, policy loss: 8.364626481391525
Experience 18, Iter 87, disc loss: 0.001552169944718957, policy loss: 8.120329398082054
Experience 18, Iter 88, disc loss: 0.0015554125119164098, policy loss: 9.092815991111431
Experience 18, Iter 89, disc loss: 0.001671860936337305, policy loss: 8.297604974984313
Experience 18, Iter 90, disc loss: 0.002179215513309062, policy loss: 7.942610373483776
Experience 18, Iter 91, disc loss: 0.0017415862602477752, policy loss: 7.848688917362112
Experience 18, Iter 92, disc loss: 0.001866150434872742, policy loss: 7.568322166138472
Experience 18, Iter 93, disc loss: 0.0018869370984391947, policy loss: 8.537652755504013
Experience 18, Iter 94, disc loss: 0.0015483888566714022, policy loss: 7.9869264927262735
Experience 18, Iter 95, disc loss: 0.001980454361203228, policy loss: 7.781098505265032
Experience 18, Iter 96, disc loss: 0.0019264705859982716, policy loss: 7.63074381201601
Experience 18, Iter 97, disc loss: 0.001757326823251833, policy loss: 8.293980991088665
Experience 18, Iter 98, disc loss: 0.0016762963124838642, policy loss: 8.158221880838106
Experience 18, Iter 99, disc loss: 0.001781040697799079, policy loss: 7.7019628750779745
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0066],
        [0.1716],
        [2.1018],
        [0.0300]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.2460, 1.4074, 0.0273, 0.0198, 4.3652]],

        [[0.0128, 0.2460, 1.4074, 0.0273, 0.0198, 4.3652]],

        [[0.0128, 0.2460, 1.4074, 0.0273, 0.0198, 4.3652]],

        [[0.0128, 0.2460, 1.4074, 0.0273, 0.0198, 4.3652]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0264, 0.6865, 8.4074, 0.1202], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0264, 0.6865, 8.4074, 0.1202])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.743
Iter 2/2000 - Loss: 3.645
Iter 3/2000 - Loss: 3.558
Iter 4/2000 - Loss: 3.497
Iter 5/2000 - Loss: 3.453
Iter 6/2000 - Loss: 3.362
Iter 7/2000 - Loss: 3.255
Iter 8/2000 - Loss: 3.153
Iter 9/2000 - Loss: 3.032
Iter 10/2000 - Loss: 2.876
Iter 11/2000 - Loss: 2.697
Iter 12/2000 - Loss: 2.507
Iter 13/2000 - Loss: 2.305
Iter 14/2000 - Loss: 2.082
Iter 15/2000 - Loss: 1.832
Iter 16/2000 - Loss: 1.559
Iter 17/2000 - Loss: 1.269
Iter 18/2000 - Loss: 0.964
Iter 19/2000 - Loss: 0.648
Iter 20/2000 - Loss: 0.320
Iter 1981/2000 - Loss: -8.056
Iter 1982/2000 - Loss: -8.057
Iter 1983/2000 - Loss: -8.057
Iter 1984/2000 - Loss: -8.057
Iter 1985/2000 - Loss: -8.057
Iter 1986/2000 - Loss: -8.057
Iter 1987/2000 - Loss: -8.057
Iter 1988/2000 - Loss: -8.057
Iter 1989/2000 - Loss: -8.057
Iter 1990/2000 - Loss: -8.057
Iter 1991/2000 - Loss: -8.057
Iter 1992/2000 - Loss: -8.057
Iter 1993/2000 - Loss: -8.057
Iter 1994/2000 - Loss: -8.057
Iter 1995/2000 - Loss: -8.057
Iter 1996/2000 - Loss: -8.057
Iter 1997/2000 - Loss: -8.057
Iter 1998/2000 - Loss: -8.057
Iter 1999/2000 - Loss: -8.057
Iter 2000/2000 - Loss: -8.057
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[10.6112,  8.9843, 23.2202,  4.4283,  6.4644, 61.1183]],

        [[16.8325, 36.6337,  7.6932,  1.4559,  1.7426, 30.7052]],

        [[18.1704, 35.8712,  8.3468,  1.1129,  1.1428, 22.4660]],

        [[14.0254, 30.7469, 16.4296,  2.5121,  2.0551, 47.6441]]])
Signal Variance: tensor([ 0.1171,  1.9800, 12.2612,  0.5296])
Estimated target variance: tensor([0.0264, 0.6865, 8.4074, 0.1202])
N: 190
Signal to noise ratio: tensor([19.6868, 79.1573, 76.7057, 48.5012])
Bound on condition number: tensor([  73639.3559, 1190518.8591, 1117915.8523,  446950.6316])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.0018048809881984608, policy loss: 8.2310716176192
Experience 19, Iter 1, disc loss: 0.0018035645205213276, policy loss: 8.044549093833403
Experience 19, Iter 2, disc loss: 0.0014872019852386618, policy loss: 8.072429815593962
Experience 19, Iter 3, disc loss: 0.0017593559928152564, policy loss: 8.449198280573142
Experience 19, Iter 4, disc loss: 0.001739733093447117, policy loss: 8.315306877308617
Experience 19, Iter 5, disc loss: 0.001909433392419733, policy loss: 8.506007757020203
Experience 19, Iter 6, disc loss: 0.001502381245099727, policy loss: 8.444694624793069
Experience 19, Iter 7, disc loss: 0.0020469022324888526, policy loss: 7.978996455606097
Experience 19, Iter 8, disc loss: 0.0018466212391180959, policy loss: 8.009272006065803
Experience 19, Iter 9, disc loss: 0.0018585533407463155, policy loss: 7.885333371227869
Experience 19, Iter 10, disc loss: 0.0020776590605922126, policy loss: 8.364618338811143
Experience 19, Iter 11, disc loss: 0.0019080118527362066, policy loss: 8.43268561803472
Experience 19, Iter 12, disc loss: 0.0016044012596170758, policy loss: 8.333564580960124
Experience 19, Iter 13, disc loss: 0.0016760867607129321, policy loss: 8.093202621106787
Experience 19, Iter 14, disc loss: 0.0019036664185476272, policy loss: 7.9202781878141035
Experience 19, Iter 15, disc loss: 0.002020760674957696, policy loss: 8.783875025542098
Experience 19, Iter 16, disc loss: 0.0017150966945880172, policy loss: 8.224379595183184
Experience 19, Iter 17, disc loss: 0.0016300066447815817, policy loss: 8.490305778595339
Experience 19, Iter 18, disc loss: 0.001983190962171846, policy loss: 8.523538545952233
Experience 19, Iter 19, disc loss: 0.001750482395947796, policy loss: 8.370125145290018
Experience 19, Iter 20, disc loss: 0.0018038729765958946, policy loss: 7.762855578335027
Experience 19, Iter 21, disc loss: 0.0018264333875899147, policy loss: 8.171975629470188
Experience 19, Iter 22, disc loss: 0.0018089297726276866, policy loss: 8.576185344811503
Experience 19, Iter 23, disc loss: 0.00198505352135236, policy loss: 8.886084426676334
Experience 19, Iter 24, disc loss: 0.0015797162386314682, policy loss: 8.526902975761661
Experience 19, Iter 25, disc loss: 0.0016423297433644487, policy loss: 8.593983363020783
Experience 19, Iter 26, disc loss: 0.0018642113408780018, policy loss: 7.792540235159743
Experience 19, Iter 27, disc loss: 0.0017031163983787915, policy loss: 8.332901605432308
Experience 19, Iter 28, disc loss: 0.00174480331750598, policy loss: 7.987110284813711
Experience 19, Iter 29, disc loss: 0.0017255347220768791, policy loss: 8.705537020330965
Experience 19, Iter 30, disc loss: 0.0018918410367070968, policy loss: 7.973637457323897
Experience 19, Iter 31, disc loss: 0.0019094260051103259, policy loss: 8.619950678831303
Experience 19, Iter 32, disc loss: 0.0017418017928292164, policy loss: 7.920129905476535
Experience 19, Iter 33, disc loss: 0.0015124325706217215, policy loss: 8.972667413997812
Experience 19, Iter 34, disc loss: 0.0016990096130587396, policy loss: 8.226623938926373
Experience 19, Iter 35, disc loss: 0.0016614485572013407, policy loss: 8.907829268053273
Experience 19, Iter 36, disc loss: 0.0017503165495399652, policy loss: 8.943190850879555
Experience 19, Iter 37, disc loss: 0.0017048518249228799, policy loss: 7.961233421627759
Experience 19, Iter 38, disc loss: 0.001511325029212016, policy loss: 9.231038038360754
Experience 19, Iter 39, disc loss: 0.0015164314785627434, policy loss: 8.377510454955509
Experience 19, Iter 40, disc loss: 0.0014235256144061307, policy loss: 8.578163097436159
Experience 19, Iter 41, disc loss: 0.001427860259484365, policy loss: 8.448109850389889
Experience 19, Iter 42, disc loss: 0.0017225958874381177, policy loss: 8.369542201669146
Experience 19, Iter 43, disc loss: 0.0016444224791932042, policy loss: 7.732567424587691
Experience 19, Iter 44, disc loss: 0.0014426438885644993, policy loss: 8.473490074664715
Experience 19, Iter 45, disc loss: 0.0016780655823996466, policy loss: 8.132543595216497
Experience 19, Iter 46, disc loss: 0.002007224409415196, policy loss: 7.695056286511976
Experience 19, Iter 47, disc loss: 0.0014153542308656911, policy loss: 8.755614501432756
Experience 19, Iter 48, disc loss: 0.0015677366916205591, policy loss: 8.500436763898769
Experience 19, Iter 49, disc loss: 0.0016425728340870088, policy loss: 8.480312877801978
Experience 19, Iter 50, disc loss: 0.0016894760054716256, policy loss: 8.537050018318919
Experience 19, Iter 51, disc loss: 0.0017467023306556155, policy loss: 8.213632920321627
Experience 19, Iter 52, disc loss: 0.0014560065143180813, policy loss: 8.275851667269562
Experience 19, Iter 53, disc loss: 0.002157364003075761, policy loss: 7.445486505390502
Experience 19, Iter 54, disc loss: 0.0017721615650703173, policy loss: 9.02614508552991
Experience 19, Iter 55, disc loss: 0.0014618978553861978, policy loss: 8.486559784889675
Experience 19, Iter 56, disc loss: 0.0016053697204629703, policy loss: 8.361096711998623
Experience 19, Iter 57, disc loss: 0.0016965248379179213, policy loss: 8.323914819951284
Experience 19, Iter 58, disc loss: 0.0017049819916025255, policy loss: 8.193091538446104
Experience 19, Iter 59, disc loss: 0.0017238102291082045, policy loss: 7.9653075981688675
Experience 19, Iter 60, disc loss: 0.001558326599118357, policy loss: 8.363774000085257
Experience 19, Iter 61, disc loss: 0.0019328131068429638, policy loss: 7.912438569779726
Experience 19, Iter 62, disc loss: 0.0014748587798633578, policy loss: 8.627763557602785
Experience 19, Iter 63, disc loss: 0.0014389622808489559, policy loss: 8.37013729101656
Experience 19, Iter 64, disc loss: 0.0014327741771274212, policy loss: 8.509868557679317
Experience 19, Iter 65, disc loss: 0.0014926062523188902, policy loss: 8.176896906395367
Experience 19, Iter 66, disc loss: 0.0017045493008491066, policy loss: 8.31631842674871
Experience 19, Iter 67, disc loss: 0.001498645101039883, policy loss: 8.333178461496871
Experience 19, Iter 68, disc loss: 0.001643191428882067, policy loss: 8.056644728847623
Experience 19, Iter 69, disc loss: 0.001628292012268511, policy loss: 8.62926645874418
Experience 19, Iter 70, disc loss: 0.0018305956863364642, policy loss: 7.872726318692397
Experience 19, Iter 71, disc loss: 0.001507708418779129, policy loss: 8.270555654501521
Experience 19, Iter 72, disc loss: 0.001556655678532018, policy loss: 8.74779592660693
Experience 19, Iter 73, disc loss: 0.0017491814102023938, policy loss: 8.230448364695018
Experience 19, Iter 74, disc loss: 0.0015292100529869454, policy loss: 8.659762754321749
Experience 19, Iter 75, disc loss: 0.001580945747687194, policy loss: 8.621469933954286
Experience 19, Iter 76, disc loss: 0.0014296221808116853, policy loss: 9.309445957963897
Experience 19, Iter 77, disc loss: 0.0015940135356402214, policy loss: 8.480389104125187
Experience 19, Iter 78, disc loss: 0.0015323286031401623, policy loss: 8.016457128042099
Experience 19, Iter 79, disc loss: 0.001760059134391511, policy loss: 8.467608297102942
Experience 19, Iter 80, disc loss: 0.0012674188435890668, policy loss: 8.505401613200554
Experience 19, Iter 81, disc loss: 0.0014690366189861943, policy loss: 8.437554650810933
Experience 19, Iter 82, disc loss: 0.0016260905121333615, policy loss: 7.951968908475944
Experience 19, Iter 83, disc loss: 0.00158982126896317, policy loss: 8.314704651339476
Experience 19, Iter 84, disc loss: 0.001683043782502792, policy loss: 8.308785941339735
Experience 19, Iter 85, disc loss: 0.0017380711276885588, policy loss: 7.7658510559453084
Experience 19, Iter 86, disc loss: 0.0015234285584269204, policy loss: 8.429980737967213
Experience 19, Iter 87, disc loss: 0.001532972634107024, policy loss: 8.170051896883338
Experience 19, Iter 88, disc loss: 0.001829415980383626, policy loss: 7.9985150406273
Experience 19, Iter 89, disc loss: 0.0015147026368190616, policy loss: 8.662979075561703
Experience 19, Iter 90, disc loss: 0.0017751070566790275, policy loss: 8.044664673420812
Experience 19, Iter 91, disc loss: 0.001397586989375045, policy loss: 9.000679129563586
Experience 19, Iter 92, disc loss: 0.001716467297201837, policy loss: 8.189836265980855
Experience 19, Iter 93, disc loss: 0.0017264797795483837, policy loss: 7.963831191782328
Experience 19, Iter 94, disc loss: 0.0016829865861216523, policy loss: 8.135989765677634
Experience 19, Iter 95, disc loss: 0.0013763207052237862, policy loss: 8.487374835044443
Experience 19, Iter 96, disc loss: 0.001684007020073673, policy loss: 8.016920673402499
Experience 19, Iter 97, disc loss: 0.0010408325073593178, policy loss: 11.816990902317638
Experience 19, Iter 98, disc loss: 0.001419367737173106, policy loss: 8.52358173994567
Experience 19, Iter 99, disc loss: 0.0014788958854792841, policy loss: 8.771372400725447
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0065],
        [0.1797],
        [2.1760],
        [0.0306]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0126, 0.2451, 1.4346, 0.0280, 0.0196, 4.5061]],

        [[0.0126, 0.2451, 1.4346, 0.0280, 0.0196, 4.5061]],

        [[0.0126, 0.2451, 1.4346, 0.0280, 0.0196, 4.5061]],

        [[0.0126, 0.2451, 1.4346, 0.0280, 0.0196, 4.5061]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0261, 0.7190, 8.7040, 0.1223], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0261, 0.7190, 8.7040, 0.1223])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.792
Iter 2/2000 - Loss: 3.689
Iter 3/2000 - Loss: 3.608
Iter 4/2000 - Loss: 3.544
Iter 5/2000 - Loss: 3.501
Iter 6/2000 - Loss: 3.415
Iter 7/2000 - Loss: 3.310
Iter 8/2000 - Loss: 3.209
Iter 9/2000 - Loss: 3.093
Iter 10/2000 - Loss: 2.942
Iter 11/2000 - Loss: 2.767
Iter 12/2000 - Loss: 2.582
Iter 13/2000 - Loss: 2.384
Iter 14/2000 - Loss: 2.166
Iter 15/2000 - Loss: 1.920
Iter 16/2000 - Loss: 1.650
Iter 17/2000 - Loss: 1.361
Iter 18/2000 - Loss: 1.057
Iter 19/2000 - Loss: 0.739
Iter 20/2000 - Loss: 0.409
Iter 1981/2000 - Loss: -8.138
Iter 1982/2000 - Loss: -8.138
Iter 1983/2000 - Loss: -8.138
Iter 1984/2000 - Loss: -8.139
Iter 1985/2000 - Loss: -8.139
Iter 1986/2000 - Loss: -8.139
Iter 1987/2000 - Loss: -8.139
Iter 1988/2000 - Loss: -8.139
Iter 1989/2000 - Loss: -8.139
Iter 1990/2000 - Loss: -8.139
Iter 1991/2000 - Loss: -8.139
Iter 1992/2000 - Loss: -8.139
Iter 1993/2000 - Loss: -8.139
Iter 1994/2000 - Loss: -8.139
Iter 1995/2000 - Loss: -8.139
Iter 1996/2000 - Loss: -8.139
Iter 1997/2000 - Loss: -8.139
Iter 1998/2000 - Loss: -8.139
Iter 1999/2000 - Loss: -8.139
Iter 2000/2000 - Loss: -8.139
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[10.1339,  9.0544, 20.2604,  4.7405,  7.1175, 60.7238]],

        [[16.7935, 36.9027,  7.4577,  1.4848,  1.8522, 31.4958]],

        [[17.9508, 35.3416,  8.1243,  1.0974,  1.2118, 22.5776]],

        [[13.8427, 30.4924, 16.9156,  2.5297,  1.9868, 47.6242]]])
Signal Variance: tensor([ 0.1173,  1.9574, 12.3943,  0.5396])
Estimated target variance: tensor([0.0261, 0.7190, 8.7040, 0.1223])
N: 200
Signal to noise ratio: tensor([19.6290, 80.4393, 78.9688, 49.7496])
Bound on condition number: tensor([  77060.6717, 1294096.8081, 1247216.8352,  495004.8949])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0012831559870967644, policy loss: 9.440828055122008
Experience 20, Iter 1, disc loss: 0.0011017278779717108, policy loss: 9.479236541289756
Experience 20, Iter 2, disc loss: 0.0010255477554510927, policy loss: 9.520740133984818
Experience 20, Iter 3, disc loss: 0.0009815791671013493, policy loss: 9.513411374457394
Experience 20, Iter 4, disc loss: 0.0009282596105723348, policy loss: 10.080302895509298
Experience 20, Iter 5, disc loss: 0.0009136177559592611, policy loss: 10.578124307041886
Experience 20, Iter 6, disc loss: 0.0009876314892470037, policy loss: 9.312833187054318
Experience 20, Iter 7, disc loss: 0.0010539775041528265, policy loss: 9.5462282739467
Experience 20, Iter 8, disc loss: 0.0010775051244203495, policy loss: 8.910255355768076
Experience 20, Iter 9, disc loss: 0.0013130986500744716, policy loss: 8.428800137399957
Experience 20, Iter 10, disc loss: 0.001223531962202744, policy loss: 8.407892185628095
Experience 20, Iter 11, disc loss: 0.001151075111099183, policy loss: 9.52690311748086
Experience 20, Iter 12, disc loss: 0.0012139754600451519, policy loss: 8.540568401029123
Experience 20, Iter 13, disc loss: 0.0010981224872898883, policy loss: 8.505664996750351
Experience 20, Iter 14, disc loss: 0.0010732060580368396, policy loss: 8.619598619518992
Experience 20, Iter 15, disc loss: 0.0010160064715752553, policy loss: 9.661435240866927
Experience 20, Iter 16, disc loss: 0.0010287099373360391, policy loss: 8.952020506155433
Experience 20, Iter 17, disc loss: 0.0009415504716575952, policy loss: 8.5915129475251
Experience 20, Iter 18, disc loss: 0.0010243178221094652, policy loss: 8.939572465767181
Experience 20, Iter 19, disc loss: 0.0011499595434544406, policy loss: 8.302319345069437
Experience 20, Iter 20, disc loss: 0.0011826357289089248, policy loss: 8.043670179277603
Experience 20, Iter 21, disc loss: 0.0012924082359031023, policy loss: 8.476006079007847
Experience 20, Iter 22, disc loss: 0.00128863415322476, policy loss: 8.362176323204132
Experience 20, Iter 23, disc loss: 0.0012432778734062728, policy loss: 8.544279018440395
Experience 20, Iter 24, disc loss: 0.0013758031708586004, policy loss: 8.374534520177567
Experience 20, Iter 25, disc loss: 0.0014434709367365326, policy loss: 8.132851023992973
Experience 20, Iter 26, disc loss: 0.0012755245475828039, policy loss: 7.992306904933172
Experience 20, Iter 27, disc loss: 0.0012695783196794896, policy loss: 8.1844824659683
Experience 20, Iter 28, disc loss: 0.0015016222128736052, policy loss: 8.483801587976888
Experience 20, Iter 29, disc loss: 0.0012098640004115082, policy loss: 8.944876455943957
Experience 20, Iter 30, disc loss: 0.0012516873162949698, policy loss: 8.626526366606267
Experience 20, Iter 31, disc loss: 0.0013201475683698848, policy loss: 8.425704216233108
Experience 20, Iter 32, disc loss: 0.0013212386201335862, policy loss: 7.964588980024175
Experience 20, Iter 33, disc loss: 0.0015873867699242837, policy loss: 8.307151282285897
Experience 20, Iter 34, disc loss: 0.0014540679198475313, policy loss: 8.545035920373365
Experience 20, Iter 35, disc loss: 0.0018441360996651198, policy loss: 8.146976372108947
Experience 20, Iter 36, disc loss: 0.0012776001934284482, policy loss: 8.412942322042337
Experience 20, Iter 37, disc loss: 0.0011858556416919834, policy loss: 8.448525238221977
Experience 20, Iter 38, disc loss: 0.00121634662052764, policy loss: 8.475698047697383
Experience 20, Iter 39, disc loss: 0.0012568267306377568, policy loss: 8.275515517905072
Experience 20, Iter 40, disc loss: 0.0013847693822484045, policy loss: 8.09526918320557
Experience 20, Iter 41, disc loss: 0.0015177945326552715, policy loss: 8.376281134341777
Experience 20, Iter 42, disc loss: 0.0013535489147426796, policy loss: 8.29326615606103
Experience 20, Iter 43, disc loss: 0.001633463449738909, policy loss: 7.624370902241571
Experience 20, Iter 44, disc loss: 0.0011791855421629522, policy loss: 8.1058493148691
Experience 20, Iter 45, disc loss: 0.0013025678527655324, policy loss: 8.689852987067177
Experience 20, Iter 46, disc loss: 0.0011000403501275156, policy loss: 8.709816410798545
Experience 20, Iter 47, disc loss: 0.0011134815463723527, policy loss: 8.695223760377896
Experience 20, Iter 48, disc loss: 0.0013340997961461135, policy loss: 8.365810056825975
Experience 20, Iter 49, disc loss: 0.0013834448490143768, policy loss: 7.866604147424026
Experience 20, Iter 50, disc loss: 0.0013494244963734785, policy loss: 8.554879332128456
Experience 20, Iter 51, disc loss: 0.0013532073885383336, policy loss: 7.932867338217385
Experience 20, Iter 52, disc loss: 0.0011640374398194193, policy loss: 8.509228505043211
Experience 20, Iter 53, disc loss: 0.0013492075541830942, policy loss: 8.036750386745101
Experience 20, Iter 54, disc loss: 0.0012242333848946853, policy loss: 8.21168148331911
Experience 20, Iter 55, disc loss: 0.0014966761109371477, policy loss: 8.237956654150963
Experience 20, Iter 56, disc loss: 0.0015659601888713122, policy loss: 8.146895772718224
Experience 20, Iter 57, disc loss: 0.0012014873625216813, policy loss: 8.490257330735501
Experience 20, Iter 58, disc loss: 0.0013668917470421147, policy loss: 8.466086518822518
Experience 20, Iter 59, disc loss: 0.00145469376742562, policy loss: 8.13885558083799
Experience 20, Iter 60, disc loss: 0.0016153018273753652, policy loss: 7.624735543585115
Experience 20, Iter 61, disc loss: 0.0015013232291235315, policy loss: 7.9116463492105655
Experience 20, Iter 62, disc loss: 0.0014115819125020011, policy loss: 8.139766300548985
Experience 20, Iter 63, disc loss: 0.0014942725514541792, policy loss: 7.654277461260941
Experience 20, Iter 64, disc loss: 0.0014148517193793052, policy loss: 8.479970078529618
Experience 20, Iter 65, disc loss: 0.0013828631748730772, policy loss: 7.854213008271197
Experience 20, Iter 66, disc loss: 0.0013536041273914993, policy loss: 8.365426345839989
Experience 20, Iter 67, disc loss: 0.0014273552233757397, policy loss: 7.932853342267836
Experience 20, Iter 68, disc loss: 0.0013620735960982442, policy loss: 8.47031686834315
Experience 20, Iter 69, disc loss: 0.0013149824267900184, policy loss: 8.356742659803137
Experience 20, Iter 70, disc loss: 0.0012502774176279987, policy loss: 8.580251902797743
Experience 20, Iter 71, disc loss: 0.001320160115544105, policy loss: 8.429103233366172
Experience 20, Iter 72, disc loss: 0.0014650245808585567, policy loss: 8.33755808256946
Experience 20, Iter 73, disc loss: 0.0013720769594521148, policy loss: 8.187119467310021
Experience 20, Iter 74, disc loss: 0.0012847062124253738, policy loss: 8.343430608881912
Experience 20, Iter 75, disc loss: 0.0012682698995670023, policy loss: 8.398741074800702
Experience 20, Iter 76, disc loss: 0.0013307837637032915, policy loss: 8.85127084363967
Experience 20, Iter 77, disc loss: 0.0012864786590926418, policy loss: 8.074947694144953
Experience 20, Iter 78, disc loss: 0.0012130519245433004, policy loss: 8.498021967604348
Experience 20, Iter 79, disc loss: 0.0013485654582447981, policy loss: 7.918655312904466
Experience 20, Iter 80, disc loss: 0.0014529515332680429, policy loss: 8.055376826640199
Experience 20, Iter 81, disc loss: 0.0011755825395517892, policy loss: 8.75518426126333
Experience 20, Iter 82, disc loss: 0.001371202388539053, policy loss: 8.025243510211716
Experience 20, Iter 83, disc loss: 0.0012383857276032039, policy loss: 8.06231924359949
Experience 20, Iter 84, disc loss: 0.001466205005939633, policy loss: 7.941423907879783
Experience 20, Iter 85, disc loss: 0.0014875649776112019, policy loss: 8.362197639356738
Experience 20, Iter 86, disc loss: 0.001312896609757256, policy loss: 8.27451242573867
Experience 20, Iter 87, disc loss: 0.001308704218725104, policy loss: 8.230263720503846
Experience 20, Iter 88, disc loss: 0.0013327216279876452, policy loss: 8.495050299162273
Experience 20, Iter 89, disc loss: 0.0013767030827682991, policy loss: 8.264044188743675
Experience 20, Iter 90, disc loss: 0.001391016509354956, policy loss: 8.401626191816824
Experience 20, Iter 91, disc loss: 0.0011922894172156692, policy loss: 8.740339586674784
Experience 20, Iter 92, disc loss: 0.0011937219156322586, policy loss: 8.299064932104706
Experience 20, Iter 93, disc loss: 0.0012669115129534379, policy loss: 8.2720095842702
Experience 20, Iter 94, disc loss: 0.001340697771684526, policy loss: 8.4913810038593
Experience 20, Iter 95, disc loss: 0.001262732541617404, policy loss: 9.132750410307247
Experience 20, Iter 96, disc loss: 0.0013637168395463555, policy loss: 8.095232381635789
Experience 20, Iter 97, disc loss: 0.0013878356943625358, policy loss: 8.371041820055225
Experience 20, Iter 98, disc loss: 0.001469612494490495, policy loss: 8.836400067854006
Experience 20, Iter 99, disc loss: 0.0015550524776188363, policy loss: 7.700412836692339
