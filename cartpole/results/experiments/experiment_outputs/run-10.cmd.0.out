Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0031],
        [0.1303],
        [0.0027]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0028, 0.0125, 0.5212, 0.0109], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0028, 0.0125, 0.5212, 0.0109])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.274
Iter 2/2000 - Loss: -1.581
Iter 3/2000 - Loss: -1.722
Iter 4/2000 - Loss: -1.027
Iter 5/2000 - Loss: -0.873
Iter 6/2000 - Loss: -1.255
Iter 7/2000 - Loss: -1.662
Iter 8/2000 - Loss: -1.804
Iter 9/2000 - Loss: -1.743
Iter 10/2000 - Loss: -1.668
Iter 11/2000 - Loss: -1.652
Iter 12/2000 - Loss: -1.670
Iter 13/2000 - Loss: -1.703
Iter 14/2000 - Loss: -1.768
Iter 15/2000 - Loss: -1.845
Iter 16/2000 - Loss: -1.885
Iter 17/2000 - Loss: -1.872
Iter 18/2000 - Loss: -1.849
Iter 19/2000 - Loss: -1.863
Iter 20/2000 - Loss: -1.909
Iter 1981/2000 - Loss: -2.248
Iter 1982/2000 - Loss: -2.256
Iter 1983/2000 - Loss: -2.253
Iter 1984/2000 - Loss: -2.245
Iter 1985/2000 - Loss: -2.244
Iter 1986/2000 - Loss: -2.251
Iter 1987/2000 - Loss: -2.256
Iter 1988/2000 - Loss: -2.253
Iter 1989/2000 - Loss: -2.249
Iter 1990/2000 - Loss: -2.251
Iter 1991/2000 - Loss: -2.256
Iter 1992/2000 - Loss: -2.255
Iter 1993/2000 - Loss: -2.252
Iter 1994/2000 - Loss: -2.250
Iter 1995/2000 - Loss: -2.253
Iter 1996/2000 - Loss: -2.256
Iter 1997/2000 - Loss: -2.256
Iter 1998/2000 - Loss: -2.254
Iter 1999/2000 - Loss: -2.253
Iter 2000/2000 - Loss: -2.254
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0022],
        [0.0855],
        [0.0020]])
Lengthscale: tensor([[[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]],

        [[1.4184e-02, 3.2278e-02, 1.2241e-01, 3.3186e-03, 1.2197e-04,
          6.2090e-02]]])
Signal Variance: tensor([0.0020, 0.0090, 0.3840, 0.0079])
Estimated target variance: tensor([0.0028, 0.0125, 0.5212, 0.0109])
N: 10
Signal to noise ratio: tensor([2.0006, 2.0042, 2.1189, 2.0051])
Bound on condition number: tensor([41.0250, 41.1688, 45.8978, 41.2031])
Policy Optimizer learning rate:
0.001
Experience 1, Iter 0, disc loss: 1.3662936770928409, policy loss: 0.6307496443920091
Experience 1, Iter 1, disc loss: 1.3573808095877844, policy loss: 0.626747757293123
Experience 1, Iter 2, disc loss: 1.3416503486953586, policy loss: 0.6290266279623522
Experience 1, Iter 3, disc loss: 1.325672278547065, policy loss: 0.6314578439697708
Experience 1, Iter 4, disc loss: 1.3123231748162798, policy loss: 0.6318164603095385
Experience 1, Iter 5, disc loss: 1.300220412569004, policy loss: 0.631292164415143
Experience 1, Iter 6, disc loss: 1.2887138065481198, policy loss: 0.6304185750752748
Experience 1, Iter 7, disc loss: 1.2718300816219554, policy loss: 0.6341803099222042
Experience 1, Iter 8, disc loss: 1.2568434243635123, policy loss: 0.6365417394040183
Experience 1, Iter 9, disc loss: 1.244307199983147, policy loss: 0.6369378296806213
Experience 1, Iter 10, disc loss: 1.2362117813502698, policy loss: 0.6337951297892962
Experience 1, Iter 11, disc loss: 1.2191853164029718, policy loss: 0.6392107532545439
Experience 1, Iter 12, disc loss: 1.2071696951525066, policy loss: 0.6408170579686849
Experience 1, Iter 13, disc loss: 1.1955080607927868, policy loss: 0.642468564006586
Experience 1, Iter 14, disc loss: 1.184980678231497, policy loss: 0.6437711014791488
Experience 1, Iter 15, disc loss: 1.1761199251701955, policy loss: 0.6432842446063308
Experience 1, Iter 16, disc loss: 1.1643751503862425, policy loss: 0.6460039536287517
Experience 1, Iter 17, disc loss: 1.1568273362937742, policy loss: 0.6449296408869093
Experience 1, Iter 18, disc loss: 1.1436010275438395, policy loss: 0.6487615771341328
Experience 1, Iter 19, disc loss: 1.1311290121086242, policy loss: 0.6522197149548261
Experience 1, Iter 20, disc loss: 1.1244543495793051, policy loss: 0.6505534823415342
Experience 1, Iter 21, disc loss: 1.109736723479154, policy loss: 0.6564344715446664
Experience 1, Iter 22, disc loss: 1.1013907481603025, policy loss: 0.655862675493575
Experience 1, Iter 23, disc loss: 1.0923124165736424, policy loss: 0.6563697462498399
Experience 1, Iter 24, disc loss: 1.0763176087438793, policy loss: 0.6635116550546696
Experience 1, Iter 25, disc loss: 1.0710003873305984, policy loss: 0.6607198306233073
Experience 1, Iter 26, disc loss: 1.0633433737921565, policy loss: 0.6603744077441422
Experience 1, Iter 27, disc loss: 1.0529646208793075, policy loss: 0.6624641347629286
Experience 1, Iter 28, disc loss: 1.0409518676070075, policy loss: 0.6664178870617699
Experience 1, Iter 29, disc loss: 1.029735172365926, policy loss: 0.6700541881755655
Experience 1, Iter 30, disc loss: 1.0177779862145653, policy loss: 0.6741128941265957
Experience 1, Iter 31, disc loss: 1.0129237908267497, policy loss: 0.6717357417599312
Experience 1, Iter 32, disc loss: 1.010877657122535, policy loss: 0.6662629104799405
Experience 1, Iter 33, disc loss: 0.9937923872911205, policy loss: 0.6754734279138516
Experience 1, Iter 34, disc loss: 0.9897236859425664, policy loss: 0.6722203039647714
Experience 1, Iter 35, disc loss: 0.9788937637013975, policy loss: 0.6757800991873069
Experience 1, Iter 36, disc loss: 0.9606861508139719, policy loss: 0.6864368649651258
Experience 1, Iter 37, disc loss: 0.9557461443033384, policy loss: 0.684483101187935
Experience 1, Iter 38, disc loss: 0.9428651429366333, policy loss: 0.6905428459981386
Experience 1, Iter 39, disc loss: 0.9382398664431194, policy loss: 0.6888621526768944
Experience 1, Iter 40, disc loss: 0.9263837569505077, policy loss: 0.6940254008445756
Experience 1, Iter 41, disc loss: 0.9204651204170613, policy loss: 0.6928501509523226
Experience 1, Iter 42, disc loss: 0.9158563506344869, policy loss: 0.6907331656747299
Experience 1, Iter 43, disc loss: 0.8984097554943389, policy loss: 0.7019969637380458
Experience 1, Iter 44, disc loss: 0.8963784566383879, policy loss: 0.6990575384451568
Experience 1, Iter 45, disc loss: 0.8797045395359051, policy loss: 0.7092652432545541
Experience 1, Iter 46, disc loss: 0.8805330618737406, policy loss: 0.7027547780300422
Experience 1, Iter 47, disc loss: 0.8666685438651829, policy loss: 0.7099618872078185
Experience 1, Iter 48, disc loss: 0.849488807240612, policy loss: 0.7232651783719265
Experience 1, Iter 49, disc loss: 0.8423927014833337, policy loss: 0.723462049271355
Experience 1, Iter 50, disc loss: 0.8398846125687899, policy loss: 0.7208304148378137
Experience 1, Iter 51, disc loss: 0.8283731025841339, policy loss: 0.7275904481397637
Experience 1, Iter 52, disc loss: 0.8142088509270964, policy loss: 0.7380686678242834
Experience 1, Iter 53, disc loss: 0.813161240364055, policy loss: 0.732754999779216
Experience 1, Iter 54, disc loss: 0.7975181493924, policy loss: 0.7443370785090471
Experience 1, Iter 55, disc loss: 0.7952844979866924, policy loss: 0.7436176029311725
Experience 1, Iter 56, disc loss: 0.7815207893819249, policy loss: 0.7534949022016989
Experience 1, Iter 57, disc loss: 0.7869217086959295, policy loss: 0.7415261295980958
Experience 1, Iter 58, disc loss: 0.7663168774302594, policy loss: 0.7596889517728878
Experience 1, Iter 59, disc loss: 0.7620555735892233, policy loss: 0.7623055900484073
Experience 1, Iter 60, disc loss: 0.7491677826632666, policy loss: 0.7741257661794628
Experience 1, Iter 61, disc loss: 0.7506789754355521, policy loss: 0.7663630824630787
Experience 1, Iter 62, disc loss: 0.7303641681421775, policy loss: 0.7882996414510819
Experience 1, Iter 63, disc loss: 0.7178491501934068, policy loss: 0.7991875030145579
Experience 1, Iter 64, disc loss: 0.7238582858974725, policy loss: 0.7861274672675331
Experience 1, Iter 65, disc loss: 0.7114291706849744, policy loss: 0.7974363117375332
Experience 1, Iter 66, disc loss: 0.703274466753944, policy loss: 0.8023715894810424
Experience 1, Iter 67, disc loss: 0.6796020315303406, policy loss: 0.830112126284004
Experience 1, Iter 68, disc loss: 0.6963764923917088, policy loss: 0.8053758117047498
Experience 1, Iter 69, disc loss: 0.6811576418818813, policy loss: 0.8203640905877326
Experience 1, Iter 70, disc loss: 0.6668246279817218, policy loss: 0.8386823798991195
Experience 1, Iter 71, disc loss: 0.6428154012939226, policy loss: 0.8710917099865814
Experience 1, Iter 72, disc loss: 0.6466660536462866, policy loss: 0.863221763230777
Experience 1, Iter 73, disc loss: 0.6394380422347814, policy loss: 0.8654277204790046
Experience 1, Iter 74, disc loss: 0.6198011767095385, policy loss: 0.8926659861864911
Experience 1, Iter 75, disc loss: 0.6077744542279726, policy loss: 0.9068841671428747
Experience 1, Iter 76, disc loss: 0.610279342853949, policy loss: 0.9034758805213576
Experience 1, Iter 77, disc loss: 0.5945601436133392, policy loss: 0.9251369246083878
Experience 1, Iter 78, disc loss: 0.5901217395215115, policy loss: 0.9241804089775469
Experience 1, Iter 79, disc loss: 0.5706478908432684, policy loss: 0.9542449918196421
Experience 1, Iter 80, disc loss: 0.569726925243574, policy loss: 0.9513755269222488
Experience 1, Iter 81, disc loss: 0.5589594498183464, policy loss: 0.9734686890849987
Experience 1, Iter 82, disc loss: 0.5376218359468791, policy loss: 1.0053933387202352
Experience 1, Iter 83, disc loss: 0.5399499364400419, policy loss: 1.0010529137874187
Experience 1, Iter 84, disc loss: 0.5158380714659601, policy loss: 1.0380899854203658
Experience 1, Iter 85, disc loss: 0.5221646037519552, policy loss: 1.0181398692785544
Experience 1, Iter 86, disc loss: 0.49567911936841835, policy loss: 1.077289034680963
Experience 1, Iter 87, disc loss: 0.4991262163110685, policy loss: 1.0627363522782418
Experience 1, Iter 88, disc loss: 0.48434885502958247, policy loss: 1.0880354729357482
Experience 1, Iter 89, disc loss: 0.45715500720153324, policy loss: 1.1380646710555258
Experience 1, Iter 90, disc loss: 0.4626831911606162, policy loss: 1.135664996124981
Experience 1, Iter 91, disc loss: 0.44713618275753414, policy loss: 1.1585527061548158
Experience 1, Iter 92, disc loss: 0.42171704150546774, policy loss: 1.2154615372188502
Experience 1, Iter 93, disc loss: 0.3986327315243317, policy loss: 1.274912964548934
Experience 1, Iter 94, disc loss: 0.4064720620331782, policy loss: 1.2459257761468714
Experience 1, Iter 95, disc loss: 0.39845221469418207, policy loss: 1.261156352853832
Experience 1, Iter 96, disc loss: 0.3743021888215714, policy loss: 1.3226835047464771
Experience 1, Iter 97, disc loss: 0.37327911297057903, policy loss: 1.3316825875543168
Experience 1, Iter 98, disc loss: 0.3660148695348825, policy loss: 1.3382267085489032
Experience 1, Iter 99, disc loss: 0.3615626175269697, policy loss: 1.3426013457323966
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0008],
        [0.0016],
        [0.0628],
        [0.0014]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0031, 0.0064, 0.2512, 0.0057], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0031, 0.0064, 0.2512, 0.0057])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.486
Iter 2/2000 - Loss: -2.034
Iter 3/2000 - Loss: -2.506
Iter 4/2000 - Loss: -1.856
Iter 5/2000 - Loss: -1.601
Iter 6/2000 - Loss: -1.945
Iter 7/2000 - Loss: -2.364
Iter 8/2000 - Loss: -2.534
Iter 9/2000 - Loss: -2.489
Iter 10/2000 - Loss: -2.419
Iter 11/2000 - Loss: -2.449
Iter 12/2000 - Loss: -2.550
Iter 13/2000 - Loss: -2.629
Iter 14/2000 - Loss: -2.626
Iter 15/2000 - Loss: -2.558
Iter 16/2000 - Loss: -2.503
Iter 17/2000 - Loss: -2.535
Iter 18/2000 - Loss: -2.653
Iter 19/2000 - Loss: -2.787
Iter 20/2000 - Loss: -2.860
Iter 1981/2000 - Loss: -3.120
Iter 1982/2000 - Loss: -3.120
Iter 1983/2000 - Loss: -3.120
Iter 1984/2000 - Loss: -3.120
Iter 1985/2000 - Loss: -3.120
Iter 1986/2000 - Loss: -3.120
Iter 1987/2000 - Loss: -3.120
Iter 1988/2000 - Loss: -3.120
Iter 1989/2000 - Loss: -3.120
Iter 1990/2000 - Loss: -3.119
Iter 1991/2000 - Loss: -3.119
Iter 1992/2000 - Loss: -3.117
Iter 1993/2000 - Loss: -3.115
Iter 1994/2000 - Loss: -3.109
Iter 1995/2000 - Loss: -3.101
Iter 1996/2000 - Loss: -3.091
Iter 1997/2000 - Loss: -3.089
Iter 1998/2000 - Loss: -3.104
Iter 1999/2000 - Loss: -3.118
Iter 2000/2000 - Loss: -3.113
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0006],
        [0.0012],
        [0.0458],
        [0.0011]])
Lengthscale: tensor([[[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]],

        [[1.0466e-02, 3.0593e-02, 6.1290e-02, 1.6342e-03, 7.0403e-05,
          3.3584e-02]]])
Signal Variance: tensor([0.0023, 0.0049, 0.1928, 0.0043])
Estimated target variance: tensor([0.0031, 0.0064, 0.2512, 0.0057])
N: 20
Signal to noise ratio: tensor([2.0009, 2.0024, 2.0519, 2.0016])
Bound on condition number: tensor([81.0712, 81.1951, 85.2062, 81.1300])
Policy Optimizer learning rate:
0.0009989469496904544
Experience 2, Iter 0, disc loss: 0.3557498896660156, policy loss: 1.3597885043106197
Experience 2, Iter 1, disc loss: 0.35375913542066917, policy loss: 1.3592225556173076
Experience 2, Iter 2, disc loss: 0.3363560854164563, policy loss: 1.4054743448335885
Experience 2, Iter 3, disc loss: 0.3262471384771542, policy loss: 1.4313475879968243
Experience 2, Iter 4, disc loss: 0.3114400858148047, policy loss: 1.4825646380606037
Experience 2, Iter 5, disc loss: 0.3037333154143495, policy loss: 1.4938325316174346
Experience 2, Iter 6, disc loss: 0.28387793888327195, policy loss: 1.5732617750037425
Experience 2, Iter 7, disc loss: 0.2893202419564328, policy loss: 1.542868263300917
Experience 2, Iter 8, disc loss: 0.2836576173075695, policy loss: 1.5549145435308294
Experience 2, Iter 9, disc loss: 0.2695808264441587, policy loss: 1.6023873893325926
Experience 2, Iter 10, disc loss: 0.2570337595444796, policy loss: 1.6618560304414205
Experience 2, Iter 11, disc loss: 0.25250845507851305, policy loss: 1.6765875343607382
Experience 2, Iter 12, disc loss: 0.2428056685971769, policy loss: 1.711050050914977
Experience 2, Iter 13, disc loss: 0.23089362528970703, policy loss: 1.7889965132885952
Experience 2, Iter 14, disc loss: 0.22437842310638442, policy loss: 1.795750050104786
Experience 2, Iter 15, disc loss: 0.22111896468059733, policy loss: 1.8114939739459297
Experience 2, Iter 16, disc loss: 0.20795110652343218, policy loss: 1.8670351081035674
Experience 2, Iter 17, disc loss: 0.19609779513870526, policy loss: 1.9343038959183718
Experience 2, Iter 18, disc loss: 0.19925697319756108, policy loss: 1.90447642799505
Experience 2, Iter 19, disc loss: 0.18299633372258584, policy loss: 2.004292981014856
Experience 2, Iter 20, disc loss: 0.1712587167458624, policy loss: 2.065302842228083
Experience 2, Iter 21, disc loss: 0.1754584656923016, policy loss: 2.038732278550461
Experience 2, Iter 22, disc loss: 0.16256421991140246, policy loss: 2.105647314747721
Experience 2, Iter 23, disc loss: 0.1648170617089501, policy loss: 2.1081641265718143
Experience 2, Iter 24, disc loss: 0.15069754925459627, policy loss: 2.184882260338191
Experience 2, Iter 25, disc loss: 0.1519286114221872, policy loss: 2.1870570067520223
Experience 2, Iter 26, disc loss: 0.14746500998587514, policy loss: 2.186514588410794
Experience 2, Iter 27, disc loss: 0.1446634109576721, policy loss: 2.2323994192545213
Experience 2, Iter 28, disc loss: 0.14167457353403057, policy loss: 2.246225013695655
Experience 2, Iter 29, disc loss: 0.1327699998557401, policy loss: 2.3186313123420654
Experience 2, Iter 30, disc loss: 0.12904249375103755, policy loss: 2.345135993014722
Experience 2, Iter 31, disc loss: 0.12228679351287783, policy loss: 2.4031326099223334
Experience 2, Iter 32, disc loss: 0.12294561869249089, policy loss: 2.3824508479718958
Experience 2, Iter 33, disc loss: 0.11878820510714483, policy loss: 2.4369275575476808
Experience 2, Iter 34, disc loss: 0.11681799652963709, policy loss: 2.4477939054701277
Experience 2, Iter 35, disc loss: 0.10406548983629385, policy loss: 2.592095910810375
Experience 2, Iter 36, disc loss: 0.1070598186568255, policy loss: 2.5377035449632546
Experience 2, Iter 37, disc loss: 0.09869621228983928, policy loss: 2.625874197532773
Experience 2, Iter 38, disc loss: 0.10327726754113334, policy loss: 2.5598119875671776
Experience 2, Iter 39, disc loss: 0.08903772709397427, policy loss: 2.7301937181330356
Experience 2, Iter 40, disc loss: 0.09051671571977263, policy loss: 2.714999002448696
Experience 2, Iter 41, disc loss: 0.08713312702797432, policy loss: 2.791629895998534
Experience 2, Iter 42, disc loss: 0.08590667113133223, policy loss: 2.77860526588091
Experience 2, Iter 43, disc loss: 0.08558320549354774, policy loss: 2.783400675642174
Experience 2, Iter 44, disc loss: 0.07787582254120279, policy loss: 2.8834227887163357
Experience 2, Iter 45, disc loss: 0.07202865228229711, policy loss: 2.9761651976270596
Experience 2, Iter 46, disc loss: 0.07103941609662243, policy loss: 2.9967052175423103
Experience 2, Iter 47, disc loss: 0.07212909303812001, policy loss: 2.9638078442338758
Experience 2, Iter 48, disc loss: 0.07326544357048675, policy loss: 2.992177890877599
Experience 2, Iter 49, disc loss: 0.06377343853685905, policy loss: 3.1301065426534613
Experience 2, Iter 50, disc loss: 0.06306763004809064, policy loss: 3.105831188014886
Experience 2, Iter 51, disc loss: 0.060376566472656185, policy loss: 3.164679868241496
Experience 2, Iter 52, disc loss: 0.06662216412521339, policy loss: 3.0386509337669714
Experience 2, Iter 53, disc loss: 0.0618899992063172, policy loss: 3.1546353023507825
Experience 2, Iter 54, disc loss: 0.06604990235891317, policy loss: 3.1275291521122317
Experience 2, Iter 55, disc loss: 0.05100503131070723, policy loss: 3.345784895004015
Experience 2, Iter 56, disc loss: 0.05255828937706289, policy loss: 3.3012937727554297
Experience 2, Iter 57, disc loss: 0.054998522214111764, policy loss: 3.2985679086250266
Experience 2, Iter 58, disc loss: 0.04913241992347055, policy loss: 3.375492492631946
Experience 2, Iter 59, disc loss: 0.050090135232714196, policy loss: 3.3770546757327233
Experience 2, Iter 60, disc loss: 0.04842241214909407, policy loss: 3.395857079035279
Experience 2, Iter 61, disc loss: 0.04856387336904873, policy loss: 3.3777151168924973
Experience 2, Iter 62, disc loss: 0.04642903823828206, policy loss: 3.461755377137962
Experience 2, Iter 63, disc loss: 0.043124129683065504, policy loss: 3.5164378223368136
Experience 2, Iter 64, disc loss: 0.044034004459155576, policy loss: 3.505222339776175
Experience 2, Iter 65, disc loss: 0.04082798326249419, policy loss: 3.599345394146992
Experience 2, Iter 66, disc loss: 0.03905300844223587, policy loss: 3.640516492652148
Experience 2, Iter 67, disc loss: 0.041055422165928994, policy loss: 3.5681740645030686
Experience 2, Iter 68, disc loss: 0.03806587299541225, policy loss: 3.6891380016869686
Experience 2, Iter 69, disc loss: 0.0394972130418141, policy loss: 3.618967544799184
Experience 2, Iter 70, disc loss: 0.03917108541772052, policy loss: 3.6639818751066793
Experience 2, Iter 71, disc loss: 0.04404264623739488, policy loss: 3.5619138965703883
Experience 2, Iter 72, disc loss: 0.03942451446172743, policy loss: 3.6181338955471656
Experience 2, Iter 73, disc loss: 0.03482788013419501, policy loss: 3.7333096346111825
Experience 2, Iter 74, disc loss: 0.03472763845821719, policy loss: 3.7909702253813884
Experience 2, Iter 75, disc loss: 0.03378517633205707, policy loss: 3.827025728218273
Experience 2, Iter 76, disc loss: 0.030631039017045995, policy loss: 3.919370204732421
Experience 2, Iter 77, disc loss: 0.0310914578138866, policy loss: 3.8865710724608262
Experience 2, Iter 78, disc loss: 0.030899843658527083, policy loss: 3.9104781247275793
Experience 2, Iter 79, disc loss: 0.033296786902833865, policy loss: 3.8141420563301827
Experience 2, Iter 80, disc loss: 0.03169198601389734, policy loss: 3.9495059101742864
Experience 2, Iter 81, disc loss: 0.028543807269309704, policy loss: 3.972750081995943
Experience 2, Iter 82, disc loss: 0.02901696429078257, policy loss: 3.9340255677537064
Experience 2, Iter 83, disc loss: 0.030660503539420195, policy loss: 3.955499902248377
Experience 2, Iter 84, disc loss: 0.029360248405199484, policy loss: 3.968433903383799
Experience 2, Iter 85, disc loss: 0.031640134789981206, policy loss: 3.9100221796219006
Experience 2, Iter 86, disc loss: 0.02898649042686373, policy loss: 3.974592930848123
Experience 2, Iter 87, disc loss: 0.026353216428207333, policy loss: 4.0547082368553795
Experience 2, Iter 88, disc loss: 0.03021908464135848, policy loss: 3.9837652801083605
Experience 2, Iter 89, disc loss: 0.025344994598306574, policy loss: 4.1198063849590225
Experience 2, Iter 90, disc loss: 0.025440547871852487, policy loss: 4.198620372466136
Experience 2, Iter 91, disc loss: 0.024991316080170668, policy loss: 4.1509206906533045
Experience 2, Iter 92, disc loss: 0.02842878073518396, policy loss: 4.07430866732157
Experience 2, Iter 93, disc loss: 0.02363438589625509, policy loss: 4.202652515931599
Experience 2, Iter 94, disc loss: 0.02150280960550604, policy loss: 4.308946555353023
Experience 2, Iter 95, disc loss: 0.024561540831324814, policy loss: 4.24650393584034
Experience 2, Iter 96, disc loss: 0.023405520879385276, policy loss: 4.177413173038878
Experience 2, Iter 97, disc loss: 0.024834107530434246, policy loss: 4.133416697079352
Experience 2, Iter 98, disc loss: 0.021488855389815577, policy loss: 4.267981914222934
Experience 2, Iter 99, disc loss: 0.024140233882121852, policy loss: 4.215924456548642
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0018],
        [0.0612],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0070, 0.2446, 0.0053], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0070, 0.2446, 0.0053])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.641
Iter 2/2000 - Loss: -1.591
Iter 3/2000 - Loss: -2.476
Iter 4/2000 - Loss: -1.906
Iter 5/2000 - Loss: -1.485
Iter 6/2000 - Loss: -1.681
Iter 7/2000 - Loss: -2.075
Iter 8/2000 - Loss: -2.313
Iter 9/2000 - Loss: -2.344
Iter 10/2000 - Loss: -2.298
Iter 11/2000 - Loss: -2.307
Iter 12/2000 - Loss: -2.392
Iter 13/2000 - Loss: -2.489
Iter 14/2000 - Loss: -2.527
Iter 15/2000 - Loss: -2.483
Iter 16/2000 - Loss: -2.400
Iter 17/2000 - Loss: -2.354
Iter 18/2000 - Loss: -2.405
Iter 19/2000 - Loss: -2.541
Iter 20/2000 - Loss: -2.690
Iter 1981/2000 - Loss: -2.990
Iter 1982/2000 - Loss: -2.990
Iter 1983/2000 - Loss: -2.990
Iter 1984/2000 - Loss: -2.990
Iter 1985/2000 - Loss: -2.990
Iter 1986/2000 - Loss: -2.990
Iter 1987/2000 - Loss: -2.990
Iter 1988/2000 - Loss: -2.990
Iter 1989/2000 - Loss: -2.990
Iter 1990/2000 - Loss: -2.990
Iter 1991/2000 - Loss: -2.990
Iter 1992/2000 - Loss: -2.990
Iter 1993/2000 - Loss: -2.990
Iter 1994/2000 - Loss: -2.990
Iter 1995/2000 - Loss: -2.990
Iter 1996/2000 - Loss: -2.990
Iter 1997/2000 - Loss: -2.990
Iter 1998/2000 - Loss: -2.990
Iter 1999/2000 - Loss: -2.990
Iter 2000/2000 - Loss: -2.990
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0007],
        [0.0014],
        [0.0455],
        [0.0010]])
Lengthscale: tensor([[[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]],

        [[1.2160e-02, 3.4056e-02, 5.5474e-02, 1.4463e-03, 6.1552e-05,
          3.3812e-02]]])
Signal Variance: tensor([0.0029, 0.0054, 0.1910, 0.0041])
Estimated target variance: tensor([0.0037, 0.0070, 0.2446, 0.0053])
N: 30
Signal to noise ratio: tensor([2.0012, 2.0028, 2.0493, 2.0017])
Bound on condition number: tensor([121.1459, 121.3314, 126.9874, 121.2093])
Policy Optimizer learning rate:
0.0009978950082958633
Experience 3, Iter 0, disc loss: 0.026682063118610686, policy loss: 4.130941271214859
Experience 3, Iter 1, disc loss: 0.022680044648118368, policy loss: 4.239600728268136
Experience 3, Iter 2, disc loss: 0.022078391118384205, policy loss: 4.25103539929445
Experience 3, Iter 3, disc loss: 0.023016441138640726, policy loss: 4.258403700101366
Experience 3, Iter 4, disc loss: 0.024556871571797124, policy loss: 4.210540090157151
Experience 3, Iter 5, disc loss: 0.021301708237496584, policy loss: 4.354409301546273
Experience 3, Iter 6, disc loss: 0.019976679834464724, policy loss: 4.484440568177247
Experience 3, Iter 7, disc loss: 0.02168087505424546, policy loss: 4.32386964832924
Experience 3, Iter 8, disc loss: 0.02139700954944224, policy loss: 4.291667950356183
Experience 3, Iter 9, disc loss: 0.019112235574019063, policy loss: 4.500110690631627
Experience 3, Iter 10, disc loss: 0.019100201628054606, policy loss: 4.4692488522032265
Experience 3, Iter 11, disc loss: 0.020077851587226282, policy loss: 4.510900243743407
Experience 3, Iter 12, disc loss: 0.021661280630979662, policy loss: 4.38463362245665
Experience 3, Iter 13, disc loss: 0.01681799851317907, policy loss: 4.584254414013079
Experience 3, Iter 14, disc loss: 0.016783316193526382, policy loss: 4.565904875394212
Experience 3, Iter 15, disc loss: 0.01886597368810498, policy loss: 4.515543481625565
Experience 3, Iter 16, disc loss: 0.017660279660905047, policy loss: 4.554377849895067
Experience 3, Iter 17, disc loss: 0.017059501849483634, policy loss: 4.5960794530570475
Experience 3, Iter 18, disc loss: 0.017108418413737274, policy loss: 4.547078621831087
Experience 3, Iter 19, disc loss: 0.017684960483184298, policy loss: 4.526362166359508
Experience 3, Iter 20, disc loss: 0.0202838353951292, policy loss: 4.572756270294948
Experience 3, Iter 21, disc loss: 0.01659667768489876, policy loss: 4.580875568664206
Experience 3, Iter 22, disc loss: 0.016682955708596146, policy loss: 4.615960287758074
Experience 3, Iter 23, disc loss: 0.016141694106088893, policy loss: 4.612858764670223
Experience 3, Iter 24, disc loss: 0.01819518892434585, policy loss: 4.5776659352506375
Experience 3, Iter 25, disc loss: 0.015079369645698077, policy loss: 4.715367300963495
Experience 3, Iter 26, disc loss: 0.016025525543994563, policy loss: 4.663975811551737
Experience 3, Iter 27, disc loss: 0.0166451581762582, policy loss: 4.599834515287096
Experience 3, Iter 28, disc loss: 0.01615379742365792, policy loss: 4.703690779911208
Experience 3, Iter 29, disc loss: 0.017523391752836206, policy loss: 4.627877472818127
Experience 3, Iter 30, disc loss: 0.016950863295866217, policy loss: 4.616822141194202
Experience 3, Iter 31, disc loss: 0.014165820514609049, policy loss: 4.816653757545449
Experience 3, Iter 32, disc loss: 0.01687217643300382, policy loss: 4.685247751280642
Experience 3, Iter 33, disc loss: 0.015914645307306827, policy loss: 4.685908444504866
Experience 3, Iter 34, disc loss: 0.015011593159725767, policy loss: 4.879926706786705
Experience 3, Iter 35, disc loss: 0.015790143769510843, policy loss: 4.794355029809619
Experience 3, Iter 36, disc loss: 0.013706651301542761, policy loss: 4.841493826604271
Experience 3, Iter 37, disc loss: 0.01551979501784858, policy loss: 4.797157788629891
Experience 3, Iter 38, disc loss: 0.015984026644877383, policy loss: 4.818596090572535
Experience 3, Iter 39, disc loss: 0.016581299442478232, policy loss: 4.817997981902639
Experience 3, Iter 40, disc loss: 0.01302683194310387, policy loss: 4.825863799576152
Experience 3, Iter 41, disc loss: 0.013289524097619167, policy loss: 4.943036042127666
Experience 3, Iter 42, disc loss: 0.012272029048978565, policy loss: 4.9300037929905995
Experience 3, Iter 43, disc loss: 0.01135780814112244, policy loss: 5.025085000031141
Experience 3, Iter 44, disc loss: 0.01211000491708406, policy loss: 5.002849003782186
Experience 3, Iter 45, disc loss: 0.012249733026839054, policy loss: 4.989668270057883
Experience 3, Iter 46, disc loss: 0.011573640873651675, policy loss: 5.091039883668234
Experience 3, Iter 47, disc loss: 0.010547931970235775, policy loss: 5.124267338446245
Experience 3, Iter 48, disc loss: 0.014228103781786154, policy loss: 4.788754117694243
Experience 3, Iter 49, disc loss: 0.01276930443380825, policy loss: 4.943601782757089
Experience 3, Iter 50, disc loss: 0.012452573775465532, policy loss: 5.003031107299887
Experience 3, Iter 51, disc loss: 0.012455464178200558, policy loss: 5.030675392302135
Experience 3, Iter 52, disc loss: 0.011832054661604637, policy loss: 5.036072303382293
Experience 3, Iter 53, disc loss: 0.010446581099471691, policy loss: 5.181240462516703
Experience 3, Iter 54, disc loss: 0.011935339982132961, policy loss: 4.9468582082163595
Experience 3, Iter 55, disc loss: 0.013309729294807792, policy loss: 4.947604436342306
Experience 3, Iter 56, disc loss: 0.010225633010201997, policy loss: 5.249707692628871
Experience 3, Iter 57, disc loss: 0.011518767750411617, policy loss: 5.073571838674779
Experience 3, Iter 58, disc loss: 0.01390663435435204, policy loss: 4.89900025200617
Experience 3, Iter 59, disc loss: 0.010740946503536682, policy loss: 5.068483646135795
Experience 3, Iter 60, disc loss: 0.009792361663386867, policy loss: 5.204314925594335
Experience 3, Iter 61, disc loss: 0.010540848753816728, policy loss: 5.156552607477696
Experience 3, Iter 62, disc loss: 0.011195345641036233, policy loss: 5.113022032782197
Experience 3, Iter 63, disc loss: 0.01229107629010145, policy loss: 5.133146851687174
Experience 3, Iter 64, disc loss: 0.009797871007920109, policy loss: 5.225712827512753
Experience 3, Iter 65, disc loss: 0.011172565006279623, policy loss: 5.148139206025594
Experience 3, Iter 66, disc loss: 0.010321782323662144, policy loss: 5.111511998252345
Experience 3, Iter 67, disc loss: 0.010221403197096964, policy loss: 5.177140155766619
Experience 3, Iter 68, disc loss: 0.010539818230039973, policy loss: 5.0910679587024905
Experience 3, Iter 69, disc loss: 0.011282971621285538, policy loss: 5.155278898409666
Experience 3, Iter 70, disc loss: 0.010823494854107692, policy loss: 5.1344433094386535
Experience 3, Iter 71, disc loss: 0.009593039932814632, policy loss: 5.23156696715767
Experience 3, Iter 72, disc loss: 0.009115943164934949, policy loss: 5.4070211136944195
Experience 3, Iter 73, disc loss: 0.011638900900374458, policy loss: 5.374988250144678
Experience 3, Iter 74, disc loss: 0.010643671563444045, policy loss: 5.225784207166881
Experience 3, Iter 75, disc loss: 0.01030286385924295, policy loss: 5.219436055655815
Experience 3, Iter 76, disc loss: 0.009540668963989004, policy loss: 5.316248895629259
Experience 3, Iter 77, disc loss: 0.008819797706120439, policy loss: 5.383114746486873
Experience 3, Iter 78, disc loss: 0.008489175841198092, policy loss: 5.440509183387942
Experience 3, Iter 79, disc loss: 0.008489590713742456, policy loss: 5.382377476847536
Experience 3, Iter 80, disc loss: 0.009517109732300771, policy loss: 5.354115275456353
Experience 3, Iter 81, disc loss: 0.007980327036299863, policy loss: 5.436711535507627
Experience 3, Iter 82, disc loss: 0.009275082515226846, policy loss: 5.364427267235141
Experience 3, Iter 83, disc loss: 0.008900623988458516, policy loss: 5.308336755868014
Experience 3, Iter 84, disc loss: 0.008138028093033904, policy loss: 5.475054388321913
Experience 3, Iter 85, disc loss: 0.008719759482170589, policy loss: 5.3693136078131705
Experience 3, Iter 86, disc loss: 0.00812286302341269, policy loss: 5.530370687417596
Experience 3, Iter 87, disc loss: 0.008665888834273208, policy loss: 5.495433991299495
Experience 3, Iter 88, disc loss: 0.010082098027313116, policy loss: 5.414721786663932
Experience 3, Iter 89, disc loss: 0.008956842497581614, policy loss: 5.331191710483981
Experience 3, Iter 90, disc loss: 0.008081935094575367, policy loss: 5.476751720809483
Experience 3, Iter 91, disc loss: 0.007864485993834968, policy loss: 5.612392571446922
Experience 3, Iter 92, disc loss: 0.008913001852157409, policy loss: 5.402163397173236
Experience 3, Iter 93, disc loss: 0.008039386193327712, policy loss: 5.413052967784422
Experience 3, Iter 94, disc loss: 0.007547028643422245, policy loss: 5.556630609057898
Experience 3, Iter 95, disc loss: 0.007840375873024523, policy loss: 5.501721820977712
Experience 3, Iter 96, disc loss: 0.006969124227988417, policy loss: 5.644945480811454
Experience 3, Iter 97, disc loss: 0.0074613486951960566, policy loss: 5.540239011451598
Experience 3, Iter 98, disc loss: 0.0072606815614598445, policy loss: 5.580625275726784
Experience 3, Iter 99, disc loss: 0.006975163017711078, policy loss: 5.729422158852672
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0027],
        [0.0467],
        [0.0010]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0402e-02, 4.4163e-02, 4.2465e-02, 1.6735e-03, 5.8132e-05,
          9.2476e-02]],

        [[1.0402e-02, 4.4163e-02, 4.2465e-02, 1.6735e-03, 5.8132e-05,
          9.2476e-02]],

        [[1.0402e-02, 4.4163e-02, 4.2465e-02, 1.6735e-03, 5.8132e-05,
          9.2476e-02]],

        [[1.0402e-02, 4.4163e-02, 4.2465e-02, 1.6735e-03, 5.8132e-05,
          9.2476e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0106, 0.1869, 0.0042], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0106, 0.1869, 0.0042])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.277
Iter 2/2000 - Loss: -1.643
Iter 3/2000 - Loss: -2.382
Iter 4/2000 - Loss: -2.069
Iter 5/2000 - Loss: -1.789
Iter 6/2000 - Loss: -2.008
Iter 7/2000 - Loss: -2.335
Iter 8/2000 - Loss: -2.507
Iter 9/2000 - Loss: -2.520
Iter 10/2000 - Loss: -2.459
Iter 11/2000 - Loss: -2.403
Iter 12/2000 - Loss: -2.393
Iter 13/2000 - Loss: -2.426
Iter 14/2000 - Loss: -2.499
Iter 15/2000 - Loss: -2.597
Iter 16/2000 - Loss: -2.684
Iter 17/2000 - Loss: -2.718
Iter 18/2000 - Loss: -2.691
Iter 19/2000 - Loss: -2.653
Iter 20/2000 - Loss: -2.670
Iter 1981/2000 - Loss: -8.060
Iter 1982/2000 - Loss: -8.060
Iter 1983/2000 - Loss: -8.061
Iter 1984/2000 - Loss: -8.061
Iter 1985/2000 - Loss: -8.061
Iter 1986/2000 - Loss: -8.061
Iter 1987/2000 - Loss: -8.061
Iter 1988/2000 - Loss: -8.061
Iter 1989/2000 - Loss: -8.061
Iter 1990/2000 - Loss: -8.061
Iter 1991/2000 - Loss: -8.061
Iter 1992/2000 - Loss: -8.061
Iter 1993/2000 - Loss: -8.061
Iter 1994/2000 - Loss: -8.061
Iter 1995/2000 - Loss: -8.061
Iter 1996/2000 - Loss: -8.061
Iter 1997/2000 - Loss: -8.061
Iter 1998/2000 - Loss: -8.061
Iter 1999/2000 - Loss: -8.061
Iter 2000/2000 - Loss: -8.061
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[18.8505,  2.2401, 35.8666, 14.4688,  2.1008, 26.0999]],

        [[34.1016, 40.4466, 22.0516,  1.3454,  4.8357,  6.4490]],

        [[32.2153, 61.9138, 13.8359,  0.9429,  3.7839,  9.3764]],

        [[27.3753, 44.8737,  5.7312,  1.5797,  5.1042, 14.4571]]])
Signal Variance: tensor([0.0201, 0.2884, 5.1405, 0.1004])
Estimated target variance: tensor([0.0046, 0.0106, 0.1869, 0.0042])
N: 40
Signal to noise ratio: tensor([ 8.8561, 28.9986, 47.6622, 17.0442])
Bound on condition number: tensor([ 3138.1956, 33637.7066, 90868.3153, 11621.1840])
Policy Optimizer learning rate:
0.0009968441746484834
Experience 4, Iter 0, disc loss: 0.06888358281592402, policy loss: 2.939388768508503
Experience 4, Iter 1, disc loss: 0.08030788806574789, policy loss: 2.7446382839711667
Experience 4, Iter 2, disc loss: 0.09120210076566254, policy loss: 2.61419430806635
Experience 4, Iter 3, disc loss: 0.10238639239758845, policy loss: 2.4940674396086324
Experience 4, Iter 4, disc loss: 0.13355222667851047, policy loss: 2.227973199651557
Experience 4, Iter 5, disc loss: 0.14252153632292214, policy loss: 2.190446370390164
Experience 4, Iter 6, disc loss: 0.14989653109565654, policy loss: 2.126916181757548
Experience 4, Iter 7, disc loss: 0.16460593262504286, policy loss: 2.031131208673285
Experience 4, Iter 8, disc loss: 0.18025935723606187, policy loss: 1.9277317124185376
Experience 4, Iter 9, disc loss: 0.19032488246660534, policy loss: 1.8829400427940541
Experience 4, Iter 10, disc loss: 0.16780530844366046, policy loss: 2.0367395009795457
Experience 4, Iter 11, disc loss: 0.19113746188542868, policy loss: 1.9914573812948455
Experience 4, Iter 12, disc loss: 0.17924536085162304, policy loss: 2.018820535166693
Experience 4, Iter 13, disc loss: 0.18398419555931272, policy loss: 1.9911193184668823
Experience 4, Iter 14, disc loss: 0.18496691473877475, policy loss: 2.006173434826274
Experience 4, Iter 15, disc loss: 0.16872056502364413, policy loss: 2.1694013258365685
Experience 4, Iter 16, disc loss: 0.14442471708103427, policy loss: 2.321306776331377
Experience 4, Iter 17, disc loss: 0.13520291960091807, policy loss: 2.3541150266998203
Experience 4, Iter 18, disc loss: 0.12753175797431765, policy loss: 2.404061414751337
Experience 4, Iter 19, disc loss: 0.14661266479385215, policy loss: 2.206535370743009
Experience 4, Iter 20, disc loss: 0.12063683517699701, policy loss: 2.424570836800147
Experience 4, Iter 21, disc loss: 0.11413533764190992, policy loss: 2.4353992033171803
Experience 4, Iter 22, disc loss: 0.11700133752535034, policy loss: 2.42630053295427
Experience 4, Iter 23, disc loss: 0.11066315162328699, policy loss: 2.4688916853570277
Experience 4, Iter 24, disc loss: 0.09794048298071882, policy loss: 2.570262062619583
Experience 4, Iter 25, disc loss: 0.10221203166640908, policy loss: 2.5572577693503526
Experience 4, Iter 26, disc loss: 0.10012031537195742, policy loss: 2.5535031112257154
Experience 4, Iter 27, disc loss: 0.08155799305077517, policy loss: 2.7455662813811266
Experience 4, Iter 28, disc loss: 0.08305082127516365, policy loss: 2.7516971791905354
Experience 4, Iter 29, disc loss: 0.09568749898103418, policy loss: 2.64155243993338
Experience 4, Iter 30, disc loss: 0.07994387311571655, policy loss: 2.8025644370100453
Experience 4, Iter 31, disc loss: 0.09108928145032712, policy loss: 2.7391902266223154
Experience 4, Iter 32, disc loss: 0.08802732429075937, policy loss: 2.6931204477316024
Experience 4, Iter 33, disc loss: 0.09676651032654679, policy loss: 2.7239845725410863
Experience 4, Iter 34, disc loss: 0.10925904189192741, policy loss: 2.6639842997388876
Experience 4, Iter 35, disc loss: 0.11183615507352515, policy loss: 2.698621900606786
Experience 4, Iter 36, disc loss: 0.132678477886956, policy loss: 2.7827032288218807
Experience 4, Iter 37, disc loss: 0.1462305689674561, policy loss: 2.55750555948733
Experience 4, Iter 38, disc loss: 0.14918335125987064, policy loss: 2.568403284666868
Experience 4, Iter 39, disc loss: 0.15863628650859718, policy loss: 2.6513382013943763
Experience 4, Iter 40, disc loss: 0.23277966067263245, policy loss: 2.3245184977648052
Experience 4, Iter 41, disc loss: 0.2029197672447963, policy loss: 2.683657853975255
Experience 4, Iter 42, disc loss: 0.16065008969350678, policy loss: 2.941666749690226
Experience 4, Iter 43, disc loss: 0.26023895034094996, policy loss: 2.3419525184290126
Experience 4, Iter 44, disc loss: 0.21687694693241627, policy loss: 2.5892006372243097
Experience 4, Iter 45, disc loss: 0.1923008699511523, policy loss: 2.808257470542664
Experience 4, Iter 46, disc loss: 0.21181022681394887, policy loss: 2.595040030279758
Experience 4, Iter 47, disc loss: 0.23100582894953106, policy loss: 2.7221098251964926
Experience 4, Iter 48, disc loss: 0.19382914158011572, policy loss: 3.040979676526025
Experience 4, Iter 49, disc loss: 0.2043570510954613, policy loss: 2.834269106733635
Experience 4, Iter 50, disc loss: 0.20033325604439475, policy loss: 2.9272871571974877
Experience 4, Iter 51, disc loss: 0.19233899251047182, policy loss: 2.892864753297555
Experience 4, Iter 52, disc loss: 0.18155738160401186, policy loss: 2.942581959148858
Experience 4, Iter 53, disc loss: 0.19115977334369036, policy loss: 2.845883704105767
Experience 4, Iter 54, disc loss: 0.1683713732406281, policy loss: 3.0720631003162833
Experience 4, Iter 55, disc loss: 0.1555588792817939, policy loss: 3.119554117762733
Experience 4, Iter 56, disc loss: 0.17073317282972683, policy loss: 2.8902201677796233
Experience 4, Iter 57, disc loss: 0.14450683294072664, policy loss: 3.0827692463505127
Experience 4, Iter 58, disc loss: 0.18849227286303188, policy loss: 2.6129007415128256
Experience 4, Iter 59, disc loss: 0.17735920502615884, policy loss: 3.2806321452614773
Experience 4, Iter 60, disc loss: 0.14340070614405076, policy loss: 3.1732299104301998
Experience 4, Iter 61, disc loss: 0.15870049156437302, policy loss: 3.0769596707184075
Experience 4, Iter 62, disc loss: 0.14840849053922445, policy loss: 3.3512631037636433
Experience 4, Iter 63, disc loss: 0.150402459971862, policy loss: 3.3513424792233186
Experience 4, Iter 64, disc loss: 0.13039126234021653, policy loss: 3.4246513948040134
Experience 4, Iter 65, disc loss: 0.18054694864832127, policy loss: 3.0690477618035823
Experience 4, Iter 66, disc loss: 0.13380284713156637, policy loss: 3.7558664434979745
Experience 4, Iter 67, disc loss: 0.12302825589122644, policy loss: 3.8743150914325426
Experience 4, Iter 68, disc loss: 0.15575204817798827, policy loss: 3.3309005204971607
Experience 4, Iter 69, disc loss: 0.14578556980136703, policy loss: 3.4474549742983274
Experience 4, Iter 70, disc loss: 0.11693933810888124, policy loss: 4.000741927717529
Experience 4, Iter 71, disc loss: 0.13091404430343798, policy loss: 3.3990122405550247
Experience 4, Iter 72, disc loss: 0.1506294406810884, policy loss: 3.316244890891393
Experience 4, Iter 73, disc loss: 0.13576649689922804, policy loss: 3.5801254775571345
Experience 4, Iter 74, disc loss: 0.1269690202400376, policy loss: 3.91004600864627
Experience 4, Iter 75, disc loss: 0.12597864260602554, policy loss: 3.512841005037051
Experience 4, Iter 76, disc loss: 0.12290200734275854, policy loss: 3.478001761148335
Experience 4, Iter 77, disc loss: 0.12114448040544012, policy loss: 3.602165415642581
Experience 4, Iter 78, disc loss: 0.09275796430050846, policy loss: 3.880766655442547
Experience 4, Iter 79, disc loss: 0.09561769369495166, policy loss: 4.276822642696439
Experience 4, Iter 80, disc loss: 0.12722316280664642, policy loss: 3.865549191160618
Experience 4, Iter 81, disc loss: 0.10874571778588821, policy loss: 4.21972889151201
Experience 4, Iter 82, disc loss: 0.0961446792369067, policy loss: 4.0713068274944995
Experience 4, Iter 83, disc loss: 0.08519843535419597, policy loss: 4.433542469915309
Experience 4, Iter 84, disc loss: 0.0922185432962238, policy loss: 4.17243947252202
Experience 4, Iter 85, disc loss: 0.08737717089810773, policy loss: 4.398353629124029
Experience 4, Iter 86, disc loss: 0.06445263439975385, policy loss: 5.410388564302637
Experience 4, Iter 87, disc loss: 0.10751380960078014, policy loss: 4.4091358363610595
Experience 4, Iter 88, disc loss: 0.07498225604941344, policy loss: 4.234934083353188
Experience 4, Iter 89, disc loss: 0.07320144291090155, policy loss: 4.652139299225254
Experience 4, Iter 90, disc loss: 0.06863942612592602, policy loss: 4.804379359192497
Experience 4, Iter 91, disc loss: 0.07894001935274256, policy loss: 4.290895571580167
Experience 4, Iter 92, disc loss: 0.06506704170101563, policy loss: 4.793832996272636
Experience 4, Iter 93, disc loss: 0.07529913071409382, policy loss: 4.26833241003383
Experience 4, Iter 94, disc loss: 0.05595688427344569, policy loss: 4.310009172689897
Experience 4, Iter 95, disc loss: 0.08979486627392498, policy loss: 4.4206448147820225
Experience 4, Iter 96, disc loss: 0.07208638678987173, policy loss: 4.189853017956546
Experience 4, Iter 97, disc loss: 0.06175260233535718, policy loss: 4.653192494957123
Experience 4, Iter 98, disc loss: 0.06374227480387626, policy loss: 4.667051629721863
Experience 4, Iter 99, disc loss: 0.06721933555161523, policy loss: 4.591970074909659
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0261],
        [0.3577],
        [0.0056]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0119, 0.0713, 0.2322, 0.0081, 0.0041, 0.7559]],

        [[0.0119, 0.0713, 0.2322, 0.0081, 0.0041, 0.7559]],

        [[0.0119, 0.0713, 0.2322, 0.0081, 0.0041, 0.7559]],

        [[0.0119, 0.0713, 0.2322, 0.0081, 0.0041, 0.7559]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0066, 0.1044, 1.4307, 0.0225], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0066, 0.1044, 1.4307, 0.0225])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.891
Iter 2/2000 - Loss: 0.446
Iter 3/2000 - Loss: 0.560
Iter 4/2000 - Loss: 0.592
Iter 5/2000 - Loss: 0.480
Iter 6/2000 - Loss: 0.343
Iter 7/2000 - Loss: 0.319
Iter 8/2000 - Loss: 0.380
Iter 9/2000 - Loss: 0.401
Iter 10/2000 - Loss: 0.328
Iter 11/2000 - Loss: 0.221
Iter 12/2000 - Loss: 0.156
Iter 13/2000 - Loss: 0.145
Iter 14/2000 - Loss: 0.136
Iter 15/2000 - Loss: 0.076
Iter 16/2000 - Loss: -0.026
Iter 17/2000 - Loss: -0.122
Iter 18/2000 - Loss: -0.187
Iter 19/2000 - Loss: -0.245
Iter 20/2000 - Loss: -0.337
Iter 1981/2000 - Loss: -7.191
Iter 1982/2000 - Loss: -7.191
Iter 1983/2000 - Loss: -7.191
Iter 1984/2000 - Loss: -7.191
Iter 1985/2000 - Loss: -7.191
Iter 1986/2000 - Loss: -7.191
Iter 1987/2000 - Loss: -7.191
Iter 1988/2000 - Loss: -7.191
Iter 1989/2000 - Loss: -7.191
Iter 1990/2000 - Loss: -7.191
Iter 1991/2000 - Loss: -7.192
Iter 1992/2000 - Loss: -7.192
Iter 1993/2000 - Loss: -7.192
Iter 1994/2000 - Loss: -7.192
Iter 1995/2000 - Loss: -7.192
Iter 1996/2000 - Loss: -7.192
Iter 1997/2000 - Loss: -7.192
Iter 1998/2000 - Loss: -7.192
Iter 1999/2000 - Loss: -7.192
Iter 2000/2000 - Loss: -7.192
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[15.2558,  2.5408, 41.2738, 14.0133, 17.9396, 35.3247]],

        [[19.3180, 32.6408,  7.9895,  1.6569,  3.3684,  7.3958]],

        [[23.4418, 41.2852, 19.6418,  1.3622, 16.7060, 20.1705]],

        [[19.7829, 33.6162, 11.1431,  2.0386, 10.5614, 28.7708]]])
Signal Variance: tensor([1.8698e-02, 7.0357e-01, 2.2147e+01, 2.2787e-01])
Estimated target variance: tensor([0.0066, 0.1044, 1.4307, 0.0225])
N: 50
Signal to noise ratio: tensor([ 7.9573, 47.5474, 99.7068, 26.4075])
Bound on condition number: tensor([  3166.9275, 113038.8047, 497073.1947,  34868.7484])
Policy Optimizer learning rate:
0.000995794447581801
Experience 5, Iter 0, disc loss: 0.10109051459233975, policy loss: 3.4367252547175884
Experience 5, Iter 1, disc loss: 0.17682363092881404, policy loss: 2.874865138844866
Experience 5, Iter 2, disc loss: 0.24427129413799858, policy loss: 2.249719694931403
Experience 5, Iter 3, disc loss: 0.26328168020701476, policy loss: 2.195413461727761
Experience 5, Iter 4, disc loss: 0.2788836187002468, policy loss: 2.2523643739504298
Experience 5, Iter 5, disc loss: 0.28728687213284326, policy loss: 2.27984934377975
Experience 5, Iter 6, disc loss: 0.30064327940986707, policy loss: 2.345419226397716
Experience 5, Iter 7, disc loss: 0.30769308358904424, policy loss: 2.4157932837845233
Experience 5, Iter 8, disc loss: 0.38025545481409284, policy loss: 2.1687690165555313
Experience 5, Iter 9, disc loss: 0.35337759713240297, policy loss: 2.33437009808493
Experience 5, Iter 10, disc loss: 0.4189317938952209, policy loss: 1.9482564924928927
Experience 5, Iter 11, disc loss: 0.3973856815052425, policy loss: 1.9678575572419863
Experience 5, Iter 12, disc loss: 0.4612345115541495, policy loss: 1.683172380005812
Experience 5, Iter 13, disc loss: 0.4787230088801432, policy loss: 1.6737289701032068
Experience 5, Iter 14, disc loss: 0.4927546250141752, policy loss: 1.7623408253949089
Experience 5, Iter 15, disc loss: 0.543337246857756, policy loss: 1.6899624926904309
Experience 5, Iter 16, disc loss: 0.5121545592714487, policy loss: 2.076584181972211
Experience 5, Iter 17, disc loss: 0.44052286378322936, policy loss: 2.444500261285203
Experience 5, Iter 18, disc loss: 0.4275837649179628, policy loss: 2.5236064600851367
Experience 5, Iter 19, disc loss: 0.40493768288295195, policy loss: 2.533762345886288
Experience 5, Iter 20, disc loss: 0.36759083460599185, policy loss: 2.6806896337617716
Experience 5, Iter 21, disc loss: 0.315465648887582, policy loss: 2.763174147300587
Experience 5, Iter 22, disc loss: 0.33360727928707484, policy loss: 2.4455528069718886
Experience 5, Iter 23, disc loss: 0.2616834878322542, policy loss: 2.7130486099745577
Experience 5, Iter 24, disc loss: 0.32002429601185833, policy loss: 2.211886124886149
Experience 5, Iter 25, disc loss: 0.29254591947701025, policy loss: 2.438917438547622
Experience 5, Iter 26, disc loss: 0.24601597811152603, policy loss: 2.577718174074959
Experience 5, Iter 27, disc loss: 0.23727162002596092, policy loss: 2.5515641078354503
Experience 5, Iter 28, disc loss: 0.19804267344648732, policy loss: 2.9781485572832924
Experience 5, Iter 29, disc loss: 0.16541462282560254, policy loss: 3.192318478281175
Experience 5, Iter 30, disc loss: 0.19022179004945172, policy loss: 2.79807849304202
Experience 5, Iter 31, disc loss: 0.16802471829674748, policy loss: 2.924734033280241
Experience 5, Iter 32, disc loss: 0.16005926061161618, policy loss: 3.155285050320006
Experience 5, Iter 33, disc loss: 0.1534061668022691, policy loss: 3.0731991494686444
Experience 5, Iter 34, disc loss: 0.13412172140443449, policy loss: 3.2824946525410827
Experience 5, Iter 35, disc loss: 0.13218273220332988, policy loss: 3.2824076809977125
Experience 5, Iter 36, disc loss: 0.11815662521620632, policy loss: 3.208892589163858
Experience 5, Iter 37, disc loss: 0.11753659590323298, policy loss: 3.1704251065194
Experience 5, Iter 38, disc loss: 0.10632062736246488, policy loss: 3.309311255127867
Experience 5, Iter 39, disc loss: 0.12475997142874763, policy loss: 2.976833215966482
Experience 5, Iter 40, disc loss: 0.10845673910455925, policy loss: 3.3393004242061957
Experience 5, Iter 41, disc loss: 0.11781387403890281, policy loss: 3.202838995512439
Experience 5, Iter 42, disc loss: 0.11591825992034058, policy loss: 3.452428779205878
Experience 5, Iter 43, disc loss: 0.09522855876057656, policy loss: 3.5249453083417928
Experience 5, Iter 44, disc loss: 0.09461096201518313, policy loss: 3.5945692414524975
Experience 5, Iter 45, disc loss: 0.09976616462502973, policy loss: 3.7101661071904273
Experience 5, Iter 46, disc loss: 0.08315840651168561, policy loss: 3.9625311937187533
Experience 5, Iter 47, disc loss: 0.08204198622743815, policy loss: 3.7634477699809628
Experience 5, Iter 48, disc loss: 0.09499066107509327, policy loss: 3.550583330445238
Experience 5, Iter 49, disc loss: 0.09031545001549225, policy loss: 3.787564606685715
Experience 5, Iter 50, disc loss: 0.08727706461432652, policy loss: 3.7667787627572076
Experience 5, Iter 51, disc loss: 0.07339097232229637, policy loss: 4.329467979224278
Experience 5, Iter 52, disc loss: 0.07429247126908146, policy loss: 3.8368627394672403
Experience 5, Iter 53, disc loss: 0.08093111983723911, policy loss: 3.6049533116264763
Experience 5, Iter 54, disc loss: 0.09243326526267442, policy loss: 3.5954679700112786
Experience 5, Iter 55, disc loss: 0.0785389832357786, policy loss: 3.951884256865477
Experience 5, Iter 56, disc loss: 0.07213148434683714, policy loss: 3.8414139074701494
Experience 5, Iter 57, disc loss: 0.07425980855387744, policy loss: 3.768619099158758
Experience 5, Iter 58, disc loss: 0.05991570696202582, policy loss: 4.067509378581913
Experience 5, Iter 59, disc loss: 0.06977947749001003, policy loss: 3.8308897882213575
Experience 5, Iter 60, disc loss: 0.06092736484422652, policy loss: 3.8416890468565548
Experience 5, Iter 61, disc loss: 0.06332309778116114, policy loss: 3.842926447830402
Experience 5, Iter 62, disc loss: 0.06495957415202708, policy loss: 3.7825853876929703
Experience 5, Iter 63, disc loss: 0.0627579934891975, policy loss: 3.919708457588065
Experience 5, Iter 64, disc loss: 0.0714377016427298, policy loss: 3.7350604531804796
Experience 5, Iter 65, disc loss: 0.07125402335439401, policy loss: 3.7135561317085006
Experience 5, Iter 66, disc loss: 0.058464255908860736, policy loss: 4.051635342006562
Experience 5, Iter 67, disc loss: 0.06721880992127557, policy loss: 4.093030238286096
Experience 5, Iter 68, disc loss: 0.055894516341293346, policy loss: 4.174666002983493
Experience 5, Iter 69, disc loss: 0.056755946701609786, policy loss: 4.009128220309654
Experience 5, Iter 70, disc loss: 0.06385583290211268, policy loss: 3.9796490800355175
Experience 5, Iter 71, disc loss: 0.05340574496015339, policy loss: 4.32903393837294
Experience 5, Iter 72, disc loss: 0.05739650907853383, policy loss: 4.11338274023039
Experience 5, Iter 73, disc loss: 0.05679041971759362, policy loss: 4.208724022252475
Experience 5, Iter 74, disc loss: 0.05466534827810248, policy loss: 4.322677984037375
Experience 5, Iter 75, disc loss: 0.05666756690435955, policy loss: 4.137877923499604
Experience 5, Iter 76, disc loss: 0.05697416066394926, policy loss: 4.19810867715646
Experience 5, Iter 77, disc loss: 0.056437884648282526, policy loss: 4.018017052122165
Experience 5, Iter 78, disc loss: 0.053345735792614424, policy loss: 4.196599835792832
Experience 5, Iter 79, disc loss: 0.05073660251197676, policy loss: 4.153346801394813
Experience 5, Iter 80, disc loss: 0.054800134441247136, policy loss: 4.193828605595771
Experience 5, Iter 81, disc loss: 0.051360599632668605, policy loss: 4.167390575273586
Experience 5, Iter 82, disc loss: 0.05553223690593115, policy loss: 4.3311532210103305
Experience 5, Iter 83, disc loss: 0.04479234813672292, policy loss: 4.549817586088087
Experience 5, Iter 84, disc loss: 0.05440590423693646, policy loss: 3.9260101265845484
Experience 5, Iter 85, disc loss: 0.0481346239095878, policy loss: 4.21748539870614
Experience 5, Iter 86, disc loss: 0.048235034950969896, policy loss: 4.35714661470691
Experience 5, Iter 87, disc loss: 0.043262104983814824, policy loss: 4.271160418699232
Experience 5, Iter 88, disc loss: 0.048400401277667635, policy loss: 4.5146092932449715
Experience 5, Iter 89, disc loss: 0.04892820453356203, policy loss: 4.131862733112929
Experience 5, Iter 90, disc loss: 0.05051411709052196, policy loss: 4.059157687107633
Experience 5, Iter 91, disc loss: 0.046473982853467634, policy loss: 4.325206428825801
Experience 5, Iter 92, disc loss: 0.03925950060175842, policy loss: 4.529706505464288
Experience 5, Iter 93, disc loss: 0.04178796433401752, policy loss: 4.463867247477702
Experience 5, Iter 94, disc loss: 0.03998995438650233, policy loss: 4.442173189653316
Experience 5, Iter 95, disc loss: 0.040152357060869795, policy loss: 4.443691593958216
Experience 5, Iter 96, disc loss: 0.04307238617370478, policy loss: 4.43962075253264
Experience 5, Iter 97, disc loss: 0.04076590797536049, policy loss: 4.534176910961025
Experience 5, Iter 98, disc loss: 0.05600436920484267, policy loss: 4.150439671428856
Experience 5, Iter 99, disc loss: 0.042186339020393204, policy loss: 4.387295824676568
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0378],
        [0.5561],
        [0.0100]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0125, 0.0928, 0.4347, 0.0122, 0.0079, 1.2401]],

        [[0.0125, 0.0928, 0.4347, 0.0122, 0.0079, 1.2401]],

        [[0.0125, 0.0928, 0.4347, 0.0122, 0.0079, 1.2401]],

        [[0.0125, 0.0928, 0.4347, 0.0122, 0.0079, 1.2401]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0088, 0.1514, 2.2245, 0.0400], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0088, 0.1514, 2.2245, 0.0400])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.592
Iter 2/2000 - Loss: 1.186
Iter 3/2000 - Loss: 1.321
Iter 4/2000 - Loss: 1.283
Iter 5/2000 - Loss: 1.182
Iter 6/2000 - Loss: 1.117
Iter 7/2000 - Loss: 1.108
Iter 8/2000 - Loss: 1.114
Iter 9/2000 - Loss: 1.074
Iter 10/2000 - Loss: 0.990
Iter 11/2000 - Loss: 0.913
Iter 12/2000 - Loss: 0.877
Iter 13/2000 - Loss: 0.853
Iter 14/2000 - Loss: 0.788
Iter 15/2000 - Loss: 0.672
Iter 16/2000 - Loss: 0.543
Iter 17/2000 - Loss: 0.426
Iter 18/2000 - Loss: 0.310
Iter 19/2000 - Loss: 0.163
Iter 20/2000 - Loss: -0.022
Iter 1981/2000 - Loss: -7.068
Iter 1982/2000 - Loss: -7.068
Iter 1983/2000 - Loss: -7.068
Iter 1984/2000 - Loss: -7.068
Iter 1985/2000 - Loss: -7.069
Iter 1986/2000 - Loss: -7.069
Iter 1987/2000 - Loss: -7.069
Iter 1988/2000 - Loss: -7.069
Iter 1989/2000 - Loss: -7.069
Iter 1990/2000 - Loss: -7.069
Iter 1991/2000 - Loss: -7.069
Iter 1992/2000 - Loss: -7.069
Iter 1993/2000 - Loss: -7.069
Iter 1994/2000 - Loss: -7.069
Iter 1995/2000 - Loss: -7.069
Iter 1996/2000 - Loss: -7.069
Iter 1997/2000 - Loss: -7.069
Iter 1998/2000 - Loss: -7.069
Iter 1999/2000 - Loss: -7.069
Iter 2000/2000 - Loss: -7.069
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[15.9863,  4.3111, 48.1993, 13.2190, 23.6768, 38.1795]],

        [[17.7345, 27.4462, 11.9717,  1.3125,  2.7534,  7.3445]],

        [[21.1333, 32.7769,  9.9989,  1.1521,  3.8918, 11.7845]],

        [[17.5789, 29.7629, 11.5515,  2.3540, 20.2362, 35.9512]]])
Signal Variance: tensor([0.0449, 0.6093, 9.8944, 0.2797])
Estimated target variance: tensor([0.0088, 0.1514, 2.2245, 0.0400])
N: 60
Signal to noise ratio: tensor([12.3032, 45.5945, 68.1513, 28.3818])
Bound on condition number: tensor([  9083.1876, 124732.3266, 278676.5863,  48332.7465])
Policy Optimizer learning rate:
0.0009947458259305312
Experience 6, Iter 0, disc loss: 0.023592408459624466, policy loss: 5.698190929116719
Experience 6, Iter 1, disc loss: 0.02426829819375373, policy loss: 5.4596280539740505
Experience 6, Iter 2, disc loss: 0.023870403892920625, policy loss: 5.351747493355339
Experience 6, Iter 3, disc loss: 0.022369197690176548, policy loss: 5.400943826744207
Experience 6, Iter 4, disc loss: 0.020622347273712247, policy loss: 5.56040041381379
Experience 6, Iter 5, disc loss: 0.020899987556536298, policy loss: 5.371304365400214
Experience 6, Iter 6, disc loss: 0.01790401305116572, policy loss: 5.545427104747006
Experience 6, Iter 7, disc loss: 0.017847886107545424, policy loss: 5.279766451341597
Experience 6, Iter 8, disc loss: 0.0171307267546715, policy loss: 5.319508706994884
Experience 6, Iter 9, disc loss: 0.017656513589748145, policy loss: 5.042106010191231
Experience 6, Iter 10, disc loss: 0.01649690806062709, policy loss: 5.109679946247477
Experience 6, Iter 11, disc loss: 0.016144003910360125, policy loss: 5.039061399375211
Experience 6, Iter 12, disc loss: 0.014927718573371693, policy loss: 5.318686313397313
Experience 6, Iter 13, disc loss: 0.016805620630598317, policy loss: 4.957221618472346
Experience 6, Iter 14, disc loss: 0.01645220730360847, policy loss: 5.145337730571648
Experience 6, Iter 15, disc loss: 0.016635861632069125, policy loss: 5.002134273777317
Experience 6, Iter 16, disc loss: 0.016051274698013873, policy loss: 4.94233633042419
Experience 6, Iter 17, disc loss: 0.01992474007649669, policy loss: 4.43927252796078
Experience 6, Iter 18, disc loss: 0.016845525282982936, policy loss: 4.703094508526072
Experience 6, Iter 19, disc loss: 0.019080368884615912, policy loss: 4.653965136706027
Experience 6, Iter 20, disc loss: 0.01821709193821977, policy loss: 4.590284439179936
Experience 6, Iter 21, disc loss: 0.017677634765447817, policy loss: 4.671960019058625
Experience 6, Iter 22, disc loss: 0.017439979545553694, policy loss: 4.787763857437312
Experience 6, Iter 23, disc loss: 0.018167004520592167, policy loss: 4.667292578640821
Experience 6, Iter 24, disc loss: 0.018044718622977764, policy loss: 4.7274931283673585
Experience 6, Iter 25, disc loss: 0.016586491666465145, policy loss: 4.865472239184085
Experience 6, Iter 26, disc loss: 0.016807712606106497, policy loss: 5.023351029577019
Experience 6, Iter 27, disc loss: 0.017943627549150385, policy loss: 4.787430748537137
Experience 6, Iter 28, disc loss: 0.016890935017309275, policy loss: 4.892848179856584
Experience 6, Iter 29, disc loss: 0.018768780948644113, policy loss: 4.895834785766539
Experience 6, Iter 30, disc loss: 0.018157139898868926, policy loss: 4.8251327048183565
Experience 6, Iter 31, disc loss: 0.018211148684370064, policy loss: 4.769069167613067
Experience 6, Iter 32, disc loss: 0.018616297242760767, policy loss: 4.806624552376437
Experience 6, Iter 33, disc loss: 0.022174542339713162, policy loss: 4.709198130171123
Experience 6, Iter 34, disc loss: 0.03954815832275828, policy loss: 4.423046414697118
Experience 6, Iter 35, disc loss: 0.027364526371959297, policy loss: 4.531954535941182
Experience 6, Iter 36, disc loss: 0.03799502111467954, policy loss: 4.73053738353236
Experience 6, Iter 37, disc loss: 0.03341447995358936, policy loss: 4.620134740109352
Experience 6, Iter 38, disc loss: 0.030395917941876543, policy loss: 4.482662621416329
Experience 6, Iter 39, disc loss: 0.035169953877448014, policy loss: 4.3728203162255355
Experience 6, Iter 40, disc loss: 0.028982095524985835, policy loss: 4.806281946086089
Experience 6, Iter 41, disc loss: 0.03772734427385854, policy loss: 4.798128308930561
Experience 6, Iter 42, disc loss: 0.03178069133892156, policy loss: 4.58266271614261
Experience 6, Iter 43, disc loss: 0.03564018629856881, policy loss: 4.665056848046985
Experience 6, Iter 44, disc loss: 0.03238730585681779, policy loss: 4.739671231000058
Experience 6, Iter 45, disc loss: 0.029110357969147897, policy loss: 4.860983642256
Experience 6, Iter 46, disc loss: 0.03656532577098397, policy loss: 4.749896947820005
Experience 6, Iter 47, disc loss: 0.05524598892744225, policy loss: 4.835206973930481
Experience 6, Iter 48, disc loss: 0.033914141238824094, policy loss: 4.796106541235677
Experience 6, Iter 49, disc loss: 0.04836191849393508, policy loss: 4.497513839284556
Experience 6, Iter 50, disc loss: 0.0475517432454252, policy loss: 4.614096224879512
Experience 6, Iter 51, disc loss: 0.0354681937130532, policy loss: 4.884932657625177
Experience 6, Iter 52, disc loss: 0.037807301560721, policy loss: 4.686731576518289
Experience 6, Iter 53, disc loss: 0.03474845913618488, policy loss: 4.676225359471696
Experience 6, Iter 54, disc loss: 0.032294967878043715, policy loss: 4.858632838552198
Experience 6, Iter 55, disc loss: 0.029805801988858592, policy loss: 4.9788224873109135
Experience 6, Iter 56, disc loss: 0.030646409818100545, policy loss: 4.806857421992548
Experience 6, Iter 57, disc loss: 0.03566142487029529, policy loss: 4.461003067862439
Experience 6, Iter 58, disc loss: 0.028824678675697005, policy loss: 5.095450327421641
Experience 6, Iter 59, disc loss: 0.04292159950739313, policy loss: 4.211006285235619
Experience 6, Iter 60, disc loss: 0.03213596768496528, policy loss: 4.7958890927575055
Experience 6, Iter 61, disc loss: 0.030155532163429897, policy loss: 4.693915470739408
Experience 6, Iter 62, disc loss: 0.03265587057737872, policy loss: 4.65639434216535
Experience 6, Iter 63, disc loss: 0.03399822822186979, policy loss: 4.481966694612326
Experience 6, Iter 64, disc loss: 0.024300952324631257, policy loss: 5.25012139482948
Experience 6, Iter 65, disc loss: 0.027499899145403878, policy loss: 4.62868243619635
Experience 6, Iter 66, disc loss: 0.03385126718060477, policy loss: 4.7275307087288105
Experience 6, Iter 67, disc loss: 0.023898815572219012, policy loss: 5.017958854492669
Experience 6, Iter 68, disc loss: 0.041515520939620264, policy loss: 4.595358553476188
Experience 6, Iter 69, disc loss: 0.029994539272608547, policy loss: 4.812832929481086
Experience 6, Iter 70, disc loss: 0.02673060427835861, policy loss: 4.918646164955147
Experience 6, Iter 71, disc loss: 0.03263800437659742, policy loss: 4.487263023820461
Experience 6, Iter 72, disc loss: 0.02798491840937549, policy loss: 4.926164749820648
Experience 6, Iter 73, disc loss: 0.024340994743588355, policy loss: 4.861475882564867
Experience 6, Iter 74, disc loss: 0.02550117729106878, policy loss: 4.76897704941328
Experience 6, Iter 75, disc loss: 0.025955762108076083, policy loss: 4.982593234428676
Experience 6, Iter 76, disc loss: 0.029340190792236126, policy loss: 4.697680292505149
Experience 6, Iter 77, disc loss: 0.03477135030463309, policy loss: 5.053348132997671
Experience 6, Iter 78, disc loss: 0.02073539447780006, policy loss: 5.070147048624991
Experience 6, Iter 79, disc loss: 0.022823071707256323, policy loss: 4.970539999760781
Experience 6, Iter 80, disc loss: 0.019778531851126026, policy loss: 5.14520593990658
Experience 6, Iter 81, disc loss: 0.019990564146326967, policy loss: 5.186382997906648
Experience 6, Iter 82, disc loss: 0.019058338592340622, policy loss: 5.3602251531871
Experience 6, Iter 83, disc loss: 0.023959349641937848, policy loss: 4.832669120702576
Experience 6, Iter 84, disc loss: 0.02308056048149315, policy loss: 4.924597330426108
Experience 6, Iter 85, disc loss: 0.0230649131078371, policy loss: 4.865707234245706
Experience 6, Iter 86, disc loss: 0.01904401803084652, policy loss: 5.214378195837109
Experience 6, Iter 87, disc loss: 0.018564399618218125, policy loss: 5.5191624219779305
Experience 6, Iter 88, disc loss: 0.023493759909070003, policy loss: 5.023646868278898
Experience 6, Iter 89, disc loss: 0.023256857173876398, policy loss: 5.150511728699324
Experience 6, Iter 90, disc loss: 0.019357575767002904, policy loss: 5.346777852540824
Experience 6, Iter 91, disc loss: 0.020221041986938318, policy loss: 5.112627945094569
Experience 6, Iter 92, disc loss: 0.019937368024602078, policy loss: 5.128194401802489
Experience 6, Iter 93, disc loss: 0.01672878273725574, policy loss: 5.217609466939709
Experience 6, Iter 94, disc loss: 0.013502690213056018, policy loss: 5.568318815900929
Experience 6, Iter 95, disc loss: 0.015166571771652801, policy loss: 5.605682736768729
Experience 6, Iter 96, disc loss: 0.012105696122949136, policy loss: 5.700451551962736
Experience 6, Iter 97, disc loss: 0.013035889211608018, policy loss: 5.5996480793491745
Experience 6, Iter 98, disc loss: 0.015160639393980643, policy loss: 5.221391223804337
Experience 6, Iter 99, disc loss: 0.020596741372402014, policy loss: 4.901549151536359
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.0678],
        [0.8069],
        [0.0107]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0156, 0.1190, 0.4906, 0.0140, 0.0069, 1.8680]],

        [[0.0156, 0.1190, 0.4906, 0.0140, 0.0069, 1.8680]],

        [[0.0156, 0.1190, 0.4906, 0.0140, 0.0069, 1.8680]],

        [[0.0156, 0.1190, 0.4906, 0.0140, 0.0069, 1.8680]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0113, 0.2711, 3.2276, 0.0427], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0113, 0.2711, 3.2276, 0.0427])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.981
Iter 2/2000 - Loss: 1.833
Iter 3/2000 - Loss: 1.882
Iter 4/2000 - Loss: 1.767
Iter 5/2000 - Loss: 1.716
Iter 6/2000 - Loss: 1.748
Iter 7/2000 - Loss: 1.716
Iter 8/2000 - Loss: 1.630
Iter 9/2000 - Loss: 1.582
Iter 10/2000 - Loss: 1.573
Iter 11/2000 - Loss: 1.533
Iter 12/2000 - Loss: 1.447
Iter 13/2000 - Loss: 1.354
Iter 14/2000 - Loss: 1.274
Iter 15/2000 - Loss: 1.188
Iter 16/2000 - Loss: 1.074
Iter 17/2000 - Loss: 0.930
Iter 18/2000 - Loss: 0.768
Iter 19/2000 - Loss: 0.597
Iter 20/2000 - Loss: 0.417
Iter 1981/2000 - Loss: -7.050
Iter 1982/2000 - Loss: -7.050
Iter 1983/2000 - Loss: -7.050
Iter 1984/2000 - Loss: -7.050
Iter 1985/2000 - Loss: -7.050
Iter 1986/2000 - Loss: -7.050
Iter 1987/2000 - Loss: -7.050
Iter 1988/2000 - Loss: -7.050
Iter 1989/2000 - Loss: -7.050
Iter 1990/2000 - Loss: -7.050
Iter 1991/2000 - Loss: -7.050
Iter 1992/2000 - Loss: -7.050
Iter 1993/2000 - Loss: -7.050
Iter 1994/2000 - Loss: -7.051
Iter 1995/2000 - Loss: -7.051
Iter 1996/2000 - Loss: -7.051
Iter 1997/2000 - Loss: -7.051
Iter 1998/2000 - Loss: -7.051
Iter 1999/2000 - Loss: -7.051
Iter 2000/2000 - Loss: -7.051
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[17.6039,  7.1618, 54.1669, 13.4569, 20.6763, 51.5559]],

        [[19.7658, 34.3759, 10.5257,  1.3793,  4.2165, 24.7632]],

        [[22.1604, 38.5509,  9.5649,  1.0711,  2.8031, 22.2936]],

        [[18.0456, 32.5317,  9.3727,  1.1688, 13.5629, 39.8381]]])
Signal Variance: tensor([ 0.1031,  1.5926, 12.9201,  0.2018])
Estimated target variance: tensor([0.0113, 0.2711, 3.2276, 0.0427])
N: 70
Signal to noise ratio: tensor([19.1604, 68.4592, 76.2639, 26.1794])
Bound on condition number: tensor([ 25699.3850, 328066.8853, 407133.4590,  47976.3675])
Policy Optimizer learning rate:
0.0009936983085306159
Experience 7, Iter 0, disc loss: 0.014532538248295455, policy loss: 5.760042825419218
Experience 7, Iter 1, disc loss: 0.014374904819080016, policy loss: 5.629597628664982
Experience 7, Iter 2, disc loss: 0.01624743234160317, policy loss: 5.26677681600907
Experience 7, Iter 3, disc loss: 0.020593021105752014, policy loss: 4.609405395304345
Experience 7, Iter 4, disc loss: 0.01541377380894339, policy loss: 5.211614036102957
Experience 7, Iter 5, disc loss: 0.0119990926165214, policy loss: 5.457060016994088
Experience 7, Iter 6, disc loss: 0.010217701899049397, policy loss: 5.585675189970404
Experience 7, Iter 7, disc loss: 0.009447800547270395, policy loss: 5.876670711221882
Experience 7, Iter 8, disc loss: 0.01035413516079865, policy loss: 5.680749316950671
Experience 7, Iter 9, disc loss: 0.011507364491959226, policy loss: 5.303529172669223
Experience 7, Iter 10, disc loss: 0.016320670444060582, policy loss: 4.932824014639663
Experience 7, Iter 11, disc loss: 0.01377541387973244, policy loss: 5.185692141918306
Experience 7, Iter 12, disc loss: 0.014670020313521822, policy loss: 5.4293406643461815
Experience 7, Iter 13, disc loss: 0.010023040400857879, policy loss: 5.924265722396421
Experience 7, Iter 14, disc loss: 0.012662784867285268, policy loss: 5.857782360800847
Experience 7, Iter 15, disc loss: 0.010648051466302516, policy loss: 5.663272525054982
Experience 7, Iter 16, disc loss: 0.012940099483610305, policy loss: 5.352378462696243
Experience 7, Iter 17, disc loss: 0.016932225451465832, policy loss: 4.911505819182512
Experience 7, Iter 18, disc loss: 0.015190293811691579, policy loss: 5.117879326871527
Experience 7, Iter 19, disc loss: 0.013606397541153599, policy loss: 5.098448964545169
Experience 7, Iter 20, disc loss: 0.01241312513910074, policy loss: 5.305527700810339
Experience 7, Iter 21, disc loss: 0.009638615804417976, policy loss: 5.646024932335462
Experience 7, Iter 22, disc loss: 0.008779791173262951, policy loss: 5.89151534090455
Experience 7, Iter 23, disc loss: 0.010102441907725893, policy loss: 5.525589622054881
Experience 7, Iter 24, disc loss: 0.01032135393275738, policy loss: 5.491776883420276
Experience 7, Iter 25, disc loss: 0.01163851839095284, policy loss: 5.293454907309014
Experience 7, Iter 26, disc loss: 0.014242937711042786, policy loss: 4.975564449324685
Experience 7, Iter 27, disc loss: 0.01724571192632054, policy loss: 4.867251483427979
Experience 7, Iter 28, disc loss: 0.015224265268759416, policy loss: 5.066859819868224
Experience 7, Iter 29, disc loss: 0.011168636970878248, policy loss: 5.637064439029634
Experience 7, Iter 30, disc loss: 0.011785968598325751, policy loss: 5.4786581355713935
Experience 7, Iter 31, disc loss: 0.014062970055261076, policy loss: 5.11097014604431
Experience 7, Iter 32, disc loss: 0.015263320166668308, policy loss: 4.973382696972687
Experience 7, Iter 33, disc loss: 0.01350404212612683, policy loss: 5.07058188190816
Experience 7, Iter 34, disc loss: 0.012421501374214852, policy loss: 5.091323699877389
Experience 7, Iter 35, disc loss: 0.012523026929408704, policy loss: 5.081374232075484
Experience 7, Iter 36, disc loss: 0.011130760484917443, policy loss: 5.416681738484179
Experience 7, Iter 37, disc loss: 0.011757502844478336, policy loss: 5.351558253483692
Experience 7, Iter 38, disc loss: 0.010569358338032603, policy loss: 5.344506201627081
Experience 7, Iter 39, disc loss: 0.011071857585525877, policy loss: 5.182202625814247
Experience 7, Iter 40, disc loss: 0.012902319410985123, policy loss: 5.0390433233664185
Experience 7, Iter 41, disc loss: 0.012324864346187504, policy loss: 5.211013984233237
Experience 7, Iter 42, disc loss: 0.011645784005286989, policy loss: 5.472612164847511
Experience 7, Iter 43, disc loss: 0.012891040261808283, policy loss: 5.174301492189844
Experience 7, Iter 44, disc loss: 0.013647630913856033, policy loss: 5.106355793846157
Experience 7, Iter 45, disc loss: 0.013168827433679996, policy loss: 5.036136615504629
Experience 7, Iter 46, disc loss: 0.012818886122891547, policy loss: 5.130225185475519
Experience 7, Iter 47, disc loss: 0.012279385685718423, policy loss: 5.119445925768423
Experience 7, Iter 48, disc loss: 0.01237024913310329, policy loss: 5.232289283645207
Experience 7, Iter 49, disc loss: 0.011119231995737017, policy loss: 5.232900417915718
Experience 7, Iter 50, disc loss: 0.010527342949109059, policy loss: 5.477263255084989
Experience 7, Iter 51, disc loss: 0.010770425940949674, policy loss: 5.335318792498336
Experience 7, Iter 52, disc loss: 0.012081657583161202, policy loss: 5.149698413313897
Experience 7, Iter 53, disc loss: 0.011385065396309691, policy loss: 5.210187149418897
Experience 7, Iter 54, disc loss: 0.011851308596899071, policy loss: 5.121040189323363
Experience 7, Iter 55, disc loss: 0.0112262860558272, policy loss: 5.166117982234036
Experience 7, Iter 56, disc loss: 0.011449748920699474, policy loss: 5.270113683718592
Experience 7, Iter 57, disc loss: 0.010915336808558886, policy loss: 5.3307409118759885
Experience 7, Iter 58, disc loss: 0.011260114916284068, policy loss: 5.2991453641789565
Experience 7, Iter 59, disc loss: 0.010817491822147456, policy loss: 5.363789434919575
Experience 7, Iter 60, disc loss: 0.0105240903536246, policy loss: 5.360841942982631
Experience 7, Iter 61, disc loss: 0.011634916729334949, policy loss: 5.165124655429151
Experience 7, Iter 62, disc loss: 0.010647996562183688, policy loss: 5.280512908202582
Experience 7, Iter 63, disc loss: 0.01011483472228782, policy loss: 5.379285948958534
Experience 7, Iter 64, disc loss: 0.00952154722594177, policy loss: 5.494772215209819
Experience 7, Iter 65, disc loss: 0.011508801768413911, policy loss: 5.19697528254239
Experience 7, Iter 66, disc loss: 0.008888615986810988, policy loss: 5.602710962241236
Experience 7, Iter 67, disc loss: 0.008197695253283562, policy loss: 5.653608724061598
Experience 7, Iter 68, disc loss: 0.009549043714616318, policy loss: 5.450151623629102
Experience 7, Iter 69, disc loss: 0.009201895660659373, policy loss: 5.579712701886102
Experience 7, Iter 70, disc loss: 0.009730653595585375, policy loss: 5.438931111064548
Experience 7, Iter 71, disc loss: 0.010325680709233944, policy loss: 5.363838835271021
Experience 7, Iter 72, disc loss: 0.00852859944837436, policy loss: 5.775349637986395
Experience 7, Iter 73, disc loss: 0.009606192645938951, policy loss: 5.600308525750165
Experience 7, Iter 74, disc loss: 0.009078926745928372, policy loss: 5.8723232687460465
Experience 7, Iter 75, disc loss: 0.008695958333071995, policy loss: 5.621118518577024
Experience 7, Iter 76, disc loss: 0.008919131078199015, policy loss: 5.616519226071808
Experience 7, Iter 77, disc loss: 0.00980071612797838, policy loss: 5.336955610708921
Experience 7, Iter 78, disc loss: 0.009040106137608245, policy loss: 5.433410236694909
Experience 7, Iter 79, disc loss: 0.008574022431886463, policy loss: 5.632907510123218
Experience 7, Iter 80, disc loss: 0.009308146890549086, policy loss: 5.385701727456695
Experience 7, Iter 81, disc loss: 0.009089148140930307, policy loss: 5.397086004965516
Experience 7, Iter 82, disc loss: 0.008563928627403804, policy loss: 5.467205216647855
Experience 7, Iter 83, disc loss: 0.009019291152363103, policy loss: 5.461254257567227
Experience 7, Iter 84, disc loss: 0.008024312894257826, policy loss: 5.62683754218603
Experience 7, Iter 85, disc loss: 0.008280339979410089, policy loss: 5.659414005639627
Experience 7, Iter 86, disc loss: 0.008634720772516764, policy loss: 5.67122650682551
Experience 7, Iter 87, disc loss: 0.008326126592841158, policy loss: 5.590577260261749
Experience 7, Iter 88, disc loss: 0.009814078845275222, policy loss: 5.3647838436736865
Experience 7, Iter 89, disc loss: 0.008143680695644417, policy loss: 5.763832436821131
Experience 7, Iter 90, disc loss: 0.008413181093320309, policy loss: 5.433364177896632
Experience 7, Iter 91, disc loss: 0.007275773746719187, policy loss: 5.802854667111204
Experience 7, Iter 92, disc loss: 0.007584935822553292, policy loss: 5.653552372641851
Experience 7, Iter 93, disc loss: 0.008304298787673709, policy loss: 5.501044719275285
Experience 7, Iter 94, disc loss: 0.00776816154216012, policy loss: 5.718616654562968
Experience 7, Iter 95, disc loss: 0.0078122557387964, policy loss: 5.6256576587406055
Experience 7, Iter 96, disc loss: 0.008327176697188176, policy loss: 5.585205689892916
Experience 7, Iter 97, disc loss: 0.00917707602869142, policy loss: 5.342536042128685
Experience 7, Iter 98, disc loss: 0.008692042315127976, policy loss: 5.656210196863224
Experience 7, Iter 99, disc loss: 0.008513014604689149, policy loss: 5.649428288826579
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.0849],
        [0.9938],
        [0.0133]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0153, 0.1227, 0.6169, 0.0167, 0.0069, 2.1965]],

        [[0.0153, 0.1227, 0.6169, 0.0167, 0.0069, 2.1965]],

        [[0.0153, 0.1227, 0.6169, 0.0167, 0.0069, 2.1965]],

        [[0.0153, 0.1227, 0.6169, 0.0167, 0.0069, 2.1965]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.3396, 3.9750, 0.0531], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.3396, 3.9750, 0.0531])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.258
Iter 2/2000 - Loss: 2.177
Iter 3/2000 - Loss: 2.141
Iter 4/2000 - Loss: 2.054
Iter 5/2000 - Loss: 2.065
Iter 6/2000 - Loss: 2.063
Iter 7/2000 - Loss: 1.992
Iter 8/2000 - Loss: 1.936
Iter 9/2000 - Loss: 1.918
Iter 10/2000 - Loss: 1.883
Iter 11/2000 - Loss: 1.812
Iter 12/2000 - Loss: 1.734
Iter 13/2000 - Loss: 1.660
Iter 14/2000 - Loss: 1.575
Iter 15/2000 - Loss: 1.465
Iter 16/2000 - Loss: 1.331
Iter 17/2000 - Loss: 1.181
Iter 18/2000 - Loss: 1.019
Iter 19/2000 - Loss: 0.837
Iter 20/2000 - Loss: 0.633
Iter 1981/2000 - Loss: -7.151
Iter 1982/2000 - Loss: -7.151
Iter 1983/2000 - Loss: -7.151
Iter 1984/2000 - Loss: -7.151
Iter 1985/2000 - Loss: -7.151
Iter 1986/2000 - Loss: -7.151
Iter 1987/2000 - Loss: -7.151
Iter 1988/2000 - Loss: -7.151
Iter 1989/2000 - Loss: -7.151
Iter 1990/2000 - Loss: -7.151
Iter 1991/2000 - Loss: -7.151
Iter 1992/2000 - Loss: -7.151
Iter 1993/2000 - Loss: -7.152
Iter 1994/2000 - Loss: -7.152
Iter 1995/2000 - Loss: -7.152
Iter 1996/2000 - Loss: -7.152
Iter 1997/2000 - Loss: -7.152
Iter 1998/2000 - Loss: -7.152
Iter 1999/2000 - Loss: -7.152
Iter 2000/2000 - Loss: -7.152
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[16.2130,  6.8855, 51.4464, 14.2362, 19.3669, 51.7229]],

        [[20.4435, 38.5243, 10.6736,  1.2674,  4.7401, 27.1572]],

        [[22.4861, 38.7383, 10.3699,  1.0413,  2.8150, 24.1483]],

        [[18.3871, 30.4100, 12.5372,  3.5045,  1.2345, 41.8626]]])
Signal Variance: tensor([ 0.0851,  2.1296, 15.4198,  0.3270])
Estimated target variance: tensor([0.0114, 0.3396, 3.9750, 0.0531])
N: 80
Signal to noise ratio: tensor([17.9186, 84.8491, 87.3817, 34.1701])
Bound on condition number: tensor([ 25686.9991, 575950.2723, 610845.4486,  93408.4817])
Policy Optimizer learning rate:
0.0009926518942192228
Experience 8, Iter 0, disc loss: 0.008349731826356635, policy loss: 5.542604799581149
Experience 8, Iter 1, disc loss: 0.008685269340421093, policy loss: 5.332957617611795
Experience 8, Iter 2, disc loss: 0.007979639099761073, policy loss: 5.541356487689609
Experience 8, Iter 3, disc loss: 0.00868399886067538, policy loss: 5.429978098606762
Experience 8, Iter 4, disc loss: 0.008966330793638753, policy loss: 5.382389203788696
Experience 8, Iter 5, disc loss: 0.008188024298918652, policy loss: 5.573945243765066
Experience 8, Iter 6, disc loss: 0.007790654076437294, policy loss: 5.499246870169387
Experience 8, Iter 7, disc loss: 0.007727273160469162, policy loss: 5.642486729476843
Experience 8, Iter 8, disc loss: 0.007924297285184673, policy loss: 5.619619748553173
Experience 8, Iter 9, disc loss: 0.006663453545338341, policy loss: 6.035641084089145
Experience 8, Iter 10, disc loss: 0.007645101312986304, policy loss: 5.680512874319906
Experience 8, Iter 11, disc loss: 0.007208836613851661, policy loss: 5.559141550930028
Experience 8, Iter 12, disc loss: 0.006591024465042663, policy loss: 5.785132341572497
Experience 8, Iter 13, disc loss: 0.005054836798681895, policy loss: 6.1947920914378
Experience 8, Iter 14, disc loss: 0.00545147714848118, policy loss: 6.05748902954678
Experience 8, Iter 15, disc loss: 0.004441438520339078, policy loss: 6.534473877952694
Experience 8, Iter 16, disc loss: 0.005698701891879099, policy loss: 6.0060389410268105
Experience 8, Iter 17, disc loss: 0.006063691641164088, policy loss: 5.890593986833567
Experience 8, Iter 18, disc loss: 0.006572130409736047, policy loss: 5.697110658193658
Experience 8, Iter 19, disc loss: 0.006951100147841475, policy loss: 5.594649533776776
Experience 8, Iter 20, disc loss: 0.006611720583324096, policy loss: 5.617639746673566
Experience 8, Iter 21, disc loss: 0.006173433549148477, policy loss: 5.874180097399936
Experience 8, Iter 22, disc loss: 0.006944293973661649, policy loss: 5.68537392606484
Experience 8, Iter 23, disc loss: 0.006872122984414111, policy loss: 5.95489032908936
Experience 8, Iter 24, disc loss: 0.0069635437253070835, policy loss: 5.580132780210096
Experience 8, Iter 25, disc loss: 0.006403739019664184, policy loss: 5.725323628039758
Experience 8, Iter 26, disc loss: 0.0059459228767045885, policy loss: 5.776110277943776
Experience 8, Iter 27, disc loss: 0.006180204696903476, policy loss: 5.775811594678712
Experience 8, Iter 28, disc loss: 0.006191064788772179, policy loss: 5.799907370028957
Experience 8, Iter 29, disc loss: 0.006095155303624403, policy loss: 5.845334094746125
Experience 8, Iter 30, disc loss: 0.006596932482220165, policy loss: 5.598020422566478
Experience 8, Iter 31, disc loss: 0.006638886784096639, policy loss: 5.601558596114407
Experience 8, Iter 32, disc loss: 0.007603094632634631, policy loss: 5.404574887888359
Experience 8, Iter 33, disc loss: 0.006155408477797904, policy loss: 5.783719009074555
Experience 8, Iter 34, disc loss: 0.006199575576295934, policy loss: 5.802187284261917
Experience 8, Iter 35, disc loss: 0.005958485126939488, policy loss: 5.838589099679396
Experience 8, Iter 36, disc loss: 0.006237534168340805, policy loss: 5.738140294874535
Experience 8, Iter 37, disc loss: 0.006136078286065711, policy loss: 5.761176544973161
Experience 8, Iter 38, disc loss: 0.006217844917601704, policy loss: 5.708281389914306
Experience 8, Iter 39, disc loss: 0.006486980875287766, policy loss: 5.6118976021800275
Experience 8, Iter 40, disc loss: 0.006241033646019845, policy loss: 5.853004343397858
Experience 8, Iter 41, disc loss: 0.0057486930233593275, policy loss: 5.73176255742173
Experience 8, Iter 42, disc loss: 0.0063912226814880155, policy loss: 5.866484891363198
Experience 8, Iter 43, disc loss: 0.005953955277856567, policy loss: 5.820693698058838
Experience 8, Iter 44, disc loss: 0.006048126176782506, policy loss: 5.783977800003417
Experience 8, Iter 45, disc loss: 0.006466442671311617, policy loss: 5.567771246323624
Experience 8, Iter 46, disc loss: 0.006457103801744107, policy loss: 5.732509430612884
Experience 8, Iter 47, disc loss: 0.0062769080049134, policy loss: 5.830696205414569
Experience 8, Iter 48, disc loss: 0.005847565936245023, policy loss: 5.944691004033638
Experience 8, Iter 49, disc loss: 0.005645309496473725, policy loss: 5.859301147579121
Experience 8, Iter 50, disc loss: 0.005949897936656182, policy loss: 5.792083854237835
Experience 8, Iter 51, disc loss: 0.005353743671568979, policy loss: 6.030531987791614
Experience 8, Iter 52, disc loss: 0.005841066524970321, policy loss: 5.75363585206304
Experience 8, Iter 53, disc loss: 0.00579128223687175, policy loss: 5.819804908204376
Experience 8, Iter 54, disc loss: 0.005504372046495681, policy loss: 5.85169546079869
Experience 8, Iter 55, disc loss: 0.0051931760627910006, policy loss: 5.942359193077067
Experience 8, Iter 56, disc loss: 0.00544927569871819, policy loss: 6.043263476794681
Experience 8, Iter 57, disc loss: 0.005438292332001095, policy loss: 6.090883842751355
Experience 8, Iter 58, disc loss: 0.005176863705390392, policy loss: 5.9872855365004565
Experience 8, Iter 59, disc loss: 0.005738518106828469, policy loss: 5.7727338138130895
Experience 8, Iter 60, disc loss: 0.005712319680179127, policy loss: 5.688874054195159
Experience 8, Iter 61, disc loss: 0.005485365561898626, policy loss: 5.86059625058254
Experience 8, Iter 62, disc loss: 0.005078198704143095, policy loss: 6.048839116229152
Experience 8, Iter 63, disc loss: 0.005202225089376937, policy loss: 5.9317220940943525
Experience 8, Iter 64, disc loss: 0.0054266937125115435, policy loss: 5.847586124696454
Experience 8, Iter 65, disc loss: 0.00507270635240554, policy loss: 6.143764339603823
Experience 8, Iter 66, disc loss: 0.0053563573432365294, policy loss: 5.861079422260384
Experience 8, Iter 67, disc loss: 0.005863844766373418, policy loss: 5.712113707888022
Experience 8, Iter 68, disc loss: 0.005574928135336495, policy loss: 5.784331827572313
Experience 8, Iter 69, disc loss: 0.005371093165061546, policy loss: 5.9524160787816
Experience 8, Iter 70, disc loss: 0.0051047515930883235, policy loss: 5.874026441506222
Experience 8, Iter 71, disc loss: 0.0054573729302906954, policy loss: 5.761009150406597
Experience 8, Iter 72, disc loss: 0.005276539956344857, policy loss: 6.081900542496124
Experience 8, Iter 73, disc loss: 0.005367741074330836, policy loss: 6.060178420983655
Experience 8, Iter 74, disc loss: 0.005298160570181204, policy loss: 5.948123472711192
Experience 8, Iter 75, disc loss: 0.005091228406696718, policy loss: 5.968179375695678
Experience 8, Iter 76, disc loss: 0.0051429161107309955, policy loss: 5.891911351934262
Experience 8, Iter 77, disc loss: 0.00518092673400498, policy loss: 6.065760870020979
Experience 8, Iter 78, disc loss: 0.004909749573644417, policy loss: 5.947921238077754
Experience 8, Iter 79, disc loss: 0.005747315389012847, policy loss: 5.720254375293588
Experience 8, Iter 80, disc loss: 0.004796455318254447, policy loss: 5.924570728313883
Experience 8, Iter 81, disc loss: 0.005008746369636481, policy loss: 5.956763308710708
Experience 8, Iter 82, disc loss: 0.005240625287957204, policy loss: 5.928881235358524
Experience 8, Iter 83, disc loss: 0.004933202060041354, policy loss: 5.875586826228845
Experience 8, Iter 84, disc loss: 0.004922475644692987, policy loss: 5.954491086244534
Experience 8, Iter 85, disc loss: 0.004861741548243193, policy loss: 6.026395286908338
Experience 8, Iter 86, disc loss: 0.005009066374878231, policy loss: 5.848377214039692
Experience 8, Iter 87, disc loss: 0.004672930379138506, policy loss: 5.940272355163696
Experience 8, Iter 88, disc loss: 0.005108205103263052, policy loss: 5.878495022429206
Experience 8, Iter 89, disc loss: 0.005046151002044189, policy loss: 5.920128812020859
Experience 8, Iter 90, disc loss: 0.004493431348202651, policy loss: 6.097100119413703
Experience 8, Iter 91, disc loss: 0.004583795906195558, policy loss: 6.027590859570612
Experience 8, Iter 92, disc loss: 0.004366108841014179, policy loss: 6.2713280760651395
Experience 8, Iter 93, disc loss: 0.004377666231847944, policy loss: 6.081969881391266
Experience 8, Iter 94, disc loss: 0.0050622033299726095, policy loss: 5.919320787935602
Experience 8, Iter 95, disc loss: 0.004479774345703423, policy loss: 6.171211607351028
Experience 8, Iter 96, disc loss: 0.004602069956312845, policy loss: 5.970980617418939
Experience 8, Iter 97, disc loss: 0.004959763177960235, policy loss: 5.972772219185153
Experience 8, Iter 98, disc loss: 0.004399794996413301, policy loss: 6.1133684396854635
Experience 8, Iter 99, disc loss: 0.004260477120795355, policy loss: 6.137684978101643
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0029],
        [0.1025],
        [1.1603],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0145, 0.1264, 0.6967, 0.0181, 0.0063, 2.5330]],

        [[0.0145, 0.1264, 0.6967, 0.0181, 0.0063, 2.5330]],

        [[0.0145, 0.1264, 0.6967, 0.0181, 0.0063, 2.5330]],

        [[0.0145, 0.1264, 0.6967, 0.0181, 0.0063, 2.5330]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0117, 0.4102, 4.6412, 0.0583], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0117, 0.4102, 4.6412, 0.0583])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.443
Iter 2/2000 - Loss: 2.449
Iter 3/2000 - Loss: 2.343
Iter 4/2000 - Loss: 2.292
Iter 5/2000 - Loss: 2.318
Iter 6/2000 - Loss: 2.283
Iter 7/2000 - Loss: 2.211
Iter 8/2000 - Loss: 2.175
Iter 9/2000 - Loss: 2.156
Iter 10/2000 - Loss: 2.103
Iter 11/2000 - Loss: 2.022
Iter 12/2000 - Loss: 1.943
Iter 13/2000 - Loss: 1.866
Iter 14/2000 - Loss: 1.769
Iter 15/2000 - Loss: 1.643
Iter 16/2000 - Loss: 1.498
Iter 17/2000 - Loss: 1.341
Iter 18/2000 - Loss: 1.172
Iter 19/2000 - Loss: 0.982
Iter 20/2000 - Loss: 0.768
Iter 1981/2000 - Loss: -7.317
Iter 1982/2000 - Loss: -7.317
Iter 1983/2000 - Loss: -7.317
Iter 1984/2000 - Loss: -7.317
Iter 1985/2000 - Loss: -7.317
Iter 1986/2000 - Loss: -7.317
Iter 1987/2000 - Loss: -7.317
Iter 1988/2000 - Loss: -7.318
Iter 1989/2000 - Loss: -7.318
Iter 1990/2000 - Loss: -7.318
Iter 1991/2000 - Loss: -7.318
Iter 1992/2000 - Loss: -7.318
Iter 1993/2000 - Loss: -7.318
Iter 1994/2000 - Loss: -7.318
Iter 1995/2000 - Loss: -7.318
Iter 1996/2000 - Loss: -7.318
Iter 1997/2000 - Loss: -7.318
Iter 1998/2000 - Loss: -7.318
Iter 1999/2000 - Loss: -7.318
Iter 2000/2000 - Loss: -7.318
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0018],
        [0.0003]])
Lengthscale: tensor([[[15.0391,  6.8981, 52.0075, 13.0557, 18.3426, 51.7821]],

        [[19.3603, 38.8819, 10.5213,  1.2770,  4.3426, 29.4897]],

        [[21.3036, 38.5837, 10.2540,  1.1222,  2.3006, 21.5468]],

        [[17.0121, 30.6874, 12.4312,  3.3512,  1.2279, 40.5253]]])
Signal Variance: tensor([ 0.0883,  2.2725, 15.4195,  0.2888])
Estimated target variance: tensor([0.0117, 0.4102, 4.6412, 0.0583])
N: 90
Signal to noise ratio: tensor([17.6153, 89.3928, 92.6378, 33.1053])
Bound on condition number: tensor([ 27928.0342, 719197.3169, 772358.7551,  98637.2519])
Policy Optimizer learning rate:
0.0009916065818347441
Experience 9, Iter 0, disc loss: 0.00495609290354683, policy loss: 5.958640214131065
Experience 9, Iter 1, disc loss: 0.005246597626759506, policy loss: 5.841738928833548
Experience 9, Iter 2, disc loss: 0.00528938303703617, policy loss: 5.766829937272352
Experience 9, Iter 3, disc loss: 0.005386003323009177, policy loss: 5.68441377089382
Experience 9, Iter 4, disc loss: 0.004902613234570503, policy loss: 5.93202082995507
Experience 9, Iter 5, disc loss: 0.005160165821714692, policy loss: 5.788192489566965
Experience 9, Iter 6, disc loss: 0.005363190330134524, policy loss: 5.733160996788909
Experience 9, Iter 7, disc loss: 0.005296757234611519, policy loss: 5.84036521589362
Experience 9, Iter 8, disc loss: 0.005056618254481535, policy loss: 5.8673341904321585
Experience 9, Iter 9, disc loss: 0.004588539029123907, policy loss: 5.982921257102213
Experience 9, Iter 10, disc loss: 0.005092790782660925, policy loss: 5.934951808491086
Experience 9, Iter 11, disc loss: 0.0043181091784113765, policy loss: 6.084375657471252
Experience 9, Iter 12, disc loss: 0.004554495406977003, policy loss: 6.0171567280313685
Experience 9, Iter 13, disc loss: 0.005061668147549034, policy loss: 5.804839164163203
Experience 9, Iter 14, disc loss: 0.004768477966193512, policy loss: 5.999541049238031
Experience 9, Iter 15, disc loss: 0.004320394161638606, policy loss: 6.099349218575256
Experience 9, Iter 16, disc loss: 0.004448046917743479, policy loss: 5.973341489255921
Experience 9, Iter 17, disc loss: 0.004502497061453897, policy loss: 6.078569276406313
Experience 9, Iter 18, disc loss: 0.004863003906478043, policy loss: 5.84627355438468
Experience 9, Iter 19, disc loss: 0.004414433495270068, policy loss: 6.1114429929476515
Experience 9, Iter 20, disc loss: 0.004232952572934394, policy loss: 6.061708470682182
Experience 9, Iter 21, disc loss: 0.00437540741140487, policy loss: 6.074243699458388
Experience 9, Iter 22, disc loss: 0.004422369835883357, policy loss: 6.193605458838977
Experience 9, Iter 23, disc loss: 0.004740865678992255, policy loss: 5.942768753072956
Experience 9, Iter 24, disc loss: 0.004509038555275063, policy loss: 5.975095543795844
Experience 9, Iter 25, disc loss: 0.0038595954152367863, policy loss: 6.370959606152433
Experience 9, Iter 26, disc loss: 0.004238472341643638, policy loss: 6.060575282463189
Experience 9, Iter 27, disc loss: 0.004272898439719184, policy loss: 6.123579370369861
Experience 9, Iter 28, disc loss: 0.004536529317181552, policy loss: 6.037420606186193
Experience 9, Iter 29, disc loss: 0.004345853395623227, policy loss: 6.074966080368597
Experience 9, Iter 30, disc loss: 0.004416983487554569, policy loss: 5.988444613745637
Experience 9, Iter 31, disc loss: 0.004321870529257891, policy loss: 6.373542822850505
Experience 9, Iter 32, disc loss: 0.004088527131039068, policy loss: 6.168747080870382
Experience 9, Iter 33, disc loss: 0.004279341883272869, policy loss: 6.087137880583852
Experience 9, Iter 34, disc loss: 0.0043322224740316335, policy loss: 6.073942097840709
Experience 9, Iter 35, disc loss: 0.004217073763588864, policy loss: 6.052919700932982
Experience 9, Iter 36, disc loss: 0.004033062323231274, policy loss: 6.15073472806826
Experience 9, Iter 37, disc loss: 0.0035721900141915813, policy loss: 6.3886284591074505
Experience 9, Iter 38, disc loss: 0.004057612852794018, policy loss: 6.2080529112899425
Experience 9, Iter 39, disc loss: 0.00406029117362376, policy loss: 6.060397075805267
Experience 9, Iter 40, disc loss: 0.004064116855434728, policy loss: 6.109767649000645
Experience 9, Iter 41, disc loss: 0.0041004528023872765, policy loss: 6.135292554722699
Experience 9, Iter 42, disc loss: 0.003977874175927027, policy loss: 6.271687547256933
Experience 9, Iter 43, disc loss: 0.003939731652947019, policy loss: 6.167976946074776
Experience 9, Iter 44, disc loss: 0.0036885895519999995, policy loss: 6.303531249381383
Experience 9, Iter 45, disc loss: 0.003747831077778957, policy loss: 6.24841018114388
Experience 9, Iter 46, disc loss: 0.004026854568484228, policy loss: 6.078893818373088
Experience 9, Iter 47, disc loss: 0.0034887718223872892, policy loss: 6.320722726227308
Experience 9, Iter 48, disc loss: 0.003630192849214651, policy loss: 6.5034731142574325
Experience 9, Iter 49, disc loss: 0.003944085836989143, policy loss: 6.137126728935577
Experience 9, Iter 50, disc loss: 0.003835937040526751, policy loss: 6.247068998905293
Experience 9, Iter 51, disc loss: 0.0038467529194889352, policy loss: 6.169910179669204
Experience 9, Iter 52, disc loss: 0.0036870512398940246, policy loss: 6.4072506130105875
Experience 9, Iter 53, disc loss: 0.003635603343918151, policy loss: 6.282007725258136
Experience 9, Iter 54, disc loss: 0.003875075079477912, policy loss: 6.058245888667289
Experience 9, Iter 55, disc loss: 0.003666286511130079, policy loss: 6.289845749411094
Experience 9, Iter 56, disc loss: 0.0036733445088761416, policy loss: 6.289413428557733
Experience 9, Iter 57, disc loss: 0.0035441015733023765, policy loss: 6.323640527065052
Experience 9, Iter 58, disc loss: 0.0037753394129092597, policy loss: 6.11074578274282
Experience 9, Iter 59, disc loss: 0.0035926355347018772, policy loss: 6.294569779944336
Experience 9, Iter 60, disc loss: 0.003503420439082873, policy loss: 6.36589793820428
Experience 9, Iter 61, disc loss: 0.003473316248423141, policy loss: 6.326862533944714
Experience 9, Iter 62, disc loss: 0.003466099058151866, policy loss: 6.359414260352839
Experience 9, Iter 63, disc loss: 0.0034856751380689006, policy loss: 6.32488395609508
Experience 9, Iter 64, disc loss: 0.0035334589157060194, policy loss: 6.246899743585937
Experience 9, Iter 65, disc loss: 0.003921828466779053, policy loss: 6.1641445771130785
Experience 9, Iter 66, disc loss: 0.0035632610042645877, policy loss: 6.224312076367164
Experience 9, Iter 67, disc loss: 0.00341369202815528, policy loss: 6.370392845762396
Experience 9, Iter 68, disc loss: 0.003616547514563035, policy loss: 6.188322831653441
Experience 9, Iter 69, disc loss: 0.0034687449922801788, policy loss: 6.4202226632650365
Experience 9, Iter 70, disc loss: 0.0033005698398209994, policy loss: 6.491347422264189
Experience 9, Iter 71, disc loss: 0.0031947026886115235, policy loss: 6.388115318336875
Experience 9, Iter 72, disc loss: 0.0031574002977596493, policy loss: 6.493920731051084
Experience 9, Iter 73, disc loss: 0.0029205275662626657, policy loss: 6.59617457960041
Experience 9, Iter 74, disc loss: 0.0030567277338456214, policy loss: 6.430705848190701
Experience 9, Iter 75, disc loss: 0.0028164014626782673, policy loss: 6.5861790294038505
Experience 9, Iter 76, disc loss: 0.0029871206579438468, policy loss: 6.518454588355325
Experience 9, Iter 77, disc loss: 0.003070779031634738, policy loss: 6.44864666817287
Experience 9, Iter 78, disc loss: 0.0033024721554292657, policy loss: 6.316219641052163
Experience 9, Iter 79, disc loss: 0.003474123962397959, policy loss: 6.333745016347475
Experience 9, Iter 80, disc loss: 0.003225020556556239, policy loss: 6.35796552538666
Experience 9, Iter 81, disc loss: 0.0030663286475785265, policy loss: 6.450876071660552
Experience 9, Iter 82, disc loss: 0.002984116345278695, policy loss: 6.622073574999445
Experience 9, Iter 83, disc loss: 0.003261254071177556, policy loss: 6.379190277413085
Experience 9, Iter 84, disc loss: 0.003317559063316744, policy loss: 6.261185437828512
Experience 9, Iter 85, disc loss: 0.0029676526431702817, policy loss: 6.672983526238495
Experience 9, Iter 86, disc loss: 0.0028242834877187624, policy loss: 6.720093459855983
Experience 9, Iter 87, disc loss: 0.003067142251909837, policy loss: 6.478280135996751
Experience 9, Iter 88, disc loss: 0.0029557823480642533, policy loss: 6.5014232396393234
Experience 9, Iter 89, disc loss: 0.003049654544798706, policy loss: 6.414267293029227
Experience 9, Iter 90, disc loss: 0.0032169629592269503, policy loss: 6.309582279338329
Experience 9, Iter 91, disc loss: 0.0029670122575486905, policy loss: 6.376875957572212
Experience 9, Iter 92, disc loss: 0.0034529250838687253, policy loss: 6.160841994219975
Experience 9, Iter 93, disc loss: 0.003017516118968798, policy loss: 6.571753778180286
Experience 9, Iter 94, disc loss: 0.0030096474691770013, policy loss: 6.3831631926773555
Experience 9, Iter 95, disc loss: 0.0032092550364559145, policy loss: 6.331497322527007
Experience 9, Iter 96, disc loss: 0.0028749212283064136, policy loss: 6.506511805453197
Experience 9, Iter 97, disc loss: 0.0029701407325171663, policy loss: 6.743273773931442
Experience 9, Iter 98, disc loss: 0.002817530053190766, policy loss: 6.550136504089275
Experience 9, Iter 99, disc loss: 0.002687571351584866, policy loss: 6.6339964394296835
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1179],
        [1.3118],
        [0.0156]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0137, 0.1251, 0.7609, 0.0188, 0.0058, 2.7899]],

        [[0.0137, 0.1251, 0.7609, 0.0188, 0.0058, 2.7899]],

        [[0.0137, 0.1251, 0.7609, 0.0188, 0.0058, 2.7899]],

        [[0.0137, 0.1251, 0.7609, 0.0188, 0.0058, 2.7899]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0114, 0.4714, 5.2470, 0.0623], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0114, 0.4714, 5.2470, 0.0623])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.575
Iter 2/2000 - Loss: 2.636
Iter 3/2000 - Loss: 2.485
Iter 4/2000 - Loss: 2.453
Iter 5/2000 - Loss: 2.481
Iter 6/2000 - Loss: 2.426
Iter 7/2000 - Loss: 2.340
Iter 8/2000 - Loss: 2.293
Iter 9/2000 - Loss: 2.266
Iter 10/2000 - Loss: 2.200
Iter 11/2000 - Loss: 2.097
Iter 12/2000 - Loss: 1.992
Iter 13/2000 - Loss: 1.895
Iter 14/2000 - Loss: 1.783
Iter 15/2000 - Loss: 1.640
Iter 16/2000 - Loss: 1.472
Iter 17/2000 - Loss: 1.292
Iter 18/2000 - Loss: 1.107
Iter 19/2000 - Loss: 0.909
Iter 20/2000 - Loss: 0.690
Iter 1981/2000 - Loss: -7.488
Iter 1982/2000 - Loss: -7.488
Iter 1983/2000 - Loss: -7.488
Iter 1984/2000 - Loss: -7.488
Iter 1985/2000 - Loss: -7.489
Iter 1986/2000 - Loss: -7.489
Iter 1987/2000 - Loss: -7.489
Iter 1988/2000 - Loss: -7.489
Iter 1989/2000 - Loss: -7.489
Iter 1990/2000 - Loss: -7.489
Iter 1991/2000 - Loss: -7.489
Iter 1992/2000 - Loss: -7.489
Iter 1993/2000 - Loss: -7.489
Iter 1994/2000 - Loss: -7.489
Iter 1995/2000 - Loss: -7.489
Iter 1996/2000 - Loss: -7.489
Iter 1997/2000 - Loss: -7.489
Iter 1998/2000 - Loss: -7.489
Iter 1999/2000 - Loss: -7.489
Iter 2000/2000 - Loss: -7.489
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.7108,  6.6036, 54.8952, 13.4403, 17.5065, 50.8692]],

        [[17.7724, 37.2892, 11.0205,  1.2267,  4.3088, 28.5743]],

        [[21.7293, 38.2325,  9.8278,  1.1401,  2.1565, 24.0072]],

        [[16.8633, 30.5430, 14.6374,  4.0214,  1.3110, 44.1502]]])
Signal Variance: tensor([ 0.0794,  2.2784, 16.2380,  0.3848])
Estimated target variance: tensor([0.0114, 0.4714, 5.2470, 0.0623])
N: 100
Signal to noise ratio: tensor([17.0557, 87.5025, 97.5666, 37.6877])
Bound on condition number: tensor([ 29090.6930, 765669.2752, 951925.5613, 142037.4143])
Policy Optimizer learning rate:
0.0009905623702167955
Experience 10, Iter 0, disc loss: 0.002947828825925202, policy loss: 6.448827200469918
Experience 10, Iter 1, disc loss: 0.002947199551685577, policy loss: 6.383088209973464
Experience 10, Iter 2, disc loss: 0.0029659380477695327, policy loss: 6.4696478617241215
Experience 10, Iter 3, disc loss: 0.0028055284226556464, policy loss: 6.59029008959762
Experience 10, Iter 4, disc loss: 0.00275686782127223, policy loss: 6.4869541322537785
Experience 10, Iter 5, disc loss: 0.0025054936992877663, policy loss: 6.742865262290065
Experience 10, Iter 6, disc loss: 0.0031237354088289393, policy loss: 6.282526180168957
Experience 10, Iter 7, disc loss: 0.0028127197341599563, policy loss: 6.528538628768461
Experience 10, Iter 8, disc loss: 0.002892528355834737, policy loss: 6.451292013804553
Experience 10, Iter 9, disc loss: 0.002579790063857098, policy loss: 6.656859985967241
Experience 10, Iter 10, disc loss: 0.002450901117922656, policy loss: 6.757757007212307
Experience 10, Iter 11, disc loss: 0.0027685956174234687, policy loss: 6.502092224985557
Experience 10, Iter 12, disc loss: 0.0026339065338467455, policy loss: 6.6037893644316386
Experience 10, Iter 13, disc loss: 0.0030823235409224133, policy loss: 6.379603902437548
Experience 10, Iter 14, disc loss: 0.002723222924935145, policy loss: 6.492879038545985
Experience 10, Iter 15, disc loss: 0.002641438735711847, policy loss: 6.609877369087343
Experience 10, Iter 16, disc loss: 0.002621736814781338, policy loss: 6.584794046450995
Experience 10, Iter 17, disc loss: 0.0028458267022845097, policy loss: 6.459398601097661
Experience 10, Iter 18, disc loss: 0.0029659903359321466, policy loss: 6.348426058991283
Experience 10, Iter 19, disc loss: 0.0026409002949745602, policy loss: 6.545996808953597
Experience 10, Iter 20, disc loss: 0.0026988229961323004, policy loss: 6.521642621069235
Experience 10, Iter 21, disc loss: 0.002801355586589714, policy loss: 6.445705311389652
Experience 10, Iter 22, disc loss: 0.0028058251898135725, policy loss: 6.40391680056365
Experience 10, Iter 23, disc loss: 0.0028069394899001296, policy loss: 6.44320737762413
Experience 10, Iter 24, disc loss: 0.00268900437838519, policy loss: 6.50778782320085
Experience 10, Iter 25, disc loss: 0.0026517837911137067, policy loss: 6.502490502848017
Experience 10, Iter 26, disc loss: 0.0028248653962847565, policy loss: 6.435883583121097
Experience 10, Iter 27, disc loss: 0.0026701417380487606, policy loss: 6.460778649110535
Experience 10, Iter 28, disc loss: 0.002522971147850739, policy loss: 6.65257849368046
Experience 10, Iter 29, disc loss: 0.00283598515658272, policy loss: 6.417337550646895
Experience 10, Iter 30, disc loss: 0.002807385272971242, policy loss: 6.528863948942144
Experience 10, Iter 31, disc loss: 0.002722604593504097, policy loss: 6.4169604124877155
Experience 10, Iter 32, disc loss: 0.0027492678168214196, policy loss: 6.425818920921083
Experience 10, Iter 33, disc loss: 0.0028077646071899267, policy loss: 6.3950803273277685
Experience 10, Iter 34, disc loss: 0.002677874335245589, policy loss: 6.447695318416448
Experience 10, Iter 35, disc loss: 0.0027222723255311246, policy loss: 6.5070567792771286
Experience 10, Iter 36, disc loss: 0.0026074647219404825, policy loss: 6.54624837972597
Experience 10, Iter 37, disc loss: 0.0026185260653989297, policy loss: 6.51194822061337
Experience 10, Iter 38, disc loss: 0.0027259547233428744, policy loss: 6.4759412135718595
Experience 10, Iter 39, disc loss: 0.0029091624920814813, policy loss: 6.4249920939767025
Experience 10, Iter 40, disc loss: 0.0026340382450949634, policy loss: 6.640082722653622
Experience 10, Iter 41, disc loss: 0.0027520298680176053, policy loss: 6.4940688958294785
Experience 10, Iter 42, disc loss: 0.0025119088984877453, policy loss: 6.605498712469778
Experience 10, Iter 43, disc loss: 0.00236759836608952, policy loss: 6.7764008884132725
Experience 10, Iter 44, disc loss: 0.002757029607250371, policy loss: 6.4483775189448895
Experience 10, Iter 45, disc loss: 0.002389124753856542, policy loss: 6.664621936045787
Experience 10, Iter 46, disc loss: 0.0021474616365492775, policy loss: 6.9007145924130135
Experience 10, Iter 47, disc loss: 0.002374099456134388, policy loss: 6.673961790203272
Experience 10, Iter 48, disc loss: 0.002197362880519023, policy loss: 6.766023727890563
Experience 10, Iter 49, disc loss: 0.002528747491627088, policy loss: 6.611165698236148
Experience 10, Iter 50, disc loss: 0.0026080981645100624, policy loss: 6.532971984614053
Experience 10, Iter 51, disc loss: 0.0026691003492318346, policy loss: 6.490909301110585
Experience 10, Iter 52, disc loss: 0.002428904219271743, policy loss: 6.664509676662319
Experience 10, Iter 53, disc loss: 0.002581490859269915, policy loss: 6.622304849960363
Experience 10, Iter 54, disc loss: 0.0024951805564601116, policy loss: 6.573598979707532
Experience 10, Iter 55, disc loss: 0.0025572892661629753, policy loss: 6.545777017762765
Experience 10, Iter 56, disc loss: 0.002707398911920799, policy loss: 6.450615417977014
Experience 10, Iter 57, disc loss: 0.0024715241714908715, policy loss: 6.630335412614726
Experience 10, Iter 58, disc loss: 0.0025509477123779366, policy loss: 6.56221832776891
Experience 10, Iter 59, disc loss: 0.0025682040704061958, policy loss: 6.529232230501419
Experience 10, Iter 60, disc loss: 0.002296677243175171, policy loss: 6.705593267997384
Experience 10, Iter 61, disc loss: 0.0023654270840111286, policy loss: 6.683515968955055
Experience 10, Iter 62, disc loss: 0.00232131331566204, policy loss: 6.732393236087837
Experience 10, Iter 63, disc loss: 0.0024689705515095932, policy loss: 6.659412548906737
Experience 10, Iter 64, disc loss: 0.0025473785795243975, policy loss: 6.562569179374327
Experience 10, Iter 65, disc loss: 0.0024419862537651427, policy loss: 6.71151747320295
Experience 10, Iter 66, disc loss: 0.002540835829462945, policy loss: 6.628517743482353
Experience 10, Iter 67, disc loss: 0.0025722947210235275, policy loss: 6.521161257937909
Experience 10, Iter 68, disc loss: 0.0025190994798968075, policy loss: 6.739751082781397
Experience 10, Iter 69, disc loss: 0.0025466287898839816, policy loss: 6.63343472278034
Experience 10, Iter 70, disc loss: 0.0023399996941361944, policy loss: 6.674064344764497
Experience 10, Iter 71, disc loss: 0.0024204039925506083, policy loss: 6.585985734210816
Experience 10, Iter 72, disc loss: 0.002114062446413798, policy loss: 6.79360937419516
Experience 10, Iter 73, disc loss: 0.002293916557696037, policy loss: 6.766781490907361
Experience 10, Iter 74, disc loss: 0.002143027058743877, policy loss: 6.810744295979893
Experience 10, Iter 75, disc loss: 0.002272709781620864, policy loss: 6.648080885858416
Experience 10, Iter 76, disc loss: 0.002140622711769314, policy loss: 6.811613425745141
Experience 10, Iter 77, disc loss: 0.0022466201901464727, policy loss: 6.674409455069945
Experience 10, Iter 78, disc loss: 0.002482997024280489, policy loss: 6.566753249037099
Experience 10, Iter 79, disc loss: 0.002612774305872709, policy loss: 6.559578958585097
Experience 10, Iter 80, disc loss: 0.0022961035657273338, policy loss: 6.681535966995118
Experience 10, Iter 81, disc loss: 0.002143531145957816, policy loss: 6.750522598554216
Experience 10, Iter 82, disc loss: 0.002217100532851158, policy loss: 6.7996702054712514
Experience 10, Iter 83, disc loss: 0.0023532170855502304, policy loss: 6.744853960914643
Experience 10, Iter 84, disc loss: 0.0024497269150767285, policy loss: 6.5689376259359795
Experience 10, Iter 85, disc loss: 0.0023318508634008776, policy loss: 6.707096026533346
Experience 10, Iter 86, disc loss: 0.0021104289726278994, policy loss: 6.833054720935374
Experience 10, Iter 87, disc loss: 0.002003601972550212, policy loss: 6.8532306063381325
Experience 10, Iter 88, disc loss: 0.001946633123438952, policy loss: 7.017537723725655
Experience 10, Iter 89, disc loss: 0.002098464362111793, policy loss: 6.775833792526867
Experience 10, Iter 90, disc loss: 0.0022491426582449403, policy loss: 6.732406175377824
Experience 10, Iter 91, disc loss: 0.002645997607070948, policy loss: 6.4398342147612695
Experience 10, Iter 92, disc loss: 0.0022921297600639445, policy loss: 6.786177320736889
Experience 10, Iter 93, disc loss: 0.0020833928641043045, policy loss: 6.882240637465455
Experience 10, Iter 94, disc loss: 0.0020630493114356975, policy loss: 6.934725482749747
Experience 10, Iter 95, disc loss: 0.0022461926203996866, policy loss: 6.670516039799195
Experience 10, Iter 96, disc loss: 0.0023558856000531854, policy loss: 6.74199166791376
Experience 10, Iter 97, disc loss: 0.002150443112526115, policy loss: 6.7216487498823945
Experience 10, Iter 98, disc loss: 0.0021935842819652573, policy loss: 6.7381116313772615
Experience 10, Iter 99, disc loss: 0.002048204411450491, policy loss: 6.857692513695784
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0028],
        [0.1295],
        [1.4231],
        [0.0161]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0132, 0.1224, 0.8008, 0.0192, 0.0053, 3.0098]],

        [[0.0132, 0.1224, 0.8008, 0.0192, 0.0053, 3.0098]],

        [[0.0132, 0.1224, 0.8008, 0.0192, 0.0053, 3.0098]],

        [[0.0132, 0.1224, 0.8008, 0.0192, 0.0053, 3.0098]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0112, 0.5182, 5.6923, 0.0643], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0112, 0.5182, 5.6923, 0.0643])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.664
Iter 2/2000 - Loss: 2.755
Iter 3/2000 - Loss: 2.574
Iter 4/2000 - Loss: 2.551
Iter 5/2000 - Loss: 2.582
Iter 6/2000 - Loss: 2.518
Iter 7/2000 - Loss: 2.423
Iter 8/2000 - Loss: 2.369
Iter 9/2000 - Loss: 2.341
Iter 10/2000 - Loss: 2.278
Iter 11/2000 - Loss: 2.169
Iter 12/2000 - Loss: 2.052
Iter 13/2000 - Loss: 1.946
Iter 14/2000 - Loss: 1.835
Iter 15/2000 - Loss: 1.694
Iter 16/2000 - Loss: 1.522
Iter 17/2000 - Loss: 1.333
Iter 18/2000 - Loss: 1.139
Iter 19/2000 - Loss: 0.937
Iter 20/2000 - Loss: 0.717
Iter 1981/2000 - Loss: -7.550
Iter 1982/2000 - Loss: -7.550
Iter 1983/2000 - Loss: -7.550
Iter 1984/2000 - Loss: -7.550
Iter 1985/2000 - Loss: -7.550
Iter 1986/2000 - Loss: -7.550
Iter 1987/2000 - Loss: -7.550
Iter 1988/2000 - Loss: -7.550
Iter 1989/2000 - Loss: -7.550
Iter 1990/2000 - Loss: -7.550
Iter 1991/2000 - Loss: -7.550
Iter 1992/2000 - Loss: -7.550
Iter 1993/2000 - Loss: -7.550
Iter 1994/2000 - Loss: -7.551
Iter 1995/2000 - Loss: -7.551
Iter 1996/2000 - Loss: -7.551
Iter 1997/2000 - Loss: -7.551
Iter 1998/2000 - Loss: -7.551
Iter 1999/2000 - Loss: -7.551
Iter 2000/2000 - Loss: -7.551
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0003]])
Lengthscale: tensor([[[14.3636,  6.4220, 51.6453, 12.9476, 16.3938, 51.6916]],

        [[19.8019, 36.8943, 10.9874,  1.2986,  4.9906, 32.7592]],

        [[21.9037, 37.8172, 10.2408,  1.1363,  2.7542, 21.6536]],

        [[16.4508, 31.7099, 14.8578,  3.8369,  1.3081, 43.8811]]])
Signal Variance: tensor([ 0.0739,  2.6509, 17.9949,  0.3613])
Estimated target variance: tensor([0.0112, 0.5182, 5.6923, 0.0643])
N: 110
Signal to noise ratio: tensor([ 15.7578,  91.0837, 101.6723,  36.2882])
Bound on condition number: tensor([  27314.8304,  912586.7615, 1137100.1329,  144852.5598])
Policy Optimizer learning rate:
0.0009895192582062146
Experience 11, Iter 0, disc loss: 0.002285963618099099, policy loss: 6.732719387850789
Experience 11, Iter 1, disc loss: 0.0022807486609954823, policy loss: 6.719003031668147
Experience 11, Iter 2, disc loss: 0.002154971875917955, policy loss: 6.747235443430047
Experience 11, Iter 3, disc loss: 0.0021129258367573902, policy loss: 6.828238896730095
Experience 11, Iter 4, disc loss: 0.002232918814472898, policy loss: 6.6431694091504045
Experience 11, Iter 5, disc loss: 0.0022331744961534934, policy loss: 6.815849774562869
Experience 11, Iter 6, disc loss: 0.002464125743553242, policy loss: 6.503313786534495
Experience 11, Iter 7, disc loss: 0.0021930195645211022, policy loss: 6.7214660730721265
Experience 11, Iter 8, disc loss: 0.002140892943647369, policy loss: 6.89342756468816
Experience 11, Iter 9, disc loss: 0.0022097397404138605, policy loss: 6.691344977089272
Experience 11, Iter 10, disc loss: 0.0021614464537404324, policy loss: 6.739081948409531
Experience 11, Iter 11, disc loss: 0.002162268612463537, policy loss: 6.716973261313024
Experience 11, Iter 12, disc loss: 0.0023462317093382533, policy loss: 6.634713760956394
Experience 11, Iter 13, disc loss: 0.0019501601980421693, policy loss: 6.945640748626886
Experience 11, Iter 14, disc loss: 0.0020335012628250513, policy loss: 6.950534032401478
Experience 11, Iter 15, disc loss: 0.0020137464311649413, policy loss: 6.923389111510559
Experience 11, Iter 16, disc loss: 0.001999333184655008, policy loss: 7.069751953658025
Experience 11, Iter 17, disc loss: 0.002162147503269451, policy loss: 6.7376862775595745
Experience 11, Iter 18, disc loss: 0.0018659913643100494, policy loss: 7.1242863308947815
Experience 11, Iter 19, disc loss: 0.0021007267902561503, policy loss: 6.7523949410950586
Experience 11, Iter 20, disc loss: 0.0021660745172329436, policy loss: 6.713273648110549
Experience 11, Iter 21, disc loss: 0.0020099233922176094, policy loss: 6.7896583938435136
Experience 11, Iter 22, disc loss: 0.001953383133093735, policy loss: 6.990602089756565
Experience 11, Iter 23, disc loss: 0.002047286448674724, policy loss: 6.819521084815281
Experience 11, Iter 24, disc loss: 0.0021642098303308725, policy loss: 6.753143812817448
Experience 11, Iter 25, disc loss: 0.0020889946808194046, policy loss: 6.958411390951519
Experience 11, Iter 26, disc loss: 0.002065588756577522, policy loss: 6.83061427703284
Experience 11, Iter 27, disc loss: 0.0020866899570926123, policy loss: 6.79854944238393
Experience 11, Iter 28, disc loss: 0.0019492016995056203, policy loss: 6.894642753133806
Experience 11, Iter 29, disc loss: 0.002087903658718933, policy loss: 6.802506981372058
Experience 11, Iter 30, disc loss: 0.002098420107418929, policy loss: 6.742282971737469
Experience 11, Iter 31, disc loss: 0.0019722042286754774, policy loss: 7.04990175937068
Experience 11, Iter 32, disc loss: 0.0018878573216001763, policy loss: 6.975230945601249
Experience 11, Iter 33, disc loss: 0.002018793068762813, policy loss: 7.111572382453004
Experience 11, Iter 34, disc loss: 0.0020058200787385605, policy loss: 7.016261441841836
Experience 11, Iter 35, disc loss: 0.00212878875194322, policy loss: 6.759982209281514
Experience 11, Iter 36, disc loss: 0.0020749348207030885, policy loss: 6.789879253064635
Experience 11, Iter 37, disc loss: 0.0021801355343184737, policy loss: 6.66401654862993
Experience 11, Iter 38, disc loss: 0.0021491613082725495, policy loss: 6.689971980590638
Experience 11, Iter 39, disc loss: 0.0021726097279942593, policy loss: 6.684225726017569
Experience 11, Iter 40, disc loss: 0.0019510555565280478, policy loss: 7.2415840238397235
Experience 11, Iter 41, disc loss: 0.0019858161377389653, policy loss: 6.897959814740501
Experience 11, Iter 42, disc loss: 0.00218151330102831, policy loss: 6.6736200955780935
Experience 11, Iter 43, disc loss: 0.0020216700173402834, policy loss: 6.872920812332417
Experience 11, Iter 44, disc loss: 0.002082632854250948, policy loss: 6.790202221289196
Experience 11, Iter 45, disc loss: 0.0020137225538887594, policy loss: 6.859053509719859
Experience 11, Iter 46, disc loss: 0.0021013948683333687, policy loss: 6.894138023289082
Experience 11, Iter 47, disc loss: 0.0019626805052156355, policy loss: 6.866158388785854
Experience 11, Iter 48, disc loss: 0.0019046259790272979, policy loss: 6.942065507500115
Experience 11, Iter 49, disc loss: 0.001972039096678908, policy loss: 6.959477759871873
Experience 11, Iter 50, disc loss: 0.0020216411431460838, policy loss: 6.750506541743366
Experience 11, Iter 51, disc loss: 0.0018358439527176545, policy loss: 6.98735782100297
Experience 11, Iter 52, disc loss: 0.0019170149289043526, policy loss: 6.93507739389852
Experience 11, Iter 53, disc loss: 0.00190270827092664, policy loss: 6.881622743390162
Experience 11, Iter 54, disc loss: 0.0017544866618307313, policy loss: 7.083620778288083
Experience 11, Iter 55, disc loss: 0.001895671216946585, policy loss: 6.847401306576252
Experience 11, Iter 56, disc loss: 0.0019386439377852831, policy loss: 6.8481321192839
Experience 11, Iter 57, disc loss: 0.0019363789193849613, policy loss: 6.870677665849768
Experience 11, Iter 58, disc loss: 0.0019346328477182957, policy loss: 6.963549882753321
Experience 11, Iter 59, disc loss: 0.0018291863576694724, policy loss: 6.956143602585182
Experience 11, Iter 60, disc loss: 0.0017350580635422308, policy loss: 7.039954288404191
Experience 11, Iter 61, disc loss: 0.0020082479550160804, policy loss: 6.772720509220141
Experience 11, Iter 62, disc loss: 0.0019467641052764227, policy loss: 6.823904370234909
Experience 11, Iter 63, disc loss: 0.0018379626312390318, policy loss: 7.053347076200929
Experience 11, Iter 64, disc loss: 0.0017559772104407236, policy loss: 6.970069870910498
Experience 11, Iter 65, disc loss: 0.0020414823927126582, policy loss: 6.861587469753334
Experience 11, Iter 66, disc loss: 0.0018590771578396192, policy loss: 6.892287183957271
Experience 11, Iter 67, disc loss: 0.0017497023629785924, policy loss: 6.990391226303199
Experience 11, Iter 68, disc loss: 0.0017878549190000039, policy loss: 7.127717943625049
Experience 11, Iter 69, disc loss: 0.0018290898202479218, policy loss: 6.868858773301371
Experience 11, Iter 70, disc loss: 0.0016544919800947004, policy loss: 7.079270384626925
Experience 11, Iter 71, disc loss: 0.0017834200784845153, policy loss: 6.927473433998699
Experience 11, Iter 72, disc loss: 0.0017446897666435726, policy loss: 7.045249377237749
Experience 11, Iter 73, disc loss: 0.0018226151775335177, policy loss: 6.93847578366664
Experience 11, Iter 74, disc loss: 0.0017846098804080199, policy loss: 7.007409337116387
Experience 11, Iter 75, disc loss: 0.0017202530189565845, policy loss: 7.047079401445629
Experience 11, Iter 76, disc loss: 0.0019044548702973484, policy loss: 6.9341635222263545
Experience 11, Iter 77, disc loss: 0.0018093206141633878, policy loss: 6.958464432561319
Experience 11, Iter 78, disc loss: 0.0016072181320850476, policy loss: 7.163452847534658
Experience 11, Iter 79, disc loss: 0.0016556234606289178, policy loss: 7.032555217711872
Experience 11, Iter 80, disc loss: 0.0014573714048990447, policy loss: 7.2703390075849
Experience 11, Iter 81, disc loss: 0.0018456258920002066, policy loss: 6.911793137374881
Experience 11, Iter 82, disc loss: 0.0016840490976111288, policy loss: 7.0316660213581095
Experience 11, Iter 83, disc loss: 0.001666275769476921, policy loss: 7.030460757379585
Experience 11, Iter 84, disc loss: 0.0014994896394705279, policy loss: 7.234749196847845
Experience 11, Iter 85, disc loss: 0.0016334791955550107, policy loss: 7.160019544688264
Experience 11, Iter 86, disc loss: 0.0016600582876794157, policy loss: 7.16189359940848
Experience 11, Iter 87, disc loss: 0.001731318972178466, policy loss: 6.9633770269221955
Experience 11, Iter 88, disc loss: 0.0018446202765674876, policy loss: 6.8202261373583175
Experience 11, Iter 89, disc loss: 0.0017068704498203358, policy loss: 7.001015839433155
Experience 11, Iter 90, disc loss: 0.0015078477099178162, policy loss: 7.36491134555896
Experience 11, Iter 91, disc loss: 0.0017441310152217612, policy loss: 6.8524531634081844
Experience 11, Iter 92, disc loss: 0.0016588164677161532, policy loss: 7.0502640033783415
Experience 11, Iter 93, disc loss: 0.001672707511978743, policy loss: 7.043868396960139
Experience 11, Iter 94, disc loss: 0.0017144214668535717, policy loss: 6.967201950283618
Experience 11, Iter 95, disc loss: 0.0016607274246078624, policy loss: 7.150812541076597
Experience 11, Iter 96, disc loss: 0.0017398696018957132, policy loss: 6.957104995677869
Experience 11, Iter 97, disc loss: 0.0016687626864686327, policy loss: 7.107572746459813
Experience 11, Iter 98, disc loss: 0.0017486407204726855, policy loss: 6.878781001030271
Experience 11, Iter 99, disc loss: 0.0016467039226786154, policy loss: 6.979866912881743
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1361],
        [1.4617],
        [0.0157]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0129, 0.1204, 0.7890, 0.0189, 0.0050, 3.1411]],

        [[0.0129, 0.1204, 0.7890, 0.0189, 0.0050, 3.1411]],

        [[0.0129, 0.1204, 0.7890, 0.0189, 0.0050, 3.1411]],

        [[0.0129, 0.1204, 0.7890, 0.0189, 0.0050, 3.1411]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0110, 0.5442, 5.8467, 0.0626], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0110, 0.5442, 5.8467, 0.0626])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.662
Iter 2/2000 - Loss: 2.805
Iter 3/2000 - Loss: 2.583
Iter 4/2000 - Loss: 2.577
Iter 5/2000 - Loss: 2.618
Iter 6/2000 - Loss: 2.549
Iter 7/2000 - Loss: 2.444
Iter 8/2000 - Loss: 2.382
Iter 9/2000 - Loss: 2.357
Iter 10/2000 - Loss: 2.305
Iter 11/2000 - Loss: 2.200
Iter 12/2000 - Loss: 2.072
Iter 13/2000 - Loss: 1.954
Iter 14/2000 - Loss: 1.846
Iter 15/2000 - Loss: 1.719
Iter 16/2000 - Loss: 1.557
Iter 17/2000 - Loss: 1.366
Iter 18/2000 - Loss: 1.165
Iter 19/2000 - Loss: 0.961
Iter 20/2000 - Loss: 0.749
Iter 1981/2000 - Loss: -7.579
Iter 1982/2000 - Loss: -7.579
Iter 1983/2000 - Loss: -7.579
Iter 1984/2000 - Loss: -7.579
Iter 1985/2000 - Loss: -7.579
Iter 1986/2000 - Loss: -7.580
Iter 1987/2000 - Loss: -7.580
Iter 1988/2000 - Loss: -7.580
Iter 1989/2000 - Loss: -7.580
Iter 1990/2000 - Loss: -7.580
Iter 1991/2000 - Loss: -7.580
Iter 1992/2000 - Loss: -7.580
Iter 1993/2000 - Loss: -7.580
Iter 1994/2000 - Loss: -7.580
Iter 1995/2000 - Loss: -7.580
Iter 1996/2000 - Loss: -7.580
Iter 1997/2000 - Loss: -7.580
Iter 1998/2000 - Loss: -7.580
Iter 1999/2000 - Loss: -7.580
Iter 2000/2000 - Loss: -7.580
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[14.0813,  6.2966, 48.9827, 12.4922, 15.6816, 50.6499]],

        [[19.8019, 36.2128, 11.9559,  1.2181,  5.4228, 31.4391]],

        [[21.2998, 36.6820, 11.4352,  1.0725,  3.3322, 22.6451]],

        [[16.1835, 30.5855, 14.2544,  3.6972,  1.3823, 43.6844]]])
Signal Variance: tensor([ 0.0720,  2.7575, 20.1410,  0.3395])
Estimated target variance: tensor([0.0110, 0.5442, 5.8467, 0.0626])
N: 120
Signal to noise ratio: tensor([15.2735, 94.4623, 98.3032, 34.4758])
Bound on condition number: tensor([  27994.6036, 1070776.8085, 1159623.5758,  142631.0106])
Policy Optimizer learning rate:
0.0009884772446450592
Experience 12, Iter 0, disc loss: 0.001803387898457692, policy loss: 6.864769196348421
Experience 12, Iter 1, disc loss: 0.001655589987570766, policy loss: 7.07591602939673
Experience 12, Iter 2, disc loss: 0.0017281169947397294, policy loss: 6.973392682869552
Experience 12, Iter 3, disc loss: 0.0017769413365543536, policy loss: 6.925978798906085
Experience 12, Iter 4, disc loss: 0.0016809512634189273, policy loss: 7.03180044104618
Experience 12, Iter 5, disc loss: 0.0015926989841028927, policy loss: 7.156136859571705
Experience 12, Iter 6, disc loss: 0.0016465043996729159, policy loss: 6.990075333736966
Experience 12, Iter 7, disc loss: 0.0016684151274477144, policy loss: 7.030022673621122
Experience 12, Iter 8, disc loss: 0.0017654005104647055, policy loss: 6.867437661745081
Experience 12, Iter 9, disc loss: 0.0016788902117020775, policy loss: 7.070559310155225
Experience 12, Iter 10, disc loss: 0.0016982560982037912, policy loss: 6.934647168344946
Experience 12, Iter 11, disc loss: 0.0016000110515576762, policy loss: 7.061106252668716
Experience 12, Iter 12, disc loss: 0.0016005126197488363, policy loss: 7.122765686795567
Experience 12, Iter 13, disc loss: 0.0016909684854985868, policy loss: 6.913121247705448
Experience 12, Iter 14, disc loss: 0.0016575922026712037, policy loss: 7.10076157765373
Experience 12, Iter 15, disc loss: 0.0017293940983475857, policy loss: 6.945362268686033
Experience 12, Iter 16, disc loss: 0.0017580948976191892, policy loss: 6.8645858305341285
Experience 12, Iter 17, disc loss: 0.0017384932075176175, policy loss: 6.912878692946593
Experience 12, Iter 18, disc loss: 0.0017851416471890054, policy loss: 7.119707747940791
Experience 12, Iter 19, disc loss: 0.001686959741498636, policy loss: 6.99907884640866
Experience 12, Iter 20, disc loss: 0.0016709098779012946, policy loss: 7.19617702572772
Experience 12, Iter 21, disc loss: 0.001744404284310713, policy loss: 6.860739434781644
Experience 12, Iter 22, disc loss: 0.0015684771633861544, policy loss: 7.090066320805532
Experience 12, Iter 23, disc loss: 0.0016697661124509032, policy loss: 6.9753286547038265
Experience 12, Iter 24, disc loss: 0.0017932414254783749, policy loss: 6.870176952583533
Experience 12, Iter 25, disc loss: 0.0017604731886729282, policy loss: 6.904320853884549
Experience 12, Iter 26, disc loss: 0.0017250335949621468, policy loss: 7.106124722517942
Experience 12, Iter 27, disc loss: 0.001704943820890089, policy loss: 6.973807178050461
Experience 12, Iter 28, disc loss: 0.0016637960410266096, policy loss: 6.9496636688778715
Experience 12, Iter 29, disc loss: 0.0016485862391931276, policy loss: 6.968938322068974
Experience 12, Iter 30, disc loss: 0.0017104016490767093, policy loss: 6.952296581496438
Experience 12, Iter 31, disc loss: 0.0017231251281734083, policy loss: 6.92601014398681
Experience 12, Iter 32, disc loss: 0.0015802216617383557, policy loss: 7.104143087541552
Experience 12, Iter 33, disc loss: 0.0015157514852134411, policy loss: 7.069649394406534
Experience 12, Iter 34, disc loss: 0.001613691165914988, policy loss: 7.012995013386657
Experience 12, Iter 35, disc loss: 0.0016476241192127214, policy loss: 7.038534002729701
Experience 12, Iter 36, disc loss: 0.0014412610622887038, policy loss: 7.261718406252832
Experience 12, Iter 37, disc loss: 0.001585225604887299, policy loss: 7.042674581145091
Experience 12, Iter 38, disc loss: 0.0016150327209480725, policy loss: 6.990779009828869
Experience 12, Iter 39, disc loss: 0.001567914031963082, policy loss: 6.994531127151166
Experience 12, Iter 40, disc loss: 0.0014395461823916572, policy loss: 7.199680950882586
Experience 12, Iter 41, disc loss: 0.0015846680440952529, policy loss: 7.169871676638207
Experience 12, Iter 42, disc loss: 0.0015975426010976963, policy loss: 7.005596902988934
Experience 12, Iter 43, disc loss: 0.0015865814028771465, policy loss: 7.026730097731358
Experience 12, Iter 44, disc loss: 0.0015637307113844675, policy loss: 7.033496058975253
Experience 12, Iter 45, disc loss: 0.0015258707435997455, policy loss: 7.157074746532454
Experience 12, Iter 46, disc loss: 0.0015092704832893812, policy loss: 7.366518154798507
Experience 12, Iter 47, disc loss: 0.0016168626596135547, policy loss: 7.018592564250058
Experience 12, Iter 48, disc loss: 0.0016209483171397477, policy loss: 7.002692610271614
Experience 12, Iter 49, disc loss: 0.0014587844060541875, policy loss: 7.260207625767599
Experience 12, Iter 50, disc loss: 0.0014873505275255995, policy loss: 7.172829125769111
Experience 12, Iter 51, disc loss: 0.001499354826412684, policy loss: 7.087518795065414
Experience 12, Iter 52, disc loss: 0.001549190729711973, policy loss: 7.077208238507913
Experience 12, Iter 53, disc loss: 0.0014321729108735337, policy loss: 7.245743435526364
Experience 12, Iter 54, disc loss: 0.0014402650984587711, policy loss: 7.435787697119143
Experience 12, Iter 55, disc loss: 0.001448577590533052, policy loss: 7.127250798083365
Experience 12, Iter 56, disc loss: 0.0014358647438977524, policy loss: 7.231187120029649
Experience 12, Iter 57, disc loss: 0.001524318481807106, policy loss: 7.082405641877247
Experience 12, Iter 58, disc loss: 0.0014592076201704082, policy loss: 7.0665729634057515
Experience 12, Iter 59, disc loss: 0.00135971991031062, policy loss: 7.288554811118226
Experience 12, Iter 60, disc loss: 0.0015280138670242416, policy loss: 7.0661362915411114
Experience 12, Iter 61, disc loss: 0.0014740490099415502, policy loss: 7.176580016827124
Experience 12, Iter 62, disc loss: 0.0015007062978690275, policy loss: 7.036736151645513
Experience 12, Iter 63, disc loss: 0.0013875106947492956, policy loss: 7.21545913847471
Experience 12, Iter 64, disc loss: 0.0012954357205271275, policy loss: 7.34798672686323
Experience 12, Iter 65, disc loss: 0.0011673320833401994, policy loss: 7.560073466534174
Experience 12, Iter 66, disc loss: 0.0013100161026521917, policy loss: 7.371443174082037
Experience 12, Iter 67, disc loss: 0.0013153260620665523, policy loss: 7.310888773786981
Experience 12, Iter 68, disc loss: 0.0013871390051327375, policy loss: 7.279551506965759
Experience 12, Iter 69, disc loss: 0.0013271124118106963, policy loss: 7.2636967297632005
Experience 12, Iter 70, disc loss: 0.001254464432562931, policy loss: 7.361118694938843
Experience 12, Iter 71, disc loss: 0.0014310342051882133, policy loss: 7.101024512277679
Experience 12, Iter 72, disc loss: 0.0013736827938705077, policy loss: 7.267513706944183
Experience 12, Iter 73, disc loss: 0.0013597084722956352, policy loss: 7.265459902937946
Experience 12, Iter 74, disc loss: 0.0013301471092834498, policy loss: 7.407630000041175
Experience 12, Iter 75, disc loss: 0.0012916889628965921, policy loss: 7.329411403418085
Experience 12, Iter 76, disc loss: 0.0013254499862481936, policy loss: 7.241571142524681
Experience 12, Iter 77, disc loss: 0.0014424561044565677, policy loss: 7.490976574968058
Experience 12, Iter 78, disc loss: 0.0013127567154283689, policy loss: 7.300229517313748
Experience 12, Iter 79, disc loss: 0.0014009886474572334, policy loss: 7.159614958038746
Experience 12, Iter 80, disc loss: 0.0014054768584063435, policy loss: 7.2326303571359745
Experience 12, Iter 81, disc loss: 0.0013236536563689084, policy loss: 7.246451745693296
Experience 12, Iter 82, disc loss: 0.0014119360112282329, policy loss: 7.12280458654751
Experience 12, Iter 83, disc loss: 0.0014972002298115354, policy loss: 7.055298185743544
Experience 12, Iter 84, disc loss: 0.0013027719954242375, policy loss: 7.27201987435509
Experience 12, Iter 85, disc loss: 0.0013495028697325332, policy loss: 7.193861730211292
Experience 12, Iter 86, disc loss: 0.0012292639192755003, policy loss: 7.324415012225316
Experience 12, Iter 87, disc loss: 0.0014316782438635058, policy loss: 7.167994897372534
Experience 12, Iter 88, disc loss: 0.0012608210103867696, policy loss: 7.322862686785669
Experience 12, Iter 89, disc loss: 0.0011594899820528853, policy loss: 7.490262570524756
Experience 12, Iter 90, disc loss: 0.0011525835615619495, policy loss: 7.612722636090816
Experience 12, Iter 91, disc loss: 0.0013175621620032931, policy loss: 7.282958323718818
Experience 12, Iter 92, disc loss: 0.0012537170203932727, policy loss: 7.3483430130188125
Experience 12, Iter 93, disc loss: 0.0013507668176429583, policy loss: 7.213186753605704
Experience 12, Iter 94, disc loss: 0.0012219314111344493, policy loss: 7.381381988922746
Experience 12, Iter 95, disc loss: 0.0010917060486720603, policy loss: 7.595119546268993
Experience 12, Iter 96, disc loss: 0.001268930097432919, policy loss: 7.609920731062445
Experience 12, Iter 97, disc loss: 0.0013516201732396466, policy loss: 7.4682198939603115
Experience 12, Iter 98, disc loss: 0.0010604216312369707, policy loss: 7.719847283101879
Experience 12, Iter 99, disc loss: 0.0011989915109308878, policy loss: 7.3566624671133285
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1419],
        [1.4901],
        [0.0149]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1194, 0.7636, 0.0191, 0.0048, 3.3075]],

        [[0.0122, 0.1194, 0.7636, 0.0191, 0.0048, 3.3075]],

        [[0.0122, 0.1194, 0.7636, 0.0191, 0.0048, 3.3075]],

        [[0.0122, 0.1194, 0.7636, 0.0191, 0.0048, 3.3075]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.5674, 5.9605, 0.0597], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.5674, 5.9605, 0.0597])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.676
Iter 2/2000 - Loss: 2.822
Iter 3/2000 - Loss: 2.597
Iter 4/2000 - Loss: 2.595
Iter 5/2000 - Loss: 2.635
Iter 6/2000 - Loss: 2.565
Iter 7/2000 - Loss: 2.463
Iter 8/2000 - Loss: 2.401
Iter 9/2000 - Loss: 2.375
Iter 10/2000 - Loss: 2.326
Iter 11/2000 - Loss: 2.225
Iter 12/2000 - Loss: 2.100
Iter 13/2000 - Loss: 1.980
Iter 14/2000 - Loss: 1.869
Iter 15/2000 - Loss: 1.741
Iter 16/2000 - Loss: 1.578
Iter 17/2000 - Loss: 1.386
Iter 18/2000 - Loss: 1.180
Iter 19/2000 - Loss: 0.970
Iter 20/2000 - Loss: 0.752
Iter 1981/2000 - Loss: -7.720
Iter 1982/2000 - Loss: -7.720
Iter 1983/2000 - Loss: -7.720
Iter 1984/2000 - Loss: -7.720
Iter 1985/2000 - Loss: -7.720
Iter 1986/2000 - Loss: -7.720
Iter 1987/2000 - Loss: -7.720
Iter 1988/2000 - Loss: -7.720
Iter 1989/2000 - Loss: -7.720
Iter 1990/2000 - Loss: -7.720
Iter 1991/2000 - Loss: -7.720
Iter 1992/2000 - Loss: -7.720
Iter 1993/2000 - Loss: -7.720
Iter 1994/2000 - Loss: -7.720
Iter 1995/2000 - Loss: -7.720
Iter 1996/2000 - Loss: -7.720
Iter 1997/2000 - Loss: -7.721
Iter 1998/2000 - Loss: -7.721
Iter 1999/2000 - Loss: -7.721
Iter 2000/2000 - Loss: -7.721
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[13.7689,  6.1484, 46.8099, 12.0437, 15.5983, 52.0635]],

        [[19.2989, 34.4143, 11.9871,  1.2117,  5.4539, 30.5198]],

        [[20.3733, 36.2219, 11.2780,  1.1008,  3.2241, 21.5829]],

        [[16.0153, 30.1432, 14.6687,  3.6676,  1.3699, 44.0697]]])
Signal Variance: tensor([ 0.0681,  2.6938, 18.9097,  0.3557])
Estimated target variance: tensor([0.0109, 0.5674, 5.9605, 0.0597])
N: 130
Signal to noise ratio: tensor([15.0177, 95.3986, 93.7947, 35.9514])
Bound on condition number: tensor([  29320.1462, 1183117.5433, 1143667.8234,  168026.1193])
Policy Optimizer learning rate:
0.000987436328376607
Experience 13, Iter 0, disc loss: 0.0011902292838911355, policy loss: 7.461343567272685
Experience 13, Iter 1, disc loss: 0.0012607656337743787, policy loss: 7.294421011950221
Experience 13, Iter 2, disc loss: 0.0013272776585950914, policy loss: 7.364039422114063
Experience 13, Iter 3, disc loss: 0.001381451550957267, policy loss: 7.263778341453738
Experience 13, Iter 4, disc loss: 0.0012798313399704597, policy loss: 7.311283012535635
Experience 13, Iter 5, disc loss: 0.0012470033544052653, policy loss: 7.267423375717018
Experience 13, Iter 6, disc loss: 0.001247680584529795, policy loss: 7.430271249277921
Experience 13, Iter 7, disc loss: 0.001273056285158425, policy loss: 7.19862277333016
Experience 13, Iter 8, disc loss: 0.0012311877363367215, policy loss: 7.271788169705789
Experience 13, Iter 9, disc loss: 0.0010727922547735605, policy loss: 7.586863586724142
Experience 13, Iter 10, disc loss: 0.0010736697047008734, policy loss: 7.543018425732059
Experience 13, Iter 11, disc loss: 0.0011845406928089283, policy loss: 7.587328185068295
Experience 13, Iter 12, disc loss: 0.0012665423864166973, policy loss: 7.297716897908678
Experience 13, Iter 13, disc loss: 0.0011818088583180353, policy loss: 7.322599307022632
Experience 13, Iter 14, disc loss: 0.0010489959475513868, policy loss: 7.723146820501638
Experience 13, Iter 15, disc loss: 0.001261950838467674, policy loss: 7.380899190881418
Experience 13, Iter 16, disc loss: 0.0010126395974484168, policy loss: 7.603029855494386
Experience 13, Iter 17, disc loss: 0.001033668110220984, policy loss: 7.581358656892702
Experience 13, Iter 18, disc loss: 0.0009360004818742104, policy loss: 7.7069145683818245
Experience 13, Iter 19, disc loss: 0.000940615235610995, policy loss: 7.71479873809886
Experience 13, Iter 20, disc loss: 0.0011011420069630992, policy loss: 7.476720502844011
Experience 13, Iter 21, disc loss: 0.0011750525641143995, policy loss: 7.336235216029901
Experience 13, Iter 22, disc loss: 0.0011171569198440375, policy loss: 7.553565163039282
Experience 13, Iter 23, disc loss: 0.0011152614354133967, policy loss: 7.453850505322129
Experience 13, Iter 24, disc loss: 0.0009944533556848424, policy loss: 7.644826519423686
Experience 13, Iter 25, disc loss: 0.0010335217390120802, policy loss: 7.545606468507056
Experience 13, Iter 26, disc loss: 0.0010885761699331478, policy loss: 7.408629327994312
Experience 13, Iter 27, disc loss: 0.001007901830981924, policy loss: 7.547235297494299
Experience 13, Iter 28, disc loss: 0.0009502667163358491, policy loss: 7.657162606474314
Experience 13, Iter 29, disc loss: 0.0009062238611211975, policy loss: 7.749852058274772
Experience 13, Iter 30, disc loss: 0.000984283980520342, policy loss: 7.57208674881265
Experience 13, Iter 31, disc loss: 0.0010785159578495145, policy loss: 7.442157462063925
Experience 13, Iter 32, disc loss: 0.001043181337991425, policy loss: 7.560709032610065
Experience 13, Iter 33, disc loss: 0.001199226918756993, policy loss: 7.276748866537001
Experience 13, Iter 34, disc loss: 0.0011432464305691403, policy loss: 7.4066423046229115
Experience 13, Iter 35, disc loss: 0.001221574211189766, policy loss: 7.257509773199409
Experience 13, Iter 36, disc loss: 0.001069306211653822, policy loss: 7.488534710481665
Experience 13, Iter 37, disc loss: 0.0011381213014647908, policy loss: 7.363184067995443
Experience 13, Iter 38, disc loss: 0.0009953570389262209, policy loss: 7.577502622514974
Experience 13, Iter 39, disc loss: 0.0011911379849226008, policy loss: 7.25374827767788
Experience 13, Iter 40, disc loss: 0.0010969606194761589, policy loss: 7.4187354352608335
Experience 13, Iter 41, disc loss: 0.001113111750234905, policy loss: 7.372943803390957
Experience 13, Iter 42, disc loss: 0.0009903372727796057, policy loss: 7.693448444110799
Experience 13, Iter 43, disc loss: 0.0010012957308080136, policy loss: 7.697972635960516
Experience 13, Iter 44, disc loss: 0.001124351399715536, policy loss: 7.430863795779975
Experience 13, Iter 45, disc loss: 0.0012149217302712821, policy loss: 7.272331090210106
Experience 13, Iter 46, disc loss: 0.0011534083957238043, policy loss: 7.327133910286027
Experience 13, Iter 47, disc loss: 0.0011072440757292578, policy loss: 7.401399908712686
Experience 13, Iter 48, disc loss: 0.001204205081853787, policy loss: 7.267413400276665
Experience 13, Iter 49, disc loss: 0.0011324456606946996, policy loss: 7.40611937313362
Experience 13, Iter 50, disc loss: 0.0011994499734286138, policy loss: 7.235910084037554
Experience 13, Iter 51, disc loss: 0.0011008061653950923, policy loss: 7.477686116467099
Experience 13, Iter 52, disc loss: 0.0011838469177471108, policy loss: 7.393329036021122
Experience 13, Iter 53, disc loss: 0.0011414836495190306, policy loss: 7.351760286922518
Experience 13, Iter 54, disc loss: 0.001167821164692645, policy loss: 7.45751540194349
Experience 13, Iter 55, disc loss: 0.0012030852804198583, policy loss: 7.235694977820558
Experience 13, Iter 56, disc loss: 0.0011408456752415943, policy loss: 7.323070140492682
Experience 13, Iter 57, disc loss: 0.0011964231696738185, policy loss: 7.264486536935545
Experience 13, Iter 58, disc loss: 0.001235203768088468, policy loss: 7.200984931102583
Experience 13, Iter 59, disc loss: 0.001168400995686151, policy loss: 7.4497733185270505
Experience 13, Iter 60, disc loss: 0.0011596838081705586, policy loss: 7.3413031770150425
Experience 13, Iter 61, disc loss: 0.0011630826048851998, policy loss: 7.436194866241628
Experience 13, Iter 62, disc loss: 0.0011655603308976675, policy loss: 7.253053356033201
Experience 13, Iter 63, disc loss: 0.0012007333402768948, policy loss: 7.437818785634709
Experience 13, Iter 64, disc loss: 0.0011767078278175996, policy loss: 7.299051211536222
Experience 13, Iter 65, disc loss: 0.0011181666483065559, policy loss: 7.349416941475561
Experience 13, Iter 66, disc loss: 0.0012041357899478463, policy loss: 7.213920546713155
Experience 13, Iter 67, disc loss: 0.001221553651109928, policy loss: 7.199635431022231
Experience 13, Iter 68, disc loss: 0.0011399127908427286, policy loss: 7.324179868698459
Experience 13, Iter 69, disc loss: 0.001151473814567143, policy loss: 7.328992833515971
Experience 13, Iter 70, disc loss: 0.0011593059474579689, policy loss: 7.2829352664035465
Experience 13, Iter 71, disc loss: 0.001176744241045304, policy loss: 7.378634124561728
Experience 13, Iter 72, disc loss: 0.0012070755789505587, policy loss: 7.272018436964267
Experience 13, Iter 73, disc loss: 0.0011438649952737981, policy loss: 7.369584415620972
Experience 13, Iter 74, disc loss: 0.0011512725580054852, policy loss: 7.339724216173879
Experience 13, Iter 75, disc loss: 0.0011539085380642294, policy loss: 7.391696178319703
Experience 13, Iter 76, disc loss: 0.001167126510843091, policy loss: 7.456327734860549
Experience 13, Iter 77, disc loss: 0.001157479812809002, policy loss: 7.28390054700966
Experience 13, Iter 78, disc loss: 0.0011240654016173954, policy loss: 7.411532988417804
Experience 13, Iter 79, disc loss: 0.0012249196531026684, policy loss: 7.260134679619318
Experience 13, Iter 80, disc loss: 0.001176619504826241, policy loss: 7.219563262408149
Experience 13, Iter 81, disc loss: 0.0011232771828061099, policy loss: 7.3892990501449916
Experience 13, Iter 82, disc loss: 0.0011238302686704974, policy loss: 7.3422719182020115
Experience 13, Iter 83, disc loss: 0.0011314884111147602, policy loss: 7.360958898783314
Experience 13, Iter 84, disc loss: 0.0011784130959105585, policy loss: 7.277562410978434
Experience 13, Iter 85, disc loss: 0.0011558120667429564, policy loss: 7.300005168386061
Experience 13, Iter 86, disc loss: 0.001142135987500667, policy loss: 7.384166195693451
Experience 13, Iter 87, disc loss: 0.0011705385874928899, policy loss: 7.2993722638636545
Experience 13, Iter 88, disc loss: 0.0011925374790377236, policy loss: 7.2681052377940185
Experience 13, Iter 89, disc loss: 0.0010812638344251863, policy loss: 7.487707170139531
Experience 13, Iter 90, disc loss: 0.0011694944418982538, policy loss: 7.237226366559469
Experience 13, Iter 91, disc loss: 0.0010643720754204477, policy loss: 7.51203588495671
Experience 13, Iter 92, disc loss: 0.0010435711147489376, policy loss: 7.750143604049419
Experience 13, Iter 93, disc loss: 0.0010998956262753656, policy loss: 7.434461664198735
Experience 13, Iter 94, disc loss: 0.001073043400174812, policy loss: 7.496260679426175
Experience 13, Iter 95, disc loss: 0.0011399163860520932, policy loss: 7.250769959573286
Experience 13, Iter 96, disc loss: 0.001137271673296098, policy loss: 7.35629117546978
Experience 13, Iter 97, disc loss: 0.0010862381559507478, policy loss: 7.628243008488392
Experience 13, Iter 98, disc loss: 0.001142936016130561, policy loss: 7.300663458968122
Experience 13, Iter 99, disc loss: 0.0011810748542311319, policy loss: 7.354921034197924
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.1486],
        [1.5410],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0116, 0.1178, 0.7587, 0.0188, 0.0045, 3.4234]],

        [[0.0116, 0.1178, 0.7587, 0.0188, 0.0045, 3.4234]],

        [[0.0116, 0.1178, 0.7587, 0.0188, 0.0045, 3.4234]],

        [[0.0116, 0.1178, 0.7587, 0.0188, 0.0045, 3.4234]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0107, 0.5944, 6.1641, 0.0585], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0107, 0.5944, 6.1641, 0.0585])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.694
Iter 2/2000 - Loss: 2.885
Iter 3/2000 - Loss: 2.626
Iter 4/2000 - Loss: 2.635
Iter 5/2000 - Loss: 2.691
Iter 6/2000 - Loss: 2.623
Iter 7/2000 - Loss: 2.515
Iter 8/2000 - Loss: 2.448
Iter 9/2000 - Loss: 2.428
Iter 10/2000 - Loss: 2.397
Iter 11/2000 - Loss: 2.314
Iter 12/2000 - Loss: 2.191
Iter 13/2000 - Loss: 2.065
Iter 14/2000 - Loss: 1.952
Iter 15/2000 - Loss: 1.836
Iter 16/2000 - Loss: 1.689
Iter 17/2000 - Loss: 1.503
Iter 18/2000 - Loss: 1.290
Iter 19/2000 - Loss: 1.069
Iter 20/2000 - Loss: 0.843
Iter 1981/2000 - Loss: -7.833
Iter 1982/2000 - Loss: -7.833
Iter 1983/2000 - Loss: -7.833
Iter 1984/2000 - Loss: -7.833
Iter 1985/2000 - Loss: -7.833
Iter 1986/2000 - Loss: -7.833
Iter 1987/2000 - Loss: -7.833
Iter 1988/2000 - Loss: -7.833
Iter 1989/2000 - Loss: -7.833
Iter 1990/2000 - Loss: -7.833
Iter 1991/2000 - Loss: -7.833
Iter 1992/2000 - Loss: -7.833
Iter 1993/2000 - Loss: -7.833
Iter 1994/2000 - Loss: -7.833
Iter 1995/2000 - Loss: -7.833
Iter 1996/2000 - Loss: -7.833
Iter 1997/2000 - Loss: -7.833
Iter 1998/2000 - Loss: -7.833
Iter 1999/2000 - Loss: -7.833
Iter 2000/2000 - Loss: -7.833
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[13.4776,  5.9152, 43.9104, 12.1027, 15.1183, 50.4650]],

        [[19.3435, 34.1367, 12.2381,  1.2050,  5.4968, 30.1346]],

        [[20.1375, 36.5527, 11.5238,  1.0672,  3.1076, 23.3316]],

        [[15.4354, 29.7465, 14.2030,  3.5652,  1.3686, 43.7290]]])
Signal Variance: tensor([ 0.0626,  2.7052, 19.4153,  0.3415])
Estimated target variance: tensor([0.0107, 0.5944, 6.1641, 0.0585])
N: 140
Signal to noise ratio: tensor([14.2846, 96.1551, 93.6556, 35.8991])
Bound on condition number: tensor([  28568.0550, 1294412.9458, 1227994.2181,  180425.3996])
Policy Optimizer learning rate:
0.0009863965082453534
Experience 14, Iter 0, disc loss: 0.0010358303740614487, policy loss: 7.6390309306230995
Experience 14, Iter 1, disc loss: 0.00113572084753429, policy loss: 7.3642549747230035
Experience 14, Iter 2, disc loss: 0.0011817759347730761, policy loss: 7.237530948362831
Experience 14, Iter 3, disc loss: 0.0010768270837065053, policy loss: 7.4514698483827955
Experience 14, Iter 4, disc loss: 0.0010818031498324252, policy loss: 7.359183034724116
Experience 14, Iter 5, disc loss: 0.0010928024249348242, policy loss: 7.356627527584543
Experience 14, Iter 6, disc loss: 0.00114758909396426, policy loss: 7.36638115872768
Experience 14, Iter 7, disc loss: 0.0010552486014590245, policy loss: 7.548616639378102
Experience 14, Iter 8, disc loss: 0.0010200408500715967, policy loss: 7.475401135844855
Experience 14, Iter 9, disc loss: 0.0010973155770873865, policy loss: 7.406366588290249
Experience 14, Iter 10, disc loss: 0.0011018955596149875, policy loss: 7.2940478517330725
Experience 14, Iter 11, disc loss: 0.0011016674405896396, policy loss: 7.350218598690644
Experience 14, Iter 12, disc loss: 0.0011185361196559395, policy loss: 7.464768941515555
Experience 14, Iter 13, disc loss: 0.001087160469468123, policy loss: 7.385810917269844
Experience 14, Iter 14, disc loss: 0.001010010501155375, policy loss: 7.70958584379982
Experience 14, Iter 15, disc loss: 0.001148672026569828, policy loss: 7.293557265230904
Experience 14, Iter 16, disc loss: 0.0009860531692755187, policy loss: 7.688089044578822
Experience 14, Iter 17, disc loss: 0.0010512093797402401, policy loss: 7.498715766212839
Experience 14, Iter 18, disc loss: 0.0009833704159240705, policy loss: 7.617433487909249
Experience 14, Iter 19, disc loss: 0.001096110060286419, policy loss: 7.405228546097195
Experience 14, Iter 20, disc loss: 0.0010027689314680652, policy loss: 7.537391241711974
Experience 14, Iter 21, disc loss: 0.0009609832024338735, policy loss: 7.585515004671874
Experience 14, Iter 22, disc loss: 0.0009842522784409967, policy loss: 7.5553036989068705
Experience 14, Iter 23, disc loss: 0.0010722798886700076, policy loss: 7.4372499483483345
Experience 14, Iter 24, disc loss: 0.0010070231341832762, policy loss: 7.497593967018529
Experience 14, Iter 25, disc loss: 0.0010369975316504357, policy loss: 7.44249316999047
Experience 14, Iter 26, disc loss: 0.0010119980537179937, policy loss: 7.416085942226815
Experience 14, Iter 27, disc loss: 0.0010302264143240048, policy loss: 7.487617384127548
Experience 14, Iter 28, disc loss: 0.0010172278480454725, policy loss: 7.542316616832009
Experience 14, Iter 29, disc loss: 0.0010820372550312276, policy loss: 7.417126123294877
Experience 14, Iter 30, disc loss: 0.0010855323113314426, policy loss: 7.376194260826743
Experience 14, Iter 31, disc loss: 0.001063767191598986, policy loss: 7.394743565208947
Experience 14, Iter 32, disc loss: 0.0010708485198463627, policy loss: 7.461117318393736
Experience 14, Iter 33, disc loss: 0.0010807883903652775, policy loss: 7.38390687684193
Experience 14, Iter 34, disc loss: 0.0011012846663373778, policy loss: 7.471251441536303
Experience 14, Iter 35, disc loss: 0.000960963345256271, policy loss: 7.5721818411991375
Experience 14, Iter 36, disc loss: 0.0009760231801236916, policy loss: 7.571212261338195
Experience 14, Iter 37, disc loss: 0.0009792331857127147, policy loss: 7.609813130962583
Experience 14, Iter 38, disc loss: 0.0010351176563219496, policy loss: 7.489918634711744
Experience 14, Iter 39, disc loss: 0.000997805490966798, policy loss: 7.653177644100756
Experience 14, Iter 40, disc loss: 0.0010447152565283425, policy loss: 7.4500288137056865
Experience 14, Iter 41, disc loss: 0.0010463149016286342, policy loss: 7.40778668455156
Experience 14, Iter 42, disc loss: 0.001068413765407645, policy loss: 7.427567981506916
Experience 14, Iter 43, disc loss: 0.0009916090960925778, policy loss: 7.482650317009346
Experience 14, Iter 44, disc loss: 0.0009571720348934423, policy loss: 7.531178890871975
Experience 14, Iter 45, disc loss: 0.0009661758734347247, policy loss: 7.5423547550762615
Experience 14, Iter 46, disc loss: 0.0010665247273962668, policy loss: 7.361672602868192
Experience 14, Iter 47, disc loss: 0.001059741852225468, policy loss: 7.4396332799570315
Experience 14, Iter 48, disc loss: 0.001029955280819781, policy loss: 7.483243679377437
Experience 14, Iter 49, disc loss: 0.00098741685351781, policy loss: 7.569364500230489
Experience 14, Iter 50, disc loss: 0.0010150523178763062, policy loss: 7.545191016425155
Experience 14, Iter 51, disc loss: 0.0009805011033746965, policy loss: 7.458242332257785
Experience 14, Iter 52, disc loss: 0.000941960184582201, policy loss: 7.689141642981317
Experience 14, Iter 53, disc loss: 0.0010255634552300925, policy loss: 7.426784710892088
Experience 14, Iter 54, disc loss: 0.0010513151011615751, policy loss: 7.419235584516905
Experience 14, Iter 55, disc loss: 0.0009294908548857323, policy loss: 7.643876561332978
Experience 14, Iter 56, disc loss: 0.0009444492322052463, policy loss: 7.660760015443445
Experience 14, Iter 57, disc loss: 0.0009966115630209039, policy loss: 7.552503393123043
Experience 14, Iter 58, disc loss: 0.0009042001507884234, policy loss: 7.621159901078274
Experience 14, Iter 59, disc loss: 0.0009005169492257315, policy loss: 7.920000159465767
Experience 14, Iter 60, disc loss: 0.000969809846770574, policy loss: 7.432269570518898
Experience 14, Iter 61, disc loss: 0.0009807295522568998, policy loss: 7.501716615214824
Experience 14, Iter 62, disc loss: 0.0009374619954390984, policy loss: 7.566156171038598
Experience 14, Iter 63, disc loss: 0.0009264609274410974, policy loss: 7.592882632254153
Experience 14, Iter 64, disc loss: 0.0009556056844841217, policy loss: 7.600365647692863
Experience 14, Iter 65, disc loss: 0.0009413242034017891, policy loss: 7.690245972526374
Experience 14, Iter 66, disc loss: 0.00098363731647217, policy loss: 7.468835976303601
Experience 14, Iter 67, disc loss: 0.0009378311378674004, policy loss: 7.525781812368901
Experience 14, Iter 68, disc loss: 0.0009109058092327279, policy loss: 7.648668780364489
Experience 14, Iter 69, disc loss: 0.000986441689641911, policy loss: 7.500400609459774
Experience 14, Iter 70, disc loss: 0.0009362381206762309, policy loss: 7.5447188418739
Experience 14, Iter 71, disc loss: 0.0009256527445896761, policy loss: 7.5232504255429165
Experience 14, Iter 72, disc loss: 0.0009497447728029611, policy loss: 7.501050481300307
Experience 14, Iter 73, disc loss: 0.0010029472427433399, policy loss: 7.521491523530042
Experience 14, Iter 74, disc loss: 0.0008861652342218115, policy loss: 7.719310022677918
Experience 14, Iter 75, disc loss: 0.0009464817526063233, policy loss: 7.574056406271323
Experience 14, Iter 76, disc loss: 0.0009465004205771121, policy loss: 7.539389349918981
Experience 14, Iter 77, disc loss: 0.0008752797698917388, policy loss: 7.658256336619978
Experience 14, Iter 78, disc loss: 0.0009615785726497388, policy loss: 7.602582398981346
Experience 14, Iter 79, disc loss: 0.0009612793358663549, policy loss: 7.462124333302322
Experience 14, Iter 80, disc loss: 0.0009109167934710975, policy loss: 8.091488656891686
Experience 14, Iter 81, disc loss: 0.00090492016590857, policy loss: 7.605421958695114
Experience 14, Iter 82, disc loss: 0.0009349524280988472, policy loss: 7.724173926181029
Experience 14, Iter 83, disc loss: 0.0009250990469328927, policy loss: 7.858169869377782
Experience 14, Iter 84, disc loss: 0.0008325351032738711, policy loss: 7.755434166975199
Experience 14, Iter 85, disc loss: 0.0009587877592709071, policy loss: 7.479159196114612
Experience 14, Iter 86, disc loss: 0.0008920439567873812, policy loss: 7.760813087884989
Experience 14, Iter 87, disc loss: 0.0008659986881664211, policy loss: 7.907836104903591
Experience 14, Iter 88, disc loss: 0.0009502131082361231, policy loss: 7.5486131519934325
Experience 14, Iter 89, disc loss: 0.000881703140313007, policy loss: 7.630456401477749
Experience 14, Iter 90, disc loss: 0.0008646654327603377, policy loss: 7.702009810342608
Experience 14, Iter 91, disc loss: 0.0008439116245965621, policy loss: 7.701737561767626
Experience 14, Iter 92, disc loss: 0.0008939537972249674, policy loss: 7.632600716059342
Experience 14, Iter 93, disc loss: 0.0009684339820141209, policy loss: 7.473696908349271
Experience 14, Iter 94, disc loss: 0.0008637801179917047, policy loss: 7.936793301044432
Experience 14, Iter 95, disc loss: 0.0009253533384733939, policy loss: 7.5026775974285185
Experience 14, Iter 96, disc loss: 0.0008877350594941828, policy loss: 7.682154221510702
Experience 14, Iter 97, disc loss: 0.000880029293333112, policy loss: 7.613347273081531
Experience 14, Iter 98, disc loss: 0.0009935085123516292, policy loss: 7.3929473659843445
Experience 14, Iter 99, disc loss: 0.0008487825545140735, policy loss: 7.804897444005582
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0026],
        [0.1529],
        [1.5698],
        [0.0146]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0112, 0.1142, 0.7590, 0.0185, 0.0042, 3.4983]],

        [[0.0112, 0.1142, 0.7590, 0.0185, 0.0042, 3.4983]],

        [[0.0112, 0.1142, 0.7590, 0.0185, 0.0042, 3.4983]],

        [[0.0112, 0.1142, 0.7590, 0.0185, 0.0042, 3.4983]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0103, 0.6116, 6.2791, 0.0583], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0103, 0.6116, 6.2791, 0.0583])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.701
Iter 2/2000 - Loss: 2.924
Iter 3/2000 - Loss: 2.638
Iter 4/2000 - Loss: 2.654
Iter 5/2000 - Loss: 2.721
Iter 6/2000 - Loss: 2.653
Iter 7/2000 - Loss: 2.538
Iter 8/2000 - Loss: 2.463
Iter 9/2000 - Loss: 2.441
Iter 10/2000 - Loss: 2.417
Iter 11/2000 - Loss: 2.340
Iter 12/2000 - Loss: 2.217
Iter 13/2000 - Loss: 2.084
Iter 14/2000 - Loss: 1.964
Iter 15/2000 - Loss: 1.850
Iter 16/2000 - Loss: 1.711
Iter 17/2000 - Loss: 1.530
Iter 18/2000 - Loss: 1.314
Iter 19/2000 - Loss: 1.083
Iter 20/2000 - Loss: 0.846
Iter 1981/2000 - Loss: -7.883
Iter 1982/2000 - Loss: -7.883
Iter 1983/2000 - Loss: -7.883
Iter 1984/2000 - Loss: -7.883
Iter 1985/2000 - Loss: -7.883
Iter 1986/2000 - Loss: -7.883
Iter 1987/2000 - Loss: -7.883
Iter 1988/2000 - Loss: -7.883
Iter 1989/2000 - Loss: -7.883
Iter 1990/2000 - Loss: -7.883
Iter 1991/2000 - Loss: -7.883
Iter 1992/2000 - Loss: -7.883
Iter 1993/2000 - Loss: -7.883
Iter 1994/2000 - Loss: -7.883
Iter 1995/2000 - Loss: -7.883
Iter 1996/2000 - Loss: -7.883
Iter 1997/2000 - Loss: -7.883
Iter 1998/2000 - Loss: -7.883
Iter 1999/2000 - Loss: -7.883
Iter 2000/2000 - Loss: -7.883
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[13.3645,  5.9701, 44.1411, 12.4855, 14.7267, 51.0801]],

        [[18.2906, 33.4713, 11.8919,  1.1859,  5.0445, 27.4338]],

        [[19.4126, 35.8872, 11.2688,  1.0700,  3.0774, 22.3039]],

        [[15.1987, 28.9532, 15.1075,  3.5384,  1.3842, 44.0761]]])
Signal Variance: tensor([ 0.0620,  2.3416, 18.4089,  0.3576])
Estimated target variance: tensor([0.0103, 0.6116, 6.2791, 0.0583])
N: 150
Signal to noise ratio: tensor([14.3464, 85.1489, 90.8014, 36.3492])
Bound on condition number: tensor([  30873.7473, 1087552.0871, 1236734.5018,  198190.5557])
Policy Optimizer learning rate:
0.000985357783097011
Experience 15, Iter 0, disc loss: 0.0009151254499211395, policy loss: 7.6473654354104745
Experience 15, Iter 1, disc loss: 0.0009073650517735474, policy loss: 7.6039922074987105
Experience 15, Iter 2, disc loss: 0.0009025501710435267, policy loss: 7.588083941940268
Experience 15, Iter 3, disc loss: 0.0009876723154513558, policy loss: 7.418270366431939
Experience 15, Iter 4, disc loss: 0.0009200244999201402, policy loss: 7.562118602674791
Experience 15, Iter 5, disc loss: 0.0009021305713105977, policy loss: 7.577702102031758
Experience 15, Iter 6, disc loss: 0.0009233654303368154, policy loss: 7.499636441538225
Experience 15, Iter 7, disc loss: 0.0009044269613533259, policy loss: 7.6052080016293235
Experience 15, Iter 8, disc loss: 0.0009561247037630645, policy loss: 7.558050195039902
Experience 15, Iter 9, disc loss: 0.0009331913939165877, policy loss: 7.637145560702618
Experience 15, Iter 10, disc loss: 0.0008993230130624753, policy loss: 7.759916413911798
Experience 15, Iter 11, disc loss: 0.000908104053593037, policy loss: 7.543015422910152
Experience 15, Iter 12, disc loss: 0.0009640996842264217, policy loss: 7.456096447497552
Experience 15, Iter 13, disc loss: 0.0009808333585331346, policy loss: 7.443864163921067
Experience 15, Iter 14, disc loss: 0.0008257281520172785, policy loss: 7.879772298080274
Experience 15, Iter 15, disc loss: 0.0009207672551554775, policy loss: 7.584626683025268
Experience 15, Iter 16, disc loss: 0.0009072370247083448, policy loss: 7.61625894751012
Experience 15, Iter 17, disc loss: 0.0008215944426374496, policy loss: 7.650589446451418
Experience 15, Iter 18, disc loss: 0.0008826986159204225, policy loss: 7.60139129321435
Experience 15, Iter 19, disc loss: 0.0008742947916752405, policy loss: 7.624350531038729
Experience 15, Iter 20, disc loss: 0.000854900277596232, policy loss: 7.643699663299532
Experience 15, Iter 21, disc loss: 0.0009086380546072007, policy loss: 7.600227351569734
Experience 15, Iter 22, disc loss: 0.0008755355043013186, policy loss: 7.606677549953721
Experience 15, Iter 23, disc loss: 0.0008363034797373157, policy loss: 7.66949607032198
Experience 15, Iter 24, disc loss: 0.0007880423396947438, policy loss: 7.762152053706696
Experience 15, Iter 25, disc loss: 0.0008021340307714942, policy loss: 7.798573133588392
Experience 15, Iter 26, disc loss: 0.0008311686603482607, policy loss: 7.807808371636385
Experience 15, Iter 27, disc loss: 0.000877507929298132, policy loss: 7.565254854058088
Experience 15, Iter 28, disc loss: 0.0009376056056294806, policy loss: 7.8603108517944165
Experience 15, Iter 29, disc loss: 0.0009043998738660464, policy loss: 7.580889015324043
Experience 15, Iter 30, disc loss: 0.0008950450971519301, policy loss: 7.574413439317031
Experience 15, Iter 31, disc loss: 0.0008173060682397889, policy loss: 7.734701823047538
Experience 15, Iter 32, disc loss: 0.0009326766265794494, policy loss: 7.464406789120305
Experience 15, Iter 33, disc loss: 0.0009393681962902851, policy loss: 7.47704560463238
Experience 15, Iter 34, disc loss: 0.0009144267360155002, policy loss: 7.523478549827803
Experience 15, Iter 35, disc loss: 0.000895971396597485, policy loss: 7.558373903647728
Experience 15, Iter 36, disc loss: 0.0008324845939825768, policy loss: 7.802348811793968
Experience 15, Iter 37, disc loss: 0.0008770571929146511, policy loss: 7.618889702485372
Experience 15, Iter 38, disc loss: 0.0008262450533548173, policy loss: 7.687277097139226
Experience 15, Iter 39, disc loss: 0.000864192867766472, policy loss: 7.841253464424311
Experience 15, Iter 40, disc loss: 0.0007832081105654182, policy loss: 7.783129425639235
Experience 15, Iter 41, disc loss: 0.0007949716723628709, policy loss: 7.750443056048057
Experience 15, Iter 42, disc loss: 0.0007523671816323669, policy loss: 7.8965878869832355
Experience 15, Iter 43, disc loss: 0.0008427203836709307, policy loss: 7.6399564141054865
Experience 15, Iter 44, disc loss: 0.0008103904774229446, policy loss: 7.7559179488973315
Experience 15, Iter 45, disc loss: 0.0008057321927829729, policy loss: 7.766437072259839
Experience 15, Iter 46, disc loss: 0.0008053096029990027, policy loss: 7.698076090484402
Experience 15, Iter 47, disc loss: 0.0007605658371759415, policy loss: 7.99466478202527
Experience 15, Iter 48, disc loss: 0.0007856743762498832, policy loss: 7.727667053332613
Experience 15, Iter 49, disc loss: 0.000835746633013004, policy loss: 7.71335621429937
Experience 15, Iter 50, disc loss: 0.0008486470397537367, policy loss: 7.6534498245598686
Experience 15, Iter 51, disc loss: 0.0007824161344834089, policy loss: 7.81080993488319
Experience 15, Iter 52, disc loss: 0.000835736761066952, policy loss: 7.612785389346444
Experience 15, Iter 53, disc loss: 0.0008204678670355081, policy loss: 7.607875757856759
Experience 15, Iter 54, disc loss: 0.0008024278855843154, policy loss: 7.771434105640048
Experience 15, Iter 55, disc loss: 0.0008620765736589387, policy loss: 7.635358025340724
Experience 15, Iter 56, disc loss: 0.0008263715024798817, policy loss: 7.73906737519368
Experience 15, Iter 57, disc loss: 0.0008193359030848095, policy loss: 7.682019843158351
Experience 15, Iter 58, disc loss: 0.0008094560936144521, policy loss: 7.812727389971106
Experience 15, Iter 59, disc loss: 0.0007617193811439691, policy loss: 8.037632028463015
Experience 15, Iter 60, disc loss: 0.0008095957880084959, policy loss: 7.7014448763225865
Experience 15, Iter 61, disc loss: 0.0008180357071634047, policy loss: 7.742634670477138
Experience 15, Iter 62, disc loss: 0.0007967613463460241, policy loss: 7.737506793471931
Experience 15, Iter 63, disc loss: 0.0008092104532640846, policy loss: 7.657930165468566
Experience 15, Iter 64, disc loss: 0.0007953123846200252, policy loss: 7.781166284771698
Experience 15, Iter 65, disc loss: 0.0007716809855307961, policy loss: 7.796212184931315
Experience 15, Iter 66, disc loss: 0.0007619454687536443, policy loss: 7.973076597787335
Experience 15, Iter 67, disc loss: 0.0007968868956011597, policy loss: 7.694452031203548
Experience 15, Iter 68, disc loss: 0.0007866507149612502, policy loss: 7.802490177787066
Experience 15, Iter 69, disc loss: 0.0008652881631402552, policy loss: 7.607867839056082
Experience 15, Iter 70, disc loss: 0.0007375310345315361, policy loss: 7.915904708901018
Experience 15, Iter 71, disc loss: 0.0007197322601868556, policy loss: 7.832464274518529
Experience 15, Iter 72, disc loss: 0.0007402203376780556, policy loss: 7.913339054558955
Experience 15, Iter 73, disc loss: 0.0008047367371932477, policy loss: 7.700841512981999
Experience 15, Iter 74, disc loss: 0.0007688052797630701, policy loss: 7.893388189709968
Experience 15, Iter 75, disc loss: 0.0008317685842686532, policy loss: 7.708057971854292
Experience 15, Iter 76, disc loss: 0.000795427992365842, policy loss: 7.69602898885649
Experience 15, Iter 77, disc loss: 0.0007782896264211589, policy loss: 7.7342056802053225
Experience 15, Iter 78, disc loss: 0.0007999008371656023, policy loss: 7.677994098622331
Experience 15, Iter 79, disc loss: 0.0007692096652918594, policy loss: 7.738649782604207
Experience 15, Iter 80, disc loss: 0.0007565286559669832, policy loss: 7.810096780861819
Experience 15, Iter 81, disc loss: 0.0007890152767775077, policy loss: 7.680149209936322
Experience 15, Iter 82, disc loss: 0.0007672480002284723, policy loss: 7.767034835492745
Experience 15, Iter 83, disc loss: 0.0007490731751840218, policy loss: 7.7687729595556965
Experience 15, Iter 84, disc loss: 0.0007486279276210936, policy loss: 7.797217658661982
Experience 15, Iter 85, disc loss: 0.0007639709267113208, policy loss: 7.733442706971324
Experience 15, Iter 86, disc loss: 0.0008125777945026304, policy loss: 7.610953017139122
Experience 15, Iter 87, disc loss: 0.0007538977485862612, policy loss: 7.870788088022006
Experience 15, Iter 88, disc loss: 0.0007829120733540459, policy loss: 8.042500865923046
Experience 15, Iter 89, disc loss: 0.0007761312109695845, policy loss: 7.832535173003624
Experience 15, Iter 90, disc loss: 0.0008252500174286101, policy loss: 7.602020210850475
Experience 15, Iter 91, disc loss: 0.000735701587322015, policy loss: 7.992496663899683
Experience 15, Iter 92, disc loss: 0.0007928264711181631, policy loss: 7.704921998886005
Experience 15, Iter 93, disc loss: 0.000736625982825939, policy loss: 7.965339786481384
Experience 15, Iter 94, disc loss: 0.0007879865919905298, policy loss: 7.6359204298293015
Experience 15, Iter 95, disc loss: 0.0007716787828110597, policy loss: 7.686479319016602
Experience 15, Iter 96, disc loss: 0.0007796090391615351, policy loss: 7.846109423381063
Experience 15, Iter 97, disc loss: 0.0007916685754066175, policy loss: 7.684585344045898
Experience 15, Iter 98, disc loss: 0.0008169899467184876, policy loss: 7.652927629350859
Experience 15, Iter 99, disc loss: 0.0007292587159400757, policy loss: 7.910814540316278
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.1581],
        [1.6026],
        [0.0143]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0109, 0.1108, 0.7544, 0.0183, 0.0040, 3.5887]],

        [[0.0109, 0.1108, 0.7544, 0.0183, 0.0040, 3.5887]],

        [[0.0109, 0.1108, 0.7544, 0.0183, 0.0040, 3.5887]],

        [[0.0109, 0.1108, 0.7544, 0.0183, 0.0040, 3.5887]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.6324, 6.4103, 0.0573], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.6324, 6.4103, 0.0573])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.701
Iter 2/2000 - Loss: 2.952
Iter 3/2000 - Loss: 2.642
Iter 4/2000 - Loss: 2.664
Iter 5/2000 - Loss: 2.741
Iter 6/2000 - Loss: 2.674
Iter 7/2000 - Loss: 2.556
Iter 8/2000 - Loss: 2.475
Iter 9/2000 - Loss: 2.452
Iter 10/2000 - Loss: 2.434
Iter 11/2000 - Loss: 2.368
Iter 12/2000 - Loss: 2.249
Iter 13/2000 - Loss: 2.112
Iter 14/2000 - Loss: 1.985
Iter 15/2000 - Loss: 1.866
Iter 16/2000 - Loss: 1.728
Iter 17/2000 - Loss: 1.549
Iter 18/2000 - Loss: 1.328
Iter 19/2000 - Loss: 1.083
Iter 20/2000 - Loss: 0.830
Iter 1981/2000 - Loss: -7.995
Iter 1982/2000 - Loss: -7.995
Iter 1983/2000 - Loss: -7.995
Iter 1984/2000 - Loss: -7.995
Iter 1985/2000 - Loss: -7.995
Iter 1986/2000 - Loss: -7.995
Iter 1987/2000 - Loss: -7.995
Iter 1988/2000 - Loss: -7.995
Iter 1989/2000 - Loss: -7.995
Iter 1990/2000 - Loss: -7.995
Iter 1991/2000 - Loss: -7.995
Iter 1992/2000 - Loss: -7.996
Iter 1993/2000 - Loss: -7.996
Iter 1994/2000 - Loss: -7.996
Iter 1995/2000 - Loss: -7.996
Iter 1996/2000 - Loss: -7.996
Iter 1997/2000 - Loss: -7.996
Iter 1998/2000 - Loss: -7.996
Iter 1999/2000 - Loss: -7.996
Iter 2000/2000 - Loss: -7.996
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[13.0930,  5.8981, 43.4689, 12.6922, 14.1314, 50.2419]],

        [[17.3336, 32.6290, 12.0580,  1.1837,  5.0404, 26.4125]],

        [[19.0278, 35.4104, 11.2256,  1.0646,  3.0257, 20.6082]],

        [[14.8365, 28.2882, 15.1111,  3.6371,  1.3986, 43.3161]]])
Signal Variance: tensor([ 0.0611,  2.3289, 16.8339,  0.3597])
Estimated target variance: tensor([0.0099, 0.6324, 6.4103, 0.0573])
N: 160
Signal to noise ratio: tensor([14.0575, 86.0953, 88.7834, 37.2391])
Bound on condition number: tensor([  31619.2650, 1185985.4474, 1261200.5113,  221880.6132])
Policy Optimizer learning rate:
0.0009843201517785074
Experience 16, Iter 0, disc loss: 0.0007701882035725422, policy loss: 7.702485841090384
Experience 16, Iter 1, disc loss: 0.0007986186985121048, policy loss: 7.707107267281063
Experience 16, Iter 2, disc loss: 0.0007834362931382571, policy loss: 7.710868181114179
Experience 16, Iter 3, disc loss: 0.0007658639099394897, policy loss: 7.763847099012184
Experience 16, Iter 4, disc loss: 0.0007358143136486718, policy loss: 7.881743368176995
Experience 16, Iter 5, disc loss: 0.0007690430319367786, policy loss: 7.79245668997609
Experience 16, Iter 6, disc loss: 0.0007133163525361787, policy loss: 7.890424566675762
Experience 16, Iter 7, disc loss: 0.0007641499857945616, policy loss: 7.870397898335139
Experience 16, Iter 8, disc loss: 0.0008012862517131818, policy loss: 7.778623763733491
Experience 16, Iter 9, disc loss: 0.000804748103957288, policy loss: 7.9033735565952306
Experience 16, Iter 10, disc loss: 0.0007384716678901591, policy loss: 7.740993660922161
Experience 16, Iter 11, disc loss: 0.0007371087362682865, policy loss: 7.775867686296481
Experience 16, Iter 12, disc loss: 0.0007495255080697558, policy loss: 7.7295133668351665
Experience 16, Iter 13, disc loss: 0.000783238970768259, policy loss: 7.695841591372812
Experience 16, Iter 14, disc loss: 0.0007645637041316373, policy loss: 7.848464036068927
Experience 16, Iter 15, disc loss: 0.0007309665180948034, policy loss: 7.768351633176998
Experience 16, Iter 16, disc loss: 0.000799994178638015, policy loss: 7.643763171084376
Experience 16, Iter 17, disc loss: 0.0007183069043352208, policy loss: 7.939304092509288
Experience 16, Iter 18, disc loss: 0.0006950598377438648, policy loss: 7.887533976888017
Experience 16, Iter 19, disc loss: 0.0007695450209598904, policy loss: 7.656095188249859
Experience 16, Iter 20, disc loss: 0.0007140044107547714, policy loss: 8.019195164439845
Experience 16, Iter 21, disc loss: 0.0007614521707482432, policy loss: 7.866724913807601
Experience 16, Iter 22, disc loss: 0.0007160730047658975, policy loss: 7.894212992330804
Experience 16, Iter 23, disc loss: 0.0007570551189067302, policy loss: 7.721538288104949
Experience 16, Iter 24, disc loss: 0.0007421864041103767, policy loss: 7.9586162053195775
Experience 16, Iter 25, disc loss: 0.0007262085197489176, policy loss: 7.836683741077826
Experience 16, Iter 26, disc loss: 0.0007028543312412606, policy loss: 7.864830016198296
Experience 16, Iter 27, disc loss: 0.0007434669406937317, policy loss: 7.824929086815251
Experience 16, Iter 28, disc loss: 0.0006736040668835274, policy loss: 7.9770400887222035
Experience 16, Iter 29, disc loss: 0.0007850058361405927, policy loss: 7.6923769863391644
Experience 16, Iter 30, disc loss: 0.000727366739654777, policy loss: 7.815992650716485
Experience 16, Iter 31, disc loss: 0.0007253273579253554, policy loss: 7.762762358998624
Experience 16, Iter 32, disc loss: 0.0007660619695868025, policy loss: 7.771769900123813
Experience 16, Iter 33, disc loss: 0.0006799210568362497, policy loss: 7.9646418735192155
Experience 16, Iter 34, disc loss: 0.0007232971654534488, policy loss: 7.838529352206979
Experience 16, Iter 35, disc loss: 0.0007080303858099753, policy loss: 7.819569257710578
Experience 16, Iter 36, disc loss: 0.0006472410346292997, policy loss: 8.076787428802316
Experience 16, Iter 37, disc loss: 0.0007138463330939628, policy loss: 7.777079536847693
Experience 16, Iter 38, disc loss: 0.0006457677502402629, policy loss: 7.953151625477767
Experience 16, Iter 39, disc loss: 0.0006176484122954456, policy loss: 8.385727391259604
Experience 16, Iter 40, disc loss: 0.0006642627775686513, policy loss: 7.9033704766936905
Experience 16, Iter 41, disc loss: 0.0006961867108220891, policy loss: 8.103618647115892
Experience 16, Iter 42, disc loss: 0.0006986895999390303, policy loss: 7.840739596615766
Experience 16, Iter 43, disc loss: 0.0006037879230972381, policy loss: 8.46804277432468
Experience 16, Iter 44, disc loss: 0.0006528278461422842, policy loss: 7.902788759588407
Experience 16, Iter 45, disc loss: 0.0005809954522979441, policy loss: 8.111469433615557
Experience 16, Iter 46, disc loss: 0.0005254352674441598, policy loss: 8.340417923334865
Experience 16, Iter 47, disc loss: 0.0005080863258630907, policy loss: 8.412761482291433
Experience 16, Iter 48, disc loss: 0.0005271830252383973, policy loss: 8.371335764693496
Experience 16, Iter 49, disc loss: 0.000525583143857724, policy loss: 8.560865777064091
Experience 16, Iter 50, disc loss: 0.0005228185559667248, policy loss: 8.44996536596935
Experience 16, Iter 51, disc loss: 0.0005923858666988665, policy loss: 8.141823587678642
Experience 16, Iter 52, disc loss: 0.0006133655680212273, policy loss: 8.05559537964502
Experience 16, Iter 53, disc loss: 0.0006602106578091206, policy loss: 7.967560793663482
Experience 16, Iter 54, disc loss: 0.0005380006302968278, policy loss: 8.475882201659505
Experience 16, Iter 55, disc loss: 0.0006785045694611792, policy loss: 7.822374316439945
Experience 16, Iter 56, disc loss: 0.0005487624396049946, policy loss: 8.211663941804844
Experience 16, Iter 57, disc loss: 0.0005227577866220531, policy loss: 8.293078323707125
Experience 16, Iter 58, disc loss: 0.0005069318416114684, policy loss: 8.438545107068668
Experience 16, Iter 59, disc loss: 0.0004556036239413655, policy loss: 8.659592200530247
Experience 16, Iter 60, disc loss: 0.0004785549462416208, policy loss: 8.59897021407158
Experience 16, Iter 61, disc loss: 0.00048028090838480225, policy loss: 8.493879766346502
Experience 16, Iter 62, disc loss: 0.0005068351429823802, policy loss: 8.384322062610238
Experience 16, Iter 63, disc loss: 0.0005307050232824001, policy loss: 8.339516128807428
Experience 16, Iter 64, disc loss: 0.0006009588826762429, policy loss: 8.179551128021657
Experience 16, Iter 65, disc loss: 0.0006369780455606, policy loss: 7.926013434865869
Experience 16, Iter 66, disc loss: 0.0005792310005137153, policy loss: 8.296571771247395
Experience 16, Iter 67, disc loss: 0.0005532455020313291, policy loss: 8.332826614585606
Experience 16, Iter 68, disc loss: 0.0007163760456508372, policy loss: 7.752968456068971
Experience 16, Iter 69, disc loss: 0.0006398641527419373, policy loss: 7.8844994784267435
Experience 16, Iter 70, disc loss: 0.0005717863932416787, policy loss: 8.114880209570181
Experience 16, Iter 71, disc loss: 0.0005745453094713235, policy loss: 8.126165510553315
Experience 16, Iter 72, disc loss: 0.0005618506528245085, policy loss: 8.161494535059825
Experience 16, Iter 73, disc loss: 0.0006193508021562146, policy loss: 8.105140237291131
Experience 16, Iter 74, disc loss: 0.0006254689076694612, policy loss: 8.15792984679381
Experience 16, Iter 75, disc loss: 0.0006821601532379889, policy loss: 7.912724528352339
Experience 16, Iter 76, disc loss: 0.0006444182840237077, policy loss: 7.892233257442895
Experience 16, Iter 77, disc loss: 0.0006069064749038807, policy loss: 8.14872217071267
Experience 16, Iter 78, disc loss: 0.0006359863838776431, policy loss: 8.109221124604217
Experience 16, Iter 79, disc loss: 0.0006514550157124273, policy loss: 7.956662578060275
Experience 16, Iter 80, disc loss: 0.0005976014026755612, policy loss: 8.116817167518557
Experience 16, Iter 81, disc loss: 0.000649805599929945, policy loss: 7.94014268028074
Experience 16, Iter 82, disc loss: 0.0006161199763929988, policy loss: 7.970383440766533
Experience 16, Iter 83, disc loss: 0.0006669699503941499, policy loss: 7.9121021043898345
Experience 16, Iter 84, disc loss: 0.0006453930163432804, policy loss: 8.002682484606858
Experience 16, Iter 85, disc loss: 0.0006665516178106172, policy loss: 8.136047945017243
Experience 16, Iter 86, disc loss: 0.0006552341863813138, policy loss: 7.951427003115964
Experience 16, Iter 87, disc loss: 0.0006289179276950671, policy loss: 7.991542290122866
Experience 16, Iter 88, disc loss: 0.0006628718934032567, policy loss: 7.944108093762825
Experience 16, Iter 89, disc loss: 0.0006135952186969162, policy loss: 7.9827194405387365
Experience 16, Iter 90, disc loss: 0.0006257778198073217, policy loss: 8.0615297611098
Experience 16, Iter 91, disc loss: 0.000653816050030414, policy loss: 8.117269000905846
Experience 16, Iter 92, disc loss: 0.0006233547501141484, policy loss: 8.04433030990151
Experience 16, Iter 93, disc loss: 0.0006293247581815383, policy loss: 7.950968093904908
Experience 16, Iter 94, disc loss: 0.0006494132951097797, policy loss: 7.9721235813572235
Experience 16, Iter 95, disc loss: 0.0006560352168651852, policy loss: 8.04303824692347
Experience 16, Iter 96, disc loss: 0.0006246022843433374, policy loss: 7.939166701338392
Experience 16, Iter 97, disc loss: 0.0006292841546570233, policy loss: 8.016361339850047
Experience 16, Iter 98, disc loss: 0.0006390045175546453, policy loss: 7.9184499747141786
Experience 16, Iter 99, disc loss: 0.0006809132725993538, policy loss: 7.831618479867982
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.1615],
        [1.6145],
        [0.0140]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0107, 0.1078, 0.7425, 0.0180, 0.0038, 3.6597]],

        [[0.0107, 0.1078, 0.7425, 0.0180, 0.0038, 3.6597]],

        [[0.0107, 0.1078, 0.7425, 0.0180, 0.0038, 3.6597]],

        [[0.0107, 0.1078, 0.7425, 0.0180, 0.0038, 3.6597]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0097, 0.6461, 6.4579, 0.0558], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0097, 0.6461, 6.4579, 0.0558])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.693
Iter 2/2000 - Loss: 2.963
Iter 3/2000 - Loss: 2.636
Iter 4/2000 - Loss: 2.662
Iter 5/2000 - Loss: 2.746
Iter 6/2000 - Loss: 2.679
Iter 7/2000 - Loss: 2.559
Iter 8/2000 - Loss: 2.477
Iter 9/2000 - Loss: 2.453
Iter 10/2000 - Loss: 2.440
Iter 11/2000 - Loss: 2.382
Iter 12/2000 - Loss: 2.268
Iter 13/2000 - Loss: 2.130
Iter 14/2000 - Loss: 1.998
Iter 15/2000 - Loss: 1.876
Iter 16/2000 - Loss: 1.741
Iter 17/2000 - Loss: 1.567
Iter 18/2000 - Loss: 1.350
Iter 19/2000 - Loss: 1.103
Iter 20/2000 - Loss: 0.846
Iter 1981/2000 - Loss: -8.046
Iter 1982/2000 - Loss: -8.046
Iter 1983/2000 - Loss: -8.046
Iter 1984/2000 - Loss: -8.046
Iter 1985/2000 - Loss: -8.046
Iter 1986/2000 - Loss: -8.046
Iter 1987/2000 - Loss: -8.046
Iter 1988/2000 - Loss: -8.046
Iter 1989/2000 - Loss: -8.046
Iter 1990/2000 - Loss: -8.046
Iter 1991/2000 - Loss: -8.046
Iter 1992/2000 - Loss: -8.046
Iter 1993/2000 - Loss: -8.046
Iter 1994/2000 - Loss: -8.046
Iter 1995/2000 - Loss: -8.046
Iter 1996/2000 - Loss: -8.046
Iter 1997/2000 - Loss: -8.046
Iter 1998/2000 - Loss: -8.046
Iter 1999/2000 - Loss: -8.046
Iter 2000/2000 - Loss: -8.046
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[12.7163,  5.8037, 42.9766, 12.1803, 13.8941, 51.7381]],

        [[15.2422, 31.6976, 12.3624,  1.1647,  5.1920, 25.4069]],

        [[17.8227, 34.7229, 11.2520,  1.0616,  3.0336, 19.0047]],

        [[14.6709, 28.0763, 15.5290,  3.7385,  1.3768, 44.0551]]])
Signal Variance: tensor([ 0.0589,  2.3330, 15.7386,  0.3747])
Estimated target variance: tensor([0.0097, 0.6461, 6.4579, 0.0558])
N: 170
Signal to noise ratio: tensor([13.6230, 85.2114, 86.4843, 38.0016])
Bound on condition number: tensor([  31550.5398, 1234366.8644, 1271521.4900,  245501.0706])
Policy Optimizer learning rate:
0.000983283613137985
Experience 17, Iter 0, disc loss: 0.0006144135140152898, policy loss: 8.097260836263263
Experience 17, Iter 1, disc loss: 0.0006267349196310269, policy loss: 8.108530114357846
Experience 17, Iter 2, disc loss: 0.0006649998138289906, policy loss: 7.827440181338757
Experience 17, Iter 3, disc loss: 0.0005832264695020455, policy loss: 8.065583239408976
Experience 17, Iter 4, disc loss: 0.0006513732734809694, policy loss: 7.933535188737977
Experience 17, Iter 5, disc loss: 0.0006462414979473889, policy loss: 7.926590556159248
Experience 17, Iter 6, disc loss: 0.0006515248121950386, policy loss: 8.060087882679422
Experience 17, Iter 7, disc loss: 0.000598426666377221, policy loss: 8.203431156550447
Experience 17, Iter 8, disc loss: 0.0006262593117363262, policy loss: 8.194272645036332
Experience 17, Iter 9, disc loss: 0.0006423393537155396, policy loss: 8.158294050890717
Experience 17, Iter 10, disc loss: 0.00061261919353281, policy loss: 7.98788922969866
Experience 17, Iter 11, disc loss: 0.0006244487017794197, policy loss: 7.977412382312712
Experience 17, Iter 12, disc loss: 0.000625564595617035, policy loss: 7.987267987647649
Experience 17, Iter 13, disc loss: 0.000639314393362202, policy loss: 8.039535375598168
Experience 17, Iter 14, disc loss: 0.0006674356808909609, policy loss: 7.839174357906236
Experience 17, Iter 15, disc loss: 0.000614208163230924, policy loss: 8.220312677282177
Experience 17, Iter 16, disc loss: 0.0006350771268453959, policy loss: 7.957479090634214
Experience 17, Iter 17, disc loss: 0.0005930083482274136, policy loss: 8.098098614730183
Experience 17, Iter 18, disc loss: 0.0006445413715875316, policy loss: 7.991263319536004
Experience 17, Iter 19, disc loss: 0.0005998632726817172, policy loss: 8.0928946035016
Experience 17, Iter 20, disc loss: 0.0006389992064155179, policy loss: 8.027273167734158
Experience 17, Iter 21, disc loss: 0.0006832579570333268, policy loss: 7.794410438608569
Experience 17, Iter 22, disc loss: 0.000630110701134261, policy loss: 8.253644983375587
Experience 17, Iter 23, disc loss: 0.0006451168211282919, policy loss: 7.865334323687748
Experience 17, Iter 24, disc loss: 0.0006322318490179669, policy loss: 7.922749997314128
Experience 17, Iter 25, disc loss: 0.0006743830366936825, policy loss: 7.993529702813774
Experience 17, Iter 26, disc loss: 0.0006741378621573875, policy loss: 7.8256910336518795
Experience 17, Iter 27, disc loss: 0.0006657352750562092, policy loss: 7.8326659314665905
Experience 17, Iter 28, disc loss: 0.0006194060056415956, policy loss: 8.04458349083696
Experience 17, Iter 29, disc loss: 0.0006359279147667067, policy loss: 8.034491365701204
Experience 17, Iter 30, disc loss: 0.0006151299182174991, policy loss: 8.079574061838066
Experience 17, Iter 31, disc loss: 0.0006260854223665598, policy loss: 7.926057049428202
Experience 17, Iter 32, disc loss: 0.0006360902586550691, policy loss: 7.9139591175172495
Experience 17, Iter 33, disc loss: 0.0006237233555705875, policy loss: 7.996071466810923
Experience 17, Iter 34, disc loss: 0.0006415904151789188, policy loss: 8.08806672023903
Experience 17, Iter 35, disc loss: 0.0006154668114546271, policy loss: 8.155721316384689
Experience 17, Iter 36, disc loss: 0.0006433751345273446, policy loss: 7.849516593389916
Experience 17, Iter 37, disc loss: 0.0006345567046098265, policy loss: 7.980033784524163
Experience 17, Iter 38, disc loss: 0.0006016664717091765, policy loss: 8.161038699938615
Experience 17, Iter 39, disc loss: 0.0006376084752860399, policy loss: 7.874066661027751
Experience 17, Iter 40, disc loss: 0.0005609787054258443, policy loss: 8.250582618446064
Experience 17, Iter 41, disc loss: 0.000600243941788635, policy loss: 7.971284886616986
Experience 17, Iter 42, disc loss: 0.0006331088581886402, policy loss: 7.945490527677354
Experience 17, Iter 43, disc loss: 0.0005916838727808648, policy loss: 7.9965437688854974
Experience 17, Iter 44, disc loss: 0.0005869101181275277, policy loss: 8.207648710709964
Experience 17, Iter 45, disc loss: 0.0005779948902568804, policy loss: 8.003806978044341
Experience 17, Iter 46, disc loss: 0.0005643516990642612, policy loss: 8.086743248810905
Experience 17, Iter 47, disc loss: 0.0005468199157075167, policy loss: 8.153713304542915
Experience 17, Iter 48, disc loss: 0.0005452575847214121, policy loss: 8.237666238343973
Experience 17, Iter 49, disc loss: 0.000582009209330116, policy loss: 8.101408713314498
Experience 17, Iter 50, disc loss: 0.0005982989667909302, policy loss: 8.024868124276441
Experience 17, Iter 51, disc loss: 0.0006193349172067948, policy loss: 7.976402044989021
Experience 17, Iter 52, disc loss: 0.0005806647528763473, policy loss: 8.171593153420291
Experience 17, Iter 53, disc loss: 0.0006724258947108538, policy loss: 7.925243128936376
Experience 17, Iter 54, disc loss: 0.0005974072143508622, policy loss: 8.012643593404366
Experience 17, Iter 55, disc loss: 0.0006134005535490171, policy loss: 7.914744826296694
Experience 17, Iter 56, disc loss: 0.0005911180723867553, policy loss: 7.990509609588818
Experience 17, Iter 57, disc loss: 0.000599891294400081, policy loss: 8.023439746908911
Experience 17, Iter 58, disc loss: 0.0006252035523179263, policy loss: 7.99714980632552
Experience 17, Iter 59, disc loss: 0.0006042502490626341, policy loss: 7.983923349837241
Experience 17, Iter 60, disc loss: 0.0006059624135590136, policy loss: 7.97047705601125
Experience 17, Iter 61, disc loss: 0.0005672577488854627, policy loss: 8.087295731230077
Experience 17, Iter 62, disc loss: 0.0005805533820171654, policy loss: 8.150540870857998
Experience 17, Iter 63, disc loss: 0.0006243131446544744, policy loss: 7.880600233853551
Experience 17, Iter 64, disc loss: 0.0005845009410551483, policy loss: 8.151432952423027
Experience 17, Iter 65, disc loss: 0.0005777339998808618, policy loss: 8.187480825741094
Experience 17, Iter 66, disc loss: 0.0005871004234634985, policy loss: 8.058008318997238
Experience 17, Iter 67, disc loss: 0.0005646697041056196, policy loss: 8.125456708548551
Experience 17, Iter 68, disc loss: 0.0005854873997671358, policy loss: 8.014259359980759
Experience 17, Iter 69, disc loss: 0.0006129330081113964, policy loss: 7.9023026162477015
Experience 17, Iter 70, disc loss: 0.0005827225084234914, policy loss: 7.973286988342917
Experience 17, Iter 71, disc loss: 0.0006096260629727534, policy loss: 8.056013190207764
Experience 17, Iter 72, disc loss: 0.0006017494668548988, policy loss: 7.924155186980097
Experience 17, Iter 73, disc loss: 0.0005716302854440471, policy loss: 8.063568622547768
Experience 17, Iter 74, disc loss: 0.000541401048638823, policy loss: 8.411145281793754
Experience 17, Iter 75, disc loss: 0.000587400591129135, policy loss: 7.99392672784983
Experience 17, Iter 76, disc loss: 0.000557596896816716, policy loss: 8.07910267272559
Experience 17, Iter 77, disc loss: 0.0005618230575610974, policy loss: 8.158431998641298
Experience 17, Iter 78, disc loss: 0.0005589999135983601, policy loss: 8.110670657083135
Experience 17, Iter 79, disc loss: 0.0005506018120038518, policy loss: 8.143656794417133
Experience 17, Iter 80, disc loss: 0.0005658646635597106, policy loss: 8.284834232663354
Experience 17, Iter 81, disc loss: 0.0005762234895989288, policy loss: 8.102505639892088
Experience 17, Iter 82, disc loss: 0.0005664833306532988, policy loss: 8.197253857531761
Experience 17, Iter 83, disc loss: 0.0005578972399394367, policy loss: 8.109327020241944
Experience 17, Iter 84, disc loss: 0.0005702160400941111, policy loss: 8.309831293823368
Experience 17, Iter 85, disc loss: 0.0005628417450771834, policy loss: 8.060820575084751
Experience 17, Iter 86, disc loss: 0.0005448112026111293, policy loss: 8.275853318195614
Experience 17, Iter 87, disc loss: 0.0005571556522811424, policy loss: 8.119981276000845
Experience 17, Iter 88, disc loss: 0.0005684305056000004, policy loss: 8.039806035181755
Experience 17, Iter 89, disc loss: 0.0006161159963828707, policy loss: 8.016631796532323
Experience 17, Iter 90, disc loss: 0.0005621524938426559, policy loss: 8.478518150580024
Experience 17, Iter 91, disc loss: 0.0005945648124396102, policy loss: 8.099894571611467
Experience 17, Iter 92, disc loss: 0.0005788099686842657, policy loss: 7.987296394743088
Experience 17, Iter 93, disc loss: 0.0005701945082585804, policy loss: 8.075206777402023
Experience 17, Iter 94, disc loss: 0.0005680589383461977, policy loss: 8.012264674445928
Experience 17, Iter 95, disc loss: 0.0005909233731609596, policy loss: 7.953935754753253
Experience 17, Iter 96, disc loss: 0.0005461570734335929, policy loss: 8.26128217402645
Experience 17, Iter 97, disc loss: 0.0005544582973795575, policy loss: 8.15584540698146
Experience 17, Iter 98, disc loss: 0.0005169383123870507, policy loss: 8.321415722523316
Experience 17, Iter 99, disc loss: 0.000553352576914977, policy loss: 8.091914139983437
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1625],
        [1.6234],
        [0.0141]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0256e-02, 1.0461e-01, 7.4957e-01, 1.7879e-02, 3.6090e-03,
          3.6737e+00]],

        [[1.0256e-02, 1.0461e-01, 7.4957e-01, 1.7879e-02, 3.6090e-03,
          3.6737e+00]],

        [[1.0256e-02, 1.0461e-01, 7.4957e-01, 1.7879e-02, 3.6090e-03,
          3.6737e+00]],

        [[1.0256e-02, 1.0461e-01, 7.4957e-01, 1.7879e-02, 3.6090e-03,
          3.6737e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0093, 0.6501, 6.4934, 0.0564], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0093, 0.6501, 6.4934, 0.0564])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.685
Iter 2/2000 - Loss: 2.981
Iter 3/2000 - Loss: 2.631
Iter 4/2000 - Loss: 2.661
Iter 5/2000 - Loss: 2.755
Iter 6/2000 - Loss: 2.689
Iter 7/2000 - Loss: 2.564
Iter 8/2000 - Loss: 2.475
Iter 9/2000 - Loss: 2.450
Iter 10/2000 - Loss: 2.441
Iter 11/2000 - Loss: 2.387
Iter 12/2000 - Loss: 2.276
Iter 13/2000 - Loss: 2.135
Iter 14/2000 - Loss: 1.998
Iter 15/2000 - Loss: 1.873
Iter 16/2000 - Loss: 1.738
Iter 17/2000 - Loss: 1.568
Iter 18/2000 - Loss: 1.351
Iter 19/2000 - Loss: 1.099
Iter 20/2000 - Loss: 0.833
Iter 1981/2000 - Loss: -8.109
Iter 1982/2000 - Loss: -8.109
Iter 1983/2000 - Loss: -8.109
Iter 1984/2000 - Loss: -8.109
Iter 1985/2000 - Loss: -8.109
Iter 1986/2000 - Loss: -8.109
Iter 1987/2000 - Loss: -8.109
Iter 1988/2000 - Loss: -8.109
Iter 1989/2000 - Loss: -8.110
Iter 1990/2000 - Loss: -8.110
Iter 1991/2000 - Loss: -8.110
Iter 1992/2000 - Loss: -8.110
Iter 1993/2000 - Loss: -8.110
Iter 1994/2000 - Loss: -8.110
Iter 1995/2000 - Loss: -8.110
Iter 1996/2000 - Loss: -8.110
Iter 1997/2000 - Loss: -8.110
Iter 1998/2000 - Loss: -8.110
Iter 1999/2000 - Loss: -8.110
Iter 2000/2000 - Loss: -8.110
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[12.5161,  5.9203, 43.1467, 12.0686, 13.5289, 51.5331]],

        [[14.9561, 31.4784, 12.4460,  1.1786,  5.6225, 28.4416]],

        [[17.1062, 34.1185, 11.2309,  1.0447,  3.1593, 18.2280]],

        [[14.4688, 27.0541, 14.9875,  3.6621,  1.3720, 44.3789]]])
Signal Variance: tensor([ 0.0599,  2.6613, 15.1290,  0.3620])
Estimated target variance: tensor([0.0093, 0.6501, 6.4934, 0.0564])
N: 180
Signal to noise ratio: tensor([13.9303, 90.0149, 86.2819, 36.6302])
Bound on condition number: tensor([  34930.3908, 1458484.0722, 1340022.7148,  241519.8357])
Policy Optimizer learning rate:
0.000982248166024799
Experience 18, Iter 0, disc loss: 0.0005059553686577289, policy loss: 8.306607204428715
Experience 18, Iter 1, disc loss: 0.0005935777334810926, policy loss: 7.93048543776211
Experience 18, Iter 2, disc loss: 0.000505587990152636, policy loss: 8.506954583388557
Experience 18, Iter 3, disc loss: 0.0005842307402527892, policy loss: 7.999382796871812
Experience 18, Iter 4, disc loss: 0.0005625959267987748, policy loss: 8.06938306588323
Experience 18, Iter 5, disc loss: 0.0004976936222457239, policy loss: 8.334746377114648
Experience 18, Iter 6, disc loss: 0.0005168765671404241, policy loss: 8.150196873078826
Experience 18, Iter 7, disc loss: 0.0005568284786014867, policy loss: 8.048308652308172
Experience 18, Iter 8, disc loss: 0.0004743114879155306, policy loss: 8.433498952423307
Experience 18, Iter 9, disc loss: 0.0005751325300827117, policy loss: 8.027473593140673
Experience 18, Iter 10, disc loss: 0.0005012987587293791, policy loss: 8.259164444669636
Experience 18, Iter 11, disc loss: 0.0005042237793454459, policy loss: 8.316190931218031
Experience 18, Iter 12, disc loss: 0.0005129338016899086, policy loss: 8.3114615408605
Experience 18, Iter 13, disc loss: 0.0005446002593158812, policy loss: 8.203847935472695
Experience 18, Iter 14, disc loss: 0.0005547784592898118, policy loss: 8.033391907816577
Experience 18, Iter 15, disc loss: 0.0005175211885162619, policy loss: 8.139184148703647
Experience 18, Iter 16, disc loss: 0.0005371388603708968, policy loss: 8.145619208637106
Experience 18, Iter 17, disc loss: 0.0005227500639210505, policy loss: 8.200982925381497
Experience 18, Iter 18, disc loss: 0.0005034511249601226, policy loss: 8.290406568662345
Experience 18, Iter 19, disc loss: 0.0005570706206205813, policy loss: 8.04066253486165
Experience 18, Iter 20, disc loss: 0.0005524383708393196, policy loss: 8.205973757586177
Experience 18, Iter 21, disc loss: 0.0005275868348835705, policy loss: 8.12553142438251
Experience 18, Iter 22, disc loss: 0.0005405250885860584, policy loss: 8.080744697033284
Experience 18, Iter 23, disc loss: 0.0005378884775408713, policy loss: 8.097003971569514
Experience 18, Iter 24, disc loss: 0.0005278893414365516, policy loss: 8.181436714019332
Experience 18, Iter 25, disc loss: 0.0005230084026935479, policy loss: 8.197479991999323
Experience 18, Iter 26, disc loss: 0.0005297174857865543, policy loss: 8.175151947330079
Experience 18, Iter 27, disc loss: 0.0005581494870240352, policy loss: 8.120508291022432
Experience 18, Iter 28, disc loss: 0.0005392391868517196, policy loss: 8.208640437500552
Experience 18, Iter 29, disc loss: 0.0005156651545368452, policy loss: 8.300522676484196
Experience 18, Iter 30, disc loss: 0.0005260354196226901, policy loss: 8.170326780974486
Experience 18, Iter 31, disc loss: 0.0005555333759154359, policy loss: 8.014397028577466
Experience 18, Iter 32, disc loss: 0.0005367371185888055, policy loss: 8.0561225568385
Experience 18, Iter 33, disc loss: 0.0005193219576352118, policy loss: 8.272759709184678
Experience 18, Iter 34, disc loss: 0.0005346739223293549, policy loss: 8.136397848701051
Experience 18, Iter 35, disc loss: 0.0005352012050692901, policy loss: 8.038779571270911
Experience 18, Iter 36, disc loss: 0.0005090255608257788, policy loss: 8.247525285013955
Experience 18, Iter 37, disc loss: 0.0005165412970321579, policy loss: 8.262379734888384
Experience 18, Iter 38, disc loss: 0.000539254462082016, policy loss: 8.1224476233564
Experience 18, Iter 39, disc loss: 0.0005098270333984635, policy loss: 8.145713946315695
Experience 18, Iter 40, disc loss: 0.0005088632386735805, policy loss: 8.19490387792248
Experience 18, Iter 41, disc loss: 0.0005265116339230677, policy loss: 8.16370303113847
Experience 18, Iter 42, disc loss: 0.0005081658122207315, policy loss: 8.125224256601136
Experience 18, Iter 43, disc loss: 0.0005282304389505458, policy loss: 8.088036518941847
Experience 18, Iter 44, disc loss: 0.00047891566080003573, policy loss: 8.26551922819796
Experience 18, Iter 45, disc loss: 0.0005161573120455276, policy loss: 8.120756047574528
Experience 18, Iter 46, disc loss: 0.0005382861824664096, policy loss: 8.194204620178311
Experience 18, Iter 47, disc loss: 0.0004957677324458772, policy loss: 8.209025755930986
Experience 18, Iter 48, disc loss: 0.0005307908163937196, policy loss: 8.154627642912587
Experience 18, Iter 49, disc loss: 0.0005557151238684752, policy loss: 8.140428852030219
Experience 18, Iter 50, disc loss: 0.0005003530678604356, policy loss: 8.23058921829032
Experience 18, Iter 51, disc loss: 0.0005440814035761817, policy loss: 8.126361936912563
Experience 18, Iter 52, disc loss: 0.0004943727350370665, policy loss: 8.214458927437551
Experience 18, Iter 53, disc loss: 0.0005482244736773703, policy loss: 8.082792346001618
Experience 18, Iter 54, disc loss: 0.0005429972424962428, policy loss: 8.113179385869078
Experience 18, Iter 55, disc loss: 0.0005116736779105166, policy loss: 8.350088859843936
Experience 18, Iter 56, disc loss: 0.0005503981137351156, policy loss: 8.040694349568444
Experience 18, Iter 57, disc loss: 0.0005181785540856728, policy loss: 8.437649624135807
Experience 18, Iter 58, disc loss: 0.0005439444526920069, policy loss: 8.072391109320236
Experience 18, Iter 59, disc loss: 0.0005023603821154245, policy loss: 8.18695802669167
Experience 18, Iter 60, disc loss: 0.0005303762826151617, policy loss: 8.201963656291252
Experience 18, Iter 61, disc loss: 0.0005322017302075359, policy loss: 8.07690310884844
Experience 18, Iter 62, disc loss: 0.0005173199496218448, policy loss: 8.123525055542679
Experience 18, Iter 63, disc loss: 0.000495803262103871, policy loss: 8.16544305652615
Experience 18, Iter 64, disc loss: 0.0005227308893790438, policy loss: 8.141298682488575
Experience 18, Iter 65, disc loss: 0.0005014811291405622, policy loss: 8.274030834315901
Experience 18, Iter 66, disc loss: 0.00046834025504677296, policy loss: 8.347360193871385
Experience 18, Iter 67, disc loss: 0.0005077110094570105, policy loss: 8.142738929312559
Experience 18, Iter 68, disc loss: 0.0004983596269637601, policy loss: 8.333457608634927
Experience 18, Iter 69, disc loss: 0.00048714631103187825, policy loss: 8.386188513830989
Experience 18, Iter 70, disc loss: 0.00047073389002717325, policy loss: 8.366122423239666
Experience 18, Iter 71, disc loss: 0.0004948616953764269, policy loss: 8.154097661988118
Experience 18, Iter 72, disc loss: 0.0004907604834627818, policy loss: 8.182306903259512
Experience 18, Iter 73, disc loss: 0.00047113349344836744, policy loss: 8.252785962973949
Experience 18, Iter 74, disc loss: 0.00044545510477569375, policy loss: 8.349775945404037
Experience 18, Iter 75, disc loss: 0.0004949867280077934, policy loss: 8.155981045973235
Experience 18, Iter 76, disc loss: 0.0004891519621339387, policy loss: 8.291594238118641
Experience 18, Iter 77, disc loss: 0.0004984963191150204, policy loss: 8.312387516470137
Experience 18, Iter 78, disc loss: 0.0005151806197117322, policy loss: 8.13829513174474
Experience 18, Iter 79, disc loss: 0.0005144798636457374, policy loss: 8.145850586296547
Experience 18, Iter 80, disc loss: 0.00045138181318289715, policy loss: 8.295284504380486
Experience 18, Iter 81, disc loss: 0.00046432370841696424, policy loss: 8.242717828855568
Experience 18, Iter 82, disc loss: 0.00048068520779447944, policy loss: 8.22343827900781
Experience 18, Iter 83, disc loss: 0.0005457059804565295, policy loss: 8.136171699450278
Experience 18, Iter 84, disc loss: 0.0005146381988308679, policy loss: 8.280224176479708
Experience 18, Iter 85, disc loss: 0.00044167010633859035, policy loss: 8.527697014774782
Experience 18, Iter 86, disc loss: 0.0004807689303112098, policy loss: 8.341500032390018
Experience 18, Iter 87, disc loss: 0.00043733161676353326, policy loss: 8.322322599866538
Experience 18, Iter 88, disc loss: 0.00041949914242781665, policy loss: 8.507860818404076
Experience 18, Iter 89, disc loss: 0.0004037769091315231, policy loss: 8.5171372329081
Experience 18, Iter 90, disc loss: 0.00040456186427532866, policy loss: 8.593332194943013
Experience 18, Iter 91, disc loss: 0.0004233450124716219, policy loss: 8.479623903663253
Experience 18, Iter 92, disc loss: 0.0004385861509678023, policy loss: 8.409873285310953
Experience 18, Iter 93, disc loss: 0.00046771456280239045, policy loss: 8.27152944710802
Experience 18, Iter 94, disc loss: 0.0005135677623686068, policy loss: 8.11904557810765
Experience 18, Iter 95, disc loss: 0.0004725975846169803, policy loss: 8.518731122125935
Experience 18, Iter 96, disc loss: 0.0004845597183762696, policy loss: 8.291188640014177
Experience 18, Iter 97, disc loss: 0.00046542498191375124, policy loss: 8.380678342665036
Experience 18, Iter 98, disc loss: 0.00047072813620965744, policy loss: 8.347702999623381
Experience 18, Iter 99, disc loss: 0.0004850031356285876, policy loss: 8.276442527115286
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.1636],
        [1.6135],
        [0.0138]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.8887e-03, 1.0182e-01, 7.3449e-01, 1.7720e-02, 3.4512e-03,
          3.7204e+00]],

        [[9.8887e-03, 1.0182e-01, 7.3449e-01, 1.7720e-02, 3.4512e-03,
          3.7204e+00]],

        [[9.8887e-03, 1.0182e-01, 7.3449e-01, 1.7720e-02, 3.4512e-03,
          3.7204e+00]],

        [[9.8887e-03, 1.0182e-01, 7.3449e-01, 1.7720e-02, 3.4512e-03,
          3.7204e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0091, 0.6543, 6.4541, 0.0552], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0091, 0.6543, 6.4541, 0.0552])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.668
Iter 2/2000 - Loss: 2.974
Iter 3/2000 - Loss: 2.612
Iter 4/2000 - Loss: 2.645
Iter 5/2000 - Loss: 2.742
Iter 6/2000 - Loss: 2.674
Iter 7/2000 - Loss: 2.547
Iter 8/2000 - Loss: 2.455
Iter 9/2000 - Loss: 2.425
Iter 10/2000 - Loss: 2.414
Iter 11/2000 - Loss: 2.361
Iter 12/2000 - Loss: 2.250
Iter 13/2000 - Loss: 2.106
Iter 14/2000 - Loss: 1.963
Iter 15/2000 - Loss: 1.832
Iter 16/2000 - Loss: 1.695
Iter 17/2000 - Loss: 1.525
Iter 18/2000 - Loss: 1.310
Iter 19/2000 - Loss: 1.058
Iter 20/2000 - Loss: 0.787
Iter 1981/2000 - Loss: -8.182
Iter 1982/2000 - Loss: -8.182
Iter 1983/2000 - Loss: -8.182
Iter 1984/2000 - Loss: -8.182
Iter 1985/2000 - Loss: -8.182
Iter 1986/2000 - Loss: -8.182
Iter 1987/2000 - Loss: -8.182
Iter 1988/2000 - Loss: -8.183
Iter 1989/2000 - Loss: -8.183
Iter 1990/2000 - Loss: -8.183
Iter 1991/2000 - Loss: -8.183
Iter 1992/2000 - Loss: -8.183
Iter 1993/2000 - Loss: -8.183
Iter 1994/2000 - Loss: -8.183
Iter 1995/2000 - Loss: -8.183
Iter 1996/2000 - Loss: -8.183
Iter 1997/2000 - Loss: -8.183
Iter 1998/2000 - Loss: -8.183
Iter 1999/2000 - Loss: -8.183
Iter 2000/2000 - Loss: -8.183
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0003]])
Lengthscale: tensor([[[12.0566,  5.8396, 41.9423, 11.8211, 13.2535, 49.2307]],

        [[14.7246, 30.3384, 12.0668,  1.2070,  5.6059, 27.4117]],

        [[16.8389, 33.2135, 11.2025,  1.0430,  3.2188, 17.6637]],

        [[14.2084, 26.3529, 14.2348,  3.6417,  1.3625, 42.3438]]])
Signal Variance: tensor([ 0.0587,  2.5021, 14.7126,  0.3392])
Estimated target variance: tensor([0.0091, 0.6543, 6.4541, 0.0552])
N: 190
Signal to noise ratio: tensor([13.6135, 88.2708, 86.5072, 35.6829])
Bound on condition number: tensor([  35213.2782, 1480429.6601, 1421864.2451,  241922.2709])
Policy Optimizer learning rate:
0.000981213809289516
Experience 19, Iter 0, disc loss: 0.00048695852521393176, policy loss: 8.20921236607715
Experience 19, Iter 1, disc loss: 0.0004949634645396856, policy loss: 8.179626843062728
Experience 19, Iter 2, disc loss: 0.00046895434848277144, policy loss: 8.382516391306215
Experience 19, Iter 3, disc loss: 0.00046825023284380297, policy loss: 8.299135505012952
Experience 19, Iter 4, disc loss: 0.0004923414984032064, policy loss: 8.134099223396893
Experience 19, Iter 5, disc loss: 0.0004966548288016946, policy loss: 8.12581207138675
Experience 19, Iter 6, disc loss: 0.00046463375569681176, policy loss: 8.287712056852442
Experience 19, Iter 7, disc loss: 0.0004887884941918007, policy loss: 8.219359422906956
Experience 19, Iter 8, disc loss: 0.00048353212710184224, policy loss: 8.205377947376256
Experience 19, Iter 9, disc loss: 0.0004511609868631819, policy loss: 8.362718231251606
Experience 19, Iter 10, disc loss: 0.0004749034368917378, policy loss: 8.401522334700445
Experience 19, Iter 11, disc loss: 0.00046164359232089337, policy loss: 8.2072075376921
Experience 19, Iter 12, disc loss: 0.0004515171551138315, policy loss: 8.300198164518358
Experience 19, Iter 13, disc loss: 0.0004905674288543197, policy loss: 8.230908814160076
Experience 19, Iter 14, disc loss: 0.00047978737931128883, policy loss: 8.38532560827651
Experience 19, Iter 15, disc loss: 0.000480353768908071, policy loss: 8.335693896214847
Experience 19, Iter 16, disc loss: 0.00046898025559323324, policy loss: 8.300911541071411
Experience 19, Iter 17, disc loss: 0.0005193082404287132, policy loss: 8.106603691791943
Experience 19, Iter 18, disc loss: 0.0004978921923224102, policy loss: 8.227173986874805
Experience 19, Iter 19, disc loss: 0.0004682222628819994, policy loss: 8.23601997785394
Experience 19, Iter 20, disc loss: 0.0004910815292307024, policy loss: 8.188329681257265
Experience 19, Iter 21, disc loss: 0.0004659697599020471, policy loss: 8.373392236154409
Experience 19, Iter 22, disc loss: 0.0004978426326225315, policy loss: 8.123566918055287
Experience 19, Iter 23, disc loss: 0.00047230814816755847, policy loss: 8.272001997491943
Experience 19, Iter 24, disc loss: 0.00047334757822088447, policy loss: 8.344708995332041
Experience 19, Iter 25, disc loss: 0.0004683467023481158, policy loss: 8.339265750579985
Experience 19, Iter 26, disc loss: 0.0004953599288222665, policy loss: 8.101362607700853
Experience 19, Iter 27, disc loss: 0.0004898242206246925, policy loss: 8.160913506050395
Experience 19, Iter 28, disc loss: 0.000478538161269582, policy loss: 8.299763894209084
Experience 19, Iter 29, disc loss: 0.00044421651509925117, policy loss: 8.623274206595038
Experience 19, Iter 30, disc loss: 0.0004813374059857953, policy loss: 8.2609372549757
Experience 19, Iter 31, disc loss: 0.0004353600487367552, policy loss: 8.486764237846298
Experience 19, Iter 32, disc loss: 0.00039656297901519957, policy loss: 8.672564330400826
Experience 19, Iter 33, disc loss: 0.00040246274756817004, policy loss: 8.434256367261831
Experience 19, Iter 34, disc loss: 0.0004314516044438787, policy loss: 8.363569370113865
Experience 19, Iter 35, disc loss: 0.00047038437404825246, policy loss: 8.242261116161663
Experience 19, Iter 36, disc loss: 0.0004361393608097689, policy loss: 8.357399070911313
Experience 19, Iter 37, disc loss: 0.00042381345955659974, policy loss: 8.527187868550403
Experience 19, Iter 38, disc loss: 0.0004322144363577068, policy loss: 8.317126457074913
Experience 19, Iter 39, disc loss: 0.00041385368666114753, policy loss: 8.450086967618123
Experience 19, Iter 40, disc loss: 0.0003598454822240387, policy loss: 8.66733999186311
Experience 19, Iter 41, disc loss: 0.0003799277473889385, policy loss: 8.559657507141996
Experience 19, Iter 42, disc loss: 0.00038464415731135525, policy loss: 8.611326589078585
Experience 19, Iter 43, disc loss: 0.00036002511661536806, policy loss: 9.000967883017935
Experience 19, Iter 44, disc loss: 0.0003854527153106072, policy loss: 8.628095444167057
Experience 19, Iter 45, disc loss: 0.00039420177121659466, policy loss: 8.50341121172211
Experience 19, Iter 46, disc loss: 0.00039257575444031246, policy loss: 8.782645026411034
Experience 19, Iter 47, disc loss: 0.0004266904096614345, policy loss: 8.329534344526426
Experience 19, Iter 48, disc loss: 0.00044420891478427836, policy loss: 8.476096261436627
Experience 19, Iter 49, disc loss: 0.0004259210706766747, policy loss: 8.494684783829062
Experience 19, Iter 50, disc loss: 0.00044404366893504747, policy loss: 8.408723019716852
Experience 19, Iter 51, disc loss: 0.0004576033802489606, policy loss: 8.315884593000302
Experience 19, Iter 52, disc loss: 0.00041347141470995984, policy loss: 8.497477523236853
Experience 19, Iter 53, disc loss: 0.00042773118607559226, policy loss: 8.427381628771606
Experience 19, Iter 54, disc loss: 0.0004147627293291852, policy loss: 8.531007094266144
Experience 19, Iter 55, disc loss: 0.0004015479061818176, policy loss: 8.47022355828766
Experience 19, Iter 56, disc loss: 0.0004779818001306511, policy loss: 8.173265762201938
Experience 19, Iter 57, disc loss: 0.0004579766651776841, policy loss: 8.473650267768324
Experience 19, Iter 58, disc loss: 0.000435143574120846, policy loss: 8.550740219754442
Experience 19, Iter 59, disc loss: 0.0004573916762377686, policy loss: 8.226340272396579
Experience 19, Iter 60, disc loss: 0.00042859791171843243, policy loss: 8.552979381495366
Experience 19, Iter 61, disc loss: 0.00044836941372426167, policy loss: 8.42446726417073
Experience 19, Iter 62, disc loss: 0.00046934050338679476, policy loss: 8.187870716817107
Experience 19, Iter 63, disc loss: 0.0004478625056133452, policy loss: 8.249662320986696
Experience 19, Iter 64, disc loss: 0.00046695853774885907, policy loss: 8.389906078652126
Experience 19, Iter 65, disc loss: 0.0004060276439675556, policy loss: 8.749083995885668
Experience 19, Iter 66, disc loss: 0.00044230195693722594, policy loss: 8.272054046436875
Experience 19, Iter 67, disc loss: 0.0004438212756835548, policy loss: 8.26963271415079
Experience 19, Iter 68, disc loss: 0.00045739086186173417, policy loss: 8.273852484799743
Experience 19, Iter 69, disc loss: 0.0004391797690909883, policy loss: 8.35821697676123
Experience 19, Iter 70, disc loss: 0.0004598838051417381, policy loss: 8.42196612430969
Experience 19, Iter 71, disc loss: 0.0004783090147422157, policy loss: 8.210470167673162
Experience 19, Iter 72, disc loss: 0.0004457250574455464, policy loss: 8.350866168131631
Experience 19, Iter 73, disc loss: 0.00045989867407363885, policy loss: 8.22994012228574
Experience 19, Iter 74, disc loss: 0.000423193777834666, policy loss: 8.44974023337591
Experience 19, Iter 75, disc loss: 0.0004438842077214654, policy loss: 8.415888796361372
Experience 19, Iter 76, disc loss: 0.0004468551709016573, policy loss: 8.295998702123397
Experience 19, Iter 77, disc loss: 0.00043311151894919776, policy loss: 8.559031127909641
Experience 19, Iter 78, disc loss: 0.00041079403713369073, policy loss: 8.447826195951684
Experience 19, Iter 79, disc loss: 0.0004578605663391344, policy loss: 8.437577560992446
Experience 19, Iter 80, disc loss: 0.00043244145764804114, policy loss: 8.380748746021123
Experience 19, Iter 81, disc loss: 0.0004024621410588869, policy loss: 8.631606298439308
Experience 19, Iter 82, disc loss: 0.0004099471036118876, policy loss: 8.538926322922812
Experience 19, Iter 83, disc loss: 0.00043069286074382397, policy loss: 8.554704565870244
Experience 19, Iter 84, disc loss: 0.0004464093713435836, policy loss: 8.283570014559277
Experience 19, Iter 85, disc loss: 0.00043532549685280377, policy loss: 8.57977586821081
Experience 19, Iter 86, disc loss: 0.0004061428019606021, policy loss: 8.610802989343705
Experience 19, Iter 87, disc loss: 0.0004572966650505444, policy loss: 8.313270624963863
Experience 19, Iter 88, disc loss: 0.00040402902096312607, policy loss: 8.491579679808897
Experience 19, Iter 89, disc loss: 0.0003988733788069746, policy loss: 8.576417064817083
Experience 19, Iter 90, disc loss: 0.0004361367810117129, policy loss: 8.440227302403283
Experience 19, Iter 91, disc loss: 0.0004294947037402963, policy loss: 8.372277568582756
Experience 19, Iter 92, disc loss: 0.00046564768498416295, policy loss: 8.189624043625106
Experience 19, Iter 93, disc loss: 0.0004418070839388202, policy loss: 8.517116219552015
Experience 19, Iter 94, disc loss: 0.00042437165155776824, policy loss: 8.324777714606611
Experience 19, Iter 95, disc loss: 0.00045006983531493266, policy loss: 8.243702695513546
Experience 19, Iter 96, disc loss: 0.00046304657499995146, policy loss: 8.24390486154119
Experience 19, Iter 97, disc loss: 0.00044138522668039416, policy loss: 8.38484703572825
Experience 19, Iter 98, disc loss: 0.00042067429266175193, policy loss: 8.385644065239578
Experience 19, Iter 99, disc loss: 0.0004414443108243668, policy loss: 8.22319439772742
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.1632],
        [1.6058],
        [0.0137]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.5297e-03, 9.8848e-02, 7.2901e-01, 1.7418e-02, 3.3055e-03,
          3.7115e+00]],

        [[9.5297e-03, 9.8848e-02, 7.2901e-01, 1.7418e-02, 3.3055e-03,
          3.7115e+00]],

        [[9.5297e-03, 9.8848e-02, 7.2901e-01, 1.7418e-02, 3.3055e-03,
          3.7115e+00]],

        [[9.5297e-03, 9.8848e-02, 7.2901e-01, 1.7418e-02, 3.3055e-03,
          3.7115e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0089, 0.6527, 6.4233, 0.0549], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0089, 0.6527, 6.4233, 0.0549])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.650
Iter 2/2000 - Loss: 2.975
Iter 3/2000 - Loss: 2.596
Iter 4/2000 - Loss: 2.630
Iter 5/2000 - Loss: 2.733
Iter 6/2000 - Loss: 2.666
Iter 7/2000 - Loss: 2.533
Iter 8/2000 - Loss: 2.436
Iter 9/2000 - Loss: 2.404
Iter 10/2000 - Loss: 2.393
Iter 11/2000 - Loss: 2.341
Iter 12/2000 - Loss: 2.230
Iter 13/2000 - Loss: 2.083
Iter 14/2000 - Loss: 1.934
Iter 15/2000 - Loss: 1.796
Iter 16/2000 - Loss: 1.655
Iter 17/2000 - Loss: 1.484
Iter 18/2000 - Loss: 1.268
Iter 19/2000 - Loss: 1.012
Iter 20/2000 - Loss: 0.735
Iter 1981/2000 - Loss: -8.269
Iter 1982/2000 - Loss: -8.269
Iter 1983/2000 - Loss: -8.270
Iter 1984/2000 - Loss: -8.270
Iter 1985/2000 - Loss: -8.270
Iter 1986/2000 - Loss: -8.270
Iter 1987/2000 - Loss: -8.270
Iter 1988/2000 - Loss: -8.270
Iter 1989/2000 - Loss: -8.270
Iter 1990/2000 - Loss: -8.270
Iter 1991/2000 - Loss: -8.270
Iter 1992/2000 - Loss: -8.270
Iter 1993/2000 - Loss: -8.270
Iter 1994/2000 - Loss: -8.270
Iter 1995/2000 - Loss: -8.270
Iter 1996/2000 - Loss: -8.270
Iter 1997/2000 - Loss: -8.270
Iter 1998/2000 - Loss: -8.270
Iter 1999/2000 - Loss: -8.270
Iter 2000/2000 - Loss: -8.270
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[11.8471,  5.8671, 41.8248, 11.5292, 13.0455, 49.7708]],

        [[15.2099, 29.7478, 11.8721,  1.2157,  5.5589, 25.9477]],

        [[17.0165, 32.8438, 11.1852,  1.0284,  3.0558, 17.7854]],

        [[14.0417, 25.7666, 13.9237,  3.4542,  1.3649, 41.4933]]])
Signal Variance: tensor([ 0.0586,  2.3663, 14.2501,  0.3210])
Estimated target variance: tensor([0.0089, 0.6527, 6.4233, 0.0549])
N: 200
Signal to noise ratio: tensor([13.7681, 84.5766, 86.3209, 35.3871])
Bound on condition number: tensor([  37913.1255, 1430640.2457, 1490260.5225,  250450.4493])
Policy Optimizer learning rate:
0.000980180541783913
Experience 20, Iter 0, disc loss: 0.0004180840470984791, policy loss: 8.60796688043689
Experience 20, Iter 1, disc loss: 0.000429158224010046, policy loss: 8.357536845780213
Experience 20, Iter 2, disc loss: 0.0004545636441224595, policy loss: 8.45295636514692
Experience 20, Iter 3, disc loss: 0.0004472217999782707, policy loss: 8.637428217572609
Experience 20, Iter 4, disc loss: 0.00045665073381799916, policy loss: 8.203414265516482
Experience 20, Iter 5, disc loss: 0.0004474464574043965, policy loss: 8.226815588067296
Experience 20, Iter 6, disc loss: 0.0004299122271225849, policy loss: 8.336383454184539
Experience 20, Iter 7, disc loss: 0.000452943145259867, policy loss: 8.284162004480937
Experience 20, Iter 8, disc loss: 0.0004113823874426874, policy loss: 8.613885749539875
Experience 20, Iter 9, disc loss: 0.0004492635318218954, policy loss: 8.203913470865697
Experience 20, Iter 10, disc loss: 0.0004340039354358419, policy loss: 8.435108821378808
Experience 20, Iter 11, disc loss: 0.00043638645945588, policy loss: 8.281436492269423
Experience 20, Iter 12, disc loss: 0.0004234754072906425, policy loss: 8.537136464801389
Experience 20, Iter 13, disc loss: 0.00043550514424204646, policy loss: 8.300009028073116
Experience 20, Iter 14, disc loss: 0.0004503620236544856, policy loss: 8.268339841980328
Experience 20, Iter 15, disc loss: 0.00042952315996401004, policy loss: 8.357054639419555
Experience 20, Iter 16, disc loss: 0.00046773837952377985, policy loss: 8.207697977689238
Experience 20, Iter 17, disc loss: 0.00045936850955501514, policy loss: 8.309514720952391
Experience 20, Iter 18, disc loss: 0.00045964968679535845, policy loss: 8.290771495050715
Experience 20, Iter 19, disc loss: 0.0004321461225334599, policy loss: 8.427367638138662
Experience 20, Iter 20, disc loss: 0.00040536490468919364, policy loss: 8.54917205613246
Experience 20, Iter 21, disc loss: 0.00043800497629449065, policy loss: 8.237713992134857
Experience 20, Iter 22, disc loss: 0.00044529108698020435, policy loss: 8.298292619139822
Experience 20, Iter 23, disc loss: 0.00043325625973042627, policy loss: 8.349141155678243
Experience 20, Iter 24, disc loss: 0.00043703461258236997, policy loss: 8.5047232778001
Experience 20, Iter 25, disc loss: 0.0004158432948145511, policy loss: 8.418527371426231
Experience 20, Iter 26, disc loss: 0.0004251099585903781, policy loss: 8.337146622492329
Experience 20, Iter 27, disc loss: 0.00044074137609772233, policy loss: 8.229325019494759
Experience 20, Iter 28, disc loss: 0.0004254639217141939, policy loss: 8.30487096632222
Experience 20, Iter 29, disc loss: 0.00044356726563026937, policy loss: 8.240324182639396
Experience 20, Iter 30, disc loss: 0.0004200589290616346, policy loss: 8.401490112532953
Experience 20, Iter 31, disc loss: 0.000453673175104891, policy loss: 8.498701440632288
Experience 20, Iter 32, disc loss: 0.00042001462380872144, policy loss: 8.356345304485608
Experience 20, Iter 33, disc loss: 0.00042526389521473367, policy loss: 8.730406979126169
Experience 20, Iter 34, disc loss: 0.00044984062036085016, policy loss: 8.234909858186303
Experience 20, Iter 35, disc loss: 0.00041979183803523136, policy loss: 8.32401954824338
Experience 20, Iter 36, disc loss: 0.0004206274944742248, policy loss: 8.328812308013067
Experience 20, Iter 37, disc loss: 0.0004435285249429358, policy loss: 8.2135466423779
Experience 20, Iter 38, disc loss: 0.0004305313661346744, policy loss: 8.346415474269143
Experience 20, Iter 39, disc loss: 0.00041417391705198767, policy loss: 8.457896282852051
Experience 20, Iter 40, disc loss: 0.00041501379325731833, policy loss: 8.328949623512877
Experience 20, Iter 41, disc loss: 0.00043563929293386904, policy loss: 8.351324004890358
Experience 20, Iter 42, disc loss: 0.0004073229351170836, policy loss: 8.562351949925262
Experience 20, Iter 43, disc loss: 0.00040873056530929216, policy loss: 8.37976587371216
Experience 20, Iter 44, disc loss: 0.00041317920884852545, policy loss: 8.481634162597077
Experience 20, Iter 45, disc loss: 0.0004272345091780708, policy loss: 8.466639598805681
Experience 20, Iter 46, disc loss: 0.0004141143240625028, policy loss: 8.463412542155229
Experience 20, Iter 47, disc loss: 0.0004300065070799485, policy loss: 8.41783772956865
Experience 20, Iter 48, disc loss: 0.00040697530236371074, policy loss: 8.78130020693549
Experience 20, Iter 49, disc loss: 0.00043060872212220293, policy loss: 8.291647151380616
Experience 20, Iter 50, disc loss: 0.0004266136692510405, policy loss: 8.337322073396905
Experience 20, Iter 51, disc loss: 0.0004260589540025048, policy loss: 8.322920636423873
Experience 20, Iter 52, disc loss: 0.00040086686570726666, policy loss: 8.478746610899291
Experience 20, Iter 53, disc loss: 0.0004191981665339154, policy loss: 8.502800871488077
Experience 20, Iter 54, disc loss: 0.0004118937155953326, policy loss: 8.336800350763465
Experience 20, Iter 55, disc loss: 0.00043948381101666746, policy loss: 8.401462724538618
Experience 20, Iter 56, disc loss: 0.0003859439259810742, policy loss: 8.544363244938353
Experience 20, Iter 57, disc loss: 0.0004211463725597335, policy loss: 8.506618219751228
Experience 20, Iter 58, disc loss: 0.00042314002875352486, policy loss: 8.501926190146436
Experience 20, Iter 59, disc loss: 0.00039189540828378814, policy loss: 8.504706348908405
Experience 20, Iter 60, disc loss: 0.00039467430669140255, policy loss: 8.42726262796814
Experience 20, Iter 61, disc loss: 0.00042505071698670254, policy loss: 8.404634517126652
Experience 20, Iter 62, disc loss: 0.0003994717141365965, policy loss: 8.46523848740259
Experience 20, Iter 63, disc loss: 0.00045466366352852895, policy loss: 8.348870507092785
Experience 20, Iter 64, disc loss: 0.0004316963164157485, policy loss: 8.340719676894173
Experience 20, Iter 65, disc loss: 0.000419383329058859, policy loss: 8.294244037652138
Experience 20, Iter 66, disc loss: 0.000417326048779169, policy loss: 8.333759140324652
Experience 20, Iter 67, disc loss: 0.00038251821238550815, policy loss: 8.682046880290194
Experience 20, Iter 68, disc loss: 0.00040552044143056325, policy loss: 8.54585165350839
Experience 20, Iter 69, disc loss: 0.0004037014889174121, policy loss: 8.430940416478773
Experience 20, Iter 70, disc loss: 0.0003819365238021666, policy loss: 8.530831965645884
Experience 20, Iter 71, disc loss: 0.0004130875470164687, policy loss: 8.332268190486548
Experience 20, Iter 72, disc loss: 0.00041159070116593584, policy loss: 8.347746335369562
Experience 20, Iter 73, disc loss: 0.00042035951519692937, policy loss: 8.31589277144559
Experience 20, Iter 74, disc loss: 0.0004014215879874301, policy loss: 8.42739219565884
Experience 20, Iter 75, disc loss: 0.00040683949732966203, policy loss: 8.462754422446292
Experience 20, Iter 76, disc loss: 0.0004282309262898787, policy loss: 8.315422206867884
Experience 20, Iter 77, disc loss: 0.0003912946160295874, policy loss: 8.494849838188777
Experience 20, Iter 78, disc loss: 0.0003872324536639927, policy loss: 8.499559732629471
Experience 20, Iter 79, disc loss: 0.00040948268734458475, policy loss: 8.517856787740438
Experience 20, Iter 80, disc loss: 0.0004008736763011451, policy loss: 8.44665773540981
Experience 20, Iter 81, disc loss: 0.0004042261750320511, policy loss: 8.333983446476921
Experience 20, Iter 82, disc loss: 0.0003909356926195027, policy loss: 8.445787547151571
Experience 20, Iter 83, disc loss: 0.0003955318834124135, policy loss: 8.498182011500468
Experience 20, Iter 84, disc loss: 0.00037803677014407194, policy loss: 8.633290968032444
Experience 20, Iter 85, disc loss: 0.0003921499882004233, policy loss: 8.441308412942703
Experience 20, Iter 86, disc loss: 0.0004131392972819259, policy loss: 8.272676349003717
Experience 20, Iter 87, disc loss: 0.00036323680149389845, policy loss: 8.812747976800106
Experience 20, Iter 88, disc loss: 0.000415620524645801, policy loss: 8.353665309816684
Experience 20, Iter 89, disc loss: 0.000387825941054908, policy loss: 8.706105194785973
Experience 20, Iter 90, disc loss: 0.00041434815463052414, policy loss: 8.298234688658706
Experience 20, Iter 91, disc loss: 0.0003953465889525498, policy loss: 8.555756537068808
Experience 20, Iter 92, disc loss: 0.0004181398832306153, policy loss: 8.301683120123334
Experience 20, Iter 93, disc loss: 0.0004003683636379726, policy loss: 8.455736055056143
Experience 20, Iter 94, disc loss: 0.00038899543532197687, policy loss: 8.497080346345435
Experience 20, Iter 95, disc loss: 0.0003785443654592001, policy loss: 8.558786808906927
Experience 20, Iter 96, disc loss: 0.00039463152594718164, policy loss: 8.470105854488446
Experience 20, Iter 97, disc loss: 0.00042243169413454533, policy loss: 8.310732134311234
Experience 20, Iter 98, disc loss: 0.00040682280071280643, policy loss: 8.328105357063999
Experience 20, Iter 99, disc loss: 0.0003958619019787853, policy loss: 8.402260144384325
