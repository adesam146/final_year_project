Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0130],
        [0.8980],
        [0.0210]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0518, 3.5922, 0.0841], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 1.196
Iter 3/2000 - Loss: 0.953
Iter 4/2000 - Loss: 1.383
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.493
Iter 7/2000 - Loss: 1.248
Iter 8/2000 - Loss: 1.053
Iter 9/2000 - Loss: 0.964
Iter 10/2000 - Loss: 0.972
Iter 11/2000 - Loss: 1.033
Iter 12/2000 - Loss: 1.091
Iter 13/2000 - Loss: 1.105
Iter 14/2000 - Loss: 1.071
Iter 15/2000 - Loss: 1.012
Iter 16/2000 - Loss: 0.951
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.883
Iter 19/2000 - Loss: 0.889
Iter 20/2000 - Loss: 0.912
Iter 1981/2000 - Loss: 0.698
Iter 1982/2000 - Loss: 0.698
Iter 1983/2000 - Loss: 0.698
Iter 1984/2000 - Loss: 0.698
Iter 1985/2000 - Loss: 0.698
Iter 1986/2000 - Loss: 0.698
Iter 1987/2000 - Loss: 0.698
Iter 1988/2000 - Loss: 0.698
Iter 1989/2000 - Loss: 0.698
Iter 1990/2000 - Loss: 0.698
Iter 1991/2000 - Loss: 0.698
Iter 1992/2000 - Loss: 0.698
Iter 1993/2000 - Loss: 0.698
Iter 1994/2000 - Loss: 0.698
Iter 1995/2000 - Loss: 0.698
Iter 1996/2000 - Loss: 0.698
Iter 1997/2000 - Loss: 0.698
Iter 1998/2000 - Loss: 0.698
Iter 1999/2000 - Loss: 0.698
Iter 2000/2000 - Loss: 0.698
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0093],
        [0.4134],
        [0.0149]])
Lengthscale: tensor([[[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]],

        [[0.0217, 0.0458, 0.9688, 0.0209, 0.0021, 0.1202]]])
Signal Variance: tensor([0.0033, 0.0374, 2.8196, 0.0608])
Estimated target variance: tensor([0.0046, 0.0518, 3.5922, 0.0841])
N: 10
Signal to noise ratio: tensor([1.9681, 2.0097, 2.6117, 2.0186])
Bound on condition number: tensor([39.7326, 41.3892, 69.2095, 41.7472])
Policy Optimizer learning rate:
0.0001
Experience 1, Iter 0, disc loss: 1.2474456489459007, policy loss: 0.7896666701180209
Experience 1, Iter 1, disc loss: 1.2385532125649845, policy loss: 0.7873821741503558
Experience 1, Iter 2, disc loss: 1.221290619333432, policy loss: 0.7971532739084635
Experience 1, Iter 3, disc loss: 1.2120407527921415, policy loss: 0.796528899715701
Experience 1, Iter 4, disc loss: 1.19794361561656, policy loss: 0.8018655867537564
Experience 1, Iter 5, disc loss: 1.1900400752458522, policy loss: 0.8005680086396202
Experience 1, Iter 6, disc loss: 1.1812296289261666, policy loss: 0.7994107856472283
Experience 1, Iter 7, disc loss: 1.1722217506131944, policy loss: 0.801106110309323
Experience 1, Iter 8, disc loss: 1.153116236015522, policy loss: 0.8132473043258102
Experience 1, Iter 9, disc loss: 1.1224181802406852, policy loss: 0.8429433644396453
Experience 1, Iter 10, disc loss: 1.137937109394109, policy loss: 0.8168134930732802
Experience 1, Iter 11, disc loss: 1.1172321298219194, policy loss: 0.830784868383674
Experience 1, Iter 12, disc loss: 1.099056527771582, policy loss: 0.8477232833843891
Experience 1, Iter 13, disc loss: 1.1019562547423725, policy loss: 0.8297714806613308
Experience 1, Iter 14, disc loss: 1.0776981400292054, policy loss: 0.8528092466831967
Experience 1, Iter 15, disc loss: 1.0670486306746154, policy loss: 0.8581159883799094
Experience 1, Iter 16, disc loss: 1.0540704302990007, policy loss: 0.862009636772347
Experience 1, Iter 17, disc loss: 1.0580977793504378, policy loss: 0.8483899489013845
Experience 1, Iter 18, disc loss: 1.019200929908263, policy loss: 0.8854721377123478
Experience 1, Iter 19, disc loss: 1.028336763134054, policy loss: 0.8658189107616476
Experience 1, Iter 20, disc loss: 1.0055546913715054, policy loss: 0.8822730119365393
Experience 1, Iter 21, disc loss: 0.9911847954581123, policy loss: 0.8926408850108114
Experience 1, Iter 22, disc loss: 0.9756610219531502, policy loss: 0.9003903865125997
Experience 1, Iter 23, disc loss: 0.9525216550237462, policy loss: 0.9241074915275997
Experience 1, Iter 24, disc loss: 0.9698794264233499, policy loss: 0.8907730738717938
Experience 1, Iter 25, disc loss: 0.9357453844842655, policy loss: 0.9279118489095812
Experience 1, Iter 26, disc loss: 0.9435578049992643, policy loss: 0.9100138658925003
Experience 1, Iter 27, disc loss: 0.8940860666632963, policy loss: 0.9704439828797448
Experience 1, Iter 28, disc loss: 0.8965324883907179, policy loss: 0.9472868508478762
Experience 1, Iter 29, disc loss: 0.8748818646642196, policy loss: 0.9638878740692044
Experience 1, Iter 30, disc loss: 0.8543063563658355, policy loss: 0.9879710978393057
Experience 1, Iter 31, disc loss: 0.8306455129108752, policy loss: 1.011941940007712
Experience 1, Iter 32, disc loss: 0.830390924795473, policy loss: 1.0039128643423596
Experience 1, Iter 33, disc loss: 0.8114405498076834, policy loss: 1.0323835726404236
Experience 1, Iter 34, disc loss: 0.7926320877892159, policy loss: 1.0436492302453708
Experience 1, Iter 35, disc loss: 0.7656056542665037, policy loss: 1.0864139464334026
Experience 1, Iter 36, disc loss: 0.7450456924813975, policy loss: 1.0991918101742713
Experience 1, Iter 37, disc loss: 0.7451842748498192, policy loss: 1.0894849297420528
Experience 1, Iter 38, disc loss: 0.7391925927061522, policy loss: 1.0864570890617293
Experience 1, Iter 39, disc loss: 0.7031802884215157, policy loss: 1.1407901887231848
Experience 1, Iter 40, disc loss: 0.7074767735350171, policy loss: 1.1217342913617065
Experience 1, Iter 41, disc loss: 0.6955274840240468, policy loss: 1.1255673710226355
Experience 1, Iter 42, disc loss: 0.6716638680981264, policy loss: 1.1547867542828607
Experience 1, Iter 43, disc loss: 0.6463518541391337, policy loss: 1.1986589073458642
Experience 1, Iter 44, disc loss: 0.6482815384879281, policy loss: 1.1799507853684845
Experience 1, Iter 45, disc loss: 0.643679269088878, policy loss: 1.1687911743414028
Experience 1, Iter 46, disc loss: 0.6184601954933155, policy loss: 1.2134617935159924
Experience 1, Iter 47, disc loss: 0.634852809450805, policy loss: 1.1772424175794889
Experience 1, Iter 48, disc loss: 0.5892083413587061, policy loss: 1.2654758112069164
Experience 1, Iter 49, disc loss: 0.5718341794210051, policy loss: 1.290416795171531
Experience 1, Iter 50, disc loss: 0.5457006771201396, policy loss: 1.3308308497162864
Experience 1, Iter 51, disc loss: 0.5470467875861504, policy loss: 1.310355654931473
Experience 1, Iter 52, disc loss: 0.5410505763309326, policy loss: 1.351787502789557
Experience 1, Iter 53, disc loss: 0.5374809714948843, policy loss: 1.3231506859496458
Experience 1, Iter 54, disc loss: 0.49421512502471776, policy loss: 1.4196074305633746
Experience 1, Iter 55, disc loss: 0.4714000245751798, policy loss: 1.452921699839538
Experience 1, Iter 56, disc loss: 0.46831202407753697, policy loss: 1.4444235990004688
Experience 1, Iter 57, disc loss: 0.4812985954439729, policy loss: 1.4163693465480465
Experience 1, Iter 58, disc loss: 0.4414017762583568, policy loss: 1.5045908870272533
Experience 1, Iter 59, disc loss: 0.4151337926276093, policy loss: 1.5713169012122832
Experience 1, Iter 60, disc loss: 0.440825617911773, policy loss: 1.489563243885228
Experience 1, Iter 61, disc loss: 0.44578571259676103, policy loss: 1.5013550062035494
Experience 1, Iter 62, disc loss: 0.40570182568732605, policy loss: 1.5694830179566188
Experience 1, Iter 63, disc loss: 0.3896564862377173, policy loss: 1.6342872052631807
Experience 1, Iter 64, disc loss: 0.4080966969654457, policy loss: 1.5835413503023528
Experience 1, Iter 65, disc loss: 0.40198671680656917, policy loss: 1.5954593460587212
Experience 1, Iter 66, disc loss: 0.3713538134185343, policy loss: 1.6582737708976185
Experience 1, Iter 67, disc loss: 0.38273412905753146, policy loss: 1.5963659573581466
Experience 1, Iter 68, disc loss: 0.3485302909014796, policy loss: 1.723501845719099
Experience 1, Iter 69, disc loss: 0.3415044542880134, policy loss: 1.7293710255039407
Experience 1, Iter 70, disc loss: 0.3264932606481297, policy loss: 1.7892747570452303
Experience 1, Iter 71, disc loss: 0.31073368418016, policy loss: 1.848347748089096
Experience 1, Iter 72, disc loss: 0.3079178841562935, policy loss: 1.8702976927311206
Experience 1, Iter 73, disc loss: 0.32099815906210527, policy loss: 1.848129549961107
Experience 1, Iter 74, disc loss: 0.29209759911894806, policy loss: 1.9158605027598679
Experience 1, Iter 75, disc loss: 0.2970412032446237, policy loss: 1.89469608141306
Experience 1, Iter 76, disc loss: 0.263433104876326, policy loss: 1.9922630214191943
Experience 1, Iter 77, disc loss: 0.2753407861905729, policy loss: 1.963583684920431
Experience 1, Iter 78, disc loss: 0.2730630930930204, policy loss: 1.9727259921587703
Experience 1, Iter 79, disc loss: 0.2222116160117163, policy loss: 2.1978079900686396
Experience 1, Iter 80, disc loss: 0.2645387203579874, policy loss: 1.999950821203061
Experience 1, Iter 81, disc loss: 0.23908349881790653, policy loss: 2.0900304116452206
Experience 1, Iter 82, disc loss: 0.22899368529977607, policy loss: 2.1298482758985027
Experience 1, Iter 83, disc loss: 0.24046553127821385, policy loss: 1.9982250462176006
Experience 1, Iter 84, disc loss: 0.19050565799624586, policy loss: 2.2997141465121027
Experience 1, Iter 85, disc loss: 0.21097256293625707, policy loss: 2.2366998855299407
Experience 1, Iter 86, disc loss: 0.2106436465569468, policy loss: 2.171216943465118
Experience 1, Iter 87, disc loss: 0.21287608449377254, policy loss: 2.1305679156444035
Experience 1, Iter 88, disc loss: 0.1892898119669365, policy loss: 2.3703740747561395
Experience 1, Iter 89, disc loss: 0.19847481063041347, policy loss: 2.2778288120483636
Experience 1, Iter 90, disc loss: 0.1700484685161203, policy loss: 2.3675504241756045
Experience 1, Iter 91, disc loss: 0.19264018886884604, policy loss: 2.4405365558055845
Experience 1, Iter 92, disc loss: 0.16941774892705752, policy loss: 2.4959882510259392
Experience 1, Iter 93, disc loss: 0.16844750022061672, policy loss: 2.5408164160953834
Experience 1, Iter 94, disc loss: 0.17855725642066503, policy loss: 2.4050947655045403
Experience 1, Iter 95, disc loss: 0.16760987074609757, policy loss: 2.4889613771013854
Experience 1, Iter 96, disc loss: 0.17406391812849736, policy loss: 2.462860951337506
Experience 1, Iter 97, disc loss: 0.1671362106085686, policy loss: 2.4994018307075763
Experience 1, Iter 98, disc loss: 0.13417745015955512, policy loss: 2.8242154077590924
Experience 1, Iter 99, disc loss: 0.15285392276296944, policy loss: 2.4775425109609985
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0117],
        [0.7878],
        [0.0178]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0222, 0.0470, 0.8280, 0.0177, 0.0018, 0.0896]],

        [[0.0222, 0.0470, 0.8280, 0.0177, 0.0018, 0.0896]],

        [[0.0222, 0.0470, 0.8280, 0.0177, 0.0018, 0.0896]],

        [[0.0222, 0.0470, 0.8280, 0.0177, 0.0018, 0.0896]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0470, 3.1510, 0.0713], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0470, 3.1510, 0.0713])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.637
Iter 2/2000 - Loss: 1.165
Iter 3/2000 - Loss: 0.766
Iter 4/2000 - Loss: 1.251
Iter 5/2000 - Loss: 1.579
Iter 6/2000 - Loss: 1.500
Iter 7/2000 - Loss: 1.232
Iter 8/2000 - Loss: 0.986
Iter 9/2000 - Loss: 0.848
Iter 10/2000 - Loss: 0.825
Iter 11/2000 - Loss: 0.883
Iter 12/2000 - Loss: 0.960
Iter 13/2000 - Loss: 1.002
Iter 14/2000 - Loss: 0.989
Iter 15/2000 - Loss: 0.936
Iter 16/2000 - Loss: 0.869
Iter 17/2000 - Loss: 0.808
Iter 18/2000 - Loss: 0.766
Iter 19/2000 - Loss: 0.750
Iter 20/2000 - Loss: 0.758
Iter 1981/2000 - Loss: -5.439
Iter 1982/2000 - Loss: -5.439
Iter 1983/2000 - Loss: -5.439
Iter 1984/2000 - Loss: -5.439
Iter 1985/2000 - Loss: -5.439
Iter 1986/2000 - Loss: -5.439
Iter 1987/2000 - Loss: -5.439
Iter 1988/2000 - Loss: -5.439
Iter 1989/2000 - Loss: -5.439
Iter 1990/2000 - Loss: -5.439
Iter 1991/2000 - Loss: -5.439
Iter 1992/2000 - Loss: -5.439
Iter 1993/2000 - Loss: -5.439
Iter 1994/2000 - Loss: -5.439
Iter 1995/2000 - Loss: -5.439
Iter 1996/2000 - Loss: -5.439
Iter 1997/2000 - Loss: -5.439
Iter 1998/2000 - Loss: -5.439
Iter 1999/2000 - Loss: -5.439
Iter 2000/2000 - Loss: -5.439
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0004],
        [0.0005],
        [0.0005]])
Lengthscale: tensor([[[17.3249,  2.2270, 78.4825,  3.9155,  4.6491, 39.3977]],

        [[13.1207, 17.7492,  7.3255,  0.5670,  0.4662,  2.9462]],

        [[20.7380,  4.0564, 10.1825,  0.6618,  6.7095,  7.4403]],

        [[30.9536, 50.3131, 16.6867,  3.9583, 11.7472, 20.0101]]])
Signal Variance: tensor([0.0178, 0.0750, 3.3890, 0.6318])
Estimated target variance: tensor([0.0041, 0.0470, 3.1510, 0.0713])
N: 20
Signal to noise ratio: tensor([ 6.5100, 14.3169, 80.3066, 34.1513])
Bound on condition number: tensor([   848.6070,   4100.4568, 128983.8403,  23327.2803])
Policy Optimizer learning rate:
9.989469496904545e-05
Experience 2, Iter 0, disc loss: 0.11395157232662989, policy loss: 2.6574680948652936
Experience 2, Iter 1, disc loss: 0.11265266735280849, policy loss: 2.656613274502918
Experience 2, Iter 2, disc loss: 0.11144638246260316, policy loss: 2.654046125413367
Experience 2, Iter 3, disc loss: 0.11295188541555529, policy loss: 2.6203137366270837
Experience 2, Iter 4, disc loss: 0.10991070068168304, policy loss: 2.645848908016938
Experience 2, Iter 5, disc loss: 0.10825889259275026, policy loss: 2.676363269663187
Experience 2, Iter 6, disc loss: 0.10086488489385026, policy loss: 2.7342142203755238
Experience 2, Iter 7, disc loss: 0.09843958568601664, policy loss: 2.7576602928892084
Experience 2, Iter 8, disc loss: 0.09916981344860523, policy loss: 2.747370142858401
Experience 2, Iter 9, disc loss: 0.09080677095584794, policy loss: 2.838523079814175
Experience 2, Iter 10, disc loss: 0.09558877330073892, policy loss: 2.7647001171678056
Experience 2, Iter 11, disc loss: 0.08620870301547189, policy loss: 2.888803209104286
Experience 2, Iter 12, disc loss: 0.08326541870816127, policy loss: 2.9125969908547
Experience 2, Iter 13, disc loss: 0.08205833196800244, policy loss: 2.9197829841105607
Experience 2, Iter 14, disc loss: 0.083049507622414, policy loss: 2.90660365786286
Experience 2, Iter 15, disc loss: 0.08498502140841485, policy loss: 2.8436473535266025
Experience 2, Iter 16, disc loss: 0.07647805539525679, policy loss: 2.9847699987334577
Experience 2, Iter 17, disc loss: 0.07649393851850898, policy loss: 2.9752604576318644
Experience 2, Iter 18, disc loss: 0.07112308083516063, policy loss: 3.032479757067863
Experience 2, Iter 19, disc loss: 0.0742437491057371, policy loss: 2.9818610557187393
Experience 2, Iter 20, disc loss: 0.07407080383836867, policy loss: 2.9877862855848942
Experience 2, Iter 21, disc loss: 0.07066073099466938, policy loss: 3.0321641509281125
Experience 2, Iter 22, disc loss: 0.06609021786963386, policy loss: 3.092527703917892
Experience 2, Iter 23, disc loss: 0.06796310956263593, policy loss: 3.0513452423761156
Experience 2, Iter 24, disc loss: 0.06856591984468269, policy loss: 3.030508389684782
Experience 2, Iter 25, disc loss: 0.06537502108133121, policy loss: 3.086963030279161
Experience 2, Iter 26, disc loss: 0.06495431724603615, policy loss: 3.1224414826977362
Experience 2, Iter 27, disc loss: 0.06063787300437588, policy loss: 3.168115417397793
Experience 2, Iter 28, disc loss: 0.05880143306817185, policy loss: 3.203170044528577
Experience 2, Iter 29, disc loss: 0.05969439919204028, policy loss: 3.1984325405521186
Experience 2, Iter 30, disc loss: 0.06212956326857126, policy loss: 3.122218979018803
Experience 2, Iter 31, disc loss: 0.05840790144746162, policy loss: 3.2011533775999603
Experience 2, Iter 32, disc loss: 0.0549089242155295, policy loss: 3.2495856836493653
Experience 2, Iter 33, disc loss: 0.05347021032213947, policy loss: 3.2689466172026123
Experience 2, Iter 34, disc loss: 0.04895918133212468, policy loss: 3.389615248736045
Experience 2, Iter 35, disc loss: 0.05102711713980751, policy loss: 3.3181364618767786
Experience 2, Iter 36, disc loss: 0.05424836726838527, policy loss: 3.259601912830039
Experience 2, Iter 37, disc loss: 0.05403817549695709, policy loss: 3.2500792223709762
Experience 2, Iter 38, disc loss: 0.050978895546680766, policy loss: 3.312938035055545
Experience 2, Iter 39, disc loss: 0.051273676471530936, policy loss: 3.299836794358769
Experience 2, Iter 40, disc loss: 0.052537334647134136, policy loss: 3.2747062076893814
Experience 2, Iter 41, disc loss: 0.048775201378758994, policy loss: 3.3436056738122133
Experience 2, Iter 42, disc loss: 0.04901542986234665, policy loss: 3.35153301058645
Experience 2, Iter 43, disc loss: 0.05011925965535631, policy loss: 3.282982765708715
Experience 2, Iter 44, disc loss: 0.05440036259984374, policy loss: 3.227059518202865
Experience 2, Iter 45, disc loss: 0.04513478093709689, policy loss: 3.4153944594783434
Experience 2, Iter 46, disc loss: 0.04333125552620237, policy loss: 3.454594197611444
Experience 2, Iter 47, disc loss: 0.045790689926888624, policy loss: 3.413462284113867
Experience 2, Iter 48, disc loss: 0.050018920751167824, policy loss: 3.296802433681295
Experience 2, Iter 49, disc loss: 0.04315530787999425, policy loss: 3.4799553875282703
Experience 2, Iter 50, disc loss: 0.043022114316088295, policy loss: 3.4954291953280245
Experience 2, Iter 51, disc loss: 0.0382844660060466, policy loss: 3.599514018428557
Experience 2, Iter 52, disc loss: 0.03664276635023038, policy loss: 3.6971383045083512
Experience 2, Iter 53, disc loss: 0.04198027414126437, policy loss: 3.5263143121115634
Experience 2, Iter 54, disc loss: 0.04336127718583534, policy loss: 3.4921500106736167
Experience 2, Iter 55, disc loss: 0.04584960388558075, policy loss: 3.4332402713737977
Experience 2, Iter 56, disc loss: 0.042455932767544315, policy loss: 3.5606052401070802
Experience 2, Iter 57, disc loss: 0.04507151039749588, policy loss: 3.682750719332785
Experience 2, Iter 58, disc loss: 0.0493693363870937, policy loss: 3.6082312321725727
Experience 2, Iter 59, disc loss: 0.04141091112645348, policy loss: 3.5532093352474923
Experience 2, Iter 60, disc loss: 0.03826214928504941, policy loss: 3.5587310771921947
Experience 2, Iter 61, disc loss: 0.03959234043599426, policy loss: 3.545489915902856
Experience 2, Iter 62, disc loss: 0.037424845495016706, policy loss: 3.6107996497276686
Experience 2, Iter 63, disc loss: 0.040611929352806095, policy loss: 3.5953950766149525
Experience 2, Iter 64, disc loss: 0.034792643104948, policy loss: 3.649738695549159
Experience 2, Iter 65, disc loss: 0.035313184631845594, policy loss: 3.70025491524888
Experience 2, Iter 66, disc loss: 0.036911007466668706, policy loss: 3.6322015044205385
Experience 2, Iter 67, disc loss: 0.03525568187360718, policy loss: 3.6931949115776064
Experience 2, Iter 68, disc loss: 0.037969318792569276, policy loss: 3.6073429949139237
Experience 2, Iter 69, disc loss: 0.039145016753309024, policy loss: 3.695329249397246
Experience 2, Iter 70, disc loss: 0.04005527449555038, policy loss: 3.680392689313688
Experience 2, Iter 71, disc loss: 0.03221907693322265, policy loss: 3.910047580823553
Experience 2, Iter 72, disc loss: 0.06604164071465228, policy loss: 3.818220749273916
Experience 2, Iter 73, disc loss: 0.03785926312661586, policy loss: 3.779685189522255
Experience 2, Iter 74, disc loss: 0.05100562645413396, policy loss: 3.7340227833446153
Experience 2, Iter 75, disc loss: 0.032364143722077306, policy loss: 3.761139876298996
Experience 2, Iter 76, disc loss: 0.053387698256357016, policy loss: 3.6807885209911317
Experience 2, Iter 77, disc loss: 0.07625933983020622, policy loss: 4.010976010206063
Experience 2, Iter 78, disc loss: 0.05030769307684428, policy loss: 3.7059398686788243
Experience 2, Iter 79, disc loss: 0.03121135671954963, policy loss: 3.852477751410365
Experience 2, Iter 80, disc loss: 0.034194898923131975, policy loss: 3.97055022163636
Experience 2, Iter 81, disc loss: 0.03337707328356086, policy loss: 3.905891027094939
Experience 2, Iter 82, disc loss: 0.04199997865137179, policy loss: 3.7339610740194167
Experience 2, Iter 83, disc loss: 0.03879893847410397, policy loss: 3.9445462096179997
Experience 2, Iter 84, disc loss: 0.03123869906289658, policy loss: 4.05896989739826
Experience 2, Iter 85, disc loss: 0.03947609825072939, policy loss: 4.0235380433431285
Experience 2, Iter 86, disc loss: 0.025795649656454176, policy loss: 4.287259446545482
Experience 2, Iter 87, disc loss: 0.04024448506115741, policy loss: 4.099530951794608
Experience 2, Iter 88, disc loss: 0.038326761095558756, policy loss: 4.058269195881039
Experience 2, Iter 89, disc loss: 0.02904383057636541, policy loss: 4.108770162052995
Experience 2, Iter 90, disc loss: 0.0311632086063014, policy loss: 4.233662661094575
Experience 2, Iter 91, disc loss: 0.030892087751728487, policy loss: 4.116132554785154
Experience 2, Iter 92, disc loss: 0.029951226965929237, policy loss: 4.117064410325199
Experience 2, Iter 93, disc loss: 0.03049245477757881, policy loss: 4.107712139202299
Experience 2, Iter 94, disc loss: 0.02766958296224617, policy loss: 4.357277507711376
Experience 2, Iter 95, disc loss: 0.04146484877554849, policy loss: 3.944444813642278
Experience 2, Iter 96, disc loss: 0.026204383039912734, policy loss: 4.360932550708689
Experience 2, Iter 97, disc loss: 0.02570558197925544, policy loss: 4.4019579112236515
Experience 2, Iter 98, disc loss: 0.05052489468091549, policy loss: 4.264753304666106
Experience 2, Iter 99, disc loss: 0.028306995827459984, policy loss: 4.345601005205115
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0091],
        [0.5450],
        [0.0123]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0239, 0.0500, 0.5697, 0.0123, 0.0015, 0.0964]],

        [[0.0239, 0.0500, 0.5697, 0.0123, 0.0015, 0.0964]],

        [[0.0239, 0.0500, 0.5697, 0.0123, 0.0015, 0.0964]],

        [[0.0239, 0.0500, 0.5697, 0.0123, 0.0015, 0.0964]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0050, 0.0365, 2.1802, 0.0491], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0050, 0.0365, 2.1802, 0.0491])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.260
Iter 2/2000 - Loss: 0.583
Iter 3/2000 - Loss: 0.463
Iter 4/2000 - Loss: 0.843
Iter 5/2000 - Loss: 0.948
Iter 6/2000 - Loss: 0.780
Iter 7/2000 - Loss: 0.580
Iter 8/2000 - Loss: 0.482
Iter 9/2000 - Loss: 0.483
Iter 10/2000 - Loss: 0.521
Iter 11/2000 - Loss: 0.546
Iter 12/2000 - Loss: 0.536
Iter 13/2000 - Loss: 0.496
Iter 14/2000 - Loss: 0.443
Iter 15/2000 - Loss: 0.396
Iter 16/2000 - Loss: 0.372
Iter 17/2000 - Loss: 0.372
Iter 18/2000 - Loss: 0.383
Iter 19/2000 - Loss: 0.382
Iter 20/2000 - Loss: 0.357
Iter 1981/2000 - Loss: -6.490
Iter 1982/2000 - Loss: -6.490
Iter 1983/2000 - Loss: -6.490
Iter 1984/2000 - Loss: -6.490
Iter 1985/2000 - Loss: -6.490
Iter 1986/2000 - Loss: -6.490
Iter 1987/2000 - Loss: -6.490
Iter 1988/2000 - Loss: -6.490
Iter 1989/2000 - Loss: -6.490
Iter 1990/2000 - Loss: -6.491
Iter 1991/2000 - Loss: -6.491
Iter 1992/2000 - Loss: -6.491
Iter 1993/2000 - Loss: -6.491
Iter 1994/2000 - Loss: -6.491
Iter 1995/2000 - Loss: -6.491
Iter 1996/2000 - Loss: -6.491
Iter 1997/2000 - Loss: -6.491
Iter 1998/2000 - Loss: -6.491
Iter 1999/2000 - Loss: -6.491
Iter 2000/2000 - Loss: -6.491
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0011],
        [0.0005]])
Lengthscale: tensor([[[21.0044,  2.6131, 57.9140,  5.3685,  6.3571, 14.7579]],

        [[20.6636, 33.7703,  9.4699,  0.5917,  0.3366,  3.3607]],

        [[24.7343, 51.4630, 15.7637,  0.7090,  5.8414,  9.7414]],

        [[34.0285, 46.4118, 16.0330,  4.0824, 12.6158, 29.0720]]])
Signal Variance: tensor([0.0269, 0.0833, 6.8747, 0.5308])
Estimated target variance: tensor([0.0050, 0.0365, 2.1802, 0.0491])
N: 30
Signal to noise ratio: tensor([ 8.9970, 16.9987, 78.2950, 34.1812])
Bound on condition number: tensor([  2429.4071,   8669.6661, 183904.0382,  35051.7079])
Policy Optimizer learning rate:
9.978950082958633e-05
Experience 3, Iter 0, disc loss: 0.04220330533989633, policy loss: 3.3725126079388605
Experience 3, Iter 1, disc loss: 0.04296005013413506, policy loss: 3.3533563524438113
Experience 3, Iter 2, disc loss: 0.046304951893773184, policy loss: 3.2726738393335912
Experience 3, Iter 3, disc loss: 0.044449010846297404, policy loss: 3.3165037612671258
Experience 3, Iter 4, disc loss: 0.04415651967539719, policy loss: 3.316520916532752
Experience 3, Iter 5, disc loss: 0.043103514768612, policy loss: 3.333136710730018
Experience 3, Iter 6, disc loss: 0.044267807051386374, policy loss: 3.312063354150037
Experience 3, Iter 7, disc loss: 0.042001058423825954, policy loss: 3.358195420546606
Experience 3, Iter 8, disc loss: 0.043268634413050316, policy loss: 3.332386025490739
Experience 3, Iter 9, disc loss: 0.042805970609159104, policy loss: 3.33241851555982
Experience 3, Iter 10, disc loss: 0.043759458060227185, policy loss: 3.300791363447315
Experience 3, Iter 11, disc loss: 0.04233037987733255, policy loss: 3.3324569489101368
Experience 3, Iter 12, disc loss: 0.04670083065365498, policy loss: 3.233684325440134
Experience 3, Iter 13, disc loss: 0.04231551243802745, policy loss: 3.329543155080799
Experience 3, Iter 14, disc loss: 0.04500918731439789, policy loss: 3.263728751427811
Experience 3, Iter 15, disc loss: 0.04537857487594175, policy loss: 3.2563389657754924
Experience 3, Iter 16, disc loss: 0.04342568802363123, policy loss: 3.2968017688450315
Experience 3, Iter 17, disc loss: 0.041349259931360544, policy loss: 3.352797739464335
Experience 3, Iter 18, disc loss: 0.04411295942342055, policy loss: 3.284718933543921
Experience 3, Iter 19, disc loss: 0.04178526000649648, policy loss: 3.3430899297718475
Experience 3, Iter 20, disc loss: 0.04493958210217313, policy loss: 3.2678324199057878
Experience 3, Iter 21, disc loss: 0.044390303950382515, policy loss: 3.2671994137431875
Experience 3, Iter 22, disc loss: 0.043057071018318474, policy loss: 3.297616750766508
Experience 3, Iter 23, disc loss: 0.04273883481421703, policy loss: 3.307826901643595
Experience 3, Iter 24, disc loss: 0.04515846038523942, policy loss: 3.246012790695517
Experience 3, Iter 25, disc loss: 0.042658569811631714, policy loss: 3.2979082255126007
Experience 3, Iter 26, disc loss: 0.04394433685940186, policy loss: 3.261170267761856
Experience 3, Iter 27, disc loss: 0.043802251571278976, policy loss: 3.265578295685981
Experience 3, Iter 28, disc loss: 0.04223439142337357, policy loss: 3.2961130250510102
Experience 3, Iter 29, disc loss: 0.04252421132749284, policy loss: 3.2929837930720263
Experience 3, Iter 30, disc loss: 0.04568977840976623, policy loss: 3.224928138591155
Experience 3, Iter 31, disc loss: 0.04401534890895241, policy loss: 3.264247522332884
Experience 3, Iter 32, disc loss: 0.0427236476314031, policy loss: 3.286846284651361
Experience 3, Iter 33, disc loss: 0.04165433199006537, policy loss: 3.3277689443139744
Experience 3, Iter 34, disc loss: 0.04142766447940149, policy loss: 3.316659243402608
Experience 3, Iter 35, disc loss: 0.042214136199711615, policy loss: 3.2950186470894964
Experience 3, Iter 36, disc loss: 0.04594650794246446, policy loss: 3.199981221221553
Experience 3, Iter 37, disc loss: 0.044904579721626106, policy loss: 3.2418565247834312
Experience 3, Iter 38, disc loss: 0.0447551488620872, policy loss: 3.2438558035488096
Experience 3, Iter 39, disc loss: 0.04317709534482497, policy loss: 3.281104119412568
Experience 3, Iter 40, disc loss: 0.0419768873193308, policy loss: 3.3212063721571066
Experience 3, Iter 41, disc loss: 0.04373966401684981, policy loss: 3.2645782881851213
Experience 3, Iter 42, disc loss: 0.04088220485729695, policy loss: 3.3281911077168567
Experience 3, Iter 43, disc loss: 0.04515599820395768, policy loss: 3.2256538837283077
Experience 3, Iter 44, disc loss: 0.04443422449909398, policy loss: 3.253713190047927
Experience 3, Iter 45, disc loss: 0.04406988439104056, policy loss: 3.2508205389240574
Experience 3, Iter 46, disc loss: 0.042125658000570575, policy loss: 3.2995149495476026
Experience 3, Iter 47, disc loss: 0.041970529179235515, policy loss: 3.305897502578626
Experience 3, Iter 48, disc loss: 0.04279496208450924, policy loss: 3.2853957881698377
Experience 3, Iter 49, disc loss: 0.039584937497790455, policy loss: 3.3650712467142037
Experience 3, Iter 50, disc loss: 0.04527063540051646, policy loss: 3.241609962426344
Experience 3, Iter 51, disc loss: 0.04280971498493806, policy loss: 3.28642267974214
Experience 3, Iter 52, disc loss: 0.03913914900733409, policy loss: 3.3891026219459435
Experience 3, Iter 53, disc loss: 0.045720712395850266, policy loss: 3.2297249796859235
Experience 3, Iter 54, disc loss: 0.04261709120606387, policy loss: 3.3245893295774476
Experience 3, Iter 55, disc loss: 0.044289729095337814, policy loss: 3.262664673378349
Experience 3, Iter 56, disc loss: 0.042842113653553414, policy loss: 3.3090478900326623
Experience 3, Iter 57, disc loss: 0.04342159023066949, policy loss: 3.2857253027095075
Experience 3, Iter 58, disc loss: 0.046266539184624485, policy loss: 3.221153303277693
Experience 3, Iter 59, disc loss: 0.04277103977754038, policy loss: 3.3111841253615406
Experience 3, Iter 60, disc loss: 0.04254513854555988, policy loss: 3.3292318864635266
Experience 3, Iter 61, disc loss: 0.04204933001299918, policy loss: 3.343656510146222
Experience 3, Iter 62, disc loss: 0.04331221195301776, policy loss: 3.286025563058037
Experience 3, Iter 63, disc loss: 0.04279454402139117, policy loss: 3.3108020798068685
Experience 3, Iter 64, disc loss: 0.046649839597275006, policy loss: 3.2150119460370776
Experience 3, Iter 65, disc loss: 0.04381506077271548, policy loss: 3.2725189849271272
Experience 3, Iter 66, disc loss: 0.04133055915977952, policy loss: 3.355613390464443
Experience 3, Iter 67, disc loss: 0.04679210564188892, policy loss: 3.237924816222336
Experience 3, Iter 68, disc loss: 0.04160877945498086, policy loss: 3.36557223043391
Experience 3, Iter 69, disc loss: 0.04308037237943832, policy loss: 3.327659924169229
Experience 3, Iter 70, disc loss: 0.04237645793328752, policy loss: 3.326099227198311
Experience 3, Iter 71, disc loss: 0.043440190570118484, policy loss: 3.329921852145569
Experience 3, Iter 72, disc loss: 0.0416934174888324, policy loss: 3.395639013771392
Experience 3, Iter 73, disc loss: 0.038949466216464945, policy loss: 3.4278330792399188
Experience 3, Iter 74, disc loss: 0.03888714335325878, policy loss: 3.445256480452447
Experience 3, Iter 75, disc loss: 0.04329285977343611, policy loss: 3.327881765540071
Experience 3, Iter 76, disc loss: 0.03946869289518478, policy loss: 3.4830958951972164
Experience 3, Iter 77, disc loss: 0.039626089191533535, policy loss: 3.4466920509264503
Experience 3, Iter 78, disc loss: 0.04093619737631513, policy loss: 3.426351702468883
Experience 3, Iter 79, disc loss: 0.040637677789148334, policy loss: 3.417508352299573
Experience 3, Iter 80, disc loss: 0.04326121956302102, policy loss: 3.340804169454647
Experience 3, Iter 81, disc loss: 0.04025040519258016, policy loss: 3.4525649834986587
Experience 3, Iter 82, disc loss: 0.041317111998513525, policy loss: 3.3982472635829617
Experience 3, Iter 83, disc loss: 0.04252639280164839, policy loss: 3.396203019410584
Experience 3, Iter 84, disc loss: 0.03899540703416206, policy loss: 3.489235499775891
Experience 3, Iter 85, disc loss: 0.039680822807634, policy loss: 3.4281542982648934
Experience 3, Iter 86, disc loss: 0.03846201718854592, policy loss: 3.489535309921253
Experience 3, Iter 87, disc loss: 0.036324490206691526, policy loss: 3.579797687936325
Experience 3, Iter 88, disc loss: 0.03740838198167895, policy loss: 3.5043789282656848
Experience 3, Iter 89, disc loss: 0.035569309903471305, policy loss: 3.5678043656068654
Experience 3, Iter 90, disc loss: 0.0413860399829923, policy loss: 3.4503146599638583
Experience 3, Iter 91, disc loss: 0.04000537357372817, policy loss: 3.4304867406721233
Experience 3, Iter 92, disc loss: 0.03951938614612587, policy loss: 3.4900926476654845
Experience 3, Iter 93, disc loss: 0.041937049182330675, policy loss: 3.454091378321934
Experience 3, Iter 94, disc loss: 0.0331375629278522, policy loss: 3.672729160953431
Experience 3, Iter 95, disc loss: 0.038915498334377394, policy loss: 3.5208857841512273
Experience 3, Iter 96, disc loss: 0.04313544693081814, policy loss: 3.3733519344453478
Experience 3, Iter 97, disc loss: 0.03152108600494108, policy loss: 3.702492179741734
Experience 3, Iter 98, disc loss: 0.03612616871009181, policy loss: 3.6531023379641514
Experience 3, Iter 99, disc loss: 0.03313140851289912, policy loss: 3.702219603733506
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0018],
        [0.0089],
        [0.4057],
        [0.0092]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0229, 0.0663, 0.4242, 0.0101, 0.0013, 0.1856]],

        [[0.0229, 0.0663, 0.4242, 0.0101, 0.0013, 0.1856]],

        [[0.0229, 0.0663, 0.4242, 0.0101, 0.0013, 0.1856]],

        [[0.0229, 0.0663, 0.4242, 0.0101, 0.0013, 0.1856]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0072, 0.0357, 1.6227, 0.0367], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0072, 0.0357, 1.6227, 0.0367])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.050
Iter 2/2000 - Loss: 0.237
Iter 3/2000 - Loss: 0.246
Iter 4/2000 - Loss: 0.378
Iter 5/2000 - Loss: 0.331
Iter 6/2000 - Loss: 0.175
Iter 7/2000 - Loss: 0.058
Iter 8/2000 - Loss: 0.028
Iter 9/2000 - Loss: 0.016
Iter 10/2000 - Loss: -0.049
Iter 11/2000 - Loss: -0.162
Iter 12/2000 - Loss: -0.276
Iter 13/2000 - Loss: -0.359
Iter 14/2000 - Loss: -0.417
Iter 15/2000 - Loss: -0.488
Iter 16/2000 - Loss: -0.604
Iter 17/2000 - Loss: -0.756
Iter 18/2000 - Loss: -0.904
Iter 19/2000 - Loss: -1.018
Iter 20/2000 - Loss: -1.108
Iter 1981/2000 - Loss: -7.010
Iter 1982/2000 - Loss: -7.010
Iter 1983/2000 - Loss: -7.010
Iter 1984/2000 - Loss: -7.010
Iter 1985/2000 - Loss: -7.010
Iter 1986/2000 - Loss: -7.010
Iter 1987/2000 - Loss: -7.010
Iter 1988/2000 - Loss: -7.010
Iter 1989/2000 - Loss: -7.010
Iter 1990/2000 - Loss: -7.010
Iter 1991/2000 - Loss: -7.010
Iter 1992/2000 - Loss: -7.010
Iter 1993/2000 - Loss: -7.010
Iter 1994/2000 - Loss: -7.010
Iter 1995/2000 - Loss: -7.010
Iter 1996/2000 - Loss: -7.010
Iter 1997/2000 - Loss: -7.010
Iter 1998/2000 - Loss: -7.010
Iter 1999/2000 - Loss: -7.011
Iter 2000/2000 - Loss: -7.011
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0014],
        [0.0004]])
Lengthscale: tensor([[[15.4373,  2.6260, 52.7061,  5.5955,  6.3793, 19.3414]],

        [[17.3590, 32.1985, 10.6638,  0.6926,  0.3654,  4.4699]],

        [[16.4219, 42.8381, 14.4232,  0.6743,  4.8162,  6.2867]],

        [[23.8829, 42.3839, 15.6848,  3.7916, 11.7699, 29.0999]]])
Signal Variance: tensor([0.0240, 0.1229, 4.9122, 0.4662])
Estimated target variance: tensor([0.0072, 0.0357, 1.6227, 0.0367])
N: 40
Signal to noise ratio: tensor([ 7.9543, 19.9221, 59.1209, 34.4384])
Bound on condition number: tensor([  2531.8372,  15876.6569, 139812.2896,  47441.1208])
Policy Optimizer learning rate:
9.968441746484834e-05
Experience 4, Iter 0, disc loss: 0.03696702123256718, policy loss: 3.408341363677938
Experience 4, Iter 1, disc loss: 0.03875275195856537, policy loss: 3.3718981034268616
Experience 4, Iter 2, disc loss: 0.03717092755304232, policy loss: 3.4100227130687872
Experience 4, Iter 3, disc loss: 0.036314006647665195, policy loss: 3.446541556522115
Experience 4, Iter 4, disc loss: 0.03592924085947171, policy loss: 3.4462519845810204
Experience 4, Iter 5, disc loss: 0.0365179057039843, policy loss: 3.434740186176944
Experience 4, Iter 6, disc loss: 0.03766590027581728, policy loss: 3.3934471967583413
Experience 4, Iter 7, disc loss: 0.0382209570347295, policy loss: 3.377754312256135
Experience 4, Iter 8, disc loss: 0.03512462230303281, policy loss: 3.4754878754628113
Experience 4, Iter 9, disc loss: 0.03594675055942351, policy loss: 3.4470123924712404
Experience 4, Iter 10, disc loss: 0.03433646859584133, policy loss: 3.4875013910203885
Experience 4, Iter 11, disc loss: 0.034485640975941326, policy loss: 3.479258056780813
Experience 4, Iter 12, disc loss: 0.033928118190095946, policy loss: 3.519141299791403
Experience 4, Iter 13, disc loss: 0.034563286603767215, policy loss: 3.489041968514727
Experience 4, Iter 14, disc loss: 0.03625777658779601, policy loss: 3.443192278171418
Experience 4, Iter 15, disc loss: 0.03492275029513821, policy loss: 3.4668028223086367
Experience 4, Iter 16, disc loss: 0.034003105584301016, policy loss: 3.498271281491136
Experience 4, Iter 17, disc loss: 0.03395724997007865, policy loss: 3.495268838525357
Experience 4, Iter 18, disc loss: 0.03515336981007671, policy loss: 3.467015349115017
Experience 4, Iter 19, disc loss: 0.03277382679839912, policy loss: 3.5386462297776298
Experience 4, Iter 20, disc loss: 0.03328242801382677, policy loss: 3.538902437827568
Experience 4, Iter 21, disc loss: 0.03299917014817982, policy loss: 3.533632379178349
Experience 4, Iter 22, disc loss: 0.033513933286233935, policy loss: 3.51617264868088
Experience 4, Iter 23, disc loss: 0.03352130492802059, policy loss: 3.5105825047358326
Experience 4, Iter 24, disc loss: 0.03326169504160741, policy loss: 3.519268218385373
Experience 4, Iter 25, disc loss: 0.03240906450696263, policy loss: 3.555350346562327
Experience 4, Iter 26, disc loss: 0.03234666614239152, policy loss: 3.558956010817813
Experience 4, Iter 27, disc loss: 0.031168974069732257, policy loss: 3.58846144454128
Experience 4, Iter 28, disc loss: 0.032322127422094254, policy loss: 3.5546558170501523
Experience 4, Iter 29, disc loss: 0.030686086370958477, policy loss: 3.599716321947026
Experience 4, Iter 30, disc loss: 0.031062653741492786, policy loss: 3.5805481104268626
Experience 4, Iter 31, disc loss: 0.03036615659375437, policy loss: 3.6061826476896375
Experience 4, Iter 32, disc loss: 0.030303662371961776, policy loss: 3.6075303886122496
Experience 4, Iter 33, disc loss: 0.028421642520377408, policy loss: 3.6707015744015017
Experience 4, Iter 34, disc loss: 0.029876828102682572, policy loss: 3.6243491468251303
Experience 4, Iter 35, disc loss: 0.028056273621433737, policy loss: 3.6950018038238484
Experience 4, Iter 36, disc loss: 0.026087529200856885, policy loss: 3.7637444349983307
Experience 4, Iter 37, disc loss: 0.026563241752386352, policy loss: 3.746740006910823
Experience 4, Iter 38, disc loss: 0.027497349986880644, policy loss: 3.7161271203193875
Experience 4, Iter 39, disc loss: 0.026107801628498226, policy loss: 3.783954337318578
Experience 4, Iter 40, disc loss: 0.025791823155469875, policy loss: 3.7816229379034287
Experience 4, Iter 41, disc loss: 0.025579045030577924, policy loss: 3.7848722166174986
Experience 4, Iter 42, disc loss: 0.025504022386178982, policy loss: 3.775477213252038
Experience 4, Iter 43, disc loss: 0.025792790621609836, policy loss: 3.7703501322039497
Experience 4, Iter 44, disc loss: 0.02457921327530249, policy loss: 3.8318453518371465
Experience 4, Iter 45, disc loss: 0.02462178819966018, policy loss: 3.837689333727276
Experience 4, Iter 46, disc loss: 0.024314569781391827, policy loss: 3.8365243652439833
Experience 4, Iter 47, disc loss: 0.022275709893898828, policy loss: 3.933978031149222
Experience 4, Iter 48, disc loss: 0.023582722773653204, policy loss: 3.850556620592999
Experience 4, Iter 49, disc loss: 0.022645074393493166, policy loss: 3.905744437518965
Experience 4, Iter 50, disc loss: 0.02335841063230625, policy loss: 3.8868707023837867
Experience 4, Iter 51, disc loss: 0.021474632560200196, policy loss: 3.9712472669485193
Experience 4, Iter 52, disc loss: 0.020886159245772314, policy loss: 4.00070505108017
Experience 4, Iter 53, disc loss: 0.021054780161651722, policy loss: 3.966737823084525
Experience 4, Iter 54, disc loss: 0.021150938810377582, policy loss: 3.9772989894123003
Experience 4, Iter 55, disc loss: 0.02075865258450452, policy loss: 3.986356803811968
Experience 4, Iter 56, disc loss: 0.02144976780198754, policy loss: 3.9664930689455336
Experience 4, Iter 57, disc loss: 0.019291901087787368, policy loss: 4.055598834285995
Experience 4, Iter 58, disc loss: 0.01924995967263263, policy loss: 4.069799429591534
Experience 4, Iter 59, disc loss: 0.018459028806095224, policy loss: 4.093576310799903
Experience 4, Iter 60, disc loss: 0.01994410168274715, policy loss: 4.0279854054111945
Experience 4, Iter 61, disc loss: 0.01718582963519551, policy loss: 4.210714104003331
Experience 4, Iter 62, disc loss: 0.017475223104882285, policy loss: 4.188012020428149
Experience 4, Iter 63, disc loss: 0.01814525424629207, policy loss: 4.1311872251683
Experience 4, Iter 64, disc loss: 0.017655453348736902, policy loss: 4.168527879986188
Experience 4, Iter 65, disc loss: 0.018598160057321785, policy loss: 4.133626037542607
Experience 4, Iter 66, disc loss: 0.01549885977488886, policy loss: 4.287345279236724
Experience 4, Iter 67, disc loss: 0.016552993342106515, policy loss: 4.216553263080923
Experience 4, Iter 68, disc loss: 0.015445691752084031, policy loss: 4.316474522344559
Experience 4, Iter 69, disc loss: 0.015284413371985755, policy loss: 4.298606010744413
Experience 4, Iter 70, disc loss: 0.015332092164780226, policy loss: 4.317676455201287
Experience 4, Iter 71, disc loss: 0.015100542222844201, policy loss: 4.3338833885646535
Experience 4, Iter 72, disc loss: 0.014836718792616, policy loss: 4.34202879010378
Experience 4, Iter 73, disc loss: 0.014766601611308227, policy loss: 4.342006892062885
Experience 4, Iter 74, disc loss: 0.013806746980864067, policy loss: 4.430561908277139
Experience 4, Iter 75, disc loss: 0.01490545733883723, policy loss: 4.324177820205394
Experience 4, Iter 76, disc loss: 0.013799390673335939, policy loss: 4.434914372091059
Experience 4, Iter 77, disc loss: 0.014335368395859627, policy loss: 4.370662657168434
Experience 4, Iter 78, disc loss: 0.013928145163659965, policy loss: 4.40968131745873
Experience 4, Iter 79, disc loss: 0.01321128688509587, policy loss: 4.439513618700296
Experience 4, Iter 80, disc loss: 0.012981061473842228, policy loss: 4.472182668127919
Experience 4, Iter 81, disc loss: 0.012292255779719184, policy loss: 4.533785574149324
Experience 4, Iter 82, disc loss: 0.01228539795568879, policy loss: 4.530012124628116
Experience 4, Iter 83, disc loss: 0.012766460763018102, policy loss: 4.468911903838172
Experience 4, Iter 84, disc loss: 0.012198499376469593, policy loss: 4.555843354286592
Experience 4, Iter 85, disc loss: 0.01187804936774243, policy loss: 4.5691466307181035
Experience 4, Iter 86, disc loss: 0.011798191219646552, policy loss: 4.612054968871117
Experience 4, Iter 87, disc loss: 0.010911093167454778, policy loss: 4.653232090702817
Experience 4, Iter 88, disc loss: 0.011509891219634551, policy loss: 4.609762946069573
Experience 4, Iter 89, disc loss: 0.011348918364405882, policy loss: 4.638613846807031
Experience 4, Iter 90, disc loss: 0.010979235757811007, policy loss: 4.6378305490013165
Experience 4, Iter 91, disc loss: 0.010175258929908476, policy loss: 4.744660142514048
Experience 4, Iter 92, disc loss: 0.011276968075488518, policy loss: 4.584507610572041
Experience 4, Iter 93, disc loss: 0.010676565783226368, policy loss: 4.652313983608597
Experience 4, Iter 94, disc loss: 0.010638614363546088, policy loss: 4.694436734486043
Experience 4, Iter 95, disc loss: 0.010707918247046627, policy loss: 4.686766606938953
Experience 4, Iter 96, disc loss: 0.009986460790055775, policy loss: 4.738188071971452
Experience 4, Iter 97, disc loss: 0.009288073962331935, policy loss: 4.7964922284784794
Experience 4, Iter 98, disc loss: 0.010302066128332676, policy loss: 4.699017411389778
Experience 4, Iter 99, disc loss: 0.009744300616183284, policy loss: 4.771445350117581
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0022],
        [0.0098],
        [0.3232],
        [0.0074]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0233, 0.0835, 0.3397, 0.0094, 0.0011, 0.2819]],

        [[0.0233, 0.0835, 0.3397, 0.0094, 0.0011, 0.2819]],

        [[0.0233, 0.0835, 0.3397, 0.0094, 0.0011, 0.2819]],

        [[0.0233, 0.0835, 0.3397, 0.0094, 0.0011, 0.2819]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0089, 0.0392, 1.2926, 0.0296], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0089, 0.0392, 1.2926, 0.0296])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.474
Iter 2/2000 - Loss: 0.190
Iter 3/2000 - Loss: 0.202
Iter 4/2000 - Loss: 0.187
Iter 5/2000 - Loss: 0.110
Iter 6/2000 - Loss: 0.014
Iter 7/2000 - Loss: -0.038
Iter 8/2000 - Loss: -0.083
Iter 9/2000 - Loss: -0.145
Iter 10/2000 - Loss: -0.220
Iter 11/2000 - Loss: -0.295
Iter 12/2000 - Loss: -0.380
Iter 13/2000 - Loss: -0.501
Iter 14/2000 - Loss: -0.642
Iter 15/2000 - Loss: -0.763
Iter 16/2000 - Loss: -0.863
Iter 17/2000 - Loss: -0.980
Iter 18/2000 - Loss: -1.143
Iter 19/2000 - Loss: -1.329
Iter 20/2000 - Loss: -1.506
Iter 1981/2000 - Loss: -7.250
Iter 1982/2000 - Loss: -7.250
Iter 1983/2000 - Loss: -7.250
Iter 1984/2000 - Loss: -7.250
Iter 1985/2000 - Loss: -7.250
Iter 1986/2000 - Loss: -7.250
Iter 1987/2000 - Loss: -7.250
Iter 1988/2000 - Loss: -7.250
Iter 1989/2000 - Loss: -7.250
Iter 1990/2000 - Loss: -7.250
Iter 1991/2000 - Loss: -7.250
Iter 1992/2000 - Loss: -7.250
Iter 1993/2000 - Loss: -7.250
Iter 1994/2000 - Loss: -7.250
Iter 1995/2000 - Loss: -7.250
Iter 1996/2000 - Loss: -7.250
Iter 1997/2000 - Loss: -7.250
Iter 1998/2000 - Loss: -7.250
Iter 1999/2000 - Loss: -7.251
Iter 2000/2000 - Loss: -7.251
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0015],
        [0.0004]])
Lengthscale: tensor([[[13.1832,  2.7599, 50.5467,  5.9382,  6.3178, 20.9987]],

        [[15.3389, 34.3664, 11.6001,  0.8040,  0.3853,  5.2032]],

        [[16.0446, 37.2849, 13.4352,  0.6825,  4.0511,  4.5072]],

        [[21.3138, 40.4541, 15.4236,  3.9040, 11.6710, 30.8079]]])
Signal Variance: tensor([0.0236, 0.1623, 4.1604, 0.4572])
Estimated target variance: tensor([0.0089, 0.0392, 1.2926, 0.0296])
N: 50
Signal to noise ratio: tensor([ 7.8609, 21.6365, 51.9930, 32.7832])
Bound on condition number: tensor([  3090.7147,  23407.9533, 135164.7394,  53737.9840])
Policy Optimizer learning rate:
9.95794447581801e-05
Experience 5, Iter 0, disc loss: 0.00889881709355792, policy loss: 4.8265594438532755
Experience 5, Iter 1, disc loss: 0.00883607439360521, policy loss: 4.832588998039216
Experience 5, Iter 2, disc loss: 0.00867716623743092, policy loss: 4.858262195318577
Experience 5, Iter 3, disc loss: 0.009047684906267043, policy loss: 4.8056670965256245
Experience 5, Iter 4, disc loss: 0.008688134240892487, policy loss: 4.8530363328172506
Experience 5, Iter 5, disc loss: 0.008570507733403798, policy loss: 4.867281327780899
Experience 5, Iter 6, disc loss: 0.008185646811857265, policy loss: 4.9228061314394775
Experience 5, Iter 7, disc loss: 0.014325807934814411, policy loss: 4.863868268454656
Experience 5, Iter 8, disc loss: 0.008258371135299791, policy loss: 4.906658896811576
Experience 5, Iter 9, disc loss: 0.00823114277083654, policy loss: 4.910310808038004
Experience 5, Iter 10, disc loss: 0.008023679454052031, policy loss: 4.928300552162561
Experience 5, Iter 11, disc loss: 0.00749152694205506, policy loss: 5.007518129585696
Experience 5, Iter 12, disc loss: 0.008107574971753712, policy loss: 4.920196902226793
Experience 5, Iter 13, disc loss: 0.007536472763310537, policy loss: 5.006369235712847
Experience 5, Iter 14, disc loss: 0.007137089116821279, policy loss: 5.057699916399285
Experience 5, Iter 15, disc loss: 0.007486361137095719, policy loss: 5.008652268989522
Experience 5, Iter 16, disc loss: 0.007229863957814087, policy loss: 5.061608118243646
Experience 5, Iter 17, disc loss: 0.0073437914696575755, policy loss: 5.028044717507545
Experience 5, Iter 18, disc loss: 0.007569320203341714, policy loss: 4.9966158352658105
Experience 5, Iter 19, disc loss: 0.007090647317326236, policy loss: 5.066377185064985
Experience 5, Iter 20, disc loss: 0.007310002761743147, policy loss: 5.037082183463969
Experience 5, Iter 21, disc loss: 0.0065657422080210815, policy loss: 5.161493403233333
Experience 5, Iter 22, disc loss: 0.006786395957141609, policy loss: 5.110452077723382
Experience 5, Iter 23, disc loss: 0.006497757932056716, policy loss: 5.164007275486513
Experience 5, Iter 24, disc loss: 0.006962160436637824, policy loss: 5.082750540691331
Experience 5, Iter 25, disc loss: 0.0063931878841993065, policy loss: 5.1726250124066
Experience 5, Iter 26, disc loss: 0.006574363031741754, policy loss: 5.141299816046264
Experience 5, Iter 27, disc loss: 0.006481298861842309, policy loss: 5.160355970761029
Experience 5, Iter 28, disc loss: 0.006483989694977386, policy loss: 5.157651879340433
Experience 5, Iter 29, disc loss: 0.1129151937794832, policy loss: 5.0615993738355405
Experience 5, Iter 30, disc loss: 0.00621512094549969, policy loss: 5.20463819729517
Experience 5, Iter 31, disc loss: 0.006403310074937185, policy loss: 5.177918655103747
Experience 5, Iter 32, disc loss: 0.006141172984657063, policy loss: 5.2153666573854025
Experience 5, Iter 33, disc loss: 0.005973561217998953, policy loss: 5.255899365705006
Experience 5, Iter 34, disc loss: 0.006018884882861675, policy loss: 5.253386399739908
Experience 5, Iter 35, disc loss: 0.005864732233041464, policy loss: 5.259982849547897
Experience 5, Iter 36, disc loss: 0.005696346806635655, policy loss: 5.302225583545464
Experience 5, Iter 37, disc loss: 0.005892211377606994, policy loss: 5.265485484131664
Experience 5, Iter 38, disc loss: 0.005844774415773029, policy loss: 5.275101448387536
Experience 5, Iter 39, disc loss: 0.005821245887234155, policy loss: 5.27421645077022
Experience 5, Iter 40, disc loss: 0.005877203795013112, policy loss: 5.269694659342658
Experience 5, Iter 41, disc loss: 0.005317406560219626, policy loss: 5.3712852281775145
Experience 5, Iter 42, disc loss: 0.005485900358836112, policy loss: 5.328789406582168
Experience 5, Iter 43, disc loss: 0.0054211908402467274, policy loss: 5.355357735142972
Experience 5, Iter 44, disc loss: 0.005310785884425389, policy loss: 5.3742495972788245
Experience 5, Iter 45, disc loss: 0.005379378964634029, policy loss: 5.355857200549172
Experience 5, Iter 46, disc loss: 0.005297423732033839, policy loss: 5.373517826328926
Experience 5, Iter 47, disc loss: 0.005209759578006883, policy loss: 5.401923580912732
Experience 5, Iter 48, disc loss: 0.00496583279234414, policy loss: 5.451165660690398
Experience 5, Iter 49, disc loss: 0.00518349947395905, policy loss: 5.393279544440693
Experience 5, Iter 50, disc loss: 0.005108051602843947, policy loss: 5.410316105130732
Experience 5, Iter 51, disc loss: 0.005379547121609262, policy loss: 5.352621033053353
Experience 5, Iter 52, disc loss: 0.005310715572544989, policy loss: 5.37047806926469
Experience 5, Iter 53, disc loss: 0.00492051965925791, policy loss: 5.454716319512093
Experience 5, Iter 54, disc loss: 0.005027802827725691, policy loss: 5.416579416614135
Experience 5, Iter 55, disc loss: 0.0052738489036568085, policy loss: 5.3687642402030935
Experience 5, Iter 56, disc loss: 0.00478191496684346, policy loss: 5.486130536933629
Experience 5, Iter 57, disc loss: 0.004704315255165588, policy loss: 5.504191820417925
Experience 5, Iter 58, disc loss: 0.004985337297085715, policy loss: 5.4360610072950974
Experience 5, Iter 59, disc loss: 0.004967942273938008, policy loss: 5.438521279743146
Experience 5, Iter 60, disc loss: 0.004879433137772388, policy loss: 5.464991886063264
Experience 5, Iter 61, disc loss: 0.004821337784721794, policy loss: 5.456869957128248
Experience 5, Iter 62, disc loss: 0.004647265348865843, policy loss: 5.506022265897592
Experience 5, Iter 63, disc loss: 0.004750652848254649, policy loss: 5.469499637285248
Experience 5, Iter 64, disc loss: 0.004368915299068887, policy loss: 5.582205930147256
Experience 5, Iter 65, disc loss: 0.004399599879864022, policy loss: 5.5558924844204265
Experience 5, Iter 66, disc loss: 0.004405371726951902, policy loss: 5.562431923874979
Experience 5, Iter 67, disc loss: 0.0043936642231133015, policy loss: 5.572629176528078
Experience 5, Iter 68, disc loss: 0.004579895465787281, policy loss: 5.510393370306301
Experience 5, Iter 69, disc loss: 0.0043048541146647416, policy loss: 5.587642880641468
Experience 5, Iter 70, disc loss: 0.00456667348364211, policy loss: 5.525564528711756
Experience 5, Iter 71, disc loss: 0.0040900540203106656, policy loss: 5.633990494844895
Experience 5, Iter 72, disc loss: 0.004340127222867433, policy loss: 5.579631493710166
Experience 5, Iter 73, disc loss: 0.004543967985307357, policy loss: 5.529786577594129
Experience 5, Iter 74, disc loss: 0.004209509204510185, policy loss: 5.60830605165175
Experience 5, Iter 75, disc loss: 0.003933042172704638, policy loss: 5.6845441914623835
Experience 5, Iter 76, disc loss: 0.003958011825955968, policy loss: 5.679481141682006
Experience 5, Iter 77, disc loss: 0.004328370392034005, policy loss: 5.581934590868665
Experience 5, Iter 78, disc loss: 0.004053114494589252, policy loss: 5.676070045520784
Experience 5, Iter 79, disc loss: 0.004168454051574067, policy loss: 5.624863379873332
Experience 5, Iter 80, disc loss: 0.004191326107757306, policy loss: 5.613205101744194
Experience 5, Iter 81, disc loss: 0.004017585336752607, policy loss: 5.663938351633105
Experience 5, Iter 82, disc loss: 0.0039002367577619895, policy loss: 5.697059059844475
Experience 5, Iter 83, disc loss: 0.0039424840994827974, policy loss: 5.690331805117168
Experience 5, Iter 84, disc loss: 0.0037142197045598856, policy loss: 5.744656279364772
Experience 5, Iter 85, disc loss: 0.004018671153782713, policy loss: 5.663232965684844
Experience 5, Iter 86, disc loss: 0.003766559688815699, policy loss: 5.727987959914798
Experience 5, Iter 87, disc loss: 0.003733572152196829, policy loss: 5.742300379945769
Experience 5, Iter 88, disc loss: 0.0035862750031655, policy loss: 5.772783843114857
Experience 5, Iter 89, disc loss: 0.0038211627860626923, policy loss: 5.715034455923759
Experience 5, Iter 90, disc loss: 0.003646889607523561, policy loss: 5.75458305848383
Experience 5, Iter 91, disc loss: 0.003642470259946738, policy loss: 5.767076695450361
Experience 5, Iter 92, disc loss: 0.0034937043033146224, policy loss: 5.798685569535365
Experience 5, Iter 93, disc loss: 0.0037104315248492376, policy loss: 5.745828271557281
Experience 5, Iter 94, disc loss: 0.003696281469702109, policy loss: 5.738679711389757
Experience 5, Iter 95, disc loss: 0.0036808678507674894, policy loss: 5.743767727301144
Experience 5, Iter 96, disc loss: 0.0037366608267190527, policy loss: 5.7215469898260585
Experience 5, Iter 97, disc loss: 0.003518695592151542, policy loss: 5.788056158140791
Experience 5, Iter 98, disc loss: 0.0036899654187143318, policy loss: 5.7481564059580945
Experience 5, Iter 99, disc loss: 0.0034513988045143298, policy loss: 5.81753874200634
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0024],
        [0.0095],
        [0.2697],
        [0.0062]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0204, 0.0864, 0.2840, 0.0084, 0.0009, 0.3017]],

        [[0.0204, 0.0864, 0.2840, 0.0084, 0.0009, 0.3017]],

        [[0.0204, 0.0864, 0.2840, 0.0084, 0.0009, 0.3017]],

        [[0.0204, 0.0864, 0.2840, 0.0084, 0.0009, 0.3017]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0096, 0.0380, 1.0787, 0.0249], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0096, 0.0380, 1.0787, 0.0249])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.128
Iter 2/2000 - Loss: 0.103
Iter 3/2000 - Loss: -0.005
Iter 4/2000 - Loss: -0.079
Iter 5/2000 - Loss: -0.080
Iter 6/2000 - Loss: -0.120
Iter 7/2000 - Loss: -0.228
Iter 8/2000 - Loss: -0.296
Iter 9/2000 - Loss: -0.301
Iter 10/2000 - Loss: -0.337
Iter 11/2000 - Loss: -0.453
Iter 12/2000 - Loss: -0.589
Iter 13/2000 - Loss: -0.692
Iter 14/2000 - Loss: -0.779
Iter 15/2000 - Loss: -0.895
Iter 16/2000 - Loss: -1.052
Iter 17/2000 - Loss: -1.231
Iter 18/2000 - Loss: -1.413
Iter 19/2000 - Loss: -1.595
Iter 20/2000 - Loss: -1.788
Iter 1981/2000 - Loss: -7.421
Iter 1982/2000 - Loss: -7.421
Iter 1983/2000 - Loss: -7.421
Iter 1984/2000 - Loss: -7.421
Iter 1985/2000 - Loss: -7.421
Iter 1986/2000 - Loss: -7.421
Iter 1987/2000 - Loss: -7.421
Iter 1988/2000 - Loss: -7.421
Iter 1989/2000 - Loss: -7.421
Iter 1990/2000 - Loss: -7.421
Iter 1991/2000 - Loss: -7.421
Iter 1992/2000 - Loss: -7.421
Iter 1993/2000 - Loss: -7.421
Iter 1994/2000 - Loss: -7.421
Iter 1995/2000 - Loss: -7.421
Iter 1996/2000 - Loss: -7.421
Iter 1997/2000 - Loss: -7.421
Iter 1998/2000 - Loss: -7.421
Iter 1999/2000 - Loss: -7.421
Iter 2000/2000 - Loss: -7.421
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0004],
        [0.0003],
        [0.0028],
        [0.0004]])
Lengthscale: tensor([[[12.9776,  2.9760, 45.5152,  6.0857,  7.5755, 22.1807]],

        [[16.5916, 34.2420, 11.3969,  0.8905,  0.4104,  6.4188]],

        [[19.6937, 31.5339, 14.3388,  0.7042,  4.2369,  9.2812]],

        [[18.6125, 35.8424, 14.4821,  3.4698, 10.5781, 28.3704]]])
Signal Variance: tensor([0.0274, 0.1982, 5.5627, 0.3899])
Estimated target variance: tensor([0.0096, 0.0380, 1.0787, 0.0249])
N: 60
Signal to noise ratio: tensor([ 8.5290, 24.6188, 44.7277, 29.6410])
Bound on condition number: tensor([  4365.6624,  36366.1071, 120035.2838,  52716.1658])
Policy Optimizer learning rate:
9.947458259305312e-05
Experience 6, Iter 0, disc loss: 0.005862797152361731, policy loss: 5.328015945597366
Experience 6, Iter 1, disc loss: 0.005797076043503727, policy loss: 5.30139801780163
Experience 6, Iter 2, disc loss: 0.006054428952923376, policy loss: 5.285806716236676
Experience 6, Iter 3, disc loss: 0.0068268574751980445, policy loss: 5.18790475145583
Experience 6, Iter 4, disc loss: 0.005553391686778247, policy loss: 5.37407132934166
Experience 6, Iter 5, disc loss: 0.0050444214306913275, policy loss: 5.454490138475714
Experience 6, Iter 6, disc loss: 0.006113459147483295, policy loss: 5.2561241584600875
Experience 6, Iter 7, disc loss: 0.005497427771334662, policy loss: 5.346268756472578
Experience 6, Iter 8, disc loss: 0.005403654937990252, policy loss: 5.385449934205668
Experience 6, Iter 9, disc loss: 0.005625557501626287, policy loss: 5.400515296611806
Experience 6, Iter 10, disc loss: 0.006074326137444026, policy loss: 5.340782481575793
Experience 6, Iter 11, disc loss: 0.005503107216291316, policy loss: 5.3886071227310595
Experience 6, Iter 12, disc loss: 0.005848872745721819, policy loss: 5.309473847929585
Experience 6, Iter 13, disc loss: 0.005515244320821048, policy loss: 5.357264529330827
Experience 6, Iter 14, disc loss: 0.005788529440885142, policy loss: 5.314297161565696
Experience 6, Iter 15, disc loss: 0.005077274041722436, policy loss: 5.430315851316406
Experience 6, Iter 16, disc loss: 0.005341768675811842, policy loss: 5.430388001381135
Experience 6, Iter 17, disc loss: 0.0064355086826973894, policy loss: 5.237347737327262
Experience 6, Iter 18, disc loss: 0.0060203923861431256, policy loss: 5.324594765320032
Experience 6, Iter 19, disc loss: 0.006355847112177199, policy loss: 5.322384259332936
Experience 6, Iter 20, disc loss: 0.0059739936312485625, policy loss: 5.310366798334071
Experience 6, Iter 21, disc loss: 0.005885329402761468, policy loss: 5.312345802533436
Experience 6, Iter 22, disc loss: 0.006075019110250668, policy loss: 5.285120031553195
Experience 6, Iter 23, disc loss: 0.006202961072849324, policy loss: 5.2771480962664175
Experience 6, Iter 24, disc loss: 0.006108469516751754, policy loss: 5.326366300604006
Experience 6, Iter 25, disc loss: 0.005807373654290192, policy loss: 5.370913706510427
Experience 6, Iter 26, disc loss: 0.005971485825032962, policy loss: 5.358325406790263
Experience 6, Iter 27, disc loss: 0.006141239795089791, policy loss: 5.285864061976241
Experience 6, Iter 28, disc loss: 0.027171171727135162, policy loss: 5.17542346482246
Experience 6, Iter 29, disc loss: 0.00567703236594582, policy loss: 5.390524387934454
Experience 6, Iter 30, disc loss: 0.005979847206900813, policy loss: 5.365876263790788
Experience 6, Iter 31, disc loss: 0.0064678709199447355, policy loss: 5.318168670971206
Experience 6, Iter 32, disc loss: 0.0054912373794896285, policy loss: 5.445300244209607
Experience 6, Iter 33, disc loss: 0.006113293138482712, policy loss: 5.29757796864913
Experience 6, Iter 34, disc loss: 0.005996334142558539, policy loss: 5.358815904589322
Experience 6, Iter 35, disc loss: 0.00686289723556227, policy loss: 5.310644219430282
Experience 6, Iter 36, disc loss: 0.00554919341770671, policy loss: 5.390250278218932
Experience 6, Iter 37, disc loss: 0.005381754552413201, policy loss: 5.445969278581405
Experience 6, Iter 38, disc loss: 0.005412251414132126, policy loss: 5.469008203041787
Experience 6, Iter 39, disc loss: 0.00601734573507118, policy loss: 5.394332224653771
Experience 6, Iter 40, disc loss: 0.008675641331879494, policy loss: 5.2655808777847755
Experience 6, Iter 41, disc loss: 0.005195662614157896, policy loss: 5.5490362426471815
Experience 6, Iter 42, disc loss: 0.006403453274292759, policy loss: 5.497376165244535
Experience 6, Iter 43, disc loss: 0.006523217690572847, policy loss: 5.431511951724882
Experience 6, Iter 44, disc loss: 0.006487488002346036, policy loss: 5.393570609502268
Experience 6, Iter 45, disc loss: 0.005406070276089479, policy loss: 5.455232200569889
Experience 6, Iter 46, disc loss: 0.005509154940960842, policy loss: 5.421721646106894
Experience 6, Iter 47, disc loss: 0.006349212712972804, policy loss: 5.383667963949945
Experience 6, Iter 48, disc loss: 0.007044578172449568, policy loss: 5.299951759037293
Experience 6, Iter 49, disc loss: 0.005523090524103031, policy loss: 5.460903765519019
Experience 6, Iter 50, disc loss: 0.010078239586823507, policy loss: 5.288320777979278
Experience 6, Iter 51, disc loss: 0.007797285465836162, policy loss: 5.209993789933306
Experience 6, Iter 52, disc loss: 0.00588167345377879, policy loss: 5.435946473210823
Experience 6, Iter 53, disc loss: 0.005923223226789825, policy loss: 5.477382070844469
Experience 6, Iter 54, disc loss: 0.007711642769010597, policy loss: 5.533445097490526
Experience 6, Iter 55, disc loss: 0.006048259178364981, policy loss: 5.542944837101208
Experience 6, Iter 56, disc loss: 0.0066534589260762575, policy loss: 5.55399555943911
Experience 6, Iter 57, disc loss: 0.0061738153805936955, policy loss: 5.429815741546766
Experience 6, Iter 58, disc loss: 0.005019413075952519, policy loss: 5.594213138896883
Experience 6, Iter 59, disc loss: 0.007066628390053441, policy loss: 5.477682150490522
Experience 6, Iter 60, disc loss: 0.005787600734403725, policy loss: 5.534416152647651
Experience 6, Iter 61, disc loss: 0.006988761390575315, policy loss: 5.332969971067778
Experience 6, Iter 62, disc loss: 0.0069667519384773915, policy loss: 5.376442849683066
Experience 6, Iter 63, disc loss: 0.006479505705745475, policy loss: 5.421070433216123
Experience 6, Iter 64, disc loss: 0.007858500643037355, policy loss: 5.385763254480019
Experience 6, Iter 65, disc loss: 0.006901605632606781, policy loss: 5.418726831741463
Experience 6, Iter 66, disc loss: 0.006159633546240143, policy loss: 5.515390175556417
Experience 6, Iter 67, disc loss: 0.006469963235071613, policy loss: 5.403970753359072
Experience 6, Iter 68, disc loss: 0.0071053188876089675, policy loss: 5.36153541610356
Experience 6, Iter 69, disc loss: 0.007199248524833486, policy loss: 5.388903182931198
Experience 6, Iter 70, disc loss: 0.006823125442042927, policy loss: 5.471385809189049
Experience 6, Iter 71, disc loss: 0.00790749218729529, policy loss: 5.3048894774654105
Experience 6, Iter 72, disc loss: 0.006378142832905498, policy loss: 5.446979523004247
Experience 6, Iter 73, disc loss: 0.006805404923850128, policy loss: 5.4667355121372365
Experience 6, Iter 74, disc loss: 0.008303148083822539, policy loss: 5.308830525223911
Experience 6, Iter 75, disc loss: 0.006694372719317968, policy loss: 5.368299139922786
Experience 6, Iter 76, disc loss: 0.008051487778035507, policy loss: 5.3351231307502704
Experience 6, Iter 77, disc loss: 0.008503628500650211, policy loss: 5.339731078454026
Experience 6, Iter 78, disc loss: 0.007094935814618338, policy loss: 5.505418671199804
Experience 6, Iter 79, disc loss: 0.00762076022914926, policy loss: 5.350622165970091
Experience 6, Iter 80, disc loss: 0.008225644579338597, policy loss: 5.601605261391789
Experience 6, Iter 81, disc loss: 0.007516823459253162, policy loss: 5.546961145934601
Experience 6, Iter 82, disc loss: 0.013103409224039655, policy loss: 5.431134385942226
Experience 6, Iter 83, disc loss: 0.0074444889925805715, policy loss: 5.508707637937166
Experience 6, Iter 84, disc loss: 0.00900893822752407, policy loss: 5.397201758170695
Experience 6, Iter 85, disc loss: 0.011118100831506343, policy loss: 5.214824515985496
Experience 6, Iter 86, disc loss: 0.007810612554049661, policy loss: 5.36230666325132
Experience 6, Iter 87, disc loss: 0.008817762506912425, policy loss: 5.355823875970332
Experience 6, Iter 88, disc loss: 0.008633833972004993, policy loss: 5.363994402732578
Experience 6, Iter 89, disc loss: 0.007574732639163018, policy loss: 5.482227663865382
Experience 6, Iter 90, disc loss: 0.01034801999235206, policy loss: 5.555605189181407
Experience 6, Iter 91, disc loss: 0.008181976895512953, policy loss: 5.635868440688148
Experience 6, Iter 92, disc loss: 0.00863718949214906, policy loss: 5.624325096539271
Experience 6, Iter 93, disc loss: 0.007489758932787028, policy loss: 5.704671753449683
Experience 6, Iter 94, disc loss: 0.010030421182707309, policy loss: 5.807283939815834
Experience 6, Iter 95, disc loss: 0.009411694664665016, policy loss: 5.442981490436183
Experience 6, Iter 96, disc loss: 0.015479476919977982, policy loss: 5.10471385089026
Experience 6, Iter 97, disc loss: 0.008253502047265078, policy loss: 5.497589512310212
Experience 6, Iter 98, disc loss: 0.008380230657803349, policy loss: 5.5534147996458145
Experience 6, Iter 99, disc loss: 0.007982740896425701, policy loss: 5.608683624826297
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0025],
        [0.0121],
        [0.3209],
        [0.0076]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0203, 0.0913, 0.3448, 0.0103, 0.0009, 0.3795]],

        [[0.0203, 0.0913, 0.3448, 0.0103, 0.0009, 0.3795]],

        [[0.0203, 0.0913, 0.3448, 0.0103, 0.0009, 0.3795]],

        [[0.0203, 0.0913, 0.3448, 0.0103, 0.0009, 0.3795]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0099, 0.0482, 1.2835, 0.0302], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0099, 0.0482, 1.2835, 0.0302])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.323
Iter 2/2000 - Loss: 0.467
Iter 3/2000 - Loss: 0.236
Iter 4/2000 - Loss: 0.201
Iter 5/2000 - Loss: 0.267
Iter 6/2000 - Loss: 0.204
Iter 7/2000 - Loss: 0.100
Iter 8/2000 - Loss: 0.055
Iter 9/2000 - Loss: 0.051
Iter 10/2000 - Loss: 0.005
Iter 11/2000 - Loss: -0.085
Iter 12/2000 - Loss: -0.174
Iter 13/2000 - Loss: -0.252
Iter 14/2000 - Loss: -0.345
Iter 15/2000 - Loss: -0.464
Iter 16/2000 - Loss: -0.595
Iter 17/2000 - Loss: -0.728
Iter 18/2000 - Loss: -0.870
Iter 19/2000 - Loss: -1.030
Iter 20/2000 - Loss: -1.205
Iter 1981/2000 - Loss: -7.341
Iter 1982/2000 - Loss: -7.341
Iter 1983/2000 - Loss: -7.341
Iter 1984/2000 - Loss: -7.341
Iter 1985/2000 - Loss: -7.341
Iter 1986/2000 - Loss: -7.341
Iter 1987/2000 - Loss: -7.341
Iter 1988/2000 - Loss: -7.341
Iter 1989/2000 - Loss: -7.341
Iter 1990/2000 - Loss: -7.341
Iter 1991/2000 - Loss: -7.341
Iter 1992/2000 - Loss: -7.341
Iter 1993/2000 - Loss: -7.341
Iter 1994/2000 - Loss: -7.341
Iter 1995/2000 - Loss: -7.341
Iter 1996/2000 - Loss: -7.341
Iter 1997/2000 - Loss: -7.341
Iter 1998/2000 - Loss: -7.341
Iter 1999/2000 - Loss: -7.341
Iter 2000/2000 - Loss: -7.341
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0005]])
Lengthscale: tensor([[[14.7937,  3.3974, 45.6056,  7.8265,  7.9939, 25.4069]],

        [[21.6662, 38.1572, 10.6882,  1.4960,  0.6252, 12.3732]],

        [[21.5757, 29.7461, 16.0610,  0.7057,  5.2716, 10.4801]],

        [[22.0707, 40.7081, 17.7584,  4.6407, 11.7947, 36.4924]]])
Signal Variance: tensor([0.0314, 0.5858, 6.5910, 0.5226])
Estimated target variance: tensor([0.0099, 0.0482, 1.2835, 0.0302])
N: 70
Signal to noise ratio: tensor([ 9.7543, 40.0953, 47.8654, 33.9325])
Bound on condition number: tensor([  6661.2429, 112535.2555, 160377.4819,  80600.2462])
Policy Optimizer learning rate:
9.936983085306159e-05
Experience 7, Iter 0, disc loss: 0.028921837714555785, policy loss: 4.264045161636122
Experience 7, Iter 1, disc loss: 0.02847700789411349, policy loss: 4.351515594313938
Experience 7, Iter 2, disc loss: 0.02394558713385753, policy loss: 4.587537278257612
Experience 7, Iter 3, disc loss: 0.02844767204457582, policy loss: 4.200552398364592
Experience 7, Iter 4, disc loss: 0.02618754169585592, policy loss: 4.4939595895069875
Experience 7, Iter 5, disc loss: 0.023560441018617484, policy loss: 4.660409727465511
Experience 7, Iter 6, disc loss: 0.024702289189162827, policy loss: 4.632275852627911
Experience 7, Iter 7, disc loss: 0.024975101934077197, policy loss: 4.5969360669483486
Experience 7, Iter 8, disc loss: 0.023626675416030286, policy loss: 4.6290986775666605
Experience 7, Iter 9, disc loss: 0.029804543108732102, policy loss: 4.53002547222526
Experience 7, Iter 10, disc loss: 0.026732127718653995, policy loss: 4.790630769940178
Experience 7, Iter 11, disc loss: 0.02611764490075215, policy loss: 4.862779272333914
Experience 7, Iter 12, disc loss: 0.0289499456389078, policy loss: 4.498336195635175
Experience 7, Iter 13, disc loss: 0.03263330981575073, policy loss: 4.630360820700783
Experience 7, Iter 14, disc loss: 0.0291537662221079, policy loss: 4.778915233628954
Experience 7, Iter 15, disc loss: 0.03144514631459547, policy loss: 4.675728531737793
Experience 7, Iter 16, disc loss: 0.030092790548966762, policy loss: 4.46714671262416
Experience 7, Iter 17, disc loss: 0.03215358097295411, policy loss: 4.462750285108168
Experience 7, Iter 18, disc loss: 0.031575855157096294, policy loss: 4.581801097748551
Experience 7, Iter 19, disc loss: 0.03173020218341344, policy loss: 4.70697116038704
Experience 7, Iter 20, disc loss: 0.039389592109797256, policy loss: 4.1710730134367555
Experience 7, Iter 21, disc loss: 0.04237234546252888, policy loss: 4.302332926945832
Experience 7, Iter 22, disc loss: 0.04043999702488885, policy loss: 4.143632821179175
Experience 7, Iter 23, disc loss: 0.04121659425240906, policy loss: 4.300555793233787
Experience 7, Iter 24, disc loss: 0.04278422367418387, policy loss: 4.336564220137475
Experience 7, Iter 25, disc loss: 0.046765870574127275, policy loss: 4.377051543739558
Experience 7, Iter 26, disc loss: 0.0460084441702429, policy loss: 4.583828637352511
Experience 7, Iter 27, disc loss: 0.047192010596262005, policy loss: 4.6466017267111335
Experience 7, Iter 28, disc loss: 0.04866821652298388, policy loss: 4.635826330727564
Experience 7, Iter 29, disc loss: 0.05512649355846825, policy loss: 4.708033567515696
Experience 7, Iter 30, disc loss: 0.050442366889511814, policy loss: 4.5365600523470455
Experience 7, Iter 31, disc loss: 0.04528206188634776, policy loss: 5.076733228540292
Experience 7, Iter 32, disc loss: 0.04646010881126032, policy loss: 5.008923288958499
Experience 7, Iter 33, disc loss: 0.053770341465108416, policy loss: 4.820501805309498
Experience 7, Iter 34, disc loss: 0.045538659996621184, policy loss: 5.178415560277481
Experience 7, Iter 35, disc loss: 0.051687191835540006, policy loss: 4.636865831117076
Experience 7, Iter 36, disc loss: 0.047861023440252504, policy loss: 4.960342387009429
Experience 7, Iter 37, disc loss: 0.06176147845485411, policy loss: 4.761821878537097
Experience 7, Iter 38, disc loss: 0.0559999481142252, policy loss: 5.07476578168744
Experience 7, Iter 39, disc loss: 0.04161487784662622, policy loss: 5.413283531169494
Experience 7, Iter 40, disc loss: 0.062006275647324506, policy loss: 5.017384873045876
Experience 7, Iter 41, disc loss: 0.06616428154923157, policy loss: 4.633716399794233
Experience 7, Iter 42, disc loss: 0.05686477131353966, policy loss: 5.500554842118053
Experience 7, Iter 43, disc loss: 0.06394112708594084, policy loss: 4.901756856850231
Experience 7, Iter 44, disc loss: 0.0480973772210955, policy loss: 5.5192677461428925
Experience 7, Iter 45, disc loss: 0.06393682357939107, policy loss: 5.466204065489558
Experience 7, Iter 46, disc loss: 0.058040265472985686, policy loss: 5.501943137028351
Experience 7, Iter 47, disc loss: 0.05161118457685946, policy loss: 5.280215216980908
Experience 7, Iter 48, disc loss: 0.06895132105431981, policy loss: 4.979532785050877
Experience 7, Iter 49, disc loss: 0.060986814709675985, policy loss: 4.620774053217214
Experience 7, Iter 50, disc loss: 0.05919728587098783, policy loss: 5.256403330605342
Experience 7, Iter 51, disc loss: 0.05411774014592029, policy loss: 5.002045374269631
Experience 7, Iter 52, disc loss: 0.06177250677209702, policy loss: 5.518886262766712
Experience 7, Iter 53, disc loss: 0.0526366246381047, policy loss: 5.461942293987526
Experience 7, Iter 54, disc loss: 0.07480888044957151, policy loss: 4.881254857667008
Experience 7, Iter 55, disc loss: 0.04944941358203815, policy loss: 5.3531537498402315
Experience 7, Iter 56, disc loss: 0.04752378708854675, policy loss: 5.782137186431296
Experience 7, Iter 57, disc loss: 0.04744847661307186, policy loss: 5.504540872605986
Experience 7, Iter 58, disc loss: 0.0552044894311392, policy loss: 4.704672716833418
Experience 7, Iter 59, disc loss: 0.042488104140506064, policy loss: 5.4968862278539135
Experience 7, Iter 60, disc loss: 0.049857667385081086, policy loss: 4.596251322699386
Experience 7, Iter 61, disc loss: 0.04232297012583566, policy loss: 5.058629298288704
Experience 7, Iter 62, disc loss: 0.0433383862942982, policy loss: 4.826695557827879
Experience 7, Iter 63, disc loss: 0.038925950610447374, policy loss: 5.228302499313687
Experience 7, Iter 64, disc loss: 0.038134355703167755, policy loss: 5.142930108336806
Experience 7, Iter 65, disc loss: 0.046475887147049644, policy loss: 4.750390510310045
Experience 7, Iter 66, disc loss: 0.03761915473264574, policy loss: 5.073741289563209
Experience 7, Iter 67, disc loss: 0.047079432956333225, policy loss: 4.945903410455022
Experience 7, Iter 68, disc loss: 0.033351610567107115, policy loss: 4.883694271852873
Experience 7, Iter 69, disc loss: 0.031697923813965495, policy loss: 4.999038612654144
Experience 7, Iter 70, disc loss: 0.0323574930832347, policy loss: 5.447474797723281
Experience 7, Iter 71, disc loss: 0.03237288863282563, policy loss: 5.115726960959435
Experience 7, Iter 72, disc loss: 0.02511060423291276, policy loss: 5.374354551024753
Experience 7, Iter 73, disc loss: 0.030552720077288736, policy loss: 5.176510682565818
Experience 7, Iter 74, disc loss: 0.035437171960765915, policy loss: 5.17184068910769
Experience 7, Iter 75, disc loss: 0.029893768176885688, policy loss: 4.946937673088035
Experience 7, Iter 76, disc loss: 0.03424592361502772, policy loss: 4.954824246726275
Experience 7, Iter 77, disc loss: 0.03435072053530153, policy loss: 5.073179576680218
Experience 7, Iter 78, disc loss: 0.029621281101709626, policy loss: 5.178612406371018
Experience 7, Iter 79, disc loss: 0.02363572296814756, policy loss: 5.172820239015255
Experience 7, Iter 80, disc loss: 0.029645023000226766, policy loss: 5.1287937756557795
Experience 7, Iter 81, disc loss: 0.02408844458925532, policy loss: 5.463813854314369
Experience 7, Iter 82, disc loss: 0.02150587246019763, policy loss: 5.591424834089838
Experience 7, Iter 83, disc loss: 0.027676989434327677, policy loss: 5.158768357509363
Experience 7, Iter 84, disc loss: 0.02108894178329179, policy loss: 5.548036913821877
Experience 7, Iter 85, disc loss: 0.023014685990082374, policy loss: 5.2287340149631785
Experience 7, Iter 86, disc loss: 0.022831465026971663, policy loss: 5.4370533424543765
Experience 7, Iter 87, disc loss: 0.023404481479931308, policy loss: 5.160334689260683
Experience 7, Iter 88, disc loss: 0.020991721390993222, policy loss: 5.410083446816785
Experience 7, Iter 89, disc loss: 0.021449536687120685, policy loss: 5.455988608120622
Experience 7, Iter 90, disc loss: 0.02289362918130463, policy loss: 5.576753992122988
Experience 7, Iter 91, disc loss: 0.025535611946592235, policy loss: 5.304083918717297
Experience 7, Iter 92, disc loss: 0.0261791292090792, policy loss: 4.769156700309319
Experience 7, Iter 93, disc loss: 0.023274006472789384, policy loss: 5.189791485090253
Experience 7, Iter 94, disc loss: 0.026111648366430956, policy loss: 5.34512343714563
Experience 7, Iter 95, disc loss: 0.025434887100247765, policy loss: 5.411876333335142
Experience 7, Iter 96, disc loss: 0.017993827493449514, policy loss: 5.948117378615696
Experience 7, Iter 97, disc loss: 0.02573520656786188, policy loss: 5.627336481305222
Experience 7, Iter 98, disc loss: 0.020671808615531504, policy loss: 5.869748359572101
Experience 7, Iter 99, disc loss: 0.02009797304586798, policy loss: 5.9450364848003
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0023],
        [0.0182],
        [0.3999],
        [0.0104]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0181, 0.0849, 0.4268, 0.0111, 0.0010, 0.4870]],

        [[0.0181, 0.0849, 0.4268, 0.0111, 0.0010, 0.4870]],

        [[0.0181, 0.0849, 0.4268, 0.0111, 0.0010, 0.4870]],

        [[0.0181, 0.0849, 0.4268, 0.0111, 0.0010, 0.4870]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0092, 0.0727, 1.5997, 0.0415], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0092, 0.0727, 1.5997, 0.0415])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.726
Iter 2/2000 - Loss: 0.942
Iter 3/2000 - Loss: 0.657
Iter 4/2000 - Loss: 0.644
Iter 5/2000 - Loss: 0.742
Iter 6/2000 - Loss: 0.674
Iter 7/2000 - Loss: 0.556
Iter 8/2000 - Loss: 0.520
Iter 9/2000 - Loss: 0.540
Iter 10/2000 - Loss: 0.511
Iter 11/2000 - Loss: 0.423
Iter 12/2000 - Loss: 0.334
Iter 13/2000 - Loss: 0.273
Iter 14/2000 - Loss: 0.209
Iter 15/2000 - Loss: 0.110
Iter 16/2000 - Loss: -0.011
Iter 17/2000 - Loss: -0.136
Iter 18/2000 - Loss: -0.260
Iter 19/2000 - Loss: -0.397
Iter 20/2000 - Loss: -0.555
Iter 1981/2000 - Loss: -7.395
Iter 1982/2000 - Loss: -7.395
Iter 1983/2000 - Loss: -7.395
Iter 1984/2000 - Loss: -7.396
Iter 1985/2000 - Loss: -7.396
Iter 1986/2000 - Loss: -7.396
Iter 1987/2000 - Loss: -7.396
Iter 1988/2000 - Loss: -7.396
Iter 1989/2000 - Loss: -7.396
Iter 1990/2000 - Loss: -7.396
Iter 1991/2000 - Loss: -7.396
Iter 1992/2000 - Loss: -7.396
Iter 1993/2000 - Loss: -7.396
Iter 1994/2000 - Loss: -7.396
Iter 1995/2000 - Loss: -7.396
Iter 1996/2000 - Loss: -7.396
Iter 1997/2000 - Loss: -7.396
Iter 1998/2000 - Loss: -7.396
Iter 1999/2000 - Loss: -7.396
Iter 2000/2000 - Loss: -7.396
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0027],
        [0.0004]])
Lengthscale: tensor([[[13.8444,  3.0183, 46.8168,  7.4410,  8.6427, 30.0636]],

        [[22.9302, 40.7641, 10.2523,  1.3184,  0.7880, 14.0571]],

        [[20.0987, 31.4786,  9.3673,  0.8744,  5.7003, 14.4420]],

        [[20.5009, 36.6742, 23.4675,  5.9670, 13.4365, 36.4486]]])
Signal Variance: tensor([0.0247, 0.7468, 9.4370, 0.8348])
Estimated target variance: tensor([0.0092, 0.0727, 1.5997, 0.0415])
N: 80
Signal to noise ratio: tensor([ 8.8484, 48.2757, 58.9318, 43.5016])
Bound on condition number: tensor([  6264.4753, 186444.1539, 277838.0175, 151392.4593])
Policy Optimizer learning rate:
9.926518942192229e-05
Experience 8, Iter 0, disc loss: 0.019319560309169023, policy loss: 5.274272205313666
Experience 8, Iter 1, disc loss: 0.02329119279823669, policy loss: 5.219383194824306
Experience 8, Iter 2, disc loss: 0.02107121353982208, policy loss: 5.2030844098512
Experience 8, Iter 3, disc loss: 0.020388903918300637, policy loss: 5.319989910315173
Experience 8, Iter 4, disc loss: 0.022272616558003097, policy loss: 5.164832157311691
Experience 8, Iter 5, disc loss: 0.024142365027635233, policy loss: 5.3851635025039615
Experience 8, Iter 6, disc loss: 0.019974047573095725, policy loss: 5.157448584008172
Experience 8, Iter 7, disc loss: 0.019404794496143178, policy loss: 5.331244335901652
Experience 8, Iter 8, disc loss: 0.01974852311409856, policy loss: 5.216314599642184
Experience 8, Iter 9, disc loss: 0.021755291656182493, policy loss: 5.259299339253166
Experience 8, Iter 10, disc loss: 0.02236915224963034, policy loss: 5.688697634606273
Experience 8, Iter 11, disc loss: 0.01695341997909545, policy loss: 5.818088187003643
Experience 8, Iter 12, disc loss: 0.02331933617967752, policy loss: 5.405879536149191
Experience 8, Iter 13, disc loss: 0.026536972436857195, policy loss: 5.37243612463711
Experience 8, Iter 14, disc loss: 0.01705813062249433, policy loss: 5.767906875548318
Experience 8, Iter 15, disc loss: 0.018302521874306338, policy loss: 5.631740151191902
Experience 8, Iter 16, disc loss: 0.022125446711080687, policy loss: 5.636257947135343
Experience 8, Iter 17, disc loss: 0.018211667555518618, policy loss: 5.80127164364388
Experience 8, Iter 18, disc loss: 0.019553840738647344, policy loss: 5.73025336746085
Experience 8, Iter 19, disc loss: 0.019611064946415832, policy loss: 5.68186899367762
Experience 8, Iter 20, disc loss: 0.026504507309737982, policy loss: 5.553390624124438
Experience 8, Iter 21, disc loss: 0.01782766722437428, policy loss: 5.993754098836209
Experience 8, Iter 22, disc loss: 0.017548074566944943, policy loss: 5.621078055429989
Experience 8, Iter 23, disc loss: 0.012605569091520438, policy loss: 6.4554210458459895
Experience 8, Iter 24, disc loss: 0.014294269471016322, policy loss: 6.351450253961465
Experience 8, Iter 25, disc loss: 0.0159036353857129, policy loss: 5.825050527435632
Experience 8, Iter 26, disc loss: 0.014440067670947033, policy loss: 5.929252531123492
Experience 8, Iter 27, disc loss: 0.014554337262256044, policy loss: 5.585291749110045
Experience 8, Iter 28, disc loss: 0.01710469644789465, policy loss: 5.72141030432564
Experience 8, Iter 29, disc loss: 0.020626426044366125, policy loss: 5.673576820858534
Experience 8, Iter 30, disc loss: 0.016949986831365598, policy loss: 5.381266256655838
Experience 8, Iter 31, disc loss: 0.01867852968407749, policy loss: 5.973097596299057
Experience 8, Iter 32, disc loss: 0.01677767719624973, policy loss: 5.657295542220515
Experience 8, Iter 33, disc loss: 0.011662723105803158, policy loss: 6.044465327795915
Experience 8, Iter 34, disc loss: 0.014680291764891659, policy loss: 5.9922197323382616
Experience 8, Iter 35, disc loss: 0.016879792448495988, policy loss: 6.159244313975443
Experience 8, Iter 36, disc loss: 0.014634620086660902, policy loss: 6.19572450125673
Experience 8, Iter 37, disc loss: 0.021352873559379933, policy loss: 6.1007018124207075
Experience 8, Iter 38, disc loss: 0.012583680517745884, policy loss: 6.494064556055937
Experience 8, Iter 39, disc loss: 0.015458830584840018, policy loss: 6.295267859711931
Experience 8, Iter 40, disc loss: 0.013309355200269532, policy loss: 6.539705899511739
Experience 8, Iter 41, disc loss: 0.010528478146405104, policy loss: 6.374645334606061
Experience 8, Iter 42, disc loss: 0.013158900041213825, policy loss: 5.981362187428643
Experience 8, Iter 43, disc loss: 0.012619345783468133, policy loss: 6.417936030672808
Experience 8, Iter 44, disc loss: 0.013610165510281911, policy loss: 6.140810609242538
Experience 8, Iter 45, disc loss: 0.00936039705305677, policy loss: 6.381615226494473
Experience 8, Iter 46, disc loss: 0.015567878323730152, policy loss: 6.256449953729609
Experience 8, Iter 47, disc loss: 0.013009443304280315, policy loss: 6.205897078797539
Experience 8, Iter 48, disc loss: 0.010603399838078065, policy loss: 6.272263512443162
Experience 8, Iter 49, disc loss: 0.017013196114629474, policy loss: 6.677171351055286
Experience 8, Iter 50, disc loss: 0.010761469615172804, policy loss: 6.4848861256439925
Experience 8, Iter 51, disc loss: 0.019959998498652685, policy loss: 5.933835053686577
Experience 8, Iter 52, disc loss: 0.009867903197661908, policy loss: 6.581047756987021
Experience 8, Iter 53, disc loss: 0.02341132412446069, policy loss: 6.115384147725635
Experience 8, Iter 54, disc loss: 0.009663595544927656, policy loss: 6.724293089088036
Experience 8, Iter 55, disc loss: 0.009630068445404712, policy loss: 6.894777832239496
Experience 8, Iter 56, disc loss: 0.009097324407710232, policy loss: 7.135557316168581
Experience 8, Iter 57, disc loss: 0.010242513944861267, policy loss: 6.823542184261653
Experience 8, Iter 58, disc loss: 0.01105069079188872, policy loss: 6.633404718260544
Experience 8, Iter 59, disc loss: 0.01403224883147324, policy loss: 6.665869199991306
Experience 8, Iter 60, disc loss: 0.01729474031101505, policy loss: 6.697087116939038
Experience 8, Iter 61, disc loss: 0.009731806602660143, policy loss: 6.891507258961855
Experience 8, Iter 62, disc loss: 0.02436988532699603, policy loss: 6.331245571742668
Experience 8, Iter 63, disc loss: 0.008927680394503307, policy loss: 6.931088107002801
Experience 8, Iter 64, disc loss: 0.009865431711894029, policy loss: 6.773871615090622
Experience 8, Iter 65, disc loss: 0.008844154334360702, policy loss: 6.875870848974684
Experience 8, Iter 66, disc loss: 0.008192046240866495, policy loss: 7.293716252372066
Experience 8, Iter 67, disc loss: 0.011392378716605839, policy loss: 6.38838787271614
Experience 8, Iter 68, disc loss: 0.010833584831406444, policy loss: 6.34112402812802
Experience 8, Iter 69, disc loss: 0.0074138370836675325, policy loss: 7.148298319343235
Experience 8, Iter 70, disc loss: 0.009442504881968211, policy loss: 6.83391042910622
Experience 8, Iter 71, disc loss: 0.01013610497587988, policy loss: 6.706427771527677
Experience 8, Iter 72, disc loss: 0.008803562288057116, policy loss: 6.763876226385493
Experience 8, Iter 73, disc loss: 0.010675959556195576, policy loss: 6.56614515314977
Experience 8, Iter 74, disc loss: 0.008228427102359454, policy loss: 6.777990987391764
Experience 8, Iter 75, disc loss: 0.012336322349582308, policy loss: 6.252199879616387
Experience 8, Iter 76, disc loss: 0.008540939304176832, policy loss: 7.173327545332901
Experience 8, Iter 77, disc loss: 0.01802160535219949, policy loss: 7.717713398114464
Experience 8, Iter 78, disc loss: 0.019417516719492444, policy loss: 7.035644432089198
Experience 8, Iter 79, disc loss: 0.0086874455696094, policy loss: 6.800744100530687
Experience 8, Iter 80, disc loss: 0.007788259495068702, policy loss: 7.39972347439695
Experience 8, Iter 81, disc loss: 0.011055303005720893, policy loss: 7.154345741711122
Experience 8, Iter 82, disc loss: 0.007549932790697215, policy loss: 7.892545502857063
Experience 8, Iter 83, disc loss: 0.007366389957422647, policy loss: 7.545801775467881
Experience 8, Iter 84, disc loss: 0.010109395716549115, policy loss: 6.825213373306921
Experience 8, Iter 85, disc loss: 0.011605947261811456, policy loss: 7.317336125332959
Experience 8, Iter 86, disc loss: 0.007711918552628598, policy loss: 7.079655785866393
Experience 8, Iter 87, disc loss: 0.00771797306567959, policy loss: 7.31629287117571
Experience 8, Iter 88, disc loss: 0.011407532538436401, policy loss: 7.292292072912781
Experience 8, Iter 89, disc loss: 0.009420976117491004, policy loss: 7.243970525494021
Experience 8, Iter 90, disc loss: 0.008825944156629944, policy loss: 8.009290318193113
Experience 8, Iter 91, disc loss: 0.0071219359354092755, policy loss: 7.215886946678974
Experience 8, Iter 92, disc loss: 0.008245618393299391, policy loss: 7.208413550437921
Experience 8, Iter 93, disc loss: 0.007813971599555738, policy loss: 7.234241515367318
Experience 8, Iter 94, disc loss: 0.007956640251368005, policy loss: 7.268545820767239
Experience 8, Iter 95, disc loss: 0.006463110811878607, policy loss: 7.586334233099546
Experience 8, Iter 96, disc loss: 0.0061624685658407214, policy loss: 7.157680623528657
Experience 8, Iter 97, disc loss: 0.007470271433530881, policy loss: 6.876820698793822
Experience 8, Iter 98, disc loss: 0.00598964282058018, policy loss: 7.238476184578476
Experience 8, Iter 99, disc loss: 0.007296243837928537, policy loss: 7.483776154500429
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0027],
        [0.0269],
        [0.5391],
        [0.0134]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0170, 0.1056, 0.5631, 0.0144, 0.0060, 0.8670]],

        [[0.0170, 0.1056, 0.5631, 0.0144, 0.0060, 0.8670]],

        [[0.0170, 0.1056, 0.5631, 0.0144, 0.0060, 0.8670]],

        [[0.0170, 0.1056, 0.5631, 0.0144, 0.0060, 0.8670]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0109, 0.1075, 2.1564, 0.0536], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0109, 0.1075, 2.1564, 0.0536])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.296
Iter 2/2000 - Loss: 1.347
Iter 3/2000 - Loss: 1.197
Iter 4/2000 - Loss: 1.150
Iter 5/2000 - Loss: 1.184
Iter 6/2000 - Loss: 1.125
Iter 7/2000 - Loss: 1.040
Iter 8/2000 - Loss: 1.003
Iter 9/2000 - Loss: 0.978
Iter 10/2000 - Loss: 0.904
Iter 11/2000 - Loss: 0.802
Iter 12/2000 - Loss: 0.714
Iter 13/2000 - Loss: 0.633
Iter 14/2000 - Loss: 0.529
Iter 15/2000 - Loss: 0.392
Iter 16/2000 - Loss: 0.239
Iter 17/2000 - Loss: 0.084
Iter 18/2000 - Loss: -0.075
Iter 19/2000 - Loss: -0.248
Iter 20/2000 - Loss: -0.439
Iter 1981/2000 - Loss: -7.167
Iter 1982/2000 - Loss: -7.167
Iter 1983/2000 - Loss: -7.167
Iter 1984/2000 - Loss: -7.168
Iter 1985/2000 - Loss: -7.168
Iter 1986/2000 - Loss: -7.168
Iter 1987/2000 - Loss: -7.168
Iter 1988/2000 - Loss: -7.168
Iter 1989/2000 - Loss: -7.168
Iter 1990/2000 - Loss: -7.168
Iter 1991/2000 - Loss: -7.168
Iter 1992/2000 - Loss: -7.168
Iter 1993/2000 - Loss: -7.168
Iter 1994/2000 - Loss: -7.168
Iter 1995/2000 - Loss: -7.168
Iter 1996/2000 - Loss: -7.168
Iter 1997/2000 - Loss: -7.168
Iter 1998/2000 - Loss: -7.168
Iter 1999/2000 - Loss: -7.168
Iter 2000/2000 - Loss: -7.168
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0026],
        [0.0004]])
Lengthscale: tensor([[[13.1291,  3.8749, 41.8898,  8.7524, 12.1951, 32.5271]],

        [[21.5011, 37.8543,  9.9536,  0.9675,  1.7822, 15.8131]],

        [[19.4026, 31.2079,  9.6372,  0.8395,  1.4806, 16.2944]],

        [[18.6949, 33.3709, 26.5479,  6.5982,  4.8513, 38.1058]]])
Signal Variance: tensor([ 0.0342,  0.9033, 10.4331,  1.0169])
Estimated target variance: tensor([0.0109, 0.1075, 2.1564, 0.0536])
N: 90
Signal to noise ratio: tensor([10.2488, 53.9009, 63.3857, 49.8429])
Bound on condition number: tensor([  9454.3651, 261478.8466, 361597.7932, 223589.0713])
Policy Optimizer learning rate:
9.916065818347443e-05
Experience 9, Iter 0, disc loss: 0.006326964463072722, policy loss: 6.103762412550033
Experience 9, Iter 1, disc loss: 0.006180574130906195, policy loss: 6.198588786340481
Experience 9, Iter 2, disc loss: 0.0066382094480618905, policy loss: 6.127526545526306
Experience 9, Iter 3, disc loss: 0.005591172438678755, policy loss: 6.199411637691329
Experience 9, Iter 4, disc loss: 0.005237176296462332, policy loss: 6.586830942195922
Experience 9, Iter 5, disc loss: 0.006190733273545312, policy loss: 6.021593494251242
Experience 9, Iter 6, disc loss: 0.005364263626803317, policy loss: 6.559840904979899
Experience 9, Iter 7, disc loss: 0.006200940728302344, policy loss: 6.063923951630688
Experience 9, Iter 8, disc loss: 0.005641519935679434, policy loss: 6.140943013907387
Experience 9, Iter 9, disc loss: 0.006596788410157197, policy loss: 6.451711431810127
Experience 9, Iter 10, disc loss: 0.005566166746595665, policy loss: 6.329066151470331
Experience 9, Iter 11, disc loss: 0.005558364631585018, policy loss: 6.303487335907784
Experience 9, Iter 12, disc loss: 0.0051659169722711815, policy loss: 6.323208579011772
Experience 9, Iter 13, disc loss: 0.004963702541824027, policy loss: 7.029296903545901
Experience 9, Iter 14, disc loss: 0.00529993253270632, policy loss: 6.464048502576833
Experience 9, Iter 15, disc loss: 0.005475498832132764, policy loss: 6.293023762945513
Experience 9, Iter 16, disc loss: 0.005235281826541252, policy loss: 6.553369757627155
Experience 9, Iter 17, disc loss: 0.004849552127161141, policy loss: 6.83845251646097
Experience 9, Iter 18, disc loss: 0.00591358576244316, policy loss: 6.191361486746328
Experience 9, Iter 19, disc loss: 0.004466091237988114, policy loss: 6.985301517489844
Experience 9, Iter 20, disc loss: 0.004524778410775147, policy loss: 6.50150861317114
Experience 9, Iter 21, disc loss: 0.005309615917721304, policy loss: 6.32496447265707
Experience 9, Iter 22, disc loss: 0.004360884931427549, policy loss: 6.997290416278057
Experience 9, Iter 23, disc loss: 0.0051180383871195145, policy loss: 6.350179420407009
Experience 9, Iter 24, disc loss: 0.004628166452374358, policy loss: 6.775222209386904
Experience 9, Iter 25, disc loss: 0.003792521876207057, policy loss: 7.18929144132075
Experience 9, Iter 26, disc loss: 0.004201714815870014, policy loss: 6.974804534186919
Experience 9, Iter 27, disc loss: 0.004721776410054461, policy loss: 6.878451810829466
Experience 9, Iter 28, disc loss: 0.0046717091011629535, policy loss: 6.556164724034527
Experience 9, Iter 29, disc loss: 0.0035977260101869, policy loss: 7.043936973668968
Experience 9, Iter 30, disc loss: 0.0047618320721866, policy loss: 6.9834173960801795
Experience 9, Iter 31, disc loss: 0.004473793227805636, policy loss: 6.9163546118722055
Experience 9, Iter 32, disc loss: 0.0037849731094476615, policy loss: 7.038458148871786
Experience 9, Iter 33, disc loss: 0.003499309442272269, policy loss: 7.2890220446004665
Experience 9, Iter 34, disc loss: 0.003989234608982029, policy loss: 6.747786810711925
Experience 9, Iter 35, disc loss: 0.003967551757485605, policy loss: 7.1476353878440015
Experience 9, Iter 36, disc loss: 0.003984717058393398, policy loss: 7.021324528199143
Experience 9, Iter 37, disc loss: 0.0037641184203706154, policy loss: 6.750997871213154
Experience 9, Iter 38, disc loss: 0.0034697063707807454, policy loss: 6.987350249384876
Experience 9, Iter 39, disc loss: 0.003497773527055466, policy loss: 7.1277746917573905
Experience 9, Iter 40, disc loss: 0.004487075649405876, policy loss: 6.687356018756521
Experience 9, Iter 41, disc loss: 0.0035021669182962126, policy loss: 7.09648881982537
Experience 9, Iter 42, disc loss: 0.0036185109094399705, policy loss: 7.002075489082118
Experience 9, Iter 43, disc loss: 0.003173079063333114, policy loss: 6.878357607471319
Experience 9, Iter 44, disc loss: 0.003696165846480811, policy loss: 7.160788237120199
Experience 9, Iter 45, disc loss: 0.0035782489431254857, policy loss: 7.032365011612718
Experience 9, Iter 46, disc loss: 0.005198842246376201, policy loss: 6.917892566451288
Experience 9, Iter 47, disc loss: 0.0033181717881412874, policy loss: 7.347535767694309
Experience 9, Iter 48, disc loss: 0.003188752119559765, policy loss: 7.3144653827492565
Experience 9, Iter 49, disc loss: 0.002924738883048156, policy loss: 7.121132039952435
Experience 9, Iter 50, disc loss: 0.0035109711421092557, policy loss: 7.068404364847096
Experience 9, Iter 51, disc loss: 0.0035803274615991846, policy loss: 7.258681247470463
Experience 9, Iter 52, disc loss: 0.002971639209429994, policy loss: 7.31677279844067
Experience 9, Iter 53, disc loss: 0.003091416059480951, policy loss: 7.40398034743845
Experience 9, Iter 54, disc loss: 0.002831523483083729, policy loss: 7.485757220559778
Experience 9, Iter 55, disc loss: 0.0033221898446762783, policy loss: 7.39584974812707
Experience 9, Iter 56, disc loss: 0.003283636569387113, policy loss: 7.2076926198037805
Experience 9, Iter 57, disc loss: 0.002884296816872052, policy loss: 7.588863515325884
Experience 9, Iter 58, disc loss: 0.0031274222505753876, policy loss: 7.28721520906527
Experience 9, Iter 59, disc loss: 0.0033531710697015348, policy loss: 7.334035855307555
Experience 9, Iter 60, disc loss: 0.0025997402717317653, policy loss: 7.462034914984921
Experience 9, Iter 61, disc loss: 0.0028234495450096993, policy loss: 7.328506190921613
Experience 9, Iter 62, disc loss: 0.002829465028295371, policy loss: 7.3243300189533
Experience 9, Iter 63, disc loss: 0.0025419679597372324, policy loss: 7.868760877835754
Experience 9, Iter 64, disc loss: 0.002967279861679298, policy loss: 7.3234921277982865
Experience 9, Iter 65, disc loss: 0.002466043022167995, policy loss: 7.744156076211633
Experience 9, Iter 66, disc loss: 0.002458173788831224, policy loss: 7.862272993431051
Experience 9, Iter 67, disc loss: 0.0028951109865741464, policy loss: 7.446838375137853
Experience 9, Iter 68, disc loss: 0.0024970431663685053, policy loss: 7.788830564785769
Experience 9, Iter 69, disc loss: 0.002549275197813063, policy loss: 7.3470899751323495
Experience 9, Iter 70, disc loss: 0.0029526369318034506, policy loss: 7.2711673933879295
Experience 9, Iter 71, disc loss: 0.0030949590139787704, policy loss: 7.18850853297991
Experience 9, Iter 72, disc loss: 0.00251818486199304, policy loss: 7.648390738940254
Experience 9, Iter 73, disc loss: 0.0025068309062825323, policy loss: 7.356185828632605
Experience 9, Iter 74, disc loss: 0.0027330784767356283, policy loss: 7.297674876976918
Experience 9, Iter 75, disc loss: 0.0030637258552192142, policy loss: 7.37421903139048
Experience 9, Iter 76, disc loss: 0.003508809926443869, policy loss: 7.410621017645391
Experience 9, Iter 77, disc loss: 0.0023097759054820713, policy loss: 7.6220185269955145
Experience 9, Iter 78, disc loss: 0.0024440207601404617, policy loss: 7.876446088052191
Experience 9, Iter 79, disc loss: 0.002647432083833851, policy loss: 7.6002172315059475
Experience 9, Iter 80, disc loss: 0.002668068668781826, policy loss: 7.447356267728843
Experience 9, Iter 81, disc loss: 0.002522243226468124, policy loss: 7.4823040964456
Experience 9, Iter 82, disc loss: 0.002361927017952626, policy loss: 7.484077933951945
Experience 9, Iter 83, disc loss: 0.002276697393348387, policy loss: 8.01291422098496
Experience 9, Iter 84, disc loss: 0.002744118966563262, policy loss: 7.526147057661872
Experience 9, Iter 85, disc loss: 0.0023802490198795976, policy loss: 7.384560928578267
Experience 9, Iter 86, disc loss: 0.002233526975915992, policy loss: 7.876083631380899
Experience 9, Iter 87, disc loss: 0.002747463522782721, policy loss: 7.450214428724314
Experience 9, Iter 88, disc loss: 0.003021391254609283, policy loss: 7.1613773512577
Experience 9, Iter 89, disc loss: 0.0021916627443924544, policy loss: 8.182714549740775
Experience 9, Iter 90, disc loss: 0.0030361525494803088, policy loss: 7.761736566262282
Experience 9, Iter 91, disc loss: 0.0022767260153352016, policy loss: 7.9612597986384435
Experience 9, Iter 92, disc loss: 0.0021469571472433604, policy loss: 8.370560864901032
Experience 9, Iter 93, disc loss: 0.0020063114992023923, policy loss: 7.893124406884855
Experience 9, Iter 94, disc loss: 0.002436464449564339, policy loss: 7.641519704651435
Experience 9, Iter 95, disc loss: 0.0020774189152526564, policy loss: 7.877558168717129
Experience 9, Iter 96, disc loss: 0.002254683432400811, policy loss: 7.785300343821026
Experience 9, Iter 97, disc loss: 0.0020653110886965534, policy loss: 7.693630934391405
Experience 9, Iter 98, disc loss: 0.002159130741410096, policy loss: 8.19625980769546
Experience 9, Iter 99, disc loss: 0.0016856583492679818, policy loss: 8.22273368387648
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0032],
        [0.0391],
        [0.7413],
        [0.0178]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0163, 0.1253, 0.7709, 0.0169, 0.0106, 1.2510]],

        [[0.0163, 0.1253, 0.7709, 0.0169, 0.0106, 1.2510]],

        [[0.0163, 0.1253, 0.7709, 0.0169, 0.0106, 1.2510]],

        [[0.0163, 0.1253, 0.7709, 0.0169, 0.0106, 1.2510]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0128, 0.1565, 2.9653, 0.0713], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0128, 0.1565, 2.9653, 0.0713])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.872
Iter 2/2000 - Loss: 1.854
Iter 3/2000 - Loss: 1.754
Iter 4/2000 - Loss: 1.701
Iter 5/2000 - Loss: 1.722
Iter 6/2000 - Loss: 1.665
Iter 7/2000 - Loss: 1.585
Iter 8/2000 - Loss: 1.548
Iter 9/2000 - Loss: 1.515
Iter 10/2000 - Loss: 1.436
Iter 11/2000 - Loss: 1.333
Iter 12/2000 - Loss: 1.237
Iter 13/2000 - Loss: 1.146
Iter 14/2000 - Loss: 1.033
Iter 15/2000 - Loss: 0.891
Iter 16/2000 - Loss: 0.733
Iter 17/2000 - Loss: 0.570
Iter 18/2000 - Loss: 0.402
Iter 19/2000 - Loss: 0.220
Iter 20/2000 - Loss: 0.019
Iter 1981/2000 - Loss: -7.070
Iter 1982/2000 - Loss: -7.070
Iter 1983/2000 - Loss: -7.070
Iter 1984/2000 - Loss: -7.070
Iter 1985/2000 - Loss: -7.070
Iter 1986/2000 - Loss: -7.070
Iter 1987/2000 - Loss: -7.070
Iter 1988/2000 - Loss: -7.070
Iter 1989/2000 - Loss: -7.070
Iter 1990/2000 - Loss: -7.070
Iter 1991/2000 - Loss: -7.070
Iter 1992/2000 - Loss: -7.071
Iter 1993/2000 - Loss: -7.071
Iter 1994/2000 - Loss: -7.071
Iter 1995/2000 - Loss: -7.071
Iter 1996/2000 - Loss: -7.071
Iter 1997/2000 - Loss: -7.071
Iter 1998/2000 - Loss: -7.071
Iter 1999/2000 - Loss: -7.071
Iter 2000/2000 - Loss: -7.071
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0025],
        [0.0004]])
Lengthscale: tensor([[[12.9784,  4.9153, 37.9966,  9.6210, 12.3061, 39.7180]],

        [[19.5766, 34.4184,  8.2526,  1.0190,  1.4429, 12.4982]],

        [[19.1633, 32.5355,  9.5651,  0.8801,  1.7007, 16.0965]],

        [[16.7592, 33.4263, 28.4246,  4.3388,  2.0550, 50.1199]]])
Signal Variance: tensor([0.0538, 0.6173, 9.8495, 0.9771])
Estimated target variance: tensor([0.0128, 0.1565, 2.9653, 0.0713])
N: 100
Signal to noise ratio: tensor([13.1280, 43.9985, 62.2408, 49.9472])
Bound on condition number: tensor([ 17235.3415, 193587.7987, 387392.8303, 249472.8281])
Policy Optimizer learning rate:
9.905623702167958e-05
Experience 10, Iter 0, disc loss: 0.0018067539196047321, policy loss: 7.837600989296865
Experience 10, Iter 1, disc loss: 0.001872773395124185, policy loss: 7.924815538601857
Experience 10, Iter 2, disc loss: 0.0017853352809718676, policy loss: 7.998779321577199
Experience 10, Iter 3, disc loss: 0.0016114106676511978, policy loss: 8.218034827878636
Experience 10, Iter 4, disc loss: 0.00171757723569609, policy loss: 8.05619215378124
Experience 10, Iter 5, disc loss: 0.0016529367328860546, policy loss: 7.821868937278925
Experience 10, Iter 6, disc loss: 0.001804972267274553, policy loss: 7.490453426857911
Experience 10, Iter 7, disc loss: 0.001770745136364837, policy loss: 7.463762285469663
Experience 10, Iter 8, disc loss: 0.0018020461635436331, policy loss: 7.427257865273782
Experience 10, Iter 9, disc loss: 0.0017125482167682957, policy loss: 7.739406081973236
Experience 10, Iter 10, disc loss: 0.0015927058071990767, policy loss: 7.759205354219422
Experience 10, Iter 11, disc loss: 0.001681553351463737, policy loss: 7.6723116657782375
Experience 10, Iter 12, disc loss: 0.0017299605650941052, policy loss: 7.795695866741536
Experience 10, Iter 13, disc loss: 0.0016741811043205392, policy loss: 7.557477191509291
Experience 10, Iter 14, disc loss: 0.0016201006715743872, policy loss: 7.721811350521333
Experience 10, Iter 15, disc loss: 0.0017207911117756542, policy loss: 7.461161554810099
Experience 10, Iter 16, disc loss: 0.0016061028205818957, policy loss: 7.982290988978715
Experience 10, Iter 17, disc loss: 0.0017749654681879785, policy loss: 7.426429487736289
Experience 10, Iter 18, disc loss: 0.0015965080539384257, policy loss: 7.524884357918085
Experience 10, Iter 19, disc loss: 0.0015752712731557541, policy loss: 7.564236405486607
Experience 10, Iter 20, disc loss: 0.0017168760932964409, policy loss: 7.548752543341243
Experience 10, Iter 21, disc loss: 0.001433224236069283, policy loss: 8.100003566243727
Experience 10, Iter 22, disc loss: 0.0015351493690014873, policy loss: 7.584282868689969
Experience 10, Iter 23, disc loss: 0.0016736702781350197, policy loss: 7.787925589297247
Experience 10, Iter 24, disc loss: 0.0016986118286405959, policy loss: 7.565804862369311
Experience 10, Iter 25, disc loss: 0.0015663931840039645, policy loss: 7.550965595935304
Experience 10, Iter 26, disc loss: 0.001472250717247931, policy loss: 7.765463553161116
Experience 10, Iter 27, disc loss: 0.0017340067120181389, policy loss: 7.520463793765549
Experience 10, Iter 28, disc loss: 0.0016673795266789677, policy loss: 7.562164693853868
Experience 10, Iter 29, disc loss: 0.0016677750425531093, policy loss: 7.779561303453015
Experience 10, Iter 30, disc loss: 0.0016611964265116892, policy loss: 7.403976705765438
Experience 10, Iter 31, disc loss: 0.0017362474961532739, policy loss: 7.532744471866929
Experience 10, Iter 32, disc loss: 0.0017438680473934765, policy loss: 7.78237062513249
Experience 10, Iter 33, disc loss: 0.0016033249659562847, policy loss: 7.804973907534938
Experience 10, Iter 34, disc loss: 0.0015257184003224303, policy loss: 8.225557434994986
Experience 10, Iter 35, disc loss: 0.0015065914306607868, policy loss: 7.892984406274884
Experience 10, Iter 36, disc loss: 0.0013576150650870108, policy loss: 8.015048697717178
Experience 10, Iter 37, disc loss: 0.0015756909197286726, policy loss: 7.757266898567626
Experience 10, Iter 38, disc loss: 0.0016353958043875845, policy loss: 7.706102504502406
Experience 10, Iter 39, disc loss: 0.0016099404185771445, policy loss: 7.691027520049269
Experience 10, Iter 40, disc loss: 0.0014891640059550046, policy loss: 7.8567406086639515
Experience 10, Iter 41, disc loss: 0.0015404977843490528, policy loss: 7.8121884160608746
Experience 10, Iter 42, disc loss: 0.0013972058257065174, policy loss: 8.131199739346087
Experience 10, Iter 43, disc loss: 0.0015986595308881547, policy loss: 7.913635452230734
Experience 10, Iter 44, disc loss: 0.0018465856406861192, policy loss: 7.350124555103194
Experience 10, Iter 45, disc loss: 0.0014399290240796913, policy loss: 7.795816855323525
Experience 10, Iter 46, disc loss: 0.0015625643763293477, policy loss: 7.714549117863981
Experience 10, Iter 47, disc loss: 0.0014857724726592583, policy loss: 8.037331772957842
Experience 10, Iter 48, disc loss: 0.0015495850380237456, policy loss: 7.960337993281374
Experience 10, Iter 49, disc loss: 0.0014999905253163326, policy loss: 7.79862328255155
Experience 10, Iter 50, disc loss: 0.0014497312378015195, policy loss: 7.909001412651692
Experience 10, Iter 51, disc loss: 0.0015088057992508571, policy loss: 8.023932737633464
Experience 10, Iter 52, disc loss: 0.0015302343850039308, policy loss: 7.731903487149738
Experience 10, Iter 53, disc loss: 0.001370828161188804, policy loss: 8.024927391358322
Experience 10, Iter 54, disc loss: 0.001495697876537337, policy loss: 7.67650047883747
Experience 10, Iter 55, disc loss: 0.0015066717709241897, policy loss: 7.667526516132668
Experience 10, Iter 56, disc loss: 0.0015716538988919235, policy loss: 7.5559949862402025
Experience 10, Iter 57, disc loss: 0.0013864350645339462, policy loss: 7.931639588712456
Experience 10, Iter 58, disc loss: 0.0014307276989856776, policy loss: 7.830523270161466
Experience 10, Iter 59, disc loss: 0.0013970050773761292, policy loss: 8.086896572498894
Experience 10, Iter 60, disc loss: 0.0013956364990021807, policy loss: 7.988594270587228
Experience 10, Iter 61, disc loss: 0.0012626394003561472, policy loss: 8.203248702428462
Experience 10, Iter 62, disc loss: 0.0012580067503655515, policy loss: 8.201851350555422
Experience 10, Iter 63, disc loss: 0.0013939645755961902, policy loss: 8.08714037466726
Experience 10, Iter 64, disc loss: 0.0015044866926864423, policy loss: 8.139525297054423
Experience 10, Iter 65, disc loss: 0.0013242913795816604, policy loss: 7.883488632939323
Experience 10, Iter 66, disc loss: 0.0014223254316809457, policy loss: 7.667764635683126
Experience 10, Iter 67, disc loss: 0.0012698369313527268, policy loss: 7.944177085142209
Experience 10, Iter 68, disc loss: 0.0013247446798550944, policy loss: 7.9733807714799285
Experience 10, Iter 69, disc loss: 0.0013429770792999342, policy loss: 8.13757685590824
Experience 10, Iter 70, disc loss: 0.001367871263543112, policy loss: 7.836178053105513
Experience 10, Iter 71, disc loss: 0.00131039703665792, policy loss: 7.924372273869045
Experience 10, Iter 72, disc loss: 0.0013470345459694832, policy loss: 8.047457044762693
Experience 10, Iter 73, disc loss: 0.0012958263250393044, policy loss: 7.819981434504509
Experience 10, Iter 74, disc loss: 0.0011662113772574131, policy loss: 8.27014165738236
Experience 10, Iter 75, disc loss: 0.0013716931521853062, policy loss: 7.629584486359432
Experience 10, Iter 76, disc loss: 0.0012776045464588832, policy loss: 8.196916623910466
Experience 10, Iter 77, disc loss: 0.0012619067589330127, policy loss: 7.904314719149574
Experience 10, Iter 78, disc loss: 0.0011999962542406884, policy loss: 8.159507531134079
Experience 10, Iter 79, disc loss: 0.0012283772754870786, policy loss: 8.008535346773806
Experience 10, Iter 80, disc loss: 0.00126239440191517, policy loss: 8.083969277046434
Experience 10, Iter 81, disc loss: 0.001373042871216128, policy loss: 8.04830748246988
Experience 10, Iter 82, disc loss: 0.0013291716435355073, policy loss: 8.063824936582632
Experience 10, Iter 83, disc loss: 0.0014011357337552766, policy loss: 7.781008294575065
Experience 10, Iter 84, disc loss: 0.0012747171740298613, policy loss: 8.283997085129847
Experience 10, Iter 85, disc loss: 0.0011789040239983386, policy loss: 8.439449762447376
Experience 10, Iter 86, disc loss: 0.001304115208835751, policy loss: 7.945137603887316
Experience 10, Iter 87, disc loss: 0.0012508184583907328, policy loss: 8.075634553201112
Experience 10, Iter 88, disc loss: 0.0013177185758515503, policy loss: 7.869018254980976
Experience 10, Iter 89, disc loss: 0.0012999674697279475, policy loss: 8.192422811780817
Experience 10, Iter 90, disc loss: 0.0013077235757503413, policy loss: 7.800179260451484
Experience 10, Iter 91, disc loss: 0.0012750153139644447, policy loss: 7.817489842563756
Experience 10, Iter 92, disc loss: 0.0011918832411998673, policy loss: 8.222196714632158
Experience 10, Iter 93, disc loss: 0.0012620586499563474, policy loss: 8.249253694101366
Experience 10, Iter 94, disc loss: 0.0011166749660068294, policy loss: 8.209917830372436
Experience 10, Iter 95, disc loss: 0.0012545560805408559, policy loss: 8.128954343112461
Experience 10, Iter 96, disc loss: 0.0013411152340692808, policy loss: 8.004421087503976
Experience 10, Iter 97, disc loss: 0.0011729115996563645, policy loss: 8.449035512703206
Experience 10, Iter 98, disc loss: 0.0012359366012674243, policy loss: 7.82137209024337
Experience 10, Iter 99, disc loss: 0.001292579153809613, policy loss: 8.189601210435974
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0036],
        [0.0516],
        [0.9384],
        [0.0224]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0161, 0.1431, 0.9934, 0.0193, 0.0143, 1.5814]],

        [[0.0161, 0.1431, 0.9934, 0.0193, 0.0143, 1.5814]],

        [[0.0161, 0.1431, 0.9934, 0.0193, 0.0143, 1.5814]],

        [[0.0161, 0.1431, 0.9934, 0.0193, 0.0143, 1.5814]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0146, 0.2065, 3.7537, 0.0897], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0146, 0.2065, 3.7537, 0.0897])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.311
Iter 2/2000 - Loss: 2.258
Iter 3/2000 - Loss: 2.186
Iter 4/2000 - Loss: 2.127
Iter 5/2000 - Loss: 2.140
Iter 6/2000 - Loss: 2.096
Iter 7/2000 - Loss: 2.015
Iter 8/2000 - Loss: 1.965
Iter 9/2000 - Loss: 1.928
Iter 10/2000 - Loss: 1.857
Iter 11/2000 - Loss: 1.756
Iter 12/2000 - Loss: 1.651
Iter 13/2000 - Loss: 1.544
Iter 14/2000 - Loss: 1.421
Iter 15/2000 - Loss: 1.270
Iter 16/2000 - Loss: 1.099
Iter 17/2000 - Loss: 0.914
Iter 18/2000 - Loss: 0.717
Iter 19/2000 - Loss: 0.500
Iter 20/2000 - Loss: 0.262
Iter 1981/2000 - Loss: -6.898
Iter 1982/2000 - Loss: -6.898
Iter 1983/2000 - Loss: -6.898
Iter 1984/2000 - Loss: -6.899
Iter 1985/2000 - Loss: -6.899
Iter 1986/2000 - Loss: -6.899
Iter 1987/2000 - Loss: -6.899
Iter 1988/2000 - Loss: -6.899
Iter 1989/2000 - Loss: -6.899
Iter 1990/2000 - Loss: -6.899
Iter 1991/2000 - Loss: -6.899
Iter 1992/2000 - Loss: -6.899
Iter 1993/2000 - Loss: -6.899
Iter 1994/2000 - Loss: -6.899
Iter 1995/2000 - Loss: -6.899
Iter 1996/2000 - Loss: -6.899
Iter 1997/2000 - Loss: -6.899
Iter 1998/2000 - Loss: -6.899
Iter 1999/2000 - Loss: -6.899
Iter 2000/2000 - Loss: -6.899
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0029],
        [0.0004]])
Lengthscale: tensor([[[12.3563,  3.5796, 29.8620,  6.4258,  5.1051, 37.3295]],

        [[18.2686, 34.0018,  7.1890,  1.5496,  0.8520, 11.7564]],

        [[18.6383, 32.5970,  8.6084,  1.0393,  1.0584, 15.7803]],

        [[15.9881, 34.7567, 28.1257,  4.0216,  1.9340, 49.5743]]])
Signal Variance: tensor([ 0.0405,  0.6842, 10.0726,  1.1284])
Estimated target variance: tensor([0.0146, 0.2065, 3.7537, 0.0897])
N: 110
Signal to noise ratio: tensor([11.4030, 45.1153, 58.4591, 55.8665])
Bound on condition number: tensor([ 14304.1240, 223893.8793, 375922.6706, 343317.9188])
Policy Optimizer learning rate:
9.895192582062148e-05
Experience 11, Iter 0, disc loss: 0.0023606214580401664, policy loss: 6.929463669920985
Experience 11, Iter 1, disc loss: 0.0022544158567100406, policy loss: 6.8235149289124255
Experience 11, Iter 2, disc loss: 0.0020840895044988306, policy loss: 6.933324789645705
Experience 11, Iter 3, disc loss: 0.0020212735868989458, policy loss: 6.935470208711106
Experience 11, Iter 4, disc loss: 0.002338347775856789, policy loss: 6.943778355910277
Experience 11, Iter 5, disc loss: 0.002315925514903872, policy loss: 7.019408845174708
Experience 11, Iter 6, disc loss: 0.0023164957309407827, policy loss: 6.924511667853223
Experience 11, Iter 7, disc loss: 0.0022096697518071366, policy loss: 7.036538385546358
Experience 11, Iter 8, disc loss: 0.002259720724905455, policy loss: 6.986596263426017
Experience 11, Iter 9, disc loss: 0.0021760849627400535, policy loss: 7.086108541334485
Experience 11, Iter 10, disc loss: 0.002350163480967991, policy loss: 7.103948852091031
Experience 11, Iter 11, disc loss: 0.002112215932746429, policy loss: 7.1095734569019395
Experience 11, Iter 12, disc loss: 0.0022785972542874273, policy loss: 7.090010299045321
Experience 11, Iter 13, disc loss: 0.0024213168456011526, policy loss: 7.210085864310374
Experience 11, Iter 14, disc loss: 0.002063703259633949, policy loss: 7.265670117106902
Experience 11, Iter 15, disc loss: 0.002335203458865817, policy loss: 7.329635589006817
Experience 11, Iter 16, disc loss: 0.002358054393174632, policy loss: 7.148038006526679
Experience 11, Iter 17, disc loss: 0.0026977999408528894, policy loss: 6.95758152967573
Experience 11, Iter 18, disc loss: 0.0030389945216632197, policy loss: 6.850173846692364
Experience 11, Iter 19, disc loss: 0.002493554909563659, policy loss: 7.345712592360892
Experience 11, Iter 20, disc loss: 0.002517581839251712, policy loss: 7.185030537905709
Experience 11, Iter 21, disc loss: 0.0024640363438403505, policy loss: 7.253942753678569
Experience 11, Iter 22, disc loss: 0.0024851296027248535, policy loss: 7.432342141492928
Experience 11, Iter 23, disc loss: 0.0025075802643296677, policy loss: 7.181704495993515
Experience 11, Iter 24, disc loss: 0.002969204932813346, policy loss: 7.002946571703076
Experience 11, Iter 25, disc loss: 0.0025276988772264366, policy loss: 7.6737511464985175
Experience 11, Iter 26, disc loss: 0.0027229544701454023, policy loss: 7.279254432199
Experience 11, Iter 27, disc loss: 0.002464531723749377, policy loss: 7.4437662744187545
Experience 11, Iter 28, disc loss: 0.0026591653061203, policy loss: 7.281329777365595
Experience 11, Iter 29, disc loss: 0.0026697978842031445, policy loss: 7.39433077026435
Experience 11, Iter 30, disc loss: 0.0025342396530438362, policy loss: 7.306949041378488
Experience 11, Iter 31, disc loss: 0.002694182524040059, policy loss: 7.14324257078601
Experience 11, Iter 32, disc loss: 0.0028090838888315587, policy loss: 6.930239358725924
Experience 11, Iter 33, disc loss: 0.0025686602391404145, policy loss: 7.446900732790262
Experience 11, Iter 34, disc loss: 0.002378809616357508, policy loss: 7.641505822822709
Experience 11, Iter 35, disc loss: 0.0027339201971944734, policy loss: 7.215269201286055
Experience 11, Iter 36, disc loss: 0.002657231247006451, policy loss: 7.280822658780807
Experience 11, Iter 37, disc loss: 0.00285829485869696, policy loss: 7.061867618784193
Experience 11, Iter 38, disc loss: 0.0026443944592652864, policy loss: 7.2079628751816545
Experience 11, Iter 39, disc loss: 0.0026298297867639827, policy loss: 7.116235528749662
Experience 11, Iter 40, disc loss: 0.0029990868281526157, policy loss: 7.201600813534061
Experience 11, Iter 41, disc loss: 0.0025047206892558527, policy loss: 7.306029415271377
Experience 11, Iter 42, disc loss: 0.002722712125749304, policy loss: 7.41849774168398
Experience 11, Iter 43, disc loss: 0.0027841589796260393, policy loss: 7.052717822680315
Experience 11, Iter 44, disc loss: 0.0028396345897873064, policy loss: 7.0853097347236815
Experience 11, Iter 45, disc loss: 0.0027678314841761585, policy loss: 7.235001434282367
Experience 11, Iter 46, disc loss: 0.0030171020443797323, policy loss: 7.106262523698529
Experience 11, Iter 47, disc loss: 0.0027533282333286288, policy loss: 7.324074312106992
Experience 11, Iter 48, disc loss: 0.002334016130453296, policy loss: 7.397729289040566
Experience 11, Iter 49, disc loss: 0.0029081646259376667, policy loss: 6.875685570355618
Experience 11, Iter 50, disc loss: 0.0026747817618278944, policy loss: 6.981235419678525
Experience 11, Iter 51, disc loss: 0.002895062790889834, policy loss: 7.02425854774464
Experience 11, Iter 52, disc loss: 0.0027930704792191547, policy loss: 7.070301630192623
Experience 11, Iter 53, disc loss: 0.0028383834904789243, policy loss: 6.9770221072932745
Experience 11, Iter 54, disc loss: 0.0025575379138617228, policy loss: 7.5863483253118815
Experience 11, Iter 55, disc loss: 0.0028466776704663345, policy loss: 6.982242048036328
Experience 11, Iter 56, disc loss: 0.0029893635575230816, policy loss: 6.962600581531636
Experience 11, Iter 57, disc loss: 0.0028122508624125165, policy loss: 7.134732195495019
Experience 11, Iter 58, disc loss: 0.003130288635479825, policy loss: 6.791004238619324
Experience 11, Iter 59, disc loss: 0.002913944356392217, policy loss: 7.188422913344446
Experience 11, Iter 60, disc loss: 0.003204319885367532, policy loss: 6.861210171487292
Experience 11, Iter 61, disc loss: 0.0032862468865697807, policy loss: 6.76814660060882
Experience 11, Iter 62, disc loss: 0.0029203938907969816, policy loss: 6.9834193873780235
Experience 11, Iter 63, disc loss: 0.0031570115707114897, policy loss: 7.121472418629243
Experience 11, Iter 64, disc loss: 0.0035612822033955337, policy loss: 6.917662150084075
Experience 11, Iter 65, disc loss: 0.0027832033855337193, policy loss: 7.284219380201219
Experience 11, Iter 66, disc loss: 0.0030090115993924474, policy loss: 7.168861742327673
Experience 11, Iter 67, disc loss: 0.00327143652112186, policy loss: 6.96245162613552
Experience 11, Iter 68, disc loss: 0.0030769225625300782, policy loss: 7.099054692566969
Experience 11, Iter 69, disc loss: 0.003643027266749655, policy loss: 6.743623835581059
Experience 11, Iter 70, disc loss: 0.0032105701961853113, policy loss: 7.040633137944114
Experience 11, Iter 71, disc loss: 0.0031030698957527003, policy loss: 7.31997200587852
Experience 11, Iter 72, disc loss: 0.0033466766362252633, policy loss: 7.022150312339435
Experience 11, Iter 73, disc loss: 0.0031272570035877313, policy loss: 7.152486303730808
Experience 11, Iter 74, disc loss: 0.0037187339638851337, policy loss: 6.938070809106257
Experience 11, Iter 75, disc loss: 0.0032233811224171628, policy loss: 7.055188214823291
Experience 11, Iter 76, disc loss: 0.00307494169023652, policy loss: 7.293360023280829
Experience 11, Iter 77, disc loss: 0.0027333433517208816, policy loss: 7.5453077603055165
Experience 11, Iter 78, disc loss: 0.003247372372236352, policy loss: 7.1209822941229195
Experience 11, Iter 79, disc loss: 0.0032027869802592345, policy loss: 7.22265561584936
Experience 11, Iter 80, disc loss: 0.002804096431392667, policy loss: 7.284898125954549
Experience 11, Iter 81, disc loss: 0.0030883401709353693, policy loss: 7.283126537739568
Experience 11, Iter 82, disc loss: 0.0033274866846216746, policy loss: 6.769023817993378
Experience 11, Iter 83, disc loss: 0.003107746892378701, policy loss: 7.148832308086561
Experience 11, Iter 84, disc loss: 0.00305135791987653, policy loss: 7.00024934211754
Experience 11, Iter 85, disc loss: 0.003193841365242015, policy loss: 7.051105002110406
Experience 11, Iter 86, disc loss: 0.0030355351734694855, policy loss: 6.948031875523194
Experience 11, Iter 87, disc loss: 0.0027276802059898516, policy loss: 7.375865470488529
Experience 11, Iter 88, disc loss: 0.0036350028483050356, policy loss: 7.096239513438648
Experience 11, Iter 89, disc loss: 0.0029097418827408227, policy loss: 6.867882453939588
Experience 11, Iter 90, disc loss: 0.003188172841630239, policy loss: 6.9317052931810474
Experience 11, Iter 91, disc loss: 0.003251290824153925, policy loss: 7.309327341935699
Experience 11, Iter 92, disc loss: 0.0029230547579983408, policy loss: 7.000913344134535
Experience 11, Iter 93, disc loss: 0.0031828978311494657, policy loss: 7.092720926944345
Experience 11, Iter 94, disc loss: 0.0034560291895925424, policy loss: 6.909281649382777
Experience 11, Iter 95, disc loss: 0.0034176259334767153, policy loss: 6.960477232837114
Experience 11, Iter 96, disc loss: 0.0029438236563857343, policy loss: 7.060825529858139
Experience 11, Iter 97, disc loss: 0.0030642951328753746, policy loss: 6.91364958085316
Experience 11, Iter 98, disc loss: 0.003000362274069188, policy loss: 7.520837998359324
Experience 11, Iter 99, disc loss: 0.00311723243845566, policy loss: 7.347251998077606
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.0601],
        [1.0536],
        [0.0260]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0158, 0.1520, 1.1610, 0.0212, 0.0178, 1.8210]],

        [[0.0158, 0.1520, 1.1610, 0.0212, 0.0178, 1.8210]],

        [[0.0158, 0.1520, 1.1610, 0.0212, 0.0178, 1.8210]],

        [[0.0158, 0.1520, 1.1610, 0.0212, 0.0178, 1.8210]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.2403, 4.2142, 0.1040], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.2403, 4.2142, 0.1040])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.546
Iter 2/2000 - Loss: 2.481
Iter 3/2000 - Loss: 2.418
Iter 4/2000 - Loss: 2.360
Iter 5/2000 - Loss: 2.356
Iter 6/2000 - Loss: 2.304
Iter 7/2000 - Loss: 2.226
Iter 8/2000 - Loss: 2.167
Iter 9/2000 - Loss: 2.109
Iter 10/2000 - Loss: 2.020
Iter 11/2000 - Loss: 1.909
Iter 12/2000 - Loss: 1.790
Iter 13/2000 - Loss: 1.661
Iter 14/2000 - Loss: 1.508
Iter 15/2000 - Loss: 1.327
Iter 16/2000 - Loss: 1.124
Iter 17/2000 - Loss: 0.910
Iter 18/2000 - Loss: 0.684
Iter 19/2000 - Loss: 0.441
Iter 20/2000 - Loss: 0.178
Iter 1981/2000 - Loss: -6.893
Iter 1982/2000 - Loss: -6.893
Iter 1983/2000 - Loss: -6.893
Iter 1984/2000 - Loss: -6.893
Iter 1985/2000 - Loss: -6.893
Iter 1986/2000 - Loss: -6.893
Iter 1987/2000 - Loss: -6.893
Iter 1988/2000 - Loss: -6.893
Iter 1989/2000 - Loss: -6.893
Iter 1990/2000 - Loss: -6.893
Iter 1991/2000 - Loss: -6.893
Iter 1992/2000 - Loss: -6.893
Iter 1993/2000 - Loss: -6.893
Iter 1994/2000 - Loss: -6.893
Iter 1995/2000 - Loss: -6.893
Iter 1996/2000 - Loss: -6.893
Iter 1997/2000 - Loss: -6.893
Iter 1998/2000 - Loss: -6.893
Iter 1999/2000 - Loss: -6.893
Iter 2000/2000 - Loss: -6.893
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0029],
        [0.0003]])
Lengthscale: tensor([[[12.0968,  3.8882, 27.5376,  7.2571,  6.2477, 39.6888]],

        [[17.5992, 32.9690,  7.9883,  1.4513,  0.8143, 14.2664]],

        [[17.8929, 31.1788,  7.6896,  1.1003,  1.1185, 16.7596]],

        [[15.3204, 33.5693, 23.4881,  2.8229,  1.9610, 44.0931]]])
Signal Variance: tensor([ 0.0483,  0.8094, 11.0479,  0.9636])
Estimated target variance: tensor([0.0155, 0.2403, 4.2142, 0.1040])
N: 120
Signal to noise ratio: tensor([12.4411, 47.8902, 61.2880, 52.8283])
Bound on condition number: tensor([ 18574.7061, 275217.6775, 450746.9908, 334900.4335])
Policy Optimizer learning rate:
9.884772446450595e-05
Experience 12, Iter 0, disc loss: 0.0038061429611637955, policy loss: 6.7441386392903295
Experience 12, Iter 1, disc loss: 0.0051309076557131475, policy loss: 6.453674840969699
Experience 12, Iter 2, disc loss: 0.004515502386306463, policy loss: 6.431977404235089
Experience 12, Iter 3, disc loss: 0.0035681122327127727, policy loss: 6.851397908585657
Experience 12, Iter 4, disc loss: 0.003666234968758489, policy loss: 7.021250796297257
Experience 12, Iter 5, disc loss: 0.004213100710793799, policy loss: 6.677126519750804
Experience 12, Iter 6, disc loss: 0.00385409694421403, policy loss: 6.788667674990468
Experience 12, Iter 7, disc loss: 0.004165130583299372, policy loss: 6.709975690581372
Experience 12, Iter 8, disc loss: 0.004098922087958578, policy loss: 6.699490568070058
Experience 12, Iter 9, disc loss: 0.00430587325168417, policy loss: 6.698871477062119
Experience 12, Iter 10, disc loss: 0.004062089232641959, policy loss: 6.9591306100599475
Experience 12, Iter 11, disc loss: 0.0039634929734838915, policy loss: 6.844777883451102
Experience 12, Iter 12, disc loss: 0.00399612919009198, policy loss: 7.3251213159104305
Experience 12, Iter 13, disc loss: 0.003831348829109423, policy loss: 6.9145383417329
Experience 12, Iter 14, disc loss: 0.004273648186136375, policy loss: 6.811482113862063
Experience 12, Iter 15, disc loss: 0.0037065592451550663, policy loss: 7.115499930521716
Experience 12, Iter 16, disc loss: 0.0037694419752152958, policy loss: 7.186714372002364
Experience 12, Iter 17, disc loss: 0.003913542191565063, policy loss: 6.769810261293096
Experience 12, Iter 18, disc loss: 0.003781718690911089, policy loss: 6.785630069572538
Experience 12, Iter 19, disc loss: 0.003880220408815534, policy loss: 6.8722785330274085
Experience 12, Iter 20, disc loss: 0.003348532622127567, policy loss: 7.401532681622684
Experience 12, Iter 21, disc loss: 0.0038828768027621362, policy loss: 6.7360246551526295
Experience 12, Iter 22, disc loss: 0.0038562487702485456, policy loss: 6.685538499886483
Experience 12, Iter 23, disc loss: 0.003698686127725135, policy loss: 7.136792464816969
Experience 12, Iter 24, disc loss: 0.003967967788184639, policy loss: 6.701865817912167
Experience 12, Iter 25, disc loss: 0.003967299074934836, policy loss: 6.701173448545202
Experience 12, Iter 26, disc loss: 0.0037237482241778705, policy loss: 7.6167429437499665
Experience 12, Iter 27, disc loss: 0.004315158737297138, policy loss: 6.477233524684266
Experience 12, Iter 28, disc loss: 0.004028033809025327, policy loss: 6.614477505338364
Experience 12, Iter 29, disc loss: 0.004278213193575872, policy loss: 6.481301751563083
Experience 12, Iter 30, disc loss: 0.0038148371521332953, policy loss: 6.753752855763543
Experience 12, Iter 31, disc loss: 0.003937883368023112, policy loss: 6.781768414346682
Experience 12, Iter 32, disc loss: 0.004458262586364649, policy loss: 6.742875802201905
Experience 12, Iter 33, disc loss: 0.004208189741849887, policy loss: 6.638959302906388
Experience 12, Iter 34, disc loss: 0.004051174161592691, policy loss: 6.776826998099416
Experience 12, Iter 35, disc loss: 0.004316606877807636, policy loss: 6.6535769569109275
Experience 12, Iter 36, disc loss: 0.004473153539879106, policy loss: 7.054910663610551
Experience 12, Iter 37, disc loss: 0.004716737867810201, policy loss: 6.548845228896347
Experience 12, Iter 38, disc loss: 0.0042298402392029096, policy loss: 6.7263682334349335
Experience 12, Iter 39, disc loss: 0.004403397119736448, policy loss: 6.666790181950378
Experience 12, Iter 40, disc loss: 0.004178291399919711, policy loss: 6.932146021982322
Experience 12, Iter 41, disc loss: 0.004585247605634058, policy loss: 6.782479159685882
Experience 12, Iter 42, disc loss: 0.004301297651964573, policy loss: 6.812893030060779
Experience 12, Iter 43, disc loss: 0.003956985249086392, policy loss: 7.210695126896591
Experience 12, Iter 44, disc loss: 0.00436573931994669, policy loss: 7.121294552630719
Experience 12, Iter 45, disc loss: 0.004472233083162902, policy loss: 6.821992201658442
Experience 12, Iter 46, disc loss: 0.004384553725465416, policy loss: 6.719837164541787
Experience 12, Iter 47, disc loss: 0.004697846585144848, policy loss: 6.579820142752219
Experience 12, Iter 48, disc loss: 0.003952043703769231, policy loss: 7.377294777222055
Experience 12, Iter 49, disc loss: 0.004291275209053168, policy loss: 6.965272591851413
Experience 12, Iter 50, disc loss: 0.004291135037653132, policy loss: 6.98923380571933
Experience 12, Iter 51, disc loss: 0.004571292227184368, policy loss: 7.189659022327307
Experience 12, Iter 52, disc loss: 0.004179239309150533, policy loss: 7.2170482055813245
Experience 12, Iter 53, disc loss: 0.004226618516772938, policy loss: 6.737702432715158
Experience 12, Iter 54, disc loss: 0.004046949507026627, policy loss: 7.010931181341938
Experience 12, Iter 55, disc loss: 0.003557814087468098, policy loss: 7.4669255014964016
Experience 12, Iter 56, disc loss: 0.004060313044854155, policy loss: 6.775733308876802
Experience 12, Iter 57, disc loss: 0.004109404786726722, policy loss: 6.847485713778082
Experience 12, Iter 58, disc loss: 0.00444048530447675, policy loss: 7.059388380296678
Experience 12, Iter 59, disc loss: 0.0038152164400179225, policy loss: 6.757760491574685
Experience 12, Iter 60, disc loss: 0.0038091550876834103, policy loss: 6.868435037895557
Experience 12, Iter 61, disc loss: 0.003790812755206621, policy loss: 7.242900691004004
Experience 12, Iter 62, disc loss: 0.004011530538555077, policy loss: 6.823496044376107
Experience 12, Iter 63, disc loss: 0.0035634414265745193, policy loss: 7.099836774176822
Experience 12, Iter 64, disc loss: 0.00394012521454343, policy loss: 6.848234065858547
Experience 12, Iter 65, disc loss: 0.0038924780204493224, policy loss: 6.98952408729778
Experience 12, Iter 66, disc loss: 0.0037694378387299636, policy loss: 7.474000141911957
Experience 12, Iter 67, disc loss: 0.003258024697876154, policy loss: 7.30866977725214
Experience 12, Iter 68, disc loss: 0.003627492424284482, policy loss: 7.411272800244484
Experience 12, Iter 69, disc loss: 0.0033297901536409485, policy loss: 7.33978351090453
Experience 12, Iter 70, disc loss: 0.003525939954319314, policy loss: 7.220573839568841
Experience 12, Iter 71, disc loss: 0.004076575694919146, policy loss: 6.723218220621203
Experience 12, Iter 72, disc loss: 0.003553316725380201, policy loss: 7.677005119189573
Experience 12, Iter 73, disc loss: 0.0033568009521002186, policy loss: 7.387654234015255
Experience 12, Iter 74, disc loss: 0.0038556689371488026, policy loss: 6.835601441608046
Experience 12, Iter 75, disc loss: 0.004558717352056076, policy loss: 6.743054340284198
Experience 12, Iter 76, disc loss: 0.0034225962835148253, policy loss: 7.075490246135908
Experience 12, Iter 77, disc loss: 0.0035929531728296758, policy loss: 7.214106571149544
Experience 12, Iter 78, disc loss: 0.0036904664944276424, policy loss: 7.093266173326862
Experience 12, Iter 79, disc loss: 0.0032345396260636513, policy loss: 7.3137672014203945
Experience 12, Iter 80, disc loss: 0.003514595795525219, policy loss: 6.870136856030307
Experience 12, Iter 81, disc loss: 0.0033341380522435293, policy loss: 7.220041687380085
Experience 12, Iter 82, disc loss: 0.003525921625425976, policy loss: 7.213057657698579
Experience 12, Iter 83, disc loss: 0.0030022049048241983, policy loss: 7.864999497177437
Experience 12, Iter 84, disc loss: 0.003438956447577881, policy loss: 7.084039250207079
Experience 12, Iter 85, disc loss: 0.0034545366423752843, policy loss: 7.549813648793399
Experience 12, Iter 86, disc loss: 0.0031605363174631988, policy loss: 7.4112968529622805
Experience 12, Iter 87, disc loss: 0.0030744026502154236, policy loss: 7.351112554860961
Experience 12, Iter 88, disc loss: 0.0038563052306912816, policy loss: 7.096994449889616
Experience 12, Iter 89, disc loss: 0.003427977181353238, policy loss: 6.820337836293557
Experience 12, Iter 90, disc loss: 0.0036405791635856113, policy loss: 7.09925536391003
Experience 12, Iter 91, disc loss: 0.0034259440413886818, policy loss: 6.9989530964063755
Experience 12, Iter 92, disc loss: 0.0034946116810967666, policy loss: 7.488135218966935
Experience 12, Iter 93, disc loss: 0.0032317697927914703, policy loss: 7.405462680020051
Experience 12, Iter 94, disc loss: 0.003223758006240241, policy loss: 7.161885998150338
Experience 12, Iter 95, disc loss: 0.0038554581979809164, policy loss: 6.925358377511337
Experience 12, Iter 96, disc loss: 0.0033342040281203345, policy loss: 7.299660344551807
Experience 12, Iter 97, disc loss: 0.0026833744666176235, policy loss: 8.047872808600914
Experience 12, Iter 98, disc loss: 0.0030893355633163926, policy loss: 7.496507237542476
Experience 12, Iter 99, disc loss: 0.003327551969432313, policy loss: 7.341327954202843
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0653],
        [1.1300],
        [0.0272]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0155, 0.1515, 1.2180, 0.0228, 0.0190, 1.9154]],

        [[0.0155, 0.1515, 1.2180, 0.0228, 0.0190, 1.9154]],

        [[0.0155, 0.1515, 1.2180, 0.0228, 0.0190, 1.9154]],

        [[0.0155, 0.1515, 1.2180, 0.0228, 0.0190, 1.9154]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0153, 0.2613, 4.5202, 0.1089], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0153, 0.2613, 4.5202, 0.1089])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.635
Iter 2/2000 - Loss: 2.585
Iter 3/2000 - Loss: 2.509
Iter 4/2000 - Loss: 2.462
Iter 5/2000 - Loss: 2.454
Iter 6/2000 - Loss: 2.390
Iter 7/2000 - Loss: 2.312
Iter 8/2000 - Loss: 2.256
Iter 9/2000 - Loss: 2.190
Iter 10/2000 - Loss: 2.085
Iter 11/2000 - Loss: 1.957
Iter 12/2000 - Loss: 1.827
Iter 13/2000 - Loss: 1.688
Iter 14/2000 - Loss: 1.524
Iter 15/2000 - Loss: 1.328
Iter 16/2000 - Loss: 1.111
Iter 17/2000 - Loss: 0.883
Iter 18/2000 - Loss: 0.646
Iter 19/2000 - Loss: 0.393
Iter 20/2000 - Loss: 0.123
Iter 1981/2000 - Loss: -6.938
Iter 1982/2000 - Loss: -6.938
Iter 1983/2000 - Loss: -6.938
Iter 1984/2000 - Loss: -6.938
Iter 1985/2000 - Loss: -6.938
Iter 1986/2000 - Loss: -6.938
Iter 1987/2000 - Loss: -6.938
Iter 1988/2000 - Loss: -6.938
Iter 1989/2000 - Loss: -6.938
Iter 1990/2000 - Loss: -6.938
Iter 1991/2000 - Loss: -6.938
Iter 1992/2000 - Loss: -6.938
Iter 1993/2000 - Loss: -6.938
Iter 1994/2000 - Loss: -6.938
Iter 1995/2000 - Loss: -6.938
Iter 1996/2000 - Loss: -6.938
Iter 1997/2000 - Loss: -6.938
Iter 1998/2000 - Loss: -6.938
Iter 1999/2000 - Loss: -6.939
Iter 2000/2000 - Loss: -6.939
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[11.9438,  4.5571, 25.1218,  7.8168,  7.1663, 43.7950]],

        [[17.6047, 34.1218,  7.5174,  1.2001,  1.3708, 17.0681]],

        [[17.7728, 32.6086,  7.6022,  0.9831,  0.8966, 18.3199]],

        [[15.6758, 34.2860, 13.1501,  2.4225,  1.8289, 39.7308]]])
Signal Variance: tensor([ 0.0509,  0.9771, 10.3483,  0.5411])
Estimated target variance: tensor([0.0153, 0.2613, 4.5202, 0.1089])
N: 130
Signal to noise ratio: tensor([12.7008, 51.8601, 61.3754, 39.9598])
Bound on condition number: tensor([ 20971.4416, 349631.4682, 489702.8994, 207583.5584])
Policy Optimizer learning rate:
9.874363283766072e-05
Experience 13, Iter 0, disc loss: 0.002080068887722801, policy loss: 9.237183563164999
Experience 13, Iter 1, disc loss: 0.0022432840067026765, policy loss: 8.761190712662975
Experience 13, Iter 2, disc loss: 0.002286577354642177, policy loss: 8.053887147059555
Experience 13, Iter 3, disc loss: 0.0023875587907532594, policy loss: 7.921323555248183
Experience 13, Iter 4, disc loss: 0.0026028731544923073, policy loss: 7.420865961899484
Experience 13, Iter 5, disc loss: 0.003235329566646127, policy loss: 7.459709516383407
Experience 13, Iter 6, disc loss: 0.0030306175551071436, policy loss: 7.321420478681717
Experience 13, Iter 7, disc loss: 0.0038171962810726956, policy loss: 6.947582136356597
Experience 13, Iter 8, disc loss: 0.003225483292446641, policy loss: 7.530268691011289
Experience 13, Iter 9, disc loss: 0.0033592614123865655, policy loss: 7.341024148064848
Experience 13, Iter 10, disc loss: 0.0036616313129074836, policy loss: 7.363822751489792
Experience 13, Iter 11, disc loss: 0.0038973215772984304, policy loss: 7.490698742004245
Experience 13, Iter 12, disc loss: 0.003320311159617001, policy loss: 7.399371291484075
Experience 13, Iter 13, disc loss: 0.003217757346555595, policy loss: 8.088393211844675
Experience 13, Iter 14, disc loss: 0.003102843076395666, policy loss: 7.402782915981042
Experience 13, Iter 15, disc loss: 0.0035822609671818557, policy loss: 7.756913796127756
Experience 13, Iter 16, disc loss: 0.002944848124277352, policy loss: 7.790342747926862
Experience 13, Iter 17, disc loss: 0.003091387572540078, policy loss: 7.8685936970955215
Experience 13, Iter 18, disc loss: 0.0030304987767661943, policy loss: 7.740260982616209
Experience 13, Iter 19, disc loss: 0.002844891136219357, policy loss: 7.524157761402744
Experience 13, Iter 20, disc loss: 0.002716827349599423, policy loss: 8.036131274333178
Experience 13, Iter 21, disc loss: 0.002757653500351833, policy loss: 7.851164421782469
Experience 13, Iter 22, disc loss: 0.0025905753574329126, policy loss: 8.111832782789069
Experience 13, Iter 23, disc loss: 0.002693492283111303, policy loss: 7.833101473509895
Experience 13, Iter 24, disc loss: 0.0027089008597074287, policy loss: 7.313976608784711
Experience 13, Iter 25, disc loss: 0.0033609646565948134, policy loss: 7.2126834991798585
Experience 13, Iter 26, disc loss: 0.0027546577141621936, policy loss: 7.729303663457302
Experience 13, Iter 27, disc loss: 0.0026286730517531194, policy loss: 7.290027090297461
Experience 13, Iter 28, disc loss: 0.0027182662695466276, policy loss: 7.4333667264021415
Experience 13, Iter 29, disc loss: 0.0025857163711785073, policy loss: 8.437181287866785
Experience 13, Iter 30, disc loss: 0.0029100556384749265, policy loss: 7.144990946184851
Experience 13, Iter 31, disc loss: 0.002633311478015453, policy loss: 7.196777840311242
Experience 13, Iter 32, disc loss: 0.00309703637480839, policy loss: 7.144370125538415
Experience 13, Iter 33, disc loss: 0.0027310733177800042, policy loss: 7.540795386466116
Experience 13, Iter 34, disc loss: 0.003050503450090936, policy loss: 6.940312188484378
Experience 13, Iter 35, disc loss: 0.0028788538104865154, policy loss: 7.121572890106563
Experience 13, Iter 36, disc loss: 0.002865812420556665, policy loss: 7.430394744475964
Experience 13, Iter 37, disc loss: 0.0025176185408834015, policy loss: 7.637613872549168
Experience 13, Iter 38, disc loss: 0.002659976128251956, policy loss: 7.229219829488273
Experience 13, Iter 39, disc loss: 0.0030493482732853227, policy loss: 7.488111274256353
Experience 13, Iter 40, disc loss: 0.0028887810371633887, policy loss: 7.22192251433305
Experience 13, Iter 41, disc loss: 0.003137736324164086, policy loss: 7.57906141573038
Experience 13, Iter 42, disc loss: 0.003057151351215934, policy loss: 7.05981717213848
Experience 13, Iter 43, disc loss: 0.0029367220333360936, policy loss: 7.222252849120327
Experience 13, Iter 44, disc loss: 0.0028402627781984394, policy loss: 7.503538586088904
Experience 13, Iter 45, disc loss: 0.0026439475966123747, policy loss: 7.5584875337953354
Experience 13, Iter 46, disc loss: 0.0031217976992804616, policy loss: 7.526680758578374
Experience 13, Iter 47, disc loss: 0.0029722250530269077, policy loss: 7.2866992384328775
Experience 13, Iter 48, disc loss: 0.002918493170683909, policy loss: 8.096842786918621
Experience 13, Iter 49, disc loss: 0.002761439546111398, policy loss: 7.601835328934474
Experience 13, Iter 50, disc loss: 0.00307089056336997, policy loss: 7.4145456620001635
Experience 13, Iter 51, disc loss: 0.002865172295589323, policy loss: 7.609542354126659
Experience 13, Iter 52, disc loss: 0.00295264385371774, policy loss: 7.447868875854233
Experience 13, Iter 53, disc loss: 0.0033027426885640703, policy loss: 6.986301455821597
Experience 13, Iter 54, disc loss: 0.0027533527156072385, policy loss: 7.653723505337739
Experience 13, Iter 55, disc loss: 0.0028812940935814164, policy loss: 7.809189745294615
Experience 13, Iter 56, disc loss: 0.0034035820534367434, policy loss: 7.254091430234056
Experience 13, Iter 57, disc loss: 0.002957495551723359, policy loss: 7.674694235093636
Experience 13, Iter 58, disc loss: 0.0026232861147766347, policy loss: 7.6677876161811485
Experience 13, Iter 59, disc loss: 0.00292667866568533, policy loss: 7.925537971856145
Experience 13, Iter 60, disc loss: 0.002925523309244635, policy loss: 7.349073111155709
Experience 13, Iter 61, disc loss: 0.002538340069869598, policy loss: 7.85676363456551
Experience 13, Iter 62, disc loss: 0.002935772238038937, policy loss: 7.5167738186137205
Experience 13, Iter 63, disc loss: 0.002877345476511984, policy loss: 7.494388014831704
Experience 13, Iter 64, disc loss: 0.002715210698705563, policy loss: 7.73979909125319
Experience 13, Iter 65, disc loss: 0.0029982325237847976, policy loss: 7.5801708312038025
Experience 13, Iter 66, disc loss: 0.0028656598477268596, policy loss: 7.212554865976658
Experience 13, Iter 67, disc loss: 0.0024971377314248287, policy loss: 7.559161753926187
Experience 13, Iter 68, disc loss: 0.0024495672433451797, policy loss: 8.208043332691986
Experience 13, Iter 69, disc loss: 0.0024173448700549917, policy loss: 8.430277159606979
Experience 13, Iter 70, disc loss: 0.002492932208863003, policy loss: 7.377446741833612
Experience 13, Iter 71, disc loss: 0.002696682575886571, policy loss: 7.369489450575952
Experience 13, Iter 72, disc loss: 0.0027494372322798277, policy loss: 7.224033791968093
Experience 13, Iter 73, disc loss: 0.0031501135596255147, policy loss: 6.852640572804047
Experience 13, Iter 74, disc loss: 0.0027872759173061196, policy loss: 7.217290030910522
Experience 13, Iter 75, disc loss: 0.0026573554965747116, policy loss: 7.371920241183162
Experience 13, Iter 76, disc loss: 0.0025603189207723275, policy loss: 8.01210953120408
Experience 13, Iter 77, disc loss: 0.0023565893758618756, policy loss: 7.8338657779508
Experience 13, Iter 78, disc loss: 0.002949156932696227, policy loss: 6.942439137870771
Experience 13, Iter 79, disc loss: 0.0025841292099010845, policy loss: 7.263932975942124
Experience 13, Iter 80, disc loss: 0.0027109943403342144, policy loss: 7.185113534477882
Experience 13, Iter 81, disc loss: 0.00250615456634291, policy loss: 7.586108356616968
Experience 13, Iter 82, disc loss: 0.0025737824241237, policy loss: 7.592671647641904
Experience 13, Iter 83, disc loss: 0.003073910144416729, policy loss: 7.531751275244878
Experience 13, Iter 84, disc loss: 0.002754781896885787, policy loss: 7.5759253247546114
Experience 13, Iter 85, disc loss: 0.0027646497686499936, policy loss: 8.07695521951855
Experience 13, Iter 86, disc loss: 0.0027483278042453943, policy loss: 7.733439969752295
Experience 13, Iter 87, disc loss: 0.002688354157410789, policy loss: 8.156479450769213
Experience 13, Iter 88, disc loss: 0.002778127299887068, policy loss: 7.574399724832407
Experience 13, Iter 89, disc loss: 0.00226030469568659, policy loss: 8.043815246838989
Experience 13, Iter 90, disc loss: 0.002798059593642922, policy loss: 7.482356777152915
Experience 13, Iter 91, disc loss: 0.002844161659393403, policy loss: 7.2411309235019115
Experience 13, Iter 92, disc loss: 0.0025232553863782695, policy loss: 7.441642770368995
Experience 13, Iter 93, disc loss: 0.002877913133363775, policy loss: 7.368477031341571
Experience 13, Iter 94, disc loss: 0.0028547459849503336, policy loss: 7.321711111650716
Experience 13, Iter 95, disc loss: 0.003247300719192824, policy loss: 7.737928093958024
Experience 13, Iter 96, disc loss: 0.002847582114569121, policy loss: 7.501702933631299
Experience 13, Iter 97, disc loss: 0.002544616886942762, policy loss: 8.366283451620966
Experience 13, Iter 98, disc loss: 0.0025050226257788384, policy loss: 7.753011065219923
Experience 13, Iter 99, disc loss: 0.0028408177320405443, policy loss: 7.655728011167323
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0754],
        [1.2411],
        [0.0302]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0149, 0.1546, 1.3627, 0.0245, 0.0207, 2.0417]],

        [[0.0149, 0.1546, 1.3627, 0.0245, 0.0207, 2.0417]],

        [[0.0149, 0.1546, 1.3627, 0.0245, 0.0207, 2.0417]],

        [[0.0149, 0.1546, 1.3627, 0.0245, 0.0207, 2.0417]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0154, 0.3014, 4.9643, 0.1208], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0154, 0.3014, 4.9643, 0.1208])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.802
Iter 2/2000 - Loss: 2.764
Iter 3/2000 - Loss: 2.673
Iter 4/2000 - Loss: 2.637
Iter 5/2000 - Loss: 2.629
Iter 6/2000 - Loss: 2.555
Iter 7/2000 - Loss: 2.474
Iter 8/2000 - Loss: 2.421
Iter 9/2000 - Loss: 2.355
Iter 10/2000 - Loss: 2.244
Iter 11/2000 - Loss: 2.108
Iter 12/2000 - Loss: 1.971
Iter 13/2000 - Loss: 1.829
Iter 14/2000 - Loss: 1.660
Iter 15/2000 - Loss: 1.459
Iter 16/2000 - Loss: 1.234
Iter 17/2000 - Loss: 0.996
Iter 18/2000 - Loss: 0.749
Iter 19/2000 - Loss: 0.487
Iter 20/2000 - Loss: 0.210
Iter 1981/2000 - Loss: -6.965
Iter 1982/2000 - Loss: -6.965
Iter 1983/2000 - Loss: -6.965
Iter 1984/2000 - Loss: -6.965
Iter 1985/2000 - Loss: -6.965
Iter 1986/2000 - Loss: -6.966
Iter 1987/2000 - Loss: -6.966
Iter 1988/2000 - Loss: -6.966
Iter 1989/2000 - Loss: -6.966
Iter 1990/2000 - Loss: -6.966
Iter 1991/2000 - Loss: -6.966
Iter 1992/2000 - Loss: -6.966
Iter 1993/2000 - Loss: -6.966
Iter 1994/2000 - Loss: -6.966
Iter 1995/2000 - Loss: -6.966
Iter 1996/2000 - Loss: -6.966
Iter 1997/2000 - Loss: -6.966
Iter 1998/2000 - Loss: -6.966
Iter 1999/2000 - Loss: -6.966
Iter 2000/2000 - Loss: -6.966
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0027],
        [0.0003]])
Lengthscale: tensor([[[10.4045,  3.8437, 25.4852,  6.3303,  4.8838, 41.4086]],

        [[16.4332, 33.9856,  7.2715,  1.1545,  1.9232, 17.9520]],

        [[17.9115, 33.5854,  7.6686,  0.9736,  0.8782, 17.4897]],

        [[15.4203, 33.3054, 11.6502,  2.4708,  1.8167, 40.1203]]])
Signal Variance: tensor([ 0.0414,  0.9988, 10.1255,  0.4649])
Estimated target variance: tensor([0.0154, 0.3014, 4.9643, 0.1208])
N: 140
Signal to noise ratio: tensor([11.2814, 51.6931, 61.5656, 37.3977])
Bound on condition number: tensor([ 17818.9394, 374105.2821, 530646.7553, 195803.3623])
Policy Optimizer learning rate:
9.863965082453537e-05
Experience 14, Iter 0, disc loss: 0.0025754273478674128, policy loss: 7.774119275134225
Experience 14, Iter 1, disc loss: 0.0028860441605007632, policy loss: 7.8229941555701075
Experience 14, Iter 2, disc loss: 0.0025987423777937032, policy loss: 7.812298763804855
Experience 14, Iter 3, disc loss: 0.0028403127514371033, policy loss: 7.335485672812819
Experience 14, Iter 4, disc loss: 0.0024721526624290726, policy loss: 7.700351105004325
Experience 14, Iter 5, disc loss: 0.002686557337688822, policy loss: 7.784824270579163
Experience 14, Iter 6, disc loss: 0.0030992676289292513, policy loss: 7.132762213615287
Experience 14, Iter 7, disc loss: 0.0026502564452908404, policy loss: 7.473798339996345
Experience 14, Iter 8, disc loss: 0.0026891818989638362, policy loss: 7.075011136722438
Experience 14, Iter 9, disc loss: 0.0023451485413131085, policy loss: 8.35178442326973
Experience 14, Iter 10, disc loss: 0.002694910420336459, policy loss: 7.356856957973764
Experience 14, Iter 11, disc loss: 0.002358134113519814, policy loss: 7.796478815677521
Experience 14, Iter 12, disc loss: 0.0026206935140548385, policy loss: 7.376612999345515
Experience 14, Iter 13, disc loss: 0.00240015436247085, policy loss: 7.677779942303976
Experience 14, Iter 14, disc loss: 0.002371332900534007, policy loss: 7.595516112739331
Experience 14, Iter 15, disc loss: 0.0026143482504739665, policy loss: 7.558841138015687
Experience 14, Iter 16, disc loss: 0.002511985317737929, policy loss: 7.449379252282266
Experience 14, Iter 17, disc loss: 0.0027330358153477455, policy loss: 7.207230680291952
Experience 14, Iter 18, disc loss: 0.0024146088434253677, policy loss: 7.489194938543113
Experience 14, Iter 19, disc loss: 0.002460460080219688, policy loss: 8.306699010718804
Experience 14, Iter 20, disc loss: 0.002552593279554617, policy loss: 7.232846495128456
Experience 14, Iter 21, disc loss: 0.002623248400743525, policy loss: 7.491407870869925
Experience 14, Iter 22, disc loss: 0.0025167705868503025, policy loss: 7.7207304021546825
Experience 14, Iter 23, disc loss: 0.0025921397654090276, policy loss: 7.298875191468718
Experience 14, Iter 24, disc loss: 0.0027258004998050363, policy loss: 7.2288171104860535
Experience 14, Iter 25, disc loss: 0.0026562591919113074, policy loss: 7.840486767735996
Experience 14, Iter 26, disc loss: 0.0023772257516553505, policy loss: 7.591728010279628
Experience 14, Iter 27, disc loss: 0.0023964266148064433, policy loss: 7.886123349317855
Experience 14, Iter 28, disc loss: 0.002374227456720921, policy loss: 7.702362506915268
Experience 14, Iter 29, disc loss: 0.002630687810382943, policy loss: 7.83199549227752
Experience 14, Iter 30, disc loss: 0.003015845363539229, policy loss: 7.651285482698581
Experience 14, Iter 31, disc loss: 0.0025605830644680203, policy loss: 7.9636529208746865
Experience 14, Iter 32, disc loss: 0.002511120872263333, policy loss: 7.538694839208194
Experience 14, Iter 33, disc loss: 0.00217492874767252, policy loss: 8.238036340338077
Experience 14, Iter 34, disc loss: 0.002373993795204414, policy loss: 7.375159499417535
Experience 14, Iter 35, disc loss: 0.0026869230791687782, policy loss: 8.172727510213624
Experience 14, Iter 36, disc loss: 0.0023553565555023137, policy loss: 7.446276720111251
Experience 14, Iter 37, disc loss: 0.0024899640429142888, policy loss: 8.53614357808967
Experience 14, Iter 38, disc loss: 0.0025221129724618957, policy loss: 7.378464674258211
Experience 14, Iter 39, disc loss: 0.0025051922905359077, policy loss: 7.937249365400529
Experience 14, Iter 40, disc loss: 0.0030770415313498946, policy loss: 7.46703131525279
Experience 14, Iter 41, disc loss: 0.002697032490618816, policy loss: 7.369986360477849
Experience 14, Iter 42, disc loss: 0.002401061323069935, policy loss: 7.685787854014706
Experience 14, Iter 43, disc loss: 0.00209497885868887, policy loss: 8.281708144126327
Experience 14, Iter 44, disc loss: 0.0026630172480909212, policy loss: 8.097260307412311
Experience 14, Iter 45, disc loss: 0.0025915705920936537, policy loss: 8.189263811583078
Experience 14, Iter 46, disc loss: 0.002632753774012112, policy loss: 7.683133370019297
Experience 14, Iter 47, disc loss: 0.0021898020378263113, policy loss: 7.5847580393575695
Experience 14, Iter 48, disc loss: 0.0026497489963481255, policy loss: 7.275236517098527
Experience 14, Iter 49, disc loss: 0.0021983451512834033, policy loss: 7.953631473927372
Experience 14, Iter 50, disc loss: 0.002335213204813739, policy loss: 7.8842489695632025
Experience 14, Iter 51, disc loss: 0.0023395343758496924, policy loss: 7.787015334984301
Experience 14, Iter 52, disc loss: 0.0024037687544803807, policy loss: 7.845624563499153
Experience 14, Iter 53, disc loss: 0.002335772527983542, policy loss: 7.587419931719363
Experience 14, Iter 54, disc loss: 0.0025494560196545604, policy loss: 7.264364387036117
Experience 14, Iter 55, disc loss: 0.002381786615235868, policy loss: 7.5029042126825205
Experience 14, Iter 56, disc loss: 0.0024761752206037997, policy loss: 7.553829732298471
Experience 14, Iter 57, disc loss: 0.0023906183027156856, policy loss: 7.6919601079478195
Experience 14, Iter 58, disc loss: 0.002510322511877527, policy loss: 7.2274235135709874
Experience 14, Iter 59, disc loss: 0.0022793265776704236, policy loss: 7.574931841521613
Experience 14, Iter 60, disc loss: 0.0022172507713981186, policy loss: 8.042024622261815
Experience 14, Iter 61, disc loss: 0.0021580234073374, policy loss: 7.716579525338692
Experience 14, Iter 62, disc loss: 0.002345736987007979, policy loss: 7.586409428739362
Experience 14, Iter 63, disc loss: 0.002431644262940065, policy loss: 7.470325041644067
Experience 14, Iter 64, disc loss: 0.0024250315181350143, policy loss: 7.965450055388001
Experience 14, Iter 65, disc loss: 0.0026605260012517876, policy loss: 7.334823723799574
Experience 14, Iter 66, disc loss: 0.002247352307663396, policy loss: 7.439136305939671
Experience 14, Iter 67, disc loss: 0.0022940437188021436, policy loss: 7.6803438466529865
Experience 14, Iter 68, disc loss: 0.0023184632969950056, policy loss: 7.515911802404848
Experience 14, Iter 69, disc loss: 0.0021614100671662794, policy loss: 8.58792219195123
Experience 14, Iter 70, disc loss: 0.0019296329655728484, policy loss: 8.250474581143006
Experience 14, Iter 71, disc loss: 0.002599708923448939, policy loss: 7.7019228062074205
Experience 14, Iter 72, disc loss: 0.0021167464744206283, policy loss: 7.747187869576173
Experience 14, Iter 73, disc loss: 0.0026297240959501176, policy loss: 7.697044711697647
Experience 14, Iter 74, disc loss: 0.0023592270891358647, policy loss: 7.670765797843556
Experience 14, Iter 75, disc loss: 0.002508870348399809, policy loss: 7.551536784336394
Experience 14, Iter 76, disc loss: 0.002304600624224975, policy loss: 7.451329525200586
Experience 14, Iter 77, disc loss: 0.0023334392171354785, policy loss: 8.233038436276816
Experience 14, Iter 78, disc loss: 0.0027196674951532234, policy loss: 7.159726046872792
Experience 14, Iter 79, disc loss: 0.002376036159327058, policy loss: 7.7644717970165775
Experience 14, Iter 80, disc loss: 0.0022999303788763404, policy loss: 7.719155557411859
Experience 14, Iter 81, disc loss: 0.0023554262104821958, policy loss: 8.181291699519807
Experience 14, Iter 82, disc loss: 0.0022370360376788127, policy loss: 7.6653964104646874
Experience 14, Iter 83, disc loss: 0.002096905003209022, policy loss: 8.022876992644402
Experience 14, Iter 84, disc loss: 0.002203378639707643, policy loss: 7.6639373406745595
Experience 14, Iter 85, disc loss: 0.002086899337167503, policy loss: 8.142550761720925
Experience 14, Iter 86, disc loss: 0.0024435568504927643, policy loss: 7.860933747963754
Experience 14, Iter 87, disc loss: 0.0022161940485485995, policy loss: 7.793562302937214
Experience 14, Iter 88, disc loss: 0.00212776016907294, policy loss: 8.433981682752012
Experience 14, Iter 89, disc loss: 0.0028049442345497166, policy loss: 7.096527029305012
Experience 14, Iter 90, disc loss: 0.0017620153108066586, policy loss: 8.390009807279881
Experience 14, Iter 91, disc loss: 0.002282458935281389, policy loss: 8.370453128582996
Experience 14, Iter 92, disc loss: 0.0020705573986642993, policy loss: 8.529244537384049
Experience 14, Iter 93, disc loss: 0.002479096912751293, policy loss: 8.124968113814495
Experience 14, Iter 94, disc loss: 0.0021104836562737175, policy loss: 7.686025886904551
Experience 14, Iter 95, disc loss: 0.0022435305184315864, policy loss: 7.726415817038121
Experience 14, Iter 96, disc loss: 0.002060863585563848, policy loss: 8.377518840251057
Experience 14, Iter 97, disc loss: 0.002364308753101451, policy loss: 7.540904363341777
Experience 14, Iter 98, disc loss: 0.0020650148294680953, policy loss: 7.750002784147469
Experience 14, Iter 99, disc loss: 0.0022560119979219066, policy loss: 7.822611380170679
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0835],
        [1.3179],
        [0.0322]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0145, 0.1571, 1.4577, 0.0257, 0.0222, 2.1672]],

        [[0.0145, 0.1571, 1.4577, 0.0257, 0.0222, 2.1672]],

        [[0.0145, 0.1571, 1.4577, 0.0257, 0.0222, 2.1672]],

        [[0.0145, 0.1571, 1.4577, 0.0257, 0.0222, 2.1672]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0154, 0.3340, 5.2716, 0.1288], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0154, 0.3340, 5.2716, 0.1288])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.876
Iter 2/2000 - Loss: 2.852
Iter 3/2000 - Loss: 2.733
Iter 4/2000 - Loss: 2.699
Iter 5/2000 - Loss: 2.682
Iter 6/2000 - Loss: 2.589
Iter 7/2000 - Loss: 2.495
Iter 8/2000 - Loss: 2.433
Iter 9/2000 - Loss: 2.357
Iter 10/2000 - Loss: 2.232
Iter 11/2000 - Loss: 2.079
Iter 12/2000 - Loss: 1.925
Iter 13/2000 - Loss: 1.768
Iter 14/2000 - Loss: 1.588
Iter 15/2000 - Loss: 1.373
Iter 16/2000 - Loss: 1.133
Iter 17/2000 - Loss: 0.881
Iter 18/2000 - Loss: 0.621
Iter 19/2000 - Loss: 0.351
Iter 20/2000 - Loss: 0.067
Iter 1981/2000 - Loss: -7.109
Iter 1982/2000 - Loss: -7.109
Iter 1983/2000 - Loss: -7.109
Iter 1984/2000 - Loss: -7.109
Iter 1985/2000 - Loss: -7.109
Iter 1986/2000 - Loss: -7.109
Iter 1987/2000 - Loss: -7.109
Iter 1988/2000 - Loss: -7.109
Iter 1989/2000 - Loss: -7.109
Iter 1990/2000 - Loss: -7.109
Iter 1991/2000 - Loss: -7.109
Iter 1992/2000 - Loss: -7.109
Iter 1993/2000 - Loss: -7.109
Iter 1994/2000 - Loss: -7.109
Iter 1995/2000 - Loss: -7.109
Iter 1996/2000 - Loss: -7.109
Iter 1997/2000 - Loss: -7.109
Iter 1998/2000 - Loss: -7.109
Iter 1999/2000 - Loss: -7.109
Iter 2000/2000 - Loss: -7.109
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0026],
        [0.0003]])
Lengthscale: tensor([[[10.6390,  4.1541, 27.3258,  7.1997,  5.6730, 41.4101]],

        [[16.1916, 33.3221,  7.4758,  1.1655,  1.8449, 19.9871]],

        [[17.6129, 33.1939,  7.8845,  0.9559,  0.8618, 16.9549]],

        [[15.0061, 30.8463, 11.6366,  2.2691,  1.7701, 41.8992]]])
Signal Variance: tensor([0.0454, 1.1451, 9.9097, 0.4520])
Estimated target variance: tensor([0.0154, 0.3340, 5.2716, 0.1288])
N: 150
Signal to noise ratio: tensor([11.9690, 55.1625, 62.3150, 37.7629])
Bound on condition number: tensor([ 21489.6591, 456436.9580, 582474.5929, 213906.5597])
Policy Optimizer learning rate:
9.853577830970112e-05
Experience 15, Iter 0, disc loss: 0.00222567370097911, policy loss: 8.419056920173075
Experience 15, Iter 1, disc loss: 0.0023667309868942933, policy loss: 7.8239334647691825
Experience 15, Iter 2, disc loss: 0.0023092279020763854, policy loss: 7.268888399579106
Experience 15, Iter 3, disc loss: 0.002304466437264032, policy loss: 7.733390567273078
Experience 15, Iter 4, disc loss: 0.0022911902917877418, policy loss: 7.914253356934087
Experience 15, Iter 5, disc loss: 0.002237151988013171, policy loss: 7.865656996329368
Experience 15, Iter 6, disc loss: 0.002334690374699207, policy loss: 8.616011480878736
Experience 15, Iter 7, disc loss: 0.002353979039233999, policy loss: 7.545814444783545
Experience 15, Iter 8, disc loss: 0.0024046655888745145, policy loss: 8.002040626986476
Experience 15, Iter 9, disc loss: 0.0020694150035636847, policy loss: 8.544515498997765
Experience 15, Iter 10, disc loss: 0.0019437854256627771, policy loss: 8.04861895349632
Experience 15, Iter 11, disc loss: 0.0020632529919453113, policy loss: 8.595683067432319
Experience 15, Iter 12, disc loss: 0.0022574701155319617, policy loss: 8.101771155700751
Experience 15, Iter 13, disc loss: 0.002523611731305071, policy loss: 7.52097386337788
Experience 15, Iter 14, disc loss: 0.002404323906403733, policy loss: 8.0265072092218
Experience 15, Iter 15, disc loss: 0.002671880134480366, policy loss: 7.237356792221987
Experience 15, Iter 16, disc loss: 0.0020469589591618166, policy loss: 7.989010848902369
Experience 15, Iter 17, disc loss: 0.0022960621482183076, policy loss: 7.7412474388067976
Experience 15, Iter 18, disc loss: 0.0023309664167327415, policy loss: 7.45471958900331
Experience 15, Iter 19, disc loss: 0.0022911121030319143, policy loss: 7.5977550101981075
Experience 15, Iter 20, disc loss: 0.00193705743135197, policy loss: 8.36373914268778
Experience 15, Iter 21, disc loss: 0.002257841298318811, policy loss: 7.547970500598074
Experience 15, Iter 22, disc loss: 0.0022736752664875917, policy loss: 7.46285268521342
Experience 15, Iter 23, disc loss: 0.002175719560306856, policy loss: 8.12024975266209
Experience 15, Iter 24, disc loss: 0.002128708066841871, policy loss: 8.072510643681223
Experience 15, Iter 25, disc loss: 0.0021669361812890844, policy loss: 8.157583792366783
Experience 15, Iter 26, disc loss: 0.002538611222927368, policy loss: 7.633856700549239
Experience 15, Iter 27, disc loss: 0.002634609313144502, policy loss: 7.900496835712668
Experience 15, Iter 28, disc loss: 0.0023740985605378425, policy loss: 8.058732743510745
Experience 15, Iter 29, disc loss: 0.0021695617691509317, policy loss: 9.012965926355887
Experience 15, Iter 30, disc loss: 0.0023115956211665284, policy loss: 7.959776561328066
Experience 15, Iter 31, disc loss: 0.0026061795940113767, policy loss: 7.1743461686500485
Experience 15, Iter 32, disc loss: 0.002370948061029077, policy loss: 7.903211747111851
Experience 15, Iter 33, disc loss: 0.0022955970862059364, policy loss: 8.395694462781801
Experience 15, Iter 34, disc loss: 0.0021002531144329963, policy loss: 8.120589060220492
Experience 15, Iter 35, disc loss: 0.0026530251565423154, policy loss: 7.143706168229468
Experience 15, Iter 36, disc loss: 0.00253774563161596, policy loss: 7.660678909764808
Experience 15, Iter 37, disc loss: 0.002388907199033424, policy loss: 7.718351976283151
Experience 15, Iter 38, disc loss: 0.0022973734805267995, policy loss: 7.551663213874732
Experience 15, Iter 39, disc loss: 0.0020449923445450017, policy loss: 8.527101333079477
Experience 15, Iter 40, disc loss: 0.002389936979450267, policy loss: 7.354077320014483
Experience 15, Iter 41, disc loss: 0.0024938786429880527, policy loss: 7.927617742206179
Experience 15, Iter 42, disc loss: 0.0024271954224437254, policy loss: 8.110359946507245
Experience 15, Iter 43, disc loss: 0.0017687345590315182, policy loss: 8.662057578223072
Experience 15, Iter 44, disc loss: 0.002251541555620783, policy loss: 7.8303920236987725
Experience 15, Iter 45, disc loss: 0.002326443729647798, policy loss: 8.44728786419817
Experience 15, Iter 46, disc loss: 0.002187561207863083, policy loss: 7.499055715867704
Experience 15, Iter 47, disc loss: 0.002282314247364339, policy loss: 8.752275780428674
Experience 15, Iter 48, disc loss: 0.002656838647112522, policy loss: 8.124205059292844
Experience 15, Iter 49, disc loss: 0.002829026904131449, policy loss: 7.609367224596407
Experience 15, Iter 50, disc loss: 0.002123125962947604, policy loss: 7.794141223305921
Experience 15, Iter 51, disc loss: 0.002570892595302409, policy loss: 8.027402185171255
Experience 15, Iter 52, disc loss: 0.0021752316040306438, policy loss: 7.757393208504242
Experience 15, Iter 53, disc loss: 0.0022205958300069275, policy loss: 7.890584209691822
Experience 15, Iter 54, disc loss: 0.002190499866693033, policy loss: 8.017693563459627
Experience 15, Iter 55, disc loss: 0.002392604502665239, policy loss: 7.811825793325371
Experience 15, Iter 56, disc loss: 0.0024584852267116966, policy loss: 7.6281561743034345
Experience 15, Iter 57, disc loss: 0.0021474995907439834, policy loss: 7.982382640928
Experience 15, Iter 58, disc loss: 0.002523470944509649, policy loss: 7.56747963943535
Experience 15, Iter 59, disc loss: 0.0020785613814887013, policy loss: 7.89954315556391
Experience 15, Iter 60, disc loss: 0.00231315589136808, policy loss: 8.175980236988662
Experience 15, Iter 61, disc loss: 0.0016458391635414762, policy loss: 8.708416736534819
Experience 15, Iter 62, disc loss: 0.0022558729637498073, policy loss: 7.951859362196901
Experience 15, Iter 63, disc loss: 0.00214935256658805, policy loss: 8.025243545952673
Experience 15, Iter 64, disc loss: 0.002070910777825406, policy loss: 8.03688736473624
Experience 15, Iter 65, disc loss: 0.002446771846812676, policy loss: 7.373968742896917
Experience 15, Iter 66, disc loss: 0.001954980422314786, policy loss: 9.004750638299512
Experience 15, Iter 67, disc loss: 0.00210767479699192, policy loss: 8.106500229764457
Experience 15, Iter 68, disc loss: 0.0021185309868298327, policy loss: 7.749395471167691
Experience 15, Iter 69, disc loss: 0.002483172917031201, policy loss: 7.494902755951777
Experience 15, Iter 70, disc loss: 0.002329598965738362, policy loss: 7.739972807655457
Experience 15, Iter 71, disc loss: 0.0018712219620729427, policy loss: 8.372034101144084
Experience 15, Iter 72, disc loss: 0.0018791331805103402, policy loss: 8.056694616348407
Experience 15, Iter 73, disc loss: 0.002029979099378849, policy loss: 8.257474347318427
Experience 15, Iter 74, disc loss: 0.002203653742096585, policy loss: 8.18552623638514
Experience 15, Iter 75, disc loss: 0.002410449222710405, policy loss: 7.966160161475566
Experience 15, Iter 76, disc loss: 0.002237914340982585, policy loss: 7.7688900989058745
Experience 15, Iter 77, disc loss: 0.0017859642560688496, policy loss: 8.603032828525889
Experience 15, Iter 78, disc loss: 0.001880407866668764, policy loss: 8.506520342409077
Experience 15, Iter 79, disc loss: 0.002897560451574453, policy loss: 7.562309005070946
Experience 15, Iter 80, disc loss: 0.002038283221287931, policy loss: 8.29882614113899
Experience 15, Iter 81, disc loss: 0.0022384698202384213, policy loss: 7.718636028531557
Experience 15, Iter 82, disc loss: 0.0023498255976512057, policy loss: 8.343983676744925
Experience 15, Iter 83, disc loss: 0.0020159706941018842, policy loss: 8.457463192116263
Experience 15, Iter 84, disc loss: 0.002340186543057036, policy loss: 8.062706826746831
Experience 15, Iter 85, disc loss: 0.002375151180057558, policy loss: 8.645151224019367
Experience 15, Iter 86, disc loss: 0.002204533410069665, policy loss: 8.250390608650774
Experience 15, Iter 87, disc loss: 0.0017979166210414824, policy loss: 8.732776938120875
Experience 15, Iter 88, disc loss: 0.0021413097171995273, policy loss: 8.098984428459175
Experience 15, Iter 89, disc loss: 0.0019206762043614293, policy loss: 8.597504286706323
Experience 15, Iter 90, disc loss: 0.002347911767331505, policy loss: 8.426943811675779
Experience 15, Iter 91, disc loss: 0.0021388585683949817, policy loss: 8.642992257449313
Experience 15, Iter 92, disc loss: 0.0023018320232195528, policy loss: 7.859628920435973
Experience 15, Iter 93, disc loss: 0.0019299169801532503, policy loss: 7.987590922930014
Experience 15, Iter 94, disc loss: 0.0023545457543345544, policy loss: 7.861904692779857
Experience 15, Iter 95, disc loss: 0.0022227293372932277, policy loss: 7.661812773770626
Experience 15, Iter 96, disc loss: 0.0021849524028875796, policy loss: 7.691309108776187
Experience 15, Iter 97, disc loss: 0.002472397974869185, policy loss: 7.769662990855521
Experience 15, Iter 98, disc loss: 0.0021549224285227512, policy loss: 7.525626256250897
Experience 15, Iter 99, disc loss: 0.0023379508550306887, policy loss: 8.217439748917352
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.0942],
        [1.4057],
        [0.0344]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0139, 0.1593, 1.5680, 0.0271, 0.0234, 2.3177]],

        [[0.0139, 0.1593, 1.5680, 0.0271, 0.0234, 2.3177]],

        [[0.0139, 0.1593, 1.5680, 0.0271, 0.0234, 2.3177]],

        [[0.0139, 0.1593, 1.5680, 0.0271, 0.0234, 2.3177]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0154, 0.3768, 5.6227, 0.1374], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0154, 0.3768, 5.6227, 0.1374])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.998
Iter 2/2000 - Loss: 2.988
Iter 3/2000 - Loss: 2.851
Iter 4/2000 - Loss: 2.825
Iter 5/2000 - Loss: 2.810
Iter 6/2000 - Loss: 2.709
Iter 7/2000 - Loss: 2.604
Iter 8/2000 - Loss: 2.534
Iter 9/2000 - Loss: 2.453
Iter 10/2000 - Loss: 2.322
Iter 11/2000 - Loss: 2.160
Iter 12/2000 - Loss: 1.996
Iter 13/2000 - Loss: 1.830
Iter 14/2000 - Loss: 1.641
Iter 15/2000 - Loss: 1.419
Iter 16/2000 - Loss: 1.172
Iter 17/2000 - Loss: 0.913
Iter 18/2000 - Loss: 0.647
Iter 19/2000 - Loss: 0.370
Iter 20/2000 - Loss: 0.081
Iter 1981/2000 - Loss: -7.188
Iter 1982/2000 - Loss: -7.188
Iter 1983/2000 - Loss: -7.189
Iter 1984/2000 - Loss: -7.189
Iter 1985/2000 - Loss: -7.189
Iter 1986/2000 - Loss: -7.189
Iter 1987/2000 - Loss: -7.189
Iter 1988/2000 - Loss: -7.189
Iter 1989/2000 - Loss: -7.189
Iter 1990/2000 - Loss: -7.189
Iter 1991/2000 - Loss: -7.189
Iter 1992/2000 - Loss: -7.189
Iter 1993/2000 - Loss: -7.189
Iter 1994/2000 - Loss: -7.189
Iter 1995/2000 - Loss: -7.189
Iter 1996/2000 - Loss: -7.189
Iter 1997/2000 - Loss: -7.189
Iter 1998/2000 - Loss: -7.189
Iter 1999/2000 - Loss: -7.189
Iter 2000/2000 - Loss: -7.189
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0025],
        [0.0003]])
Lengthscale: tensor([[[10.5480,  4.5804, 21.6259,  7.2701,  7.5658, 45.9153]],

        [[15.3569, 33.5777,  7.2511,  1.2399,  1.7497, 22.3073]],

        [[17.3674, 33.0572,  7.9152,  0.9670,  0.8615, 18.8232]],

        [[14.2521, 32.1063, 11.5693,  2.4416,  1.7475, 41.0176]]])
Signal Variance: tensor([ 0.0471,  1.3142, 11.1277,  0.4371])
Estimated target variance: tensor([0.0154, 0.3768, 5.6227, 0.1374])
N: 160
Signal to noise ratio: tensor([12.0822, 61.0518, 66.8453, 36.8594])
Bound on condition number: tensor([ 23357.8349, 596373.2431, 714928.8552, 217379.4434])
Policy Optimizer learning rate:
9.843201517785077e-05
Experience 16, Iter 0, disc loss: 0.0022236098036767854, policy loss: 7.938441889159705
Experience 16, Iter 1, disc loss: 0.002325981090763598, policy loss: 8.175455419472913
Experience 16, Iter 2, disc loss: 0.002353987792829483, policy loss: 7.928762017451391
Experience 16, Iter 3, disc loss: 0.002110342995642738, policy loss: 8.448246298597956
Experience 16, Iter 4, disc loss: 0.002087821414404367, policy loss: 7.863325256350821
Experience 16, Iter 5, disc loss: 0.0025685794589002534, policy loss: 8.855833114436916
Experience 16, Iter 6, disc loss: 0.0020952858831986493, policy loss: 7.947216684471542
Experience 16, Iter 7, disc loss: 0.002259372634227064, policy loss: 7.760012826987095
Experience 16, Iter 8, disc loss: 0.002187277753006834, policy loss: 7.768913071294384
Experience 16, Iter 9, disc loss: 0.0021724117067566055, policy loss: 7.853108259567566
Experience 16, Iter 10, disc loss: 0.002433531005682013, policy loss: 8.358186152279135
Experience 16, Iter 11, disc loss: 0.0025469282056481387, policy loss: 7.76967636450556
Experience 16, Iter 12, disc loss: 0.002359850112241552, policy loss: 8.355061712648785
Experience 16, Iter 13, disc loss: 0.0024159004808380984, policy loss: 8.024967783370526
Experience 16, Iter 14, disc loss: 0.0022910557627706203, policy loss: 7.794111984672901
Experience 16, Iter 15, disc loss: 0.002222320713918032, policy loss: 7.904360913018863
Experience 16, Iter 16, disc loss: 0.0022414669936156855, policy loss: 7.708037857867685
Experience 16, Iter 17, disc loss: 0.0024080025189023233, policy loss: 7.75903422657905
Experience 16, Iter 18, disc loss: 0.002704681254360518, policy loss: 7.94829837220211
Experience 16, Iter 19, disc loss: 0.0021571464316463085, policy loss: 7.649787777863962
Experience 16, Iter 20, disc loss: 0.002577405206365639, policy loss: 8.074872289581146
Experience 16, Iter 21, disc loss: 0.002069803937547486, policy loss: 8.269914505694004
Experience 16, Iter 22, disc loss: 0.0021627474929402394, policy loss: 7.935318581399629
Experience 16, Iter 23, disc loss: 0.0025033698344904287, policy loss: 7.822527254957006
Experience 16, Iter 24, disc loss: 0.002309249517859229, policy loss: 8.0221492336547
Experience 16, Iter 25, disc loss: 0.0028809281561818396, policy loss: 7.822602212415971
Experience 16, Iter 26, disc loss: 0.0025596016473376153, policy loss: 8.185829871079868
Experience 16, Iter 27, disc loss: 0.0024011983416595975, policy loss: 8.276323186004294
Experience 16, Iter 28, disc loss: 0.002460140053128527, policy loss: 8.148274661032186
Experience 16, Iter 29, disc loss: 0.0024310099285320625, policy loss: 8.233925614951954
Experience 16, Iter 30, disc loss: 0.002315581970778119, policy loss: 7.825262365369463
Experience 16, Iter 31, disc loss: 0.002125830095620114, policy loss: 8.526855352754623
Experience 16, Iter 32, disc loss: 0.0023857091471906067, policy loss: 7.8633201335519285
Experience 16, Iter 33, disc loss: 0.002447008383416668, policy loss: 8.012145978880955
Experience 16, Iter 34, disc loss: 0.0025401205125730394, policy loss: 8.30024086663939
Experience 16, Iter 35, disc loss: 0.0021844153911260754, policy loss: 7.782239075976081
Experience 16, Iter 36, disc loss: 0.0023692114390463795, policy loss: 7.8964664627766386
Experience 16, Iter 37, disc loss: 0.0026520956228837035, policy loss: 8.042194517947442
Experience 16, Iter 38, disc loss: 0.0021876356069622724, policy loss: 7.773849558722537
Experience 16, Iter 39, disc loss: 0.0022567459250599576, policy loss: 8.240993817881801
Experience 16, Iter 40, disc loss: 0.0027281053105196206, policy loss: 7.560215994368484
Experience 16, Iter 41, disc loss: 0.0023750559582340017, policy loss: 8.333765939162666
Experience 16, Iter 42, disc loss: 0.002212267326166223, policy loss: 8.237929555905279
Experience 16, Iter 43, disc loss: 0.002535679826555469, policy loss: 7.880329933915157
Experience 16, Iter 44, disc loss: 0.002437358288826672, policy loss: 8.389428020899226
Experience 16, Iter 45, disc loss: 0.002437572782062543, policy loss: 7.9220138114348355
Experience 16, Iter 46, disc loss: 0.0024027021258450085, policy loss: 8.569007571379293
Experience 16, Iter 47, disc loss: 0.0025707537894752615, policy loss: 7.573190996498956
Experience 16, Iter 48, disc loss: 0.002296735148809001, policy loss: 8.775353685495144
Experience 16, Iter 49, disc loss: 0.0021911825197417057, policy loss: 8.773321391808608
Experience 16, Iter 50, disc loss: 0.0022907773972404956, policy loss: 8.349697492656466
Experience 16, Iter 51, disc loss: 0.001970348282773793, policy loss: 8.091586967866029
Experience 16, Iter 52, disc loss: 0.002336538248802574, policy loss: 8.612089220318659
Experience 16, Iter 53, disc loss: 0.00293168377084443, policy loss: 7.784421533844231
Experience 16, Iter 54, disc loss: 0.0025735180620618464, policy loss: 7.56042493710204
Experience 16, Iter 55, disc loss: 0.00217570931051548, policy loss: 8.046360403871528
Experience 16, Iter 56, disc loss: 0.002322014511188115, policy loss: 7.9012199218215
Experience 16, Iter 57, disc loss: 0.002961191187114641, policy loss: 7.614206790425433
Experience 16, Iter 58, disc loss: 0.0026700487198605474, policy loss: 8.219041926152194
Experience 16, Iter 59, disc loss: 0.0025797231292613823, policy loss: 8.040871933412534
Experience 16, Iter 60, disc loss: 0.002311005497882, policy loss: 8.274554196340038
Experience 16, Iter 61, disc loss: 0.0021064795992678806, policy loss: 8.65196180099007
Experience 16, Iter 62, disc loss: 0.002730119737856419, policy loss: 8.6965635966639
Experience 16, Iter 63, disc loss: 0.002700783618473163, policy loss: 7.830035804356718
Experience 16, Iter 64, disc loss: 0.0024297133636359968, policy loss: 8.114978084673686
Experience 16, Iter 65, disc loss: 0.0025107626923631, policy loss: 7.564960783979975
Experience 16, Iter 66, disc loss: 0.002729088133105342, policy loss: 7.650990319501231
Experience 16, Iter 67, disc loss: 0.0024386958986952994, policy loss: 8.15072391112413
Experience 16, Iter 68, disc loss: 0.0027064182442945884, policy loss: 7.791339675201307
Experience 16, Iter 69, disc loss: 0.0022177402813277364, policy loss: 9.194174610661301
Experience 16, Iter 70, disc loss: 0.0025853126769495452, policy loss: 7.772621858353196
Experience 16, Iter 71, disc loss: 0.0023010843040267162, policy loss: 7.818738140486688
Experience 16, Iter 72, disc loss: 0.0023831686086385547, policy loss: 8.02429077034317
Experience 16, Iter 73, disc loss: 0.001961872944648925, policy loss: 8.858250178576721
Experience 16, Iter 74, disc loss: 0.0022048047116096803, policy loss: 8.475418302140387
Experience 16, Iter 75, disc loss: 0.002621107633770575, policy loss: 7.998976241058498
Experience 16, Iter 76, disc loss: 0.002154195684134524, policy loss: 8.42874416398031
Experience 16, Iter 77, disc loss: 0.0027613236393654603, policy loss: 7.613024374944469
Experience 16, Iter 78, disc loss: 0.0029264975829810663, policy loss: 7.434712590242758
Experience 16, Iter 79, disc loss: 0.0025341327991200953, policy loss: 7.693325485282045
Experience 16, Iter 80, disc loss: 0.002543685753837888, policy loss: 8.178556312848555
Experience 16, Iter 81, disc loss: 0.0021681838751113126, policy loss: 8.855224879811125
Experience 16, Iter 82, disc loss: 0.0018061565820953495, policy loss: 8.635885768981215
Experience 16, Iter 83, disc loss: 0.00251632742525682, policy loss: 7.596809134723797
Experience 16, Iter 84, disc loss: 0.0025481120759702925, policy loss: 7.668336805752626
Experience 16, Iter 85, disc loss: 0.0027184340281356753, policy loss: 8.318664171681888
Experience 16, Iter 86, disc loss: 0.0021766784296726347, policy loss: 8.816559037960635
Experience 16, Iter 87, disc loss: 0.0022868400762286645, policy loss: 8.054075831422558
Experience 16, Iter 88, disc loss: 0.002284150004164108, policy loss: 8.711946440758167
Experience 16, Iter 89, disc loss: 0.002451707259439712, policy loss: 8.010842927091181
Experience 16, Iter 90, disc loss: 0.0021968073769910506, policy loss: 8.138003772366694
Experience 16, Iter 91, disc loss: 0.0024534789278399674, policy loss: 7.795465623885834
Experience 16, Iter 92, disc loss: 0.0021248928005020737, policy loss: 8.348373554954748
Experience 16, Iter 93, disc loss: 0.00275879682481357, policy loss: 7.999808238663654
Experience 16, Iter 94, disc loss: 0.0021173631584689868, policy loss: 8.617444689065934
Experience 16, Iter 95, disc loss: 0.0024133105784387232, policy loss: 7.72346525435665
Experience 16, Iter 96, disc loss: 0.0022033705995325014, policy loss: 7.833281336811254
Experience 16, Iter 97, disc loss: 0.0026328283274819056, policy loss: 7.714293182747724
Experience 16, Iter 98, disc loss: 0.0028076558790389927, policy loss: 7.5544325841932505
Experience 16, Iter 99, disc loss: 0.0022824616142989858, policy loss: 8.198681293408422
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0038],
        [0.1017],
        [1.4480],
        [0.0352]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0137, 0.1604, 1.6156, 0.0278, 0.0244, 2.4719]],

        [[0.0137, 0.1604, 1.6156, 0.0278, 0.0244, 2.4719]],

        [[0.0137, 0.1604, 1.6156, 0.0278, 0.0244, 2.4719]],

        [[0.0137, 0.1604, 1.6156, 0.0278, 0.0244, 2.4719]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0154, 0.4068, 5.7920, 0.1410], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0154, 0.4068, 5.7920, 0.1410])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.066
Iter 2/2000 - Loss: 3.071
Iter 3/2000 - Loss: 2.910
Iter 4/2000 - Loss: 2.891
Iter 5/2000 - Loss: 2.874
Iter 6/2000 - Loss: 2.760
Iter 7/2000 - Loss: 2.637
Iter 8/2000 - Loss: 2.549
Iter 9/2000 - Loss: 2.458
Iter 10/2000 - Loss: 2.319
Iter 11/2000 - Loss: 2.143
Iter 12/2000 - Loss: 1.961
Iter 13/2000 - Loss: 1.778
Iter 14/2000 - Loss: 1.578
Iter 15/2000 - Loss: 1.348
Iter 16/2000 - Loss: 1.094
Iter 17/2000 - Loss: 0.829
Iter 18/2000 - Loss: 0.559
Iter 19/2000 - Loss: 0.282
Iter 20/2000 - Loss: -0.006
Iter 1981/2000 - Loss: -7.285
Iter 1982/2000 - Loss: -7.285
Iter 1983/2000 - Loss: -7.285
Iter 1984/2000 - Loss: -7.285
Iter 1985/2000 - Loss: -7.285
Iter 1986/2000 - Loss: -7.285
Iter 1987/2000 - Loss: -7.285
Iter 1988/2000 - Loss: -7.285
Iter 1989/2000 - Loss: -7.285
Iter 1990/2000 - Loss: -7.285
Iter 1991/2000 - Loss: -7.285
Iter 1992/2000 - Loss: -7.285
Iter 1993/2000 - Loss: -7.285
Iter 1994/2000 - Loss: -7.285
Iter 1995/2000 - Loss: -7.285
Iter 1996/2000 - Loss: -7.285
Iter 1997/2000 - Loss: -7.285
Iter 1998/2000 - Loss: -7.285
Iter 1999/2000 - Loss: -7.285
Iter 2000/2000 - Loss: -7.285
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.8541,  5.2950, 18.3992,  8.3434, 10.2041, 50.3791]],

        [[15.7846, 33.5234,  7.6229,  1.2240,  1.5834, 22.8904]],

        [[17.2747, 32.8325,  8.1130,  0.9318,  0.8684, 20.0491]],

        [[14.3798, 31.6210, 11.6165,  2.4027,  1.7161, 40.8516]]])
Signal Variance: tensor([ 0.0557,  1.3986, 11.7320,  0.4273])
Estimated target variance: tensor([0.0154, 0.4068, 5.7920, 0.1410])
N: 170
Signal to noise ratio: tensor([12.8877, 63.0849, 70.8891, 37.1150])
Bound on condition number: tensor([ 28236.5646, 676551.3169, 854296.4795, 234179.3329])
Policy Optimizer learning rate:
9.832836131379853e-05
Experience 17, Iter 0, disc loss: 0.0027135492140417565, policy loss: 7.692790926263463
Experience 17, Iter 1, disc loss: 0.002263116298149337, policy loss: 8.156090496422587
Experience 17, Iter 2, disc loss: 0.002237236828182069, policy loss: 7.849742000877217
Experience 17, Iter 3, disc loss: 0.0024570440748923005, policy loss: 7.808264353402436
Experience 17, Iter 4, disc loss: 0.0021852911786530505, policy loss: 8.354840118150324
Experience 17, Iter 5, disc loss: 0.002456907460582923, policy loss: 7.777917371068723
Experience 17, Iter 6, disc loss: 0.002222081927372225, policy loss: 8.04803601919067
Experience 17, Iter 7, disc loss: 0.002326813610358094, policy loss: 8.95688831813986
Experience 17, Iter 8, disc loss: 0.0031895413485981605, policy loss: 8.148345368051354
Experience 17, Iter 9, disc loss: 0.002753669730316018, policy loss: 7.634794824614308
Experience 17, Iter 10, disc loss: 0.002216653954666142, policy loss: 8.914470824739622
Experience 17, Iter 11, disc loss: 0.002225142232917812, policy loss: 8.28778170115691
Experience 17, Iter 12, disc loss: 0.0021791635896828507, policy loss: 8.281166333416573
Experience 17, Iter 13, disc loss: 0.002345994493321027, policy loss: 8.748261799957165
Experience 17, Iter 14, disc loss: 0.0022135132923815277, policy loss: 7.966353581175341
Experience 17, Iter 15, disc loss: 0.0023344115505214316, policy loss: 7.890023021626426
Experience 17, Iter 16, disc loss: 0.0028385304117625873, policy loss: 7.675928350230032
Experience 17, Iter 17, disc loss: 0.002555081916922163, policy loss: 7.991752053395397
Experience 17, Iter 18, disc loss: 0.002785524588279223, policy loss: 7.627300370272809
Experience 17, Iter 19, disc loss: 0.0029441879760114115, policy loss: 8.437510520118826
Experience 17, Iter 20, disc loss: 0.002509662540156175, policy loss: 8.2477009763255
Experience 17, Iter 21, disc loss: 0.002230313482513925, policy loss: 8.467362960397802
Experience 17, Iter 22, disc loss: 0.0026573390739439936, policy loss: 8.200615565136994
Experience 17, Iter 23, disc loss: 0.00236647439665591, policy loss: 8.923096465067653
Experience 17, Iter 24, disc loss: 0.002763018046771855, policy loss: 7.787429510516003
Experience 17, Iter 25, disc loss: 0.00234334200871235, policy loss: 7.8451608540782445
Experience 17, Iter 26, disc loss: 0.002271122372137796, policy loss: 8.051559003349585
Experience 17, Iter 27, disc loss: 0.002266082685584358, policy loss: 8.188798201662898
Experience 17, Iter 28, disc loss: 0.002396675770734037, policy loss: 8.385566067135711
Experience 17, Iter 29, disc loss: 0.0028388906192994352, policy loss: 7.891681107562555
Experience 17, Iter 30, disc loss: 0.0021207584109400315, policy loss: 9.313303444930128
Experience 17, Iter 31, disc loss: 0.002644166919917729, policy loss: 7.879703782419027
Experience 17, Iter 32, disc loss: 0.002767601783291565, policy loss: 8.440549507567063
Experience 17, Iter 33, disc loss: 0.002577373655671891, policy loss: 8.48345741114806
Experience 17, Iter 34, disc loss: 0.0028391985353955628, policy loss: 7.935830795143637
Experience 17, Iter 35, disc loss: 0.0020966179329409925, policy loss: 9.073932140230687
Experience 17, Iter 36, disc loss: 0.002890258798061638, policy loss: 7.65719437453601
Experience 17, Iter 37, disc loss: 0.0024378898991785384, policy loss: 8.951105453127662
Experience 17, Iter 38, disc loss: 0.0022544581528970904, policy loss: 8.56605401187656
Experience 17, Iter 39, disc loss: 0.0023196319715050673, policy loss: 7.938133723904533
Experience 17, Iter 40, disc loss: 0.0025149718606237224, policy loss: 8.601866499747704
Experience 17, Iter 41, disc loss: 0.0021110797325191674, policy loss: 9.546624483426953
Experience 17, Iter 42, disc loss: 0.0023348373522939685, policy loss: 7.756372785114567
Experience 17, Iter 43, disc loss: 0.002571365939063009, policy loss: 8.187797789750595
Experience 17, Iter 44, disc loss: 0.0020082605440088466, policy loss: 8.798213841102385
Experience 17, Iter 45, disc loss: 0.001910340336087038, policy loss: 9.227269669118328
Experience 17, Iter 46, disc loss: 0.00195245110819637, policy loss: 9.399645099581669
Experience 17, Iter 47, disc loss: 0.0022225055634783296, policy loss: 7.80053152606979
Experience 17, Iter 48, disc loss: 0.0021405764149172403, policy loss: 9.065273398619578
Experience 17, Iter 49, disc loss: 0.002123474359555428, policy loss: 8.642148145405036
Experience 17, Iter 50, disc loss: 0.002850106898778401, policy loss: 8.302437265581712
Experience 17, Iter 51, disc loss: 0.002793513898420294, policy loss: 8.567321272580777
Experience 17, Iter 52, disc loss: 0.0024929844445890368, policy loss: 7.646484483940284
Experience 17, Iter 53, disc loss: 0.0023914826003854557, policy loss: 8.337690842548357
Experience 17, Iter 54, disc loss: 0.002140074810983431, policy loss: 8.39258699130105
Experience 17, Iter 55, disc loss: 0.0025158116167185956, policy loss: 8.26250208032077
Experience 17, Iter 56, disc loss: 0.0019565866935258858, policy loss: 8.784568709218679
Experience 17, Iter 57, disc loss: 0.002448301630094392, policy loss: 8.896206759456984
Experience 17, Iter 58, disc loss: 0.0024280773357911057, policy loss: 8.962536418269412
Experience 17, Iter 59, disc loss: 0.0022116995896845027, policy loss: 8.79876727253921
Experience 17, Iter 60, disc loss: 0.002313347987400679, policy loss: 8.709264555394556
Experience 17, Iter 61, disc loss: 0.0022520059673906594, policy loss: 8.842128696205005
Experience 17, Iter 62, disc loss: 0.002190382291876915, policy loss: 8.23655396205398
Experience 17, Iter 63, disc loss: 0.00210018230951428, policy loss: 8.393763884164212
Experience 17, Iter 64, disc loss: 0.0024644207517980476, policy loss: 8.46996344249387
Experience 17, Iter 65, disc loss: 0.0020354978912135403, policy loss: 9.016433855115533
Experience 17, Iter 66, disc loss: 0.0019869714687482517, policy loss: 8.845736224683796
Experience 17, Iter 67, disc loss: 0.0024445378637481304, policy loss: 8.21892584214272
Experience 17, Iter 68, disc loss: 0.002596620988075668, policy loss: 7.873147885988243
Experience 17, Iter 69, disc loss: 0.0019490647269515635, policy loss: 8.197995154773514
Experience 17, Iter 70, disc loss: 0.0021610861798899983, policy loss: 8.084389570905019
Experience 17, Iter 71, disc loss: 0.0024191057229575154, policy loss: 8.893190681651394
Experience 17, Iter 72, disc loss: 0.002015316727171428, policy loss: 8.544338474035563
Experience 17, Iter 73, disc loss: 0.0018647237504998968, policy loss: 9.430199177762942
Experience 17, Iter 74, disc loss: 0.002562775381203206, policy loss: 7.437790664502927
Experience 17, Iter 75, disc loss: 0.0023186588763578157, policy loss: 8.601404994556184
Experience 17, Iter 76, disc loss: 0.0024593789057503544, policy loss: 7.9149934826111545
Experience 17, Iter 77, disc loss: 0.0017203585280576234, policy loss: 8.68733198318742
Experience 17, Iter 78, disc loss: 0.001867223193315394, policy loss: 8.647063125555555
Experience 17, Iter 79, disc loss: 0.002120828614744893, policy loss: 8.389146450356924
Experience 17, Iter 80, disc loss: 0.0019375349931251796, policy loss: 8.60003480241944
Experience 17, Iter 81, disc loss: 0.0021326785557645566, policy loss: 8.767440017847548
Experience 17, Iter 82, disc loss: 0.0020969267452028966, policy loss: 8.879525086251526
Experience 17, Iter 83, disc loss: 0.002179665993649509, policy loss: 8.144427920862228
Experience 17, Iter 84, disc loss: 0.0019351090102402789, policy loss: 9.909696808033516
Experience 17, Iter 85, disc loss: 0.002652435359528948, policy loss: 8.238867190474929
Experience 17, Iter 86, disc loss: 0.0023878832839570813, policy loss: 8.162249821119103
Experience 17, Iter 87, disc loss: 0.0020750218147089243, policy loss: 8.224440986133526
Experience 17, Iter 88, disc loss: 0.0019469655771558393, policy loss: 9.002320523328725
Experience 17, Iter 89, disc loss: 0.0017381336584417302, policy loss: 9.242098593659122
Experience 17, Iter 90, disc loss: 0.0018131333076032736, policy loss: 8.993126144852331
Experience 17, Iter 91, disc loss: 0.0019144869213167206, policy loss: 8.279288878507895
Experience 17, Iter 92, disc loss: 0.001636742258635507, policy loss: 9.049510822309982
Experience 17, Iter 93, disc loss: 0.0021055907706222284, policy loss: 8.139318947382309
Experience 17, Iter 94, disc loss: 0.002036427959718129, policy loss: 8.117165036684815
Experience 17, Iter 95, disc loss: 0.0017693226328986378, policy loss: 9.500165626676152
Experience 17, Iter 96, disc loss: 0.0020341083712200362, policy loss: 8.605659414853339
Experience 17, Iter 97, disc loss: 0.0018662557298430144, policy loss: 8.82512442487898
Experience 17, Iter 98, disc loss: 0.0016112156763511709, policy loss: 8.984908894091166
Experience 17, Iter 99, disc loss: 0.001659304593475688, policy loss: 8.974234123979109
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1124],
        [1.5097],
        [0.0369]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0133, 0.1624, 1.6912, 0.0287, 0.0255, 2.6633]],

        [[0.0133, 0.1624, 1.6912, 0.0287, 0.0255, 2.6633]],

        [[0.0133, 0.1624, 1.6912, 0.0287, 0.0255, 2.6633]],

        [[0.0133, 0.1624, 1.6912, 0.0287, 0.0255, 2.6633]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0156, 0.4496, 6.0390, 0.1477], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0156, 0.4496, 6.0390, 0.1477])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.159
Iter 2/2000 - Loss: 3.171
Iter 3/2000 - Loss: 2.992
Iter 4/2000 - Loss: 2.974
Iter 5/2000 - Loss: 2.959
Iter 6/2000 - Loss: 2.839
Iter 7/2000 - Loss: 2.704
Iter 8/2000 - Loss: 2.605
Iter 9/2000 - Loss: 2.507
Iter 10/2000 - Loss: 2.363
Iter 11/2000 - Loss: 2.178
Iter 12/2000 - Loss: 1.979
Iter 13/2000 - Loss: 1.779
Iter 14/2000 - Loss: 1.565
Iter 15/2000 - Loss: 1.325
Iter 16/2000 - Loss: 1.061
Iter 17/2000 - Loss: 0.786
Iter 18/2000 - Loss: 0.507
Iter 19/2000 - Loss: 0.224
Iter 20/2000 - Loss: -0.067
Iter 1981/2000 - Loss: -7.337
Iter 1982/2000 - Loss: -7.337
Iter 1983/2000 - Loss: -7.337
Iter 1984/2000 - Loss: -7.337
Iter 1985/2000 - Loss: -7.337
Iter 1986/2000 - Loss: -7.337
Iter 1987/2000 - Loss: -7.337
Iter 1988/2000 - Loss: -7.337
Iter 1989/2000 - Loss: -7.337
Iter 1990/2000 - Loss: -7.337
Iter 1991/2000 - Loss: -7.337
Iter 1992/2000 - Loss: -7.337
Iter 1993/2000 - Loss: -7.337
Iter 1994/2000 - Loss: -7.337
Iter 1995/2000 - Loss: -7.337
Iter 1996/2000 - Loss: -7.337
Iter 1997/2000 - Loss: -7.338
Iter 1998/2000 - Loss: -7.338
Iter 1999/2000 - Loss: -7.338
Iter 2000/2000 - Loss: -7.338
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.9176,  5.3238, 22.2043,  7.7656,  8.6926, 48.5571]],

        [[15.3506, 33.7117,  7.5116,  1.2535,  1.6015, 24.1872]],

        [[17.0543, 32.9975,  8.1482,  0.8954,  0.8673, 20.8367]],

        [[13.7904, 30.6704, 12.2294,  2.5091,  1.6763, 43.0808]]])
Signal Variance: tensor([ 0.0587,  1.5205, 11.7881,  0.4766])
Estimated target variance: tensor([0.0156, 0.4496, 6.0390, 0.1477])
N: 180
Signal to noise ratio: tensor([13.1358, 66.1119, 72.2468, 38.5386])
Bound on condition number: tensor([ 31059.9138, 786740.9047, 939529.8506, 267341.7405])
Policy Optimizer learning rate:
9.822481660247992e-05
Experience 18, Iter 0, disc loss: 0.0022277435496726684, policy loss: 8.21768932036664
Experience 18, Iter 1, disc loss: 0.0020883886633736126, policy loss: 9.086131492434257
Experience 18, Iter 2, disc loss: 0.0024232329283617586, policy loss: 7.690194804479564
Experience 18, Iter 3, disc loss: 0.0020833044206429023, policy loss: 8.566805084250642
Experience 18, Iter 4, disc loss: 0.0018920062541341212, policy loss: 9.245382669564844
Experience 18, Iter 5, disc loss: 0.0025712280066327348, policy loss: 8.408447661153932
Experience 18, Iter 6, disc loss: 0.0024093624017405236, policy loss: 8.557712834104136
Experience 18, Iter 7, disc loss: 0.0023623713249136978, policy loss: 8.513050979765922
Experience 18, Iter 8, disc loss: 0.002210539398698578, policy loss: 9.25505005351804
Experience 18, Iter 9, disc loss: 0.0021465905653065233, policy loss: 9.13686046875834
Experience 18, Iter 10, disc loss: 0.001687441156519435, policy loss: 9.106074004982457
Experience 18, Iter 11, disc loss: 0.00199537016523578, policy loss: 8.914255629691505
Experience 18, Iter 12, disc loss: 0.0018967238129859875, policy loss: 8.540615197548565
Experience 18, Iter 13, disc loss: 0.0022296539769910426, policy loss: 8.705309828217192
Experience 18, Iter 14, disc loss: 0.001986703382800053, policy loss: 9.5909671098212
Experience 18, Iter 15, disc loss: 0.0021829769928246347, policy loss: 8.893629966210938
Experience 18, Iter 16, disc loss: 0.002021158387990225, policy loss: 9.302775606978585
Experience 18, Iter 17, disc loss: 0.002344395835283372, policy loss: 8.222423629029398
Experience 18, Iter 18, disc loss: 0.00172004286661405, policy loss: 8.86664874220052
Experience 18, Iter 19, disc loss: 0.0018915412744705093, policy loss: 8.495910373362175
Experience 18, Iter 20, disc loss: 0.002796136832555656, policy loss: 8.59969841873031
Experience 18, Iter 21, disc loss: 0.001906153065563181, policy loss: 8.411123852756543
Experience 18, Iter 22, disc loss: 0.0020287667907168544, policy loss: 8.693819414575646
Experience 18, Iter 23, disc loss: 0.002227991778744677, policy loss: 7.882728445830524
Experience 18, Iter 24, disc loss: 0.0017914785716568386, policy loss: 9.30442547783517
Experience 18, Iter 25, disc loss: 0.002298617474598734, policy loss: 9.035599711610715
Experience 18, Iter 26, disc loss: 0.0018434613548053073, policy loss: 8.868520872953242
Experience 18, Iter 27, disc loss: 0.002016245691629228, policy loss: 9.03491921525988
Experience 18, Iter 28, disc loss: 0.0015280966022965443, policy loss: 9.032506015274368
Experience 18, Iter 29, disc loss: 0.0019716407685182206, policy loss: 9.76534643890156
Experience 18, Iter 30, disc loss: 0.0018170473296611844, policy loss: 8.618676243395917
Experience 18, Iter 31, disc loss: 0.0014480451903314917, policy loss: 10.215810827097224
Experience 18, Iter 32, disc loss: 0.0017807356330943012, policy loss: 8.384003948937838
Experience 18, Iter 33, disc loss: 0.001697144990192316, policy loss: 9.103805697062242
Experience 18, Iter 34, disc loss: 0.0017467372620828025, policy loss: 8.646558046442463
Experience 18, Iter 35, disc loss: 0.0016984865859085087, policy loss: 8.933131342756377
Experience 18, Iter 36, disc loss: 0.0017638658727095091, policy loss: 8.392031976094781
Experience 18, Iter 37, disc loss: 0.002006290125121018, policy loss: 9.097417447248835
Experience 18, Iter 38, disc loss: 0.0014655253444284446, policy loss: 9.838848479566641
Experience 18, Iter 39, disc loss: 0.0017183067699259099, policy loss: 9.892156763969773
Experience 18, Iter 40, disc loss: 0.001472641468650759, policy loss: 9.785072351888768
Experience 18, Iter 41, disc loss: 0.002298574445561223, policy loss: 8.730184358588081
Experience 18, Iter 42, disc loss: 0.001604607396070214, policy loss: 9.210007874537737
Experience 18, Iter 43, disc loss: 0.0015519587245884773, policy loss: 8.947311995528038
Experience 18, Iter 44, disc loss: 0.0016684037718543766, policy loss: 8.619077319343884
Experience 18, Iter 45, disc loss: 0.0014556223726311934, policy loss: 8.58743294624696
Experience 18, Iter 46, disc loss: 0.001697147091909738, policy loss: 9.147344951105955
Experience 18, Iter 47, disc loss: 0.002012545283312528, policy loss: 8.645292229435942
Experience 18, Iter 48, disc loss: 0.0019475054815581956, policy loss: 8.69156967063905
Experience 18, Iter 49, disc loss: 0.001517684667859243, policy loss: 8.743073316125962
Experience 18, Iter 50, disc loss: 0.0014697480829604878, policy loss: 9.053947926915288
Experience 18, Iter 51, disc loss: 0.0015633522693894745, policy loss: 8.95472408681534
Experience 18, Iter 52, disc loss: 0.0015999523466877223, policy loss: 8.874897021300571
Experience 18, Iter 53, disc loss: 0.0020301292335596346, policy loss: 9.029436332140008
Experience 18, Iter 54, disc loss: 0.0015629895435903334, policy loss: 9.53577879169016
Experience 18, Iter 55, disc loss: 0.0016461435641080123, policy loss: 8.247333889847948
Experience 18, Iter 56, disc loss: 0.0015347934325234778, policy loss: 9.924509104461695
Experience 18, Iter 57, disc loss: 0.0018536207348495261, policy loss: 8.864423222759115
Experience 18, Iter 58, disc loss: 0.0016112160760449606, policy loss: 9.569461196944482
Experience 18, Iter 59, disc loss: 0.0016712679237119747, policy loss: 9.70276715266083
Experience 18, Iter 60, disc loss: 0.0017355854942322584, policy loss: 9.275361579031944
Experience 18, Iter 61, disc loss: 0.0014844573882496265, policy loss: 9.13190146330489
Experience 18, Iter 62, disc loss: 0.0012959498701620295, policy loss: 9.108551181698708
Experience 18, Iter 63, disc loss: 0.0014128887939254898, policy loss: 9.569937686902982
Experience 18, Iter 64, disc loss: 0.0017296864311022158, policy loss: 8.356043363137797
Experience 18, Iter 65, disc loss: 0.0016731897066225708, policy loss: 8.659119760782623
Experience 18, Iter 66, disc loss: 0.0014948684404447622, policy loss: 9.500171425907117
Experience 18, Iter 67, disc loss: 0.0015151821030302255, policy loss: 9.349508151743263
Experience 18, Iter 68, disc loss: 0.001509539736996283, policy loss: 9.532868807094768
Experience 18, Iter 69, disc loss: 0.0015539946803140988, policy loss: 9.44386583371135
Experience 18, Iter 70, disc loss: 0.0016685241272784777, policy loss: 8.098614634586442
Experience 18, Iter 71, disc loss: 0.0017177256446473106, policy loss: 8.695303769310156
Experience 18, Iter 72, disc loss: 0.0017798634946159667, policy loss: 8.766966582060652
Experience 18, Iter 73, disc loss: 0.0014519614537476908, policy loss: 9.101464157049595
Experience 18, Iter 74, disc loss: 0.0013367277530687812, policy loss: 8.748000027969507
Experience 18, Iter 75, disc loss: 0.0012620912481479873, policy loss: 9.152963273388977
Experience 18, Iter 76, disc loss: 0.0014073941210912134, policy loss: 9.744603662123312
Experience 18, Iter 77, disc loss: 0.0016418412155645452, policy loss: 9.185529903446207
Experience 18, Iter 78, disc loss: 0.0013354373498664683, policy loss: 9.395627899386977
Experience 18, Iter 79, disc loss: 0.0013679659915783425, policy loss: 8.881790704169866
Experience 18, Iter 80, disc loss: 0.0012893836175626467, policy loss: 9.582098225892839
Experience 18, Iter 81, disc loss: 0.001463704616130474, policy loss: 9.13307926856755
Experience 18, Iter 82, disc loss: 0.0012120177164187556, policy loss: 9.255639467218888
Experience 18, Iter 83, disc loss: 0.0015015492666332337, policy loss: 8.57367638457735
Experience 18, Iter 84, disc loss: 0.001407101169284915, policy loss: 8.671543794557259
Experience 18, Iter 85, disc loss: 0.0018297352574918996, policy loss: 8.562478066724498
Experience 18, Iter 86, disc loss: 0.0016090815684218082, policy loss: 8.74828143967796
Experience 18, Iter 87, disc loss: 0.0016880656487749611, policy loss: 8.53249353390435
Experience 18, Iter 88, disc loss: 0.0014962177460070415, policy loss: 9.038565124155156
Experience 18, Iter 89, disc loss: 0.001269980524759858, policy loss: 8.869835754445926
Experience 18, Iter 90, disc loss: 0.001511217408431204, policy loss: 10.058743434671264
Experience 18, Iter 91, disc loss: 0.0012411404962457263, policy loss: 9.743368835270564
Experience 18, Iter 92, disc loss: 0.0021134102992689105, policy loss: 11.007156414127824
Experience 18, Iter 93, disc loss: 0.0014447221255258387, policy loss: 9.253828214095147
Experience 18, Iter 94, disc loss: 0.0016062920456133682, policy loss: 9.184398901860186
Experience 18, Iter 95, disc loss: 0.0014985353500314605, policy loss: 9.221347437428964
Experience 18, Iter 96, disc loss: 0.0012032130411653568, policy loss: 10.030033191738088
Experience 18, Iter 97, disc loss: 0.0018628312305764587, policy loss: 8.440593118495924
Experience 18, Iter 98, disc loss: 0.0013569975280067663, policy loss: 10.197325331000568
Experience 18, Iter 99, disc loss: 0.0013751714797850226, policy loss: 9.671243771324058
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1241],
        [1.5758],
        [0.0384]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0128, 0.1642, 1.7609, 0.0297, 0.0261, 2.8755]],

        [[0.0128, 0.1642, 1.7609, 0.0297, 0.0261, 2.8755]],

        [[0.0128, 0.1642, 1.7609, 0.0297, 0.0261, 2.8755]],

        [[0.0128, 0.1642, 1.7609, 0.0297, 0.0261, 2.8755]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.4962, 6.3030, 0.1535], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.4962, 6.3030, 0.1535])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.197
Iter 2/2000 - Loss: 3.208
Iter 3/2000 - Loss: 3.004
Iter 4/2000 - Loss: 2.976
Iter 5/2000 - Loss: 2.953
Iter 6/2000 - Loss: 2.821
Iter 7/2000 - Loss: 2.671
Iter 8/2000 - Loss: 2.559
Iter 9/2000 - Loss: 2.455
Iter 10/2000 - Loss: 2.305
Iter 11/2000 - Loss: 2.112
Iter 12/2000 - Loss: 1.904
Iter 13/2000 - Loss: 1.697
Iter 14/2000 - Loss: 1.480
Iter 15/2000 - Loss: 1.240
Iter 16/2000 - Loss: 0.975
Iter 17/2000 - Loss: 0.698
Iter 18/2000 - Loss: 0.419
Iter 19/2000 - Loss: 0.137
Iter 20/2000 - Loss: -0.151
Iter 1981/2000 - Loss: -7.439
Iter 1982/2000 - Loss: -7.439
Iter 1983/2000 - Loss: -7.439
Iter 1984/2000 - Loss: -7.439
Iter 1985/2000 - Loss: -7.439
Iter 1986/2000 - Loss: -7.439
Iter 1987/2000 - Loss: -7.439
Iter 1988/2000 - Loss: -7.439
Iter 1989/2000 - Loss: -7.439
Iter 1990/2000 - Loss: -7.439
Iter 1991/2000 - Loss: -7.439
Iter 1992/2000 - Loss: -7.439
Iter 1993/2000 - Loss: -7.439
Iter 1994/2000 - Loss: -7.439
Iter 1995/2000 - Loss: -7.439
Iter 1996/2000 - Loss: -7.439
Iter 1997/2000 - Loss: -7.439
Iter 1998/2000 - Loss: -7.439
Iter 1999/2000 - Loss: -7.439
Iter 2000/2000 - Loss: -7.440
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.8527,  5.1943, 23.6544,  6.4747,  7.9787, 47.2369]],

        [[14.8479, 33.2426,  7.4374,  1.2544,  1.7384, 24.7894]],

        [[17.0442, 32.7232,  7.9830,  0.9429,  0.8931, 20.8282]],

        [[13.2783, 29.6025, 12.1827,  2.5017,  1.6718, 43.8869]]])
Signal Variance: tensor([ 0.0537,  1.5943, 12.2327,  0.4911])
Estimated target variance: tensor([0.0157, 0.4962, 6.3030, 0.1535])
N: 190
Signal to noise ratio: tensor([12.6648, 68.6353, 74.6635, 39.8441])
Bound on condition number: tensor([  30476.4281,  895053.9526, 1059182.6423,  301636.2314])
Policy Optimizer learning rate:
9.812138092895162e-05
Experience 19, Iter 0, disc loss: 0.0014087428215242108, policy loss: 8.84128891405677
Experience 19, Iter 1, disc loss: 0.0014215657260504225, policy loss: 9.130631939549117
Experience 19, Iter 2, disc loss: 0.0013395834215980823, policy loss: 9.660824310030169
Experience 19, Iter 3, disc loss: 0.001113018892905067, policy loss: 9.982845604299072
Experience 19, Iter 4, disc loss: 0.001159667388816037, policy loss: 9.823342891213045
Experience 19, Iter 5, disc loss: 0.0014033500563451185, policy loss: 9.018903009876585
Experience 19, Iter 6, disc loss: 0.0013977753720451418, policy loss: 9.035559067801248
Experience 19, Iter 7, disc loss: 0.001640880969430592, policy loss: 8.276446514686043
Experience 19, Iter 8, disc loss: 0.0011105357874062703, policy loss: 9.260176870261866
Experience 19, Iter 9, disc loss: 0.001219782648453517, policy loss: 9.75287445272533
Experience 19, Iter 10, disc loss: 0.001354085183035387, policy loss: 8.795155906947915
Experience 19, Iter 11, disc loss: 0.0015117404377291404, policy loss: 9.243973832763075
Experience 19, Iter 12, disc loss: 0.0013223213819280972, policy loss: 8.784957499287039
Experience 19, Iter 13, disc loss: 0.001406989830137628, policy loss: 9.604601073823375
Experience 19, Iter 14, disc loss: 0.0011091255435099012, policy loss: 8.783312301775421
Experience 19, Iter 15, disc loss: 0.0013639499377171391, policy loss: 9.100616607678415
Experience 19, Iter 16, disc loss: 0.0016297444482634288, policy loss: 8.366059653881708
Experience 19, Iter 17, disc loss: 0.0011823357285833346, policy loss: 10.101781575366807
Experience 19, Iter 18, disc loss: 0.0013591785701084652, policy loss: 9.397810226731877
Experience 19, Iter 19, disc loss: 0.0011477561506864898, policy loss: 9.60169034632212
Experience 19, Iter 20, disc loss: 0.0011582173177283922, policy loss: 10.132873794041128
Experience 19, Iter 21, disc loss: 0.0013155122524121142, policy loss: 9.403865016388771
Experience 19, Iter 22, disc loss: 0.001511532794028958, policy loss: 9.350769486664875
Experience 19, Iter 23, disc loss: 0.0013955121922789044, policy loss: 8.973693197656946
Experience 19, Iter 24, disc loss: 0.001219665347752826, policy loss: 8.792415653763685
Experience 19, Iter 25, disc loss: 0.0012117468145543863, policy loss: 9.28574975946562
Experience 19, Iter 26, disc loss: 0.0012398985506429584, policy loss: 9.498047013644442
Experience 19, Iter 27, disc loss: 0.0017356110157836625, policy loss: 8.732283216184646
Experience 19, Iter 28, disc loss: 0.0011856156310227177, policy loss: 10.701830526399275
Experience 19, Iter 29, disc loss: 0.0009112688807943375, policy loss: 10.671595940400447
Experience 19, Iter 30, disc loss: 0.00125891723875798, policy loss: 9.201789261455625
Experience 19, Iter 31, disc loss: 0.0014948272545805997, policy loss: 9.332362681373372
Experience 19, Iter 32, disc loss: 0.0011027873658072261, policy loss: 9.170606837740591
Experience 19, Iter 33, disc loss: 0.0009980527650622017, policy loss: 10.47462404726767
Experience 19, Iter 34, disc loss: 0.0012774158359615977, policy loss: 10.421079285309396
Experience 19, Iter 35, disc loss: 0.0014554242608819886, policy loss: 8.740782365182728
Experience 19, Iter 36, disc loss: 0.0012118352755363976, policy loss: 10.089530272350563
Experience 19, Iter 37, disc loss: 0.0010848799291897607, policy loss: 10.030491325807581
Experience 19, Iter 38, disc loss: 0.0011278623264109363, policy loss: 9.934057683603779
Experience 19, Iter 39, disc loss: 0.001041350887425664, policy loss: 9.56778635216497
Experience 19, Iter 40, disc loss: 0.0012645677438344756, policy loss: 8.837164737965379
Experience 19, Iter 41, disc loss: 0.00112879198679765, policy loss: 10.084850638536473
Experience 19, Iter 42, disc loss: 0.001288091083665218, policy loss: 9.215817730386915
Experience 19, Iter 43, disc loss: 0.001088218616225759, policy loss: 9.600240889423567
Experience 19, Iter 44, disc loss: 0.0011904052928661132, policy loss: 9.236034145098142
Experience 19, Iter 45, disc loss: 0.0009684384945078648, policy loss: 9.288572363139023
Experience 19, Iter 46, disc loss: 0.0011372975809108648, policy loss: 9.558926684722625
Experience 19, Iter 47, disc loss: 0.0012209697595321144, policy loss: 10.053972095212465
Experience 19, Iter 48, disc loss: 0.0008885385019382123, policy loss: 10.4975225975974
Experience 19, Iter 49, disc loss: 0.001063037486977116, policy loss: 10.143268915073975
Experience 19, Iter 50, disc loss: 0.0010077861028295223, policy loss: 9.448061417038328
Experience 19, Iter 51, disc loss: 0.0012479849175204656, policy loss: 10.065785481344827
Experience 19, Iter 52, disc loss: 0.0011030984240267745, policy loss: 9.554274119294142
Experience 19, Iter 53, disc loss: 0.0012944327332566866, policy loss: 8.867871507848271
Experience 19, Iter 54, disc loss: 0.0009137061421240097, policy loss: 10.222582563429281
Experience 19, Iter 55, disc loss: 0.0008650111013802989, policy loss: 9.48304608569914
Experience 19, Iter 56, disc loss: 0.0010748206653080333, policy loss: 9.303306701923145
Experience 19, Iter 57, disc loss: 0.0009606038437656536, policy loss: 10.166314441903246
Experience 19, Iter 58, disc loss: 0.0008543634293065726, policy loss: 10.288563276007535
Experience 19, Iter 59, disc loss: 0.0010919222142671962, policy loss: 8.996858540395289
Experience 19, Iter 60, disc loss: 0.0013479379222044296, policy loss: 8.75277535122392
Experience 19, Iter 61, disc loss: 0.0009255279282470419, policy loss: 10.292410540409826
Experience 19, Iter 62, disc loss: 0.0011134183825796586, policy loss: 9.7172998891243
Experience 19, Iter 63, disc loss: 0.0010687808486906096, policy loss: 10.89091724643786
Experience 19, Iter 64, disc loss: 0.0008763847098910286, policy loss: 10.04292116791127
Experience 19, Iter 65, disc loss: 0.0010121240573081418, policy loss: 8.797793166142462
Experience 19, Iter 66, disc loss: 0.001304508355382497, policy loss: 9.106844748745184
Experience 19, Iter 67, disc loss: 0.0011478698046980412, policy loss: 8.931401248070213
Experience 19, Iter 68, disc loss: 0.0009430096745218445, policy loss: 10.025815515422542
Experience 19, Iter 69, disc loss: 0.0011213649076701197, policy loss: 9.45460354131783
Experience 19, Iter 70, disc loss: 0.001172556951734089, policy loss: 8.590364885261373
Experience 19, Iter 71, disc loss: 0.0008887810480776039, policy loss: 9.225694751095341
Experience 19, Iter 72, disc loss: 0.0009654021845903614, policy loss: 10.205346897200283
Experience 19, Iter 73, disc loss: 0.0008980040409506322, policy loss: 10.007691885062151
Experience 19, Iter 74, disc loss: 0.001121302430589577, policy loss: 9.654679189723765
Experience 19, Iter 75, disc loss: 0.0009011710235503178, policy loss: 9.267128030912424
Experience 19, Iter 76, disc loss: 0.001199438543488072, policy loss: 9.307055599278025
Experience 19, Iter 77, disc loss: 0.000889823231102193, policy loss: 9.742389183955076
Experience 19, Iter 78, disc loss: 0.0009187173253619935, policy loss: 9.004452776798823
Experience 19, Iter 79, disc loss: 0.0011287582039965413, policy loss: 8.931647377613775
Experience 19, Iter 80, disc loss: 0.0010634953226857768, policy loss: 9.776044228040782
Experience 19, Iter 81, disc loss: 0.0009022538190566567, policy loss: 9.933826684321007
Experience 19, Iter 82, disc loss: 0.0012543883909532272, policy loss: 9.07191670694291
Experience 19, Iter 83, disc loss: 0.0009824936305651974, policy loss: 9.705248211513496
Experience 19, Iter 84, disc loss: 0.0009195551446789197, policy loss: 9.601815049785436
Experience 19, Iter 85, disc loss: 0.0008596933229479933, policy loss: 9.70220560595583
Experience 19, Iter 86, disc loss: 0.0006904598427952423, policy loss: 10.337549448361893
Experience 19, Iter 87, disc loss: 0.0009044655384381442, policy loss: 9.349866298169445
Experience 19, Iter 88, disc loss: 0.0008348320012812973, policy loss: 10.049739060216114
Experience 19, Iter 89, disc loss: 0.0010527574086251351, policy loss: 9.498257075587688
Experience 19, Iter 90, disc loss: 0.0007249549290039847, policy loss: 9.971795807856154
Experience 19, Iter 91, disc loss: 0.000996229666540319, policy loss: 9.206839445457092
Experience 19, Iter 92, disc loss: 0.0007754500871604258, policy loss: 9.722731423682944
Experience 19, Iter 93, disc loss: 0.0008204509679767086, policy loss: 9.7089105220349
Experience 19, Iter 94, disc loss: 0.0008984509806479902, policy loss: 9.655172668185566
Experience 19, Iter 95, disc loss: 0.0009150678598248495, policy loss: 9.463339312432703
Experience 19, Iter 96, disc loss: 0.0010181452763999726, policy loss: 9.679517818493604
Experience 19, Iter 97, disc loss: 0.0008774320120071375, policy loss: 10.261181464873449
Experience 19, Iter 98, disc loss: 0.0010309409559757896, policy loss: 10.241454162649251
Experience 19, Iter 99, disc loss: 0.001068071988151513, policy loss: 9.487515552724577
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1266],
        [1.6261],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1634, 1.7856, 0.0294, 0.0250, 2.8554]],

        [[0.0122, 0.1634, 1.7856, 0.0294, 0.0250, 2.8554]],

        [[0.0122, 0.1634, 1.7856, 0.0294, 0.0250, 2.8554]],

        [[0.0122, 0.1634, 1.7856, 0.0294, 0.0250, 2.8554]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.5062, 6.5043, 0.1551], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.5062, 6.5043, 0.1551])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.225
Iter 2/2000 - Loss: 3.273
Iter 3/2000 - Loss: 3.059
Iter 4/2000 - Loss: 3.044
Iter 5/2000 - Loss: 3.031
Iter 6/2000 - Loss: 2.904
Iter 7/2000 - Loss: 2.759
Iter 8/2000 - Loss: 2.656
Iter 9/2000 - Loss: 2.567
Iter 10/2000 - Loss: 2.434
Iter 11/2000 - Loss: 2.251
Iter 12/2000 - Loss: 2.048
Iter 13/2000 - Loss: 1.849
Iter 14/2000 - Loss: 1.647
Iter 15/2000 - Loss: 1.424
Iter 16/2000 - Loss: 1.174
Iter 17/2000 - Loss: 0.902
Iter 18/2000 - Loss: 0.622
Iter 19/2000 - Loss: 0.340
Iter 20/2000 - Loss: 0.054
Iter 1981/2000 - Loss: -7.462
Iter 1982/2000 - Loss: -7.462
Iter 1983/2000 - Loss: -7.462
Iter 1984/2000 - Loss: -7.462
Iter 1985/2000 - Loss: -7.462
Iter 1986/2000 - Loss: -7.462
Iter 1987/2000 - Loss: -7.462
Iter 1988/2000 - Loss: -7.463
Iter 1989/2000 - Loss: -7.463
Iter 1990/2000 - Loss: -7.463
Iter 1991/2000 - Loss: -7.463
Iter 1992/2000 - Loss: -7.463
Iter 1993/2000 - Loss: -7.463
Iter 1994/2000 - Loss: -7.463
Iter 1995/2000 - Loss: -7.463
Iter 1996/2000 - Loss: -7.463
Iter 1997/2000 - Loss: -7.463
Iter 1998/2000 - Loss: -7.463
Iter 1999/2000 - Loss: -7.463
Iter 2000/2000 - Loss: -7.463
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[10.5821,  4.9348, 30.9669,  6.2120,  8.0983, 45.0375]],

        [[14.9964, 33.4736,  7.5913,  1.1941,  1.7593, 24.0616]],

        [[16.8295, 32.4985,  8.6856,  0.8775,  0.9009, 22.1396]],

        [[13.2160, 29.5456, 12.2497,  2.5873,  1.6837, 44.0045]]])
Signal Variance: tensor([ 0.0518,  1.6037, 13.6528,  0.4823])
Estimated target variance: tensor([0.0155, 0.5062, 6.5043, 0.1551])
N: 200
Signal to noise ratio: tensor([12.4009, 69.3438, 78.5904, 39.4756])
Bound on condition number: tensor([  30757.4228,  961712.9227, 1235291.4450,  311665.4173])
Policy Optimizer learning rate:
9.801805417839134e-05
Experience 20, Iter 0, disc loss: 0.0008631454955495373, policy loss: 10.378719719473668
Experience 20, Iter 1, disc loss: 0.0007785467481045611, policy loss: 9.957966444124114
Experience 20, Iter 2, disc loss: 0.0010852341997839125, policy loss: 9.761929424617858
Experience 20, Iter 3, disc loss: 0.0008660997557257699, policy loss: 10.037866875630337
Experience 20, Iter 4, disc loss: 0.0009315075052730561, policy loss: 9.167963953217152
Experience 20, Iter 5, disc loss: 0.0009395418296453494, policy loss: 10.023663804845086
Experience 20, Iter 6, disc loss: 0.0008590333558862748, policy loss: 9.733685186952288
Experience 20, Iter 7, disc loss: 0.0011123995272746077, policy loss: 9.360138727825058
Experience 20, Iter 8, disc loss: 0.0008497953111244647, policy loss: 10.156962338450889
Experience 20, Iter 9, disc loss: 0.0008794010254436241, policy loss: 10.00877373501014
Experience 20, Iter 10, disc loss: 0.000903365595436555, policy loss: 9.657438709205072
Experience 20, Iter 11, disc loss: 0.0007234435976198712, policy loss: 10.112748445429538
Experience 20, Iter 12, disc loss: 0.0008549539834068332, policy loss: 10.857677038311166
Experience 20, Iter 13, disc loss: 0.000910618828607879, policy loss: 9.55312101498457
Experience 20, Iter 14, disc loss: 0.0009694957488103214, policy loss: 10.243685667899591
Experience 20, Iter 15, disc loss: 0.0010356931158028732, policy loss: 10.404689130020657
Experience 20, Iter 16, disc loss: 0.0007926012851546459, policy loss: 10.512947873137733
Experience 20, Iter 17, disc loss: 0.00100280298360657, policy loss: 9.286744028835624
Experience 20, Iter 18, disc loss: 0.0009370777010223013, policy loss: 9.71372279740895
Experience 20, Iter 19, disc loss: 0.0008193549647427334, policy loss: 9.599609872149465
Experience 20, Iter 20, disc loss: 0.0008329658549548829, policy loss: 10.041487978736445
Experience 20, Iter 21, disc loss: 0.00077997397162972, policy loss: 10.531896957810835
Experience 20, Iter 22, disc loss: 0.0008417920750363379, policy loss: 11.441964662460354
Experience 20, Iter 23, disc loss: 0.000849088334646801, policy loss: 10.364466735307317
Experience 20, Iter 24, disc loss: 0.0008286061910878898, policy loss: 10.042654879418375
Experience 20, Iter 25, disc loss: 0.0007602382189334166, policy loss: 10.15284442010389
Experience 20, Iter 26, disc loss: 0.0008363185457403832, policy loss: 10.132879395948354
Experience 20, Iter 27, disc loss: 0.0008825486298351267, policy loss: 9.998225961888608
Experience 20, Iter 28, disc loss: 0.0008071782621741983, policy loss: 10.478724842785212
Experience 20, Iter 29, disc loss: 0.0009427599441434728, policy loss: 9.188695108121141
Experience 20, Iter 30, disc loss: 0.000993829739671643, policy loss: 9.393541715546537
Experience 20, Iter 31, disc loss: 0.0006484759978354183, policy loss: 9.725879552863226
Experience 20, Iter 32, disc loss: 0.0007456382798611513, policy loss: 10.709992042080524
Experience 20, Iter 33, disc loss: 0.0007946269018639094, policy loss: 9.439214762684982
Experience 20, Iter 34, disc loss: 0.000806641339359683, policy loss: 9.863330517827436
Experience 20, Iter 35, disc loss: 0.0008286231675302448, policy loss: 11.131294819578866
Experience 20, Iter 36, disc loss: 0.0008404953876034354, policy loss: 10.300760956552999
Experience 20, Iter 37, disc loss: 0.000745750698745284, policy loss: 10.781457561186345
Experience 20, Iter 38, disc loss: 0.0007188042653877306, policy loss: 10.266890746088746
Experience 20, Iter 39, disc loss: 0.0006843688104169153, policy loss: 9.988893191013876
Experience 20, Iter 40, disc loss: 0.0007007393742902701, policy loss: 10.397258784834847
Experience 20, Iter 41, disc loss: 0.000845263999316912, policy loss: 9.519757825815626
Experience 20, Iter 42, disc loss: 0.0008190628375591247, policy loss: 9.576491920761386
Experience 20, Iter 43, disc loss: 0.0006685671786795048, policy loss: 10.805386978617976
Experience 20, Iter 44, disc loss: 0.0006882881947328648, policy loss: 10.40361972630551
Experience 20, Iter 45, disc loss: 0.0006955895908702559, policy loss: 11.000080150480965
Experience 20, Iter 46, disc loss: 0.0005998183537276375, policy loss: 10.652677176622312
Experience 20, Iter 47, disc loss: 0.0008021259293923616, policy loss: 10.981575274565687
Experience 20, Iter 48, disc loss: 0.00078087843510399, policy loss: 9.933697793511213
Experience 20, Iter 49, disc loss: 0.0006309776427785212, policy loss: 9.860268967895323
Experience 20, Iter 50, disc loss: 0.0008119058969338507, policy loss: 9.83358292975125
Experience 20, Iter 51, disc loss: 0.0008014119021162816, policy loss: 10.102713668698406
Experience 20, Iter 52, disc loss: 0.0006913646135495977, policy loss: 9.973349084337285
Experience 20, Iter 53, disc loss: 0.0005768672506456303, policy loss: 10.638694828647326
Experience 20, Iter 54, disc loss: 0.0006824888710579128, policy loss: 9.93250860664698
Experience 20, Iter 55, disc loss: 0.000975327510861479, policy loss: 9.187449453362856
Experience 20, Iter 56, disc loss: 0.0007812374338869324, policy loss: 9.737558848425133
Experience 20, Iter 57, disc loss: 0.000720122303920586, policy loss: 10.194155874304354
Experience 20, Iter 58, disc loss: 0.0007347708264906199, policy loss: 9.954551627304898
Experience 20, Iter 59, disc loss: 0.0007191841805207597, policy loss: 9.664204193781622
Experience 20, Iter 60, disc loss: 0.0007643227367287418, policy loss: 9.560157441567313
Experience 20, Iter 61, disc loss: 0.0007290311306470249, policy loss: 10.042708893833414
Experience 20, Iter 62, disc loss: 0.0007225214577152465, policy loss: 10.341751035353646
Experience 20, Iter 63, disc loss: 0.0008603804491421115, policy loss: 10.226164980688145
Experience 20, Iter 64, disc loss: 0.0008106571947981195, policy loss: 10.28029208672568
Experience 20, Iter 65, disc loss: 0.000765016002666227, policy loss: 10.38268557587579
Experience 20, Iter 66, disc loss: 0.0006518531093913597, policy loss: 10.89308297140884
Experience 20, Iter 67, disc loss: 0.0007281466821578088, policy loss: 9.758088845370034
Experience 20, Iter 68, disc loss: 0.0006663935950162396, policy loss: 9.993043126161513
Experience 20, Iter 69, disc loss: 0.0006243560894541387, policy loss: 10.563217844266191
Experience 20, Iter 70, disc loss: 0.0007391111988849822, policy loss: 9.985975416342978
Experience 20, Iter 71, disc loss: 0.0006406485539831689, policy loss: 9.965561682588433
Experience 20, Iter 72, disc loss: 0.0007355950959414815, policy loss: 9.967870502454701
Experience 20, Iter 73, disc loss: 0.0008461296177590611, policy loss: 10.43633180438126
Experience 20, Iter 74, disc loss: 0.0005619161758532442, policy loss: 10.524148791857277
Experience 20, Iter 75, disc loss: 0.0008156477067435138, policy loss: 9.595192474207291
Experience 20, Iter 76, disc loss: 0.0007287826449891538, policy loss: 9.90695292368277
Experience 20, Iter 77, disc loss: 0.0007154905265639835, policy loss: 10.341332184589232
Experience 20, Iter 78, disc loss: 0.0005267074163658985, policy loss: 10.596076439927787
Experience 20, Iter 79, disc loss: 0.0010081099514842717, policy loss: 10.383975295034357
Experience 20, Iter 80, disc loss: 0.0005969787207022455, policy loss: 10.92976567375373
Experience 20, Iter 81, disc loss: 0.0006521002563675039, policy loss: 10.42176770033615
Experience 20, Iter 82, disc loss: 0.0005907408441875308, policy loss: 10.666156113230798
Experience 20, Iter 83, disc loss: 0.0006710563582627061, policy loss: 10.36968859720108
Experience 20, Iter 84, disc loss: 0.0005528627638016655, policy loss: 10.336861452319445
Experience 20, Iter 85, disc loss: 0.0006709885594783559, policy loss: 9.84711952665685
Experience 20, Iter 86, disc loss: 0.0006282336277353478, policy loss: 9.713167714087652
Experience 20, Iter 87, disc loss: 0.0005347040878348007, policy loss: 11.151749098794706
Experience 20, Iter 88, disc loss: 0.0006319139983206048, policy loss: 10.463619628223693
Experience 20, Iter 89, disc loss: 0.0005878908241540401, policy loss: 9.599656066435118
Experience 20, Iter 90, disc loss: 0.0008934083918311672, policy loss: 10.333211349647234
Experience 20, Iter 91, disc loss: 0.0006312205325230369, policy loss: 10.465499827613419
Experience 20, Iter 92, disc loss: 0.0007684809323833162, policy loss: 9.893716054337581
Experience 20, Iter 93, disc loss: 0.0005723752853049659, policy loss: 9.82470528067428
Experience 20, Iter 94, disc loss: 0.0005821557451935194, policy loss: 10.3562846075022
Experience 20, Iter 95, disc loss: 0.0007166984249426441, policy loss: 10.04873710913752
Experience 20, Iter 96, disc loss: 0.0006214395995954812, policy loss: 11.345952552445043
Experience 20, Iter 97, disc loss: 0.0008619566305491943, policy loss: 9.586233811536164
Experience 20, Iter 98, disc loss: 0.0005874261721152962, policy loss: 10.885252290621192
Experience 20, Iter 99, disc loss: 0.0006489805694348367, policy loss: 10.21793440778753
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1319],
        [1.6482],
        [0.0388]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1629, 1.7936, 0.0299, 0.0255, 3.0071]],

        [[0.0122, 0.1629, 1.7936, 0.0299, 0.0255, 3.0071]],

        [[0.0122, 0.1629, 1.7936, 0.0299, 0.0255, 3.0071]],

        [[0.0122, 0.1629, 1.7936, 0.0299, 0.0255, 3.0071]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.5276, 6.5928, 0.1552], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.5276, 6.5928, 0.1552])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.254
Iter 2/2000 - Loss: 3.317
Iter 3/2000 - Loss: 3.089
Iter 4/2000 - Loss: 3.080
Iter 5/2000 - Loss: 3.076
Iter 6/2000 - Loss: 2.950
Iter 7/2000 - Loss: 2.801
Iter 8/2000 - Loss: 2.694
Iter 9/2000 - Loss: 2.607
Iter 10/2000 - Loss: 2.483
Iter 11/2000 - Loss: 2.305
Iter 12/2000 - Loss: 2.101
Iter 13/2000 - Loss: 1.895
Iter 14/2000 - Loss: 1.689
Iter 15/2000 - Loss: 1.469
Iter 16/2000 - Loss: 1.223
Iter 17/2000 - Loss: 0.952
Iter 18/2000 - Loss: 0.667
Iter 19/2000 - Loss: 0.377
Iter 20/2000 - Loss: 0.085
Iter 1981/2000 - Loss: -7.485
Iter 1982/2000 - Loss: -7.485
Iter 1983/2000 - Loss: -7.485
Iter 1984/2000 - Loss: -7.485
Iter 1985/2000 - Loss: -7.485
Iter 1986/2000 - Loss: -7.485
Iter 1987/2000 - Loss: -7.485
Iter 1988/2000 - Loss: -7.485
Iter 1989/2000 - Loss: -7.485
Iter 1990/2000 - Loss: -7.485
Iter 1991/2000 - Loss: -7.485
Iter 1992/2000 - Loss: -7.485
Iter 1993/2000 - Loss: -7.485
Iter 1994/2000 - Loss: -7.485
Iter 1995/2000 - Loss: -7.485
Iter 1996/2000 - Loss: -7.485
Iter 1997/2000 - Loss: -7.485
Iter 1998/2000 - Loss: -7.485
Iter 1999/2000 - Loss: -7.485
Iter 2000/2000 - Loss: -7.485
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[10.6947,  5.0047, 30.1102,  6.4078,  8.2783, 44.8323]],

        [[12.5746, 33.5272,  7.9291,  1.2172,  1.6170, 25.0039]],

        [[16.6200, 33.0362,  8.6484,  0.8610,  0.9324, 22.1911]],

        [[12.1876, 29.4705, 13.4844,  2.5374,  1.7123, 45.2331]]])
Signal Variance: tensor([ 0.0529,  1.7422, 13.5130,  0.5404])
Estimated target variance: tensor([0.0155, 0.5276, 6.5928, 0.1552])
N: 210
Signal to noise ratio: tensor([12.4468, 69.5600, 78.3178, 41.7457])
Bound on condition number: tensor([  32534.7350, 1016106.0609, 1288073.0147,  365968.7694])
Policy Optimizer learning rate:
9.791483623609773e-05
Experience 21, Iter 0, disc loss: 0.000595958179431822, policy loss: 10.254524716182592
Experience 21, Iter 1, disc loss: 0.0007013099221430262, policy loss: 9.682475091604967
Experience 21, Iter 2, disc loss: 0.0006890922969902684, policy loss: 10.279995255745451
Experience 21, Iter 3, disc loss: 0.0005646125525933425, policy loss: 9.954023408248826
Experience 21, Iter 4, disc loss: 0.0005467736458795837, policy loss: 10.619567356489227
Experience 21, Iter 5, disc loss: 0.0006578507114220681, policy loss: 10.357859703651881
Experience 21, Iter 6, disc loss: 0.0005161921133532128, policy loss: 11.475193877653044
Experience 21, Iter 7, disc loss: 0.0006137983735789619, policy loss: 10.489698793210284
Experience 21, Iter 8, disc loss: 0.0005640106431382958, policy loss: 10.212394999921978
Experience 21, Iter 9, disc loss: 0.000648148897856054, policy loss: 9.689513781890577
Experience 21, Iter 10, disc loss: 0.0005381914218774791, policy loss: 10.618643136208195
Experience 21, Iter 11, disc loss: 0.0006049410213047238, policy loss: 10.369377564541821
Experience 21, Iter 12, disc loss: 0.000669877551387656, policy loss: 10.059413069035744
Experience 21, Iter 13, disc loss: 0.0005624820685283145, policy loss: 11.42699381636789
Experience 21, Iter 14, disc loss: 0.0005435907463440091, policy loss: 10.478969516417298
Experience 21, Iter 15, disc loss: 0.0006327671383524145, policy loss: 10.458208691889833
Experience 21, Iter 16, disc loss: 0.0006089275376734598, policy loss: 10.32102318293684
Experience 21, Iter 17, disc loss: 0.0006005793439678422, policy loss: 10.487963556915389
Experience 21, Iter 18, disc loss: 0.0005077698669693891, policy loss: 10.64590046866166
Experience 21, Iter 19, disc loss: 0.0006870415980778378, policy loss: 9.825818204950027
Experience 21, Iter 20, disc loss: 0.0005098396975569295, policy loss: 10.128614844128677
Experience 21, Iter 21, disc loss: 0.0006413433939148911, policy loss: 10.033452252711465
Experience 21, Iter 22, disc loss: 0.0005810050214924504, policy loss: 10.892471336564942
Experience 21, Iter 23, disc loss: 0.0005316809051697947, policy loss: 10.801868507068717
Experience 21, Iter 24, disc loss: 0.0005695863757618654, policy loss: 10.378947669922585
Experience 21, Iter 25, disc loss: 0.0006128125861077917, policy loss: 10.317888497083178
Experience 21, Iter 26, disc loss: 0.0005069747384124484, policy loss: 10.797726431580754
Experience 21, Iter 27, disc loss: 0.0005899181808348743, policy loss: 11.50446534823984
Experience 21, Iter 28, disc loss: 0.0006107118734313868, policy loss: 9.981374367263337
Experience 21, Iter 29, disc loss: 0.0004774441074392035, policy loss: 10.489569390652177
Experience 21, Iter 30, disc loss: 0.0006200532238226952, policy loss: 9.178259446647484
Experience 21, Iter 31, disc loss: 0.0005465737642915307, policy loss: 10.60763914753165
Experience 21, Iter 32, disc loss: 0.0005235982594418328, policy loss: 10.783175268411249
Experience 21, Iter 33, disc loss: 0.0006364338571517599, policy loss: 9.99543977706901
Experience 21, Iter 34, disc loss: 0.000644033644959815, policy loss: 10.807876410369976
Experience 21, Iter 35, disc loss: 0.0005257128571968589, policy loss: 10.880606701664208
Experience 21, Iter 36, disc loss: 0.00047604333128743775, policy loss: 10.618092939743125
Experience 21, Iter 37, disc loss: 0.0005043103192590346, policy loss: 10.891522145965123
Experience 21, Iter 38, disc loss: 0.000616541094895216, policy loss: 10.328253730808466
Experience 21, Iter 39, disc loss: 0.0004789472259830415, policy loss: 10.807374513030995
Experience 21, Iter 40, disc loss: 0.0004870969440062716, policy loss: 10.76821691077965
Experience 21, Iter 41, disc loss: 0.000520528299333816, policy loss: 10.409249039143514
Experience 21, Iter 42, disc loss: 0.0005045159512228736, policy loss: 9.905248233948193
Experience 21, Iter 43, disc loss: 0.000618510550027628, policy loss: 9.939308885417033
Experience 21, Iter 44, disc loss: 0.00046305181231280385, policy loss: 11.414421017220112
Experience 21, Iter 45, disc loss: 0.0004515380686094171, policy loss: 11.087849316633532
Experience 21, Iter 46, disc loss: 0.0005519222377376957, policy loss: 11.414245545050703
Experience 21, Iter 47, disc loss: 0.0005818538328675759, policy loss: 10.007852112051037
Experience 21, Iter 48, disc loss: 0.0005045396815233753, policy loss: 10.1291072978083
Experience 21, Iter 49, disc loss: 0.000551797280201582, policy loss: 10.395584741008046
Experience 21, Iter 50, disc loss: 0.0004010805717448592, policy loss: 10.683632649613148
Experience 21, Iter 51, disc loss: 0.000525544885231747, policy loss: 10.18567720138457
Experience 21, Iter 52, disc loss: 0.0005795869501197281, policy loss: 10.358292618807083
Experience 21, Iter 53, disc loss: 0.0005319430809588975, policy loss: 10.226868751464739
Experience 21, Iter 54, disc loss: 0.0005034923410453468, policy loss: 10.503898449175846
Experience 21, Iter 55, disc loss: 0.0005582180611908253, policy loss: 9.965903106546182
Experience 21, Iter 56, disc loss: 0.0004549406877732367, policy loss: 10.7329933518353
Experience 21, Iter 57, disc loss: 0.0005795545471241081, policy loss: 9.684571295956788
Experience 21, Iter 58, disc loss: 0.00042267971304998697, policy loss: 10.731936830793423
Experience 21, Iter 59, disc loss: 0.0005495911680868694, policy loss: 10.338086084035007
Experience 21, Iter 60, disc loss: 0.0004497478020651961, policy loss: 11.081012527915087
Experience 21, Iter 61, disc loss: 0.0005298174143939013, policy loss: 10.603792697000594
Experience 21, Iter 62, disc loss: 0.0005447201960983746, policy loss: 10.175398287648127
Experience 21, Iter 63, disc loss: 0.0005837407197508012, policy loss: 9.778052591804947
Experience 21, Iter 64, disc loss: 0.0004728424267816645, policy loss: 10.417720884379639
Experience 21, Iter 65, disc loss: 0.000515871536543462, policy loss: 10.841074729634173
Experience 21, Iter 66, disc loss: 0.0005074818484805707, policy loss: 11.199782797765087
Experience 21, Iter 67, disc loss: 0.00042427847253228125, policy loss: 10.049138330153374
Experience 21, Iter 68, disc loss: 0.0004608791743323267, policy loss: 10.678424985734413
Experience 21, Iter 69, disc loss: 0.0005185032668302556, policy loss: 9.910591450568731
Experience 21, Iter 70, disc loss: 0.000518148523198629, policy loss: 10.291567216966376
Experience 21, Iter 71, disc loss: 0.0004949185730202136, policy loss: 10.929806304279023
Experience 21, Iter 72, disc loss: 0.0005406472659755545, policy loss: 9.740309864850854
Experience 21, Iter 73, disc loss: 0.0005329805106184804, policy loss: 10.898951946292302
Experience 21, Iter 74, disc loss: 0.0004795746403184717, policy loss: 10.257387020784506
Experience 21, Iter 75, disc loss: 0.0006029465351782626, policy loss: 10.624344516367142
Experience 21, Iter 76, disc loss: 0.0004418657710047177, policy loss: 10.351553570391134
Experience 21, Iter 77, disc loss: 0.0004545149948229437, policy loss: 10.373161748483389
Experience 21, Iter 78, disc loss: 0.0004393813900510095, policy loss: 10.000245736520608
Experience 21, Iter 79, disc loss: 0.000609109565104633, policy loss: 10.097173426541953
Experience 21, Iter 80, disc loss: 0.0004937440909109142, policy loss: 10.154179678616087
Experience 21, Iter 81, disc loss: 0.0005634380574208758, policy loss: 9.498241501006326
Experience 21, Iter 82, disc loss: 0.0005753444940794166, policy loss: 9.862467924135164
Experience 21, Iter 83, disc loss: 0.0005913542703225573, policy loss: 10.619998245959973
Experience 21, Iter 84, disc loss: 0.00057781009846399, policy loss: 10.91014778995569
Experience 21, Iter 85, disc loss: 0.0005295825858753099, policy loss: 10.857918051165637
Experience 21, Iter 86, disc loss: 0.0004997909981510692, policy loss: 10.692001322083058
Experience 21, Iter 87, disc loss: 0.00040951243840624393, policy loss: 12.093953805644093
Experience 21, Iter 88, disc loss: 0.0005683547588699371, policy loss: 10.498245254956604
Experience 21, Iter 89, disc loss: 0.0005711420905725101, policy loss: 10.2650007868715
Experience 21, Iter 90, disc loss: 0.0004704259535197301, policy loss: 10.77209423409484
Experience 21, Iter 91, disc loss: 0.00044414051937424325, policy loss: 11.327970857075574
Experience 21, Iter 92, disc loss: 0.0005361221323906412, policy loss: 10.464552490464998
Experience 21, Iter 93, disc loss: 0.0004434621349864978, policy loss: 11.093385607951399
Experience 21, Iter 94, disc loss: 0.0005523077980653807, policy loss: 10.890423767538636
Experience 21, Iter 95, disc loss: 0.0005339901040607616, policy loss: 11.099092835668351
Experience 21, Iter 96, disc loss: 0.0004943574576106163, policy loss: 10.857735323263483
Experience 21, Iter 97, disc loss: 0.0004452370885820185, policy loss: 10.297488648441629
Experience 21, Iter 98, disc loss: 0.000586554416462504, policy loss: 10.575361177042208
Experience 21, Iter 99, disc loss: 0.0005031159317735654, policy loss: 11.017261973251342
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1412],
        [1.7011],
        [0.0396]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0122, 0.1644, 1.8419, 0.0305, 0.0261, 3.1833]],

        [[0.0122, 0.1644, 1.8419, 0.0305, 0.0261, 3.1833]],

        [[0.0122, 0.1644, 1.8419, 0.0305, 0.0261, 3.1833]],

        [[0.0122, 0.1644, 1.8419, 0.0305, 0.0261, 3.1833]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0155, 0.5646, 6.8046, 0.1586], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0155, 0.5646, 6.8046, 0.1586])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.322
Iter 2/2000 - Loss: 3.393
Iter 3/2000 - Loss: 3.158
Iter 4/2000 - Loss: 3.154
Iter 5/2000 - Loss: 3.157
Iter 6/2000 - Loss: 3.034
Iter 7/2000 - Loss: 2.885
Iter 8/2000 - Loss: 2.778
Iter 9/2000 - Loss: 2.694
Iter 10/2000 - Loss: 2.576
Iter 11/2000 - Loss: 2.405
Iter 12/2000 - Loss: 2.203
Iter 13/2000 - Loss: 1.997
Iter 14/2000 - Loss: 1.790
Iter 15/2000 - Loss: 1.570
Iter 16/2000 - Loss: 1.323
Iter 17/2000 - Loss: 1.050
Iter 18/2000 - Loss: 0.759
Iter 19/2000 - Loss: 0.461
Iter 20/2000 - Loss: 0.159
Iter 1981/2000 - Loss: -7.546
Iter 1982/2000 - Loss: -7.546
Iter 1983/2000 - Loss: -7.546
Iter 1984/2000 - Loss: -7.546
Iter 1985/2000 - Loss: -7.546
Iter 1986/2000 - Loss: -7.546
Iter 1987/2000 - Loss: -7.546
Iter 1988/2000 - Loss: -7.546
Iter 1989/2000 - Loss: -7.546
Iter 1990/2000 - Loss: -7.546
Iter 1991/2000 - Loss: -7.546
Iter 1992/2000 - Loss: -7.546
Iter 1993/2000 - Loss: -7.546
Iter 1994/2000 - Loss: -7.546
Iter 1995/2000 - Loss: -7.546
Iter 1996/2000 - Loss: -7.546
Iter 1997/2000 - Loss: -7.546
Iter 1998/2000 - Loss: -7.546
Iter 1999/2000 - Loss: -7.546
Iter 2000/2000 - Loss: -7.546
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[10.8617,  5.1505, 32.2710,  4.8938,  6.8599, 45.4107]],

        [[14.4019, 33.6091,  7.6316,  1.2961,  1.6309, 24.9718]],

        [[16.6870, 34.1475,  8.6833,  0.8455,  0.9281, 21.7839]],

        [[12.1681, 28.5440, 13.8361,  2.4501,  1.7608, 45.7952]]])
Signal Variance: tensor([ 0.0517,  1.7407, 12.5394,  0.5475])
Estimated target variance: tensor([0.0155, 0.5646, 6.8046, 0.1586])
N: 220
Signal to noise ratio: tensor([12.4756, 68.8482, 76.0443, 41.5114])
Bound on condition number: tensor([  34242.1243, 1042817.3026, 1272202.0159,  379105.0859])
Policy Optimizer learning rate:
9.78117269874902e-05
Experience 22, Iter 0, disc loss: 0.0004995062490941041, policy loss: 10.80362186500733
Experience 22, Iter 1, disc loss: 0.0005567631531055897, policy loss: 9.673650557072165
Experience 22, Iter 2, disc loss: 0.0003856885830807132, policy loss: 10.214170235892706
Experience 22, Iter 3, disc loss: 0.0005204738267849448, policy loss: 10.177952292459185
Experience 22, Iter 4, disc loss: 0.0005217661733809178, policy loss: 9.946279633889086
Experience 22, Iter 5, disc loss: 0.0005266360893252402, policy loss: 9.401429785160301
Experience 22, Iter 6, disc loss: 0.00042856117283480977, policy loss: 10.794440724049743
Experience 22, Iter 7, disc loss: 0.0005475882779544918, policy loss: 11.080555869128823
Experience 22, Iter 8, disc loss: 0.0005193772517806073, policy loss: 10.427605876613168
Experience 22, Iter 9, disc loss: 0.00039367268733792526, policy loss: 11.355797956373484
Experience 22, Iter 10, disc loss: 0.0005586888263880449, policy loss: 11.148887356741888
Experience 22, Iter 11, disc loss: 0.0006705797080632995, policy loss: 9.511026289777828
Experience 22, Iter 12, disc loss: 0.0005441137785518055, policy loss: 10.122746247084988
Experience 22, Iter 13, disc loss: 0.000525913422642474, policy loss: 10.59797055127343
Experience 22, Iter 14, disc loss: 0.0004356233230754401, policy loss: 10.57907121885685
Experience 22, Iter 15, disc loss: 0.0004592873553276689, policy loss: 9.452956380183167
Experience 22, Iter 16, disc loss: 0.0005420029225214568, policy loss: 9.660481136352724
Experience 22, Iter 17, disc loss: 0.0005320863016437505, policy loss: 10.50218901948104
Experience 22, Iter 18, disc loss: 0.000526491489885175, policy loss: 10.04846821480898
Experience 22, Iter 19, disc loss: 0.0005666011767322348, policy loss: 10.397111127947188
Experience 22, Iter 20, disc loss: 0.0004720134275623298, policy loss: 10.94885216610999
Experience 22, Iter 21, disc loss: 0.00036542305741488396, policy loss: 11.066763900005238
Experience 22, Iter 22, disc loss: 0.0004700431409165091, policy loss: 10.205498163715788
Experience 22, Iter 23, disc loss: 0.0005010386382350797, policy loss: 10.425598046375455
Experience 22, Iter 24, disc loss: 0.00042249991260514787, policy loss: 10.69283261548506
Experience 22, Iter 25, disc loss: 0.0005372240328160689, policy loss: 9.843049450920438
Experience 22, Iter 26, disc loss: 0.0005568169171003118, policy loss: 10.775652886549292
Experience 22, Iter 27, disc loss: 0.00042678182464934744, policy loss: 9.929850141819536
Experience 22, Iter 28, disc loss: 0.0005553620239720974, policy loss: 11.0798332480238
Experience 22, Iter 29, disc loss: 0.00047601854378171197, policy loss: 10.322508159444542
Experience 22, Iter 30, disc loss: 0.00038340187767442284, policy loss: 10.631399209974933
Experience 22, Iter 31, disc loss: 0.0004866570949634929, policy loss: 10.447584687583765
Experience 22, Iter 32, disc loss: 0.0004550504309065638, policy loss: 10.063342524098003
Experience 22, Iter 33, disc loss: 0.000446848600305858, policy loss: 10.378214427532122
Experience 22, Iter 34, disc loss: 0.00042335255282922326, policy loss: 10.04456099949195
Experience 22, Iter 35, disc loss: 0.000387590294505325, policy loss: 10.98555651598035
Experience 22, Iter 36, disc loss: 0.0005296066302294181, policy loss: 9.749147872672998
Experience 22, Iter 37, disc loss: 0.00041287119978677077, policy loss: 10.720102159020819
Experience 22, Iter 38, disc loss: 0.0003920023147900716, policy loss: 11.26447269625541
Experience 22, Iter 39, disc loss: 0.0004660220989642937, policy loss: 10.377442489420972
Experience 22, Iter 40, disc loss: 0.0003841704830940695, policy loss: 10.922260951352577
Experience 22, Iter 41, disc loss: 0.00042529948706520113, policy loss: 11.30296230671234
Experience 22, Iter 42, disc loss: 0.0004415298378748951, policy loss: 10.45972438053606
Experience 22, Iter 43, disc loss: 0.00042429760887413386, policy loss: 10.765749589622573
Experience 22, Iter 44, disc loss: 0.00038522891256132603, policy loss: 10.37475893790417
Experience 22, Iter 45, disc loss: 0.0005270661102489485, policy loss: 9.99656717659203
Experience 22, Iter 46, disc loss: 0.00041405752364986534, policy loss: 10.908413455810532
Experience 22, Iter 47, disc loss: 0.0005007745349948046, policy loss: 9.585338662647622
Experience 22, Iter 48, disc loss: 0.0005055051859285413, policy loss: 9.448489922757364
Experience 22, Iter 49, disc loss: 0.0005408984840355564, policy loss: 9.95359581207749
Experience 22, Iter 50, disc loss: 0.0004612169910265513, policy loss: 10.331486902651312
Experience 22, Iter 51, disc loss: 0.00035739182718889245, policy loss: 11.028627008735693
Experience 22, Iter 52, disc loss: 0.0005604585985758304, policy loss: 9.924638877001136
Experience 22, Iter 53, disc loss: 0.00041072091238972817, policy loss: 10.19598916593419
Experience 22, Iter 54, disc loss: 0.0003930794984649571, policy loss: 10.699416416647317
Experience 22, Iter 55, disc loss: 0.00048182943474160087, policy loss: 10.25872539265669
Experience 22, Iter 56, disc loss: 0.0004096235368406309, policy loss: 10.3524787541017
Experience 22, Iter 57, disc loss: 0.0005185136559566065, policy loss: 9.975030115753079
Experience 22, Iter 58, disc loss: 0.0005800552016893289, policy loss: 9.550669986094015
Experience 22, Iter 59, disc loss: 0.00038741393202730383, policy loss: 11.654334838783061
Experience 22, Iter 60, disc loss: 0.00038170445488376923, policy loss: 11.257546189794503
Experience 22, Iter 61, disc loss: 0.0004247242048588646, policy loss: 9.828168722609798
Experience 22, Iter 62, disc loss: 0.0004108984801864104, policy loss: 10.64588274122161
Experience 22, Iter 63, disc loss: 0.00047121077579000256, policy loss: 10.335000805865935
Experience 22, Iter 64, disc loss: 0.0004645828946008717, policy loss: 10.51563620703928
Experience 22, Iter 65, disc loss: 0.00047520438562446553, policy loss: 9.97701729870387
Experience 22, Iter 66, disc loss: 0.00043464114823850355, policy loss: 10.907043799643564
Experience 22, Iter 67, disc loss: 0.00043507450187608797, policy loss: 10.060706359937257
Experience 22, Iter 68, disc loss: 0.000425253941450872, policy loss: 9.989669745520462
Experience 22, Iter 69, disc loss: 0.0004401618577101415, policy loss: 10.484813044455088
Experience 22, Iter 70, disc loss: 0.00037445663400634635, policy loss: 10.655960668261589
Experience 22, Iter 71, disc loss: 0.0004846428648391834, policy loss: 10.313833692632553
Experience 22, Iter 72, disc loss: 0.0004403242698320381, policy loss: 10.40231820801872
Experience 22, Iter 73, disc loss: 0.0003311944588989728, policy loss: 11.506118602065552
Experience 22, Iter 74, disc loss: 0.00031350329375761233, policy loss: 11.331173595128224
Experience 22, Iter 75, disc loss: 0.0004228378283257573, policy loss: 10.333321920737909
Experience 22, Iter 76, disc loss: 0.00043437997253653523, policy loss: 10.274009717302889
Experience 22, Iter 77, disc loss: 0.00042559410709114927, policy loss: 10.93181212431875
Experience 22, Iter 78, disc loss: 0.0004072242819245994, policy loss: 10.417825640071456
Experience 22, Iter 79, disc loss: 0.0005147848889392217, policy loss: 10.159505061889313
Experience 22, Iter 80, disc loss: 0.0003811483050053056, policy loss: 11.318270786298491
Experience 22, Iter 81, disc loss: 0.0005579388406654689, policy loss: 10.458194438515338
Experience 22, Iter 82, disc loss: 0.00037176081617719604, policy loss: 10.802001429227541
Experience 22, Iter 83, disc loss: 0.0004159252925945846, policy loss: 10.716081726836748
Experience 22, Iter 84, disc loss: 0.00040099511791309326, policy loss: 11.172555634146097
Experience 22, Iter 85, disc loss: 0.0004144699111382471, policy loss: 9.698173668341699
Experience 22, Iter 86, disc loss: 0.0004105383048251534, policy loss: 10.031131148650196
Experience 22, Iter 87, disc loss: 0.00039666252399643385, policy loss: 11.408266279032276
Experience 22, Iter 88, disc loss: 0.00039265280538949355, policy loss: 10.49019144661565
Experience 22, Iter 89, disc loss: 0.00039473162912223976, policy loss: 10.658547015004192
Experience 22, Iter 90, disc loss: 0.00047773189456866026, policy loss: 10.511883097416643
Experience 22, Iter 91, disc loss: 0.0004363797727511505, policy loss: 10.512922159020427
Experience 22, Iter 92, disc loss: 0.0003860958780500276, policy loss: 10.831032287847101
Experience 22, Iter 93, disc loss: 0.0004132218686264807, policy loss: 11.333336870758377
Experience 22, Iter 94, disc loss: 0.00046517111479037155, policy loss: 10.98172654664287
Experience 22, Iter 95, disc loss: 0.0004750248249655428, policy loss: 10.858255590917135
Experience 22, Iter 96, disc loss: 0.0003794350664052654, policy loss: 11.814550337543146
Experience 22, Iter 97, disc loss: 0.00038447199838307494, policy loss: 11.31963770079942
Experience 22, Iter 98, disc loss: 0.0004310517542301593, policy loss: 11.107371066073199
Experience 22, Iter 99, disc loss: 0.0002892088899679442, policy loss: 10.691318193218112
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1505],
        [1.7453],
        [0.0407]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0119, 0.1658, 1.8915, 0.0311, 0.0265, 3.3582]],

        [[0.0119, 0.1658, 1.8915, 0.0311, 0.0265, 3.3582]],

        [[0.0119, 0.1658, 1.8915, 0.0311, 0.0265, 3.3582]],

        [[0.0119, 0.1658, 1.8915, 0.0311, 0.0265, 3.3582]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.6022, 6.9814, 0.1627], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.6022, 6.9814, 0.1627])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.349
Iter 2/2000 - Loss: 3.413
Iter 3/2000 - Loss: 3.161
Iter 4/2000 - Loss: 3.145
Iter 5/2000 - Loss: 3.135
Iter 6/2000 - Loss: 2.997
Iter 7/2000 - Loss: 2.831
Iter 8/2000 - Loss: 2.707
Iter 9/2000 - Loss: 2.608
Iter 10/2000 - Loss: 2.480
Iter 11/2000 - Loss: 2.300
Iter 12/2000 - Loss: 2.088
Iter 13/2000 - Loss: 1.870
Iter 14/2000 - Loss: 1.654
Iter 15/2000 - Loss: 1.428
Iter 16/2000 - Loss: 1.179
Iter 17/2000 - Loss: 0.904
Iter 18/2000 - Loss: 0.613
Iter 19/2000 - Loss: 0.315
Iter 20/2000 - Loss: 0.015
Iter 1981/2000 - Loss: -7.606
Iter 1982/2000 - Loss: -7.606
Iter 1983/2000 - Loss: -7.607
Iter 1984/2000 - Loss: -7.607
Iter 1985/2000 - Loss: -7.607
Iter 1986/2000 - Loss: -7.607
Iter 1987/2000 - Loss: -7.607
Iter 1988/2000 - Loss: -7.607
Iter 1989/2000 - Loss: -7.607
Iter 1990/2000 - Loss: -7.607
Iter 1991/2000 - Loss: -7.607
Iter 1992/2000 - Loss: -7.607
Iter 1993/2000 - Loss: -7.607
Iter 1994/2000 - Loss: -7.607
Iter 1995/2000 - Loss: -7.607
Iter 1996/2000 - Loss: -7.607
Iter 1997/2000 - Loss: -7.607
Iter 1998/2000 - Loss: -7.607
Iter 1999/2000 - Loss: -7.607
Iter 2000/2000 - Loss: -7.607
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[10.7266,  5.0665, 34.3756,  3.8069,  5.8670, 45.0809]],

        [[14.8279, 33.8690,  7.6275,  1.2993,  1.6713, 24.5576]],

        [[16.8099, 33.4595,  8.5103,  0.9234,  0.9350, 23.1320]],

        [[11.6763, 27.7018, 13.8560,  2.6483,  1.7459, 46.4813]]])
Signal Variance: tensor([ 0.0509,  1.8076, 14.8513,  0.5786])
Estimated target variance: tensor([0.0157, 0.6022, 6.9814, 0.1627])
N: 230
Signal to noise ratio: tensor([12.5913, 70.4450, 82.5221, 42.7935])
Bound on condition number: tensor([  36465.1795, 1141374.9439, 1566276.3668,  421195.5344])
Policy Optimizer learning rate:
9.770872631810884e-05
Experience 23, Iter 0, disc loss: 0.00034064852418130636, policy loss: 11.730951463637254
Experience 23, Iter 1, disc loss: 0.00040637350379420906, policy loss: 10.35424890633476
Experience 23, Iter 2, disc loss: 0.0004507452157744532, policy loss: 10.110572187724447
Experience 23, Iter 3, disc loss: 0.00041973869941737646, policy loss: 10.013289012515763
Experience 23, Iter 4, disc loss: 0.0004028865588217617, policy loss: 10.809802628852653
Experience 23, Iter 5, disc loss: 0.0004981190449461674, policy loss: 9.77003893272695
Experience 23, Iter 6, disc loss: 0.0004176429528655511, policy loss: 10.970593416897849
Experience 23, Iter 7, disc loss: 0.00042862420672583107, policy loss: 11.868577778741422
Experience 23, Iter 8, disc loss: 0.000462205282003737, policy loss: 10.047244675238154
Experience 23, Iter 9, disc loss: 0.000433315926509834, policy loss: 10.177100688143247
Experience 23, Iter 10, disc loss: 0.00036993804772281994, policy loss: 10.795184890629812
Experience 23, Iter 11, disc loss: 0.0003545735140147294, policy loss: 11.216929565465055
Experience 23, Iter 12, disc loss: 0.0004139613185711338, policy loss: 10.924106458001617
Experience 23, Iter 13, disc loss: 0.0003972432050242149, policy loss: 11.682650488492696
Experience 23, Iter 14, disc loss: 0.00034069382141995575, policy loss: 11.169940727269788
Experience 23, Iter 15, disc loss: 0.00041345335063726725, policy loss: 11.445225952969324
Experience 23, Iter 16, disc loss: 0.0004178412529065252, policy loss: 10.614688429805033
Experience 23, Iter 17, disc loss: 0.00038165620051312235, policy loss: 11.270816519016385
Experience 23, Iter 18, disc loss: 0.0004232566704420966, policy loss: 10.702176065900115
Experience 23, Iter 19, disc loss: 0.00038875076146770296, policy loss: 10.746066388075777
Experience 23, Iter 20, disc loss: 0.00044114869625516023, policy loss: 11.034090199594438
Experience 23, Iter 21, disc loss: 0.0004332257713593066, policy loss: 9.950044099550905
Experience 23, Iter 22, disc loss: 0.0003618112868115414, policy loss: 11.384789035415817
Experience 23, Iter 23, disc loss: 0.0003715733599955679, policy loss: 11.655223743393787
Experience 23, Iter 24, disc loss: 0.0003798301455868264, policy loss: 10.971139045936965
Experience 23, Iter 25, disc loss: 0.00035108985932716467, policy loss: 10.89942132111733
Experience 23, Iter 26, disc loss: 0.00040317565880672985, policy loss: 11.252188571347833
Experience 23, Iter 27, disc loss: 0.00040847103056125116, policy loss: 10.742260888170904
Experience 23, Iter 28, disc loss: 0.0003757321542675727, policy loss: 10.615516810071828
Experience 23, Iter 29, disc loss: 0.0003852642187119616, policy loss: 10.398549490111833
Experience 23, Iter 30, disc loss: 0.0004044627303657949, policy loss: 10.364006336739031
Experience 23, Iter 31, disc loss: 0.00039841202720229956, policy loss: 10.310982045211276
Experience 23, Iter 32, disc loss: 0.00034102264151896356, policy loss: 10.691039468211926
Experience 23, Iter 33, disc loss: 0.0003308202560946133, policy loss: 11.181621308302441
Experience 23, Iter 34, disc loss: 0.0003529064074027212, policy loss: 10.390196187949101
Experience 23, Iter 35, disc loss: 0.0003136322963324547, policy loss: 10.735727739427608
Experience 23, Iter 36, disc loss: 0.00041822424849230467, policy loss: 10.757791911113433
Experience 23, Iter 37, disc loss: 0.00036808085244021175, policy loss: 11.359109818997009
Experience 23, Iter 38, disc loss: 0.00045440676062325446, policy loss: 9.974663914564005
Experience 23, Iter 39, disc loss: 0.0003522853500966584, policy loss: 11.157055697263957
Experience 23, Iter 40, disc loss: 0.0003843968473968088, policy loss: 10.571279098168613
Experience 23, Iter 41, disc loss: 0.0004419819278489398, policy loss: 10.122972618089928
Experience 23, Iter 42, disc loss: 0.0004137632835445841, policy loss: 11.075640816330074
Experience 23, Iter 43, disc loss: 0.0003515600434807241, policy loss: 10.926087966734315
Experience 23, Iter 44, disc loss: 0.0003190795917732351, policy loss: 10.843735122293332
Experience 23, Iter 45, disc loss: 0.0004513361893783938, policy loss: 11.081449554277954
Experience 23, Iter 46, disc loss: 0.0003977651232979506, policy loss: 11.317298468495453
Experience 23, Iter 47, disc loss: 0.00033744333482621405, policy loss: 10.687520153906833
Experience 23, Iter 48, disc loss: 0.0004561399496233573, policy loss: 9.539990872999732
Experience 23, Iter 49, disc loss: 0.0004059261026302035, policy loss: 10.582581857238845
Experience 23, Iter 50, disc loss: 0.0003839406856150717, policy loss: 10.738245840606751
Experience 23, Iter 51, disc loss: 0.0003479569849675965, policy loss: 11.60664186371248
Experience 23, Iter 52, disc loss: 0.0003537606604043273, policy loss: 12.31798476054392
Experience 23, Iter 53, disc loss: 0.0003272467902683293, policy loss: 10.43613553682208
Experience 23, Iter 54, disc loss: 0.00035994231148878423, policy loss: 10.448830014397268
Experience 23, Iter 55, disc loss: 0.0003525771410582404, policy loss: 10.306899386247675
Experience 23, Iter 56, disc loss: 0.0004404497267563205, policy loss: 11.275258119093253
Experience 23, Iter 57, disc loss: 0.00041100365841836503, policy loss: 10.061940282451964
Experience 23, Iter 58, disc loss: 0.0003993150305286751, policy loss: 10.050007760256022
Experience 23, Iter 59, disc loss: 0.00039962428591138984, policy loss: 10.769718904056369
Experience 23, Iter 60, disc loss: 0.0004186264699745559, policy loss: 10.201632187225735
Experience 23, Iter 61, disc loss: 0.0003755456828198934, policy loss: 10.271675238319446
Experience 23, Iter 62, disc loss: 0.0004735906079259615, policy loss: 10.513915828866345
Experience 23, Iter 63, disc loss: 0.00040617377574231155, policy loss: 10.778833269914166
Experience 23, Iter 64, disc loss: 0.00032221774941832733, policy loss: 12.656006042594804
Experience 23, Iter 65, disc loss: 0.0003518927483118587, policy loss: 10.842739191625427
Experience 23, Iter 66, disc loss: 0.00036235285664731323, policy loss: 10.604613405241471
Experience 23, Iter 67, disc loss: 0.00034405969099620705, policy loss: 10.805516285907698
Experience 23, Iter 68, disc loss: 0.00040937038198053606, policy loss: 10.432072720846195
Experience 23, Iter 69, disc loss: 0.0003612183113073994, policy loss: 10.73181205417876
Experience 23, Iter 70, disc loss: 0.00035768365176646916, policy loss: 10.160975851154191
Experience 23, Iter 71, disc loss: 0.00033090156348506176, policy loss: 11.65132217698715
Experience 23, Iter 72, disc loss: 0.0003538518347272817, policy loss: 10.589415486786383
Experience 23, Iter 73, disc loss: 0.000395106193186265, policy loss: 11.321342919597022
Experience 23, Iter 74, disc loss: 0.00039515446655284886, policy loss: 10.20705416399155
Experience 23, Iter 75, disc loss: 0.00035153066490921717, policy loss: 11.194083134802268
Experience 23, Iter 76, disc loss: 0.0003144528249609067, policy loss: 10.923679559251783
Experience 23, Iter 77, disc loss: 0.0003337095283399902, policy loss: 10.573559703065127
Experience 23, Iter 78, disc loss: 0.0003966155054631006, policy loss: 10.627695932744134
Experience 23, Iter 79, disc loss: 0.00038570686558003553, policy loss: 10.506159664369589
Experience 23, Iter 80, disc loss: 0.0003471582844466037, policy loss: 11.058381856253796
Experience 23, Iter 81, disc loss: 0.00039349934900779717, policy loss: 9.809107460364139
Experience 23, Iter 82, disc loss: 0.00032008986971123935, policy loss: 10.837904661492436
Experience 23, Iter 83, disc loss: 0.00030795726960042313, policy loss: 10.582414742330723
Experience 23, Iter 84, disc loss: 0.00029381795134893565, policy loss: 10.724634261319745
Experience 23, Iter 85, disc loss: 0.0003750755494271328, policy loss: 10.552646172301785
Experience 23, Iter 86, disc loss: 0.0003797669019357434, policy loss: 10.67463758708601
Experience 23, Iter 87, disc loss: 0.00045855794344147035, policy loss: 10.424244816617605
Experience 23, Iter 88, disc loss: 0.0003980455530872998, policy loss: 9.630176792917835
Experience 23, Iter 89, disc loss: 0.0003969540350172309, policy loss: 9.967234993958135
Experience 23, Iter 90, disc loss: 0.0003266597882739112, policy loss: 10.313487935135282
Experience 23, Iter 91, disc loss: 0.00037909752991472474, policy loss: 10.154073951031418
Experience 23, Iter 92, disc loss: 0.00030390892033071363, policy loss: 11.377311242669908
Experience 23, Iter 93, disc loss: 0.00033803719848324006, policy loss: 10.420760771789153
Experience 23, Iter 94, disc loss: 0.00036701316448623986, policy loss: 11.077119696081889
Experience 23, Iter 95, disc loss: 0.0003023215937760817, policy loss: 10.542067896218269
Experience 23, Iter 96, disc loss: 0.0003875389411350983, policy loss: 10.790668201570632
Experience 23, Iter 97, disc loss: 0.00032893516311144496, policy loss: 10.515356382684637
Experience 23, Iter 98, disc loss: 0.00040250916745325824, policy loss: 10.9661084666918
Experience 23, Iter 99, disc loss: 0.0003565760828539536, policy loss: 10.311559230032088
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1573],
        [1.7730],
        [0.0413]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0116, 0.1660, 1.9188, 0.0316, 0.0269, 3.5115]],

        [[0.0116, 0.1660, 1.9188, 0.0316, 0.0269, 3.5115]],

        [[0.0116, 0.1660, 1.9188, 0.0316, 0.0269, 3.5115]],

        [[0.0116, 0.1660, 1.9188, 0.0316, 0.0269, 3.5115]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.6293, 7.0921, 0.1651], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.6293, 7.0921, 0.1651])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.352
Iter 2/2000 - Loss: 3.415
Iter 3/2000 - Loss: 3.152
Iter 4/2000 - Loss: 3.130
Iter 5/2000 - Loss: 3.115
Iter 6/2000 - Loss: 2.970
Iter 7/2000 - Loss: 2.793
Iter 8/2000 - Loss: 2.658
Iter 9/2000 - Loss: 2.551
Iter 10/2000 - Loss: 2.417
Iter 11/2000 - Loss: 2.230
Iter 12/2000 - Loss: 2.010
Iter 13/2000 - Loss: 1.783
Iter 14/2000 - Loss: 1.559
Iter 15/2000 - Loss: 1.328
Iter 16/2000 - Loss: 1.075
Iter 17/2000 - Loss: 0.798
Iter 18/2000 - Loss: 0.505
Iter 19/2000 - Loss: 0.205
Iter 20/2000 - Loss: -0.096
Iter 1981/2000 - Loss: -7.687
Iter 1982/2000 - Loss: -7.687
Iter 1983/2000 - Loss: -7.687
Iter 1984/2000 - Loss: -7.688
Iter 1985/2000 - Loss: -7.688
Iter 1986/2000 - Loss: -7.688
Iter 1987/2000 - Loss: -7.688
Iter 1988/2000 - Loss: -7.688
Iter 1989/2000 - Loss: -7.688
Iter 1990/2000 - Loss: -7.688
Iter 1991/2000 - Loss: -7.688
Iter 1992/2000 - Loss: -7.688
Iter 1993/2000 - Loss: -7.688
Iter 1994/2000 - Loss: -7.688
Iter 1995/2000 - Loss: -7.688
Iter 1996/2000 - Loss: -7.688
Iter 1997/2000 - Loss: -7.688
Iter 1998/2000 - Loss: -7.688
Iter 1999/2000 - Loss: -7.688
Iter 2000/2000 - Loss: -7.688
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[10.4817,  4.9162, 31.3230,  3.8948,  6.3291, 45.7628]],

        [[14.9691, 33.7789,  7.3307,  1.3282,  1.7811, 25.3271]],

        [[16.6235, 32.9100,  8.4398,  0.9251,  0.9279, 23.1623]],

        [[11.4529, 27.1129, 14.5444,  2.5622,  1.7710, 45.7896]]])
Signal Variance: tensor([ 0.0502,  1.8293, 14.7230,  0.5860])
Estimated target variance: tensor([0.0157, 0.6293, 7.0921, 0.1651])
N: 240
Signal to noise ratio: tensor([12.5682, 70.6614, 83.6289, 43.2668])
Bound on condition number: tensor([  37911.4272, 1198329.7090, 1678509.9931,  449284.2423])
Policy Optimizer learning rate:
9.760583411361425e-05
Experience 24, Iter 0, disc loss: 0.000381109330143267, policy loss: 10.983173967317095
Experience 24, Iter 1, disc loss: 0.0003538090800123807, policy loss: 11.276737456922913
Experience 24, Iter 2, disc loss: 0.00038011031416275307, policy loss: 10.623926726296556
Experience 24, Iter 3, disc loss: 0.00034667119763412147, policy loss: 11.0699494978668
Experience 24, Iter 4, disc loss: 0.00037384092047242, policy loss: 10.026102794092525
Experience 24, Iter 5, disc loss: 0.00036913939625329286, policy loss: 10.314466754482936
Experience 24, Iter 6, disc loss: 0.00038196791705858585, policy loss: 10.649835593421297
Experience 24, Iter 7, disc loss: 0.00036754377170983646, policy loss: 10.71881184163485
Experience 24, Iter 8, disc loss: 0.00041539506142027397, policy loss: 10.549801746411084
Experience 24, Iter 9, disc loss: 0.00034266757582902607, policy loss: 10.161842808081984
Experience 24, Iter 10, disc loss: 0.00037708057313896035, policy loss: 10.897177472755768
Experience 24, Iter 11, disc loss: 0.000354704201418106, policy loss: 10.603419284592096
Experience 24, Iter 12, disc loss: 0.00036542658173384975, policy loss: 10.14953072845152
Experience 24, Iter 13, disc loss: 0.00039791506724832604, policy loss: 12.001526501640553
Experience 24, Iter 14, disc loss: 0.0004586698088492664, policy loss: 9.802465377012908
Experience 24, Iter 15, disc loss: 0.00029303498052800787, policy loss: 11.004422916920184
Experience 24, Iter 16, disc loss: 0.0003267345658892676, policy loss: 11.005622574019995
Experience 24, Iter 17, disc loss: 0.0003315555832495827, policy loss: 10.595875098571405
Experience 24, Iter 18, disc loss: 0.0004211004476599127, policy loss: 10.815915911830022
Experience 24, Iter 19, disc loss: 0.0003178425551940004, policy loss: 10.94527275963813
Experience 24, Iter 20, disc loss: 0.00041094035526974124, policy loss: 10.716692702349683
Experience 24, Iter 21, disc loss: 0.0003651130460586578, policy loss: 11.453746737611578
Experience 24, Iter 22, disc loss: 0.0003278576239017068, policy loss: 10.450491018126323
Experience 24, Iter 23, disc loss: 0.00035812202496499524, policy loss: 11.017841610639486
Experience 24, Iter 24, disc loss: 0.00039236255161953574, policy loss: 10.02262446879626
Experience 24, Iter 25, disc loss: 0.000398439346845437, policy loss: 11.458841304549837
Experience 24, Iter 26, disc loss: 0.00034594986386118144, policy loss: 11.060925602598408
Experience 24, Iter 27, disc loss: 0.00035003262180652653, policy loss: 10.82906157175826
Experience 24, Iter 28, disc loss: 0.0003622604469757237, policy loss: 10.139603030910443
Experience 24, Iter 29, disc loss: 0.0004095190064453113, policy loss: 11.669049647318712
Experience 24, Iter 30, disc loss: 0.00035988636085571495, policy loss: 10.02889220105539
Experience 24, Iter 31, disc loss: 0.0003538833377538658, policy loss: 11.221741425039372
Experience 24, Iter 32, disc loss: 0.00037960246187476916, policy loss: 9.912172716473236
Experience 24, Iter 33, disc loss: 0.0004150772467361283, policy loss: 10.636890483301508
Experience 24, Iter 34, disc loss: 0.00033074019140703643, policy loss: 11.07963439433634
Experience 24, Iter 35, disc loss: 0.00036238345190537383, policy loss: 10.194807613001927
Experience 24, Iter 36, disc loss: 0.00035971963683373927, policy loss: 10.555283371917657
Experience 24, Iter 37, disc loss: 0.00035873027723805936, policy loss: 10.282572573431516
Experience 24, Iter 38, disc loss: 0.0003094534481486734, policy loss: 11.150751529089028
Experience 24, Iter 39, disc loss: 0.0003738554234784346, policy loss: 10.88182391739042
Experience 24, Iter 40, disc loss: 0.0003268565590504682, policy loss: 10.870616051990112
Experience 24, Iter 41, disc loss: 0.00034474587235626153, policy loss: 10.60066679160577
Experience 24, Iter 42, disc loss: 0.0003320440488628573, policy loss: 10.596752115986618
Experience 24, Iter 43, disc loss: 0.0003542833324043296, policy loss: 10.45487688941403
Experience 24, Iter 44, disc loss: 0.0003415699929081547, policy loss: 10.577606802593133
Experience 24, Iter 45, disc loss: 0.0003164991814565317, policy loss: 11.924069325359993
Experience 24, Iter 46, disc loss: 0.0003542508497800436, policy loss: 10.085492919493259
Experience 24, Iter 47, disc loss: 0.00036756962531936973, policy loss: 11.38204349111276
Experience 24, Iter 48, disc loss: 0.0003544281182083466, policy loss: 10.542416107358349
Experience 24, Iter 49, disc loss: 0.00037394416936510036, policy loss: 11.89328748400196
Experience 24, Iter 50, disc loss: 0.0003709782916491295, policy loss: 11.00055361960932
Experience 24, Iter 51, disc loss: 0.00034489458289477693, policy loss: 11.392581986784917
Experience 24, Iter 52, disc loss: 0.0003416144819656297, policy loss: 10.933630373059472
Experience 24, Iter 53, disc loss: 0.00030314992242061304, policy loss: 11.829456075753894
Experience 24, Iter 54, disc loss: 0.00033235361637527354, policy loss: 11.304250071848932
Experience 24, Iter 55, disc loss: 0.0003625436959834917, policy loss: 10.517802850288124
Experience 24, Iter 56, disc loss: 0.00027986871215581534, policy loss: 12.70613750079912
Experience 24, Iter 57, disc loss: 0.0004028177637288168, policy loss: 9.87533041542112
Experience 24, Iter 58, disc loss: 0.0003599686074598896, policy loss: 10.344154663237036
Experience 24, Iter 59, disc loss: 0.000304525914206022, policy loss: 11.233023346029306
Experience 24, Iter 60, disc loss: 0.00031034227694468, policy loss: 11.373176556631298
Experience 24, Iter 61, disc loss: 0.0003571487361017571, policy loss: 10.702135786133855
Experience 24, Iter 62, disc loss: 0.00031949623827531165, policy loss: 10.55353024348811
Experience 24, Iter 63, disc loss: 0.00027754588836354855, policy loss: 11.200598630835751
Experience 24, Iter 64, disc loss: 0.00032224194008172, policy loss: 10.960346465115233
Experience 24, Iter 65, disc loss: 0.00038336797566233366, policy loss: 10.015035570394401
Experience 24, Iter 66, disc loss: 0.00030019858184289884, policy loss: 11.796437119391422
Experience 24, Iter 67, disc loss: 0.0003350188695939564, policy loss: 10.245236213772603
Experience 24, Iter 68, disc loss: 0.0003779197064997582, policy loss: 10.485411734542314
Experience 24, Iter 69, disc loss: 0.0003866728209359923, policy loss: 11.660122814753025
Experience 24, Iter 70, disc loss: 0.0002879680736296061, policy loss: 12.487469788028527
Experience 24, Iter 71, disc loss: 0.0002955853674489305, policy loss: 11.68844827801855
Experience 24, Iter 72, disc loss: 0.0003426313001932069, policy loss: 11.05554792617529
Experience 24, Iter 73, disc loss: 0.00030920611836079954, policy loss: 10.701210301546597
Experience 24, Iter 74, disc loss: 0.00034223904054174123, policy loss: 10.96563913650382
Experience 24, Iter 75, disc loss: 0.000381380262242532, policy loss: 9.975092131882729
Experience 24, Iter 76, disc loss: 0.0002846921567863299, policy loss: 11.04485774723677
Experience 24, Iter 77, disc loss: 0.0003524000753459211, policy loss: 11.516019393772808
Experience 24, Iter 78, disc loss: 0.0003622757591072289, policy loss: 10.416749923144511
Experience 24, Iter 79, disc loss: 0.000376825070551304, policy loss: 10.409833641424541
Experience 24, Iter 80, disc loss: 0.0003707789210682667, policy loss: 11.165912623506415
Experience 24, Iter 81, disc loss: 0.0003292617615125409, policy loss: 11.282808392109551
Experience 24, Iter 82, disc loss: 0.00033907936674678577, policy loss: 9.999539257700217
Experience 24, Iter 83, disc loss: 0.0002983207752929293, policy loss: 11.517090773112788
Experience 24, Iter 84, disc loss: 0.00032096624054417215, policy loss: 11.388361548714567
Experience 24, Iter 85, disc loss: 0.0003012825419250896, policy loss: 10.364184622080009
Experience 24, Iter 86, disc loss: 0.00032030875521587063, policy loss: 10.175472554626044
Experience 24, Iter 87, disc loss: 0.00033197583889078483, policy loss: 10.45310667017413
Experience 24, Iter 88, disc loss: 0.00031709156050056803, policy loss: 10.356282356684183
Experience 24, Iter 89, disc loss: 0.00032499079206011535, policy loss: 11.104847936058565
Experience 24, Iter 90, disc loss: 0.0003625910583266406, policy loss: 12.032484752371271
Experience 24, Iter 91, disc loss: 0.000300950690473578, policy loss: 11.697878848700576
Experience 24, Iter 92, disc loss: 0.00029551900356855165, policy loss: 10.639166429163884
Experience 24, Iter 93, disc loss: 0.00029339791822120993, policy loss: 11.149712814564747
Experience 24, Iter 94, disc loss: 0.0003039823292879203, policy loss: 10.928192650775955
Experience 24, Iter 95, disc loss: 0.00036105117297961667, policy loss: 11.130187125656322
Experience 24, Iter 96, disc loss: 0.0002713161114592827, policy loss: 11.363213397541891
Experience 24, Iter 97, disc loss: 0.0003572255168004745, policy loss: 10.766752981377614
Experience 24, Iter 98, disc loss: 0.0003044649349522384, policy loss: 10.136011683916383
Experience 24, Iter 99, disc loss: 0.00031706446921394817, policy loss: 11.552451154533209
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1640],
        [1.8049],
        [0.0417]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0113, 0.1669, 1.9455, 0.0321, 0.0274, 3.6510]],

        [[0.0113, 0.1669, 1.9455, 0.0321, 0.0274, 3.6510]],

        [[0.0113, 0.1669, 1.9455, 0.0321, 0.0274, 3.6510]],

        [[0.0113, 0.1669, 1.9455, 0.0321, 0.0274, 3.6510]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.6561, 7.2197, 0.1669], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.6561, 7.2197, 0.1669])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.379
Iter 2/2000 - Loss: 3.436
Iter 3/2000 - Loss: 3.161
Iter 4/2000 - Loss: 3.131
Iter 5/2000 - Loss: 3.110
Iter 6/2000 - Loss: 2.957
Iter 7/2000 - Loss: 2.768
Iter 8/2000 - Loss: 2.618
Iter 9/2000 - Loss: 2.500
Iter 10/2000 - Loss: 2.359
Iter 11/2000 - Loss: 2.167
Iter 12/2000 - Loss: 1.940
Iter 13/2000 - Loss: 1.707
Iter 14/2000 - Loss: 1.476
Iter 15/2000 - Loss: 1.240
Iter 16/2000 - Loss: 0.985
Iter 17/2000 - Loss: 0.708
Iter 18/2000 - Loss: 0.415
Iter 19/2000 - Loss: 0.116
Iter 20/2000 - Loss: -0.183
Iter 1981/2000 - Loss: -7.766
Iter 1982/2000 - Loss: -7.766
Iter 1983/2000 - Loss: -7.766
Iter 1984/2000 - Loss: -7.766
Iter 1985/2000 - Loss: -7.766
Iter 1986/2000 - Loss: -7.766
Iter 1987/2000 - Loss: -7.766
Iter 1988/2000 - Loss: -7.766
Iter 1989/2000 - Loss: -7.766
Iter 1990/2000 - Loss: -7.766
Iter 1991/2000 - Loss: -7.766
Iter 1992/2000 - Loss: -7.766
Iter 1993/2000 - Loss: -7.766
Iter 1994/2000 - Loss: -7.766
Iter 1995/2000 - Loss: -7.766
Iter 1996/2000 - Loss: -7.766
Iter 1997/2000 - Loss: -7.766
Iter 1998/2000 - Loss: -7.766
Iter 1999/2000 - Loss: -7.766
Iter 2000/2000 - Loss: -7.766
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[10.3524,  4.9606, 30.1471,  4.0095,  6.6134, 45.6855]],

        [[15.2186, 32.1289,  7.3116,  1.3039,  1.8129, 25.5512]],

        [[16.2808, 32.9885,  8.6927,  0.8943,  0.9348, 23.4296]],

        [[11.4030, 26.3557, 14.4283,  2.5869,  1.7647, 46.3995]]])
Signal Variance: tensor([ 0.0511,  1.8284, 14.8305,  0.5783])
Estimated target variance: tensor([0.0158, 0.6561, 7.2197, 0.1669])
N: 250
Signal to noise ratio: tensor([12.7366, 71.1430, 84.2243, 43.4843])
Bound on condition number: tensor([  40556.0252, 1265332.8403, 1773432.1100,  472722.9054])
Policy Optimizer learning rate:
9.750305025978745e-05
Experience 25, Iter 0, disc loss: 0.0003479631981396738, policy loss: 10.843912360218354
Experience 25, Iter 1, disc loss: 0.00028741238810608993, policy loss: 10.551134205066493
Experience 25, Iter 2, disc loss: 0.00031059044989104805, policy loss: 10.770088218197376
Experience 25, Iter 3, disc loss: 0.0003168374754183202, policy loss: 10.418860424743091
Experience 25, Iter 4, disc loss: 0.0003656198444546715, policy loss: 10.612239062472517
Experience 25, Iter 5, disc loss: 0.0003145106905630562, policy loss: 10.566044262324139
Experience 25, Iter 6, disc loss: 0.0002663592774606264, policy loss: 11.103775866319442
Experience 25, Iter 7, disc loss: 0.0003520882237802682, policy loss: 9.921845657063315
Experience 25, Iter 8, disc loss: 0.0003494499224417254, policy loss: 10.462587068359188
Experience 25, Iter 9, disc loss: 0.00033714077786608956, policy loss: 10.774122631715242
Experience 25, Iter 10, disc loss: 0.0003631832900222072, policy loss: 11.195319453954479
Experience 25, Iter 11, disc loss: 0.00037628116115719437, policy loss: 10.059317573479404
Experience 25, Iter 12, disc loss: 0.00041103043731080763, policy loss: 9.47522688282099
Experience 25, Iter 13, disc loss: 0.000337094685056811, policy loss: 10.569059621881228
Experience 25, Iter 14, disc loss: 0.0004148102093662639, policy loss: 9.923704269284276
Experience 25, Iter 15, disc loss: 0.00032757545292918524, policy loss: 10.577588347075523
Experience 25, Iter 16, disc loss: 0.0004090880051270232, policy loss: 10.086204893377527
Experience 25, Iter 17, disc loss: 0.0003232742834662505, policy loss: 10.565028092819185
Experience 25, Iter 18, disc loss: 0.00035707482145503965, policy loss: 10.711702356651308
Experience 25, Iter 19, disc loss: 0.00033002669907203266, policy loss: 10.95689482730392
Experience 25, Iter 20, disc loss: 0.0003502394963803352, policy loss: 11.33201171916893
Experience 25, Iter 21, disc loss: 0.0003644778553190183, policy loss: 10.344383817017833
Experience 25, Iter 22, disc loss: 0.0003057016930575866, policy loss: 11.301443910274955
Experience 25, Iter 23, disc loss: 0.00033359985779147843, policy loss: 10.529196786264036
Experience 25, Iter 24, disc loss: 0.00039398142644058657, policy loss: 10.203488656418347
Experience 25, Iter 25, disc loss: 0.0003306548682466825, policy loss: 10.229623149240652
Experience 25, Iter 26, disc loss: 0.00035847948918279565, policy loss: 10.629396634825435
Experience 25, Iter 27, disc loss: 0.0003583171918329401, policy loss: 10.925721456912806
Experience 25, Iter 28, disc loss: 0.00035003315660254867, policy loss: 11.762010272403092
Experience 25, Iter 29, disc loss: 0.0002723531739578777, policy loss: 12.357168632936384
Experience 25, Iter 30, disc loss: 0.00036665572214467047, policy loss: 10.66571488659045
Experience 25, Iter 31, disc loss: 0.0003584953490483922, policy loss: 9.962042789595987
Experience 25, Iter 32, disc loss: 0.00038873000966183047, policy loss: 10.649591163822596
Experience 25, Iter 33, disc loss: 0.0003262443374205205, policy loss: 10.564217755697527
Experience 25, Iter 34, disc loss: 0.0003427240641520579, policy loss: 10.899390958685867
Experience 25, Iter 35, disc loss: 0.0003377267241431441, policy loss: 10.899887125054539
Experience 25, Iter 36, disc loss: 0.00034260442020919637, policy loss: 11.027263342290395
Experience 25, Iter 37, disc loss: 0.0003483677939106506, policy loss: 10.81175163344168
Experience 25, Iter 38, disc loss: 0.0003632952471006707, policy loss: 10.686850120673878
Experience 25, Iter 39, disc loss: 0.00039440535200766424, policy loss: 10.92186863462003
Experience 25, Iter 40, disc loss: 0.0003246826159243658, policy loss: 11.884703769491836
Experience 25, Iter 41, disc loss: 0.000345771541787049, policy loss: 10.141106233706504
Experience 25, Iter 42, disc loss: 0.00044050220700956526, policy loss: 10.662818104959271
Experience 25, Iter 43, disc loss: 0.00037662266935881123, policy loss: 10.643443758542249
Experience 25, Iter 44, disc loss: 0.00030869891425815897, policy loss: 12.128303364963912
Experience 25, Iter 45, disc loss: 0.0002513831724065099, policy loss: 11.244163696918733
Experience 25, Iter 46, disc loss: 0.00032510025610487953, policy loss: 10.716411481785501
Experience 25, Iter 47, disc loss: 0.000307688424580568, policy loss: 11.63411061297612
Experience 25, Iter 48, disc loss: 0.00033224147797425083, policy loss: 10.724459512109542
Experience 25, Iter 49, disc loss: 0.0002971656549578002, policy loss: 10.624655605112352
Experience 25, Iter 50, disc loss: 0.0002875344776536203, policy loss: 10.778342850450407
Experience 25, Iter 51, disc loss: 0.0003925520293489864, policy loss: 10.247630605223284
Experience 25, Iter 52, disc loss: 0.0003002727373927859, policy loss: 10.985174503323254
Experience 25, Iter 53, disc loss: 0.00030443197595404493, policy loss: 10.681851789311375
Experience 25, Iter 54, disc loss: 0.0002878443040017318, policy loss: 11.453958570892471
Experience 25, Iter 55, disc loss: 0.0003998872898643554, policy loss: 10.173792902683466
Experience 25, Iter 56, disc loss: 0.0003514680775844833, policy loss: 10.455439913757239
Experience 25, Iter 57, disc loss: 0.00031319684492007473, policy loss: 11.649587470930618
Experience 25, Iter 58, disc loss: 0.00030314818813858755, policy loss: 11.655528610514235
Experience 25, Iter 59, disc loss: 0.00031042811315716464, policy loss: 11.00976393504237
Experience 25, Iter 60, disc loss: 0.00029859710941497524, policy loss: 11.561707832471662
Experience 25, Iter 61, disc loss: 0.0003426770321965295, policy loss: 11.153454299372083
Experience 25, Iter 62, disc loss: 0.0003282825446701928, policy loss: 11.073167407392251
Experience 25, Iter 63, disc loss: 0.00034685742946657445, policy loss: 11.139796158730396
Experience 25, Iter 64, disc loss: 0.00028387262407127405, policy loss: 11.668882371293943
Experience 25, Iter 65, disc loss: 0.00032342702379573713, policy loss: 10.33161705353319
Experience 25, Iter 66, disc loss: 0.0003758146849191221, policy loss: 10.813735012378629
Experience 25, Iter 67, disc loss: 0.0003037801382713589, policy loss: 10.948478267218873
Experience 25, Iter 68, disc loss: 0.00030359474778725976, policy loss: 10.654248731742545
Experience 25, Iter 69, disc loss: 0.00026849519713803105, policy loss: 12.05483195674742
Experience 25, Iter 70, disc loss: 0.00034750793985032977, policy loss: 11.097366671123593
Experience 25, Iter 71, disc loss: 0.00035571268207464573, policy loss: 10.263663855886076
Experience 25, Iter 72, disc loss: 0.0003081212305527774, policy loss: 10.71936021547689
Experience 25, Iter 73, disc loss: 0.0003089683148160903, policy loss: 10.545967108498118
Experience 25, Iter 74, disc loss: 0.0003055824799229746, policy loss: 11.403631546615909
Experience 25, Iter 75, disc loss: 0.0002577443480782503, policy loss: 12.047641961681883
Experience 25, Iter 76, disc loss: 0.00033328166065749377, policy loss: 10.41258370727179
Experience 25, Iter 77, disc loss: 0.000272433440811071, policy loss: 11.642308238120526
Experience 25, Iter 78, disc loss: 0.0002459052276143225, policy loss: 11.177722445598274
Experience 25, Iter 79, disc loss: 0.00030044707027718784, policy loss: 10.86609628395907
Experience 25, Iter 80, disc loss: 0.00032290454075840816, policy loss: 10.50903316883019
Experience 25, Iter 81, disc loss: 0.00029018618466682367, policy loss: 10.904987305772387
Experience 25, Iter 82, disc loss: 0.0002999299169928948, policy loss: 11.027743946420344
Experience 25, Iter 83, disc loss: 0.0002657623460809147, policy loss: 10.79736105026642
Experience 25, Iter 84, disc loss: 0.0002720183119532951, policy loss: 11.258716167637957
Experience 25, Iter 85, disc loss: 0.0002729047923067515, policy loss: 10.233022370496787
Experience 25, Iter 86, disc loss: 0.00023730051553822834, policy loss: 11.075663619211381
Experience 25, Iter 87, disc loss: 0.00030079663700324007, policy loss: 10.742010682677364
Experience 25, Iter 88, disc loss: 0.0002647079160608063, policy loss: 11.158220651647495
Experience 25, Iter 89, disc loss: 0.00025706747481556294, policy loss: 11.504150914989317
Experience 25, Iter 90, disc loss: 0.00024358052846980014, policy loss: 10.575094443632516
Experience 25, Iter 91, disc loss: 0.0002878839789731721, policy loss: 10.845962010371625
Experience 25, Iter 92, disc loss: 0.0002886273050695129, policy loss: 10.720738010264428
Experience 25, Iter 93, disc loss: 0.00028587322456886087, policy loss: 10.30618825214828
Experience 25, Iter 94, disc loss: 0.0003831333451050795, policy loss: 10.675025871045188
Experience 25, Iter 95, disc loss: 0.0003049422886720155, policy loss: 10.652737698665282
Experience 25, Iter 96, disc loss: 0.0003034299204073327, policy loss: 10.446536200557384
Experience 25, Iter 97, disc loss: 0.000235801813414237, policy loss: 11.528092744612174
Experience 25, Iter 98, disc loss: 0.0003316650919282213, policy loss: 10.678896202197278
Experience 25, Iter 99, disc loss: 0.000348213271775414, policy loss: 10.243624278831998
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.1684],
        [1.8210],
        [0.0418]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0111, 0.1664, 1.9506, 0.0323, 0.0276, 3.7672]],

        [[0.0111, 0.1664, 1.9506, 0.0323, 0.0276, 3.7672]],

        [[0.0111, 0.1664, 1.9506, 0.0323, 0.0276, 3.7672]],

        [[0.0111, 0.1664, 1.9506, 0.0323, 0.0276, 3.7672]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0157, 0.6735, 7.2841, 0.1671], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0157, 0.6735, 7.2841, 0.1671])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.385
Iter 2/2000 - Loss: 3.446
Iter 3/2000 - Loss: 3.158
Iter 4/2000 - Loss: 3.122
Iter 5/2000 - Loss: 3.100
Iter 6/2000 - Loss: 2.942
Iter 7/2000 - Loss: 2.743
Iter 8/2000 - Loss: 2.579
Iter 9/2000 - Loss: 2.451
Iter 10/2000 - Loss: 2.307
Iter 11/2000 - Loss: 2.114
Iter 12/2000 - Loss: 1.883
Iter 13/2000 - Loss: 1.640
Iter 14/2000 - Loss: 1.401
Iter 15/2000 - Loss: 1.161
Iter 16/2000 - Loss: 0.906
Iter 17/2000 - Loss: 0.629
Iter 18/2000 - Loss: 0.335
Iter 19/2000 - Loss: 0.033
Iter 20/2000 - Loss: -0.269
Iter 1981/2000 - Loss: -7.826
Iter 1982/2000 - Loss: -7.826
Iter 1983/2000 - Loss: -7.826
Iter 1984/2000 - Loss: -7.826
Iter 1985/2000 - Loss: -7.826
Iter 1986/2000 - Loss: -7.826
Iter 1987/2000 - Loss: -7.826
Iter 1988/2000 - Loss: -7.826
Iter 1989/2000 - Loss: -7.826
Iter 1990/2000 - Loss: -7.826
Iter 1991/2000 - Loss: -7.826
Iter 1992/2000 - Loss: -7.826
Iter 1993/2000 - Loss: -7.826
Iter 1994/2000 - Loss: -7.826
Iter 1995/2000 - Loss: -7.826
Iter 1996/2000 - Loss: -7.826
Iter 1997/2000 - Loss: -7.826
Iter 1998/2000 - Loss: -7.826
Iter 1999/2000 - Loss: -7.826
Iter 2000/2000 - Loss: -7.826
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.9899,  4.9830, 27.7143,  3.9021,  7.4620, 46.2985]],

        [[15.2460, 31.8076,  7.2991,  1.2826,  1.7752, 24.8827]],

        [[16.2830, 32.0608,  8.7208,  0.9184,  0.9049, 23.6131]],

        [[11.3982, 26.2467, 14.2467,  2.5602,  1.8049, 46.5903]]])
Signal Variance: tensor([ 0.0514,  1.7554, 15.2702,  0.5729])
Estimated target variance: tensor([0.0157, 0.6735, 7.2841, 0.1671])
N: 260
Signal to noise ratio: tensor([12.9051, 70.1884, 84.2528, 43.7277])
Bound on condition number: tensor([  43302.0930, 1280867.6860, 1845620.7474,  497150.6702])
Policy Optimizer learning rate:
9.740037464252974e-05
Experience 26, Iter 0, disc loss: 0.0002662869524176198, policy loss: 10.78096489631412
Experience 26, Iter 1, disc loss: 0.00024858495870791903, policy loss: 11.277382629053092
Experience 26, Iter 2, disc loss: 0.000338148867572633, policy loss: 10.394432304131854
Experience 26, Iter 3, disc loss: 0.0002866373726058328, policy loss: 10.392584963403852
Experience 26, Iter 4, disc loss: 0.00032027259136282923, policy loss: 10.84982626170394
Experience 26, Iter 5, disc loss: 0.00031105265717518496, policy loss: 10.530284132381945
Experience 26, Iter 6, disc loss: 0.0002601205915836902, policy loss: 10.749842687560912
Experience 26, Iter 7, disc loss: 0.0002975736413814981, policy loss: 10.617161572975334
Experience 26, Iter 8, disc loss: 0.0002934825542655735, policy loss: 10.95619065002171
Experience 26, Iter 9, disc loss: 0.00032335711575609983, policy loss: 10.65973531080966
Experience 26, Iter 10, disc loss: 0.0003055661624494543, policy loss: 11.480983255390564
Experience 26, Iter 11, disc loss: 0.0003109714959631182, policy loss: 10.279234385356318
Experience 26, Iter 12, disc loss: 0.00036446375704863584, policy loss: 10.150678502116545
Experience 26, Iter 13, disc loss: 0.00030405735701031274, policy loss: 11.46416912755415
Experience 26, Iter 14, disc loss: 0.00027483314669081806, policy loss: 10.94558069001895
Experience 26, Iter 15, disc loss: 0.0002836049802687732, policy loss: 10.26066253981444
Experience 26, Iter 16, disc loss: 0.00027741144080993454, policy loss: 11.366547083385267
Experience 26, Iter 17, disc loss: 0.00029482685147649173, policy loss: 11.01862925561046
Experience 26, Iter 18, disc loss: 0.000281388564208611, policy loss: 10.581032487823762
Experience 26, Iter 19, disc loss: 0.000275196247434145, policy loss: 11.643034105696346
Experience 26, Iter 20, disc loss: 0.000301030884527883, policy loss: 10.7681688832655
Experience 26, Iter 21, disc loss: 0.00026916625414873086, policy loss: 11.174111370565658
Experience 26, Iter 22, disc loss: 0.00031227166674614845, policy loss: 11.279912696570587
Experience 26, Iter 23, disc loss: 0.0002896343442329722, policy loss: 10.323848586660434
Experience 26, Iter 24, disc loss: 0.0003015887625221127, policy loss: 10.269179940504129
Experience 26, Iter 25, disc loss: 0.00027553152409351536, policy loss: 10.810891295175649
Experience 26, Iter 26, disc loss: 0.0003298759486473121, policy loss: 10.737496716707962
Experience 26, Iter 27, disc loss: 0.0003092000403557238, policy loss: 10.751378797964355
Experience 26, Iter 28, disc loss: 0.0003219411910502188, policy loss: 11.58061308455039
Experience 26, Iter 29, disc loss: 0.00028855995888985356, policy loss: 11.81502529296514
Experience 26, Iter 30, disc loss: 0.0003844180252934801, policy loss: 9.897483564895555
Experience 26, Iter 31, disc loss: 0.0002901779452754059, policy loss: 10.997570392458968
Experience 26, Iter 32, disc loss: 0.0002843279297534706, policy loss: 10.917882379099927
Experience 26, Iter 33, disc loss: 0.0003186304686971104, policy loss: 11.298610668357362
Experience 26, Iter 34, disc loss: 0.0002810857951984001, policy loss: 10.874084569196192
Experience 26, Iter 35, disc loss: 0.0002940371165539691, policy loss: 10.707936944949362
Experience 26, Iter 36, disc loss: 0.00029361981603129463, policy loss: 10.932425103931322
Experience 26, Iter 37, disc loss: 0.0003033972238962367, policy loss: 11.337659639000714
Experience 26, Iter 38, disc loss: 0.00030090175041251274, policy loss: 10.802179218896356
Experience 26, Iter 39, disc loss: 0.0002915386382493527, policy loss: 10.999689378224772
Experience 26, Iter 40, disc loss: 0.00027583569259465203, policy loss: 11.682839885820915
Experience 26, Iter 41, disc loss: 0.0003225179770393285, policy loss: 11.541402405860596
Experience 26, Iter 42, disc loss: 0.0002755866257512503, policy loss: 11.722329338124304
Experience 26, Iter 43, disc loss: 0.00028154845318718554, policy loss: 10.941260806532277
Experience 26, Iter 44, disc loss: 0.000305376172785576, policy loss: 10.132237080855784
Experience 26, Iter 45, disc loss: 0.0002658311445970491, policy loss: 10.968235187511667
Experience 26, Iter 46, disc loss: 0.00030635689613907785, policy loss: 11.36947551528966
Experience 26, Iter 47, disc loss: 0.0002896658698332109, policy loss: 10.870303020160954
Experience 26, Iter 48, disc loss: 0.00028973730451669325, policy loss: 11.567306618959782
Experience 26, Iter 49, disc loss: 0.0003168618754273292, policy loss: 10.709912059342265
Experience 26, Iter 50, disc loss: 0.00027733504353637457, policy loss: 10.746827956853863
Experience 26, Iter 51, disc loss: 0.0003912216245229616, policy loss: 10.17477459626689
Experience 26, Iter 52, disc loss: 0.00030382356837391276, policy loss: 11.109503466149881
Experience 26, Iter 53, disc loss: 0.00030540149078672535, policy loss: 10.78150365358627
Experience 26, Iter 54, disc loss: 0.0002906677360446148, policy loss: 10.92227657401546
Experience 26, Iter 55, disc loss: 0.00029200945348907813, policy loss: 10.643180172912249
Experience 26, Iter 56, disc loss: 0.0002835177492260733, policy loss: 11.157225838722297
Experience 26, Iter 57, disc loss: 0.0003035197675794317, policy loss: 10.978571333279811
Experience 26, Iter 58, disc loss: 0.00030261277514954043, policy loss: 10.267919477564012
Experience 26, Iter 59, disc loss: 0.0002596362078242622, policy loss: 11.373495634186662
Experience 26, Iter 60, disc loss: 0.00030131258728107976, policy loss: 10.440226906499106
Experience 26, Iter 61, disc loss: 0.0002705891383017738, policy loss: 10.858150326823054
Experience 26, Iter 62, disc loss: 0.0002936333301384402, policy loss: 10.364295121883966
Experience 26, Iter 63, disc loss: 0.00029685183078895856, policy loss: 10.403178971220168
Experience 26, Iter 64, disc loss: 0.00026117023267810516, policy loss: 11.48714182091901
Experience 26, Iter 65, disc loss: 0.0002475796336598904, policy loss: 11.160469325533452
Experience 26, Iter 66, disc loss: 0.000258172466133457, policy loss: 10.575876105099821
Experience 26, Iter 67, disc loss: 0.000314002201563992, policy loss: 10.724955849541805
Experience 26, Iter 68, disc loss: 0.00024083801350724166, policy loss: 11.716470930336548
Experience 26, Iter 69, disc loss: 0.00029916366549765565, policy loss: 11.581677784469807
Experience 26, Iter 70, disc loss: 0.00027113010987103183, policy loss: 11.143137879530661
Experience 26, Iter 71, disc loss: 0.0002591202312492955, policy loss: 11.120247768999343
Experience 26, Iter 72, disc loss: 0.00026925673639656173, policy loss: 10.19625867246569
Experience 26, Iter 73, disc loss: 0.00029060770680128765, policy loss: 11.337520646790788
Experience 26, Iter 74, disc loss: 0.0002787309257442421, policy loss: 10.92719652576378
Experience 26, Iter 75, disc loss: 0.00029746348875004313, policy loss: 11.087297739632977
Experience 26, Iter 76, disc loss: 0.00026086560820713796, policy loss: 10.848947254743832
Experience 26, Iter 77, disc loss: 0.00025315491657329546, policy loss: 11.250241488882343
Experience 26, Iter 78, disc loss: 0.0003114581679153867, policy loss: 10.485553398212698
Experience 26, Iter 79, disc loss: 0.0002499978156790707, policy loss: 10.900000689250515
Experience 26, Iter 80, disc loss: 0.00024955915750278816, policy loss: 11.768117196125102
Experience 26, Iter 81, disc loss: 0.000238797817118473, policy loss: 11.63187224196144
Experience 26, Iter 82, disc loss: 0.00022055132399606677, policy loss: 10.971851995194
Experience 26, Iter 83, disc loss: 0.000316204545993631, policy loss: 10.419095715933897
Experience 26, Iter 84, disc loss: 0.0002633004722132236, policy loss: 10.652120503031309
Experience 26, Iter 85, disc loss: 0.0002554000824379868, policy loss: 10.758071742497926
Experience 26, Iter 86, disc loss: 0.00029511427766434385, policy loss: 10.670374907521119
Experience 26, Iter 87, disc loss: 0.0003113201290333323, policy loss: 11.040354075425205
Experience 26, Iter 88, disc loss: 0.00021501374220631375, policy loss: 10.981410420407041
Experience 26, Iter 89, disc loss: 0.0002602868762538493, policy loss: 10.682212323389772
Experience 26, Iter 90, disc loss: 0.0002464960265853446, policy loss: 10.558441702726103
Experience 26, Iter 91, disc loss: 0.00024398428457250896, policy loss: 11.056065444714918
Experience 26, Iter 92, disc loss: 0.0002790584829864748, policy loss: 11.456291640142414
Experience 26, Iter 93, disc loss: 0.00022045500384022462, policy loss: 10.872727569106384
Experience 26, Iter 94, disc loss: 0.000235202614850354, policy loss: 10.810671569571127
Experience 26, Iter 95, disc loss: 0.000246162827918715, policy loss: 10.346141245354247
Experience 26, Iter 96, disc loss: 0.00026635234946557104, policy loss: 10.865497643479188
Experience 26, Iter 97, disc loss: 0.0002645777357034255, policy loss: 11.126574055661859
Experience 26, Iter 98, disc loss: 0.00027165796471258376, policy loss: 10.516359097590069
Experience 26, Iter 99, disc loss: 0.0002442390128150194, policy loss: 11.494832617196804
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1748],
        [1.8536],
        [0.0423]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0108, 0.1673, 1.9795, 0.0329, 0.0281, 3.8928]],

        [[0.0108, 0.1673, 1.9795, 0.0329, 0.0281, 3.8928]],

        [[0.0108, 0.1673, 1.9795, 0.0329, 0.0281, 3.8928]],

        [[0.0108, 0.1673, 1.9795, 0.0329, 0.0281, 3.8928]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.6990, 7.4143, 0.1692], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.6990, 7.4143, 0.1692])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.427
Iter 2/2000 - Loss: 3.489
Iter 3/2000 - Loss: 3.194
Iter 4/2000 - Loss: 3.157
Iter 5/2000 - Loss: 3.136
Iter 6/2000 - Loss: 2.979
Iter 7/2000 - Loss: 2.777
Iter 8/2000 - Loss: 2.607
Iter 9/2000 - Loss: 2.472
Iter 10/2000 - Loss: 2.323
Iter 11/2000 - Loss: 2.128
Iter 12/2000 - Loss: 1.894
Iter 13/2000 - Loss: 1.647
Iter 14/2000 - Loss: 1.402
Iter 15/2000 - Loss: 1.157
Iter 16/2000 - Loss: 0.899
Iter 17/2000 - Loss: 0.620
Iter 18/2000 - Loss: 0.325
Iter 19/2000 - Loss: 0.021
Iter 20/2000 - Loss: -0.282
Iter 1981/2000 - Loss: -7.872
Iter 1982/2000 - Loss: -7.872
Iter 1983/2000 - Loss: -7.872
Iter 1984/2000 - Loss: -7.872
Iter 1985/2000 - Loss: -7.872
Iter 1986/2000 - Loss: -7.872
Iter 1987/2000 - Loss: -7.872
Iter 1988/2000 - Loss: -7.872
Iter 1989/2000 - Loss: -7.872
Iter 1990/2000 - Loss: -7.872
Iter 1991/2000 - Loss: -7.872
Iter 1992/2000 - Loss: -7.872
Iter 1993/2000 - Loss: -7.873
Iter 1994/2000 - Loss: -7.873
Iter 1995/2000 - Loss: -7.873
Iter 1996/2000 - Loss: -7.873
Iter 1997/2000 - Loss: -7.873
Iter 1998/2000 - Loss: -7.873
Iter 1999/2000 - Loss: -7.873
Iter 2000/2000 - Loss: -7.873
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[ 9.8236,  5.0014, 29.2070,  3.3298,  6.4571, 46.6671]],

        [[14.4896, 31.2969,  7.2529,  1.2733,  1.7771, 23.3021]],

        [[16.6598, 31.5774,  8.5486,  0.9110,  0.9046, 23.1446]],

        [[11.5684, 25.5308, 14.4064,  2.5112,  1.8214, 45.9194]]])
Signal Variance: tensor([ 0.0512,  1.6189, 14.4572,  0.5813])
Estimated target variance: tensor([0.0158, 0.6990, 7.4143, 0.1692])
N: 270
Signal to noise ratio: tensor([12.8275, 67.9549, 82.7063, 44.0921])
Bound on condition number: tensor([  44427.8190, 1246825.0696, 1846889.5395,  524911.4625])
Policy Optimizer learning rate:
9.729780714786256e-05
Experience 27, Iter 0, disc loss: 0.00026888330508703857, policy loss: 10.53294160441639
Experience 27, Iter 1, disc loss: 0.00026909404037029104, policy loss: 10.698956064973533
Experience 27, Iter 2, disc loss: 0.0002463257859895719, policy loss: 11.276798269606452
Experience 27, Iter 3, disc loss: 0.00022914403121516747, policy loss: 11.623088431618728
Experience 27, Iter 4, disc loss: 0.00023826130321460628, policy loss: 11.303134155383587
Experience 27, Iter 5, disc loss: 0.00026331786623818806, policy loss: 10.340533553412651
Experience 27, Iter 6, disc loss: 0.00026647492318082014, policy loss: 11.450310552904892
Experience 27, Iter 7, disc loss: 0.000271428748253573, policy loss: 11.31492986239924
Experience 27, Iter 8, disc loss: 0.0002925931040643787, policy loss: 11.319699679222131
Experience 27, Iter 9, disc loss: 0.000272541069551241, policy loss: 11.402958122610652
Experience 27, Iter 10, disc loss: 0.00027460627450542484, policy loss: 10.504027164698687
Experience 27, Iter 11, disc loss: 0.0002926459683658687, policy loss: 10.959805275610835
Experience 27, Iter 12, disc loss: 0.00033350223582149705, policy loss: 10.109715655863884
Experience 27, Iter 13, disc loss: 0.00027994128018435534, policy loss: 10.854947356176934
Experience 27, Iter 14, disc loss: 0.0002619882926702964, policy loss: 11.736923182430834
Experience 27, Iter 15, disc loss: 0.00024645363257383353, policy loss: 11.370425470818683
Experience 27, Iter 16, disc loss: 0.0002692008245616651, policy loss: 10.118988612128469
Experience 27, Iter 17, disc loss: 0.0003282793873072002, policy loss: 10.146448583588302
Experience 27, Iter 18, disc loss: 0.00025821055362168033, policy loss: 10.993848745901337
Experience 27, Iter 19, disc loss: 0.00026847397099155976, policy loss: 11.87465391570244
Experience 27, Iter 20, disc loss: 0.0002901834230341113, policy loss: 11.634526929898861
Experience 27, Iter 21, disc loss: 0.00026604715729021703, policy loss: 11.309209139934365
Experience 27, Iter 22, disc loss: 0.00030319629980012666, policy loss: 10.614761973839617
Experience 27, Iter 23, disc loss: 0.00023549376098394673, policy loss: 11.543839277224496
Experience 27, Iter 24, disc loss: 0.00022165963202786798, policy loss: 11.894474804383957
Experience 27, Iter 25, disc loss: 0.0002601791678876497, policy loss: 10.436108889737234
Experience 27, Iter 26, disc loss: 0.00029121243058687746, policy loss: 10.669687717313186
Experience 27, Iter 27, disc loss: 0.00031494933481122136, policy loss: 10.513950491455132
Experience 27, Iter 28, disc loss: 0.00031586040445540336, policy loss: 10.956722838678196
Experience 27, Iter 29, disc loss: 0.0002819199301737649, policy loss: 11.2426344104217
Experience 27, Iter 30, disc loss: 0.0002760823449565158, policy loss: 10.543712564898454
Experience 27, Iter 31, disc loss: 0.0002922816156636603, policy loss: 10.930691946362053
Experience 27, Iter 32, disc loss: 0.0002661004499426501, policy loss: 11.385057616170851
Experience 27, Iter 33, disc loss: 0.0002632773014126592, policy loss: 11.965912770870393
Experience 27, Iter 34, disc loss: 0.00027347681267633945, policy loss: 10.92931126132974
Experience 27, Iter 35, disc loss: 0.00022035785221796245, policy loss: 10.752871039999581
Experience 27, Iter 36, disc loss: 0.0002484313776472848, policy loss: 11.126191792503194
Experience 27, Iter 37, disc loss: 0.0002555567197740546, policy loss: 11.317989077858835
Experience 27, Iter 38, disc loss: 0.0002832671267499808, policy loss: 11.240432880561984
Experience 27, Iter 39, disc loss: 0.0002955171928010009, policy loss: 10.210835143783838
Experience 27, Iter 40, disc loss: 0.0002932440122740836, policy loss: 10.932256739653184
Experience 27, Iter 41, disc loss: 0.0002672295585023512, policy loss: 10.97792362602783
Experience 27, Iter 42, disc loss: 0.0002851553040768472, policy loss: 10.931014318166124
Experience 27, Iter 43, disc loss: 0.0002589373079136813, policy loss: 11.136137771799419
Experience 27, Iter 44, disc loss: 0.00026567301650529855, policy loss: 10.372677003338055
Experience 27, Iter 45, disc loss: 0.0002708032049793654, policy loss: 10.541997016084768
Experience 27, Iter 46, disc loss: 0.00026755280301240245, policy loss: 10.55247226128964
Experience 27, Iter 47, disc loss: 0.00021416849375279547, policy loss: 11.459200483374556
Experience 27, Iter 48, disc loss: 0.00029837427625492303, policy loss: 10.984639237521883
Experience 27, Iter 49, disc loss: 0.0002416742450377217, policy loss: 10.57974025756586
Experience 27, Iter 50, disc loss: 0.0002574275980128767, policy loss: 10.504098829994728
Experience 27, Iter 51, disc loss: 0.000256619503313399, policy loss: 10.354475743171243
Experience 27, Iter 52, disc loss: 0.00021701153018594097, policy loss: 11.953358957372604
Experience 27, Iter 53, disc loss: 0.0002693549422558234, policy loss: 10.207048808798355
Experience 27, Iter 54, disc loss: 0.00023937351041707186, policy loss: 11.485099373039123
Experience 27, Iter 55, disc loss: 0.00024339919050680387, policy loss: 12.026406189315214
Experience 27, Iter 56, disc loss: 0.00019383261666498084, policy loss: 12.449023437203577
Experience 27, Iter 57, disc loss: 0.0002260693455152492, policy loss: 11.398736286343441
Experience 27, Iter 58, disc loss: 0.0002455478115651673, policy loss: 10.726182226108035
Experience 27, Iter 59, disc loss: 0.00026005846395247103, policy loss: 10.457094199746841
Experience 27, Iter 60, disc loss: 0.0002556178239831428, policy loss: 11.277155587537697
Experience 27, Iter 61, disc loss: 0.00022405624619334803, policy loss: 11.798991240440646
Experience 27, Iter 62, disc loss: 0.00023876306100640448, policy loss: 11.288911380639036
Experience 27, Iter 63, disc loss: 0.00024349146510703335, policy loss: 10.670855808530694
Experience 27, Iter 64, disc loss: 0.00030449854470711594, policy loss: 10.54872652354534
Experience 27, Iter 65, disc loss: 0.00025325177981846264, policy loss: 10.864022605880635
Experience 27, Iter 66, disc loss: 0.000294757138941108, policy loss: 11.044033105320418
Experience 27, Iter 67, disc loss: 0.0002500599027559882, policy loss: 10.766439542277965
Experience 27, Iter 68, disc loss: 0.00021645681557456932, policy loss: 10.713069730275397
Experience 27, Iter 69, disc loss: 0.00027257200617779656, policy loss: 11.282336409075398
Experience 27, Iter 70, disc loss: 0.0001843726668209396, policy loss: 11.712294431124455
Experience 27, Iter 71, disc loss: 0.0002473422521953371, policy loss: 11.727134700110785
Experience 27, Iter 72, disc loss: 0.00027629620379093355, policy loss: 10.91563355155457
Experience 27, Iter 73, disc loss: 0.00022162421135406673, policy loss: 10.706554482130318
Experience 27, Iter 74, disc loss: 0.00025981466087481795, policy loss: 11.062070905907134
Experience 27, Iter 75, disc loss: 0.0002774210316264093, policy loss: 10.565442041880065
Experience 27, Iter 76, disc loss: 0.00026028131413498485, policy loss: 11.34153523616199
Experience 27, Iter 77, disc loss: 0.00024996225702312506, policy loss: 11.043887490986487
Experience 27, Iter 78, disc loss: 0.00024441723874023854, policy loss: 11.069562684829506
Experience 27, Iter 79, disc loss: 0.00023051227053429106, policy loss: 11.260525225040737
Experience 27, Iter 80, disc loss: 0.0002295940580031925, policy loss: 11.55194902267838
Experience 27, Iter 81, disc loss: 0.00022350470450063388, policy loss: 11.292515041893704
Experience 27, Iter 82, disc loss: 0.00021974163658907792, policy loss: 11.10447134517798
Experience 27, Iter 83, disc loss: 0.0002319315136709982, policy loss: 10.790301067281623
Experience 27, Iter 84, disc loss: 0.0002652528272412494, policy loss: 10.956145575272231
Experience 27, Iter 85, disc loss: 0.00021874923872645003, policy loss: 10.591914094593758
Experience 27, Iter 86, disc loss: 0.00024078599419118183, policy loss: 10.400503970595778
Experience 27, Iter 87, disc loss: 0.00020325474717832753, policy loss: 11.575787427082362
Experience 27, Iter 88, disc loss: 0.00024258355942089732, policy loss: 11.382265320014962
Experience 27, Iter 89, disc loss: 0.00028381275869736837, policy loss: 10.76944956747587
Experience 27, Iter 90, disc loss: 0.00022560949224568658, policy loss: 10.767411028726478
Experience 27, Iter 91, disc loss: 0.0001845821536581689, policy loss: 11.09600834616674
Experience 27, Iter 92, disc loss: 0.00019043864703576066, policy loss: 11.258454629561639
Experience 27, Iter 93, disc loss: 0.00021173914949988032, policy loss: 11.337427384553624
Experience 27, Iter 94, disc loss: 0.00024397349972674225, policy loss: 11.077633699211471
Experience 27, Iter 95, disc loss: 0.00024224567292244057, policy loss: 10.676236510770774
Experience 27, Iter 96, disc loss: 0.0002213547814359234, policy loss: 10.686803234717921
Experience 27, Iter 97, disc loss: 0.00023018974318150023, policy loss: 11.463399058289546
Experience 27, Iter 98, disc loss: 0.00027764036521458373, policy loss: 11.054482047915101
Experience 27, Iter 99, disc loss: 0.00021248085852372767, policy loss: 11.283976076114325
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1810],
        [1.8864],
        [0.0430]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0106, 0.1683, 2.0109, 0.0333, 0.0283, 4.0140]],

        [[0.0106, 0.1683, 2.0109, 0.0333, 0.0283, 4.0140]],

        [[0.0106, 0.1683, 2.0109, 0.0333, 0.0283, 4.0140]],

        [[0.0106, 0.1683, 2.0109, 0.0333, 0.0283, 4.0140]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.7240, 7.5456, 0.1718], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.7240, 7.5456, 0.1718])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.462
Iter 2/2000 - Loss: 3.523
Iter 3/2000 - Loss: 3.222
Iter 4/2000 - Loss: 3.178
Iter 5/2000 - Loss: 3.153
Iter 6/2000 - Loss: 2.991
Iter 7/2000 - Loss: 2.780
Iter 8/2000 - Loss: 2.600
Iter 9/2000 - Loss: 2.454
Iter 10/2000 - Loss: 2.300
Iter 11/2000 - Loss: 2.101
Iter 12/2000 - Loss: 1.864
Iter 13/2000 - Loss: 1.612
Iter 14/2000 - Loss: 1.362
Iter 15/2000 - Loss: 1.113
Iter 16/2000 - Loss: 0.855
Iter 17/2000 - Loss: 0.578
Iter 18/2000 - Loss: 0.284
Iter 19/2000 - Loss: -0.019
Iter 20/2000 - Loss: -0.321
Iter 1981/2000 - Loss: -7.897
Iter 1982/2000 - Loss: -7.897
Iter 1983/2000 - Loss: -7.897
Iter 1984/2000 - Loss: -7.897
Iter 1985/2000 - Loss: -7.897
Iter 1986/2000 - Loss: -7.897
Iter 1987/2000 - Loss: -7.897
Iter 1988/2000 - Loss: -7.897
Iter 1989/2000 - Loss: -7.897
Iter 1990/2000 - Loss: -7.897
Iter 1991/2000 - Loss: -7.897
Iter 1992/2000 - Loss: -7.897
Iter 1993/2000 - Loss: -7.897
Iter 1994/2000 - Loss: -7.897
Iter 1995/2000 - Loss: -7.897
Iter 1996/2000 - Loss: -7.897
Iter 1997/2000 - Loss: -7.897
Iter 1998/2000 - Loss: -7.897
Iter 1999/2000 - Loss: -7.898
Iter 2000/2000 - Loss: -7.898
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.6285,  5.1640, 30.5808,  3.4210,  6.7114, 47.4274]],

        [[14.6497, 30.6645,  7.2374,  1.2781,  1.7552, 23.3771]],

        [[16.5370, 29.8837,  8.4347,  0.9259,  0.8964, 22.4270]],

        [[11.3140, 25.4405, 14.7414,  2.4846,  1.8438, 44.8049]]])
Signal Variance: tensor([ 0.0518,  1.6236, 13.8969,  0.5767])
Estimated target variance: tensor([0.0158, 0.7240, 7.5456, 0.1718])
N: 280
Signal to noise ratio: tensor([12.7246, 68.8677, 80.0770, 43.5604])
Bound on condition number: tensor([  45337.5052, 1327975.3279, 1795450.1729,  531302.5747])
Policy Optimizer learning rate:
9.71953476619274e-05
Experience 28, Iter 0, disc loss: 0.0002729806602789699, policy loss: 10.952241351518694
Experience 28, Iter 1, disc loss: 0.00026714687131936633, policy loss: 10.911245594634003
Experience 28, Iter 2, disc loss: 0.0002613923902584092, policy loss: 10.220897101765198
Experience 28, Iter 3, disc loss: 0.00020138741733485236, policy loss: 11.92153205083144
Experience 28, Iter 4, disc loss: 0.0002511308260903896, policy loss: 10.591709860306421
Experience 28, Iter 5, disc loss: 0.00025311480694393645, policy loss: 10.99958308983447
Experience 28, Iter 6, disc loss: 0.0002881310625307452, policy loss: 10.297176814533318
Experience 28, Iter 7, disc loss: 0.00023224736769489972, policy loss: 10.890372623007272
Experience 28, Iter 8, disc loss: 0.00029221868739504747, policy loss: 10.216251677217016
Experience 28, Iter 9, disc loss: 0.0002708351416401206, policy loss: 10.48557885469376
Experience 28, Iter 10, disc loss: 0.00029427102593728593, policy loss: 11.683313116097434
Experience 28, Iter 11, disc loss: 0.000310454009990311, policy loss: 10.245160406420357
Experience 28, Iter 12, disc loss: 0.00028642632066492246, policy loss: 11.049680691638217
Experience 28, Iter 13, disc loss: 0.0002701043743255321, policy loss: 10.987329198539507
Experience 28, Iter 14, disc loss: 0.0002468220238666889, policy loss: 12.054916182748167
Experience 28, Iter 15, disc loss: 0.0002623497329219111, policy loss: 11.853766459847241
Experience 28, Iter 16, disc loss: 0.00022534471112145522, policy loss: 11.988033918536768
Experience 28, Iter 17, disc loss: 0.00026022019182939636, policy loss: 11.388622779013268
Experience 28, Iter 18, disc loss: 0.0003059066298874499, policy loss: 10.276722731421863
Experience 28, Iter 19, disc loss: 0.0002482944479013841, policy loss: 11.032961296878208
Experience 28, Iter 20, disc loss: 0.0002538687730704568, policy loss: 10.613600107157952
Experience 28, Iter 21, disc loss: 0.0002505699555590881, policy loss: 11.153372468066765
Experience 28, Iter 22, disc loss: 0.00022923139596162437, policy loss: 12.019954929899914
Experience 28, Iter 23, disc loss: 0.0003055005333694268, policy loss: 10.880464665546363
Experience 28, Iter 24, disc loss: 0.0002657846150659994, policy loss: 10.69304297800517
Experience 28, Iter 25, disc loss: 0.00026246705748646443, policy loss: 11.494063431987474
Experience 28, Iter 26, disc loss: 0.00028586000098777765, policy loss: 10.615111406809842
Experience 28, Iter 27, disc loss: 0.00021711302732375595, policy loss: 10.932630970022817
Experience 28, Iter 28, disc loss: 0.00027159145921061654, policy loss: 10.868061407353132
Experience 28, Iter 29, disc loss: 0.0002335578454681005, policy loss: 10.768829263467843
Experience 28, Iter 30, disc loss: 0.00025424505581393263, policy loss: 10.509906891896536
Experience 28, Iter 31, disc loss: 0.0002238326741359621, policy loss: 10.870028980496905
Experience 28, Iter 32, disc loss: 0.0002725282551319361, policy loss: 10.572357793691713
Experience 28, Iter 33, disc loss: 0.0002520690128324902, policy loss: 10.254474218319956
Experience 28, Iter 34, disc loss: 0.00025293837965227985, policy loss: 10.888466880817509
Experience 28, Iter 35, disc loss: 0.00030137488699846, policy loss: 10.42263006393949
Experience 28, Iter 36, disc loss: 0.0002599091266967435, policy loss: 11.08690367243871
Experience 28, Iter 37, disc loss: 0.00025154766182894687, policy loss: 12.606190060891592
Experience 28, Iter 38, disc loss: 0.0002910526442450529, policy loss: 10.341339982789352
Experience 28, Iter 39, disc loss: 0.0002205510460004878, policy loss: 11.361888667758492
Experience 28, Iter 40, disc loss: 0.00022941216084390695, policy loss: 11.670222967703669
Experience 28, Iter 41, disc loss: 0.00026218943624766963, policy loss: 11.017963976859864
Experience 28, Iter 42, disc loss: 0.0001970788143467644, policy loss: 11.352097047562953
Experience 28, Iter 43, disc loss: 0.00024383767206343396, policy loss: 10.635393414995576
Experience 28, Iter 44, disc loss: 0.00022841047728101128, policy loss: 10.996722280648681
Experience 28, Iter 45, disc loss: 0.00022760689351718956, policy loss: 11.41954216894191
Experience 28, Iter 46, disc loss: 0.00022751533205023558, policy loss: 12.200448442217468
Experience 28, Iter 47, disc loss: 0.00022896957329017752, policy loss: 11.682126566390775
Experience 28, Iter 48, disc loss: 0.0002330004552603191, policy loss: 11.269897852558433
Experience 28, Iter 49, disc loss: 0.00021556216341549913, policy loss: 12.06272789249025
Experience 28, Iter 50, disc loss: 0.0002082111692034409, policy loss: 11.19551243110593
Experience 28, Iter 51, disc loss: 0.0002668692376018449, policy loss: 10.631682736754732
Experience 28, Iter 52, disc loss: 0.00023379486722899687, policy loss: 11.10266158175854
Experience 28, Iter 53, disc loss: 0.00024335395087724096, policy loss: 10.224263012489175
Experience 28, Iter 54, disc loss: 0.00023927784112699022, policy loss: 10.846764213212055
Experience 28, Iter 55, disc loss: 0.00023275459840281406, policy loss: 10.372022692362474
Experience 28, Iter 56, disc loss: 0.00021273584609105077, policy loss: 10.84213443408282
Experience 28, Iter 57, disc loss: 0.0002057269042189302, policy loss: 10.706178641584472
Experience 28, Iter 58, disc loss: 0.0002754044745401798, policy loss: 11.153923079599018
Experience 28, Iter 59, disc loss: 0.00022116135233420434, policy loss: 10.5885766498907
Experience 28, Iter 60, disc loss: 0.0002094125822797849, policy loss: 11.533085514762663
Experience 28, Iter 61, disc loss: 0.00025804110885686415, policy loss: 11.009458902009577
Experience 28, Iter 62, disc loss: 0.00018433202928075701, policy loss: 11.376687337689564
Experience 28, Iter 63, disc loss: 0.00027804614707876963, policy loss: 10.522357850261406
Experience 28, Iter 64, disc loss: 0.00024666738871543354, policy loss: 10.410393273961432
Experience 28, Iter 65, disc loss: 0.0002381680912726615, policy loss: 10.955799162355264
Experience 28, Iter 66, disc loss: 0.00026739068841320676, policy loss: 11.46367211304186
Experience 28, Iter 67, disc loss: 0.00022539714258136898, policy loss: 10.619816216747918
Experience 28, Iter 68, disc loss: 0.0002387585168764372, policy loss: 11.871888448772118
Experience 28, Iter 69, disc loss: 0.00023397327840116974, policy loss: 11.51255914447549
Experience 28, Iter 70, disc loss: 0.0002410435731395736, policy loss: 10.608544012092844
Experience 28, Iter 71, disc loss: 0.00029721362820739653, policy loss: 10.274159373691452
Experience 28, Iter 72, disc loss: 0.00021214773453051848, policy loss: 11.426841316108863
Experience 28, Iter 73, disc loss: 0.0002609130766357516, policy loss: 11.314788108982917
Experience 28, Iter 74, disc loss: 0.00026773972737659506, policy loss: 10.513279743218
Experience 28, Iter 75, disc loss: 0.00023402785255544794, policy loss: 11.482814622573192
Experience 28, Iter 76, disc loss: 0.000271299057608408, policy loss: 11.071474721773678
Experience 28, Iter 77, disc loss: 0.00029548899431776013, policy loss: 11.148849680608166
Experience 28, Iter 78, disc loss: 0.0002322855987788905, policy loss: 12.076268792088563
Experience 28, Iter 79, disc loss: 0.0002568885086526279, policy loss: 11.112314472567263
Experience 28, Iter 80, disc loss: 0.00027040185933273103, policy loss: 10.455294006587486
Experience 28, Iter 81, disc loss: 0.0002661362127139347, policy loss: 10.653500687080282
Experience 28, Iter 82, disc loss: 0.00031439664701345664, policy loss: 10.009154280277652
Experience 28, Iter 83, disc loss: 0.0002785719313903494, policy loss: 11.080417777584778
Experience 28, Iter 84, disc loss: 0.00024628772479316904, policy loss: 12.347229762818195
Experience 28, Iter 85, disc loss: 0.00021433240555256044, policy loss: 11.195091062191082
Experience 28, Iter 86, disc loss: 0.0002412863774461252, policy loss: 10.955670473652239
Experience 28, Iter 87, disc loss: 0.0002741546864320989, policy loss: 10.646728079834645
Experience 28, Iter 88, disc loss: 0.000238368694408941, policy loss: 11.152967334156157
Experience 28, Iter 89, disc loss: 0.00022539982736153544, policy loss: 11.41447701102314
Experience 28, Iter 90, disc loss: 0.0002816856975579859, policy loss: 10.312865006636484
Experience 28, Iter 91, disc loss: 0.0002994664684349065, policy loss: 10.883341138069184
Experience 28, Iter 92, disc loss: 0.00027297280196448453, policy loss: 10.537619410200005
Experience 28, Iter 93, disc loss: 0.00022180599738848538, policy loss: 10.835872230231839
Experience 28, Iter 94, disc loss: 0.00024377235426423344, policy loss: 11.873614222694094
Experience 28, Iter 95, disc loss: 0.00022406057514238046, policy loss: 12.358290969845957
Experience 28, Iter 96, disc loss: 0.0002927975036101582, policy loss: 10.853718169999333
Experience 28, Iter 97, disc loss: 0.000261998117713141, policy loss: 11.178742453197769
Experience 28, Iter 98, disc loss: 0.0002602597808089233, policy loss: 10.73122468575379
Experience 28, Iter 99, disc loss: 0.00021117449560901976, policy loss: 11.769461175082865
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1868],
        [1.9136],
        [0.0435]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0103, 0.1691, 2.0354, 0.0336, 0.0286, 4.1285]],

        [[0.0103, 0.1691, 2.0354, 0.0336, 0.0286, 4.1285]],

        [[0.0103, 0.1691, 2.0354, 0.0336, 0.0286, 4.1285]],

        [[0.0103, 0.1691, 2.0354, 0.0336, 0.0286, 4.1285]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.7472, 7.6546, 0.1739], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.7472, 7.6546, 0.1739])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.488
Iter 2/2000 - Loss: 3.551
Iter 3/2000 - Loss: 3.244
Iter 4/2000 - Loss: 3.198
Iter 5/2000 - Loss: 3.174
Iter 6/2000 - Loss: 3.013
Iter 7/2000 - Loss: 2.800
Iter 8/2000 - Loss: 2.614
Iter 9/2000 - Loss: 2.462
Iter 10/2000 - Loss: 2.305
Iter 11/2000 - Loss: 2.106
Iter 12/2000 - Loss: 1.866
Iter 13/2000 - Loss: 1.608
Iter 14/2000 - Loss: 1.351
Iter 15/2000 - Loss: 1.096
Iter 16/2000 - Loss: 0.834
Iter 17/2000 - Loss: 0.556
Iter 18/2000 - Loss: 0.261
Iter 19/2000 - Loss: -0.044
Iter 20/2000 - Loss: -0.350
Iter 1981/2000 - Loss: -7.929
Iter 1982/2000 - Loss: -7.930
Iter 1983/2000 - Loss: -7.930
Iter 1984/2000 - Loss: -7.930
Iter 1985/2000 - Loss: -7.930
Iter 1986/2000 - Loss: -7.930
Iter 1987/2000 - Loss: -7.930
Iter 1988/2000 - Loss: -7.930
Iter 1989/2000 - Loss: -7.930
Iter 1990/2000 - Loss: -7.930
Iter 1991/2000 - Loss: -7.930
Iter 1992/2000 - Loss: -7.930
Iter 1993/2000 - Loss: -7.930
Iter 1994/2000 - Loss: -7.930
Iter 1995/2000 - Loss: -7.930
Iter 1996/2000 - Loss: -7.930
Iter 1997/2000 - Loss: -7.930
Iter 1998/2000 - Loss: -7.930
Iter 1999/2000 - Loss: -7.930
Iter 2000/2000 - Loss: -7.930
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.3839,  4.9500, 29.1775,  3.2207,  6.7433, 47.6022]],

        [[14.5065, 30.1320,  7.2409,  1.2727,  1.8211, 23.4701]],

        [[16.4100, 30.3228,  8.3994,  0.9317,  0.8987, 22.4686]],

        [[11.1817, 25.0725, 14.0800,  2.4262,  1.8504, 44.2146]]])
Signal Variance: tensor([ 0.0496,  1.6548, 13.7703,  0.5432])
Estimated target variance: tensor([0.0159, 0.7472, 7.6546, 0.1739])
N: 290
Signal to noise ratio: tensor([12.4621, 69.5307, 78.4064, 42.3318])
Bound on condition number: tensor([  45039.1389, 1402011.0416, 1782792.1749,  519676.6965])
Policy Optimizer learning rate:
9.70929960709856e-05
Experience 29, Iter 0, disc loss: 0.00023273301360007912, policy loss: 10.913907875115417
Experience 29, Iter 1, disc loss: 0.0002492275307558387, policy loss: 10.428621041997276
Experience 29, Iter 2, disc loss: 0.00026221492562741215, policy loss: 11.049560261875044
Experience 29, Iter 3, disc loss: 0.0002522276412525163, policy loss: 10.147280377797223
Experience 29, Iter 4, disc loss: 0.00027432759644838004, policy loss: 10.591908282393197
Experience 29, Iter 5, disc loss: 0.00025836188288102195, policy loss: 10.801343282820977
Experience 29, Iter 6, disc loss: 0.00022537578850159333, policy loss: 11.68440784129114
Experience 29, Iter 7, disc loss: 0.0002590232369271102, policy loss: 10.483172058943904
Experience 29, Iter 8, disc loss: 0.0002121902944676076, policy loss: 11.195506423322241
Experience 29, Iter 9, disc loss: 0.0002133035358400036, policy loss: 10.793620641301837
Experience 29, Iter 10, disc loss: 0.00024291204675811205, policy loss: 11.561469151159292
Experience 29, Iter 11, disc loss: 0.00022935742688436625, policy loss: 10.680360887492089
Experience 29, Iter 12, disc loss: 0.00025013237218852915, policy loss: 10.969471302426928
Experience 29, Iter 13, disc loss: 0.000219894965170039, policy loss: 10.307538776079996
Experience 29, Iter 14, disc loss: 0.00022261167040273907, policy loss: 11.03805891477737
Experience 29, Iter 15, disc loss: 0.00021128045501913454, policy loss: 10.828451522344048
Experience 29, Iter 16, disc loss: 0.0002588456923541719, policy loss: 10.837637765165095
Experience 29, Iter 17, disc loss: 0.00021087794798129667, policy loss: 11.935110006526504
Experience 29, Iter 18, disc loss: 0.0002448621093723924, policy loss: 10.541228700681556
Experience 29, Iter 19, disc loss: 0.0001985596976828991, policy loss: 11.741767946768324
Experience 29, Iter 20, disc loss: 0.0002292103890377644, policy loss: 10.54809689722508
Experience 29, Iter 21, disc loss: 0.0001746447261648836, policy loss: 12.823740155511167
Experience 29, Iter 22, disc loss: 0.00021905581049987767, policy loss: 10.436691301783622
Experience 29, Iter 23, disc loss: 0.00019962396364127199, policy loss: 11.71620158178708
Experience 29, Iter 24, disc loss: 0.000200094898038194, policy loss: 11.47539677169447
Experience 29, Iter 25, disc loss: 0.00022106585340680646, policy loss: 10.729004200438245
Experience 29, Iter 26, disc loss: 0.00020621124159280688, policy loss: 10.955678493065104
Experience 29, Iter 27, disc loss: 0.0002007059311613259, policy loss: 11.469226402467788
Experience 29, Iter 28, disc loss: 0.00021246573875644036, policy loss: 11.413865474126828
Experience 29, Iter 29, disc loss: 0.00022316978632011498, policy loss: 10.978811120699099
Experience 29, Iter 30, disc loss: 0.00019921119457089512, policy loss: 11.056452469956485
Experience 29, Iter 31, disc loss: 0.0002090190856254109, policy loss: 10.852431006767517
Experience 29, Iter 32, disc loss: 0.00021156425534780114, policy loss: 11.359624454448893
Experience 29, Iter 33, disc loss: 0.000252763367657312, policy loss: 10.934338285165621
Experience 29, Iter 34, disc loss: 0.0002523990324581985, policy loss: 10.699242399605286
Experience 29, Iter 35, disc loss: 0.00024925782232060724, policy loss: 9.963543554148098
Experience 29, Iter 36, disc loss: 0.00019181224778261035, policy loss: 12.661890630145665
Experience 29, Iter 37, disc loss: 0.00023371043278148972, policy loss: 10.934777680450853
Experience 29, Iter 38, disc loss: 0.00023941874102363848, policy loss: 10.40989030560442
Experience 29, Iter 39, disc loss: 0.0002558598808051389, policy loss: 10.922959642322807
Experience 29, Iter 40, disc loss: 0.00021904980524198087, policy loss: 11.008606553333099
Experience 29, Iter 41, disc loss: 0.00023563950324283583, policy loss: 10.641857247530542
Experience 29, Iter 42, disc loss: 0.0002564578607417356, policy loss: 9.861711366891113
Experience 29, Iter 43, disc loss: 0.00021820929669680915, policy loss: 11.240249973604787
Experience 29, Iter 44, disc loss: 0.00024480669859756464, policy loss: 11.72908703951044
Experience 29, Iter 45, disc loss: 0.00024688015020328183, policy loss: 10.53229824078632
Experience 29, Iter 46, disc loss: 0.00026784327759390633, policy loss: 11.167239320732985
Experience 29, Iter 47, disc loss: 0.00021526100241735, policy loss: 11.014031707568407
Experience 29, Iter 48, disc loss: 0.0002823786039690708, policy loss: 11.577696219059288
Experience 29, Iter 49, disc loss: 0.00018826395605732314, policy loss: 11.629547244545797
Experience 29, Iter 50, disc loss: 0.00023150476512402566, policy loss: 10.254232928208157
Experience 29, Iter 51, disc loss: 0.00023298810935205074, policy loss: 10.783927338780952
Experience 29, Iter 52, disc loss: 0.00021989190690929977, policy loss: 12.575333239812867
Experience 29, Iter 53, disc loss: 0.00024366391407819836, policy loss: 11.69380662777019
Experience 29, Iter 54, disc loss: 0.0002429699870254656, policy loss: 11.59730442929216
Experience 29, Iter 55, disc loss: 0.00019744582676164355, policy loss: 11.375257511610817
Experience 29, Iter 56, disc loss: 0.00020758984957039127, policy loss: 11.337785154351527
Experience 29, Iter 57, disc loss: 0.0002250577013166756, policy loss: 10.876204598837012
Experience 29, Iter 58, disc loss: 0.00025248221494287757, policy loss: 10.371704374030656
Experience 29, Iter 59, disc loss: 0.00026935878527891186, policy loss: 9.866747835158694
Experience 29, Iter 60, disc loss: 0.00029465812581365767, policy loss: 10.32817360717499
Experience 29, Iter 61, disc loss: 0.00020980113355323511, policy loss: 11.542481632675583
Experience 29, Iter 62, disc loss: 0.0002402178060850011, policy loss: 10.340125194905548
Experience 29, Iter 63, disc loss: 0.0002188415662143605, policy loss: 10.466807390526762
Experience 29, Iter 64, disc loss: 0.0002443835850475661, policy loss: 10.980234215349187
Experience 29, Iter 65, disc loss: 0.00024292663254307431, policy loss: 11.018272070300847
Experience 29, Iter 66, disc loss: 0.00025320937931051015, policy loss: 10.827082965138208
Experience 29, Iter 67, disc loss: 0.0001976135471940194, policy loss: 11.34234827609399
Experience 29, Iter 68, disc loss: 0.00022480076189523634, policy loss: 10.96727568428457
Experience 29, Iter 69, disc loss: 0.00025061304309891767, policy loss: 10.19222332260841
Experience 29, Iter 70, disc loss: 0.0002475950454376627, policy loss: 10.505379788078162
Experience 29, Iter 71, disc loss: 0.00024624742925469375, policy loss: 10.549134284845563
Experience 29, Iter 72, disc loss: 0.00022196890474328226, policy loss: 10.823561748618523
Experience 29, Iter 73, disc loss: 0.00025704663286413743, policy loss: 10.776673553179425
Experience 29, Iter 74, disc loss: 0.0002470875507972771, policy loss: 10.9543522652563
Experience 29, Iter 75, disc loss: 0.0002330567997203463, policy loss: 10.912161682473933
Experience 29, Iter 76, disc loss: 0.00025541397000512756, policy loss: 10.188570931326826
Experience 29, Iter 77, disc loss: 0.0002329224463361176, policy loss: 10.71875706863306
Experience 29, Iter 78, disc loss: 0.00022173925252947708, policy loss: 11.018808700338989
Experience 29, Iter 79, disc loss: 0.00021018193621935924, policy loss: 10.991796413340152
Experience 29, Iter 80, disc loss: 0.00019284830732277792, policy loss: 11.62907929865735
Experience 29, Iter 81, disc loss: 0.00022788269998495788, policy loss: 10.879649356422116
Experience 29, Iter 82, disc loss: 0.00021121201287069665, policy loss: 10.946680363683413
Experience 29, Iter 83, disc loss: 0.00023990077765764556, policy loss: 10.789439662808597
Experience 29, Iter 84, disc loss: 0.0002451871336588, policy loss: 10.974854478866874
Experience 29, Iter 85, disc loss: 0.0002591577078551492, policy loss: 10.653494667900489
Experience 29, Iter 86, disc loss: 0.00022916684608216895, policy loss: 11.278489549367844
Experience 29, Iter 87, disc loss: 0.00020318501960219746, policy loss: 11.904260927289847
Experience 29, Iter 88, disc loss: 0.0002393642499501556, policy loss: 11.498781310086573
Experience 29, Iter 89, disc loss: 0.000211622168241635, policy loss: 11.99790138158981
Experience 29, Iter 90, disc loss: 0.00022460384924763568, policy loss: 11.331277992449671
Experience 29, Iter 91, disc loss: 0.0002378578920445076, policy loss: 10.906428113085159
Experience 29, Iter 92, disc loss: 0.0001935993579769734, policy loss: 11.645354335689925
Experience 29, Iter 93, disc loss: 0.00020705145300332472, policy loss: 10.555594402322477
Experience 29, Iter 94, disc loss: 0.0002051448033001911, policy loss: 11.02103699034884
Experience 29, Iter 95, disc loss: 0.00021716707868976048, policy loss: 11.48231066062255
Experience 29, Iter 96, disc loss: 0.00024116050924882635, policy loss: 10.486049716232138
Experience 29, Iter 97, disc loss: 0.00022549603045366812, policy loss: 10.570762038385851
Experience 29, Iter 98, disc loss: 0.00020653744691033886, policy loss: 11.76411765714214
Experience 29, Iter 99, disc loss: 0.00024224956754480514, policy loss: 10.669165605706915
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1915],
        [1.9357],
        [0.0438]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0101, 0.1693, 2.0506, 0.0340, 0.0289, 4.2323]],

        [[0.0101, 0.1693, 2.0506, 0.0340, 0.0289, 4.2323]],

        [[0.0101, 0.1693, 2.0506, 0.0340, 0.0289, 4.2323]],

        [[0.0101, 0.1693, 2.0506, 0.0340, 0.0289, 4.2323]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.7660, 7.7428, 0.1752], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.7660, 7.7428, 0.1752])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.486
Iter 2/2000 - Loss: 3.542
Iter 3/2000 - Loss: 3.232
Iter 4/2000 - Loss: 3.180
Iter 5/2000 - Loss: 3.151
Iter 6/2000 - Loss: 2.985
Iter 7/2000 - Loss: 2.767
Iter 8/2000 - Loss: 2.576
Iter 9/2000 - Loss: 2.419
Iter 10/2000 - Loss: 2.255
Iter 11/2000 - Loss: 2.049
Iter 12/2000 - Loss: 1.803
Iter 13/2000 - Loss: 1.542
Iter 14/2000 - Loss: 1.282
Iter 15/2000 - Loss: 1.025
Iter 16/2000 - Loss: 0.761
Iter 17/2000 - Loss: 0.482
Iter 18/2000 - Loss: 0.186
Iter 19/2000 - Loss: -0.117
Iter 20/2000 - Loss: -0.421
Iter 1981/2000 - Loss: -7.965
Iter 1982/2000 - Loss: -7.965
Iter 1983/2000 - Loss: -7.965
Iter 1984/2000 - Loss: -7.965
Iter 1985/2000 - Loss: -7.965
Iter 1986/2000 - Loss: -7.965
Iter 1987/2000 - Loss: -7.966
Iter 1988/2000 - Loss: -7.966
Iter 1989/2000 - Loss: -7.966
Iter 1990/2000 - Loss: -7.966
Iter 1991/2000 - Loss: -7.966
Iter 1992/2000 - Loss: -7.966
Iter 1993/2000 - Loss: -7.966
Iter 1994/2000 - Loss: -7.966
Iter 1995/2000 - Loss: -7.966
Iter 1996/2000 - Loss: -7.966
Iter 1997/2000 - Loss: -7.966
Iter 1998/2000 - Loss: -7.966
Iter 1999/2000 - Loss: -7.966
Iter 2000/2000 - Loss: -7.966
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[ 9.3805,  4.9152, 29.9538,  2.9695,  6.3280, 46.7789]],

        [[14.2579, 30.4325,  7.1520,  1.3141,  1.8436, 24.7591]],

        [[16.4496, 29.9538,  8.4070,  0.9192,  0.8922, 22.5648]],

        [[11.1686, 25.1045, 13.9411,  2.4251,  1.8684, 44.3716]]])
Signal Variance: tensor([ 0.0490,  1.7476, 13.4961,  0.5410])
Estimated target variance: tensor([0.0159, 0.7660, 7.7428, 0.1752])
N: 300
Signal to noise ratio: tensor([12.4286, 71.1065, 77.9802, 41.7895])
Bound on condition number: tensor([  46342.2100, 1516841.6527, 1824276.4321,  523910.0889])
Policy Optimizer learning rate:
9.699075226141834e-05
Experience 30, Iter 0, disc loss: 0.00018971051603976387, policy loss: 11.387002298839201
Experience 30, Iter 1, disc loss: 0.00024201059031141398, policy loss: 11.210493682083348
Experience 30, Iter 2, disc loss: 0.0002638991689228689, policy loss: 10.839758553582179
Experience 30, Iter 3, disc loss: 0.00020139729812060402, policy loss: 10.725021836222423
Experience 30, Iter 4, disc loss: 0.00019137705279583417, policy loss: 11.142190803201121
Experience 30, Iter 5, disc loss: 0.0002339545044074634, policy loss: 11.498062614411662
Experience 30, Iter 6, disc loss: 0.00019482034029891191, policy loss: 10.948293224692422
Experience 30, Iter 7, disc loss: 0.00022498353953149234, policy loss: 10.216701198326192
Experience 30, Iter 8, disc loss: 0.00024535464517714314, policy loss: 10.665647694574352
Experience 30, Iter 9, disc loss: 0.0002358291746452697, policy loss: 10.17891044880266
Experience 30, Iter 10, disc loss: 0.00020359819280280281, policy loss: 11.107793998571198
Experience 30, Iter 11, disc loss: 0.00023978937784201467, policy loss: 10.63581340583906
Experience 30, Iter 12, disc loss: 0.0002066400933697309, policy loss: 11.094032766290232
Experience 30, Iter 13, disc loss: 0.00023095011591886262, policy loss: 11.233392016881368
Experience 30, Iter 14, disc loss: 0.0002337763314413584, policy loss: 10.667022283590061
Experience 30, Iter 15, disc loss: 0.00023031139541651298, policy loss: 10.945233648686992
Experience 30, Iter 16, disc loss: 0.00020339036204740317, policy loss: 11.025694022068011
Experience 30, Iter 17, disc loss: 0.00019985864749841808, policy loss: 11.248616454859347
Experience 30, Iter 18, disc loss: 0.00021902017452353159, policy loss: 10.826536717772047
Experience 30, Iter 19, disc loss: 0.00021084334580900947, policy loss: 10.647556038407519
Experience 30, Iter 20, disc loss: 0.0002102654818027403, policy loss: 10.823826944026294
Experience 30, Iter 21, disc loss: 0.00023187865775105453, policy loss: 10.141542752804941
Experience 30, Iter 22, disc loss: 0.00022596003644562835, policy loss: 10.943541059512576
Experience 30, Iter 23, disc loss: 0.0002482957380082225, policy loss: 10.867845625871526
Experience 30, Iter 24, disc loss: 0.00028432786821283053, policy loss: 10.149830180855703
Experience 30, Iter 25, disc loss: 0.00019206531099439224, policy loss: 11.145830081539799
Experience 30, Iter 26, disc loss: 0.0002250137250827306, policy loss: 10.971089142364741
Experience 30, Iter 27, disc loss: 0.00023940129194292657, policy loss: 10.463562131294431
Experience 30, Iter 28, disc loss: 0.0002224570291960219, policy loss: 11.002807236756013
Experience 30, Iter 29, disc loss: 0.00022322001006943841, policy loss: 10.558426732275345
Experience 30, Iter 30, disc loss: 0.00022117270887288737, policy loss: 10.370550351118705
Experience 30, Iter 31, disc loss: 0.00022372604176810302, policy loss: 11.373110231437463
Experience 30, Iter 32, disc loss: 0.00024457655011206683, policy loss: 10.368156230448253
Experience 30, Iter 33, disc loss: 0.0002391822617937155, policy loss: 10.48076872117348
Experience 30, Iter 34, disc loss: 0.00021703661700390295, policy loss: 10.55110536544973
Experience 30, Iter 35, disc loss: 0.00026880522010838323, policy loss: 9.985246377533343
Experience 30, Iter 36, disc loss: 0.00022465906321438326, policy loss: 11.553385032962913
Experience 30, Iter 37, disc loss: 0.00021615419273545827, policy loss: 11.727091107281133
Experience 30, Iter 38, disc loss: 0.00021471954308961399, policy loss: 10.63929738155964
Experience 30, Iter 39, disc loss: 0.00024115074890296664, policy loss: 10.858895582945237
Experience 30, Iter 40, disc loss: 0.00023716946544809404, policy loss: 10.94290483587065
Experience 30, Iter 41, disc loss: 0.00019412028625403034, policy loss: 11.95301466196683
Experience 30, Iter 42, disc loss: 0.00020371881744483183, policy loss: 10.998049054443463
Experience 30, Iter 43, disc loss: 0.00018418069008344646, policy loss: 11.532090354515958
Experience 30, Iter 44, disc loss: 0.00020602555296701737, policy loss: 10.443389568482758
Experience 30, Iter 45, disc loss: 0.00019933359143640045, policy loss: 11.403329017115318
Experience 30, Iter 46, disc loss: 0.00023770814181525777, policy loss: 10.367355143994654
Experience 30, Iter 47, disc loss: 0.00020561514291697594, policy loss: 11.148189097587224
Experience 30, Iter 48, disc loss: 0.00024056221918220588, policy loss: 10.979412346311719
Experience 30, Iter 49, disc loss: 0.0002552060651758182, policy loss: 10.67990811362386
Experience 30, Iter 50, disc loss: 0.00023913156420305148, policy loss: 11.115578332711433
Experience 30, Iter 51, disc loss: 0.00020258972488155602, policy loss: 11.280767181510463
Experience 30, Iter 52, disc loss: 0.00020077476822975517, policy loss: 11.294172412629841
Experience 30, Iter 53, disc loss: 0.00021336642669017027, policy loss: 10.70051691262129
Experience 30, Iter 54, disc loss: 0.00022329854716853376, policy loss: 11.333127968654935
Experience 30, Iter 55, disc loss: 0.00023357041633176193, policy loss: 10.224542193644929
Experience 30, Iter 56, disc loss: 0.00019904276784022586, policy loss: 10.791178879607056
Experience 30, Iter 57, disc loss: 0.0002312103991489166, policy loss: 10.536636266085885
Experience 30, Iter 58, disc loss: 0.00020327582900220542, policy loss: 10.57098271261312
Experience 30, Iter 59, disc loss: 0.0002518150456628528, policy loss: 10.375230902705926
Experience 30, Iter 60, disc loss: 0.00019010454406441873, policy loss: 11.142950587941918
Experience 30, Iter 61, disc loss: 0.0002102430303481164, policy loss: 11.139985731729205
Experience 30, Iter 62, disc loss: 0.00021431475026018195, policy loss: 10.244447773406337
Experience 30, Iter 63, disc loss: 0.00021972669493525458, policy loss: 11.512184397408621
Experience 30, Iter 64, disc loss: 0.0002230743578842625, policy loss: 10.120448480033932
Experience 30, Iter 65, disc loss: 0.00018674741981297802, policy loss: 11.184605250535519
Experience 30, Iter 66, disc loss: 0.0002172927845630475, policy loss: 11.044174077014059
Experience 30, Iter 67, disc loss: 0.00023915530745212936, policy loss: 11.929005904056243
Experience 30, Iter 68, disc loss: 0.00023530923372464457, policy loss: 10.344903988436412
Experience 30, Iter 69, disc loss: 0.0002141840080019598, policy loss: 10.732809452336607
Experience 30, Iter 70, disc loss: 0.00022664560134205903, policy loss: 10.949599278231783
Experience 30, Iter 71, disc loss: 0.00022804856663551705, policy loss: 10.61047295185956
Experience 30, Iter 72, disc loss: 0.00024347481515602583, policy loss: 10.704809865987352
Experience 30, Iter 73, disc loss: 0.00024510495718954714, policy loss: 11.723069943438109
Experience 30, Iter 74, disc loss: 0.00027822271738538304, policy loss: 10.283080974375814
Experience 30, Iter 75, disc loss: 0.0002623013714923659, policy loss: 10.65884157910336
Experience 30, Iter 76, disc loss: 0.00020148937803163058, policy loss: 11.61803286527992
Experience 30, Iter 77, disc loss: 0.00022106034272054736, policy loss: 11.578760970791398
Experience 30, Iter 78, disc loss: 0.0001794427644089257, policy loss: 12.768884694772371
Experience 30, Iter 79, disc loss: 0.00021741471100459253, policy loss: 10.41266988469541
Experience 30, Iter 80, disc loss: 0.00021909069656307026, policy loss: 11.39253653109419
Experience 30, Iter 81, disc loss: 0.00019772720701616089, policy loss: 12.020020413959225
Experience 30, Iter 82, disc loss: 0.00022516369495333794, policy loss: 10.51190666737036
Experience 30, Iter 83, disc loss: 0.00019844895422249922, policy loss: 10.803578648366829
Experience 30, Iter 84, disc loss: 0.00019745170169470453, policy loss: 11.897891576496598
Experience 30, Iter 85, disc loss: 0.00022079241351799096, policy loss: 10.746709013379679
Experience 30, Iter 86, disc loss: 0.00021804086907893888, policy loss: 11.157621248011093
Experience 30, Iter 87, disc loss: 0.00020399280630841756, policy loss: 11.287220495879238
Experience 30, Iter 88, disc loss: 0.0001901488692946641, policy loss: 11.375606922423144
Experience 30, Iter 89, disc loss: 0.00023200579586655817, policy loss: 11.703404046323003
Experience 30, Iter 90, disc loss: 0.000206719991409324, policy loss: 11.726147444110154
Experience 30, Iter 91, disc loss: 0.00019673018764361727, policy loss: 10.644566283312372
Experience 30, Iter 92, disc loss: 0.00020517232711703555, policy loss: 11.464274892245323
Experience 30, Iter 93, disc loss: 0.00021142916562225429, policy loss: 10.606386471455863
Experience 30, Iter 94, disc loss: 0.00020258956455682485, policy loss: 11.836916481465023
Experience 30, Iter 95, disc loss: 0.0002346940655521571, policy loss: 10.535225066957942
Experience 30, Iter 96, disc loss: 0.0001806674468039261, policy loss: 11.774441607422135
Experience 30, Iter 97, disc loss: 0.00020424661103714194, policy loss: 10.539277063195337
Experience 30, Iter 98, disc loss: 0.00019422393882939252, policy loss: 11.281795228277574
Experience 30, Iter 99, disc loss: 0.00021661997143976398, policy loss: 10.564997441052345
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.1969],
        [1.9646],
        [0.0443]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0099, 0.1704, 2.0777, 0.0344, 0.0290, 4.3336]],

        [[0.0099, 0.1704, 2.0777, 0.0344, 0.0290, 4.3336]],

        [[0.0099, 0.1704, 2.0777, 0.0344, 0.0290, 4.3336]],

        [[0.0099, 0.1704, 2.0777, 0.0344, 0.0290, 4.3336]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.7875, 7.8583, 0.1773], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.7875, 7.8583, 0.1773])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.507
Iter 2/2000 - Loss: 3.562
Iter 3/2000 - Loss: 3.246
Iter 4/2000 - Loss: 3.190
Iter 5/2000 - Loss: 3.158
Iter 6/2000 - Loss: 2.990
Iter 7/2000 - Loss: 2.767
Iter 8/2000 - Loss: 2.568
Iter 9/2000 - Loss: 2.404
Iter 10/2000 - Loss: 2.236
Iter 11/2000 - Loss: 2.030
Iter 12/2000 - Loss: 1.782
Iter 13/2000 - Loss: 1.517
Iter 14/2000 - Loss: 1.253
Iter 15/2000 - Loss: 0.992
Iter 16/2000 - Loss: 0.727
Iter 17/2000 - Loss: 0.448
Iter 18/2000 - Loss: 0.153
Iter 19/2000 - Loss: -0.151
Iter 20/2000 - Loss: -0.457
Iter 1981/2000 - Loss: -7.973
Iter 1982/2000 - Loss: -7.973
Iter 1983/2000 - Loss: -7.973
Iter 1984/2000 - Loss: -7.973
Iter 1985/2000 - Loss: -7.973
Iter 1986/2000 - Loss: -7.973
Iter 1987/2000 - Loss: -7.973
Iter 1988/2000 - Loss: -7.973
Iter 1989/2000 - Loss: -7.973
Iter 1990/2000 - Loss: -7.973
Iter 1991/2000 - Loss: -7.973
Iter 1992/2000 - Loss: -7.973
Iter 1993/2000 - Loss: -7.973
Iter 1994/2000 - Loss: -7.973
Iter 1995/2000 - Loss: -7.973
Iter 1996/2000 - Loss: -7.973
Iter 1997/2000 - Loss: -7.973
Iter 1998/2000 - Loss: -7.973
Iter 1999/2000 - Loss: -7.973
Iter 2000/2000 - Loss: -7.973
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.3583,  4.5841, 29.3898,  3.1792,  6.8186, 45.4838]],

        [[14.5710, 30.0900,  7.2313,  1.3007,  1.8082, 24.8075]],

        [[16.1481, 30.1276,  8.5107,  0.9050,  0.9001, 23.4978]],

        [[11.3110, 24.0216, 14.5014,  2.4002,  1.9328, 43.5630]]])
Signal Variance: tensor([ 0.0476,  1.7327, 14.1528,  0.5634])
Estimated target variance: tensor([0.0159, 0.7875, 7.8583, 0.1773])
N: 310
Signal to noise ratio: tensor([12.2406, 69.2498, 79.0127, 42.3393])
Bound on condition number: tensor([  46449.3794, 1486616.0886, 1935333.0226,  555712.7250])
Policy Optimizer learning rate:
9.688861611972639e-05
Experience 31, Iter 0, disc loss: 0.00023498669399330528, policy loss: 10.643098275120892
Experience 31, Iter 1, disc loss: 0.00021605806917968293, policy loss: 10.901316821280048
Experience 31, Iter 2, disc loss: 0.0001885944508144553, policy loss: 11.461282573552083
Experience 31, Iter 3, disc loss: 0.00017460969700293605, policy loss: 11.35300166675608
Experience 31, Iter 4, disc loss: 0.00019556950434849644, policy loss: 10.764244554020223
Experience 31, Iter 5, disc loss: 0.00018597391306044575, policy loss: 11.568927072769187
Experience 31, Iter 6, disc loss: 0.0001932157933729009, policy loss: 11.354766713497233
Experience 31, Iter 7, disc loss: 0.00019632843863182217, policy loss: 10.669405369872642
Experience 31, Iter 8, disc loss: 0.0002307486460489553, policy loss: 10.662196193627548
Experience 31, Iter 9, disc loss: 0.00022198428528997306, policy loss: 10.817291163684251
Experience 31, Iter 10, disc loss: 0.00022088358732294013, policy loss: 11.553300431376762
Experience 31, Iter 11, disc loss: 0.00021785140883618019, policy loss: 10.881334684286848
Experience 31, Iter 12, disc loss: 0.0001915367428463563, policy loss: 11.586742277777606
Experience 31, Iter 13, disc loss: 0.0002444674403675185, policy loss: 9.883710055073932
Experience 31, Iter 14, disc loss: 0.00019720558938276025, policy loss: 11.379639768975307
Experience 31, Iter 15, disc loss: 0.0002352883440208321, policy loss: 10.019682117365345
Experience 31, Iter 16, disc loss: 0.000202494133041823, policy loss: 11.525081546624527
Experience 31, Iter 17, disc loss: 0.0001961051058512829, policy loss: 10.934389782653279
Experience 31, Iter 18, disc loss: 0.00020073908933836816, policy loss: 10.455960440915579
Experience 31, Iter 19, disc loss: 0.00019411417215944269, policy loss: 11.174111242563658
Experience 31, Iter 20, disc loss: 0.00021956862912614707, policy loss: 10.666383923317236
Experience 31, Iter 21, disc loss: 0.00021769587824126943, policy loss: 11.10587699973085
Experience 31, Iter 22, disc loss: 0.00019150687871594114, policy loss: 11.51777675276578
Experience 31, Iter 23, disc loss: 0.00021586769179603215, policy loss: 10.61833765293024
Experience 31, Iter 24, disc loss: 0.00024449023890380655, policy loss: 10.577054039432058
Experience 31, Iter 25, disc loss: 0.00019824774077856224, policy loss: 10.595042092944132
Experience 31, Iter 26, disc loss: 0.0002219229856154778, policy loss: 10.356958259773421
Experience 31, Iter 27, disc loss: 0.00023697079272691498, policy loss: 10.710019026585766
Experience 31, Iter 28, disc loss: 0.0002230624003712316, policy loss: 10.318860472616052
Experience 31, Iter 29, disc loss: 0.00024088758220406183, policy loss: 10.691615715656573
Experience 31, Iter 30, disc loss: 0.0002367607264760793, policy loss: 10.472523124173097
Experience 31, Iter 31, disc loss: 0.0002788248753596075, policy loss: 10.802200616678318
Experience 31, Iter 32, disc loss: 0.00020143280458256456, policy loss: 11.327202427055713
Experience 31, Iter 33, disc loss: 0.00022151393266701367, policy loss: 10.696846540472064
Experience 31, Iter 34, disc loss: 0.00021561327402136866, policy loss: 11.055271804296494
Experience 31, Iter 35, disc loss: 0.00023521368168522862, policy loss: 9.987020647140497
Experience 31, Iter 36, disc loss: 0.00019134494114852893, policy loss: 11.713581971236321
Experience 31, Iter 37, disc loss: 0.0001980005944023657, policy loss: 10.64786774679159
Experience 31, Iter 38, disc loss: 0.0002083423038850755, policy loss: 11.377313425625559
Experience 31, Iter 39, disc loss: 0.00022083738208210607, policy loss: 11.13592033321394
Experience 31, Iter 40, disc loss: 0.00020611914548760048, policy loss: 11.220048974330627
Experience 31, Iter 41, disc loss: 0.0002060915317346667, policy loss: 11.427726277052283
Experience 31, Iter 42, disc loss: 0.00020252570300522378, policy loss: 11.306417797356833
Experience 31, Iter 43, disc loss: 0.00021204411944426786, policy loss: 10.433955254360123
Experience 31, Iter 44, disc loss: 0.00017832706991997678, policy loss: 11.673457117588692
Experience 31, Iter 45, disc loss: 0.00019380411463516768, policy loss: 10.792974894262333
Experience 31, Iter 46, disc loss: 0.00019721180982311767, policy loss: 10.392533965031166
Experience 31, Iter 47, disc loss: 0.00019680925606284933, policy loss: 11.450177180424959
Experience 31, Iter 48, disc loss: 0.00021999334028161043, policy loss: 10.170643420173823
Experience 31, Iter 49, disc loss: 0.0002054081411512025, policy loss: 11.455651497637062
Experience 31, Iter 50, disc loss: 0.0002351805076485252, policy loss: 10.07827225459032
Experience 31, Iter 51, disc loss: 0.00019329688710650227, policy loss: 10.609478028899312
Experience 31, Iter 52, disc loss: 0.00020643621495449247, policy loss: 10.940730426685477
Experience 31, Iter 53, disc loss: 0.00022031799308595915, policy loss: 10.944251517089572
Experience 31, Iter 54, disc loss: 0.00018876902310744424, policy loss: 11.105147122164649
Experience 31, Iter 55, disc loss: 0.000188618340954916, policy loss: 11.051881078763603
Experience 31, Iter 56, disc loss: 0.00019404162165423462, policy loss: 11.545945832129686
Experience 31, Iter 57, disc loss: 0.0001823426827992354, policy loss: 11.159098160029146
Experience 31, Iter 58, disc loss: 0.00021240456593304857, policy loss: 10.648321261348478
Experience 31, Iter 59, disc loss: 0.0002043795562444333, policy loss: 10.925754917796366
Experience 31, Iter 60, disc loss: 0.000177741236917857, policy loss: 11.985978481457114
Experience 31, Iter 61, disc loss: 0.00019003341869598222, policy loss: 11.281009303502046
Experience 31, Iter 62, disc loss: 0.00021293242777080135, policy loss: 10.182246857479289
Experience 31, Iter 63, disc loss: 0.0002024395896991038, policy loss: 10.315494685082264
Experience 31, Iter 64, disc loss: 0.00018275586296511868, policy loss: 11.453152577791865
Experience 31, Iter 65, disc loss: 0.00021452353657289567, policy loss: 10.395237085526198
Experience 31, Iter 66, disc loss: 0.00020910982012037078, policy loss: 10.710760092342415
Experience 31, Iter 67, disc loss: 0.00021138771972014622, policy loss: 11.195465201932869
Experience 31, Iter 68, disc loss: 0.0001984219803076443, policy loss: 10.610247996549994
Experience 31, Iter 69, disc loss: 0.00022244726915047282, policy loss: 10.980557732167409
Experience 31, Iter 70, disc loss: 0.00021649294844115362, policy loss: 10.375540909584139
Experience 31, Iter 71, disc loss: 0.00019030661433176043, policy loss: 11.104094398638637
Experience 31, Iter 72, disc loss: 0.00022969292121143435, policy loss: 10.566492034633455
Experience 31, Iter 73, disc loss: 0.00021350600623920698, policy loss: 11.255658349505143
Experience 31, Iter 74, disc loss: 0.00019124457727618034, policy loss: 11.521954671095962
Experience 31, Iter 75, disc loss: 0.00021996498852992083, policy loss: 10.814160256979731
Experience 31, Iter 76, disc loss: 0.00019883890108012985, policy loss: 11.105331915574247
Experience 31, Iter 77, disc loss: 0.0001830071378887998, policy loss: 10.89311047048476
Experience 31, Iter 78, disc loss: 0.0002181962949175113, policy loss: 10.13443236128425
Experience 31, Iter 79, disc loss: 0.00019252218598317337, policy loss: 11.830571799040163
Experience 31, Iter 80, disc loss: 0.0002019965153365971, policy loss: 11.900976659215095
Experience 31, Iter 81, disc loss: 0.00020577346564194224, policy loss: 10.426241400500318
Experience 31, Iter 82, disc loss: 0.0001981748663308046, policy loss: 10.432514899077045
Experience 31, Iter 83, disc loss: 0.0002033685995310632, policy loss: 11.738531932606893
Experience 31, Iter 84, disc loss: 0.00022091163464499716, policy loss: 11.048622864293376
Experience 31, Iter 85, disc loss: 0.00021705531817573805, policy loss: 10.463865036921472
Experience 31, Iter 86, disc loss: 0.00017704361665990488, policy loss: 11.45787132468099
Experience 31, Iter 87, disc loss: 0.00022859931242498016, policy loss: 10.261639145649408
Experience 31, Iter 88, disc loss: 0.0002064937096631463, policy loss: 10.43325574496114
Experience 31, Iter 89, disc loss: 0.00018728431198033308, policy loss: 10.447021072561121
Experience 31, Iter 90, disc loss: 0.00019443276964002334, policy loss: 10.68415764289205
Experience 31, Iter 91, disc loss: 0.0001784040005626818, policy loss: 11.330558309965683
Experience 31, Iter 92, disc loss: 0.00020269384053486593, policy loss: 10.612705921067768
Experience 31, Iter 93, disc loss: 0.00017543988675524134, policy loss: 10.92382617859337
Experience 31, Iter 94, disc loss: 0.0001935753482725872, policy loss: 10.970028033390776
Experience 31, Iter 95, disc loss: 0.00022643376117858688, policy loss: 10.382170037766787
Experience 31, Iter 96, disc loss: 0.0001977727079159893, policy loss: 10.978418391655737
Experience 31, Iter 97, disc loss: 0.00019734034102852213, policy loss: 10.921750624828169
Experience 31, Iter 98, disc loss: 0.0002185226810963424, policy loss: 10.994061578199048
Experience 31, Iter 99, disc loss: 0.0002303002892086362, policy loss: 10.287522303620065
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2015],
        [2.0037],
        [0.0449]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0097, 0.1717, 2.1058, 0.0347, 0.0291, 4.4132]],

        [[0.0097, 0.1717, 2.1058, 0.0347, 0.0291, 4.4132]],

        [[0.0097, 0.1717, 2.1058, 0.0347, 0.0291, 4.4132]],

        [[0.0097, 0.1717, 2.1058, 0.0347, 0.0291, 4.4132]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0160, 0.8059, 8.0150, 0.1795], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0160, 0.8059, 8.0150, 0.1795])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.545
Iter 2/2000 - Loss: 3.603
Iter 3/2000 - Loss: 3.288
Iter 4/2000 - Loss: 3.232
Iter 5/2000 - Loss: 3.201
Iter 6/2000 - Loss: 3.034
Iter 7/2000 - Loss: 2.811
Iter 8/2000 - Loss: 2.608
Iter 9/2000 - Loss: 2.440
Iter 10/2000 - Loss: 2.269
Iter 11/2000 - Loss: 2.059
Iter 12/2000 - Loss: 1.808
Iter 13/2000 - Loss: 1.538
Iter 14/2000 - Loss: 1.268
Iter 15/2000 - Loss: 1.003
Iter 16/2000 - Loss: 0.733
Iter 17/2000 - Loss: 0.450
Iter 18/2000 - Loss: 0.152
Iter 19/2000 - Loss: -0.154
Iter 20/2000 - Loss: -0.462
Iter 1981/2000 - Loss: -8.009
Iter 1982/2000 - Loss: -8.009
Iter 1983/2000 - Loss: -8.009
Iter 1984/2000 - Loss: -8.009
Iter 1985/2000 - Loss: -8.009
Iter 1986/2000 - Loss: -8.009
Iter 1987/2000 - Loss: -8.009
Iter 1988/2000 - Loss: -8.009
Iter 1989/2000 - Loss: -8.010
Iter 1990/2000 - Loss: -8.010
Iter 1991/2000 - Loss: -8.010
Iter 1992/2000 - Loss: -8.010
Iter 1993/2000 - Loss: -8.010
Iter 1994/2000 - Loss: -8.010
Iter 1995/2000 - Loss: -8.010
Iter 1996/2000 - Loss: -8.010
Iter 1997/2000 - Loss: -8.010
Iter 1998/2000 - Loss: -8.010
Iter 1999/2000 - Loss: -8.010
Iter 2000/2000 - Loss: -8.010
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.4561,  4.6465, 28.0475,  2.9750,  6.8321, 45.2526]],

        [[14.8885, 29.9318,  7.1126,  1.3518,  1.8489, 25.3066]],

        [[16.4261, 30.5185,  8.4183,  0.9290,  0.9159, 24.0276]],

        [[11.0215, 24.2370, 14.6587,  2.3873,  1.9957, 41.8764]]])
Signal Variance: tensor([ 0.0459,  1.8091, 14.6490,  0.5693])
Estimated target variance: tensor([0.0160, 0.8059, 8.0150, 0.1795])
N: 320
Signal to noise ratio: tensor([12.0360, 71.3707, 80.3302, 42.4949])
Bound on condition number: tensor([  46357.5459, 1630009.2618, 2064941.4475,  577861.9029])
Policy Optimizer learning rate:
9.678658753253006e-05
Experience 32, Iter 0, disc loss: 0.00019953554708396432, policy loss: 10.623730599011237
Experience 32, Iter 1, disc loss: 0.00019511325807312902, policy loss: 11.897838032227392
Experience 32, Iter 2, disc loss: 0.000198842183681387, policy loss: 10.785641388950186
Experience 32, Iter 3, disc loss: 0.00020051797334490608, policy loss: 10.715086159854321
Experience 32, Iter 4, disc loss: 0.00020252886213548114, policy loss: 10.416390813992692
Experience 32, Iter 5, disc loss: 0.00019659944321719444, policy loss: 11.684004802715979
Experience 32, Iter 6, disc loss: 0.00019721823624607582, policy loss: 11.673222453886527
Experience 32, Iter 7, disc loss: 0.00017972943740232774, policy loss: 11.49887165773997
Experience 32, Iter 8, disc loss: 0.0002142674209626864, policy loss: 10.663167946993159
Experience 32, Iter 9, disc loss: 0.00019460258981025358, policy loss: 11.284987850921972
Experience 32, Iter 10, disc loss: 0.00022213875264226671, policy loss: 10.949528173466799
Experience 32, Iter 11, disc loss: 0.00019109841741488196, policy loss: 11.23828138337936
Experience 32, Iter 12, disc loss: 0.00023713715408434864, policy loss: 10.673580819303782
Experience 32, Iter 13, disc loss: 0.0001922374183560114, policy loss: 11.066453762269266
Experience 32, Iter 14, disc loss: 0.00020817339809309432, policy loss: 10.480439564865378
Experience 32, Iter 15, disc loss: 0.00020909777773841555, policy loss: 12.34783649626634
Experience 32, Iter 16, disc loss: 0.00021675423679004823, policy loss: 10.942682832323927
Experience 32, Iter 17, disc loss: 0.00020650583467658713, policy loss: 11.49043058901688
Experience 32, Iter 18, disc loss: 0.00021092207366996515, policy loss: 11.23245899353087
Experience 32, Iter 19, disc loss: 0.00023951374729439768, policy loss: 11.360775978649489
Experience 32, Iter 20, disc loss: 0.0002506501619772961, policy loss: 10.65110946033801
Experience 32, Iter 21, disc loss: 0.00021165881826736726, policy loss: 10.790008589624264
Experience 32, Iter 22, disc loss: 0.0002006817355817139, policy loss: 11.156105757477562
Experience 32, Iter 23, disc loss: 0.0002000635311438939, policy loss: 11.177538416842038
Experience 32, Iter 24, disc loss: 0.0002034628087714744, policy loss: 10.17973012965496
Experience 32, Iter 25, disc loss: 0.00019264249084804543, policy loss: 10.765538675556039
Experience 32, Iter 26, disc loss: 0.00020825020416993654, policy loss: 10.163698598633355
Experience 32, Iter 27, disc loss: 0.0002087474568363373, policy loss: 11.037848509490932
Experience 32, Iter 28, disc loss: 0.00019367308352457436, policy loss: 11.17111216846062
Experience 32, Iter 29, disc loss: 0.000207486075245805, policy loss: 11.096066259423438
Experience 32, Iter 30, disc loss: 0.0002209147018450426, policy loss: 10.853144186304695
Experience 32, Iter 31, disc loss: 0.00020429554382900804, policy loss: 10.928470559761859
Experience 32, Iter 32, disc loss: 0.00020981138940038906, policy loss: 11.488545438048746
Experience 32, Iter 33, disc loss: 0.0002178780212954622, policy loss: 10.848389735700927
Experience 32, Iter 34, disc loss: 0.0002222614336838897, policy loss: 10.559752051794314
Experience 32, Iter 35, disc loss: 0.00021126184389777487, policy loss: 10.317972599332078
Experience 32, Iter 36, disc loss: 0.00021313483630402117, policy loss: 11.851857215098153
Experience 32, Iter 37, disc loss: 0.00020095981675712709, policy loss: 12.311185529391295
Experience 32, Iter 38, disc loss: 0.0002162287885903495, policy loss: 11.014958936211835
Experience 32, Iter 39, disc loss: 0.00022321180337638342, policy loss: 10.684725318477078
Experience 32, Iter 40, disc loss: 0.00019853322189649, policy loss: 10.772994727567418
Experience 32, Iter 41, disc loss: 0.00020969480102630238, policy loss: 10.697349445889337
Experience 32, Iter 42, disc loss: 0.00020449505587160225, policy loss: 10.829072820369628
Experience 32, Iter 43, disc loss: 0.00021543053162478165, policy loss: 10.955135474083008
Experience 32, Iter 44, disc loss: 0.00020405605128706296, policy loss: 11.084357250788093
Experience 32, Iter 45, disc loss: 0.00019440057152434343, policy loss: 11.216170715172051
Experience 32, Iter 46, disc loss: 0.00021133195835446261, policy loss: 10.854093571668775
Experience 32, Iter 47, disc loss: 0.00023435439032411126, policy loss: 10.78376970379701
Experience 32, Iter 48, disc loss: 0.00020195296744137935, policy loss: 11.871625608006987
Experience 32, Iter 49, disc loss: 0.00022191207900050308, policy loss: 9.955648873542591
Experience 32, Iter 50, disc loss: 0.00020332372458759787, policy loss: 11.03683966869283
Experience 32, Iter 51, disc loss: 0.0002107125534436289, policy loss: 11.416764157236125
Experience 32, Iter 52, disc loss: 0.00024073860659841722, policy loss: 10.09841048726312
Experience 32, Iter 53, disc loss: 0.00021093651101741756, policy loss: 11.341867075986196
Experience 32, Iter 54, disc loss: 0.0002085503299538095, policy loss: 10.736211419805318
Experience 32, Iter 55, disc loss: 0.0002153250364151242, policy loss: 10.841418550437357
Experience 32, Iter 56, disc loss: 0.00019883665610529296, policy loss: 11.543924043466564
Experience 32, Iter 57, disc loss: 0.0002156312154833981, policy loss: 11.492340727415677
Experience 32, Iter 58, disc loss: 0.00020813164445613209, policy loss: 10.598558796996366
Experience 32, Iter 59, disc loss: 0.00024240271420387997, policy loss: 10.306098208307386
Experience 32, Iter 60, disc loss: 0.0002177716052290916, policy loss: 10.871800610886654
Experience 32, Iter 61, disc loss: 0.00021960873263813358, policy loss: 10.536528469687298
Experience 32, Iter 62, disc loss: 0.0002099010353117563, policy loss: 12.43976023563848
Experience 32, Iter 63, disc loss: 0.00021049539651947204, policy loss: 11.071928928642855
Experience 32, Iter 64, disc loss: 0.00018857065751633353, policy loss: 11.881849771792655
Experience 32, Iter 65, disc loss: 0.00021738627210626403, policy loss: 10.780420978611463
Experience 32, Iter 66, disc loss: 0.00023889543428536256, policy loss: 10.61046876884958
Experience 32, Iter 67, disc loss: 0.00020959115798095446, policy loss: 10.870886155067604
Experience 32, Iter 68, disc loss: 0.00021932639417118332, policy loss: 11.380481405689034
Experience 32, Iter 69, disc loss: 0.000237257977567665, policy loss: 10.487595278708426
Experience 32, Iter 70, disc loss: 0.0002170064850693374, policy loss: 10.656275606759106
Experience 32, Iter 71, disc loss: 0.00021938428448403825, policy loss: 10.654762456538162
Experience 32, Iter 72, disc loss: 0.00024201694042454925, policy loss: 10.4532362283675
Experience 32, Iter 73, disc loss: 0.00022483760452170865, policy loss: 12.046412988483484
Experience 32, Iter 74, disc loss: 0.00021580818853417062, policy loss: 11.046850826233712
Experience 32, Iter 75, disc loss: 0.00021986337307762833, policy loss: 10.673375089054513
Experience 32, Iter 76, disc loss: 0.00022197266087681394, policy loss: 10.557527360638623
Experience 32, Iter 77, disc loss: 0.00021644478443241113, policy loss: 11.438715601883441
Experience 32, Iter 78, disc loss: 0.00020548723212524803, policy loss: 10.911650781784363
Experience 32, Iter 79, disc loss: 0.00021347285469920343, policy loss: 10.665638980044871
Experience 32, Iter 80, disc loss: 0.00022807730640011964, policy loss: 10.697089168568084
Experience 32, Iter 81, disc loss: 0.00023466942536176492, policy loss: 10.88421841808329
Experience 32, Iter 82, disc loss: 0.00023220022758029741, policy loss: 11.644201441576653
Experience 32, Iter 83, disc loss: 0.00023900344049856617, policy loss: 10.939538541656411
Experience 32, Iter 84, disc loss: 0.00020504920756354565, policy loss: 10.815726598602353
Experience 32, Iter 85, disc loss: 0.0002130011425553653, policy loss: 11.039529261501858
Experience 32, Iter 86, disc loss: 0.00021753764163798516, policy loss: 11.40388006352812
Experience 32, Iter 87, disc loss: 0.00021984921882977148, policy loss: 10.577584049209841
Experience 32, Iter 88, disc loss: 0.00020822170170008137, policy loss: 11.783461963025369
Experience 32, Iter 89, disc loss: 0.00024091658593412114, policy loss: 10.50400238197707
Experience 32, Iter 90, disc loss: 0.00019924150809704825, policy loss: 11.254229314157172
Experience 32, Iter 91, disc loss: 0.0002153965405310905, policy loss: 10.96299856598514
Experience 32, Iter 92, disc loss: 0.00023635974608502672, policy loss: 11.186246317025404
Experience 32, Iter 93, disc loss: 0.00019435643515177778, policy loss: 11.989568573068956
Experience 32, Iter 94, disc loss: 0.0001933870843743795, policy loss: 10.777882134192009
Experience 32, Iter 95, disc loss: 0.0002069298208859368, policy loss: 11.00028791453856
Experience 32, Iter 96, disc loss: 0.00019602490056095756, policy loss: 11.645239141745442
Experience 32, Iter 97, disc loss: 0.0001877839802047841, policy loss: 10.904184813367717
Experience 32, Iter 98, disc loss: 0.0002600899280285417, policy loss: 10.362663742457624
Experience 32, Iter 99, disc loss: 0.00023057814423311086, policy loss: 10.580715101885119
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2056],
        [2.0237],
        [0.0450]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0095, 0.1721, 2.1178, 0.0350, 0.0293, 4.5041]],

        [[0.0095, 0.1721, 2.1178, 0.0350, 0.0293, 4.5041]],

        [[0.0095, 0.1721, 2.1178, 0.0350, 0.0293, 4.5041]],

        [[0.0095, 0.1721, 2.1178, 0.0350, 0.0293, 4.5041]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.8225, 8.0949, 0.1801], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.8225, 8.0949, 0.1801])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.562
Iter 2/2000 - Loss: 3.614
Iter 3/2000 - Loss: 3.298
Iter 4/2000 - Loss: 3.239
Iter 5/2000 - Loss: 3.204
Iter 6/2000 - Loss: 3.034
Iter 7/2000 - Loss: 2.807
Iter 8/2000 - Loss: 2.601
Iter 9/2000 - Loss: 2.426
Iter 10/2000 - Loss: 2.248
Iter 11/2000 - Loss: 2.031
Iter 12/2000 - Loss: 1.776
Iter 13/2000 - Loss: 1.503
Iter 14/2000 - Loss: 1.231
Iter 15/2000 - Loss: 0.964
Iter 16/2000 - Loss: 0.692
Iter 17/2000 - Loss: 0.407
Iter 18/2000 - Loss: 0.108
Iter 19/2000 - Loss: -0.198
Iter 20/2000 - Loss: -0.505
Iter 1981/2000 - Loss: -8.045
Iter 1982/2000 - Loss: -8.045
Iter 1983/2000 - Loss: -8.045
Iter 1984/2000 - Loss: -8.045
Iter 1985/2000 - Loss: -8.045
Iter 1986/2000 - Loss: -8.045
Iter 1987/2000 - Loss: -8.045
Iter 1988/2000 - Loss: -8.045
Iter 1989/2000 - Loss: -8.045
Iter 1990/2000 - Loss: -8.045
Iter 1991/2000 - Loss: -8.045
Iter 1992/2000 - Loss: -8.045
Iter 1993/2000 - Loss: -8.045
Iter 1994/2000 - Loss: -8.045
Iter 1995/2000 - Loss: -8.045
Iter 1996/2000 - Loss: -8.045
Iter 1997/2000 - Loss: -8.045
Iter 1998/2000 - Loss: -8.045
Iter 1999/2000 - Loss: -8.045
Iter 2000/2000 - Loss: -8.045
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.4240,  4.3936, 29.2100,  3.0768,  6.7022, 43.8680]],

        [[14.7035, 29.6608,  7.1348,  1.3597,  1.8717, 25.5306]],

        [[16.3792, 30.0612,  8.3036,  0.9372,  0.9191, 23.9390]],

        [[10.7833, 24.2495, 13.8778,  2.3526,  1.9814, 42.9864]]])
Signal Variance: tensor([ 0.0428,  1.8521, 14.5202,  0.5355])
Estimated target variance: tensor([0.0159, 0.8225, 8.0949, 0.1801])
N: 330
Signal to noise ratio: tensor([11.4871, 73.0398, 80.0835, 41.2732])
Bound on condition number: tensor([  43545.3490, 1760487.9497, 2116413.8382,  562148.8415])
Policy Optimizer learning rate:
9.668466638656907e-05
Experience 33, Iter 0, disc loss: 0.00016289263053763262, policy loss: 11.926809863697587
Experience 33, Iter 1, disc loss: 0.00018547308627613404, policy loss: 11.183586385349548
Experience 33, Iter 2, disc loss: 0.00021212258378403717, policy loss: 11.876058937965574
Experience 33, Iter 3, disc loss: 0.0001974464772785178, policy loss: 10.628645756354235
Experience 33, Iter 4, disc loss: 0.00019920097390075764, policy loss: 10.74710479934476
Experience 33, Iter 5, disc loss: 0.00024003897104971086, policy loss: 11.009734316523879
Experience 33, Iter 6, disc loss: 0.0002210839602132527, policy loss: 10.381444281844358
Experience 33, Iter 7, disc loss: 0.00020328460192390192, policy loss: 10.621909214991298
Experience 33, Iter 8, disc loss: 0.00019599292393197246, policy loss: 11.117713488325744
Experience 33, Iter 9, disc loss: 0.00019272271043775126, policy loss: 11.354955475217368
Experience 33, Iter 10, disc loss: 0.00022136455752806342, policy loss: 10.967868977047733
Experience 33, Iter 11, disc loss: 0.00019776222894054874, policy loss: 11.85013736270292
Experience 33, Iter 12, disc loss: 0.00021276509556687441, policy loss: 10.714393507335
Experience 33, Iter 13, disc loss: 0.00020337712071377592, policy loss: 10.764700098949938
Experience 33, Iter 14, disc loss: 0.00020636795639207258, policy loss: 11.027752197276232
Experience 33, Iter 15, disc loss: 0.00017974004726570286, policy loss: 11.288975754879484
Experience 33, Iter 16, disc loss: 0.0001961097199352543, policy loss: 11.43339716033144
Experience 33, Iter 17, disc loss: 0.00019624707533553556, policy loss: 11.363381569919364
Experience 33, Iter 18, disc loss: 0.0001965124478163274, policy loss: 10.495255066035615
Experience 33, Iter 19, disc loss: 0.000192457989682926, policy loss: 11.079931711733273
Experience 33, Iter 20, disc loss: 0.00018322781642961564, policy loss: 11.79240135471971
Experience 33, Iter 21, disc loss: 0.00020079326081778997, policy loss: 10.501451511611984
Experience 33, Iter 22, disc loss: 0.0001847424576049619, policy loss: 11.773349621377033
Experience 33, Iter 23, disc loss: 0.00020545231144113472, policy loss: 10.642048159451903
Experience 33, Iter 24, disc loss: 0.00019249247329127455, policy loss: 10.869012571445525
Experience 33, Iter 25, disc loss: 0.00021810583602672348, policy loss: 10.587096046344007
Experience 33, Iter 26, disc loss: 0.0002214151628952976, policy loss: 10.900310826154119
Experience 33, Iter 27, disc loss: 0.0001884350539631447, policy loss: 10.962481005155526
Experience 33, Iter 28, disc loss: 0.00018265406306682255, policy loss: 11.120004074650748
Experience 33, Iter 29, disc loss: 0.00022242143925321653, policy loss: 10.947524011644212
Experience 33, Iter 30, disc loss: 0.000221995888060331, policy loss: 9.829240382925391
Experience 33, Iter 31, disc loss: 0.00018580283937600004, policy loss: 12.347709868019633
Experience 33, Iter 32, disc loss: 0.00018405931602879833, policy loss: 11.300231943311873
Experience 33, Iter 33, disc loss: 0.00016784793948447667, policy loss: 11.351748938456172
Experience 33, Iter 34, disc loss: 0.00020492391129522887, policy loss: 10.712283029791742
Experience 33, Iter 35, disc loss: 0.00019245438310217368, policy loss: 10.590596788679202
Experience 33, Iter 36, disc loss: 0.00022738947288120068, policy loss: 10.052813514082413
Experience 33, Iter 37, disc loss: 0.00018683170306407846, policy loss: 10.948226246156311
Experience 33, Iter 38, disc loss: 0.00018921553555000783, policy loss: 11.561331206069207
Experience 33, Iter 39, disc loss: 0.0001789260811588134, policy loss: 11.741182541638985
Experience 33, Iter 40, disc loss: 0.00019307958219114493, policy loss: 10.894742274781327
Experience 33, Iter 41, disc loss: 0.00019443287948403405, policy loss: 10.596377730633785
Experience 33, Iter 42, disc loss: 0.00021327693616027618, policy loss: 10.291046837437646
Experience 33, Iter 43, disc loss: 0.00020154407415545855, policy loss: 10.990246508098735
Experience 33, Iter 44, disc loss: 0.00020796738776679647, policy loss: 10.678671073917796
Experience 33, Iter 45, disc loss: 0.00020934507752706346, policy loss: 10.762820177405956
Experience 33, Iter 46, disc loss: 0.00019655060164670154, policy loss: 10.278126927725523
Experience 33, Iter 47, disc loss: 0.00019240064688761706, policy loss: 10.663156841939962
Experience 33, Iter 48, disc loss: 0.0001969897348525064, policy loss: 10.985826990013953
Experience 33, Iter 49, disc loss: 0.00021087898094620904, policy loss: 11.782232240245282
Experience 33, Iter 50, disc loss: 0.00020349402434141167, policy loss: 10.747612861311678
Experience 33, Iter 51, disc loss: 0.00020284991067476803, policy loss: 10.634807671751789
Experience 33, Iter 52, disc loss: 0.00020291064086597682, policy loss: 10.5485426154703
Experience 33, Iter 53, disc loss: 0.00019427652457935544, policy loss: 11.457762851734975
Experience 33, Iter 54, disc loss: 0.00020213952318505924, policy loss: 10.970677399032478
Experience 33, Iter 55, disc loss: 0.00021561732597242292, policy loss: 10.508737927752298
Experience 33, Iter 56, disc loss: 0.00022230169360412612, policy loss: 10.769214282255048
Experience 33, Iter 57, disc loss: 0.000176864187776982, policy loss: 11.72791762663164
Experience 33, Iter 58, disc loss: 0.00018760608564798734, policy loss: 10.725653048329468
Experience 33, Iter 59, disc loss: 0.00018895997126589748, policy loss: 11.624097370170356
Experience 33, Iter 60, disc loss: 0.00019565845747991515, policy loss: 10.804004827236813
Experience 33, Iter 61, disc loss: 0.00021295498232196816, policy loss: 10.383099344803925
Experience 33, Iter 62, disc loss: 0.00018193603612792926, policy loss: 11.058495281171217
Experience 33, Iter 63, disc loss: 0.0001928892737901669, policy loss: 10.751643820371237
Experience 33, Iter 64, disc loss: 0.00021794542795487302, policy loss: 10.378826806846304
Experience 33, Iter 65, disc loss: 0.00017021123107433098, policy loss: 11.616054591261644
Experience 33, Iter 66, disc loss: 0.00019873294370849202, policy loss: 10.659523992653018
Experience 33, Iter 67, disc loss: 0.00018158831148131058, policy loss: 10.704183905819626
Experience 33, Iter 68, disc loss: 0.00016515504719125085, policy loss: 11.346083850545188
Experience 33, Iter 69, disc loss: 0.00015874099477138038, policy loss: 12.375524013141217
Experience 33, Iter 70, disc loss: 0.00016030870136909432, policy loss: 12.126393089760485
Experience 33, Iter 71, disc loss: 0.00019289111850927514, policy loss: 11.350295498392196
Experience 33, Iter 72, disc loss: 0.00019132733749754476, policy loss: 11.33560373805423
Experience 33, Iter 73, disc loss: 0.00019143965362908197, policy loss: 10.374036510585638
Experience 33, Iter 74, disc loss: 0.00020265027732384585, policy loss: 10.385899841194142
Experience 33, Iter 75, disc loss: 0.00018771960353294507, policy loss: 10.351917177841027
Experience 33, Iter 76, disc loss: 0.00021332696040293807, policy loss: 10.077965748246093
Experience 33, Iter 77, disc loss: 0.00020281254513990725, policy loss: 11.10257750443776
Experience 33, Iter 78, disc loss: 0.000207499588158688, policy loss: 10.915757273881056
Experience 33, Iter 79, disc loss: 0.00019047941060334862, policy loss: 10.946538090391865
Experience 33, Iter 80, disc loss: 0.0001937953001467062, policy loss: 11.029639814650961
Experience 33, Iter 81, disc loss: 0.00019544191579799755, policy loss: 10.460828061684126
Experience 33, Iter 82, disc loss: 0.00021370552933902955, policy loss: 11.43894181645287
Experience 33, Iter 83, disc loss: 0.00017981656475913043, policy loss: 11.155477425846492
Experience 33, Iter 84, disc loss: 0.00017983448943342532, policy loss: 11.527073318359767
Experience 33, Iter 85, disc loss: 0.0001987837921190455, policy loss: 10.452280438468215
Experience 33, Iter 86, disc loss: 0.0001783189796435957, policy loss: 11.54245095594142
Experience 33, Iter 87, disc loss: 0.0001931939275633344, policy loss: 10.861923237386478
Experience 33, Iter 88, disc loss: 0.0002038963394819201, policy loss: 10.836574572897604
Experience 33, Iter 89, disc loss: 0.00016587454201690037, policy loss: 11.224197930642145
Experience 33, Iter 90, disc loss: 0.00018987630787028418, policy loss: 10.792284878218098
Experience 33, Iter 91, disc loss: 0.00019664770942356177, policy loss: 11.207879014872047
Experience 33, Iter 92, disc loss: 0.00019675726038138885, policy loss: 10.688579538651126
Experience 33, Iter 93, disc loss: 0.00018727563455796275, policy loss: 10.814008154385782
Experience 33, Iter 94, disc loss: 0.00017177963425317572, policy loss: 10.520817081161578
Experience 33, Iter 95, disc loss: 0.00019400691971986294, policy loss: 11.081137720532347
Experience 33, Iter 96, disc loss: 0.00019950162388214005, policy loss: 10.301415434652167
Experience 33, Iter 97, disc loss: 0.00018811241173583953, policy loss: 11.257711168856812
Experience 33, Iter 98, disc loss: 0.00017810444030374442, policy loss: 10.831222258872577
Experience 33, Iter 99, disc loss: 0.0001639250028733967, policy loss: 11.461467927664408
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2099],
        [2.0451],
        [0.0454]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0094, 0.1728, 2.1370, 0.0352, 0.0294, 4.5918]],

        [[0.0094, 0.1728, 2.1370, 0.0352, 0.0294, 4.5918]],

        [[0.0094, 0.1728, 2.1370, 0.0352, 0.0294, 4.5918]],

        [[0.0094, 0.1728, 2.1370, 0.0352, 0.0294, 4.5918]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0160, 0.8396, 8.1805, 0.1816], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0160, 0.8396, 8.1805, 0.1816])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.580
Iter 2/2000 - Loss: 3.628
Iter 3/2000 - Loss: 3.310
Iter 4/2000 - Loss: 3.246
Iter 5/2000 - Loss: 3.207
Iter 6/2000 - Loss: 3.034
Iter 7/2000 - Loss: 2.804
Iter 8/2000 - Loss: 2.593
Iter 9/2000 - Loss: 2.414
Iter 10/2000 - Loss: 2.232
Iter 11/2000 - Loss: 2.013
Iter 12/2000 - Loss: 1.755
Iter 13/2000 - Loss: 1.481
Iter 14/2000 - Loss: 1.208
Iter 15/2000 - Loss: 0.939
Iter 16/2000 - Loss: 0.667
Iter 17/2000 - Loss: 0.382
Iter 18/2000 - Loss: 0.083
Iter 19/2000 - Loss: -0.224
Iter 20/2000 - Loss: -0.530
Iter 1981/2000 - Loss: -8.061
Iter 1982/2000 - Loss: -8.061
Iter 1983/2000 - Loss: -8.061
Iter 1984/2000 - Loss: -8.061
Iter 1985/2000 - Loss: -8.061
Iter 1986/2000 - Loss: -8.061
Iter 1987/2000 - Loss: -8.061
Iter 1988/2000 - Loss: -8.061
Iter 1989/2000 - Loss: -8.061
Iter 1990/2000 - Loss: -8.061
Iter 1991/2000 - Loss: -8.061
Iter 1992/2000 - Loss: -8.061
Iter 1993/2000 - Loss: -8.061
Iter 1994/2000 - Loss: -8.061
Iter 1995/2000 - Loss: -8.061
Iter 1996/2000 - Loss: -8.061
Iter 1997/2000 - Loss: -8.061
Iter 1998/2000 - Loss: -8.061
Iter 1999/2000 - Loss: -8.061
Iter 2000/2000 - Loss: -8.061
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.3136,  4.7948, 25.3710,  3.0146,  6.8362, 44.6531]],

        [[14.5813, 29.1525,  6.9897,  1.3455,  1.8670, 25.4466]],

        [[16.3612, 29.7712,  8.3643,  0.9036,  0.9177, 22.9701]],

        [[10.7522, 23.9438, 13.5857,  2.3239,  1.9533, 43.6964]]])
Signal Variance: tensor([ 0.0454,  1.7612, 13.5650,  0.5133])
Estimated target variance: tensor([0.0160, 0.8396, 8.1805, 0.1816])
N: 340
Signal to noise ratio: tensor([11.8209, 69.8816, 77.4885, 40.6289])
Bound on condition number: tensor([  47510.6065, 1660370.4321, 2041521.4552,  561241.3559])
Policy Optimizer learning rate:
9.658285256870238e-05
Experience 34, Iter 0, disc loss: 0.0001895712566834022, policy loss: 10.400613004747074
Experience 34, Iter 1, disc loss: 0.00016362236625295926, policy loss: 11.455601715550149
Experience 34, Iter 2, disc loss: 0.00018475595281995395, policy loss: 10.64715769136483
Experience 34, Iter 3, disc loss: 0.00015778527851073834, policy loss: 12.035575638447812
Experience 34, Iter 4, disc loss: 0.00021443691336483776, policy loss: 10.244675378777941
Experience 34, Iter 5, disc loss: 0.0001675195385065526, policy loss: 11.958248670109306
Experience 34, Iter 6, disc loss: 0.0002091660522403106, policy loss: 10.280149641812649
Experience 34, Iter 7, disc loss: 0.00015895125120926764, policy loss: 11.614996646304142
Experience 34, Iter 8, disc loss: 0.00019295745092309504, policy loss: 10.599935142930079
Experience 34, Iter 9, disc loss: 0.0001912724952363133, policy loss: 10.44088657730956
Experience 34, Iter 10, disc loss: 0.00016428666395341317, policy loss: 10.797688494221623
Experience 34, Iter 11, disc loss: 0.000198396356073844, policy loss: 10.810876187255861
Experience 34, Iter 12, disc loss: 0.00017346587110652291, policy loss: 12.059522209620447
Experience 34, Iter 13, disc loss: 0.0001621051794677464, policy loss: 11.39138778513584
Experience 34, Iter 14, disc loss: 0.0001583007283462125, policy loss: 11.746499634455908
Experience 34, Iter 15, disc loss: 0.00017369727293934018, policy loss: 10.879822703876954
Experience 34, Iter 16, disc loss: 0.0001944376444633974, policy loss: 10.502608741646101
Experience 34, Iter 17, disc loss: 0.00018946245144466986, policy loss: 10.335703811670985
Experience 34, Iter 18, disc loss: 0.00017736325275389812, policy loss: 10.724064867326467
Experience 34, Iter 19, disc loss: 0.00018460244369545496, policy loss: 10.415872237092817
Experience 34, Iter 20, disc loss: 0.00019669511576196594, policy loss: 10.501302828473253
Experience 34, Iter 21, disc loss: 0.00018600178102128035, policy loss: 10.503283810746668
Experience 34, Iter 22, disc loss: 0.00017101628942453926, policy loss: 10.618631907444378
Experience 34, Iter 23, disc loss: 0.000182642296347635, policy loss: 11.308109842297496
Experience 34, Iter 24, disc loss: 0.0001813388095905476, policy loss: 11.82713699614466
Experience 34, Iter 25, disc loss: 0.00015839389703007901, policy loss: 11.923991578174347
Experience 34, Iter 26, disc loss: 0.0001683351016378976, policy loss: 11.192500784559565
Experience 34, Iter 27, disc loss: 0.00016890061442088354, policy loss: 10.748362629462292
Experience 34, Iter 28, disc loss: 0.0002121645876647667, policy loss: 9.952062094198144
Experience 34, Iter 29, disc loss: 0.00018840275735057549, policy loss: 11.011300986349955
Experience 34, Iter 30, disc loss: 0.00017466634525401952, policy loss: 10.584055211994515
Experience 34, Iter 31, disc loss: 0.000199394533824164, policy loss: 10.792838709140803
Experience 34, Iter 32, disc loss: 0.00018787736884236556, policy loss: 10.690408875830661
Experience 34, Iter 33, disc loss: 0.00016556644115575837, policy loss: 11.32182910057247
Experience 34, Iter 34, disc loss: 0.00018583262783719123, policy loss: 10.75639572563451
Experience 34, Iter 35, disc loss: 0.00019781528534996127, policy loss: 11.21205901029249
Experience 34, Iter 36, disc loss: 0.00017563764439793478, policy loss: 11.100211850671972
Experience 34, Iter 37, disc loss: 0.0001872235869923662, policy loss: 11.756221209245313
Experience 34, Iter 38, disc loss: 0.00015978928216361208, policy loss: 11.667724635499109
Experience 34, Iter 39, disc loss: 0.00018239815476046365, policy loss: 11.149994804752563
Experience 34, Iter 40, disc loss: 0.0001921109572444873, policy loss: 11.201767519204783
Experience 34, Iter 41, disc loss: 0.00018787069846075, policy loss: 10.710424782479485
Experience 34, Iter 42, disc loss: 0.00020021227384360525, policy loss: 11.155552637329428
Experience 34, Iter 43, disc loss: 0.00018721390270318573, policy loss: 11.18551427984544
Experience 34, Iter 44, disc loss: 0.00017911166514394773, policy loss: 11.411738373835428
Experience 34, Iter 45, disc loss: 0.0001923871860613385, policy loss: 10.649904162861734
Experience 34, Iter 46, disc loss: 0.0001880640149256446, policy loss: 10.496132604255473
Experience 34, Iter 47, disc loss: 0.00018418130741014238, policy loss: 10.459116068366495
Experience 34, Iter 48, disc loss: 0.00016483208206658184, policy loss: 10.791723725684529
Experience 34, Iter 49, disc loss: 0.0001471726919800438, policy loss: 11.178367323697474
Experience 34, Iter 50, disc loss: 0.00017182738995259413, policy loss: 10.969581303666281
Experience 34, Iter 51, disc loss: 0.00018241972665020202, policy loss: 10.196508800818334
Experience 34, Iter 52, disc loss: 0.0001699181130532456, policy loss: 10.605596564229884
Experience 34, Iter 53, disc loss: 0.00019912775796264106, policy loss: 10.694472195289924
Experience 34, Iter 54, disc loss: 0.0001747929027556899, policy loss: 11.289395613538508
Experience 34, Iter 55, disc loss: 0.00017326720456489124, policy loss: 11.734198665916786
Experience 34, Iter 56, disc loss: 0.0002087022687178873, policy loss: 10.302097629836352
Experience 34, Iter 57, disc loss: 0.0002113293920455833, policy loss: 10.679508600646574
Experience 34, Iter 58, disc loss: 0.00018554561210646768, policy loss: 11.366841904093796
Experience 34, Iter 59, disc loss: 0.00020803159372934554, policy loss: 10.05766776737558
Experience 34, Iter 60, disc loss: 0.000188687654557521, policy loss: 10.207446894866255
Experience 34, Iter 61, disc loss: 0.0001688927769129679, policy loss: 12.216473238114192
Experience 34, Iter 62, disc loss: 0.0001888704201717441, policy loss: 11.394846036808186
Experience 34, Iter 63, disc loss: 0.0001840308401164843, policy loss: 10.691248489939468
Experience 34, Iter 64, disc loss: 0.0001811539482374467, policy loss: 11.397181446023549
Experience 34, Iter 65, disc loss: 0.00020662185583365773, policy loss: 10.113866540138941
Experience 34, Iter 66, disc loss: 0.00018492642990985497, policy loss: 10.547611153345816
Experience 34, Iter 67, disc loss: 0.00018281965441437038, policy loss: 11.344015318480709
Experience 34, Iter 68, disc loss: 0.00019249684432512176, policy loss: 10.912969035494454
Experience 34, Iter 69, disc loss: 0.00018363233899292673, policy loss: 11.018372922228531
Experience 34, Iter 70, disc loss: 0.00018240762936285254, policy loss: 10.599827311554957
Experience 34, Iter 71, disc loss: 0.00017644389079999044, policy loss: 11.27412644926958
Experience 34, Iter 72, disc loss: 0.0001537468050422283, policy loss: 10.914281306390482
Experience 34, Iter 73, disc loss: 0.00018378704910421235, policy loss: 10.140100449466983
Experience 34, Iter 74, disc loss: 0.00016718514410267308, policy loss: 11.175286265402669
Experience 34, Iter 75, disc loss: 0.0001733183771829545, policy loss: 11.749277143346113
Experience 34, Iter 76, disc loss: 0.00018196443709340147, policy loss: 10.634645375347898
Experience 34, Iter 77, disc loss: 0.000154504888785022, policy loss: 11.143936259511165
Experience 34, Iter 78, disc loss: 0.00018119583279585565, policy loss: 10.762115474170468
Experience 34, Iter 79, disc loss: 0.0001577418607588298, policy loss: 10.781054298790455
Experience 34, Iter 80, disc loss: 0.00017204405723634028, policy loss: 10.891042987276755
Experience 34, Iter 81, disc loss: 0.00016389161667603055, policy loss: 11.00284379963013
Experience 34, Iter 82, disc loss: 0.00015536415040464524, policy loss: 11.50435410408911
Experience 34, Iter 83, disc loss: 0.00018239046561424092, policy loss: 10.375277394447
Experience 34, Iter 84, disc loss: 0.00016911215965156764, policy loss: 10.39825689581226
Experience 34, Iter 85, disc loss: 0.0001736748677998439, policy loss: 10.495503792489664
Experience 34, Iter 86, disc loss: 0.00017707409838695678, policy loss: 10.331094447598375
Experience 34, Iter 87, disc loss: 0.00018801426629891046, policy loss: 11.0275711328471
Experience 34, Iter 88, disc loss: 0.00019530778485756038, policy loss: 10.434904285159904
Experience 34, Iter 89, disc loss: 0.00018660415520971936, policy loss: 10.17066757054403
Experience 34, Iter 90, disc loss: 0.00018915968238515744, policy loss: 10.422538355133689
Experience 34, Iter 91, disc loss: 0.00019072823136115856, policy loss: 10.472391144068013
Experience 34, Iter 92, disc loss: 0.000196852925098665, policy loss: 10.267321314037403
Experience 34, Iter 93, disc loss: 0.00017887842439899852, policy loss: 10.20844308054145
Experience 34, Iter 94, disc loss: 0.00019263513911670196, policy loss: 10.532822949993374
Experience 34, Iter 95, disc loss: 0.00018502712494749955, policy loss: 10.428891863731241
Experience 34, Iter 96, disc loss: 0.0001664973035732597, policy loss: 10.809442028476052
Experience 34, Iter 97, disc loss: 0.0001716095970376008, policy loss: 10.51566015819315
Experience 34, Iter 98, disc loss: 0.00016846998390237055, policy loss: 10.274596925617221
Experience 34, Iter 99, disc loss: 0.00017970878225839884, policy loss: 10.595976291946487
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2122],
        [2.0525],
        [0.0456]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0092, 0.1724, 2.1445, 0.0353, 0.0293, 4.6374]],

        [[0.0092, 0.1724, 2.1445, 0.0353, 0.0293, 4.6374]],

        [[0.0092, 0.1724, 2.1445, 0.0353, 0.0293, 4.6374]],

        [[0.0092, 0.1724, 2.1445, 0.0353, 0.0293, 4.6374]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0159, 0.8486, 8.2098, 0.1824], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0159, 0.8486, 8.2098, 0.1824])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.600
Iter 2/2000 - Loss: 3.657
Iter 3/2000 - Loss: 3.336
Iter 4/2000 - Loss: 3.275
Iter 5/2000 - Loss: 3.244
Iter 6/2000 - Loss: 3.077
Iter 7/2000 - Loss: 2.852
Iter 8/2000 - Loss: 2.643
Iter 9/2000 - Loss: 2.468
Iter 10/2000 - Loss: 2.292
Iter 11/2000 - Loss: 2.081
Iter 12/2000 - Loss: 1.829
Iter 13/2000 - Loss: 1.557
Iter 14/2000 - Loss: 1.284
Iter 15/2000 - Loss: 1.015
Iter 16/2000 - Loss: 0.742
Iter 17/2000 - Loss: 0.456
Iter 18/2000 - Loss: 0.155
Iter 19/2000 - Loss: -0.157
Iter 20/2000 - Loss: -0.471
Iter 1981/2000 - Loss: -8.098
Iter 1982/2000 - Loss: -8.098
Iter 1983/2000 - Loss: -8.098
Iter 1984/2000 - Loss: -8.098
Iter 1985/2000 - Loss: -8.098
Iter 1986/2000 - Loss: -8.098
Iter 1987/2000 - Loss: -8.098
Iter 1988/2000 - Loss: -8.098
Iter 1989/2000 - Loss: -8.098
Iter 1990/2000 - Loss: -8.098
Iter 1991/2000 - Loss: -8.098
Iter 1992/2000 - Loss: -8.098
Iter 1993/2000 - Loss: -8.098
Iter 1994/2000 - Loss: -8.098
Iter 1995/2000 - Loss: -8.098
Iter 1996/2000 - Loss: -8.099
Iter 1997/2000 - Loss: -8.099
Iter 1998/2000 - Loss: -8.099
Iter 1999/2000 - Loss: -8.099
Iter 2000/2000 - Loss: -8.099
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.3053,  4.9475, 25.7442,  2.7912,  6.7170, 44.9538]],

        [[14.3822, 28.8524,  7.0132,  1.3989,  1.8566, 26.6485]],

        [[16.2292, 29.9117,  8.2391,  0.9702,  0.9207, 23.1656]],

        [[11.2000, 24.1075, 13.6693,  2.3143,  1.9340, 43.0389]]])
Signal Variance: tensor([ 0.0460,  1.9261, 13.9257,  0.5043])
Estimated target variance: tensor([0.0159, 0.8486, 8.2098, 0.1824])
N: 350
Signal to noise ratio: tensor([12.0132, 73.3858, 78.2235, 40.1606])
Bound on condition number: tensor([  50512.3002, 1884918.9657, 2141620.6234,  564507.5007])
Policy Optimizer learning rate:
9.648114596590812e-05
Experience 35, Iter 0, disc loss: 0.00019554365848139984, policy loss: 10.786288528745827
Experience 35, Iter 1, disc loss: 0.00020438320821057182, policy loss: 10.423422676138829
Experience 35, Iter 2, disc loss: 0.00019724138404273898, policy loss: 10.09659097070735
Experience 35, Iter 3, disc loss: 0.00017669342662555756, policy loss: 11.718526128385427
Experience 35, Iter 4, disc loss: 0.00020518419826361046, policy loss: 10.410144801954392
Experience 35, Iter 5, disc loss: 0.00019123081194485106, policy loss: 10.410717725691034
Experience 35, Iter 6, disc loss: 0.00016284072405801128, policy loss: 11.459046320691566
Experience 35, Iter 7, disc loss: 0.00017698676638950556, policy loss: 10.063657955841087
Experience 35, Iter 8, disc loss: 0.00018517242566759145, policy loss: 10.81939018234267
Experience 35, Iter 9, disc loss: 0.00020858575122117593, policy loss: 10.229649422663137
Experience 35, Iter 10, disc loss: 0.00018143943973402592, policy loss: 11.096536741310665
Experience 35, Iter 11, disc loss: 0.00018676156677134643, policy loss: 11.272360674711358
Experience 35, Iter 12, disc loss: 0.00018925901443206665, policy loss: 10.235499006328364
Experience 35, Iter 13, disc loss: 0.00020695466640460302, policy loss: 10.47243904815571
Experience 35, Iter 14, disc loss: 0.00017668938484211835, policy loss: 11.269422682365969
Experience 35, Iter 15, disc loss: 0.00017723164140987625, policy loss: 11.649555440121283
Experience 35, Iter 16, disc loss: 0.00021154762462787273, policy loss: 10.247387910715325
Experience 35, Iter 17, disc loss: 0.00017449967196348594, policy loss: 10.861066496635864
Experience 35, Iter 18, disc loss: 0.00021696389770941066, policy loss: 9.732884603790618
Experience 35, Iter 19, disc loss: 0.00020926737445802287, policy loss: 10.448042452264623
Experience 35, Iter 20, disc loss: 0.00018070128123550447, policy loss: 11.343936821502925
Experience 35, Iter 21, disc loss: 0.0001901018346070192, policy loss: 11.180570139078299
Experience 35, Iter 22, disc loss: 0.00019330112603096913, policy loss: 10.330285138991883
Experience 35, Iter 23, disc loss: 0.00020540742110773002, policy loss: 10.265036394873915
Experience 35, Iter 24, disc loss: 0.00018869098318291495, policy loss: 10.788258228410307
Experience 35, Iter 25, disc loss: 0.00019799980716458024, policy loss: 9.834203626287808
Experience 35, Iter 26, disc loss: 0.0002058034340431409, policy loss: 10.972216943619944
Experience 35, Iter 27, disc loss: 0.0001870163150482998, policy loss: 10.551886808370414
Experience 35, Iter 28, disc loss: 0.00020875882626022002, policy loss: 10.96086891420158
Experience 35, Iter 29, disc loss: 0.0001871506387224126, policy loss: 10.869570587574232
Experience 35, Iter 30, disc loss: 0.0001871823613689392, policy loss: 10.692437627768758
Experience 35, Iter 31, disc loss: 0.00019553340287814958, policy loss: 10.474809010674193
Experience 35, Iter 32, disc loss: 0.00020481736890666405, policy loss: 11.462015309472557
Experience 35, Iter 33, disc loss: 0.00018947604122694308, policy loss: 10.920329470640784
Experience 35, Iter 34, disc loss: 0.00017855615151024935, policy loss: 11.234738802778745
Experience 35, Iter 35, disc loss: 0.0002253414772723664, policy loss: 10.288724159541362
Experience 35, Iter 36, disc loss: 0.00018863818371864523, policy loss: 10.404284802276628
Experience 35, Iter 37, disc loss: 0.00017681710229979332, policy loss: 10.940599930675388
Experience 35, Iter 38, disc loss: 0.0002075606749155101, policy loss: 10.386327523681459
Experience 35, Iter 39, disc loss: 0.00019256507466500607, policy loss: 11.32477124379614
Experience 35, Iter 40, disc loss: 0.00018193718740083158, policy loss: 10.211327832608708
Experience 35, Iter 41, disc loss: 0.00016964649484113266, policy loss: 10.640138657392798
Experience 35, Iter 42, disc loss: 0.00019779336701132303, policy loss: 10.76764334681767
Experience 35, Iter 43, disc loss: 0.00019891249493586774, policy loss: 10.501423824531802
Experience 35, Iter 44, disc loss: 0.00018860347879500215, policy loss: 11.014265877179247
Experience 35, Iter 45, disc loss: 0.00017558989708565243, policy loss: 10.666718274470993
Experience 35, Iter 46, disc loss: 0.00019353787551542446, policy loss: 11.067580976680658
Experience 35, Iter 47, disc loss: 0.00020437047509129616, policy loss: 10.207690408497953
Experience 35, Iter 48, disc loss: 0.00018526784182244322, policy loss: 10.436598638890061
Experience 35, Iter 49, disc loss: 0.00019287720062915488, policy loss: 10.437493622885654
Experience 35, Iter 50, disc loss: 0.00019032812304873697, policy loss: 10.941678416960352
Experience 35, Iter 51, disc loss: 0.0002062354108168386, policy loss: 10.039018248269127
Experience 35, Iter 52, disc loss: 0.0001746939239181499, policy loss: 10.63154171728746
Experience 35, Iter 53, disc loss: 0.0001634876166581432, policy loss: 11.35972188106868
Experience 35, Iter 54, disc loss: 0.00017017385416378189, policy loss: 11.813417773710356
Experience 35, Iter 55, disc loss: 0.00019419573960809727, policy loss: 10.662958404211757
Experience 35, Iter 56, disc loss: 0.00015539539737676462, policy loss: 10.89812328957493
Experience 35, Iter 57, disc loss: 0.00018193221241223065, policy loss: 10.823921117032658
Experience 35, Iter 58, disc loss: 0.000194452681128567, policy loss: 10.607300315684377
Experience 35, Iter 59, disc loss: 0.00019405648993443646, policy loss: 11.780953771930275
Experience 35, Iter 60, disc loss: 0.00019089434106566786, policy loss: 11.285835631860962
Experience 35, Iter 61, disc loss: 0.00016436267195289484, policy loss: 11.495272116563694
Experience 35, Iter 62, disc loss: 0.0001736380735644688, policy loss: 10.598283474035131
Experience 35, Iter 63, disc loss: 0.00019668737384264876, policy loss: 10.430316833124412
Experience 35, Iter 64, disc loss: 0.000180515263962534, policy loss: 10.676283070611369
Experience 35, Iter 65, disc loss: 0.0001983297993686502, policy loss: 10.292809143858504
Experience 35, Iter 66, disc loss: 0.00017215015776373435, policy loss: 10.266673924228202
Experience 35, Iter 67, disc loss: 0.00017629657919744136, policy loss: 10.464846711449766
Experience 35, Iter 68, disc loss: 0.00018657467378981139, policy loss: 10.34601154969956
Experience 35, Iter 69, disc loss: 0.00018160681988768778, policy loss: 10.671697616816074
Experience 35, Iter 70, disc loss: 0.0001661503505308481, policy loss: 10.121057347674885
Experience 35, Iter 71, disc loss: 0.00019066303100217404, policy loss: 10.366376540247881
Experience 35, Iter 72, disc loss: 0.0001797779548150459, policy loss: 10.633353864394987
Experience 35, Iter 73, disc loss: 0.00019262822546851745, policy loss: 9.919027937253016
Experience 35, Iter 74, disc loss: 0.0001943557377405657, policy loss: 10.275153166971872
Experience 35, Iter 75, disc loss: 0.00017163991786179063, policy loss: 10.281186212210788
Experience 35, Iter 76, disc loss: 0.0001857445281141278, policy loss: 11.119715939562845
Experience 35, Iter 77, disc loss: 0.00015051571501613508, policy loss: 10.747002084018298
Experience 35, Iter 78, disc loss: 0.0002040917251386829, policy loss: 10.87842996781468
Experience 35, Iter 79, disc loss: 0.00019623548576275132, policy loss: 10.714574181613463
Experience 35, Iter 80, disc loss: 0.00017498224093265936, policy loss: 11.100644982334968
Experience 35, Iter 81, disc loss: 0.00017485620743236873, policy loss: 10.40025262836572
Experience 35, Iter 82, disc loss: 0.00019923967847492168, policy loss: 10.112826423015449
Experience 35, Iter 83, disc loss: 0.00018604170832083755, policy loss: 11.090085591320156
Experience 35, Iter 84, disc loss: 0.00017560898817542344, policy loss: 10.640264391499294
Experience 35, Iter 85, disc loss: 0.00016950256247069814, policy loss: 10.851857871947464
Experience 35, Iter 86, disc loss: 0.00016957822231565634, policy loss: 10.320131614567778
Experience 35, Iter 87, disc loss: 0.00019147568533674648, policy loss: 10.67421590860938
Experience 35, Iter 88, disc loss: 0.00020371755151276104, policy loss: 9.857928692347375
Experience 35, Iter 89, disc loss: 0.00020472625706268415, policy loss: 10.167382201590112
Experience 35, Iter 90, disc loss: 0.00019663074925542354, policy loss: 10.652279732920949
Experience 35, Iter 91, disc loss: 0.0001913540228248293, policy loss: 10.72907740354368
Experience 35, Iter 92, disc loss: 0.00015958186697878098, policy loss: 11.331768195757812
Experience 35, Iter 93, disc loss: 0.00016478401014828205, policy loss: 10.87787412038847
Experience 35, Iter 94, disc loss: 0.00019925252628867233, policy loss: 10.37194683554853
Experience 35, Iter 95, disc loss: 0.0001788807241483841, policy loss: 10.657432517623006
Experience 35, Iter 96, disc loss: 0.00018938747064114805, policy loss: 11.165343702328354
Experience 35, Iter 97, disc loss: 0.00017318927304888935, policy loss: 10.586675807403033
Experience 35, Iter 98, disc loss: 0.0001914250143391974, policy loss: 10.582808626906308
Experience 35, Iter 99, disc loss: 0.00018420343035915654, policy loss: 10.800734743395271
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0040],
        [0.2157],
        [2.0736],
        [0.0458]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0091, 0.1732, 2.1588, 0.0355, 0.0295, 4.7094]],

        [[0.0091, 0.1732, 2.1588, 0.0355, 0.0295, 4.7094]],

        [[0.0091, 0.1732, 2.1588, 0.0355, 0.0295, 4.7094]],

        [[0.0091, 0.1732, 2.1588, 0.0355, 0.0295, 4.7094]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0160, 0.8627, 8.2944, 0.1834], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0160, 0.8627, 8.2944, 0.1834])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.607
Iter 2/2000 - Loss: 3.659
Iter 3/2000 - Loss: 3.334
Iter 4/2000 - Loss: 3.268
Iter 5/2000 - Loss: 3.231
Iter 6/2000 - Loss: 3.059
Iter 7/2000 - Loss: 2.829
Iter 8/2000 - Loss: 2.615
Iter 9/2000 - Loss: 2.434
Iter 10/2000 - Loss: 2.254
Iter 11/2000 - Loss: 2.039
Iter 12/2000 - Loss: 1.785
Iter 13/2000 - Loss: 1.510
Iter 14/2000 - Loss: 1.234
Iter 15/2000 - Loss: 0.962
Iter 16/2000 - Loss: 0.689
Iter 17/2000 - Loss: 0.403
Iter 18/2000 - Loss: 0.102
Iter 19/2000 - Loss: -0.208
Iter 20/2000 - Loss: -0.521
Iter 1981/2000 - Loss: -8.111
Iter 1982/2000 - Loss: -8.111
Iter 1983/2000 - Loss: -8.111
Iter 1984/2000 - Loss: -8.111
Iter 1985/2000 - Loss: -8.111
Iter 1986/2000 - Loss: -8.111
Iter 1987/2000 - Loss: -8.111
Iter 1988/2000 - Loss: -8.111
Iter 1989/2000 - Loss: -8.111
Iter 1990/2000 - Loss: -8.111
Iter 1991/2000 - Loss: -8.111
Iter 1992/2000 - Loss: -8.111
Iter 1993/2000 - Loss: -8.111
Iter 1994/2000 - Loss: -8.111
Iter 1995/2000 - Loss: -8.112
Iter 1996/2000 - Loss: -8.112
Iter 1997/2000 - Loss: -8.112
Iter 1998/2000 - Loss: -8.112
Iter 1999/2000 - Loss: -8.112
Iter 2000/2000 - Loss: -8.112
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[ 9.2254,  4.8065, 26.3138,  2.6536,  6.6957, 44.0930]],

        [[14.2498, 28.4852,  6.9174,  1.4182,  1.8353, 26.3491]],

        [[16.2311, 28.9700,  8.1291,  0.9839,  0.9113, 23.1715]],

        [[11.1410, 24.1202, 13.2700,  2.2787,  1.9095, 43.7987]]])
Signal Variance: tensor([ 0.0459,  1.8875, 13.9110,  0.4828])
Estimated target variance: tensor([0.0160, 0.8627, 8.2944, 0.1834])
N: 360
Signal to noise ratio: tensor([11.9266, 72.1474, 77.9965, 39.3697])
Bound on condition number: tensor([  51208.5395, 1873890.0970, 2190044.2132,  557992.3805])
Policy Optimizer learning rate:
9.63795464652834e-05
