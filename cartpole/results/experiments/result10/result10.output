Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0069],
        [0.0282],
        [1.0610],
        [0.0180]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0689, 0.2956, 0.8325, 0.0142, 0.0053, 0.2355]],

        [[0.0689, 0.2956, 0.8325, 0.0142, 0.0053, 0.2355]],

        [[0.0689, 0.2956, 0.8325, 0.0142, 0.0053, 0.2355]],

        [[0.0689, 0.2956, 0.8325, 0.0142, 0.0053, 0.2355]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0274, 0.1128, 4.2441, 0.0722], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0274, 0.1128, 4.2441, 0.0722])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 4.802
Iter 2/2000 - Loss: 3.547
Iter 3/2000 - Loss: 2.819
Iter 4/2000 - Loss: 2.470
Iter 5/2000 - Loss: 2.382
Iter 6/2000 - Loss: 2.403
Iter 7/2000 - Loss: 2.446
Iter 8/2000 - Loss: 2.477
Iter 9/2000 - Loss: 2.473
Iter 10/2000 - Loss: 2.436
Iter 11/2000 - Loss: 2.392
Iter 12/2000 - Loss: 2.359
Iter 13/2000 - Loss: 2.337
Iter 14/2000 - Loss: 2.327
Iter 15/2000 - Loss: 2.331
Iter 16/2000 - Loss: 2.344
Iter 17/2000 - Loss: 2.356
Iter 18/2000 - Loss: 2.354
Iter 19/2000 - Loss: 2.332
Iter 20/2000 - Loss: 2.291
Iter 1981/2000 - Loss: -2.177
Iter 1982/2000 - Loss: -2.177
Iter 1983/2000 - Loss: -2.177
Iter 1984/2000 - Loss: -2.177
Iter 1985/2000 - Loss: -2.177
Iter 1986/2000 - Loss: -2.177
Iter 1987/2000 - Loss: -2.178
Iter 1988/2000 - Loss: -2.178
Iter 1989/2000 - Loss: -2.178
Iter 1990/2000 - Loss: -2.178
Iter 1991/2000 - Loss: -2.178
Iter 1992/2000 - Loss: -2.178
Iter 1993/2000 - Loss: -2.178
Iter 1994/2000 - Loss: -2.178
Iter 1995/2000 - Loss: -2.178
Iter 1996/2000 - Loss: -2.178
Iter 1997/2000 - Loss: -2.178
Iter 1998/2000 - Loss: -2.178
Iter 1999/2000 - Loss: -2.178
Iter 2000/2000 - Loss: -2.178
***AFTER OPTIMATION***
Noise Variance: tensor([[1.2396e-04],
        [2.0542e-06],
        [1.4166e-06],
        [2.5458e-05]])
Lengthscale: tensor([[[3.6280e+00, 5.0060e+00, 7.7851e+01, 1.8508e+01, 1.9371e+01,
          7.4422e+01]],

        [[7.9795e+01, 1.0210e+02, 7.9852e+01, 8.0397e-01, 6.6404e-01,
          5.1461e+00]],

        [[6.1630e+01, 7.6812e+01, 2.0994e+01, 9.9246e-01, 9.9929e+00,
          7.1289e+00]],

        [[6.5960e+00, 1.1821e+01, 1.7091e+01, 1.4003e-02, 1.5137e+00,
          1.3884e+00]]])
Signal Variance: tensor([ 0.0562,  0.3371, 10.4260,  0.0671])
Estimated target variance: tensor([0.0274, 0.1128, 4.2441, 0.0722])
N: 10
Signal to noise ratio: tensor([  21.2880,  405.0807, 2712.8927,   51.3393])
Bound on condition number: tensor([4.5328e+03, 1.6409e+06, 7.3598e+07, 2.6358e+04])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.4187186230851854, policy loss: 0.6141456817740404
Experience 1, Iter 1, disc loss: 1.3991178964988726, policy loss: 0.6292810780191042
Experience 1, Iter 2, disc loss: 1.3980201659528761, policy loss: 0.6310363122015858
Experience 1, Iter 3, disc loss: 1.3964616528978402, policy loss: 0.6326307446217934
Experience 1, Iter 4, disc loss: 1.3894512360141746, policy loss: 0.6386639200000226
Experience 1, Iter 5, disc loss: 1.3919239070587839, policy loss: 0.6359952869608188
Experience 1, Iter 6, disc loss: 1.3825150060650269, policy loss: 0.6444200299331714
Experience 1, Iter 7, disc loss: 1.382364236690797, policy loss: 0.6444647256021474
Experience 1, Iter 8, disc loss: 1.3769112231469114, policy loss: 0.6498040552026583
Experience 1, Iter 9, disc loss: 1.3737533636032482, policy loss: 0.6535127089299907
Experience 1, Iter 10, disc loss: 1.3740831574426682, policy loss: 0.6532680702974214
Experience 1, Iter 11, disc loss: 1.3688453573413077, policy loss: 0.6586655409723116
Experience 1, Iter 12, disc loss: 1.3644087567514451, policy loss: 0.6635628262212339
Experience 1, Iter 13, disc loss: 1.3606879123120863, policy loss: 0.6677059069717155
Experience 1, Iter 14, disc loss: 1.357171032351956, policy loss: 0.6714780128939085
Experience 1, Iter 15, disc loss: 1.3497364072814526, policy loss: 0.68000079251882
Experience 1, Iter 16, disc loss: 1.3466083936078155, policy loss: 0.682835742352508
Experience 1, Iter 17, disc loss: 1.3438283411237981, policy loss: 0.6854313182401826
Experience 1, Iter 18, disc loss: 1.3392630708394933, policy loss: 0.6901920915312191
Experience 1, Iter 19, disc loss: 1.3310515275872397, policy loss: 0.699396648603274
Experience 1, Iter 20, disc loss: 1.3161969966646634, policy loss: 0.7168943450166351
Experience 1, Iter 21, disc loss: 1.3168629831357095, policy loss: 0.7142513189984468
Experience 1, Iter 22, disc loss: 1.300638336092234, policy loss: 0.7345098940327798
Experience 1, Iter 23, disc loss: 1.2985170233127754, policy loss: 0.7355960130670172
Experience 1, Iter 24, disc loss: 1.2846715199444088, policy loss: 0.75134481887665
Experience 1, Iter 25, disc loss: 1.2728261584102076, policy loss: 0.7655047286550408
Experience 1, Iter 26, disc loss: 1.2573842664578319, policy loss: 0.782590996605463
Experience 1, Iter 27, disc loss: 1.2378808865123359, policy loss: 0.8056459983454538
Experience 1, Iter 28, disc loss: 1.1954463563797273, policy loss: 0.8633547945547891
Experience 1, Iter 29, disc loss: 1.1601584811879078, policy loss: 0.913790440955575
Experience 1, Iter 30, disc loss: 1.1251274048818285, policy loss: 0.9706278998459832
Experience 1, Iter 31, disc loss: 1.0884045309201182, policy loss: 1.0312303790342776
Experience 1, Iter 32, disc loss: 1.051369006482437, policy loss: 1.0981456794172242
Experience 1, Iter 33, disc loss: 1.0400557898436908, policy loss: 1.1118643881614836
Experience 1, Iter 34, disc loss: 1.0335376359989823, policy loss: 1.1202803534943584
Experience 1, Iter 35, disc loss: 1.0231325578098578, policy loss: 1.1377154257991928
Experience 1, Iter 36, disc loss: 1.0382038948788974, policy loss: 1.100804312800685
Experience 1, Iter 37, disc loss: 1.0309264206374287, policy loss: 1.1136991530117744
Experience 1, Iter 38, disc loss: 1.0284524821650578, policy loss: 1.1110045630848795
Experience 1, Iter 39, disc loss: 1.005370825621203, policy loss: 1.1735482393324839
Experience 1, Iter 40, disc loss: 0.9988876436873013, policy loss: 1.1834080542485736
Experience 1, Iter 41, disc loss: 0.9942264738518756, policy loss: 1.1854243222894654
Experience 1, Iter 42, disc loss: 0.9637653667676578, policy loss: 1.2758100445283727
Experience 1, Iter 43, disc loss: 0.9582898319433564, policy loss: 1.2966918863975794
Experience 1, Iter 44, disc loss: 0.953108139805128, policy loss: 1.3272214603888197
Experience 1, Iter 45, disc loss: 0.90207527035354, policy loss: 1.4763769198626053
Experience 1, Iter 46, disc loss: 0.9011224170872782, policy loss: 1.4605684542129493
Experience 1, Iter 47, disc loss: 0.88109503963402, policy loss: 1.522417456467919
Experience 1, Iter 48, disc loss: 0.8582910649920981, policy loss: 1.5722457884135754
Experience 1, Iter 49, disc loss: 0.8361186787717119, policy loss: 1.6629437138025613
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.0588],
        [0.8873],
        [0.0089]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0456, 0.2429, 0.4502, 0.0081, 0.0032, 1.0997]],

        [[0.0456, 0.2429, 0.4502, 0.0081, 0.0032, 1.0997]],

        [[0.0456, 0.2429, 0.4502, 0.0081, 0.0032, 1.0997]],

        [[0.0456, 0.2429, 0.4502, 0.0081, 0.0032, 1.0997]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0225, 0.2354, 3.5492, 0.0355], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0225, 0.2354, 3.5492, 0.0355])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.438
Iter 2/2000 - Loss: 2.591
Iter 3/2000 - Loss: 2.204
Iter 4/2000 - Loss: 2.122
Iter 5/2000 - Loss: 2.217
Iter 6/2000 - Loss: 2.290
Iter 7/2000 - Loss: 2.296
Iter 8/2000 - Loss: 2.268
Iter 9/2000 - Loss: 2.214
Iter 10/2000 - Loss: 2.147
Iter 11/2000 - Loss: 2.098
Iter 12/2000 - Loss: 2.082
Iter 13/2000 - Loss: 2.087
Iter 14/2000 - Loss: 2.097
Iter 15/2000 - Loss: 2.100
Iter 16/2000 - Loss: 2.092
Iter 17/2000 - Loss: 2.069
Iter 18/2000 - Loss: 2.030
Iter 19/2000 - Loss: 1.986
Iter 20/2000 - Loss: 1.954
Iter 1981/2000 - Loss: 0.139
Iter 1982/2000 - Loss: 0.139
Iter 1983/2000 - Loss: 0.139
Iter 1984/2000 - Loss: 0.139
Iter 1985/2000 - Loss: 0.139
Iter 1986/2000 - Loss: 0.139
Iter 1987/2000 - Loss: 0.139
Iter 1988/2000 - Loss: 0.139
Iter 1989/2000 - Loss: 0.139
Iter 1990/2000 - Loss: 0.139
Iter 1991/2000 - Loss: 0.139
Iter 1992/2000 - Loss: 0.139
Iter 1993/2000 - Loss: 0.139
Iter 1994/2000 - Loss: 0.139
Iter 1995/2000 - Loss: 0.139
Iter 1996/2000 - Loss: 0.139
Iter 1997/2000 - Loss: 0.139
Iter 1998/2000 - Loss: 0.139
Iter 1999/2000 - Loss: 0.139
Iter 2000/2000 - Loss: 0.139
***AFTER OPTIMATION***
Noise Variance: tensor([[1.3862e-04],
        [4.2473e-02],
        [4.6328e-01],
        [6.6759e-03]])
Lengthscale: tensor([[[3.6595e+01, 8.5403e+00, 6.3164e+01, 1.0101e+01, 2.2685e+01,
          5.8896e+01]],

        [[3.6974e-02, 2.4103e-01, 3.9303e-01, 7.8643e-03, 3.0693e-03,
          7.8588e-01]],

        [[3.8128e-02, 2.4146e-01, 4.0210e-01, 7.9212e-03, 3.0935e-03,
          7.8657e-01]],

        [[4.0956e-02, 2.4201e-01, 4.2041e-01, 7.9980e-03, 3.1264e-03,
          9.1140e-01]]])
Signal Variance: tensor([0.1660, 0.1812, 2.9084, 0.0270])
Estimated target variance: tensor([0.0225, 0.2354, 3.5492, 0.0355])
N: 20
Signal to noise ratio: tensor([34.6059,  2.0652,  2.5056,  2.0118])
Bound on condition number: tensor([23952.4029,    86.3021,   126.5593,    81.9435])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.8386730230942282, policy loss: 1.5604062505004967
Experience 2, Iter 1, disc loss: 0.8189850347382011, policy loss: 1.6734995573325648
Experience 2, Iter 2, disc loss: 0.8181209176344613, policy loss: 1.6290315593206208
Experience 2, Iter 3, disc loss: 0.796430588670169, policy loss: 1.6918441916104983
Experience 2, Iter 4, disc loss: 0.7889369117707253, policy loss: 1.7159427534681408
Experience 2, Iter 5, disc loss: 0.786072201177351, policy loss: 1.6857972469938385
Experience 2, Iter 6, disc loss: 0.7555330545168586, policy loss: 1.791332147446633
Experience 2, Iter 7, disc loss: 0.7256789961783763, policy loss: 1.9301752249405697
Experience 2, Iter 8, disc loss: 0.7229688339428942, policy loss: 1.9109255668921323
Experience 2, Iter 9, disc loss: 0.7254501442290681, policy loss: 1.8068243277011264
Experience 2, Iter 10, disc loss: 0.6820608305454785, policy loss: 2.02087181614165
Experience 2, Iter 11, disc loss: 0.6612888190279572, policy loss: 2.200330241048612
Experience 2, Iter 12, disc loss: 0.6605196570048442, policy loss: 2.0733399005780035
Experience 2, Iter 13, disc loss: 0.6497433826314112, policy loss: 2.1402056321710754
Experience 2, Iter 14, disc loss: 0.6738853619930572, policy loss: 1.9337512060120536
Experience 2, Iter 15, disc loss: 0.6368933635631657, policy loss: 2.130083762345716
Experience 2, Iter 16, disc loss: 0.5798557373983482, policy loss: 2.328525610730466
Experience 2, Iter 17, disc loss: 0.5676479926825917, policy loss: 2.312542248374127
Experience 2, Iter 18, disc loss: 0.5597508143294517, policy loss: 2.459639398094976
Experience 2, Iter 19, disc loss: 0.5321227728498147, policy loss: 2.516242607469584
Experience 2, Iter 20, disc loss: 0.527008620602083, policy loss: 2.576890044672732
Experience 2, Iter 21, disc loss: 0.5104566292718601, policy loss: 2.4583587593493634
Experience 2, Iter 22, disc loss: 0.5015434719676193, policy loss: 2.583606491675861
Experience 2, Iter 23, disc loss: 0.4802560702056924, policy loss: 2.651798163434163
Experience 2, Iter 24, disc loss: 0.48558992497025705, policy loss: 2.5813155137211443
Experience 2, Iter 25, disc loss: 0.47116020159273364, policy loss: 2.5179153922184714
Experience 2, Iter 26, disc loss: 0.45648587239307825, policy loss: 2.6915839693852206
Experience 2, Iter 27, disc loss: 0.41315649085811246, policy loss: 2.851522489492008
Experience 2, Iter 28, disc loss: 0.41056977273880907, policy loss: 2.820364480858788
Experience 2, Iter 29, disc loss: 0.40392590871983036, policy loss: 2.9924131473285063
Experience 2, Iter 30, disc loss: 0.3892953358166866, policy loss: 2.8381037926270363
Experience 2, Iter 31, disc loss: 0.3873226057061442, policy loss: 2.990839934459184
Experience 2, Iter 32, disc loss: 0.3489410552112815, policy loss: 3.196719456128221
Experience 2, Iter 33, disc loss: 0.35023666319104396, policy loss: 3.100719956100675
Experience 2, Iter 34, disc loss: 0.3470510853102991, policy loss: 3.1498299695454244
Experience 2, Iter 35, disc loss: 0.33143584018176536, policy loss: 3.046065686653609
Experience 2, Iter 36, disc loss: 0.30767422275301126, policy loss: 3.449825840707006
Experience 2, Iter 37, disc loss: 0.28648214836960734, policy loss: 3.487291048333672
Experience 2, Iter 38, disc loss: 0.2809989403632841, policy loss: 3.4579671772818967
Experience 2, Iter 39, disc loss: 0.29295455027436346, policy loss: 3.260731161015604
Experience 2, Iter 40, disc loss: 0.26247091198563005, policy loss: 3.6266604478705657
Experience 2, Iter 41, disc loss: 0.2608296439512861, policy loss: 3.11100307411303
Experience 2, Iter 42, disc loss: 0.25787501340741614, policy loss: 3.3631028166581594
Experience 2, Iter 43, disc loss: 0.25276786189538974, policy loss: 3.444815489968992
Experience 2, Iter 44, disc loss: 0.24105842959778812, policy loss: 3.530864961882394
Experience 2, Iter 45, disc loss: 0.23319714111348835, policy loss: 3.572622212352189
Experience 2, Iter 46, disc loss: 0.2330095123758895, policy loss: 3.493931506948112
Experience 2, Iter 47, disc loss: 0.2117997595244038, policy loss: 3.7897447891621074
Experience 2, Iter 48, disc loss: 0.21721518231029416, policy loss: 3.726982070813236
Experience 2, Iter 49, disc loss: 0.20213245766830704, policy loss: 3.708936223637872
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0051],
        [0.1239],
        [1.3023],
        [0.0064]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0353, 0.2185, 0.3879, 0.0071, 0.0024, 2.3842]],

        [[0.0353, 0.2185, 0.3879, 0.0071, 0.0024, 2.3842]],

        [[0.0353, 0.2185, 0.3879, 0.0071, 0.0024, 2.3842]],

        [[0.0353, 0.2185, 0.3879, 0.0071, 0.0024, 2.3842]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0206, 0.4956, 5.2091, 0.0258], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0206, 0.4956, 5.2091, 0.0258])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 3.199
Iter 2/2000 - Loss: 2.643
Iter 3/2000 - Loss: 2.428
Iter 4/2000 - Loss: 2.474
Iter 5/2000 - Loss: 2.597
Iter 6/2000 - Loss: 2.603
Iter 7/2000 - Loss: 2.544
Iter 8/2000 - Loss: 2.491
Iter 9/2000 - Loss: 2.441
Iter 10/2000 - Loss: 2.399
Iter 11/2000 - Loss: 2.389
Iter 12/2000 - Loss: 2.413
Iter 13/2000 - Loss: 2.436
Iter 14/2000 - Loss: 2.431
Iter 15/2000 - Loss: 2.396
Iter 16/2000 - Loss: 2.353
Iter 17/2000 - Loss: 2.318
Iter 18/2000 - Loss: 2.296
Iter 19/2000 - Loss: 2.286
Iter 20/2000 - Loss: 2.286
Iter 1981/2000 - Loss: -5.819
Iter 1982/2000 - Loss: -5.819
Iter 1983/2000 - Loss: -5.820
Iter 1984/2000 - Loss: -5.820
Iter 1985/2000 - Loss: -5.820
Iter 1986/2000 - Loss: -5.820
Iter 1987/2000 - Loss: -5.820
Iter 1988/2000 - Loss: -5.820
Iter 1989/2000 - Loss: -5.820
Iter 1990/2000 - Loss: -5.820
Iter 1991/2000 - Loss: -5.820
Iter 1992/2000 - Loss: -5.820
Iter 1993/2000 - Loss: -5.820
Iter 1994/2000 - Loss: -5.820
Iter 1995/2000 - Loss: -5.821
Iter 1996/2000 - Loss: -5.821
Iter 1997/2000 - Loss: -5.821
Iter 1998/2000 - Loss: -5.821
Iter 1999/2000 - Loss: -5.821
Iter 2000/2000 - Loss: -5.821
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0021],
        [0.0002]])
Lengthscale: tensor([[[30.0724,  9.2384, 55.2852, 10.9097, 20.6086, 61.1361]],

        [[28.3161, 42.8848, 31.1471,  2.2528,  0.8090,  9.0541]],

        [[26.1692, 52.6196, 15.3686,  1.0801,  1.4829, 11.8192]],

        [[29.5543, 51.0280, 14.1806,  4.0778,  1.1004, 46.7243]]])
Signal Variance: tensor([ 0.1837,  1.1374, 10.9509,  0.4016])
Estimated target variance: tensor([0.0206, 0.4956, 5.2091, 0.0258])
N: 30
Signal to noise ratio: tensor([30.0954, 51.2173, 72.8834, 43.7414])
Bound on condition number: tensor([ 27172.9085,  78697.3983, 159360.5011,  57400.2690])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.2916666435642921, policy loss: 1.9287441416984883
Experience 3, Iter 1, disc loss: 0.2915150503718833, policy loss: 1.9050940225621296
Experience 3, Iter 2, disc loss: 0.2887864762435913, policy loss: 1.8966333546014917
Experience 3, Iter 3, disc loss: 0.2939615592728646, policy loss: 1.8435487748684536
Experience 3, Iter 4, disc loss: 0.2920189954039449, policy loss: 1.8405558803996296
Experience 3, Iter 5, disc loss: 0.2956794352505954, policy loss: 1.7985386297690635
Experience 3, Iter 6, disc loss: 0.30131910918781313, policy loss: 1.7561645149362966
Experience 3, Iter 7, disc loss: 0.3153566943746986, policy loss: 1.681872897832045
Experience 3, Iter 8, disc loss: 0.32436363924102385, policy loss: 1.6275258046287768
Experience 3, Iter 9, disc loss: 0.3234813754118345, policy loss: 1.6239843062580797
Experience 3, Iter 10, disc loss: 0.3209757187695277, policy loss: 1.6222830138014048
Experience 3, Iter 11, disc loss: 0.3308214153692312, policy loss: 1.5807549013419278
Experience 3, Iter 12, disc loss: 0.34638130882503115, policy loss: 1.5132500374710427
Experience 3, Iter 13, disc loss: 0.34217444565226934, policy loss: 1.5254240147039315
Experience 3, Iter 14, disc loss: 0.36411567452642724, policy loss: 1.4595760256561165
Experience 3, Iter 15, disc loss: 0.36759734128044874, policy loss: 1.4315785204766711
Experience 3, Iter 16, disc loss: 0.36050148191206377, policy loss: 1.4640796418719444
Experience 3, Iter 17, disc loss: 0.3736108463739646, policy loss: 1.4056291163179866
Experience 3, Iter 18, disc loss: 0.3729777290949479, policy loss: 1.4346184971275622
Experience 3, Iter 19, disc loss: 0.38422193234505575, policy loss: 1.3883972128606343
Experience 3, Iter 20, disc loss: 0.4010560445274164, policy loss: 1.3251694773578855
Experience 3, Iter 21, disc loss: 0.39740128364371907, policy loss: 1.3552548168962923
Experience 3, Iter 22, disc loss: 0.404920134916509, policy loss: 1.3524896787984622
Experience 3, Iter 23, disc loss: 0.44342333049589555, policy loss: 1.2779914574725968
Experience 3, Iter 24, disc loss: 0.47154236758708656, policy loss: 1.228697715443358
Experience 3, Iter 25, disc loss: 0.46502576550727215, policy loss: 1.260044512522911
Experience 3, Iter 26, disc loss: 0.5235103651154952, policy loss: 1.1545831272706515
Experience 3, Iter 27, disc loss: 0.5114564986601973, policy loss: 1.142787814819798
Experience 3, Iter 28, disc loss: 0.5617802634917267, policy loss: 1.032030996848648
Experience 3, Iter 29, disc loss: 0.640430434079175, policy loss: 0.8986055636266707
Experience 3, Iter 30, disc loss: 0.6411912790718288, policy loss: 0.9213128514061864
Experience 3, Iter 31, disc loss: 0.5788781539726439, policy loss: 1.0594520025011593
Experience 3, Iter 32, disc loss: 0.5754182329785128, policy loss: 1.0691537760439604
Experience 3, Iter 33, disc loss: 0.4895458103212705, policy loss: 1.2244083217034745
Experience 3, Iter 34, disc loss: 0.5664792499256063, policy loss: 1.0900392350612684
Experience 3, Iter 35, disc loss: 0.5120914223011144, policy loss: 1.228459235333991
Experience 3, Iter 36, disc loss: 0.5012066769349677, policy loss: 1.2508410695258354
Experience 3, Iter 37, disc loss: 0.5198964469825083, policy loss: 1.1348253610188608
Experience 3, Iter 38, disc loss: 0.5094164855845479, policy loss: 1.1301184139193976
Experience 3, Iter 39, disc loss: 0.4522366115406889, policy loss: 1.2278892383079159
Experience 3, Iter 40, disc loss: 0.4185446771095327, policy loss: 1.3710839645395616
Experience 3, Iter 41, disc loss: 0.4272680796674564, policy loss: 1.2914459893728563
Experience 3, Iter 42, disc loss: 0.4529110240934679, policy loss: 1.2180981768249448
Experience 3, Iter 43, disc loss: 0.4471492465831597, policy loss: 1.2165739875913357
Experience 3, Iter 44, disc loss: 0.37343820611670525, policy loss: 1.4365765622705733
Experience 3, Iter 45, disc loss: 0.3457666179136484, policy loss: 1.5466697087771517
Experience 3, Iter 46, disc loss: 0.3625974466858508, policy loss: 1.5667531372903132
Experience 3, Iter 47, disc loss: 0.34110910801178584, policy loss: 1.5820463313186033
Experience 3, Iter 48, disc loss: 0.3374985610014689, policy loss: 1.5625663097854012
Experience 3, Iter 49, disc loss: 0.31410018906122095, policy loss: 1.7037253756396566
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0074],
        [0.1199],
        [1.0975],
        [0.0050]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.9095e-02, 2.9199e-01, 3.1200e-01, 1.0576e-02, 1.9823e-03,
          2.6916e+00]],

        [[4.9095e-02, 2.9199e-01, 3.1200e-01, 1.0576e-02, 1.9823e-03,
          2.6916e+00]],

        [[4.9095e-02, 2.9199e-01, 3.1200e-01, 1.0576e-02, 1.9823e-03,
          2.6916e+00]],

        [[4.9095e-02, 2.9199e-01, 3.1200e-01, 1.0576e-02, 1.9823e-03,
          2.6916e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0297, 0.4797, 4.3898, 0.0200], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0297, 0.4797, 4.3898, 0.0200])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.608
Iter 2/2000 - Loss: 2.465
Iter 3/2000 - Loss: 2.381
Iter 4/2000 - Loss: 2.421
Iter 5/2000 - Loss: 2.437
Iter 6/2000 - Loss: 2.358
Iter 7/2000 - Loss: 2.300
Iter 8/2000 - Loss: 2.324
Iter 9/2000 - Loss: 2.354
Iter 10/2000 - Loss: 2.325
Iter 11/2000 - Loss: 2.269
Iter 12/2000 - Loss: 2.237
Iter 13/2000 - Loss: 2.234
Iter 14/2000 - Loss: 2.230
Iter 15/2000 - Loss: 2.202
Iter 16/2000 - Loss: 2.155
Iter 17/2000 - Loss: 2.105
Iter 18/2000 - Loss: 2.056
Iter 19/2000 - Loss: 2.001
Iter 20/2000 - Loss: 1.933
Iter 1981/2000 - Loss: -6.485
Iter 1982/2000 - Loss: -6.485
Iter 1983/2000 - Loss: -6.485
Iter 1984/2000 - Loss: -6.485
Iter 1985/2000 - Loss: -6.485
Iter 1986/2000 - Loss: -6.485
Iter 1987/2000 - Loss: -6.485
Iter 1988/2000 - Loss: -6.485
Iter 1989/2000 - Loss: -6.485
Iter 1990/2000 - Loss: -6.486
Iter 1991/2000 - Loss: -6.486
Iter 1992/2000 - Loss: -6.486
Iter 1993/2000 - Loss: -6.486
Iter 1994/2000 - Loss: -6.486
Iter 1995/2000 - Loss: -6.486
Iter 1996/2000 - Loss: -6.486
Iter 1997/2000 - Loss: -6.486
Iter 1998/2000 - Loss: -6.486
Iter 1999/2000 - Loss: -6.486
Iter 2000/2000 - Loss: -6.486
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0003],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[30.1372, 11.0727, 45.0061, 11.7552, 17.4180, 60.2124]],

        [[28.6725, 41.8476, 17.8077,  2.1086,  1.0388,  9.3144]],

        [[32.7685, 54.4790, 15.1506,  0.9814,  2.1721, 12.7845]],

        [[35.3124, 52.7204, 17.5510,  4.5458, 14.4246, 56.3401]]])
Signal Variance: tensor([ 0.2307,  1.1194, 12.3550,  0.6260])
Estimated target variance: tensor([0.0297, 0.4797, 4.3898, 0.0200])
N: 40
Signal to noise ratio: tensor([34.2875, 57.7159, 83.3893, 51.5466])
Bound on condition number: tensor([ 47026.4168, 133246.0392, 278151.7559, 106283.0448])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.4998390464541226, policy loss: 1.0202134708690402
Experience 4, Iter 1, disc loss: 0.46583387472673077, policy loss: 1.0835454278419971
Experience 4, Iter 2, disc loss: 0.4606858599699335, policy loss: 1.0907309341680538
Experience 4, Iter 3, disc loss: 0.44966915973805327, policy loss: 1.111144143365795
Experience 4, Iter 4, disc loss: 0.43912433957991903, policy loss: 1.1289404594355492
Experience 4, Iter 5, disc loss: 0.42238721202237095, policy loss: 1.1660543521731097
Experience 4, Iter 6, disc loss: 0.41869012574302983, policy loss: 1.1731424070319847
Experience 4, Iter 7, disc loss: 0.4127475079235326, policy loss: 1.1804901008826576
Experience 4, Iter 8, disc loss: 0.3850709028420966, policy loss: 1.243167822622615
Experience 4, Iter 9, disc loss: 0.38231505542468097, policy loss: 1.2485536890239781
Experience 4, Iter 10, disc loss: 0.3846251740346794, policy loss: 1.2427844843048097
Experience 4, Iter 11, disc loss: 0.37559944255958555, policy loss: 1.2614032331094753
Experience 4, Iter 12, disc loss: 0.37224646047338406, policy loss: 1.2703303641921946
Experience 4, Iter 13, disc loss: 0.37887733486444436, policy loss: 1.2512607799709203
Experience 4, Iter 14, disc loss: 0.35635221563332065, policy loss: 1.3069081622067489
Experience 4, Iter 15, disc loss: 0.36301827827971933, policy loss: 1.2859033163515075
Experience 4, Iter 16, disc loss: 0.36285605543055416, policy loss: 1.2844986118462574
Experience 4, Iter 17, disc loss: 0.37194073646119263, policy loss: 1.2596984728545562
Experience 4, Iter 18, disc loss: 0.374461735383278, policy loss: 1.2498674219219972
Experience 4, Iter 19, disc loss: 0.36915842168579854, policy loss: 1.2615310020240882
Experience 4, Iter 20, disc loss: 0.36623572139925353, policy loss: 1.2670798734915116
Experience 4, Iter 21, disc loss: 0.3561156987358504, policy loss: 1.2923441451909436
Experience 4, Iter 22, disc loss: 0.3558960755115985, policy loss: 1.290794265311884
Experience 4, Iter 23, disc loss: 0.3553128617915435, policy loss: 1.293526113788062
Experience 4, Iter 24, disc loss: 0.3434304580142165, policy loss: 1.320513782500112
Experience 4, Iter 25, disc loss: 0.3434089987461725, policy loss: 1.3197267010659361
Experience 4, Iter 26, disc loss: 0.35123194721352136, policy loss: 1.2995099861689918
Experience 4, Iter 27, disc loss: 0.3414322037214561, policy loss: 1.3231304786204152
Experience 4, Iter 28, disc loss: 0.3458548715456346, policy loss: 1.3155413308601334
Experience 4, Iter 29, disc loss: 0.34960104764924793, policy loss: 1.3033682227485464
Experience 4, Iter 30, disc loss: 0.34884759659864706, policy loss: 1.3084926456607424
Experience 4, Iter 31, disc loss: 0.3904898158340311, policy loss: 1.224217179806911
Experience 4, Iter 32, disc loss: 0.42471218132207883, policy loss: 1.1689018417896433
Experience 4, Iter 33, disc loss: 0.5225724435741115, policy loss: 1.0204620738375376
Experience 4, Iter 34, disc loss: 0.6196176765544661, policy loss: 0.9174683058931858
Experience 4, Iter 35, disc loss: 0.7324081572643124, policy loss: 0.8663765487844215
Experience 4, Iter 36, disc loss: 0.7033251482810587, policy loss: 1.0195899754467526
Experience 4, Iter 37, disc loss: 0.7032926723199878, policy loss: 1.0253040185002689
Experience 4, Iter 38, disc loss: 0.6894494529285552, policy loss: 1.0165169720802434
Experience 4, Iter 39, disc loss: 0.6025644678944782, policy loss: 0.9529717623779681
Experience 4, Iter 40, disc loss: 0.413798881889117, policy loss: 1.2362168065530692
Experience 4, Iter 41, disc loss: 0.38039195214494415, policy loss: 1.3290989461688474
Experience 4, Iter 42, disc loss: 0.39450379679212594, policy loss: 1.3004012292504843
Experience 4, Iter 43, disc loss: 0.358172191332153, policy loss: 1.3834173248347816
Experience 4, Iter 44, disc loss: 0.4036900873865612, policy loss: 1.290662347396264
Experience 4, Iter 45, disc loss: 0.4809562417943324, policy loss: 1.1543284627499029
Experience 4, Iter 46, disc loss: 0.5808257998358424, policy loss: 1.0069465597192127
Experience 4, Iter 47, disc loss: 0.5609242438190325, policy loss: 1.3408889866308744
Experience 4, Iter 48, disc loss: 0.562255288097661, policy loss: 1.1887761788202116
Experience 4, Iter 49, disc loss: 0.5080029260528559, policy loss: 1.3770346425529136
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0073],
        [0.1118],
        [1.0911],
        [0.0068]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.1404e-02, 2.8778e-01, 3.8758e-01, 1.3546e-02, 1.7397e-03,
          2.5526e+00]],

        [[5.1404e-02, 2.8778e-01, 3.8758e-01, 1.3546e-02, 1.7397e-03,
          2.5526e+00]],

        [[5.1404e-02, 2.8778e-01, 3.8758e-01, 1.3546e-02, 1.7397e-03,
          2.5526e+00]],

        [[5.1404e-02, 2.8778e-01, 3.8758e-01, 1.3546e-02, 1.7397e-03,
          2.5526e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0293, 0.4474, 4.3643, 0.0272], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0293, 0.4474, 4.3643, 0.0272])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.608
Iter 2/2000 - Loss: 2.509
Iter 3/2000 - Loss: 2.498
Iter 4/2000 - Loss: 2.460
Iter 5/2000 - Loss: 2.454
Iter 6/2000 - Loss: 2.444
Iter 7/2000 - Loss: 2.439
Iter 8/2000 - Loss: 2.430
Iter 9/2000 - Loss: 2.404
Iter 10/2000 - Loss: 2.376
Iter 11/2000 - Loss: 2.366
Iter 12/2000 - Loss: 2.365
Iter 13/2000 - Loss: 2.349
Iter 14/2000 - Loss: 2.313
Iter 15/2000 - Loss: 2.270
Iter 16/2000 - Loss: 2.235
Iter 17/2000 - Loss: 2.201
Iter 18/2000 - Loss: 2.151
Iter 19/2000 - Loss: 2.082
Iter 20/2000 - Loss: 2.000
Iter 1981/2000 - Loss: -6.732
Iter 1982/2000 - Loss: -6.733
Iter 1983/2000 - Loss: -6.733
Iter 1984/2000 - Loss: -6.733
Iter 1985/2000 - Loss: -6.733
Iter 1986/2000 - Loss: -6.733
Iter 1987/2000 - Loss: -6.733
Iter 1988/2000 - Loss: -6.733
Iter 1989/2000 - Loss: -6.733
Iter 1990/2000 - Loss: -6.733
Iter 1991/2000 - Loss: -6.733
Iter 1992/2000 - Loss: -6.733
Iter 1993/2000 - Loss: -6.733
Iter 1994/2000 - Loss: -6.734
Iter 1995/2000 - Loss: -6.734
Iter 1996/2000 - Loss: -6.734
Iter 1997/2000 - Loss: -6.734
Iter 1998/2000 - Loss: -6.734
Iter 1999/2000 - Loss: -6.734
Iter 2000/2000 - Loss: -6.734
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0018],
        [0.0002]])
Lengthscale: tensor([[[30.5382, 11.1627, 46.0518, 21.2915, 12.0434, 59.8743]],

        [[31.6277, 47.1290, 15.1987,  1.7611,  0.9066,  9.4933]],

        [[37.2410, 56.5190, 12.4186,  1.0528,  7.4709, 14.2368]],

        [[33.2933, 49.1380, 22.3476,  5.1797, 15.5692, 62.4810]]])
Signal Variance: tensor([ 0.1978,  0.8549, 14.5711,  0.9033])
Estimated target variance: tensor([0.0293, 0.4474, 4.3643, 0.0272])
N: 50
Signal to noise ratio: tensor([32.0592, 49.3324, 89.4110, 61.1013])
Bound on condition number: tensor([ 51390.5726, 121685.4441, 399717.4941, 186669.2935])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.4115429834071322, policy loss: 1.3552585352890738
Experience 5, Iter 1, disc loss: 0.37910330933857106, policy loss: 1.4481547705918223
Experience 5, Iter 2, disc loss: 0.3728273767904214, policy loss: 1.4745252829663344
Experience 5, Iter 3, disc loss: 0.358708824486167, policy loss: 1.5334176357563936
Experience 5, Iter 4, disc loss: 0.36188707354602256, policy loss: 1.5420977846270694
Experience 5, Iter 5, disc loss: 0.3494334679430996, policy loss: 1.601665243874061
Experience 5, Iter 6, disc loss: 0.3577774848554985, policy loss: 1.592821392937194
Experience 5, Iter 7, disc loss: 0.4073430673733391, policy loss: 1.4537396771833968
Experience 5, Iter 8, disc loss: 0.4160867723027827, policy loss: 1.4771345689447422
Experience 5, Iter 9, disc loss: 0.3979446743222556, policy loss: 1.5645511832575087
Experience 5, Iter 10, disc loss: 0.4105252651475889, policy loss: 1.6165157648183253
Experience 5, Iter 11, disc loss: 0.437585447663135, policy loss: 1.4858283547284536
Experience 5, Iter 12, disc loss: 0.4441492760779431, policy loss: 1.5145647481874036
Experience 5, Iter 13, disc loss: 0.44195311221262, policy loss: 1.5102623457414406
Experience 5, Iter 14, disc loss: 0.4365747226493323, policy loss: 1.5970909590808575
Experience 5, Iter 15, disc loss: 0.42875493770193546, policy loss: 1.6156462427581917
Experience 5, Iter 16, disc loss: 0.4092389122203717, policy loss: 1.728551135777002
Experience 5, Iter 17, disc loss: 0.38195237284421035, policy loss: 1.9207646612809275
Experience 5, Iter 18, disc loss: 0.36288848304768717, policy loss: 1.974195822888111
Experience 5, Iter 19, disc loss: 0.35919128895630037, policy loss: 2.0843279106019406
Experience 5, Iter 20, disc loss: 0.3605575069461275, policy loss: 2.0497412899248255
Experience 5, Iter 21, disc loss: 0.3699850707726671, policy loss: 2.0076288087840233
Experience 5, Iter 22, disc loss: 0.3319710928837283, policy loss: 2.249752028289924
Experience 5, Iter 23, disc loss: 0.3440707201309174, policy loss: 2.1425797536041467
Experience 5, Iter 24, disc loss: 0.34170951272329586, policy loss: 2.122171712007226
Experience 5, Iter 25, disc loss: 0.33041085013988014, policy loss: 2.145397781286719
Experience 5, Iter 26, disc loss: 0.3357626333271919, policy loss: 2.059913307789913
Experience 5, Iter 27, disc loss: 0.3180012677948674, policy loss: 2.1258806028193185
Experience 5, Iter 28, disc loss: 0.3144064037844809, policy loss: 2.080595292514168
Experience 5, Iter 29, disc loss: 0.3022716260288852, policy loss: 2.1602136408467807
Experience 5, Iter 30, disc loss: 0.30603238785532016, policy loss: 2.0173396547554985
Experience 5, Iter 31, disc loss: 0.28394617018667306, policy loss: 2.1332399860462017
Experience 5, Iter 32, disc loss: 0.2787910987466057, policy loss: 2.1496768812611027
Experience 5, Iter 33, disc loss: 0.27523639820744583, policy loss: 2.106629935202116
Experience 5, Iter 34, disc loss: 0.2673955383763035, policy loss: 2.0874076866383824
Experience 5, Iter 35, disc loss: 0.2585923339933185, policy loss: 2.0728297861327483
Experience 5, Iter 36, disc loss: 0.2535160461392526, policy loss: 2.0569469632749833
Experience 5, Iter 37, disc loss: 0.2552360523090334, policy loss: 2.020440140859406
Experience 5, Iter 38, disc loss: 0.24562492886852855, policy loss: 2.039231844554669
Experience 5, Iter 39, disc loss: 0.2430742249969868, policy loss: 2.054250826086074
Experience 5, Iter 40, disc loss: 0.24071994861274015, policy loss: 2.025558477887672
Experience 5, Iter 41, disc loss: 0.2244904416667015, policy loss: 2.1163952444326317
Experience 5, Iter 42, disc loss: 0.2205358596550144, policy loss: 2.1344090103747515
Experience 5, Iter 43, disc loss: 0.22636455955886922, policy loss: 2.0431907123709867
Experience 5, Iter 44, disc loss: 0.2097579286370754, policy loss: 2.1915618693674492
Experience 5, Iter 45, disc loss: 0.2252564258570565, policy loss: 2.0311492302089595
Experience 5, Iter 46, disc loss: 0.22289590561838515, policy loss: 2.0506922036102333
Experience 5, Iter 47, disc loss: 0.21588970396202561, policy loss: 2.0810230650152066
Experience 5, Iter 48, disc loss: 0.215477172403956, policy loss: 2.088191961635096
Experience 5, Iter 49, disc loss: 0.20651699021703127, policy loss: 2.161217716416302
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0068],
        [0.1061],
        [1.0995],
        [0.0095]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.7361e-02, 2.6965e-01, 5.0361e-01, 1.7027e-02, 2.1316e-03,
          2.5574e+00]],

        [[4.7361e-02, 2.6965e-01, 5.0361e-01, 1.7027e-02, 2.1316e-03,
          2.5574e+00]],

        [[4.7361e-02, 2.6965e-01, 5.0361e-01, 1.7027e-02, 2.1316e-03,
          2.5574e+00]],

        [[4.7361e-02, 2.6965e-01, 5.0361e-01, 1.7027e-02, 2.1316e-03,
          2.5574e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0273, 0.4245, 4.3980, 0.0379], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0273, 0.4245, 4.3980, 0.0379])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.688
Iter 2/2000 - Loss: 2.600
Iter 3/2000 - Loss: 2.593
Iter 4/2000 - Loss: 2.528
Iter 5/2000 - Loss: 2.547
Iter 6/2000 - Loss: 2.559
Iter 7/2000 - Loss: 2.511
Iter 8/2000 - Loss: 2.473
Iter 9/2000 - Loss: 2.465
Iter 10/2000 - Loss: 2.456
Iter 11/2000 - Loss: 2.427
Iter 12/2000 - Loss: 2.389
Iter 13/2000 - Loss: 2.356
Iter 14/2000 - Loss: 2.322
Iter 15/2000 - Loss: 2.276
Iter 16/2000 - Loss: 2.213
Iter 17/2000 - Loss: 2.142
Iter 18/2000 - Loss: 2.067
Iter 19/2000 - Loss: 1.980
Iter 20/2000 - Loss: 1.873
Iter 1981/2000 - Loss: -6.783
Iter 1982/2000 - Loss: -6.783
Iter 1983/2000 - Loss: -6.783
Iter 1984/2000 - Loss: -6.783
Iter 1985/2000 - Loss: -6.783
Iter 1986/2000 - Loss: -6.783
Iter 1987/2000 - Loss: -6.783
Iter 1988/2000 - Loss: -6.783
Iter 1989/2000 - Loss: -6.783
Iter 1990/2000 - Loss: -6.783
Iter 1991/2000 - Loss: -6.783
Iter 1992/2000 - Loss: -6.783
Iter 1993/2000 - Loss: -6.784
Iter 1994/2000 - Loss: -6.784
Iter 1995/2000 - Loss: -6.784
Iter 1996/2000 - Loss: -6.784
Iter 1997/2000 - Loss: -6.784
Iter 1998/2000 - Loss: -6.784
Iter 1999/2000 - Loss: -6.784
Iter 2000/2000 - Loss: -6.784
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[27.1796, 11.2897, 42.6665, 18.7384, 11.9381, 60.6956]],

        [[29.3909, 45.7333, 14.3923,  1.5133,  1.1807, 10.1485]],

        [[32.7668, 55.5454,  9.7844,  1.0614,  8.7389, 16.2457]],

        [[26.6864, 45.5582, 11.1278,  3.9695,  2.2579, 44.2864]]])
Signal Variance: tensor([ 0.2017,  0.8033, 14.4260,  0.4500])
Estimated target variance: tensor([0.0273, 0.4245, 4.3980, 0.0379])
N: 60
Signal to noise ratio: tensor([30.4214, 43.6965, 95.2456, 43.1074])
Bound on condition number: tensor([ 55528.5792, 114564.2438, 544304.5727, 111495.6869])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.20217798316179292, policy loss: 2.2227398636812836
Experience 6, Iter 1, disc loss: 0.20372114974513245, policy loss: 2.1704248166960025
Experience 6, Iter 2, disc loss: 0.19440295962715834, policy loss: 2.2687464217021907
Experience 6, Iter 3, disc loss: 0.20612835263599966, policy loss: 2.1306627967928478
Experience 6, Iter 4, disc loss: 0.19955247247595514, policy loss: 2.1871218920144018
Experience 6, Iter 5, disc loss: 0.18501776061282701, policy loss: 2.3175705982678783
Experience 6, Iter 6, disc loss: 0.1796356490763299, policy loss: 2.333050006620344
Experience 6, Iter 7, disc loss: 0.18166008089678895, policy loss: 2.3340110009509285
Experience 6, Iter 8, disc loss: 0.18421613160174455, policy loss: 2.28567991150793
Experience 6, Iter 9, disc loss: 0.17553628559338652, policy loss: 2.4089530456143926
Experience 6, Iter 10, disc loss: 0.17778783546731342, policy loss: 2.3186747675943615
Experience 6, Iter 11, disc loss: 0.17159587572961515, policy loss: 2.3971665732709524
Experience 6, Iter 12, disc loss: 0.17189554545037028, policy loss: 2.3558631235848666
Experience 6, Iter 13, disc loss: 0.16080502024540386, policy loss: 2.4574312621072814
Experience 6, Iter 14, disc loss: 0.15194947533455658, policy loss: 2.5433154486313074
Experience 6, Iter 15, disc loss: 0.1504896902561201, policy loss: 2.538220804797702
Experience 6, Iter 16, disc loss: 0.15145741220140718, policy loss: 2.495113136842505
Experience 6, Iter 17, disc loss: 0.14967062456933397, policy loss: 2.519962742818406
Experience 6, Iter 18, disc loss: 0.14418302601844493, policy loss: 2.563505868186402
Experience 6, Iter 19, disc loss: 0.13845189965845234, policy loss: 2.6704626100265267
Experience 6, Iter 20, disc loss: 0.1460185391173381, policy loss: 2.5433673525619143
Experience 6, Iter 21, disc loss: 0.14112056588097757, policy loss: 2.6106180579401066
Experience 6, Iter 22, disc loss: 0.1329647179531661, policy loss: 2.6483630957314364
Experience 6, Iter 23, disc loss: 0.13190342206543249, policy loss: 2.686143577184783
Experience 6, Iter 24, disc loss: 0.13271708549813532, policy loss: 2.6344887111233293
Experience 6, Iter 25, disc loss: 0.128982240469968, policy loss: 2.658542186151829
Experience 6, Iter 26, disc loss: 0.13759180555965286, policy loss: 2.5615469872547796
Experience 6, Iter 27, disc loss: 0.12396401396313714, policy loss: 2.735390224630806
Experience 6, Iter 28, disc loss: 0.12443904358040768, policy loss: 2.6928899686441086
Experience 6, Iter 29, disc loss: 0.1232776464837948, policy loss: 2.6914551284031534
Experience 6, Iter 30, disc loss: 0.11962991316597185, policy loss: 2.7310352531747384
Experience 6, Iter 31, disc loss: 0.12023841796767308, policy loss: 2.69026165048977
Experience 6, Iter 32, disc loss: 0.12131777621245554, policy loss: 2.6762142572842857
Experience 6, Iter 33, disc loss: 0.11763396053716094, policy loss: 2.750566146438362
Experience 6, Iter 34, disc loss: 0.1128088603175868, policy loss: 2.7683308333087675
Experience 6, Iter 35, disc loss: 0.10729109727369035, policy loss: 2.940048693471941
Experience 6, Iter 36, disc loss: 0.10377868911451335, policy loss: 2.9734623205752095
Experience 6, Iter 37, disc loss: 0.1079530647285818, policy loss: 2.8566494305437207
Experience 6, Iter 38, disc loss: 0.11287612643842289, policy loss: 2.8063997641552927
Experience 6, Iter 39, disc loss: 0.09835062740096127, policy loss: 2.958641865652554
Experience 6, Iter 40, disc loss: 0.09422876754525916, policy loss: 3.00441919051725
Experience 6, Iter 41, disc loss: 0.09482642791559093, policy loss: 2.9617010929175716
Experience 6, Iter 42, disc loss: 0.09619000722905742, policy loss: 2.969626809480916
Experience 6, Iter 43, disc loss: 0.10011423752913737, policy loss: 2.877592325110456
Experience 6, Iter 44, disc loss: 0.09474203765728052, policy loss: 2.9800433113453284
Experience 6, Iter 45, disc loss: 0.09362123935416375, policy loss: 2.994701613200954
Experience 6, Iter 46, disc loss: 0.09873561505261194, policy loss: 2.853652061820042
Experience 6, Iter 47, disc loss: 0.0988907849488243, policy loss: 2.831369780877335
Experience 6, Iter 48, disc loss: 0.09026607997064778, policy loss: 2.9793041798559647
Experience 6, Iter 49, disc loss: 0.09379271872278298, policy loss: 2.9505568597293594
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0062],
        [0.1035],
        [1.0836],
        [0.0103]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.1787e-02, 2.4424e-01, 5.3040e-01, 1.8550e-02, 2.3275e-03,
          2.5786e+00]],

        [[4.1787e-02, 2.4424e-01, 5.3040e-01, 1.8550e-02, 2.3275e-03,
          2.5786e+00]],

        [[4.1787e-02, 2.4424e-01, 5.3040e-01, 1.8550e-02, 2.3275e-03,
          2.5786e+00]],

        [[4.1787e-02, 2.4424e-01, 5.3040e-01, 1.8550e-02, 2.3275e-03,
          2.5786e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0246, 0.4139, 4.3342, 0.0412], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0246, 0.4139, 4.3342, 0.0412])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.653
Iter 2/2000 - Loss: 2.566
Iter 3/2000 - Loss: 2.548
Iter 4/2000 - Loss: 2.478
Iter 5/2000 - Loss: 2.498
Iter 6/2000 - Loss: 2.503
Iter 7/2000 - Loss: 2.445
Iter 8/2000 - Loss: 2.399
Iter 9/2000 - Loss: 2.387
Iter 10/2000 - Loss: 2.368
Iter 11/2000 - Loss: 2.323
Iter 12/2000 - Loss: 2.272
Iter 13/2000 - Loss: 2.227
Iter 14/2000 - Loss: 2.179
Iter 15/2000 - Loss: 2.112
Iter 16/2000 - Loss: 2.027
Iter 17/2000 - Loss: 1.935
Iter 18/2000 - Loss: 1.838
Iter 19/2000 - Loss: 1.726
Iter 20/2000 - Loss: 1.591
Iter 1981/2000 - Loss: -7.008
Iter 1982/2000 - Loss: -7.009
Iter 1983/2000 - Loss: -7.009
Iter 1984/2000 - Loss: -7.009
Iter 1985/2000 - Loss: -7.009
Iter 1986/2000 - Loss: -7.009
Iter 1987/2000 - Loss: -7.009
Iter 1988/2000 - Loss: -7.009
Iter 1989/2000 - Loss: -7.009
Iter 1990/2000 - Loss: -7.009
Iter 1991/2000 - Loss: -7.009
Iter 1992/2000 - Loss: -7.009
Iter 1993/2000 - Loss: -7.009
Iter 1994/2000 - Loss: -7.009
Iter 1995/2000 - Loss: -7.009
Iter 1996/2000 - Loss: -7.009
Iter 1997/2000 - Loss: -7.010
Iter 1998/2000 - Loss: -7.010
Iter 1999/2000 - Loss: -7.010
Iter 2000/2000 - Loss: -7.010
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[24.8664, 11.3321, 41.5257, 15.7177, 13.1132, 59.9927]],

        [[28.1807, 46.0872, 13.4500,  1.3051,  1.3452, 10.3665]],

        [[30.4670, 50.8259, 11.6279,  1.0201,  6.6796, 15.5219]],

        [[26.3226, 46.9048, 12.5542,  4.3035,  1.7014, 37.6837]]])
Signal Variance: tensor([ 0.1899,  0.7783, 14.5573,  0.3856])
Estimated target variance: tensor([0.0246, 0.4139, 4.3342, 0.0412])
N: 70
Signal to noise ratio: tensor([30.4576, 43.3225, 83.0195, 37.9764])
Bound on condition number: tensor([ 64937.6391, 131379.9011, 482458.0934, 100955.4917])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.10181219209806325, policy loss: 2.7693826864247058
Experience 7, Iter 1, disc loss: 0.09914654686332902, policy loss: 2.7952474137003356
Experience 7, Iter 2, disc loss: 0.09429557250150652, policy loss: 2.8692476648220007
Experience 7, Iter 3, disc loss: 0.09905028900713052, policy loss: 2.814759267067914
Experience 7, Iter 4, disc loss: 0.09449785558732221, policy loss: 2.8721857544659573
Experience 7, Iter 5, disc loss: 0.09475558339275766, policy loss: 2.851671714971252
Experience 7, Iter 6, disc loss: 0.0928071811662997, policy loss: 2.895199138734704
Experience 7, Iter 7, disc loss: 0.09819741343998828, policy loss: 2.7776711124002436
Experience 7, Iter 8, disc loss: 0.09248925502661089, policy loss: 2.9703373352885456
Experience 7, Iter 9, disc loss: 0.08921094069214078, policy loss: 3.011618823086497
Experience 7, Iter 10, disc loss: 0.09441171470537241, policy loss: 2.902433273269865
Experience 7, Iter 11, disc loss: 0.09480217503064811, policy loss: 2.840129527841764
Experience 7, Iter 12, disc loss: 0.08862657054726916, policy loss: 2.9655050682201107
Experience 7, Iter 13, disc loss: 0.09578706363058317, policy loss: 2.816656373569752
Experience 7, Iter 14, disc loss: 0.0965418735835118, policy loss: 2.8014373535839368
Experience 7, Iter 15, disc loss: 0.09315276736328684, policy loss: 2.8927765520266426
Experience 7, Iter 16, disc loss: 0.08768694432398703, policy loss: 2.983022854452847
Experience 7, Iter 17, disc loss: 0.09137995766267733, policy loss: 2.9400617395533826
Experience 7, Iter 18, disc loss: 0.08665186229050288, policy loss: 2.9956966992733265
Experience 7, Iter 19, disc loss: 0.09265313784094172, policy loss: 2.8803578596110073
Experience 7, Iter 20, disc loss: 0.08800611147794431, policy loss: 2.98810241101745
Experience 7, Iter 21, disc loss: 0.09456814577505016, policy loss: 2.8380865449343267
Experience 7, Iter 22, disc loss: 0.08773968265177409, policy loss: 2.9793184499959553
Experience 7, Iter 23, disc loss: 0.09072052841311191, policy loss: 2.931364949388038
Experience 7, Iter 24, disc loss: 0.0870153133361772, policy loss: 2.965357201459158
Experience 7, Iter 25, disc loss: 0.08089575795857146, policy loss: 3.030201286840939
Experience 7, Iter 26, disc loss: 0.07890491826658726, policy loss: 3.1150972439377043
Experience 7, Iter 27, disc loss: 0.07418156859467814, policy loss: 3.220570417032194
Experience 7, Iter 28, disc loss: 0.08282697209778879, policy loss: 3.136377905745248
Experience 7, Iter 29, disc loss: 0.07960763502121912, policy loss: 3.0838448983408218
Experience 7, Iter 30, disc loss: 0.06934502659186435, policy loss: 3.2521717826391914
Experience 7, Iter 31, disc loss: 0.0750664644452402, policy loss: 3.1107788240521765
Experience 7, Iter 32, disc loss: 0.0721155140744521, policy loss: 3.381379977801041
Experience 7, Iter 33, disc loss: 0.07742164953595329, policy loss: 3.1220730241953447
Experience 7, Iter 34, disc loss: 0.056107678163305905, policy loss: 3.7023107219891562
Experience 7, Iter 35, disc loss: 0.07235559869165427, policy loss: 3.37372064435741
Experience 7, Iter 36, disc loss: 0.06155999514732864, policy loss: 3.399810859883109
Experience 7, Iter 37, disc loss: 0.05608924739201663, policy loss: 3.5641539375082587
Experience 7, Iter 38, disc loss: 0.05701862616167494, policy loss: 3.4508673911193037
Experience 7, Iter 39, disc loss: 0.05791203694191602, policy loss: 3.421849187718449
Experience 7, Iter 40, disc loss: 0.04722711278828391, policy loss: 4.126741062943816
Experience 7, Iter 41, disc loss: 0.04117431212983276, policy loss: 4.2128192368175075
Experience 7, Iter 42, disc loss: 0.03621709777941631, policy loss: 4.451409239648823
Experience 7, Iter 43, disc loss: 0.03565769422647379, policy loss: 4.331440619297403
Experience 7, Iter 44, disc loss: 0.046248865086801955, policy loss: 3.796387191431723
Experience 7, Iter 45, disc loss: 0.05289424508550053, policy loss: 3.438596296356757
Experience 7, Iter 46, disc loss: 0.043712922630383705, policy loss: 3.7371115124049243
Experience 7, Iter 47, disc loss: 0.041624142406709995, policy loss: 3.7650809638008838
Experience 7, Iter 48, disc loss: 0.04615045830676032, policy loss: 3.7349917065778913
Experience 7, Iter 49, disc loss: 0.05222289139518589, policy loss: 3.4121990835708336
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0056],
        [0.1012],
        [1.0498],
        [0.0096]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.7107e-02, 2.1997e-01, 4.9677e-01, 1.7141e-02, 2.1359e-03,
          2.5029e+00]],

        [[3.7107e-02, 2.1997e-01, 4.9677e-01, 1.7141e-02, 2.1359e-03,
          2.5029e+00]],

        [[3.7107e-02, 2.1997e-01, 4.9677e-01, 1.7141e-02, 2.1359e-03,
          2.5029e+00]],

        [[3.7107e-02, 2.1997e-01, 4.9677e-01, 1.7141e-02, 2.1359e-03,
          2.5029e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0223, 0.4047, 4.1990, 0.0384], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0223, 0.4047, 4.1990, 0.0384])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.530
Iter 2/2000 - Loss: 2.433
Iter 3/2000 - Loss: 2.407
Iter 4/2000 - Loss: 2.327
Iter 5/2000 - Loss: 2.341
Iter 6/2000 - Loss: 2.342
Iter 7/2000 - Loss: 2.279
Iter 8/2000 - Loss: 2.227
Iter 9/2000 - Loss: 2.205
Iter 10/2000 - Loss: 2.178
Iter 11/2000 - Loss: 2.127
Iter 12/2000 - Loss: 2.071
Iter 13/2000 - Loss: 2.020
Iter 14/2000 - Loss: 1.963
Iter 15/2000 - Loss: 1.890
Iter 16/2000 - Loss: 1.799
Iter 17/2000 - Loss: 1.701
Iter 18/2000 - Loss: 1.596
Iter 19/2000 - Loss: 1.477
Iter 20/2000 - Loss: 1.338
Iter 1981/2000 - Loss: -7.186
Iter 1982/2000 - Loss: -7.186
Iter 1983/2000 - Loss: -7.187
Iter 1984/2000 - Loss: -7.187
Iter 1985/2000 - Loss: -7.187
Iter 1986/2000 - Loss: -7.187
Iter 1987/2000 - Loss: -7.187
Iter 1988/2000 - Loss: -7.187
Iter 1989/2000 - Loss: -7.187
Iter 1990/2000 - Loss: -7.187
Iter 1991/2000 - Loss: -7.187
Iter 1992/2000 - Loss: -7.187
Iter 1993/2000 - Loss: -7.187
Iter 1994/2000 - Loss: -7.187
Iter 1995/2000 - Loss: -7.187
Iter 1996/2000 - Loss: -7.187
Iter 1997/2000 - Loss: -7.187
Iter 1998/2000 - Loss: -7.187
Iter 1999/2000 - Loss: -7.188
Iter 2000/2000 - Loss: -7.188
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[23.9201, 10.8362, 40.3510, 17.0948, 11.8804, 57.0898]],

        [[27.2919, 46.8642, 11.6243,  1.3154,  1.8011, 10.9824]],

        [[28.9854, 50.8171, 11.2652,  1.0547,  7.1116, 15.3646]],

        [[25.8545, 46.7852, 13.8570,  4.1761,  1.3096, 38.2261]]])
Signal Variance: tensor([ 0.1766,  0.7803, 14.1474,  0.3732])
Estimated target variance: tensor([0.0223, 0.4047, 4.1990, 0.0384])
N: 80
Signal to noise ratio: tensor([26.8272, 43.5173, 81.3693, 36.0448])
Bound on condition number: tensor([ 57576.9649, 151501.1165, 529678.4031, 103939.2518])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.05096106374804388, policy loss: 3.52109206170105
Experience 8, Iter 1, disc loss: 0.0428020677646814, policy loss: 4.0124511003287875
Experience 8, Iter 2, disc loss: 0.04148809296000423, policy loss: 3.9023206593164237
Experience 8, Iter 3, disc loss: 0.048457757568329235, policy loss: 3.5021544328250696
Experience 8, Iter 4, disc loss: 0.05211331423712973, policy loss: 3.3529839837210447
Experience 8, Iter 5, disc loss: 0.04515052446444349, policy loss: 3.476216497763911
Experience 8, Iter 6, disc loss: 0.04438147650525407, policy loss: 3.53917469366276
Experience 8, Iter 7, disc loss: 0.04794646879642425, policy loss: 3.36512152578992
Experience 8, Iter 8, disc loss: 0.05150398859898766, policy loss: 3.341677678202343
Experience 8, Iter 9, disc loss: 0.05202780986878028, policy loss: 3.2857555331756094
Experience 8, Iter 10, disc loss: 0.04823895986849542, policy loss: 3.411982821047379
Experience 8, Iter 11, disc loss: 0.057354593306069804, policy loss: 3.2092958507163782
Experience 8, Iter 12, disc loss: 0.05302459912979012, policy loss: 3.313720993497665
Experience 8, Iter 13, disc loss: 0.04695812370355796, policy loss: 3.4383840708678166
Experience 8, Iter 14, disc loss: 0.05101721829345581, policy loss: 3.3150020375124525
Experience 8, Iter 15, disc loss: 0.047610892517271845, policy loss: 3.4365047487803966
Experience 8, Iter 16, disc loss: 0.048994410567934146, policy loss: 3.3220713056010336
Experience 8, Iter 17, disc loss: 0.041951539804292386, policy loss: 3.590164147898955
Experience 8, Iter 18, disc loss: 0.04262988428840264, policy loss: 3.581004689347985
Experience 8, Iter 19, disc loss: 0.04411955579061719, policy loss: 3.535520014166907
Experience 8, Iter 20, disc loss: 0.04594297207170835, policy loss: 3.5067951098639134
Experience 8, Iter 21, disc loss: 0.05306672461764464, policy loss: 3.2458743749432752
Experience 8, Iter 22, disc loss: 0.047864740412448335, policy loss: 3.3566780960045333
Experience 8, Iter 23, disc loss: 0.046847853000296626, policy loss: 3.4066732307871277
Experience 8, Iter 24, disc loss: 0.04796396754759988, policy loss: 3.416014140302565
Experience 8, Iter 25, disc loss: 0.04667838616703515, policy loss: 3.3946980975313386
Experience 8, Iter 26, disc loss: 0.04483300829453796, policy loss: 3.4733411931014038
Experience 8, Iter 27, disc loss: 0.046932698791291076, policy loss: 3.4304086703827092
Experience 8, Iter 28, disc loss: 0.0456963890983831, policy loss: 3.4904246904786813
Experience 8, Iter 29, disc loss: 0.04492302581453294, policy loss: 3.470870105526389
Experience 8, Iter 30, disc loss: 0.04485444632235129, policy loss: 3.482557461004913
Experience 8, Iter 31, disc loss: 0.04540594945054311, policy loss: 3.4860964050931265
Experience 8, Iter 32, disc loss: 0.04577303547407703, policy loss: 3.487448831775062
Experience 8, Iter 33, disc loss: 0.044117857987389694, policy loss: 3.484147263032483
Experience 8, Iter 34, disc loss: 0.04447819462345722, policy loss: 3.4835266787337407
Experience 8, Iter 35, disc loss: 0.04341268887819246, policy loss: 3.5007445242037334
Experience 8, Iter 36, disc loss: 0.04639835446397396, policy loss: 3.4183296783637522
Experience 8, Iter 37, disc loss: 0.04735886936529769, policy loss: 3.3853784517689443
Experience 8, Iter 38, disc loss: 0.046999018409353506, policy loss: 3.40822244481634
Experience 8, Iter 39, disc loss: 0.04447123573816984, policy loss: 3.4864910514524254
Experience 8, Iter 40, disc loss: 0.04439581692005813, policy loss: 3.5446001943504797
Experience 8, Iter 41, disc loss: 0.04164885497936288, policy loss: 3.6640506833381616
Experience 8, Iter 42, disc loss: 0.044244061864720255, policy loss: 3.5253117196938533
Experience 8, Iter 43, disc loss: 0.03922827390683634, policy loss: 3.712061474553219
Experience 8, Iter 44, disc loss: 0.0400938700672835, policy loss: 3.6946123390539687
Experience 8, Iter 45, disc loss: 0.042986874652538476, policy loss: 3.6199265867931008
Experience 8, Iter 46, disc loss: 0.04341436397320869, policy loss: 3.567148359384493
Experience 8, Iter 47, disc loss: 0.03996015773816612, policy loss: 3.9324816317462323
Experience 8, Iter 48, disc loss: 0.03792031143028815, policy loss: 3.7250462777027806
Experience 8, Iter 49, disc loss: 0.03700602057029282, policy loss: 3.786013604247769
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0050],
        [0.0924],
        [0.9470],
        [0.0088]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.3769e-02, 1.9687e-01, 4.4542e-01, 1.5384e-02, 1.9863e-03,
          2.2851e+00]],

        [[3.3769e-02, 1.9687e-01, 4.4542e-01, 1.5384e-02, 1.9863e-03,
          2.2851e+00]],

        [[3.3769e-02, 1.9687e-01, 4.4542e-01, 1.5384e-02, 1.9863e-03,
          2.2851e+00]],

        [[3.3769e-02, 1.9687e-01, 4.4542e-01, 1.5384e-02, 1.9863e-03,
          2.2851e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0200, 0.3694, 3.7879, 0.0351], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0200, 0.3694, 3.7879, 0.0351])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.308
Iter 2/2000 - Loss: 2.241
Iter 3/2000 - Loss: 2.189
Iter 4/2000 - Loss: 2.117
Iter 5/2000 - Loss: 2.144
Iter 6/2000 - Loss: 2.125
Iter 7/2000 - Loss: 2.054
Iter 8/2000 - Loss: 2.013
Iter 9/2000 - Loss: 1.998
Iter 10/2000 - Loss: 1.962
Iter 11/2000 - Loss: 1.902
Iter 12/2000 - Loss: 1.843
Iter 13/2000 - Loss: 1.790
Iter 14/2000 - Loss: 1.726
Iter 15/2000 - Loss: 1.641
Iter 16/2000 - Loss: 1.540
Iter 17/2000 - Loss: 1.435
Iter 18/2000 - Loss: 1.322
Iter 19/2000 - Loss: 1.191
Iter 20/2000 - Loss: 1.038
Iter 1981/2000 - Loss: -7.396
Iter 1982/2000 - Loss: -7.396
Iter 1983/2000 - Loss: -7.396
Iter 1984/2000 - Loss: -7.396
Iter 1985/2000 - Loss: -7.396
Iter 1986/2000 - Loss: -7.396
Iter 1987/2000 - Loss: -7.396
Iter 1988/2000 - Loss: -7.396
Iter 1989/2000 - Loss: -7.396
Iter 1990/2000 - Loss: -7.396
Iter 1991/2000 - Loss: -7.396
Iter 1992/2000 - Loss: -7.396
Iter 1993/2000 - Loss: -7.396
Iter 1994/2000 - Loss: -7.396
Iter 1995/2000 - Loss: -7.396
Iter 1996/2000 - Loss: -7.396
Iter 1997/2000 - Loss: -7.397
Iter 1998/2000 - Loss: -7.397
Iter 1999/2000 - Loss: -7.397
Iter 2000/2000 - Loss: -7.397
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[22.0900, 10.7607, 39.3286, 15.5024, 11.1344, 55.1014]],

        [[25.5272, 42.8304, 12.0855,  1.3185,  1.9744, 11.8343]],

        [[29.5174, 47.2732, 11.9854,  1.0155,  6.9499, 15.4824]],

        [[25.0503, 45.2983, 13.4759,  4.0811,  1.2643, 37.5465]]])
Signal Variance: tensor([ 0.1800,  0.9152, 14.4585,  0.3461])
Estimated target variance: tensor([0.0200, 0.3694, 3.7879, 0.0351])
N: 90
Signal to noise ratio: tensor([26.1448, 46.9950, 82.0571, 35.6944])
Bound on condition number: tensor([ 61520.5873, 198768.7650, 606003.5874, 114668.8757])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.03875594842106833, policy loss: 3.6680179798338033
Experience 9, Iter 1, disc loss: 0.041394840736366095, policy loss: 3.5818343245678586
Experience 9, Iter 2, disc loss: 0.03723285372115613, policy loss: 3.7362893115329063
Experience 9, Iter 3, disc loss: 0.037850309235191955, policy loss: 3.743262736254218
Experience 9, Iter 4, disc loss: 0.037182434295475734, policy loss: 3.745824214991112
Experience 9, Iter 5, disc loss: 0.034817293486251524, policy loss: 3.83762305985858
Experience 9, Iter 6, disc loss: 0.03382926745207451, policy loss: 3.8365572181471115
Experience 9, Iter 7, disc loss: 0.033553358580545714, policy loss: 3.842797766280204
Experience 9, Iter 8, disc loss: 0.03444967954133124, policy loss: 3.7847003047225534
Experience 9, Iter 9, disc loss: 0.035975801237005986, policy loss: 3.871899610072411
Experience 9, Iter 10, disc loss: 0.03396651075504451, policy loss: 3.8181063497613903
Experience 9, Iter 11, disc loss: 0.024721386533188824, policy loss: 4.386596292951143
Experience 9, Iter 12, disc loss: 0.025772710415244735, policy loss: 4.363493676401751
Experience 9, Iter 13, disc loss: 0.03423067070020361, policy loss: 3.7403138426732934
Experience 9, Iter 14, disc loss: 0.027390086120789133, policy loss: 4.09880723787722
Experience 9, Iter 15, disc loss: 0.024068064269204662, policy loss: 4.312470725206657
Experience 9, Iter 16, disc loss: 0.02267136241200681, policy loss: 4.371217350163893
Experience 9, Iter 17, disc loss: 0.02648599940592201, policy loss: 4.110694781190183
Experience 9, Iter 18, disc loss: 0.03064877839079224, policy loss: 3.849537403940575
Experience 9, Iter 19, disc loss: 0.029354749491266218, policy loss: 4.092175814453097
Experience 9, Iter 20, disc loss: 0.03131307262904985, policy loss: 3.8393134422131543
Experience 9, Iter 21, disc loss: 0.03126373170279229, policy loss: 3.8012574396921126
Experience 9, Iter 22, disc loss: 0.029670489127539116, policy loss: 3.8707876273726805
Experience 9, Iter 23, disc loss: 0.02682267231121674, policy loss: 4.032074587300327
Experience 9, Iter 24, disc loss: 0.027341157311081388, policy loss: 4.0065438042994534
Experience 9, Iter 25, disc loss: 0.02789973072493782, policy loss: 3.9377166174506484
Experience 9, Iter 26, disc loss: 0.03059998163522995, policy loss: 3.8376307534330447
Experience 9, Iter 27, disc loss: 0.02737951261661484, policy loss: 3.9778514961482596
Experience 9, Iter 28, disc loss: 0.02666498668546472, policy loss: 4.0009717146179815
Experience 9, Iter 29, disc loss: 0.027814961929939895, policy loss: 3.9588593776034244
Experience 9, Iter 30, disc loss: 0.02790386624591111, policy loss: 3.9169660947612166
Experience 9, Iter 31, disc loss: 0.025950073031316256, policy loss: 4.0066213510368085
Experience 9, Iter 32, disc loss: 0.028230867366601985, policy loss: 3.902389829787289
Experience 9, Iter 33, disc loss: 0.025915539960589932, policy loss: 3.9940520459141116
Experience 9, Iter 34, disc loss: 0.027501924572961214, policy loss: 3.950813967600708
Experience 9, Iter 35, disc loss: 0.026787980075199416, policy loss: 3.985862645578293
Experience 9, Iter 36, disc loss: 0.026005793202961352, policy loss: 4.131887238131407
Experience 9, Iter 37, disc loss: 0.025541406052895444, policy loss: 4.140625215829933
Experience 9, Iter 38, disc loss: 0.024615444439255712, policy loss: 4.062143403641317
Experience 9, Iter 39, disc loss: 0.025306767249101587, policy loss: 4.015980916853811
Experience 9, Iter 40, disc loss: 0.026290442783623323, policy loss: 4.0814998710117365
Experience 9, Iter 41, disc loss: 0.025582546525968856, policy loss: 4.1701676441888615
Experience 9, Iter 42, disc loss: 0.022395044283899868, policy loss: 4.5554065809161
Experience 9, Iter 43, disc loss: 0.02414230555304165, policy loss: 4.412224694735109
Experience 9, Iter 44, disc loss: 0.024648654058445206, policy loss: 4.1487303957323185
Experience 9, Iter 45, disc loss: 0.019249279727518737, policy loss: 4.406677580760796
Experience 9, Iter 46, disc loss: 0.01567384442393206, policy loss: 4.690317259971293
Experience 9, Iter 47, disc loss: 0.01534782148513085, policy loss: 4.703651774767756
Experience 9, Iter 48, disc loss: 0.015205296576289933, policy loss: 4.726208415131494
Experience 9, Iter 49, disc loss: 0.017679914410068213, policy loss: 4.477450930257419
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0046],
        [0.0849],
        [0.8624],
        [0.0080]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[3.0421e-02, 1.8088e-01, 4.0728e-01, 1.4407e-02, 1.8347e-03,
          2.1253e+00]],

        [[3.0421e-02, 1.8088e-01, 4.0728e-01, 1.4407e-02, 1.8347e-03,
          2.1253e+00]],

        [[3.0421e-02, 1.8088e-01, 4.0728e-01, 1.4407e-02, 1.8347e-03,
          2.1253e+00]],

        [[3.0421e-02, 1.8088e-01, 4.0728e-01, 1.4407e-02, 1.8347e-03,
          2.1253e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0185, 0.3397, 3.4497, 0.0320], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0185, 0.3397, 3.4497, 0.0320])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 2.142
Iter 2/2000 - Loss: 2.101
Iter 3/2000 - Loss: 2.025
Iter 4/2000 - Loss: 1.963
Iter 5/2000 - Loss: 1.997
Iter 6/2000 - Loss: 1.969
Iter 7/2000 - Loss: 1.897
Iter 8/2000 - Loss: 1.865
Iter 9/2000 - Loss: 1.854
Iter 10/2000 - Loss: 1.815
Iter 11/2000 - Loss: 1.754
Iter 12/2000 - Loss: 1.698
Iter 13/2000 - Loss: 1.646
Iter 14/2000 - Loss: 1.582
Iter 15/2000 - Loss: 1.495
Iter 16/2000 - Loss: 1.395
Iter 17/2000 - Loss: 1.291
Iter 18/2000 - Loss: 1.179
Iter 19/2000 - Loss: 1.047
Iter 20/2000 - Loss: 0.893
Iter 1981/2000 - Loss: -7.590
Iter 1982/2000 - Loss: -7.590
Iter 1983/2000 - Loss: -7.590
Iter 1984/2000 - Loss: -7.590
Iter 1985/2000 - Loss: -7.590
Iter 1986/2000 - Loss: -7.590
Iter 1987/2000 - Loss: -7.590
Iter 1988/2000 - Loss: -7.590
Iter 1989/2000 - Loss: -7.590
Iter 1990/2000 - Loss: -7.590
Iter 1991/2000 - Loss: -7.590
Iter 1992/2000 - Loss: -7.591
Iter 1993/2000 - Loss: -7.591
Iter 1994/2000 - Loss: -7.591
Iter 1995/2000 - Loss: -7.591
Iter 1996/2000 - Loss: -7.591
Iter 1997/2000 - Loss: -7.591
Iter 1998/2000 - Loss: -7.591
Iter 1999/2000 - Loss: -7.591
Iter 2000/2000 - Loss: -7.591
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0021],
        [0.0003]])
Lengthscale: tensor([[[21.4353, 10.5179, 38.7881, 14.9032, 11.2467, 53.4190]],

        [[24.6279, 41.4160, 12.4420,  1.3194,  1.9441, 12.1989]],

        [[28.6501, 45.2944, 11.5072,  1.0275,  6.7523, 15.3110]],

        [[24.5495, 43.8982, 14.3844,  4.0933,  1.2918, 40.8412]]])
Signal Variance: tensor([ 0.1666,  0.9422, 13.7642,  0.3670])
Estimated target variance: tensor([0.0185, 0.3397, 3.4497, 0.0320])
N: 100
Signal to noise ratio: tensor([24.9469, 46.4278, 81.6895, 37.0475])
Bound on condition number: tensor([ 62235.9421, 215555.1963, 667319.2065, 137252.3939])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.01934879570749636, policy loss: 4.358169159049066
Experience 10, Iter 1, disc loss: 0.02219490877530604, policy loss: 4.140277530925524
Experience 10, Iter 2, disc loss: 0.02029359389405405, policy loss: 4.364402097277606
Experience 10, Iter 3, disc loss: 0.022710715563177313, policy loss: 4.109681460189875
Experience 10, Iter 4, disc loss: 0.023779110921865135, policy loss: 4.029618435294228
Experience 10, Iter 5, disc loss: 0.021883637246800473, policy loss: 4.143440231222148
Experience 10, Iter 6, disc loss: 0.022440320182300556, policy loss: 4.092744134383825
Experience 10, Iter 7, disc loss: 0.02392963509965971, policy loss: 4.0101730850867945
Experience 10, Iter 8, disc loss: 0.023032385480410825, policy loss: 4.082116294577147
Experience 10, Iter 9, disc loss: 0.023444041439883368, policy loss: 4.084389069266946
Experience 10, Iter 10, disc loss: 0.02321481454747482, policy loss: 4.1033985384252905
Experience 10, Iter 11, disc loss: 0.022390118596821256, policy loss: 4.142158056779979
Experience 10, Iter 12, disc loss: 0.02092965086342819, policy loss: 4.269694310131153
Experience 10, Iter 13, disc loss: 0.02056907112755778, policy loss: 4.297411060532847
Experience 10, Iter 14, disc loss: 0.021119946457119183, policy loss: 4.2520075560030675
Experience 10, Iter 15, disc loss: 0.021734090751030957, policy loss: 4.217562785551941
Experience 10, Iter 16, disc loss: 0.02143282761702873, policy loss: 4.240636612997912
Experience 10, Iter 17, disc loss: 0.022323447332056507, policy loss: 4.123004012321566
Experience 10, Iter 18, disc loss: 0.021928592239317705, policy loss: 4.1391018330402805
Experience 10, Iter 19, disc loss: 0.020609161055687993, policy loss: 4.2290946792493616
Experience 10, Iter 20, disc loss: 0.02116723882111277, policy loss: 4.198298576009384
Experience 10, Iter 21, disc loss: 0.021034603203180804, policy loss: 4.171726148072891
Experience 10, Iter 22, disc loss: 0.02085410072741911, policy loss: 4.205506779757309
Experience 10, Iter 23, disc loss: 0.02177229092202216, policy loss: 4.1087707233358195
Experience 10, Iter 24, disc loss: 0.02028759191780094, policy loss: 4.237682053132753
Experience 10, Iter 25, disc loss: 0.019618608604343467, policy loss: 4.275929510507329
Experience 10, Iter 26, disc loss: 0.02090552506059718, policy loss: 4.215649096399767
Experience 10, Iter 27, disc loss: 0.021672160558445577, policy loss: 4.195306323767411
Experience 10, Iter 28, disc loss: 0.019257395542890215, policy loss: 4.350976073997305
Experience 10, Iter 29, disc loss: 0.020482709749206306, policy loss: 4.236789986508235
Experience 10, Iter 30, disc loss: 0.018317189435150226, policy loss: 4.361477476331243
Experience 10, Iter 31, disc loss: 0.01769177893090709, policy loss: 4.440855282078609
Experience 10, Iter 32, disc loss: 0.01903109116436645, policy loss: 4.337560623513121
Experience 10, Iter 33, disc loss: 0.019833001639920904, policy loss: 4.286085573392992
Experience 10, Iter 34, disc loss: 0.017161139399679642, policy loss: 4.48541950402612
Experience 10, Iter 35, disc loss: 0.016296065524845464, policy loss: 4.545538195853203
Experience 10, Iter 36, disc loss: 0.017178182296287377, policy loss: 4.427037671890025
Experience 10, Iter 37, disc loss: 0.017075296674536798, policy loss: 4.4712802313613755
Experience 10, Iter 38, disc loss: 0.01950356363533012, policy loss: 4.29388394895979
Experience 10, Iter 39, disc loss: 0.01794581974722532, policy loss: 4.398809739084732
Experience 10, Iter 40, disc loss: 0.01680520505076492, policy loss: 4.49592670172461
Experience 10, Iter 41, disc loss: 0.017261709559294815, policy loss: 4.444654884448543
Experience 10, Iter 42, disc loss: 0.018530365564201734, policy loss: 4.349445964242142
Experience 10, Iter 43, disc loss: 0.016774797749042603, policy loss: 4.450716564574886
Experience 10, Iter 44, disc loss: 0.018183745758472196, policy loss: 4.320232971904861
Experience 10, Iter 45, disc loss: 0.019198496416542095, policy loss: 4.279084222206655
Experience 10, Iter 46, disc loss: 0.01815419882519175, policy loss: 4.322686051402519
Experience 10, Iter 47, disc loss: 0.01832465525228426, policy loss: 4.3684750345199905
Experience 10, Iter 48, disc loss: 0.018756440848610466, policy loss: 4.294447520859426
Experience 10, Iter 49, disc loss: 0.01934872624737758, policy loss: 4.272129298522788
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0042],
        [0.0779],
        [0.7860],
        [0.0074]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.8003e-02, 1.6553e-01, 3.7380e-01, 1.3436e-02, 1.7179e-03,
          1.9680e+00]],

        [[2.8003e-02, 1.6553e-01, 3.7380e-01, 1.3436e-02, 1.7179e-03,
          1.9680e+00]],

        [[2.8003e-02, 1.6553e-01, 3.7380e-01, 1.3436e-02, 1.7179e-03,
          1.9680e+00]],

        [[2.8003e-02, 1.6553e-01, 3.7380e-01, 1.3436e-02, 1.7179e-03,
          1.9680e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0169, 0.3114, 3.1441, 0.0295], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0169, 0.3114, 3.1441, 0.0295])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.967
Iter 2/2000 - Loss: 1.945
Iter 3/2000 - Loss: 1.850
Iter 4/2000 - Loss: 1.792
Iter 5/2000 - Loss: 1.826
Iter 6/2000 - Loss: 1.787
Iter 7/2000 - Loss: 1.708
Iter 8/2000 - Loss: 1.675
Iter 9/2000 - Loss: 1.661
Iter 10/2000 - Loss: 1.614
Iter 11/2000 - Loss: 1.541
Iter 12/2000 - Loss: 1.472
Iter 13/2000 - Loss: 1.409
Iter 14/2000 - Loss: 1.333
Iter 15/2000 - Loss: 1.233
Iter 16/2000 - Loss: 1.117
Iter 17/2000 - Loss: 0.995
Iter 18/2000 - Loss: 0.868
Iter 19/2000 - Loss: 0.725
Iter 20/2000 - Loss: 0.560
Iter 1981/2000 - Loss: -7.704
Iter 1982/2000 - Loss: -7.704
Iter 1983/2000 - Loss: -7.704
Iter 1984/2000 - Loss: -7.704
Iter 1985/2000 - Loss: -7.704
Iter 1986/2000 - Loss: -7.704
Iter 1987/2000 - Loss: -7.704
Iter 1988/2000 - Loss: -7.704
Iter 1989/2000 - Loss: -7.705
Iter 1990/2000 - Loss: -7.705
Iter 1991/2000 - Loss: -7.705
Iter 1992/2000 - Loss: -7.705
Iter 1993/2000 - Loss: -7.705
Iter 1994/2000 - Loss: -7.705
Iter 1995/2000 - Loss: -7.705
Iter 1996/2000 - Loss: -7.705
Iter 1997/2000 - Loss: -7.705
Iter 1998/2000 - Loss: -7.705
Iter 1999/2000 - Loss: -7.705
Iter 2000/2000 - Loss: -7.705
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[20.4174, 10.3993, 37.7230, 13.9789, 11.1125, 52.2566]],

        [[24.0354, 41.3374, 12.1096,  1.3085,  1.9877, 12.0177]],

        [[27.0838, 45.2895, 11.5847,  1.0300,  6.5508, 15.1708]],

        [[23.2440, 41.6400, 14.3755,  4.0661,  1.2813, 40.8858]]])
Signal Variance: tensor([ 0.1650,  0.8995, 13.2944,  0.3666])
Estimated target variance: tensor([0.0169, 0.3114, 3.1441, 0.0295])
N: 110
Signal to noise ratio: tensor([24.1550, 47.7315, 75.8688, 36.1397])
Bound on condition number: tensor([ 64182.0397, 250613.6986, 633168.6153, 143669.6408])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.018674573034951004, policy loss: 4.378582743354968
Experience 11, Iter 1, disc loss: 0.018087064285834783, policy loss: 4.3691652483889065
Experience 11, Iter 2, disc loss: 0.01785958996458026, policy loss: 4.352782338092785
Experience 11, Iter 3, disc loss: 0.019709472613950712, policy loss: 4.2549089695229165
Experience 11, Iter 4, disc loss: 0.01896912481770694, policy loss: 4.275925799025984
Experience 11, Iter 5, disc loss: 0.01860523564198619, policy loss: 4.343513063947384
Experience 11, Iter 6, disc loss: 0.01902285767554157, policy loss: 4.25440939982204
Experience 11, Iter 7, disc loss: 0.01735029000106843, policy loss: 4.4085019161612395
Experience 11, Iter 8, disc loss: 0.015760736351695802, policy loss: 4.56017258194092
Experience 11, Iter 9, disc loss: 0.016367986371021197, policy loss: 4.492606723335236
Experience 11, Iter 10, disc loss: 0.01746823578912817, policy loss: 4.407198297954405
Experience 11, Iter 11, disc loss: 0.0173459728826469, policy loss: 4.3569926060670445
Experience 11, Iter 12, disc loss: 0.016158069202929344, policy loss: 4.475646553737801
Experience 11, Iter 13, disc loss: 0.016617550203438268, policy loss: 4.424134569901069
Experience 11, Iter 14, disc loss: 0.014887139463228583, policy loss: 4.609886689561778
Experience 11, Iter 15, disc loss: 0.014461063158242148, policy loss: 4.654970573565178
Experience 11, Iter 16, disc loss: 0.013663623471999162, policy loss: 4.743473362630952
Experience 11, Iter 17, disc loss: 0.015065362999612014, policy loss: 4.573564079664013
Experience 11, Iter 18, disc loss: 0.016635269866045976, policy loss: 4.433939319572424
Experience 11, Iter 19, disc loss: 0.015897991363482872, policy loss: 4.496389576482453
Experience 11, Iter 20, disc loss: 0.016179590441884092, policy loss: 4.449441672703983
Experience 11, Iter 21, disc loss: 0.01581020744393267, policy loss: 4.5097291906460155
Experience 11, Iter 22, disc loss: 0.01589276933248468, policy loss: 4.480432108054126
Experience 11, Iter 23, disc loss: 0.01560099863481395, policy loss: 4.526835098816582
Experience 11, Iter 24, disc loss: 0.016556293130666602, policy loss: 4.417899568583667
Experience 11, Iter 25, disc loss: 0.015927569625293892, policy loss: 4.517814008487874
Experience 11, Iter 26, disc loss: 0.014169690075408506, policy loss: 4.6662244060484035
Experience 11, Iter 27, disc loss: 0.014517186274840076, policy loss: 4.580184573016081
Experience 11, Iter 28, disc loss: 0.016174719721603534, policy loss: 4.502689997042641
Experience 11, Iter 29, disc loss: 0.013260860720193042, policy loss: 4.761875486448429
Experience 11, Iter 30, disc loss: 0.013002922744619231, policy loss: 4.740813737453184
Experience 11, Iter 31, disc loss: 0.013494192738680385, policy loss: 4.725546065195092
Experience 11, Iter 32, disc loss: 0.0157258646854655, policy loss: 4.510015212491538
Experience 11, Iter 33, disc loss: 0.01463370784749166, policy loss: 4.649027660152162
Experience 11, Iter 34, disc loss: 0.013162130206770132, policy loss: 4.714926260419625
Experience 11, Iter 35, disc loss: 0.01341805301171271, policy loss: 4.702353426334977
Experience 11, Iter 36, disc loss: 0.015146409916518529, policy loss: 4.540241611809368
Experience 11, Iter 37, disc loss: 0.013073838659987493, policy loss: 4.738649039248354
Experience 11, Iter 38, disc loss: 0.01208801782172089, policy loss: 4.813909943050824
Experience 11, Iter 39, disc loss: 0.012651873991299831, policy loss: 4.733662501279905
Experience 11, Iter 40, disc loss: 0.012965697277713507, policy loss: 4.695377265737697
Experience 11, Iter 41, disc loss: 0.014723930357770795, policy loss: 4.510301821477938
Experience 11, Iter 42, disc loss: 0.01450485973442207, policy loss: 4.542253971139181
Experience 11, Iter 43, disc loss: 0.01287997363506313, policy loss: 4.720785304909093
Experience 11, Iter 44, disc loss: 0.014195223378285916, policy loss: 4.571083196297013
Experience 11, Iter 45, disc loss: 0.013876600935934949, policy loss: 4.591582297504904
Experience 11, Iter 46, disc loss: 0.013202147270496728, policy loss: 4.6528706033736285
Experience 11, Iter 47, disc loss: 0.013627435229358694, policy loss: 4.606411389940103
Experience 11, Iter 48, disc loss: 0.013094430873286675, policy loss: 4.665070238110091
Experience 11, Iter 49, disc loss: 0.01385077374379616, policy loss: 4.605425859921733
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0039],
        [0.0718],
        [0.7213],
        [0.0068]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.5720e-02, 1.5429e-01, 3.4505e-01, 1.2474e-02, 1.6204e-03,
          1.8273e+00]],

        [[2.5720e-02, 1.5429e-01, 3.4505e-01, 1.2474e-02, 1.6204e-03,
          1.8273e+00]],

        [[2.5720e-02, 1.5429e-01, 3.4505e-01, 1.2474e-02, 1.6204e-03,
          1.8273e+00]],

        [[2.5720e-02, 1.5429e-01, 3.4505e-01, 1.2474e-02, 1.6204e-03,
          1.8273e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0158, 0.2873, 2.8851, 0.0273], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0158, 0.2873, 2.8851, 0.0273])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.796
Iter 2/2000 - Loss: 1.806
Iter 3/2000 - Loss: 1.685
Iter 4/2000 - Loss: 1.638
Iter 5/2000 - Loss: 1.676
Iter 6/2000 - Loss: 1.627
Iter 7/2000 - Loss: 1.542
Iter 8/2000 - Loss: 1.510
Iter 9/2000 - Loss: 1.499
Iter 10/2000 - Loss: 1.447
Iter 11/2000 - Loss: 1.363
Iter 12/2000 - Loss: 1.285
Iter 13/2000 - Loss: 1.215
Iter 14/2000 - Loss: 1.134
Iter 15/2000 - Loss: 1.026
Iter 16/2000 - Loss: 0.898
Iter 17/2000 - Loss: 0.762
Iter 18/2000 - Loss: 0.622
Iter 19/2000 - Loss: 0.471
Iter 20/2000 - Loss: 0.298
Iter 1981/2000 - Loss: -7.835
Iter 1982/2000 - Loss: -7.835
Iter 1983/2000 - Loss: -7.835
Iter 1984/2000 - Loss: -7.835
Iter 1985/2000 - Loss: -7.835
Iter 1986/2000 - Loss: -7.835
Iter 1987/2000 - Loss: -7.835
Iter 1988/2000 - Loss: -7.835
Iter 1989/2000 - Loss: -7.835
Iter 1990/2000 - Loss: -7.835
Iter 1991/2000 - Loss: -7.835
Iter 1992/2000 - Loss: -7.835
Iter 1993/2000 - Loss: -7.835
Iter 1994/2000 - Loss: -7.836
Iter 1995/2000 - Loss: -7.836
Iter 1996/2000 - Loss: -7.836
Iter 1997/2000 - Loss: -7.836
Iter 1998/2000 - Loss: -7.836
Iter 1999/2000 - Loss: -7.836
Iter 2000/2000 - Loss: -7.836
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0023],
        [0.0003]])
Lengthscale: tensor([[[20.0093, 10.4943, 37.0783, 13.5593, 11.1017, 50.0802]],

        [[23.2571, 40.8045, 12.2661,  1.2984,  1.9401, 11.9296]],

        [[26.1042, 44.2142, 11.7431,  1.0275,  6.4332, 14.9197]],

        [[22.1758, 41.1437, 13.8588,  3.9908,  1.2415, 39.1157]]])
Signal Variance: tensor([ 0.1609,  0.8727, 13.1460,  0.3406])
Estimated target variance: tensor([0.0158, 0.2873, 2.8851, 0.0273])
N: 120
Signal to noise ratio: tensor([23.4054, 46.8137, 75.2796, 35.6135])
Bound on condition number: tensor([ 65738.5003, 262984.1904, 680043.5725, 152199.1880])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.01382637391679245, policy loss: 4.668216904051471
Experience 12, Iter 1, disc loss: 0.013815081941547023, policy loss: 4.592216994511816
Experience 12, Iter 2, disc loss: 0.01411507949356576, policy loss: 4.544377075699202
Experience 12, Iter 3, disc loss: 0.014168053734283429, policy loss: 4.530927148001085
Experience 12, Iter 4, disc loss: 0.01326146396818327, policy loss: 4.664551083144499
Experience 12, Iter 5, disc loss: 0.012583006450696401, policy loss: 4.745942716146299
Experience 12, Iter 6, disc loss: 0.012363999162061714, policy loss: 4.792594376762417
Experience 12, Iter 7, disc loss: 0.013730045098329209, policy loss: 4.5993086326412325
Experience 12, Iter 8, disc loss: 0.012110632722288116, policy loss: 4.8026389625089045
Experience 12, Iter 9, disc loss: 0.01121393885363735, policy loss: 4.882163672193421
Experience 12, Iter 10, disc loss: 0.013333268455311208, policy loss: 4.670651819122158
Experience 12, Iter 11, disc loss: 0.01127220710751934, policy loss: 4.928986628248375
Experience 12, Iter 12, disc loss: 0.011117283351759585, policy loss: 4.930881550005227
Experience 12, Iter 13, disc loss: 0.010612444819051159, policy loss: 4.913017595243454
Experience 12, Iter 14, disc loss: 0.010790923149580859, policy loss: 4.898449044163865
Experience 12, Iter 15, disc loss: 0.011025629870216588, policy loss: 4.832559439206619
Experience 12, Iter 16, disc loss: 0.010539929714502018, policy loss: 4.8986298848445164
Experience 12, Iter 17, disc loss: 0.01030359565825809, policy loss: 5.0673500485087875
Experience 12, Iter 18, disc loss: 0.0113421844084794, policy loss: 4.827269023714929
Experience 12, Iter 19, disc loss: 0.011115870364126569, policy loss: 4.993914993624804
Experience 12, Iter 20, disc loss: 0.011045111614329164, policy loss: 4.829533183724028
Experience 12, Iter 21, disc loss: 0.010507337942595417, policy loss: 4.891224920127304
Experience 12, Iter 22, disc loss: 0.01045209132744282, policy loss: 4.929508052172923
Experience 12, Iter 23, disc loss: 0.009749263185573957, policy loss: 5.004042327229791
Experience 12, Iter 24, disc loss: 0.010261073319114702, policy loss: 4.959456682398805
Experience 12, Iter 25, disc loss: 0.010204439777264044, policy loss: 4.93825918166503
Experience 12, Iter 26, disc loss: 0.010501340355672982, policy loss: 4.968666428328065
Experience 12, Iter 27, disc loss: 0.01139542746395289, policy loss: 4.770321097708793
Experience 12, Iter 28, disc loss: 0.010627271779538063, policy loss: 4.857644539392372
Experience 12, Iter 29, disc loss: 0.01004767117809417, policy loss: 5.014404758400186
Experience 12, Iter 30, disc loss: 0.01132034259482656, policy loss: 4.779944010510349
Experience 12, Iter 31, disc loss: 0.011261448182801077, policy loss: 4.765821162141975
Experience 12, Iter 32, disc loss: 0.010724077039935937, policy loss: 4.841326109361555
Experience 12, Iter 33, disc loss: 0.010539255988031564, policy loss: 4.929039115986956
Experience 12, Iter 34, disc loss: 0.011155268177877674, policy loss: 4.757413502181601
Experience 12, Iter 35, disc loss: 0.01146789941987892, policy loss: 4.7334694699484805
Experience 12, Iter 36, disc loss: 0.011282693211014348, policy loss: 4.913109753393687
Experience 12, Iter 37, disc loss: 0.011642158440191171, policy loss: 4.7826856737329795
Experience 12, Iter 38, disc loss: 0.011228695566094361, policy loss: 4.7483231856476
Experience 12, Iter 39, disc loss: 0.010834592731854144, policy loss: 4.817830602928753
Experience 12, Iter 40, disc loss: 0.010317401977548414, policy loss: 4.933779910485626
Experience 12, Iter 41, disc loss: 0.010469785686694539, policy loss: 4.878377410981449
Experience 12, Iter 42, disc loss: 0.01101151241735078, policy loss: 4.802764080603252
Experience 12, Iter 43, disc loss: 0.010172514962231517, policy loss: 5.00839173173215
Experience 12, Iter 44, disc loss: 0.010430512005806282, policy loss: 4.87669285975354
Experience 12, Iter 45, disc loss: 0.009914303323581222, policy loss: 4.964817864146128
Experience 12, Iter 46, disc loss: 0.010482717160396948, policy loss: 4.869002834721672
Experience 12, Iter 47, disc loss: 0.009894544964205793, policy loss: 4.907742172477969
Experience 12, Iter 48, disc loss: 0.010000478103184872, policy loss: 4.919321588538376
Experience 12, Iter 49, disc loss: 0.009592726540057366, policy loss: 4.9546206133235815
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0037],
        [0.0686],
        [0.6888],
        [0.0065]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.3812e-02, 1.4374e-01, 3.2982e-01, 1.1799e-02, 1.5206e-03,
          1.7379e+00]],

        [[2.3812e-02, 1.4374e-01, 3.2982e-01, 1.1799e-02, 1.5206e-03,
          1.7379e+00]],

        [[2.3812e-02, 1.4374e-01, 3.2982e-01, 1.1799e-02, 1.5206e-03,
          1.7379e+00]],

        [[2.3812e-02, 1.4374e-01, 3.2982e-01, 1.1799e-02, 1.5206e-03,
          1.7379e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0147, 0.2745, 2.7552, 0.0261], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0147, 0.2745, 2.7552, 0.0261])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.699
Iter 2/2000 - Loss: 1.742
Iter 3/2000 - Loss: 1.597
Iter 4/2000 - Loss: 1.561
Iter 5/2000 - Loss: 1.609
Iter 6/2000 - Loss: 1.554
Iter 7/2000 - Loss: 1.466
Iter 8/2000 - Loss: 1.440
Iter 9/2000 - Loss: 1.437
Iter 10/2000 - Loss: 1.387
Iter 11/2000 - Loss: 1.301
Iter 12/2000 - Loss: 1.221
Iter 13/2000 - Loss: 1.155
Iter 14/2000 - Loss: 1.076
Iter 15/2000 - Loss: 0.967
Iter 16/2000 - Loss: 0.834
Iter 17/2000 - Loss: 0.692
Iter 18/2000 - Loss: 0.548
Iter 19/2000 - Loss: 0.393
Iter 20/2000 - Loss: 0.217
Iter 1981/2000 - Loss: -7.952
Iter 1982/2000 - Loss: -7.952
Iter 1983/2000 - Loss: -7.952
Iter 1984/2000 - Loss: -7.952
Iter 1985/2000 - Loss: -7.952
Iter 1986/2000 - Loss: -7.952
Iter 1987/2000 - Loss: -7.952
Iter 1988/2000 - Loss: -7.952
Iter 1989/2000 - Loss: -7.952
Iter 1990/2000 - Loss: -7.952
Iter 1991/2000 - Loss: -7.952
Iter 1992/2000 - Loss: -7.952
Iter 1993/2000 - Loss: -7.952
Iter 1994/2000 - Loss: -7.953
Iter 1995/2000 - Loss: -7.953
Iter 1996/2000 - Loss: -7.953
Iter 1997/2000 - Loss: -7.953
Iter 1998/2000 - Loss: -7.953
Iter 1999/2000 - Loss: -7.953
Iter 2000/2000 - Loss: -7.953
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[19.3708, 10.3662, 36.5044, 12.7396, 10.7532, 49.2481]],

        [[22.8360, 40.3540, 12.3633,  1.2496,  2.2564, 12.3727]],

        [[25.1588, 43.1712, 11.4096,  1.0476,  6.4472, 15.5014]],

        [[21.6564, 40.9012, 14.5758,  3.8281,  1.2527, 39.5098]]])
Signal Variance: tensor([ 0.1598,  0.9168, 13.6914,  0.3497])
Estimated target variance: tensor([0.0147, 0.2745, 2.7552, 0.0261])
N: 130
Signal to noise ratio: tensor([22.9134, 48.8819, 78.0568, 35.9835])
Bound on condition number: tensor([ 68254.2544, 310628.2473, 792073.9806, 168326.9818])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.009724021670255562, policy loss: 4.914285881603411
Experience 13, Iter 1, disc loss: 0.009198539430145524, policy loss: 5.011751214602431
Experience 13, Iter 2, disc loss: 0.009733018496425641, policy loss: 4.9334748110285
Experience 13, Iter 3, disc loss: 0.010198498419763568, policy loss: 4.852379975271484
Experience 13, Iter 4, disc loss: 0.009994262525322484, policy loss: 4.898668595985058
Experience 13, Iter 5, disc loss: 0.008919121401351435, policy loss: 5.067863094783995
Experience 13, Iter 6, disc loss: 0.010008595099403956, policy loss: 4.878091496332348
Experience 13, Iter 7, disc loss: 0.009461720143783936, policy loss: 4.968379253276632
Experience 13, Iter 8, disc loss: 0.009876845876065417, policy loss: 4.921790998413932
Experience 13, Iter 9, disc loss: 0.010075933315528942, policy loss: 4.868536603638825
Experience 13, Iter 10, disc loss: 0.009281249282759004, policy loss: 5.011524300801672
Experience 13, Iter 11, disc loss: 0.009431520357003204, policy loss: 4.9783335070943115
Experience 13, Iter 12, disc loss: 0.010333797063060056, policy loss: 4.809369342097147
Experience 13, Iter 13, disc loss: 0.009250210944094074, policy loss: 5.0291974574982525
Experience 13, Iter 14, disc loss: 0.009842574338479525, policy loss: 4.8838752952662094
Experience 13, Iter 15, disc loss: 0.010245859864549152, policy loss: 4.836348852591615
Experience 13, Iter 16, disc loss: 0.010278769758483871, policy loss: 4.845717413488632
Experience 13, Iter 17, disc loss: 0.010006006284459483, policy loss: 4.9471407830200045
Experience 13, Iter 18, disc loss: 0.009662645925902684, policy loss: 4.91412094850257
Experience 13, Iter 19, disc loss: 0.010147681253272506, policy loss: 4.8364997006626425
Experience 13, Iter 20, disc loss: 0.010067967607321792, policy loss: 4.875432793830182
Experience 13, Iter 21, disc loss: 0.010484293411121608, policy loss: 4.817729029370321
Experience 13, Iter 22, disc loss: 0.00991982333468748, policy loss: 4.890218549409832
Experience 13, Iter 23, disc loss: 0.00941866092586356, policy loss: 4.982484903625292
Experience 13, Iter 24, disc loss: 0.009632892474898114, policy loss: 4.961706642177519
Experience 13, Iter 25, disc loss: 0.010188030589379952, policy loss: 4.874780881955459
Experience 13, Iter 26, disc loss: 0.010336550128545003, policy loss: 4.851488707170556
Experience 13, Iter 27, disc loss: 0.009551940349237653, policy loss: 5.002676527974287
Experience 13, Iter 28, disc loss: 0.009899552735188336, policy loss: 4.881001687843073
Experience 13, Iter 29, disc loss: 0.00965159536280915, policy loss: 4.918503988901462
Experience 13, Iter 30, disc loss: 0.009140308350345686, policy loss: 5.013928300987576
Experience 13, Iter 31, disc loss: 0.009730088851887293, policy loss: 4.903711298945629
Experience 13, Iter 32, disc loss: 0.010212022295228381, policy loss: 4.853599372371346
Experience 13, Iter 33, disc loss: 0.008535882037228512, policy loss: 5.079337965531404
Experience 13, Iter 34, disc loss: 0.00911042434227095, policy loss: 5.0080156347326685
Experience 13, Iter 35, disc loss: 0.009378231103026964, policy loss: 5.003894289106637
Experience 13, Iter 36, disc loss: 0.009092230416221262, policy loss: 4.997129547831162
Experience 13, Iter 37, disc loss: 0.009244146369084192, policy loss: 5.020041519637985
Experience 13, Iter 38, disc loss: 0.008310150844155168, policy loss: 5.16338215096374
Experience 13, Iter 39, disc loss: 0.009309884877086207, policy loss: 4.990870688947846
Experience 13, Iter 40, disc loss: 0.008900831287420847, policy loss: 5.073800604547777
Experience 13, Iter 41, disc loss: 0.008327557375754937, policy loss: 5.19001762747139
Experience 13, Iter 42, disc loss: 0.00827583863687255, policy loss: 5.158159208638974
Experience 13, Iter 43, disc loss: 0.00953445507154969, policy loss: 4.981259137816238
Experience 13, Iter 44, disc loss: 0.007527722425083748, policy loss: 5.268442573292814
Experience 13, Iter 45, disc loss: 0.009313249027275056, policy loss: 4.991547814890521
Experience 13, Iter 46, disc loss: 0.008945810903717481, policy loss: 5.052405398199346
Experience 13, Iter 47, disc loss: 0.008762639150261424, policy loss: 5.050066270612771
Experience 13, Iter 48, disc loss: 0.008778090818056573, policy loss: 5.031555231106202
Experience 13, Iter 49, disc loss: 0.009135076256346126, policy loss: 4.99473832659401
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0035],
        [0.0716],
        [0.7292],
        [0.0070]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.2270e-02, 1.3680e-01, 3.5320e-01, 1.1891e-02, 1.4413e-03,
          1.7864e+00]],

        [[2.2270e-02, 1.3680e-01, 3.5320e-01, 1.1891e-02, 1.4413e-03,
          1.7864e+00]],

        [[2.2270e-02, 1.3680e-01, 3.5320e-01, 1.1891e-02, 1.4413e-03,
          1.7864e+00]],

        [[2.2270e-02, 1.3680e-01, 3.5320e-01, 1.1891e-02, 1.4413e-03,
          1.7864e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0141, 0.2865, 2.9166, 0.0282], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0141, 0.2865, 2.9166, 0.0282])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.766
Iter 2/2000 - Loss: 1.819
Iter 3/2000 - Loss: 1.666
Iter 4/2000 - Loss: 1.635
Iter 5/2000 - Loss: 1.689
Iter 6/2000 - Loss: 1.634
Iter 7/2000 - Loss: 1.547
Iter 8/2000 - Loss: 1.525
Iter 9/2000 - Loss: 1.528
Iter 10/2000 - Loss: 1.483
Iter 11/2000 - Loss: 1.403
Iter 12/2000 - Loss: 1.331
Iter 13/2000 - Loss: 1.274
Iter 14/2000 - Loss: 1.204
Iter 15/2000 - Loss: 1.103
Iter 16/2000 - Loss: 0.978
Iter 17/2000 - Loss: 0.847
Iter 18/2000 - Loss: 0.714
Iter 19/2000 - Loss: 0.567
Iter 20/2000 - Loss: 0.398
Iter 1981/2000 - Loss: -8.033
Iter 1982/2000 - Loss: -8.033
Iter 1983/2000 - Loss: -8.033
Iter 1984/2000 - Loss: -8.033
Iter 1985/2000 - Loss: -8.033
Iter 1986/2000 - Loss: -8.033
Iter 1987/2000 - Loss: -8.033
Iter 1988/2000 - Loss: -8.033
Iter 1989/2000 - Loss: -8.033
Iter 1990/2000 - Loss: -8.033
Iter 1991/2000 - Loss: -8.033
Iter 1992/2000 - Loss: -8.033
Iter 1993/2000 - Loss: -8.033
Iter 1994/2000 - Loss: -8.033
Iter 1995/2000 - Loss: -8.033
Iter 1996/2000 - Loss: -8.033
Iter 1997/2000 - Loss: -8.033
Iter 1998/2000 - Loss: -8.033
Iter 1999/2000 - Loss: -8.033
Iter 2000/2000 - Loss: -8.034
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[18.8761, 10.0524, 36.4901, 13.7389,  9.9026, 50.8917]],

        [[22.1271, 40.8603, 12.1537,  1.2242,  3.7189, 13.4471]],

        [[25.0395, 44.4515, 12.6186,  1.0228,  6.2667, 15.2246]],

        [[20.5121, 39.8197, 16.2188,  3.9506,  1.2298, 40.7583]]])
Signal Variance: tensor([ 0.1415,  1.0586, 14.2162,  0.3957])
Estimated target variance: tensor([0.0141, 0.2865, 2.9166, 0.0282])
N: 140
Signal to noise ratio: tensor([21.2622, 54.3819, 80.9787, 38.0380])
Bound on condition number: tensor([ 63292.0849, 414036.3889, 918056.8696, 202565.3481])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.009350653176167826, policy loss: 4.991009282502395
Experience 14, Iter 1, disc loss: 0.008403152265522846, policy loss: 5.082089026332791
Experience 14, Iter 2, disc loss: 0.008632704000719558, policy loss: 5.064414356177191
Experience 14, Iter 3, disc loss: 0.008227550198771437, policy loss: 5.1098818966621895
Experience 14, Iter 4, disc loss: 0.008544244440189648, policy loss: 5.069175677159847
Experience 14, Iter 5, disc loss: 0.008542285562829235, policy loss: 5.0507255804704965
Experience 14, Iter 6, disc loss: 0.008762086588823239, policy loss: 5.025129755547281
Experience 14, Iter 7, disc loss: 0.008014759377346286, policy loss: 5.182841983980685
Experience 14, Iter 8, disc loss: 0.008629794395994594, policy loss: 5.073758696944176
Experience 14, Iter 9, disc loss: 0.00789253267653298, policy loss: 5.164032934309604
Experience 14, Iter 10, disc loss: 0.007897045442047496, policy loss: 5.2304417246399275
Experience 14, Iter 11, disc loss: 0.008746134558013301, policy loss: 5.076573785394683
Experience 14, Iter 12, disc loss: 0.008961523730642339, policy loss: 5.004341201619006
Experience 14, Iter 13, disc loss: 0.008681300066281834, policy loss: 5.165595037245725
Experience 14, Iter 14, disc loss: 0.008394057103963022, policy loss: 5.136305846873867
Experience 14, Iter 15, disc loss: 0.008450019965731023, policy loss: 5.062462627421746
Experience 14, Iter 16, disc loss: 0.008509680283131982, policy loss: 5.149133793816126
Experience 14, Iter 17, disc loss: 0.00868162969910689, policy loss: 5.07277178557876
Experience 14, Iter 18, disc loss: 0.007591811054226066, policy loss: 5.238572318426829
Experience 14, Iter 19, disc loss: 0.008485514461544611, policy loss: 5.093521753454689
Experience 14, Iter 20, disc loss: 0.008563299360237598, policy loss: 5.072292201774216
Experience 14, Iter 21, disc loss: 0.008056362551521565, policy loss: 5.163427632247027
Experience 14, Iter 22, disc loss: 0.008362907371618308, policy loss: 5.122609991788156
Experience 14, Iter 23, disc loss: 0.00757469595012963, policy loss: 5.241297624551331
Experience 14, Iter 24, disc loss: 0.00845467033105645, policy loss: 5.067373636089947
Experience 14, Iter 25, disc loss: 0.008077888761130423, policy loss: 5.1427501533727416
Experience 14, Iter 26, disc loss: 0.007662585316315716, policy loss: 5.288925986980775
Experience 14, Iter 27, disc loss: 0.007277096264631248, policy loss: 5.318812789262007
Experience 14, Iter 28, disc loss: 0.007959081281186818, policy loss: 5.154551384060565
Experience 14, Iter 29, disc loss: 0.007636440200274537, policy loss: 5.182532835352489
Experience 14, Iter 30, disc loss: 0.007715544107881937, policy loss: 5.20553113573064
Experience 14, Iter 31, disc loss: 0.007764083253920421, policy loss: 5.207383558166304
Experience 14, Iter 32, disc loss: 0.007330754526418451, policy loss: 5.282790392301948
Experience 14, Iter 33, disc loss: 0.007352298254893915, policy loss: 5.249520259438361
Experience 14, Iter 34, disc loss: 0.0075211325326815705, policy loss: 5.218714179924278
Experience 14, Iter 35, disc loss: 0.007534529879563486, policy loss: 5.219645732607559
Experience 14, Iter 36, disc loss: 0.007645348960229156, policy loss: 5.176678699656824
Experience 14, Iter 37, disc loss: 0.007258082173054361, policy loss: 5.253229072064753
Experience 14, Iter 38, disc loss: 0.007344941410197946, policy loss: 5.324928462732725
Experience 14, Iter 39, disc loss: 0.007423354743023514, policy loss: 5.242613758221868
Experience 14, Iter 40, disc loss: 0.006711315696734208, policy loss: 5.348234160030355
Experience 14, Iter 41, disc loss: 0.00699055168144026, policy loss: 5.30199959979146
Experience 14, Iter 42, disc loss: 0.006958739112397465, policy loss: 5.382569110527274
Experience 14, Iter 43, disc loss: 0.006633145796778484, policy loss: 5.403500706993701
Experience 14, Iter 44, disc loss: 0.006489244209009393, policy loss: 5.482869690021461
Experience 14, Iter 45, disc loss: 0.006269673627259304, policy loss: 5.428166218038768
Experience 14, Iter 46, disc loss: 0.006197532338685658, policy loss: 5.448498874905017
Experience 14, Iter 47, disc loss: 0.006085947643579663, policy loss: 5.477871952949673
Experience 14, Iter 48, disc loss: 0.0066960260591629696, policy loss: 5.331974834961128
Experience 14, Iter 49, disc loss: 0.006840499323680268, policy loss: 5.398827233889005
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0034],
        [0.0691],
        [0.6831],
        [0.0067]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.0799e-02, 1.2939e-01, 3.3215e-01, 1.1646e-02, 1.3653e-03,
          1.7639e+00]],

        [[2.0799e-02, 1.2939e-01, 3.3215e-01, 1.1646e-02, 1.3653e-03,
          1.7639e+00]],

        [[2.0799e-02, 1.2939e-01, 3.3215e-01, 1.1646e-02, 1.3653e-03,
          1.7639e+00]],

        [[2.0799e-02, 1.2939e-01, 3.3215e-01, 1.1646e-02, 1.3653e-03,
          1.7639e+00]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0135, 0.2764, 2.7323, 0.0266], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0135, 0.2764, 2.7323, 0.0266])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.681
Iter 2/2000 - Loss: 1.729
Iter 3/2000 - Loss: 1.577
Iter 4/2000 - Loss: 1.543
Iter 5/2000 - Loss: 1.599
Iter 6/2000 - Loss: 1.549
Iter 7/2000 - Loss: 1.460
Iter 8/2000 - Loss: 1.434
Iter 9/2000 - Loss: 1.440
Iter 10/2000 - Loss: 1.401
Iter 11/2000 - Loss: 1.322
Iter 12/2000 - Loss: 1.249
Iter 13/2000 - Loss: 1.191
Iter 14/2000 - Loss: 1.123
Iter 15/2000 - Loss: 1.022
Iter 16/2000 - Loss: 0.896
Iter 17/2000 - Loss: 0.763
Iter 18/2000 - Loss: 0.626
Iter 19/2000 - Loss: 0.478
Iter 20/2000 - Loss: 0.307
Iter 1981/2000 - Loss: -8.080
Iter 1982/2000 - Loss: -8.080
Iter 1983/2000 - Loss: -8.080
Iter 1984/2000 - Loss: -8.080
Iter 1985/2000 - Loss: -8.080
Iter 1986/2000 - Loss: -8.080
Iter 1987/2000 - Loss: -8.080
Iter 1988/2000 - Loss: -8.080
Iter 1989/2000 - Loss: -8.080
Iter 1990/2000 - Loss: -8.080
Iter 1991/2000 - Loss: -8.080
Iter 1992/2000 - Loss: -8.080
Iter 1993/2000 - Loss: -8.080
Iter 1994/2000 - Loss: -8.081
Iter 1995/2000 - Loss: -8.081
Iter 1996/2000 - Loss: -8.081
Iter 1997/2000 - Loss: -8.081
Iter 1998/2000 - Loss: -8.081
Iter 1999/2000 - Loss: -8.081
Iter 2000/2000 - Loss: -8.081
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0004],
        [0.0022],
        [0.0003]])
Lengthscale: tensor([[[18.3250,  9.9252, 35.9389, 12.2227,  9.9703, 51.0378]],

        [[21.7950, 40.1360, 12.0580,  1.2418,  4.0557, 14.8702]],

        [[24.7533, 43.9008, 12.4602,  1.0290,  5.7438, 16.3954]],

        [[19.6849, 38.5844, 16.0429,  4.1721,  1.2203, 39.1768]]])
Signal Variance: tensor([ 0.1410,  1.1897, 14.9880,  0.3871])
Estimated target variance: tensor([0.0135, 0.2764, 2.7323, 0.0266])
N: 150
Signal to noise ratio: tensor([20.9863, 56.8690, 82.0836, 37.0811])
Bound on condition number: tensor([  66064.4110,  485112.7728, 1010658.9396,  206252.2161])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.006447454066434544, policy loss: 5.416477001303868
Experience 15, Iter 1, disc loss: 0.006667698414434979, policy loss: 5.404835860377963
Experience 15, Iter 2, disc loss: 0.006088580804429235, policy loss: 5.451024142970503
Experience 15, Iter 3, disc loss: 0.005622392765234961, policy loss: 5.567342418591828
Experience 15, Iter 4, disc loss: 0.005383075522802456, policy loss: 5.695665802789506
Experience 15, Iter 5, disc loss: 0.005330165468225093, policy loss: 5.725832959545619
Experience 15, Iter 6, disc loss: 0.005489460884140371, policy loss: 5.65311564926559
Experience 15, Iter 7, disc loss: 0.0058069071212957886, policy loss: 5.51460836693671
Experience 15, Iter 8, disc loss: 0.006255770338166345, policy loss: 5.490236439696556
Experience 15, Iter 9, disc loss: 0.005434601592917709, policy loss: 5.662581055358402
Experience 15, Iter 10, disc loss: 0.005378281347308288, policy loss: 5.640653294656869
Experience 15, Iter 11, disc loss: 0.006401608372661081, policy loss: 5.413087218138418
Experience 15, Iter 12, disc loss: 0.006088506952211721, policy loss: 5.469451921642691
Experience 15, Iter 13, disc loss: 0.005757007776354799, policy loss: 5.594132342741324
Experience 15, Iter 14, disc loss: 0.004821301196916519, policy loss: 5.85132970453785
Experience 15, Iter 15, disc loss: 0.005109541270122222, policy loss: 5.805975574468675
Experience 15, Iter 16, disc loss: 0.005644302185299071, policy loss: 5.567591154265905
Experience 15, Iter 17, disc loss: 0.006228217193508006, policy loss: 5.420348886355286
Experience 15, Iter 18, disc loss: 0.005613895454689947, policy loss: 5.618306273789439
Experience 15, Iter 19, disc loss: 0.004310966921810669, policy loss: 6.121256544651262
Experience 15, Iter 20, disc loss: 0.004229103774588509, policy loss: 6.096422277177713
Experience 15, Iter 21, disc loss: 0.004875388617507872, policy loss: 5.851224847287098
Experience 15, Iter 22, disc loss: 0.0052383514415457475, policy loss: 5.627648972312304
Experience 15, Iter 23, disc loss: 0.005135870595676105, policy loss: 5.663258158094518
Experience 15, Iter 24, disc loss: 0.0047835457042073835, policy loss: 5.810500176412759
Experience 15, Iter 25, disc loss: 0.00443729084296981, policy loss: 5.91046295226161
Experience 15, Iter 26, disc loss: 0.004321262588185656, policy loss: 5.921219181373342
Experience 15, Iter 27, disc loss: 0.004853399531247142, policy loss: 5.72304860666595
Experience 15, Iter 28, disc loss: 0.005258600139469254, policy loss: 5.656548345152209
Experience 15, Iter 29, disc loss: 0.004678121381041745, policy loss: 5.751886874714444
Experience 15, Iter 30, disc loss: 0.005346024352452131, policy loss: 5.5584089990686145
Experience 15, Iter 31, disc loss: 0.005106157132352778, policy loss: 5.702891643916011
Experience 15, Iter 32, disc loss: 0.004841617230599895, policy loss: 5.7045574129722265
Experience 15, Iter 33, disc loss: 0.004661758950401896, policy loss: 5.97920333604291
Experience 15, Iter 34, disc loss: 0.0047990784351007705, policy loss: 5.75283280639143
Experience 15, Iter 35, disc loss: 0.004770839538325276, policy loss: 5.6817686585163765
Experience 15, Iter 36, disc loss: 0.005227095234433925, policy loss: 5.573062195315297
Experience 15, Iter 37, disc loss: 0.005408356468343493, policy loss: 5.509374479906885
Experience 15, Iter 38, disc loss: 0.005551238514656315, policy loss: 5.5069058795716845
Experience 15, Iter 39, disc loss: 0.005106811121539519, policy loss: 5.585226066856784
Experience 15, Iter 40, disc loss: 0.005066395205034192, policy loss: 5.781840738591678
Experience 15, Iter 41, disc loss: 0.005492045392740855, policy loss: 5.48739446921304
Experience 15, Iter 42, disc loss: 0.005198461316197578, policy loss: 5.606523150379957
Experience 15, Iter 43, disc loss: 0.005132563437157667, policy loss: 5.6233957506764325
Experience 15, Iter 44, disc loss: 0.005179983414568672, policy loss: 5.588675621306771
Experience 15, Iter 45, disc loss: 0.005526757163491316, policy loss: 5.483384951112647
Experience 15, Iter 46, disc loss: 0.005632243663182592, policy loss: 5.469395968998696
Experience 15, Iter 47, disc loss: 0.00591899504137086, policy loss: 5.426138764518404
Experience 15, Iter 48, disc loss: 0.005817297284340239, policy loss: 5.424425148944376
Experience 15, Iter 49, disc loss: 0.0058514023422134055, policy loss: 5.466880912749768
