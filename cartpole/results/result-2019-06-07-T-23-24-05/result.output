Experience: 1
***BEFORE OPTIMATION***
Noise Variance: tensor([[1.1727e-04],
        [1.1183e-04],
        [1.9543e-03],
        [6.0508e-05]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0004, 0.0078, 0.0002], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0004, 0.0078, 0.0002])
N: 10
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([41.0000, 41.0000, 41.0000, 41.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.581
Iter 2/2000 - Loss: 13.831
Iter 3/2000 - Loss: -5.064
Iter 4/2000 - Loss: -4.386
Iter 5/2000 - Loss: 1.872
Iter 6/2000 - Loss: 0.245
Iter 7/2000 - Loss: -4.804
Iter 8/2000 - Loss: -7.739
Iter 9/2000 - Loss: -7.152
Iter 10/2000 - Loss: -5.055
Iter 11/2000 - Loss: -3.931
Iter 12/2000 - Loss: -4.545
Iter 13/2000 - Loss: -6.106
Iter 14/2000 - Loss: -7.452
Iter 15/2000 - Loss: -7.923
Iter 16/2000 - Loss: -7.578
Iter 17/2000 - Loss: -6.915
Iter 18/2000 - Loss: -6.447
Iter 19/2000 - Loss: -6.425
Iter 20/2000 - Loss: -6.798
Iter 1981/2000 - Loss: -8.769
Iter 1982/2000 - Loss: -8.793
Iter 1983/2000 - Loss: -8.771
Iter 1984/2000 - Loss: -8.785
Iter 1985/2000 - Loss: -8.808
Iter 1986/2000 - Loss: -8.793
Iter 1987/2000 - Loss: -8.778
Iter 1988/2000 - Loss: -8.789
Iter 1989/2000 - Loss: -8.804
Iter 1990/2000 - Loss: -8.805
Iter 1991/2000 - Loss: -8.791
Iter 1992/2000 - Loss: -8.773
Iter 1993/2000 - Loss: -8.755
Iter 1994/2000 - Loss: -8.741
Iter 1995/2000 - Loss: -8.737
Iter 1996/2000 - Loss: -8.751
Iter 1997/2000 - Loss: -8.778
Iter 1998/2000 - Loss: -8.792
Iter 1999/2000 - Loss: -8.775
Iter 2000/2000 - Loss: -8.751
***AFTER OPTIMATION***
Noise Variance: tensor([[9.4533e-05],
        [8.3171e-05],
        [1.4072e-03],
        [4.6244e-05]])
Lengthscale: tensor([[[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]],

        [[2.5824e-04, 2.1551e-03, 1.3917e-03, 5.6207e-05, 6.1215e-08,
          2.8030e-03]]])
Signal Variance: tensor([0.0004, 0.0003, 0.0056, 0.0002])
Estimated target variance: tensor([0.0005, 0.0004, 0.0078, 0.0002])
N: 10
Signal to noise ratio: tensor([1.9777, 1.9661, 1.9999, 1.9381])
Bound on condition number: tensor([40.1117, 39.6549, 40.9975, 38.5627])
Policy Optimizer learning rate:
0.01
Experience 1, Iter 0, disc loss: 1.4853364255914285, policy loss: 0.7062380117761548
Experience 1, Iter 1, disc loss: 1.47437495901675, policy loss: 0.704849447934031
Experience 1, Iter 2, disc loss: 1.4624030929235858, policy loss: 0.7032041632329438
Experience 1, Iter 3, disc loss: 1.4486840845963869, policy loss: 0.7021053902664796
Experience 1, Iter 4, disc loss: 1.434368834235736, policy loss: 0.7009513628133801
Experience 1, Iter 5, disc loss: 1.4196755253237683, policy loss: 0.6997132658342538
Experience 1, Iter 6, disc loss: 1.4044012746447947, policy loss: 0.698705925412712
Experience 1, Iter 7, disc loss: 1.3892526738280846, policy loss: 0.6976103714351055
Experience 1, Iter 8, disc loss: 1.3741890105947445, policy loss: 0.6965176338104787
Experience 1, Iter 9, disc loss: 1.3593175432734137, policy loss: 0.695526494109894
Experience 1, Iter 10, disc loss: 1.3440198783803798, policy loss: 0.6952266864830123
Experience 1, Iter 11, disc loss: 1.3298905459653763, policy loss: 0.6940331626402314
Experience 1, Iter 12, disc loss: 1.315264070282602, policy loss: 0.6936434796123165
Experience 1, Iter 13, disc loss: 1.3011505814116286, policy loss: 0.6930170450167634
Experience 1, Iter 14, disc loss: 1.286919445251348, policy loss: 0.6927663328108229
Experience 1, Iter 15, disc loss: 1.272341317413707, policy loss: 0.6931238968354946
Experience 1, Iter 16, disc loss: 1.2587745160278423, policy loss: 0.6926693135793245
Experience 1, Iter 17, disc loss: 1.2446843944991457, policy loss: 0.6929397606322489
Experience 1, Iter 18, disc loss: 1.2307660091001733, policy loss: 0.6932012341306617
Experience 1, Iter 19, disc loss: 1.2168370354300297, policy loss: 0.6936501054034954
Experience 1, Iter 20, disc loss: 1.2033753422811657, policy loss: 0.6937589542059763
Experience 1, Iter 21, disc loss: 1.1894646994200506, policy loss: 0.6944665993647506
Experience 1, Iter 22, disc loss: 1.1755121666067012, policy loss: 0.6953356081563133
Experience 1, Iter 23, disc loss: 1.1622179167542286, policy loss: 0.6956826679410191
Experience 1, Iter 24, disc loss: 1.1487042505785556, policy loss: 0.6963173685965035
Experience 1, Iter 25, disc loss: 1.1355638566204753, policy loss: 0.6967277818256403
Experience 1, Iter 26, disc loss: 1.1215903585205398, policy loss: 0.6981196365548894
Experience 1, Iter 27, disc loss: 1.1086811548300655, policy loss: 0.6985393480464741
Experience 1, Iter 28, disc loss: 1.0949319875438897, policy loss: 0.6999666967355532
Experience 1, Iter 29, disc loss: 1.0830162742406175, policy loss: 0.6995853184053409
Experience 1, Iter 30, disc loss: 1.0699695231025388, policy loss: 0.7005961738081217
Experience 1, Iter 31, disc loss: 1.0574928285584682, policy loss: 0.7011504978445046
Experience 1, Iter 32, disc loss: 1.04456767536197, policy loss: 0.7023453014338881
Experience 1, Iter 33, disc loss: 1.0318112102175172, policy loss: 0.7035815647629694
Experience 1, Iter 34, disc loss: 1.020040390214659, policy loss: 0.7040432301017038
Experience 1, Iter 35, disc loss: 1.0073146966533644, policy loss: 0.7055124709869363
Experience 1, Iter 36, disc loss: 0.9955696443433326, policy loss: 0.705985994354247
Experience 1, Iter 37, disc loss: 0.9827632994053747, policy loss: 0.7076090166926143
Experience 1, Iter 38, disc loss: 0.9706219679342039, policy loss: 0.7088180122730288
Experience 1, Iter 39, disc loss: 0.9582908960720831, policy loss: 0.7104823816437205
Experience 1, Iter 40, disc loss: 0.9469672330498866, policy loss: 0.7113634578529531
Experience 1, Iter 41, disc loss: 0.9352246587488904, policy loss: 0.713016245484049
Experience 1, Iter 42, disc loss: 0.9234077868152033, policy loss: 0.7150039394386873
Experience 1, Iter 43, disc loss: 0.9129760757947292, policy loss: 0.7158426937672956
Experience 1, Iter 44, disc loss: 0.9014754953070354, policy loss: 0.718066867033413
Experience 1, Iter 45, disc loss: 0.8911884146094262, policy loss: 0.7192787211640156
Experience 1, Iter 46, disc loss: 0.8801404588986238, policy loss: 0.7214766148383602
Experience 1, Iter 47, disc loss: 0.870413063906718, policy loss: 0.7226565070494413
Experience 1, Iter 48, disc loss: 0.8601427556450867, policy loss: 0.7246341979523514
Experience 1, Iter 49, disc loss: 0.8509405916754836, policy loss: 0.7257747752187771
Experience: 2
***BEFORE OPTIMATION***
Noise Variance: tensor([[1.1465e-04],
        [9.9760e-05],
        [1.2963e-03],
        [7.9405e-05]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0004, 0.0052, 0.0003], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0004, 0.0052, 0.0003])
N: 20
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([81.0000, 81.0000, 81.0000, 81.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.517
Iter 2/2000 - Loss: 10.251
Iter 3/2000 - Loss: -4.443
Iter 4/2000 - Loss: -4.843
Iter 5/2000 - Loss: -0.080
Iter 6/2000 - Loss: -1.044
Iter 7/2000 - Loss: -4.854
Iter 8/2000 - Loss: -7.387
Iter 9/2000 - Loss: -7.290
Iter 10/2000 - Loss: -5.819
Iter 11/2000 - Loss: -4.843
Iter 12/2000 - Loss: -5.106
Iter 13/2000 - Loss: -6.181
Iter 14/2000 - Loss: -7.256
Iter 15/2000 - Loss: -7.776
Iter 16/2000 - Loss: -7.678
Iter 17/2000 - Loss: -7.264
Iter 18/2000 - Loss: -6.907
Iter 19/2000 - Loss: -6.822
Iter 20/2000 - Loss: -7.003
Iter 1981/2000 - Loss: -8.830
Iter 1982/2000 - Loss: -8.833
Iter 1983/2000 - Loss: -8.835
Iter 1984/2000 - Loss: -8.835
Iter 1985/2000 - Loss: -8.834
Iter 1986/2000 - Loss: -8.833
Iter 1987/2000 - Loss: -8.831
Iter 1988/2000 - Loss: -8.829
Iter 1989/2000 - Loss: -8.828
Iter 1990/2000 - Loss: -8.825
Iter 1991/2000 - Loss: -8.821
Iter 1992/2000 - Loss: -8.815
Iter 1993/2000 - Loss: -8.806
Iter 1994/2000 - Loss: -8.796
Iter 1995/2000 - Loss: -8.789
Iter 1996/2000 - Loss: -8.792
Iter 1997/2000 - Loss: -8.809
Iter 1998/2000 - Loss: -8.831
Iter 1999/2000 - Loss: -8.839
Iter 2000/2000 - Loss: -8.829
***AFTER OPTIMATION***
Noise Variance: tensor([[9.7338e-05],
        [7.7764e-05],
        [9.8566e-04],
        [6.2778e-05]])
Lengthscale: tensor([[[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]],

        [[5.5025e-04, 3.5936e-03, 1.0274e-03, 6.0640e-05, 1.5535e-07,
          2.5786e-03]]])
Signal Variance: tensor([0.0004, 0.0003, 0.0039, 0.0002])
Estimated target variance: tensor([0.0005, 0.0004, 0.0052, 0.0003])
N: 20
Signal to noise ratio: tensor([1.9832, 1.9684, 1.9994, 1.9616])
Bound on condition number: tensor([79.6595, 78.4957, 80.9551, 77.9608])
Policy Optimizer learning rate:
0.009989469496904544
Experience 2, Iter 0, disc loss: 0.8405386917194155, policy loss: 0.7284445014907959
Experience 2, Iter 1, disc loss: 0.830849082212667, policy loss: 0.7306662828243076
Experience 2, Iter 2, disc loss: 0.8218728429012668, policy loss: 0.7323751892663302
Experience 2, Iter 3, disc loss: 0.8120527867339258, policy loss: 0.7353663472896854
Experience 2, Iter 4, disc loss: 0.8037138876991483, policy loss: 0.7369780951722342
Experience 2, Iter 5, disc loss: 0.7943818257711406, policy loss: 0.7399743732020649
Experience 2, Iter 6, disc loss: 0.7867494576728844, policy loss: 0.7414194246420169
Experience 2, Iter 7, disc loss: 0.7786116633067741, policy loss: 0.7436524500677848
Experience 2, Iter 8, disc loss: 0.7702562119272642, policy loss: 0.7464730418940504
Experience 2, Iter 9, disc loss: 0.762563529862458, policy loss: 0.7486684801656229
Experience 2, Iter 10, disc loss: 0.7554261105823729, policy loss: 0.7504513128110624
Experience 2, Iter 11, disc loss: 0.74653012990056, policy loss: 0.7543775394039283
Experience 2, Iter 12, disc loss: 0.7414867545119285, policy loss: 0.7541353030709794
Experience 2, Iter 13, disc loss: 0.7327412685456842, policy loss: 0.7583648471203441
Experience 2, Iter 14, disc loss: 0.7265018686666653, policy loss: 0.7599844823865834
Experience 2, Iter 15, disc loss: 0.7193383609562483, policy loss: 0.7629136719443347
Experience 2, Iter 16, disc loss: 0.7136728238365176, policy loss: 0.764304346975499
Experience 2, Iter 17, disc loss: 0.7065018965059127, policy loss: 0.7678019607999598
Experience 2, Iter 18, disc loss: 0.6997919321554404, policy loss: 0.7708828524797566
Experience 2, Iter 19, disc loss: 0.6956569305354607, policy loss: 0.7712986284854733
Experience 2, Iter 20, disc loss: 0.688861297967432, policy loss: 0.7750672788472482
Experience 2, Iter 21, disc loss: 0.6840757284069476, policy loss: 0.7766375128558247
Experience 2, Iter 22, disc loss: 0.6779903140535997, policy loss: 0.7800690692879738
Experience 2, Iter 23, disc loss: 0.6740232787238142, policy loss: 0.7811002639229685
Experience 2, Iter 24, disc loss: 0.6676202140168792, policy loss: 0.7853968823103353
Experience 2, Iter 25, disc loss: 0.6647275401851677, policy loss: 0.7854899688075309
Experience 2, Iter 26, disc loss: 0.6597145836137595, policy loss: 0.7884605621128993
Experience 2, Iter 27, disc loss: 0.654887098356204, policy loss: 0.7913343951812373
Experience 2, Iter 28, disc loss: 0.6502875132681066, policy loss: 0.7941367304871807
Experience 2, Iter 29, disc loss: 0.6469029788389848, policy loss: 0.7955641995264031
Experience 2, Iter 30, disc loss: 0.6436525627428245, policy loss: 0.7969442909739699
Experience 2, Iter 31, disc loss: 0.6383809808609526, policy loss: 0.8010222419051567
Experience 2, Iter 32, disc loss: 0.634064470427695, policy loss: 0.8039626190885127
Experience 2, Iter 33, disc loss: 0.6301046320592619, policy loss: 0.8066859872661886
Experience 2, Iter 34, disc loss: 0.6286419666577463, policy loss: 0.806380381457765
Experience 2, Iter 35, disc loss: 0.6240556581529151, policy loss: 0.8100416898542945
Experience 2, Iter 36, disc loss: 0.6198607054819326, policy loss: 0.8135129886460631
Experience 2, Iter 37, disc loss: 0.6186829141247633, policy loss: 0.8131453315494076
Experience 2, Iter 38, disc loss: 0.6139911863247502, policy loss: 0.8174620496729929
Experience 2, Iter 39, disc loss: 0.6121998715749729, policy loss: 0.8181448196769898
Experience 2, Iter 40, disc loss: 0.6080121339935263, policy loss: 0.8220759370621449
Experience 2, Iter 41, disc loss: 0.6079585895094892, policy loss: 0.8205952635858681
Experience 2, Iter 42, disc loss: 0.6041159906801982, policy loss: 0.8242003257098628
Experience 2, Iter 43, disc loss: 0.6004382581671932, policy loss: 0.8276875764438076
Experience 2, Iter 44, disc loss: 0.5980846960910626, policy loss: 0.8295243278610253
Experience 2, Iter 45, disc loss: 0.5955461327662355, policy loss: 0.8317011628355238
Experience 2, Iter 46, disc loss: 0.5927245941371829, policy loss: 0.8342971892280702
Experience 2, Iter 47, disc loss: 0.5892095617660063, policy loss: 0.8378561816428631
Experience 2, Iter 48, disc loss: 0.5878412573657884, policy loss: 0.838652531147219
Experience 2, Iter 49, disc loss: 0.5832724199572039, policy loss: 0.8437961207034883
Experience: 3
***BEFORE OPTIMATION***
Noise Variance: tensor([[1.2610e-04],
        [1.4582e-04],
        [1.5682e-03],
        [9.1117e-05]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0005, 0.0006, 0.0063, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0005, 0.0006, 0.0063, 0.0004])
N: 30
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([121.0000, 121.0000, 121.0000, 121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.754
Iter 2/2000 - Loss: 7.167
Iter 3/2000 - Loss: -4.773
Iter 4/2000 - Loss: -5.114
Iter 5/2000 - Loss: -1.207
Iter 6/2000 - Loss: -2.034
Iter 7/2000 - Loss: -5.177
Iter 8/2000 - Loss: -7.194
Iter 9/2000 - Loss: -7.040
Iter 10/2000 - Loss: -5.804
Iter 11/2000 - Loss: -5.039
Iter 12/2000 - Loss: -5.315
Iter 13/2000 - Loss: -6.220
Iter 14/2000 - Loss: -7.075
Iter 15/2000 - Loss: -7.453
Iter 16/2000 - Loss: -7.337
Iter 17/2000 - Loss: -6.992
Iter 18/2000 - Loss: -6.725
Iter 19/2000 - Loss: -6.687
Iter 20/2000 - Loss: -6.856
Iter 1981/2000 - Loss: -8.405
Iter 1982/2000 - Loss: -8.405
Iter 1983/2000 - Loss: -8.405
Iter 1984/2000 - Loss: -8.405
Iter 1985/2000 - Loss: -8.405
Iter 1986/2000 - Loss: -8.405
Iter 1987/2000 - Loss: -8.405
Iter 1988/2000 - Loss: -8.405
Iter 1989/2000 - Loss: -8.405
Iter 1990/2000 - Loss: -8.405
Iter 1991/2000 - Loss: -8.405
Iter 1992/2000 - Loss: -8.405
Iter 1993/2000 - Loss: -8.404
Iter 1994/2000 - Loss: -8.404
Iter 1995/2000 - Loss: -8.403
Iter 1996/2000 - Loss: -8.403
Iter 1997/2000 - Loss: -8.402
Iter 1998/2000 - Loss: -8.401
Iter 1999/2000 - Loss: -8.399
Iter 2000/2000 - Loss: -8.397
***AFTER OPTIMATION***
Noise Variance: tensor([[9.9372e-05],
        [1.1473e-04],
        [1.2126e-03],
        [7.2552e-05]])
Lengthscale: tensor([[[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]],

        [[1.2314e-03, 4.5175e-03, 1.4673e-03, 7.5947e-05, 2.2063e-07,
          3.6074e-03]]])
Signal Variance: tensor([0.0004, 0.0004, 0.0049, 0.0003])
Estimated target variance: tensor([0.0005, 0.0006, 0.0063, 0.0004])
N: 30
Signal to noise ratio: tensor([1.9770, 1.9803, 2.0001, 1.9684])
Bound on condition number: tensor([118.2505, 118.6426, 121.0171, 117.2426])
Policy Optimizer learning rate:
0.009978950082958633
Experience 3, Iter 0, disc loss: 0.5828320282651092, policy loss: 0.8434768487419796
Experience 3, Iter 1, disc loss: 0.5791704511200655, policy loss: 0.8474665992663156
Experience 3, Iter 2, disc loss: 0.5748639859984256, policy loss: 0.8525200681782211
Experience 3, Iter 3, disc loss: 0.5735788338058528, policy loss: 0.8534965837055535
Experience 3, Iter 4, disc loss: 0.5692440917109627, policy loss: 0.85870200050766
Experience 3, Iter 5, disc loss: 0.5668983823231158, policy loss: 0.8612680243561117
Experience 3, Iter 6, disc loss: 0.5649296616648353, policy loss: 0.8632125800100214
Experience 3, Iter 7, disc loss: 0.5627181749725026, policy loss: 0.8656531889312759
Experience 3, Iter 8, disc loss: 0.5623147970481488, policy loss: 0.865530277023895
Experience 3, Iter 9, disc loss: 0.5575734782878538, policy loss: 0.8716373602750619
Experience 3, Iter 10, disc loss: 0.556406965056927, policy loss: 0.8727677246616434
Experience 3, Iter 11, disc loss: 0.5537193646617758, policy loss: 0.875907498662954
Experience 3, Iter 12, disc loss: 0.5524386121163911, policy loss: 0.8771333760824884
Experience 3, Iter 13, disc loss: 0.5482594878275066, policy loss: 0.8826136821964685
Experience 3, Iter 14, disc loss: 0.5424006639092265, policy loss: 0.8906976185707731
Experience 3, Iter 15, disc loss: 0.5424406301049404, policy loss: 0.8901170524967044
Experience 3, Iter 16, disc loss: 0.5393834665231959, policy loss: 0.8940232040830648
Experience 3, Iter 17, disc loss: 0.5365211435362102, policy loss: 0.8978441668209599
Experience 3, Iter 18, disc loss: 0.5347778629012099, policy loss: 0.9000270390408277
Experience 3, Iter 19, disc loss: 0.527204368121603, policy loss: 0.9109249460093265
Experience 3, Iter 20, disc loss: 0.5284119627254604, policy loss: 0.9086740095853008
Experience 3, Iter 21, disc loss: 0.5226662574375577, policy loss: 0.9169896967181003
Experience 3, Iter 22, disc loss: 0.5200919762148994, policy loss: 0.9206333371541046
Experience 3, Iter 23, disc loss: 0.5164974914955375, policy loss: 0.9260138170167207
Experience 3, Iter 24, disc loss: 0.5127771297635623, policy loss: 0.9312952908093297
Experience 3, Iter 25, disc loss: 0.5152666945020927, policy loss: 0.9268436130281056
Experience 3, Iter 26, disc loss: 0.5089287721825703, policy loss: 0.936675597711443
Experience 3, Iter 27, disc loss: 0.5062249522375029, policy loss: 0.9404139172416377
Experience 3, Iter 28, disc loss: 0.5013318878920837, policy loss: 0.9481350354409694
Experience 3, Iter 29, disc loss: 0.5037401050080672, policy loss: 0.9437293066457392
Experience 3, Iter 30, disc loss: 0.5022094644642939, policy loss: 0.9457944854746496
Experience 3, Iter 31, disc loss: 0.4894575305083647, policy loss: 0.9664301076594488
Experience 3, Iter 32, disc loss: 0.49331554330157895, policy loss: 0.9597026496398187
Experience 3, Iter 33, disc loss: 0.4872250503691213, policy loss: 0.9696039532519763
Experience 3, Iter 34, disc loss: 0.4844989405894838, policy loss: 0.9738387898581873
Experience 3, Iter 35, disc loss: 0.4808560091814516, policy loss: 0.9801370425196567
Experience 3, Iter 36, disc loss: 0.4767213549204765, policy loss: 0.986458162901074
Experience 3, Iter 37, disc loss: 0.4758547859715305, policy loss: 0.9879237690892684
Experience 3, Iter 38, disc loss: 0.4697298975717029, policy loss: 0.9980973217738386
Experience 3, Iter 39, disc loss: 0.469737210434757, policy loss: 0.9982332904666841
Experience 3, Iter 40, disc loss: 0.46382154883958066, policy loss: 1.008062650639357
Experience 3, Iter 41, disc loss: 0.4618823090201213, policy loss: 1.0118951735719066
Experience 3, Iter 42, disc loss: 0.460035349671984, policy loss: 1.0146585557267016
Experience 3, Iter 43, disc loss: 0.4545660849993303, policy loss: 1.023967930829239
Experience 3, Iter 44, disc loss: 0.4547813747403851, policy loss: 1.0237484506662917
Experience 3, Iter 45, disc loss: 0.44693997974313643, policy loss: 1.038001269762915
Experience 3, Iter 46, disc loss: 0.4404354897635128, policy loss: 1.050027112308768
Experience 3, Iter 47, disc loss: 0.43748423558437793, policy loss: 1.055305517091158
Experience 3, Iter 48, disc loss: 0.4337594024510647, policy loss: 1.06200035127216
Experience 3, Iter 49, disc loss: 0.43177519796138075, policy loss: 1.065285165680181
Experience: 4
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0037],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0008, 0.0150, 0.0005], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0008, 0.0150, 0.0005])
N: 40
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([161.0000, 161.0000, 161.0000, 161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.137
Iter 2/2000 - Loss: 4.775
Iter 3/2000 - Loss: -5.012
Iter 4/2000 - Loss: -4.924
Iter 5/2000 - Loss: -1.721
Iter 6/2000 - Loss: -2.554
Iter 7/2000 - Loss: -5.131
Iter 8/2000 - Loss: -6.644
Iter 9/2000 - Loss: -6.387
Iter 10/2000 - Loss: -5.369
Iter 11/2000 - Loss: -4.824
Iter 12/2000 - Loss: -5.111
Iter 13/2000 - Loss: -5.839
Iter 14/2000 - Loss: -6.474
Iter 15/2000 - Loss: -6.729
Iter 16/2000 - Loss: -6.626
Iter 17/2000 - Loss: -6.359
Iter 18/2000 - Loss: -6.141
Iter 19/2000 - Loss: -6.095
Iter 20/2000 - Loss: -6.235
Iter 1981/2000 - Loss: -7.489
Iter 1982/2000 - Loss: -7.472
Iter 1983/2000 - Loss: -7.457
Iter 1984/2000 - Loss: -7.464
Iter 1985/2000 - Loss: -7.497
Iter 1986/2000 - Loss: -7.519
Iter 1987/2000 - Loss: -7.501
Iter 1988/2000 - Loss: -7.491
Iter 1989/2000 - Loss: -7.511
Iter 1990/2000 - Loss: -7.518
Iter 1991/2000 - Loss: -7.503
Iter 1992/2000 - Loss: -7.497
Iter 1993/2000 - Loss: -7.506
Iter 1994/2000 - Loss: -7.518
Iter 1995/2000 - Loss: -7.522
Iter 1996/2000 - Loss: -7.519
Iter 1997/2000 - Loss: -7.509
Iter 1998/2000 - Loss: -7.497
Iter 1999/2000 - Loss: -7.485
Iter 2000/2000 - Loss: -7.483
***AFTER OPTIMATION***
Noise Variance: tensor([[1.3531e-04],
        [1.6782e-04],
        [2.9145e-03],
        [8.9714e-05]])
Lengthscale: tensor([[[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]],

        [[1.2143e-03, 4.4631e-03, 3.0357e-03, 1.2495e-04, 2.0720e-07,
          4.2073e-03]]])
Signal Variance: tensor([0.0005, 0.0007, 0.0117, 0.0003])
Estimated target variance: tensor([0.0007, 0.0008, 0.0150, 0.0005])
N: 40
Signal to noise ratio: tensor([1.9838, 1.9891, 2.0043, 1.9750])
Bound on condition number: tensor([158.4262, 159.2553, 161.6919, 157.0314])
Policy Optimizer learning rate:
0.009968441746484834
Experience 4, Iter 0, disc loss: 0.4267762441831407, policy loss: 1.0770922710665787
Experience 4, Iter 1, disc loss: 0.41824675190375216, policy loss: 1.094116450026913
Experience 4, Iter 2, disc loss: 0.4008400610179537, policy loss: 1.1292471865232796
Experience 4, Iter 3, disc loss: 0.4059927080515688, policy loss: 1.1191617264308964
Experience 4, Iter 4, disc loss: 0.4089564774507934, policy loss: 1.1118028014519155
Experience 4, Iter 5, disc loss: 0.3940106486163545, policy loss: 1.1449331703411367
Experience 4, Iter 6, disc loss: 0.3944025000621519, policy loss: 1.1416590802440132
Experience 4, Iter 7, disc loss: 0.4075517438686122, policy loss: 1.1134767196450897
Experience 4, Iter 8, disc loss: 0.391088634504753, policy loss: 1.1490298682545712
Experience 4, Iter 9, disc loss: 0.3935992088109734, policy loss: 1.1444721557434756
Experience 4, Iter 10, disc loss: 0.37740209081264, policy loss: 1.181093918882073
Experience 4, Iter 11, disc loss: 0.3854133221422886, policy loss: 1.1616929631525426
Experience 4, Iter 12, disc loss: 0.3694415403256291, policy loss: 1.1995087337685457
Experience 4, Iter 13, disc loss: 0.36761528094195334, policy loss: 1.2020064720949633
Experience 4, Iter 14, disc loss: 0.3585432964482436, policy loss: 1.2235506294217247
Experience 4, Iter 15, disc loss: 0.35397881841203643, policy loss: 1.2356028927016678
Experience 4, Iter 16, disc loss: 0.3647898470737212, policy loss: 1.2107058972473954
Experience 4, Iter 17, disc loss: 0.34785037200619756, policy loss: 1.252275073000809
Experience 4, Iter 18, disc loss: 0.3441762219579141, policy loss: 1.259679546063066
Experience 4, Iter 19, disc loss: 0.3431401892758018, policy loss: 1.2624322286299925
Experience 4, Iter 20, disc loss: 0.3279972096707723, policy loss: 1.3017169437583225
Experience 4, Iter 21, disc loss: 0.33322194204492683, policy loss: 1.2899388070093
Experience 4, Iter 22, disc loss: 0.3302942890412694, policy loss: 1.2974009993913063
Experience 4, Iter 23, disc loss: 0.32885969747500265, policy loss: 1.3015973188118855
Experience 4, Iter 24, disc loss: 0.3143162462991875, policy loss: 1.3391419493112353
Experience 4, Iter 25, disc loss: 0.32277626966432144, policy loss: 1.3192977083589166
Experience 4, Iter 26, disc loss: 0.31974838202821243, policy loss: 1.3240202046602814
Experience 4, Iter 27, disc loss: 0.30358592936064716, policy loss: 1.3740440330559642
Experience 4, Iter 28, disc loss: 0.3083566023599954, policy loss: 1.360016588673786
Experience 4, Iter 29, disc loss: 0.29720010404750025, policy loss: 1.3893093481976115
Experience 4, Iter 30, disc loss: 0.29272982121974633, policy loss: 1.4057172631226176
Experience 4, Iter 31, disc loss: 0.28633642138873894, policy loss: 1.425702131560581
Experience 4, Iter 32, disc loss: 0.28964564663295356, policy loss: 1.4160508579887412
Experience 4, Iter 33, disc loss: 0.2841101559508831, policy loss: 1.4346244390803007
Experience 4, Iter 34, disc loss: 0.2748446523692957, policy loss: 1.4603092206241939
Experience 4, Iter 35, disc loss: 0.26739747894656957, policy loss: 1.4928105729358156
Experience 4, Iter 36, disc loss: 0.27360003640680813, policy loss: 1.4657841308280246
Experience 4, Iter 37, disc loss: 0.2624165109138915, policy loss: 1.5032737304327104
Experience 4, Iter 38, disc loss: 0.2551628492524828, policy loss: 1.5352866036079527
Experience 4, Iter 39, disc loss: 0.2492840209522195, policy loss: 1.5511551384776985
Experience 4, Iter 40, disc loss: 0.2584957326952265, policy loss: 1.51989927288068
Experience 4, Iter 41, disc loss: 0.2461628588820559, policy loss: 1.5634131935142732
Experience 4, Iter 42, disc loss: 0.2328591198903414, policy loss: 1.6246112300305993
Experience 4, Iter 43, disc loss: 0.23631995492929092, policy loss: 1.6016086133454566
Experience 4, Iter 44, disc loss: 0.24393993641146308, policy loss: 1.5719714529299869
Experience 4, Iter 45, disc loss: 0.23761968195846908, policy loss: 1.5969772833086635
Experience 4, Iter 46, disc loss: 0.22561976839648448, policy loss: 1.6504824158318487
Experience 4, Iter 47, disc loss: 0.22909341775788883, policy loss: 1.6289619016323231
Experience 4, Iter 48, disc loss: 0.22760146401383002, policy loss: 1.6374625018747664
Experience 4, Iter 49, disc loss: 0.21948315110554523, policy loss: 1.6700273806226347
Experience: 5
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0035],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0009, 0.0139, 0.0005], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0009, 0.0139, 0.0005])
N: 50
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([201.0000, 201.0000, 201.0000, 201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.658
Iter 2/2000 - Loss: 4.657
Iter 3/2000 - Loss: -5.382
Iter 4/2000 - Loss: -5.084
Iter 5/2000 - Loss: -1.724
Iter 6/2000 - Loss: -2.552
Iter 7/2000 - Loss: -5.214
Iter 8/2000 - Loss: -6.766
Iter 9/2000 - Loss: -6.455
Iter 10/2000 - Loss: -5.343
Iter 11/2000 - Loss: -4.741
Iter 12/2000 - Loss: -5.052
Iter 13/2000 - Loss: -5.860
Iter 14/2000 - Loss: -6.553
Iter 15/2000 - Loss: -6.790
Iter 16/2000 - Loss: -6.606
Iter 17/2000 - Loss: -6.262
Iter 18/2000 - Loss: -6.029
Iter 19/2000 - Loss: -6.043
Iter 20/2000 - Loss: -6.266
Iter 1981/2000 - Loss: -7.455
Iter 1982/2000 - Loss: -7.456
Iter 1983/2000 - Loss: -7.457
Iter 1984/2000 - Loss: -7.458
Iter 1985/2000 - Loss: -7.459
Iter 1986/2000 - Loss: -7.460
Iter 1987/2000 - Loss: -7.459
Iter 1988/2000 - Loss: -7.459
Iter 1989/2000 - Loss: -7.458
Iter 1990/2000 - Loss: -7.457
Iter 1991/2000 - Loss: -7.456
Iter 1992/2000 - Loss: -7.454
Iter 1993/2000 - Loss: -7.454
Iter 1994/2000 - Loss: -7.453
Iter 1995/2000 - Loss: -7.452
Iter 1996/2000 - Loss: -7.452
Iter 1997/2000 - Loss: -7.452
Iter 1998/2000 - Loss: -7.451
Iter 1999/2000 - Loss: -7.449
Iter 2000/2000 - Loss: -7.447
***AFTER OPTIMATION***
Noise Variance: tensor([[1.4075e-04],
        [1.7541e-04],
        [2.7066e-03],
        [9.7472e-05]])
Lengthscale: tensor([[[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]],

        [[9.8120e-04, 5.4621e-03, 2.8485e-03, 1.2747e-04, 1.9768e-07,
          4.7197e-03]]])
Signal Variance: tensor([0.0006, 0.0007, 0.0109, 0.0004])
Estimated target variance: tensor([0.0007, 0.0009, 0.0139, 0.0005])
N: 50
Signal to noise ratio: tensor([1.9852, 1.9880, 2.0039, 1.9778])
Bound on condition number: tensor([198.0426, 198.6108, 201.7738, 196.5935])
Policy Optimizer learning rate:
0.00995794447581801
Experience 5, Iter 0, disc loss: 0.23294453247176322, policy loss: 1.6103558427383975
Experience 5, Iter 1, disc loss: 0.22303878008891243, policy loss: 1.6582510557647017
Experience 5, Iter 2, disc loss: 0.23427515748240602, policy loss: 1.6134261431543804
Experience 5, Iter 3, disc loss: 0.22852857488977668, policy loss: 1.645198182268405
Experience 5, Iter 4, disc loss: 0.21002009629512172, policy loss: 1.7078300800265473
Experience 5, Iter 5, disc loss: 0.22176672159857028, policy loss: 1.669674981220654
Experience 5, Iter 6, disc loss: 0.21931543581712074, policy loss: 1.675192906810386
Experience 5, Iter 7, disc loss: 0.20812150938196017, policy loss: 1.7123478080193348
Experience 5, Iter 8, disc loss: 0.21220343095807967, policy loss: 1.7052360284672596
Experience 5, Iter 9, disc loss: 0.20535428197821415, policy loss: 1.7318313817725695
Experience 5, Iter 10, disc loss: 0.20838265588203936, policy loss: 1.7246696515144269
Experience 5, Iter 11, disc loss: 0.19919333790490543, policy loss: 1.7644223742896004
Experience 5, Iter 12, disc loss: 0.19826293794300145, policy loss: 1.7739560483508483
Experience 5, Iter 13, disc loss: 0.2104904490760115, policy loss: 1.719215460995478
Experience 5, Iter 14, disc loss: 0.1754352984056251, policy loss: 1.888751915446502
Experience 5, Iter 15, disc loss: 0.18067448415216955, policy loss: 1.859604707425195
Experience 5, Iter 16, disc loss: 0.18271300105805555, policy loss: 1.8498795824781729
Experience 5, Iter 17, disc loss: 0.18679850241954238, policy loss: 1.821229179880663
Experience 5, Iter 18, disc loss: 0.1655250211827094, policy loss: 1.9445560763897358
Experience 5, Iter 19, disc loss: 0.17170693103201073, policy loss: 1.9128942575234538
Experience 5, Iter 20, disc loss: 0.1696342155549285, policy loss: 1.9213738971717933
Experience 5, Iter 21, disc loss: 0.17729270604693637, policy loss: 1.887627808993399
Experience 5, Iter 22, disc loss: 0.17048774744767436, policy loss: 1.923528489455381
Experience 5, Iter 23, disc loss: 0.156235633830894, policy loss: 1.9982213331506318
Experience 5, Iter 24, disc loss: 0.16754066859230077, policy loss: 1.9348981419056677
Experience 5, Iter 25, disc loss: 0.15517834194995486, policy loss: 2.010476241426909
Experience 5, Iter 26, disc loss: 0.15941379778597553, policy loss: 2.0030923501625457
Experience 5, Iter 27, disc loss: 0.13224343789495527, policy loss: 2.1692862616401136
Experience 5, Iter 28, disc loss: 0.14936178199727007, policy loss: 2.044460526296229
Experience 5, Iter 29, disc loss: 0.14464017838313606, policy loss: 2.088577132691512
Experience 5, Iter 30, disc loss: 0.15584107680742898, policy loss: 2.017832308411087
Experience 5, Iter 31, disc loss: 0.14950476110320335, policy loss: 2.0436655528127714
Experience 5, Iter 32, disc loss: 0.13877721361433026, policy loss: 2.11544336837249
Experience 5, Iter 33, disc loss: 0.13572694256038545, policy loss: 2.137852704179301
Experience 5, Iter 34, disc loss: 0.13472515952871722, policy loss: 2.155846164756353
Experience 5, Iter 35, disc loss: 0.1508421844668651, policy loss: 2.056595596718281
Experience 5, Iter 36, disc loss: 0.12799573000627543, policy loss: 2.2009170329275314
Experience 5, Iter 37, disc loss: 0.12732087532747866, policy loss: 2.2050514062026485
Experience 5, Iter 38, disc loss: 0.12873097281831042, policy loss: 2.2035164489267975
Experience 5, Iter 39, disc loss: 0.13150954992941238, policy loss: 2.186138770975533
Experience 5, Iter 40, disc loss: 0.12910733401991675, policy loss: 2.2027985349476267
Experience 5, Iter 41, disc loss: 0.11663078294575746, policy loss: 2.293587405245644
Experience 5, Iter 42, disc loss: 0.12456402318133766, policy loss: 2.2398491631026998
Experience 5, Iter 43, disc loss: 0.11389258787695872, policy loss: 2.3174874858271672
Experience 5, Iter 44, disc loss: 0.13019000929890603, policy loss: 2.2091109497312584
Experience 5, Iter 45, disc loss: 0.12547717135206307, policy loss: 2.2377330249423486
Experience 5, Iter 46, disc loss: 0.11503921020328454, policy loss: 2.315748079802785
Experience 5, Iter 47, disc loss: 0.10135249423033323, policy loss: 2.4382449112433897
Experience 5, Iter 48, disc loss: 0.11476604440044055, policy loss: 2.3220875588695575
Experience 5, Iter 49, disc loss: 0.11033505104343685, policy loss: 2.3510031745318725
Experience: 6
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0039],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0009, 0.0154, 0.0005], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0009, 0.0154, 0.0005])
N: 60
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([241.0000, 241.0000, 241.0000, 241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.388
Iter 2/2000 - Loss: 3.244
Iter 3/2000 - Loss: -5.222
Iter 4/2000 - Loss: -5.171
Iter 5/2000 - Loss: -2.333
Iter 6/2000 - Loss: -2.961
Iter 7/2000 - Loss: -5.197
Iter 8/2000 - Loss: -6.549
Iter 9/2000 - Loss: -6.322
Iter 10/2000 - Loss: -5.392
Iter 11/2000 - Loss: -4.874
Iter 12/2000 - Loss: -5.120
Iter 13/2000 - Loss: -5.788
Iter 14/2000 - Loss: -6.371
Iter 15/2000 - Loss: -6.581
Iter 16/2000 - Loss: -6.448
Iter 17/2000 - Loss: -6.179
Iter 18/2000 - Loss: -5.989
Iter 19/2000 - Loss: -5.990
Iter 20/2000 - Loss: -6.160
Iter 1981/2000 - Loss: -7.248
Iter 1982/2000 - Loss: -7.249
Iter 1983/2000 - Loss: -7.250
Iter 1984/2000 - Loss: -7.250
Iter 1985/2000 - Loss: -7.250
Iter 1986/2000 - Loss: -7.250
Iter 1987/2000 - Loss: -7.250
Iter 1988/2000 - Loss: -7.250
Iter 1989/2000 - Loss: -7.250
Iter 1990/2000 - Loss: -7.250
Iter 1991/2000 - Loss: -7.250
Iter 1992/2000 - Loss: -7.250
Iter 1993/2000 - Loss: -7.250
Iter 1994/2000 - Loss: -7.250
Iter 1995/2000 - Loss: -7.250
Iter 1996/2000 - Loss: -7.250
Iter 1997/2000 - Loss: -7.250
Iter 1998/2000 - Loss: -7.250
Iter 1999/2000 - Loss: -7.250
Iter 2000/2000 - Loss: -7.250
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0030],
        [0.0001]])
Lengthscale: tensor([[[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]],

        [[1.1393e-03, 6.2599e-03, 3.1548e-03, 1.4712e-04, 2.9273e-07,
          5.3603e-03]]])
Signal Variance: tensor([0.0007, 0.0007, 0.0122, 0.0004])
Estimated target variance: tensor([0.0008, 0.0009, 0.0154, 0.0005])
N: 60
Signal to noise ratio: tensor([1.9875, 1.9885, 2.0058, 1.9802])
Bound on condition number: tensor([238.0195, 238.2504, 242.4010, 236.2820])
Policy Optimizer learning rate:
0.009947458259305311
Experience 6, Iter 0, disc loss: 0.08698582176716894, policy loss: 2.600523518010365
Experience 6, Iter 1, disc loss: 0.08956463119829561, policy loss: 2.582605688439152
Experience 6, Iter 2, disc loss: 0.09621078955113811, policy loss: 2.4969836246948285
Experience 6, Iter 3, disc loss: 0.09163923647197034, policy loss: 2.5528269162041086
Experience 6, Iter 4, disc loss: 0.08314134345367453, policy loss: 2.6529959074665292
Experience 6, Iter 5, disc loss: 0.08730578528272467, policy loss: 2.5797351226182936
Experience 6, Iter 6, disc loss: 0.08628563451392143, policy loss: 2.621278333447514
Experience 6, Iter 7, disc loss: 0.08683219607299467, policy loss: 2.636126714005143
Experience 6, Iter 8, disc loss: 0.0772418622333385, policy loss: 2.7108295861407044
Experience 6, Iter 9, disc loss: 0.07479300652895322, policy loss: 2.755643415728187
Experience 6, Iter 10, disc loss: 0.0770256241500325, policy loss: 2.7436822553098015
Experience 6, Iter 11, disc loss: 0.07757084033340923, policy loss: 2.7445141737638266
Experience 6, Iter 12, disc loss: 0.08644391725942378, policy loss: 2.6614431223939037
Experience 6, Iter 13, disc loss: 0.08139349984607423, policy loss: 2.6408196537382764
Experience 6, Iter 14, disc loss: 0.08300525086467828, policy loss: 2.650236219704415
Experience 6, Iter 15, disc loss: 0.06446465627680932, policy loss: 2.8826463023986553
Experience 6, Iter 16, disc loss: 0.07231659328541036, policy loss: 2.800671335623062
Experience 6, Iter 17, disc loss: 0.07000643538035016, policy loss: 2.839971363374593
Experience 6, Iter 18, disc loss: 0.07004027265922932, policy loss: 2.8277024995459428
Experience 6, Iter 19, disc loss: 0.07518298580772331, policy loss: 2.788759163475122
Experience 6, Iter 20, disc loss: 0.06793190115458794, policy loss: 2.861167421869038
Experience 6, Iter 21, disc loss: 0.06644684124013371, policy loss: 2.8922926224605168
Experience 6, Iter 22, disc loss: 0.06977868120585212, policy loss: 2.8281602737462004
Experience 6, Iter 23, disc loss: 0.06426995770674766, policy loss: 2.922075019951635
Experience 6, Iter 24, disc loss: 0.06790597038832571, policy loss: 2.8522141765602207
Experience 6, Iter 25, disc loss: 0.06945560137246028, policy loss: 2.853140892577729
Experience 6, Iter 26, disc loss: 0.06456209439904931, policy loss: 2.920467668981092
Experience 6, Iter 27, disc loss: 0.06675675200791287, policy loss: 2.8794445493494565
Experience 6, Iter 28, disc loss: 0.06675533854589347, policy loss: 2.9333882698009455
Experience 6, Iter 29, disc loss: 0.05736197336415297, policy loss: 3.0346967944435854
Experience 6, Iter 30, disc loss: 0.05893191274258105, policy loss: 2.9822453580474324
Experience 6, Iter 31, disc loss: 0.05717904655158883, policy loss: 3.0635665489968154
Experience 6, Iter 32, disc loss: 0.065696264694971, policy loss: 2.9812301424434255
Experience 6, Iter 33, disc loss: 0.05162388281475341, policy loss: 3.1283591102671626
Experience 6, Iter 34, disc loss: 0.05327110121567774, policy loss: 3.1141312127880116
Experience 6, Iter 35, disc loss: 0.05575974111318681, policy loss: 3.065216469362865
Experience 6, Iter 36, disc loss: 0.05387570453698417, policy loss: 3.1059739713808088
Experience 6, Iter 37, disc loss: 0.0528813287561952, policy loss: 3.162937472569039
Experience 6, Iter 38, disc loss: 0.05780619108791437, policy loss: 3.050793153666069
Experience 6, Iter 39, disc loss: 0.058640132742120106, policy loss: 3.016180607472321
Experience 6, Iter 40, disc loss: 0.05286196509870682, policy loss: 3.1636886036813907
Experience 6, Iter 41, disc loss: 0.054626439695603136, policy loss: 3.09779231427754
Experience 6, Iter 42, disc loss: 0.05318014194208715, policy loss: 3.1622156114019027
Experience 6, Iter 43, disc loss: 0.057768676023407586, policy loss: 3.0980270216847448
Experience 6, Iter 44, disc loss: 0.04962363171068244, policy loss: 3.2419719676393615
Experience 6, Iter 45, disc loss: 0.05686384442778163, policy loss: 3.1325910720569436
Experience 6, Iter 46, disc loss: 0.05495518718672455, policy loss: 3.1191044682051965
Experience 6, Iter 47, disc loss: 0.0450910341896552, policy loss: 3.273715841414387
Experience 6, Iter 48, disc loss: 0.05548900114883695, policy loss: 3.1116698069595303
Experience 6, Iter 49, disc loss: 0.04357238904112249, policy loss: 3.355376006394277
Experience: 7
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0036],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0008, 0.0145, 0.0005], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0008, 0.0145, 0.0005])
N: 70
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([281.0000, 281.0000, 281.0000, 281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.284
Iter 2/2000 - Loss: 4.046
Iter 3/2000 - Loss: -5.109
Iter 4/2000 - Loss: -5.065
Iter 5/2000 - Loss: -2.010
Iter 6/2000 - Loss: -2.663
Iter 7/2000 - Loss: -5.070
Iter 8/2000 - Loss: -6.581
Iter 9/2000 - Loss: -6.410
Iter 10/2000 - Loss: -5.433
Iter 11/2000 - Loss: -4.838
Iter 12/2000 - Loss: -5.047
Iter 13/2000 - Loss: -5.737
Iter 14/2000 - Loss: -6.381
Iter 15/2000 - Loss: -6.656
Iter 16/2000 - Loss: -6.565
Iter 17/2000 - Loss: -6.303
Iter 18/2000 - Loss: -6.085
Iter 19/2000 - Loss: -6.036
Iter 20/2000 - Loss: -6.164
Iter 1981/2000 - Loss: -7.364
Iter 1982/2000 - Loss: -7.365
Iter 1983/2000 - Loss: -7.366
Iter 1984/2000 - Loss: -7.368
Iter 1985/2000 - Loss: -7.370
Iter 1986/2000 - Loss: -7.371
Iter 1987/2000 - Loss: -7.372
Iter 1988/2000 - Loss: -7.372
Iter 1989/2000 - Loss: -7.372
Iter 1990/2000 - Loss: -7.372
Iter 1991/2000 - Loss: -7.372
Iter 1992/2000 - Loss: -7.372
Iter 1993/2000 - Loss: -7.371
Iter 1994/2000 - Loss: -7.371
Iter 1995/2000 - Loss: -7.371
Iter 1996/2000 - Loss: -7.372
Iter 1997/2000 - Loss: -7.372
Iter 1998/2000 - Loss: -7.372
Iter 1999/2000 - Loss: -7.372
Iter 2000/2000 - Loss: -7.373
***AFTER OPTIMATION***
Noise Variance: tensor([[1.6851e-04],
        [1.6341e-04],
        [2.8507e-03],
        [9.8279e-05]])
Lengthscale: tensor([[[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]],

        [[1.2054e-03, 5.8720e-03, 3.0647e-03, 1.3604e-04, 2.8413e-07,
          4.7991e-03]]])
Signal Variance: tensor([0.0007, 0.0006, 0.0114, 0.0004])
Estimated target variance: tensor([0.0008, 0.0008, 0.0145, 0.0005])
N: 70
Signal to noise ratio: tensor([1.9877, 1.9875, 2.0040, 1.9785])
Bound on condition number: tensor([277.5587, 277.5192, 282.1163, 275.0060])
Policy Optimizer learning rate:
0.009936983085306158
Experience 7, Iter 0, disc loss: 0.047523339341525105, policy loss: 3.284058751136177
Experience 7, Iter 1, disc loss: 0.047971994781379985, policy loss: 3.280521280420471
Experience 7, Iter 2, disc loss: 0.03782259368086629, policy loss: 3.4612241037446996
Experience 7, Iter 3, disc loss: 0.05269930181323405, policy loss: 3.195016807048007
Experience 7, Iter 4, disc loss: 0.04549886866803786, policy loss: 3.298922374351166
Experience 7, Iter 5, disc loss: 0.04400786624246017, policy loss: 3.322459230003455
Experience 7, Iter 6, disc loss: 0.03657886813398939, policy loss: 3.521235921334217
Experience 7, Iter 7, disc loss: 0.04062962251046757, policy loss: 3.415230463042604
Experience 7, Iter 8, disc loss: 0.04308001823332739, policy loss: 3.3497080592815847
Experience 7, Iter 9, disc loss: 0.05410457600687106, policy loss: 3.2257830172098605
Experience 7, Iter 10, disc loss: 0.03994766884240602, policy loss: 3.3796911192189754
Experience 7, Iter 11, disc loss: 0.03836986265700576, policy loss: 3.4933016597806317
Experience 7, Iter 12, disc loss: 0.03389641125958865, policy loss: 3.611830327426629
Experience 7, Iter 13, disc loss: 0.04082221356960061, policy loss: 3.4179919231081035
Experience 7, Iter 14, disc loss: 0.039448078585213285, policy loss: 3.427192292121094
Experience 7, Iter 15, disc loss: 0.03867773992888815, policy loss: 3.4900151658960015
Experience 7, Iter 16, disc loss: 0.03440812540643342, policy loss: 3.5719491098305216
Experience 7, Iter 17, disc loss: 0.037071628373936696, policy loss: 3.490307094964311
Experience 7, Iter 18, disc loss: 0.03499619834116828, policy loss: 3.5720608022278633
Experience 7, Iter 19, disc loss: 0.03997949616414423, policy loss: 3.4621499473463064
Experience 7, Iter 20, disc loss: 0.0326191305851085, policy loss: 3.62920366846008
Experience 7, Iter 21, disc loss: 0.034194886206036855, policy loss: 3.626900762290462
Experience 7, Iter 22, disc loss: 0.03801410875703405, policy loss: 3.4965788002913953
Experience 7, Iter 23, disc loss: 0.03231150274419652, policy loss: 3.6629390343525414
Experience 7, Iter 24, disc loss: 0.031470822431908275, policy loss: 3.704062313343812
Experience 7, Iter 25, disc loss: 0.034625179329378385, policy loss: 3.6983404747160726
Experience 7, Iter 26, disc loss: 0.03284114035462609, policy loss: 3.6233816657458515
Experience 7, Iter 27, disc loss: 0.030079721984517437, policy loss: 3.713744874248195
Experience 7, Iter 28, disc loss: 0.035421397829983006, policy loss: 3.617310731308642
Experience 7, Iter 29, disc loss: 0.03435601697236526, policy loss: 3.6831022841781698
Experience 7, Iter 30, disc loss: 0.02982622604675605, policy loss: 3.722565781301071
Experience 7, Iter 31, disc loss: 0.030545838169697047, policy loss: 3.703245137358895
Experience 7, Iter 32, disc loss: 0.03191132721554623, policy loss: 3.6607926614188933
Experience 7, Iter 33, disc loss: 0.031708644436726795, policy loss: 3.765177051846697
Experience 7, Iter 34, disc loss: 0.028736910516880876, policy loss: 3.805828213900615
Experience 7, Iter 35, disc loss: 0.029123048174991853, policy loss: 3.7170308058126675
Experience 7, Iter 36, disc loss: 0.032280894721566096, policy loss: 3.730748860501678
Experience 7, Iter 37, disc loss: 0.03200118765865356, policy loss: 3.6564937265025863
Experience 7, Iter 38, disc loss: 0.032006959585203866, policy loss: 3.7333843930034867
Experience 7, Iter 39, disc loss: 0.02780511393339673, policy loss: 3.868447603204202
Experience 7, Iter 40, disc loss: 0.026468468488884137, policy loss: 3.891115952398228
Experience 7, Iter 41, disc loss: 0.032080257229194345, policy loss: 3.68316586062509
Experience 7, Iter 42, disc loss: 0.025451445634458288, policy loss: 3.9051827753518165
Experience 7, Iter 43, disc loss: 0.029024601308945793, policy loss: 3.834663923193884
Experience 7, Iter 44, disc loss: 0.027366069416695504, policy loss: 3.839476877726179
Experience 7, Iter 45, disc loss: 0.02949290413244574, policy loss: 3.813843534800478
Experience 7, Iter 46, disc loss: 0.028385344055264036, policy loss: 3.7726840154421954
Experience 7, Iter 47, disc loss: 0.02813771513238335, policy loss: 3.937941160451654
Experience 7, Iter 48, disc loss: 0.027862452356317725, policy loss: 3.882905597922052
Experience 7, Iter 49, disc loss: 0.022542213957524267, policy loss: 3.9955348394730623
Experience: 8
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0032],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0008, 0.0128, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0008, 0.0128, 0.0004])
N: 80
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([321.0000, 321.0000, 321.0000, 321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.514
Iter 2/2000 - Loss: 5.254
Iter 3/2000 - Loss: -5.232
Iter 4/2000 - Loss: -5.010
Iter 5/2000 - Loss: -1.481
Iter 6/2000 - Loss: -2.233
Iter 7/2000 - Loss: -5.001
Iter 8/2000 - Loss: -6.740
Iter 9/2000 - Loss: -6.548
Iter 10/2000 - Loss: -5.413
Iter 11/2000 - Loss: -4.699
Iter 12/2000 - Loss: -4.921
Iter 13/2000 - Loss: -5.725
Iter 14/2000 - Loss: -6.487
Iter 15/2000 - Loss: -6.813
Iter 16/2000 - Loss: -6.697
Iter 17/2000 - Loss: -6.377
Iter 18/2000 - Loss: -6.119
Iter 19/2000 - Loss: -6.071
Iter 20/2000 - Loss: -6.227
Iter 1981/2000 - Loss: -7.510
Iter 1982/2000 - Loss: -7.508
Iter 1983/2000 - Loss: -7.511
Iter 1984/2000 - Loss: -7.513
Iter 1985/2000 - Loss: -7.512
Iter 1986/2000 - Loss: -7.509
Iter 1987/2000 - Loss: -7.507
Iter 1988/2000 - Loss: -7.506
Iter 1989/2000 - Loss: -7.505
Iter 1990/2000 - Loss: -7.504
Iter 1991/2000 - Loss: -7.500
Iter 1992/2000 - Loss: -7.494
Iter 1993/2000 - Loss: -7.485
Iter 1994/2000 - Loss: -7.476
Iter 1995/2000 - Loss: -7.468
Iter 1996/2000 - Loss: -7.467
Iter 1997/2000 - Loss: -7.474
Iter 1998/2000 - Loss: -7.489
Iter 1999/2000 - Loss: -7.505
Iter 2000/2000 - Loss: -7.512
***AFTER OPTIMATION***
Noise Variance: tensor([[1.6906e-04],
        [1.5587e-04],
        [2.5207e-03],
        [9.3688e-05]])
Lengthscale: tensor([[[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]],

        [[1.0573e-03, 5.7941e-03, 2.7025e-03, 1.2104e-04, 2.5851e-07,
          4.5176e-03]]])
Signal Variance: tensor([0.0007, 0.0006, 0.0101, 0.0004])
Estimated target variance: tensor([0.0008, 0.0008, 0.0128, 0.0004])
N: 80
Signal to noise ratio: tensor([1.9879, 1.9872, 2.0038, 1.9833])
Bound on condition number: tensor([317.1261, 316.9329, 322.2271, 315.6929])
Policy Optimizer learning rate:
0.009926518942192228
Experience 8, Iter 0, disc loss: 0.03570788642738479, policy loss: 3.5878962705394777
Experience 8, Iter 1, disc loss: 0.02654391092296599, policy loss: 3.852421976839275
Experience 8, Iter 2, disc loss: 0.03043508893815747, policy loss: 3.714254923308342
Experience 8, Iter 3, disc loss: 0.027722881177061762, policy loss: 3.7911423370529276
Experience 8, Iter 4, disc loss: 0.028361946317233724, policy loss: 3.8472714989323826
Experience 8, Iter 5, disc loss: 0.02661851052497212, policy loss: 3.844026546863942
Experience 8, Iter 6, disc loss: 0.026068739896873118, policy loss: 3.8925327239266805
Experience 8, Iter 7, disc loss: 0.02611586685653986, policy loss: 3.8497446975040743
Experience 8, Iter 8, disc loss: 0.02861497509143337, policy loss: 3.7939163215100673
Experience 8, Iter 9, disc loss: 0.031035230350132358, policy loss: 3.737798034287776
Experience 8, Iter 10, disc loss: 0.02876890056126597, policy loss: 3.7725295819492137
Experience 8, Iter 11, disc loss: 0.02716841337678982, policy loss: 3.8596534575025614
Experience 8, Iter 12, disc loss: 0.02610842829935325, policy loss: 3.9057679863651185
Experience 8, Iter 13, disc loss: 0.026528533809518074, policy loss: 3.8538569925333634
Experience 8, Iter 14, disc loss: 0.02207769133825936, policy loss: 4.027489950636829
Experience 8, Iter 15, disc loss: 0.022746807397760346, policy loss: 4.0501117006423035
Experience 8, Iter 16, disc loss: 0.02416196819017298, policy loss: 3.97917510887885
Experience 8, Iter 17, disc loss: 0.022373760725329456, policy loss: 4.082522512487596
Experience 8, Iter 18, disc loss: 0.02984950489639895, policy loss: 3.7502294393472115
Experience 8, Iter 19, disc loss: 0.026853968851675236, policy loss: 3.9445266274834436
Experience 8, Iter 20, disc loss: 0.030822400006234405, policy loss: 3.8450479837297036
Experience 8, Iter 21, disc loss: 0.021735460953669922, policy loss: 4.1134720704348275
Experience 8, Iter 22, disc loss: 0.02221771398535319, policy loss: 4.076239761470069
Experience 8, Iter 23, disc loss: 0.02207640883506189, policy loss: 4.088486799041153
Experience 8, Iter 24, disc loss: 0.021219790115982694, policy loss: 4.141618664817998
Experience 8, Iter 25, disc loss: 0.022696826902487515, policy loss: 4.0633306659539015
Experience 8, Iter 26, disc loss: 0.02369219648049336, policy loss: 4.006633504298275
Experience 8, Iter 27, disc loss: 0.027876845258033144, policy loss: 3.9367397596871654
Experience 8, Iter 28, disc loss: 0.021693785761680247, policy loss: 4.108976197329813
Experience 8, Iter 29, disc loss: 0.020943615823327374, policy loss: 4.110172224520023
Experience 8, Iter 30, disc loss: 0.023751868787986997, policy loss: 4.089144637545052
Experience 8, Iter 31, disc loss: 0.021051865480278216, policy loss: 4.167402963700675
Experience 8, Iter 32, disc loss: 0.025138891669823846, policy loss: 3.9542279903989455
Experience 8, Iter 33, disc loss: 0.02500232838416692, policy loss: 3.966246556076877
Experience 8, Iter 34, disc loss: 0.024188526871001884, policy loss: 3.985121537125696
Experience 8, Iter 35, disc loss: 0.021407675734618898, policy loss: 4.1758653220843165
Experience 8, Iter 36, disc loss: 0.02009127204476106, policy loss: 4.1622449527943495
Experience 8, Iter 37, disc loss: 0.021643795813873584, policy loss: 4.108850251081427
Experience 8, Iter 38, disc loss: 0.02049318169484734, policy loss: 4.19413317819589
Experience 8, Iter 39, disc loss: 0.022950078350521284, policy loss: 4.00157055345351
Experience 8, Iter 40, disc loss: 0.02287760506979681, policy loss: 4.113752351888699
Experience 8, Iter 41, disc loss: 0.022165300317310593, policy loss: 4.1406412698894695
Experience 8, Iter 42, disc loss: 0.01852013974893134, policy loss: 4.298711975408008
Experience 8, Iter 43, disc loss: 0.017062242601962204, policy loss: 4.309956773290945
Experience 8, Iter 44, disc loss: 0.021125933969002027, policy loss: 4.175535942024711
Experience 8, Iter 45, disc loss: 0.018774511874162812, policy loss: 4.241868472098764
Experience 8, Iter 46, disc loss: 0.01842174271881489, policy loss: 4.288107386017856
Experience 8, Iter 47, disc loss: 0.02144980720215813, policy loss: 4.199621366727326
Experience 8, Iter 48, disc loss: 0.01920738667574007, policy loss: 4.240062777736574
Experience 8, Iter 49, disc loss: 0.024825867621346916, policy loss: 4.079191840888984
Experience: 9
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0029],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0007, 0.0114, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0007, 0.0114, 0.0004])
N: 90
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([361.0000, 361.0000, 361.0000, 361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.464
Iter 2/2000 - Loss: 5.331
Iter 3/2000 - Loss: -5.211
Iter 4/2000 - Loss: -5.091
Iter 5/2000 - Loss: -1.539
Iter 6/2000 - Loss: -2.253
Iter 7/2000 - Loss: -5.032
Iter 8/2000 - Loss: -6.809
Iter 9/2000 - Loss: -6.642
Iter 10/2000 - Loss: -5.502
Iter 11/2000 - Loss: -4.772
Iter 12/2000 - Loss: -4.985
Iter 13/2000 - Loss: -5.792
Iter 14/2000 - Loss: -6.566
Iter 15/2000 - Loss: -6.901
Iter 16/2000 - Loss: -6.787
Iter 17/2000 - Loss: -6.470
Iter 18/2000 - Loss: -6.216
Iter 19/2000 - Loss: -6.167
Iter 20/2000 - Loss: -6.311
Iter 1981/2000 - Loss: -7.538
Iter 1982/2000 - Loss: -7.584
Iter 1983/2000 - Loss: -7.614
Iter 1984/2000 - Loss: -7.600
Iter 1985/2000 - Loss: -7.574
Iter 1986/2000 - Loss: -7.578
Iter 1987/2000 - Loss: -7.601
Iter 1988/2000 - Loss: -7.601
Iter 1989/2000 - Loss: -7.581
Iter 1990/2000 - Loss: -7.577
Iter 1991/2000 - Loss: -7.592
Iter 1992/2000 - Loss: -7.602
Iter 1993/2000 - Loss: -7.595
Iter 1994/2000 - Loss: -7.585
Iter 1995/2000 - Loss: -7.583
Iter 1996/2000 - Loss: -7.588
Iter 1997/2000 - Loss: -7.590
Iter 1998/2000 - Loss: -7.589
Iter 1999/2000 - Loss: -7.588
Iter 2000/2000 - Loss: -7.588
***AFTER OPTIMATION***
Noise Variance: tensor([[1.7350e-04],
        [1.4484e-04],
        [2.2525e-03],
        [8.6768e-05]])
Lengthscale: tensor([[[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]],

        [[1.0797e-03, 5.6613e-03, 2.4174e-03, 1.1447e-04, 2.5099e-07,
          4.3105e-03]]])
Signal Variance: tensor([0.0007, 0.0006, 0.0090, 0.0003])
Estimated target variance: tensor([0.0008, 0.0007, 0.0114, 0.0004])
N: 90
Signal to noise ratio: tensor([1.9909, 1.9854, 2.0033, 1.9752])
Bound on condition number: tensor([357.7356, 355.7596, 362.1818, 352.1244])
Policy Optimizer learning rate:
0.009916065818347442
Experience 9, Iter 0, disc loss: 0.016262597869711057, policy loss: 4.464977925432052
Experience 9, Iter 1, disc loss: 0.01339776035428251, policy loss: 4.596527511417084
Experience 9, Iter 2, disc loss: 0.016546254400688916, policy loss: 4.394163675037295
Experience 9, Iter 3, disc loss: 0.013948640247122559, policy loss: 4.611240250413211
Experience 9, Iter 4, disc loss: 0.016547467457294237, policy loss: 4.453697341442362
Experience 9, Iter 5, disc loss: 0.013834903282199966, policy loss: 4.568716338354683
Experience 9, Iter 6, disc loss: 0.015354500156408397, policy loss: 4.545533361493636
Experience 9, Iter 7, disc loss: 0.01426465383688434, policy loss: 4.538780176430158
Experience 9, Iter 8, disc loss: 0.016466208476681714, policy loss: 4.462463509744656
Experience 9, Iter 9, disc loss: 0.01633270900795748, policy loss: 4.422266275223588
Experience 9, Iter 10, disc loss: 0.01330179825321818, policy loss: 4.6285500691976775
Experience 9, Iter 11, disc loss: 0.014173907658987964, policy loss: 4.547778064482667
Experience 9, Iter 12, disc loss: 0.012091504407020998, policy loss: 4.69160925213346
Experience 9, Iter 13, disc loss: 0.014085312096073091, policy loss: 4.6626555133431
Experience 9, Iter 14, disc loss: 0.013200062409159222, policy loss: 4.636443713991889
Experience 9, Iter 15, disc loss: 0.015990560384713356, policy loss: 4.4915540079893015
Experience 9, Iter 16, disc loss: 0.01152746811436426, policy loss: 4.77465713298561
Experience 9, Iter 17, disc loss: 0.013202525076424765, policy loss: 4.677442212262491
Experience 9, Iter 18, disc loss: 0.015877428390775754, policy loss: 4.510570268550636
Experience 9, Iter 19, disc loss: 0.012015341738019311, policy loss: 4.707159531533721
Experience 9, Iter 20, disc loss: 0.013979392881262046, policy loss: 4.603065984446364
Experience 9, Iter 21, disc loss: 0.014882465945759907, policy loss: 4.556328135230231
Experience 9, Iter 22, disc loss: 0.012557995762694202, policy loss: 4.603051336875449
Experience 9, Iter 23, disc loss: 0.011044779895002391, policy loss: 4.766300561258532
Experience 9, Iter 24, disc loss: 0.012713162895973039, policy loss: 4.645217092653777
Experience 9, Iter 25, disc loss: 0.014031324958676199, policy loss: 4.708029122859349
Experience 9, Iter 26, disc loss: 0.01351022891841301, policy loss: 4.7434178673330205
Experience 9, Iter 27, disc loss: 0.010921364171919095, policy loss: 4.766413645730286
Experience 9, Iter 28, disc loss: 0.011162071975925978, policy loss: 4.8171759775983745
Experience 9, Iter 29, disc loss: 0.012076666618314254, policy loss: 4.746365112680316
Experience 9, Iter 30, disc loss: 0.01299680935438531, policy loss: 4.673295144976607
Experience 9, Iter 31, disc loss: 0.013070596977607296, policy loss: 4.698237641435944
Experience 9, Iter 32, disc loss: 0.013316363066286311, policy loss: 4.7380538251541156
Experience 9, Iter 33, disc loss: 0.011684185675883743, policy loss: 4.81382133097433
Experience 9, Iter 34, disc loss: 0.011852531403752828, policy loss: 4.745804998197217
Experience 9, Iter 35, disc loss: 0.011776328574300146, policy loss: 4.777039433176471
Experience 9, Iter 36, disc loss: 0.012309837067531339, policy loss: 4.820178350266851
Experience 9, Iter 37, disc loss: 0.011444657800807497, policy loss: 4.834828200667159
Experience 9, Iter 38, disc loss: 0.011713683465378781, policy loss: 4.814544729048164
Experience 9, Iter 39, disc loss: 0.011795019059853777, policy loss: 4.744757793339874
Experience 9, Iter 40, disc loss: 0.01023320221313479, policy loss: 4.910372322608858
Experience 9, Iter 41, disc loss: 0.01371539615081588, policy loss: 4.685045538525975
Experience 9, Iter 42, disc loss: 0.011411823994050524, policy loss: 4.765227126231823
Experience 9, Iter 43, disc loss: 0.011359758953734884, policy loss: 4.863071334120972
Experience 9, Iter 44, disc loss: 0.010121774775684413, policy loss: 4.846131826445237
Experience 9, Iter 45, disc loss: 0.00988219904127103, policy loss: 4.922280918742828
Experience 9, Iter 46, disc loss: 0.010324060941794118, policy loss: 4.893339639577993
Experience 9, Iter 47, disc loss: 0.012741567347125723, policy loss: 4.729388811552019
Experience 9, Iter 48, disc loss: 0.010356930335730853, policy loss: 4.889647668874605
Experience 9, Iter 49, disc loss: 0.009929457190735519, policy loss: 4.940846310699731
Experience: 10
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0027],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0007, 0.0108, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0007, 0.0108, 0.0004])
N: 100
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([401.0000, 401.0000, 401.0000, 401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.505
Iter 2/2000 - Loss: 5.827
Iter 3/2000 - Loss: -5.227
Iter 4/2000 - Loss: -5.092
Iter 5/2000 - Loss: -1.362
Iter 6/2000 - Loss: -2.092
Iter 7/2000 - Loss: -5.007
Iter 8/2000 - Loss: -6.885
Iter 9/2000 - Loss: -6.720
Iter 10/2000 - Loss: -5.523
Iter 11/2000 - Loss: -4.744
Iter 12/2000 - Loss: -4.956
Iter 13/2000 - Loss: -5.803
Iter 14/2000 - Loss: -6.623
Iter 15/2000 - Loss: -6.982
Iter 16/2000 - Loss: -6.864
Iter 17/2000 - Loss: -6.526
Iter 18/2000 - Loss: -6.253
Iter 19/2000 - Loss: -6.199
Iter 20/2000 - Loss: -6.352
Iter 1981/2000 - Loss: -7.643
Iter 1982/2000 - Loss: -7.645
Iter 1983/2000 - Loss: -7.661
Iter 1984/2000 - Loss: -7.685
Iter 1985/2000 - Loss: -7.703
Iter 1986/2000 - Loss: -7.700
Iter 1987/2000 - Loss: -7.688
Iter 1988/2000 - Loss: -7.690
Iter 1989/2000 - Loss: -7.702
Iter 1990/2000 - Loss: -7.701
Iter 1991/2000 - Loss: -7.693
Iter 1992/2000 - Loss: -7.695
Iter 1993/2000 - Loss: -7.703
Iter 1994/2000 - Loss: -7.704
Iter 1995/2000 - Loss: -7.699
Iter 1996/2000 - Loss: -7.694
Iter 1997/2000 - Loss: -7.694
Iter 1998/2000 - Loss: -7.699
Iter 1999/2000 - Loss: -7.703
Iter 2000/2000 - Loss: -7.705
***AFTER OPTIMATION***
Noise Variance: tensor([[1.6041e-04],
        [1.3644e-04],
        [2.1256e-03],
        [8.4725e-05]])
Lengthscale: tensor([[[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]],

        [[9.9939e-04, 5.4636e-03, 2.3631e-03, 1.0720e-04, 2.3741e-07,
          4.0052e-03]]])
Signal Variance: tensor([0.0006, 0.0005, 0.0085, 0.0003])
Estimated target variance: tensor([0.0008, 0.0007, 0.0108, 0.0004])
N: 100
Signal to noise ratio: tensor([1.9867, 1.9849, 2.0027, 1.9754])
Bound on condition number: tensor([395.6849, 394.9792, 402.0999, 391.2386])
Policy Optimizer learning rate:
0.009905623702167956
Experience 10, Iter 0, disc loss: 0.013887244471070492, policy loss: 4.641094195672604
Experience 10, Iter 1, disc loss: 0.010417758209116112, policy loss: 4.78434910861608
Experience 10, Iter 2, disc loss: 0.011168296486439588, policy loss: 4.787924156571229
Experience 10, Iter 3, disc loss: 0.00897780942826423, policy loss: 4.983881270461366
Experience 10, Iter 4, disc loss: 0.01060742984014866, policy loss: 4.83500845905915
Experience 10, Iter 5, disc loss: 0.012011172723049041, policy loss: 4.845952935360042
Experience 10, Iter 6, disc loss: 0.011252968139535012, policy loss: 4.736540080620424
Experience 10, Iter 7, disc loss: 0.010777097764545957, policy loss: 4.775692184559871
Experience 10, Iter 8, disc loss: 0.011015648037382664, policy loss: 4.845025970375016
Experience 10, Iter 9, disc loss: 0.0105138688239086, policy loss: 4.860327760808497
Experience 10, Iter 10, disc loss: 0.012087402529388488, policy loss: 4.708317080494247
Experience 10, Iter 11, disc loss: 0.009579370900943046, policy loss: 4.962486493631117
Experience 10, Iter 12, disc loss: 0.008537345969348844, policy loss: 5.009771736405993
Experience 10, Iter 13, disc loss: 0.012037138006824693, policy loss: 4.846260998671256
Experience 10, Iter 14, disc loss: 0.011059463540394086, policy loss: 4.881866243196158
Experience 10, Iter 15, disc loss: 0.013085763511596354, policy loss: 4.803538603207556
Experience 10, Iter 16, disc loss: 0.010843649048010391, policy loss: 4.906546885324222
Experience 10, Iter 17, disc loss: 0.00953544117232897, policy loss: 4.946404706210513
Experience 10, Iter 18, disc loss: 0.012656731392893536, policy loss: 4.6692593669777445
Experience 10, Iter 19, disc loss: 0.009702998588102692, policy loss: 4.97550321229114
Experience 10, Iter 20, disc loss: 0.012602473707698536, policy loss: 4.9408202420683835
Experience 10, Iter 21, disc loss: 0.012467843946314377, policy loss: 4.760948573294617
Experience 10, Iter 22, disc loss: 0.011021995604458908, policy loss: 4.750153782628135
Experience 10, Iter 23, disc loss: 0.009938128187312004, policy loss: 4.987800048201097
Experience 10, Iter 24, disc loss: 0.011914413354801387, policy loss: 4.787223738318289
Experience 10, Iter 25, disc loss: 0.010004337805064715, policy loss: 4.977101137454701
Experience 10, Iter 26, disc loss: 0.009229036041156416, policy loss: 5.0451974391050625
Experience 10, Iter 27, disc loss: 0.012028706808174137, policy loss: 4.877132739170968
Experience 10, Iter 28, disc loss: 0.009971261028392485, policy loss: 5.006082707529938
Experience 10, Iter 29, disc loss: 0.009206568202396936, policy loss: 5.021382554281339
Experience 10, Iter 30, disc loss: 0.01070118411043207, policy loss: 4.892538211039376
Experience 10, Iter 31, disc loss: 0.009854160120363567, policy loss: 5.089507380545712
Experience 10, Iter 32, disc loss: 0.010924691945107645, policy loss: 4.914517041989196
Experience 10, Iter 33, disc loss: 0.008577081717067683, policy loss: 5.178603227662181
Experience 10, Iter 34, disc loss: 0.00758410439427561, policy loss: 5.179378035175738
Experience 10, Iter 35, disc loss: 0.009018986896056694, policy loss: 5.053718364326581
Experience 10, Iter 36, disc loss: 0.011041932058485842, policy loss: 4.946623720692054
Experience 10, Iter 37, disc loss: 0.008254437778729005, policy loss: 5.119968965941553
Experience 10, Iter 38, disc loss: 0.009720161903815784, policy loss: 4.965882666252099
Experience 10, Iter 39, disc loss: 0.009436940628972072, policy loss: 5.058291555688317
Experience 10, Iter 40, disc loss: 0.00950176466070656, policy loss: 5.047980434460087
Experience 10, Iter 41, disc loss: 0.01012552940917812, policy loss: 4.984335916064428
Experience 10, Iter 42, disc loss: 0.010366361658164598, policy loss: 5.032991925256539
Experience 10, Iter 43, disc loss: 0.010741333732631411, policy loss: 4.953202154379984
Experience 10, Iter 44, disc loss: 0.008339584946638329, policy loss: 5.06563087718417
Experience 10, Iter 45, disc loss: 0.008820021543657158, policy loss: 5.067102930931183
Experience 10, Iter 46, disc loss: 0.008809213760796377, policy loss: 5.123248049610554
Experience 10, Iter 47, disc loss: 0.00713329820646668, policy loss: 5.3379480958010035
Experience 10, Iter 48, disc loss: 0.009759357901801914, policy loss: 5.145885888535133
Experience 10, Iter 49, disc loss: 0.010433239433330452, policy loss: 5.009055489381386
Experience: 11
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0026],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0007, 0.0105, 0.0005], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0007, 0.0105, 0.0005])
N: 110
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([441.0000, 441.0000, 441.0000, 441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.458
Iter 2/2000 - Loss: 5.355
Iter 3/2000 - Loss: -5.214
Iter 4/2000 - Loss: -5.213
Iter 5/2000 - Loss: -1.625
Iter 6/2000 - Loss: -2.246
Iter 7/2000 - Loss: -5.022
Iter 8/2000 - Loss: -6.865
Iter 9/2000 - Loss: -6.758
Iter 10/2000 - Loss: -5.626
Iter 11/2000 - Loss: -4.856
Iter 12/2000 - Loss: -5.024
Iter 13/2000 - Loss: -5.814
Iter 14/2000 - Loss: -6.601
Iter 15/2000 - Loss: -6.963
Iter 16/2000 - Loss: -6.872
Iter 17/2000 - Loss: -6.563
Iter 18/2000 - Loss: -6.305
Iter 19/2000 - Loss: -6.242
Iter 20/2000 - Loss: -6.371
Iter 1981/2000 - Loss: -7.698
Iter 1982/2000 - Loss: -7.695
Iter 1983/2000 - Loss: -7.692
Iter 1984/2000 - Loss: -7.689
Iter 1985/2000 - Loss: -7.685
Iter 1986/2000 - Loss: -7.679
Iter 1987/2000 - Loss: -7.674
Iter 1988/2000 - Loss: -7.671
Iter 1989/2000 - Loss: -7.672
Iter 1990/2000 - Loss: -7.678
Iter 1991/2000 - Loss: -7.687
Iter 1992/2000 - Loss: -7.696
Iter 1993/2000 - Loss: -7.699
Iter 1994/2000 - Loss: -7.696
Iter 1995/2000 - Loss: -7.691
Iter 1996/2000 - Loss: -7.688
Iter 1997/2000 - Loss: -7.689
Iter 1998/2000 - Loss: -7.692
Iter 1999/2000 - Loss: -7.695
Iter 2000/2000 - Loss: -7.695
***AFTER OPTIMATION***
Noise Variance: tensor([[1.5533e-04],
        [1.3742e-04],
        [2.0804e-03],
        [9.4348e-05]])
Lengthscale: tensor([[[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]],

        [[9.7928e-04, 5.3328e-03, 2.3423e-03, 1.0668e-04, 2.5884e-07,
          3.8024e-03]]])
Signal Variance: tensor([0.0006, 0.0005, 0.0083, 0.0004])
Estimated target variance: tensor([0.0008, 0.0007, 0.0105, 0.0005])
N: 110
Signal to noise ratio: tensor([1.9871, 1.9860, 2.0026, 1.9808])
Bound on condition number: tensor([435.3419, 434.8571, 442.1297, 432.5957])
Policy Optimizer learning rate:
0.009895192582062146
Experience 11, Iter 0, disc loss: 0.009491338286532952, policy loss: 5.074412592487653
Experience 11, Iter 1, disc loss: 0.006954490327555417, policy loss: 5.357806103800478
Experience 11, Iter 2, disc loss: 0.009197016840367264, policy loss: 5.158265150580705
Experience 11, Iter 3, disc loss: 0.008499071291724343, policy loss: 5.251118485720478
Experience 11, Iter 4, disc loss: 0.008367936359751407, policy loss: 5.107174799920182
Experience 11, Iter 5, disc loss: 0.005674235756969409, policy loss: 5.442832351093134
Experience 11, Iter 6, disc loss: 0.008251723909684643, policy loss: 5.205174349306823
Experience 11, Iter 7, disc loss: 0.007099034826443095, policy loss: 5.299945394719647
Experience 11, Iter 8, disc loss: 0.007623105425904645, policy loss: 5.246743437767289
Experience 11, Iter 9, disc loss: 0.010220335799100743, policy loss: 5.1230733404496664
Experience 11, Iter 10, disc loss: 0.008811044759120899, policy loss: 5.185449115519883
Experience 11, Iter 11, disc loss: 0.008001927146400802, policy loss: 5.1484302811903975
Experience 11, Iter 12, disc loss: 0.007749606822279889, policy loss: 5.368415146148924
Experience 11, Iter 13, disc loss: 0.007955166539466499, policy loss: 5.228050006431837
Experience 11, Iter 14, disc loss: 0.0066051981216746596, policy loss: 5.377618889734256
Experience 11, Iter 15, disc loss: 0.008049091691476858, policy loss: 5.2009920328415005
Experience 11, Iter 16, disc loss: 0.008812768035920043, policy loss: 5.215684655621432
Experience 11, Iter 17, disc loss: 0.007992712263299834, policy loss: 5.208935587185136
Experience 11, Iter 18, disc loss: 0.007098612890065284, policy loss: 5.295789178193807
Experience 11, Iter 19, disc loss: 0.006956531811384513, policy loss: 5.3430971316651625
Experience 11, Iter 20, disc loss: 0.006831255027157644, policy loss: 5.373171893318071
Experience 11, Iter 21, disc loss: 0.007527793353433054, policy loss: 5.292681366379227
Experience 11, Iter 22, disc loss: 0.008385520234480172, policy loss: 5.1593558979166545
Experience 11, Iter 23, disc loss: 0.005996102479771257, policy loss: 5.506777379486919
Experience 11, Iter 24, disc loss: 0.00793432797282431, policy loss: 5.359987040465448
Experience 11, Iter 25, disc loss: 0.006738028880657456, policy loss: 5.347946474040761
Experience 11, Iter 26, disc loss: 0.0074364807761270685, policy loss: 5.381772754346279
Experience 11, Iter 27, disc loss: 0.008655236846227612, policy loss: 5.215914355163522
Experience 11, Iter 28, disc loss: 0.007334245036603421, policy loss: 5.319191011774466
Experience 11, Iter 29, disc loss: 0.007597320342058148, policy loss: 5.305904319104908
Experience 11, Iter 30, disc loss: 0.00616717833791867, policy loss: 5.468770293384028
Experience 11, Iter 31, disc loss: 0.009192369522768737, policy loss: 5.147668279331867
Experience 11, Iter 32, disc loss: 0.006098125122242023, policy loss: 5.444189096794918
Experience 11, Iter 33, disc loss: 0.006191208039611831, policy loss: 5.471680245026523
Experience 11, Iter 34, disc loss: 0.006392938188602692, policy loss: 5.457485587041575
Experience 11, Iter 35, disc loss: 0.007436160404871445, policy loss: 5.311652394473693
Experience 11, Iter 36, disc loss: 0.007009311102426158, policy loss: 5.350827341474894
Experience 11, Iter 37, disc loss: 0.00646992060876755, policy loss: 5.440084497436343
Experience 11, Iter 38, disc loss: 0.006658828994072551, policy loss: 5.405179893307279
Experience 11, Iter 39, disc loss: 0.005364640242913188, policy loss: 5.700730564062486
Experience 11, Iter 40, disc loss: 0.007804668516512572, policy loss: 5.40752547739276
Experience 11, Iter 41, disc loss: 0.006223961867917933, policy loss: 5.515433737788214
Experience 11, Iter 42, disc loss: 0.004698499693046006, policy loss: 5.688006645739465
Experience 11, Iter 43, disc loss: 0.006947411255850027, policy loss: 5.347604162577461
Experience 11, Iter 44, disc loss: 0.008069168204793014, policy loss: 5.262527547935581
Experience 11, Iter 45, disc loss: 0.006926151509027654, policy loss: 5.351599459906383
Experience 11, Iter 46, disc loss: 0.006434872460002252, policy loss: 5.572536030296745
Experience 11, Iter 47, disc loss: 0.0057946756574441365, policy loss: 5.507395865537751
Experience 11, Iter 48, disc loss: 0.004596089566108055, policy loss: 5.6845253281297765
Experience 11, Iter 49, disc loss: 0.006511114458431698, policy loss: 5.492720965645908
Experience: 12
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0025],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0008, 0.0007, 0.0099, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0008, 0.0007, 0.0099, 0.0004])
N: 120
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([481.0000, 481.0000, 481.0000, 481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.545
Iter 2/2000 - Loss: 5.875
Iter 3/2000 - Loss: -5.262
Iter 4/2000 - Loss: -5.207
Iter 5/2000 - Loss: -1.415
Iter 6/2000 - Loss: -2.069
Iter 7/2000 - Loss: -4.997
Iter 8/2000 - Loss: -6.942
Iter 9/2000 - Loss: -6.831
Iter 10/2000 - Loss: -5.635
Iter 11/2000 - Loss: -4.814
Iter 12/2000 - Loss: -4.983
Iter 13/2000 - Loss: -5.817
Iter 14/2000 - Loss: -6.653
Iter 15/2000 - Loss: -7.040
Iter 16/2000 - Loss: -6.941
Iter 17/2000 - Loss: -6.608
Iter 18/2000 - Loss: -6.332
Iter 19/2000 - Loss: -6.269
Iter 20/2000 - Loss: -6.411
Iter 1981/2000 - Loss: -7.732
Iter 1982/2000 - Loss: -7.737
Iter 1983/2000 - Loss: -7.746
Iter 1984/2000 - Loss: -7.747
Iter 1985/2000 - Loss: -7.739
Iter 1986/2000 - Loss: -7.729
Iter 1987/2000 - Loss: -7.724
Iter 1988/2000 - Loss: -7.732
Iter 1989/2000 - Loss: -7.753
Iter 1990/2000 - Loss: -7.772
Iter 1991/2000 - Loss: -7.771
Iter 1992/2000 - Loss: -7.760
Iter 1993/2000 - Loss: -7.765
Iter 1994/2000 - Loss: -7.775
Iter 1995/2000 - Loss: -7.770
Iter 1996/2000 - Loss: -7.762
Iter 1997/2000 - Loss: -7.768
Iter 1998/2000 - Loss: -7.777
Iter 1999/2000 - Loss: -7.778
Iter 2000/2000 - Loss: -7.773
***AFTER OPTIMATION***
Noise Variance: tensor([[1.4782e-04],
        [1.2663e-04],
        [1.9530e-03],
        [9.0041e-05]])
Lengthscale: tensor([[[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]],

        [[9.9328e-04, 5.1447e-03, 2.2021e-03, 1.0098e-04, 2.4864e-07,
          3.5974e-03]]])
Signal Variance: tensor([0.0006, 0.0005, 0.0078, 0.0004])
Estimated target variance: tensor([0.0008, 0.0007, 0.0099, 0.0004])
N: 120
Signal to noise ratio: tensor([1.9850, 1.9816, 2.0020, 1.9782])
Bound on condition number: tensor([473.8221, 472.2112, 481.9614, 470.6138])
Policy Optimizer learning rate:
0.009884772446450592
Experience 12, Iter 0, disc loss: 0.00610938444765013, policy loss: 5.619702169331826
Experience 12, Iter 1, disc loss: 0.005626526922698196, policy loss: 5.538864064405997
Experience 12, Iter 2, disc loss: 0.0074335512527580645, policy loss: 5.356209746829815
Experience 12, Iter 3, disc loss: 0.004612408883921754, policy loss: 5.706201402893157
Experience 12, Iter 4, disc loss: 0.008308031058992693, policy loss: 5.3317974840390185
Experience 12, Iter 5, disc loss: 0.005483233001843473, policy loss: 5.623000538171166
Experience 12, Iter 6, disc loss: 0.006692983090858276, policy loss: 5.503602221100724
Experience 12, Iter 7, disc loss: 0.007654490369337192, policy loss: 5.254005138047362
Experience 12, Iter 8, disc loss: 0.006194000634341745, policy loss: 5.4434107411484645
Experience 12, Iter 9, disc loss: 0.004862772999112385, policy loss: 5.700111256828921
Experience 12, Iter 10, disc loss: 0.004538832848424184, policy loss: 5.725960969527257
Experience 12, Iter 11, disc loss: 0.005972776089864078, policy loss: 5.598940534422326
Experience 12, Iter 12, disc loss: 0.006471699839768625, policy loss: 5.469207764401514
Experience 12, Iter 13, disc loss: 0.006262158680419697, policy loss: 5.388780276763891
Experience 12, Iter 14, disc loss: 0.0062023831363725405, policy loss: 5.499377864681933
Experience 12, Iter 15, disc loss: 0.00627384435542438, policy loss: 5.575344815124765
Experience 12, Iter 16, disc loss: 0.00648726407534909, policy loss: 5.502195077280233
Experience 12, Iter 17, disc loss: 0.005608072905552242, policy loss: 5.5934561372973
Experience 12, Iter 18, disc loss: 0.004879663565229739, policy loss: 5.7027428859581555
Experience 12, Iter 19, disc loss: 0.005068396835412232, policy loss: 5.673051203508619
Experience 12, Iter 20, disc loss: 0.0060074947965201965, policy loss: 5.556539622342912
Experience 12, Iter 21, disc loss: 0.00782966858748407, policy loss: 5.360068656596127
Experience 12, Iter 22, disc loss: 0.007148290018251389, policy loss: 5.472248533667947
Experience 12, Iter 23, disc loss: 0.00512812236237179, policy loss: 5.699630730830934
Experience 12, Iter 24, disc loss: 0.0065631393727387626, policy loss: 5.3750557923230815
Experience 12, Iter 25, disc loss: 0.006274130159673387, policy loss: 5.482389836705373
Experience 12, Iter 26, disc loss: 0.0058037367664832504, policy loss: 5.591829918306654
Experience 12, Iter 27, disc loss: 0.006135059976340764, policy loss: 5.410023485517091
Experience 12, Iter 28, disc loss: 0.005719815401193188, policy loss: 5.66690973381935
Experience 12, Iter 29, disc loss: 0.005335817646801804, policy loss: 5.70453451792287
Experience 12, Iter 30, disc loss: 0.006274451144009176, policy loss: 5.514265581428646
Experience 12, Iter 31, disc loss: 0.005144624065098871, policy loss: 5.833536956295175
Experience 12, Iter 32, disc loss: 0.005674258910052636, policy loss: 5.665735904644724
Experience 12, Iter 33, disc loss: 0.005501377072983428, policy loss: 5.632309565450583
Experience 12, Iter 34, disc loss: 0.004957176462473633, policy loss: 5.643508573558968
Experience 12, Iter 35, disc loss: 0.005635255514465346, policy loss: 5.607372234704816
Experience 12, Iter 36, disc loss: 0.005474536467505593, policy loss: 5.5481325599383515
Experience 12, Iter 37, disc loss: 0.006081295083768194, policy loss: 5.6042363634804016
Experience 12, Iter 38, disc loss: 0.006232020568833855, policy loss: 5.501699740089633
Experience 12, Iter 39, disc loss: 0.005730395561481371, policy loss: 5.632295733428181
Experience 12, Iter 40, disc loss: 0.007491006513365896, policy loss: 5.520518195483952
Experience 12, Iter 41, disc loss: 0.004662331450048897, policy loss: 5.807510249803458
Experience 12, Iter 42, disc loss: 0.005575551783517697, policy loss: 5.727798300565231
Experience 12, Iter 43, disc loss: 0.005337883907769891, policy loss: 5.609510867459286
Experience 12, Iter 44, disc loss: 0.005098734580467986, policy loss: 5.633493301755131
Experience 12, Iter 45, disc loss: 0.005978661380199462, policy loss: 5.587289755450533
Experience 12, Iter 46, disc loss: 0.005558697302856326, policy loss: 5.637745010220433
Experience 12, Iter 47, disc loss: 0.005862159141373055, policy loss: 5.6068994058766135
Experience 12, Iter 48, disc loss: 0.004776345024290897, policy loss: 5.688126018406642
Experience 12, Iter 49, disc loss: 0.006428907901001737, policy loss: 5.476867293999976
Experience: 13
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0024],
        [0.0001]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0006, 0.0097, 0.0004], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0006, 0.0097, 0.0004])
N: 130
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([521.0000, 521.0000, 521.0000, 521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.538
Iter 2/2000 - Loss: 5.954
Iter 3/2000 - Loss: -5.260
Iter 4/2000 - Loss: -5.236
Iter 5/2000 - Loss: -1.416
Iter 6/2000 - Loss: -2.062
Iter 7/2000 - Loss: -5.012
Iter 8/2000 - Loss: -6.974
Iter 9/2000 - Loss: -6.860
Iter 10/2000 - Loss: -5.651
Iter 11/2000 - Loss: -4.823
Iter 12/2000 - Loss: -4.998
Iter 13/2000 - Loss: -5.844
Iter 14/2000 - Loss: -6.687
Iter 15/2000 - Loss: -7.072
Iter 16/2000 - Loss: -6.966
Iter 17/2000 - Loss: -6.626
Iter 18/2000 - Loss: -6.348
Iter 19/2000 - Loss: -6.290
Iter 20/2000 - Loss: -6.437
Iter 1981/2000 - Loss: -7.809
Iter 1982/2000 - Loss: -7.804
Iter 1983/2000 - Loss: -7.799
Iter 1984/2000 - Loss: -7.798
Iter 1985/2000 - Loss: -7.797
Iter 1986/2000 - Loss: -7.795
Iter 1987/2000 - Loss: -7.790
Iter 1988/2000 - Loss: -7.789
Iter 1989/2000 - Loss: -7.792
Iter 1990/2000 - Loss: -7.798
Iter 1991/2000 - Loss: -7.801
Iter 1992/2000 - Loss: -7.801
Iter 1993/2000 - Loss: -7.801
Iter 1994/2000 - Loss: -7.802
Iter 1995/2000 - Loss: -7.804
Iter 1996/2000 - Loss: -7.807
Iter 1997/2000 - Loss: -7.809
Iter 1998/2000 - Loss: -7.810
Iter 1999/2000 - Loss: -7.810
Iter 2000/2000 - Loss: -7.810
***AFTER OPTIMATION***
Noise Variance: tensor([[1.4828e-04],
        [1.2724e-04],
        [1.9257e-03],
        [9.0432e-05]])
Lengthscale: tensor([[[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]],

        [[9.7400e-04, 4.9750e-03, 2.2056e-03, 9.7541e-05, 2.4259e-07,
          3.3986e-03]]])
Signal Variance: tensor([0.0006, 0.0005, 0.0077, 0.0004])
Estimated target variance: tensor([0.0007, 0.0006, 0.0097, 0.0004])
N: 130
Signal to noise ratio: tensor([1.9867, 1.9839, 2.0018, 1.9787])
Bound on condition number: tensor([514.0945, 512.6737, 521.9624, 510.0081])
Policy Optimizer learning rate:
0.009874363283766069
Experience 13, Iter 0, disc loss: 0.0055869813723167005, policy loss: 5.559786896180286
Experience 13, Iter 1, disc loss: 0.006384635458548377, policy loss: 5.4849971107892905
Experience 13, Iter 2, disc loss: 0.00573169686618772, policy loss: 5.529234695081925
Experience 13, Iter 3, disc loss: 0.005226135727342783, policy loss: 5.630528376172801
Experience 13, Iter 4, disc loss: 0.005687505302218918, policy loss: 5.619958215494906
Experience 13, Iter 5, disc loss: 0.005286001533280137, policy loss: 5.6386185036498055
Experience 13, Iter 6, disc loss: 0.004450241522156092, policy loss: 5.765777532933715
Experience 13, Iter 7, disc loss: 0.00580543464228893, policy loss: 5.66266251282244
Experience 13, Iter 8, disc loss: 0.005941613960188537, policy loss: 5.578453811934915
Experience 13, Iter 9, disc loss: 0.006328455781588321, policy loss: 5.463951112035496
Experience 13, Iter 10, disc loss: 0.006235749233854826, policy loss: 5.683757729685572
Experience 13, Iter 11, disc loss: 0.005006634162772161, policy loss: 5.757651158266887
Experience 13, Iter 12, disc loss: 0.004744684746924393, policy loss: 5.713815107198905
Experience 13, Iter 13, disc loss: 0.004617074391827751, policy loss: 5.802351730684193
Experience 13, Iter 14, disc loss: 0.004907094720775044, policy loss: 5.7630487082965525
Experience 13, Iter 15, disc loss: 0.004474583717555692, policy loss: 5.758671467420653
Experience 13, Iter 16, disc loss: 0.004925607855563092, policy loss: 5.756454752958876
Experience 13, Iter 17, disc loss: 0.006156740500449443, policy loss: 5.693814156035252
Experience 13, Iter 18, disc loss: 0.004667276797398557, policy loss: 5.758938202628136
Experience 13, Iter 19, disc loss: 0.006600870111859092, policy loss: 5.533087815026008
Experience 13, Iter 20, disc loss: 0.004923631281735061, policy loss: 5.733220806627428
Experience 13, Iter 21, disc loss: 0.0048907066225129, policy loss: 5.7332285083449825
Experience 13, Iter 22, disc loss: 0.004262020339531485, policy loss: 5.811834993715803
Experience 13, Iter 23, disc loss: 0.004215142115401016, policy loss: 5.8949572341334235
Experience 13, Iter 24, disc loss: 0.004330181142454843, policy loss: 5.894563961314827
Experience 13, Iter 25, disc loss: 0.004799608696176993, policy loss: 5.714499086077485
Experience 13, Iter 26, disc loss: 0.004301796810507006, policy loss: 5.740625727707902
Experience 13, Iter 27, disc loss: 0.004966675638279234, policy loss: 5.7699606430387504
Experience 13, Iter 28, disc loss: 0.004414556790253831, policy loss: 5.831680115864996
Experience 13, Iter 29, disc loss: 0.004206990449837806, policy loss: 6.008187679463051
Experience 13, Iter 30, disc loss: 0.0051550520976114365, policy loss: 5.592513546188353
Experience 13, Iter 31, disc loss: 0.0046776430131002435, policy loss: 5.775650487517915
Experience 13, Iter 32, disc loss: 0.005124030687510051, policy loss: 5.753412832423148
Experience 13, Iter 33, disc loss: 0.004628322413069312, policy loss: 5.72970371437828
Experience 13, Iter 34, disc loss: 0.003844167309671234, policy loss: 5.934144779530342
Experience 13, Iter 35, disc loss: 0.00538700642095206, policy loss: 5.7220952276039245
Experience 13, Iter 36, disc loss: 0.005580531779512241, policy loss: 5.584700422535365
Experience 13, Iter 37, disc loss: 0.006819372457110567, policy loss: 5.5763937447422425
Experience 13, Iter 38, disc loss: 0.005962981439783556, policy loss: 5.519302949863608
Experience 13, Iter 39, disc loss: 0.004103296958373046, policy loss: 6.022638501325158
Experience 13, Iter 40, disc loss: 0.0037247300771556862, policy loss: 6.019534145742238
Experience 13, Iter 41, disc loss: 0.004141367288892424, policy loss: 5.793341810422811
Experience 13, Iter 42, disc loss: 0.004448879867478183, policy loss: 5.850681882874058
Experience 13, Iter 43, disc loss: 0.0048677714818041356, policy loss: 5.9254661438598335
Experience 13, Iter 44, disc loss: 0.005708578060177133, policy loss: 5.720976870586648
Experience 13, Iter 45, disc loss: 0.003978801917286556, policy loss: 6.0277048894853875
Experience 13, Iter 46, disc loss: 0.00401419706601578, policy loss: 5.991202581697148
Experience 13, Iter 47, disc loss: 0.006206452310233737, policy loss: 5.530889172855346
Experience 13, Iter 48, disc loss: 0.004749282949154689, policy loss: 5.770938647088532
Experience 13, Iter 49, disc loss: 0.004640586951283643, policy loss: 5.8775811240073015
Experience: 14
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0048],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0008, 0.0193, 0.0007], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0008, 0.0193, 0.0007])
N: 140
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([561.0000, 561.0000, 561.0000, 561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.349
Iter 2/2000 - Loss: 3.110
Iter 3/2000 - Loss: -5.177
Iter 4/2000 - Loss: -5.295
Iter 5/2000 - Loss: -2.444
Iter 6/2000 - Loss: -2.907
Iter 7/2000 - Loss: -5.095
Iter 8/2000 - Loss: -6.502
Iter 9/2000 - Loss: -6.337
Iter 10/2000 - Loss: -5.409
Iter 11/2000 - Loss: -4.840
Iter 12/2000 - Loss: -5.037
Iter 13/2000 - Loss: -5.693
Iter 14/2000 - Loss: -6.287
Iter 15/2000 - Loss: -6.506
Iter 16/2000 - Loss: -6.366
Iter 17/2000 - Loss: -6.084
Iter 18/2000 - Loss: -5.895
Iter 19/2000 - Loss: -5.912
Iter 20/2000 - Loss: -6.092
Iter 1981/2000 - Loss: -7.128
Iter 1982/2000 - Loss: -7.124
Iter 1983/2000 - Loss: -7.127
Iter 1984/2000 - Loss: -7.139
Iter 1985/2000 - Loss: -7.153
Iter 1986/2000 - Loss: -7.158
Iter 1987/2000 - Loss: -7.153
Iter 1988/2000 - Loss: -7.145
Iter 1989/2000 - Loss: -7.142
Iter 1990/2000 - Loss: -7.144
Iter 1991/2000 - Loss: -7.149
Iter 1992/2000 - Loss: -7.155
Iter 1993/2000 - Loss: -7.158
Iter 1994/2000 - Loss: -7.159
Iter 1995/2000 - Loss: -7.159
Iter 1996/2000 - Loss: -7.158
Iter 1997/2000 - Loss: -7.155
Iter 1998/2000 - Loss: -7.151
Iter 1999/2000 - Loss: -7.144
Iter 2000/2000 - Loss: -7.133
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0002],
        [0.0038],
        [0.0001]])
Lengthscale: tensor([[[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]],

        [[1.0867e-03, 4.7499e-03, 4.7116e-03, 1.7180e-04, 5.2574e-07,
          4.1542e-03]]])
Signal Variance: tensor([0.0006, 0.0006, 0.0153, 0.0005])
Estimated target variance: tensor([0.0007, 0.0008, 0.0193, 0.0007])
N: 140
Signal to noise ratio: tensor([1.9862, 1.9876, 2.0073, 1.9847])
Bound on condition number: tensor([553.2741, 554.0701, 565.1174, 552.4544])
Policy Optimizer learning rate:
0.009863965082453532
Experience 14, Iter 0, disc loss: 0.006560537037328995, policy loss: 5.408220196273403
Experience 14, Iter 1, disc loss: 0.011338370624942171, policy loss: 5.314563573140231
Experience 14, Iter 2, disc loss: 0.008155058447619209, policy loss: 5.341036119897881
Experience 14, Iter 3, disc loss: 0.010480199697506186, policy loss: 5.302374052566827
Experience 14, Iter 4, disc loss: 0.008067975542101273, policy loss: 5.48304588592265
Experience 14, Iter 5, disc loss: 0.007361202982056783, policy loss: 5.372443414300406
Experience 14, Iter 6, disc loss: 0.007436584374117422, policy loss: 5.345686515801228
Experience 14, Iter 7, disc loss: 0.00711877884319374, policy loss: 5.443434806695899
Experience 14, Iter 8, disc loss: 0.005887443392344742, policy loss: 5.6500517528009855
Experience 14, Iter 9, disc loss: 0.007822779573529549, policy loss: 5.429195859303887
Experience 14, Iter 10, disc loss: 0.007931931312239144, policy loss: 5.569150482721907
Experience 14, Iter 11, disc loss: 0.0062838328927145234, policy loss: 5.732961621567582
Experience 14, Iter 12, disc loss: 0.0059529258120537215, policy loss: 5.707343035497934
Experience 14, Iter 13, disc loss: 0.006197530425975352, policy loss: 5.498776734873481
Experience 14, Iter 14, disc loss: 0.00700302060304641, policy loss: 5.547536456488289
Experience 14, Iter 15, disc loss: 0.008082179009552987, policy loss: 5.434390584896231
Experience 14, Iter 16, disc loss: 0.006101217944623406, policy loss: 5.6349081857077215
Experience 14, Iter 17, disc loss: 0.0060297781488971094, policy loss: 5.627146234661726
Experience 14, Iter 18, disc loss: 0.006131277472354092, policy loss: 5.505560737181909
Experience 14, Iter 19, disc loss: 0.006397488086177236, policy loss: 5.6538056155383885
Experience 14, Iter 20, disc loss: 0.005281541980056286, policy loss: 5.735559994361117
Experience 14, Iter 21, disc loss: 0.006098602167461579, policy loss: 5.650694666612299
Experience 14, Iter 22, disc loss: 0.007197629391666301, policy loss: 5.660349509472123
Experience 14, Iter 23, disc loss: 0.00562881225521543, policy loss: 5.674191678963123
Experience 14, Iter 24, disc loss: 0.004786240831665496, policy loss: 5.964478893826067
Experience 14, Iter 25, disc loss: 0.008741317351068098, policy loss: 5.3464709713529155
Experience 14, Iter 26, disc loss: 0.005719344689592335, policy loss: 5.552950824339652
Experience 14, Iter 27, disc loss: 0.006868323603327685, policy loss: 5.444889874548943
Experience 14, Iter 28, disc loss: 0.0057122316254893885, policy loss: 5.842094583877506
Experience 14, Iter 29, disc loss: 0.006709185010136138, policy loss: 5.660818244056035
Experience 14, Iter 30, disc loss: 0.006600218714809927, policy loss: 5.67340480761164
Experience 14, Iter 31, disc loss: 0.007250282117373006, policy loss: 5.617132904585055
Experience 14, Iter 32, disc loss: 0.00550749165879976, policy loss: 5.634968984433612
Experience 14, Iter 33, disc loss: 0.009052336835572421, policy loss: 5.484272608365929
Experience 14, Iter 34, disc loss: 0.006571870635789407, policy loss: 5.651151051306309
Experience 14, Iter 35, disc loss: 0.0080659279354106, policy loss: 5.436274135543095
Experience 14, Iter 36, disc loss: 0.0060190450563201055, policy loss: 5.635471975652385
Experience 14, Iter 37, disc loss: 0.00600177543825923, policy loss: 5.748790058089059
Experience 14, Iter 38, disc loss: 0.005741734822853933, policy loss: 5.803047335685113
Experience 14, Iter 39, disc loss: 0.006231784761108758, policy loss: 5.5862049700907965
Experience 14, Iter 40, disc loss: 0.007071128900141916, policy loss: 5.500646068370386
Experience 14, Iter 41, disc loss: 0.005460374280390372, policy loss: 6.022976239697581
Experience 14, Iter 42, disc loss: 0.004459721699652571, policy loss: 5.905721949936458
Experience 14, Iter 43, disc loss: 0.006016266899311378, policy loss: 5.791656397897115
Experience 14, Iter 44, disc loss: 0.005432371577335537, policy loss: 5.839664719253479
Experience 14, Iter 45, disc loss: 0.006138443572728227, policy loss: 5.732614486075499
Experience 14, Iter 46, disc loss: 0.006452927990773429, policy loss: 5.63549316239599
Experience 14, Iter 47, disc loss: 0.004121932627149009, policy loss: 5.962775748977126
Experience 14, Iter 48, disc loss: 0.0061348468416179336, policy loss: 5.674806872974669
Experience 14, Iter 49, disc loss: 0.005923043033293644, policy loss: 5.760554997185532
Experience: 15
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0047],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0008, 0.0189, 0.0007], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0008, 0.0189, 0.0007])
N: 150
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([601.0000, 601.0000, 601.0000, 601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.412
Iter 2/2000 - Loss: 3.209
Iter 3/2000 - Loss: -5.221
Iter 4/2000 - Loss: -5.278
Iter 5/2000 - Loss: -2.386
Iter 6/2000 - Loss: -2.887
Iter 7/2000 - Loss: -5.116
Iter 8/2000 - Loss: -6.526
Iter 9/2000 - Loss: -6.338
Iter 10/2000 - Loss: -5.390
Iter 11/2000 - Loss: -4.820
Iter 12/2000 - Loss: -5.034
Iter 13/2000 - Loss: -5.708
Iter 14/2000 - Loss: -6.311
Iter 15/2000 - Loss: -6.525
Iter 16/2000 - Loss: -6.371
Iter 17/2000 - Loss: -6.076
Iter 18/2000 - Loss: -5.885
Iter 19/2000 - Loss: -5.909
Iter 20/2000 - Loss: -6.103
Iter 1981/2000 - Loss: -7.170
Iter 1982/2000 - Loss: -7.170
Iter 1983/2000 - Loss: -7.170
Iter 1984/2000 - Loss: -7.170
Iter 1985/2000 - Loss: -7.170
Iter 1986/2000 - Loss: -7.170
Iter 1987/2000 - Loss: -7.170
Iter 1988/2000 - Loss: -7.170
Iter 1989/2000 - Loss: -7.170
Iter 1990/2000 - Loss: -7.170
Iter 1991/2000 - Loss: -7.170
Iter 1992/2000 - Loss: -7.170
Iter 1993/2000 - Loss: -7.170
Iter 1994/2000 - Loss: -7.170
Iter 1995/2000 - Loss: -7.170
Iter 1996/2000 - Loss: -7.170
Iter 1997/2000 - Loss: -7.170
Iter 1998/2000 - Loss: -7.170
Iter 1999/2000 - Loss: -7.170
Iter 2000/2000 - Loss: -7.170
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0001],
        [0.0002],
        [0.0037],
        [0.0001]])
Lengthscale: tensor([[[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]],

        [[1.1039e-03, 4.5992e-03, 4.5626e-03, 1.6739e-04, 5.0095e-07,
          4.1296e-03]]])
Signal Variance: tensor([0.0006, 0.0006, 0.0150, 0.0005])
Estimated target variance: tensor([0.0007, 0.0008, 0.0189, 0.0007])
N: 150
Signal to noise ratio: tensor([1.9854, 1.9878, 2.0066, 1.9862])
Bound on condition number: tensor([592.2924, 593.7090, 604.9645, 592.7748])
Policy Optimizer learning rate:
0.009853577830970107
Experience 15, Iter 0, disc loss: 0.0033634795296717227, policy loss: 6.135482837808235
Experience 15, Iter 1, disc loss: 0.005336236751006, policy loss: 6.006987846298521
Experience 15, Iter 2, disc loss: 0.004478510532468287, policy loss: 5.993673148895555
Experience 15, Iter 3, disc loss: 0.004435130677738632, policy loss: 5.983634580562786
Experience 15, Iter 4, disc loss: 0.004508936380500391, policy loss: 5.987217264539073
Experience 15, Iter 5, disc loss: 0.0036902447148020783, policy loss: 6.226467821298401
Experience 15, Iter 6, disc loss: 0.005634444593125717, policy loss: 5.821898888337795
Experience 15, Iter 7, disc loss: 0.00362453083529696, policy loss: 6.152112115709181
Experience 15, Iter 8, disc loss: 0.0046715129645377, policy loss: 5.930444684061337
Experience 15, Iter 9, disc loss: 0.004285408078955252, policy loss: 5.9980786850537875
Experience 15, Iter 10, disc loss: 0.004375830334940686, policy loss: 6.292395402366434
Experience 15, Iter 11, disc loss: 0.004857703664262992, policy loss: 5.895356404926565
Experience 15, Iter 12, disc loss: 0.0029130749690215708, policy loss: 6.334287319220573
Experience 15, Iter 13, disc loss: 0.0043471093469718575, policy loss: 6.137502331668199
Experience 15, Iter 14, disc loss: 0.0044483625071672, policy loss: 5.995211739845102
Experience 15, Iter 15, disc loss: 0.0038991186707951577, policy loss: 6.18249843514034
Experience 15, Iter 16, disc loss: 0.005524896004771076, policy loss: 5.9025905410005155
Experience 15, Iter 17, disc loss: 0.00433811093727599, policy loss: 5.942539959038309
Experience 15, Iter 18, disc loss: 0.00447874565342827, policy loss: 5.907591611620731
Experience 15, Iter 19, disc loss: 0.0040586786002888295, policy loss: 6.151061887634025
Experience 15, Iter 20, disc loss: 0.0038692074762759487, policy loss: 6.19408576205079
Experience 15, Iter 21, disc loss: 0.005078933662822627, policy loss: 5.97349977761761
Experience 15, Iter 22, disc loss: 0.003398552740593032, policy loss: 6.215495710015298
Experience 15, Iter 23, disc loss: 0.0032954880316490003, policy loss: 6.337794915421027
Experience 15, Iter 24, disc loss: 0.00406248725679971, policy loss: 6.270099748255027
Experience 15, Iter 25, disc loss: 0.0042679381319339835, policy loss: 6.049447795413965
Experience 15, Iter 26, disc loss: 0.00391947290249207, policy loss: 6.0922920332905885
Experience 15, Iter 27, disc loss: 0.003450775006566428, policy loss: 6.195931382944105
Experience 15, Iter 28, disc loss: 0.004022627065139065, policy loss: 6.23403814137763
Experience 15, Iter 29, disc loss: 0.0034301962386301087, policy loss: 6.214678658958139
Experience 15, Iter 30, disc loss: 0.0025687333339000293, policy loss: 6.3899620548971345
Experience 15, Iter 31, disc loss: 0.00446483230266586, policy loss: 5.960910868073473
Experience 15, Iter 32, disc loss: 0.004128916150531428, policy loss: 6.159510582985215
Experience 15, Iter 33, disc loss: 0.004130520673900164, policy loss: 6.118000405465779
Experience 15, Iter 34, disc loss: 0.003657593808523145, policy loss: 6.153697856680196
Experience 15, Iter 35, disc loss: 0.0028414364219636035, policy loss: 6.520926261497148
Experience 15, Iter 36, disc loss: 0.003225826653214696, policy loss: 6.41771857702306
Experience 15, Iter 37, disc loss: 0.0032082535024754374, policy loss: 6.2993797898326935
Experience 15, Iter 38, disc loss: 0.0037742153035894735, policy loss: 6.198624421660108
Experience 15, Iter 39, disc loss: 0.002825815997981404, policy loss: 6.414182695334757
Experience 15, Iter 40, disc loss: 0.003519645922114806, policy loss: 6.175510791410049
Experience 15, Iter 41, disc loss: 0.004012689332984573, policy loss: 6.2147521974975035
Experience 15, Iter 42, disc loss: 0.003388463327531168, policy loss: 6.355023173926208
Experience 15, Iter 43, disc loss: 0.004643632437062899, policy loss: 6.024878071260352
Experience 15, Iter 44, disc loss: 0.0037197617323945676, policy loss: 6.17728880141642
Experience 15, Iter 45, disc loss: 0.004335217324908816, policy loss: 6.022047630435196
Experience 15, Iter 46, disc loss: 0.004054391816320055, policy loss: 6.213913448985549
Experience 15, Iter 47, disc loss: 0.003679527965403394, policy loss: 6.211700527280819
Experience 15, Iter 48, disc loss: 0.00361868747918038, policy loss: 6.1696141578253565
Experience 15, Iter 49, disc loss: 0.0032727891051971, policy loss: 6.436801775477274
Experience: 16
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0056],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0007, 0.0009, 0.0223, 0.0007], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0007, 0.0009, 0.0223, 0.0007])
N: 160
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([641.0000, 641.0000, 641.0000, 641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.351
Iter 2/2000 - Loss: 2.731
Iter 3/2000 - Loss: -5.181
Iter 4/2000 - Loss: -5.246
Iter 5/2000 - Loss: -2.524
Iter 6/2000 - Loss: -3.011
Iter 7/2000 - Loss: -5.108
Iter 8/2000 - Loss: -6.402
Iter 9/2000 - Loss: -6.190
Iter 10/2000 - Loss: -5.294
Iter 11/2000 - Loss: -4.782
Iter 12/2000 - Loss: -5.009
Iter 13/2000 - Loss: -5.650
Iter 14/2000 - Loss: -6.205
Iter 15/2000 - Loss: -6.385
Iter 16/2000 - Loss: -6.220
Iter 17/2000 - Loss: -5.936
Iter 18/2000 - Loss: -5.768
Iter 19/2000 - Loss: -5.817
Iter 20/2000 - Loss: -6.018
Iter 1981/2000 - Loss: -6.954
Iter 1982/2000 - Loss: -6.961
Iter 1983/2000 - Loss: -6.980
Iter 1984/2000 - Loss: -6.997
Iter 1985/2000 - Loss: -6.995
Iter 1986/2000 - Loss: -6.982
Iter 1987/2000 - Loss: -6.979
Iter 1988/2000 - Loss: -6.990
Iter 1989/2000 - Loss: -7.003
Iter 1990/2000 - Loss: -7.006
Iter 1991/2000 - Loss: -6.999
Iter 1992/2000 - Loss: -6.988
Iter 1993/2000 - Loss: -6.973
Iter 1994/2000 - Loss: -6.953
Iter 1995/2000 - Loss: -6.927
Iter 1996/2000 - Loss: -6.901
Iter 1997/2000 - Loss: -6.900
Iter 1998/2000 - Loss: -6.945
Iter 1999/2000 - Loss: -6.992
Iter 2000/2000 - Loss: -6.973
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0002],
        [0.0044],
        [0.0001]])
Lengthscale: tensor([[[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]],

        [[1.2011e-03, 4.5900e-03, 5.2053e-03, 1.9230e-04, 5.5196e-07,
          4.5048e-03]]])
Signal Variance: tensor([0.0007, 0.0007, 0.0177, 0.0006])
Estimated target variance: tensor([0.0007, 0.0009, 0.0223, 0.0007])
N: 160
Signal to noise ratio: tensor([1.9974, 1.9893, 2.0079, 1.9860])
Bound on condition number: tensor([639.3578, 634.1954, 646.0424, 632.0707])
Policy Optimizer learning rate:
0.009843201517785073
Experience 16, Iter 0, disc loss: 0.0028153957925216235, policy loss: 6.657563547766912
Experience 16, Iter 1, disc loss: 0.0024805030497564522, policy loss: 6.54121598277002
Experience 16, Iter 2, disc loss: 0.002568411906732307, policy loss: 6.585191748049324
Experience 16, Iter 3, disc loss: 0.003112684338319455, policy loss: 6.483982902510142
Experience 16, Iter 4, disc loss: 0.002599473395855829, policy loss: 6.634161142101453
Experience 16, Iter 5, disc loss: 0.002927682447509328, policy loss: 6.494096050902316
Experience 16, Iter 6, disc loss: 0.0027443867011623683, policy loss: 6.806896893548482
Experience 16, Iter 7, disc loss: 0.00251469197077826, policy loss: 6.676950906146238
Experience 16, Iter 8, disc loss: 0.002416096555933568, policy loss: 6.686119517870074
Experience 16, Iter 9, disc loss: 0.00223805859876573, policy loss: 6.642708963763731
Experience 16, Iter 10, disc loss: 0.004659675198084656, policy loss: 6.539240367677534
Experience 16, Iter 11, disc loss: 0.0033162348584193854, policy loss: 6.3350242234009855
Experience 16, Iter 12, disc loss: 0.002874170773532784, policy loss: 6.540860555696224
Experience 16, Iter 13, disc loss: 0.0018718750704449269, policy loss: 6.779563150984623
Experience 16, Iter 14, disc loss: 0.0019325281101493946, policy loss: 6.944680958014347
Experience 16, Iter 15, disc loss: 0.0030064206580001844, policy loss: 6.52883077235146
Experience 16, Iter 16, disc loss: 0.0022127365820226357, policy loss: 6.761190525293128
Experience 16, Iter 17, disc loss: 0.002527514862703903, policy loss: 6.59386500665444
Experience 16, Iter 18, disc loss: 0.002908284603047701, policy loss: 6.7488554483593415
Experience 16, Iter 19, disc loss: 0.002196502243690487, policy loss: 6.809948492274097
Experience 16, Iter 20, disc loss: 0.0024559975883382775, policy loss: 6.72956728149098
Experience 16, Iter 21, disc loss: 0.0026215123324607945, policy loss: 6.653063685337914
Experience 16, Iter 22, disc loss: 0.0018919424128678905, policy loss: 6.877063843971414
Experience 16, Iter 23, disc loss: 0.0024076973674798524, policy loss: 6.920421567455814
Experience 16, Iter 24, disc loss: 0.003312853250927076, policy loss: 6.497354359760903
Experience 16, Iter 25, disc loss: 0.0026425313709173824, policy loss: 6.587384081353246
Experience 16, Iter 26, disc loss: 0.002463887426327885, policy loss: 6.69243650853995
Experience 16, Iter 27, disc loss: 0.002178111609948501, policy loss: 6.89879961924562
Experience 16, Iter 28, disc loss: 0.0021957656115360155, policy loss: 6.824383601328348
Experience 16, Iter 29, disc loss: 0.0022755252378698224, policy loss: 6.789298317187354
Experience 16, Iter 30, disc loss: 0.003056114744409642, policy loss: 6.365458831899426
Experience 16, Iter 31, disc loss: 0.0018648991147740242, policy loss: 6.856056714761564
Experience 16, Iter 32, disc loss: 0.0027581416325257488, policy loss: 6.540800422241462
Experience 16, Iter 33, disc loss: 0.0027267030439730544, policy loss: 6.711141905662824
Experience 16, Iter 34, disc loss: 0.002154435075719797, policy loss: 6.914999863261325
Experience 16, Iter 35, disc loss: 0.0025690083706745377, policy loss: 6.565854983620698
Experience 16, Iter 36, disc loss: 0.0021053128966094787, policy loss: 6.784310725492246
Experience 16, Iter 37, disc loss: 0.0025072760299319724, policy loss: 6.851493317608359
Experience 16, Iter 38, disc loss: 0.0023211372944982895, policy loss: 6.895932751192854
Experience 16, Iter 39, disc loss: 0.0016034175374045694, policy loss: 6.953544222908414
Experience 16, Iter 40, disc loss: 0.0021939025335064076, policy loss: 6.794711235228574
Experience 16, Iter 41, disc loss: 0.0026158498670646214, policy loss: 6.5227449684061884
Experience 16, Iter 42, disc loss: 0.004439672140631662, policy loss: 6.481232742887641
Experience 16, Iter 43, disc loss: 0.002444839400182938, policy loss: 6.726242862181449
Experience 16, Iter 44, disc loss: 0.002131869354627917, policy loss: 6.723755383267636
Experience 16, Iter 45, disc loss: 0.0026167019491897575, policy loss: 6.750119339424434
Experience 16, Iter 46, disc loss: 0.002580897457441532, policy loss: 6.546230116404439
Experience 16, Iter 47, disc loss: 0.002506803177093318, policy loss: 6.576969584694512
Experience 16, Iter 48, disc loss: 0.0019961671154167305, policy loss: 6.805531141209783
Experience 16, Iter 49, disc loss: 0.0019275401558947148, policy loss: 6.932061814415222
Experience: 17
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0005],
        [0.0091],
        [0.0002]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0009, 0.0022, 0.0364, 0.0009], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0009, 0.0022, 0.0364, 0.0009])
N: 170
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([681.0000, 681.0000, 681.0000, 681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -5.038
Iter 2/2000 - Loss: 0.836
Iter 3/2000 - Loss: -4.890
Iter 4/2000 - Loss: -4.741
Iter 5/2000 - Loss: -2.775
Iter 6/2000 - Loss: -3.266
Iter 7/2000 - Loss: -4.792
Iter 8/2000 - Loss: -5.623
Iter 9/2000 - Loss: -5.386
Iter 10/2000 - Loss: -4.743
Iter 11/2000 - Loss: -4.436
Iter 12/2000 - Loss: -4.655
Iter 13/2000 - Loss: -5.136
Iter 14/2000 - Loss: -5.520
Iter 15/2000 - Loss: -5.611
Iter 16/2000 - Loss: -5.451
Iter 17/2000 - Loss: -5.240
Iter 18/2000 - Loss: -5.170
Iter 19/2000 - Loss: -5.280
Iter 20/2000 - Loss: -5.447
Iter 1981/2000 - Loss: -6.078
Iter 1982/2000 - Loss: -6.079
Iter 1983/2000 - Loss: -6.082
Iter 1984/2000 - Loss: -6.083
Iter 1985/2000 - Loss: -6.082
Iter 1986/2000 - Loss: -6.081
Iter 1987/2000 - Loss: -6.081
Iter 1988/2000 - Loss: -6.083
Iter 1989/2000 - Loss: -6.083
Iter 1990/2000 - Loss: -6.082
Iter 1991/2000 - Loss: -6.082
Iter 1992/2000 - Loss: -6.082
Iter 1993/2000 - Loss: -6.083
Iter 1994/2000 - Loss: -6.083
Iter 1995/2000 - Loss: -6.082
Iter 1996/2000 - Loss: -6.082
Iter 1997/2000 - Loss: -6.083
Iter 1998/2000 - Loss: -6.083
Iter 1999/2000 - Loss: -6.083
Iter 2000/2000 - Loss: -6.082
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0002],
        [0.0004],
        [0.0072],
        [0.0002]])
Lengthscale: tensor([[[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]],

        [[1.4034e-03, 6.5741e-03, 7.9612e-03, 3.5048e-04, 3.7260e-06,
          1.6138e-02]]])
Signal Variance: tensor([0.0007, 0.0017, 0.0290, 0.0007])
Estimated target variance: tensor([0.0009, 0.0022, 0.0364, 0.0009])
N: 170
Signal to noise ratio: tensor([1.9876, 1.9964, 2.0100, 1.9893])
Bound on condition number: tensor([672.6198, 678.5497, 687.8204, 673.7689])
Policy Optimizer learning rate:
0.009832836131379849
Experience 17, Iter 0, disc loss: 0.0026896511098954447, policy loss: 6.634690520989928
Experience 17, Iter 1, disc loss: 0.0018522146112356643, policy loss: 7.125034725436047
Experience 17, Iter 2, disc loss: 0.006241313226517445, policy loss: 6.275142320093902
Experience 17, Iter 3, disc loss: 0.002935917728639295, policy loss: 6.45545011156565
Experience 17, Iter 4, disc loss: 0.003282861438470144, policy loss: 6.655247585430027
Experience 17, Iter 5, disc loss: 0.0034763058091835477, policy loss: 6.830105171174423
Experience 17, Iter 6, disc loss: 0.00219938685097046, policy loss: 6.759017443346212
Experience 17, Iter 7, disc loss: 0.002948042163866622, policy loss: 6.787824323044231
Experience 17, Iter 8, disc loss: 0.0020608865279320436, policy loss: 6.936176487644681
Experience 17, Iter 9, disc loss: 0.003660883293402832, policy loss: 6.514007430805774
Experience 17, Iter 10, disc loss: 0.0035906493144668523, policy loss: 6.908625373876037
Experience 17, Iter 11, disc loss: 0.002523691555632618, policy loss: 6.782228867819782
Experience 17, Iter 12, disc loss: 0.0019840425620633417, policy loss: 6.9019759642017675
Experience 17, Iter 13, disc loss: 0.003967269195339506, policy loss: 6.477749702659805
Experience 17, Iter 14, disc loss: 0.002074823892532329, policy loss: 6.900243945642671
Experience 17, Iter 15, disc loss: 0.0022352266684234237, policy loss: 6.803403665173158
Experience 17, Iter 16, disc loss: 0.001690637262101062, policy loss: 7.002527179658127
Experience 17, Iter 17, disc loss: 0.0026336447314158546, policy loss: 6.7514655298923945
Experience 17, Iter 18, disc loss: 0.0025079812103022954, policy loss: 6.687407432329721
Experience 17, Iter 19, disc loss: 0.0030256903253887066, policy loss: 6.527232897955792
Experience 17, Iter 20, disc loss: 0.003775462466091685, policy loss: 6.660884363014394
Experience 17, Iter 21, disc loss: 0.00262987475957084, policy loss: 6.818315912168731
Experience 17, Iter 22, disc loss: 0.004053045069206835, policy loss: 6.547155023870953
Experience 17, Iter 23, disc loss: 0.0027147185256595346, policy loss: 6.864143151476304
Experience 17, Iter 24, disc loss: 0.0029907360018353968, policy loss: 6.74485439148515
Experience 17, Iter 25, disc loss: 0.00312176525461207, policy loss: 6.5792743163814436
Experience 17, Iter 26, disc loss: 0.0016023603449545434, policy loss: 7.041151867120486
Experience 17, Iter 27, disc loss: 0.002071397107369994, policy loss: 6.829714116730198
Experience 17, Iter 28, disc loss: 0.001569411945974849, policy loss: 7.0784175007412795
Experience 17, Iter 29, disc loss: 0.002318651797418581, policy loss: 6.871783960891305
Experience 17, Iter 30, disc loss: 0.0016826977254027842, policy loss: 7.003300156526223
Experience 17, Iter 31, disc loss: 0.0022654924169715684, policy loss: 6.795297799852477
Experience 17, Iter 32, disc loss: 0.0018847284060543221, policy loss: 6.962721653199456
Experience 17, Iter 33, disc loss: 0.0025870868712977104, policy loss: 6.708427181393515
Experience 17, Iter 34, disc loss: 0.0022181997026995354, policy loss: 7.065292289515316
Experience 17, Iter 35, disc loss: 0.002446385437746665, policy loss: 6.846373240496293
Experience 17, Iter 36, disc loss: 0.0022382986258611404, policy loss: 7.033201659207307
Experience 17, Iter 37, disc loss: 0.001390370202132319, policy loss: 7.338067842002593
Experience 17, Iter 38, disc loss: 0.002143377599158409, policy loss: 6.977353911462131
Experience 17, Iter 39, disc loss: 0.002858271873668528, policy loss: 6.734823201591331
Experience 17, Iter 40, disc loss: 0.0021953512286994833, policy loss: 6.94543988164077
Experience 17, Iter 41, disc loss: 0.0018170879507923288, policy loss: 7.025744877864314
Experience 17, Iter 42, disc loss: 0.002986995265558453, policy loss: 6.7885748765514276
Experience 17, Iter 43, disc loss: 0.0036427633547701597, policy loss: 6.577442610552634
Experience 17, Iter 44, disc loss: 0.0024546855062903797, policy loss: 6.719588355556259
Experience 17, Iter 45, disc loss: 0.002284270673418779, policy loss: 6.958502991730425
Experience 17, Iter 46, disc loss: 0.0018011125365428584, policy loss: 7.110682764235959
Experience 17, Iter 47, disc loss: 0.0022577175297650407, policy loss: 6.8895129249649365
Experience 17, Iter 48, disc loss: 0.0019035594639171256, policy loss: 6.938232276179553
Experience 17, Iter 49, disc loss: 0.0031898368340986293, policy loss: 6.7720903722082735
Experience: 18
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0005],
        [0.0018],
        [0.0127],
        [0.0003]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[2.6453e-03, 1.6565e-02, 1.1226e-02, 1.0491e-03, 2.2950e-05,
          8.2199e-02]],

        [[2.6453e-03, 1.6565e-02, 1.1226e-02, 1.0491e-03, 2.2950e-05,
          8.2199e-02]],

        [[2.6453e-03, 1.6565e-02, 1.1226e-02, 1.0491e-03, 2.2950e-05,
          8.2199e-02]],

        [[2.6453e-03, 1.6565e-02, 1.1226e-02, 1.0491e-03, 2.2950e-05,
          8.2199e-02]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0019, 0.0073, 0.0510, 0.0013], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0019, 0.0073, 0.0510, 0.0013])
N: 180
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([721.0000, 721.0000, 721.0000, 721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -4.519
Iter 2/2000 - Loss: 0.279
Iter 3/2000 - Loss: -4.308
Iter 4/2000 - Loss: -3.776
Iter 5/2000 - Loss: -2.197
Iter 6/2000 - Loss: -2.711
Iter 7/2000 - Loss: -3.991
Iter 8/2000 - Loss: -4.606
Iter 9/2000 - Loss: -4.329
Iter 10/2000 - Loss: -3.809
Iter 11/2000 - Loss: -3.621
Iter 12/2000 - Loss: -3.794
Iter 13/2000 - Loss: -4.102
Iter 14/2000 - Loss: -4.365
Iter 15/2000 - Loss: -4.459
Iter 16/2000 - Loss: -4.372
Iter 17/2000 - Loss: -4.226
Iter 18/2000 - Loss: -4.157
Iter 19/2000 - Loss: -4.197
Iter 20/2000 - Loss: -4.290
Iter 1981/2000 - Loss: -9.256
Iter 1982/2000 - Loss: -9.256
Iter 1983/2000 - Loss: -9.256
Iter 1984/2000 - Loss: -9.256
Iter 1985/2000 - Loss: -9.256
Iter 1986/2000 - Loss: -9.256
Iter 1987/2000 - Loss: -9.256
Iter 1988/2000 - Loss: -9.256
Iter 1989/2000 - Loss: -9.256
Iter 1990/2000 - Loss: -9.256
Iter 1991/2000 - Loss: -9.256
Iter 1992/2000 - Loss: -9.256
Iter 1993/2000 - Loss: -9.256
Iter 1994/2000 - Loss: -9.256
Iter 1995/2000 - Loss: -9.256
Iter 1996/2000 - Loss: -9.256
Iter 1997/2000 - Loss: -9.256
Iter 1998/2000 - Loss: -9.256
Iter 1999/2000 - Loss: -9.256
Iter 2000/2000 - Loss: -9.256
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[14.0625,  3.8726, 21.3053, 15.2222,  3.2027, 34.0925]],

        [[15.0665, 24.3458, 18.3163,  2.1853,  4.7240, 10.2087]],

        [[13.6905, 23.9579, 15.3398,  1.0502,  2.5376, 11.2966]],

        [[15.0539, 26.6990,  4.7350,  1.6362,  4.1927, 16.1287]]])
Signal Variance: tensor([0.0352, 0.4568, 6.4169, 0.0655])
Estimated target variance: tensor([0.0019, 0.0073, 0.0510, 0.0013])
N: 180
Signal to noise ratio: tensor([10.2887, 38.2445, 57.8187, 16.7544])
Bound on condition number: tensor([ 19055.3191, 263276.5540, 601741.9071,  50529.0562])
Policy Optimizer learning rate:
0.009822481660247987
Experience 18, Iter 0, disc loss: 0.6614847223530684, policy loss: 0.7804041028716232
Experience 18, Iter 1, disc loss: 0.7308008797753046, policy loss: 0.7200028900300202
Experience 18, Iter 2, disc loss: 0.7068840171298342, policy loss: 0.7682803808603669
Experience 18, Iter 3, disc loss: 0.7040366874740397, policy loss: 0.7683039379688098
Experience 18, Iter 4, disc loss: 0.7100626211408528, policy loss: 0.7652197208839078
Experience 18, Iter 5, disc loss: 0.6805125764321802, policy loss: 0.7923399847775284
Experience 18, Iter 6, disc loss: 0.6844718351920989, policy loss: 0.7923637212920783
Experience 18, Iter 7, disc loss: 0.617147825986603, policy loss: 0.8766478152983932
Experience 18, Iter 8, disc loss: 0.5599695542633701, policy loss: 0.9590160961919041
Experience 18, Iter 9, disc loss: 0.5461284853722308, policy loss: 0.9785069221832352
Experience 18, Iter 10, disc loss: 0.494611923429927, policy loss: 1.0612642885003778
Experience 18, Iter 11, disc loss: 0.4742599267848737, policy loss: 1.1041614560872262
Experience 18, Iter 12, disc loss: 0.46033434153363506, policy loss: 1.1143517280981683
Experience 18, Iter 13, disc loss: 0.41401969962618024, policy loss: 1.2042746566219393
Experience 18, Iter 14, disc loss: 0.3898032214139823, policy loss: 1.2470656089128263
Experience 18, Iter 15, disc loss: 0.3695118032351927, policy loss: 1.298479544628857
Experience 18, Iter 16, disc loss: 0.34370799970399857, policy loss: 1.3610503426920024
Experience 18, Iter 17, disc loss: 0.3284302497621981, policy loss: 1.4080225698837818
Experience 18, Iter 18, disc loss: 0.3082523098616481, policy loss: 1.466125932449718
Experience 18, Iter 19, disc loss: 0.3052181018921244, policy loss: 1.4859791810886431
Experience 18, Iter 20, disc loss: 0.294189498127631, policy loss: 1.529335001614503
Experience 18, Iter 21, disc loss: 0.28634663712171365, policy loss: 1.5580030902794018
Experience 18, Iter 22, disc loss: 0.2765177216445114, policy loss: 1.5998009088321945
Experience 18, Iter 23, disc loss: 0.2598266583457313, policy loss: 1.6776186626545244
Experience 18, Iter 24, disc loss: 0.23501170213452655, policy loss: 1.776012944430138
Experience 18, Iter 25, disc loss: 0.2353278451735657, policy loss: 1.802258367279109
Experience 18, Iter 26, disc loss: 0.24148310974224055, policy loss: 1.8086579571647148
Experience 18, Iter 27, disc loss: 0.22519075207779035, policy loss: 1.86690285419236
Experience 18, Iter 28, disc loss: 0.2069532277584209, policy loss: 1.9723510916746738
Experience 18, Iter 29, disc loss: 0.19838481395209223, policy loss: 1.9742827858579637
Experience 18, Iter 30, disc loss: 0.17351412876568847, policy loss: 2.117752907508575
Experience 18, Iter 31, disc loss: 0.154486525751389, policy loss: 2.2215874498075223
Experience 18, Iter 32, disc loss: 0.14974444732325465, policy loss: 2.278319486869403
Experience 18, Iter 33, disc loss: 0.15238593711212706, policy loss: 2.2207162090891077
Experience 18, Iter 34, disc loss: 0.12577564458831433, policy loss: 2.426849506804484
Experience 18, Iter 35, disc loss: 0.11271875771975068, policy loss: 2.571309980551861
Experience 18, Iter 36, disc loss: 0.11106621765275473, policy loss: 2.5414253010721692
Experience 18, Iter 37, disc loss: 0.11436337051593097, policy loss: 2.458093619517497
Experience 18, Iter 38, disc loss: 0.09602800748854304, policy loss: 2.6237951193415743
Experience 18, Iter 39, disc loss: 0.08276315884423711, policy loss: 2.7887803854638396
Experience 18, Iter 40, disc loss: 0.08987497036245934, policy loss: 2.645271084851722
Experience 18, Iter 41, disc loss: 0.08714649381331757, policy loss: 2.6921456239077055
Experience 18, Iter 42, disc loss: 0.07685190059527694, policy loss: 2.9469589353824492
Experience 18, Iter 43, disc loss: 0.07927520894578932, policy loss: 2.7646765424897954
Experience 18, Iter 44, disc loss: 0.07457853261458133, policy loss: 2.7941790602311487
Experience 18, Iter 45, disc loss: 0.07487194077829942, policy loss: 2.800060973639662
Experience 18, Iter 46, disc loss: 0.06746841608163691, policy loss: 2.8576040361988664
Experience 18, Iter 47, disc loss: 0.062540640976645, policy loss: 2.932643222004644
Experience 18, Iter 48, disc loss: 0.06925341372581427, policy loss: 2.876467963465858
Experience 18, Iter 49, disc loss: 0.05410003221399877, policy loss: 3.085352554031238
Experience: 19
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0106],
        [0.0734],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.8120e-03, 4.0568e-02, 4.0212e-02, 2.0962e-03, 1.6156e-04,
          3.5800e-01]],

        [[4.8120e-03, 4.0568e-02, 4.0212e-02, 2.0962e-03, 1.6156e-04,
          3.5800e-01]],

        [[4.8120e-03, 4.0568e-02, 4.0212e-02, 2.0962e-03, 1.6156e-04,
          3.5800e-01]],

        [[4.8120e-03, 4.0568e-02, 4.0212e-02, 2.0962e-03, 1.6156e-04,
          3.5800e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0426, 0.2938, 0.0035], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0426, 0.2938, 0.0035])
N: 190
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([761.0000, 761.0000, 761.0000, 761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.981
Iter 2/2000 - Loss: 0.022
Iter 3/2000 - Loss: -1.900
Iter 4/2000 - Loss: -1.621
Iter 5/2000 - Loss: -0.949
Iter 6/2000 - Loss: -1.168
Iter 7/2000 - Loss: -1.690
Iter 8/2000 - Loss: -1.950
Iter 9/2000 - Loss: -1.868
Iter 10/2000 - Loss: -1.665
Iter 11/2000 - Loss: -1.570
Iter 12/2000 - Loss: -1.645
Iter 13/2000 - Loss: -1.817
Iter 14/2000 - Loss: -1.984
Iter 15/2000 - Loss: -2.092
Iter 16/2000 - Loss: -2.147
Iter 17/2000 - Loss: -2.189
Iter 18/2000 - Loss: -2.259
Iter 19/2000 - Loss: -2.383
Iter 20/2000 - Loss: -2.561
Iter 1981/2000 - Loss: -9.079
Iter 1982/2000 - Loss: -9.079
Iter 1983/2000 - Loss: -9.079
Iter 1984/2000 - Loss: -9.079
Iter 1985/2000 - Loss: -9.079
Iter 1986/2000 - Loss: -9.079
Iter 1987/2000 - Loss: -9.079
Iter 1988/2000 - Loss: -9.079
Iter 1989/2000 - Loss: -9.079
Iter 1990/2000 - Loss: -9.079
Iter 1991/2000 - Loss: -9.079
Iter 1992/2000 - Loss: -9.079
Iter 1993/2000 - Loss: -9.079
Iter 1994/2000 - Loss: -9.079
Iter 1995/2000 - Loss: -9.079
Iter 1996/2000 - Loss: -9.079
Iter 1997/2000 - Loss: -9.079
Iter 1998/2000 - Loss: -9.079
Iter 1999/2000 - Loss: -9.079
Iter 2000/2000 - Loss: -9.079
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.9794,  6.1193, 23.5161, 13.3098,  5.7738, 44.4257]],

        [[13.5169, 25.2869, 15.8206,  4.4112,  8.4427, 28.1075]],

        [[12.6955, 16.5759, 20.2204,  1.2194,  6.3985, 16.8400]],

        [[13.4923, 25.3931, 10.4912,  2.4773,  7.9112, 34.7366]]])
Signal Variance: tensor([ 0.0714,  2.9029, 14.4031,  0.2159])
Estimated target variance: tensor([0.0046, 0.0426, 0.2938, 0.0035])
N: 190
Signal to noise ratio: tensor([14.6830, 96.5678, 85.0823, 30.5833])
Bound on condition number: tensor([  40963.0839, 1771815.2247, 1375409.5512,  177715.4207])
Policy Optimizer learning rate:
0.009812138092895157
Experience 19, Iter 0, disc loss: 0.04886075638441778, policy loss: 3.7798593890091343
Experience 19, Iter 1, disc loss: 0.050383693665067855, policy loss: 3.394869387431843
Experience 19, Iter 2, disc loss: 0.04299928139248233, policy loss: 3.3615959078723465
Experience 19, Iter 3, disc loss: 0.039379710858397456, policy loss: 3.3998991504591762
Experience 19, Iter 4, disc loss: 0.0351343704778392, policy loss: 3.5290696957674297
Experience 19, Iter 5, disc loss: 0.03713758983382304, policy loss: 3.4316840509487796
Experience 19, Iter 6, disc loss: 0.03614345156060769, policy loss: 3.458036990693373
Experience 19, Iter 7, disc loss: 0.03271938001587295, policy loss: 3.555868320748066
Experience 19, Iter 8, disc loss: 0.0311584853772827, policy loss: 3.605059740645391
Experience 19, Iter 9, disc loss: 0.030745810758964362, policy loss: 3.6120756991745524
Experience 19, Iter 10, disc loss: 0.030403272777392923, policy loss: 3.6203119935497967
Experience 19, Iter 11, disc loss: 0.029462852373062304, policy loss: 3.660637081373134
Experience 19, Iter 12, disc loss: 0.029777703411516445, policy loss: 3.6477805611805354
Experience 19, Iter 13, disc loss: 0.030569782554013276, policy loss: 3.613047867937138
Experience 19, Iter 14, disc loss: 0.03304403591432021, policy loss: 3.53104846696262
Experience 19, Iter 15, disc loss: 0.030871268828004048, policy loss: 3.6268982914965875
Experience 19, Iter 16, disc loss: 0.0321805367499433, policy loss: 3.563086214417597
Experience 19, Iter 17, disc loss: 0.03350357519155635, policy loss: 3.5187203230310615
Experience 19, Iter 18, disc loss: 0.03575052322836739, policy loss: 3.443567095755945
Experience 19, Iter 19, disc loss: 0.0325909270204917, policy loss: 3.5867833183761766
Experience 19, Iter 20, disc loss: 0.0324759473286248, policy loss: 3.5531594567376716
Experience 19, Iter 21, disc loss: 0.03677260285071841, policy loss: 3.414759082109644
Experience 19, Iter 22, disc loss: 0.03583474432449672, policy loss: 3.4682535213036116
Experience 19, Iter 23, disc loss: 0.033716458057927695, policy loss: 3.537398727327224
Experience 19, Iter 24, disc loss: 0.03197828986513427, policy loss: 3.602657961034888
Experience 19, Iter 25, disc loss: 0.031177406372656345, policy loss: 3.63853073934235
Experience 19, Iter 26, disc loss: 0.032626427162315556, policy loss: 3.602566867097254
Experience 19, Iter 27, disc loss: 0.02961664240258433, policy loss: 3.6822918307609322
Experience 19, Iter 28, disc loss: 0.03264575328605729, policy loss: 3.5638859646714343
Experience 19, Iter 29, disc loss: 0.03053933664231157, policy loss: 3.656839347607088
Experience 19, Iter 30, disc loss: 0.02790309300702588, policy loss: 3.746680310869986
Experience 19, Iter 31, disc loss: 0.028627538647712326, policy loss: 3.710820206162908
Experience 19, Iter 32, disc loss: 0.02808846044772833, policy loss: 3.7003892999637937
Experience 19, Iter 33, disc loss: 0.026698063301169767, policy loss: 3.7574604393259783
Experience 19, Iter 34, disc loss: 0.029122287976479543, policy loss: 3.690453530516256
Experience 19, Iter 35, disc loss: 0.025646431009233698, policy loss: 3.8499039477544965
Experience 19, Iter 36, disc loss: 0.025481778437491772, policy loss: 3.8455402850517606
Experience 19, Iter 37, disc loss: 0.025549889371453388, policy loss: 3.821842589376753
Experience 19, Iter 38, disc loss: 0.025585469256425408, policy loss: 3.8033887139616436
Experience 19, Iter 39, disc loss: 0.02550739213453821, policy loss: 3.7960328283676095
Experience 19, Iter 40, disc loss: 0.02418739052055469, policy loss: 3.849810265243013
Experience 19, Iter 41, disc loss: 0.025016086650000737, policy loss: 3.8355900763105817
Experience 19, Iter 42, disc loss: 0.02403730897094073, policy loss: 3.8541451521776597
Experience 19, Iter 43, disc loss: 0.025516134903597023, policy loss: 3.8264274755606644
Experience 19, Iter 44, disc loss: 0.025237366599065832, policy loss: 3.8514764497434912
Experience 19, Iter 45, disc loss: 0.024703672223509672, policy loss: 3.861483473754159
Experience 19, Iter 46, disc loss: 0.025061707241792092, policy loss: 3.83161085675922
Experience 19, Iter 47, disc loss: 0.023371566603211644, policy loss: 3.935016639085503
Experience 19, Iter 48, disc loss: 0.024209952891122582, policy loss: 3.8822058736742298
Experience 19, Iter 49, disc loss: 0.025141060277500037, policy loss: 3.8241478876089685
Experience: 20
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0012],
        [0.0106],
        [0.0710],
        [0.0009]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.1976e-03, 4.1232e-02, 3.8485e-02, 2.0735e-03, 1.5361e-04,
          3.5446e-01]],

        [[5.1976e-03, 4.1232e-02, 3.8485e-02, 2.0735e-03, 1.5361e-04,
          3.5446e-01]],

        [[5.1976e-03, 4.1232e-02, 3.8485e-02, 2.0735e-03, 1.5361e-04,
          3.5446e-01]],

        [[5.1976e-03, 4.1232e-02, 3.8485e-02, 2.0735e-03, 1.5361e-04,
          3.5446e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0047, 0.0422, 0.2840, 0.0034], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0047, 0.0422, 0.2840, 0.0034])
N: 200
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([801.0000, 801.0000, 801.0000, 801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.014
Iter 2/2000 - Loss: 0.088
Iter 3/2000 - Loss: -1.922
Iter 4/2000 - Loss: -1.633
Iter 5/2000 - Loss: -0.924
Iter 6/2000 - Loss: -1.141
Iter 7/2000 - Loss: -1.688
Iter 8/2000 - Loss: -1.976
Iter 9/2000 - Loss: -1.902
Iter 10/2000 - Loss: -1.685
Iter 11/2000 - Loss: -1.571
Iter 12/2000 - Loss: -1.638
Iter 13/2000 - Loss: -1.816
Iter 14/2000 - Loss: -1.994
Iter 15/2000 - Loss: -2.113
Iter 16/2000 - Loss: -2.173
Iter 17/2000 - Loss: -2.211
Iter 18/2000 - Loss: -2.273
Iter 19/2000 - Loss: -2.388
Iter 20/2000 - Loss: -2.559
Iter 1981/2000 - Loss: -9.096
Iter 1982/2000 - Loss: -9.096
Iter 1983/2000 - Loss: -9.096
Iter 1984/2000 - Loss: -9.096
Iter 1985/2000 - Loss: -9.096
Iter 1986/2000 - Loss: -9.096
Iter 1987/2000 - Loss: -9.096
Iter 1988/2000 - Loss: -9.096
Iter 1989/2000 - Loss: -9.096
Iter 1990/2000 - Loss: -9.096
Iter 1991/2000 - Loss: -9.096
Iter 1992/2000 - Loss: -9.096
Iter 1993/2000 - Loss: -9.096
Iter 1994/2000 - Loss: -9.096
Iter 1995/2000 - Loss: -9.096
Iter 1996/2000 - Loss: -9.096
Iter 1997/2000 - Loss: -9.096
Iter 1998/2000 - Loss: -9.096
Iter 1999/2000 - Loss: -9.096
Iter 2000/2000 - Loss: -9.096
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.8717,  5.9255, 22.8992, 12.9574,  6.2507, 42.8557]],

        [[12.3655, 25.1345, 14.7919,  4.3364,  8.4676, 28.0126]],

        [[13.7204, 17.9759, 19.8118,  1.2812,  6.5280, 17.2892]],

        [[14.2147, 25.2373, 10.1923,  2.4799,  7.7127, 33.6845]]])
Signal Variance: tensor([ 0.0658,  2.8744, 15.3914,  0.2056])
Estimated target variance: tensor([0.0047, 0.0422, 0.2840, 0.0034])
N: 200
Signal to noise ratio: tensor([14.3193, 96.7367, 87.1379, 29.2580])
Bound on condition number: tensor([  41009.1886, 1871599.8594, 1518605.4540,  171207.2156])
Policy Optimizer learning rate:
0.00980180541783913
Experience 20, Iter 0, disc loss: 0.02323723441622561, policy loss: 3.9173999135100503
Experience 20, Iter 1, disc loss: 0.024002388669443864, policy loss: 3.862835637858139
Experience 20, Iter 2, disc loss: 0.02425700442758712, policy loss: 3.841281468836195
Experience 20, Iter 3, disc loss: 0.02453793212413723, policy loss: 3.8334471569159523
Experience 20, Iter 4, disc loss: 0.02399868047581542, policy loss: 3.869074490701358
Experience 20, Iter 5, disc loss: 0.022822499321263207, policy loss: 3.915837913819022
Experience 20, Iter 6, disc loss: 0.023117875127312712, policy loss: 3.9046618834960243
Experience 20, Iter 7, disc loss: 0.022412506190683194, policy loss: 3.9371671727867126
Experience 20, Iter 8, disc loss: 0.022541353159840304, policy loss: 3.9223242907615616
Experience 20, Iter 9, disc loss: 0.02414311750263582, policy loss: 3.8503828788270917
Experience 20, Iter 10, disc loss: 0.02304003881103625, policy loss: 3.904495721101986
Experience 20, Iter 11, disc loss: 0.02212617489736404, policy loss: 3.926092478197994
Experience 20, Iter 12, disc loss: 0.022503473877250525, policy loss: 3.9072593902638624
Experience 20, Iter 13, disc loss: 0.022180929275269677, policy loss: 3.9366950526928504
Experience 20, Iter 14, disc loss: 0.022133954369070542, policy loss: 3.9342567140420464
Experience 20, Iter 15, disc loss: 0.02121234854118763, policy loss: 3.9968984005780337
Experience 20, Iter 16, disc loss: 0.021327670939956966, policy loss: 3.986399847436945
Experience 20, Iter 17, disc loss: 0.02177889987377781, policy loss: 3.9382660524390287
Experience 20, Iter 18, disc loss: 0.022652128911416693, policy loss: 3.895214693922089
Experience 20, Iter 19, disc loss: 0.020277211939308357, policy loss: 4.020254156569353
Experience 20, Iter 20, disc loss: 0.02043695642871298, policy loss: 3.9866455856874787
Experience 20, Iter 21, disc loss: 0.019955106012430328, policy loss: 4.015722613011887
Experience 20, Iter 22, disc loss: 0.019999264396171276, policy loss: 4.02447673478482
Experience 20, Iter 23, disc loss: 0.021408018820670983, policy loss: 3.944673040701235
Experience 20, Iter 24, disc loss: 0.020857856966247472, policy loss: 3.964513955885592
Experience 20, Iter 25, disc loss: 0.019927516600573927, policy loss: 4.007287910182171
Experience 20, Iter 26, disc loss: 0.02000649031228157, policy loss: 4.012809197954531
Experience 20, Iter 27, disc loss: 0.019560100681681757, policy loss: 4.0533710435471635
Experience 20, Iter 28, disc loss: 0.019423036596164504, policy loss: 4.037909887616523
Experience 20, Iter 29, disc loss: 0.02084300265381487, policy loss: 3.9588010812581182
Experience 20, Iter 30, disc loss: 0.020858418503978154, policy loss: 3.9625432192538286
Experience 20, Iter 31, disc loss: 0.01974303901740472, policy loss: 4.035246138395048
Experience 20, Iter 32, disc loss: 0.019591803759778068, policy loss: 4.027219687500012
Experience 20, Iter 33, disc loss: 0.017827479260947576, policy loss: 4.12859563726718
Experience 20, Iter 34, disc loss: 0.018681768244779698, policy loss: 4.093943971153944
Experience 20, Iter 35, disc loss: 0.019132415286720668, policy loss: 4.054859102830146
Experience 20, Iter 36, disc loss: 0.018913816715636342, policy loss: 4.078801122616005
Experience 20, Iter 37, disc loss: 0.017930267551423587, policy loss: 4.138386570537909
Experience 20, Iter 38, disc loss: 0.019196681879645586, policy loss: 4.041223964333018
Experience 20, Iter 39, disc loss: 0.017098175865862823, policy loss: 4.195523169485799
Experience 20, Iter 40, disc loss: 0.017352977588909767, policy loss: 4.153905464149123
Experience 20, Iter 41, disc loss: 0.01758550771763681, policy loss: 4.12465319530192
Experience 20, Iter 42, disc loss: 0.01736336633198647, policy loss: 4.156598503908889
Experience 20, Iter 43, disc loss: 0.017312193591933287, policy loss: 4.1419051412139964
Experience 20, Iter 44, disc loss: 0.017101388577934324, policy loss: 4.163365231288765
Experience 20, Iter 45, disc loss: 0.017607138861231653, policy loss: 4.135595937096703
Experience 20, Iter 46, disc loss: 0.016501298377156214, policy loss: 4.200263927193256
Experience 20, Iter 47, disc loss: 0.01721702992232397, policy loss: 4.163063308926736
Experience 20, Iter 48, disc loss: 0.017554272881796687, policy loss: 4.154539519190537
Experience 20, Iter 49, disc loss: 0.017078639113818187, policy loss: 4.157872281156612
Experience: 21
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0110],
        [0.0767],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.1516e-03, 4.0352e-02, 3.8586e-02, 2.0347e-03, 1.4643e-04,
          3.5703e-01]],

        [[5.1516e-03, 4.0352e-02, 3.8586e-02, 2.0347e-03, 1.4643e-04,
          3.5703e-01]],

        [[5.1516e-03, 4.0352e-02, 3.8586e-02, 2.0347e-03, 1.4643e-04,
          3.5703e-01]],

        [[5.1516e-03, 4.0352e-02, 3.8586e-02, 2.0347e-03, 1.4643e-04,
          3.5703e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0046, 0.0442, 0.3069, 0.0034], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0046, 0.0442, 0.3069, 0.0034])
N: 210
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([841.0000, 841.0000, 841.0000, 841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.975
Iter 2/2000 - Loss: 0.192
Iter 3/2000 - Loss: -1.877
Iter 4/2000 - Loss: -1.582
Iter 5/2000 - Loss: -0.849
Iter 6/2000 - Loss: -1.066
Iter 7/2000 - Loss: -1.630
Iter 8/2000 - Loss: -1.933
Iter 9/2000 - Loss: -1.860
Iter 10/2000 - Loss: -1.632
Iter 11/2000 - Loss: -1.506
Iter 12/2000 - Loss: -1.570
Iter 13/2000 - Loss: -1.752
Iter 14/2000 - Loss: -1.935
Iter 15/2000 - Loss: -2.054
Iter 16/2000 - Loss: -2.106
Iter 17/2000 - Loss: -2.135
Iter 18/2000 - Loss: -2.187
Iter 19/2000 - Loss: -2.295
Iter 20/2000 - Loss: -2.462
Iter 1981/2000 - Loss: -9.073
Iter 1982/2000 - Loss: -9.073
Iter 1983/2000 - Loss: -9.073
Iter 1984/2000 - Loss: -9.073
Iter 1985/2000 - Loss: -9.073
Iter 1986/2000 - Loss: -9.074
Iter 1987/2000 - Loss: -9.074
Iter 1988/2000 - Loss: -9.074
Iter 1989/2000 - Loss: -9.074
Iter 1990/2000 - Loss: -9.074
Iter 1991/2000 - Loss: -9.074
Iter 1992/2000 - Loss: -9.074
Iter 1993/2000 - Loss: -9.074
Iter 1994/2000 - Loss: -9.074
Iter 1995/2000 - Loss: -9.074
Iter 1996/2000 - Loss: -9.074
Iter 1997/2000 - Loss: -9.074
Iter 1998/2000 - Loss: -9.074
Iter 1999/2000 - Loss: -9.074
Iter 2000/2000 - Loss: -9.074
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.6255,  5.8088, 22.1395, 12.9357,  6.4353, 41.2394]],

        [[12.7635, 24.7745, 15.8368,  3.9820,  9.1153, 27.3322]],

        [[13.6545, 15.5815, 15.1313,  1.1449,  5.2394, 10.9169]],

        [[14.0571, 25.0807, 10.5132,  2.5617,  7.6133, 34.6570]]])
Signal Variance: tensor([ 0.0626,  2.8171, 10.8028,  0.2156])
Estimated target variance: tensor([0.0046, 0.0442, 0.3069, 0.0034])
N: 210
Signal to noise ratio: tensor([14.1840, 93.7818, 72.6722, 29.6638])
Bound on condition number: tensor([  42249.7656, 1846957.8030, 1109062.7356,  184788.1730])
Policy Optimizer learning rate:
0.009791483623609768
Experience 21, Iter 0, disc loss: 0.016543232496458005, policy loss: 4.186774724942555
Experience 21, Iter 1, disc loss: 0.017202182874319964, policy loss: 4.146328890089286
Experience 21, Iter 2, disc loss: 0.01757214986981101, policy loss: 4.117160367638535
Experience 21, Iter 3, disc loss: 0.016290599503876936, policy loss: 4.199150878404058
Experience 21, Iter 4, disc loss: 0.017654769824130646, policy loss: 4.123842856677741
Experience 21, Iter 5, disc loss: 0.016961726997672167, policy loss: 4.163495305162204
Experience 21, Iter 6, disc loss: 0.01669567524919511, policy loss: 4.168683856773416
Experience 21, Iter 7, disc loss: 0.01659625496014951, policy loss: 4.1737096449499544
Experience 21, Iter 8, disc loss: 0.015797162035761234, policy loss: 4.23558192151051
Experience 21, Iter 9, disc loss: 0.015968725006431216, policy loss: 4.225328003668491
Experience 21, Iter 10, disc loss: 0.01576744906825839, policy loss: 4.232416876070468
Experience 21, Iter 11, disc loss: 0.016211610794823028, policy loss: 4.212735784746501
Experience 21, Iter 12, disc loss: 0.016154588794023532, policy loss: 4.210412198333381
Experience 21, Iter 13, disc loss: 0.015544672517666606, policy loss: 4.245768370782941
Experience 21, Iter 14, disc loss: 0.014910090615542078, policy loss: 4.2953512077760365
Experience 21, Iter 15, disc loss: 0.01552640782579219, policy loss: 4.240810098105724
Experience 21, Iter 16, disc loss: 0.01433808929366207, policy loss: 4.324871886247775
Experience 21, Iter 17, disc loss: 0.014809740836088277, policy loss: 4.3031997880288015
Experience 21, Iter 18, disc loss: 0.015704967992075335, policy loss: 4.2291191968475985
Experience 21, Iter 19, disc loss: 0.01452465063204228, policy loss: 4.319139431232814
Experience 21, Iter 20, disc loss: 0.014929337032894943, policy loss: 4.284272450656325
Experience 21, Iter 21, disc loss: 0.015282853722899836, policy loss: 4.250424855149177
Experience 21, Iter 22, disc loss: 0.014615857572384101, policy loss: 4.290905956846977
Experience 21, Iter 23, disc loss: 0.013672785498651124, policy loss: 4.381110750675678
Experience 21, Iter 24, disc loss: 0.01462455217638872, policy loss: 4.316328441972726
Experience 21, Iter 25, disc loss: 0.014557278167678584, policy loss: 4.31092588614421
Experience 21, Iter 26, disc loss: 0.014601177467374639, policy loss: 4.303115690377945
Experience 21, Iter 27, disc loss: 0.014640502921064296, policy loss: 4.2980928365201345
Experience 21, Iter 28, disc loss: 0.01442791134159224, policy loss: 4.329511481468522
Experience 21, Iter 29, disc loss: 0.013658755753072387, policy loss: 4.375172689937213
Experience 21, Iter 30, disc loss: 0.01400959433977291, policy loss: 4.355879571411808
Experience 21, Iter 31, disc loss: 0.014211359255197304, policy loss: 4.331108114183169
Experience 21, Iter 32, disc loss: 0.01367223248924117, policy loss: 4.3679390842234636
Experience 21, Iter 33, disc loss: 0.013373083751417042, policy loss: 4.3894465063602786
Experience 21, Iter 34, disc loss: 0.013508404790207668, policy loss: 4.38074177115887
Experience 21, Iter 35, disc loss: 0.013078320834301637, policy loss: 4.413150177226278
Experience 21, Iter 36, disc loss: 0.012939053164030706, policy loss: 4.431946798443931
Experience 21, Iter 37, disc loss: 0.012571104809824447, policy loss: 4.458612172441055
Experience 21, Iter 38, disc loss: 0.013233881889986664, policy loss: 4.405597541234977
Experience 21, Iter 39, disc loss: 0.012760579859029036, policy loss: 4.435665421938219
Experience 21, Iter 40, disc loss: 0.013308601919572562, policy loss: 4.39992649778098
Experience 21, Iter 41, disc loss: 0.013698151596839597, policy loss: 4.370523720844178
Experience 21, Iter 42, disc loss: 0.013173340873700548, policy loss: 4.4079959707511325
Experience 21, Iter 43, disc loss: 0.012504754469318015, policy loss: 4.478767644880735
Experience 21, Iter 44, disc loss: 0.012569236318826973, policy loss: 4.451346416017988
Experience 21, Iter 45, disc loss: 0.01255104311271192, policy loss: 4.451872317572172
Experience 21, Iter 46, disc loss: 0.012457684480357526, policy loss: 4.459789644034954
Experience 21, Iter 47, disc loss: 0.01263063260413428, policy loss: 4.455630372806785
Experience 21, Iter 48, disc loss: 0.012622660660857422, policy loss: 4.439135157495283
Experience 21, Iter 49, disc loss: 0.01266440546449112, policy loss: 4.455891267580169
Experience: 22
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0107],
        [0.0743],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.4715e-03, 3.9254e-02, 3.7002e-02, 1.9624e-03, 1.4009e-04,
          3.4640e-01]],

        [[5.4715e-03, 3.9254e-02, 3.7002e-02, 1.9624e-03, 1.4009e-04,
          3.4640e-01]],

        [[5.4715e-03, 3.9254e-02, 3.7002e-02, 1.9624e-03, 1.4009e-04,
          3.4640e-01]],

        [[5.4715e-03, 3.9254e-02, 3.7002e-02, 1.9624e-03, 1.4009e-04,
          3.4640e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0045, 0.0430, 0.2973, 0.0032], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0045, 0.0430, 0.2973, 0.0032])
N: 220
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([881.0000, 881.0000, 881.0000, 881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.036
Iter 2/2000 - Loss: 0.251
Iter 3/2000 - Loss: -1.928
Iter 4/2000 - Loss: -1.625
Iter 5/2000 - Loss: -0.849
Iter 6/2000 - Loss: -1.063
Iter 7/2000 - Loss: -1.656
Iter 8/2000 - Loss: -1.987
Iter 9/2000 - Loss: -1.922
Iter 10/2000 - Loss: -1.681
Iter 11/2000 - Loss: -1.538
Iter 12/2000 - Loss: -1.592
Iter 13/2000 - Loss: -1.776
Iter 14/2000 - Loss: -1.968
Iter 15/2000 - Loss: -2.093
Iter 16/2000 - Loss: -2.146
Iter 17/2000 - Loss: -2.168
Iter 18/2000 - Loss: -2.211
Iter 19/2000 - Loss: -2.309
Iter 20/2000 - Loss: -2.468
Iter 1981/2000 - Loss: -9.129
Iter 1982/2000 - Loss: -9.129
Iter 1983/2000 - Loss: -9.129
Iter 1984/2000 - Loss: -9.129
Iter 1985/2000 - Loss: -9.129
Iter 1986/2000 - Loss: -9.129
Iter 1987/2000 - Loss: -9.129
Iter 1988/2000 - Loss: -9.129
Iter 1989/2000 - Loss: -9.129
Iter 1990/2000 - Loss: -9.129
Iter 1991/2000 - Loss: -9.129
Iter 1992/2000 - Loss: -9.129
Iter 1993/2000 - Loss: -9.129
Iter 1994/2000 - Loss: -9.129
Iter 1995/2000 - Loss: -9.129
Iter 1996/2000 - Loss: -9.129
Iter 1997/2000 - Loss: -9.129
Iter 1998/2000 - Loss: -9.129
Iter 1999/2000 - Loss: -9.129
Iter 2000/2000 - Loss: -9.129
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.6388,  5.8648, 21.9666, 12.7710,  6.4570, 41.4156]],

        [[13.0873, 24.3098, 15.8186,  3.9232,  8.8442, 27.2961]],

        [[13.5008, 15.6831, 15.3557,  1.1067,  5.0579, 11.3680]],

        [[13.8619, 24.6718, 10.1414,  2.4402,  7.2189, 33.5260]]])
Signal Variance: tensor([ 0.0612,  2.7704, 10.8842,  0.1978])
Estimated target variance: tensor([0.0045, 0.0430, 0.2973, 0.0032])
N: 220
Signal to noise ratio: tensor([14.0455, 94.6045, 73.7561, 28.5762])
Bound on condition number: tensor([  43401.7525, 1969001.9139, 1196793.9678,  179653.2908])
Policy Optimizer learning rate:
0.009781172698749015
Experience 22, Iter 0, disc loss: 0.011782539131363724, policy loss: 4.532014104890066
Experience 22, Iter 1, disc loss: 0.011954702295105615, policy loss: 4.514196113089886
Experience 22, Iter 2, disc loss: 0.011913151029923597, policy loss: 4.5012488903688155
Experience 22, Iter 3, disc loss: 0.011537487134728151, policy loss: 4.534669241900539
Experience 22, Iter 4, disc loss: 0.012107371943722441, policy loss: 4.492571618958869
Experience 22, Iter 5, disc loss: 0.01126366216675262, policy loss: 4.5737690483406315
Experience 22, Iter 6, disc loss: 0.011476688134164494, policy loss: 4.553783314552673
Experience 22, Iter 7, disc loss: 0.012023228081169937, policy loss: 4.505988752307751
Experience 22, Iter 8, disc loss: 0.011852118113777876, policy loss: 4.511067194638958
Experience 22, Iter 9, disc loss: 0.011277593315475808, policy loss: 4.562522150570298
Experience 22, Iter 10, disc loss: 0.011772608040295425, policy loss: 4.531391400836656
Experience 22, Iter 11, disc loss: 0.011167644749814166, policy loss: 4.580726065223045
Experience 22, Iter 12, disc loss: 0.011831878765688945, policy loss: 4.522448019941942
Experience 22, Iter 13, disc loss: 0.011425182682353836, policy loss: 4.56903443367587
Experience 22, Iter 14, disc loss: 0.011579376410866397, policy loss: 4.547067417444
Experience 22, Iter 15, disc loss: 0.011084263714648853, policy loss: 4.595473922525035
Experience 22, Iter 16, disc loss: 0.010628424947511171, policy loss: 4.626573489120105
Experience 22, Iter 17, disc loss: 0.011019071337962039, policy loss: 4.5867797647566375
Experience 22, Iter 18, disc loss: 0.010488630282548376, policy loss: 4.628313111786413
Experience 22, Iter 19, disc loss: 0.011460626138409955, policy loss: 4.544155700339532
Experience 22, Iter 20, disc loss: 0.010287407536133001, policy loss: 4.668318819083421
Experience 22, Iter 21, disc loss: 0.010683130125101092, policy loss: 4.627306421924752
Experience 22, Iter 22, disc loss: 0.010208902467935679, policy loss: 4.657043239119327
Experience 22, Iter 23, disc loss: 0.010661997854312424, policy loss: 4.630782529361187
Experience 22, Iter 24, disc loss: 0.011081767297833564, policy loss: 4.581804497432029
Experience 22, Iter 25, disc loss: 0.010112141777378136, policy loss: 4.675572732052552
Experience 22, Iter 26, disc loss: 0.010364611435248531, policy loss: 4.652342607480378
Experience 22, Iter 27, disc loss: 0.010484597578166723, policy loss: 4.6417722984915235
Experience 22, Iter 28, disc loss: 0.010222563301734622, policy loss: 4.667897832499371
Experience 22, Iter 29, disc loss: 0.010170290055897455, policy loss: 4.681825950681353
Experience 22, Iter 30, disc loss: 0.01004505873585819, policy loss: 4.684416387556016
Experience 22, Iter 31, disc loss: 0.010323227370230387, policy loss: 4.649692391494485
Experience 22, Iter 32, disc loss: 0.00966738090233855, policy loss: 4.7136782516252165
Experience 22, Iter 33, disc loss: 0.00980816665170246, policy loss: 4.704903576609553
Experience 22, Iter 34, disc loss: 0.010409147157581957, policy loss: 4.64009635484909
Experience 22, Iter 35, disc loss: 0.010075572156295941, policy loss: 4.668055220281391
Experience 22, Iter 36, disc loss: 0.009721648514984242, policy loss: 4.721666392098196
Experience 22, Iter 37, disc loss: 0.009908157260738546, policy loss: 4.68863006749119
Experience 22, Iter 38, disc loss: 0.009610698576691326, policy loss: 4.736098356182545
Experience 22, Iter 39, disc loss: 0.009843824512381992, policy loss: 4.697068547354558
Experience 22, Iter 40, disc loss: 0.009524043168162952, policy loss: 4.739379379590389
Experience 22, Iter 41, disc loss: 0.009761487461265865, policy loss: 4.724483718867654
Experience 22, Iter 42, disc loss: 0.009274184586463114, policy loss: 4.764225519176442
Experience 22, Iter 43, disc loss: 0.009340706626455857, policy loss: 4.763310309132457
Experience 22, Iter 44, disc loss: 0.00950049945642592, policy loss: 4.740050774812814
Experience 22, Iter 45, disc loss: 0.008909963143186938, policy loss: 4.823692005278753
Experience 22, Iter 46, disc loss: 0.009807353361665399, policy loss: 4.698200316033105
Experience 22, Iter 47, disc loss: 0.009023138225175744, policy loss: 4.800984160019829
Experience 22, Iter 48, disc loss: 0.008844202142944117, policy loss: 4.799432872377994
Experience 22, Iter 49, disc loss: 0.009421535170695513, policy loss: 4.749034692944777
Experience: 23
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0104],
        [0.0715],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.2800e-03, 3.8344e-02, 3.5498e-02, 1.9261e-03, 1.3409e-04,
          3.3636e-01]],

        [[5.2800e-03, 3.8344e-02, 3.5498e-02, 1.9261e-03, 1.3409e-04,
          3.3636e-01]],

        [[5.2800e-03, 3.8344e-02, 3.5498e-02, 1.9261e-03, 1.3409e-04,
          3.3636e-01]],

        [[5.2800e-03, 3.8344e-02, 3.5498e-02, 1.9261e-03, 1.3409e-04,
          3.3636e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0044, 0.0417, 0.2858, 0.0031], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0044, 0.0417, 0.2858, 0.0031])
N: 230
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([921.0000, 921.0000, 921.0000, 921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.104
Iter 2/2000 - Loss: 0.260
Iter 3/2000 - Loss: -1.993
Iter 4/2000 - Loss: -1.674
Iter 5/2000 - Loss: -0.870
Iter 6/2000 - Loss: -1.095
Iter 7/2000 - Loss: -1.708
Iter 8/2000 - Loss: -2.049
Iter 9/2000 - Loss: -1.981
Iter 10/2000 - Loss: -1.733
Iter 11/2000 - Loss: -1.585
Iter 12/2000 - Loss: -1.641
Iter 13/2000 - Loss: -1.826
Iter 14/2000 - Loss: -2.011
Iter 15/2000 - Loss: -2.120
Iter 16/2000 - Loss: -2.150
Iter 17/2000 - Loss: -2.148
Iter 18/2000 - Loss: -2.172
Iter 19/2000 - Loss: -2.261
Iter 20/2000 - Loss: -2.417
Iter 1981/2000 - Loss: -9.186
Iter 1982/2000 - Loss: -9.186
Iter 1983/2000 - Loss: -9.186
Iter 1984/2000 - Loss: -9.186
Iter 1985/2000 - Loss: -9.186
Iter 1986/2000 - Loss: -9.186
Iter 1987/2000 - Loss: -9.186
Iter 1988/2000 - Loss: -9.186
Iter 1989/2000 - Loss: -9.186
Iter 1990/2000 - Loss: -9.186
Iter 1991/2000 - Loss: -9.186
Iter 1992/2000 - Loss: -9.186
Iter 1993/2000 - Loss: -9.186
Iter 1994/2000 - Loss: -9.186
Iter 1995/2000 - Loss: -9.186
Iter 1996/2000 - Loss: -9.186
Iter 1997/2000 - Loss: -9.186
Iter 1998/2000 - Loss: -9.186
Iter 1999/2000 - Loss: -9.186
Iter 2000/2000 - Loss: -9.186
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.4570,  5.8247, 21.5881, 12.8516,  6.3445, 40.6464]],

        [[12.7416, 23.8361, 15.3584,  3.8572,  8.5145, 26.7167]],

        [[13.4210, 16.7230, 13.6928,  1.0886,  4.7767, 11.2940]],

        [[13.5572, 24.2599, 10.1822,  2.4583,  7.1380, 33.5798]]])
Signal Variance: tensor([ 0.0600,  2.5891, 10.0613,  0.1991])
Estimated target variance: tensor([0.0044, 0.0417, 0.2858, 0.0031])
N: 230
Signal to noise ratio: tensor([14.0836, 91.7292, 71.2956, 29.1683])
Bound on condition number: tensor([  45620.9306, 1935279.6500, 1169104.7707,  195682.1515])
Policy Optimizer learning rate:
0.009770872631810878
Experience 23, Iter 0, disc loss: 0.009049976273494994, policy loss: 4.799630892614773
Experience 23, Iter 1, disc loss: 0.009407161596962841, policy loss: 4.743842310747027
Experience 23, Iter 2, disc loss: 0.009224036235144638, policy loss: 4.77200270595144
Experience 23, Iter 3, disc loss: 0.008788074443957907, policy loss: 4.817003442479216
Experience 23, Iter 4, disc loss: 0.008600188958460655, policy loss: 4.840908586074672
Experience 23, Iter 5, disc loss: 0.008282678801795345, policy loss: 4.869294609160832
Experience 23, Iter 6, disc loss: 0.008953455516293335, policy loss: 4.790652467893498
Experience 23, Iter 7, disc loss: 0.008132988571414757, policy loss: 4.89486499128142
Experience 23, Iter 8, disc loss: 0.009095776474991163, policy loss: 4.7713015951493265
Experience 23, Iter 9, disc loss: 0.007998644263121179, policy loss: 4.936773664022279
Experience 23, Iter 10, disc loss: 0.008725070287380062, policy loss: 4.826963317095161
Experience 23, Iter 11, disc loss: 0.00870946776727482, policy loss: 4.822291672085816
Experience 23, Iter 12, disc loss: 0.009199365419491302, policy loss: 4.7561804841394295
Experience 23, Iter 13, disc loss: 0.008222940364273344, policy loss: 4.900909265332309
Experience 23, Iter 14, disc loss: 0.008842338142462878, policy loss: 4.796090831080102
Experience 23, Iter 15, disc loss: 0.008209166501414624, policy loss: 4.885269376344743
Experience 23, Iter 16, disc loss: 0.008756633932968385, policy loss: 4.825291299909394
Experience 23, Iter 17, disc loss: 0.008308770967473475, policy loss: 4.865886297987743
Experience 23, Iter 18, disc loss: 0.008517522359712891, policy loss: 4.86456384985968
Experience 23, Iter 19, disc loss: 0.008726215070085367, policy loss: 4.816521681053681
Experience 23, Iter 20, disc loss: 0.008351373453958486, policy loss: 4.874061032909897
Experience 23, Iter 21, disc loss: 0.008382009151490698, policy loss: 4.87001480467752
Experience 23, Iter 22, disc loss: 0.00806531417202331, policy loss: 4.887600033584881
Experience 23, Iter 23, disc loss: 0.00797010380079865, policy loss: 4.915629183932609
Experience 23, Iter 24, disc loss: 0.007805140232453607, policy loss: 4.934802659862114
Experience 23, Iter 25, disc loss: 0.00770586831362915, policy loss: 4.948327392495706
Experience 23, Iter 26, disc loss: 0.007868985078940996, policy loss: 4.940025592767543
Experience 23, Iter 27, disc loss: 0.007613197466174652, policy loss: 4.979796691388843
Experience 23, Iter 28, disc loss: 0.007915255430413822, policy loss: 4.9194774820053535
Experience 23, Iter 29, disc loss: 0.008319892994762818, policy loss: 4.870411682766936
Experience 23, Iter 30, disc loss: 0.007824088988067879, policy loss: 4.920643614990645
Experience 23, Iter 31, disc loss: 0.007927381660712355, policy loss: 4.91591612158983
Experience 23, Iter 32, disc loss: 0.0076963170686557315, policy loss: 4.952175013506787
Experience 23, Iter 33, disc loss: 0.00759153090208326, policy loss: 4.957243151142572
Experience 23, Iter 34, disc loss: 0.00802534088455509, policy loss: 4.911857464723013
Experience 23, Iter 35, disc loss: 0.007319253555446332, policy loss: 4.991373577775723
Experience 23, Iter 36, disc loss: 0.007209460076799416, policy loss: 5.0227747026695635
Experience 23, Iter 37, disc loss: 0.007381752357707642, policy loss: 4.984921921545703
Experience 23, Iter 38, disc loss: 0.007257670646279804, policy loss: 5.004886451728524
Experience 23, Iter 39, disc loss: 0.007328869822783588, policy loss: 4.994436678457982
Experience 23, Iter 40, disc loss: 0.007548960928052774, policy loss: 4.956612422517182
Experience 23, Iter 41, disc loss: 0.006977966470173146, policy loss: 5.049954873901208
Experience 23, Iter 42, disc loss: 0.00719122283446614, policy loss: 5.028517062205542
Experience 23, Iter 43, disc loss: 0.007546329096129722, policy loss: 4.967499707514578
Experience 23, Iter 44, disc loss: 0.006970542175103444, policy loss: 5.048675672443656
Experience 23, Iter 45, disc loss: 0.006935916210426557, policy loss: 5.049059572164852
Experience 23, Iter 46, disc loss: 0.007022144351582754, policy loss: 5.0414649210593385
Experience 23, Iter 47, disc loss: 0.007498992098749322, policy loss: 4.963330959070472
Experience 23, Iter 48, disc loss: 0.007131611499209671, policy loss: 5.023980269838753
Experience 23, Iter 49, disc loss: 0.007066809043208215, policy loss: 5.039577233200106
Experience: 24
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0105],
        [0.0722],
        [0.0008]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.1739e-03, 3.7995e-02, 3.4664e-02, 1.8808e-03, 1.2863e-04,
          3.3176e-01]],

        [[5.1739e-03, 3.7995e-02, 3.4664e-02, 1.8808e-03, 1.2863e-04,
          3.3176e-01]],

        [[5.1739e-03, 3.7995e-02, 3.4664e-02, 1.8808e-03, 1.2863e-04,
          3.3176e-01]],

        [[5.1739e-03, 3.7995e-02, 3.4664e-02, 1.8808e-03, 1.2863e-04,
          3.3176e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0043, 0.0419, 0.2886, 0.0030], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0043, 0.0419, 0.2886, 0.0030])
N: 240
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([961.0000, 961.0000, 961.0000, 961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.113
Iter 2/2000 - Loss: 0.237
Iter 3/2000 - Loss: -2.005
Iter 4/2000 - Loss: -1.679
Iter 5/2000 - Loss: -0.886
Iter 6/2000 - Loss: -1.127
Iter 7/2000 - Loss: -1.739
Iter 8/2000 - Loss: -2.063
Iter 9/2000 - Loss: -1.979
Iter 10/2000 - Loss: -1.728
Iter 11/2000 - Loss: -1.591
Iter 12/2000 - Loss: -1.660
Iter 13/2000 - Loss: -1.849
Iter 14/2000 - Loss: -2.029
Iter 15/2000 - Loss: -2.127
Iter 16/2000 - Loss: -2.145
Iter 17/2000 - Loss: -2.137
Iter 18/2000 - Loss: -2.162
Iter 19/2000 - Loss: -2.256
Iter 20/2000 - Loss: -2.416
Iter 1981/2000 - Loss: -9.198
Iter 1982/2000 - Loss: -9.198
Iter 1983/2000 - Loss: -9.198
Iter 1984/2000 - Loss: -9.198
Iter 1985/2000 - Loss: -9.198
Iter 1986/2000 - Loss: -9.198
Iter 1987/2000 - Loss: -9.198
Iter 1988/2000 - Loss: -9.198
Iter 1989/2000 - Loss: -9.198
Iter 1990/2000 - Loss: -9.198
Iter 1991/2000 - Loss: -9.198
Iter 1992/2000 - Loss: -9.198
Iter 1993/2000 - Loss: -9.198
Iter 1994/2000 - Loss: -9.198
Iter 1995/2000 - Loss: -9.198
Iter 1996/2000 - Loss: -9.198
Iter 1997/2000 - Loss: -9.198
Iter 1998/2000 - Loss: -9.198
Iter 1999/2000 - Loss: -9.198
Iter 2000/2000 - Loss: -9.198
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0020],
        [0.0002]])
Lengthscale: tensor([[[12.2925,  5.8132, 21.5217, 12.6917,  6.3450, 40.7271]],

        [[11.9473, 22.7207, 14.2102,  3.4620,  7.6957, 24.6125]],

        [[13.5064, 16.8771, 13.6339,  1.1057,  4.8334, 12.1591]],

        [[13.5022, 24.3035, 10.2748,  2.3829,  6.9464, 32.3814]]])
Signal Variance: tensor([ 0.0598,  2.1353, 10.5356,  0.1990])
Estimated target variance: tensor([0.0043, 0.0419, 0.2886, 0.0030])
N: 240
Signal to noise ratio: tensor([14.2571, 82.8091, 73.3831, 28.5591])
Bound on condition number: tensor([  48784.4580, 1645764.8312, 1292418.3943,  195750.2972])
Policy Optimizer learning rate:
0.009760583411361419
Experience 24, Iter 0, disc loss: 0.007030831157422925, policy loss: 5.068500157307435
Experience 24, Iter 1, disc loss: 0.0066091064942425636, policy loss: 5.100686988332336
Experience 24, Iter 2, disc loss: 0.0069621194749458735, policy loss: 5.055016137041428
Experience 24, Iter 3, disc loss: 0.006768607806875797, policy loss: 5.071753554762248
Experience 24, Iter 4, disc loss: 0.006626678161776686, policy loss: 5.099737329331717
Experience 24, Iter 5, disc loss: 0.007360111530406017, policy loss: 5.003719604806735
Experience 24, Iter 6, disc loss: 0.006935088476756548, policy loss: 5.0714176003285765
Experience 24, Iter 7, disc loss: 0.006475792962561255, policy loss: 5.145737690598808
Experience 24, Iter 8, disc loss: 0.006748844522910972, policy loss: 5.099350378402747
Experience 24, Iter 9, disc loss: 0.00683575758227823, policy loss: 5.0695147608143944
Experience 24, Iter 10, disc loss: 0.0072673069056613275, policy loss: 4.988747700892596
Experience 24, Iter 11, disc loss: 0.006756781656751691, policy loss: 5.104739428984528
Experience 24, Iter 12, disc loss: 0.006624234875592424, policy loss: 5.100216794714395
Experience 24, Iter 13, disc loss: 0.0070659654694528815, policy loss: 5.025928215246251
Experience 24, Iter 14, disc loss: 0.006125398650166257, policy loss: 5.20288382334478
Experience 24, Iter 15, disc loss: 0.0063536208666807766, policy loss: 5.141417228734301
Experience 24, Iter 16, disc loss: 0.0064970374743827716, policy loss: 5.106009179031531
Experience 24, Iter 17, disc loss: 0.006331322081723233, policy loss: 5.1406793918576605
Experience 24, Iter 18, disc loss: 0.006217884691716846, policy loss: 5.165920130149804
Experience 24, Iter 19, disc loss: 0.006314382353176357, policy loss: 5.15892078485836
Experience 24, Iter 20, disc loss: 0.006893916411639081, policy loss: 5.075486828080546
Experience 24, Iter 21, disc loss: 0.006486773368152867, policy loss: 5.1231772506762265
Experience 24, Iter 22, disc loss: 0.006819027182062099, policy loss: 5.079197710176823
Experience 24, Iter 23, disc loss: 0.006779252860570992, policy loss: 5.080699638910923
Experience 24, Iter 24, disc loss: 0.006542357337951688, policy loss: 5.116149515355591
Experience 24, Iter 25, disc loss: 0.006546923237265904, policy loss: 5.103489428158145
Experience 24, Iter 26, disc loss: 0.006596527826463331, policy loss: 5.111936626341077
Experience 24, Iter 27, disc loss: 0.0064486675943791275, policy loss: 5.119341031764538
Experience 24, Iter 28, disc loss: 0.006284683187754381, policy loss: 5.165780029278273
Experience 24, Iter 29, disc loss: 0.006306666379067575, policy loss: 5.16089296941515
Experience 24, Iter 30, disc loss: 0.006278752811067579, policy loss: 5.146803643051655
Experience 24, Iter 31, disc loss: 0.006522174117470022, policy loss: 5.107857226246418
Experience 24, Iter 32, disc loss: 0.006045217569458998, policy loss: 5.197886409560758
Experience 24, Iter 33, disc loss: 0.006017825631357455, policy loss: 5.18611381235289
Experience 24, Iter 34, disc loss: 0.00614747248315715, policy loss: 5.1729770962501105
Experience 24, Iter 35, disc loss: 0.006142863167450248, policy loss: 5.179686643337243
Experience 24, Iter 36, disc loss: 0.006014166280112109, policy loss: 5.197218342943454
Experience 24, Iter 37, disc loss: 0.00584214424962396, policy loss: 5.234626426751362
Experience 24, Iter 38, disc loss: 0.006011911532056353, policy loss: 5.195124374987316
Experience 24, Iter 39, disc loss: 0.005787169830360982, policy loss: 5.228383364181978
Experience 24, Iter 40, disc loss: 0.005996169140228506, policy loss: 5.192420010636395
Experience 24, Iter 41, disc loss: 0.006237992790328245, policy loss: 5.152000611065812
Experience 24, Iter 42, disc loss: 0.005844748280650739, policy loss: 5.221917963929859
Experience 24, Iter 43, disc loss: 0.006400688836660313, policy loss: 5.135130901451334
Experience 24, Iter 44, disc loss: 0.006002347128189164, policy loss: 5.200074465160583
Experience 24, Iter 45, disc loss: 0.006075174780617756, policy loss: 5.188443539456523
Experience 24, Iter 46, disc loss: 0.005639813221600857, policy loss: 5.265681849594785
Experience 24, Iter 47, disc loss: 0.005696416328574136, policy loss: 5.253524039860919
Experience 24, Iter 48, disc loss: 0.005779427671172575, policy loss: 5.243477807436459
Experience 24, Iter 49, disc loss: 0.005551338199701438, policy loss: 5.277540639477521
Experience: 25
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0104],
        [0.0717],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.9968e-03, 3.6903e-02, 3.3761e-02, 1.8382e-03, 1.2359e-04,
          3.2498e-01]],

        [[4.9968e-03, 3.6903e-02, 3.3761e-02, 1.8382e-03, 1.2359e-04,
          3.2498e-01]],

        [[4.9968e-03, 3.6903e-02, 3.3761e-02, 1.8382e-03, 1.2359e-04,
          3.2498e-01]],

        [[4.9968e-03, 3.6903e-02, 3.3761e-02, 1.8382e-03, 1.2359e-04,
          3.2498e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0415, 0.2870, 0.0029], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0415, 0.2870, 0.0029])
N: 250
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1001.0000, 1001.0000, 1001.0000, 1001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.150
Iter 2/2000 - Loss: 0.234
Iter 3/2000 - Loss: -2.043
Iter 4/2000 - Loss: -1.708
Iter 5/2000 - Loss: -0.907
Iter 6/2000 - Loss: -1.160
Iter 7/2000 - Loss: -1.782
Iter 8/2000 - Loss: -2.102
Iter 9/2000 - Loss: -2.007
Iter 10/2000 - Loss: -1.752
Iter 11/2000 - Loss: -1.618
Iter 12/2000 - Loss: -1.693
Iter 13/2000 - Loss: -1.886
Iter 14/2000 - Loss: -2.065
Iter 15/2000 - Loss: -2.158
Iter 16/2000 - Loss: -2.171
Iter 17/2000 - Loss: -2.158
Iter 18/2000 - Loss: -2.181
Iter 19/2000 - Loss: -2.275
Iter 20/2000 - Loss: -2.437
Iter 1981/2000 - Loss: -9.236
Iter 1982/2000 - Loss: -9.236
Iter 1983/2000 - Loss: -9.236
Iter 1984/2000 - Loss: -9.236
Iter 1985/2000 - Loss: -9.236
Iter 1986/2000 - Loss: -9.236
Iter 1987/2000 - Loss: -9.236
Iter 1988/2000 - Loss: -9.236
Iter 1989/2000 - Loss: -9.236
Iter 1990/2000 - Loss: -9.237
Iter 1991/2000 - Loss: -9.237
Iter 1992/2000 - Loss: -9.237
Iter 1993/2000 - Loss: -9.237
Iter 1994/2000 - Loss: -9.237
Iter 1995/2000 - Loss: -9.237
Iter 1996/2000 - Loss: -9.237
Iter 1997/2000 - Loss: -9.237
Iter 1998/2000 - Loss: -9.237
Iter 1999/2000 - Loss: -9.237
Iter 2000/2000 - Loss: -9.237
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[12.1876,  5.9304, 21.4958, 12.8122,  6.2473, 40.6832]],

        [[11.5265, 22.4201, 13.5498,  3.5473,  7.9654, 24.9036]],

        [[13.2076, 14.7807, 14.0005,  1.1388,  4.9058, 11.7862]],

        [[13.1714, 23.7994, 10.3913,  2.3917,  6.8264, 31.5360]]])
Signal Variance: tensor([ 0.0606,  2.1235, 10.4454,  0.2030])
Estimated target variance: tensor([0.0042, 0.0415, 0.2870, 0.0029])
N: 250
Signal to noise ratio: tensor([14.4472, 82.5438, 73.8746, 29.0397])
Bound on condition number: tensor([  52181.3925, 1703368.8932, 1364365.6545,  210827.3140])
Policy Optimizer learning rate:
0.009750305025978739
Experience 25, Iter 0, disc loss: 0.005700034639563323, policy loss: 5.249258619360894
Experience 25, Iter 1, disc loss: 0.005744799628810855, policy loss: 5.239157713344691
Experience 25, Iter 2, disc loss: 0.005719861410435122, policy loss: 5.26689093439824
Experience 25, Iter 3, disc loss: 0.00604652158862694, policy loss: 5.187998660028782
Experience 25, Iter 4, disc loss: 0.005976551706008458, policy loss: 5.205038147734166
Experience 25, Iter 5, disc loss: 0.005621342399139866, policy loss: 5.259315362353291
Experience 25, Iter 6, disc loss: 0.0054583649121858506, policy loss: 5.289512918869235
Experience 25, Iter 7, disc loss: 0.005501152771457618, policy loss: 5.292919358857137
Experience 25, Iter 8, disc loss: 0.00547779133066428, policy loss: 5.296011741413556
Experience 25, Iter 9, disc loss: 0.005688155710462981, policy loss: 5.249157101201026
Experience 25, Iter 10, disc loss: 0.005341090404650237, policy loss: 5.322534134288066
Experience 25, Iter 11, disc loss: 0.005337017452697643, policy loss: 5.3079173594591795
Experience 25, Iter 12, disc loss: 0.005365491087141377, policy loss: 5.312377924506414
Experience 25, Iter 13, disc loss: 0.005251925337360228, policy loss: 5.32972681691259
Experience 25, Iter 14, disc loss: 0.005353143617535883, policy loss: 5.305792670518287
Experience 25, Iter 15, disc loss: 0.005068380697590182, policy loss: 5.363207399682272
Experience 25, Iter 16, disc loss: 0.0056018879875271675, policy loss: 5.24837755686841
Experience 25, Iter 17, disc loss: 0.0049622703885861975, policy loss: 5.399496092656455
Experience 25, Iter 18, disc loss: 0.005358386707018985, policy loss: 5.299475664976246
Experience 25, Iter 19, disc loss: 0.005281488577180143, policy loss: 5.330847347636388
Experience 25, Iter 20, disc loss: 0.005130763130775033, policy loss: 5.356728591723632
Experience 25, Iter 21, disc loss: 0.0050428466962354635, policy loss: 5.36825097668839
Experience 25, Iter 22, disc loss: 0.005027313282171516, policy loss: 5.371826769635258
Experience 25, Iter 23, disc loss: 0.005023727420020676, policy loss: 5.373915841134552
Experience 25, Iter 24, disc loss: 0.005248433697754973, policy loss: 5.334388811649553
Experience 25, Iter 25, disc loss: 0.0053058382074606895, policy loss: 5.320890819865779
Experience 25, Iter 26, disc loss: 0.005341786343178505, policy loss: 5.317938806665113
Experience 25, Iter 27, disc loss: 0.005224305105164633, policy loss: 5.335651213454794
Experience 25, Iter 28, disc loss: 0.004984430150711203, policy loss: 5.3953297246876115
Experience 25, Iter 29, disc loss: 0.005336180836260768, policy loss: 5.308769648851641
Experience 25, Iter 30, disc loss: 0.005300936938794708, policy loss: 5.311389609792892
Experience 25, Iter 31, disc loss: 0.005066896135561543, policy loss: 5.368300094831121
Experience 25, Iter 32, disc loss: 0.005005017607521422, policy loss: 5.3869219440067
Experience 25, Iter 33, disc loss: 0.005092084400668791, policy loss: 5.379681407982797
Experience 25, Iter 34, disc loss: 0.005028529285414139, policy loss: 5.3760098822392415
Experience 25, Iter 35, disc loss: 0.004881898723824639, policy loss: 5.409926561244046
Experience 25, Iter 36, disc loss: 0.004915471622650346, policy loss: 5.390066139766131
Experience 25, Iter 37, disc loss: 0.004764599538877066, policy loss: 5.427847394041203
Experience 25, Iter 38, disc loss: 0.005044369757471086, policy loss: 5.373574961461758
Experience 25, Iter 39, disc loss: 0.004872569867503526, policy loss: 5.4174803176853565
Experience 25, Iter 40, disc loss: 0.004783237589975838, policy loss: 5.451524839320653
Experience 25, Iter 41, disc loss: 0.004815253034937673, policy loss: 5.414356699886118
Experience 25, Iter 42, disc loss: 0.004824203828195765, policy loss: 5.426983762570263
Experience 25, Iter 43, disc loss: 0.004844384919502085, policy loss: 5.410579884886446
Experience 25, Iter 44, disc loss: 0.004837101941727702, policy loss: 5.428837532468545
Experience 25, Iter 45, disc loss: 0.0045677274538657555, policy loss: 5.4724653284645575
Experience 25, Iter 46, disc loss: 0.004602229140935009, policy loss: 5.447501225212023
Experience 25, Iter 47, disc loss: 0.004550267976339983, policy loss: 5.461948172631873
Experience 25, Iter 48, disc loss: 0.004774277926850612, policy loss: 5.435645859397074
Experience 25, Iter 49, disc loss: 0.0044624084211749415, policy loss: 5.4939045242296505
Experience: 26
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0102],
        [0.0701],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.9400e-03, 3.7217e-02, 3.2764e-02, 1.8208e-03, 1.1892e-04,
          3.1894e-01]],

        [[4.9400e-03, 3.7217e-02, 3.2764e-02, 1.8208e-03, 1.1892e-04,
          3.1894e-01]],

        [[4.9400e-03, 3.7217e-02, 3.2764e-02, 1.8208e-03, 1.1892e-04,
          3.1894e-01]],

        [[4.9400e-03, 3.7217e-02, 3.2764e-02, 1.8208e-03, 1.1892e-04,
          3.1894e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0406, 0.2806, 0.0028], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0406, 0.2806, 0.0028])
N: 260
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1041.0000, 1041.0000, 1041.0000, 1041.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.182
Iter 2/2000 - Loss: 0.202
Iter 3/2000 - Loss: -2.077
Iter 4/2000 - Loss: -1.740
Iter 5/2000 - Loss: -0.942
Iter 6/2000 - Loss: -1.204
Iter 7/2000 - Loss: -1.826
Iter 8/2000 - Loss: -2.137
Iter 9/2000 - Loss: -2.033
Iter 10/2000 - Loss: -1.777
Iter 11/2000 - Loss: -1.650
Iter 12/2000 - Loss: -1.732
Iter 13/2000 - Loss: -1.926
Iter 14/2000 - Loss: -2.100
Iter 15/2000 - Loss: -2.184
Iter 16/2000 - Loss: -2.188
Iter 17/2000 - Loss: -2.172
Iter 18/2000 - Loss: -2.197
Iter 19/2000 - Loss: -2.294
Iter 20/2000 - Loss: -2.457
Iter 1981/2000 - Loss: -9.273
Iter 1982/2000 - Loss: -9.273
Iter 1983/2000 - Loss: -9.273
Iter 1984/2000 - Loss: -9.273
Iter 1985/2000 - Loss: -9.273
Iter 1986/2000 - Loss: -9.273
Iter 1987/2000 - Loss: -9.273
Iter 1988/2000 - Loss: -9.273
Iter 1989/2000 - Loss: -9.273
Iter 1990/2000 - Loss: -9.273
Iter 1991/2000 - Loss: -9.273
Iter 1992/2000 - Loss: -9.273
Iter 1993/2000 - Loss: -9.273
Iter 1994/2000 - Loss: -9.274
Iter 1995/2000 - Loss: -9.274
Iter 1996/2000 - Loss: -9.274
Iter 1997/2000 - Loss: -9.274
Iter 1998/2000 - Loss: -9.274
Iter 1999/2000 - Loss: -9.274
Iter 2000/2000 - Loss: -9.274
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[12.0640,  5.9539, 21.4755, 12.8979,  6.0490, 40.6474]],

        [[11.8884, 21.9680, 14.0475,  3.4523,  7.6461, 24.8970]],

        [[12.6710, 11.2172, 13.8950,  1.1191,  4.7464, 12.3559]],

        [[13.1023, 23.4646, 10.4372,  2.3582,  6.6518, 31.8037]]])
Signal Variance: tensor([ 0.0610,  2.1120, 10.1112,  0.2027])
Estimated target variance: tensor([0.0042, 0.0406, 0.2806, 0.0028])
N: 260
Signal to noise ratio: tensor([14.6506, 82.8158, 72.5680, 29.3145])
Bound on condition number: tensor([  55807.4731, 1783197.7056, 1369190.7815,  223428.6346])
Policy Optimizer learning rate:
0.009740037464252968
Experience 26, Iter 0, disc loss: 0.004831139039468325, policy loss: 5.42287769468986
Experience 26, Iter 1, disc loss: 0.004723004960557366, policy loss: 5.430423432417148
Experience 26, Iter 2, disc loss: 0.0043762393273403056, policy loss: 5.5237266332071755
Experience 26, Iter 3, disc loss: 0.004677260192129858, policy loss: 5.461879160017656
Experience 26, Iter 4, disc loss: 0.004509666684520621, policy loss: 5.483672946835052
Experience 26, Iter 5, disc loss: 0.004847531881344177, policy loss: 5.400350058235015
Experience 26, Iter 6, disc loss: 0.004552057623229934, policy loss: 5.485257942041013
Experience 26, Iter 7, disc loss: 0.004446910575768554, policy loss: 5.5029370338372825
Experience 26, Iter 8, disc loss: 0.004435746507226696, policy loss: 5.512017527111521
Experience 26, Iter 9, disc loss: 0.004513286778591898, policy loss: 5.477332474083424
Experience 26, Iter 10, disc loss: 0.0048089615103200775, policy loss: 5.417642478771942
Experience 26, Iter 11, disc loss: 0.004561976281226853, policy loss: 5.472987578102218
Experience 26, Iter 12, disc loss: 0.004497426322909925, policy loss: 5.496768029498732
Experience 26, Iter 13, disc loss: 0.004371568827924208, policy loss: 5.51707974869186
Experience 26, Iter 14, disc loss: 0.004607683515658595, policy loss: 5.468693886247768
Experience 26, Iter 15, disc loss: 0.004287150543333603, policy loss: 5.54143580331983
Experience 26, Iter 16, disc loss: 0.004345548347311902, policy loss: 5.525743968522234
Experience 26, Iter 17, disc loss: 0.004441931576219178, policy loss: 5.504103660428994
Experience 26, Iter 18, disc loss: 0.004311129359012615, policy loss: 5.5402206821428575
Experience 26, Iter 19, disc loss: 0.0042702406186284955, policy loss: 5.536961486891107
Experience 26, Iter 20, disc loss: 0.004228111090886079, policy loss: 5.565368003710528
Experience 26, Iter 21, disc loss: 0.004463681406994864, policy loss: 5.510781401872583
Experience 26, Iter 22, disc loss: 0.004291977455482572, policy loss: 5.554340302542374
Experience 26, Iter 23, disc loss: 0.004304694699660794, policy loss: 5.535680616457027
Experience 26, Iter 24, disc loss: 0.003966026860548219, policy loss: 5.6122626802476265
Experience 26, Iter 25, disc loss: 0.0043734464515853095, policy loss: 5.508209987202908
Experience 26, Iter 26, disc loss: 0.004246028348719142, policy loss: 5.545448065874433
Experience 26, Iter 27, disc loss: 0.004159088574974499, policy loss: 5.576535517701148
Experience 26, Iter 28, disc loss: 0.004034325715553621, policy loss: 5.621114476038261
Experience 26, Iter 29, disc loss: 0.004038857478108391, policy loss: 5.617982281710344
Experience 26, Iter 30, disc loss: 0.004117885331748924, policy loss: 5.5919774231058375
Experience 26, Iter 31, disc loss: 0.004175662468763661, policy loss: 5.566606581337829
Experience 26, Iter 32, disc loss: 0.004255166661391471, policy loss: 5.541758461492255
Experience 26, Iter 33, disc loss: 0.0039385327912079314, policy loss: 5.618223258240525
Experience 26, Iter 34, disc loss: 0.0038520077982717674, policy loss: 5.659462465359168
Experience 26, Iter 35, disc loss: 0.004044176829933314, policy loss: 5.596113255777645
Experience 26, Iter 36, disc loss: 0.004028532253978033, policy loss: 5.6114962214712625
Experience 26, Iter 37, disc loss: 0.004090823356141766, policy loss: 5.600436044104002
Experience 26, Iter 38, disc loss: 0.003931658936089982, policy loss: 5.636081642679302
Experience 26, Iter 39, disc loss: 0.004115274970915463, policy loss: 5.582214106412207
Experience 26, Iter 40, disc loss: 0.003977541729033292, policy loss: 5.616710729897523
Experience 26, Iter 41, disc loss: 0.0037554189438062688, policy loss: 5.676734954791222
Experience 26, Iter 42, disc loss: 0.003872204234452465, policy loss: 5.637489500236983
Experience 26, Iter 43, disc loss: 0.004061533949812696, policy loss: 5.596299297618941
Experience 26, Iter 44, disc loss: 0.003820197898570695, policy loss: 5.6505198597887745
Experience 26, Iter 45, disc loss: 0.004148023762633579, policy loss: 5.587920945886745
Experience 26, Iter 46, disc loss: 0.003937949667991363, policy loss: 5.622010643709441
Experience 26, Iter 47, disc loss: 0.003897582357530069, policy loss: 5.63338307815119
Experience 26, Iter 48, disc loss: 0.0037471657450058125, policy loss: 5.663765856469345
Experience 26, Iter 49, disc loss: 0.003994613176871348, policy loss: 5.604724221411258
Experience: 27
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0100],
        [0.0696],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.8270e-03, 3.6694e-02, 3.2007e-02, 1.7916e-03, 1.1456e-04,
          3.1363e-01]],

        [[4.8270e-03, 3.6694e-02, 3.2007e-02, 1.7916e-03, 1.1456e-04,
          3.1363e-01]],

        [[4.8270e-03, 3.6694e-02, 3.2007e-02, 1.7916e-03, 1.1456e-04,
          3.1363e-01]],

        [[4.8270e-03, 3.6694e-02, 3.2007e-02, 1.7916e-03, 1.1456e-04,
          3.1363e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0042, 0.0400, 0.2784, 0.0027], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0042, 0.0400, 0.2784, 0.0027])
N: 270
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1081.0000, 1081.0000, 1081.0000, 1081.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.214
Iter 2/2000 - Loss: 0.193
Iter 3/2000 - Loss: -2.111
Iter 4/2000 - Loss: -1.770
Iter 5/2000 - Loss: -0.968
Iter 6/2000 - Loss: -1.238
Iter 7/2000 - Loss: -1.866
Iter 8/2000 - Loss: -2.174
Iter 9/2000 - Loss: -2.066
Iter 10/2000 - Loss: -1.810
Iter 11/2000 - Loss: -1.687
Iter 12/2000 - Loss: -1.774
Iter 13/2000 - Loss: -1.970
Iter 14/2000 - Loss: -2.144
Iter 15/2000 - Loss: -2.227
Iter 16/2000 - Loss: -2.230
Iter 17/2000 - Loss: -2.214
Iter 18/2000 - Loss: -2.241
Iter 19/2000 - Loss: -2.341
Iter 20/2000 - Loss: -2.506
Iter 1981/2000 - Loss: -9.296
Iter 1982/2000 - Loss: -9.296
Iter 1983/2000 - Loss: -9.296
Iter 1984/2000 - Loss: -9.296
Iter 1985/2000 - Loss: -9.296
Iter 1986/2000 - Loss: -9.296
Iter 1987/2000 - Loss: -9.296
Iter 1988/2000 - Loss: -9.296
Iter 1989/2000 - Loss: -9.296
Iter 1990/2000 - Loss: -9.296
Iter 1991/2000 - Loss: -9.296
Iter 1992/2000 - Loss: -9.296
Iter 1993/2000 - Loss: -9.296
Iter 1994/2000 - Loss: -9.296
Iter 1995/2000 - Loss: -9.296
Iter 1996/2000 - Loss: -9.296
Iter 1997/2000 - Loss: -9.296
Iter 1998/2000 - Loss: -9.296
Iter 1999/2000 - Loss: -9.296
Iter 2000/2000 - Loss: -9.296
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.8313,  5.8111, 21.2452, 12.7394,  6.1330, 40.4702]],

        [[11.8379, 21.9388, 14.8103,  3.5474,  7.8306, 25.5736]],

        [[11.7464, 10.2091, 13.8863,  1.1552,  4.8370, 12.1588]],

        [[12.9042, 23.2029, 10.6833,  2.3145,  6.1448, 31.8723]]])
Signal Variance: tensor([0.0586, 2.2681, 9.9930, 0.2063])
Estimated target variance: tensor([0.0042, 0.0400, 0.2784, 0.0027])
N: 270
Signal to noise ratio: tensor([14.5440, 84.6883, 72.2188, 29.8216])
Bound on condition number: tensor([  57113.9134, 1936468.6610, 1408199.4848,  240120.2316])
Policy Optimizer learning rate:
0.00972978071478625
Experience 27, Iter 0, disc loss: 0.003997079798536154, policy loss: 5.604944246675552
Experience 27, Iter 1, disc loss: 0.003881212353228264, policy loss: 5.662644686395137
Experience 27, Iter 2, disc loss: 0.003976973383688729, policy loss: 5.60835737544537
Experience 27, Iter 3, disc loss: 0.0036138471911269445, policy loss: 5.715322608945562
Experience 27, Iter 4, disc loss: 0.004110311955888721, policy loss: 5.564762174106583
Experience 27, Iter 5, disc loss: 0.004015957336340206, policy loss: 5.587518578683815
Experience 27, Iter 6, disc loss: 0.003874364011283139, policy loss: 5.634644592110568
Experience 27, Iter 7, disc loss: 0.0040143316409116895, policy loss: 5.600040717930979
Experience 27, Iter 8, disc loss: 0.003624736394169071, policy loss: 5.721082567311187
Experience 27, Iter 9, disc loss: 0.003817712739229851, policy loss: 5.663270220617895
Experience 27, Iter 10, disc loss: 0.00390005438550139, policy loss: 5.641712908345372
Experience 27, Iter 11, disc loss: 0.003859408760745275, policy loss: 5.642705505904557
Experience 27, Iter 12, disc loss: 0.0037119421481331275, policy loss: 5.682867001624463
Experience 27, Iter 13, disc loss: 0.0038625676436355397, policy loss: 5.64959638798806
Experience 27, Iter 14, disc loss: 0.003676036175694765, policy loss: 5.706836520551731
Experience 27, Iter 15, disc loss: 0.0039341817641642554, policy loss: 5.625205296238481
Experience 27, Iter 16, disc loss: 0.0037261738502820524, policy loss: 5.67108870515632
Experience 27, Iter 17, disc loss: 0.003513508909082427, policy loss: 5.752912276657052
Experience 27, Iter 18, disc loss: 0.0036676737904619084, policy loss: 5.682191785169769
Experience 27, Iter 19, disc loss: 0.0036367306329276013, policy loss: 5.702749013886191
Experience 27, Iter 20, disc loss: 0.0036101234347725576, policy loss: 5.708636315675259
Experience 27, Iter 21, disc loss: 0.003602724512161382, policy loss: 5.7174034198303545
Experience 27, Iter 22, disc loss: 0.0035366626345858763, policy loss: 5.754515470184404
Experience 27, Iter 23, disc loss: 0.0037857650516142484, policy loss: 5.662362299857929
Experience 27, Iter 24, disc loss: 0.0034408795466128786, policy loss: 5.7713176473917365
Experience 27, Iter 25, disc loss: 0.0037165714403892434, policy loss: 5.69056089223452
Experience 27, Iter 26, disc loss: 0.0034528513624741616, policy loss: 5.746284818974724
Experience 27, Iter 27, disc loss: 0.0033792925562472474, policy loss: 5.783695936217994
Experience 27, Iter 28, disc loss: 0.0034132249421514765, policy loss: 5.776502703145083
Experience 27, Iter 29, disc loss: 0.003421389753683385, policy loss: 5.783463483156466
Experience 27, Iter 30, disc loss: 0.0035627618844360307, policy loss: 5.7305708677603135
Experience 27, Iter 31, disc loss: 0.0035587627108305403, policy loss: 5.743682923260787
Experience 27, Iter 32, disc loss: 0.0035097044943136267, policy loss: 5.736628222668126
Experience 27, Iter 33, disc loss: 0.0032497662301842673, policy loss: 5.814158329257847
Experience 27, Iter 34, disc loss: 0.0033119099256085135, policy loss: 5.799224750183946
Experience 27, Iter 35, disc loss: 0.0035908248056945374, policy loss: 5.697492373875244
Experience 27, Iter 36, disc loss: 0.003432899946360893, policy loss: 5.778309590483063
Experience 27, Iter 37, disc loss: 0.0033224669772408716, policy loss: 5.782387639581804
Experience 27, Iter 38, disc loss: 0.00323244825654709, policy loss: 5.8230389340179105
Experience 27, Iter 39, disc loss: 0.0036156767850524852, policy loss: 5.703644314375152
Experience 27, Iter 40, disc loss: 0.00343922280112688, policy loss: 5.754016257343144
Experience 27, Iter 41, disc loss: 0.003496840942083028, policy loss: 5.752838432820615
Experience 27, Iter 42, disc loss: 0.003388458650942251, policy loss: 5.778193090809623
Experience 27, Iter 43, disc loss: 0.003309431897014406, policy loss: 5.788492443814766
Experience 27, Iter 44, disc loss: 0.003301551038243077, policy loss: 5.795881126420378
Experience 27, Iter 45, disc loss: 0.003289480731984871, policy loss: 5.8293496499225395
Experience 27, Iter 46, disc loss: 0.003252068941081935, policy loss: 5.8291989129465325
Experience 27, Iter 47, disc loss: 0.003309761504942656, policy loss: 5.7938205778025225
Experience 27, Iter 48, disc loss: 0.0031013535530660568, policy loss: 5.861350493040923
Experience 27, Iter 49, disc loss: 0.003483425544281861, policy loss: 5.745739054011781
Experience: 28
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0098],
        [0.0682],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.8421e-03, 3.6218e-02, 3.1107e-02, 1.7596e-03, 1.1053e-04,
          3.0676e-01]],

        [[4.8421e-03, 3.6218e-02, 3.1107e-02, 1.7596e-03, 1.1053e-04,
          3.0676e-01]],

        [[4.8421e-03, 3.6218e-02, 3.1107e-02, 1.7596e-03, 1.1053e-04,
          3.0676e-01]],

        [[4.8421e-03, 3.6218e-02, 3.1107e-02, 1.7596e-03, 1.1053e-04,
          3.0676e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0394, 0.2727, 0.0027], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0394, 0.2727, 0.0027])
N: 280
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1121.0000, 1121.0000, 1121.0000, 1121.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.249
Iter 2/2000 - Loss: 0.184
Iter 3/2000 - Loss: -2.147
Iter 4/2000 - Loss: -1.803
Iter 5/2000 - Loss: -0.994
Iter 6/2000 - Loss: -1.271
Iter 7/2000 - Loss: -1.904
Iter 8/2000 - Loss: -2.211
Iter 9/2000 - Loss: -2.100
Iter 10/2000 - Loss: -1.843
Iter 11/2000 - Loss: -1.722
Iter 12/2000 - Loss: -1.812
Iter 13/2000 - Loss: -2.009
Iter 14/2000 - Loss: -2.183
Iter 15/2000 - Loss: -2.265
Iter 16/2000 - Loss: -2.269
Iter 17/2000 - Loss: -2.255
Iter 18/2000 - Loss: -2.284
Iter 19/2000 - Loss: -2.385
Iter 20/2000 - Loss: -2.550
Iter 1981/2000 - Loss: -9.303
Iter 1982/2000 - Loss: -9.303
Iter 1983/2000 - Loss: -9.303
Iter 1984/2000 - Loss: -9.303
Iter 1985/2000 - Loss: -9.304
Iter 1986/2000 - Loss: -9.304
Iter 1987/2000 - Loss: -9.304
Iter 1988/2000 - Loss: -9.304
Iter 1989/2000 - Loss: -9.304
Iter 1990/2000 - Loss: -9.304
Iter 1991/2000 - Loss: -9.304
Iter 1992/2000 - Loss: -9.304
Iter 1993/2000 - Loss: -9.304
Iter 1994/2000 - Loss: -9.304
Iter 1995/2000 - Loss: -9.304
Iter 1996/2000 - Loss: -9.304
Iter 1997/2000 - Loss: -9.304
Iter 1998/2000 - Loss: -9.304
Iter 1999/2000 - Loss: -9.304
Iter 2000/2000 - Loss: -9.304
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.6510,  5.7160, 21.1714, 12.5521,  6.0771, 40.2782]],

        [[11.4385, 21.0750, 14.7938,  3.3625,  7.3144, 24.6900]],

        [[11.7891, 11.8954, 13.0221,  1.1708,  4.7726, 11.6716]],

        [[12.9502, 22.8754, 10.7767,  2.3728,  6.1718, 32.2869]]])
Signal Variance: tensor([0.0569, 2.0892, 9.8538, 0.2098])
Estimated target variance: tensor([0.0041, 0.0394, 0.2727, 0.0027])
N: 280
Signal to noise ratio: tensor([14.4742, 80.2214, 72.0141, 29.8027])
Bound on condition number: tensor([  58661.6007, 1801931.7782, 1452089.6541,  248697.3122])
Policy Optimizer learning rate:
0.009719534766192733
Experience 28, Iter 0, disc loss: 0.003518864338820965, policy loss: 5.724276641917983
Experience 28, Iter 1, disc loss: 0.0032687253914323987, policy loss: 5.812192080582668
Experience 28, Iter 2, disc loss: 0.003192315030862136, policy loss: 5.849981221006039
Experience 28, Iter 3, disc loss: 0.003156011363843089, policy loss: 5.867977059475372
Experience 28, Iter 4, disc loss: 0.0032390343090435377, policy loss: 5.826835638666141
Experience 28, Iter 5, disc loss: 0.0032698099365551245, policy loss: 5.810357582490753
Experience 28, Iter 6, disc loss: 0.0033098424363796957, policy loss: 5.796847301612878
Experience 28, Iter 7, disc loss: 0.0031194908127975107, policy loss: 5.869389772268543
Experience 28, Iter 8, disc loss: 0.0033928648927286087, policy loss: 5.76661446684108
Experience 28, Iter 9, disc loss: 0.0030877003194017317, policy loss: 5.864393340757313
Experience 28, Iter 10, disc loss: 0.0033281564159513923, policy loss: 5.788297332204076
Experience 28, Iter 11, disc loss: 0.003286298975081234, policy loss: 5.803089673973837
Experience 28, Iter 12, disc loss: 0.003288632494395799, policy loss: 5.816121915991458
Experience 28, Iter 13, disc loss: 0.002889354368719472, policy loss: 5.943338814560585
Experience 28, Iter 14, disc loss: 0.0029327844628785214, policy loss: 5.91937584272115
Experience 28, Iter 15, disc loss: 0.003379722721266369, policy loss: 5.771884794742958
Experience 28, Iter 16, disc loss: 0.0031601658512491336, policy loss: 5.842587031250115
Experience 28, Iter 17, disc loss: 0.003159707830727664, policy loss: 5.839706801562347
Experience 28, Iter 18, disc loss: 0.002992432606976546, policy loss: 5.91398280024987
Experience 28, Iter 19, disc loss: 0.0032572982232913364, policy loss: 5.8241640192157975
Experience 28, Iter 20, disc loss: 0.0029380260416162034, policy loss: 5.913389890448542
Experience 28, Iter 21, disc loss: 0.0030295514789943218, policy loss: 5.898352781208844
Experience 28, Iter 22, disc loss: 0.0029949228041018807, policy loss: 5.922472148931878
Experience 28, Iter 23, disc loss: 0.003063133902912785, policy loss: 5.872305433886647
Experience 28, Iter 24, disc loss: 0.002848347792708073, policy loss: 5.959183711605101
Experience 28, Iter 25, disc loss: 0.002862493062334691, policy loss: 5.929768060830567
Experience 28, Iter 26, disc loss: 0.0030641779844615864, policy loss: 5.87182574445064
Experience 28, Iter 27, disc loss: 0.0030820116552646446, policy loss: 5.8630142615792735
Experience 28, Iter 28, disc loss: 0.0031461380827609567, policy loss: 5.870655487725497
Experience 28, Iter 29, disc loss: 0.0031083465378007207, policy loss: 5.871779492486413
Experience 28, Iter 30, disc loss: 0.0030345082285832404, policy loss: 5.8829477989632855
Experience 28, Iter 31, disc loss: 0.002955299837377779, policy loss: 5.920570103309374
Experience 28, Iter 32, disc loss: 0.003048537319565481, policy loss: 5.8758232739033165
Experience 28, Iter 33, disc loss: 0.002852311582957616, policy loss: 5.946136891080809
Experience 28, Iter 34, disc loss: 0.0031169632404156322, policy loss: 5.850048957185344
Experience 28, Iter 35, disc loss: 0.0028411398164667626, policy loss: 5.947761614649711
Experience 28, Iter 36, disc loss: 0.0030089145549552, policy loss: 5.8919227447346945
Experience 28, Iter 37, disc loss: 0.0029650341756781284, policy loss: 5.931504649312929
Experience 28, Iter 38, disc loss: 0.002972274102630905, policy loss: 5.9047394581826484
Experience 28, Iter 39, disc loss: 0.0028139791136698594, policy loss: 5.96767394269912
Experience 28, Iter 40, disc loss: 0.0029168066899532, policy loss: 5.926404905745317
Experience 28, Iter 41, disc loss: 0.002898137975129384, policy loss: 5.9461074750679215
Experience 28, Iter 42, disc loss: 0.0027974153581412893, policy loss: 5.952602249609272
Experience 28, Iter 43, disc loss: 0.002836886289818477, policy loss: 5.944556457332359
Experience 28, Iter 44, disc loss: 0.00298189689616444, policy loss: 5.8963094230218065
Experience 28, Iter 45, disc loss: 0.0029362816533082944, policy loss: 5.9120264525475745
Experience 28, Iter 46, disc loss: 0.002925613662541897, policy loss: 5.930319246177863
Experience 28, Iter 47, disc loss: 0.002690234944810319, policy loss: 6.022666402237201
Experience 28, Iter 48, disc loss: 0.0029522706417504222, policy loss: 5.917091084318956
Experience 28, Iter 49, disc loss: 0.002776508125458908, policy loss: 5.985552721840967
Experience: 29
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0104],
        [0.0754],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.7661e-03, 3.5751e-02, 3.2446e-02, 1.7351e-03, 1.0677e-04,
          3.1059e-01]],

        [[4.7661e-03, 3.5751e-02, 3.2446e-02, 1.7351e-03, 1.0677e-04,
          3.1059e-01]],

        [[4.7661e-03, 3.5751e-02, 3.2446e-02, 1.7351e-03, 1.0677e-04,
          3.1059e-01]],

        [[4.7661e-03, 3.5751e-02, 3.2446e-02, 1.7351e-03, 1.0677e-04,
          3.1059e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0414, 0.3017, 0.0027], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0414, 0.3017, 0.0027])
N: 290
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1161.0000, 1161.0000, 1161.0000, 1161.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.163
Iter 2/2000 - Loss: 0.187
Iter 3/2000 - Loss: -2.069
Iter 4/2000 - Loss: -1.735
Iter 5/2000 - Loss: -0.955
Iter 6/2000 - Loss: -1.232
Iter 7/2000 - Loss: -1.846
Iter 8/2000 - Loss: -2.133
Iter 9/2000 - Loss: -2.015
Iter 10/2000 - Loss: -1.765
Iter 11/2000 - Loss: -1.658
Iter 12/2000 - Loss: -1.755
Iter 13/2000 - Loss: -1.951
Iter 14/2000 - Loss: -2.116
Iter 15/2000 - Loss: -2.189
Iter 16/2000 - Loss: -2.187
Iter 17/2000 - Loss: -2.174
Iter 18/2000 - Loss: -2.210
Iter 19/2000 - Loss: -2.320
Iter 20/2000 - Loss: -2.488
Iter 1981/2000 - Loss: -9.301
Iter 1982/2000 - Loss: -9.301
Iter 1983/2000 - Loss: -9.301
Iter 1984/2000 - Loss: -9.301
Iter 1985/2000 - Loss: -9.301
Iter 1986/2000 - Loss: -9.301
Iter 1987/2000 - Loss: -9.301
Iter 1988/2000 - Loss: -9.301
Iter 1989/2000 - Loss: -9.301
Iter 1990/2000 - Loss: -9.301
Iter 1991/2000 - Loss: -9.301
Iter 1992/2000 - Loss: -9.301
Iter 1993/2000 - Loss: -9.301
Iter 1994/2000 - Loss: -9.301
Iter 1995/2000 - Loss: -9.301
Iter 1996/2000 - Loss: -9.301
Iter 1997/2000 - Loss: -9.301
Iter 1998/2000 - Loss: -9.301
Iter 1999/2000 - Loss: -9.301
Iter 2000/2000 - Loss: -9.301
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.4391,  5.5804, 19.4860, 12.0254,  6.0063, 39.0955]],

        [[11.2585, 20.1899, 13.9105,  3.0373,  6.8288, 25.6914]],

        [[11.3722, 12.5131, 13.1546,  1.2402,  4.8816, 12.3796]],

        [[12.6786, 22.3233, 10.8901,  2.6251,  6.4342, 34.8770]]])
Signal Variance: tensor([ 0.0556,  2.0819, 10.6988,  0.2180])
Estimated target variance: tensor([0.0041, 0.0414, 0.3017, 0.0027])
N: 290
Signal to noise ratio: tensor([14.1122, 79.6136, 75.1696, 30.5177])
Bound on condition number: tensor([  57755.4427, 1838116.9468, 1638638.7432,  270087.0338])
Policy Optimizer learning rate:
0.009709299607098555
Experience 29, Iter 0, disc loss: 0.002795862483077181, policy loss: 5.970393332168209
Experience 29, Iter 1, disc loss: 0.0028362628176956256, policy loss: 5.948967620627451
Experience 29, Iter 2, disc loss: 0.0029224488102985876, policy loss: 5.9213612809595295
Experience 29, Iter 3, disc loss: 0.0027363237723733893, policy loss: 6.013257270956889
Experience 29, Iter 4, disc loss: 0.002926812361987284, policy loss: 5.916506761528879
Experience 29, Iter 5, disc loss: 0.0029847681563234746, policy loss: 5.893977432209537
Experience 29, Iter 6, disc loss: 0.002786758090593188, policy loss: 5.971080297200645
Experience 29, Iter 7, disc loss: 0.002883450380391412, policy loss: 5.933244082885453
Experience 29, Iter 8, disc loss: 0.002931069425221449, policy loss: 5.916015554104426
Experience 29, Iter 9, disc loss: 0.0028138351701233136, policy loss: 5.960524354481553
Experience 29, Iter 10, disc loss: 0.0026338640129487554, policy loss: 6.038721764680132
Experience 29, Iter 11, disc loss: 0.0026642107788046997, policy loss: 6.018833098876922
Experience 29, Iter 12, disc loss: 0.002739829984609819, policy loss: 5.991390867573585
Experience 29, Iter 13, disc loss: 0.002818292677210398, policy loss: 5.9514182937105975
Experience 29, Iter 14, disc loss: 0.0028402311367905658, policy loss: 5.945167385868038
Experience 29, Iter 15, disc loss: 0.002789419366695571, policy loss: 5.957898040313614
Experience 29, Iter 16, disc loss: 0.0027244379397542155, policy loss: 5.996119881965988
Experience 29, Iter 17, disc loss: 0.00280826684675674, policy loss: 5.980845218921415
Experience 29, Iter 18, disc loss: 0.002587294510562847, policy loss: 6.042025174613606
Experience 29, Iter 19, disc loss: 0.0027200140497101255, policy loss: 6.008234756176586
Experience 29, Iter 20, disc loss: 0.0026345963025181853, policy loss: 6.031616208954242
Experience 29, Iter 21, disc loss: 0.002711961829842056, policy loss: 5.996288257454864
Experience 29, Iter 22, disc loss: 0.002729358111504424, policy loss: 5.990572977230959
Experience 29, Iter 23, disc loss: 0.002804951200726718, policy loss: 5.987928645237434
Experience 29, Iter 24, disc loss: 0.0026676054078450017, policy loss: 6.018247494655038
Experience 29, Iter 25, disc loss: 0.002675205893568747, policy loss: 6.014665182750489
Experience 29, Iter 26, disc loss: 0.002710710026055544, policy loss: 5.994985871480985
Experience 29, Iter 27, disc loss: 0.002605412076323239, policy loss: 6.052474541553423
Experience 29, Iter 28, disc loss: 0.0025706842325919255, policy loss: 6.059552001072703
Experience 29, Iter 29, disc loss: 0.002671009612590378, policy loss: 6.023219636043201
Experience 29, Iter 30, disc loss: 0.0027031707508153786, policy loss: 6.00040747026517
Experience 29, Iter 31, disc loss: 0.002458840091264017, policy loss: 6.10301970810981
Experience 29, Iter 32, disc loss: 0.0027346694008156293, policy loss: 6.004830903048646
Experience 29, Iter 33, disc loss: 0.002702151883846302, policy loss: 6.005190573850486
Experience 29, Iter 34, disc loss: 0.0023851398453045816, policy loss: 6.140522760839078
Experience 29, Iter 35, disc loss: 0.002531349064667666, policy loss: 6.076370385144306
Experience 29, Iter 36, disc loss: 0.0023589966513007338, policy loss: 6.140491173576423
Experience 29, Iter 37, disc loss: 0.0025824211288045093, policy loss: 6.044661832415082
Experience 29, Iter 38, disc loss: 0.0025413643917708457, policy loss: 6.0752937043199875
Experience 29, Iter 39, disc loss: 0.0023188950737021443, policy loss: 6.178765851392013
Experience 29, Iter 40, disc loss: 0.0026058321195896864, policy loss: 6.054964058440806
Experience 29, Iter 41, disc loss: 0.0025951572491590197, policy loss: 6.036769735296185
Experience 29, Iter 42, disc loss: 0.002404453546970934, policy loss: 6.1139213909423
Experience 29, Iter 43, disc loss: 0.002586638277070585, policy loss: 6.032598730804021
Experience 29, Iter 44, disc loss: 0.0024069108501434326, policy loss: 6.111314219258159
Experience 29, Iter 45, disc loss: 0.0023776559033917336, policy loss: 6.146415954570558
Experience 29, Iter 46, disc loss: 0.002446757125270721, policy loss: 6.127311464829317
Experience 29, Iter 47, disc loss: 0.0025032902593188854, policy loss: 6.08450001101812
Experience 29, Iter 48, disc loss: 0.002435238278001961, policy loss: 6.1350076107339255
Experience 29, Iter 49, disc loss: 0.00250736837252846, policy loss: 6.103793331304398
Experience: 30
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0108],
        [0.0820],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.6163e-03, 3.4794e-02, 3.3463e-02, 1.7091e-03, 1.0327e-04,
          3.1352e-01]],

        [[4.6163e-03, 3.4794e-02, 3.3463e-02, 1.7091e-03, 1.0327e-04,
          3.1352e-01]],

        [[4.6163e-03, 3.4794e-02, 3.3463e-02, 1.7091e-03, 1.0327e-04,
          3.1352e-01]],

        [[4.6163e-03, 3.4794e-02, 3.3463e-02, 1.7091e-03, 1.0327e-04,
          3.1352e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0040, 0.0434, 0.3279, 0.0028], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0040, 0.0434, 0.3279, 0.0028])
N: 300
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1201.0000, 1201.0000, 1201.0000, 1201.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.100
Iter 2/2000 - Loss: 0.216
Iter 3/2000 - Loss: -2.010
Iter 4/2000 - Loss: -1.680
Iter 5/2000 - Loss: -0.912
Iter 6/2000 - Loss: -1.190
Iter 7/2000 - Loss: -1.795
Iter 8/2000 - Loss: -2.074
Iter 9/2000 - Loss: -1.952
Iter 10/2000 - Loss: -1.704
Iter 11/2000 - Loss: -1.601
Iter 12/2000 - Loss: -1.701
Iter 13/2000 - Loss: -1.898
Iter 14/2000 - Loss: -2.061
Iter 15/2000 - Loss: -2.130
Iter 16/2000 - Loss: -2.122
Iter 17/2000 - Loss: -2.106
Iter 18/2000 - Loss: -2.142
Iter 19/2000 - Loss: -2.252
Iter 20/2000 - Loss: -2.423
Iter 1981/2000 - Loss: -9.288
Iter 1982/2000 - Loss: -9.288
Iter 1983/2000 - Loss: -9.288
Iter 1984/2000 - Loss: -9.288
Iter 1985/2000 - Loss: -9.288
Iter 1986/2000 - Loss: -9.288
Iter 1987/2000 - Loss: -9.288
Iter 1988/2000 - Loss: -9.288
Iter 1989/2000 - Loss: -9.288
Iter 1990/2000 - Loss: -9.288
Iter 1991/2000 - Loss: -9.288
Iter 1992/2000 - Loss: -9.288
Iter 1993/2000 - Loss: -9.288
Iter 1994/2000 - Loss: -9.288
Iter 1995/2000 - Loss: -9.288
Iter 1996/2000 - Loss: -9.288
Iter 1997/2000 - Loss: -9.288
Iter 1998/2000 - Loss: -9.288
Iter 1999/2000 - Loss: -9.288
Iter 2000/2000 - Loss: -9.289
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.4010,  5.6298, 20.5555, 12.2533,  5.8894, 39.8714]],

        [[10.9973, 19.5387, 14.2524,  3.0726,  6.9198, 25.7652]],

        [[11.2805, 16.0708, 12.7585,  1.2864,  4.7757, 12.8784]],

        [[12.4614, 22.0256, 10.7518,  2.6231,  6.6949, 35.0487]]])
Signal Variance: tensor([ 0.0562,  2.1075, 11.5134,  0.2162])
Estimated target variance: tensor([0.0040, 0.0434, 0.3279, 0.0028])
N: 300
Signal to noise ratio: tensor([14.2412, 79.9088, 77.2044, 29.8260])
Bound on condition number: tensor([  60844.5963, 1915624.7228, 1788155.7546,  266877.8839])
Policy Optimizer learning rate:
0.009699075226141829
Experience 30, Iter 0, disc loss: 0.002507291387800605, policy loss: 6.075982185895613
Experience 30, Iter 1, disc loss: 0.0024782215745343295, policy loss: 6.0744872780781884
Experience 30, Iter 2, disc loss: 0.002267698199445428, policy loss: 6.182838104167064
Experience 30, Iter 3, disc loss: 0.002473945460674092, policy loss: 6.0846197205593135
Experience 30, Iter 4, disc loss: 0.0024716936482985018, policy loss: 6.102097450400167
Experience 30, Iter 5, disc loss: 0.002540198224273523, policy loss: 6.061935746777783
Experience 30, Iter 6, disc loss: 0.0024428055141232207, policy loss: 6.105057094663545
Experience 30, Iter 7, disc loss: 0.0024991865639541052, policy loss: 6.07361574321686
Experience 30, Iter 8, disc loss: 0.002372008427747655, policy loss: 6.130636929594246
Experience 30, Iter 9, disc loss: 0.002369957611820557, policy loss: 6.1581828970930115
Experience 30, Iter 10, disc loss: 0.0024106573051324155, policy loss: 6.116880014738948
Experience 30, Iter 11, disc loss: 0.0023834804748142577, policy loss: 6.13983003176722
Experience 30, Iter 12, disc loss: 0.002395498154469182, policy loss: 6.120729749640224
Experience 30, Iter 13, disc loss: 0.002268413466014012, policy loss: 6.201845346746248
Experience 30, Iter 14, disc loss: 0.0025121856272070594, policy loss: 6.0510810217799085
Experience 30, Iter 15, disc loss: 0.0023428770802978705, policy loss: 6.14497421637867
Experience 30, Iter 16, disc loss: 0.0024638519611210746, policy loss: 6.09434663758891
Experience 30, Iter 17, disc loss: 0.0022011971615126863, policy loss: 6.20616592070537
Experience 30, Iter 18, disc loss: 0.0023358829604542106, policy loss: 6.137250247492712
Experience 30, Iter 19, disc loss: 0.002327478228294811, policy loss: 6.154070203004067
Experience 30, Iter 20, disc loss: 0.00225578964198817, policy loss: 6.191545588556327
Experience 30, Iter 21, disc loss: 0.0023895393073570166, policy loss: 6.1203091203453726
Experience 30, Iter 22, disc loss: 0.0024124204016613767, policy loss: 6.128567105873857
Experience 30, Iter 23, disc loss: 0.002502011248616238, policy loss: 6.076604735423152
Experience 30, Iter 24, disc loss: 0.002255619142121853, policy loss: 6.178669517295183
Experience 30, Iter 25, disc loss: 0.002300806409846763, policy loss: 6.160877858130826
Experience 30, Iter 26, disc loss: 0.002245825615639444, policy loss: 6.196962158082587
Experience 30, Iter 27, disc loss: 0.0022979903025674, policy loss: 6.158702898215186
Experience 30, Iter 28, disc loss: 0.0024121977780424127, policy loss: 6.12725767522904
Experience 30, Iter 29, disc loss: 0.002536491190653149, policy loss: 6.041984142488381
Experience 30, Iter 30, disc loss: 0.0023359291925235744, policy loss: 6.167303341021969
Experience 30, Iter 31, disc loss: 0.002061262061190052, policy loss: 6.273887164058701
Experience 30, Iter 32, disc loss: 0.002241509773837984, policy loss: 6.179380708580601
Experience 30, Iter 33, disc loss: 0.0022312247325505566, policy loss: 6.19730451778496
Experience 30, Iter 34, disc loss: 0.0024022830802944654, policy loss: 6.1272429371467725
Experience 30, Iter 35, disc loss: 0.0023041707162355196, policy loss: 6.166192408834735
Experience 30, Iter 36, disc loss: 0.0023413471618115686, policy loss: 6.160350829867657
Experience 30, Iter 37, disc loss: 0.002100993188709078, policy loss: 6.257650805748804
Experience 30, Iter 38, disc loss: 0.002164257020674897, policy loss: 6.23763709748369
Experience 30, Iter 39, disc loss: 0.0022069462109623648, policy loss: 6.2014122563968925
Experience 30, Iter 40, disc loss: 0.002201927779155655, policy loss: 6.189293937737705
Experience 30, Iter 41, disc loss: 0.0023204764827131834, policy loss: 6.14870699509198
Experience 30, Iter 42, disc loss: 0.002132211280039131, policy loss: 6.238125519502737
Experience 30, Iter 43, disc loss: 0.0022604177558770643, policy loss: 6.197105865709888
Experience 30, Iter 44, disc loss: 0.0022156164170198315, policy loss: 6.188015223727257
Experience 30, Iter 45, disc loss: 0.0021729451852139, policy loss: 6.234070461905454
Experience 30, Iter 46, disc loss: 0.0022362072913417195, policy loss: 6.188560688197471
Experience 30, Iter 47, disc loss: 0.002208559376394144, policy loss: 6.197661647718574
Experience 30, Iter 48, disc loss: 0.00223754497352636, policy loss: 6.178538383676739
Experience 30, Iter 49, disc loss: 0.002220196043917412, policy loss: 6.201931282092035
Experience: 31
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0106],
        [0.0795],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5897e-03, 3.4658e-02, 3.2433e-02, 1.6740e-03, 9.9985e-05,
          3.0563e-01]],

        [[4.5897e-03, 3.4658e-02, 3.2433e-02, 1.6740e-03, 9.9985e-05,
          3.0563e-01]],

        [[4.5897e-03, 3.4658e-02, 3.2433e-02, 1.6740e-03, 9.9985e-05,
          3.0563e-01]],

        [[4.5897e-03, 3.4658e-02, 3.2433e-02, 1.6740e-03, 9.9985e-05,
          3.0563e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0039, 0.0422, 0.3181, 0.0027], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0039, 0.0422, 0.3181, 0.0027])
N: 310
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1241.0000, 1241.0000, 1241.0000, 1241.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.143
Iter 2/2000 - Loss: 0.194
Iter 3/2000 - Loss: -2.055
Iter 4/2000 - Loss: -1.724
Iter 5/2000 - Loss: -0.949
Iter 6/2000 - Loss: -1.232
Iter 7/2000 - Loss: -1.843
Iter 8/2000 - Loss: -2.120
Iter 9/2000 - Loss: -1.993
Iter 10/2000 - Loss: -1.744
Iter 11/2000 - Loss: -1.644
Iter 12/2000 - Loss: -1.748
Iter 13/2000 - Loss: -1.945
Iter 14/2000 - Loss: -2.106
Iter 15/2000 - Loss: -2.171
Iter 16/2000 - Loss: -2.162
Iter 17/2000 - Loss: -2.148
Iter 18/2000 - Loss: -2.187
Iter 19/2000 - Loss: -2.300
Iter 20/2000 - Loss: -2.470
Iter 1981/2000 - Loss: -9.324
Iter 1982/2000 - Loss: -9.324
Iter 1983/2000 - Loss: -9.324
Iter 1984/2000 - Loss: -9.324
Iter 1985/2000 - Loss: -9.324
Iter 1986/2000 - Loss: -9.324
Iter 1987/2000 - Loss: -9.324
Iter 1988/2000 - Loss: -9.324
Iter 1989/2000 - Loss: -9.324
Iter 1990/2000 - Loss: -9.324
Iter 1991/2000 - Loss: -9.324
Iter 1992/2000 - Loss: -9.324
Iter 1993/2000 - Loss: -9.324
Iter 1994/2000 - Loss: -9.324
Iter 1995/2000 - Loss: -9.324
Iter 1996/2000 - Loss: -9.324
Iter 1997/2000 - Loss: -9.324
Iter 1998/2000 - Loss: -9.324
Iter 1999/2000 - Loss: -9.324
Iter 2000/2000 - Loss: -9.324
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.3530,  5.6626, 20.2894, 12.2196,  5.8285, 39.7579]],

        [[10.9222, 19.1606, 14.3050,  3.0012,  6.7499, 25.5824]],

        [[11.1146, 15.7967, 12.7848,  1.2850,  4.6843, 12.6478]],

        [[12.3495, 21.5348, 10.7503,  2.6234,  6.5830, 34.9802]]])
Signal Variance: tensor([ 0.0561,  2.0527, 11.3231,  0.2150])
Estimated target variance: tensor([0.0039, 0.0422, 0.3181, 0.0027])
N: 310
Signal to noise ratio: tensor([14.2342, 78.9464, 77.6338, 30.0363])
Bound on condition number: tensor([  62811.2086, 1932087.5197, 1868373.4725,  279677.2520])
Policy Optimizer learning rate:
0.009688861611972634
Experience 31, Iter 0, disc loss: 0.0020026711247771913, policy loss: 6.308542290827425
Experience 31, Iter 1, disc loss: 0.0021297985723626235, policy loss: 6.243248747856238
Experience 31, Iter 2, disc loss: 0.002272811282816226, policy loss: 6.178308651532229
Experience 31, Iter 3, disc loss: 0.0020412748007582527, policy loss: 6.302431255561137
Experience 31, Iter 4, disc loss: 0.0021843229743547343, policy loss: 6.202701227487223
Experience 31, Iter 5, disc loss: 0.0021132833381377666, policy loss: 6.249841420593243
Experience 31, Iter 6, disc loss: 0.0021474929320310695, policy loss: 6.23931672607034
Experience 31, Iter 7, disc loss: 0.002112202060228368, policy loss: 6.253904745661773
Experience 31, Iter 8, disc loss: 0.0022316430793207255, policy loss: 6.198163299749362
Experience 31, Iter 9, disc loss: 0.0023054775574564068, policy loss: 6.169472320184923
Experience 31, Iter 10, disc loss: 0.0020701653488576686, policy loss: 6.266140463245817
Experience 31, Iter 11, disc loss: 0.0020889649136372605, policy loss: 6.274746999002051
Experience 31, Iter 12, disc loss: 0.002130802551696982, policy loss: 6.238774326211996
Experience 31, Iter 13, disc loss: 0.0021399337018393667, policy loss: 6.226443903262652
Experience 31, Iter 14, disc loss: 0.0022058561522921767, policy loss: 6.205312693989381
Experience 31, Iter 15, disc loss: 0.002089755032724454, policy loss: 6.253769512918139
Experience 31, Iter 16, disc loss: 0.0021025078037819134, policy loss: 6.253924108979602
Experience 31, Iter 17, disc loss: 0.002218162059916207, policy loss: 6.207858119870908
Experience 31, Iter 18, disc loss: 0.002058489467869532, policy loss: 6.2784463501681325
Experience 31, Iter 19, disc loss: 0.0021039468927811587, policy loss: 6.261701981756631
Experience 31, Iter 20, disc loss: 0.0020823666545086933, policy loss: 6.248907692249043
Experience 31, Iter 21, disc loss: 0.0021069775592318703, policy loss: 6.236847047765211
Experience 31, Iter 22, disc loss: 0.0020787394051491733, policy loss: 6.27465790573373
Experience 31, Iter 23, disc loss: 0.0021071239866075385, policy loss: 6.264825525891529
Experience 31, Iter 24, disc loss: 0.002069471242497851, policy loss: 6.272629992732106
Experience 31, Iter 25, disc loss: 0.0021644378365508524, policy loss: 6.2152926715486805
Experience 31, Iter 26, disc loss: 0.002081588733840363, policy loss: 6.264349931370977
Experience 31, Iter 27, disc loss: 0.0020541026706025435, policy loss: 6.284142447718009
Experience 31, Iter 28, disc loss: 0.0019662359433761573, policy loss: 6.324082921404642
Experience 31, Iter 29, disc loss: 0.002103808351186782, policy loss: 6.243311389732369
Experience 31, Iter 30, disc loss: 0.002136067425439695, policy loss: 6.240364404809262
Experience 31, Iter 31, disc loss: 0.002139033236885174, policy loss: 6.224047971962928
Experience 31, Iter 32, disc loss: 0.0018930136182930529, policy loss: 6.401506185008726
Experience 31, Iter 33, disc loss: 0.0019324707694048874, policy loss: 6.328671121540862
Experience 31, Iter 34, disc loss: 0.0020219510722860987, policy loss: 6.281570448999025
Experience 31, Iter 35, disc loss: 0.0021287142541109227, policy loss: 6.238262111034944
Experience 31, Iter 36, disc loss: 0.0019858124578882117, policy loss: 6.317267595219336
Experience 31, Iter 37, disc loss: 0.0019436899481994196, policy loss: 6.345354219304275
Experience 31, Iter 38, disc loss: 0.002030116242647642, policy loss: 6.307211616306571
Experience 31, Iter 39, disc loss: 0.002039165646582152, policy loss: 6.283959546693984
Experience 31, Iter 40, disc loss: 0.0020307696531873184, policy loss: 6.310894736273631
Experience 31, Iter 41, disc loss: 0.0018817942786540608, policy loss: 6.367933095402527
Experience 31, Iter 42, disc loss: 0.0020389382135609603, policy loss: 6.294968495509032
Experience 31, Iter 43, disc loss: 0.0019476304501210321, policy loss: 6.336140207465598
Experience 31, Iter 44, disc loss: 0.0019321321697713064, policy loss: 6.3290612696020965
Experience 31, Iter 45, disc loss: 0.00183753914258474, policy loss: 6.39406130004458
Experience 31, Iter 46, disc loss: 0.0019025808855360371, policy loss: 6.345893805113571
Experience 31, Iter 47, disc loss: 0.0019722347677967023, policy loss: 6.314461109770894
Experience 31, Iter 48, disc loss: 0.001974791996528465, policy loss: 6.320791127340353
Experience 31, Iter 49, disc loss: 0.0018853967462169772, policy loss: 6.3761720357855545
Experience: 32
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0103],
        [0.0778],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5697e-03, 3.4804e-02, 3.1568e-02, 1.6472e-03, 9.6882e-05,
          2.9966e-01]],

        [[4.5697e-03, 3.4804e-02, 3.1568e-02, 1.6472e-03, 9.6882e-05,
          2.9966e-01]],

        [[4.5697e-03, 3.4804e-02, 3.1568e-02, 1.6472e-03, 9.6882e-05,
          2.9966e-01]],

        [[4.5697e-03, 3.4804e-02, 3.1568e-02, 1.6472e-03, 9.6882e-05,
          2.9966e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0039, 0.0414, 0.3113, 0.0026], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0039, 0.0414, 0.3113, 0.0026])
N: 320
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1281.0000, 1281.0000, 1281.0000, 1281.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.172
Iter 2/2000 - Loss: 0.165
Iter 3/2000 - Loss: -2.087
Iter 4/2000 - Loss: -1.759
Iter 5/2000 - Loss: -0.984
Iter 6/2000 - Loss: -1.270
Iter 7/2000 - Loss: -1.882
Iter 8/2000 - Loss: -2.154
Iter 9/2000 - Loss: -2.024
Iter 10/2000 - Loss: -1.776
Iter 11/2000 - Loss: -1.681
Iter 12/2000 - Loss: -1.789
Iter 13/2000 - Loss: -1.986
Iter 14/2000 - Loss: -2.143
Iter 15/2000 - Loss: -2.203
Iter 16/2000 - Loss: -2.193
Iter 17/2000 - Loss: -2.182
Iter 18/2000 - Loss: -2.226
Iter 19/2000 - Loss: -2.341
Iter 20/2000 - Loss: -2.508
Iter 1981/2000 - Loss: -9.325
Iter 1982/2000 - Loss: -9.325
Iter 1983/2000 - Loss: -9.325
Iter 1984/2000 - Loss: -9.325
Iter 1985/2000 - Loss: -9.325
Iter 1986/2000 - Loss: -9.325
Iter 1987/2000 - Loss: -9.325
Iter 1988/2000 - Loss: -9.325
Iter 1989/2000 - Loss: -9.325
Iter 1990/2000 - Loss: -9.325
Iter 1991/2000 - Loss: -9.325
Iter 1992/2000 - Loss: -9.325
Iter 1993/2000 - Loss: -9.325
Iter 1994/2000 - Loss: -9.325
Iter 1995/2000 - Loss: -9.325
Iter 1996/2000 - Loss: -9.325
Iter 1997/2000 - Loss: -9.325
Iter 1998/2000 - Loss: -9.325
Iter 1999/2000 - Loss: -9.325
Iter 2000/2000 - Loss: -9.325
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.3456,  5.7292, 20.1515, 12.3687,  5.6051, 39.5870]],

        [[10.8987, 19.0158, 14.6881,  3.0525,  6.6984, 25.9244]],

        [[11.1444, 15.6964, 12.4428,  1.2838,  4.5213, 12.7478]],

        [[12.1649, 21.3176, 10.6563,  2.6215,  6.5148, 34.5516]]])
Signal Variance: tensor([ 0.0565,  2.1268, 11.1883,  0.2113])
Estimated target variance: tensor([0.0039, 0.0414, 0.3113, 0.0026])
N: 320
Signal to noise ratio: tensor([14.1471, 80.5542, 77.0910, 29.7301])
Bound on condition number: tensor([  64045.6035, 2076474.1461, 1901766.2320,  282842.2617])
Policy Optimizer learning rate:
0.009678658753253001
Experience 32, Iter 0, disc loss: 0.0018783476628417973, policy loss: 6.37375449976157
Experience 32, Iter 1, disc loss: 0.0021764917780787126, policy loss: 6.207344636791003
Experience 32, Iter 2, disc loss: 0.0019844401788210266, policy loss: 6.294084486392562
Experience 32, Iter 3, disc loss: 0.0019913063194261114, policy loss: 6.314645330819012
Experience 32, Iter 4, disc loss: 0.0017761299883192762, policy loss: 6.424671596202376
Experience 32, Iter 5, disc loss: 0.0019895320701726693, policy loss: 6.29683951704806
Experience 32, Iter 6, disc loss: 0.0019495294093345317, policy loss: 6.329693496853951
Experience 32, Iter 7, disc loss: 0.0019171094825601164, policy loss: 6.360776467238269
Experience 32, Iter 8, disc loss: 0.0018304061492467256, policy loss: 6.413902478154672
Experience 32, Iter 9, disc loss: 0.0018657023699239243, policy loss: 6.38283827067359
Experience 32, Iter 10, disc loss: 0.0018736475877734655, policy loss: 6.368878571596742
Experience 32, Iter 11, disc loss: 0.0019101812221903915, policy loss: 6.3485930028293955
Experience 32, Iter 12, disc loss: 0.0020305292505184216, policy loss: 6.282430976064207
Experience 32, Iter 13, disc loss: 0.0019318007111909959, policy loss: 6.349441819064763
Experience 32, Iter 14, disc loss: 0.0018116372715532782, policy loss: 6.435745870054661
Experience 32, Iter 15, disc loss: 0.0019849371294714366, policy loss: 6.307333488227265
Experience 32, Iter 16, disc loss: 0.0017763687076362655, policy loss: 6.422952325167529
Experience 32, Iter 17, disc loss: 0.0017559945817003165, policy loss: 6.427428577130907
Experience 32, Iter 18, disc loss: 0.0018695502671872497, policy loss: 6.35819873412731
Experience 32, Iter 19, disc loss: 0.0018765561943211823, policy loss: 6.3594553737860435
Experience 32, Iter 20, disc loss: 0.001799543247636972, policy loss: 6.414981058264329
Experience 32, Iter 21, disc loss: 0.0019182707994365434, policy loss: 6.350644075921357
Experience 32, Iter 22, disc loss: 0.0017585905345179223, policy loss: 6.441597557343281
Experience 32, Iter 23, disc loss: 0.0018126272088676786, policy loss: 6.403768760278279
Experience 32, Iter 24, disc loss: 0.0018511465103135728, policy loss: 6.379796568880116
Experience 32, Iter 25, disc loss: 0.0017852373072542627, policy loss: 6.414170855910683
Experience 32, Iter 26, disc loss: 0.0018917240066662633, policy loss: 6.359396394616114
Experience 32, Iter 27, disc loss: 0.0019398301039465816, policy loss: 6.35197612886718
Experience 32, Iter 28, disc loss: 0.0017696051877931608, policy loss: 6.449296762486691
Experience 32, Iter 29, disc loss: 0.0017248869303077145, policy loss: 6.457216933195249
Experience 32, Iter 30, disc loss: 0.001764031395066065, policy loss: 6.450532769890732
Experience 32, Iter 31, disc loss: 0.0017504441206119194, policy loss: 6.444085759096188
Experience 32, Iter 32, disc loss: 0.001681111874443054, policy loss: 6.484507683236748
Experience 32, Iter 33, disc loss: 0.0017093977203470013, policy loss: 6.466975166375342
Experience 32, Iter 34, disc loss: 0.0017882913025451636, policy loss: 6.413488009930793
Experience 32, Iter 35, disc loss: 0.0018002918953316962, policy loss: 6.397997021229599
Experience 32, Iter 36, disc loss: 0.00185993551038406, policy loss: 6.369797092500638
Experience 32, Iter 37, disc loss: 0.0017341308117151783, policy loss: 6.477124714058361
Experience 32, Iter 38, disc loss: 0.0018100742379735778, policy loss: 6.401893117027943
Experience 32, Iter 39, disc loss: 0.0017184993998111127, policy loss: 6.452079587426107
Experience 32, Iter 40, disc loss: 0.001758681652071493, policy loss: 6.438900166662688
Experience 32, Iter 41, disc loss: 0.001660820139277663, policy loss: 6.491878568171079
Experience 32, Iter 42, disc loss: 0.0018532979121871653, policy loss: 6.401035771122785
Experience 32, Iter 43, disc loss: 0.0016299525750226639, policy loss: 6.5103460649764004
Experience 32, Iter 44, disc loss: 0.0017014492841194602, policy loss: 6.4723590714198345
Experience 32, Iter 45, disc loss: 0.0017306578361214512, policy loss: 6.454281767507944
Experience 32, Iter 46, disc loss: 0.0016168220016238065, policy loss: 6.52214035131863
Experience 32, Iter 47, disc loss: 0.0018541202589092726, policy loss: 6.38310356559205
Experience 32, Iter 48, disc loss: 0.0016386782768027044, policy loss: 6.523139477619442
Experience 32, Iter 49, disc loss: 0.0016720949788132159, policy loss: 6.506232256154138
Experience: 33
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0106],
        [0.0817],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.4576e-03, 3.3989e-02, 3.2139e-02, 1.6175e-03, 9.4006e-05,
          2.9970e-01]],

        [[4.4576e-03, 3.3989e-02, 3.2139e-02, 1.6175e-03, 9.4006e-05,
          2.9970e-01]],

        [[4.4576e-03, 3.3989e-02, 3.2139e-02, 1.6175e-03, 9.4006e-05,
          2.9970e-01]],

        [[4.4576e-03, 3.3989e-02, 3.2139e-02, 1.6175e-03, 9.4006e-05,
          2.9970e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0038, 0.0423, 0.3267, 0.0027], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0038, 0.0423, 0.3267, 0.0027])
N: 330
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1321.0000, 1321.0000, 1321.0000, 1321.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.136
Iter 2/2000 - Loss: 0.162
Iter 3/2000 - Loss: -2.056
Iter 4/2000 - Loss: -1.733
Iter 5/2000 - Loss: -0.970
Iter 6/2000 - Loss: -1.257
Iter 7/2000 - Loss: -1.860
Iter 8/2000 - Loss: -2.122
Iter 9/2000 - Loss: -1.985
Iter 10/2000 - Loss: -1.739
Iter 11/2000 - Loss: -1.651
Iter 12/2000 - Loss: -1.765
Iter 13/2000 - Loss: -1.961
Iter 14/2000 - Loss: -2.112
Iter 15/2000 - Loss: -2.163
Iter 16/2000 - Loss: -2.148
Iter 17/2000 - Loss: -2.136
Iter 18/2000 - Loss: -2.183
Iter 19/2000 - Loss: -2.302
Iter 20/2000 - Loss: -2.471
Iter 1981/2000 - Loss: -9.346
Iter 1982/2000 - Loss: -9.346
Iter 1983/2000 - Loss: -9.346
Iter 1984/2000 - Loss: -9.346
Iter 1985/2000 - Loss: -9.346
Iter 1986/2000 - Loss: -9.346
Iter 1987/2000 - Loss: -9.346
Iter 1988/2000 - Loss: -9.347
Iter 1989/2000 - Loss: -9.347
Iter 1990/2000 - Loss: -9.347
Iter 1991/2000 - Loss: -9.347
Iter 1992/2000 - Loss: -9.347
Iter 1993/2000 - Loss: -9.347
Iter 1994/2000 - Loss: -9.347
Iter 1995/2000 - Loss: -9.347
Iter 1996/2000 - Loss: -9.347
Iter 1997/2000 - Loss: -9.347
Iter 1998/2000 - Loss: -9.347
Iter 1999/2000 - Loss: -9.347
Iter 2000/2000 - Loss: -9.347
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[11.2574,  5.8002, 20.9790, 12.0215,  5.2855, 40.1602]],

        [[10.7483, 18.7356, 14.6330,  3.0040,  6.5366, 26.0204]],

        [[10.9243, 16.0361, 12.3202,  1.3188,  4.3443, 13.4255]],

        [[11.9199, 20.9344, 10.3565,  2.6818,  6.5508, 34.8258]]])
Signal Variance: tensor([ 0.0582,  2.1219, 11.7726,  0.2070])
Estimated target variance: tensor([0.0038, 0.0423, 0.3267, 0.0027])
N: 330
Signal to noise ratio: tensor([14.3414, 81.0933, 79.0605, 29.5635])
Bound on condition number: tensor([  67873.8615, 2170124.0186, 2062687.8522,  288422.0313])
Policy Optimizer learning rate:
0.009668466638656902
Experience 33, Iter 0, disc loss: 0.0016855546041121042, policy loss: 6.489517307054109
Experience 33, Iter 1, disc loss: 0.0017965835549731265, policy loss: 6.441033625588137
Experience 33, Iter 2, disc loss: 0.0017843453739037108, policy loss: 6.432481382208197
Experience 33, Iter 3, disc loss: 0.0017167686615983677, policy loss: 6.459195157732515
Experience 33, Iter 4, disc loss: 0.0017771268576156742, policy loss: 6.42536092010812
Experience 33, Iter 5, disc loss: 0.0017202420246104268, policy loss: 6.46064911246328
Experience 33, Iter 6, disc loss: 0.0016126521496692254, policy loss: 6.512912532591063
Experience 33, Iter 7, disc loss: 0.0016605697353541716, policy loss: 6.493417271984303
Experience 33, Iter 8, disc loss: 0.0016677653093250587, policy loss: 6.471809063632614
Experience 33, Iter 9, disc loss: 0.00171763822954644, policy loss: 6.448850977366079
Experience 33, Iter 10, disc loss: 0.0018297622705892227, policy loss: 6.393890021141827
Experience 33, Iter 11, disc loss: 0.0017492746757668469, policy loss: 6.458655394965304
Experience 33, Iter 12, disc loss: 0.0016729585279579044, policy loss: 6.486124022774336
Experience 33, Iter 13, disc loss: 0.0017844719808027304, policy loss: 6.410903054433688
Experience 33, Iter 14, disc loss: 0.0016865504555649013, policy loss: 6.4687827652005385
Experience 33, Iter 15, disc loss: 0.001718408397620158, policy loss: 6.441674631482872
Experience 33, Iter 16, disc loss: 0.001734110206537227, policy loss: 6.44807409241934
Experience 33, Iter 17, disc loss: 0.0016104850910886263, policy loss: 6.545252406748402
Experience 33, Iter 18, disc loss: 0.001628761385160712, policy loss: 6.519187555617844
Experience 33, Iter 19, disc loss: 0.0016375027160957364, policy loss: 6.512341555566261
Experience 33, Iter 20, disc loss: 0.0015557378799225564, policy loss: 6.592035497230434
Experience 33, Iter 21, disc loss: 0.001668977828229222, policy loss: 6.4942982809661896
Experience 33, Iter 22, disc loss: 0.001669760330939684, policy loss: 6.4819605004780145
Experience 33, Iter 23, disc loss: 0.0016312903658796995, policy loss: 6.511743726513427
Experience 33, Iter 24, disc loss: 0.0016980580045549327, policy loss: 6.476287911734509
Experience 33, Iter 25, disc loss: 0.0016397182974341824, policy loss: 6.522436421673669
Experience 33, Iter 26, disc loss: 0.001604211750462298, policy loss: 6.5411631211956305
Experience 33, Iter 27, disc loss: 0.0016472137575294628, policy loss: 6.498706055843903
Experience 33, Iter 28, disc loss: 0.0016919841936288897, policy loss: 6.472649841027537
Experience 33, Iter 29, disc loss: 0.0015753545613670318, policy loss: 6.5417657180664115
Experience 33, Iter 30, disc loss: 0.0016471222535467587, policy loss: 6.5083780352731955
Experience 33, Iter 31, disc loss: 0.00160941826802489, policy loss: 6.518365473979594
Experience 33, Iter 32, disc loss: 0.0015576973206896968, policy loss: 6.554878303129665
Experience 33, Iter 33, disc loss: 0.0016058736524996882, policy loss: 6.517545402507102
Experience 33, Iter 34, disc loss: 0.0014835364346334316, policy loss: 6.6215247294314565
Experience 33, Iter 35, disc loss: 0.001499447088784876, policy loss: 6.617122480936839
Experience 33, Iter 36, disc loss: 0.0016825777701306175, policy loss: 6.456081891058278
Experience 33, Iter 37, disc loss: 0.0015607433485448905, policy loss: 6.548184017585171
Experience 33, Iter 38, disc loss: 0.001670222341882124, policy loss: 6.485132940275641
Experience 33, Iter 39, disc loss: 0.0016063763404829516, policy loss: 6.5276622128422055
Experience 33, Iter 40, disc loss: 0.0016252302621560398, policy loss: 6.516402376334608
Experience 33, Iter 41, disc loss: 0.0015466790235689014, policy loss: 6.554215203479593
Experience 33, Iter 42, disc loss: 0.001550564186535733, policy loss: 6.571263816599686
Experience 33, Iter 43, disc loss: 0.0015312014778560727, policy loss: 6.579285321733636
Experience 33, Iter 44, disc loss: 0.0014651456000647181, policy loss: 6.615478602214074
Experience 33, Iter 45, disc loss: 0.0015037996343202809, policy loss: 6.595406142400062
Experience 33, Iter 46, disc loss: 0.0016234597634587474, policy loss: 6.516413822152347
Experience 33, Iter 47, disc loss: 0.0016358919724948388, policy loss: 6.520327880631912
Experience 33, Iter 48, disc loss: 0.0015282198516915375, policy loss: 6.580237578113939
Experience 33, Iter 49, disc loss: 0.001454975760869064, policy loss: 6.649029531583636
Experience: 34
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0106],
        [0.0831],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.4255e-03, 3.3355e-02, 3.1972e-02, 1.5837e-03, 9.1307e-05,
          2.9709e-01]],

        [[4.4255e-03, 3.3355e-02, 3.1972e-02, 1.5837e-03, 9.1307e-05,
          2.9709e-01]],

        [[4.4255e-03, 3.3355e-02, 3.1972e-02, 1.5837e-03, 9.1307e-05,
          2.9709e-01]],

        [[4.4255e-03, 3.3355e-02, 3.1972e-02, 1.5837e-03, 9.1307e-05,
          2.9709e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0038, 0.0424, 0.3325, 0.0026], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0038, 0.0424, 0.3325, 0.0026])
N: 340
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1361.0000, 1361.0000, 1361.0000, 1361.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.139
Iter 2/2000 - Loss: 0.171
Iter 3/2000 - Loss: -2.062
Iter 4/2000 - Loss: -1.738
Iter 5/2000 - Loss: -0.971
Iter 6/2000 - Loss: -1.262
Iter 7/2000 - Loss: -1.870
Iter 8/2000 - Loss: -2.129
Iter 9/2000 - Loss: -1.987
Iter 10/2000 - Loss: -1.739
Iter 11/2000 - Loss: -1.653
Iter 12/2000 - Loss: -1.770
Iter 13/2000 - Loss: -1.968
Iter 14/2000 - Loss: -2.116
Iter 15/2000 - Loss: -2.163
Iter 16/2000 - Loss: -2.143
Iter 17/2000 - Loss: -2.130
Iter 18/2000 - Loss: -2.178
Iter 19/2000 - Loss: -2.298
Iter 20/2000 - Loss: -2.466
Iter 1981/2000 - Loss: -9.313
Iter 1982/2000 - Loss: -9.313
Iter 1983/2000 - Loss: -9.313
Iter 1984/2000 - Loss: -9.313
Iter 1985/2000 - Loss: -9.313
Iter 1986/2000 - Loss: -9.313
Iter 1987/2000 - Loss: -9.313
Iter 1988/2000 - Loss: -9.313
Iter 1989/2000 - Loss: -9.313
Iter 1990/2000 - Loss: -9.313
Iter 1991/2000 - Loss: -9.313
Iter 1992/2000 - Loss: -9.313
Iter 1993/2000 - Loss: -9.313
Iter 1994/2000 - Loss: -9.314
Iter 1995/2000 - Loss: -9.314
Iter 1996/2000 - Loss: -9.314
Iter 1997/2000 - Loss: -9.314
Iter 1998/2000 - Loss: -9.314
Iter 1999/2000 - Loss: -9.314
Iter 2000/2000 - Loss: -9.314
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[11.2363,  5.8278, 20.8094, 11.6655,  5.1733, 39.4098]],

        [[10.7433, 18.5905, 14.8083,  3.0846,  6.5554, 26.1400]],

        [[ 8.0548, 16.5006, 14.0451,  0.1708,  1.3256, 10.8590]],

        [[11.6795, 20.5844, 10.4990,  2.7337,  6.5259, 35.6654]]])
Signal Variance: tensor([0.0590, 2.1474, 5.4476, 0.2114])
Estimated target variance: tensor([0.0038, 0.0424, 0.3325, 0.0026])
N: 340
Signal to noise ratio: tensor([14.4614, 81.2038, 58.3118, 30.0798])
Bound on condition number: tensor([  71106.3948, 2241981.1552, 1156090.8114,  307631.7683])
Policy Optimizer learning rate:
0.009658285256870233
Experience 34, Iter 0, disc loss: 0.0015651286972623562, policy loss: 6.539100845888564
Experience 34, Iter 1, disc loss: 0.0014900675151255731, policy loss: 6.602959918436872
Experience 34, Iter 2, disc loss: 0.001467708075057115, policy loss: 6.610866699374629
Experience 34, Iter 3, disc loss: 0.001566885024870764, policy loss: 6.5571197862444945
Experience 34, Iter 4, disc loss: 0.001466282132468753, policy loss: 6.636994382638108
Experience 34, Iter 5, disc loss: 0.0016175705400187266, policy loss: 6.511816426107101
Experience 34, Iter 6, disc loss: 0.0015881062536752717, policy loss: 6.5387065802578705
Experience 34, Iter 7, disc loss: 0.0016181313692449914, policy loss: 6.517562152785283
Experience 34, Iter 8, disc loss: 0.0015843487513712154, policy loss: 6.546376049426648
Experience 34, Iter 9, disc loss: 0.001521649925735064, policy loss: 6.58065447762821
Experience 34, Iter 10, disc loss: 0.0014481017870030077, policy loss: 6.620067863911588
Experience 34, Iter 11, disc loss: 0.0015814454427692398, policy loss: 6.5510400937640405
Experience 34, Iter 12, disc loss: 0.0015687740992070996, policy loss: 6.548419912394297
Experience 34, Iter 13, disc loss: 0.0015296866028583875, policy loss: 6.583813439472712
Experience 34, Iter 14, disc loss: 0.0014845558312283618, policy loss: 6.634642956919487
Experience 34, Iter 15, disc loss: 0.0015132304579150207, policy loss: 6.55964936784006
Experience 34, Iter 16, disc loss: 0.0015389614152530953, policy loss: 6.563447351963166
Experience 34, Iter 17, disc loss: 0.0014413202741240575, policy loss: 6.643009746195001
Experience 34, Iter 18, disc loss: 0.0015082809225276262, policy loss: 6.5793675707398345
Experience 34, Iter 19, disc loss: 0.0014936244614327501, policy loss: 6.602830549557261
Experience 34, Iter 20, disc loss: 0.0015586372054521881, policy loss: 6.578900767497805
Experience 34, Iter 21, disc loss: 0.0014881962841573246, policy loss: 6.6034048088609385
Experience 34, Iter 22, disc loss: 0.0015522594719007419, policy loss: 6.57168762200513
Experience 34, Iter 23, disc loss: 0.0014836115726719465, policy loss: 6.640392958050683
Experience 34, Iter 24, disc loss: 0.001519388936207135, policy loss: 6.569094526362141
Experience 34, Iter 25, disc loss: 0.0014868180497397412, policy loss: 6.604287221617716
Experience 34, Iter 26, disc loss: 0.0015008627849949052, policy loss: 6.5805521882627565
Experience 34, Iter 27, disc loss: 0.0014965108935004624, policy loss: 6.595846931057811
Experience 34, Iter 28, disc loss: 0.001409727527266593, policy loss: 6.6697982040571455
Experience 34, Iter 29, disc loss: 0.0015343830469448498, policy loss: 6.573774685372969
Experience 34, Iter 30, disc loss: 0.0014093874078968192, policy loss: 6.651021082028553
Experience 34, Iter 31, disc loss: 0.0015700517953738486, policy loss: 6.538839136912395
Experience 34, Iter 32, disc loss: 0.0015004523015935006, policy loss: 6.587555125452317
Experience 34, Iter 33, disc loss: 0.0014003925219713525, policy loss: 6.664641140727028
Experience 34, Iter 34, disc loss: 0.0014039356079012233, policy loss: 6.657371183110293
Experience 34, Iter 35, disc loss: 0.0013743774720794786, policy loss: 6.691491156415463
Experience 34, Iter 36, disc loss: 0.0014318500819199976, policy loss: 6.635716865979284
Experience 34, Iter 37, disc loss: 0.0014932854637324901, policy loss: 6.607186688831259
Experience 34, Iter 38, disc loss: 0.0014138410600894173, policy loss: 6.663643389513993
Experience 34, Iter 39, disc loss: 0.0015047881800034912, policy loss: 6.581544240570871
Experience 34, Iter 40, disc loss: 0.001416471495509838, policy loss: 6.648556578696089
Experience 34, Iter 41, disc loss: 0.0013962945067985237, policy loss: 6.682057428345388
Experience 34, Iter 42, disc loss: 0.0013768743742814268, policy loss: 6.6726324787471105
Experience 34, Iter 43, disc loss: 0.0014897781844798311, policy loss: 6.5989637564555945
Experience 34, Iter 44, disc loss: 0.001358482314439997, policy loss: 6.713146219784887
Experience 34, Iter 45, disc loss: 0.0015026610658568044, policy loss: 6.588678690470539
Experience 34, Iter 46, disc loss: 0.0014478070627457042, policy loss: 6.626223366457753
Experience 34, Iter 47, disc loss: 0.0014138010060142888, policy loss: 6.639738830922451
Experience 34, Iter 48, disc loss: 0.0014050453435276399, policy loss: 6.647920050631633
Experience 34, Iter 49, disc loss: 0.00135211837017128, policy loss: 6.700721315891135
Experience: 35
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0105],
        [0.0824],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.4629e-03, 3.3828e-02, 3.1537e-02, 1.5717e-03, 8.8811e-05,
          2.9356e-01]],

        [[4.4629e-03, 3.3828e-02, 3.1537e-02, 1.5717e-03, 8.8811e-05,
          2.9356e-01]],

        [[4.4629e-03, 3.3828e-02, 3.1537e-02, 1.5717e-03, 8.8811e-05,
          2.9356e-01]],

        [[4.4629e-03, 3.3828e-02, 3.1537e-02, 1.5717e-03, 8.8811e-05,
          2.9356e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0038, 0.0420, 0.3297, 0.0026], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0038, 0.0420, 0.3297, 0.0026])
N: 350
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1401.0000, 1401.0000, 1401.0000, 1401.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.143
Iter 2/2000 - Loss: 0.151
Iter 3/2000 - Loss: -2.069
Iter 4/2000 - Loss: -1.754
Iter 5/2000 - Loss: -0.988
Iter 6/2000 - Loss: -1.275
Iter 7/2000 - Loss: -1.878
Iter 8/2000 - Loss: -2.137
Iter 9/2000 - Loss: -1.996
Iter 10/2000 - Loss: -1.752
Iter 11/2000 - Loss: -1.669
Iter 12/2000 - Loss: -1.785
Iter 13/2000 - Loss: -1.978
Iter 14/2000 - Loss: -2.120
Iter 15/2000 - Loss: -2.164
Iter 16/2000 - Loss: -2.148
Iter 17/2000 - Loss: -2.141
Iter 18/2000 - Loss: -2.193
Iter 19/2000 - Loss: -2.310
Iter 20/2000 - Loss: -2.469
Iter 1981/2000 - Loss: -9.290
Iter 1982/2000 - Loss: -9.290
Iter 1983/2000 - Loss: -9.290
Iter 1984/2000 - Loss: -9.290
Iter 1985/2000 - Loss: -9.290
Iter 1986/2000 - Loss: -9.290
Iter 1987/2000 - Loss: -9.290
Iter 1988/2000 - Loss: -9.290
Iter 1989/2000 - Loss: -9.290
Iter 1990/2000 - Loss: -9.290
Iter 1991/2000 - Loss: -9.290
Iter 1992/2000 - Loss: -9.290
Iter 1993/2000 - Loss: -9.290
Iter 1994/2000 - Loss: -9.290
Iter 1995/2000 - Loss: -9.290
Iter 1996/2000 - Loss: -9.290
Iter 1997/2000 - Loss: -9.290
Iter 1998/2000 - Loss: -9.290
Iter 1999/2000 - Loss: -9.290
Iter 2000/2000 - Loss: -9.290
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0017],
        [0.0002]])
Lengthscale: tensor([[[11.1196,  5.8528, 19.6755, 11.3656,  5.1836, 38.1201]],

        [[10.8031, 18.5087, 14.8865,  3.1209,  6.4480, 26.2509]],

        [[ 7.0038,  4.4301, 11.4273,  0.4031,  0.0679,  5.4881]],

        [[11.6062, 20.4519, 10.3196,  2.6871,  6.5135, 35.2613]]])
Signal Variance: tensor([0.0597, 2.1826, 2.7909, 0.2060])
Estimated target variance: tensor([0.0038, 0.0420, 0.3297, 0.0026])
N: 350
Signal to noise ratio: tensor([14.5796, 81.7848, 41.0111, 29.0826])
Bound on condition number: tensor([  74398.3242, 2341063.0033,  588670.8537,  296031.0669])
Policy Optimizer learning rate:
0.009648114596590806
Experience 35, Iter 0, disc loss: 0.001352125837738426, policy loss: 6.719264971051938
Experience 35, Iter 1, disc loss: 0.0013681632664386274, policy loss: 6.704985326257929
Experience 35, Iter 2, disc loss: 0.0014235343733349354, policy loss: 6.642868000639327
Experience 35, Iter 3, disc loss: 0.0014013091482860541, policy loss: 6.664911440479005
Experience 35, Iter 4, disc loss: 0.0014248777951980471, policy loss: 6.626696615209444
Experience 35, Iter 5, disc loss: 0.001366037627202389, policy loss: 6.689803824059222
Experience 35, Iter 6, disc loss: 0.001390278281503467, policy loss: 6.676288217928824
Experience 35, Iter 7, disc loss: 0.0014669920796754681, policy loss: 6.610326739367577
Experience 35, Iter 8, disc loss: 0.0014952412548677732, policy loss: 6.602310287878037
Experience 35, Iter 9, disc loss: 0.0013966003412565274, policy loss: 6.663762545644377
Experience 35, Iter 10, disc loss: 0.0014778604144703464, policy loss: 6.617570561500932
Experience 35, Iter 11, disc loss: 0.0014359441065453045, policy loss: 6.63235672909139
Experience 35, Iter 12, disc loss: 0.0015217363574720223, policy loss: 6.567337413282815
Experience 35, Iter 13, disc loss: 0.001447783108423641, policy loss: 6.614206535028624
Experience 35, Iter 14, disc loss: 0.001374122987034449, policy loss: 6.688540138606751
Experience 35, Iter 15, disc loss: 0.0014042407433565286, policy loss: 6.679511795014184
Experience 35, Iter 16, disc loss: 0.0013749766084104503, policy loss: 6.677987169326208
Experience 35, Iter 17, disc loss: 0.0013197143257295987, policy loss: 6.717874760819921
Experience 35, Iter 18, disc loss: 0.0014228682959355468, policy loss: 6.647008203518812
Experience 35, Iter 19, disc loss: 0.0013762444390200803, policy loss: 6.675749176966484
Experience 35, Iter 20, disc loss: 0.0013999844957612003, policy loss: 6.655206986730307
Experience 35, Iter 21, disc loss: 0.0013596539854419407, policy loss: 6.6930569523533245
Experience 35, Iter 22, disc loss: 0.0013908078743533271, policy loss: 6.675115985825057
Experience 35, Iter 23, disc loss: 0.0013873359180094043, policy loss: 6.686599339072693
Experience 35, Iter 24, disc loss: 0.001330662794728221, policy loss: 6.727795012973425
Experience 35, Iter 25, disc loss: 0.001357892343612649, policy loss: 6.693703099975438
Experience 35, Iter 26, disc loss: 0.001302741780586452, policy loss: 6.736825405990679
Experience 35, Iter 27, disc loss: 0.0013862873464000568, policy loss: 6.656428018866864
Experience 35, Iter 28, disc loss: 0.0013364138981631012, policy loss: 6.712404228677576
Experience 35, Iter 29, disc loss: 0.0013917166172714066, policy loss: 6.678094376852065
Experience 35, Iter 30, disc loss: 0.001385937201972704, policy loss: 6.686639663306284
Experience 35, Iter 31, disc loss: 0.0013584151809826988, policy loss: 6.6799057652716085
Experience 35, Iter 32, disc loss: 0.001291340442286818, policy loss: 6.76267750107011
Experience 35, Iter 33, disc loss: 0.0014020536963044378, policy loss: 6.6500358911899236
Experience 35, Iter 34, disc loss: 0.0013179554404570012, policy loss: 6.698534124699494
Experience 35, Iter 35, disc loss: 0.0013325892515568048, policy loss: 6.704449911634663
Experience 35, Iter 36, disc loss: 0.0013323485266457577, policy loss: 6.721361579743399
Experience 35, Iter 37, disc loss: 0.0012966653353832375, policy loss: 6.733991079965548
Experience 35, Iter 38, disc loss: 0.0013003786575375566, policy loss: 6.747284769997913
Experience 35, Iter 39, disc loss: 0.0012807683541765493, policy loss: 6.736817252863138
Experience 35, Iter 40, disc loss: 0.0012266357273899113, policy loss: 6.78501612520023
Experience 35, Iter 41, disc loss: 0.0013229077241991194, policy loss: 6.712920211413367
Experience 35, Iter 42, disc loss: 0.0013638852720696912, policy loss: 6.692930313418038
Experience 35, Iter 43, disc loss: 0.0011816781045854842, policy loss: 6.839858504301626
Experience 35, Iter 44, disc loss: 0.0013399958944395097, policy loss: 6.736693049363354
Experience 35, Iter 45, disc loss: 0.0012573742093988352, policy loss: 6.769455778688087
Experience 35, Iter 46, disc loss: 0.001301417856089163, policy loss: 6.719052436487657
Experience 35, Iter 47, disc loss: 0.001251726385921162, policy loss: 6.767168326383873
Experience 35, Iter 48, disc loss: 0.0012530818652771295, policy loss: 6.764220872187253
Experience 35, Iter 49, disc loss: 0.0012662461108940682, policy loss: 6.7624568846325905
Experience: 36
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0105],
        [0.0829],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3975e-03, 3.3272e-02, 3.1181e-02, 1.5400e-03, 8.6401e-05,
          2.9003e-01]],

        [[4.3975e-03, 3.3272e-02, 3.1181e-02, 1.5400e-03, 8.6401e-05,
          2.9003e-01]],

        [[4.3975e-03, 3.3272e-02, 3.1181e-02, 1.5400e-03, 8.6401e-05,
          2.9003e-01]],

        [[4.3975e-03, 3.3272e-02, 3.1181e-02, 1.5400e-03, 8.6401e-05,
          2.9003e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0419, 0.3316, 0.0026], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0419, 0.3316, 0.0026])
N: 360
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1441.0000, 1441.0000, 1441.0000, 1441.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.156
Iter 2/2000 - Loss: 0.143
Iter 3/2000 - Loss: -2.084
Iter 4/2000 - Loss: -1.772
Iter 5/2000 - Loss: -1.003
Iter 6/2000 - Loss: -1.292
Iter 7/2000 - Loss: -1.898
Iter 8/2000 - Loss: -2.154
Iter 9/2000 - Loss: -2.011
Iter 10/2000 - Loss: -1.767
Iter 11/2000 - Loss: -1.687
Iter 12/2000 - Loss: -1.806
Iter 13/2000 - Loss: -1.998
Iter 14/2000 - Loss: -2.139
Iter 15/2000 - Loss: -2.181
Iter 16/2000 - Loss: -2.166
Iter 17/2000 - Loss: -2.164
Iter 18/2000 - Loss: -2.220
Iter 19/2000 - Loss: -2.340
Iter 20/2000 - Loss: -2.500
Iter 1981/2000 - Loss: -9.315
Iter 1982/2000 - Loss: -9.315
Iter 1983/2000 - Loss: -9.315
Iter 1984/2000 - Loss: -9.315
Iter 1985/2000 - Loss: -9.315
Iter 1986/2000 - Loss: -9.315
Iter 1987/2000 - Loss: -9.315
Iter 1988/2000 - Loss: -9.315
Iter 1989/2000 - Loss: -9.315
Iter 1990/2000 - Loss: -9.315
Iter 1991/2000 - Loss: -9.315
Iter 1992/2000 - Loss: -9.315
Iter 1993/2000 - Loss: -9.315
Iter 1994/2000 - Loss: -9.315
Iter 1995/2000 - Loss: -9.315
Iter 1996/2000 - Loss: -9.315
Iter 1997/2000 - Loss: -9.315
Iter 1998/2000 - Loss: -9.315
Iter 1999/2000 - Loss: -9.315
Iter 2000/2000 - Loss: -9.315
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.9446,  5.8799, 19.4660, 10.9987,  5.0234, 37.5348]],

        [[10.5566, 18.3401, 14.7325,  3.0497,  6.2458, 26.1807]],

        [[ 7.2803,  3.4960,  9.7757,  0.4116,  0.0641,  4.9620]],

        [[11.4062, 20.2393, 10.3888,  2.7383,  6.5653, 36.2340]]])
Signal Variance: tensor([0.0609, 2.1328, 2.4865, 0.2094])
Estimated target variance: tensor([0.0037, 0.0419, 0.3316, 0.0026])
N: 360
Signal to noise ratio: tensor([14.7401, 81.5948, 39.2866, 29.4255])
Bound on condition number: tensor([  78218.1791, 2396775.4155,  555637.4914,  311710.4513])
Policy Optimizer learning rate:
0.009637954646528333
Experience 36, Iter 0, disc loss: 0.0012826784380069983, policy loss: 6.7658931061032614
Experience 36, Iter 1, disc loss: 0.001178519846585663, policy loss: 6.849163370909529
Experience 36, Iter 2, disc loss: 0.0012436910071714413, policy loss: 6.775356618959592
Experience 36, Iter 3, disc loss: 0.00126835075726181, policy loss: 6.771803822802017
Experience 36, Iter 4, disc loss: 0.0013150505313256058, policy loss: 6.721732592706664
Experience 36, Iter 5, disc loss: 0.0011755579151966479, policy loss: 6.826899201332445
Experience 36, Iter 6, disc loss: 0.0012030161541956015, policy loss: 6.803712817518406
Experience 36, Iter 7, disc loss: 0.0012767799799504585, policy loss: 6.74615978310074
Experience 36, Iter 8, disc loss: 0.0012326208674575446, policy loss: 6.795478353022896
Experience 36, Iter 9, disc loss: 0.0012898879335163974, policy loss: 6.751829987108806
Experience 36, Iter 10, disc loss: 0.0011880960688556343, policy loss: 6.841377054103845
Experience 36, Iter 11, disc loss: 0.0012117170078135928, policy loss: 6.8134411793058725
Experience 36, Iter 12, disc loss: 0.0012598206412916173, policy loss: 6.7567579095123875
Experience 36, Iter 13, disc loss: 0.0012633231719602314, policy loss: 6.758550085461579
Experience 36, Iter 14, disc loss: 0.0012626398804836793, policy loss: 6.775255746345293
Experience 36, Iter 15, disc loss: 0.0012782525279379513, policy loss: 6.7403830756622565
Experience 36, Iter 16, disc loss: 0.001209197378874542, policy loss: 6.8395077935770665
Experience 36, Iter 17, disc loss: 0.0012785872854097471, policy loss: 6.732857645933715
Experience 36, Iter 18, disc loss: 0.0011204396639634055, policy loss: 6.881263826021259
Experience 36, Iter 19, disc loss: 0.0011842574958077579, policy loss: 6.830478896812877
Experience 36, Iter 20, disc loss: 0.0012096437509353094, policy loss: 6.801624429261118
Experience 36, Iter 21, disc loss: 0.0012594412499549541, policy loss: 6.756481055849299
Experience 36, Iter 22, disc loss: 0.0012098034113920987, policy loss: 6.7971428469210124
Experience 36, Iter 23, disc loss: 0.0013370884948175402, policy loss: 6.688017851520275
Experience 36, Iter 24, disc loss: 0.0012985024646968647, policy loss: 6.74210557904159
Experience 36, Iter 25, disc loss: 0.001174024774183274, policy loss: 6.8469901981215955
Experience 36, Iter 26, disc loss: 0.001224491011463519, policy loss: 6.794228269701534
Experience 36, Iter 27, disc loss: 0.001213420734065503, policy loss: 6.807594506617097
Experience 36, Iter 28, disc loss: 0.0012605429263888106, policy loss: 6.75982978614995
Experience 36, Iter 29, disc loss: 0.0011808532548968275, policy loss: 6.832064909854902
Experience 36, Iter 30, disc loss: 0.001194997489078134, policy loss: 6.833292722399877
Experience 36, Iter 31, disc loss: 0.0011903904849871582, policy loss: 6.812426423365274
Experience 36, Iter 32, disc loss: 0.0012708641707525443, policy loss: 6.74729026256137
Experience 36, Iter 33, disc loss: 0.0012279430473396435, policy loss: 6.791669010399781
Experience 36, Iter 34, disc loss: 0.0011376106429770959, policy loss: 6.874556335863684
Experience 36, Iter 35, disc loss: 0.001194206053097384, policy loss: 6.811893602922227
Experience 36, Iter 36, disc loss: 0.001192914770040532, policy loss: 6.822488295957948
Experience 36, Iter 37, disc loss: 0.0012410117278599294, policy loss: 6.770659412665978
Experience 36, Iter 38, disc loss: 0.0011929750854005705, policy loss: 6.812978011326723
Experience 36, Iter 39, disc loss: 0.0011516471778684452, policy loss: 6.887895963673408
Experience 36, Iter 40, disc loss: 0.0011519412923503666, policy loss: 6.8765233552002165
Experience 36, Iter 41, disc loss: 0.0012206033987784205, policy loss: 6.789352163612717
Experience 36, Iter 42, disc loss: 0.0011551007233420417, policy loss: 6.853433497031406
Experience 36, Iter 43, disc loss: 0.0011104394311157056, policy loss: 6.90731526583804
Experience 36, Iter 44, disc loss: 0.001293436775233932, policy loss: 6.721666571651514
Experience 36, Iter 45, disc loss: 0.0012009164320636939, policy loss: 6.815856553642082
Experience 36, Iter 46, disc loss: 0.001131656742718756, policy loss: 6.876753124789543
Experience 36, Iter 47, disc loss: 0.001192867206682923, policy loss: 6.821734888106544
Experience 36, Iter 48, disc loss: 0.0012036042887670325, policy loss: 6.827265948468449
Experience 36, Iter 49, disc loss: 0.0011968888580194438, policy loss: 6.812375210939649
Experience: 37
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0103],
        [0.0818],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3129e-03, 3.2655e-02, 3.0566e-02, 1.5136e-03, 8.4101e-05,
          2.8496e-01]],

        [[4.3129e-03, 3.2655e-02, 3.0566e-02, 1.5136e-03, 8.4101e-05,
          2.8496e-01]],

        [[4.3129e-03, 3.2655e-02, 3.0566e-02, 1.5136e-03, 8.4101e-05,
          2.8496e-01]],

        [[4.3129e-03, 3.2655e-02, 3.0566e-02, 1.5136e-03, 8.4101e-05,
          2.8496e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0037, 0.0413, 0.3272, 0.0025], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0037, 0.0413, 0.3272, 0.0025])
N: 370
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1481.0000, 1481.0000, 1481.0000, 1481.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.183
Iter 2/2000 - Loss: 0.135
Iter 3/2000 - Loss: -2.112
Iter 4/2000 - Loss: -1.799
Iter 5/2000 - Loss: -1.023
Iter 6/2000 - Loss: -1.317
Iter 7/2000 - Loss: -1.928
Iter 8/2000 - Loss: -2.185
Iter 9/2000 - Loss: -2.037
Iter 10/2000 - Loss: -1.790
Iter 11/2000 - Loss: -1.711
Iter 12/2000 - Loss: -1.832
Iter 13/2000 - Loss: -2.027
Iter 14/2000 - Loss: -2.167
Iter 15/2000 - Loss: -2.207
Iter 16/2000 - Loss: -2.189
Iter 17/2000 - Loss: -2.183
Iter 18/2000 - Loss: -2.239
Iter 19/2000 - Loss: -2.358
Iter 20/2000 - Loss: -2.517
Iter 1981/2000 - Loss: -9.311
Iter 1982/2000 - Loss: -9.311
Iter 1983/2000 - Loss: -9.311
Iter 1984/2000 - Loss: -9.311
Iter 1985/2000 - Loss: -9.311
Iter 1986/2000 - Loss: -9.311
Iter 1987/2000 - Loss: -9.311
Iter 1988/2000 - Loss: -9.311
Iter 1989/2000 - Loss: -9.311
Iter 1990/2000 - Loss: -9.311
Iter 1991/2000 - Loss: -9.311
Iter 1992/2000 - Loss: -9.311
Iter 1993/2000 - Loss: -9.311
Iter 1994/2000 - Loss: -9.311
Iter 1995/2000 - Loss: -9.311
Iter 1996/2000 - Loss: -9.311
Iter 1997/2000 - Loss: -9.311
Iter 1998/2000 - Loss: -9.311
Iter 1999/2000 - Loss: -9.311
Iter 2000/2000 - Loss: -9.311
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.9623,  5.8417, 19.2639, 10.7518,  4.9110, 36.9847]],

        [[10.4880, 18.3957, 14.6059,  3.1821,  6.3540, 26.5297]],

        [[ 8.9970,  9.4714, 11.9287,  0.1776,  1.2161,  7.7513]],

        [[11.3712, 20.0746, 10.2368,  2.7449,  6.5976, 35.7565]]])
Signal Variance: tensor([0.0601, 2.1832, 3.7373, 0.2062])
Estimated target variance: tensor([0.0037, 0.0413, 0.3272, 0.0025])
N: 370
Signal to noise ratio: tensor([14.5445, 82.1136, 48.1936, 28.8658])
Bound on condition number: tensor([  78271.4790, 2494781.1871,  859370.2614,  308296.9789])
Policy Optimizer learning rate:
0.00962780539540442
Experience 37, Iter 0, disc loss: 0.0011160333667682033, policy loss: 6.888580992900776
Experience 37, Iter 1, disc loss: 0.0011134542010176863, policy loss: 6.900849748023932
Experience 37, Iter 2, disc loss: 0.0010880469306980827, policy loss: 6.914697075310347
Experience 37, Iter 3, disc loss: 0.0011638675136611974, policy loss: 6.852409009708714
Experience 37, Iter 4, disc loss: 0.0011783893465949278, policy loss: 6.865993625517943
Experience 37, Iter 5, disc loss: 0.0010407000431106154, policy loss: 6.980167199555904
Experience 37, Iter 6, disc loss: 0.0011402441598465124, policy loss: 6.857113684926068
Experience 37, Iter 7, disc loss: 0.001121774369871797, policy loss: 6.89267023966014
Experience 37, Iter 8, disc loss: 0.0011325306282480132, policy loss: 6.868212166261124
Experience 37, Iter 9, disc loss: 0.001105334331885381, policy loss: 6.9332025183790424
Experience 37, Iter 10, disc loss: 0.001222391918465606, policy loss: 6.829387814678762
Experience 37, Iter 11, disc loss: 0.0011318378100196984, policy loss: 6.856259178627297
Experience 37, Iter 12, disc loss: 0.0011675095911535436, policy loss: 6.842491642622468
Experience 37, Iter 13, disc loss: 0.0011375604561689912, policy loss: 6.861262254654627
Experience 37, Iter 14, disc loss: 0.0010325939536599284, policy loss: 6.996810897700936
Experience 37, Iter 15, disc loss: 0.0011322934117107598, policy loss: 6.861367999187662
Experience 37, Iter 16, disc loss: 0.0011497564186354267, policy loss: 6.840301468705849
Experience 37, Iter 17, disc loss: 0.0010808546196635687, policy loss: 6.919622842863937
Experience 37, Iter 18, disc loss: 0.0011676299370089963, policy loss: 6.842390730368424
Experience 37, Iter 19, disc loss: 0.0009967711933201765, policy loss: 7.022225015638425
Experience 37, Iter 20, disc loss: 0.0011342706635286658, policy loss: 6.882125632334482
Experience 37, Iter 21, disc loss: 0.0011559011810899874, policy loss: 6.840222722159451
Experience 37, Iter 22, disc loss: 0.0011412346318389341, policy loss: 6.851619061261589
Experience 37, Iter 23, disc loss: 0.0010415301362438805, policy loss: 6.972986544797608
Experience 37, Iter 24, disc loss: 0.0010982333334299535, policy loss: 6.92165287518106
Experience 37, Iter 25, disc loss: 0.0011962246857119738, policy loss: 6.8188856776783915
Experience 37, Iter 26, disc loss: 0.0010884792713134752, policy loss: 6.922047289412423
Experience 37, Iter 27, disc loss: 0.001073952141167618, policy loss: 6.935971615988489
Experience 37, Iter 28, disc loss: 0.0010592559098839042, policy loss: 6.947130247776706
Experience 37, Iter 29, disc loss: 0.0010836460874059674, policy loss: 6.924321304605883
Experience 37, Iter 30, disc loss: 0.0011530122319951212, policy loss: 6.847158833222709
Experience 37, Iter 31, disc loss: 0.001086410071111016, policy loss: 6.932696188441583
Experience 37, Iter 32, disc loss: 0.0010656951294854065, policy loss: 6.944215722142406
Experience 37, Iter 33, disc loss: 0.0011466026272443437, policy loss: 6.847851184799319
Experience 37, Iter 34, disc loss: 0.001148035411408404, policy loss: 6.840210088276185
Experience 37, Iter 35, disc loss: 0.0010663460945948245, policy loss: 6.9653441636884
Experience 37, Iter 36, disc loss: 0.001033684001073171, policy loss: 6.990069274252212
Experience 37, Iter 37, disc loss: 0.001040789042724562, policy loss: 6.944740560306286
Experience 37, Iter 38, disc loss: 0.0011371345311146134, policy loss: 6.860933377253271
Experience 37, Iter 39, disc loss: 0.0011416476992377432, policy loss: 6.86477665563813
Experience 37, Iter 40, disc loss: 0.0010626646680015623, policy loss: 6.964421390898619
Experience 37, Iter 41, disc loss: 0.0010936626197029261, policy loss: 6.912190433992327
Experience 37, Iter 42, disc loss: 0.001145049106990437, policy loss: 6.85393055243777
Experience 37, Iter 43, disc loss: 0.0010747064120717165, policy loss: 6.944585063917102
Experience 37, Iter 44, disc loss: 0.0010646733652719114, policy loss: 6.927888596651088
Experience 37, Iter 45, disc loss: 0.0010618006788668196, policy loss: 6.94706448073922
Experience 37, Iter 46, disc loss: 0.0010339022188198303, policy loss: 6.967676697993247
Experience 37, Iter 47, disc loss: 0.00111976456594501, policy loss: 6.895786505824572
Experience 37, Iter 48, disc loss: 0.001122504789834664, policy loss: 6.8767163088465155
Experience 37, Iter 49, disc loss: 0.0010946900040286725, policy loss: 6.895345630898
Experience: 38
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0101],
        [0.0799],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.2379e-03, 3.2075e-02, 2.9809e-02, 1.4830e-03, 8.1927e-05,
          2.7844e-01]],

        [[4.2379e-03, 3.2075e-02, 2.9809e-02, 1.4830e-03, 8.1927e-05,
          2.7844e-01]],

        [[4.2379e-03, 3.2075e-02, 2.9809e-02, 1.4830e-03, 8.1927e-05,
          2.7844e-01]],

        [[4.2379e-03, 3.2075e-02, 2.9809e-02, 1.4830e-03, 8.1927e-05,
          2.7844e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0036, 0.0403, 0.3195, 0.0024], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0036, 0.0403, 0.3195, 0.0024])
N: 380
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1521.0000, 1521.0000, 1521.0000, 1521.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.222
Iter 2/2000 - Loss: 0.136
Iter 3/2000 - Loss: -2.151
Iter 4/2000 - Loss: -1.835
Iter 5/2000 - Loss: -1.045
Iter 6/2000 - Loss: -1.342
Iter 7/2000 - Loss: -1.964
Iter 8/2000 - Loss: -2.227
Iter 9/2000 - Loss: -2.078
Iter 10/2000 - Loss: -1.827
Iter 11/2000 - Loss: -1.747
Iter 12/2000 - Loss: -1.869
Iter 13/2000 - Loss: -2.065
Iter 14/2000 - Loss: -2.208
Iter 15/2000 - Loss: -2.251
Iter 16/2000 - Loss: -2.236
Iter 17/2000 - Loss: -2.231
Iter 18/2000 - Loss: -2.286
Iter 19/2000 - Loss: -2.403
Iter 20/2000 - Loss: -2.560
Iter 1981/2000 - Loss: -9.311
Iter 1982/2000 - Loss: -9.311
Iter 1983/2000 - Loss: -9.311
Iter 1984/2000 - Loss: -9.311
Iter 1985/2000 - Loss: -9.311
Iter 1986/2000 - Loss: -9.311
Iter 1987/2000 - Loss: -9.311
Iter 1988/2000 - Loss: -9.311
Iter 1989/2000 - Loss: -9.311
Iter 1990/2000 - Loss: -9.311
Iter 1991/2000 - Loss: -9.311
Iter 1992/2000 - Loss: -9.311
Iter 1993/2000 - Loss: -9.311
Iter 1994/2000 - Loss: -9.311
Iter 1995/2000 - Loss: -9.311
Iter 1996/2000 - Loss: -9.311
Iter 1997/2000 - Loss: -9.311
Iter 1998/2000 - Loss: -9.311
Iter 1999/2000 - Loss: -9.311
Iter 2000/2000 - Loss: -9.311
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.8715,  5.7527, 19.2106, 10.4778,  4.8485, 36.7943]],

        [[10.4434, 18.3363, 14.6851,  3.2169,  6.2734, 26.7362]],

        [[ 8.8149,  7.6124, 11.2305,  0.1786,  1.1860,  6.6069]],

        [[11.2165, 19.9359, 10.2548,  2.7447,  6.5277, 35.5878]]])
Signal Variance: tensor([0.0593, 2.2175, 3.2277, 0.2058])
Estimated target variance: tensor([0.0036, 0.0403, 0.3195, 0.0024])
N: 380
Signal to noise ratio: tensor([14.2438, 82.5077, 45.3291, 28.8958])
Bound on condition number: tensor([  77098.1156, 2586859.8014,  780795.8947,  317287.6990])
Policy Optimizer learning rate:
0.009617666831952545
Experience 38, Iter 0, disc loss: 0.001118742737018778, policy loss: 6.878086968454825
Experience 38, Iter 1, disc loss: 0.0010346610859631653, policy loss: 6.95651587317071
Experience 38, Iter 2, disc loss: 0.001049498726267371, policy loss: 6.946069331080211
Experience 38, Iter 3, disc loss: 0.00110247734211218, policy loss: 6.8966149668243535
Experience 38, Iter 4, disc loss: 0.0010992789279905533, policy loss: 6.887249242024375
Experience 38, Iter 5, disc loss: 0.0010675102203406125, policy loss: 6.942449541213051
Experience 38, Iter 6, disc loss: 0.0010289758442788546, policy loss: 7.0068271999293605
Experience 38, Iter 7, disc loss: 0.0009656221724865393, policy loss: 7.071411095254431
Experience 38, Iter 8, disc loss: 0.00109341042925239, policy loss: 6.894816334500622
Experience 38, Iter 9, disc loss: 0.0010329695748690607, policy loss: 6.9594738200499595
Experience 38, Iter 10, disc loss: 0.001019170383000896, policy loss: 6.987130635819813
Experience 38, Iter 11, disc loss: 0.0010056905962858233, policy loss: 6.996979608010359
Experience 38, Iter 12, disc loss: 0.0010453040980659206, policy loss: 6.951821299559956
Experience 38, Iter 13, disc loss: 0.0009798486415056112, policy loss: 7.042456949457798
Experience 38, Iter 14, disc loss: 0.000992488378408381, policy loss: 7.021342491494183
Experience 38, Iter 15, disc loss: 0.0009679723942933356, policy loss: 7.036527022696876
Experience 38, Iter 16, disc loss: 0.0009926197170249571, policy loss: 7.0034458788495435
Experience 38, Iter 17, disc loss: 0.0009874787536799576, policy loss: 7.005255921890842
Experience 38, Iter 18, disc loss: 0.0009697959761453944, policy loss: 7.029938356724103
Experience 38, Iter 19, disc loss: 0.0010351032363788905, policy loss: 6.958747812768445
Experience 38, Iter 20, disc loss: 0.0010527375109662485, policy loss: 6.94335009465142
Experience 38, Iter 21, disc loss: 0.0010467352977054794, policy loss: 6.975001373181303
Experience 38, Iter 22, disc loss: 0.0010705846110297026, policy loss: 6.915984317707198
Experience 38, Iter 23, disc loss: 0.0009536145516668036, policy loss: 7.047585069743473
Experience 38, Iter 24, disc loss: 0.0009706564140273926, policy loss: 7.029011997180774
Experience 38, Iter 25, disc loss: 0.0009471614742777961, policy loss: 7.044541317973488
Experience 38, Iter 26, disc loss: 0.0010538471299433162, policy loss: 6.942329638809852
Experience 38, Iter 27, disc loss: 0.001001815955949357, policy loss: 6.997634640241717
Experience 38, Iter 28, disc loss: 0.0009807738628098733, policy loss: 7.006273632792352
Experience 38, Iter 29, disc loss: 0.0009894596792970093, policy loss: 7.045783049635009
Experience 38, Iter 30, disc loss: 0.0009725136032627523, policy loss: 7.054651262932806
Experience 38, Iter 31, disc loss: 0.0009889220945871707, policy loss: 7.010800033922861
Experience 38, Iter 32, disc loss: 0.0010288944381433311, policy loss: 6.959510704359403
Experience 38, Iter 33, disc loss: 0.0009935903457455439, policy loss: 7.012381756291434
Experience 38, Iter 34, disc loss: 0.0010616652843744932, policy loss: 6.927705298349396
Experience 38, Iter 35, disc loss: 0.000993540312552228, policy loss: 7.001213330376805
Experience 38, Iter 36, disc loss: 0.0009956665155100787, policy loss: 7.0114264091081
Experience 38, Iter 37, disc loss: 0.0009330417389781142, policy loss: 7.062109151495132
Experience 38, Iter 38, disc loss: 0.0009778738711494026, policy loss: 7.026677463221708
Experience 38, Iter 39, disc loss: 0.0009809114698155715, policy loss: 7.011844810884913
Experience 38, Iter 40, disc loss: 0.0009889747087918615, policy loss: 7.00623320850899
Experience 38, Iter 41, disc loss: 0.0009673165197989926, policy loss: 7.018301004157312
Experience 38, Iter 42, disc loss: 0.0009766603453084536, policy loss: 7.023879853781738
Experience 38, Iter 43, disc loss: 0.0010044396538930645, policy loss: 7.019343463882123
Experience 38, Iter 44, disc loss: 0.0009900415776536503, policy loss: 7.022307650231329
Experience 38, Iter 45, disc loss: 0.0009566455313386284, policy loss: 7.041252552600416
Experience 38, Iter 46, disc loss: 0.0010171870618969365, policy loss: 6.972715820295858
Experience 38, Iter 47, disc loss: 0.0009025091611091796, policy loss: 7.090116517417589
Experience 38, Iter 48, disc loss: 0.0009209084263248889, policy loss: 7.075375964285813
Experience 38, Iter 49, disc loss: 0.0010205944801920885, policy loss: 6.974939989274298
Experience: 39
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0103],
        [0.0827],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3346e-03, 3.1446e-02, 3.0039e-02, 1.4507e-03, 7.9921e-05,
          2.7843e-01]],

        [[4.3346e-03, 3.1446e-02, 3.0039e-02, 1.4507e-03, 7.9921e-05,
          2.7843e-01]],

        [[4.3346e-03, 3.1446e-02, 3.0039e-02, 1.4507e-03, 7.9921e-05,
          2.7843e-01]],

        [[4.3346e-03, 3.1446e-02, 3.0039e-02, 1.4507e-03, 7.9921e-05,
          2.7843e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0410, 0.3309, 0.0024], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0410, 0.3309, 0.0024])
N: 390
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1561.0000, 1561.0000, 1561.0000, 1561.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.208
Iter 2/2000 - Loss: 0.170
Iter 3/2000 - Loss: -2.138
Iter 4/2000 - Loss: -1.821
Iter 5/2000 - Loss: -1.024
Iter 6/2000 - Loss: -1.324
Iter 7/2000 - Loss: -1.952
Iter 8/2000 - Loss: -2.215
Iter 9/2000 - Loss: -2.064
Iter 10/2000 - Loss: -1.811
Iter 11/2000 - Loss: -1.732
Iter 12/2000 - Loss: -1.855
Iter 13/2000 - Loss: -2.052
Iter 14/2000 - Loss: -2.193
Iter 15/2000 - Loss: -2.234
Iter 16/2000 - Loss: -2.217
Iter 17/2000 - Loss: -2.213
Iter 18/2000 - Loss: -2.267
Iter 19/2000 - Loss: -2.382
Iter 20/2000 - Loss: -2.536
Iter 1981/2000 - Loss: -9.332
Iter 1982/2000 - Loss: -9.332
Iter 1983/2000 - Loss: -9.332
Iter 1984/2000 - Loss: -9.332
Iter 1985/2000 - Loss: -9.332
Iter 1986/2000 - Loss: -9.332
Iter 1987/2000 - Loss: -9.332
Iter 1988/2000 - Loss: -9.332
Iter 1989/2000 - Loss: -9.332
Iter 1990/2000 - Loss: -9.332
Iter 1991/2000 - Loss: -9.332
Iter 1992/2000 - Loss: -9.332
Iter 1993/2000 - Loss: -9.332
Iter 1994/2000 - Loss: -9.332
Iter 1995/2000 - Loss: -9.332
Iter 1996/2000 - Loss: -9.332
Iter 1997/2000 - Loss: -9.332
Iter 1998/2000 - Loss: -9.332
Iter 1999/2000 - Loss: -9.332
Iter 2000/2000 - Loss: -9.332
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.7895,  5.7332, 19.0679, 10.6547,  4.8730, 37.1863]],

        [[10.4642, 18.1952, 14.4638,  3.1895,  6.1516, 26.5959]],

        [[ 8.8830,  8.1879, 11.8419,  0.1798,  1.2026,  7.1831]],

        [[11.2280, 19.9615, 10.4279,  2.7356,  6.3657, 35.7676]]])
Signal Variance: tensor([0.0579, 2.1622, 3.4933, 0.2062])
Estimated target variance: tensor([0.0035, 0.0410, 0.3309, 0.0024])
N: 390
Signal to noise ratio: tensor([14.1203, 81.9990, 47.1895, 28.8274])
Bound on condition number: tensor([  77760.4465, 2622297.1951,  868471.3282,  324097.8528])
Policy Optimizer learning rate:
0.00960753894491805
Experience 39, Iter 0, disc loss: 0.0009927420852027197, policy loss: 7.01587132596448
Experience 39, Iter 1, disc loss: 0.000955889219651202, policy loss: 7.033743713136461
Experience 39, Iter 2, disc loss: 0.0009383941904711054, policy loss: 7.068379458989612
Experience 39, Iter 3, disc loss: 0.001025438909914431, policy loss: 6.970273633960634
Experience 39, Iter 4, disc loss: 0.0009505185383215923, policy loss: 7.050463857059113
Experience 39, Iter 5, disc loss: 0.0009294468185196349, policy loss: 7.095687698281003
Experience 39, Iter 6, disc loss: 0.0009592781955213848, policy loss: 7.0429065079384845
Experience 39, Iter 7, disc loss: 0.0010743460920864837, policy loss: 6.927404205797542
Experience 39, Iter 8, disc loss: 0.0009327678267855284, policy loss: 7.071470832373855
Experience 39, Iter 9, disc loss: 0.0009806872704248176, policy loss: 7.018681877386275
Experience 39, Iter 10, disc loss: 0.0009627699689830227, policy loss: 7.047336034701419
Experience 39, Iter 11, disc loss: 0.0009566634570239749, policy loss: 7.0275241764584315
Experience 39, Iter 12, disc loss: 0.000908732586408592, policy loss: 7.09347981689614
Experience 39, Iter 13, disc loss: 0.0009970418939402, policy loss: 6.980830617222729
Experience 39, Iter 14, disc loss: 0.0009339497102853137, policy loss: 7.064464946801207
Experience 39, Iter 15, disc loss: 0.0009686441708677503, policy loss: 7.049319699362144
Experience 39, Iter 16, disc loss: 0.0009643921837955602, policy loss: 7.042707562340381
Experience 39, Iter 17, disc loss: 0.0009808486361540412, policy loss: 6.998738554673278
Experience 39, Iter 18, disc loss: 0.0009737827078816473, policy loss: 7.030457835560535
Experience 39, Iter 19, disc loss: 0.000896615883881858, policy loss: 7.0997260065428325
Experience 39, Iter 20, disc loss: 0.0009267648583897476, policy loss: 7.070841899464757
Experience 39, Iter 21, disc loss: 0.0009342652800248546, policy loss: 7.056519707162914
Experience 39, Iter 22, disc loss: 0.0008963332096820967, policy loss: 7.1076523370493785
Experience 39, Iter 23, disc loss: 0.0009337441871794587, policy loss: 7.084970148903928
Experience 39, Iter 24, disc loss: 0.0009593595299886601, policy loss: 7.054644323684927
Experience 39, Iter 25, disc loss: 0.000972167752478659, policy loss: 7.021337483948067
Experience 39, Iter 26, disc loss: 0.0009928573149100121, policy loss: 7.009320723502794
Experience 39, Iter 27, disc loss: 0.0009152218262479182, policy loss: 7.089773760281933
Experience 39, Iter 28, disc loss: 0.0009499390916643648, policy loss: 7.0388266199463425
Experience 39, Iter 29, disc loss: 0.0008641608859776095, policy loss: 7.14463775672978
Experience 39, Iter 30, disc loss: 0.0008673684053685399, policy loss: 7.140305344852813
Experience 39, Iter 31, disc loss: 0.0009059529222679651, policy loss: 7.093012279945404
Experience 39, Iter 32, disc loss: 0.0008839561077032764, policy loss: 7.124451202039268
Experience 39, Iter 33, disc loss: 0.0008882737378431168, policy loss: 7.1393082796182705
Experience 39, Iter 34, disc loss: 0.000953656795418935, policy loss: 7.034766806193162
Experience 39, Iter 35, disc loss: 0.0009113341668731379, policy loss: 7.0804199713447264
Experience 39, Iter 36, disc loss: 0.0009543146294208258, policy loss: 7.039911257387694
Experience 39, Iter 37, disc loss: 0.0008775331543839363, policy loss: 7.14259265272155
Experience 39, Iter 38, disc loss: 0.0008873588901530789, policy loss: 7.1237609899307675
Experience 39, Iter 39, disc loss: 0.000909831299263951, policy loss: 7.083501701336903
Experience 39, Iter 40, disc loss: 0.0008677848510521907, policy loss: 7.159336826743077
Experience 39, Iter 41, disc loss: 0.0008774254956853076, policy loss: 7.127736549491277
Experience 39, Iter 42, disc loss: 0.0009011030416604243, policy loss: 7.096077653616952
Experience 39, Iter 43, disc loss: 0.0009558526148490675, policy loss: 7.0349431442465145
Experience 39, Iter 44, disc loss: 0.0008729819484819759, policy loss: 7.130228805358269
Experience 39, Iter 45, disc loss: 0.0008819536969188689, policy loss: 7.118040343728653
Experience 39, Iter 46, disc loss: 0.0009169034651878558, policy loss: 7.081040558974191
Experience 39, Iter 47, disc loss: 0.0009009165755124113, policy loss: 7.131500276147612
Experience 39, Iter 48, disc loss: 0.0008650236697012632, policy loss: 7.13369333377835
Experience 39, Iter 49, disc loss: 0.0009464343948583811, policy loss: 7.056279474203237
Experience: 40
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0102],
        [0.0824],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3236e-03, 3.0960e-02, 2.9650e-02, 1.4204e-03, 7.7982e-05,
          2.7448e-01]],

        [[4.3236e-03, 3.0960e-02, 2.9650e-02, 1.4204e-03, 7.7982e-05,
          2.7448e-01]],

        [[4.3236e-03, 3.0960e-02, 2.9650e-02, 1.4204e-03, 7.7982e-05,
          2.7448e-01]],

        [[4.3236e-03, 3.0960e-02, 2.9650e-02, 1.4204e-03, 7.7982e-05,
          2.7448e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0407, 0.3297, 0.0024], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0407, 0.3297, 0.0024])
N: 400
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1601.0000, 1601.0000, 1601.0000, 1601.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.226
Iter 2/2000 - Loss: 0.172
Iter 3/2000 - Loss: -2.158
Iter 4/2000 - Loss: -1.841
Iter 5/2000 - Loss: -1.037
Iter 6/2000 - Loss: -1.340
Iter 7/2000 - Loss: -1.974
Iter 8/2000 - Loss: -2.238
Iter 9/2000 - Loss: -2.082
Iter 10/2000 - Loss: -1.827
Iter 11/2000 - Loss: -1.749
Iter 12/2000 - Loss: -1.875
Iter 13/2000 - Loss: -2.073
Iter 14/2000 - Loss: -2.212
Iter 15/2000 - Loss: -2.251
Iter 16/2000 - Loss: -2.233
Iter 17/2000 - Loss: -2.229
Iter 18/2000 - Loss: -2.284
Iter 19/2000 - Loss: -2.398
Iter 20/2000 - Loss: -2.550
Iter 1981/2000 - Loss: -9.374
Iter 1982/2000 - Loss: -9.374
Iter 1983/2000 - Loss: -9.374
Iter 1984/2000 - Loss: -9.374
Iter 1985/2000 - Loss: -9.374
Iter 1986/2000 - Loss: -9.374
Iter 1987/2000 - Loss: -9.374
Iter 1988/2000 - Loss: -9.374
Iter 1989/2000 - Loss: -9.374
Iter 1990/2000 - Loss: -9.374
Iter 1991/2000 - Loss: -9.374
Iter 1992/2000 - Loss: -9.374
Iter 1993/2000 - Loss: -9.374
Iter 1994/2000 - Loss: -9.374
Iter 1995/2000 - Loss: -9.374
Iter 1996/2000 - Loss: -9.374
Iter 1997/2000 - Loss: -9.374
Iter 1998/2000 - Loss: -9.374
Iter 1999/2000 - Loss: -9.374
Iter 2000/2000 - Loss: -9.374
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[10.7270,  5.7395, 18.8791, 10.7735,  4.8172, 36.9026]],

        [[10.3404, 17.8575, 14.4449,  3.0831,  5.9547, 26.1732]],

        [[ 8.7535,  7.3159, 11.5377,  0.1792,  1.1967,  6.9039]],

        [[11.1898, 19.7560, 10.4414,  2.7471,  6.3114, 35.6589]]])
Signal Variance: tensor([0.0574, 2.0785, 3.3508, 0.2059])
Estimated target variance: tensor([0.0035, 0.0407, 0.3297, 0.0024])
N: 400
Signal to noise ratio: tensor([14.1954, 80.9984, 46.7932, 29.0340])
Bound on condition number: tensor([  80604.2385, 2624299.6422,  875841.9457,  337189.3761])
Policy Optimizer learning rate:
0.009597421723058133
Experience 40, Iter 0, disc loss: 0.000892596690391305, policy loss: 7.098716330091872
Experience 40, Iter 1, disc loss: 0.0009333868743006052, policy loss: 7.055739049436742
Experience 40, Iter 2, disc loss: 0.0009566440538087128, policy loss: 7.027596726103205
Experience 40, Iter 3, disc loss: 0.0008622496201193792, policy loss: 7.139086063185445
Experience 40, Iter 4, disc loss: 0.0008489430114554956, policy loss: 7.1554332753315535
Experience 40, Iter 5, disc loss: 0.0008917143959185618, policy loss: 7.119155551168758
Experience 40, Iter 6, disc loss: 0.000910612684152139, policy loss: 7.079519131271875
Experience 40, Iter 7, disc loss: 0.0008406591005721561, policy loss: 7.167404213771557
Experience 40, Iter 8, disc loss: 0.0009738625843671799, policy loss: 7.014480538505243
Experience 40, Iter 9, disc loss: 0.0009150057555601031, policy loss: 7.082356512844952
Experience 40, Iter 10, disc loss: 0.0008505634571720536, policy loss: 7.147202755846218
Experience 40, Iter 11, disc loss: 0.0008415460706451563, policy loss: 7.152485018837302
Experience 40, Iter 12, disc loss: 0.000933468201264097, policy loss: 7.055169951668482
Experience 40, Iter 13, disc loss: 0.0009269236477240553, policy loss: 7.063303166627719
Experience 40, Iter 14, disc loss: 0.0008867517649237566, policy loss: 7.126467605104088
Experience 40, Iter 15, disc loss: 0.0008314262642667878, policy loss: 7.171427048865753
Experience 40, Iter 16, disc loss: 0.0008681890058236273, policy loss: 7.130469577252048
Experience 40, Iter 17, disc loss: 0.0008864127709406382, policy loss: 7.117013059479117
Experience 40, Iter 18, disc loss: 0.0008770106017675031, policy loss: 7.115330501862768
Experience 40, Iter 19, disc loss: 0.0008649045434742119, policy loss: 7.125594141872321
Experience 40, Iter 20, disc loss: 0.0009084069957173484, policy loss: 7.1136459157278376
Experience 40, Iter 21, disc loss: 0.0008707195992767248, policy loss: 7.131349412153277
Experience 40, Iter 22, disc loss: 0.0009220538147203501, policy loss: 7.063894021696818
Experience 40, Iter 23, disc loss: 0.000838338529764606, policy loss: 7.175859312417996
Experience 40, Iter 24, disc loss: 0.0009017081361586335, policy loss: 7.08028286269176
Experience 40, Iter 25, disc loss: 0.0008599124275250243, policy loss: 7.1504568133683275
Experience 40, Iter 26, disc loss: 0.0008827628299803933, policy loss: 7.107776724927739
Experience 40, Iter 27, disc loss: 0.0008558361475876475, policy loss: 7.144501662021952
Experience 40, Iter 28, disc loss: 0.0009004657396720948, policy loss: 7.078569700318081
Experience 40, Iter 29, disc loss: 0.0008352088537112939, policy loss: 7.189280671543594
Experience 40, Iter 30, disc loss: 0.0008550195329669522, policy loss: 7.140820231050942
Experience 40, Iter 31, disc loss: 0.0009088987438055102, policy loss: 7.102890387594599
Experience 40, Iter 32, disc loss: 0.0008425202333032149, policy loss: 7.15989333043753
Experience 40, Iter 33, disc loss: 0.0008387023322149898, policy loss: 7.16945316996392
Experience 40, Iter 34, disc loss: 0.0008232873772739656, policy loss: 7.1879838519380765
Experience 40, Iter 35, disc loss: 0.0008683459363440845, policy loss: 7.132526480634527
Experience 40, Iter 36, disc loss: 0.0009134831982651239, policy loss: 7.090142736761895
Experience 40, Iter 37, disc loss: 0.0008997408247329676, policy loss: 7.092128387396434
Experience 40, Iter 38, disc loss: 0.0008269375190997502, policy loss: 7.185659214502299
Experience 40, Iter 39, disc loss: 0.0008679520370418578, policy loss: 7.1815496626940725
Experience 40, Iter 40, disc loss: 0.0009744624545663393, policy loss: 7.069829074368521
Experience 40, Iter 41, disc loss: 0.0008642811102946797, policy loss: 7.144051182410346
Experience 40, Iter 42, disc loss: 0.000935939714443876, policy loss: 7.071244200103459
Experience 40, Iter 43, disc loss: 0.0008915674984258822, policy loss: 7.113211233310226
Experience 40, Iter 44, disc loss: 0.0008517942647811238, policy loss: 7.167569783754557
Experience 40, Iter 45, disc loss: 0.0009116033900793777, policy loss: 7.096687332821341
Experience 40, Iter 46, disc loss: 0.0010176861194251515, policy loss: 7.014847771147866
Experience 40, Iter 47, disc loss: 0.0009577451698246118, policy loss: 7.0741243927788116
Experience 40, Iter 48, disc loss: 0.000890665857984903, policy loss: 7.1046227669145114
Experience 40, Iter 49, disc loss: 0.0008877113567306384, policy loss: 7.132189842804403
Experience: 41
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0100],
        [0.0814],
        [0.0006]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3697e-03, 3.0788e-02, 2.9100e-02, 1.3969e-03, 7.6104e-05,
          2.7002e-01]],

        [[4.3697e-03, 3.0788e-02, 2.9100e-02, 1.3969e-03, 7.6104e-05,
          2.7002e-01]],

        [[4.3697e-03, 3.0788e-02, 2.9100e-02, 1.3969e-03, 7.6104e-05,
          2.7002e-01]],

        [[4.3697e-03, 3.0788e-02, 2.9100e-02, 1.3969e-03, 7.6104e-05,
          2.7002e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0401, 0.3255, 0.0023], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0401, 0.3255, 0.0023])
N: 410
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1641.0000, 1641.0000, 1641.0000, 1641.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -2.245
Iter 2/2000 - Loss: 0.161
Iter 3/2000 - Loss: -2.179
Iter 4/2000 - Loss: -1.868
Iter 5/2000 - Loss: -1.057
Iter 6/2000 - Loss: -1.360
Iter 7/2000 - Loss: -1.998
Iter 8/2000 - Loss: -2.262
Iter 9/2000 - Loss: -2.106
Iter 10/2000 - Loss: -1.852
Iter 11/2000 - Loss: -1.776
Iter 12/2000 - Loss: -1.903
Iter 13/2000 - Loss: -2.099
Iter 14/2000 - Loss: -2.236
Iter 15/2000 - Loss: -2.275
Iter 16/2000 - Loss: -2.261
Iter 17/2000 - Loss: -2.262
Iter 18/2000 - Loss: -2.318
Iter 19/2000 - Loss: -2.429
Iter 20/2000 - Loss: -2.575
Iter 1981/2000 - Loss: -9.393
Iter 1982/2000 - Loss: -9.393
Iter 1983/2000 - Loss: -9.393
Iter 1984/2000 - Loss: -9.393
Iter 1985/2000 - Loss: -9.393
Iter 1986/2000 - Loss: -9.393
Iter 1987/2000 - Loss: -9.393
Iter 1988/2000 - Loss: -9.393
Iter 1989/2000 - Loss: -9.393
Iter 1990/2000 - Loss: -9.393
Iter 1991/2000 - Loss: -9.393
Iter 1992/2000 - Loss: -9.393
Iter 1993/2000 - Loss: -9.393
Iter 1994/2000 - Loss: -9.393
Iter 1995/2000 - Loss: -9.393
Iter 1996/2000 - Loss: -9.393
Iter 1997/2000 - Loss: -9.393
Iter 1998/2000 - Loss: -9.393
Iter 1999/2000 - Loss: -9.393
Iter 2000/2000 - Loss: -9.393
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0015],
        [0.0002]])
Lengthscale: tensor([[[10.6780,  5.6639, 18.8058, 10.6267,  4.7823, 36.8850]],

        [[10.2655, 17.7171, 14.1529,  3.1732,  5.9070, 25.8923]],

        [[ 8.4194,  7.2635, 11.6139,  0.1790,  1.1820,  7.8928]],

        [[11.2036, 19.6575, 10.4075,  2.7250,  6.1755, 35.3999]]])
Signal Variance: tensor([0.0562, 2.0832, 3.7089, 0.2029])
Estimated target variance: tensor([0.0035, 0.0401, 0.3255, 0.0023])
N: 410
Signal to noise ratio: tensor([14.0410, 81.0970, 49.2628, 29.0143])
Bound on condition number: tensor([  80832.5697, 2696459.2858,  995000.1293,  345151.7430])
Policy Optimizer learning rate:
0.009587315155141827
Experience 41, Iter 0, disc loss: 0.0008866595197481827, policy loss: 7.118115421230045
Experience 41, Iter 1, disc loss: 0.000930075118390259, policy loss: 7.085755065781957
Experience 41, Iter 2, disc loss: 0.0010243407061213097, policy loss: 7.085134670061697
Experience 41, Iter 3, disc loss: 0.0009590129539340702, policy loss: 7.082232948047691
Experience 41, Iter 4, disc loss: 0.0011098870362553723, policy loss: 7.034819143406832
Experience 41, Iter 5, disc loss: 0.000955047179156299, policy loss: 7.306374402780265
Experience 41, Iter 6, disc loss: 0.0013268539806168202, policy loss: 7.0573298316878725
Experience 41, Iter 7, disc loss: 0.0010525934119354962, policy loss: 7.15364182961118
Experience 41, Iter 8, disc loss: 0.001524802342222084, policy loss: 6.984577670619634
Experience 41, Iter 9, disc loss: 0.0011956221892543578, policy loss: 7.167163523025216
Experience 41, Iter 10, disc loss: 0.0022781454793722278, policy loss: 6.860818582944001
Experience 41, Iter 11, disc loss: 0.002089000304818133, policy loss: 7.029598955323147
Experience 41, Iter 12, disc loss: 0.0022807446026284106, policy loss: 6.844302982124046
Experience 41, Iter 13, disc loss: 0.00208364116543322, policy loss: 6.868859842052534
Experience 41, Iter 14, disc loss: 0.005117316708514835, policy loss: 6.5804482324898785
Experience 41, Iter 15, disc loss: 0.0022232299210131617, policy loss: 6.875638673378008
Experience 41, Iter 16, disc loss: 0.003841126340848734, policy loss: 6.620364206350397
Experience 41, Iter 17, disc loss: 0.014928470951465584, policy loss: 7.261560444953691
Experience 41, Iter 18, disc loss: 0.0042966502942372465, policy loss: 6.968559503066857
Experience 41, Iter 19, disc loss: 0.0068757948693954065, policy loss: 6.68462615679739
Experience 41, Iter 20, disc loss: 0.0069327401499840605, policy loss: 6.74489723691001
Experience 41, Iter 21, disc loss: 0.01685297076832574, policy loss: 6.490101767347281
Experience 41, Iter 22, disc loss: 0.007677258565310712, policy loss: 7.099827610487423
Experience 41, Iter 23, disc loss: 0.004214523597665678, policy loss: 7.002847365855789
Experience 41, Iter 24, disc loss: 0.006021806078128985, policy loss: 7.110187948668272
Experience 41, Iter 25, disc loss: 0.0079300320903282, policy loss: 6.988753692924886
Experience 41, Iter 26, disc loss: 0.007542766325008117, policy loss: 7.023891528755136
Experience 41, Iter 27, disc loss: 0.010452648119358393, policy loss: 7.540314434564179
Experience 41, Iter 28, disc loss: 0.009162184324172613, policy loss: 7.343955705503913
Experience 41, Iter 29, disc loss: 0.0071741972204690556, policy loss: 7.2802227527710315
Experience 41, Iter 30, disc loss: 0.0021807082825908456, policy loss: 7.37405756544302
Experience 41, Iter 31, disc loss: 0.0014872616218100203, policy loss: 7.63525529917128
Experience 41, Iter 32, disc loss: 0.0015238466444200752, policy loss: 7.730144359100575
Experience 41, Iter 33, disc loss: 0.00183341496648728, policy loss: 7.641402288149373
Experience 41, Iter 34, disc loss: 0.0019645972605344605, policy loss: 7.749020736582431
Experience 41, Iter 35, disc loss: 0.0020255750628045063, policy loss: 8.0378515985788
Experience 41, Iter 36, disc loss: 0.0021625446156606086, policy loss: 7.923216469392786
Experience 41, Iter 37, disc loss: 0.0023398351047110443, policy loss: 7.728968207954371
Experience 41, Iter 38, disc loss: 0.002380133536180303, policy loss: 7.711326971114518
Experience 41, Iter 39, disc loss: 0.0023683221177000476, policy loss: 7.761429680958944
Experience 41, Iter 40, disc loss: 0.0023991229572710797, policy loss: 7.721044706942531
Experience 41, Iter 41, disc loss: 0.002371223664922953, policy loss: 7.663846216276432
Experience 41, Iter 42, disc loss: 0.002299636018792149, policy loss: 7.675291832945464
Experience 41, Iter 43, disc loss: 0.002240025773794773, policy loss: 7.684385871208834
Experience 41, Iter 44, disc loss: 0.002085984490413497, policy loss: 7.685968439306082
Experience 41, Iter 45, disc loss: 0.0020248882084097533, policy loss: 7.66796387673566
Experience 41, Iter 46, disc loss: 0.0019598481001760005, policy loss: 7.65085120192568
Experience 41, Iter 47, disc loss: 0.0017528826220485178, policy loss: 7.7157018791270655
Experience 41, Iter 48, disc loss: 0.002212511635862867, policy loss: 7.490734327567351
Experience 41, Iter 49, disc loss: 0.0020093134058590447, policy loss: 7.420894959750991
Experience: 42
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0009],
        [0.0144],
        [0.1294],
        [0.0007]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.3318e-03, 3.1264e-02, 4.0307e-02, 1.4086e-03, 7.4818e-05,
          3.3471e-01]],

        [[4.3318e-03, 3.1264e-02, 4.0307e-02, 1.4086e-03, 7.4818e-05,
          3.3471e-01]],

        [[4.3318e-03, 3.1264e-02, 4.0307e-02, 1.4086e-03, 7.4818e-05,
          3.3471e-01]],

        [[4.3318e-03, 3.1264e-02, 4.0307e-02, 1.4086e-03, 7.4818e-05,
          3.3471e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0035, 0.0575, 0.5176, 0.0029], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0035, 0.0575, 0.5176, 0.0029])
N: 420
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1681.0000, 1681.0000, 1681.0000, 1681.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.727
Iter 2/2000 - Loss: 0.309
Iter 3/2000 - Loss: -1.683
Iter 4/2000 - Loss: -1.420
Iter 5/2000 - Loss: -0.730
Iter 6/2000 - Loss: -0.994
Iter 7/2000 - Loss: -1.538
Iter 8/2000 - Loss: -1.753
Iter 9/2000 - Loss: -1.605
Iter 10/2000 - Loss: -1.387
Iter 11/2000 - Loss: -1.337
Iter 12/2000 - Loss: -1.464
Iter 13/2000 - Loss: -1.642
Iter 14/2000 - Loss: -1.757
Iter 15/2000 - Loss: -1.783
Iter 16/2000 - Loss: -1.770
Iter 17/2000 - Loss: -1.784
Iter 18/2000 - Loss: -1.857
Iter 19/2000 - Loss: -1.983
Iter 20/2000 - Loss: -2.138
Iter 1981/2000 - Loss: -9.369
Iter 1982/2000 - Loss: -9.369
Iter 1983/2000 - Loss: -9.369
Iter 1984/2000 - Loss: -9.369
Iter 1985/2000 - Loss: -9.369
Iter 1986/2000 - Loss: -9.369
Iter 1987/2000 - Loss: -9.369
Iter 1988/2000 - Loss: -9.369
Iter 1989/2000 - Loss: -9.369
Iter 1990/2000 - Loss: -9.369
Iter 1991/2000 - Loss: -9.369
Iter 1992/2000 - Loss: -9.369
Iter 1993/2000 - Loss: -9.369
Iter 1994/2000 - Loss: -9.369
Iter 1995/2000 - Loss: -9.369
Iter 1996/2000 - Loss: -9.369
Iter 1997/2000 - Loss: -9.369
Iter 1998/2000 - Loss: -9.369
Iter 1999/2000 - Loss: -9.369
Iter 2000/2000 - Loss: -9.369
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0016],
        [0.0002]])
Lengthscale: tensor([[[10.6102,  5.7552, 21.0073, 10.7603,  4.7088, 39.3089]],

        [[ 9.6394, 16.8230, 18.3737,  3.5708,  1.1469, 17.6363]],

        [[ 9.6510,  7.9722, 14.8391,  0.1819,  1.2918, 10.0597]],

        [[10.9770, 19.4121, 10.4451,  2.5005,  5.9922, 27.2691]]])
Signal Variance: tensor([0.0562, 2.0056, 5.1046, 0.1649])
Estimated target variance: tensor([0.0035, 0.0575, 0.5176, 0.0029])
N: 420
Signal to noise ratio: tensor([14.0451, 80.4960, 57.1328, 26.2198])
Bound on condition number: tensor([  82851.9762, 2721437.9611, 1370947.7006,  288741.8425])
Policy Optimizer learning rate:
0.009577219229949994
Experience 42, Iter 0, disc loss: 0.0013958538184298875, policy loss: 7.827047913030871
Experience 42, Iter 1, disc loss: 0.0013354225203351727, policy loss: 7.765498529613974
Experience 42, Iter 2, disc loss: 0.0012775843608439116, policy loss: 7.6937769870814785
Experience 42, Iter 3, disc loss: 0.0012372898802536366, policy loss: 7.685500991683723
Experience 42, Iter 4, disc loss: 0.0013356523190753858, policy loss: 7.62484375861713
Experience 42, Iter 5, disc loss: 0.0012777294564871125, policy loss: 7.523179830232026
Experience 42, Iter 6, disc loss: 0.0012689850960152023, policy loss: 7.751260725796081
Experience 42, Iter 7, disc loss: 0.0013503851320648805, policy loss: 7.44603833473964
Experience 42, Iter 8, disc loss: 0.001202104408291174, policy loss: 7.565106686489845
Experience 42, Iter 9, disc loss: 0.0011836306243003226, policy loss: 7.714352860327882
Experience 42, Iter 10, disc loss: 0.0012363519715708569, policy loss: 7.492119286074119
Experience 42, Iter 11, disc loss: 0.00103039415124389, policy loss: 7.691428409857698
Experience 42, Iter 12, disc loss: 0.0010819620104036017, policy loss: 7.469702758469191
Experience 42, Iter 13, disc loss: 0.0011124724973610996, policy loss: 7.432864926994262
Experience 42, Iter 14, disc loss: 0.0011443283609194387, policy loss: 7.477189508771584
Experience 42, Iter 15, disc loss: 0.0012900122660567436, policy loss: 7.350404621536002
Experience 42, Iter 16, disc loss: 0.0012662357729281784, policy loss: 7.495673742254059
Experience 42, Iter 17, disc loss: 0.0013843903168253386, policy loss: 7.50829590418416
Experience 42, Iter 18, disc loss: 0.0013878100785560286, policy loss: 7.345436743994344
Experience 42, Iter 19, disc loss: 0.001217450930314889, policy loss: 7.516328125296175
Experience 42, Iter 20, disc loss: 0.0014753151977580032, policy loss: 7.320254344694019
Experience 42, Iter 21, disc loss: 0.00094822574626019, policy loss: 7.752063053694453
Experience 42, Iter 22, disc loss: 0.001238442476624511, policy loss: 7.455067083171411
Experience 42, Iter 23, disc loss: 0.001382266821853955, policy loss: 7.240299077903465
Experience 42, Iter 24, disc loss: 0.0015183258860715137, policy loss: 7.123815566650354
Experience 42, Iter 25, disc loss: 0.0018099189589122137, policy loss: 7.089389785155869
Experience 42, Iter 26, disc loss: 0.0016314358224625222, policy loss: 7.3668873939587645
Experience 42, Iter 27, disc loss: 0.002542375631270287, policy loss: 7.090331196817294
Experience 42, Iter 28, disc loss: 0.0024593448728288682, policy loss: 7.163877633072406
Experience 42, Iter 29, disc loss: 0.0026997064602017062, policy loss: 7.206612231627129
Experience 42, Iter 30, disc loss: 0.003949375045340383, policy loss: 7.416377482950738
Experience 42, Iter 31, disc loss: 0.002977426681412339, policy loss: 6.8370072868403575
Experience 42, Iter 32, disc loss: 0.0016567715166810027, policy loss: 7.46263138710938
Experience 42, Iter 33, disc loss: 0.0011040600423187711, policy loss: 7.466600583166673
Experience 42, Iter 34, disc loss: 0.0011830260259615092, policy loss: 7.376770901801764
Experience 42, Iter 35, disc loss: 0.0015027633541130486, policy loss: 7.217112940569769
Experience 42, Iter 36, disc loss: 0.0019545807948391503, policy loss: 7.245670103930653
Experience 42, Iter 37, disc loss: 0.0024637161233472806, policy loss: 7.975098038782968
Experience 42, Iter 38, disc loss: 0.004353404460356627, policy loss: 7.9892446049194294
Experience 42, Iter 39, disc loss: 0.005634001785416779, policy loss: 7.044729698166389
Experience 42, Iter 40, disc loss: 0.0025343304292366254, policy loss: 7.698549729735996
Experience 42, Iter 41, disc loss: 0.0052916185741127214, policy loss: 7.020600560074006
Experience 42, Iter 42, disc loss: 0.0034706714376504916, policy loss: 7.139442591753984
Experience 42, Iter 43, disc loss: 0.0023933408772026567, policy loss: 7.456013163303933
Experience 42, Iter 44, disc loss: 0.0017432581802394706, policy loss: 7.464798828256713
Experience 42, Iter 45, disc loss: 0.001500454481476825, policy loss: 7.893245958581597
Experience 42, Iter 46, disc loss: 0.001769417398606615, policy loss: 7.523913266809741
Experience 42, Iter 47, disc loss: 0.0025144877384152163, policy loss: 7.137666534168273
Experience 42, Iter 48, disc loss: 0.0026273627611205145, policy loss: 7.398617168487262
Experience 42, Iter 49, disc loss: 0.0035541404591123947, policy loss: 7.383801649144148
Experience: 43
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0196],
        [0.1879],
        [0.0013]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.6584e-03, 3.7297e-02, 6.9973e-02, 1.9205e-03, 2.6251e-04,
          4.6205e-01]],

        [[4.6584e-03, 3.7297e-02, 6.9973e-02, 1.9205e-03, 2.6251e-04,
          4.6205e-01]],

        [[4.6584e-03, 3.7297e-02, 6.9973e-02, 1.9205e-03, 2.6251e-04,
          4.6205e-01]],

        [[4.6584e-03, 3.7297e-02, 6.9973e-02, 1.9205e-03, 2.6251e-04,
          4.6205e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.0784, 0.7518, 0.0053], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.0784, 0.7518, 0.0053])
N: 430
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1721.0000, 1721.0000, 1721.0000, 1721.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -1.003
Iter 2/2000 - Loss: 0.188
Iter 3/2000 - Loss: -1.012
Iter 4/2000 - Loss: -0.873
Iter 5/2000 - Loss: -0.458
Iter 6/2000 - Loss: -0.633
Iter 7/2000 - Loss: -0.971
Iter 8/2000 - Loss: -1.089
Iter 9/2000 - Loss: -0.988
Iter 10/2000 - Loss: -0.877
Iter 11/2000 - Loss: -0.904
Iter 12/2000 - Loss: -1.044
Iter 13/2000 - Loss: -1.197
Iter 14/2000 - Loss: -1.299
Iter 15/2000 - Loss: -1.353
Iter 16/2000 - Loss: -1.409
Iter 17/2000 - Loss: -1.514
Iter 18/2000 - Loss: -1.679
Iter 19/2000 - Loss: -1.889
Iter 20/2000 - Loss: -2.119
Iter 1981/2000 - Loss: -9.332
Iter 1982/2000 - Loss: -9.332
Iter 1983/2000 - Loss: -9.332
Iter 1984/2000 - Loss: -9.332
Iter 1985/2000 - Loss: -9.332
Iter 1986/2000 - Loss: -9.332
Iter 1987/2000 - Loss: -9.332
Iter 1988/2000 - Loss: -9.332
Iter 1989/2000 - Loss: -9.332
Iter 1990/2000 - Loss: -9.332
Iter 1991/2000 - Loss: -9.332
Iter 1992/2000 - Loss: -9.332
Iter 1993/2000 - Loss: -9.332
Iter 1994/2000 - Loss: -9.332
Iter 1995/2000 - Loss: -9.332
Iter 1996/2000 - Loss: -9.332
Iter 1997/2000 - Loss: -9.332
Iter 1998/2000 - Loss: -9.332
Iter 1999/2000 - Loss: -9.332
Iter 2000/2000 - Loss: -9.332
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.8444,  5.8690, 24.1750,  9.1787,  9.1163, 38.5233]],

        [[ 9.5118, 17.2640, 12.9775,  2.4163,  1.1178, 12.9102]],

        [[10.2754, 19.3298, 13.4103,  0.9599,  1.6862, 14.8298]],

        [[11.0599, 20.1981, 17.2736,  4.0354,  3.6736, 30.3012]]])
Signal Variance: tensor([ 0.0522,  1.0456, 10.7985,  0.3986])
Estimated target variance: tensor([0.0041, 0.0784, 0.7518, 0.0053])
N: 430
Signal to noise ratio: tensor([13.5822, 57.7365, 75.1647, 40.7685])
Bound on condition number: tensor([  79325.8673, 1433406.1975, 2429387.0764,  714691.3672])
Policy Optimizer learning rate:
0.00956713393627531
Experience 43, Iter 0, disc loss: 0.0035852020070991364, policy loss: 7.759091366495385
Experience 43, Iter 1, disc loss: 0.004638877170350359, policy loss: 7.855293035658373
Experience 43, Iter 2, disc loss: 0.005295733464462077, policy loss: 7.680175914072699
Experience 43, Iter 3, disc loss: 0.00374779193058006, policy loss: 8.597605542041666
Experience 43, Iter 4, disc loss: 0.0017032062648094644, policy loss: 8.873689206293264
Experience 43, Iter 5, disc loss: 0.0013181711670916722, policy loss: 8.674287757381439
Experience 43, Iter 6, disc loss: 0.0013831570140599915, policy loss: 8.337570680055821
Experience 43, Iter 7, disc loss: 0.0013523860003561543, policy loss: 8.622523613383382
Experience 43, Iter 8, disc loss: 0.0012863966596564232, policy loss: 8.99649229092866
Experience 43, Iter 9, disc loss: 0.0013298994835825475, policy loss: 8.850158464873443
Experience 43, Iter 10, disc loss: 0.0012651247558654669, policy loss: 8.859444938032272
Experience 43, Iter 11, disc loss: 0.0012796512898079198, policy loss: 8.61649501720795
Experience 43, Iter 12, disc loss: 0.0012524936553273535, policy loss: 8.575682217648193
Experience 43, Iter 13, disc loss: 0.0012415728228682725, policy loss: 8.346696799258043
Experience 43, Iter 14, disc loss: 0.0011439535141466919, policy loss: 8.537783877825323
Experience 43, Iter 15, disc loss: 0.0010747306000418767, policy loss: 8.6527622203561
Experience 43, Iter 16, disc loss: 0.001056914306731941, policy loss: 8.479142445000704
Experience 43, Iter 17, disc loss: 0.0010319073795120779, policy loss: 8.445337072100056
Experience 43, Iter 18, disc loss: 0.0009947103490880614, policy loss: 8.392305849302437
Experience 43, Iter 19, disc loss: 0.0009780079609949819, policy loss: 8.324447845032594
Experience 43, Iter 20, disc loss: 0.0008980780384511947, policy loss: 8.362950649782874
Experience 43, Iter 21, disc loss: 0.0008803438783085679, policy loss: 8.300826811625232
Experience 43, Iter 22, disc loss: 0.0008711198367487888, policy loss: 8.257429026819818
Experience 43, Iter 23, disc loss: 0.0007855196494712647, policy loss: 8.425706972087006
Experience 43, Iter 24, disc loss: 0.0008110953011660606, policy loss: 8.156026754281758
Experience 43, Iter 25, disc loss: 0.0008057647633797601, policy loss: 8.114074273583668
Experience 43, Iter 26, disc loss: 0.0007538160811731973, policy loss: 8.17418752449795
Experience 43, Iter 27, disc loss: 0.0007131059202474604, policy loss: 8.18145374469251
Experience 43, Iter 28, disc loss: 0.0006890016914935602, policy loss: 8.217805240493176
Experience 43, Iter 29, disc loss: 0.0007204242029756353, policy loss: 8.039542539222696
Experience 43, Iter 30, disc loss: 0.0007114250859129674, policy loss: 8.006754528462558
Experience 43, Iter 31, disc loss: 0.0007310624135272564, policy loss: 7.896110473414842
Experience 43, Iter 32, disc loss: 0.0007264092669235151, policy loss: 7.890396075938586
Experience 43, Iter 33, disc loss: 0.0007710433536692058, policy loss: 7.731528195530991
Experience 43, Iter 34, disc loss: 0.00075594943394222, policy loss: 7.723199305095006
Experience 43, Iter 35, disc loss: 0.000782834625540211, policy loss: 7.73117742485722
Experience 43, Iter 36, disc loss: 0.0007737463789647282, policy loss: 7.6597935547571945
Experience 43, Iter 37, disc loss: 0.0007467170978126339, policy loss: 7.6790412380737925
Experience 43, Iter 38, disc loss: 0.0007866119738278749, policy loss: 7.700722655751861
Experience 43, Iter 39, disc loss: 0.0009418379048945156, policy loss: 7.492293688179921
Experience 43, Iter 40, disc loss: 0.0010025635938937334, policy loss: 7.5245276318159835
Experience 43, Iter 41, disc loss: 0.0009612414824022887, policy loss: 7.550849078003001
Experience 43, Iter 42, disc loss: 0.0017792884628106314, policy loss: 7.231948545247301
Experience 43, Iter 43, disc loss: 0.0018780397545126496, policy loss: 7.100079824878581
Experience 43, Iter 44, disc loss: 0.0018640395688874118, policy loss: 7.403286848352236
Experience 43, Iter 45, disc loss: 0.0021583085564714213, policy loss: 7.801992521534825
Experience 43, Iter 46, disc loss: 0.0030514077870580835, policy loss: 6.639044143820432
Experience 43, Iter 47, disc loss: 0.0037359084398584496, policy loss: 6.83850352631886
Experience 43, Iter 48, disc loss: 0.0033415513661130686, policy loss: 6.581301300800467
Experience 43, Iter 49, disc loss: 0.003034916536628275, policy loss: 6.438141248432499
Experience: 44
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0010],
        [0.0265],
        [0.2610],
        [0.0015]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[4.5975e-03, 3.8219e-02, 8.6959e-02, 1.9679e-03, 2.5826e-04,
          5.6845e-01]],

        [[4.5975e-03, 3.8219e-02, 8.6959e-02, 1.9679e-03, 2.5826e-04,
          5.6845e-01]],

        [[4.5975e-03, 3.8219e-02, 8.6959e-02, 1.9679e-03, 2.5826e-04,
          5.6845e-01]],

        [[4.5975e-03, 3.8219e-02, 8.6959e-02, 1.9679e-03, 2.5826e-04,
          5.6845e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0041, 0.1059, 1.0442, 0.0062], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0041, 0.1059, 1.0442, 0.0062])
N: 440
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1761.0000, 1761.0000, 1761.0000, 1761.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.611
Iter 2/2000 - Loss: 0.470
Iter 3/2000 - Loss: -0.625
Iter 4/2000 - Loss: -0.501
Iter 5/2000 - Loss: -0.124
Iter 6/2000 - Loss: -0.286
Iter 7/2000 - Loss: -0.595
Iter 8/2000 - Loss: -0.702
Iter 9/2000 - Loss: -0.609
Iter 10/2000 - Loss: -0.513
Iter 11/2000 - Loss: -0.547
Iter 12/2000 - Loss: -0.687
Iter 13/2000 - Loss: -0.836
Iter 14/2000 - Loss: -0.934
Iter 15/2000 - Loss: -0.988
Iter 16/2000 - Loss: -1.049
Iter 17/2000 - Loss: -1.161
Iter 18/2000 - Loss: -1.333
Iter 19/2000 - Loss: -1.545
Iter 20/2000 - Loss: -1.774
Iter 1981/2000 - Loss: -9.307
Iter 1982/2000 - Loss: -9.307
Iter 1983/2000 - Loss: -9.307
Iter 1984/2000 - Loss: -9.307
Iter 1985/2000 - Loss: -9.307
Iter 1986/2000 - Loss: -9.307
Iter 1987/2000 - Loss: -9.307
Iter 1988/2000 - Loss: -9.307
Iter 1989/2000 - Loss: -9.307
Iter 1990/2000 - Loss: -9.307
Iter 1991/2000 - Loss: -9.307
Iter 1992/2000 - Loss: -9.307
Iter 1993/2000 - Loss: -9.307
Iter 1994/2000 - Loss: -9.308
Iter 1995/2000 - Loss: -9.308
Iter 1996/2000 - Loss: -9.308
Iter 1997/2000 - Loss: -9.308
Iter 1998/2000 - Loss: -9.308
Iter 1999/2000 - Loss: -9.308
Iter 2000/2000 - Loss: -9.308
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.9197,  6.0906, 23.5636,  8.9569,  8.5948, 38.9529]],

        [[11.3129, 18.3017, 12.7057,  1.9462,  1.1610, 19.0144]],

        [[11.2622, 19.7482, 14.5955,  0.9324,  2.0301, 18.5523]],

        [[11.0374, 20.3247, 18.2049,  4.3700,  2.9810, 41.9668]]])
Signal Variance: tensor([ 0.0586,  2.0907, 15.3532,  0.4740])
Estimated target variance: tensor([0.0041, 0.1059, 1.0442, 0.0062])
N: 440
Signal to noise ratio: tensor([14.4095, 81.4556, 90.0225, 44.4204])
Bound on condition number: tensor([  91359.7795, 2919409.6076, 3565781.6134,  868196.3850])
Policy Optimizer learning rate:
0.00955705926292225
Experience 44, Iter 0, disc loss: 0.004508234452734195, policy loss: 6.307937971151994
Experience 44, Iter 1, disc loss: 0.01320169909406085, policy loss: 5.860654649395197
Experience 44, Iter 2, disc loss: 0.023917203479447254, policy loss: 5.341012833186143
Experience 44, Iter 3, disc loss: 0.015389833911361912, policy loss: 6.961592086753289
Experience 44, Iter 4, disc loss: 0.015782202121064577, policy loss: 5.917809910185189
Experience 44, Iter 5, disc loss: 0.015114370092404447, policy loss: 6.142631091739062
Experience 44, Iter 6, disc loss: 0.010329894928286978, policy loss: 7.137416012264223
Experience 44, Iter 7, disc loss: 0.011133414425807077, policy loss: 6.973584983269023
Experience 44, Iter 8, disc loss: 0.009855177804976849, policy loss: 6.453183808270215
Experience 44, Iter 9, disc loss: 0.012792158874505404, policy loss: 6.476210369594359
Experience 44, Iter 10, disc loss: 0.009061667311079078, policy loss: 6.764486663217406
Experience 44, Iter 11, disc loss: 0.011076440228665722, policy loss: 7.2278739408103005
Experience 44, Iter 12, disc loss: 0.009706198396702278, policy loss: 7.61793836831035
Experience 44, Iter 13, disc loss: 0.011490855696289218, policy loss: 7.3903050361114815
Experience 44, Iter 14, disc loss: 0.010474418957680469, policy loss: 7.598500129124889
Experience 44, Iter 15, disc loss: 0.012571512857430316, policy loss: 6.991919235866284
Experience 44, Iter 16, disc loss: 0.011492133394680748, policy loss: 6.981087866192865
Experience 44, Iter 17, disc loss: 0.00952569263564357, policy loss: 7.158355248563123
Experience 44, Iter 18, disc loss: 0.007173934812706544, policy loss: 7.6949120556127335
Experience 44, Iter 19, disc loss: 0.007668428317357322, policy loss: 7.2466691605624085
Experience 44, Iter 20, disc loss: 0.006502402218999794, policy loss: 7.099716338296913
Experience 44, Iter 21, disc loss: 0.006713566710817, policy loss: 6.842543786722758
Experience 44, Iter 22, disc loss: 0.005662057138962431, policy loss: 6.828381715111159
Experience 44, Iter 23, disc loss: 0.0054176355740783964, policy loss: 7.278608978635581
Experience 44, Iter 24, disc loss: 0.007192131861336508, policy loss: 6.968572522936855
Experience 44, Iter 25, disc loss: 0.0070190300309141175, policy loss: 6.806638205796093
Experience 44, Iter 26, disc loss: 0.007713225909177937, policy loss: 6.933387364083217
Experience 44, Iter 27, disc loss: 0.009325011193489992, policy loss: 6.317578415771019
Experience 44, Iter 28, disc loss: 0.011249178579513583, policy loss: 6.535490149676441
Experience 44, Iter 29, disc loss: 0.009860088883531853, policy loss: 6.828302409507925
Experience 44, Iter 30, disc loss: 0.008833601373676813, policy loss: 7.037287864137057
Experience 44, Iter 31, disc loss: 0.005746137535142176, policy loss: 7.304487731107783
Experience 44, Iter 32, disc loss: 0.006215099020127698, policy loss: 7.274469321503252
Experience 44, Iter 33, disc loss: 0.005057228984284359, policy loss: 7.089404025314462
Experience 44, Iter 34, disc loss: 0.0029695986910769166, policy loss: 8.483976896600957
Experience 44, Iter 35, disc loss: 0.0019883218163934393, policy loss: 8.974790642565281
Experience 44, Iter 36, disc loss: 0.0019689813137669008, policy loss: 10.622962639357453
Experience 44, Iter 37, disc loss: 0.0017367321673979396, policy loss: 10.734354314122562
Experience 44, Iter 38, disc loss: 0.0018066994334087847, policy loss: 9.882858782397872
Experience 44, Iter 39, disc loss: 0.00171165276908882, policy loss: 9.98082417825025
Experience 44, Iter 40, disc loss: 0.0016159893330110746, policy loss: 10.897865406531974
Experience 44, Iter 41, disc loss: 0.0015437430050236874, policy loss: 11.236256916954968
Experience 44, Iter 42, disc loss: 0.001525445742156635, policy loss: 10.630430095967416
Experience 44, Iter 43, disc loss: 0.0017364921932260758, policy loss: 9.429638743914023
Experience 44, Iter 44, disc loss: 0.0026455894384007153, policy loss: 8.28125499493068
Experience 44, Iter 45, disc loss: 0.014932450674950703, policy loss: 6.217799833793301
Experience 44, Iter 46, disc loss: 0.6699471952048205, policy loss: 2.5970130820363835
Experience 44, Iter 47, disc loss: 2.2262286823807957, policy loss: 1.0711375811258252
Experience 44, Iter 48, disc loss: 2.6576589542816413, policy loss: 1.7555267763428064
Experience 44, Iter 49, disc loss: 2.4493220319073763, policy loss: 1.3359339878769543
Experience: 45
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0280],
        [0.2804],
        [0.0021]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[5.5885e-03, 4.1032e-02, 1.1063e-01, 2.7777e-03, 4.6179e-04,
          6.3607e-01]],

        [[5.5885e-03, 4.1032e-02, 1.1063e-01, 2.7777e-03, 4.6179e-04,
          6.3607e-01]],

        [[5.5885e-03, 4.1032e-02, 1.1063e-01, 2.7777e-03, 4.6179e-04,
          6.3607e-01]],

        [[5.5885e-03, 4.1032e-02, 1.1063e-01, 2.7777e-03, 4.6179e-04,
          6.3607e-01]]], grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0043, 0.1120, 1.1217, 0.0083], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0043, 0.1120, 1.1217, 0.0083])
N: 450
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1801.0000, 1801.0000, 1801.0000, 1801.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.392
Iter 2/2000 - Loss: 0.502
Iter 3/2000 - Loss: -0.417
Iter 4/2000 - Loss: -0.316
Iter 5/2000 - Loss: -0.012
Iter 6/2000 - Loss: -0.161
Iter 7/2000 - Loss: -0.430
Iter 8/2000 - Loss: -0.531
Iter 9/2000 - Loss: -0.472
Iter 10/2000 - Loss: -0.419
Iter 11/2000 - Loss: -0.482
Iter 12/2000 - Loss: -0.640
Iter 13/2000 - Loss: -0.812
Iter 14/2000 - Loss: -0.947
Iter 15/2000 - Loss: -1.050
Iter 16/2000 - Loss: -1.161
Iter 17/2000 - Loss: -1.320
Iter 18/2000 - Loss: -1.537
Iter 19/2000 - Loss: -1.795
Iter 20/2000 - Loss: -2.072
Iter 1981/2000 - Loss: -9.230
Iter 1982/2000 - Loss: -9.230
Iter 1983/2000 - Loss: -9.230
Iter 1984/2000 - Loss: -9.230
Iter 1985/2000 - Loss: -9.230
Iter 1986/2000 - Loss: -9.230
Iter 1987/2000 - Loss: -9.230
Iter 1988/2000 - Loss: -9.230
Iter 1989/2000 - Loss: -9.230
Iter 1990/2000 - Loss: -9.230
Iter 1991/2000 - Loss: -9.230
Iter 1992/2000 - Loss: -9.230
Iter 1993/2000 - Loss: -9.230
Iter 1994/2000 - Loss: -9.230
Iter 1995/2000 - Loss: -9.230
Iter 1996/2000 - Loss: -9.230
Iter 1997/2000 - Loss: -9.230
Iter 1998/2000 - Loss: -9.230
Iter 1999/2000 - Loss: -9.230
Iter 2000/2000 - Loss: -9.230
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.5300,  6.8261, 24.7169,  9.5498,  9.5407, 35.9255]],

        [[12.2788, 18.4521,  9.3889,  1.8673,  1.3740, 16.9613]],

        [[12.5549, 20.5349, 11.8782,  1.0963,  2.1410, 19.7476]],

        [[11.8986, 20.5919, 16.5285,  3.6041,  1.2787, 41.9665]]])
Signal Variance: tensor([ 0.0706,  1.5364, 16.2191,  0.3460])
Estimated target variance: tensor([0.0043, 0.1120, 1.1217, 0.0083])
N: 450
Signal to noise ratio: tensor([15.8897, 69.7825, 92.2929, 37.6986])
Bound on condition number: tensor([ 113617.4532, 2191320.4500, 3833091.0090,  639534.1812])
Policy Optimizer learning rate:
0.009546995198707083
Experience 45, Iter 0, disc loss: 2.5996117330575674, policy loss: 0.5333310901726449
Experience 45, Iter 1, disc loss: 1.9369262164494279, policy loss: 1.1669748528392248
Experience 45, Iter 2, disc loss: 1.9116192608611706, policy loss: 2.053009082907506
Experience 45, Iter 3, disc loss: 2.184100627518984, policy loss: 2.664055650094082
Experience 45, Iter 4, disc loss: 2.1705448446947795, policy loss: 3.345195428275134
Experience 45, Iter 5, disc loss: 1.8534566635745586, policy loss: 3.4787181423311377
Experience 45, Iter 6, disc loss: 1.323824780770873, policy loss: 3.664286656274366
Experience 45, Iter 7, disc loss: 0.8013778866543155, policy loss: 2.9519850458814227
Experience 45, Iter 8, disc loss: 0.4531165958478139, policy loss: 2.6581074837882133
Experience 45, Iter 9, disc loss: 0.3031023638321833, policy loss: 2.0531619843666062
Experience 45, Iter 10, disc loss: 0.24472504106520165, policy loss: 2.2243168915338347
Experience 45, Iter 11, disc loss: 0.3635352605133963, policy loss: 1.6756234319895555
Experience 45, Iter 12, disc loss: 0.31486346672131466, policy loss: 1.7176295767089613
Experience 45, Iter 13, disc loss: 0.300494801575794, policy loss: 1.8760869165865266
Experience 45, Iter 14, disc loss: 0.33006586046779407, policy loss: 1.7961878983978106
Experience 45, Iter 15, disc loss: 0.2339070119671569, policy loss: 2.192171525897658
Experience 45, Iter 16, disc loss: 0.20549018205447295, policy loss: 2.340611523771223
Experience 45, Iter 17, disc loss: 0.1487526641789928, policy loss: 2.64952340132422
Experience 45, Iter 18, disc loss: 0.10738406271320472, policy loss: 2.9971031726865816
Experience 45, Iter 19, disc loss: 0.07490797195415344, policy loss: 3.3893150762322133
Experience 45, Iter 20, disc loss: 0.05748082668915779, policy loss: 3.7360874454029913
Experience 45, Iter 21, disc loss: 0.04834901362912726, policy loss: 3.93769003400107
Experience 45, Iter 22, disc loss: 0.03935323877511139, policy loss: 4.180782045621429
Experience 45, Iter 23, disc loss: 0.036603394996269065, policy loss: 4.361328140691637
Experience 45, Iter 24, disc loss: 0.03337682043793963, policy loss: 4.612500986713368
Experience 45, Iter 25, disc loss: 0.032278427191160836, policy loss: 4.566335374235299
Experience 45, Iter 26, disc loss: 0.033214864552535674, policy loss: 4.685059509230741
Experience 45, Iter 27, disc loss: 0.033619117465039705, policy loss: 4.620108210418822
Experience 45, Iter 28, disc loss: 0.03477964729036412, policy loss: 4.796838547487397
Experience 45, Iter 29, disc loss: 0.03580879730036383, policy loss: 5.024779275592011
Experience 45, Iter 30, disc loss: 0.03775868507360089, policy loss: 4.883916117204247
Experience 45, Iter 31, disc loss: 0.036876353895473654, policy loss: 5.117209327427133
Experience 45, Iter 32, disc loss: 0.03827908069332567, policy loss: 5.302350647604661
Experience 45, Iter 33, disc loss: 0.0380289261417316, policy loss: 5.602403805498549
Experience 45, Iter 34, disc loss: 0.041076929452862954, policy loss: 4.885535086583934
Experience 45, Iter 35, disc loss: 0.040488201417703894, policy loss: 5.263093480563725
Experience 45, Iter 36, disc loss: 0.04176224858238836, policy loss: 4.536964824742939
Experience 45, Iter 37, disc loss: 0.04016066064164597, policy loss: 4.88608244849076
Experience 45, Iter 38, disc loss: 0.039821064835537955, policy loss: 5.001141719041305
Experience 45, Iter 39, disc loss: 0.038371135257565636, policy loss: 5.045065107136124
Experience 45, Iter 40, disc loss: 0.03724989561853276, policy loss: 4.857398690135005
Experience 45, Iter 41, disc loss: 0.03812165444136409, policy loss: 4.942484819027428
Experience 45, Iter 42, disc loss: 0.03965380503097006, policy loss: 4.7937555890998595
Experience 45, Iter 43, disc loss: 0.04362905493365184, policy loss: 4.474952057610929
Experience 45, Iter 44, disc loss: 0.0480780790875676, policy loss: 4.149462943486316
Experience 45, Iter 45, disc loss: 0.051352952421118564, policy loss: 4.246161083346636
Experience 45, Iter 46, disc loss: 0.05427595543844001, policy loss: 3.961238554623147
Experience 45, Iter 47, disc loss: 0.05813210412614975, policy loss: 4.278968660648732
Experience 45, Iter 48, disc loss: 0.06492843745872227, policy loss: 3.857655755190619
Experience 45, Iter 49, disc loss: 0.06808966330965988, policy loss: 3.751050789364401
Experience: 46
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0011],
        [0.0294],
        [0.3025],
        [0.0024]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0057, 0.0431, 0.1248, 0.0034, 0.0007, 0.6821]],

        [[0.0057, 0.0431, 0.1248, 0.0034, 0.0007, 0.6821]],

        [[0.0057, 0.0431, 0.1248, 0.0034, 0.0007, 0.6821]],

        [[0.0057, 0.0431, 0.1248, 0.0034, 0.0007, 0.6821]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0045, 0.1175, 1.2099, 0.0096], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0045, 0.1175, 1.2099, 0.0096])
N: 460
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1841.0000, 1841.0000, 1841.0000, 1841.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: -0.238
Iter 2/2000 - Loss: 0.538
Iter 3/2000 - Loss: -0.275
Iter 4/2000 - Loss: -0.196
Iter 5/2000 - Loss: 0.067
Iter 6/2000 - Loss: -0.072
Iter 7/2000 - Loss: -0.318
Iter 8/2000 - Loss: -0.416
Iter 9/2000 - Loss: -0.377
Iter 10/2000 - Loss: -0.351
Iter 11/2000 - Loss: -0.433
Iter 12/2000 - Loss: -0.601
Iter 13/2000 - Loss: -0.780
Iter 14/2000 - Loss: -0.926
Iter 15/2000 - Loss: -1.049
Iter 16/2000 - Loss: -1.187
Iter 17/2000 - Loss: -1.372
Iter 18/2000 - Loss: -1.610
Iter 19/2000 - Loss: -1.884
Iter 20/2000 - Loss: -2.172
Iter 1981/2000 - Loss: -9.205
Iter 1982/2000 - Loss: -9.205
Iter 1983/2000 - Loss: -9.205
Iter 1984/2000 - Loss: -9.205
Iter 1985/2000 - Loss: -9.205
Iter 1986/2000 - Loss: -9.205
Iter 1987/2000 - Loss: -9.205
Iter 1988/2000 - Loss: -9.205
Iter 1989/2000 - Loss: -9.205
Iter 1990/2000 - Loss: -9.205
Iter 1991/2000 - Loss: -9.205
Iter 1992/2000 - Loss: -9.205
Iter 1993/2000 - Loss: -9.205
Iter 1994/2000 - Loss: -9.205
Iter 1995/2000 - Loss: -9.205
Iter 1996/2000 - Loss: -9.206
Iter 1997/2000 - Loss: -9.206
Iter 1998/2000 - Loss: -9.206
Iter 1999/2000 - Loss: -9.206
Iter 2000/2000 - Loss: -9.206
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0003]])
Lengthscale: tensor([[[10.3002,  7.2192, 26.6352,  9.6014, 10.9120, 38.4529]],

        [[12.1689, 18.9719,  9.5939,  1.7025,  1.3420, 15.1802]],

        [[13.1595, 21.1586, 12.6816,  1.0038,  1.5586, 16.7884]],

        [[12.0713, 20.2791, 15.2961,  3.3850,  1.5054, 40.1445]]])
Signal Variance: tensor([ 0.0756,  1.2304, 12.3414,  0.3603])
Estimated target variance: tensor([0.0045, 0.1175, 1.2099, 0.0096])
N: 460
Signal to noise ratio: tensor([16.5235, 62.6407, 80.9040, 37.8928])
Bound on condition number: tensor([ 125593.1111, 1804972.8002, 3010913.0240,  660498.1809])
Policy Optimizer learning rate:
0.009536941732457853
Experience 46, Iter 0, disc loss: 0.07736500372760491, policy loss: 3.5465301579281046
Experience 46, Iter 1, disc loss: 0.06337308434149587, policy loss: 4.409011072350475
Experience 46, Iter 2, disc loss: 0.07539121634716431, policy loss: 3.7072699746089794
Experience 46, Iter 3, disc loss: 0.08147060015083268, policy loss: 4.0290844773415
Experience 46, Iter 4, disc loss: 0.08041344609447806, policy loss: 3.8165076172213652
Experience 46, Iter 5, disc loss: 0.08252071579152882, policy loss: 4.099525230751326
Experience 46, Iter 6, disc loss: 0.0820040409719429, policy loss: 4.016706474336508
Experience 46, Iter 7, disc loss: 0.08387213179677185, policy loss: 4.400165914645516
Experience 46, Iter 8, disc loss: 0.0788110432995023, policy loss: 3.9802385816544836
Experience 46, Iter 9, disc loss: 0.07335707651261159, policy loss: 5.213743718381504
Experience 46, Iter 10, disc loss: 0.07449189858569552, policy loss: 5.1008787158314455
Experience 46, Iter 11, disc loss: 0.08661507319547679, policy loss: 4.645082195789466
Experience 46, Iter 12, disc loss: 0.09387729546841343, policy loss: 4.650776125876116
Experience 46, Iter 13, disc loss: 0.08874224421590998, policy loss: 4.513627998812288
Experience 46, Iter 14, disc loss: 0.0951278304151618, policy loss: 4.880684419919439
Experience 46, Iter 15, disc loss: 0.0824252088507151, policy loss: 4.767023271443002
Experience 46, Iter 16, disc loss: 0.07930269456247062, policy loss: 4.943285579961601
Experience 46, Iter 17, disc loss: 0.09210849550274472, policy loss: 4.692333054945803
Experience 46, Iter 18, disc loss: 0.08912797103849605, policy loss: 4.778352761063889
Experience 46, Iter 19, disc loss: 0.08880385791974267, policy loss: 5.109873954127289
Experience 46, Iter 20, disc loss: 0.10217363456361005, policy loss: 4.546680536750367
Experience 46, Iter 21, disc loss: 0.12343304124400767, policy loss: 4.112142169599232
Experience 46, Iter 22, disc loss: 0.10975513582145804, policy loss: 4.682085981525596
Experience 46, Iter 23, disc loss: 0.10565282702708242, policy loss: 4.925006658366962
Experience 46, Iter 24, disc loss: 0.13768596627976737, policy loss: 4.166806903020911
Experience 46, Iter 25, disc loss: 0.1261508433246512, policy loss: 4.544231838082119
Experience 46, Iter 26, disc loss: 0.1358289643302329, policy loss: 4.9062074737254155
Experience 46, Iter 27, disc loss: 0.13363226716378585, policy loss: 4.783711832878712
Experience 46, Iter 28, disc loss: 0.1720560807242585, policy loss: 4.158491306262732
Experience 46, Iter 29, disc loss: 0.1589218118497541, policy loss: 4.502661933601415
Experience 46, Iter 30, disc loss: 0.2072391218952647, policy loss: 3.544931352508231
Experience 46, Iter 31, disc loss: 0.17185626074449567, policy loss: 5.272142595841141
Experience 46, Iter 32, disc loss: 0.2031183530893322, policy loss: 4.6887809379285175
Experience 46, Iter 33, disc loss: 0.17948018970464527, policy loss: 4.966888145688699
Experience 46, Iter 34, disc loss: 0.2264468770721192, policy loss: 4.5783157381314865
Experience 46, Iter 35, disc loss: 0.22055893169110663, policy loss: 4.723669996911564
Experience 46, Iter 36, disc loss: 0.26988073167863935, policy loss: 5.358629959476891
Experience 46, Iter 37, disc loss: 0.2908737957056179, policy loss: 4.703790820673467
Experience 46, Iter 38, disc loss: 0.28320468283631095, policy loss: 5.122237482402721
Experience 46, Iter 39, disc loss: 0.3459347588321652, policy loss: 5.465535687064639
Experience 46, Iter 40, disc loss: 0.28588275954421305, policy loss: 5.306662624950421
Experience 46, Iter 41, disc loss: 0.3551715168600112, policy loss: 4.535057511869862
Experience 46, Iter 42, disc loss: 0.4181398541597139, policy loss: 4.051062524826235
Experience 46, Iter 43, disc loss: 0.3545557188985118, policy loss: 4.8536080186183135
Experience 46, Iter 44, disc loss: 0.4601640405767003, policy loss: 5.6288191361522095
Experience 46, Iter 45, disc loss: 0.5239528252595471, policy loss: 4.001040245960116
Experience 46, Iter 46, disc loss: 0.4049374124393557, policy loss: 4.114476278335682
Experience 46, Iter 47, disc loss: 0.4459317534715829, policy loss: 4.794694968737971
Experience 46, Iter 48, disc loss: 0.45678343301355745, policy loss: 4.34303458409392
Experience 46, Iter 49, disc loss: 0.4852922496821585, policy loss: 4.629564074764412
Experience: 47
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0013],
        [0.0351],
        [0.3790],
        [0.0040]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0060, 0.0508, 0.1977, 0.0045, 0.0020, 0.8148]],

        [[0.0060, 0.0508, 0.1977, 0.0045, 0.0020, 0.8148]],

        [[0.0060, 0.0508, 0.1977, 0.0045, 0.0020, 0.8148]],

        [[0.0060, 0.0508, 0.1977, 0.0045, 0.0020, 0.8148]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0052, 0.1403, 1.5160, 0.0160], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0052, 0.1403, 1.5160, 0.0160])
N: 470
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1881.0000, 1881.0000, 1881.0000, 1881.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.271
Iter 2/2000 - Loss: 0.780
Iter 3/2000 - Loss: 0.209
Iter 4/2000 - Loss: 0.240
Iter 5/2000 - Loss: 0.403
Iter 6/2000 - Loss: 0.284
Iter 7/2000 - Loss: 0.087
Iter 8/2000 - Loss: -0.010
Iter 9/2000 - Loss: -0.022
Iter 10/2000 - Loss: -0.061
Iter 11/2000 - Loss: -0.185
Iter 12/2000 - Loss: -0.370
Iter 13/2000 - Loss: -0.565
Iter 14/2000 - Loss: -0.746
Iter 15/2000 - Loss: -0.923
Iter 16/2000 - Loss: -1.120
Iter 17/2000 - Loss: -1.355
Iter 18/2000 - Loss: -1.627
Iter 19/2000 - Loss: -1.922
Iter 20/2000 - Loss: -2.227
Iter 1981/2000 - Loss: -9.097
Iter 1982/2000 - Loss: -9.097
Iter 1983/2000 - Loss: -9.097
Iter 1984/2000 - Loss: -9.097
Iter 1985/2000 - Loss: -9.097
Iter 1986/2000 - Loss: -9.097
Iter 1987/2000 - Loss: -9.098
Iter 1988/2000 - Loss: -9.098
Iter 1989/2000 - Loss: -9.098
Iter 1990/2000 - Loss: -9.098
Iter 1991/2000 - Loss: -9.098
Iter 1992/2000 - Loss: -9.098
Iter 1993/2000 - Loss: -9.098
Iter 1994/2000 - Loss: -9.098
Iter 1995/2000 - Loss: -9.098
Iter 1996/2000 - Loss: -9.098
Iter 1997/2000 - Loss: -9.098
Iter 1998/2000 - Loss: -9.098
Iter 1999/2000 - Loss: -9.098
Iter 2000/2000 - Loss: -9.098
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.0728,  7.6689, 30.5154,  9.0610, 17.3971, 40.9855]],

        [[11.9219, 17.5695,  9.4310,  1.5575,  1.9556, 15.7558]],

        [[13.0739, 20.9326, 11.0956,  1.1310,  1.4121, 20.3378]],

        [[11.4294, 19.6435, 12.8356,  3.0553,  1.8014, 36.3179]]])
Signal Variance: tensor([ 0.0817,  1.3220, 15.3041,  0.4673])
Estimated target variance: tensor([0.0052, 0.1403, 1.5160, 0.0160])
N: 470
Signal to noise ratio: tensor([17.1858, 64.7585, 90.2792, 43.4292])
Bound on condition number: tensor([ 138816.8752, 1971025.5264, 3830658.5747,  886467.2741])
Policy Optimizer learning rate:
0.00952689885301437
Experience 47, Iter 0, disc loss: 0.4984545579649755, policy loss: 4.2400192369478145
Experience 47, Iter 1, disc loss: 0.37787229664086497, policy loss: 3.511662200950109
Experience 47, Iter 2, disc loss: 0.4027760358049463, policy loss: 3.1661690429185843
Experience 47, Iter 3, disc loss: 0.3577087989275498, policy loss: 4.423069645607975
Experience 47, Iter 4, disc loss: 0.3379433859036549, policy loss: 4.846568778381029
Experience 47, Iter 5, disc loss: 0.3713175690965846, policy loss: 3.1676988984203884
Experience 47, Iter 6, disc loss: 0.3621556843321887, policy loss: 3.915513911623254
Experience 47, Iter 7, disc loss: 0.3771394953749098, policy loss: 3.9440227983383345
Experience 47, Iter 8, disc loss: 0.2932054202067843, policy loss: 3.7426912484447694
Experience 47, Iter 9, disc loss: 0.38790199496428457, policy loss: 4.154122367633042
Experience 47, Iter 10, disc loss: 0.3340356497699155, policy loss: 4.551726363356939
Experience 47, Iter 11, disc loss: 0.38979327532346053, policy loss: 4.467641129727192
Experience 47, Iter 12, disc loss: 0.3044781470350668, policy loss: 4.742351072846874
Experience 47, Iter 13, disc loss: 0.3653606801158121, policy loss: 4.2597301519779105
Experience 47, Iter 14, disc loss: 0.32496723145759526, policy loss: 4.663225075632317
Experience 47, Iter 15, disc loss: 0.25018629568632, policy loss: 4.700569644429191
Experience 47, Iter 16, disc loss: 0.2971266928451264, policy loss: 3.5601885107382816
Experience 47, Iter 17, disc loss: 0.22494274799050712, policy loss: 4.811402099834917
Experience 47, Iter 18, disc loss: 0.2950194514536723, policy loss: 4.180758149340351
Experience 47, Iter 19, disc loss: 0.20241522743790796, policy loss: 4.975264105564865
Experience 47, Iter 20, disc loss: 0.17910198781096012, policy loss: 5.108829099851493
Experience 47, Iter 21, disc loss: 0.25897291430027813, policy loss: 4.617518452671769
Experience 47, Iter 22, disc loss: 0.16247937687733366, policy loss: 5.730582010226493
Experience 47, Iter 23, disc loss: 0.2493944239050748, policy loss: 4.317726943402919
Experience 47, Iter 24, disc loss: 0.2566455677253627, policy loss: 3.9550020547198272
Experience 47, Iter 25, disc loss: 0.21778503387058573, policy loss: 3.890386032944015
Experience 47, Iter 26, disc loss: 0.24050722761182583, policy loss: 4.754933529628253
Experience 47, Iter 27, disc loss: 0.15582345978044862, policy loss: 4.767643156105635
Experience 47, Iter 28, disc loss: 0.20452274227380243, policy loss: 4.8896908841663675
Experience 47, Iter 29, disc loss: 0.15383889294515968, policy loss: 4.903433640318972
Experience 47, Iter 30, disc loss: 0.15556302699647087, policy loss: 4.820557820875902
Experience 47, Iter 31, disc loss: 0.15663304840671288, policy loss: 4.588962454468103
Experience 47, Iter 32, disc loss: 0.15227156445673878, policy loss: 4.736053566115686
Experience 47, Iter 33, disc loss: 0.168338348543469, policy loss: 4.145464583664646
Experience 47, Iter 34, disc loss: 0.16840760616605804, policy loss: 4.075427533509164
Experience 47, Iter 35, disc loss: 0.17317983574105733, policy loss: 5.256139389201483
Experience 47, Iter 36, disc loss: 0.13838851531918409, policy loss: 5.4024078292346775
Experience 47, Iter 37, disc loss: 0.173727504977627, policy loss: 4.339880997725402
Experience 47, Iter 38, disc loss: 0.19284124528299634, policy loss: 4.713974989158303
Experience 47, Iter 39, disc loss: 0.17859215395235462, policy loss: 4.513775701686418
Experience 47, Iter 40, disc loss: 0.12707978257836994, policy loss: 4.703623309150434
Experience 47, Iter 41, disc loss: 0.14559072266617828, policy loss: 5.812253518269315
Experience 47, Iter 42, disc loss: 0.29430113401661806, policy loss: 4.647735822365794
Experience 47, Iter 43, disc loss: 0.16116402842693606, policy loss: 5.048147614910623
Experience 47, Iter 44, disc loss: 0.1785874693710556, policy loss: 5.860807605464155
Experience 47, Iter 45, disc loss: 0.1651042507339143, policy loss: 6.174783011813051
Experience 47, Iter 46, disc loss: 0.17285224849804612, policy loss: 5.026639229444625
Experience 47, Iter 47, disc loss: 0.17237371572876098, policy loss: 5.0937856027704225
Experience 47, Iter 48, disc loss: 0.14479221585554833, policy loss: 6.0910210539993255
Experience 47, Iter 49, disc loss: 0.1420048037145582, policy loss: 5.386316452974641
Experience: 48
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0014],
        [0.0430],
        [0.4407],
        [0.0071]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0063, 0.0565, 0.3222, 0.0054, 0.0040, 0.9828]],

        [[0.0063, 0.0565, 0.3222, 0.0054, 0.0040, 0.9828]],

        [[0.0063, 0.0565, 0.3222, 0.0054, 0.0040, 0.9828]],

        [[0.0063, 0.0565, 0.3222, 0.0054, 0.0040, 0.9828]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0056, 0.1718, 1.7628, 0.0286], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0056, 0.1718, 1.7628, 0.0286])
N: 480
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1921.0000, 1921.0000, 1921.0000, 1921.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 0.760
Iter 2/2000 - Loss: 1.109
Iter 3/2000 - Loss: 0.675
Iter 4/2000 - Loss: 0.671
Iter 5/2000 - Loss: 0.778
Iter 6/2000 - Loss: 0.669
Iter 7/2000 - Loss: 0.497
Iter 8/2000 - Loss: 0.393
Iter 9/2000 - Loss: 0.338
Iter 10/2000 - Loss: 0.257
Iter 11/2000 - Loss: 0.110
Iter 12/2000 - Loss: -0.084
Iter 13/2000 - Loss: -0.294
Iter 14/2000 - Loss: -0.502
Iter 15/2000 - Loss: -0.711
Iter 16/2000 - Loss: -0.939
Iter 17/2000 - Loss: -1.195
Iter 18/2000 - Loss: -1.480
Iter 19/2000 - Loss: -1.784
Iter 20/2000 - Loss: -2.096
Iter 1981/2000 - Loss: -8.993
Iter 1982/2000 - Loss: -8.993
Iter 1983/2000 - Loss: -8.993
Iter 1984/2000 - Loss: -8.993
Iter 1985/2000 - Loss: -8.993
Iter 1986/2000 - Loss: -8.993
Iter 1987/2000 - Loss: -8.993
Iter 1988/2000 - Loss: -8.993
Iter 1989/2000 - Loss: -8.993
Iter 1990/2000 - Loss: -8.993
Iter 1991/2000 - Loss: -8.993
Iter 1992/2000 - Loss: -8.993
Iter 1993/2000 - Loss: -8.993
Iter 1994/2000 - Loss: -8.993
Iter 1995/2000 - Loss: -8.993
Iter 1996/2000 - Loss: -8.993
Iter 1997/2000 - Loss: -8.993
Iter 1998/2000 - Loss: -8.993
Iter 1999/2000 - Loss: -8.993
Iter 2000/2000 - Loss: -8.993
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.4454,  8.0982, 28.8213,  4.1359, 22.1981, 43.4073]],

        [[12.3397, 17.7061,  9.2452,  1.4548,  1.9151, 16.0846]],

        [[11.9947, 21.6926, 10.7454,  1.1786,  1.1724, 19.6796]],

        [[10.8670, 19.0869, 12.0375,  3.3048,  2.0760, 35.8512]]])
Signal Variance: tensor([ 0.0930,  1.4611, 15.8064,  0.4674])
Estimated target variance: tensor([0.0056, 0.1718, 1.7628, 0.0286])
N: 480
Signal to noise ratio: tensor([18.3686, 68.2603, 91.0289, 43.5583])
Bound on condition number: tensor([ 161955.2826, 2236545.9971, 3977408.9103,  910718.5723])
Policy Optimizer learning rate:
0.009516866549228193
Experience 48, Iter 0, disc loss: 0.8716381124645698, policy loss: 2.1514737082231035
Experience 48, Iter 1, disc loss: 0.780223588506418, policy loss: 2.265651799347874
Experience 48, Iter 2, disc loss: 0.678725306379367, policy loss: 3.1271355626976094
Experience 48, Iter 3, disc loss: 0.5069674752595348, policy loss: 4.775132266443127
Experience 48, Iter 4, disc loss: 0.6487941402274202, policy loss: 4.626500384612899
Experience 48, Iter 5, disc loss: 0.6931375343635554, policy loss: 5.924597774605998
Experience 48, Iter 6, disc loss: 0.6362452730583317, policy loss: 4.684715264175211
Experience 48, Iter 7, disc loss: 0.5574435600701954, policy loss: 3.401067836570753
Experience 48, Iter 8, disc loss: 0.4209976481887735, policy loss: 3.55790310528847
Experience 48, Iter 9, disc loss: 0.36494893733216827, policy loss: 3.60252317270693
Experience 48, Iter 10, disc loss: 0.44945157075306436, policy loss: 2.7155677529091617
Experience 48, Iter 11, disc loss: 0.42429929433354197, policy loss: 2.9656227462279894
Experience 48, Iter 12, disc loss: 0.33368378088102796, policy loss: 3.837360436965909
Experience 48, Iter 13, disc loss: 0.5002093882637797, policy loss: 3.186895062243803
Experience 48, Iter 14, disc loss: 0.41386324942508107, policy loss: 4.093330382456729
Experience 48, Iter 15, disc loss: 0.2704667192559441, policy loss: 5.320095229373643
Experience 48, Iter 16, disc loss: 0.27936299463984704, policy loss: 5.098954017522016
Experience 48, Iter 17, disc loss: 0.307007051643316, policy loss: 4.597715109379399
Experience 48, Iter 18, disc loss: 0.29230137152818864, policy loss: 5.906715653942145
Experience 48, Iter 19, disc loss: 0.30192119601060463, policy loss: 5.125092169150028
Experience 48, Iter 20, disc loss: 0.2772142678881793, policy loss: 4.910796305516391
Experience 48, Iter 21, disc loss: 0.27095125880579013, policy loss: 4.387582692462773
Experience 48, Iter 22, disc loss: 0.2321391501056105, policy loss: 4.598858162345168
Experience 48, Iter 23, disc loss: 0.22605880671146777, policy loss: 4.365409576918971
Experience 48, Iter 24, disc loss: 0.22007309550954585, policy loss: 4.433983513370392
Experience 48, Iter 25, disc loss: 0.1799014487657995, policy loss: 4.974650336517884
Experience 48, Iter 26, disc loss: 0.16643570914013267, policy loss: 4.932476859989306
Experience 48, Iter 27, disc loss: 0.17570036225338764, policy loss: 3.99447308536865
Experience 48, Iter 28, disc loss: 0.18875811244120067, policy loss: 4.110259016917578
Experience 48, Iter 29, disc loss: 0.16396435744836582, policy loss: 4.427697750432241
Experience 48, Iter 30, disc loss: 0.15581028031619748, policy loss: 4.607865613294357
Experience 48, Iter 31, disc loss: 0.15397626470030445, policy loss: 4.669281716816686
Experience 48, Iter 32, disc loss: 0.15878553455465827, policy loss: 4.543697880282847
Experience 48, Iter 33, disc loss: 0.1596890714845991, policy loss: 4.050249649200413
Experience 48, Iter 34, disc loss: 0.13687077827253744, policy loss: 4.919036941074752
Experience 48, Iter 35, disc loss: 0.1299761619798067, policy loss: 5.788585449851022
Experience 48, Iter 36, disc loss: 0.12019468975824382, policy loss: 5.473415645415429
Experience 48, Iter 37, disc loss: 0.13001141640737188, policy loss: 4.674245201991891
Experience 48, Iter 38, disc loss: 0.11789405682155121, policy loss: 5.476152480220749
Experience 48, Iter 39, disc loss: 0.10423097582393569, policy loss: 5.8933646567966385
Experience 48, Iter 40, disc loss: 0.09901796795882616, policy loss: 5.85763241875148
Experience 48, Iter 41, disc loss: 0.09708298900657962, policy loss: 6.497374674471543
Experience 48, Iter 42, disc loss: 0.09015918547348273, policy loss: 5.970223103242391
Experience 48, Iter 43, disc loss: 0.08709268771862487, policy loss: 6.088670915569486
Experience 48, Iter 44, disc loss: 0.08524945544506893, policy loss: 5.399907581887365
Experience 48, Iter 45, disc loss: 0.07568246182303778, policy loss: 5.204340054111181
Experience 48, Iter 46, disc loss: 0.08042356497271518, policy loss: 5.990577687829054
Experience 48, Iter 47, disc loss: 0.09671878072833026, policy loss: 5.795458636594477
Experience 48, Iter 48, disc loss: 0.0865181262494007, policy loss: 4.682005799110895
Experience 48, Iter 49, disc loss: 0.10495529046819016, policy loss: 4.762340407925484
Experience: 49
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0015],
        [0.0493],
        [0.4953],
        [0.0101]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0065, 0.0630, 0.4324, 0.0064, 0.0053, 1.1407]],

        [[0.0065, 0.0630, 0.4324, 0.0064, 0.0053, 1.1407]],

        [[0.0065, 0.0630, 0.4324, 0.0064, 0.0053, 1.1407]],

        [[0.0065, 0.0630, 0.4324, 0.0064, 0.0053, 1.1407]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0062, 0.1974, 1.9811, 0.0404], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0062, 0.1974, 1.9811, 0.0404])
N: 490
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([1961.0000, 1961.0000, 1961.0000, 1961.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.084
Iter 2/2000 - Loss: 1.359
Iter 3/2000 - Loss: 0.988
Iter 4/2000 - Loss: 0.966
Iter 5/2000 - Loss: 1.043
Iter 6/2000 - Loss: 0.936
Iter 7/2000 - Loss: 0.766
Iter 8/2000 - Loss: 0.645
Iter 9/2000 - Loss: 0.565
Iter 10/2000 - Loss: 0.462
Iter 11/2000 - Loss: 0.301
Iter 12/2000 - Loss: 0.095
Iter 13/2000 - Loss: -0.128
Iter 14/2000 - Loss: -0.352
Iter 15/2000 - Loss: -0.578
Iter 16/2000 - Loss: -0.818
Iter 17/2000 - Loss: -1.082
Iter 18/2000 - Loss: -1.370
Iter 19/2000 - Loss: -1.674
Iter 20/2000 - Loss: -1.986
Iter 1981/2000 - Loss: -8.889
Iter 1982/2000 - Loss: -8.889
Iter 1983/2000 - Loss: -8.889
Iter 1984/2000 - Loss: -8.889
Iter 1985/2000 - Loss: -8.889
Iter 1986/2000 - Loss: -8.889
Iter 1987/2000 - Loss: -8.889
Iter 1988/2000 - Loss: -8.889
Iter 1989/2000 - Loss: -8.889
Iter 1990/2000 - Loss: -8.889
Iter 1991/2000 - Loss: -8.889
Iter 1992/2000 - Loss: -8.889
Iter 1993/2000 - Loss: -8.889
Iter 1994/2000 - Loss: -8.889
Iter 1995/2000 - Loss: -8.889
Iter 1996/2000 - Loss: -8.889
Iter 1997/2000 - Loss: -8.889
Iter 1998/2000 - Loss: -8.889
Iter 1999/2000 - Loss: -8.889
Iter 2000/2000 - Loss: -8.889
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[10.4652,  7.5215, 28.9235,  3.1163, 22.5402, 42.6895]],

        [[11.9457, 19.2846,  8.4962,  1.8979,  0.8874, 16.8759]],

        [[12.7448, 20.5690,  9.0617,  1.0935,  0.7880, 19.8256]],

        [[10.3254, 18.2830, 11.8717,  1.5379,  1.7319, 42.5471]]])
Signal Variance: tensor([ 0.0886,  1.7882, 12.2667,  0.3775])
Estimated target variance: tensor([0.0062, 0.1974, 1.9811, 0.0404])
N: 490
Signal to noise ratio: tensor([18.0176, 76.0738, 80.3971, 39.1015])
Bound on condition number: tensor([ 159071.6726, 2835739.7065, 3167213.8525,  749175.7703])
Policy Optimizer learning rate:
0.009506844809962623
Experience 49, Iter 0, disc loss: 0.08479246854747116, policy loss: 5.433817348915045
Experience 49, Iter 1, disc loss: 0.09196740784792376, policy loss: 5.155516143802521
Experience 49, Iter 2, disc loss: 0.07766403610112538, policy loss: 6.368564109782147
Experience 49, Iter 3, disc loss: 0.07319953370283143, policy loss: 5.826659399338542
Experience 49, Iter 4, disc loss: 0.06376045513787494, policy loss: 5.52445882580768
Experience 49, Iter 5, disc loss: 0.06550553112774854, policy loss: 5.879420880863812
Experience 49, Iter 6, disc loss: 0.056912709455625515, policy loss: 5.804084590353107
Experience 49, Iter 7, disc loss: 0.05290479676398117, policy loss: 5.800456593828965
Experience 49, Iter 8, disc loss: 0.05314966421812797, policy loss: 5.856767767952032
Experience 49, Iter 9, disc loss: 0.0532150944258895, policy loss: 5.989040196508094
Experience 49, Iter 10, disc loss: 0.05447708733692895, policy loss: 5.230186840214034
Experience 49, Iter 11, disc loss: 0.052831204117169815, policy loss: 5.500317222712761
Experience 49, Iter 12, disc loss: 0.05106846422531884, policy loss: 6.086393305773679
Experience 49, Iter 13, disc loss: 0.047016549088065365, policy loss: 6.440071463711454
Experience 49, Iter 14, disc loss: 0.05088167892569104, policy loss: 5.017523138464755
Experience 49, Iter 15, disc loss: 0.051877866662267985, policy loss: 5.2211052988634465
Experience 49, Iter 16, disc loss: 0.044002377825679395, policy loss: 5.427257986505303
Experience 49, Iter 17, disc loss: 0.055172237268570366, policy loss: 4.551634218708678
Experience 49, Iter 18, disc loss: 0.04838933489763979, policy loss: 5.530640689269354
Experience 49, Iter 19, disc loss: 0.04804306872674398, policy loss: 5.131232449133232
Experience 49, Iter 20, disc loss: 0.045003162093521384, policy loss: 5.264646338118464
Experience 49, Iter 21, disc loss: 0.04518861155425012, policy loss: 5.20291608962422
Experience 49, Iter 22, disc loss: 0.0459427944493354, policy loss: 5.338562781743214
Experience 49, Iter 23, disc loss: 0.04013885709390572, policy loss: 6.341000108881702
Experience 49, Iter 24, disc loss: 0.04758956802322982, policy loss: 5.349204701142373
Experience 49, Iter 25, disc loss: 0.03604246805144925, policy loss: 5.724905829228978
Experience 49, Iter 26, disc loss: 0.041337148980964356, policy loss: 5.577633349447002
Experience 49, Iter 27, disc loss: 0.047046906652516235, policy loss: 5.402855848282947
Experience 49, Iter 28, disc loss: 0.038315578479626314, policy loss: 5.4874808518105205
Experience 49, Iter 29, disc loss: 0.04382189601226678, policy loss: 5.793325745716321
Experience 49, Iter 30, disc loss: 0.0440224337835943, policy loss: 6.460740369516076
Experience 49, Iter 31, disc loss: 0.03752093522150239, policy loss: 5.690937538004036
Experience 49, Iter 32, disc loss: 0.03670893980321113, policy loss: 5.609031253772189
Experience 49, Iter 33, disc loss: 0.03634268440841332, policy loss: 5.817965103515813
Experience 49, Iter 34, disc loss: 0.03473672626520749, policy loss: 5.557807122140123
Experience 49, Iter 35, disc loss: 0.03669804410439119, policy loss: 5.601518534688806
Experience 49, Iter 36, disc loss: 0.03653385370202681, policy loss: 5.631929251618767
Experience 49, Iter 37, disc loss: 0.04873632708015326, policy loss: 5.633893676841633
Experience 49, Iter 38, disc loss: 0.04114839711869236, policy loss: 5.543052364371381
Experience 49, Iter 39, disc loss: 0.047009839963436134, policy loss: 5.810978030747014
Experience 49, Iter 40, disc loss: 0.04010711686115624, policy loss: 6.111357736385479
Experience 49, Iter 41, disc loss: 0.03908102603488019, policy loss: 5.783332356557331
Experience 49, Iter 42, disc loss: 0.03191622639913033, policy loss: 6.260228361360673
Experience 49, Iter 43, disc loss: 0.032213231683236934, policy loss: 6.500120103256165
Experience 49, Iter 44, disc loss: 0.031431473827961166, policy loss: 6.169187263048775
Experience 49, Iter 45, disc loss: 0.03361409818742169, policy loss: 5.946329067658317
Experience 49, Iter 46, disc loss: 0.03850038914964385, policy loss: 6.167851413100218
Experience 49, Iter 47, disc loss: 0.037075494224348554, policy loss: 5.844138677584353
Experience 49, Iter 48, disc loss: 0.03230931539485789, policy loss: 6.202941899997008
Experience 49, Iter 49, disc loss: 0.03645106440472696, policy loss: 5.400976279423419
Experience: 50
***BEFORE OPTIMATION***
Noise Variance: tensor([[0.0016],
        [0.0546],
        [0.5489],
        [0.0124]], grad_fn=<AddBackward0>)
Lengthscale: tensor([[[0.0067, 0.0676, 0.5216, 0.0071, 0.0071, 1.2945]],

        [[0.0067, 0.0676, 0.5216, 0.0071, 0.0071, 1.2945]],

        [[0.0067, 0.0676, 0.5216, 0.0071, 0.0071, 1.2945]],

        [[0.0067, 0.0676, 0.5216, 0.0071, 0.0071, 1.2945]]],
       grad_fn=<SoftplusBackward>)
Signal Variance: tensor([0.0065, 0.2184, 2.1957, 0.0496], grad_fn=<SoftplusBackward>)
Estimated target variance: tensor([0.0065, 0.2184, 2.1957, 0.0496])
N: 500
Signal to noise ratio: tensor([2.0000, 2.0000, 2.0000, 2.0000], grad_fn=<SqrtBackward>)
Bound on condition number: tensor([2001.0000, 2001.0000, 2001.0000, 2001.0000], grad_fn=<AddBackward0>)
Iter 1/2000 - Loss: 1.302
Iter 2/2000 - Loss: 1.531
Iter 3/2000 - Loss: 1.197
Iter 4/2000 - Loss: 1.162
Iter 5/2000 - Loss: 1.217
Iter 6/2000 - Loss: 1.113
Iter 7/2000 - Loss: 0.941
Iter 8/2000 - Loss: 0.807
Iter 9/2000 - Loss: 0.709
Iter 10/2000 - Loss: 0.593
Iter 11/2000 - Loss: 0.422
Iter 12/2000 - Loss: 0.205
Iter 13/2000 - Loss: -0.030
Iter 14/2000 - Loss: -0.265
Iter 15/2000 - Loss: -0.502
Iter 16/2000 - Loss: -0.751
Iter 17/2000 - Loss: -1.021
Iter 18/2000 - Loss: -1.313
Iter 19/2000 - Loss: -1.620
Iter 20/2000 - Loss: -1.933
Iter 1981/2000 - Loss: -8.844
Iter 1982/2000 - Loss: -8.844
Iter 1983/2000 - Loss: -8.844
Iter 1984/2000 - Loss: -8.844
Iter 1985/2000 - Loss: -8.844
Iter 1986/2000 - Loss: -8.844
Iter 1987/2000 - Loss: -8.844
Iter 1988/2000 - Loss: -8.844
Iter 1989/2000 - Loss: -8.844
Iter 1990/2000 - Loss: -8.844
Iter 1991/2000 - Loss: -8.844
Iter 1992/2000 - Loss: -8.844
Iter 1993/2000 - Loss: -8.844
Iter 1994/2000 - Loss: -8.844
Iter 1995/2000 - Loss: -8.844
Iter 1996/2000 - Loss: -8.844
Iter 1997/2000 - Loss: -8.844
Iter 1998/2000 - Loss: -8.844
Iter 1999/2000 - Loss: -8.844
Iter 2000/2000 - Loss: -8.844
***AFTER OPTIMATION***
Noise Variance: tensor([[0.0003],
        [0.0003],
        [0.0019],
        [0.0002]])
Lengthscale: tensor([[[ 9.9924,  7.8095, 29.5603,  2.7713, 17.8840, 42.3403]],

        [[11.6359, 18.9631,  7.7403,  1.9017,  0.9026, 17.7980]],

        [[12.3870, 20.1808,  8.8833,  1.0958,  0.6968, 19.7972]],

        [[10.7607, 19.4565, 13.1496,  1.2755,  2.0065, 45.2705]]])
Signal Variance: tensor([ 0.0876,  1.6321, 10.7605,  0.3996])
Estimated target variance: tensor([0.0065, 0.2184, 2.1957, 0.0496])
N: 500
Signal to noise ratio: tensor([17.8398, 72.6228, 75.2393, 40.1257])
Bound on condition number: tensor([ 159130.9547, 2637037.3421, 2830478.9271,  805036.3876])
Policy Optimizer learning rate:
0.00949683362409269
Experience 50, Iter 0, disc loss: 0.03587411653158562, policy loss: 5.485192758893842
Experience 50, Iter 1, disc loss: 0.031058115190019466, policy loss: 5.956177401653721
Experience 50, Iter 2, disc loss: 0.035437987221111196, policy loss: 5.031624836796697
Experience 50, Iter 3, disc loss: 0.031672869612191366, policy loss: 5.5030292368394775
Experience 50, Iter 4, disc loss: 0.029658019549059216, policy loss: 5.8349692709379815
Experience 50, Iter 5, disc loss: 0.02893013995174905, policy loss: 5.821205575494027
Experience 50, Iter 6, disc loss: 0.029954497801554014, policy loss: 5.543188256275862
Experience 50, Iter 7, disc loss: 0.031495448488037564, policy loss: 5.407385799245973
Experience 50, Iter 8, disc loss: 0.035879311763646615, policy loss: 5.902005349888873
Experience 50, Iter 9, disc loss: 0.0318766534994204, policy loss: 5.559937951720598
Experience 50, Iter 10, disc loss: 0.0311124998514445, policy loss: 5.581894549100358
Experience 50, Iter 11, disc loss: 0.027557574581777634, policy loss: 6.20025744595046
Experience 50, Iter 12, disc loss: 0.02865132906964104, policy loss: 5.61610284758746
Experience 50, Iter 13, disc loss: 0.0337911597946941, policy loss: 5.678864976375043
Experience 50, Iter 14, disc loss: 0.03164259037764482, policy loss: 5.354134647525371
Experience 50, Iter 15, disc loss: 0.029491922594807284, policy loss: 5.699507564118277
Experience 50, Iter 16, disc loss: 0.028569900710040275, policy loss: 6.023786880668638
Experience 50, Iter 17, disc loss: 0.02963354359738767, policy loss: 6.461294482357916
Experience 50, Iter 18, disc loss: 0.031198158098559744, policy loss: 5.3429497601226785
Experience 50, Iter 19, disc loss: 0.025341251627312454, policy loss: 6.4747312823484915
Experience 50, Iter 20, disc loss: 0.02823980662958986, policy loss: 5.760402933781175
Experience 50, Iter 21, disc loss: 0.030739192516095845, policy loss: 6.051882800014546
Experience 50, Iter 22, disc loss: 0.025733879009011626, policy loss: 6.063013318018911
Experience 50, Iter 23, disc loss: 0.027830676663116945, policy loss: 5.531920508739715
Experience 50, Iter 24, disc loss: 0.02649106063137064, policy loss: 6.130686779112288
Experience 50, Iter 25, disc loss: 0.030986976938643218, policy loss: 5.848797009360155
Experience 50, Iter 26, disc loss: 0.02719613922323182, policy loss: 6.131448050989132
Experience 50, Iter 27, disc loss: 0.034888411756175165, policy loss: 5.766977882892831
Experience 50, Iter 28, disc loss: 0.023514114632283804, policy loss: 6.308888779851982
Experience 50, Iter 29, disc loss: 0.02535078254606179, policy loss: 6.482842426378715
Experience 50, Iter 30, disc loss: 0.030162615797475457, policy loss: 5.354471260725715
Experience 50, Iter 31, disc loss: 0.03279256299033677, policy loss: 5.581284575886287
Experience 50, Iter 32, disc loss: 0.026544581830303365, policy loss: 6.197760164099624
Experience 50, Iter 33, disc loss: 0.026229611173325045, policy loss: 5.760509448818646
Experience 50, Iter 34, disc loss: 0.02301671327702511, policy loss: 6.511095809093776
Experience 50, Iter 35, disc loss: 0.023168047814687103, policy loss: 6.022788767955237
Experience 50, Iter 36, disc loss: 0.024865336937798757, policy loss: 6.197528857231433
Experience 50, Iter 37, disc loss: 0.02180691635370397, policy loss: 6.140719527348364
Experience 50, Iter 38, disc loss: 0.02056550044538166, policy loss: 6.144595669504819
Experience 50, Iter 39, disc loss: 0.01912679320516808, policy loss: 6.758445846997038
Experience 50, Iter 40, disc loss: 0.019455483769827163, policy loss: 6.436349492391009
Experience 50, Iter 41, disc loss: 0.021317907591225392, policy loss: 6.109014461246334
Experience 50, Iter 42, disc loss: 0.02597004367007624, policy loss: 6.34194967177411
Experience 50, Iter 43, disc loss: 0.026009060878272705, policy loss: 6.105246746588694
Experience 50, Iter 44, disc loss: 0.02921548447211491, policy loss: 6.134678810036624
Experience 50, Iter 45, disc loss: 0.0295324753660303, policy loss: 6.311818995193731
Experience 50, Iter 46, disc loss: 0.034113705748740744, policy loss: 5.748860901268045
Experience 50, Iter 47, disc loss: 0.02537197418935324, policy loss: 6.389075349071343
Experience 50, Iter 48, disc loss: 0.02696162131267798, policy loss: 6.599777850889509
Experience 50, Iter 49, disc loss: 0.021531773140172926, policy loss: 6.1159327630032125
