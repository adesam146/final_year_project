Epoch:  1879 ratio_loss: 1.3663406895907406 post_loss: 5.748957584916866
post mean tensor([0.9862], requires_grad=True) post variance tensor([[0.0384]], grad_fn=<MmBackward>)
Epoch:  1880 ratio_loss: 1.3341125013521151 post_loss: 8.60973617003591
post mean tensor([0.9816], requires_grad=True) post variance tensor([[0.0385]], grad_fn=<MmBackward>)
Epoch:  1881 ratio_loss: 1.3924902666372143 post_loss: 13.198584252634372
post mean tensor([0.9767], requires_grad=True) post variance tensor([[0.0386]], grad_fn=<MmBackward>)
Epoch:  1882 ratio_loss: 1.2820627830409113 post_loss: 4.95498981273273
post mean tensor([0.9721], requires_grad=True) post variance tensor([[0.0386]], grad_fn=<MmBackward>)
Epoch:  1883 ratio_loss: 1.2948832574027693 post_loss: 8.9985531515682
post mean tensor([0.9671], requires_grad=True) post variance tensor([[0.0387]], grad_fn=<MmBackward>)
Epoch:  1884 ratio_loss: 1.3008022680351865 post_loss: 15.41463425924457
post mean tensor([0.9617], requires_grad=True) post variance tensor([[0.0388]], grad_fn=<MmBackward>)
Epoch:  1885 ratio_loss: 1.2862772757393133 post_loss: 3.892312268019593
post mean tensor([0.9569], requires_grad=True) post variance tensor([[0.0389]], grad_fn=<MmBackward>)
Epoch:  1886 ratio_loss: 1.3672182833638404 post_loss: 11.104539069802202
post mean tensor([0.9531], requires_grad=True) post variance tensor([[0.0390]], grad_fn=<MmBackward>)
Epoch:  1887 ratio_loss: 1.3622666412218862 post_loss: 12.957319880972172
post mean tensor([0.9500], requires_grad=True) post variance tensor([[0.0392]], grad_fn=<MmBackward>)
Epoch:  1888 ratio_loss: 1.2332688797504185 post_loss: 4.902036455411798
post mean tensor([0.9480], requires_grad=True) post variance tensor([[0.0394]], grad_fn=<MmBackward>)
Epoch:  1889 ratio_loss: 1.3282442024573338 post_loss: 8.76192848624924
post mean tensor([0.9468], requires_grad=True) post variance tensor([[0.0396]], grad_fn=<MmBackward>)
Epoch:  1890 ratio_loss: 1.2314071433371765 post_loss: 9.166326842738211
post mean tensor([0.9457], requires_grad=True) post variance tensor([[0.0398]], grad_fn=<MmBackward>)
Epoch:  1891 ratio_loss: 1.2624650281816343 post_loss: 14.300943776976432
post mean tensor([0.9442], requires_grad=True) post variance tensor([[0.0400]], grad_fn=<MmBackward>)
Epoch:  1892 ratio_loss: 1.218080276186345 post_loss: 3.6026940082988785
post mean tensor([0.9454], requires_grad=True) post variance tensor([[0.0402]], grad_fn=<MmBackward>)
Epoch:  1893 ratio_loss: 1.3518791736133724 post_loss: 7.977109079467924
post mean tensor([0.9476], requires_grad=True) post variance tensor([[0.0403]], grad_fn=<MmBackward>)
Epoch:  1894 ratio_loss: 1.3651781561706433 post_loss: 13.571243939545425
post mean tensor([0.9495], requires_grad=True) post variance tensor([[0.0405]], grad_fn=<MmBackward>)
Epoch:  1895 ratio_loss: 1.307641111494032 post_loss: -2.5320144119408687
post mean tensor([0.9521], requires_grad=True) post variance tensor([[0.0408]], grad_fn=<MmBackward>)
Epoch:  1896 ratio_loss: 1.326819679408164 post_loss: 16.605898390609347
post mean tensor([0.9563], requires_grad=True) post variance tensor([[0.0410]], grad_fn=<MmBackward>)
Epoch:  1897 ratio_loss: 1.336642154715547 post_loss: 12.761841299830852
post mean tensor([0.9611], requires_grad=True) post variance tensor([[0.0412]], grad_fn=<MmBackward>)
Epoch:  1898 ratio_loss: 1.2920295375422932 post_loss: 1.3982287974914782
post mean tensor([0.9661], requires_grad=True) post variance tensor([[0.0415]], grad_fn=<MmBackward>)
Epoch:  1899 ratio_loss: 1.3191949724799348 post_loss: 9.745929987642052
post mean tensor([0.9717], requires_grad=True) post variance tensor([[0.0418]], grad_fn=<MmBackward>)
Epoch:  1900 ratio_loss: 1.3431246288662901 post_loss: 5.176819185062323
post mean tensor([0.9775], requires_grad=True) post variance tensor([[0.0421]], grad_fn=<MmBackward>)
Epoch:  1901 ratio_loss: 1.3281024970565203 post_loss: 5.111843425683719
post mean tensor([0.9837], requires_grad=True) post variance tensor([[0.0425]], grad_fn=<MmBackward>)
Epoch:  1902 ratio_loss: 1.324439867525363 post_loss: 17.838461145154977
post mean tensor([0.9895], requires_grad=True) post variance tensor([[0.0428]], grad_fn=<MmBackward>)
Epoch:  1903 ratio_loss: 1.3174405971339294 post_loss: 8.37685819337657
post mean tensor([0.9984], requires_grad=True) post variance tensor([[0.0429]], grad_fn=<MmBackward>)
Epoch:  1904 ratio_loss: 1.3360569580862205 post_loss: 4.017697848184895
post mean tensor([1.0092], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1905 ratio_loss: 1.353657023767831 post_loss: 11.906452914622834
post mean tensor([1.0209], requires_grad=True) post variance tensor([[0.0429]], grad_fn=<MmBackward>)
Epoch:  1906 ratio_loss: 1.3648740606627126 post_loss: 9.932901498464197
post mean tensor([1.0315], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1907 ratio_loss: 1.3548654439535321 post_loss: 2.202751971799035
post mean tensor([1.0407], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1908 ratio_loss: 1.318663391910735 post_loss: 7.266545058457201
post mean tensor([1.0495], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1909 ratio_loss: 1.2983281836995546 post_loss: 12.039011186128675
post mean tensor([1.0593], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1910 ratio_loss: 1.3529637164259782 post_loss: 8.588246944589224
post mean tensor([1.0684], requires_grad=True) post variance tensor([[0.0430]], grad_fn=<MmBackward>)
Epoch:  1911 ratio_loss: 1.3065497643106723 post_loss: 8.610158191043524
post mean tensor([1.0773], requires_grad=True) post variance tensor([[0.0431]], grad_fn=<MmBackward>)
Epoch:  1912 ratio_loss: 1.3494355255937682 post_loss: 9.612914232874623
post mean tensor([1.0856], requires_grad=True) post variance tensor([[0.0431]], grad_fn=<MmBackward>)
Epoch:  1913 ratio_loss: 1.3687582139664316 post_loss: 9.223682478551599
post mean tensor([1.0945], requires_grad=True) post variance tensor([[0.0431]], grad_fn=<MmBackward>)
Epoch:  1914 ratio_loss: 1.3314239160672994 post_loss: 9.200130339798948
post mean tensor([1.1022], requires_grad=True) post variance tensor([[0.0432]], grad_fn=<MmBackward>)
Epoch:  1915 ratio_loss: 1.3681943993835146 post_loss: 12.283587581957857
post mean tensor([1.1093], requires_grad=True) post variance tensor([[0.0432]], grad_fn=<MmBackward>)
Epoch:  1916 ratio_loss: 1.3133144251908093 post_loss: 7.95330905136825
post mean tensor([1.1158], requires_grad=True) post variance tensor([[0.0433]], grad_fn=<MmBackward>)
Epoch:  1917 ratio_loss: 1.3193238423322988 post_loss: 4.8880267641676785
post mean tensor([1.1217], requires_grad=True) post variance tensor([[0.0435]], grad_fn=<MmBackward>)
Epoch:  1918 ratio_loss: 1.2778908719494728 post_loss: 7.629949759642452
post mean tensor([1.1269], requires_grad=True) post variance tensor([[0.0437]], grad_fn=<MmBackward>)
Epoch:  1919 ratio_loss: 1.3537025458443834 post_loss: 6.902715525090532
post mean tensor([1.1314], requires_grad=True) post variance tensor([[0.0439]], grad_fn=<MmBackward>)
Epoch:  1920 ratio_loss: 1.3686348671218886 post_loss: 9.319017543965604
post mean tensor([1.1370], requires_grad=True) post variance tensor([[0.0440]], grad_fn=<MmBackward>)
Epoch:  1921 ratio_loss: 1.4025784008525752 post_loss: 8.63637812033669
post mean tensor([1.1417], requires_grad=True) post variance tensor([[0.0441]], grad_fn=<MmBackward>)
Epoch:  1922 ratio_loss: 1.3314575219547578 post_loss: 3.6564418505657352
post mean tensor([1.1462], requires_grad=True) post variance tensor([[0.0442]], grad_fn=<MmBackward>)
Epoch:  1923 ratio_loss: 1.29260138365854 post_loss: 16.436709964319657
post mean tensor([1.1512], requires_grad=True) post variance tensor([[0.0443]], grad_fn=<MmBackward>)
Epoch:  1924 ratio_loss: 1.2781317342896557 post_loss: 9.170032036096812
post mean tensor([1.1553], requires_grad=True) post variance tensor([[0.0444]], grad_fn=<MmBackward>)
Epoch:  1925 ratio_loss: 1.2894150886933657 post_loss: 7.187590947209848
post mean tensor([1.1588], requires_grad=True) post variance tensor([[0.0445]], grad_fn=<MmBackward>)
Epoch:  1926 ratio_loss: 1.3121063451025208 post_loss: 7.448030108992773
post mean tensor([1.1641], requires_grad=True) post variance tensor([[0.0445]], grad_fn=<MmBackward>)
Epoch:  1927 ratio_loss: 1.286956109882995 post_loss: 12.517840501724994
post mean tensor([1.1706], requires_grad=True) post variance tensor([[0.0443]], grad_fn=<MmBackward>)
Epoch:  1928 ratio_loss: 1.3538752855520424 post_loss: 7.245975356340635
post mean tensor([1.1758], requires_grad=True) post variance tensor([[0.0442]], grad_fn=<MmBackward>)
Epoch:  1929 ratio_loss: 1.2860365854567144 post_loss: 7.281329540289095
post mean tensor([1.1796], requires_grad=True) post variance tensor([[0.0442]], grad_fn=<MmBackward>)
Epoch:  1930 ratio_loss: 1.3284881401865984 post_loss: 12.71339085676517
post mean tensor([1.1817], requires_grad=True) post variance tensor([[0.0443]], grad_fn=<MmBackward>)
Epoch:  1931 ratio_loss: 1.2908872633828468 post_loss: 8.563437028731613
post mean tensor([1.1828], requires_grad=True) post variance tensor([[0.0444]], grad_fn=<MmBackward>)
Epoch:  1932 ratio_loss: 1.282849958378748 post_loss: 7.395882690767259
post mean tensor([1.1828], requires_grad=True) post variance tensor([[0.0445]], grad_fn=<MmBackward>)
Epoch:  1933 ratio_loss: 1.3267632117291164 post_loss: 10.548634748664606
post mean tensor([1.1812], requires_grad=True) post variance tensor([[0.0447]], grad_fn=<MmBackward>)
Epoch:  1934 ratio_loss: 1.2630140243599945 post_loss: 8.425914183674415
post mean tensor([1.1786], requires_grad=True) post variance tensor([[0.0450]], grad_fn=<MmBackward>)
Epoch:  1935 ratio_loss: 1.368488152817846 post_loss: 4.342212727949132
post mean tensor([1.1778], requires_grad=True) post variance tensor([[0.0451]], grad_fn=<MmBackward>)
Epoch:  1936 ratio_loss: 1.342185194040181 post_loss: 14.27129441013614
post mean tensor([1.1756], requires_grad=True) post variance tensor([[0.0451]], grad_fn=<MmBackward>)
Epoch:  1937 ratio_loss: 1.3513363286934956 post_loss: 7.600380720465833
post mean tensor([1.1749], requires_grad=True) post variance tensor([[0.0450]], grad_fn=<MmBackward>)
Epoch:  1938 ratio_loss: 1.3426845879478253 post_loss: 13.548254176766505
post mean tensor([1.1731], requires_grad=True) post variance tensor([[0.0448]], grad_fn=<MmBackward>)
Epoch:  1939 ratio_loss: 1.3079576366323054 post_loss: 9.892302406288408
post mean tensor([1.1705], requires_grad=True) post variance tensor([[0.0447]], grad_fn=<MmBackward>)
Epoch:  1940 ratio_loss: 1.286960504334718 post_loss: 5.01051792656691
post mean tensor([1.1668], requires_grad=True) post variance tensor([[0.0446]], grad_fn=<MmBackward>)
Epoch:  1941 ratio_loss: 1.274995422447223 post_loss: 13.707705896446353
post mean tensor([1.1620], requires_grad=True) post variance tensor([[0.0446]], grad_fn=<MmBackward>)
Epoch:  1942 ratio_loss: 1.3644430628527777 post_loss: 4.623358253121625
post mean tensor([1.1596], requires_grad=True) post variance tensor([[0.0445]], grad_fn=<MmBackward>)
Epoch:  1943 ratio_loss: 1.3175959761181648 post_loss: 4.662712296049574
post mean tensor([1.1562], requires_grad=True) post variance tensor([[0.0444]], grad_fn=<MmBackward>)
Epoch:  1944 ratio_loss: 1.3361735555216647 post_loss: 15.697425699209301
post mean tensor([1.1514], requires_grad=True) post variance tensor([[0.0444]], grad_fn=<MmBackward>)
Epoch:  1945 ratio_loss: 1.3272141566606794 post_loss: 5.118011651654518
post mean tensor([1.1459], requires_grad=True) post variance tensor([[0.0445]], grad_fn=<MmBackward>)
Epoch:  1946 ratio_loss: 1.3634005577692603 post_loss: 12.71598243182725
post mean tensor([1.1397], requires_grad=True) post variance tensor([[0.0446]], grad_fn=<MmBackward>)
Epoch:  1947 ratio_loss: 1.3076247114937138 post_loss: 4.960742947797101
post mean tensor([1.1325], requires_grad=True) post variance tensor([[0.0446]], grad_fn=<MmBackward>)
Epoch:  1948 ratio_loss: 1.3188849358290329 post_loss: 15.960826036374401
post mean tensor([1.1241], requires_grad=True) post variance tensor([[0.0449]], grad_fn=<MmBackward>)
Epoch:  1949 ratio_loss: 1.3006003725329176 post_loss: 0.06239389106138926
post mean tensor([1.1154], requires_grad=True) post variance tensor([[0.0451]], grad_fn=<MmBackward>)
Epoch:  1950 ratio_loss: 1.3976767537026793 post_loss: 6.37778771291298
post mean tensor([1.1059], requires_grad=True) post variance tensor([[0.0455]], grad_fn=<MmBackward>)
Epoch:  1951 ratio_loss: 1.2862020928444733 post_loss: 16.04218780092464
post mean tensor([1.0958], requires_grad=True) post variance tensor([[0.0456]], grad_fn=<MmBackward>)
Epoch:  1952 ratio_loss: 1.28820761836357 post_loss: 10.358868056518881
post mean tensor([1.0849], requires_grad=True) post variance tensor([[0.0457]], grad_fn=<MmBackward>)
Epoch:  1953 ratio_loss: 1.349167356498803 post_loss: 11.192699638665653
post mean tensor([1.0741], requires_grad=True) post variance tensor([[0.0457]], grad_fn=<MmBackward>)
Epoch:  1954 ratio_loss: 1.3006593405554465 post_loss: 12.143341911173632
post mean tensor([1.0631], requires_grad=True) post variance tensor([[0.0456]], grad_fn=<MmBackward>)
Epoch:  1955 ratio_loss: 1.3402755729711726 post_loss: 9.671294116291362
post mean tensor([1.0516], requires_grad=True) post variance tensor([[0.0456]], grad_fn=<MmBackward>)
Epoch:  1956 ratio_loss: 1.2728187540518343 post_loss: 7.986936879004153
post mean tensor([1.0398], requires_grad=True) post variance tensor([[0.0457]], grad_fn=<MmBackward>)
Epoch:  1957 ratio_loss: 1.3480293970343706 post_loss: 10.014320659626874
post mean tensor([1.0276], requires_grad=True) post variance tensor([[0.0458]], grad_fn=<MmBackward>)
Epoch:  1958 ratio_loss: 1.2775993833167756 post_loss: 3.8000136947853007
post mean tensor([1.0168], requires_grad=True) post variance tensor([[0.0459]], grad_fn=<MmBackward>)
Epoch:  1959 ratio_loss: 1.29404130489751 post_loss: 8.7726590973625
post mean tensor([1.0058], requires_grad=True) post variance tensor([[0.0461]], grad_fn=<MmBackward>)
Epoch:  1960 ratio_loss: 1.3525621784494346 post_loss: 7.619783536198742
post mean tensor([0.9970], requires_grad=True) post variance tensor([[0.0463]], grad_fn=<MmBackward>)
Epoch:  1961 ratio_loss: 1.298534678494626 post_loss: 6.292778516261617
post mean tensor([0.9879], requires_grad=True) post variance tensor([[0.0465]], grad_fn=<MmBackward>)
Epoch:  1962 ratio_loss: 1.3234687714927507 post_loss: 6.976492125395568
post mean tensor([0.9811], requires_grad=True) post variance tensor([[0.0467]], grad_fn=<MmBackward>)
Epoch:  1963 ratio_loss: 1.3549311828691788 post_loss: 7.605508558606028
post mean tensor([0.9758], requires_grad=True) post variance tensor([[0.0468]], grad_fn=<MmBackward>)
Epoch:  1964 ratio_loss: 1.3396240231366892 post_loss: 11.052262569370109
post mean tensor([0.9720], requires_grad=True) post variance tensor([[0.0468]], grad_fn=<MmBackward>)
Epoch:  1965 ratio_loss: 1.3029675057674908 post_loss: 13.167531481457686
post mean tensor([0.9672], requires_grad=True) post variance tensor([[0.0469]], grad_fn=<MmBackward>)
Epoch:  1966 ratio_loss: 1.252989598361911 post_loss: 5.942574070947737
post mean tensor([0.9617], requires_grad=True) post variance tensor([[0.0469]], grad_fn=<MmBackward>)
Epoch:  1967 ratio_loss: 1.3121622616741204 post_loss: 9.56436025613268
post mean tensor([0.9556], requires_grad=True) post variance tensor([[0.0468]], grad_fn=<MmBackward>)
Epoch:  1968 ratio_loss: 1.3632403300130749 post_loss: 10.392773331743003
post mean tensor([0.9490], requires_grad=True) post variance tensor([[0.0467]], grad_fn=<MmBackward>)
Epoch:  1969 ratio_loss: 1.3537825104577235 post_loss: 9.764377265011356
post mean tensor([0.9446], requires_grad=True) post variance tensor([[0.0466]], grad_fn=<MmBackward>)
Epoch:  1970 ratio_loss: 1.2339952018550435 post_loss: 11.085706347089022
post mean tensor([0.9424], requires_grad=True) post variance tensor([[0.0464]], grad_fn=<MmBackward>)
Epoch:  1971 ratio_loss: 1.281141159275105 post_loss: 9.093436069389302
post mean tensor([0.9398], requires_grad=True) post variance tensor([[0.0462]], grad_fn=<MmBackward>)
Epoch:  1972 ratio_loss: 1.2617659286050293 post_loss: 11.47120695091138
post mean tensor([0.9392], requires_grad=True) post variance tensor([[0.0460]], grad_fn=<MmBackward>)
Epoch:  1973 ratio_loss: 1.234946780160545 post_loss: 10.830674139051766
post mean tensor([0.9405], requires_grad=True) post variance tensor([[0.0458]], grad_fn=<MmBackward>)
Epoch:  1974 ratio_loss: 1.2741038012814547 post_loss: 9.18666533918739
post mean tensor([0.9414], requires_grad=True) post variance tensor([[0.0457]], grad_fn=<MmBackward>)
Epoch:  1975 ratio_loss: 1.3427560495646023 post_loss: 4.9878203890667265
post mean tensor([0.9420], requires_grad=True) post variance tensor([[0.0456]], grad_fn=<MmBackward>)
Epoch:  1976 ratio_loss: 1.2840473957250467 post_loss: 11.726950650076946
post mean tensor([0.9444], requires_grad=True) post variance tensor([[0.0455]], grad_fn=<MmBackward>)
Epoch:  1977 ratio_loss: 1.3224885037597942 post_loss: 10.663856152033288
post mean tensor([0.9487], requires_grad=True) post variance tensor([[0.0453]], grad_fn=<MmBackward>)
Epoch:  1978 ratio_loss: 1.3277243956162634 post_loss: 11.356395853309357
post mean tensor([0.9549], requires_grad=True) post variance tensor([[0.0450]], grad_fn=<MmBackward>)
Epoch:  1979 ratio_loss: 1.3578893430252532 post_loss: 9.227988114621436
post mean tensor([0.9628], requires_grad=True) post variance tensor([[0.0446]], grad_fn=<MmBackward>)
Epoch:  1980 ratio_loss: 1.278820286313561 post_loss: 7.736229465003386
post mean tensor([0.9695], requires_grad=True) post variance tensor([[0.0443]], grad_fn=<MmBackward>)
Epoch:  1981 ratio_loss: 1.3470032438173638 post_loss: 13.233286543861443
post mean tensor([0.9774], requires_grad=True) post variance tensor([[0.0439]], grad_fn=<MmBackward>)
Epoch:  1982 ratio_loss: 1.33782132801102 post_loss: 9.990393782890461
post mean tensor([0.9840], requires_grad=True) post variance tensor([[0.0435]], grad_fn=<MmBackward>)
Epoch:  1983 ratio_loss: 1.353508972915147 post_loss: 17.698585258470402
post mean tensor([0.9932], requires_grad=True) post variance tensor([[0.0427]], grad_fn=<MmBackward>)
Epoch:  1984 ratio_loss: 1.3235320038772076 post_loss: 3.0988653101870227
post mean tensor([1.0022], requires_grad=True) post variance tensor([[0.0419]], grad_fn=<MmBackward>)
Epoch:  1985 ratio_loss: 1.3699859836518486 post_loss: 8.989012271569546
post mean tensor([1.0102], requires_grad=True) post variance tensor([[0.0413]], grad_fn=<MmBackward>)
Epoch:  1986 ratio_loss: 1.3086642385489986 post_loss: 10.82307560479627
post mean tensor([1.0171], requires_grad=True) post variance tensor([[0.0408]], grad_fn=<MmBackward>)
Epoch:  1987 ratio_loss: 1.2522251737650218 post_loss: 10.965885099883279
post mean tensor([1.0258], requires_grad=True) post variance tensor([[0.0402]], grad_fn=<MmBackward>)
Epoch:  1988 ratio_loss: 1.280261160728723 post_loss: 9.902109869931806
post mean tensor([1.0358], requires_grad=True) post variance tensor([[0.0396]], grad_fn=<MmBackward>)
Epoch:  1989 ratio_loss: 1.2974542573019345 post_loss: 9.68082191665318
post mean tensor([1.0441], requires_grad=True) post variance tensor([[0.0390]], grad_fn=<MmBackward>)
Epoch:  1990 ratio_loss: 1.3057623265502487 post_loss: 15.728249052566134
post mean tensor([1.0544], requires_grad=True) post variance tensor([[0.0381]], grad_fn=<MmBackward>)
Epoch:  1991 ratio_loss: 1.3052869938552227 post_loss: 10.647071773983185
post mean tensor([1.0660], requires_grad=True) post variance tensor([[0.0371]], grad_fn=<MmBackward>)
Epoch:  1992 ratio_loss: 1.3242268598647975 post_loss: 14.09827205947071
post mean tensor([1.0786], requires_grad=True) post variance tensor([[0.0360]], grad_fn=<MmBackward>)
Epoch:  1993 ratio_loss: 1.311443435323929 post_loss: 11.826499143747682
post mean tensor([1.0900], requires_grad=True) post variance tensor([[0.0350]], grad_fn=<MmBackward>)
Epoch:  1994 ratio_loss: 1.3219572247593163 post_loss: 5.110742131047601
post mean tensor([1.1009], requires_grad=True) post variance tensor([[0.0342]], grad_fn=<MmBackward>)
Epoch:  1995 ratio_loss: 1.3265702662518204 post_loss: 12.085361860975123
post mean tensor([1.1103], requires_grad=True) post variance tensor([[0.0335]], grad_fn=<MmBackward>)
Epoch:  1996 ratio_loss: 1.3219936980283515 post_loss: 7.161973519440693
post mean tensor([1.1184], requires_grad=True) post variance tensor([[0.0328]], grad_fn=<MmBackward>)
Epoch:  1997 ratio_loss: 1.3085615667961066 post_loss: 11.616329056679772
post mean tensor([1.1250], requires_grad=True) post variance tensor([[0.0323]], grad_fn=<MmBackward>)
Epoch:  1998 ratio_loss: 1.3510979325730874 post_loss: 10.729556469911168
post mean tensor([1.1304], requires_grad=True) post variance tensor([[0.0319]], grad_fn=<MmBackward>)
Epoch:  1999 ratio_loss: 1.3429760344969934 post_loss: 8.383932584865944
post mean tensor([1.1348], requires_grad=True) post variance tensor([[0.0315]], grad_fn=<MmBackward>)
Learnt mean tensor([1.1348], requires_grad=True)
Learnt variance tensor([[0.0315]], grad_fn=<MmBackward>)
Expected mean tensor([0.9892])
Expected variance tensor([[0.0018]])